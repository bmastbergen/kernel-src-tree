x86-64: Use RIP-relative addressing for most per-CPU accesses

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jan Beulich <JBeulich@suse.com>
commit 97b67ae559947f1e208439a1bf6a734da3087006
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/97b67ae5.failed

Observing that per-CPU data (in the SMP case) is reachable by
exploiting 64-bit address wraparound (building on the default kernel
load address being at 16Mb), the one byte shorter RIP-relative
addressing form can be used for most per-CPU accesses. The one
exception are the "stable" reads, where the use of the "P" operand
modifier prevents the compiler from using RIP-relative addressing, but
is unavoidable due to the use of the "p" constraint (side note: with
gcc 4.9.x the intended effect of this isn't being achieved anymore,
see gcc bug 63637).

With the dependency on the minimum kernel load address, arbitrarily
low values for CONFIG_PHYSICAL_START are now no longer possible. A
link time assertion is being added, directing to the need to increase
that value when it triggers.

	Signed-off-by: Jan Beulich <jbeulich@suse.com>
Link: http://lkml.kernel.org/r/5458A1780200007800044A9D@mail.emea.novell.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 97b67ae559947f1e208439a1bf6a734da3087006)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/percpu.h
diff --cc arch/x86/include/asm/percpu.h
index 0da5200ee79d,e0ba66ca68c6..000000000000
--- a/arch/x86/include/asm/percpu.h
+++ b/arch/x86/include/asm/percpu.h
@@@ -359,34 -388,31 +388,40 @@@ do {									
   * per-thread variables implemented as per-cpu variables and thus
   * stable for the duration of the respective task.
   */
- #define this_cpu_read_stable(var)	percpu_from_op("mov", var, "p" (&(var)))
+ #define this_cpu_read_stable(var)	percpu_stable_op("mov", var)
  
++<<<<<<< HEAD
 +#define __this_cpu_read_1(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
 +#define __this_cpu_read_2(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
 +#define __this_cpu_read_4(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
++=======
+ #define raw_cpu_read_1(pcp)		percpu_from_op("mov", pcp)
+ #define raw_cpu_read_2(pcp)		percpu_from_op("mov", pcp)
+ #define raw_cpu_read_4(pcp)		percpu_from_op("mov", pcp)
 -
 -#define raw_cpu_write_1(pcp, val)	percpu_to_op("mov", (pcp), val)
 -#define raw_cpu_write_2(pcp, val)	percpu_to_op("mov", (pcp), val)
 -#define raw_cpu_write_4(pcp, val)	percpu_to_op("mov", (pcp), val)
 -#define raw_cpu_add_1(pcp, val)		percpu_add_op((pcp), val)
 -#define raw_cpu_add_2(pcp, val)		percpu_add_op((pcp), val)
 -#define raw_cpu_add_4(pcp, val)		percpu_add_op((pcp), val)
 -#define raw_cpu_and_1(pcp, val)		percpu_to_op("and", (pcp), val)
 -#define raw_cpu_and_2(pcp, val)		percpu_to_op("and", (pcp), val)
 -#define raw_cpu_and_4(pcp, val)		percpu_to_op("and", (pcp), val)
 -#define raw_cpu_or_1(pcp, val)		percpu_to_op("or", (pcp), val)
 -#define raw_cpu_or_2(pcp, val)		percpu_to_op("or", (pcp), val)
 -#define raw_cpu_or_4(pcp, val)		percpu_to_op("or", (pcp), val)
 -#define raw_cpu_xchg_1(pcp, val)	percpu_xchg_op(pcp, val)
 -#define raw_cpu_xchg_2(pcp, val)	percpu_xchg_op(pcp, val)
 -#define raw_cpu_xchg_4(pcp, val)	percpu_xchg_op(pcp, val)
++>>>>>>> 97b67ae55994 (x86-64: Use RIP-relative addressing for most per-CPU accesses)
 +
 +#define __this_cpu_write_1(pcp, val)	percpu_to_op("mov", (pcp), val)
 +#define __this_cpu_write_2(pcp, val)	percpu_to_op("mov", (pcp), val)
 +#define __this_cpu_write_4(pcp, val)	percpu_to_op("mov", (pcp), val)
 +#define __this_cpu_add_1(pcp, val)	percpu_add_op((pcp), val)
 +#define __this_cpu_add_2(pcp, val)	percpu_add_op((pcp), val)
 +#define __this_cpu_add_4(pcp, val)	percpu_add_op((pcp), val)
 +#define __this_cpu_and_1(pcp, val)	percpu_to_op("and", (pcp), val)
 +#define __this_cpu_and_2(pcp, val)	percpu_to_op("and", (pcp), val)
 +#define __this_cpu_and_4(pcp, val)	percpu_to_op("and", (pcp), val)
 +#define __this_cpu_or_1(pcp, val)	percpu_to_op("or", (pcp), val)
 +#define __this_cpu_or_2(pcp, val)	percpu_to_op("or", (pcp), val)
 +#define __this_cpu_or_4(pcp, val)	percpu_to_op("or", (pcp), val)
 +#define __this_cpu_xor_1(pcp, val)	percpu_to_op("xor", (pcp), val)
 +#define __this_cpu_xor_2(pcp, val)	percpu_to_op("xor", (pcp), val)
 +#define __this_cpu_xor_4(pcp, val)	percpu_to_op("xor", (pcp), val)
 +#define __this_cpu_xchg_1(pcp, val)	percpu_xchg_op(pcp, val)
 +#define __this_cpu_xchg_2(pcp, val)	percpu_xchg_op(pcp, val)
 +#define __this_cpu_xchg_4(pcp, val)	percpu_xchg_op(pcp, val)
  
- #define this_cpu_read_1(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
- #define this_cpu_read_2(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
- #define this_cpu_read_4(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
+ #define this_cpu_read_1(pcp)		percpu_from_op("mov", pcp)
+ #define this_cpu_read_2(pcp)		percpu_from_op("mov", pcp)
+ #define this_cpu_read_4(pcp)		percpu_from_op("mov", pcp)
  #define this_cpu_write_1(pcp, val)	percpu_to_op("mov", (pcp), val)
  #define this_cpu_write_2(pcp, val)	percpu_to_op("mov", (pcp), val)
  #define this_cpu_write_4(pcp, val)	percpu_to_op("mov", (pcp), val)
@@@ -441,24 -464,22 +476,43 @@@
   * 32 bit must fall back to generic operations.
   */
  #ifdef CONFIG_X86_64
++<<<<<<< HEAD
 +#define __this_cpu_read_8(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
 +#define __this_cpu_write_8(pcp, val)	percpu_to_op("mov", (pcp), val)
 +#define __this_cpu_add_8(pcp, val)	percpu_add_op((pcp), val)
 +#define __this_cpu_and_8(pcp, val)	percpu_to_op("and", (pcp), val)
 +#define __this_cpu_or_8(pcp, val)	percpu_to_op("or", (pcp), val)
 +#define __this_cpu_xor_8(pcp, val)	percpu_to_op("xor", (pcp), val)
 +#define __this_cpu_add_return_8(pcp, val) percpu_add_return_op(pcp, val)
 +#define __this_cpu_xchg_8(pcp, nval)	percpu_xchg_op(pcp, nval)
 +#define __this_cpu_cmpxchg_8(pcp, oval, nval)	percpu_cmpxchg_op(pcp, oval, nval)
 +
 +#define this_cpu_read_8(pcp)		percpu_from_op("mov", (pcp), "m"(pcp))
 +#define this_cpu_write_8(pcp, val)	percpu_to_op("mov", (pcp), val)
 +#define this_cpu_add_8(pcp, val)	percpu_add_op((pcp), val)
 +#define this_cpu_and_8(pcp, val)	percpu_to_op("and", (pcp), val)
 +#define this_cpu_or_8(pcp, val)		percpu_to_op("or", (pcp), val)
 +#define this_cpu_xor_8(pcp, val)	percpu_to_op("xor", (pcp), val)
 +#define this_cpu_add_return_8(pcp, val)	percpu_add_return_op(pcp, val)
 +#define this_cpu_xchg_8(pcp, nval)	percpu_xchg_op(pcp, nval)
++=======
+ #define raw_cpu_read_8(pcp)			percpu_from_op("mov", pcp)
+ #define raw_cpu_write_8(pcp, val)		percpu_to_op("mov", (pcp), val)
+ #define raw_cpu_add_8(pcp, val)			percpu_add_op((pcp), val)
+ #define raw_cpu_and_8(pcp, val)			percpu_to_op("and", (pcp), val)
+ #define raw_cpu_or_8(pcp, val)			percpu_to_op("or", (pcp), val)
+ #define raw_cpu_add_return_8(pcp, val)		percpu_add_return_op(pcp, val)
+ #define raw_cpu_xchg_8(pcp, nval)		percpu_xchg_op(pcp, nval)
+ #define raw_cpu_cmpxchg_8(pcp, oval, nval)	percpu_cmpxchg_op(pcp, oval, nval)
+ 
+ #define this_cpu_read_8(pcp)			percpu_from_op("mov", pcp)
+ #define this_cpu_write_8(pcp, val)		percpu_to_op("mov", (pcp), val)
+ #define this_cpu_add_8(pcp, val)		percpu_add_op((pcp), val)
+ #define this_cpu_and_8(pcp, val)		percpu_to_op("and", (pcp), val)
+ #define this_cpu_or_8(pcp, val)			percpu_to_op("or", (pcp), val)
+ #define this_cpu_add_return_8(pcp, val)		percpu_add_return_op(pcp, val)
+ #define this_cpu_xchg_8(pcp, nval)		percpu_xchg_op(pcp, nval)
++>>>>>>> 97b67ae55994 (x86-64: Use RIP-relative addressing for most per-CPU accesses)
  #define this_cpu_cmpxchg_8(pcp, oval, nval)	percpu_cmpxchg_op(pcp, oval, nval)
  
  /*
* Unmerged path arch/x86/include/asm/percpu.h
diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
index 9b638c1dbfb1..3878bdeb2754 100644
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@ -184,6 +184,8 @@ SECTIONS
 	 * start another segment - init.
 	 */
 	PERCPU_VADDR(INTERNODE_CACHE_BYTES, 0, :percpu)
+	ASSERT(SIZEOF(.data..percpu) < CONFIG_PHYSICAL_START,
+	       "per-CPU data too large - increase CONFIG_PHYSICAL_START")
 #endif
 
 	INIT_TEXT_SECTION(PAGE_SIZE)
