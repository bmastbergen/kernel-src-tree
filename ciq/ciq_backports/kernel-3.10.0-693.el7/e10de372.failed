xfs: don't chain ioends during writepage submission

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit e10de3723c53378e7cf441529f563c316fdc0dd3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e10de372.failed

Currently we can build a long ioend chain during ->writepages that
gets attached to the writepage context. IO submission only then
occurs when we finish all the writepage processing. This means we
can have many ioends allocated and pending, and this violates the
mempool guarantees that we need to give about forwards progress.
i.e. we really should only have one ioend being built at a time,
otherwise we may drain the mempool trying to allocate a new ioend
and that blocks submission, completion and freeing of ioends that
are already in progress.

To prevent this situation from happening, we need to submit ioends
for IO as soon as they are ready for dispatch rather than queuing
them for later submission. This means the ioends have bios built
immediately and they get queued on any plug that is current active.
Hence if we schedule away from writeback, the ioends that have been
built will make forwards progress due to the plug flushing on
context switch. This will also prevent context switches from
creating unnecessary IO submission latency.

We can't completely avoid having nested IO allocation - when we have
a block size smaller than a page size, we still need to hold the
ioend submission until after we have marked the current page dirty.
Hence we may need multiple ioends to be held while the current page
is completely mapped and made ready for IO dispatch. We cannot avoid
this problem - the current code already has this ioend chaining
within a page so we can mostly ignore that it occurs.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit e10de3723c53378e7cf441529f563c316fdc0dd3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index acf6c4a54883,46dc9211bae7..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -37,6 -36,17 +37,20 @@@
  #include <linux/pagevec.h>
  #include <linux/writeback.h>
  
++<<<<<<< HEAD
++=======
+ /*
+  * structure owned by writepages passed to individual writepage calls
+  */
+ struct xfs_writepage_ctx {
+ 	struct xfs_bmbt_irec    imap;
+ 	bool			imap_valid;
+ 	unsigned int		io_type;
+ 	struct xfs_ioend	*ioend;
+ 	sector_t		last_block;
+ };
+ 
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  void
  xfs_count_page_state(
  	struct page		*page,
@@@ -540,29 -516,30 +527,51 @@@ xfs_add_to_ioend
  	struct inode		*inode,
  	struct buffer_head	*bh,
  	xfs_off_t		offset,
++<<<<<<< HEAD
 +	unsigned int		type,
 +	xfs_ioend_t		**result,
 +	int			need_ioend)
++=======
+ 	struct xfs_writepage_ctx *wpc,
+ 	struct list_head	*iolist)
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  {
 -	if (!wpc->ioend || wpc->io_type != wpc->ioend->io_type ||
 -	    bh->b_blocknr != wpc->last_block + 1) {
 -		struct xfs_ioend	*new;
 +	xfs_ioend_t		*ioend = *result;
 +
++<<<<<<< HEAD
 +	if (!ioend || need_ioend || type != ioend->io_type) {
 +		xfs_ioend_t	*previous = *result;
  
 +		ioend = xfs_alloc_ioend(inode, type);
 +		ioend->io_offset = offset;
 +		ioend->io_buffer_head = bh;
 +		ioend->io_buffer_tail = bh;
 +		if (previous)
 +			previous->io_list = ioend;
 +		*result = ioend;
++=======
+ 		if (wpc->ioend)
+ 			list_add(&wpc->ioend->io_list, iolist);
+ 
+ 		new = xfs_alloc_ioend(inode, wpc->io_type);
+ 		new->io_offset = offset;
+ 		new->io_buffer_head = bh;
+ 		new->io_buffer_tail = bh;
+ 		wpc->ioend = new;
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  	} else {
 -		wpc->ioend->io_buffer_tail->b_private = bh;
 -		wpc->ioend->io_buffer_tail = bh;
 +		ioend->io_buffer_tail->b_private = bh;
 +		ioend->io_buffer_tail = bh;
  	}
  
  	bh->b_private = NULL;
++<<<<<<< HEAD
 +	ioend->io_size += bh->b_size;
++=======
+ 	wpc->ioend->io_size += bh->b_size;
+ 	wpc->last_block = bh->b_blocknr;
+ 	xfs_start_buffer_writeback(bh);
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  }
  
  STATIC void
@@@ -901,26 -701,162 +910,185 @@@ out_invalidate
  	return;
  }
  
++<<<<<<< HEAD
 +static int
 +xfs_writepage_submit(
 +	struct xfs_ioend	*ioend,
 +	struct xfs_ioend	*iohead,
 +	struct writeback_control *wbc,
 +	int			status)
 +{
 +	struct blk_plug		plug;
 +
 +	/* Reserve log space if we might write beyond the on-disk inode size. */
 +	if (!status && ioend && ioend->io_type != XFS_IO_UNWRITTEN &&
 +	    xfs_ioend_is_append(ioend))
 +		status = xfs_setfilesize_trans_alloc(ioend);
 +
 +	if (iohead) {
 +		blk_start_plug(&plug);
 +		xfs_submit_ioend(wbc, iohead, status);
 +		blk_finish_plug(&plug);
 +	}
 +	return status;
++=======
+ /*
+  * We implement an immediate ioend submission policy here to avoid needing to
+  * chain multiple ioends and hence nest mempool allocations which can violate
+  * forward progress guarantees we need to provide. The current ioend we are
+  * adding buffers to is cached on the writepage context, and if the new buffer
+  * does not append to the cached ioend it will create a new ioend and cache that
+  * instead.
+  *
+  * If a new ioend is created and cached, the old ioend is returned and queued
+  * locally for submission once the entire page is processed or an error has been
+  * detected.  While ioends are submitted immediately after they are completed,
+  * batching optimisations are provided by higher level block plugging.
+  *
+  * At the end of a writeback pass, there will be a cached ioend remaining on the
+  * writepage context that the caller will need to submit.
+  */
+ static int
+ xfs_writepage_map(
+ 	struct xfs_writepage_ctx *wpc,
+ 	struct writeback_control *wbc,
+ 	struct inode		*inode,
+ 	struct page		*page,
+ 	loff_t			offset,
+ 	__uint64_t              end_offset)
+ {
+ 	LIST_HEAD(submit_list);
+ 	struct xfs_ioend	*ioend, *next;
+ 	struct buffer_head	*bh, *head;
+ 	ssize_t			len = 1 << inode->i_blkbits;
+ 	int			error = 0;
+ 	int			count = 0;
+ 	int			uptodate = 1;
+ 
+ 	bh = head = page_buffers(page);
+ 	offset = page_offset(page);
+ 	do {
+ 		if (offset >= end_offset)
+ 			break;
+ 		if (!buffer_uptodate(bh))
+ 			uptodate = 0;
+ 
+ 		/*
+ 		 * set_page_dirty dirties all buffers in a page, independent
+ 		 * of their state.  The dirty state however is entirely
+ 		 * meaningless for holes (!mapped && uptodate), so skip
+ 		 * buffers covering holes here.
+ 		 */
+ 		if (!buffer_mapped(bh) && buffer_uptodate(bh)) {
+ 			wpc->imap_valid = false;
+ 			continue;
+ 		}
+ 
+ 		if (buffer_unwritten(bh)) {
+ 			if (wpc->io_type != XFS_IO_UNWRITTEN) {
+ 				wpc->io_type = XFS_IO_UNWRITTEN;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else if (buffer_delay(bh)) {
+ 			if (wpc->io_type != XFS_IO_DELALLOC) {
+ 				wpc->io_type = XFS_IO_DELALLOC;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else if (buffer_uptodate(bh)) {
+ 			if (wpc->io_type != XFS_IO_OVERWRITE) {
+ 				wpc->io_type = XFS_IO_OVERWRITE;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else {
+ 			if (PageUptodate(page))
+ 				ASSERT(buffer_mapped(bh));
+ 			/*
+ 			 * This buffer is not uptodate and will not be
+ 			 * written to disk.  Ensure that we will put any
+ 			 * subsequent writeable buffers into a new
+ 			 * ioend.
+ 			 */
+ 			wpc->imap_valid = false;
+ 			continue;
+ 		}
+ 
+ 		if (wpc->imap_valid)
+ 			wpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,
+ 							 offset);
+ 		if (!wpc->imap_valid) {
+ 			error = xfs_map_blocks(inode, offset, &wpc->imap,
+ 					     wpc->io_type);
+ 			if (error)
+ 				goto out;
+ 			wpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,
+ 							 offset);
+ 		}
+ 		if (wpc->imap_valid) {
+ 			lock_buffer(bh);
+ 			if (wpc->io_type != XFS_IO_OVERWRITE)
+ 				xfs_map_at_offset(inode, bh, &wpc->imap, offset);
+ 			xfs_add_to_ioend(inode, bh, offset, wpc, &submit_list);
+ 			count++;
+ 		}
+ 
+ 	} while (offset += len, ((bh = bh->b_this_page) != head));
+ 
+ 	if (uptodate && bh == head)
+ 		SetPageUptodate(page);
+ 
+ 	ASSERT(wpc->ioend || list_empty(&submit_list));
+ 
+ out:
+ 	/*
+ 	 * On error, we have to fail the ioend here because we have locked
+ 	 * buffers in the ioend. If we don't do this, we'll deadlock
+ 	 * invalidating the page as that tries to lock the buffers on the page.
+ 	 * Also, because we may have set pages under writeback, we have to make
+ 	 * sure we run IO completion to mark the error state of the IO
+ 	 * appropriately, so we can't cancel the ioend directly here. That means
+ 	 * we have to mark this page as under writeback if we included any
+ 	 * buffers from it in the ioend chain so that completion treats it
+ 	 * correctly.
+ 	 *
+ 	 * If we didn't include the page in the ioend, the on error we can
+ 	 * simply discard and unlock it as there are no other users of the page
+ 	 * or it's buffers right now. The caller will still need to trigger
+ 	 * submission of outstanding ioends on the writepage context so they are
+ 	 * treated correctly on error.
+ 	 */
+ 	if (count) {
+ 		xfs_start_page_writeback(page, !error);
+ 
+ 		/*
+ 		 * Preserve the original error if there was one, otherwise catch
+ 		 * submission errors here and propagate into subsequent ioend
+ 		 * submissions.
+ 		 */
+ 		list_for_each_entry_safe(ioend, next, &submit_list, io_list) {
+ 			int error2;
+ 
+ 			list_del_init(&ioend->io_list);
+ 			error2 = xfs_submit_ioend(wbc, ioend, error);
+ 			if (error2 && !error)
+ 				error = error2;
+ 		}
+ 	} else if (error) {
+ 		xfs_aops_discard_page(page);
+ 		ClearPageUptodate(page);
+ 		unlock_page(page);
+ 	} else {
+ 		/*
+ 		 * We can end up here with no error and nothing to write if we
+ 		 * race with a partial page truncate on a sub-page block sized
+ 		 * filesystem. In that case we need to mark the page clean.
+ 		 */
+ 		xfs_start_page_writeback(page, 1);
+ 		end_page_writeback(page);
+ 	}
+ 
+ 	mapping_set_error(page->mapping, error);
+ 	return error;
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  }
  
  /*
@@@ -1040,154 -970,7 +1208,158 @@@ xfs_vm_writepage
  		end_offset = offset;
  	}
  
++<<<<<<< HEAD
 +	len = 1 << inode->i_blkbits;
 +
 +	bh = head = page_buffers(page);
 +	offset = page_offset(page);
 +	type = XFS_IO_OVERWRITE;
 +
 +	do {
 +		int new_ioend = 0;
 +
 +		if (offset >= end_offset)
 +			break;
 +		if (!buffer_uptodate(bh))
 +			uptodate = 0;
 +
 +		/*
 +		 * set_page_dirty dirties all buffers in a page, independent
 +		 * of their state.  The dirty state however is entirely
 +		 * meaningless for holes (!mapped && uptodate), so skip
 +		 * buffers covering holes here.
 +		 */
 +		if (!buffer_mapped(bh) && buffer_uptodate(bh)) {
 +			imap_valid = 0;
 +			continue;
 +		}
 +
 +		if (buffer_unwritten(bh)) {
 +			if (type != XFS_IO_UNWRITTEN) {
 +				type = XFS_IO_UNWRITTEN;
 +				imap_valid = 0;
 +			}
 +		} else if (buffer_delay(bh)) {
 +			if (type != XFS_IO_DELALLOC) {
 +				type = XFS_IO_DELALLOC;
 +				imap_valid = 0;
 +			}
 +		} else if (buffer_uptodate(bh)) {
 +			if (type != XFS_IO_OVERWRITE) {
 +				type = XFS_IO_OVERWRITE;
 +				imap_valid = 0;
 +			}
 +		} else {
 +			if (PageUptodate(page))
 +				ASSERT(buffer_mapped(bh));
 +			/*
 +			 * This buffer is not uptodate and will not be
 +			 * written to disk.  Ensure that we will put any
 +			 * subsequent writeable buffers into a new
 +			 * ioend.
 +			 */
 +			imap_valid = 0;
 +			continue;
 +		}
 +
 +		if (imap_valid)
 +			imap_valid = xfs_imap_valid(inode, &imap, offset);
 +		if (!imap_valid) {
 +			/*
 +			 * If we didn't have a valid mapping then we need to
 +			 * put the new mapping into a separate ioend structure.
 +			 * This ensures non-contiguous extents always have
 +			 * separate ioends, which is particularly important
 +			 * for unwritten extent conversion at I/O completion
 +			 * time.
 +			 */
 +			new_ioend = 1;
 +			err = xfs_map_blocks(inode, offset, &imap, type);
 +			if (err)
 +				goto error;
 +			imap_valid = xfs_imap_valid(inode, &imap, offset);
 +		}
 +		if (imap_valid) {
 +			lock_buffer(bh);
 +			if (type != XFS_IO_OVERWRITE)
 +				xfs_map_at_offset(inode, bh, &imap, offset);
 +			xfs_add_to_ioend(inode, bh, offset, type, &ioend,
 +					 new_ioend);
 +			count++;
 +		}
 +
 +		if (!iohead)
 +			iohead = ioend;
 +
 +	} while (offset += len, ((bh = bh->b_this_page) != head));
 +
 +	if (uptodate && bh == head)
 +		SetPageUptodate(page);
 +
 +	xfs_start_page_writeback(page, 1, count);
 +
 +	/* if there is no IO to be submitted for this page, we are done */
 +	if (!ioend)
 +		return 0;
 +
 +	ASSERT(iohead);
 +	ASSERT(err == 0);
 +
 +	/*
 +	 * Any errors from this point onwards need tobe reported through the IO
 +	 * completion path as we have marked the initial page as under writeback
 +	 * and unlocked it.
 +	 */
 +	if (imap_valid) {
 +		xfs_off_t		end_index;
 +
 +		end_index = imap.br_startoff + imap.br_blockcount;
 +
 +		/* to bytes */
 +		end_index <<= inode->i_blkbits;
 +
 +		/* to pages */
 +		end_index = (end_index - 1) >> PAGE_CACHE_SHIFT;
 +
 +		/* check against file size */
 +		if (end_index > last_index)
 +			end_index = last_index;
 +
 +		xfs_cluster_write(inode, page->index + 1, &imap, &ioend,
 +				  wbc, end_index);
 +	}
 +
 +	return xfs_writepage_submit(ioend, iohead, wbc, 0);
 +
 +error:
 +	/*
 +	 * On error, we have to fail the iohead here because we buffers locked
 +	 * in the ioend chain. If we don't do this, we'll deadlock invalidating
 +	 * the page as that tries to lock the buffers on the page. Also, because
 +	 * we may have set pages under writeback, we have to run IO completion to
 +	 * mark the error state of the IO appropriately, so we can't cancel the
 +	 * ioend directly here. That means we have to mark this page as under
 +	 * writeback if we included any buffers from it in the ioend chain.
 +	 */
 +	if (count)
 +		xfs_start_page_writeback(page, 0, count);
 +	xfs_writepage_submit(ioend, iohead, wbc, err);
 +
 +	/*
 +	 * We can only discard the page we had the IO error on if we haven't
 +	 * included it in the ioend above. If it has already been errored out,
 +	 * the it is unlocked and we can't touch it here.
 +	 */
 +	if (!count) {
 +		xfs_aops_discard_page(page);
 +		ClearPageUptodate(page);
 +		unlock_page(page);
 +	}
 +	mapping_set_error(page->mapping, err);
 +	return err;
++=======
+ 	return xfs_writepage_map(wpc, wbc, inode, page, offset, end_offset);
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  
  redirty:
  	redirty_page_for_writepage(wbc, page);
@@@ -1196,16 -979,36 +1368,42 @@@
  }
  
  STATIC int
++<<<<<<< HEAD
++=======
+ xfs_vm_writepage(
+ 	struct page		*page,
+ 	struct writeback_control *wbc)
+ {
+ 	struct xfs_writepage_ctx wpc = {
+ 		.io_type = XFS_IO_INVALID,
+ 	};
+ 	int			ret;
+ 
+ 	ret = xfs_do_writepage(page, wbc, &wpc);
+ 	if (wpc.ioend)
+ 		ret = xfs_submit_ioend(wbc, wpc.ioend, ret);
+ 	return ret;
+ }
+ 
+ STATIC int
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  xfs_vm_writepages(
  	struct address_space	*mapping,
  	struct writeback_control *wbc)
  {
 -	struct xfs_writepage_ctx wpc = {
 -		.io_type = XFS_IO_INVALID,
 -	};
 -	int			ret;
 -
  	xfs_iflags_clear(XFS_I(mapping->host), XFS_ITRUNCATED);
++<<<<<<< HEAD
 +	if (dax_mapping(mapping))
 +		return dax_writeback_mapping_range(mapping,
 +				xfs_find_bdev_for_inode(mapping->host), wbc);
 +
 +	return generic_writepages(mapping, wbc);
++=======
+ 	ret = write_cache_pages(mapping, wbc, xfs_do_writepage, &wpc);
+ 	if (wpc.ioend)
+ 		ret = xfs_submit_ioend(wbc, wpc.ioend, ret);
+ 	return ret;
++>>>>>>> e10de3723c53 (xfs: don't chain ioends during writepage submission)
  }
  
  /*
* Unmerged path fs/xfs/xfs_aops.c
diff --git a/fs/xfs/xfs_aops.h b/fs/xfs/xfs_aops.h
index a4343c63fb38..24fd72d8ad06 100644
--- a/fs/xfs/xfs_aops.h
+++ b/fs/xfs/xfs_aops.h
@@ -39,7 +39,7 @@ enum {
  * It can manage several multi-page bio's at once.
  */
 typedef struct xfs_ioend {
-	struct xfs_ioend	*io_list;	/* next ioend in chain */
+	struct list_head	io_list;	/* next ioend in chain */
 	unsigned int		io_type;	/* delalloc / unwritten */
 	int			io_error;	/* I/O error code */
 	atomic_t		io_remaining;	/* hold count */
