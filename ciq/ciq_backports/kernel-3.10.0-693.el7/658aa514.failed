sh: Provide atomic_{or,xor,and}

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] Provide atomic_{or,xor,and} (Rob Clark) [1406119]
Rebuild_FUZZ: 93.10%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 658aa51459c2f5284183d35b6dd0beca0e0bfe2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/658aa514.failed

Implement atomic logic ops -- atomic_{or,xor,and}.

These will replace the atomic_{set,clear}_mask functions that are
available on some archs.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 658aa51459c2f5284183d35b6dd0beca0e0bfe2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/sh/include/asm/atomic-grb.h
#	arch/sh/include/asm/atomic-irq.h
#	arch/sh/include/asm/atomic-llsc.h
diff --cc arch/sh/include/asm/atomic-grb.h
index a273c88578fc,4b03830d48c7..000000000000
--- a/arch/sh/include/asm/atomic-grb.h
+++ b/arch/sh/include/asm/atomic-grb.h
@@@ -1,123 -1,61 +1,96 @@@
  #ifndef __ASM_SH_ATOMIC_GRB_H
  #define __ASM_SH_ATOMIC_GRB_H
  
 -#define ATOMIC_OP(op)							\
 -static inline void atomic_##op(int i, atomic_t *v)			\
 -{									\
 -	int tmp;							\
 -									\
 -	__asm__ __volatile__ (						\
 -		"   .align 2              \n\t"				\
 -		"   mova    1f,   r0      \n\t" /* r0 = end point */	\
 -		"   mov    r15,   r1      \n\t" /* r1 = saved sp */	\
 -		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */	\
 -		"   mov.l  @%1,   %0      \n\t" /* load  old value */	\
 -		" " #op "   %2,   %0      \n\t" /* $op */		\
 -		"   mov.l   %0,   @%1     \n\t" /* store new value */	\
 -		"1: mov     r1,   r15     \n\t" /* LOGOUT */		\
 -		: "=&r" (tmp),						\
 -		  "+r"  (v)						\
 -		: "r"   (i)						\
 -		: "memory" , "r0", "r1");				\
 -}									\
 -
 -#define ATOMIC_OP_RETURN(op)						\
 -static inline int atomic_##op##_return(int i, atomic_t *v)		\
 -{									\
 -	int tmp;							\
 -									\
 -	__asm__ __volatile__ (						\
 -		"   .align 2              \n\t"				\
 -		"   mova    1f,   r0      \n\t" /* r0 = end point */	\
 -		"   mov    r15,   r1      \n\t" /* r1 = saved sp */	\
 -		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */	\
 -		"   mov.l  @%1,   %0      \n\t" /* load  old value */	\
 -		" " #op "   %2,   %0      \n\t" /* $op */		\
 -		"   mov.l   %0,   @%1     \n\t" /* store new value */	\
 -		"1: mov     r1,   r15     \n\t" /* LOGOUT */		\
 -		: "=&r" (tmp),						\
 -		  "+r"  (v)						\
 -		: "r"   (i)						\
 -		: "memory" , "r0", "r1");				\
 -									\
 -	return tmp;							\
 +static inline void atomic_add(int i, atomic_t *v)
 +{
 +	int tmp;
 +
 +	__asm__ __volatile__ (
 +		"   .align 2              \n\t"
 +		"   mova    1f,   r0      \n\t" /* r0 = end point */
 +		"   mov    r15,   r1      \n\t" /* r1 = saved sp */
 +		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
 +		"   mov.l  @%1,   %0      \n\t" /* load  old value */
 +		"   add     %2,   %0      \n\t" /* add */
 +		"   mov.l   %0,   @%1     \n\t" /* store new value */
 +		"1: mov     r1,   r15     \n\t" /* LOGOUT */
 +		: "=&r" (tmp),
 +		  "+r"  (v)
 +		: "r"   (i)
 +		: "memory" , "r0", "r1");
  }
  
 -#define ATOMIC_OPS(op) ATOMIC_OP(op) ATOMIC_OP_RETURN(op)
 +static inline void atomic_sub(int i, atomic_t *v)
 +{
 +	int tmp;
 +
 +	__asm__ __volatile__ (
 +		"   .align 2              \n\t"
 +		"   mova    1f,   r0      \n\t" /* r0 = end point */
 +		"   mov     r15,  r1      \n\t" /* r1 = saved sp */
 +		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
 +		"   mov.l  @%1,   %0      \n\t" /* load  old value */
 +		"   sub     %2,   %0      \n\t" /* sub */
 +		"   mov.l   %0,   @%1     \n\t" /* store new value */
 +		"1: mov     r1,   r15     \n\t" /* LOGOUT */
 +		: "=&r" (tmp),
 +		  "+r"  (v)
 +		: "r"   (i)
 +		: "memory" , "r0", "r1");
 +}
  
 -ATOMIC_OPS(add)
 -ATOMIC_OPS(sub)
++<<<<<<< HEAD
 +static inline int atomic_add_return(int i, atomic_t *v)
 +{
 +	int tmp;
  
 +	__asm__ __volatile__ (
 +		"   .align 2              \n\t"
 +		"   mova    1f,   r0      \n\t" /* r0 = end point */
 +		"   mov    r15,   r1      \n\t" /* r1 = saved sp */
 +		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
 +		"   mov.l  @%1,   %0      \n\t" /* load  old value */
 +		"   add     %2,   %0      \n\t" /* add */
 +		"   mov.l   %0,   @%1     \n\t" /* store new value */
 +		"1: mov     r1,   r15     \n\t" /* LOGOUT */
 +		: "=&r" (tmp),
 +		  "+r"  (v)
 +		: "r"   (i)
 +		: "memory" , "r0", "r1");
 +
 +	return tmp;
 +}
 +
 +static inline int atomic_sub_return(int i, atomic_t *v)
 +{
 +	int tmp;
 +
 +	__asm__ __volatile__ (
 +		"   .align 2              \n\t"
 +		"   mova    1f,   r0      \n\t" /* r0 = end point */
 +		"   mov    r15,   r1      \n\t" /* r1 = saved sp */
 +		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
 +		"   mov.l  @%1,   %0      \n\t" /* load  old value */
 +		"   sub     %2,   %0      \n\t" /* sub */
 +		"   mov.l   %0,   @%1     \n\t" /* store new value */
 +		"1: mov     r1,   r15     \n\t" /* LOGOUT */
 +		: "=&r" (tmp),
 +		  "+r"  (v)
 +		: "r"   (i)
 +		: "memory", "r0", "r1");
 +
 +	return tmp;
 +}
++=======
+ #define CONFIG_ARCH_HAS_ATOMIC_OR
  
- static inline void atomic_clear_mask(unsigned int mask, atomic_t *v)
- {
- 	int tmp;
- 	unsigned int _mask = ~mask;
- 
- 	__asm__ __volatile__ (
- 		"   .align 2              \n\t"
- 		"   mova    1f,   r0      \n\t" /* r0 = end point */
- 		"   mov    r15,   r1      \n\t" /* r1 = saved sp */
- 		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
- 		"   mov.l  @%1,   %0      \n\t" /* load  old value */
- 		"   and     %2,   %0      \n\t" /* add */
- 		"   mov.l   %0,   @%1     \n\t" /* store new value */
- 		"1: mov     r1,   r15     \n\t" /* LOGOUT */
- 		: "=&r" (tmp),
- 		  "+r"  (v)
- 		: "r"   (_mask)
- 		: "memory" , "r0", "r1");
- }
+ ATOMIC_OP(and)
+ ATOMIC_OP(or)
+ ATOMIC_OP(xor)
  
- static inline void atomic_set_mask(unsigned int mask, atomic_t *v)
- {
- 	int tmp;
- 
- 	__asm__ __volatile__ (
- 		"   .align 2              \n\t"
- 		"   mova    1f,   r0      \n\t" /* r0 = end point */
- 		"   mov    r15,   r1      \n\t" /* r1 = saved sp */
- 		"   mov    #-6,   r15     \n\t" /* LOGIN: r15 = size */
- 		"   mov.l  @%1,   %0      \n\t" /* load  old value */
- 		"   or      %2,   %0      \n\t" /* or */
- 		"   mov.l   %0,   @%1     \n\t" /* store new value */
- 		"1: mov     r1,   r15     \n\t" /* LOGOUT */
- 		: "=&r" (tmp),
- 		  "+r"  (v)
- 		: "r"   (mask)
- 		: "memory" , "r0", "r1");
- }
+ #undef ATOMIC_OPS
+ #undef ATOMIC_OP_RETURN
+ #undef ATOMIC_OP
++>>>>>>> 658aa51459c2 (sh: Provide atomic_{or,xor,and})
  
  #endif /* __ASM_SH_ATOMIC_GRB_H */
diff --cc arch/sh/include/asm/atomic-irq.h
index 9f7c56609e53,23fcdad5773e..000000000000
--- a/arch/sh/include/asm/atomic-irq.h
+++ b/arch/sh/include/asm/atomic-irq.h
@@@ -8,66 -8,41 +8,56 @@@
   * forward to code at the end of this object's .text section, then
   * branch back to restart the operation.
   */
 +static inline void atomic_add(int i, atomic_t *v)
 +{
 +	unsigned long flags;
  
 -#define ATOMIC_OP(op, c_op)						\
 -static inline void atomic_##op(int i, atomic_t *v)			\
 -{									\
 -	unsigned long flags;						\
 -									\
 -	raw_local_irq_save(flags);					\
 -	v->counter c_op i;						\
 -	raw_local_irq_restore(flags);					\
 +	raw_local_irq_save(flags);
 +	v->counter += i;
 +	raw_local_irq_restore(flags);
  }
  
 -#define ATOMIC_OP_RETURN(op, c_op)					\
 -static inline int atomic_##op##_return(int i, atomic_t *v)		\
 -{									\
 -	unsigned long temp, flags;					\
 -									\
 -	raw_local_irq_save(flags);					\
 -	temp = v->counter;						\
 -	temp c_op i;							\
 -	v->counter = temp;						\
 -	raw_local_irq_restore(flags);					\
 -									\
 -	return temp;							\
 -}
 +static inline void atomic_sub(int i, atomic_t *v)
 +{
 +	unsigned long flags;
  
 -#define ATOMIC_OPS(op, c_op) ATOMIC_OP(op, c_op) ATOMIC_OP_RETURN(op, c_op)
 +	raw_local_irq_save(flags);
 +	v->counter -= i;
 +	raw_local_irq_restore(flags);
 +}
  
 +static inline int atomic_add_return(int i, atomic_t *v)
 +{
 +	unsigned long temp, flags;
 +
++<<<<<<< HEAD
 +	raw_local_irq_save(flags);
 +	temp = v->counter;
 +	temp += i;
 +	v->counter = temp;
 +	raw_local_irq_restore(flags);
++=======
+ ATOMIC_OPS(add, +=)
+ ATOMIC_OPS(sub, -=)
+ ATOMIC_OP(and, &=)
+ ATOMIC_OP(or, |=)
+ ATOMIC_OP(xor, ^=)
++>>>>>>> 658aa51459c2 (sh: Provide atomic_{or,xor,and})
 +
 +	return temp;
 +}
  
 -#undef ATOMIC_OPS
 -#undef ATOMIC_OP_RETURN
 -#undef ATOMIC_OP
 +static inline int atomic_sub_return(int i, atomic_t *v)
 +{
 +	unsigned long temp, flags;
 +
 +	raw_local_irq_save(flags);
 +	temp = v->counter;
 +	temp -= i;
 +	v->counter = temp;
 +	raw_local_irq_restore(flags);
 +
 +	return temp;
 +}
  
- static inline void atomic_clear_mask(unsigned int mask, atomic_t *v)
- {
- 	unsigned long flags;
- 
- 	raw_local_irq_save(flags);
- 	v->counter &= ~mask;
- 	raw_local_irq_restore(flags);
- }
- 
- static inline void atomic_set_mask(unsigned int mask, atomic_t *v)
- {
- 	unsigned long flags;
- 
- 	raw_local_irq_save(flags);
- 	v->counter |= mask;
- 	raw_local_irq_restore(flags);
- }
- 
  #endif /* __ASM_SH_ATOMIC_IRQ_H */
diff --cc arch/sh/include/asm/atomic-llsc.h
index 4b00b78e3f4f,33d34b16d4d6..000000000000
--- a/arch/sh/include/asm/atomic-llsc.h
+++ b/arch/sh/include/asm/atomic-llsc.h
@@@ -42,66 -9,55 +42,80 @@@ static inline void atomic_sub(int i, at
   * encoding, so the retval is automatically set without having to
   * do any special work.
   */
 -/*
 - * To get proper branch prediction for the main line, we must branch
 - * forward to code at the end of this object's .text section, then
 - * branch back to restart the operation.
 - */
 +static inline int atomic_add_return(int i, atomic_t *v)
 +{
 +	unsigned long temp;
 +
 +	__asm__ __volatile__ (
 +"1:	movli.l @%2, %0		! atomic_add_return	\n"
 +"	add	%1, %0					\n"
 +"	movco.l	%0, @%2					\n"
 +"	bf	1b					\n"
 +"	synco						\n"
 +	: "=&z" (temp)
 +	: "r" (i), "r" (&v->counter)
 +	: "t");
 +
 +	return temp;
 +}
 +
 +static inline int atomic_sub_return(int i, atomic_t *v)
 +{
 +	unsigned long temp;
 +
 +	__asm__ __volatile__ (
 +"1:	movli.l @%2, %0		! atomic_sub_return	\n"
 +"	sub	%1, %0					\n"
 +"	movco.l	%0, @%2					\n"
 +"	bf	1b					\n"
 +"	synco						\n"
 +	: "=&z" (temp)
 +	: "r" (i), "r" (&v->counter)
 +	: "t");
  
 -#define ATOMIC_OP(op)							\
 -static inline void atomic_##op(int i, atomic_t *v)			\
 -{									\
 -	unsigned long tmp;						\
 -									\
 -	__asm__ __volatile__ (						\
 -"1:	movli.l @%2, %0		! atomic_" #op "\n"			\
 -"	" #op "	%1, %0				\n"			\
 -"	movco.l	%0, @%2				\n"			\
 -"	bf	1b				\n"			\
 -	: "=&z" (tmp)							\
 -	: "r" (i), "r" (&v->counter)					\
 -	: "t");								\
 +	return temp;
  }
  
 -#define ATOMIC_OP_RETURN(op)						\
 -static inline int atomic_##op##_return(int i, atomic_t *v)		\
 -{									\
 -	unsigned long temp;						\
 -									\
 -	__asm__ __volatile__ (						\
 -"1:	movli.l @%2, %0		! atomic_" #op "_return	\n"		\
 -"	" #op "	%1, %0					\n"		\
 -"	movco.l	%0, @%2					\n"		\
 -"	bf	1b					\n"		\
 -"	synco						\n"		\
 -	: "=&z" (temp)							\
 -	: "r" (i), "r" (&v->counter)					\
 -	: "t");								\
 -									\
 -	return temp;							\
++<<<<<<< HEAD
 +static inline void atomic_clear_mask(unsigned int mask, atomic_t *v)
 +{
 +	unsigned long tmp;
 +
 +	__asm__ __volatile__ (
 +"1:	movli.l @%2, %0		! atomic_clear_mask	\n"
 +"	and	%1, %0					\n"
 +"	movco.l	%0, @%2					\n"
 +"	bf	1b					\n"
 +	: "=&z" (tmp)
 +	: "r" (~mask), "r" (&v->counter)
 +	: "t");
  }
  
 +static inline void atomic_set_mask(unsigned int mask, atomic_t *v)
 +{
 +	unsigned long tmp;
 +
 +	__asm__ __volatile__ (
 +"1:	movli.l @%2, %0		! atomic_set_mask	\n"
 +"	or	%1, %0					\n"
 +"	movco.l	%0, @%2					\n"
 +"	bf	1b					\n"
 +	: "=&z" (tmp)
 +	: "r" (mask), "r" (&v->counter)
 +	: "t");
 +}
++=======
+ #define ATOMIC_OPS(op) ATOMIC_OP(op) ATOMIC_OP_RETURN(op)
+ 
+ ATOMIC_OPS(add)
+ ATOMIC_OPS(sub)
+ ATOMIC_OP(and)
+ ATOMIC_OP(or)
+ ATOMIC_OP(xor)
+ 
+ #undef ATOMIC_OPS
+ #undef ATOMIC_OP_RETURN
+ #undef ATOMIC_OP
++>>>>>>> 658aa51459c2 (sh: Provide atomic_{or,xor,and})
  
  #endif /* __ASM_SH_ATOMIC_LLSC_H */
* Unmerged path arch/sh/include/asm/atomic-grb.h
* Unmerged path arch/sh/include/asm/atomic-irq.h
* Unmerged path arch/sh/include/asm/atomic-llsc.h
diff --git a/arch/sh/include/asm/atomic.h b/arch/sh/include/asm/atomic.h
index f4c1c20bcdf6..2c8cc911cc56 100644
--- a/arch/sh/include/asm/atomic.h
+++ b/arch/sh/include/asm/atomic.h
@@ -24,6 +24,16 @@
 #include <asm/atomic-irq.h>
 #endif
 
+static inline __deprecated void atomic_clear_mask(unsigned int mask, atomic_t *v)
+{
+	atomic_and(~mask, v);
+}
+
+static inline __deprecated void atomic_set_mask(unsigned int mask, atomic_t *v)
+{
+	atomic_or(mask, v);
+}
+
 #define atomic_add_negative(a, v)	(atomic_add_return((a), (v)) < 0)
 #define atomic_dec_return(v)		atomic_sub_return(1, (v))
 #define atomic_inc_return(v)		atomic_add_return(1, (v))
