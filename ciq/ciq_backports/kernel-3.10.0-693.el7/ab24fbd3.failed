s390/dasd: suppress command reject error for query host access command

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] dasd: suppress command reject error for query host access command (Hendrik Brueckner) [1440709]
Rebuild_FUZZ: 96.30%
commit-author Stefan Haberland <sth@linux.vnet.ibm.com>
commit ab24fbd35a6ee77a58c24bd50582c51610a194f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ab24fbd3.failed

On some z/VM systems the query host access command is not supported for
temp disks, though the corresponding feature code is set.
This does not have any impact beside that the information is not available.
Suppress the full blown command reject error messages to not confuse the
user. The error is still logged in the s390dbf.

	Signed-off-by: Stefan Haberland <sth@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit ab24fbd35a6ee77a58c24bd50582c51610a194f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/block/dasd_eckd.c
#	drivers/s390/block/dasd_int.h
diff --cc drivers/s390/block/dasd_eckd.c
index 75d5d21ad443,122456e4db89..000000000000
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@@ -4343,10 -4909,38 +4343,27 @@@ static void dasd_eckd_dump_sense_tcw(st
  static void dasd_eckd_dump_sense(struct dasd_device *device,
  				 struct dasd_ccw_req *req, struct irb *irb)
  {
 -	u8 *sense = dasd_get_sense(irb);
 -
 -	if (scsw_is_tm(&irb->scsw)) {
 -		/*
 -		 * In some cases the 'File Protected' or 'Incorrect Length'
 -		 * error might be expected and log messages shouldn't be written
 -		 * then. Check if the according suppress bit is set.
 -		 */
 -		if (sense && (sense[1] & SNS1_FILE_PROTECTED) &&
 -		    test_bit(DASD_CQR_SUPPRESS_FP, &req->flags))
 -			return;
 -		if (scsw_cstat(&irb->scsw) == 0x40 &&
 -		    test_bit(DASD_CQR_SUPPRESS_IL, &req->flags))
 -			return;
 -
 +	if (scsw_is_tm(&irb->scsw))
  		dasd_eckd_dump_sense_tcw(device, req, irb);
++<<<<<<< HEAD
 +	else
++=======
+ 	} else {
+ 		/*
+ 		 * In some cases the 'Command Reject' or 'No Record Found'
+ 		 * error might be expected and log messages shouldn't be
+ 		 * written then. Check if the according suppress bit is set.
+ 		 */
+ 		if (sense && sense[0] & SNS0_CMD_REJECT &&
+ 		    test_bit(DASD_CQR_SUPPRESS_CR, &req->flags))
+ 			return;
+ 
+ 		if (sense && sense[1] & SNS1_NO_REC_FOUND &&
+ 		    test_bit(DASD_CQR_SUPPRESS_NRF, &req->flags))
+ 			return;
+ 
++>>>>>>> ab24fbd35a6e (s390/dasd: suppress command reject error for query host access command)
  		dasd_eckd_dump_sense_ccw(device, req, irb);
 -	}
  }
  
  static int dasd_eckd_pm_freeze(struct dasd_device *device)
@@@ -4484,6 -5080,701 +4501,704 @@@ out_err
  	return -1;
  }
  
++<<<<<<< HEAD
++=======
+ static int dasd_eckd_read_message_buffer(struct dasd_device *device,
+ 					 struct dasd_rssd_messages *messages,
+ 					 __u8 lpum)
+ {
+ 	struct dasd_rssd_messages *message_buf;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   (sizeof(struct dasd_psf_prssd_data) +
+ 				    sizeof(struct dasd_rssd_messages)),
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	cqr->lpm = lpum;
+ retry:
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->expires = 10 * HZ;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 	/* dasd_sleep_on_immediatly does not do complex error
+ 	 * recovery so clear erp flag and set retry counter to
+ 	 * do basic erp */
+ 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
+ 	cqr->retries = 256;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = 0x03;	/* Message Buffer */
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - message buffer */
+ 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
+ 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
+ 
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_rssd_messages);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) message_buf;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_immediatly(cqr);
+ 	if (rc == 0) {
+ 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 		message_buf = (struct dasd_rssd_messages *)
+ 			(prssdp + 1);
+ 		memcpy(messages, message_buf,
+ 		       sizeof(struct dasd_rssd_messages));
+ 	} else if (cqr->lpm) {
+ 		/*
+ 		 * on z/VM we might not be able to do I/O on the requested path
+ 		 * but instead we get the required information on any path
+ 		 * so retry with open path mask
+ 		 */
+ 		cqr->lpm = 0;
+ 		goto retry;
+ 	} else
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading messages failed with rc=%d\n"
+ 				, rc);
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ static int dasd_eckd_query_host_access(struct dasd_device *device,
+ 				       struct dasd_psf_query_host_access *data)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_psf_query_host_access *host_access;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	/* not available for HYPER PAV alias devices */
+ 	if (!device->block && private->lcu->pav == HYPER_PAV)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* may not be supported by the storage server */
+ 	if (!(private->features.feature[14] & 0x80))
+ 		return -EOPNOTSUPP;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   sizeof(struct dasd_psf_prssd_data) + 1,
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 	host_access = kzalloc(sizeof(*host_access), GFP_KERNEL | GFP_DMA);
+ 	if (!host_access) {
+ 		dasd_sfree_request(cqr, device);
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate host_access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10 * HZ;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = PSF_SUBORDER_QHA;	/* query host access */
+ 	/* LSS and Volume that will be queried */
+ 	prssdp->lss = private->ned->ID;
+ 	prssdp->volume = private->ned->unit_addr;
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - query host access */
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_psf_query_host_access);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) host_access;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	/* the command might not be supported, suppress error message */
+ 	__set_bit(DASD_CQR_SUPPRESS_CR, &cqr->flags);
+ 	rc = dasd_sleep_on_interruptible(cqr);
+ 	if (rc == 0) {
+ 		*data = *host_access;
+ 	} else {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading host access data failed with rc=%d\n",
+ 				rc);
+ 		rc = -EOPNOTSUPP;
+ 	}
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	kfree(host_access);
+ 	return rc;
+ }
+ /*
+  * return number of grouped devices
+  */
+ static int dasd_eckd_host_access_count(struct dasd_device *device)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	int count = 0;
+ 	int rc, i;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		if (entry->status_flags & DASD_ECKD_PG_GROUPED)
+ 			count++;
+ 	}
+ 
+ 	kfree(access);
+ 	return count;
+ }
+ 
+ /*
+  * write host access information to a sequential file
+  */
+ static int dasd_hosts_print(struct dasd_device *device, struct seq_file *m)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	char sysplex[9] = "";
+ 	int rc, i, j;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		/* PGID */
+ 		seq_puts(m, "pgid ");
+ 		for (j = 0; j < 11; j++)
+ 			seq_printf(m, "%02x", entry->pgid[j]);
+ 		seq_putc(m, '\n');
+ 		/* FLAGS */
+ 		seq_printf(m, "status_flags %02x\n", entry->status_flags);
+ 		/* SYSPLEX NAME */
+ 		memcpy(&sysplex, &entry->sysplex_name, sizeof(sysplex) - 1);
+ 		EBCASC(sysplex, sizeof(sysplex));
+ 		seq_printf(m, "sysplex_name %8s\n", sysplex);
+ 		/* SUPPORTED CYLINDER */
+ 		seq_printf(m, "supported_cylinder %d\n", entry->cylinder);
+ 		/* TIMESTAMP */
+ 		seq_printf(m, "timestamp %lu\n", (unsigned long)
+ 			   entry->timestamp);
+ 	}
+ 	kfree(access);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Perform Subsystem Function - CUIR response
+  */
+ static int
+ dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
+ 			    __u32 message_id, __u8 lpum)
+ {
+ 	struct dasd_psf_cuir_response *psf_cuir;
+ 	int pos = pathmask_to_pos(lpum);
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */ ,
+ 				  sizeof(struct dasd_psf_cuir_response),
+ 				  device);
+ 
+ 	if (IS_ERR(cqr)) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			   "Could not allocate PSF-CUIR request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
+ 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
+ 	psf_cuir->cc = response;
+ 	psf_cuir->chpid = device->path[pos].chpid;
+ 	psf_cuir->message_id = message_id;
+ 	psf_cuir->cssid = device->path[pos].cssid;
+ 	psf_cuir->ssid = device->path[pos].ssid;
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->cda = (__u32)(addr_t)psf_cuir;
+ 	ccw->flags = CCW_FLAG_SLI;
+ 	ccw->count = sizeof(struct dasd_psf_cuir_response);
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10*HZ;
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 
+ 	rc = dasd_sleep_on(cqr);
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * return configuration data that is referenced by record selector
+  * if a record selector is specified or per default return the
+  * conf_data pointer for the path specified by lpum
+  */
+ static struct dasd_conf_data *dasd_eckd_get_ref_conf(struct dasd_device *device,
+ 						     __u8 lpum,
+ 						     struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *conf_data;
+ 	int path, pos;
+ 
+ 	if (cuir->record_selector == 0)
+ 		goto out;
+ 	for (path = 0x80, pos = 0; path; path >>= 1, pos++) {
+ 		conf_data = device->path[pos].conf_data;
+ 		if (conf_data->gneq.record_selector ==
+ 		    cuir->record_selector)
+ 			return conf_data;
+ 	}
+ out:
+ 	return device->path[pathmask_to_pos(lpum)].conf_data;
+ }
+ 
+ /*
+  * This function determines the scope of a reconfiguration request by
+  * analysing the path and device selection data provided in the CUIR request.
+  * Returns a path mask containing CUIR affected paths for the give device.
+  *
+  * If the CUIR request does not contain the required information return the
+  * path mask of the path the attention message for the CUIR request was reveived
+  * on.
+  */
+ static int dasd_eckd_cuir_scope(struct dasd_device *device, __u8 lpum,
+ 				struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *ref_conf_data;
+ 	unsigned long bitmask = 0, mask = 0;
+ 	struct dasd_conf_data *conf_data;
+ 	unsigned int pos, path;
+ 	char *ref_gneq, *gneq;
+ 	char *ref_ned, *ned;
+ 	int tbcpm = 0;
+ 
+ 	/* if CUIR request does not specify the scope use the path
+ 	   the attention message was presented on */
+ 	if (!cuir->ned_map ||
+ 	    !(cuir->neq_map[0] | cuir->neq_map[1] | cuir->neq_map[2]))
+ 		return lpum;
+ 
+ 	/* get reference conf data */
+ 	ref_conf_data = dasd_eckd_get_ref_conf(device, lpum, cuir);
+ 	/* reference ned is determined by ned_map field */
+ 	pos = 8 - ffs(cuir->ned_map);
+ 	ref_ned = (char *)&ref_conf_data->neds[pos];
+ 	ref_gneq = (char *)&ref_conf_data->gneq;
+ 	/* transfer 24 bit neq_map to mask */
+ 	mask = cuir->neq_map[2];
+ 	mask |= cuir->neq_map[1] << 8;
+ 	mask |= cuir->neq_map[0] << 16;
+ 
+ 	for (path = 0; path < 8; path++) {
+ 		/* initialise data per path */
+ 		bitmask = mask;
+ 		conf_data = device->path[path].conf_data;
+ 		pos = 8 - ffs(cuir->ned_map);
+ 		ned = (char *) &conf_data->neds[pos];
+ 		/* compare reference ned and per path ned */
+ 		if (memcmp(ref_ned, ned, sizeof(*ned)) != 0)
+ 			continue;
+ 		gneq = (char *)&conf_data->gneq;
+ 		/* compare reference gneq and per_path gneq under
+ 		   24 bit mask where mask bit 0 equals byte 7 of
+ 		   the gneq and mask bit 24 equals byte 31 */
+ 		while (bitmask) {
+ 			pos = ffs(bitmask) - 1;
+ 			if (memcmp(&ref_gneq[31 - pos], &gneq[31 - pos], 1)
+ 			    != 0)
+ 				break;
+ 			clear_bit(pos, &bitmask);
+ 		}
+ 		if (bitmask)
+ 			continue;
+ 		/* device and path match the reference values
+ 		   add path to CUIR scope */
+ 		tbcpm |= 0x80 >> path;
+ 	}
+ 	return tbcpm;
+ }
+ 
+ static void dasd_eckd_cuir_notify_user(struct dasd_device *device,
+ 				       unsigned long paths, int action)
+ {
+ 	int pos;
+ 
+ 	while (paths) {
+ 		/* get position of bit in mask */
+ 		pos = 8 - ffs(paths);
+ 		/* get channel path descriptor from this position */
+ 		if (action == CUIR_QUIESCE)
+ 			pr_warn("Service on the storage server caused path %x.%02x to go offline",
+ 				device->path[pos].cssid,
+ 				device->path[pos].chpid);
+ 		else if (action == CUIR_RESUME)
+ 			pr_info("Path %x.%02x is back online after service on the storage server",
+ 				device->path[pos].cssid,
+ 				device->path[pos].chpid);
+ 		clear_bit(7 - pos, &paths);
+ 	}
+ }
+ 
+ static int dasd_eckd_cuir_remove_path(struct dasd_device *device, __u8 lpum,
+ 				      struct dasd_cuir_message *cuir)
+ {
+ 	unsigned long tbcpm;
+ 
+ 	tbcpm = dasd_eckd_cuir_scope(device, lpum, cuir);
+ 	/* nothing to do if path is not in use */
+ 	if (!(dasd_path_get_opm(device) & tbcpm))
+ 		return 0;
+ 	if (!(dasd_path_get_opm(device) & ~tbcpm)) {
+ 		/* no path would be left if the CUIR action is taken
+ 		   return error */
+ 		return -EINVAL;
+ 	}
+ 	/* remove device from operational path mask */
+ 	dasd_path_remove_opm(device, tbcpm);
+ 	dasd_path_add_cuirpm(device, tbcpm);
+ 	return tbcpm;
+ }
+ 
+ /*
+  * walk through all devices and build a path mask to quiesce them
+  * return an error if the last path to a device would be removed
+  *
+  * if only part of the devices are quiesced and an error
+  * occurs no onlining necessary, the storage server will
+  * notify the already set offline devices again
+  */
+ static int dasd_eckd_cuir_quiesce(struct dasd_device *device, __u8 lpum,
+ 				  struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	unsigned long flags;
+ 	int tbcpm;
+ 
+ 	/* active devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->active_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* inactive devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist, group) {
+ 		list_for_each_entry_safe(dev, n, &pavgroup->baselist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 		list_for_each_entry_safe(dev, n, &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, CUIR_QUIESCE);
+ 	return 0;
+ out_err:
+ 	return tbcpm;
+ }
+ 
+ static int dasd_eckd_cuir_resume(struct dasd_device *device, __u8 lpum,
+ 				 struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	int tbcpm;
+ 
+ 	/*
+ 	 * the path may have been added through a generic path event before
+ 	 * only trigger path verification if the path is not already in use
+ 	 */
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->active_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist,
+ 				 group) {
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->baselist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, CUIR_RESUME);
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_handle_cuir(struct dasd_device *device, void *messages,
+ 				 __u8 lpum)
+ {
+ 	struct dasd_cuir_message *cuir = messages;
+ 	int response;
+ 
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR request: %016llx %016llx %016llx %08x",
+ 		      ((u64 *)cuir)[0], ((u64 *)cuir)[1], ((u64 *)cuir)[2],
+ 		      ((u32 *)cuir)[3]);
+ 
+ 	if (cuir->code == CUIR_QUIESCE) {
+ 		/* quiesce */
+ 		if (dasd_eckd_cuir_quiesce(device, lpum, cuir))
+ 			response = PSF_CUIR_LAST_PATH;
+ 		else
+ 			response = PSF_CUIR_COMPLETED;
+ 	} else if (cuir->code == CUIR_RESUME) {
+ 		/* resume */
+ 		dasd_eckd_cuir_resume(device, lpum, cuir);
+ 		response = PSF_CUIR_COMPLETED;
+ 	} else
+ 		response = PSF_CUIR_NOT_SUPPORTED;
+ 
+ 	dasd_eckd_psf_cuir_response(device, response,
+ 				    cuir->message_id, lpum);
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR response: %d on message ID %08x", response,
+ 		      cuir->message_id);
+ 	/* to make sure there is no attention left schedule work again */
+ 	device->discipline->check_attention(device, lpum);
+ }
+ 
+ static void dasd_eckd_check_attention_work(struct work_struct *work)
+ {
+ 	struct check_attention_work_data *data;
+ 	struct dasd_rssd_messages *messages;
+ 	struct dasd_device *device;
+ 	int rc;
+ 
+ 	data = container_of(work, struct check_attention_work_data, worker);
+ 	device = data->device;
+ 	messages = kzalloc(sizeof(*messages), GFP_KERNEL);
+ 	if (!messages) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Could not allocate attention message buffer");
+ 		goto out;
+ 	}
+ 	rc = dasd_eckd_read_message_buffer(device, messages, data->lpum);
+ 	if (rc)
+ 		goto out;
+ 	if (messages->length == ATTENTION_LENGTH_CUIR &&
+ 	    messages->format == ATTENTION_FORMAT_CUIR)
+ 		dasd_eckd_handle_cuir(device, messages, data->lpum);
+ out:
+ 	dasd_put_device(device);
+ 	kfree(messages);
+ 	kfree(data);
+ }
+ 
+ static int dasd_eckd_check_attention(struct dasd_device *device, __u8 lpum)
+ {
+ 	struct check_attention_work_data *data;
+ 
+ 	data = kzalloc(sizeof(*data), GFP_ATOMIC);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	INIT_WORK(&data->worker, dasd_eckd_check_attention_work);
+ 	dasd_get_device(device);
+ 	data->device = device;
+ 	data->lpum = lpum;
+ 	schedule_work(&data->worker);
+ 	return 0;
+ }
+ 
+ static int dasd_eckd_disable_hpf_path(struct dasd_device *device, __u8 lpum)
+ {
+ 	if (~lpum & dasd_path_get_opm(device)) {
+ 		dasd_path_add_nohpfpm(device, lpum);
+ 		dasd_path_remove_opm(device, lpum);
+ 		dev_err(&device->cdev->dev,
+ 			"Channel path %02X lost HPF functionality and is disabled\n",
+ 			lpum);
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_disable_hpf_device(struct dasd_device *device)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	dev_err(&device->cdev->dev,
+ 		"High Performance FICON disabled\n");
+ 	private->fcx_max_data = 0;
+ }
+ 
+ static int dasd_eckd_hpf_enabled(struct dasd_device *device)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	return private->fcx_max_data ? 1 : 0;
+ }
+ 
+ static void dasd_eckd_handle_hpf_error(struct dasd_device *device,
+ 				       struct irb *irb)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	if (!private->fcx_max_data) {
+ 		/* sanity check for no HPF, the error makes no sense */
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Trying to disable HPF for a non HPF device");
+ 		return;
+ 	}
+ 	if (irb->scsw.tm.sesq == SCSW_SESQ_DEV_NOFCX) {
+ 		dasd_eckd_disable_hpf_device(device);
+ 	} else if (irb->scsw.tm.sesq == SCSW_SESQ_PATH_NOFCX) {
+ 		if (dasd_eckd_disable_hpf_path(device, irb->esw.esw1.lpum))
+ 			return;
+ 		dasd_eckd_disable_hpf_device(device);
+ 		dasd_path_set_tbvpm(device,
+ 				  dasd_path_get_hpfpm(device));
+ 	}
+ 	/*
+ 	 * prevent that any new I/O ist started on the device and schedule a
+ 	 * requeue of existing requests
+ 	 */
+ 	dasd_device_set_stop_bits(device, DASD_STOPPED_NOT_ACC);
+ 	dasd_schedule_requeue(device);
+ }
+ 
++>>>>>>> ab24fbd35a6e (s390/dasd: suppress command reject error for query host access command)
  static struct ccw_driver dasd_eckd_driver = {
  	.driver = {
  		.name	= "dasd-eckd",
diff --cc drivers/s390/block/dasd_int.h
index aa498f7fe95b,dca7cb1e6f65..000000000000
--- a/drivers/s390/block/dasd_int.h
+++ b/drivers/s390/block/dasd_int.h
@@@ -234,6 -237,13 +234,16 @@@ struct dasd_ccw_req 
  					 * stolen. Should not be combined with
  					 * DASD_CQR_FLAGS_USE_ERP
  					 */
++<<<<<<< HEAD
++=======
+ /*
+  * The following flags are used to suppress output of certain errors.
+  */
+ #define DASD_CQR_SUPPRESS_NRF	4	/* Suppress 'No Record Found' error */
+ #define DASD_CQR_SUPPRESS_FP	5	/* Suppress 'File Protected' error*/
+ #define DASD_CQR_SUPPRESS_IL	6	/* Suppress 'Incorrect Length' error */
+ #define DASD_CQR_SUPPRESS_CR	7	/* Suppress 'Command Reject' error */
++>>>>>>> ab24fbd35a6e (s390/dasd: suppress command reject error for query host access command)
  
  /* Signature for error recovery functions. */
  typedef struct dasd_ccw_req *(*dasd_erp_fn_t) (struct dasd_ccw_req *);
diff --git a/drivers/s390/block/dasd_3990_erp.c b/drivers/s390/block/dasd_3990_erp.c
index d26134713682..5410bc7fd559 100644
--- a/drivers/s390/block/dasd_3990_erp.c
+++ b/drivers/s390/block/dasd_3990_erp.c
@@ -1052,8 +1052,9 @@ dasd_3990_erp_com_rej(struct dasd_ccw_req * erp, char *sense)
 	} else {
 		/* fatal error -  set status to FAILED
 		   internal error 09 - Command Reject */
-		dev_err(&device->cdev->dev, "An error occurred in the DASD "
-			"device driver, reason=%s\n", "09");
+		if (!test_bit(DASD_CQR_SUPPRESS_CR, &erp->flags))
+			dev_err(&device->cdev->dev,
+				"An error occurred in the DASD device driver, reason=09\n");
 
 		erp = dasd_3990_erp_cleanup(erp, DASD_CQR_FAILED);
 	}
* Unmerged path drivers/s390/block/dasd_eckd.c
* Unmerged path drivers/s390/block/dasd_int.h
