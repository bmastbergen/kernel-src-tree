locking: Remove ACCESS_ONCE() usage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Davidlohr Bueso <dave@stgolabs.net>
commit 4d3199e4ca8e6670b54dc5ee070ffd54385988e9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/4d3199e4.failed

With the new standardized functions, we can replace all
ACCESS_ONCE() calls across relevant locking - this includes
lockref and seqlock while at it.

ACCESS_ONCE() does not work reliably on non-scalar types.
For example gcc 4.6 and 4.7 might remove the volatile tag
for such accesses during the SRA (scalar replacement of
aggregates) step:

  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58145

Update the new calls regardless of if it is a scalar type,
this is cleaner than having three alternatives.

	Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Link: http://lkml.kernel.org/r/1424662301.6539.18.camel@stgolabs.net
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4d3199e4ca8e6670b54dc5ee070ffd54385988e9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/mcs_spinlock.c
#	kernel/mcs_spinlock.h
#	kernel/mutex.c
diff --cc kernel/mcs_spinlock.c
index 13c1bbdbe694,dc85ee23a26f..000000000000
--- a/kernel/mcs_spinlock.c
+++ b/kernel/mcs_spinlock.c
@@@ -66,12 -90,15 +66,18 @@@ bool osq_lock(struct optimistic_spin_qu
  
  	node->locked = 0;
  	node->next = NULL;
 -	node->cpu = curr;
  
 -	old = atomic_xchg(&lock->tail, curr);
 -	if (old == OSQ_UNLOCKED_VAL)
 +	node->prev = prev = xchg(lock, node);
 +	if (likely(prev == NULL))
  		return true;
  
++<<<<<<< HEAD:kernel/mcs_spinlock.c
 +	ACCESS_ONCE(prev->next) = node;
++=======
+ 	prev = decode_cpu(old);
+ 	node->prev = prev;
+ 	WRITE_ONCE(prev->next, node);
++>>>>>>> 4d3199e4ca8e (locking: Remove ACCESS_ONCE() usage):kernel/locking/osq_lock.c
  
  	/*
  	 * Normally @prev is untouchable after the above store; because at that
@@@ -163,16 -190,14 +169,16 @@@ void osq_unlock(struct optimistic_spin_
  	/*
  	 * Second most likely case.
  	 */
 -	node = this_cpu_ptr(&osq_node);
  	next = xchg(&node->next, NULL);
  	if (next) {
- 		ACCESS_ONCE(next->locked) = 1;
+ 		WRITE_ONCE(next->locked, 1);
  		return;
  	}
  
  	next = osq_wait_next(lock, node, NULL);
  	if (next)
- 		ACCESS_ONCE(next->locked) = 1;
+ 		WRITE_ONCE(next->locked, 1);
  }
 +
 +#endif
 +
diff --cc kernel/mcs_spinlock.h
index 074c62536f83,75e114bdf3f2..000000000000
--- a/kernel/mcs_spinlock.h
+++ b/kernel/mcs_spinlock.h
@@@ -101,8 -100,8 +101,13 @@@ void mcs_spin_unlock(struct mcs_spinloc
  		if (likely(cmpxchg(lock, node, NULL) == node))
  			return;
  		/* Wait until the next pointer is set */
++<<<<<<< HEAD:kernel/mcs_spinlock.h
 +		while (!(next = ACCESS_ONCE(node->next)))
 +			arch_mutex_cpu_relax();
++=======
+ 		while (!(next = READ_ONCE(node->next)))
+ 			cpu_relax_lowlatency();
++>>>>>>> 4d3199e4ca8e (locking: Remove ACCESS_ONCE() usage):kernel/locking/mcs_spinlock.h
  	}
  
  	/* Pass lock to next waiter. */
diff --cc kernel/mutex.c
index d8af8cf9c6e9,16b2d3cc88b0..000000000000
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@@ -175,8 -266,9 +175,14 @@@ static inline int mutex_can_spin_on_own
  		return 0;
  
  	rcu_read_lock();
++<<<<<<< HEAD:kernel/mutex.c
 +	if (lock->owner)
 +		retval = lock->owner->on_cpu;
++=======
+ 	owner = READ_ONCE(lock->owner);
+ 	if (owner)
+ 		retval = owner->on_cpu;
++>>>>>>> 4d3199e4ca8e (locking: Remove ACCESS_ONCE() usage):kernel/locking/mutex.c
  	rcu_read_unlock();
  	/*
  	 * if lock->owner is not set, the mutex owner may have just acquired
@@@ -184,9 -276,144 +190,146 @@@
  	 */
  	return retval;
  }
++<<<<<<< HEAD:kernel/mutex.c
++=======
+ 
+ /*
+  * Atomically try to take the lock when it is available
+  */
+ static inline bool mutex_try_to_acquire(struct mutex *lock)
+ {
+ 	return !mutex_is_locked(lock) &&
+ 		(atomic_cmpxchg(&lock->count, 1, 0) == 1);
+ }
+ 
+ /*
+  * Optimistic spinning.
+  *
+  * We try to spin for acquisition when we find that the lock owner
+  * is currently running on a (different) CPU and while we don't
+  * need to reschedule. The rationale is that if the lock owner is
+  * running, it is likely to release the lock soon.
+  *
+  * Since this needs the lock owner, and this mutex implementation
+  * doesn't track the owner atomically in the lock field, we need to
+  * track it non-atomically.
+  *
+  * We can't do this for DEBUG_MUTEXES because that relies on wait_lock
+  * to serialize everything.
+  *
+  * The mutex spinners are queued up using MCS lock so that only one
+  * spinner can compete for the mutex. However, if mutex spinning isn't
+  * going to happen, there is no point in going through the lock/unlock
+  * overhead.
+  *
+  * Returns true when the lock was taken, otherwise false, indicating
+  * that we need to jump to the slowpath and sleep.
+  */
+ static bool mutex_optimistic_spin(struct mutex *lock,
+ 				  struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)
+ {
+ 	struct task_struct *task = current;
+ 
+ 	if (!mutex_can_spin_on_owner(lock))
+ 		goto done;
+ 
+ 	/*
+ 	 * In order to avoid a stampede of mutex spinners trying to
+ 	 * acquire the mutex all at once, the spinners need to take a
+ 	 * MCS (queued) lock first before spinning on the owner field.
+ 	 */
+ 	if (!osq_lock(&lock->osq))
+ 		goto done;
+ 
+ 	while (true) {
+ 		struct task_struct *owner;
+ 
+ 		if (use_ww_ctx && ww_ctx->acquired > 0) {
+ 			struct ww_mutex *ww;
+ 
+ 			ww = container_of(lock, struct ww_mutex, base);
+ 			/*
+ 			 * If ww->ctx is set the contents are undefined, only
+ 			 * by acquiring wait_lock there is a guarantee that
+ 			 * they are not invalid when reading.
+ 			 *
+ 			 * As such, when deadlock detection needs to be
+ 			 * performed the optimistic spinning cannot be done.
+ 			 */
+ 			if (READ_ONCE(ww->ctx))
+ 				break;
+ 		}
+ 
+ 		/*
+ 		 * If there's an owner, wait for it to either
+ 		 * release the lock or go to sleep.
+ 		 */
+ 		owner = READ_ONCE(lock->owner);
+ 		if (owner && !mutex_spin_on_owner(lock, owner))
+ 			break;
+ 
+ 		/* Try to acquire the mutex if it is unlocked. */
+ 		if (mutex_try_to_acquire(lock)) {
+ 			lock_acquired(&lock->dep_map, ip);
+ 
+ 			if (use_ww_ctx) {
+ 				struct ww_mutex *ww;
+ 				ww = container_of(lock, struct ww_mutex, base);
+ 
+ 				ww_mutex_set_context_fastpath(ww, ww_ctx);
+ 			}
+ 
+ 			mutex_set_owner(lock);
+ 			osq_unlock(&lock->osq);
+ 			return true;
+ 		}
+ 
+ 		/*
+ 		 * When there's no owner, we might have preempted between the
+ 		 * owner acquiring the lock and setting the owner field. If
+ 		 * we're an RT task that will live-lock because we won't let
+ 		 * the owner complete.
+ 		 */
+ 		if (!owner && (need_resched() || rt_task(task)))
+ 			break;
+ 
+ 		/*
+ 		 * The cpu_relax() call is a compiler barrier which forces
+ 		 * everything in this loop to be re-loaded. We don't need
+ 		 * memory barriers as we'll eventually observe the right
+ 		 * values at the cost of a few extra spins.
+ 		 */
+ 		cpu_relax_lowlatency();
+ 	}
+ 
+ 	osq_unlock(&lock->osq);
+ done:
+ 	/*
+ 	 * If we fell out of the spin path because of need_resched(),
+ 	 * reschedule now, before we try-lock the mutex. This avoids getting
+ 	 * scheduled out right after we obtained the mutex.
+ 	 */
+ 	if (need_resched()) {
+ 		/*
+ 		 * We _should_ have TASK_RUNNING here, but just in case
+ 		 * we do not, make it so, otherwise we might get stuck.
+ 		 */
+ 		__set_current_state(TASK_RUNNING);
+ 		schedule_preempt_disabled();
+ 	}
+ 
+ 	return false;
+ }
+ #else
+ static bool mutex_optimistic_spin(struct mutex *lock,
+ 				  struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)
+ {
+ 	return false;
+ }
++>>>>>>> 4d3199e4ca8e (locking: Remove ACCESS_ONCE() usage):kernel/locking/mutex.c
  #endif
  
 -__visible __used noinline
 -void __sched __mutex_unlock_slowpath(atomic_t *lock_count);
 +static __used noinline void __sched __mutex_unlock_slowpath(atomic_t *lock_count);
  
  /**
   * mutex_unlock - release the mutex
@@@ -257,10 -484,10 +400,10 @@@ void __sched ww_mutex_unlock(struct ww_
  EXPORT_SYMBOL(ww_mutex_unlock);
  
  static inline int __sched
 -__ww_mutex_lock_check_stamp(struct mutex *lock, struct ww_acquire_ctx *ctx)
 +__mutex_lock_check_stamp(struct mutex *lock, struct ww_acquire_ctx *ctx)
  {
  	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base);
- 	struct ww_acquire_ctx *hold_ctx = ACCESS_ONCE(ww->ctx);
+ 	struct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);
  
  	if (!hold_ctx)
  		return 0;
diff --git a/include/linux/seqlock.h b/include/linux/seqlock.h
index 2a4d40660259..48f2f69e3867 100644
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@ -74,7 +74,7 @@ static inline unsigned __read_seqcount_begin(const seqcount_t *s)
 	unsigned ret;
 
 repeat:
-	ret = ACCESS_ONCE(s->sequence);
+	ret = READ_ONCE(s->sequence);
 	if (unlikely(ret & 1)) {
 		cpu_relax();
 		goto repeat;
@@ -93,7 +93,7 @@ repeat:
  */
 static inline unsigned raw_read_seqcount(const seqcount_t *s)
 {
-	unsigned ret = ACCESS_ONCE(s->sequence);
+	unsigned ret = READ_ONCE(s->sequence);
 	smp_rmb();
 	return ret;
 }
@@ -130,7 +130,7 @@ static inline unsigned read_seqcount_begin(const seqcount_t *s)
  */
 static inline unsigned raw_seqcount_begin(const seqcount_t *s)
 {
-	unsigned ret = ACCESS_ONCE(s->sequence);
+	unsigned ret = READ_ONCE(s->sequence);
 	smp_rmb();
 	return ret & ~1;
 }
* Unmerged path kernel/mcs_spinlock.c
* Unmerged path kernel/mcs_spinlock.h
* Unmerged path kernel/mutex.c
diff --git a/lib/lockref.c b/lib/lockref.c
index af6e95d0bed6..eee9a8d6dc18 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -26,7 +26,7 @@
 #define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
 	struct lockref old;							\
 	BUILD_BUG_ON(sizeof(old) != 8);						\
-	old.lock_count = ACCESS_ONCE(lockref->lock_count);			\
+	old.lock_count = READ_ONCE(lockref->lock_count);			\
 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
 		struct lockref new = old, prev = old;				\
 		CODE								\
diff --git a/lib/rwsem.c b/lib/rwsem.c
index ce8c3c801945..74a37b88ebb5 100644
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@ -277,7 +277,7 @@ static inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)
  */
 static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)
 {
-	long old, count = ACCESS_ONCE(sem->count);
+	long old, count = READ_ONCE(sem->count);
 
 	while (true) {
 		if (!(count == 0 || count == RWSEM_WAITING_BIAS))
@@ -300,9 +300,9 @@ static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)
 		return false;
 
 	rcu_read_lock();
-	owner = ACCESS_ONCE(sem->owner);
+	owner = READ_ONCE(sem->owner);
 	if (!owner) {
-		long count = ACCESS_ONCE(sem->count);
+		long count = READ_ONCE(sem->count);
 		/*
 		 * If sem->owner is not set, yet we have just recently entered the
 		 * slowpath with the lock being active, then there is a possibility
@@ -381,7 +381,7 @@ static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
 		goto done;
 
 	while (true) {
-		owner = ACCESS_ONCE(sem->owner);
+		owner = READ_ONCE(sem->owner);
 		if (owner && !rwsem_spin_on_owner(sem, owner))
 			break;
 
@@ -455,7 +455,7 @@ struct rw_semaphore __sched *rwsem_down_write_failed(struct rw_semaphore *sem)
 
 	/* we're now waiting on the lock, but no longer actively locking */
 	if (waiting) {
-		count = ACCESS_ONCE(sem->count);
+		count = READ_ONCE(sem->count);
 
 		/*
 		 * If there were already threads queued before us and there are
