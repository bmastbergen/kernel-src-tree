mm/hugetlb: introduce minimum hugepage order

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hugetlb: introduce minimum hugepage order (Andrea Arcangeli) [1430172]
Rebuild_FUZZ: 96.47%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit 641844f5616d7c6597309f560838f996466d7aac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/641844f5.failed

Currently the initial value of order in dissolve_free_huge_page is 64 or
32, which leads to the following warning in static checker:

  mm/hugetlb.c:1203 dissolve_free_huge_pages()
  warn: potential right shift more than type allows '9,18,64'

This is a potential risk of infinite loop, because 1 << order (== 0) is used
in for-loop like this:

  for (pfn =3D start_pfn; pfn < end_pfn; pfn +=3D 1 << order)
      ...

So this patch fixes it by using global minimum_order calculated at boot time.

    text    data     bss     dec     hex filename
   28313     469   84236  113018   1b97a mm/hugetlb.o
   28256     473   84236  112965   1b945 mm/hugetlb.o (patched)

Fixes: c8721bbbdd36 ("mm: memory-hotplug: enable memory hotplug to handle hugepage")
	Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 641844f5616d7c6597309f560838f996466d7aac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 2b990a3a9cf5,10de25cf1f99..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1126,16 -1193,13 +1131,22 @@@ static void dissolve_free_huge_page(str
   */
  void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)
  {
- 	unsigned int order = 8 * sizeof(void *);
  	unsigned long pfn;
- 	struct hstate *h;
  
++<<<<<<< HEAD
 +	/* Set scan step to minimum hugepage size */
 +	for_each_hstate(h)
 +		if (order > huge_page_order(h))
 +			order = huge_page_order(h);
 +	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 << order));
 +	for (pfn = start_pfn; pfn < end_pfn; pfn += 1 << order)
++=======
+ 	if (!hugepages_supported())
+ 		return;
+ 
+ 	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 << minimum_order));
+ 	for (pfn = start_pfn; pfn < end_pfn; pfn += 1 << minimum_order)
++>>>>>>> 641844f5616d (mm/hugetlb: introduce minimum hugepage order)
  		dissolve_free_huge_page(pfn_to_page(pfn));
  }
  
* Unmerged path mm/hugetlb.c
