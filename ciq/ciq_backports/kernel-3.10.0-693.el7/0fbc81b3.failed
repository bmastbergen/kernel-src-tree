chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit 0fbc81b3ad513fecaaf62b48f42b89fcd57f7682
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/0fbc81b3.failed

Allocate resources dynamically to cxgb4's Upper layer driver's(ULD) like
cxgbit, iw_cxgb4 and cxgb4i. Allocate resources when they register with
cxgb4 driver and free them while unregistering. All the queues and the
interrupts for them will be allocated during ULD probe only and freed
during remove.

	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 0fbc81b3ad513fecaaf62b48f42b89fcd57f7682)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 70d5542c0f8b,1f9867db3b78..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@@ -679,11 -698,8 +673,7 @@@ struct sge 
  	struct sge_ctrl_txq ctrlq[MAX_CTRL_QUEUES];
  
  	struct sge_eth_rxq ethrxq[MAX_ETH_QSETS];
- 	struct sge_ofld_rxq iscsirxq[MAX_OFLD_QSETS];
- 	struct sge_ofld_rxq iscsitrxq[MAX_ISCSIT_QUEUES];
- 	struct sge_ofld_rxq rdmarxq[MAX_RDMA_QUEUES];
- 	struct sge_ofld_rxq rdmaciq[MAX_RDMA_CIQS];
  	struct sge_rspq fw_evtq ____cacheline_aligned_in_smp;
 -	struct sge_uld_rxq_info **uld_rxq_info;
  
  	struct sge_rspq intrq ____cacheline_aligned_in_smp;
  	spinlock_t intrq_lock;
@@@ -691,14 -707,8 +681,19 @@@
  	u16 max_ethqsets;           /* # of available Ethernet queue sets */
  	u16 ethqsets;               /* # of active Ethernet queue sets */
  	u16 ethtxq_rover;           /* Tx queue to clean up next */
++<<<<<<< HEAD
 +	u16 iscsiqsets;              /* # of active iSCSI queue sets */
 +	u16 niscsitq;               /* # of available iSCST Rx queues */
 +	u16 rdmaqs;                 /* # of available RDMA Rx queues */
 +	u16 rdmaciqs;               /* # of available RDMA concentrator IQs */
 +	u16 iscsi_rxq[MAX_OFLD_QSETS];
 +	u16 iscsit_rxq[MAX_ISCSIT_QUEUES];
 +	u16 rdma_rxq[MAX_RDMA_QUEUES];
 +	u16 rdma_ciq[MAX_RDMA_CIQS];
++=======
+ 	u16 ofldqsets;              /* # of active ofld queue sets */
+ 	u16 nqs_per_uld;	    /* # of Rx queues per ULD */
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	u16 timer_val[SGE_NTIMERS];
  	u8 counter_val[SGE_NCOUNTERS];
  	u32 fl_pg_order;            /* large page allocation size */
@@@ -750,6 -757,23 +742,26 @@@ struct hash_mac_addr 
  	u8 addr[ETH_ALEN];
  };
  
++<<<<<<< HEAD
++=======
+ struct uld_msix_bmap {
+ 	unsigned long *msix_bmap;
+ 	unsigned int mapsize;
+ 	spinlock_t lock; /* lock for acquiring bitmap */
+ };
+ 
+ struct uld_msix_info {
+ 	unsigned short vec;
+ 	char desc[IFNAMSIZ + 10];
+ 	unsigned int idx;
+ };
+ 
+ struct vf_info {
+ 	unsigned char vf_mac_addr[ETH_ALEN];
+ 	bool pf_set_mac;
+ };
+ 
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  struct adapter {
  	void __iomem *regs;
  	void __iomem *bar2;
@@@ -773,6 -797,9 +785,12 @@@
  		unsigned short vec;
  		char desc[IFNAMSIZ + 10];
  	} msix_info[MAX_INGQ + 1];
++<<<<<<< HEAD
++=======
+ 	struct uld_msix_info *msix_info_ulds; /* msix info for uld's */
+ 	struct uld_msix_bmap msix_bmap_ulds; /* msix bitmap for all uld */
+ 	int msi_idx;
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  
  	struct doorbell_stats db_stats;
  	struct sge sge;
@@@ -787,7 -817,10 +805,14 @@@
  	unsigned int clipt_start;
  	unsigned int clipt_end;
  	struct clip_tbl *clipt;
++<<<<<<< HEAD
++	void *uld_handle[CXGB4_ULD_MAX];
++=======
+ 	struct cxgb4_uld_info *uld;
  	void *uld_handle[CXGB4_ULD_MAX];
+ 	unsigned int num_uld;
+ 	unsigned int num_ofld_uld;
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	struct list_head list_node;
  	struct list_head rcu_node;
  	struct list_head mac_hlist; /* list of MAC addresses in MPS Hash */
@@@ -807,9 -840,11 +832,11 @@@
  #define T4_OS_LOG_MBOX_CMDS 256
  	struct mbox_cmd_log *mbox_log;
  
+ 	struct mutex uld_mutex;
+ 
  	struct dentry *debugfs_root;
 -	bool use_bd;     /* Use SGE Back Door intfc for reading SGE Contexts */
 -	bool trace_rss;	/* 1 implies that different RSS flit per filter is
 +	u32 use_bd;     /* Use SGE Back Door intfc for reading SGE Contexts */
 +	u32 trace_rss;	/* 1 implies that different RSS flit per filter is
  			 * used per filter else if 0 default RSS flit is
  			 * used for all 4 filters.
  			 */
@@@ -946,6 -1030,16 +973,19 @@@ static inline int is_offload(const stru
  	return adap->params.offload;
  }
  
++<<<<<<< HEAD
++=======
+ static inline int is_pci_uld(const struct adapter *adap)
+ {
+ 	return adap->params.crypto;
+ }
+ 
+ static inline int is_uld(const struct adapter *adap)
+ {
+ 	return (adap->params.offload || adap->params.crypto);
+ }
+ 
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  static inline u32 t4_read_reg(struct adapter *adap, u32 reg_addr)
  {
  	return readl(adap->regs + reg_addr);
@@@ -1517,5 -1626,9 +1559,13 @@@ void t4_idma_monitor(struct adapter *ad
  		     int hz, int ticks);
  int t4_set_vf_mac_acl(struct adapter *adapter, unsigned int vf,
  		      unsigned int naddr, u8 *addr);
- 
++<<<<<<< HEAD
++
++=======
+ void t4_uld_mem_free(struct adapter *adap);
+ int t4_uld_mem_alloc(struct adapter *adap);
+ void t4_uld_clean_up(struct adapter *adap);
+ void t4_register_netevent_notifier(void);
+ void free_rspq_fl(struct adapter *adap, struct sge_rspq *rq, struct sge_fl *fl);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  #endif /* __CXGB4_H__ */
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index b2bcf538138f,d1ebb84c073e..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@@ -223,13 -224,8 +223,18 @@@ MODULE_PARM_DESC(select_queue
  
  static struct dentry *cxgb4_debugfs_root;
  
++<<<<<<< HEAD
 +static LIST_HEAD(adapter_list);
 +static DEFINE_MUTEX(uld_mutex);
 +/* Adapter list to be accessed from atomic context */
 +static LIST_HEAD(adap_rcu_list);
 +static DEFINE_SPINLOCK(adap_rcu_lock);
 +static struct cxgb4_uld_info ulds[CXGB4_ULD_MAX];
 +static const char *const uld_str[] = { "RDMA", "iSCSI", "iSCSIT" };
++=======
+ LIST_HEAD(adapter_list);
+ DEFINE_MUTEX(uld_mutex);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  
  static void link_report(struct net_device *dev)
  {
@@@ -1032,42 -902,11 +911,15 @@@ static void enable_rx(struct adapter *a
  	}
  }
  
- static int alloc_ofld_rxqs(struct adapter *adap, struct sge_ofld_rxq *q,
- 			   unsigned int nq, unsigned int per_chan, int msi_idx,
- 			   u16 *ids, bool lro)
- {
- 	int i, err;
- 
- 	for (i = 0; i < nq; i++, q++) {
- 		if (msi_idx > 0)
- 			msi_idx++;
- 		err = t4_sge_alloc_rxq(adap, &q->rspq, false,
- 				       adap->port[i / per_chan],
- 				       msi_idx, q->fl.size ? &q->fl : NULL,
- 				       uldrx_handler,
- 				       lro ? uldrx_flush_handler : NULL,
- 				       0);
- 		if (err)
- 			return err;
- 		memset(&q->stats, 0, sizeof(q->stats));
- 		if (ids)
- 			ids[i] = q->rspq.abs_id;
- 	}
- 	return 0;
- }
  
- /**
-  *	setup_sge_queues - configure SGE Tx/Rx/response queues
-  *	@adap: the adapter
-  *
-  *	Determines how many sets of SGE queues to use and initializes them.
-  *	We support multiple queue sets per port if we have MSI-X, otherwise
-  *	just one queue set per port.
-  */
- static int setup_sge_queues(struct adapter *adap)
+ static int setup_fw_sge_queues(struct adapter *adap)
  {
++<<<<<<< HEAD
 +	int err, msi_idx, i, j;
++=======
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	struct sge *s = &adap->sge;
+ 	int err = 0;
  
  	bitmap_zero(s->starving_fl, s->egr_sz);
  	bitmap_zero(s->txq_maperr, s->egr_sz);
@@@ -1079,28 -918,30 +931,38 @@@
  				       NULL, NULL, NULL, -1);
  		if (err)
  			return err;
 -		adap->msi_idx = -((int)s->intrq.abs_id + 1);
 +		msi_idx = -((int)s->intrq.abs_id + 1);
  	}
  
- 	/* NOTE: If you add/delete any Ingress/Egress Queue allocations in here,
- 	 * don't forget to update the following which need to be
- 	 * synchronized to and changes here.
- 	 *
- 	 * 1. The calculations of MAX_INGQ in cxgb4.h.
- 	 *
- 	 * 2. Update enable_msix/name_msix_vecs/request_msix_queue_irqs
- 	 *    to accommodate any new/deleted Ingress Queues
- 	 *    which need MSI-X Vectors.
- 	 *
- 	 * 3. Update sge_qinfo_show() to include information on the
- 	 *    new/deleted queues.
- 	 */
  	err = t4_sge_alloc_rxq(adap, &s->fw_evtq, true, adap->port[0],
++<<<<<<< HEAD
 +			       msi_idx, NULL, fwevtq_handler, NULL, -1);
 +	if (err) {
 +freeout:	t4_free_sge_resources(adap);
 +		return err;
 +	}
++=======
+ 			       adap->msi_idx, NULL, fwevtq_handler, NULL, -1);
+ 	if (err)
+ 		t4_free_sge_resources(adap);
+ 	return err;
+ }
+ 
+ /**
+  *	setup_sge_queues - configure SGE Tx/Rx/response queues
+  *	@adap: the adapter
+  *
+  *	Determines how many sets of SGE queues to use and initializes them.
+  *	We support multiple queue sets per port if we have MSI-X, otherwise
+  *	just one queue set per port.
+  */
+ static int setup_sge_queues(struct adapter *adap)
+ {
+ 	int err, i, j;
+ 	struct sge *s = &adap->sge;
+ 	struct sge_uld_rxq_info *rxq_info = s->uld_rxq_info[CXGB4_ULD_RDMA];
+ 	unsigned int cmplqid = 0;
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  
  	for_each_port(adap, i) {
  		struct net_device *dev = adap->port[i];
@@@ -1140,30 -981,15 +1002,34 @@@
  			goto freeout;
  	}
  
++<<<<<<< HEAD
 +#define ALLOC_OFLD_RXQS(firstq, nq, per_chan, ids, lro) do { \
 +	err = alloc_ofld_rxqs(adap, firstq, nq, per_chan, msi_idx, ids, lro); \
 +	if (err) \
 +		goto freeout; \
 +	if (msi_idx > 0) \
 +		msi_idx += nq; \
 +} while (0)
 +
 +	ALLOC_OFLD_RXQS(s->iscsirxq, s->iscsiqsets, j, s->iscsi_rxq, false);
 +	ALLOC_OFLD_RXQS(s->iscsitrxq, s->niscsitq, j, s->iscsit_rxq, true);
 +	ALLOC_OFLD_RXQS(s->rdmarxq, s->rdmaqs, 1, s->rdma_rxq, false);
 +	j = s->rdmaciqs / adap->params.nports; /* rdmaq queues per channel */
 +	ALLOC_OFLD_RXQS(s->rdmaciq, s->rdmaciqs, j, s->rdma_ciq, false);
 +
 +#undef ALLOC_OFLD_RXQS
 +
++=======
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	for_each_port(adap, i) {
- 		/*
- 		 * Note that ->rdmarxq[i].rspq.cntxt_id below is 0 if we don't
+ 		/* Note that cmplqid below is 0 if we don't
  		 * have RDMA queues, and that's the right value.
  		 */
+ 		if (rxq_info)
+ 			cmplqid	= rxq_info->uldrxq[i].rspq.cntxt_id;
+ 
  		err = t4_sge_alloc_ctrl_txq(adap, &s->ctrlq[i], adap->port[i],
- 					    s->fw_evtq.cntxt_id,
- 					    s->rdmarxq[i].rspq.cntxt_id);
+ 					    s->fw_evtq.cntxt_id, cmplqid);
  		if (err)
  			goto freeout;
  	}
@@@ -2556,10 -2309,10 +2345,17 @@@ static void detach_ulds(struct adapter 
  	mutex_lock(&uld_mutex);
  	list_del(&adap->list_node);
  	for (i = 0; i < CXGB4_ULD_MAX; i++)
++<<<<<<< HEAD
 +		if (adap->uld_handle[i]) {
 +			ulds[i].state_change(adap->uld_handle[i],
 +					     CXGB4_STATE_DETACH);
 +			adap->uld_handle[i] = NULL;
++=======
+ 		if (adap->uld && adap->uld[i].handle) {
+ 			adap->uld[i].state_change(adap->uld[i].handle,
+ 					     CXGB4_STATE_DETACH);
+ 			adap->uld[i].handle = NULL;
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  		}
  	if (netevent_registered && list_empty(&adapter_list)) {
  		unregister_netevent_notifier(&cxgb4_netevent_nb);
@@@ -2578,64 -2327,15 +2370,20 @@@ static void notify_ulds(struct adapter 
  
  	mutex_lock(&uld_mutex);
  	for (i = 0; i < CXGB4_ULD_MAX; i++)
++<<<<<<< HEAD
 +		if (adap->uld_handle[i])
 +			ulds[i].state_change(adap->uld_handle[i], new_state);
++=======
+ 		if (adap->uld && adap->uld[i].handle)
+ 			adap->uld[i].state_change(adap->uld[i].handle,
+ 						  new_state);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	mutex_unlock(&uld_mutex);
  }
  
- /**
-  *	cxgb4_register_uld - register an upper-layer driver
-  *	@type: the ULD type
-  *	@p: the ULD methods
-  *
-  *	Registers an upper-layer driver with this driver and notifies the ULD
-  *	about any presently available devices that support its type.  Returns
-  *	%-EBUSY if a ULD of the same type is already registered.
-  */
- int cxgb4_register_uld(enum cxgb4_uld type, const struct cxgb4_uld_info *p)
- {
- 	int ret = 0;
- 	struct adapter *adap;
- 
- 	if (type >= CXGB4_ULD_MAX)
- 		return -EINVAL;
- 	mutex_lock(&uld_mutex);
- 	if (ulds[type].add) {
- 		ret = -EBUSY;
- 		goto out;
- 	}
- 	ulds[type] = *p;
- 	list_for_each_entry(adap, &adapter_list, list_node)
- 		uld_attach(adap, type);
- out:	mutex_unlock(&uld_mutex);
- 	return ret;
- }
- EXPORT_SYMBOL(cxgb4_register_uld);
- 
- /**
-  *	cxgb4_unregister_uld - unregister an upper-layer driver
-  *	@type: the ULD type
-  *
-  *	Unregisters an existing upper-layer driver.
-  */
- int cxgb4_unregister_uld(enum cxgb4_uld type)
- {
- 	struct adapter *adap;
- 
- 	if (type >= CXGB4_ULD_MAX)
- 		return -EINVAL;
- 	mutex_lock(&uld_mutex);
- 	list_for_each_entry(adap, &adapter_list, list_node)
- 		adap->uld_handle[type] = NULL;
- 	ulds[type].add = NULL;
- 	mutex_unlock(&uld_mutex);
- 	return 0;
- }
- EXPORT_SYMBOL(cxgb4_unregister_uld);
- 
  #if IS_ENABLED(CONFIG_IPV6)
  static int cxgb4_inet6addr_handler(struct notifier_block *this,
 -				   unsigned long event, void *data)
 +				  unsigned long event, void *data)
  {
  	struct inet6_ifaddr *ifa = data;
  	struct net_device *event_dev = ifa->idev->dev;
@@@ -4166,7 -4010,14 +3915,9 @@@ static int adap_init0(struct adapter *a
  			goto bye;
  		adap->vres.iscsi.start = val[0];
  		adap->vres.iscsi.size = val[1] - val[0] + 1;
+ 		/* LIO target and cxgb4i initiaitor */
+ 		adap->num_ofld_uld += 2;
  	}
 -	if (caps_cmd.cryptocaps) {
 -		/* Should query params here...TODO */
 -		adap->params.crypto |= ULP_CRYPTO_LOOKASIDE;
 -		adap->num_uld += 1;
 -	}
  #undef FW_PARAM_PFVF
  #undef FW_PARAM_DEV
  
@@@ -4380,9 -4214,13 +4131,16 @@@ static void cfg_queues(struct adapter *
  
  	/* Reduce memory usage in kdump environment, disable all offload.
  	 */
 -	if (is_kdump_kernel()) {
 +	if (is_kdump_kernel())
  		adap->params.offload = 0;
++<<<<<<< HEAD
++=======
+ 		adap->params.crypto = 0;
+ 	} else if (is_uld(adap) && t4_uld_mem_alloc(adap)) {
+ 		adap->params.offload = 0;
+ 		adap->params.crypto = 0;
+ 	}
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  
  	for_each_port(adap, i)
  		n10g += is_x_10g_port(&adap2pinfo(adap, i)->link_cfg);
@@@ -4542,6 -4326,44 +4246,47 @@@ static void reduce_ethqs(struct adapte
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int get_msix_info(struct adapter *adap)
+ {
+ 	struct uld_msix_info *msix_info;
+ 	unsigned int max_ingq = 0;
+ 
+ 	if (is_offload(adap))
+ 		max_ingq += MAX_OFLD_QSETS * adap->num_ofld_uld;
+ 	if (is_pci_uld(adap))
+ 		max_ingq += MAX_OFLD_QSETS * adap->num_uld;
+ 
+ 	if (!max_ingq)
+ 		goto out;
+ 
+ 	msix_info = kcalloc(max_ingq, sizeof(*msix_info), GFP_KERNEL);
+ 	if (!msix_info)
+ 		return -ENOMEM;
+ 
+ 	adap->msix_bmap_ulds.msix_bmap = kcalloc(BITS_TO_LONGS(max_ingq),
+ 						 sizeof(long), GFP_KERNEL);
+ 	if (!adap->msix_bmap_ulds.msix_bmap) {
+ 		kfree(msix_info);
+ 		return -ENOMEM;
+ 	}
+ 	spin_lock_init(&adap->msix_bmap_ulds.lock);
+ 	adap->msix_info_ulds = msix_info;
+ out:
+ 	return 0;
+ }
+ 
+ static void free_msix_info(struct adapter *adap)
+ {
+ 	if (!(adap->num_uld && adap->num_ofld_uld))
+ 		return;
+ 
+ 	kfree(adap->msix_info_ulds);
+ 	kfree(adap->msix_bmap_ulds.msix_bmap);
+ }
+ 
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  /* 2 MSI-X vectors needed for the FW queue and non-data interrupts */
  #define EXTRA_VECS 2
  
@@@ -4552,25 -4374,35 +4297,45 @@@ static int enable_msix(struct adapter *
  	struct sge *s = &adap->sge;
  	unsigned int nchan = adap->params.nports;
  	struct msix_entry *entries;
 -	int max_ingq = MAX_INGQ;
  
++<<<<<<< HEAD
 +	entries = kmalloc(sizeof(*entries) * (MAX_INGQ + 1),
++=======
+ 	if (is_pci_uld(adap))
+ 		max_ingq += (MAX_OFLD_QSETS * adap->num_uld);
+ 	if (is_offload(adap))
+ 		max_ingq += (MAX_OFLD_QSETS * adap->num_ofld_uld);
+ 	entries = kmalloc(sizeof(*entries) * (max_ingq + 1),
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  			  GFP_KERNEL);
  	if (!entries)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	for (i = 0; i < MAX_INGQ + 1; ++i)
++=======
+ 	/* map for msix */
+ 	if (get_msix_info(adap)) {
+ 		adap->params.offload = 0;
+ 		adap->params.crypto = 0;
+ 	}
+ 
+ 	for (i = 0; i < max_ingq + 1; ++i)
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  		entries[i].entry = i;
  
  	want = s->max_ethqsets + EXTRA_VECS;
  	if (is_offload(adap)) {
- 		want += s->rdmaqs + s->rdmaciqs + s->iscsiqsets	+
- 			s->niscsitq;
- 		/* need nchan for each possible ULD */
- 		if (is_t4(adap->params.chip))
- 			ofld_need = 3 * nchan;
- 		else
- 			ofld_need = 4 * nchan;
+ 		want += adap->num_ofld_uld * s->ofldqsets;
+ 		ofld_need = adap->num_ofld_uld * nchan;
+ 	}
++<<<<<<< HEAD
++=======
+ 	if (is_pci_uld(adap)) {
+ 		want += adap->num_uld * s->ofldqsets;
+ 		uld_need = adap->num_uld * nchan;
  	}
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  #ifdef CONFIG_CHELSIO_T4_DCB
  	/* For Data Center Bridging we need 8 Ethernet TX Priority Queues for
  	 * each port.
@@@ -4597,27 -4429,25 +4362,49 @@@
  		if (i < s->ethqsets)
  			reduce_ethqs(adap, i);
  	}
++<<<<<<< HEAD
 +	if (is_offload(adap)) {
 +		if (allocated < want) {
 +			s->rdmaqs = nchan;
 +			s->rdmaciqs = nchan;
 +
 +			if (!is_t4(adap->params.chip))
 +				s->niscsitq = nchan;
 +		}
 +
 +		/* leftovers go to OFLD */
 +		i = allocated - EXTRA_VECS - s->max_ethqsets -
 +		    s->rdmaqs - s->rdmaciqs - s->niscsitq;
 +		s->iscsiqsets = (i / nchan) * nchan;  /* round down */
 +
 +	}
 +	for (i = 0; i < allocated; ++i)
 +		adap->msix_info[i].vec = entries[i].vector;
 +	dev_info(adap->pdev_dev, "%d MSI-X vectors allocated, "
 +		 "nic %d iscsi %d rdma cpl %d rdma ciq %d\n",
 +		 allocated, s->max_ethqsets, s->iscsiqsets, s->rdmaqs,
 +		 s->rdmaciqs);
++=======
+ 	if (is_uld(adap)) {
+ 		if (allocated < want)
+ 			s->nqs_per_uld = nchan;
+ 		else
+ 			s->nqs_per_uld = s->ofldqsets;
+ 	}
+ 
+ 	for (i = 0; i < (s->max_ethqsets + EXTRA_VECS); ++i)
+ 		adap->msix_info[i].vec = entries[i].vector;
+ 	if (is_uld(adap)) {
+ 		for (j = 0 ; i < allocated; ++i, j++) {
+ 			adap->msix_info_ulds[j].vec = entries[i].vector;
+ 			adap->msix_info_ulds[j].idx = i;
+ 		}
+ 		adap->msix_bmap_ulds.mapsize = j;
+ 	}
+ 	dev_info(adap->pdev_dev, "%d MSI-X vectors allocated, "
+ 		 "nic %d per uld %d\n",
+ 		 allocated, s->max_ethqsets, s->nqs_per_uld);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  
  	kfree(entries);
  	return 0;
@@@ -5366,6 -5220,10 +5157,13 @@@ sriov
  
   out_free_dev:
  	free_some_resources(adapter);
++<<<<<<< HEAD
++=======
+ 	if (adapter->flags & USING_MSIX)
+ 		free_msix_info(adapter);
+ 	if (adapter->num_uld || adapter->num_ofld_uld)
+ 		t4_uld_mem_free(adapter);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
   out_unmap_bar:
  	if (!is_t4(adapter->params.chip))
  		iounmap(adapter->bar2);
@@@ -5427,6 -5285,10 +5225,13 @@@ static void remove_one(struct pci_dev *
  		if (adapter->flags & FULL_INIT_DONE)
  			cxgb_down(adapter);
  
++<<<<<<< HEAD
++=======
+ 		if (adapter->flags & USING_MSIX)
+ 			free_msix_info(adapter);
+ 		if (adapter->num_uld || adapter->num_ofld_uld)
+ 			t4_uld_mem_free(adapter);
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  		free_some_resources(adapter);
  #if IS_ENABLED(CONFIG_IPV6)
  		t4_cleanup_clip_tbl(adapter);
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
index f3c58aaa932d,b3544f6b88ca..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
@@@ -284,6 -288,11 +288,14 @@@ struct cxgb4_lld_info 
  
  struct cxgb4_uld_info {
  	const char *name;
++<<<<<<< HEAD
++=======
+ 	void *handle;
+ 	unsigned int nrxq;
+ 	unsigned int rxq_size;
+ 	bool ciq;
+ 	bool lro;
++>>>>>>> 0fbc81b3ad51 (chcr/cxgb4i/cxgbit/RDMA/cxgb4: Allocate resources dynamically for all cxgb4 ULD's)
  	void *(*add)(const struct cxgb4_lld_info *p);
  	int (*rx_handler)(void *handle, const __be64 *rsp,
  			  const struct pkt_gl *gl);
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
diff --git a/drivers/crypto/chelsio/chcr_core.c b/drivers/crypto/chelsio/chcr_core.c
index 2f6156b672ce..fb5f9bbfa09c 100644
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@ -39,12 +39,10 @@ static chcr_handler_func work_handlers[NUM_CPL_CMDS] = {
 	[CPL_FW6_PLD] = cpl_fw6_pld_handler,
 };
 
-static struct cxgb4_pci_uld_info chcr_uld_info = {
+static struct cxgb4_uld_info chcr_uld_info = {
 	.name = DRV_MODULE_NAME,
-	.nrxq = 4,
+	.nrxq = MAX_ULD_QSETS,
 	.rxq_size = 1024,
-	.nciq = 0,
-	.ciq_size = 0,
 	.add = chcr_uld_add,
 	.state_change = chcr_uld_state_change,
 	.rx_handler = chcr_uld_rx_handler,
@@ -205,7 +203,7 @@ static int chcr_uld_state_change(void *handle, enum cxgb4_state state)
 
 static int __init chcr_crypto_init(void)
 {
-	if (cxgb4_register_pci_uld(CXGB4_PCI_ULD1, &chcr_uld_info)) {
+	if (cxgb4_register_uld(CXGB4_ULD_CRYPTO, &chcr_uld_info)) {
 		pr_err("ULD register fail: No chcr crypto support in cxgb4");
 		return -1;
 	}
@@ -228,7 +226,7 @@ static void __exit chcr_crypto_exit(void)
 		kfree(u_ctx);
 	}
 	mutex_unlock(&dev_mutex);
-	cxgb4_unregister_pci_uld(CXGB4_PCI_ULD1);
+	cxgb4_unregister_uld(CXGB4_ULD_CRYPTO);
 }
 
 module_init(chcr_crypto_init);
diff --git a/drivers/infiniband/hw/cxgb4/device.c b/drivers/infiniband/hw/cxgb4/device.c
index eccc412109b6..8d1ecb34107b 100644
--- a/drivers/infiniband/hw/cxgb4/device.c
+++ b/drivers/infiniband/hw/cxgb4/device.c
@@ -1490,6 +1490,10 @@ static int c4iw_uld_control(void *handle, enum cxgb4_control control, ...)
 
 static struct cxgb4_uld_info c4iw_uld_info = {
 	.name = DRV_NAME,
+	.nrxq = MAX_ULD_QSETS,
+	.rxq_size = 511,
+	.ciq = true,
+	.lro = false,
 	.add = c4iw_uld_add,
 	.rx_handler = c4iw_uld_rx_handler,
 	.state_change = c4iw_uld_state_change,
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
index 77378ddfd11d..6aa33e43f732 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
@@ -2432,17 +2432,11 @@ static int sge_qinfo_show(struct seq_file *seq, void *v)
 {
 	struct adapter *adap = seq->private;
 	int eth_entries = DIV_ROUND_UP(adap->sge.ethqsets, 4);
-	int iscsi_entries = DIV_ROUND_UP(adap->sge.iscsiqsets, 4);
-	int iscsit_entries = DIV_ROUND_UP(adap->sge.niscsitq, 4);
-	int rdma_entries = DIV_ROUND_UP(adap->sge.rdmaqs, 4);
-	int ciq_entries = DIV_ROUND_UP(adap->sge.rdmaciqs, 4);
+	int ofld_entries = DIV_ROUND_UP(adap->sge.ofldqsets, 4);
 	int ctrl_entries = DIV_ROUND_UP(MAX_CTRL_QUEUES, 4);
 	int i, r = (uintptr_t)v - 1;
-	int iscsi_idx = r - eth_entries;
-	int iscsit_idx = iscsi_idx - iscsi_entries;
-	int rdma_idx = iscsit_idx - iscsit_entries;
-	int ciq_idx = rdma_idx - rdma_entries;
-	int ctrl_idx =  ciq_idx - ciq_entries;
+	int ofld_idx = r - eth_entries;
+	int ctrl_idx =  ofld_idx - ofld_entries;
 	int fq_idx =  ctrl_idx - ctrl_entries;
 
 	if (r)
@@ -2518,119 +2512,17 @@ do { \
 		RL("FLLow:", fl.low);
 		RL("FLStarving:", fl.starving);
 
-	} else if (iscsi_idx < iscsi_entries) {
-		const struct sge_ofld_rxq *rx =
-			&adap->sge.iscsirxq[iscsi_idx * 4];
+	} else if (ofld_idx < ofld_entries) {
 		const struct sge_ofld_txq *tx =
-			&adap->sge.ofldtxq[iscsi_idx * 4];
-		int n = min(4, adap->sge.iscsiqsets - 4 * iscsi_idx);
+			&adap->sge.ofldtxq[ofld_idx * 4];
+		int n = min(4, adap->sge.ofldqsets - 4 * ofld_idx);
 
-		S("QType:", "iSCSI");
+		S("QType:", "OFLD-Txq");
 		T("TxQ ID:", q.cntxt_id);
 		T("TxQ size:", q.size);
 		T("TxQ inuse:", q.in_use);
 		T("TxQ CIDX:", q.cidx);
 		T("TxQ PIDX:", q.pidx);
-		R("RspQ ID:", rspq.abs_id);
-		R("RspQ size:", rspq.size);
-		R("RspQE size:", rspq.iqe_len);
-		R("RspQ CIDX:", rspq.cidx);
-		R("RspQ Gen:", rspq.gen);
-		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
-		S3("u", "Intr pktcnt:",
-		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
-		R("FL ID:", fl.cntxt_id);
-		R("FL size:", fl.size - 8);
-		R("FL pend:", fl.pend_cred);
-		R("FL avail:", fl.avail);
-		R("FL PIDX:", fl.pidx);
-		R("FL CIDX:", fl.cidx);
-		RL("RxPackets:", stats.pkts);
-		RL("RxImmPkts:", stats.imm);
-		RL("RxNoMem:", stats.nomem);
-		RL("FLAllocErr:", fl.alloc_failed);
-		RL("FLLrgAlcErr:", fl.large_alloc_failed);
-		RL("FLMapErr:", fl.mapping_err);
-		RL("FLLow:", fl.low);
-		RL("FLStarving:", fl.starving);
-
-	} else if (iscsit_idx < iscsit_entries) {
-		const struct sge_ofld_rxq *rx =
-			&adap->sge.iscsitrxq[iscsit_idx * 4];
-		int n = min(4, adap->sge.niscsitq - 4 * iscsit_idx);
-
-		S("QType:", "iSCSIT");
-		R("RspQ ID:", rspq.abs_id);
-		R("RspQ size:", rspq.size);
-		R("RspQE size:", rspq.iqe_len);
-		R("RspQ CIDX:", rspq.cidx);
-		R("RspQ Gen:", rspq.gen);
-		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
-		S3("u", "Intr pktcnt:",
-		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
-		R("FL ID:", fl.cntxt_id);
-		R("FL size:", fl.size - 8);
-		R("FL pend:", fl.pend_cred);
-		R("FL avail:", fl.avail);
-		R("FL PIDX:", fl.pidx);
-		R("FL CIDX:", fl.cidx);
-		RL("RxPackets:", stats.pkts);
-		RL("RxImmPkts:", stats.imm);
-		RL("RxNoMem:", stats.nomem);
-		RL("FLAllocErr:", fl.alloc_failed);
-		RL("FLLrgAlcErr:", fl.large_alloc_failed);
-		RL("FLMapErr:", fl.mapping_err);
-		RL("FLLow:", fl.low);
-		RL("FLStarving:", fl.starving);
-
-	} else if (rdma_idx < rdma_entries) {
-		const struct sge_ofld_rxq *rx =
-				&adap->sge.rdmarxq[rdma_idx * 4];
-		int n = min(4, adap->sge.rdmaqs - 4 * rdma_idx);
-
-		S("QType:", "RDMA-CPL");
-		S("Interface:",
-		  rx[i].rspq.netdev ? rx[i].rspq.netdev->name : "N/A");
-		R("RspQ ID:", rspq.abs_id);
-		R("RspQ size:", rspq.size);
-		R("RspQE size:", rspq.iqe_len);
-		R("RspQ CIDX:", rspq.cidx);
-		R("RspQ Gen:", rspq.gen);
-		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
-		S3("u", "Intr pktcnt:",
-		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
-		R("FL ID:", fl.cntxt_id);
-		R("FL size:", fl.size - 8);
-		R("FL pend:", fl.pend_cred);
-		R("FL avail:", fl.avail);
-		R("FL PIDX:", fl.pidx);
-		R("FL CIDX:", fl.cidx);
-		RL("RxPackets:", stats.pkts);
-		RL("RxImmPkts:", stats.imm);
-		RL("RxNoMem:", stats.nomem);
-		RL("FLAllocErr:", fl.alloc_failed);
-		RL("FLLrgAlcErr:", fl.large_alloc_failed);
-		RL("FLMapErr:", fl.mapping_err);
-		RL("FLLow:", fl.low);
-		RL("FLStarving:", fl.starving);
-
-	} else if (ciq_idx < ciq_entries) {
-		const struct sge_ofld_rxq *rx = &adap->sge.rdmaciq[ciq_idx * 4];
-		int n = min(4, adap->sge.rdmaciqs - 4 * ciq_idx);
-
-		S("QType:", "RDMA-CIQ");
-		S("Interface:",
-		  rx[i].rspq.netdev ? rx[i].rspq.netdev->name : "N/A");
-		R("RspQ ID:", rspq.abs_id);
-		R("RspQ size:", rspq.size);
-		R("RspQE size:", rspq.iqe_len);
-		R("RspQ CIDX:", rspq.cidx);
-		R("RspQ Gen:", rspq.gen);
-		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
-		S3("u", "Intr pktcnt:",
-		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
-		RL("RxAN:", stats.an);
-		RL("RxNoMem:", stats.nomem);
 
 	} else if (ctrl_idx < ctrl_entries) {
 		const struct sge_ctrl_txq *tx = &adap->sge.ctrlq[ctrl_idx * 4];
@@ -2672,10 +2564,7 @@ do { \
 static int sge_queue_entries(const struct adapter *adap)
 {
 	return DIV_ROUND_UP(adap->sge.ethqsets, 4) +
-	       DIV_ROUND_UP(adap->sge.iscsiqsets, 4) +
-	       DIV_ROUND_UP(adap->sge.niscsitq, 4) +
-	       DIV_ROUND_UP(adap->sge.rdmaqs, 4) +
-	       DIV_ROUND_UP(adap->sge.rdmaciqs, 4) +
+	       DIV_ROUND_UP(adap->sge.ofldqsets, 4) +
 	       DIV_ROUND_UP(MAX_CTRL_QUEUES, 4) + 1;
 }
 
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index fa95604acbec..0a2cde5ad74b 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -2796,6 +2796,18 @@ int t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,
 	return 0;
 }
 
+int t4_sge_mod_ctrl_txq(struct adapter *adap, unsigned int eqid,
+			unsigned int cmplqid)
+{
+	u32 param, val;
+
+	param = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |
+		 FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL) |
+		 FW_PARAMS_PARAM_YZ_V(eqid));
+	val = cmplqid;
+	return t4_set_params(adap, adap->mbox, adap->pf, 0, 1, &param, &val);
+}
+
 int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,
 			  struct net_device *dev, unsigned int iqid)
 {
@@ -2950,12 +2962,6 @@ void t4_free_sge_resources(struct adapter *adap)
 		}
 	}
 
-	/* clean up RDMA and iSCSI Rx queues */
-	t4_free_ofld_rxqs(adap, adap->sge.iscsiqsets, adap->sge.iscsirxq);
-	t4_free_ofld_rxqs(adap, adap->sge.niscsitq, adap->sge.iscsitrxq);
-	t4_free_ofld_rxqs(adap, adap->sge.rdmaqs, adap->sge.rdmarxq);
-	t4_free_ofld_rxqs(adap, adap->sge.rdmaciqs, adap->sge.rdmaciq);
-
 	/* clean up offload Tx queues */
 	for (i = 0; i < ARRAY_SIZE(adap->sge.ofldtxq); i++) {
 		struct sge_ofld_txq *q = &adap->sge.ofldtxq[i];
diff --git a/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c b/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
index 15f969f8f2f7..8012d4af7b76 100644
--- a/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
+++ b/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
@@ -83,6 +83,9 @@ static inline int send_tx_flowc_wr(struct cxgbi_sock *);
 
 static const struct cxgb4_uld_info cxgb4i_uld_info = {
 	.name = DRV_MODULE_NAME,
+	.nrxq = MAX_ULD_QSETS,
+	.rxq_size = 1024,
+	.lro = false,
 	.add = t4_uld_add,
 	.rx_handler = t4_uld_rx_handler,
 	.state_change = t4_uld_state_change,
diff --git a/drivers/target/iscsi/cxgbit/cxgbit_main.c b/drivers/target/iscsi/cxgbit/cxgbit_main.c
index 60dccd02bd85..64f980e0b532 100644
--- a/drivers/target/iscsi/cxgbit/cxgbit_main.c
+++ b/drivers/target/iscsi/cxgbit/cxgbit_main.c
@@ -650,6 +650,9 @@ static struct iscsit_transport cxgbit_transport = {
 
 static struct cxgb4_uld_info cxgbit_uld_info = {
 	.name		= DRV_NAME,
+	.nrxq		= MAX_ULD_QSETS,
+	.rxq_size	= 1024,
+	.lro		= true,
 	.add		= cxgbit_uld_add,
 	.state_change	= cxgbit_uld_state_change,
 	.lro_rx_handler = cxgbit_uld_lro_rx_handler,
