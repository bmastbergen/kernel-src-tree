Btrfs: use FLUSH_LIMIT for relocation in reserve_metadata_bytes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Josef Bacik <jbacik@fb.com>
commit 8ca17f0f59529df9e2132ca594ff00c52a9bc556
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/8ca17f0f.failed

We used to allow you to set FLUSH_ALL and then just wouldn't do things like
commit transactions or wait on ordered extents if we noticed you were in a
transaction.  However now that all the flushing for FLUSH_ALL is asynchronous
we've lost the ability to tell, and we could end up deadlocking.  So instead use
FLUSH_LIMIT in reserve_metadata_bytes in relocation and then return -EAGAIN if
we error out to preserve the previous behavior.  I've also added an ASSERT() to
catch anybody else who tries to do this.  Thanks,

	Signed-off-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: David Sterba <dsterba@suse.com>
(cherry picked from commit 8ca17f0f59529df9e2132ca594ff00c52a9bc556)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/extent-tree.c
#	fs/btrfs/relocation.c
diff --cc fs/btrfs/extent-tree.c
index 82ba0ba07377,3a129c42658e..000000000000
--- a/fs/btrfs/extent-tree.c
+++ b/fs/btrfs/extent-tree.c
@@@ -4910,6 -5037,203 +4910,206 @@@ void btrfs_init_async_reclaim_work(stru
  	INIT_WORK(work, btrfs_async_reclaim_metadata_space);
  }
  
++<<<<<<< HEAD
++=======
+ static void priority_reclaim_metadata_space(struct btrfs_fs_info *fs_info,
+ 					    struct btrfs_space_info *space_info,
+ 					    struct reserve_ticket *ticket)
+ {
+ 	u64 to_reclaim;
+ 	int flush_state = FLUSH_DELAYED_ITEMS_NR;
+ 
+ 	spin_lock(&space_info->lock);
+ 	to_reclaim = btrfs_calc_reclaim_metadata_size(fs_info->fs_root,
+ 						      space_info);
+ 	if (!to_reclaim) {
+ 		spin_unlock(&space_info->lock);
+ 		return;
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 
+ 	do {
+ 		flush_space(fs_info->fs_root, space_info, to_reclaim,
+ 			    to_reclaim, flush_state);
+ 		flush_state++;
+ 		spin_lock(&space_info->lock);
+ 		if (ticket->bytes == 0) {
+ 			spin_unlock(&space_info->lock);
+ 			return;
+ 		}
+ 		spin_unlock(&space_info->lock);
+ 
+ 		/*
+ 		 * Priority flushers can't wait on delalloc without
+ 		 * deadlocking.
+ 		 */
+ 		if (flush_state == FLUSH_DELALLOC ||
+ 		    flush_state == FLUSH_DELALLOC_WAIT)
+ 			flush_state = ALLOC_CHUNK;
+ 	} while (flush_state < COMMIT_TRANS);
+ }
+ 
+ static int wait_reserve_ticket(struct btrfs_fs_info *fs_info,
+ 			       struct btrfs_space_info *space_info,
+ 			       struct reserve_ticket *ticket, u64 orig_bytes)
+ 
+ {
+ 	DEFINE_WAIT(wait);
+ 	int ret = 0;
+ 
+ 	spin_lock(&space_info->lock);
+ 	while (ticket->bytes > 0 && ticket->error == 0) {
+ 		ret = prepare_to_wait_event(&ticket->wait, &wait, TASK_KILLABLE);
+ 		if (ret) {
+ 			ret = -EINTR;
+ 			break;
+ 		}
+ 		spin_unlock(&space_info->lock);
+ 
+ 		schedule();
+ 
+ 		finish_wait(&ticket->wait, &wait);
+ 		spin_lock(&space_info->lock);
+ 	}
+ 	if (!ret)
+ 		ret = ticket->error;
+ 	if (!list_empty(&ticket->list))
+ 		list_del_init(&ticket->list);
+ 	if (ticket->bytes && ticket->bytes < orig_bytes) {
+ 		u64 num_bytes = orig_bytes - ticket->bytes;
+ 		space_info->bytes_may_use -= num_bytes;
+ 		trace_btrfs_space_reservation(fs_info, "space_info",
+ 					      space_info->flags, num_bytes, 0);
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * reserve_metadata_bytes - try to reserve bytes from the block_rsv's space
+  * @root - the root we're allocating for
+  * @space_info - the space info we want to allocate from
+  * @orig_bytes - the number of bytes we want
+  * @flush - whether or not we can flush to make our reservation
+  *
+  * This will reserve orig_bytes number of bytes from the space info associated
+  * with the block_rsv.  If there is not enough space it will make an attempt to
+  * flush out space to make room.  It will do this by flushing delalloc if
+  * possible or committing the transaction.  If flush is 0 then no attempts to
+  * regain reservations will be made and this will fail if there is not enough
+  * space already.
+  */
+ static int __reserve_metadata_bytes(struct btrfs_root *root,
+ 				    struct btrfs_space_info *space_info,
+ 				    u64 orig_bytes,
+ 				    enum btrfs_reserve_flush_enum flush)
+ {
+ 	struct reserve_ticket ticket;
+ 	u64 used;
+ 	int ret = 0;
+ 
+ 	ASSERT(orig_bytes);
+ 	ASSERT(!current->journal_info || flush != BTRFS_RESERVE_FLUSH_ALL);
+ 
+ 	spin_lock(&space_info->lock);
+ 	ret = -ENOSPC;
+ 	used = space_info->bytes_used + space_info->bytes_reserved +
+ 		space_info->bytes_pinned + space_info->bytes_readonly +
+ 		space_info->bytes_may_use;
+ 
+ 	/*
+ 	 * If we have enough space then hooray, make our reservation and carry
+ 	 * on.  If not see if we can overcommit, and if we can, hooray carry on.
+ 	 * If not things get more complicated.
+ 	 */
+ 	if (used + orig_bytes <= space_info->total_bytes) {
+ 		space_info->bytes_may_use += orig_bytes;
+ 		trace_btrfs_space_reservation(root->fs_info, "space_info",
+ 					      space_info->flags, orig_bytes,
+ 					      1);
+ 		ret = 0;
+ 	} else if (can_overcommit(root, space_info, orig_bytes, flush)) {
+ 		space_info->bytes_may_use += orig_bytes;
+ 		trace_btrfs_space_reservation(root->fs_info, "space_info",
+ 					      space_info->flags, orig_bytes,
+ 					      1);
+ 		ret = 0;
+ 	}
+ 
+ 	/*
+ 	 * If we couldn't make a reservation then setup our reservation ticket
+ 	 * and kick the async worker if it's not already running.
+ 	 *
+ 	 * If we are a priority flusher then we just need to add our ticket to
+ 	 * the list and we will do our own flushing further down.
+ 	 */
+ 	if (ret && flush != BTRFS_RESERVE_NO_FLUSH) {
+ 		ticket.bytes = orig_bytes;
+ 		ticket.error = 0;
+ 		init_waitqueue_head(&ticket.wait);
+ 		if (flush == BTRFS_RESERVE_FLUSH_ALL) {
+ 			list_add_tail(&ticket.list, &space_info->tickets);
+ 			if (!space_info->flush) {
+ 				space_info->flush = 1;
+ 				trace_btrfs_trigger_flush(root->fs_info,
+ 							  space_info->flags,
+ 							  orig_bytes, flush,
+ 							  "enospc");
+ 				queue_work(system_unbound_wq,
+ 					   &root->fs_info->async_reclaim_work);
+ 			}
+ 		} else {
+ 			list_add_tail(&ticket.list,
+ 				      &space_info->priority_tickets);
+ 		}
+ 	} else if (!ret && space_info->flags & BTRFS_BLOCK_GROUP_METADATA) {
+ 		used += orig_bytes;
+ 		/*
+ 		 * We will do the space reservation dance during log replay,
+ 		 * which means we won't have fs_info->fs_root set, so don't do
+ 		 * the async reclaim as we will panic.
+ 		 */
+ 		if (!root->fs_info->log_root_recovering &&
+ 		    need_do_async_reclaim(space_info, root, used) &&
+ 		    !work_busy(&root->fs_info->async_reclaim_work)) {
+ 			trace_btrfs_trigger_flush(root->fs_info,
+ 						  space_info->flags,
+ 						  orig_bytes, flush,
+ 						  "preempt");
+ 			queue_work(system_unbound_wq,
+ 				   &root->fs_info->async_reclaim_work);
+ 		}
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 	if (!ret || flush == BTRFS_RESERVE_NO_FLUSH)
+ 		return ret;
+ 
+ 	if (flush == BTRFS_RESERVE_FLUSH_ALL)
+ 		return wait_reserve_ticket(root->fs_info, space_info, &ticket,
+ 					   orig_bytes);
+ 
+ 	ret = 0;
+ 	priority_reclaim_metadata_space(root->fs_info, space_info, &ticket);
+ 	spin_lock(&space_info->lock);
+ 	if (ticket.bytes) {
+ 		if (ticket.bytes < orig_bytes) {
+ 			u64 num_bytes = orig_bytes - ticket.bytes;
+ 			space_info->bytes_may_use -= num_bytes;
+ 			trace_btrfs_space_reservation(root->fs_info,
+ 					"space_info", space_info->flags,
+ 					num_bytes, 0);
+ 
+ 		}
+ 		list_del_init(&ticket.list);
+ 		ret = -ENOSPC;
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 	ASSERT(list_empty(&ticket.list));
+ 	return ret;
+ }
+ 
++>>>>>>> 8ca17f0f5952 (Btrfs: use FLUSH_LIMIT for relocation in reserve_metadata_bytes)
  /**
   * reserve_metadata_bytes - try to reserve bytes from the block_rsv's space
   * @root - the root we're allocating for
diff --cc fs/btrfs/relocation.c
index a505fca100fc,fc067b07e31f..000000000000
--- a/fs/btrfs/relocation.c
+++ b/fs/btrfs/relocation.c
@@@ -2604,25 -2604,28 +2604,47 @@@ static int reserve_metadata_space(struc
  
  	trans->block_rsv = rc->block_rsv;
  	rc->reserved_bytes += num_bytes;
+ 
+ 	/*
+ 	 * We are under a transaction here so we can only do limited flushing.
+ 	 * If we get an enospc just kick back -EAGAIN so we know to drop the
+ 	 * transaction and try to refill when we can flush all the things.
+ 	 */
  	ret = btrfs_block_rsv_refill(root, rc->block_rsv, num_bytes,
- 				BTRFS_RESERVE_FLUSH_ALL);
+ 				BTRFS_RESERVE_FLUSH_LIMIT);
  	if (ret) {
++<<<<<<< HEAD
 +		if (ret == -EAGAIN) {
 +			tmp = rc->extent_root->nodesize *
 +				RELOCATION_RESERVED_NODES;
 +			while (tmp <= rc->reserved_bytes)
 +				tmp <<= 1;
 +			/*
 +			 * only one thread can access block_rsv at this point,
 +			 * so we don't need hold lock to protect block_rsv.
 +			 * we expand more reservation size here to allow enough
 +			 * space for relocation and we will return eailer in
 +			 * enospc case.
 +			 */
 +			rc->block_rsv->size = tmp + rc->extent_root->nodesize *
 +					      RELOCATION_RESERVED_NODES;
 +		}
 +		return ret;
++=======
+ 		tmp = rc->extent_root->nodesize * RELOCATION_RESERVED_NODES;
+ 		while (tmp <= rc->reserved_bytes)
+ 			tmp <<= 1;
+ 		/*
+ 		 * only one thread can access block_rsv at this point,
+ 		 * so we don't need hold lock to protect block_rsv.
+ 		 * we expand more reservation size here to allow enough
+ 		 * space for relocation and we will return eailer in
+ 		 * enospc case.
+ 		 */
+ 		rc->block_rsv->size = tmp + rc->extent_root->nodesize *
+ 			RELOCATION_RESERVED_NODES;
+ 		return -EAGAIN;
++>>>>>>> 8ca17f0f5952 (Btrfs: use FLUSH_LIMIT for relocation in reserve_metadata_bytes)
  	}
  
  	return 0;
* Unmerged path fs/btrfs/extent-tree.c
* Unmerged path fs/btrfs/relocation.c
