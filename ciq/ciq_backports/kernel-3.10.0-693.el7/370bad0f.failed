net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode (Jonathan Toppins) [1383217]
Rebuild_FUZZ: 97.22%
commit-author Or Gerlitz <ogerlitz@mellanox.com>
commit 370bad0f9a5261da0ef0bc76705f5b0b8af148ab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/370bad0f.failed

Switchdev driver net-device port statistics should follow the model introduced
in commit a5ea31f57309 'Merge branch net-offloaded-stats'.

For VF reps we return the SRIOV eswitch vport stats as the usual ones and SW stats
if asked. For the PF, if we're in the switchdev mode, we return the uplink stats
and SW stats if asked, otherwise as before. The uplink stats are implemented using
the PPCNT 802_3 counters which are already being read/cached by the driver.

	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 370bad0f9a5261da0ef0bc76705f5b0b8af148ab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index f49539a1ae6e,ebf5dbc85bff..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -648,6 -827,80 +648,57 @@@ extern const struct dcbnl_rtnl_ops mlx5
  int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
  #endif
  
 -#ifndef CONFIG_RFS_ACCEL
 -static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
 -{
 -	return 0;
 -}
 -
 -static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
 -
 -static inline int mlx5e_arfs_enable(struct mlx5e_priv *priv)
 -{
 -	return -ENOTSUPP;
 -}
 -
 -static inline int mlx5e_arfs_disable(struct mlx5e_priv *priv)
 -{
 -	return -ENOTSUPP;
 -}
 -#else
 -int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
 -void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
 -int mlx5e_arfs_enable(struct mlx5e_priv *priv);
 -int mlx5e_arfs_disable(struct mlx5e_priv *priv);
 -int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 -			u16 rxq_index, u32 flow_id);
 -#endif
 -
  u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
- 
++<<<<<<< HEAD
++=======
+ int mlx5e_create_tir(struct mlx5_core_dev *mdev,
+ 		     struct mlx5e_tir *tir, u32 *in, int inlen);
+ void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
+ 		       struct mlx5e_tir *tir);
+ int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
+ void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
+ int mlx5e_refresh_tirs_self_loopback_enable(struct mlx5_core_dev *mdev);
+ 
+ struct mlx5_eswitch_rep;
+ int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,
+ 			 struct mlx5_eswitch_rep *rep);
+ void mlx5e_vport_rep_unload(struct mlx5_eswitch *esw,
+ 			    struct mlx5_eswitch_rep *rep);
+ int mlx5e_nic_rep_load(struct mlx5_eswitch *esw, struct mlx5_eswitch_rep *rep);
+ void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
+ 			  struct mlx5_eswitch_rep *rep);
+ int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
+ void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
+ int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
+ void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
+ void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
+ 
+ int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
+ void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
+ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
+ void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
+ int mlx5e_create_tises(struct mlx5e_priv *priv);
+ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
+ int mlx5e_close(struct net_device *netdev);
+ int mlx5e_open(struct net_device *netdev);
+ void mlx5e_update_stats_work(struct work_struct *work);
+ struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
+ 				       const struct mlx5e_profile *profile,
+ 				       void *ppriv);
+ void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
+ int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
+ void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
+ u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
+ void mlx5e_add_vxlan_port(struct net_device *netdev,
+ 			  struct udp_tunnel_info *ti);
+ void mlx5e_del_vxlan_port(struct net_device *netdev,
+ 			  struct udp_tunnel_info *ti);
++>>>>>>> 370bad0f9a52 (net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode)
+ 
+ int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
+ 			    void *sp);
+ bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
+ 
+ bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
+ bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv);
  #endif /* __MLX5_EN_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index ae009c7fda21,8e8d809bf3fd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -296,6 -369,107 +296,110 @@@ static void mlx5e_disable_async_events(
  #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq, struct mlx5e_sq *sq,
+ 				       struct mlx5e_umr_wqe *wqe, u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u32 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq, ix);
+ 
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
+ 				     struct mlx5e_channel *c)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
+ 	int i;
+ 
+ 	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				      GFP_KERNEL, cpu_to_node(c->cpu));
+ 	if (!rq->mpwqe.info)
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
+ 					cpu_to_node(c->cpu));
+ 	if (unlikely(!rq->mpwqe.mtt_no_align))
+ 		goto err_free_wqe_info;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
+ 					MLX5_UMR_ALIGN);
+ 		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
+ 						  PCI_DMA_TODEVICE);
+ 		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
+ 			goto err_unmap_mtts;
+ 
+ 		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap_mtts:
+ 	while (--i >= 0) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ err_free_wqe_info:
+ 	kfree(rq->mpwqe.info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ 	kfree(rq->mpwqe.info);
+ }
+ 
++>>>>>>> 370bad0f9a52 (net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode)
  static int mlx5e_create_rq(struct mlx5e_channel *c,
  			   struct mlx5e_rq_param *param,
  			   struct mlx5e_rq *rq)
@@@ -2628,6 -3279,16 +2739,16 @@@ static const struct net_device_ops mlx5
  	.ndo_get_vf_config       = mlx5e_get_vf_config,
  	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
  	.ndo_get_vf_stats        = mlx5e_get_vf_stats,
++<<<<<<< HEAD
++=======
+ 	.ndo_tx_timeout          = mlx5e_tx_timeout,
+ 	.ndo_xdp		 = mlx5e_xdp,
+ #ifdef CONFIG_NET_POLL_CONTROLLER
+ 	.ndo_poll_controller     = mlx5e_netpoll,
+ #endif
+ 	.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
+ 	.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
++>>>>>>> 370bad0f9a52 (net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode)
  };
  
  static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
index fe1433b95149,f202f872f57f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
@@@ -369,6 -406,8 +369,11 @@@ struct mlx5e_stats 
  	struct mlx5e_qcounter_stats qcnt;
  	struct mlx5e_vport_stats vport;
  	struct mlx5e_pport_stats pport;
++<<<<<<< HEAD
++=======
+ 	struct mlx5e_pcie_stats pcie;
+ 	struct rtnl_link_stats64 vf_vport;
++>>>>>>> 370bad0f9a52 (net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode)
  };
  
  static const struct counter_desc mlx5e_pme_status_desc[] = {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
