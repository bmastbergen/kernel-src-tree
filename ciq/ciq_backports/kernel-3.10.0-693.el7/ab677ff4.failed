cxgb4: Allocate Tx queues dynamically

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit ab677ff4ad15bc26c359490ee201557f3a6d20df
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ab677ff4.failed

Allocate resources dynamically for Upper layer driver's (ULD) like
cxgbit, iw_cxgb4, cxgb4i and chcr. The resources allocated include Tx
queues which are allocated when ULD register with cxgb4 driver and freed
while un-registering. The Tx queues which are shared by ULD shall be
allocated by first registering driver and un-allocated by last
unregistering driver.

	Signed-off-by: Atul Gupta <atul.gupta@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ab677ff4ad15bc26c359490ee201557f3a6d20df)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_core.c
#	drivers/infiniband/hw/cxgb4/device.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
#	drivers/net/ethernet/chelsio/cxgb4/sge.c
#	drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
#	drivers/target/iscsi/cxgbit/cxgbit_main.c
diff --cc drivers/crypto/chelsio/chcr_core.c
index 2f6156b672ce,4d7f6700fd7e..000000000000
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@@ -39,12 -39,11 +39,17 @@@ static chcr_handler_func work_handlers[
  	[CPL_FW6_PLD] = cpl_fw6_pld_handler,
  };
  
 -static struct cxgb4_uld_info chcr_uld_info = {
 +static struct cxgb4_pci_uld_info chcr_uld_info = {
  	.name = DRV_MODULE_NAME,
++<<<<<<< HEAD
 +	.nrxq = 4,
++=======
+ 	.nrxq = MAX_ULD_QSETS,
+ 	.ntxq = MAX_ULD_QSETS,
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	.rxq_size = 1024,
 +	.nciq = 0,
 +	.ciq_size = 0,
  	.add = chcr_uld_add,
  	.state_change = chcr_uld_state_change,
  	.rx_handler = chcr_uld_rx_handler,
diff --cc drivers/infiniband/hw/cxgb4/device.c
index eccc412109b6,4e5baf4fe15e..000000000000
--- a/drivers/infiniband/hw/cxgb4/device.c
+++ b/drivers/infiniband/hw/cxgb4/device.c
@@@ -1490,6 -1480,11 +1490,14 @@@ static int c4iw_uld_control(void *handl
  
  static struct cxgb4_uld_info c4iw_uld_info = {
  	.name = DRV_NAME,
++<<<<<<< HEAD
++=======
+ 	.nrxq = MAX_ULD_QSETS,
+ 	.ntxq = MAX_ULD_QSETS,
+ 	.rxq_size = 511,
+ 	.ciq = true,
+ 	.lro = false,
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	.add = c4iw_uld_add,
  	.rx_handler = c4iw_uld_rx_handler,
  	.state_change = c4iw_uld_state_change,
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index ecb195d07c07,0bce1bf9ca0f..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@@ -674,17 -684,30 +675,40 @@@ struct sge_ctrl_txq {               /* 
  	u8 full;                    /* the Tx ring is full */
  } ____cacheline_aligned_in_smp;
  
++<<<<<<< HEAD
++=======
+ struct sge_uld_rxq_info {
+ 	char name[IFNAMSIZ];	/* name of ULD driver */
+ 	struct sge_ofld_rxq *uldrxq; /* Rxq's for ULD */
+ 	u16 *msix_tbl;		/* msix_tbl for uld */
+ 	u16 *rspq_id;		/* response queue id's of rxq */
+ 	u16 nrxq;		/* # of ingress uld queues */
+ 	u16 nciq;		/* # of completion queues */
+ 	u8 uld;			/* uld type */
+ };
+ 
+ struct sge_uld_txq_info {
+ 	struct sge_uld_txq *uldtxq; /* Txq's for ULD */
+ 	atomic_t users;		/* num users */
+ 	u16 ntxq;		/* # of egress uld queues */
+ };
+ 
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  struct sge {
  	struct sge_eth_txq ethtxq[MAX_ETH_QSETS];
- 	struct sge_ofld_txq ofldtxq[MAX_OFLD_QSETS];
  	struct sge_ctrl_txq ctrlq[MAX_CTRL_QUEUES];
  
  	struct sge_eth_rxq ethrxq[MAX_ETH_QSETS];
 +	struct sge_ofld_rxq iscsirxq[MAX_OFLD_QSETS];
 +	struct sge_ofld_rxq iscsitrxq[MAX_ISCSIT_QUEUES];
 +	struct sge_ofld_rxq rdmarxq[MAX_RDMA_QUEUES];
 +	struct sge_ofld_rxq rdmaciq[MAX_RDMA_CIQS];
  	struct sge_rspq fw_evtq ____cacheline_aligned_in_smp;
++<<<<<<< HEAD
++=======
+ 	struct sge_uld_rxq_info **uld_rxq_info;
+ 	struct sge_uld_txq_info **uld_txq_info;
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  
  	struct sge_rspq intrq ____cacheline_aligned_in_smp;
  	spinlock_t intrq_lock;
@@@ -1173,8 -1303,11 +1197,16 @@@ int t4_sge_alloc_eth_txq(struct adapte
  int t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,
  			  struct net_device *dev, unsigned int iqid,
  			  unsigned int cmplqid);
++<<<<<<< HEAD
 +int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,
 +			  struct net_device *dev, unsigned int iqid);
++=======
+ int t4_sge_mod_ctrl_txq(struct adapter *adap, unsigned int eqid,
+ 			unsigned int cmplqid);
+ int t4_sge_alloc_uld_txq(struct adapter *adap, struct sge_uld_txq *txq,
+ 			 struct net_device *dev, unsigned int iqid,
+ 			 unsigned int uld_type);
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  irqreturn_t t4_sge_intr_msix(int irq, void *cookie);
  int t4_sge_init(struct adapter *adap);
  void t4_sge_start(struct adapter *adap);
@@@ -1518,5 -1664,12 +1550,16 @@@ void t4_idma_monitor(struct adapter *ad
  		     int hz, int ticks);
  int t4_set_vf_mac_acl(struct adapter *adapter, unsigned int vf,
  		      unsigned int naddr, u8 *addr);
- 
++<<<<<<< HEAD
++
++=======
+ void t4_uld_mem_free(struct adapter *adap);
+ int t4_uld_mem_alloc(struct adapter *adap);
+ void t4_uld_clean_up(struct adapter *adap);
+ void t4_register_netevent_notifier(void);
+ void free_rspq_fl(struct adapter *adap, struct sge_rspq *rq, struct sge_fl *fl);
+ void free_tx_desc(struct adapter *adap, struct sge_txq *q,
+ 		  unsigned int n, bool unmap);
+ void free_txq(struct adapter *adap, struct sge_txq *q);
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  #endif /* __CXGB4_H__ */
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
index 7b95a4741ec5,acc231293e4d..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
@@@ -2518,120 -2512,6 +2518,123 @@@ do { 
  		RL("FLLow:", fl.low);
  		RL("FLStarving:", fl.starving);
  
++<<<<<<< HEAD
 +	} else if (iscsi_idx < iscsi_entries) {
 +		const struct sge_ofld_rxq *rx =
 +			&adap->sge.iscsirxq[iscsi_idx * 4];
 +		const struct sge_ofld_txq *tx =
 +			&adap->sge.ofldtxq[iscsi_idx * 4];
 +		int n = min(4, adap->sge.iscsiqsets - 4 * iscsi_idx);
 +
 +		S("QType:", "iSCSI");
 +		T("TxQ ID:", q.cntxt_id);
 +		T("TxQ size:", q.size);
 +		T("TxQ inuse:", q.in_use);
 +		T("TxQ CIDX:", q.cidx);
 +		T("TxQ PIDX:", q.pidx);
 +		R("RspQ ID:", rspq.abs_id);
 +		R("RspQ size:", rspq.size);
 +		R("RspQE size:", rspq.iqe_len);
 +		R("RspQ CIDX:", rspq.cidx);
 +		R("RspQ Gen:", rspq.gen);
 +		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
 +		S3("u", "Intr pktcnt:",
 +		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
 +		R("FL ID:", fl.cntxt_id);
 +		R("FL size:", fl.size - 8);
 +		R("FL pend:", fl.pend_cred);
 +		R("FL avail:", fl.avail);
 +		R("FL PIDX:", fl.pidx);
 +		R("FL CIDX:", fl.cidx);
 +		RL("RxPackets:", stats.pkts);
 +		RL("RxImmPkts:", stats.imm);
 +		RL("RxNoMem:", stats.nomem);
 +		RL("FLAllocErr:", fl.alloc_failed);
 +		RL("FLLrgAlcErr:", fl.large_alloc_failed);
 +		RL("FLMapErr:", fl.mapping_err);
 +		RL("FLLow:", fl.low);
 +		RL("FLStarving:", fl.starving);
 +
 +	} else if (iscsit_idx < iscsit_entries) {
 +		const struct sge_ofld_rxq *rx =
 +			&adap->sge.iscsitrxq[iscsit_idx * 4];
 +		int n = min(4, adap->sge.niscsitq - 4 * iscsit_idx);
 +
 +		S("QType:", "iSCSIT");
 +		R("RspQ ID:", rspq.abs_id);
 +		R("RspQ size:", rspq.size);
 +		R("RspQE size:", rspq.iqe_len);
 +		R("RspQ CIDX:", rspq.cidx);
 +		R("RspQ Gen:", rspq.gen);
 +		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
 +		S3("u", "Intr pktcnt:",
 +		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
 +		R("FL ID:", fl.cntxt_id);
 +		R("FL size:", fl.size - 8);
 +		R("FL pend:", fl.pend_cred);
 +		R("FL avail:", fl.avail);
 +		R("FL PIDX:", fl.pidx);
 +		R("FL CIDX:", fl.cidx);
 +		RL("RxPackets:", stats.pkts);
 +		RL("RxImmPkts:", stats.imm);
 +		RL("RxNoMem:", stats.nomem);
 +		RL("FLAllocErr:", fl.alloc_failed);
 +		RL("FLLrgAlcErr:", fl.large_alloc_failed);
 +		RL("FLMapErr:", fl.mapping_err);
 +		RL("FLLow:", fl.low);
 +		RL("FLStarving:", fl.starving);
 +
 +	} else if (rdma_idx < rdma_entries) {
 +		const struct sge_ofld_rxq *rx =
 +				&adap->sge.rdmarxq[rdma_idx * 4];
 +		int n = min(4, adap->sge.rdmaqs - 4 * rdma_idx);
 +
 +		S("QType:", "RDMA-CPL");
 +		S("Interface:",
 +		  rx[i].rspq.netdev ? rx[i].rspq.netdev->name : "N/A");
 +		R("RspQ ID:", rspq.abs_id);
 +		R("RspQ size:", rspq.size);
 +		R("RspQE size:", rspq.iqe_len);
 +		R("RspQ CIDX:", rspq.cidx);
 +		R("RspQ Gen:", rspq.gen);
 +		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
 +		S3("u", "Intr pktcnt:",
 +		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
 +		R("FL ID:", fl.cntxt_id);
 +		R("FL size:", fl.size - 8);
 +		R("FL pend:", fl.pend_cred);
 +		R("FL avail:", fl.avail);
 +		R("FL PIDX:", fl.pidx);
 +		R("FL CIDX:", fl.cidx);
 +		RL("RxPackets:", stats.pkts);
 +		RL("RxImmPkts:", stats.imm);
 +		RL("RxNoMem:", stats.nomem);
 +		RL("FLAllocErr:", fl.alloc_failed);
 +		RL("FLLrgAlcErr:", fl.large_alloc_failed);
 +		RL("FLMapErr:", fl.mapping_err);
 +		RL("FLLow:", fl.low);
 +		RL("FLStarving:", fl.starving);
 +
 +	} else if (ciq_idx < ciq_entries) {
 +		const struct sge_ofld_rxq *rx = &adap->sge.rdmaciq[ciq_idx * 4];
 +		int n = min(4, adap->sge.rdmaciqs - 4 * ciq_idx);
 +
 +		S("QType:", "RDMA-CIQ");
 +		S("Interface:",
 +		  rx[i].rspq.netdev ? rx[i].rspq.netdev->name : "N/A");
 +		R("RspQ ID:", rspq.abs_id);
 +		R("RspQ size:", rspq.size);
 +		R("RspQE size:", rspq.iqe_len);
 +		R("RspQ CIDX:", rspq.cidx);
 +		R("RspQ Gen:", rspq.gen);
 +		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
 +		S3("u", "Intr pktcnt:",
 +		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
 +		RL("RxAN:", stats.an);
 +		RL("RxNoMem:", stats.nomem);
 +
++=======
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	} else if (ctrl_idx < ctrl_entries) {
  		const struct sge_ctrl_txq *tx = &adap->sge.ctrlq[ctrl_idx * 4];
  		int n = min(4, adap->params.nports - 4 * ctrl_idx);
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index 8f2bd7d85ee8,449884f8dd67..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@@ -1132,39 -885,15 +1132,42 @@@ freeout:	t4_free_sge_resources(adap)
  		}
  	}
  
++<<<<<<< HEAD
 +	j = s->iscsiqsets / adap->params.nports; /* iscsi queues per channel */
 +	for_each_iscsirxq(s, i) {
 +		err = t4_sge_alloc_ofld_txq(adap, &s->ofldtxq[i],
 +					    adap->port[i / j],
 +					    s->fw_evtq.cntxt_id);
 +		if (err)
 +			goto freeout;
 +	}
 +
 +#define ALLOC_OFLD_RXQS(firstq, nq, per_chan, ids, lro) do { \
 +	err = alloc_ofld_rxqs(adap, firstq, nq, per_chan, msi_idx, ids, lro); \
 +	if (err) \
 +		goto freeout; \
 +	if (msi_idx > 0) \
 +		msi_idx += nq; \
 +} while (0)
 +
 +	ALLOC_OFLD_RXQS(s->iscsirxq, s->iscsiqsets, j, s->iscsi_rxq, false);
 +	ALLOC_OFLD_RXQS(s->iscsitrxq, s->niscsitq, j, s->iscsit_rxq, true);
 +	ALLOC_OFLD_RXQS(s->rdmarxq, s->rdmaqs, 1, s->rdma_rxq, false);
 +	j = s->rdmaciqs / adap->params.nports; /* rdmaq queues per channel */
 +	ALLOC_OFLD_RXQS(s->rdmaciq, s->rdmaciqs, j, s->rdma_ciq, false);
 +
 +#undef ALLOC_OFLD_RXQS
 +
++=======
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	for_each_port(adap, i) {
 -		/* Note that cmplqid below is 0 if we don't
 +		/*
 +		 * Note that ->rdmarxq[i].rspq.cntxt_id below is 0 if we don't
  		 * have RDMA queues, and that's the right value.
  		 */
 -		if (rxq_info)
 -			cmplqid	= rxq_info->uldrxq[i].rspq.cntxt_id;
 -
  		err = t4_sge_alloc_ctrl_txq(adap, &s->ctrlq[i], adap->port[i],
 -					    s->fw_evtq.cntxt_id, cmplqid);
 +					    s->fw_evtq.cntxt_id,
 +					    s->rdmarxq[i].rspq.cntxt_id);
  		if (err)
  			goto freeout;
  	}
@@@ -2313,8 -1913,18 +2316,23 @@@ static void disable_dbs(struct adapter 
  
  	for_each_ethrxq(&adap->sge, i)
  		disable_txq_db(&adap->sge.ethtxq[i].q);
++<<<<<<< HEAD
 +	for_each_iscsirxq(&adap->sge, i)
 +		disable_txq_db(&adap->sge.ofldtxq[i].q);
++=======
+ 	if (is_offload(adap)) {
+ 		struct sge_uld_txq_info *txq_info =
+ 			adap->sge.uld_txq_info[CXGB4_TX_OFLD];
+ 
+ 		if (txq_info) {
+ 			for_each_ofldtxq(&adap->sge, i) {
+ 				struct sge_uld_txq *txq = &txq_info->uldtxq[i];
+ 
+ 				disable_txq_db(&txq->q);
+ 			}
+ 		}
+ 	}
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	for_each_port(adap, i)
  		disable_txq_db(&adap->sge.ctrlq[i].q);
  }
@@@ -2325,8 -1935,18 +2343,23 @@@ static void enable_dbs(struct adapter *
  
  	for_each_ethrxq(&adap->sge, i)
  		enable_txq_db(adap, &adap->sge.ethtxq[i].q);
++<<<<<<< HEAD
 +	for_each_iscsirxq(&adap->sge, i)
 +		enable_txq_db(adap, &adap->sge.ofldtxq[i].q);
++=======
+ 	if (is_offload(adap)) {
+ 		struct sge_uld_txq_info *txq_info =
+ 			adap->sge.uld_txq_info[CXGB4_TX_OFLD];
+ 
+ 		if (txq_info) {
+ 			for_each_ofldtxq(&adap->sge, i) {
+ 				struct sge_uld_txq *txq = &txq_info->uldtxq[i];
+ 
+ 				enable_txq_db(adap, &txq->q);
+ 			}
+ 		}
+ 	}
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	for_each_port(adap, i)
  		enable_txq_db(adap, &adap->sge.ctrlq[i].q);
  }
@@@ -2395,8 -2017,17 +2428,22 @@@ static void recover_all_queues(struct a
  
  	for_each_ethrxq(&adap->sge, i)
  		sync_txq_pidx(adap, &adap->sge.ethtxq[i].q);
++<<<<<<< HEAD
 +	for_each_iscsirxq(&adap->sge, i)
 +		sync_txq_pidx(adap, &adap->sge.ofldtxq[i].q);
++=======
+ 	if (is_offload(adap)) {
+ 		struct sge_uld_txq_info *txq_info =
+ 			adap->sge.uld_txq_info[CXGB4_TX_OFLD];
+ 		if (txq_info) {
+ 			for_each_ofldtxq(&adap->sge, i) {
+ 				struct sge_uld_txq *txq = &txq_info->uldtxq[i];
+ 
+ 				sync_txq_pidx(adap, &txq->q);
+ 			}
+ 		}
+ 	}
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	for_each_port(adap, i)
  		sync_txq_pidx(adap, &adap->sge.ctrlq[i].q);
  }
@@@ -4389,11 -4018,15 +4436,10 @@@ static void cfg_queues(struct adapter *
  
  	/* Reduce memory usage in kdump environment, disable all offload.
  	 */
 -	if (is_kdump_kernel()) {
 -		adap->params.offload = 0;
 -		adap->params.crypto = 0;
 -	} else if (is_uld(adap) && t4_uld_mem_alloc(adap)) {
 +	if (is_kdump_kernel())
  		adap->params.offload = 0;
 -		adap->params.crypto = 0;
 -	}
  
- 	for_each_port(adap, i)
- 		n10g += is_x_10g_port(&adap2pinfo(adap, i)->link_cfg);
+ 	n10g += is_x_10g_port(&adap2pinfo(adap, i)->link_cfg);
  #ifdef CONFIG_CHELSIO_T4_DCB
  	/* For Data Center Bridging support we need to be able to support up
  	 * to 8 Traffic Priorities; each of which will be assigned to its
@@@ -4476,50 -4094,8 +4522,53 @@@
  	for (i = 0; i < ARRAY_SIZE(s->ctrlq); i++)
  		s->ctrlq[i].q.size = 512;
  
++<<<<<<< HEAD
 +	for (i = 0; i < ARRAY_SIZE(s->ofldtxq); i++)
 +		s->ofldtxq[i].q.size = 1024;
 +
 +	for (i = 0; i < ARRAY_SIZE(s->iscsirxq); i++) {
 +		struct sge_ofld_rxq *r = &s->iscsirxq[i];
 +
 +		init_rspq(adap, &r->rspq, 5, 1, 1024, 64);
 +		r->rspq.uld = CXGB4_ULD_ISCSI;
 +		r->fl.size = 72;
 +	}
 +
 +	if (!is_t4(adap->params.chip)) {
 +		for (i = 0; i < ARRAY_SIZE(s->iscsitrxq); i++) {
 +			struct sge_ofld_rxq *r = &s->iscsitrxq[i];
 +
 +			init_rspq(adap, &r->rspq, 5, 1, 1024, 64);
 +			r->rspq.uld = CXGB4_ULD_ISCSIT;
 +			r->fl.size = 72;
 +		}
 +	}
 +
 +	for (i = 0; i < ARRAY_SIZE(s->rdmarxq); i++) {
 +		struct sge_ofld_rxq *r = &s->rdmarxq[i];
 +
 +		init_rspq(adap, &r->rspq, 5, 1, 511, 64);
 +		r->rspq.uld = CXGB4_ULD_RDMA;
 +		r->fl.size = 72;
 +	}
 +
 +	ciq_size = 64 + adap->vres.cq.size + adap->tids.nftids;
 +	if (ciq_size > SGE_MAX_IQ_SIZE) {
 +		CH_WARN(adap, "CIQ size too small for available IQs\n");
 +		ciq_size = SGE_MAX_IQ_SIZE;
 +	}
 +
 +	for (i = 0; i < ARRAY_SIZE(s->rdmaciq); i++) {
 +		struct sge_ofld_rxq *r = &s->rdmaciq[i];
 +
 +		init_rspq(adap, &r->rspq, 5, 1, ciq_size, 64);
 +		r->rspq.uld = CXGB4_ULD_RDMA;
 +	}
 +
++=======
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	init_rspq(adap, &s->fw_evtq, 0, 1, 1024, 64);
 -	init_rspq(adap, &s->intrq, 0, 1, 512, 64);
 +	init_rspq(adap, &s->intrq, 0, 1, 2 * MAX_INGQ, 64);
  }
  
  /*
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
index 42e73f7aa0d5,4c856605fdfa..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
@@@ -285,6 -328,12 +300,15 @@@ struct cxgb4_lld_info 
  
  struct cxgb4_uld_info {
  	const char *name;
++<<<<<<< HEAD
++=======
+ 	void *handle;
+ 	unsigned int nrxq;
+ 	unsigned int rxq_size;
+ 	unsigned int ntxq;
+ 	bool ciq;
+ 	bool lro;
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	void *(*add)(const struct cxgb4_lld_info *p);
  	int (*rx_handler)(void *handle, const __be64 *rsp,
  			  const struct pkt_gl *gl);
diff --cc drivers/net/ethernet/chelsio/cxgb4/sge.c
index 66aedc0b4e81,b7d0753b9242..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@@ -2796,8 -2901,21 +2837,26 @@@ int t4_sge_alloc_ctrl_txq(struct adapte
  	return 0;
  }
  
++<<<<<<< HEAD
 +int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,
 +			  struct net_device *dev, unsigned int iqid)
++=======
+ int t4_sge_mod_ctrl_txq(struct adapter *adap, unsigned int eqid,
+ 			unsigned int cmplqid)
+ {
+ 	u32 param, val;
+ 
+ 	param = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |
+ 		 FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL) |
+ 		 FW_PARAMS_PARAM_YZ_V(eqid));
+ 	val = cmplqid;
+ 	return t4_set_params(adap, adap->mbox, adap->pf, 0, 1, &param, &val);
+ }
+ 
+ int t4_sge_alloc_uld_txq(struct adapter *adap, struct sge_uld_txq *txq,
+ 			 struct net_device *dev, unsigned int iqid,
+ 			 unsigned int uld_type)
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  {
  	int ret, nentries;
  	struct fw_eq_ofld_cmd c;
@@@ -2949,27 -3072,6 +3012,30 @@@ void t4_free_sge_resources(struct adapt
  		}
  	}
  
++<<<<<<< HEAD
 +	/* clean up RDMA and iSCSI Rx queues */
 +	t4_free_ofld_rxqs(adap, adap->sge.iscsiqsets, adap->sge.iscsirxq);
 +	t4_free_ofld_rxqs(adap, adap->sge.niscsitq, adap->sge.iscsitrxq);
 +	t4_free_ofld_rxqs(adap, adap->sge.rdmaqs, adap->sge.rdmarxq);
 +	t4_free_ofld_rxqs(adap, adap->sge.rdmaciqs, adap->sge.rdmaciq);
 +
 +	/* clean up offload Tx queues */
 +	for (i = 0; i < ARRAY_SIZE(adap->sge.ofldtxq); i++) {
 +		struct sge_ofld_txq *q = &adap->sge.ofldtxq[i];
 +
 +		if (q->q.desc) {
 +			tasklet_kill(&q->qresume_tsk);
 +			t4_ofld_eq_free(adap, adap->mbox, adap->pf, 0,
 +					q->q.cntxt_id);
 +			free_tx_desc(adap, &q->q, q->q.in_use, false);
 +			kfree(q->q.sdesc);
 +			__skb_queue_purge(&q->sendq);
 +			free_txq(adap, &q->q);
 +		}
 +	}
 +
++=======
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	/* clean up control Tx queues */
  	for (i = 0; i < ARRAY_SIZE(adap->sge.ctrlq); i++) {
  		struct sge_ctrl_txq *cq = &adap->sge.ctrlq[i];
diff --cc drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
index 6343fc89fa89,4655a9f9dcea..000000000000
--- a/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
+++ b/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
@@@ -83,6 -84,10 +83,13 @@@ static inline int send_tx_flowc_wr(stru
  
  static const struct cxgb4_uld_info cxgb4i_uld_info = {
  	.name = DRV_MODULE_NAME,
++<<<<<<< HEAD
++=======
+ 	.nrxq = MAX_ULD_QSETS,
+ 	.ntxq = MAX_ULD_QSETS,
+ 	.rxq_size = 1024,
+ 	.lro = false,
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	.add = t4_uld_add,
  	.rx_handler = t4_uld_rx_handler,
  	.state_change = t4_uld_state_change,
diff --cc drivers/target/iscsi/cxgbit/cxgbit_main.c
index 4301a4586f5b,96eedfc49c94..000000000000
--- a/drivers/target/iscsi/cxgbit/cxgbit_main.c
+++ b/drivers/target/iscsi/cxgbit/cxgbit_main.c
@@@ -681,6 -652,10 +681,13 @@@ static struct iscsit_transport cxgbit_t
  
  static struct cxgb4_uld_info cxgbit_uld_info = {
  	.name		= DRV_NAME,
++<<<<<<< HEAD
++=======
+ 	.nrxq		= MAX_ULD_QSETS,
+ 	.ntxq		= MAX_ULD_QSETS,
+ 	.rxq_size	= 1024,
+ 	.lro		= true,
++>>>>>>> ab677ff4ad15 (cxgb4: Allocate Tx queues dynamically)
  	.add		= cxgbit_uld_add,
  	.state_change	= cxgbit_uld_state_change,
  	.lro_rx_handler = cxgbit_uld_lro_rx_handler,
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
diff --git a/drivers/crypto/chelsio/chcr_algo.c b/drivers/crypto/chelsio/chcr_algo.c
index e4ddb921d7b3..56b153805462 100644
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@ -592,16 +592,18 @@ badkey_err:
 
 static int cxgb4_is_crypto_q_full(struct net_device *dev, unsigned int idx)
 {
-	int ret = 0;
-	struct sge_ofld_txq *q;
 	struct adapter *adap = netdev2adap(dev);
+	struct sge_uld_txq_info *txq_info =
+		adap->sge.uld_txq_info[CXGB4_TX_CRYPTO];
+	struct sge_uld_txq *txq;
+	int ret = 0;
 
 	local_bh_disable();
-	q = &adap->sge.ofldtxq[idx];
-	spin_lock(&q->sendq.lock);
-	if (q->full)
+	txq = &txq_info->uldtxq[idx];
+	spin_lock(&txq->sendq.lock);
+	if (txq->full)
 		ret = -1;
-	spin_unlock(&q->sendq.lock);
+	spin_unlock(&txq->sendq.lock);
 	local_bh_enable();
 	return ret;
 }
@@ -674,11 +676,11 @@ static int chcr_device_init(struct chcr_context *ctx)
 		}
 		u_ctx = ULD_CTX(ctx);
 		rxq_perchan = u_ctx->lldi.nrxq / u_ctx->lldi.nchan;
-		ctx->dev->tx_channel_id = 0;
 		rxq_idx = ctx->dev->tx_channel_id * rxq_perchan;
 		rxq_idx += id % rxq_perchan;
 		spin_lock(&ctx->dev->lock_chcr_dev);
 		ctx->tx_channel_id = rxq_idx;
+		ctx->dev->tx_channel_id = !ctx->dev->tx_channel_id;
 		spin_unlock(&ctx->dev->lock_chcr_dev);
 	}
 out:
* Unmerged path drivers/crypto/chelsio/chcr_core.c
* Unmerged path drivers/infiniband/hw/cxgb4/device.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/sge.c
* Unmerged path drivers/scsi/cxgbi/cxgb4i/cxgb4i.c
* Unmerged path drivers/target/iscsi/cxgbit/cxgbit_main.c
