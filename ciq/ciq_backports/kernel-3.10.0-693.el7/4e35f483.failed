mm, hugetlb: use vma_resv_map() map types

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hugetlb: use vma_resv_map() map types (Andrea Arcangeli) [1430172]
Rebuild_FUZZ: 94.87%
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit 4e35f483850ba46b838adfd312b3052416e15204
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/4e35f483.failed

Util now, we get a resv_map by two ways according to each mapping type.
This makes code dirty and unreadable.  Unify it.

[davidlohr@hp.com: code cleanups]
	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
	Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4e35f483850ba46b838adfd312b3052416e15204)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 756884ed9453,1c7baff65f9d..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1452,48 -1178,34 +1463,59 @@@ static void return_unused_surplus_pages
  static long vma_needs_reservation(struct hstate *h,
  			struct vm_area_struct *vma, unsigned long addr)
  {
- 	struct address_space *mapping = vma->vm_file->f_mapping;
- 	struct inode *inode = mapping->host;
- 
- 	if (vma->vm_flags & VM_MAYSHARE) {
- 		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
- 		struct resv_map *resv = inode->i_mapping->private_data;
- 
- 		return region_chg(resv, idx, idx + 1);
+ 	struct resv_map *resv;
+ 	pgoff_t idx;
+ 	long chg;
  
- 	} else if (!is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
+ 	resv = vma_resv_map(vma);
+ 	if (!resv)
  		return 1;
  
++<<<<<<< HEAD
 +	} else  {
 +		long err;
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *reservations = vma_resv_map(vma);
 +
 +		err = region_chg(reservations, idx, idx + 1);
 +		if (err < 0)
 +			return err;
 +		return 0;
 +	}
++=======
+ 	idx = vma_hugecache_offset(h, vma, addr);
+ 	chg = region_chg(resv, idx, idx + 1);
+ 
+ 	if (vma->vm_flags & VM_MAYSHARE)
+ 		return chg;
+ 	else
+ 		return chg < 0 ? chg : 0;
++>>>>>>> 4e35f483850b (mm, hugetlb: use vma_resv_map() map types)
  }
  static void vma_commit_reservation(struct hstate *h,
  			struct vm_area_struct *vma, unsigned long addr)
  {
- 	struct address_space *mapping = vma->vm_file->f_mapping;
- 	struct inode *inode = mapping->host;
+ 	struct resv_map *resv;
+ 	pgoff_t idx;
  
- 	if (vma->vm_flags & VM_MAYSHARE) {
- 		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
- 		struct resv_map *resv = inode->i_mapping->private_data;
+ 	resv = vma_resv_map(vma);
+ 	if (!resv)
+ 		return;
  
++<<<<<<< HEAD
 +		region_add(resv, idx, idx + 1);
 +
 +	} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *reservations = vma_resv_map(vma);
 +
 +		/* Mark this page used in the map. */
 +		region_add(reservations, idx, idx + 1);
 +	}
++=======
+ 	idx = vma_hugecache_offset(h, vma, addr);
+ 	region_add(resv, idx, idx + 1);
++>>>>>>> 4e35f483850b (mm, hugetlb: use vma_resv_map() map types)
  }
  
  static struct page *alloc_huge_page(struct vm_area_struct *vma,
@@@ -2587,41 -2268,30 +2609,55 @@@ static void hugetlb_vm_op_open(struct v
  	 * after this open call completes.  It is therefore safe to take a
  	 * new reference here without additional locking.
  	 */
++<<<<<<< HEAD
 +	if (reservations)
 +		kref_get(&reservations->refs);
 +}
 +
 +static void resv_map_put(struct vm_area_struct *vma)
 +{
 +	struct resv_map *reservations = vma_resv_map(vma);
 +
 +	if (!reservations)
 +		return;
 +	kref_put(&reservations->refs, resv_map_release);
++=======
+ 	if (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER))
+ 		kref_get(&resv->refs);
++>>>>>>> 4e35f483850b (mm, hugetlb: use vma_resv_map() map types)
  }
  
  static void hugetlb_vm_op_close(struct vm_area_struct *vma)
  {
  	struct hstate *h = hstate_vma(vma);
 -	struct resv_map *resv = vma_resv_map(vma);
 +	struct resv_map *reservations = vma_resv_map(vma);
  	struct hugepage_subpool *spool = subpool_vma(vma);
- 	unsigned long reserve;
- 	unsigned long start;
- 	unsigned long end;
+ 	unsigned long reserve, start, end;
  
++<<<<<<< HEAD
 +	if (reservations) {
 +		start = vma_hugecache_offset(h, vma, vma->vm_start);
 +		end = vma_hugecache_offset(h, vma, vma->vm_end);
 +
 +		reserve = (end - start) -
 +			region_count(reservations, start, end);
 +
 +		resv_map_put(vma);
++=======
+ 	if (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))
+ 		return;
  
- 		if (reserve) {
- 			hugetlb_acct_memory(h, -reserve);
- 			hugepage_subpool_put_pages(spool, reserve);
- 		}
+ 	start = vma_hugecache_offset(h, vma, vma->vm_start);
+ 	end = vma_hugecache_offset(h, vma, vma->vm_end);
+ 
+ 	reserve = (end - start) - region_count(resv, start, end);
++>>>>>>> 4e35f483850b (mm, hugetlb: use vma_resv_map() map types)
+ 
+ 	kref_put(&resv->refs, resv_map_release);
+ 
+ 	if (reserve) {
+ 		hugetlb_acct_memory(h, -reserve);
+ 		hugepage_subpool_put_pages(spool, reserve);
  	}
  }
  
* Unmerged path mm/hugetlb.c
