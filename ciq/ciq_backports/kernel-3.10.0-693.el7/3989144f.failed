kthread: kthread worker API cleanup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [sound] alsa: kthread: kthread worker API cleanup (Jaroslav Kysela) [1399503]
Rebuild_FUZZ: 92.11%
commit-author Petr Mladek <pmladek@suse.com>
commit 3989144f863ac576e6efba298d24b0b02a10d4bb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/3989144f.failed

A good practice is to prefix the names of functions by the name
of the subsystem.

The kthread worker API is a mix of classic kthreads and workqueues.  Each
worker has a dedicated kthread.  It runs a generic function that process
queued works.  It is implemented as part of the kthread subsystem.

This patch renames the existing kthread worker API to use
the corresponding name from the workqueues API prefixed by
kthread_:

__init_kthread_worker()		-> __kthread_init_worker()
init_kthread_worker()		-> kthread_init_worker()
init_kthread_work()		-> kthread_init_work()
insert_kthread_work()		-> kthread_insert_work()
queue_kthread_work()		-> kthread_queue_work()
flush_kthread_work()		-> kthread_flush_work()
flush_kthread_worker()		-> kthread_flush_worker()

Note that the names of DEFINE_KTHREAD_WORK*() macros stay
as they are. It is common that the "DEFINE_" prefix has
precedence over the subsystem names.

Note that INIT() macros and init() functions use different
naming scheme. There is no good solution. There are several
reasons for this solution:

  + "init" in the function names stands for the verb "initialize"
    aka "initialize worker". While "INIT" in the macro names
    stands for the noun "INITIALIZER" aka "worker initializer".

  + INIT() macros are used only in DEFINE() macros

  + init() functions are used close to the other kthread()
    functions. It looks much better if all the functions
    use the same scheme.

  + There will be also kthread_destroy_worker() that will
    be used close to kthread_cancel_work(). It is related
    to the init() function. Again it looks better if all
    functions use the same naming scheme.

  + there are several precedents for such init() function
    names, e.g. amd_iommu_init_device(), free_area_init_node(),
    jump_label_init_type(),  regmap_init_mmio_clk(),

  + It is not an argument but it was inconsistent even before.

[arnd@arndb.de: fix linux-next merge conflict]
 Link: http://lkml.kernel.org/r/20160908135724.1311726-1-arnd@arndb.de
Link: http://lkml.kernel.org/r/1470754545-17632-3-git-send-email-pmladek@suse.com
	Suggested-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Petr Mladek <pmladek@suse.com>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Josh Triplett <josh@joshtriplett.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Arnd Bergmann <arnd@arndb.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3989144f863ac576e6efba298d24b0b02a10d4bb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/i8254.c
#	crypto/crypto_engine.c
#	drivers/block/loop.c
#	drivers/net/ethernet/microchip/encx24j600.c
#	drivers/spi/spi.c
#	drivers/tty/serial/sc16is7xx.c
diff --cc arch/x86/kvm/i8254.c
index df13abcf72e1,16a7134eedac..000000000000
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@@ -236,22 -204,15 +236,32 @@@ static void kvm_pit_ack_irq(struct kvm_
  {
  	struct kvm_kpit_state *ps = container_of(kian, struct kvm_kpit_state,
  						 irq_ack_notifier);
 -	struct kvm_pit *pit = pit_state_to_pit(ps);
 -
 +	int value;
 +
++<<<<<<< HEAD
 +	spin_lock(&ps->inject_lock);
 +	value = atomic_dec_return(&ps->pending);
 +	if (value < 0)
 +		/* spurious acks can be generated if, for example, the
 +		 * PIC is being reset.  Handle it gracefully here
 +		 */
 +		atomic_inc(&ps->pending);
 +	else if (value > 0 && ps->reinject)
 +		/* in this case, we had multiple outstanding pit interrupts
 +		 * that we needed to inject.  Reinject
 +		 */
 +		queue_kthread_work(&ps->pit->worker, &ps->pit->expired);
 +	ps->irq_ack = 1;
 +	spin_unlock(&ps->inject_lock);
++=======
+ 	atomic_set(&ps->irq_ack, 1);
+ 	/* irq_ack should be set before pending is read.  Order accesses with
+ 	 * inc(pending) in pit_timer_fn and xchg(irq_ack, 0) in pit_do_work.
+ 	 */
+ 	smp_mb();
+ 	if (atomic_dec_if_positive(&ps->pending) > 0)
+ 		kthread_queue_work(&pit->worker, &pit->expired);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  }
  
  void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)
@@@ -317,12 -267,12 +327,12 @@@ static void pit_do_work(struct kthread_
  static enum hrtimer_restart pit_timer_fn(struct hrtimer *data)
  {
  	struct kvm_kpit_state *ps = container_of(data, struct kvm_kpit_state, timer);
 -	struct kvm_pit *pt = pit_state_to_pit(ps);
 +	struct kvm_pit *pt = ps->kvm->arch.vpit;
  
 -	if (atomic_read(&ps->reinject))
 +	if (ps->reinject)
  		atomic_inc(&ps->pending);
  
- 	queue_kthread_work(&pt->worker, &pt->expired);
+ 	kthread_queue_work(&pt->worker, &pt->expired);
  
  	if (ps->is_periodic) {
  		hrtimer_add_expires_ns(&ps->timer, ps->period);
@@@ -346,7 -324,7 +356,11 @@@ static void create_pit_timer(struct kv
  
  	/* TODO The new value only affected after the retriggered */
  	hrtimer_cancel(&ps->timer);
++<<<<<<< HEAD
 +	flush_kthread_work(&ps->pit->expired);
++=======
+ 	kthread_flush_work(&pit->expired);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  	ps->period = interval;
  	ps->is_periodic = is_period;
  
@@@ -701,18 -667,14 +715,25 @@@ struct kvm_pit *kvm_create_pit(struct k
  	pid_nr = pid_vnr(pid);
  	put_pid(pid);
  
- 	init_kthread_worker(&pit->worker);
+ 	kthread_init_worker(&pit->worker);
  	pit->worker_task = kthread_run(kthread_worker_fn, &pit->worker,
  				       "kvm-pit/%d", pid_nr);
++<<<<<<< HEAD
 +	if (IS_ERR(pit->worker_task)) {
 +		mutex_unlock(&pit->pit_state.lock);
 +		kvm_free_irq_source_id(kvm, pit->irq_source_id);
 +		kfree(pit);
 +		return NULL;
 +	}
 +	init_kthread_work(&pit->expired, pit_do_work);
++=======
+ 	if (IS_ERR(pit->worker_task))
+ 		goto fail_kthread;
+ 
+ 	kthread_init_work(&pit->expired, pit_do_work);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  
 +	kvm->arch.vpit = pit;
  	pit->kvm = kvm;
  
  	pit_state = &pit->pit_state;
@@@ -760,23 -723,16 +781,35 @@@ fail
  
  void kvm_free_pit(struct kvm *kvm)
  {
 -	struct kvm_pit *pit = kvm->arch.vpit;
 +	struct hrtimer *timer;
  
++<<<<<<< HEAD
 +	if (kvm->arch.vpit) {
 +		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &kvm->arch.vpit->dev);
 +		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS,
 +					      &kvm->arch.vpit->speaker_dev);
 +		kvm_unregister_irq_mask_notifier(kvm, 0,
 +					       &kvm->arch.vpit->mask_notifier);
 +		kvm_unregister_irq_ack_notifier(kvm,
 +				&kvm->arch.vpit->pit_state.irq_ack_notifier);
 +		mutex_lock(&kvm->arch.vpit->pit_state.lock);
 +		timer = &kvm->arch.vpit->pit_state.timer;
 +		hrtimer_cancel(timer);
 +		flush_kthread_work(&kvm->arch.vpit->expired);
 +		kthread_stop(kvm->arch.vpit->worker_task);
 +		kvm_free_irq_source_id(kvm, kvm->arch.vpit->irq_source_id);
 +		mutex_unlock(&kvm->arch.vpit->pit_state.lock);
 +		kfree(kvm->arch.vpit);
++=======
+ 	if (pit) {
+ 		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &pit->dev);
+ 		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &pit->speaker_dev);
+ 		kvm_pit_set_reinject(pit, false);
+ 		hrtimer_cancel(&pit->pit_state.timer);
+ 		kthread_flush_work(&pit->expired);
+ 		kthread_stop(pit->worker_task);
+ 		kvm_free_irq_source_id(kvm, pit->irq_source_id);
+ 		kfree(pit);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  	}
  }
diff --cc drivers/block/loop.c
index d5de459aa504,fa1b7a90ba11..000000000000
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@@ -841,6 -838,23 +841,26 @@@ static void loop_config_discard(struct 
  	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
  }
  
++<<<<<<< HEAD
++=======
+ static void loop_unprepare_queue(struct loop_device *lo)
+ {
+ 	kthread_flush_worker(&lo->worker);
+ 	kthread_stop(lo->worker_task);
+ }
+ 
+ static int loop_prepare_queue(struct loop_device *lo)
+ {
+ 	kthread_init_worker(&lo->worker);
+ 	lo->worker_task = kthread_run(kthread_worker_fn,
+ 			&lo->worker, "loop%d", lo->lo_number);
+ 	if (IS_ERR(lo->worker_task))
+ 		return -ENOMEM;
+ 	set_user_nice(lo->worker_task, MIN_NICE);
+ 	return 0;
+ }
+ 
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  static int loop_set_fd(struct loop_device *lo, fmode_t mode,
  		       struct block_device *bdev, unsigned int arg)
  {
@@@ -1619,6 -1637,75 +1639,78 @@@ int loop_unregister_transfer(int number
  EXPORT_SYMBOL(loop_register_transfer);
  EXPORT_SYMBOL(loop_unregister_transfer);
  
++<<<<<<< HEAD
++=======
+ static int loop_queue_rq(struct blk_mq_hw_ctx *hctx,
+ 		const struct blk_mq_queue_data *bd)
+ {
+ 	struct loop_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);
+ 	struct loop_device *lo = cmd->rq->q->queuedata;
+ 
+ 	blk_mq_start_request(bd->rq);
+ 
+ 	if (lo->lo_state != Lo_bound)
+ 		return -EIO;
+ 
+ 	switch (req_op(cmd->rq)) {
+ 	case REQ_OP_FLUSH:
+ 	case REQ_OP_DISCARD:
+ 		cmd->use_aio = false;
+ 		break;
+ 	default:
+ 		cmd->use_aio = lo->use_dio;
+ 		break;
+ 	}
+ 
+ 	kthread_queue_work(&lo->worker, &cmd->work);
+ 
+ 	return BLK_MQ_RQ_QUEUE_OK;
+ }
+ 
+ static void loop_handle_cmd(struct loop_cmd *cmd)
+ {
+ 	const bool write = op_is_write(req_op(cmd->rq));
+ 	struct loop_device *lo = cmd->rq->q->queuedata;
+ 	int ret = 0;
+ 
+ 	if (write && (lo->lo_flags & LO_FLAGS_READ_ONLY)) {
+ 		ret = -EIO;
+ 		goto failed;
+ 	}
+ 
+ 	ret = do_req_filebacked(lo, cmd->rq);
+  failed:
+ 	/* complete non-aio request */
+ 	if (!cmd->use_aio || ret)
+ 		blk_mq_complete_request(cmd->rq, ret ? -EIO : 0);
+ }
+ 
+ static void loop_queue_work(struct kthread_work *work)
+ {
+ 	struct loop_cmd *cmd =
+ 		container_of(work, struct loop_cmd, work);
+ 
+ 	loop_handle_cmd(cmd);
+ }
+ 
+ static int loop_init_request(void *data, struct request *rq,
+ 		unsigned int hctx_idx, unsigned int request_idx,
+ 		unsigned int numa_node)
+ {
+ 	struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 	cmd->rq = rq;
+ 	kthread_init_work(&cmd->work, loop_queue_work);
+ 
+ 	return 0;
+ }
+ 
+ static struct blk_mq_ops loop_mq_ops = {
+ 	.queue_rq       = loop_queue_rq,
+ 	.init_request	= loop_init_request,
+ };
+ 
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  static int loop_add(struct loop_device **l, int i)
  {
  	struct loop_device *lo;
diff --cc drivers/spi/spi.c
index bff6fccdf1f3,5787b723b593..000000000000
--- a/drivers/spi/spi.c
+++ b/drivers/spi/spi.c
@@@ -561,9 -1109,56 +561,59 @@@ static void spi_pump_messages(struct kt
  		spin_unlock_irqrestore(&master->queue_lock, flags);
  		return;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/* If another context is idling the device then defer */
+ 	if (master->idling) {
+ 		kthread_queue_work(&master->kworker, &master->pump_messages);
+ 		spin_unlock_irqrestore(&master->queue_lock, flags);
+ 		return;
+ 	}
+ 
+ 	/* Check if the queue is idle */
+ 	if (list_empty(&master->queue) || !master->running) {
+ 		if (!master->busy) {
+ 			spin_unlock_irqrestore(&master->queue_lock, flags);
+ 			return;
+ 		}
+ 
+ 		/* Only do teardown in the thread */
+ 		if (!in_kthread) {
+ 			kthread_queue_work(&master->kworker,
+ 					   &master->pump_messages);
+ 			spin_unlock_irqrestore(&master->queue_lock, flags);
+ 			return;
+ 		}
+ 
+ 		master->busy = false;
+ 		master->idling = true;
+ 		spin_unlock_irqrestore(&master->queue_lock, flags);
+ 
+ 		kfree(master->dummy_rx);
+ 		master->dummy_rx = NULL;
+ 		kfree(master->dummy_tx);
+ 		master->dummy_tx = NULL;
+ 		if (master->unprepare_transfer_hardware &&
+ 		    master->unprepare_transfer_hardware(master))
+ 			dev_err(&master->dev,
+ 				"failed to unprepare transfer hardware\n");
+ 		if (master->auto_runtime_pm) {
+ 			pm_runtime_mark_last_busy(master->dev.parent);
+ 			pm_runtime_put_autosuspend(master->dev.parent);
+ 		}
+ 		trace_spi_master_idle(master);
+ 
+ 		spin_lock_irqsave(&master->queue_lock, flags);
+ 		master->idling = false;
+ 		spin_unlock_irqrestore(&master->queue_lock, flags);
+ 		return;
+ 	}
+ 
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  	/* Extract head of queue */
  	master->cur_msg =
 -		list_first_entry(&master->queue, struct spi_message, queue);
 +	    list_entry(master->queue.next, struct spi_message, queue);
  
  	list_del_init(&master->cur_msg->queue);
  	if (master->busy)
@@@ -599,15 -1250,15 +649,15 @@@ static int spi_init_queue(struct spi_ma
  	master->running = false;
  	master->busy = false;
  
- 	init_kthread_worker(&master->kworker);
+ 	kthread_init_worker(&master->kworker);
  	master->kworker_task = kthread_run(kthread_worker_fn,
 -					   &master->kworker, "%s",
 +					   &master->kworker,
  					   dev_name(&master->dev));
  	if (IS_ERR(master->kworker_task)) {
  		dev_err(&master->dev, "failed to create message pump task\n");
 -		return PTR_ERR(master->kworker_task);
 +		return -ENOMEM;
  	}
- 	init_kthread_work(&master->pump_messages, spi_pump_messages);
+ 	kthread_init_work(&master->pump_messages, spi_pump_messages);
  
  	/*
  	 * Master config will indicate if this controller should run the
@@@ -661,15 -1311,31 +711,20 @@@ EXPORT_SYMBOL_GPL(spi_get_next_queued_m
  void spi_finalize_current_message(struct spi_master *master)
  {
  	struct spi_message *mesg;
 -	unsigned long flags;
 -	int ret;
 -
 -	spin_lock_irqsave(&master->queue_lock, flags);
 -	mesg = master->cur_msg;
 -	spin_unlock_irqrestore(&master->queue_lock, flags);
 -
 -	spi_unmap_msg(master, mesg);
 -
 -	if (master->cur_msg_prepared && master->unprepare_message) {
 -		ret = master->unprepare_message(master, mesg);
 -		if (ret) {
 -			dev_err(&master->dev,
 -				"failed to unprepare message: %d\n", ret);
 -		}
 -	}
 +	unsigned long flags;
  
  	spin_lock_irqsave(&master->queue_lock, flags);
 +	mesg = master->cur_msg;
  	master->cur_msg = NULL;
++<<<<<<< HEAD
 +
 +	queue_kthread_work(&master->kworker, &master->pump_messages);
++=======
+ 	master->cur_msg_prepared = false;
+ 	kthread_queue_work(&master->kworker, &master->pump_messages);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  	spin_unlock_irqrestore(&master->queue_lock, flags);
  
 -	trace_spi_message_done(mesg);
 -
  	mesg->state = NULL;
  	if (mesg->complete)
  		mesg->complete(mesg->context);
@@@ -774,8 -1437,8 +829,13 @@@ static int spi_queued_transfer(struct s
  	msg->status = -EINPROGRESS;
  
  	list_add_tail(&msg->queue, &master->queue);
++<<<<<<< HEAD
 +	if (master->running && !master->busy)
 +		queue_kthread_work(&master->kworker, &master->pump_messages);
++=======
+ 	if (!master->busy && need_pump)
+ 		kthread_queue_work(&master->kworker, &master->pump_messages);
++>>>>>>> 3989144f863a (kthread: kthread worker API cleanup)
  
  	spin_unlock_irqrestore(&master->queue_lock, flags);
  	return 0;
* Unmerged path crypto/crypto_engine.c
* Unmerged path drivers/net/ethernet/microchip/encx24j600.c
* Unmerged path drivers/tty/serial/sc16is7xx.c
diff --git a/Documentation/RCU/lockdep-splat.txt b/Documentation/RCU/lockdep-splat.txt
index bf9061142827..238e9f61352f 100644
--- a/Documentation/RCU/lockdep-splat.txt
+++ b/Documentation/RCU/lockdep-splat.txt
@@ -57,7 +57,7 @@ Call Trace:
  [<ffffffff817db154>] kernel_thread_helper+0x4/0x10
  [<ffffffff81066430>] ? finish_task_switch+0x80/0x110
  [<ffffffff817d9c04>] ? retint_restore_args+0xe/0xe
- [<ffffffff81097510>] ? __init_kthread_worker+0x70/0x70
+ [<ffffffff81097510>] ? __kthread_init_worker+0x70/0x70
  [<ffffffff817db150>] ? gs_change+0xb/0xb
 
 Line 2776 of block/cfq-iosched.c in v3.0-rc5 is as follows:
* Unmerged path arch/x86/kvm/i8254.c
* Unmerged path crypto/crypto_engine.c
* Unmerged path drivers/block/loop.c
diff --git a/drivers/infiniband/sw/rdmavt/cq.c b/drivers/infiniband/sw/rdmavt/cq.c
index 3bbc3f36d3d6..b783e59d7ae9 100644
--- a/drivers/infiniband/sw/rdmavt/cq.c
+++ b/drivers/infiniband/sw/rdmavt/cq.c
@@ -129,7 +129,7 @@ void rvt_cq_enter(struct rvt_cq *cq, struct ib_wc *entry, bool solicited)
 		if (likely(worker)) {
 			cq->notify = RVT_CQ_NONE;
 			cq->triggered++;
-			queue_kthread_work(worker, &cq->comptask);
+			kthread_queue_work(worker, &cq->comptask);
 		}
 	}
 
@@ -265,7 +265,7 @@ struct ib_cq *rvt_create_cq(struct ib_device *ibdev,
 	cq->ibcq.cqe = entries;
 	cq->notify = RVT_CQ_NONE;
 	spin_lock_init(&cq->lock);
-	init_kthread_work(&cq->comptask, send_complete);
+	kthread_init_work(&cq->comptask, send_complete);
 	cq->queue = wc;
 
 	ret = &cq->ibcq;
@@ -295,7 +295,7 @@ int rvt_destroy_cq(struct ib_cq *ibcq)
 	struct rvt_cq *cq = ibcq_to_rvtcq(ibcq);
 	struct rvt_dev_info *rdi = cq->rdi;
 
-	flush_kthread_work(&cq->comptask);
+	kthread_flush_work(&cq->comptask);
 	spin_lock(&rdi->n_cqs_lock);
 	rdi->n_cqs_allocated--;
 	spin_unlock(&rdi->n_cqs_lock);
@@ -514,7 +514,7 @@ int rvt_driver_cq_init(struct rvt_dev_info *rdi)
 	rdi->worker = kzalloc(sizeof(*rdi->worker), GFP_KERNEL);
 	if (!rdi->worker)
 		return -ENOMEM;
-	init_kthread_worker(rdi->worker);
+	kthread_init_worker(rdi->worker);
 	task = kthread_create_on_node(
 		kthread_worker_fn,
 		rdi->worker,
@@ -547,7 +547,7 @@ void rvt_cq_exit(struct rvt_dev_info *rdi)
 	/* blocks future queuing from send_complete() */
 	rdi->worker = NULL;
 	smp_wmb(); /* See rdi_cq_enter */
-	flush_kthread_worker(worker);
+	kthread_flush_worker(worker);
 	kthread_stop(worker->task);
 	kfree(worker);
 }
diff --git a/drivers/md/dm-rq.c b/drivers/md/dm-rq.c
index 10a7844312c5..b502a5f106a5 100644
--- a/drivers/md/dm-rq.c
+++ b/drivers/md/dm-rq.c
@@ -562,7 +562,7 @@ static void init_tio(struct dm_rq_target_io *tio, struct request *rq,
 	if (!md->init_tio_pdu)
 		memset(&tio->info, 0, sizeof(tio->info));
 	if (md->kworker_task)
-		init_kthread_work(&tio->work, map_tio_request);
+		kthread_init_work(&tio->work, map_tio_request);
 }
 
 static struct dm_rq_target_io *dm_old_prep_tio(struct request *rq,
@@ -811,7 +811,7 @@ static void dm_old_request_fn(struct request_queue *q)
 		tio = tio_from_request(rq);
 		/* Establish tio->ti before queuing work (map_tio_request) */
 		tio->ti = ti;
-		queue_kthread_work(&md->kworker, &tio->work);
+		kthread_queue_work(&md->kworker, &tio->work);
 		BUG_ON(!irqs_disabled());
 	}
 }
@@ -833,7 +833,7 @@ int dm_old_init_request_queue(struct mapped_device *md)
 	blk_queue_prep_rq(md->queue, dm_old_prep_fn);
 
 	/* Initialize the request-based DM worker thread */
-	init_kthread_worker(&md->kworker);
+	kthread_init_worker(&md->kworker);
 	md->kworker_task = kthread_run(kthread_worker_fn, &md->kworker,
 				       "kdmwork-%s", dm_device_name(md));
 	if (IS_ERR(md->kworker_task)) {
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index ec8896186c46..bedde652d815 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -2088,7 +2088,7 @@ static void __dm_destroy(struct mapped_device *md, bool wait)
 	blk_set_queue_dying(q);
 
 	if (dm_request_based(md) && md->kworker_task)
-		flush_kthread_worker(&md->kworker);
+		kthread_flush_worker(&md->kworker);
 
 	/*
 	 * Take suspend_lock so that presuspend and postsuspend methods
@@ -2351,7 +2351,7 @@ static int __dm_suspend(struct mapped_device *md, struct dm_table *map,
 	if (dm_request_based(md)) {
 		dm_stop_queue(md->queue);
 		if (md->kworker_task)
-			flush_kthread_worker(&md->kworker);
+			kthread_flush_worker(&md->kworker);
 	}
 
 	flush_workqueue(md->wq);
diff --git a/drivers/media/pci/ivtv/ivtv-driver.c b/drivers/media/pci/ivtv/ivtv-driver.c
index 07b8460953b6..fa858e0e47ee 100644
--- a/drivers/media/pci/ivtv/ivtv-driver.c
+++ b/drivers/media/pci/ivtv/ivtv-driver.c
@@ -751,7 +751,7 @@ static int ivtv_init_struct1(struct ivtv *itv)
 	spin_lock_init(&itv->lock);
 	spin_lock_init(&itv->dma_reg_lock);
 
-	init_kthread_worker(&itv->irq_worker);
+	kthread_init_worker(&itv->irq_worker);
 	itv->irq_worker_task = kthread_run(kthread_worker_fn, &itv->irq_worker,
 					   itv->v4l2_dev.name);
 	if (IS_ERR(itv->irq_worker_task)) {
@@ -761,7 +761,7 @@ static int ivtv_init_struct1(struct ivtv *itv)
 	/* must use the FIFO scheduler as it is realtime sensitive */
 	sched_setscheduler(itv->irq_worker_task, SCHED_FIFO, &param);
 
-	init_kthread_work(&itv->irq_work, ivtv_irq_work_handler);
+	kthread_init_work(&itv->irq_work, ivtv_irq_work_handler);
 
 	/* Initial settings */
 	itv->cxhdl.port = CX2341X_PORT_MEMORY;
@@ -1447,7 +1447,7 @@ static void ivtv_remove(struct pci_dev *pdev)
 	del_timer_sync(&itv->dma_timer);
 
 	/* Kill irq worker */
-	flush_kthread_worker(&itv->irq_worker);
+	kthread_flush_worker(&itv->irq_worker);
 	kthread_stop(itv->irq_worker_task);
 
 	ivtv_streams_cleanup(itv, 1);
diff --git a/drivers/media/pci/ivtv/ivtv-irq.c b/drivers/media/pci/ivtv/ivtv-irq.c
index 19a7c9b990a3..3277c25678fa 100644
--- a/drivers/media/pci/ivtv/ivtv-irq.c
+++ b/drivers/media/pci/ivtv/ivtv-irq.c
@@ -1060,7 +1060,7 @@ irqreturn_t ivtv_irq_handler(int irq, void *dev_id)
 	}
 
 	if (test_and_clear_bit(IVTV_F_I_HAVE_WORK, &itv->i_flags)) {
-		queue_kthread_work(&itv->irq_worker, &itv->irq_work);
+		kthread_queue_work(&itv->irq_worker, &itv->irq_work);
 	}
 
 	spin_unlock(&itv->dma_reg_lock);
* Unmerged path drivers/net/ethernet/microchip/encx24j600.c
* Unmerged path drivers/spi/spi.c
* Unmerged path drivers/tty/serial/sc16is7xx.c
diff --git a/include/linux/kthread.h b/include/linux/kthread.h
index 7dcef3317689..37957815441b 100644
--- a/include/linux/kthread.h
+++ b/include/linux/kthread.h
@@ -56,7 +56,7 @@ extern int tsk_fork_get_node(struct task_struct *tsk);
  * Simple work processor based on kthread.
  *
  * This provides easier way to make use of kthreads.  A kthread_work
- * can be queued and flushed using queue/flush_kthread_work()
+ * can be queued and flushed using queue/kthread_flush_work()
  * respectively.  Queued kthread_works are processed by a kthread
  * running kthread_worker_fn().
  */
@@ -101,7 +101,7 @@ struct kthread_work {
  */
 #ifdef CONFIG_LOCKDEP
 # define KTHREAD_WORKER_INIT_ONSTACK(worker)				\
-	({ init_kthread_worker(&worker); worker; })
+	({ kthread_init_worker(&worker); worker; })
 # define DEFINE_KTHREAD_WORKER_ONSTACK(worker)				\
 	struct kthread_worker worker = KTHREAD_WORKER_INIT_ONSTACK(worker)
 # define KTHREAD_WORK_INIT_ONSTACK(work, fn)				\
@@ -113,16 +113,16 @@ struct kthread_work {
 # define DEFINE_KTHREAD_WORK_ONSTACK(work, fn) DEFINE_KTHREAD_WORK(work, fn)
 #endif
 
-extern void __init_kthread_worker(struct kthread_worker *worker,
+extern void __kthread_init_worker(struct kthread_worker *worker,
 			const char *name, struct lock_class_key *key);
 
-#define init_kthread_worker(worker)					\
+#define kthread_init_worker(worker)					\
 	do {								\
 		static struct lock_class_key __key;			\
-		__init_kthread_worker((worker), "("#worker")->lock", &__key); \
+		__kthread_init_worker((worker), "("#worker")->lock", &__key); \
 	} while (0)
 
-#define init_kthread_work(work, fn)					\
+#define kthread_init_work(work, fn)					\
 	do {								\
 		memset((work), 0, sizeof(struct kthread_work));		\
 		INIT_LIST_HEAD(&(work)->node);				\
@@ -132,9 +132,9 @@ extern void __init_kthread_worker(struct kthread_worker *worker,
 
 int kthread_worker_fn(void *worker_ptr);
 
-bool queue_kthread_work(struct kthread_worker *worker,
+bool kthread_queue_work(struct kthread_worker *worker,
 			struct kthread_work *work);
-void flush_kthread_work(struct kthread_work *work);
-void flush_kthread_worker(struct kthread_worker *worker);
+void kthread_flush_work(struct kthread_work *work);
+void kthread_flush_worker(struct kthread_worker *worker);
 
 #endif /* _LINUX_KTHREAD_H */
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 310519af560d..5429e4acd631 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -480,7 +480,7 @@ int kthreadd(void *unused)
 	return 0;
 }
 
-void __init_kthread_worker(struct kthread_worker *worker,
+void __kthread_init_worker(struct kthread_worker *worker,
 				const char *name,
 				struct lock_class_key *key)
 {
@@ -489,7 +489,7 @@ void __init_kthread_worker(struct kthread_worker *worker,
 	INIT_LIST_HEAD(&worker->work_list);
 	worker->task = NULL;
 }
-EXPORT_SYMBOL_GPL(__init_kthread_worker);
+EXPORT_SYMBOL_GPL(__kthread_init_worker);
 
 /**
  * kthread_worker_fn - kthread function to process kthread_worker
@@ -546,7 +546,7 @@ repeat:
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
 /* insert @work before @pos in @worker */
-static void insert_kthread_work(struct kthread_worker *worker,
+static void kthread_insert_work(struct kthread_worker *worker,
 			       struct kthread_work *work,
 			       struct list_head *pos)
 {
@@ -559,7 +559,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
 }
 
 /**
- * queue_kthread_work - queue a kthread_work
+ * kthread_queue_work - queue a kthread_work
  * @worker: target kthread_worker
  * @work: kthread_work to queue
  *
@@ -567,7 +567,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
  * must have been created with kthread_worker_create().  Returns %true
  * if @work was successfully queued, %false if it was already pending.
  */
-bool queue_kthread_work(struct kthread_worker *worker,
+bool kthread_queue_work(struct kthread_worker *worker,
 			struct kthread_work *work)
 {
 	bool ret = false;
@@ -575,13 +575,13 @@ bool queue_kthread_work(struct kthread_worker *worker,
 
 	spin_lock_irqsave(&worker->lock, flags);
 	if (list_empty(&work->node)) {
-		insert_kthread_work(worker, work, &worker->work_list);
+		kthread_insert_work(worker, work, &worker->work_list);
 		ret = true;
 	}
 	spin_unlock_irqrestore(&worker->lock, flags);
 	return ret;
 }
-EXPORT_SYMBOL_GPL(queue_kthread_work);
+EXPORT_SYMBOL_GPL(kthread_queue_work);
 
 struct kthread_flush_work {
 	struct kthread_work	work;
@@ -596,12 +596,12 @@ static void kthread_flush_work_fn(struct kthread_work *work)
 }
 
 /**
- * flush_kthread_work - flush a kthread_work
+ * kthread_flush_work - flush a kthread_work
  * @work: work to flush
  *
  * If @work is queued or executing, wait for it to finish execution.
  */
-void flush_kthread_work(struct kthread_work *work)
+void kthread_flush_work(struct kthread_work *work)
 {
 	struct kthread_flush_work fwork = {
 		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
@@ -622,9 +622,10 @@ retry:
 	}
 
 	if (!list_empty(&work->node))
-		insert_kthread_work(worker, &fwork.work, work->node.next);
+		kthread_insert_work(worker, &fwork.work, work->node.next);
 	else if (worker->current_work == work)
-		insert_kthread_work(worker, &fwork.work, worker->work_list.next);
+		kthread_insert_work(worker, &fwork.work,
+				    worker->work_list.next);
 	else
 		noop = true;
 
@@ -633,23 +634,23 @@ retry:
 	if (!noop)
 		wait_for_completion(&fwork.done);
 }
-EXPORT_SYMBOL_GPL(flush_kthread_work);
+EXPORT_SYMBOL_GPL(kthread_flush_work);
 
 /**
- * flush_kthread_worker - flush all current works on a kthread_worker
+ * kthread_flush_worker - flush all current works on a kthread_worker
  * @worker: worker to flush
  *
  * Wait until all currently executing or pending works on @worker are
  * finished.
  */
-void flush_kthread_worker(struct kthread_worker *worker)
+void kthread_flush_worker(struct kthread_worker *worker)
 {
 	struct kthread_flush_work fwork = {
 		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
 		COMPLETION_INITIALIZER_ONSTACK(fwork.done),
 	};
 
-	queue_kthread_work(worker, &fwork.work);
+	kthread_queue_work(worker, &fwork.work);
 	wait_for_completion(&fwork.done);
 }
-EXPORT_SYMBOL_GPL(flush_kthread_worker);
+EXPORT_SYMBOL_GPL(kthread_flush_worker);
diff --git a/sound/soc/intel/baytrail/sst-baytrail-ipc.c b/sound/soc/intel/baytrail/sst-baytrail-ipc.c
index c8455b47388b..7ab14ce65a73 100644
--- a/sound/soc/intel/baytrail/sst-baytrail-ipc.c
+++ b/sound/soc/intel/baytrail/sst-baytrail-ipc.c
@@ -338,7 +338,7 @@ static irqreturn_t sst_byt_irq_thread(int irq, void *context)
 	spin_unlock_irqrestore(&sst->spinlock, flags);
 
 	/* continue to send any remaining messages... */
-	queue_kthread_work(&ipc->kworker, &ipc->kwork);
+	kthread_queue_work(&ipc->kworker, &ipc->kwork);
 
 	return IRQ_HANDLED;
 }
diff --git a/sound/soc/intel/common/sst-ipc.c b/sound/soc/intel/common/sst-ipc.c
index a12c7bb08d3b..6c672ac79cce 100644
--- a/sound/soc/intel/common/sst-ipc.c
+++ b/sound/soc/intel/common/sst-ipc.c
@@ -111,7 +111,7 @@ static int ipc_tx_message(struct sst_generic_ipc *ipc, u64 header,
 	list_add_tail(&msg->list, &ipc->tx_list);
 	spin_unlock_irqrestore(&ipc->dsp->spinlock, flags);
 
-	queue_kthread_work(&ipc->kworker, &ipc->kwork);
+	kthread_queue_work(&ipc->kworker, &ipc->kwork);
 
 	if (wait)
 		return tx_wait_done(ipc, msg, rx_data);
@@ -281,7 +281,7 @@ int sst_ipc_init(struct sst_generic_ipc *ipc)
 		return -ENOMEM;
 
 	/* start the IPC message thread */
-	init_kthread_worker(&ipc->kworker);
+	kthread_init_worker(&ipc->kworker);
 	ipc->tx_thread = kthread_run(kthread_worker_fn,
 					&ipc->kworker, "%s",
 					dev_name(ipc->dev));
@@ -292,7 +292,7 @@ int sst_ipc_init(struct sst_generic_ipc *ipc)
 		return ret;
 	}
 
-	init_kthread_work(&ipc->kwork, ipc_tx_msgs);
+	kthread_init_work(&ipc->kwork, ipc_tx_msgs);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(sst_ipc_init);
diff --git a/sound/soc/intel/haswell/sst-haswell-ipc.c b/sound/soc/intel/haswell/sst-haswell-ipc.c
index 91565229d074..e432a31fd9f2 100644
--- a/sound/soc/intel/haswell/sst-haswell-ipc.c
+++ b/sound/soc/intel/haswell/sst-haswell-ipc.c
@@ -818,7 +818,7 @@ static irqreturn_t hsw_irq_thread(int irq, void *context)
 	spin_unlock_irqrestore(&sst->spinlock, flags);
 
 	/* continue to send any remaining messages... */
-	queue_kthread_work(&ipc->kworker, &ipc->kwork);
+	kthread_queue_work(&ipc->kworker, &ipc->kwork);
 
 	return IRQ_HANDLED;
 }
diff --git a/sound/soc/intel/skylake/skl-sst-ipc.c b/sound/soc/intel/skylake/skl-sst-ipc.c
index 0bd01e62622c..797cf4053235 100644
--- a/sound/soc/intel/skylake/skl-sst-ipc.c
+++ b/sound/soc/intel/skylake/skl-sst-ipc.c
@@ -464,7 +464,7 @@ irqreturn_t skl_dsp_irq_thread_handler(int irq, void *context)
 	skl_ipc_int_enable(dsp);
 
 	/* continue to send any remaining messages... */
-	queue_kthread_work(&ipc->kworker, &ipc->kwork);
+	kthread_queue_work(&ipc->kworker, &ipc->kwork);
 
 	return IRQ_HANDLED;
 }
