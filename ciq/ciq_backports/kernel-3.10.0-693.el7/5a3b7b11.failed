s390/dasd: add query host access to volume support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] dasd: add query host access to volume support (Hendrik Brueckner) [1274412]
Rebuild_FUZZ: 94.74%
commit-author Stefan Haberland <stefan.haberland@de.ibm.com>
commit 5a3b7b112884f80ff19b18028fabeb4f9c035518
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/5a3b7b11.failed

With this feature, applications can query if a DASD volume is online
to another operating system instances by checking the online status of
all attached hosts from the storage server.

	Reviewed-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
	Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
	Signed-off-by: Stefan Haberland <stefan.haberland@de.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 5a3b7b112884f80ff19b18028fabeb4f9c035518)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/block/dasd.c
#	drivers/s390/block/dasd_devmap.c
#	drivers/s390/block/dasd_eckd.c
#	drivers/s390/block/dasd_eckd.h
#	drivers/s390/block/dasd_int.h
diff --cc drivers/s390/block/dasd.c
index 5f422bc676a9,4adb6d14d562..000000000000
--- a/drivers/s390/block/dasd.c
+++ b/drivers/s390/block/dasd.c
@@@ -302,9 -307,8 +305,14 @@@ static int dasd_state_basic_to_known(st
  		return rc;
  	dasd_device_clear_timer(device);
  	dasd_profile_exit(&device->profile);
++<<<<<<< HEAD
 +	if (device->debugfs_dentry)
 +		debugfs_remove(device->debugfs_dentry);
 +
++=======
+ 	dasd_hosts_exit(device);
+ 	debugfs_remove(device->debugfs_dentry);
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
  	DBF_DEV_EVENT(DBF_EMERG, device, "%p debug area deleted", device);
  	if (device->debug_area != NULL) {
  		debug_unregister(device->debug_area);
diff --cc drivers/s390/block/dasd_devmap.c
index a71bb8aaca1d,3cdbce45e464..000000000000
--- a/drivers/s390/block/dasd_devmap.c
+++ b/drivers/s390/block/dasd_devmap.c
@@@ -1353,6 -1495,10 +1379,11 @@@ static struct attribute * dasd_attrs[] 
  	&dev_attr_reservation_policy.attr,
  	&dev_attr_last_known_reservation_state.attr,
  	&dev_attr_safe_offline.attr,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_host_access_count.attr,
+ 	&dev_attr_path_masks.attr,
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
  	NULL,
  };
  
diff --cc drivers/s390/block/dasd_eckd.c
index d5740c400937,3b70d378d1c1..000000000000
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@@ -4461,6 -4556,646 +4462,649 @@@ out_err
  	return -1;
  }
  
++<<<<<<< HEAD
++=======
+ static int dasd_eckd_read_message_buffer(struct dasd_device *device,
+ 					 struct dasd_rssd_messages *messages,
+ 					 __u8 lpum)
+ {
+ 	struct dasd_rssd_messages *message_buf;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   (sizeof(struct dasd_psf_prssd_data) +
+ 				    sizeof(struct dasd_rssd_messages)),
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->expires = 10 * HZ;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 	/* dasd_sleep_on_immediatly does not do complex error
+ 	 * recovery so clear erp flag and set retry counter to
+ 	 * do basic erp */
+ 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
+ 	cqr->retries = 256;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = 0x03;	/* Message Buffer */
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - message buffer */
+ 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
+ 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
+ 
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_rssd_messages);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) message_buf;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_immediatly(cqr);
+ 	if (rc == 0) {
+ 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 		message_buf = (struct dasd_rssd_messages *)
+ 			(prssdp + 1);
+ 		memcpy(messages, message_buf,
+ 		       sizeof(struct dasd_rssd_messages));
+ 	} else
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading messages failed with rc=%d\n"
+ 				, rc);
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ static int dasd_eckd_query_host_access(struct dasd_device *device,
+ 				       struct dasd_psf_query_host_access *data)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_psf_query_host_access *host_access;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	/* not available for HYPER PAV alias devices */
+ 	if (!device->block && private->lcu->pav == HYPER_PAV)
+ 		return -EOPNOTSUPP;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   sizeof(struct dasd_psf_prssd_data) + 1,
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 	host_access = kzalloc(sizeof(*host_access), GFP_KERNEL | GFP_DMA);
+ 	if (!host_access) {
+ 		dasd_sfree_request(cqr, device);
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate host_access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10 * HZ;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = PSF_SUBORDER_QHA;	/* query host access */
+ 	/* LSS and Volume that will be queried */
+ 	prssdp->lss = private->ned->ID;
+ 	prssdp->volume = private->ned->unit_addr;
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - query host access */
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_psf_query_host_access);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) host_access;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on(cqr);
+ 	if (rc == 0) {
+ 		*data = *host_access;
+ 	} else {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading host access data failed with rc=%d\n",
+ 				rc);
+ 		rc = -EOPNOTSUPP;
+ 	}
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	kfree(host_access);
+ 	return rc;
+ }
+ /*
+  * return number of grouped devices
+  */
+ static int dasd_eckd_host_access_count(struct dasd_device *device)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	int count = 0;
+ 	int rc, i;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		if (entry->status_flags & DASD_ECKD_PG_GROUPED)
+ 			count++;
+ 	}
+ 
+ 	kfree(access);
+ 	return count;
+ }
+ 
+ /*
+  * write host access information to a sequential file
+  */
+ static int dasd_hosts_print(struct dasd_device *device, struct seq_file *m)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	char sysplex[9] = "";
+ 	int rc, i, j;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		/* PGID */
+ 		seq_puts(m, "pgid ");
+ 		for (j = 0; j < 11; j++)
+ 			seq_printf(m, "%02x", entry->pgid[j]);
+ 		seq_putc(m, '\n');
+ 		/* FLAGS */
+ 		seq_printf(m, "status_flags %02x\n", entry->status_flags);
+ 		/* SYSPLEX NAME */
+ 		memcpy(&sysplex, &entry->sysplex_name, sizeof(sysplex) - 1);
+ 		EBCASC(sysplex, sizeof(sysplex));
+ 		seq_printf(m, "sysplex_name %8s\n", sysplex);
+ 		/* SUPPORTED CYLINDER */
+ 		seq_printf(m, "supported_cylinder %d\n", entry->cylinder);
+ 		/* TIMESTAMP */
+ 		seq_printf(m, "timestamp %lu\n", (unsigned long)
+ 			   entry->timestamp);
+ 	}
+ 	kfree(access);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Perform Subsystem Function - CUIR response
+  */
+ static int
+ dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
+ 			    __u32 message_id,
+ 			    struct channel_path_desc *desc,
+ 			    struct subchannel_id sch_id)
+ {
+ 	struct dasd_psf_cuir_response *psf_cuir;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */ ,
+ 				  sizeof(struct dasd_psf_cuir_response),
+ 				  device);
+ 
+ 	if (IS_ERR(cqr)) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			   "Could not allocate PSF-CUIR request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
+ 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
+ 	psf_cuir->cc = response;
+ 	if (desc)
+ 		psf_cuir->chpid = desc->chpid;
+ 	psf_cuir->message_id = message_id;
+ 	psf_cuir->cssid = sch_id.cssid;
+ 	psf_cuir->ssid = sch_id.ssid;
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->cda = (__u32)(addr_t)psf_cuir;
+ 	ccw->flags = CCW_FLAG_SLI;
+ 	ccw->count = sizeof(struct dasd_psf_cuir_response);
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10*HZ;
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 
+ 	rc = dasd_sleep_on(cqr);
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * return configuration data that is referenced by record selector
+  * if a record selector is specified or per default return the
+  * conf_data pointer for the path specified by lpum
+  */
+ static struct dasd_conf_data *dasd_eckd_get_ref_conf(struct dasd_device *device,
+ 						     __u8 lpum,
+ 						     struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_conf_data *conf_data;
+ 	int path, pos;
+ 
+ 	if (cuir->record_selector == 0)
+ 		goto out;
+ 	for (path = 0x80, pos = 0; path; path >>= 1, pos++) {
+ 		conf_data = private->path_conf_data[pos];
+ 		if (conf_data->gneq.record_selector ==
+ 		    cuir->record_selector)
+ 			return conf_data;
+ 	}
+ out:
+ 	return private->path_conf_data[pathmask_to_pos(lpum)];
+ }
+ 
+ /*
+  * This function determines the scope of a reconfiguration request by
+  * analysing the path and device selection data provided in the CUIR request.
+  * Returns a path mask containing CUIR affected paths for the give device.
+  *
+  * If the CUIR request does not contain the required information return the
+  * path mask of the path the attention message for the CUIR request was reveived
+  * on.
+  */
+ static int dasd_eckd_cuir_scope(struct dasd_device *device, __u8 lpum,
+ 				struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_conf_data *ref_conf_data;
+ 	unsigned long bitmask = 0, mask = 0;
+ 	struct dasd_conf_data *conf_data;
+ 	unsigned int pos, path;
+ 	char *ref_gneq, *gneq;
+ 	char *ref_ned, *ned;
+ 	int tbcpm = 0;
+ 
+ 	/* if CUIR request does not specify the scope use the path
+ 	   the attention message was presented on */
+ 	if (!cuir->ned_map ||
+ 	    !(cuir->neq_map[0] | cuir->neq_map[1] | cuir->neq_map[2]))
+ 		return lpum;
+ 
+ 	/* get reference conf data */
+ 	ref_conf_data = dasd_eckd_get_ref_conf(device, lpum, cuir);
+ 	/* reference ned is determined by ned_map field */
+ 	pos = 8 - ffs(cuir->ned_map);
+ 	ref_ned = (char *)&ref_conf_data->neds[pos];
+ 	ref_gneq = (char *)&ref_conf_data->gneq;
+ 	/* transfer 24 bit neq_map to mask */
+ 	mask = cuir->neq_map[2];
+ 	mask |= cuir->neq_map[1] << 8;
+ 	mask |= cuir->neq_map[0] << 16;
+ 
+ 	for (path = 0x80; path; path >>= 1) {
+ 		/* initialise data per path */
+ 		bitmask = mask;
+ 		pos = pathmask_to_pos(path);
+ 		conf_data = private->path_conf_data[pos];
+ 		pos = 8 - ffs(cuir->ned_map);
+ 		ned = (char *) &conf_data->neds[pos];
+ 		/* compare reference ned and per path ned */
+ 		if (memcmp(ref_ned, ned, sizeof(*ned)) != 0)
+ 			continue;
+ 		gneq = (char *)&conf_data->gneq;
+ 		/* compare reference gneq and per_path gneq under
+ 		   24 bit mask where mask bit 0 equals byte 7 of
+ 		   the gneq and mask bit 24 equals byte 31 */
+ 		while (bitmask) {
+ 			pos = ffs(bitmask) - 1;
+ 			if (memcmp(&ref_gneq[31 - pos], &gneq[31 - pos], 1)
+ 			    != 0)
+ 				break;
+ 			clear_bit(pos, &bitmask);
+ 		}
+ 		if (bitmask)
+ 			continue;
+ 		/* device and path match the reference values
+ 		   add path to CUIR scope */
+ 		tbcpm |= path;
+ 	}
+ 	return tbcpm;
+ }
+ 
+ static void dasd_eckd_cuir_notify_user(struct dasd_device *device,
+ 				       unsigned long paths,
+ 				       struct subchannel_id sch_id, int action)
+ {
+ 	struct channel_path_desc *desc;
+ 	int pos;
+ 
+ 	while (paths) {
+ 		/* get position of bit in mask */
+ 		pos = ffs(paths) - 1;
+ 		/* get channel path descriptor from this position */
+ 		desc = ccw_device_get_chp_desc(device->cdev, 7 - pos);
+ 		if (action == CUIR_QUIESCE)
+ 			pr_warn("Service on the storage server caused path "
+ 				"%x.%02x to go offline", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		else if (action == CUIR_RESUME)
+ 			pr_info("Path %x.%02x is back online after service "
+ 				"on the storage server", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		kfree(desc);
+ 		clear_bit(pos, &paths);
+ 	}
+ }
+ 
+ static int dasd_eckd_cuir_remove_path(struct dasd_device *device, __u8 lpum,
+ 				      struct dasd_cuir_message *cuir)
+ {
+ 	unsigned long tbcpm;
+ 
+ 	tbcpm = dasd_eckd_cuir_scope(device, lpum, cuir);
+ 	/* nothing to do if path is not in use */
+ 	if (!(device->path_data.opm & tbcpm))
+ 		return 0;
+ 	if (!(device->path_data.opm & ~tbcpm)) {
+ 		/* no path would be left if the CUIR action is taken
+ 		   return error */
+ 		return -EINVAL;
+ 	}
+ 	/* remove device from operational path mask */
+ 	device->path_data.opm &= ~tbcpm;
+ 	device->path_data.cuirpm |= tbcpm;
+ 	return tbcpm;
+ }
+ 
+ /*
+  * walk through all devices and build a path mask to quiesce them
+  * return an error if the last path to a device would be removed
+  *
+  * if only part of the devices are quiesced and an error
+  * occurs no onlining necessary, the storage server will
+  * notify the already set offline devices again
+  */
+ static int dasd_eckd_cuir_quiesce(struct dasd_device *device, __u8 lpum,
+ 				  struct subchannel_id sch_id,
+ 				  struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	unsigned long flags;
+ 	int tbcpm;
+ 
+ 	/* active devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->active_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* inactive devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist, group) {
+ 		list_for_each_entry_safe(dev, n, &pavgroup->baselist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 		list_for_each_entry_safe(dev, n, &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_QUIESCE);
+ 	return 0;
+ out_err:
+ 	return tbcpm;
+ }
+ 
+ static int dasd_eckd_cuir_resume(struct dasd_device *device, __u8 lpum,
+ 				 struct subchannel_id sch_id,
+ 				 struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	int tbcpm;
+ 
+ 	/*
+ 	 * the path may have been added through a generic path event before
+ 	 * only trigger path verification if the path is not already in use
+ 	 */
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->active_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dev->path_data.opm & tbcpm)) {
+ 			dev->path_data.tbvpm |= tbcpm;
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dev->path_data.opm & tbcpm)) {
+ 			dev->path_data.tbvpm |= tbcpm;
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist,
+ 				 group) {
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->baselist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dev->path_data.opm & tbcpm)) {
+ 				dev->path_data.tbvpm |= tbcpm;
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dev->path_data.opm & tbcpm)) {
+ 				dev->path_data.tbvpm |= tbcpm;
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_RESUME);
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_handle_cuir(struct dasd_device *device, void *messages,
+ 				 __u8 lpum)
+ {
+ 	struct dasd_cuir_message *cuir = messages;
+ 	struct channel_path_desc *desc;
+ 	struct subchannel_id sch_id;
+ 	int pos, response;
+ 
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR request: %016llx %016llx %016llx %08x",
+ 		      ((u64 *)cuir)[0], ((u64 *)cuir)[1], ((u64 *)cuir)[2],
+ 		      ((u32 *)cuir)[3]);
+ 	ccw_device_get_schid(device->cdev, &sch_id);
+ 	pos = pathmask_to_pos(lpum);
+ 	desc = ccw_device_get_chp_desc(device->cdev, pos);
+ 
+ 	if (cuir->code == CUIR_QUIESCE) {
+ 		/* quiesce */
+ 		if (dasd_eckd_cuir_quiesce(device, lpum, sch_id, cuir))
+ 			response = PSF_CUIR_LAST_PATH;
+ 		else
+ 			response = PSF_CUIR_COMPLETED;
+ 	} else if (cuir->code == CUIR_RESUME) {
+ 		/* resume */
+ 		dasd_eckd_cuir_resume(device, lpum, sch_id, cuir);
+ 		response = PSF_CUIR_COMPLETED;
+ 	} else
+ 		response = PSF_CUIR_NOT_SUPPORTED;
+ 
+ 	dasd_eckd_psf_cuir_response(device, response,
+ 				    cuir->message_id, desc, sch_id);
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR response: %d on message ID %08x", response,
+ 		      cuir->message_id);
+ 	/* free descriptor copy */
+ 	kfree(desc);
+ 	/* to make sure there is no attention left schedule work again */
+ 	device->discipline->check_attention(device, lpum);
+ }
+ 
+ static void dasd_eckd_check_attention_work(struct work_struct *work)
+ {
+ 	struct check_attention_work_data *data;
+ 	struct dasd_rssd_messages *messages;
+ 	struct dasd_device *device;
+ 	int rc;
+ 
+ 	data = container_of(work, struct check_attention_work_data, worker);
+ 	device = data->device;
+ 	messages = kzalloc(sizeof(*messages), GFP_KERNEL);
+ 	if (!messages) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Could not allocate attention message buffer");
+ 		goto out;
+ 	}
+ 	rc = dasd_eckd_read_message_buffer(device, messages, data->lpum);
+ 	if (rc)
+ 		goto out;
+ 	if (messages->length == ATTENTION_LENGTH_CUIR &&
+ 	    messages->format == ATTENTION_FORMAT_CUIR)
+ 		dasd_eckd_handle_cuir(device, messages, data->lpum);
+ out:
+ 	dasd_put_device(device);
+ 	kfree(messages);
+ 	kfree(data);
+ }
+ 
+ static int dasd_eckd_check_attention(struct dasd_device *device, __u8 lpum)
+ {
+ 	struct check_attention_work_data *data;
+ 
+ 	data = kzalloc(sizeof(*data), GFP_ATOMIC);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	INIT_WORK(&data->worker, dasd_eckd_check_attention_work);
+ 	dasd_get_device(device);
+ 	data->device = device;
+ 	data->lpum = lpum;
+ 	schedule_work(&data->worker);
+ 	return 0;
+ }
+ 
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
  static struct ccw_driver dasd_eckd_driver = {
  	.driver = {
  		.name	= "dasd-eckd",
@@@ -4525,6 -5260,9 +5169,12 @@@ static struct dasd_discipline dasd_eckd
  	.reload = dasd_eckd_reload_device,
  	.get_uid = dasd_eckd_get_uid,
  	.kick_validate = dasd_eckd_kick_validate_server,
++<<<<<<< HEAD
++=======
+ 	.check_attention = dasd_eckd_check_attention,
+ 	.host_access_count = dasd_eckd_host_access_count,
+ 	.hosts_print = dasd_hosts_print,
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
  };
  
  static int __init
diff --cc drivers/s390/block/dasd_eckd.h
index 2555e494591f,862ee4291abd..000000000000
--- a/drivers/s390/block/dasd_eckd.h
+++ b/drivers/s390/block/dasd_eckd.h
@@@ -51,8 -51,38 +51,43 @@@
  /*
   * Perform Subsystem Function / Sub-Orders
   */
++<<<<<<< HEAD
 +#define PSF_ORDER_PRSSD 0x18
 +#define PSF_ORDER_SSC	0x1D
++=======
+ #define PSF_ORDER_PRSSD			 0x18
+ #define PSF_ORDER_CUIR_RESPONSE		 0x1A
+ #define PSF_SUBORDER_QHA		 0x1C
+ #define PSF_ORDER_SSC			 0x1D
+ 
+ /*
+  * CUIR response condition codes
+  */
+ #define PSF_CUIR_INVALID		 0x00
+ #define PSF_CUIR_COMPLETED		 0x01
+ #define PSF_CUIR_NOT_SUPPORTED		 0x02
+ #define PSF_CUIR_ERROR_IN_REQ		 0x03
+ #define PSF_CUIR_DENIED			 0x04
+ #define PSF_CUIR_LAST_PATH		 0x05
+ #define PSF_CUIR_DEVICE_ONLINE		 0x06
+ #define PSF_CUIR_VARY_FAILURE		 0x07
+ #define PSF_CUIR_SOFTWARE_FAILURE	 0x08
+ #define PSF_CUIR_NOT_RECOGNIZED		 0x09
+ 
+ /*
+  * CUIR codes
+  */
+ #define CUIR_QUIESCE			 0x01
+ #define CUIR_RESUME			 0x02
+ 
+ /*
+  * attention message definitions
+  */
+ #define ATTENTION_LENGTH_CUIR		 0x0e
+ #define ATTENTION_FORMAT_CUIR		 0x01
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
+ 
+ #define DASD_ECKD_PG_GROUPED		 0x10
  
  /*
   * Size that is reportet for large volumes in the old 16-bit no_cyl field
@@@ -342,7 -373,64 +377,32 @@@ struct dasd_rssd_features 
  	char feature[256];
  } __attribute__((packed));
  
 -struct dasd_rssd_messages {
 -	__u16 length;
 -	__u8 format;
 -	__u8 code;
 -	__u32 message_id;
 -	__u8 flags;
 -	char messages[4087];
 -} __packed;
 -
 -struct dasd_cuir_message {
 -	__u16 length;
 -	__u8 format;
 -	__u8 code;
 -	__u32 message_id;
 -	__u8 flags;
 -	__u8 neq_map[3];
 -	__u8 ned_map;
 -	__u8 record_selector;
 -} __packed;
 -
 -struct dasd_psf_cuir_response {
 -	__u8 order;
 -	__u8 flags;
 -	__u8 cc;
 -	__u8 chpid;
 -	__u16 device_nr;
 -	__u16 reserved;
 -	__u32 message_id;
 -	__u64 system_id;
 -	__u8 cssid;
 -	__u8 ssid;
 -} __packed;
  
+ struct dasd_ckd_path_group_entry {
+ 	__u8 status_flags;
+ 	__u8 pgid[11];
+ 	__u8 sysplex_name[8];
+ 	__u32 timestamp;
+ 	__u32 cylinder;
+ 	__u8 reserved[4];
+ } __packed;
+ 
+ struct dasd_ckd_host_information {
+ 	__u8 access_flags;
+ 	__u8 entry_size;
+ 	__u16 entry_count;
+ 	__u8 entry[16390];
+ } __packed;
+ 
+ struct dasd_psf_query_host_access {
+ 	__u8 access_flag;
+ 	__u8 version;
+ 	__u16 CKD_length;
+ 	__u16 SCSI_length;
+ 	__u8 unused[10];
+ 	__u8 host_access_information[16394];
+ } __packed;
+ 
  /*
   * Perform Subsystem Function - Prepare for Read Subsystem Data
   */
diff --cc drivers/s390/block/dasd_int.h
index aa498f7fe95b,6132733bcd95..000000000000
--- a/drivers/s390/block/dasd_int.h
+++ b/drivers/s390/block/dasd_int.h
@@@ -355,6 -364,9 +355,12 @@@ struct dasd_discipline 
  
  	int (*get_uid) (struct dasd_device *, struct dasd_uid *);
  	void (*kick_validate) (struct dasd_device *);
++<<<<<<< HEAD
++=======
+ 	int (*check_attention)(struct dasd_device *, __u8);
+ 	int (*host_access_count)(struct dasd_device *);
+ 	int (*hosts_print)(struct dasd_device *, struct seq_file *);
++>>>>>>> 5a3b7b112884 (s390/dasd: add query host access to volume support)
  };
  
  extern struct dasd_discipline *dasd_diag_discipline_pointer;
@@@ -467,8 -484,12 +473,9 @@@ struct dasd_device 
  
  	/* default expiration time in s */
  	unsigned long default_expires;
 -	unsigned long default_retries;
 -
 -	unsigned long blk_timeout;
  
  	struct dentry *debugfs_dentry;
+ 	struct dentry *hosts_dentry;
  	struct dasd_profile profile;
  };
  
* Unmerged path drivers/s390/block/dasd.c
* Unmerged path drivers/s390/block/dasd_devmap.c
* Unmerged path drivers/s390/block/dasd_eckd.c
* Unmerged path drivers/s390/block/dasd_eckd.h
* Unmerged path drivers/s390/block/dasd_int.h
