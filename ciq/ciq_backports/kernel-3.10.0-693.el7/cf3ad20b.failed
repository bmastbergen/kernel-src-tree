mm/hugetlb: compute/return the number of regions added by region_add()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hugetlb: compute/return the number of regions added by region_add() (Andrea Arcangeli) [1430172]
Rebuild_FUZZ: 97.81%
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit cf3ad20bfeadda693e408d85684790714fc29b08
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/cf3ad20b.failed

Modify region_add() to keep track of regions(pages) added to the reserve
map and return this value.  The return value can be compared to the return
value of region_chg() to determine if the map was modified between calls.

Make vma_commit_reservation() also pass along the return value of
region_add().  In the normal case, we want vma_commit_reservation to
return the same value as the preceding call to vma_needs_reservation.
Create a common __vma_reservation_common routine to help keep the special
case return values in sync

	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Luiz Capitulino <lcapitulino@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit cf3ad20bfeadda693e408d85684790714fc29b08)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 567cd0ffc031,cd3fc4194733..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1506,60 -1484,56 +1520,98 @@@ static void return_unused_surplus_pages
  }
  
  /*
-  * Determine if the huge page at addr within the vma has an associated
-  * reservation.  Where it does not we will need to logically increase
-  * reservation and actually increase subpool usage before an allocation
-  * can occur.  Where any new reservation would be required the
-  * reservation change is prepared, but not committed.  Once the page
-  * has been allocated from the subpool and instantiated the change should
-  * be committed via vma_commit_reservation.  No action is required on
-  * failure.
+  * vma_needs_reservation and vma_commit_reservation are used by the huge
+  * page allocation routines to manage reservations.
+  *
+  * vma_needs_reservation is called to determine if the huge page at addr
+  * within the vma has an associated reservation.  If a reservation is
+  * needed, the value 1 is returned.  The caller is then responsible for
+  * managing the global reservation and subpool usage counts.  After
+  * the huge page has been allocated, vma_commit_reservation is called
+  * to add the page to the reservation map.
+  *
+  * In the normal case, vma_commit_reservation returns the same value
+  * as the preceding vma_needs_reservation call.  The only time this
+  * is not the case is if a reserve map was changed between calls.  It
+  * is the responsibility of the caller to notice the difference and
+  * take appropriate action.
   */
- static long vma_needs_reservation(struct hstate *h,
- 			struct vm_area_struct *vma, unsigned long addr)
+ static long __vma_reservation_common(struct hstate *h,
+ 				struct vm_area_struct *vma, unsigned long addr,
+ 				bool commit)
  {
++<<<<<<< HEAD
 +	struct address_space *mapping = vma->vm_file->f_mapping;
 +	struct inode *inode = mapping->host;
++=======
+ 	struct resv_map *resv;
+ 	pgoff_t idx;
+ 	long ret;
++>>>>>>> cf3ad20bfead (mm/hugetlb: compute/return the number of regions added by region_add())
 +
 +	if (vma->vm_flags & VM_MAYSHARE) {
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *resv = inode->i_mapping->private_data;
 +
 +		return region_chg(resv, idx, idx + 1);
  
 -	resv = vma_resv_map(vma);
 -	if (!resv)
 +	} else if (!is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
  		return 1;
  
++<<<<<<< HEAD
 +	} else  {
 +		long err;
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *reservations = vma_resv_map(vma);
 +
 +		err = region_chg(reservations, idx, idx + 1);
 +		if (err < 0)
 +			return err;
 +		return 0;
 +	}
++=======
+ 	idx = vma_hugecache_offset(h, vma, addr);
+ 	if (commit)
+ 		ret = region_add(resv, idx, idx + 1);
+ 	else
+ 		ret = region_chg(resv, idx, idx + 1);
+ 
+ 	if (vma->vm_flags & VM_MAYSHARE)
+ 		return ret;
+ 	else
+ 		return ret < 0 ? ret : 0;
++>>>>>>> cf3ad20bfead (mm/hugetlb: compute/return the number of regions added by region_add())
  }
- static void vma_commit_reservation(struct hstate *h,
+ 
+ static long vma_needs_reservation(struct hstate *h,
  			struct vm_area_struct *vma, unsigned long addr)
  {
++<<<<<<< HEAD
 +	struct address_space *mapping = vma->vm_file->f_mapping;
 +	struct inode *inode = mapping->host;
 +
 +	if (vma->vm_flags & VM_MAYSHARE) {
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *resv = inode->i_mapping->private_data;
 +
 +		region_add(resv, idx, idx + 1);
 +
 +	} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
 +		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
 +		struct resv_map *reservations = vma_resv_map(vma);
 +
 +		/* Mark this page used in the map. */
 +		region_add(reservations, idx, idx + 1);
 +	}
++=======
+ 	return __vma_reservation_common(h, vma, addr, false);
+ }
+ 
+ static long vma_commit_reservation(struct hstate *h,
+ 			struct vm_area_struct *vma, unsigned long addr)
+ {
+ 	return __vma_reservation_common(h, vma, addr, true);
++>>>>>>> cf3ad20bfead (mm/hugetlb: compute/return the number of regions added by region_add())
  }
  
  static struct page *alloc_huge_page(struct vm_area_struct *vma,
* Unmerged path mm/hugetlb.c
