dmaengine: reference counted unmap data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 45c463ae924c62af4aa64ded1ca831f334a1db65
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/45c463ae.failed

Hang a common 'unmap' object off of dma descriptors for the purpose of
providing a unified unmapping interface.  The lifetime of a mapping may
span multiple descriptors, so these unmap objects are reference counted
by related descriptor.

	Cc: Vinod Koul <vinod.koul@intel.com>
	Cc: Tomasz Figa <t.figa@samsung.com>
	Cc: Dave Jiang <dave.jiang@intel.com>
[bzolnier: fix IS_ENABLED() check]
[bzolnier: fix release ordering in dmaengine_destroy_unmap_pool()]
[bzolnier: fix check for success in dmaengine_init_unmap_pool()]
[bzolnier: use mempool_free() instead of kmem_cache_free()]
[bzolnier: add missing unmap->len initializations]
[bzolnier: add __init tag to dmaengine_init_unmap_pool()]
	Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
	Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
[djbw: move DMAENGINE=n support to this patch for async_tx]
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 45c463ae924c62af4aa64ded1ca831f334a1db65)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/dmaengine.c
#	include/linux/dmaengine.h
diff --cc drivers/dma/dmaengine.c
index d50b0664c5a9,e721a1caff7f..000000000000
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@@ -881,6 -902,234 +882,237 @@@ void dma_async_device_unregister(struc
  }
  EXPORT_SYMBOL(dma_async_device_unregister);
  
++<<<<<<< HEAD
++=======
+ struct dmaengine_unmap_pool {
+ 	struct kmem_cache *cache;
+ 	const char *name;
+ 	mempool_t *pool;
+ 	size_t size;
+ };
+ 
+ #define __UNMAP_POOL(x) { .size = x, .name = "dmaengine-unmap-" __stringify(x) }
+ static struct dmaengine_unmap_pool unmap_pool[] = {
+ 	__UNMAP_POOL(2),
+ 	#if IS_ENABLED(CONFIG_ASYNC_TX_DMA)
+ 	__UNMAP_POOL(16),
+ 	__UNMAP_POOL(128),
+ 	__UNMAP_POOL(256),
+ 	#endif
+ };
+ 
+ static struct dmaengine_unmap_pool *__get_unmap_pool(int nr)
+ {
+ 	int order = get_count_order(nr);
+ 
+ 	switch (order) {
+ 	case 0 ... 1:
+ 		return &unmap_pool[0];
+ 	case 2 ... 4:
+ 		return &unmap_pool[1];
+ 	case 5 ... 7:
+ 		return &unmap_pool[2];
+ 	case 8:
+ 		return &unmap_pool[3];
+ 	default:
+ 		BUG();
+ 		return NULL;
+ 	}
+ }
+ 
+ static void dmaengine_unmap(struct kref *kref)
+ {
+ 	struct dmaengine_unmap_data *unmap = container_of(kref, typeof(*unmap), kref);
+ 	struct device *dev = unmap->dev;
+ 	int cnt, i;
+ 
+ 	cnt = unmap->to_cnt;
+ 	for (i = 0; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_TO_DEVICE);
+ 	cnt += unmap->from_cnt;
+ 	for (; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_FROM_DEVICE);
+ 	cnt += unmap->bidi_cnt;
+ 	for (; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_BIDIRECTIONAL);
+ 	mempool_free(unmap, __get_unmap_pool(cnt)->pool);
+ }
+ 
+ void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)
+ {
+ 	if (unmap)
+ 		kref_put(&unmap->kref, dmaengine_unmap);
+ }
+ EXPORT_SYMBOL_GPL(dmaengine_unmap_put);
+ 
+ static void dmaengine_destroy_unmap_pool(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {
+ 		struct dmaengine_unmap_pool *p = &unmap_pool[i];
+ 
+ 		if (p->pool)
+ 			mempool_destroy(p->pool);
+ 		p->pool = NULL;
+ 		if (p->cache)
+ 			kmem_cache_destroy(p->cache);
+ 		p->cache = NULL;
+ 	}
+ }
+ 
+ static int __init dmaengine_init_unmap_pool(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {
+ 		struct dmaengine_unmap_pool *p = &unmap_pool[i];
+ 		size_t size;
+ 
+ 		size = sizeof(struct dmaengine_unmap_data) +
+ 		       sizeof(dma_addr_t) * p->size;
+ 
+ 		p->cache = kmem_cache_create(p->name, size, 0,
+ 					     SLAB_HWCACHE_ALIGN, NULL);
+ 		if (!p->cache)
+ 			break;
+ 		p->pool = mempool_create_slab_pool(1, p->cache);
+ 		if (!p->pool)
+ 			break;
+ 	}
+ 
+ 	if (i == ARRAY_SIZE(unmap_pool))
+ 		return 0;
+ 
+ 	dmaengine_destroy_unmap_pool();
+ 	return -ENOMEM;
+ }
+ 
+ static struct dmaengine_unmap_data *
+ dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)
+ {
+ 	struct dmaengine_unmap_data *unmap;
+ 
+ 	unmap = mempool_alloc(__get_unmap_pool(nr)->pool, flags);
+ 	if (!unmap)
+ 		return NULL;
+ 
+ 	memset(unmap, 0, sizeof(*unmap));
+ 	kref_init(&unmap->kref);
+ 	unmap->dev = dev;
+ 
+ 	return unmap;
+ }
+ 
+ /**
+  * dma_async_memcpy_pg_to_pg - offloaded copy from page to page
+  * @chan: DMA channel to offload copy to
+  * @dest_pg: destination page
+  * @dest_off: offset in page to copy to
+  * @src_pg: source page
+  * @src_off: offset in page to copy from
+  * @len: length
+  *
+  * Both @dest_page/@dest_off and @src_page/@src_off must be mappable to a bus
+  * address according to the DMA mapping API rules for streaming mappings.
+  * Both @dest_page/@dest_off and @src_page/@src_off must stay memory resident
+  * (kernel memory or locked user space pages).
+  */
+ dma_cookie_t
+ dma_async_memcpy_pg_to_pg(struct dma_chan *chan, struct page *dest_pg,
+ 	unsigned int dest_off, struct page *src_pg, unsigned int src_off,
+ 	size_t len)
+ {
+ 	struct dma_device *dev = chan->device;
+ 	struct dma_async_tx_descriptor *tx;
+ 	struct dmaengine_unmap_data *unmap;
+ 	dma_cookie_t cookie;
+ 	unsigned long flags;
+ 
+ 	unmap = dmaengine_get_unmap_data(dev->dev, 2, GFP_NOIO);
+ 	if (!unmap)
+ 		return -ENOMEM;
+ 
+ 	unmap->to_cnt = 1;
+ 	unmap->from_cnt = 1;
+ 	unmap->addr[0] = dma_map_page(dev->dev, src_pg, src_off, len,
+ 				      DMA_TO_DEVICE);
+ 	unmap->addr[1] = dma_map_page(dev->dev, dest_pg, dest_off, len,
+ 				      DMA_FROM_DEVICE);
+ 	unmap->len = len;
+ 	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_SRC_UNMAP |
+ 		DMA_COMPL_SKIP_DEST_UNMAP;
+ 	tx = dev->device_prep_dma_memcpy(chan, unmap->addr[1], unmap->addr[0],
+ 					 len, flags);
+ 
+ 	if (!tx) {
+ 		dmaengine_unmap_put(unmap);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	dma_set_unmap(tx, unmap);
+ 	cookie = tx->tx_submit(tx);
+ 	dmaengine_unmap_put(unmap);
+ 
+ 	preempt_disable();
+ 	__this_cpu_add(chan->local->bytes_transferred, len);
+ 	__this_cpu_inc(chan->local->memcpy_count);
+ 	preempt_enable();
+ 
+ 	return cookie;
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_pg_to_pg);
+ 
+ /**
+  * dma_async_memcpy_buf_to_buf - offloaded copy between virtual addresses
+  * @chan: DMA channel to offload copy to
+  * @dest: destination address (virtual)
+  * @src: source address (virtual)
+  * @len: length
+  *
+  * Both @dest and @src must be mappable to a bus address according to the
+  * DMA mapping API rules for streaming mappings.
+  * Both @dest and @src must stay memory resident (kernel memory or locked
+  * user space pages).
+  */
+ dma_cookie_t
+ dma_async_memcpy_buf_to_buf(struct dma_chan *chan, void *dest,
+ 			    void *src, size_t len)
+ {
+ 	return dma_async_memcpy_pg_to_pg(chan, virt_to_page(dest),
+ 					 (unsigned long) dest & ~PAGE_MASK,
+ 					 virt_to_page(src),
+ 					 (unsigned long) src & ~PAGE_MASK, len);
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_buf_to_buf);
+ 
+ /**
+  * dma_async_memcpy_buf_to_pg - offloaded copy from address to page
+  * @chan: DMA channel to offload copy to
+  * @page: destination page
+  * @offset: offset in page to copy to
+  * @kdata: source address (virtual)
+  * @len: length
+  *
+  * Both @page/@offset and @kdata must be mappable to a bus address according
+  * to the DMA mapping API rules for streaming mappings.
+  * Both @page/@offset and @kdata must stay memory resident (kernel memory or
+  * locked user space pages)
+  */
+ dma_cookie_t
+ dma_async_memcpy_buf_to_pg(struct dma_chan *chan, struct page *page,
+ 			   unsigned int offset, void *kdata, size_t len)
+ {
+ 	return dma_async_memcpy_pg_to_pg(chan, page, offset,
+ 					 virt_to_page(kdata),
+ 					 (unsigned long) kdata & ~PAGE_MASK, len);
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_buf_to_pg);
+ 
++>>>>>>> 45c463ae924c (dmaengine: reference counted unmap data)
  void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
  	struct dma_chan *chan)
  {
diff --cc include/linux/dmaengine.h
index 9ffca149944f,2fe855a7cab1..000000000000
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@@ -471,6 -457,23 +471,26 @@@ struct dma_async_tx_descriptor 
  #endif
  };
  
++<<<<<<< HEAD
++=======
+ static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
+ 				 struct dmaengine_unmap_data *unmap)
+ {
+ 	kref_get(&unmap->kref);
+ 	tx->unmap = unmap;
+ }
+ 
+ void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap);
+ 
+ static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
+ {
+ 	if (tx->unmap) {
+ 		dmaengine_unmap_put(tx->unmap);
+ 		tx->unmap = NULL;
+ 	}
+ }
+ 
++>>>>>>> 45c463ae924c (dmaengine: reference counted unmap data)
  #ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
  static inline void txd_lock(struct dma_async_tx_descriptor *txd)
  {
* Unmerged path drivers/dma/dmaengine.c
* Unmerged path include/linux/dmaengine.h
