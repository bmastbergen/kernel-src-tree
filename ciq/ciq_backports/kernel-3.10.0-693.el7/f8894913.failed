vhost: introduce O(1) vq metadata cache

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [vhost] introduce O(1) vq metadata cache (Wei Xu) [1425127 1283257]
Rebuild_FUZZ: 90.14%
commit-author Jason Wang <jasowang@redhat.com>
commit f889491380582b4ba2981cf0b0d7d6a40fb30ab7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f8894913.failed

When device IOTLB is enabled, all address translations were stored in
interval tree. O(lgN) searching time could be slow for virtqueue
metadata (avail, used and descriptors) since they were accessed much
often than other addresses. So this patch introduces an O(1) array
which points to the interval tree nodes that store the translations of
vq metadata. Those array were update during vq IOTLB prefetching and
were reset during each invalidation and tlb update. Each time we want
to access vq metadata, this small array were queried before interval
tree. This would be sufficient for static mappings but not dynamic
mappings, we could do optimizations on top.

Test were done with l2fwd in guest (2M hugepage):

   noiommu  | before        | after
tx 1.32Mpps | 1.06Mpps(82%) | 1.30Mpps(98%)
rx 2.33Mpps | 1.46Mpps(63%) | 2.29Mpps(98%)

We can almost reach the same performance as noiommu mode.

	Signed-off-by: Jason Wang <jasowang@redhat.com>
	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
(cherry picked from commit f889491380582b4ba2981cf0b0d7d6a40fb30ab7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/vhost.c
#	drivers/vhost/vhost.h
diff --cc drivers/vhost/vhost.c
index f65052baafd1,998bed505530..000000000000
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@@ -280,10 -323,12 +296,16 @@@ static void vhost_vq_reset(struct vhost
  	vq->call_ctx = NULL;
  	vq->call = NULL;
  	vq->log_ctx = NULL;
 -	vhost_reset_is_le(vq);
 -	vhost_disable_cross_endian(vq);
 +	vq->memory = NULL;
 +	vq->is_le = virtio_legacy_is_little_endian();
 +	vhost_vq_reset_user_be(vq);
  	vq->busyloop_timeout = 0;
++<<<<<<< HEAD
++=======
+ 	vq->umem = NULL;
+ 	vq->iotlb = NULL;
+ 	__vhost_vq_meta_reset(vq);
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  }
  
  static int vhost_worker(void *data)
@@@ -607,9 -708,21 +629,21 @@@ static int vq_memory_access_ok(void __u
  	return 1;
  }
  
+ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
+ 					       u64 addr, unsigned int size,
+ 					       int type)
+ {
+ 	const struct vhost_umem_node *node = vq->meta_iotlb[type];
+ 
+ 	if (!node)
+ 		return NULL;
+ 
+ 	return (void *)(uintptr_t)(node->userspace_addr + addr - node->start);
+ }
+ 
  /* Can we switch to this memory table? */
  /* Caller should have device mutex but not vq mutex */
 -static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,
 +static int memory_access_ok(struct vhost_dev *d, struct vhost_memory *mem,
  			    int log_all)
  {
  	int i;
@@@ -632,6 -746,412 +666,415 @@@
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
+ 			  struct iovec iov[], int iov_size, int access);
+ 
+ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
+ 			      const void *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_to_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that all vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		struct iov_iter t;
+ 		void __user *uaddr = vhost_vq_meta_fetch(vq,
+ 				     (u64)(uintptr_t)to, size,
+ 				     VHOST_ADDR_DESC);
+ 
+ 		if (uaddr)
+ 			return __copy_to_user(uaddr, from, size);
+ 
+ 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_WO);
+ 		if (ret < 0)
+ 			goto out;
+ 		iov_iter_init(&t, WRITE, vq->iotlb_iov, ret, size);
+ 		ret = copy_to_iter(from, size, &t);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ out:
+ 	return ret;
+ }
+ 
+ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
+ 				void __user *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_from_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		void __user *uaddr = vhost_vq_meta_fetch(vq,
+ 				     (u64)(uintptr_t)from, size,
+ 				     VHOST_ADDR_DESC);
+ 		struct iov_iter f;
+ 
+ 		if (uaddr)
+ 			return __copy_from_user(to, uaddr, size);
+ 
+ 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_RO);
+ 		if (ret < 0) {
+ 			vq_err(vq, "IOTLB translation failure: uaddr "
+ 			       "%p size 0x%llx\n", from,
+ 			       (unsigned long long) size);
+ 			goto out;
+ 		}
+ 		iov_iter_init(&f, READ, vq->iotlb_iov, ret, size);
+ 		ret = copy_from_iter(to, size, &f);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ 
+ out:
+ 	return ret;
+ }
+ 
+ static void __user *__vhost_get_user_slow(struct vhost_virtqueue *vq,
+ 					  void __user *addr, unsigned int size,
+ 					  int type)
+ {
+ 	int ret;
+ 
+ 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ 			     ARRAY_SIZE(vq->iotlb_iov),
+ 			     VHOST_ACCESS_RO);
+ 	if (ret < 0) {
+ 		vq_err(vq, "IOTLB translation failure: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	if (ret != 1 || vq->iotlb_iov[0].iov_len != size) {
+ 		vq_err(vq, "Non atomic userspace memory access: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	return vq->iotlb_iov[0].iov_base;
+ }
+ 
+ /* This function should be called after iotlb
+  * prefetch, which means we're sure that vq
+  * could be access through iotlb. So -EAGAIN should
+  * not happen in this case.
+  */
+ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
+ 					    void *addr, unsigned int size,
+ 					    int type)
+ {
+ 	void __user *uaddr = vhost_vq_meta_fetch(vq,
+ 			     (u64)(uintptr_t)addr, size, type);
+ 	if (uaddr)
+ 		return uaddr;
+ 
+ 	return __vhost_get_user_slow(vq, addr, size, type);
+ }
+ 
+ #define vhost_put_user(vq, x, ptr)		\
+ ({ \
+ 	int ret = -EFAULT; \
+ 	if (!vq->iotlb) { \
+ 		ret = __put_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) to = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr,	\
+ 					  sizeof(*ptr), VHOST_ADDR_USED); \
+ 		if (to != NULL) \
+ 			ret = __put_user(x, to); \
+ 		else \
+ 			ret = -EFAULT;	\
+ 	} \
+ 	ret; \
+ })
+ 
+ #define vhost_get_user(vq, x, ptr, type)		\
+ ({ \
+ 	int ret; \
+ 	if (!vq->iotlb) { \
+ 		ret = __get_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) from = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr, \
+ 							   sizeof(*ptr), \
+ 							   type); \
+ 		if (from != NULL) \
+ 			ret = __get_user(x, from); \
+ 		else \
+ 			ret = -EFAULT; \
+ 	} \
+ 	ret; \
+ })
+ 
+ #define vhost_get_avail(vq, x, ptr) \
+ 	vhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)
+ 
+ #define vhost_get_used(vq, x, ptr) \
+ 	vhost_get_user(vq, x, ptr, VHOST_ADDR_USED)
+ 
+ static void vhost_dev_lock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_lock(&d->vqs[i]->mutex);
+ }
+ 
+ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_unlock(&d->vqs[i]->mutex);
+ }
+ 
+ static int vhost_new_umem_range(struct vhost_umem *umem,
+ 				u64 start, u64 size, u64 end,
+ 				u64 userspace_addr, int perm)
+ {
+ 	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
+ 
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	if (umem->numem == max_iotlb_entries) {
+ 		tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+ 		vhost_umem_free(umem, tmp);
+ 	}
+ 
+ 	node->start = start;
+ 	node->size = size;
+ 	node->last = end;
+ 	node->userspace_addr = userspace_addr;
+ 	node->perm = perm;
+ 	INIT_LIST_HEAD(&node->link);
+ 	list_add_tail(&node->link, &umem->umem_list);
+ 	vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+ 	umem->numem++;
+ 
+ 	return 0;
+ }
+ 
+ static void vhost_del_umem_range(struct vhost_umem *umem,
+ 				 u64 start, u64 end)
+ {
+ 	struct vhost_umem_node *node;
+ 
+ 	while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+ 							   start, end)))
+ 		vhost_umem_free(umem, node);
+ }
+ 
+ static void vhost_iotlb_notify_vq(struct vhost_dev *d,
+ 				  struct vhost_iotlb_msg *msg)
+ {
+ 	struct vhost_msg_node *node, *n;
+ 
+ 	spin_lock(&d->iotlb_lock);
+ 
+ 	list_for_each_entry_safe(node, n, &d->pending_list, node) {
+ 		struct vhost_iotlb_msg *vq_msg = &node->msg.iotlb;
+ 		if (msg->iova <= vq_msg->iova &&
+ 		    msg->iova + msg->size - 1 > vq_msg->iova &&
+ 		    vq_msg->type == VHOST_IOTLB_MISS) {
+ 			vhost_poll_queue(&node->vq->poll);
+ 			list_del(&node->node);
+ 			kfree(node);
+ 		}
+ 	}
+ 
+ 	spin_unlock(&d->iotlb_lock);
+ }
+ 
+ static int umem_access_ok(u64 uaddr, u64 size, int access)
+ {
+ 	unsigned long a = uaddr;
+ 
+ 	/* Make sure 64 bit math will not overflow. */
+ 	if (vhost_overflow(uaddr, size))
+ 		return -EFAULT;
+ 
+ 	if ((access & VHOST_ACCESS_RO) &&
+ 	    !access_ok(VERIFY_READ, (void __user *)a, size))
+ 		return -EFAULT;
+ 	if ((access & VHOST_ACCESS_WO) &&
+ 	    !access_ok(VERIFY_WRITE, (void __user *)a, size))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ static int vhost_process_iotlb_msg(struct vhost_dev *dev,
+ 				   struct vhost_iotlb_msg *msg)
+ {
+ 	int ret = 0;
+ 
+ 	vhost_dev_lock_vqs(dev);
+ 	switch (msg->type) {
+ 	case VHOST_IOTLB_UPDATE:
+ 		if (!dev->iotlb) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		if (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		vhost_vq_meta_reset(dev);
+ 		if (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,
+ 					 msg->iova + msg->size - 1,
+ 					 msg->uaddr, msg->perm)) {
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 		vhost_iotlb_notify_vq(dev, msg);
+ 		break;
+ 	case VHOST_IOTLB_INVALIDATE:
+ 		vhost_vq_meta_reset(dev);
+ 		vhost_del_umem_range(dev->iotlb, msg->iova,
+ 				     msg->iova + msg->size - 1);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	vhost_dev_unlock_vqs(dev);
+ 	return ret;
+ }
+ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
+ 			     struct iov_iter *from)
+ {
+ 	struct vhost_msg_node node;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 	size_t ret;
+ 	int err;
+ 
+ 	if (iov_iter_count(from) < size)
+ 		return 0;
+ 	ret = copy_from_iter(&node.msg, size, from);
+ 	if (ret != size)
+ 		goto done;
+ 
+ 	switch (node.msg.type) {
+ 	case VHOST_IOTLB_MSG:
+ 		err = vhost_process_iotlb_msg(dev, &node.msg.iotlb);
+ 		if (err)
+ 			ret = err;
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ done:
+ 	return ret;
+ }
+ EXPORT_SYMBOL(vhost_chr_write_iter);
+ 
+ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
+ 			    poll_table *wait)
+ {
+ 	unsigned int mask = 0;
+ 
+ 	poll_wait(file, &dev->wait, wait);
+ 
+ 	if (!list_empty(&dev->read_list))
+ 		mask |= POLLIN | POLLRDNORM;
+ 
+ 	return mask;
+ }
+ EXPORT_SYMBOL(vhost_chr_poll);
+ 
+ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
+ 			    int noblock)
+ {
+ 	DEFINE_WAIT(wait);
+ 	struct vhost_msg_node *node;
+ 	ssize_t ret = 0;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 
+ 	if (iov_iter_count(to) < size)
+ 		return 0;
+ 
+ 	while (1) {
+ 		if (!noblock)
+ 			prepare_to_wait(&dev->wait, &wait,
+ 					TASK_INTERRUPTIBLE);
+ 
+ 		node = vhost_dequeue_msg(dev, &dev->read_list);
+ 		if (node)
+ 			break;
+ 		if (noblock) {
+ 			ret = -EAGAIN;
+ 			break;
+ 		}
+ 		if (signal_pending(current)) {
+ 			ret = -ERESTARTSYS;
+ 			break;
+ 		}
+ 		if (!dev->iotlb) {
+ 			ret = -EBADFD;
+ 			break;
+ 		}
+ 
+ 		schedule();
+ 	}
+ 
+ 	if (!noblock)
+ 		finish_wait(&dev->wait, &wait);
+ 
+ 	if (node) {
+ 		ret = copy_to_iter(&node->msg, size, to);
+ 
+ 		if (ret != size || node->msg.type != VHOST_IOTLB_MISS) {
+ 			kfree(node);
+ 			return ret;
+ 		}
+ 
+ 		vhost_enqueue_msg(dev, &dev->pending_list, node);
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(vhost_chr_read_iter);
+ 
+ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
+ {
+ 	struct vhost_dev *dev = vq->dev;
+ 	struct vhost_msg_node *node;
+ 	struct vhost_iotlb_msg *msg;
+ 
+ 	node = vhost_new_msg(vq, VHOST_IOTLB_MISS);
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	msg = &node->msg.iotlb;
+ 	msg->type = VHOST_IOTLB_MISS;
+ 	msg->iova = iova;
+ 	msg->perm = access;
+ 
+ 	vhost_enqueue_msg(dev, &dev->read_list, node);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
  			struct vring_desc __user *desc,
  			struct vring_avail __user *avail,
@@@ -645,6 -1167,74 +1088,77 @@@
  			sizeof *used + num * sizeof *used->ring + s);
  }
  
++<<<<<<< HEAD
++=======
+ static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
+ 				 const struct vhost_umem_node *node,
+ 				 int type)
+ {
+ 	int access = (type == VHOST_ADDR_USED) ?
+ 		     VHOST_ACCESS_WO : VHOST_ACCESS_RO;
+ 
+ 	if (likely(node->perm & access))
+ 		vq->meta_iotlb[type] = node;
+ }
+ 
+ static int iotlb_access_ok(struct vhost_virtqueue *vq,
+ 			   int access, u64 addr, u64 len, int type)
+ {
+ 	const struct vhost_umem_node *node;
+ 	struct vhost_umem *umem = vq->iotlb;
+ 	u64 s = 0, size, orig_addr = addr;
+ 
+ 	if (vhost_vq_meta_fetch(vq, addr, len, type))
+ 		return true;
+ 
+ 	while (len > s) {
+ 		node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+ 							   addr,
+ 							   addr + len - 1);
+ 		if (node == NULL || node->start > addr) {
+ 			vhost_iotlb_miss(vq, addr, access);
+ 			return false;
+ 		} else if (!(node->perm & access)) {
+ 			/* Report the possible access violation by
+ 			 * request another translation from userspace.
+ 			 */
+ 			return false;
+ 		}
+ 
+ 		size = node->size - addr + node->start;
+ 
+ 		if (orig_addr == addr && size >= len)
+ 			vhost_vq_meta_update(vq, node, type);
+ 
+ 		s += size;
+ 		addr += size;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ int vq_iotlb_prefetch(struct vhost_virtqueue *vq)
+ {
+ 	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ 	unsigned int num = vq->num;
+ 
+ 	if (!vq->iotlb)
+ 		return 1;
+ 
+ 	return iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->desc,
+ 			       num * sizeof(*vq->desc), VHOST_ADDR_DESC) &&
+ 	       iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->avail,
+ 			       sizeof *vq->avail +
+ 			       num * sizeof(*vq->avail->ring) + s,
+ 			       VHOST_ADDR_AVAIL) &&
+ 	       iotlb_access_ok(vq, VHOST_ACCESS_WO, (u64)(uintptr_t)vq->used,
+ 			       sizeof *vq->used +
+ 			       num * sizeof(*vq->used->ring) + s,
+ 			       VHOST_ADDR_USED);
+ }
+ EXPORT_SYMBOL_GPL(vq_iotlb_prefetch);
+ 
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  /* Can we log writes? */
  /* Caller should have device mutex but not vq mutex */
  int vhost_log_access_ok(struct vhost_dev *dev)
@@@ -1184,23 -1805,34 +1698,37 @@@ int vhost_init_used(struct vhost_virtqu
  
  	r = vhost_update_used_flags(vq);
  	if (r)
 -		goto err;
 +		return r;
  	vq->signalled_used_valid = false;
++<<<<<<< HEAD
 +	if (!access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx))
 +		return -EFAULT;
 +	r = __get_user(last_used_idx, &vq->used->idx);
 +	if (r)
 +		return r;
++=======
+ 	if (!vq->iotlb &&
+ 	    !access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx)) {
+ 		r = -EFAULT;
+ 		goto err;
+ 	}
+ 	r = vhost_get_used(vq, last_used_idx, &vq->used->idx);
+ 	if (r) {
+ 		vq_err(vq, "Can't access used idx at %p\n",
+ 		       &vq->used->idx);
+ 		goto err;
+ 	}
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  	vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
  	return 0;
 -
 -err:
 -	vq->is_le = is_le;
 -	return r;
  }
 -EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 +EXPORT_SYMBOL_GPL(vhost_init_used);
  
  static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 -			  struct iovec iov[], int iov_size, int access)
 +			  struct iovec iov[], int iov_size)
  {
 -	const struct vhost_umem_node *node;
 -	struct vhost_dev *dev = vq->dev;
 -	struct vhost_umem *umem = dev->iotlb ? dev->iotlb : dev->umem;
 +	const struct vhost_memory_region *reg;
 +	struct vhost_memory *mem;
  	struct iovec *_iov;
  	u64 s = 0;
  	int ret = 0;
@@@ -1363,30 -2014,37 +1891,63 @@@ int vhost_get_vq_desc(struct vhost_virt
  
  	/* Check it isn't doing very strange things with descriptor numbers. */
  	last_avail_idx = vq->last_avail_idx;
++<<<<<<< HEAD
 +	if (unlikely(__get_user(avail_idx, &vq->avail->idx))) {
 +		vq_err(vq, "Failed to access avail idx at %p\n",
 +		       &vq->avail->idx);
 +		return -EFAULT;
++=======
+ 
+ 	if (vq->avail_idx == vq->last_avail_idx) {
+ 		if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
+ 			vq_err(vq, "Failed to access avail idx at %p\n",
+ 				&vq->avail->idx);
+ 			return -EFAULT;
+ 		}
+ 		vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+ 
+ 		if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+ 			vq_err(vq, "Guest moved used index from %u to %u",
+ 				last_avail_idx, vq->avail_idx);
+ 			return -EFAULT;
+ 		}
+ 
+ 		/* If there's nothing new since last we looked, return
+ 		 * invalid.
+ 		 */
+ 		if (vq->avail_idx == last_avail_idx)
+ 			return vq->num;
+ 
+ 		/* Only get avail ring entries after they have been
+ 		 * exposed by guest.
+ 		 */
+ 		smp_rmb();
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  	}
 +	vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
 +
 +	if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
 +		vq_err(vq, "Guest moved used index from %u to %u",
 +		       last_avail_idx, vq->avail_idx);
 +		return -EFAULT;
 +	}
 +
 +	/* If there's nothing new since last we looked, return invalid. */
 +	if (vq->avail_idx == last_avail_idx)
 +		return vq->num;
 +
 +	/* Only get avail ring entries after they have been exposed by guest. */
 +	smp_rmb();
  
  	/* Grab the next descriptor number they're advertising, and increment
  	 * the index we've seen. */
++<<<<<<< HEAD
 +	if (unlikely(__get_user(ring_head,
 +				&vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {
++=======
+ 	if (unlikely(vhost_get_avail(vq, ring_head,
+ 		     &vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  		vq_err(vq, "Failed to read head: idx %d address %p\n",
  		       last_avail_idx,
  		       &vq->avail->ring[last_avail_idx % vq->num]);
@@@ -1593,7 -2255,11 +2154,15 @@@ static bool vhost_notify(struct vhost_d
  
  	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
  		__virtio16 flags;
++<<<<<<< HEAD
 +		if (__get_user(flags, &vq->avail->flags)) {
++=======
+ 		/* Flush out used index updates. This is paired
+ 		 * with the barrier that the Guest executes when enabling
+ 		 * interrupts. */
+ 		smp_mb();
+ 		if (vhost_get_avail(vq, flags, &vq->avail->flags)) {
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  			vq_err(vq, "Failed to get flags");
  			return true;
  		}
@@@ -1607,7 -2273,20 +2176,24 @@@
  	if (unlikely(!v))
  		return true;
  
++<<<<<<< HEAD
 +	if (__get_user(event, vhost_used_event(vq))) {
++=======
+ 	/* We're sure if the following conditions are met, there's no
+ 	 * need to notify guest:
+ 	 * 1) cached used event is ahead of new
+ 	 * 2) old to new updating does not cross cached used event. */
+ 	if (vring_need_event(vq->last_used_event, new + vq->num, new) &&
+ 	    !vring_need_event(vq->last_used_event, new, old))
+ 		return false;
+ 
+ 	/* Flush out used index updates. This is paired
+ 	 * with the barrier that the Guest executes when enabling
+ 	 * interrupts. */
+ 	smp_mb();
+ 
+ 	if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  		vq_err(vq, "Failed to get used event idx");
  		return true;
  	}
@@@ -1649,7 -2330,7 +2235,11 @@@ bool vhost_vq_avail_empty(struct vhost_
  	__virtio16 avail_idx;
  	int r;
  
++<<<<<<< HEAD
 +	r = __get_user(avail_idx, &vq->avail->idx);
++=======
+ 	r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  	if (r)
  		return false;
  
@@@ -1684,7 -2365,7 +2274,11 @@@ bool vhost_enable_notify(struct vhost_d
  	/* They could have slipped one in as we were doing that: make
  	 * sure it's written, then check again. */
  	smp_mb();
++<<<<<<< HEAD
 +	r = __get_user(avail_idx, &vq->avail->idx);
++=======
+ 	r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  	if (r) {
  		vq_err(vq, "Failed to check avail idx at %p: %d\n",
  		       &vq->avail->idx, r);
diff --cc drivers/vhost/vhost.h
index 5fd914ff48dd,f55671d53f28..000000000000
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@@ -53,6 -55,34 +53,37 @@@ struct vhost_log 
  	u64 len;
  };
  
++<<<<<<< HEAD
++=======
+ #define START(node) ((node)->start)
+ #define LAST(node) ((node)->last)
+ 
+ struct vhost_umem_node {
+ 	struct rb_node rb;
+ 	struct list_head link;
+ 	__u64 start;
+ 	__u64 last;
+ 	__u64 size;
+ 	__u64 userspace_addr;
+ 	__u32 perm;
+ 	__u32 flags_padding;
+ 	__u64 __subtree_last;
+ };
+ 
+ struct vhost_umem {
+ 	struct rb_root umem_tree;
+ 	struct list_head umem_list;
+ 	int numem;
+ };
+ 
+ enum vhost_uaddr_type {
+ 	VHOST_ADDR_DESC = 0,
+ 	VHOST_ADDR_AVAIL = 1,
+ 	VHOST_ADDR_USED = 2,
+ 	VHOST_NUM_ADDRS = 3,
+ };
+ 
++>>>>>>> f88949138058 (vhost: introduce O(1) vq metadata cache)
  /* The virtqueue structure describes a queue attached to a device. */
  struct vhost_virtqueue {
  	struct vhost_dev *dev;
* Unmerged path drivers/vhost/vhost.c
* Unmerged path drivers/vhost/vhost.h
