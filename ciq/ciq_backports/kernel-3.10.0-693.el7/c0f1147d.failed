net/mlx5e: Change the SQ/RQ operational state to positive logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Change the SQ/RQ operational state to positive logic (Don Dutile) [1385330 1417285]
Rebuild_FUZZ: 96.72%
commit-author Mohamad Haj Yahia <mohamad@mellanox.com>
commit c0f1147d14e4b09018a495c5095094e5707a4f44
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c0f1147d.failed

When using the negative logic (i.e. FLUSH state), after the RQ/SQ reopen
we will have a time interval that the RQ/SQ is not really ready and the
state indicates that its not in FLUSH state because the initial SQ/RQ struct
memory starts as zeros.
Now we changed the state to indicate if the SQ/RQ is opened and we will
set the READY state after finishing preparing all the SQ/RQ resources.

Fixes: 6e8dd6d6f4bd ("net/mlx5e: Don't wait for SQ completions on close")
Fixes: f2fde18c52a7 ("net/mlx5e: Don't wait for RQ completions on close")
	Signed-off-by: Mohamad Haj Yahia <mohamad@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c0f1147d14e4b09018a495c5095094e5707a4f44)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index b01f5bb32ed7,71382df59fc0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -206,9 -241,9 +206,13 @@@ struct mlx5e_tstamp 
  };
  
  enum {
++<<<<<<< HEAD
 +	MLX5E_RQ_STATE_POST_WQES_ENABLE,
++=======
+ 	MLX5E_RQ_STATE_ENABLED,
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
 -	MLX5E_RQ_STATE_AM,
 +	MLX5E_RQ_STATE_FLUSH_TIMEOUT,
  };
  
  struct mlx5e_cq {
@@@ -324,12 -394,11 +328,16 @@@ struct mlx5e_sq_dma 
  };
  
  enum {
++<<<<<<< HEAD
 +	MLX5E_SQ_STATE_WAKE_TXQ_ENABLE,
++=======
+ 	MLX5E_SQ_STATE_ENABLED,
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	MLX5E_SQ_STATE_BF_ENABLE,
 +	MLX5E_SQ_STATE_TX_TIMEOUT,
  };
  
 -struct mlx5e_sq_wqe_info {
 +struct mlx5e_ico_wqe_info {
  	u8  opcode;
  	u8  num_wqebbs;
  };
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 98d2ea9b4528,246d98ebb588..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -537,36 -784,62 +539,40 @@@ err_destroy_rq
  
  static void mlx5e_close_rq(struct mlx5e_rq *rq)
  {
++<<<<<<< HEAD
 +	int tout = 0;
 +	int err;
 +
 +	clear_bit(MLX5E_RQ_STATE_POST_WQES_ENABLE, &rq->state);
++=======
+ 	clear_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	napi_synchronize(&rq->channel->napi); /* prevent mlx5e_post_rx_wqes */
 -	cancel_work_sync(&rq->am.work);
 -
 -	mlx5e_disable_rq(rq);
 -	mlx5e_free_rx_descs(rq);
 -	mlx5e_destroy_rq(rq);
 -}
 -
 -static void mlx5e_free_sq_xdp_db(struct mlx5e_sq *sq)
 -{
 -	kfree(sq->db.xdp.di);
 -	kfree(sq->db.xdp.wqe_info);
 -}
 -
 -static int mlx5e_alloc_sq_xdp_db(struct mlx5e_sq *sq, int numa)
 -{
 -	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
 -
 -	sq->db.xdp.di = kzalloc_node(sizeof(*sq->db.xdp.di) * wq_sz,
 -				     GFP_KERNEL, numa);
 -	sq->db.xdp.wqe_info = kzalloc_node(sizeof(*sq->db.xdp.wqe_info) * wq_sz,
 -					   GFP_KERNEL, numa);
 -	if (!sq->db.xdp.di || !sq->db.xdp.wqe_info) {
 -		mlx5e_free_sq_xdp_db(sq);
 -		return -ENOMEM;
 -	}
 -
 -	return 0;
 -}
  
 -static void mlx5e_free_sq_ico_db(struct mlx5e_sq *sq)
 -{
 -	kfree(sq->db.ico_wqe);
 -}
 +	err = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RDY, MLX5_RQC_STATE_ERR);
 +	while (!mlx5_wq_ll_is_empty(&rq->wq) && !err &&
 +	       tout++ < MLX5_EN_QP_FLUSH_MAX_ITER)
 +		msleep(MLX5_EN_QP_FLUSH_MSLEEP_QUANT);
  
 -static int mlx5e_alloc_sq_ico_db(struct mlx5e_sq *sq, int numa)
 -{
 -	u8 wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
 +	if (err || tout == MLX5_EN_QP_FLUSH_MAX_ITER)
 +		set_bit(MLX5E_RQ_STATE_FLUSH_TIMEOUT, &rq->state);
  
 -	sq->db.ico_wqe = kzalloc_node(sizeof(*sq->db.ico_wqe) * wq_sz,
 -				      GFP_KERNEL, numa);
 -	if (!sq->db.ico_wqe)
 -		return -ENOMEM;
 +	/* avoid destroying rq before mlx5e_poll_rx_cq() is done with it */
 +	napi_synchronize(&rq->channel->napi);
  
 -	return 0;
 +	mlx5e_disable_rq(rq);
 +	mlx5e_free_rx_descs(rq);
 +	mlx5e_destroy_rq(rq);
  }
  
 -static void mlx5e_free_sq_txq_db(struct mlx5e_sq *sq)
 +static void mlx5e_free_sq_db(struct mlx5e_sq *sq)
  {
 -	kfree(sq->db.txq.wqe_info);
 -	kfree(sq->db.txq.dma_fifo);
 -	kfree(sq->db.txq.skb);
 +	kfree(sq->wqe_info);
 +	kfree(sq->dma_fifo);
 +	kfree(sq->skb);
  }
  
 -static int mlx5e_alloc_sq_txq_db(struct mlx5e_sq *sq, int numa)
 +static int mlx5e_alloc_sq_db(struct mlx5e_sq *sq, int numa)
  {
  	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
@@@ -773,7 -1084,9 +779,13 @@@ static int mlx5e_open_sq(struct mlx5e_c
  	if (err)
  		goto err_destroy_sq;
  
++<<<<<<< HEAD
 +	err = mlx5e_modify_sq(sq, MLX5_SQC_STATE_RST, MLX5_SQC_STATE_RDY);
++=======
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_modify_sq(sq, MLX5_SQC_STATE_RST, MLX5_SQC_STATE_RDY,
+ 			      false, 0);
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	if (err)
  		goto err_disable_sq;
  
@@@ -802,38 -1115,22 +815,44 @@@ static inline void netif_tx_disable_que
  
  static void mlx5e_close_sq(struct mlx5e_sq *sq)
  {
++<<<<<<< HEAD
 +	int tout = 0;
 +	int err;
++=======
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	/* prevent netif_tx_wake_queue */
+ 	napi_synchronize(&sq->channel->napi);
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  
  	if (sq->txq) {
 +		clear_bit(MLX5E_SQ_STATE_WAKE_TXQ_ENABLE, &sq->state);
 +		/* prevent netif_tx_wake_queue */
 +		napi_synchronize(&sq->channel->napi);
  		netif_tx_disable_queue(sq->txq);
  
 -		/* last doorbell out, godspeed .. */
 -		if (mlx5e_sq_has_room_for(sq, 1)) {
 -			sq->db.txq.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
 +		/* ensure hw is notified of all pending wqes */
 +		if (mlx5e_sq_has_room_for(sq, 1))
  			mlx5e_send_nop(sq, true);
 -		}
 +
 +		err = mlx5e_modify_sq(sq, MLX5_SQC_STATE_RDY,
 +				      MLX5_SQC_STATE_ERR);
 +		if (err)
 +			set_bit(MLX5E_SQ_STATE_TX_TIMEOUT, &sq->state);
  	}
  
 +	/* wait till sq is empty, unless a TX timeout occurred on this SQ */
 +	while (sq->cc != sq->pc &&
 +	       !test_bit(MLX5E_SQ_STATE_TX_TIMEOUT, &sq->state)) {
 +		msleep(MLX5_EN_QP_FLUSH_MSLEEP_QUANT);
 +		if (tout++ > MLX5_EN_QP_FLUSH_MAX_ITER)
 +			set_bit(MLX5E_SQ_STATE_TX_TIMEOUT, &sq->state);
 +	}
 +
 +	/* avoid destroying sq before mlx5e_poll_tx_cq() is done with it */
 +	napi_synchronize(&sq->channel->napi);
 +
 +	mlx5e_free_tx_descs(sq);
  	mlx5e_disable_sq(sq);
 -	mlx5e_free_sq_descs(sq);
  	mlx5e_destroy_sq(sq);
  }
  
@@@ -2574,6 -3080,128 +2593,131 @@@ static netdev_features_t mlx5e_features
  
  	return features;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void mlx5e_tx_timeout(struct net_device *dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	bool sched_work = false;
+ 	int i;
+ 
+ 	netdev_err(dev, "TX timeout detected\n");
+ 
+ 	for (i = 0; i < priv->params.num_channels * priv->params.num_tc; i++) {
+ 		struct mlx5e_sq *sq = priv->txq_to_sq_map[i];
+ 
+ 		if (!netif_xmit_stopped(netdev_get_tx_queue(dev, i)))
+ 			continue;
+ 		sched_work = true;
+ 		clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 		netdev_err(dev, "TX timeout on queue: %d, SQ: 0x%x, CQ: 0x%x, SQ Cons: 0x%x SQ Prod: 0x%x\n",
+ 			   i, sq->sqn, sq->cq.mcq.cqn, sq->cc, sq->pc);
+ 	}
+ 
+ 	if (sched_work && test_bit(MLX5E_STATE_OPENED, &priv->state))
+ 		schedule_work(&priv->tx_timeout_work);
+ }
+ 
+ static int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	struct bpf_prog *old_prog;
+ 	int err = 0;
+ 	bool reset, was_opened;
+ 	int i;
+ 
+ 	mutex_lock(&priv->state_lock);
+ 
+ 	if ((netdev->features & NETIF_F_LRO) && prog) {
+ 		netdev_warn(netdev, "can't set XDP while LRO is on, disable LRO first\n");
+ 		err = -EINVAL;
+ 		goto unlock;
+ 	}
+ 
+ 	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
+ 	/* no need for full reset when exchanging programs */
+ 	reset = (!priv->xdp_prog || !prog);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_close_locked(netdev);
+ 
+ 	/* exchange programs */
+ 	old_prog = xchg(&priv->xdp_prog, prog);
+ 	if (prog)
+ 		bpf_prog_add(prog, 1);
+ 	if (old_prog)
+ 		bpf_prog_put(old_prog);
+ 
+ 	if (reset) /* change RQ type according to priv->xdp_prog */
+ 		mlx5e_set_rq_priv_params(priv);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_open_locked(netdev);
+ 
+ 	if (!test_bit(MLX5E_STATE_OPENED, &priv->state) || reset)
+ 		goto unlock;
+ 
+ 	/* exchanging programs w/o reset, we update ref counts on behalf
+ 	 * of the channels RQs here.
+ 	 */
+ 	bpf_prog_add(prog, priv->params.num_channels);
+ 	for (i = 0; i < priv->params.num_channels; i++) {
+ 		struct mlx5e_channel *c = priv->channel[i];
+ 
+ 		clear_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		napi_synchronize(&c->napi);
+ 		/* prevent mlx5e_poll_rx_cq from accessing rq->xdp_prog */
+ 
+ 		old_prog = xchg(&c->rq.xdp_prog, prog);
+ 
+ 		set_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		/* napi_schedule in case we have missed anything */
+ 		set_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags);
+ 		napi_schedule(&c->napi);
+ 
+ 		if (old_prog)
+ 			bpf_prog_put(old_prog);
+ 	}
+ 
+ unlock:
+ 	mutex_unlock(&priv->state_lock);
+ 	return err;
+ }
+ 
+ static bool mlx5e_xdp_attached(struct net_device *dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 
+ 	return !!priv->xdp_prog;
+ }
+ 
+ static int mlx5e_xdp(struct net_device *dev, struct netdev_xdp *xdp)
+ {
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return mlx5e_xdp_set(dev, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = mlx5e_xdp_attached(dev);
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ #ifdef CONFIG_NET_POLL_CONTROLLER
+ /* Fake "interrupt" called by netpoll (eg netconsole) to send skbs without
+  * reenabling interrupts.
+  */
+ static void mlx5e_netpoll(struct net_device *dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	int i;
+ 
+ 	for (i = 0; i < priv->params.num_channels; i++)
+ 		napi_schedule(&priv->channel[i]->napi);
+ }
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  #endif
  
  static const struct net_device_ops mlx5e_netdev_ops_basic = {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index d795e95774bc,33495d88aeb2..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -506,8 -411,13 +506,17 @@@ void mlx5e_post_rx_fragmented_mpwqe(str
  	struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
  
  	clear_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state);
++<<<<<<< HEAD
++=======
+ 
+ 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state))) {
+ 		mlx5e_free_rx_mpwqe(rq, &rq->mpwqe.info[wq->head]);
+ 		return;
+ 	}
+ 
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 +	rq->stats.mpwqe_frag++;
  
  	/* ensure wqes are visible to device before updating doorbell record */
  	dma_wmb();
@@@ -613,8 -445,8 +622,13 @@@ void mlx5e_free_rx_descs(struct mlx5e_r
  }
  
  #define RQ_CANNOT_POST(rq) \
++<<<<<<< HEAD
 +		(!test_bit(MLX5E_RQ_STATE_POST_WQES_ENABLE, &rq->state) || \
 +		 test_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state))
++=======
+ 	(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state) || \
+ 	 test_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state))
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  
  bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq)
  {
@@@ -920,9 -921,10 +934,13 @@@ mpwrq_cqe_out
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
  {
  	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
 -	struct mlx5e_sq *xdp_sq = &rq->channel->xdp_sq;
  	int work_done = 0;
  
++<<<<<<< HEAD
 +	if (unlikely(test_bit(MLX5E_RQ_STATE_FLUSH_TIMEOUT, &rq->state)))
++=======
+ 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  		return 0;
  
  	if (cq->decmprs_left)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 4cd4781ab384,cfb68371c397..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -435,7 -409,7 +435,11 @@@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *
  
  	sq = container_of(cq, struct mlx5e_sq, cq);
  
++<<<<<<< HEAD
 +	if (unlikely(test_bit(MLX5E_SQ_STATE_TX_TIMEOUT, &sq->state)))
++=======
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  		return false;
  
  	npkts = 0;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index c38781fa567d,e5c12a732aa1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@@ -51,11 -51,14 +51,17 @@@ struct mlx5_cqe64 *mlx5e_get_cqe(struc
  
  static void mlx5e_poll_ico_cq(struct mlx5e_cq *cq)
  {
 -	struct mlx5e_sq *sq = container_of(cq, struct mlx5e_sq, cq);
  	struct mlx5_wq_cyc *wq;
  	struct mlx5_cqe64 *cqe;
 +	struct mlx5e_sq *sq;
  	u16 sqcc;
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return;
+ 
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  	cqe = mlx5e_get_cqe(cq);
  	if (likely(!cqe))
  		return;
@@@ -103,6 -105,66 +109,69 @@@
  	sq->cc = sqcc;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool mlx5e_poll_xdp_tx_cq(struct mlx5e_cq *cq)
+ {
+ 	struct mlx5e_sq *sq;
+ 	u16 sqcc;
+ 	int i;
+ 
+ 	sq = container_of(cq, struct mlx5e_sq, cq);
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return false;
+ 
+ 	/* sq->cc must be updated only after mlx5_cqwq_update_db_record(),
+ 	 * otherwise a cq overrun may occur
+ 	 */
+ 	sqcc = sq->cc;
+ 
+ 	for (i = 0; i < MLX5E_TX_CQ_POLL_BUDGET; i++) {
+ 		struct mlx5_cqe64 *cqe;
+ 		u16 wqe_counter;
+ 		bool last_wqe;
+ 
+ 		cqe = mlx5e_get_cqe(cq);
+ 		if (!cqe)
+ 			break;
+ 
+ 		mlx5_cqwq_pop(&cq->wq);
+ 
+ 		wqe_counter = be16_to_cpu(cqe->wqe_counter);
+ 
+ 		do {
+ 			struct mlx5e_sq_wqe_info *wi;
+ 			struct mlx5e_dma_info *di;
+ 			u16 ci;
+ 
+ 			last_wqe = (sqcc == wqe_counter);
+ 
+ 			ci = sqcc & sq->wq.sz_m1;
+ 			di = &sq->db.xdp.di[ci];
+ 			wi = &sq->db.xdp.wqe_info[ci];
+ 
+ 			if (unlikely(wi->opcode == MLX5_OPCODE_NOP)) {
+ 				sqcc++;
+ 				continue;
+ 			}
+ 
+ 			sqcc += wi->num_wqebbs;
+ 			/* Recycle RX page */
+ 			mlx5e_page_release(&sq->channel->rq, di, true);
+ 		} while (!last_wqe);
+ 	}
+ 
+ 	mlx5_cqwq_update_db_record(&cq->wq);
+ 
+ 	/* ensure cq space is freed before enabling more cqes */
+ 	wmb();
+ 
+ 	sq->cc = sqcc;
+ 	return (i == MLX5E_TX_CQ_POLL_BUDGET);
+ }
+ 
++>>>>>>> c0f1147d14e4 (net/mlx5e: Change the SQ/RQ operational state to positive logic)
  int mlx5e_napi_poll(struct napi_struct *napi, int budget)
  {
  	struct mlx5e_channel *c = container_of(napi, struct mlx5e_channel,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
