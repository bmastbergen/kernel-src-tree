xprtrdma: Move send_wr to struct rpcrdma_req

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 90aab6029606152d3d7ea91b41064580f77d7d19
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/90aab602.failed

Clean up: Most of the fields in each send_wr do not vary. There is
no need to initialize them before each ib_post_send(). This removes
a large-ish data structure from the stack.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 90aab6029606152d3d7ea91b41064580f77d7d19)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/verbs.c
index a763d5a5c6b0,79a6346b96c2..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -794,6 -848,11 +794,14 @@@ rpcrdma_create_req(struct rpcrdma_xprt 
  	spin_unlock(&buffer->rb_reqslock);
  	req->rl_cqe.done = rpcrdma_wc_send;
  	req->rl_buffer = &r_xprt->rx_buf;
++<<<<<<< HEAD
++=======
+ 	INIT_LIST_HEAD(&req->rl_registered);
+ 	req->rl_send_wr.next = NULL;
+ 	req->rl_send_wr.wr_cqe = &req->rl_cqe;
+ 	req->rl_send_wr.sg_list = req->rl_send_iov;
+ 	req->rl_send_wr.opcode = IB_WR_SEND;
++>>>>>>> 90aab6029606 (xprtrdma: Move send_wr to struct rpcrdma_req)
  	return req;
  }
  
@@@ -1151,43 -1263,39 +1159,37 @@@ rpcrdma_ep_post(struct rpcrdma_ia *ia
  		struct rpcrdma_req *req)
  {
  	struct ib_device *device = ia->ri_device;
- 	struct ib_send_wr send_wr, *send_wr_fail;
- 	struct rpcrdma_rep *rep = req->rl_reply;
- 	struct ib_sge *iov = req->rl_send_iov;
+ 	struct ib_send_wr *send_wr = &req->rl_send_wr;
+ 	struct ib_send_wr *send_wr_fail;
+ 	struct ib_sge *sge = req->rl_send_iov;
  	int i, rc;
  
- 	if (rep) {
- 		rc = rpcrdma_ep_post_recv(ia, rep);
+ 	if (req->rl_reply) {
+ 		rc = rpcrdma_ep_post_recv(ia, req->rl_reply);
  		if (rc)
 -			return rc;
 +			goto out;
  		req->rl_reply = NULL;
  	}
  
- 	send_wr.next = NULL;
- 	send_wr.wr_cqe = &req->rl_cqe;
- 	send_wr.sg_list = iov;
- 	send_wr.num_sge = req->rl_niovs;
- 	send_wr.opcode = IB_WR_SEND;
- 
- 	for (i = 0; i < send_wr.num_sge; i++)
- 		ib_dma_sync_single_for_device(device, iov[i].addr,
- 					      iov[i].length, DMA_TO_DEVICE);
+ 	for (i = 0; i < send_wr->num_sge; i++)
+ 		ib_dma_sync_single_for_device(device, sge[i].addr,
+ 					      sge[i].length, DMA_TO_DEVICE);
  	dprintk("RPC:       %s: posting %d s/g entries\n",
- 		__func__, send_wr.num_sge);
+ 		__func__, send_wr->num_sge);
  
  	if (DECR_CQCOUNT(ep) > 0)
- 		send_wr.send_flags = 0;
+ 		send_wr->send_flags = 0;
  	else { /* Provider must take a send completion every now and then */
  		INIT_CQCOUNT(ep);
- 		send_wr.send_flags = IB_SEND_SIGNALED;
+ 		send_wr->send_flags = IB_SEND_SIGNALED;
  	}
  
- 	rc = ib_post_send(ia->ri_id->qp, &send_wr, &send_wr_fail);
+ 	rc = ib_post_send(ia->ri_id->qp, send_wr, &send_wr_fail);
  	if (rc)
 -		goto out_postsend_err;
 -	return 0;
 -
 -out_postsend_err:
 -	pr_err("rpcrdma: RDMA Send ib_post_send returned %i\n", rc);
 -	return -ENOTCONN;
 +		dprintk("RPC:       %s: ib_post_send returned %i\n", __func__,
 +			rc);
 +out:
 +	return rc;
  }
  
  int
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index 3f48e7e27d19,3c5a89a4ff4f..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -285,19 -281,17 +285,23 @@@ struct rpcrdma_mr_seg {		/* chunk descr
  
  #define RPCRDMA_MAX_IOVS	(2)
  
 -struct rpcrdma_buffer;
  struct rpcrdma_req {
  	struct list_head	rl_free;
++<<<<<<< HEAD
 +	unsigned int		rl_niovs;
 +	unsigned int		rl_nchunks;
++=======
++>>>>>>> 90aab6029606 (xprtrdma: Move send_wr to struct rpcrdma_req)
  	unsigned int		rl_connect_cookie;
 +	struct rpc_task		*rl_task;
  	struct rpcrdma_buffer	*rl_buffer;
- 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
+ 	struct rpcrdma_rep	*rl_reply;
+ 	struct ib_send_wr	rl_send_wr;
  	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 -	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 -	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
 -	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 +	struct rpcrdma_regbuf	*rl_rdmabuf;
 +	struct rpcrdma_regbuf	*rl_sendbuf;
 +	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 +	struct rpcrdma_mr_seg	*rl_nextseg;
  
  	struct ib_cqe		rl_cqe;
  	struct list_head	rl_all;
diff --git a/net/sunrpc/xprtrdma/backchannel.c b/net/sunrpc/xprtrdma/backchannel.c
index 5b6f45443e47..ecec51a82869 100644
--- a/net/sunrpc/xprtrdma/backchannel.c
+++ b/net/sunrpc/xprtrdma/backchannel.c
@@ -240,7 +240,8 @@ int rpcrdma_bc_marshal_reply(struct rpc_rqst *rqst)
 	req->rl_send_iov[1].length = rpclen;
 	req->rl_send_iov[1].lkey = rdmab_lkey(req->rl_sendbuf);
 
-	req->rl_niovs = 2;
+	req->rl_send_wr.num_sge = 2;
+
 	return 0;
 }
 
diff --git a/net/sunrpc/xprtrdma/rpc_rdma.c b/net/sunrpc/xprtrdma/rpc_rdma.c
index 4fe71c22d4ff..f7fd0be8e391 100644
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@ -835,7 +835,7 @@ rpcrdma_marshal_req(struct rpc_rqst *rqst)
 	req->rl_send_iov[0].length = hdrlen;
 	req->rl_send_iov[0].lkey = rdmab_lkey(req->rl_rdmabuf);
 
-	req->rl_niovs = 1;
+	req->rl_send_wr.num_sge = 1;
 	if (rtype == rpcrdma_areadch)
 		return 0;
 
@@ -843,7 +843,8 @@ rpcrdma_marshal_req(struct rpc_rqst *rqst)
 	req->rl_send_iov[1].length = rpclen;
 	req->rl_send_iov[1].lkey = rdmab_lkey(req->rl_sendbuf);
 
-	req->rl_niovs = 2;
+	req->rl_send_wr.num_sge = 2;
+
 	return 0;
 
 out_overflow:
* Unmerged path net/sunrpc/xprtrdma/verbs.c
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
