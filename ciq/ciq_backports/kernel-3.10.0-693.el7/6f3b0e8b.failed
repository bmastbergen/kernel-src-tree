blk-mq: add a flags parameter to blk_mq_alloc_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 6f3b0e8bcf3cbb87a7459b3ed018d31d918df3f8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/6f3b0e8b.failed

We already have the reserved flag, and a nowait flag awkwardly encoded as
a gfp_t.  Add a real flags argument to make the scheme more extensible and
allow for a nicer calling convention.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 6f3b0e8bcf3cbb87a7459b3ed018d31d918df3f8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq-tag.c
#	block/blk-mq.c
#	drivers/block/mtip32xx/mtip32xx.c
#	drivers/block/null_blk.c
#	drivers/nvme/host/lightnvm.c
#	drivers/nvme/host/pci.c
#	include/linux/blkdev.h
diff --cc block/blk-core.c
index defb77328143,5ec996036e16..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -591,16 -630,7 +591,20 @@@ struct request_queue *blk_alloc_queue(g
  }
  EXPORT_SYMBOL(blk_alloc_queue);
  
++<<<<<<< HEAD
 +static void queue_limits_init_aux(struct request_queue *q, struct queue_limits_aux *limits_aux)
 +{
 +	if (!limits_aux)
 +	    return;
 +
 +	memset(limits_aux, 0, sizeof(*limits_aux));
 +	q->limits.limits_aux = limits_aux;
 +}
 +
 +int blk_queue_enter(struct request_queue *q, gfp_t gfp)
++=======
+ int blk_queue_enter(struct request_queue *q, bool nowait)
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  {
  	while (true) {
  		int ret;
@@@ -608,7 -638,7 +612,11 @@@
  		if (percpu_ref_tryget_live(&q->q_usage_counter))
  			return 0;
  
++<<<<<<< HEAD
 +		if (!(gfp & __GFP_WAIT))
++=======
+ 		if (nowait)
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  			return -EBUSY;
  
  		ret = wait_event_interruptible(q->mq_freeze_wq,
@@@ -2010,9 -2046,8 +2020,14 @@@ void generic_make_request(struct bio *b
  	do {
  		struct request_queue *q = bdev_get_queue(bio->bi_bdev);
  
++<<<<<<< HEAD
 +		if (likely(blk_queue_enter(q, __GFP_WAIT) == 0)) {
 +
 +			q->make_request_fn(q, bio);
++=======
+ 		if (likely(blk_queue_enter(q, false) == 0)) {
+ 			ret = q->make_request_fn(q, bio);
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  
  			blk_queue_exit(q);
  
diff --cc block/blk-mq-tag.c
index d97f0822078c,abdbb47405cb..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -264,7 -268,7 +264,11 @@@ static int bt_get(struct blk_mq_alloc_d
  	if (tag != -1)
  		return tag;
  
++<<<<<<< HEAD
 +	if (!(data->gfp & __GFP_WAIT))
++=======
+ 	if (data->flags & BLK_MQ_REQ_NOWAIT)
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  		return -1;
  
  	bs = bt_wait_ptr(bt, hctx);
diff --cc block/blk-mq.c
index 3caf05b09404,93a4e1956915..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -244,11 -244,10 +244,18 @@@ struct request *blk_mq_alloc_request(st
  
  	ctx = blk_mq_get_ctx(q);
  	hctx = q->mq_ops->map_queue(q, ctx->cpu);
++<<<<<<< HEAD
 +	blk_mq_set_alloc_data(&alloc_data, q, gfp & ~__GFP_WAIT,
 +			reserved, ctx, hctx);
 +
 +	rq = __blk_mq_alloc_request(&alloc_data, rw);
 +	if (!rq && (gfp & __GFP_WAIT)) {
++=======
+ 	blk_mq_set_alloc_data(&alloc_data, q, flags, ctx, hctx);
+ 
+ 	rq = __blk_mq_alloc_request(&alloc_data, rw);
+ 	if (!rq && !(flags & BLK_MQ_REQ_NOWAIT)) {
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  		__blk_mq_run_hw_queue(hctx);
  		blk_mq_put_ctx(ctx);
  
@@@ -1214,8 -1182,7 +1219,12 @@@ static struct request *blk_mq_map_reque
  
  		ctx = blk_mq_get_ctx(q);
  		hctx = q->mq_ops->map_queue(q, ctx->cpu);
++<<<<<<< HEAD
 +		blk_mq_set_alloc_data(&alloc_data, q,
 +				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);
++=======
+ 		blk_mq_set_alloc_data(&alloc_data, q, 0, ctx, hctx);
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  		rq = __blk_mq_alloc_request(&alloc_data, rw);
  		ctx = alloc_data.ctx;
  		hctx = alloc_data.hctx;
diff --cc drivers/block/mtip32xx/mtip32xx.c
index 0ac1aff3e74e,10bd8d0a9d9c..000000000000
--- a/drivers/block/mtip32xx/mtip32xx.c
+++ b/drivers/block/mtip32xx/mtip32xx.c
@@@ -173,13 -173,7 +173,17 @@@ static struct mtip_cmd *mtip_get_int_co
  {
  	struct request *rq;
  
++<<<<<<< HEAD
 +	if (mtip_check_surprise_removal(dd->pdev))
 +		return NULL;
 +
 +	rq = blk_mq_alloc_request(dd->queue, 0, __GFP_WAIT, true);
 +	if (IS_ERR(rq))
 +		return NULL;
 +
++=======
+ 	rq = blk_mq_alloc_request(dd->queue, 0, BLK_MQ_REQ_RESERVED);
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  	return blk_mq_rq_to_pdu(rq);
  }
  
diff --cc drivers/block/null_blk.c
index afe77ddcd8ce,fa742dddf3f8..000000000000
--- a/drivers/block/null_blk.c
+++ b/drivers/block/null_blk.c
@@@ -368,6 -447,143 +368,146 @@@ static void null_del_dev(struct nullb *
  	kfree(nullb);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NVM
+ 
+ static void null_lnvm_end_io(struct request *rq, int error)
+ {
+ 	struct nvm_rq *rqd = rq->end_io_data;
+ 	struct nvm_dev *dev = rqd->dev;
+ 
+ 	dev->mt->end_io(rqd, error);
+ 
+ 	blk_put_request(rq);
+ }
+ 
+ static int null_lnvm_submit_io(struct request_queue *q, struct nvm_rq *rqd)
+ {
+ 	struct request *rq;
+ 	struct bio *bio = rqd->bio;
+ 
+ 	rq = blk_mq_alloc_request(q, bio_rw(bio), 0);
+ 	if (IS_ERR(rq))
+ 		return -ENOMEM;
+ 
+ 	rq->cmd_type = REQ_TYPE_DRV_PRIV;
+ 	rq->__sector = bio->bi_iter.bi_sector;
+ 	rq->ioprio = bio_prio(bio);
+ 
+ 	if (bio_has_data(bio))
+ 		rq->nr_phys_segments = bio_phys_segments(q, bio);
+ 
+ 	rq->__data_len = bio->bi_iter.bi_size;
+ 	rq->bio = rq->biotail = bio;
+ 
+ 	rq->end_io_data = rqd;
+ 
+ 	blk_execute_rq_nowait(q, NULL, rq, 0, null_lnvm_end_io);
+ 
+ 	return 0;
+ }
+ 
+ static int null_lnvm_id(struct request_queue *q, struct nvm_id *id)
+ {
+ 	sector_t size = gb * 1024 * 1024 * 1024ULL;
+ 	sector_t blksize;
+ 	struct nvm_id_group *grp;
+ 
+ 	id->ver_id = 0x1;
+ 	id->vmnt = 0;
+ 	id->cgrps = 1;
+ 	id->cap = 0x3;
+ 	id->dom = 0x1;
+ 
+ 	id->ppaf.blk_offset = 0;
+ 	id->ppaf.blk_len = 16;
+ 	id->ppaf.pg_offset = 16;
+ 	id->ppaf.pg_len = 16;
+ 	id->ppaf.sect_offset = 32;
+ 	id->ppaf.sect_len = 8;
+ 	id->ppaf.pln_offset = 40;
+ 	id->ppaf.pln_len = 8;
+ 	id->ppaf.lun_offset = 48;
+ 	id->ppaf.lun_len = 8;
+ 	id->ppaf.ch_offset = 56;
+ 	id->ppaf.ch_len = 8;
+ 
+ 	do_div(size, bs); /* convert size to pages */
+ 	do_div(size, 256); /* concert size to pgs pr blk */
+ 	grp = &id->groups[0];
+ 	grp->mtype = 0;
+ 	grp->fmtype = 0;
+ 	grp->num_ch = 1;
+ 	grp->num_pg = 256;
+ 	blksize = size;
+ 	do_div(size, (1 << 16));
+ 	grp->num_lun = size + 1;
+ 	do_div(blksize, grp->num_lun);
+ 	grp->num_blk = blksize;
+ 	grp->num_pln = 1;
+ 
+ 	grp->fpg_sz = bs;
+ 	grp->csecs = bs;
+ 	grp->trdt = 25000;
+ 	grp->trdm = 25000;
+ 	grp->tprt = 500000;
+ 	grp->tprm = 500000;
+ 	grp->tbet = 1500000;
+ 	grp->tbem = 1500000;
+ 	grp->mpos = 0x010101; /* single plane rwe */
+ 	grp->cpar = hw_queue_depth;
+ 
+ 	return 0;
+ }
+ 
+ static void *null_lnvm_create_dma_pool(struct request_queue *q, char *name)
+ {
+ 	mempool_t *virtmem_pool;
+ 
+ 	virtmem_pool = mempool_create_slab_pool(64, ppa_cache);
+ 	if (!virtmem_pool) {
+ 		pr_err("null_blk: Unable to create virtual memory pool\n");
+ 		return NULL;
+ 	}
+ 
+ 	return virtmem_pool;
+ }
+ 
+ static void null_lnvm_destroy_dma_pool(void *pool)
+ {
+ 	mempool_destroy(pool);
+ }
+ 
+ static void *null_lnvm_dev_dma_alloc(struct request_queue *q, void *pool,
+ 				gfp_t mem_flags, dma_addr_t *dma_handler)
+ {
+ 	return mempool_alloc(pool, mem_flags);
+ }
+ 
+ static void null_lnvm_dev_dma_free(void *pool, void *entry,
+ 							dma_addr_t dma_handler)
+ {
+ 	mempool_free(entry, pool);
+ }
+ 
+ static struct nvm_dev_ops null_lnvm_dev_ops = {
+ 	.identity		= null_lnvm_id,
+ 	.submit_io		= null_lnvm_submit_io,
+ 
+ 	.create_dma_pool	= null_lnvm_create_dma_pool,
+ 	.destroy_dma_pool	= null_lnvm_destroy_dma_pool,
+ 	.dev_dma_alloc		= null_lnvm_dev_dma_alloc,
+ 	.dev_dma_free		= null_lnvm_dev_dma_free,
+ 
+ 	/* Simulate nvme protocol restriction */
+ 	.max_phys_sect		= 64,
+ };
+ #else
+ static struct nvm_dev_ops null_lnvm_dev_ops;
+ #endif /* CONFIG_NVM */
+ 
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  static int null_open(struct block_device *bdev, fmode_t mode)
  {
  	return 0;
diff --cc drivers/nvme/host/pci.c
index 6cf19a97b3a4,b8a02221233c..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -746,22 -1011,129 +746,146 @@@ static irqreturn_t nvme_irq_check(int i
  	return IRQ_WAKE_THREAD;
  }
  
++<<<<<<< HEAD
 +static void nvme_async_event_work(struct work_struct *work)
++=======
+ static int nvme_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
+ {
+ 	struct nvme_queue *nvmeq = hctx->driver_data;
+ 
+ 	if ((le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
+ 	    nvmeq->cq_phase) {
+ 		spin_lock_irq(&nvmeq->q_lock);
+ 		__nvme_process_cq(nvmeq, &tag);
+ 		spin_unlock_irq(&nvmeq->q_lock);
+ 
+ 		if (tag == -1)
+ 			return 1;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Returns 0 on success.  If the result is negative, it's a Linux error code;
+  * if the result is positive, it's an NVM Express status code
+  */
+ int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+ 		void *buffer, void __user *ubuffer, unsigned bufflen,
+ 		u32 *result, unsigned timeout)
+ {
+ 	bool write = cmd->common.opcode & 1;
+ 	struct bio *bio = NULL;
+ 	struct request *req;
+ 	int ret;
+ 
+ 	req = blk_mq_alloc_request(q, write, 0);
+ 	if (IS_ERR(req))
+ 		return PTR_ERR(req);
+ 
+ 	req->cmd_type = REQ_TYPE_DRV_PRIV;
+ 	req->cmd_flags |= REQ_FAILFAST_DRIVER;
+ 	req->__data_len = 0;
+ 	req->__sector = (sector_t) -1;
+ 	req->bio = req->biotail = NULL;
+ 
+ 	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ 
+ 	req->cmd = (unsigned char *)cmd;
+ 	req->cmd_len = sizeof(struct nvme_command);
+ 	req->special = (void *)0;
+ 
+ 	if (buffer && bufflen) {
+ 		ret = blk_rq_map_kern(q, req, buffer, bufflen,
+ 				      __GFP_DIRECT_RECLAIM);
+ 		if (ret)
+ 			goto out;
+ 	} else if (ubuffer && bufflen) {
+ 		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+ 				      __GFP_DIRECT_RECLAIM);
+ 		if (ret)
+ 			goto out;
+ 		bio = req->bio;
+ 	}
+ 
+ 	blk_execute_rq(req->q, NULL, req, 0);
+ 	if (bio)
+ 		blk_rq_unmap_user(bio);
+ 	if (result)
+ 		*result = (u32)(uintptr_t)req->special;
+ 	ret = req->errors;
+  out:
+ 	blk_mq_free_request(req);
+ 	return ret;
+ }
+ 
+ int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+ 		void *buffer, unsigned bufflen)
+ {
+ 	return __nvme_submit_sync_cmd(q, cmd, buffer, NULL, bufflen, NULL, 0);
+ }
+ 
+ static int nvme_submit_async_admin_req(struct nvme_dev *dev)
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  {
 +	struct nvme_dev *dev = container_of(work, struct nvme_dev, async_work);
  	struct nvme_queue *nvmeq = dev->queues[0];
  	struct nvme_command c;
++<<<<<<< HEAD
++=======
+ 	struct nvme_cmd_info *cmd_info;
+ 	struct request *req;
+ 
+ 	req = blk_mq_alloc_request(dev->admin_q, WRITE,
+ 			BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_RESERVED);
+ 	if (IS_ERR(req))
+ 		return PTR_ERR(req);
+ 
+ 	req->cmd_flags |= REQ_NO_TIMEOUT;
+ 	cmd_info = blk_mq_rq_to_pdu(req);
+ 	nvme_set_info(cmd_info, NULL, async_req_completion);
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  
  	memset(&c, 0, sizeof(c));
  	c.common.opcode = nvme_admin_async_event;
 -	c.common.command_id = req->tag;
  
++<<<<<<< HEAD
 +	spin_lock_irq(&nvmeq->q_lock);
 +	while (dev->ctrl.event_limit > 0) {
 +		c.common.command_id = NVME_AQ_BLKMQ_DEPTH +
 +			--dev->ctrl.event_limit;
 +		__nvme_submit_cmd(nvmeq, &c);
 +	}
 +	spin_unlock_irq(&nvmeq->q_lock);
++=======
+ 	blk_mq_free_request(req);
+ 	__nvme_submit_cmd(nvmeq, &c);
+ 	return 0;
+ }
+ 
+ static int nvme_submit_admin_async_cmd(struct nvme_dev *dev,
+ 			struct nvme_command *cmd,
+ 			struct async_cmd_info *cmdinfo, unsigned timeout)
+ {
+ 	struct nvme_queue *nvmeq = dev->queues[0];
+ 	struct request *req;
+ 	struct nvme_cmd_info *cmd_rq;
+ 
+ 	req = blk_mq_alloc_request(dev->admin_q, WRITE, 0);
+ 	if (IS_ERR(req))
+ 		return PTR_ERR(req);
+ 
+ 	req->timeout = timeout;
+ 	cmd_rq = blk_mq_rq_to_pdu(req);
+ 	cmdinfo->req = req;
+ 	nvme_set_info(cmd_rq, cmdinfo, async_completion);
+ 	cmdinfo->status = -EINTR;
+ 
+ 	cmd->common.command_id = req->tag;
+ 
+ 	nvme_submit_cmd(nvmeq, cmd);
+ 	return 0;
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  }
  
  static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
@@@ -827,68 -1199,135 +951,75 @@@ static int adapter_delete_sq(struct nvm
  	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
  }
  
 -int nvme_identify_ctrl(struct nvme_dev *dev, struct nvme_id_ctrl **id)
 -{
 -	struct nvme_command c = { };
 -	int error;
 -
 -	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
 -	c.identify.opcode = nvme_admin_identify;
 -	c.identify.cns = cpu_to_le32(1);
 -
 -	*id = kmalloc(sizeof(struct nvme_id_ctrl), GFP_KERNEL);
 -	if (!*id)
 -		return -ENOMEM;
 -
 -	error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
 -			sizeof(struct nvme_id_ctrl));
 -	if (error)
 -		kfree(*id);
 -	return error;
 -}
 -
 -int nvme_identify_ns(struct nvme_dev *dev, unsigned nsid,
 -		struct nvme_id_ns **id)
 -{
 -	struct nvme_command c = { };
 -	int error;
 -
 -	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
 -	c.identify.opcode = nvme_admin_identify,
 -	c.identify.nsid = cpu_to_le32(nsid),
 -
 -	*id = kmalloc(sizeof(struct nvme_id_ns), GFP_KERNEL);
 -	if (!*id)
 -		return -ENOMEM;
 -
 -	error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
 -			sizeof(struct nvme_id_ns));
 -	if (error)
 -		kfree(*id);
 -	return error;
 -}
 -
 -int nvme_get_features(struct nvme_dev *dev, unsigned fid, unsigned nsid,
 -					dma_addr_t dma_addr, u32 *result)
 -{
 -	struct nvme_command c;
 -
 -	memset(&c, 0, sizeof(c));
 -	c.features.opcode = nvme_admin_get_features;
 -	c.features.nsid = cpu_to_le32(nsid);
 -	c.features.prp1 = cpu_to_le64(dma_addr);
 -	c.features.fid = cpu_to_le32(fid);
 -
 -	return __nvme_submit_sync_cmd(dev->admin_q, &c, NULL, NULL, 0,
 -			result, 0);
 -}
 -
 -int nvme_set_features(struct nvme_dev *dev, unsigned fid, unsigned dword11,
 -					dma_addr_t dma_addr, u32 *result)
 -{
 -	struct nvme_command c;
 -
 -	memset(&c, 0, sizeof(c));
 -	c.features.opcode = nvme_admin_set_features;
 -	c.features.prp1 = cpu_to_le64(dma_addr);
 -	c.features.fid = cpu_to_le32(fid);
 -	c.features.dword11 = cpu_to_le32(dword11);
 -
 -	return __nvme_submit_sync_cmd(dev->admin_q, &c, NULL, NULL, 0,
 -			result, 0);
 -}
 -
 -int nvme_get_log_page(struct nvme_dev *dev, struct nvme_smart_log **log)
 +static void abort_endio(struct request *req, int error)
  {
 -	struct nvme_command c = { };
 -	int error;
 +	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 +	struct nvme_queue *nvmeq = iod->nvmeq;
 +	u32 result = (u32)(uintptr_t)req->special;
 +	u16 status = req->errors;
  
 -	c.common.opcode = nvme_admin_get_log_page,
 -	c.common.nsid = cpu_to_le32(0xFFFFFFFF),
 -	c.common.cdw10[0] = cpu_to_le32(
 -			(((sizeof(struct nvme_smart_log) / 4) - 1) << 16) |
 -			 NVME_LOG_SMART),
 -
 -	*log = kmalloc(sizeof(struct nvme_smart_log), GFP_KERNEL);
 -	if (!*log)
 -		return -ENOMEM;
 +	dev_warn(nvmeq->q_dmadev, "Abort status:%x result:%x", status, result);
 +	atomic_inc(&nvmeq->dev->ctrl.abort_limit);
  
 -	error = nvme_submit_sync_cmd(dev->admin_q, &c, *log,
 -			sizeof(struct nvme_smart_log));
 -	if (error)
 -		kfree(*log);
 -	return error;
 +	blk_mq_free_request(req);
  }
  
 -/**
 - * nvme_abort_req - Attempt aborting a request
 - *
 - * Schedule controller reset if the command was already aborted once before and
 - * still hasn't been returned to the driver, or if this is the admin queue.
 - */
 -static void nvme_abort_req(struct request *req)
 +static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
  {
 -	struct nvme_cmd_info *cmd_rq = blk_mq_rq_to_pdu(req);
 -	struct nvme_queue *nvmeq = cmd_rq->nvmeq;
 +	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 +	struct nvme_queue *nvmeq = iod->nvmeq;
  	struct nvme_dev *dev = nvmeq->dev;
  	struct request *abort_req;
 -	struct nvme_cmd_info *abort_cmd;
  	struct nvme_command cmd;
  
 -	if (!nvmeq->qid || cmd_rq->aborted) {
 -		spin_lock(&dev_list_lock);
 -		if (!__nvme_reset(dev)) {
 -			dev_warn(dev->dev,
 -				 "I/O %d QID %d timeout, reset controller\n",
 -				 req->tag, nvmeq->qid);
 -		}
 -		spin_unlock(&dev_list_lock);
 -		return;
 +	/*
 +	 * Shutdown immediately if controller times out while starting. The
 +	 * reset work will see the pci device disabled when it gets the forced
 +	 * cancellation error. All outstanding requests are completed on
 +	 * shutdown, so we return BLK_EH_HANDLED.
 +	 */
 +	if (test_bit(NVME_CTRL_RESETTING, &dev->flags)) {
 +		dev_warn(dev->dev,
 +			 "I/O %d QID %d timeout, disable controller\n",
 +			 req->tag, nvmeq->qid);
 +		nvme_dev_disable(dev, false);
 +		req->errors = NVME_SC_CANCELLED;
 +		return BLK_EH_HANDLED;
  	}
  
 -	if (!dev->abort_limit)
 -		return;
 -
 +	/*
 + 	 * Shutdown the controller immediately and schedule a reset if the
 + 	 * command was already aborted once before and still hasn't been
 + 	 * returned to the driver, or if this is the admin queue.
 +	 */
 +	if (!nvmeq->qid || iod->aborted) {
 +		dev_warn(dev->dev,
 +			 "I/O %d QID %d timeout, reset controller\n",
 +			 req->tag, nvmeq->qid);
 +		nvme_dev_disable(dev, false);
 +		queue_work(nvme_workq, &dev->reset_work);
 +
++<<<<<<< HEAD
 +		/*
 +		 * Mark the request as handled, since the inline shutdown
 +		 * forces all outstanding requests to complete.
 +		 */
 +		req->errors = NVME_SC_CANCELLED;
 +		return BLK_EH_HANDLED;
 +	}
++=======
+ 	abort_req = blk_mq_alloc_request(dev->admin_q, WRITE,
+ 			BLK_MQ_REQ_NOWAIT);
+ 	if (IS_ERR(abort_req))
+ 		return;
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
 +
 +	iod->aborted = 1;
  
 -	abort_cmd = blk_mq_rq_to_pdu(abort_req);
 -	nvme_set_info(abort_cmd, abort_req, abort_completion);
 +	if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
 +		atomic_inc(&dev->ctrl.abort_limit);
 +		return BLK_EH_RESET_TIMER;
 +	}
  
  	memset(&cmd, 0, sizeof(cmd));
  	cmd.abort.opcode = nvme_admin_abort_cmd;
diff --cc include/linux/blkdev.h
index d0c02b856b7c,e711f294934c..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -876,28 -794,7 +876,32 @@@ extern int scsi_cmd_ioctl(struct reques
  extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
  			 struct scsi_ioctl_command __user *);
  
++<<<<<<< HEAD
 +extern void blk_queue_bio(struct request_queue *q, struct bio *bio);
 +
 +/*
 + * A queue has just exitted congestion.  Note this in the global counter of
 + * congested queues, and wake up anyone who was waiting for requests to be
 + * put back.
 + */
 +static inline void blk_clear_queue_congested(struct request_queue *q, int sync)
 +{
 +	clear_bdi_congested(&q->backing_dev_info, sync);
 +}
 +
 +/*
 + * A queue has just entered congestion.  Flag that in the queue's VM-visible
 + * state flags and increment the global gounter of congested queues.
 + */
 +static inline void blk_set_queue_congested(struct request_queue *q, int sync)
 +{
 +	set_bdi_congested(&q->backing_dev_info, sync);
 +}
 +
 +extern int blk_queue_enter(struct request_queue *q, gfp_t gfp);
++=======
+ extern int blk_queue_enter(struct request_queue *q, bool nowait);
++>>>>>>> 6f3b0e8bcf3c (blk-mq: add a flags parameter to blk_mq_alloc_request)
  extern void blk_queue_exit(struct request_queue *q);
  extern void blk_start_queue(struct request_queue *q);
  extern void blk_stop_queue(struct request_queue *q);
* Unmerged path drivers/nvme/host/lightnvm.c
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq-tag.c
* Unmerged path block/blk-mq.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 5da28965c4af..87e8ee37cfde 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -100,8 +100,7 @@ static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
 struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
-	gfp_t gfp;
-	bool reserved;
+	unsigned int flags;
 
 	/* input & output parameter */
 	struct blk_mq_ctx *ctx;
@@ -109,13 +108,11 @@ struct blk_mq_alloc_data {
 };
 
 static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
-		struct request_queue *q, gfp_t gfp, bool reserved,
-		struct blk_mq_ctx *ctx,
-		struct blk_mq_hw_ctx *hctx)
+		struct request_queue *q, unsigned int flags,
+		struct blk_mq_ctx *ctx, struct blk_mq_hw_ctx *hctx)
 {
 	data->q = q;
-	data->gfp = gfp;
-	data->reserved = reserved;
+	data->flags = flags;
 	data->ctx = ctx;
 	data->hctx = hctx;
 }
* Unmerged path drivers/block/mtip32xx/mtip32xx.c
* Unmerged path drivers/block/null_blk.c
* Unmerged path drivers/nvme/host/lightnvm.c
* Unmerged path drivers/nvme/host/pci.c
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 7d6ea6f10f58..75f37fb99b67 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -481,7 +481,7 @@ int bdev_read_page(struct block_device *bdev, sector_t sector,
 	if (!ops->rw_page || bdev_get_integrity(bdev))
 		return result;
 
-	result = blk_queue_enter(bdev->bd_queue, GFP_KERNEL);
+	result = blk_queue_enter(bdev->bd_queue, false);
 	if (result)
 		return result;
 	result = ops->rw_page(bdev, sector + get_start_sect(bdev), page, READ);
@@ -518,7 +518,7 @@ int bdev_write_page(struct block_device *bdev, sector_t sector,
 
 	if (!ops->rw_page || bdev_get_integrity(bdev))
 		return -EOPNOTSUPP;
-	result = blk_queue_enter(bdev->bd_queue, GFP_KERNEL);
+	result = blk_queue_enter(bdev->bd_queue, false);
 	if (result)
 		return result;
 
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0e7b0244ac77..00d3f90be9b0 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -221,8 +221,14 @@ void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_free_request(struct request *rq);
 void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
+
+enum {
+	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
+	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
+};
+
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
-		gfp_t gfp, bool reserved);
+		unsigned int flags);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
 
* Unmerged path include/linux/blkdev.h
