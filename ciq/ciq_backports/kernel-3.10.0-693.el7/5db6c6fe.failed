locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Davidlohr Bueso <davidlohr@hp.com>
commit 5db6c6fefb1ca0e81e3bd6dd8998bf51c453d823
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/5db6c6fe.failed

Just like with mutexes (CONFIG_MUTEX_SPIN_ON_OWNER),
encapsulate the dependencies for rwsem optimistic spinning.
No logical changes here as it continues to depend on both
SMP and the XADD algorithm variant.

	Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
	Acked-by: Jason Low <jason.low2@hp.com>
[ Also make it depend on ARCH_SUPPORTS_ATOMIC_RMW. ]
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1405112406-13052-2-git-send-email-davidlohr@hp.com
	Cc: aswin@hp.com
	Cc: Chris Mason <clm@fb.com>
	Cc: Davidlohr Bueso <davidlohr@hp.com>
	Cc: Josef Bacik <jbacik@fusionio.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Waiman Long <Waiman.Long@hp.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>

	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 5db6c6fefb1ca0e81e3bd6dd8998bf51c453d823)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rwsem.h
#	kernel/Kconfig.locks
diff --cc include/linux/rwsem.h
index 8d79708146aa,035d3c57fc8a..000000000000
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@@ -13,10 -13,11 +13,17 @@@
  #include <linux/kernel.h>
  #include <linux/list.h>
  #include <linux/spinlock.h>
++<<<<<<< HEAD
++=======
+ #include <linux/atomic.h>
+ #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
+ #include <linux/osq_lock.h>
+ #endif
++>>>>>>> 5db6c6fefb1c (locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER)
  
 +#include <linux/atomic.h>
 +
 +struct optimistic_spin_queue;
  struct rw_semaphore;
  
  #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
@@@ -25,9 -26,10 +32,14 @@@
  /* All arch specific implementations share the same struct */
  struct rw_semaphore {
  	long count;
 -	struct list_head wait_list;
  	raw_spinlock_t wait_lock;
++<<<<<<< HEAD
 +	struct list_head wait_list;
 +#ifdef CONFIG_SMP
++=======
+ #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
+ 	struct optimistic_spin_queue osq; /* spinner MCS lock */
++>>>>>>> 5db6c6fefb1c (locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER)
  	/*
  	 * Write owner. Used as a speculative check to see
  	 * if the owner is running on the cpu.
@@@ -64,21 -65,18 +76,35 @@@ static inline int rwsem_is_locked(struc
  # define __RWSEM_DEP_MAP_INIT(lockname)
  #endif
  
++<<<<<<< HEAD
 +#if defined(CONFIG_SMP) && !defined(CONFIG_RWSEM_GENERIC_SPINLOCK)
 +#define __RWSEM_INITIALIZER(name)			\
 +	{ RWSEM_UNLOCKED_VALUE,				\
 +	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
 +	  LIST_HEAD_INIT((name).wait_list),		\
 +	  NULL, /* owner */				\
 +	  NULL /* mcs lock */                           \
++=======
+ #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
+ #define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED, .owner = NULL
+ #else
+ #define __RWSEM_OPT_INIT(lockname)
+ #endif
+ 
+ #define __RWSEM_INITIALIZER(name)				\
+ 	{ .count = RWSEM_UNLOCKED_VALUE,			\
+ 	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
+ 	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
+ 	  __RWSEM_OPT_INIT(name)				\
++>>>>>>> 5db6c6fefb1c (locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER)
  	  __RWSEM_DEP_MAP_INIT(name) }
 +#else
 +#define __RWSEM_INITIALIZER(name)			\
 +	{ RWSEM_UNLOCKED_VALUE,				\
 +	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
 +	  LIST_HEAD_INIT((name).wait_list)		\
 +	  __RWSEM_DEP_MAP_INIT(name) }
 +#endif
  
  #define DECLARE_RWSEM(name) \
  	struct rw_semaphore name = __RWSEM_INITIALIZER(name)
diff --cc kernel/Kconfig.locks
index 44511d100eaa,76768ee812b2..000000000000
--- a/kernel/Kconfig.locks
+++ b/kernel/Kconfig.locks
@@@ -220,6 -220,20 +220,21 @@@ config INLINE_WRITE_UNLOCK_IRQRESTOR
  
  endif
  
 -config ARCH_SUPPORTS_ATOMIC_RMW
 -	bool
 -
  config MUTEX_SPIN_ON_OWNER
  	def_bool y
++<<<<<<< HEAD
 +	depends on SMP && !DEBUG_MUTEXES
++=======
+ 	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config RWSEM_SPIN_ON_OWNER
+        def_bool y
+        depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config ARCH_USE_QUEUE_RWLOCK
+ 	bool
+ 
+ config QUEUE_RWLOCK
+ 	def_bool y if ARCH_USE_QUEUE_RWLOCK
+ 	depends on SMP
++>>>>>>> 5db6c6fefb1c (locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER)
* Unmerged path include/linux/rwsem.h
* Unmerged path kernel/Kconfig.locks
diff --git a/kernel/rwsem.c b/kernel/rwsem.c
index 42f806de49d4..e2d3bc7f03b4 100644
--- a/kernel/rwsem.c
+++ b/kernel/rwsem.c
@@ -12,7 +12,7 @@
 
 #include <linux/atomic.h>
 
-#if defined(CONFIG_SMP) && defined(CONFIG_RWSEM_XCHGADD_ALGORITHM)
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 static inline void rwsem_set_owner(struct rw_semaphore *sem)
 {
 	sem->owner = current;
diff --git a/lib/rwsem.c b/lib/rwsem.c
index c40c7d28661d..e59a7f97edf4 100644
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@ -82,7 +82,7 @@ void __init_rwsem(struct rw_semaphore *sem, const char *name,
 	sem->count = RWSEM_UNLOCKED_VALUE;
 	raw_spin_lock_init(&sem->wait_lock);
 	INIT_LIST_HEAD(&sem->wait_list);
-#ifdef CONFIG_SMP
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 	sem->owner = NULL;
 	sem->osq = NULL;
 #endif
@@ -262,7 +262,7 @@ static inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)
 	return false;
 }
 
-#ifdef CONFIG_SMP
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 /*
  * Try to acquire write lock before the writer has been put on wait queue.
  */
