xen-netfront: release per-queue Tx and Rx resource when disconnecting

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author David Vrabel <david.vrabel@citrix.com>
commit a5b5dc3ce4df4f05f4d81c7d3c56a7604b242093
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a5b5dc3c.failed

Since netfront may reconnect to a backend with a different number of
queues, all per-queue Rx and Tx resources (skbs and grant references)
should be freed when disconnecting.

Without this fix, the Tx and Rx grant refs are not released and
netfront will exhaust them after only a few reconnections.  netfront
will fail to connect when no free grant references are available.

Since all Rx bufs are freed and reallocated instead of reused this
will add some additional delay to the reconnection but this is
expected to be small compared to the time taken by any backend hotplug
scripts etc.

	Signed-off-by: David Vrabel <david.vrabel@citrix.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a5b5dc3ce4df4f05f4d81c7d3c56a7604b242093)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/xen-netfront.c
diff --cc drivers/net/xen-netfront.c
index 6c33c68dceb8,0b133a3d4312..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -1163,18 -1193,9 +1163,21 @@@ static void xennet_release_rx_bufs(stru
  		kfree_skb(skb);
  	}
  
 -	spin_unlock_bh(&queue->rx_lock);
 +	spin_unlock_bh(&np->rx_lock);
 +}
 +
++<<<<<<< HEAD
 +static void xennet_uninit(struct net_device *dev)
 +{
 +	struct netfront_info *np = netdev_priv(dev);
 +	xennet_release_tx_bufs(np);
 +	xennet_release_rx_bufs(np);
 +	gnttab_free_grant_references(np->gref_tx_head);
 +	gnttab_free_grant_references(np->gref_rx_head);
  }
  
++=======
++>>>>>>> a5b5dc3ce4df (xen-netfront: release per-queue Tx and Rx resource when disconnecting)
  static netdev_features_t xennet_fix_features(struct net_device *dev,
  	netdev_features_t features)
  {
@@@ -1414,30 -1419,39 +1416,60 @@@ static void xennet_end_access(int ref, 
  
  static void xennet_disconnect_backend(struct netfront_info *info)
  {
 -	unsigned int i = 0;
 -	unsigned int num_queues = info->netdev->real_num_tx_queues;
 -
 +	/* Stop old i/f to prevent errors whilst we rebuild the state. */
 +	spin_lock_bh(&info->rx_lock);
 +	spin_lock_irq(&info->tx_lock);
  	netif_carrier_off(info->netdev);
 -
 +	spin_unlock_irq(&info->tx_lock);
 +	spin_unlock_bh(&info->rx_lock);
 +
++<<<<<<< HEAD
 +	if (info->tx_irq && (info->tx_irq == info->rx_irq))
 +		unbind_from_irqhandler(info->tx_irq, info);
 +	if (info->tx_irq && (info->tx_irq != info->rx_irq)) {
 +		unbind_from_irqhandler(info->tx_irq, info);
 +		unbind_from_irqhandler(info->rx_irq, info);
++=======
+ 	for (i = 0; i < num_queues; ++i) {
+ 		struct netfront_queue *queue = &info->queues[i];
+ 
+ 		if (queue->tx_irq && (queue->tx_irq == queue->rx_irq))
+ 			unbind_from_irqhandler(queue->tx_irq, queue);
+ 		if (queue->tx_irq && (queue->tx_irq != queue->rx_irq)) {
+ 			unbind_from_irqhandler(queue->tx_irq, queue);
+ 			unbind_from_irqhandler(queue->rx_irq, queue);
+ 		}
+ 		queue->tx_evtchn = queue->rx_evtchn = 0;
+ 		queue->tx_irq = queue->rx_irq = 0;
+ 
+ 		napi_synchronize(&queue->napi);
+ 
+ 		xennet_release_tx_bufs(queue);
+ 		xennet_release_rx_bufs(queue);
+ 		gnttab_free_grant_references(queue->gref_tx_head);
+ 		gnttab_free_grant_references(queue->gref_rx_head);
+ 
+ 		/* End access and free the pages */
+ 		xennet_end_access(queue->tx_ring_ref, queue->tx.sring);
+ 		xennet_end_access(queue->rx_ring_ref, queue->rx.sring);
+ 
+ 		queue->tx_ring_ref = GRANT_INVALID_REF;
+ 		queue->rx_ring_ref = GRANT_INVALID_REF;
+ 		queue->tx.sring = NULL;
+ 		queue->rx.sring = NULL;
++>>>>>>> a5b5dc3ce4df (xen-netfront: release per-queue Tx and Rx resource when disconnecting)
  	}
 +	info->tx_evtchn = info->rx_evtchn = 0;
 +	info->tx_irq = info->rx_irq = 0;
 +
 +	/* End access and free the pages */
 +	xennet_end_access(info->tx_ring_ref, info->tx.sring);
 +	xennet_end_access(info->rx_ring_ref, info->rx.sring);
 +
 +	info->tx_ring_ref = GRANT_INVALID_REF;
 +	info->rx_ring_ref = GRANT_INVALID_REF;
 +	info->tx.sring = NULL;
 +	info->rx.sring = NULL;
  }
  
  /**
@@@ -1732,11 -1997,11 +1764,16 @@@ again
  static int xennet_connect(struct net_device *dev)
  {
  	struct netfront_info *np = netdev_priv(dev);
++<<<<<<< HEAD
 +	int i, requeue_idx, err;
 +	struct sk_buff *skb;
 +	grant_ref_t ref;
 +	struct xen_netif_rx_request *req;
++=======
+ 	unsigned int num_queues = 0;
+ 	int err;
++>>>>>>> a5b5dc3ce4df (xen-netfront: release per-queue Tx and Rx resource when disconnecting)
  	unsigned int feature_rx_copy;
 -	unsigned int j = 0;
 -	struct netfront_queue *queue = NULL;
  
  	err = xenbus_scanf(XBT_NIL, np->xbdev->otherend,
  			   "feature-rx-copy", "%u", &feature_rx_copy);
@@@ -1757,39 -2025,8 +1794,42 @@@
  	netdev_update_features(dev);
  	rtnl_unlock();
  
++<<<<<<< HEAD
 +	spin_lock_bh(&np->rx_lock);
 +	spin_lock_irq(&np->tx_lock);
 +
 +	/* Step 1: Discard all pending TX packet fragments. */
 +	xennet_release_tx_bufs(np);
 +
 +	/* Step 2: Rebuild the RX buffer freelist and the RX ring itself. */
 +	for (requeue_idx = 0, i = 0; i < NET_RX_RING_SIZE; i++) {
 +		skb_frag_t *frag;
 +		const struct page *page;
 +		if (!np->rx_skbs[i])
 +			continue;
 +
 +		skb = np->rx_skbs[requeue_idx] = xennet_get_rx_skb(np, i);
 +		ref = np->grant_rx_ref[requeue_idx] = xennet_get_rx_ref(np, i);
 +		req = RING_GET_REQUEST(&np->rx, requeue_idx);
 +
 +		frag = &skb_shinfo(skb)->frags[0];
 +		page = skb_frag_page(frag);
 +		gnttab_grant_foreign_access_ref(
 +			ref, np->xbdev->otherend_id,
 +			pfn_to_mfn(page_to_pfn(page)),
 +			0);
 +		req->gref = ref;
 +		req->id   = requeue_idx;
 +
 +		requeue_idx++;
 +	}
 +
 +	np->rx.req_prod_pvt = requeue_idx;
 +
++=======
++>>>>>>> a5b5dc3ce4df (xen-netfront: release per-queue Tx and Rx resource when disconnecting)
  	/*
- 	 * Step 3: All public and private state should now be sane.  Get
+ 	 * All public and private state should now be sane.  Get
  	 * ready to start sending and receiving packets and give the driver
  	 * domain a kick because we've probably just requeued some
  	 * packets.
* Unmerged path drivers/net/xen-netfront.c
