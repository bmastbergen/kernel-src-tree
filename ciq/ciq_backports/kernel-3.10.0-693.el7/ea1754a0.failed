mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [fs] mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get, release} usage(cifs only) (Sachin Prabhu) [1416808]
Rebuild_FUZZ: 92.31%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit ea1754a084760e68886f5b725c8eaada9cc57155
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ea1754a0.failed

Mostly direct substitution with occasional adjustment or removing
outdated comments.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ea1754a084760e68886f5b725c8eaada9cc57155)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/filesystems/vfs.txt
#	arch/parisc/mm/init.c
#	drivers/staging/lustre/include/linux/lnet/types.h
#	drivers/staging/lustre/lnet/selftest/selftest.h
#	drivers/staging/lustre/lustre/include/lu_object.h
#	drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
#	drivers/staging/lustre/lustre/include/lustre_net.h
#	drivers/staging/lustre/lustre/include/obd.h
#	drivers/staging/lustre/lustre/llite/dir.c
#	drivers/staging/lustre/lustre/llite/rw.c
#	drivers/staging/lustre/lustre/llite/vvp_io.c
#	drivers/staging/lustre/lustre/lmv/lmv_obd.c
#	drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
#	drivers/staging/lustre/lustre/osc/osc_cache.c
#	fs/btrfs/check-integrity.c
#	fs/btrfs/extent_io.c
#	fs/ecryptfs/inode.c
#	fs/ext4/readpage.c
#	fs/ntfs/compress.c
#	fs/ntfs/inode.c
#	fs/ntfs/super.c
#	include/linux/backing-dev-defs.h
#	mm/memory.c
#	mm/swap.c
diff --cc Documentation/filesystems/vfs.txt
index c487fe5b7a5c,4164bd6397a2..000000000000
--- a/Documentation/filesystems/vfs.txt
+++ b/Documentation/filesystems/vfs.txt
@@@ -707,14 -706,14 +707,25 @@@ struct address_space_operations 
    invalidatepage: If a page has PagePrivate set, then invalidatepage
          will be called when part or all of the page is to be removed
  	from the address space.  This generally corresponds to either a
++<<<<<<< HEAD
 +	truncation or a complete invalidation of the address space
 +	(in the latter case 'offset' will always be 0).
 +	Any private data associated with the page should be updated
 +	to reflect this truncation.  If offset is 0, then
 +	the private data should be released, because the page
 +	must be able to be completely discarded.  This may be done by
 +        calling the ->releasepage function, but in this case the
 +        release MUST succeed.
++=======
+ 	truncation, punch hole  or a complete invalidation of the address
+ 	space (in the latter case 'offset' will always be 0 and 'length'
+ 	will be PAGE_SIZE). Any private data associated with the page
+ 	should be updated to reflect this truncation.  If offset is 0 and
+ 	length is PAGE_SIZE, then the private data should be released,
+ 	because the page must be able to be completely discarded.  This may
+ 	be done by calling the ->releasepage function, but in this case the
+ 	release MUST succeed.
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  
    releasepage: releasepage is called on PagePrivate pages to indicate
          that the page should be freed if possible.  ->releasepage
diff --cc arch/parisc/mm/init.c
index 505b56c6b9b9,6b3e7c6ee096..000000000000
--- a/arch/parisc/mm/init.c
+++ b/arch/parisc/mm/init.c
@@@ -22,7 -22,8 +22,12 @@@
  #include <linux/swap.h>
  #include <linux/unistd.h>
  #include <linux/nodemask.h>	/* for node_online_map */
++<<<<<<< HEAD
 +#include <linux/pagemap.h>	/* for release_pages and page_cache_release */
++=======
+ #include <linux/pagemap.h>	/* for release_pages */
+ #include <linux/compat.h>
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  
  #include <asm/pgalloc.h>
  #include <asm/pgtable.h>
diff --cc fs/btrfs/check-integrity.c
index a1b56d0c9961,516e19d1d202..000000000000
--- a/fs/btrfs/check-integrity.c
+++ b/fs/btrfs/check-integrity.c
@@@ -3079,16 -3037,16 +3079,26 @@@ int btrfsic_mount(struct btrfs_root *ro
  	struct list_head *dev_head = &fs_devices->devices;
  	struct btrfs_device *device;
  
 -	if (root->nodesize & ((u64)PAGE_SIZE - 1)) {
 +	if (root->nodesize & ((u64)PAGE_CACHE_SIZE - 1)) {
  		printk(KERN_INFO
++<<<<<<< HEAD
 +		       "btrfsic: cannot handle nodesize %d not being a multiple of PAGE_CACHE_SIZE %ld!\n",
 +		       root->nodesize, PAGE_CACHE_SIZE);
++=======
+ 		       "btrfsic: cannot handle nodesize %d not being a multiple of PAGE_SIZE %ld!\n",
+ 		       root->nodesize, PAGE_SIZE);
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		return -1;
  	}
 -	if (root->sectorsize & ((u64)PAGE_SIZE - 1)) {
 +	if (root->sectorsize & ((u64)PAGE_CACHE_SIZE - 1)) {
  		printk(KERN_INFO
++<<<<<<< HEAD
 +		       "btrfsic: cannot handle sectorsize %d not being a multiple of PAGE_CACHE_SIZE %ld!\n",
 +		       root->sectorsize, PAGE_CACHE_SIZE);
++=======
+ 		       "btrfsic: cannot handle sectorsize %d not being a multiple of PAGE_SIZE %ld!\n",
+ 		       root->sectorsize, PAGE_SIZE);
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		return -1;
  	}
  	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
diff --cc fs/btrfs/extent_io.c
index dddb24f01842,d247fc0eea19..000000000000
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@@ -3387,13 -3264,11 +3387,16 @@@ static noinline_for_stack int writepage
  			goto done;
  		}
  		/*
- 		 * delalloc_end is already one less than the total
- 		 * length, so we don't subtract one from
- 		 * PAGE_CACHE_SIZE
+ 		 * delalloc_end is already one less than the total length, so
+ 		 * we don't subtract one from PAGE_SIZE
  		 */
  		delalloc_to_write += (delalloc_end - delalloc_start +
++<<<<<<< HEAD
 +				      PAGE_CACHE_SIZE) >>
 +				      PAGE_CACHE_SHIFT;
++=======
+ 				      PAGE_SIZE) >> PAGE_SHIFT;
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		delalloc_start = delalloc_end + 1;
  	}
  	if (wbc->nr_to_write < delalloc_to_write) {
diff --cc fs/ecryptfs/inode.c
index 1648908cf236,224b49e71aa4..000000000000
--- a/fs/ecryptfs/inode.c
+++ b/fs/ecryptfs/inode.c
@@@ -781,10 -763,10 +781,17 @@@ static int truncate_upper(struct dentr
  	} else { /* ia->ia_size < i_size_read(inode) */
  		/* We're chopping off all the pages down to the page
  		 * in which ia->ia_size is located. Fill in the end of
++<<<<<<< HEAD
 +		 * that page from (ia->ia_size & ~PAGE_CACHE_MASK) to
 +		 * PAGE_CACHE_SIZE with zeros. */
 +		size_t num_zeros = (PAGE_CACHE_SIZE
 +				    - (ia->ia_size & ~PAGE_CACHE_MASK));
++=======
+ 		 * that page from (ia->ia_size & ~PAGE_MASK) to
+ 		 * PAGE_SIZE with zeros. */
+ 		size_t num_zeros = (PAGE_SIZE
+ 				    - (ia->ia_size & ~PAGE_MASK));
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  
  		if (!(crypt_stat->flags & ECRYPTFS_ENCRYPTED)) {
  			truncate_setsize(inode, ia->ia_size);
diff --cc fs/ntfs/compress.c
index ee4144ce5d7c,f2b5e746f49b..000000000000
--- a/fs/ntfs/compress.c
+++ b/fs/ntfs/compress.c
@@@ -104,11 -104,7 +104,15 @@@ static void zero_partial_compressed_pag
  	unsigned int kp_ofs;
  
  	ntfs_debug("Zeroing page region outside initialized size.");
++<<<<<<< HEAD
 +	if (((s64)page->index << PAGE_CACHE_SHIFT) >= initialized_size) {
 +		/*
 +		 * FIXME: Using clear_page() will become wrong when we get
 +		 * PAGE_CACHE_SIZE != PAGE_SIZE but for now there is no problem.
 +		 */
++=======
+ 	if (((s64)page->index << PAGE_SHIFT) >= initialized_size) {
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		clear_page(kp);
  		return;
  	}
@@@ -497,14 -493,14 +501,19 @@@ int ntfs_read_compressed_block(struct p
  	u64 cb_size_mask = cb_size - 1UL;
  	VCN vcn;
  	LCN lcn;
++<<<<<<< HEAD
 +	/* The first wanted vcn (minimum alignment is PAGE_CACHE_SIZE). */
 +	VCN start_vcn = (((s64)index << PAGE_CACHE_SHIFT) & ~cb_size_mask) >>
++=======
+ 	/* The first wanted vcn (minimum alignment is PAGE_SIZE). */
+ 	VCN start_vcn = (((s64)index << PAGE_SHIFT) & ~cb_size_mask) >>
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  			vol->cluster_size_bits;
  	/*
  	 * The first vcn after the last wanted vcn (minimum alignment is again
- 	 * PAGE_CACHE_SIZE.
+ 	 * PAGE_SIZE.
  	 */
 -	VCN end_vcn = ((((s64)(index + 1UL) << PAGE_SHIFT) + cb_size - 1)
 +	VCN end_vcn = ((((s64)(index + 1UL) << PAGE_CACHE_SHIFT) + cb_size - 1)
  			& ~cb_size_mask) >> vol->cluster_size_bits;
  	/* Number of compression blocks (cbs) in the wanted vcn range. */
  	unsigned int nr_cbs = (end_vcn - start_vcn) << vol->cluster_size_bits
diff --cc fs/ntfs/inode.c
index bd50adc1e6a7,f40972d6df90..000000000000
--- a/fs/ntfs/inode.c
+++ b/fs/ntfs/inode.c
@@@ -869,12 -868,12 +869,12 @@@ skip_attr_list_load
  					ni->itype.index.block_size);
  			goto unm_err_out;
  		}
 -		if (ni->itype.index.block_size > PAGE_SIZE) {
 +		if (ni->itype.index.block_size > PAGE_CACHE_SIZE) {
  			ntfs_error(vi->i_sb, "Index block size (%u) > "
- 					"PAGE_CACHE_SIZE (%ld) is not "
+ 					"PAGE_SIZE (%ld) is not "
  					"supported.  Sorry.",
  					ni->itype.index.block_size,
 -					PAGE_SIZE);
 +					PAGE_CACHE_SIZE);
  			err = -EOPNOTSUPP;
  			goto unm_err_out;
  		}
@@@ -1583,10 -1585,10 +1583,15 @@@ static int ntfs_read_locked_index_inode
  				"two.", ni->itype.index.block_size);
  		goto unm_err_out;
  	}
++<<<<<<< HEAD
 +	if (ni->itype.index.block_size > PAGE_CACHE_SIZE) {
 +		ntfs_error(vi->i_sb, "Index block size (%u) > PAGE_CACHE_SIZE "
++=======
+ 	if (ni->itype.index.block_size > PAGE_SIZE) {
+ 		ntfs_error(vi->i_sb, "Index block size (%u) > PAGE_SIZE "
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  				"(%ld) is not supported.  Sorry.",
 -				ni->itype.index.block_size, PAGE_SIZE);
 +				ni->itype.index.block_size, PAGE_CACHE_SIZE);
  		err = -EOPNOTSUPP;
  		goto unm_err_out;
  	}
diff --cc fs/ntfs/super.c
index 82650d52d916,ecb49870a680..000000000000
--- a/fs/ntfs/super.c
+++ b/fs/ntfs/super.c
@@@ -820,14 -823,14 +820,14 @@@ static bool parse_ntfs_boot_sector(ntfs
  	ntfs_debug("vol->mft_record_size_bits = %i (0x%x)",
  			vol->mft_record_size_bits, vol->mft_record_size_bits);
  	/*
- 	 * We cannot support mft record sizes above the PAGE_CACHE_SIZE since
+ 	 * We cannot support mft record sizes above the PAGE_SIZE since
  	 * we store $MFT/$DATA, the table of mft records in the page cache.
  	 */
 -	if (vol->mft_record_size > PAGE_SIZE) {
 +	if (vol->mft_record_size > PAGE_CACHE_SIZE) {
  		ntfs_error(vol->sb, "Mft record size (%i) exceeds the "
- 				"PAGE_CACHE_SIZE on your system (%lu).  "
+ 				"PAGE_SIZE on your system (%lu).  "
  				"This is not supported.  Sorry.",
 -				vol->mft_record_size, PAGE_SIZE);
 +				vol->mft_record_size, PAGE_CACHE_SIZE);
  		return false;
  	}
  	/* We cannot support mft record sizes below the sector size. */
@@@ -2475,14 -2471,14 +2475,20 @@@ static s64 get_nr_free_clusters(ntfs_vo
  	down_read(&vol->lcnbmp_lock);
  	/*
  	 * Convert the number of bits into bytes rounded up, then convert into
- 	 * multiples of PAGE_CACHE_SIZE, rounding up so that if we have one
+ 	 * multiples of PAGE_SIZE, rounding up so that if we have one
  	 * full and one partial page max_index = 2.
  	 */
++<<<<<<< HEAD
 +	max_index = (((vol->nr_clusters + 7) >> 3) + PAGE_CACHE_SIZE - 1) >>
 +			PAGE_CACHE_SHIFT;
 +	/* Use multiples of 4 bytes, thus max_size is PAGE_CACHE_SIZE / 4. */
++=======
+ 	max_index = (((vol->nr_clusters + 7) >> 3) + PAGE_SIZE - 1) >>
+ 			PAGE_SHIFT;
+ 	/* Use multiples of 4 bytes, thus max_size is PAGE_SIZE / 4. */
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  	ntfs_debug("Reading $Bitmap, max_index = 0x%lx, max_size = 0x%lx.",
 -			max_index, PAGE_SIZE / 4);
 +			max_index, PAGE_CACHE_SIZE / 4);
  	for (index = 0; index < max_index; index++) {
  		unsigned long *kaddr;
  
@@@ -2551,9 -2547,9 +2557,9 @@@ static unsigned long __get_nr_free_mft_
  	pgoff_t index;
  
  	ntfs_debug("Entering.");
- 	/* Use multiples of 4 bytes, thus max_size is PAGE_CACHE_SIZE / 4. */
+ 	/* Use multiples of 4 bytes, thus max_size is PAGE_SIZE / 4. */
  	ntfs_debug("Reading $MFT/$BITMAP, max_index = 0x%lx, max_size = "
 -			"0x%lx.", max_index, PAGE_SIZE / 4);
 +			"0x%lx.", max_index, PAGE_CACHE_SIZE / 4);
  	for (index = 0; index < max_index; index++) {
  		unsigned long *kaddr;
  
@@@ -2769,8 -2765,8 +2775,13 @@@ static int ntfs_fill_super(struct super
  	if (!parse_options(vol, (char*)opt))
  		goto err_out_now;
  
++<<<<<<< HEAD
 +	/* We support sector sizes up to the PAGE_CACHE_SIZE. */
 +	if (bdev_logical_block_size(sb->s_bdev) > PAGE_CACHE_SIZE) {
++=======
+ 	/* We support sector sizes up to the PAGE_SIZE. */
+ 	if (bdev_logical_block_size(sb->s_bdev) > PAGE_SIZE) {
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		if (!silent)
  			ntfs_error(sb, "Device has unsupported sector size "
  					"(%i).  The maximum supported sector "
diff --cc mm/memory.c
index e63691293747,93897f23cc11..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2429,8 -2399,7 +2429,12 @@@ static inline void unmap_mapping_range_
  			details->first_index, details->last_index) {
  
  		vba = vma->vm_pgoff;
++<<<<<<< HEAD
 +		vea = vba + ((vma->vm_end - vma->vm_start) >> PAGE_SHIFT) - 1;
 +		/* Assume for now that PAGE_CACHE_SHIFT == PAGE_SHIFT */
++=======
+ 		vea = vba + vma_pages(vma) - 1;
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
  		zba = details->first_index;
  		if (zba < vba)
  			zba = vba;
diff --cc mm/swap.c
index 710acc9fa0d6,a0bc206b4ac6..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -835,50 -697,14 +835,58 @@@ void lru_add_drain_all(void
  	mutex_unlock(&lock);
  }
  
++<<<<<<< HEAD
 +static inline struct zone *zone_lru_lock(struct zone *zone,
 +					 struct page *page,
 +					 unsigned int *lock_batch,
 +					 unsigned long *_flags)
 +{
 +	struct zone *pagezone = page_zone(page);
 +
 +	if (pagezone != zone) {
 +		unsigned long flags = *_flags;
 +
 +		if (zone)
 +			spin_unlock_irqrestore(&zone->lru_lock, flags);
 +		*lock_batch = 0;
 +		zone = pagezone;
 +		spin_lock_irqsave(&zone->lru_lock, flags);
 +
 +		*_flags = flags;
 +	}
 +
 +	return zone;
 +}
 +
 +static inline struct zone *zone_lru_unlock(struct zone *zone,
 +					   unsigned long flags)
 +{
 +	if (zone) {
 +		spin_unlock_irqrestore(&zone->lru_lock, flags);
 +		zone = NULL;
 +	}
 +	return zone;
 +}
 +
 +/*
 + * Batched page_cache_release().  Decrement the reference count on all the
 + * passed pages.  If it fell to zero then remove the page from the LRU and
 + * free it.
++=======
+ /**
+  * release_pages - batched put_page()
+  * @pages: array of pages to release
+  * @nr: number of pages
+  * @cold: whether the pages are cache cold
++>>>>>>> ea1754a08476 (mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage)
   *
 - * Decrement the reference count on all the pages in @pages.  If it
 - * fell to zero, remove the page from the LRU and free it.
 + * Avoid taking zone->lru_lock if possible, but if it is taken, retain it
 + * for the remainder of the operation.
 + *
 + * The locking in this function is against shrink_inactive_list(): we recheck
 + * the page count inside the lock to see whether shrink_inactive_list()
 + * grabbed the page via the LRU.  If it did, give up: shrink_inactive_list()
 + * will free it.
   */
  void release_pages(struct page **pages, int nr, bool cold)
  {
* Unmerged path drivers/staging/lustre/include/linux/lnet/types.h
* Unmerged path drivers/staging/lustre/lnet/selftest/selftest.h
* Unmerged path drivers/staging/lustre/lustre/include/lu_object.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_net.h
* Unmerged path drivers/staging/lustre/lustre/include/obd.h
* Unmerged path drivers/staging/lustre/lustre/llite/dir.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_io.c
* Unmerged path drivers/staging/lustre/lustre/lmv/lmv_obd.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_cache.c
* Unmerged path fs/ext4/readpage.c
* Unmerged path include/linux/backing-dev-defs.h
diff --git a/Documentation/filesystems/cramfs.txt b/Documentation/filesystems/cramfs.txt
index 31f53f0ab957..4006298f6707 100644
--- a/Documentation/filesystems/cramfs.txt
+++ b/Documentation/filesystems/cramfs.txt
@@ -38,7 +38,7 @@ the update lasts only as long as the inode is cached in memory, after
 which the timestamp reverts to 1970, i.e. moves backwards in time.
 
 Currently, cramfs must be written and read with architectures of the
-same endianness, and can be read only by kernels with PAGE_CACHE_SIZE
+same endianness, and can be read only by kernels with PAGE_SIZE
 == 4096.  At least the latter of these is a bug, but it hasn't been
 decided what the best fix is.  For the moment if you have larger pages
 you can just change the #define in mkcramfs.c, so long as you don't
diff --git a/Documentation/filesystems/tmpfs.txt b/Documentation/filesystems/tmpfs.txt
index d392e1505f17..d9c11d25bf02 100644
--- a/Documentation/filesystems/tmpfs.txt
+++ b/Documentation/filesystems/tmpfs.txt
@@ -60,7 +60,7 @@ size:      The limit of allocated bytes for this tmpfs instance. The
            default is half of your physical RAM without swap. If you
            oversize your tmpfs instances the machine will deadlock
            since the OOM handler will not be able to free that memory.
-nr_blocks: The same as size, but in blocks of PAGE_CACHE_SIZE.
+nr_blocks: The same as size, but in blocks of PAGE_SIZE.
 nr_inodes: The maximum number of inodes for this instance. The default
            is half of the number of your physical RAM pages, or (on a
            machine with highmem) the number of lowmem RAM pages,
* Unmerged path Documentation/filesystems/vfs.txt
* Unmerged path arch/parisc/mm/init.c
diff --git a/drivers/block/drbd/drbd_int.h b/drivers/block/drbd/drbd_int.h
index f943aacfdad8..d34c99bb7e49 100644
--- a/drivers/block/drbd/drbd_int.h
+++ b/drivers/block/drbd/drbd_int.h
@@ -1333,8 +1333,8 @@ struct bm_extent {
 #endif
 #endif
 
-/* BIO_MAX_SIZE is 256 * PAGE_CACHE_SIZE,
- * so for typical PAGE_CACHE_SIZE of 4k, that is (1<<20) Byte.
+/* BIO_MAX_SIZE is 256 * PAGE_SIZE,
+ * so for typical PAGE_SIZE of 4k, that is (1<<20) Byte.
  * Since we may live in a mixed-platform cluster,
  * we limit us to a platform agnostic constant here for now.
  * A followup commit may allow even bigger BIO sizes,
* Unmerged path drivers/staging/lustre/include/linux/lnet/types.h
* Unmerged path drivers/staging/lustre/lnet/selftest/selftest.h
* Unmerged path drivers/staging/lustre/lustre/include/lu_object.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_net.h
* Unmerged path drivers/staging/lustre/lustre/include/obd.h
* Unmerged path drivers/staging/lustre/lustre/llite/dir.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_io.c
* Unmerged path drivers/staging/lustre/lustre/lmv/lmv_obd.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_cache.c
diff --git a/fs/bio.c b/fs/bio.c
index ba5bd4020a51..da10dc8de3fe 100644
--- a/fs/bio.c
+++ b/fs/bio.c
@@ -1679,8 +1679,8 @@ static void bio_release_pages(struct bio *bio)
  * the BIO and the offending pages and re-dirty the pages in process context.
  *
  * It is expected that bio_check_pages_dirty() will wholly own the BIO from
- * here on.  It will run one page_cache_release() against each page and will
- * run one bio_put() against the BIO.
+ * here on.  It will run one put_page() against each page and will run one
+ * bio_put() against the BIO.
  */
 
 static void bio_dirty_fn(struct work_struct *work);
* Unmerged path fs/btrfs/check-integrity.c
* Unmerged path fs/btrfs/extent_io.c
diff --git a/fs/btrfs/struct-funcs.c b/fs/btrfs/struct-funcs.c
index b976597b0721..e05619f241be 100644
--- a/fs/btrfs/struct-funcs.c
+++ b/fs/btrfs/struct-funcs.c
@@ -66,7 +66,7 @@ u##bits btrfs_get_token_##bits(struct extent_buffer *eb, void *ptr,	\
 									\
 	if (token && token->kaddr && token->offset <= offset &&		\
 	    token->eb == eb &&						\
-	   (token->offset + PAGE_CACHE_SIZE >= offset + size)) {	\
+	   (token->offset + PAGE_SIZE >= offset + size)) {	\
 		kaddr = token->kaddr;					\
 		p = kaddr + part_offset - token->offset;		\
 		res = get_unaligned_le##bits(p + off);			\
@@ -104,7 +104,7 @@ void btrfs_set_token_##bits(struct extent_buffer *eb,			\
 									\
 	if (token && token->kaddr && token->offset <= offset &&		\
 	    token->eb == eb &&						\
-	   (token->offset + PAGE_CACHE_SIZE >= offset + size)) {	\
+	   (token->offset + PAGE_SIZE >= offset + size)) {	\
 		kaddr = token->kaddr;					\
 		p = kaddr + part_offset - token->offset;		\
 		put_unaligned_le##bits(val, p + off);			\
diff --git a/fs/btrfs/tests/extent-io-tests.c b/fs/btrfs/tests/extent-io-tests.c
index 9e9f2368177d..26398ecb7a24 100644
--- a/fs/btrfs/tests/extent-io-tests.c
+++ b/fs/btrfs/tests/extent-io-tests.c
@@ -235,7 +235,7 @@ static int test_find_delalloc(void)
 	end = 0;
 	/*
 	 * Currently if we fail to find dirty pages in the delalloc range we
-	 * will adjust max_bytes down to PAGE_CACHE_SIZE and then re-search.  If
+	 * will adjust max_bytes down to PAGE_SIZE and then re-search.  If
 	 * this changes at any point in the future we will need to fix this
 	 * tests expected behavior.
 	 */
diff --git a/fs/cifs/cifsglob.h b/fs/cifs/cifsglob.h
index 9d14926531ba..c69dea17893d 100644
--- a/fs/cifs/cifsglob.h
+++ b/fs/cifs/cifsglob.h
@@ -713,7 +713,7 @@ compare_mid(__u16 mid, const struct smb_hdr *smb)
  *
  * Note that this might make for "interesting" allocation problems during
  * writeback however as we have to allocate an array of pointers for the
- * pages. A 16M write means ~32kb page array with PAGE_CACHE_SIZE == 4096.
+ * pages. A 16M write means ~32kb page array with PAGE_SIZE == 4096.
  *
  * For reads, there is a similar problem as we need to allocate an array
  * of kvecs to handle the receive, though that should only need to be done
@@ -732,7 +732,7 @@ compare_mid(__u16 mid, const struct smb_hdr *smb)
 
 /*
  * The default wsize is 1M. find_get_pages seems to return a maximum of 256
- * pages in a single call. With PAGE_CACHE_SIZE == 4k, this means we can fill
+ * pages in a single call. With PAGE_SIZE == 4k, this means we can fill
  * a single wsize request with a single call.
  */
 #define CIFS_DEFAULT_IOSIZE (1024 * 1024)
diff --git a/fs/cifs/file.c b/fs/cifs/file.c
index c259c09ba911..a383564bb62b 100644
--- a/fs/cifs/file.c
+++ b/fs/cifs/file.c
@@ -1907,7 +1907,7 @@ wdata_alloc_and_fillpages(pgoff_t tofind, struct address_space *mapping,
 	 * find_get_pages_tag seems to return a max of 256 on each
 	 * iteration, so we must call it several times in order to
 	 * fill the array or the wsize is effectively limited to
-	 * 256 * PAGE_CACHE_SIZE.
+	 * 256 * PAGE_SIZE.
 	 */
 	*found_pages = 0;
 	pages = wdata->pages;
diff --git a/fs/cramfs/README b/fs/cramfs/README
index 445d1c2d7646..9d4e7ea311f4 100644
--- a/fs/cramfs/README
+++ b/fs/cramfs/README
@@ -86,26 +86,26 @@ Block Size
 
 (Block size in cramfs refers to the size of input data that is
 compressed at a time.  It's intended to be somewhere around
-PAGE_CACHE_SIZE for cramfs_readpage's convenience.)
+PAGE_SIZE for cramfs_readpage's convenience.)
 
 The superblock ought to indicate the block size that the fs was
 written for, since comments in <linux/pagemap.h> indicate that
-PAGE_CACHE_SIZE may grow in future (if I interpret the comment
+PAGE_SIZE may grow in future (if I interpret the comment
 correctly).
 
-Currently, mkcramfs #define's PAGE_CACHE_SIZE as 4096 and uses that
-for blksize, whereas Linux-2.3.39 uses its PAGE_CACHE_SIZE, which in
+Currently, mkcramfs #define's PAGE_SIZE as 4096 and uses that
+for blksize, whereas Linux-2.3.39 uses its PAGE_SIZE, which in
 turn is defined as PAGE_SIZE (which can be as large as 32KB on arm).
 This discrepancy is a bug, though it's not clear which should be
 changed.
 
-One option is to change mkcramfs to take its PAGE_CACHE_SIZE from
+One option is to change mkcramfs to take its PAGE_SIZE from
 <asm/page.h>.  Personally I don't like this option, but it does
 require the least amount of change: just change `#define
-PAGE_CACHE_SIZE (4096)' to `#include <asm/page.h>'.  The disadvantage
+PAGE_SIZE (4096)' to `#include <asm/page.h>'.  The disadvantage
 is that the generated cramfs cannot always be shared between different
 kernels, not even necessarily kernels of the same architecture if
-PAGE_CACHE_SIZE is subject to change between kernel versions
+PAGE_SIZE is subject to change between kernel versions
 (currently possible with arm and ia64).
 
 The remaining options try to make cramfs more sharable.
@@ -126,22 +126,22 @@ size.  The options are:
   1. Always 4096 bytes.
 
   2. Writer chooses blocksize; kernel adapts but rejects blocksize >
-     PAGE_CACHE_SIZE.
+     PAGE_SIZE.
 
   3. Writer chooses blocksize; kernel adapts even to blocksize >
-     PAGE_CACHE_SIZE.
+     PAGE_SIZE.
 
 It's easy enough to change the kernel to use a smaller value than
-PAGE_CACHE_SIZE: just make cramfs_readpage read multiple blocks.
+PAGE_SIZE: just make cramfs_readpage read multiple blocks.
 
-The cost of option 1 is that kernels with a larger PAGE_CACHE_SIZE
+The cost of option 1 is that kernels with a larger PAGE_SIZE
 value don't get as good compression as they can.
 
 The cost of option 2 relative to option 1 is that the code uses
 variables instead of #define'd constants.  The gain is that people
-with kernels having larger PAGE_CACHE_SIZE can make use of that if
+with kernels having larger PAGE_SIZE can make use of that if
 they don't mind their cramfs being inaccessible to kernels with
-smaller PAGE_CACHE_SIZE values.
+smaller PAGE_SIZE values.
 
 Option 3 is easy to implement if we don't mind being CPU-inefficient:
 e.g. get readpage to decompress to a buffer of size MAX_BLKSIZE (which
diff --git a/fs/cramfs/inode.c b/fs/cramfs/inode.c
index 35b1c7bd18b7..dcb3381fadb9 100644
--- a/fs/cramfs/inode.c
+++ b/fs/cramfs/inode.c
@@ -118,7 +118,7 @@ static struct inode *get_cramfs_inode(struct super_block *sb,
  * page cache and dentry tree anyway..
  *
  * This also acts as a way to guarantee contiguous areas of up to
- * BLKS_PER_BUF*PAGE_CACHE_SIZE, so that the caller doesn't need to
+ * BLKS_PER_BUF*PAGE_SIZE, so that the caller doesn't need to
  * worry about end-of-buffer issues even when decompressing a full
  * page cache.
  */
diff --git a/fs/dax.c b/fs/dax.c
index 365b0662df80..f9d2b5fc5618 100644
--- a/fs/dax.c
+++ b/fs/dax.c
@@ -1070,7 +1070,7 @@ EXPORT_SYMBOL_GPL(dax_pfn_mkwrite);
  * you are truncating a file, the helper function dax_truncate_page() may be
  * more convenient.
  *
- * We work in terms of PAGE_CACHE_SIZE here for commonality with
+ * We work in terms of PAGE_SIZE here for commonality with
  * block_truncate_page(), but we could go down to PAGE_SIZE if the filesystem
  * took care of disposing of the unnecessary blocks.  Even if the filesystem
  * block size is smaller than PAGE_SIZE, we have to zero the rest of the page
@@ -1121,7 +1121,7 @@ EXPORT_SYMBOL_GPL(dax_zero_page_range);
  * Similar to block_truncate_page(), this function can be called by a
  * filesystem when it is truncating a DAX file to handle the partial page.
  *
- * We work in terms of PAGE_CACHE_SIZE here for commonality with
+ * We work in terms of PAGE_SIZE here for commonality with
  * block_truncate_page(), but we could go down to PAGE_SIZE if the filesystem
  * took care of disposing of the unnecessary blocks.  Even if the filesystem
  * block size is smaller than PAGE_SIZE, we have to zero the rest of the page
* Unmerged path fs/ecryptfs/inode.c
diff --git a/fs/ext2/dir.c b/fs/ext2/dir.c
index 4237722bfd27..94dfaeb65d23 100644
--- a/fs/ext2/dir.c
+++ b/fs/ext2/dir.c
@@ -37,7 +37,7 @@ static inline unsigned ext2_rec_len_from_disk(__le16 dlen)
 {
 	unsigned len = le16_to_cpu(dlen);
 
-#if (PAGE_CACHE_SIZE >= 65536)
+#if (PAGE_SIZE >= 65536)
 	if (len == EXT2_MAX_REC_LEN)
 		return 1 << 16;
 #endif
@@ -46,7 +46,7 @@ static inline unsigned ext2_rec_len_from_disk(__le16 dlen)
 
 static inline __le16 ext2_rec_len_to_disk(unsigned len)
 {
-#if (PAGE_CACHE_SIZE >= 65536)
+#if (PAGE_SIZE >= 65536)
 	if (len == (1 << 16))
 		return cpu_to_le16(EXT2_MAX_REC_LEN);
 	else
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 22282c061b41..d76f1e50ab60 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1757,7 +1757,7 @@ ext4_rec_len_from_disk(__le16 dlen, unsigned blocksize)
 {
 	unsigned len = le16_to_cpu(dlen);
 
-#if (PAGE_CACHE_SIZE >= 65536)
+#if (PAGE_SIZE >= 65536)
 	if (len == EXT4_MAX_REC_LEN || len == 0)
 		return blocksize;
 	return (len & 65532) | ((len & 3) << 16);
@@ -1770,7 +1770,7 @@ static inline __le16 ext4_rec_len_to_disk(unsigned len, unsigned blocksize)
 {
 	if ((len > blocksize) || (blocksize > (1 << 18)) || (len & 3))
 		BUG();
-#if (PAGE_CACHE_SIZE >= 65536)
+#if (PAGE_SIZE >= 65536)
 	if (len < 65536)
 		return cpu_to_le16(len);
 	if (len == blocksize) {
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 46409f183977..1c857ab076b2 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -4613,7 +4613,7 @@ static void ext4_wait_for_tail_page_commit(struct inode *inode)
 	offset = inode->i_size & (PAGE_CACHE_SIZE - 1);
 	/*
 	 * All buffers in the last page remain valid? Then there's nothing to
-	 * do. We do the check mainly to optimize the common PAGE_CACHE_SIZE ==
+	 * do. We do the check mainly to optimize the common PAGE_SIZE ==
 	 * blocksize case
 	 */
 	if (offset > PAGE_CACHE_SIZE - (1 << inode->i_blkbits))
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 343f1ad2e4b3..0ab4843aa37b 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -118,7 +118,7 @@ MODULE_PARM_DESC(mballoc_debug, "Debugging level for ext4's mballoc");
  *
  *
  * one block each for bitmap and buddy information.  So for each group we
- * take up 2 blocks. A page can contain blocks_per_page (PAGE_CACHE_SIZE /
+ * take up 2 blocks. A page can contain blocks_per_page (PAGE_SIZE /
  * blocksize) blocks.  So it can have information regarding groups_per_page
  * which is blocks_per_page/2
  *
@@ -806,7 +806,7 @@ static void mb_regenerate_buddy(struct ext4_buddy *e4b)
  *
  * one block each for bitmap and buddy information.
  * So for each group we take up 2 blocks. A page can
- * contain blocks_per_page (PAGE_CACHE_SIZE / blocksize)  blocks.
+ * contain blocks_per_page (PAGE_SIZE / blocksize)  blocks.
  * So it can have information regarding groups_per_page which
  * is blocks_per_page/2
  *
* Unmerged path fs/ext4/readpage.c
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index 7b7a94a0485b..772377b8cccc 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -224,7 +224,7 @@ hugetlbfs_read_actor(struct page *page, unsigned long offset,
 /*
  * Support for read() - Find the page attached to f_mapping and copy out the
  * data. Its *very* similar to do_generic_mapping_read(), we can't use that
- * since it has PAGE_CACHE_SIZE assumptions.
+ * since it has PAGE_SIZE assumptions.
  */
 static ssize_t hugetlbfs_read(struct file *filp, char __user *buf,
 			      size_t len, loff_t *ppos)
diff --git a/fs/mpage.c b/fs/mpage.c
index 9a5c19c68127..f5c926dd3327 100644
--- a/fs/mpage.c
+++ b/fs/mpage.c
@@ -328,7 +328,7 @@ confused:
  *
  * then this code just gives up and calls the buffer_head-based read function.
  * It does handle a page which has holes at the end - that is a common case:
- * the end-of-file on blocksize < PAGE_CACHE_SIZE setups.
+ * the end-of-file on blocksize < PAGE_SIZE setups.
  *
  * BH_Boundary explanation:
  *
diff --git a/fs/ntfs/aops.c b/fs/ntfs/aops.c
index fa9c05f97af4..3c8b3a69c8af 100644
--- a/fs/ntfs/aops.c
+++ b/fs/ntfs/aops.c
@@ -675,7 +675,7 @@ static int ntfs_write_block(struct page *page, struct writeback_control *wbc)
 				// in the inode.
 				// Again, for each page do:
 				//	__set_page_dirty_buffers();
-				// page_cache_release()
+				// put_page()
 				// We don't need to wait on the writes.
 				// Update iblock.
 			}
diff --git a/fs/ntfs/aops.h b/fs/ntfs/aops.h
index caecc58f529c..e7d0516eef03 100644
--- a/fs/ntfs/aops.h
+++ b/fs/ntfs/aops.h
@@ -49,7 +49,7 @@ static inline void ntfs_unmap_page(struct page *page)
  * @index:	index into the page cache for @mapping of the page to map
  *
  * Read a page from the page cache of the address space @mapping at position
- * @index, where @index is in units of PAGE_CACHE_SIZE, and not in bytes.
+ * @index, where @index is in units of PAGE_SIZE, and not in bytes.
  *
  * If the page is not in memory it is loaded from disk first using the readpage
  * method defined in the address space operations of @mapping and the page is
* Unmerged path fs/ntfs/compress.c
diff --git a/fs/ntfs/dir.c b/fs/ntfs/dir.c
index aa411c3f20e9..2942bb3eba99 100644
--- a/fs/ntfs/dir.c
+++ b/fs/ntfs/dir.c
@@ -315,7 +315,7 @@ found_it:
 descend_into_child_node:
 	/*
 	 * Convert vcn to index into the index allocation attribute in units
-	 * of PAGE_CACHE_SIZE and map the page cache page, reading it from
+	 * of PAGE_SIZE and map the page cache page, reading it from
 	 * disk if necessary.
 	 */
 	page = ntfs_map_page(ia_mapping, vcn <<
@@ -793,11 +793,11 @@ found_it:
 descend_into_child_node:
 	/*
 	 * Convert vcn to index into the index allocation attribute in units
-	 * of PAGE_CACHE_SIZE and map the page cache page, reading it from
+	 * of PAGE_SIZE and map the page cache page, reading it from
 	 * disk if necessary.
 	 */
 	page = ntfs_map_page(ia_mapping, vcn <<
-			dir_ni->itype.index.vcn_size_bits >> PAGE_CACHE_SHIFT);
+			dir_ni->itype.index.vcn_size_bits >> PAGE_SHIFT);
 	if (IS_ERR(page)) {
 		ntfs_error(sb, "Failed to map directory index page, error %ld.",
 				-PTR_ERR(page));
@@ -809,9 +809,9 @@ descend_into_child_node:
 fast_descend_into_child_node:
 	/* Get to the index allocation block. */
 	ia = (INDEX_ALLOCATION*)(kaddr + ((vcn <<
-			dir_ni->itype.index.vcn_size_bits) & ~PAGE_CACHE_MASK));
+			dir_ni->itype.index.vcn_size_bits) & ~PAGE_MASK));
 	/* Bounds checks. */
-	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) {
+	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Out of bounds check failed. Corrupt directory "
 				"inode 0x%lx or driver bug.", dir_ni->mft_no);
 		goto unm_err_out;
@@ -844,7 +844,7 @@ fast_descend_into_child_node:
 		goto unm_err_out;
 	}
 	index_end = (u8*)ia + dir_ni->itype.index.block_size;
-	if (index_end > kaddr + PAGE_CACHE_SIZE) {
+	if (index_end > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Index buffer (VCN 0x%llx) of directory inode "
 				"0x%lx crosses page boundary. Impossible! "
 				"Cannot access! This is probably a bug in the "
@@ -968,9 +968,9 @@ found_it2:
 			/* If vcn is in the same page cache page as old_vcn we
 			 * recycle the mapped page. */
 			if (old_vcn << vol->cluster_size_bits >>
-					PAGE_CACHE_SHIFT == vcn <<
+					PAGE_SHIFT == vcn <<
 					vol->cluster_size_bits >>
-					PAGE_CACHE_SHIFT)
+					PAGE_SHIFT)
 				goto fast_descend_into_child_node;
 			unlock_page(page);
 			ntfs_unmap_page(page);
diff --git a/fs/ntfs/file.c b/fs/ntfs/file.c
index 81adff3fd865..cc276e1df7ed 100644
--- a/fs/ntfs/file.c
+++ b/fs/ntfs/file.c
@@ -470,7 +470,7 @@ static inline int ntfs_submit_bh_for_read(struct buffer_head *bh)
  * only partially being written to.
  *
  * If @nr_pages is greater than one, we are guaranteed that the cluster size is
- * greater than PAGE_CACHE_SIZE, that all pages in @pages are entirely inside
+ * greater than PAGE_SIZE, that all pages in @pages are entirely inside
  * the same cluster and that they are the entirety of that cluster, and that
  * the cluster is sparse, i.e. we need to allocate a cluster to fill the hole.
  *
diff --git a/fs/ntfs/index.c b/fs/ntfs/index.c
index 096c135691ae..a3cecbd1e719 100644
--- a/fs/ntfs/index.c
+++ b/fs/ntfs/index.c
@@ -272,7 +272,7 @@ done:
 descend_into_child_node:
 	/*
 	 * Convert vcn to index into the index allocation attribute in units
-	 * of PAGE_CACHE_SIZE and map the page cache page, reading it from
+	 * of PAGE_SIZE and map the page cache page, reading it from
 	 * disk if necessary.
 	 */
 	page = ntfs_map_page(ia_mapping, vcn <<
* Unmerged path fs/ntfs/inode.c
* Unmerged path fs/ntfs/super.c
diff --git a/fs/ocfs2/aops.c b/fs/ocfs2/aops.c
index 7fee7b2e1225..995c913f8c56 100644
--- a/fs/ocfs2/aops.c
+++ b/fs/ocfs2/aops.c
@@ -824,7 +824,7 @@ next_bh:
 	return ret;
 }
 
-#if (PAGE_CACHE_SIZE >= OCFS2_MAX_CLUSTERSIZE)
+#if (PAGE_SIZE >= OCFS2_MAX_CLUSTERSIZE)
 #define OCFS2_MAX_CTXT_PAGES	1
 #else
 #define OCFS2_MAX_CTXT_PAGES	(OCFS2_MAX_CLUSTERSIZE / PAGE_CACHE_SIZE)
diff --git a/fs/ocfs2/refcounttree.c b/fs/ocfs2/refcounttree.c
index 998b17eda09d..2aa7ebb20f3f 100644
--- a/fs/ocfs2/refcounttree.c
+++ b/fs/ocfs2/refcounttree.c
@@ -2967,7 +2967,7 @@ int ocfs2_duplicate_clusters_by_page(handle_t *handle,
 		page = find_or_create_page(mapping, page_index, GFP_NOFS);
 
 		/*
-		 * In case PAGE_CACHE_SIZE <= CLUSTER_SIZE, This page
+		 * In case PAGE_SIZE <= CLUSTER_SIZE, This page
 		 * can't be dirtied before we CoW it out.
 		 */
 		if (PAGE_CACHE_SIZE <= OCFS2_SB(sb)->s_clustersize)
diff --git a/fs/reiserfs/journal.c b/fs/reiserfs/journal.c
index 742fdd4c209a..521cfd8eea1d 100644
--- a/fs/reiserfs/journal.c
+++ b/fs/reiserfs/journal.c
@@ -598,7 +598,7 @@ static int journal_list_still_alive(struct super_block *s,
  * This does a check to see if the buffer belongs to one of these
  * lost pages before doing the final put_bh.  If page->mapping was
  * null, it tries to free buffers on the page, which should make the
- * final page_cache_release drop the page from the lru.
+ * final put_page drop the page from the lru.
  */
 static void release_buffer_page(struct buffer_head *bh)
 {
diff --git a/fs/squashfs/cache.c b/fs/squashfs/cache.c
index af0b73802592..65a6ed74c82b 100644
--- a/fs/squashfs/cache.c
+++ b/fs/squashfs/cache.c
@@ -30,7 +30,7 @@
  * access the metadata and fragment caches.
  *
  * To avoid out of memory and fragmentation issues with vmalloc the cache
- * uses sequences of kmalloced PAGE_CACHE_SIZE buffers.
+ * uses sequences of kmalloced PAGE_SIZE buffers.
  *
  * It should be noted that the cache is not used for file datablocks, these
  * are decompressed and cached in the page-cache in the normal way.  The
@@ -230,7 +230,7 @@ void squashfs_cache_delete(struct squashfs_cache *cache)
 /*
  * Initialise cache allocating the specified number of entries, each of
  * size block_size.  To avoid vmalloc fragmentation issues each entry
- * is allocated as a sequence of kmalloced PAGE_CACHE_SIZE buffers.
+ * is allocated as a sequence of kmalloced PAGE_SIZE buffers.
  */
 struct squashfs_cache *squashfs_cache_init(char *name, int entries,
 	int block_size)
diff --git a/fs/squashfs/file.c b/fs/squashfs/file.c
index 8ca62c28fe12..c62d136e16a2 100644
--- a/fs/squashfs/file.c
+++ b/fs/squashfs/file.c
@@ -444,7 +444,7 @@ static int squashfs_readpage(struct file *file, struct page *page)
 
 	/*
 	 * Loop copying datablock into pages.  As the datablock likely covers
-	 * many PAGE_CACHE_SIZE pages (default block size is 128 KiB) explicitly
+	 * many PAGE_SIZE pages (default block size is 128 KiB) explicitly
 	 * grab the pages from the page cache, except for the page that we've
 	 * been called to fill.
 	 */
diff --git a/fs/ubifs/file.c b/fs/ubifs/file.c
index 14374530784c..f891af5a37b4 100644
--- a/fs/ubifs/file.c
+++ b/fs/ubifs/file.c
@@ -554,7 +554,7 @@ static int ubifs_write_end(struct file *file, struct address_space *mapping,
 		 * VFS copied less data to the page that it intended and
 		 * declared in its '->write_begin()' call via the @len
 		 * argument. If the page was not up-to-date, and @len was
-		 * @PAGE_CACHE_SIZE, the 'ubifs_write_begin()' function did
+		 * @PAGE_SIZE, the 'ubifs_write_begin()' function did
 		 * not load it from the media (for optimization reasons). This
 		 * means that part of the page contains garbage. So read the
 		 * page now.
diff --git a/fs/ubifs/super.c b/fs/ubifs/super.c
index 59ff78d90694..420a38af56d2 100644
--- a/fs/ubifs/super.c
+++ b/fs/ubifs/super.c
@@ -2237,7 +2237,7 @@ static int __init ubifs_init(void)
 	BUILD_BUG_ON(UBIFS_COMPR_TYPES_CNT > 4);
 
 	/*
-	 * We require that PAGE_CACHE_SIZE is greater-than-or-equal-to
+	 * We require that PAGE_SIZE is greater-than-or-equal-to
 	 * UBIFS_BLOCK_SIZE. It is assumed that both are powers of 2.
 	 */
 	if (PAGE_CACHE_SIZE < UBIFS_BLOCK_SIZE) {
diff --git a/fs/xfs/xfs_aops.c b/fs/xfs/xfs_aops.c
index acf6c4a54883..a1df673de709 100644
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@ -1855,7 +1855,7 @@ xfs_vm_write_begin(
 	struct page		*page;
 	int			status;
 
-	ASSERT(len <= PAGE_CACHE_SIZE);
+	ASSERT(len <= PAGE_SIZE);
 
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
@@ -1908,7 +1908,7 @@ xfs_vm_write_end(
 {
 	int			ret;
 
-	ASSERT(len <= PAGE_CACHE_SIZE);
+	ASSERT(len <= PAGE_SIZE);
 
 	ret = generic_write_end(file, mapping, pos, len, copied, page, fsdata);
 	if (unlikely(ret < len)) {
diff --git a/fs/xfs/xfs_super.c b/fs/xfs/xfs_super.c
index 4dffe6e01151..b25d8ff70937 100644
--- a/fs/xfs/xfs_super.c
+++ b/fs/xfs/xfs_super.c
@@ -568,10 +568,10 @@ xfs_max_file_offset(
 	/* Figure out maximum filesize, on Linux this can depend on
 	 * the filesystem blocksize (on 32 bit platforms).
 	 * __block_write_begin does this in an [unsigned] long...
-	 *      page->index << (PAGE_CACHE_SHIFT - bbits)
+	 *      page->index << (PAGE_SHIFT - bbits)
 	 * So, for page sized blocks (4K on 32 bit platforms),
 	 * this wraps at around 8Tb (hence MAX_LFS_FILESIZE which is
-	 *      (((u64)PAGE_CACHE_SIZE << (BITS_PER_LONG-1))-1)
+	 *      (((u64)PAGE_SIZE << (BITS_PER_LONG-1))-1)
 	 * but for smaller blocksizes it is less (bbits = log2 bsize).
 	 * Note1: get_block_t takes a long (implicit cast from above)
 	 * Note2: The Large Block Device (LBD and HAVE_SECTOR_T) patch
* Unmerged path include/linux/backing-dev-defs.h
diff --git a/include/linux/mm.h b/include/linux/mm.h
index e7edaaec02da..01661d5e3bce 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -622,7 +622,7 @@ static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
  *
  * A page may belong to an inode's memory mapping. In this case, page->mapping
  * is the pointer to the inode, and page->index is the file offset of the page,
- * in units of PAGE_CACHE_SIZE.
+ * in units of PAGE_SIZE.
  *
  * If pagecache pages are not associated with an inode, they are said to be
  * anonymous pages. These may become associated with the swapcache, and in that
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 068f002863a5..d5f3a58ec6af 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -332,7 +332,7 @@ struct vm_area_struct {
 
 	/* Information about our backing store: */
 	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
-					   units, *not* PAGE_CACHE_SIZE */
+					   units */
 	struct file * vm_file;		/* File we map to (can be NULL). */
 	void * vm_private_data;		/* was vm_pte (shared mem) */
 
diff --git a/include/linux/nfs_page.h b/include/linux/nfs_page.h
index f2f650f136ee..ab5173fd19ff 100644
--- a/include/linux/nfs_page.h
+++ b/include/linux/nfs_page.h
@@ -41,8 +41,8 @@ struct nfs_page {
 	struct page		*wb_page;	/* page to read in/write out */
 	struct nfs_open_context	*wb_context;	/* File state context info */
 	struct nfs_lock_context	*wb_lock_context;	/* lock context info */
-	pgoff_t			wb_index;	/* Offset >> PAGE_CACHE_SHIFT */
-	unsigned int		wb_offset,	/* Offset & ~PAGE_CACHE_MASK */
+	pgoff_t			wb_index;	/* Offset >> PAGE_SHIFT */
+	unsigned int		wb_offset,	/* Offset & ~PAGE_MASK */
 				wb_pgbase,	/* Start of page data */
 				wb_bytes;	/* Length of request */
 	struct kref		wb_kref;	/* reference count */
diff --git a/include/linux/nilfs2_fs.h b/include/linux/nilfs2_fs.h
index 98755767c7b0..162912601693 100644
--- a/include/linux/nilfs2_fs.h
+++ b/include/linux/nilfs2_fs.h
@@ -329,7 +329,7 @@ static inline unsigned nilfs_rec_len_from_disk(__le16 dlen)
 {
 	unsigned len = le16_to_cpu(dlen);
 
-#if !defined(__KERNEL__) || (PAGE_CACHE_SIZE >= 65536)
+#if !defined(__KERNEL__) || (PAGE_SIZE >= 65536)
 	if (len == NILFS_MAX_REC_LEN)
 		return 1 << 16;
 #endif
@@ -338,7 +338,7 @@ static inline unsigned nilfs_rec_len_from_disk(__le16 dlen)
 
 static inline __le16 nilfs_rec_len_to_disk(unsigned len)
 {
-#if !defined(__KERNEL__) || (PAGE_CACHE_SIZE >= 65536)
+#if !defined(__KERNEL__) || (PAGE_SIZE >= 65536)
 	if (len == (1 << 16))
 		return cpu_to_le16(NILFS_MAX_REC_LEN);
 	else if (len > (1 << 16))
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index 8576311e4195..8c1b4d4210a8 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -446,8 +446,7 @@ extern void add_page_wait_queue(struct page *page, wait_queue_t *waiter);
 /*
  * Fault a userspace page into pagetables.  Return non-zero on a fault.
  *
- * This assumes that two userspace pages are always sufficient.  That's
- * not true if PAGE_CACHE_SIZE > PAGE_SIZE.
+ * This assumes that two userspace pages are always sufficient.
  */
 static inline int fault_in_pages_writeable(char __user *uaddr, int size)
 {
diff --git a/include/linux/sunrpc/svc.h b/include/linux/sunrpc/svc.h
index 2b3086893194..7321ae933867 100644
--- a/include/linux/sunrpc/svc.h
+++ b/include/linux/sunrpc/svc.h
@@ -129,7 +129,7 @@ static inline void svc_get(struct svc_serv *serv)
  *
  * These happen to all be powers of 2, which is not strictly
  * necessary but helps enforce the real limitation, which is
- * that they should be multiples of PAGE_CACHE_SIZE.
+ * that they should be multiples of PAGE_SIZE.
  *
  * For UDP transports, a block plus NFS,RPC, and UDP headers
  * has to fit into the IP datagram limit of 64K.  The largest
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 030df6e74d64..516c2f4e2041 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -454,7 +454,7 @@ mem_cgroup_uncharge_swapcache(struct page *page, swp_entry_t ent, bool swapout)
 #define si_swapinfo(val) \
 	do { (val)->freeswap = (val)->totalswap = 0; } while (0)
 /* only sparc can not include linux/pagemap.h in this file
- * so leave page_cache_release and release_pages undeclared... */
+ * so leave put_page and release_pages undeclared... */
 #define free_page_and_swap_cache(page) \
 	page_cache_release(page)
 #define free_pages_and_swap_cache(pages, nr) \
diff --git a/mm/gup.c b/mm/gup.c
index a09210b61df2..4ef6f1533bde 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -861,7 +861,7 @@ EXPORT_SYMBOL(get_user_pages);
  * @addr: user address
  *
  * Returns struct page pointer of user page pinned for dump,
- * to be freed afterwards by page_cache_release() or put_page().
+ * to be freed afterwards by put_page().
  *
  * Returns NULL on any kind of failure - a hole must then be inserted into
  * the corefile, to preserve alignment with its headers; and also returns
* Unmerged path mm/memory.c
diff --git a/mm/mincore.c b/mm/mincore.c
index ad411ec86a55..4b8591895036 100644
--- a/mm/mincore.c
+++ b/mm/mincore.c
@@ -265,7 +265,7 @@ static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *v
  * return values:
  *  zero    - success
  *  -EFAULT - vec points to an illegal address
- *  -EINVAL - addr is not a multiple of PAGE_CACHE_SIZE
+ *  -EINVAL - addr is not a multiple of PAGE_SIZE
  *  -ENOMEM - Addresses in the range [addr, addr + len] are
  *		invalid for the address space of this process, or
  *		specify one or more pages which are not currently
@@ -287,7 +287,7 @@ SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,
 	if (!access_ok(VERIFY_READ, (void __user *) start, len))
 		return -ENOMEM;
 
-	/* This also avoids any overflows on PAGE_CACHE_ALIGN */
+	/* This also avoids any overflows on PAGE_ALIGN */
 	pages = len >> PAGE_SHIFT;
 	pages += (len & ~PAGE_MASK) != 0;
 
* Unmerged path mm/swap.c
diff --git a/net/sunrpc/xdr.c b/net/sunrpc/xdr.c
index 18309b895cce..fab07ad4a034 100644
--- a/net/sunrpc/xdr.c
+++ b/net/sunrpc/xdr.c
@@ -164,7 +164,7 @@ EXPORT_SYMBOL_GPL(xdr_inline_pages);
  * Note: the addresses pgto_base and pgfrom_base are both calculated in
  *       the same way:
  *            if a memory area starts at byte 'base' in page 'pages[i]',
- *            then its address is given as (i << PAGE_CACHE_SHIFT) + base
+ *            then its address is given as (i << PAGE_SHIFT) + base
  * Also note: pgfrom_base must be < pgto_base, but the memory areas
  * 	they point to may overlap.
  */
