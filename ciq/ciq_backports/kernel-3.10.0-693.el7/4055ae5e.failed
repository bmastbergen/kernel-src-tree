cxgb4: Synchronize access to mailbox

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit 4055ae5e6d00e09ff4206843638323d1d5dfd85d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/4055ae5e.failed

The issue comes when there are multiple threads attempting to use
the mailbox facility at the same time.
When DCB operations and interface up/down is run in a loop for every
0.1 sec, we observed mailbox collisions. And out of the two commands
one would fail with the present code, since we don't queue the second
command.

To overcome the above issue, added a queue to access the mailbox.
Whenever a mailbox command is issued add it to the queue. If its at
the head issue the mailbox command, else wait for the existing command
to complete. Usually command takes less than a milli-second to
complete.

Also timeout from the loop, if the command under execution takes
long time to run.

In reality, the number of mailbox access collisions is going to be
very rare since no one runs such abusive script.

	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 4055ae5e6d00e09ff4206843638323d1d5dfd85d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index ecb195d07c07,ad0096e74813..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@@ -751,6 -770,27 +751,30 @@@ struct hash_mac_addr 
  	u8 addr[ETH_ALEN];
  };
  
++<<<<<<< HEAD
++=======
+ struct uld_msix_bmap {
+ 	unsigned long *msix_bmap;
+ 	unsigned int mapsize;
+ 	spinlock_t lock; /* lock for acquiring bitmap */
+ };
+ 
+ struct uld_msix_info {
+ 	unsigned short vec;
+ 	char desc[IFNAMSIZ + 10];
+ 	unsigned int idx;
+ };
+ 
+ struct vf_info {
+ 	unsigned char vf_mac_addr[ETH_ALEN];
+ 	bool pf_set_mac;
+ };
+ 
+ struct mbox_list {
+ 	struct list_head list;
+ };
+ 
++>>>>>>> 4055ae5e6d00 (cxgb4: Synchronize access to mailbox)
  struct adapter {
  	void __iomem *regs;
  	void __iomem *bar2;
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index f7843ae91302,9d2fe5140b88..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@@ -5101,6 -4706,10 +5101,13 @@@ static int init_one(struct pci_dev *pde
  
  	spin_lock_init(&adapter->stats_lock);
  	spin_lock_init(&adapter->tid_release_lock);
++<<<<<<< HEAD
++=======
+ 	spin_lock_init(&adapter->win0_lock);
+ 	spin_lock_init(&adapter->mbox_lock);
+ 
+ 	INIT_LIST_HEAD(&adapter->mlist.list);
++>>>>>>> 4055ae5e6d00 (cxgb4: Synchronize access to mailbox)
  
  	INIT_WORK(&adapter->tid_release_task, process_tid_release_list);
  	INIT_WORK(&adapter->db_full_task, process_db_full);
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
diff --git a/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c b/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
index 7be511590d42..6d8e280065fc 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
@@ -285,6 +285,7 @@ int t4_wr_mbox_meat_timeout(struct adapter *adap, int mbox, const void *cmd,
 		1, 1, 3, 5, 10, 10, 20, 50, 100, 200
 	};
 
+	struct mbox_list entry;
 	u16 access = 0;
 	u16 execute = 0;
 	u32 v;
@@ -312,11 +313,61 @@ int t4_wr_mbox_meat_timeout(struct adapter *adap, int mbox, const void *cmd,
 		timeout = -timeout;
 	}
 
+	/* Queue ourselves onto the mailbox access list.  When our entry is at
+	 * the front of the list, we have rights to access the mailbox.  So we
+	 * wait [for a while] till we're at the front [or bail out with an
+	 * EBUSY] ...
+	 */
+	spin_lock(&adap->mbox_lock);
+	list_add_tail(&entry.list, &adap->mlist.list);
+	spin_unlock(&adap->mbox_lock);
+
+	delay_idx = 0;
+	ms = delay[0];
+
+	for (i = 0; ; i += ms) {
+		/* If we've waited too long, return a busy indication.  This
+		 * really ought to be based on our initial position in the
+		 * mailbox access list but this is a start.  We very rearely
+		 * contend on access to the mailbox ...
+		 */
+		if (i > FW_CMD_MAX_TIMEOUT) {
+			spin_lock(&adap->mbox_lock);
+			list_del(&entry.list);
+			spin_unlock(&adap->mbox_lock);
+			ret = -EBUSY;
+			t4_record_mbox(adap, cmd, size, access, ret);
+			return ret;
+		}
+
+		/* If we're at the head, break out and start the mailbox
+		 * protocol.
+		 */
+		if (list_first_entry(&adap->mlist.list, struct mbox_list,
+				     list) == &entry)
+			break;
+
+		/* Delay for a bit before checking again ... */
+		if (sleep_ok) {
+			ms = delay[delay_idx];  /* last element may repeat */
+			if (delay_idx < ARRAY_SIZE(delay) - 1)
+				delay_idx++;
+			msleep(ms);
+		} else {
+			mdelay(ms);
+		}
+	}
+
+	/* Loop trying to get ownership of the mailbox.  Return an error
+	 * if we can't gain ownership.
+	 */
 	v = MBOWNER_G(t4_read_reg(adap, ctl_reg));
 	for (i = 0; v == MBOX_OWNER_NONE && i < 3; i++)
 		v = MBOWNER_G(t4_read_reg(adap, ctl_reg));
-
 	if (v != MBOX_OWNER_DRV) {
+		spin_lock(&adap->mbox_lock);
+		list_del(&entry.list);
+		spin_unlock(&adap->mbox_lock);
 		ret = (v == MBOX_OWNER_FW) ? -EBUSY : -ETIMEDOUT;
 		t4_record_mbox(adap, cmd, MBOX_LEN, access, ret);
 		return ret;
@@ -367,6 +418,9 @@ int t4_wr_mbox_meat_timeout(struct adapter *adap, int mbox, const void *cmd,
 			execute = i + ms;
 			t4_record_mbox(adap, cmd_rpl,
 				       MBOX_LEN, access, execute);
+			spin_lock(&adap->mbox_lock);
+			list_del(&entry.list);
+			spin_unlock(&adap->mbox_lock);
 			return -FW_CMD_RETVAL_G((int)res);
 		}
 	}
@@ -376,6 +430,9 @@ int t4_wr_mbox_meat_timeout(struct adapter *adap, int mbox, const void *cmd,
 	dev_err(adap->pdev_dev, "command %#x in mailbox %d timed out\n",
 		*(const u8 *)cmd, mbox);
 	t4_report_fw_error(adap);
+	spin_lock(&adap->mbox_lock);
+	list_del(&entry.list);
+	spin_unlock(&adap->mbox_lock);
 	return ret;
 }
 
