i40e/i40evf: eliminate i40e_pull_tail()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Scott Peterson <scott.d.peterson@intel.com>
commit 9b37c937313bf6769d0b018ca35180b379d40862
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/9b37c937.failed

Reorganize the i40e_pull_tail() logic, doing it in i40e_add_rx_frag()
where it's cheaper.  The igb driver does this the same way.

Also renames i40e_page_is_reserved() to reflect what it actually
tests.

Change-ID: Icd9cc507aae1fcdc02308b3a09034111b4c24071
	Signed-off-by: Scott Peterson <scott.d.peterson@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 9b37c937313bf6769d0b018ca35180b379d40862)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
#	drivers/net/ethernet/intel/i40evf/i40e_txrx.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index f75ac6dc63cd,1202b4c74da7..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -1474,29 -1390,369 +1474,373 @@@ static inline void i40e_rx_hash(struct 
  }
  
  /**
 - * i40e_process_skb_fields - Populate skb header fields from Rx descriptor
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @rx_desc: pointer to the EOP Rx descriptor
 - * @skb: pointer to current skb being populated
 - * @rx_ptype: the packet type decoded by hardware
 + * i40e_clean_rx_irq_1buf - Reclaim resources after receive; single buffer
 + * @rx_ring:  rx ring to clean
 + * @budget:   how many cleans we're allowed
   *
 - * This function checks the ring, descriptor, and packet information in
 - * order to populate the hash, checksum, VLAN, protocol, and
 - * other fields within the skb.
 + * Returns number of packets cleaned
   **/
++<<<<<<< HEAD
 +static int i40e_clean_rx_irq_1buf(struct i40e_ring *rx_ring, int budget)
++=======
+ static inline
+ void i40e_process_skb_fields(struct i40e_ring *rx_ring,
+ 			     union i40e_rx_desc *rx_desc, struct sk_buff *skb,
+ 			     u8 rx_ptype)
+ {
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	u32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>
+ 			I40E_RXD_QW1_STATUS_SHIFT;
+ 	u32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;
+ 	u32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
+ 		   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;
+ 
+ 	if (unlikely(tsynvalid))
+ 		i40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);
+ 
+ 	i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
+ 
+ 	/* modifies the skb - consumes the enet header */
+ 	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
+ 
+ 	i40e_rx_checksum(rx_ring->vsi, skb, rx_desc);
+ 
+ 	skb_record_rx_queue(skb, rx_ring->queue_index);
+ }
+ 
+ /**
+  * i40e_cleanup_headers - Correct empty headers
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being fixed
+  *
+  * Also address the case where we are pulling data in on pages only
+  * and as such no data is present in the skb header.
+  *
+  * In addition if skb is not at least 60 bytes we need to pad it so that
+  * it is large enough to qualify as a valid Ethernet frame.
+  *
+  * Returns true if an error was encountered and skb was freed.
+  **/
+ static bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	/* if eth_skb_pad returns an error the skb was freed */
+ 	if (eth_skb_pad(skb))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /**
+  * i40e_reuse_rx_page - page flip buffer and store it back on the ring
+  * @rx_ring: rx descriptor ring to store buffers on
+  * @old_buff: donor buffer to have page reused
+  *
+  * Synchronizes page for reuse by the adapter
+  **/
+ static void i40e_reuse_rx_page(struct i40e_ring *rx_ring,
+ 			       struct i40e_rx_buffer *old_buff)
+ {
+ 	struct i40e_rx_buffer *new_buff;
+ 	u16 nta = rx_ring->next_to_alloc;
+ 
+ 	new_buff = &rx_ring->rx_bi[nta];
+ 
+ 	/* update, and store next to alloc */
+ 	nta++;
+ 	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+ 
+ 	/* transfer page from old buffer to new buffer */
+ 	*new_buff = *old_buff;
+ }
+ 
+ /**
+  * i40e_page_is_reusable - check if any reuse is possible
+  * @page: page struct to check
+  *
+  * A page is not reusable if it was allocated under low memory
+  * conditions, or it's not in the same NUMA node as this CPU.
+  */
+ static inline bool i40e_page_is_reusable(struct page *page)
+ {
+ 	return (page_to_nid(page) == numa_mem_id()) &&
+ 		!page_is_pfmemalloc(page);
+ }
+ 
+ /**
+  * i40e_can_reuse_rx_page - Determine if this page can be reused by
+  * the adapter for another receive
+  *
+  * @rx_buffer: buffer containing the page
+  * @page: page address from rx_buffer
+  * @truesize: actual size of the buffer in this page
+  *
+  * If page is reusable, rx_buffer->page_offset is adjusted to point to
+  * an unused region in the page.
+  *
+  * For small pages, @truesize will be a constant value, half the size
+  * of the memory at page.  We'll attempt to alternate between high and
+  * low halves of the page, with one half ready for use by the hardware
+  * and the other half being consumed by the stack.  We use the page
+  * ref count to determine whether the stack has finished consuming the
+  * portion of this page that was passed up with a previous packet.  If
+  * the page ref count is >1, we'll assume the "other" half page is
+  * still busy, and this page cannot be reused.
+  *
+  * For larger pages, @truesize will be the actual space used by the
+  * received packet (adjusted upward to an even multiple of the cache
+  * line size).  This will advance through the page by the amount
+  * actually consumed by the received packets while there is still
+  * space for a buffer.  Each region of larger pages will be used at
+  * most once, after which the page will not be reused.
+  *
+  * In either case, if the page is reusable its refcount is increased.
+  **/
+ static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer,
+ 				   struct page *page,
+ 				   const unsigned int truesize)
+ {
+ #if (PAGE_SIZE >= 8192)
+ 	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
+ #endif
+ 
+ 	/* Is any reuse possible? */
+ 	if (unlikely(!i40e_page_is_reusable(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely(page_count(page) != 1))
+ 		return false;
+ 
+ 	/* flip page offset to other buffer */
+ 	rx_buffer->page_offset ^= truesize;
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buffer->page_offset += truesize;
+ 
+ 	if (rx_buffer->page_offset > last_offset)
+ 		return false;
+ #endif
+ 
+ 	/* Inc ref count on page before passing it up to the stack */
+ 	get_page(page);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_add_rx_frag - Add contents of Rx buffer to sk_buff
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_buffer: buffer containing page to add
+  * @size: packet length from rx_desc
+  * @skb: sk_buff to place the data into
+  *
+  * This function will add the data contained in rx_buffer->page to the skb.
+  * This is done either through a direct copy if the data in the buffer is
+  * less than the skb header size, otherwise it will just attach the page as
+  * a frag to the skb.
+  *
+  * The function will then update the page offset if necessary and return
+  * true if the buffer can be reused by the adapter.
+  **/
+ static bool i40e_add_rx_frag(struct i40e_ring *rx_ring,
+ 			     struct i40e_rx_buffer *rx_buffer,
+ 			     unsigned int size,
+ 			     struct sk_buff *skb)
+ {
+ 	struct page *page = rx_buffer->page;
+ 	unsigned char *va = page_address(page) + rx_buffer->page_offset;
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = I40E_RXBUFFER_2048;
+ #else
+ 	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+ #endif
+ 	unsigned int pull_len;
+ 
+ 	if (unlikely(skb_is_nonlinear(skb)))
+ 		goto add_tail_frag;
+ 
+ 	/* will the data fit in the skb we allocated? if so, just
+ 	 * copy it as it is pretty small anyway
+ 	 */
+ 	if (size <= I40E_RX_HDR_SIZE) {
+ 		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+ 
+ 		/* page is reusable, we can reuse buffer as-is */
+ 		if (likely(i40e_page_is_reusable(page)))
+ 			return true;
+ 
+ 		/* this page cannot be reused so discard it */
+ 		__free_pages(page, 0);
+ 		return false;
+ 	}
+ 
+ 	/* we need the header to contain the greater of either
+ 	 * ETH_HLEN or 60 bytes if the skb->len is less than
+ 	 * 60 for skb_pad.
+ 	 */
+ 	pull_len = eth_get_headlen(va, I40E_RX_HDR_SIZE);
+ 
+ 	/* align pull length to size of long to optimize
+ 	 * memcpy performance
+ 	 */
+ 	memcpy(__skb_put(skb, pull_len), va, ALIGN(pull_len, sizeof(long)));
+ 
+ 	/* update all of the pointers */
+ 	va += pull_len;
+ 	size -= pull_len;
+ 
+ add_tail_frag:
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 			(unsigned long)va & ~PAGE_MASK, size, truesize);
+ 
+ 	return i40e_can_reuse_rx_page(rx_buffer, page, truesize);
+ }
+ 
+ /**
+  * i40e_fetch_rx_buffer - Allocate skb and populate it
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_desc: descriptor containing info written by hardware
+  *
+  * This function allocates an skb on the fly, and populates it with the page
+  * data from the current receive descriptor, taking care to set up the skb
+  * correctly, as well as handling calling the page recycle function if
+  * necessary.
+  */
+ static inline
+ struct sk_buff *i40e_fetch_rx_buffer(struct i40e_ring *rx_ring,
+ 				     union i40e_rx_desc *rx_desc,
+ 				     struct sk_buff *skb)
+ {
+ 	u64 local_status_error_len =
+ 		le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	unsigned int size =
+ 		(local_status_error_len & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
+ 		I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
+ 	struct i40e_rx_buffer *rx_buffer;
+ 	struct page *page;
+ 
+ 	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+ 	page = rx_buffer->page;
+ 	prefetchw(page);
+ 
+ 	if (likely(!skb)) {
+ 		void *page_addr = page_address(page) + rx_buffer->page_offset;
+ 
+ 		/* prefetch first cache line of first page */
+ 		prefetch(page_addr);
+ #if L1_CACHE_BYTES < 128
+ 		prefetch(page_addr + L1_CACHE_BYTES);
+ #endif
+ 
+ 		/* allocate a skb to store the frags */
+ 		skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
+ 				       I40E_RX_HDR_SIZE,
+ 				       GFP_ATOMIC | __GFP_NOWARN);
+ 		if (unlikely(!skb)) {
+ 			rx_ring->rx_stats.alloc_buff_failed++;
+ 			return NULL;
+ 		}
+ 
+ 		/* we will be copying header into skb->data in
+ 		 * pskb_may_pull so it is in our interest to prefetch
+ 		 * it now to avoid a possible cache miss
+ 		 */
+ 		prefetchw(skb->data);
+ 	}
+ 
+ 	/* we are reusing so sync this buffer for CPU use */
+ 	dma_sync_single_range_for_cpu(rx_ring->dev,
+ 				      rx_buffer->dma,
+ 				      rx_buffer->page_offset,
+ 				      size,
+ 				      DMA_FROM_DEVICE);
+ 
+ 	/* pull page into skb */
+ 	if (i40e_add_rx_frag(rx_ring, rx_buffer, size, skb)) {
+ 		/* hand second half of page back to the ring */
+ 		i40e_reuse_rx_page(rx_ring, rx_buffer);
+ 		rx_ring->rx_stats.page_reuse_count++;
+ 	} else {
+ 		/* we are not reusing the buffer so unmap it */
+ 		dma_unmap_page(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+ 			       DMA_FROM_DEVICE);
+ 	}
+ 
+ 	/* clear contents of buffer_info */
+ 	rx_buffer->page = NULL;
+ 
+ 	return skb;
+ }
+ 
+ /**
+  * i40e_is_non_eop - process handling of non-EOP buffers
+  * @rx_ring: Rx ring being processed
+  * @rx_desc: Rx descriptor for current buffer
+  * @skb: Current socket buffer containing buffer in progress
+  *
+  * This function updates next to clean.  If the buffer is an EOP buffer
+  * this function exits returning false, otherwise it will place the
+  * sk_buff in the next buffer to be chained and return true indicating
+  * that this is in fact a non-EOP buffer.
+  **/
+ static bool i40e_is_non_eop(struct i40e_ring *rx_ring,
+ 			    union i40e_rx_desc *rx_desc,
+ 			    struct sk_buff *skb)
+ {
+ 	u32 ntc = rx_ring->next_to_clean + 1;
+ 
+ 	/* fetch, update, and store next to clean */
+ 	ntc = (ntc < rx_ring->count) ? ntc : 0;
+ 	rx_ring->next_to_clean = ntc;
+ 
+ 	prefetch(I40E_RX_DESC(rx_ring, ntc));
+ 
+ #define staterrlen rx_desc->wb.qword1.status_error_len
+ 	if (unlikely(i40e_rx_is_programming_status(le64_to_cpu(staterrlen)))) {
+ 		i40e_clean_programming_status(rx_ring, rx_desc);
+ 		return true;
+ 	}
+ 	/* if we are the last buffer then there is nothing else to do */
+ #define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)
+ 	if (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))
+ 		return false;
+ 
+ 	rx_ring->rx_stats.non_eop_descs++;
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @budget: Total limit on number of packets to process
+  *
+  * This function provides a "bounce buffer" approach to Rx interrupt
+  * processing.  The advantage to this is that on systems that have
+  * expensive overhead for IOMMU access this provides a means of avoiding
+  * it by maintaining the mapping of the page to the system.
+  *
+  * Returns amount of work completed
+  **/
+ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
++>>>>>>> 9b37c937313b (i40e/i40evf: eliminate i40e_pull_tail())
  {
  	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
 -	struct sk_buff *skb = rx_ring->skb;
  	u16 cleaned_count = I40E_DESC_UNUSED(rx_ring);
 +	struct i40e_vsi *vsi = rx_ring->vsi;
 +	union i40e_rx_desc *rx_desc;
 +	u32 rx_error, rx_status;
 +	u16 rx_packet_len;
  	bool failure = false;
 +	u8 rx_ptype;
 +	u64 qword;
 +	u16 i;
  
 -	while (likely(total_rx_packets < budget)) {
 -		union i40e_rx_desc *rx_desc;
 +	do {
 +		struct i40e_rx_buffer *rx_bi;
 +		struct sk_buff *skb;
  		u16 vlan_tag;
 -		u8 rx_ptype;
 -		u64 qword;
 -
  		/* return some buffers to hardware, one at a time is too slow */
  		if (cleaned_count >= I40E_RX_BUFFER_WRITE) {
  			failure = failure ||
diff --cc drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index 49386aab2722,b758846d4dc5..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@@ -1017,9 -1044,7 +1040,13 @@@ static bool i40e_add_rx_frag(struct i40
  			     struct sk_buff *skb)
  {
  	struct page *page = rx_buffer->page;
++<<<<<<< HEAD
 +	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
 +	unsigned int size = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
 +			    I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
++=======
+ 	unsigned char *va = page_address(page) + rx_buffer->page_offset;
++>>>>>>> 9b37c937313b (i40e/i40evf: eliminate i40e_pull_tail())
  #if (PAGE_SIZE < 8192)
  	unsigned int truesize = I40E_RXBUFFER_2048;
  #else
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
* Unmerged path drivers/net/ethernet/intel/i40evf/i40e_txrx.c
