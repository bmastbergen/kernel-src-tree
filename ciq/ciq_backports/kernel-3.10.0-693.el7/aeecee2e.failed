KVM: MMU: introduce kvm_mmu_slot_gfn_write_protect

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Xiao Guangrong <guangrong.xiao@linux.intel.com>
commit aeecee2ea6e2b020de8bb562f4e79ab34eda3e22
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/aeecee2e.failed

Split rmap_write_protect() and introduce the function to abstract the write
protection based on the slot

This function will be used in the later patch

	Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit aeecee2ea6e2b020de8bb562f4e79ab34eda3e22)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index d38cbe631292,7184218acf78..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -1345,24 -1336,30 +1345,44 @@@ void kvm_arch_mmu_enable_log_dirty_pt_m
  		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
  }
  
- static bool rmap_write_protect(struct kvm_vcpu *vcpu, u64 gfn)
+ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
+ 				    struct kvm_memory_slot *slot, u64 gfn)
  {
++<<<<<<< HEAD
 +	struct kvm_memory_slot *slot;
 +	unsigned long *rmapp;
++=======
+ 	struct kvm_rmap_head *rmap_head;
++>>>>>>> aeecee2ea6e2 (KVM: MMU: introduce kvm_mmu_slot_gfn_write_protect)
  	int i;
  	bool write_protected = false;
  
- 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
- 
  	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
++<<<<<<< HEAD
 +		rmapp = __gfn_to_rmap(gfn, i, slot);
 +		write_protected |= __rmap_write_protect(vcpu->kvm, rmapp, true);
++=======
+ 		rmap_head = __gfn_to_rmap(gfn, i, slot);
+ 		write_protected |= __rmap_write_protect(kvm, rmap_head, true);
++>>>>>>> aeecee2ea6e2 (KVM: MMU: introduce kvm_mmu_slot_gfn_write_protect)
  	}
  
  	return write_protected;
  }
  
++<<<<<<< HEAD
 +static bool kvm_zap_rmapp(struct kvm *kvm, unsigned long *rmapp)
++=======
+ static bool rmap_write_protect(struct kvm_vcpu *vcpu, u64 gfn)
+ {
+ 	struct kvm_memory_slot *slot;
+ 
+ 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ 	return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn);
+ }
+ 
+ static bool kvm_zap_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
++>>>>>>> aeecee2ea6e2 (KVM: MMU: introduce kvm_mmu_slot_gfn_write_protect)
  {
  	u64 *sptep;
  	struct rmap_iterator iter;
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index de92bed207f1..58fe98a0a526 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -177,4 +177,6 @@ void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end);
 
 void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
+bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
+				    struct kvm_memory_slot *slot, u64 gfn);
 #endif
