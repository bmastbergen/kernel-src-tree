vhost: new device IOTLB API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jason Wang <jasowang@redhat.com>
commit 6b1e6cc7855b09a0a9bfa1d9f30172ba366f161c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/6b1e6cc7.failed

This patch tries to implement an device IOTLB for vhost. This could be
used with userspace(qemu) implementation of DMA remapping
to emulate an IOMMU for the guest.

The idea is simple, cache the translation in a software device IOTLB
(which is implemented as an interval tree) in vhost and use vhost_net
file descriptor for reporting IOTLB miss and IOTLB
update/invalidation. When vhost meets an IOTLB miss, the fault
address, size and access can be read from the file. After userspace
finishes the translation, it writes the translated address to the
vhost_net file to update the device IOTLB.

When device IOTLB is enabled by setting VIRTIO_F_IOMMU_PLATFORM all vq
addresses set by ioctl are treated as iova instead of virtual address and
the accessing can only be done through IOTLB instead of direct userspace
memory access. Before each round or vq processing, all vq metadata is
prefetched in device IOTLB to make sure no translation fault happens
during vq processing.

In most cases, virtqueues are contiguous even in virtual address space.
The IOTLB translation for virtqueue itself may make it a little
slower. We might add fast path cache on top of this patch.

	Signed-off-by: Jason Wang <jasowang@redhat.com>
[mst: use virtio feature bit: VHOST_F_DEVICE_IOTLB -> VIRTIO_F_IOMMU_PLATFORM ]
[mst: fix build warnings ]
	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
[ weiyj.lk: missing unlock on error ]
	Signed-off-by: Wei Yongjun <weiyj.lk@gmail.com>
(cherry picked from commit 6b1e6cc7855b09a0a9bfa1d9f30172ba366f161c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/net.c
#	drivers/vhost/vhost.c
#	drivers/vhost/vhost.h
diff --cc drivers/vhost/net.c
index 13cfb2d2863d,0965f869dc57..000000000000
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@@ -62,7 -62,7 +62,11 @@@ enum 
  	VHOST_NET_FEATURES = VHOST_FEATURES |
  			 (1ULL << VHOST_NET_F_VIRTIO_NET_HDR) |
  			 (1ULL << VIRTIO_NET_F_MRG_RXBUF) |
++<<<<<<< HEAD
 +			 (1ULL << VIRTIO_F_VERSION_1),
++=======
+ 			 (1ULL << VIRTIO_F_IOMMU_PLATFORM)
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  };
  
  enum {
@@@ -356,10 -316,10 +360,10 @@@ static int vhost_net_tx_get_vq_desc(str
  		endtime = busy_clock() + vq->busyloop_timeout;
  		while (vhost_can_busy_poll(vq->dev, endtime) &&
  		       vhost_vq_avail_empty(vq->dev, vq))
 -			cpu_relax_lowlatency();
 +			cpu_relax();
  		preempt_enable();
  		r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
- 					out_num, in_num, NULL, NULL);
+ 				      out_num, in_num, NULL, NULL);
  	}
  
  	return r;
diff --cc drivers/vhost/vhost.c
index f65052baafd1,d02c1614921f..000000000000
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@@ -280,10 -306,11 +284,15 @@@ static void vhost_vq_reset(struct vhost
  	vq->call_ctx = NULL;
  	vq->call = NULL;
  	vq->log_ctx = NULL;
 -	vhost_reset_is_le(vq);
 -	vhost_disable_cross_endian(vq);
 +	vq->memory = NULL;
 +	vq->is_le = virtio_legacy_is_little_endian();
 +	vhost_vq_reset_user_be(vq);
  	vq->busyloop_timeout = 0;
++<<<<<<< HEAD
++=======
+ 	vq->umem = NULL;
+ 	vq->iotlb = NULL;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  }
  
  static int vhost_worker(void *data)
@@@ -387,25 -404,29 +396,39 @@@ long vhost_dev_init(struct vhost_dev *d
  	mutex_init(&dev->mutex);
  	dev->log_ctx = NULL;
  	dev->log_file = NULL;
++<<<<<<< HEAD
 +	dev->memory = NULL;
++=======
+ 	dev->umem = NULL;
+ 	dev->iotlb = NULL;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  	dev->mm = NULL;
 +	spin_lock_init(&dev->work_lock);
 +	INIT_LIST_HEAD(&dev->work_list);
  	dev->worker = NULL;
++<<<<<<< HEAD
++=======
+ 	init_llist_head(&dev->work_list);
+ 	init_waitqueue_head(&dev->wait);
+ 	INIT_LIST_HEAD(&dev->read_list);
+ 	INIT_LIST_HEAD(&dev->pending_list);
+ 	spin_lock_init(&dev->iotlb_lock);
+ 
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  
  	for (i = 0; i < dev->nvqs; ++i) {
 -		vq = dev->vqs[i];
 -		vq->log = NULL;
 -		vq->indirect = NULL;
 -		vq->heads = NULL;
 -		vq->dev = dev;
 -		mutex_init(&vq->mutex);
 -		vhost_vq_reset(dev, vq);
 -		if (vq->handle_kick)
 -			vhost_poll_init(&vq->poll, vq->handle_kick,
 -					POLLIN, dev);
 +		dev->vqs[i]->log = NULL;
 +		dev->vqs[i]->indirect = NULL;
 +		dev->vqs[i]->heads = NULL;
 +		dev->vqs[i]->dev = dev;
 +		mutex_init(&dev->vqs[i]->mutex);
 +		vhost_vq_reset(dev, dev->vqs[i]);
 +		if (dev->vqs[i]->handle_kick)
 +			vhost_poll_init(&dev->vqs[i]->poll,
 +					dev->vqs[i]->handle_kick, POLLIN, dev);
  	}
 +
 +	return 0;
  }
  EXPORT_SYMBOL_GPL(vhost_dev_init);
  
@@@ -530,6 -560,47 +553,50 @@@ void vhost_dev_stop(struct vhost_dev *d
  }
  EXPORT_SYMBOL_GPL(vhost_dev_stop);
  
++<<<<<<< HEAD
++=======
+ static void vhost_umem_free(struct vhost_umem *umem,
+ 			    struct vhost_umem_node *node)
+ {
+ 	vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+ 	list_del(&node->link);
+ 	kfree(node);
+ 	umem->numem--;
+ }
+ 
+ static void vhost_umem_clean(struct vhost_umem *umem)
+ {
+ 	struct vhost_umem_node *node, *tmp;
+ 
+ 	if (!umem)
+ 		return;
+ 
+ 	list_for_each_entry_safe(node, tmp, &umem->umem_list, link)
+ 		vhost_umem_free(umem, node);
+ 
+ 	kvfree(umem);
+ }
+ 
+ static void vhost_clear_msg(struct vhost_dev *dev)
+ {
+ 	struct vhost_msg_node *node, *n;
+ 
+ 	spin_lock(&dev->iotlb_lock);
+ 
+ 	list_for_each_entry_safe(node, n, &dev->read_list, node) {
+ 		list_del(&node->node);
+ 		kfree(node);
+ 	}
+ 
+ 	list_for_each_entry_safe(node, n, &dev->pending_list, node) {
+ 		list_del(&node->node);
+ 		kfree(node);
+ 	}
+ 
+ 	spin_unlock(&dev->iotlb_lock);
+ }
+ 
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  /* Caller should have device mutex if and only if locked is set */
  void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
  {
@@@ -556,9 -627,13 +623,19 @@@
  		fput(dev->log_file);
  	dev->log_file = NULL;
  	/* No one will access memory at this point */
++<<<<<<< HEAD
 +	kvfree(dev->memory);
 +	dev->memory = NULL;
 +	WARN_ON(!list_empty(&dev->work_list));
++=======
+ 	vhost_umem_clean(dev->umem);
+ 	dev->umem = NULL;
+ 	vhost_umem_clean(dev->iotlb);
+ 	dev->iotlb = NULL;
+ 	vhost_clear_msg(dev);
+ 	wake_up_interruptible_poll(&dev->wait, POLLIN | POLLRDNORM);
+ 	WARN_ON(!llist_empty(&dev->work_list));
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  	if (dev->worker) {
  		kthread_stop(dev->worker);
  		dev->worker = NULL;
@@@ -632,6 -708,373 +709,376 @@@ static int memory_access_ok(struct vhos
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
+ 			  struct iovec iov[], int iov_size, int access);
+ 
+ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void *to,
+ 			      const void *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_to_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that all vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		/* TODO: more fast path */
+ 		struct iov_iter t;
+ 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_WO);
+ 		if (ret < 0)
+ 			goto out;
+ 		iov_iter_init(&t, WRITE, vq->iotlb_iov, ret, size);
+ 		ret = copy_to_iter(from, size, &t);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ out:
+ 	return ret;
+ }
+ 
+ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
+ 				void *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_from_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		/* TODO: more fast path */
+ 		struct iov_iter f;
+ 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_RO);
+ 		if (ret < 0) {
+ 			vq_err(vq, "IOTLB translation failure: uaddr "
+ 			       "%p size 0x%llx\n", from,
+ 			       (unsigned long long) size);
+ 			goto out;
+ 		}
+ 		iov_iter_init(&f, READ, vq->iotlb_iov, ret, size);
+ 		ret = copy_from_iter(to, size, &f);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ 
+ out:
+ 	return ret;
+ }
+ 
+ static void __user *__vhost_get_user(struct vhost_virtqueue *vq,
+ 				     void *addr, unsigned size)
+ {
+ 	int ret;
+ 
+ 	/* This function should be called after iotlb
+ 	 * prefetch, which means we're sure that vq
+ 	 * could be access through iotlb. So -EAGAIN should
+ 	 * not happen in this case.
+ 	 */
+ 	/* TODO: more fast path */
+ 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ 			     ARRAY_SIZE(vq->iotlb_iov),
+ 			     VHOST_ACCESS_RO);
+ 	if (ret < 0) {
+ 		vq_err(vq, "IOTLB translation failure: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	if (ret != 1 || vq->iotlb_iov[0].iov_len != size) {
+ 		vq_err(vq, "Non atomic userspace memory access: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	return vq->iotlb_iov[0].iov_base;
+ }
+ 
+ #define vhost_put_user(vq, x, ptr) \
+ ({ \
+ 	int ret = -EFAULT; \
+ 	if (!vq->iotlb) { \
+ 		ret = __put_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) to = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \
+ 		if (to != NULL) \
+ 			ret = __put_user(x, to); \
+ 		else \
+ 			ret = -EFAULT;	\
+ 	} \
+ 	ret; \
+ })
+ 
+ #define vhost_get_user(vq, x, ptr) \
+ ({ \
+ 	int ret; \
+ 	if (!vq->iotlb) { \
+ 		ret = __get_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) from = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \
+ 		if (from != NULL) \
+ 			ret = __get_user(x, from); \
+ 		else \
+ 			ret = -EFAULT; \
+ 	} \
+ 	ret; \
+ })
+ 
+ static void vhost_dev_lock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_lock(&d->vqs[i]->mutex);
+ }
+ 
+ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_unlock(&d->vqs[i]->mutex);
+ }
+ 
+ static int vhost_new_umem_range(struct vhost_umem *umem,
+ 				u64 start, u64 size, u64 end,
+ 				u64 userspace_addr, int perm)
+ {
+ 	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
+ 
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	if (umem->numem == max_iotlb_entries) {
+ 		tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+ 		vhost_umem_free(umem, tmp);
+ 	}
+ 
+ 	node->start = start;
+ 	node->size = size;
+ 	node->last = end;
+ 	node->userspace_addr = userspace_addr;
+ 	node->perm = perm;
+ 	INIT_LIST_HEAD(&node->link);
+ 	list_add_tail(&node->link, &umem->umem_list);
+ 	vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+ 	umem->numem++;
+ 
+ 	return 0;
+ }
+ 
+ static void vhost_del_umem_range(struct vhost_umem *umem,
+ 				 u64 start, u64 end)
+ {
+ 	struct vhost_umem_node *node;
+ 
+ 	while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+ 							   start, end)))
+ 		vhost_umem_free(umem, node);
+ }
+ 
+ static void vhost_iotlb_notify_vq(struct vhost_dev *d,
+ 				  struct vhost_iotlb_msg *msg)
+ {
+ 	struct vhost_msg_node *node, *n;
+ 
+ 	spin_lock(&d->iotlb_lock);
+ 
+ 	list_for_each_entry_safe(node, n, &d->pending_list, node) {
+ 		struct vhost_iotlb_msg *vq_msg = &node->msg.iotlb;
+ 		if (msg->iova <= vq_msg->iova &&
+ 		    msg->iova + msg->size - 1 > vq_msg->iova &&
+ 		    vq_msg->type == VHOST_IOTLB_MISS) {
+ 			vhost_poll_queue(&node->vq->poll);
+ 			list_del(&node->node);
+ 			kfree(node);
+ 		}
+ 	}
+ 
+ 	spin_unlock(&d->iotlb_lock);
+ }
+ 
+ static int umem_access_ok(u64 uaddr, u64 size, int access)
+ {
+ 	unsigned long a = uaddr;
+ 
+ 	if ((access & VHOST_ACCESS_RO) &&
+ 	    !access_ok(VERIFY_READ, (void __user *)a, size))
+ 		return -EFAULT;
+ 	if ((access & VHOST_ACCESS_WO) &&
+ 	    !access_ok(VERIFY_WRITE, (void __user *)a, size))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ int vhost_process_iotlb_msg(struct vhost_dev *dev,
+ 			    struct vhost_iotlb_msg *msg)
+ {
+ 	int ret = 0;
+ 
+ 	vhost_dev_lock_vqs(dev);
+ 	switch (msg->type) {
+ 	case VHOST_IOTLB_UPDATE:
+ 		if (!dev->iotlb) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		if (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		if (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,
+ 					 msg->iova + msg->size - 1,
+ 					 msg->uaddr, msg->perm)) {
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 		vhost_iotlb_notify_vq(dev, msg);
+ 		break;
+ 	case VHOST_IOTLB_INVALIDATE:
+ 		vhost_del_umem_range(dev->iotlb, msg->iova,
+ 				     msg->iova + msg->size - 1);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	vhost_dev_unlock_vqs(dev);
+ 	return ret;
+ }
+ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
+ 			     struct iov_iter *from)
+ {
+ 	struct vhost_msg_node node;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 	size_t ret;
+ 	int err;
+ 
+ 	if (iov_iter_count(from) < size)
+ 		return 0;
+ 	ret = copy_from_iter(&node.msg, size, from);
+ 	if (ret != size)
+ 		goto done;
+ 
+ 	switch (node.msg.type) {
+ 	case VHOST_IOTLB_MSG:
+ 		err = vhost_process_iotlb_msg(dev, &node.msg.iotlb);
+ 		if (err)
+ 			ret = err;
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ done:
+ 	return ret;
+ }
+ EXPORT_SYMBOL(vhost_chr_write_iter);
+ 
+ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
+ 			    poll_table *wait)
+ {
+ 	unsigned int mask = 0;
+ 
+ 	poll_wait(file, &dev->wait, wait);
+ 
+ 	if (!list_empty(&dev->read_list))
+ 		mask |= POLLIN | POLLRDNORM;
+ 
+ 	return mask;
+ }
+ EXPORT_SYMBOL(vhost_chr_poll);
+ 
+ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
+ 			    int noblock)
+ {
+ 	DEFINE_WAIT(wait);
+ 	struct vhost_msg_node *node;
+ 	ssize_t ret = 0;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 
+ 	if (iov_iter_count(to) < size)
+ 		return 0;
+ 
+ 	while (1) {
+ 		if (!noblock)
+ 			prepare_to_wait(&dev->wait, &wait,
+ 					TASK_INTERRUPTIBLE);
+ 
+ 		node = vhost_dequeue_msg(dev, &dev->read_list);
+ 		if (node)
+ 			break;
+ 		if (noblock) {
+ 			ret = -EAGAIN;
+ 			break;
+ 		}
+ 		if (signal_pending(current)) {
+ 			ret = -ERESTARTSYS;
+ 			break;
+ 		}
+ 		if (!dev->iotlb) {
+ 			ret = -EBADFD;
+ 			break;
+ 		}
+ 
+ 		schedule();
+ 	}
+ 
+ 	if (!noblock)
+ 		finish_wait(&dev->wait, &wait);
+ 
+ 	if (node) {
+ 		ret = copy_to_iter(&node->msg, size, to);
+ 
+ 		if (ret != size || node->msg.type != VHOST_IOTLB_MISS) {
+ 			kfree(node);
+ 			return ret;
+ 		}
+ 
+ 		vhost_enqueue_msg(dev, &dev->pending_list, node);
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(vhost_chr_read_iter);
+ 
+ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
+ {
+ 	struct vhost_dev *dev = vq->dev;
+ 	struct vhost_msg_node *node;
+ 	struct vhost_iotlb_msg *msg;
+ 
+ 	node = vhost_new_msg(vq, VHOST_IOTLB_MISS);
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	msg = &node->msg.iotlb;
+ 	msg->type = VHOST_IOTLB_MISS;
+ 	msg->iova = iova;
+ 	msg->perm = access;
+ 
+ 	vhost_enqueue_msg(dev, &dev->read_list, node);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
  			struct vring_desc __user *desc,
  			struct vring_avail __user *avail,
@@@ -676,28 -1175,25 +1179,49 @@@ int vhost_vq_access_ok(struct vhost_vir
  }
  EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
  
++<<<<<<< HEAD
 +static int vhost_memory_reg_sort_cmp(const void *p1, const void *p2)
 +{
 +	const struct vhost_memory_region *r1 = p1, *r2 = p2;
 +	if (r1->guest_phys_addr < r2->guest_phys_addr)
 +		return 1;
 +	if (r1->guest_phys_addr > r2->guest_phys_addr)
 +		return -1;
 +	return 0;
 +}
 +
 +static void *vhost_kvzalloc(unsigned long size)
 +{
 +	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
 +
 +	if (!n)
 +		n = vzalloc(size);
 +	return n;
++=======
+ static struct vhost_umem *vhost_umem_alloc(void)
+ {
+ 	struct vhost_umem *umem = vhost_kvzalloc(sizeof(*umem));
+ 
+ 	if (!umem)
+ 		return NULL;
+ 
+ 	umem->umem_tree = RB_ROOT;
+ 	umem->numem = 0;
+ 	INIT_LIST_HEAD(&umem->umem_list);
+ 
+ 	return umem;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  }
  
  static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
  {
++<<<<<<< HEAD
 +	struct vhost_memory mem, *newmem, *oldmem;
++=======
+ 	struct vhost_memory mem, *newmem;
+ 	struct vhost_memory_region *region;
+ 	struct vhost_umem *newumem, *oldumem;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  	unsigned long size = offsetof(struct vhost_memory, regions);
  	int i;
  
@@@ -717,15 -1213,31 +1241,42 @@@
  		kvfree(newmem);
  		return -EFAULT;
  	}
 +	sort(newmem->regions, newmem->nregions, sizeof(*newmem->regions),
 +		vhost_memory_reg_sort_cmp, NULL);
  
++<<<<<<< HEAD
 +	if (!memory_access_ok(d, newmem, 0)) {
++=======
+ 	newumem = vhost_umem_alloc();
+ 	if (!newumem) {
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  		kvfree(newmem);
 -		return -ENOMEM;
 +		return -EFAULT;
  	}
++<<<<<<< HEAD
 +	oldmem = d->memory;
 +	d->memory = newmem;
++=======
+ 
+ 	for (region = newmem->regions;
+ 	     region < newmem->regions + mem.nregions;
+ 	     region++) {
+ 		if (vhost_new_umem_range(newumem,
+ 					 region->guest_phys_addr,
+ 					 region->memory_size,
+ 					 region->guest_phys_addr +
+ 					 region->memory_size - 1,
+ 					 region->userspace_addr,
+ 					 VHOST_ACCESS_RW))
+ 			goto err;
+ 	}
+ 
+ 	if (!memory_access_ok(d, newumem, 0))
+ 		goto err;
+ 
+ 	oldumem = d->umem;
+ 	d->umem = newumem;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  
  	/* All memory accesses are done under some VQ mutex. */
  	for (i = 0; i < d->nvqs; ++i) {
@@@ -1184,23 -1710,34 +1759,49 @@@ int vhost_init_used(struct vhost_virtqu
  
  	r = vhost_update_used_flags(vq);
  	if (r)
 -		goto err;
 +		return r;
  	vq->signalled_used_valid = false;
++<<<<<<< HEAD
 +	if (!access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx))
 +		return -EFAULT;
 +	r = __get_user(last_used_idx, &vq->used->idx);
 +	if (r)
 +		return r;
 +	vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
 +	return 0;
++=======
+ 	if (!vq->iotlb &&
+ 	    !access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx)) {
+ 		r = -EFAULT;
+ 		goto err;
+ 	}
+ 	r = vhost_get_user(vq, last_used_idx, &vq->used->idx);
+ 	if (r) {
+ 		vq_err(vq, "Can't access used idx at %p\n",
+ 		       &vq->used->idx);
+ 		goto err;
+ 	}
+ 	vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+ 	return 0;
+ 
+ err:
+ 	vq->is_le = is_le;
+ 	return r;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  }
 -EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 +EXPORT_SYMBOL_GPL(vhost_init_used);
  
  static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
- 			  struct iovec iov[], int iov_size)
+ 			  struct iovec iov[], int iov_size, int access)
  {
++<<<<<<< HEAD
 +	const struct vhost_memory_region *reg;
 +	struct vhost_memory *mem;
++=======
+ 	const struct vhost_umem_node *node;
+ 	struct vhost_dev *dev = vq->dev;
+ 	struct vhost_umem *umem = dev->iotlb ? dev->iotlb : dev->umem;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  	struct iovec *_iov;
  	u64 s = 0;
  	int ret = 0;
@@@ -1212,16 -1748,26 +1813,32 @@@
  			ret = -ENOBUFS;
  			break;
  		}
++<<<<<<< HEAD
 +		reg = find_region(mem, addr, len);
 +		if (unlikely(!reg)) {
 +			ret = -EFAULT;
++=======
+ 
+ 		node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+ 							addr, addr + len - 1);
+ 		if (node == NULL || node->start > addr) {
+ 			if (umem != dev->iotlb) {
+ 				ret = -EFAULT;
+ 				break;
+ 			}
+ 			ret = -EAGAIN;
+ 			break;
+ 		} else if (!(node->perm & access)) {
+ 			ret = -EPERM;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  			break;
  		}
+ 
  		_iov = iov + ret;
 -		size = node->size - addr + node->start;
 +		size = reg->memory_size - addr + reg->guest_phys_addr;
  		_iov->iov_len = min((u64)len - s, size);
  		_iov->iov_base = (void __user *)(unsigned long)
 -			(node->userspace_addr + addr - node->start);
 +			(reg->userspace_addr + addr - reg->guest_phys_addr);
  		s += size;
  		addr += size;
  		++ret;
@@@ -1260,7 -1808,8 +1879,12 @@@ static int get_indirect(struct vhost_vi
  	struct vring_desc desc;
  	unsigned int i = 0, count, found = 0;
  	u32 len = vhost32_to_cpu(vq, indirect->len);
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	struct iov_iter from;
+ 	int ret, access;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  
  	/* Sanity check */
  	if (unlikely(len % sizeof desc)) {
@@@ -1272,11 -1821,13 +1896,12 @@@
  	}
  
  	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
- 			     UIO_MAXIOV);
+ 			     UIO_MAXIOV, VHOST_ACCESS_RO);
  	if (unlikely(ret < 0)) {
- 		vq_err(vq, "Translation failure %d in indirect.\n", ret);
+ 		if (ret != -EAGAIN)
+ 			vq_err(vq, "Translation failure %d in indirect.\n", ret);
  		return ret;
  	}
 -	iov_iter_init(&from, READ, vq->indirect, ret, len);
  
  	/* We will use the result as an address to read from, so most
  	 * architectures only need a compiler barrier here. */
diff --cc drivers/vhost/vhost.h
index 5fd914ff48dd,78f3c5fc02e4..000000000000
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@@ -53,6 -55,27 +53,30 @@@ struct vhost_log 
  	u64 len;
  };
  
++<<<<<<< HEAD
++=======
+ #define START(node) ((node)->start)
+ #define LAST(node) ((node)->last)
+ 
+ struct vhost_umem_node {
+ 	struct rb_node rb;
+ 	struct list_head link;
+ 	__u64 start;
+ 	__u64 last;
+ 	__u64 size;
+ 	__u64 userspace_addr;
+ 	__u32 perm;
+ 	__u32 flags_padding;
+ 	__u64 __subtree_last;
+ };
+ 
+ struct vhost_umem {
+ 	struct rb_root umem_tree;
+ 	struct list_head umem_list;
+ 	int numem;
+ };
+ 
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  /* The virtqueue structure describes a queue attached to a device. */
  struct vhost_virtqueue {
  	struct vhost_dev *dev;
@@@ -101,7 -125,8 +126,12 @@@
  	struct iovec *indirect;
  	struct vring_used_elem *heads;
  	/* Protected by virtqueue mutex. */
++<<<<<<< HEAD
 +	struct vhost_memory *memory;
++=======
+ 	struct vhost_umem *umem;
+ 	struct vhost_umem *iotlb;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  	void *private_data;
  	u64 acked_features;
  	/* Log write descriptors */
@@@ -118,20 -143,30 +148,35 @@@
  	u32 busyloop_timeout;
  };
  
+ struct vhost_msg_node {
+   struct vhost_msg msg;
+   struct vhost_virtqueue *vq;
+   struct list_head node;
+ };
+ 
  struct vhost_dev {
 +	struct vhost_memory *memory;
  	struct mm_struct *mm;
  	struct mutex mutex;
  	struct vhost_virtqueue **vqs;
  	int nvqs;
  	struct file *log_file;
  	struct eventfd_ctx *log_ctx;
 -	struct llist_head work_list;
 +	spinlock_t work_lock;
 +	struct list_head work_list;
  	struct task_struct *worker;
++<<<<<<< HEAD
++=======
+ 	struct vhost_umem *umem;
+ 	struct vhost_umem *iotlb;
+ 	spinlock_t iotlb_lock;
+ 	struct list_head read_list;
+ 	struct list_head pending_list;
+ 	wait_queue_head_t wait;
++>>>>>>> 6b1e6cc7855b (vhost: new device IOTLB API)
  };
  
 -void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);
 +long vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);
  long vhost_dev_set_owner(struct vhost_dev *dev);
  bool vhost_dev_has_owner(struct vhost_dev *dev);
  long vhost_dev_check_owner(struct vhost_dev *);
* Unmerged path drivers/vhost/net.c
* Unmerged path drivers/vhost/vhost.c
* Unmerged path drivers/vhost/vhost.h
diff --git a/include/uapi/linux/vhost.h b/include/uapi/linux/vhost.h
index 61a8777178c6..8cb0a65afd64 100644
--- a/include/uapi/linux/vhost.h
+++ b/include/uapi/linux/vhost.h
@@ -47,6 +47,32 @@ struct vhost_vring_addr {
 	__u64 log_guest_addr;
 };
 
+/* no alignment requirement */
+struct vhost_iotlb_msg {
+	__u64 iova;
+	__u64 size;
+	__u64 uaddr;
+#define VHOST_ACCESS_RO      0x1
+#define VHOST_ACCESS_WO      0x2
+#define VHOST_ACCESS_RW      0x3
+	__u8 perm;
+#define VHOST_IOTLB_MISS           1
+#define VHOST_IOTLB_UPDATE         2
+#define VHOST_IOTLB_INVALIDATE     3
+#define VHOST_IOTLB_ACCESS_FAIL    4
+	__u8 type;
+};
+
+#define VHOST_IOTLB_MSG 0x1
+
+struct vhost_msg {
+	int type;
+	union {
+		struct vhost_iotlb_msg iotlb;
+		__u8 padding[64];
+	};
+};
+
 struct vhost_memory_region {
 	__u64 guest_phys_addr;
 	__u64 memory_size; /* bytes */
@@ -146,6 +172,8 @@ struct vhost_memory {
 #define VHOST_F_LOG_ALL 26
 /* vhost-net should add virtio_net_hdr for RX, and strip for TX packets. */
 #define VHOST_NET_F_VIRTIO_NET_HDR 27
+/* Vhost have device IOTLB */
+#define VHOST_F_DEVICE_IOTLB 63
 
 /* VHOST_SCSI specific definitions */
 
