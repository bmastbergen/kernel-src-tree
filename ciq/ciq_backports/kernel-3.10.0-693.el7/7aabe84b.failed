scsi: lpfc: sanity check hrq is null before dereferencing it

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] revert "lpfc: sanity check hrq is null before dereferencing it" (Ewan Milne) [1444045]
Rebuild_FUZZ: 89.43%
commit-author James Smart <jsmart2021@gmail.com>
commit 7aabe84b8a1bb7c3f75fb6a23dc96f8999bdcc8f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/7aabe84b.failed

From: Colin Ian King <colin.king@canonical.com>

The sanity check for hrq should be moved to before the deference
of hrq to ensure we don't perform a null pointer deference.

Detected by CoverityScan, CID#1411650 ("Dereference before null check")

	Signed-off-by: Colin Ian King <colin.king@canonical.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 7aabe84b8a1bb7c3f75fb6a23dc96f8999bdcc8f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_sli.c
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c1522c6b2e42,562a5286bc47..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -13983,6 -15108,197 +13983,200 @@@ out
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_mrq_create - Create MRQ Receive Queues on the HBA
+  * @phba: HBA structure that indicates port to create a queue on.
+  * @hrqp: The queue structure array to use to create the header receive queues.
+  * @drqp: The queue structure array to use to create the data receive queues.
+  * @cqp: The completion queue array to bind these receive queues to.
+  *
+  * This function creates a receive buffer queue pair , as detailed in @hrq and
+  * @drq, on a port, described by @phba by sending a RQ_CREATE mailbox command
+  * to the HBA.
+  *
+  * The @phba struct is used to send mailbox command to HBA. The @drq and @hrq
+  * struct is used to get the entry count that is necessary to determine the
+  * number of pages to use for this queue. The @cq is used to indicate which
+  * completion queue to bind received buffers that are posted to these queues to.
+  * This function will send the RQ_CREATE mailbox command to the HBA to setup the
+  * receive queue pair. This function is asynchronous and will wait for the
+  * mailbox command to finish before continuing.
+  *
+  * On success this function will return a zero. If unable to allocate enough
+  * memory this function will return -ENOMEM. If the queue create mailbox command
+  * fails this function will return -ENXIO.
+  **/
+ int
+ lpfc_mrq_create(struct lpfc_hba *phba, struct lpfc_queue **hrqp,
+ 		struct lpfc_queue **drqp, struct lpfc_queue **cqp,
+ 		uint32_t subtype)
+ {
+ 	struct lpfc_queue *hrq, *drq, *cq;
+ 	struct lpfc_mbx_rq_create_v2 *rq_create;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	LPFC_MBOXQ_t *mbox;
+ 	int rc, length, alloclen, status = 0;
+ 	int cnt, idx, numrq, page_idx = 0;
+ 	uint32_t shdr_status, shdr_add_status;
+ 	union lpfc_sli4_cfg_shdr *shdr;
+ 	uint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;
+ 
+ 	numrq = phba->cfg_nvmet_mrq;
+ 	/* sanity check on array memory */
+ 	if (!hrqp || !drqp || !cqp || !numrq)
+ 		return -ENODEV;
+ 	if (!phba->sli4_hba.pc_sli4_params.supported)
+ 		hw_page_size = SLI4_PAGE_SIZE;
+ 
+ 	mbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);
+ 	if (!mbox)
+ 		return -ENOMEM;
+ 
+ 	length = sizeof(struct lpfc_mbx_rq_create_v2);
+ 	length += ((2 * numrq * hrqp[0]->page_count) *
+ 		   sizeof(struct dma_address));
+ 
+ 	alloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,
+ 				    LPFC_MBOX_OPCODE_FCOE_RQ_CREATE, length,
+ 				    LPFC_SLI4_MBX_NEMBED);
+ 	if (alloclen < length) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"3099 Allocated DMA memory size (%d) is "
+ 				"less than the requested DMA memory size "
+ 				"(%d)\n", alloclen, length);
+ 		status = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 
+ 
+ 	rq_create = mbox->sge_array->addr[0];
+ 	shdr = (union lpfc_sli4_cfg_shdr *)&rq_create->cfg_shdr;
+ 
+ 	bf_set(lpfc_mbox_hdr_version, &shdr->request, LPFC_Q_CREATE_VERSION_2);
+ 	cnt = 0;
+ 
+ 	for (idx = 0; idx < numrq; idx++) {
+ 		hrq = hrqp[idx];
+ 		drq = drqp[idx];
+ 		cq  = cqp[idx];
+ 
+ 		/* sanity check on queue memory */
+ 		if (!hrq || !drq || !cq) {
+ 			status = -ENODEV;
+ 			goto out;
+ 		}
+ 
+ 		if (hrq->entry_count != drq->entry_count) {
+ 			status = -EINVAL;
+ 			goto out;
+ 		}
+ 
+ 		if (idx == 0) {
+ 			bf_set(lpfc_mbx_rq_create_num_pages,
+ 			       &rq_create->u.request,
+ 			       hrq->page_count);
+ 			bf_set(lpfc_mbx_rq_create_rq_cnt,
+ 			       &rq_create->u.request, (numrq * 2));
+ 			bf_set(lpfc_mbx_rq_create_dnb, &rq_create->u.request,
+ 			       1);
+ 			bf_set(lpfc_rq_context_base_cq,
+ 			       &rq_create->u.request.context,
+ 			       cq->queue_id);
+ 			bf_set(lpfc_rq_context_data_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_DATA_BUF_SIZE);
+ 			bf_set(lpfc_rq_context_hdr_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_HDR_BUF_SIZE);
+ 			bf_set(lpfc_rq_context_rqe_count_1,
+ 			       &rq_create->u.request.context,
+ 			       hrq->entry_count);
+ 			bf_set(lpfc_rq_context_rqe_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_RQE_SIZE_8);
+ 			bf_set(lpfc_rq_context_page_size,
+ 			       &rq_create->u.request.context,
+ 			       (PAGE_SIZE/SLI4_PAGE_SIZE));
+ 		}
+ 		rc = 0;
+ 		list_for_each_entry(dmabuf, &hrq->page_list, list) {
+ 			memset(dmabuf->virt, 0, hw_page_size);
+ 			cnt = page_idx + dmabuf->buffer_tag;
+ 			rq_create->u.request.page[cnt].addr_lo =
+ 					putPaddrLow(dmabuf->phys);
+ 			rq_create->u.request.page[cnt].addr_hi =
+ 					putPaddrHigh(dmabuf->phys);
+ 			rc++;
+ 		}
+ 		page_idx += rc;
+ 
+ 		rc = 0;
+ 		list_for_each_entry(dmabuf, &drq->page_list, list) {
+ 			memset(dmabuf->virt, 0, hw_page_size);
+ 			cnt = page_idx + dmabuf->buffer_tag;
+ 			rq_create->u.request.page[cnt].addr_lo =
+ 					putPaddrLow(dmabuf->phys);
+ 			rq_create->u.request.page[cnt].addr_hi =
+ 					putPaddrHigh(dmabuf->phys);
+ 			rc++;
+ 		}
+ 		page_idx += rc;
+ 
+ 		hrq->db_format = LPFC_DB_RING_FORMAT;
+ 		hrq->db_regaddr = phba->sli4_hba.RQDBregaddr;
+ 		hrq->type = LPFC_HRQ;
+ 		hrq->assoc_qid = cq->queue_id;
+ 		hrq->subtype = subtype;
+ 		hrq->host_index = 0;
+ 		hrq->hba_index = 0;
+ 
+ 		drq->db_format = LPFC_DB_RING_FORMAT;
+ 		drq->db_regaddr = phba->sli4_hba.RQDBregaddr;
+ 		drq->type = LPFC_DRQ;
+ 		drq->assoc_qid = cq->queue_id;
+ 		drq->subtype = subtype;
+ 		drq->host_index = 0;
+ 		drq->hba_index = 0;
+ 
+ 		list_add_tail(&hrq->list, &cq->child_list);
+ 		list_add_tail(&drq->list, &cq->child_list);
+ 	}
+ 
+ 	rc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);
+ 	/* The IOCTL status is embedded in the mailbox subheader. */
+ 	shdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);
+ 	shdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);
+ 	if (shdr_status || shdr_add_status || rc) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"3120 RQ_CREATE mailbox failed with "
+ 				"status x%x add_status x%x, mbx status x%x\n",
+ 				shdr_status, shdr_add_status, rc);
+ 		status = -ENXIO;
+ 		goto out;
+ 	}
+ 	rc = bf_get(lpfc_mbx_rq_create_q_id, &rq_create->u.response);
+ 	if (rc == 0xFFFF) {
+ 		status = -ENXIO;
+ 		goto out;
+ 	}
+ 
+ 	/* Initialize all RQs with associated queue id */
+ 	for (idx = 0; idx < numrq; idx++) {
+ 		hrq = hrqp[idx];
+ 		hrq->queue_id = rc + (2 * idx);
+ 		drq = drqp[idx];
+ 		drq->queue_id = rc + (2 * idx) + 1;
+ 	}
+ 
+ out:
+ 	lpfc_sli4_mbox_cmd_free(phba, mbox);
+ 	return status;
+ }
+ 
+ /**
++>>>>>>> 7aabe84b8a1b (scsi: lpfc: sanity check hrq is null before dereferencing it)
   * lpfc_eq_destroy - Destroy an event Queue on the HBA
   * @eq: The queue structure associated with the queue to destroy.
   *
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
