amd-xgbe: Add the __GFP_NOWARN flag to Rx buffer allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 472cfe7127760d68b819cf35a26e5a1b44b30f4e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/472cfe71.failed

When allocating Rx related buffers, alloc_pages is called using an order
number that is decreased until successful. A system under stress can
experience failures during this allocation process resulting in a warning
being issued. This message can be of concern to end users even though the
failure is not fatal. Since the failure is not fatal and can occur
multiple times, the driver should include the __GFP_NOWARN flag to
suppress the warning message from being issued.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 472cfe7127760d68b819cf35a26e5a1b44b30f4e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-desc.c
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-desc.c
index 28954354521f,661cdaa7ea96..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
@@@ -234,6 -260,96 +234,99 @@@ err_ring
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int xgbe_alloc_pages(struct xgbe_prv_data *pdata,
+ 			    struct xgbe_page_alloc *pa, gfp_t gfp, int order)
+ {
+ 	struct page *pages = NULL;
+ 	dma_addr_t pages_dma;
+ 	int ret;
+ 
+ 	/* Try to obtain pages, decreasing order if necessary */
+ 	gfp |= __GFP_COLD | __GFP_COMP | __GFP_NOWARN;
+ 	while (order >= 0) {
+ 		pages = alloc_pages(gfp, order);
+ 		if (pages)
+ 			break;
+ 
+ 		order--;
+ 	}
+ 	if (!pages)
+ 		return -ENOMEM;
+ 
+ 	/* Map the pages */
+ 	pages_dma = dma_map_page(pdata->dev, pages, 0,
+ 				 PAGE_SIZE << order, DMA_FROM_DEVICE);
+ 	ret = dma_mapping_error(pdata->dev, pages_dma);
+ 	if (ret) {
+ 		put_page(pages);
+ 		return ret;
+ 	}
+ 
+ 	pa->pages = pages;
+ 	pa->pages_len = PAGE_SIZE << order;
+ 	pa->pages_offset = 0;
+ 	pa->pages_dma = pages_dma;
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_set_buffer_data(struct xgbe_buffer_data *bd,
+ 				 struct xgbe_page_alloc *pa,
+ 				 unsigned int len)
+ {
+ 	get_page(pa->pages);
+ 	bd->pa = *pa;
+ 
+ 	bd->dma = pa->pages_dma + pa->pages_offset;
+ 	bd->dma_len = len;
+ 
+ 	pa->pages_offset += len;
+ 	if ((pa->pages_offset + len) > pa->pages_len) {
+ 		/* This data descriptor is responsible for unmapping page(s) */
+ 		bd->pa_unmap = *pa;
+ 
+ 		/* Get a new allocation next time */
+ 		pa->pages = NULL;
+ 		pa->pages_len = 0;
+ 		pa->pages_offset = 0;
+ 		pa->pages_dma = 0;
+ 	}
+ }
+ 
+ static int xgbe_map_rx_buffer(struct xgbe_prv_data *pdata,
+ 			      struct xgbe_ring *ring,
+ 			      struct xgbe_ring_data *rdata)
+ {
+ 	int order, ret;
+ 
+ 	if (!ring->rx_hdr_pa.pages) {
+ 		ret = xgbe_alloc_pages(pdata, &ring->rx_hdr_pa, GFP_ATOMIC, 0);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	if (!ring->rx_buf_pa.pages) {
+ 		order = max_t(int, PAGE_ALLOC_COSTLY_ORDER - 1, 0);
+ 		ret = xgbe_alloc_pages(pdata, &ring->rx_buf_pa, GFP_ATOMIC,
+ 				       order);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/* Set up the header page info */
+ 	xgbe_set_buffer_data(&rdata->rx.hdr, &ring->rx_hdr_pa,
+ 			     XGBE_SKB_ALLOC_SIZE);
+ 
+ 	/* Set up the buffer page info */
+ 	xgbe_set_buffer_data(&rdata->rx.buf, &ring->rx_buf_pa,
+ 			     pdata->rx_buf_size);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 472cfe712776 (amd-xgbe: Add the __GFP_NOWARN flag to Rx buffer allocation)
  static void xgbe_wrapper_tx_descriptor_init(struct xgbe_prv_data *pdata)
  {
  	struct xgbe_hw_if *hw_if = &pdata->hw_if;
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-desc.c
