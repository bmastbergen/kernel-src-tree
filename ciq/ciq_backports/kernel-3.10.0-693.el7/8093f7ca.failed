nvme: add helper nvme_setup_cmd()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [nvme] add helper nvme_setup_cmd() (David Milburn) [1384066]
Rebuild_FUZZ: 90.00%
commit-author Ming Lin <mlin@kernel.org>
commit 8093f7ca73c1633e458c16a74b51bcc3c94564c4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/8093f7ca.failed

This moves nvme_setup_{flush,discard,rw} calls into a common
nvme_setup_cmd() helper. So we can eventually hide all the command
setup in the core module and don't even need to update the fabrics
drivers for any specific command type.

	Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 8093f7ca73c1633e458c16a74b51bcc3c94564c4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/nvme.h
diff --cc drivers/nvme/host/nvme.h
index c84f7f8e647f,8e8fae8722f8..000000000000
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@@ -180,41 -173,12 +180,50 @@@ static inline u64 nvme_block_nr(struct 
  	return (sector >> (ns->lba_shift - 9));
  }
  
++<<<<<<< HEAD
 +static inline void nvme_setup_flush(struct nvme_ns *ns, struct nvme_command *cmnd)
 +{
 +	memset(cmnd, 0, sizeof(*cmnd));
 +	cmnd->common.opcode = nvme_cmd_flush;
 +	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
 +}
 +
 +static inline void nvme_setup_rw(struct nvme_ns *ns, struct request *req,
 +		struct nvme_command *cmnd)
 +{
 +	u16 control = 0;
 +	u32 dsmgmt = 0;
 +
 +	if (req->cmd_flags & REQ_FUA)
 +		control |= NVME_RW_FUA;
 +	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
 +		control |= NVME_RW_LR;
 +
 +	if (req->cmd_flags & REQ_RAHEAD)
 +		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
 +
 +	memset(cmnd, 0, sizeof(*cmnd));
 +	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
 +	cmnd->rw.command_id = req->tag;
 +	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
 +	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
 +	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
 +
 +	if (ns->ms) {
 +		if (!blk_integrity_rq(req))
 +			control |= NVME_RW_PRINFO_PRACT;
 +	}
 +
 +	cmnd->rw.control = cpu_to_le16(control);
 +	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
++=======
+ static inline unsigned nvme_map_len(struct request *rq)
+ {
+ 	if (rq->cmd_flags & REQ_DISCARD)
+ 		return sizeof(struct nvme_dsm_range);
+ 	else
+ 		return blk_rq_bytes(rq);
++>>>>>>> 8093f7ca73c1 (nvme: add helper nvme_setup_cmd())
  }
  
  static inline int nvme_error_status(u16 status)
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 438a32d6bf31..983544a98e48 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -132,6 +132,111 @@ struct request *nvme_alloc_request(struct request_queue *q,
 	return req;
 }
 
+static inline void nvme_setup_flush(struct nvme_ns *ns,
+		struct nvme_command *cmnd)
+{
+	memset(cmnd, 0, sizeof(*cmnd));
+	cmnd->common.opcode = nvme_cmd_flush;
+	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
+}
+
+static inline int nvme_setup_discard(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmnd)
+{
+	struct nvme_dsm_range *range;
+	struct page *page;
+	int offset;
+	unsigned int nr_bytes = blk_rq_bytes(req);
+
+	range = kmalloc(sizeof(*range), GFP_ATOMIC);
+	if (!range)
+		return BLK_MQ_RQ_QUEUE_BUSY;
+
+	range->cattr = cpu_to_le32(0);
+	range->nlb = cpu_to_le32(nr_bytes >> ns->lba_shift);
+	range->slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+
+	memset(cmnd, 0, sizeof(*cmnd));
+	cmnd->dsm.opcode = nvme_cmd_dsm;
+	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
+	cmnd->dsm.nr = 0;
+	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
+
+	req->completion_data = range;
+	page = virt_to_page(range);
+	offset = offset_in_page(range);
+	blk_add_request_payload(req, page, offset, sizeof(*range));
+
+	/*
+	 * we set __data_len back to the size of the area to be discarded
+	 * on disk. This allows us to report completion on the full amount
+	 * of blocks described by the request.
+	 */
+	req->__data_len = nr_bytes;
+
+	return 0;
+}
+
+static inline void nvme_setup_rw(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmnd)
+{
+	u16 control = 0;
+	u32 dsmgmt = 0;
+
+	if (req->cmd_flags & REQ_FUA)
+		control |= NVME_RW_FUA;
+	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
+		control |= NVME_RW_LR;
+
+	if (req->cmd_flags & REQ_RAHEAD)
+		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
+
+	memset(cmnd, 0, sizeof(*cmnd));
+	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
+	cmnd->rw.command_id = req->tag;
+	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
+	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
+
+	if (ns->ms) {
+		switch (ns->pi_type) {
+		case NVME_NS_DPS_PI_TYPE3:
+			control |= NVME_RW_PRINFO_PRCHK_GUARD;
+			break;
+		case NVME_NS_DPS_PI_TYPE1:
+		case NVME_NS_DPS_PI_TYPE2:
+			control |= NVME_RW_PRINFO_PRCHK_GUARD |
+					NVME_RW_PRINFO_PRCHK_REF;
+			cmnd->rw.reftag = cpu_to_le32(
+					nvme_block_nr(ns, blk_rq_pos(req)));
+			break;
+		}
+		if (!blk_integrity_rq(req))
+			control |= NVME_RW_PRINFO_PRACT;
+	}
+
+	cmnd->rw.control = cpu_to_le16(control);
+	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
+}
+
+int nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmd)
+{
+	int ret = 0;
+
+	if (req->cmd_type == REQ_TYPE_DRV_PRIV)
+		memcpy(cmd, req->cmd, sizeof(*cmd));
+	else if (req->cmd_flags & REQ_FLUSH)
+		nvme_setup_flush(ns, cmd);
+	else if (req->cmd_flags & REQ_DISCARD)
+		ret = nvme_setup_discard(ns, req, cmd);
+	else
+		nvme_setup_rw(ns, req, cmd);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_setup_cmd);
+
 /*
  * Returns 0 on success.  If the result is negative, it's a Linux error code;
  * if the result is positive, it's an NVM Express status code
* Unmerged path drivers/nvme/host/nvme.h
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 954a8c9d5b8e..ccc845e9c566 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -525,43 +525,6 @@ static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
 	nvme_free_iod(dev, req);
 }
 
-static inline int nvme_setup_discard(struct nvme_ns *ns, struct request *req,
-		struct nvme_command *cmnd)
-{
-	struct nvme_dsm_range *range;
-	struct page *page;
-	int offset;
-	unsigned int nr_bytes = blk_rq_bytes(req);
-
-	range = kmalloc(sizeof(*range), GFP_ATOMIC);
-	if (!range)
-		return BLK_MQ_RQ_QUEUE_BUSY;
-
-	range->cattr = cpu_to_le32(0);
-	range->nlb = cpu_to_le32(nr_bytes >> ns->lba_shift);
-	range->slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->dsm.opcode = nvme_cmd_dsm;
-	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->dsm.nr = 0;
-	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	req->completion_data = range;
-	page = virt_to_page(range);
-	offset = offset_in_page(range);
-	blk_add_request_payload(req, page, offset, sizeof(*range));
-
-	/*
-	 * we set __data_len back to the size of the area to be discarded
-	 * on disk. This allows us to report completion on the full amount
-	 * of blocks described by the request.
-	 */
-	req->__data_len = nr_bytes;
-
-	return 0;
-}
-
 /*
  * NOTE: ns is NULL when called on the admin queue.
  */
@@ -592,15 +555,7 @@ static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (ret)
 		return ret;
 
-	if (req->cmd_type == REQ_TYPE_DRV_PRIV)
-		memcpy(&cmnd, req->cmd, sizeof(cmnd));
-	else if (req->cmd_flags & REQ_FLUSH)
-		nvme_setup_flush(ns, &cmnd);
-	else if (req->cmd_flags & REQ_DISCARD)
-		ret = nvme_setup_discard(ns, req, &cmnd);
-	else
-		nvme_setup_rw(ns, req, &cmnd);
-
+	ret = nvme_setup_cmd(ns, req, &cmnd);
 	if (ret)
 		goto out;
 
