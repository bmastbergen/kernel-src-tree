nvme: move nvme_cancel_request() to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [nvme] move nvme_cancel_request() to common code (David Milburn) [1384526 1389755 1366753 1374291 1383834]
Rebuild_FUZZ: 93.18%
commit-author Ming Lin <ming.l@samsung.com>
commit c55a2fd4bb16bcdd8c42e3d64fccd326416b7492
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c55a2fd4.failed

So it can be used by fabrics driver also.

	Signed-off-by: Ming Lin <ming.l@samsung.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Reviewed-by: Keith Busch <keith.bsuch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit c55a2fd4bb16bcdd8c42e3d64fccd326416b7492)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/core.c
#	drivers/nvme/host/nvme.h
#	drivers/nvme/host/pci.c
diff --cc drivers/nvme/host/core.c
index 15fd21c9c560,24f91fbd253c..000000000000
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -56,6 -58,81 +56,84 @@@ DEFINE_SPINLOCK(dev_list_lock)
  
  static struct class *nvme_class;
  
++<<<<<<< HEAD
++=======
+ void nvme_cancel_request(struct request *req, void *data, bool reserved)
+ {
+ 	int status;
+ 
+ 	if (!blk_mq_request_started(req))
+ 		return;
+ 
+ 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
+ 				"Cancelling I/O %d", req->tag);
+ 
+ 	status = NVME_SC_ABORT_REQ;
+ 	if (blk_queue_dying(req->q))
+ 		status |= NVME_SC_DNR;
+ 	blk_mq_complete_request(req, status);
+ }
+ EXPORT_SYMBOL_GPL(nvme_cancel_request);
+ 
+ bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
+ 		enum nvme_ctrl_state new_state)
+ {
+ 	enum nvme_ctrl_state old_state = ctrl->state;
+ 	bool changed = false;
+ 
+ 	spin_lock_irq(&ctrl->lock);
+ 	switch (new_state) {
+ 	case NVME_CTRL_LIVE:
+ 		switch (old_state) {
+ 		case NVME_CTRL_RESETTING:
+ 			changed = true;
+ 			/* FALLTHRU */
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 	case NVME_CTRL_RESETTING:
+ 		switch (old_state) {
+ 		case NVME_CTRL_NEW:
+ 		case NVME_CTRL_LIVE:
+ 			changed = true;
+ 			/* FALLTHRU */
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 	case NVME_CTRL_DELETING:
+ 		switch (old_state) {
+ 		case NVME_CTRL_LIVE:
+ 		case NVME_CTRL_RESETTING:
+ 			changed = true;
+ 			/* FALLTHRU */
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 	case NVME_CTRL_DEAD:
+ 		switch (old_state) {
+ 		case NVME_CTRL_DELETING:
+ 			changed = true;
+ 			/* FALLTHRU */
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	spin_unlock_irq(&ctrl->lock);
+ 
+ 	if (changed)
+ 		ctrl->state = new_state;
+ 
+ 	return changed;
+ }
+ EXPORT_SYMBOL_GPL(nvme_change_ctrl_state);
+ 
++>>>>>>> c55a2fd4bb16 (nvme: move nvme_cancel_request() to common code)
  static void nvme_free_ns(struct kref *kref)
  {
  	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
diff --cc drivers/nvme/host/nvme.h
index ddd7fc3f3881,282421fec27c..000000000000
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@@ -229,6 -207,9 +229,12 @@@ static inline bool nvme_req_needs_retry
  		(jiffies - req->start_time) < req->timeout;
  }
  
++<<<<<<< HEAD
++=======
+ void nvme_cancel_request(struct request *req, void *data, bool reserved);
+ bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
+ 		enum nvme_ctrl_state new_state);
++>>>>>>> c55a2fd4bb16 (nvme: move nvme_cancel_request() to common code)
  int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
  int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
  int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl);
diff --cc drivers/nvme/host/pci.c
index 6cf19a97b3a4,37aa25046eb8..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -917,23 -919,6 +917,26 @@@ static enum blk_eh_timer_return nvme_ti
  	return BLK_EH_RESET_TIMER;
  }
  
++<<<<<<< HEAD
 +static void nvme_cancel_queue_ios(struct request *req, void *data, bool reserved)
 +{
 +	struct nvme_queue *nvmeq = data;
 +	int status;
 +
 +	if (!blk_mq_request_started(req))
 +		return;
 +
 +	dev_dbg_ratelimited(nvmeq->q_dmadev,
 +		 "Cancelling I/O %d QID %d\n", req->tag, nvmeq->qid);
 +
 +	status = NVME_SC_ABORT_REQ;
 +	if (blk_queue_dying(req->q))
 +		status |= NVME_SC_DNR;
 +	blk_mq_complete_request(req, status);
 +}
 +
++=======
++>>>>>>> c55a2fd4bb16 (nvme: move nvme_cancel_request() to common code)
  static void nvme_free_queue(struct nvme_queue *nvmeq)
  {
  	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/nvme/host/pci.c
