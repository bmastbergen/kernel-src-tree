locking/rwsem: Remove rwsem_atomic_add() and rwsem_atomic_update()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jason Low <jason.low2@hpe.com>
commit d157bd860f1c828593730dca594d0ce51956833b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/d157bd86.failed

The rwsem-xadd count has been converted to an atomic variable and the
rwsem code now directly uses atomic_long_add() and
atomic_long_add_return(), so we can remove the arch implementations of
rwsem_atomic_add() and rwsem_atomic_update().

	Signed-off-by: Jason Low <jason.low2@hpe.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Fenghua Yu <fenghua.yu@intel.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
	Cc: Jason Low <jason.low2@hp.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Hurley <peter@hurleysoftware.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Richard Henderson <rth@twiddle.net>
	Cc: Terry Rudd <terry.rudd@hpe.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Waiman Long <Waiman.Long@hpe.com>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d157bd860f1c828593730dca594d0ce51956833b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/rwsem.h
diff --cc arch/s390/include/asm/rwsem.h
index 487f9b64efb9,597e7e96b59e..000000000000
--- a/arch/s390/include/asm/rwsem.h
+++ b/arch/s390/include/asm/rwsem.h
@@@ -262,57 -207,4 +262,60 @@@ static inline void __downgrade_write(st
  		rwsem_downgrade_wake(sem);
  }
  
++<<<<<<< HEAD
 +/*
 + * implement atomic add functionality
 + */
 +static inline void rwsem_atomic_add(long delta, struct rw_semaphore *sem)
 +{
 +	signed long old, new;
 +
 +	asm volatile(
 +#ifndef CONFIG_64BIT
 +		"	l	%0,%2\n"
 +		"0:	lr	%1,%0\n"
 +		"	ar	%1,%4\n"
 +		"	cs	%0,%1,%2\n"
 +		"	jl	0b"
 +#else /* CONFIG_64BIT */
 +		"	lg	%0,%2\n"
 +		"0:	lgr	%1,%0\n"
 +		"	agr	%1,%4\n"
 +		"	csg	%0,%1,%2\n"
 +		"	jl	0b"
 +#endif /* CONFIG_64BIT */
 +		: "=&d" (old), "=&d" (new), "=Q" (sem->count)
 +		: "Q" (sem->count), "d" (delta)
 +		: "cc", "memory");
 +}
 +
 +/*
 + * implement exchange and add functionality
 + */
 +static inline long rwsem_atomic_update(long delta, struct rw_semaphore *sem)
 +{
 +	signed long old, new;
 +
 +	asm volatile(
 +#ifndef CONFIG_64BIT
 +		"	l	%0,%2\n"
 +		"0:	lr	%1,%0\n"
 +		"	ar	%1,%4\n"
 +		"	cs	%0,%1,%2\n"
 +		"	jl	0b"
 +#else /* CONFIG_64BIT */
 +		"	lg	%0,%2\n"
 +		"0:	lgr	%1,%0\n"
 +		"	agr	%1,%4\n"
 +		"	csg	%0,%1,%2\n"
 +		"	jl	0b"
 +#endif /* CONFIG_64BIT */
 +		: "=&d" (old), "=&d" (new), "=Q" (sem->count)
 +		: "Q" (sem->count), "d" (delta)
 +		: "cc", "memory");
 +	return new;
 +}
 +
++=======
++>>>>>>> d157bd860f1c (locking/rwsem: Remove rwsem_atomic_add() and rwsem_atomic_update())
  #endif /* _S390_RWSEM_H */
diff --git a/arch/alpha/include/asm/rwsem.h b/arch/alpha/include/asm/rwsem.h
index a83bbea62c67..195ad09b5142 100644
--- a/arch/alpha/include/asm/rwsem.h
+++ b/arch/alpha/include/asm/rwsem.h
@@ -177,47 +177,5 @@ static inline void __downgrade_write(struct rw_semaphore *sem)
 		rwsem_downgrade_wake(sem);
 }
 
-static inline void rwsem_atomic_add(long val, struct rw_semaphore *sem)
-{
-#ifndef	CONFIG_SMP
-	sem->count += val;
-#else
-	long temp;
-	__asm__ __volatile__(
-	"1:	ldq_l	%0,%1\n"
-	"	addq	%0,%2,%0\n"
-	"	stq_c	%0,%1\n"
-	"	beq	%0,2f\n"
-	".subsection 2\n"
-	"2:	br	1b\n"
-	".previous"
-	:"=&r" (temp), "=m" (sem->count)
-	:"Ir" (val), "m" (sem->count));
-#endif
-}
-
-static inline long rwsem_atomic_update(long val, struct rw_semaphore *sem)
-{
-#ifndef	CONFIG_SMP
-	sem->count += val;
-	return sem->count;
-#else
-	long ret, temp;
-	__asm__ __volatile__(
-	"1:	ldq_l	%0,%1\n"
-	"	addq 	%0,%3,%2\n"
-	"	addq	%0,%3,%0\n"
-	"	stq_c	%2,%1\n"
-	"	beq	%2,2f\n"
-	".subsection 2\n"
-	"2:	br	1b\n"
-	".previous"
-	:"=&r" (ret), "=m" (sem->count), "=&r" (temp)
-	:"Ir" (val), "m" (sem->count));
-
-	return ret;
-#endif
-}
-
 #endif /* __KERNEL__ */
 #endif /* _ALPHA_RWSEM_H */
diff --git a/arch/ia64/include/asm/rwsem.h b/arch/ia64/include/asm/rwsem.h
index 3027e7516d85..50c6d9e0ae84 100644
--- a/arch/ia64/include/asm/rwsem.h
+++ b/arch/ia64/include/asm/rwsem.h
@@ -135,11 +135,4 @@ __downgrade_write (struct rw_semaphore *sem)
 		rwsem_downgrade_wake(sem);
 }
 
-/*
- * Implement atomic add functionality.  These used to be "inline" functions, but GCC v3.1
- * doesn't quite optimize this stuff right and ends up with bad calls to fetchandadd.
- */
-#define rwsem_atomic_add(delta, sem)	atomic64_add(delta, (atomic64_t *)(&(sem)->count))
-#define rwsem_atomic_update(delta, sem)	atomic64_add_return(delta, (atomic64_t *)(&(sem)->count))
-
 #endif /* _ASM_IA64_RWSEM_H */
* Unmerged path arch/s390/include/asm/rwsem.h
diff --git a/arch/x86/include/asm/rwsem.h b/arch/x86/include/asm/rwsem.h
index cad82c9c2fde..9bb3d8f94ba6 100644
--- a/arch/x86/include/asm/rwsem.h
+++ b/arch/x86/include/asm/rwsem.h
@@ -203,23 +203,5 @@ static inline void __downgrade_write(struct rw_semaphore *sem)
 		     : "memory", "cc");
 }
 
-/*
- * implement atomic add functionality
- */
-static inline void rwsem_atomic_add(long delta, struct rw_semaphore *sem)
-{
-	asm volatile(LOCK_PREFIX _ASM_ADD "%1,%0"
-		     : "+m" (sem->count)
-		     : "er" (delta));
-}
-
-/*
- * implement exchange and add functionality
- */
-static inline long rwsem_atomic_update(long delta, struct rw_semaphore *sem)
-{
-	return delta + xadd(&sem->count, delta);
-}
-
 #endif /* __KERNEL__ */
 #endif /* _ASM_X86_RWSEM_H */
diff --git a/include/asm-generic/rwsem.h b/include/asm-generic/rwsem.h
index bb1e2cdeb9bf..72fcb8103ea6 100644
--- a/include/asm-generic/rwsem.h
+++ b/include/asm-generic/rwsem.h
@@ -99,14 +99,6 @@ static inline void __up_write(struct rw_semaphore *sem)
 		rwsem_wake(sem);
 }
 
-/*
- * implement atomic add functionality
- */
-static inline void rwsem_atomic_add(long delta, struct rw_semaphore *sem)
-{
-	atomic_long_add(delta, (atomic_long_t *)&sem->count);
-}
-
 /*
  * downgrade write lock to read lock
  */
@@ -120,13 +112,5 @@ static inline void __downgrade_write(struct rw_semaphore *sem)
 		rwsem_downgrade_wake(sem);
 }
 
-/*
- * implement exchange and add functionality
- */
-static inline long rwsem_atomic_update(long delta, struct rw_semaphore *sem)
-{
-	return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
-}
-
 #endif	/* __KERNEL__ */
 #endif	/* _ASM_POWERPC_RWSEM_H */
