xen-netfront: always keep the Rx ring full of requests

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author David Vrabel <david.vrabel@citrix.com>
commit 1f3c2eba1e2d866ef99bb9b10ade4096e3d7607c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1f3c2eba.failed

A full Rx ring only requires 1 MiB of memory.  This is not enough
memory that it is useful to dynamically scale the number of Rx
requests in the ring based on traffic rates, because:

a) Even the full 1 MiB is a tiny fraction of a typically modern Linux
   VM (for example, the AWS micro instance still has 1 GiB of memory).

b) Netfront would have used up to 1 MiB already even with moderate
   data rates (there was no adjustment of target based on memory
   pressure).

c) Small VMs are going to typically have one VCPU and hence only one
   queue.

Keeping the ring full of Rx requests handles bursty traffic better
than trying to converge on an optimal number of requests to keep
filled.

On a 4 core host, an iperf -P 64 -t 60 run from dom0 to a 4 VCPU guest
improved from 5.1 Gbit/s to 5.6 Gbit/s.  Gains with more bursty
traffic are expected to be higher.

	Signed-off-by: David Vrabel <david.vrabel@citrix.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1f3c2eba1e2d866ef99bb9b10ade4096e3d7607c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/xen-netfront.c
diff --cc drivers/net/xen-netfront.c
index 6c33c68dceb8,88a70f5ed594..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -71,8 -77,16 +71,10 @@@ struct netfront_cb 
  
  #define NET_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
  #define NET_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
- #define TX_MAX_TARGET min_t(int, NET_TX_RING_SIZE, 256)
+ 
+ /* Minimum number of Rx slots (includes slot for GSO metadata). */
+ #define NET_RX_SLOTS_MIN (XEN_NETIF_NR_SLOTS_MIN + 1)
  
 -/* Queue name is interface name with "-qNNN" appended */
 -#define QUEUE_NAME_SIZE (IFNAMSIZ + 6)
 -
 -/* IRQ name is queue name with "-tx" or "-rx" appended */
 -#define IRQ_NAME_SIZE (QUEUE_NAME_SIZE + 3)
 -
  struct netfront_stats {
  	u64			rx_packets;
  	u64			tx_packets;
@@@ -221,112 -239,89 +216,171 @@@ static bool xennet_can_sg(struct net_de
  
  static void rx_refill_timeout(unsigned long data)
  {
 -	struct netfront_queue *queue = (struct netfront_queue *)data;
 -	napi_schedule(&queue->napi);
 +	struct net_device *dev = (struct net_device *)data;
 +	struct netfront_info *np = netdev_priv(dev);
 +	napi_schedule(&np->napi);
  }
  
 -static int netfront_tx_slot_available(struct netfront_queue *queue)
 +static int netfront_tx_slot_available(struct netfront_info *np)
  {
++<<<<<<< HEAD
 +	return (np->tx.req_prod_pvt - np->tx.rsp_cons) <
 +		(TX_MAX_TARGET - MAX_SKB_FRAGS - 2);
++=======
+ 	return (queue->tx.req_prod_pvt - queue->tx.rsp_cons) <
+ 		(NET_TX_RING_SIZE - MAX_SKB_FRAGS - 2);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  }
  
 -static void xennet_maybe_wake_tx(struct netfront_queue *queue)
 +static void xennet_maybe_wake_tx(struct net_device *dev)
  {
 -	struct net_device *dev = queue->info->netdev;
 -	struct netdev_queue *dev_queue = netdev_get_tx_queue(dev, queue->id);
 +	struct netfront_info *np = netdev_priv(dev);
  
 -	if (unlikely(netif_tx_queue_stopped(dev_queue)) &&
 -	    netfront_tx_slot_available(queue) &&
 +	if (unlikely(netif_queue_stopped(dev)) &&
 +	    netfront_tx_slot_available(np) &&
  	    likely(netif_running(dev)))
 -		netif_tx_wake_queue(netdev_get_tx_queue(dev, queue->id));
 +		netif_wake_queue(dev);
  }
  
++<<<<<<< HEAD
 +static void xennet_alloc_rx_buffers(struct net_device *dev)
 +{
 +	unsigned short id;
 +	struct netfront_info *np = netdev_priv(dev);
 +	struct sk_buff *skb;
 +	struct page *page;
 +	int i, batch_target, notify;
 +	RING_IDX req_prod = np->rx.req_prod_pvt;
 +	grant_ref_t ref;
 +	unsigned long pfn;
 +	void *vaddr;
 +	struct xen_netif_rx_request *req;
++=======
+ 
+ static struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)
+ {
+ 	struct sk_buff *skb;
+ 	struct page *page;
+ 
+ 	skb = __netdev_alloc_skb(queue->info->netdev,
+ 				 RX_COPY_THRESHOLD + NET_IP_ALIGN,
+ 				 GFP_ATOMIC | __GFP_NOWARN);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
+ 	if (!page) {
+ 		kfree_skb(skb);
+ 		return NULL;
+ 	}
+ 	skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
+ 
+ 	/* Align ip header to a 16 bytes boundary */
+ 	skb_reserve(skb, NET_IP_ALIGN);
+ 	skb->dev = queue->info->netdev;
+ 
+ 	return skb;
+ }
+ 
+ 
+ static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
+ {
+ 	RING_IDX req_prod = queue->rx.req_prod_pvt;
+ 	int notify;
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  
 -	if (unlikely(!netif_carrier_ok(queue->info->netdev)))
 +	if (unlikely(!netif_carrier_ok(dev)))
  		return;
  
++<<<<<<< HEAD
 +	/*
 +	 * Allocate skbuffs greedily, even though we batch updates to the
 +	 * receive ring. This creates a less bursty demand on the memory
 +	 * allocator, so should reduce the chance of failed allocation requests
 +	 * both for ourself and for other kernel subsystems.
 +	 */
 +	batch_target = np->rx_target - (req_prod - np->rx.rsp_cons);
 +	for (i = skb_queue_len(&np->rx_batch); i < batch_target; i++) {
 +		skb = __netdev_alloc_skb(dev, RX_COPY_THRESHOLD + NET_IP_ALIGN,
 +					 GFP_ATOMIC | __GFP_NOWARN);
 +		if (unlikely(!skb))
 +			goto no_skb;
 +
 +		/* Align ip header to a 16 bytes boundary */
 +		skb_reserve(skb, NET_IP_ALIGN);
 +
 +		page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
 +		if (!page) {
 +			kfree_skb(skb);
 +no_skb:
 +			/* Could not allocate any skbuffs. Try again later. */
 +			mod_timer(&np->rx_refill_timer,
 +				  jiffies + (HZ/10));
 +
 +			/* Any skbuffs queued for refill? Force them out. */
 +			if (i != 0)
 +				goto refill;
 +			break;
 +		}
 +
 +		skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
 +		__skb_queue_tail(&np->rx_batch, skb);
 +	}
 +
 +	/* Is the batch large enough to be worthwhile? */
 +	if (i < (np->rx_target/2)) {
 +		if (req_prod > np->rx.sring->req_prod)
 +			goto push;
 +		return;
 +	}
 +
 +	/* Adjust our fill target if we risked running out of buffers. */
 +	if (((req_prod - np->rx.sring->rsp_prod) < (np->rx_target / 4)) &&
 +	    ((np->rx_target *= 2) > np->rx_max_target))
 +		np->rx_target = np->rx_max_target;
 +
 + refill:
 +	for (i = 0; ; i++) {
 +		skb = __skb_dequeue(&np->rx_batch);
 +		if (skb == NULL)
 +			break;
 +
 +		skb->dev = dev;
 +
 +		id = xennet_rxidx(req_prod + i);
++=======
+ 	for (req_prod = queue->rx.req_prod_pvt;
+ 	     req_prod - queue->rx.rsp_cons < NET_RX_RING_SIZE;
+ 	     req_prod++) {
+ 		struct sk_buff *skb;
+ 		unsigned short id;
+ 		grant_ref_t ref;
+ 		unsigned long pfn;
+ 		struct xen_netif_rx_request *req;
+ 
+ 		skb = xennet_alloc_one_rx_buffer(queue);
+ 		if (!skb)
+ 			break;
+ 
+ 		id = xennet_rxidx(req_prod);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  
 -		BUG_ON(queue->rx_skbs[id]);
 -		queue->rx_skbs[id] = skb;
 +		BUG_ON(np->rx_skbs[id]);
 +		np->rx_skbs[id] = skb;
  
 -		ref = gnttab_claim_grant_reference(&queue->gref_rx_head);
 +		ref = gnttab_claim_grant_reference(&np->gref_rx_head);
  		BUG_ON((signed short)ref < 0);
 -		queue->grant_rx_ref[id] = ref;
 +		np->grant_rx_ref[id] = ref;
  
  		pfn = page_to_pfn(skb_frag_page(&skb_shinfo(skb)->frags[0]));
- 		vaddr = page_address(skb_frag_page(&skb_shinfo(skb)->frags[0]));
  
++<<<<<<< HEAD
 +		req = RING_GET_REQUEST(&np->rx, req_prod + i);
++=======
+ 		req = RING_GET_REQUEST(&queue->rx, req_prod);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  		gnttab_grant_foreign_access_ref(ref,
 -						queue->info->xbdev->otherend_id,
 +						np->xbdev->otherend_id,
  						pfn_to_mfn(pfn),
  						0);
  
@@@ -334,14 -329,19 +388,26 @@@
  		req->gref = ref;
  	}
  
+ 	queue->rx.req_prod_pvt = req_prod;
+ 
+ 	/* Not enough requests? Try again later. */
+ 	if (req_prod - queue->rx.rsp_cons < NET_RX_SLOTS_MIN) {
+ 		mod_timer(&queue->rx_refill_timer, jiffies + (HZ/10));
+ 		return;
+ 	}
+ 
  	wmb();		/* barrier so backend seens requests */
  
++<<<<<<< HEAD
 +	/* Above is a suitable barrier to ensure backend will see requests. */
 +	np->rx.req_prod_pvt = req_prod + i;
 + push:
 +	RING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&np->rx, notify);
++=======
+ 	RING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->rx, notify);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  	if (notify)
 -		notify_remote_via_irq(queue->rx_irq);
 +		notify_remote_via_irq(np->rx_irq);
  }
  
  static int xennet_open(struct net_device *dev)
@@@ -1041,16 -1045,9 +1107,20 @@@ err
  
  	__skb_queue_purge(&errq);
  
 -	work_done -= handle_incoming_queue(queue, &rxq);
 +	work_done -= handle_incoming_queue(dev, &rxq);
  
++<<<<<<< HEAD
 +	/* If we get a callback with very few responses, reduce fill target. */
 +	/* NB. Note exponential increase, linear decrease. */
 +	if (((np->rx.req_prod_pvt - np->rx.sring->rsp_prod) >
 +	     ((3*np->rx_target) / 4)) &&
 +	    (--np->rx_target < np->rx_min_target))
 +		np->rx_target = np->rx_min_target;
 +
 +	xennet_alloc_rx_buffers(dev);
++=======
+ 	xennet_alloc_rx_buffers(queue);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  
  	if (work_done < budget) {
  		int more_to_do = 0;
@@@ -1630,6 -1601,209 +1700,212 @@@ fail
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ /* Queue-specific initialisation
+  * This used to be done in xennet_create_dev() but must now
+  * be run per-queue.
+  */
+ static int xennet_init_queue(struct netfront_queue *queue)
+ {
+ 	unsigned short i;
+ 	int err = 0;
+ 
+ 	spin_lock_init(&queue->tx_lock);
+ 	spin_lock_init(&queue->rx_lock);
+ 
+ 	init_timer(&queue->rx_refill_timer);
+ 	queue->rx_refill_timer.data = (unsigned long)queue;
+ 	queue->rx_refill_timer.function = rx_refill_timeout;
+ 
+ 	snprintf(queue->name, sizeof(queue->name), "%s-q%u",
+ 		 queue->info->netdev->name, queue->id);
+ 
+ 	/* Initialise tx_skbs as a free chain containing every entry. */
+ 	queue->tx_skb_freelist = 0;
+ 	for (i = 0; i < NET_TX_RING_SIZE; i++) {
+ 		skb_entry_set_link(&queue->tx_skbs[i], i+1);
+ 		queue->grant_tx_ref[i] = GRANT_INVALID_REF;
+ 		queue->grant_tx_page[i] = NULL;
+ 	}
+ 
+ 	/* Clear out rx_skbs */
+ 	for (i = 0; i < NET_RX_RING_SIZE; i++) {
+ 		queue->rx_skbs[i] = NULL;
+ 		queue->grant_rx_ref[i] = GRANT_INVALID_REF;
+ 	}
+ 
+ 	/* A grant for every tx ring slot */
+ 	if (gnttab_alloc_grant_references(NET_TX_RING_SIZE,
+ 					  &queue->gref_tx_head) < 0) {
+ 		pr_alert("can't alloc tx grant refs\n");
+ 		err = -ENOMEM;
+ 		goto exit;
+ 	}
+ 
+ 	/* A grant for every rx ring slot */
+ 	if (gnttab_alloc_grant_references(NET_RX_RING_SIZE,
+ 					  &queue->gref_rx_head) < 0) {
+ 		pr_alert("can't alloc rx grant refs\n");
+ 		err = -ENOMEM;
+ 		goto exit_free_tx;
+ 	}
+ 
+ 	return 0;
+ 
+  exit_free_tx:
+ 	gnttab_free_grant_references(queue->gref_tx_head);
+  exit:
+ 	return err;
+ }
+ 
+ static int write_queue_xenstore_keys(struct netfront_queue *queue,
+ 			   struct xenbus_transaction *xbt, int write_hierarchical)
+ {
+ 	/* Write the queue-specific keys into XenStore in the traditional
+ 	 * way for a single queue, or in a queue subkeys for multiple
+ 	 * queues.
+ 	 */
+ 	struct xenbus_device *dev = queue->info->xbdev;
+ 	int err;
+ 	const char *message;
+ 	char *path;
+ 	size_t pathsize;
+ 
+ 	/* Choose the correct place to write the keys */
+ 	if (write_hierarchical) {
+ 		pathsize = strlen(dev->nodename) + 10;
+ 		path = kzalloc(pathsize, GFP_KERNEL);
+ 		if (!path) {
+ 			err = -ENOMEM;
+ 			message = "out of memory while writing ring references";
+ 			goto error;
+ 		}
+ 		snprintf(path, pathsize, "%s/queue-%u",
+ 				dev->nodename, queue->id);
+ 	} else {
+ 		path = (char *)dev->nodename;
+ 	}
+ 
+ 	/* Write ring references */
+ 	err = xenbus_printf(*xbt, path, "tx-ring-ref", "%u",
+ 			queue->tx_ring_ref);
+ 	if (err) {
+ 		message = "writing tx-ring-ref";
+ 		goto error;
+ 	}
+ 
+ 	err = xenbus_printf(*xbt, path, "rx-ring-ref", "%u",
+ 			queue->rx_ring_ref);
+ 	if (err) {
+ 		message = "writing rx-ring-ref";
+ 		goto error;
+ 	}
+ 
+ 	/* Write event channels; taking into account both shared
+ 	 * and split event channel scenarios.
+ 	 */
+ 	if (queue->tx_evtchn == queue->rx_evtchn) {
+ 		/* Shared event channel */
+ 		err = xenbus_printf(*xbt, path,
+ 				"event-channel", "%u", queue->tx_evtchn);
+ 		if (err) {
+ 			message = "writing event-channel";
+ 			goto error;
+ 		}
+ 	} else {
+ 		/* Split event channels */
+ 		err = xenbus_printf(*xbt, path,
+ 				"event-channel-tx", "%u", queue->tx_evtchn);
+ 		if (err) {
+ 			message = "writing event-channel-tx";
+ 			goto error;
+ 		}
+ 
+ 		err = xenbus_printf(*xbt, path,
+ 				"event-channel-rx", "%u", queue->rx_evtchn);
+ 		if (err) {
+ 			message = "writing event-channel-rx";
+ 			goto error;
+ 		}
+ 	}
+ 
+ 	if (write_hierarchical)
+ 		kfree(path);
+ 	return 0;
+ 
+ error:
+ 	if (write_hierarchical)
+ 		kfree(path);
+ 	xenbus_dev_fatal(dev, err, "%s", message);
+ 	return err;
+ }
+ 
+ static void xennet_destroy_queues(struct netfront_info *info)
+ {
+ 	unsigned int i;
+ 
+ 	rtnl_lock();
+ 
+ 	for (i = 0; i < info->netdev->real_num_tx_queues; i++) {
+ 		struct netfront_queue *queue = &info->queues[i];
+ 
+ 		if (netif_running(info->netdev))
+ 			napi_disable(&queue->napi);
+ 		netif_napi_del(&queue->napi);
+ 	}
+ 
+ 	rtnl_unlock();
+ 
+ 	kfree(info->queues);
+ 	info->queues = NULL;
+ }
+ 
+ static int xennet_create_queues(struct netfront_info *info,
+ 				unsigned int num_queues)
+ {
+ 	unsigned int i;
+ 	int ret;
+ 
+ 	info->queues = kcalloc(num_queues, sizeof(struct netfront_queue),
+ 			       GFP_KERNEL);
+ 	if (!info->queues)
+ 		return -ENOMEM;
+ 
+ 	rtnl_lock();
+ 
+ 	for (i = 0; i < num_queues; i++) {
+ 		struct netfront_queue *queue = &info->queues[i];
+ 
+ 		queue->id = i;
+ 		queue->info = info;
+ 
+ 		ret = xennet_init_queue(queue);
+ 		if (ret < 0) {
+ 			dev_warn(&info->netdev->dev,
+ 				 "only created %d queues\n", i);
+ 			num_queues = i;
+ 			break;
+ 		}
+ 
+ 		netif_napi_add(queue->info->netdev, &queue->napi,
+ 			       xennet_poll, 64);
+ 		if (netif_running(info->netdev))
+ 			napi_enable(&queue->napi);
+ 	}
+ 
+ 	netif_set_real_num_tx_queues(info->netdev, num_queues);
+ 
+ 	rtnl_unlock();
+ 
+ 	if (num_queues == 0) {
+ 		dev_err(&info->netdev->dev, "no queues\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  /* Common code used when first setting up, and when resuming. */
  static int talk_to_netback(struct xenbus_device *dev,
  			   struct netfront_info *info)
@@@ -1901,21 -2111,16 +2177,28 @@@ static const struct ethtool_ops xennet_
  };
  
  #ifdef CONFIG_SYSFS
- static ssize_t show_rxbuf_min(struct device *dev,
- 			      struct device_attribute *attr, char *buf)
+ static ssize_t show_rxbuf(struct device *dev,
+ 			  struct device_attribute *attr, char *buf)
  {
++<<<<<<< HEAD
 +	struct net_device *netdev = to_net_dev(dev);
 +	struct netfront_info *info = netdev_priv(netdev);
 +
 +	return sprintf(buf, "%u\n", info->rx_min_target);
++=======
+ 	return sprintf(buf, "%lu\n", NET_RX_RING_SIZE);
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  }
  
- static ssize_t store_rxbuf_min(struct device *dev,
- 			       struct device_attribute *attr,
- 			       const char *buf, size_t len)
+ static ssize_t store_rxbuf(struct device *dev,
+ 			   struct device_attribute *attr,
+ 			   const char *buf, size_t len)
  {
++<<<<<<< HEAD
 +	struct net_device *netdev = to_net_dev(dev);
 +	struct netfront_info *np = netdev_priv(netdev);
++=======
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  	char *endp;
  	unsigned long target;
  
@@@ -1926,80 -2131,15 +2209,83 @@@
  	if (endp == buf)
  		return -EBADMSG;
  
- 	if (target < RX_MIN_TARGET)
- 		target = RX_MIN_TARGET;
- 	if (target > RX_MAX_TARGET)
- 		target = RX_MAX_TARGET;
+ 	/* rxbuf_min and rxbuf_max are no longer configurable. */
  
++<<<<<<< HEAD
 +	spin_lock_bh(&np->rx_lock);
 +	if (target > np->rx_max_target)
 +		np->rx_max_target = target;
 +	np->rx_min_target = target;
 +	if (target > np->rx_target)
 +		np->rx_target = target;
 +
 +	xennet_alloc_rx_buffers(netdev);
 +
 +	spin_unlock_bh(&np->rx_lock);
  	return len;
  }
  
 +static ssize_t show_rxbuf_max(struct device *dev,
 +			      struct device_attribute *attr, char *buf)
 +{
 +	struct net_device *netdev = to_net_dev(dev);
 +	struct netfront_info *info = netdev_priv(netdev);
 +
 +	return sprintf(buf, "%u\n", info->rx_max_target);
 +}
 +
 +static ssize_t store_rxbuf_max(struct device *dev,
 +			       struct device_attribute *attr,
 +			       const char *buf, size_t len)
 +{
 +	struct net_device *netdev = to_net_dev(dev);
 +	struct netfront_info *np = netdev_priv(netdev);
 +	char *endp;
 +	unsigned long target;
 +
 +	if (!capable(CAP_NET_ADMIN))
 +		return -EPERM;
 +
 +	target = simple_strtoul(buf, &endp, 0);
 +	if (endp == buf)
 +		return -EBADMSG;
 +
 +	if (target < RX_MIN_TARGET)
 +		target = RX_MIN_TARGET;
 +	if (target > RX_MAX_TARGET)
 +		target = RX_MAX_TARGET;
 +
 +	spin_lock_bh(&np->rx_lock);
 +	if (target < np->rx_min_target)
 +		np->rx_min_target = target;
 +	np->rx_max_target = target;
 +	if (target < np->rx_target)
 +		np->rx_target = target;
 +
 +	xennet_alloc_rx_buffers(netdev);
 +
 +	spin_unlock_bh(&np->rx_lock);
 +	return len;
 +}
 +
 +static ssize_t show_rxbuf_cur(struct device *dev,
 +			      struct device_attribute *attr, char *buf)
 +{
 +	struct net_device *netdev = to_net_dev(dev);
 +	struct netfront_info *info = netdev_priv(netdev);
 +
 +	return sprintf(buf, "%u\n", info->rx_target);
 +}
 +
++=======
++	return len;
++}
++
++>>>>>>> 1f3c2eba1e2d (xen-netfront: always keep the Rx ring full of requests)
  static struct device_attribute xennet_attrs[] = {
- 	__ATTR(rxbuf_min, S_IRUGO|S_IWUSR, show_rxbuf_min, store_rxbuf_min),
- 	__ATTR(rxbuf_max, S_IRUGO|S_IWUSR, show_rxbuf_max, store_rxbuf_max),
- 	__ATTR(rxbuf_cur, S_IRUGO, show_rxbuf_cur, NULL),
+ 	__ATTR(rxbuf_min, S_IRUGO|S_IWUSR, show_rxbuf, store_rxbuf),
+ 	__ATTR(rxbuf_max, S_IRUGO|S_IWUSR, show_rxbuf, store_rxbuf),
+ 	__ATTR(rxbuf_cur, S_IRUGO, show_rxbuf, NULL),
  };
  
  static int xennet_sysfs_addif(struct net_device *netdev)
* Unmerged path drivers/net/xen-netfront.c
