userfaultfd: hugetlbfs: gup: support VM_FAULT_RETRY

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit 87ffc118b54dcd4cc642723603d944673248152f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/87ffc118.failed

Add support for VM_FAULT_RETRY to follow_hugetlb_page() so that
get_user_pages_unlocked/locked and "nonblocking/FOLL_NOWAIT" features
will work on hugetlbfs.

This is required for fully functional userfaultfd non-present support on
hugetlbfs.

Link: http://lkml.kernel.org/r/20161216144821.5183-25-aarcange@redhat.com
	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
	Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
	Cc: Michael Rapoport <RAPOPORT@il.ibm.com>
	Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 87ffc118b54dcd4cc642723603d944673248152f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index a09210b61df2,40abe4c90383..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -366,189 -548,112 +366,197 @@@ long __get_user_pages(struct task_struc
  	if (!(gup_flags & FOLL_FORCE))
  		gup_flags |= FOLL_NUMA;
  
 -	do {
 -		struct page *page;
 -		unsigned int foll_flags = gup_flags;
 -		unsigned int page_increm;
 -
 -		/* first iteration or cross vma bound */
 -		if (!vma || start >= vma->vm_end) {
 -			vma = find_extend_vma(mm, start);
 -			if (!vma && in_gate_area(mm, start)) {
 -				int ret;
 -				ret = get_gate_page(mm, start & PAGE_MASK,
 -						gup_flags, &vma,
 -						pages ? &pages[i] : NULL);
 -				if (ret)
 -					return i ? : ret;
 -				page_mask = 0;
 -				goto next_page;
 -			}
 +	i = 0;
  
 -			if (!vma || check_vma_flags(vma, gup_flags))
 +	do {
 +		struct vm_area_struct *vma;
 +
 +		vma = find_extend_vma(mm, start);
 +		if (!vma && in_gate_area(mm, start)) {
 +			unsigned long pg = start & PAGE_MASK;
 +			pgd_t *pgd;
 +			pud_t *pud;
 +			pmd_t *pmd;
 +			pte_t *pte;
 +
 +			/* user gate pages are read-only */
 +			if (gup_flags & FOLL_WRITE)
 +				return i ? : -EFAULT;
 +			if (pg > TASK_SIZE)
 +				pgd = pgd_offset_k(pg);
 +			else
 +				pgd = pgd_offset_gate(mm, pg);
 +			BUG_ON(pgd_none(*pgd));
 +			pud = pud_offset(pgd, pg);
 +			BUG_ON(pud_none(*pud));
 +			pmd = pmd_offset(pud, pg);
 +			if (pmd_none(*pmd))
  				return i ? : -EFAULT;
 +			VM_BUG_ON(pmd_trans_huge(*pmd));
 +			pte = pte_offset_map(pmd, pg);
 +			if (pte_none(*pte)) {
 +				pte_unmap(pte);
 +				return i ? : -EFAULT;
++<<<<<<< HEAD
++=======
+ 			if (is_vm_hugetlb_page(vma)) {
+ 				i = follow_hugetlb_page(mm, vma, pages, vmas,
+ 						&start, &nr_pages, i,
+ 						gup_flags, nonblocking);
+ 				continue;
++>>>>>>> 87ffc118b54d (userfaultfd: hugetlbfs: gup: support VM_FAULT_RETRY)
  			}
 -		}
 -retry:
 -		/*
 -		 * If we have a pending SIGKILL, don't keep faulting pages and
 -		 * potentially allocating memory.
 -		 */
 -		if (unlikely(fatal_signal_pending(current)))
 -			return i ? i : -ERESTARTSYS;
 -		cond_resched();
 -		page = follow_page_mask(vma, start, foll_flags, &page_mask);
 -		if (!page) {
 -			int ret;
 -			ret = faultin_page(tsk, vma, start, &foll_flags,
 -					nonblocking);
 -			switch (ret) {
 -			case 0:
 -				goto retry;
 -			case -EFAULT:
 -			case -ENOMEM:
 -			case -EHWPOISON:
 -				return i ? i : ret;
 -			case -EBUSY:
 -				return i;
 -			case -ENOENT:
 -				goto next_page;
 +			vma = get_gate_vma(mm);
 +			if (pages) {
 +				struct page *page;
 +
 +				page = vm_normal_page(vma, start, *pte);
 +				if (!page) {
 +					if (!(gup_flags & FOLL_DUMP) &&
 +					     is_zero_pfn(pte_pfn(*pte)))
 +						page = pte_page(*pte);
 +					else {
 +						pte_unmap(pte);
 +						return i ? : -EFAULT;
 +					}
 +				}
 +				pages[i] = page;
 +				get_page(page);
  			}
 -			BUG();
 -		} else if (PTR_ERR(page) == -EEXIST) {
 -			/*
 -			 * Proper page table entry exists, but no corresponding
 -			 * struct page.
 -			 */
 -			goto next_page;
 -		} else if (IS_ERR(page)) {
 -			return i ? i : PTR_ERR(page);
 -		}
 -		if (pages) {
 -			pages[i] = page;
 -			flush_anon_page(vma, page, start);
 -			flush_dcache_page(page);
 +			pte_unmap(pte);
  			page_mask = 0;
 +			goto next_page;
  		}
 -next_page:
 -		if (vmas) {
 -			vmas[i] = vma;
 -			page_mask = 0;
 +
 +		if (!vma ||
 +		    (vma->vm_flags & (VM_IO | VM_PFNMAP)) ||
 +		    !(vm_flags & vma->vm_flags))
 +			return i ? : -EFAULT;
 +
 +		if (is_vm_hugetlb_page(vma)) {
 +			i = follow_hugetlb_page(mm, vma, pages, vmas,
 +					&start, &nr_pages, i, gup_flags);
 +			continue;
  		}
 -		page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
 -		if (page_increm > nr_pages)
 -			page_increm = nr_pages;
 -		i += page_increm;
 -		start += page_increm * PAGE_SIZE;
 -		nr_pages -= page_increm;
 -	} while (nr_pages);
 -	return i;
 -}
  
 -static bool vma_permits_fault(struct vm_area_struct *vma,
 -			      unsigned int fault_flags)
 -{
 -	bool write   = !!(fault_flags & FAULT_FLAG_WRITE);
 -	bool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);
 -	vm_flags_t vm_flags = write ? VM_WRITE : VM_READ;
 +		do {
 +			struct page *page;
 +			unsigned int foll_flags = gup_flags;
 +			unsigned int page_increm;
  
 -	if (!(vm_flags & vma->vm_flags))
 -		return false;
 +			/*
 +			 * If we have a pending SIGKILL, don't keep faulting
 +			 * pages and potentially allocating memory.
 +			 */
 +			if (unlikely(fatal_signal_pending(current)))
 +				return i ? i : -ERESTARTSYS;
  
 -	/*
 -	 * The architecture might have a hardware protection
 -	 * mechanism other than read/write that can deny access.
 -	 *
 -	 * gup always represents data access, not instruction
 -	 * fetches, so execute=false here:
 -	 */
 -	if (!arch_vma_access_permitted(vma, write, false, foreign))
 -		return false;
 +			cond_resched();
 +			while (!(page = follow_page_mask(vma, start,
 +						foll_flags, &page_mask))) {
 +				int ret;
 +				unsigned int fault_flags = 0;
 +
 +				/* For mlock, just skip the stack guard page. */
 +				if (foll_flags & FOLL_MLOCK) {
 +					if (stack_guard_page(vma, start))
 +						goto next_page;
 +				}
 +				if (foll_flags & FOLL_WRITE)
 +					fault_flags |= FAULT_FLAG_WRITE;
 +				if (nonblocking)
 +					fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 +				if (foll_flags & FOLL_NOWAIT)
 +					fault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);
 +				if (foll_flags & FOLL_TRIED) {
 +					WARN_ON_ONCE(fault_flags &
 +						     FAULT_FLAG_ALLOW_RETRY);
 +					fault_flags |= FAULT_FLAG_TRIED;
 +				}
 +
 +				ret = handle_mm_fault(mm, vma, start,
 +							fault_flags);
 +
 +				if (ret & VM_FAULT_ERROR) {
 +					if (ret & VM_FAULT_OOM)
 +						return i ? i : -ENOMEM;
 +					if (ret & (VM_FAULT_HWPOISON |
 +						   VM_FAULT_HWPOISON_LARGE)) {
 +						if (i)
 +							return i;
 +						else if (gup_flags & FOLL_HWPOISON)
 +							return -EHWPOISON;
 +						else
 +							return -EFAULT;
 +					}
 +					if (ret & VM_FAULT_SIGBUS)
 +						return i ? i : -EFAULT;
 +					BUG();
 +				}
 +
 +				if (tsk) {
 +					if (ret & VM_FAULT_MAJOR)
 +						tsk->maj_flt++;
 +					else
 +						tsk->min_flt++;
 +				}
 +
 +				if (ret & VM_FAULT_RETRY) {
 +					if (nonblocking)
 +						*nonblocking = 0;
 +					return i;
 +				}
 +
 +				/*
 +				 * The VM_FAULT_WRITE bit tells us that
 +				 * do_wp_page has broken COW when necessary,
 +				 * even if maybe_mkwrite decided not to set
 +				 * pte_write. We can thus safely do subsequent
 +				 * page lookups as if they were reads. But only
 +				 * do so when looping for pte_write is futile:
 +				 * in some cases userspace may also be wanting
 +				 * to write to the gotten user page, which a
 +				 * read fault here might prevent (a readonly
 +				 * page might get reCOWed by userspace write).
 +				 */
 +				if ((ret & VM_FAULT_WRITE) &&
 +				    !(vma->vm_flags & VM_WRITE))
 +					foll_flags |= FOLL_COW;
 +
 +				cond_resched();
 +			}
 +			if (PTR_ERR(page) == -EEXIST) {
 +				/*
 +				 * Proper page table entry exists, but
 +				 * no corresponding struct page.
 +				 */
 +				goto next_page;
 +			} else if (IS_ERR(page)) {
 +				return i ? i : PTR_ERR(page);
 +			}
 +			if (pages) {
 +				pages[i] = page;
  
 -	return true;
 +				flush_anon_page(vma, page, start);
 +				flush_dcache_page(page);
 +				page_mask = 0;
 +			}
 +next_page:
 +			if (vmas) {
 +				vmas[i] = vma;
 +				page_mask = 0;
 +			}
 +			page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
 +			if (page_increm > nr_pages)
 +				page_increm = nr_pages;
 +			i += page_increm;
 +			start += page_increm * PAGE_SIZE;
 +			nr_pages -= page_increm;
 +		} while (nr_pages && start < vma->vm_end);
 +	} while (nr_pages);
 +	return i;
  }
 +EXPORT_SYMBOL(__get_user_pages);
  
  /*
   * fixup_user_fault() - manually resolve a user page fault
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1eaa9f59de86..b52177e5537f 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -65,7 +65,8 @@ int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 struct page **, struct vm_area_struct **,
-			 unsigned long *, unsigned long *, long, unsigned int);
+			 unsigned long *, unsigned long *, long, unsigned int,
+			 int *);
 void unmap_hugepage_range(struct vm_area_struct *,
 			  unsigned long, unsigned long, struct page *);
 void __unmap_hugepage_range_final(struct mmu_gather *tlb,
@@ -143,7 +144,7 @@ static inline unsigned long hugetlb_total_pages(void)
 	return 0;
 }
 
-#define follow_hugetlb_page(m,v,p,vs,a,b,i,w)	({ BUG(); 0; })
+#define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n)	({ BUG(); 0; })
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
* Unmerged path mm/gup.c
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 3fedf80c0dc1..ab391f3060fa 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3679,7 +3679,7 @@ out_release_unlock:
 long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			 struct page **pages, struct vm_area_struct **vmas,
 			 unsigned long *position, unsigned long *nr_pages,
-			 long i, unsigned int flags)
+			 long i, unsigned int flags, int *nonblocking)
 {
 	unsigned long pfn_offset;
 	unsigned long vaddr = *position;
@@ -3742,16 +3742,43 @@ long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    ((flags & FOLL_WRITE) &&
 		      !huge_pte_write(huge_ptep_get(pte)))) {
 			int ret;
+			unsigned int fault_flags = 0;
 
 			if (pte)
 				spin_unlock(ptl);
-			ret = hugetlb_fault(mm, vma, vaddr,
-				(flags & FOLL_WRITE) ? FAULT_FLAG_WRITE : 0);
-			if (!(ret & VM_FAULT_ERROR))
-				continue;
-
-			remainder = 0;
-			break;
+			if (flags & FOLL_WRITE)
+				fault_flags |= FAULT_FLAG_WRITE;
+			if (nonblocking)
+				fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+			if (flags & FOLL_NOWAIT)
+				fault_flags |= FAULT_FLAG_ALLOW_RETRY |
+					FAULT_FLAG_RETRY_NOWAIT;
+			if (flags & FOLL_TRIED) {
+				VM_WARN_ON_ONCE(fault_flags &
+						FAULT_FLAG_ALLOW_RETRY);
+				fault_flags |= FAULT_FLAG_TRIED;
+			}
+			ret = hugetlb_fault(mm, vma, vaddr, fault_flags);
+			if (ret & VM_FAULT_ERROR) {
+				remainder = 0;
+				break;
+			}
+			if (ret & VM_FAULT_RETRY) {
+				if (nonblocking)
+					*nonblocking = 0;
+				*nr_pages = 0;
+				/*
+				 * VM_FAULT_RETRY must not return an
+				 * error, it will return zero
+				 * instead.
+				 *
+				 * No need to update "position" as the
+				 * caller will not check it after
+				 * *nr_pages is set to 0.
+				 */
+				return i;
+			}
+			continue;
 		}
 
 		pfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;
@@ -3780,6 +3807,11 @@ same_page:
 		spin_unlock(ptl);
 	}
 	*nr_pages = remainder;
+	/*
+	 * setting position is actually required only if remainder is
+	 * not zero but it's faster not to add a "if (remainder)"
+	 * branch.
+	 */
 	*position = vaddr;
 
 	return i ? i : -EFAULT;
