blk-mq: add blk_mq_alloc_request_hctx

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Ming Lin <ming.l@ssi.samsung.com>
commit 1f5bd336b9150560458b03460cbcfcfbcf8995b1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1f5bd336.failed

For some protocols like NVMe over Fabrics we need to be able to send
initialization commands to a specific queue.

Based on an earlier patch from Christoph Hellwig <hch@lst.de>.

	Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
[hch: disallow sleeping allocation, req_op fixes]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 1f5bd336b9150560458b03460cbcfcfbcf8995b1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blk-mq.h
diff --cc include/linux/blk-mq.h
index 0e7b0244ac77,cbfd8ca5f13e..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -221,8 -188,16 +221,14 @@@ void blk_mq_insert_request(struct reque
  void blk_mq_free_request(struct request *rq);
  void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
  bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 -
 -enum {
 -	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
 -	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
 -};
 -
  struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
++<<<<<<< HEAD
 +		gfp_t gfp, bool reserved);
++=======
+ 		unsigned int flags);
+ struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int op,
+ 		unsigned int flags, unsigned int hctx_idx);
++>>>>>>> 1f5bd336b915 (blk-mq: add blk_mq_alloc_request_hctx)
  struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
  struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
  
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 3caf05b09404..b7992fe42f65 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -268,6 +268,45 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int rw,
+		unsigned int flags, unsigned int hctx_idx)
+{
+	struct blk_mq_hw_ctx *hctx;
+	struct blk_mq_ctx *ctx;
+	struct request *rq;
+	struct blk_mq_alloc_data alloc_data;
+	int ret;
+
+	/*
+	 * If the tag allocator sleeps we could get an allocation for a
+	 * different hardware context.  No need to complicate the low level
+	 * allocator for this for the rare use case of a command tied to
+	 * a specific queue.
+	 */
+	if (WARN_ON_ONCE(!(flags & BLK_MQ_REQ_NOWAIT)))
+		return ERR_PTR(-EINVAL);
+
+	if (hctx_idx >= q->nr_hw_queues)
+		return ERR_PTR(-EIO);
+
+	ret = blk_queue_enter(q, true);
+	if (ret)
+		return ERR_PTR(ret);
+
+	hctx = q->queue_hw_ctx[hctx_idx];
+	ctx = __blk_mq_get_ctx(q, cpumask_first(hctx->cpumask));
+
+	blk_mq_set_alloc_data(&alloc_data, q, flags, ctx, hctx);
+	rq = __blk_mq_alloc_request(&alloc_data, rw, 0);
+	if (!rq) {
+		blk_queue_exit(q);
+		return ERR_PTR(-EWOULDBLOCK);
+	}
+
+	return rq;
+}
+EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
+
 static void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx, struct request *rq)
 {
* Unmerged path include/linux/blk-mq.h
