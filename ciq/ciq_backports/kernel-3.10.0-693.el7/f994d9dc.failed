fabrics: define admin sqsize min default, per spec

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jay Freyensee <james_p_freyensee@linux.intel.com>
commit f994d9dc28bc27353acde2caaf718222d92a3e24
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f994d9dc.failed

Upon admin queue connect(), the rdma qp was being
set based on NVMF_AQ_DEPTH.  However, the fabrics layer was
using the sqsize field value set for I/O queues for the admin
queue, which threw the nvme layer and rdma layer off-whack:

root@fedora23-fabrics-host1 nvmf]# dmesg
[ 3507.798642] nvme_fabrics: nvmf_connect_admin_queue():admin sqsize
being sent is: 128
[ 3507.798858] nvme nvme0: creating 16 I/O queues.
[ 3507.896407] nvme nvme0: new ctrl: NQN "nullside-nqn", addr
192.168.1.3:4420

Thus, to have a different admin queue value, we use
NVMF_AQ_DEPTH for connect() and RDMA private data
as the minimum depth specified in the NVMe-over-Fabrics 1.0 spec
(and in that RDMA private data we treat hrqsize as 1's-based
value, per current understanding of the fabrics spec).

	Reported-by: Daniel Verkamp <daniel.verkamp@intel.com>
	Signed-off-by: Jay Freyensee <james_p_freyensee@linux.intel.com>
	Reviewed-by: Daniel Verkamp <daniel.verkamp@intel.com>
	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
(cherry picked from commit f994d9dc28bc27353acde2caaf718222d92a3e24)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/fabrics.c
#	drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/host/fabrics.c
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/host/fabrics.c
* Unmerged path drivers/nvme/host/rdma.c
