kvm/x86: per-vcpu apicv deactivation support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Andrey Smetanin <asmetanin@virtuozzo.com>
commit d62caabb41f33d96333f9ef15e09cd26e1c12760
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/d62caabb.failed

The decision on whether to use hardware APIC virtualization used to be
taken globally, based on the availability of the feature in the CPU
and the value of a module parameter.

However, under certain circumstances we want to control it on per-vcpu
basis.  In particular, when the userspace activates HyperV synthetic
interrupt controller (SynIC), APICv has to be disabled as it's
incompatible with SynIC auto-EOI behavior.

To achieve that, introduce 'apicv_active' flag on struct
kvm_vcpu_arch, and kvm_vcpu_deactivate_apicv() function to turn APICv
off.  The flag is initialized based on the module parameter and CPU
capability, and consulted whenever an APICv-specific action is
performed.

	Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
	Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
	Signed-off-by: Denis V. Lunev <den@openvz.org>
CC: Gleb Natapov <gleb@kernel.org>
CC: Paolo Bonzini <pbonzini@redhat.com>
CC: Roman Kagan <rkagan@virtuozzo.com>
CC: Denis V. Lunev <den@openvz.org>
CC: qemu-devel@nongnu.org
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d62caabb41f33d96333f9ef15e09cd26e1c12760)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index e9b71b8ce24b,bac0d540f49c..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -382,7 -400,8 +382,12 @@@ struct kvm_vcpu_arch 
  	u64 efer;
  	u64 apic_base;
  	struct kvm_lapic *apic;    /* kernel irqchip context */
++<<<<<<< HEAD
 +	u64 eoi_exit_bitmap[4];
++=======
+ 	bool apicv_active;
+ 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
++>>>>>>> d62caabb41f3 (kvm/x86: per-vcpu apicv deactivation support)
  	unsigned long apic_attention;
  	int32_t apic_arb_prio;
  	int mp_state;
@@@ -815,10 -832,11 +820,11 @@@ struct kvm_x86_ops 
  	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
  	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
  	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
- 	int (*cpu_uses_apicv)(struct kvm_vcpu *vcpu);
+ 	bool (*get_enable_apicv)(void);
+ 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
  	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
  	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
 -	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 +	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu);
  	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
  	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
  	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
diff --cc arch/x86/kvm/vmx.c
index cebaaac0b163,1a8bfaab89c7..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -858,7 -861,8 +859,12 @@@ static unsigned long nested_ept_get_cr3
  static u64 construct_eptp(unsigned long root_hpa);
  static void kvm_cpu_vmxon(u64 addr);
  static void kvm_cpu_vmxoff(void);
++<<<<<<< HEAD
 +static int vmx_cpu_uses_apicv(struct kvm_vcpu *vcpu);
++=======
+ static bool vmx_mpx_supported(void);
+ static bool vmx_xsaves_supported(void);
++>>>>>>> d62caabb41f3 (kvm/x86: per-vcpu apicv deactivation support)
  static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr);
  static void vmx_set_segment(struct kvm_vcpu *vcpu,
  			    struct kvm_segment *var, int seg);
@@@ -8223,10 -8249,9 +8221,14 @@@ static void vmx_hwapic_irr_update(struc
  	}
  }
  
 -static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	u64 *eoi_exit_bitmap = vcpu->arch.eoi_exit_bitmap;
 +	if (!vmx_cpu_uses_apicv(vcpu))
++=======
+ 	if (!kvm_vcpu_apicv_active(vcpu))
++>>>>>>> d62caabb41f3 (kvm/x86: per-vcpu apicv deactivation support)
  		return;
  
  	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);
diff --cc arch/x86/kvm/x86.c
index ae62c46b426c,f0250a092ef3..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -2951,9 -2748,12 +2951,16 @@@ void kvm_arch_vcpu_put(struct kvm_vcpu 
  static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
  				    struct kvm_lapic_state *s)
  {
++<<<<<<< HEAD
 +	kvm_x86_ops->sync_pir_to_irr(vcpu);
++=======
+ 	if (vcpu->arch.apicv_active)
+ 		kvm_x86_ops->sync_pir_to_irr(vcpu);
+ 
+ 	memcpy(s->regs, vcpu->arch.apic->regs, sizeof *s);
++>>>>>>> d62caabb41f3 (kvm/x86: per-vcpu apicv deactivation support)
  
 -	return 0;
 +	return kvm_apic_get_state(vcpu, s);
  }
  
  static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
@@@ -6183,9 -5965,12 +6196,12 @@@ static void update_cr8_intercept(struc
  	if (!kvm_x86_ops->update_cr8_intercept)
  		return;
  
 -	if (!vcpu->arch.apic)
 +	if (!lapic_in_kernel(vcpu))
  		return;
  
+ 	if (vcpu->arch.apicv_active)
+ 		return;
+ 
  	if (!vcpu->arch.apic->vapic_addr)
  		max_irr = kvm_lapic_find_highest_irr(vcpu);
  	else
@@@ -6531,14 -6312,23 +6547,31 @@@ static void vcpu_scan_ioapic(struct kvm
  	if (!kvm_apic_hw_enabled(vcpu->arch.apic))
  		return;
  
 -	bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
 +	memset(vcpu->arch.eoi_exit_bitmap, 0, 256 / 8);
  
 +	kvm_x86_ops->sync_pir_to_irr(vcpu);
  	if (irqchip_split(vcpu->kvm))
++<<<<<<< HEAD
 +		kvm_scan_ioapic_routes(vcpu, vcpu->arch.eoi_exit_bitmap);
 +	else
 +		kvm_ioapic_scan_entry(vcpu, vcpu->arch.eoi_exit_bitmap);
 +	kvm_x86_ops->load_eoi_exitmap(vcpu);
++=======
+ 		kvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);
+ 	else {
+ 		if (vcpu->arch.apicv_active)
+ 			kvm_x86_ops->sync_pir_to_irr(vcpu);
+ 		kvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);
+ 	}
+ 	kvm_x86_ops->load_eoi_exitmap(vcpu,
+ 				      (u64 *)vcpu->arch.ioapic_handled_vectors);
+ }
+ 
+ static void kvm_vcpu_flush_tlb(struct kvm_vcpu *vcpu)
+ {
+ 	++vcpu->stat.tlb_flush;
+ 	kvm_x86_ops->tlb_flush(vcpu);
++>>>>>>> d62caabb41f3 (kvm/x86: per-vcpu apicv deactivation support)
  }
  
  void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
* Unmerged path arch/x86/include/asm/kvm_host.h
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 9aab276cb977..95fcc7b13866 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -79,7 +79,7 @@ int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v)
 	if (kvm_cpu_has_extint(v))
 		return 1;
 
-	if (kvm_vcpu_apic_vid_enabled(v))
+	if (kvm_vcpu_apicv_active(v))
 		return 0;
 
 	return kvm_apic_has_interrupt(v) != -1; /* LAPIC */
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 47c6b9ccd177..0af3845d4af3 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -402,7 +402,8 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 	if (!apic->irr_pending)
 		return -1;
 
-	kvm_x86_ops->sync_pir_to_irr(apic->vcpu);
+	if (apic->vcpu->arch.apicv_active)
+		kvm_x86_ops->sync_pir_to_irr(apic->vcpu);
 	result = apic_search_irr(apic);
 	ASSERT(result == -1 || result >= 16);
 
@@ -415,7 +416,7 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 
 	vcpu = apic->vcpu;
 
-	if (unlikely(kvm_vcpu_apic_vid_enabled(vcpu))) {
+	if (unlikely(vcpu->arch.apicv_active)) {
 		/* try to update RVI */
 		apic_clear_vector(vec, apic->regs + APIC_IRR);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
@@ -441,7 +442,7 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	 * because the processor can modify ISR under the hood.  Instead
 	 * just set SVI.
 	 */
-	if (unlikely(kvm_x86_ops->hwapic_isr_update))
+	if (unlikely(vcpu->arch.apicv_active))
 		kvm_x86_ops->hwapic_isr_update(vcpu->kvm, vec);
 	else {
 		++apic->isr_count;
@@ -489,7 +490,7 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	 * on the other hand isr_count and highest_isr_cache are unused
 	 * and must be left alone.
 	 */
-	if (unlikely(kvm_x86_ops->hwapic_isr_update))
+	if (unlikely(vcpu->arch.apicv_active))
 		kvm_x86_ops->hwapic_isr_update(vcpu->kvm,
 					       apic_find_highest_isr(apic));
 	else {
@@ -937,7 +938,7 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 				apic_clear_vector(vector, apic->regs + APIC_TMR);
 		}
 
-		if (kvm_x86_ops->deliver_posted_interrupt)
+		if (vcpu->arch.apicv_active)
 			kvm_x86_ops->deliver_posted_interrupt(vcpu, vector);
 		else {
 			apic_set_irr(vector, apic);
@@ -1274,7 +1275,7 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 		int vec = reg & APIC_VECTOR_MASK;
 		void *bitmap = apic->regs + APIC_ISR;
 
-		if (kvm_x86_ops->deliver_posted_interrupt)
+		if (vcpu->arch.apicv_active)
 			bitmap = apic->regs + APIC_IRR;
 
 		if (apic_test_vector(vec, bitmap))
@@ -1767,8 +1768,8 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 		apic_set_reg(apic, APIC_ISR + 0x10 * i, 0);
 		apic_set_reg(apic, APIC_TMR + 0x10 * i, 0);
 	}
-	apic->irr_pending = kvm_vcpu_apic_vid_enabled(vcpu);
-	apic->isr_count = kvm_x86_ops->hwapic_isr_update ? 1 : 0;
+	apic->irr_pending = vcpu->arch.apicv_active;
+	apic->isr_count = vcpu->arch.apicv_active ? 1 : 0;
 	apic->highest_isr_cache = -1;
 	update_divide_count(apic);
 	atomic_set(&apic->lapic_timer.pending, 0);
@@ -2010,15 +2011,15 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	update_divide_count(apic);
 	start_apic_timer(apic);
 	apic->irr_pending = true;
-	apic->isr_count = kvm_x86_ops->hwapic_isr_update ?
+	apic->isr_count = vcpu->arch.apicv_active ?
 				1 : count_vectors(apic->regs + APIC_ISR);
 	apic->highest_isr_cache = -1;
-	if (kvm_x86_ops->hwapic_irr_update)
+	if (vcpu->arch.apicv_active) {
 		kvm_x86_ops->hwapic_irr_update(vcpu,
 				apic_find_highest_irr(apic));
-	if (unlikely(kvm_x86_ops->hwapic_isr_update))
 		kvm_x86_ops->hwapic_isr_update(vcpu->kvm,
 				apic_find_highest_isr(apic));
+	}
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 	if (ioapic_in_kernel(vcpu->kvm))
 		kvm_rtc_eoi_tracking_restore_one(vcpu);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 639882f11237..ff8adbaf6159 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -146,9 +146,9 @@ static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
 }
 
-static inline bool kvm_vcpu_apic_vid_enabled(struct kvm_vcpu *vcpu)
+static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 {
-	return kvm_x86_ops->cpu_uses_apicv(vcpu);
+	return vcpu->arch.apic && vcpu->arch.apicv_active;
 }
 
 static inline bool kvm_apic_has_events(struct kvm_vcpu *vcpu)
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index b645837920e7..3228580636fc 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -3545,9 +3545,13 @@ static void svm_set_virtual_x2apic_mode(struct kvm_vcpu *vcpu, bool set)
 	return;
 }
 
-static int svm_cpu_uses_apicv(struct kvm_vcpu *vcpu)
+static bool svm_get_enable_apicv(void)
+{
+	return false;
+}
+
+static void svm_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
-	return 0;
 }
 
 static void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu)
@@ -4307,7 +4311,8 @@ static struct kvm_x86_ops svm_x86_ops = {
 	.enable_irq_window = enable_irq_window,
 	.update_cr8_intercept = update_cr8_intercept,
 	.set_virtual_x2apic_mode = svm_set_virtual_x2apic_mode,
-	.cpu_uses_apicv = svm_cpu_uses_apicv,
+	.get_enable_apicv = svm_get_enable_apicv,
+	.refresh_apicv_exec_ctrl = svm_refresh_apicv_exec_ctrl,
 	.load_eoi_exitmap = svm_load_eoi_exitmap,
 	.sync_pir_to_irr = svm_sync_pir_to_irr,
 
* Unmerged path arch/x86/kvm/vmx.c
* Unmerged path arch/x86/kvm/x86.c
