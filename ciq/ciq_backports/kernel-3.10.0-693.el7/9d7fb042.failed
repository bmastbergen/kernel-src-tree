sched/cputime: Guarantee stime + utime == rtime

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 9d7fb04276481c59610983362d8e023d262b58ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/9d7fb042.failed

While the current code guarantees monotonicity for stime and utime
independently of one another, it does not guarantee that the sum of
both is equal to the total time we started out with.

This confuses things (and peoples) who look at this sum, like top, and
will report >100% usage followed by a matching period of 0%.

Rework the code to provide both individual monotonicity and a coherent
sum.

	Suggested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
	Reported-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
	Tested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Stanislaw Gruszka <sgruszka@redhat.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: jason.low2@hp.com
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9d7fb04276481c59610983362d8e023d262b58ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/init_task.h
#	kernel/sched/cputime.c
diff --cc include/linux/init_task.h
index d0167bda04ee,d0b380ee7d67..000000000000
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@@ -25,20 -25,21 +25,28 @@@
  extern struct files_struct init_files;
  extern struct fs_struct init_fs;
  
 +#ifdef CONFIG_CGROUPS
 +#define INIT_GROUP_RWSEM(sig)						\
 +	.group_rwsem = __RWSEM_INITIALIZER(sig.group_rwsem),
 +#else
 +#define INIT_GROUP_RWSEM(sig)
 +#endif
 +
  #ifdef CONFIG_CPUSETS
 -#define INIT_CPUSET_SEQ(tsk)							\
 -	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
 +#define INIT_CPUSET_SEQ							\
 +	.mems_allowed_seq = SEQCNT_ZERO,
  #else
 -#define INIT_CPUSET_SEQ(tsk)
 +#define INIT_CPUSET_SEQ
  #endif
  
+ #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
+ #define INIT_PREV_CPUTIME(x)	.prev_cputime = {			\
+ 	.lock = __RAW_SPIN_LOCK_UNLOCKED(x.prev_cputime.lock),		\
+ },
+ #else
+ #define INIT_PREV_CPUTIME(x)
+ #endif
+ 
  #define INIT_SIGNALS(sig) {						\
  	.nr_threads	= 1,						\
  	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),	\
@@@ -50,13 -51,12 +58,14 @@@
  	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
  	.rlim		= INIT_RLIMITS,					\
  	.cputimer	= { 						\
 -		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
 -		.running	= 0,					\
 +		.cputime = INIT_CPUTIME,				\
 +		.running = 0,						\
 +		.lock = __RAW_SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
  	},								\
+ 	INIT_PREV_CPUTIME(sig)						\
  	.cred_guard_mutex =						\
  		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
 +	INIT_GROUP_RWSEM(sig)						\
  }
  
  extern struct nsproxy init_nsproxy;
@@@ -231,9 -252,13 +240,16 @@@ extern struct task_group root_task_grou
  	INIT_FTRACE_GRAPH						\
  	INIT_TRACE_RECURSION						\
  	INIT_TASK_RCU_PREEMPT(tsk)					\
++<<<<<<< HEAD
 +	INIT_CPUSET_SEQ							\
 +	INIT_RT_MUTEXES(tsk)					\
++=======
+ 	INIT_TASK_RCU_TASKS(tsk)					\
+ 	INIT_CPUSET_SEQ(tsk)						\
+ 	INIT_RT_MUTEXES(tsk)						\
+ 	INIT_PREV_CPUTIME(tsk)						\
++>>>>>>> 9d7fb0427648 (sched/cputime: Guarantee stime + utime == rtime)
  	INIT_VTIME(tsk)							\
 -	INIT_NUMA_BALANCING(tsk)					\
 -	INIT_KASAN(tsk)							\
  }
  
  
diff --cc kernel/sched/cputime.c
index cebed7e6f9b8,8cbc3db671df..000000000000
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@@ -555,11 -555,27 +555,32 @@@ drop_precision
  }
  
  /*
++<<<<<<< HEAD
 + * Adjust tick based cputime random precision against scheduler
 + * runtime accounting.
++=======
+  * Adjust tick based cputime random precision against scheduler runtime
+  * accounting.
+  *
+  * Tick based cputime accounting depend on random scheduling timeslices of a
+  * task to be interrupted or not by the timer.  Depending on these
+  * circumstances, the number of these interrupts may be over or
+  * under-optimistic, matching the real user and system cputime with a variable
+  * precision.
+  *
+  * Fix this by scaling these tick based values against the total runtime
+  * accounted by the CFS scheduler.
+  *
+  * This code provides the following guarantees:
+  *
+  *   stime + utime == rtime
+  *   stime_i+1 >= stime_i, utime_i+1 >= utime_i
+  *
+  * Assuming that rtime_i+1 >= rtime_i.
++>>>>>>> 9d7fb0427648 (sched/cputime: Guarantee stime + utime == rtime)
   */
  static void cputime_adjust(struct task_cputime *curr,
- 			   struct cputime *prev,
+ 			   struct prev_cputime *prev,
  			   cputime_t *ut, cputime_t *st)
  {
  	cputime_t rtime, stime, utime;
@@@ -586,29 -598,45 +603,57 @@@
  
  	stime = curr->stime;
  	utime = curr->utime;
 -
  	if (utime == 0) {
  		stime = rtime;
- 	} else if (stime == 0) {
- 		utime = rtime;
- 	} else {
- 		cputime_t total = stime + utime;
- 
- 		stime = scale_stime((__force u64)stime,
- 				    (__force u64)rtime, (__force u64)total);
- 		utime = rtime - stime;
+ 		goto update;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * If the tick based count grows faster than the scheduler one,
 +	 * the result of the scaling may go backward.
 +	 * Let's enforce monotonicity.
 +	 * Atomic exchange protects against concurrent cputime_adjust.
 +	 */
 +	while (stime > (rtime = ACCESS_ONCE(prev->stime)))
 +		cmpxchg(&prev->stime, rtime, stime);
 +	while (utime > (rtime = ACCESS_ONCE(prev->utime)))
 +		cmpxchg(&prev->utime, rtime, utime);
++=======
+ 	if (stime == 0) {
+ 		utime = rtime;
+ 		goto update;
+ 	}
++>>>>>>> 9d7fb0427648 (sched/cputime: Guarantee stime + utime == rtime)
+ 
+ 	stime = scale_stime((__force u64)stime, (__force u64)rtime,
+ 			    (__force u64)(stime + utime));
+ 
+ 	/*
+ 	 * Make sure stime doesn't go backwards; this preserves monotonicity
+ 	 * for utime because rtime is monotonic.
+ 	 *
+ 	 *  utime_i+1 = rtime_i+1 - stime_i
+ 	 *            = rtime_i+1 - (rtime_i - utime_i)
+ 	 *            = (rtime_i+1 - rtime_i) + utime_i
+ 	 *            >= utime_i
+ 	 */
+ 	if (stime < prev->stime)
+ 		stime = prev->stime;
+ 	utime = rtime - stime;
+ 
+ 	/*
+ 	 * Make sure utime doesn't go backwards; this still preserves
+ 	 * monotonicity for stime, analogous argument to above.
+ 	 */
+ 	if (utime < prev->utime) {
+ 		utime = prev->utime;
+ 		stime = rtime - utime;
+ 	}
  
+ update:
+ 	prev->stime = stime;
+ 	prev->utime = utime;
  out:
  	*ut = prev->utime;
  	*st = prev->stime;
* Unmerged path include/linux/init_task.h
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 02fc1f8f117b..6cd41944534b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -478,39 +478,49 @@ struct cpu_itimer {
 };
 
 /**
- * struct cputime - snaphsot of system and user cputime
+ * struct prev_cputime - snaphsot of system and user cputime
  * @utime: time spent in user mode
  * @stime: time spent in system mode
+ * @lock: protects the above two fields
  *
- * Gathers a generic snapshot of user and system time.
+ * Stores previous user/system time values such that we can guarantee
+ * monotonicity.
  */
-struct cputime {
+struct prev_cputime {
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 	cputime_t utime;
 	cputime_t stime;
+	raw_spinlock_t lock;
+#endif
 };
 
+static inline void prev_cputime_init(struct prev_cputime *prev)
+{
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
+	prev->utime = prev->stime = 0;
+	raw_spin_lock_init(&prev->lock);
+#endif
+}
+
 /**
  * struct task_cputime - collected CPU time counts
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
  *
- * This is an extension of struct cputime that includes the total runtime
- * spent by the task from the scheduler point of view.
- *
- * As a result, this structure groups together three kinds of CPU time
- * that are tracked for threads and thread groups.  Most things considering
- * CPU time want to group these counts together and treat all three
- * of them in parallel.
+ * This structure groups together three kinds of CPU time that are tracked for
+ * threads and thread groups.  Most things considering CPU time want to group
+ * these counts together and treat all three of them in parallel.
  */
 struct task_cputime {
 	cputime_t utime;
 	cputime_t stime;
 	unsigned long long sum_exec_runtime;
 };
+
 /* Alternate field names when used to cache expirations. */
-#define prof_exp	stime
 #define virt_exp	utime
+#define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
 #define INIT_CPUTIME	\
@@ -641,9 +651,7 @@ struct signal_struct {
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	struct cputime prev_cputime;
-#endif
+	struct prev_cputime prev_cputime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
@@ -1435,9 +1443,7 @@ struct task_struct {
 
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	struct cputime prev_cputime;
-#endif
+	struct prev_cputime prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqlock_t vtime_seqlock;
 	unsigned long long vtime_snap;
diff --git a/kernel/fork.c b/kernel/fork.c
index 4d1b25f88335..1a15ac50c62d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1076,6 +1076,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	rcu_assign_pointer(tsk->sighand, sig);
 	if (!sig)
 		return -ENOMEM;
+
 	atomic_set(&sig->count, 1);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 	return 0;
@@ -1137,6 +1138,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 	seqlock_init(&sig->stats_lock);
+	prev_cputime_init(&sig->prev_cputime);
 
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;
@@ -1336,9 +1338,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->utime = p->stime = p->gtime = 0;
 	p->utimescaled = p->stimescaled = 0;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	p->prev_cputime.utime = p->prev_cputime.stime = 0;
-#endif
+	prev_cputime_init(&p->prev_cputime);
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqlock_init(&p->vtime_seqlock);
 	p->vtime_snap = 0;
* Unmerged path kernel/sched/cputime.c
