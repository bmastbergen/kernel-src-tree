swiotlb: move global variables into a new io_tlb_mem structure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Claire Chang <tientzu@chromium.org>
commit 73f620951b2b594bdc38722c0d647c3b3312af7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/73f62095.failed

Added a new struct, io_tlb_mem, as the IO TLB memory pool descriptor and
moved relevant global variables into that struct.
This will be useful later to allow for restricted DMA pool.

	Signed-off-by: Claire Chang <tientzu@chromium.org>
[hch: rebased]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 73f620951b2b594bdc38722c0d647c3b3312af7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,d9c097f0f78c..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -91,25 -71,6 +70,28 @@@ struct io_tlb_mem io_tlb_default_mem
   */
  static unsigned int max_segment;
  
++<<<<<<< HEAD
 +/*
 + * We need to save away the original address corresponding to a mapped entry
 + * for the sync operations.
 + */
 +#define INVALID_PHYS_ADDR (~(phys_addr_t)0)
 +static phys_addr_t *io_tlb_orig_addr;
 +
 +/*
 + * The mapped buffer's size should be validated during a sync operation.
 + */
 +static size_t *io_tlb_orig_size;
 +
 +/*
 + * Protect the above data structures in the map and unmap calls
 + */
 +static DEFINE_SPINLOCK(io_tlb_lock);
 +
 +static int late_alloc;
 +
++=======
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  static int __init
  setup_io_tlb_npages(char *str)
  {
@@@ -255,18 -224,17 +245,30 @@@ int __init swiotlb_init_with_tbl(char *
  		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
  		      __func__, alloc_size, PAGE_SIZE);
  
++<<<<<<< HEAD
 +	alloc_size = PAGE_ALIGN(io_tlb_nslabs * sizeof(size_t));
 +	io_tlb_orig_size = memblock_alloc(alloc_size, PAGE_SIZE);
 +	if (!io_tlb_orig_size)
 +		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 +		      __func__, alloc_size, PAGE_SIZE);
 +
 +	for (i = 0; i < io_tlb_nslabs; i++) {
 +		io_tlb_list[i] = IO_TLB_SEGSIZE - io_tlb_offset(i);
 +		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 +		io_tlb_orig_size[i] = 0;
++=======
+ 	alloc_size = PAGE_ALIGN(mem->nslabs * sizeof(size_t));
+ 	mem->alloc_size = memblock_alloc(alloc_size, PAGE_SIZE);
+ 	if (mem->alloc_size)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
+ 		      __func__, alloc_size, PAGE_SIZE);
+ 
+ 	for (i = 0; i < mem->nslabs; i++) {
+ 		mem->list[i] = IO_TLB_SEGSIZE - io_tlb_offset(i);
+ 		mem->orig_addr[i] = INVALID_PHYS_ADDR;
+ 		mem->alloc_size[i] = 0;
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	}
- 	io_tlb_index = 0;
  	no_iotlb_memory = false;
  
  	if (verbose)
@@@ -287,22 -256,22 +290,27 @@@ swiotlb_init(int verbose
  	unsigned char *vstart;
  	unsigned long bytes;
  
- 	if (!io_tlb_nslabs) {
- 		io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
- 		io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ 	if (!mem->nslabs) {
+ 		mem->nslabs = (default_size >> IO_TLB_SHIFT);
+ 		mem->nslabs = ALIGN(mem->nslabs, IO_TLB_SEGSIZE);
  	}
  
- 	bytes = io_tlb_nslabs << IO_TLB_SHIFT;
+ 	bytes = mem->nslabs << IO_TLB_SHIFT;
  
  	/* Get IO TLB memory from the low pages */
++<<<<<<< HEAD
 +	vstart = memblock_alloc_low_nopanic(PAGE_ALIGN(bytes), PAGE_SIZE);
 +	if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))
++=======
+ 	vstart = memblock_alloc_low(PAGE_ALIGN(bytes), PAGE_SIZE);
+ 	if (vstart && !swiotlb_init_with_tbl(vstart, mem->nslabs, verbose))
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  		return;
  
- 	if (io_tlb_start) {
- 		memblock_free_early(io_tlb_start,
- 				    PAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));
- 		io_tlb_start = 0;
+ 	if (mem->start) {
+ 		memblock_free_early(mem->start,
+ 				    PAGE_ALIGN(mem->nslabs << IO_TLB_SHIFT));
+ 		mem->start = 0;
  	}
  	pr_warn("Cannot allocate buffer");
  	no_iotlb_memory = true;
@@@ -386,34 -362,32 +401,47 @@@ swiotlb_late_init_with_tbl(char *tlb, u
  	/*
  	 * Allocate and initialize the free list array.  This array is used
  	 * to find contiguous free memory regions of size up to IO_TLB_SEGSIZE
- 	 * between io_tlb_start and io_tlb_end.
+ 	 * between mem->start and mem->end.
  	 */
- 	io_tlb_list = (unsigned int *)__get_free_pages(GFP_KERNEL,
- 				      get_order(io_tlb_nslabs * sizeof(int)));
- 	if (!io_tlb_list)
+ 	mem->list = (unsigned int *)__get_free_pages(GFP_KERNEL,
+ 	                              get_order(mem->nslabs * sizeof(int)));
+ 	if (!mem->list)
  		goto cleanup3;
  
- 	io_tlb_orig_addr = (phys_addr_t *)
+ 	mem->orig_addr = (phys_addr_t *)
  		__get_free_pages(GFP_KERNEL,
- 				 get_order(io_tlb_nslabs *
+ 				 get_order(mem->nslabs *
  					   sizeof(phys_addr_t)));
- 	if (!io_tlb_orig_addr)
+ 	if (!mem->orig_addr)
  		goto cleanup4;
  
++<<<<<<< HEAD
 +	io_tlb_orig_size = (size_t *)
++=======
+ 	mem->alloc_size = (size_t *)
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  		__get_free_pages(GFP_KERNEL,
- 				 get_order(io_tlb_nslabs *
+ 				 get_order(mem->nslabs *
  					   sizeof(size_t)));
++<<<<<<< HEAD
 +	if (!io_tlb_orig_size)
 +		goto cleanup5;
 +
 +
 +	for (i = 0; i < io_tlb_nslabs; i++) {
 +		io_tlb_list[i] = IO_TLB_SEGSIZE - io_tlb_offset(i);
 +		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 +		io_tlb_orig_size[i] = 0;
++=======
+ 	if (!mem->alloc_size)
+ 		goto cleanup5;
+ 
+ 	for (i = 0; i < mem->nslabs; i++) {
+ 		mem->list[i] = IO_TLB_SEGSIZE - io_tlb_offset(i);
+ 		mem->orig_addr[i] = INVALID_PHYS_ADDR;
+ 		mem->alloc_size[i] = 0;
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	}
- 	io_tlb_index = 0;
  	no_iotlb_memory = false;
  
  	swiotlb_print_info();
@@@ -439,27 -408,29 +462,50 @@@ cleanup3
  
  void __init swiotlb_exit(void)
  {
- 	if (!io_tlb_orig_addr)
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 
+ 	if (!mem->orig_addr)
  		return;
  
++<<<<<<< HEAD
 +	if (late_alloc) {
 +		free_pages((unsigned long)io_tlb_orig_size,
 +			   get_order(io_tlb_nslabs * sizeof(size_t)));
 +		free_pages((unsigned long)io_tlb_orig_addr,
 +			   get_order(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		free_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *
 +								 sizeof(int)));
 +		free_pages((unsigned long)phys_to_virt(io_tlb_start),
 +			   get_order(io_tlb_nslabs << IO_TLB_SHIFT));
 +	} else {
 +		memblock_free_late(__pa(io_tlb_orig_addr),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		memblock_free_late(__pa(io_tlb_orig_size),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(size_t)));
 +		memblock_free_late(__pa(io_tlb_list),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(int)));
 +		memblock_free_late(io_tlb_start,
 +				   PAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));
++=======
+ 	if (mem->late_alloc) {
+ 		free_pages((unsigned long)mem->alloc_size,
+ 			   get_order(mem->nslabs * sizeof(size_t)));
+ 		free_pages((unsigned long)mem->orig_addr,
+ 			   get_order(mem->nslabs * sizeof(phys_addr_t)));
+ 		free_pages((unsigned long)mem->list,
+ 			   get_order(mem->nslabs * sizeof(int)));
+ 		free_pages((unsigned long)phys_to_virt(mem->start),
+ 			   get_order(mem->nslabs << IO_TLB_SHIFT));
+ 	} else {
+ 		memblock_free_late(__pa(mem->alloc_size),
+ 				   PAGE_ALIGN(mem->nslabs * sizeof(size_t)));
+ 		memblock_free_late(__pa(mem->orig_addr),
+ 				   PAGE_ALIGN(mem->nslabs * sizeof(phys_addr_t)));
+ 		memblock_free_late(__pa(mem->list),
+ 				   PAGE_ALIGN(mem->nslabs * sizeof(int)));
+ 		memblock_free_late(mem->start,
+ 				   PAGE_ALIGN(mem->nslabs << IO_TLB_SHIFT));
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	}
  	swiotlb_cleanup();
  }
@@@ -467,9 -438,13 +513,16 @@@
  /*
   * Bounce: copy the swiotlb buffer from or back to the original dma location
   */
 -static void swiotlb_bounce(struct device *dev, phys_addr_t tlb_addr, size_t size,
 -			   enum dma_data_direction dir)
 +static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 +			   size_t size, enum dma_data_direction dir)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 	int index = (tlb_addr - mem->start) >> IO_TLB_SHIFT;
+ 	phys_addr_t orig_addr = mem->orig_addr[index];
+ 	size_t alloc_size = mem->alloc_size[index];
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	unsigned long pfn = PFN_DOWN(orig_addr);
  	unsigned char *vaddr = phys_to_virt(tlb_addr);
  
@@@ -614,9 -600,9 +668,10 @@@ phys_addr_t swiotlb_tbl_map_single(stru
  		size_t mapping_size, size_t alloc_size,
  		enum dma_data_direction dir, unsigned long attrs)
  {
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
  	unsigned int offset = swiotlb_align_offset(dev, orig_addr);
 -	unsigned int index, i;
 +	unsigned int i;
 +	int index;
  	phys_addr_t tlb_addr;
  
  	if (no_iotlb_memory)
@@@ -646,13 -632,13 +701,18 @@@
  	 * needed.
  	 */
  	for (i = 0; i < nr_slots(alloc_size + offset); i++) {
++<<<<<<< HEAD
 +		io_tlb_orig_addr[index + i] = slot_addr(orig_addr, i);
 +		io_tlb_orig_size[index+i] = alloc_size - (i << IO_TLB_SHIFT);
++=======
+ 		mem->orig_addr[index + i] = slot_addr(orig_addr, i);
+ 		mem->alloc_size[index + i] = alloc_size - (i << IO_TLB_SHIFT);
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	}
- 	tlb_addr = slot_addr(io_tlb_start, index) + offset;
+ 	tlb_addr = slot_addr(mem->start, index) + offset;
  	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
  	    (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
 -		swiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);
 +		swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_TO_DEVICE);
  	return tlb_addr;
  }
  
@@@ -671,16 -646,15 +731,23 @@@ static void validate_sync_size_and_trun
   * tlb_addr is the physical address of the bounce buffer to unmap.
   */
  void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 -			      size_t mapping_size, enum dma_data_direction dir,
 -			      unsigned long attrs)
 +			      size_t mapping_size, size_t alloc_size,
 +			      enum dma_data_direction dir, unsigned long attrs)
  {
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
  	unsigned long flags;
  	unsigned int offset = swiotlb_align_offset(hwdev, tlb_addr);
++<<<<<<< HEAD
 +	int i, count, nslots = nr_slots(alloc_size + offset);
 +	int index = (tlb_addr - offset - io_tlb_start) >> IO_TLB_SHIFT;
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
 +
 +	validate_sync_size_and_truncate(hwdev, io_tlb_orig_size[index], &mapping_size);
++=======
+ 	int index = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;
+ 	int nslots = nr_slots(mem->alloc_size[index] + offset);
+ 	int count, i;
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  
  	/*
  	 * First, sync the memory before unmapping the entry
@@@ -707,9 -680,9 +774,15 @@@
  	 * superceeding slots
  	 */
  	for (i = index + nslots - 1; i >= index; i--) {
++<<<<<<< HEAD
 +		io_tlb_list[i] = ++count;
 +		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 +		io_tlb_orig_size[i] = 0;
++=======
+ 		mem->list[i] = ++count;
+ 		mem->orig_addr[i] = INVALID_PHYS_ADDR;
+ 		mem->alloc_size[i] = 0;
++>>>>>>> 73f620951b2b (swiotlb: move global variables into a new io_tlb_mem structure)
  	}
  
  	/*
@@@ -717,44 -690,29 +790,44 @@@
  	 * available (non zero)
  	 */
  	for (i = index - 1;
- 	     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && io_tlb_list[i];
+ 	     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && mem->list[i];
  	     i--)
- 		io_tlb_list[i] = ++count;
- 	io_tlb_used -= nslots;
- 	spin_unlock_irqrestore(&io_tlb_lock, flags);
+ 		mem->list[i] = ++count;
+ 	mem->used -= nslots;
+ 	spin_unlock_irqrestore(&mem->lock, flags);
  }
  
 -void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,
 -		size_t size, enum dma_data_direction dir)
 +void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 +			     size_t size, enum dma_data_direction dir,
 +			     enum dma_sync_target target)
  {
 -	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)
 -		swiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);
 -	else
 -		BUG_ON(dir != DMA_FROM_DEVICE);
 -}
 +	int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
 +	size_t orig_size = io_tlb_orig_size[index];
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
  
 -void swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,
 -		size_t size, enum dma_data_direction dir)
 -{
 -	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
 -		swiotlb_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);
 -	else
 -		BUG_ON(dir != DMA_TO_DEVICE);
 +	if (orig_addr == INVALID_PHYS_ADDR)
 +		return;
 +
 +	validate_sync_size_and_truncate(hwdev, orig_size, &size);
 +
 +	switch (target) {
 +	case SYNC_FOR_CPU:
 +		if (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
 +			swiotlb_bounce(orig_addr, tlb_addr,
 +				       size, DMA_FROM_DEVICE);
 +		else
 +			BUG_ON(dir != DMA_TO_DEVICE);
 +		break;
 +	case SYNC_FOR_DEVICE:
 +		if (likely(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
 +			swiotlb_bounce(orig_addr, tlb_addr,
 +				       size, DMA_TO_DEVICE);
 +		else
 +			BUG_ON(dir != DMA_FROM_DEVICE);
 +		break;
 +	default:
 +		BUG();
 +	}
  }
  
  /*
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 8ccd85660984..d15f401e8b5e 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -526,7 +526,7 @@ xen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
 static int
 xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
-	return xen_phys_to_dma(hwdev, io_tlb_end - 1) <= mask;
+	return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
 }
 
 /*
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index 5857a937c637..b2ed0fa359ca 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -6,6 +6,7 @@
 #include <linux/init.h>
 #include <linux/types.h>
 #include <linux/limits.h>
+#include <linux/spinlock.h>
 
 struct device;
 struct page;
@@ -71,11 +72,49 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t phys,
 
 #ifdef CONFIG_SWIOTLB
 extern enum swiotlb_force swiotlb_force;
-extern phys_addr_t io_tlb_start, io_tlb_end;
+
+/**
+ * struct io_tlb_mem - IO TLB Memory Pool Descriptor
+ *
+ * @start:	The start address of the swiotlb memory pool. Used to do a quick
+ *		range check to see if the memory was in fact allocated by this
+ *		API.
+ * @end:	The end address of the swiotlb memory pool. Used to do a quick
+ *		range check to see if the memory was in fact allocated by this
+ *		API.
+ * @nslabs:	The number of IO TLB blocks (in groups of 64) between @start and
+ *		@end. This is command line adjustable via setup_io_tlb_npages.
+ * @used:	The number of used IO TLB block.
+ * @list:	The free list describing the number of free entries available
+ *		from each index.
+ * @index:	The index to start searching in the next round.
+ * @orig_addr:	The original address corresponding to a mapped entry.
+ * @alloc_size:	Size of the allocated buffer.
+ * @lock:	The lock to protect the above data structures in the map and
+ *		unmap calls.
+ * @debugfs:	The dentry to debugfs.
+ * @late_alloc:	%true if allocated using the page allocator
+ */
+struct io_tlb_mem {
+	phys_addr_t start;
+	phys_addr_t end;
+	unsigned long nslabs;
+	unsigned long used;
+	unsigned int *list;
+	unsigned int index;
+	phys_addr_t *orig_addr;
+	size_t *alloc_size;
+	spinlock_t lock;
+	struct dentry *debugfs;
+	bool late_alloc;
+};
+extern struct io_tlb_mem io_tlb_default_mem;
 
 static inline bool is_swiotlb_buffer(phys_addr_t paddr)
 {
-	return paddr >= io_tlb_start && paddr < io_tlb_end;
+	struct io_tlb_mem *mem = &io_tlb_default_mem;
+
+	return paddr >= mem->start && paddr < mem->end;
 }
 
 void __init swiotlb_exit(void);
* Unmerged path kernel/dma/swiotlb.c
