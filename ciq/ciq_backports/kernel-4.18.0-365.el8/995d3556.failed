swiotlb-xen: add struct device * parameter to xen_dma_sync_for_device

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Stefano Stabellini <stefano.stabellini@xilinx.com>
commit 995d3556694edd3c6dda7671b46ad4a6b3043f2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/995d3556.failed

No functional changes. The parameter is unused in this patch but will be
used by next patches.

	Signed-off-by: Stefano Stabellini <stefano.stabellini@xilinx.com>
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Tested-by: Corey Minyard <cminyard@mvista.com>
	Tested-by: Roman Shaposhnik <roman@zededa.com>
Link: https://lore.kernel.org/r/20200710223427.6897-6-sstabellini@kernel.org
	Signed-off-by: Juergen Gross <jgross@suse.com>
(cherry picked from commit 995d3556694edd3c6dda7671b46ad4a6b3043f2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/xen/mm.c
#	include/xen/swiotlb-xen.h
diff --cc arch/arm/xen/mm.c
index cb44aa290e73,f2414ea40a79..000000000000
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@@ -35,105 -39,58 +35,111 @@@ unsigned long xen_get_swiotlb_free_page
  	return __get_free_pages(flags, order);
  }
  
 +enum dma_cache_op {
 +       DMA_UNMAP,
 +       DMA_MAP,
 +};
  static bool hypercall_cflush = false;
  
 -/* buffers in highmem or foreign pages cannot cross page boundaries */
 -static void dma_cache_maint(dma_addr_t handle, size_t size, u32 op)
 +/* functions called by SWIOTLB */
 +
 +static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
 +	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
  {
  	struct gnttab_cache_flush cflush;
 +	unsigned long xen_pfn;
 +	size_t left = size;
  
 -	cflush.a.dev_bus_addr = handle & XEN_PAGE_MASK;
 -	cflush.offset = xen_offset_in_page(handle);
 -	cflush.op = op;
 +	xen_pfn = (handle >> XEN_PAGE_SHIFT) + offset / XEN_PAGE_SIZE;
 +	offset %= XEN_PAGE_SIZE;
  
  	do {
 -		if (size + cflush.offset > XEN_PAGE_SIZE)
 -			cflush.length = XEN_PAGE_SIZE - cflush.offset;
 -		else
 -			cflush.length = size;
 -
 -		HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
 +		size_t len = left;
 +	
 +		/* buffers in highmem or foreign pages cannot cross page
 +		 * boundaries */
 +		if (len + offset > XEN_PAGE_SIZE)
 +			len = XEN_PAGE_SIZE - offset;
 +
 +		cflush.op = 0;
 +		cflush.a.dev_bus_addr = xen_pfn << XEN_PAGE_SHIFT;
 +		cflush.offset = offset;
 +		cflush.length = len;
 +
 +		if (op == DMA_UNMAP && dir != DMA_TO_DEVICE)
 +			cflush.op = GNTTAB_CACHE_INVAL;
 +		if (op == DMA_MAP) {
 +			if (dir == DMA_FROM_DEVICE)
 +				cflush.op = GNTTAB_CACHE_INVAL;
 +			else
 +				cflush.op = GNTTAB_CACHE_CLEAN;
 +		}
 +		if (cflush.op)
 +			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
  
 -		cflush.offset = 0;
 -		cflush.a.dev_bus_addr += cflush.length;
 -		size -= cflush.length;
 -	} while (size);
 +		offset = 0;
 +		xen_pfn++;
 +		left -= len;
 +	} while (left);
  }
  
 -/*
 - * Dom0 is mapped 1:1, and while the Linux page can span across multiple Xen
 - * pages, it is not possible for it to contain a mix of local and foreign Xen
 - * pages.  Calling pfn_valid on a foreign mfn will always return false, so if
 - * pfn_valid returns true the pages is local and we can use the native
 - * dma-direct functions, otherwise we call the Xen specific version.
 - */
 -void xen_dma_sync_for_cpu(struct device *dev, dma_addr_t handle,
 -			  phys_addr_t paddr, size_t size,
 -			  enum dma_data_direction dir)
 +static void __xen_dma_page_dev_to_cpu(struct device *hwdev, dma_addr_t handle,
 +		size_t size, enum dma_data_direction dir)
  {
 -	if (pfn_valid(PFN_DOWN(handle)))
 -		arch_sync_dma_for_cpu(paddr, size, dir);
 -	else if (dir != DMA_TO_DEVICE)
 -		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 +	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_UNMAP);
  }
  
++<<<<<<< HEAD
 +static void __xen_dma_page_cpu_to_dev(struct device *hwdev, dma_addr_t handle,
 +		size_t size, enum dma_data_direction dir)
++=======
+ void xen_dma_sync_for_device(struct device *dev, dma_addr_t handle,
+ 			     phys_addr_t paddr, size_t size,
+ 			     enum dma_data_direction dir)
++>>>>>>> 995d3556694e (swiotlb-xen: add struct device * parameter to xen_dma_sync_for_device)
  {
 -	if (pfn_valid(PFN_DOWN(handle)))
 -		arch_sync_dma_for_device(paddr, size, dir);
 -	else if (dir == DMA_FROM_DEVICE)
 -		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 -	else
 -		dma_cache_maint(handle, size, GNTTAB_CACHE_CLEAN);
 +	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_MAP);
 +}
 +
 +void __xen_dma_map_page(struct device *hwdev, struct page *page,
 +	     dma_addr_t dev_addr, unsigned long offset, size_t size,
 +	     enum dma_data_direction dir, unsigned long attrs)
 +{
 +	if (is_device_dma_coherent(hwdev))
 +		return;
 +	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 +		return;
 +
 +	__xen_dma_page_cpu_to_dev(hwdev, dev_addr, size, dir);
 +}
 +
 +void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
 +		size_t size, enum dma_data_direction dir,
 +		unsigned long attrs)
 +
 +{
 +	if (is_device_dma_coherent(hwdev))
 +		return;
 +	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 +		return;
 +
 +	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
 +}
 +
 +void __xen_dma_sync_single_for_cpu(struct device *hwdev,
 +		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 +{
 +	if (is_device_dma_coherent(hwdev))
 +		return;
 +	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
 +}
 +
 +void __xen_dma_sync_single_for_device(struct device *hwdev,
 +		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 +{
 +	if (is_device_dma_coherent(hwdev))
 +		return;
 +	__xen_dma_page_cpu_to_dev(hwdev, handle, size, dir);
  }
  
  bool xen_arch_need_swiotlb(struct device *dev,
diff --cc include/xen/swiotlb-xen.h
index ffc0d3902b71,6d235fe2b92d..000000000000
--- a/include/xen/swiotlb-xen.h
+++ b/include/xen/swiotlb-xen.h
@@@ -4,10 -4,12 +4,19 @@@
  
  #include <linux/swiotlb.h>
  
++<<<<<<< HEAD
 +void xen_dma_sync_for_cpu(dma_addr_t handle, phys_addr_t paddr, size_t size,
 +		enum dma_data_direction dir);
 +void xen_dma_sync_for_device(dma_addr_t handle, phys_addr_t paddr, size_t size,
 +		enum dma_data_direction dir);
++=======
+ void xen_dma_sync_for_cpu(struct device *dev, dma_addr_t handle,
+ 			  phys_addr_t paddr, size_t size,
+ 			  enum dma_data_direction dir);
+ void xen_dma_sync_for_device(struct device *dev, dma_addr_t handle,
+ 			     phys_addr_t paddr, size_t size,
+ 			     enum dma_data_direction dir);
++>>>>>>> 995d3556694e (swiotlb-xen: add struct device * parameter to xen_dma_sync_for_device)
  
  extern int xen_swiotlb_init(int verbose, bool early);
  extern const struct dma_map_ops xen_swiotlb_dma_ops;
* Unmerged path arch/arm/xen/mm.c
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index d199863cc446..a90b2697c1d7 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -422,7 +422,7 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 
 done:
 	if (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))
-		xen_dma_sync_for_device(dev_addr, phys, size, dir);
+		xen_dma_sync_for_device(dev, dev_addr, phys, size, dir);
 	return dev_addr;
 }
 
@@ -472,7 +472,7 @@ xen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,
 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
 
 	if (!dev_is_dma_coherent(dev))
-		xen_dma_sync_for_device(dma_addr, paddr, size, dir);
+		xen_dma_sync_for_device(dev, dma_addr, paddr, size, dir);
 }
 
 /*
* Unmerged path include/xen/swiotlb-xen.h
