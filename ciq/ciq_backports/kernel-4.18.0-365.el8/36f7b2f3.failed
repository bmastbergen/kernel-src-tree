swiotlb: Move alloc_size to swiotlb_find_slots

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Claire Chang <tientzu@chromium.org>
commit 36f7b2f3ca5f0bee00abf9ea52748ce0644a21c6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/36f7b2f3.failed

Rename find_slots to swiotlb_find_slots and move the maintenance of
alloc_size to it for better code reusability later.

	Signed-off-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Stefano Stabellini <sstabellini@kernel.org>
	Tested-by: Will Deacon <will@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 36f7b2f3ca5f0bee00abf9ea52748ce0644a21c6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,0116a630dc13..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -535,12 -430,13 +535,12 @@@ static unsigned int wrap_index(unsigne
   * Find a suitable number of IO TLB entries size that will fit this request and
   * allocate a buffer from that IO TLB pool.
   */
- static int find_slots(struct device *dev, phys_addr_t orig_addr,
- 		size_t alloc_size)
+ static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,
+ 			      size_t alloc_size)
  {
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
  	unsigned long boundary_mask = dma_get_seg_boundary(dev);
  	dma_addr_t tbl_dma_addr =
 -		phys_to_dma_unencrypted(dev, mem->start) & boundary_mask;
 +		phys_to_dma_unencrypted(dev, io_tlb_start) & boundary_mask;
  	unsigned long max_slots = get_max_slots(boundary_mask);
  	unsigned int iotlb_align_mask =
  		dma_get_min_align_mask(dev) & ~(IO_TLB_SIZE - 1);
@@@ -590,12 -487,15 +591,20 @@@ not_found
  	return -1;
  
  found:
++<<<<<<< HEAD
 +	for (i = index; i < index + nslots; i++)
 +		io_tlb_list[i] = 0;
++=======
+ 	for (i = index; i < index + nslots; i++) {
+ 		mem->slots[i].list = 0;
+ 		mem->slots[i].alloc_size =
+ 			alloc_size - (offset + ((i - index) << IO_TLB_SHIFT));
+ 	}
++>>>>>>> 36f7b2f3ca5f (swiotlb: Move alloc_size to swiotlb_find_slots)
  	for (i = index - 1;
  	     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 &&
 -	     mem->slots[i].list; i--)
 -		mem->slots[i].list = ++count;
 +	     io_tlb_list[i]; i--)
 +		io_tlb_list[i] = ++count;
  
  	/*
  	 * Update the indices to avoid searching in the next round.
@@@ -645,14 -546,12 +654,20 @@@ phys_addr_t swiotlb_tbl_map_single(stru
  	 * This is needed when we sync the memory.  Then we sync the buffer if
  	 * needed.
  	 */
++<<<<<<< HEAD
 +	for (i = 0; i < nr_slots(alloc_size + offset); i++) {
 +		io_tlb_orig_addr[index + i] = slot_addr(orig_addr, i);
 +		io_tlb_orig_size[index+i] = alloc_size - (i << IO_TLB_SHIFT);
 +	}
 +	tlb_addr = slot_addr(io_tlb_start, index) + offset;
++=======
+ 	for (i = 0; i < nr_slots(alloc_size + offset); i++)
+ 		mem->slots[index + i].orig_addr = slot_addr(orig_addr, i);
+ 	tlb_addr = slot_addr(mem->start, index) + offset;
++>>>>>>> 36f7b2f3ca5f (swiotlb: Move alloc_size to swiotlb_find_slots)
  	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
  	    (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
 -		swiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);
 +		swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_TO_DEVICE);
  	return tlb_addr;
  }
  
* Unmerged path kernel/dma/swiotlb.c
