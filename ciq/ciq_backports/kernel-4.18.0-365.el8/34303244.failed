kasan, mm: check kasan_enabled in annotations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 34303244f2615add92076a4bf2d4f39323bde4f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/34303244.failed

Declare the kasan_enabled static key in include/linux/kasan.h and in
include/linux/mm.h and check it in all kasan annotations. This allows to
avoid any slowdown caused by function calls when kasan_enabled is
disabled.

Link: https://lkml.kernel.org/r/9f90e3c0aa840dbb4833367c2335193299f69023.1606162397.git.andreyknvl@google.com
Link: https://linux-review.googlesource.com/id/I2589451d3c96c97abbcbf714baabe6161c6f153e
Co-developed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
	Signed-off-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Marco Elver <elver@google.com>
	Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
	Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Branislav Rankov <Branislav.Rankov@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Evgenii Stepanov <eugenis@google.com>
	Cc: Kevin Brodsky <kevin.brodsky@arm.com>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 34303244f2615add92076a4bf2d4f39323bde4f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	include/linux/mm.h
#	mm/kasan/common.c
diff --cc include/linux/kasan.h
index f00d17cf6822,9176849c4934..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -35,78 -59,189 +36,253 @@@ extern void kasan_enable_current(void)
  /* Disable reporting bugs for current task */
  extern void kasan_disable_current(void);
  
 -#else /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
 +void kasan_unpoison_shadow(const void *address, size_t size);
 +
++<<<<<<< HEAD
 +void kasan_unpoison_task_stack(struct task_struct *task);
  
 +void kasan_alloc_pages(struct page *page, unsigned int order);
 +void kasan_free_pages(struct page *page, unsigned int order);
 +
 +void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 +			slab_flags_t *flags);
 +
 +void kasan_poison_slab(struct page *page);
 +void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 +void kasan_poison_object_data(struct kmem_cache *cache, void *object);
 +void * __must_check kasan_init_slab_obj(struct kmem_cache *cache,
 +					const void *object);
 +
 +void * __must_check kasan_kmalloc_large(const void *ptr, size_t size,
 +						gfp_t flags);
 +void kasan_kfree_large(void *ptr, unsigned long ip);
 +void kasan_poison_kfree(void *ptr, unsigned long ip);
 +void * __must_check kasan_kmalloc(struct kmem_cache *s, const void *object,
 +					size_t size, gfp_t flags);
 +void * __must_check kasan_krealloc(const void *object, size_t new_size,
 +					gfp_t flags);
 +
 +void * __must_check kasan_slab_alloc(struct kmem_cache *s, void *object,
 +					gfp_t flags);
 +bool kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
++=======
+ static inline int kasan_add_zero_shadow(void *start, unsigned long size)
+ {
+ 	return 0;
+ }
+ static inline void kasan_remove_zero_shadow(void *start,
+ 					unsigned long size)
+ {}
+ 
+ static inline void kasan_enable_current(void) {}
+ static inline void kasan_disable_current(void) {}
+ 
+ #endif /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
+ 
+ #ifdef CONFIG_KASAN
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  
  struct kasan_cache {
  	int alloc_meta_offset;
  	int free_meta_offset;
  };
  
++<<<<<<< HEAD
 +/*
 + * These functions provide a special case to support backing module
 + * allocations with real shadow memory. With KASAN vmalloc, the special
 + * case is unnecessary, as the work is handled in the generic case.
 + */
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size);
 +void kasan_free_shadow(const struct vm_struct *vm);
 +#else
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +#endif
 +
 +int kasan_add_zero_shadow(void *start, unsigned long size);
 +void kasan_remove_zero_shadow(void *start, unsigned long size);
 +
 +size_t __ksize(const void *);
 +static inline void kasan_unpoison_slab(const void *ptr)
 +{
 +	kasan_unpoison_shadow(ptr, __ksize(ptr));
 +}
 +size_t kasan_metadata_size(struct kmem_cache *cache);
++=======
+ #ifdef CONFIG_KASAN_HW_TAGS
+ DECLARE_STATIC_KEY_FALSE(kasan_flag_enabled);
+ static __always_inline bool kasan_enabled(void)
+ {
+ 	return static_branch_likely(&kasan_flag_enabled);
+ }
+ #else
+ static inline bool kasan_enabled(void)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ void __kasan_unpoison_range(const void *addr, size_t size);
+ static __always_inline void kasan_unpoison_range(const void *addr, size_t size)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_unpoison_range(addr, size);
+ }
+ 
+ void __kasan_alloc_pages(struct page *page, unsigned int order);
+ static __always_inline void kasan_alloc_pages(struct page *page,
+ 						unsigned int order)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_alloc_pages(page, order);
+ }
+ 
+ void __kasan_free_pages(struct page *page, unsigned int order);
+ static __always_inline void kasan_free_pages(struct page *page,
+ 						unsigned int order)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_free_pages(page, order);
+ }
+ 
+ void __kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
+ 				slab_flags_t *flags);
+ static __always_inline void kasan_cache_create(struct kmem_cache *cache,
+ 				unsigned int *size, slab_flags_t *flags)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_cache_create(cache, size, flags);
+ }
+ 
+ size_t __kasan_metadata_size(struct kmem_cache *cache);
+ static __always_inline size_t kasan_metadata_size(struct kmem_cache *cache)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_metadata_size(cache);
+ 	return 0;
+ }
+ 
+ void __kasan_poison_slab(struct page *page);
+ static __always_inline void kasan_poison_slab(struct page *page)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_slab(page);
+ }
+ 
+ void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_unpoison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_unpoison_object_data(cache, object);
+ }
+ 
+ void __kasan_poison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_poison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_object_data(cache, object);
+ }
+ 
+ void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
+ 					  const void *object);
+ static __always_inline void * __must_check kasan_init_slab_obj(
+ 				struct kmem_cache *cache, const void *object)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_init_slab_obj(cache, object);
+ 	return (void *)object;
+ }
+ 
+ bool __kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
+ static __always_inline bool kasan_slab_free(struct kmem_cache *s, void *object,
+ 						unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_free(s, object, ip);
+ 	return false;
+ }
+ 
+ void * __must_check __kasan_slab_alloc(struct kmem_cache *s,
+ 				       void *object, gfp_t flags);
+ static __always_inline void * __must_check kasan_slab_alloc(
+ 				struct kmem_cache *s, void *object, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_alloc(s, object, flags);
+ 	return object;
+ }
+ 
+ void * __must_check __kasan_kmalloc(struct kmem_cache *s, const void *object,
+ 				    size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc(struct kmem_cache *s,
+ 				const void *object, size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc(s, object, size, flags);
+ 	return (void *)object;
+ }
+ 
+ void * __must_check __kasan_kmalloc_large(const void *ptr,
+ 					  size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc_large(const void *ptr,
+ 						      size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc_large(ptr, size, flags);
+ 	return (void *)ptr;
+ }
+ 
+ void * __must_check __kasan_krealloc(const void *object,
+ 				     size_t new_size, gfp_t flags);
+ static __always_inline void * __must_check kasan_krealloc(const void *object,
+ 						 size_t new_size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_krealloc(object, new_size, flags);
+ 	return (void *)object;
+ }
+ 
+ void __kasan_poison_kfree(void *ptr, unsigned long ip);
+ static __always_inline void kasan_poison_kfree(void *ptr, unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_kfree(ptr, ip);
+ }
+ 
+ void __kasan_kfree_large(void *ptr, unsigned long ip);
+ static __always_inline void kasan_kfree_large(void *ptr, unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_kfree_large(ptr, ip);
+ }
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  
  bool kasan_save_enable_multi_shot(void);
  void kasan_restore_multi_shot(bool enabled);
  
  #else /* CONFIG_KASAN */
  
++<<<<<<< HEAD
 +static inline void kasan_unpoison_shadow(const void *address, size_t size) {}
 +
 +static inline void kasan_unpoison_task_stack(struct task_struct *task) {}
 +
 +static inline void kasan_enable_current(void) {}
 +static inline void kasan_disable_current(void) {}
 +
++=======
+ static inline bool kasan_enabled(void)
+ {
+ 	return false;
+ }
+ static inline void kasan_unpoison_range(const void *address, size_t size) {}
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  static inline void kasan_alloc_pages(struct page *page, unsigned int order) {}
  static inline void kasan_free_pages(struct page *page, unsigned int order) {}
- 
  static inline void kasan_cache_create(struct kmem_cache *cache,
  				      unsigned int *size,
  				      slab_flags_t *flags) {}
@@@ -138,31 -280,8 +321,36 @@@ static inline void *kasan_krealloc(cons
  {
  	return (void *)object;
  }
++<<<<<<< HEAD
 +
 +static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
 +				   gfp_t flags)
 +{
 +	return object;
 +}
 +static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 +				   unsigned long ip)
 +{
 +	return false;
 +}
 +
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +
 +static inline int kasan_add_zero_shadow(void *start, unsigned long size)
 +{
 +	return 0;
 +}
 +static inline void kasan_remove_zero_shadow(void *start,
 +					unsigned long size)
 +{}
 +
 +static inline void kasan_unpoison_slab(const void *ptr) { }
 +static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
++=======
+ static inline void kasan_poison_kfree(void *ptr, unsigned long ip) {}
+ static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  
  #endif /* CONFIG_KASAN */
  
diff --cc include/linux/mm.h
index 3b01604de2d9,5299b90a6c40..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -29,8 -30,8 +29,13 @@@
  #include <linux/overflow.h>
  #include <linux/sizes.h>
  #include <linux/sched.h>
++<<<<<<< HEAD
 +
 +#include <linux/rh_kabi.h>
++=======
+ #include <linux/pgtable.h>
+ #include <linux/kasan.h>
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  
  struct mempolicy;
  struct anon_vma;
@@@ -1375,10 -1422,13 +1380,17 @@@ static inline bool cpupid_match_pid(str
  }
  #endif /* CONFIG_NUMA_BALANCING */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_KASAN_SW_TAGS
++=======
+ #if defined(CONFIG_KASAN_SW_TAGS) || defined(CONFIG_KASAN_HW_TAGS)
+ 
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  static inline u8 page_kasan_tag(const struct page *page)
  {
- 	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+ 	if (kasan_enabled())
+ 		return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+ 	return 0xff;
  }
  
  static inline void page_kasan_tag_set(struct page *page, u8 tag)
diff --cc mm/kasan/common.c
index e989322c0f3f,ae0130cf9de3..000000000000
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@@ -84,106 -56,20 +84,110 @@@ void kasan_disable_current(void
  {
  	current->kasan_depth--;
  }
 -#endif /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
  
++<<<<<<< HEAD
 +bool __kasan_check_read(const volatile void *p, unsigned int size)
++=======
+ void __kasan_unpoison_range(const void *address, size_t size)
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  {
 -	unpoison_range(address, size);
 +	return check_memory_region((unsigned long)p, size, false, _RET_IP_);
  }
 +EXPORT_SYMBOL(__kasan_check_read);
  
 -#if CONFIG_KASAN_STACK
 -/* Unpoison the entire stack for a task. */
 -void kasan_unpoison_task_stack(struct task_struct *task)
 +bool __kasan_check_write(const volatile void *p, unsigned int size)
 +{
 +	return check_memory_region((unsigned long)p, size, true, _RET_IP_);
 +}
 +EXPORT_SYMBOL(__kasan_check_write);
 +
 +#undef memset
 +void *memset(void *addr, int c, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)addr, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memset(addr, c, len);
 +}
 +
 +#ifdef __HAVE_ARCH_MEMMOVE
 +#undef memmove
 +void *memmove(void *dest, const void *src, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)src, len, false, _RET_IP_) ||
 +	    !check_memory_region((unsigned long)dest, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memmove(dest, src, len);
 +}
 +#endif
 +
 +#undef memcpy
 +void *memcpy(void *dest, const void *src, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)src, len, false, _RET_IP_) ||
 +	    !check_memory_region((unsigned long)dest, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memcpy(dest, src, len);
 +}
 +
 +/*
 + * Poisons the shadow memory for 'size' bytes starting from 'addr'.
 + * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
 + */
 +void kasan_poison_shadow(const void *address, size_t size, u8 value)
 +{
 +	void *shadow_start, *shadow_end;
 +
 +	/*
 +	 * Perform shadow offset calculation based on untagged address, as
 +	 * some of the callers (e.g. kasan_poison_object_data) pass tagged
 +	 * addresses to this function.
 +	 */
 +	address = reset_tag(address);
 +
 +	shadow_start = kasan_mem_to_shadow(address);
 +	shadow_end = kasan_mem_to_shadow(address + size);
 +
 +	__memset(shadow_start, value, shadow_end - shadow_start);
 +}
 +
 +void kasan_unpoison_shadow(const void *address, size_t size)
 +{
 +	u8 tag = get_tag(address);
 +
 +	/*
 +	 * Perform shadow offset calculation based on untagged address, as
 +	 * some of the callers (e.g. kasan_unpoison_object_data) pass tagged
 +	 * addresses to this function.
 +	 */
 +	address = reset_tag(address);
 +
 +	kasan_poison_shadow(address, size, tag);
 +
 +	if (size & KASAN_SHADOW_MASK) {
 +		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
 +
 +		if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
 +			*shadow = tag;
 +		else
 +			*shadow = size & KASAN_SHADOW_MASK;
 +	}
 +}
 +
 +static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
  {
  	void *base = task_stack_page(task);
 +	size_t size = sp - base;
  
 -	unpoison_range(base, THREAD_SIZE);
 +	kasan_unpoison_shadow(base, size);
 +}
 +
 +/* Unpoison the entire stack for a task. */
 +void kasan_unpoison_task_stack(struct task_struct *task)
 +{
 +	__kasan_unpoison_stack(task, task_stack_page(task) + THREAD_SIZE);
  }
  
  /* Unpoison the stack for the current task beyond a watermark sp value. */
@@@ -196,10 -82,11 +200,10 @@@ asmlinkage void kasan_unpoison_task_sta
  	 */
  	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
  
 -	unpoison_range(base, watermark - base);
 +	kasan_unpoison_shadow(base, watermark - base);
  }
 -#endif /* CONFIG_KASAN_STACK */
  
- void kasan_alloc_pages(struct page *page, unsigned int order)
+ void __kasan_alloc_pages(struct page *page, unsigned int order)
  {
  	u8 tag;
  	unsigned long i;
@@@ -210,13 -97,13 +214,13 @@@
  	tag = random_tag();
  	for (i = 0; i < (1 << order); i++)
  		page_kasan_tag_set(page + i, tag);
 -	unpoison_range(page_address(page), PAGE_SIZE << order);
 +	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
  }
  
- void kasan_free_pages(struct page *page, unsigned int order)
+ void __kasan_free_pages(struct page *page, unsigned int order)
  {
  	if (likely(!PageHighMem(page)))
 -		poison_range(page_address(page),
 +		kasan_poison_shadow(page_address(page),
  				PAGE_SIZE << order,
  				KASAN_FREE_PAGE);
  }
@@@ -281,46 -173,48 +285,46 @@@ void __kasan_cache_create(struct kmem_c
  	*flags |= SLAB_KASAN;
  }
  
- size_t kasan_metadata_size(struct kmem_cache *cache)
+ size_t __kasan_metadata_size(struct kmem_cache *cache)
  {
 -	if (!kasan_stack_collection_enabled())
 -		return 0;
  	return (cache->kasan_info.alloc_meta_offset ?
  		sizeof(struct kasan_alloc_meta) : 0) +
  		(cache->kasan_info.free_meta_offset ?
  		sizeof(struct kasan_free_meta) : 0);
  }
  
 -struct kasan_alloc_meta *kasan_get_alloc_meta(struct kmem_cache *cache,
 -					      const void *object)
 +struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
 +					const void *object)
  {
 -	return kasan_reset_tag(object) + cache->kasan_info.alloc_meta_offset;
 +	return (void *)object + cache->kasan_info.alloc_meta_offset;
  }
  
 -struct kasan_free_meta *kasan_get_free_meta(struct kmem_cache *cache,
 -					    const void *object)
 +struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 +				      const void *object)
  {
  	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
 -	return kasan_reset_tag(object) + cache->kasan_info.free_meta_offset;
 +	return (void *)object + cache->kasan_info.free_meta_offset;
  }
  
- void kasan_poison_slab(struct page *page)
+ void __kasan_poison_slab(struct page *page)
  {
  	unsigned long i;
  
 -	for (i = 0; i < compound_nr(page); i++)
 +	for (i = 0; i < (1 << compound_order(page)); i++)
  		page_kasan_tag_reset(page + i);
 -	poison_range(page_address(page), page_size(page),
 -		     KASAN_KMALLOC_REDZONE);
 +	kasan_poison_shadow(page_address(page), page_size(page),
 +			KASAN_KMALLOC_REDZONE);
  }
  
- void kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
+ void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
  {
 -	unpoison_range(object, cache->object_size);
 +	kasan_unpoison_shadow(object, cache->object_size);
  }
  
- void kasan_poison_object_data(struct kmem_cache *cache, void *object)
+ void __kasan_poison_object_data(struct kmem_cache *cache, void *object)
  {
 -	poison_range(object,
 -			round_up(cache->object_size, KASAN_GRANULE_SIZE),
 +	kasan_poison_shadow(object,
 +			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
  			KASAN_KMALLOC_REDZONE);
  }
  
@@@ -370,43 -264,28 +374,47 @@@ static u8 assign_tag(struct kmem_cache 
  #endif
  }
  
- void * __must_check kasan_init_slab_obj(struct kmem_cache *cache,
+ void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
  						const void *object)
  {
 -	struct kasan_alloc_meta *alloc_meta;
 +	struct kasan_alloc_meta *alloc_info;
  
 -	if (kasan_stack_collection_enabled()) {
 -		if (!(cache->flags & SLAB_KASAN))
 -			return (void *)object;
 +	if (!(cache->flags & SLAB_KASAN))
 +		return (void *)object;
  
 -		alloc_meta = kasan_get_alloc_meta(cache, object);
 -		__memset(alloc_meta, 0, sizeof(*alloc_meta));
 -	}
 +	alloc_info = get_alloc_info(cache, object);
 +	__memset(alloc_info, 0, sizeof(*alloc_info));
  
 -	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS) || IS_ENABLED(CONFIG_KASAN_HW_TAGS))
 -		object = set_tag(object, assign_tag(cache, object, true, false));
 +	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
 +		object = set_tag(object,
 +				assign_tag(cache, object, true, false));
  
  	return (void *)object;
  }
  
++<<<<<<< HEAD
 +static inline bool shadow_invalid(u8 tag, s8 shadow_byte)
 +{
 +	if (IS_ENABLED(CONFIG_KASAN_GENERIC))
 +		return shadow_byte < 0 ||
 +			shadow_byte >= KASAN_SHADOW_SCALE_SIZE;
 +
 +	/* else CONFIG_KASAN_SW_TAGS: */
 +	if ((u8)shadow_byte == KASAN_TAG_INVALID)
 +		return true;
 +	if ((tag != KASAN_TAG_KERNEL) && (tag != (u8)shadow_byte))
 +		return true;
 +
 +	return false;
 +}
 +
 +static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
++=======
+ static bool ____kasan_slab_free(struct kmem_cache *cache, void *object,
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  			      unsigned long ip, bool quarantine)
  {
 +	s8 shadow_byte;
  	u8 tag;
  	void *tagged_object;
  	unsigned long rounded_up_size;
@@@ -445,12 -326,17 +453,21 @@@
  	return IS_ENABLED(CONFIG_KASAN_GENERIC);
  }
  
- bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
+ bool __kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
  {
- 	return __kasan_slab_free(cache, object, ip, true);
+ 	return ____kasan_slab_free(cache, object, ip, true);
  }
  
++<<<<<<< HEAD
 +static void *__kasan_kmalloc(struct kmem_cache *cache, const void *object,
++=======
+ static void set_alloc_info(struct kmem_cache *cache, void *object, gfp_t flags)
+ {
+ 	kasan_set_track(&kasan_get_alloc_meta(cache, object)->alloc_track, flags);
+ }
+ 
+ static void *____kasan_kmalloc(struct kmem_cache *cache, const void *object,
++>>>>>>> 34303244f261 (kasan, mm: check kasan_enabled in annotations)
  				size_t size, gfp_t flags, bool keep_tag)
  {
  	unsigned long redzone_start;
@@@ -547,9 -433,9 +564,9 @@@ void __kasan_poison_kfree(void *ptr, un
  			kasan_report_invalid_free(ptr, ip);
  			return;
  		}
 -		poison_range(ptr, page_size(page), KASAN_FREE_PAGE);
 +		kasan_poison_shadow(ptr, page_size(page), KASAN_FREE_PAGE);
  	} else {
- 		__kasan_slab_free(page->slab_cache, ptr, ip, false);
+ 		____kasan_slab_free(page->slab_cache, ptr, ip, false);
  	}
  }
  
* Unmerged path include/linux/kasan.h
* Unmerged path include/linux/mm.h
* Unmerged path mm/kasan/common.c
