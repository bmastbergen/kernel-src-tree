kasan: move _RET_IP_ to inline wrappers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 027b37b552f326aa94ef06c7ea77088b16c41e6e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/027b37b5.failed

Generic mm functions that call KASAN annotations that might report a bug
pass _RET_IP_ to them as an argument. This allows KASAN to include the
name of the function that called the mm function in its report's header.

Now that KASAN has inline wrappers for all of its annotations, move
_RET_IP_ to those wrappers to simplify annotation call sites.

Link: https://linux-review.googlesource.com/id/I8fb3c06d49671305ee184175a39591bc26647a67
Link: https://lkml.kernel.org/r/5c1490eddf20b436b8c4eeea83fce47687d5e4a4.1610733117.git.andreyknvl@google.com
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Marco Elver <elver@google.com>
	Reviewed-by: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Branislav Rankov <Branislav.Rankov@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Evgenii Stepanov <eugenis@google.com>
	Cc: Kevin Brodsky <kevin.brodsky@arm.com>
	Cc: Peter Collingbourne <pcc@google.com>
	Cc: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 027b37b552f326aa94ef06c7ea77088b16c41e6e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	mm/mempool.c
diff --cc include/linux/kasan.h
index 71e7b6777f1b,a7254186558a..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -69,28 -85,166 +69,184 @@@ struct kasan_cache 
  	int free_meta_offset;
  };
  
 -#ifdef CONFIG_KASAN_HW_TAGS
 +/*
 + * These functions provide a special case to support backing module
 + * allocations with real shadow memory. With KASAN vmalloc, the special
 + * case is unnecessary, as the work is handled in the generic case.
 + */
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size);
 +void kasan_free_shadow(const struct vm_struct *vm);
 +#else
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +#endif
  
 -DECLARE_STATIC_KEY_FALSE(kasan_flag_enabled);
 +int kasan_add_zero_shadow(void *start, unsigned long size);
 +void kasan_remove_zero_shadow(void *start, unsigned long size);
  
 -static __always_inline bool kasan_enabled(void)
 +size_t __ksize(const void *);
 +static inline void kasan_unpoison_slab(const void *ptr)
  {
++<<<<<<< HEAD
 +	kasan_unpoison_shadow(ptr, __ksize(ptr));
++=======
+ 	return static_branch_likely(&kasan_flag_enabled);
+ }
+ 
+ #else /* CONFIG_KASAN_HW_TAGS */
+ 
+ static inline bool kasan_enabled(void)
+ {
+ 	return true;
+ }
+ 
+ #endif /* CONFIG_KASAN_HW_TAGS */
+ 
+ slab_flags_t __kasan_never_merge(void);
+ static __always_inline slab_flags_t kasan_never_merge(void)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_never_merge();
+ 	return 0;
+ }
+ 
+ void __kasan_unpoison_range(const void *addr, size_t size);
+ static __always_inline void kasan_unpoison_range(const void *addr, size_t size)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_unpoison_range(addr, size);
+ }
+ 
+ void __kasan_alloc_pages(struct page *page, unsigned int order);
+ static __always_inline void kasan_alloc_pages(struct page *page,
+ 						unsigned int order)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_alloc_pages(page, order);
+ }
+ 
+ void __kasan_free_pages(struct page *page, unsigned int order);
+ static __always_inline void kasan_free_pages(struct page *page,
+ 						unsigned int order)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_free_pages(page, order);
+ }
+ 
+ void __kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
+ 				slab_flags_t *flags);
+ static __always_inline void kasan_cache_create(struct kmem_cache *cache,
+ 				unsigned int *size, slab_flags_t *flags)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_cache_create(cache, size, flags);
+ }
+ 
+ size_t __kasan_metadata_size(struct kmem_cache *cache);
+ static __always_inline size_t kasan_metadata_size(struct kmem_cache *cache)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_metadata_size(cache);
+ 	return 0;
+ }
+ 
+ void __kasan_poison_slab(struct page *page);
+ static __always_inline void kasan_poison_slab(struct page *page)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_slab(page);
+ }
+ 
+ void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_unpoison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_unpoison_object_data(cache, object);
+ }
+ 
+ void __kasan_poison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_poison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_object_data(cache, object);
+ }
+ 
+ void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
+ 					  const void *object);
+ static __always_inline void * __must_check kasan_init_slab_obj(
+ 				struct kmem_cache *cache, const void *object)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_init_slab_obj(cache, object);
+ 	return (void *)object;
+ }
+ 
+ bool __kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
+ static __always_inline bool kasan_slab_free(struct kmem_cache *s, void *object)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_free(s, object, _RET_IP_);
+ 	return false;
+ }
+ 
+ void __kasan_slab_free_mempool(void *ptr, unsigned long ip);
+ static __always_inline void kasan_slab_free_mempool(void *ptr)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_slab_free_mempool(ptr, _RET_IP_);
+ }
+ 
+ void * __must_check __kasan_slab_alloc(struct kmem_cache *s,
+ 				       void *object, gfp_t flags);
+ static __always_inline void * __must_check kasan_slab_alloc(
+ 				struct kmem_cache *s, void *object, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_alloc(s, object, flags);
+ 	return object;
+ }
+ 
+ void * __must_check __kasan_kmalloc(struct kmem_cache *s, const void *object,
+ 				    size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc(struct kmem_cache *s,
+ 				const void *object, size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc(s, object, size, flags);
+ 	return (void *)object;
+ }
+ 
+ void * __must_check __kasan_kmalloc_large(const void *ptr,
+ 					  size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc_large(const void *ptr,
+ 						      size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc_large(ptr, size, flags);
+ 	return (void *)ptr;
+ }
+ 
+ void * __must_check __kasan_krealloc(const void *object,
+ 				     size_t new_size, gfp_t flags);
+ static __always_inline void * __must_check kasan_krealloc(const void *object,
+ 						 size_t new_size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_krealloc(object, new_size, flags);
+ 	return (void *)object;
+ }
+ 
+ void __kasan_kfree_large(void *ptr, unsigned long ip);
+ static __always_inline void kasan_kfree_large(void *ptr)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_kfree_large(ptr, _RET_IP_);
++>>>>>>> 027b37b552f3 (kasan: move _RET_IP_ to inline wrappers)
  }
 +size_t kasan_metadata_size(struct kmem_cache *cache);
  
  bool kasan_save_enable_multi_shot(void);
  void kasan_restore_multi_shot(bool enabled);
@@@ -121,13 -276,16 +277,25 @@@ static inline void *kasan_init_slab_obj
  {
  	return (void *)object;
  }
++<<<<<<< HEAD
 +
 +static inline void *kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags)
 +{
 +	return ptr;
++=======
+ static inline bool kasan_slab_free(struct kmem_cache *s, void *object)
+ {
+ 	return false;
+ }
+ static inline void kasan_slab_free_mempool(void *ptr) {}
+ static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
+ 				   gfp_t flags)
+ {
+ 	return object;
++>>>>>>> 027b37b552f3 (kasan: move _RET_IP_ to inline wrappers)
  }
 +static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
 +static inline void kasan_poison_kfree(void *ptr, unsigned long ip) {}
  static inline void *kasan_kmalloc(struct kmem_cache *s, const void *object,
  				size_t size, gfp_t flags)
  {
@@@ -138,31 -300,7 +306,35 @@@ static inline void *kasan_krealloc(cons
  {
  	return (void *)object;
  }
++<<<<<<< HEAD
 +
 +static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
 +				   gfp_t flags)
 +{
 +	return object;
 +}
 +static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 +				   unsigned long ip)
 +{
 +	return false;
 +}
 +
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +
 +static inline int kasan_add_zero_shadow(void *start, unsigned long size)
 +{
 +	return 0;
 +}
 +static inline void kasan_remove_zero_shadow(void *start,
 +					unsigned long size)
 +{}
 +
 +static inline void kasan_unpoison_slab(const void *ptr) { }
 +static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
++=======
+ static inline void kasan_kfree_large(void *ptr) {}
++>>>>>>> 027b37b552f3 (kasan: move _RET_IP_ to inline wrappers)
  
  #endif /* CONFIG_KASAN */
  
diff --cc mm/mempool.c
index 6bfbd4d73ec5,79959fac27d7..000000000000
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@@ -106,8 -104,8 +106,13 @@@ static inline void poison_element(mempo
  static __always_inline void kasan_poison_element(mempool_t *pool, void *element)
  {
  	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
++<<<<<<< HEAD
 +		kasan_poison_kfree(element, _RET_IP_);
 +	if (pool->alloc == mempool_alloc_pages)
++=======
+ 		kasan_slab_free_mempool(element);
+ 	else if (pool->alloc == mempool_alloc_pages)
++>>>>>>> 027b37b552f3 (kasan: move _RET_IP_ to inline wrappers)
  		kasan_free_pages(element, (unsigned long)pool->pool_data);
  }
  
* Unmerged path include/linux/kasan.h
* Unmerged path mm/mempool.c
diff --git a/mm/slab.c b/mm/slab.c
index b1310abca889..6fc25c913ad7 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -3449,7 +3449,7 @@ static __always_inline void __cache_free(struct kmem_cache *cachep, void *objp,
 					 unsigned long caller)
 {
 	/* Put the object into the quarantine, don't touch it for now. */
-	if (kasan_slab_free(cachep, objp, _RET_IP_))
+	if (kasan_slab_free(cachep, objp))
 		return;
 
 	___cache_free(cachep, objp, caller);
diff --git a/mm/slub.c b/mm/slub.c
index f75fc1d87a77..5d2e69fa72b9 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1616,7 +1616,7 @@ static inline void *kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
 static __always_inline void kfree_hook(void *x)
 {
 	kmemleak_free(x);
-	kasan_kfree_large(x, _RET_IP_);
+	kasan_kfree_large(x);
 }
 
 static __always_inline bool slab_free_hook(struct kmem_cache *s, void *x)
@@ -1629,7 +1629,7 @@ static __always_inline bool slab_free_hook(struct kmem_cache *s, void *x)
 		debug_check_no_obj_freed(x, s->object_size);
 
 	/* KASAN might put x into memory quarantine, delaying its reuse */
-	return kasan_slab_free(s, x, _RET_IP_);
+	return kasan_slab_free(s, x);
 }
 
 static inline bool slab_free_freelist_hook(struct kmem_cache *s,
