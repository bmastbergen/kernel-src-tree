md/raid5: avoid redundant bio clone in raid5_read_one_chunk

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Guoqing Jiang <jgq516@gmail.com>
commit 1147f58e1010b8688bac1fd3bbab753b1379291d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/1147f58e.failed

After enable io accounting, chunk read bio could be cloned twice which
is not good. To avoid such inefficiency, let's clone align_bio from
io_acct_set too, then we need only call md_account_bio in make_request
unconditionally.

	Signed-off-by: Guoqing Jiang <jiangguoqing@kylinos.cn>
	Signed-off-by: Song Liu <song@kernel.org>
(cherry picked from commit 1147f58e1010b8688bac1fd3bbab753b1379291d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5.c
diff --cc drivers/md/raid5.c
index 08041f68d898,f83623ac8c34..000000000000
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@@ -5423,10 -5398,11 +5427,16 @@@ static void raid5_align_endio(struct bi
  static int raid5_read_one_chunk(struct mddev *mddev, struct bio *raid_bio)
  {
  	struct r5conf *conf = mddev->private;
 -	struct bio *align_bio;
 +	int dd_idx;
 +	struct bio* align_bi;
  	struct md_rdev *rdev;
++<<<<<<< HEAD
 +	sector_t end_sector;
++=======
+ 	sector_t sector, end_sector, first_bad;
+ 	int bad_sectors, dd_idx;
+ 	struct md_io_acct *md_io_acct;
++>>>>>>> 1147f58e1010 (md/raid5: avoid redundant bio clone in raid5_read_one_chunk)
  
  	if (!in_chunk_boundary(mddev, raid_bio)) {
  		pr_debug("%s: non aligned\n", __func__);
@@@ -5470,45 -5439,36 +5480,62 @@@
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	if (rdev) {
 +		sector_t first_bad;
 +		int bad_sectors;
 +
 +		atomic_inc(&rdev->nr_pending);
 +		rcu_read_unlock();
 +		raid_bio->bi_next = (void*)rdev;
 +		bio_set_dev(align_bi, rdev->bdev);
 +		bio_clear_flag(align_bi, BIO_SEG_VALID);
 +
 +		if (is_badblock(rdev, align_bi->bi_iter.bi_sector,
 +				bio_sectors(align_bi),
 +				&first_bad, &bad_sectors)) {
 +			bio_put(align_bi);
 +			rdev_dec_pending(rdev, mddev);
 +			return 0;
 +		}
++=======
+ 	align_bio = bio_clone_fast(raid_bio, GFP_NOIO, &mddev->io_acct_set);
+ 	md_io_acct = container_of(align_bio, struct md_io_acct, bio_clone);
+ 	raid_bio->bi_next = (void *)rdev;
+ 	if (blk_queue_io_stat(raid_bio->bi_bdev->bd_disk->queue))
+ 		md_io_acct->start_time = bio_start_io_acct(raid_bio);
+ 	md_io_acct->orig_bio = raid_bio;
+ 
+ 	bio_set_dev(align_bio, rdev->bdev);
+ 	align_bio->bi_end_io = raid5_align_endio;
+ 	align_bio->bi_private = md_io_acct;
+ 	align_bio->bi_iter.bi_sector = sector;
+ 
+ 	/* No reshape active, so we can trust rdev->data_offset */
+ 	align_bio->bi_iter.bi_sector += rdev->data_offset;
++>>>>>>> 1147f58e1010 (md/raid5: avoid redundant bio clone in raid5_read_one_chunk)
  
 -	spin_lock_irq(&conf->device_lock);
 -	wait_event_lock_irq(conf->wait_for_quiescent, conf->quiesce == 0,
 -			    conf->device_lock);
 -	atomic_inc(&conf->active_aligned_reads);
 -	spin_unlock_irq(&conf->device_lock);
 +		/* No reshape active, so we can trust rdev->data_offset */
 +		align_bi->bi_iter.bi_sector += rdev->data_offset;
  
 -	if (mddev->gendisk)
 -		trace_block_bio_remap(align_bio, disk_devt(mddev->gendisk),
 -				      raid_bio->bi_iter.bi_sector);
 -	submit_bio_noacct(align_bio);
 -	return 1;
 +		spin_lock_irq(&conf->device_lock);
 +		wait_event_lock_irq(conf->wait_for_quiescent,
 +				    conf->quiesce == 0,
 +				    conf->device_lock);
 +		atomic_inc(&conf->active_aligned_reads);
 +		spin_unlock_irq(&conf->device_lock);
  
 -out_rcu_unlock:
 -	rcu_read_unlock();
 -	return 0;
 +		if (mddev->gendisk)
 +			trace_block_bio_remap(align_bi->bi_disk->queue,
 +					      align_bi, disk_devt(mddev->gendisk),
 +					      raid_bio->bi_iter.bi_sector);
 +		generic_make_request(align_bi);
 +		return 1;
 +	} else {
 +		rcu_read_unlock();
 +		bio_put(align_bi);
 +		return 0;
 +	}
  }
  
  static struct bio *chunk_aligned_read(struct mddev *mddev, struct bio *raid_bio)
@@@ -5847,6 -5807,7 +5874,10 @@@ static bool raid5_make_request(struct m
  	last_sector = bio_end_sector(bi);
  	bi->bi_next = NULL;
  
++<<<<<<< HEAD
++=======
+ 	md_account_bio(mddev, &bi);
++>>>>>>> 1147f58e1010 (md/raid5: avoid redundant bio clone in raid5_read_one_chunk)
  	prepare_to_wait(&conf->wait_for_overlap, &w, TASK_UNINTERRUPTIBLE);
  	for (; logical_sector < last_sector; logical_sector += RAID5_STRIPE_SECTORS(conf)) {
  		int previous;
* Unmerged path drivers/md/raid5.c
