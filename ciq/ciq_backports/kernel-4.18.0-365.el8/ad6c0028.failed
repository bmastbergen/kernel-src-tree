swiotlb: Free tbl memory in swiotlb_exit()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Will Deacon <will@kernel.org>
commit ad6c00283163cb7ad52cdf97d2850547446f7d98
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/ad6c0028.failed

Although swiotlb_exit() frees the 'slots' metadata array referenced by
'io_tlb_default_mem', it leaves the underlying buffer pages allocated
despite no longer being usable.

Extend swiotlb_exit() to free the buffer pages as well as the slots
array.

	Cc: Claire Chang <tientzu@chromium.org>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Robin Murphy <robin.murphy@arm.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Tested-by: Nathan Chancellor <nathan@kernel.org>
	Tested-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Will Deacon <will@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad@kernel.org>
(cherry picked from commit ad6c00283163cb7ad52cdf97d2850547446f7d98)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,87c40517e822..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -439,29 -328,36 +439,62 @@@ cleanup3
  
  void __init swiotlb_exit(void)
  {
++<<<<<<< HEAD
 +	if (!io_tlb_orig_addr)
 +		return;
 +
 +	if (late_alloc) {
 +		free_pages((unsigned long)io_tlb_orig_size,
 +			   get_order(io_tlb_nslabs * sizeof(size_t)));
 +		free_pages((unsigned long)io_tlb_orig_addr,
 +			   get_order(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		free_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *
 +								 sizeof(int)));
 +		free_pages((unsigned long)phys_to_virt(io_tlb_start),
 +			   get_order(io_tlb_nslabs << IO_TLB_SHIFT));
 +	} else {
 +		memblock_free_late(__pa(io_tlb_orig_addr),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		memblock_free_late(__pa(io_tlb_orig_size),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(size_t)));
 +		memblock_free_late(__pa(io_tlb_list),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(int)));
 +		memblock_free_late(io_tlb_start,
 +				   PAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));
 +	}
 +	swiotlb_cleanup();
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 	unsigned long tbl_vaddr;
+ 	size_t tbl_size, slots_size;
+ 
+ 	if (!mem->nslabs)
+ 		return;
+ 
+ 	pr_info("tearing down default memory pool\n");
+ 	tbl_vaddr = (unsigned long)phys_to_virt(mem->start);
+ 	tbl_size = PAGE_ALIGN(mem->end - mem->start);
+ 	slots_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), mem->nslabs));
+ 
+ 	set_memory_encrypted(tbl_vaddr, tbl_size >> PAGE_SHIFT);
+ 	if (mem->late_alloc) {
+ 		free_pages(tbl_vaddr, get_order(tbl_size));
+ 		free_pages((unsigned long)mem->slots, get_order(slots_size));
+ 	} else {
+ 		memblock_free_late(mem->start, tbl_size);
+ 		memblock_free_late(__pa(mem->slots), slots_size);
+ 	}
+ 
+ 	memset(mem, 0, sizeof(*mem));
+ }
+ 
+ /*
+  * Return the offset into a iotlb slot required to keep the device happy.
+  */
+ static unsigned int swiotlb_align_offset(struct device *dev, u64 addr)
+ {
+ 	return addr & dma_get_min_align_mask(dev) & (IO_TLB_SIZE - 1);
++>>>>>>> ad6c00283163 (swiotlb: Free tbl memory in swiotlb_exit())
  }
  
  /*
* Unmerged path kernel/dma/swiotlb.c
