arm64: Fix kernel address detection of __is_lm_address()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Vincenzo Frascino <vincenzo.frascino@arm.com>
commit 519ea6f1c82fcdc9842908155ae379de47818778
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/519ea6f1.failed

Currently, the __is_lm_address() check just masks out the top 12 bits
of the address, but if they are 0, it still yields a true result.
This has as a side effect that virt_addr_valid() returns true even for
invalid virtual addresses (e.g. 0x0).

Fix the detection checking that it's actually a kernel address starting
at PAGE_OFFSET.

Fixes: 68dd8ef32162 ("arm64: memory: Fix virt_addr_valid() using __is_lm_address()")
	Cc: <stable@vger.kernel.org> # 5.4.x
	Cc: Will Deacon <will@kernel.org>
	Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Link: https://lore.kernel.org/r/20210126134056.45747-1-vincenzo.frascino@arm.com
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit 519ea6f1c82fcdc9842908155ae379de47818778)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/memory.h
diff --cc arch/arm64/include/asm/memory.h
index 630eaf618e50,99d7e1494aaa..000000000000
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@@ -268,11 -247,11 +268,19 @@@ static inline const void *__tag_set(con
  
  
  /*
++<<<<<<< HEAD
 + * The linear kernel range starts in the middle of the virtual adddress
 + * space. Testing the top bit for the start of the region is a
 + * sufficient check.
 + */
 +#define __is_lm_address(addr)	(!((addr) & BIT(VA_BITS - 1)))
++=======
+  * Check whether an arbitrary address is within the linear map, which
+  * lives in the [PAGE_OFFSET, PAGE_END) interval at the bottom of the
+  * kernel's TTBR1 address range.
+  */
+ #define __is_lm_address(addr)	(((u64)(addr) ^ PAGE_OFFSET) < (PAGE_END - PAGE_OFFSET))
++>>>>>>> 519ea6f1c82f (arm64: Fix kernel address detection of __is_lm_address())
  
  #define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
  #define __kimg_to_phys(addr)	((addr) - kimage_voffset)
* Unmerged path arch/arm64/include/asm/memory.h
