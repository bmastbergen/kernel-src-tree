x86/fpu: Move xstate size to fpu_*_cfg

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 2bd264bce238cedbf00bde1f28ad51ba45b9114e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/2bd264bc.failed

Use the new kernel and user space config storage to store and retrieve the
XSTATE buffer sizes. The default and the maximum size are the same for now,
but will change when support for dynamically enabled features is added.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20211014230739.296830097@linutronix.de
(cherry picked from commit 2bd264bce238cedbf00bde1f28ad51ba45b9114e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/core.c
#	arch/x86/kernel/fpu/init.c
#	arch/x86/kernel/fpu/internal.h
#	arch/x86/kernel/fpu/signal.c
#	arch/x86/kernel/fpu/xstate.c
#	arch/x86/kernel/fpu/xstate.h
diff --cc arch/x86/kernel/fpu/core.c
index 2859edb41245,69abf3a2299d..000000000000
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@@ -285,16 -298,16 +285,16 @@@ void fpu_sync_fpstate(struct fpu *fpu
  static inline unsigned int init_fpstate_copy_size(void)
  {
  	if (!use_xsave())
- 		return fpu_kernel_xstate_size;
+ 		return fpu_kernel_cfg.default_size;
  
  	/* XSAVE(S) just needs the legacy and the xstate header part */
 -	return sizeof(init_fpstate.regs.xsave);
 +	return sizeof(init_fpstate.xsave);
  }
  
 -static inline void fpstate_init_fxstate(struct fpstate *fpstate)
 +static inline void fpstate_init_fxstate(struct fxregs_state *fx)
  {
 -	fpstate->regs.fxsave.cwd = 0x37f;
 -	fpstate->regs.fxsave.mxcsr = MXCSR_DEFAULT;
 +	fx->cwd = 0x37f;
 +	fx->mxcsr = MXCSR_DEFAULT;
  }
  
  /*
@@@ -320,12 -333,24 +320,28 @@@ void fpstate_init_user(union fpregs_sta
  		return;
  	}
  
 -	xstate_init_xcomp_bv(&fpstate->regs.xsave, xfeatures_mask_uabi());
 +	xstate_init_xcomp_bv(&state->xsave, xfeatures_mask_uabi());
  
  	if (cpu_feature_enabled(X86_FEATURE_FXSR))
 -		fpstate_init_fxstate(fpstate);
 +		fpstate_init_fxstate(&state->fxsave);
  	else
++<<<<<<< HEAD
 +		fpstate_init_fstate(&state->fsave);
++=======
+ 		fpstate_init_fstate(fpstate);
+ }
+ 
+ void fpstate_reset(struct fpu *fpu)
+ {
+ 	/* Set the fpstate pointer to the default fpstate */
+ 	fpu->fpstate = &fpu->__fpstate;
+ 
+ 	/* Initialize sizes and feature masks */
+ 	fpu->fpstate->size		= fpu_kernel_cfg.default_size;
+ 	fpu->fpstate->user_size		= fpu_user_cfg.default_size;
+ 	fpu->fpstate->xfeatures		= xfeatures_mask_all;
+ 	fpu->fpstate->user_xfeatures	= xfeatures_mask_uabi();
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  }
  
  #if IS_ENABLED(CONFIG_KVM)
@@@ -385,6 -414,16 +401,19 @@@ int fpu_clone(struct task_struct *dst
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Whitelist the FPU register state embedded into task_struct for hardened
+  * usercopy.
+  */
+ void fpu_thread_struct_whitelist(unsigned long *offset, unsigned long *size)
+ {
+ 	*offset = offsetof(struct thread_struct, fpu.__fpstate.regs);
+ 	*size = fpu_kernel_cfg.default_size;
+ }
+ 
+ /*
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
   * Drops current FPU state: deactivates the fpregs and
   * the fpstate. NOTE: it still leaves previous contents
   * in the fpregs in the eager-FPU case.
diff --cc arch/x86/kernel/fpu/init.c
index f076729aabd0,58043ed08662..000000000000
--- a/arch/x86/kernel/fpu/init.c
+++ b/arch/x86/kernel/fpu/init.c
@@@ -193,18 -187,31 +185,35 @@@ static void __init fpu__init_task_struc
   */
  static void __init fpu__init_system_xstate_size_legacy(void)
  {
+ 	unsigned int size;
+ 
  	/*
- 	 * Note that xstate sizes might be overwritten later during
- 	 * fpu__init_system_xstate().
+ 	 * Note that the size configuration might be overwritten later
+ 	 * during fpu__init_system_xstate().
  	 */
  	if (!cpu_feature_enabled(X86_FEATURE_FPU))
- 		fpu_kernel_xstate_size = sizeof(struct swregs_state);
+ 		size = sizeof(struct swregs_state);
  	else if (cpu_feature_enabled(X86_FEATURE_FXSR))
- 		fpu_kernel_xstate_size = sizeof(struct fxregs_state);
+ 		size = sizeof(struct fxregs_state);
  	else
- 		fpu_kernel_xstate_size = sizeof(struct fregs_state);
+ 		size = sizeof(struct fregs_state);
  
++<<<<<<< HEAD
 +	fpu_user_xstate_size = fpu_kernel_xstate_size;
++=======
+ 	fpu_kernel_cfg.max_size = size;
+ 	fpu_kernel_cfg.default_size = size;
+ 	fpu_user_cfg.max_size = size;
+ 	fpu_user_cfg.default_size = size;
+ 	fpstate_reset(&current->thread.fpu);
+ }
+ 
+ static void __init fpu__init_init_fpstate(void)
+ {
+ 	/* Bring init_fpstate size and features up to date */
+ 	init_fpstate.size		= fpu_kernel_cfg.max_size;
+ 	init_fpstate.xfeatures		= xfeatures_mask_all;
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  }
  
  /*
@@@ -223,6 -231,7 +232,6 @@@ void __init fpu__init_system(struct cpu
  
  	fpu__init_system_generic();
  	fpu__init_system_xstate_size_legacy();
- 	fpu__init_system_xstate();
+ 	fpu__init_system_xstate(fpu_kernel_cfg.max_size);
  	fpu__init_task_struct_size();
 -	fpu__init_init_fpstate();
  }
diff --cc arch/x86/kernel/fpu/internal.h
index 5ddc09e03c2a,e1d8a352f12d..000000000000
--- a/arch/x86/kernel/fpu/internal.h
+++ b/arch/x86/kernel/fpu/internal.h
@@@ -2,6 -2,8 +2,11 @@@
  #ifndef __X86_KERNEL_FPU_INTERNAL_H
  #define __X86_KERNEL_FPU_INTERNAL_H
  
++<<<<<<< HEAD
++=======
+ extern struct fpstate init_fpstate;
+ 
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  /* CPU feature check wrappers */
  static __always_inline __pure bool use_xsave(void)
  {
diff --cc arch/x86/kernel/fpu/signal.c
index f74c29985497,fab440369663..000000000000
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@@ -492,9 -501,12 +492,16 @@@ fpu__alloc_mathframe(unsigned long sp, 
  	return sp;
  }
  
 -unsigned long __init fpu__get_fpstate_size(void)
 +unsigned long fpu__get_fpstate_size(void)
  {
++<<<<<<< HEAD
 +	unsigned long ret = xstate_sigframe_size();
++=======
+ 	unsigned long ret = fpu_user_cfg.max_size;
+ 
+ 	if (use_xsave())
+ 		ret += FP_XSTATE_MAGIC2_SIZE;
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  
  	/*
  	 * This space is needed on (most) 32-bit kernels, or when a 32-bit
diff --cc arch/x86/kernel/fpu/xstate.c
index bffa08a391ba,94f5e3739ae0..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -729,6 -727,14 +725,17 @@@ static void __init fpu__init_disable_sy
  	xfeatures_mask_all = 0;
  	cr4_clear_bits(X86_CR4_OSXSAVE);
  	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
++<<<<<<< HEAD
++=======
+ 
+ 	/* Restore the legacy size.*/
+ 	fpu_kernel_cfg.max_size = legacy_size;
+ 	fpu_kernel_cfg.default_size = legacy_size;
+ 	fpu_user_cfg.max_size = legacy_size;
+ 	fpu_user_cfg.default_size = legacy_size;
+ 
+ 	fpstate_reset(&current->thread.fpu);
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  }
  
  /*
diff --cc arch/x86/kernel/fpu/xstate.h
index 0789a04ee705,3d45eb04471b..000000000000
--- a/arch/x86/kernel/fpu/xstate.h
+++ b/arch/x86/kernel/fpu/xstate.h
@@@ -15,4 -15,198 +15,201 @@@ static inline void xstate_init_xcomp_bv
  		xsave->header.xcomp_bv = mask | XCOMP_BV_COMPACTED_FORMAT;
  }
  
++<<<<<<< HEAD
++=======
+ enum xstate_copy_mode {
+ 	XSTATE_COPY_FP,
+ 	XSTATE_COPY_FX,
+ 	XSTATE_COPY_XSAVE,
+ };
+ 
+ struct membuf;
+ extern void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
+ 				      u32 pkru_val, enum xstate_copy_mode copy_mode);
+ extern void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 				    enum xstate_copy_mode mode);
+ extern int copy_uabi_from_kernel_to_xstate(struct fpstate *fpstate, const void *kbuf);
+ extern int copy_sigframe_from_user_to_xstate(struct fpstate *fpstate, const void __user *ubuf);
+ 
+ 
+ extern void fpu__init_cpu_xstate(void);
+ extern void fpu__init_system_xstate(unsigned int legacy_size);
+ 
+ extern void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);
+ 
+ /* XSAVE/XRSTOR wrapper functions */
+ 
+ #ifdef CONFIG_X86_64
+ #define REX_PREFIX	"0x48, "
+ #else
+ #define REX_PREFIX
+ #endif
+ 
+ /* These macros all use (%edi)/(%rdi) as the single memory argument. */
+ #define XSAVE		".byte " REX_PREFIX "0x0f,0xae,0x27"
+ #define XSAVEOPT	".byte " REX_PREFIX "0x0f,0xae,0x37"
+ #define XSAVES		".byte " REX_PREFIX "0x0f,0xc7,0x2f"
+ #define XRSTOR		".byte " REX_PREFIX "0x0f,0xae,0x2f"
+ #define XRSTORS		".byte " REX_PREFIX "0x0f,0xc7,0x1f"
+ 
+ /*
+  * After this @err contains 0 on success or the trap number when the
+  * operation raises an exception.
+  */
+ #define XSTATE_OP(op, st, lmask, hmask, err)				\
+ 	asm volatile("1:" op "\n\t"					\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "2:\n\t"						\
+ 		     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_FAULT_MCE_SAFE)	\
+ 		     : [err] "=a" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * If XSAVES is enabled, it replaces XSAVEOPT because it supports a compact
+  * format and supervisor states in addition to modified optimization in
+  * XSAVEOPT.
+  *
+  * Otherwise, if XSAVEOPT is enabled, XSAVEOPT replaces XSAVE because XSAVEOPT
+  * supports modified optimization which is not supported by XSAVE.
+  *
+  * We use XSAVE as a fallback.
+  *
+  * The 661 label is defined in the ALTERNATIVE* macros as the address of the
+  * original instruction which gets replaced. We need to use it here as the
+  * address of the instruction where we might get an exception at.
+  */
+ #define XSTATE_XSAVE(st, lmask, hmask, err)				\
+ 	asm volatile(ALTERNATIVE_2(XSAVE,				\
+ 				   XSAVEOPT, X86_FEATURE_XSAVEOPT,	\
+ 				   XSAVES,   X86_FEATURE_XSAVES)	\
+ 		     "\n"						\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "3:\n"						\
+ 		     ".pushsection .fixup,\"ax\"\n"			\
+ 		     "4: movl $-2, %[err]\n"				\
+ 		     "jmp 3b\n"						\
+ 		     ".popsection\n"					\
+ 		     _ASM_EXTABLE(661b, 4b)				\
+ 		     : [err] "=r" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact
+  * XSAVE area format.
+  */
+ #define XSTATE_XRESTORE(st, lmask, hmask)				\
+ 	asm volatile(ALTERNATIVE(XRSTOR,				\
+ 				 XRSTORS, X86_FEATURE_XSAVES)		\
+ 		     "\n"						\
+ 		     "3:\n"						\
+ 		     _ASM_EXTABLE_TYPE(661b, 3b, EX_TYPE_FPU_RESTORE)	\
+ 		     :							\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * Save processor xstate to xsave area.
+  *
+  * Uses either XSAVE or XSAVEOPT or XSAVES depending on the CPU features
+  * and command line options. The choice is permanent until the next reboot.
+  */
+ static inline void os_xsave(struct fpstate *fpstate)
+ {
+ 	u64 mask = fpstate->xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	WARN_ON_FPU(!alternatives_patched);
+ 
+ 	XSTATE_XSAVE(&fpstate->regs.xsave, lmask, hmask, err);
+ 
+ 	/* We should never fault when copying to a kernel buffer: */
+ 	WARN_ON_FPU(err);
+ }
+ 
+ /*
+  * Restore processor xstate from xsave area.
+  *
+  * Uses XRSTORS when XSAVES is used, XRSTOR otherwise.
+  */
+ static inline void os_xrstor(struct xregs_state *xstate, u64 mask)
+ {
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 
+ 	XSTATE_XRESTORE(xstate, lmask, hmask);
+ }
+ 
+ /*
+  * Save xstate to user space xsave area.
+  *
+  * We don't use modified optimization because xrstor/xrstors might track
+  * a different application.
+  *
+  * We don't use compacted format xsave area for backward compatibility for
+  * old applications which don't understand the compacted format of the
+  * xsave area.
+  *
+  * The caller has to zero buf::header before calling this because XSAVE*
+  * does not touch the reserved fields in the header.
+  */
+ static inline int xsave_to_user_sigframe(struct xregs_state __user *buf)
+ {
+ 	/*
+ 	 * Include the features which are not xsaved/rstored by the kernel
+ 	 * internally, e.g. PKRU. That's user space ABI and also required
+ 	 * to allow the signal handler to modify PKRU.
+ 	 */
+ 	u64 mask = current->thread.fpu.fpstate->user_xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	stac();
+ 	XSTATE_OP(XSAVE, buf, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from user space xsave area.
+  */
+ static inline int xrstor_from_user_sigframe(struct xregs_state __user *buf, u64 mask)
+ {
+ 	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	stac();
+ 	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from kernel space xsave area, return an error code instead of
+  * an exception.
+  */
+ static inline int os_xrstor_safe(struct xregs_state *xstate, u64 mask)
+ {
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_XSAVES))
+ 		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
+ 	else
+ 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 
+ 	return err;
+ }
+ 
+ 
++>>>>>>> 2bd264bce238 (x86/fpu: Move xstate size to fpu_*_cfg)
  #endif
* Unmerged path arch/x86/kernel/fpu/core.c
* Unmerged path arch/x86/kernel/fpu/init.c
* Unmerged path arch/x86/kernel/fpu/internal.h
diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
index 843a02b1037d..aefae69fe165 100644
--- a/arch/x86/kernel/fpu/regset.c
+++ b/arch/x86/kernel/fpu/regset.c
@@ -152,7 +152,7 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
 	/*
 	 * A whole standard-format XSAVE buffer is needed:
 	 */
-	if (pos != 0 || count != fpu_user_xstate_size)
+	if (pos != 0 || count != fpu_user_cfg.max_size)
 		return -EFAULT;
 
 	if (!kbuf) {
* Unmerged path arch/x86/kernel/fpu/signal.c
* Unmerged path arch/x86/kernel/fpu/xstate.c
* Unmerged path arch/x86/kernel/fpu/xstate.h
