swiotlb: move orig addr and size validation into swiotlb_bounce

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 2bdba622c351259317b0294c6e9fe243b2404316
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/2bdba622.failed

Move the code to find and validate the original buffer address and size
from the callers into swiotlb_bounce.  This means a tiny bit of extra
work in the swiotlb_map path, but avoids code duplication and a leads to
a better code structure.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 2bdba622c351259317b0294c6e9fe243b2404316)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index 017e72bac63a,a431c6b64e82..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -648,17 -661,6 +661,20 @@@ phys_addr_t swiotlb_tbl_map_single(stru
  	return tlb_addr;
  }
  
++<<<<<<< HEAD
 +static void validate_sync_size_and_truncate(struct device *hwdev, size_t orig_size, size_t *size)
 +{
 +	if (*size > orig_size) {
 +		/* Warn and truncate mapping_size */
 +		dev_WARN_ONCE(hwdev, 1,
 +			"Attempt for buffer overflow. Original size: %zu. Mapping size: %zu.\n",
 +			orig_size, *size);
 +		*size = orig_size;
 +	}
 +}
 +
++=======
++>>>>>>> 2bdba622c351 (swiotlb: move orig addr and size validation into swiotlb_bounce)
  /*
   * tlb_addr is the physical address of the bounce buffer to unmap.
   */
@@@ -668,11 -670,9 +684,16 @@@ void swiotlb_tbl_unmap_single(struct de
  {
  	unsigned long flags;
  	unsigned int offset = swiotlb_align_offset(hwdev, tlb_addr);
 +	int i, count, nslots = nr_slots(alloc_size + offset);
  	int index = (tlb_addr - offset - io_tlb_start) >> IO_TLB_SHIFT;
++<<<<<<< HEAD
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
 +
 +	validate_sync_size_and_truncate(hwdev, io_tlb_orig_size[index], &mapping_size);
++=======
+ 	int nslots = nr_slots(io_tlb_alloc_size[index] + offset);
+ 	int count, i;
++>>>>>>> 2bdba622c351 (swiotlb: move orig addr and size validation into swiotlb_bounce)
  
  	/*
  	 * First, sync the memory before unmapping the entry
@@@ -720,15 -719,6 +740,18 @@@ void swiotlb_tbl_sync_single(struct dev
  			     size_t size, enum dma_data_direction dir,
  			     enum dma_sync_target target)
  {
++<<<<<<< HEAD
 +	int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
 +	size_t orig_size = io_tlb_orig_size[index];
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
 +
 +	if (orig_addr == INVALID_PHYS_ADDR)
 +		return;
 +
 +	validate_sync_size_and_truncate(hwdev, orig_size, &size);
 +
++=======
++>>>>>>> 2bdba622c351 (swiotlb: move orig addr and size validation into swiotlb_bounce)
  	switch (target) {
  	case SYNC_FOR_CPU:
  		if (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
* Unmerged path kernel/dma/swiotlb.c
