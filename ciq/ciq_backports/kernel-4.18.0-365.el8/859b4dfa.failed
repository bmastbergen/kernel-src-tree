igc: Replace IGC_TX_FLAGS_XDP flag by an enum

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andre Guedes <andre.guedes@intel.com>
commit 859b4dfa4115d11aa1fda7d0628a93a9d61a7c46
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/859b4dfa.failed

Up to this point, Tx buffers are associated with either a skb or a xdpf,
and the IGC_TX_FLAGS_XDP flag was enough to distinguish between these
two case. However, with upcoming patches that will add AF_XDP zero-copy
support, a third case will be introduced so this flag-based approach
won't fit well.

In preparation to land AF_XDP zero-copy support, replace the
IGC_TX_FLAGS_XDP flag by an enum which will be extended once zero-copy
support is introduced to the driver.

	Signed-off-by: Andre Guedes <andre.guedes@intel.com>
	Signed-off-by: Vedang Patel <vedang.patel@intel.com>
	Signed-off-by: Jithu Joseph <jithu.joseph@intel.com>
	Reviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: Dvora Fuxbrumer <dvorax.fuxbrumer@linux.intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 859b4dfa4115d11aa1fda7d0628a93a9d61a7c46)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/igc/igc.h
#	drivers/net/ethernet/intel/igc/igc_main.c
diff --cc drivers/net/ethernet/intel/igc/igc.h
index b00cd8696b6d,4815c520652b..000000000000
--- a/drivers/net/ethernet/intel/igc/igc.h
+++ b/drivers/net/ethernet/intel/igc/igc.h
@@@ -395,7 -417,11 +400,15 @@@ enum igc_tx_buffer_type 
  struct igc_tx_buffer {
  	union igc_adv_tx_desc *next_to_watch;
  	unsigned long time_stamp;
++<<<<<<< HEAD
 +	struct sk_buff *skb;
++=======
+ 	enum igc_tx_buffer_type type;
+ 	union {
+ 		struct sk_buff *skb;
+ 		struct xdp_frame *xdpf;
+ 	};
++>>>>>>> 859b4dfa4115 (igc: Replace IGC_TX_FLAGS_XDP flag by an enum)
  	unsigned int bytecount;
  	u16 gso_segs;
  	__be16 protocol;
diff --cc drivers/net/ethernet/intel/igc/igc_main.c
index 992dd6933ec4,4e1327a5a61e..000000000000
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@@ -186,8 -191,17 +186,22 @@@ static void igc_clean_tx_ring(struct ig
  	while (i != tx_ring->next_to_use) {
  		union igc_adv_tx_desc *eop_desc, *tx_desc;
  
++<<<<<<< HEAD
 +		/* Free all the Tx ring sk_buffs */
 +		dev_kfree_skb_any(tx_buffer->skb);
++=======
+ 		switch (tx_buffer->type) {
+ 		case IGC_TX_BUFFER_TYPE_XDP:
+ 			xdp_return_frame(tx_buffer->xdpf);
+ 			break;
+ 		case IGC_TX_BUFFER_TYPE_SKB:
+ 			dev_kfree_skb_any(tx_buffer->skb);
+ 			break;
+ 		default:
+ 			netdev_warn_once(tx_ring->netdev, "Unknown Tx buffer type\n");
+ 			break;
+ 		}
++>>>>>>> 859b4dfa4115 (igc: Replace IGC_TX_FLAGS_XDP flag by an enum)
  
  		igc_unmap_tx_buffer(tx_ring->dev, tx_buffer);
  
@@@ -1878,6 -1939,193 +1893,196 @@@ static void igc_alloc_rx_buffers(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int igc_xdp_init_tx_buffer(struct igc_tx_buffer *buffer,
+ 				  struct xdp_frame *xdpf,
+ 				  struct igc_ring *ring)
+ {
+ 	dma_addr_t dma;
+ 
+ 	dma = dma_map_single(ring->dev, xdpf->data, xdpf->len, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ring->dev, dma)) {
+ 		netdev_err_once(ring->netdev, "Failed to map DMA for TX\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	buffer->type = IGC_TX_BUFFER_TYPE_XDP;
+ 	buffer->xdpf = xdpf;
+ 	buffer->protocol = 0;
+ 	buffer->bytecount = xdpf->len;
+ 	buffer->gso_segs = 1;
+ 	buffer->time_stamp = jiffies;
+ 	dma_unmap_len_set(buffer, len, xdpf->len);
+ 	dma_unmap_addr_set(buffer, dma, dma);
+ 	return 0;
+ }
+ 
+ /* This function requires __netif_tx_lock is held by the caller. */
+ static int igc_xdp_init_tx_descriptor(struct igc_ring *ring,
+ 				      struct xdp_frame *xdpf)
+ {
+ 	struct igc_tx_buffer *buffer;
+ 	union igc_adv_tx_desc *desc;
+ 	u32 cmd_type, olinfo_status;
+ 	int err;
+ 
+ 	if (!igc_desc_unused(ring))
+ 		return -EBUSY;
+ 
+ 	buffer = &ring->tx_buffer_info[ring->next_to_use];
+ 	err = igc_xdp_init_tx_buffer(buffer, xdpf, ring);
+ 	if (err)
+ 		return err;
+ 
+ 	cmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |
+ 		   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |
+ 		   buffer->bytecount;
+ 	olinfo_status = buffer->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;
+ 
+ 	desc = IGC_TX_DESC(ring, ring->next_to_use);
+ 	desc->read.cmd_type_len = cpu_to_le32(cmd_type);
+ 	desc->read.olinfo_status = cpu_to_le32(olinfo_status);
+ 	desc->read.buffer_addr = cpu_to_le64(dma_unmap_addr(buffer, dma));
+ 
+ 	netdev_tx_sent_queue(txring_txq(ring), buffer->bytecount);
+ 
+ 	buffer->next_to_watch = desc;
+ 
+ 	ring->next_to_use++;
+ 	if (ring->next_to_use == ring->count)
+ 		ring->next_to_use = 0;
+ 
+ 	return 0;
+ }
+ 
+ static struct igc_ring *igc_xdp_get_tx_ring(struct igc_adapter *adapter,
+ 					    int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= adapter->num_tx_queues)
+ 		index -= adapter->num_tx_queues;
+ 
+ 	return adapter->tx_ring[index];
+ }
+ 
+ static int igc_xdp_xmit_back(struct igc_adapter *adapter, struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return -EFAULT;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	res = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 	__netif_tx_unlock(nq);
+ 	return res;
+ }
+ 
+ /* This function assumes rcu_read_lock() is held by the caller. */
+ static int __igc_xdp_run_prog(struct igc_adapter *adapter,
+ 			      struct bpf_prog *prog,
+ 			      struct xdp_buff *xdp)
+ {
+ 	u32 act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		return IGC_XDP_PASS;
+ 	case XDP_TX:
+ 		return igc_xdp_xmit_back(adapter, xdp) < 0 ?
+ 			IGC_XDP_CONSUMED : IGC_XDP_TX;
+ 	case XDP_REDIRECT:
+ 		return xdp_do_redirect(adapter->netdev, xdp, prog) < 0 ?
+ 			IGC_XDP_CONSUMED : IGC_XDP_REDIRECT;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(adapter->netdev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		return IGC_XDP_CONSUMED;
+ 	}
+ }
+ 
+ static struct sk_buff *igc_xdp_run_prog(struct igc_adapter *adapter,
+ 					struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 
+ 	rcu_read_lock();
+ 
+ 	prog = READ_ONCE(adapter->xdp_prog);
+ 	if (!prog) {
+ 		res = IGC_XDP_PASS;
+ 		goto unlock;
+ 	}
+ 
+ 	res = __igc_xdp_run_prog(adapter, prog, xdp);
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 	return ERR_PTR(-res);
+ }
+ 
+ /* This function assumes __netif_tx_lock is held by the caller. */
+ static void igc_flush_tx_descriptors(struct igc_ring *ring)
+ {
+ 	/* Once tail pointer is updated, hardware can fetch the descriptors
+ 	 * any time so we issue a write membar here to ensure all memory
+ 	 * writes are complete before the tail pointer is updated.
+ 	 */
+ 	wmb();
+ 	writel(ring->next_to_use, ring->tail);
+ }
+ 
+ static void igc_finalize_xdp(struct igc_adapter *adapter, int status)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 
+ 	if (status & IGC_XDP_TX) {
+ 		ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 		nq = txring_txq(ring);
+ 
+ 		__netif_tx_lock(nq, cpu);
+ 		igc_flush_tx_descriptors(ring);
+ 		__netif_tx_unlock(nq);
+ 	}
+ 
+ 	if (status & IGC_XDP_REDIRECT)
+ 		xdp_do_flush();
+ }
+ 
+ static void igc_update_rx_stats(struct igc_q_vector *q_vector,
+ 				unsigned int packets, unsigned int bytes)
+ {
+ 	struct igc_ring *ring = q_vector->rx.ring;
+ 
+ 	u64_stats_update_begin(&ring->rx_syncp);
+ 	ring->rx_stats.packets += packets;
+ 	ring->rx_stats.bytes += bytes;
+ 	u64_stats_update_end(&ring->rx_syncp);
+ 
+ 	q_vector->rx.total_packets += packets;
+ 	q_vector->rx.total_bytes += bytes;
+ }
+ 
++>>>>>>> 859b4dfa4115 (igc: Replace IGC_TX_FLAGS_XDP flag by an enum)
  static int igc_clean_rx_irq(struct igc_q_vector *q_vector, const int budget)
  {
  	unsigned int total_bytes = 0, total_packets = 0;
@@@ -2026,8 -2313,17 +2231,22 @@@ static bool igc_clean_tx_irq(struct igc
  		total_bytes += tx_buffer->bytecount;
  		total_packets += tx_buffer->gso_segs;
  
++<<<<<<< HEAD
 +		/* free the skb */
 +		napi_consume_skb(tx_buffer->skb, napi_budget);
++=======
+ 		switch (tx_buffer->type) {
+ 		case IGC_TX_BUFFER_TYPE_XDP:
+ 			xdp_return_frame(tx_buffer->xdpf);
+ 			break;
+ 		case IGC_TX_BUFFER_TYPE_SKB:
+ 			napi_consume_skb(tx_buffer->skb, napi_budget);
+ 			break;
+ 		default:
+ 			netdev_warn_once(tx_ring->netdev, "Unknown Tx buffer type\n");
+ 			break;
+ 		}
++>>>>>>> 859b4dfa4115 (igc: Replace IGC_TX_FLAGS_XDP flag by an enum)
  
  		igc_unmap_tx_buffer(tx_ring->dev, tx_buffer);
  
* Unmerged path drivers/net/ethernet/intel/igc/igc.h
* Unmerged path drivers/net/ethernet/intel/igc/igc_main.c
