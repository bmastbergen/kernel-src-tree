x86: Fix leftover comment typos

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit c43426334b3169b6c9e6855483aa7384ff09fd33
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/c4342633.failed

	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit c43426334b3169b6c9e6855483aa7384ff09fd33)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/hyperv/hv_init.c
#	arch/x86/include/asm/stackprotector.h
#	arch/x86/kernel/kprobes/core.c
diff --cc arch/x86/hyperv/hv_init.c
index 2a13a6c9e0f2,256ad0e34dd2..000000000000
--- a/arch/x86/hyperv/hv_init.c
+++ b/arch/x86/hyperv/hv_init.c
@@@ -537,3 -613,51 +537,54 @@@ bool hv_is_isolation_supported(void
  {
  	return hv_get_isolation_type() != HV_ISOLATION_TYPE_NONE;
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(hv_is_isolation_supported);
+ 
+ /* Bit mask of the extended capability to query: see HV_EXT_CAPABILITY_xxx */
+ bool hv_query_ext_cap(u64 cap_query)
+ {
+ 	/*
+ 	 * The address of the 'hv_extended_cap' variable will be used as an
+ 	 * output parameter to the hypercall below and so it should be
+ 	 * compatible with 'virt_to_phys'. Which means, it's address should be
+ 	 * directly mapped. Use 'static' to keep it compatible; stack variables
+ 	 * can be virtually mapped, making them incompatible with
+ 	 * 'virt_to_phys'.
+ 	 * Hypercall input/output addresses should also be 8-byte aligned.
+ 	 */
+ 	static u64 hv_extended_cap __aligned(8);
+ 	static bool hv_extended_cap_queried;
+ 	u64 status;
+ 
+ 	/*
+ 	 * Querying extended capabilities is an extended hypercall. Check if the
+ 	 * partition supports extended hypercall, first.
+ 	 */
+ 	if (!(ms_hyperv.priv_high & HV_ENABLE_EXTENDED_HYPERCALLS))
+ 		return false;
+ 
+ 	/* Extended capabilities do not change at runtime. */
+ 	if (hv_extended_cap_queried)
+ 		return hv_extended_cap & cap_query;
+ 
+ 	status = hv_do_hypercall(HV_EXT_CALL_QUERY_CAPABILITIES, NULL,
+ 				 &hv_extended_cap);
+ 
+ 	/*
+ 	 * The query extended capabilities hypercall should not fail under
+ 	 * any normal circumstances. Avoid repeatedly making the hypercall, on
+ 	 * error.
+ 	 */
+ 	hv_extended_cap_queried = true;
+ 	status &= HV_HYPERCALL_RESULT_MASK;
+ 	if (status != HV_STATUS_SUCCESS) {
+ 		pr_err("Hyper-V: Extended query capabilities hypercall failed 0x%llx\n",
+ 		       status);
+ 		return false;
+ 	}
+ 
+ 	return hv_extended_cap & cap_query;
+ }
+ EXPORT_SYMBOL_GPL(hv_query_ext_cap);
++>>>>>>> c43426334b31 (x86: Fix leftover comment typos)
diff --cc arch/x86/include/asm/stackprotector.h
index 91e29b6a86a5,24a8d6c4fb18..000000000000
--- a/arch/x86/include/asm/stackprotector.h
+++ b/arch/x86/include/asm/stackprotector.h
@@@ -5,30 -5,23 +5,39 @@@
   * Stack protector works by putting predefined pattern at the start of
   * the stack frame and verifying that it hasn't been overwritten when
   * returning from the function.  The pattern is called stack canary
 - * and unfortunately gcc historically required it to be at a fixed offset
 - * from the percpu segment base.  On x86_64, the offset is 40 bytes.
 + * and unfortunately gcc requires it to be at a fixed offset from %gs.
 + * On x86_64, the offset is 40 bytes and on x86_32 20 bytes.  x86_64
 + * and x86_32 use segment registers differently and thus handles this
 + * requirement differently.
   *
++<<<<<<< HEAD
 + * On x86_64, %gs is shared by percpu area and stack canary.  All
 + * percpu symbols are zero based and %gs points to the base of percpu
 + * area.  The first occupant of the percpu area is always
 + * fixed_percpu_data which contains stack_canary at offset 40.  Userland
 + * %gs is always saved and restored on kernel entry and exit using
 + * swapgs, so stack protector doesn't add any complexity there.
++=======
+  * The same segment is shared by percpu area and stack canary.  On
+  * x86_64, percpu symbols are zero based and %gs (64-bit) points to the
+  * base of percpu area.  The first occupant of the percpu area is always
+  * fixed_percpu_data which contains stack_canary at the appropriate
+  * offset.  On x86_32, the stack canary is just a regular percpu
+  * variable.
++>>>>>>> c43426334b31 (x86: Fix leftover comment typos)
   *
 - * Putting percpu data in %fs on 32-bit is a minor optimization compared to
 - * using %gs.  Since 32-bit userspace normally has %fs == 0, we are likely
 - * to load 0 into %fs on exit to usermode, whereas with percpu data in
 - * %gs, we are likely to load a non-null %gs on return to user mode.
 + * On x86_32, it's slightly more complicated.  As in x86_64, %gs is
 + * used for userland TLS.  Unfortunately, some processors are much
 + * slower at loading segment registers with different value when
 + * entering and leaving the kernel, so the kernel uses %fs for percpu
 + * area and manages %gs lazily so that %gs is switched only when
 + * necessary, usually during task switch.
   *
 - * Once we are willing to require GCC 8.1 or better for 64-bit stackprotector
 - * support, we can remove some of this complexity.
 + * As gcc requires the stack canary at %gs:20, %gs can't be managed
 + * lazily if stack protector is enabled, so the kernel saves and
 + * restores userland %gs on kernel entry and exit.  This behavior is
 + * controlled by CONFIG_X86_32_LAZY_GS and accessors are defined in
 + * system.h to hide the details.
   */
  
  #ifndef _ASM_STACKPROTECTOR_H
diff --cc arch/x86/kernel/kprobes/core.c
index 5c51d0f94e9c,7c4d0736a998..000000000000
--- a/arch/x86/kernel/kprobes/core.c
+++ b/arch/x86/kernel/kprobes/core.c
@@@ -464,6 -428,270 +464,273 @@@ void free_insn_page(void *page
  	module_memfree(page);
  }
  
++<<<<<<< HEAD
++=======
+ /* Kprobe x86 instruction emulation - only regs->ip or IF flag modifiers */
+ 
+ static void kprobe_emulate_ifmodifiers(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	switch (p->ainsn.opcode) {
+ 	case 0xfa:	/* cli */
+ 		regs->flags &= ~(X86_EFLAGS_IF);
+ 		break;
+ 	case 0xfb:	/* sti */
+ 		regs->flags |= X86_EFLAGS_IF;
+ 		break;
+ 	case 0x9c:	/* pushf */
+ 		int3_emulate_push(regs, regs->flags);
+ 		break;
+ 	case 0x9d:	/* popf */
+ 		regs->flags = int3_emulate_pop(regs);
+ 		break;
+ 	}
+ 	regs->ip = regs->ip - INT3_INSN_SIZE + p->ainsn.size;
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_ifmodifiers);
+ 
+ static void kprobe_emulate_ret(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	int3_emulate_ret(regs);
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_ret);
+ 
+ static void kprobe_emulate_call(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	unsigned long func = regs->ip - INT3_INSN_SIZE + p->ainsn.size;
+ 
+ 	func += p->ainsn.rel32;
+ 	int3_emulate_call(regs, func);
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_call);
+ 
+ static nokprobe_inline
+ void __kprobe_emulate_jmp(struct kprobe *p, struct pt_regs *regs, bool cond)
+ {
+ 	unsigned long ip = regs->ip - INT3_INSN_SIZE + p->ainsn.size;
+ 
+ 	if (cond)
+ 		ip += p->ainsn.rel32;
+ 	int3_emulate_jmp(regs, ip);
+ }
+ 
+ static void kprobe_emulate_jmp(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	__kprobe_emulate_jmp(p, regs, true);
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_jmp);
+ 
+ static const unsigned long jcc_mask[6] = {
+ 	[0] = X86_EFLAGS_OF,
+ 	[1] = X86_EFLAGS_CF,
+ 	[2] = X86_EFLAGS_ZF,
+ 	[3] = X86_EFLAGS_CF | X86_EFLAGS_ZF,
+ 	[4] = X86_EFLAGS_SF,
+ 	[5] = X86_EFLAGS_PF,
+ };
+ 
+ static void kprobe_emulate_jcc(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	bool invert = p->ainsn.jcc.type & 1;
+ 	bool match;
+ 
+ 	if (p->ainsn.jcc.type < 0xc) {
+ 		match = regs->flags & jcc_mask[p->ainsn.jcc.type >> 1];
+ 	} else {
+ 		match = ((regs->flags & X86_EFLAGS_SF) >> X86_EFLAGS_SF_BIT) ^
+ 			((regs->flags & X86_EFLAGS_OF) >> X86_EFLAGS_OF_BIT);
+ 		if (p->ainsn.jcc.type >= 0xe)
+ 			match = match && (regs->flags & X86_EFLAGS_ZF);
+ 	}
+ 	__kprobe_emulate_jmp(p, regs, (match && !invert) || (!match && invert));
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_jcc);
+ 
+ static void kprobe_emulate_loop(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	bool match;
+ 
+ 	if (p->ainsn.loop.type != 3) {	/* LOOP* */
+ 		if (p->ainsn.loop.asize == 32)
+ 			match = ((*(u32 *)&regs->cx)--) != 0;
+ #ifdef CONFIG_X86_64
+ 		else if (p->ainsn.loop.asize == 64)
+ 			match = ((*(u64 *)&regs->cx)--) != 0;
+ #endif
+ 		else
+ 			match = ((*(u16 *)&regs->cx)--) != 0;
+ 	} else {			/* JCXZ */
+ 		if (p->ainsn.loop.asize == 32)
+ 			match = *(u32 *)(&regs->cx) == 0;
+ #ifdef CONFIG_X86_64
+ 		else if (p->ainsn.loop.asize == 64)
+ 			match = *(u64 *)(&regs->cx) == 0;
+ #endif
+ 		else
+ 			match = *(u16 *)(&regs->cx) == 0;
+ 	}
+ 
+ 	if (p->ainsn.loop.type == 0)	/* LOOPNE */
+ 		match = match && !(regs->flags & X86_EFLAGS_ZF);
+ 	else if (p->ainsn.loop.type == 1)	/* LOOPE */
+ 		match = match && (regs->flags & X86_EFLAGS_ZF);
+ 
+ 	__kprobe_emulate_jmp(p, regs, match);
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_loop);
+ 
+ static const int addrmode_regoffs[] = {
+ 	offsetof(struct pt_regs, ax),
+ 	offsetof(struct pt_regs, cx),
+ 	offsetof(struct pt_regs, dx),
+ 	offsetof(struct pt_regs, bx),
+ 	offsetof(struct pt_regs, sp),
+ 	offsetof(struct pt_regs, bp),
+ 	offsetof(struct pt_regs, si),
+ 	offsetof(struct pt_regs, di),
+ #ifdef CONFIG_X86_64
+ 	offsetof(struct pt_regs, r8),
+ 	offsetof(struct pt_regs, r9),
+ 	offsetof(struct pt_regs, r10),
+ 	offsetof(struct pt_regs, r11),
+ 	offsetof(struct pt_regs, r12),
+ 	offsetof(struct pt_regs, r13),
+ 	offsetof(struct pt_regs, r14),
+ 	offsetof(struct pt_regs, r15),
+ #endif
+ };
+ 
+ static void kprobe_emulate_call_indirect(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	unsigned long offs = addrmode_regoffs[p->ainsn.indirect.reg];
+ 
+ 	int3_emulate_call(regs, regs_get_register(regs, offs));
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_call_indirect);
+ 
+ static void kprobe_emulate_jmp_indirect(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	unsigned long offs = addrmode_regoffs[p->ainsn.indirect.reg];
+ 
+ 	int3_emulate_jmp(regs, regs_get_register(regs, offs));
+ }
+ NOKPROBE_SYMBOL(kprobe_emulate_jmp_indirect);
+ 
+ static int prepare_emulation(struct kprobe *p, struct insn *insn)
+ {
+ 	insn_byte_t opcode = insn->opcode.bytes[0];
+ 
+ 	switch (opcode) {
+ 	case 0xfa:		/* cli */
+ 	case 0xfb:		/* sti */
+ 	case 0x9c:		/* pushfl */
+ 	case 0x9d:		/* popf/popfd */
+ 		/*
+ 		 * IF modifiers must be emulated since it will enable interrupt while
+ 		 * int3 single stepping.
+ 		 */
+ 		p->ainsn.emulate_op = kprobe_emulate_ifmodifiers;
+ 		p->ainsn.opcode = opcode;
+ 		break;
+ 	case 0xc2:	/* ret/lret */
+ 	case 0xc3:
+ 	case 0xca:
+ 	case 0xcb:
+ 		p->ainsn.emulate_op = kprobe_emulate_ret;
+ 		break;
+ 	case 0x9a:	/* far call absolute -- segment is not supported */
+ 	case 0xea:	/* far jmp absolute -- segment is not supported */
+ 	case 0xcc:	/* int3 */
+ 	case 0xcf:	/* iret -- in-kernel IRET is not supported */
+ 		return -EOPNOTSUPP;
+ 		break;
+ 	case 0xe8:	/* near call relative */
+ 		p->ainsn.emulate_op = kprobe_emulate_call;
+ 		if (insn->immediate.nbytes == 2)
+ 			p->ainsn.rel32 = *(s16 *)&insn->immediate.value;
+ 		else
+ 			p->ainsn.rel32 = *(s32 *)&insn->immediate.value;
+ 		break;
+ 	case 0xeb:	/* short jump relative */
+ 	case 0xe9:	/* near jump relative */
+ 		p->ainsn.emulate_op = kprobe_emulate_jmp;
+ 		if (insn->immediate.nbytes == 1)
+ 			p->ainsn.rel32 = *(s8 *)&insn->immediate.value;
+ 		else if (insn->immediate.nbytes == 2)
+ 			p->ainsn.rel32 = *(s16 *)&insn->immediate.value;
+ 		else
+ 			p->ainsn.rel32 = *(s32 *)&insn->immediate.value;
+ 		break;
+ 	case 0x70 ... 0x7f:
+ 		/* 1 byte conditional jump */
+ 		p->ainsn.emulate_op = kprobe_emulate_jcc;
+ 		p->ainsn.jcc.type = opcode & 0xf;
+ 		p->ainsn.rel32 = *(char *)insn->immediate.bytes;
+ 		break;
+ 	case 0x0f:
+ 		opcode = insn->opcode.bytes[1];
+ 		if ((opcode & 0xf0) == 0x80) {
+ 			/* 2 bytes Conditional Jump */
+ 			p->ainsn.emulate_op = kprobe_emulate_jcc;
+ 			p->ainsn.jcc.type = opcode & 0xf;
+ 			if (insn->immediate.nbytes == 2)
+ 				p->ainsn.rel32 = *(s16 *)&insn->immediate.value;
+ 			else
+ 				p->ainsn.rel32 = *(s32 *)&insn->immediate.value;
+ 		} else if (opcode == 0x01 &&
+ 			   X86_MODRM_REG(insn->modrm.bytes[0]) == 0 &&
+ 			   X86_MODRM_MOD(insn->modrm.bytes[0]) == 3) {
+ 			/* VM extensions - not supported */
+ 			return -EOPNOTSUPP;
+ 		}
+ 		break;
+ 	case 0xe0:	/* Loop NZ */
+ 	case 0xe1:	/* Loop */
+ 	case 0xe2:	/* Loop */
+ 	case 0xe3:	/* J*CXZ */
+ 		p->ainsn.emulate_op = kprobe_emulate_loop;
+ 		p->ainsn.loop.type = opcode & 0x3;
+ 		p->ainsn.loop.asize = insn->addr_bytes * 8;
+ 		p->ainsn.rel32 = *(s8 *)&insn->immediate.value;
+ 		break;
+ 	case 0xff:
+ 		/*
+ 		 * Since the 0xff is an extended group opcode, the instruction
+ 		 * is determined by the MOD/RM byte.
+ 		 */
+ 		opcode = insn->modrm.bytes[0];
+ 		if ((opcode & 0x30) == 0x10) {
+ 			if ((opcode & 0x8) == 0x8)
+ 				return -EOPNOTSUPP;	/* far call */
+ 			/* call absolute, indirect */
+ 			p->ainsn.emulate_op = kprobe_emulate_call_indirect;
+ 		} else if ((opcode & 0x30) == 0x20) {
+ 			if ((opcode & 0x8) == 0x8)
+ 				return -EOPNOTSUPP;	/* far jmp */
+ 			/* jmp near absolute indirect */
+ 			p->ainsn.emulate_op = kprobe_emulate_jmp_indirect;
+ 		} else
+ 			break;
+ 
+ 		if (insn->addr_bytes != sizeof(unsigned long))
+ 			return -EOPNOTSUPP;	/* Don't support different size */
+ 		if (X86_MODRM_MOD(opcode) != 3)
+ 			return -EOPNOTSUPP;	/* TODO: support memory addressing */
+ 
+ 		p->ainsn.indirect.reg = X86_MODRM_RM(opcode);
+ #ifdef CONFIG_X86_64
+ 		if (X86_REX_B(insn->rex_prefix.value))
+ 			p->ainsn.indirect.reg += 8;
+ #endif
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	p->ainsn.size = insn->length;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> c43426334b31 (x86: Fix leftover comment typos)
  static int arch_copy_kprobe(struct kprobe *p)
  {
  	struct insn insn;
* Unmerged path arch/x86/hyperv/hv_init.c
diff --git a/arch/x86/include/asm/sgx.h b/arch/x86/include/asm/sgx.h
index a16e2c9154a3..6ed6621287d9 100644
--- a/arch/x86/include/asm/sgx.h
+++ b/arch/x86/include/asm/sgx.h
@@ -13,7 +13,7 @@
 /*
  * This file contains both data structures defined by SGX architecture and Linux
  * defined software data structures and functions.  The two should not be mixed
- * together for better readibility.  The architectural definitions come first.
+ * together for better readability.  The architectural definitions come first.
  */
 
 /* The SGX specific CPUID function. */
* Unmerged path arch/x86/include/asm/stackprotector.h
* Unmerged path arch/x86/kernel/kprobes/core.c
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 4401a1a9e22e..add2d9ba90b6 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -2510,7 +2510,7 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 	 * page is available, while the caller may end up allocating as many as
 	 * four pages, e.g. for PAE roots or for 5-level paging.  Temporarily
 	 * exceeding the (arbitrary by default) limit will not harm the host,
-	 * being too agressive may unnecessarily kill the guest, and getting an
+	 * being too aggressive may unnecessarily kill the guest, and getting an
 	 * exact count is far more trouble than it's worth, especially in the
 	 * page fault paths.
 	 */
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index d2ced27fa216..e4ee397a17dd 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -870,7 +870,7 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 
 		if (!is_shadow_present_pte(iter.old_spte)) {
 			/*
-			 * If SPTE has been forzen by another thread, just
+			 * If SPTE has been frozen by another thread, just
 			 * give up and retry, avoiding unnecessary page table
 			 * allocation and free.
 			 */
