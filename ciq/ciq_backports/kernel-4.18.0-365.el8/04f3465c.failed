mm/memory_hotplug: remove is_mem_section_removable()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author David Hildenbrand <david@redhat.com>
commit 04f3465c98665b7c5a3484d7194f1858954069f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/04f3465c.failed

Fortunately, all users of is_mem_section_removable() are gone.  Get rid of
it, including some now unnecessary functions.

	Signed-off-by: David Hildenbrand <david@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
	Reviewed-by: Baoquan He <bhe@redhat.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Oscar Salvador <osalvador@suse.de>
Link: http://lkml.kernel.org/r/20200407135416.24093-3-david@redhat.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 04f3465c98665b7c5a3484d7194f1858954069f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory_hotplug.c
diff --cc mm/memory_hotplug.c
index 6e00841a4b9e,bfe8cd2a685f..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -1117,81 -1114,6 +1117,84 @@@ EXPORT_SYMBOL_GPL(add_memory)
  
  #ifdef CONFIG_MEMORY_HOTREMOVE
  /*
++<<<<<<< HEAD
 + * A free page on the buddy free lists (not the per-cpu lists) has PageBuddy
 + * set and the size of the free page is given by buddy_order(). Using this,
 + * the function determines if the pageblock contains only free pages.
 + * Due to buddy contraints, a free page at least the size of a pageblock will
 + * be located at the start of the pageblock
 + */
 +static inline int pageblock_free(struct page *page)
 +{
 +	return PageBuddy(page) && buddy_order(page) >= pageblock_order;
 +}
 +
 +/* Return the pfn of the start of the next active pageblock after a given pfn */
 +static unsigned long next_active_pageblock(unsigned long pfn)
 +{
 +	struct page *page = pfn_to_page(pfn);
 +
 +	/* Ensure the starting page is pageblock-aligned */
 +	BUG_ON(pfn & (pageblock_nr_pages - 1));
 +
 +	/* If the entire pageblock is free, move to the end of free page */
 +	if (pageblock_free(page)) {
 +		int order;
 +		/* be careful. we don't have locks, buddy_order can be changed.*/
 +		order = buddy_order(page);
 +		if ((order < MAX_ORDER) && (order >= pageblock_order))
 +			return pfn + (1 << order);
 +	}
 +
 +	return pfn + pageblock_nr_pages;
 +}
 +
 +static bool is_pageblock_removable_nolock(unsigned long pfn)
 +{
 +	struct page *page = pfn_to_page(pfn);
 +	struct zone *zone;
 +
 +	/*
 +	 * We have to be careful here because we are iterating over memory
 +	 * sections which are not zone aware so we might end up outside of
 +	 * the zone but still within the section.
 +	 * We have to take care about the node as well. If the node is offline
 +	 * its NODE_DATA will be NULL - see page_zone.
 +	 */
 +	if (!node_online(page_to_nid(page)))
 +		return false;
 +
 +	zone = page_zone(page);
 +	pfn = page_to_pfn(page);
 +	if (!zone_spans_pfn(zone, pfn))
 +		return false;
 +
 +	return !has_unmovable_pages(zone, page, MIGRATE_MOVABLE,
 +				    MEMORY_OFFLINE);
 +}
 +
 +/* Checks if this range of memory is likely to be hot-removable. */
 +bool is_mem_section_removable(unsigned long start_pfn, unsigned long nr_pages)
 +{
 +	unsigned long end_pfn, pfn;
 +
 +	end_pfn = min(start_pfn + nr_pages,
 +			zone_end_pfn(page_zone(pfn_to_page(start_pfn))));
 +
 +	/* Check the starting page of each pageblock within the range */
 +	for (pfn = start_pfn; pfn < end_pfn; pfn = next_active_pageblock(pfn)) {
 +		if (!is_pageblock_removable_nolock(pfn))
 +			return false;
 +		cond_resched();
 +	}
 +
 +	/* All pageblocks in the memory block are likely to be hot-removable */
 +	return true;
 +}
 +
 +/*
++=======
++>>>>>>> 04f3465c9866 (mm/memory_hotplug: remove is_mem_section_removable())
   * Confirm all pages in a range [start, end) belong to the same zone (skipping
   * memory holes). When true, return the zone.
   */
diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index de87e393f978..e6c393d69651 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -317,19 +317,12 @@ static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
-extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int remove_memory(int nid, u64 start, u64 size);
 extern void __remove_memory(int nid, u64 start, u64 size);
 
 #else
-static inline bool is_mem_section_removable(unsigned long pfn,
-					unsigned long nr_pages)
-{
-	return false;
-}
-
 static inline void try_offline_node(int nid) {}
 
 static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
* Unmerged path mm/memory_hotplug.c
