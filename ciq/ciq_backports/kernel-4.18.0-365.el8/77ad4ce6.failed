arm64: memory: rename VA_START to PAGE_END

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 77ad4ce69321abbe26ec92b2a2691a66531eb688
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/77ad4ce6.failed

Prior to commit:

  14c127c957c1c607 ("arm64: mm: Flip kernel VA space")

... VA_START described the start of the TTBR1 address space for a given
VA size described by VA_BITS, where all kernel mappings began.

Since that commit, VA_START described a portion midway through the
address space, where the linear map ends and other kernel mappings
begin.

To avoid confusion, let's rename VA_START to PAGE_END, making it clear
that it's not the start of the TTBR1 address space and implying that
it's related to PAGE_OFFSET. Comments and other mnemonics are updated
accordingly, along with a typo fix in the decription of VMEMMAP_SIZE.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Tested-by: Steve Capper <steve.capper@arm.com>
	Reviewed-by: Steve Capper <steve.capper@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 77ad4ce69321abbe26ec92b2a2691a66531eb688)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/memory.h
diff --cc arch/arm64/include/asm/memory.h
index 630eaf618e50,a713bad71db5..000000000000
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@@ -57,19 -37,17 +57,19 @@@
  			>> (PAGE_SHIFT - STRUCT_PAGE_MAX_SHIFT))
  
  /*
-  * PAGE_OFFSET - the virtual address of the start of the linear map (top
-  *		 (VA_BITS - 1))
-  * KIMAGE_VADDR - the virtual address of the start of the kernel image
+  * PAGE_OFFSET - the virtual address of the start of the linear map, at the
+  *               start of the TTBR1 address space.
+  * PAGE_END - the end of the linear map, where all other kernel mappings begin.
+  * KIMAGE_VADDR - the virtual address of the start of the kernel image.
   * VA_BITS - the maximum number of bits for virtual addresses.
-  * VA_START - the first kernel virtual address.
   */
  #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 -#define _PAGE_OFFSET(va)	(-(UL(1) << (va)))
 -#define PAGE_OFFSET		(_PAGE_OFFSET(VA_BITS))
 +#define VA_START		(UL(0xffffffffffffffff) - \
 +	(UL(1) << (VA_BITS - 1)) + 1)
 +#define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
 +	(UL(1) << VA_BITS) + 1)
  #define KIMAGE_VADDR		(MODULES_END)
 -#define BPF_JIT_REGION_START	(KASAN_SHADOW_END)
 +#define BPF_JIT_REGION_START	(VA_START + KASAN_SHADOW_SIZE)
  #define BPF_JIT_REGION_SIZE	(SZ_128M)
  #define BPF_JIT_REGION_END	(BPF_JIT_REGION_START + BPF_JIT_REGION_SIZE)
  #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
@@@ -80,10 -58,18 +80,19 @@@
  #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
  #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
  
 -#if VA_BITS > 48
 -#define VA_BITS_MIN		(48)
 -#else
 -#define VA_BITS_MIN		(VA_BITS)
 -#endif
 +#define KERNEL_START      _text
 +#define KERNEL_END        _end
  
++<<<<<<< HEAD
 +#ifdef CONFIG_ARM64_USER_VA_BITS_52
++=======
+ #define _PAGE_END(va)		(-(UL(1) << ((va) - 1)))
+ 
+ #define KERNEL_START		_text
+ #define KERNEL_END		_end
+ 
+ #ifdef CONFIG_ARM64_VA_BITS_52
++>>>>>>> 77ad4ce69321 (arm64: memory: rename VA_START to PAGE_END)
  #define MAX_USER_VA_BITS	52
  #else
  #define MAX_USER_VA_BITS	VA_BITS
@@@ -95,12 -81,14 +104,17 @@@
   * significantly, so double the (minimum) stack size when they are in use.
   */
  #ifdef CONFIG_KASAN
 -#define KASAN_SHADOW_OFFSET	_AC(CONFIG_KASAN_SHADOW_OFFSET, UL)
 -#define KASAN_SHADOW_END	((UL(1) << (64 - KASAN_SHADOW_SCALE_SHIFT)) \
 -					+ KASAN_SHADOW_OFFSET)
 +#define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
  #define KASAN_THREAD_SHIFT	1
  #else
 +#define KASAN_SHADOW_SIZE	(0)
  #define KASAN_THREAD_SHIFT	0
++<<<<<<< HEAD
 +#endif
++=======
+ #define KASAN_SHADOW_END	(_PAGE_END(VA_BITS_MIN))
+ #endif /* CONFIG_KASAN */
++>>>>>>> 77ad4ce69321 (arm64: memory: rename VA_START to PAGE_END)
  
  #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)
  
@@@ -183,15 -171,9 +197,20 @@@
  #define IOREMAP_MAX_ORDER	(PMD_SHIFT)
  #endif
  
 +#ifdef CONFIG_BLK_DEV_INITRD
 +#define __early_init_dt_declare_initrd(__start, __end)			\
 +	do {								\
 +		initrd_start = (__start);				\
 +		initrd_end = (__end);					\
 +	} while (0)
 +#endif
 +
  #ifndef __ASSEMBLY__
++<<<<<<< HEAD
++=======
+ extern u64			vabits_actual;
+ #define PAGE_END		(_PAGE_END(vabits_actual))
++>>>>>>> 77ad4ce69321 (arm64: memory: rename VA_START to PAGE_END)
  
  #include <linux/bitops.h>
  #include <linux/mmdebug.h>
* Unmerged path arch/arm64/include/asm/memory.h
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d4a2e93735be..c5b7e5083def 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -827,8 +827,8 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
 
-#define kc_vaddr_to_offset(v)	((v) & ~VA_START)
-#define kc_offset_to_vaddr(o)	((o) | VA_START)
+#define kc_vaddr_to_offset(v)	((v) & ~PAGE_END)
+#define kc_offset_to_vaddr(o)	((o) | PAGE_END)
 
 #ifdef CONFIG_ARM64_PA_BITS_52
 #define phys_to_ttbr(addr)	(((addr) | ((addr) >> 46)) & TTBR_BADDR_MASK_52)
diff --git a/arch/arm64/kernel/hibernate.c b/arch/arm64/kernel/hibernate.c
index b69f7d641720..e5b0f7be44d1 100644
--- a/arch/arm64/kernel/hibernate.c
+++ b/arch/arm64/kernel/hibernate.c
@@ -495,7 +495,7 @@ int swsusp_arch_resume(void)
 		rc = -ENOMEM;
 		goto out;
 	}
-	rc = copy_page_tables(tmp_pg_dir, PAGE_OFFSET, VA_START);
+	rc = copy_page_tables(tmp_pg_dir, PAGE_OFFSET, PAGE_END);
 	if (rc)
 		goto out;
 
diff --git a/arch/arm64/mm/dump.c b/arch/arm64/mm/dump.c
index e38089453a79..796e0170a4b0 100644
--- a/arch/arm64/mm/dump.c
+++ b/arch/arm64/mm/dump.c
@@ -33,7 +33,7 @@
 
 enum address_markers_idx {
 	PAGE_OFFSET_NR = 0,
-	VA_START_NR,
+	PAGE_END_NR,
 #ifdef CONFIG_KASAN
 	KASAN_START_NR,
 #endif
@@ -41,7 +41,7 @@ enum address_markers_idx {
 
 static struct addr_marker address_markers[] = {
 	{ PAGE_OFFSET,			"Linear Mapping start" },
-	{ 0 /* VA_START */,		"Linear Mapping end" },
+	{ 0 /* PAGE_END */,		"Linear Mapping end" },
 #ifdef CONFIG_KASAN
 	{ 0 /* KASAN_SHADOW_START */,	"Kasan shadow start" },
 	{ KASAN_SHADOW_END,		"Kasan shadow end" },
@@ -374,7 +374,7 @@ void ptdump_check_wx(void)
 
 static int ptdump_init(void)
 {
-	address_markers[VA_START_NR].start_address = VA_START;
+	address_markers[PAGE_END_NR].start_address = PAGE_END;
 #ifdef CONFIG_KASAN
 	address_markers[KASAN_START_NR].start_address = KASAN_SHADOW_START;
 #endif
diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index b6bb51a47354..2b6bf329093a 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -229,7 +229,7 @@ void __init kasan_init(void)
 	kasan_map_populate(kimg_shadow_start, kimg_shadow_end,
 			   early_pfn_to_nid(virt_to_pfn(lm_alias(_text))));
 
-	kasan_populate_early_shadow(kasan_mem_to_shadow((void *) VA_START),
+	kasan_populate_early_shadow(kasan_mem_to_shadow((void *)PAGE_END),
 				   (void *)mod_shadow_start);
 	kasan_populate_early_shadow((void *)kimg_shadow_end,
 				   (void *)KASAN_SHADOW_END);
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index 89b746333b14..d5b53631dfd7 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -409,7 +409,7 @@ static phys_addr_t pgd_pgtable_alloc(int shift)
 static void __init create_mapping_noalloc(phys_addr_t phys, unsigned long virt,
 				  phys_addr_t size, pgprot_t prot)
 {
-	if ((virt >= VA_START) && (virt < VMALLOC_START)) {
+	if ((virt >= PAGE_END) && (virt < VMALLOC_START)) {
 		pr_warn("BUG: not creating mapping for %pa at 0x%016lx - outside kernel range\n",
 			&phys, virt);
 		return;
@@ -436,7 +436,7 @@ void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 static void update_mapping_prot(phys_addr_t phys, unsigned long virt,
 				phys_addr_t size, pgprot_t prot)
 {
-	if ((virt >= VA_START) && (virt < VMALLOC_START)) {
+	if ((virt >= PAGE_END) && (virt < VMALLOC_START)) {
 		pr_warn("BUG: not updating mapping for %pa at 0x%016lx - outside kernel range\n",
 			&phys, virt);
 		return;
