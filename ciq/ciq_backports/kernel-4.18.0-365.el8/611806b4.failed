kasan: fix bug detection via ksize for HW_TAGS mode

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 611806b4bf8dd97a4f3d73f5cf3c2c7730c51eb2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/611806b4.failed

The currently existing kasan_check_read/write() annotations are intended
to be used for kernel modules that have KASAN compiler instrumentation
disabled. Thus, they are only relevant for the software KASAN modes that
rely on compiler instrumentation.

However there's another use case for these annotations: ksize() checks
that the object passed to it is indeed accessible before unpoisoning the
whole object. This is currently done via __kasan_check_read(), which is
compiled away for the hardware tag-based mode that doesn't rely on
compiler instrumentation. This leads to KASAN missing detecting some
memory corruptions.

Provide another annotation called kasan_check_byte() that is available
for all KASAN modes. As the implementation rename and reuse
kasan_check_invalid_free(). Use this new annotation in ksize().
To avoid having ksize() as the top frame in the reported stack trace
pass _RET_IP_ to __kasan_check_byte().

Also add a new ksize_uaf() test that checks that a use-after-free is
detected via ksize() itself, and via plain accesses that happen later.

Link: https://linux-review.googlesource.com/id/Iaabf771881d0f9ce1b969f2a62938e99d3308ec5
Link: https://lkml.kernel.org/r/f32ad74a60b28d8402482a38476f02bb7600f620.1610733117.git.andreyknvl@google.com
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Marco Elver <elver@google.com>
	Reviewed-by: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Branislav Rankov <Branislav.Rankov@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Evgenii Stepanov <eugenis@google.com>
	Cc: Kevin Brodsky <kevin.brodsky@arm.com>
	Cc: Peter Collingbourne <pcc@google.com>
	Cc: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 611806b4bf8dd97a4f3d73f5cf3c2c7730c51eb2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	lib/test_kasan.c
#	mm/kasan/common.c
#	mm/kasan/generic.c
#	mm/kasan/kasan.h
#	mm/kasan/tags.c
diff --cc include/linux/kasan.h
index 71e7b6777f1b,7eaf2d9effb4..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -69,29 -85,180 +69,42 @@@ struct kasan_cache 
  	int free_meta_offset;
  };
  
 -#ifdef CONFIG_KASAN_HW_TAGS
 -
 -DECLARE_STATIC_KEY_FALSE(kasan_flag_enabled);
 -
 -static __always_inline bool kasan_enabled(void)
 -{
 -	return static_branch_likely(&kasan_flag_enabled);
 -}
 -
 -#else /* CONFIG_KASAN_HW_TAGS */
 -
 -static inline bool kasan_enabled(void)
 -{
 -	return true;
 -}
 -
 -#endif /* CONFIG_KASAN_HW_TAGS */
 -
 -slab_flags_t __kasan_never_merge(void);
 -static __always_inline slab_flags_t kasan_never_merge(void)
 -{
 -	if (kasan_enabled())
 -		return __kasan_never_merge();
 -	return 0;
 -}
 -
 -void __kasan_unpoison_range(const void *addr, size_t size);
 -static __always_inline void kasan_unpoison_range(const void *addr, size_t size)
 -{
 -	if (kasan_enabled())
 -		__kasan_unpoison_range(addr, size);
 -}
 -
 -void __kasan_alloc_pages(struct page *page, unsigned int order);
 -static __always_inline void kasan_alloc_pages(struct page *page,
 -						unsigned int order)
 -{
 -	if (kasan_enabled())
 -		__kasan_alloc_pages(page, order);
 -}
 -
 -void __kasan_free_pages(struct page *page, unsigned int order);
 -static __always_inline void kasan_free_pages(struct page *page,
 -						unsigned int order)
 -{
 -	if (kasan_enabled())
 -		__kasan_free_pages(page, order);
 -}
 -
 -void __kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 -				slab_flags_t *flags);
 -static __always_inline void kasan_cache_create(struct kmem_cache *cache,
 -				unsigned int *size, slab_flags_t *flags)
 -{
 -	if (kasan_enabled())
 -		__kasan_cache_create(cache, size, flags);
 -}
 -
 -size_t __kasan_metadata_size(struct kmem_cache *cache);
 -static __always_inline size_t kasan_metadata_size(struct kmem_cache *cache)
 -{
 -	if (kasan_enabled())
 -		return __kasan_metadata_size(cache);
 -	return 0;
 -}
 -
 -void __kasan_poison_slab(struct page *page);
 -static __always_inline void kasan_poison_slab(struct page *page)
 -{
 -	if (kasan_enabled())
 -		__kasan_poison_slab(page);
 -}
 -
 -void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 -static __always_inline void kasan_unpoison_object_data(struct kmem_cache *cache,
 -							void *object)
 -{
 -	if (kasan_enabled())
 -		__kasan_unpoison_object_data(cache, object);
 -}
 -
 -void __kasan_poison_object_data(struct kmem_cache *cache, void *object);
 -static __always_inline void kasan_poison_object_data(struct kmem_cache *cache,
 -							void *object)
 -{
 -	if (kasan_enabled())
 -		__kasan_poison_object_data(cache, object);
 -}
 -
 -void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
 -					  const void *object);
 -static __always_inline void * __must_check kasan_init_slab_obj(
 -				struct kmem_cache *cache, const void *object)
 -{
 -	if (kasan_enabled())
 -		return __kasan_init_slab_obj(cache, object);
 -	return (void *)object;
 -}
 -
 -bool __kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 -static __always_inline bool kasan_slab_free(struct kmem_cache *s, void *object)
 -{
 -	if (kasan_enabled())
 -		return __kasan_slab_free(s, object, _RET_IP_);
 -	return false;
 -}
 -
 -void __kasan_slab_free_mempool(void *ptr, unsigned long ip);
 -static __always_inline void kasan_slab_free_mempool(void *ptr)
 -{
 -	if (kasan_enabled())
 -		__kasan_slab_free_mempool(ptr, _RET_IP_);
 -}
 -
 -void * __must_check __kasan_slab_alloc(struct kmem_cache *s,
 -				       void *object, gfp_t flags);
 -static __always_inline void * __must_check kasan_slab_alloc(
 -				struct kmem_cache *s, void *object, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_slab_alloc(s, object, flags);
 -	return object;
 -}
 -
 -void * __must_check __kasan_kmalloc(struct kmem_cache *s, const void *object,
 -				    size_t size, gfp_t flags);
 -static __always_inline void * __must_check kasan_kmalloc(struct kmem_cache *s,
 -				const void *object, size_t size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_kmalloc(s, object, size, flags);
 -	return (void *)object;
 -}
 -
 -void * __must_check __kasan_kmalloc_large(const void *ptr,
 -					  size_t size, gfp_t flags);
 -static __always_inline void * __must_check kasan_kmalloc_large(const void *ptr,
 -						      size_t size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_kmalloc_large(ptr, size, flags);
 -	return (void *)ptr;
 -}
 +/*
 + * These functions provide a special case to support backing module
 + * allocations with real shadow memory. With KASAN vmalloc, the special
 + * case is unnecessary, as the work is handled in the generic case.
 + */
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size);
 +void kasan_free_shadow(const struct vm_struct *vm);
 +#else
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +#endif
  
 -void * __must_check __kasan_krealloc(const void *object,
 -				     size_t new_size, gfp_t flags);
 -static __always_inline void * __must_check kasan_krealloc(const void *object,
 -						 size_t new_size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_krealloc(object, new_size, flags);
 -	return (void *)object;
 -}
 +int kasan_add_zero_shadow(void *start, unsigned long size);
 +void kasan_remove_zero_shadow(void *start, unsigned long size);
  
 -void __kasan_kfree_large(void *ptr, unsigned long ip);
 -static __always_inline void kasan_kfree_large(void *ptr)
 +size_t __ksize(const void *);
 +static inline void kasan_unpoison_slab(const void *ptr)
  {
 -	if (kasan_enabled())
 -		__kasan_kfree_large(ptr, _RET_IP_);
 +	kasan_unpoison_shadow(ptr, __ksize(ptr));
  }
 +size_t kasan_metadata_size(struct kmem_cache *cache);
  
+ /*
+  * Unlike kasan_check_read/write(), kasan_check_byte() is performed even for
+  * the hardware tag-based mode that doesn't rely on compiler instrumentation.
+  */
+ bool __kasan_check_byte(const void *addr, unsigned long ip);
+ static __always_inline bool kasan_check_byte(const void *addr)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_check_byte(addr, _RET_IP_);
+ 	return true;
+ }
+ 
+ 
  bool kasan_save_enable_multi_shot(void);
  void kasan_restore_multi_shot(bool enabled);
  
@@@ -138,31 -313,11 +151,39 @@@ static inline void *kasan_krealloc(cons
  {
  	return (void *)object;
  }
++<<<<<<< HEAD
 +
 +static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
 +				   gfp_t flags)
 +{
 +	return object;
 +}
 +static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 +				   unsigned long ip)
 +{
 +	return false;
 +}
 +
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +
 +static inline int kasan_add_zero_shadow(void *start, unsigned long size)
 +{
 +	return 0;
 +}
 +static inline void kasan_remove_zero_shadow(void *start,
 +					unsigned long size)
 +{}
 +
 +static inline void kasan_unpoison_slab(const void *ptr) { }
 +static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
++=======
+ static inline void kasan_kfree_large(void *ptr) {}
+ static inline bool kasan_check_byte(const void *address)
+ {
+ 	return true;
+ }
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
  
  #endif /* CONFIG_KASAN */
  
diff --cc lib/test_kasan.c
index 5679dda37d78,3f771fabd0ec..000000000000
--- a/lib/test_kasan.c
+++ b/lib/test_kasan.c
@@@ -485,11 -490,50 +485,52 @@@ static noinline void __init kasan_globa
  	volatile int i = 3;
  	char *p = &global_array[ARRAY_SIZE(global_array) + i];
  
 -	/* Only generic mode instruments globals. */
 -	KASAN_TEST_NEEDS_CONFIG_ON(test, CONFIG_KASAN_GENERIC);
 -
 -	KUNIT_EXPECT_KASAN_FAIL(test, *(volatile char *)p);
 +	pr_info("out-of-bounds global variable\n");
 +	*(volatile char *)p;
  }
  
++<<<<<<< HEAD
 +static noinline void __init kasan_stack_oob(void)
++=======
+ /* Check that ksize() makes the whole object accessible. */
+ static void ksize_unpoisons_memory(struct kunit *test)
+ {
+ 	char *ptr;
+ 	size_t size = 123, real_size;
+ 
+ 	ptr = kmalloc(size, GFP_KERNEL);
+ 	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
+ 	real_size = ksize(ptr);
+ 
+ 	/* This access shouldn't trigger a KASAN report. */
+ 	ptr[size] = 'x';
+ 
+ 	/* This one must. */
+ 	KUNIT_EXPECT_KASAN_FAIL(test, ptr[real_size] = 'y');
+ 
+ 	kfree(ptr);
+ }
+ 
+ /*
+  * Check that a use-after-free is detected by ksize() and via normal accesses
+  * after it.
+  */
+ static void ksize_uaf(struct kunit *test)
+ {
+ 	char *ptr;
+ 	int size = 128 - KASAN_GRANULE_SIZE;
+ 
+ 	ptr = kmalloc(size, GFP_KERNEL);
+ 	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
+ 	kfree(ptr);
+ 
+ 	KUNIT_EXPECT_KASAN_FAIL(test, ksize(ptr));
+ 	KUNIT_EXPECT_KASAN_FAIL(test, kasan_int_result = *ptr);
+ 	KUNIT_EXPECT_KASAN_FAIL(test, kasan_int_result = *(ptr + size));
+ }
+ 
+ static void kasan_stack_oob(struct kunit *test)
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
  {
  	char stack_array[10];
  	volatile int i = OOB_TAG_OFF;
@@@ -808,100 -802,153 +849,148 @@@ static noinline void __init vmalloc_oob
  	 * The MMU will catch that and crash us.
  	 */
  	area = vmalloc(3000);
 -	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, area);
 +	if (!area) {
 +		pr_err("Allocation failed\n");
 +		return;
 +	}
  
 -	KUNIT_EXPECT_KASAN_FAIL(test, ((volatile char *)area)[3100]);
 +	((volatile char *)area)[3100];
  	vfree(area);
  }
 +#else
 +static void __init vmalloc_oob(void) {}
 +#endif
  
 -/*
 - * Check that the assigned pointer tag falls within the [KASAN_TAG_MIN,
 - * KASAN_TAG_KERNEL) range (note: excluding the match-all tag) for tag-based
 - * modes.
 - */
 -static void match_all_not_assigned(struct kunit *test)
 -{
 -	char *ptr;
 -	struct page *pages;
 -	int i, size, order;
 -
 -	KASAN_TEST_NEEDS_CONFIG_OFF(test, CONFIG_KASAN_GENERIC);
 -
 -	for (i = 0; i < 256; i++) {
 -		size = (get_random_int() % 1024) + 1;
 -		ptr = kmalloc(size, GFP_KERNEL);
 -		KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
 -		KUNIT_EXPECT_GE(test, (u8)get_tag(ptr), (u8)KASAN_TAG_MIN);
 -		KUNIT_EXPECT_LT(test, (u8)get_tag(ptr), (u8)KASAN_TAG_KERNEL);
 -		kfree(ptr);
 -	}
 -
 -	for (i = 0; i < 256; i++) {
 -		order = (get_random_int() % 4) + 1;
 -		pages = alloc_pages(GFP_KERNEL, order);
 -		ptr = page_address(pages);
 -		KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
 -		KUNIT_EXPECT_GE(test, (u8)get_tag(ptr), (u8)KASAN_TAG_MIN);
 -		KUNIT_EXPECT_LT(test, (u8)get_tag(ptr), (u8)KASAN_TAG_KERNEL);
 -		free_pages((unsigned long)ptr, order);
 -	}
 -}
 +static struct kasan_rcu_info {
 +	int i;
 +	struct rcu_head rcu;
 +} *global_rcu_ptr;
  
 -/* Check that 0xff works as a match-all pointer tag for tag-based modes. */
 -static void match_all_ptr_tag(struct kunit *test)
 +static noinline void __init kasan_rcu_reclaim(struct rcu_head *rp)
  {
 -	char *ptr;
 -	u8 tag;
 +	struct kasan_rcu_info *fp = container_of(rp,
 +						struct kasan_rcu_info, rcu);
  
 -	KASAN_TEST_NEEDS_CONFIG_OFF(test, CONFIG_KASAN_GENERIC);
 -
 -	ptr = kmalloc(128, GFP_KERNEL);
 -	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
 -
 -	/* Backup the assigned tag. */
 -	tag = get_tag(ptr);
 -	KUNIT_EXPECT_NE(test, tag, (u8)KASAN_TAG_KERNEL);
 -
 -	/* Reset the tag to 0xff.*/
 -	ptr = set_tag(ptr, KASAN_TAG_KERNEL);
 -
 -	/* This access shouldn't trigger a KASAN report. */
 -	*ptr = 0;
 -
 -	/* Recover the pointer tag and free. */
 -	ptr = set_tag(ptr, tag);
 -	kfree(ptr);
 +	kfree(fp);
 +	fp->i = 1;
  }
  
 -/* Check that there are no match-all memory tags for tag-based modes. */
 -static void match_all_mem_tag(struct kunit *test)
 +static noinline void __init kasan_rcu_uaf(void)
  {
 -	char *ptr;
 -	int tag;
 -
 -	KASAN_TEST_NEEDS_CONFIG_OFF(test, CONFIG_KASAN_GENERIC);
 -
 -	ptr = kmalloc(128, GFP_KERNEL);
 -	KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
 -	KUNIT_EXPECT_NE(test, (u8)get_tag(ptr), (u8)KASAN_TAG_KERNEL);
 +	struct kasan_rcu_info *ptr;
  
 -	/* For each possible tag value not matching the pointer tag. */
 -	for (tag = KASAN_TAG_MIN; tag <= KASAN_TAG_KERNEL; tag++) {
 -		if (tag == get_tag(ptr))
 -			continue;
 -
 -		/* Mark the first memory granule with the chosen memory tag. */
 -		kasan_poison(ptr, KASAN_GRANULE_SIZE, (u8)tag);
 -
 -		/* This access must cause a KASAN report. */
 -		KUNIT_EXPECT_KASAN_FAIL(test, *ptr = 0);
 +	pr_info("use-after-free in kasan_rcu_reclaim\n");
 +	ptr = kmalloc(sizeof(struct kasan_rcu_info), GFP_KERNEL);
 +	if (!ptr) {
 +		pr_err("Allocation failed\n");
 +		return;
  	}
  
 -	/* Recover the memory tag and free. */
 -	kasan_poison(ptr, KASAN_GRANULE_SIZE, get_tag(ptr));
 -	kfree(ptr);
 +	global_rcu_ptr = rcu_dereference_protected(ptr, NULL);
 +	call_rcu(&global_rcu_ptr->rcu, kasan_rcu_reclaim);
  }
  
++<<<<<<< HEAD
 +static int __init kmalloc_tests_init(void)
 +{
 +	/*
 +	 * Temporarily enable multi-shot mode. Otherwise, we'd only get a
 +	 * report for the first case.
 +	 */
 +	bool multishot = kasan_save_enable_multi_shot();
++=======
+ static struct kunit_case kasan_kunit_test_cases[] = {
+ 	KUNIT_CASE(kmalloc_oob_right),
+ 	KUNIT_CASE(kmalloc_oob_left),
+ 	KUNIT_CASE(kmalloc_node_oob_right),
+ 	KUNIT_CASE(kmalloc_pagealloc_oob_right),
+ 	KUNIT_CASE(kmalloc_pagealloc_uaf),
+ 	KUNIT_CASE(kmalloc_pagealloc_invalid_free),
+ 	KUNIT_CASE(kmalloc_large_oob_right),
+ 	KUNIT_CASE(kmalloc_oob_krealloc_more),
+ 	KUNIT_CASE(kmalloc_oob_krealloc_less),
+ 	KUNIT_CASE(kmalloc_oob_16),
+ 	KUNIT_CASE(kmalloc_uaf_16),
+ 	KUNIT_CASE(kmalloc_oob_in_memset),
+ 	KUNIT_CASE(kmalloc_oob_memset_2),
+ 	KUNIT_CASE(kmalloc_oob_memset_4),
+ 	KUNIT_CASE(kmalloc_oob_memset_8),
+ 	KUNIT_CASE(kmalloc_oob_memset_16),
+ 	KUNIT_CASE(kmalloc_memmove_invalid_size),
+ 	KUNIT_CASE(kmalloc_uaf),
+ 	KUNIT_CASE(kmalloc_uaf_memset),
+ 	KUNIT_CASE(kmalloc_uaf2),
+ 	KUNIT_CASE(kfree_via_page),
+ 	KUNIT_CASE(kfree_via_phys),
+ 	KUNIT_CASE(kmem_cache_oob),
+ 	KUNIT_CASE(memcg_accounted_kmem_cache),
+ 	KUNIT_CASE(kasan_global_oob),
+ 	KUNIT_CASE(kasan_stack_oob),
+ 	KUNIT_CASE(kasan_alloca_oob_left),
+ 	KUNIT_CASE(kasan_alloca_oob_right),
+ 	KUNIT_CASE(ksize_unpoisons_memory),
+ 	KUNIT_CASE(ksize_uaf),
+ 	KUNIT_CASE(kmem_cache_double_free),
+ 	KUNIT_CASE(kmem_cache_invalid_free),
+ 	KUNIT_CASE(kasan_memchr),
+ 	KUNIT_CASE(kasan_memcmp),
+ 	KUNIT_CASE(kasan_strings),
+ 	KUNIT_CASE(kasan_bitops_generic),
+ 	KUNIT_CASE(kasan_bitops_tags),
+ 	KUNIT_CASE(kmalloc_double_kzfree),
+ 	KUNIT_CASE(vmalloc_oob),
+ 	KUNIT_CASE(match_all_not_assigned),
+ 	KUNIT_CASE(match_all_ptr_tag),
+ 	KUNIT_CASE(match_all_mem_tag),
+ 	{}
+ };
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
 +
 +	kmalloc_oob_right();
 +	kmalloc_oob_left();
 +	kmalloc_node_oob_right();
 +#ifdef CONFIG_SLUB
 +	kmalloc_pagealloc_oob_right();
 +	kmalloc_pagealloc_uaf();
 +	kmalloc_pagealloc_invalid_free();
 +#endif
 +	kmalloc_large_oob_right();
 +	kmalloc_oob_krealloc_more();
 +	kmalloc_oob_krealloc_less();
 +	kmalloc_oob_16();
 +	kmalloc_oob_in_memset();
 +	kmalloc_oob_memset_2();
 +	kmalloc_oob_memset_4();
 +	kmalloc_oob_memset_8();
 +	kmalloc_oob_memset_16();
 +	kmalloc_memmove_invalid_size();
 +	kmalloc_uaf();
 +	kmalloc_uaf_memset();
 +	kmalloc_uaf2();
 +	kfree_via_page();
 +	kfree_via_phys();
 +	kmem_cache_oob();
 +	memcg_accounted_kmem_cache();
 +	kasan_stack_oob();
 +	kasan_global_oob();
 +	kasan_alloca_oob_left();
 +	kasan_alloca_oob_right();
 +	ksize_unpoisons_memory();
 +	copy_user_test();
 +	kmem_cache_double_free();
 +	kmem_cache_invalid_free();
 +	kasan_memchr();
 +	kasan_memcmp();
 +	kasan_strings();
 +	kasan_bitops();
 +	kmalloc_double_kzfree();
 +	vmalloc_oob();
 +	kasan_rcu_uaf();
  
 -static struct kunit_suite kasan_kunit_test_suite = {
 -	.name = "kasan",
 -	.init = kasan_test_init,
 -	.test_cases = kasan_kunit_test_cases,
 -	.exit = kasan_test_exit,
 -};
 +	kasan_restore_multi_shot(multishot);
  
 -kunit_test_suite(kasan_kunit_test_suite);
 +	return -EAGAIN;
 +}
  
 +module_init(kmalloc_tests_init);
  MODULE_LICENSE("GPL");
diff --cc mm/kasan/common.c
index 0d0cb20ec1a4,b18189ef3a92..000000000000
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@@ -425,8 -345,7 +425,12 @@@ static bool __kasan_slab_free(struct km
  	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
  		return false;
  
++<<<<<<< HEAD
 +	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
 +	if (shadow_invalid(tag, shadow_byte)) {
++=======
+ 	if (!kasan_byte_accessible(tagged_object)) {
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
  		kasan_report_invalid_free(tagged_object, ip);
  		return true;
  	}
@@@ -560,409 -491,11 +564,420 @@@ void kasan_kfree_large(void *ptr, unsig
  	/* The object will be poisoned by kasan_free_pages(). */
  }
  
++<<<<<<< HEAD
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size)
 +{
 +	void *ret;
 +	size_t scaled_size;
 +	size_t shadow_size;
 +	unsigned long shadow_start;
 +
 +	shadow_start = (unsigned long)kasan_mem_to_shadow(addr);
 +	scaled_size = (size + KASAN_SHADOW_MASK) >> KASAN_SHADOW_SCALE_SHIFT;
 +	shadow_size = round_up(scaled_size, PAGE_SIZE);
 +
 +	if (WARN_ON(!PAGE_ALIGNED(shadow_start)))
 +		return -EINVAL;
 +
 +	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
 +			shadow_start + shadow_size,
 +			GFP_KERNEL,
 +			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
 +			__builtin_return_address(0));
 +
 +	if (ret) {
 +		__memset(ret, KASAN_SHADOW_INIT, shadow_size);
 +		find_vm_area(addr)->flags |= VM_KASAN;
 +		kmemleak_ignore(ret);
 +		return 0;
 +	}
 +
 +	return -ENOMEM;
 +}
 +
 +void kasan_free_shadow(const struct vm_struct *vm)
 +{
 +	if (vm->flags & VM_KASAN)
 +		vfree(kasan_mem_to_shadow(vm->addr));
 +}
 +#endif
 +
 +extern void __kasan_report(unsigned long addr, size_t size, bool is_write, unsigned long ip);
 +extern bool report_enabled(void);
 +
 +bool kasan_report(unsigned long addr, size_t size, bool is_write, unsigned long ip)
 +{
 +	unsigned long flags = user_access_save();
 +	bool ret = false;
 +
 +	if (likely(report_enabled())) {
 +		__kasan_report(addr, size, is_write, ip);
 +		ret = true;
 +	}
 +
 +	user_access_restore(flags);
 +
 +	return ret;
 +}
 +
 +#ifdef CONFIG_MEMORY_HOTPLUG
 +static bool shadow_mapped(unsigned long addr)
 +{
 +	pgd_t *pgd = pgd_offset_k(addr);
 +	p4d_t *p4d;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +	pte_t *pte;
 +
 +	if (pgd_none(*pgd))
 +		return false;
 +	p4d = p4d_offset(pgd, addr);
 +	if (p4d_none(*p4d))
 +		return false;
 +	pud = pud_offset(p4d, addr);
 +	if (pud_none(*pud))
 +		return false;
 +
 +	/*
 +	 * We can't use pud_large() or pud_huge(), the first one is
 +	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
 +	 * pud_bad(), if pud is bad then it's bad because it's huge.
 +	 */
 +	if (pud_bad(*pud))
 +		return true;
 +	pmd = pmd_offset(pud, addr);
 +	if (pmd_none(*pmd))
 +		return false;
 +
 +	if (pmd_bad(*pmd))
 +		return true;
 +	pte = pte_offset_kernel(pmd, addr);
 +	return !pte_none(*pte);
 +}
 +
 +static int __meminit kasan_mem_notifier(struct notifier_block *nb,
 +			unsigned long action, void *data)
 +{
 +	struct memory_notify *mem_data = data;
 +	unsigned long nr_shadow_pages, start_kaddr, shadow_start;
 +	unsigned long shadow_end, shadow_size;
 +
 +	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
 +	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
 +	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);
 +	shadow_size = nr_shadow_pages << PAGE_SHIFT;
 +	shadow_end = shadow_start + shadow_size;
 +
 +	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
 +		WARN_ON(start_kaddr % (KASAN_SHADOW_SCALE_SIZE << PAGE_SHIFT)))
 +		return NOTIFY_BAD;
 +
 +	switch (action) {
 +	case MEM_GOING_ONLINE: {
 +		void *ret;
 +
 +		/*
 +		 * If shadow is mapped already than it must have been mapped
 +		 * during the boot. This could happen if we onlining previously
 +		 * offlined memory.
 +		 */
 +		if (shadow_mapped(shadow_start))
 +			return NOTIFY_OK;
 +
 +		ret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,
 +					shadow_end, GFP_KERNEL,
 +					PAGE_KERNEL, VM_NO_GUARD,
 +					pfn_to_nid(mem_data->start_pfn),
 +					__builtin_return_address(0));
 +		if (!ret)
 +			return NOTIFY_BAD;
 +
 +		kmemleak_ignore(ret);
 +		return NOTIFY_OK;
 +	}
 +	case MEM_CANCEL_ONLINE:
 +	case MEM_OFFLINE: {
 +		struct vm_struct *vm;
 +
 +		/*
 +		 * shadow_start was either mapped during boot by kasan_init()
 +		 * or during memory online by __vmalloc_node_range().
 +		 * In the latter case we can use vfree() to free shadow.
 +		 * Non-NULL result of the find_vm_area() will tell us if
 +		 * that was the second case.
 +		 *
 +		 * Currently it's not possible to free shadow mapped
 +		 * during boot by kasan_init(). It's because the code
 +		 * to do that hasn't been written yet. So we'll just
 +		 * leak the memory.
 +		 */
 +		vm = find_vm_area((void *)shadow_start);
 +		if (vm)
 +			vfree((void *)shadow_start);
 +	}
 +	}
 +
 +	return NOTIFY_OK;
 +}
 +
 +static int __init kasan_memhotplug_init(void)
 +{
 +	hotplug_memory_notifier(kasan_mem_notifier, 0);
 +
 +	return 0;
 +}
 +
 +core_initcall(kasan_memhotplug_init);
 +#endif
 +
 +#ifdef CONFIG_KASAN_VMALLOC
 +static int kasan_populate_vmalloc_pte(pte_t *ptep, unsigned long addr,
 +				      void *unused)
 +{
 +	unsigned long page;
 +	pte_t pte;
 +
 +	if (likely(!pte_none(*ptep)))
 +		return 0;
 +
 +	page = __get_free_page(GFP_KERNEL);
 +	if (!page)
 +		return -ENOMEM;
 +
 +	memset((void *)page, KASAN_VMALLOC_INVALID, PAGE_SIZE);
 +	pte = pfn_pte(PFN_DOWN(__pa(page)), PAGE_KERNEL);
 +
 +	spin_lock(&init_mm.page_table_lock);
 +	if (likely(pte_none(*ptep))) {
 +		set_pte_at(&init_mm, addr, ptep, pte);
 +		page = 0;
 +	}
 +	spin_unlock(&init_mm.page_table_lock);
 +	if (page)
 +		free_page(page);
 +	return 0;
 +}
 +
 +int kasan_populate_vmalloc(unsigned long addr, unsigned long size)
 +{
 +	unsigned long shadow_start, shadow_end;
 +	int ret;
 +
 +	if (!is_vmalloc_or_module_addr((void *)addr))
 +		return 0;
 +
 +	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)addr);
 +	shadow_start = ALIGN_DOWN(shadow_start, PAGE_SIZE);
 +	shadow_end = (unsigned long)kasan_mem_to_shadow((void *)addr + size);
 +	shadow_end = ALIGN(shadow_end, PAGE_SIZE);
 +
 +	ret = apply_to_page_range(&init_mm, shadow_start,
 +				  shadow_end - shadow_start,
 +				  kasan_populate_vmalloc_pte, NULL);
 +	if (ret)
 +		return ret;
 +
 +	flush_cache_vmap(shadow_start, shadow_end);
 +
 +	/*
 +	 * We need to be careful about inter-cpu effects here. Consider:
 +	 *
 +	 *   CPU#0				  CPU#1
 +	 * WRITE_ONCE(p, vmalloc(100));		while (x = READ_ONCE(p)) ;
 +	 *					p[99] = 1;
 +	 *
 +	 * With compiler instrumentation, that ends up looking like this:
 +	 *
 +	 *   CPU#0				  CPU#1
 +	 * // vmalloc() allocates memory
 +	 * // let a = area->addr
 +	 * // we reach kasan_populate_vmalloc
 +	 * // and call kasan_unpoison_shadow:
 +	 * STORE shadow(a), unpoison_val
 +	 * ...
 +	 * STORE shadow(a+99), unpoison_val	x = LOAD p
 +	 * // rest of vmalloc process		<data dependency>
 +	 * STORE p, a				LOAD shadow(x+99)
 +	 *
 +	 * If there is no barrier between the end of unpoisioning the shadow
 +	 * and the store of the result to p, the stores could be committed
 +	 * in a different order by CPU#0, and CPU#1 could erroneously observe
 +	 * poison in the shadow.
 +	 *
 +	 * We need some sort of barrier between the stores.
 +	 *
 +	 * In the vmalloc() case, this is provided by a smp_wmb() in
 +	 * clear_vm_uninitialized_flag(). In the per-cpu allocator and in
 +	 * get_vm_area() and friends, the caller gets shadow allocated but
 +	 * doesn't have any pages mapped into the virtual address space that
 +	 * has been reserved. Mapping those pages in will involve taking and
 +	 * releasing a page-table lock, which will provide the barrier.
 +	 */
 +
 +	return 0;
 +}
 +
 +/*
 + * Poison the shadow for a vmalloc region. Called as part of the
 + * freeing process at the time the region is freed.
 + */
 +void kasan_poison_vmalloc(const void *start, unsigned long size)
 +{
 +	if (!is_vmalloc_or_module_addr(start))
 +		return;
 +
 +	size = round_up(size, KASAN_SHADOW_SCALE_SIZE);
 +	kasan_poison_shadow(start, size, KASAN_VMALLOC_INVALID);
 +}
 +
 +void kasan_unpoison_vmalloc(const void *start, unsigned long size)
 +{
 +	if (!is_vmalloc_or_module_addr(start))
 +		return;
 +
 +	kasan_unpoison_shadow(start, size);
 +}
 +
 +static int kasan_depopulate_vmalloc_pte(pte_t *ptep, unsigned long addr,
 +					void *unused)
 +{
 +	unsigned long page;
 +
 +	page = (unsigned long)__va(pte_pfn(*ptep) << PAGE_SHIFT);
 +
 +	spin_lock(&init_mm.page_table_lock);
 +
 +	if (likely(!pte_none(*ptep))) {
 +		pte_clear(&init_mm, addr, ptep);
 +		free_page(page);
 +	}
 +	spin_unlock(&init_mm.page_table_lock);
 +
 +	return 0;
 +}
 +
 +/*
 + * Release the backing for the vmalloc region [start, end), which
 + * lies within the free region [free_region_start, free_region_end).
 + *
 + * This can be run lazily, long after the region was freed. It runs
 + * under vmap_area_lock, so it's not safe to interact with the vmalloc/vmap
 + * infrastructure.
 + *
 + * How does this work?
 + * -------------------
 + *
 + * We have a region that is page aligned, labelled as A.
 + * That might not map onto the shadow in a way that is page-aligned:
 + *
 + *                    start                     end
 + *                    v                         v
 + * |????????|????????|AAAAAAAA|AA....AA|AAAAAAAA|????????| < vmalloc
 + *  -------- -------- --------          -------- --------
 + *      |        |       |                 |        |
 + *      |        |       |         /-------/        |
 + *      \-------\|/------/         |/---------------/
 + *              |||                ||
 + *             |??AAAAAA|AAAAAAAA|AA??????|                < shadow
 + *                 (1)      (2)      (3)
 + *
 + * First we align the start upwards and the end downwards, so that the
 + * shadow of the region aligns with shadow page boundaries. In the
 + * example, this gives us the shadow page (2). This is the shadow entirely
 + * covered by this allocation.
 + *
 + * Then we have the tricky bits. We want to know if we can free the
 + * partially covered shadow pages - (1) and (3) in the example. For this,
 + * we are given the start and end of the free region that contains this
 + * allocation. Extending our previous example, we could have:
 + *
 + *  free_region_start                                    free_region_end
 + *  |                 start                     end      |
 + *  v                 v                         v        v
 + * |FFFFFFFF|FFFFFFFF|AAAAAAAA|AA....AA|AAAAAAAA|FFFFFFFF| < vmalloc
 + *  -------- -------- --------          -------- --------
 + *      |        |       |                 |        |
 + *      |        |       |         /-------/        |
 + *      \-------\|/------/         |/---------------/
 + *              |||                ||
 + *             |FFAAAAAA|AAAAAAAA|AAF?????|                < shadow
 + *                 (1)      (2)      (3)
 + *
 + * Once again, we align the start of the free region up, and the end of
 + * the free region down so that the shadow is page aligned. So we can free
 + * page (1) - we know no allocation currently uses anything in that page,
 + * because all of it is in the vmalloc free region. But we cannot free
 + * page (3), because we can't be sure that the rest of it is unused.
 + *
 + * We only consider pages that contain part of the original region for
 + * freeing: we don't try to free other pages from the free region or we'd
 + * end up trying to free huge chunks of virtual address space.
 + *
 + * Concurrency
 + * -----------
 + *
 + * How do we know that we're not freeing a page that is simultaneously
 + * being used for a fresh allocation in kasan_populate_vmalloc(_pte)?
 + *
 + * We _can_ have kasan_release_vmalloc and kasan_populate_vmalloc running
 + * at the same time. While we run under free_vmap_area_lock, the population
 + * code does not.
 + *
 + * free_vmap_area_lock instead operates to ensure that the larger range
 + * [free_region_start, free_region_end) is safe: because __alloc_vmap_area and
 + * the per-cpu region-finding algorithm both run under free_vmap_area_lock,
 + * no space identified as free will become used while we are running. This
 + * means that so long as we are careful with alignment and only free shadow
 + * pages entirely covered by the free region, we will not run in to any
 + * trouble - any simultaneous allocations will be for disjoint regions.
 + */
 +void kasan_release_vmalloc(unsigned long start, unsigned long end,
 +			   unsigned long free_region_start,
 +			   unsigned long free_region_end)
 +{
 +	void *shadow_start, *shadow_end;
 +	unsigned long region_start, region_end;
 +	unsigned long size;
 +
 +	region_start = ALIGN(start, PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE);
 +	region_end = ALIGN_DOWN(end, PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE);
 +
 +	free_region_start = ALIGN(free_region_start,
 +				  PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE);
 +
 +	if (start != region_start &&
 +	    free_region_start < region_start)
 +		region_start -= PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE;
 +
 +	free_region_end = ALIGN_DOWN(free_region_end,
 +				     PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE);
 +
 +	if (end != region_end &&
 +	    free_region_end > region_end)
 +		region_end += PAGE_SIZE * KASAN_SHADOW_SCALE_SIZE;
 +
 +	shadow_start = kasan_mem_to_shadow((void *)region_start);
 +	shadow_end = kasan_mem_to_shadow((void *)region_end);
 +
 +	if (shadow_end > shadow_start) {
 +		size = shadow_end - shadow_start;
 +		apply_to_existing_page_range(&init_mm,
 +					     (unsigned long)shadow_start,
 +					     size, kasan_depopulate_vmalloc_pte,
 +					     NULL);
 +		flush_tlb_kernel_range((unsigned long)shadow_start,
 +				       (unsigned long)shadow_end);
 +	}
 +}
 +#endif
++=======
+ bool __kasan_check_byte(const void *address, unsigned long ip)
+ {
+ 	if (!kasan_byte_accessible(address)) {
+ 		kasan_report((unsigned long)address, 1, false, ip);
+ 		return false;
+ 	}
+ 	return true;
+ }
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
diff --cc mm/kasan/generic.c
index d341859a1b95,3f17a1218055..000000000000
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@@ -181,10 -179,17 +181,21 @@@ static __always_inline bool check_memor
  	return !kasan_report(addr, size, write, ret_ip);
  }
  
 -bool kasan_check_range(unsigned long addr, size_t size, bool write,
 -					unsigned long ret_ip)
 +bool check_memory_region(unsigned long addr, size_t size, bool write,
 +				unsigned long ret_ip)
  {
++<<<<<<< HEAD
 +	return check_memory_region_inline(addr, size, write, ret_ip);
++=======
+ 	return check_region_inline(addr, size, write, ret_ip);
+ }
+ 
+ bool kasan_byte_accessible(const void *addr)
+ {
+ 	s8 shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(addr));
+ 
+ 	return shadow_byte >= 0 && shadow_byte < KASAN_GRANULE_SIZE;
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
  }
  
  void kasan_cache_shrink(struct kmem_cache *cache)
diff --cc mm/kasan/kasan.h
index 2db4c5c1b473,cc14b6e6c14c..000000000000
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@@ -247,6 -300,62 +247,65 @@@ static inline const void *arch_kasan_se
  #define hw_get_mem_tag(addr)			arch_get_mem_tag(addr)
  #define hw_set_mem_tag_range(addr, size, tag)	arch_set_mem_tag_range((addr), (size), (tag))
  
++<<<<<<< HEAD
++=======
+ #else /* CONFIG_KASAN_HW_TAGS */
+ 
+ #define hw_enable_tagging()
+ #define hw_set_tagging_report_once(state)
+ 
+ #endif /* CONFIG_KASAN_HW_TAGS */
+ 
+ #if defined(CONFIG_KASAN_HW_TAGS) && IS_ENABLED(CONFIG_KASAN_KUNIT_TEST)
+ 
+ void kasan_set_tagging_report_once(bool state);
+ void kasan_enable_tagging(void);
+ 
+ #else /* CONFIG_KASAN_HW_TAGS || CONFIG_KASAN_KUNIT_TEST */
+ 
+ static inline void kasan_set_tagging_report_once(bool state) { }
+ static inline void kasan_enable_tagging(void) { }
+ 
+ #endif /* CONFIG_KASAN_HW_TAGS || CONFIG_KASAN_KUNIT_TEST */
+ 
+ #ifdef CONFIG_KASAN_SW_TAGS
+ u8 kasan_random_tag(void);
+ #elif defined(CONFIG_KASAN_HW_TAGS)
+ static inline u8 kasan_random_tag(void) { return hw_get_random_tag(); }
+ #else
+ static inline u8 kasan_random_tag(void) { return 0; }
+ #endif
+ 
+ #ifdef CONFIG_KASAN_HW_TAGS
+ 
+ static inline void kasan_poison(const void *address, size_t size, u8 value)
+ {
+ 	hw_set_mem_tag_range(kasan_reset_tag(address),
+ 			round_up(size, KASAN_GRANULE_SIZE), value);
+ }
+ 
+ static inline void kasan_unpoison(const void *address, size_t size)
+ {
+ 	hw_set_mem_tag_range(kasan_reset_tag(address),
+ 			round_up(size, KASAN_GRANULE_SIZE), get_tag(address));
+ }
+ 
+ static inline bool kasan_byte_accessible(const void *addr)
+ {
+ 	u8 ptr_tag = get_tag(addr);
+ 	u8 mem_tag = hw_get_mem_tag((void *)addr);
+ 
+ 	return (mem_tag != KASAN_TAG_INVALID) &&
+ 		(ptr_tag == KASAN_TAG_KERNEL || ptr_tag == mem_tag);
+ }
+ 
+ #else /* CONFIG_KASAN_HW_TAGS */
+ 
+ void kasan_poison(const void *address, size_t size, u8 value);
+ void kasan_unpoison(const void *address, size_t size);
+ bool kasan_byte_accessible(const void *addr);
+ 
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode)
  #endif /* CONFIG_KASAN_HW_TAGS */
  
  /*
diff --cc mm/kasan/tags.c
index 5c8b08a25715,94c2d33be333..000000000000
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@@ -121,6 -118,15 +121,18 @@@ bool check_memory_region(unsigned long 
  	return true;
  }
  
++<<<<<<< HEAD:mm/kasan/tags.c
++=======
+ bool kasan_byte_accessible(const void *addr)
+ {
+ 	u8 tag = get_tag(addr);
+ 	u8 shadow_byte = READ_ONCE(*(u8 *)kasan_mem_to_shadow(kasan_reset_tag(addr)));
+ 
+ 	return (shadow_byte != KASAN_TAG_INVALID) &&
+ 		(tag == KASAN_TAG_KERNEL || tag == shadow_byte);
+ }
+ 
++>>>>>>> 611806b4bf8d (kasan: fix bug detection via ksize for HW_TAGS mode):mm/kasan/sw_tags.c
  #define DEFINE_HWASAN_LOAD_STORE(size)					\
  	void __hwasan_load##size##_noabort(unsigned long addr)		\
  	{								\
diff --git a/include/linux/kasan-checks.h b/include/linux/kasan-checks.h
index ac6aba632f2d..fe59e4d8a305 100644
--- a/include/linux/kasan-checks.h
+++ b/include/linux/kasan-checks.h
@@ -4,6 +4,12 @@
 
 #include <linux/types.h>
 
+/*
+ * The annotations present in this file are only relevant for the software
+ * KASAN modes that rely on compiler instrumentation, and will be optimized
+ * away for the hardware tag-based KASAN mode. Use kasan_check_byte() instead.
+ */
+
 /*
  * __kasan_check_*: Always available when KASAN is enabled. This may be used
  * even in compilation units that selectively disable KASAN, but must use KASAN
* Unmerged path include/linux/kasan.h
* Unmerged path lib/test_kasan.c
* Unmerged path mm/kasan/common.c
* Unmerged path mm/kasan/generic.c
* Unmerged path mm/kasan/kasan.h
* Unmerged path mm/kasan/tags.c
diff --git a/mm/slab_common.c b/mm/slab_common.c
index b485099f723a..f4b149916c86 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1281,19 +1281,21 @@ size_t ksize(const void *objp)
 	size_t size;
 
 	/*
-	 * We need to check that the pointed to object is valid, and only then
-	 * unpoison the shadow memory below. We use __kasan_check_read(), to
-	 * generate a more useful report at the time ksize() is called (rather
-	 * than later where behaviour is undefined due to potential
-	 * use-after-free or double-free).
+	 * We need to first check that the pointer to the object is valid, and
+	 * only then unpoison the memory. The report printed from ksize() is
+	 * more useful, then when it's printed later when the behaviour could
+	 * be undefined due to a potential use-after-free or double-free.
 	 *
-	 * If the pointed to memory is invalid we return 0, to avoid users of
+	 * We use kasan_check_byte(), which is supported for the hardware
+	 * tag-based KASAN mode, unlike kasan_check_read/write().
+	 *
+	 * If the pointed to memory is invalid, we return 0 to avoid users of
 	 * ksize() writing to and potentially corrupting the memory region.
 	 *
 	 * We want to perform the check before __ksize(), to avoid potentially
 	 * crashing in __ksize() due to accessing invalid metadata.
 	 */
-	if (unlikely(ZERO_OR_NULL_PTR(objp)) || !__kasan_check_read(objp, 1))
+	if (unlikely(ZERO_OR_NULL_PTR(objp)) || !kasan_check_byte(objp))
 		return 0;
 
 	size = __ksize(objp);
