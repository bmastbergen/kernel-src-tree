writeback: track number of inodes under writeback

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Jan Kara <jack@suse.cz>
commit 633a2abb9e1cd5c95f3b600f4b2c12cce22ae4a0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/633a2abb.failed

Patch series "writeback: Fix bandwidth estimates", v4.

Fix estimate of writeback throughput when device is not fully busy doing
writeback.  Michael Stapelberg has reported that such workload (e.g.
generated by linking) tends to push estimated throughput down to 0 and as
a result writeback on the device is practically stalled.

The first three patches fix the reported issue, the remaining two patches
are unrelated cleanups of problems I've noticed when reading the code.

This patch (of 4):

Track number of inodes under writeback for each bdi_writeback structure.
We will use this to decide whether wb does any IO and so we can estimate
its writeback throughput.  In principle we could use number of pages under
writeback (WB_WRITEBACK counter) for this however normal percpu counter
reads are too inaccurate for our purposes and summing the counter is too
expensive.

Link: https://lkml.kernel.org/r/20210713104519.16394-1-jack@suse.cz
Link: https://lkml.kernel.org/r/20210713104716.22868-1-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Cc: Wu Fengguang <fengguang.wu@intel.com>
	Cc: Michael Stapelberg <stapelberg+linux@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 633a2abb9e1cd5c95f3b600f4b2c12cce22ae4a0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page-writeback.c
diff --cc mm/page-writeback.c
index ad83d12ad7d6,e1aa1c9d8e36..000000000000
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@@ -2791,8 -2807,13 +2804,18 @@@ int __test_set_page_writeback(struct pa
  						   PAGECACHE_TAG_WRITEBACK);
  
  			xas_set_mark(&xas, PAGECACHE_TAG_WRITEBACK);
++<<<<<<< HEAD
 +			if (bdi_cap_account_writeback(bdi))
 +				inc_wb_stat(inode_to_wb(inode), WB_WRITEBACK);
++=======
+ 			if (bdi->capabilities & BDI_CAP_WRITEBACK_ACCT) {
+ 				struct bdi_writeback *wb = inode_to_wb(inode);
+ 
+ 				inc_wb_stat(wb, WB_WRITEBACK);
+ 				if (!on_wblist)
+ 					wb_inode_writeback_start(wb);
+ 			}
++>>>>>>> 633a2abb9e1c (writeback: track number of inodes under writeback)
  
  			/*
  			 * We can come through here when swapping anonymous
diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index b1955f3daaec..5ed54e2eb7eb 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -415,6 +415,11 @@ static void inode_switch_wbs_work_fn(struct work_struct *work)
 		inc_wb_stat(new_wb, WB_WRITEBACK);
 	}
 
+	if (mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK)) {
+		atomic_dec(&old_wb->writeback_inodes);
+		atomic_inc(&new_wb->writeback_inodes);
+	}
+
 	wb_get(new_wb);
 
 	/*
diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index cd06e2f0c2c6..158a329c0858 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -141,6 +141,7 @@ struct bdi_writeback {
 	struct list_head b_dirty_time;	/* time stamps are dirty */
 	spinlock_t list_lock;		/* protects the b_* lists */
 
+	atomic_t writeback_inodes;	/* number of inodes under writeback */
 	struct percpu_counter stat[NR_WB_STAT_ITEMS];
 
 	struct bdi_writeback_congested *congested;
diff --git a/mm/backing-dev.c b/mm/backing-dev.c
index 38a0dae72f1f..eeaab03a9be8 100644
--- a/mm/backing-dev.c
+++ b/mm/backing-dev.c
@@ -313,6 +313,7 @@ static int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi,
 	INIT_LIST_HEAD(&wb->b_dirty_time);
 	spin_lock_init(&wb->list_lock);
 
+	atomic_set(&wb->writeback_inodes, 0);
 	wb->bw_time_stamp = jiffies;
 	wb->balanced_dirty_ratelimit = INIT_BW;
 	wb->dirty_ratelimit = INIT_BW;
* Unmerged path mm/page-writeback.c
