skmsg: Avoid lock_sock() in sk_psock_backlog()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Cong Wang <cong.wang@bytedance.com>
commit 799aa7f98d53e0f541fa6b4dc9aa47b4ff2178e3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/799aa7f9.failed

We do not have to lock the sock to avoid losing sk_socket,
instead we can purge all the ingress queues when we close
the socket. Sending or receiving packets after orphaning
socket makes no sense.

We do purge these queues when psock refcnt reaches zero but
here we want to purge them explicitly in sock_map_close().
There are also some nasty race conditions on testing bit
SK_PSOCK_TX_ENABLED and queuing/canceling the psock work,
we can expand psock->ingress_lock a bit to protect them too.

As noticed by John, we still have to lock the psock->work,
because the same work item could be running concurrently on
different CPU's.

	Signed-off-by: Cong Wang <cong.wang@bytedance.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
Link: https://lore.kernel.org/bpf/20210331023237.41094-5-xiyou.wangcong@gmail.com
(cherry picked from commit 799aa7f98d53e0f541fa6b4dc9aa47b4ff2178e3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skmsg.c
diff --cc net/core/skmsg.c
index c39c0a361217,9c25020086a9..000000000000
--- a/net/core/skmsg.c
+++ b/net/core/skmsg.c
@@@ -526,10 -524,11 +525,10 @@@ static void sk_psock_backlog(struct wor
  		len = skb->len;
  		off = 0;
  start:
 -		ingress = skb_bpf_ingress(skb);
 -		skb_bpf_redirect_clear(skb);
 +		ingress = tcp_skb_bpf_ingress(skb);
  		do {
  			ret = -EIO;
- 			if (likely(psock->sk->sk_socket))
+ 			if (!sock_flag(psock->sk, SOCK_DEAD))
  				ret = sk_psock_handle_skb(psock, skb, off,
  							  len, ingress);
  			if (ret <= 0) {
@@@ -591,7 -590,9 +590,8 @@@ struct sk_psock *sk_psock_init(struct s
  	spin_lock_init(&psock->link_lock);
  
  	INIT_WORK(&psock->work, sk_psock_backlog);
+ 	mutex_init(&psock->work_mutex);
  	INIT_LIST_HEAD(&psock->ingress_msg);
 -	spin_lock_init(&psock->ingress_lock);
  	skb_queue_head_init(&psock->ingress_skb);
  
  	sk_psock_set_state(psock, SK_PSOCK_TX_ENABLED);
@@@ -630,9 -631,14 +630,18 @@@ static void __sk_psock_purge_ingress_ms
  	}
  }
  
- static void sk_psock_zap_ingress(struct sk_psock *psock)
+ static void __sk_psock_zap_ingress(struct sk_psock *psock)
  {
++<<<<<<< HEAD
 +	__skb_queue_purge(&psock->ingress_skb);
++=======
+ 	struct sk_buff *skb;
+ 
+ 	while ((skb = skb_dequeue(&psock->ingress_skb)) != NULL) {
+ 		skb_bpf_redirect_clear(skb);
+ 		kfree_skb(skb);
+ 	}
++>>>>>>> 799aa7f98d53 (skmsg: Avoid lock_sock() in sk_psock_backlog())
  	__sk_psock_purge_ingress_msg(psock);
  }
  
@@@ -646,17 -652,30 +655,35 @@@ static void sk_psock_link_destroy(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ void sk_psock_stop(struct sk_psock *psock, bool wait)
+ {
+ 	spin_lock_bh(&psock->ingress_lock);
+ 	sk_psock_clear_state(psock, SK_PSOCK_TX_ENABLED);
+ 	sk_psock_cork_free(psock);
+ 	__sk_psock_zap_ingress(psock);
+ 	spin_unlock_bh(&psock->ingress_lock);
+ 
+ 	if (wait)
+ 		cancel_work_sync(&psock->work);
+ }
+ 
+ static void sk_psock_done_strp(struct sk_psock *psock);
+ 
++>>>>>>> 799aa7f98d53 (skmsg: Avoid lock_sock() in sk_psock_backlog())
  static void sk_psock_destroy_deferred(struct work_struct *gc)
  {
  	struct sk_psock *psock = container_of(gc, struct sk_psock, gc);
  
  	/* No sk_callback_lock since already detached. */
  
 -	sk_psock_done_strp(psock);
 +	/* Parser has been stopped */
 +	if (psock->progs.skb_parser)
 +		strp_done(&psock->parser.strp);
  
  	cancel_work_sync(&psock->work);
+ 	mutex_destroy(&psock->work_mutex);
  
  	psock_progs_drop(&psock->progs);
  
@@@ -686,12 -703,11 +711,11 @@@ void sk_psock_drop(struct sock *sk, str
  	write_lock_bh(&sk->sk_callback_lock);
  	sk_psock_restore_proto(sk, psock);
  	rcu_assign_sk_user_data(sk, NULL);
 -	if (psock->progs.stream_parser)
 +	if (psock->progs.skb_parser)
  		sk_psock_stop_strp(sk, psock);
 -	else if (psock->progs.stream_verdict)
 +	else if (psock->progs.skb_verdict)
  		sk_psock_stop_verdict(sk, psock);
  	write_unlock_bh(&sk->sk_callback_lock);
- 	sk_psock_clear_state(psock, SK_PSOCK_TX_ENABLED);
  
  	call_rcu(&psock->rcu, sk_psock_destroy);
  }
diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index a85cacd89635..c1bfa84ea1e9 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -101,6 +101,7 @@ struct sk_psock {
 	void (*saved_close)(struct sock *sk, long timeout);
 	void (*saved_write_space)(struct sock *sk);
 	struct proto			*sk_proto;
+	struct mutex			work_mutex;
 	struct sk_psock_work_state	work_state;
 	struct work_struct		work;
 	union {
@@ -304,6 +305,7 @@ static inline void sk_psock_report_error(struct sk_psock *psock, int err)
 }
 
 struct sk_psock *sk_psock_init(struct sock *sk, int node);
+void sk_psock_stop(struct sk_psock *psock, bool wait);
 
 int sk_psock_init_strp(struct sock *sk, struct sk_psock *psock);
 void sk_psock_start_strp(struct sock *sk, struct sk_psock *psock);
* Unmerged path net/core/skmsg.c
diff --git a/net/core/sock_map.c b/net/core/sock_map.c
index f2963865e78e..40c0795cd9b5 100644
--- a/net/core/sock_map.c
+++ b/net/core/sock_map.c
@@ -1543,6 +1543,7 @@ void sock_map_close(struct sock *sk, long timeout)
 	saved_close = psock->saved_close;
 	sock_map_remove_links(sk, psock);
 	rcu_read_unlock();
+	sk_psock_stop(psock, true);
 	release_sock(sk);
 	saved_close(sk, timeout);
 }
