swiotlb: Convert io_default_tlb_mem to static allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Will Deacon <will@kernel.org>
commit 463e862ac63ef27fca423782536f6465abc3f180
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/463e862a.failed

Since commit 69031f500865 ("swiotlb: Set dev->dma_io_tlb_mem to the
swiotlb pool used"), 'struct device' may hold a copy of the global
'io_default_tlb_mem' pointer if the device is using swiotlb for DMA. A
subsequent call to swiotlb_exit() will therefore leave dangling pointers
behind in these device structures, resulting in KASAN splats such as:

  |  BUG: KASAN: use-after-free in __iommu_dma_unmap_swiotlb+0x64/0xb0
  |  Read of size 8 at addr ffff8881d7830000 by task swapper/0/0
  |
  |  CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.12.0-rc3-debug #1
  |  Hardware name: HP HP Desktop M01-F1xxx/87D6, BIOS F.12 12/17/2020
  |  Call Trace:
  |   <IRQ>
  |   dump_stack+0x9c/0xcf
  |   print_address_description.constprop.0+0x18/0x130
  |   kasan_report.cold+0x7f/0x111
  |   __iommu_dma_unmap_swiotlb+0x64/0xb0
  |   nvme_pci_complete_rq+0x73/0x130
  |   blk_complete_reqs+0x6f/0x80
  |   __do_softirq+0xfc/0x3be

Convert 'io_default_tlb_mem' to a static structure, so that the
per-device pointers remain valid after swiotlb_exit() has been invoked.
All users are updated to reference the static structure directly, using
the 'nslabs' field to determine whether swiotlb has been initialised.
The 'slots' array is still allocated dynamically and referenced via a
pointer rather than a flexible array member.

	Cc: Claire Chang <tientzu@chromium.org>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Robin Murphy <robin.murphy@arm.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Fixes: 69031f500865 ("swiotlb: Set dev->dma_io_tlb_mem to the swiotlb pool used")
	Reported-by: Nathan Chancellor <nathan@kernel.org>
	Tested-by: Nathan Chancellor <nathan@kernel.org>
	Tested-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Will Deacon <will@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad@kernel.org>
(cherry picked from commit 463e862ac63ef27fca423782536f6465abc3f180)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/base/core.c
#	drivers/xen/swiotlb-xen.c
#	include/linux/swiotlb.h
#	kernel/dma/swiotlb.c
diff --cc drivers/base/core.c
index 88a9aafab2f4,b49824001cfa..000000000000
--- a/drivers/base/core.c
+++ b/drivers/base/core.c
@@@ -2579,8 -2840,16 +2579,19 @@@ void device_initialize(struct device *d
  #endif
  	INIT_LIST_HEAD(&dev->links.consumers);
  	INIT_LIST_HEAD(&dev->links.suppliers);
 -	INIT_LIST_HEAD(&dev->links.defer_sync);
 +	INIT_LIST_HEAD(&dev->links_defer_sync);
  	dev->links.status = DL_DEV_NO_DRIVER;
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
+ 	dev->dma_coherent = dma_default_coherent;
+ #endif
+ #ifdef CONFIG_SWIOTLB
+ 	dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ #endif
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  }
  EXPORT_SYMBOL_GPL(device_initialize);
  
diff --cc drivers/xen/swiotlb-xen.c
index 8ccd85660984,f06d9b4f1e0f..000000000000
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@@ -165,20 -155,22 +165,28 @@@ static const char *xen_swiotlb_error(en
  
  #define DEFAULT_NSLABS		ALIGN(SZ_64M >> IO_TLB_SHIFT, IO_TLB_SEGSIZE)
  
 -int __ref xen_swiotlb_init(void)
 +int __ref xen_swiotlb_init(int verbose, bool early)
  {
 -	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
 -	unsigned long bytes = swiotlb_size_or_default();
 -	unsigned long nslabs = bytes >> IO_TLB_SHIFT;
 -	unsigned int order, repeat = 3;
 +	unsigned long bytes, order;
  	int rc = -ENOMEM;
 +	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
 +	unsigned int repeat = 3;
  	char *start;
 +	unsigned long nslabs;
  
++<<<<<<< HEAD
 +	nslabs = swiotlb_nr_tbl();
++=======
+ 	if (io_tlb_default_mem.nslabs) {
+ 		pr_warn("swiotlb buffer already initialized\n");
+ 		return -EEXIST;
+ 	}
+ 
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  retry:
 -	m_ret = XEN_SWIOTLB_ENOMEM;
 +	if (!nslabs)
 +		nslabs = DEFAULT_NSLABS;
 +	bytes = nslabs << IO_TLB_SHIFT;
  	order = get_order(bytes);
  
  	/*
@@@ -526,52 -547,7 +534,56 @@@ xen_swiotlb_sync_sg_for_device(struct d
  static int
  xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
  {
++<<<<<<< HEAD
 +	return xen_phys_to_dma(hwdev, io_tlb_end - 1) <= mask;
 +}
 +
 +/*
 + * Create userspace mapping for the DMA-coherent memory.
 + * This function should be called with the pages from the current domain only,
 + * passing pages mapped from other domains would lead to memory corruption.
 + */
 +static int
 +xen_swiotlb_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 +		     void *cpu_addr, dma_addr_t dma_addr, size_t size,
 +		     unsigned long attrs)
 +{
 +#if defined(CONFIG_ARM) || defined(CONFIG_ARM64)
 +	if (xen_get_dma_ops(dev)->mmap)
 +		return xen_get_dma_ops(dev)->mmap(dev, vma, cpu_addr,
 +						    dma_addr, size, attrs);
 +#endif
 +	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 +}
 +
 +/*
 + * This function should be called with the pages from the current domain only,
 + * passing pages mapped from other domains would lead to memory corruption.
 + */
 +static int
 +xen_swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,
 +			void *cpu_addr, dma_addr_t handle, size_t size,
 +			unsigned long attrs)
 +{
 +#if defined(CONFIG_ARM) || defined(CONFIG_ARM64)
 +	if (xen_get_dma_ops(dev)->get_sgtable) {
 +#if 0
 +	/*
 +	 * This check verifies that the page belongs to the current domain and
 +	 * is not one mapped from another domain.
 +	 * This check is for debug only, and should not go to production build
 +	 */
 +		unsigned long bfn = PHYS_PFN(dma_to_phys(dev, handle));
 +		BUG_ON (!page_is_ram(bfn));
 +#endif
 +		return xen_get_dma_ops(dev)->get_sgtable(dev, sgt, cpu_addr,
 +							   handle, size, attrs);
 +	}
 +#endif
 +	return dma_common_get_sgtable(dev, sgt, cpu_addr, handle, size, attrs);
++=======
+ 	return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  }
  
  const struct dma_map_ops xen_swiotlb_dma_ops = {
diff --cc include/linux/swiotlb.h
index 5857a937c637,b0cb2a9973f4..000000000000
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@@ -71,11 -62,63 +71,59 @@@ dma_addr_t swiotlb_map(struct device *d
  
  #ifdef CONFIG_SWIOTLB
  extern enum swiotlb_force swiotlb_force;
 +extern phys_addr_t io_tlb_start, io_tlb_end;
  
++<<<<<<< HEAD
 +static inline bool is_swiotlb_buffer(phys_addr_t paddr)
++=======
+ /**
+  * struct io_tlb_mem - IO TLB Memory Pool Descriptor
+  *
+  * @start:	The start address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @end:	The end address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @nslabs:	The number of IO TLB blocks (in groups of 64) between @start and
+  *		@end. For default swiotlb, this is command line adjustable via
+  *		setup_io_tlb_npages.
+  * @used:	The number of used IO TLB block.
+  * @list:	The free list describing the number of free entries available
+  *		from each index.
+  * @index:	The index to start searching in the next round.
+  * @orig_addr:	The original address corresponding to a mapped entry.
+  * @alloc_size:	Size of the allocated buffer.
+  * @lock:	The lock to protect the above data structures in the map and
+  *		unmap calls.
+  * @debugfs:	The dentry to debugfs.
+  * @late_alloc:	%true if allocated using the page allocator
+  * @force_bounce: %true if swiotlb bouncing is forced
+  * @for_alloc:  %true if the pool is used for memory allocation
+  */
+ struct io_tlb_mem {
+ 	phys_addr_t start;
+ 	phys_addr_t end;
+ 	unsigned long nslabs;
+ 	unsigned long used;
+ 	unsigned int index;
+ 	spinlock_t lock;
+ 	struct dentry *debugfs;
+ 	bool late_alloc;
+ 	bool force_bounce;
+ 	bool for_alloc;
+ 	struct io_tlb_slot {
+ 		phys_addr_t orig_addr;
+ 		size_t alloc_size;
+ 		unsigned int list;
+ 	} *slots;
+ };
+ extern struct io_tlb_mem io_tlb_default_mem;
+ 
+ static inline bool is_swiotlb_buffer(struct device *dev, phys_addr_t paddr)
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  {
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && paddr >= mem->start && paddr < mem->end;
 -}
 -
 -static inline bool is_swiotlb_force_bounce(struct device *dev)
 -{
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && mem->force_bounce;
 +	return paddr >= io_tlb_start && paddr < io_tlb_end;
  }
  
  void __init swiotlb_exit(void);
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,7948f274f9bb..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -58,32 -66,11 +58,36 @@@
   */
  #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
  
 -#define INVALID_PHYS_ADDR (~(phys_addr_t)0)
 -
  enum swiotlb_force swiotlb_force;
  
++<<<<<<< HEAD
 +/*
 + * Used to do a quick range check in swiotlb_tbl_unmap_single and
 + * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
 + * API.
 + */
 +phys_addr_t io_tlb_start, io_tlb_end;
 +
 +/*
 + * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
 + * io_tlb_end.  This is command line adjustable via setup_io_tlb_npages.
 + */
 +static unsigned long io_tlb_nslabs;
 +
 +/*
 + * The number of used IO TLB block
 + */
 +static unsigned long io_tlb_used;
 +
 +/*
 + * This is a free list describing the number of free entries available from
 + * each index
 + */
 +static unsigned int *io_tlb_list;
 +static unsigned int io_tlb_index;
++=======
+ struct io_tlb_mem io_tlb_default_mem;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  
  /*
   * Max segment that we can provide which (if pages are contingous) will
@@@ -131,17 -99,9 +135,21 @@@ setup_io_tlb_npages(char *str
  }
  early_param("swiotlb", setup_io_tlb_npages);
  
 +static bool no_iotlb_memory;
 +
 +unsigned long swiotlb_nr_tbl(void)
 +{
 +	return unlikely(no_iotlb_memory) ? 0 : io_tlb_nslabs;
 +}
 +EXPORT_SYMBOL_GPL(swiotlb_nr_tbl);
 +
  unsigned int swiotlb_max_segment(void)
  {
++<<<<<<< HEAD
 +	return unlikely(no_iotlb_memory) ? 0 : max_segment;
++=======
+ 	return io_tlb_default_mem.nslabs ? max_segment : 0;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  }
  EXPORT_SYMBOL_GPL(swiotlb_max_segment);
  
@@@ -182,9 -134,9 +190,15 @@@ void __init swiotlb_adjust_size(unsigne
  
  void swiotlb_print_info(void)
  {
++<<<<<<< HEAD
 +	unsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;
 +
 +	if (no_iotlb_memory) {
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 
+ 	if (!mem->nslabs) {
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  		pr_warn("No low mem\n");
  		return;
  	}
@@@ -211,41 -163,57 +225,62 @@@ static inline unsigned long nr_slots(u6
   */
  void __init swiotlb_update_mem_attributes(void)
  {
++<<<<<<< HEAD
 +	void *vaddr;
 +	unsigned long bytes;
 +
 +	if (no_iotlb_memory || late_alloc)
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 	void *vaddr;
+ 	unsigned long bytes;
+ 
+ 	if (!mem->nslabs || mem->late_alloc)
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  		return;
 -	vaddr = phys_to_virt(mem->start);
 -	bytes = PAGE_ALIGN(mem->nslabs << IO_TLB_SHIFT);
 -	set_memory_decrypted((unsigned long)vaddr, bytes >> PAGE_SHIFT);
 -	memset(vaddr, 0, bytes);
 -}
 -
 -static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 -				    unsigned long nslabs, bool late_alloc)
 -{
 -	void *vaddr = phys_to_virt(start);
 -	unsigned long bytes = nslabs << IO_TLB_SHIFT, i;
 -
 -	mem->nslabs = nslabs;
 -	mem->start = start;
 -	mem->end = mem->start + bytes;
 -	mem->index = 0;
 -	mem->late_alloc = late_alloc;
  
 -	if (swiotlb_force == SWIOTLB_FORCE)
 -		mem->force_bounce = true;
 -
 -	spin_lock_init(&mem->lock);
 -	for (i = 0; i < mem->nslabs; i++) {
 -		mem->slots[i].list = IO_TLB_SEGSIZE - io_tlb_offset(i);
 -		mem->slots[i].orig_addr = INVALID_PHYS_ADDR;
 -		mem->slots[i].alloc_size = 0;
 -	}
 +	vaddr = phys_to_virt(io_tlb_start);
 +	bytes = PAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT);
 +	set_memory_decrypted((unsigned long)vaddr, bytes >> PAGE_SHIFT);
  	memset(vaddr, 0, bytes);
  }
  
  int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  {
++<<<<<<< HEAD
 +	unsigned long i, bytes;
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  	size_t alloc_size;
  
 -	if (swiotlb_force == SWIOTLB_NO_FORCE)
 -		return 0;
 -
  	/* protect against double initialization */
++<<<<<<< HEAD
 +	if (WARN_ON_ONCE(io_tlb_start))
 +		return -ENOMEM;
 +
 +	bytes = nslabs << IO_TLB_SHIFT;
 +
 +	io_tlb_nslabs = nslabs;
 +	io_tlb_start = __pa(tlb);
 +	io_tlb_end = io_tlb_start + bytes;
 +
 +	/*
 +	 * Allocate and initialize the free list array.  This array is used
 +	 * to find contiguous free memory regions of size up to IO_TLB_SEGSIZE
 +	 * between io_tlb_start and io_tlb_end.
 +	 */
 +	alloc_size = PAGE_ALIGN(io_tlb_nslabs * sizeof(int));
 +	io_tlb_list = memblock_alloc(alloc_size, PAGE_SIZE);
 +	if (!io_tlb_list)
++=======
+ 	if (WARN_ON_ONCE(mem->nslabs))
+ 		return -ENOMEM;
+ 
+ 	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
+ 	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
+ 	if (!mem->slots)
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
  		      __func__, alloc_size, PAGE_SIZE);
  
@@@ -368,100 -303,51 +403,140 @@@ static void swiotlb_cleanup(void
  int
  swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
  {
++<<<<<<< HEAD
 +	unsigned long i, bytes;
 +
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(io_tlb_start))
 +		return -ENOMEM;
 +
 +	bytes = nslabs << IO_TLB_SHIFT;
 +
 +	io_tlb_nslabs = nslabs;
 +	io_tlb_start = virt_to_phys(tlb);
 +	io_tlb_end = io_tlb_start + bytes;
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 	unsigned long bytes = nslabs << IO_TLB_SHIFT;
+ 
+ 	if (swiotlb_force == SWIOTLB_NO_FORCE)
+ 		return 0;
+ 
+ 	/* protect against double initialization */
+ 	if (WARN_ON_ONCE(mem->nslabs))
+ 		return -ENOMEM;
+ 
+ 	mem->slots = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,
+ 		get_order(array_size(sizeof(*mem->slots), nslabs)));
+ 	if (!mem->slots)
+ 		return -ENOMEM;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  
  	set_memory_decrypted((unsigned long)tlb, bytes >> PAGE_SHIFT);
 -	swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
 +	memset(tlb, 0, bytes);
 +
 +	/*
 +	 * Allocate and initialize the free list array.  This array is used
 +	 * to find contiguous free memory regions of size up to IO_TLB_SEGSIZE
 +	 * between io_tlb_start and io_tlb_end.
 +	 */
 +	io_tlb_list = (unsigned int *)__get_free_pages(GFP_KERNEL,
 +				      get_order(io_tlb_nslabs * sizeof(int)));
 +	if (!io_tlb_list)
 +		goto cleanup3;
 +
 +	io_tlb_orig_addr = (phys_addr_t *)
 +		__get_free_pages(GFP_KERNEL,
 +				 get_order(io_tlb_nslabs *
 +					   sizeof(phys_addr_t)));
 +	if (!io_tlb_orig_addr)
 +		goto cleanup4;
 +
 +	io_tlb_orig_size = (size_t *)
 +		__get_free_pages(GFP_KERNEL,
 +				 get_order(io_tlb_nslabs *
 +					   sizeof(size_t)));
 +	if (!io_tlb_orig_size)
 +		goto cleanup5;
 +
 +
 +	for (i = 0; i < io_tlb_nslabs; i++) {
 +		io_tlb_list[i] = IO_TLB_SEGSIZE - io_tlb_offset(i);
 +		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 +		io_tlb_orig_size[i] = 0;
 +	}
 +	io_tlb_index = 0;
 +	no_iotlb_memory = false;
  
  	swiotlb_print_info();
 -	swiotlb_set_max_segment(mem->nslabs << IO_TLB_SHIFT);
 +
 +	late_alloc = 1;
 +
 +	swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
 +
  	return 0;
 +
 +cleanup5:
 +	free_pages((unsigned long)io_tlb_orig_addr, get_order(io_tlb_nslabs *
 +							      sizeof(phys_addr_t)));
 +
 +cleanup4:
 +	free_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *
 +	                                                 sizeof(int)));
 +	io_tlb_list = NULL;
 +cleanup3:
 +	swiotlb_cleanup();
 +	return -ENOMEM;
  }
  
  void __init swiotlb_exit(void)
  {
++<<<<<<< HEAD
 +	if (!io_tlb_orig_addr)
 +		return;
 +
 +	if (late_alloc) {
 +		free_pages((unsigned long)io_tlb_orig_size,
 +			   get_order(io_tlb_nslabs * sizeof(size_t)));
 +		free_pages((unsigned long)io_tlb_orig_addr,
 +			   get_order(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		free_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *
 +								 sizeof(int)));
 +		free_pages((unsigned long)phys_to_virt(io_tlb_start),
 +			   get_order(io_tlb_nslabs << IO_TLB_SHIFT));
 +	} else {
 +		memblock_free_late(__pa(io_tlb_orig_addr),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));
 +		memblock_free_late(__pa(io_tlb_orig_size),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(size_t)));
 +		memblock_free_late(__pa(io_tlb_list),
 +				   PAGE_ALIGN(io_tlb_nslabs * sizeof(int)));
 +		memblock_free_late(io_tlb_start,
 +				   PAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));
 +	}
 +	swiotlb_cleanup();
++=======
+ 	size_t size;
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 
+ 	if (!mem->nslabs)
+ 		return;
+ 
+ 	size = array_size(sizeof(*mem->slots), mem->nslabs);
+ 	if (mem->late_alloc)
+ 		free_pages((unsigned long)mem->slots, get_order(size));
+ 	else
+ 		memblock_free_late(__pa(mem->slots), PAGE_ALIGN(size));
+ 	memset(mem, 0, sizeof(*mem));
+ }
+ 
+ /*
+  * Return the offset into a iotlb slot required to keep the device happy.
+  */
+ static unsigned int swiotlb_align_offset(struct device *dev, u64 addr)
+ {
+ 	return addr & dma_get_min_align_mask(dev) & (IO_TLB_SIZE - 1);
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  }
  
  /*
@@@ -796,27 -691,155 +871,141 @@@ size_t swiotlb_max_mapping_size(struct 
  	return ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE;
  }
  
 -bool is_swiotlb_active(struct device *dev)
 +bool is_swiotlb_active(void)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 
+ 	return mem && mem->nslabs;
+ }
+ EXPORT_SYMBOL_GPL(is_swiotlb_active);
+ 
+ #ifdef CONFIG_DEBUG_FS
+ static struct dentry *debugfs_dir;
+ 
+ static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem)
+ {
+ 	debugfs_create_ulong("io_tlb_nslabs", 0400, mem->debugfs, &mem->nslabs);
+ 	debugfs_create_ulong("io_tlb_used", 0400, mem->debugfs, &mem->used);
+ }
+ 
+ static int __init swiotlb_create_default_debugfs(void)
+ {
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 
+ 	debugfs_dir = debugfs_create_dir("swiotlb", NULL);
+ 	if (mem->nslabs) {
+ 		mem->debugfs = debugfs_dir;
+ 		swiotlb_create_debugfs_files(mem);
+ 	}
+ 	return 0;
+ }
+ 
+ late_initcall(swiotlb_create_default_debugfs);
+ 
+ #endif
+ 
+ #ifdef CONFIG_DMA_RESTRICTED_POOL
+ 
+ #ifdef CONFIG_DEBUG_FS
+ static void rmem_swiotlb_debugfs_init(struct reserved_mem *rmem)
+ {
+ 	struct io_tlb_mem *mem = rmem->priv;
+ 
+ 	mem->debugfs = debugfs_create_dir(rmem->name, debugfs_dir);
+ 	swiotlb_create_debugfs_files(mem);
+ }
+ #else
+ static void rmem_swiotlb_debugfs_init(struct reserved_mem *rmem)
+ {
+ }
+ #endif
+ 
+ struct page *swiotlb_alloc(struct device *dev, size_t size)
+ {
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 	phys_addr_t tlb_addr;
+ 	int index;
+ 
+ 	if (!mem)
+ 		return NULL;
+ 
+ 	index = swiotlb_find_slots(dev, 0, size);
+ 	if (index == -1)
+ 		return NULL;
+ 
+ 	tlb_addr = slot_addr(mem->start, index);
+ 
+ 	return pfn_to_page(PFN_DOWN(tlb_addr));
+ }
+ 
+ bool swiotlb_free(struct device *dev, struct page *page, size_t size)
+ {
+ 	phys_addr_t tlb_addr = page_to_phys(page);
+ 
+ 	if (!is_swiotlb_buffer(dev, tlb_addr))
+ 		return false;
+ 
+ 	swiotlb_release_slots(dev, tlb_addr);
+ 
+ 	return true;
+ }
+ 
+ static int rmem_swiotlb_device_init(struct reserved_mem *rmem,
+ 				    struct device *dev)
+ {
+ 	struct io_tlb_mem *mem = rmem->priv;
+ 	unsigned long nslabs = rmem->size >> IO_TLB_SHIFT;
+ 
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  	/*
 -	 * Since multiple devices can share the same pool, the private data,
 -	 * io_tlb_mem struct, will be initialized by the first device attached
 -	 * to it.
 +	 * When SWIOTLB is initialized, even if io_tlb_start points to physical
 +	 * address zero, io_tlb_end surely doesn't.
  	 */
++<<<<<<< HEAD
 +	return io_tlb_end != 0;
 +}
 +
 +#ifdef CONFIG_DEBUG_FS
++=======
+ 	if (!mem) {
+ 		mem = kzalloc(sizeof(*mem), GFP_KERNEL);
+ 		if (!mem)
+ 			return -ENOMEM;
+ 
+ 		mem->slots = kzalloc(array_size(sizeof(*mem->slots), nslabs),
+ 				     GFP_KERNEL);
+ 		if (!mem->slots) {
+ 			kfree(mem);
+ 			return -ENOMEM;
+ 		}
+ 
+ 		set_memory_decrypted((unsigned long)phys_to_virt(rmem->base),
+ 				     rmem->size >> PAGE_SHIFT);
+ 		swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ 		mem->force_bounce = true;
+ 		mem->for_alloc = true;
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  
 -		rmem->priv = mem;
 -
 -		rmem_swiotlb_debugfs_init(rmem);
 -	}
 -
 -	dev->dma_io_tlb_mem = mem;
 +static int __init swiotlb_create_debugfs(void)
 +{
 +	struct dentry *root;
  
 +	root = debugfs_create_dir("swiotlb", NULL);
 +	debugfs_create_ulong("io_tlb_nslabs", 0400, root, &io_tlb_nslabs);
 +	debugfs_create_ulong("io_tlb_used", 0400, root, &io_tlb_used);
  	return 0;
  }
  
++<<<<<<< HEAD
 +late_initcall(swiotlb_create_debugfs);
++=======
+ static void rmem_swiotlb_device_release(struct reserved_mem *rmem,
+ 					struct device *dev)
+ {
+ 	dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ }
++>>>>>>> 463e862ac63e (swiotlb: Convert io_default_tlb_mem to static allocation)
  
 -static const struct reserved_mem_ops rmem_swiotlb_ops = {
 -	.device_init = rmem_swiotlb_device_init,
 -	.device_release = rmem_swiotlb_device_release,
 -};
 -
 -static int __init rmem_swiotlb_setup(struct reserved_mem *rmem)
 -{
 -	unsigned long node = rmem->fdt_node;
 -
 -	if (of_get_flat_dt_prop(node, "reusable", NULL) ||
 -	    of_get_flat_dt_prop(node, "linux,cma-default", NULL) ||
 -	    of_get_flat_dt_prop(node, "linux,dma-default", NULL) ||
 -	    of_get_flat_dt_prop(node, "no-map", NULL))
 -		return -EINVAL;
 -
 -	if (PageHighMem(pfn_to_page(PHYS_PFN(rmem->base)))) {
 -		pr_err("Restricted DMA pool must be accessible within the linear mapping.");
 -		return -EINVAL;
 -	}
 -
 -	rmem->ops = &rmem_swiotlb_ops;
 -	pr_info("Reserved memory: created restricted DMA pool at %pa, size %ld MiB\n",
 -		&rmem->base, (unsigned long)rmem->size / SZ_1M);
 -	return 0;
 -}
 -
 -RESERVEDMEM_OF_DECLARE(dma, "restricted-dma-pool", rmem_swiotlb_setup);
 -#endif /* CONFIG_DMA_RESTRICTED_POOL */
 +#endif
* Unmerged path drivers/base/core.c
* Unmerged path drivers/xen/swiotlb-xen.c
* Unmerged path include/linux/swiotlb.h
* Unmerged path kernel/dma/swiotlb.c
