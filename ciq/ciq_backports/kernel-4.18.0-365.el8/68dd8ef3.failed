arm64: memory: Fix virt_addr_valid() using __is_lm_address()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Will Deacon <will@kernel.org>
commit 68dd8ef321626f14ae9ef2039b7a03c707149489
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/68dd8ef3.failed

virt_addr_valid() is intended to test whether or not the passed address
is a valid linear map address. Unfortunately, it relies on
_virt_addr_is_linear() which is broken because it assumes the linear
map is at the top of the address space, which it no longer is.

Reimplement virt_addr_valid() using __is_lm_address() and remove
_virt_addr_is_linear() entirely. At the same time, ensure we evaluate
the macro parameter only once and move it within the __ASSEMBLY__ block.

	Reported-by: Qian Cai <cai@lca.pw>
	Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
	Tested-by: Steve Capper <steve.capper@arm.com>
	Reviewed-by: Steve Capper <steve.capper@arm.com>
	Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Reviewed-by: Mark Rutland <mark.rutland@arm.com>
Fixes: 14c127c957c1 ("arm64: mm: Flip kernel VA space")
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 68dd8ef321626f14ae9ef2039b7a03c707149489)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/memory.h
diff --cc arch/arm64/include/asm/memory.h
index 630eaf618e50,93ef8e5c6971..000000000000
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@@ -268,13 -242,13 +268,17 @@@ static inline const void *__tag_set(con
  
  
  /*
-  * The linear kernel range starts in the middle of the virtual adddress
+  * The linear kernel range starts at the bottom of the virtual address
   * space. Testing the top bit for the start of the region is a
-  * sufficient check.
+  * sufficient check and avoids having to worry about the tag.
   */
++<<<<<<< HEAD
 +#define __is_lm_address(addr)	(!((addr) & BIT(VA_BITS - 1)))
++=======
+ #define __is_lm_address(addr)	(!(((u64)addr) & BIT(vabits_actual - 1)))
++>>>>>>> 68dd8ef32162 (arm64: memory: Fix virt_addr_valid() using __is_lm_address())
  
 -#define __lm_to_phys(addr)	(((addr) + physvirt_offset))
 +#define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
  #define __kimg_to_phys(addr)	((addr) - kimage_voffset)
  
  #define __virt_to_phys_nodebug(x) ({					\
@@@ -350,16 -324,26 +354,35 @@@ static inline void *phys_to_virt(phys_a
  	((void *)__addr_tag);						\
  })
  
 -#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) + VMEMMAP_START))
 +#define virt_to_page(vaddr)	\
 +	((struct page *)((__virt_to_pgoff(__tag_reset(vaddr))) + VMEMMAP_START))
  #endif
+ 
+ #define virt_addr_valid(addr)	({					\
+ 	__typeof__(addr) __addr = addr;					\
+ 	__is_lm_address(__addr) && pfn_valid(virt_to_pfn(__addr));	\
+ })
+ 
  #endif
  
++<<<<<<< HEAD
 +#define _virt_addr_is_linear(kaddr)	\
 +	(__tag_reset((u64)(kaddr)) >= PAGE_OFFSET)
 +
 +#define virt_addr_valid(kaddr)		\
 +	(_virt_addr_is_linear(kaddr) && pfn_valid(virt_to_pfn(kaddr)))
++=======
+ /*
+  * Given that the GIC architecture permits ITS implementations that can only be
+  * configured with a LPI table address once, GICv3 systems with many CPUs may
+  * end up reserving a lot of different regions after a kexec for their LPI
+  * tables (one per CPU), as we are forced to reuse the same memory after kexec
+  * (and thus reserve it persistently with EFI beforehand)
+  */
+ #if defined(CONFIG_EFI) && defined(CONFIG_ARM_GIC_V3_ITS)
+ # define INIT_MEMBLOCK_RESERVED_REGIONS	(INIT_MEMBLOCK_REGIONS + NR_CPUS + 1)
+ #endif
++>>>>>>> 68dd8ef32162 (arm64: memory: Fix virt_addr_valid() using __is_lm_address())
  
  #include <asm-generic/memory_model.h>
  
* Unmerged path arch/arm64/include/asm/memory.h
