x86/fpu/core: Convert to fpstate

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit c20942ce5128ef92e2c451f943ba33462ad2fbc4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/c20942ce.failed

Convert the rest of the core code to the new register storage mechanism in
preparation for dynamically sized buffers.

No functional change.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20211013145322.659456185@linutronix.de
(cherry picked from commit c20942ce5128ef92e2c451f943ba33462ad2fbc4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/core.c
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/kernel/fpu/core.c
index 3f16056105e8,14560fda15c2..000000000000
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@@ -104,7 -105,14 +104,18 @@@ EXPORT_SYMBOL(irq_fpu_usable)
  void save_fpregs_to_fpstate(struct fpu *fpu)
  {
  	if (likely(use_xsave())) {
++<<<<<<< HEAD
 +		os_xsave(&fpu->state.xsave);
++=======
+ 		os_xsave(&fpu->fpstate->regs.xsave);
+ 
+ 		/*
+ 		 * AVX512 state is tracked here because its use is
+ 		 * known to slow the max clock speed of the core.
+ 		 */
+ 		if (fpu->fpstate->regs.xsave.header.xfeatures & XFEATURE_MASK_AVX512)
+ 			fpu->avx512_timestamp = jiffies;
++>>>>>>> c20942ce5128 (x86/fpu/core: Convert to fpstate)
  		return;
  	}
  
@@@ -117,11 -125,11 +128,11 @@@
  	 * Legacy FPU register saving, FNSAVE always clears FPU registers,
  	 * so we have to reload them from the memory state.
  	 */
- 	asm volatile("fnsave %[fp]; fwait" : [fp] "=m" (fpu->state.fsave));
- 	frstor(&fpu->state.fsave);
+ 	asm volatile("fnsave %[fp]; fwait" : [fp] "=m" (fpu->fpstate->regs.fsave));
+ 	frstor(&fpu->fpstate->regs.fsave);
  }
  
 -void restore_fpregs_from_fpstate(struct fpstate *fpstate, u64 mask)
 +void restore_fpregs_from_fpstate(union fpregs_state *fpstate, u64 mask)
  {
  	/*
  	 * AMD K7/K8 and later CPUs up to Zen don't save/restore
@@@ -356,7 -379,7 +368,11 @@@ int fpu_clone(struct task_struct *dst
  	 */
  	if (dst->flags & (PF_KTHREAD | PF_IO_WORKER)) {
  		/* Clear out the minimal state */
++<<<<<<< HEAD
 +		memcpy(&dst_fpu->state, &init_fpstate,
++=======
+ 		memcpy(&dst_fpu->fpstate->regs, &init_fpstate.regs,
++>>>>>>> c20942ce5128 (x86/fpu/core: Convert to fpstate)
  		       init_fpstate_copy_size());
  		return 0;
  	}
@@@ -444,7 -468,7 +461,11 @@@ static void fpu_reset_fpstate(void
  	 * user space as PKRU is eagerly written in switch_to() and
  	 * flush_thread().
  	 */
++<<<<<<< HEAD
 +	memcpy(&fpu->state, &init_fpstate, init_fpstate_copy_size());
++=======
+ 	memcpy(&fpu->fpstate->regs, &init_fpstate.regs, init_fpstate_copy_size());
++>>>>>>> c20942ce5128 (x86/fpu/core: Convert to fpstate)
  	set_thread_flag(TIF_NEED_FPU_LOAD);
  	fpregs_unlock();
  }
diff --cc arch/x86/kernel/fpu/xstate.c
index 8c27dcecbad5,ca72a3e9080c..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1110,6 -1079,25 +1110,28 @@@ out
  		membuf_zero(&to, to.left);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
+  * @to:		membuf descriptor
+  * @tsk:	The task from which to copy the saved xstate
+  * @copy_mode:	The requested copy mode
+  *
+  * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
+  * format, i.e. from the kernel internal hardware dependent storage format
+  * to the requested @mode. UABI XSTATE is always uncompacted!
+  *
+  * It supports partial copy but @to.pos always starts from zero.
+  */
+ void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 			     enum xstate_copy_mode copy_mode)
+ {
+ 	__copy_xstate_to_uabi_buf(to, &tsk->thread.fpu.fpstate->regs.xsave,
+ 				  tsk->thread.pkru, copy_mode);
+ }
+ 
++>>>>>>> c20942ce5128 (x86/fpu/core: Convert to fpstate)
  static int copy_from_buffer(void *dst, unsigned int offset, unsigned int size,
  			    const void *kbuf, const void __user *ubuf)
  {
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index 9833eb36228b..0858b1894907 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -50,9 +50,9 @@ static inline void kernel_fpu_begin(void)
 }
 
 /*
- * Use fpregs_lock() while editing CPU's FPU registers or fpu->state.
+ * Use fpregs_lock() while editing CPU's FPU registers or fpu->fpstate.
  * A context switch will (and softirq might) save CPU's FPU registers to
- * fpu->state and set TIF_NEED_FPU_LOAD leaving CPU's FPU registers in
+ * fpu->fpstate.regs and set TIF_NEED_FPU_LOAD leaving CPU's FPU registers in
  * a random state.
  */
 static inline void fpregs_lock(void)
* Unmerged path arch/x86/kernel/fpu/core.c
diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c
index 24873dfe2dba..fca42bde8c89 100644
--- a/arch/x86/kernel/fpu/init.c
+++ b/arch/x86/kernel/fpu/init.c
@@ -36,7 +36,7 @@ static void fpu__init_cpu_generic(void)
 	/* Flush out any pending x87 state: */
 #ifdef CONFIG_MATH_EMULATION
 	if (!boot_cpu_has(X86_FEATURE_FPU))
-		fpstate_init_soft(&current->thread.fpu.state.soft);
+		fpstate_init_soft(&current->thread.fpu.fpstate->regs.soft);
 	else
 #endif
 		asm volatile ("fninit");
* Unmerged path arch/x86/kernel/fpu/xstate.c
