kasan, mm: allow cache merging with no metadata

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit e86f8b09f215e3755cd2d56930487dec2de02433
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/e86f8b09.failed

The reason cache merging is disabled with KASAN is because KASAN puts its
metadata right after the allocated object. When the merged caches have
slightly different sizes, the metadata ends up in different places, which
KASAN doesn't support.

It might be possible to adjust the metadata allocation algorithm and make
it friendly to the cache merging code. Instead this change takes a simpler
approach and allows merging caches when no metadata is present. Which is
the case for hardware tag-based KASAN with kasan.mode=prod.

Link: https://lkml.kernel.org/r/37497e940bfd4b32c0a93a702a9ae4cf061d5392.1606162397.git.andreyknvl@google.com
Link: https://linux-review.googlesource.com/id/Ia114847dfb2244f297d2cb82d592bf6a07455dba
Co-developed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
	Signed-off-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
	Reviewed-by: Marco Elver <elver@google.com>
	Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Branislav Rankov <Branislav.Rankov@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Evgenii Stepanov <eugenis@google.com>
	Cc: Kevin Brodsky <kevin.brodsky@arm.com>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e86f8b09f215e3755cd2d56930487dec2de02433)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	mm/kasan/common.c
diff --cc include/linux/kasan.h
index f00d17cf6822,5e0655fb2a6f..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -69,44 -81,184 +69,84 @@@ struct kasan_cache 
  	int free_meta_offset;
  };
  
++<<<<<<< HEAD
 +/*
 + * These functions provide a special case to support backing module
 + * allocations with real shadow memory. With KASAN vmalloc, the special
 + * case is unnecessary, as the work is handled in the generic case.
 + */
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size);
 +void kasan_free_shadow(const struct vm_struct *vm);
 +#else
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +#endif
++=======
+ #ifdef CONFIG_KASAN_HW_TAGS
+ 
+ DECLARE_STATIC_KEY_FALSE(kasan_flag_enabled);
+ 
+ static __always_inline bool kasan_enabled(void)
+ {
+ 	return static_branch_likely(&kasan_flag_enabled);
+ }
+ 
+ #else /* CONFIG_KASAN_HW_TAGS */
+ 
+ static inline bool kasan_enabled(void)
+ {
+ 	return true;
+ }
+ 
+ #endif /* CONFIG_KASAN_HW_TAGS */
+ 
+ slab_flags_t __kasan_never_merge(void);
+ static __always_inline slab_flags_t kasan_never_merge(void)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_never_merge();
+ 	return 0;
+ }
++>>>>>>> e86f8b09f215 (kasan, mm: allow cache merging with no metadata)
  
 -void __kasan_unpoison_range(const void *addr, size_t size);
 -static __always_inline void kasan_unpoison_range(const void *addr, size_t size)
 -{
 -	if (kasan_enabled())
 -		__kasan_unpoison_range(addr, size);
 -}
 -
 -void __kasan_alloc_pages(struct page *page, unsigned int order);
 -static __always_inline void kasan_alloc_pages(struct page *page,
 -						unsigned int order)
 -{
 -	if (kasan_enabled())
 -		__kasan_alloc_pages(page, order);
 -}
 -
 -void __kasan_free_pages(struct page *page, unsigned int order);
 -static __always_inline void kasan_free_pages(struct page *page,
 -						unsigned int order)
 -{
 -	if (kasan_enabled())
 -		__kasan_free_pages(page, order);
 -}
 -
 -void __kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 -				slab_flags_t *flags);
 -static __always_inline void kasan_cache_create(struct kmem_cache *cache,
 -				unsigned int *size, slab_flags_t *flags)
 -{
 -	if (kasan_enabled())
 -		__kasan_cache_create(cache, size, flags);
 -}
 -
 -size_t __kasan_metadata_size(struct kmem_cache *cache);
 -static __always_inline size_t kasan_metadata_size(struct kmem_cache *cache)
 -{
 -	if (kasan_enabled())
 -		return __kasan_metadata_size(cache);
 -	return 0;
 -}
 -
 -void __kasan_poison_slab(struct page *page);
 -static __always_inline void kasan_poison_slab(struct page *page)
 -{
 -	if (kasan_enabled())
 -		__kasan_poison_slab(page);
 -}
 -
 -void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 -static __always_inline void kasan_unpoison_object_data(struct kmem_cache *cache,
 -							void *object)
 -{
 -	if (kasan_enabled())
 -		__kasan_unpoison_object_data(cache, object);
 -}
 -
 -void __kasan_poison_object_data(struct kmem_cache *cache, void *object);
 -static __always_inline void kasan_poison_object_data(struct kmem_cache *cache,
 -							void *object)
 -{
 -	if (kasan_enabled())
 -		__kasan_poison_object_data(cache, object);
 -}
 -
 -void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
 -					  const void *object);
 -static __always_inline void * __must_check kasan_init_slab_obj(
 -				struct kmem_cache *cache, const void *object)
 -{
 -	if (kasan_enabled())
 -		return __kasan_init_slab_obj(cache, object);
 -	return (void *)object;
 -}
 -
 -bool __kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 -static __always_inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 -						unsigned long ip)
 -{
 -	if (kasan_enabled())
 -		return __kasan_slab_free(s, object, ip);
 -	return false;
 -}
 -
 -void __kasan_slab_free_mempool(void *ptr, unsigned long ip);
 -static __always_inline void kasan_slab_free_mempool(void *ptr, unsigned long ip)
 -{
 -	if (kasan_enabled())
 -		__kasan_slab_free_mempool(ptr, ip);
 -}
 -
 -void * __must_check __kasan_slab_alloc(struct kmem_cache *s,
 -				       void *object, gfp_t flags);
 -static __always_inline void * __must_check kasan_slab_alloc(
 -				struct kmem_cache *s, void *object, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_slab_alloc(s, object, flags);
 -	return object;
 -}
 -
 -void * __must_check __kasan_kmalloc(struct kmem_cache *s, const void *object,
 -				    size_t size, gfp_t flags);
 -static __always_inline void * __must_check kasan_kmalloc(struct kmem_cache *s,
 -				const void *object, size_t size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_kmalloc(s, object, size, flags);
 -	return (void *)object;
 -}
 -
 -void * __must_check __kasan_kmalloc_large(const void *ptr,
 -					  size_t size, gfp_t flags);
 -static __always_inline void * __must_check kasan_kmalloc_large(const void *ptr,
 -						      size_t size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_kmalloc_large(ptr, size, flags);
 -	return (void *)ptr;
 -}
 -
 -void * __must_check __kasan_krealloc(const void *object,
 -				     size_t new_size, gfp_t flags);
 -static __always_inline void * __must_check kasan_krealloc(const void *object,
 -						 size_t new_size, gfp_t flags)
 -{
 -	if (kasan_enabled())
 -		return __kasan_krealloc(object, new_size, flags);
 -	return (void *)object;
 -}
 +int kasan_add_zero_shadow(void *start, unsigned long size);
 +void kasan_remove_zero_shadow(void *start, unsigned long size);
  
 -void __kasan_kfree_large(void *ptr, unsigned long ip);
 -static __always_inline void kasan_kfree_large(void *ptr, unsigned long ip)
 +size_t __ksize(const void *);
 +static inline void kasan_unpoison_slab(const void *ptr)
  {
 -	if (kasan_enabled())
 -		__kasan_kfree_large(ptr, ip);
 +	kasan_unpoison_shadow(ptr, __ksize(ptr));
  }
 +size_t kasan_metadata_size(struct kmem_cache *cache);
  
  bool kasan_save_enable_multi_shot(void);
  void kasan_restore_multi_shot(bool enabled);
  
  #else /* CONFIG_KASAN */
  
++<<<<<<< HEAD
 +static inline void kasan_unpoison_shadow(const void *address, size_t size) {}
 +
 +static inline void kasan_unpoison_task_stack(struct task_struct *task) {}
 +
 +static inline void kasan_enable_current(void) {}
 +static inline void kasan_disable_current(void) {}
 +
++=======
+ static inline bool kasan_enabled(void)
+ {
+ 	return false;
+ }
+ static inline slab_flags_t kasan_never_merge(void)
+ {
+ 	return 0;
+ }
+ static inline void kasan_unpoison_range(const void *address, size_t size) {}
++>>>>>>> e86f8b09f215 (kasan, mm: allow cache merging with no metadata)
  static inline void kasan_alloc_pages(struct page *page, unsigned int order) {}
  static inline void kasan_free_pages(struct page *page, unsigned int order) {}
 +
  static inline void kasan_cache_create(struct kmem_cache *cache,
  				      unsigned int *size,
  				      slab_flags_t *flags) {}
diff --cc mm/kasan/common.c
index 0d0cb20ec1a4,b25167664ead..000000000000
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@@ -196,10 -82,22 +196,25 @@@ asmlinkage void kasan_unpoison_task_sta
  	 */
  	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
  
 -	unpoison_range(base, watermark - base);
 +	kasan_unpoison_shadow(base, watermark - base);
  }
 -#endif /* CONFIG_KASAN_STACK */
  
++<<<<<<< HEAD
 +void kasan_alloc_pages(struct page *page, unsigned int order)
++=======
+ /*
+  * Only allow cache merging when stack collection is disabled and no metadata
+  * is present.
+  */
+ slab_flags_t __kasan_never_merge(void)
+ {
+ 	if (kasan_stack_collection_enabled())
+ 		return SLAB_KASAN;
+ 	return 0;
+ }
+ 
+ void __kasan_alloc_pages(struct page *page, unsigned int order)
++>>>>>>> e86f8b09f215 (kasan, mm: allow cache merging with no metadata)
  {
  	u8 tag;
  	unsigned long i;
* Unmerged path include/linux/kasan.h
* Unmerged path mm/kasan/common.c
diff --git a/mm/slab_common.c b/mm/slab_common.c
index b485099f723a..8124b8baa26f 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -18,6 +18,7 @@
 #include <linux/seq_file.h>
 #include <linux/proc_fs.h>
 #include <linux/debugfs.h>
+#include <linux/kasan.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 #include <asm/page.h>
@@ -53,7 +54,7 @@ static DECLARE_WORK(slab_caches_to_rcu_destroy_work,
  */
 #define SLAB_NEVER_MERGE (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \
 		SLAB_TRACE | SLAB_TYPESAFE_BY_RCU | SLAB_NOLEAKTRACE | \
-		SLAB_FAILSLAB | SLAB_KASAN)
+		SLAB_FAILSLAB | kasan_never_merge())
 
 #define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \
 			 SLAB_CACHE_DMA32 | SLAB_ACCOUNT)
