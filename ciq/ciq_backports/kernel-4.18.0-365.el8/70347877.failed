swiotlb: Refactor swiotlb_tbl_unmap_single

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Claire Chang <tientzu@chromium.org>
commit 70347877231eeb505a27abe80af7ae3d3a281519
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/70347877.failed

Add a new function, swiotlb_release_slots, to make the code reusable for
supporting different bounce buffer pools.

	Signed-off-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Stefano Stabellini <sstabellini@kernel.org>
	Tested-by: Will Deacon <will@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 70347877231eeb505a27abe80af7ae3d3a281519)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,23b6df3a6ab7..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -656,41 -555,16 +656,54 @@@ phys_addr_t swiotlb_tbl_map_single(stru
  	return tlb_addr;
  }
  
++<<<<<<< HEAD
 +static void validate_sync_size_and_truncate(struct device *hwdev, size_t orig_size, size_t *size)
 +{
 +	if (*size > orig_size) {
 +		/* Warn and truncate mapping_size */
 +		dev_WARN_ONCE(hwdev, 1,
 +			"Attempt for buffer overflow. Original size: %zu. Mapping size: %zu.\n",
 +			orig_size, *size);
 +		*size = orig_size;
 +	}
 +}
 +
 +/*
 + * tlb_addr is the physical address of the bounce buffer to unmap.
 + */
 +void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 +			      size_t mapping_size, size_t alloc_size,
 +			      enum dma_data_direction dir, unsigned long attrs)
 +{
 +	unsigned long flags;
 +	unsigned int offset = swiotlb_align_offset(hwdev, tlb_addr);
 +	int i, count, nslots = nr_slots(alloc_size + offset);
 +	int index = (tlb_addr - offset - io_tlb_start) >> IO_TLB_SHIFT;
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
 +
 +	validate_sync_size_and_truncate(hwdev, io_tlb_orig_size[index], &mapping_size);
 +
 +	/*
 +	 * First, sync the memory before unmapping the entry
 +	 */
 +	if (orig_addr != INVALID_PHYS_ADDR &&
 +	    !(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
 +	    ((dir == DMA_FROM_DEVICE) || (dir == DMA_BIDIRECTIONAL)))
 +		swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_FROM_DEVICE);
 +
 +	/*
++=======
+ static void swiotlb_release_slots(struct device *dev, phys_addr_t tlb_addr)
+ {
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 	unsigned long flags;
+ 	unsigned int offset = swiotlb_align_offset(dev, tlb_addr);
+ 	int index = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;
+ 	int nslots = nr_slots(mem->slots[index].alloc_size + offset);
+ 	int count, i;
+ 
+ 	/*
++>>>>>>> 70347877231e (swiotlb: Refactor swiotlb_tbl_unmap_single)
  	 * Return the buffer to the free list by setting the corresponding
  	 * entries to indicate the number of contiguous entries available.
  	 * While returning the entries to the free list, we merge the entries
@@@ -717,44 -591,46 +730,66 @@@
  	 * available (non zero)
  	 */
  	for (i = index - 1;
 -	     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && mem->slots[i].list;
 +	     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && io_tlb_list[i];
  	     i--)
 -		mem->slots[i].list = ++count;
 -	mem->used -= nslots;
 -	spin_unlock_irqrestore(&mem->lock, flags);
 +		io_tlb_list[i] = ++count;
 +	io_tlb_used -= nslots;
 +	spin_unlock_irqrestore(&io_tlb_lock, flags);
  }
  
++<<<<<<< HEAD
 +void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 +			     size_t size, enum dma_data_direction dir,
 +			     enum dma_sync_target target)
++=======
+ /*
+  * tlb_addr is the physical address of the bounce buffer to unmap.
+  */
+ void swiotlb_tbl_unmap_single(struct device *dev, phys_addr_t tlb_addr,
+ 			      size_t mapping_size, enum dma_data_direction dir,
+ 			      unsigned long attrs)
+ {
+ 	/*
+ 	 * First, sync the memory before unmapping the entry
+ 	 */
+ 	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
+ 	    (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
+ 		swiotlb_bounce(dev, tlb_addr, mapping_size, DMA_FROM_DEVICE);
+ 
+ 	swiotlb_release_slots(dev, tlb_addr);
+ }
+ 
+ void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,
+ 		size_t size, enum dma_data_direction dir)
++>>>>>>> 70347877231e (swiotlb: Refactor swiotlb_tbl_unmap_single)
  {
 -	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)
 -		swiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);
 -	else
 -		BUG_ON(dir != DMA_FROM_DEVICE);
 -}
 +	int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
 +	size_t orig_size = io_tlb_orig_size[index];
 +	phys_addr_t orig_addr = io_tlb_orig_addr[index];
  
 -void swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,
 -		size_t size, enum dma_data_direction dir)
 -{
 -	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
 -		swiotlb_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);
 -	else
 -		BUG_ON(dir != DMA_TO_DEVICE);
 +	if (orig_addr == INVALID_PHYS_ADDR)
 +		return;
 +
 +	validate_sync_size_and_truncate(hwdev, orig_size, &size);
 +
 +	switch (target) {
 +	case SYNC_FOR_CPU:
 +		if (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
 +			swiotlb_bounce(orig_addr, tlb_addr,
 +				       size, DMA_FROM_DEVICE);
 +		else
 +			BUG_ON(dir != DMA_TO_DEVICE);
 +		break;
 +	case SYNC_FOR_DEVICE:
 +		if (likely(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
 +			swiotlb_bounce(orig_addr, tlb_addr,
 +				       size, DMA_TO_DEVICE);
 +		else
 +			BUG_ON(dir != DMA_FROM_DEVICE);
 +		break;
 +	default:
 +		BUG();
 +	}
  }
  
  /*
* Unmerged path kernel/dma/swiotlb.c
