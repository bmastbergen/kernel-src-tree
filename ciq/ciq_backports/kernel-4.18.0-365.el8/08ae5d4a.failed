iommu/dma: Fix sync_sg with swiotlb

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author David Stevens <stevensd@chromium.org>
commit 08ae5d4a1ae96b72222e7b02d072bb997ff29dac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/08ae5d4a.failed

The is_swiotlb_buffer function takes the physical address of the swiotlb
buffer, not the physical address of the original buffer. The sglist
contains the physical addresses of the original buffer, so for the
sync_sg functions to work properly when a bounce buffer might have been
used, we need to use iommu_iova_to_phys to look up the physical address.
This is what sync_single does, so call that function on each sglist
segment.

The previous code mostly worked because swiotlb does the transfer on map
and unmap. However, any callers which use DMA_ATTR_SKIP_CPU_SYNC with
sglists or which call sync_sg would not have had anything copied to the
bounce buffer.

Fixes: 82612d66d51d ("iommu: Allow the iommu/dma api to use bounce buffers")
	Signed-off-by: David Stevens <stevensd@chromium.org>
	Reviewed-by: Robin Murphy <robin.murphy@arm.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
Link: https://lore.kernel.org/r/20210929023300.335969-2-stevensd@google.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 08ae5d4a1ae96b72222e7b02d072bb997ff29dac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/dma-iommu.c
diff --cc drivers/iommu/dma-iommu.c
index 7a9d033bfda4,c4d205b63c58..000000000000
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@@ -825,17 -828,13 +825,21 @@@ static void iommu_dma_sync_sg_for_cpu(s
  	struct scatterlist *sg;
  	int i;
  
- 	if (dev_is_dma_coherent(dev) && !dev_is_untrusted(dev))
- 		return;
- 
- 	for_each_sg(sgl, sg, nelems, i) {
- 		if (!dev_is_dma_coherent(dev))
+ 	if (dev_is_untrusted(dev))
+ 		for_each_sg(sgl, sg, nelems, i)
+ 			iommu_dma_sync_single_for_cpu(dev, sg_dma_address(sg),
+ 						      sg->length, dir);
+ 	else if (!dev_is_dma_coherent(dev))
+ 		for_each_sg(sgl, sg, nelems, i)
  			arch_sync_dma_for_cpu(sg_phys(sg), sg->length, dir);
++<<<<<<< HEAD
 +
 +		if (is_swiotlb_buffer(sg_phys(sg)))
 +			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length,
 +						dir, SYNC_FOR_CPU);
 +	}
++=======
++>>>>>>> 08ae5d4a1ae9 (iommu/dma: Fix sync_sg with swiotlb)
  }
  
  static void iommu_dma_sync_sg_for_device(struct device *dev,
@@@ -845,17 -844,14 +849,26 @@@
  	struct scatterlist *sg;
  	int i;
  
++<<<<<<< HEAD
 +	if (dev_is_dma_coherent(dev) && !dev_is_untrusted(dev))
 +		return;
 +
 +	for_each_sg(sgl, sg, nelems, i) {
 +		if (is_swiotlb_buffer(sg_phys(sg)))
 +			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length,
 +						dir, SYNC_FOR_DEVICE);
 +
 +		if (!dev_is_dma_coherent(dev))
++=======
+ 	if (dev_is_untrusted(dev))
+ 		for_each_sg(sgl, sg, nelems, i)
+ 			iommu_dma_sync_single_for_device(dev,
+ 							 sg_dma_address(sg),
+ 							 sg->length, dir);
+ 	else if (!dev_is_dma_coherent(dev))
+ 		for_each_sg(sgl, sg, nelems, i)
++>>>>>>> 08ae5d4a1ae9 (iommu/dma: Fix sync_sg with swiotlb)
  			arch_sync_dma_for_device(sg_phys(sg), sg->length, dir);
- 	}
  }
  
  static dma_addr_t iommu_dma_map_page(struct device *dev, struct page *page,
* Unmerged path drivers/iommu/dma-iommu.c
