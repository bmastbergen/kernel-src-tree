swiotlb: Add restricted DMA alloc/free support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Claire Chang <tientzu@chromium.org>
commit f4111e39a52aa5d5136d890bbd1aa87c1c8fe3bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/f4111e39.failed

Add the functions, swiotlb_{alloc,free} and is_swiotlb_for_alloc to
support the memory allocation from restricted DMA pool.

The restricted DMA pool is preferred if available.

Note that since coherent allocation needs remapping, one must set up
another device coherent pool by shared-dma-pool and use
dma_alloc_from_dev_coherent instead for atomic coherent allocation.

	Signed-off-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Stefano Stabellini <sstabellini@kernel.org>
	Tested-by: Will Deacon <will@kernel.org>
	Acked-by: Stefano Stabellini <sstabellini@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit f4111e39a52aa5d5136d890bbd1aa87c1c8fe3bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swiotlb.h
#	kernel/dma/swiotlb.c
diff --cc include/linux/swiotlb.h
index 5857a937c637,3b9454d1e498..000000000000
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@@ -71,11 -62,62 +71,58 @@@ dma_addr_t swiotlb_map(struct device *d
  
  #ifdef CONFIG_SWIOTLB
  extern enum swiotlb_force swiotlb_force;
 +extern phys_addr_t io_tlb_start, io_tlb_end;
  
++<<<<<<< HEAD
 +static inline bool is_swiotlb_buffer(phys_addr_t paddr)
++=======
+ /**
+  * struct io_tlb_mem - IO TLB Memory Pool Descriptor
+  *
+  * @start:	The start address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @end:	The end address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @nslabs:	The number of IO TLB blocks (in groups of 64) between @start and
+  *		@end. This is command line adjustable via setup_io_tlb_npages.
+  * @used:	The number of used IO TLB block.
+  * @list:	The free list describing the number of free entries available
+  *		from each index.
+  * @index:	The index to start searching in the next round.
+  * @orig_addr:	The original address corresponding to a mapped entry.
+  * @alloc_size:	Size of the allocated buffer.
+  * @lock:	The lock to protect the above data structures in the map and
+  *		unmap calls.
+  * @debugfs:	The dentry to debugfs.
+  * @late_alloc:	%true if allocated using the page allocator
+  * @force_bounce: %true if swiotlb bouncing is forced
+  * @for_alloc:  %true if the pool is used for memory allocation
+  */
+ struct io_tlb_mem {
+ 	phys_addr_t start;
+ 	phys_addr_t end;
+ 	unsigned long nslabs;
+ 	unsigned long used;
+ 	unsigned int index;
+ 	spinlock_t lock;
+ 	struct dentry *debugfs;
+ 	bool late_alloc;
+ 	bool force_bounce;
+ 	bool for_alloc;
+ 	struct io_tlb_slot {
+ 		phys_addr_t orig_addr;
+ 		size_t alloc_size;
+ 		unsigned int list;
+ 	} slots[];
+ };
+ extern struct io_tlb_mem *io_tlb_default_mem;
+ 
+ static inline bool is_swiotlb_buffer(struct device *dev, phys_addr_t paddr)
++>>>>>>> f4111e39a52a (swiotlb: Add restricted DMA alloc/free support)
  {
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && paddr >= mem->start && paddr < mem->end;
 -}
 -
 -static inline bool is_swiotlb_force_bounce(struct device *dev)
 -{
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && mem->force_bounce;
 +	return paddr >= io_tlb_start && paddr < io_tlb_end;
  }
  
  void __init swiotlb_exit(void);
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,44fc3d10f017..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -559,15 -456,16 +559,22 @@@ static int find_slots(struct device *de
  	if (alloc_size >= PAGE_SIZE)
  		stride = max(stride, stride << (PAGE_SHIFT - IO_TLB_SHIFT));
  
 -	spin_lock_irqsave(&mem->lock, flags);
 -	if (unlikely(nslots > mem->nslabs - mem->used))
 +	spin_lock_irqsave(&io_tlb_lock, flags);
 +	if (unlikely(nslots > io_tlb_nslabs - io_tlb_used))
  		goto not_found;
  
 -	index = wrap = wrap_index(mem, ALIGN(mem->index, stride));
 +	index = wrap = wrap_index(ALIGN(io_tlb_index, stride));
  	do {
++<<<<<<< HEAD
 +		if ((slot_addr(tbl_dma_addr, index) & iotlb_align_mask) !=
 +		    (orig_addr & iotlb_align_mask)) {
 +			index = wrap_index(index + 1);
++=======
+ 		if (orig_addr &&
+ 		    (slot_addr(tbl_dma_addr, index) & iotlb_align_mask) !=
+ 			    (orig_addr & iotlb_align_mask)) {
+ 			index = wrap_index(mem, index + 1);
++>>>>>>> f4111e39a52a (swiotlb: Add restricted DMA alloc/free support)
  			continue;
  		}
  
@@@ -817,6 -700,39 +824,39 @@@ static int __init swiotlb_create_debugf
  	return 0;
  }
  
 -late_initcall(swiotlb_create_default_debugfs);
 +late_initcall(swiotlb_create_debugfs);
  
  #endif
+ 
+ #ifdef CONFIG_DMA_RESTRICTED_POOL
+ struct page *swiotlb_alloc(struct device *dev, size_t size)
+ {
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 	phys_addr_t tlb_addr;
+ 	int index;
+ 
+ 	if (!mem)
+ 		return NULL;
+ 
+ 	index = swiotlb_find_slots(dev, 0, size);
+ 	if (index == -1)
+ 		return NULL;
+ 
+ 	tlb_addr = slot_addr(mem->start, index);
+ 
+ 	return pfn_to_page(PFN_DOWN(tlb_addr));
+ }
+ 
+ bool swiotlb_free(struct device *dev, struct page *page, size_t size)
+ {
+ 	phys_addr_t tlb_addr = page_to_phys(page);
+ 
+ 	if (!is_swiotlb_buffer(dev, tlb_addr))
+ 		return false;
+ 
+ 	swiotlb_release_slots(dev, tlb_addr);
+ 
+ 	return true;
+ }
+ 
+ #endif /* CONFIG_DMA_RESTRICTED_POOL */
* Unmerged path include/linux/swiotlb.h
diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
index 7d488b64b9de..86cf4aa243a3 100644
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@ -70,6 +70,15 @@ static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
 			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);
 }
 
+static void __dma_direct_free_pages(struct device *dev, struct page *page,
+				    size_t size)
+{
+	if (IS_ENABLED(CONFIG_DMA_RESTRICTED_POOL) &&
+	    swiotlb_free(dev, page, size))
+		return;
+	dma_free_contiguous(dev, page, size);
+}
+
 static struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		gfp_t gfp)
 {
@@ -81,6 +90,16 @@ static struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 
 	gfp |= dma_direct_optimal_gfp_mask(dev, dev->coherent_dma_mask,
 					   &phys_limit);
+	if (IS_ENABLED(CONFIG_DMA_RESTRICTED_POOL) &&
+	    is_swiotlb_for_alloc(dev)) {
+		page = swiotlb_alloc(dev, size);
+		if (page && !dma_coherent_ok(dev, page_to_phys(page), size)) {
+			__dma_direct_free_pages(dev, page, size);
+			return NULL;
+		}
+		return page;
+	}
+
 	page = dma_alloc_contiguous(dev, size, gfp);
 	if (page && !dma_coherent_ok(dev, page_to_phys(page), size)) {
 		dma_free_contiguous(dev, page, size);
@@ -137,7 +156,7 @@ void *dma_direct_alloc(struct device *dev, size_t size,
 		gfp |= __GFP_NOWARN;
 
 	if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
-	    !force_dma_unencrypted(dev)) {
+	    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {
 		page = __dma_direct_alloc_pages(dev, size, gfp & ~__GFP_ZERO);
 		if (!page)
 			return NULL;
@@ -150,18 +169,23 @@ void *dma_direct_alloc(struct device *dev, size_t size,
 	}
 
 	if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
-	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
-	    !dev_is_dma_coherent(dev))
+	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) && !dev_is_dma_coherent(dev) &&
+	    !is_swiotlb_for_alloc(dev))
 		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
 
 	/*
 	 * Remapping or decrypting memory may block. If either is required and
 	 * we can't block, allocate the memory from the atomic pools.
+	 * If restricted DMA (i.e., is_swiotlb_for_alloc) is required, one must
+	 * set up another device coherent pool by shared-dma-pool and use
+	 * dma_alloc_from_dev_coherent instead.
 	 */
 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
 	    !gfpflags_allow_blocking(gfp) &&
 	    (force_dma_unencrypted(dev) ||
-	     (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) && !dev_is_dma_coherent(dev))))
+	     (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
+	      !dev_is_dma_coherent(dev))) &&
+	    !is_swiotlb_for_alloc(dev))
 		return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);
 
 	/* we always manually zero the memory once we are done */
@@ -232,7 +256,7 @@ void *dma_direct_alloc(struct device *dev, size_t size,
 			return NULL;
 	}
 out_free_pages:
-	dma_free_contiguous(dev, page, size);
+	__dma_direct_free_pages(dev, page, size);
 	return NULL;
 }
 
@@ -242,15 +266,15 @@ void dma_direct_free(struct device *dev, size_t size,
 	unsigned int page_order = get_order(size);
 
 	if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
-	    !force_dma_unencrypted(dev)) {
+	    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {
 		/* cpu_addr is a struct page cookie, not a kernel address */
 		dma_free_contiguous(dev, cpu_addr, size);
 		return;
 	}
 
 	if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
-	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
-	    !dev_is_dma_coherent(dev)) {
+	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) && !dev_is_dma_coherent(dev) &&
+	    !is_swiotlb_for_alloc(dev)) {
 		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
 		return;
 	}
@@ -268,7 +292,7 @@ void dma_direct_free(struct device *dev, size_t size,
 	else if (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))
 		arch_dma_clear_uncached(cpu_addr, size);
 
-	dma_free_contiguous(dev, dma_direct_to_page(dev, dma_addr), size);
+	__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);
 }
 
 struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
@@ -278,7 +302,8 @@ struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
 	void *ret;
 
 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
-	    force_dma_unencrypted(dev) && !gfpflags_allow_blocking(gfp))
+	    force_dma_unencrypted(dev) && !gfpflags_allow_blocking(gfp) &&
+	    !is_swiotlb_for_alloc(dev))
 		return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);
 
 	page = __dma_direct_alloc_pages(dev, size, gfp);
@@ -305,7 +330,7 @@ struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
 	*dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
 	return page;
 out_free_pages:
-	dma_free_contiguous(dev, page, size);
+	__dma_direct_free_pages(dev, page, size);
 	return NULL;
 }
 
@@ -324,7 +349,7 @@ void dma_direct_free_pages(struct device *dev, size_t size,
 	if (force_dma_unencrypted(dev))
 		set_memory_encrypted((unsigned long)vaddr, 1 << page_order);
 
-	dma_free_contiguous(dev, page, size);
+	__dma_direct_free_pages(dev, page, size);
 }
 
 #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
* Unmerged path kernel/dma/swiotlb.c
