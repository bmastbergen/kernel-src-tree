arch_topology: Fix missing clear cluster_cpumask in remove_cpu_topology()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Wang ShaoBo <bobo.shaobowang@huawei.com>
commit 4cc4cc28ec4154c4f1395648ab67ac9fd3e71fdc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/4cc4cc28.failed

When testing cpu online and offline, warning happened like this:

[  146.746743] WARNING: CPU: 92 PID: 974 at kernel/sched/topology.c:2215 build_sched_domains+0x81c/0x11b0
[  146.749988] CPU: 92 PID: 974 Comm: kworker/92:2 Not tainted 5.15.0 #9
[  146.750402] Hardware name: Huawei TaiShan 2280 V2/BC82AMDDA, BIOS 1.79 08/21/2021
[  146.751213] Workqueue: events cpuset_hotplug_workfn
[  146.751629] pstate: 00400009 (nzcv daif +PAN -UAO -TCO -DIT -SSBS BTYPE=--)
[  146.752048] pc : build_sched_domains+0x81c/0x11b0
[  146.752461] lr : build_sched_domains+0x414/0x11b0
[  146.752860] sp : ffff800040a83a80
[  146.753247] x29: ffff800040a83a80 x28: ffff20801f13a980 x27: ffff20800448ae00
[  146.753644] x26: ffff800012a858e8 x25: ffff800012ea48c0 x24: 0000000000000000
[  146.754039] x23: ffff800010ab7d60 x22: ffff800012f03758 x21: 000000000000005f
[  146.754427] x20: 000000000000005c x19: ffff004080012840 x18: ffffffffffffffff
[  146.754814] x17: 3661613030303230 x16: 30303078303a3239 x15: ffff800011f92b48
[  146.755197] x14: ffff20be3f95cef6 x13: 2e6e69616d6f642d x12: 6465686373204c4c
[  146.755578] x11: ffff20bf7fc83a00 x10: 0000000000000040 x9 : 0000000000000000
[  146.755957] x8 : 0000000000000002 x7 : ffffffffe0000000 x6 : 0000000000000002
[  146.756334] x5 : 0000000090000000 x4 : 00000000f0000000 x3 : 0000000000000001
[  146.756705] x2 : 0000000000000080 x1 : ffff800012f03860 x0 : 0000000000000001
[  146.757070] Call trace:
[  146.757421]  build_sched_domains+0x81c/0x11b0
[  146.757771]  partition_sched_domains_locked+0x57c/0x978
[  146.758118]  rebuild_sched_domains_locked+0x44c/0x7f0
[  146.758460]  rebuild_sched_domains+0x2c/0x48
[  146.758791]  cpuset_hotplug_workfn+0x3fc/0x888
[  146.759114]  process_one_work+0x1f4/0x480
[  146.759429]  worker_thread+0x48/0x460
[  146.759734]  kthread+0x158/0x168
[  146.760030]  ret_from_fork+0x10/0x20
[  146.760318] ---[ end trace 82c44aad6900e81a ]---

For some architectures like risc-v and arm64 which use common code
clear_cpu_topology() in shutting down CPUx, When CONFIG_SCHED_CLUSTER
is set, cluster_sibling in cpu_topology of each sibling adjacent
to CPUx is missed clearing, this causes checking failed in
topology_span_sane() and rebuilding topology failure at end when CPU online.

Different sibling's cluster_sibling in cpu_topology[] when CPU92 offline
(CPU 92, 93, 94, 95 are in one cluster):

Before revision:
CPU                 [92]      [93]      [94]      [95]
cluster_sibling     [92]     [92-95]   [92-95]   [92-95]

After revision:
CPU                 [92]      [93]      [94]      [95]
cluster_sibling     [92]     [93-95]   [93-95]   [93-95]

	Signed-off-by: Wang ShaoBo <bobo.shaobowang@huawei.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Acked-by: Barry Song <song.bao.hua@hisilicon.com>
	Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
Link: https://lore.kernel.org/r/20211110095856.469360-1-bobo.shaobowang@huawei.com
(cherry picked from commit 4cc4cc28ec4154c4f1395648ab67ac9fd3e71fdc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/base/arch_topology.c
diff --cc drivers/base/arch_topology.c
index 347cddcd2879,ff16a36a908b..000000000000
--- a/drivers/base/arch_topology.c
+++ b/drivers/base/arch_topology.c
@@@ -244,3 -385,323 +244,326 @@@ static void parsing_done_workfn(struct 
  #else
  core_initcall(free_raw_capacity);
  #endif
++<<<<<<< HEAD
++=======
+ 
+ #if defined(CONFIG_ARM64) || defined(CONFIG_RISCV)
+ /*
+  * This function returns the logic cpu number of the node.
+  * There are basically three kinds of return values:
+  * (1) logic cpu number which is > 0.
+  * (2) -ENODEV when the device tree(DT) node is valid and found in the DT but
+  * there is no possible logical CPU in the kernel to match. This happens
+  * when CONFIG_NR_CPUS is configure to be smaller than the number of
+  * CPU nodes in DT. We need to just ignore this case.
+  * (3) -1 if the node does not exist in the device tree
+  */
+ static int __init get_cpu_for_node(struct device_node *node)
+ {
+ 	struct device_node *cpu_node;
+ 	int cpu;
+ 
+ 	cpu_node = of_parse_phandle(node, "cpu", 0);
+ 	if (!cpu_node)
+ 		return -1;
+ 
+ 	cpu = of_cpu_node_to_id(cpu_node);
+ 	if (cpu >= 0)
+ 		topology_parse_cpu_capacity(cpu_node, cpu);
+ 	else
+ 		pr_info("CPU node for %pOF exist but the possible cpu range is :%*pbl\n",
+ 			cpu_node, cpumask_pr_args(cpu_possible_mask));
+ 
+ 	of_node_put(cpu_node);
+ 	return cpu;
+ }
+ 
+ static int __init parse_core(struct device_node *core, int package_id,
+ 			     int core_id)
+ {
+ 	char name[20];
+ 	bool leaf = true;
+ 	int i = 0;
+ 	int cpu;
+ 	struct device_node *t;
+ 
+ 	do {
+ 		snprintf(name, sizeof(name), "thread%d", i);
+ 		t = of_get_child_by_name(core, name);
+ 		if (t) {
+ 			leaf = false;
+ 			cpu = get_cpu_for_node(t);
+ 			if (cpu >= 0) {
+ 				cpu_topology[cpu].package_id = package_id;
+ 				cpu_topology[cpu].core_id = core_id;
+ 				cpu_topology[cpu].thread_id = i;
+ 			} else if (cpu != -ENODEV) {
+ 				pr_err("%pOF: Can't get CPU for thread\n", t);
+ 				of_node_put(t);
+ 				return -EINVAL;
+ 			}
+ 			of_node_put(t);
+ 		}
+ 		i++;
+ 	} while (t);
+ 
+ 	cpu = get_cpu_for_node(core);
+ 	if (cpu >= 0) {
+ 		if (!leaf) {
+ 			pr_err("%pOF: Core has both threads and CPU\n",
+ 			       core);
+ 			return -EINVAL;
+ 		}
+ 
+ 		cpu_topology[cpu].package_id = package_id;
+ 		cpu_topology[cpu].core_id = core_id;
+ 	} else if (leaf && cpu != -ENODEV) {
+ 		pr_err("%pOF: Can't get CPU for leaf core\n", core);
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int __init parse_cluster(struct device_node *cluster, int depth)
+ {
+ 	char name[20];
+ 	bool leaf = true;
+ 	bool has_cores = false;
+ 	struct device_node *c;
+ 	static int package_id __initdata;
+ 	int core_id = 0;
+ 	int i, ret;
+ 
+ 	/*
+ 	 * First check for child clusters; we currently ignore any
+ 	 * information about the nesting of clusters and present the
+ 	 * scheduler with a flat list of them.
+ 	 */
+ 	i = 0;
+ 	do {
+ 		snprintf(name, sizeof(name), "cluster%d", i);
+ 		c = of_get_child_by_name(cluster, name);
+ 		if (c) {
+ 			leaf = false;
+ 			ret = parse_cluster(c, depth + 1);
+ 			of_node_put(c);
+ 			if (ret != 0)
+ 				return ret;
+ 		}
+ 		i++;
+ 	} while (c);
+ 
+ 	/* Now check for cores */
+ 	i = 0;
+ 	do {
+ 		snprintf(name, sizeof(name), "core%d", i);
+ 		c = of_get_child_by_name(cluster, name);
+ 		if (c) {
+ 			has_cores = true;
+ 
+ 			if (depth == 0) {
+ 				pr_err("%pOF: cpu-map children should be clusters\n",
+ 				       c);
+ 				of_node_put(c);
+ 				return -EINVAL;
+ 			}
+ 
+ 			if (leaf) {
+ 				ret = parse_core(c, package_id, core_id++);
+ 			} else {
+ 				pr_err("%pOF: Non-leaf cluster with core %s\n",
+ 				       cluster, name);
+ 				ret = -EINVAL;
+ 			}
+ 
+ 			of_node_put(c);
+ 			if (ret != 0)
+ 				return ret;
+ 		}
+ 		i++;
+ 	} while (c);
+ 
+ 	if (leaf && !has_cores)
+ 		pr_warn("%pOF: empty cluster\n", cluster);
+ 
+ 	if (leaf)
+ 		package_id++;
+ 
+ 	return 0;
+ }
+ 
+ static int __init parse_dt_topology(void)
+ {
+ 	struct device_node *cn, *map;
+ 	int ret = 0;
+ 	int cpu;
+ 
+ 	cn = of_find_node_by_path("/cpus");
+ 	if (!cn) {
+ 		pr_err("No CPU information found in DT\n");
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * When topology is provided cpu-map is essentially a root
+ 	 * cluster with restricted subnodes.
+ 	 */
+ 	map = of_get_child_by_name(cn, "cpu-map");
+ 	if (!map)
+ 		goto out;
+ 
+ 	ret = parse_cluster(map, 0);
+ 	if (ret != 0)
+ 		goto out_map;
+ 
+ 	topology_normalize_cpu_scale();
+ 
+ 	/*
+ 	 * Check that all cores are in the topology; the SMP code will
+ 	 * only mark cores described in the DT as possible.
+ 	 */
+ 	for_each_possible_cpu(cpu)
+ 		if (cpu_topology[cpu].package_id == -1)
+ 			ret = -EINVAL;
+ 
+ out_map:
+ 	of_node_put(map);
+ out:
+ 	of_node_put(cn);
+ 	return ret;
+ }
+ #endif
+ 
+ /*
+  * cpu topology table
+  */
+ struct cpu_topology cpu_topology[NR_CPUS];
+ EXPORT_SYMBOL_GPL(cpu_topology);
+ 
+ const struct cpumask *cpu_coregroup_mask(int cpu)
+ {
+ 	const cpumask_t *core_mask = cpumask_of_node(cpu_to_node(cpu));
+ 
+ 	/* Find the smaller of NUMA, core or LLC siblings */
+ 	if (cpumask_subset(&cpu_topology[cpu].core_sibling, core_mask)) {
+ 		/* not numa in package, lets use the package siblings */
+ 		core_mask = &cpu_topology[cpu].core_sibling;
+ 	}
+ 	if (cpu_topology[cpu].llc_id != -1) {
+ 		if (cpumask_subset(&cpu_topology[cpu].llc_sibling, core_mask))
+ 			core_mask = &cpu_topology[cpu].llc_sibling;
+ 	}
+ 
+ 	return core_mask;
+ }
+ 
+ const struct cpumask *cpu_clustergroup_mask(int cpu)
+ {
+ 	return &cpu_topology[cpu].cluster_sibling;
+ }
+ 
+ void update_siblings_masks(unsigned int cpuid)
+ {
+ 	struct cpu_topology *cpu_topo, *cpuid_topo = &cpu_topology[cpuid];
+ 	int cpu;
+ 
+ 	/* update core and thread sibling masks */
+ 	for_each_online_cpu(cpu) {
+ 		cpu_topo = &cpu_topology[cpu];
+ 
+ 		if (cpuid_topo->llc_id == cpu_topo->llc_id) {
+ 			cpumask_set_cpu(cpu, &cpuid_topo->llc_sibling);
+ 			cpumask_set_cpu(cpuid, &cpu_topo->llc_sibling);
+ 		}
+ 
+ 		if (cpuid_topo->package_id != cpu_topo->package_id)
+ 			continue;
+ 
+ 		if (cpuid_topo->cluster_id == cpu_topo->cluster_id &&
+ 		    cpuid_topo->cluster_id != -1) {
+ 			cpumask_set_cpu(cpu, &cpuid_topo->cluster_sibling);
+ 			cpumask_set_cpu(cpuid, &cpu_topo->cluster_sibling);
+ 		}
+ 
+ 		cpumask_set_cpu(cpuid, &cpu_topo->core_sibling);
+ 		cpumask_set_cpu(cpu, &cpuid_topo->core_sibling);
+ 
+ 		if (cpuid_topo->core_id != cpu_topo->core_id)
+ 			continue;
+ 
+ 		cpumask_set_cpu(cpuid, &cpu_topo->thread_sibling);
+ 		cpumask_set_cpu(cpu, &cpuid_topo->thread_sibling);
+ 	}
+ }
+ 
+ static void clear_cpu_topology(int cpu)
+ {
+ 	struct cpu_topology *cpu_topo = &cpu_topology[cpu];
+ 
+ 	cpumask_clear(&cpu_topo->llc_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->llc_sibling);
+ 
+ 	cpumask_clear(&cpu_topo->cluster_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->cluster_sibling);
+ 
+ 	cpumask_clear(&cpu_topo->core_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->core_sibling);
+ 	cpumask_clear(&cpu_topo->thread_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->thread_sibling);
+ }
+ 
+ void __init reset_cpu_topology(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct cpu_topology *cpu_topo = &cpu_topology[cpu];
+ 
+ 		cpu_topo->thread_id = -1;
+ 		cpu_topo->core_id = -1;
+ 		cpu_topo->cluster_id = -1;
+ 		cpu_topo->package_id = -1;
+ 		cpu_topo->llc_id = -1;
+ 
+ 		clear_cpu_topology(cpu);
+ 	}
+ }
+ 
+ void remove_cpu_topology(unsigned int cpu)
+ {
+ 	int sibling;
+ 
+ 	for_each_cpu(sibling, topology_core_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_core_cpumask(sibling));
+ 	for_each_cpu(sibling, topology_sibling_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));
+ 	for_each_cpu(sibling, topology_cluster_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_cluster_cpumask(sibling));
+ 	for_each_cpu(sibling, topology_llc_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_llc_cpumask(sibling));
+ 
+ 	clear_cpu_topology(cpu);
+ }
+ 
+ __weak int __init parse_acpi_topology(void)
+ {
+ 	return 0;
+ }
+ 
+ #if defined(CONFIG_ARM64) || defined(CONFIG_RISCV)
+ void __init init_cpu_topology(void)
+ {
+ 	reset_cpu_topology();
+ 
+ 	/*
+ 	 * Discard anything that was parsed if we hit an error so we
+ 	 * don't use partial information.
+ 	 */
+ 	if (parse_acpi_topology())
+ 		reset_cpu_topology();
+ 	else if (of_have_populated_dt() && parse_dt_topology())
+ 		reset_cpu_topology();
+ }
+ #endif
++>>>>>>> 4cc4cc28ec41 (arch_topology: Fix missing clear cluster_cpumask in remove_cpu_topology())
* Unmerged path drivers/base/arch_topology.c
