kasan: remove redundant config option

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Walter Wu <walter-zh.wu@mediatek.com>
commit 02c587733c8161355a43e6e110c2e29bd0acff72
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/02c58773.failed

CONFIG_KASAN_STACK and CONFIG_KASAN_STACK_ENABLE both enable KASAN stack
instrumentation, but we should only need one config, so that we remove
CONFIG_KASAN_STACK_ENABLE and make CONFIG_KASAN_STACK workable.  see [1].

When enable KASAN stack instrumentation, then for gcc we could do no
prompt and default value y, and for clang prompt and default value n.

This patch fixes the following compilation warning:

  include/linux/kasan.h:333:30: warning: 'CONFIG_KASAN_STACK' is not defined, evaluates to 0 [-Wundef]

[akpm@linux-foundation.org: fix merge snafu]

Link: https://bugzilla.kernel.org/show_bug.cgi?id=210221 [1]
Link: https://lkml.kernel.org/r/20210226012531.29231-1-walter-zh.wu@mediatek.com
Fixes: d9b571c885a8 ("kasan: fix KASAN_STACK dependency for HW_TAGS")
	Signed-off-by: Walter Wu <walter-zh.wu@mediatek.com>
	Suggested-by: Dmitry Vyukov <dvyukov@google.com>
	Reviewed-by: Nathan Chancellor <natechancellor@gmail.com>
	Acked-by: Arnd Bergmann <arnd@arndb.de>
	Reviewed-by: Andrey Konovalov <andreyknvl@google.com>
	Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 02c587733c8161355a43e6e110c2e29bd0acff72)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/sleep.S
#	arch/x86/kernel/acpi/wakeup_64.S
#	include/linux/kasan.h
#	lib/Kconfig.kasan
#	mm/kasan/common.c
#	mm/kasan/kasan.h
#	mm/kasan/report_generic.c
#	security/Kconfig.hardening
diff --cc arch/arm64/kernel/sleep.S
index 3e53ffa07994,4ea9392f86e0..000000000000
--- a/arch/arm64/kernel/sleep.S
+++ b/arch/arm64/kernel/sleep.S
@@@ -132,7 -134,7 +132,11 @@@ ENTRY(_cpu_resume
  	 */
  	bl	cpu_do_resume
  
++<<<<<<< HEAD
 +#ifdef CONFIG_KASAN
++=======
+ #if defined(CONFIG_KASAN) && defined(CONFIG_KASAN_STACK)
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  	mov	x0, sp
  	bl	kasan_unpoison_task_stack_below
  #endif
diff --cc arch/x86/kernel/acpi/wakeup_64.S
index cc758cae111e,d5d8a352eafa..000000000000
--- a/arch/x86/kernel/acpi/wakeup_64.S
+++ b/arch/x86/kernel/acpi/wakeup_64.S
@@@ -109,7 -115,7 +109,11 @@@ SYM_FUNC_START(do_suspend_lowlevel
  	movq	pt_regs_r14(%rax), %r14
  	movq	pt_regs_r15(%rax), %r15
  
++<<<<<<< HEAD
 +#ifdef CONFIG_KASAN
++=======
+ #if defined(CONFIG_KASAN) && defined(CONFIG_KASAN_STACK)
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  	/*
  	 * The suspend path may have poisoned some areas deeper in the stack,
  	 * which we now need to unpoison.
diff --cc include/linux/kasan.h
index 71e7b6777f1b,14f72ec96492..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -138,38 -323,21 +138,47 @@@ static inline void *kasan_krealloc(cons
  {
  	return (void *)object;
  }
 -static inline bool kasan_check_byte(const void *address)
 +
 +static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
 +				   gfp_t flags)
 +{
 +	return object;
 +}
 +static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 +				   unsigned long ip)
  {
 -	return true;
 +	return false;
  }
  
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +
 +static inline int kasan_add_zero_shadow(void *start, unsigned long size)
 +{
 +	return 0;
 +}
 +static inline void kasan_remove_zero_shadow(void *start,
 +					unsigned long size)
 +{}
 +
 +static inline void kasan_unpoison_slab(const void *ptr) { }
 +static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 +
  #endif /* CONFIG_KASAN */
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_KASAN) && defined(CONFIG_KASAN_STACK)
+ void kasan_unpoison_task_stack(struct task_struct *task);
+ #else
+ static inline void kasan_unpoison_task_stack(struct task_struct *task) {}
+ #endif
+ 
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  #ifdef CONFIG_KASAN_GENERIC
  
 +#define KASAN_SHADOW_INIT 0
 +
  void kasan_cache_shrink(struct kmem_cache *cache);
  void kasan_cache_shutdown(struct kmem_cache *cache);
  void kasan_record_aux_stack(void *ptr);
diff --cc lib/Kconfig.kasan
index 9098a6f8ef1d,cffc2ebbf185..000000000000
--- a/lib/Kconfig.kasan
+++ b/lib/Kconfig.kasan
@@@ -153,21 -155,6 +154,24 @@@ config KASAN_STAC
  	  CONFIG_COMPILE_TEST.	On gcc it is assumed to always be safe
  	  to use and enabled by default.
  
++<<<<<<< HEAD
 +config KASAN_STACK
 +	int
 +	depends on KASAN_GENERIC || KASAN_SW_TAGS
 +	default 1 if KASAN_STACK_ENABLE || CC_IS_GCC
 +	default 0
 +
 +config KASAN_S390_4_LEVEL_PAGING
 +	bool "KASan: use 4-level paging"
 +	depends on S390
 +	help
 +	  Compiling the kernel with KASan disables automatic 3-level vs
 +	  4-level paging selection. 3-level paging is used by default (up
 +	  to 3TB of RAM with KASan enabled). This options allows to force
 +	  4-level paging instead.
 +
++=======
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  config KASAN_SW_TAGS_IDENTIFY
  	bool "Enable memory corruption identification"
  	depends on KASAN_SW_TAGS
diff --cc mm/kasan/common.c
index 0d0cb20ec1a4,7b53291dafa1..000000000000
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@@ -84,102 -56,14 +84,106 @@@ void kasan_disable_current(void
  {
  	current->kasan_depth--;
  }
 -#endif /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
  
 -void __kasan_unpoison_range(const void *address, size_t size)
 +bool __kasan_check_read(const volatile void *p, unsigned int size)
 +{
 +	return check_memory_region((unsigned long)p, size, false, _RET_IP_);
 +}
 +EXPORT_SYMBOL(__kasan_check_read);
 +
 +bool __kasan_check_write(const volatile void *p, unsigned int size)
 +{
 +	return check_memory_region((unsigned long)p, size, true, _RET_IP_);
 +}
 +EXPORT_SYMBOL(__kasan_check_write);
 +
 +#undef memset
 +void *memset(void *addr, int c, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)addr, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memset(addr, c, len);
 +}
 +
 +#ifdef __HAVE_ARCH_MEMMOVE
 +#undef memmove
 +void *memmove(void *dest, const void *src, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)src, len, false, _RET_IP_) ||
 +	    !check_memory_region((unsigned long)dest, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memmove(dest, src, len);
 +}
 +#endif
 +
 +#undef memcpy
 +void *memcpy(void *dest, const void *src, size_t len)
 +{
 +	if (!check_memory_region((unsigned long)src, len, false, _RET_IP_) ||
 +	    !check_memory_region((unsigned long)dest, len, true, _RET_IP_))
 +		return NULL;
 +
 +	return __memcpy(dest, src, len);
 +}
 +
 +/*
 + * Poisons the shadow memory for 'size' bytes starting from 'addr'.
 + * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
 + */
 +void kasan_poison_shadow(const void *address, size_t size, u8 value)
 +{
 +	void *shadow_start, *shadow_end;
 +
 +	/*
 +	 * Perform shadow offset calculation based on untagged address, as
 +	 * some of the callers (e.g. kasan_poison_object_data) pass tagged
 +	 * addresses to this function.
 +	 */
 +	address = reset_tag(address);
 +
 +	shadow_start = kasan_mem_to_shadow(address);
 +	shadow_end = kasan_mem_to_shadow(address + size);
 +
 +	__memset(shadow_start, value, shadow_end - shadow_start);
 +}
 +
 +void kasan_unpoison_shadow(const void *address, size_t size)
  {
 -	kasan_unpoison(address, size);
 +	u8 tag = get_tag(address);
 +
 +	/*
 +	 * Perform shadow offset calculation based on untagged address, as
 +	 * some of the callers (e.g. kasan_unpoison_object_data) pass tagged
 +	 * addresses to this function.
 +	 */
 +	address = reset_tag(address);
 +
 +	kasan_poison_shadow(address, size, tag);
 +
 +	if (size & KASAN_SHADOW_MASK) {
 +		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
 +
 +		if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
 +			*shadow = tag;
 +		else
 +			*shadow = size & KASAN_SHADOW_MASK;
 +	}
  }
  
 +static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
 +{
 +	void *base = task_stack_page(task);
 +	size_t size = sp - base;
 +
 +	kasan_unpoison_shadow(base, size);
 +}
 +
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_KASAN_STACK
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  /* Unpoison the entire stack for a task. */
  void kasan_unpoison_task_stack(struct task_struct *task)
  {
diff --cc mm/kasan/kasan.h
index 2db4c5c1b473,3436c6bf7c0c..000000000000
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@@ -160,11 -209,33 +160,38 @@@ void kasan_poison_shadow(const void *ad
   * @ret_ip: return address
   * @return: true if access was valid, false if invalid
   */
 -bool kasan_check_range(unsigned long addr, size_t size, bool write,
 +bool check_memory_region(unsigned long addr, size_t size, bool write,
  				unsigned long ret_ip);
  
++<<<<<<< HEAD
 +void *find_first_bad_addr(void *addr, size_t size);
 +const char *get_bug_type(struct kasan_access_info *info);
++=======
+ #else /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
+ 
+ static inline bool addr_has_metadata(const void *addr)
+ {
+ 	return (is_vmalloc_addr(addr) || virt_addr_valid(addr));
+ }
+ 
+ #endif /* CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */
+ 
+ #if defined(CONFIG_KASAN_SW_TAGS) || defined(CONFIG_KASAN_HW_TAGS)
+ void kasan_print_tags(u8 addr_tag, const void *addr);
+ #else
+ static inline void kasan_print_tags(u8 addr_tag, const void *addr) { }
+ #endif
+ 
+ void *kasan_find_first_bad_addr(void *addr, size_t size);
+ const char *kasan_get_bug_type(struct kasan_access_info *info);
+ void kasan_metadata_fetch_row(char *buffer, void *row);
+ 
+ #if defined(CONFIG_KASAN_GENERIC) && defined(CONFIG_KASAN_STACK)
+ void kasan_print_address_stack_frame(const void *addr);
+ #else
+ static inline void kasan_print_address_stack_frame(const void *addr) { }
+ #endif
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  
  bool kasan_report(unsigned long addr, size_t size,
  		bool is_write, unsigned long ip);
diff --cc security/Kconfig.hardening
index e6e60aaf0762,a56c36470cb1..000000000000
--- a/security/Kconfig.hardening
+++ b/security/Kconfig.hardening
@@@ -1,5 -1,193 +1,195 @@@
 -# SPDX-License-Identifier: GPL-2.0-only
  menu "Kernel hardening options"
  
++<<<<<<< HEAD
++=======
+ config GCC_PLUGIN_STRUCTLEAK
+ 	bool
+ 	help
+ 	  While the kernel is built with warnings enabled for any missed
+ 	  stack variable initializations, this warning is silenced for
+ 	  anything passed by reference to another function, under the
+ 	  occasionally misguided assumption that the function will do
+ 	  the initialization. As this regularly leads to exploitable
+ 	  flaws, this plugin is available to identify and zero-initialize
+ 	  such variables, depending on the chosen level of coverage.
+ 
+ 	  This plugin was originally ported from grsecurity/PaX. More
+ 	  information at:
+ 	   * https://grsecurity.net/
+ 	   * https://pax.grsecurity.net/
+ 
+ menu "Memory initialization"
+ 
+ config CC_HAS_AUTO_VAR_INIT_PATTERN
+ 	def_bool $(cc-option,-ftrivial-auto-var-init=pattern)
+ 
+ config CC_HAS_AUTO_VAR_INIT_ZERO
+ 	def_bool $(cc-option,-ftrivial-auto-var-init=zero -enable-trivial-auto-var-init-zero-knowing-it-will-be-removed-from-clang)
+ 
+ choice
+ 	prompt "Initialize kernel stack variables at function entry"
+ 	default GCC_PLUGIN_STRUCTLEAK_BYREF_ALL if COMPILE_TEST && GCC_PLUGINS
+ 	default INIT_STACK_ALL_PATTERN if COMPILE_TEST && CC_HAS_AUTO_VAR_INIT_PATTERN
+ 	default INIT_STACK_NONE
+ 	help
+ 	  This option enables initialization of stack variables at
+ 	  function entry time. This has the possibility to have the
+ 	  greatest coverage (since all functions can have their
+ 	  variables initialized), but the performance impact depends
+ 	  on the function calling complexity of a given workload's
+ 	  syscalls.
+ 
+ 	  This chooses the level of coverage over classes of potentially
+ 	  uninitialized variables. The selected class will be
+ 	  initialized before use in a function.
+ 
+ 	config INIT_STACK_NONE
+ 		bool "no automatic initialization (weakest)"
+ 		help
+ 		  Disable automatic stack variable initialization.
+ 		  This leaves the kernel vulnerable to the standard
+ 		  classes of uninitialized stack variable exploits
+ 		  and information exposures.
+ 
+ 	config GCC_PLUGIN_STRUCTLEAK_USER
+ 		bool "zero-init structs marked for userspace (weak)"
+ 		depends on GCC_PLUGINS
+ 		select GCC_PLUGIN_STRUCTLEAK
+ 		help
+ 		  Zero-initialize any structures on the stack containing
+ 		  a __user attribute. This can prevent some classes of
+ 		  uninitialized stack variable exploits and information
+ 		  exposures, like CVE-2013-2141:
+ 		  https://git.kernel.org/linus/b9e146d8eb3b9eca
+ 
+ 	config GCC_PLUGIN_STRUCTLEAK_BYREF
+ 		bool "zero-init structs passed by reference (strong)"
+ 		depends on GCC_PLUGINS
+ 		depends on !(KASAN && KASAN_STACK)
+ 		select GCC_PLUGIN_STRUCTLEAK
+ 		help
+ 		  Zero-initialize any structures on the stack that may
+ 		  be passed by reference and had not already been
+ 		  explicitly initialized. This can prevent most classes
+ 		  of uninitialized stack variable exploits and information
+ 		  exposures, like CVE-2017-1000410:
+ 		  https://git.kernel.org/linus/06e7e776ca4d3654
+ 
+ 		  As a side-effect, this keeps a lot of variables on the
+ 		  stack that can otherwise be optimized out, so combining
+ 		  this with CONFIG_KASAN_STACK can lead to a stack overflow
+ 		  and is disallowed.
+ 
+ 	config GCC_PLUGIN_STRUCTLEAK_BYREF_ALL
+ 		bool "zero-init anything passed by reference (very strong)"
+ 		depends on GCC_PLUGINS
+ 		depends on !(KASAN && KASAN_STACK)
+ 		select GCC_PLUGIN_STRUCTLEAK
+ 		help
+ 		  Zero-initialize any stack variables that may be passed
+ 		  by reference and had not already been explicitly
+ 		  initialized. This is intended to eliminate all classes
+ 		  of uninitialized stack variable exploits and information
+ 		  exposures.
+ 
+ 	config INIT_STACK_ALL_PATTERN
+ 		bool "0xAA-init everything on the stack (strongest)"
+ 		depends on CC_HAS_AUTO_VAR_INIT_PATTERN
+ 		help
+ 		  Initializes everything on the stack with a 0xAA
+ 		  pattern. This is intended to eliminate all classes
+ 		  of uninitialized stack variable exploits and information
+ 		  exposures, even variables that were warned to have been
+ 		  left uninitialized.
+ 
+ 		  Pattern initialization is known to provoke many existing bugs
+ 		  related to uninitialized locals, e.g. pointers receive
+ 		  non-NULL values, buffer sizes and indices are very big.
+ 
+ 	config INIT_STACK_ALL_ZERO
+ 		bool "zero-init everything on the stack (strongest and safest)"
+ 		depends on CC_HAS_AUTO_VAR_INIT_ZERO
+ 		help
+ 		  Initializes everything on the stack with a zero
+ 		  value. This is intended to eliminate all classes
+ 		  of uninitialized stack variable exploits and information
+ 		  exposures, even variables that were warned to have been
+ 		  left uninitialized.
+ 
+ 		  Zero initialization provides safe defaults for strings,
+ 		  pointers, indices and sizes, and is therefore
+ 		  more suitable as a security mitigation measure.
+ 
+ endchoice
+ 
+ config GCC_PLUGIN_STRUCTLEAK_VERBOSE
+ 	bool "Report forcefully initialized variables"
+ 	depends on GCC_PLUGIN_STRUCTLEAK
+ 	depends on !COMPILE_TEST	# too noisy
+ 	help
+ 	  This option will cause a warning to be printed each time the
+ 	  structleak plugin finds a variable it thinks needs to be
+ 	  initialized. Since not all existing initializers are detected
+ 	  by the plugin, this can produce false positive warnings.
+ 
+ config GCC_PLUGIN_STACKLEAK
+ 	bool "Poison kernel stack before returning from syscalls"
+ 	depends on GCC_PLUGINS
+ 	depends on HAVE_ARCH_STACKLEAK
+ 	help
+ 	  This option makes the kernel erase the kernel stack before
+ 	  returning from system calls. This has the effect of leaving
+ 	  the stack initialized to the poison value, which both reduces
+ 	  the lifetime of any sensitive stack contents and reduces
+ 	  potential for uninitialized stack variable exploits or information
+ 	  exposures (it does not cover functions reaching the same stack
+ 	  depth as prior functions during the same syscall). This blocks
+ 	  most uninitialized stack variable attacks, with the performance
+ 	  impact being driven by the depth of the stack usage, rather than
+ 	  the function calling complexity.
+ 
+ 	  The performance impact on a single CPU system kernel compilation
+ 	  sees a 1% slowdown, other systems and workloads may vary and you
+ 	  are advised to test this feature on your expected workload before
+ 	  deploying it.
+ 
+ 	  This plugin was ported from grsecurity/PaX. More information at:
+ 	   * https://grsecurity.net/
+ 	   * https://pax.grsecurity.net/
+ 
+ config STACKLEAK_TRACK_MIN_SIZE
+ 	int "Minimum stack frame size of functions tracked by STACKLEAK"
+ 	default 100
+ 	range 0 4096
+ 	depends on GCC_PLUGIN_STACKLEAK
+ 	help
+ 	  The STACKLEAK gcc plugin instruments the kernel code for tracking
+ 	  the lowest border of the kernel stack (and for some other purposes).
+ 	  It inserts the stackleak_track_stack() call for the functions with
+ 	  a stack frame size greater than or equal to this parameter.
+ 	  If unsure, leave the default value 100.
+ 
+ config STACKLEAK_METRICS
+ 	bool "Show STACKLEAK metrics in the /proc file system"
+ 	depends on GCC_PLUGIN_STACKLEAK
+ 	depends on PROC_FS
+ 	help
+ 	  If this is set, STACKLEAK metrics for every task are available in
+ 	  the /proc file system. In particular, /proc/<pid>/stack_depth
+ 	  shows the maximum kernel stack consumption for the current and
+ 	  previous syscalls. Although this information is not precise, it
+ 	  can be useful for estimating the STACKLEAK performance impact for
+ 	  your workloads.
+ 
+ config STACKLEAK_RUNTIME_DISABLE
+ 	bool "Allow runtime disabling of kernel stack erasing"
+ 	depends on GCC_PLUGIN_STACKLEAK
+ 	help
+ 	  This option provides 'stack_erasing' sysctl, which can be used in
+ 	  runtime to control kernel stack erasing for kernels built with
+ 	  CONFIG_GCC_PLUGIN_STACKLEAK.
+ 
++>>>>>>> 02c587733c81 (kasan: remove redundant config option)
  config INIT_ON_ALLOC_DEFAULT_ON
  	bool "Enable heap memory zeroing on allocation by default"
  	help
* Unmerged path mm/kasan/report_generic.c
* Unmerged path arch/arm64/kernel/sleep.S
* Unmerged path arch/x86/kernel/acpi/wakeup_64.S
* Unmerged path include/linux/kasan.h
* Unmerged path lib/Kconfig.kasan
* Unmerged path mm/kasan/common.c
* Unmerged path mm/kasan/kasan.h
* Unmerged path mm/kasan/report_generic.c
diff --git a/scripts/Makefile.kasan b/scripts/Makefile.kasan
index a4a61265f0f8..32a8f58a667c 100644
--- a/scripts/Makefile.kasan
+++ b/scripts/Makefile.kasan
@@ -6,6 +6,12 @@ endif
 
 cc-param = $(call cc-option, -mllvm -$(1), $(call cc-option, --param $(1)))
 
+ifdef CONFIG_KASAN_STACK
+	stack_enable := 1
+else
+	stack_enable := 0
+endif
+
 ifdef CONFIG_KASAN_GENERIC
 
 ifdef CONFIG_KASAN_INLINE
@@ -29,7 +35,7 @@ else
 	CFLAGS_KASAN := $(CFLAGS_KASAN_SHADOW) \
 	 $(call cc-param,asan-globals=1) \
 	 $(call cc-param,asan-instrumentation-with-call-threshold=$(call_threshold)) \
-	 $(call cc-param,asan-stack=$(CONFIG_KASAN_STACK)) \
+	 $(call cc-param,asan-stack=$(stack_enable)) \
 	 $(call cc-param,asan-instrument-allocas=1)
 endif
 
@@ -44,7 +50,7 @@ else
 endif
 
 CFLAGS_KASAN := -fsanitize=kernel-hwaddress \
-		$(call cc-param,hwasan-instrument-stack=$(CONFIG_KASAN_STACK)) \
+		$(call cc-param,hwasan-instrument-stack=$(stack_enable)) \
 		$(call cc-param,hwasan-use-short-granules=0) \
 		$(instrumentation_flags)
 
* Unmerged path security/Kconfig.hardening
