writeback: use READ_ONCE for unlocked reads of writeback stats

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Jan Kara <jack@suse.cz>
commit 20792ebf3eeb828a692b29f3000673cb9ca83c3a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/20792ebf.failed

We do some unlocked reads of writeback statistics like
avg_write_bandwidth, dirty_ratelimit, or bw_time_stamp.  Generally we are
fine with getting somewhat out-of-date values but actually getting
different values in various parts of the functions because the compiler
decided to reload value from original memory location could confuse
calculations.  Use READ_ONCE for these unlocked accesses and WRITE_ONCE
for the updates to be on the safe side.

Link: https://lkml.kernel.org/r/20210713104716.22868-5-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Cc: Michael Stapelberg <stapelberg+linux@google.com>
	Cc: Wu Fengguang <fengguang.wu@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 20792ebf3eeb828a692b29f3000673cb9ca83c3a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page-writeback.c
diff --cc mm/page-writeback.c
index aa8d74254368,c3b00c6f30ce..000000000000
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@@ -1378,10 -1369,11 +1378,15 @@@ static void __wb_update_bandwidth(struc
  
  	wb->dirtied_stamp = dirtied;
  	wb->written_stamp = written;
++<<<<<<< HEAD
 +	wb->bw_time_stamp = now;
++=======
+ 	WRITE_ONCE(wb->bw_time_stamp, now);
+ 	spin_unlock(&wb->list_lock);
++>>>>>>> 20792ebf3eeb (writeback: use READ_ONCE for unlocked reads of writeback stats)
  }
  
 -void wb_update_bandwidth(struct bdi_writeback *wb)
 +static void wb_update_bandwidth(struct bdi_writeback *wb)
  {
  	struct dirty_throttle_control gdtc = { GDTC_INIT(wb) };
  
@@@ -1731,15 -1721,12 +1736,20 @@@ free_running
  		if (dirty_exceeded && !wb->dirty_exceeded)
  			wb->dirty_exceeded = 1;
  
++<<<<<<< HEAD
 +		if (time_is_before_jiffies(wb->bw_time_stamp +
 +					   BANDWIDTH_INTERVAL)) {
 +			spin_lock(&wb->list_lock);
++=======
+ 		if (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +
+ 					   BANDWIDTH_INTERVAL))
++>>>>>>> 20792ebf3eeb (writeback: use READ_ONCE for unlocked reads of writeback stats)
  			__wb_update_bandwidth(gdtc, mdtc, true);
 +			spin_unlock(&wb->list_lock);
 +		}
  
  		/* throttle according to the chosen dtc */
- 		dirty_ratelimit = wb->dirty_ratelimit;
+ 		dirty_ratelimit = READ_ONCE(wb->dirty_ratelimit);
  		task_ratelimit = ((u64)dirty_ratelimit * sdtc->pos_ratio) >>
  							RATELIMIT_CALC_SHIFT;
  		max_pause = wb_max_pause(wb, sdtc->wb_dirty);
@@@ -2379,7 -2371,14 +2389,18 @@@ int do_writepages(struct address_space 
  		cond_resched();
  		congestion_wait(BLK_RW_ASYNC, HZ/50);
  	}
++<<<<<<< HEAD
 +	wb_update_bandwidth(wb);
++=======
+ 	/*
+ 	 * Usually few pages are written by now from those we've just submitted
+ 	 * but if there's constant writeback being submitted, this makes sure
+ 	 * writeback bandwidth is updated once in a while.
+ 	 */
+ 	if (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +
+ 				   BANDWIDTH_INTERVAL))
+ 		wb_update_bandwidth(wb);
++>>>>>>> 20792ebf3eeb (writeback: use READ_ONCE for unlocked reads of writeback stats)
  	return ret;
  }
  
* Unmerged path mm/page-writeback.c
