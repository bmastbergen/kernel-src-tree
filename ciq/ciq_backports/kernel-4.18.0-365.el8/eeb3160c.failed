kasan, mm: rename kasan_poison_kfree

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit eeb3160c2419e0f1045537acac7b19cba64112f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/eeb3160c.failed

Rename kasan_poison_kfree() to kasan_slab_free_mempool() as it better
reflects what this annotation does. Also add a comment that explains the
PageSlab() check.

No functional changes.

Link: https://lkml.kernel.org/r/141675fb493555e984c5dca555e9d9f768c7bbaa.1606162397.git.andreyknvl@google.com
Link: https://linux-review.googlesource.com/id/I5026f87364e556b506ef1baee725144bb04b8810
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Marco Elver <elver@google.com>
	Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Branislav Rankov <Branislav.Rankov@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Evgenii Stepanov <eugenis@google.com>
	Cc: Kevin Brodsky <kevin.brodsky@arm.com>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit eeb3160c2419e0f1045537acac7b19cba64112f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	mm/kasan/common.c
#	mm/mempool.c
diff --cc include/linux/kasan.h
index f00d17cf6822,6f0c5d9aa43f..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -69,28 -81,154 +69,154 @@@ struct kasan_cache 
  	int free_meta_offset;
  };
  
 -#ifdef CONFIG_KASAN_HW_TAGS
 -DECLARE_STATIC_KEY_FALSE(kasan_flag_enabled);
 -static __always_inline bool kasan_enabled(void)
 -{
 -	return static_branch_likely(&kasan_flag_enabled);
 -}
 +/*
 + * These functions provide a special case to support backing module
 + * allocations with real shadow memory. With KASAN vmalloc, the special
 + * case is unnecessary, as the work is handled in the generic case.
 + */
 +#ifndef CONFIG_KASAN_VMALLOC
 +int kasan_module_alloc(void *addr, size_t size);
 +void kasan_free_shadow(const struct vm_struct *vm);
  #else
 -static inline bool kasan_enabled(void)
 -{
 -	return true;
 -}
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
  #endif
  
 -void __kasan_unpoison_range(const void *addr, size_t size);
 -static __always_inline void kasan_unpoison_range(const void *addr, size_t size)
 -{
 -	if (kasan_enabled())
 -		__kasan_unpoison_range(addr, size);
 -}
 +int kasan_add_zero_shadow(void *start, unsigned long size);
 +void kasan_remove_zero_shadow(void *start, unsigned long size);
  
 -void __kasan_alloc_pages(struct page *page, unsigned int order);
 -static __always_inline void kasan_alloc_pages(struct page *page,
 -						unsigned int order)
 +size_t __ksize(const void *);
 +static inline void kasan_unpoison_slab(const void *ptr)
  {
++<<<<<<< HEAD
 +	kasan_unpoison_shadow(ptr, __ksize(ptr));
++=======
+ 	if (kasan_enabled())
+ 		__kasan_alloc_pages(page, order);
+ }
+ 
+ void __kasan_free_pages(struct page *page, unsigned int order);
+ static __always_inline void kasan_free_pages(struct page *page,
+ 						unsigned int order)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_free_pages(page, order);
+ }
+ 
+ void __kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
+ 				slab_flags_t *flags);
+ static __always_inline void kasan_cache_create(struct kmem_cache *cache,
+ 				unsigned int *size, slab_flags_t *flags)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_cache_create(cache, size, flags);
+ }
+ 
+ size_t __kasan_metadata_size(struct kmem_cache *cache);
+ static __always_inline size_t kasan_metadata_size(struct kmem_cache *cache)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_metadata_size(cache);
+ 	return 0;
+ }
+ 
+ void __kasan_poison_slab(struct page *page);
+ static __always_inline void kasan_poison_slab(struct page *page)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_slab(page);
+ }
+ 
+ void __kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_unpoison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_unpoison_object_data(cache, object);
+ }
+ 
+ void __kasan_poison_object_data(struct kmem_cache *cache, void *object);
+ static __always_inline void kasan_poison_object_data(struct kmem_cache *cache,
+ 							void *object)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_poison_object_data(cache, object);
+ }
+ 
+ void * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,
+ 					  const void *object);
+ static __always_inline void * __must_check kasan_init_slab_obj(
+ 				struct kmem_cache *cache, const void *object)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_init_slab_obj(cache, object);
+ 	return (void *)object;
+ }
+ 
+ bool __kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
+ static __always_inline bool kasan_slab_free(struct kmem_cache *s, void *object,
+ 						unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_free(s, object, ip);
+ 	return false;
+ }
+ 
+ void __kasan_slab_free_mempool(void *ptr, unsigned long ip);
+ static __always_inline void kasan_slab_free_mempool(void *ptr, unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_slab_free_mempool(ptr, ip);
+ }
+ 
+ void * __must_check __kasan_slab_alloc(struct kmem_cache *s,
+ 				       void *object, gfp_t flags);
+ static __always_inline void * __must_check kasan_slab_alloc(
+ 				struct kmem_cache *s, void *object, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_slab_alloc(s, object, flags);
+ 	return object;
+ }
+ 
+ void * __must_check __kasan_kmalloc(struct kmem_cache *s, const void *object,
+ 				    size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc(struct kmem_cache *s,
+ 				const void *object, size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc(s, object, size, flags);
+ 	return (void *)object;
+ }
+ 
+ void * __must_check __kasan_kmalloc_large(const void *ptr,
+ 					  size_t size, gfp_t flags);
+ static __always_inline void * __must_check kasan_kmalloc_large(const void *ptr,
+ 						      size_t size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_kmalloc_large(ptr, size, flags);
+ 	return (void *)ptr;
+ }
+ 
+ void * __must_check __kasan_krealloc(const void *object,
+ 				     size_t new_size, gfp_t flags);
+ static __always_inline void * __must_check kasan_krealloc(const void *object,
+ 						 size_t new_size, gfp_t flags)
+ {
+ 	if (kasan_enabled())
+ 		return __kasan_krealloc(object, new_size, flags);
+ 	return (void *)object;
+ }
+ 
+ void __kasan_kfree_large(void *ptr, unsigned long ip);
+ static __always_inline void kasan_kfree_large(void *ptr, unsigned long ip)
+ {
+ 	if (kasan_enabled())
+ 		__kasan_kfree_large(ptr, ip);
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  }
 +size_t kasan_metadata_size(struct kmem_cache *cache);
  
  bool kasan_save_enable_multi_shot(void);
  void kasan_restore_multi_shot(bool enabled);
@@@ -121,13 -256,17 +247,23 @@@ static inline void *kasan_init_slab_obj
  {
  	return (void *)object;
  }
 -static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 -				   unsigned long ip)
 +
 +static inline void *kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags)
  {
++<<<<<<< HEAD
 +	return ptr;
++=======
+ 	return false;
+ }
+ static inline void kasan_slab_free_mempool(void *ptr, unsigned long ip) {}
+ static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
+ 				   gfp_t flags)
+ {
+ 	return object;
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  }
 +static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
 +static inline void kasan_poison_kfree(void *ptr, unsigned long ip) {}
  static inline void *kasan_kmalloc(struct kmem_cache *s, const void *object,
  				size_t size, gfp_t flags)
  {
@@@ -138,31 -281,7 +274,35 @@@ static inline void *kasan_krealloc(cons
  {
  	return (void *)object;
  }
++<<<<<<< HEAD
 +
 +static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
 +				   gfp_t flags)
 +{
 +	return object;
 +}
 +static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 +				   unsigned long ip)
 +{
 +	return false;
 +}
 +
 +static inline int kasan_module_alloc(void *addr, size_t size) { return 0; }
 +static inline void kasan_free_shadow(const struct vm_struct *vm) {}
 +
 +static inline int kasan_add_zero_shadow(void *start, unsigned long size)
 +{
 +	return 0;
 +}
 +static inline void kasan_remove_zero_shadow(void *start,
 +					unsigned long size)
 +{}
 +
 +static inline void kasan_unpoison_slab(const void *ptr) { }
 +static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
++=======
+ static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  
  #endif /* CONFIG_KASAN */
  
diff --cc mm/kasan/common.c
index e989322c0f3f,d0f8d7a955cd..000000000000
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@@ -445,12 -326,40 +445,44 @@@ static bool __kasan_slab_free(struct km
  	return IS_ENABLED(CONFIG_KASAN_GENERIC);
  }
  
 -bool __kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
 +bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
  {
 -	return ____kasan_slab_free(cache, object, ip, true);
 +	return __kasan_slab_free(cache, object, ip, true);
  }
  
++<<<<<<< HEAD
 +static void *__kasan_kmalloc(struct kmem_cache *cache, const void *object,
++=======
+ void __kasan_slab_free_mempool(void *ptr, unsigned long ip)
+ {
+ 	struct page *page;
+ 
+ 	page = virt_to_head_page(ptr);
+ 
+ 	/*
+ 	 * Even though this function is only called for kmem_cache_alloc and
+ 	 * kmalloc backed mempool allocations, those allocations can still be
+ 	 * !PageSlab() when the size provided to kmalloc is larger than
+ 	 * KMALLOC_MAX_SIZE, and kmalloc falls back onto page_alloc.
+ 	 */
+ 	if (unlikely(!PageSlab(page))) {
+ 		if (ptr != page_address(page)) {
+ 			kasan_report_invalid_free(ptr, ip);
+ 			return;
+ 		}
+ 		poison_range(ptr, page_size(page), KASAN_FREE_PAGE);
+ 	} else {
+ 		____kasan_slab_free(page->slab_cache, ptr, ip, false);
+ 	}
+ }
+ 
+ static void set_alloc_info(struct kmem_cache *cache, void *object, gfp_t flags)
+ {
+ 	kasan_set_track(&kasan_get_alloc_meta(cache, object)->alloc_track, flags);
+ }
+ 
+ static void *____kasan_kmalloc(struct kmem_cache *cache, const void *object,
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  				size_t size, gfp_t flags, bool keep_tag)
  {
  	unsigned long redzone_start;
@@@ -536,24 -445,7 +568,28 @@@ void * __must_check kasan_krealloc(cons
  						flags, true);
  }
  
++<<<<<<< HEAD
 +void kasan_poison_kfree(void *ptr, unsigned long ip)
 +{
 +	struct page *page;
 +
 +	page = virt_to_head_page(ptr);
 +
 +	if (unlikely(!PageSlab(page))) {
 +		if (ptr != page_address(page)) {
 +			kasan_report_invalid_free(ptr, ip);
 +			return;
 +		}
 +		kasan_poison_shadow(ptr, page_size(page), KASAN_FREE_PAGE);
 +	} else {
 +		__kasan_slab_free(page->slab_cache, ptr, ip, false);
 +	}
 +}
 +
 +void kasan_kfree_large(void *ptr, unsigned long ip)
++=======
+ void __kasan_kfree_large(void *ptr, unsigned long ip)
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  {
  	if (ptr != page_address(virt_to_head_page(ptr)))
  		kasan_report_invalid_free(ptr, ip);
diff --cc mm/mempool.c
index 6bfbd4d73ec5,624ed51b060f..000000000000
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@@ -106,8 -104,8 +106,13 @@@ static inline void poison_element(mempo
  static __always_inline void kasan_poison_element(mempool_t *pool, void *element)
  {
  	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
++<<<<<<< HEAD
 +		kasan_poison_kfree(element, _RET_IP_);
 +	if (pool->alloc == mempool_alloc_pages)
++=======
+ 		kasan_slab_free_mempool(element, _RET_IP_);
+ 	else if (pool->alloc == mempool_alloc_pages)
++>>>>>>> eeb3160c2419 (kasan, mm: rename kasan_poison_kfree)
  		kasan_free_pages(element, (unsigned long)pool->pool_data);
  }
  
* Unmerged path include/linux/kasan.h
* Unmerged path mm/kasan/common.c
* Unmerged path mm/mempool.c
