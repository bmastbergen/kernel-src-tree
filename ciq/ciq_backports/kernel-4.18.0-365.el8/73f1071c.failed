igc: Add support for XDP_TX action

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andre Guedes <andre.guedes@intel.com>
commit 73f1071c1d2952b8c93cd6cd99744768c59ec840
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/73f1071c.failed

Add support for XDP_TX action which enables XDP programs to transmit
back receiving frames.

I225 controller has only 4 Tx hardware queues. Since XDP programs may
not even issue an XDP_TX action, this patch doesn't reserve dedicated
queues just for XDP like other Intel drivers do. Instead, the queues
are shared between the network stack and XDP. The netdev queue lock is
used to ensure mutual exclusion.

Since frames can now be transmitted via XDP_TX, the igc_tx_buffer
structure is modified so we are able to save a reference to the xdp
frame for later clean up once the packet is transmitted. The tx_buffer
is mapped to either a skb or a xdpf so we use a union to save the skb
or xdpf pointer and have a bit in tx_flags to indicate which field to
use.

This patch has been tested with the sample app "xdp2" located in
samples/bpf/ dir.

	Signed-off-by: Andre Guedes <andre.guedes@intel.com>
	Signed-off-by: Vedang Patel <vedang.patel@intel.com>
	Signed-off-by: Jithu Joseph <jithu.joseph@intel.com>
	Reviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: Dvora Fuxbrumer <dvorax.fuxbrumer@linux.intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 73f1071c1d2952b8c93cd6cd99744768c59ec840)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/igc/igc_main.c
#	drivers/net/ethernet/intel/igc/igc_xdp.c
#	drivers/net/ethernet/intel/igc/igc_xdp.h
diff --cc drivers/net/ethernet/intel/igc/igc_main.c
index 261f28037c1a,5ad360e52038..000000000000
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@@ -22,6 -23,10 +22,13 @@@
  
  #define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)
  
++<<<<<<< HEAD
++=======
+ #define IGC_XDP_PASS		0
+ #define IGC_XDP_CONSUMED	BIT(0)
+ #define IGC_XDP_TX		BIT(1)
+ 
++>>>>>>> 73f1071c1d29 (igc: Add support for XDP_TX action)
  static int debug = -1;
  
  MODULE_AUTHOR("Intel Corporation, <linux.nics@intel.com>");
@@@ -1885,6 -1919,170 +1901,173 @@@ static void igc_alloc_rx_buffers(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int igc_xdp_init_tx_buffer(struct igc_tx_buffer *buffer,
+ 				  struct xdp_frame *xdpf,
+ 				  struct igc_ring *ring)
+ {
+ 	dma_addr_t dma;
+ 
+ 	dma = dma_map_single(ring->dev, xdpf->data, xdpf->len, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ring->dev, dma)) {
+ 		netdev_err_once(ring->netdev, "Failed to map DMA for TX\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	buffer->xdpf = xdpf;
+ 	buffer->tx_flags = IGC_TX_FLAGS_XDP;
+ 	buffer->protocol = 0;
+ 	buffer->bytecount = xdpf->len;
+ 	buffer->gso_segs = 1;
+ 	buffer->time_stamp = jiffies;
+ 	dma_unmap_len_set(buffer, len, xdpf->len);
+ 	dma_unmap_addr_set(buffer, dma, dma);
+ 	return 0;
+ }
+ 
+ /* This function requires __netif_tx_lock is held by the caller. */
+ static int igc_xdp_init_tx_descriptor(struct igc_ring *ring,
+ 				      struct xdp_frame *xdpf)
+ {
+ 	struct igc_tx_buffer *buffer;
+ 	union igc_adv_tx_desc *desc;
+ 	u32 cmd_type, olinfo_status;
+ 	int err;
+ 
+ 	if (!igc_desc_unused(ring))
+ 		return -EBUSY;
+ 
+ 	buffer = &ring->tx_buffer_info[ring->next_to_use];
+ 	err = igc_xdp_init_tx_buffer(buffer, xdpf, ring);
+ 	if (err)
+ 		return err;
+ 
+ 	cmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |
+ 		   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |
+ 		   buffer->bytecount;
+ 	olinfo_status = buffer->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;
+ 
+ 	desc = IGC_TX_DESC(ring, ring->next_to_use);
+ 	desc->read.cmd_type_len = cpu_to_le32(cmd_type);
+ 	desc->read.olinfo_status = cpu_to_le32(olinfo_status);
+ 	desc->read.buffer_addr = cpu_to_le64(dma_unmap_addr(buffer, dma));
+ 
+ 	netdev_tx_sent_queue(txring_txq(ring), buffer->bytecount);
+ 
+ 	buffer->next_to_watch = desc;
+ 
+ 	ring->next_to_use++;
+ 	if (ring->next_to_use == ring->count)
+ 		ring->next_to_use = 0;
+ 
+ 	return 0;
+ }
+ 
+ static struct igc_ring *igc_xdp_get_tx_ring(struct igc_adapter *adapter,
+ 					    int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= adapter->num_tx_queues)
+ 		index -= adapter->num_tx_queues;
+ 
+ 	return adapter->tx_ring[index];
+ }
+ 
+ static int igc_xdp_xmit_back(struct igc_adapter *adapter, struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return -EFAULT;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	res = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 	__netif_tx_unlock(nq);
+ 	return res;
+ }
+ 
+ static struct sk_buff *igc_xdp_run_prog(struct igc_adapter *adapter,
+ 					struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 	u32 act;
+ 
+ 	rcu_read_lock();
+ 
+ 	prog = READ_ONCE(adapter->xdp_prog);
+ 	if (!prog) {
+ 		res = IGC_XDP_PASS;
+ 		goto unlock;
+ 	}
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		res = IGC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		if (igc_xdp_xmit_back(adapter, xdp) < 0)
+ 			res = IGC_XDP_CONSUMED;
+ 		else
+ 			res = IGC_XDP_TX;
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(adapter->netdev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		res = IGC_XDP_CONSUMED;
+ 		break;
+ 	}
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 	return ERR_PTR(-res);
+ }
+ 
+ /* This function assumes __netif_tx_lock is held by the caller. */
+ static void igc_flush_tx_descriptors(struct igc_ring *ring)
+ {
+ 	/* Once tail pointer is updated, hardware can fetch the descriptors
+ 	 * any time so we issue a write membar here to ensure all memory
+ 	 * writes are complete before the tail pointer is updated.
+ 	 */
+ 	wmb();
+ 	writel(ring->next_to_use, ring->tail);
+ }
+ 
+ static void igc_finalize_xdp(struct igc_adapter *adapter, int status)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 
+ 	if (status & IGC_XDP_TX) {
+ 		ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 		nq = txring_txq(ring);
+ 
+ 		__netif_tx_lock(nq, cpu);
+ 		igc_flush_tx_descriptors(ring);
+ 		__netif_tx_unlock(nq);
+ 	}
+ }
+ 
++>>>>>>> 73f1071c1d29 (igc: Add support for XDP_TX action)
  static int igc_clean_rx_irq(struct igc_q_vector *q_vector, const int budget)
  {
  	unsigned int total_bytes = 0, total_packets = 0;
@@@ -1895,9 -2095,11 +2080,14 @@@
  	while (likely(total_packets < budget)) {
  		union igc_adv_rx_desc *rx_desc;
  		struct igc_rx_buffer *rx_buffer;
+ 		unsigned int size, truesize;
  		ktime_t timestamp = 0;
 -		struct xdp_buff xdp;
  		int pkt_offset = 0;
++<<<<<<< HEAD
 +		unsigned int size;
++=======
+ 		void *pktbuf;
++>>>>>>> 73f1071c1d29 (igc: Add support for XDP_TX action)
  
  		/* return some buffers to hardware, one at a time is too slow */
  		if (cleaned_count >= IGC_RX_BUFFER_WRITE) {
@@@ -1917,19 -2119,44 +2107,50 @@@
  		dma_rmb();
  
  		rx_buffer = igc_get_rx_buffer(rx_ring, size);
+ 		truesize = igc_get_rx_frame_truesize(rx_ring, size);
  
 -		pktbuf = page_address(rx_buffer->page) + rx_buffer->page_offset;
 -
  		if (igc_test_staterr(rx_desc, IGC_RXDADV_STAT_TSIP)) {
 +			void *pktbuf = page_address(rx_buffer->page) +
 +				       rx_buffer->page_offset;
 +
  			timestamp = igc_ptp_rx_pktstamp(q_vector->adapter,
  							pktbuf);
  			pkt_offset = IGC_TS_HDR_LEN;
  			size -= IGC_TS_HDR_LEN;
  		}
  
++<<<<<<< HEAD
 +		/* retrieve a buffer from the ring */
 +		if (skb)
++=======
+ 		if (!skb) {
+ 			xdp.data = pktbuf + pkt_offset;
+ 			xdp.data_end = xdp.data + size;
+ 			xdp.data_hard_start = pktbuf - igc_rx_offset(rx_ring);
+ 			xdp_set_data_meta_invalid(&xdp);
+ 			xdp.frame_sz = truesize;
+ 			xdp.rxq = &rx_ring->xdp_rxq;
+ 
+ 			skb = igc_xdp_run_prog(adapter, &xdp);
+ 		}
+ 
+ 		if (IS_ERR(skb)) {
+ 			unsigned int xdp_res = -PTR_ERR(skb);
+ 
+ 			switch (xdp_res) {
+ 			case IGC_XDP_CONSUMED:
+ 				rx_buffer->pagecnt_bias++;
+ 				break;
+ 			case IGC_XDP_TX:
+ 				igc_rx_buffer_flip(rx_buffer, truesize);
+ 				xdp_status |= xdp_res;
+ 				break;
+ 			}
+ 
+ 			total_packets++;
+ 			total_bytes += size;
+ 		} else if (skb)
++>>>>>>> 73f1071c1d29 (igc: Add support for XDP_TX action)
  			igc_add_rx_frag(rx_ring, rx_buffer, skb, size);
  		else if (ring_uses_build_skb(rx_ring))
  			skb = igc_build_skb(rx_ring, rx_buffer, rx_desc, size);
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.h
diff --git a/drivers/net/ethernet/intel/igc/igc.h b/drivers/net/ethernet/intel/igc/igc.h
index b00cd8696b6d..3d514b5d240d 100644
--- a/drivers/net/ethernet/intel/igc/igc.h
+++ b/drivers/net/ethernet/intel/igc/igc.h
@@ -111,6 +111,8 @@ struct igc_ring {
 			struct sk_buff *skb;
 		};
 	};
+
+	struct xdp_rxq_info xdp_rxq;
 } ____cacheline_internodealigned_in_smp;
 
 /* Board specific private data structure */
@@ -373,6 +375,8 @@ enum igc_tx_flags {
 	/* olinfo flags */
 	IGC_TX_FLAGS_IPV4	= 0x10,
 	IGC_TX_FLAGS_CSUM	= 0x20,
+
+	IGC_TX_FLAGS_XDP	= 0x100,
 };
 
 enum igc_boards {
@@ -395,7 +399,10 @@ enum igc_boards {
 struct igc_tx_buffer {
 	union igc_adv_tx_desc *next_to_watch;
 	unsigned long time_stamp;
-	struct sk_buff *skb;
+	union {
+		struct sk_buff *skb;
+		struct xdp_frame *xdpf;
+	};
 	unsigned int bytecount;
 	u16 gso_segs;
 	__be16 protocol;
* Unmerged path drivers/net/ethernet/intel/igc/igc_main.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.h
