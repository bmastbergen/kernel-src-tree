x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 3ac8d75778fc8c1c22daad9bc674166b862f6f6e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/3ac8d757.failed

With dynamically enabled features the copy function must know the features
and the size which is valid for the task. Retrieve them from fpstate.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20211013145323.181495492@linutronix.de
(cherry picked from commit 3ac8d75778fc8c1c22daad9bc674166b862f6f6e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/core.c
#	arch/x86/kernel/fpu/xstate.c
#	arch/x86/kernel/fpu/xstate.h
diff --cc arch/x86/kernel/fpu/core.c
index 3f16056105e8,04fef4795211..000000000000
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@@ -174,7 -187,7 +174,11 @@@ EXPORT_SYMBOL_GPL(fpu_swap_kvm_fpu)
  void fpu_copy_fpstate_to_kvm_uabi(struct fpu *fpu, void *buf,
  			       unsigned int size, u32 pkru)
  {
++<<<<<<< HEAD
 +	union fpregs_state *kstate = &fpu->state;
++=======
+ 	struct fpstate *kstate = fpu->fpstate;
++>>>>>>> 3ac8d75778fc (x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf())
  	union fpregs_state *ustate = buf;
  	struct membuf mb = { .p = buf, .left = size };
  
diff --cc arch/x86/kernel/fpu/xstate.c
index 8c27dcecbad5,54cc0a4db8e8..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -994,9 -967,10 +994,14 @@@ static void copy_feature(bool from_xsta
  }
  
  /**
 - * __copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
 + * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
   * @to:		membuf descriptor
++<<<<<<< HEAD
 + * @tsk:	The task from which to copy the saved xstate
++=======
+  * @fpstate:	The fpstate buffer from which to copy
+  * @pkru_val:	The PKRU value to store in the PKRU component
++>>>>>>> 3ac8d75778fc (x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf())
   * @copy_mode:	The requested copy mode
   *
   * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
@@@ -1005,12 -979,12 +1010,21 @@@
   *
   * It supports partial copy but @to.pos always starts from zero.
   */
++<<<<<<< HEAD
 +void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
 +			     enum xstate_copy_mode copy_mode)
 +{
 +	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 +	struct xregs_state *xsave = &tsk->thread.fpu.state.xsave;
 +	struct xregs_state *xinit = &init_fpstate.xsave;
++=======
+ void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
+ 			       u32 pkru_val, enum xstate_copy_mode copy_mode)
+ {
+ 	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
+ 	struct xregs_state *xinit = &init_fpstate.regs.xsave;
+ 	struct xregs_state *xsave = &fpstate->regs.xsave;
++>>>>>>> 3ac8d75778fc (x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf())
  	struct xstate_header header;
  	unsigned int zerofrom;
  	u64 mask;
@@@ -1110,6 -1083,25 +1124,28 @@@ out
  		membuf_zero(&to, to.left);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
+  * @to:		membuf descriptor
+  * @tsk:	The task from which to copy the saved xstate
+  * @copy_mode:	The requested copy mode
+  *
+  * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
+  * format, i.e. from the kernel internal hardware dependent storage format
+  * to the requested @mode. UABI XSTATE is always uncompacted!
+  *
+  * It supports partial copy but @to.pos always starts from zero.
+  */
+ void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 			     enum xstate_copy_mode copy_mode)
+ {
+ 	__copy_xstate_to_uabi_buf(to, tsk->thread.fpu.fpstate,
+ 				  tsk->thread.pkru, copy_mode);
+ }
+ 
++>>>>>>> 3ac8d75778fc (x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf())
  static int copy_from_buffer(void *dst, unsigned int offset, unsigned int size,
  			    const void *kbuf, const void __user *ubuf)
  {
diff --cc arch/x86/kernel/fpu/xstate.h
index 0789a04ee705,b74c5953558c..000000000000
--- a/arch/x86/kernel/fpu/xstate.h
+++ b/arch/x86/kernel/fpu/xstate.h
@@@ -15,4 -15,186 +15,189 @@@ static inline void xstate_init_xcomp_bv
  		xsave->header.xcomp_bv = mask | XCOMP_BV_COMPACTED_FORMAT;
  }
  
++<<<<<<< HEAD
++=======
+ extern void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
+ 				      u32 pkru_val, enum xstate_copy_mode copy_mode);
+ 
+ extern void fpu__init_cpu_xstate(void);
+ extern void fpu__init_system_xstate(void);
+ 
+ extern void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);
+ 
+ /* XSAVE/XRSTOR wrapper functions */
+ 
+ #ifdef CONFIG_X86_64
+ #define REX_PREFIX	"0x48, "
+ #else
+ #define REX_PREFIX
+ #endif
+ 
+ /* These macros all use (%edi)/(%rdi) as the single memory argument. */
+ #define XSAVE		".byte " REX_PREFIX "0x0f,0xae,0x27"
+ #define XSAVEOPT	".byte " REX_PREFIX "0x0f,0xae,0x37"
+ #define XSAVES		".byte " REX_PREFIX "0x0f,0xc7,0x2f"
+ #define XRSTOR		".byte " REX_PREFIX "0x0f,0xae,0x2f"
+ #define XRSTORS		".byte " REX_PREFIX "0x0f,0xc7,0x1f"
+ 
+ /*
+  * After this @err contains 0 on success or the trap number when the
+  * operation raises an exception.
+  */
+ #define XSTATE_OP(op, st, lmask, hmask, err)				\
+ 	asm volatile("1:" op "\n\t"					\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "2:\n\t"						\
+ 		     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_FAULT_MCE_SAFE)	\
+ 		     : [err] "=a" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * If XSAVES is enabled, it replaces XSAVEOPT because it supports a compact
+  * format and supervisor states in addition to modified optimization in
+  * XSAVEOPT.
+  *
+  * Otherwise, if XSAVEOPT is enabled, XSAVEOPT replaces XSAVE because XSAVEOPT
+  * supports modified optimization which is not supported by XSAVE.
+  *
+  * We use XSAVE as a fallback.
+  *
+  * The 661 label is defined in the ALTERNATIVE* macros as the address of the
+  * original instruction which gets replaced. We need to use it here as the
+  * address of the instruction where we might get an exception at.
+  */
+ #define XSTATE_XSAVE(st, lmask, hmask, err)				\
+ 	asm volatile(ALTERNATIVE_2(XSAVE,				\
+ 				   XSAVEOPT, X86_FEATURE_XSAVEOPT,	\
+ 				   XSAVES,   X86_FEATURE_XSAVES)	\
+ 		     "\n"						\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "3:\n"						\
+ 		     ".pushsection .fixup,\"ax\"\n"			\
+ 		     "4: movl $-2, %[err]\n"				\
+ 		     "jmp 3b\n"						\
+ 		     ".popsection\n"					\
+ 		     _ASM_EXTABLE(661b, 4b)				\
+ 		     : [err] "=r" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact
+  * XSAVE area format.
+  */
+ #define XSTATE_XRESTORE(st, lmask, hmask)				\
+ 	asm volatile(ALTERNATIVE(XRSTOR,				\
+ 				 XRSTORS, X86_FEATURE_XSAVES)		\
+ 		     "\n"						\
+ 		     "3:\n"						\
+ 		     _ASM_EXTABLE_TYPE(661b, 3b, EX_TYPE_FPU_RESTORE)	\
+ 		     :							\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * Save processor xstate to xsave area.
+  *
+  * Uses either XSAVE or XSAVEOPT or XSAVES depending on the CPU features
+  * and command line options. The choice is permanent until the next reboot.
+  */
+ static inline void os_xsave(struct fpstate *fpstate)
+ {
+ 	u64 mask = fpstate->xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	WARN_ON_FPU(!alternatives_patched);
+ 
+ 	XSTATE_XSAVE(&fpstate->regs.xsave, lmask, hmask, err);
+ 
+ 	/* We should never fault when copying to a kernel buffer: */
+ 	WARN_ON_FPU(err);
+ }
+ 
+ /*
+  * Restore processor xstate from xsave area.
+  *
+  * Uses XRSTORS when XSAVES is used, XRSTOR otherwise.
+  */
+ static inline void os_xrstor(struct xregs_state *xstate, u64 mask)
+ {
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 
+ 	XSTATE_XRESTORE(xstate, lmask, hmask);
+ }
+ 
+ /*
+  * Save xstate to user space xsave area.
+  *
+  * We don't use modified optimization because xrstor/xrstors might track
+  * a different application.
+  *
+  * We don't use compacted format xsave area for backward compatibility for
+  * old applications which don't understand the compacted format of the
+  * xsave area.
+  *
+  * The caller has to zero buf::header before calling this because XSAVE*
+  * does not touch the reserved fields in the header.
+  */
+ static inline int xsave_to_user_sigframe(struct xregs_state __user *buf)
+ {
+ 	/*
+ 	 * Include the features which are not xsaved/rstored by the kernel
+ 	 * internally, e.g. PKRU. That's user space ABI and also required
+ 	 * to allow the signal handler to modify PKRU.
+ 	 */
+ 	u64 mask = current->thread.fpu.fpstate->user_xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	stac();
+ 	XSTATE_OP(XSAVE, buf, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from user space xsave area.
+  */
+ static inline int xrstor_from_user_sigframe(struct xregs_state __user *buf, u64 mask)
+ {
+ 	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	stac();
+ 	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from kernel space xsave area, return an error code instead of
+  * an exception.
+  */
+ static inline int os_xrstor_safe(struct xregs_state *xstate, u64 mask)
+ {
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_XSAVES))
+ 		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
+ 	else
+ 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 
+ 	return err;
+ }
+ 
+ 
++>>>>>>> 3ac8d75778fc (x86/fpu: Use fpstate in __copy_xstate_to_uabi_buf())
  #endif
* Unmerged path arch/x86/kernel/fpu/core.c
* Unmerged path arch/x86/kernel/fpu/xstate.c
* Unmerged path arch/x86/kernel/fpu/xstate.h
