arm64: mm: Remove vabits_user

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Steve Capper <steve.capper@arm.com>
commit 2c624fe68715e76eba1a7089f91e122310dc663c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/2c624fe6.failed

Previous patches have enabled 52-bit kernel + user VAs and there is no
longer any scenario where user VA != kernel VA size.

This patch removes the, now redundant, vabits_user variable and replaces
usage with vabits_actual where appropriate.

	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Signed-off-by: Steve Capper <steve.capper@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 2c624fe68715e76eba1a7089f91e122310dc663c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/processor.h
#	arch/arm64/kernel/head.S
#	arch/arm64/mm/fault.c
#	arch/arm64/mm/proc.S
diff --cc arch/arm64/include/asm/processor.h
index 2d679c4f5884,e4c93945e477..000000000000
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@@ -54,11 -42,19 +54,16 @@@
   * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
   */
  
++<<<<<<< HEAD
 +#define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS)
 +#define TASK_SIZE_64		(UL(1) << vabits_user)
++=======
+ #define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS_MIN)
+ #define TASK_SIZE_64		(UL(1) << vabits_actual)
++>>>>>>> 2c624fe68715 (arm64: mm: Remove vabits_user)
  
  #ifdef CONFIG_COMPAT
 -#if defined(CONFIG_ARM64_64K_PAGES) && defined(CONFIG_KUSER_HELPERS)
 -/*
 - * With CONFIG_ARM64_64K_PAGES enabled, the last page is occupied
 - * by the compat vectors page.
 - */
  #define TASK_SIZE_32		UL(0x100000000)
 -#else
 -#define TASK_SIZE_32		(UL(0x100000000) - PAGE_SIZE)
 -#endif /* CONFIG_ARM64_64K_PAGES */
  #define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
  				TASK_SIZE_32 : TASK_SIZE_64)
  #define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \
diff --cc arch/arm64/kernel/head.S
index e9dba3e173cf,949b001a73bb..000000000000
--- a/arch/arm64/kernel/head.S
+++ b/arch/arm64/kernel/head.S
@@@ -325,9 -314,9 +325,13 @@@ __create_page_tables
  	mov	x5, #52
  	cbnz	x6, 1f
  #endif
 -	mov	x5, #VA_BITS_MIN
 +	mov	x5, #VA_BITS
  1:
++<<<<<<< HEAD
 +	adr_l	x6, vabits_user
++=======
+ 	adr_l	x6, vabits_actual
++>>>>>>> 2c624fe68715 (arm64: mm: Remove vabits_user)
  	str	x5, [x6]
  	dmb	sy
  	dc	ivac, x6		// Invalidate potentially stale cache line
@@@ -801,8 -789,8 +805,13 @@@ ENTRY(__enable_mmu
  ENDPROC(__enable_mmu)
  
  ENTRY(__cpu_secondary_check52bitva)
++<<<<<<< HEAD
 +#ifdef CONFIG_ARM64_USER_VA_BITS_52
 +	ldr_l	x0, vabits_user
++=======
+ #ifdef CONFIG_ARM64_VA_BITS_52
+ 	ldr_l	x0, vabits_actual
++>>>>>>> 2c624fe68715 (arm64: mm: Remove vabits_user)
  	cmp	x0, #52
  	b.ne	2f
  
diff --cc arch/arm64/mm/fault.c
index 9216265c0f74,75eff57bd9ef..000000000000
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@@ -171,10 -138,9 +171,14 @@@ void show_pte(unsigned long addr
  		return;
  	}
  
 -	pr_alert("%s pgtable: %luk pages, %llu-bit VAs, pgdp=%016lx\n",
 +	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp=%016lx\n",
  		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
++<<<<<<< HEAD
 +		 mm == &init_mm ? VA_BITS : (int)vabits_user,
 +		 (unsigned long)virt_to_phys(mm->pgd));
++=======
+ 		 vabits_actual, (unsigned long)virt_to_phys(mm->pgd));
++>>>>>>> 2c624fe68715 (arm64: mm: Remove vabits_user)
  	pgdp = pgd_offset(mm, addr);
  	pgd = READ_ONCE(*pgdp);
  	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));
diff --cc arch/arm64/mm/proc.S
index 2467ad5de151,391f9cabfe60..000000000000
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@@ -447,10 -438,11 +447,15 @@@ ENTRY(__cpu_setup
  			TCR_TBI0 | TCR_A1 | TCR_KASAN_FLAGS
  	tcr_clear_errata_bits x10, x9, x5
  
++<<<<<<< HEAD
 +#ifdef CONFIG_ARM64_USER_VA_BITS_52
 +	ldr_l		x9, vabits_user
++=======
+ #ifdef CONFIG_ARM64_VA_BITS_52
+ 	ldr_l		x9, vabits_actual
++>>>>>>> 2c624fe68715 (arm64: mm: Remove vabits_user)
  	sub		x9, xzr, x9
  	add		x9, x9, #64
 -	tcr_set_t1sz	x10, x9
  #else
  	ldr_l		x9, idmap_t0sz
  #endif
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 56c652c93ca8..4556aa44b564 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -211,9 +211,6 @@ static inline unsigned long kaslr_offset(void)
 	return kimage_vaddr - KIMAGE_VADDR;
 }
 
-/* the actual size of a user virtual address */
-extern u64			vabits_user;
-
 /*
  * Allow all memory at the discovery stage. We will clip it later.
  */
diff --git a/arch/arm64/include/asm/pointer_auth.h b/arch/arm64/include/asm/pointer_auth.h
index 15d49515efdd..3d0d2ac69b68 100644
--- a/arch/arm64/include/asm/pointer_auth.h
+++ b/arch/arm64/include/asm/pointer_auth.h
@@ -69,7 +69,7 @@ extern int ptrauth_prctl_reset_keys(struct task_struct *tsk, unsigned long arg);
  * The EL0 pointer bits used by a pointer authentication code.
  * This is dependent on TBI0 being enabled, or bits 63:56 would also apply.
  */
-#define ptrauth_user_pac_mask()	GENMASK(54, vabits_user)
+#define ptrauth_user_pac_mask()	GENMASK(54, vabits_actual)
 
 /* Only valid for EL0 TTBR0 instruction pointers */
 static inline unsigned long ptrauth_strip_insn_pac(unsigned long ptr)
* Unmerged path arch/arm64/include/asm/processor.h
* Unmerged path arch/arm64/kernel/head.S
* Unmerged path arch/arm64/mm/fault.c
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index 89b746333b14..2dfe0b40b620 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -53,8 +53,6 @@
 
 u64 idmap_t0sz = TCR_T0SZ(VA_BITS);
 u64 idmap_ptrs_per_pgd = PTRS_PER_PGD;
-u64 vabits_user __ro_after_init;
-EXPORT_SYMBOL(vabits_user);
 
 u64 kimage_voffset __ro_after_init;
 EXPORT_SYMBOL(kimage_voffset);
* Unmerged path arch/arm64/mm/proc.S
