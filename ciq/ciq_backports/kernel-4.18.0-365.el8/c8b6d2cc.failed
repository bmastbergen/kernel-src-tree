arm64: mm: Separate out vmemmap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Steve Capper <steve.capper@arm.com>
commit c8b6d2ccf9b10ce872cdea037f9685804440bb7e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/c8b6d2cc.failed

vmemmap is a preprocessor definition that depends on a variable,
memstart_addr. In a later patch we will need to expand the size of
the VMEMMAP region and optionally modify vmemmap depending upon
whether or not hardware support is available for 52-bit virtual
addresses.

This patch changes vmemmap to be a variable. As the old definition
depended on a variable load, this should not affect performance
noticeably.

	Signed-off-by: Steve Capper <steve.capper@arm.com>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit c8b6d2ccf9b10ce872cdea037f9685804440bb7e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/init.c
diff --cc arch/arm64/mm/init.c
index 49bc8730c5b8,2940221e5519..000000000000
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@@ -61,25 -50,13 +61,32 @@@
  s64 memstart_addr __ro_after_init = -1;
  EXPORT_SYMBOL(memstart_addr);
  
 -s64 physvirt_offset __ro_after_init;
 -EXPORT_SYMBOL(physvirt_offset);
 +phys_addr_t arm64_dma32_phys_limit __ro_after_init;
 +
++<<<<<<< HEAD
 +#ifdef CONFIG_BLK_DEV_INITRD
 +static int __init early_initrd(char *p)
 +{
 +	unsigned long start, size;
 +	char *endp;
 +
 +	start = memparse(p, &endp);
 +	if (*endp == ',') {
 +		size = memparse(endp + 1, NULL);
  
 +		initrd_start = start;
 +		initrd_end = start + size;
 +	}
 +	return 0;
 +}
 +early_param("initrd", early_initrd);
 +#endif
++=======
+ struct page *vmemmap __ro_after_init;
+ EXPORT_SYMBOL(vmemmap);
+ 
+ phys_addr_t arm64_dma_phys_limit __ro_after_init;
++>>>>>>> c8b6d2ccf9b1 (arm64: mm: Separate out vmemmap)
  
  #ifdef CONFIG_KEXEC_CORE
  /*
@@@ -343,6 -321,10 +350,13 @@@ void __init arm64_memblock_init(void
  	memstart_addr = round_down(memblock_start_of_DRAM(),
  				   ARM64_MEMSTART_ALIGN);
  
++<<<<<<< HEAD
++=======
+ 	physvirt_offset = PHYS_OFFSET - PAGE_OFFSET;
+ 
+ 	vmemmap = ((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT));
+ 
++>>>>>>> c8b6d2ccf9b1 (arm64: mm: Separate out vmemmap)
  	/*
  	 * Remove the memory that we will not be able to cover with the
  	 * linear mapping. Take care not to clip the kernel which may be
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d4a2e93735be..1d3738c8a967 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -34,8 +34,6 @@
 #define VMALLOC_START		(MODULES_END)
 #define VMALLOC_END		(- PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
-#define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
-
 #define FIRST_USER_ADDRESS	0UL
 
 #ifndef __ASSEMBLY__
@@ -46,6 +44,8 @@
 #include <linux/mm_types.h>
 #include <linux/sched.h>
 
+extern struct page *vmemmap;
+
 extern void __pte_error(const char *file, int line, unsigned long val);
 extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);
* Unmerged path arch/arm64/mm/init.c
