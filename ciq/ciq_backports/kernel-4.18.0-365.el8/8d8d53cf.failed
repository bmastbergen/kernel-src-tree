dma-mapping: Allow mixing bypass and mapped DMA operation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 8d8d53cf8fd028310b1189165b939cde124895d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/8d8d53cf.failed

At the moment we allow bypassing DMA ops only when we can do this for
the entire RAM. However there are configs with mixed type memory
where we could still allow bypassing IOMMU in most cases;
POWERPC with persistent memory is one example.

This adds an arch hook to determine where bypass can still work and
we invoke direct DMA API. The following patch checks the bus limit
on POWERPC to allow or disallow direct mapping.

This adds a ARCH_HAS_DMA_MAP_DIRECT config option to make the arch_xxxx
hooks no-op by default.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8d8d53cf8fd028310b1189165b939cde124895d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-map-ops.h
#	kernel/dma/Kconfig
diff --cc kernel/dma/Kconfig
index 35c60ac3ee93,43d106598e82..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -12,7 -20,8 +12,12 @@@ config HAS_DM
  config DMA_OPS_BYPASS
  	bool
  
++<<<<<<< HEAD
 +config DMA_OPS
++=======
+ # Lets platform IOMMU driver choose between bypass and IOMMU
+ config ARCH_HAS_DMA_MAP_DIRECT
++>>>>>>> 8d8d53cf8fd0 (dma-mapping: Allow mixing bypass and mapped DMA operation)
  	bool
  
  config NEED_SG_DMA_LENGTH
* Unmerged path include/linux/dma-map-ops.h
* Unmerged path include/linux/dma-map-ops.h
* Unmerged path kernel/dma/Kconfig
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index b152e4e4a653..5bebbaa553a5 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -159,7 +159,8 @@ dma_addr_t dma_map_page_attrs(struct device *dev, struct page *page,
 	if (WARN_ON_ONCE(!dev->dma_mask))
 		return DMA_MAPPING_ERROR;
 
-	if (dma_map_direct(dev, ops))
+	if (dma_map_direct(dev, ops) ||
+	    arch_dma_map_page_direct(dev, page_to_phys(page) + offset + size))
 		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
 	else
 		addr = ops->map_page(dev, page, offset, size, dir, attrs);
@@ -175,7 +176,8 @@ void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr, size_t size,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (dma_map_direct(dev, ops))
+	if (dma_map_direct(dev, ops) ||
+	    arch_dma_unmap_page_direct(dev, addr + size))
 		dma_direct_unmap_page(dev, addr, size, dir, attrs);
 	else if (ops->unmap_page)
 		ops->unmap_page(dev, addr, size, dir, attrs);
@@ -198,7 +200,8 @@ int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg, int nents,
 	if (WARN_ON_ONCE(!dev->dma_mask))
 		return 0;
 
-	if (dma_map_direct(dev, ops))
+	if (dma_map_direct(dev, ops) ||
+	    arch_dma_map_sg_direct(dev, sg, nents))
 		ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
 	else
 		ents = ops->map_sg(dev, sg, nents, dir, attrs);
@@ -217,7 +220,8 @@ void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_unmap_sg(dev, sg, nents, dir);
-	if (dma_map_direct(dev, ops))
+	if (dma_map_direct(dev, ops) ||
+	    arch_dma_unmap_sg_direct(dev, sg, nents))
 		dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
 	else if (ops->unmap_sg)
 		ops->unmap_sg(dev, sg, nents, dir, attrs);
