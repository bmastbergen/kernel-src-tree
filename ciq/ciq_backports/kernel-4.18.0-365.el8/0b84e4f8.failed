swiotlb: Add restricted DMA pool initialization

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Claire Chang <tientzu@chromium.org>
commit 0b84e4f8b793eb4045fd64f6f514165a7974cd16
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/0b84e4f8.failed

Add the initialization function to create restricted DMA pools from
matching reserved-memory nodes.

Regardless of swiotlb setting, the restricted DMA pool is preferred if
available.

The restricted DMA pools provide a basic level of protection against the
DMA overwriting buffer contents at unexpected times. However, to protect
against general data leakage and system memory corruption, the system
needs to provide a way to lock down the memory access, e.g., MPU.

	Signed-off-by: Claire Chang <tientzu@chromium.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Stefano Stabellini <sstabellini@kernel.org>
	Tested-by: Will Deacon <will@kernel.org>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 0b84e4f8b793eb4045fd64f6f514165a7974cd16)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swiotlb.h
#	kernel/dma/swiotlb.c
diff --cc include/linux/swiotlb.h
index 5857a937c637,39284ff2a6cd..000000000000
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@@ -71,11 -62,63 +71,59 @@@ dma_addr_t swiotlb_map(struct device *d
  
  #ifdef CONFIG_SWIOTLB
  extern enum swiotlb_force swiotlb_force;
 +extern phys_addr_t io_tlb_start, io_tlb_end;
  
++<<<<<<< HEAD
 +static inline bool is_swiotlb_buffer(phys_addr_t paddr)
++=======
+ /**
+  * struct io_tlb_mem - IO TLB Memory Pool Descriptor
+  *
+  * @start:	The start address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @end:	The end address of the swiotlb memory pool. Used to do a quick
+  *		range check to see if the memory was in fact allocated by this
+  *		API.
+  * @nslabs:	The number of IO TLB blocks (in groups of 64) between @start and
+  *		@end. For default swiotlb, this is command line adjustable via
+  *		setup_io_tlb_npages.
+  * @used:	The number of used IO TLB block.
+  * @list:	The free list describing the number of free entries available
+  *		from each index.
+  * @index:	The index to start searching in the next round.
+  * @orig_addr:	The original address corresponding to a mapped entry.
+  * @alloc_size:	Size of the allocated buffer.
+  * @lock:	The lock to protect the above data structures in the map and
+  *		unmap calls.
+  * @debugfs:	The dentry to debugfs.
+  * @late_alloc:	%true if allocated using the page allocator
+  * @force_bounce: %true if swiotlb bouncing is forced
+  * @for_alloc:  %true if the pool is used for memory allocation
+  */
+ struct io_tlb_mem {
+ 	phys_addr_t start;
+ 	phys_addr_t end;
+ 	unsigned long nslabs;
+ 	unsigned long used;
+ 	unsigned int index;
+ 	spinlock_t lock;
+ 	struct dentry *debugfs;
+ 	bool late_alloc;
+ 	bool force_bounce;
+ 	bool for_alloc;
+ 	struct io_tlb_slot {
+ 		phys_addr_t orig_addr;
+ 		size_t alloc_size;
+ 		unsigned int list;
+ 	} slots[];
+ };
+ extern struct io_tlb_mem *io_tlb_default_mem;
+ 
+ static inline bool is_swiotlb_buffer(struct device *dev, phys_addr_t paddr)
++>>>>>>> 0b84e4f8b793 (swiotlb: Add restricted DMA pool initialization)
  {
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && paddr >= mem->start && paddr < mem->end;
 -}
 -
 -static inline bool is_swiotlb_force_bounce(struct device *dev)
 -{
 -	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 -
 -	return mem && mem->force_bounce;
 +	return paddr >= io_tlb_start && paddr < io_tlb_end;
  }
  
  void __init swiotlb_exit(void);
diff --cc kernel/dma/swiotlb.c
index be0e4e04c6fd,0ffbaae9fba2..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -817,6 -707,108 +824,111 @@@ static int __init swiotlb_create_debugf
  	return 0;
  }
  
 -late_initcall(swiotlb_create_default_debugfs);
 +late_initcall(swiotlb_create_debugfs);
  
  #endif
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_DMA_RESTRICTED_POOL
+ struct page *swiotlb_alloc(struct device *dev, size_t size)
+ {
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 	phys_addr_t tlb_addr;
+ 	int index;
+ 
+ 	if (!mem)
+ 		return NULL;
+ 
+ 	index = swiotlb_find_slots(dev, 0, size);
+ 	if (index == -1)
+ 		return NULL;
+ 
+ 	tlb_addr = slot_addr(mem->start, index);
+ 
+ 	return pfn_to_page(PFN_DOWN(tlb_addr));
+ }
+ 
+ bool swiotlb_free(struct device *dev, struct page *page, size_t size)
+ {
+ 	phys_addr_t tlb_addr = page_to_phys(page);
+ 
+ 	if (!is_swiotlb_buffer(dev, tlb_addr))
+ 		return false;
+ 
+ 	swiotlb_release_slots(dev, tlb_addr);
+ 
+ 	return true;
+ }
+ 
+ static int rmem_swiotlb_device_init(struct reserved_mem *rmem,
+ 				    struct device *dev)
+ {
+ 	struct io_tlb_mem *mem = rmem->priv;
+ 	unsigned long nslabs = rmem->size >> IO_TLB_SHIFT;
+ 
+ 	/*
+ 	 * Since multiple devices can share the same pool, the private data,
+ 	 * io_tlb_mem struct, will be initialized by the first device attached
+ 	 * to it.
+ 	 */
+ 	if (!mem) {
+ 		mem = kzalloc(struct_size(mem, slots, nslabs), GFP_KERNEL);
+ 		if (!mem)
+ 			return -ENOMEM;
+ 
+ 		set_memory_decrypted((unsigned long)phys_to_virt(rmem->base),
+ 				     rmem->size >> PAGE_SHIFT);
+ 		swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ 		mem->force_bounce = true;
+ 		mem->for_alloc = true;
+ 
+ 		rmem->priv = mem;
+ 
+ 		if (IS_ENABLED(CONFIG_DEBUG_FS)) {
+ 			mem->debugfs =
+ 				debugfs_create_dir(rmem->name, debugfs_dir);
+ 			swiotlb_create_debugfs_files(mem);
+ 		}
+ 	}
+ 
+ 	dev->dma_io_tlb_mem = mem;
+ 
+ 	return 0;
+ }
+ 
+ static void rmem_swiotlb_device_release(struct reserved_mem *rmem,
+ 					struct device *dev)
+ {
+ 	dev->dma_io_tlb_mem = io_tlb_default_mem;
+ }
+ 
+ static const struct reserved_mem_ops rmem_swiotlb_ops = {
+ 	.device_init = rmem_swiotlb_device_init,
+ 	.device_release = rmem_swiotlb_device_release,
+ };
+ 
+ static int __init rmem_swiotlb_setup(struct reserved_mem *rmem)
+ {
+ 	unsigned long node = rmem->fdt_node;
+ 
+ 	if (of_get_flat_dt_prop(node, "reusable", NULL) ||
+ 	    of_get_flat_dt_prop(node, "linux,cma-default", NULL) ||
+ 	    of_get_flat_dt_prop(node, "linux,dma-default", NULL) ||
+ 	    of_get_flat_dt_prop(node, "no-map", NULL))
+ 		return -EINVAL;
+ 
+ 	if (PageHighMem(pfn_to_page(PHYS_PFN(rmem->base)))) {
+ 		pr_err("Restricted DMA pool must be accessible within the linear mapping.");
+ 		return -EINVAL;
+ 	}
+ 
+ 	rmem->ops = &rmem_swiotlb_ops;
+ 	pr_info("Reserved memory: created restricted DMA pool at %pa, size %ld MiB\n",
+ 		&rmem->base, (unsigned long)rmem->size / SZ_1M);
+ 	return 0;
+ }
+ 
+ RESERVEDMEM_OF_DECLARE(dma, "restricted-dma-pool", rmem_swiotlb_setup);
+ #endif /* CONFIG_DMA_RESTRICTED_POOL */
++>>>>>>> 0b84e4f8b793 (swiotlb: Add restricted DMA pool initialization)
* Unmerged path include/linux/swiotlb.h
diff --git a/kernel/dma/Kconfig b/kernel/dma/Kconfig
index 35c60ac3ee93..c25132a15b38 100644
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@ -73,6 +73,20 @@ config SWIOTLB
 	bool
 	select NEED_DMA_MAP_STATE
 
+config DMA_RESTRICTED_POOL
+	bool "DMA Restricted Pool"
+	depends on OF && OF_RESERVED_MEM
+	select SWIOTLB
+	help
+	  This enables support for restricted DMA pools which provide a level of
+	  DMA memory protection on systems with limited hardware protection
+	  capabilities, such as those lacking an IOMMU.
+
+	  For more information see
+	  <Documentation/devicetree/bindings/reserved-memory/reserved-memory.txt>
+	  and <kernel/dma/swiotlb.c>.
+	  If unsure, say "n".
+
 #
 # Should be selected if we can mmap non-coherent mappings to userspace.
 # The only thing that is really required is a way to set an uncached bit
* Unmerged path kernel/dma/swiotlb.c
