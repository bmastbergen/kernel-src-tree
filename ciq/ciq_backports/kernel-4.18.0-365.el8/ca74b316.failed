arm: Use common cpu_topology structure and functions.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Atish Patra <atish.patra@wdc.com>
commit ca74b316df96d7c40ee3e8301065607c11c60c27
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/ca74b316.failed

Currently, ARM32 and ARM64 uses different data structures to represent
their cpu topologies. Since, we are moving the ARM64 topology to common
code to be used by other architectures, we can reuse that for ARM32 as
well.

Take this opprtunity to remove the redundant functions from ARM32 and
reuse the common code instead.

To: Russell King <linux@armlinux.org.uk>
	Signed-off-by: Atish Patra <atish.patra@wdc.com>
	Tested-by: Sudeep Holla <sudeep.holla@arm.com> (on TC2)
	Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
	Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>
(cherry picked from commit ca74b316df96d7c40ee3e8301065607c11c60c27)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/base/arch_topology.c
#	include/linux/arch_topology.h
diff --cc drivers/base/arch_topology.c
index 347cddcd2879,b54d241a2ff5..000000000000
--- a/drivers/base/arch_topology.c
+++ b/drivers/base/arch_topology.c
@@@ -244,3 -246,296 +244,299 @@@ static void parsing_done_workfn(struct 
  #else
  core_initcall(free_raw_capacity);
  #endif
++<<<<<<< HEAD
++=======
+ 
+ #if defined(CONFIG_ARM64) || defined(CONFIG_RISCV)
+ static int __init get_cpu_for_node(struct device_node *node)
+ {
+ 	struct device_node *cpu_node;
+ 	int cpu;
+ 
+ 	cpu_node = of_parse_phandle(node, "cpu", 0);
+ 	if (!cpu_node)
+ 		return -1;
+ 
+ 	cpu = of_cpu_node_to_id(cpu_node);
+ 	if (cpu >= 0)
+ 		topology_parse_cpu_capacity(cpu_node, cpu);
+ 	else
+ 		pr_crit("Unable to find CPU node for %pOF\n", cpu_node);
+ 
+ 	of_node_put(cpu_node);
+ 	return cpu;
+ }
+ 
+ static int __init parse_core(struct device_node *core, int package_id,
+ 			     int core_id)
+ {
+ 	char name[10];
+ 	bool leaf = true;
+ 	int i = 0;
+ 	int cpu;
+ 	struct device_node *t;
+ 
+ 	do {
+ 		snprintf(name, sizeof(name), "thread%d", i);
+ 		t = of_get_child_by_name(core, name);
+ 		if (t) {
+ 			leaf = false;
+ 			cpu = get_cpu_for_node(t);
+ 			if (cpu >= 0) {
+ 				cpu_topology[cpu].package_id = package_id;
+ 				cpu_topology[cpu].core_id = core_id;
+ 				cpu_topology[cpu].thread_id = i;
+ 			} else {
+ 				pr_err("%pOF: Can't get CPU for thread\n",
+ 				       t);
+ 				of_node_put(t);
+ 				return -EINVAL;
+ 			}
+ 			of_node_put(t);
+ 		}
+ 		i++;
+ 	} while (t);
+ 
+ 	cpu = get_cpu_for_node(core);
+ 	if (cpu >= 0) {
+ 		if (!leaf) {
+ 			pr_err("%pOF: Core has both threads and CPU\n",
+ 			       core);
+ 			return -EINVAL;
+ 		}
+ 
+ 		cpu_topology[cpu].package_id = package_id;
+ 		cpu_topology[cpu].core_id = core_id;
+ 	} else if (leaf) {
+ 		pr_err("%pOF: Can't get CPU for leaf core\n", core);
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int __init parse_cluster(struct device_node *cluster, int depth)
+ {
+ 	char name[10];
+ 	bool leaf = true;
+ 	bool has_cores = false;
+ 	struct device_node *c;
+ 	static int package_id __initdata;
+ 	int core_id = 0;
+ 	int i, ret;
+ 
+ 	/*
+ 	 * First check for child clusters; we currently ignore any
+ 	 * information about the nesting of clusters and present the
+ 	 * scheduler with a flat list of them.
+ 	 */
+ 	i = 0;
+ 	do {
+ 		snprintf(name, sizeof(name), "cluster%d", i);
+ 		c = of_get_child_by_name(cluster, name);
+ 		if (c) {
+ 			leaf = false;
+ 			ret = parse_cluster(c, depth + 1);
+ 			of_node_put(c);
+ 			if (ret != 0)
+ 				return ret;
+ 		}
+ 		i++;
+ 	} while (c);
+ 
+ 	/* Now check for cores */
+ 	i = 0;
+ 	do {
+ 		snprintf(name, sizeof(name), "core%d", i);
+ 		c = of_get_child_by_name(cluster, name);
+ 		if (c) {
+ 			has_cores = true;
+ 
+ 			if (depth == 0) {
+ 				pr_err("%pOF: cpu-map children should be clusters\n",
+ 				       c);
+ 				of_node_put(c);
+ 				return -EINVAL;
+ 			}
+ 
+ 			if (leaf) {
+ 				ret = parse_core(c, package_id, core_id++);
+ 			} else {
+ 				pr_err("%pOF: Non-leaf cluster with core %s\n",
+ 				       cluster, name);
+ 				ret = -EINVAL;
+ 			}
+ 
+ 			of_node_put(c);
+ 			if (ret != 0)
+ 				return ret;
+ 		}
+ 		i++;
+ 	} while (c);
+ 
+ 	if (leaf && !has_cores)
+ 		pr_warn("%pOF: empty cluster\n", cluster);
+ 
+ 	if (leaf)
+ 		package_id++;
+ 
+ 	return 0;
+ }
+ 
+ static int __init parse_dt_topology(void)
+ {
+ 	struct device_node *cn, *map;
+ 	int ret = 0;
+ 	int cpu;
+ 
+ 	cn = of_find_node_by_path("/cpus");
+ 	if (!cn) {
+ 		pr_err("No CPU information found in DT\n");
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * When topology is provided cpu-map is essentially a root
+ 	 * cluster with restricted subnodes.
+ 	 */
+ 	map = of_get_child_by_name(cn, "cpu-map");
+ 	if (!map)
+ 		goto out;
+ 
+ 	ret = parse_cluster(map, 0);
+ 	if (ret != 0)
+ 		goto out_map;
+ 
+ 	topology_normalize_cpu_scale();
+ 
+ 	/*
+ 	 * Check that all cores are in the topology; the SMP code will
+ 	 * only mark cores described in the DT as possible.
+ 	 */
+ 	for_each_possible_cpu(cpu)
+ 		if (cpu_topology[cpu].package_id == -1)
+ 			ret = -EINVAL;
+ 
+ out_map:
+ 	of_node_put(map);
+ out:
+ 	of_node_put(cn);
+ 	return ret;
+ }
+ #endif
+ 
+ /*
+  * cpu topology table
+  */
+ struct cpu_topology cpu_topology[NR_CPUS];
+ EXPORT_SYMBOL_GPL(cpu_topology);
+ 
+ const struct cpumask *cpu_coregroup_mask(int cpu)
+ {
+ 	const cpumask_t *core_mask = cpumask_of_node(cpu_to_node(cpu));
+ 
+ 	/* Find the smaller of NUMA, core or LLC siblings */
+ 	if (cpumask_subset(&cpu_topology[cpu].core_sibling, core_mask)) {
+ 		/* not numa in package, lets use the package siblings */
+ 		core_mask = &cpu_topology[cpu].core_sibling;
+ 	}
+ 	if (cpu_topology[cpu].llc_id != -1) {
+ 		if (cpumask_subset(&cpu_topology[cpu].llc_sibling, core_mask))
+ 			core_mask = &cpu_topology[cpu].llc_sibling;
+ 	}
+ 
+ 	return core_mask;
+ }
+ 
+ void update_siblings_masks(unsigned int cpuid)
+ {
+ 	struct cpu_topology *cpu_topo, *cpuid_topo = &cpu_topology[cpuid];
+ 	int cpu;
+ 
+ 	/* update core and thread sibling masks */
+ 	for_each_online_cpu(cpu) {
+ 		cpu_topo = &cpu_topology[cpu];
+ 
+ 		if (cpuid_topo->llc_id == cpu_topo->llc_id) {
+ 			cpumask_set_cpu(cpu, &cpuid_topo->llc_sibling);
+ 			cpumask_set_cpu(cpuid, &cpu_topo->llc_sibling);
+ 		}
+ 
+ 		if (cpuid_topo->package_id != cpu_topo->package_id)
+ 			continue;
+ 
+ 		cpumask_set_cpu(cpuid, &cpu_topo->core_sibling);
+ 		cpumask_set_cpu(cpu, &cpuid_topo->core_sibling);
+ 
+ 		if (cpuid_topo->core_id != cpu_topo->core_id)
+ 			continue;
+ 
+ 		cpumask_set_cpu(cpuid, &cpu_topo->thread_sibling);
+ 		cpumask_set_cpu(cpu, &cpuid_topo->thread_sibling);
+ 	}
+ }
+ 
+ static void clear_cpu_topology(int cpu)
+ {
+ 	struct cpu_topology *cpu_topo = &cpu_topology[cpu];
+ 
+ 	cpumask_clear(&cpu_topo->llc_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->llc_sibling);
+ 
+ 	cpumask_clear(&cpu_topo->core_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->core_sibling);
+ 	cpumask_clear(&cpu_topo->thread_sibling);
+ 	cpumask_set_cpu(cpu, &cpu_topo->thread_sibling);
+ }
+ 
+ void __init reset_cpu_topology(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct cpu_topology *cpu_topo = &cpu_topology[cpu];
+ 
+ 		cpu_topo->thread_id = -1;
+ 		cpu_topo->core_id = -1;
+ 		cpu_topo->package_id = -1;
+ 		cpu_topo->llc_id = -1;
+ 
+ 		clear_cpu_topology(cpu);
+ 	}
+ }
+ 
+ void remove_cpu_topology(unsigned int cpu)
+ {
+ 	int sibling;
+ 
+ 	for_each_cpu(sibling, topology_core_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_core_cpumask(sibling));
+ 	for_each_cpu(sibling, topology_sibling_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));
+ 	for_each_cpu(sibling, topology_llc_cpumask(cpu))
+ 		cpumask_clear_cpu(cpu, topology_llc_cpumask(sibling));
+ 
+ 	clear_cpu_topology(cpu);
+ }
+ 
+ __weak int __init parse_acpi_topology(void)
+ {
+ 	return 0;
+ }
+ 
+ #if defined(CONFIG_ARM64) || defined(CONFIG_RISCV)
+ void __init init_cpu_topology(void)
+ {
+ 	reset_cpu_topology();
+ 
+ 	/*
+ 	 * Discard anything that was parsed if we hit an error so we
+ 	 * don't use partial information.
+ 	 */
+ 	if (parse_acpi_topology())
+ 		reset_cpu_topology();
+ 	else if (of_have_populated_dt() && parse_dt_topology())
+ 		reset_cpu_topology();
+ }
+ #endif
++>>>>>>> ca74b316df96 (arm: Use common cpu_topology structure and functions.)
diff --cc include/linux/arch_topology.h
index 1cfe05ea1d89,42f2b5126094..000000000000
--- a/include/linux/arch_topology.h
+++ b/include/linux/arch_topology.h
@@@ -33,4 -33,30 +33,33 @@@ unsigned long topology_get_freq_scale(i
  	return per_cpu(freq_scale, cpu);
  }
  
++<<<<<<< HEAD
++=======
+ struct cpu_topology {
+ 	int thread_id;
+ 	int core_id;
+ 	int package_id;
+ 	int llc_id;
+ 	cpumask_t thread_sibling;
+ 	cpumask_t core_sibling;
+ 	cpumask_t llc_sibling;
+ };
+ 
+ #ifdef CONFIG_GENERIC_ARCH_TOPOLOGY
+ extern struct cpu_topology cpu_topology[NR_CPUS];
+ 
+ #define topology_physical_package_id(cpu)	(cpu_topology[cpu].package_id)
+ #define topology_core_id(cpu)		(cpu_topology[cpu].core_id)
+ #define topology_core_cpumask(cpu)	(&cpu_topology[cpu].core_sibling)
+ #define topology_sibling_cpumask(cpu)	(&cpu_topology[cpu].thread_sibling)
+ #define topology_llc_cpumask(cpu)	(&cpu_topology[cpu].llc_sibling)
+ void init_cpu_topology(void);
+ void store_cpu_topology(unsigned int cpuid);
+ const struct cpumask *cpu_coregroup_mask(int cpu);
+ void update_siblings_masks(unsigned int cpu);
+ void remove_cpu_topology(unsigned int cpuid);
+ void reset_cpu_topology(void);
+ #endif
+ 
++>>>>>>> ca74b316df96 (arm: Use common cpu_topology structure and functions.)
  #endif /* _LINUX_ARCH_TOPOLOGY_H_ */
diff --git a/arch/arm/include/asm/topology.h b/arch/arm/include/asm/topology.h
index 2a786f54d8b8..8a0fae94d45e 100644
--- a/arch/arm/include/asm/topology.h
+++ b/arch/arm/include/asm/topology.h
@@ -5,26 +5,6 @@
 #ifdef CONFIG_ARM_CPU_TOPOLOGY
 
 #include <linux/cpumask.h>
-
-struct cputopo_arm {
-	int thread_id;
-	int core_id;
-	int socket_id;
-	cpumask_t thread_sibling;
-	cpumask_t core_sibling;
-};
-
-extern struct cputopo_arm cpu_topology[NR_CPUS];
-
-#define topology_physical_package_id(cpu)	(cpu_topology[cpu].socket_id)
-#define topology_core_id(cpu)		(cpu_topology[cpu].core_id)
-#define topology_core_cpumask(cpu)	(&cpu_topology[cpu].core_sibling)
-#define topology_sibling_cpumask(cpu)	(&cpu_topology[cpu].thread_sibling)
-
-void init_cpu_topology(void);
-void store_cpu_topology(unsigned int cpuid);
-const struct cpumask *cpu_coregroup_mask(int cpu);
-
 #include <linux/arch_topology.h>
 
 /* Replace task scheduler's default frequency-invariant accounting */
diff --git a/arch/arm/kernel/topology.c b/arch/arm/kernel/topology.c
index d3d75c58832c..09a33a69fb83 100644
--- a/arch/arm/kernel/topology.c
+++ b/arch/arm/kernel/topology.c
@@ -183,17 +183,6 @@ static inline void parse_dt_topology(void) {}
 static inline void update_cpu_capacity(unsigned int cpuid) {}
 #endif
 
- /*
- * cpu topology table
- */
-struct cputopo_arm cpu_topology[NR_CPUS];
-EXPORT_SYMBOL_GPL(cpu_topology);
-
-const struct cpumask *cpu_coregroup_mask(int cpu)
-{
-	return &cpu_topology[cpu].core_sibling;
-}
-
 /*
  * The current assumption is that we can power gate each core independently.
  * This will be superseded by DT binding once available.
@@ -203,32 +192,6 @@ const struct cpumask *cpu_corepower_mask(int cpu)
 	return &cpu_topology[cpu].thread_sibling;
 }
 
-static void update_siblings_masks(unsigned int cpuid)
-{
-	struct cputopo_arm *cpu_topo, *cpuid_topo = &cpu_topology[cpuid];
-	int cpu;
-
-	/* update core and thread sibling masks */
-	for_each_possible_cpu(cpu) {
-		cpu_topo = &cpu_topology[cpu];
-
-		if (cpuid_topo->socket_id != cpu_topo->socket_id)
-			continue;
-
-		cpumask_set_cpu(cpuid, &cpu_topo->core_sibling);
-		if (cpu != cpuid)
-			cpumask_set_cpu(cpu, &cpuid_topo->core_sibling);
-
-		if (cpuid_topo->core_id != cpu_topo->core_id)
-			continue;
-
-		cpumask_set_cpu(cpuid, &cpu_topo->thread_sibling);
-		if (cpu != cpuid)
-			cpumask_set_cpu(cpu, &cpuid_topo->thread_sibling);
-	}
-	smp_wmb();
-}
-
 /*
  * store_cpu_topology is called at boot when only one cpu is running
  * and with the mutex cpu_hotplug.lock locked, when several cpus have booted,
@@ -236,7 +199,7 @@ static void update_siblings_masks(unsigned int cpuid)
  */
 void store_cpu_topology(unsigned int cpuid)
 {
-	struct cputopo_arm *cpuid_topo = &cpu_topology[cpuid];
+	struct cpu_topology *cpuid_topo = &cpu_topology[cpuid];
 	unsigned int mpidr;
 
 	/* If the cpu topology has been already set, just return */
@@ -256,12 +219,12 @@ void store_cpu_topology(unsigned int cpuid)
 			/* core performance interdependency */
 			cpuid_topo->thread_id = MPIDR_AFFINITY_LEVEL(mpidr, 0);
 			cpuid_topo->core_id = MPIDR_AFFINITY_LEVEL(mpidr, 1);
-			cpuid_topo->socket_id = MPIDR_AFFINITY_LEVEL(mpidr, 2);
+			cpuid_topo->package_id = MPIDR_AFFINITY_LEVEL(mpidr, 2);
 		} else {
 			/* largely independent cores */
 			cpuid_topo->thread_id = -1;
 			cpuid_topo->core_id = MPIDR_AFFINITY_LEVEL(mpidr, 0);
-			cpuid_topo->socket_id = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+			cpuid_topo->package_id = MPIDR_AFFINITY_LEVEL(mpidr, 1);
 		}
 	} else {
 		/*
@@ -271,7 +234,7 @@ void store_cpu_topology(unsigned int cpuid)
 		 */
 		cpuid_topo->thread_id = -1;
 		cpuid_topo->core_id = 0;
-		cpuid_topo->socket_id = -1;
+		cpuid_topo->package_id = -1;
 	}
 
 	update_siblings_masks(cpuid);
@@ -281,7 +244,7 @@ void store_cpu_topology(unsigned int cpuid)
 	pr_info("CPU%u: thread %d, cpu %d, socket %d, mpidr %x\n",
 		cpuid, cpu_topology[cpuid].thread_id,
 		cpu_topology[cpuid].core_id,
-		cpu_topology[cpuid].socket_id, mpidr);
+		cpu_topology[cpuid].package_id, mpidr);
 }
 
 static inline int cpu_corepower_flags(void)
@@ -304,18 +267,7 @@ static struct sched_domain_topology_level arm_topology[] = {
  */
 void __init init_cpu_topology(void)
 {
-	unsigned int cpu;
-
-	/* init core mask and capacity */
-	for_each_possible_cpu(cpu) {
-		struct cputopo_arm *cpu_topo = &(cpu_topology[cpu]);
-
-		cpu_topo->thread_id = -1;
-		cpu_topo->core_id =  -1;
-		cpu_topo->socket_id = -1;
-		cpumask_clear(&cpu_topo->core_sibling);
-		cpumask_clear(&cpu_topo->thread_sibling);
-	}
+	reset_cpu_topology();
 	smp_wmb();
 
 	parse_dt_topology();
* Unmerged path drivers/base/arch_topology.c
* Unmerged path include/linux/arch_topology.h
