igc: Add support for XDP_REDIRECT action

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andre Guedes <andre.guedes@intel.com>
commit 4ff3203610928cac82d5627ce803559e78d61b91
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/4ff32036.failed

Add support for the XDP_REDIRECT action which enables XDP programs to
redirect packets arriving at I225 NIC. It also implements the
ndo_xdp_xmit ops, enabling the igc driver to transmit packets forwarded
to it by xdp programs running on other interfaces.

The patch tweaks the driver's page counting and recycling scheme as
described in the following two commits and implemented by other Intel
drivers in order to properly support XDP_REDIRECT action:
  commit 8ce29c679a6e ("i40e: tweak page counting for XDP_REDIRECT")
  commit 75aab4e10ae6 ("i40e: avoid premature Rx buffer reuse")

This patch has been tested with the sample apps "xdp_redirect_cpu" and
"xdp_redirect_map" located in samples/bpf/.

	Signed-off-by: Andre Guedes <andre.guedes@intel.com>
	Signed-off-by: Vedang Patel <vedang.patel@intel.com>
	Signed-off-by: Jithu Joseph <jithu.joseph@intel.com>
	Reviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: Dvora Fuxbrumer <dvorax.fuxbrumer@linux.intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 4ff3203610928cac82d5627ce803559e78d61b91)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/igc/igc_main.c
diff --cc drivers/net/ethernet/intel/igc/igc_main.c
index 261f28037c1a,10765491e357..000000000000
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@@ -22,6 -23,11 +22,14 @@@
  
  #define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)
  
++<<<<<<< HEAD
++=======
+ #define IGC_XDP_PASS		0
+ #define IGC_XDP_CONSUMED	BIT(0)
+ #define IGC_XDP_TX		BIT(1)
+ #define IGC_XDP_REDIRECT	BIT(2)
+ 
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  static int debug = -1;
  
  MODULE_AUTHOR("Intel Corporation, <linux.nics@intel.com>");
@@@ -1885,12 -1930,187 +1903,192 @@@ static void igc_alloc_rx_buffers(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int igc_xdp_init_tx_buffer(struct igc_tx_buffer *buffer,
+ 				  struct xdp_frame *xdpf,
+ 				  struct igc_ring *ring)
+ {
+ 	dma_addr_t dma;
+ 
+ 	dma = dma_map_single(ring->dev, xdpf->data, xdpf->len, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ring->dev, dma)) {
+ 		netdev_err_once(ring->netdev, "Failed to map DMA for TX\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	buffer->xdpf = xdpf;
+ 	buffer->tx_flags = IGC_TX_FLAGS_XDP;
+ 	buffer->protocol = 0;
+ 	buffer->bytecount = xdpf->len;
+ 	buffer->gso_segs = 1;
+ 	buffer->time_stamp = jiffies;
+ 	dma_unmap_len_set(buffer, len, xdpf->len);
+ 	dma_unmap_addr_set(buffer, dma, dma);
+ 	return 0;
+ }
+ 
+ /* This function requires __netif_tx_lock is held by the caller. */
+ static int igc_xdp_init_tx_descriptor(struct igc_ring *ring,
+ 				      struct xdp_frame *xdpf)
+ {
+ 	struct igc_tx_buffer *buffer;
+ 	union igc_adv_tx_desc *desc;
+ 	u32 cmd_type, olinfo_status;
+ 	int err;
+ 
+ 	if (!igc_desc_unused(ring))
+ 		return -EBUSY;
+ 
+ 	buffer = &ring->tx_buffer_info[ring->next_to_use];
+ 	err = igc_xdp_init_tx_buffer(buffer, xdpf, ring);
+ 	if (err)
+ 		return err;
+ 
+ 	cmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |
+ 		   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |
+ 		   buffer->bytecount;
+ 	olinfo_status = buffer->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;
+ 
+ 	desc = IGC_TX_DESC(ring, ring->next_to_use);
+ 	desc->read.cmd_type_len = cpu_to_le32(cmd_type);
+ 	desc->read.olinfo_status = cpu_to_le32(olinfo_status);
+ 	desc->read.buffer_addr = cpu_to_le64(dma_unmap_addr(buffer, dma));
+ 
+ 	netdev_tx_sent_queue(txring_txq(ring), buffer->bytecount);
+ 
+ 	buffer->next_to_watch = desc;
+ 
+ 	ring->next_to_use++;
+ 	if (ring->next_to_use == ring->count)
+ 		ring->next_to_use = 0;
+ 
+ 	return 0;
+ }
+ 
+ static struct igc_ring *igc_xdp_get_tx_ring(struct igc_adapter *adapter,
+ 					    int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= adapter->num_tx_queues)
+ 		index -= adapter->num_tx_queues;
+ 
+ 	return adapter->tx_ring[index];
+ }
+ 
+ static int igc_xdp_xmit_back(struct igc_adapter *adapter, struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return -EFAULT;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	res = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 	__netif_tx_unlock(nq);
+ 	return res;
+ }
+ 
+ static struct sk_buff *igc_xdp_run_prog(struct igc_adapter *adapter,
+ 					struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 	u32 act;
+ 
+ 	rcu_read_lock();
+ 
+ 	prog = READ_ONCE(adapter->xdp_prog);
+ 	if (!prog) {
+ 		res = IGC_XDP_PASS;
+ 		goto unlock;
+ 	}
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		res = IGC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		if (igc_xdp_xmit_back(adapter, xdp) < 0)
+ 			res = IGC_XDP_CONSUMED;
+ 		else
+ 			res = IGC_XDP_TX;
+ 		break;
+ 	case XDP_REDIRECT:
+ 		if (xdp_do_redirect(adapter->netdev, xdp, prog) < 0)
+ 			res = IGC_XDP_CONSUMED;
+ 		else
+ 			res = IGC_XDP_REDIRECT;
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(adapter->netdev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		res = IGC_XDP_CONSUMED;
+ 		break;
+ 	}
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 	return ERR_PTR(-res);
+ }
+ 
+ /* This function assumes __netif_tx_lock is held by the caller. */
+ static void igc_flush_tx_descriptors(struct igc_ring *ring)
+ {
+ 	/* Once tail pointer is updated, hardware can fetch the descriptors
+ 	 * any time so we issue a write membar here to ensure all memory
+ 	 * writes are complete before the tail pointer is updated.
+ 	 */
+ 	wmb();
+ 	writel(ring->next_to_use, ring->tail);
+ }
+ 
+ static void igc_finalize_xdp(struct igc_adapter *adapter, int status)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 
+ 	if (status & IGC_XDP_TX) {
+ 		ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 		nq = txring_txq(ring);
+ 
+ 		__netif_tx_lock(nq, cpu);
+ 		igc_flush_tx_descriptors(ring);
+ 		__netif_tx_unlock(nq);
+ 	}
+ 
+ 	if (status & IGC_XDP_REDIRECT)
+ 		xdp_do_flush();
+ }
+ 
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  static int igc_clean_rx_irq(struct igc_q_vector *q_vector, const int budget)
  {
  	unsigned int total_bytes = 0, total_packets = 0;
  	struct igc_ring *rx_ring = q_vector->rx.ring;
  	struct sk_buff *skb = rx_ring->skb;
  	u16 cleaned_count = igc_desc_unused(rx_ring);
++<<<<<<< HEAD
++=======
+ 	int xdp_status = 0, rx_buffer_pgcnt;
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  
  	while (likely(total_packets < budget)) {
  		union igc_adv_rx_desc *rx_desc;
@@@ -1916,20 -2138,46 +2114,58 @@@
  		 */
  		dma_rmb();
  
++<<<<<<< HEAD
 +		rx_buffer = igc_get_rx_buffer(rx_ring, size);
++=======
+ 		rx_buffer = igc_get_rx_buffer(rx_ring, size, &rx_buffer_pgcnt);
+ 		truesize = igc_get_rx_frame_truesize(rx_ring, size);
+ 
+ 		pktbuf = page_address(rx_buffer->page) + rx_buffer->page_offset;
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  
  		if (igc_test_staterr(rx_desc, IGC_RXDADV_STAT_TSIP)) {
 +			void *pktbuf = page_address(rx_buffer->page) +
 +				       rx_buffer->page_offset;
 +
  			timestamp = igc_ptp_rx_pktstamp(q_vector->adapter,
  							pktbuf);
  			pkt_offset = IGC_TS_HDR_LEN;
  			size -= IGC_TS_HDR_LEN;
  		}
  
++<<<<<<< HEAD
 +		/* retrieve a buffer from the ring */
 +		if (skb)
++=======
+ 		if (!skb) {
+ 			xdp.data = pktbuf + pkt_offset;
+ 			xdp.data_end = xdp.data + size;
+ 			xdp.data_hard_start = pktbuf - igc_rx_offset(rx_ring);
+ 			xdp_set_data_meta_invalid(&xdp);
+ 			xdp.frame_sz = truesize;
+ 			xdp.rxq = &rx_ring->xdp_rxq;
+ 
+ 			skb = igc_xdp_run_prog(adapter, &xdp);
+ 		}
+ 
+ 		if (IS_ERR(skb)) {
+ 			unsigned int xdp_res = -PTR_ERR(skb);
+ 
+ 			switch (xdp_res) {
+ 			case IGC_XDP_CONSUMED:
+ 				rx_buffer->pagecnt_bias++;
+ 				break;
+ 			case IGC_XDP_TX:
+ 			case IGC_XDP_REDIRECT:
+ 				igc_rx_buffer_flip(rx_buffer, truesize);
+ 				xdp_status |= xdp_res;
+ 				break;
+ 			}
+ 
+ 			total_packets++;
+ 			total_bytes += size;
+ 		} else if (skb)
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  			igc_add_rx_frag(rx_ring, rx_buffer, skb, size);
  		else if (ring_uses_build_skb(rx_ring))
  			skb = igc_build_skb(rx_ring, rx_buffer, rx_desc, size);
@@@ -4862,6 -5120,58 +5098,61 @@@ static int igc_setup_tc(struct net_devi
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int igc_bpf(struct net_device *dev, struct netdev_bpf *bpf)
+ {
+ 	struct igc_adapter *adapter = netdev_priv(dev);
+ 
+ 	switch (bpf->command) {
+ 	case XDP_SETUP_PROG:
+ 		return igc_xdp_set_prog(adapter, bpf->prog, bpf->extack);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int igc_xdp_xmit(struct net_device *dev, int num_frames,
+ 			struct xdp_frame **frames, u32 flags)
+ {
+ 	struct igc_adapter *adapter = netdev_priv(dev);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int i, drops;
+ 
+ 	if (unlikely(test_bit(__IGC_DOWN, &adapter->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 
+ 	drops = 0;
+ 	for (i = 0; i < num_frames; i++) {
+ 		int err;
+ 		struct xdp_frame *xdpf = frames[i];
+ 
+ 		err = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 		if (err) {
+ 			xdp_return_frame_rx_napi(xdpf);
+ 			drops++;
+ 		}
+ 	}
+ 
+ 	if (flags & XDP_XMIT_FLUSH)
+ 		igc_flush_tx_descriptors(ring);
+ 
+ 	__netif_tx_unlock(nq);
+ 
+ 	return num_frames - drops;
+ }
+ 
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  static const struct net_device_ops igc_netdev_ops = {
  	.ndo_open		= igc_open,
  	.ndo_stop		= igc_close,
@@@ -4875,6 -5185,8 +5166,11 @@@
  	.ndo_features_check	= igc_features_check,
  	.ndo_do_ioctl		= igc_ioctl,
  	.ndo_setup_tc		= igc_setup_tc,
++<<<<<<< HEAD
++=======
+ 	.ndo_bpf		= igc_bpf,
+ 	.ndo_xdp_xmit		= igc_xdp_xmit,
++>>>>>>> 4ff320361092 (igc: Add support for XDP_REDIRECT action)
  };
  
  /* PCIe configuration access */
* Unmerged path drivers/net/ethernet/intel/igc/igc_main.c
