igc: Enable RX via AF_XDP zero-copy

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Andre Guedes <andre.guedes@intel.com>
commit fc9df2a0b520d7d439ecf464794d53e91be74b93
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/fc9df2a0.failed

Add support for receiving packets via AF_XDP zero-copy mechanism.

Add a new flag to 'enum igc_ring_flags_t' to indicate the ring has
AF_XDP zero-copy enabled so proper ring setup is carried out during ring
configuration in igc_configure_rx_ring().

RX buffers can now be allocated via the shared pages mechanism (default
behavior of the driver) or via xsk pool (when AF_XDP zero-copy is
enabled) so a union is added to the 'struct igc_rx_buffer' to cover both
cases.

When AF_XDP zero-copy is enabled, rx buffers are allocated from the xsk
pool using the new helper igc_alloc_rx_buffers_zc() which is the
counterpart of igc_alloc_rx_buffers().

Likewise other Intel drivers that support AF_XDP zero-copy, in igc we
have a dedicated path for cleaning up rx irqs when zero-copy is enabled.
This avoids adding too many checks within igc_clean_rx_irq(), resulting
in a more readable and efficient code since this function is called from
the hot-path of the driver.

	Signed-off-by: Andre Guedes <andre.guedes@intel.com>
	Signed-off-by: Vedang Patel <vedang.patel@intel.com>
	Signed-off-by: Jithu Joseph <jithu.joseph@intel.com>
	Reviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: Dvora Fuxbrumer <dvorax.fuxbrumer@linux.intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit fc9df2a0b520d7d439ecf464794d53e91be74b93)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/igc/igc.h
#	drivers/net/ethernet/intel/igc/igc_main.c
#	drivers/net/ethernet/intel/igc/igc_xdp.c
#	drivers/net/ethernet/intel/igc/igc_xdp.h
diff --cc drivers/net/ethernet/intel/igc/igc.h
index b00cd8696b6d,cd6f4c94c4dd..000000000000
--- a/drivers/net/ethernet/intel/igc/igc.h
+++ b/drivers/net/ethernet/intel/igc/igc.h
@@@ -111,6 -116,9 +111,12 @@@ struct igc_ring 
  			struct sk_buff *skb;
  		};
  	};
++<<<<<<< HEAD
++=======
+ 
+ 	struct xdp_rxq_info xdp_rxq;
+ 	struct xsk_buff_pool *xsk_pool;
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  } ____cacheline_internodealigned_in_smp;
  
  /* Board specific private data structure */
diff --cc drivers/net/ethernet/intel/igc/igc_main.c
index 992dd6933ec4,3ffc20fae4c6..000000000000
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@@ -10,7 -10,8 +10,12 @@@
  #include <linux/ip.h>
  #include <linux/pm_runtime.h>
  #include <net/pkt_sched.h>
++<<<<<<< HEAD
 +
++=======
+ #include <linux/bpf_trace.h>
+ #include <net/xdp_sock_drv.h>
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  #include <net/ipv6.h>
  
  #include "igc.h"
@@@ -377,10 -387,39 +382,44 @@@ static void igc_clean_rx_ring(struct ig
  		if (i == rx_ring->count)
  			i = 0;
  	}
 -}
  
++<<<<<<< HEAD
 +	rx_ring->next_to_alloc = 0;
 +	rx_ring->next_to_clean = 0;
 +	rx_ring->next_to_use = 0;
++=======
+ static void igc_clean_rx_ring_xsk_pool(struct igc_ring *ring)
+ {
+ 	struct igc_rx_buffer *bi;
+ 	u16 i;
+ 
+ 	for (i = 0; i < ring->count; i++) {
+ 		bi = &ring->rx_buffer_info[i];
+ 		if (!bi->xdp)
+ 			continue;
+ 
+ 		xsk_buff_free(bi->xdp);
+ 		bi->xdp = NULL;
+ 	}
+ }
+ 
+ /**
+  * igc_clean_rx_ring - Free Rx Buffers per Queue
+  * @ring: ring to free buffers from
+  */
+ static void igc_clean_rx_ring(struct igc_ring *ring)
+ {
+ 	if (ring->xsk_pool)
+ 		igc_clean_rx_ring_xsk_pool(ring);
+ 	else
+ 		igc_clean_rx_ring_page_shared(ring);
+ 
+ 	clear_ring_uses_large_buffer(ring);
+ 
+ 	ring->next_to_alloc = 0;
+ 	ring->next_to_clean = 0;
+ 	ring->next_to_use = 0;
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  }
  
  /**
@@@ -515,7 -576,24 +564,27 @@@ static void igc_configure_rx_ring(struc
  	int reg_idx = ring->reg_idx;
  	u32 srrctl = 0, rxdctl = 0;
  	u64 rdba = ring->dma;
+ 	u32 buf_size;
+ 
++<<<<<<< HEAD
++=======
+ 	xdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);
+ 	ring->xsk_pool = igc_get_xsk_pool(adapter, ring);
+ 	if (ring->xsk_pool) {
+ 		WARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
+ 						   MEM_TYPE_XSK_BUFF_POOL,
+ 						   NULL));
+ 		xsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);
+ 	} else {
+ 		WARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
+ 						   MEM_TYPE_PAGE_SHARED,
+ 						   NULL));
+ 	}
  
+ 	if (igc_xdp_is_enabled(adapter))
+ 		set_ring_uses_large_buffer(ring);
+ 
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  	/* disable the queue */
  	wr32(IGC_RXDCTL(reg_idx), 0);
  
@@@ -1878,6 -1981,250 +1950,253 @@@ static void igc_alloc_rx_buffers(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static bool igc_alloc_rx_buffers_zc(struct igc_ring *ring, u16 count)
+ {
+ 	union igc_adv_rx_desc *desc;
+ 	u16 i = ring->next_to_use;
+ 	struct igc_rx_buffer *bi;
+ 	dma_addr_t dma;
+ 	bool ok = true;
+ 
+ 	if (!count)
+ 		return ok;
+ 
+ 	desc = IGC_RX_DESC(ring, i);
+ 	bi = &ring->rx_buffer_info[i];
+ 	i -= ring->count;
+ 
+ 	do {
+ 		bi->xdp = xsk_buff_alloc(ring->xsk_pool);
+ 		if (!bi->xdp) {
+ 			ok = false;
+ 			break;
+ 		}
+ 
+ 		dma = xsk_buff_xdp_get_dma(bi->xdp);
+ 		desc->read.pkt_addr = cpu_to_le64(dma);
+ 
+ 		desc++;
+ 		bi++;
+ 		i++;
+ 		if (unlikely(!i)) {
+ 			desc = IGC_RX_DESC(ring, 0);
+ 			bi = ring->rx_buffer_info;
+ 			i -= ring->count;
+ 		}
+ 
+ 		/* Clear the length for the next_to_use descriptor. */
+ 		desc->wb.upper.length = 0;
+ 
+ 		count--;
+ 	} while (count);
+ 
+ 	i += ring->count;
+ 
+ 	if (ring->next_to_use != i) {
+ 		ring->next_to_use = i;
+ 
+ 		/* Force memory writes to complete before letting h/w
+ 		 * know there are new descriptors to fetch.  (Only
+ 		 * applicable for weak-ordered memory model archs,
+ 		 * such as IA-64).
+ 		 */
+ 		wmb();
+ 		writel(i, ring->tail);
+ 	}
+ 
+ 	return ok;
+ }
+ 
+ static int igc_xdp_init_tx_buffer(struct igc_tx_buffer *buffer,
+ 				  struct xdp_frame *xdpf,
+ 				  struct igc_ring *ring)
+ {
+ 	dma_addr_t dma;
+ 
+ 	dma = dma_map_single(ring->dev, xdpf->data, xdpf->len, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ring->dev, dma)) {
+ 		netdev_err_once(ring->netdev, "Failed to map DMA for TX\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	buffer->type = IGC_TX_BUFFER_TYPE_XDP;
+ 	buffer->xdpf = xdpf;
+ 	buffer->protocol = 0;
+ 	buffer->bytecount = xdpf->len;
+ 	buffer->gso_segs = 1;
+ 	buffer->time_stamp = jiffies;
+ 	dma_unmap_len_set(buffer, len, xdpf->len);
+ 	dma_unmap_addr_set(buffer, dma, dma);
+ 	return 0;
+ }
+ 
+ /* This function requires __netif_tx_lock is held by the caller. */
+ static int igc_xdp_init_tx_descriptor(struct igc_ring *ring,
+ 				      struct xdp_frame *xdpf)
+ {
+ 	struct igc_tx_buffer *buffer;
+ 	union igc_adv_tx_desc *desc;
+ 	u32 cmd_type, olinfo_status;
+ 	int err;
+ 
+ 	if (!igc_desc_unused(ring))
+ 		return -EBUSY;
+ 
+ 	buffer = &ring->tx_buffer_info[ring->next_to_use];
+ 	err = igc_xdp_init_tx_buffer(buffer, xdpf, ring);
+ 	if (err)
+ 		return err;
+ 
+ 	cmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |
+ 		   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |
+ 		   buffer->bytecount;
+ 	olinfo_status = buffer->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;
+ 
+ 	desc = IGC_TX_DESC(ring, ring->next_to_use);
+ 	desc->read.cmd_type_len = cpu_to_le32(cmd_type);
+ 	desc->read.olinfo_status = cpu_to_le32(olinfo_status);
+ 	desc->read.buffer_addr = cpu_to_le64(dma_unmap_addr(buffer, dma));
+ 
+ 	netdev_tx_sent_queue(txring_txq(ring), buffer->bytecount);
+ 
+ 	buffer->next_to_watch = desc;
+ 
+ 	ring->next_to_use++;
+ 	if (ring->next_to_use == ring->count)
+ 		ring->next_to_use = 0;
+ 
+ 	return 0;
+ }
+ 
+ static struct igc_ring *igc_xdp_get_tx_ring(struct igc_adapter *adapter,
+ 					    int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= adapter->num_tx_queues)
+ 		index -= adapter->num_tx_queues;
+ 
+ 	return adapter->tx_ring[index];
+ }
+ 
+ static int igc_xdp_xmit_back(struct igc_adapter *adapter, struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return -EFAULT;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	res = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 	__netif_tx_unlock(nq);
+ 	return res;
+ }
+ 
+ /* This function assumes rcu_read_lock() is held by the caller. */
+ static int __igc_xdp_run_prog(struct igc_adapter *adapter,
+ 			      struct bpf_prog *prog,
+ 			      struct xdp_buff *xdp)
+ {
+ 	u32 act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		return IGC_XDP_PASS;
+ 	case XDP_TX:
+ 		return igc_xdp_xmit_back(adapter, xdp) < 0 ?
+ 			IGC_XDP_CONSUMED : IGC_XDP_TX;
+ 	case XDP_REDIRECT:
+ 		return xdp_do_redirect(adapter->netdev, xdp, prog) < 0 ?
+ 			IGC_XDP_CONSUMED : IGC_XDP_REDIRECT;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(adapter->netdev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		return IGC_XDP_CONSUMED;
+ 	}
+ }
+ 
+ static struct sk_buff *igc_xdp_run_prog(struct igc_adapter *adapter,
+ 					struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 
+ 	rcu_read_lock();
+ 
+ 	prog = READ_ONCE(adapter->xdp_prog);
+ 	if (!prog) {
+ 		res = IGC_XDP_PASS;
+ 		goto unlock;
+ 	}
+ 
+ 	res = __igc_xdp_run_prog(adapter, prog, xdp);
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 	return ERR_PTR(-res);
+ }
+ 
+ /* This function assumes __netif_tx_lock is held by the caller. */
+ static void igc_flush_tx_descriptors(struct igc_ring *ring)
+ {
+ 	/* Once tail pointer is updated, hardware can fetch the descriptors
+ 	 * any time so we issue a write membar here to ensure all memory
+ 	 * writes are complete before the tail pointer is updated.
+ 	 */
+ 	wmb();
+ 	writel(ring->next_to_use, ring->tail);
+ }
+ 
+ static void igc_finalize_xdp(struct igc_adapter *adapter, int status)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 
+ 	if (status & IGC_XDP_TX) {
+ 		ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 		nq = txring_txq(ring);
+ 
+ 		__netif_tx_lock(nq, cpu);
+ 		igc_flush_tx_descriptors(ring);
+ 		__netif_tx_unlock(nq);
+ 	}
+ 
+ 	if (status & IGC_XDP_REDIRECT)
+ 		xdp_do_flush();
+ }
+ 
+ static void igc_update_rx_stats(struct igc_q_vector *q_vector,
+ 				unsigned int packets, unsigned int bytes)
+ {
+ 	struct igc_ring *ring = q_vector->rx.ring;
+ 
+ 	u64_stats_update_begin(&ring->rx_syncp);
+ 	ring->rx_stats.packets += packets;
+ 	ring->rx_stats.bytes += bytes;
+ 	u64_stats_update_end(&ring->rx_syncp);
+ 
+ 	q_vector->rx.total_packets += packets;
+ 	q_vector->rx.total_bytes += bytes;
+ }
+ 
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  static int igc_clean_rx_irq(struct igc_q_vector *q_vector, const int budget)
  {
  	unsigned int total_bytes = 0, total_packets = 0;
@@@ -1981,6 -2353,162 +2300,165 @@@
  	return total_packets;
  }
  
++<<<<<<< HEAD
++=======
+ static struct sk_buff *igc_construct_skb_zc(struct igc_ring *ring,
+ 					    struct xdp_buff *xdp)
+ {
+ 	unsigned int metasize = xdp->data - xdp->data_meta;
+ 	unsigned int datasize = xdp->data_end - xdp->data;
+ 	unsigned int totalsize = metasize + datasize;
+ 	struct sk_buff *skb;
+ 
+ 	skb = __napi_alloc_skb(&ring->q_vector->napi,
+ 			       xdp->data_end - xdp->data_hard_start,
+ 			       GFP_ATOMIC | __GFP_NOWARN);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	skb_reserve(skb, xdp->data_meta - xdp->data_hard_start);
+ 	memcpy(__skb_put(skb, totalsize), xdp->data_meta, totalsize);
+ 	if (metasize)
+ 		skb_metadata_set(skb, metasize);
+ 
+ 	return skb;
+ }
+ 
+ static void igc_dispatch_skb_zc(struct igc_q_vector *q_vector,
+ 				union igc_adv_rx_desc *desc,
+ 				struct xdp_buff *xdp,
+ 				ktime_t timestamp)
+ {
+ 	struct igc_ring *ring = q_vector->rx.ring;
+ 	struct sk_buff *skb;
+ 
+ 	skb = igc_construct_skb_zc(ring, xdp);
+ 	if (!skb) {
+ 		ring->rx_stats.alloc_failed++;
+ 		return;
+ 	}
+ 
+ 	if (timestamp)
+ 		skb_hwtstamps(skb)->hwtstamp = timestamp;
+ 
+ 	if (igc_cleanup_headers(ring, desc, skb))
+ 		return;
+ 
+ 	igc_process_skb_fields(ring, desc, skb);
+ 	napi_gro_receive(&q_vector->napi, skb);
+ }
+ 
+ static int igc_clean_rx_irq_zc(struct igc_q_vector *q_vector, const int budget)
+ {
+ 	struct igc_adapter *adapter = q_vector->adapter;
+ 	struct igc_ring *ring = q_vector->rx.ring;
+ 	u16 cleaned_count = igc_desc_unused(ring);
+ 	int total_bytes = 0, total_packets = 0;
+ 	u16 ntc = ring->next_to_clean;
+ 	struct bpf_prog *prog;
+ 	bool failure = false;
+ 	int xdp_status = 0;
+ 
+ 	rcu_read_lock();
+ 
+ 	prog = READ_ONCE(adapter->xdp_prog);
+ 
+ 	while (likely(total_packets < budget)) {
+ 		union igc_adv_rx_desc *desc;
+ 		struct igc_rx_buffer *bi;
+ 		ktime_t timestamp = 0;
+ 		unsigned int size;
+ 		int res;
+ 
+ 		desc = IGC_RX_DESC(ring, ntc);
+ 		size = le16_to_cpu(desc->wb.upper.length);
+ 		if (!size)
+ 			break;
+ 
+ 		/* This memory barrier is needed to keep us from reading
+ 		 * any other fields out of the rx_desc until we know the
+ 		 * descriptor has been written back
+ 		 */
+ 		dma_rmb();
+ 
+ 		bi = &ring->rx_buffer_info[ntc];
+ 
+ 		if (igc_test_staterr(desc, IGC_RXDADV_STAT_TSIP)) {
+ 			timestamp = igc_ptp_rx_pktstamp(q_vector->adapter,
+ 							bi->xdp->data);
+ 
+ 			bi->xdp->data += IGC_TS_HDR_LEN;
+ 
+ 			/* HW timestamp has been copied into local variable. Metadata
+ 			 * length when XDP program is called should be 0.
+ 			 */
+ 			bi->xdp->data_meta += IGC_TS_HDR_LEN;
+ 			size -= IGC_TS_HDR_LEN;
+ 		}
+ 
+ 		bi->xdp->data_end = bi->xdp->data + size;
+ 		xsk_buff_dma_sync_for_cpu(bi->xdp, ring->xsk_pool);
+ 
+ 		res = __igc_xdp_run_prog(adapter, prog, bi->xdp);
+ 		switch (res) {
+ 		case IGC_XDP_PASS:
+ 			igc_dispatch_skb_zc(q_vector, desc, bi->xdp, timestamp);
+ 			fallthrough;
+ 		case IGC_XDP_CONSUMED:
+ 			xsk_buff_free(bi->xdp);
+ 			break;
+ 		case IGC_XDP_TX:
+ 		case IGC_XDP_REDIRECT:
+ 			xdp_status |= res;
+ 			break;
+ 		}
+ 
+ 		bi->xdp = NULL;
+ 		total_bytes += size;
+ 		total_packets++;
+ 		cleaned_count++;
+ 		ntc++;
+ 		if (ntc == ring->count)
+ 			ntc = 0;
+ 	}
+ 
+ 	ring->next_to_clean = ntc;
+ 	rcu_read_unlock();
+ 
+ 	if (cleaned_count >= IGC_RX_BUFFER_WRITE)
+ 		failure = !igc_alloc_rx_buffers_zc(ring, cleaned_count);
+ 
+ 	if (xdp_status)
+ 		igc_finalize_xdp(adapter, xdp_status);
+ 
+ 	igc_update_rx_stats(q_vector, total_packets, total_bytes);
+ 
+ 	if (xsk_uses_need_wakeup(ring->xsk_pool)) {
+ 		if (failure || ring->next_to_clean == ring->next_to_use)
+ 			xsk_set_rx_need_wakeup(ring->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(ring->xsk_pool);
+ 		return total_packets;
+ 	}
+ 
+ 	return failure ? budget : total_packets;
+ }
+ 
+ static void igc_update_tx_stats(struct igc_q_vector *q_vector,
+ 				unsigned int packets, unsigned int bytes)
+ {
+ 	struct igc_ring *ring = q_vector->tx.ring;
+ 
+ 	u64_stats_update_begin(&ring->tx_syncp);
+ 	ring->tx_stats.bytes += bytes;
+ 	ring->tx_stats.packets += packets;
+ 	u64_stats_update_end(&ring->tx_syncp);
+ 
+ 	q_vector->tx.total_bytes += bytes;
+ 	q_vector->tx.total_packets += packets;
+ }
+ 
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  /**
   * igc_clean_tx_irq - Reclaim resources after transmit completes
   * @q_vector: pointer to q_vector containing needed info
@@@ -4843,6 -5446,98 +5327,101 @@@ static int igc_setup_tc(struct net_devi
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int igc_bpf(struct net_device *dev, struct netdev_bpf *bpf)
+ {
+ 	struct igc_adapter *adapter = netdev_priv(dev);
+ 
+ 	switch (bpf->command) {
+ 	case XDP_SETUP_PROG:
+ 		return igc_xdp_set_prog(adapter, bpf->prog, bpf->extack);
+ 	case XDP_SETUP_XSK_POOL:
+ 		return igc_xdp_setup_pool(adapter, bpf->xsk.pool,
+ 					  bpf->xsk.queue_id);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int igc_xdp_xmit(struct net_device *dev, int num_frames,
+ 			struct xdp_frame **frames, u32 flags)
+ {
+ 	struct igc_adapter *adapter = netdev_priv(dev);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	struct igc_ring *ring;
+ 	int i, drops;
+ 
+ 	if (unlikely(test_bit(__IGC_DOWN, &adapter->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	ring = igc_xdp_get_tx_ring(adapter, cpu);
+ 	nq = txring_txq(ring);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 
+ 	drops = 0;
+ 	for (i = 0; i < num_frames; i++) {
+ 		int err;
+ 		struct xdp_frame *xdpf = frames[i];
+ 
+ 		err = igc_xdp_init_tx_descriptor(ring, xdpf);
+ 		if (err) {
+ 			xdp_return_frame_rx_napi(xdpf);
+ 			drops++;
+ 		}
+ 	}
+ 
+ 	if (flags & XDP_XMIT_FLUSH)
+ 		igc_flush_tx_descriptors(ring);
+ 
+ 	__netif_tx_unlock(nq);
+ 
+ 	return num_frames - drops;
+ }
+ 
+ static void igc_trigger_rxtxq_interrupt(struct igc_adapter *adapter,
+ 					struct igc_q_vector *q_vector)
+ {
+ 	struct igc_hw *hw = &adapter->hw;
+ 	u32 eics = 0;
+ 
+ 	eics |= q_vector->eims_value;
+ 	wr32(IGC_EICS, eics);
+ }
+ 
+ int igc_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags)
+ {
+ 	struct igc_adapter *adapter = netdev_priv(dev);
+ 	struct igc_q_vector *q_vector;
+ 	struct igc_ring *ring;
+ 
+ 	if (test_bit(__IGC_DOWN, &adapter->state))
+ 		return -ENETDOWN;
+ 
+ 	if (!igc_xdp_is_enabled(adapter))
+ 		return -ENXIO;
+ 
+ 	if (queue_id >= adapter->num_rx_queues)
+ 		return -EINVAL;
+ 
+ 	ring = adapter->rx_ring[queue_id];
+ 
+ 	if (!ring->xsk_pool)
+ 		return -ENXIO;
+ 
+ 	q_vector = adapter->q_vector[queue_id];
+ 	if (!napi_if_scheduled_mark_missed(&q_vector->napi))
+ 		igc_trigger_rxtxq_interrupt(adapter, q_vector);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  static const struct net_device_ops igc_netdev_ops = {
  	.ndo_open		= igc_open,
  	.ndo_stop		= igc_close,
@@@ -4856,6 -5551,9 +5435,12 @@@
  	.ndo_features_check	= igc_features_check,
  	.ndo_do_ioctl		= igc_ioctl,
  	.ndo_setup_tc		= igc_setup_tc,
++<<<<<<< HEAD
++=======
+ 	.ndo_bpf		= igc_bpf,
+ 	.ndo_xdp_xmit		= igc_xdp_xmit,
+ 	.ndo_xsk_wakeup		= igc_xsk_wakeup,
++>>>>>>> fc9df2a0b520 (igc: Enable RX via AF_XDP zero-copy)
  };
  
  /* PCIe configuration access */
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.h
* Unmerged path drivers/net/ethernet/intel/igc/igc.h
diff --git a/drivers/net/ethernet/intel/igc/igc_base.h b/drivers/net/ethernet/intel/igc/igc_base.h
index ea627ce52525..2ca028c1919f 100644
--- a/drivers/net/ethernet/intel/igc/igc_base.h
+++ b/drivers/net/ethernet/intel/igc/igc_base.h
@@ -81,6 +81,7 @@ union igc_adv_rx_desc {
 
 /* Additional Receive Descriptor Control definitions */
 #define IGC_RXDCTL_QUEUE_ENABLE	0x02000000 /* Ena specific Rx Queue */
+#define IGC_RXDCTL_SWFLUSH		0x04000000 /* Receive Software Flush */
 
 /* SRRCTL bit definitions */
 #define IGC_SRRCTL_BSIZEPKT_SHIFT		10 /* Shift _right_ */
* Unmerged path drivers/net/ethernet/intel/igc/igc_main.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.c
* Unmerged path drivers/net/ethernet/intel/igc/igc_xdp.h
