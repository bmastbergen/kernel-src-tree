x86/fpu: Add sanity checks for XFD

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 5529acf47ec31ece0815f69d43f5e6a1e485a0f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/5529acf4.failed

Add debug functionality to ensure that the XFD MSR is up to date for XSAVE*
and XRSTOR* operations.

 [ tglx: Improve comment. ]

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20211021225527.10184-16-chang.seok.bae@intel.com
(cherry picked from commit 5529acf47ec31ece0815f69d43f5e6a1e485a0f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/core.c
#	arch/x86/kernel/fpu/signal.c
#	arch/x86/kernel/fpu/xstate.c
#	arch/x86/kernel/fpu/xstate.h
diff --cc arch/x86/kernel/fpu/core.c
index 483daf3e67a3,b5f5b08b84d7..000000000000
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@@ -142,12 -155,23 +142,27 @@@ void restore_fpregs_from_fpstate(union 
  	}
  
  	if (use_xsave()) {
++<<<<<<< HEAD
 +		os_xrstor(&fpstate->xsave, mask);
++=======
+ 		/*
+ 		 * Restoring state always needs to modify all features
+ 		 * which are in @mask even if the current task cannot use
+ 		 * extended features.
+ 		 *
+ 		 * So fpstate->xfeatures cannot be used here, because then
+ 		 * a feature for which the task has no permission but was
+ 		 * used by the previous task would not go into init state.
+ 		 */
+ 		mask = fpu_kernel_cfg.max_features & mask;
+ 
+ 		os_xrstor(fpstate, mask);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  	} else {
  		if (use_fxsr())
 -			fxrstor(&fpstate->regs.fxsave);
 +			fxrstor(&fpstate->fxsave);
  		else
 -			frstor(&fpstate->regs.fsave);
 +			frstor(&fpstate->fsave);
  	}
  }
  
@@@ -417,11 -534,11 +432,15 @@@ void fpu__drop(struct fpu *fpu
  static inline void restore_fpregs_from_init_fpstate(u64 features_mask)
  {
  	if (use_xsave())
++<<<<<<< HEAD
 +		os_xrstor(&init_fpstate.xsave, features_mask);
++=======
+ 		os_xrstor(&init_fpstate, features_mask);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  	else if (use_fxsr())
 -		fxrstor(&init_fpstate.regs.fxsave);
 +		fxrstor(&init_fpstate.fxsave);
  	else
 -		frstor(&init_fpstate.regs.fsave);
 +		frstor(&init_fpstate.fsave);
  
  	pkru_write_default();
  }
@@@ -474,12 -591,11 +493,17 @@@ void fpu__clear_user_states(struct fpu 
  	 * corresponding registers.
  	 */
  	if (xfeatures_mask_supervisor() &&
++<<<<<<< HEAD
 +	    !fpregs_state_valid(fpu, smp_processor_id())) {
 +		os_xrstor(&fpu->state.xsave, xfeatures_mask_supervisor());
 +	}
++=======
+ 	    !fpregs_state_valid(fpu, smp_processor_id()))
+ 		os_xrstor_supervisor(fpu->fpstate);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  
  	/* Reset user states in registers. */
 -	restore_fpregs_from_init_fpstate(XFEATURE_MASK_USER_RESTORE);
 +	restore_fpregs_from_init_fpstate(xfeatures_mask_restore_user());
  
  	/*
  	 * Now all FPU registers have their desired values.  Inform the FPU
diff --cc arch/x86/kernel/fpu/signal.c
index f74c29985497,16fdecd02341..000000000000
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@@ -240,7 -261,7 +240,11 @@@ static int __restore_fpregs_from_user(v
  			ret = fxrstor_from_user_sigframe(buf);
  
  		if (!ret && unlikely(init_bv))
++<<<<<<< HEAD
 +			os_xrstor(&init_fpstate.xsave, init_bv);
++=======
+ 			os_xrstor(&init_fpstate, init_bv);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  		return ret;
  	} else if (use_fxsr()) {
  		return fxrstor_from_user_sigframe(buf);
@@@ -301,7 -322,7 +305,11 @@@ retry
  	 * been restored from a user buffer directly.
  	 */
  	if (test_thread_flag(TIF_NEED_FPU_LOAD) && xfeatures_mask_supervisor())
++<<<<<<< HEAD
 +		os_xrstor(&fpu->state.xsave, xfeatures_mask_supervisor());
++=======
+ 		os_xrstor_supervisor(fpu->fpstate);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  
  	fpregs_mark_activate();
  	fpregs_unlock();
@@@ -411,13 -431,14 +419,19 @@@ static int __fpu_restore_sig(void __use
  		 */
  		u64 mask = user_xfeatures | xfeatures_mask_supervisor();
  
++<<<<<<< HEAD
 +		fpu->state.xsave.header.xfeatures &= mask;
 +		ret = os_xrstor_safe(&fpu->state.xsave, xfeatures_mask_all);
++=======
+ 		fpregs->xsave.header.xfeatures &= mask;
+ 		success = !os_xrstor_safe(fpu->fpstate,
+ 					  fpu_kernel_cfg.max_features);
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  	} else {
 -		success = !fxrstor_safe(&fpregs->fxsave);
 +		ret = fxrstor_safe(&fpu->state.fxsave);
  	}
  
 -	if (likely(success))
 +	if (likely(!ret))
  		fpregs_mark_activate();
  
  	fpregs_unlock();
diff --cc arch/x86/kernel/fpu/xstate.c
index f744359fb635,603edeb7b913..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1288,8 -1289,229 +1288,233 @@@ void xrstors(struct xregs_state *xstate
  	WARN_ON_ONCE(err);
  }
  
++<<<<<<< HEAD
++=======
+ #if IS_ENABLED(CONFIG_KVM)
+ void fpstate_clear_xstate_component(struct fpstate *fps, unsigned int xfeature)
+ {
+ 	void *addr = get_xsave_addr(&fps->regs.xsave, xfeature);
+ 
+ 	if (addr)
+ 		memset(addr, 0, xstate_sizes[xfeature]);
+ }
+ EXPORT_SYMBOL_GPL(fpstate_clear_xstate_component);
+ #endif
+ 
+ #ifdef CONFIG_X86_64
+ 
+ #ifdef CONFIG_X86_DEBUG_FPU
+ /*
+  * Ensure that a subsequent XSAVE* or XRSTOR* instruction with RFBM=@mask
+  * can safely operate on the @fpstate buffer.
+  */
+ static bool xstate_op_valid(struct fpstate *fpstate, u64 mask, bool rstor)
+ {
+ 	u64 xfd = __this_cpu_read(xfd_state);
+ 
+ 	if (fpstate->xfd == xfd)
+ 		return true;
+ 
+ 	 /*
+ 	  * The XFD MSR does not match fpstate->xfd. That's invalid when
+ 	  * the passed in fpstate is current's fpstate.
+ 	  */
+ 	if (fpstate->xfd == current->thread.fpu.fpstate->xfd)
+ 		return false;
+ 
+ 	/*
+ 	 * XRSTOR(S) from init_fpstate are always correct as it will just
+ 	 * bring all components into init state and not read from the
+ 	 * buffer. XSAVE(S) raises #PF after init.
+ 	 */
+ 	if (fpstate == &init_fpstate)
+ 		return rstor;
+ 
+ 	/*
+ 	 * XSAVE(S): clone(), fpu_swap_kvm_fpu()
+ 	 * XRSTORS(S): fpu_swap_kvm_fpu()
+ 	 */
+ 
+ 	/*
+ 	 * No XSAVE/XRSTOR instructions (except XSAVE itself) touch
+ 	 * the buffer area for XFD-disabled state components.
+ 	 */
+ 	mask &= ~xfd;
+ 
+ 	/*
+ 	 * Remove features which are valid in fpstate. They
+ 	 * have space allocated in fpstate.
+ 	 */
+ 	mask &= ~fpstate->xfeatures;
+ 
+ 	/*
+ 	 * Any remaining state components in 'mask' might be written
+ 	 * by XSAVE/XRSTOR. Fail validation it found.
+ 	 */
+ 	return !mask;
+ }
+ 
+ void xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor)
+ {
+ 	WARN_ON_ONCE(!xstate_op_valid(fpstate, mask, rstor));
+ }
+ #endif /* CONFIG_X86_DEBUG_FPU */
+ 
+ static int validate_sigaltstack(unsigned int usize)
+ {
+ 	struct task_struct *thread, *leader = current->group_leader;
+ 	unsigned long framesize = get_sigframe_size();
+ 
+ 	lockdep_assert_held(&current->sighand->siglock);
+ 
+ 	/* get_sigframe_size() is based on fpu_user_cfg.max_size */
+ 	framesize -= fpu_user_cfg.max_size;
+ 	framesize += usize;
+ 	for_each_thread(leader, thread) {
+ 		if (thread->sas_ss_size && thread->sas_ss_size < framesize)
+ 			return -ENOSPC;
+ 	}
+ 	return 0;
+ }
+ 
+ static int __xstate_request_perm(u64 permitted, u64 requested)
+ {
+ 	/*
+ 	 * This deliberately does not exclude !XSAVES as we still might
+ 	 * decide to optionally context switch XCR0 or talk the silicon
+ 	 * vendors into extending XFD for the pre AMX states.
+ 	 */
+ 	bool compacted = cpu_feature_enabled(X86_FEATURE_XSAVES);
+ 	struct fpu *fpu = &current->group_leader->thread.fpu;
+ 	unsigned int ksize, usize;
+ 	u64 mask;
+ 	int ret;
+ 
+ 	/* Check whether fully enabled */
+ 	if ((permitted & requested) == requested)
+ 		return 0;
+ 
+ 	/* Calculate the resulting kernel state size */
+ 	mask = permitted | requested;
+ 	ksize = xstate_calculate_size(mask, compacted);
+ 
+ 	/* Calculate the resulting user state size */
+ 	mask &= XFEATURE_MASK_USER_SUPPORTED;
+ 	usize = xstate_calculate_size(mask, false);
+ 
+ 	ret = validate_sigaltstack(usize);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Pairs with the READ_ONCE() in xstate_get_group_perm() */
+ 	WRITE_ONCE(fpu->perm.__state_perm, requested);
+ 	/* Protected by sighand lock */
+ 	fpu->perm.__state_size = ksize;
+ 	fpu->perm.__user_state_size = usize;
+ 	return ret;
+ }
+ 
+ /*
+  * Permissions array to map facilities with more than one component
+  */
+ static const u64 xstate_prctl_req[XFEATURE_MAX] = {
+ 	/* [XFEATURE_XTILE_DATA] = XFEATURE_MASK_XTILE, */
+ };
+ 
+ static int xstate_request_perm(unsigned long idx)
+ {
+ 	u64 permitted, requested;
+ 	int ret;
+ 
+ 	if (idx >= XFEATURE_MAX)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Look up the facility mask which can require more than
+ 	 * one xstate component.
+ 	 */
+ 	idx = array_index_nospec(idx, ARRAY_SIZE(xstate_prctl_req));
+ 	requested = xstate_prctl_req[idx];
+ 	if (!requested)
+ 		return -EOPNOTSUPP;
+ 
+ 	if ((fpu_user_cfg.max_features & requested) != requested)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Lockless quick check */
+ 	permitted = xstate_get_host_group_perm();
+ 	if ((permitted & requested) == requested)
+ 		return 0;
+ 
+ 	/* Protect against concurrent modifications */
+ 	spin_lock_irq(&current->sighand->siglock);
+ 	permitted = xstate_get_host_group_perm();
+ 	ret = __xstate_request_perm(permitted, requested);
+ 	spin_unlock_irq(&current->sighand->siglock);
+ 	return ret;
+ }
+ #else /* CONFIG_X86_64 */
+ static inline int xstate_request_perm(unsigned long idx)
+ {
+ 	return -EPERM;
+ }
+ #endif  /* !CONFIG_X86_64 */
+ 
+ /**
+  * fpu_xstate_prctl - xstate permission operations
+  * @tsk:	Redundant pointer to current
+  * @option:	A subfunction of arch_prctl()
+  * @arg2:	option argument
+  * Return:	0 if successful; otherwise, an error code
+  *
+  * Option arguments:
+  *
+  * ARCH_GET_XCOMP_SUPP: Pointer to user space u64 to store the info
+  * ARCH_GET_XCOMP_PERM: Pointer to user space u64 to store the info
+  * ARCH_REQ_XCOMP_PERM: Facility number requested
+  *
+  * For facilities which require more than one XSTATE component, the request
+  * must be the highest state component number related to that facility,
+  * e.g. for AMX which requires XFEATURE_XTILE_CFG(17) and
+  * XFEATURE_XTILE_DATA(18) this would be XFEATURE_XTILE_DATA(18).
+  */
+ long fpu_xstate_prctl(struct task_struct *tsk, int option, unsigned long arg2)
+ {
+ 	u64 __user *uptr = (u64 __user *)arg2;
+ 	u64 permitted, supported;
+ 	unsigned long idx = arg2;
+ 
+ 	if (tsk != current)
+ 		return -EPERM;
+ 
+ 	switch (option) {
+ 	case ARCH_GET_XCOMP_SUPP:
+ 		supported = fpu_user_cfg.max_features |	fpu_user_cfg.legacy_features;
+ 		return put_user(supported, uptr);
+ 
+ 	case ARCH_GET_XCOMP_PERM:
+ 		/*
+ 		 * Lockless snapshot as it can also change right after the
+ 		 * dropping the lock.
+ 		 */
+ 		permitted = xstate_get_host_group_perm();
+ 		permitted &= XFEATURE_MASK_USER_SUPPORTED;
+ 		return put_user(permitted, uptr);
+ 
+ 	case ARCH_REQ_XCOMP_PERM:
+ 		if (!IS_ENABLED(CONFIG_X86_64))
+ 			return -EOPNOTSUPP;
+ 
+ 		return xstate_request_perm(idx);
+ 
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  #ifdef CONFIG_PROC_PID_ARCH_STATUS
 +
  /*
   * Report the amount of time elapsed in millisecond since last AVX512
   * use in the task.
diff --cc arch/x86/kernel/fpu/xstate.h
index 0789a04ee705,29024244965b..000000000000
--- a/arch/x86/kernel/fpu/xstate.h
+++ b/arch/x86/kernel/fpu/xstate.h
@@@ -15,4 -19,243 +15,246 @@@ static inline void xstate_init_xcomp_bv
  		xsave->header.xcomp_bv = mask | XCOMP_BV_COMPACTED_FORMAT;
  }
  
++<<<<<<< HEAD
++=======
+ static inline u64 xstate_get_host_group_perm(void)
+ {
+ 	/* Pairs with WRITE_ONCE() in xstate_request_perm() */
+ 	return READ_ONCE(current->group_leader->thread.fpu.perm.__state_perm);
+ }
+ 
+ enum xstate_copy_mode {
+ 	XSTATE_COPY_FP,
+ 	XSTATE_COPY_FX,
+ 	XSTATE_COPY_XSAVE,
+ };
+ 
+ struct membuf;
+ extern void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
+ 				      u32 pkru_val, enum xstate_copy_mode copy_mode);
+ extern void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 				    enum xstate_copy_mode mode);
+ extern int copy_uabi_from_kernel_to_xstate(struct fpstate *fpstate, const void *kbuf);
+ extern int copy_sigframe_from_user_to_xstate(struct fpstate *fpstate, const void __user *ubuf);
+ 
+ 
+ extern void fpu__init_cpu_xstate(void);
+ extern void fpu__init_system_xstate(unsigned int legacy_size);
+ 
+ extern void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);
+ 
+ static inline u64 xfeatures_mask_supervisor(void)
+ {
+ 	return fpu_kernel_cfg.max_features & XFEATURE_MASK_SUPERVISOR_SUPPORTED;
+ }
+ 
+ static inline u64 xfeatures_mask_independent(void)
+ {
+ 	if (!cpu_feature_enabled(X86_FEATURE_ARCH_LBR))
+ 		return XFEATURE_MASK_INDEPENDENT & ~XFEATURE_MASK_LBR;
+ 
+ 	return XFEATURE_MASK_INDEPENDENT;
+ }
+ 
+ /* XSAVE/XRSTOR wrapper functions */
+ 
+ #ifdef CONFIG_X86_64
+ #define REX_PREFIX	"0x48, "
+ #else
+ #define REX_PREFIX
+ #endif
+ 
+ /* These macros all use (%edi)/(%rdi) as the single memory argument. */
+ #define XSAVE		".byte " REX_PREFIX "0x0f,0xae,0x27"
+ #define XSAVEOPT	".byte " REX_PREFIX "0x0f,0xae,0x37"
+ #define XSAVES		".byte " REX_PREFIX "0x0f,0xc7,0x2f"
+ #define XRSTOR		".byte " REX_PREFIX "0x0f,0xae,0x2f"
+ #define XRSTORS		".byte " REX_PREFIX "0x0f,0xc7,0x1f"
+ 
+ /*
+  * After this @err contains 0 on success or the trap number when the
+  * operation raises an exception.
+  */
+ #define XSTATE_OP(op, st, lmask, hmask, err)				\
+ 	asm volatile("1:" op "\n\t"					\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "2:\n\t"						\
+ 		     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_FAULT_MCE_SAFE)	\
+ 		     : [err] "=a" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * If XSAVES is enabled, it replaces XSAVEOPT because it supports a compact
+  * format and supervisor states in addition to modified optimization in
+  * XSAVEOPT.
+  *
+  * Otherwise, if XSAVEOPT is enabled, XSAVEOPT replaces XSAVE because XSAVEOPT
+  * supports modified optimization which is not supported by XSAVE.
+  *
+  * We use XSAVE as a fallback.
+  *
+  * The 661 label is defined in the ALTERNATIVE* macros as the address of the
+  * original instruction which gets replaced. We need to use it here as the
+  * address of the instruction where we might get an exception at.
+  */
+ #define XSTATE_XSAVE(st, lmask, hmask, err)				\
+ 	asm volatile(ALTERNATIVE_2(XSAVE,				\
+ 				   XSAVEOPT, X86_FEATURE_XSAVEOPT,	\
+ 				   XSAVES,   X86_FEATURE_XSAVES)	\
+ 		     "\n"						\
+ 		     "xor %[err], %[err]\n"				\
+ 		     "3:\n"						\
+ 		     ".pushsection .fixup,\"ax\"\n"			\
+ 		     "4: movl $-2, %[err]\n"				\
+ 		     "jmp 3b\n"						\
+ 		     ".popsection\n"					\
+ 		     _ASM_EXTABLE(661b, 4b)				\
+ 		     : [err] "=r" (err)					\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ /*
+  * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact
+  * XSAVE area format.
+  */
+ #define XSTATE_XRESTORE(st, lmask, hmask)				\
+ 	asm volatile(ALTERNATIVE(XRSTOR,				\
+ 				 XRSTORS, X86_FEATURE_XSAVES)		\
+ 		     "\n"						\
+ 		     "3:\n"						\
+ 		     _ASM_EXTABLE_TYPE(661b, 3b, EX_TYPE_FPU_RESTORE)	\
+ 		     :							\
+ 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+ 		     : "memory")
+ 
+ #if defined(CONFIG_X86_64) && defined(CONFIG_X86_DEBUG_FPU)
+ extern void xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor);
+ #else
+ static inline void xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor) { }
+ #endif
+ 
+ /*
+  * Save processor xstate to xsave area.
+  *
+  * Uses either XSAVE or XSAVEOPT or XSAVES depending on the CPU features
+  * and command line options. The choice is permanent until the next reboot.
+  */
+ static inline void os_xsave(struct fpstate *fpstate)
+ {
+ 	u64 mask = fpstate->xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	WARN_ON_FPU(!alternatives_patched);
+ 	xfd_validate_state(fpstate, mask, false);
+ 
+ 	XSTATE_XSAVE(&fpstate->regs.xsave, lmask, hmask, err);
+ 
+ 	/* We should never fault when copying to a kernel buffer: */
+ 	WARN_ON_FPU(err);
+ }
+ 
+ /*
+  * Restore processor xstate from xsave area.
+  *
+  * Uses XRSTORS when XSAVES is used, XRSTOR otherwise.
+  */
+ static inline void os_xrstor(struct fpstate *fpstate, u64 mask)
+ {
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 
+ 	xfd_validate_state(fpstate, mask, true);
+ 	XSTATE_XRESTORE(&fpstate->regs.xsave, lmask, hmask);
+ }
+ 
+ /* Restore of supervisor state. Does not require XFD */
+ static inline void os_xrstor_supervisor(struct fpstate *fpstate)
+ {
+ 	u64 mask = xfeatures_mask_supervisor();
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 
+ 	XSTATE_XRESTORE(&fpstate->regs.xsave, lmask, hmask);
+ }
+ 
+ /*
+  * Save xstate to user space xsave area.
+  *
+  * We don't use modified optimization because xrstor/xrstors might track
+  * a different application.
+  *
+  * We don't use compacted format xsave area for backward compatibility for
+  * old applications which don't understand the compacted format of the
+  * xsave area.
+  *
+  * The caller has to zero buf::header before calling this because XSAVE*
+  * does not touch the reserved fields in the header.
+  */
+ static inline int xsave_to_user_sigframe(struct xregs_state __user *buf)
+ {
+ 	/*
+ 	 * Include the features which are not xsaved/rstored by the kernel
+ 	 * internally, e.g. PKRU. That's user space ABI and also required
+ 	 * to allow the signal handler to modify PKRU.
+ 	 */
+ 	struct fpstate *fpstate = current->thread.fpu.fpstate;
+ 	u64 mask = fpstate->user_xfeatures;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	xfd_validate_state(fpstate, mask, false);
+ 
+ 	stac();
+ 	XSTATE_OP(XSAVE, buf, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from user space xsave area.
+  */
+ static inline int xrstor_from_user_sigframe(struct xregs_state __user *buf, u64 mask)
+ {
+ 	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	xfd_validate_state(current->thread.fpu.fpstate, mask, true);
+ 
+ 	stac();
+ 	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 	clac();
+ 
+ 	return err;
+ }
+ 
+ /*
+  * Restore xstate from kernel space xsave area, return an error code instead of
+  * an exception.
+  */
+ static inline int os_xrstor_safe(struct fpstate *fpstate, u64 mask)
+ {
+ 	struct xregs_state *xstate = &fpstate->regs.xsave;
+ 	u32 lmask = mask;
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+ 	/* Must enforce XFD update here */
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_XSAVES))
+ 		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
+ 	else
+ 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 
+ 	return err;
+ }
+ 
+ 
++>>>>>>> 5529acf47ec3 (x86/fpu: Add sanity checks for XFD)
  #endif
* Unmerged path arch/x86/kernel/fpu/core.c
* Unmerged path arch/x86/kernel/fpu/signal.c
* Unmerged path arch/x86/kernel/fpu/xstate.c
* Unmerged path arch/x86/kernel/fpu/xstate.h
