skmsg: Increase sk->sk_drops when dropping packets

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-365.el8
commit-author Cong Wang <cong.wang@bytedance.com>
commit 781dd0431eb549f9cb1fdddf91a50d985febe884
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-365.el8/781dd043.failed

It is hard to observe packet drops without increasing relevant
drop counters, here we should increase sk->sk_drops which is
a protocol-independent counter. Fortunately psock is always
associated with a struct sock, we can just use psock->sk.

	Suggested-by: John Fastabend <john.fastabend@gmail.com>
	Signed-off-by: Cong Wang <cong.wang@bytedance.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
	Acked-by: Jakub Sitnicki <jakub@cloudflare.com>
Link: https://lore.kernel.org/bpf/20210615021342.7416-9-xiyou.wangcong@gmail.com
(cherry picked from commit 781dd0431eb549f9cb1fdddf91a50d985febe884)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skmsg.c
diff --cc net/core/skmsg.c
index cfa6fc7cc1e0,9b6160a191f8..000000000000
--- a/net/core/skmsg.c
+++ b/net/core/skmsg.c
@@@ -630,9 -708,14 +636,18 @@@ static void __sk_psock_purge_ingress_ms
  	}
  }
  
 -static void __sk_psock_zap_ingress(struct sk_psock *psock)
 +static void sk_psock_zap_ingress(struct sk_psock *psock)
  {
++<<<<<<< HEAD
 +	__skb_queue_purge(&psock->ingress_skb);
++=======
+ 	struct sk_buff *skb;
+ 
+ 	while ((skb = skb_dequeue(&psock->ingress_skb)) != NULL) {
+ 		skb_bpf_redirect_clear(skb);
+ 		sock_drop(psock->sk, skb);
+ 	}
++>>>>>>> 781dd0431eb5 (skmsg: Increase sk->sk_drops when dropping packets)
  	__sk_psock_purge_ingress_msg(psock);
  }
  
@@@ -768,18 -840,25 +783,37 @@@ static void sk_psock_skb_redirect(struc
  	 * return code, but then didn't set a redirect interface.
  	 */
  	if (unlikely(!sk_other)) {
++<<<<<<< HEAD
 +		kfree_skb(skb);
 +		return;
++=======
+ 		sock_drop(from->sk, skb);
+ 		return -EIO;
++>>>>>>> 781dd0431eb5 (skmsg: Increase sk->sk_drops when dropping packets)
  	}
  	psock_other = sk_psock(sk_other);
  	/* This error indicates the socket is being torn down or had another
  	 * error that caused the pipe to break. We can't send a packet on
  	 * a socket that is in this state so we drop the skb.
  	 */
++<<<<<<< HEAD
 +	if (!psock_other || sock_flag(sk_other, SOCK_DEAD) ||
 +	    !sk_psock_test_state(psock_other, SK_PSOCK_TX_ENABLED)) {
 +		kfree_skb(skb);
 +		return;
++=======
+ 	if (!psock_other || sock_flag(sk_other, SOCK_DEAD)) {
+ 		skb_bpf_redirect_clear(skb);
+ 		sock_drop(from->sk, skb);
+ 		return -EIO;
+ 	}
+ 	spin_lock_bh(&psock_other->ingress_lock);
+ 	if (!sk_psock_test_state(psock_other, SK_PSOCK_TX_ENABLED)) {
+ 		spin_unlock_bh(&psock_other->ingress_lock);
+ 		skb_bpf_redirect_clear(skb);
+ 		sock_drop(from->sk, skb);
+ 		return -EIO;
++>>>>>>> 781dd0431eb5 (skmsg: Increase sk->sk_drops when dropping packets)
  	}
  
  	skb_queue_tail(&psock_other->ingress_skb, skb);
@@@ -852,16 -943,35 +886,16 @@@ static void sk_psock_verdict_apply(stru
  		}
  		break;
  	case __SK_REDIRECT:
 -		err = sk_psock_skb_redirect(psock, skb);
 +		sk_psock_skb_redirect(skb);
  		break;
  	case __SK_DROP:
 +		/* fall-through */
  	default:
  out_free:
- 		kfree_skb(skb);
+ 		sock_drop(psock->sk, skb);
  	}
 -
 -	return err;
  }
  
 -static void sk_psock_write_space(struct sock *sk)
 -{
 -	struct sk_psock *psock;
 -	void (*write_space)(struct sock *sk) = NULL;
 -
 -	rcu_read_lock();
 -	psock = sk_psock(sk);
 -	if (likely(psock)) {
 -		if (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED))
 -			schedule_work(&psock->work);
 -		write_space = psock->saved_write_space;
 -	}
 -	rcu_read_unlock();
 -	if (write_space)
 -		write_space(sk);
 -}
 -
 -#if IS_ENABLED(CONFIG_BPF_STREAM_PARSER)
  static void sk_psock_strp_read(struct strparser *strp, struct sk_buff *skb)
  {
  	struct sk_psock *psock;
@@@ -873,15 -983,16 +907,15 @@@
  	sk = strp->sk;
  	psock = sk_psock(sk);
  	if (unlikely(!psock)) {
- 		kfree_skb(skb);
+ 		sock_drop(sk, skb);
  		goto out;
  	}
 -	prog = READ_ONCE(psock->progs.stream_verdict);
 +	prog = READ_ONCE(psock->progs.skb_verdict);
  	if (likely(prog)) {
  		skb->sk = sk;
 -		skb_dst_drop(skb);
 -		skb_bpf_redirect_clear(skb);
 -		ret = bpf_prog_run_pin_on_cpu(prog, skb);
 -		ret = sk_psock_map_verd(ret, skb_bpf_redirect_fetch(skb));
 +		tcp_skb_bpf_redirect_clear(skb);
 +		ret = sk_psock_bpf_run(psock, prog, skb);
 +		ret = sk_psock_map_verd(ret, tcp_skb_bpf_redirect_fetch(skb));
  		skb->sk = NULL;
  	}
  	sk_psock_verdict_apply(psock, skb, ret);
@@@ -950,18 -1104,22 +984,18 @@@ static int sk_psock_verdict_recv(read_d
  	psock = sk_psock(sk);
  	if (unlikely(!psock)) {
  		len = 0;
- 		kfree_skb(skb);
+ 		sock_drop(sk, skb);
  		goto out;
  	}
 -	prog = READ_ONCE(psock->progs.stream_verdict);
 -	if (!prog)
 -		prog = READ_ONCE(psock->progs.skb_verdict);
 +	prog = READ_ONCE(psock->progs.skb_verdict);
  	if (likely(prog)) {
  		skb->sk = sk;
 -		skb_dst_drop(skb);
 -		skb_bpf_redirect_clear(skb);
 -		ret = bpf_prog_run_pin_on_cpu(prog, skb);
 -		ret = sk_psock_map_verd(ret, skb_bpf_redirect_fetch(skb));
 +		tcp_skb_bpf_redirect_clear(skb);
 +		ret = sk_psock_bpf_run(psock, prog, skb);
 +		ret = sk_psock_map_verd(ret, tcp_skb_bpf_redirect_fetch(skb));
  		skb->sk = NULL;
  	}
 -	if (sk_psock_verdict_apply(psock, skb, ret) < 0)
 -		len = 0;
 +	sk_psock_verdict_apply(psock, skb, ret);
  out:
  	rcu_read_unlock();
  	return len;
* Unmerged path net/core/skmsg.c
