ice: Gather the rx buf clean-up logic for better reuse

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Maciej Fijalkowski <maciej.fijalkowski@intel.com>
commit 1d032bc77bb8f85d8adfd14a1a8c67c12b07eece
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/1d032bc7.failed

Pull out the code responsible for page counting and buffer recycling so
that it will be possible to clean up the Rx buffers in cases where we
won't allocate skb (ex. XDP)

	Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 1d032bc77bb8f85d8adfd14a1a8c67c12b07eece)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_txrx.c
diff --cc drivers/net/ethernet/intel/ice/ice_txrx.c
index b0086743621b,122a0af1ea52..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@@ -497,22 -499,85 +497,94 @@@ static bool ice_page_is_reserved(struc
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ice_rx_buf_adjust_pg_offset - Prepare Rx buffer for reuse
+  * @rx_buf: Rx buffer to adjust
+  * @size: Size of adjustment
+  *
+  * Update the offset within page so that Rx buf will be ready to be reused.
+  * For systems with PAGE_SIZE < 8192 this function will flip the page offset
+  * so the second half of page assigned to Rx buffer will be used, otherwise
+  * the offset is moved by the @size bytes
+  */
+ static void
+ ice_rx_buf_adjust_pg_offset(struct ice_rx_buf *rx_buf, unsigned int size)
+ {
+ #if (PAGE_SIZE < 8192)
+ 	/* flip page offset to other buffer */
+ 	rx_buf->page_offset ^= size;
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buf->page_offset += size;
+ #endif
+ }
+ 
+ /**
+  * ice_can_reuse_rx_page - Determine if page can be reused for another Rx
+  * @rx_buf: buffer containing the page
+  *
+  * If page is reusable, we have a green light for calling ice_reuse_rx_page,
+  * which will assign the current buffer to the buffer that next_to_alloc is
+  * pointing to; otherwise, the DMA mapping needs to be destroyed and
+  * page freed
+  */
+ static bool ice_can_reuse_rx_page(struct ice_rx_buf *rx_buf)
+ {
+ #if (PAGE_SIZE >= 8192)
+ 	unsigned int last_offset = PAGE_SIZE - ICE_RXBUF_2048;
+ #endif
+ 	unsigned int pagecnt_bias = rx_buf->pagecnt_bias;
+ 	struct page *page = rx_buf->page;
+ 
+ 	/* avoid re-using remote pages */
+ 	if (unlikely(ice_page_is_reserved(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely((page_count(page) - pagecnt_bias) > 1))
+ 		return false;
+ #else
+ 	if (rx_buf->page_offset > last_offset)
+ 		return false;
+ #endif /* PAGE_SIZE < 8192) */
+ 
+ 	/* If we have drained the page fragment pool we need to update
+ 	 * the pagecnt_bias and page count so that we fully restock the
+ 	 * number of references the driver holds.
+ 	 */
+ 	if (unlikely(pagecnt_bias == 1)) {
+ 		page_ref_add(page, USHRT_MAX - 1);
+ 		rx_buf->pagecnt_bias = USHRT_MAX;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ /**
++>>>>>>> 1d032bc77bb8 (ice: Gather the rx buf clean-up logic for better reuse)
   * ice_add_rx_frag - Add contents of Rx buffer to sk_buff
   * @rx_buf: buffer containing page to add
 + * @rx_desc: descriptor containing length of buffer written by hardware
   * @skb: sk_buf to place the data into
 - * @size: the length of the packet
   *
   * This function will add the data contained in rx_buf->page to the skb.
   * This is done either through a direct copy if the data in the buffer is
   * less than the skb header size, otherwise it will just attach the page as
   * a frag to the skb.
   *
-  * The function will then update the page offset if necessary and return
-  * true if the buffer can be reused by the adapter.
+  * The function will then update the page offset
   */
++<<<<<<< HEAD
 +static bool ice_add_rx_frag(struct ice_rx_buf *rx_buf,
 +			    union ice_32b_rx_flex_desc *rx_desc,
 +			    struct sk_buff *skb)
++=======
+ static void
+ ice_add_rx_frag(struct ice_rx_buf *rx_buf, struct sk_buff *skb,
+ 		unsigned int size)
++>>>>>>> 1d032bc77bb8 (ice: Gather the rx buf clean-up logic for better reuse)
  {
  #if (PAGE_SIZE < 8192)
  	unsigned int truesize = ICE_RXBUF_2048;
@@@ -536,48 -595,32 +608,58 @@@
  	/* will the data fit in the skb we allocated? if so, just
  	 * copy it as it is pretty small anyway
  	 */
 -	if (size <= ICE_RX_HDR_SIZE) {
 +	if (size <= ICE_RX_HDR_SIZE && !skb_is_nonlinear(skb)) {
 +		unsigned char *va = page_address(page) + rx_buf->page_offset;
 +
  		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
  
++<<<<<<< HEAD
 +		/* page is not reserved, we can reuse buffer as-is */
 +		if (likely(!ice_page_is_reserved(page)))
 +			return true;
 +
 +		/* this page cannot be reused so discard it */
 +		__free_pages(page, 0);
 +		return false;
++=======
+ 		rx_buf->pagecnt_bias++;
+ 		return;
++>>>>>>> 1d032bc77bb8 (ice: Gather the rx buf clean-up logic for better reuse)
  	}
  
 -	/* we need the header to contain the greater of either ETH_HLEN or
 -	 * 60 bytes if the skb->len is less than 60 for skb_pad.
 -	 */
 -	pull_len = eth_get_headlen(va, ICE_RX_HDR_SIZE);
 +	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
++<<<<<<< HEAD
 +			rx_buf->page_offset, size, truesize);
  
 -	/* align pull length to size of long to optimize memcpy performance */
 -	memcpy(__skb_put(skb, pull_len), va, ALIGN(pull_len, sizeof(long)));
 +	/* avoid re-using remote pages */
 +	if (unlikely(ice_page_is_reserved(page)))
 +		return false;
 +
 +#if (PAGE_SIZE < 8192)
 +	/* if we are only owner of page we can reuse it */
 +	if (unlikely(page_count(page) != 1))
 +		return false;
  
 -	/* the header from the frame that we're adding as a frag was added to
 -	 * linear part of skb so move the pointer past that header and
 -	 * reduce the size of data
 +	/* flip page offset to other buffer */
 +	rx_buf->page_offset ^= truesize;
 +#else
 +	/* move offset up to the next cache line */
 +	rx_buf->page_offset += truesize;
 +
 +	if (rx_buf->page_offset > last_offset)
 +		return false;
 +#endif /* PAGE_SIZE < 8192) */
 +
 +	/* Even if we own the page, we are not allowed to use atomic_set()
 +	 * This would break get_page_unless_zero() users.
  	 */
 -	va += pull_len;
 -	size -= pull_len;
 +	get_page(rx_buf->page);
  
 -add_tail_frag:
 -	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
 +	return true;
++=======
+ 			(unsigned long)va & ~PAGE_MASK, size, truesize);
+ 	ice_rx_buf_adjust_pg_offset(rx_buf, truesize);
++>>>>>>> 1d032bc77bb8 (ice: Gather the rx buf clean-up logic for better reuse)
  }
  
  /**
@@@ -662,8 -716,23 +745,27 @@@ static struct sk_buff *ice_fetch_rx_buf
  	}
  
  	/* pull page into skb */
++<<<<<<< HEAD
 +	if (ice_add_rx_frag(rx_buf, rx_desc, skb)) {
++=======
+ 	ice_add_rx_frag(rx_buf, skb, size);
+ 
+ 	return skb;
+ }
+ 
+ /**
+  * ice_put_rx_buf - Clean up used buffer and either recycle or free
+  * @rx_ring: Rx descriptor ring to transact packets on
+  * @rx_buf: Rx buffer to pull data from
+  *
+  * This function will  clean up the contents of the rx_buf. It will
+  * either recycle the buffer or unmap it and free the associated resources.
+  */
+ static void ice_put_rx_buf(struct ice_ring *rx_ring, struct ice_rx_buf *rx_buf)
+ {
++>>>>>>> 1d032bc77bb8 (ice: Gather the rx buf clean-up logic for better reuse)
  		/* hand second half of page back to the ring */
+ 	if (ice_can_reuse_rx_page(rx_buf)) {
  		ice_reuse_rx_page(rx_ring, rx_buf);
  		rx_ring->rx_stats.page_reuse_count++;
  	} else {
@@@ -674,48 -744,8 +776,46 @@@
  
  	/* clear contents of buffer_info */
  	rx_buf->page = NULL;
- 
- 	return skb;
  }
  
 +/**
 + * ice_pull_tail - ice specific version of skb_pull_tail
 + * @skb: pointer to current skb being adjusted
 + *
 + * This function is an ice specific version of __pskb_pull_tail. The
 + * main difference between this version and the original function is that
 + * this function can make several assumptions about the state of things
 + * that allow for significant optimizations versus the standard function.
 + * As a result we can do things like drop a frag and maintain an accurate
 + * truesize for the skb.
 + */
 +static void ice_pull_tail(struct sk_buff *skb)
 +{
 +	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
 +	unsigned int pull_len;
 +	unsigned char *va;
 +
 +	/* it is valid to use page_address instead of kmap since we are
 +	 * working with pages allocated out of the lomem pool per
 +	 * alloc_page(GFP_ATOMIC)
 +	 */
 +	va = skb_frag_address(frag);
 +
 +	/* we need the header to contain the greater of either ETH_HLEN or
 +	 * 60 bytes if the skb->len is less than 60 for skb_pad.
 +	 */
 +	pull_len = eth_get_headlen(va, ICE_RX_HDR_SIZE);
 +
 +	/* align pull length to size of long to optimize memcpy performance */
 +	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
 +
 +	/* update all of the pointers */
 +	skb_frag_size_sub(frag, pull_len);
 +	frag->page_offset += pull_len;
 +	skb->data_len -= pull_len;
 +	skb->tail += pull_len;
 +}
 +
  /**
   * ice_cleanup_headers - Correct empty headers
   * @skb: pointer to current skb being fixed
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx.c
