RDMA/hns: Initialize ib_device_ops struct

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Kamal Heib <kamalheib1@gmail.com>
commit 7f645a58d00155b93800fdd4e157c8f58fb3122f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/7f645a58.failed

Initialize ib_device_ops with the supported operations using
ib_set_device_ops().

	Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 7f645a58d00155b93800fdd4e157c8f58fb3122f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_device.h
#	drivers/infiniband/hw/hns/hns_roce_hw_v2.c
#	drivers/infiniband/hw/hns/hns_roce_main.c
diff --cc drivers/infiniband/hw/hns/hns_roce_device.h
index da994e7aed0e,67609cc6a45e..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@@ -793,6 -872,19 +793,22 @@@ struct hns_roce_hw 
  	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
  	int (*init_eq)(struct hns_roce_dev *hr_dev);
  	void (*cleanup_eq)(struct hns_roce_dev *hr_dev);
++<<<<<<< HEAD
++=======
+ 	void (*write_srqc)(struct hns_roce_dev *hr_dev,
+ 			   struct hns_roce_srq *srq, u32 pdn, u16 xrcd, u32 cqn,
+ 			   void *mb_buf, u64 *mtts_wqe, u64 *mtts_idx,
+ 			   dma_addr_t dma_handle_wqe,
+ 			   dma_addr_t dma_handle_idx);
+ 	int (*modify_srq)(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr,
+ 		       enum ib_srq_attr_mask srq_attr_mask,
+ 		       struct ib_udata *udata);
+ 	int (*query_srq)(struct ib_srq *ibsrq, struct ib_srq_attr *attr);
+ 	int (*post_srq_recv)(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
+ 			     const struct ib_recv_wr **bad_wr);
+ 	const struct ib_device_ops *hns_roce_dev_ops;
+ 	const struct ib_device_ops *hns_roce_dev_srq_ops;
++>>>>>>> 7f645a58d001 (RDMA/hns: Initialize ib_device_ops struct)
  };
  
  struct hns_roce_dev {
diff --cc drivers/infiniband/hw/hns/hns_roce_hw_v2.c
index dbf69dec706b,22236ebf033d..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
@@@ -5204,6 -5456,300 +5204,303 @@@ static void hns_roce_v2_cleanup_eq_tabl
  	destroy_workqueue(hr_dev->irq_workq);
  }
  
++<<<<<<< HEAD
++=======
+ static void hns_roce_v2_write_srqc(struct hns_roce_dev *hr_dev,
+ 				   struct hns_roce_srq *srq, u32 pdn, u16 xrcd,
+ 				   u32 cqn, void *mb_buf, u64 *mtts_wqe,
+ 				   u64 *mtts_idx, dma_addr_t dma_handle_wqe,
+ 				   dma_addr_t dma_handle_idx)
+ {
+ 	struct hns_roce_srq_context *srq_context;
+ 
+ 	srq_context = mb_buf;
+ 	memset(srq_context, 0, sizeof(*srq_context));
+ 
+ 	roce_set_field(srq_context->byte_4_srqn_srqst, SRQC_BYTE_4_SRQ_ST_M,
+ 		       SRQC_BYTE_4_SRQ_ST_S, 1);
+ 
+ 	roce_set_field(srq_context->byte_4_srqn_srqst,
+ 		       SRQC_BYTE_4_SRQ_WQE_HOP_NUM_M,
+ 		       SRQC_BYTE_4_SRQ_WQE_HOP_NUM_S,
+ 		       (hr_dev->caps.srqwqe_hop_num == HNS_ROCE_HOP_NUM_0 ? 0 :
+ 		       hr_dev->caps.srqwqe_hop_num));
+ 	roce_set_field(srq_context->byte_4_srqn_srqst,
+ 		       SRQC_BYTE_4_SRQ_SHIFT_M, SRQC_BYTE_4_SRQ_SHIFT_S,
+ 		       ilog2(srq->max));
+ 
+ 	roce_set_field(srq_context->byte_4_srqn_srqst, SRQC_BYTE_4_SRQN_M,
+ 		       SRQC_BYTE_4_SRQN_S, srq->srqn);
+ 
+ 	roce_set_field(srq_context->byte_8_limit_wl, SRQC_BYTE_8_SRQ_LIMIT_WL_M,
+ 		       SRQC_BYTE_8_SRQ_LIMIT_WL_S, 0);
+ 
+ 	roce_set_field(srq_context->byte_12_xrcd, SRQC_BYTE_12_SRQ_XRCD_M,
+ 		       SRQC_BYTE_12_SRQ_XRCD_S, xrcd);
+ 
+ 	srq_context->wqe_bt_ba = cpu_to_le32((u32)(dma_handle_wqe >> 3));
+ 
+ 	roce_set_field(srq_context->byte_24_wqe_bt_ba,
+ 		       SRQC_BYTE_24_SRQ_WQE_BT_BA_M,
+ 		       SRQC_BYTE_24_SRQ_WQE_BT_BA_S,
+ 		       cpu_to_le32(dma_handle_wqe >> 35));
+ 
+ 	roce_set_field(srq_context->byte_28_rqws_pd, SRQC_BYTE_28_PD_M,
+ 		       SRQC_BYTE_28_PD_S, pdn);
+ 	roce_set_field(srq_context->byte_28_rqws_pd, SRQC_BYTE_28_RQWS_M,
+ 		       SRQC_BYTE_28_RQWS_S, srq->max_gs <= 0 ? 0 :
+ 		       fls(srq->max_gs - 1));
+ 
+ 	srq_context->idx_bt_ba = (u32)(dma_handle_idx >> 3);
+ 	srq_context->idx_bt_ba = cpu_to_le32(srq_context->idx_bt_ba);
+ 	roce_set_field(srq_context->rsv_idx_bt_ba,
+ 		       SRQC_BYTE_36_SRQ_IDX_BT_BA_M,
+ 		       SRQC_BYTE_36_SRQ_IDX_BT_BA_S,
+ 		       cpu_to_le32(dma_handle_idx >> 35));
+ 
+ 	srq_context->idx_cur_blk_addr = (u32)(mtts_idx[0] >> PAGE_ADDR_SHIFT);
+ 	srq_context->idx_cur_blk_addr =
+ 				     cpu_to_le32(srq_context->idx_cur_blk_addr);
+ 	roce_set_field(srq_context->byte_44_idxbufpgsz_addr,
+ 		       SRQC_BYTE_44_SRQ_IDX_CUR_BLK_ADDR_M,
+ 		       SRQC_BYTE_44_SRQ_IDX_CUR_BLK_ADDR_S,
+ 		       cpu_to_le32((mtts_idx[0]) >> (32 + PAGE_ADDR_SHIFT)));
+ 	roce_set_field(srq_context->byte_44_idxbufpgsz_addr,
+ 		       SRQC_BYTE_44_SRQ_IDX_HOP_NUM_M,
+ 		       SRQC_BYTE_44_SRQ_IDX_HOP_NUM_S,
+ 		       hr_dev->caps.idx_hop_num == HNS_ROCE_HOP_NUM_0 ? 0 :
+ 		       hr_dev->caps.idx_hop_num);
+ 
+ 	roce_set_field(srq_context->byte_44_idxbufpgsz_addr,
+ 		       SRQC_BYTE_44_SRQ_IDX_BA_PG_SZ_M,
+ 		       SRQC_BYTE_44_SRQ_IDX_BA_PG_SZ_S,
+ 		       hr_dev->caps.idx_ba_pg_sz);
+ 	roce_set_field(srq_context->byte_44_idxbufpgsz_addr,
+ 		       SRQC_BYTE_44_SRQ_IDX_BUF_PG_SZ_M,
+ 		       SRQC_BYTE_44_SRQ_IDX_BUF_PG_SZ_S,
+ 		       hr_dev->caps.idx_buf_pg_sz);
+ 
+ 	srq_context->idx_nxt_blk_addr = (u32)(mtts_idx[1] >> PAGE_ADDR_SHIFT);
+ 	srq_context->idx_nxt_blk_addr =
+ 				   cpu_to_le32(srq_context->idx_nxt_blk_addr);
+ 	roce_set_field(srq_context->rsv_idxnxtblkaddr,
+ 		       SRQC_BYTE_52_SRQ_IDX_NXT_BLK_ADDR_M,
+ 		       SRQC_BYTE_52_SRQ_IDX_NXT_BLK_ADDR_S,
+ 		       cpu_to_le32((mtts_idx[1]) >> (32 + PAGE_ADDR_SHIFT)));
+ 	roce_set_field(srq_context->byte_56_xrc_cqn,
+ 		       SRQC_BYTE_56_SRQ_XRC_CQN_M, SRQC_BYTE_56_SRQ_XRC_CQN_S,
+ 		       cqn);
+ 	roce_set_field(srq_context->byte_56_xrc_cqn,
+ 		       SRQC_BYTE_56_SRQ_WQE_BA_PG_SZ_M,
+ 		       SRQC_BYTE_56_SRQ_WQE_BA_PG_SZ_S,
+ 		       hr_dev->caps.srqwqe_ba_pg_sz + PG_SHIFT_OFFSET);
+ 	roce_set_field(srq_context->byte_56_xrc_cqn,
+ 		       SRQC_BYTE_56_SRQ_WQE_BUF_PG_SZ_M,
+ 		       SRQC_BYTE_56_SRQ_WQE_BUF_PG_SZ_S,
+ 		       hr_dev->caps.srqwqe_buf_pg_sz + PG_SHIFT_OFFSET);
+ 
+ 	roce_set_bit(srq_context->db_record_addr_record_en,
+ 		     SRQC_BYTE_60_SRQ_RECORD_EN_S, 0);
+ }
+ 
+ static int hns_roce_v2_modify_srq(struct ib_srq *ibsrq,
+ 				  struct ib_srq_attr *srq_attr,
+ 				  enum ib_srq_attr_mask srq_attr_mask,
+ 				  struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);
+ 	struct hns_roce_srq *srq = to_hr_srq(ibsrq);
+ 	struct hns_roce_srq_context *srq_context;
+ 	struct hns_roce_srq_context *srqc_mask;
+ 	struct hns_roce_cmd_mailbox *mailbox;
+ 	int ret;
+ 
+ 	if (srq_attr_mask & IB_SRQ_LIMIT) {
+ 		if (srq_attr->srq_limit >= srq->max)
+ 			return -EINVAL;
+ 
+ 		mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 		if (IS_ERR(mailbox))
+ 			return PTR_ERR(mailbox);
+ 
+ 		srq_context = mailbox->buf;
+ 		srqc_mask = (struct hns_roce_srq_context *)mailbox->buf + 1;
+ 
+ 		memset(srqc_mask, 0xff, sizeof(*srqc_mask));
+ 
+ 		roce_set_field(srq_context->byte_8_limit_wl,
+ 			       SRQC_BYTE_8_SRQ_LIMIT_WL_M,
+ 			       SRQC_BYTE_8_SRQ_LIMIT_WL_S, srq_attr->srq_limit);
+ 		roce_set_field(srqc_mask->byte_8_limit_wl,
+ 			       SRQC_BYTE_8_SRQ_LIMIT_WL_M,
+ 			       SRQC_BYTE_8_SRQ_LIMIT_WL_S, 0);
+ 
+ 		ret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0, srq->srqn, 0,
+ 					HNS_ROCE_CMD_MODIFY_SRQC,
+ 					HNS_ROCE_CMD_TIMEOUT_MSECS);
+ 		hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 		if (ret) {
+ 			dev_err(hr_dev->dev,
+ 				"MODIFY SRQ Failed to cmd mailbox.\n");
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int hns_roce_v2_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);
+ 	struct hns_roce_srq *srq = to_hr_srq(ibsrq);
+ 	struct hns_roce_srq_context *srq_context;
+ 	struct hns_roce_cmd_mailbox *mailbox;
+ 	int limit_wl;
+ 	int ret;
+ 
+ 	mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 	if (IS_ERR(mailbox))
+ 		return PTR_ERR(mailbox);
+ 
+ 	srq_context = mailbox->buf;
+ 	ret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, srq->srqn, 0,
+ 				HNS_ROCE_CMD_QUERY_SRQC,
+ 				HNS_ROCE_CMD_TIMEOUT_MSECS);
+ 	if (ret) {
+ 		dev_err(hr_dev->dev, "QUERY SRQ cmd process error\n");
+ 		goto out;
+ 	}
+ 
+ 	limit_wl = roce_get_field(srq_context->byte_8_limit_wl,
+ 				  SRQC_BYTE_8_SRQ_LIMIT_WL_M,
+ 				  SRQC_BYTE_8_SRQ_LIMIT_WL_S);
+ 
+ 	attr->srq_limit = limit_wl;
+ 	attr->max_wr    = srq->max - 1;
+ 	attr->max_sge   = srq->max_gs;
+ 
+ 	memcpy(srq_context, mailbox->buf, sizeof(*srq_context));
+ 
+ out:
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 	return ret;
+ }
+ 
+ static int find_empty_entry(struct hns_roce_idx_que *idx_que)
+ {
+ 	int bit_num;
+ 	int i;
+ 
+ 	/* bitmap[i] is set zero if all bits are allocated */
+ 	for (i = 0; idx_que->bitmap[i] == 0; ++i)
+ 		;
+ 	bit_num = ffs(idx_que->bitmap[i]);
+ 	idx_que->bitmap[i] &= ~(1ULL << (bit_num - 1));
+ 
+ 	return i * sizeof(u64) * 8 + (bit_num - 1);
+ }
+ 
+ static void fill_idx_queue(struct hns_roce_idx_que *idx_que,
+ 			   int cur_idx, int wqe_idx)
+ {
+ 	unsigned int *addr;
+ 
+ 	addr = (unsigned int *)hns_roce_buf_offset(&idx_que->idx_buf,
+ 						   cur_idx * idx_que->entry_sz);
+ 	*addr = wqe_idx;
+ }
+ 
+ static int hns_roce_v2_post_srq_recv(struct ib_srq *ibsrq,
+ 				     const struct ib_recv_wr *wr,
+ 				     const struct ib_recv_wr **bad_wr)
+ {
+ 	struct hns_roce_srq *srq = to_hr_srq(ibsrq);
+ 	struct hns_roce_v2_wqe_data_seg *dseg;
+ 	struct hns_roce_v2_db srq_db;
+ 	unsigned long flags;
+ 	int ret = 0;
+ 	int wqe_idx;
+ 	void *wqe;
+ 	int nreq;
+ 	int ind;
+ 	int i;
+ 
+ 	spin_lock_irqsave(&srq->lock, flags);
+ 
+ 	ind = srq->head & (srq->max - 1);
+ 
+ 	for (nreq = 0; wr; ++nreq, wr = wr->next) {
+ 		if (unlikely(wr->num_sge > srq->max_gs)) {
+ 			ret = -EINVAL;
+ 			*bad_wr = wr;
+ 			break;
+ 		}
+ 
+ 		if (unlikely(srq->head == srq->tail)) {
+ 			ret = -ENOMEM;
+ 			*bad_wr = wr;
+ 			break;
+ 		}
+ 
+ 		wqe_idx = find_empty_entry(&srq->idx_que);
+ 		fill_idx_queue(&srq->idx_que, ind, wqe_idx);
+ 		wqe = get_srq_wqe(srq, wqe_idx);
+ 		dseg = (struct hns_roce_v2_wqe_data_seg *)wqe;
+ 
+ 		for (i = 0; i < wr->num_sge; ++i) {
+ 			dseg[i].len = cpu_to_le32(wr->sg_list[i].length);
+ 			dseg[i].lkey = cpu_to_le32(wr->sg_list[i].lkey);
+ 			dseg[i].addr = cpu_to_le64(wr->sg_list[i].addr);
+ 		}
+ 
+ 		if (i < srq->max_gs) {
+ 			dseg->len = 0;
+ 			dseg->lkey = cpu_to_le32(0x100);
+ 			dseg->addr = 0;
+ 		}
+ 
+ 		srq->wrid[wqe_idx] = wr->wr_id;
+ 		ind = (ind + 1) & (srq->max - 1);
+ 	}
+ 
+ 	if (likely(nreq)) {
+ 		srq->head += nreq;
+ 
+ 		/*
+ 		 * Make sure that descriptors are written before
+ 		 * doorbell record.
+ 		 */
+ 		wmb();
+ 
+ 		srq_db.byte_4 = HNS_ROCE_V2_SRQ_DB << 24 | srq->srqn;
+ 		srq_db.parameter = srq->head;
+ 
+ 		hns_roce_write64_k((__le32 *)&srq_db, srq->db_reg_l);
+ 
+ 	}
+ 
+ 	spin_unlock_irqrestore(&srq->lock, flags);
+ 
+ 	return ret;
+ }
+ 
+ static const struct ib_device_ops hns_roce_v2_dev_ops = {
+ 	.destroy_qp = hns_roce_v2_destroy_qp,
+ 	.modify_cq = hns_roce_v2_modify_cq,
+ 	.poll_cq = hns_roce_v2_poll_cq,
+ 	.post_recv = hns_roce_v2_post_recv,
+ 	.post_send = hns_roce_v2_post_send,
+ 	.query_qp = hns_roce_v2_query_qp,
+ 	.req_notify_cq = hns_roce_v2_req_notify_cq,
+ };
+ 
+ static const struct ib_device_ops hns_roce_v2_dev_srq_ops = {
+ 	.modify_srq = hns_roce_v2_modify_srq,
+ 	.post_srq_recv = hns_roce_v2_post_srq_recv,
+ 	.query_srq = hns_roce_v2_query_srq,
+ };
+ 
++>>>>>>> 7f645a58d001 (RDMA/hns: Initialize ib_device_ops struct)
  static const struct hns_roce_hw hns_roce_hw_v2 = {
  	.cmq_init = hns_roce_v2_cmq_init,
  	.cmq_exit = hns_roce_v2_cmq_exit,
@@@ -5229,6 -5777,12 +5526,15 @@@
  	.poll_cq = hns_roce_v2_poll_cq,
  	.init_eq = hns_roce_v2_init_eq_table,
  	.cleanup_eq = hns_roce_v2_cleanup_eq_table,
++<<<<<<< HEAD
++=======
+ 	.write_srqc = hns_roce_v2_write_srqc,
+ 	.modify_srq = hns_roce_v2_modify_srq,
+ 	.query_srq = hns_roce_v2_query_srq,
+ 	.post_srq_recv = hns_roce_v2_post_srq_recv,
+ 	.hns_roce_dev_ops = &hns_roce_v2_dev_ops,
+ 	.hns_roce_dev_srq_ops = &hns_roce_v2_dev_srq_ops,
++>>>>>>> 7f645a58d001 (RDMA/hns: Initialize ib_device_ops struct)
  };
  
  static const struct pci_device_id hns_roce_hw_v2_pci_tbl[] = {
diff --cc drivers/infiniband/hw/hns/hns_roce_main.c
index bb35475fc0ff,c79054ba9495..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_main.c
+++ b/drivers/infiniband/hw/hns/hns_roce_main.c
@@@ -538,59 -532,39 +586,48 @@@ static int hns_roce_register_device(str
  	ib_dev->uverbs_ex_cmd_mask |=
  		(1ULL << IB_USER_VERBS_EX_CMD_MODIFY_CQ);
  
- 	/* HCA||device||port */
- 	ib_dev->modify_device		= hns_roce_modify_device;
- 	ib_dev->query_device		= hns_roce_query_device;
- 	ib_dev->query_port		= hns_roce_query_port;
- 	ib_dev->modify_port		= hns_roce_modify_port;
- 	ib_dev->get_link_layer		= hns_roce_get_link_layer;
- 	ib_dev->get_netdev		= hns_roce_get_netdev;
- 	ib_dev->add_gid			= hns_roce_add_gid;
- 	ib_dev->del_gid			= hns_roce_del_gid;
- 	ib_dev->query_pkey		= hns_roce_query_pkey;
- 	ib_dev->alloc_ucontext		= hns_roce_alloc_ucontext;
- 	ib_dev->dealloc_ucontext	= hns_roce_dealloc_ucontext;
- 	ib_dev->mmap			= hns_roce_mmap;
- 
- 	/* PD */
- 	ib_dev->alloc_pd		= hns_roce_alloc_pd;
- 	ib_dev->dealloc_pd		= hns_roce_dealloc_pd;
- 
- 	/* AH */
- 	ib_dev->create_ah		= hns_roce_create_ah;
- 	ib_dev->query_ah		= hns_roce_query_ah;
- 	ib_dev->destroy_ah		= hns_roce_destroy_ah;
- 
- 	/* QP */
- 	ib_dev->create_qp		= hns_roce_create_qp;
- 	ib_dev->modify_qp		= hns_roce_modify_qp;
- 	ib_dev->query_qp		= hr_dev->hw->query_qp;
- 	ib_dev->destroy_qp		= hr_dev->hw->destroy_qp;
- 	ib_dev->post_send		= hr_dev->hw->post_send;
- 	ib_dev->post_recv		= hr_dev->hw->post_recv;
- 
- 	/* CQ */
- 	ib_dev->create_cq		= hns_roce_ib_create_cq;
- 	ib_dev->modify_cq		= hr_dev->hw->modify_cq;
- 	ib_dev->destroy_cq		= hns_roce_ib_destroy_cq;
- 	ib_dev->req_notify_cq		= hr_dev->hw->req_notify_cq;
- 	ib_dev->poll_cq			= hr_dev->hw->poll_cq;
- 
- 	/* MR */
- 	ib_dev->get_dma_mr		= hns_roce_get_dma_mr;
- 	ib_dev->reg_user_mr		= hns_roce_reg_user_mr;
- 	ib_dev->dereg_mr		= hns_roce_dereg_mr;
  	if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_REREG_MR) {
- 		ib_dev->rereg_user_mr	= hns_roce_rereg_user_mr;
  		ib_dev->uverbs_cmd_mask |= (1ULL << IB_USER_VERBS_CMD_REREG_MR);
+ 		ib_set_device_ops(ib_dev, &hns_roce_dev_mr_ops);
  	}
  
++<<<<<<< HEAD
 +	/* OTHERS */
 +	ib_dev->get_port_immutable	= hns_roce_port_immutable;
 +	ib_dev->disassociate_ucontext	= hns_roce_disassociate_ucontext;
 +
 +	ib_dev->driver_id = RDMA_DRIVER_HNS;
 +	ret = ib_register_device(ib_dev, NULL);
++=======
+ 	/* MW */
+ 	if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_MW) {
+ 		ib_dev->uverbs_cmd_mask |=
+ 					(1ULL << IB_USER_VERBS_CMD_ALLOC_MW) |
+ 					(1ULL << IB_USER_VERBS_CMD_DEALLOC_MW);
+ 		ib_set_device_ops(ib_dev, &hns_roce_dev_mw_ops);
+ 	}
+ 
+ 	/* FRMR */
+ 	if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_FRMR)
+ 		ib_set_device_ops(ib_dev, &hns_roce_dev_frmr_ops);
+ 
+ 	/* SRQ */
+ 	if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_SRQ) {
+ 		ib_dev->uverbs_cmd_mask |=
+ 				(1ULL << IB_USER_VERBS_CMD_CREATE_SRQ) |
+ 				(1ULL << IB_USER_VERBS_CMD_MODIFY_SRQ) |
+ 				(1ULL << IB_USER_VERBS_CMD_QUERY_SRQ) |
+ 				(1ULL << IB_USER_VERBS_CMD_DESTROY_SRQ) |
+ 				(1ULL << IB_USER_VERBS_CMD_POST_SRQ_RECV);
+ 		ib_set_device_ops(ib_dev, &hns_roce_dev_srq_ops);
+ 		ib_set_device_ops(ib_dev, hr_dev->hw->hns_roce_dev_srq_ops);
+ 	}
+ 
+ 	ib_dev->driver_id = RDMA_DRIVER_HNS;
+ 	ib_set_device_ops(ib_dev, hr_dev->hw->hns_roce_dev_ops);
+ 	ib_set_device_ops(ib_dev, &hns_roce_dev_ops);
+ 	ret = ib_register_device(ib_dev, "hns_%d", NULL);
++>>>>>>> 7f645a58d001 (RDMA/hns: Initialize ib_device_ops struct)
  	if (ret) {
  		dev_err(dev, "ib_register_device failed!\n");
  		return ret;
* Unmerged path drivers/infiniband/hw/hns/hns_roce_device.h
diff --git a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
index df0e14579c57..b6ac80a566f7 100644
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
@@ -4789,6 +4789,16 @@ static void hns_roce_v1_cleanup_eq_table(struct hns_roce_dev *hr_dev)
 	kfree(eq_table->eq);
 }
 
+static const struct ib_device_ops hns_roce_v1_dev_ops = {
+	.destroy_qp = hns_roce_v1_destroy_qp,
+	.modify_cq = hns_roce_v1_modify_cq,
+	.poll_cq = hns_roce_v1_poll_cq,
+	.post_recv = hns_roce_v1_post_recv,
+	.post_send = hns_roce_v1_post_send,
+	.query_qp = hns_roce_v1_query_qp,
+	.req_notify_cq = hns_roce_v1_req_notify_cq,
+};
+
 static const struct hns_roce_hw hns_roce_hw_v1 = {
 	.reset = hns_roce_v1_reset,
 	.hw_profile = hns_roce_v1_profile,
@@ -4814,6 +4824,7 @@ static const struct hns_roce_hw hns_roce_hw_v1 = {
 	.destroy_cq = hns_roce_v1_destroy_cq,
 	.init_eq = hns_roce_v1_init_eq_table,
 	.cleanup_eq = hns_roce_v1_cleanup_eq_table,
+	.hns_roce_dev_ops = &hns_roce_v1_dev_ops,
 };
 
 static const struct of_device_id hns_roce_of_match[] = {
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v2.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_main.c
