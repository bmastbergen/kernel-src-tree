bpf: Support socket lookup in CGROUP_SOCK_ADDR progs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Andrey Ignatov <rdna@fb.com>
commit 6c49e65e0d462963b4fac97ebd87014342167027
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/6c49e65e.failed

Make bpf_sk_lookup_tcp, bpf_sk_lookup_udp and bpf_sk_release helpers
available in programs of type BPF_PROG_TYPE_CGROUP_SOCK_ADDR.

Such programs operate on sockets and have access to socket and struct
sockaddr passed by user to system calls such as sys_bind, sys_connect,
sys_sendmsg.

It's useful to be able to lookup other sockets from these programs.
E.g. sys_connect may lookup IP:port endpoint and if there is a server
socket bound to that endpoint ("server" can be defined by saddr & sport
being zero), redirect client connection to it by rewriting IP:port in
sockaddr passed to sys_connect.

	Signed-off-by: Andrey Ignatov <rdna@fb.com>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Martin KaFai Lau <kafai@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit 6c49e65e0d462963b4fac97ebd87014342167027)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/filter.c
diff --cc net/core/filter.c
index ed8de8b22015,f6ca38a7d433..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -4757,6 -4843,244 +4757,247 @@@ static const struct bpf_func_proto bpf_
  };
  #endif /* CONFIG_IPV6_SEG6_BPF */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INET
+ static struct sock *sk_lookup(struct net *net, struct bpf_sock_tuple *tuple,
+ 			      int dif, int sdif, u8 family, u8 proto)
+ {
+ 	bool refcounted = false;
+ 	struct sock *sk = NULL;
+ 
+ 	if (family == AF_INET) {
+ 		__be32 src4 = tuple->ipv4.saddr;
+ 		__be32 dst4 = tuple->ipv4.daddr;
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					   src4, tuple->ipv4.sport,
+ 					   dst4, tuple->ipv4.dport,
+ 					   dif, sdif, &refcounted);
+ 		else
+ 			sk = __udp4_lib_lookup(net, src4, tuple->ipv4.sport,
+ 					       dst4, tuple->ipv4.dport,
+ 					       dif, sdif, &udp_table, NULL);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	} else {
+ 		struct in6_addr *src6 = (struct in6_addr *)&tuple->ipv6.saddr;
+ 		struct in6_addr *dst6 = (struct in6_addr *)&tuple->ipv6.daddr;
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet6_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					    src6, tuple->ipv6.sport,
+ 					    dst6, ntohs(tuple->ipv6.dport),
+ 					    dif, sdif, &refcounted);
+ 		else if (likely(ipv6_bpf_stub))
+ 			sk = ipv6_bpf_stub->udp6_lib_lookup(net,
+ 							    src6, tuple->ipv6.sport,
+ 							    dst6, tuple->ipv6.dport,
+ 							    dif, sdif,
+ 							    &udp_table, NULL);
+ #endif
+ 	}
+ 
+ 	if (unlikely(sk && !refcounted && !sock_flag(sk, SOCK_RCU_FREE))) {
+ 		WARN_ONCE(1, "Found non-RCU, unreferenced socket!");
+ 		sk = NULL;
+ 	}
+ 	return sk;
+ }
+ 
+ /* bpf_sk_lookup performs the core lookup for different types of sockets,
+  * taking a reference on the socket if it doesn't have the flag SOCK_RCU_FREE.
+  * Returns the socket as an 'unsigned long' to simplify the casting in the
+  * callers to satisfy BPF_CALL declarations.
+  */
+ static unsigned long
+ __bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 		struct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,
+ 		u64 flags)
+ {
+ 	struct sock *sk = NULL;
+ 	u8 family = AF_UNSPEC;
+ 	struct net *net;
+ 	int sdif;
+ 
+ 	family = len == sizeof(tuple->ipv4) ? AF_INET : AF_INET6;
+ 	if (unlikely(family == AF_UNSPEC || netns_id > U32_MAX || flags))
+ 		goto out;
+ 
+ 	if (family == AF_INET)
+ 		sdif = inet_sdif(skb);
+ 	else
+ 		sdif = inet6_sdif(skb);
+ 
+ 	if (netns_id) {
+ 		net = get_net_ns_by_id(caller_net, netns_id);
+ 		if (unlikely(!net))
+ 			goto out;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 		put_net(net);
+ 	} else {
+ 		net = caller_net;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 	}
+ 
+ 	if (sk)
+ 		sk = sk_to_full_sk(sk);
+ out:
+ 	return (unsigned long) sk;
+ }
+ 
+ static unsigned long
+ bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 	      u8 proto, u64 netns_id, u64 flags)
+ {
+ 	struct net *caller_net;
+ 	int ifindex;
+ 
+ 	if (skb->dev) {
+ 		caller_net = dev_net(skb->dev);
+ 		ifindex = skb->dev->ifindex;
+ 	} else {
+ 		caller_net = sock_net(skb->sk);
+ 		ifindex = 0;
+ 	}
+ 
+ 	return __bpf_sk_lookup(skb, tuple, len, caller_net, ifindex,
+ 			      proto, netns_id, flags);
+ }
+ 
+ BPF_CALL_5(bpf_sk_lookup_tcp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sk_lookup_udp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_udp_proto = {
+ 	.func		= bpf_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_1(bpf_sk_release, struct sock *, sk)
+ {
+ 	if (!sock_flag(sk, SOCK_RCU_FREE))
+ 		sock_gen_put(sk);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_release_proto = {
+ 	.func		= bpf_sk_release,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_SOCKET,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_udp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return __bpf_sk_lookup(NULL, tuple, len, caller_net, ifindex,
+ 			      IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_udp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_udp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_tcp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return __bpf_sk_lookup(NULL, tuple, len, caller_net, ifindex,
+ 			      IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_tcp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_tcp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sock_addr_sk_lookup_tcp, struct bpf_sock_addr_kern *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return __bpf_sk_lookup(NULL, tuple, len, sock_net(ctx->sk), 0,
+ 			       IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sock_addr_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sock_addr_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sock_addr_sk_lookup_udp, struct bpf_sock_addr_kern *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return __bpf_sk_lookup(NULL, tuple, len, sock_net(ctx->sk), 0,
+ 			       IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sock_addr_sk_lookup_udp_proto = {
+ 	.func		= bpf_sock_addr_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ #endif /* CONFIG_INET */
+ 
++>>>>>>> 6c49e65e0d46 (bpf: Support socket lookup in CGROUP_SOCK_ADDR progs)
  bool bpf_helper_changes_pkt_data(void *func)
  {
  	if (func == bpf_skb_vlan_push ||
* Unmerged path net/core/filter.c
