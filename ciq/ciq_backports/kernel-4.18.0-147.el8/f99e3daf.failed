KVM: x86: Add Intel PT virtualization work mode

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Chao Peng <chao.p.peng@linux.intel.com>
commit f99e3daf94ff35dd4a878d32ff66e1fd35223ad6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f99e3daf.failed

Intel Processor Trace virtualization can be work in one
of 2 possible modes:

a. System-Wide mode (default):
   When the host configures Intel PT to collect trace packets
   of the entire system, it can leave the relevant VMX controls
   clear to allow VMX-specific packets to provide information
   across VMX transitions.
   KVM guest will not aware this feature in this mode and both
   host and KVM guest trace will output to host buffer.

b. Host-Guest mode:
   Host can configure trace-packet generation while in
   VMX non-root operation for guests and root operation
   for native executing normally.
   Intel PT will be exposed to KVM guest in this mode, and
   the trace output to respective buffer of host and guest.
   In this mode, tht status of PT will be saved and disabled
   before VM-entry and restored after VM-exit if trace
   a virtual machine.

	Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
	Signed-off-by: Luwei Kang <luwei.kang@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f99e3daf94ff35dd4a878d32ff66e1fd35223ad6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/capabilities.h
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/vmx/vmx.h
diff --cc arch/x86/kvm/vmx/vmx.c
index d8e69f24edee,338977e6f552..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -191,14 -169,9 +191,20 @@@ module_param(ple_window_shrink, uint, 0
  static unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
  module_param(ple_window_max, uint, 0444);
  
++<<<<<<< HEAD
 +extern const ulong vmx_return;
 +extern const ulong vmx_early_consistency_check_return;
 +
 +enum ept_pointers_status {
 +	EPT_POINTERS_CHECK = 0,
 +	EPT_POINTERS_MATCH = 1,
 +	EPT_POINTERS_MISMATCH = 2
 +};
++=======
+ /* Default is SYSTEM mode, 1 for host-guest mode */
+ int __read_mostly pt_mode = PT_MODE_SYSTEM;
+ module_param(pt_mode, int, S_IRUGO);
++>>>>>>> f99e3daf94ff (KVM: x86: Add Intel PT virtualization work mode)
  
  static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
  static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
@@@ -2625,5186 -1484,1205 +2631,5194 @@@ static bool msr_write_intercepted(struc
  }
  
  /*
 - * Reads an msr value (of 'msr_index') into 'pdata'.
 - * Returns 0 on success, non-0 otherwise.
 - * Assumes vcpu_load() was already called.
 + * Check if MSR is intercepted for L01 MSR bitmap.
   */
 -static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +static bool msr_write_intercepted_l01(struct kvm_vcpu *vcpu, u32 msr)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct shared_msr_entry *msr;
 +	unsigned long *msr_bitmap;
 +	int f = sizeof(unsigned long);
  
 -	switch (msr_info->index) {
 -#ifdef CONFIG_X86_64
 -	case MSR_FS_BASE:
 -		msr_info->data = vmcs_readl(GUEST_FS_BASE);
 -		break;
 -	case MSR_GS_BASE:
 -		msr_info->data = vmcs_readl(GUEST_GS_BASE);
 -		break;
 -	case MSR_KERNEL_GS_BASE:
 -		msr_info->data = vmx_read_guest_kernel_gs_base(vmx);
 -		break;
 -#endif
 -	case MSR_EFER:
 -		return kvm_get_msr_common(vcpu, msr_info);
 -	case MSR_IA32_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 -			return 1;
 +	if (!cpu_has_vmx_msr_bitmap())
 +		return true;
  
 -		msr_info->data = to_vmx(vcpu)->spec_ctrl;
 -		break;
 -	case MSR_IA32_ARCH_CAPABILITIES:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))
 -			return 1;
 -		msr_info->data = to_vmx(vcpu)->arch_capabilities;
 -		break;
 -	case MSR_IA32_SYSENTER_CS:
 -		msr_info->data = vmcs_read32(GUEST_SYSENTER_CS);
 -		break;
 -	case MSR_IA32_SYSENTER_EIP:
 -		msr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);
 -		break;
 -	case MSR_IA32_SYSENTER_ESP:
 -		msr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);
 -		break;
 -	case MSR_IA32_BNDCFGS:
 -		if (!kvm_mpx_supported() ||
 -		    (!msr_info->host_initiated &&
 -		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))
 -			return 1;
 -		msr_info->data = vmcs_read64(GUEST_BNDCFGS);
 -		break;
 -	case MSR_IA32_MCG_EXT_CTL:
 -		if (!msr_info->host_initiated &&
 -		    !(vmx->msr_ia32_feature_control &
 -		      FEATURE_CONTROL_LMCE))
 -			return 1;
 -		msr_info->data = vcpu->arch.mcg_ext_ctl;
 -		break;
 -	case MSR_IA32_FEATURE_CONTROL:
 -		msr_info->data = vmx->msr_ia32_feature_control;
 -		break;
 -	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 -		if (!nested_vmx_allowed(vcpu))
 -			return 1;
 -		return vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,
 -				       &msr_info->data);
 -	case MSR_IA32_XSS:
 -		if (!vmx_xsaves_supported())
 -			return 1;
 -		msr_info->data = vcpu->arch.ia32_xss;
 -		break;
 -	case MSR_TSC_AUX:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))
 -			return 1;
 -		/* Otherwise falls through */
 -	default:
 -		msr = find_msr_entry(vmx, msr_info->index);
 -		if (msr) {
 -			msr_info->data = msr->data;
 -			break;
 -		}
 -		return kvm_get_msr_common(vcpu, msr_info);
 +	msr_bitmap = to_vmx(vcpu)->vmcs01.msr_bitmap;
 +
 +	if (msr <= 0x1fff) {
 +		return !!test_bit(msr, msr_bitmap + 0x800 / f);
 +	} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {
 +		msr &= 0x1fff;
 +		return !!test_bit(msr, msr_bitmap + 0xc00 / f);
  	}
  
 -	return 0;
 +	return true;
  }
  
 -/*
 - * Writes msr value into into the appropriate "register".
 - * Returns 0 on success, non-0 otherwise.
 - * Assumes vcpu_load() was already called.
 - */
 -static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +static void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 +		unsigned long entry, unsigned long exit)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct shared_msr_entry *msr;
 -	int ret = 0;
 -	u32 msr_index = msr_info->index;
 -	u64 data = msr_info->data;
 +	vm_entry_controls_clearbit(vmx, entry);
 +	vm_exit_controls_clearbit(vmx, exit);
 +}
  
 -	switch (msr_index) {
 +static int find_msr(struct vmx_msrs *m, unsigned int msr)
 +{
 +	unsigned int i;
 +
 +	for (i = 0; i < m->nr; ++i) {
 +		if (m->val[i].index == msr)
 +			return i;
 +	}
 +	return -ENOENT;
 +}
 +
 +static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 +{
 +	int i;
 +	struct msr_autoload *m = &vmx->msr_autoload;
 +
 +	switch (msr) {
  	case MSR_EFER:
 -		ret = kvm_set_msr_common(vcpu, msr_info);
 -		break;
 -#ifdef CONFIG_X86_64
 -	case MSR_FS_BASE:
 -		vmx_segment_cache_clear(vmx);
 -		vmcs_writel(GUEST_FS_BASE, data);
 -		break;
 -	case MSR_GS_BASE:
 -		vmx_segment_cache_clear(vmx);
 -		vmcs_writel(GUEST_GS_BASE, data);
 -		break;
 -	case MSR_KERNEL_GS_BASE:
 -		vmx_write_guest_kernel_gs_base(vmx, data);
 -		break;
 -#endif
 -	case MSR_IA32_SYSENTER_CS:
 -		vmcs_write32(GUEST_SYSENTER_CS, data);
 +		if (cpu_has_load_ia32_efer()) {
 +			clear_atomic_switch_msr_special(vmx,
 +					VM_ENTRY_LOAD_IA32_EFER,
 +					VM_EXIT_LOAD_IA32_EFER);
 +			return;
 +		}
  		break;
 -	case MSR_IA32_SYSENTER_EIP:
 -		vmcs_writel(GUEST_SYSENTER_EIP, data);
 +	case MSR_CORE_PERF_GLOBAL_CTRL:
 +		if (cpu_has_load_perf_global_ctrl()) {
 +			clear_atomic_switch_msr_special(vmx,
 +					VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
 +					VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
 +			return;
 +		}
  		break;
 -	case MSR_IA32_SYSENTER_ESP:
 -		vmcs_writel(GUEST_SYSENTER_ESP, data);
 +	}
 +	i = find_msr(&m->guest, msr);
 +	if (i < 0)
 +		goto skip_guest;
 +	--m->guest.nr;
 +	m->guest.val[i] = m->guest.val[m->guest.nr];
 +	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
 +
 +skip_guest:
 +	i = find_msr(&m->host, msr);
 +	if (i < 0)
 +		return;
 +
 +	--m->host.nr;
 +	m->host.val[i] = m->host.val[m->host.nr];
 +	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
 +}
 +
 +static void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 +		unsigned long entry, unsigned long exit,
 +		unsigned long guest_val_vmcs, unsigned long host_val_vmcs,
 +		u64 guest_val, u64 host_val)
 +{
 +	vmcs_write64(guest_val_vmcs, guest_val);
 +	if (host_val_vmcs != HOST_IA32_EFER)
 +		vmcs_write64(host_val_vmcs, host_val);
 +	vm_entry_controls_setbit(vmx, entry);
 +	vm_exit_controls_setbit(vmx, exit);
 +}
 +
 +static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 +				  u64 guest_val, u64 host_val, bool entry_only)
 +{
 +	int i, j = 0;
 +	struct msr_autoload *m = &vmx->msr_autoload;
 +
 +	switch (msr) {
 +	case MSR_EFER:
 +		if (cpu_has_load_ia32_efer()) {
 +			add_atomic_switch_msr_special(vmx,
 +					VM_ENTRY_LOAD_IA32_EFER,
 +					VM_EXIT_LOAD_IA32_EFER,
 +					GUEST_IA32_EFER,
 +					HOST_IA32_EFER,
 +					guest_val, host_val);
 +			return;
 +		}
  		break;
 -	case MSR_IA32_BNDCFGS:
 -		if (!kvm_mpx_supported() ||
 -		    (!msr_info->host_initiated &&
 -		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))
 -			return 1;
 -		if (is_noncanonical_address(data & PAGE_MASK, vcpu) ||
 -		    (data & MSR_IA32_BNDCFGS_RSVD))
 -			return 1;
 -		vmcs_write64(GUEST_BNDCFGS, data);
 +	case MSR_CORE_PERF_GLOBAL_CTRL:
 +		if (cpu_has_load_perf_global_ctrl()) {
 +			add_atomic_switch_msr_special(vmx,
 +					VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
 +					VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,
 +					GUEST_IA32_PERF_GLOBAL_CTRL,
 +					HOST_IA32_PERF_GLOBAL_CTRL,
 +					guest_val, host_val);
 +			return;
 +		}
  		break;
 -	case MSR_IA32_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 -			return 1;
 +	case MSR_IA32_PEBS_ENABLE:
 +		/* PEBS needs a quiescent period after being disabled (to write
 +		 * a record).  Disabling PEBS through VMX MSR swapping doesn't
 +		 * provide that period, so a CPU could write host's record into
 +		 * guest's memory.
 +		 */
 +		wrmsrl(MSR_IA32_PEBS_ENABLE, 0);
 +	}
  
 -		/* The STIBP bit doesn't fault even if it's not advertised */
 -		if (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD))
 -			return 1;
 +	i = find_msr(&m->guest, msr);
 +	if (!entry_only)
 +		j = find_msr(&m->host, msr);
  
 -		vmx->spec_ctrl = data;
 +	if (i == NR_AUTOLOAD_MSRS || j == NR_AUTOLOAD_MSRS) {
 +		printk_once(KERN_WARNING "Not enough msr switch entries. "
 +				"Can't add msr %x\n", msr);
 +		return;
 +	}
 +	if (i < 0) {
 +		i = m->guest.nr++;
 +		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
 +	}
 +	m->guest.val[i].index = msr;
 +	m->guest.val[i].value = guest_val;
  
 -		if (!data)
 -			break;
 +	if (entry_only)
 +		return;
 +
 +	if (j < 0) {
 +		j = m->host.nr++;
 +		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
 +	}
 +	m->host.val[j].index = msr;
 +	m->host.val[j].value = host_val;
 +}
 +
 +static bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)
 +{
 +	u64 guest_efer = vmx->vcpu.arch.efer;
 +	u64 ignore_bits = 0;
  
 +	if (!enable_ept) {
  		/*
 -		 * For non-nested:
 -		 * When it's written (to non-zero) for the first time, pass
 -		 * it through.
 -		 *
 -		 * For nested:
 -		 * The handling of the MSR bitmap for L2 guests is done in
 -		 * nested_vmx_merge_msr_bitmap. We should not touch the
 -		 * vmcs02.msr_bitmap here since it gets completely overwritten
 -		 * in the merging. We update the vmcs01 here for L1 as well
 -		 * since it will end up touching the MSR anyway now.
 +		 * NX is needed to handle CR0.WP=1, CR4.SMEP=1.  Testing
 +		 * host CPUID is more efficient than testing guest CPUID
 +		 * or CR4.  Host SMEP is anyway a requirement for guest SMEP.
  		 */
 -		vmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap,
 -					      MSR_IA32_SPEC_CTRL,
 -					      MSR_TYPE_RW);
 -		break;
 -	case MSR_IA32_PRED_CMD:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 -			return 1;
 +		if (boot_cpu_has(X86_FEATURE_SMEP))
 +			guest_efer |= EFER_NX;
 +		else if (!(guest_efer & EFER_NX))
 +			ignore_bits |= EFER_NX;
 +	}
 +
 +	/*
 +	 * LMA and LME handled by hardware; SCE meaningless outside long mode.
 +	 */
 +	ignore_bits |= EFER_SCE;
 +#ifdef CONFIG_X86_64
 +	ignore_bits |= EFER_LMA | EFER_LME;
 +	/* SCE is meaningful only in long mode on Intel */
 +	if (guest_efer & EFER_LMA)
 +		ignore_bits &= ~(u64)EFER_SCE;
 +#endif
 +
 +	/*
 +	 * On EPT, we can't emulate NX, so we must switch EFER atomically.
 +	 * On CPUs that support "load IA32_EFER", always switch EFER
 +	 * atomically, since it's faster than switching it manually.
 +	 */
 +	if (cpu_has_load_ia32_efer() ||
 +	    (enable_ept && ((vmx->vcpu.arch.efer ^ host_efer) & EFER_NX))) {
 +		if (!(guest_efer & EFER_LMA))
 +			guest_efer &= ~EFER_LME;
 +		if (guest_efer != host_efer)
 +			add_atomic_switch_msr(vmx, MSR_EFER,
 +					      guest_efer, host_efer, false);
 +		else
 +			clear_atomic_switch_msr(vmx, MSR_EFER);
 +		return false;
 +	} else {
 +		clear_atomic_switch_msr(vmx, MSR_EFER);
 +
 +		guest_efer &= ~ignore_bits;
 +		guest_efer |= host_efer & ignore_bits;
 +
 +		vmx->guest_msrs[efer_offset].data = guest_efer;
 +		vmx->guest_msrs[efer_offset].mask = ~ignore_bits;
 +
 +		return true;
 +	}
 +}
 +
 +#ifdef CONFIG_X86_32
 +/*
 + * On 32-bit kernels, VM exits still load the FS and GS bases from the
 + * VMCS rather than the segment table.  KVM uses this helper to figure
 + * out the current bases to poke them into the VMCS before entry.
 + */
 +static unsigned long segment_base(u16 selector)
 +{
 +	struct desc_struct *table;
 +	unsigned long v;
 +
 +	if (!(selector & ~SEGMENT_RPL_MASK))
 +		return 0;
 +
 +	table = get_current_gdt_ro();
 +
 +	if ((selector & SEGMENT_TI_MASK) == SEGMENT_LDT) {
 +		u16 ldt_selector = kvm_read_ldt();
 +
 +		if (!(ldt_selector & ~SEGMENT_RPL_MASK))
 +			return 0;
 +
 +		table = (struct desc_struct *)segment_base(ldt_selector);
 +	}
 +	v = get_desc_base(&table[selector >> 3]);
 +	return v;
 +}
 +#endif
 +
 +static void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct vmcs_host_state *host_state;
 +#ifdef CONFIG_X86_64
 +	int cpu = raw_smp_processor_id();
 +#endif
 +	unsigned long fs_base, gs_base;
 +	u16 fs_sel, gs_sel;
 +	int i;
 +
 +	vmx->req_immediate_exit = false;
 +
 +	/*
 +	 * Note that guest MSRs to be saved/restored can also be changed
 +	 * when guest state is loaded. This happens when guest transitions
 +	 * to/from long-mode by setting MSR_EFER.LMA.
 +	 */
 +	if (!vmx->loaded_cpu_state || vmx->guest_msrs_dirty) {
 +		vmx->guest_msrs_dirty = false;
 +		for (i = 0; i < vmx->save_nmsrs; ++i)
 +			kvm_set_shared_msr(vmx->guest_msrs[i].index,
 +					   vmx->guest_msrs[i].data,
 +					   vmx->guest_msrs[i].mask);
 +
 +	}
 +
 +	if (vmx->loaded_cpu_state)
 +		return;
 +
 +	vmx->loaded_cpu_state = vmx->loaded_vmcs;
 +	host_state = &vmx->loaded_cpu_state->host_state;
 +
 +	/*
 +	 * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not
 +	 * allow segment selectors with cpl > 0 or ti == 1.
 +	 */
 +	host_state->ldt_sel = kvm_read_ldt();
 +
 +#ifdef CONFIG_X86_64
 +	savesegment(ds, host_state->ds_sel);
 +	savesegment(es, host_state->es_sel);
 +
 +	gs_base = cpu_kernelmode_gs_base(cpu);
 +	if (likely(is_64bit_mm(current->mm))) {
 +		save_fsgs_for_kvm();
 +		fs_sel = current->thread.fsindex;
 +		gs_sel = current->thread.gsindex;
 +		fs_base = current->thread.fsbase;
 +		vmx->msr_host_kernel_gs_base = current->thread.gsbase;
 +	} else {
 +		savesegment(fs, fs_sel);
 +		savesegment(gs, gs_sel);
 +		fs_base = read_msr(MSR_FS_BASE);
 +		vmx->msr_host_kernel_gs_base = read_msr(MSR_KERNEL_GS_BASE);
 +	}
 +
 +	wrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);
 +#else
 +	savesegment(fs, fs_sel);
 +	savesegment(gs, gs_sel);
 +	fs_base = segment_base(fs_sel);
 +	gs_base = segment_base(gs_sel);
 +#endif
 +
 +	if (unlikely(fs_sel != host_state->fs_sel)) {
 +		if (!(fs_sel & 7))
 +			vmcs_write16(HOST_FS_SELECTOR, fs_sel);
 +		else
 +			vmcs_write16(HOST_FS_SELECTOR, 0);
 +		host_state->fs_sel = fs_sel;
 +	}
 +	if (unlikely(gs_sel != host_state->gs_sel)) {
 +		if (!(gs_sel & 7))
 +			vmcs_write16(HOST_GS_SELECTOR, gs_sel);
 +		else
 +			vmcs_write16(HOST_GS_SELECTOR, 0);
 +		host_state->gs_sel = gs_sel;
 +	}
 +	if (unlikely(fs_base != host_state->fs_base)) {
 +		vmcs_writel(HOST_FS_BASE, fs_base);
 +		host_state->fs_base = fs_base;
 +	}
 +	if (unlikely(gs_base != host_state->gs_base)) {
 +		vmcs_writel(HOST_GS_BASE, gs_base);
 +		host_state->gs_base = gs_base;
 +	}
 +}
 +
 +static void vmx_prepare_switch_to_host(struct vcpu_vmx *vmx)
 +{
 +	struct vmcs_host_state *host_state;
 +
 +	if (!vmx->loaded_cpu_state)
 +		return;
 +
 +	WARN_ON_ONCE(vmx->loaded_cpu_state != vmx->loaded_vmcs);
 +	host_state = &vmx->loaded_cpu_state->host_state;
 +
 +	++vmx->vcpu.stat.host_state_reload;
 +	vmx->loaded_cpu_state = NULL;
 +
 +#ifdef CONFIG_X86_64
 +	rdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);
 +#endif
 +	if (host_state->ldt_sel || (host_state->gs_sel & 7)) {
 +		kvm_load_ldt(host_state->ldt_sel);
 +#ifdef CONFIG_X86_64
 +		load_gs_index(host_state->gs_sel);
 +#else
 +		loadsegment(gs, host_state->gs_sel);
 +#endif
 +	}
 +	if (host_state->fs_sel & 7)
 +		loadsegment(fs, host_state->fs_sel);
 +#ifdef CONFIG_X86_64
 +	if (unlikely(host_state->ds_sel | host_state->es_sel)) {
 +		loadsegment(ds, host_state->ds_sel);
 +		loadsegment(es, host_state->es_sel);
 +	}
 +#endif
 +	invalidate_tss_limit();
 +#ifdef CONFIG_X86_64
 +	wrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);
 +#endif
 +	load_fixmap_gdt(raw_smp_processor_id());
 +}
 +
 +#ifdef CONFIG_X86_64
 +static u64 vmx_read_guest_kernel_gs_base(struct vcpu_vmx *vmx)
 +{
 +	preempt_disable();
 +	if (vmx->loaded_cpu_state)
 +		rdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);
 +	preempt_enable();
 +	return vmx->msr_guest_kernel_gs_base;
 +}
 +
 +static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
 +{
 +	preempt_disable();
 +	if (vmx->loaded_cpu_state)
 +		wrmsrl(MSR_KERNEL_GS_BASE, data);
 +	preempt_enable();
 +	vmx->msr_guest_kernel_gs_base = data;
 +}
 +#endif
 +
 +static void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 +{
 +	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
 +	struct pi_desc old, new;
 +	unsigned int dest;
 +
 +	/*
 +	 * In case of hot-plug or hot-unplug, we may have to undo
 +	 * vmx_vcpu_pi_put even if there is no assigned device.  And we
 +	 * always keep PI.NDST up to date for simplicity: it makes the
 +	 * code easier, and CPU migration is not a fast path.
 +	 */
 +	if (!pi_test_sn(pi_desc) && vcpu->cpu == cpu)
 +		return;
 +
 +	/*
 +	 * First handle the simple case where no cmpxchg is necessary; just
 +	 * allow posting non-urgent interrupts.
 +	 *
 +	 * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change
 +	 * PI.NDST: pi_post_block will do it for us and the wakeup_handler
 +	 * expects the VCPU to be on the blocked_vcpu_list that matches
 +	 * PI.NDST.
 +	 */
 +	if (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR ||
 +	    vcpu->cpu == cpu) {
 +		pi_clear_sn(pi_desc);
 +		return;
 +	}
 +
 +	/* The full case.  */
 +	do {
 +		old.control = new.control = pi_desc->control;
 +
 +		dest = cpu_physical_id(cpu);
 +
 +		if (x2apic_enabled())
 +			new.ndst = dest;
 +		else
 +			new.ndst = (dest << 8) & 0xFF00;
 +
 +		new.sn = 0;
 +	} while (cmpxchg64(&pi_desc->control, old.control,
 +			   new.control) != old.control);
 +}
 +
 +static void decache_tsc_multiplier(struct vcpu_vmx *vmx)
 +{
 +	vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
 +	vmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);
 +}
 +
 +/*
 + * Switches to specified vcpu, until a matching vcpu_put(), but assumes
 + * vcpu mutex is already taken.
 + */
 +static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	bool already_loaded = vmx->loaded_vmcs->cpu == cpu;
 +
 +	if (!already_loaded) {
 +		loaded_vmcs_clear(vmx->loaded_vmcs);
 +		local_irq_disable();
 +		crash_disable_local_vmclear(cpu);
 +
 +		/*
 +		 * Read loaded_vmcs->cpu should be before fetching
 +		 * loaded_vmcs->loaded_vmcss_on_cpu_link.
 +		 * See the comments in __loaded_vmcs_clear().
 +		 */
 +		smp_rmb();
 +
 +		list_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,
 +			 &per_cpu(loaded_vmcss_on_cpu, cpu));
 +		crash_enable_local_vmclear(cpu);
 +		local_irq_enable();
 +	}
 +
 +	if (per_cpu(current_vmcs, cpu) != vmx->loaded_vmcs->vmcs) {
 +		per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;
 +		vmcs_load(vmx->loaded_vmcs->vmcs);
 +		indirect_branch_prediction_barrier();
 +	}
 +
 +	if (!already_loaded) {
 +		void *gdt = get_current_gdt_ro();
 +		unsigned long sysenter_esp;
 +
 +		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +
 +		/*
 +		 * Linux uses per-cpu TSS and GDT, so set these when switching
 +		 * processors.  See 22.2.4.
 +		 */
 +		vmcs_writel(HOST_TR_BASE,
 +			    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);
 +		vmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */
 +
 +		/*
 +		 * VM exits change the host TR limit to 0x67 after a VM
 +		 * exit.  This is okay, since 0x67 covers everything except
 +		 * the IO bitmap and have have code to handle the IO bitmap
 +		 * being lost after a VM exit.
 +		 */
 +		BUILD_BUG_ON(IO_BITMAP_OFFSET - 1 != 0x67);
 +
 +		rdmsrl(MSR_IA32_SYSENTER_ESP, sysenter_esp);
 +		vmcs_writel(HOST_IA32_SYSENTER_ESP, sysenter_esp); /* 22.2.3 */
 +
 +		vmx->loaded_vmcs->cpu = cpu;
 +	}
 +
 +	/* Setup TSC multiplier */
 +	if (kvm_has_tsc_control &&
 +	    vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
 +		decache_tsc_multiplier(vmx);
 +
 +	vmx_vcpu_pi_load(vcpu, cpu);
 +	vmx->host_pkru = read_pkru();
 +	vmx->host_debugctlmsr = get_debugctlmsr();
 +}
 +
 +static void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 +{
 +	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
 +
 +	if (!kvm_arch_has_assigned_device(vcpu->kvm) ||
 +		!irq_remapping_cap(IRQ_POSTING_CAP)  ||
 +		!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	/* Set SN when the vCPU is preempted */
 +	if (vcpu->preempted)
 +		pi_set_sn(pi_desc);
 +}
 +
 +static void vmx_vcpu_put(struct kvm_vcpu *vcpu)
 +{
 +	vmx_vcpu_pi_put(vcpu);
 +
 +	vmx_prepare_switch_to_host(to_vmx(vcpu));
 +}
 +
 +static bool emulation_required(struct kvm_vcpu *vcpu)
 +{
 +	return emulate_invalid_guest_state && !guest_state_valid(vcpu);
 +}
 +
 +static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);
 +
 +/*
 + * Return the cr0 value that a nested guest would read. This is a combination
 + * of the real cr0 used to run the guest (guest_cr0), and the bits shadowed by
 + * its hypervisor (cr0_read_shadow).
 + */
 +static inline unsigned long nested_read_cr0(struct vmcs12 *fields)
 +{
 +	return (fields->guest_cr0 & ~fields->cr0_guest_host_mask) |
 +		(fields->cr0_read_shadow & fields->cr0_guest_host_mask);
 +}
 +static inline unsigned long nested_read_cr4(struct vmcs12 *fields)
 +{
 +	return (fields->guest_cr4 & ~fields->cr4_guest_host_mask) |
 +		(fields->cr4_read_shadow & fields->cr4_guest_host_mask);
 +}
 +
 +static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long rflags, save_rflags;
 +
 +	if (!test_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail)) {
 +		__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);
 +		rflags = vmcs_readl(GUEST_RFLAGS);
 +		if (to_vmx(vcpu)->rmode.vm86_active) {
 +			rflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;
 +			save_rflags = to_vmx(vcpu)->rmode.save_rflags;
 +			rflags |= save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;
 +		}
 +		to_vmx(vcpu)->rflags = rflags;
 +	}
 +	return to_vmx(vcpu)->rflags;
 +}
 +
 +static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 +{
 +	unsigned long old_rflags = vmx_get_rflags(vcpu);
 +
 +	__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);
 +	to_vmx(vcpu)->rflags = rflags;
 +	if (to_vmx(vcpu)->rmode.vm86_active) {
 +		to_vmx(vcpu)->rmode.save_rflags = rflags;
 +		rflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;
 +	}
 +	vmcs_writel(GUEST_RFLAGS, rflags);
 +
 +	if ((old_rflags ^ to_vmx(vcpu)->rflags) & X86_EFLAGS_VM)
 +		to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
 +}
 +
 +static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 +{
 +	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
 +	int ret = 0;
 +
 +	if (interruptibility & GUEST_INTR_STATE_STI)
 +		ret |= KVM_X86_SHADOW_INT_STI;
 +	if (interruptibility & GUEST_INTR_STATE_MOV_SS)
 +		ret |= KVM_X86_SHADOW_INT_MOV_SS;
 +
 +	return ret;
 +}
 +
 +static void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 +{
 +	u32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
 +	u32 interruptibility = interruptibility_old;
 +
 +	interruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);
 +
 +	if (mask & KVM_X86_SHADOW_INT_MOV_SS)
 +		interruptibility |= GUEST_INTR_STATE_MOV_SS;
 +	else if (mask & KVM_X86_SHADOW_INT_STI)
 +		interruptibility |= GUEST_INTR_STATE_STI;
 +
 +	if ((interruptibility != interruptibility_old))
 +		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);
 +}
 +
 +static void skip_emulated_instruction(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long rip;
 +
 +	rip = kvm_rip_read(vcpu);
 +	rip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 +	kvm_rip_write(vcpu, rip);
 +
 +	/* skipping an emulated instruction also counts */
 +	vmx_set_interrupt_shadow(vcpu, 0);
 +}
 +
 +static void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu,
 +					       unsigned long exit_qual)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +	unsigned int nr = vcpu->arch.exception.nr;
 +	u32 intr_info = nr | INTR_INFO_VALID_MASK;
 +
 +	if (vcpu->arch.exception.has_error_code) {
 +		vmcs12->vm_exit_intr_error_code = vcpu->arch.exception.error_code;
 +		intr_info |= INTR_INFO_DELIVER_CODE_MASK;
 +	}
 +
 +	if (kvm_exception_is_soft(nr))
 +		intr_info |= INTR_TYPE_SOFT_EXCEPTION;
 +	else
 +		intr_info |= INTR_TYPE_HARD_EXCEPTION;
 +
 +	if (!(vmcs12->idt_vectoring_info_field & VECTORING_INFO_VALID_MASK) &&
 +	    vmx_get_nmi_mask(vcpu))
 +		intr_info |= INTR_INFO_UNBLOCK_NMI;
 +
 +	nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
 +}
 +
 +/*
 + * KVM wants to inject page-faults which it got to the guest. This function
 + * checks whether in a nested guest, we need to inject them to L1 or L2.
 + */
 +static int nested_vmx_check_exception(struct kvm_vcpu *vcpu, unsigned long *exit_qual)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +	unsigned int nr = vcpu->arch.exception.nr;
 +	bool has_payload = vcpu->arch.exception.has_payload;
 +	unsigned long payload = vcpu->arch.exception.payload;
 +
 +	if (nr == PF_VECTOR) {
 +		if (vcpu->arch.exception.nested_apf) {
 +			*exit_qual = vcpu->arch.apf.nested_apf_token;
 +			return 1;
 +		}
 +		if (nested_vmx_is_page_fault_vmexit(vmcs12,
 +						    vcpu->arch.exception.error_code)) {
 +			*exit_qual = has_payload ? payload : vcpu->arch.cr2;
 +			return 1;
 +		}
 +	} else if (vmcs12->exception_bitmap & (1u << nr)) {
 +		if (nr == DB_VECTOR) {
 +			if (!has_payload) {
 +				payload = vcpu->arch.dr6;
 +				payload &= ~(DR6_FIXED_1 | DR6_BT);
 +				payload ^= DR6_RTM;
 +			}
 +			*exit_qual = payload;
 +		} else
 +			*exit_qual = 0;
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 +{
 +	/*
 +	 * Ensure that we clear the HLT state in the VMCS.  We don't need to
 +	 * explicitly skip the instruction because if the HLT state is set,
 +	 * then the instruction is already executing and RIP has already been
 +	 * advanced.
 +	 */
 +	if (kvm_hlt_in_guest(vcpu->kvm) &&
 +			vmcs_read32(GUEST_ACTIVITY_STATE) == GUEST_ACTIVITY_HLT)
 +		vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);
 +}
 +
 +static void vmx_queue_exception(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned nr = vcpu->arch.exception.nr;
 +	bool has_error_code = vcpu->arch.exception.has_error_code;
 +	u32 error_code = vcpu->arch.exception.error_code;
 +	u32 intr_info = nr | INTR_INFO_VALID_MASK;
 +
 +	kvm_deliver_exception_payload(vcpu);
 +
 +	if (has_error_code) {
 +		vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);
 +		intr_info |= INTR_INFO_DELIVER_CODE_MASK;
 +	}
 +
 +	if (vmx->rmode.vm86_active) {
 +		int inc_eip = 0;
 +		if (kvm_exception_is_soft(nr))
 +			inc_eip = vcpu->arch.event_exit_inst_len;
 +		if (kvm_inject_realmode_interrupt(vcpu, nr, inc_eip) != EMULATE_DONE)
 +			kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 +		return;
 +	}
 +
 +	WARN_ON_ONCE(vmx->emulation_required);
 +
 +	if (kvm_exception_is_soft(nr)) {
 +		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,
 +			     vmx->vcpu.arch.event_exit_inst_len);
 +		intr_info |= INTR_TYPE_SOFT_EXCEPTION;
 +	} else
 +		intr_info |= INTR_TYPE_HARD_EXCEPTION;
 +
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);
 +
 +	vmx_clear_hlt(vcpu);
 +}
 +
 +static bool vmx_rdtscp_supported(void)
 +{
 +	return cpu_has_vmx_rdtscp();
 +}
 +
 +static bool vmx_invpcid_supported(void)
 +{
 +	return cpu_has_vmx_invpcid();
 +}
 +
 +/*
 + * Swap MSR entry in host/guest MSR entry array.
 + */
 +static void move_msr_up(struct vcpu_vmx *vmx, int from, int to)
 +{
 +	struct shared_msr_entry tmp;
 +
 +	tmp = vmx->guest_msrs[to];
 +	vmx->guest_msrs[to] = vmx->guest_msrs[from];
 +	vmx->guest_msrs[from] = tmp;
 +}
 +
 +/*
 + * Set up the vmcs to automatically save and restore system
 + * msrs.  Don't touch the 64-bit msrs if the guest is in legacy
 + * mode, as fiddling with msrs is very expensive.
 + */
 +static void setup_msrs(struct vcpu_vmx *vmx)
 +{
 +	int save_nmsrs, index;
 +
 +	save_nmsrs = 0;
 +#ifdef CONFIG_X86_64
 +	/*
 +	 * The SYSCALL MSRs are only needed on long mode guests, and only
 +	 * when EFER.SCE is set.
 +	 */
 +	if (is_long_mode(&vmx->vcpu) && (vmx->vcpu.arch.efer & EFER_SCE)) {
 +		index = __find_msr_index(vmx, MSR_STAR);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +		index = __find_msr_index(vmx, MSR_LSTAR);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +		index = __find_msr_index(vmx, MSR_SYSCALL_MASK);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +	}
 +#endif
 +	index = __find_msr_index(vmx, MSR_EFER);
 +	if (index >= 0 && update_transition_efer(vmx, index))
 +		move_msr_up(vmx, index, save_nmsrs++);
 +	index = __find_msr_index(vmx, MSR_TSC_AUX);
 +	if (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))
 +		move_msr_up(vmx, index, save_nmsrs++);
 +
 +	vmx->save_nmsrs = save_nmsrs;
 +	vmx->guest_msrs_dirty = true;
 +
 +	if (cpu_has_vmx_msr_bitmap())
 +		vmx_update_msr_bitmap(&vmx->vcpu);
 +}
 +
 +static u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +
 +	if (is_guest_mode(vcpu) &&
 +	    (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING))
 +		return vcpu->arch.tsc_offset - vmcs12->tsc_offset;
 +
 +	return vcpu->arch.tsc_offset;
 +}
 +
 +static u64 vmx_write_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +	u64 g_tsc_offset = 0;
 +
 +	/*
 +	 * We're here if L1 chose not to trap WRMSR to TSC. According
 +	 * to the spec, this should set L1's TSC; The offset that L1
 +	 * set for L2 remains unchanged, and still needs to be added
 +	 * to the newly set TSC to get L2's TSC.
 +	 */
 +	if (is_guest_mode(vcpu) &&
 +	    (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING))
 +		g_tsc_offset = vmcs12->tsc_offset;
 +
 +	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
 +				   vcpu->arch.tsc_offset - g_tsc_offset,
 +				   offset);
 +	vmcs_write64(TSC_OFFSET, offset + g_tsc_offset);
 +	return offset + g_tsc_offset;
 +}
 +
 +/*
 + * nested_vmx_allowed() checks whether a guest should be allowed to use VMX
 + * instructions and MSRs (i.e., nested VMX). Nested VMX is disabled for
 + * all guests if the "nested" module option is off, and can also be disabled
 + * for a single guest by disabling its VMX cpuid bit.
 + */
 +static inline bool nested_vmx_allowed(struct kvm_vcpu *vcpu)
 +{
 +	return nested && guest_cpuid_has(vcpu, X86_FEATURE_VMX);
 +}
 +
 +/*
 + * nested_vmx_setup_ctls_msrs() sets up variables containing the values to be
 + * returned for the various VMX controls MSRs when nested VMX is enabled.
 + * The same values should also be used to verify that vmcs12 control fields are
 + * valid during nested entry from L1 to L2.
 + * Each of these control msrs has a low and high 32-bit half: A low bit is on
 + * if the corresponding bit in the (32-bit) control field *must* be on, and a
 + * bit in the high half is on if the corresponding bit in the control field
 + * may be on. See also vmx_control_verify().
 + */
 +static void nested_vmx_setup_ctls_msrs(struct nested_vmx_msrs *msrs,
 +				       u32 ept_caps, bool apicv)
 +{
 +	if (!nested) {
 +		memset(msrs, 0, sizeof(*msrs));
 +		return;
 +	}
 +
 +	/*
 +	 * Note that as a general rule, the high half of the MSRs (bits in
 +	 * the control fields which may be 1) should be initialized by the
 +	 * intersection of the underlying hardware's MSR (i.e., features which
 +	 * can be supported) and the list of features we want to expose -
 +	 * because they are known to be properly supported in our code.
 +	 * Also, usually, the low half of the MSRs (bits which must be 1) can
 +	 * be set to 0, meaning that L1 may turn off any of these bits. The
 +	 * reason is that if one of these bits is necessary, it will appear
 +	 * in vmcs01 and prepare_vmcs02, when it bitwise-or's the control
 +	 * fields of vmcs01 and vmcs02, will turn these bits off - and
 +	 * nested_vmx_exit_reflected() will not pass related exits to L1.
 +	 * These rules have exceptions below.
 +	 */
 +
 +	/* pin-based controls */
 +	rdmsr(MSR_IA32_VMX_PINBASED_CTLS,
 +		msrs->pinbased_ctls_low,
 +		msrs->pinbased_ctls_high);
 +	msrs->pinbased_ctls_low |=
 +		PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 +	msrs->pinbased_ctls_high &=
 +		PIN_BASED_EXT_INTR_MASK |
 +		PIN_BASED_NMI_EXITING |
 +		PIN_BASED_VIRTUAL_NMIS |
 +		(apicv ? PIN_BASED_POSTED_INTR : 0);
 +	msrs->pinbased_ctls_high |=
 +		PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |
 +		PIN_BASED_VMX_PREEMPTION_TIMER;
 +
 +	/* exit controls */
 +	rdmsr(MSR_IA32_VMX_EXIT_CTLS,
 +		msrs->exit_ctls_low,
 +		msrs->exit_ctls_high);
 +	msrs->exit_ctls_low =
 +		VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;
 +
 +	msrs->exit_ctls_high &=
 +#ifdef CONFIG_X86_64
 +		VM_EXIT_HOST_ADDR_SPACE_SIZE |
 +#endif
 +		VM_EXIT_LOAD_IA32_PAT | VM_EXIT_SAVE_IA32_PAT;
 +	msrs->exit_ctls_high |=
 +		VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR |
 +		VM_EXIT_LOAD_IA32_EFER | VM_EXIT_SAVE_IA32_EFER |
 +		VM_EXIT_SAVE_VMX_PREEMPTION_TIMER | VM_EXIT_ACK_INTR_ON_EXIT;
 +
 +	/* We support free control of debug control saving. */
 +	msrs->exit_ctls_low &= ~VM_EXIT_SAVE_DEBUG_CONTROLS;
 +
 +	/* entry controls */
 +	rdmsr(MSR_IA32_VMX_ENTRY_CTLS,
 +		msrs->entry_ctls_low,
 +		msrs->entry_ctls_high);
 +	msrs->entry_ctls_low =
 +		VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;
 +	msrs->entry_ctls_high &=
 +#ifdef CONFIG_X86_64
 +		VM_ENTRY_IA32E_MODE |
 +#endif
 +		VM_ENTRY_LOAD_IA32_PAT;
 +	msrs->entry_ctls_high |=
 +		(VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR | VM_ENTRY_LOAD_IA32_EFER);
 +
 +	/* We support free control of debug control loading. */
 +	msrs->entry_ctls_low &= ~VM_ENTRY_LOAD_DEBUG_CONTROLS;
 +
 +	/* cpu-based controls */
 +	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS,
 +		msrs->procbased_ctls_low,
 +		msrs->procbased_ctls_high);
 +	msrs->procbased_ctls_low =
 +		CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 +	msrs->procbased_ctls_high &=
 +		CPU_BASED_VIRTUAL_INTR_PENDING |
 +		CPU_BASED_VIRTUAL_NMI_PENDING | CPU_BASED_USE_TSC_OFFSETING |
 +		CPU_BASED_HLT_EXITING | CPU_BASED_INVLPG_EXITING |
 +		CPU_BASED_MWAIT_EXITING | CPU_BASED_CR3_LOAD_EXITING |
 +		CPU_BASED_CR3_STORE_EXITING |
 +#ifdef CONFIG_X86_64
 +		CPU_BASED_CR8_LOAD_EXITING | CPU_BASED_CR8_STORE_EXITING |
 +#endif
 +		CPU_BASED_MOV_DR_EXITING | CPU_BASED_UNCOND_IO_EXITING |
 +		CPU_BASED_USE_IO_BITMAPS | CPU_BASED_MONITOR_TRAP_FLAG |
 +		CPU_BASED_MONITOR_EXITING | CPU_BASED_RDPMC_EXITING |
 +		CPU_BASED_RDTSC_EXITING | CPU_BASED_PAUSE_EXITING |
 +		CPU_BASED_TPR_SHADOW | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
 +	/*
 +	 * We can allow some features even when not supported by the
 +	 * hardware. For example, L1 can specify an MSR bitmap - and we
 +	 * can use it to avoid exits to L1 - even when L0 runs L2
 +	 * without MSR bitmaps.
 +	 */
 +	msrs->procbased_ctls_high |=
 +		CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR |
 +		CPU_BASED_USE_MSR_BITMAPS;
 +
 +	/* We support free control of CR3 access interception. */
 +	msrs->procbased_ctls_low &=
 +		~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);
 +
 +	/*
 +	 * secondary cpu-based controls.  Do not include those that
 +	 * depend on CPUID bits, they are added later by vmx_cpuid_update.
 +	 */
 +	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,
 +		msrs->secondary_ctls_low,
 +		msrs->secondary_ctls_high);
 +	msrs->secondary_ctls_low = 0;
 +	msrs->secondary_ctls_high &=
 +		SECONDARY_EXEC_DESC |
 +		SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 +		SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
 +		SECONDARY_EXEC_WBINVD_EXITING;
 +
 +	/*
 +	 * We can emulate "VMCS shadowing," even if the hardware
 +	 * doesn't support it.
 +	 */
 +	msrs->secondary_ctls_high |=
 +		SECONDARY_EXEC_SHADOW_VMCS;
 +
 +	if (enable_ept) {
 +		/* nested EPT: emulate EPT also to L1 */
 +		msrs->secondary_ctls_high |=
 +			SECONDARY_EXEC_ENABLE_EPT;
 +		msrs->ept_caps = VMX_EPT_PAGE_WALK_4_BIT |
 +			 VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT;
 +		if (cpu_has_vmx_ept_execute_only())
 +			msrs->ept_caps |=
 +				VMX_EPT_EXECUTE_ONLY_BIT;
 +		msrs->ept_caps &= ept_caps;
 +		msrs->ept_caps |= VMX_EPT_EXTENT_GLOBAL_BIT |
 +			VMX_EPT_EXTENT_CONTEXT_BIT | VMX_EPT_2MB_PAGE_BIT |
 +			VMX_EPT_1GB_PAGE_BIT;
 +		if (enable_ept_ad_bits) {
 +			msrs->secondary_ctls_high |=
 +				SECONDARY_EXEC_ENABLE_PML;
 +			msrs->ept_caps |= VMX_EPT_AD_BIT;
 +		}
 +	}
 +
 +	if (cpu_has_vmx_vmfunc()) {
 +		msrs->secondary_ctls_high |=
 +			SECONDARY_EXEC_ENABLE_VMFUNC;
 +		/*
 +		 * Advertise EPTP switching unconditionally
 +		 * since we emulate it
 +		 */
 +		if (enable_ept)
 +			msrs->vmfunc_controls =
 +				VMX_VMFUNC_EPTP_SWITCHING;
 +	}
 +
 +	/*
 +	 * Old versions of KVM use the single-context version without
 +	 * checking for support, so declare that it is supported even
 +	 * though it is treated as global context.  The alternative is
 +	 * not failing the single-context invvpid, and it is worse.
 +	 */
 +	if (enable_vpid) {
 +		msrs->secondary_ctls_high |=
 +			SECONDARY_EXEC_ENABLE_VPID;
 +		msrs->vpid_caps = VMX_VPID_INVVPID_BIT |
 +			VMX_VPID_EXTENT_SUPPORTED_MASK;
 +	}
 +
 +	if (enable_unrestricted_guest)
 +		msrs->secondary_ctls_high |=
 +			SECONDARY_EXEC_UNRESTRICTED_GUEST;
 +
 +	if (flexpriority_enabled)
 +		msrs->secondary_ctls_high |=
 +			SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +
 +	/* miscellaneous data */
 +	rdmsr(MSR_IA32_VMX_MISC,
 +		msrs->misc_low,
 +		msrs->misc_high);
 +	msrs->misc_low &= VMX_MISC_SAVE_EFER_LMA;
 +	msrs->misc_low |=
 +		MSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS |
 +		VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE |
 +		VMX_MISC_ACTIVITY_HLT;
 +	msrs->misc_high = 0;
 +
 +	/*
 +	 * This MSR reports some information about VMX support. We
 +	 * should return information about the VMX we emulate for the
 +	 * guest, and the VMCS structure we give it - not about the
 +	 * VMX support of the underlying hardware.
 +	 */
 +	msrs->basic =
 +		VMCS12_REVISION |
 +		VMX_BASIC_TRUE_CTLS |
 +		((u64)VMCS12_SIZE << VMX_BASIC_VMCS_SIZE_SHIFT) |
 +		(VMX_BASIC_MEM_TYPE_WB << VMX_BASIC_MEM_TYPE_SHIFT);
 +
 +	if (cpu_has_vmx_basic_inout())
 +		msrs->basic |= VMX_BASIC_INOUT;
 +
 +	/*
 +	 * These MSRs specify bits which the guest must keep fixed on
 +	 * while L1 is in VMXON mode (in L1's root mode, or running an L2).
 +	 * We picked the standard core2 setting.
 +	 */
 +#define VMXON_CR0_ALWAYSON     (X86_CR0_PE | X86_CR0_PG | X86_CR0_NE)
 +#define VMXON_CR4_ALWAYSON     X86_CR4_VMXE
 +	msrs->cr0_fixed0 = VMXON_CR0_ALWAYSON;
 +	msrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;
 +
 +	/* These MSRs specify bits which the guest must keep fixed off. */
 +	rdmsrl(MSR_IA32_VMX_CR0_FIXED1, msrs->cr0_fixed1);
 +	rdmsrl(MSR_IA32_VMX_CR4_FIXED1, msrs->cr4_fixed1);
 +
 +	/* highest index: VMX_PREEMPTION_TIMER_VALUE */
 +	msrs->vmcs_enum = VMCS12_MAX_FIELD_INDEX << 1;
 +}
 +
 +/*
 + * if fixed0[i] == 1: val[i] must be 1
 + * if fixed1[i] == 0: val[i] must be 0
 + */
 +static inline bool fixed_bits_valid(u64 val, u64 fixed0, u64 fixed1)
 +{
 +	return ((val & fixed1) | fixed0) == val;
 +}
 +
 +static inline bool vmx_control_verify(u32 control, u32 low, u32 high)
 +{
 +	return fixed_bits_valid(control, low, high);
 +}
 +
 +static inline u64 vmx_control_msr(u32 low, u32 high)
 +{
 +	return low | ((u64)high << 32);
 +}
 +
 +static bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)
 +{
 +	superset &= mask;
 +	subset &= mask;
 +
 +	return (superset | subset) == superset;
 +}
 +
 +static int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)
 +{
 +	const u64 feature_and_reserved =
 +		/* feature (except bit 48; see below) */
 +		BIT_ULL(49) | BIT_ULL(54) | BIT_ULL(55) |
 +		/* reserved */
 +		BIT_ULL(31) | GENMASK_ULL(47, 45) | GENMASK_ULL(63, 56);
 +	u64 vmx_basic = vmx->nested.msrs.basic;
 +
 +	if (!is_bitwise_subset(vmx_basic, data, feature_and_reserved))
 +		return -EINVAL;
 +
 +	/*
 +	 * KVM does not emulate a version of VMX that constrains physical
 +	 * addresses of VMX structures (e.g. VMCS) to 32-bits.
 +	 */
 +	if (data & BIT_ULL(48))
 +		return -EINVAL;
 +
 +	if (vmx_basic_vmcs_revision_id(vmx_basic) !=
 +	    vmx_basic_vmcs_revision_id(data))
 +		return -EINVAL;
 +
 +	if (vmx_basic_vmcs_size(vmx_basic) > vmx_basic_vmcs_size(data))
 +		return -EINVAL;
 +
 +	vmx->nested.msrs.basic = data;
 +	return 0;
 +}
 +
 +static int
 +vmx_restore_control_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)
 +{
 +	u64 supported;
 +	u32 *lowp, *highp;
 +
 +	switch (msr_index) {
 +	case MSR_IA32_VMX_TRUE_PINBASED_CTLS:
 +		lowp = &vmx->nested.msrs.pinbased_ctls_low;
 +		highp = &vmx->nested.msrs.pinbased_ctls_high;
 +		break;
 +	case MSR_IA32_VMX_TRUE_PROCBASED_CTLS:
 +		lowp = &vmx->nested.msrs.procbased_ctls_low;
 +		highp = &vmx->nested.msrs.procbased_ctls_high;
 +		break;
 +	case MSR_IA32_VMX_TRUE_EXIT_CTLS:
 +		lowp = &vmx->nested.msrs.exit_ctls_low;
 +		highp = &vmx->nested.msrs.exit_ctls_high;
 +		break;
 +	case MSR_IA32_VMX_TRUE_ENTRY_CTLS:
 +		lowp = &vmx->nested.msrs.entry_ctls_low;
 +		highp = &vmx->nested.msrs.entry_ctls_high;
 +		break;
 +	case MSR_IA32_VMX_PROCBASED_CTLS2:
 +		lowp = &vmx->nested.msrs.secondary_ctls_low;
 +		highp = &vmx->nested.msrs.secondary_ctls_high;
 +		break;
 +	default:
 +		BUG();
 +	}
 +
 +	supported = vmx_control_msr(*lowp, *highp);
 +
 +	/* Check must-be-1 bits are still 1. */
 +	if (!is_bitwise_subset(data, supported, GENMASK_ULL(31, 0)))
 +		return -EINVAL;
 +
 +	/* Check must-be-0 bits are still 0. */
 +	if (!is_bitwise_subset(supported, data, GENMASK_ULL(63, 32)))
 +		return -EINVAL;
 +
 +	*lowp = data;
 +	*highp = data >> 32;
 +	return 0;
 +}
 +
 +static int vmx_restore_vmx_misc(struct vcpu_vmx *vmx, u64 data)
 +{
 +	const u64 feature_and_reserved_bits =
 +		/* feature */
 +		BIT_ULL(5) | GENMASK_ULL(8, 6) | BIT_ULL(14) | BIT_ULL(15) |
 +		BIT_ULL(28) | BIT_ULL(29) | BIT_ULL(30) |
 +		/* reserved */
 +		GENMASK_ULL(13, 9) | BIT_ULL(31);
 +	u64 vmx_misc;
 +
 +	vmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,
 +				   vmx->nested.msrs.misc_high);
 +
 +	if (!is_bitwise_subset(vmx_misc, data, feature_and_reserved_bits))
 +		return -EINVAL;
 +
 +	if ((vmx->nested.msrs.pinbased_ctls_high &
 +	     PIN_BASED_VMX_PREEMPTION_TIMER) &&
 +	    vmx_misc_preemption_timer_rate(data) !=
 +	    vmx_misc_preemption_timer_rate(vmx_misc))
 +		return -EINVAL;
 +
 +	if (vmx_misc_cr3_count(data) > vmx_misc_cr3_count(vmx_misc))
 +		return -EINVAL;
 +
 +	if (vmx_misc_max_msr(data) > vmx_misc_max_msr(vmx_misc))
 +		return -EINVAL;
 +
 +	if (vmx_misc_mseg_revid(data) != vmx_misc_mseg_revid(vmx_misc))
 +		return -EINVAL;
 +
 +	vmx->nested.msrs.misc_low = data;
 +	vmx->nested.msrs.misc_high = data >> 32;
 +
 +	/*
 +	 * If L1 has read-only VM-exit information fields, use the
 +	 * less permissive vmx_vmwrite_bitmap to specify write
 +	 * permissions for the shadow VMCS.
 +	 */
 +	if (enable_shadow_vmcs && !nested_cpu_has_vmwrite_any_field(&vmx->vcpu))
 +		vmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));
 +
 +	return 0;
 +}
 +
 +static int vmx_restore_vmx_ept_vpid_cap(struct vcpu_vmx *vmx, u64 data)
 +{
 +	u64 vmx_ept_vpid_cap;
 +
 +	vmx_ept_vpid_cap = vmx_control_msr(vmx->nested.msrs.ept_caps,
 +					   vmx->nested.msrs.vpid_caps);
 +
 +	/* Every bit is either reserved or a feature bit. */
 +	if (!is_bitwise_subset(vmx_ept_vpid_cap, data, -1ULL))
 +		return -EINVAL;
 +
 +	vmx->nested.msrs.ept_caps = data;
 +	vmx->nested.msrs.vpid_caps = data >> 32;
 +	return 0;
 +}
 +
 +static int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)
 +{
 +	u64 *msr;
 +
 +	switch (msr_index) {
 +	case MSR_IA32_VMX_CR0_FIXED0:
 +		msr = &vmx->nested.msrs.cr0_fixed0;
 +		break;
 +	case MSR_IA32_VMX_CR4_FIXED0:
 +		msr = &vmx->nested.msrs.cr4_fixed0;
 +		break;
 +	default:
 +		BUG();
 +	}
 +
 +	/*
 +	 * 1 bits (which indicates bits which "must-be-1" during VMX operation)
 +	 * must be 1 in the restored value.
 +	 */
 +	if (!is_bitwise_subset(data, *msr, -1ULL))
 +		return -EINVAL;
 +
 +	*msr = data;
 +	return 0;
 +}
 +
 +/*
 + * Called when userspace is restoring VMX MSRs.
 + *
 + * Returns 0 on success, non-0 otherwise.
 + */
 +static int vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	/*
 +	 * Don't allow changes to the VMX capability MSRs while the vCPU
 +	 * is in VMX operation.
 +	 */
 +	if (vmx->nested.vmxon)
 +		return -EBUSY;
 +
 +	switch (msr_index) {
 +	case MSR_IA32_VMX_BASIC:
 +		return vmx_restore_vmx_basic(vmx, data);
 +	case MSR_IA32_VMX_PINBASED_CTLS:
 +	case MSR_IA32_VMX_PROCBASED_CTLS:
 +	case MSR_IA32_VMX_EXIT_CTLS:
 +	case MSR_IA32_VMX_ENTRY_CTLS:
 +		/*
 +		 * The "non-true" VMX capability MSRs are generated from the
 +		 * "true" MSRs, so we do not support restoring them directly.
 +		 *
 +		 * If userspace wants to emulate VMX_BASIC[55]=0, userspace
 +		 * should restore the "true" MSRs with the must-be-1 bits
 +		 * set according to the SDM Vol 3. A.2 "RESERVED CONTROLS AND
 +		 * DEFAULT SETTINGS".
 +		 */
 +		return -EINVAL;
 +	case MSR_IA32_VMX_TRUE_PINBASED_CTLS:
 +	case MSR_IA32_VMX_TRUE_PROCBASED_CTLS:
 +	case MSR_IA32_VMX_TRUE_EXIT_CTLS:
 +	case MSR_IA32_VMX_TRUE_ENTRY_CTLS:
 +	case MSR_IA32_VMX_PROCBASED_CTLS2:
 +		return vmx_restore_control_msr(vmx, msr_index, data);
 +	case MSR_IA32_VMX_MISC:
 +		return vmx_restore_vmx_misc(vmx, data);
 +	case MSR_IA32_VMX_CR0_FIXED0:
 +	case MSR_IA32_VMX_CR4_FIXED0:
 +		return vmx_restore_fixed0_msr(vmx, msr_index, data);
 +	case MSR_IA32_VMX_CR0_FIXED1:
 +	case MSR_IA32_VMX_CR4_FIXED1:
 +		/*
 +		 * These MSRs are generated based on the vCPU's CPUID, so we
 +		 * do not support restoring them directly.
 +		 */
 +		return -EINVAL;
 +	case MSR_IA32_VMX_EPT_VPID_CAP:
 +		return vmx_restore_vmx_ept_vpid_cap(vmx, data);
 +	case MSR_IA32_VMX_VMCS_ENUM:
 +		vmx->nested.msrs.vmcs_enum = data;
 +		return 0;
 +	default:
 +		/*
 +		 * The rest of the VMX capability MSRs do not support restore.
 +		 */
 +		return -EINVAL;
 +	}
 +}
 +
 +/* Returns 0 on success, non-0 otherwise. */
 +static int vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)
 +{
 +	switch (msr_index) {
 +	case MSR_IA32_VMX_BASIC:
 +		*pdata = msrs->basic;
 +		break;
 +	case MSR_IA32_VMX_TRUE_PINBASED_CTLS:
 +	case MSR_IA32_VMX_PINBASED_CTLS:
 +		*pdata = vmx_control_msr(
 +			msrs->pinbased_ctls_low,
 +			msrs->pinbased_ctls_high);
 +		if (msr_index == MSR_IA32_VMX_PINBASED_CTLS)
 +			*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 +		break;
 +	case MSR_IA32_VMX_TRUE_PROCBASED_CTLS:
 +	case MSR_IA32_VMX_PROCBASED_CTLS:
 +		*pdata = vmx_control_msr(
 +			msrs->procbased_ctls_low,
 +			msrs->procbased_ctls_high);
 +		if (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)
 +			*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 +		break;
 +	case MSR_IA32_VMX_TRUE_EXIT_CTLS:
 +	case MSR_IA32_VMX_EXIT_CTLS:
 +		*pdata = vmx_control_msr(
 +			msrs->exit_ctls_low,
 +			msrs->exit_ctls_high);
 +		if (msr_index == MSR_IA32_VMX_EXIT_CTLS)
 +			*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;
 +		break;
 +	case MSR_IA32_VMX_TRUE_ENTRY_CTLS:
 +	case MSR_IA32_VMX_ENTRY_CTLS:
 +		*pdata = vmx_control_msr(
 +			msrs->entry_ctls_low,
 +			msrs->entry_ctls_high);
 +		if (msr_index == MSR_IA32_VMX_ENTRY_CTLS)
 +			*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;
 +		break;
 +	case MSR_IA32_VMX_MISC:
 +		*pdata = vmx_control_msr(
 +			msrs->misc_low,
 +			msrs->misc_high);
 +		break;
 +	case MSR_IA32_VMX_CR0_FIXED0:
 +		*pdata = msrs->cr0_fixed0;
 +		break;
 +	case MSR_IA32_VMX_CR0_FIXED1:
 +		*pdata = msrs->cr0_fixed1;
 +		break;
 +	case MSR_IA32_VMX_CR4_FIXED0:
 +		*pdata = msrs->cr4_fixed0;
 +		break;
 +	case MSR_IA32_VMX_CR4_FIXED1:
 +		*pdata = msrs->cr4_fixed1;
 +		break;
 +	case MSR_IA32_VMX_VMCS_ENUM:
 +		*pdata = msrs->vmcs_enum;
 +		break;
 +	case MSR_IA32_VMX_PROCBASED_CTLS2:
 +		*pdata = vmx_control_msr(
 +			msrs->secondary_ctls_low,
 +			msrs->secondary_ctls_high);
 +		break;
 +	case MSR_IA32_VMX_EPT_VPID_CAP:
 +		*pdata = msrs->ept_caps |
 +			((u64)msrs->vpid_caps << 32);
 +		break;
 +	case MSR_IA32_VMX_VMFUNC:
 +		*pdata = msrs->vmfunc_controls;
 +		break;
 +	default:
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,
 +						 uint64_t val)
 +{
 +	uint64_t valid_bits = to_vmx(vcpu)->msr_ia32_feature_control_valid_bits;
 +
 +	return !(val & ~valid_bits);
 +}
 +
 +static int vmx_get_msr_feature(struct kvm_msr_entry *msr)
 +{
 +	switch (msr->index) {
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		if (!nested)
 +			return 1;
 +		return vmx_get_vmx_msr(&vmcs_config.nested, msr->index, &msr->data);
 +	default:
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +/*
 + * Reads an msr value (of 'msr_index') into 'pdata'.
 + * Returns 0 on success, non-0 otherwise.
 + * Assumes vcpu_load() was already called.
 + */
 +static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct shared_msr_entry *msr;
 +
 +	switch (msr_info->index) {
 +#ifdef CONFIG_X86_64
 +	case MSR_FS_BASE:
 +		msr_info->data = vmcs_readl(GUEST_FS_BASE);
 +		break;
 +	case MSR_GS_BASE:
 +		msr_info->data = vmcs_readl(GUEST_GS_BASE);
 +		break;
 +	case MSR_KERNEL_GS_BASE:
 +		msr_info->data = vmx_read_guest_kernel_gs_base(vmx);
 +		break;
 +#endif
 +	case MSR_EFER:
 +		return kvm_get_msr_common(vcpu, msr_info);
 +	case MSR_IA32_SPEC_CTRL:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 +			return 1;
 +
 +		msr_info->data = to_vmx(vcpu)->spec_ctrl;
 +		break;
 +	case MSR_IA32_ARCH_CAPABILITIES:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))
 +			return 1;
 +		msr_info->data = to_vmx(vcpu)->arch_capabilities;
 +		break;
 +	case MSR_IA32_SYSENTER_CS:
 +		msr_info->data = vmcs_read32(GUEST_SYSENTER_CS);
 +		break;
 +	case MSR_IA32_SYSENTER_EIP:
 +		msr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);
 +		break;
 +	case MSR_IA32_SYSENTER_ESP:
 +		msr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);
 +		break;
 +	case MSR_IA32_BNDCFGS:
 +		if (!kvm_mpx_supported() ||
 +		    (!msr_info->host_initiated &&
 +		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))
 +			return 1;
 +		msr_info->data = vmcs_read64(GUEST_BNDCFGS);
 +		break;
 +	case MSR_IA32_MCG_EXT_CTL:
 +		if (!msr_info->host_initiated &&
 +		    !(vmx->msr_ia32_feature_control &
 +		      FEATURE_CONTROL_LMCE))
 +			return 1;
 +		msr_info->data = vcpu->arch.mcg_ext_ctl;
 +		break;
 +	case MSR_IA32_FEATURE_CONTROL:
 +		msr_info->data = vmx->msr_ia32_feature_control;
 +		break;
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		if (!nested_vmx_allowed(vcpu))
 +			return 1;
 +		return vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,
 +				       &msr_info->data);
 +	case MSR_IA32_XSS:
 +		if (!vmx_xsaves_supported())
 +			return 1;
 +		msr_info->data = vcpu->arch.ia32_xss;
 +		break;
 +	case MSR_TSC_AUX:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))
 +			return 1;
 +		/* Otherwise falls through */
 +	default:
 +		msr = find_msr_entry(vmx, msr_info->index);
 +		if (msr) {
 +			msr_info->data = msr->data;
 +			break;
 +		}
 +		return kvm_get_msr_common(vcpu, msr_info);
 +	}
 +
 +	return 0;
 +}
 +
 +static void vmx_leave_nested(struct kvm_vcpu *vcpu);
 +
 +/*
 + * Writes msr value into into the appropriate "register".
 + * Returns 0 on success, non-0 otherwise.
 + * Assumes vcpu_load() was already called.
 + */
 +static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct shared_msr_entry *msr;
 +	int ret = 0;
 +	u32 msr_index = msr_info->index;
 +	u64 data = msr_info->data;
 +
 +	switch (msr_index) {
 +	case MSR_EFER:
 +		ret = kvm_set_msr_common(vcpu, msr_info);
 +		break;
 +#ifdef CONFIG_X86_64
 +	case MSR_FS_BASE:
 +		vmx_segment_cache_clear(vmx);
 +		vmcs_writel(GUEST_FS_BASE, data);
 +		break;
 +	case MSR_GS_BASE:
 +		vmx_segment_cache_clear(vmx);
 +		vmcs_writel(GUEST_GS_BASE, data);
 +		break;
 +	case MSR_KERNEL_GS_BASE:
 +		vmx_write_guest_kernel_gs_base(vmx, data);
 +		break;
 +#endif
 +	case MSR_IA32_SYSENTER_CS:
 +		vmcs_write32(GUEST_SYSENTER_CS, data);
 +		break;
 +	case MSR_IA32_SYSENTER_EIP:
 +		vmcs_writel(GUEST_SYSENTER_EIP, data);
 +		break;
 +	case MSR_IA32_SYSENTER_ESP:
 +		vmcs_writel(GUEST_SYSENTER_ESP, data);
 +		break;
 +	case MSR_IA32_BNDCFGS:
 +		if (!kvm_mpx_supported() ||
 +		    (!msr_info->host_initiated &&
 +		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))
 +			return 1;
 +		if (is_noncanonical_address(data & PAGE_MASK, vcpu) ||
 +		    (data & MSR_IA32_BNDCFGS_RSVD))
 +			return 1;
 +		vmcs_write64(GUEST_BNDCFGS, data);
 +		break;
 +	case MSR_IA32_SPEC_CTRL:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 +			return 1;
 +
 +		/* The STIBP bit doesn't fault even if it's not advertised */
 +		if (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD))
 +			return 1;
 +
 +		vmx->spec_ctrl = data;
 +
 +		if (!data)
 +			break;
 +
 +		/*
 +		 * For non-nested:
 +		 * When it's written (to non-zero) for the first time, pass
 +		 * it through.
 +		 *
 +		 * For nested:
 +		 * The handling of the MSR bitmap for L2 guests is done in
 +		 * nested_vmx_merge_msr_bitmap. We should not touch the
 +		 * vmcs02.msr_bitmap here since it gets completely overwritten
 +		 * in the merging. We update the vmcs01 here for L1 as well
 +		 * since it will end up touching the MSR anyway now.
 +		 */
 +		vmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap,
 +					      MSR_IA32_SPEC_CTRL,
 +					      MSR_TYPE_RW);
 +		break;
 +	case MSR_IA32_PRED_CMD:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 +			return 1;
 +
 +		if (data & ~PRED_CMD_IBPB)
 +			return 1;
 +
 +		if (!data)
 +			break;
 +
 +		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);
 +
 +		/*
 +		 * For non-nested:
 +		 * When it's written (to non-zero) for the first time, pass
 +		 * it through.
 +		 *
 +		 * For nested:
 +		 * The handling of the MSR bitmap for L2 guests is done in
 +		 * nested_vmx_merge_msr_bitmap. We should not touch the
 +		 * vmcs02.msr_bitmap here since it gets completely overwritten
 +		 * in the merging.
 +		 */
 +		vmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap, MSR_IA32_PRED_CMD,
 +					      MSR_TYPE_W);
 +		break;
 +	case MSR_IA32_ARCH_CAPABILITIES:
 +		if (!msr_info->host_initiated)
 +			return 1;
 +		vmx->arch_capabilities = data;
 +		break;
 +	case MSR_IA32_CR_PAT:
 +		if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {
 +			if (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))
 +				return 1;
 +			vmcs_write64(GUEST_IA32_PAT, data);
 +			vcpu->arch.pat = data;
 +			break;
 +		}
 +		ret = kvm_set_msr_common(vcpu, msr_info);
 +		break;
 +	case MSR_IA32_TSC_ADJUST:
 +		ret = kvm_set_msr_common(vcpu, msr_info);
 +		break;
 +	case MSR_IA32_MCG_EXT_CTL:
 +		if ((!msr_info->host_initiated &&
 +		     !(to_vmx(vcpu)->msr_ia32_feature_control &
 +		       FEATURE_CONTROL_LMCE)) ||
 +		    (data & ~MCG_EXT_CTL_LMCE_EN))
 +			return 1;
 +		vcpu->arch.mcg_ext_ctl = data;
 +		break;
 +	case MSR_IA32_FEATURE_CONTROL:
 +		if (!vmx_feature_control_msr_valid(vcpu, data) ||
 +		    (to_vmx(vcpu)->msr_ia32_feature_control &
 +		     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))
 +			return 1;
 +		vmx->msr_ia32_feature_control = data;
 +		if (msr_info->host_initiated && data == 0)
 +			vmx_leave_nested(vcpu);
 +		break;
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		if (!msr_info->host_initiated)
 +			return 1; /* they are read-only */
 +		if (!nested_vmx_allowed(vcpu))
 +			return 1;
 +		return vmx_set_vmx_msr(vcpu, msr_index, data);
 +	case MSR_IA32_XSS:
 +		if (!vmx_xsaves_supported())
 +			return 1;
 +		/*
 +		 * The only supported bit as of Skylake is bit 8, but
 +		 * it is not supported on KVM.
 +		 */
 +		if (data != 0)
 +			return 1;
 +		vcpu->arch.ia32_xss = data;
 +		if (vcpu->arch.ia32_xss != host_xss)
 +			add_atomic_switch_msr(vmx, MSR_IA32_XSS,
 +				vcpu->arch.ia32_xss, host_xss, false);
 +		else
 +			clear_atomic_switch_msr(vmx, MSR_IA32_XSS);
 +		break;
 +	case MSR_TSC_AUX:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))
 +			return 1;
 +		/* Check reserved bit, higher 32 bits should be zero */
 +		if ((data >> 32) != 0)
 +			return 1;
 +		/* Otherwise falls through */
 +	default:
 +		msr = find_msr_entry(vmx, msr_index);
 +		if (msr) {
 +			u64 old_msr_data = msr->data;
 +			msr->data = data;
 +			if (msr - vmx->guest_msrs < vmx->save_nmsrs) {
 +				preempt_disable();
 +				ret = kvm_set_shared_msr(msr->index, msr->data,
 +							 msr->mask);
 +				preempt_enable();
 +				if (ret)
 +					msr->data = old_msr_data;
 +			}
 +			break;
 +		}
 +		ret = kvm_set_msr_common(vcpu, msr_info);
 +	}
 +
 +	return ret;
 +}
 +
 +static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 +{
 +	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
 +	switch (reg) {
 +	case VCPU_REGS_RSP:
 +		vcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);
 +		break;
 +	case VCPU_REGS_RIP:
 +		vcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
 +		break;
 +	case VCPU_EXREG_PDPTR:
 +		if (enable_ept)
 +			ept_save_pdptrs(vcpu);
 +		break;
 +	default:
 +		break;
 +	}
 +}
 +
 +static __init int cpu_has_kvm_support(void)
 +{
 +	return cpu_has_vmx();
 +}
 +
 +static __init int vmx_disabled_by_bios(void)
 +{
 +	u64 msr;
 +
 +	rdmsrl(MSR_IA32_FEATURE_CONTROL, msr);
 +	if (msr & FEATURE_CONTROL_LOCKED) {
 +		/* launched w/ TXT and VMX disabled */
 +		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)
 +			&& tboot_enabled())
 +			return 1;
 +		/* launched w/o TXT and VMX only enabled w/ TXT */
 +		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)
 +			&& (msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)
 +			&& !tboot_enabled()) {
 +			printk(KERN_WARNING "kvm: disable TXT in the BIOS or "
 +				"activate TXT before enabling KVM\n");
 +			return 1;
 +		}
 +		/* launched w/o TXT and VMX disabled */
 +		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)
 +			&& !tboot_enabled())
 +			return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static void kvm_cpu_vmxon(u64 addr)
 +{
 +	cr4_set_bits(X86_CR4_VMXE);
 +	intel_pt_handle_vmx(1);
 +
 +	asm volatile ("vmxon %0" : : "m"(addr));
 +}
 +
 +static int hardware_enable(void)
 +{
 +	int cpu = raw_smp_processor_id();
 +	u64 phys_addr = __pa(per_cpu(vmxarea, cpu));
 +	u64 old, test_bits;
 +
 +	if (cr4_read_shadow() & X86_CR4_VMXE)
 +		return -EBUSY;
 +
 +	/*
 +	 * This can happen if we hot-added a CPU but failed to allocate
 +	 * VP assist page for it.
 +	 */
 +	if (static_branch_unlikely(&enable_evmcs) &&
 +	    !hv_get_vp_assist_page(cpu))
 +		return -EFAULT;
 +
 +	INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
 +	INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
 +	spin_lock_init(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 +
 +	/*
 +	 * Now we can enable the vmclear operation in kdump
 +	 * since the loaded_vmcss_on_cpu list on this cpu
 +	 * has been initialized.
 +	 *
 +	 * Though the cpu is not in VMX operation now, there
 +	 * is no problem to enable the vmclear operation
 +	 * for the loaded_vmcss_on_cpu list is empty!
 +	 */
 +	crash_enable_local_vmclear(cpu);
 +
 +	rdmsrl(MSR_IA32_FEATURE_CONTROL, old);
 +
 +	test_bits = FEATURE_CONTROL_LOCKED;
 +	test_bits |= FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;
 +	if (tboot_enabled())
 +		test_bits |= FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX;
 +
 +	if ((old & test_bits) != test_bits) {
 +		/* enable and lock */
 +		wrmsrl(MSR_IA32_FEATURE_CONTROL, old | test_bits);
 +	}
 +	kvm_cpu_vmxon(phys_addr);
 +	if (enable_ept)
 +		ept_sync_global();
 +
 +	return 0;
 +}
 +
 +static void vmclear_local_loaded_vmcss(void)
 +{
 +	int cpu = raw_smp_processor_id();
 +	struct loaded_vmcs *v, *n;
 +
 +	list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
 +				 loaded_vmcss_on_cpu_link)
 +		__loaded_vmcs_clear(v);
 +}
 +
 +
 +/* Just like cpu_vmxoff(), but with the __kvm_handle_fault_on_reboot()
 + * tricks.
 + */
 +static void kvm_cpu_vmxoff(void)
 +{
 +	asm volatile (__ex("vmxoff"));
 +
 +	intel_pt_handle_vmx(0);
 +	cr4_clear_bits(X86_CR4_VMXE);
 +}
 +
 +static void hardware_disable(void)
 +{
 +	vmclear_local_loaded_vmcss();
 +	kvm_cpu_vmxoff();
 +}
 +
 +static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
 +				      u32 msr, u32 *result)
 +{
 +	u32 vmx_msr_low, vmx_msr_high;
 +	u32 ctl = ctl_min | ctl_opt;
 +
 +	rdmsr(msr, vmx_msr_low, vmx_msr_high);
 +
 +	ctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */
 +	ctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */
 +
 +	/* Ensure minimum (required) set of control bits are supported. */
 +	if (ctl_min & ~ctl)
 +		return -EIO;
 +
 +	*result = ctl;
 +	return 0;
 +}
 +
 +static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 +				    struct vmx_capability *vmx_cap)
 +{
 +	u32 vmx_msr_low, vmx_msr_high;
 +	u32 min, opt, min2, opt2;
 +	u32 _pin_based_exec_control = 0;
 +	u32 _cpu_based_exec_control = 0;
 +	u32 _cpu_based_2nd_exec_control = 0;
 +	u32 _vmexit_control = 0;
 +	u32 _vmentry_control = 0;
 +
 +	memset(vmcs_conf, 0, sizeof(*vmcs_conf));
 +	min = CPU_BASED_HLT_EXITING |
 +#ifdef CONFIG_X86_64
 +	      CPU_BASED_CR8_LOAD_EXITING |
 +	      CPU_BASED_CR8_STORE_EXITING |
 +#endif
 +	      CPU_BASED_CR3_LOAD_EXITING |
 +	      CPU_BASED_CR3_STORE_EXITING |
 +	      CPU_BASED_UNCOND_IO_EXITING |
 +	      CPU_BASED_MOV_DR_EXITING |
 +	      CPU_BASED_USE_TSC_OFFSETING |
 +	      CPU_BASED_MWAIT_EXITING |
 +	      CPU_BASED_MONITOR_EXITING |
 +	      CPU_BASED_INVLPG_EXITING |
 +	      CPU_BASED_RDPMC_EXITING;
 +
 +	opt = CPU_BASED_TPR_SHADOW |
 +	      CPU_BASED_USE_MSR_BITMAPS |
 +	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
 +	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
 +				&_cpu_based_exec_control) < 0)
 +		return -EIO;
 +#ifdef CONFIG_X86_64
 +	if ((_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 +		_cpu_based_exec_control &= ~CPU_BASED_CR8_LOAD_EXITING &
 +					   ~CPU_BASED_CR8_STORE_EXITING;
 +#endif
 +	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
 +		min2 = 0;
 +		opt2 = SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +			SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 +			SECONDARY_EXEC_WBINVD_EXITING |
 +			SECONDARY_EXEC_ENABLE_VPID |
 +			SECONDARY_EXEC_ENABLE_EPT |
 +			SECONDARY_EXEC_UNRESTRICTED_GUEST |
 +			SECONDARY_EXEC_PAUSE_LOOP_EXITING |
 +			SECONDARY_EXEC_DESC |
 +			SECONDARY_EXEC_RDTSCP |
 +			SECONDARY_EXEC_ENABLE_INVPCID |
 +			SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +			SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
 +			SECONDARY_EXEC_SHADOW_VMCS |
 +			SECONDARY_EXEC_XSAVES |
 +			SECONDARY_EXEC_RDSEED_EXITING |
 +			SECONDARY_EXEC_RDRAND_EXITING |
 +			SECONDARY_EXEC_ENABLE_PML |
 +			SECONDARY_EXEC_TSC_SCALING |
++			SECONDARY_EXEC_PT_USE_GPA |
++			SECONDARY_EXEC_PT_CONCEAL_VMX |
 +			SECONDARY_EXEC_ENABLE_VMFUNC |
 +			SECONDARY_EXEC_ENCLS_EXITING;
 +		if (adjust_vmx_controls(min2, opt2,
 +					MSR_IA32_VMX_PROCBASED_CTLS2,
 +					&_cpu_based_2nd_exec_control) < 0)
 +			return -EIO;
 +	}
 +#ifndef CONFIG_X86_64
 +	if (!(_cpu_based_2nd_exec_control &
 +				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
 +		_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;
 +#endif
 +
 +	if (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 +		_cpu_based_2nd_exec_control &= ~(
 +				SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 +				SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 +
 +	rdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,
 +		&vmx_cap->ept, &vmx_cap->vpid);
 +
 +	if (_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) {
 +		/* CR3 accesses and invlpg don't need to cause VM Exits when EPT
 +		   enabled */
 +		_cpu_based_exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |
 +					     CPU_BASED_CR3_STORE_EXITING |
 +					     CPU_BASED_INVLPG_EXITING);
 +	} else if (vmx_cap->ept) {
 +		vmx_cap->ept = 0;
 +		pr_warn_once("EPT CAP should not exist if not support "
 +				"1-setting enable EPT VM-execution control\n");
 +	}
 +	if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&
 +		vmx_cap->vpid) {
 +		vmx_cap->vpid = 0;
 +		pr_warn_once("VPID CAP should not exist if not support "
 +				"1-setting enable VPID VM-execution control\n");
 +	}
 +
 +	min = VM_EXIT_SAVE_DEBUG_CONTROLS | VM_EXIT_ACK_INTR_ON_EXIT;
 +#ifdef CONFIG_X86_64
 +	min |= VM_EXIT_HOST_ADDR_SPACE_SIZE;
 +#endif
 +	opt = VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL |
 +	      VM_EXIT_SAVE_IA32_PAT |
 +	      VM_EXIT_LOAD_IA32_PAT |
 +	      VM_EXIT_LOAD_IA32_EFER |
- 	      VM_EXIT_CLEAR_BNDCFGS;
++	      VM_EXIT_CLEAR_BNDCFGS |
++	      VM_EXIT_PT_CONCEAL_PIP |
++	      VM_EXIT_CLEAR_IA32_RTIT_CTL;
 +	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
 +				&_vmexit_control) < 0)
 +		return -EIO;
 +
 +	min = PIN_BASED_EXT_INTR_MASK | PIN_BASED_NMI_EXITING;
 +	opt = PIN_BASED_VIRTUAL_NMIS | PIN_BASED_POSTED_INTR |
 +		 PIN_BASED_VMX_PREEMPTION_TIMER;
 +	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
 +				&_pin_based_exec_control) < 0)
 +		return -EIO;
 +
 +	if (cpu_has_broken_vmx_preemption_timer())
 +		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
 +	if (!(_cpu_based_2nd_exec_control &
 +		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
 +		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
 +
 +	min = VM_ENTRY_LOAD_DEBUG_CONTROLS;
 +	opt = VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
 +	      VM_ENTRY_LOAD_IA32_PAT |
 +	      VM_ENTRY_LOAD_IA32_EFER |
- 	      VM_ENTRY_LOAD_BNDCFGS;
++	      VM_ENTRY_LOAD_BNDCFGS |
++	      VM_ENTRY_PT_CONCEAL_PIP |
++	      VM_ENTRY_LOAD_IA32_RTIT_CTL;
 +	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
 +				&_vmentry_control) < 0)
 +		return -EIO;
 +
 +	/*
 +	 * Some cpus support VM_{ENTRY,EXIT}_IA32_PERF_GLOBAL_CTRL but they
 +	 * can't be used due to an errata where VM Exit may incorrectly clear
 +	 * IA32_PERF_GLOBAL_CTRL[34:32].  Workaround the errata by using the
 +	 * MSR load mechanism to switch IA32_PERF_GLOBAL_CTRL.
 +	 */
 +	if (boot_cpu_data.x86 == 0x6) {
 +		switch (boot_cpu_data.x86_model) {
 +		case 26: /* AAK155 */
 +		case 30: /* AAP115 */
 +		case 37: /* AAT100 */
 +		case 44: /* BC86,AAY89,BD102 */
 +		case 46: /* BA97 */
 +			_vmexit_control &= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;
 +			_vmexit_control &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;
 +			pr_warn_once("kvm: VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL "
 +					"does not work properly. Using workaround\n");
 +			break;
 +		default:
 +			break;
 +		}
 +	}
 +
 +
 +	rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);
 +
 +	/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */
 +	if ((vmx_msr_high & 0x1fff) > PAGE_SIZE)
 +		return -EIO;
 +
 +#ifdef CONFIG_X86_64
 +	/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */
 +	if (vmx_msr_high & (1u<<16))
 +		return -EIO;
 +#endif
 +
 +	/* Require Write-Back (WB) memory type for VMCS accesses. */
 +	if (((vmx_msr_high >> 18) & 15) != 6)
 +		return -EIO;
 +
 +	vmcs_conf->size = vmx_msr_high & 0x1fff;
 +	vmcs_conf->order = get_order(vmcs_conf->size);
 +	vmcs_conf->basic_cap = vmx_msr_high & ~0x1fff;
 +
 +	vmcs_conf->revision_id = vmx_msr_low;
 +
 +	vmcs_conf->pin_based_exec_ctrl = _pin_based_exec_control;
 +	vmcs_conf->cpu_based_exec_ctrl = _cpu_based_exec_control;
 +	vmcs_conf->cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
 +	vmcs_conf->vmexit_ctrl         = _vmexit_control;
 +	vmcs_conf->vmentry_ctrl        = _vmentry_control;
 +
 +	if (static_branch_unlikely(&enable_evmcs))
 +		evmcs_sanitize_exec_ctrls(vmcs_conf);
 +
 +	return 0;
 +}
 +
 +static struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu)
 +{
 +	int node = cpu_to_node(cpu);
 +	struct page *pages;
 +	struct vmcs *vmcs;
 +
 +	pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);
 +	if (!pages)
 +		return NULL;
 +	vmcs = page_address(pages);
 +	memset(vmcs, 0, vmcs_config.size);
 +
 +	/* KVM supports Enlightened VMCS v1 only */
 +	if (static_branch_unlikely(&enable_evmcs))
 +		vmcs->hdr.revision_id = KVM_EVMCS_VERSION;
 +	else
 +		vmcs->hdr.revision_id = vmcs_config.revision_id;
 +
 +	if (shadow)
 +		vmcs->hdr.shadow_vmcs = 1;
 +	return vmcs;
 +}
 +
 +static void free_vmcs(struct vmcs *vmcs)
 +{
 +	free_pages((unsigned long)vmcs, vmcs_config.order);
 +}
 +
 +/*
 + * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded
 + */
 +static void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 +{
 +	if (!loaded_vmcs->vmcs)
 +		return;
 +	loaded_vmcs_clear(loaded_vmcs);
 +	free_vmcs(loaded_vmcs->vmcs);
 +	loaded_vmcs->vmcs = NULL;
 +	if (loaded_vmcs->msr_bitmap)
 +		free_page((unsigned long)loaded_vmcs->msr_bitmap);
 +	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
 +}
 +
 +static struct vmcs *alloc_vmcs(bool shadow)
 +{
 +	return alloc_vmcs_cpu(shadow, raw_smp_processor_id());
 +}
 +
 +static int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 +{
 +	loaded_vmcs->vmcs = alloc_vmcs(false);
 +	if (!loaded_vmcs->vmcs)
 +		return -ENOMEM;
 +
 +	loaded_vmcs->shadow_vmcs = NULL;
 +	loaded_vmcs_init(loaded_vmcs);
 +
 +	if (cpu_has_vmx_msr_bitmap()) {
 +		loaded_vmcs->msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);
 +		if (!loaded_vmcs->msr_bitmap)
 +			goto out_vmcs;
 +		memset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);
 +
 +		if (IS_ENABLED(CONFIG_HYPERV) &&
 +		    static_branch_unlikely(&enable_evmcs) &&
 +		    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {
 +			struct hv_enlightened_vmcs *evmcs =
 +				(struct hv_enlightened_vmcs *)loaded_vmcs->vmcs;
 +
 +			evmcs->hv_enlightenments_control.msr_bitmap = 1;
 +		}
 +	}
 +
 +	memset(&loaded_vmcs->host_state, 0, sizeof(struct vmcs_host_state));
 +
 +	return 0;
 +
 +out_vmcs:
 +	free_loaded_vmcs(loaded_vmcs);
 +	return -ENOMEM;
 +}
 +
 +static void free_kvm_area(void)
 +{
 +	int cpu;
 +
 +	for_each_possible_cpu(cpu) {
 +		free_vmcs(per_cpu(vmxarea, cpu));
 +		per_cpu(vmxarea, cpu) = NULL;
 +	}
 +}
 +
 +enum vmcs_field_width {
 +	VMCS_FIELD_WIDTH_U16 = 0,
 +	VMCS_FIELD_WIDTH_U64 = 1,
 +	VMCS_FIELD_WIDTH_U32 = 2,
 +	VMCS_FIELD_WIDTH_NATURAL_WIDTH = 3
 +};
 +
 +static inline int vmcs_field_width(unsigned long field)
 +{
 +	if (0x1 & field)	/* the *_HIGH fields are all 32 bit */
 +		return VMCS_FIELD_WIDTH_U32;
 +	return (field >> 13) & 0x3 ;
 +}
 +
 +static inline int vmcs_field_readonly(unsigned long field)
 +{
 +	return (((field >> 10) & 0x3) == 1);
 +}
 +
 +static void init_vmcs_shadow_fields(void)
 +{
 +	int i, j;
 +
 +	memset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);
 +	memset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);
 +
 +	for (i = j = 0; i < max_shadow_read_only_fields; i++) {
 +		u16 field = shadow_read_only_fields[i];
 +		if (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&
 +		    (i + 1 == max_shadow_read_only_fields ||
 +		     shadow_read_only_fields[i + 1] != field + 1))
 +			pr_err("Missing field from shadow_read_only_field %x\n",
 +			       field + 1);
 +
 +		clear_bit(field, vmx_vmread_bitmap);
 +#ifdef CONFIG_X86_64
 +		if (field & 1)
 +			continue;
 +#endif
 +		if (j < i)
 +			shadow_read_only_fields[j] = field;
 +		j++;
 +	}
 +	max_shadow_read_only_fields = j;
 +
 +	for (i = j = 0; i < max_shadow_read_write_fields; i++) {
 +		u16 field = shadow_read_write_fields[i];
 +		if (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&
 +		    (i + 1 == max_shadow_read_write_fields ||
 +		     shadow_read_write_fields[i + 1] != field + 1))
 +			pr_err("Missing field from shadow_read_write_field %x\n",
 +			       field + 1);
 +
 +		/*
 +		 * PML and the preemption timer can be emulated, but the
 +		 * processor cannot vmwrite to fields that don't exist
 +		 * on bare metal.
 +		 */
 +		switch (field) {
 +		case GUEST_PML_INDEX:
 +			if (!cpu_has_vmx_pml())
 +				continue;
 +			break;
 +		case VMX_PREEMPTION_TIMER_VALUE:
 +			if (!cpu_has_vmx_preemption_timer())
 +				continue;
 +			break;
 +		case GUEST_INTR_STATUS:
 +			if (!cpu_has_vmx_apicv())
 +				continue;
 +			break;
 +		default:
 +			break;
 +		}
 +
 +		clear_bit(field, vmx_vmwrite_bitmap);
 +		clear_bit(field, vmx_vmread_bitmap);
 +#ifdef CONFIG_X86_64
 +		if (field & 1)
 +			continue;
 +#endif
 +		if (j < i)
 +			shadow_read_write_fields[j] = field;
 +		j++;
 +	}
 +	max_shadow_read_write_fields = j;
 +}
 +
 +static __init int alloc_kvm_area(void)
 +{
 +	int cpu;
 +
 +	for_each_possible_cpu(cpu) {
 +		struct vmcs *vmcs;
 +
 +		vmcs = alloc_vmcs_cpu(false, cpu);
 +		if (!vmcs) {
 +			free_kvm_area();
 +			return -ENOMEM;
 +		}
 +
 +		/*
 +		 * When eVMCS is enabled, alloc_vmcs_cpu() sets
 +		 * vmcs->revision_id to KVM_EVMCS_VERSION instead of
 +		 * revision_id reported by MSR_IA32_VMX_BASIC.
 +		 *
 +		 * However, even though not explictly documented by
 +		 * TLFS, VMXArea passed as VMXON argument should
 +		 * still be marked with revision_id reported by
 +		 * physical CPU.
 +		 */
 +		if (static_branch_unlikely(&enable_evmcs))
 +			vmcs->hdr.revision_id = vmcs_config.revision_id;
 +
 +		per_cpu(vmxarea, cpu) = vmcs;
 +	}
 +	return 0;
 +}
 +
 +static void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,
 +		struct kvm_segment *save)
 +{
 +	if (!emulate_invalid_guest_state) {
 +		/*
 +		 * CS and SS RPL should be equal during guest entry according
 +		 * to VMX spec, but in reality it is not always so. Since vcpu
 +		 * is in the middle of the transition from real mode to
 +		 * protected mode it is safe to assume that RPL 0 is a good
 +		 * default value.
 +		 */
 +		if (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)
 +			save->selector &= ~SEGMENT_RPL_MASK;
 +		save->dpl = save->selector & SEGMENT_RPL_MASK;
 +		save->s = 1;
 +	}
 +	vmx_set_segment(vcpu, save, seg);
 +}
 +
 +static void enter_pmode(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long flags;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	/*
 +	 * Update real mode segment cache. It may be not up-to-date if sement
 +	 * register was written while vcpu was in a guest mode.
 +	 */
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
 +
 +	vmx->rmode.vm86_active = 0;
 +
 +	vmx_segment_cache_clear(vmx);
 +
 +	vmx_set_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
 +
 +	flags = vmcs_readl(GUEST_RFLAGS);
 +	flags &= RMODE_GUEST_OWNED_EFLAGS_BITS;
 +	flags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;
 +	vmcs_writel(GUEST_RFLAGS, flags);
 +
 +	vmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |
 +			(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));
 +
 +	update_exception_bitmap(vcpu);
 +
 +	fix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);
 +	fix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);
 +	fix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);
 +	fix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);
 +	fix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);
 +	fix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);
 +}
 +
 +static void fix_rmode_seg(int seg, struct kvm_segment *save)
 +{
 +	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
 +	struct kvm_segment var = *save;
 +
 +	var.dpl = 0x3;
 +	if (seg == VCPU_SREG_CS)
 +		var.type = 0x3;
 +
 +	if (!emulate_invalid_guest_state) {
 +		var.selector = var.base >> 4;
 +		var.base = var.base & 0xffff0;
 +		var.limit = 0xffff;
 +		var.g = 0;
 +		var.db = 0;
 +		var.present = 1;
 +		var.s = 1;
 +		var.l = 0;
 +		var.unusable = 0;
 +		var.type = 0x3;
 +		var.avl = 0;
 +		if (save->base & 0xf)
 +			printk_once(KERN_WARNING "kvm: segment base is not "
 +					"paragraph aligned when entering "
 +					"protected mode (seg=%d)", seg);
 +	}
 +
 +	vmcs_write16(sf->selector, var.selector);
 +	vmcs_writel(sf->base, var.base);
 +	vmcs_write32(sf->limit, var.limit);
 +	vmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));
 +}
 +
 +static void enter_rmode(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long flags;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);
 +
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
 +	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
 +
 +	vmx->rmode.vm86_active = 1;
 +
 +	/*
 +	 * Very old userspace does not call KVM_SET_TSS_ADDR before entering
 +	 * vcpu. Warn the user that an update is overdue.
 +	 */
 +	if (!kvm_vmx->tss_addr)
 +		printk_once(KERN_WARNING "kvm: KVM_SET_TSS_ADDR need to be "
 +			     "called before entering vcpu\n");
 +
 +	vmx_segment_cache_clear(vmx);
 +
 +	vmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);
 +	vmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);
 +	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);
 +
 +	flags = vmcs_readl(GUEST_RFLAGS);
 +	vmx->rmode.save_rflags = flags;
 +
 +	flags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;
 +
 +	vmcs_writel(GUEST_RFLAGS, flags);
 +	vmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);
 +	update_exception_bitmap(vcpu);
 +
 +	fix_rmode_seg(VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);
 +	fix_rmode_seg(VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);
 +	fix_rmode_seg(VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);
 +	fix_rmode_seg(VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);
 +	fix_rmode_seg(VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);
 +	fix_rmode_seg(VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);
 +
 +	kvm_mmu_reset_context(vcpu);
 +}
 +
 +static void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct shared_msr_entry *msr = find_msr_entry(vmx, MSR_EFER);
 +
 +	if (!msr)
 +		return;
 +
 +	vcpu->arch.efer = efer;
 +	if (efer & EFER_LMA) {
 +		vm_entry_controls_setbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 +		msr->data = efer;
 +	} else {
 +		vm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 +
 +		msr->data = efer & ~EFER_LME;
 +	}
 +	setup_msrs(vmx);
 +}
 +
 +#ifdef CONFIG_X86_64
 +
 +static void enter_lmode(struct kvm_vcpu *vcpu)
 +{
 +	u32 guest_tr_ar;
 +
 +	vmx_segment_cache_clear(to_vmx(vcpu));
 +
 +	guest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);
 +	if ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
 +		pr_debug_ratelimited("%s: tss fixup for long mode. \n",
 +				     __func__);
 +		vmcs_write32(GUEST_TR_AR_BYTES,
 +			     (guest_tr_ar & ~VMX_AR_TYPE_MASK)
 +			     | VMX_AR_TYPE_BUSY_64_TSS);
 +	}
 +	vmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);
 +}
 +
 +static void exit_lmode(struct kvm_vcpu *vcpu)
 +{
 +	vm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 +	vmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);
 +}
 +
 +#endif
 +
 +static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 +				bool invalidate_gpa)
 +{
 +	if (enable_ept && (invalidate_gpa || !enable_vpid)) {
 +		if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
 +			return;
 +		ept_sync_context(construct_eptp(vcpu,
 +						vcpu->arch.mmu->root_hpa));
 +	} else {
 +		vpid_sync_context(vpid);
 +	}
 +}
 +
 +static void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);
 +}
 +
 +static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
 +{
 +	int vpid = to_vmx(vcpu)->vpid;
 +
 +	if (!vpid_sync_vcpu_addr(vpid, addr))
 +		vpid_sync_context(vpid);
 +
 +	/*
 +	 * If VPIDs are not supported or enabled, then the above is a no-op.
 +	 * But we don't really need a TLB flush in that case anyway, because
 +	 * each VM entry/exit includes an implicit flush when VPID is 0.
 +	 */
 +}
 +
 +static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 +{
 +	ulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
 +
 +	vcpu->arch.cr0 &= ~cr0_guest_owned_bits;
 +	vcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;
 +}
 +
 +static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 +{
 +	if (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))
 +		vcpu->arch.cr3 = vmcs_readl(GUEST_CR3);
 +	__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
 +}
 +
 +static void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)
 +{
 +	ulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;
 +
 +	vcpu->arch.cr4 &= ~cr4_guest_owned_bits;
 +	vcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & cr4_guest_owned_bits;
 +}
 +
 +static void ept_load_pdptrs(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
 +
 +	if (!test_bit(VCPU_EXREG_PDPTR,
 +		      (unsigned long *)&vcpu->arch.regs_dirty))
 +		return;
 +
 +	if (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {
 +		vmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);
 +		vmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);
 +		vmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);
 +		vmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);
 +	}
 +}
 +
 +static void ept_save_pdptrs(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
 +
 +	if (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {
 +		mmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);
 +		mmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);
 +		mmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);
 +		mmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);
 +	}
 +
 +	__set_bit(VCPU_EXREG_PDPTR,
 +		  (unsigned long *)&vcpu->arch.regs_avail);
 +	__set_bit(VCPU_EXREG_PDPTR,
 +		  (unsigned long *)&vcpu->arch.regs_dirty);
 +}
 +
 +static bool nested_guest_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;
 +	u64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +
 +	if (to_vmx(vcpu)->nested.msrs.secondary_ctls_high &
 +		SECONDARY_EXEC_UNRESTRICTED_GUEST &&
 +	    nested_cpu_has2(vmcs12, SECONDARY_EXEC_UNRESTRICTED_GUEST))
 +		fixed0 &= ~(X86_CR0_PE | X86_CR0_PG);
 +
 +	return fixed_bits_valid(val, fixed0, fixed1);
 +}
 +
 +static bool nested_host_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;
 +	u64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;
 +
 +	return fixed_bits_valid(val, fixed0, fixed1);
 +}
 +
 +static bool nested_cr4_valid(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;
 +	u64 fixed1 = to_vmx(vcpu)->nested.msrs.cr4_fixed1;
 +
 +	return fixed_bits_valid(val, fixed0, fixed1);
 +}
 +
 +/* No difference in the restrictions on guest and host CR4 in VMX operation. */
 +#define nested_guest_cr4_valid	nested_cr4_valid
 +#define nested_host_cr4_valid	nested_cr4_valid
 +
 +static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 +
 +static void ept_update_paging_mode_cr0(unsigned long *hw_cr0,
 +					unsigned long cr0,
 +					struct kvm_vcpu *vcpu)
 +{
 +	if (!test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
 +		vmx_decache_cr3(vcpu);
 +	if (!(cr0 & X86_CR0_PG)) {
 +		/* From paging/starting to nonpaging */
 +		vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
 +			     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) |
 +			     (CPU_BASED_CR3_LOAD_EXITING |
 +			      CPU_BASED_CR3_STORE_EXITING));
 +		vcpu->arch.cr0 = cr0;
 +		vmx_set_cr4(vcpu, kvm_read_cr4(vcpu));
 +	} else if (!is_paging(vcpu)) {
 +		/* From nonpaging to paging */
 +		vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
 +			     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) &
 +			     ~(CPU_BASED_CR3_LOAD_EXITING |
 +			       CPU_BASED_CR3_STORE_EXITING));
 +		vcpu->arch.cr0 = cr0;
 +		vmx_set_cr4(vcpu, kvm_read_cr4(vcpu));
 +	}
 +
 +	if (!(cr0 & X86_CR0_WP))
 +		*hw_cr0 &= ~X86_CR0_WP;
 +}
 +
 +static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned long hw_cr0;
 +
 +	hw_cr0 = (cr0 & ~KVM_VM_CR0_ALWAYS_OFF);
 +	if (enable_unrestricted_guest)
 +		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;
 +	else {
 +		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON;
 +
 +		if (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))
 +			enter_pmode(vcpu);
 +
 +		if (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))
 +			enter_rmode(vcpu);
 +	}
 +
 +#ifdef CONFIG_X86_64
 +	if (vcpu->arch.efer & EFER_LME) {
 +		if (!is_paging(vcpu) && (cr0 & X86_CR0_PG))
 +			enter_lmode(vcpu);
 +		if (is_paging(vcpu) && !(cr0 & X86_CR0_PG))
 +			exit_lmode(vcpu);
 +	}
 +#endif
 +
 +	if (enable_ept && !enable_unrestricted_guest)
 +		ept_update_paging_mode_cr0(&hw_cr0, cr0, vcpu);
 +
 +	vmcs_writel(CR0_READ_SHADOW, cr0);
 +	vmcs_writel(GUEST_CR0, hw_cr0);
 +	vcpu->arch.cr0 = cr0;
 +
 +	/* depends on vcpu->arch.cr0 to be set to a new value */
 +	vmx->emulation_required = emulation_required(vcpu);
 +}
 +
 +static int get_ept_level(struct kvm_vcpu *vcpu)
 +{
 +	if (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))
 +		return 5;
 +	return 4;
 +}
 +
 +static u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)
 +{
 +	u64 eptp = VMX_EPTP_MT_WB;
 +
 +	eptp |= (get_ept_level(vcpu) == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;
 +
 +	if (enable_ept_ad_bits &&
 +	    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))
 +		eptp |= VMX_EPTP_AD_ENABLE_BIT;
 +	eptp |= (root_hpa & PAGE_MASK);
 +
 +	return eptp;
 +}
 +
 +static void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 +{
 +	struct kvm *kvm = vcpu->kvm;
 +	unsigned long guest_cr3;
 +	u64 eptp;
 +
 +	guest_cr3 = cr3;
 +	if (enable_ept) {
 +		eptp = construct_eptp(vcpu, cr3);
 +		vmcs_write64(EPT_POINTER, eptp);
 +
 +		if (kvm_x86_ops->tlb_remote_flush) {
 +			spin_lock(&to_kvm_vmx(kvm)->ept_pointer_lock);
 +			to_vmx(vcpu)->ept_pointer = eptp;
 +			to_kvm_vmx(kvm)->ept_pointers_match
 +				= EPT_POINTERS_CHECK;
 +			spin_unlock(&to_kvm_vmx(kvm)->ept_pointer_lock);
 +		}
 +
 +		if (enable_unrestricted_guest || is_paging(vcpu) ||
 +		    is_guest_mode(vcpu))
 +			guest_cr3 = kvm_read_cr3(vcpu);
 +		else
 +			guest_cr3 = to_kvm_vmx(kvm)->ept_identity_map_addr;
 +		ept_load_pdptrs(vcpu);
 +	}
 +
 +	vmcs_writel(GUEST_CR3, guest_cr3);
 +}
 +
 +static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 +{
 +	/*
 +	 * Pass through host's Machine Check Enable value to hw_cr4, which
 +	 * is in force while we are in guest mode.  Do not let guests control
 +	 * this bit, even if host CR4.MCE == 0.
 +	 */
 +	unsigned long hw_cr4;
 +
 +	hw_cr4 = (cr4_read_shadow() & X86_CR4_MCE) | (cr4 & ~X86_CR4_MCE);
 +	if (enable_unrestricted_guest)
 +		hw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;
 +	else if (to_vmx(vcpu)->rmode.vm86_active)
 +		hw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;
 +	else
 +		hw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;
 +
 +	if (!boot_cpu_has(X86_FEATURE_UMIP) && vmx_umip_emulated()) {
 +		if (cr4 & X86_CR4_UMIP) {
 +			vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 +				SECONDARY_EXEC_DESC);
 +			hw_cr4 &= ~X86_CR4_UMIP;
 +		} else if (!is_guest_mode(vcpu) ||
 +			!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC))
 +			vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,
 +					SECONDARY_EXEC_DESC);
 +	}
 +
 +	if (cr4 & X86_CR4_VMXE) {
 +		/*
 +		 * To use VMXON (and later other VMX instructions), a guest
 +		 * must first be able to turn on cr4.VMXE (see handle_vmon()).
 +		 * So basically the check on whether to allow nested VMX
 +		 * is here.  We operate under the default treatment of SMM,
 +		 * so VMX cannot be enabled under SMM.
 +		 */
 +		if (!nested_vmx_allowed(vcpu) || is_smm(vcpu))
 +			return 1;
 +	}
 +
 +	if (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))
 +		return 1;
 +
 +	vcpu->arch.cr4 = cr4;
 +
 +	if (!enable_unrestricted_guest) {
 +		if (enable_ept) {
 +			if (!is_paging(vcpu)) {
 +				hw_cr4 &= ~X86_CR4_PAE;
 +				hw_cr4 |= X86_CR4_PSE;
 +			} else if (!(cr4 & X86_CR4_PAE)) {
 +				hw_cr4 &= ~X86_CR4_PAE;
 +			}
 +		}
 +
 +		/*
 +		 * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in
 +		 * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs
 +		 * to be manually disabled when guest switches to non-paging
 +		 * mode.
 +		 *
 +		 * If !enable_unrestricted_guest, the CPU is always running
 +		 * with CR0.PG=1 and CR4 needs to be modified.
 +		 * If enable_unrestricted_guest, the CPU automatically
 +		 * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.
 +		 */
 +		if (!is_paging(vcpu))
 +			hw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);
 +	}
 +
 +	vmcs_writel(CR4_READ_SHADOW, cr4);
 +	vmcs_writel(GUEST_CR4, hw_cr4);
 +	return 0;
 +}
 +
 +static void vmx_get_segment(struct kvm_vcpu *vcpu,
 +			    struct kvm_segment *var, int seg)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 ar;
 +
 +	if (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {
 +		*var = vmx->rmode.segs[seg];
 +		if (seg == VCPU_SREG_TR
 +		    || var->selector == vmx_read_guest_seg_selector(vmx, seg))
 +			return;
 +		var->base = vmx_read_guest_seg_base(vmx, seg);
 +		var->selector = vmx_read_guest_seg_selector(vmx, seg);
 +		return;
 +	}
 +	var->base = vmx_read_guest_seg_base(vmx, seg);
 +	var->limit = vmx_read_guest_seg_limit(vmx, seg);
 +	var->selector = vmx_read_guest_seg_selector(vmx, seg);
 +	ar = vmx_read_guest_seg_ar(vmx, seg);
 +	var->unusable = (ar >> 16) & 1;
 +	var->type = ar & 15;
 +	var->s = (ar >> 4) & 1;
 +	var->dpl = (ar >> 5) & 3;
 +	/*
 +	 * Some userspaces do not preserve unusable property. Since usable
 +	 * segment has to be present according to VMX spec we can use present
 +	 * property to amend userspace bug by making unusable segment always
 +	 * nonpresent. vmx_segment_access_rights() already marks nonpresent
 +	 * segment as unusable.
 +	 */
 +	var->present = !var->unusable;
 +	var->avl = (ar >> 12) & 1;
 +	var->l = (ar >> 13) & 1;
 +	var->db = (ar >> 14) & 1;
 +	var->g = (ar >> 15) & 1;
 +}
 +
 +static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)
 +{
 +	struct kvm_segment s;
 +
 +	if (to_vmx(vcpu)->rmode.vm86_active) {
 +		vmx_get_segment(vcpu, &s, seg);
 +		return s.base;
 +	}
 +	return vmx_read_guest_seg_base(to_vmx(vcpu), seg);
 +}
 +
 +static int vmx_get_cpl(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	if (unlikely(vmx->rmode.vm86_active))
 +		return 0;
 +	else {
 +		int ar = vmx_read_guest_seg_ar(vmx, VCPU_SREG_SS);
 +		return VMX_AR_DPL(ar);
 +	}
 +}
 +
 +static u32 vmx_segment_access_rights(struct kvm_segment *var)
 +{
 +	u32 ar;
 +
 +	if (var->unusable || !var->present)
 +		ar = 1 << 16;
 +	else {
 +		ar = var->type & 15;
 +		ar |= (var->s & 1) << 4;
 +		ar |= (var->dpl & 3) << 5;
 +		ar |= (var->present & 1) << 7;
 +		ar |= (var->avl & 1) << 12;
 +		ar |= (var->l & 1) << 13;
 +		ar |= (var->db & 1) << 14;
 +		ar |= (var->g & 1) << 15;
 +	}
 +
 +	return ar;
 +}
 +
 +static void vmx_set_segment(struct kvm_vcpu *vcpu,
 +			    struct kvm_segment *var, int seg)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
 +
 +	vmx_segment_cache_clear(vmx);
 +
 +	if (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {
 +		vmx->rmode.segs[seg] = *var;
 +		if (seg == VCPU_SREG_TR)
 +			vmcs_write16(sf->selector, var->selector);
 +		else if (var->s)
 +			fix_rmode_seg(seg, &vmx->rmode.segs[seg]);
 +		goto out;
 +	}
 +
 +	vmcs_writel(sf->base, var->base);
 +	vmcs_write32(sf->limit, var->limit);
 +	vmcs_write16(sf->selector, var->selector);
 +
 +	/*
 +	 *   Fix the "Accessed" bit in AR field of segment registers for older
 +	 * qemu binaries.
 +	 *   IA32 arch specifies that at the time of processor reset the
 +	 * "Accessed" bit in the AR field of segment registers is 1. And qemu
 +	 * is setting it to 0 in the userland code. This causes invalid guest
 +	 * state vmexit when "unrestricted guest" mode is turned on.
 +	 *    Fix for this setup issue in cpu_reset is being pushed in the qemu
 +	 * tree. Newer qemu binaries with that qemu fix would not need this
 +	 * kvm hack.
 +	 */
 +	if (enable_unrestricted_guest && (seg != VCPU_SREG_LDTR))
 +		var->type |= 0x1; /* Accessed */
 +
 +	vmcs_write32(sf->ar_bytes, vmx_segment_access_rights(var));
 +
 +out:
 +	vmx->emulation_required = emulation_required(vcpu);
 +}
 +
 +static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
 +{
 +	u32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);
 +
 +	*db = (ar >> 14) & 1;
 +	*l = (ar >> 13) & 1;
 +}
 +
 +static void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 +{
 +	dt->size = vmcs_read32(GUEST_IDTR_LIMIT);
 +	dt->address = vmcs_readl(GUEST_IDTR_BASE);
 +}
 +
 +static void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 +{
 +	vmcs_write32(GUEST_IDTR_LIMIT, dt->size);
 +	vmcs_writel(GUEST_IDTR_BASE, dt->address);
 +}
 +
 +static void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 +{
 +	dt->size = vmcs_read32(GUEST_GDTR_LIMIT);
 +	dt->address = vmcs_readl(GUEST_GDTR_BASE);
 +}
 +
 +static void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 +{
 +	vmcs_write32(GUEST_GDTR_LIMIT, dt->size);
 +	vmcs_writel(GUEST_GDTR_BASE, dt->address);
 +}
 +
 +static bool rmode_segment_valid(struct kvm_vcpu *vcpu, int seg)
 +{
 +	struct kvm_segment var;
 +	u32 ar;
 +
 +	vmx_get_segment(vcpu, &var, seg);
 +	var.dpl = 0x3;
 +	if (seg == VCPU_SREG_CS)
 +		var.type = 0x3;
 +	ar = vmx_segment_access_rights(&var);
 +
 +	if (var.base != (var.selector << 4))
 +		return false;
 +	if (var.limit != 0xffff)
 +		return false;
 +	if (ar != 0xf3)
 +		return false;
 +
 +	return true;
 +}
 +
 +static bool code_segment_valid(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_segment cs;
 +	unsigned int cs_rpl;
 +
 +	vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);
 +	cs_rpl = cs.selector & SEGMENT_RPL_MASK;
 +
 +	if (cs.unusable)
 +		return false;
 +	if (~cs.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_ACCESSES_MASK))
 +		return false;
 +	if (!cs.s)
 +		return false;
 +	if (cs.type & VMX_AR_TYPE_WRITEABLE_MASK) {
 +		if (cs.dpl > cs_rpl)
 +			return false;
 +	} else {
 +		if (cs.dpl != cs_rpl)
 +			return false;
 +	}
 +	if (!cs.present)
 +		return false;
 +
 +	/* TODO: Add Reserved field check, this'll require a new member in the kvm_segment_field structure */
 +	return true;
 +}
 +
 +static bool stack_segment_valid(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_segment ss;
 +	unsigned int ss_rpl;
 +
 +	vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);
 +	ss_rpl = ss.selector & SEGMENT_RPL_MASK;
 +
 +	if (ss.unusable)
 +		return true;
 +	if (ss.type != 3 && ss.type != 7)
 +		return false;
 +	if (!ss.s)
 +		return false;
 +	if (ss.dpl != ss_rpl) /* DPL != RPL */
 +		return false;
 +	if (!ss.present)
 +		return false;
 +
 +	return true;
 +}
 +
 +static bool data_segment_valid(struct kvm_vcpu *vcpu, int seg)
 +{
 +	struct kvm_segment var;
 +	unsigned int rpl;
 +
 +	vmx_get_segment(vcpu, &var, seg);
 +	rpl = var.selector & SEGMENT_RPL_MASK;
 +
 +	if (var.unusable)
 +		return true;
 +	if (!var.s)
 +		return false;
 +	if (!var.present)
 +		return false;
 +	if (~var.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_WRITEABLE_MASK)) {
 +		if (var.dpl < rpl) /* DPL < RPL */
 +			return false;
 +	}
 +
 +	/* TODO: Add other members to kvm_segment_field to allow checking for other access
 +	 * rights flags
 +	 */
 +	return true;
 +}
 +
 +static bool tr_valid(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_segment tr;
 +
 +	vmx_get_segment(vcpu, &tr, VCPU_SREG_TR);
 +
 +	if (tr.unusable)
 +		return false;
 +	if (tr.selector & SEGMENT_TI_MASK)	/* TI = 1 */
 +		return false;
 +	if (tr.type != 3 && tr.type != 11) /* TODO: Check if guest is in IA32e mode */
 +		return false;
 +	if (!tr.present)
 +		return false;
 +
 +	return true;
 +}
 +
 +static bool ldtr_valid(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_segment ldtr;
 +
 +	vmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);
 +
 +	if (ldtr.unusable)
 +		return true;
 +	if (ldtr.selector & SEGMENT_TI_MASK)	/* TI = 1 */
 +		return false;
 +	if (ldtr.type != 2)
 +		return false;
 +	if (!ldtr.present)
 +		return false;
 +
 +	return true;
 +}
 +
 +static bool cs_ss_rpl_check(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_segment cs, ss;
 +
 +	vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);
 +	vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);
 +
 +	return ((cs.selector & SEGMENT_RPL_MASK) ==
 +		 (ss.selector & SEGMENT_RPL_MASK));
 +}
 +
 +/*
 + * Check if guest state is valid. Returns true if valid, false if
 + * not.
 + * We assume that registers are always usable
 + */
 +static bool guest_state_valid(struct kvm_vcpu *vcpu)
 +{
 +	if (enable_unrestricted_guest)
 +		return true;
 +
 +	/* real mode guest state checks */
 +	if (!is_protmode(vcpu) || (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_CS))
 +			return false;
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_SS))
 +			return false;
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_DS))
 +			return false;
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_ES))
 +			return false;
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_FS))
 +			return false;
 +		if (!rmode_segment_valid(vcpu, VCPU_SREG_GS))
 +			return false;
 +	} else {
 +	/* protected mode guest state checks */
 +		if (!cs_ss_rpl_check(vcpu))
 +			return false;
 +		if (!code_segment_valid(vcpu))
 +			return false;
 +		if (!stack_segment_valid(vcpu))
 +			return false;
 +		if (!data_segment_valid(vcpu, VCPU_SREG_DS))
 +			return false;
 +		if (!data_segment_valid(vcpu, VCPU_SREG_ES))
 +			return false;
 +		if (!data_segment_valid(vcpu, VCPU_SREG_FS))
 +			return false;
 +		if (!data_segment_valid(vcpu, VCPU_SREG_GS))
 +			return false;
 +		if (!tr_valid(vcpu))
 +			return false;
 +		if (!ldtr_valid(vcpu))
 +			return false;
 +	}
 +	/* TODO:
 +	 * - Add checks on RIP
 +	 * - Add checks on RFLAGS
 +	 */
 +
 +	return true;
 +}
 +
 +static bool page_address_valid(struct kvm_vcpu *vcpu, gpa_t gpa)
 +{
 +	return PAGE_ALIGNED(gpa) && !(gpa >> cpuid_maxphyaddr(vcpu));
 +}
 +
 +static int init_rmode_tss(struct kvm *kvm)
 +{
 +	gfn_t fn;
 +	u16 data = 0;
 +	int idx, r;
 +
 +	idx = srcu_read_lock(&kvm->srcu);
 +	fn = to_kvm_vmx(kvm)->tss_addr >> PAGE_SHIFT;
 +	r = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);
 +	if (r < 0)
 +		goto out;
 +	data = TSS_BASE_SIZE + TSS_REDIRECTION_SIZE;
 +	r = kvm_write_guest_page(kvm, fn++, &data,
 +			TSS_IOPB_BASE_OFFSET, sizeof(u16));
 +	if (r < 0)
 +		goto out;
 +	r = kvm_clear_guest_page(kvm, fn++, 0, PAGE_SIZE);
 +	if (r < 0)
 +		goto out;
 +	r = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);
 +	if (r < 0)
 +		goto out;
 +	data = ~0;
 +	r = kvm_write_guest_page(kvm, fn, &data,
 +				 RMODE_TSS_SIZE - 2 * PAGE_SIZE - 1,
 +				 sizeof(u8));
 +out:
 +	srcu_read_unlock(&kvm->srcu, idx);
 +	return r;
 +}
 +
 +static int init_rmode_identity_map(struct kvm *kvm)
 +{
 +	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);
 +	int i, idx, r = 0;
 +	kvm_pfn_t identity_map_pfn;
 +	u32 tmp;
 +
 +	/* Protect kvm_vmx->ept_identity_pagetable_done. */
 +	mutex_lock(&kvm->slots_lock);
 +
 +	if (likely(kvm_vmx->ept_identity_pagetable_done))
 +		goto out2;
 +
 +	if (!kvm_vmx->ept_identity_map_addr)
 +		kvm_vmx->ept_identity_map_addr = VMX_EPT_IDENTITY_PAGETABLE_ADDR;
 +	identity_map_pfn = kvm_vmx->ept_identity_map_addr >> PAGE_SHIFT;
 +
 +	r = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
 +				    kvm_vmx->ept_identity_map_addr, PAGE_SIZE);
 +	if (r < 0)
 +		goto out2;
 +
 +	idx = srcu_read_lock(&kvm->srcu);
 +	r = kvm_clear_guest_page(kvm, identity_map_pfn, 0, PAGE_SIZE);
 +	if (r < 0)
 +		goto out;
 +	/* Set up identity-mapping pagetable for EPT in real mode */
 +	for (i = 0; i < PT32_ENT_PER_PAGE; i++) {
 +		tmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |
 +			_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);
 +		r = kvm_write_guest_page(kvm, identity_map_pfn,
 +				&tmp, i * sizeof(tmp), sizeof(tmp));
 +		if (r < 0)
 +			goto out;
 +	}
 +	kvm_vmx->ept_identity_pagetable_done = true;
 +
 +out:
 +	srcu_read_unlock(&kvm->srcu, idx);
 +
 +out2:
 +	mutex_unlock(&kvm->slots_lock);
 +	return r;
 +}
 +
 +static void seg_setup(int seg)
 +{
 +	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
 +	unsigned int ar;
 +
 +	vmcs_write16(sf->selector, 0);
 +	vmcs_writel(sf->base, 0);
 +	vmcs_write32(sf->limit, 0xffff);
 +	ar = 0x93;
 +	if (seg == VCPU_SREG_CS)
 +		ar |= 0x08; /* code segment */
 +
 +	vmcs_write32(sf->ar_bytes, ar);
 +}
 +
 +static int alloc_apic_access_page(struct kvm *kvm)
 +{
 +	struct page *page;
 +	int r = 0;
 +
 +	mutex_lock(&kvm->slots_lock);
 +	if (kvm->arch.apic_access_page_done)
 +		goto out;
 +	r = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 +				    APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);
 +	if (r)
 +		goto out;
 +
 +	page = gfn_to_page(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);
 +	if (is_error_page(page)) {
 +		r = -EFAULT;
 +		goto out;
 +	}
 +
 +	/*
 +	 * Do not pin the page in memory, so that memory hot-unplug
 +	 * is able to migrate it.
 +	 */
 +	put_page(page);
 +	kvm->arch.apic_access_page_done = true;
 +out:
 +	mutex_unlock(&kvm->slots_lock);
 +	return r;
 +}
 +
 +static int allocate_vpid(void)
 +{
 +	int vpid;
 +
 +	if (!enable_vpid)
 +		return 0;
 +	spin_lock(&vmx_vpid_lock);
 +	vpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);
 +	if (vpid < VMX_NR_VPIDS)
 +		__set_bit(vpid, vmx_vpid_bitmap);
 +	else
 +		vpid = 0;
 +	spin_unlock(&vmx_vpid_lock);
 +	return vpid;
 +}
 +
 +static void free_vpid(int vpid)
 +{
 +	if (!enable_vpid || vpid == 0)
 +		return;
 +	spin_lock(&vmx_vpid_lock);
 +	__clear_bit(vpid, vmx_vpid_bitmap);
 +	spin_unlock(&vmx_vpid_lock);
 +}
 +
 +static __always_inline void vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,
 +							  u32 msr, int type)
 +{
 +	int f = sizeof(unsigned long);
 +
 +	if (!cpu_has_vmx_msr_bitmap())
 +		return;
 +
 +	if (static_branch_unlikely(&enable_evmcs))
 +		evmcs_touch_msr_bitmap();
 +
 +	/*
 +	 * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals
 +	 * have the write-low and read-high bitmap offsets the wrong way round.
 +	 * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.
 +	 */
 +	if (msr <= 0x1fff) {
 +		if (type & MSR_TYPE_R)
 +			/* read-low */
 +			__clear_bit(msr, msr_bitmap + 0x000 / f);
 +
 +		if (type & MSR_TYPE_W)
 +			/* write-low */
 +			__clear_bit(msr, msr_bitmap + 0x800 / f);
 +
 +	} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {
 +		msr &= 0x1fff;
 +		if (type & MSR_TYPE_R)
 +			/* read-high */
 +			__clear_bit(msr, msr_bitmap + 0x400 / f);
 +
 +		if (type & MSR_TYPE_W)
 +			/* write-high */
 +			__clear_bit(msr, msr_bitmap + 0xc00 / f);
 +
 +	}
 +}
 +
 +static __always_inline void vmx_enable_intercept_for_msr(unsigned long *msr_bitmap,
 +							 u32 msr, int type)
 +{
 +	int f = sizeof(unsigned long);
 +
 +	if (!cpu_has_vmx_msr_bitmap())
 +		return;
 +
 +	if (static_branch_unlikely(&enable_evmcs))
 +		evmcs_touch_msr_bitmap();
 +
 +	/*
 +	 * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals
 +	 * have the write-low and read-high bitmap offsets the wrong way round.
 +	 * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.
 +	 */
 +	if (msr <= 0x1fff) {
 +		if (type & MSR_TYPE_R)
 +			/* read-low */
 +			__set_bit(msr, msr_bitmap + 0x000 / f);
 +
 +		if (type & MSR_TYPE_W)
 +			/* write-low */
 +			__set_bit(msr, msr_bitmap + 0x800 / f);
 +
 +	} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {
 +		msr &= 0x1fff;
 +		if (type & MSR_TYPE_R)
 +			/* read-high */
 +			__set_bit(msr, msr_bitmap + 0x400 / f);
 +
 +		if (type & MSR_TYPE_W)
 +			/* write-high */
 +			__set_bit(msr, msr_bitmap + 0xc00 / f);
 +
 +	}
 +}
 +
 +static __always_inline void vmx_set_intercept_for_msr(unsigned long *msr_bitmap,
 +			     			      u32 msr, int type, bool value)
 +{
 +	if (value)
 +		vmx_enable_intercept_for_msr(msr_bitmap, msr, type);
 +	else
 +		vmx_disable_intercept_for_msr(msr_bitmap, msr, type);
 +}
 +
 +/*
 + * If a msr is allowed by L0, we should check whether it is allowed by L1.
 + * The corresponding bit will be cleared unless both of L0 and L1 allow it.
 + */
 +static void nested_vmx_disable_intercept_for_msr(unsigned long *msr_bitmap_l1,
 +					       unsigned long *msr_bitmap_nested,
 +					       u32 msr, int type)
 +{
 +	int f = sizeof(unsigned long);
 +
 +	/*
 +	 * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals
 +	 * have the write-low and read-high bitmap offsets the wrong way round.
 +	 * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.
 +	 */
 +	if (msr <= 0x1fff) {
 +		if (type & MSR_TYPE_R &&
 +		   !test_bit(msr, msr_bitmap_l1 + 0x000 / f))
 +			/* read-low */
 +			__clear_bit(msr, msr_bitmap_nested + 0x000 / f);
 +
 +		if (type & MSR_TYPE_W &&
 +		   !test_bit(msr, msr_bitmap_l1 + 0x800 / f))
 +			/* write-low */
 +			__clear_bit(msr, msr_bitmap_nested + 0x800 / f);
 +
 +	} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {
 +		msr &= 0x1fff;
 +		if (type & MSR_TYPE_R &&
 +		   !test_bit(msr, msr_bitmap_l1 + 0x400 / f))
 +			/* read-high */
 +			__clear_bit(msr, msr_bitmap_nested + 0x400 / f);
 +
 +		if (type & MSR_TYPE_W &&
 +		   !test_bit(msr, msr_bitmap_l1 + 0xc00 / f))
 +			/* write-high */
 +			__clear_bit(msr, msr_bitmap_nested + 0xc00 / f);
 +
 +	}
 +}
 +
 +static u8 vmx_msr_bitmap_mode(struct kvm_vcpu *vcpu)
 +{
 +	u8 mode = 0;
 +
 +	if (cpu_has_secondary_exec_ctrls() &&
 +	    (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &
 +	     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {
 +		mode |= MSR_BITMAP_MODE_X2APIC;
 +		if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
 +			mode |= MSR_BITMAP_MODE_X2APIC_APICV;
 +	}
 +
 +	return mode;
 +}
 +
 +#define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))
 +
 +static void vmx_update_msr_bitmap_x2apic(unsigned long *msr_bitmap,
 +					 u8 mode)
 +{
 +	int msr;
 +
 +	for (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {
 +		unsigned word = msr / BITS_PER_LONG;
 +		msr_bitmap[word] = (mode & MSR_BITMAP_MODE_X2APIC_APICV) ? 0 : ~0;
 +		msr_bitmap[word + (0x800 / sizeof(long))] = ~0;
 +	}
 +
 +	if (mode & MSR_BITMAP_MODE_X2APIC) {
 +		/*
 +		 * TPR reads and writes can be virtualized even if virtual interrupt
 +		 * delivery is not in use.
 +		 */
 +		vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW);
 +		if (mode & MSR_BITMAP_MODE_X2APIC_APICV) {
 +			vmx_enable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_R);
 +			vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
 +			vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
 +		}
 +	}
 +}
 +
 +static void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;
 +	u8 mode = vmx_msr_bitmap_mode(vcpu);
 +	u8 changed = mode ^ vmx->msr_bitmap_mode;
 +
 +	if (!changed)
 +		return;
 +
 +	if (changed & (MSR_BITMAP_MODE_X2APIC | MSR_BITMAP_MODE_X2APIC_APICV))
 +		vmx_update_msr_bitmap_x2apic(msr_bitmap, mode);
 +
 +	vmx->msr_bitmap_mode = mode;
 +}
 +
 +static bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)
 +{
 +	return enable_apicv;
 +}
 +
 +static void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +	gfn_t gfn;
 +
 +	/*
 +	 * Don't need to mark the APIC access page dirty; it is never
 +	 * written to by the CPU during APIC virtualization.
 +	 */
 +
 +	if (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {
 +		gfn = vmcs12->virtual_apic_page_addr >> PAGE_SHIFT;
 +		kvm_vcpu_mark_page_dirty(vcpu, gfn);
 +	}
 +
 +	if (nested_cpu_has_posted_intr(vmcs12)) {
 +		gfn = vmcs12->posted_intr_desc_addr >> PAGE_SHIFT;
 +		kvm_vcpu_mark_page_dirty(vcpu, gfn);
 +	}
 +}
 +
 +
 +static void vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int max_irr;
 +	void *vapic_page;
 +	u16 status;
 +
 +	if (!vmx->nested.pi_desc || !vmx->nested.pi_pending)
 +		return;
 +
 +	vmx->nested.pi_pending = false;
 +	if (!pi_test_and_clear_on(vmx->nested.pi_desc))
 +		return;
 +
 +	max_irr = find_last_bit((unsigned long *)vmx->nested.pi_desc->pir, 256);
 +	if (max_irr != 256) {
 +		vapic_page = kmap(vmx->nested.virtual_apic_page);
 +		__kvm_apic_update_irr(vmx->nested.pi_desc->pir,
 +			vapic_page, &max_irr);
 +		kunmap(vmx->nested.virtual_apic_page);
 +
 +		status = vmcs_read16(GUEST_INTR_STATUS);
 +		if ((u8)max_irr > ((u8)status & 0xff)) {
 +			status &= ~0xff;
 +			status |= (u8)max_irr;
 +			vmcs_write16(GUEST_INTR_STATUS, status);
 +		}
 +	}
 +
 +	nested_mark_vmcs12_pages_dirty(vcpu);
 +}
 +
 +static u8 vmx_get_rvi(void)
 +{
 +	return vmcs_read16(GUEST_INTR_STATUS) & 0xff;
 +}
 +
 +static bool vmx_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	void *vapic_page;
 +	u32 vppr;
 +	int rvi;
 +
 +	if (WARN_ON_ONCE(!is_guest_mode(vcpu)) ||
 +		!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
 +		WARN_ON_ONCE(!vmx->nested.virtual_apic_page))
 +		return false;
 +
 +	rvi = vmx_get_rvi();
 +
 +	vapic_page = kmap(vmx->nested.virtual_apic_page);
 +	vppr = *((u32 *)(vapic_page + APIC_PROCPRI));
 +	kunmap(vmx->nested.virtual_apic_page);
 +
 +	return ((rvi & 0xf0) > (vppr & 0xf0));
 +}
 +
 +static inline bool kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,
 +						     bool nested)
 +{
 +#ifdef CONFIG_SMP
 +	int pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;
 +
 +	if (vcpu->mode == IN_GUEST_MODE) {
 +		/*
 +		 * The vector of interrupt to be delivered to vcpu had
 +		 * been set in PIR before this function.
 +		 *
 +		 * Following cases will be reached in this block, and
 +		 * we always send a notification event in all cases as
 +		 * explained below.
 +		 *
 +		 * Case 1: vcpu keeps in non-root mode. Sending a
 +		 * notification event posts the interrupt to vcpu.
 +		 *
 +		 * Case 2: vcpu exits to root mode and is still
 +		 * runnable. PIR will be synced to vIRR before the
 +		 * next vcpu entry. Sending a notification event in
 +		 * this case has no effect, as vcpu is not in root
 +		 * mode.
 +		 *
 +		 * Case 3: vcpu exits to root mode and is blocked.
 +		 * vcpu_block() has already synced PIR to vIRR and
 +		 * never blocks vcpu if vIRR is not cleared. Therefore,
 +		 * a blocked vcpu here does not wait for any requested
 +		 * interrupts in PIR, and sending a notification event
 +		 * which has no effect is safe here.
 +		 */
 +
 +		apic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);
 +		return true;
 +	}
 +#endif
 +	return false;
 +}
 +
 +static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
 +						int vector)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	if (is_guest_mode(vcpu) &&
 +	    vector == vmx->nested.posted_intr_nv) {
 +		/*
 +		 * If a posted intr is not recognized by hardware,
 +		 * we will accomplish it in the next vmentry.
 +		 */
 +		vmx->nested.pi_pending = true;
 +		kvm_make_request(KVM_REQ_EVENT, vcpu);
 +		/* the PIR and ON have been set by L1. */
 +		if (!kvm_vcpu_trigger_posted_interrupt(vcpu, true))
 +			kvm_vcpu_kick(vcpu);
 +		return 0;
 +	}
 +	return -1;
 +}
 +/*
 + * Send interrupt to vcpu via posted interrupt way.
 + * 1. If target vcpu is running(non-root mode), send posted interrupt
 + * notification to vcpu and hardware will sync PIR to vIRR atomically.
 + * 2. If target vcpu isn't running(root mode), kick it to pick up the
 + * interrupt from PIR in next vmentry.
 + */
 +static void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int r;
 +
 +	r = vmx_deliver_nested_posted_interrupt(vcpu, vector);
 +	if (!r)
 +		return;
 +
 +	if (pi_test_and_set_pir(vector, &vmx->pi_desc))
 +		return;
 +
 +	/* If a previous notification has sent the IPI, nothing to do.  */
 +	if (pi_test_and_set_on(&vmx->pi_desc))
 +		return;
 +
 +	if (!kvm_vcpu_trigger_posted_interrupt(vcpu, false))
 +		kvm_vcpu_kick(vcpu);
 +}
 +
 +/*
 + * Set up the vmcs's constant host-state fields, i.e., host-state fields that
 + * will not change in the lifetime of the guest.
 + * Note that host-state that does change is set elsewhere. E.g., host-state
 + * that is set differently for each CPU is set in vmx_vcpu_load(), not here.
 + */
 +static void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
 +{
 +	u32 low32, high32;
 +	unsigned long tmpl;
 +	struct desc_ptr dt;
 +	unsigned long cr0, cr3, cr4;
 +
 +	cr0 = read_cr0();
 +	WARN_ON(cr0 & X86_CR0_TS);
 +	vmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */
 +
 +	/*
 +	 * Save the most likely value for this task's CR3 in the VMCS.
 +	 * We can't use __get_current_cr3_fast() because we're not atomic.
 +	 */
 +	cr3 = __read_cr3();
 +	vmcs_writel(HOST_CR3, cr3);		/* 22.2.3  FIXME: shadow tables */
 +	vmx->loaded_vmcs->host_state.cr3 = cr3;
 +
 +	/* Save the most likely value for this task's CR4 in the VMCS. */
 +	cr4 = cr4_read_shadow();
 +	vmcs_writel(HOST_CR4, cr4);			/* 22.2.3, 22.2.5 */
 +	vmx->loaded_vmcs->host_state.cr4 = cr4;
 +
 +	vmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */
 +#ifdef CONFIG_X86_64
 +	/*
 +	 * Load null selectors, so we can avoid reloading them in
 +	 * vmx_prepare_switch_to_host(), in case userspace uses
 +	 * the null selectors too (the expected case).
 +	 */
 +	vmcs_write16(HOST_DS_SELECTOR, 0);
 +	vmcs_write16(HOST_ES_SELECTOR, 0);
 +#else
 +	vmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 +	vmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 +#endif
 +	vmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 +	vmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */
 +
 +	store_idt(&dt);
 +	vmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */
 +	vmx->host_idt_base = dt.address;
 +
 +	vmcs_writel(HOST_RIP, vmx_return); /* 22.2.5 */
 +
 +	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
 +	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
 +	rdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);
 +	vmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */
 +
 +	if (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {
 +		rdmsr(MSR_IA32_CR_PAT, low32, high32);
 +		vmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));
 +	}
 +
 +	if (cpu_has_load_ia32_efer())
 +		vmcs_write64(HOST_IA32_EFER, host_efer);
 +}
 +
 +static void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
 +{
 +	vmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;
 +	if (enable_ept)
 +		vmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;
 +	if (is_guest_mode(&vmx->vcpu))
 +		vmx->vcpu.arch.cr4_guest_owned_bits &=
 +			~get_vmcs12(&vmx->vcpu)->cr4_guest_host_mask;
 +	vmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);
 +}
 +
 +static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 +{
 +	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;
 +
 +	if (!kvm_vcpu_apicv_active(&vmx->vcpu))
 +		pin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;
 +
 +	if (!enable_vnmi)
 +		pin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;
 +
 +	/* Enable the preemption timer dynamically */
 +	pin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
 +	return pin_based_exec_ctrl;
 +}
 +
 +static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
 +	if (cpu_has_secondary_exec_ctrls()) {
 +		if (kvm_vcpu_apicv_active(vcpu))
 +			vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 +				      SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +				      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 +		else
 +			vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,
 +					SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +					SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 +	}
 +
 +	if (cpu_has_vmx_msr_bitmap())
 +		vmx_update_msr_bitmap(vcpu);
 +}
 +
 +static u32 vmx_vmentry_ctrl(void)
 +{
 +	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 +	return vmcs_config.vmentry_ctrl &
 +		~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL | VM_ENTRY_LOAD_IA32_EFER);
 +}
 +
 +static u32 vmx_vmexit_ctrl(void)
 +{
 +	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 +	return vmcs_config.vmexit_ctrl &
 +		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
 +}
 +
 +static u32 vmx_exec_control(struct vcpu_vmx *vmx)
 +{
 +	u32 exec_control = vmcs_config.cpu_based_exec_ctrl;
 +
 +	if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)
 +		exec_control &= ~CPU_BASED_MOV_DR_EXITING;
 +
 +	if (!cpu_need_tpr_shadow(&vmx->vcpu)) {
 +		exec_control &= ~CPU_BASED_TPR_SHADOW;
 +#ifdef CONFIG_X86_64
 +		exec_control |= CPU_BASED_CR8_STORE_EXITING |
 +				CPU_BASED_CR8_LOAD_EXITING;
 +#endif
 +	}
 +	if (!enable_ept)
 +		exec_control |= CPU_BASED_CR3_STORE_EXITING |
 +				CPU_BASED_CR3_LOAD_EXITING  |
 +				CPU_BASED_INVLPG_EXITING;
 +	if (kvm_mwait_in_guest(vmx->vcpu.kvm))
 +		exec_control &= ~(CPU_BASED_MWAIT_EXITING |
 +				CPU_BASED_MONITOR_EXITING);
 +	if (kvm_hlt_in_guest(vmx->vcpu.kvm))
 +		exec_control &= ~CPU_BASED_HLT_EXITING;
 +	return exec_control;
 +}
 +
 +static bool vmx_rdrand_supported(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_RDRAND_EXITING;
 +}
 +
 +static bool vmx_rdseed_supported(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_RDSEED_EXITING;
 +}
 +
 +static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 +{
 +	struct kvm_vcpu *vcpu = &vmx->vcpu;
 +
 +	u32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;
 +
++	if (pt_mode == PT_MODE_SYSTEM)
++		exec_control &= ~(SECONDARY_EXEC_PT_USE_GPA | SECONDARY_EXEC_PT_CONCEAL_VMX);
 +	if (!cpu_need_virtualize_apic_accesses(vcpu))
 +		exec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +	if (vmx->vpid == 0)
 +		exec_control &= ~SECONDARY_EXEC_ENABLE_VPID;
 +	if (!enable_ept) {
 +		exec_control &= ~SECONDARY_EXEC_ENABLE_EPT;
 +		enable_unrestricted_guest = 0;
 +	}
 +	if (!enable_unrestricted_guest)
 +		exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;
 +	if (kvm_pause_in_guest(vmx->vcpu.kvm))
 +		exec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |
 +				  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 +	exec_control &= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
 +
 +	/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,
 +	 * in vmx_set_cr4.  */
 +	exec_control &= ~SECONDARY_EXEC_DESC;
 +
 +	/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD
 +	   (handle_vmptrld).
 +	   We can NOT enable shadow_vmcs here because we don't have yet
 +	   a current VMCS12
 +	*/
 +	exec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;
 +
 +	if (!enable_pml)
 +		exec_control &= ~SECONDARY_EXEC_ENABLE_PML;
 +
 +	if (vmx_xsaves_supported()) {
 +		/* Exposing XSAVES only when XSAVE is exposed */
 +		bool xsaves_enabled =
 +			guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 +			guest_cpuid_has(vcpu, X86_FEATURE_XSAVES);
 +
 +		if (!xsaves_enabled)
 +			exec_control &= ~SECONDARY_EXEC_XSAVES;
 +
 +		if (nested) {
 +			if (xsaves_enabled)
 +				vmx->nested.msrs.secondary_ctls_high |=
 +					SECONDARY_EXEC_XSAVES;
 +			else
 +				vmx->nested.msrs.secondary_ctls_high &=
 +					~SECONDARY_EXEC_XSAVES;
 +		}
 +	}
 +
 +	if (vmx_rdtscp_supported()) {
 +		bool rdtscp_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP);
 +		if (!rdtscp_enabled)
 +			exec_control &= ~SECONDARY_EXEC_RDTSCP;
 +
 +		if (nested) {
 +			if (rdtscp_enabled)
 +				vmx->nested.msrs.secondary_ctls_high |=
 +					SECONDARY_EXEC_RDTSCP;
 +			else
 +				vmx->nested.msrs.secondary_ctls_high &=
 +					~SECONDARY_EXEC_RDTSCP;
 +		}
 +	}
 +
 +	if (vmx_invpcid_supported()) {
 +		/* Exposing INVPCID only when PCID is exposed */
 +		bool invpcid_enabled =
 +			guest_cpuid_has(vcpu, X86_FEATURE_INVPCID) &&
 +			guest_cpuid_has(vcpu, X86_FEATURE_PCID);
 +
 +		if (!invpcid_enabled) {
 +			exec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;
 +			guest_cpuid_clear(vcpu, X86_FEATURE_INVPCID);
 +		}
 +
 +		if (nested) {
 +			if (invpcid_enabled)
 +				vmx->nested.msrs.secondary_ctls_high |=
 +					SECONDARY_EXEC_ENABLE_INVPCID;
 +			else
 +				vmx->nested.msrs.secondary_ctls_high &=
 +					~SECONDARY_EXEC_ENABLE_INVPCID;
 +		}
 +	}
 +
 +	if (vmx_rdrand_supported()) {
 +		bool rdrand_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDRAND);
 +		if (rdrand_enabled)
 +			exec_control &= ~SECONDARY_EXEC_RDRAND_EXITING;
 +
 +		if (nested) {
 +			if (rdrand_enabled)
 +				vmx->nested.msrs.secondary_ctls_high |=
 +					SECONDARY_EXEC_RDRAND_EXITING;
 +			else
 +				vmx->nested.msrs.secondary_ctls_high &=
 +					~SECONDARY_EXEC_RDRAND_EXITING;
 +		}
 +	}
 +
 +	if (vmx_rdseed_supported()) {
 +		bool rdseed_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDSEED);
 +		if (rdseed_enabled)
 +			exec_control &= ~SECONDARY_EXEC_RDSEED_EXITING;
 +
 +		if (nested) {
 +			if (rdseed_enabled)
 +				vmx->nested.msrs.secondary_ctls_high |=
 +					SECONDARY_EXEC_RDSEED_EXITING;
 +			else
 +				vmx->nested.msrs.secondary_ctls_high &=
 +					~SECONDARY_EXEC_RDSEED_EXITING;
 +		}
 +	}
 +
 +	vmx->secondary_exec_control = exec_control;
 +}
 +
 +static void ept_set_mmio_spte_mask(void)
 +{
 +	/*
 +	 * EPT Misconfigurations can be generated if the value of bits 2:0
 +	 * of an EPT paging-structure entry is 110b (write/execute).
 +	 */
 +	kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,
 +				   VMX_EPT_MISCONFIG_WX_VALUE);
 +}
 +
 +#define VMX_XSS_EXIT_BITMAP 0
 +
 +static void nested_vmx_vcpu_setup(void)
 +{
 +	if (enable_shadow_vmcs) {
 +		/*
 +		 * At vCPU creation, "VMWRITE to any supported field
 +		 * in the VMCS" is supported, so use the more
 +		 * permissive vmx_vmread_bitmap to specify both read
 +		 * and write permissions for the shadow VMCS.
 +		 */
 +		vmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));
 +		vmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmread_bitmap));
 +	}
 +}
 +
 +/*
 + * Sets up the vmcs for emulated real mode.
 + */
 +static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 +{
 +	int i;
 +
 +	if (nested)
 +		nested_vmx_vcpu_setup();
 +
 +	if (cpu_has_vmx_msr_bitmap())
 +		vmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));
 +
 +	vmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */
 +
 +	/* Control */
 +	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
 +	vmx->hv_deadline_tsc = -1;
 +
 +	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL, vmx_exec_control(vmx));
 +
 +	if (cpu_has_secondary_exec_ctrls()) {
 +		vmx_compute_secondary_exec_control(vmx);
 +		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
 +			     vmx->secondary_exec_control);
 +	}
 +
 +	if (kvm_vcpu_apicv_active(&vmx->vcpu)) {
 +		vmcs_write64(EOI_EXIT_BITMAP0, 0);
 +		vmcs_write64(EOI_EXIT_BITMAP1, 0);
 +		vmcs_write64(EOI_EXIT_BITMAP2, 0);
 +		vmcs_write64(EOI_EXIT_BITMAP3, 0);
 +
 +		vmcs_write16(GUEST_INTR_STATUS, 0);
 +
 +		vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
 +		vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
 +	}
 +
 +	if (!kvm_pause_in_guest(vmx->vcpu.kvm)) {
 +		vmcs_write32(PLE_GAP, ple_gap);
 +		vmx->ple_window = ple_window;
 +		vmx->ple_window_dirty = true;
 +	}
 +
 +	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
 +	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
 +	vmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */
 +
 +	vmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */
 +	vmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */
 +	vmx_set_constant_host_state(vmx);
 +	vmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */
 +	vmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */
 +
 +	if (cpu_has_vmx_vmfunc())
 +		vmcs_write64(VM_FUNCTION_CONTROL, 0);
 +
 +	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
 +	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
 +	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
 +	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
 +	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
 +
 +	if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)
 +		vmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);
 +
 +	for (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i) {
 +		u32 index = vmx_msr_index[i];
 +		u32 data_low, data_high;
 +		int j = vmx->nmsrs;
 +
 +		if (rdmsr_safe(index, &data_low, &data_high) < 0)
 +			continue;
 +		if (wrmsr_safe(index, data_low, data_high) < 0)
 +			continue;
 +		vmx->guest_msrs[j].index = i;
 +		vmx->guest_msrs[j].data = 0;
 +		vmx->guest_msrs[j].mask = -1ull;
 +		++vmx->nmsrs;
 +	}
 +
 +	vmx->arch_capabilities = kvm_get_arch_capabilities();
 +
 +	vm_exit_controls_init(vmx, vmx_vmexit_ctrl());
 +
 +	/* 22.2.1, 20.8.1 */
 +	vm_entry_controls_init(vmx, vmx_vmentry_ctrl());
 +
 +	vmx->vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;
 +	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
 +
 +	set_cr4_guest_host_mask(vmx);
 +
 +	if (vmx_xsaves_supported())
 +		vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);
 +
 +	if (enable_pml) {
 +		vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));
 +		vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
 +	}
 +
 +	if (cpu_has_vmx_encls_vmexit())
 +		vmcs_write64(ENCLS_EXITING_BITMAP, -1ull);
 +}
 +
 +static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct msr_data apic_base_msr;
 +	u64 cr0;
 +
 +	vmx->rmode.vm86_active = 0;
 +	vmx->spec_ctrl = 0;
 +
 +	vcpu->arch.microcode_version = 0x100000000ULL;
 +	vmx->vcpu.arch.regs[VCPU_REGS_RDX] = get_rdx_init_val();
 +	kvm_set_cr8(vcpu, 0);
 +
 +	if (!init_event) {
 +		apic_base_msr.data = APIC_DEFAULT_PHYS_BASE |
 +				     MSR_IA32_APICBASE_ENABLE;
 +		if (kvm_vcpu_is_reset_bsp(vcpu))
 +			apic_base_msr.data |= MSR_IA32_APICBASE_BSP;
 +		apic_base_msr.host_initiated = true;
 +		kvm_set_apic_base(vcpu, &apic_base_msr);
 +	}
 +
 +	vmx_segment_cache_clear(vmx);
 +
 +	seg_setup(VCPU_SREG_CS);
 +	vmcs_write16(GUEST_CS_SELECTOR, 0xf000);
 +	vmcs_writel(GUEST_CS_BASE, 0xffff0000ul);
 +
 +	seg_setup(VCPU_SREG_DS);
 +	seg_setup(VCPU_SREG_ES);
 +	seg_setup(VCPU_SREG_FS);
 +	seg_setup(VCPU_SREG_GS);
 +	seg_setup(VCPU_SREG_SS);
 +
 +	vmcs_write16(GUEST_TR_SELECTOR, 0);
 +	vmcs_writel(GUEST_TR_BASE, 0);
 +	vmcs_write32(GUEST_TR_LIMIT, 0xffff);
 +	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);
 +
 +	vmcs_write16(GUEST_LDTR_SELECTOR, 0);
 +	vmcs_writel(GUEST_LDTR_BASE, 0);
 +	vmcs_write32(GUEST_LDTR_LIMIT, 0xffff);
 +	vmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);
 +
 +	if (!init_event) {
 +		vmcs_write32(GUEST_SYSENTER_CS, 0);
 +		vmcs_writel(GUEST_SYSENTER_ESP, 0);
 +		vmcs_writel(GUEST_SYSENTER_EIP, 0);
 +		vmcs_write64(GUEST_IA32_DEBUGCTL, 0);
 +	}
 +
 +	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
 +	kvm_rip_write(vcpu, 0xfff0);
 +
 +	vmcs_writel(GUEST_GDTR_BASE, 0);
 +	vmcs_write32(GUEST_GDTR_LIMIT, 0xffff);
  
 -		if (data & ~PRED_CMD_IBPB)
 -			return 1;
 +	vmcs_writel(GUEST_IDTR_BASE, 0);
 +	vmcs_write32(GUEST_IDTR_LIMIT, 0xffff);
  
 -		if (!data)
 -			break;
 +	vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);
 +	vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);
 +	vmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);
 +	if (kvm_mpx_supported())
 +		vmcs_write64(GUEST_BNDCFGS, 0);
  
 -		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);
 +	setup_msrs(vmx);
  
 -		/*
 -		 * For non-nested:
 -		 * When it's written (to non-zero) for the first time, pass
 -		 * it through.
 -		 *
 -		 * For nested:
 -		 * The handling of the MSR bitmap for L2 guests is done in
 -		 * nested_vmx_merge_msr_bitmap. We should not touch the
 -		 * vmcs02.msr_bitmap here since it gets completely overwritten
 -		 * in the merging.
 -		 */
 -		vmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap, MSR_IA32_PRED_CMD,
 -					      MSR_TYPE_W);
 -		break;
 -	case MSR_IA32_ARCH_CAPABILITIES:
 -		if (!msr_info->host_initiated)
 -			return 1;
 -		vmx->arch_capabilities = data;
 -		break;
 -	case MSR_IA32_CR_PAT:
 -		if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {
 -			if (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))
 -				return 1;
 -			vmcs_write64(GUEST_IA32_PAT, data);
 -			vcpu->arch.pat = data;
 -			break;
 -		}
 -		ret = kvm_set_msr_common(vcpu, msr_info);
 -		break;
 -	case MSR_IA32_TSC_ADJUST:
 -		ret = kvm_set_msr_common(vcpu, msr_info);
 -		break;
 -	case MSR_IA32_MCG_EXT_CTL:
 -		if ((!msr_info->host_initiated &&
 -		     !(to_vmx(vcpu)->msr_ia32_feature_control &
 -		       FEATURE_CONTROL_LMCE)) ||
 -		    (data & ~MCG_EXT_CTL_LMCE_EN))
 -			return 1;
 -		vcpu->arch.mcg_ext_ctl = data;
 -		break;
 -	case MSR_IA32_FEATURE_CONTROL:
 -		if (!vmx_feature_control_msr_valid(vcpu, data) ||
 -		    (to_vmx(vcpu)->msr_ia32_feature_control &
 -		     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))
 -			return 1;
 -		vmx->msr_ia32_feature_control = data;
 -		if (msr_info->host_initiated && data == 0)
 -			vmx_leave_nested(vcpu);
 -		break;
 -	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 -		if (!msr_info->host_initiated)
 -			return 1; /* they are read-only */
 -		if (!nested_vmx_allowed(vcpu))
 -			return 1;
 -		return vmx_set_vmx_msr(vcpu, msr_index, data);
 -	case MSR_IA32_XSS:
 -		if (!vmx_xsaves_supported())
 -			return 1;
 -		/*
 -		 * The only supported bit as of Skylake is bit 8, but
 -		 * it is not supported on KVM.
 -		 */
 -		if (data != 0)
 -			return 1;
 -		vcpu->arch.ia32_xss = data;
 -		if (vcpu->arch.ia32_xss != host_xss)
 -			add_atomic_switch_msr(vmx, MSR_IA32_XSS,
 -				vcpu->arch.ia32_xss, host_xss, false);
 -		else
 -			clear_atomic_switch_msr(vmx, MSR_IA32_XSS);
 -		break;
 -	case MSR_TSC_AUX:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))
 -			return 1;
 -		/* Check reserved bit, higher 32 bits should be zero */
 -		if ((data >> 32) != 0)
 -			return 1;
 -		/* Otherwise falls through */
 -	default:
 -		msr = find_msr_entry(vmx, msr_index);
 -		if (msr) {
 -			u64 old_msr_data = msr->data;
 -			msr->data = data;
 -			if (msr - vmx->guest_msrs < vmx->save_nmsrs) {
 -				preempt_disable();
 -				ret = kvm_set_shared_msr(msr->index, msr->data,
 -							 msr->mask);
 -				preempt_enable();
 -				if (ret)
 -					msr->data = old_msr_data;
 -			}
 -			break;
 -		}
 -		ret = kvm_set_msr_common(vcpu, msr_info);
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */
 +
 +	if (cpu_has_vmx_tpr_shadow() && !init_event) {
 +		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
 +		if (cpu_need_tpr_shadow(vcpu))
 +			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
 +				     __pa(vcpu->arch.apic->regs));
 +		vmcs_write32(TPR_THRESHOLD, 0);
  	}
  
 -	return ret;
 +	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 +
 +	if (vmx->vpid != 0)
 +		vmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);
 +
 +	cr0 = X86_CR0_NW | X86_CR0_CD | X86_CR0_ET;
 +	vmx->vcpu.arch.cr0 = cr0;
 +	vmx_set_cr0(vcpu, cr0); /* enter rmode */
 +	vmx_set_cr4(vcpu, 0);
 +	vmx_set_efer(vcpu, 0);
 +
 +	update_exception_bitmap(vcpu);
 +
 +	vpid_sync_context(vmx->vpid);
 +	if (init_event)
 +		vmx_clear_hlt(vcpu);
  }
  
 -static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 +/*
 + * In nested virtualization, check if L1 asked to exit on external interrupts.
 + * For most existing hypervisors, this will always return true.
 + */
 +static bool nested_exit_on_intr(struct kvm_vcpu *vcpu)
  {
 -	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
 -	switch (reg) {
 -	case VCPU_REGS_RSP:
 -		vcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);
 -		break;
 -	case VCPU_REGS_RIP:
 -		vcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
 -		break;
 -	case VCPU_EXREG_PDPTR:
 -		if (enable_ept)
 -			ept_save_pdptrs(vcpu);
 -		break;
 -	default:
 -		break;
 -	}
 +	return get_vmcs12(vcpu)->pin_based_vm_exec_control &
 +		PIN_BASED_EXT_INTR_MASK;
  }
  
 -static __init int cpu_has_kvm_support(void)
 +/*
 + * In nested virtualization, check if L1 has set
 + * VM_EXIT_ACK_INTR_ON_EXIT
 + */
 +static bool nested_exit_intr_ack_set(struct kvm_vcpu *vcpu)
  {
 -	return cpu_has_vmx();
 +	return get_vmcs12(vcpu)->vm_exit_controls &
 +		VM_EXIT_ACK_INTR_ON_EXIT;
  }
  
 -static __init int vmx_disabled_by_bios(void)
 +static bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)
  {
 -	u64 msr;
 -
 -	rdmsrl(MSR_IA32_FEATURE_CONTROL, msr);
 -	if (msr & FEATURE_CONTROL_LOCKED) {
 -		/* launched w/ TXT and VMX disabled */
 -		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)
 -			&& tboot_enabled())
 -			return 1;
 -		/* launched w/o TXT and VMX only enabled w/ TXT */
 -		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)
 -			&& (msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)
 -			&& !tboot_enabled()) {
 -			printk(KERN_WARNING "kvm: disable TXT in the BIOS or "
 -				"activate TXT before enabling KVM\n");
 -			return 1;
 -		}
 -		/* launched w/o TXT and VMX disabled */
 -		if (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)
 -			&& !tboot_enabled())
 -			return 1;
 -	}
 -
 -	return 0;
 +	return nested_cpu_has_nmi_exiting(get_vmcs12(vcpu));
  }
  
 -static void kvm_cpu_vmxon(u64 addr)
 +static void enable_irq_window(struct kvm_vcpu *vcpu)
  {
 -	cr4_set_bits(X86_CR4_VMXE);
 -	intel_pt_handle_vmx(1);
 -
 -	asm volatile ("vmxon %0" : : "m"(addr));
 +	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 +		      CPU_BASED_VIRTUAL_INTR_PENDING);
  }
  
 -static int hardware_enable(void)
 +static void enable_nmi_window(struct kvm_vcpu *vcpu)
  {
 -	int cpu = raw_smp_processor_id();
 -	u64 phys_addr = __pa(per_cpu(vmxarea, cpu));
 -	u64 old, test_bits;
 -
 -	if (cr4_read_shadow() & X86_CR4_VMXE)
 -		return -EBUSY;
 -
 -	/*
 -	 * This can happen if we hot-added a CPU but failed to allocate
 -	 * VP assist page for it.
 -	 */
 -	if (static_branch_unlikely(&enable_evmcs) &&
 -	    !hv_get_vp_assist_page(cpu))
 -		return -EFAULT;
 -
 -	INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
 -	INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
 -	spin_lock_init(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 +	if (!enable_vnmi ||
 +	    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {
 +		enable_irq_window(vcpu);
 +		return;
 +	}
  
 -	/*
 -	 * Now we can enable the vmclear operation in kdump
 -	 * since the loaded_vmcss_on_cpu list on this cpu
 -	 * has been initialized.
 -	 *
 -	 * Though the cpu is not in VMX operation now, there
 -	 * is no problem to enable the vmclear operation
 -	 * for the loaded_vmcss_on_cpu list is empty!
 -	 */
 -	crash_enable_local_vmclear(cpu);
 +	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 +		      CPU_BASED_VIRTUAL_NMI_PENDING);
 +}
  
 -	rdmsrl(MSR_IA32_FEATURE_CONTROL, old);
 +static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	uint32_t intr;
 +	int irq = vcpu->arch.interrupt.nr;
  
 -	test_bits = FEATURE_CONTROL_LOCKED;
 -	test_bits |= FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;
 -	if (tboot_enabled())
 -		test_bits |= FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX;
 +	trace_kvm_inj_virq(irq);
  
 -	if ((old & test_bits) != test_bits) {
 -		/* enable and lock */
 -		wrmsrl(MSR_IA32_FEATURE_CONTROL, old | test_bits);
 +	++vcpu->stat.irq_injections;
 +	if (vmx->rmode.vm86_active) {
 +		int inc_eip = 0;
 +		if (vcpu->arch.interrupt.soft)
 +			inc_eip = vcpu->arch.event_exit_inst_len;
 +		if (kvm_inject_realmode_interrupt(vcpu, irq, inc_eip) != EMULATE_DONE)
 +			kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 +		return;
  	}
 -	kvm_cpu_vmxon(phys_addr);
 -	if (enable_ept)
 -		ept_sync_global();
 +	intr = irq | INTR_INFO_VALID_MASK;
 +	if (vcpu->arch.interrupt.soft) {
 +		intr |= INTR_TYPE_SOFT_INTR;
 +		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,
 +			     vmx->vcpu.arch.event_exit_inst_len);
 +	} else
 +		intr |= INTR_TYPE_EXT_INTR;
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
  
 -	return 0;
 +	vmx_clear_hlt(vcpu);
  }
  
 -static void vmclear_local_loaded_vmcss(void)
 +static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
  {
 -	int cpu = raw_smp_processor_id();
 -	struct loaded_vmcs *v, *n;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -	list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
 -				 loaded_vmcss_on_cpu_link)
 -		__loaded_vmcs_clear(v);
 -}
 +	if (!enable_vnmi) {
 +		/*
 +		 * Tracking the NMI-blocked state in software is built upon
 +		 * finding the next open IRQ window. This, in turn, depends on
 +		 * well-behaving guests: They have to keep IRQs disabled at
 +		 * least as long as the NMI handler runs. Otherwise we may
 +		 * cause NMI nesting, maybe breaking the guest. But as this is
 +		 * highly unlikely, we can live with the residual risk.
 +		 */
 +		vmx->loaded_vmcs->soft_vnmi_blocked = 1;
 +		vmx->loaded_vmcs->vnmi_blocked_time = 0;
 +	}
  
 +	++vcpu->stat.nmi_injections;
 +	vmx->loaded_vmcs->nmi_known_unmasked = false;
  
 -/* Just like cpu_vmxoff(), but with the __kvm_handle_fault_on_reboot()
 - * tricks.
 - */
 -static void kvm_cpu_vmxoff(void)
 -{
 -	asm volatile (__ex("vmxoff"));
 +	if (vmx->rmode.vm86_active) {
 +		if (kvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0) != EMULATE_DONE)
 +			kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 +		return;
 +	}
  
 -	intel_pt_handle_vmx(0);
 -	cr4_clear_bits(X86_CR4_VMXE);
 -}
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 +			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
  
 -static void hardware_disable(void)
 -{
 -	vmclear_local_loaded_vmcss();
 -	kvm_cpu_vmxoff();
 +	vmx_clear_hlt(vcpu);
  }
  
 -static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
 -				      u32 msr, u32 *result)
 +static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
  {
 -	u32 vmx_msr_low, vmx_msr_high;
 -	u32 ctl = ctl_min | ctl_opt;
 -
 -	rdmsr(msr, vmx_msr_low, vmx_msr_high);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	bool masked;
  
 -	ctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */
 -	ctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */
 +	if (!enable_vnmi)
 +		return vmx->loaded_vmcs->soft_vnmi_blocked;
 +	if (vmx->loaded_vmcs->nmi_known_unmasked)
 +		return false;
 +	masked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;
 +	vmx->loaded_vmcs->nmi_known_unmasked = !masked;
 +	return masked;
 +}
  
 -	/* Ensure minimum (required) set of control bits are supported. */
 -	if (ctl_min & ~ctl)
 -		return -EIO;
 +static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -	*result = ctl;
 -	return 0;
 +	if (!enable_vnmi) {
 +		if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {
 +			vmx->loaded_vmcs->soft_vnmi_blocked = masked;
 +			vmx->loaded_vmcs->vnmi_blocked_time = 0;
 +		}
 +	} else {
 +		vmx->loaded_vmcs->nmi_known_unmasked = !masked;
 +		if (masked)
 +			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				      GUEST_INTR_STATE_NMI);
 +		else
 +			vmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,
 +					GUEST_INTR_STATE_NMI);
 +	}
  }
  
 -static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 -				    struct vmx_capability *vmx_cap)
 +static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
  {
 -	u32 vmx_msr_low, vmx_msr_high;
 -	u32 min, opt, min2, opt2;
 -	u32 _pin_based_exec_control = 0;
 -	u32 _cpu_based_exec_control = 0;
 -	u32 _cpu_based_2nd_exec_control = 0;
 -	u32 _vmexit_control = 0;
 -	u32 _vmentry_control = 0;
 -
 -	memset(vmcs_conf, 0, sizeof(*vmcs_conf));
 -	min = CPU_BASED_HLT_EXITING |
 -#ifdef CONFIG_X86_64
 -	      CPU_BASED_CR8_LOAD_EXITING |
 -	      CPU_BASED_CR8_STORE_EXITING |
 -#endif
 -	      CPU_BASED_CR3_LOAD_EXITING |
 -	      CPU_BASED_CR3_STORE_EXITING |
 -	      CPU_BASED_UNCOND_IO_EXITING |
 -	      CPU_BASED_MOV_DR_EXITING |
 -	      CPU_BASED_USE_TSC_OFFSETING |
 -	      CPU_BASED_MWAIT_EXITING |
 -	      CPU_BASED_MONITOR_EXITING |
 -	      CPU_BASED_INVLPG_EXITING |
 -	      CPU_BASED_RDPMC_EXITING;
 +	if (to_vmx(vcpu)->nested.nested_run_pending)
 +		return 0;
  
 -	opt = CPU_BASED_TPR_SHADOW |
 -	      CPU_BASED_USE_MSR_BITMAPS |
 -	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
 -	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
 -				&_cpu_based_exec_control) < 0)
 -		return -EIO;
 -#ifdef CONFIG_X86_64
 -	if ((_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 -		_cpu_based_exec_control &= ~CPU_BASED_CR8_LOAD_EXITING &
 -					   ~CPU_BASED_CR8_STORE_EXITING;
 -#endif
 -	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
 -		min2 = 0;
 -		opt2 = SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 -			SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 -			SECONDARY_EXEC_WBINVD_EXITING |
 -			SECONDARY_EXEC_ENABLE_VPID |
 -			SECONDARY_EXEC_ENABLE_EPT |
 -			SECONDARY_EXEC_UNRESTRICTED_GUEST |
 -			SECONDARY_EXEC_PAUSE_LOOP_EXITING |
 -			SECONDARY_EXEC_DESC |
 -			SECONDARY_EXEC_RDTSCP |
 -			SECONDARY_EXEC_ENABLE_INVPCID |
 -			SECONDARY_EXEC_APIC_REGISTER_VIRT |
 -			SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
 -			SECONDARY_EXEC_SHADOW_VMCS |
 -			SECONDARY_EXEC_XSAVES |
 -			SECONDARY_EXEC_RDSEED_EXITING |
 -			SECONDARY_EXEC_RDRAND_EXITING |
 -			SECONDARY_EXEC_ENABLE_PML |
 -			SECONDARY_EXEC_TSC_SCALING |
 -			SECONDARY_EXEC_PT_USE_GPA |
 -			SECONDARY_EXEC_PT_CONCEAL_VMX |
 -			SECONDARY_EXEC_ENABLE_VMFUNC |
 -			SECONDARY_EXEC_ENCLS_EXITING;
 -		if (adjust_vmx_controls(min2, opt2,
 -					MSR_IA32_VMX_PROCBASED_CTLS2,
 -					&_cpu_based_2nd_exec_control) < 0)
 -			return -EIO;
 -	}
 -#ifndef CONFIG_X86_64
 -	if (!(_cpu_based_2nd_exec_control &
 -				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
 -		_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;
 -#endif
 +	if (!enable_vnmi &&
 +	    to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)
 +		return 0;
  
 -	if (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 -		_cpu_based_2nd_exec_control &= ~(
 -				SECONDARY_EXEC_APIC_REGISTER_VIRT |
 -				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 -				SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 +	return	!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &
 +		  (GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI
 +		   | GUEST_INTR_STATE_NMI));
 +}
  
 -	rdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,
 -		&vmx_cap->ept, &vmx_cap->vpid);
 +static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 +{
 +	return (!to_vmx(vcpu)->nested.nested_run_pending &&
 +		vmcs_readl(GUEST_RFLAGS) & X86_EFLAGS_IF) &&
 +		!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &
 +			(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
 +}
  
 -	if (_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) {
 -		/* CR3 accesses and invlpg don't need to cause VM Exits when EPT
 -		   enabled */
 -		_cpu_based_exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |
 -					     CPU_BASED_CR3_STORE_EXITING |
 -					     CPU_BASED_INVLPG_EXITING);
 -	} else if (vmx_cap->ept) {
 -		vmx_cap->ept = 0;
 -		pr_warn_once("EPT CAP should not exist if not support "
 -				"1-setting enable EPT VM-execution control\n");
 -	}
 -	if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&
 -		vmx_cap->vpid) {
 -		vmx_cap->vpid = 0;
 -		pr_warn_once("VPID CAP should not exist if not support "
 -				"1-setting enable VPID VM-execution control\n");
 -	}
 +static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +{
 +	int ret;
  
 -	min = VM_EXIT_SAVE_DEBUG_CONTROLS | VM_EXIT_ACK_INTR_ON_EXIT;
 -#ifdef CONFIG_X86_64
 -	min |= VM_EXIT_HOST_ADDR_SPACE_SIZE;
 -#endif
 -	opt = VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL |
 -	      VM_EXIT_SAVE_IA32_PAT |
 -	      VM_EXIT_LOAD_IA32_PAT |
 -	      VM_EXIT_LOAD_IA32_EFER |
 -	      VM_EXIT_CLEAR_BNDCFGS |
 -	      VM_EXIT_PT_CONCEAL_PIP |
 -	      VM_EXIT_CLEAR_IA32_RTIT_CTL;
 -	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
 -				&_vmexit_control) < 0)
 -		return -EIO;
 +	if (enable_unrestricted_guest)
 +		return 0;
  
 -	min = PIN_BASED_EXT_INTR_MASK | PIN_BASED_NMI_EXITING;
 -	opt = PIN_BASED_VIRTUAL_NMIS | PIN_BASED_POSTED_INTR |
 -		 PIN_BASED_VMX_PREEMPTION_TIMER;
 -	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
 -				&_pin_based_exec_control) < 0)
 -		return -EIO;
 +	ret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
 +				    PAGE_SIZE * 3);
 +	if (ret)
 +		return ret;
 +	to_kvm_vmx(kvm)->tss_addr = addr;
 +	return init_rmode_tss(kvm);
 +}
  
 -	if (cpu_has_broken_vmx_preemption_timer())
 -		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
 -	if (!(_cpu_based_2nd_exec_control &
 -		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
 -		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
 +static int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 +{
 +	to_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;
 +	return 0;
 +}
  
 -	min = VM_ENTRY_LOAD_DEBUG_CONTROLS;
 -	opt = VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
 -	      VM_ENTRY_LOAD_IA32_PAT |
 -	      VM_ENTRY_LOAD_IA32_EFER |
 -	      VM_ENTRY_LOAD_BNDCFGS |
 -	      VM_ENTRY_PT_CONCEAL_PIP |
 -	      VM_ENTRY_LOAD_IA32_RTIT_CTL;
 -	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
 -				&_vmentry_control) < 0)
 -		return -EIO;
 +static bool rmode_exception(struct kvm_vcpu *vcpu, int vec)
 +{
 +	switch (vec) {
 +	case BP_VECTOR:
 +		/*
 +		 * Update instruction length as we may reinject the exception
 +		 * from user space while in guest debugging mode.
 +		 */
 +		to_vmx(vcpu)->vcpu.arch.event_exit_inst_len =
 +			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 +		if (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 +			return false;
 +		/* fall through */
 +	case DB_VECTOR:
 +		if (vcpu->guest_debug &
 +			(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))
 +			return false;
 +		/* fall through */
 +	case DE_VECTOR:
 +	case OF_VECTOR:
 +	case BR_VECTOR:
 +	case UD_VECTOR:
 +	case DF_VECTOR:
 +	case SS_VECTOR:
 +	case GP_VECTOR:
 +	case MF_VECTOR:
 +		return true;
 +	break;
 +	}
 +	return false;
 +}
  
 +static int handle_rmode_exception(struct kvm_vcpu *vcpu,
 +				  int vec, u32 err_code)
 +{
  	/*
 -	 * Some cpus support VM_{ENTRY,EXIT}_IA32_PERF_GLOBAL_CTRL but they
 -	 * can't be used due to an errata where VM Exit may incorrectly clear
 -	 * IA32_PERF_GLOBAL_CTRL[34:32].  Workaround the errata by using the
 -	 * MSR load mechanism to switch IA32_PERF_GLOBAL_CTRL.
 +	 * Instruction with address size override prefix opcode 0x67
 +	 * Cause the #SS fault with 0 error code in VM86 mode.
  	 */
 -	if (boot_cpu_data.x86 == 0x6) {
 -		switch (boot_cpu_data.x86_model) {
 -		case 26: /* AAK155 */
 -		case 30: /* AAP115 */
 -		case 37: /* AAT100 */
 -		case 44: /* BC86,AAY89,BD102 */
 -		case 46: /* BA97 */
 -			_vmexit_control &= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;
 -			_vmexit_control &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;
 -			pr_warn_once("kvm: VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL "
 -					"does not work properly. Using workaround\n");
 -			break;
 -		default:
 -			break;
 +	if (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {
 +		if (kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE) {
 +			if (vcpu->arch.halt_request) {
 +				vcpu->arch.halt_request = 0;
 +				return kvm_vcpu_halt(vcpu);
 +			}
 +			return 1;
  		}
 +		return 0;
  	}
  
 +	/*
 +	 * Forward all other exceptions that are valid in real mode.
 +	 * FIXME: Breaks guest debugging in real mode, needs to be fixed with
 +	 *        the required debugging infrastructure rework.
 +	 */
 +	kvm_queue_exception(vcpu, vec);
 +	return 1;
 +}
  
 -	rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);
 -
 -	/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */
 -	if ((vmx_msr_high & 0x1fff) > PAGE_SIZE)
 -		return -EIO;
 +/*
 + * Trigger machine check on the host. We assume all the MSRs are already set up
 + * by the CPU and that we still run on the same CPU as the MCE occurred on.
 + * We pass a fake environment to the machine check handler because we want
 + * the guest to be always treated like user space, no matter what context
 + * it used internally.
 + */
 +static void kvm_machine_check(void)
 +{
 +#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_64)
 +	struct pt_regs regs = {
 +		.cs = 3, /* Fake ring 3 no matter what the guest ran on */
 +		.flags = X86_EFLAGS_IF,
 +	};
  
 -#ifdef CONFIG_X86_64
 -	/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */
 -	if (vmx_msr_high & (1u<<16))
 -		return -EIO;
 +	do_machine_check(&regs, 0);
  #endif
 +}
  
 -	/* Require Write-Back (WB) memory type for VMCS accesses. */
 -	if (((vmx_msr_high >> 18) & 15) != 6)
 -		return -EIO;
 +static int handle_machine_check(struct kvm_vcpu *vcpu)
 +{
 +	/* already handled by vcpu_run */
 +	return 1;
 +}
  
 -	vmcs_conf->size = vmx_msr_high & 0x1fff;
 -	vmcs_conf->order = get_order(vmcs_conf->size);
 -	vmcs_conf->basic_cap = vmx_msr_high & ~0x1fff;
 +static int handle_exception(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct kvm_run *kvm_run = vcpu->run;
 +	u32 intr_info, ex_no, error_code;
 +	unsigned long cr2, rip, dr6;
 +	u32 vect_info;
 +	enum emulation_result er;
  
 -	vmcs_conf->revision_id = vmx_msr_low;
 +	vect_info = vmx->idt_vectoring_info;
 +	intr_info = vmx->exit_intr_info;
  
 -	vmcs_conf->pin_based_exec_ctrl = _pin_based_exec_control;
 -	vmcs_conf->cpu_based_exec_ctrl = _cpu_based_exec_control;
 -	vmcs_conf->cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
 -	vmcs_conf->vmexit_ctrl         = _vmexit_control;
 -	vmcs_conf->vmentry_ctrl        = _vmentry_control;
 +	if (is_machine_check(intr_info))
 +		return handle_machine_check(vcpu);
  
 -	if (static_branch_unlikely(&enable_evmcs))
 -		evmcs_sanitize_exec_ctrls(vmcs_conf);
 +	if (is_nmi(intr_info))
 +		return 1;  /* already handled by vmx_vcpu_run() */
  
 -	return 0;
 -}
 +	if (is_invalid_opcode(intr_info))
 +		return handle_ud(vcpu);
  
 -struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu)
 -{
 -	int node = cpu_to_node(cpu);
 -	struct page *pages;
 -	struct vmcs *vmcs;
 +	error_code = 0;
 +	if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
 +		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
  
 -	pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);
 -	if (!pages)
 -		return NULL;
 -	vmcs = page_address(pages);
 -	memset(vmcs, 0, vmcs_config.size);
 +	if (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {
 +		WARN_ON_ONCE(!enable_vmware_backdoor);
 +		er = kvm_emulate_instruction(vcpu,
 +			EMULTYPE_VMWARE | EMULTYPE_NO_UD_ON_FAIL);
 +		if (er == EMULATE_USER_EXIT)
 +			return 0;
 +		else if (er != EMULATE_DONE)
 +			kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 +		return 1;
 +	}
  
 -	/* KVM supports Enlightened VMCS v1 only */
 -	if (static_branch_unlikely(&enable_evmcs))
 -		vmcs->hdr.revision_id = KVM_EVMCS_VERSION;
 -	else
 -		vmcs->hdr.revision_id = vmcs_config.revision_id;
 +	/*
 +	 * The #PF with PFEC.RSVD = 1 indicates the guest is accessing
 +	 * MMIO, it is better to report an internal error.
 +	 * See the comments in vmx_handle_exit.
 +	 */
 +	if ((vect_info & VECTORING_INFO_VALID_MASK) &&
 +	    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;
 +		vcpu->run->internal.ndata = 3;
 +		vcpu->run->internal.data[0] = vect_info;
 +		vcpu->run->internal.data[1] = intr_info;
 +		vcpu->run->internal.data[2] = error_code;
 +		return 0;
 +	}
  
 -	if (shadow)
 -		vmcs->hdr.shadow_vmcs = 1;
 -	return vmcs;
 +	if (is_page_fault(intr_info)) {
 +		cr2 = vmcs_readl(EXIT_QUALIFICATION);
 +		/* EPT won't cause page fault directly */
 +		WARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);
 +		return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
 +	}
 +
 +	ex_no = intr_info & INTR_INFO_VECTOR_MASK;
 +
 +	if (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))
 +		return handle_rmode_exception(vcpu, ex_no, error_code);
 +
 +	switch (ex_no) {
 +	case AC_VECTOR:
 +		kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 +		return 1;
 +	case DB_VECTOR:
 +		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 +		if (!(vcpu->guest_debug &
 +		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
 +			vcpu->arch.dr6 &= ~15;
 +			vcpu->arch.dr6 |= dr6 | DR6_RTM;
 +			if (is_icebp(intr_info))
 +				skip_emulated_instruction(vcpu);
 +
 +			kvm_queue_exception(vcpu, DB_VECTOR);
 +			return 1;
 +		}
 +		kvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;
 +		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 +		/* fall through */
 +	case BP_VECTOR:
 +		/*
 +		 * Update instruction length as we may reinject #BP from
 +		 * user space while in guest debugging mode. Reading it for
 +		 * #DB as well causes no harm, it is not used in that case.
 +		 */
 +		vmx->vcpu.arch.event_exit_inst_len =
 +			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 +		kvm_run->exit_reason = KVM_EXIT_DEBUG;
 +		rip = kvm_rip_read(vcpu);
 +		kvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;
 +		kvm_run->debug.arch.exception = ex_no;
 +		break;
 +	default:
 +		kvm_run->exit_reason = KVM_EXIT_EXCEPTION;
 +		kvm_run->ex.exception = ex_no;
 +		kvm_run->ex.error_code = error_code;
 +		break;
 +	}
 +	return 0;
  }
  
 -void free_vmcs(struct vmcs *vmcs)
 +static int handle_external_interrupt(struct kvm_vcpu *vcpu)
  {
 -	free_pages((unsigned long)vmcs, vmcs_config.order);
 +	++vcpu->stat.irq_exits;
 +	return 1;
  }
  
 -/*
 - * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded
 - */
 -void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 +static int handle_triple_fault(struct kvm_vcpu *vcpu)
  {
 -	if (!loaded_vmcs->vmcs)
 -		return;
 -	loaded_vmcs_clear(loaded_vmcs);
 -	free_vmcs(loaded_vmcs->vmcs);
 -	loaded_vmcs->vmcs = NULL;
 -	if (loaded_vmcs->msr_bitmap)
 -		free_page((unsigned long)loaded_vmcs->msr_bitmap);
 -	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
 +	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
 +	vcpu->mmio_needed = 0;
 +	return 0;
  }
  
 -int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 +static int handle_io(struct kvm_vcpu *vcpu)
  {
 -	loaded_vmcs->vmcs = alloc_vmcs(false);
 -	if (!loaded_vmcs->vmcs)
 -		return -ENOMEM;
 -
 -	loaded_vmcs->shadow_vmcs = NULL;
 -	loaded_vmcs_init(loaded_vmcs);
 -
 -	if (cpu_has_vmx_msr_bitmap()) {
 -		loaded_vmcs->msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);
 -		if (!loaded_vmcs->msr_bitmap)
 -			goto out_vmcs;
 -		memset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);
 +	unsigned long exit_qualification;
 +	int size, in, string;
 +	unsigned port;
  
 -		if (IS_ENABLED(CONFIG_HYPERV) &&
 -		    static_branch_unlikely(&enable_evmcs) &&
 -		    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {
 -			struct hv_enlightened_vmcs *evmcs =
 -				(struct hv_enlightened_vmcs *)loaded_vmcs->vmcs;
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	string = (exit_qualification & 16) != 0;
  
 -			evmcs->hv_enlightenments_control.msr_bitmap = 1;
 -		}
 -	}
 +	++vcpu->stat.io_exits;
  
 -	memset(&loaded_vmcs->host_state, 0, sizeof(struct vmcs_host_state));
 +	if (string)
 +		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
  
 -	return 0;
 +	port = exit_qualification >> 16;
 +	size = (exit_qualification & 7) + 1;
 +	in = (exit_qualification & 8) != 0;
  
 -out_vmcs:
 -	free_loaded_vmcs(loaded_vmcs);
 -	return -ENOMEM;
 +	return kvm_fast_pio(vcpu, size, port, in);
  }
  
 -static void free_kvm_area(void)
 +static void
 +vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
  {
 -	int cpu;
 -
 -	for_each_possible_cpu(cpu) {
 -		free_vmcs(per_cpu(vmxarea, cpu));
 -		per_cpu(vmxarea, cpu) = NULL;
 -	}
 +	/*
 +	 * Patch in the VMCALL instruction:
 +	 */
 +	hypercall[0] = 0x0f;
 +	hypercall[1] = 0x01;
 +	hypercall[2] = 0xc1;
  }
  
 -static __init int alloc_kvm_area(void)
 +/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 +static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
  {
 -	int cpu;
 -
 -	for_each_possible_cpu(cpu) {
 -		struct vmcs *vmcs;
 -
 -		vmcs = alloc_vmcs_cpu(false, cpu);
 -		if (!vmcs) {
 -			free_kvm_area();
 -			return -ENOMEM;
 -		}
 +	if (is_guest_mode(vcpu)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		unsigned long orig_val = val;
  
  		/*
 -		 * When eVMCS is enabled, alloc_vmcs_cpu() sets
 -		 * vmcs->revision_id to KVM_EVMCS_VERSION instead of
 -		 * revision_id reported by MSR_IA32_VMX_BASIC.
 -		 *
 -		 * However, even though not explictly documented by
 -		 * TLFS, VMXArea passed as VMXON argument should
 -		 * still be marked with revision_id reported by
 -		 * physical CPU.
 +		 * We get here when L2 changed cr0 in a way that did not change
 +		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 +		 * but did change L0 shadowed bits. So we first calculate the
 +		 * effective cr0 value that L1 would like to write into the
 +		 * hardware. It consists of the L2-owned bits from the new
 +		 * value combined with the L1-owned bits from L1's guest_cr0.
  		 */
 -		if (static_branch_unlikely(&enable_evmcs))
 -			vmcs->hdr.revision_id = vmcs_config.revision_id;
 +		val = (val & ~vmcs12->cr0_guest_host_mask) |
 +			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
  
 -		per_cpu(vmxarea, cpu) = vmcs;
 +		if (!nested_guest_cr0_valid(vcpu, val))
 +			return 1;
 +
 +		if (kvm_set_cr0(vcpu, val))
 +			return 1;
 +		vmcs_writel(CR0_READ_SHADOW, orig_val);
 +		return 0;
 +	} else {
 +		if (to_vmx(vcpu)->nested.vmxon &&
 +		    !nested_host_cr0_valid(vcpu, val))
 +			return 1;
 +
 +		return kvm_set_cr0(vcpu, val);
  	}
 -	return 0;
  }
  
 -static void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,
 -		struct kvm_segment *save)
 +static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
  {
 -	if (!emulate_invalid_guest_state) {
 -		/*
 -		 * CS and SS RPL should be equal during guest entry according
 -		 * to VMX spec, but in reality it is not always so. Since vcpu
 -		 * is in the middle of the transition from real mode to
 -		 * protected mode it is safe to assume that RPL 0 is a good
 -		 * default value.
 -		 */
 -		if (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)
 -			save->selector &= ~SEGMENT_RPL_MASK;
 -		save->dpl = save->selector & SEGMENT_RPL_MASK;
 -		save->s = 1;
 -	}
 -	vmx_set_segment(vcpu, save, seg);
 +	if (is_guest_mode(vcpu)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		unsigned long orig_val = val;
 +
 +		/* analogously to handle_set_cr0 */
 +		val = (val & ~vmcs12->cr4_guest_host_mask) |
 +			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 +		if (kvm_set_cr4(vcpu, val))
 +			return 1;
 +		vmcs_writel(CR4_READ_SHADOW, orig_val);
 +		return 0;
 +	} else
 +		return kvm_set_cr4(vcpu, val);
  }
  
 -static void enter_pmode(struct kvm_vcpu *vcpu)
 +static int handle_desc(struct kvm_vcpu *vcpu)
  {
 -	unsigned long flags;
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
  
 -	/*
 -	 * Update real mode segment cache. It may be not up-to-date if sement
 -	 * register was written while vcpu was in a guest mode.
 -	 */
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
 +static int handle_cr(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification, val;
 +	int cr;
 +	int reg;
 +	int err;
 +	int ret;
  
 -	vmx->rmode.vm86_active = 0;
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	cr = exit_qualification & 15;
 +	reg = (exit_qualification >> 8) & 15;
 +	switch ((exit_qualification >> 4) & 3) {
 +	case 0: /* mov to cr */
 +		val = kvm_register_readl(vcpu, reg);
 +		trace_kvm_cr_write(cr, val);
 +		switch (cr) {
 +		case 0:
 +			err = handle_set_cr0(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 3:
 +			WARN_ON_ONCE(enable_unrestricted_guest);
 +			err = kvm_set_cr3(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 4:
 +			err = handle_set_cr4(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 8: {
 +				u8 cr8_prev = kvm_get_cr8(vcpu);
 +				u8 cr8 = (u8)val;
 +				err = kvm_set_cr8(vcpu, cr8);
 +				ret = kvm_complete_insn_gp(vcpu, err);
 +				if (lapic_in_kernel(vcpu))
 +					return ret;
 +				if (cr8_prev <= cr8)
 +					return ret;
 +				/*
 +				 * TODO: we might be squashing a
 +				 * KVM_GUESTDBG_SINGLESTEP-triggered
 +				 * KVM_EXIT_DEBUG here.
 +				 */
 +				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 +				return 0;
 +			}
 +		}
 +		break;
 +	case 2: /* clts */
 +		WARN_ONCE(1, "Guest should always own CR0.TS");
 +		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 +		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 +		return kvm_skip_emulated_instruction(vcpu);
 +	case 1: /*mov from cr*/
 +		switch (cr) {
 +		case 3:
 +			WARN_ON_ONCE(enable_unrestricted_guest);
 +			val = kvm_read_cr3(vcpu);
 +			kvm_register_write(vcpu, reg, val);
 +			trace_kvm_cr_read(cr, val);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		case 8:
 +			val = kvm_get_cr8(vcpu);
 +			kvm_register_write(vcpu, reg, val);
 +			trace_kvm_cr_read(cr, val);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		break;
 +	case 3: /* lmsw */
 +		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 +		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 +		kvm_lmsw(vcpu, val);
  
 -	vmx_segment_cache_clear(vmx);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	default:
 +		break;
 +	}
 +	vcpu->run->exit_reason = 0;
 +	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 +	       (int)(exit_qualification >> 4) & 3, cr);
 +	return 0;
 +}
  
 -	vmx_set_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
 +static int handle_dr(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification;
 +	int dr, dr7, reg;
  
 -	flags = vmcs_readl(GUEST_RFLAGS);
 -	flags &= RMODE_GUEST_OWNED_EFLAGS_BITS;
 -	flags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;
 -	vmcs_writel(GUEST_RFLAGS, flags);
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
  
 -	vmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |
 -			(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));
 +	/* First, if DR does not exist, trigger UD */
 +	if (!kvm_require_dr(vcpu, dr))
 +		return 1;
  
 -	update_exception_bitmap(vcpu);
 +	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 +	if (!kvm_require_cpl(vcpu, 0))
 +		return 1;
 +	dr7 = vmcs_readl(GUEST_DR7);
 +	if (dr7 & DR7_GD) {
 +		/*
 +		 * As the vm-exit takes precedence over the debug trap, we
 +		 * need to emulate the latter, either for the host or the
 +		 * guest debugging itself.
 +		 */
 +		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 +			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 +			vcpu->run->debug.arch.dr7 = dr7;
 +			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 +			vcpu->run->debug.arch.exception = DB_VECTOR;
 +			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 +			return 0;
 +		} else {
 +			vcpu->arch.dr6 &= ~15;
 +			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 +			kvm_queue_exception(vcpu, DB_VECTOR);
 +			return 1;
 +		}
 +	}
  
 -	fix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);
 -	fix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);
 -	fix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);
 -	fix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);
 -	fix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);
 -	fix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);
 -}
 +	if (vcpu->guest_debug == 0) {
 +		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +				CPU_BASED_MOV_DR_EXITING);
  
 -static void fix_rmode_seg(int seg, struct kvm_segment *save)
 -{
 -	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
 -	struct kvm_segment var = *save;
 +		/*
 +		 * No more DR vmexits; force a reload of the debug registers
 +		 * and reenter on this instruction.  The next vmexit will
 +		 * retrieve the full state of the debug registers.
 +		 */
 +		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 +		return 1;
 +	}
  
 -	var.dpl = 0x3;
 -	if (seg == VCPU_SREG_CS)
 -		var.type = 0x3;
 +	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 +	if (exit_qualification & TYPE_MOV_FROM_DR) {
 +		unsigned long val;
  
 -	if (!emulate_invalid_guest_state) {
 -		var.selector = var.base >> 4;
 -		var.base = var.base & 0xffff0;
 -		var.limit = 0xffff;
 -		var.g = 0;
 -		var.db = 0;
 -		var.present = 1;
 -		var.s = 1;
 -		var.l = 0;
 -		var.unusable = 0;
 -		var.type = 0x3;
 -		var.avl = 0;
 -		if (save->base & 0xf)
 -			printk_once(KERN_WARNING "kvm: segment base is not "
 -					"paragraph aligned when entering "
 -					"protected mode (seg=%d)", seg);
 -	}
 +		if (kvm_get_dr(vcpu, dr, &val))
 +			return 1;
 +		kvm_register_write(vcpu, reg, val);
 +	} else
 +		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 +			return 1;
  
 -	vmcs_write16(sf->selector, var.selector);
 -	vmcs_writel(sf->base, var.base);
 -	vmcs_write32(sf->limit, var.limit);
 -	vmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));
 +	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static void enter_rmode(struct kvm_vcpu *vcpu)
 +static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
  {
 -	unsigned long flags;
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);
 -
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
 -	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
 +	return vcpu->arch.dr6;
 +}
  
 -	vmx->rmode.vm86_active = 1;
 +static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +}
  
 -	/*
 -	 * Very old userspace does not call KVM_SET_TSS_ADDR before entering
 -	 * vcpu. Warn the user that an update is overdue.
 -	 */
 -	if (!kvm_vmx->tss_addr)
 -		printk_once(KERN_WARNING "kvm: KVM_SET_TSS_ADDR need to be "
 -			     "called before entering vcpu\n");
 +static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +{
 +	get_debugreg(vcpu->arch.db[0], 0);
 +	get_debugreg(vcpu->arch.db[1], 1);
 +	get_debugreg(vcpu->arch.db[2], 2);
 +	get_debugreg(vcpu->arch.db[3], 3);
 +	get_debugreg(vcpu->arch.dr6, 6);
 +	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
  
 -	vmx_segment_cache_clear(vmx);
 +	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 +	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 +}
  
 -	vmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);
 -	vmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);
 -	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);
 +static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	vmcs_writel(GUEST_DR7, val);
 +}
  
 -	flags = vmcs_readl(GUEST_RFLAGS);
 -	vmx->rmode.save_rflags = flags;
 +static int handle_cpuid(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_cpuid(vcpu);
 +}
  
 -	flags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;
 +static int handle_rdmsr(struct kvm_vcpu *vcpu)
 +{
 +	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 +	struct msr_data msr_info;
  
 -	vmcs_writel(GUEST_RFLAGS, flags);
 -	vmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);
 -	update_exception_bitmap(vcpu);
 +	msr_info.index = ecx;
 +	msr_info.host_initiated = false;
 +	if (vmx_get_msr(vcpu, &msr_info)) {
 +		trace_kvm_msr_read_ex(ecx);
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
  
 -	fix_rmode_seg(VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);
 -	fix_rmode_seg(VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);
 -	fix_rmode_seg(VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);
 -	fix_rmode_seg(VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);
 -	fix_rmode_seg(VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);
 -	fix_rmode_seg(VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);
 +	trace_kvm_msr_read(ecx, msr_info.data);
  
 -	kvm_mmu_reset_context(vcpu);
 +	/* FIXME: handling of bits 32:63 of rax, rdx */
 +	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 +	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 +	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 +static int handle_wrmsr(struct kvm_vcpu *vcpu)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct shared_msr_entry *msr = find_msr_entry(vmx, MSR_EFER);
 -
 -	if (!msr)
 -		return;
 -
 -	vcpu->arch.efer = efer;
 -	if (efer & EFER_LMA) {
 -		vm_entry_controls_setbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 -		msr->data = efer;
 -	} else {
 -		vm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 +	struct msr_data msr;
 +	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 +	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 +		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
  
 -		msr->data = efer & ~EFER_LME;
 +	msr.data = data;
 +	msr.index = ecx;
 +	msr.host_initiated = false;
 +	if (kvm_set_msr(vcpu, &msr) != 0) {
 +		trace_kvm_msr_write_ex(ecx, data);
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
  	}
 -	setup_msrs(vmx);
 +
 +	trace_kvm_msr_write(ecx, data);
 +	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -#ifdef CONFIG_X86_64
 +static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +{
 +	kvm_apic_update_ppr(vcpu);
 +	return 1;
 +}
  
 -static void enter_lmode(struct kvm_vcpu *vcpu)
 +static int handle_interrupt_window(struct kvm_vcpu *vcpu)
  {
 -	u32 guest_tr_ar;
 +	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +			CPU_BASED_VIRTUAL_INTR_PENDING);
  
 -	vmx_segment_cache_clear(to_vmx(vcpu));
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
  
 -	guest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);
 -	if ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
 -		pr_debug_ratelimited("%s: tss fixup for long mode. \n",
 -				     __func__);
 -		vmcs_write32(GUEST_TR_AR_BYTES,
 -			     (guest_tr_ar & ~VMX_AR_TYPE_MASK)
 -			     | VMX_AR_TYPE_BUSY_64_TSS);
 -	}
 -	vmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);
 +	++vcpu->stat.irq_window_exits;
 +	return 1;
  }
  
 -static void exit_lmode(struct kvm_vcpu *vcpu)
 +static int handle_halt(struct kvm_vcpu *vcpu)
  {
 -	vm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);
 -	vmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);
 +	return kvm_emulate_halt(vcpu);
  }
  
 -#endif
 +static int handle_vmcall(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_hypercall(vcpu);
 +}
  
 -static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
 +static int handle_invd(struct kvm_vcpu *vcpu)
  {
 -	int vpid = to_vmx(vcpu)->vpid;
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
  
 -	if (!vpid_sync_vcpu_addr(vpid, addr))
 -		vpid_sync_context(vpid);
 +static int handle_invlpg(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
  
 -	/*
 -	 * If VPIDs are not supported or enabled, then the above is a no-op.
 -	 * But we don't really need a TLB flush in that case anyway, because
 -	 * each VM entry/exit includes an implicit flush when VPID is 0.
 -	 */
 +	kvm_mmu_invlpg(vcpu, exit_qualification);
 +	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 +static int handle_rdpmc(struct kvm_vcpu *vcpu)
  {
 -	ulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
 +	int err;
  
 -	vcpu->arch.cr0 &= ~cr0_guest_owned_bits;
 -	vcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;
 +	err = kvm_rdpmc(vcpu);
 +	return kvm_complete_insn_gp(vcpu, err);
  }
  
 -static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 +static int handle_wbinvd(struct kvm_vcpu *vcpu)
  {
 -	if (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))
 -		vcpu->arch.cr3 = vmcs_readl(GUEST_CR3);
 -	__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
 +	return kvm_emulate_wbinvd(vcpu);
  }
  
 -static void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)
 +static int handle_xsetbv(struct kvm_vcpu *vcpu)
  {
 -	ulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;
 +	u64 new_bv = kvm_read_edx_eax(vcpu);
 +	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
  
 -	vcpu->arch.cr4 &= ~cr4_guest_owned_bits;
 -	vcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & cr4_guest_owned_bits;
 +	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 +		return kvm_skip_emulated_instruction(vcpu);
 +	return 1;
  }
  
 -static void ept_load_pdptrs(struct kvm_vcpu *vcpu)
 +static int handle_xsaves(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
 -
 -	if (!test_bit(VCPU_EXREG_PDPTR,
 -		      (unsigned long *)&vcpu->arch.regs_dirty))
 -		return;
 -
 -	if (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {
 -		vmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);
 -		vmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);
 -		vmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);
 -		vmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);
 -	}
 +	kvm_skip_emulated_instruction(vcpu);
 +	WARN(1, "this should never happen\n");
 +	return 1;
 +}
 +
 +static int handle_xrstors(struct kvm_vcpu *vcpu)
 +{
 +	kvm_skip_emulated_instruction(vcpu);
 +	WARN(1, "this should never happen\n");
 +	return 1;
  }
  
 -void ept_save_pdptrs(struct kvm_vcpu *vcpu)
 +static int handle_apic_access(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
 +	if (likely(fasteoi)) {
 +		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +		int access_type, offset;
  
 -	if (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {
 -		mmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);
 -		mmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);
 -		mmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);
 -		mmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);
 +		access_type = exit_qualification & APIC_ACCESS_TYPE;
 +		offset = exit_qualification & APIC_ACCESS_OFFSET;
 +		/*
 +		 * Sane guest uses MOV to write EOI, with written value
 +		 * not cared. So make a short-circuit here by avoiding
 +		 * heavy instruction emulation.
 +		 */
 +		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 +		    (offset == APIC_EOI)) {
 +			kvm_lapic_set_eoi(vcpu);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
  	}
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
  
 -	__set_bit(VCPU_EXREG_PDPTR,
 -		  (unsigned long *)&vcpu->arch.regs_avail);
 -	__set_bit(VCPU_EXREG_PDPTR,
 -		  (unsigned long *)&vcpu->arch.regs_dirty);
 +static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	int vector = exit_qualification & 0xff;
 +
 +	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 +	kvm_apic_set_eoi_accelerated(vcpu, vector);
 +	return 1;
  }
  
 -static void ept_update_paging_mode_cr0(unsigned long *hw_cr0,
 -					unsigned long cr0,
 -					struct kvm_vcpu *vcpu)
 +static int handle_apic_write(struct kvm_vcpu *vcpu)
  {
 -	if (!test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
 -		vmx_decache_cr3(vcpu);
 -	if (!(cr0 & X86_CR0_PG)) {
 -		/* From paging/starting to nonpaging */
 -		vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
 -			     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) |
 -			     (CPU_BASED_CR3_LOAD_EXITING |
 -			      CPU_BASED_CR3_STORE_EXITING));
 -		vcpu->arch.cr0 = cr0;
 -		vmx_set_cr4(vcpu, kvm_read_cr4(vcpu));
 -	} else if (!is_paging(vcpu)) {
 -		/* From nonpaging to paging */
 -		vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
 -			     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) &
 -			     ~(CPU_BASED_CR3_LOAD_EXITING |
 -			       CPU_BASED_CR3_STORE_EXITING));
 -		vcpu->arch.cr0 = cr0;
 -		vmx_set_cr4(vcpu, kvm_read_cr4(vcpu));
 -	}
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 offset = exit_qualification & 0xfff;
  
 -	if (!(cr0 & X86_CR0_WP))
 -		*hw_cr0 &= ~X86_CR0_WP;
 +	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 +	kvm_apic_write_nodecode(vcpu, offset);
 +	return 1;
  }
  
 -void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 +static int handle_task_switch(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	unsigned long hw_cr0;
 +	unsigned long exit_qualification;
 +	bool has_error_code = false;
 +	u32 error_code = 0;
 +	u16 tss_selector;
 +	int reason, type, idt_v, idt_index;
  
 -	hw_cr0 = (cr0 & ~KVM_VM_CR0_ALWAYS_OFF);
 -	if (enable_unrestricted_guest)
 -		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;
 -	else {
 -		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON;
 +	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 +	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 +	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
  
 -		if (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))
 -			enter_pmode(vcpu);
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
  
 -		if (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))
 -			enter_rmode(vcpu);
 +	reason = (u32)exit_qualification >> 30;
 +	if (reason == TASK_SWITCH_GATE && idt_v) {
 +		switch (type) {
 +		case INTR_TYPE_NMI_INTR:
 +			vcpu->arch.nmi_injected = false;
 +			vmx_set_nmi_mask(vcpu, true);
 +			break;
 +		case INTR_TYPE_EXT_INTR:
 +		case INTR_TYPE_SOFT_INTR:
 +			kvm_clear_interrupt_queue(vcpu);
 +			break;
 +		case INTR_TYPE_HARD_EXCEPTION:
 +			if (vmx->idt_vectoring_info &
 +			    VECTORING_INFO_DELIVER_CODE_MASK) {
 +				has_error_code = true;
 +				error_code =
 +					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 +			}
 +			/* fall through */
 +		case INTR_TYPE_SOFT_EXCEPTION:
 +			kvm_clear_exception_queue(vcpu);
 +			break;
 +		default:
 +			break;
 +		}
  	}
 +	tss_selector = exit_qualification;
  
 -#ifdef CONFIG_X86_64
 -	if (vcpu->arch.efer & EFER_LME) {
 -		if (!is_paging(vcpu) && (cr0 & X86_CR0_PG))
 -			enter_lmode(vcpu);
 -		if (is_paging(vcpu) && !(cr0 & X86_CR0_PG))
 -			exit_lmode(vcpu);
 -	}
 -#endif
 +	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 +		       type != INTR_TYPE_EXT_INTR &&
 +		       type != INTR_TYPE_NMI_INTR))
 +		skip_emulated_instruction(vcpu);
  
 -	if (enable_ept && !enable_unrestricted_guest)
 -		ept_update_paging_mode_cr0(&hw_cr0, cr0, vcpu);
 +	if (kvm_task_switch(vcpu, tss_selector,
 +			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 +			    has_error_code, error_code) == EMULATE_FAIL) {
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 +		vcpu->run->internal.ndata = 0;
 +		return 0;
 +	}
  
 -	vmcs_writel(CR0_READ_SHADOW, cr0);
 -	vmcs_writel(GUEST_CR0, hw_cr0);
 -	vcpu->arch.cr0 = cr0;
 +	/*
 +	 * TODO: What about debug traps on tss switch?
 +	 *       Are we supposed to inject them and update dr6?
 +	 */
  
 -	/* depends on vcpu->arch.cr0 to be set to a new value */
 -	vmx->emulation_required = emulation_required(vcpu);
 +	return 1;
  }
  
 -static int get_ept_level(struct kvm_vcpu *vcpu)
 +static int handle_ept_violation(struct kvm_vcpu *vcpu)
  {
 -	if (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))
 -		return 5;
 -	return 4;
 -}
 +	unsigned long exit_qualification;
 +	gpa_t gpa;
 +	u64 error_code;
  
 -u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)
 -{
 -	u64 eptp = VMX_EPTP_MT_WB;
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
  
 -	eptp |= (get_ept_level(vcpu) == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;
 +	/*
 +	 * EPT violation happened while executing iret from NMI,
 +	 * "blocked by NMI" bit has to be set before next VM entry.
 +	 * There are errata that may cause this bit to not be set:
 +	 * AAK134, BY25.
 +	 */
 +	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			enable_vnmi &&
 +			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 +		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
  
 -	if (enable_ept_ad_bits &&
 -	    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))
 -		eptp |= VMX_EPTP_AD_ENABLE_BIT;
 -	eptp |= (root_hpa & PAGE_MASK);
 +	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +	trace_kvm_page_fault(gpa, exit_qualification);
  
 -	return eptp;
 -}
 +	/* Is it a read fault? */
 +	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 +		     ? PFERR_USER_MASK : 0;
 +	/* Is it a write fault? */
 +	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 +		      ? PFERR_WRITE_MASK : 0;
 +	/* Is it a fetch fault? */
 +	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 +		      ? PFERR_FETCH_MASK : 0;
 +	/* ept page table entry is present? */
 +	error_code |= (exit_qualification &
 +		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 +			EPT_VIOLATION_EXECUTABLE))
 +		      ? PFERR_PRESENT_MASK : 0;
  
 -void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 -{
 -	struct kvm *kvm = vcpu->kvm;
 -	unsigned long guest_cr3;
 -	u64 eptp;
 +	error_code |= (exit_qualification & 0x100) != 0 ?
 +	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
  
 -	guest_cr3 = cr3;
 -	if (enable_ept) {
 -		eptp = construct_eptp(vcpu, cr3);
 -		vmcs_write64(EPT_POINTER, eptp);
 +	vcpu->arch.exit_qualification = exit_qualification;
 +	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 +}
  
 -		if (kvm_x86_ops->tlb_remote_flush) {
 -			spin_lock(&to_kvm_vmx(kvm)->ept_pointer_lock);
 -			to_vmx(vcpu)->ept_pointer = eptp;
 -			to_kvm_vmx(kvm)->ept_pointers_match
 -				= EPT_POINTERS_CHECK;
 -			spin_unlock(&to_kvm_vmx(kvm)->ept_pointer_lock);
 -		}
 +static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 +{
 +	gpa_t gpa;
  
 -		if (enable_unrestricted_guest || is_paging(vcpu) ||
 -		    is_guest_mode(vcpu))
 -			guest_cr3 = kvm_read_cr3(vcpu);
 +	/*
 +	 * A nested guest cannot optimize MMIO vmexits, because we have an
 +	 * nGPA here instead of the required GPA.
 +	 */
 +	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +	if (!is_guest_mode(vcpu) &&
 +	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 +		trace_kvm_fast_mmio(gpa);
 +		/*
 +		 * Doing kvm_skip_emulated_instruction() depends on undefined
 +		 * behavior: Intel's manual doesn't mandate
 +		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 +		 * occurs and while on real hardware it was observed to be set,
 +		 * other hypervisors (namely Hyper-V) don't set it, we end up
 +		 * advancing IP with some random value. Disable fast mmio when
 +		 * running nested and keep it for real hardware in hope that
 +		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 +		 */
 +		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 +			return kvm_skip_emulated_instruction(vcpu);
  		else
 -			guest_cr3 = to_kvm_vmx(kvm)->ept_identity_map_addr;
 -		ept_load_pdptrs(vcpu);
 +			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 +								EMULATE_DONE;
  	}
  
 -	vmcs_writel(GUEST_CR3, guest_cr3);
 +	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 +}
 +
 +static int handle_nmi_window(struct kvm_vcpu *vcpu)
 +{
 +	WARN_ON_ONCE(!enable_vnmi);
 +	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +			CPU_BASED_VIRTUAL_NMI_PENDING);
 +	++vcpu->stat.nmi_window_exits;
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +
 +	return 1;
  }
  
 -int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 +static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
  {
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	enum emulation_result err = EMULATE_DONE;
 +	int ret = 1;
 +	u32 cpu_exec_ctrl;
 +	bool intr_window_requested;
 +	unsigned count = 130;
 +
  	/*
 -	 * Pass through host's Machine Check Enable value to hw_cr4, which
 -	 * is in force while we are in guest mode.  Do not let guests control
 -	 * this bit, even if host CR4.MCE == 0.
 +	 * We should never reach the point where we are emulating L2
 +	 * due to invalid guest state as that means we incorrectly
 +	 * allowed a nested VMEntry with an invalid vmcs12.
  	 */
 -	unsigned long hw_cr4;
 +	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
  
 -	hw_cr4 = (cr4_read_shadow() & X86_CR4_MCE) | (cr4 & ~X86_CR4_MCE);
 -	if (enable_unrestricted_guest)
 -		hw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;
 -	else if (to_vmx(vcpu)->rmode.vm86_active)
 -		hw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;
 -	else
 -		hw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;
 +	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 +	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
  
 -	if (!boot_cpu_has(X86_FEATURE_UMIP) && vmx_umip_emulated()) {
 -		if (cr4 & X86_CR4_UMIP) {
 -			vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 -				SECONDARY_EXEC_DESC);
 -			hw_cr4 &= ~X86_CR4_UMIP;
 -		} else if (!is_guest_mode(vcpu) ||
 -			!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC))
 -			vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,
 -					SECONDARY_EXEC_DESC);
 -	}
 +	while (vmx->emulation_required && count-- != 0) {
 +		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 +			return handle_interrupt_window(&vmx->vcpu);
  
 -	if (cr4 & X86_CR4_VMXE) {
 -		/*
 -		 * To use VMXON (and later other VMX instructions), a guest
 -		 * must first be able to turn on cr4.VMXE (see handle_vmon()).
 -		 * So basically the check on whether to allow nested VMX
 -		 * is here.  We operate under the default treatment of SMM,
 -		 * so VMX cannot be enabled under SMM.
 -		 */
 -		if (!nested_vmx_allowed(vcpu) || is_smm(vcpu))
 +		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
  			return 1;
 -	}
  
 -	if (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))
 -		return 1;
 +		err = kvm_emulate_instruction(vcpu, 0);
  
 -	vcpu->arch.cr4 = cr4;
 +		if (err == EMULATE_USER_EXIT) {
 +			++vcpu->stat.mmio_exits;
 +			ret = 0;
 +			goto out;
 +		}
  
 -	if (!enable_unrestricted_guest) {
 -		if (enable_ept) {
 -			if (!is_paging(vcpu)) {
 -				hw_cr4 &= ~X86_CR4_PAE;
 -				hw_cr4 |= X86_CR4_PSE;
 -			} else if (!(cr4 & X86_CR4_PAE)) {
 -				hw_cr4 &= ~X86_CR4_PAE;
 -			}
 +		if (err != EMULATE_DONE)
 +			goto emulation_error;
 +
 +		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 +		    vcpu->arch.exception.pending)
 +			goto emulation_error;
 +
 +		if (vcpu->arch.halt_request) {
 +			vcpu->arch.halt_request = 0;
 +			ret = kvm_vcpu_halt(vcpu);
 +			goto out;
  		}
  
 -		/*
 -		 * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in
 -		 * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs
 -		 * to be manually disabled when guest switches to non-paging
 -		 * mode.
 -		 *
 -		 * If !enable_unrestricted_guest, the CPU is always running
 -		 * with CR0.PG=1 and CR4 needs to be modified.
 -		 * If enable_unrestricted_guest, the CPU automatically
 -		 * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.
 -		 */
 -		if (!is_paging(vcpu))
 -			hw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);
 +		if (signal_pending(current))
 +			goto out;
 +		if (need_resched())
 +			schedule();
  	}
  
 -	vmcs_writel(CR4_READ_SHADOW, cr4);
 -	vmcs_writel(GUEST_CR4, hw_cr4);
 +out:
 +	return ret;
 +
 +emulation_error:
 +	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 +	vcpu->run->internal.ndata = 0;
  	return 0;
  }
  
@@@ -14917,8 -7260,16 +14931,13 @@@ static __init int hardware_setup(void
  
  	kvm_mce_cap_supported |= MCG_LMCE_P;
  
+ 	if (pt_mode != PT_MODE_SYSTEM && pt_mode != PT_MODE_HOST_GUEST)
+ 		return -EINVAL;
+ 	if (!enable_ept || !cpu_has_vmx_intel_pt())
+ 		pt_mode = PT_MODE_SYSTEM;
+ 
  	if (nested) {
 -		nested_vmx_setup_ctls_msrs(&vmcs_config.nested,
 -					   vmx_capability.ept, enable_apicv);
 -
 -		r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
 +		r = nested_vmx_hardware_setup();
  		if (r)
  			return r;
  	}
* Unmerged path arch/x86/kvm/vmx/capabilities.h
* Unmerged path arch/x86/kvm/vmx/vmx.h
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index cb38df7127b4..ca5bc0eacb95 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -814,6 +814,7 @@
 #define VMX_BASIC_INOUT		0x0040000000000000LLU
 
 /* MSR_IA32_VMX_MISC bits */
+#define MSR_IA32_VMX_MISC_INTEL_PT                 (1ULL << 14)
 #define MSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS (1ULL << 29)
 #define MSR_IA32_VMX_MISC_PREEMPTION_TIMER_SCALE   0x1F
 /* AMD-V MSRs */
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index 898c443eeed1..4e4133e86484 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -77,7 +77,9 @@
 #define SECONDARY_EXEC_ENCLS_EXITING		0x00008000
 #define SECONDARY_EXEC_RDSEED_EXITING		0x00010000
 #define SECONDARY_EXEC_ENABLE_PML               0x00020000
+#define SECONDARY_EXEC_PT_CONCEAL_VMX		0x00080000
 #define SECONDARY_EXEC_XSAVES			0x00100000
+#define SECONDARY_EXEC_PT_USE_GPA		0x01000000
 #define SECONDARY_EXEC_MODE_BASED_EPT_EXEC	0x00400000
 #define SECONDARY_EXEC_TSC_SCALING              0x02000000
 
@@ -99,6 +101,8 @@
 #define VM_EXIT_LOAD_IA32_EFER                  0x00200000
 #define VM_EXIT_SAVE_VMX_PREEMPTION_TIMER       0x00400000
 #define VM_EXIT_CLEAR_BNDCFGS                   0x00800000
+#define VM_EXIT_PT_CONCEAL_PIP			0x01000000
+#define VM_EXIT_CLEAR_IA32_RTIT_CTL		0x02000000
 
 #define VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR	0x00036dff
 
@@ -110,6 +114,8 @@
 #define VM_ENTRY_LOAD_IA32_PAT			0x00004000
 #define VM_ENTRY_LOAD_IA32_EFER                 0x00008000
 #define VM_ENTRY_LOAD_BNDCFGS                   0x00010000
+#define VM_ENTRY_PT_CONCEAL_PIP			0x00020000
+#define VM_ENTRY_LOAD_IA32_RTIT_CTL		0x00040000
 
 #define VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR	0x000011ff
 
@@ -241,6 +247,8 @@ enum vmcs_field {
 	GUEST_PDPTR3_HIGH               = 0x00002811,
 	GUEST_BNDCFGS                   = 0x00002812,
 	GUEST_BNDCFGS_HIGH              = 0x00002813,
+	GUEST_IA32_RTIT_CTL		= 0x00002814,
+	GUEST_IA32_RTIT_CTL_HIGH	= 0x00002815,
 	HOST_IA32_PAT			= 0x00002c00,
 	HOST_IA32_PAT_HIGH		= 0x00002c01,
 	HOST_IA32_EFER			= 0x00002c02,
* Unmerged path arch/x86/kvm/vmx/capabilities.h
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/vmx/vmx.h
