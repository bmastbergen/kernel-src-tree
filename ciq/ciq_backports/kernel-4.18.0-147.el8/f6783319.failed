sched/fair: Fix insertion in rq->leaf_cfs_rq_list

jira LE-1907
cve CVE-2018-20784
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit f6783319737f28e4436a69611853a5a098cbe974
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f6783319.failed

Sargun reported a crash:

  "I picked up c40f7d74c741a907cfaeb73a7697081881c497d0 sched/fair: Fix
   infinite loop in update_blocked_averages() by reverting a9e7f6544b9c
   and put it on top of 4.19.13. In addition to this, I uninlined
   list_add_leaf_cfs_rq for debugging.

   This revealed a new bug that we didn't get to because we kept getting
   crashes from the previous issue. When we are running with cgroups that
   are rapidly changing, with CFS bandwidth control, and in addition
   using the cpusets cgroup, we see this crash. Specifically, it seems to
   occur with cgroups that are throttled and we change the allowed
   cpuset."

The algorithm used to order cfs_rq in rq->leaf_cfs_rq_list assumes that
it will walk down to root the 1st time a cfs_rq is used and we will finish
to add either a cfs_rq without parent or a cfs_rq with a parent that is
already on the list. But this is not always true in presence of throttling.
Because a cfs_rq can be throttled even if it has never been used but other CPUs
of the cgroup have already used all the bandwdith, we are not sure to go down to
the root and add all cfs_rq in the list.

Ensure that all cfs_rq will be added in the list even if they are throttled.

[ mingo: Fix !CGROUPS build. ]

	Reported-by: Sargun Dhillon <sargun@sargun.me>
	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: tj@kernel.org
Fixes: 9c2791f936ef ("Fix hierarchical order in rq->leaf_cfs_rq_list")
Link: https://lkml.kernel.org/r/1548825767-10799-1-git-send-email-vincent.guittot@linaro.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f6783319737f28e4436a69611853a5a098cbe974)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 1579a63f92e5,ffd1ae7237e7..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -285,66 -275,72 +285,130 @@@ static inline struct cfs_rq *group_cfs_
  	return grp->my_q;
  }
  
- static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
+ static inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
  {
++<<<<<<< HEAD
 +	if (!cfs_rq->on_list) {
 +		struct rq *rq = rq_of(cfs_rq);
 +		int cpu = cpu_of(rq);
++=======
+ 	struct rq *rq = rq_of(cfs_rq);
+ 	int cpu = cpu_of(rq);
+ 
+ 	if (cfs_rq->on_list)
+ 		return rq->tmp_alone_branch == &rq->leaf_cfs_rq_list;
+ 
+ 	cfs_rq->on_list = 1;
+ 
+ 	/*
+ 	 * Ensure we either appear before our parent (if already
+ 	 * enqueued) or force our parent to appear after us when it is
+ 	 * enqueued. The fact that we always enqueue bottom-up
+ 	 * reduces this to two cases and a special case for the root
+ 	 * cfs_rq. Furthermore, it also means that we will always reset
+ 	 * tmp_alone_branch either when the branch is connected
+ 	 * to a tree or when we reach the top of the tree
+ 	 */
+ 	if (cfs_rq->tg->parent &&
+ 	    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {
++>>>>>>> f6783319737f (sched/fair: Fix insertion in rq->leaf_cfs_rq_list)
  		/*
 -		 * If parent is already on the list, we add the child
 -		 * just before. Thanks to circular linked property of
 -		 * the list, this means to put the child at the tail
 -		 * of the list that starts by parent.
 +		 * Ensure we either appear before our parent (if already
 +		 * enqueued) or force our parent to appear after us when it is
 +		 * enqueued. The fact that we always enqueue bottom-up
 +		 * reduces this to two cases and a special case for the root
 +		 * cfs_rq. Furthermore, it also means that we will always reset
 +		 * tmp_alone_branch either when the branch is connected
 +		 * to a tree or when we reach the beg of the tree
  		 */
++<<<<<<< HEAD
 +		if (cfs_rq->tg->parent &&
 +		    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {
 +			/*
 +			 * If parent is already on the list, we add the child
 +			 * just before. Thanks to circular linked property of
 +			 * the list, this means to put the child at the tail
 +			 * of the list that starts by parent.
 +			 */
 +			list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
 +				&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));
 +			/*
 +			 * The branch is now connected to its tree so we can
 +			 * reset tmp_alone_branch to the beginning of the
 +			 * list.
 +			 */
 +			rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
 +		} else if (!cfs_rq->tg->parent) {
 +			/*
 +			 * cfs rq without parent should be put
 +			 * at the tail of the list.
 +			 */
 +			list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
 +				&rq->leaf_cfs_rq_list);
 +			/*
 +			 * We have reach the beg of a tree so we can reset
 +			 * tmp_alone_branch to the beginning of the list.
 +			 */
 +			rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
 +		} else {
 +			/*
 +			 * The parent has not already been added so we want to
 +			 * make sure that it will be put after us.
 +			 * tmp_alone_branch points to the beg of the branch
 +			 * where we will add parent.
 +			 */
 +			list_add_rcu(&cfs_rq->leaf_cfs_rq_list,
 +				rq->tmp_alone_branch);
 +			/*
 +			 * update tmp_alone_branch to points to the new beg
 +			 * of the branch
 +			 */
 +			rq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;
 +		}
 +
 +		cfs_rq->on_list = 1;
 +	}
++=======
+ 		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
+ 			&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));
+ 		/*
+ 		 * The branch is now connected to its tree so we can
+ 		 * reset tmp_alone_branch to the beginning of the
+ 		 * list.
+ 		 */
+ 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
+ 		return true;
+ 	}
+ 
+ 	if (!cfs_rq->tg->parent) {
+ 		/*
+ 		 * cfs rq without parent should be put
+ 		 * at the tail of the list.
+ 		 */
+ 		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
+ 			&rq->leaf_cfs_rq_list);
+ 		/*
+ 		 * We have reach the top of a tree so we can reset
+ 		 * tmp_alone_branch to the beginning of the list.
+ 		 */
+ 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
+ 		return true;
+ 	}
+ 
+ 	/*
+ 	 * The parent has not already been added so we want to
+ 	 * make sure that it will be put after us.
+ 	 * tmp_alone_branch points to the begin of the branch
+ 	 * where we will add parent.
+ 	 */
+ 	list_add_rcu(&cfs_rq->leaf_cfs_rq_list, rq->tmp_alone_branch);
+ 	/*
+ 	 * update tmp_alone_branch to points to the new begin
+ 	 * of the branch
+ 	 */
+ 	rq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;
+ 	return false;
++>>>>>>> f6783319737f (sched/fair: Fix insertion in rq->leaf_cfs_rq_list)
  }
  
  static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)
@@@ -5446,8 -5173,43 +5517,46 @@@ enqueue_task_fair(struct rq *rq, struc
  		update_cfs_group(se);
  	}
  
 -	if (!se) {
 +	if (!se)
  		add_nr_running(rq, 1);
++<<<<<<< HEAD
++=======
+ 		/*
+ 		 * Since new tasks are assigned an initial util_avg equal to
+ 		 * half of the spare capacity of their CPU, tiny tasks have the
+ 		 * ability to cross the overutilized threshold, which will
+ 		 * result in the load balancer ruining all the task placement
+ 		 * done by EAS. As a way to mitigate that effect, do not account
+ 		 * for the first enqueue operation of new tasks during the
+ 		 * overutilized flag detection.
+ 		 *
+ 		 * A better way of solving this problem would be to wait for
+ 		 * the PELT signals of tasks to converge before taking them
+ 		 * into account, but that is not straightforward to implement,
+ 		 * and the following generally works well enough in practice.
+ 		 */
+ 		if (flags & ENQUEUE_WAKEUP)
+ 			update_overutilized_status(rq);
+ 
+ 	}
+ 
+ 	if (cfs_bandwidth_used()) {
+ 		/*
+ 		 * When bandwidth control is enabled; the cfs_rq_throttled()
+ 		 * breaks in the above iteration can result in incomplete
+ 		 * leaf list maintenance, resulting in triggering the assertion
+ 		 * below.
+ 		 */
+ 		for_each_sched_entity(se) {
+ 			cfs_rq = cfs_rq_of(se);
+ 
+ 			if (list_add_leaf_cfs_rq(cfs_rq))
+ 				break;
+ 		}
+ 	}
+ 
+ 	assert_list_leaf_cfs_rq(rq);
++>>>>>>> f6783319737f (sched/fair: Fix insertion in rq->leaf_cfs_rq_list)
  
  	hrtick_update(rq);
  }
* Unmerged path kernel/sched/fair.c
