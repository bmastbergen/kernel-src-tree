KVM: arm/arm64: Share common code in user_mem_abort()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Punit Agrawal <punit.agrawal@arm.com>
commit 3f58bf63455588a260b6abc8761c48bc8977b919
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/3f58bf63.failed

The code for operations such as marking the pfn as dirty, and
dcache/icache maintenance during stage 2 fault handling is duplicated
between normal pages and PMD hugepages.

Instead of creating another copy of the operations when we introduce
PUD hugepages, let's share them across the different pagesizes.

	Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
	Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
	Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
(cherry picked from commit 3f58bf63455588a260b6abc8761c48bc8977b919)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	virt/kvm/arm/mmu.c
diff --cc virt/kvm/arm/mmu.c
index 22e88cffe24c,59595207c5e1..000000000000
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@@ -1563,9 -1504,28 +1563,34 @@@ static int user_mem_abort(struct kvm_vc
  		return -EFAULT;
  	}
  
++<<<<<<< HEAD
 +	if (vma_kernel_pagesize(vma) == PMD_SIZE && !force_pte) {
 +		hugetlb = true;
 +		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
++=======
+ 	vma_pagesize = vma_kernel_pagesize(vma);
+ 	if (vma_pagesize == PMD_SIZE && !logging_active) {
+ 		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
+ 	} else {
+ 		/*
+ 		 * Fallback to PTE if it's not one of the Stage 2
+ 		 * supported hugepage sizes
+ 		 */
+ 		vma_pagesize = PAGE_SIZE;
+ 
+ 		/*
+ 		 * Pages belonging to memslots that don't have the same
+ 		 * alignment for userspace and IPA cannot be mapped using
+ 		 * block descriptors even if the pages belong to a THP for
+ 		 * the process, because the stage-2 block descriptor will
+ 		 * cover more than a single THP and we loose atomicity for
+ 		 * unmapping, updates, and splits of the THP or other pages
+ 		 * in the stage-2 block range.
+ 		 */
+ 		if ((memslot->userspace_addr & ~PMD_MASK) !=
+ 		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))
+ 			force_pte = true;
++>>>>>>> 3f58bf634555 (KVM: arm/arm64: Share common code in user_mem_abort())
  	}
  	up_read(&current->mm->mmap_sem);
  
* Unmerged path virt/kvm/arm/mmu.c
