RDMA/mlx5: Enable vport loopback when user context or QP mandate

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Mark Bloch <markb@mellanox.com>
commit 0042f9e458a560e13c1da2211cf6429e0c7dd812
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/0042f9e4.failed

A user can create a QP which can accept loopback traffic, but that's not
enough. We need to enable loopback on the vport as well. Currently vport
loopback is enabled only when more than 1 users are using the IB device,
update the logic to consider whatever a QP which supports loopback was
created, if so enable vport loopback even if there is only a single user.

	Signed-off-by: Mark Bloch <markb@mellanox.com>
	Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 0042f9e458a560e13c1da2211cf6429e0c7dd812)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 5400c743211c,131a1286a767..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1564,6 -1571,48 +1564,51 @@@ static void deallocate_uars(struct mlx5
  			mlx5_cmd_free_uar(dev->mdev, bfregi->sys_pages[i]);
  }
  
++<<<<<<< HEAD
++=======
+ int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)
+ {
+ 	int err = 0;
+ 
+ 	mutex_lock(&dev->lb.mutex);
+ 	if (td)
+ 		dev->lb.user_td++;
+ 	if (qp)
+ 		dev->lb.qps++;
+ 
+ 	if (dev->lb.user_td == 2 ||
+ 	    dev->lb.qps == 1) {
+ 		if (!dev->lb.enabled) {
+ 			err = mlx5_nic_vport_update_local_lb(dev->mdev, true);
+ 			dev->lb.enabled = true;
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&dev->lb.mutex);
+ 
+ 	return err;
+ }
+ 
+ void mlx5_ib_disable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)
+ {
+ 	mutex_lock(&dev->lb.mutex);
+ 	if (td)
+ 		dev->lb.user_td--;
+ 	if (qp)
+ 		dev->lb.qps--;
+ 
+ 	if (dev->lb.user_td == 1 &&
+ 	    dev->lb.qps == 0) {
+ 		if (dev->lb.enabled) {
+ 			mlx5_nic_vport_update_local_lb(dev->mdev, false);
+ 			dev->lb.enabled = false;
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&dev->lb.mutex);
+ }
+ 
++>>>>>>> 0042f9e458a5 (RDMA/mlx5: Enable vport loopback when user context or QP mandate)
  static int mlx5_ib_alloc_transport_domain(struct mlx5_ib_dev *dev, u32 *tdn)
  {
  	int err;
@@@ -1580,14 -1629,7 +1625,18 @@@
  	     !MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))
  		return err;
  
++<<<<<<< HEAD
 +	mutex_lock(&dev->lb_mutex);
 +	dev->user_td++;
 +
 +	if (dev->user_td == 2)
 +		err = mlx5_nic_vport_update_local_lb(dev->mdev, true);
 +
 +	mutex_unlock(&dev->lb_mutex);
 +	return err;
++=======
+ 	return mlx5_ib_enable_lb(dev, true, false);
++>>>>>>> 0042f9e458a5 (RDMA/mlx5: Enable vport loopback when user context or QP mandate)
  }
  
  static void mlx5_ib_dealloc_transport_domain(struct mlx5_ib_dev *dev, u32 tdn)
@@@ -1602,13 -1644,7 +1651,17 @@@
  	     !MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))
  		return;
  
++<<<<<<< HEAD
 +	mutex_lock(&dev->lb_mutex);
 +	dev->user_td--;
 +
 +	if (dev->user_td < 2)
 +		mlx5_nic_vport_update_local_lb(dev->mdev, false);
 +
 +	mutex_unlock(&dev->lb_mutex);
++=======
+ 	mlx5_ib_disable_lb(dev, true, false);
++>>>>>>> 0042f9e458a5 (RDMA/mlx5: Enable vport loopback when user context or QP mandate)
  }
  
  static struct ib_ucontext *mlx5_ib_alloc_ucontext(struct ib_device *ibdev,
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4c696e5d457f,8376408e2bc9..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -839,11 -858,17 +839,22 @@@ to_mcounters(struct ib_counters *ibcntr
  	return container_of(ibcntrs, struct mlx5_ib_mcounters, ibcntrs);
  }
  
++<<<<<<< HEAD
 +int parse_flow_flow_action(struct mlx5_ib_flow_action *maction,
 +			   bool is_egress,
 +			   struct mlx5_flow_act *action);
++=======
+ struct mlx5_ib_lb_state {
+ 	/* protect the user_td */
+ 	struct mutex		mutex;
+ 	u32			user_td;
+ 	int			qps;
+ 	bool			enabled;
+ };
+ 
++>>>>>>> 0042f9e458a5 (RDMA/mlx5: Enable vport loopback when user context or QP mandate)
  struct mlx5_ib_dev {
  	struct ib_device		ib_dev;
 -	const struct uverbs_object_tree_def *driver_trees[6];
  	struct mlx5_core_dev		*mdev;
  	struct mlx5_roce		roce[MLX5_MAX_PORTS];
  	int				num_ports;
@@@ -997,8 -1020,10 +1008,15 @@@ int mlx5_ib_modify_srq(struct ib_srq *i
  		       enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
  int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
  int mlx5_ib_destroy_srq(struct ib_srq *srq);
++<<<<<<< HEAD
 +int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,
 +			  struct ib_recv_wr **bad_wr);
++=======
+ int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
+ 			  const struct ib_recv_wr **bad_wr);
+ int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);
+ void mlx5_ib_disable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);
++>>>>>>> 0042f9e458a5 (RDMA/mlx5: Enable vport loopback when user context or QP mandate)
  struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
  				struct ib_qp_init_attr *init_attr,
  				struct ib_udata *udata);
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index 4cd4e06d1b1a..c7f398e21403 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -1252,6 +1252,16 @@ static bool tunnel_offload_supported(struct mlx5_core_dev *dev)
 		 MLX5_CAP_ETH(dev, tunnel_stateless_geneve_rx));
 }
 
+static void destroy_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
+				      struct mlx5_ib_rq *rq,
+				      u32 qp_flags_en)
+{
+	if (qp_flags_en & (MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC |
+			   MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC))
+		mlx5_ib_disable_lb(dev, false, true);
+	mlx5_core_destroy_tir(dev->mdev, rq->tirn);
+}
+
 static int create_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
 				    struct mlx5_ib_rq *rq, u32 tdn,
 				    u32 *qp_flags_en)
@@ -1289,17 +1299,17 @@ static int create_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
 
 	err = mlx5_core_create_tir(dev->mdev, in, inlen, &rq->tirn);
 
+	if (!err && MLX5_GET(tirc, tirc, self_lb_block)) {
+		err = mlx5_ib_enable_lb(dev, false, true);
+
+		if (err)
+			destroy_raw_packet_qp_tir(dev, rq, 0);
+	}
 	kvfree(in);
 
 	return err;
 }
 
-static void destroy_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
-				      struct mlx5_ib_rq *rq)
-{
-	mlx5_core_destroy_tir(dev->mdev, rq->tirn);
-}
-
 static int create_raw_packet_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 				u32 *in, size_t inlen,
 				struct ib_pd *pd)
@@ -1368,7 +1378,7 @@ static void destroy_raw_packet_qp(struct mlx5_ib_dev *dev,
 	struct mlx5_ib_rq *rq = &raw_packet_qp->rq;
 
 	if (qp->rq.wqe_cnt) {
-		destroy_raw_packet_qp_tir(dev, rq);
+		destroy_raw_packet_qp_tir(dev, rq, qp->flags_en);
 		destroy_raw_packet_qp_rq(dev, rq);
 	}
 
@@ -1392,6 +1402,9 @@ static void raw_packet_qp_copy_info(struct mlx5_ib_qp *qp,
 
 static void destroy_rss_raw_qp_tir(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
 {
+	if (qp->flags_en & (MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC |
+			    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC))
+		mlx5_ib_disable_lb(dev, false, true);
 	mlx5_core_destroy_tir(dev->mdev, qp->rss_qp.tirn);
 }
 
@@ -1602,6 +1615,13 @@ static int create_rss_raw_qp_tir(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 create_tir:
 	err = mlx5_core_create_tir(dev->mdev, in, inlen, &qp->rss_qp.tirn);
 
+	if (!err && MLX5_GET(tirc, tirc, self_lb_block)) {
+		err = mlx5_ib_enable_lb(dev, false, true);
+
+		if (err)
+			mlx5_core_destroy_tir(dev->mdev, qp->rss_qp.tirn);
+	}
+
 	if (err)
 		goto err;
 
