mlxsw: spectrum_acl: Introduce vregion mutex

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jiri Pirko <jiri@mellanox.com>
commit 1263a9ab82249a5af42302a522ea8453affc5fda
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/1263a9ab.failed

In order to remove dependency on RTNL, introduce a mutex
to guard vregion structure, list of chunks and list of entries in
chunks.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: Ido Schimmel <idosch@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1263a9ab82249a5af42302a522ea8453affc5fda)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
index c607c62a43ae,54c0519195b7..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
@@@ -163,11 -178,20 +163,14 @@@ struct mlxsw_sp_acl_tcam_group 
  };
  
  struct mlxsw_sp_acl_tcam_vregion {
+ 	struct mutex lock; /* Protects consistency of region, region2 pointers
+ 			    * and vchunk_list.
+ 			    */
  	struct mlxsw_sp_acl_tcam_region *region;
 -	struct mlxsw_sp_acl_tcam_region *region2; /* Used during migration */
  	struct list_head list; /* Member of a TCAM group */
 -	struct list_head tlist; /* Member of a TCAM */
  	struct list_head vchunk_list; /* List of vchunks under this vregion */
 +	struct mlxsw_sp_acl_tcam_group *group;
  	struct mlxsw_afk_key_info *key_info;
 -	struct mlxsw_sp_acl_tcam *tcam;
 -	struct delayed_work rehash_dw;
 -	struct mlxsw_sp *mlxsw_sp;
 -	bool failed_rollback; /* Indicates failed rollback during migration */
 -	unsigned int ref_count;
  };
  
  struct mlxsw_sp_acl_tcam_vchunk;
@@@ -616,6 -750,10 +619,13 @@@ mlxsw_sp_acl_tcam_vregion_create(struc
  	if (!vregion)
  		return ERR_PTR(-ENOMEM);
  	INIT_LIST_HEAD(&vregion->vchunk_list);
++<<<<<<< HEAD
++=======
+ 	mutex_init(&vregion->lock);
+ 	vregion->tcam = tcam;
+ 	vregion->mlxsw_sp = mlxsw_sp;
+ 	vregion->ref_count = 1;
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  
  	vregion->key_info = mlxsw_afk_key_info_get(afk, elusage);
  	if (IS_ERR(vregion->key_info)) {
@@@ -643,8 -797,17 +653,9 @@@ static voi
  mlxsw_sp_acl_tcam_vregion_destroy(struct mlxsw_sp *mlxsw_sp,
  				  struct mlxsw_sp_acl_tcam_vregion *vregion)
  {
 -	const struct mlxsw_sp_acl_tcam_ops *ops = mlxsw_sp->acl_tcam_ops;
 -
 -	if (ops->region_rehash_hints_get)
 -		cancel_delayed_work_sync(&vregion->rehash_dw);
 -	list_del(&vregion->tlist);
 -	mlxsw_sp_acl_tcam_vgroup_vregion_detach(mlxsw_sp, vregion);
 -	if (vregion->region2)
 -		mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, vregion->region2);
  	mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, vregion->region);
  	mlxsw_afk_key_info_put(vregion->key_info);
+ 	mutex_destroy(&vregion->lock);
  	kfree(vregion);
  }
  
@@@ -777,6 -961,9 +790,12 @@@ mlxsw_sp_acl_tcam_vchunk_create(struct 
  		goto err_chunk_create;
  	}
  
++<<<<<<< HEAD
++=======
+ 	list_add_tail(&vchunk->list, &vregion->vchunk_list);
+ 	mutex_unlock(&vregion->lock);
+ 
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  	return vchunk;
  
  err_chunk_create:
@@@ -793,12 -980,18 +812,25 @@@ static voi
  mlxsw_sp_acl_tcam_vchunk_destroy(struct mlxsw_sp *mlxsw_sp,
  				 struct mlxsw_sp_acl_tcam_vchunk *vchunk)
  {
++<<<<<<< HEAD
 +	struct mlxsw_sp_acl_tcam_group *group = vchunk->group;
 +
 +	mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk);
 +	rhashtable_remove_fast(&group->vchunk_ht, &vchunk->ht_node,
++=======
+ 	struct mlxsw_sp_acl_tcam_vregion *vregion = vchunk->vregion;
+ 	struct mlxsw_sp_acl_tcam_vgroup *vgroup = vchunk->vgroup;
+ 
+ 	mutex_lock(&vregion->lock);
+ 	list_del(&vchunk->list);
+ 	if (vchunk->chunk2)
+ 		mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk2);
+ 	mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk);
+ 	mutex_unlock(&vregion->lock);
+ 	rhashtable_remove_fast(&vgroup->vchunk_ht, &vchunk->ht_node,
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  			       mlxsw_sp_acl_tcam_vchunk_ht_params);
 -	mlxsw_sp_acl_tcam_vregion_put(mlxsw_sp, vchunk->vregion);
 +	mlxsw_sp_acl_tcam_vchunk_deassoc(mlxsw_sp, vchunk);
  	kfree(vchunk);
  }
  
@@@ -909,14 -1100,21 +942,28 @@@ static int mlxsw_sp_acl_tcam_ventry_add
  		return PTR_ERR(vchunk);
  
  	ventry->vchunk = vchunk;
++<<<<<<< HEAD
++=======
+ 	ventry->rulei = rulei;
+ 	vregion = vchunk->vregion;
+ 
+ 	mutex_lock(&vregion->lock);
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  	ventry->entry = mlxsw_sp_acl_tcam_entry_create(mlxsw_sp, ventry,
 -						       vchunk->chunk);
 +						       vchunk->vregion->region,
 +						       vchunk->chunk, rulei);
  	if (IS_ERR(ventry->entry)) {
+ 		mutex_unlock(&vregion->lock);
  		err = PTR_ERR(ventry->entry);
  		goto err_entry_create;
  	}
  
++<<<<<<< HEAD
++=======
+ 	list_add_tail(&ventry->list, &vchunk->ventry_list);
+ 	mutex_unlock(&vregion->lock);
+ 
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  	return 0;
  
  err_entry_create:
@@@ -928,9 -1126,12 +975,17 @@@ static void mlxsw_sp_acl_tcam_ventry_de
  					 struct mlxsw_sp_acl_tcam_ventry *ventry)
  {
  	struct mlxsw_sp_acl_tcam_vchunk *vchunk = ventry->vchunk;
+ 	struct mlxsw_sp_acl_tcam_vregion *vregion = vchunk->vregion;
  
++<<<<<<< HEAD
 +	mlxsw_sp_acl_tcam_entry_destroy(mlxsw_sp, vchunk->vregion->region,
 +					vchunk->chunk, ventry->entry);
++=======
+ 	mutex_lock(&vregion->lock);
+ 	list_del(&ventry->list);
+ 	mlxsw_sp_acl_tcam_entry_destroy(mlxsw_sp, ventry->entry);
+ 	mutex_unlock(&vregion->lock);
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  	mlxsw_sp_acl_tcam_vchunk_put(mlxsw_sp, vchunk);
  }
  
@@@ -958,6 -1156,192 +1013,195 @@@ mlxsw_sp_acl_tcam_ventry_activity_get(s
  						    ventry->entry, activity);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ mlxsw_sp_acl_tcam_ventry_migrate(struct mlxsw_sp *mlxsw_sp,
+ 				 struct mlxsw_sp_acl_tcam_ventry *ventry,
+ 				 struct mlxsw_sp_acl_tcam_chunk *chunk2)
+ {
+ 	struct mlxsw_sp_acl_tcam_entry *entry2;
+ 
+ 	entry2 = mlxsw_sp_acl_tcam_entry_create(mlxsw_sp, ventry, chunk2);
+ 	if (IS_ERR(entry2))
+ 		return PTR_ERR(entry2);
+ 	mlxsw_sp_acl_tcam_entry_destroy(mlxsw_sp, ventry->entry);
+ 	ventry->entry = entry2;
+ 	return 0;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vchunk_migrate_one(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vchunk *vchunk,
+ 				     struct mlxsw_sp_acl_tcam_region *region,
+ 				     bool this_is_rollback)
+ {
+ 	struct mlxsw_sp_acl_tcam_ventry *ventry;
+ 	struct mlxsw_sp_acl_tcam_chunk *chunk2;
+ 	int err;
+ 	int err2;
+ 
+ 	chunk2 = mlxsw_sp_acl_tcam_chunk_create(mlxsw_sp, vchunk, region);
+ 	if (IS_ERR(chunk2)) {
+ 		if (this_is_rollback)
+ 			vchunk->vregion->failed_rollback = true;
+ 		return PTR_ERR(chunk2);
+ 	}
+ 	vchunk->chunk2 = chunk2;
+ 	list_for_each_entry(ventry, &vchunk->ventry_list, list) {
+ 		err = mlxsw_sp_acl_tcam_ventry_migrate(mlxsw_sp, ventry,
+ 						       vchunk->chunk2);
+ 		if (err) {
+ 			if (this_is_rollback) {
+ 				vchunk->vregion->failed_rollback = true;
+ 				return err;
+ 			}
+ 			goto rollback;
+ 		}
+ 	}
+ 	mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk);
+ 	vchunk->chunk = chunk2;
+ 	vchunk->chunk2 = NULL;
+ 	return 0;
+ 
+ rollback:
+ 	/* Migrate the entries back to the original chunk. If some entry
+ 	 * migration fails, there's no good way how to proceed. Set the
+ 	 * vregion with "failed_rollback" flag.
+ 	 */
+ 	list_for_each_entry_continue_reverse(ventry, &vchunk->ventry_list,
+ 					     list) {
+ 		err2 = mlxsw_sp_acl_tcam_ventry_migrate(mlxsw_sp, ventry,
+ 							vchunk->chunk);
+ 		if (err2) {
+ 			vchunk->vregion->failed_rollback = true;
+ 			goto err_rollback;
+ 		}
+ 	}
+ 
+ 	mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk2);
+ 	vchunk->chunk2 = NULL;
+ 
+ err_rollback:
+ 	return err;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vchunk_migrate_all(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vregion *vregion)
+ {
+ 	struct mlxsw_sp_acl_tcam_vchunk *vchunk;
+ 	int err;
+ 
+ 	list_for_each_entry(vchunk, &vregion->vchunk_list, list) {
+ 		err = mlxsw_sp_acl_tcam_vchunk_migrate_one(mlxsw_sp, vchunk,
+ 							   vregion->region2,
+ 							   false);
+ 		if (err)
+ 			goto rollback;
+ 	}
+ 	return 0;
+ 
+ rollback:
+ 	list_for_each_entry_continue_reverse(vchunk, &vregion->vchunk_list,
+ 					     list) {
+ 		mlxsw_sp_acl_tcam_vchunk_migrate_one(mlxsw_sp, vchunk,
+ 						     vregion->region, true);
+ 	}
+ 	return err;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vregion_migrate(struct mlxsw_sp *mlxsw_sp,
+ 				  struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				  void *hints_priv)
+ {
+ 	unsigned int priority = mlxsw_sp_acl_tcam_vregion_prio(vregion);
+ 	struct mlxsw_sp_acl_tcam_region *region2, *unused_region;
+ 	int err;
+ 
+ 	trace_mlxsw_sp_acl_tcam_vregion_migrate(mlxsw_sp, vregion);
+ 
+ 	region2 = mlxsw_sp_acl_tcam_region_create(mlxsw_sp, vregion->tcam,
+ 						  vregion, hints_priv);
+ 	if (IS_ERR(region2))
+ 		return PTR_ERR(region2);
+ 
+ 	vregion->region2 = region2;
+ 	err = mlxsw_sp_acl_tcam_group_region_attach(mlxsw_sp,
+ 						    vregion->region->group,
+ 						    region2, priority,
+ 						    vregion->region);
+ 	if (err)
+ 		goto err_group_region_attach;
+ 
+ 	mutex_lock(&vregion->lock);
+ 
+ 	err = mlxsw_sp_acl_tcam_vchunk_migrate_all(mlxsw_sp, vregion);
+ 	if (!vregion->failed_rollback) {
+ 		if (!err) {
+ 			/* In case of successful migration, region2 is used and
+ 			 * the original is unused.
+ 			 */
+ 			unused_region = vregion->region;
+ 			vregion->region = vregion->region2;
+ 		} else {
+ 			/* In case of failure during migration, the original
+ 			 * region is still used.
+ 			 */
+ 			unused_region = vregion->region2;
+ 		}
+ 		mutex_unlock(&vregion->lock);
+ 		vregion->region2 = NULL;
+ 		mlxsw_sp_acl_tcam_group_region_detach(mlxsw_sp, unused_region);
+ 		mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, unused_region);
+ 	} else {
+ 		mutex_unlock(&vregion->lock);
+ 	}
+ 
+ 	return err;
+ 
+ err_group_region_attach:
+ 	vregion->region2 = NULL;
+ 	mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, region2);
+ 	return err;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vregion_rehash(struct mlxsw_sp *mlxsw_sp,
+ 				 struct mlxsw_sp_acl_tcam_vregion *vregion)
+ {
+ 	const struct mlxsw_sp_acl_tcam_ops *ops = mlxsw_sp->acl_tcam_ops;
+ 	void *hints_priv;
+ 	int err;
+ 
+ 	trace_mlxsw_sp_acl_tcam_vregion_rehash(mlxsw_sp, vregion);
+ 	if (vregion->failed_rollback)
+ 		return -EBUSY;
+ 
+ 	hints_priv = ops->region_rehash_hints_get(vregion->region->priv);
+ 	if (IS_ERR(hints_priv)) {
+ 		err = PTR_ERR(hints_priv);
+ 		if (err != -EAGAIN)
+ 			dev_err(mlxsw_sp->bus_info->dev, "Failed get rehash hints\n");
+ 		return err;
+ 	}
+ 
+ 	err = mlxsw_sp_acl_tcam_vregion_migrate(mlxsw_sp, vregion, hints_priv);
+ 	if (err) {
+ 		dev_err(mlxsw_sp->bus_info->dev, "Failed to migrate vregion\n");
+ 		if (vregion->failed_rollback) {
+ 			trace_mlxsw_sp_acl_tcam_vregion_rehash_dis(mlxsw_sp,
+ 								   vregion);
+ 			dev_err(mlxsw_sp->bus_info->dev, "Failed to rollback during vregion migration fail\n");
+ 		}
+ 	}
+ 
+ 	ops->region_rehash_hints_put(hints_priv);
+ 	return err;
+ }
+ 
++>>>>>>> 1263a9ab8224 (mlxsw: spectrum_acl: Introduce vregion mutex)
  static const enum mlxsw_afk_element mlxsw_sp_acl_tcam_pattern_ipv4[] = {
  	MLXSW_AFK_ELEMENT_SRC_SYS_PORT,
  	MLXSW_AFK_ELEMENT_DMAC_32_47,
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
