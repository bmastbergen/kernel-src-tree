arm_pmu: Add support for 64bit event counters

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Suzuki K Poulose <suzuki.poulose@arm.com>
commit e2da97d328d4951d25f6634eda7213f7257417b6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/e2da97d3.failed

Each PMU has a set of 32bit event counters. But in some
special cases, the events could be counted using counters
which are effectively 64bit wide.

e.g, Arm V8 PMUv3 has a 64 bit cycle counter which can count
only the CPU cycles. Also, the PMU can chain the event counters
to effectively count as a 64bit counter.

Add support for tracking the events that uses 64bit counters.
This only affects the periods set for each counter in the core
driver.

	Cc: Will Deacon <will.deacon@arm.com>
	Reviewed-by: Julien Thierry <julien.thierry@arm.com>
	Acked-by: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit e2da97d328d4951d25f6634eda7213f7257417b6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/perf/arm_pmu.c
diff --cc drivers/perf/arm_pmu.c
index a6347d487635,8cad6b535a2c..000000000000
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@@ -28,6 -28,14 +28,17 @@@
  static DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);
  static DEFINE_PER_CPU(int, cpu_irq);
  
++<<<<<<< HEAD
++=======
+ static inline u64 arm_pmu_event_max_period(struct perf_event *event)
+ {
+ 	if (event->hw.flags & ARMPMU_EVT_64BIT)
+ 		return GENMASK_ULL(63, 0);
+ 	else
+ 		return GENMASK_ULL(31, 0);
+ }
+ 
++>>>>>>> e2da97d328d4 (arm_pmu: Add support for 64bit event counters)
  static int
  armpmu_map_cache_event(const unsigned (*cache_map)
  				      [PERF_COUNT_HW_CACHE_MAX]
@@@ -114,8 -122,10 +125,12 @@@ int armpmu_event_set_period(struct perf
  	struct hw_perf_event *hwc = &event->hw;
  	s64 left = local64_read(&hwc->period_left);
  	s64 period = hwc->sample_period;
 -	u64 max_period;
  	int ret = 0;
  
++<<<<<<< HEAD
++=======
+ 	max_period = arm_pmu_event_max_period(event);
++>>>>>>> e2da97d328d4 (arm_pmu: Add support for 64bit event counters)
  	if (unlikely(left <= -period)) {
  		left = period;
  		local64_set(&hwc->period_left, left);
@@@ -153,6 -163,7 +168,10 @@@ u64 armpmu_event_update(struct perf_eve
  	struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
  	struct hw_perf_event *hwc = &event->hw;
  	u64 delta, prev_raw_count, new_raw_count;
++<<<<<<< HEAD
++=======
+ 	u64 max_period = arm_pmu_event_max_period(event);
++>>>>>>> e2da97d328d4 (arm_pmu: Add support for 64bit event counters)
  
  again:
  	prev_raw_count = local64_read(&hwc->prev_count);
@@@ -402,7 -414,7 +422,11 @@@ __hw_perf_event_init(struct perf_event 
  		 * is far less likely to overtake the previous one unless
  		 * you have some serious IRQ latency issues.
  		 */
++<<<<<<< HEAD
 +		hwc->sample_period  = armpmu->max_period >> 1;
++=======
+ 		hwc->sample_period  = arm_pmu_event_max_period(event) >> 1;
++>>>>>>> e2da97d328d4 (arm_pmu: Add support for 64bit event counters)
  		hwc->last_period    = hwc->sample_period;
  		local64_set(&hwc->period_left, hwc->sample_period);
  	}
* Unmerged path drivers/perf/arm_pmu.c
diff --git a/include/linux/perf/arm_pmu.h b/include/linux/perf/arm_pmu.h
index ad5444491975..ef0e6f205909 100644
--- a/include/linux/perf/arm_pmu.h
+++ b/include/linux/perf/arm_pmu.h
@@ -25,6 +25,12 @@
  */
 #define ARMPMU_MAX_HWEVENTS		32
 
+/*
+ * ARM PMU hw_event flags
+ */
+/* Event uses a 64bit counter */
+#define ARMPMU_EVT_64BIT		1
+
 #define HW_OP_UNSUPPORTED		0xFFFF
 #define C(_x)				PERF_COUNT_HW_CACHE_##_x
 #define CACHE_OP_UNSUPPORTED		0xFFFF
