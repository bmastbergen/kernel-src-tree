net/mlx5: EQ, Make EQE access methods inline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 6d2d6fc83a281d51863fb5d08b59333ed1b46cc1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/6d2d6fc8.failed

These are one/two liner generic EQ access methods, better have them
declared static inline in eq.h.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit 6d2d6fc83a281d51863fb5d08b59333ed1b46cc1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index bb2de668b530,6ba8e401a0c7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -41,9 -42,10 +41,8 @@@
  #include "fpga/core.h"
  #include "eswitch.h"
  #include "lib/clock.h"
 -#include "diag/fw_tracer.h"
  
  enum {
- 	MLX5_EQE_SIZE		= sizeof(struct mlx5_eqe),
  	MLX5_EQE_OWNER_INIT_VAL	= 0x1,
  };
  
@@@ -184,199 -189,6 +171,202 @@@ static enum mlx5_dev_event port_subtype
  	return -1;
  }
  
++<<<<<<< HEAD
 +static void eq_update_ci(struct mlx5_eq *eq, int arm)
 +{
 +	__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);
 +	u32 val = (eq->cons_index & 0xffffff) | (eq->eqn << 24);
 +
 +	__raw_writel((__force u32)cpu_to_be32(val), addr);
 +	/* We still want ordering, just not swabbing, so add a barrier */
 +	mb();
 +}
 +
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +static void eqe_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_pagefault *pfault = container_of(work,
 +						     struct mlx5_pagefault,
 +						     work);
 +	struct mlx5_eq *eq = pfault->eq;
 +
 +	mlx5_core_page_fault(eq->dev, pfault);
 +	mempool_free(pfault, eq->pf_ctx.pool);
 +}
 +
 +static void eq_pf_process(struct mlx5_eq *eq)
 +{
 +	struct mlx5_core_dev *dev = eq->dev;
 +	struct mlx5_eqe_page_fault *pf_eqe;
 +	struct mlx5_pagefault *pfault;
 +	struct mlx5_eqe *eqe;
 +	int set_ci = 0;
 +
 +	while ((eqe = next_eqe_sw(eq))) {
 +		pfault = mempool_alloc(eq->pf_ctx.pool, GFP_ATOMIC);
 +		if (!pfault) {
 +			schedule_work(&eq->pf_ctx.work);
 +			break;
 +		}
 +
 +		dma_rmb();
 +		pf_eqe = &eqe->data.page_fault;
 +		pfault->event_subtype = eqe->sub_type;
 +		pfault->bytes_committed = be32_to_cpu(pf_eqe->bytes_committed);
 +
 +		mlx5_core_dbg(dev,
 +			      "PAGE_FAULT: subtype: 0x%02x, bytes_committed: 0x%06x\n",
 +			      eqe->sub_type, pfault->bytes_committed);
 +
 +		switch (eqe->sub_type) {
 +		case MLX5_PFAULT_SUBTYPE_RDMA:
 +			/* RDMA based event */
 +			pfault->type =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) >> 24;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) &
 +				MLX5_24BIT_MASK;
 +			pfault->rdma.r_key =
 +				be32_to_cpu(pf_eqe->rdma.r_key);
 +			pfault->rdma.packet_size =
 +				be16_to_cpu(pf_eqe->rdma.packet_length);
 +			pfault->rdma.rdma_op_len =
 +				be32_to_cpu(pf_eqe->rdma.rdma_op_len);
 +			pfault->rdma.rdma_va =
 +				be64_to_cpu(pf_eqe->rdma.rdma_va);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, r_key: 0x%08x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->rdma.r_key);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: rdma_op_len: 0x%08x, rdma_va: 0x%016llx\n",
 +				      pfault->rdma.rdma_op_len,
 +				      pfault->rdma.rdma_va);
 +			break;
 +
 +		case MLX5_PFAULT_SUBTYPE_WQE:
 +			/* WQE based event */
 +			pfault->type =
 +				(be32_to_cpu(pf_eqe->wqe.pftype_wq) >> 24) & 0x7;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->wqe.token);
 +			pfault->wqe.wq_num =
 +				be32_to_cpu(pf_eqe->wqe.pftype_wq) &
 +				MLX5_24BIT_MASK;
 +			pfault->wqe.wqe_index =
 +				be16_to_cpu(pf_eqe->wqe.wqe_index);
 +			pfault->wqe.packet_size =
 +				be16_to_cpu(pf_eqe->wqe.packet_length);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, wq_num: 0x%06x, wqe_index: 0x%04x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->wqe.wq_num,
 +				      pfault->wqe.wqe_index);
 +			break;
 +
 +		default:
 +			mlx5_core_warn(dev,
 +				       "Unsupported page fault event sub-type: 0x%02hhx\n",
 +				       eqe->sub_type);
 +			/* Unsupported page faults should still be
 +			 * resolved by the page fault handler
 +			 */
 +		}
 +
 +		pfault->eq = eq;
 +		INIT_WORK(&pfault->work, eqe_pf_action);
 +		queue_work(eq->pf_ctx.wq, &pfault->work);
 +
 +		++eq->cons_index;
 +		++set_ci;
 +
 +		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
 +			eq_update_ci(eq, 0);
 +			set_ci = 0;
 +		}
 +	}
 +
 +	eq_update_ci(eq, 1);
 +}
 +
 +static irqreturn_t mlx5_eq_pf_int(int irq, void *eq_ptr)
 +{
 +	struct mlx5_eq *eq = eq_ptr;
 +	unsigned long flags;
 +
 +	if (spin_trylock_irqsave(&eq->pf_ctx.lock, flags)) {
 +		eq_pf_process(eq);
 +		spin_unlock_irqrestore(&eq->pf_ctx.lock, flags);
 +	} else {
 +		schedule_work(&eq->pf_ctx.work);
 +	}
 +
 +	return IRQ_HANDLED;
 +}
 +
 +/* mempool_refill() was proposed but unfortunately wasn't accepted
 + * http://lkml.iu.edu/hypermail/linux/kernel/1512.1/05073.html
 + * Chip workaround.
 + */
 +static void mempool_refill(mempool_t *pool)
 +{
 +	while (pool->curr_nr < pool->min_nr)
 +		mempool_free(mempool_alloc(pool, GFP_KERNEL), pool);
 +}
 +
 +static void eq_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_eq *eq = container_of(work, struct mlx5_eq, pf_ctx.work);
 +
 +	mempool_refill(eq->pf_ctx.pool);
 +
 +	spin_lock_irq(&eq->pf_ctx.lock);
 +	eq_pf_process(eq);
 +	spin_unlock_irq(&eq->pf_ctx.lock);
 +}
 +
 +static int init_pf_ctx(struct mlx5_eq_pagefault *pf_ctx, const char *name)
 +{
 +	spin_lock_init(&pf_ctx->lock);
 +	INIT_WORK(&pf_ctx->work, eq_pf_action);
 +
 +	pf_ctx->wq = alloc_workqueue(name,
 +				     WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
 +				     MLX5_NUM_CMD_EQE);
 +	if (!pf_ctx->wq)
 +		return -ENOMEM;
 +
 +	pf_ctx->pool = mempool_create_kmalloc_pool
 +		(MLX5_NUM_PF_DRAIN, sizeof(struct mlx5_pagefault));
 +	if (!pf_ctx->pool)
 +		goto err_wq;
 +
 +	return 0;
 +err_wq:
 +	destroy_workqueue(pf_ctx->wq);
 +	return -ENOMEM;
 +}
 +
 +int mlx5_core_page_fault_resume(struct mlx5_core_dev *dev, u32 token,
 +				u32 wq_num, u8 type, int error)
 +{
 +	u32 out[MLX5_ST_SZ_DW(page_fault_resume_out)] = {0};
 +	u32 in[MLX5_ST_SZ_DW(page_fault_resume_in)]   = {0};
 +
 +	MLX5_SET(page_fault_resume_in, in, opcode,
 +		 MLX5_CMD_OP_PAGE_FAULT_RESUME);
 +	MLX5_SET(page_fault_resume_in, in, error, !!error);
 +	MLX5_SET(page_fault_resume_in, in, page_fault_type, type);
 +	MLX5_SET(page_fault_resume_in, in, wq_number, wq_num);
 +	MLX5_SET(page_fault_resume_in, in, token, token);
 +
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 +}
 +EXPORT_SYMBOL_GPL(mlx5_core_page_fault_resume);
 +#endif
 +
++=======
++>>>>>>> 6d2d6fc83a28 (net/mlx5: EQ, Make EQE access methods inline)
  static void general_event_handler(struct mlx5_core_dev *dev,
  				  struct mlx5_eqe *eqe)
  {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
