KVM: x86: hyperv: optimize sparse VP set processing

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit f21dd494506ad002a5b6b32e50a5d4ccac6929fe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f21dd494.failed

Rewrite kvm_hv_flush_tlb()/send_ipi_vcpus_mask() making them cleaner and
somewhat more optimal.

hv_vcpu_in_sparse_set() is converted to sparse_set_to_vcpu_mask()
which copies sparse banks u64-at-a-time and then, depending on the
num_mismatched_vp_indexes value, returns immediately or does
vp index to vcpu index conversion by walking all vCPUs.

To support the change and make kvm_hv_send_ipi() look similar to
kvm_hv_flush_tlb() send_ipi_vcpus_mask() is introduced.

	Suggested-by: Roman Kagan <rkagan@virtuozzo.com>
	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f21dd494506ad002a5b6b32e50a5d4ccac6929fe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
diff --cc arch/x86/kvm/hyperv.c
index ff3cd8ffb638,df9ef8f1d2c8..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -1373,57 -1372,109 +1391,112 @@@ static u64 kvm_hv_flush_tlb(struct kvm_
  	 * vcpu->arch.cr3 may not be up-to-date for running vCPUs so we can't
  	 * analyze it here, flush TLB regardless of the specified address space.
  	 */
- 	cpumask_clear(&hv_vcpu->tlb_flush);
- 
- 	if (all_cpus) {
- 		kvm_make_vcpus_request_mask(kvm,
+ 	kvm_make_vcpus_request_mask(kvm,
  				    KVM_REQ_TLB_FLUSH | KVM_REQUEST_NO_WAKEUP,
- 				    NULL, &hv_vcpu->tlb_flush);
- 		goto ret_success;
- 	}
+ 				    vcpu_mask, &hv_vcpu->tlb_flush);
  
- 	if (atomic_read(&hv->num_mismatched_vp_indexes)) {
- 		kvm_for_each_vcpu(i, vcpu, kvm) {
- 			if (hv_vcpu_in_sparse_set(&vcpu->arch.hyperv,
- 						  sparse_banks,
- 						  valid_bank_mask))
- 				__set_bit(i, vcpu_bitmap);
- 		}
- 		goto flush_request;
+ ret_success:
+ 	/* We always do full TLB flush, set rep_done = rep_cnt. */
+ 	return (u64)HV_STATUS_SUCCESS |
+ 		((u64)rep_cnt << HV_HYPERCALL_REP_COMP_OFFSET);
+ }
+ 
++<<<<<<< HEAD
++=======
+ static void kvm_send_ipi_to_many(struct kvm *kvm, u32 vector,
+ 				 unsigned long *vcpu_bitmap)
+ {
+ 	struct kvm_lapic_irq irq = {
+ 		.delivery_mode = APIC_DM_FIXED,
+ 		.vector = vector
+ 	};
+ 	struct kvm_vcpu *vcpu;
+ 	int i;
+ 
+ 	kvm_for_each_vcpu(i, vcpu, kvm) {
+ 		if (vcpu_bitmap && !test_bit(i, vcpu_bitmap))
+ 			continue;
+ 
+ 		/* We fail only when APIC is disabled */
+ 		kvm_apic_set_irq(vcpu, &irq, NULL);
  	}
+ }
  
- 	/*
- 	 * num_mismatched_vp_indexes is zero so every vcpu has
- 	 * vp_index == vcpu_idx.
- 	 */
- 	sbank = 0;
- 	for_each_set_bit(bank, (unsigned long *)&valid_bank_mask,
- 			 BITS_PER_LONG) {
- 		for_each_set_bit(i,
- 				 (unsigned long *)&sparse_banks[sbank],
- 				 BITS_PER_LONG) {
- 			u32 vp_index = bank * 64 + i;
- 
- 			/* A non-existent vCPU was specified */
- 			if (vp_index >= KVM_MAX_VCPUS)
- 				return HV_STATUS_INVALID_HYPERCALL_INPUT;
+ static u64 kvm_hv_send_ipi(struct kvm_vcpu *current_vcpu, u64 ingpa, u64 outgpa,
+ 			   bool ex, bool fast)
+ {
+ 	struct kvm *kvm = current_vcpu->kvm;
+ 	struct hv_send_ipi_ex send_ipi_ex;
+ 	struct hv_send_ipi send_ipi;
+ 	u64 vp_bitmap[KVM_HV_MAX_SPARSE_VCPU_SET_BITS];
+ 	DECLARE_BITMAP(vcpu_bitmap, KVM_MAX_VCPUS);
+ 	unsigned long *vcpu_mask;
+ 	unsigned long valid_bank_mask;
+ 	u64 sparse_banks[64];
+ 	int sparse_banks_len;
+ 	u32 vector;
+ 	bool all_cpus;
  
- 			__set_bit(vp_index, vcpu_bitmap);
+ 	if (!ex) {
+ 		if (!fast) {
+ 			if (unlikely(kvm_read_guest(kvm, ingpa, &send_ipi,
+ 						    sizeof(send_ipi))))
+ 				return HV_STATUS_INVALID_HYPERCALL_INPUT;
+ 			sparse_banks[0] = send_ipi.cpu_mask;
+ 			vector = send_ipi.vector;
+ 		} else {
+ 			/* 'reserved' part of hv_send_ipi should be 0 */
+ 			if (unlikely(ingpa >> 32 != 0))
+ 				return HV_STATUS_INVALID_HYPERCALL_INPUT;
+ 			sparse_banks[0] = outgpa;
+ 			vector = (u32)ingpa;
  		}
- 		sbank++;
+ 		all_cpus = false;
+ 		valid_bank_mask = BIT_ULL(0);
+ 
+ 		trace_kvm_hv_send_ipi(vector, sparse_banks[0]);
+ 	} else {
+ 		if (unlikely(kvm_read_guest(kvm, ingpa, &send_ipi_ex,
+ 					    sizeof(send_ipi_ex))))
+ 			return HV_STATUS_INVALID_HYPERCALL_INPUT;
+ 
+ 		trace_kvm_hv_send_ipi_ex(send_ipi_ex.vector,
+ 					 send_ipi_ex.vp_set.format,
+ 					 send_ipi_ex.vp_set.valid_bank_mask);
+ 
+ 		vector = send_ipi_ex.vector;
+ 		valid_bank_mask = send_ipi_ex.vp_set.valid_bank_mask;
+ 		sparse_banks_len = bitmap_weight(&valid_bank_mask, 64) *
+ 			sizeof(sparse_banks[0]);
+ 
+ 		all_cpus = send_ipi_ex.vp_set.format == HV_GENERIC_SET_ALL;
+ 
+ 		if (!sparse_banks_len)
+ 			goto ret_success;
+ 
+ 		if (!all_cpus &&
+ 		    kvm_read_guest(kvm,
+ 				   ingpa + offsetof(struct hv_send_ipi_ex,
+ 						    vp_set.bank_contents),
+ 				   sparse_banks,
+ 				   sparse_banks_len))
+ 			return HV_STATUS_INVALID_HYPERCALL_INPUT;
  	}
  
- flush_request:
- 	kvm_make_vcpus_request_mask(kvm,
- 				    KVM_REQ_TLB_FLUSH | KVM_REQUEST_NO_WAKEUP,
- 				    vcpu_bitmap, &hv_vcpu->tlb_flush);
+ 	if ((vector < HV_IPI_LOW_VECTOR) || (vector > HV_IPI_HIGH_VECTOR))
+ 		return HV_STATUS_INVALID_HYPERCALL_INPUT;
+ 
+ 	vcpu_mask = all_cpus ? NULL :
+ 		sparse_set_to_vcpu_mask(kvm, sparse_banks, valid_bank_mask,
+ 					vp_bitmap, vcpu_bitmap);
+ 
+ 	kvm_send_ipi_to_many(kvm, vector, vcpu_mask);
  
  ret_success:
- 	/* We always do full TLB flush, set rep_done = rep_cnt. */
- 	return (u64)HV_STATUS_SUCCESS |
- 		((u64)rep_cnt << HV_HYPERCALL_REP_COMP_OFFSET);
+ 	return HV_STATUS_SUCCESS;
  }
  
++>>>>>>> f21dd494506a (KVM: x86: hyperv: optimize sparse VP set processing)
  bool kvm_hv_hypercall_enabled(struct kvm *kvm)
  {
  	return READ_ONCE(kvm->arch.hyperv.hv_hypercall) & HV_X64_MSR_HYPERCALL_ENABLE;
* Unmerged path arch/x86/kvm/hyperv.c
