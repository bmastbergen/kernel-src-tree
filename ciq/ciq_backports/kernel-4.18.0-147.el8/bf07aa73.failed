net/mlx5e: Support offloading tc priorities and chains for eswitch flows

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Paul Blakey <paulb@mellanox.com>
commit bf07aa730a04a375bc10d09df1e81357af1d4477
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/bf07aa73.failed

Currently we fail when user specify a non-zero chain, this patch adds the
support for it and tc priorities. To get to a new chain, use the tc
goto action.

Currently we support a fixed prio range 1-16, and chain range 0-3.

	Signed-off-by: Paul Blakey <paulb@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit bf07aa730a04a375bc10d09df1e81357af1d4477)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index bd15011e5202,608025ca5c04..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -816,24 -821,109 +816,41 @@@ static int mlx5e_attach_encap(struct ml
  			      struct ip_tunnel_info *tun_info,
  			      struct net_device *mirred_dev,
  			      struct net_device **encap_dev,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct netlink_ext_ack *extack);
 +			      struct mlx5e_tc_flow *flow);
  
  static struct mlx5_flow_handle *
 -mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
 -			   struct mlx5e_tc_flow *flow,
 -			   struct mlx5_flow_spec *spec,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	rule = mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
 -	if (IS_ERR(rule))
 -		return rule;
 -
 -	if (attr->mirror_count) {
 -		flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, spec, attr);
 -		if (IS_ERR(flow->rule[1])) {
 -			mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
 -			return flow->rule[1];
 -		}
 -	}
 -
 -	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
 -			     struct mlx5e_tc_flow *flow,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 -
 -	if (attr->mirror_count)
 -		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 -
 -	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 -}
 -
 -static struct mlx5_flow_handle *
 -mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct mlx5_flow_spec *spec,
 -			      struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
 -	slow_attr->mirror_count = 0,
 -	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN,
 -
 -	rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, slow_attr);
 -	if (!IS_ERR(rule))
 -		flow->flags |= MLX5E_TC_FLOW_SLOW;
 -
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
 -				  struct mlx5e_tc_flow *flow,
 -				  struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
 -	flow->flags &= ~MLX5E_TC_FLOW_SLOW;
 -}
 -
 -static int
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
  		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	u32 max_chain = mlx5_eswitch_get_chain_range(esw);
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+ 	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
  	struct net_device *out_dev, *encap_dev = NULL;
 +	struct mlx5_flow_handle *rule = NULL;
  	struct mlx5_fc *counter = NULL;
  	struct mlx5e_rep_priv *rpriv;
  	struct mlx5e_priv *out_priv;
 -	int err = 0, encap_err = 0;
 +	int err;
  
- 	/* keep the old behaviour, use same prio for all offloaded rules */
- 	attr->prio = 1;
+ 	/* if prios are not supported, keep the old behaviour of using same prio
+ 	 * for all offloaded rules.
+ 	 */
+ 	if (!mlx5_eswitch_prios_supported(esw))
+ 		attr->prio = 1;
+ 
+ 	if (attr->chain > max_chain) {
+ 		NL_SET_ERR_MSG(extack, "Requested chain is out of supported range");
+ 		err = -EOPNOTSUPP;
+ 		goto err_max_prio_chain;
+ 	}
+ 
+ 	if (attr->prio > max_prio) {
+ 		NL_SET_ERR_MSG(extack, "Requested priority is out of supported range");
+ 		err = -EOPNOTSUPP;
+ 		goto err_max_prio_chain;
+ 	}
  
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT) {
  		out_dev = __dev_get_by_index(dev_net(priv->netdev),
@@@ -906,7 -992,8 +923,12 @@@ err_add_vlan
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT)
  		mlx5e_detach_encap(priv, flow);
  err_attach_encap:
++<<<<<<< HEAD
 +	return rule;
++=======
+ err_max_prio_chain:
+ 	return err;
++>>>>>>> bf07aa730a04 (net/mlx5e: Support offloading tc priorities and chains for eswitch flows)
  }
  
  static void mlx5e_tc_del_fdb_flow(struct mlx5e_priv *priv,
@@@ -2589,10 -2789,62 +2611,11 @@@ out_err
  	return err;
  }
  
 -static int parse_tc_vlan_action(struct mlx5e_priv *priv,
 -				const struct tc_action *a,
 -				struct mlx5_esw_flow_attr *attr,
 -				u32 *action)
 -{
 -	u8 vlan_idx = attr->total_vlan;
 -
 -	if (vlan_idx >= MLX5_FS_VLAN_DEPTH)
 -		return -EOPNOTSUPP;
 -
 -	if (tcf_vlan_action(a) == TCA_VLAN_ACT_POP) {
 -		if (vlan_idx) {
 -			if (!mlx5_eswitch_vlan_actions_supported(priv->mdev,
 -								 MLX5_FS_VLAN_DEPTH))
 -				return -EOPNOTSUPP;
 -
 -			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_POP_2;
 -		} else {
 -			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_POP;
 -		}
 -	} else if (tcf_vlan_action(a) == TCA_VLAN_ACT_PUSH) {
 -		attr->vlan_vid[vlan_idx] = tcf_vlan_push_vid(a);
 -		attr->vlan_prio[vlan_idx] = tcf_vlan_push_prio(a);
 -		attr->vlan_proto[vlan_idx] = tcf_vlan_push_proto(a);
 -		if (!attr->vlan_proto[vlan_idx])
 -			attr->vlan_proto[vlan_idx] = htons(ETH_P_8021Q);
 -
 -		if (vlan_idx) {
 -			if (!mlx5_eswitch_vlan_actions_supported(priv->mdev,
 -								 MLX5_FS_VLAN_DEPTH))
 -				return -EOPNOTSUPP;
 -
 -			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH_2;
 -		} else {
 -			if (!mlx5_eswitch_vlan_actions_supported(priv->mdev, 1) &&
 -			    (tcf_vlan_push_proto(a) != htons(ETH_P_8021Q) ||
 -			     tcf_vlan_push_prio(a)))
 -				return -EOPNOTSUPP;
 -
 -			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH;
 -		}
 -	} else { /* action is TCA_VLAN_ACT_MODIFY */
 -		return -EOPNOTSUPP;
 -	}
 -
 -	attr->total_vlan = vlan_idx + 1;
 -
 -	return 0;
 -}
 -
  static int parse_tc_fdb_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
  				struct mlx5e_tc_flow_parse_attr *parse_attr,
 -				struct mlx5e_tc_flow *flow,
 -				struct netlink_ext_ack *extack)
 +				struct mlx5e_tc_flow *flow)
  {
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
  	struct mlx5e_rep_priv *rpriv = priv->ppriv;
  	struct ip_tunnel_info *info = NULL;
@@@ -2794,46 -3042,161 +2836,174 @@@ int mlx5e_configure_flower(struct mlx5e
  	flow->priv = priv;
  
  	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 -	if (err)
 +	if (err < 0)
  		goto err_free;
  
 -	*__flow = flow;
 -	*__parse_attr = parse_attr;
 +	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 +		err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
 +	} else {
 +		err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
 +	}
  
 -	return 0;
 +	if (IS_ERR(flow->rule[0])) {
 +		err = PTR_ERR(flow->rule[0]);
 +		if (err != -EAGAIN)
 +			goto err_free;
 +	}
  
 -err_free:
 -	kfree(flow);
 -	kvfree(parse_attr);
 -	return err;
 -}
 +	if (err != -EAGAIN)
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 +
++<<<<<<< HEAD
 +	if (!(flow->flags & MLX5E_TC_FLOW_ESWITCH) ||
 +	    !(flow->esw_attr->action &
 +	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
 +		kvfree(parse_attr);
  
 +	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 +	if (err) {
 +		mlx5e_tc_del_flow(priv, flow);
 +		kfree(flow);
++=======
+ static int
+ mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_ESWITCH;
+ 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	flow->esw_attr->chain = f->common.chain_index;
+ 	flow->esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+ 	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	if (!(flow->esw_attr->action &
+ 	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
+ 		kvfree(parse_attr);
+ 
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_nic_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	/* multi-chain not supported for NIC rules */
+ 	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
+ 		return -EOPNOTSUPP;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_NIC;
+ 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
+ 	kvfree(parse_attr);
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_tc_add_flow(struct mlx5e_priv *priv,
+ 		  struct tc_cls_flower_offload *f,
+ 		  int flags,
+ 		  struct mlx5e_tc_flow **flow)
+ {
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	u16 flow_flags;
+ 	int err;
+ 
+ 	get_flags(flags, &flow_flags);
+ 
+ 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (esw && esw->mode == SRIOV_OFFLOADS)
+ 		err = mlx5e_add_fdb_flow(priv, f, flow_flags, flow);
+ 	else
+ 		err = mlx5e_add_nic_flow(priv, f, flow_flags, flow);
+ 
+ 	return err;
+ }
+ 
+ int mlx5e_configure_flower(struct mlx5e_priv *priv,
+ 			   struct tc_cls_flower_offload *f, int flags)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct rhashtable *tc_ht = get_tc_ht(priv);
+ 	struct mlx5e_tc_flow *flow;
+ 	int err = 0;
+ 
+ 	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
+ 	if (flow) {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "flow cookie already exists, ignoring");
+ 		netdev_warn_once(priv->netdev,
+ 				 "flow cookie %lx already exists, ignoring\n",
+ 				 f->cookie);
+ 		goto out;
++>>>>>>> bf07aa730a04 (net/mlx5e: Support offloading tc priorities and chains for eswitch flows)
  	}
  
 -	err = mlx5e_tc_add_flow(priv, f, flags, &flow);
 -	if (err)
 -		goto out;
 -
 -	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 -	if (err)
 -		goto err_free;
 -
 -	return 0;
 +	return err;
  
  err_free:
 -	mlx5e_tc_del_flow(priv, flow);
 +	kvfree(parse_attr);
  	kfree(flow);
 -out:
  	return err;
  }
  
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index ce0d6d1bc78b..02fbe9aa629a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -3381,9 +3381,6 @@ static int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
 {
 	struct mlx5e_priv *priv = cb_priv;
 
-	if (!tc_cls_can_offload_and_chain0(priv->netdev, type_data))
-		return -EOPNOTSUPP;
-
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
 		return mlx5e_setup_tc_cls_flower(priv, type_data, MLX5E_TC_INGRESS);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index ed2450e3bde5..5c569a2df523 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@ -757,9 +757,6 @@ static int mlx5e_rep_setup_tc_cb_egdev(enum tc_setup_type type, void *type_data,
 {
 	struct mlx5e_priv *priv = cb_priv;
 
-	if (!tc_cls_can_offload_and_chain0(priv->netdev, type_data))
-		return -EOPNOTSUPP;
-
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
 		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, MLX5E_TC_EGRESS);
@@ -773,9 +770,6 @@ static int mlx5e_rep_setup_tc_cb(enum tc_setup_type type, void *type_data,
 {
 	struct mlx5e_priv *priv = cb_priv;
 
-	if (!tc_cls_can_offload_and_chain0(priv->netdev, type_data))
-		return -EOPNOTSUPP;
-
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
 		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, MLX5E_TC_INGRESS);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 64b4db7d857a..a41bf6a2da7f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@ -71,7 +71,7 @@ u16 mlx5_eswitch_get_prio_range(struct mlx5_eswitch *esw)
 	if (esw->fdb_table.flags & ESW_FDB_CHAINS_AND_PRIOS_SUPPORTED)
 		return FDB_MAX_PRIO;
 
-	return U16_MAX;
+	return 1;
 }
 
 struct mlx5_flow_handle *
