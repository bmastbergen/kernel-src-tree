net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Paul Blakey <paulb@mellanox.com>
commit 5dbe906ff1d5040013d30df1e8c769d28af7e0f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/5dbe906f.failed

When adding a vxlan tc rule, and a neighbour isn't available, we
don't insert any rule to hardware. Once we enable offloading flows
with multiple priorities, a packet that should have matched this rule
will continue in hardware pipeline and might match a wrong one.

This is unlike in tc software path where it will be matched and
forwarded to the vxlan device (which will cause a ARP lookup
eventually) and stop processing further tc filters.

To address that, when when a neighbour isn't available (EAGAIN from
attach_encap), or gets deleted, change the original action to be a
forward to slow path instead. Neighbour update will restore the original
action once the neighbour becomes available. This will be done atomically
so at any given time we will have a the correct match.

	Signed-off-by: Paul Blakey <paulb@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 5dbe906ff1d5040013d30df1e8c769d28af7e0f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index bd15011e5202,cb66964aa1ff..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -816,12 -821,81 +817,82 @@@ static int mlx5e_attach_encap(struct ml
  			      struct ip_tunnel_info *tun_info,
  			      struct net_device *mirred_dev,
  			      struct net_device **encap_dev,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct netlink_ext_ack *extack);
 +			      struct mlx5e_tc_flow *flow);
  
  static struct mlx5_flow_handle *
++<<<<<<< HEAD
++=======
+ mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
+ 			   struct mlx5e_tc_flow *flow,
+ 			   struct mlx5_flow_spec *spec,
+ 			   struct mlx5_esw_flow_attr *attr)
+ {
+ 	struct mlx5_flow_handle *rule;
+ 
+ 	rule = mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
+ 	if (IS_ERR(rule))
+ 		return rule;
+ 
+ 	if (attr->mirror_count) {
+ 		flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, spec, attr);
+ 		if (IS_ERR(flow->rule[1])) {
+ 			mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
+ 			return flow->rule[1];
+ 		}
+ 	}
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
+ 	return rule;
+ }
+ 
+ static void
+ mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
+ 			     struct mlx5e_tc_flow *flow,
+ 			   struct mlx5_esw_flow_attr *attr)
+ {
+ 	flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
+ 
+ 	if (attr->mirror_count)
+ 		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
+ 
+ 	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
+ 			      struct mlx5e_tc_flow *flow,
+ 			      struct mlx5_flow_spec *spec,
+ 			      struct mlx5_esw_flow_attr *slow_attr)
+ {
+ 	struct mlx5_flow_handle *rule;
+ 
+ 	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
+ 	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
+ 	slow_attr->mirror_count = 0,
+ 	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN,
+ 
+ 	rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, slow_attr);
+ 	if (!IS_ERR(rule))
+ 		flow->flags |= MLX5E_TC_FLOW_SLOW;
+ 
+ 	return rule;
+ }
+ 
+ static void
+ mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
+ 				  struct mlx5e_tc_flow *flow,
+ 				  struct mlx5_esw_flow_attr *slow_attr)
+ {
+ 	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
+ 	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
+ 	flow->flags &= ~MLX5E_TC_FLOW_SLOW;
+ }
+ 
+ static int
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
  		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
@@@ -876,25 -945,25 +947,45 @@@
  		attr->counter = counter;
  	}
  
 -	/* we get here if (1) there's no error or when
 +	/* we get here if (1) there's no error (rule being null) or when
  	 * (2) there's an encap action and we're on -EAGAIN (no valid neigh)
  	 */
++<<<<<<< HEAD
 +	if (rule != ERR_PTR(-EAGAIN)) {
 +		rule = mlx5_eswitch_add_offloaded_rule(esw, &parse_attr->spec, attr);
 +		if (IS_ERR(rule))
 +			goto err_add_rule;
 +
 +		if (attr->mirror_count) {
 +			flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, &parse_attr->spec, attr);
 +			if (IS_ERR(flow->rule[1]))
 +				goto err_fwd_rule;
 +		}
++=======
+ 	if (encap_err == -EAGAIN) {
+ 		/* continue with goto slow path rule instead */
+ 		struct mlx5_esw_flow_attr slow_attr;
+ 
+ 		flow->rule[0] = mlx5e_tc_offload_to_slow_path(esw, flow, &parse_attr->spec, &slow_attr);
+ 	} else {
+ 		flow->rule[0] = mlx5e_tc_offload_fdb_rules(esw, flow, &parse_attr->spec, attr);
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	}
 +	return rule;
  
++<<<<<<< HEAD
 +err_fwd_rule:
 +	mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
 +	rule = flow->rule[1];
++=======
+ 	if (IS_ERR(flow->rule[0])) {
+ 		err = PTR_ERR(flow->rule[0]);
+ 		goto err_add_rule;
+ 	}
+ 
+ 	return 0;
+ 
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  err_add_rule:
  	mlx5_fc_destroy(esw->dev, counter);
  err_create_counter:
@@@ -914,12 -983,13 +1005,20 @@@ static void mlx5e_tc_del_fdb_flow(struc
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+ 	struct mlx5_esw_flow_attr slow_attr;
  
  	if (flow->flags & MLX5E_TC_FLOW_OFFLOADED) {
++<<<<<<< HEAD
 +		flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 +		if (attr->mirror_count)
 +			mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 +		mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
++=======
+ 		if (flow->flags & MLX5E_TC_FLOW_SLOW)
+ 			mlx5e_tc_unoffload_from_slow_path(esw, flow, &slow_attr);
+ 		else
+ 			mlx5e_tc_unoffload_fdb_rules(esw, flow, attr);
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	}
  
  	mlx5_eswitch_del_vlan_action(esw, attr);
@@@ -940,7 -1010,9 +1039,13 @@@ void mlx5e_tc_encap_flows_add(struct ml
  			      struct mlx5e_encap_entry *e)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
++<<<<<<< HEAD
 +	struct mlx5_esw_flow_attr *esw_attr;
++=======
+ 	struct mlx5_esw_flow_attr slow_attr, *esw_attr;
+ 	struct mlx5_flow_handle *rule;
+ 	struct mlx5_flow_spec *spec;
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	struct mlx5e_tc_flow *flow;
  	int err;
  
@@@ -959,26 -1031,20 +1064,41 @@@
  	list_for_each_entry(flow, &e->flows, encap) {
  		esw_attr = flow->esw_attr;
  		esw_attr->encap_id = e->encap_id;
++<<<<<<< HEAD
 +		flow->rule[0] = mlx5_eswitch_add_offloaded_rule(esw, &esw_attr->parse_attr->spec, esw_attr);
 +		if (IS_ERR(flow->rule[0])) {
 +			err = PTR_ERR(flow->rule[0]);
++=======
+ 		spec = &esw_attr->parse_attr->spec;
+ 
+ 		/* update from slow path rule to encap rule */
+ 		rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, esw_attr);
+ 		if (IS_ERR(rule)) {
+ 			err = PTR_ERR(rule);
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  			mlx5_core_warn(priv->mdev, "Failed to update cached encapsulation flow, %d\n",
  				       err);
  			continue;
  		}
  
++<<<<<<< HEAD
 +		if (esw_attr->mirror_count) {
 +			flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, &esw_attr->parse_attr->spec, esw_attr);
 +			if (IS_ERR(flow->rule[1])) {
 +				mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], esw_attr);
 +				err = PTR_ERR(flow->rule[1]);
 +				mlx5_core_warn(priv->mdev, "Failed to update cached mirror flow, %d\n",
 +					       err);
 +				continue;
 +			}
 +		}
 +
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
++=======
+ 		mlx5e_tc_unoffload_from_slow_path(esw, flow, &slow_attr);
+ 		flow->flags |= MLX5E_TC_FLOW_OFFLOADED; /* was unset when slow path rule removed */
+ 		flow->rule[0] = rule;
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	}
  }
  
@@@ -986,17 -1052,28 +1106,39 @@@ void mlx5e_tc_encap_flows_del(struct ml
  			      struct mlx5e_encap_entry *e)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	struct mlx5_esw_flow_attr slow_attr;
+ 	struct mlx5_flow_handle *rule;
+ 	struct mlx5_flow_spec *spec;
  	struct mlx5e_tc_flow *flow;
+ 	int err;
  
  	list_for_each_entry(flow, &e->flows, encap) {
++<<<<<<< HEAD
 +		if (flow->flags & MLX5E_TC_FLOW_OFFLOADED) {
 +			struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 +
 +			flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 +			if (attr->mirror_count)
 +				mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 +			mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 +		}
++=======
+ 		spec = &flow->esw_attr->parse_attr->spec;
+ 
+ 		/* update from encap rule to slow path rule */
+ 		rule = mlx5e_tc_offload_to_slow_path(esw, flow, spec, &slow_attr);
+ 
+ 		if (IS_ERR(rule)) {
+ 			err = PTR_ERR(rule);
+ 			mlx5_core_warn(priv->mdev, "Failed to update slow path (encap) flow, %d\n",
+ 				       err);
+ 			continue;
+ 		}
+ 
+ 		mlx5e_tc_unoffload_fdb_rules(esw, flow, flow->esw_attr);
+ 		flow->flags |= MLX5E_TC_FLOW_OFFLOADED; /* was unset when fast path rule removed */
+ 		flow->rule[0] = rule;
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	}
  
  	if (e->flags & MLX5_ENCAP_ENTRY_VALID) {
@@@ -2756,31 -2982,15 +2898,39 @@@ static struct rhashtable *get_tc_ht(str
  		return &priv->fs.tc.ht;
  }
  
++<<<<<<< HEAD
 +int mlx5e_configure_flower(struct mlx5e_priv *priv,
 +			   struct tc_cls_flower_offload *f, int flags)
++=======
+ static int
+ mlx5e_alloc_flow(struct mlx5e_priv *priv, int attr_size,
+ 		 struct tc_cls_flower_offload *f, u16 flow_flags,
+ 		 struct mlx5e_tc_flow_parse_attr **__parse_attr,
+ 		 struct mlx5e_tc_flow **__flow)
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  {
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5e_tc_flow_parse_attr *parse_attr;
 +	struct rhashtable *tc_ht = get_tc_ht(priv);
  	struct mlx5e_tc_flow *flow;
 -	int err;
 +	int attr_size, err = 0;
 +	u8 flow_flags = 0;
 +
 +	get_flags(flags, &flow_flags);
 +
 +	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
 +	if (flow) {
 +		netdev_warn_once(priv->netdev, "flow cookie %lx already exists, ignoring\n", f->cookie);
 +		return 0;
 +	}
 +
 +	if (esw && esw->mode == SRIOV_OFFLOADS) {
 +		flow_flags |= MLX5E_TC_FLOW_ESWITCH;
 +		attr_size  = sizeof(struct mlx5_esw_flow_attr);
 +	} else {
 +		flow_flags |= MLX5E_TC_FLOW_NIC;
 +		attr_size  = sizeof(struct mlx5_nic_flow_attr);
 +	}
  
  	flow = kzalloc(sizeof(*flow) + attr_size, GFP_KERNEL);
  	parse_attr = kvzalloc(sizeof(*parse_attr), GFP_KERNEL);
@@@ -2794,46 -3004,152 +2944,171 @@@
  	flow->priv = priv;
  
  	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 -	if (err)
 +	if (err < 0)
  		goto err_free;
  
 -	*__flow = flow;
 -	*__parse_attr = parse_attr;
 +	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 +		err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
 +	} else {
 +		err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
 +	}
  
 -	return 0;
 +	if (IS_ERR(flow->rule[0])) {
 +		err = PTR_ERR(flow->rule[0]);
 +		if (err != -EAGAIN)
 +			goto err_free;
 +	}
 +
++<<<<<<< HEAD
 +	if (err != -EAGAIN)
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 +
 +	if (!(flow->flags & MLX5E_TC_FLOW_ESWITCH) ||
 +	    !(flow->esw_attr->action &
 +	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
 +		kvfree(parse_attr);
  
 +	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 +	if (err) {
 +		mlx5e_tc_del_flow(priv, flow);
 +		kfree(flow);
++=======
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_ESWITCH;
+ 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	if (!(flow->esw_attr->action &
+ 	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
+ 		kvfree(parse_attr);
+ 
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_nic_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_NIC;
+ 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
+ 	kvfree(parse_attr);
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_tc_add_flow(struct mlx5e_priv *priv,
+ 		  struct tc_cls_flower_offload *f,
+ 		  int flags,
+ 		  struct mlx5e_tc_flow **flow)
+ {
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	u16 flow_flags;
+ 	int err;
+ 
+ 	get_flags(flags, &flow_flags);
+ 
+ 	if (esw && esw->mode == SRIOV_OFFLOADS)
+ 		err = mlx5e_add_fdb_flow(priv, f, flow_flags, flow);
+ 	else
+ 		err = mlx5e_add_nic_flow(priv, f, flow_flags, flow);
+ 
+ 	return err;
+ }
+ 
+ int mlx5e_configure_flower(struct mlx5e_priv *priv,
+ 			   struct tc_cls_flower_offload *f, int flags)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct rhashtable *tc_ht = get_tc_ht(priv);
+ 	struct mlx5e_tc_flow *flow;
+ 	int err = 0;
+ 
+ 	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
+ 	if (flow) {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "flow cookie already exists, ignoring");
+ 		netdev_warn_once(priv->netdev,
+ 				 "flow cookie %lx already exists, ignoring\n",
+ 				 f->cookie);
+ 		goto out;
++>>>>>>> 5dbe906ff1d5 (net/mlx5e: Use a slow path rule instead if vxlan neighbour isn't available)
  	}
  
 -	err = mlx5e_tc_add_flow(priv, f, flags, &flow);
 -	if (err)
 -		goto out;
 -
 -	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 -	if (err)
 -		goto err_free;
 -
 -	return 0;
 +	return err;
  
  err_free:
 -	mlx5e_tc_del_flow(priv, flow);
 +	kvfree(parse_attr);
  	kfree(flow);
 -out:
  	return err;
  }
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
