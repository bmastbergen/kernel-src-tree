sched/fair: Add tmp_alone_branch assertion

jira LE-1907
cve CVE-2018-20784
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 5d299eabea5a251fbf66e8277704b874bbba92dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/5d299eab.failed

The magic in list_add_leaf_cfs_rq() requires that at the end of
enqueue_task_fair():

  rq->tmp_alone_branch == &rq->lead_cfs_rq_list

If this is violated, list integrity is compromised for list entries
and the tmp_alone_branch pointer might dangle.

Also, reflow list_add_leaf_cfs_rq() while there. This looses one
indentation level and generates a form that's convenient for the next
patch.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 5d299eabea5a251fbf66e8277704b874bbba92dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 1579a63f92e5,d6a536dec0ca..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -355,10 -350,14 +360,21 @@@ static inline void list_del_leaf_cfs_rq
  	}
  }
  
++<<<<<<< HEAD
 +/* Iterate thr' all leaf cfs_rq's on a runqueue */
 +#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)			\
 +	list_for_each_entry_safe(cfs_rq, pos, &rq->leaf_cfs_rq_list,	\
 +				 leaf_cfs_rq_list)
++=======
+ static inline void assert_list_leaf_cfs_rq(struct rq *rq)
+ {
+ 	SCHED_WARN_ON(rq->tmp_alone_branch != &rq->leaf_cfs_rq_list);
+ }
+ 
+ /* Iterate through all cfs_rq's on a runqueue in bottom-up order */
+ #define for_each_leaf_cfs_rq(rq, cfs_rq) \
+ 	list_for_each_entry_rcu(cfs_rq, &rq->leaf_cfs_rq_list, leaf_cfs_rq_list)
++>>>>>>> 5d299eabea5a (sched/fair: Add tmp_alone_branch assertion)
  
  /* Do the two (enqueued) entities belong to the same group ? */
  static inline struct cfs_rq *
@@@ -451,8 -443,12 +467,17 @@@ static inline void list_del_leaf_cfs_rq
  {
  }
  
++<<<<<<< HEAD
 +#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)	\
 +		for (cfs_rq = &rq->cfs, pos = NULL; cfs_rq; cfs_rq = pos)
++=======
+ static inline void assert_list_leaf_cfs_rq(struct rq *rq)
+ {
+ }
+ 
+ #define for_each_leaf_cfs_rq(rq, cfs_rq)	\
+ 		for (cfs_rq = &rq->cfs; cfs_rq; cfs_rq = NULL)
++>>>>>>> 5d299eabea5a (sched/fair: Add tmp_alone_branch assertion)
  
  static inline struct sched_entity *parent_entity(struct sched_entity *se)
  {
@@@ -5446,9 -5165,29 +5471,11 @@@ enqueue_task_fair(struct rq *rq, struc
  		update_cfs_group(se);
  	}
  
 -	if (!se) {
 +	if (!se)
  		add_nr_running(rq, 1);
 -		/*
 -		 * Since new tasks are assigned an initial util_avg equal to
 -		 * half of the spare capacity of their CPU, tiny tasks have the
 -		 * ability to cross the overutilized threshold, which will
 -		 * result in the load balancer ruining all the task placement
 -		 * done by EAS. As a way to mitigate that effect, do not account
 -		 * for the first enqueue operation of new tasks during the
 -		 * overutilized flag detection.
 -		 *
 -		 * A better way of solving this problem would be to wait for
 -		 * the PELT signals of tasks to converge before taking them
 -		 * into account, but that is not straightforward to implement,
 -		 * and the following generally works well enough in practice.
 -		 */
 -		if (flags & ENQUEUE_WAKEUP)
 -			update_overutilized_status(rq);
 -
 -	}
  
+ 	assert_list_leaf_cfs_rq(rq);
+ 
  	hrtick_update(rq);
  }
  
* Unmerged path kernel/sched/fair.c
