net/mlx5: EQ, Introduce atomic notifier chain subscription API

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 0f597ed435b9ea1296e25474b762bedceba97a50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/0f597ed4.failed

Use atomic_notifier_chain to fire firmware events at internal mlx5 core
components such as eswitch/fpga/clock/FW tracer/etc.., this is to
avoid explicit calls from low level mlx5_core to upper components and to
simplify the mlx5_core API for future developments.

Simply provide register/unregister notifiers API and call the notifier
chain on firmware async events.

Example: to subscribe to a FW event:
struct mlx5_nb port_event;

MLX5_NB_INIT(&port_event, port_event_handler, PORT_CHANGE);
mlx5_eq_notifier_register(mdev, &port_event);

where:
 - port_event_handler is the notifier block callback.
 - PORT_EVENT is the suffix of MLX5_EVENT_TYPE_PORT_CHANGE.

The above will guarantee that port_event_handler will receive all FW
events of the type MLX5_EVENT_TYPE_PORT_CHANGE.

To receive all FW/HW events one can subscribe to
MLX5_EVENT_TYPE_NOTIFY_ANY.

The next few patches will start moving all mlx5 core components to use
this new API and cleanup mlx5_eq_async_int misx handler from component
explicit calls and specific logic.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 0f597ed435b9ea1296e25474b762bedceba97a50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
#	include/linux/mlx5/device.h
#	include/linux/mlx5/eq.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index bb2de668b530,34e4b2c246ff..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -31,8 -31,10 +31,9 @@@
   */
  
  #include <linux/interrupt.h>
+ #include <linux/notifier.h>
  #include <linux/module.h>
  #include <linux/mlx5/driver.h>
 -#include <linux/mlx5/eq.h>
  #include <linux/mlx5/cmd.h>
  #ifdef CONFIG_RFS_ACCEL
  #include <linux/cpu_rmap.h>
@@@ -64,6 -60,28 +65,31 @@@ enum 
  	MLX5_EQ_DOORBEL_OFFSET	= 0x40,
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_irq_info {
+ 	cpumask_var_t mask;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	void *context; /* dev_id provided to request_irq */
+ };
+ 
+ struct mlx5_eq_table {
+ 	struct list_head        comp_eqs_list;
+ 	struct mlx5_eq          pages_eq;
+ 	struct mlx5_eq	        cmd_eq;
+ 	struct mlx5_eq          async_eq;
+ 
+ 	struct atomic_notifier_head nh[MLX5_EVENT_TYPE_MAX];
+ 
+ 	struct mutex            lock; /* sync async eqs creations */
+ 	int			num_comp_vectors;
+ 	struct mlx5_irq_info	*irq_info;
+ #ifdef CONFIG_RFS_ACCEL
+ 	struct cpu_rmap         *rmap;
+ #endif
+ };
+ 
++>>>>>>> 0f597ed435b9 (net/mlx5: EQ, Introduce atomic notifier chain subscription API)
  #define MLX5_ASYNC_EVENT_MASK ((1ull << MLX5_EVENT_TYPE_PATH_MIG)	    | \
  			       (1ull << MLX5_EVENT_TYPE_COMM_EST)	    | \
  			       (1ull << MLX5_EVENT_TYPE_SQ_DRAINED)	    | \
@@@ -436,24 -249,78 +462,25 @@@ static void mlx5_eq_cq_completion(struc
  	mlx5_cq_put(cq);
  }
  
 -static irqreturn_t mlx5_eq_comp_int(int irq, void *eq_ptr)
 +static void mlx5_eq_cq_event(struct mlx5_eq *eq, u32 cqn, int event_type)
  {
 -	struct mlx5_eq_comp *eq_comp = eq_ptr;
 -	struct mlx5_eq *eq = eq_ptr;
 -	struct mlx5_eqe *eqe;
 -	int set_ci = 0;
 -	u32 cqn = -1;
 -
 -	while ((eqe = next_eqe_sw(eq))) {
 -		struct mlx5_core_cq *cq;
 -		/* Make sure we read EQ entry contents after we've
 -		 * checked the ownership bit.
 -		 */
 -		dma_rmb();
 -		/* Assume (eqe->type) is always MLX5_EVENT_TYPE_COMP */
 -		cqn = be32_to_cpu(eqe->data.comp.cqn) & 0xffffff;
 -
 -		cq = mlx5_eq_cq_get(eq, cqn);
 -		if (likely(cq)) {
 -			++cq->arm_sn;
 -			cq->comp(cq);
 -			mlx5_cq_put(cq);
 -		} else {
 -			mlx5_core_warn(eq->dev, "Completion event for bogus CQ 0x%x\n", cqn);
 -		}
 -
 -		++eq->cons_index;
 -		++set_ci;
 +	struct mlx5_core_cq *cq = mlx5_eq_cq_get(eq, cqn);
  
 -		/* The HCA will think the queue has overflowed if we
 -		 * don't tell it we've been processing events.  We
 -		 * create our EQs with MLX5_NUM_SPARE_EQE extra
 -		 * entries, so we must update our consumer index at
 -		 * least that often.
 -		 */
 -		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
 -			eq_update_ci(eq, 0);
 -			set_ci = 0;
 -		}
 +	if (unlikely(!cq)) {
 +		mlx5_core_warn(eq->dev, "Async event for bogus CQ 0x%x\n", cqn);
 +		return;
  	}
  
 -	eq_update_ci(eq, 1);
 -
 -	if (cqn != -1)
 -		tasklet_schedule(&eq_comp->tasklet_ctx.task);
 -
 -	return IRQ_HANDLED;
 -}
 -
 -/* Some architectures don't latch interrupts when they are disabled, so using
 - * mlx5_eq_poll_irq_disabled could end up losing interrupts while trying to
 - * avoid losing them.  It is not recommended to use it, unless this is the last
 - * resort.
 - */
 -u32 mlx5_eq_poll_irq_disabled(struct mlx5_eq_comp *eq)
 -{
 -	u32 count_eqe;
 -
 -	disable_irq(eq->core.irqn);
 -	count_eqe = eq->core.cons_index;
 -	mlx5_eq_comp_int(eq->core.irqn, eq);
 -	count_eqe = eq->core.cons_index - count_eqe;
 -	enable_irq(eq->core.irqn);
 +	cq->event(cq, event_type);
  
 -	return count_eqe;
 +	mlx5_cq_put(cq);
  }
  
 -static irqreturn_t mlx5_eq_async_int(int irq, void *eq_ptr)
 +static irqreturn_t mlx5_eq_int(int irq, void *eq_ptr)
  {
  	struct mlx5_eq *eq = eq_ptr;
- 	struct mlx5_core_dev *dev = eq->dev;
+ 	struct mlx5_eq_table *eqt;
+ 	struct mlx5_core_dev *dev;
  	struct mlx5_eqe *eqe;
  	int set_ci = 0;
  	u32 cqn = -1;
@@@ -798,12 -636,30 +835,29 @@@ int mlx5_eq_del_cq(struct mlx5_eq *eq, 
  	return 0;
  }
  
 -int mlx5_eq_table_init(struct mlx5_core_dev *dev)
 +int mlx5_eq_init(struct mlx5_core_dev *dev)
  {
++<<<<<<< HEAD
 +	int err;
++=======
+ 	struct mlx5_eq_table *eq_table;
+ 	int i, err;
 -
 -	eq_table = kvzalloc(sizeof(*eq_table), GFP_KERNEL);
 -	if (!eq_table)
 -		return -ENOMEM;
 -
 -	dev->priv.eq_table = eq_table;
++>>>>>>> 0f597ed435b9 (net/mlx5: EQ, Introduce atomic notifier chain subscription API)
  
  	err = mlx5_eq_debugfs_init(dev);
 -	if (err)
 -		goto kvfree_eq_table;
  
++<<<<<<< HEAD
++=======
+ 	mutex_init(&eq_table->lock);
+ 	for (i = 0; i < MLX5_EVENT_TYPE_MAX; i++)
+ 		ATOMIC_INIT_NOTIFIER_HEAD(&eq_table->nh[i]);
+ 
+ 	return 0;
+ 
+ kvfree_eq_table:
+ 	kvfree(eq_table);
+ 	dev->priv.eq_table = NULL;
++>>>>>>> 0f597ed435b9 (net/mlx5: EQ, Introduce atomic notifier chain subscription API)
  	return err;
  }
  
@@@ -945,24 -1114,127 +999,128 @@@ int mlx5_core_eq_query(struct mlx5_core
  /* This function should only be called after mlx5_cmd_force_teardown_hca */
  void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
  {
 -	struct mlx5_eq_table *table = dev->priv.eq_table;
 -	int i, max_eqs;
 -
 -	clear_comp_irqs_affinity_hints(dev);
 +	struct mlx5_eq_table *table = &dev->priv.eq_table;
 +	struct mlx5_eq *eq;
  
  #ifdef CONFIG_RFS_ACCEL
 -	if (table->rmap) {
 -		free_irq_cpu_rmap(table->rmap);
 -		table->rmap = NULL;
 +	if (dev->rmap) {
 +		free_irq_cpu_rmap(dev->rmap);
 +		dev->rmap = NULL;
  	}
  #endif
 -
 -	mutex_lock(&table->lock); /* sync with create/destroy_async_eq */
 -	max_eqs = table->num_comp_vectors + MLX5_EQ_VEC_COMP_BASE;
 -	for (i = max_eqs - 1; i >= 0; i--) {
 -		if (!table->irq_info[i].context)
 -			continue;
 -		free_irq(pci_irq_vector(dev->pdev, i), table->irq_info[i].context);
 -		table->irq_info[i].context = NULL;
 -	}
 -	mutex_unlock(&table->lock);
 +	list_for_each_entry(eq, &table->comp_eqs_list, list)
 +		free_irq(eq->irqn, eq);
 +
 +	free_irq(table->pages_eq.irqn, &table->pages_eq);
 +	free_irq(table->async_eq.irqn, &table->async_eq);
 +	free_irq(table->cmd_eq.irqn, &table->cmd_eq);
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	if (MLX5_CAP_GEN(dev, pg))
 +		free_irq(table->pfault_eq.irqn, &table->pfault_eq);
 +#endif
  	pci_free_irq_vectors(dev->pdev);
  }
++<<<<<<< HEAD
++=======
+ 
+ static int alloc_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 	struct mlx5_eq_table *table = priv->eq_table;
+ 	int num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?
+ 		      MLX5_CAP_GEN(dev, max_num_eqs) :
+ 		      1 << MLX5_CAP_GEN(dev, log_max_eq);
+ 	int nvec;
+ 	int err;
+ 
+ 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
+ 	       MLX5_EQ_VEC_COMP_BASE;
+ 	nvec = min_t(int, nvec, num_eqs);
+ 	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
+ 		return -ENOMEM;
+ 
+ 	table->irq_info = kcalloc(nvec, sizeof(*table->irq_info), GFP_KERNEL);
+ 	if (!table->irq_info)
+ 		return -ENOMEM;
+ 
+ 	nvec = pci_alloc_irq_vectors(dev->pdev, MLX5_EQ_VEC_COMP_BASE + 1,
+ 				     nvec, PCI_IRQ_MSIX);
+ 	if (nvec < 0) {
+ 		err = nvec;
+ 		goto err_free_irq_info;
+ 	}
+ 
+ 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+ 
+ 	return 0;
+ 
+ err_free_irq_info:
+ 	kfree(table->irq_info);
+ 	return err;
+ }
+ 
+ static void free_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 
+ 	pci_free_irq_vectors(dev->pdev);
+ 	kfree(priv->eq_table->irq_info);
+ }
+ 
+ int mlx5_eq_table_create(struct mlx5_core_dev *dev)
+ {
+ 	int err;
+ 
+ 	err = alloc_irq_vectors(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "alloc irq vectors failed\n");
+ 		return err;
+ 	}
+ 
+ 	err = create_async_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create async EQs\n");
+ 		goto err_async_eqs;
+ 	}
+ 
+ 	err = create_comp_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create completion EQs\n");
+ 		goto err_comp_eqs;
+ 	}
+ 
+ 	return 0;
+ err_comp_eqs:
+ 	destroy_async_eqs(dev);
+ err_async_eqs:
+ 	free_irq_vectors(dev);
+ 	return err;
+ }
+ 
+ void mlx5_eq_table_destroy(struct mlx5_core_dev *dev)
+ {
+ 	destroy_comp_eqs(dev);
+ 	destroy_async_eqs(dev);
+ 	free_irq_vectors(dev);
+ }
+ 
+ int mlx5_eq_notifier_register(struct mlx5_core_dev *dev, struct mlx5_nb *nb)
+ {
+ 	struct mlx5_eq_table *eqt = dev->priv.eq_table;
+ 
+ 	if (nb->event_type >= MLX5_EVENT_TYPE_MAX)
+ 		return -EINVAL;
+ 
+ 	return atomic_notifier_chain_register(&eqt->nh[nb->event_type], &nb->nb);
+ }
+ 
+ int mlx5_eq_notifier_unregister(struct mlx5_core_dev *dev, struct mlx5_nb *nb)
+ {
+ 	struct mlx5_eq_table *eqt = dev->priv.eq_table;
+ 
+ 	if (nb->event_type >= MLX5_EVENT_TYPE_MAX)
+ 		return -EINVAL;
+ 
+ 	return atomic_notifier_chain_unregister(&eqt->nh[nb->event_type], &nb->nb);
+ }
++>>>>>>> 0f597ed435b9 (net/mlx5: EQ, Introduce atomic notifier chain subscription API)
diff --cc include/linux/mlx5/device.h
index 149434f03ae7,f7c8bebfe472..000000000000
--- a/include/linux/mlx5/device.h
+++ b/include/linux/mlx5/device.h
@@@ -339,6 -345,15 +345,18 @@@ enum mlx5_event 
  
  	MLX5_EVENT_TYPE_FPGA_ERROR         = 0x20,
  	MLX5_EVENT_TYPE_FPGA_QP_ERROR      = 0x21,
++<<<<<<< HEAD
++=======
+ 
+ 	MLX5_EVENT_TYPE_DEVICE_TRACER      = 0x26,
+ 
+ 	MLX5_EVENT_TYPE_MAX                = MLX5_EVENT_TYPE_DEVICE_TRACER + 1,
+ };
+ 
+ enum {
+ 	MLX5_TRACER_SUBTYPE_OWNERSHIP_CHANGE = 0x0,
+ 	MLX5_TRACER_SUBTYPE_TRACES_AVAILABLE = 0x1,
++>>>>>>> 0f597ed435b9 (net/mlx5: EQ, Introduce atomic notifier chain subscription API)
  };
  
  enum {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
* Unmerged path include/linux/mlx5/eq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 0594d0961cb3..0eab9213de0d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -78,6 +78,11 @@ do {									\
 		 __func__, __LINE__, current->pid,			\
 		##__VA_ARGS__)
 
+#define mlx5_core_warn_once(__dev, format, ...)				\
+	dev_warn_once(&(__dev)->pdev->dev, "%s:%d:(pid %d): " format,	\
+		      __func__, __LINE__, current->pid,			\
+		      ##__VA_ARGS__)
+
 #define mlx5_core_info(__dev, format, ...)				\
 	dev_info(&(__dev)->pdev->dev, format, ##__VA_ARGS__)
 
* Unmerged path include/linux/mlx5/device.h
* Unmerged path include/linux/mlx5/eq.h
