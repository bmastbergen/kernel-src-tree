IB/mlx5: Ranges in implicit ODP MR inherit its write access

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Moni Shoua <monis@mellanox.com>
commit da6a496a34f2fdcab14362cdc5068aac385e7b47
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/da6a496a.failed

A sub-range in ODP implicit MR should take its write permission from the
MR and not be set always to allow.

Fixes: d07d1d70ce1a ("IB/umem: Update on demand page (ODP) support")
	Signed-off-by: Moni Shoua <monis@mellanox.com>
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit da6a496a34f2fdcab14362cdc5068aac385e7b47)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_odp.c
#	drivers/infiniband/hw/mlx5/odp.c
#	include/rdma/ib_umem_odp.h
diff --cc drivers/infiniband/core/umem_odp.c
index 00222d836b64,eb8a5eb65bfa..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -250,9 -200,143 +250,149 @@@ static const struct mmu_notifier_ops ib
  	.invalidate_range_end       = ib_umem_notifier_invalidate_range_end,
  };
  
++<<<<<<< HEAD
 +struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
 +				      unsigned long addr, size_t size)
 +{
++=======
+ static void add_umem_to_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_insert(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static void remove_umem_from_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_remove(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 	complete_all(&umem_odp->notifier_completion);
+ 
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static struct ib_ucontext_per_mm *alloc_per_mm(struct ib_ucontext *ctx,
+ 					       struct mm_struct *mm)
+ {
+ 	struct ib_ucontext_per_mm *per_mm;
+ 	int ret;
+ 
+ 	per_mm = kzalloc(sizeof(*per_mm), GFP_KERNEL);
+ 	if (!per_mm)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	per_mm->context = ctx;
+ 	per_mm->mm = mm;
+ 	per_mm->umem_tree = RB_ROOT_CACHED;
+ 	init_rwsem(&per_mm->umem_rwsem);
+ 	per_mm->active = ctx->invalidate_range;
+ 
+ 	rcu_read_lock();
+ 	per_mm->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
+ 	rcu_read_unlock();
+ 
+ 	WARN_ON(mm != current->mm);
+ 
+ 	per_mm->mn.ops = &ib_umem_notifiers;
+ 	ret = mmu_notifier_register(&per_mm->mn, per_mm->mm);
+ 	if (ret) {
+ 		dev_err(&ctx->device->dev,
+ 			"Failed to register mmu_notifier %d\n", ret);
+ 		goto out_pid;
+ 	}
+ 
+ 	list_add(&per_mm->ucontext_list, &ctx->per_mm_list);
+ 	return per_mm;
+ 
+ out_pid:
+ 	put_pid(per_mm->tgid);
+ 	kfree(per_mm);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int get_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	struct ib_ucontext_per_mm *per_mm;
+ 
+ 	/*
+ 	 * Generally speaking we expect only one or two per_mm in this list,
+ 	 * so no reason to optimize this search today.
+ 	 */
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	list_for_each_entry(per_mm, &ctx->per_mm_list, ucontext_list) {
+ 		if (per_mm->mm == umem_odp->umem.owning_mm)
+ 			goto found;
+ 	}
+ 
+ 	per_mm = alloc_per_mm(ctx, umem_odp->umem.owning_mm);
+ 	if (IS_ERR(per_mm)) {
+ 		mutex_unlock(&ctx->per_mm_list_lock);
+ 		return PTR_ERR(per_mm);
+ 	}
+ 
+ found:
+ 	umem_odp->per_mm = per_mm;
+ 	per_mm->odp_mrs_count++;
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	return 0;
+ }
+ 
+ static void free_per_mm(struct rcu_head *rcu)
+ {
+ 	kfree(container_of(rcu, struct ib_ucontext_per_mm, rcu));
+ }
+ 
+ static void put_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	bool need_free;
+ 
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	umem_odp->per_mm = NULL;
+ 	per_mm->odp_mrs_count--;
+ 	need_free = per_mm->odp_mrs_count == 0;
+ 	if (need_free)
+ 		list_del(&per_mm->ucontext_list);
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	if (!need_free)
+ 		return;
+ 
+ 	/*
+ 	 * NOTE! mmu_notifier_unregister() can happen between a start/end
+ 	 * callback, resulting in an start/end, and thus an unbalanced
+ 	 * lock. This doesn't really matter to us since we are about to kfree
+ 	 * the memory that holds the lock, however LOCKDEP doesn't like this.
+ 	 */
+ 	down_write(&per_mm->umem_rwsem);
+ 	per_mm->active = false;
+ 	up_write(&per_mm->umem_rwsem);
+ 
+ 	WARN_ON(!RB_EMPTY_ROOT(&per_mm->umem_tree.rb_root));
+ 	mmu_notifier_unregister_no_release(&per_mm->mn, per_mm->mm);
+ 	put_pid(per_mm->tgid);
+ 	mmu_notifier_call_srcu(&per_mm->rcu, free_per_mm);
+ }
+ 
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_umem_odp *root,
+ 				      unsigned long addr, size_t size)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = root->per_mm;
+ 	struct ib_ucontext *ctx = per_mm->context;
++>>>>>>> da6a496a34f2 (IB/mlx5: Ranges in implicit ODP MR inherit its write access)
  	struct ib_umem_odp *odp_data;
  	struct ib_umem *umem;
  	int pages = size >> PAGE_SHIFT;
@@@ -266,7 -350,9 +406,13 @@@
  	umem->length     = size;
  	umem->address    = addr;
  	umem->page_shift = PAGE_SHIFT;
++<<<<<<< HEAD
 +	umem->writable   = 1;
++=======
+ 	umem->writable   = root->umem.writable;
+ 	umem->is_odp = 1;
+ 	odp_data->per_mm = per_mm;
++>>>>>>> da6a496a34f2 (IB/mlx5: Ranges in implicit ODP MR inherit its write access)
  
  	mutex_init(&odp_data->umem_mutex);
  	init_completion(&odp_data->notifier_completion);
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 4e6f586dbb7c,3e0d5885c026..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -386,9 -439,10 +386,14 @@@ next_mr
  		if (nentries)
  			nentries++;
  	} else {
++<<<<<<< HEAD
 +		odp = ib_alloc_odp_umem(ctx, addr, MLX5_IMR_MTT_SIZE);
++=======
+ 		odp = ib_alloc_odp_umem(odp_mr, addr,
+ 					MLX5_IMR_MTT_SIZE);
++>>>>>>> da6a496a34f2 (IB/mlx5: Ranges in implicit ODP MR inherit its write access)
  		if (IS_ERR(odp)) {
 -			mutex_unlock(&odp_mr->umem_mutex);
 +			mutex_unlock(&mr->umem->odp_data->umem_mutex);
  			return ERR_CAST(odp);
  		}
  
diff --cc include/rdma/ib_umem_odp.h
index 594840da09ca,d0024f53626e..000000000000
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@@ -107,6 -96,30 +107,33 @@@ void ib_umem_odp_release(struct ib_umem
  
  #define ODP_DMA_ADDR_MASK (~(ODP_READ_ALLOWED_BIT | ODP_WRITE_ALLOWED_BIT))
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 
+ struct ib_ucontext_per_mm {
+ 	struct ib_ucontext *context;
+ 	struct mm_struct *mm;
+ 	struct pid *tgid;
+ 	bool active;
+ 
+ 	struct rb_root_cached umem_tree;
+ 	/* Protects umem_tree */
+ 	struct rw_semaphore umem_rwsem;
+ 
+ 	struct mmu_notifier mn;
+ 	unsigned int odp_mrs_count;
+ 
+ 	struct list_head ucontext_list;
+ 	struct rcu_head rcu;
+ };
+ 
+ int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_umem_odp *root_umem,
+ 				      unsigned long addr, size_t size);
+ void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
+ 
++>>>>>>> da6a496a34f2 (IB/mlx5: Ranges in implicit ODP MR inherit its write access)
  int ib_umem_odp_map_dma_pages(struct ib_umem_odp *umem_odp, u64 start_offset,
  			      u64 bcnt, u64 access_mask,
  			      unsigned long current_seq);
* Unmerged path drivers/infiniband/core/umem_odp.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
* Unmerged path include/rdma/ib_umem_odp.h
