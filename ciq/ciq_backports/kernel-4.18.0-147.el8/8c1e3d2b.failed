arm64: Always enable spectre-v2 vulnerability detection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jeremy Linton <jeremy.linton@arm.com>
commit 8c1e3d2bb44cbb998cb28ff9a18f105fee7f1eb3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/8c1e3d2b.failed

Ensure we are always able to detect whether or not the CPU is affected
by Spectre-v2, so that we can later advertise this to userspace.

	Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
	Reviewed-by: Andre Przywara <andre.przywara@arm.com>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 8c1e3d2bb44cbb998cb28ff9a18f105fee7f1eb3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/cpu_errata.c
diff --cc arch/arm64/kernel/cpu_errata.c
index fde6f87a8b7e,a9c3ad4f7948..000000000000
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@@ -239,11 -269,11 +238,15 @@@ enable_smccc_arch_workaround_1(const st
  	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
  		cb = qcom_link_stack_sanitization;
  
++<<<<<<< HEAD
 +	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
++=======
+ 	if (IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR))
+ 		install_bp_hardening_cb(cb, smccc_start, smccc_end);
++>>>>>>> 8c1e3d2bb44c (arm64: Always enable spectre-v2 vulnerability detection)
  
 -	return 1;
 +	return;
  }
- #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
  
  #ifdef CONFIG_ARM64_SSBD
  DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
@@@ -455,57 -513,53 +458,98 @@@ static bool has_ssbd_mitigation(const s
  	CAP_MIDR_RANGE_LIST(midr_list)
  
  /*
 - * List of CPUs that do not need any Spectre-v2 mitigation at all.
 + * Generic helper for handling capabilties with multiple (match,enable) pairs
 + * of call backs, sharing the same capability bit.
 + * Iterate over each entry to see if at least one matches.
 + */
 +static bool __maybe_unused
 +multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry, int scope)
 +{
 +	const struct arm64_cpu_capabilities *caps;
 +
 +	for (caps = entry->match_list; caps->matches; caps++)
 +		if (caps->matches(caps, scope))
 +			return true;
 +
 +	return false;
 +}
 +
 +/*
 + * Take appropriate action for all matching entries in the shared capability
 + * entry.
 + */
 +static void __maybe_unused
 +multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
 +{
 +	const struct arm64_cpu_capabilities *caps;
 +
 +	for (caps = entry->match_list; caps->matches; caps++)
 +		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
 +		    caps->cpu_enable)
 +			caps->cpu_enable(caps);
 +}
 +
 +#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 +
 +/*
 + * List of CPUs where we need to issue a psci call to
 + * harden the branch predictor.
   */
 -static const struct midr_range spectre_v2_safe_list[] = {
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
 -	{ /* sentinel */ }
 +static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
 +	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
 +	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
 +	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 +	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 +	MIDR_ALL_VERSIONS(MIDR_NVIDIA_DENVER),
 +	{},
  };
  
++<<<<<<< HEAD
 +#endif
++=======
+ static bool __maybe_unused
+ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
+ {
+ 	int need_wa;
+ 
+ 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+ 
+ 	/* If the CPU has CSV2 set, we're safe */
+ 	if (cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64PFR0_EL1),
+ 						 ID_AA64PFR0_CSV2_SHIFT))
+ 		return false;
+ 
+ 	/* Alternatively, we have a list of unaffected CPUs */
+ 	if (is_midr_in_range_list(read_cpuid_id(), spectre_v2_safe_list))
+ 		return false;
+ 
+ 	/* Fallback to firmware detection */
+ 	need_wa = detect_harden_bp_fw();
+ 	if (!need_wa)
+ 		return false;
+ 
+ 	if (!IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR)) {
+ 		pr_warn_once("spectrev2 mitigation disabled by kernel configuration\n");
+ 		__hardenbp_enab = false;
+ 		return false;
+ 	}
+ 
+ 	/* forced off */
+ 	if (__nospectre_v2) {
+ 		pr_info_once("spectrev2 mitigation disabled by command line option\n");
+ 		return false;
+ 	}
+ 
+ 	if (need_wa < 0)
+ 		pr_warn_once("ARM_SMCCC_ARCH_WORKAROUND_1 missing from firmware\n");
+ 
+ 	return (need_wa > 0);
+ }
++>>>>>>> 8c1e3d2bb44c (arm64: Always enable spectre-v2 vulnerability detection)
  
  #ifdef CONFIG_HARDEN_EL2_VECTORS
  
@@@ -662,11 -737,8 +706,9 @@@ const struct arm64_cpu_capabilities arm
  	{
  		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
  		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 -		.matches = check_branch_predictor,
 +		.cpu_enable = enable_smccc_arch_workaround_1,
 +		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
  	},
- #endif
  #ifdef CONFIG_HARDEN_EL2_VECTORS
  	{
  		.desc = "EL2 vector hardening",
* Unmerged path arch/arm64/kernel/cpu_errata.c
