RDMA/ucontext: Fix regression with disassociate

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 67f269b37f9b4d52c5e7f97acea26c0852e9b8a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/67f269b3.failed

When this code was consolidated the intention was that the VMA would
become backed by anonymous zero pages after the zap_vma_pte - however this
very subtly relied on setting the vm_ops = NULL and clearing the VM_SHARED
bits to transform the VMA into an anonymous VMA. Since the vm_ops was
removed this broke.

Now userspace gets a SIGBUS if it touches the vma after disassociation.

Instead of converting the VMA to anonymous provide a fault handler that
puts a zero'd page into the VMA when user-space touches it after
disassociation.

	Cc: stable@vger.kernel.org
	Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
Fixes: 5f9794dc94f5 ("RDMA/ucontext: Add a core API for mmaping driver IO memory")
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 67f269b37f9b4d52c5e7f97acea26c0852e9b8a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs.h
#	drivers/infiniband/core/uverbs_main.c
diff --cc drivers/infiniband/core/uverbs.h
index 55f338e6340d,32cc8fe7902f..000000000000
--- a/drivers/infiniband/core/uverbs.h
+++ b/drivers/infiniband/core/uverbs.h
@@@ -144,6 -148,20 +144,23 @@@ struct ib_uverbs_file 
  	struct ib_uverbs_async_event_file       *async_file;
  	struct list_head			list;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * To access the uobjects list hw_destroy_rwsem must be held for write
+ 	 * OR hw_destroy_rwsem held for read AND uobjects_lock held.
+ 	 * hw_destroy_rwsem should be called across any destruction of the HW
+ 	 * object of an associated uobject.
+ 	 */
+ 	struct rw_semaphore	hw_destroy_rwsem;
+ 	spinlock_t		uobjects_lock;
+ 	struct list_head	uobjects;
+ 
+ 	struct mutex umap_lock;
+ 	struct list_head umaps;
+ 	struct page *disassociate_page;
+ 
++>>>>>>> 67f269b37f9b (RDMA/ucontext: Fix regression with disassociate)
  	struct idr		idr;
  	/* spinlock protects write access to idr */
  	spinlock_t		idr_lock;
diff --cc drivers/infiniband/core/uverbs_main.c
index 495189811c27,db20b6e0f253..000000000000
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@@ -256,7 -204,13 +256,17 @@@ void ib_uverbs_release_file(struct kre
  	if (atomic_dec_and_test(&file->device->refcount))
  		ib_uverbs_comp_dev(file->device);
  
++<<<<<<< HEAD
 +	kobject_put(&file->device->kobj);
++=======
+ 	if (file->async_file)
+ 		kref_put(&file->async_file->ref,
+ 			 ib_uverbs_release_async_event_file);
+ 	put_device(&file->device->dev);
+ 
+ 	if (file->disassociate_page)
+ 		__free_pages(file->disassociate_page, 0);
++>>>>>>> 67f269b37f9b (RDMA/ucontext: Fix regression with disassociate)
  	kfree(file);
  }
  
@@@ -809,11 -774,10 +819,12 @@@ static int ib_uverbs_mmap(struct file *
  	int ret = 0;
  	int srcu_key;
  
++<<<<<<< HEAD
  	srcu_key = srcu_read_lock(&file->device->disassociate_srcu);
 -	ucontext = ib_uverbs_get_ucontext_file(file);
 -	if (IS_ERR(ucontext)) {
 -		ret = PTR_ERR(ucontext);
 +	ib_dev = srcu_dereference(file->device->ib_dev,
 +				  &file->device->disassociate_srcu);
 +	if (!ib_dev) {
 +		ret = -EIO;
  		goto out;
  	}
  
@@@ -824,6 -785,277 +835,254 @@@
  out:
  	srcu_read_unlock(&file->device->disassociate_srcu, srcu_key);
  	return ret;
 -}
 -
 -/*
 - * Each time we map IO memory into user space this keeps track of the mapping.
 - * When the device is hot-unplugged we 'zap' the mmaps in user space to point
 - * to the zero page and allow the hot unplug to proceed.
 - *
 - * This is necessary for cases like PCI physical hot unplug as the actual BAR
 - * memory may vanish after this and access to it from userspace could MCE.
 - *
 - * RDMA drivers supporting disassociation must have their user space designed
 - * to cope in some way with their IO pages going to the zero page.
 - */
 -struct rdma_umap_priv {
 -	struct vm_area_struct *vma;
 -	struct list_head list;
 -};
 -
 -static const struct vm_operations_struct rdma_umap_ops;
 -
 -static void rdma_umap_priv_init(struct rdma_umap_priv *priv,
 -				struct vm_area_struct *vma)
 -{
 -	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
 -
++=======
+ 	priv->vma = vma;
+ 	vma->vm_private_data = priv;
+ 	vma->vm_ops = &rdma_umap_ops;
+ 
+ 	mutex_lock(&ufile->umap_lock);
+ 	list_add(&priv->list, &ufile->umaps);
+ 	mutex_unlock(&ufile->umap_lock);
+ }
+ 
+ /*
+  * The VMA has been dup'd, initialize the vm_private_data with a new tracking
+  * struct
+  */
+ static void rdma_umap_open(struct vm_area_struct *vma)
+ {
+ 	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
+ 	struct rdma_umap_priv *opriv = vma->vm_private_data;
+ 	struct rdma_umap_priv *priv;
+ 
+ 	if (!opriv)
+ 		return;
+ 
+ 	/* We are racing with disassociation */
+ 	if (!down_read_trylock(&ufile->hw_destroy_rwsem))
+ 		goto out_zap;
+ 	/*
+ 	 * Disassociation already completed, the VMA should already be zapped.
+ 	 */
+ 	if (!ufile->ucontext)
+ 		goto out_unlock;
+ 
+ 	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+ 	if (!priv)
+ 		goto out_unlock;
+ 	rdma_umap_priv_init(priv, vma);
+ 
+ 	up_read(&ufile->hw_destroy_rwsem);
+ 	return;
+ 
+ out_unlock:
+ 	up_read(&ufile->hw_destroy_rwsem);
+ out_zap:
+ 	/*
+ 	 * We can't allow the VMA to be created with the actual IO pages, that
+ 	 * would break our API contract, and it can't be stopped at this
+ 	 * point, so zap it.
+ 	 */
+ 	vma->vm_private_data = NULL;
+ 	zap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);
+ }
+ 
+ static void rdma_umap_close(struct vm_area_struct *vma)
+ {
+ 	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
+ 	struct rdma_umap_priv *priv = vma->vm_private_data;
+ 
+ 	if (!priv)
+ 		return;
+ 
+ 	/*
+ 	 * The vma holds a reference on the struct file that created it, which
+ 	 * in turn means that the ib_uverbs_file is guaranteed to exist at
+ 	 * this point.
+ 	 */
+ 	mutex_lock(&ufile->umap_lock);
+ 	list_del(&priv->list);
+ 	mutex_unlock(&ufile->umap_lock);
+ 	kfree(priv);
+ }
+ 
+ /*
+  * Once the zap_vma_ptes has been called touches to the VMA will come here and
+  * we return a dummy writable zero page for all the pfns.
+  */
+ static vm_fault_t rdma_umap_fault(struct vm_fault *vmf)
+ {
+ 	struct ib_uverbs_file *ufile = vmf->vma->vm_file->private_data;
+ 	struct rdma_umap_priv *priv = vmf->vma->vm_private_data;
+ 	vm_fault_t ret = 0;
+ 
+ 	if (!priv)
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	/* Read only pages can just use the system zero page. */
+ 	if (!(vmf->vma->vm_flags & (VM_WRITE | VM_MAYWRITE))) {
+ 		vmf->page = ZERO_PAGE(vmf->vm_start);
+ 		get_page(vmf->page);
+ 		return 0;
+ 	}
+ 
+ 	mutex_lock(&ufile->umap_lock);
+ 	if (!ufile->disassociate_page)
+ 		ufile->disassociate_page =
+ 			alloc_pages(vmf->gfp_mask | __GFP_ZERO, 0);
+ 
+ 	if (ufile->disassociate_page) {
+ 		/*
+ 		 * This VMA is forced to always be shared so this doesn't have
+ 		 * to worry about COW.
+ 		 */
+ 		vmf->page = ufile->disassociate_page;
+ 		get_page(vmf->page);
+ 	} else {
+ 		ret = VM_FAULT_SIGBUS;
+ 	}
+ 	mutex_unlock(&ufile->umap_lock);
+ 
+ 	return ret;
+ }
+ 
+ static const struct vm_operations_struct rdma_umap_ops = {
+ 	.open = rdma_umap_open,
+ 	.close = rdma_umap_close,
+ 	.fault = rdma_umap_fault,
+ };
+ 
+ static struct rdma_umap_priv *rdma_user_mmap_pre(struct ib_ucontext *ucontext,
+ 						 struct vm_area_struct *vma,
+ 						 unsigned long size)
+ {
+ 	struct ib_uverbs_file *ufile = ucontext->ufile;
+ 	struct rdma_umap_priv *priv;
+ 
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	if (vma->vm_end - vma->vm_start != size)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	/* Driver is using this wrong, must be called by ib_uverbs_mmap */
+ 	if (WARN_ON(!vma->vm_file ||
+ 		    vma->vm_file->private_data != ufile))
+ 		return ERR_PTR(-EINVAL);
+ 	lockdep_assert_held(&ufile->device->disassociate_srcu);
+ 
+ 	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+ 	if (!priv)
+ 		return ERR_PTR(-ENOMEM);
+ 	return priv;
+ }
+ 
+ /*
+  * Map IO memory into a process. This is to be called by drivers as part of
+  * their mmap() functions if they wish to send something like PCI-E BAR memory
+  * to userspace.
+  */
+ int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
+ 		      unsigned long pfn, unsigned long size, pgprot_t prot)
+ {
+ 	struct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);
+ 
+ 	if (IS_ERR(priv))
+ 		return PTR_ERR(priv);
+ 
+ 	vma->vm_page_prot = prot;
+ 	if (io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot)) {
+ 		kfree(priv);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	rdma_umap_priv_init(priv, vma);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(rdma_user_mmap_io);
+ 
+ /*
+  * The page case is here for a slightly different reason, the driver expects
+  * to be able to free the page it is sharing to user space when it destroys
+  * its ucontext, which means we need to zap the user space references.
+  *
+  * We could handle this differently by providing an API to allocate a shared
+  * page and then only freeing the shared page when the last ufile is
+  * destroyed.
+  */
+ int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+ 			struct vm_area_struct *vma, struct page *page,
+ 			unsigned long size)
+ {
+ 	struct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);
+ 
+ 	if (IS_ERR(priv))
+ 		return PTR_ERR(priv);
+ 
+ 	if (remap_pfn_range(vma, vma->vm_start, page_to_pfn(page), size,
+ 			    vma->vm_page_prot)) {
+ 		kfree(priv);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	rdma_umap_priv_init(priv, vma);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(rdma_user_mmap_page);
+ 
+ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
+ {
+ 	struct rdma_umap_priv *priv, *next_priv;
+ 
+ 	lockdep_assert_held(&ufile->hw_destroy_rwsem);
+ 
+ 	while (1) {
+ 		struct mm_struct *mm = NULL;
+ 
+ 		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
+ 		mutex_lock(&ufile->umap_lock);
+ 		while (!list_empty(&ufile->umaps)) {
+ 			int ret;
+ 
+ 			priv = list_first_entry(&ufile->umaps,
+ 						struct rdma_umap_priv, list);
+ 			mm = priv->vma->vm_mm;
+ 			ret = mmget_not_zero(mm);
+ 			if (!ret) {
+ 				list_del_init(&priv->list);
+ 				mm = NULL;
+ 				continue;
+ 			}
+ 			break;
+ 		}
+ 		mutex_unlock(&ufile->umap_lock);
+ 		if (!mm)
+ 			return;
+ 
+ 		/*
+ 		 * The umap_lock is nested under mmap_sem since it used within
+ 		 * the vma_ops callbacks, so we have to clean the list one mm
+ 		 * at a time to get the lock ordering right. Typically there
+ 		 * will only be one mm, so no big deal.
+ 		 */
+ 		down_read(&mm->mmap_sem);
+ 		mutex_lock(&ufile->umap_lock);
+ 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
+ 					  list) {
+ 			struct vm_area_struct *vma = priv->vma;
+ 
+ 			if (vma->vm_mm != mm)
+ 				continue;
+ 			list_del_init(&priv->list);
+ 
+ 			zap_vma_ptes(vma, vma->vm_start,
+ 				     vma->vm_end - vma->vm_start);
+ 		}
+ 		mutex_unlock(&ufile->umap_lock);
+ 		up_read(&mm->mmap_sem);
+ 		mmput(mm);
+ 	}
++>>>>>>> 67f269b37f9b (RDMA/ucontext: Fix regression with disassociate)
  }
  
  /*
* Unmerged path drivers/infiniband/core/uverbs.h
* Unmerged path drivers/infiniband/core/uverbs_main.c
