bpf: Support sk lookup in netns with id 0

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Joe Stringer <joe@wand.net.nz>
commit f71c6143c2038df1cb43a4b9c90740d14f77467c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f71c6143.failed

David Ahern and Nicolas Dichtel report that the handling of the netns id
0 is incorrect for the BPF socket lookup helpers: rather than finding
the netns with id 0, it is resolving to the current netns. This renders
the netns_id 0 inaccessible.

To fix this, adjust the API for the netns to treat all negative s32
values as a lookup in the current netns (including u64 values which when
truncated to s32 become negative), while any values with a positive
value in the signed 32-bit integer space would result in a lookup for a
socket in the netns corresponding to that id. As before, if the netns
with that ID does not exist, no socket will be found. Any netns outside
of these ranges will fail to find a corresponding socket, as those
values are reserved for future usage.

	Signed-off-by: Joe Stringer <joe@wand.net.nz>
	Acked-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
	Acked-by: Joey Pabalinas <joeypabalinas@gmail.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit f71c6143c2038df1cb43a4b9c90740d14f77467c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	net/core/filter.c
#	tools/include/uapi/linux/bpf.h
#	tools/testing/selftests/bpf/bpf_helpers.h
diff --cc include/uapi/linux/bpf.h
index 2293d9a9b442,cba518c57229..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -2121,6 -2161,106 +2121,109 @@@ union bpf_attr 
   *		the shared data.
   *	Return
   *		Pointer to the local storage area.
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_sk_select_reuseport(struct sk_reuseport_md *reuse, struct bpf_map *map, void *key, u64 flags)
+  *	Description
+  *		Select a SO_REUSEPORT sk from a	BPF_MAP_TYPE_REUSEPORT_ARRAY map
+  *		It checks the selected sk is matching the incoming
+  *		request in the skb.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * struct bpf_sock *bpf_sk_lookup_tcp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u64 netns, u64 flags)
+  *	Description
+  *		Look for TCP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is a negative signed 32-bit integer, then the
+  *		socket lookup table in the netns associated with the *ctx* will
+  *		will be used. For the TC hooks, this is the netns of the device
+  *		in the skb. For socket hooks, this is the netns of the socket.
+  *		If *netns* is any other signed 32-bit value greater than or
+  *		equal to zero then it specifies the ID of the netns relative to
+  *		the netns associated with the *ctx*. *netns* values beyond the
+  *		range of 32-bit integers are reserved for future use.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *
+  * struct bpf_sock *bpf_sk_lookup_udp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u64 netns, u64 flags)
+  *	Description
+  *		Look for UDP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is a negative signed 32-bit integer, then the
+  *		socket lookup table in the netns associated with the *ctx* will
+  *		will be used. For the TC hooks, this is the netns of the device
+  *		in the skb. For socket hooks, this is the netns of the socket.
+  *		If *netns* is any other signed 32-bit value greater than or
+  *		equal to zero then it specifies the ID of the netns relative to
+  *		the netns associated with the *ctx*. *netns* values beyond the
+  *		range of 32-bit integers are reserved for future use.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *
+  * int bpf_sk_release(struct bpf_sock *sk)
+  *	Description
+  *		Release the reference held by *sock*. *sock* must be a non-NULL
+  *		pointer that was returned from bpf_sk_lookup_xxx\ ().
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_msg_push_data(struct sk_buff *skb, u32 start, u32 len, u64 flags)
+  *	Description
+  *		For socket policies, insert *len* bytes into msg at offset
+  *		*start*.
+  *
+  *		If a program of type **BPF_PROG_TYPE_SK_MSG** is run on a
+  *		*msg* it may want to insert metadata or options into the msg.
+  *		This can later be read and used by any of the lower layer BPF
+  *		hooks.
+  *
+  *		This helper may fail if under memory pressure (a malloc
+  *		fails) in these cases BPF programs will get an appropriate
+  *		error and BPF programs will need to handle them.
+  *
+  *	Return
+  *		0 on success, or a negative error in case of failure.
++>>>>>>> f71c6143c203 (bpf: Support sk lookup in netns with id 0)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --cc net/core/filter.c
index 5c01834ee5a7,8d2c629501e2..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -4756,6 -4823,149 +4756,152 @@@ static const struct bpf_func_proto bpf_
  };
  #endif /* CONFIG_IPV6_SEG6_BPF */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INET
+ static struct sock *sk_lookup(struct net *net, struct bpf_sock_tuple *tuple,
+ 			      struct sk_buff *skb, u8 family, u8 proto)
+ {
+ 	bool refcounted = false;
+ 	struct sock *sk = NULL;
+ 	int dif = 0;
+ 
+ 	if (skb->dev)
+ 		dif = skb->dev->ifindex;
+ 
+ 	if (family == AF_INET) {
+ 		__be32 src4 = tuple->ipv4.saddr;
+ 		__be32 dst4 = tuple->ipv4.daddr;
+ 		int sdif = inet_sdif(skb);
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet_lookup(net, &tcp_hashinfo, skb, 0,
+ 					   src4, tuple->ipv4.sport,
+ 					   dst4, tuple->ipv4.dport,
+ 					   dif, sdif, &refcounted);
+ 		else
+ 			sk = __udp4_lib_lookup(net, src4, tuple->ipv4.sport,
+ 					       dst4, tuple->ipv4.dport,
+ 					       dif, sdif, &udp_table, skb);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	} else {
+ 		struct in6_addr *src6 = (struct in6_addr *)&tuple->ipv6.saddr;
+ 		struct in6_addr *dst6 = (struct in6_addr *)&tuple->ipv6.daddr;
+ 		int sdif = inet6_sdif(skb);
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet6_lookup(net, &tcp_hashinfo, skb, 0,
+ 					    src6, tuple->ipv6.sport,
+ 					    dst6, ntohs(tuple->ipv6.dport),
+ 					    dif, sdif, &refcounted);
+ 		else if (likely(ipv6_bpf_stub))
+ 			sk = ipv6_bpf_stub->udp6_lib_lookup(net,
+ 							    src6, tuple->ipv6.sport,
+ 							    dst6, tuple->ipv6.dport,
+ 							    dif, sdif,
+ 							    &udp_table, skb);
+ #endif
+ 	}
+ 
+ 	if (unlikely(sk && !refcounted && !sock_flag(sk, SOCK_RCU_FREE))) {
+ 		WARN_ONCE(1, "Found non-RCU, unreferenced socket!");
+ 		sk = NULL;
+ 	}
+ 	return sk;
+ }
+ 
+ /* bpf_sk_lookup performs the core lookup for different types of sockets,
+  * taking a reference on the socket if it doesn't have the flag SOCK_RCU_FREE.
+  * Returns the socket as an 'unsigned long' to simplify the casting in the
+  * callers to satisfy BPF_CALL declarations.
+  */
+ static unsigned long
+ bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 	      u8 proto, u64 netns_id, u64 flags)
+ {
+ 	struct net *caller_net;
+ 	struct sock *sk = NULL;
+ 	u8 family = AF_UNSPEC;
+ 	struct net *net;
+ 
+ 	family = len == sizeof(tuple->ipv4) ? AF_INET : AF_INET6;
+ 	if (unlikely(family == AF_UNSPEC || flags ||
+ 		     !((s32)netns_id < 0 || netns_id <= S32_MAX)))
+ 		goto out;
+ 
+ 	if (skb->dev)
+ 		caller_net = dev_net(skb->dev);
+ 	else
+ 		caller_net = sock_net(skb->sk);
+ 	if ((s32)netns_id < 0) {
+ 		net = caller_net;
+ 		sk = sk_lookup(net, tuple, skb, family, proto);
+ 	} else {
+ 		net = get_net_ns_by_id(caller_net, netns_id);
+ 		if (unlikely(!net))
+ 			goto out;
+ 		sk = sk_lookup(net, tuple, skb, family, proto);
+ 		put_net(net);
+ 	}
+ 
+ 	if (sk)
+ 		sk = sk_to_full_sk(sk);
+ out:
+ 	return (unsigned long) sk;
+ }
+ 
+ BPF_CALL_5(bpf_sk_lookup_tcp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sk_lookup_udp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_udp_proto = {
+ 	.func		= bpf_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_1(bpf_sk_release, struct sock *, sk)
+ {
+ 	if (!sock_flag(sk, SOCK_RCU_FREE))
+ 		sock_gen_put(sk);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_release_proto = {
+ 	.func		= bpf_sk_release,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_SOCKET,
+ };
+ #endif /* CONFIG_INET */
+ 
++>>>>>>> f71c6143c203 (bpf: Support sk lookup in netns with id 0)
  bool bpf_helper_changes_pkt_data(void *func)
  {
  	if (func == bpf_skb_vlan_push ||
diff --cc tools/include/uapi/linux/bpf.h
index dd0da04bd3ab,76b265c7d93e..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -2101,6 -2161,110 +2101,113 @@@ union bpf_attr 
   *		the shared data.
   *	Return
   *		Pointer to the local storage area.
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_sk_select_reuseport(struct sk_reuseport_md *reuse, struct bpf_map *map, void *key, u64 flags)
+  *	Description
+  *		Select a SO_REUSEPORT sk from a	BPF_MAP_TYPE_REUSEPORT_ARRAY map
+  *		It checks the selected sk is matching the incoming
+  *		request in the skb.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * struct bpf_sock *bpf_sk_lookup_tcp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u64 netns, u64 flags)
+  *	Description
+  *		Look for TCP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is a negative signed 32-bit integer, then the
+  *		socket lookup table in the netns associated with the *ctx* will
+  *		will be used. For the TC hooks, this is the netns of the device
+  *		in the skb. For socket hooks, this is the netns of the socket.
+  *		If *netns* is any other signed 32-bit value greater than or
+  *		equal to zero then it specifies the ID of the netns relative to
+  *		the netns associated with the *ctx*. *netns* values beyond the
+  *		range of 32-bit integers are reserved for future use.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * struct bpf_sock *bpf_sk_lookup_udp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u64 netns, u64 flags)
+  *	Description
+  *		Look for UDP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is a negative signed 32-bit integer, then the
+  *		socket lookup table in the netns associated with the *ctx* will
+  *		will be used. For the TC hooks, this is the netns of the device
+  *		in the skb. For socket hooks, this is the netns of the socket.
+  *		If *netns* is any other signed 32-bit value greater than or
+  *		equal to zero then it specifies the ID of the netns relative to
+  *		the netns associated with the *ctx*. *netns* values beyond the
+  *		range of 32-bit integers are reserved for future use.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * int bpf_sk_release(struct bpf_sock *sk)
+  *	Description
+  *		Release the reference held by *sock*. *sock* must be a non-NULL
+  *		pointer that was returned from bpf_sk_lookup_xxx\ ().
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_msg_push_data(struct sk_buff *skb, u32 start, u32 len, u64 flags)
+  *	Description
+  *		For socket policies, insert *len* bytes into msg at offset
+  *		*start*.
+  *
+  *		If a program of type **BPF_PROG_TYPE_SK_MSG** is run on a
+  *		*msg* it may want to insert metadata or options into the msg.
+  *		This can later be read and used by any of the lower layer BPF
+  *		hooks.
+  *
+  *		This helper may fail if under memory pressure (a malloc
+  *		fails) in these cases BPF programs will get an appropriate
+  *		error and BPF programs will need to handle them.
+  *
+  *	Return
+  *		0 on success, or a negative error in case of failure.
++>>>>>>> f71c6143c203 (bpf: Support sk lookup in netns with id 0)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --cc tools/testing/selftests/bpf/bpf_helpers.h
index e4be7730222d,efb6c13ab0de..000000000000
--- a/tools/testing/selftests/bpf/bpf_helpers.h
+++ b/tools/testing/selftests/bpf/bpf_helpers.h
@@@ -143,6 -152,22 +143,25 @@@ static unsigned long long (*bpf_skb_cgr
  	(void *) BPF_FUNC_skb_cgroup_id;
  static unsigned long long (*bpf_skb_ancestor_cgroup_id)(void *ctx, int level) =
  	(void *) BPF_FUNC_skb_ancestor_cgroup_id;
++<<<<<<< HEAD
++=======
+ static struct bpf_sock *(*bpf_sk_lookup_tcp)(void *ctx,
+ 					     struct bpf_sock_tuple *tuple,
+ 					     int size, unsigned long long netns_id,
+ 					     unsigned long long flags) =
+ 	(void *) BPF_FUNC_sk_lookup_tcp;
+ static struct bpf_sock *(*bpf_sk_lookup_udp)(void *ctx,
+ 					     struct bpf_sock_tuple *tuple,
+ 					     int size, unsigned long long netns_id,
+ 					     unsigned long long flags) =
+ 	(void *) BPF_FUNC_sk_lookup_udp;
+ static int (*bpf_sk_release)(struct bpf_sock *sk) =
+ 	(void *) BPF_FUNC_sk_release;
+ static int (*bpf_skb_vlan_push)(void *ctx, __be16 vlan_proto, __u16 vlan_tci) =
+ 	(void *) BPF_FUNC_skb_vlan_push;
+ static int (*bpf_skb_vlan_pop)(void *ctx) =
+ 	(void *) BPF_FUNC_skb_vlan_pop;
++>>>>>>> f71c6143c203 (bpf: Support sk lookup in netns with id 0)
  
  /* llvm builtin functions that eBPF C program may use to
   * emit BPF_LD_ABS and BPF_LD_IND instructions
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path net/core/filter.c
* Unmerged path tools/include/uapi/linux/bpf.h
* Unmerged path tools/testing/selftests/bpf/bpf_helpers.h
diff --git a/tools/testing/selftests/bpf/test_sk_lookup_kern.c b/tools/testing/selftests/bpf/test_sk_lookup_kern.c
index b745bdc08c2b..e21cd736c196 100644
--- a/tools/testing/selftests/bpf/test_sk_lookup_kern.c
+++ b/tools/testing/selftests/bpf/test_sk_lookup_kern.c
@@ -72,7 +72,7 @@ int bpf_sk_lookup_test0(struct __sk_buff *skb)
 		return TC_ACT_SHOT;
 
 	tuple_len = ipv4 ? sizeof(tuple->ipv4) : sizeof(tuple->ipv6);
-	sk = bpf_sk_lookup_tcp(skb, tuple, tuple_len, 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, tuple, tuple_len, BPF_F_CURRENT_NETNS, 0);
 	if (sk)
 		bpf_sk_release(sk);
 	return sk ? TC_ACT_OK : TC_ACT_UNSPEC;
@@ -84,7 +84,7 @@ int bpf_sk_lookup_test1(struct __sk_buff *skb)
 	struct bpf_sock_tuple tuple = {};
 	struct bpf_sock *sk;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	if (sk)
 		bpf_sk_release(sk);
 	return 0;
@@ -97,7 +97,7 @@ int bpf_sk_lookup_uaf(struct __sk_buff *skb)
 	struct bpf_sock *sk;
 	__u32 family = 0;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	if (sk) {
 		bpf_sk_release(sk);
 		family = sk->family;
@@ -112,7 +112,7 @@ int bpf_sk_lookup_modptr(struct __sk_buff *skb)
 	struct bpf_sock *sk;
 	__u32 family;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	if (sk) {
 		sk += 1;
 		bpf_sk_release(sk);
@@ -127,7 +127,7 @@ int bpf_sk_lookup_modptr_or_null(struct __sk_buff *skb)
 	struct bpf_sock *sk;
 	__u32 family;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	sk += 1;
 	if (sk)
 		bpf_sk_release(sk);
@@ -139,7 +139,7 @@ int bpf_sk_lookup_test2(struct __sk_buff *skb)
 {
 	struct bpf_sock_tuple tuple = {};
 
-	bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	return 0;
 }
 
@@ -149,7 +149,7 @@ int bpf_sk_lookup_test3(struct __sk_buff *skb)
 	struct bpf_sock_tuple tuple = {};
 	struct bpf_sock *sk;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	bpf_sk_release(sk);
 	bpf_sk_release(sk);
 	return 0;
@@ -161,7 +161,7 @@ int bpf_sk_lookup_test4(struct __sk_buff *skb)
 	struct bpf_sock_tuple tuple = {};
 	struct bpf_sock *sk;
 
-	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	sk = bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 	bpf_sk_release(sk);
 	return 0;
 }
@@ -169,7 +169,7 @@ int bpf_sk_lookup_test4(struct __sk_buff *skb)
 void lookup_no_release(struct __sk_buff *skb)
 {
 	struct bpf_sock_tuple tuple = {};
-	bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), 0, 0);
+	bpf_sk_lookup_tcp(skb, &tuple, sizeof(tuple), BPF_F_CURRENT_NETNS, 0);
 }
 
 SEC("fail_no_release_subcall")
