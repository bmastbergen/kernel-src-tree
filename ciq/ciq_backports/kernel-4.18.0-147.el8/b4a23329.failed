net/mlx5e: Re-attempt to offload flows on multipath port affinity events

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Roi Dayan <roid@mellanox.com>
commit b4a23329e2e940cdec3b5eae781c1f7d4c669533
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/b4a23329.failed

Under multipath it's possible for us to offload the flow only through
the e-switch for which proper route through the uplink exists.
When the port is up and the next-hop route is set again we want to
offload through it as well.

We generate SW event from the FIB event handler when multipath port
affinity changes. The tc offloads code gets this event, goes over the
flows which were marked as of having missing route and attempts to
offload them.

	Signed-off-by: Roi Dayan <roid@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit b4a23329e2e940cdec3b5eae781c1f7d4c669533)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index 48489728eca3,a1a3e2774989..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -1264,10 -1569,136 +1264,136 @@@ static int mlx5e_init_rep_tx(struct mlx
  		mlx5_core_warn(priv->mdev, "create tises failed, %d\n", err);
  		return err;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	if (rpriv->rep->vport == MLX5_VPORT_UPLINK) {
+ 		uplink_priv = &rpriv->uplink_priv;
+ 
+ 		INIT_LIST_HEAD(&uplink_priv->unready_flows);
+ 
+ 		/* init shared tc flow table */
+ 		err = mlx5e_tc_esw_init(&uplink_priv->tc_ht);
+ 		if (err)
+ 			goto destroy_tises;
+ 
+ 		mlx5_init_port_tun_entropy(&uplink_priv->tun_entropy, priv->mdev);
+ 
+ 		/* init indirect block notifications */
+ 		INIT_LIST_HEAD(&uplink_priv->tc_indr_block_priv_list);
+ 		uplink_priv->netdevice_nb.notifier_call = mlx5e_nic_rep_netdevice_event;
+ 		err = register_netdevice_notifier(&uplink_priv->netdevice_nb);
+ 		if (err) {
+ 			mlx5_core_err(priv->mdev, "Failed to register netdev notifier\n");
+ 			goto tc_esw_cleanup;
+ 		}
+ 	}
+ 
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  	return 0;
 -
 -tc_esw_cleanup:
 -	mlx5e_tc_esw_cleanup(&uplink_priv->tc_ht);
 -destroy_tises:
 -	for (tc = 0; tc < priv->profile->max_tc; tc++)
 -		mlx5e_destroy_tis(priv->mdev, priv->tisn[tc]);
 -	return err;
  }
  
++<<<<<<< HEAD
 +static const struct mlx5e_profile mlx5e_rep_profile = {
++=======
+ static void mlx5e_cleanup_rep_tx(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	int tc;
+ 
+ 	for (tc = 0; tc < priv->profile->max_tc; tc++)
+ 		mlx5e_destroy_tis(priv->mdev, priv->tisn[tc]);
+ 
+ 	if (rpriv->rep->vport == MLX5_VPORT_UPLINK) {
+ 		/* clean indirect TC block notifications */
+ 		unregister_netdevice_notifier(&rpriv->uplink_priv.netdevice_nb);
+ 		mlx5e_rep_indr_clean_block_privs(rpriv);
+ 
+ 		/* delete shared tc flow table */
+ 		mlx5e_tc_esw_cleanup(&rpriv->uplink_priv.tc_ht);
+ 	}
+ }
+ 
+ static void mlx5e_vf_rep_enable(struct mlx5e_priv *priv)
+ {
+ 	struct net_device *netdev = priv->netdev;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	u16 max_mtu;
+ 
+ 	netdev->min_mtu = ETH_MIN_MTU;
+ 	mlx5_query_port_max_mtu(mdev, &max_mtu, 1);
+ 	netdev->max_mtu = MLX5E_HW2SW_MTU(&priv->channels.params, max_mtu);
+ }
+ 
+ static int uplink_rep_async_event(struct notifier_block *nb, unsigned long event, void *data)
+ {
+ 	struct mlx5e_priv *priv = container_of(nb, struct mlx5e_priv, events_nb);
+ 
+ 	if (event == MLX5_EVENT_TYPE_PORT_CHANGE) {
+ 		struct mlx5_eqe *eqe = data;
+ 
+ 		switch (eqe->sub_type) {
+ 		case MLX5_PORT_CHANGE_SUBTYPE_DOWN:
+ 		case MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:
+ 			queue_work(priv->wq, &priv->update_carrier_work);
+ 			break;
+ 		default:
+ 			return NOTIFY_DONE;
+ 		}
+ 
+ 		return NOTIFY_OK;
+ 	}
+ 
+ 	if (event == MLX5_DEV_EVENT_PORT_AFFINITY) {
+ 		struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 
+ 		queue_work(priv->wq, &rpriv->uplink_priv.reoffload_flows_work);
+ 
+ 		return NOTIFY_OK;
+ 	}
+ 
+ 	return NOTIFY_DONE;
+ }
+ 
+ static void mlx5e_uplink_rep_enable(struct mlx5e_priv *priv)
+ {
+ 	struct net_device *netdev = priv->netdev;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	u16 max_mtu;
+ 
+ 	netdev->min_mtu = ETH_MIN_MTU;
+ 	mlx5_query_port_max_mtu(priv->mdev, &max_mtu, 1);
+ 	netdev->max_mtu = MLX5E_HW2SW_MTU(&priv->channels.params, max_mtu);
+ 	mlx5e_set_dev_port_mtu(priv);
+ 
+ 	INIT_WORK(&rpriv->uplink_priv.reoffload_flows_work,
+ 		  mlx5e_tc_reoffload_flows_work);
+ 
+ 	mlx5_lag_add(mdev, netdev);
+ 	priv->events_nb.notifier_call = uplink_rep_async_event;
+ 	mlx5_notifier_register(mdev, &priv->events_nb);
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ 	mlx5e_dcbnl_initialize(priv);
+ 	mlx5e_dcbnl_init_app(priv);
+ #endif
+ }
+ 
+ static void mlx5e_uplink_rep_disable(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ 	mlx5e_dcbnl_delete_app(priv);
+ #endif
+ 	mlx5_notifier_unregister(mdev, &priv->events_nb);
+ 	cancel_work_sync(&rpriv->uplink_priv.reoffload_flows_work);
+ 	mlx5_lag_remove(mdev);
+ }
+ 
+ static const struct mlx5e_profile mlx5e_vf_rep_profile = {
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  	.init			= mlx5e_init_rep,
  	.cleanup		= mlx5e_cleanup_rep,
  	.init_rx		= mlx5e_init_rep_rx,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
index c078c6703dc7,83b573b1abac..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
@@@ -71,6 -72,11 +71,14 @@@ struct mlx5_rep_uplink_priv 
  	 */
  	struct list_head	    tc_indr_block_priv_list;
  	struct notifier_block	    netdevice_nb;
++<<<<<<< HEAD
++=======
+ 
+ 	struct mlx5_tun_entropy tun_entropy;
+ 
+ 	struct list_head            unready_flows;
+ 	struct work_struct          reoffload_flows_work;
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  };
  
  struct mlx5e_rep_priv {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 06543506a2d4,2f0c631847fd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -82,11 -106,18 +82,16 @@@ struct mlx5e_tc_flow 
  	struct rhash_head	node;
  	struct mlx5e_priv	*priv;
  	u64			cookie;
 -	u16			flags;
 +	u8			flags;
  	struct mlx5_flow_handle *rule[MLX5E_TC_MAX_SPLITS + 1];
 -	/* Flow can be associated with multiple encap IDs.
 -	 * The number of encaps is bounded by the number of supported
 -	 * destinations.
 -	 */
 -	struct encap_flow_item encaps[MLX5_MAX_FLOW_FWD_VPORTS];
 -	struct mlx5e_tc_flow    *peer_flow;
 +	struct list_head	encap;   /* flows sharing the same encap ID */
  	struct list_head	mod_hdr; /* flows sharing the same mod hdr ID */
  	struct list_head	hairpin; /* flows sharing the same hairpin */
++<<<<<<< HEAD
++=======
+ 	struct list_head	peer;    /* flows with peer flow */
+ 	struct list_head	unready; /* flows not ready to be offloaded (e.g due to missing route) */
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  	union {
  		struct mlx5_esw_flow_attr esw_attr[0];
  		struct mlx5_nic_flow_attr nic_attr[0];
@@@ -812,23 -849,117 +817,116 @@@ static void mlx5e_tc_del_nic_flow(struc
  }
  
  static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 -			       struct mlx5e_tc_flow *flow, int out_index);
 +			       struct mlx5e_tc_flow *flow);
  
  static int mlx5e_attach_encap(struct mlx5e_priv *priv,
 -			      struct mlx5e_tc_flow *flow,
 +			      struct ip_tunnel_info *tun_info,
  			      struct net_device *mirred_dev,
 -			      int out_index,
 -			      struct netlink_ext_ack *extack,
  			      struct net_device **encap_dev,
 -			      bool *encap_valid);
 +			      struct mlx5e_tc_flow *flow);
  
  static struct mlx5_flow_handle *
++<<<<<<< HEAD
++=======
+ mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
+ 			   struct mlx5e_tc_flow *flow,
+ 			   struct mlx5_flow_spec *spec,
+ 			   struct mlx5_esw_flow_attr *attr)
+ {
+ 	struct mlx5_flow_handle *rule;
+ 
+ 	rule = mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
+ 	if (IS_ERR(rule))
+ 		return rule;
+ 
+ 	if (attr->split_count) {
+ 		flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, spec, attr);
+ 		if (IS_ERR(flow->rule[1])) {
+ 			mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
+ 			return flow->rule[1];
+ 		}
+ 	}
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
+ 	return rule;
+ }
+ 
+ static void
+ mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
+ 			     struct mlx5e_tc_flow *flow,
+ 			   struct mlx5_esw_flow_attr *attr)
+ {
+ 	flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
+ 
+ 	if (attr->split_count)
+ 		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
+ 
+ 	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
+ 			      struct mlx5e_tc_flow *flow,
+ 			      struct mlx5_flow_spec *spec,
+ 			      struct mlx5_esw_flow_attr *slow_attr)
+ {
+ 	struct mlx5_flow_handle *rule;
+ 
+ 	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
+ 	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	slow_attr->split_count = 0;
+ 	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN;
+ 
+ 	rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, slow_attr);
+ 	if (!IS_ERR(rule))
+ 		flow->flags |= MLX5E_TC_FLOW_SLOW;
+ 
+ 	return rule;
+ }
+ 
+ static void
+ mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
+ 				  struct mlx5e_tc_flow *flow,
+ 				  struct mlx5_esw_flow_attr *slow_attr)
+ {
+ 	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
+ 	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	slow_attr->split_count = 0;
+ 	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN;
+ 	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
+ 	flow->flags &= ~MLX5E_TC_FLOW_SLOW;
+ }
+ 
+ static void add_unready_flow(struct mlx5e_tc_flow *flow)
+ {
+ 	struct mlx5_rep_uplink_priv *uplink_priv;
+ 	struct mlx5e_rep_priv *rpriv;
+ 	struct mlx5_eswitch *esw;
+ 
+ 	esw = flow->priv->mdev->priv.eswitch;
+ 	rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+ 	uplink_priv = &rpriv->uplink_priv;
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_NOT_READY;
+ 	list_add_tail(&flow->unready, &uplink_priv->unready_flows);
+ }
+ 
+ static void remove_unready_flow(struct mlx5e_tc_flow *flow)
+ {
+ 	list_del(&flow->unready);
+ 	flow->flags &= ~MLX5E_TC_FLOW_NOT_READY;
+ }
+ 
+ static int
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	u32 max_chain = mlx5_eswitch_get_chain_range(esw);
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr = attr->parse_attr;
 -	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
  	struct net_device *out_dev, *encap_dev = NULL;
 +	struct mlx5_flow_handle *rule = NULL;
  	struct mlx5_fc *counter = NULL;
  	struct mlx5e_rep_priv *rpriv;
  	struct mlx5e_priv *out_priv;
@@@ -916,12 -1066,20 +1014,23 @@@ static void mlx5e_tc_del_fdb_flow(struc
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
++<<<<<<< HEAD
++=======
+ 	struct mlx5_esw_flow_attr slow_attr;
+ 	int out_index;
+ 
+ 	if (flow->flags & MLX5E_TC_FLOW_NOT_READY) {
+ 		remove_unready_flow(flow);
+ 		kvfree(attr->parse_attr);
+ 		return;
+ 	}
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  
  	if (flow->flags & MLX5E_TC_FLOW_OFFLOADED) {
 -		if (flow->flags & MLX5E_TC_FLOW_SLOW)
 -			mlx5e_tc_unoffload_from_slow_path(esw, flow, &slow_attr);
 -		else
 -			mlx5e_tc_unoffload_fdb_rules(esw, flow, attr);
 +		flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 +		if (attr->mirror_count)
 +			mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 +		mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
  	}
  
  	mlx5_eswitch_del_vlan_action(esw, attr);
@@@ -2805,47 -2757,286 +2914,52 @@@ int mlx5e_configure_flower(struct mlx5e
  	flow->flags = flow_flags;
  	flow->priv = priv;
  
 -	*__flow = flow;
 -	*__parse_attr = parse_attr;
 -
 -	return 0;
 -
 -err_free:
 -	kfree(flow);
 -	kvfree(parse_attr);
 -	return err;
 -}
 -
 -static void
 -mlx5e_flow_esw_attr_init(struct mlx5_esw_flow_attr *esw_attr,
 -			 struct mlx5e_priv *priv,
 -			 struct mlx5e_tc_flow_parse_attr *parse_attr,
 -			 struct tc_cls_flower_offload *f,
 -			 struct mlx5_eswitch_rep *in_rep,
 -			 struct mlx5_core_dev *in_mdev)
 -{
 -	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -
 -	esw_attr->parse_attr = parse_attr;
 -	esw_attr->chain = f->common.chain_index;
 -	esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
 -
 -	esw_attr->in_rep = in_rep;
 -	esw_attr->in_mdev = in_mdev;
 -
 -	if (MLX5_CAP_ESW(esw->dev, counter_eswitch_affinity) ==
 -	    MLX5_COUNTER_SOURCE_ESWITCH)
 -		esw_attr->counter_dev = in_mdev;
 -	else
 -		esw_attr->counter_dev = priv->mdev;
 -}
 -
 -static struct mlx5e_tc_flow *
 -__mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
 -		     struct tc_cls_flower_offload *f,
 -		     u16 flow_flags,
 -		     struct net_device *filter_dev,
 -		     struct mlx5_eswitch_rep *in_rep,
 -		     struct mlx5_core_dev *in_mdev)
 -{
 -	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
 -	struct netlink_ext_ack *extack = f->common.extack;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr;
 -	struct mlx5e_tc_flow *flow;
 -	int attr_size, err;
 -
 -	flow_flags |= MLX5E_TC_FLOW_ESWITCH;
 -	attr_size  = sizeof(struct mlx5_esw_flow_attr);
 -	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
 -			       &parse_attr, &flow);
 -	if (err)
 -		goto out;
 -
 -	parse_attr->filter_dev = filter_dev;
 -	mlx5e_flow_esw_attr_init(flow->esw_attr,
 -				 priv, parse_attr,
 -				 f, in_rep, in_mdev);
 -
 -	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
 -			       f, filter_dev);
 -	if (err)
 -		goto err_free;
 -
 -	err = parse_tc_fdb_actions(priv, &rule->action, parse_attr, flow, extack);
 -	if (err)
 +	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 +	if (err < 0)
  		goto err_free;
  
 -	err = mlx5e_tc_add_fdb_flow(priv, flow, extack);
 -	if (err) {
 -		if (!(err == -ENETUNREACH && mlx5_lag_is_multipath(in_mdev)))
 +	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 +		err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
  			goto err_free;
++<<<<<<< HEAD
 +		flow->rule[0] = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
 +	} else {
 +		err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
++=======
+ 
+ 		add_unready_flow(flow);
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  	}
  
 -	return flow;
 -
 -err_free:
 -	kfree(flow);
 -	kvfree(parse_attr);
 -out:
 -	return ERR_PTR(err);
 -}
 -
 -static int mlx5e_tc_add_fdb_peer_flow(struct tc_cls_flower_offload *f,
 -				      struct mlx5e_tc_flow *flow,
 -				      u16 flow_flags)
 -{
 -	struct mlx5e_priv *priv = flow->priv, *peer_priv;
 -	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch, *peer_esw;
 -	struct mlx5_devcom *devcom = priv->mdev->priv.devcom;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr;
 -	struct mlx5e_rep_priv *peer_urpriv;
 -	struct mlx5e_tc_flow *peer_flow;
 -	struct mlx5_core_dev *in_mdev;
 -	int err = 0;
 -
 -	peer_esw = mlx5_devcom_get_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 -	if (!peer_esw)
 -		return -ENODEV;
 -
 -	peer_urpriv = mlx5_eswitch_get_uplink_priv(peer_esw, REP_ETH);
 -	peer_priv = netdev_priv(peer_urpriv->netdev);
 -
 -	/* in_mdev is assigned of which the packet originated from.
 -	 * So packets redirected to uplink use the same mdev of the
 -	 * original flow and packets redirected from uplink use the
 -	 * peer mdev.
 -	 */
 -	if (flow->esw_attr->in_rep->vport == MLX5_VPORT_UPLINK)
 -		in_mdev = peer_priv->mdev;
 -	else
 -		in_mdev = priv->mdev;
 -
 -	parse_attr = flow->esw_attr->parse_attr;
 -	peer_flow = __mlx5e_add_fdb_flow(peer_priv, f, flow_flags,
 -					 parse_attr->filter_dev,
 -					 flow->esw_attr->in_rep, in_mdev);
 -	if (IS_ERR(peer_flow)) {
 -		err = PTR_ERR(peer_flow);
 -		goto out;
 +	if (IS_ERR(flow->rule[0])) {
 +		err = PTR_ERR(flow->rule[0]);
 +		if (err != -EAGAIN)
 +			goto err_free;
  	}
  
 -	flow->peer_flow = peer_flow;
 -	flow->flags |= MLX5E_TC_FLOW_DUP;
 -	mutex_lock(&esw->offloads.peer_mutex);
 -	list_add_tail(&flow->peer, &esw->offloads.peer_flows);
 -	mutex_unlock(&esw->offloads.peer_mutex);
 -
 -out:
 -	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 -	return err;
 -}
 -
 -static int
 -mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
 -		   struct tc_cls_flower_offload *f,
 -		   u16 flow_flags,
 -		   struct net_device *filter_dev,
 -		   struct mlx5e_tc_flow **__flow)
 -{
 -	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 -	struct mlx5_eswitch_rep *in_rep = rpriv->rep;
 -	struct mlx5_core_dev *in_mdev = priv->mdev;
 -	struct mlx5e_tc_flow *flow;
 -	int err;
 +	if (err != -EAGAIN)
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
  
 -	flow = __mlx5e_add_fdb_flow(priv, f, flow_flags, filter_dev, in_rep,
 -				    in_mdev);
 -	if (IS_ERR(flow))
 -		return PTR_ERR(flow);
 +	if (!(flow->flags & MLX5E_TC_FLOW_ESWITCH) ||
 +	    !(flow->esw_attr->action &
 +	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
 +		kvfree(parse_attr);
  
 -	if (is_peer_flow_needed(flow)) {
 -		err = mlx5e_tc_add_fdb_peer_flow(f, flow, flow_flags);
 -		if (err) {
 -			mlx5e_tc_del_fdb_flow(priv, flow);
 -			goto out;
 -		}
 +	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 +	if (err) {
 +		mlx5e_tc_del_flow(priv, flow);
 +		kfree(flow);
  	}
  
 -	*__flow = flow;
 -
 -	return 0;
 -
 -out:
  	return err;
 -}
 -
 -static int
 -mlx5e_add_nic_flow(struct mlx5e_priv *priv,
 -		   struct tc_cls_flower_offload *f,
 -		   u16 flow_flags,
 -		   struct net_device *filter_dev,
 -		   struct mlx5e_tc_flow **__flow)
 -{
 -	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
 -	struct netlink_ext_ack *extack = f->common.extack;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr;
 -	struct mlx5e_tc_flow *flow;
 -	int attr_size, err;
 -
 -	/* multi-chain not supported for NIC rules */
 -	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
 -		return -EOPNOTSUPP;
 -
 -	flow_flags |= MLX5E_TC_FLOW_NIC;
 -	attr_size  = sizeof(struct mlx5_nic_flow_attr);
 -	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
 -			       &parse_attr, &flow);
 -	if (err)
 -		goto out;
 -
 -	parse_attr->filter_dev = filter_dev;
 -	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
 -			       f, filter_dev);
 -	if (err)
 -		goto err_free;
 -
 -	err = parse_tc_nic_actions(priv, &rule->action, parse_attr, flow, extack);
 -	if (err)
 -		goto err_free;
 -
 -	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
 -	if (err)
 -		goto err_free;
 -
 -	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 -	kvfree(parse_attr);
 -	*__flow = flow;
 -
 -	return 0;
  
  err_free:
 -	kfree(flow);
  	kvfree(parse_attr);
 -out:
 -	return err;
 -}
 -
 -static int
 -mlx5e_tc_add_flow(struct mlx5e_priv *priv,
 -		  struct tc_cls_flower_offload *f,
 -		  int flags,
 -		  struct net_device *filter_dev,
 -		  struct mlx5e_tc_flow **flow)
 -{
 -	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	u16 flow_flags;
 -	int err;
 -
 -	get_flags(flags, &flow_flags);
 -
 -	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 -		return -EOPNOTSUPP;
 -
 -	if (esw && esw->mode == SRIOV_OFFLOADS)
 -		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
 -					 filter_dev, flow);
 -	else
 -		err = mlx5e_add_nic_flow(priv, f, flow_flags,
 -					 filter_dev, flow);
 -
 -	return err;
 -}
 -
 -int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 -			   struct tc_cls_flower_offload *f, int flags)
 -{
 -	struct netlink_ext_ack *extack = f->common.extack;
 -	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 -	struct mlx5e_tc_flow *flow;
 -	int err = 0;
 -
 -	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
 -	if (flow) {
 -		NL_SET_ERR_MSG_MOD(extack,
 -				   "flow cookie already exists, ignoring");
 -		netdev_warn_once(priv->netdev,
 -				 "flow cookie %lx already exists, ignoring\n",
 -				 f->cookie);
 -		goto out;
 -	}
 -
 -	err = mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
 -	if (err)
 -		goto out;
 -
 -	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 -	if (err)
 -		goto err_free;
 -
 -	return 0;
 -
 -err_free:
 -	mlx5e_tc_del_flow(priv, flow);
  	kfree(flow);
 -out:
  	return err;
  }
  
@@@ -3014,3 -3228,26 +3128,29 @@@ int mlx5e_tc_num_filters(struct mlx5e_p
  
  	return atomic_read(&tc_ht->nelems);
  }
++<<<<<<< HEAD
++=======
+ 
+ void mlx5e_tc_clean_fdb_peer_flows(struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5e_tc_flow *flow, *tmp;
+ 
+ 	list_for_each_entry_safe(flow, tmp, &esw->offloads.peer_flows, peer)
+ 		__mlx5e_tc_del_fdb_peer_flow(flow);
+ }
+ 
+ void mlx5e_tc_reoffload_flows_work(struct work_struct *work)
+ {
+ 	struct mlx5_rep_uplink_priv *rpriv =
+ 		container_of(work, struct mlx5_rep_uplink_priv,
+ 			     reoffload_flows_work);
+ 	struct mlx5e_tc_flow *flow, *tmp;
+ 
+ 	rtnl_lock();
+ 	list_for_each_entry_safe(flow, tmp, &rpriv->unready_flows, unready) {
+ 		if (!mlx5e_tc_add_fdb_flow(flow->priv, flow, NULL))
+ 			remove_unready_flow(flow);
+ 	}
+ 	rtnl_unlock();
+ }
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
index 378507988a32,f62e81902d27..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
@@@ -68,10 -70,9 +68,14 @@@ void mlx5e_tc_encap_flows_del(struct ml
  struct mlx5e_neigh_hash_entry;
  void mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe);
  
 -int mlx5e_tc_num_filters(struct mlx5e_priv *priv, int flags);
 +int mlx5e_tc_num_filters(struct mlx5e_priv *priv);
  
++<<<<<<< HEAD
 +bool mlx5e_tc_tun_device_to_offload(struct mlx5e_priv *priv,
 +				    struct net_device *netdev);
++=======
+ void mlx5e_tc_reoffload_flows_work(struct work_struct *work);
++>>>>>>> b4a23329e2e9 (net/mlx5e: Re-attempt to offload flows on multipath port affinity events)
  
  #else /* CONFIG_MLX5_ESWITCH */
  static inline int  mlx5e_tc_nic_init(struct mlx5e_priv *priv) { return 0; }
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
