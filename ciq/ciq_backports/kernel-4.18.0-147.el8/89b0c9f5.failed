KVM: VMX: Move VMX instruction wrappers to a dedicated header file

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 89b0c9f58350f6820f062ea12000e8a171177f3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/89b0c9f5.failed

VMX has a few hundred lines of code just to wrap various VMX specific
instructions, e.g. VMWREAD, INVVPID, etc...  Move them to a dedicated
header so it's easier to find/isolate the boilerplate.

With this change, more inlines can be moved from vmx.c to vmx.h.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 89b0c9f58350f6820f062ea12000e8a171177f3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/vmx/vmx.h
diff --cc arch/x86/kvm/vmx/vmx.c
index 461e3698b5ee,cde40c52d2c6..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -16,53 -16,53 +16,68 @@@
   *
   */
  
 -#include <linux/frame.h>
 -#include <linux/highmem.h>
 -#include <linux/hrtimer.h>
 -#include <linux/kernel.h>
 +#include "irq.h"
 +#include "mmu.h"
 +#include "cpuid.h"
 +#include "lapic.h"
 +
  #include <linux/kvm_host.h>
  #include <linux/module.h>
 -#include <linux/moduleparam.h>
 -#include <linux/mod_devicetable.h>
 +#include <linux/kernel.h>
  #include <linux/mm.h>
 +#include <linux/highmem.h>
  #include <linux/sched.h>
 +#include <linux/sched/smt.h>
 +#include <linux/moduleparam.h>
 +#include <linux/mod_devicetable.h>
 +#include <linux/trace_events.h>
  #include <linux/slab.h>
  #include <linux/tboot.h>
 -#include <linux/trace_events.h>
 +#include <linux/hrtimer.h>
 +#include <linux/frame.h>
 +#include <linux/nospec.h>
 +#include "kvm_cache_regs.h"
 +#include "x86.h"
  
 -#include <asm/apic.h>
  #include <asm/asm.h>
  #include <asm/cpu.h>
 -#include <asm/debugreg.h>
 +#include <asm/io.h>
  #include <asm/desc.h>
 +#include <asm/vmx.h>
 +#include <asm/virtext.h>
 +#include <asm/mce.h>
  #include <asm/fpu/internal.h>
 -#include <asm/io.h>
 -#include <asm/irq_remapping.h>
 -#include <asm/kexec.h>
  #include <asm/perf_event.h>
 -#include <asm/mce.h>
 +#include <asm/debugreg.h>
 +#include <asm/kexec.h>
 +#include <asm/apic.h>
 +#include <asm/irq_remapping.h>
  #include <asm/mmu_context.h>
 -#include <asm/mshyperv.h>
  #include <asm/spec-ctrl.h>
 -#include <asm/virtext.h>
 -#include <asm/vmx.h>
 +#include <asm/mshyperv.h>
  
++<<<<<<< HEAD
 +#include "trace.h"
 +#include "pmu.h"
 +#include "vmx_evmcs.h"
- 
- #define __ex(x) __kvm_handle_fault_on_reboot(x)
- #define __ex_clear(x, reg) \
- 	____kvm_handle_fault_on_reboot(x, "xor " reg ", " reg)
++=======
+ #include "capabilities.h"
+ #include "cpuid.h"
+ #include "evmcs.h"
+ #include "hyperv.h"
+ #include "irq.h"
+ #include "kvm_cache_regs.h"
+ #include "lapic.h"
+ #include "mmu.h"
+ #include "ops.h"
+ #include "pmu.h"
+ #include "trace.h"
+ #include "vmcs.h"
+ #include "vmcs12.h"
+ #include "vmx.h"
+ #include "x86.h"
+ #include "vmx.h"
++>>>>>>> 89b0c9f58350 (KVM: VMX: Move VMX instruction wrappers to a dedicated header file)
  
  MODULE_AUTHOR("Qumranet");
  MODULE_LICENSE("GPL");
@@@ -1285,8 -366,6 +1300,11 @@@ static inline struct vmcs12 *get_shadow
  
  static bool nested_ept_ad_enabled(struct kvm_vcpu *vcpu);
  static unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
 +static u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
 +static bool vmx_xsaves_supported(void);
++=======
++>>>>>>> 89b0c9f58350 (KVM: VMX: Move VMX instruction wrappers to a dedicated header file)
  static void vmx_set_segment(struct kvm_vcpu *vcpu,
  			    struct kvm_segment *var, int seg);
  static void vmx_get_segment(struct kvm_vcpu *vcpu,
@@@ -2225,292 -790,6 +2190,295 @@@ void loaded_vmcs_clear(struct loaded_vm
  			 __loaded_vmcs_clear, loaded_vmcs, 1);
  }
  
++<<<<<<< HEAD
 +static inline bool vpid_sync_vcpu_addr(int vpid, gva_t addr)
 +{
 +	if (vpid == 0)
 +		return true;
 +
 +	if (cpu_has_vmx_invvpid_individual_addr()) {
 +		__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR, vpid, addr);
 +		return true;
 +	}
 +
 +	return false;
 +}
 +
 +static inline void vpid_sync_vcpu_single(int vpid)
 +{
 +	if (vpid == 0)
 +		return;
 +
 +	if (cpu_has_vmx_invvpid_single())
 +		__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);
 +}
 +
 +static inline void vpid_sync_vcpu_global(void)
 +{
 +	if (cpu_has_vmx_invvpid_global())
 +		__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);
 +}
 +
 +static inline void vpid_sync_context(int vpid)
 +{
 +	if (cpu_has_vmx_invvpid_single())
 +		vpid_sync_vcpu_single(vpid);
 +	else
 +		vpid_sync_vcpu_global();
 +}
 +
 +static inline void ept_sync_global(void)
 +{
 +	__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);
 +}
 +
 +static inline void ept_sync_context(u64 eptp)
 +{
 +	if (cpu_has_vmx_invept_context())
 +		__invept(VMX_EPT_EXTENT_CONTEXT, eptp, 0);
 +	else
 +		ept_sync_global();
 +}
 +
 +static __always_inline void vmcs_check16(unsigned long field)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
 +			 "16-bit accessor invalid for 64-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
 +			 "16-bit accessor invalid for 64-bit high field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
 +			 "16-bit accessor invalid for 32-bit high field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
 +			 "16-bit accessor invalid for natural width field");
 +}
 +
 +static __always_inline void vmcs_check32(unsigned long field)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
 +			 "32-bit accessor invalid for 16-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
 +			 "32-bit accessor invalid for natural width field");
 +}
 +
 +static __always_inline void vmcs_check64(unsigned long field)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
 +			 "64-bit accessor invalid for 16-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
 +			 "64-bit accessor invalid for 64-bit high field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
 +			 "64-bit accessor invalid for 32-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
 +			 "64-bit accessor invalid for natural width field");
 +}
 +
 +static __always_inline void vmcs_checkl(unsigned long field)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
 +			 "Natural width accessor invalid for 16-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
 +			 "Natural width accessor invalid for 64-bit field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
 +			 "Natural width accessor invalid for 64-bit high field");
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
 +			 "Natural width accessor invalid for 32-bit field");
 +}
 +
 +static __always_inline unsigned long __vmcs_readl(unsigned long field)
 +{
 +	unsigned long value;
 +
 +	asm volatile (__ex_clear("vmread %1, %0", "%k0")
 +		      : "=r"(value) : "r"(field));
 +	return value;
 +}
 +
 +static __always_inline u16 vmcs_read16(unsigned long field)
 +{
 +	vmcs_check16(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_read16(field);
 +	return __vmcs_readl(field);
 +}
 +
 +static __always_inline u32 vmcs_read32(unsigned long field)
 +{
 +	vmcs_check32(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_read32(field);
 +	return __vmcs_readl(field);
 +}
 +
 +static __always_inline u64 vmcs_read64(unsigned long field)
 +{
 +	vmcs_check64(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_read64(field);
 +#ifdef CONFIG_X86_64
 +	return __vmcs_readl(field);
 +#else
 +	return __vmcs_readl(field) | ((u64)__vmcs_readl(field+1) << 32);
 +#endif
 +}
 +
 +static __always_inline unsigned long vmcs_readl(unsigned long field)
 +{
 +	vmcs_checkl(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_read64(field);
 +	return __vmcs_readl(field);
 +}
 +
 +static noinline void vmwrite_error(unsigned long field, unsigned long value)
 +{
 +	printk(KERN_ERR "vmwrite error: reg %lx value %lx (err %d)\n",
 +	       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
 +	dump_stack();
 +}
 +
 +static __always_inline void __vmcs_writel(unsigned long field, unsigned long value)
 +{
 +	bool error;
 +
 +	asm volatile (__ex("vmwrite %2, %1") CC_SET(na)
 +		      : CC_OUT(na) (error) : "r"(field), "rm"(value));
 +	if (unlikely(error))
 +		vmwrite_error(field, value);
 +}
 +
 +static __always_inline void vmcs_write16(unsigned long field, u16 value)
 +{
 +	vmcs_check16(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write16(field, value);
 +
 +	__vmcs_writel(field, value);
 +}
 +
 +static __always_inline void vmcs_write32(unsigned long field, u32 value)
 +{
 +	vmcs_check32(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write32(field, value);
 +
 +	__vmcs_writel(field, value);
 +}
 +
 +static __always_inline void vmcs_write64(unsigned long field, u64 value)
 +{
 +	vmcs_check64(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write64(field, value);
 +
 +	__vmcs_writel(field, value);
 +#ifndef CONFIG_X86_64
 +	asm volatile ("");
 +	__vmcs_writel(field+1, value >> 32);
 +#endif
 +}
 +
 +static __always_inline void vmcs_writel(unsigned long field, unsigned long value)
 +{
 +	vmcs_checkl(field);
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write64(field, value);
 +
 +	__vmcs_writel(field, value);
 +}
 +
 +static __always_inline void vmcs_clear_bits(unsigned long field, u32 mask)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,
 +			 "vmcs_clear_bits does not support 64-bit fields");
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write32(field, evmcs_read32(field) & ~mask);
 +
 +	__vmcs_writel(field, __vmcs_readl(field) & ~mask);
 +}
 +
 +static __always_inline void vmcs_set_bits(unsigned long field, u32 mask)
 +{
 +        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,
 +			 "vmcs_set_bits does not support 64-bit fields");
 +	if (static_branch_unlikely(&enable_evmcs))
 +		return evmcs_write32(field, evmcs_read32(field) | mask);
 +
 +	__vmcs_writel(field, __vmcs_readl(field) | mask);
 +}
 +
 +static inline void vm_entry_controls_reset_shadow(struct vcpu_vmx *vmx)
 +{
 +	vmx->vm_entry_controls_shadow = vmcs_read32(VM_ENTRY_CONTROLS);
 +}
 +
 +static inline void vm_entry_controls_init(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vmcs_write32(VM_ENTRY_CONTROLS, val);
 +	vmx->vm_entry_controls_shadow = val;
 +}
 +
 +static inline void vm_entry_controls_set(struct vcpu_vmx *vmx, u32 val)
 +{
 +	if (vmx->vm_entry_controls_shadow != val)
 +		vm_entry_controls_init(vmx, val);
 +}
 +
 +static inline u32 vm_entry_controls_get(struct vcpu_vmx *vmx)
 +{
 +	return vmx->vm_entry_controls_shadow;
 +}
 +
 +
 +static inline void vm_entry_controls_setbit(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) | val);
 +}
 +
 +static inline void vm_entry_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) & ~val);
 +}
 +
 +static inline void vm_exit_controls_reset_shadow(struct vcpu_vmx *vmx)
 +{
 +	vmx->vm_exit_controls_shadow = vmcs_read32(VM_EXIT_CONTROLS);
 +}
 +
 +static inline void vm_exit_controls_init(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vmcs_write32(VM_EXIT_CONTROLS, val);
 +	vmx->vm_exit_controls_shadow = val;
 +}
 +
 +static inline void vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val)
 +{
 +	if (vmx->vm_exit_controls_shadow != val)
 +		vm_exit_controls_init(vmx, val);
 +}
 +
 +static inline u32 vm_exit_controls_get(struct vcpu_vmx *vmx)
 +{
 +	return vmx->vm_exit_controls_shadow;
 +}
 +
 +
 +static inline void vm_exit_controls_setbit(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) | val);
 +}
 +
 +static inline void vm_exit_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
 +{
 +	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) & ~val);
 +}
 +
 +static void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 +{
 +	vmx->segment_cache.bitmask = 0;
 +}
 +
++=======
++>>>>>>> 89b0c9f58350 (KVM: VMX: Move VMX instruction wrappers to a dedicated header file)
  static bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,
  				       unsigned field)
  {
@@@ -6378,21 -4602,7 +6312,25 @@@ static void vmx_refresh_apicv_exec_ctrl
  		vmx_update_msr_bitmap(vcpu);
  }
  
++<<<<<<< HEAD
 +static u32 vmx_vmentry_ctrl(void)
 +{
 +	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 +	return vmcs_config.vmentry_ctrl &
 +		~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL | VM_ENTRY_LOAD_IA32_EFER);
 +}
 +
 +static u32 vmx_vmexit_ctrl(void)
 +{
 +	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 +	return vmcs_config.vmexit_ctrl &
 +		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
 +}
 +
 +static u32 vmx_exec_control(struct vcpu_vmx *vmx)
++=======
+ u32 vmx_exec_control(struct vcpu_vmx *vmx)
++>>>>>>> 89b0c9f58350 (KVM: VMX: Move VMX instruction wrappers to a dedicated header file)
  {
  	u32 exec_control = vmcs_config.cpu_based_exec_ctrl;
  
@@@ -6418,17 -4628,6 +6356,20 @@@
  	return exec_control;
  }
  
++<<<<<<< HEAD
 +static bool vmx_rdrand_supported(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_RDRAND_EXITING;
 +}
 +
 +static bool vmx_rdseed_supported(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_RDSEED_EXITING;
 +}
++=======
++>>>>>>> 89b0c9f58350 (KVM: VMX: Move VMX instruction wrappers to a dedicated header file)
  
  static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
  {
* Unmerged path arch/x86/kvm/vmx/vmx.h
diff --git a/arch/x86/kvm/vmx/ops.h b/arch/x86/kvm/vmx/ops.h
new file mode 100644
index 000000000000..b8e50f76fefc
--- /dev/null
+++ b/arch/x86/kvm/vmx/ops.h
@@ -0,0 +1,285 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __KVM_X86_VMX_INSN_H
+#define __KVM_X86_VMX_INSN_H
+
+#include <linux/nospec.h>
+
+#include <asm/kvm_host.h>
+#include <asm/vmx.h>
+
+#include "evmcs.h"
+#include "vmcs.h"
+
+#define __ex(x) __kvm_handle_fault_on_reboot(x)
+#define __ex_clear(x, reg) \
+	____kvm_handle_fault_on_reboot(x, "xor " reg ", " reg)
+
+static __always_inline void vmcs_check16(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
+			 "16-bit accessor invalid for 64-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "16-bit accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "16-bit accessor invalid for 32-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "16-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_check32(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "32-bit accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "32-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_check64(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "64-bit accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "64-bit accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "64-bit accessor invalid for 32-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "64-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_checkl(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "Natural width accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
+			 "Natural width accessor invalid for 64-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "Natural width accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "Natural width accessor invalid for 32-bit field");
+}
+
+static __always_inline unsigned long __vmcs_readl(unsigned long field)
+{
+	unsigned long value;
+
+	asm volatile (__ex_clear("vmread %1, %0", "%k0")
+		      : "=r"(value) : "r"(field));
+	return value;
+}
+
+static __always_inline u16 vmcs_read16(unsigned long field)
+{
+	vmcs_check16(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_read16(field);
+	return __vmcs_readl(field);
+}
+
+static __always_inline u32 vmcs_read32(unsigned long field)
+{
+	vmcs_check32(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_read32(field);
+	return __vmcs_readl(field);
+}
+
+static __always_inline u64 vmcs_read64(unsigned long field)
+{
+	vmcs_check64(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_read64(field);
+#ifdef CONFIG_X86_64
+	return __vmcs_readl(field);
+#else
+	return __vmcs_readl(field) | ((u64)__vmcs_readl(field+1) << 32);
+#endif
+}
+
+static __always_inline unsigned long vmcs_readl(unsigned long field)
+{
+	vmcs_checkl(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_read64(field);
+	return __vmcs_readl(field);
+}
+
+static noinline void vmwrite_error(unsigned long field, unsigned long value)
+{
+	printk(KERN_ERR "vmwrite error: reg %lx value %lx (err %d)\n",
+	       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
+	dump_stack();
+}
+
+static __always_inline void __vmcs_writel(unsigned long field, unsigned long value)
+{
+	bool error;
+
+	asm volatile (__ex("vmwrite %2, %1") CC_SET(na)
+		      : CC_OUT(na) (error) : "r"(field), "rm"(value));
+	if (unlikely(error))
+		vmwrite_error(field, value);
+}
+
+static __always_inline void vmcs_write16(unsigned long field, u16 value)
+{
+	vmcs_check16(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write16(field, value);
+
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_write32(unsigned long field, u32 value)
+{
+	vmcs_check32(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write32(field, value);
+
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_write64(unsigned long field, u64 value)
+{
+	vmcs_check64(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write64(field, value);
+
+	__vmcs_writel(field, value);
+#ifndef CONFIG_X86_64
+	asm volatile ("");
+	__vmcs_writel(field+1, value >> 32);
+#endif
+}
+
+static __always_inline void vmcs_writel(unsigned long field, unsigned long value)
+{
+	vmcs_checkl(field);
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write64(field, value);
+
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_clear_bits(unsigned long field, u32 mask)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,
+			 "vmcs_clear_bits does not support 64-bit fields");
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write32(field, evmcs_read32(field) & ~mask);
+
+	__vmcs_writel(field, __vmcs_readl(field) & ~mask);
+}
+
+static __always_inline void vmcs_set_bits(unsigned long field, u32 mask)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,
+			 "vmcs_set_bits does not support 64-bit fields");
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_write32(field, evmcs_read32(field) | mask);
+
+	__vmcs_writel(field, __vmcs_readl(field) | mask);
+}
+
+static inline void vmcs_clear(struct vmcs *vmcs)
+{
+	u64 phys_addr = __pa(vmcs);
+	bool error;
+
+	asm volatile (__ex("vmclear %1") CC_SET(na)
+		      : CC_OUT(na) (error) : "m"(phys_addr));
+	if (unlikely(error))
+		printk(KERN_ERR "kvm: vmclear fail: %p/%llx\n",
+		       vmcs, phys_addr);
+}
+
+static inline void vmcs_load(struct vmcs *vmcs)
+{
+	u64 phys_addr = __pa(vmcs);
+	bool error;
+
+	if (static_branch_unlikely(&enable_evmcs))
+		return evmcs_load(phys_addr);
+
+	asm volatile (__ex("vmptrld %1") CC_SET(na)
+		      : CC_OUT(na) (error) : "m"(phys_addr));
+	if (unlikely(error))
+		printk(KERN_ERR "kvm: vmptrld %p/%llx failed\n",
+		       vmcs, phys_addr);
+}
+
+static inline void __invvpid(unsigned long ext, u16 vpid, gva_t gva)
+{
+	struct {
+		u64 vpid : 16;
+		u64 rsvd : 48;
+		u64 gva;
+	} operand = { vpid, 0, gva };
+	bool error;
+
+	asm volatile (__ex("invvpid %2, %1") CC_SET(na)
+		      : CC_OUT(na) (error) : "r"(ext), "m"(operand));
+	BUG_ON(error);
+}
+
+static inline void __invept(unsigned long ext, u64 eptp, gpa_t gpa)
+{
+	struct {
+		u64 eptp, gpa;
+	} operand = {eptp, gpa};
+	bool error;
+
+	asm volatile (__ex("invept %2, %1") CC_SET(na)
+		      : CC_OUT(na) (error) : "r"(ext), "m"(operand));
+	BUG_ON(error);
+}
+
+static inline bool vpid_sync_vcpu_addr(int vpid, gva_t addr)
+{
+	if (vpid == 0)
+		return true;
+
+	if (cpu_has_vmx_invvpid_individual_addr()) {
+		__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR, vpid, addr);
+		return true;
+	}
+
+	return false;
+}
+
+static inline void vpid_sync_vcpu_single(int vpid)
+{
+	if (vpid == 0)
+		return;
+
+	if (cpu_has_vmx_invvpid_single())
+		__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);
+}
+
+static inline void vpid_sync_vcpu_global(void)
+{
+	if (cpu_has_vmx_invvpid_global())
+		__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);
+}
+
+static inline void vpid_sync_context(int vpid)
+{
+	if (cpu_has_vmx_invvpid_single())
+		vpid_sync_vcpu_single(vpid);
+	else
+		vpid_sync_vcpu_global();
+}
+
+static inline void ept_sync_global(void)
+{
+	__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);
+}
+
+static inline void ept_sync_context(u64 eptp)
+{
+	if (cpu_has_vmx_invept_context())
+		__invept(VMX_EPT_EXTENT_CONTEXT, eptp, 0);
+	else
+		ept_sync_global();
+}
+
+#endif /* __KVM_X86_VMX_INSN_H */
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/vmx/vmx.h
