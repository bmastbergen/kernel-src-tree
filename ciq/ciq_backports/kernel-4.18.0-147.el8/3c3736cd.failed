KVM: arm/arm64: Fix handling of stage2 huge mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Suzuki K Poulose <suzuki.poulose@arm.com>
commit 3c3736cd32bf5197aed1410ae826d2d254a5b277
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/3c3736cd.failed

We rely on the mmu_notifier call backs to handle the split/merge
of huge pages and thus we are guaranteed that, while creating a
block mapping, either the entire block is unmapped at stage2 or it
is missing permission.

However, we miss a case where the block mapping is split for dirty
logging case and then could later be made block mapping, if we cancel the
dirty logging. This not only creates inconsistent TLB entries for
the pages in the the block, but also leakes the table pages for
PMD level.

Handle this corner case for the huge mappings at stage2 by
unmapping the non-huge mapping for the block. This could potentially
release the upper level table. So we need to restart the table walk
once we unmap the range.

Fixes : ad361f093c1e31d ("KVM: ARM: Support hugetlbfs backed huge pages")
	Reported-by: Zheng Xiang <zhengxiang9@huawei.com>
	Cc: Zheng Xiang <zhengxiang9@huawei.com>
	Cc: Zenghui Yu <yuzenghui@huawei.com>
	Cc: Christoffer Dall <christoffer.dall@arm.com>
	Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
(cherry picked from commit 3c3736cd32bf5197aed1410ae826d2d254a5b277)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	virt/kvm/arm/mmu.c
diff --cc virt/kvm/arm/mmu.c
index 07ca5de32432,f9da2fad9bd6..000000000000
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@@ -1079,8 -1126,56 +1096,58 @@@ retry
  	return 0;
  }
  
 -static int stage2_set_pud_huge(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
 -			       phys_addr_t addr, const pud_t *new_pudp)
 +static bool stage2_is_exec(struct kvm *kvm, phys_addr_t addr)
  {
++<<<<<<< HEAD
++=======
+ 	pud_t *pudp, old_pud;
+ 
+ retry:
+ 	pudp = stage2_get_pud(kvm, cache, addr);
+ 	VM_BUG_ON(!pudp);
+ 
+ 	old_pud = *pudp;
+ 
+ 	/*
+ 	 * A large number of vcpus faulting on the same stage 2 entry,
+ 	 * can lead to a refault due to the stage2_pud_clear()/tlb_flush().
+ 	 * Skip updating the page tables if there is no change.
+ 	 */
+ 	if (pud_val(old_pud) == pud_val(*new_pudp))
+ 		return 0;
+ 
+ 	if (stage2_pud_present(kvm, old_pud)) {
+ 		/*
+ 		 * If we already have table level mapping for this block, unmap
+ 		 * the range for this block and retry.
+ 		 */
+ 		if (!stage2_pud_huge(kvm, old_pud)) {
+ 			unmap_stage2_range(kvm, addr & S2_PUD_MASK, S2_PUD_SIZE);
+ 			goto retry;
+ 		}
+ 
+ 		WARN_ON_ONCE(kvm_pud_pfn(old_pud) != kvm_pud_pfn(*new_pudp));
+ 		stage2_pud_clear(kvm, pudp);
+ 		kvm_tlb_flush_vmid_ipa(kvm, addr);
+ 	} else {
+ 		get_page(virt_to_page(pudp));
+ 	}
+ 
+ 	kvm_set_pud(pudp, *new_pudp);
+ 	return 0;
+ }
+ 
+ /*
+  * stage2_get_leaf_entry - walk the stage2 VM page tables and return
+  * true if a valid and present leaf-entry is found. A pointer to the
+  * leaf-entry is returned in the appropriate level variable - pudpp,
+  * pmdpp, ptepp.
+  */
+ static bool stage2_get_leaf_entry(struct kvm *kvm, phys_addr_t addr,
+ 				  pud_t **pudpp, pmd_t **pmdpp, pte_t **ptepp)
+ {
+ 	pud_t *pudp;
++>>>>>>> 3c3736cd32bf (KVM: arm/arm64: Fix handling of stage2 huge mappings)
  	pmd_t *pmdp;
  	pte_t *ptep;
  
diff --git a/arch/arm/include/asm/stage2_pgtable.h b/arch/arm/include/asm/stage2_pgtable.h
index d8e39a8f3148..441a5e5fe63f 100644
--- a/arch/arm/include/asm/stage2_pgtable.h
+++ b/arch/arm/include/asm/stage2_pgtable.h
@@ -70,5 +70,7 @@ stage2_pmd_addr_end(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)
 
 #define S2_PMD_MASK				PMD_MASK
 #define S2_PMD_SIZE				PMD_SIZE
+#define S2_PUD_MASK				PUD_MASK
+#define S2_PUD_SIZE				PUD_SIZE
 
 #endif	/* __ARM_S2_PGTABLE_H_ */
* Unmerged path virt/kvm/arm/mmu.c
