RDMA/mlx5: Attach a DEVX counter via raw flow creation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Mark Bloch <markb@mellanox.com>
commit bfc5d839184f53cc16d551873f9254f2d4d493be
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/bfc5d839.failed

Allow a user to attach a DEVX counter via mlx5 raw flow creation. In order
to attach a counter we introduce a new attribute:

MLX5_IB_ATTR_CREATE_FLOW_ARR_COUNTERS_DEVX

A counter can be attached to multiple flow steering rules.

	Signed-off-by: Mark Bloch <markb@mellanox.com>
	Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit bfc5d839184f53cc16d551873f9254f2d4d493be)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/flow.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	include/uapi/rdma/mlx5_user_ioctl_cmds.h
diff --cc drivers/infiniband/hw/mlx5/devx.c
index c7f8859c08ee,0aa2ee732eaa..000000000000
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@@ -71,7 -85,267 +71,271 @@@ void mlx5_ib_devx_destroy(struct mlx5_i
  	mlx5_cmd_exec(dev->mdev, in, sizeof(in), out, sizeof(out));
  }
  
++<<<<<<< HEAD
 +static bool devx_is_obj_create_cmd(const void *in)
++=======
+ bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type)
+ {
+ 	struct devx_obj *devx_obj = obj;
+ 	u16 opcode = MLX5_GET(general_obj_in_cmd_hdr, devx_obj->dinbox, opcode);
+ 
+ 	switch (opcode) {
+ 	case MLX5_CMD_OP_DESTROY_TIR:
+ 		*dest_type = MLX5_FLOW_DESTINATION_TYPE_TIR;
+ 		*dest_id = MLX5_GET(general_obj_in_cmd_hdr, devx_obj->dinbox,
+ 				    obj_id);
+ 		return true;
+ 
+ 	case MLX5_CMD_OP_DESTROY_FLOW_TABLE:
+ 		*dest_type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 		*dest_id = MLX5_GET(destroy_flow_table_in, devx_obj->dinbox,
+ 				    table_id);
+ 		return true;
+ 	default:
+ 		return false;
+ 	}
+ }
+ 
+ bool mlx5_ib_devx_is_flow_counter(void *obj, u32 *counter_id)
+ {
+ 	struct devx_obj *devx_obj = obj;
+ 	u16 opcode = MLX5_GET(general_obj_in_cmd_hdr, devx_obj->dinbox, opcode);
+ 
+ 	if (opcode == MLX5_CMD_OP_DEALLOC_FLOW_COUNTER) {
+ 		*counter_id = MLX5_GET(dealloc_flow_counter_in,
+ 				       devx_obj->dinbox,
+ 				       flow_counter_id);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * As the obj_id in the firmware is not globally unique the object type
+  * must be considered upon checking for a valid object id.
+  * For that the opcode of the creator command is encoded as part of the obj_id.
+  */
+ static u64 get_enc_obj_id(u16 opcode, u32 obj_id)
+ {
+ 	return ((u64)opcode << 32) | obj_id;
+ }
+ 
+ static int devx_is_valid_obj_id(struct devx_obj *obj, const void *in)
+ {
+ 	u16 opcode = MLX5_GET(general_obj_in_cmd_hdr, in, opcode);
+ 	u64 obj_id;
+ 
+ 	switch (opcode) {
+ 	case MLX5_CMD_OP_MODIFY_GENERAL_OBJECT:
+ 	case MLX5_CMD_OP_QUERY_GENERAL_OBJECT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_GENERAL_OBJECT,
+ 					MLX5_GET(general_obj_in_cmd_hdr, in,
+ 						 obj_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_MKEY:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_MKEY,
+ 					MLX5_GET(query_mkey_in, in,
+ 						 mkey_index));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_CQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_CQ,
+ 					MLX5_GET(query_cq_in, in, cqn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_CQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_CQ,
+ 					MLX5_GET(modify_cq_in, in, cqn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_SQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_SQ,
+ 					MLX5_GET(query_sq_in, in, sqn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_SQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_SQ,
+ 					MLX5_GET(modify_sq_in, in, sqn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_RQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RQ,
+ 					MLX5_GET(query_rq_in, in, rqn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_RQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RQ,
+ 					MLX5_GET(modify_rq_in, in, rqn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_RMP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RMP,
+ 					MLX5_GET(query_rmp_in, in, rmpn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_RMP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RMP,
+ 					MLX5_GET(modify_rmp_in, in, rmpn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_RQT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RQT,
+ 					MLX5_GET(query_rqt_in, in, rqtn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_RQT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RQT,
+ 					MLX5_GET(modify_rqt_in, in, rqtn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_TIR:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_TIR,
+ 					MLX5_GET(query_tir_in, in, tirn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_TIR:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_TIR,
+ 					MLX5_GET(modify_tir_in, in, tirn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_TIS:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_TIS,
+ 					MLX5_GET(query_tis_in, in, tisn));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_TIS:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_TIS,
+ 					MLX5_GET(modify_tis_in, in, tisn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_FLOW_TABLE:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_FLOW_TABLE,
+ 					MLX5_GET(query_flow_table_in, in,
+ 						 table_id));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_FLOW_TABLE:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_FLOW_TABLE,
+ 					MLX5_GET(modify_flow_table_in, in,
+ 						 table_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_FLOW_GROUP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_FLOW_GROUP,
+ 					MLX5_GET(query_flow_group_in, in,
+ 						 group_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_FLOW_TABLE_ENTRY:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_SET_FLOW_TABLE_ENTRY,
+ 					MLX5_GET(query_fte_in, in,
+ 						 flow_index));
+ 		break;
+ 	case MLX5_CMD_OP_SET_FLOW_TABLE_ENTRY:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_SET_FLOW_TABLE_ENTRY,
+ 					MLX5_GET(set_fte_in, in, flow_index));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_Q_COUNTER:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_ALLOC_Q_COUNTER,
+ 					MLX5_GET(query_q_counter_in, in,
+ 						 counter_set_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_FLOW_COUNTER:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_ALLOC_FLOW_COUNTER,
+ 					MLX5_GET(query_flow_counter_in, in,
+ 						 flow_counter_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_MODIFY_HEADER_CONTEXT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_ALLOC_MODIFY_HEADER_CONTEXT,
+ 					MLX5_GET(general_obj_in_cmd_hdr, in,
+ 						 obj_id));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_SCHEDULING_ELEMENT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT,
+ 					MLX5_GET(query_scheduling_element_in,
+ 						 in, scheduling_element_id));
+ 		break;
+ 	case MLX5_CMD_OP_MODIFY_SCHEDULING_ELEMENT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT,
+ 					MLX5_GET(modify_scheduling_element_in,
+ 						 in, scheduling_element_id));
+ 		break;
+ 	case MLX5_CMD_OP_ADD_VXLAN_UDP_DPORT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_ADD_VXLAN_UDP_DPORT,
+ 					MLX5_GET(add_vxlan_udp_dport_in, in,
+ 						 vxlan_udp_port));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_L2_TABLE_ENTRY:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_SET_L2_TABLE_ENTRY,
+ 					MLX5_GET(query_l2_table_entry_in, in,
+ 						 table_index));
+ 		break;
+ 	case MLX5_CMD_OP_SET_L2_TABLE_ENTRY:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_SET_L2_TABLE_ENTRY,
+ 					MLX5_GET(set_l2_table_entry_in, in,
+ 						 table_index));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(query_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_RST2INIT_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(rst2init_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_INIT2RTR_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(init2rtr_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_RTR2RTS_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(rtr2rts_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_RTS2RTS_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(rts2rts_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_SQERR2RTS_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(sqerr2rts_qp_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_2ERR_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(qp_2err_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_2RST_QP:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_QP,
+ 					MLX5_GET(qp_2rst_in, in, qpn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_DCT:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_DCT,
+ 					MLX5_GET(query_dct_in, in, dctn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_XRQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_XRQ,
+ 					MLX5_GET(query_xrq_in, in, xrqn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_XRC_SRQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_XRC_SRQ,
+ 					MLX5_GET(query_xrc_srq_in, in,
+ 						 xrc_srqn));
+ 		break;
+ 	case MLX5_CMD_OP_ARM_XRC_SRQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_XRC_SRQ,
+ 					MLX5_GET(arm_xrc_srq_in, in, xrc_srqn));
+ 		break;
+ 	case MLX5_CMD_OP_QUERY_SRQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_SRQ,
+ 					MLX5_GET(query_srq_in, in, srqn));
+ 		break;
+ 	case MLX5_CMD_OP_ARM_RQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_RQ,
+ 					MLX5_GET(arm_rq_in, in, srq_number));
+ 		break;
+ 	case MLX5_CMD_OP_DRAIN_DCT:
+ 	case MLX5_CMD_OP_ARM_DCT_FOR_KEY_VIOLATION:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_DCT,
+ 					MLX5_GET(drain_dct_in, in, dctn));
+ 		break;
+ 	case MLX5_CMD_OP_ARM_XRQ:
+ 		obj_id = get_enc_obj_id(MLX5_CMD_OP_CREATE_XRQ,
+ 					MLX5_GET(arm_xrq_in, in, xrqn));
+ 		break;
+ 	default:
+ 		return false;
+ 	}
+ 
+ 	if (obj_id == obj->obj_id)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static void devx_set_umem_valid(const void *in)
++>>>>>>> bfc5d839184f (RDMA/mlx5: Attach a DEVX counter via raw flow creation)
  {
  	u16 opcode = MLX5_GET(general_obj_in_cmd_hdr, in, opcode);
  
diff --cc drivers/infiniband/hw/mlx5/main.c
index 72ce00f38f1d,5236169c42d0..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3592,6 -3657,224 +3592,227 @@@ free_ucmd
  	return ERR_PTR(err);
  }
  
++<<<<<<< HEAD
++=======
+ static struct mlx5_ib_flow_prio *
+ _get_flow_table(struct mlx5_ib_dev *dev,
+ 		struct mlx5_ib_flow_matcher *fs_matcher,
+ 		bool mcast)
+ {
+ 	struct mlx5_flow_namespace *ns = NULL;
+ 	struct mlx5_ib_flow_prio *prio;
+ 	int max_table_size;
+ 	u32 flags = 0;
+ 	int priority;
+ 
+ 	if (fs_matcher->ns_type == MLX5_FLOW_NAMESPACE_BYPASS) {
+ 		max_table_size = BIT(MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev,
+ 					log_max_ft_size));
+ 		if (MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev, decap))
+ 			flags |= MLX5_FLOW_TABLE_TUNNEL_EN_DECAP;
+ 		if (MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev,
+ 					      reformat_l3_tunnel_to_l2))
+ 			flags |= MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT;
+ 	} else { /* Can only be MLX5_FLOW_NAMESPACE_EGRESS */
+ 		max_table_size = BIT(MLX5_CAP_FLOWTABLE_NIC_TX(dev->mdev,
+ 					log_max_ft_size));
+ 		if (MLX5_CAP_FLOWTABLE_NIC_TX(dev->mdev, reformat))
+ 			flags |= MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT;
+ 	}
+ 
+ 	if (max_table_size < MLX5_FS_MAX_ENTRIES)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (mcast)
+ 		priority = MLX5_IB_FLOW_MCAST_PRIO;
+ 	else
+ 		priority = ib_prio_to_core_prio(fs_matcher->priority, false);
+ 
+ 	ns = mlx5_get_flow_namespace(dev->mdev, fs_matcher->ns_type);
+ 	if (!ns)
+ 		return ERR_PTR(-ENOTSUPP);
+ 
+ 	if (fs_matcher->ns_type == MLX5_FLOW_NAMESPACE_BYPASS)
+ 		prio = &dev->flow_db->prios[priority];
+ 	else
+ 		prio = &dev->flow_db->egress_prios[priority];
+ 
+ 	if (prio->flow_table)
+ 		return prio;
+ 
+ 	return _get_prio(ns, prio, priority, MLX5_FS_MAX_ENTRIES,
+ 			 MLX5_FS_MAX_TYPES, flags);
+ }
+ 
+ static struct mlx5_ib_flow_handler *
+ _create_raw_flow_rule(struct mlx5_ib_dev *dev,
+ 		      struct mlx5_ib_flow_prio *ft_prio,
+ 		      struct mlx5_flow_destination *dst,
+ 		      struct mlx5_ib_flow_matcher  *fs_matcher,
+ 		      struct mlx5_flow_act *flow_act,
+ 		      void *cmd_in, int inlen,
+ 		      int dst_num)
+ {
+ 	struct mlx5_ib_flow_handler *handler;
+ 	struct mlx5_flow_spec *spec;
+ 	struct mlx5_flow_table *ft = ft_prio->flow_table;
+ 	int err = 0;
+ 
+ 	spec = kvzalloc(sizeof(*spec), GFP_KERNEL);
+ 	handler = kzalloc(sizeof(*handler), GFP_KERNEL);
+ 	if (!handler || !spec) {
+ 		err = -ENOMEM;
+ 		goto free;
+ 	}
+ 
+ 	INIT_LIST_HEAD(&handler->list);
+ 
+ 	memcpy(spec->match_value, cmd_in, inlen);
+ 	memcpy(spec->match_criteria, fs_matcher->matcher_mask.match_params,
+ 	       fs_matcher->mask_len);
+ 	spec->match_criteria_enable = fs_matcher->match_criteria_enable;
+ 
+ 	handler->rule = mlx5_add_flow_rules(ft, spec,
+ 					    flow_act, dst, dst_num);
+ 
+ 	if (IS_ERR(handler->rule)) {
+ 		err = PTR_ERR(handler->rule);
+ 		goto free;
+ 	}
+ 
+ 	ft_prio->refcount++;
+ 	handler->prio = ft_prio;
+ 	handler->dev = dev;
+ 	ft_prio->flow_table = ft;
+ 
+ free:
+ 	if (err)
+ 		kfree(handler);
+ 	kvfree(spec);
+ 	return err ? ERR_PTR(err) : handler;
+ }
+ 
+ static bool raw_fs_is_multicast(struct mlx5_ib_flow_matcher *fs_matcher,
+ 				void *match_v)
+ {
+ 	void *match_c;
+ 	void *match_v_set_lyr_2_4, *match_c_set_lyr_2_4;
+ 	void *dmac, *dmac_mask;
+ 	void *ipv4, *ipv4_mask;
+ 
+ 	if (!(fs_matcher->match_criteria_enable &
+ 	      (1 << MATCH_CRITERIA_ENABLE_OUTER_BIT)))
+ 		return false;
+ 
+ 	match_c = fs_matcher->matcher_mask.match_params;
+ 	match_v_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_v,
+ 					   outer_headers);
+ 	match_c_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_c,
+ 					   outer_headers);
+ 
+ 	dmac = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dmac_47_16);
+ 	dmac_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dmac_47_16);
+ 
+ 	if (is_multicast_ether_addr(dmac) &&
+ 	    is_multicast_ether_addr(dmac_mask))
+ 		return true;
+ 
+ 	ipv4 = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	ipv4_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	if (ipv4_is_multicast(*(__be32 *)(ipv4)) &&
+ 	    ipv4_is_multicast(*(__be32 *)(ipv4_mask)))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ struct mlx5_ib_flow_handler *
+ mlx5_ib_raw_fs_rule_add(struct mlx5_ib_dev *dev,
+ 			struct mlx5_ib_flow_matcher *fs_matcher,
+ 			struct mlx5_flow_act *flow_act,
+ 			u32 counter_id,
+ 			void *cmd_in, int inlen, int dest_id,
+ 			int dest_type)
+ {
+ 	struct mlx5_flow_destination *dst;
+ 	struct mlx5_ib_flow_prio *ft_prio;
+ 	struct mlx5_ib_flow_handler *handler;
+ 	int dst_num = 0;
+ 	bool mcast;
+ 	int err;
+ 
+ 	if (fs_matcher->flow_type != MLX5_IB_FLOW_TYPE_NORMAL)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	if (fs_matcher->priority > MLX5_IB_FLOW_LAST_PRIO)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	dst = kzalloc(sizeof(*dst) * 2, GFP_KERNEL);
+ 	if (!dst)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mcast = raw_fs_is_multicast(fs_matcher, cmd_in);
+ 	mutex_lock(&dev->flow_db->lock);
+ 
+ 	ft_prio = _get_flow_table(dev, fs_matcher, mcast);
+ 	if (IS_ERR(ft_prio)) {
+ 		err = PTR_ERR(ft_prio);
+ 		goto unlock;
+ 	}
+ 
+ 	if (dest_type == MLX5_FLOW_DESTINATION_TYPE_TIR) {
+ 		dst[dst_num].type = dest_type;
+ 		dst[dst_num].tir_num = dest_id;
+ 		flow_act->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	} else if (dest_type == MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE) {
+ 		dst[dst_num].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE_NUM;
+ 		dst[dst_num].ft_num = dest_id;
+ 		flow_act->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	} else {
+ 		dst[dst_num].type = MLX5_FLOW_DESTINATION_TYPE_PORT;
+ 		flow_act->action |= MLX5_FLOW_CONTEXT_ACTION_ALLOW;
+ 	}
+ 
+ 	dst_num++;
+ 
+ 	if (flow_act->action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {
+ 		dst[dst_num].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;
+ 		dst[dst_num].counter_id = counter_id;
+ 		dst_num++;
+ 	}
+ 
+ 	handler = _create_raw_flow_rule(dev, ft_prio, dst, fs_matcher, flow_act,
+ 					cmd_in, inlen, dst_num);
+ 
+ 	if (IS_ERR(handler)) {
+ 		err = PTR_ERR(handler);
+ 		goto destroy_ft;
+ 	}
+ 
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	atomic_inc(&fs_matcher->usecnt);
+ 	handler->flow_matcher = fs_matcher;
+ 
+ 	kfree(dst);
+ 
+ 	return handler;
+ 
+ destroy_ft:
+ 	put_flow_table(dev, ft_prio, false);
+ unlock:
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	kfree(dst);
+ 
+ 	return ERR_PTR(err);
+ }
+ 
++>>>>>>> bfc5d839184f (RDMA/mlx5: Attach a DEVX counter via raw flow creation)
  static u32 mlx5_ib_flow_action_flags_to_accel_xfrm_flags(u32 mlx5_flags)
  {
  	u32 flags = 0;
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index bc0aa5141161,a2b35a1a5031..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -1212,16 -1259,33 +1212,32 @@@ void mlx5_ib_put_native_port_mdev(struc
  				  u8 port_num);
  
  #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
++<<<<<<< HEAD
 +int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 +			struct mlx5_ib_ucontext *context);
 +void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 +			  struct mlx5_ib_ucontext *context);
++=======
+ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev);
+ void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
+ const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
+ extern const struct uapi_definition mlx5_ib_devx_defs[];
+ extern const struct uapi_definition mlx5_ib_flow_defs[];
+ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
+ 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+ 	struct mlx5_flow_act *flow_act, u32 counter_id,
+ 	void *cmd_in, int inlen, int dest_id, int dest_type);
+ bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
+ bool mlx5_ib_devx_is_flow_counter(void *obj, u32 *counter_id);
+ int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
+ void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
++>>>>>>> bfc5d839184f (RDMA/mlx5: Attach a DEVX counter via raw flow creation)
  #else
  static inline int
 -mlx5_ib_devx_create(struct mlx5_ib_dev *dev) { return -EOPNOTSUPP; };
 -static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
 -static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
 -					     int *dest_type)
 -{
 -	return false;
 -}
 -static inline void
 -mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction)
 -{
 -	return;
 -};
 +mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 +		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
 +static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 +					struct mlx5_ib_ucontext *context) {}
  #endif
  static inline void init_query_mad(struct ib_smp *mad)
  {
diff --cc include/uapi/rdma/mlx5_user_ioctl_cmds.h
index 8d285f4555cd,b8d121d457f1..000000000000
--- a/include/uapi/rdma/mlx5_user_ioctl_cmds.h
+++ b/include/uapi/rdma/mlx5_user_ioctl_cmds.h
@@@ -67,11 -75,117 +67,84 @@@ enum mlx5_ib_devx_obj_destroy_attrs 
  enum mlx5_ib_devx_obj_methods {
  	MLX5_IB_METHOD_DEVX_OBJ_CREATE = (1U << UVERBS_ID_NS_SHIFT),
  	MLX5_IB_METHOD_DEVX_OBJ_DESTROY,
 -	MLX5_IB_METHOD_DEVX_OBJ_MODIFY,
 -	MLX5_IB_METHOD_DEVX_OBJ_QUERY,
 -};
 -
 -enum mlx5_ib_devx_umem_reg_attrs {
 -	MLX5_IB_ATTR_DEVX_UMEM_REG_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
 -	MLX5_IB_ATTR_DEVX_UMEM_REG_ADDR,
 -	MLX5_IB_ATTR_DEVX_UMEM_REG_LEN,
 -	MLX5_IB_ATTR_DEVX_UMEM_REG_ACCESS,
 -	MLX5_IB_ATTR_DEVX_UMEM_REG_OUT_ID,
 -};
 -
 -enum mlx5_ib_devx_umem_dereg_attrs {
 -	MLX5_IB_ATTR_DEVX_UMEM_DEREG_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
 -};
 -
 -enum mlx5_ib_devx_umem_methods {
 -	MLX5_IB_METHOD_DEVX_UMEM_REG = (1U << UVERBS_ID_NS_SHIFT),
 -	MLX5_IB_METHOD_DEVX_UMEM_DEREG,
  };
  
 -enum mlx5_ib_objects {
 +enum mlx5_ib_devx_objects {
  	MLX5_IB_OBJECT_DEVX = (1U << UVERBS_ID_NS_SHIFT),
  	MLX5_IB_OBJECT_DEVX_OBJ,
++<<<<<<< HEAD
++=======
+ 	MLX5_IB_OBJECT_DEVX_UMEM,
+ 	MLX5_IB_OBJECT_FLOW_MATCHER,
+ };
+ 
+ enum mlx5_ib_flow_matcher_create_attrs {
+ 	MLX5_IB_ATTR_FLOW_MATCHER_CREATE_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_ATTR_FLOW_MATCHER_MATCH_MASK,
+ 	MLX5_IB_ATTR_FLOW_MATCHER_FLOW_TYPE,
+ 	MLX5_IB_ATTR_FLOW_MATCHER_MATCH_CRITERIA,
+ 	MLX5_IB_ATTR_FLOW_MATCHER_FLOW_FLAGS,
+ };
+ 
+ enum mlx5_ib_flow_matcher_destroy_attrs {
+ 	MLX5_IB_ATTR_FLOW_MATCHER_DESTROY_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ };
+ 
+ enum mlx5_ib_flow_matcher_methods {
+ 	MLX5_IB_METHOD_FLOW_MATCHER_CREATE = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_METHOD_FLOW_MATCHER_DESTROY,
+ };
+ 
+ #define MLX5_IB_DW_MATCH_PARAM 0x80
+ 
+ struct mlx5_ib_match_params {
+ 	__u32	match_params[MLX5_IB_DW_MATCH_PARAM];
+ };
+ 
+ enum mlx5_ib_flow_type {
+ 	MLX5_IB_FLOW_TYPE_NORMAL,
+ 	MLX5_IB_FLOW_TYPE_SNIFFER,
+ 	MLX5_IB_FLOW_TYPE_ALL_DEFAULT,
+ 	MLX5_IB_FLOW_TYPE_MC_DEFAULT,
+ };
+ 
+ enum mlx5_ib_create_flow_attrs {
+ 	MLX5_IB_ATTR_CREATE_FLOW_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_ATTR_CREATE_FLOW_MATCH_VALUE,
+ 	MLX5_IB_ATTR_CREATE_FLOW_DEST_QP,
+ 	MLX5_IB_ATTR_CREATE_FLOW_DEST_DEVX,
+ 	MLX5_IB_ATTR_CREATE_FLOW_MATCHER,
+ 	MLX5_IB_ATTR_CREATE_FLOW_ARR_FLOW_ACTIONS,
+ 	MLX5_IB_ATTR_CREATE_FLOW_TAG,
+ 	MLX5_IB_ATTR_CREATE_FLOW_ARR_COUNTERS_DEVX,
+ };
+ 
+ enum mlx5_ib_destoy_flow_attrs {
+ 	MLX5_IB_ATTR_DESTROY_FLOW_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ };
+ 
+ enum mlx5_ib_flow_methods {
+ 	MLX5_IB_METHOD_CREATE_FLOW = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_METHOD_DESTROY_FLOW,
+ };
+ 
+ enum mlx5_ib_flow_action_methods {
+ 	MLX5_IB_METHOD_FLOW_ACTION_CREATE_MODIFY_HEADER = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_METHOD_FLOW_ACTION_CREATE_PACKET_REFORMAT,
+ };
+ 
+ enum mlx5_ib_create_flow_action_create_modify_header_attrs {
+ 	MLX5_IB_ATTR_CREATE_MODIFY_HEADER_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_ATTR_CREATE_MODIFY_HEADER_ACTIONS_PRM,
+ 	MLX5_IB_ATTR_CREATE_MODIFY_HEADER_FT_TYPE,
+ };
+ 
+ enum mlx5_ib_create_flow_action_create_packet_reformat_attrs {
+ 	MLX5_IB_ATTR_CREATE_PACKET_REFORMAT_HANDLE = (1U << UVERBS_ID_NS_SHIFT),
+ 	MLX5_IB_ATTR_CREATE_PACKET_REFORMAT_TYPE,
+ 	MLX5_IB_ATTR_CREATE_PACKET_REFORMAT_FT_TYPE,
+ 	MLX5_IB_ATTR_CREATE_PACKET_REFORMAT_DATA_BUF,
++>>>>>>> bfc5d839184f (RDMA/mlx5: Attach a DEVX counter via raw flow creation)
  };
  
  #endif
* Unmerged path drivers/infiniband/hw/mlx5/flow.c
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
* Unmerged path drivers/infiniband/hw/mlx5/flow.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path include/uapi/rdma/mlx5_user_ioctl_cmds.h
