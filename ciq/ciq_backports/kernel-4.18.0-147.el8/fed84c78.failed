mm/memblock.c: skip kmemleak for kasan_init()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Qian Cai <cai@gmx.us>
commit fed84c78527009d4f799a3ed9a566502fa026d82
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/fed84c78.failed

Kmemleak does not play well with KASAN (tested on both HPE Apollo 70 and
Huawei TaiShan 2280 aarch64 servers).

After calling start_kernel()->setup_arch()->kasan_init(), kmemleak early
log buffer went from something like 280 to 260000 which caused kmemleak
disabled and crash dump memory reservation failed.  The multitude of
kmemleak_alloc() calls is from nested loops while KASAN is setting up full
memory mappings, so let early kmemleak allocations skip those
memblock_alloc_internal() calls came from kasan_init() given that those
early KASAN memory mappings should not reference to other memory.  Hence,
no kmemleak false positives.

kasan_init
  kasan_map_populate [1]
    kasan_pgd_populate [2]
      kasan_pud_populate [3]
        kasan_pmd_populate [4]
          kasan_pte_populate [5]
            kasan_alloc_zeroed_page
              memblock_alloc_try_nid
                memblock_alloc_internal
                  kmemleak_alloc

[1] for_each_memblock(memory, reg)
[2] while (pgdp++, addr = next, addr != end)
[3] while (pudp++, addr = next, addr != end && pud_none(READ_ONCE(*pudp)))
[4] while (pmdp++, addr = next, addr != end && pmd_none(READ_ONCE(*pmdp)))
[5] while (ptep++, addr = next, addr != end && pte_none(READ_ONCE(*ptep)))

Link: http://lkml.kernel.org/r/1543442925-17794-1-git-send-email-cai@gmx.us
	Signed-off-by: Qian Cai <cai@gmx.us>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fed84c78527009d4f799a3ed9a566502fa026d82)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memblock.h
diff --cc include/linux/memblock.h
index 8141bcfd8101,64c41cf45590..000000000000
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@@ -272,10 -316,117 +272,17 @@@ static inline int memblock_get_region_n
  }
  #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
  
++<<<<<<< HEAD
 +phys_addr_t memblock_alloc_nid(phys_addr_t size, phys_addr_t align, int nid);
 +phys_addr_t memblock_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid);
++=======
+ /* Flags for memblock allocation APIs */
+ #define MEMBLOCK_ALLOC_ANYWHERE	(~(phys_addr_t)0)
+ #define MEMBLOCK_ALLOC_ACCESSIBLE	0
+ #define MEMBLOCK_ALLOC_KASAN		1
++>>>>>>> fed84c785270 (mm/memblock.c: skip kmemleak for kasan_init())
  
 -/* We are using top down, so it is safe to use 0 here */
 -#define MEMBLOCK_LOW_LIMIT 0
 -
 -#ifndef ARCH_LOW_ADDRESS_LIMIT
 -#define ARCH_LOW_ADDRESS_LIMIT  0xffffffffUL
 -#endif
 -
 -phys_addr_t memblock_phys_alloc_nid(phys_addr_t size, phys_addr_t align, int nid);
 -phys_addr_t memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid);
 -
 -phys_addr_t memblock_phys_alloc(phys_addr_t size, phys_addr_t align);
 -
 -void *memblock_alloc_try_nid_raw(phys_addr_t size, phys_addr_t align,
 -				 phys_addr_t min_addr, phys_addr_t max_addr,
 -				 int nid);
 -void *memblock_alloc_try_nid_nopanic(phys_addr_t size, phys_addr_t align,
 -				     phys_addr_t min_addr, phys_addr_t max_addr,
 -				     int nid);
 -void *memblock_alloc_try_nid(phys_addr_t size, phys_addr_t align,
 -			     phys_addr_t min_addr, phys_addr_t max_addr,
 -			     int nid);
 -
 -static inline void * __init memblock_alloc(phys_addr_t size,  phys_addr_t align)
 -{
 -	return memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,
 -				      MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_raw(phys_addr_t size,
 -					       phys_addr_t align)
 -{
 -	return memblock_alloc_try_nid_raw(size, align, MEMBLOCK_LOW_LIMIT,
 -					  MEMBLOCK_ALLOC_ACCESSIBLE,
 -					  NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_from(phys_addr_t size,
 -						phys_addr_t align,
 -						phys_addr_t min_addr)
 -{
 -	return memblock_alloc_try_nid(size, align, min_addr,
 -				      MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_nopanic(phys_addr_t size,
 -						   phys_addr_t align)
 -{
 -	return memblock_alloc_try_nid_nopanic(size, align, MEMBLOCK_LOW_LIMIT,
 -					      MEMBLOCK_ALLOC_ACCESSIBLE,
 -					      NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_low(phys_addr_t size,
 -					       phys_addr_t align)
 -{
 -	return memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,
 -				      ARCH_LOW_ADDRESS_LIMIT, NUMA_NO_NODE);
 -}
 -static inline void * __init memblock_alloc_low_nopanic(phys_addr_t size,
 -						       phys_addr_t align)
 -{
 -	return memblock_alloc_try_nid_nopanic(size, align, MEMBLOCK_LOW_LIMIT,
 -					      ARCH_LOW_ADDRESS_LIMIT,
 -					      NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_from_nopanic(phys_addr_t size,
 -							phys_addr_t align,
 -							phys_addr_t min_addr)
 -{
 -	return memblock_alloc_try_nid_nopanic(size, align, min_addr,
 -					      MEMBLOCK_ALLOC_ACCESSIBLE,
 -					      NUMA_NO_NODE);
 -}
 -
 -static inline void * __init memblock_alloc_node(phys_addr_t size,
 -						phys_addr_t align, int nid)
 -{
 -	return memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,
 -				      MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 -}
 -
 -static inline void * __init memblock_alloc_node_nopanic(phys_addr_t size,
 -							int nid)
 -{
 -	return memblock_alloc_try_nid_nopanic(size, SMP_CACHE_BYTES,
 -					      MEMBLOCK_LOW_LIMIT,
 -					      MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 -}
 -
 -static inline void __init memblock_free_early(phys_addr_t base,
 -					      phys_addr_t size)
 -{
 -	memblock_free(base, size);
 -}
 -
 -static inline void __init memblock_free_early_nid(phys_addr_t base,
 -						  phys_addr_t size, int nid)
 -{
 -	memblock_free(base, size);
 -}
 -
 -static inline void __init memblock_free_late(phys_addr_t base, phys_addr_t size)
 -{
 -	__memblock_free_late(base, size);
 -}
 +phys_addr_t memblock_alloc(phys_addr_t size, phys_addr_t align);
  
  /*
   * Set the allocation direction to bottom-up or top-down.
diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 12145874c02b..af7fdd95f378 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -40,7 +40,7 @@ static phys_addr_t __init kasan_alloc_zeroed_page(int node)
 {
 	void *p = memblock_virt_alloc_try_nid(PAGE_SIZE, PAGE_SIZE,
 					      __pa(MAX_DMA_ADDRESS),
-					      MEMBLOCK_ALLOC_ACCESSIBLE, node);
+					      MEMBLOCK_ALLOC_KASAN, node);
 	return __pa(p);
 }
 
* Unmerged path include/linux/memblock.h
diff --git a/mm/memblock.c b/mm/memblock.c
index 20f1e27ed998..9cbd36525728 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -198,7 +198,8 @@ phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,
 	phys_addr_t kernel_end, ret;
 
 	/* pump up @end */
-	if (end == MEMBLOCK_ALLOC_ACCESSIBLE)
+	if (end == MEMBLOCK_ALLOC_ACCESSIBLE ||
+	    end == MEMBLOCK_ALLOC_KASAN)
 		end = memblock.current_limit;
 
 	/* avoid allocating the first page */
@@ -1323,13 +1324,15 @@ static void * __init memblock_virt_alloc_internal(
 done:
 	ptr = phys_to_virt(alloc);
 
-	/*
-	 * The min_count is set to 0 so that bootmem allocated blocks
-	 * are never reported as leaks. This is because many of these blocks
-	 * are only referred via the physical address which is not
-	 * looked up by kmemleak.
-	 */
-	kmemleak_alloc(ptr, size, 0, 0);
+	/* Skip kmemleak for kasan_init() due to high volume. */
+	if (max_addr != MEMBLOCK_ALLOC_KASAN)
+		/*
+		 * The min_count is set to 0 so that bootmem allocated
+		 * blocks are never reported as leaks. This is because many
+		 * of these blocks are only referred via the physical
+		 * address which is not looked up by kmemleak.
+		 */
+		kmemleak_alloc(ptr, size, 0, 0);
 
 	return ptr;
 }
