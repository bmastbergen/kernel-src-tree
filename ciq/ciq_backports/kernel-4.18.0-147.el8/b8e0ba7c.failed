KVM: arm64: Add support for creating PUD hugepages at stage 2

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Punit Agrawal <punit.agrawal@arm.com>
commit b8e0ba7c8bea994011aff3b4c35256b180fab874
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/b8e0ba7c.failed

KVM only supports PMD hugepages at stage 2. Now that the various page
handling routines are updated, extend the stage 2 fault handling to
map in PUD hugepages.

Addition of PUD hugepage support enables additional page sizes (e.g.,
1G with 4K granule) which can be useful on cores that support mapping
larger block sizes in the TLB entries.

	Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
	Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Will Deacon <will.deacon@arm.com>
[ Replace BUG() => WARN_ON(1) for arm32 PUD helpers ]
	Signed-off-by: Suzuki Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
(cherry picked from commit b8e0ba7c8bea994011aff3b4c35256b180fab874)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/kvm_mmu.h
#	arch/arm/include/asm/stage2_pgtable.h
#	arch/arm64/include/asm/kvm_mmu.h
#	arch/arm64/include/asm/pgtable-hwdef.h
#	virt/kvm/arm/mmu.c
diff --cc arch/arm/include/asm/kvm_mmu.h
index aa04390ce4a5,3a875fc1b63c..000000000000
--- a/arch/arm/include/asm/kvm_mmu.h
+++ b/arch/arm/include/asm/kvm_mmu.h
@@@ -82,6 -82,67 +82,70 @@@ void kvm_clear_hyp_idmap(void)
  #define kvm_mk_pud(pmdp)	__pud(__pa(pmdp) | PMD_TYPE_TABLE)
  #define kvm_mk_pgd(pudp)	({ BUILD_BUG(); 0; })
  
++<<<<<<< HEAD
++=======
+ #define kvm_pfn_pte(pfn, prot)	pfn_pte(pfn, prot)
+ #define kvm_pfn_pmd(pfn, prot)	pfn_pmd(pfn, prot)
+ #define kvm_pfn_pud(pfn, prot)	(__pud(0))
+ 
+ #define kvm_pud_pfn(pud)	({ WARN_ON(1); 0; })
+ 
+ 
+ #define kvm_pmd_mkhuge(pmd)	pmd_mkhuge(pmd)
+ /* No support for pud hugepages */
+ #define kvm_pud_mkhuge(pud)	( {WARN_ON(1); pud; })
+ 
+ /*
+  * The following kvm_*pud*() functions are provided strictly to allow
+  * sharing code with arm64. They should never be called in practice.
+  */
+ static inline void kvm_set_s2pud_readonly(pud_t *pud)
+ {
+ 	WARN_ON(1);
+ }
+ 
+ static inline bool kvm_s2pud_readonly(pud_t *pud)
+ {
+ 	WARN_ON(1);
+ 	return false;
+ }
+ 
+ static inline void kvm_set_pud(pud_t *pud, pud_t new_pud)
+ {
+ 	WARN_ON(1);
+ }
+ 
+ static inline pud_t kvm_s2pud_mkwrite(pud_t pud)
+ {
+ 	WARN_ON(1);
+ 	return pud;
+ }
+ 
+ static inline pud_t kvm_s2pud_mkexec(pud_t pud)
+ {
+ 	WARN_ON(1);
+ 	return pud;
+ }
+ 
+ static inline bool kvm_s2pud_exec(pud_t *pud)
+ {
+ 	WARN_ON(1);
+ 	return false;
+ }
+ 
+ static inline pud_t kvm_s2pud_mkyoung(pud_t pud)
+ {
+ 	BUG();
+ 	return pud;
+ }
+ 
+ static inline bool kvm_s2pud_young(pud_t pud)
+ {
+ 	WARN_ON(1);
+ 	return false;
+ }
+ 
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
  {
  	pte_val(pte) |= L_PTE_S2_RDWR;
diff --cc arch/arm/include/asm/stage2_pgtable.h
index d8e39a8f3148,f9017167a8d1..000000000000
--- a/arch/arm/include/asm/stage2_pgtable.h
+++ b/arch/arm/include/asm/stage2_pgtable.h
@@@ -68,7 -68,9 +68,14 @@@ stage2_pmd_addr_end(struct kvm *kvm, ph
  #define stage2_pmd_table_empty(kvm, pmdp)	kvm_page_empty(pmdp)
  #define stage2_pud_table_empty(kvm, pudp)	false
  
++<<<<<<< HEAD
 +#define S2_PMD_MASK				PMD_MASK
 +#define S2_PMD_SIZE				PMD_SIZE
++=======
+ static inline bool kvm_stage2_has_pud(struct kvm *kvm)
+ {
+ 	return false;
+ }
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  
  #endif	/* __ARM_S2_PGTABLE_H_ */
diff --cc arch/arm64/include/asm/kvm_mmu.h
index 94acbfa0650c,8af4b1befa42..000000000000
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@@ -184,6 -184,17 +184,20 @@@ void kvm_clear_hyp_idmap(void)
  #define kvm_mk_pgd(pudp)					\
  	__pgd(__phys_to_pgd_val(__pa(pudp)) | PUD_TYPE_TABLE)
  
++<<<<<<< HEAD
++=======
+ #define kvm_set_pud(pudp, pud)		set_pud(pudp, pud)
+ 
+ #define kvm_pfn_pte(pfn, prot)		pfn_pte(pfn, prot)
+ #define kvm_pfn_pmd(pfn, prot)		pfn_pmd(pfn, prot)
+ #define kvm_pfn_pud(pfn, prot)		pfn_pud(pfn, prot)
+ 
+ #define kvm_pud_pfn(pud)		pud_pfn(pud)
+ 
+ #define kvm_pmd_mkhuge(pmd)		pmd_mkhuge(pmd)
+ #define kvm_pud_mkhuge(pud)		pud_mkhuge(pud)
+ 
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
  {
  	pte_val(pte) |= PTE_S2_RDWR;
diff --cc arch/arm64/include/asm/pgtable-hwdef.h
index 54a37660b8c9,6f1c187f1c86..000000000000
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@@ -193,6 -193,10 +193,13 @@@
  #define PMD_S2_RDWR		(_AT(pmdval_t, 3) << 6)   /* HAP[2:1] */
  #define PMD_S2_XN		(_AT(pmdval_t, 2) << 53)  /* XN[1:0] */
  
++<<<<<<< HEAD
++=======
+ #define PUD_S2_RDONLY		(_AT(pudval_t, 1) << 6)   /* HAP[2:1] */
+ #define PUD_S2_RDWR		(_AT(pudval_t, 3) << 6)   /* HAP[2:1] */
+ #define PUD_S2_XN		(_AT(pudval_t, 2) << 53)  /* XN[1:0] */
+ 
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  /*
   * Memory Attribute override for Stage-2 (MemAttr[3:0])
   */
diff --cc virt/kvm/arm/mmu.c
index 22e88cffe24c,2dcff38868d4..000000000000
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@@ -1079,8 -1102,46 +1098,49 @@@ static int stage2_set_pmd_huge(struct k
  	return 0;
  }
  
++<<<<<<< HEAD
 +static bool stage2_is_exec(struct kvm *kvm, phys_addr_t addr)
++=======
+ static int stage2_set_pud_huge(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
+ 			       phys_addr_t addr, const pud_t *new_pudp)
+ {
+ 	pud_t *pudp, old_pud;
+ 
+ 	pudp = stage2_get_pud(kvm, cache, addr);
+ 	VM_BUG_ON(!pudp);
+ 
+ 	old_pud = *pudp;
+ 
+ 	/*
+ 	 * A large number of vcpus faulting on the same stage 2 entry,
+ 	 * can lead to a refault due to the
+ 	 * stage2_pud_clear()/tlb_flush(). Skip updating the page
+ 	 * tables if there is no change.
+ 	 */
+ 	if (pud_val(old_pud) == pud_val(*new_pudp))
+ 		return 0;
+ 
+ 	if (stage2_pud_present(kvm, old_pud)) {
+ 		stage2_pud_clear(kvm, pudp);
+ 		kvm_tlb_flush_vmid_ipa(kvm, addr);
+ 	} else {
+ 		get_page(virt_to_page(pudp));
+ 	}
+ 
+ 	kvm_set_pud(pudp, *new_pudp);
+ 	return 0;
+ }
+ 
+ /*
+  * stage2_get_leaf_entry - walk the stage2 VM page tables and return
+  * true if a valid and present leaf-entry is found. A pointer to the
+  * leaf-entry is returned in the appropriate level variable - pudpp,
+  * pmdpp, ptepp.
+  */
+ static bool stage2_get_leaf_entry(struct kvm *kvm, phys_addr_t addr,
+ 				  pud_t **pudpp, pmd_t **pmdpp, pte_t **ptepp)
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  {
 -	pud_t *pudp;
  	pmd_t *pmdp;
  	pte_t *ptep;
  
@@@ -1563,9 -1630,35 +1648,41 @@@ static int user_mem_abort(struct kvm_vc
  		return -EFAULT;
  	}
  
++<<<<<<< HEAD
 +	if (vma_kernel_pagesize(vma) == PMD_SIZE && !force_pte) {
 +		hugetlb = true;
 +		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
++=======
+ 	vma_pagesize = vma_kernel_pagesize(vma);
+ 	/*
+ 	 * PUD level may not exist for a VM but PMD is guaranteed to
+ 	 * exist.
+ 	 */
+ 	if ((vma_pagesize == PMD_SIZE ||
+ 	     (vma_pagesize == PUD_SIZE && kvm_stage2_has_pud(kvm))) &&
+ 	    !logging_active) {
+ 		gfn = (fault_ipa & huge_page_mask(hstate_vma(vma))) >> PAGE_SHIFT;
+ 	} else {
+ 		/*
+ 		 * Fallback to PTE if it's not one of the Stage 2
+ 		 * supported hugepage sizes or the corresponding level
+ 		 * doesn't exist
+ 		 */
+ 		vma_pagesize = PAGE_SIZE;
+ 
+ 		/*
+ 		 * Pages belonging to memslots that don't have the same
+ 		 * alignment for userspace and IPA cannot be mapped using
+ 		 * block descriptors even if the pages belong to a THP for
+ 		 * the process, because the stage-2 block descriptor will
+ 		 * cover more than a single THP and we loose atomicity for
+ 		 * unmapping, updates, and splits of the THP or other pages
+ 		 * in the stage-2 block range.
+ 		 */
+ 		if ((memslot->userspace_addr & ~PMD_MASK) !=
+ 		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))
+ 			force_pte = true;
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  	}
  	up_read(&current->mm->mmap_sem);
  
@@@ -1618,28 -1712,57 +1735,68 @@@
  	if (mmu_notifier_retry(kvm, mmu_seq))
  		goto out_unlock;
  
 -	if (vma_pagesize == PAGE_SIZE && !force_pte) {
 -		/*
 -		 * Only PMD_SIZE transparent hugepages(THP) are
 -		 * currently supported. This code will need to be
 -		 * updated to support other THP sizes.
 -		 */
 -		if (transparent_hugepage_adjust(&pfn, &fault_ipa))
 -			vma_pagesize = PMD_SIZE;
 -	}
 +	if (!hugetlb && !force_pte)
 +		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);
  
++<<<<<<< HEAD
 +	if (hugetlb) {
 +		pmd_t new_pmd = pfn_pmd(pfn, mem_type);
 +		new_pmd = pmd_mkhuge(new_pmd);
 +		if (writable) {
++=======
+ 	if (writable)
+ 		kvm_set_pfn_dirty(pfn);
+ 
+ 	if (fault_status != FSC_PERM)
+ 		clean_dcache_guest_page(pfn, vma_pagesize);
+ 
+ 	if (exec_fault)
+ 		invalidate_icache_guest_page(pfn, vma_pagesize);
+ 
+ 	/*
+ 	 * If we took an execution fault we have made the
+ 	 * icache/dcache coherent above and should now let the s2
+ 	 * mapping be executable.
+ 	 *
+ 	 * Write faults (!exec_fault && FSC_PERM) are orthogonal to
+ 	 * execute permissions, and we preserve whatever we have.
+ 	 */
+ 	needs_exec = exec_fault ||
+ 		(fault_status == FSC_PERM && stage2_is_exec(kvm, fault_ipa));
+ 
+ 	if (vma_pagesize == PUD_SIZE) {
+ 		pud_t new_pud = kvm_pfn_pud(pfn, mem_type);
+ 
+ 		new_pud = kvm_pud_mkhuge(new_pud);
+ 		if (writable)
+ 			new_pud = kvm_s2pud_mkwrite(new_pud);
+ 
+ 		if (needs_exec)
+ 			new_pud = kvm_s2pud_mkexec(new_pud);
+ 
+ 		ret = stage2_set_pud_huge(kvm, memcache, fault_ipa, &new_pud);
+ 	} else if (vma_pagesize == PMD_SIZE) {
+ 		pmd_t new_pmd = kvm_pfn_pmd(pfn, mem_type);
+ 
+ 		new_pmd = kvm_pmd_mkhuge(new_pmd);
+ 
+ 		if (writable)
++>>>>>>> b8e0ba7c8bea (KVM: arm64: Add support for creating PUD hugepages at stage 2)
  			new_pmd = kvm_s2pmd_mkwrite(new_pmd);
 +			kvm_set_pfn_dirty(pfn);
 +		}
  
 -		if (needs_exec)
 +		if (fault_status != FSC_PERM)
 +			clean_dcache_guest_page(pfn, PMD_SIZE);
 +
 +		if (exec_fault) {
  			new_pmd = kvm_s2pmd_mkexec(new_pmd);
 +			invalidate_icache_guest_page(pfn, PMD_SIZE);
 +		} else if (fault_status == FSC_PERM) {
 +			/* Preserve execute if XN was already cleared */
 +			if (stage2_is_exec(kvm, fault_ipa))
 +				new_pmd = kvm_s2pmd_mkexec(new_pmd);
 +		}
  
  		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
  	} else {
* Unmerged path arch/arm/include/asm/kvm_mmu.h
* Unmerged path arch/arm/include/asm/stage2_pgtable.h
* Unmerged path arch/arm64/include/asm/kvm_mmu.h
* Unmerged path arch/arm64/include/asm/pgtable-hwdef.h
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7ab3f4580bbc..ced0280d8985 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -383,6 +383,8 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define pud_write(pud)		pte_write(pud_pte(pud))
 
+#define pud_mkhuge(pud)		(__pud(pud_val(pud) & ~PUD_TABLE_BIT))
+
 #define __pud_to_phys(pud)	__pte_to_phys(pud_pte(pud))
 #define __phys_to_pud_val(phys)	__phys_to_pte_val(phys)
 #define pud_pfn(pud)		((__pud_to_phys(pud) & PUD_MASK) >> PAGE_SHIFT)
* Unmerged path virt/kvm/arm/mmu.c
