bpf: helper to pop data from messages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author John Fastabend <john.fastabend@gmail.com>
commit 7246d8ed4dcce23f7509949a77be15fa9f0e3d28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/7246d8ed.failed

This adds a BPF SK_MSG program helper so that we can pop data from a
msg. We use this to pop metadata from a previous push data call.

	Signed-off-by: John Fastabend <john.fastabend@gmail.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 7246d8ed4dcce23f7509949a77be15fa9f0e3d28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	net/core/filter.c
#	net/ipv4/tcp_bpf.c
#	net/tls/tls_sw.c
diff --cc include/uapi/linux/bpf.h
index 11269f3807ea,597afdbc1ab9..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -2118,6 -2168,119 +2118,122 @@@ union bpf_attr 
   *		the shared data.
   *	Return
   *		Pointer to the local storage area.
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_sk_select_reuseport(struct sk_reuseport_md *reuse, struct bpf_map *map, void *key, u64 flags)
+  *	Description
+  *		Select a SO_REUSEPORT sk from a	BPF_MAP_TYPE_REUSEPORT_ARRAY map
+  *		It checks the selected sk is matching the incoming
+  *		request in the skb.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * struct bpf_sock *bpf_sk_lookup_tcp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u32 netns, u64 flags)
+  *	Description
+  *		Look for TCP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is zero, then the socket lookup table in the
+  *		netns associated with the *ctx* will be used. For the TC hooks,
+  *		this in the netns of the device in the skb. For socket hooks,
+  *		this in the netns of the socket. If *netns* is non-zero, then
+  *		it specifies the ID of the netns relative to the netns
+  *		associated with the *ctx*.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * struct bpf_sock *bpf_sk_lookup_udp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u32 netns, u64 flags)
+  *	Description
+  *		Look for UDP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is zero, then the socket lookup table in the
+  *		netns associated with the *ctx* will be used. For the TC hooks,
+  *		this in the netns of the device in the skb. For socket hooks,
+  *		this in the netns of the socket. If *netns* is non-zero, then
+  *		it specifies the ID of the netns relative to the netns
+  *		associated with the *ctx*.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * int bpf_sk_release(struct bpf_sock *sk)
+  *	Description
+  *		Release the reference held by *sock*. *sock* must be a non-NULL
+  *		pointer that was returned from bpf_sk_lookup_xxx\ ().
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_msg_push_data(struct sk_buff *skb, u32 start, u32 len, u64 flags)
+  *	Description
+  *		For socket policies, insert *len* bytes into msg at offset
+  *		*start*.
+  *
+  *		If a program of type **BPF_PROG_TYPE_SK_MSG** is run on a
+  *		*msg* it may want to insert metadata or options into the msg.
+  *		This can later be read and used by any of the lower layer BPF
+  *		hooks.
+  *
+  *		This helper may fail if under memory pressure (a malloc
+  *		fails) in these cases BPF programs will get an appropriate
+  *		error and BPF programs will need to handle them.
+  *
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_msg_pop_data(struct sk_msg_buff *msg, u32 start, u32 pop, u64 flags)
+  *	 Description
+  *		Will remove *pop* bytes from a *msg* starting at byte *start*.
+  *		This may result in **ENOMEM** errors under certain situations if
+  *		an allocation and copy are required due to a full ring buffer.
+  *		However, the helper will try to avoid doing the allocation
+  *		if possible. Other errors can occur if input parameters are
+  *		invalid either due to *start* byte not being valid part of msg
+  *		payload and/or *pop* value being to large.
+  *
+  *	Return
+  *		0 on success, or a negative erro in case of failure.
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2201,7 -2364,17 +2317,21 @@@
  	FN(rc_keydown),			\
  	FN(skb_cgroup_id),		\
  	FN(get_current_cgroup_id),	\
++<<<<<<< HEAD
 +	FN(get_local_storage),
++=======
+ 	FN(get_local_storage),		\
+ 	FN(sk_select_reuseport),	\
+ 	FN(skb_ancestor_cgroup_id),	\
+ 	FN(sk_lookup_tcp),		\
+ 	FN(sk_lookup_udp),		\
+ 	FN(sk_release),			\
+ 	FN(map_push_elem),		\
+ 	FN(map_pop_elem),		\
+ 	FN(map_peek_elem),		\
+ 	FN(msg_push_data),		\
+ 	FN(msg_pop_data),
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc net/core/filter.c
index ed8de8b22015,bd0df75dc7b6..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -2407,6 -2294,305 +2407,308 @@@ static const struct bpf_func_proto bpf_
  	.arg4_type	= ARG_ANYTHING,
  };
  
++<<<<<<< HEAD
++=======
+ BPF_CALL_4(bpf_msg_push_data, struct sk_msg *, msg, u32, start,
+ 	   u32, len, u64, flags)
+ {
+ 	struct scatterlist sge, nsge, nnsge, rsge = {0}, *psge;
+ 	u32 new, i = 0, l, space, copy = 0, offset = 0;
+ 	u8 *raw, *to, *from;
+ 	struct page *page;
+ 
+ 	if (unlikely(flags))
+ 		return -EINVAL;
+ 
+ 	/* First find the starting scatterlist element */
+ 	i = msg->sg.start;
+ 	do {
+ 		l = sk_msg_elem(msg, i)->length;
+ 
+ 		if (start < offset + l)
+ 			break;
+ 		offset += l;
+ 		sk_msg_iter_var_next(i);
+ 	} while (i != msg->sg.end);
+ 
+ 	if (start >= offset + l)
+ 		return -EINVAL;
+ 
+ 	space = MAX_MSG_FRAGS - sk_msg_elem_used(msg);
+ 
+ 	/* If no space available will fallback to copy, we need at
+ 	 * least one scatterlist elem available to push data into
+ 	 * when start aligns to the beginning of an element or two
+ 	 * when it falls inside an element. We handle the start equals
+ 	 * offset case because its the common case for inserting a
+ 	 * header.
+ 	 */
+ 	if (!space || (space == 1 && start != offset))
+ 		copy = msg->sg.data[i].length;
+ 
+ 	page = alloc_pages(__GFP_NOWARN | GFP_ATOMIC | __GFP_COMP,
+ 			   get_order(copy + len));
+ 	if (unlikely(!page))
+ 		return -ENOMEM;
+ 
+ 	if (copy) {
+ 		int front, back;
+ 
+ 		raw = page_address(page);
+ 
+ 		psge = sk_msg_elem(msg, i);
+ 		front = start - offset;
+ 		back = psge->length - front;
+ 		from = sg_virt(psge);
+ 
+ 		if (front)
+ 			memcpy(raw, from, front);
+ 
+ 		if (back) {
+ 			from += front;
+ 			to = raw + front + len;
+ 
+ 			memcpy(to, from, back);
+ 		}
+ 
+ 		put_page(sg_page(psge));
+ 	} else if (start - offset) {
+ 		psge = sk_msg_elem(msg, i);
+ 		rsge = sk_msg_elem_cpy(msg, i);
+ 
+ 		psge->length = start - offset;
+ 		rsge.length -= psge->length;
+ 		rsge.offset += start;
+ 
+ 		sk_msg_iter_var_next(i);
+ 		sg_unmark_end(psge);
+ 		sk_msg_iter_next(msg, end);
+ 	}
+ 
+ 	/* Slot(s) to place newly allocated data */
+ 	new = i;
+ 
+ 	/* Shift one or two slots as needed */
+ 	if (!copy) {
+ 		sge = sk_msg_elem_cpy(msg, i);
+ 
+ 		sk_msg_iter_var_next(i);
+ 		sg_unmark_end(&sge);
+ 		sk_msg_iter_next(msg, end);
+ 
+ 		nsge = sk_msg_elem_cpy(msg, i);
+ 		if (rsge.length) {
+ 			sk_msg_iter_var_next(i);
+ 			nnsge = sk_msg_elem_cpy(msg, i);
+ 		}
+ 
+ 		while (i != msg->sg.end) {
+ 			msg->sg.data[i] = sge;
+ 			sge = nsge;
+ 			sk_msg_iter_var_next(i);
+ 			if (rsge.length) {
+ 				nsge = nnsge;
+ 				nnsge = sk_msg_elem_cpy(msg, i);
+ 			} else {
+ 				nsge = sk_msg_elem_cpy(msg, i);
+ 			}
+ 		}
+ 	}
+ 
+ 	/* Place newly allocated data buffer */
+ 	sk_mem_charge(msg->sk, len);
+ 	msg->sg.size += len;
+ 	msg->sg.copy[new] = false;
+ 	sg_set_page(&msg->sg.data[new], page, len + copy, 0);
+ 	if (rsge.length) {
+ 		get_page(sg_page(&rsge));
+ 		sk_msg_iter_var_next(new);
+ 		msg->sg.data[new] = rsge;
+ 	}
+ 
+ 	sk_msg_compute_data_pointers(msg);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_msg_push_data_proto = {
+ 	.func		= bpf_msg_push_data,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
+ static void sk_msg_shift_left(struct sk_msg *msg, int i)
+ {
+ 	int prev;
+ 
+ 	do {
+ 		prev = i;
+ 		sk_msg_iter_var_next(i);
+ 		msg->sg.data[prev] = msg->sg.data[i];
+ 	} while (i != msg->sg.end);
+ 
+ 	sk_msg_iter_prev(msg, end);
+ }
+ 
+ static void sk_msg_shift_right(struct sk_msg *msg, int i)
+ {
+ 	struct scatterlist tmp, sge;
+ 
+ 	sk_msg_iter_next(msg, end);
+ 	sge = sk_msg_elem_cpy(msg, i);
+ 	sk_msg_iter_var_next(i);
+ 	tmp = sk_msg_elem_cpy(msg, i);
+ 
+ 	while (i != msg->sg.end) {
+ 		msg->sg.data[i] = sge;
+ 		sk_msg_iter_var_next(i);
+ 		sge = tmp;
+ 		tmp = sk_msg_elem_cpy(msg, i);
+ 	}
+ }
+ 
+ BPF_CALL_4(bpf_msg_pop_data, struct sk_msg *, msg, u32, start,
+ 	   u32, len, u64, flags)
+ {
+ 	u32 i = 0, l, space, offset = 0;
+ 	u64 last = start + len;
+ 	int pop;
+ 
+ 	if (unlikely(flags))
+ 		return -EINVAL;
+ 
+ 	/* First find the starting scatterlist element */
+ 	i = msg->sg.start;
+ 	do {
+ 		l = sk_msg_elem(msg, i)->length;
+ 
+ 		if (start < offset + l)
+ 			break;
+ 		offset += l;
+ 		sk_msg_iter_var_next(i);
+ 	} while (i != msg->sg.end);
+ 
+ 	/* Bounds checks: start and pop must be inside message */
+ 	if (start >= offset + l || last >= msg->sg.size)
+ 		return -EINVAL;
+ 
+ 	space = MAX_MSG_FRAGS - sk_msg_elem_used(msg);
+ 
+ 	pop = len;
+ 	/* --------------| offset
+ 	 * -| start      |-------- len -------|
+ 	 *
+ 	 *  |----- a ----|-------- pop -------|----- b ----|
+ 	 *  |______________________________________________| length
+ 	 *
+ 	 *
+ 	 * a:   region at front of scatter element to save
+ 	 * b:   region at back of scatter element to save when length > A + pop
+ 	 * pop: region to pop from element, same as input 'pop' here will be
+ 	 *      decremented below per iteration.
+ 	 *
+ 	 * Two top-level cases to handle when start != offset, first B is non
+ 	 * zero and second B is zero corresponding to when a pop includes more
+ 	 * than one element.
+ 	 *
+ 	 * Then if B is non-zero AND there is no space allocate space and
+ 	 * compact A, B regions into page. If there is space shift ring to
+ 	 * the rigth free'ing the next element in ring to place B, leaving
+ 	 * A untouched except to reduce length.
+ 	 */
+ 	if (start != offset) {
+ 		struct scatterlist *nsge, *sge = sk_msg_elem(msg, i);
+ 		int a = start;
+ 		int b = sge->length - pop - a;
+ 
+ 		sk_msg_iter_var_next(i);
+ 
+ 		if (pop < sge->length - a) {
+ 			if (space) {
+ 				sge->length = a;
+ 				sk_msg_shift_right(msg, i);
+ 				nsge = sk_msg_elem(msg, i);
+ 				get_page(sg_page(sge));
+ 				sg_set_page(nsge,
+ 					    sg_page(sge),
+ 					    b, sge->offset + pop + a);
+ 			} else {
+ 				struct page *page, *orig;
+ 				u8 *to, *from;
+ 
+ 				page = alloc_pages(__GFP_NOWARN |
+ 						   __GFP_COMP   | GFP_ATOMIC,
+ 						   get_order(a + b));
+ 				if (unlikely(!page))
+ 					return -ENOMEM;
+ 
+ 				sge->length = a;
+ 				orig = sg_page(sge);
+ 				from = sg_virt(sge);
+ 				to = page_address(page);
+ 				memcpy(to, from, a);
+ 				memcpy(to + a, from + a + pop, b);
+ 				sg_set_page(sge, page, a + b, 0);
+ 				put_page(orig);
+ 			}
+ 			pop = 0;
+ 		} else if (pop >= sge->length - a) {
+ 			sge->length = a;
+ 			pop -= (sge->length - a);
+ 		}
+ 	}
+ 
+ 	/* From above the current layout _must_ be as follows,
+ 	 *
+ 	 * -| offset
+ 	 * -| start
+ 	 *
+ 	 *  |---- pop ---|---------------- b ------------|
+ 	 *  |____________________________________________| length
+ 	 *
+ 	 * Offset and start of the current msg elem are equal because in the
+ 	 * previous case we handled offset != start and either consumed the
+ 	 * entire element and advanced to the next element OR pop == 0.
+ 	 *
+ 	 * Two cases to handle here are first pop is less than the length
+ 	 * leaving some remainder b above. Simply adjust the element's layout
+ 	 * in this case. Or pop >= length of the element so that b = 0. In this
+ 	 * case advance to next element decrementing pop.
+ 	 */
+ 	while (pop) {
+ 		struct scatterlist *sge = sk_msg_elem(msg, i);
+ 
+ 		if (pop < sge->length) {
+ 			sge->length -= pop;
+ 			sge->offset += pop;
+ 			pop = 0;
+ 		} else {
+ 			pop -= sge->length;
+ 			sk_msg_shift_left(msg, i);
+ 		}
+ 		sk_msg_iter_var_next(i);
+ 	}
+ 
+ 	sk_mem_uncharge(msg->sk, len - pop);
+ 	msg->sg.size -= (len - pop);
+ 	sk_msg_compute_data_pointers(msg);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_msg_pop_data_proto = {
+ 	.func		= bpf_msg_pop_data,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
  BPF_CALL_1(bpf_get_cgroup_classid, const struct sk_buff *, skb)
  {
  	return task_get_classid(skb);
@@@ -4776,6 -5265,8 +5078,11 @@@ bool bpf_helper_changes_pkt_data(void *
  	    func == bpf_xdp_adjust_head ||
  	    func == bpf_xdp_adjust_meta ||
  	    func == bpf_msg_pull_data ||
++<<<<<<< HEAD
++=======
+ 	    func == bpf_msg_push_data ||
+ 	    func == bpf_msg_pop_data ||
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
  	    func == bpf_xdp_adjust_tail ||
  #if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)
  	    func == bpf_lwt_seg6_store_bytes ||
@@@ -5031,8 -5561,10 +5338,15 @@@ sk_msg_func_proto(enum bpf_func_id func
  		return &bpf_msg_cork_bytes_proto;
  	case BPF_FUNC_msg_pull_data:
  		return &bpf_msg_pull_data_proto;
++<<<<<<< HEAD
 +	case BPF_FUNC_get_local_storage:
 +		return &bpf_get_local_storage_proto;
++=======
+ 	case BPF_FUNC_msg_push_data:
+ 		return &bpf_msg_push_data_proto;
+ 	case BPF_FUNC_msg_pop_data:
+ 		return &bpf_msg_pop_data_proto;
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
  	default:
  		return bpf_base_func_proto(func_id);
  	}
diff --cc net/tls/tls_sw.c
index 79112fc82343,d4ecc66464e6..000000000000
--- a/net/tls/tls_sw.c
+++ b/net/tls/tls_sw.c
@@@ -445,77 -676,109 +445,176 @@@ static int tls_push_record(struct sock 
  	return tls_tx_records(sk, flags);
  }
  
 -static int bpf_exec_tx_verdict(struct sk_msg *msg, struct sock *sk,
 -			       bool full_record, u8 record_type,
 -			       size_t *copied, int flags)
 +static int tls_sw_push_pending_record(struct sock *sk, int flags)
  {
++<<<<<<< HEAD
 +	return tls_push_record(sk, flags, TLS_RECORD_TYPE_DATA);
++=======
+ 	struct tls_context *tls_ctx = tls_get_ctx(sk);
+ 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
+ 	struct sk_msg msg_redir = { };
+ 	struct sk_psock *psock;
+ 	struct sock *sk_redir;
+ 	struct tls_rec *rec;
+ 	int err = 0, send;
+ 	u32 delta = 0;
+ 	bool enospc;
+ 
+ 	psock = sk_psock_get(sk);
+ 	if (!psock)
+ 		return tls_push_record(sk, flags, record_type);
+ more_data:
+ 	enospc = sk_msg_full(msg);
+ 	if (psock->eval == __SK_NONE) {
+ 		delta = msg->sg.size;
+ 		psock->eval = sk_psock_msg_verdict(sk, psock, msg);
+ 		if (delta < msg->sg.size)
+ 			delta -= msg->sg.size;
+ 		else
+ 			delta = 0;
+ 	}
+ 	if (msg->cork_bytes && msg->cork_bytes > msg->sg.size &&
+ 	    !enospc && !full_record) {
+ 		err = -ENOSPC;
+ 		goto out_err;
+ 	}
+ 	msg->cork_bytes = 0;
+ 	send = msg->sg.size;
+ 	if (msg->apply_bytes && msg->apply_bytes < send)
+ 		send = msg->apply_bytes;
+ 
+ 	switch (psock->eval) {
+ 	case __SK_PASS:
+ 		err = tls_push_record(sk, flags, record_type);
+ 		if (err < 0) {
+ 			*copied -= sk_msg_free(sk, msg);
+ 			tls_free_open_rec(sk);
+ 			goto out_err;
+ 		}
+ 		break;
+ 	case __SK_REDIRECT:
+ 		sk_redir = psock->sk_redir;
+ 		memcpy(&msg_redir, msg, sizeof(*msg));
+ 		if (msg->apply_bytes < send)
+ 			msg->apply_bytes = 0;
+ 		else
+ 			msg->apply_bytes -= send;
+ 		sk_msg_return_zero(sk, msg, send);
+ 		msg->sg.size -= send;
+ 		release_sock(sk);
+ 		err = tcp_bpf_sendmsg_redir(sk_redir, &msg_redir, send, flags);
+ 		lock_sock(sk);
+ 		if (err < 0) {
+ 			*copied -= sk_msg_free_nocharge(sk, &msg_redir);
+ 			msg->sg.size = 0;
+ 		}
+ 		if (msg->sg.size == 0)
+ 			tls_free_open_rec(sk);
+ 		break;
+ 	case __SK_DROP:
+ 	default:
+ 		sk_msg_free_partial(sk, msg, send);
+ 		if (msg->apply_bytes < send)
+ 			msg->apply_bytes = 0;
+ 		else
+ 			msg->apply_bytes -= send;
+ 		if (msg->sg.size == 0)
+ 			tls_free_open_rec(sk);
+ 		*copied -= (send + delta);
+ 		err = -EACCES;
+ 	}
+ 
+ 	if (likely(!err)) {
+ 		bool reset_eval = !ctx->open_rec;
+ 
+ 		rec = ctx->open_rec;
+ 		if (rec) {
+ 			msg = &rec->msg_plaintext;
+ 			if (!msg->apply_bytes)
+ 				reset_eval = true;
+ 		}
+ 		if (reset_eval) {
+ 			psock->eval = __SK_NONE;
+ 			if (psock->sk_redir) {
+ 				sock_put(psock->sk_redir);
+ 				psock->sk_redir = NULL;
+ 			}
+ 		}
+ 		if (rec)
+ 			goto more_data;
+ 	}
+  out_err:
+ 	sk_psock_put(sk, psock);
+ 	return err;
++>>>>>>> 7246d8ed4dcc (bpf: helper to pop data from messages)
  }
  
 -static int tls_sw_push_pending_record(struct sock *sk, int flags)
 +static int zerocopy_from_iter(struct sock *sk, struct iov_iter *from,
 +			      int length, int *pages_used,
 +			      unsigned int *size_used,
 +			      struct scatterlist *to, int to_max_pages,
 +			      bool charge)
 +{
 +	struct page *pages[MAX_SKB_FRAGS];
 +
 +	size_t offset;
 +	ssize_t copied, use;
 +	int i = 0;
 +	unsigned int size = *size_used;
 +	int num_elem = *pages_used;
 +	int rc = 0;
 +	int maxpages;
 +
 +	while (length > 0) {
 +		i = 0;
 +		maxpages = to_max_pages - num_elem;
 +		if (maxpages == 0) {
 +			rc = -EFAULT;
 +			goto out;
 +		}
 +		copied = iov_iter_get_pages(from, pages,
 +					    length,
 +					    maxpages, &offset);
 +		if (copied <= 0) {
 +			rc = -EFAULT;
 +			goto out;
 +		}
 +
 +		iov_iter_advance(from, copied);
 +
 +		length -= copied;
 +		size += copied;
 +		while (copied) {
 +			use = min_t(int, copied, PAGE_SIZE - offset);
 +
 +			sg_set_page(&to[num_elem],
 +				    pages[i], use, offset);
 +			sg_unmark_end(&to[num_elem]);
 +			if (charge)
 +				sk_mem_charge(sk, use);
 +
 +			offset = 0;
 +			copied -= use;
 +
 +			++i;
 +			++num_elem;
 +		}
 +	}
 +
 +	/* Mark the end in the last sg entry if newly added */
 +	if (num_elem > *pages_used)
 +		sg_mark_end(&to[num_elem - 1]);
 +out:
 +	if (rc)
 +		iov_iter_revert(from, size - *size_used);
 +	*size_used = size;
 +	*pages_used = num_elem;
 +
 +	return rc;
 +}
 +
 +static int memcopy_from_iter(struct sock *sk, struct iov_iter *from,
 +			     int bytes)
  {
  	struct tls_context *tls_ctx = tls_get_ctx(sk);
  	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
* Unmerged path net/ipv4/tcp_bpf.c
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path net/core/filter.c
* Unmerged path net/ipv4/tcp_bpf.c
* Unmerged path net/tls/tls_sw.c
