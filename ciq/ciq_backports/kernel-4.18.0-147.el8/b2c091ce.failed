mlxsw: spectrum_acl: Make mlxsw_sp_acl_tcam_vregion_rehash() return void

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jiri Pirko <jiri@mellanox.com>
commit b2c091ce46a7e08d776160d2f1b8ccabc85410b3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/b2c091ce.failed

The return value is ignored anyway, so just return void.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: Ido Schimmel <idosch@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b2c091ce46a7e08d776160d2f1b8ccabc85410b3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
index c607c62a43ae,8811f6513e36..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
@@@ -603,12 -733,73 +603,73 @@@ mlxsw_sp_acl_tcam_region_destroy(struc
  	kfree(region);
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ mlxsw_sp_acl_tcam_vregion_rehash_work_schedule(struct mlxsw_sp_acl_tcam_vregion *vregion)
+ {
+ 	unsigned long interval = vregion->tcam->vregion_rehash_intrvl;
+ 
+ 	if (!interval)
+ 		return;
+ 	mlxsw_core_schedule_dw(&vregion->rehash.dw,
+ 			       msecs_to_jiffies(interval));
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_vregion_rehash(struct mlxsw_sp *mlxsw_sp,
+ 				 struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				 int *credits);
+ 
+ static void mlxsw_sp_acl_tcam_vregion_rehash_work(struct work_struct *work)
+ {
+ 	struct mlxsw_sp_acl_tcam_vregion *vregion =
+ 		container_of(work, struct mlxsw_sp_acl_tcam_vregion,
+ 			     rehash.dw.work);
+ 	int credits = MLXSW_SP_ACL_TCAM_VREGION_REHASH_CREDITS;
+ 
+ 	mlxsw_sp_acl_tcam_vregion_rehash(vregion->mlxsw_sp, vregion, &credits);
+ 	if (credits < 0)
+ 		/* Rehash gone out of credits so it was interrupted.
+ 		 * Schedule the work as soon as possible to continue.
+ 		 */
+ 		mlxsw_core_schedule_dw(&vregion->rehash.dw, 0);
+ 	else
+ 		mlxsw_sp_acl_tcam_vregion_rehash_work_schedule(vregion);
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_rehash_ctx_vchunk_changed(struct mlxsw_sp_acl_tcam_vchunk *vchunk)
+ {
+ 	struct mlxsw_sp_acl_tcam_vregion *vregion = vchunk->vregion;
+ 
+ 	/* If a rule was added or deleted from vchunk which is currently
+ 	 * under rehash migration, we have to reset the ventry pointers
+ 	 * to make sure all rules are properly migrated.
+ 	 */
+ 	if (vregion->rehash.ctx.current_vchunk == vchunk) {
+ 		vregion->rehash.ctx.start_ventry = NULL;
+ 		vregion->rehash.ctx.stop_ventry = NULL;
+ 	}
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_rehash_ctx_vregion_changed(struct mlxsw_sp_acl_tcam_vregion *vregion)
+ {
+ 	/* If a chunk was added or deleted from vregion we have to reset
+ 	 * the current chunk pointer to make sure all chunks
+ 	 * are properly migrated.
+ 	 */
+ 	vregion->rehash.ctx.current_vchunk = NULL;
+ }
+ 
++>>>>>>> b2c091ce46a7 (mlxsw: spectrum_acl: Make mlxsw_sp_acl_tcam_vregion_rehash() return void)
  static struct mlxsw_sp_acl_tcam_vregion *
  mlxsw_sp_acl_tcam_vregion_create(struct mlxsw_sp *mlxsw_sp,
 -				 struct mlxsw_sp_acl_tcam_vgroup *vgroup,
 -				 unsigned int priority,
 +				 struct mlxsw_sp_acl_tcam *tcam,
  				 struct mlxsw_afk_element_usage *elusage)
  {
 -	const struct mlxsw_sp_acl_tcam_ops *ops = mlxsw_sp->acl_tcam_ops;
  	struct mlxsw_afk *afk = mlxsw_sp_acl_afk(mlxsw_sp->acl);
 -	struct mlxsw_sp_acl_tcam *tcam = vgroup->group.tcam;
  	struct mlxsw_sp_acl_tcam_vregion *vregion;
  	int err;
  
@@@ -958,6 -1224,299 +1019,302 @@@ mlxsw_sp_acl_tcam_ventry_activity_get(s
  						    ventry->entry, activity);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ mlxsw_sp_acl_tcam_ventry_migrate(struct mlxsw_sp *mlxsw_sp,
+ 				 struct mlxsw_sp_acl_tcam_ventry *ventry,
+ 				 struct mlxsw_sp_acl_tcam_chunk *chunk,
+ 				 int *credits)
+ {
+ 	struct mlxsw_sp_acl_tcam_entry *new_entry;
+ 
+ 	/* First check if the entry is not already where we want it to be. */
+ 	if (ventry->entry->chunk == chunk)
+ 		return 0;
+ 
+ 	if (--(*credits) < 0)
+ 		return 0;
+ 
+ 	new_entry = mlxsw_sp_acl_tcam_entry_create(mlxsw_sp, ventry, chunk);
+ 	if (IS_ERR(new_entry))
+ 		return PTR_ERR(new_entry);
+ 	mlxsw_sp_acl_tcam_entry_destroy(mlxsw_sp, ventry->entry);
+ 	ventry->entry = new_entry;
+ 	return 0;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vchunk_migrate_start(struct mlxsw_sp *mlxsw_sp,
+ 				       struct mlxsw_sp_acl_tcam_vchunk *vchunk,
+ 				       struct mlxsw_sp_acl_tcam_region *region,
+ 				       struct mlxsw_sp_acl_tcam_rehash_ctx *ctx)
+ {
+ 	struct mlxsw_sp_acl_tcam_chunk *new_chunk;
+ 
+ 	new_chunk = mlxsw_sp_acl_tcam_chunk_create(mlxsw_sp, vchunk, region);
+ 	if (IS_ERR(new_chunk)) {
+ 		if (ctx->this_is_rollback)
+ 			vchunk->vregion->failed_rollback = true;
+ 		return PTR_ERR(new_chunk);
+ 	}
+ 	vchunk->chunk2 = vchunk->chunk;
+ 	vchunk->chunk = new_chunk;
+ 	ctx->current_vchunk = vchunk;
+ 	ctx->start_ventry = NULL;
+ 	ctx->stop_ventry = NULL;
+ 	return 0;
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_vchunk_migrate_end(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vchunk *vchunk,
+ 				     struct mlxsw_sp_acl_tcam_rehash_ctx *ctx)
+ {
+ 	mlxsw_sp_acl_tcam_chunk_destroy(mlxsw_sp, vchunk->chunk2);
+ 	vchunk->chunk2 = NULL;
+ 	ctx->current_vchunk = NULL;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vchunk_migrate_one(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vchunk *vchunk,
+ 				     struct mlxsw_sp_acl_tcam_region *region,
+ 				     struct mlxsw_sp_acl_tcam_rehash_ctx *ctx,
+ 				     int *credits)
+ {
+ 	struct mlxsw_sp_acl_tcam_ventry *ventry;
+ 	int err;
+ 
+ 	if (vchunk->chunk->region != region) {
+ 		err = mlxsw_sp_acl_tcam_vchunk_migrate_start(mlxsw_sp, vchunk,
+ 							     region, ctx);
+ 		if (err)
+ 			return err;
+ 	} else if (!vchunk->chunk2) {
+ 		/* The chunk is already as it should be, nothing to do. */
+ 		return 0;
+ 	}
+ 
+ 	/* If the migration got interrupted, we have the ventry to start from
+ 	 * stored in context.
+ 	 */
+ 	if (ctx->start_ventry)
+ 		ventry = ctx->start_ventry;
+ 	else
+ 		ventry = list_first_entry(&vchunk->ventry_list,
+ 					  typeof(*ventry), list);
+ 
+ 	list_for_each_entry_from(ventry, &vchunk->ventry_list, list) {
+ 		/* During rollback, once we reach the ventry that failed
+ 		 * to migrate, we are done.
+ 		 */
+ 		if (ventry == ctx->stop_ventry)
+ 			break;
+ 
+ 		err = mlxsw_sp_acl_tcam_ventry_migrate(mlxsw_sp, ventry,
+ 						       vchunk->chunk, credits);
+ 		if (err) {
+ 			if (ctx->this_is_rollback)
+ 				return err;
+ 			/* Swap the chunk and chunk2 pointers so the follow-up
+ 			 * rollback call will see the original chunk pointer
+ 			 * in vchunk->chunk.
+ 			 */
+ 			swap(vchunk->chunk, vchunk->chunk2);
+ 			/* The rollback has to be done from beginning of the
+ 			 * chunk, that is why we have to null the start_ventry.
+ 			 * However, we know where to stop the rollback,
+ 			 * at the current ventry.
+ 			 */
+ 			ctx->start_ventry = NULL;
+ 			ctx->stop_ventry = ventry;
+ 			return err;
+ 		} else if (*credits < 0) {
+ 			/* We are out of credits, the rest of the ventries
+ 			 * will be migrated later. Save the ventry
+ 			 * which we ended with.
+ 			 */
+ 			ctx->start_ventry = ventry;
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	mlxsw_sp_acl_tcam_vchunk_migrate_end(mlxsw_sp, vchunk, ctx);
+ 	return 0;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vchunk_migrate_all(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				     struct mlxsw_sp_acl_tcam_rehash_ctx *ctx,
+ 				     int *credits)
+ {
+ 	struct mlxsw_sp_acl_tcam_vchunk *vchunk;
+ 	int err;
+ 
+ 	/* If the migration got interrupted, we have the vchunk
+ 	 * we are working on stored in context.
+ 	 */
+ 	if (ctx->current_vchunk)
+ 		vchunk = ctx->current_vchunk;
+ 	else
+ 		vchunk = list_first_entry(&vregion->vchunk_list,
+ 					  typeof(*vchunk), list);
+ 
+ 	list_for_each_entry_from(vchunk, &vregion->vchunk_list, list) {
+ 		err = mlxsw_sp_acl_tcam_vchunk_migrate_one(mlxsw_sp, vchunk,
+ 							   vregion->region,
+ 							   ctx, credits);
+ 		if (err || *credits < 0)
+ 			return err;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vregion_migrate(struct mlxsw_sp *mlxsw_sp,
+ 				  struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				  struct mlxsw_sp_acl_tcam_rehash_ctx *ctx,
+ 				  int *credits)
+ {
+ 	int err, err2;
+ 
+ 	trace_mlxsw_sp_acl_tcam_vregion_migrate(mlxsw_sp, vregion);
+ 	mutex_lock(&vregion->lock);
+ 	err = mlxsw_sp_acl_tcam_vchunk_migrate_all(mlxsw_sp, vregion,
+ 						   ctx, credits);
+ 	if (err) {
+ 		/* In case migration was not successful, we need to swap
+ 		 * so the original region pointer is assigned again
+ 		 * to vregion->region.
+ 		 */
+ 		swap(vregion->region, vregion->region2);
+ 		ctx->current_vchunk = NULL;
+ 		ctx->this_is_rollback = true;
+ 		err2 = mlxsw_sp_acl_tcam_vchunk_migrate_all(mlxsw_sp, vregion,
+ 							    ctx, credits);
+ 		if (err2)
+ 			vregion->failed_rollback = true;
+ 	}
+ 	mutex_unlock(&vregion->lock);
+ 	trace_mlxsw_sp_acl_tcam_vregion_migrate_end(mlxsw_sp, vregion);
+ 	return err;
+ }
+ 
+ static bool
+ mlxsw_sp_acl_tcam_vregion_rehash_in_progress(const struct mlxsw_sp_acl_tcam_rehash_ctx *ctx)
+ {
+ 	return ctx->hints_priv;
+ }
+ 
+ static int
+ mlxsw_sp_acl_tcam_vregion_rehash_start(struct mlxsw_sp *mlxsw_sp,
+ 				       struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				       struct mlxsw_sp_acl_tcam_rehash_ctx *ctx)
+ {
+ 	const struct mlxsw_sp_acl_tcam_ops *ops = mlxsw_sp->acl_tcam_ops;
+ 	unsigned int priority = mlxsw_sp_acl_tcam_vregion_prio(vregion);
+ 	struct mlxsw_sp_acl_tcam_region *new_region;
+ 	void *hints_priv;
+ 	int err;
+ 
+ 	trace_mlxsw_sp_acl_tcam_vregion_rehash(mlxsw_sp, vregion);
+ 	if (vregion->failed_rollback)
+ 		return -EBUSY;
+ 
+ 	hints_priv = ops->region_rehash_hints_get(vregion->region->priv);
+ 	if (IS_ERR(hints_priv))
+ 		return PTR_ERR(hints_priv);
+ 
+ 	new_region = mlxsw_sp_acl_tcam_region_create(mlxsw_sp, vregion->tcam,
+ 						     vregion, hints_priv);
+ 	if (IS_ERR(new_region)) {
+ 		err = PTR_ERR(new_region);
+ 		goto err_region_create;
+ 	}
+ 
+ 	/* vregion->region contains the pointer to the new region
+ 	 * we are going to migrate to.
+ 	 */
+ 	vregion->region2 = vregion->region;
+ 	vregion->region = new_region;
+ 	err = mlxsw_sp_acl_tcam_group_region_attach(mlxsw_sp,
+ 						    vregion->region2->group,
+ 						    new_region, priority,
+ 						    vregion->region2);
+ 	if (err)
+ 		goto err_group_region_attach;
+ 
+ 	ctx->hints_priv = hints_priv;
+ 	ctx->this_is_rollback = false;
+ 
+ 	return 0;
+ 
+ err_group_region_attach:
+ 	vregion->region = vregion->region2;
+ 	vregion->region2 = NULL;
+ 	mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, new_region);
+ err_region_create:
+ 	ops->region_rehash_hints_put(hints_priv);
+ 	return err;
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_vregion_rehash_end(struct mlxsw_sp *mlxsw_sp,
+ 				     struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				     struct mlxsw_sp_acl_tcam_rehash_ctx *ctx)
+ {
+ 	struct mlxsw_sp_acl_tcam_region *unused_region = vregion->region2;
+ 	const struct mlxsw_sp_acl_tcam_ops *ops = mlxsw_sp->acl_tcam_ops;
+ 
+ 	if (!vregion->failed_rollback) {
+ 		vregion->region2 = NULL;
+ 		mlxsw_sp_acl_tcam_group_region_detach(mlxsw_sp, unused_region);
+ 		mlxsw_sp_acl_tcam_region_destroy(mlxsw_sp, unused_region);
+ 	}
+ 	ops->region_rehash_hints_put(ctx->hints_priv);
+ 	ctx->hints_priv = NULL;
+ }
+ 
+ static void
+ mlxsw_sp_acl_tcam_vregion_rehash(struct mlxsw_sp *mlxsw_sp,
+ 				 struct mlxsw_sp_acl_tcam_vregion *vregion,
+ 				 int *credits)
+ {
+ 	struct mlxsw_sp_acl_tcam_rehash_ctx *ctx = &vregion->rehash.ctx;
+ 	int err;
+ 
+ 	/* Check if the previous rehash work was interrupted
+ 	 * which means we have to continue it now.
+ 	 * If not, start a new rehash.
+ 	 */
+ 	if (!mlxsw_sp_acl_tcam_vregion_rehash_in_progress(ctx)) {
+ 		err = mlxsw_sp_acl_tcam_vregion_rehash_start(mlxsw_sp,
+ 							     vregion, ctx);
+ 		if (err) {
+ 			if (err != -EAGAIN)
+ 				dev_err(mlxsw_sp->bus_info->dev, "Failed get rehash hints\n");
+ 			return;
+ 		}
+ 	}
+ 
+ 	err = mlxsw_sp_acl_tcam_vregion_migrate(mlxsw_sp, vregion,
+ 						ctx, credits);
+ 	if (err) {
+ 		dev_err(mlxsw_sp->bus_info->dev, "Failed to migrate vregion\n");
+ 		if (vregion->failed_rollback) {
+ 			trace_mlxsw_sp_acl_tcam_vregion_rehash_dis(mlxsw_sp,
+ 								   vregion);
+ 			dev_err(mlxsw_sp->bus_info->dev, "Failed to rollback during vregion migration fail\n");
+ 		}
+ 	}
+ 
+ 	if (*credits >= 0)
+ 		mlxsw_sp_acl_tcam_vregion_rehash_end(mlxsw_sp, vregion, ctx);
+ }
+ 
++>>>>>>> b2c091ce46a7 (mlxsw: spectrum_acl: Make mlxsw_sp_acl_tcam_vregion_rehash() return void)
  static const enum mlxsw_afk_element mlxsw_sp_acl_tcam_pattern_ipv4[] = {
  	MLXSW_AFK_ELEMENT_SRC_SYS_PORT,
  	MLXSW_AFK_ELEMENT_DMAC_32_47,
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_acl_tcam.c
