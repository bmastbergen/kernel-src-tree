net/mlx5: Make RoCE and SR-IOV LAG modes explicit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Aviv Heller <avivh@mellanox.com>
commit 7c34ec19e10c0d13ca2f3435fb85d2dddccad917
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/7c34ec19.failed

With the introduction of SR-IOV LAG, checking whether LAG is active
is no longer good enough, since RoCE and SR-IOV LAG each entails
different behavior by both the core and infiniband drivers.

This patch introduces facilities to discern LAG type, in addition to
mlx5_lag_is_active(). These are implemented in such a way as to allow
more complex mode combinations in the future.

	Signed-off-by: Aviv Heller <avivh@mellanox.com>
	Reviewed-by: Roi Dayan <roid@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 7c34ec19e10c0d13ca2f3435fb85d2dddccad917)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index 12a75511c956,e85974ab06c0..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -5843,12 -6173,14 +5846,21 @@@ static int mlx5_ib_stage_populate_specs
  
  int mlx5_ib_stage_ib_reg_init(struct mlx5_ib_dev *dev)
  {
 -	const char *name;
 +	return ib_register_device(&dev->ib_dev, NULL);
 +}
  
++<<<<<<< HEAD
 +static void mlx5_ib_stage_depopulate_specs(struct mlx5_ib_dev *dev)
 +{
 +	depopulate_specs_root(dev);
++=======
+ 	rdma_set_device_sysfs_group(&dev->ib_dev, &mlx5_attr_group);
+ 	if (!mlx5_lag_is_roce(dev->mdev))
+ 		name = "mlx5_%d";
+ 	else
+ 		name = "mlx5_bond_%d";
+ 	return ib_register_device(&dev->ib_dev, name, NULL);
++>>>>>>> 7c34ec19e10c (net/mlx5: Make RoCE and SR-IOV LAG modes explicit)
  }
  
  void mlx5_ib_stage_pre_ib_reg_umr_cleanup(struct mlx5_ib_dev *dev)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index 58cddf0a96a7,e4a34c9ef700..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -211,7 -308,20 +211,18 @@@ int mlx5e_attr_get(struct net_device *d
  	switch (attr->id) {
  	case SWITCHDEV_ATTR_ID_PORT_PARENT_ID:
  		attr->u.ppid.id_len = ETH_ALEN;
++<<<<<<< HEAD
 +		ether_addr_copy(attr->u.ppid.id, rep->hw_id);
++=======
+ 		if (uplink_upper && mlx5_lag_is_sriov(uplink_priv->mdev)) {
+ 			ether_addr_copy(attr->u.ppid.id, uplink_upper->dev_addr);
+ 		} else {
+ 			struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 			struct mlx5_eswitch_rep *rep = rpriv->rep;
+ 
+ 			ether_addr_copy(attr->u.ppid.id, rep->hw_id);
+ 		}
++>>>>>>> 7c34ec19e10c (net/mlx5: Make RoCE and SR-IOV LAG modes explicit)
  		break;
  	default:
  		return -EOPNOTSUPP;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index c9ee89f4edb1,53ebb5a48018..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -2766,31 -2708,29 +2766,53 @@@ static struct rhashtable *get_tc_ht(str
  		return &priv->fs.tc.ht;
  }
  
++<<<<<<< HEAD
 +int mlx5e_configure_flower(struct mlx5e_priv *priv,
 +			   struct tc_cls_flower_offload *f, int flags)
++=======
+ static bool is_peer_flow_needed(struct mlx5e_tc_flow *flow)
+ {
+ 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+ 	bool is_rep_ingress = attr->in_rep->vport != FDB_UPLINK_VPORT &&
+ 			      flow->flags & MLX5E_TC_FLOW_INGRESS;
+ 	bool act_is_encap = !!(attr->action &
+ 			       MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT);
+ 	bool esw_paired = mlx5_devcom_is_paired(attr->in_mdev->priv.devcom,
+ 						MLX5_DEVCOM_ESW_OFFLOADS);
+ 
+ 	return esw_paired && mlx5_lag_is_sriov(attr->in_mdev) &&
+ 	       (is_rep_ingress || act_is_encap);
+ }
+ 
+ static int
+ mlx5e_alloc_flow(struct mlx5e_priv *priv, int attr_size,
+ 		 struct tc_cls_flower_offload *f, u16 flow_flags,
+ 		 struct mlx5e_tc_flow_parse_attr **__parse_attr,
+ 		 struct mlx5e_tc_flow **__flow)
++>>>>>>> 7c34ec19e10c (net/mlx5: Make RoCE and SR-IOV LAG modes explicit)
  {
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct mlx5e_tc_flow_parse_attr *parse_attr;
 +	struct rhashtable *tc_ht = get_tc_ht(priv);
  	struct mlx5e_tc_flow *flow;
 -	int err;
 +	int attr_size, err = 0;
 +	u8 flow_flags = 0;
 +
 +	get_flags(flags, &flow_flags);
 +
 +	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
 +	if (flow) {
 +		netdev_warn_once(priv->netdev, "flow cookie %lx already exists, ignoring\n", f->cookie);
 +		return 0;
 +	}
 +
 +	if (esw && esw->mode == SRIOV_OFFLOADS) {
 +		flow_flags |= MLX5E_TC_FLOW_ESWITCH;
 +		attr_size  = sizeof(struct mlx5_esw_flow_attr);
 +	} else {
 +		flow_flags |= MLX5E_TC_FLOW_NIC;
 +		attr_size  = sizeof(struct mlx5_nic_flow_attr);
 +	}
  
  	flow = kzalloc(sizeof(*flow) + attr_size, GFP_KERNEL);
  	parse_attr = kvzalloc(sizeof(*parse_attr), GFP_KERNEL);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2c8fea140d3d..e23a6e19fb1b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -886,6 +886,7 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
 	struct mlx5_eswitch_rep		*rep;
+	int				lag_active;
 
 	/* protect the user_td */
 	struct mutex		lb_mutex;
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index d3d354de1de8..79bfccac7dc4 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -3260,7 +3260,7 @@ static int __mlx5_ib_modify_qp(struct ib_qp *ibqp,
 		    (ibqp->qp_type == IB_QPT_RAW_PACKET) ||
 		    (ibqp->qp_type == IB_QPT_XRC_INI) ||
 		    (ibqp->qp_type == IB_QPT_XRC_TGT)) {
-			if (mlx5_lag_is_active(dev->mdev)) {
+			if (dev->lag_active) {
 				u8 p = mlx5_core_native_port_num(dev->mdev);
 				tx_affinity = get_tx_affinity(dev, pd, base, p);
 				context->flags |= cpu_to_be32(tx_affinity << 24);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/lag.c b/drivers/net/ethernet/mellanox/mlx5/core/lag.c
index db5ef7023371..feb8230d3f86 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/lag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lag.c
@@ -37,9 +37,12 @@
 #include "eswitch.h"
 
 enum {
-	MLX5_LAG_FLAG_BONDED = 1 << 0,
+	MLX5_LAG_FLAG_ROCE   = 1 << 0,
+	MLX5_LAG_FLAG_SRIOV  = 1 << 1,
 };
 
+#define MLX5_LAG_MODE_FLAGS (MLX5_LAG_FLAG_ROCE | MLX5_LAG_FLAG_SRIOV)
+
 struct lag_func {
 	struct mlx5_core_dev *dev;
 	struct net_device    *netdev;
@@ -161,9 +164,19 @@ static int mlx5_lag_dev_get_netdev_idx(struct mlx5_lag *ldev,
 	return -1;
 }
 
+static bool __mlx5_lag_is_roce(struct mlx5_lag *ldev)
+{
+	return !!(ldev->flags & MLX5_LAG_FLAG_ROCE);
+}
+
+static bool __mlx5_lag_is_sriov(struct mlx5_lag *ldev)
+{
+	return !!(ldev->flags & MLX5_LAG_FLAG_SRIOV);
+}
+
 static bool __mlx5_lag_is_active(struct mlx5_lag *ldev)
 {
-	return !!(ldev->flags & MLX5_LAG_FLAG_BONDED);
+	return !!(ldev->flags & MLX5_LAG_MODE_FLAGS);
 }
 
 static void mlx5_infer_tx_affinity_mapping(struct lag_tracker *tracker,
@@ -229,9 +242,10 @@ static int mlx5_create_lag(struct mlx5_lag *ldev,
 }
 
 static void mlx5_activate_lag(struct mlx5_lag *ldev,
-			      struct lag_tracker *tracker)
+			      struct lag_tracker *tracker,
+			      u8 flags)
 {
-	ldev->flags |= MLX5_LAG_FLAG_BONDED;
+	ldev->flags |= flags;
 	mlx5_create_lag(ldev, tracker);
 }
 
@@ -240,7 +254,7 @@ static void mlx5_deactivate_lag(struct mlx5_lag *ldev)
 	struct mlx5_core_dev *dev0 = ldev->pf[0].dev;
 	int err;
 
-	ldev->flags &= ~MLX5_LAG_FLAG_BONDED;
+	ldev->flags &= ~MLX5_LAG_MODE_FLAGS;
 
 	err = mlx5_cmd_destroy_lag(dev0);
 	if (err)
@@ -263,15 +277,13 @@ static void mlx5_do_bond(struct mlx5_lag *ldev)
 {
 	struct mlx5_core_dev *dev0 = ldev->pf[0].dev;
 	struct mlx5_core_dev *dev1 = ldev->pf[1].dev;
-	bool do_bond, sriov_enabled;
 	struct lag_tracker tracker;
+	bool do_bond, roce_lag;
 	int i;
 
 	if (!dev0 || !dev1)
 		return;
 
-	sriov_enabled = mlx5_sriov_is_enabled(dev0) || mlx5_sriov_is_enabled(dev1);
-
 	mutex_lock(&lag_mutex);
 	tracker = ldev->tracker;
 	mutex_unlock(&lag_mutex);
@@ -279,28 +291,35 @@ static void mlx5_do_bond(struct mlx5_lag *ldev)
 	do_bond = tracker.is_bonded && mlx5_lag_check_prereq(ldev);
 
 	if (do_bond && !__mlx5_lag_is_active(ldev)) {
-		if (!sriov_enabled)
+		roce_lag = !mlx5_sriov_is_enabled(dev0) &&
+			   !mlx5_sriov_is_enabled(dev1);
+
+		if (roce_lag)
 			for (i = 0; i < MLX5_MAX_PORTS; i++)
 				mlx5_remove_dev_by_protocol(ldev->pf[i].dev,
 							    MLX5_INTERFACE_PROTOCOL_IB);
 
-		mlx5_activate_lag(ldev, &tracker);
+		mlx5_activate_lag(ldev, &tracker,
+				  roce_lag ? MLX5_LAG_FLAG_ROCE :
+				  MLX5_LAG_FLAG_SRIOV);
 
-		if (!sriov_enabled) {
+		if (roce_lag) {
 			mlx5_add_dev_by_protocol(dev0, MLX5_INTERFACE_PROTOCOL_IB);
 			mlx5_nic_vport_enable_roce(dev1);
 		}
 	} else if (do_bond && __mlx5_lag_is_active(ldev)) {
 		mlx5_modify_lag(ldev, &tracker);
 	} else if (!do_bond && __mlx5_lag_is_active(ldev)) {
-		if (!sriov_enabled) {
+		roce_lag = __mlx5_lag_is_roce(ldev);
+
+		if (roce_lag) {
 			mlx5_remove_dev_by_protocol(dev0, MLX5_INTERFACE_PROTOCOL_IB);
 			mlx5_nic_vport_disable_roce(dev1);
 		}
 
 		mlx5_deactivate_lag(ldev);
 
-		if (!sriov_enabled)
+		if (roce_lag)
 			for (i = 0; i < MLX5_MAX_PORTS; i++)
 				if (ldev->pf[i].dev)
 					mlx5_add_dev_by_protocol(ldev->pf[i].dev,
@@ -572,6 +591,20 @@ void mlx5_lag_remove(struct mlx5_core_dev *dev)
 	}
 }
 
+bool mlx5_lag_is_roce(struct mlx5_core_dev *dev)
+{
+	struct mlx5_lag *ldev;
+	bool res;
+
+	mutex_lock(&lag_mutex);
+	ldev = mlx5_lag_dev_get(dev);
+	res  = ldev && __mlx5_lag_is_roce(ldev);
+	mutex_unlock(&lag_mutex);
+
+	return res;
+}
+EXPORT_SYMBOL(mlx5_lag_is_roce);
+
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev)
 {
 	struct mlx5_lag *ldev;
@@ -586,6 +619,20 @@ bool mlx5_lag_is_active(struct mlx5_core_dev *dev)
 }
 EXPORT_SYMBOL(mlx5_lag_is_active);
 
+bool mlx5_lag_is_sriov(struct mlx5_core_dev *dev)
+{
+	struct mlx5_lag *ldev;
+	bool res;
+
+	mutex_lock(&lag_mutex);
+	ldev = mlx5_lag_dev_get(dev);
+	res  = ldev && __mlx5_lag_is_sriov(ldev);
+	mutex_unlock(&lag_mutex);
+
+	return res;
+}
+EXPORT_SYMBOL(mlx5_lag_is_sriov);
+
 void mlx5_lag_update(struct mlx5_core_dev *dev)
 {
 	struct mlx5_lag *ldev;
@@ -609,7 +656,7 @@ struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)
 	mutex_lock(&lag_mutex);
 	ldev = mlx5_lag_dev_get(dev);
 
-	if (!(ldev && __mlx5_lag_is_active(ldev)))
+	if (!(ldev && __mlx5_lag_is_roce(ldev)))
 		goto unlock;
 
 	if (ldev->tracker.tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP) {
@@ -638,7 +685,7 @@ bool mlx5_lag_intf_add(struct mlx5_interface *intf, struct mlx5_priv *priv)
 		return true;
 
 	ldev = mlx5_lag_dev_get(dev);
-	if (!ldev || !__mlx5_lag_is_active(ldev) || ldev->pf[0].dev == dev)
+	if (!ldev || !__mlx5_lag_is_roce(ldev) || ldev->pf[0].dev == dev)
 		return true;
 
 	/* If bonded, we do not add an IB device for PF1. */
@@ -665,7 +712,7 @@ int mlx5_lag_query_cong_counters(struct mlx5_core_dev *dev,
 
 	mutex_lock(&lag_mutex);
 	ldev = mlx5_lag_dev_get(dev);
-	if (ldev && __mlx5_lag_is_active(ldev)) {
+	if (ldev && __mlx5_lag_is_roce(ldev)) {
 		num_ports = MLX5_MAX_PORTS;
 		mdev[0] = ldev->pf[0].dev;
 		mdev[1] = ldev->pf[1].dev;
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 90e47adfa1f1..8166ca0066bf 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -1207,6 +1207,8 @@ int mlx5_core_query_vendor_id(struct mlx5_core_dev *mdev, u32 *vendor_id);
 
 int mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev);
 int mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev);
+bool mlx5_lag_is_roce(struct mlx5_core_dev *dev);
+bool mlx5_lag_is_sriov(struct mlx5_core_dev *dev);
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev);
 struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev);
 int mlx5_lag_query_cong_counters(struct mlx5_core_dev *dev,
