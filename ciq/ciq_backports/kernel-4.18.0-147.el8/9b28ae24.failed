bpf: fix out-of-bounds read in __bpf_skc_lookup

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Lorenz Bauer <lmb@cloudflare.com>
commit 9b28ae243ef3b13d8a88b5451d025475c75ebdef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/9b28ae24.failed

__bpf_skc_lookup takes a socket tuple and the length of the
tuple as an argument. Based on the length, it decides which
address family to pass to the helper function sk_lookup.

In case of AF_INET6, it fails to verify that the length
of the tuple is long enough. sk_lookup may therefore access
data past the end of the tuple.

Fixes: 6acc9b432e67 ("bpf: Add helper to retrieve socket in BPF")
	Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 9b28ae243ef3b13d8a88b5451d025475c75ebdef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/filter.c
diff --cc net/core/filter.c
index 07df3c4617a6,76f1d99640e2..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -4767,6 -5195,573 +4767,576 @@@ static const struct bpf_func_proto bpf_
  };
  #endif /* CONFIG_IPV6_SEG6_BPF */
  
++<<<<<<< HEAD
++=======
+ #define CONVERT_COMMON_TCP_SOCK_FIELDS(md_type, CONVERT)		\
+ do {									\
+ 	switch (si->off) {						\
+ 	case offsetof(md_type, snd_cwnd):				\
+ 		CONVERT(snd_cwnd); break;				\
+ 	case offsetof(md_type, srtt_us):				\
+ 		CONVERT(srtt_us); break;				\
+ 	case offsetof(md_type, snd_ssthresh):				\
+ 		CONVERT(snd_ssthresh); break;				\
+ 	case offsetof(md_type, rcv_nxt):				\
+ 		CONVERT(rcv_nxt); break;				\
+ 	case offsetof(md_type, snd_nxt):				\
+ 		CONVERT(snd_nxt); break;				\
+ 	case offsetof(md_type, snd_una):				\
+ 		CONVERT(snd_una); break;				\
+ 	case offsetof(md_type, mss_cache):				\
+ 		CONVERT(mss_cache); break;				\
+ 	case offsetof(md_type, ecn_flags):				\
+ 		CONVERT(ecn_flags); break;				\
+ 	case offsetof(md_type, rate_delivered):				\
+ 		CONVERT(rate_delivered); break;				\
+ 	case offsetof(md_type, rate_interval_us):			\
+ 		CONVERT(rate_interval_us); break;			\
+ 	case offsetof(md_type, packets_out):				\
+ 		CONVERT(packets_out); break;				\
+ 	case offsetof(md_type, retrans_out):				\
+ 		CONVERT(retrans_out); break;				\
+ 	case offsetof(md_type, total_retrans):				\
+ 		CONVERT(total_retrans); break;				\
+ 	case offsetof(md_type, segs_in):				\
+ 		CONVERT(segs_in); break;				\
+ 	case offsetof(md_type, data_segs_in):				\
+ 		CONVERT(data_segs_in); break;				\
+ 	case offsetof(md_type, segs_out):				\
+ 		CONVERT(segs_out); break;				\
+ 	case offsetof(md_type, data_segs_out):				\
+ 		CONVERT(data_segs_out); break;				\
+ 	case offsetof(md_type, lost_out):				\
+ 		CONVERT(lost_out); break;				\
+ 	case offsetof(md_type, sacked_out):				\
+ 		CONVERT(sacked_out); break;				\
+ 	case offsetof(md_type, bytes_received):				\
+ 		CONVERT(bytes_received); break;				\
+ 	case offsetof(md_type, bytes_acked):				\
+ 		CONVERT(bytes_acked); break;				\
+ 	}								\
+ } while (0)
+ 
+ #ifdef CONFIG_INET
+ static struct sock *sk_lookup(struct net *net, struct bpf_sock_tuple *tuple,
+ 			      int dif, int sdif, u8 family, u8 proto)
+ {
+ 	bool refcounted = false;
+ 	struct sock *sk = NULL;
+ 
+ 	if (family == AF_INET) {
+ 		__be32 src4 = tuple->ipv4.saddr;
+ 		__be32 dst4 = tuple->ipv4.daddr;
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					   src4, tuple->ipv4.sport,
+ 					   dst4, tuple->ipv4.dport,
+ 					   dif, sdif, &refcounted);
+ 		else
+ 			sk = __udp4_lib_lookup(net, src4, tuple->ipv4.sport,
+ 					       dst4, tuple->ipv4.dport,
+ 					       dif, sdif, &udp_table, NULL);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	} else {
+ 		struct in6_addr *src6 = (struct in6_addr *)&tuple->ipv6.saddr;
+ 		struct in6_addr *dst6 = (struct in6_addr *)&tuple->ipv6.daddr;
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet6_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					    src6, tuple->ipv6.sport,
+ 					    dst6, ntohs(tuple->ipv6.dport),
+ 					    dif, sdif, &refcounted);
+ 		else if (likely(ipv6_bpf_stub))
+ 			sk = ipv6_bpf_stub->udp6_lib_lookup(net,
+ 							    src6, tuple->ipv6.sport,
+ 							    dst6, tuple->ipv6.dport,
+ 							    dif, sdif,
+ 							    &udp_table, NULL);
+ #endif
+ 	}
+ 
+ 	if (unlikely(sk && !refcounted && !sock_flag(sk, SOCK_RCU_FREE))) {
+ 		WARN_ONCE(1, "Found non-RCU, unreferenced socket!");
+ 		sk = NULL;
+ 	}
+ 	return sk;
+ }
+ 
+ /* bpf_skc_lookup performs the core lookup for different types of sockets,
+  * taking a reference on the socket if it doesn't have the flag SOCK_RCU_FREE.
+  * Returns the socket as an 'unsigned long' to simplify the casting in the
+  * callers to satisfy BPF_CALL declarations.
+  */
+ static struct sock *
+ __bpf_skc_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 		 struct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,
+ 		 u64 flags)
+ {
+ 	struct sock *sk = NULL;
+ 	u8 family = AF_UNSPEC;
+ 	struct net *net;
+ 	int sdif;
+ 
+ 	if (len == sizeof(tuple->ipv4))
+ 		family = AF_INET;
+ 	else if (len == sizeof(tuple->ipv6))
+ 		family = AF_INET6;
+ 	else
+ 		return NULL;
+ 
+ 	if (unlikely(family == AF_UNSPEC || flags ||
+ 		     !((s32)netns_id < 0 || netns_id <= S32_MAX)))
+ 		goto out;
+ 
+ 	if (family == AF_INET)
+ 		sdif = inet_sdif(skb);
+ 	else
+ 		sdif = inet6_sdif(skb);
+ 
+ 	if ((s32)netns_id < 0) {
+ 		net = caller_net;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 	} else {
+ 		net = get_net_ns_by_id(caller_net, netns_id);
+ 		if (unlikely(!net))
+ 			goto out;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 		put_net(net);
+ 	}
+ 
+ out:
+ 	return sk;
+ }
+ 
+ static struct sock *
+ __bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 		struct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,
+ 		u64 flags)
+ {
+ 	struct sock *sk = __bpf_skc_lookup(skb, tuple, len, caller_net,
+ 					   ifindex, proto, netns_id, flags);
+ 
+ 	if (sk)
+ 		sk = sk_to_full_sk(sk);
+ 
+ 	return sk;
+ }
+ 
+ static struct sock *
+ bpf_skc_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 	       u8 proto, u64 netns_id, u64 flags)
+ {
+ 	struct net *caller_net;
+ 	int ifindex;
+ 
+ 	if (skb->dev) {
+ 		caller_net = dev_net(skb->dev);
+ 		ifindex = skb->dev->ifindex;
+ 	} else {
+ 		caller_net = sock_net(skb->sk);
+ 		ifindex = 0;
+ 	}
+ 
+ 	return __bpf_skc_lookup(skb, tuple, len, caller_net, ifindex, proto,
+ 				netns_id, flags);
+ }
+ 
+ static struct sock *
+ bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 	      u8 proto, u64 netns_id, u64 flags)
+ {
+ 	struct sock *sk = bpf_skc_lookup(skb, tuple, len, proto, netns_id,
+ 					 flags);
+ 
+ 	if (sk)
+ 		sk = sk_to_full_sk(sk);
+ 
+ 	return sk;
+ }
+ 
+ BPF_CALL_5(bpf_skc_lookup_tcp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)bpf_skc_lookup(skb, tuple, len, IPPROTO_TCP,
+ 					     netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_skc_lookup_tcp_proto = {
+ 	.func		= bpf_skc_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCK_COMMON_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sk_lookup_tcp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)bpf_sk_lookup(skb, tuple, len, IPPROTO_TCP,
+ 					    netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sk_lookup_udp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)bpf_sk_lookup(skb, tuple, len, IPPROTO_UDP,
+ 					    netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_udp_proto = {
+ 	.func		= bpf_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_1(bpf_sk_release, struct sock *, sk)
+ {
+ 	if (!sock_flag(sk, SOCK_RCU_FREE))
+ 		sock_gen_put(sk);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_release_proto = {
+ 	.func		= bpf_sk_release,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_SOCK_COMMON,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_udp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return (unsigned long)__bpf_sk_lookup(NULL, tuple, len, caller_net,
+ 					      ifindex, IPPROTO_UDP, netns_id,
+ 					      flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_udp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_udp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_skc_lookup_tcp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return (unsigned long)__bpf_skc_lookup(NULL, tuple, len, caller_net,
+ 					       ifindex, IPPROTO_TCP, netns_id,
+ 					       flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_skc_lookup_tcp_proto = {
+ 	.func           = bpf_xdp_skc_lookup_tcp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCK_COMMON_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_tcp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return (unsigned long)__bpf_sk_lookup(NULL, tuple, len, caller_net,
+ 					      ifindex, IPPROTO_TCP, netns_id,
+ 					      flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_tcp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_tcp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sock_addr_skc_lookup_tcp, struct bpf_sock_addr_kern *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)__bpf_skc_lookup(NULL, tuple, len,
+ 					       sock_net(ctx->sk), 0,
+ 					       IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sock_addr_skc_lookup_tcp_proto = {
+ 	.func		= bpf_sock_addr_skc_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCK_COMMON_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sock_addr_sk_lookup_tcp, struct bpf_sock_addr_kern *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)__bpf_sk_lookup(NULL, tuple, len,
+ 					      sock_net(ctx->sk), 0, IPPROTO_TCP,
+ 					      netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sock_addr_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sock_addr_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sock_addr_sk_lookup_udp, struct bpf_sock_addr_kern *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return (unsigned long)__bpf_sk_lookup(NULL, tuple, len,
+ 					      sock_net(ctx->sk), 0, IPPROTO_UDP,
+ 					      netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sock_addr_sk_lookup_udp_proto = {
+ 	.func		= bpf_sock_addr_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ bool bpf_tcp_sock_is_valid_access(int off, int size, enum bpf_access_type type,
+ 				  struct bpf_insn_access_aux *info)
+ {
+ 	if (off < 0 || off >= offsetofend(struct bpf_tcp_sock, bytes_acked))
+ 		return false;
+ 
+ 	if (off % size != 0)
+ 		return false;
+ 
+ 	switch (off) {
+ 	case offsetof(struct bpf_tcp_sock, bytes_received):
+ 	case offsetof(struct bpf_tcp_sock, bytes_acked):
+ 		return size == sizeof(__u64);
+ 	default:
+ 		return size == sizeof(__u32);
+ 	}
+ }
+ 
+ u32 bpf_tcp_sock_convert_ctx_access(enum bpf_access_type type,
+ 				    const struct bpf_insn *si,
+ 				    struct bpf_insn *insn_buf,
+ 				    struct bpf_prog *prog, u32 *target_size)
+ {
+ 	struct bpf_insn *insn = insn_buf;
+ 
+ #define BPF_TCP_SOCK_GET_COMMON(FIELD)					\
+ 	do {								\
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct tcp_sock, FIELD) >	\
+ 			     FIELD_SIZEOF(struct bpf_tcp_sock, FIELD));	\
+ 		*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct tcp_sock, FIELD),\
+ 				      si->dst_reg, si->src_reg,		\
+ 				      offsetof(struct tcp_sock, FIELD)); \
+ 	} while (0)
+ 
+ 	CONVERT_COMMON_TCP_SOCK_FIELDS(struct bpf_tcp_sock,
+ 				       BPF_TCP_SOCK_GET_COMMON);
+ 
+ 	if (insn > insn_buf)
+ 		return insn - insn_buf;
+ 
+ 	switch (si->off) {
+ 	case offsetof(struct bpf_tcp_sock, rtt_min):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct tcp_sock, rtt_min) !=
+ 			     sizeof(struct minmax));
+ 		BUILD_BUG_ON(sizeof(struct minmax) <
+ 			     sizeof(struct minmax_sample));
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,
+ 				      offsetof(struct tcp_sock, rtt_min) +
+ 				      offsetof(struct minmax_sample, v));
+ 		break;
+ 	}
+ 
+ 	return insn - insn_buf;
+ }
+ 
+ BPF_CALL_1(bpf_tcp_sock, struct sock *, sk)
+ {
+ 	if (sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP)
+ 		return (unsigned long)sk;
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ static const struct bpf_func_proto bpf_tcp_sock_proto = {
+ 	.func		= bpf_tcp_sock,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_TCP_SOCK_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_SOCK_COMMON,
+ };
+ 
+ BPF_CALL_1(bpf_get_listener_sock, struct sock *, sk)
+ {
+ 	sk = sk_to_full_sk(sk);
+ 
+ 	if (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_RCU_FREE))
+ 		return (unsigned long)sk;
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ static const struct bpf_func_proto bpf_get_listener_sock_proto = {
+ 	.func		= bpf_get_listener_sock,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_SOCK_COMMON,
+ };
+ 
+ BPF_CALL_1(bpf_skb_ecn_set_ce, struct sk_buff *, skb)
+ {
+ 	unsigned int iphdr_len;
+ 
+ 	if (skb->protocol == cpu_to_be16(ETH_P_IP))
+ 		iphdr_len = sizeof(struct iphdr);
+ 	else if (skb->protocol == cpu_to_be16(ETH_P_IPV6))
+ 		iphdr_len = sizeof(struct ipv6hdr);
+ 	else
+ 		return 0;
+ 
+ 	if (skb_headlen(skb) < iphdr_len)
+ 		return 0;
+ 
+ 	if (skb_cloned(skb) && !skb_clone_writable(skb, iphdr_len))
+ 		return 0;
+ 
+ 	return INET_ECN_set_ce(skb);
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_ecn_set_ce_proto = {
+ 	.func           = bpf_skb_ecn_set_ce,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ 
+ BPF_CALL_5(bpf_tcp_check_syncookie, struct sock *, sk, void *, iph, u32, iph_len,
+ 	   struct tcphdr *, th, u32, th_len)
+ {
+ #ifdef CONFIG_SYN_COOKIES
+ 	u32 cookie;
+ 	int ret;
+ 
+ 	if (unlikely(th_len < sizeof(*th)))
+ 		return -EINVAL;
+ 
+ 	/* sk_listener() allows TCP_NEW_SYN_RECV, which makes no sense here. */
+ 	if (sk->sk_protocol != IPPROTO_TCP || sk->sk_state != TCP_LISTEN)
+ 		return -EINVAL;
+ 
+ 	if (!sock_net(sk)->ipv4.sysctl_tcp_syncookies)
+ 		return -EINVAL;
+ 
+ 	if (!th->ack || th->rst || th->syn)
+ 		return -ENOENT;
+ 
+ 	if (tcp_synq_no_recent_overflow(sk))
+ 		return -ENOENT;
+ 
+ 	cookie = ntohl(th->ack_seq) - 1;
+ 
+ 	switch (sk->sk_family) {
+ 	case AF_INET:
+ 		if (unlikely(iph_len < sizeof(struct iphdr)))
+ 			return -EINVAL;
+ 
+ 		ret = __cookie_v4_check((struct iphdr *)iph, th, cookie);
+ 		break;
+ 
+ #if IS_BUILTIN(CONFIG_IPV6)
+ 	case AF_INET6:
+ 		if (unlikely(iph_len < sizeof(struct ipv6hdr)))
+ 			return -EINVAL;
+ 
+ 		ret = __cookie_v6_check((struct ipv6hdr *)iph, th, cookie);
+ 		break;
+ #endif /* CONFIG_IPV6 */
+ 
+ 	default:
+ 		return -EPROTONOSUPPORT;
+ 	}
+ 
+ 	if (ret > 0)
+ 		return 0;
+ 
+ 	return -ENOENT;
+ #else
+ 	return -ENOTSUPP;
+ #endif
+ }
+ 
+ static const struct bpf_func_proto bpf_tcp_check_syncookie_proto = {
+ 	.func		= bpf_tcp_check_syncookie,
+ 	.gpl_only	= true,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_SOCK_COMMON,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_PTR_TO_MEM,
+ 	.arg5_type	= ARG_CONST_SIZE,
+ };
+ 
+ #endif /* CONFIG_INET */
+ 
++>>>>>>> 9b28ae243ef3 (bpf: fix out-of-bounds read in __bpf_skc_lookup)
  bool bpf_helper_changes_pkt_data(void *func)
  {
  	if (func == bpf_skb_vlan_push ||
* Unmerged path net/core/filter.c
