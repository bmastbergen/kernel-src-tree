KVM: nVMX: Cache host_rsp on a per-VMCS basis

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 5a8781607e677eda60b20e0a4c91d2a5f12f9244
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/5a878160.failed

Currently, host_rsp is cached on a per-vCPU basis, i.e. it's stored in
struct vcpu_vmx.  In non-nested usage the caching is for all intents
and purposes 100% effective, e.g. only the first VMLAUNCH needs to
synchronize VMCS.HOST_RSP since the call stack to vmx_vcpu_run() is
identical each and every time.  But when running a nested guest, KVM
must invalidate the cache when switching the current VMCS as it can't
guarantee the new VMCS has the same HOST_RSP as the previous VMCS.  In
other words, the cache loses almost all of its efficacy when running a
nested VM.

Move host_rsp to struct vmcs_host_state, which is per-VMCS, so that it
is cached on a per-VMCS basis and restores its 100% hit rate when
nested VMs are in play.

Note that the host_rsp cache for vmcs02 essentially "breaks" when
nested early checks are enabled as nested_vmx_check_vmentry_hw() will
see a different RSP at the time of its VM-Enter.  While it's possible
to avoid even that VMCS.HOST_RSP synchronization, e.g. by employing a
dedicated VM-Exit stack, there is little motivation for doing so as
the overhead of two VMWRITEs (~55 cycles) is dwarfed by the overhead
of the extra VMX transition (600+ cycles) and is a proverbial drop in
the ocean relative to the total cost of a nested transtion (10s of
thousands of cycles).

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Reviewed-by: Jim Mattson <jmattson@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5a8781607e677eda60b20e0a4c91d2a5f12f9244)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/nested.c
#	arch/x86/kvm/vmx/vmcs.h
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/vmx/vmx.h
diff --cc arch/x86/kvm/vmx/vmx.c
index 44f839379d76,12bb61e7aca6..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -10388,844 -4429,816 +10388,1002 @@@ static int vmx_handle_exit(struct kvm_v
  }
  
  /*
 - * Trigger machine check on the host. We assume all the MSRs are already set up
 - * by the CPU and that we still run on the same CPU as the MCE occurred on.
 - * We pass a fake environment to the machine check handler because we want
 - * the guest to be always treated like user space, no matter what context
 - * it used internally.
 + * Software based L1D cache flush which is used when microcode providing
 + * the cache control MSR is not loaded.
 + *
 + * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
 + * flush it is required to read in 64 KiB because the replacement algorithm
 + * is not exactly LRU. This could be sized at runtime via topology
 + * information but as all relevant affected CPUs have 32KiB L1D cache size
 + * there is no point in doing so.
   */
 -static void kvm_machine_check(void)
 +static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
  {
 -#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_64)
 -	struct pt_regs regs = {
 -		.cs = 3, /* Fake ring 3 no matter what the guest ran on */
 -		.flags = X86_EFLAGS_IF,
 -	};
 +	int size = PAGE_SIZE << L1D_CACHE_ORDER;
 +
 +	/*
 +	 * This code is only executed when the the flush mode is 'cond' or
 +	 * 'always'
 +	 */
 +	if (static_branch_likely(&vmx_l1d_flush_cond)) {
 +		bool flush_l1d;
  
 -	do_machine_check(&regs, 0);
 -#endif
 -}
 +		/*
 +		 * Clear the per-vcpu flush bit, it gets set again
 +		 * either from vcpu_run() or from one of the unsafe
 +		 * VMEXIT handlers.
 +		 */
 +		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 +		vcpu->arch.l1tf_flush_l1d = false;
  
 -static int handle_machine_check(struct kvm_vcpu *vcpu)
 -{
 -	/* already handled by vcpu_run */
 -	return 1;
 -}
 +		/*
 +		 * Clear the per-cpu flush bit, it gets set again from
 +		 * the interrupt handlers.
 +		 */
 +		flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
 +		kvm_clear_cpu_l1tf_flush_l1d();
  
 -static int handle_exception(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct kvm_run *kvm_run = vcpu->run;
 -	u32 intr_info, ex_no, error_code;
 -	unsigned long cr2, rip, dr6;
 -	u32 vect_info;
 -	enum emulation_result er;
 +		if (!flush_l1d)
 +			return;
 +	}
  
 -	vect_info = vmx->idt_vectoring_info;
 -	intr_info = vmx->exit_intr_info;
 +	vcpu->stat.l1d_flush++;
  
 -	if (is_machine_check(intr_info))
 -		return handle_machine_check(vcpu);
 +	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 +		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 +		return;
 +	}
  
 -	if (is_nmi(intr_info))
 -		return 1;  /* already handled by vmx_vcpu_run() */
 +	asm volatile(
 +		/* First ensure the pages are in the TLB */
 +		"xorl	%%eax, %%eax\n"
 +		".Lpopulate_tlb:\n\t"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$4096, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lpopulate_tlb\n\t"
 +		"xorl	%%eax, %%eax\n\t"
 +		"cpuid\n\t"
 +		/* Now fill the cache */
 +		"xorl	%%eax, %%eax\n"
 +		".Lfill_cache:\n"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$64, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lfill_cache\n\t"
 +		"lfence\n"
 +		:: [flush_pages] "r" (vmx_l1d_flush_pages),
 +		    [size] "r" (size)
 +		: "eax", "ebx", "ecx", "edx");
 +}
  
 -	if (is_invalid_opcode(intr_info))
 -		return handle_ud(vcpu);
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +{
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
  
 -	error_code = 0;
 -	if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
 -		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
 +	if (is_guest_mode(vcpu) &&
 +		nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
 +		return;
  
 -	if (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {
 -		WARN_ON_ONCE(!enable_vmware_backdoor);
 -		er = kvm_emulate_instruction(vcpu,
 -			EMULTYPE_VMWARE | EMULTYPE_NO_UD_ON_FAIL);
 -		if (er == EMULATE_USER_EXIT)
 -			return 0;
 -		else if (er != EMULATE_DONE)
 -			kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 -		return 1;
 +	if (irr == -1 || tpr < irr) {
 +		vmcs_write32(TPR_THRESHOLD, 0);
 +		return;
  	}
  
 -	/*
 -	 * The #PF with PFEC.RSVD = 1 indicates the guest is accessing
 -	 * MMIO, it is better to report an internal error.
 -	 * See the comments in vmx_handle_exit.
 -	 */
 -	if ((vect_info & VECTORING_INFO_VALID_MASK) &&
 -	    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;
 -		vcpu->run->internal.ndata = 3;
 -		vcpu->run->internal.data[0] = vect_info;
 -		vcpu->run->internal.data[1] = intr_info;
 -		vcpu->run->internal.data[2] = error_code;
 -		return 0;
 -	}
 +	vmcs_write32(TPR_THRESHOLD, irr);
 +}
  
 -	if (is_page_fault(intr_info)) {
 -		cr2 = vmcs_readl(EXIT_QUALIFICATION);
 -		/* EPT won't cause page fault directly */
 -		WARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);
 -		return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
 -	}
 +static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	u32 sec_exec_control;
  
 -	ex_no = intr_info & INTR_INFO_VECTOR_MASK;
 +	if (!lapic_in_kernel(vcpu))
 +		return;
  
 -	if (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))
 -		return handle_rmode_exception(vcpu, ex_no, error_code);
 +	if (!flexpriority_enabled &&
 +	    !cpu_has_vmx_virtualize_x2apic_mode())
 +		return;
  
 -	switch (ex_no) {
 -	case AC_VECTOR:
 -		kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 -		return 1;
 -	case DB_VECTOR:
 -		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 -		if (!(vcpu->guest_debug &
 -		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= dr6 | DR6_RTM;
 -			if (is_icebp(intr_info))
 -				skip_emulated_instruction(vcpu);
 +	/* Postpone execution until vmcs01 is the current VMCS. */
 +	if (is_guest_mode(vcpu)) {
 +		to_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;
 +		return;
 +	}
  
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 +	sec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
 +	sec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +			      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
 +
 +	switch (kvm_get_apic_mode(vcpu)) {
 +	case LAPIC_MODE_INVALID:
 +		WARN_ONCE(true, "Invalid local APIC state");
 +	case LAPIC_MODE_DISABLED:
 +		break;
 +	case LAPIC_MODE_XAPIC:
 +		if (flexpriority_enabled) {
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +			vmx_flush_tlb(vcpu, true);
  		}
 -		kvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;
 -		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 -		/* fall through */
 -	case BP_VECTOR:
 -		/*
 -		 * Update instruction length as we may reinject #BP from
 -		 * user space while in guest debugging mode. Reading it for
 -		 * #DB as well causes no harm, it is not used in that case.
 -		 */
 -		vmx->vcpu.arch.event_exit_inst_len =
 -			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 -		kvm_run->exit_reason = KVM_EXIT_DEBUG;
 -		rip = kvm_rip_read(vcpu);
 -		kvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;
 -		kvm_run->debug.arch.exception = ex_no;
  		break;
 -	default:
 -		kvm_run->exit_reason = KVM_EXIT_EXCEPTION;
 -		kvm_run->ex.exception = ex_no;
 -		kvm_run->ex.error_code = error_code;
 +	case LAPIC_MODE_X2APIC:
 +		if (cpu_has_vmx_virtualize_x2apic_mode())
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
  		break;
  	}
 -	return 0;
 -}
 +	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);
  
 -static int handle_external_interrupt(struct kvm_vcpu *vcpu)
 -{
 -	++vcpu->stat.irq_exits;
 -	return 1;
 +	vmx_update_msr_bitmap(vcpu);
  }
  
 -static int handle_triple_fault(struct kvm_vcpu *vcpu)
 +static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
  {
 -	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
 -	vcpu->mmio_needed = 0;
 -	return 0;
 +	if (!is_guest_mode(vcpu)) {
 +		vmcs_write64(APIC_ACCESS_ADDR, hpa);
 +		vmx_flush_tlb(vcpu, true);
 +	}
  }
  
 -static int handle_io(struct kvm_vcpu *vcpu)
 +static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
  {
 -	unsigned long exit_qualification;
 -	int size, in, string;
 -	unsigned port;
 +	u16 status;
 +	u8 old;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	string = (exit_qualification & 16) != 0;
 +	if (max_isr == -1)
 +		max_isr = 0;
  
 -	++vcpu->stat.io_exits;
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = status >> 8;
 +	if (max_isr != old) {
 +		status &= 0xff;
 +		status |= max_isr << 8;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
 +}
  
 -	if (string)
 -		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +static void vmx_set_rvi(int vector)
 +{
 +	u16 status;
 +	u8 old;
  
 -	port = exit_qualification >> 16;
 -	size = (exit_qualification & 7) + 1;
 -	in = (exit_qualification & 8) != 0;
 +	if (vector == -1)
 +		vector = 0;
  
 -	return kvm_fast_pio(vcpu, size, port, in);
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = (u8)status & 0xff;
 +	if ((u8)vector != old) {
 +		status &= ~0xff;
 +		status |= (u8)vector;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
  }
  
 -static void
 -vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
  {
  	/*
 -	 * Patch in the VMCALL instruction:
 +	 * When running L2, updating RVI is only relevant when
 +	 * vmcs12 virtual-interrupt-delivery enabled.
 +	 * However, it can be enabled only when L1 also
 +	 * intercepts external-interrupts and in that case
 +	 * we should not update vmcs02 RVI but instead intercept
 +	 * interrupt. Therefore, do nothing when running L2.
  	 */
 -	hypercall[0] = 0x0f;
 -	hypercall[1] = 0x01;
 -	hypercall[2] = 0xc1;
 +	if (!is_guest_mode(vcpu))
 +		vmx_set_rvi(max_irr);
  }
  
 -/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 -static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
 +static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
  {
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int max_irr;
 +	bool max_irr_updated;
  
 +	WARN_ON(!vcpu->arch.apicv_active);
 +	if (pi_test_on(&vmx->pi_desc)) {
 +		pi_clear_on(&vmx->pi_desc);
  		/*
 -		 * We get here when L2 changed cr0 in a way that did not change
 -		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 -		 * but did change L0 shadowed bits. So we first calculate the
 -		 * effective cr0 value that L1 would like to write into the
 -		 * hardware. It consists of the L2-owned bits from the new
 -		 * value combined with the L1-owned bits from L1's guest_cr0.
 +		 * IOMMU can write to PIR.ON, so the barrier matters even on UP.
 +		 * But on x86 this is just a compiler barrier anyway.
  		 */
 -		val = (val & ~vmcs12->cr0_guest_host_mask) |
 -			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
 -
 -		if (!nested_guest_cr0_valid(vcpu, val))
 -			return 1;
 +		smp_mb__after_atomic();
 +		max_irr_updated =
 +			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
  
 -		if (kvm_set_cr0(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR0_READ_SHADOW, orig_val);
 -		return 0;
 +		/*
 +		 * If we are running L2 and L1 has a new pending interrupt
 +		 * which can be injected, we should re-evaluate
 +		 * what should be done with this new L1 interrupt.
 +		 * If L1 intercepts external-interrupts, we should
 +		 * exit from L2 to L1. Otherwise, interrupt should be
 +		 * delivered directly to L2.
 +		 */
 +		if (is_guest_mode(vcpu) && max_irr_updated) {
 +			if (nested_exit_on_intr(vcpu))
 +				kvm_vcpu_exiting_guest_mode(vcpu);
 +			else
 +				kvm_make_request(KVM_REQ_EVENT, vcpu);
 +		}
  	} else {
 -		if (to_vmx(vcpu)->nested.vmxon &&
 -		    !nested_host_cr0_valid(vcpu, val))
 -			return 1;
 -
 -		return kvm_set_cr0(vcpu, val);
 +		max_irr = kvm_lapic_find_highest_irr(vcpu);
  	}
 +	vmx_hwapic_irr_update(vcpu, max_irr);
 +	return max_irr;
  }
  
 -static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 +static u8 vmx_has_apicv_interrupt(struct kvm_vcpu *vcpu)
  {
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	u8 rvi = vmx_get_rvi();
 +	u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
  
 -		/* analogously to handle_set_cr0 */
 -		val = (val & ~vmcs12->cr4_guest_host_mask) |
 -			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 -		if (kvm_set_cr4(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR4_READ_SHADOW, orig_val);
 -		return 0;
 -	} else
 -		return kvm_set_cr4(vcpu, val);
 +	return ((rvi & 0xf0) > (vppr & 0xf0));
  }
  
 -static int handle_desc(struct kvm_vcpu *vcpu)
 +static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
  {
 -	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);
 +	vmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);
 +	vmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);
 +	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
  }
  
 -static int handle_cr(struct kvm_vcpu *vcpu)
 +static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
  {
 -	unsigned long exit_qualification, val;
 -	int cr;
 -	int reg;
 -	int err;
 -	int ret;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	cr = exit_qualification & 15;
 -	reg = (exit_qualification >> 8) & 15;
 -	switch ((exit_qualification >> 4) & 3) {
 -	case 0: /* mov to cr */
 -		val = kvm_register_readl(vcpu, reg);
 -		trace_kvm_cr_write(cr, val);
 -		switch (cr) {
 -		case 0:
 -			err = handle_set_cr0(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			err = kvm_set_cr3(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 4:
 -			err = handle_set_cr4(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 8: {
 -				u8 cr8_prev = kvm_get_cr8(vcpu);
 -				u8 cr8 = (u8)val;
 -				err = kvm_set_cr8(vcpu, cr8);
 -				ret = kvm_complete_insn_gp(vcpu, err);
 -				if (lapic_in_kernel(vcpu))
 -					return ret;
 -				if (cr8_prev <= cr8)
 -					return ret;
 -				/*
 -				 * TODO: we might be squashing a
 -				 * KVM_GUESTDBG_SINGLESTEP-triggered
 -				 * KVM_EXIT_DEBUG here.
 -				 */
 -				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 -				return 0;
 -			}
 -		}
 -		break;
 -	case 2: /* clts */
 -		WARN_ONCE(1, "Guest should always own CR0.TS");
 -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 -		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 -		return kvm_skip_emulated_instruction(vcpu);
 -	case 1: /*mov from cr*/
 -		switch (cr) {
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			val = kvm_read_cr3(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		case 8:
 -			val = kvm_get_cr8(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -		break;
 -	case 3: /* lmsw */
 -		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 -		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 -		kvm_lmsw(vcpu, val);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -		return kvm_skip_emulated_instruction(vcpu);
 -	default:
 -		break;
 -	}
 -	vcpu->run->exit_reason = 0;
 -	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 -	       (int)(exit_qualification >> 4) & 3, cr);
 -	return 0;
 +	pi_clear_on(&vmx->pi_desc);
 +	memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
  }
  
 -static int handle_dr(struct kvm_vcpu *vcpu)
 +static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
  {
 -	unsigned long exit_qualification;
 -	int dr, dr7, reg;
 +	u32 exit_intr_info = 0;
 +	u16 basic_exit_reason = (u16)vmx->exit_reason;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 +	if (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY
 +	      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))
 +		return;
  
 -	/* First, if DR does not exist, trigger UD */
 -	if (!kvm_require_dr(vcpu, dr))
 -		return 1;
 +	if (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +	vmx->exit_intr_info = exit_intr_info;
  
 -	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 -	if (!kvm_require_cpl(vcpu, 0))
 -		return 1;
 -	dr7 = vmcs_readl(GUEST_DR7);
 -	if (dr7 & DR7_GD) {
 -		/*
 -		 * As the vm-exit takes precedence over the debug trap, we
 -		 * need to emulate the latter, either for the host or the
 -		 * guest debugging itself.
 -		 */
 -		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 -			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 -			vcpu->run->debug.arch.dr7 = dr7;
 -			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 -			vcpu->run->debug.arch.exception = DB_VECTOR;
 -			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 -			return 0;
 -		} else {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 -		}
 +	/* if exit due to PF check for async PF */
 +	if (is_page_fault(exit_intr_info))
 +		vmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	/* Handle machine checks before interrupts are enabled */
 +	if (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||
 +	    is_machine_check(exit_intr_info))
 +		kvm_machine_check();
 +
 +	/* We need to handle NMIs before interrupts are enabled */
 +	if (is_nmi(exit_intr_info)) {
 +		kvm_before_interrupt(&vmx->vcpu);
 +		asm("int $2");
 +		kvm_after_interrupt(&vmx->vcpu);
  	}
 +}
  
 -	if (vcpu->guest_debug == 0) {
 -		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -				CPU_BASED_MOV_DR_EXITING);
 +static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 +{
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
  
 -		/*
 -		 * No more DR vmexits; force a reload of the debug registers
 -		 * and reenter on this instruction.  The next vmexit will
 -		 * retrieve the full state of the debug registers.
 +	if ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))
 +			== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {
 +		unsigned int vector;
 +		unsigned long entry;
 +		gate_desc *desc;
 +		struct vcpu_vmx *vmx = to_vmx(vcpu);
 +#ifdef CONFIG_X86_64
 +		unsigned long tmp;
 +#endif
 +
 +		vector =  exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		desc = (gate_desc *)vmx->host_idt_base + vector;
 +		entry = gate_offset(desc);
 +		asm volatile(
 +#ifdef CONFIG_X86_64
 +			"mov %%" _ASM_SP ", %[sp]\n\t"
 +			"and $0xfffffffffffffff0, %%" _ASM_SP "\n\t"
 +			"push $%c[ss]\n\t"
 +			"push %[sp]\n\t"
 +#endif
 +			"pushf\n\t"
 +			__ASM_SIZE(push) " $%c[cs]\n\t"
 +			CALL_NOSPEC
 +			:
 +#ifdef CONFIG_X86_64
 +			[sp]"=&r"(tmp),
 +#endif
 +			ASM_CALL_CONSTRAINT
 +			:
 +			THUNK_TARGET(entry),
 +			[ss]"i"(__KERNEL_DS),
 +			[cs]"i"(__KERNEL_CS)
 +			);
 +	}
 +}
 +STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
 +
 +static bool vmx_has_emulated_msr(int index)
 +{
 +	switch (index) {
 +	case MSR_IA32_SMBASE:
 +		/*
 +		 * We cannot do SMM unless we can run the guest in big
 +		 * real mode.
  		 */
 -		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 -		return 1;
 +		return enable_unrestricted_guest || emulate_invalid_guest_state;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		/* This is AMD only.  */
 +		return false;
 +	default:
 +		return true;
  	}
 -
 -	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 -	if (exit_qualification & TYPE_MOV_FROM_DR) {
 -		unsigned long val;
 -
 -		if (kvm_get_dr(vcpu, dr, &val))
 -			return 1;
 -		kvm_register_write(vcpu, reg, val);
 -	} else
 -		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 -			return 1;
 -
 -	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 +static bool vmx_mpx_supported(void)
  {
 -	return vcpu->arch.dr6;
 +	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 +		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
  }
  
 -static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +static bool vmx_xsaves_supported(void)
  {
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_XSAVES;
  }
  
 -static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
  {
 -	get_debugreg(vcpu->arch.db[0], 0);
 -	get_debugreg(vcpu->arch.db[1], 1);
 -	get_debugreg(vcpu->arch.db[2], 2);
 -	get_debugreg(vcpu->arch.db[3], 3);
 -	get_debugreg(vcpu->arch.dr6, 6);
 -	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
 +	u32 exit_intr_info;
 +	bool unblock_nmi;
 +	u8 vector;
 +	bool idtv_info_valid;
  
 -	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 -}
 +	idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
  
 -static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 -{
 -	vmcs_writel(GUEST_DR7, val);
 +	if (enable_vnmi) {
 +		if (vmx->loaded_vmcs->nmi_known_unmasked)
 +			return;
 +		/*
 +		 * Can't use vmx->exit_intr_info since we're not sure what
 +		 * the exit reason is.
 +		 */
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +		unblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;
 +		vector = exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Re-set bit "block by NMI" before VM entry if vmexit caused by
 +		 * a guest IRET fault.
 +		 * SDM 3: 23.2.2 (September 2008)
 +		 * Bit 12 is undefined in any of the following cases:
 +		 *  If the VM exit sets the valid bit in the IDT-vectoring
 +		 *   information field.
 +		 *  If the VM exit is due to a double fault.
 +		 */
 +		if ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&
 +		    vector != DF_VECTOR && !idtv_info_valid)
 +			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				      GUEST_INTR_STATE_NMI);
 +		else
 +			vmx->loaded_vmcs->nmi_known_unmasked =
 +				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)
 +				  & GUEST_INTR_STATE_NMI);
 +	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->vnmi_blocked_time +=
 +			ktime_to_ns(ktime_sub(ktime_get(),
 +					      vmx->loaded_vmcs->entry_time));
  }
  
 -static int handle_cpuid(struct kvm_vcpu *vcpu)
 +static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 +				      u32 idt_vectoring_info,
 +				      int instr_len_field,
 +				      int error_code_field)
  {
 -	return kvm_emulate_cpuid(vcpu);
 -}
 +	u8 vector;
 +	int type;
 +	bool idtv_info_valid;
  
 -static int handle_rdmsr(struct kvm_vcpu *vcpu)
 -{
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	struct msr_data msr_info;
 +	idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
  
 -	msr_info.index = ecx;
 -	msr_info.host_initiated = false;
 -	if (vmx_get_msr(vcpu, &msr_info)) {
 -		trace_kvm_msr_read_ex(ecx);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +	vcpu->arch.nmi_injected = false;
 +	kvm_clear_exception_queue(vcpu);
 +	kvm_clear_interrupt_queue(vcpu);
  
 -	trace_kvm_msr_read(ecx, msr_info.data);
 +	if (!idtv_info_valid)
 +		return;
  
 -	/* FIXME: handling of bits 32:63 of rax, rdx */
 -	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 -	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
  
 -static int handle_wrmsr(struct kvm_vcpu *vcpu)
 -{
 -	struct msr_data msr;
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 -		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
 +	vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
 +	type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
  
 -	msr.data = data;
 -	msr.index = ecx;
 -	msr.host_initiated = false;
 -	if (kvm_set_msr(vcpu, &msr) != 0) {
 -		trace_kvm_msr_write_ex(ecx, data);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 +	switch (type) {
 +	case INTR_TYPE_NMI_INTR:
 +		vcpu->arch.nmi_injected = true;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Clear bit "block by NMI" before VM entry if a NMI
 +		 * delivery faulted.
 +		 */
 +		vmx_set_nmi_mask(vcpu, false);
 +		break;
 +	case INTR_TYPE_SOFT_EXCEPTION:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_HARD_EXCEPTION:
 +		if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
 +			u32 err = vmcs_read32(error_code_field);
 +			kvm_requeue_exception_e(vcpu, vector, err);
 +		} else
 +			kvm_requeue_exception(vcpu, vector);
 +		break;
 +	case INTR_TYPE_SOFT_INTR:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_EXT_INTR:
 +		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 +		break;
 +	default:
 +		break;
  	}
 -
 -	trace_kvm_msr_write(ecx, data);
 -	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
  {
 -	kvm_apic_update_ppr(vcpu);
 -	return 1;
 +	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 +				  VM_EXIT_INSTRUCTION_LEN,
 +				  IDT_VECTORING_ERROR_CODE);
  }
  
 -static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 +static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
  {
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_INTR_PENDING);
 -
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +	__vmx_complete_interrupts(vcpu,
 +				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +				  VM_ENTRY_INSTRUCTION_LEN,
 +				  VM_ENTRY_EXCEPTION_ERROR_CODE);
  
 -	++vcpu->stat.irq_window_exits;
 -	return 1;
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
  }
  
 -static int handle_halt(struct kvm_vcpu *vcpu)
 +static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
  {
 -	return kvm_emulate_halt(vcpu);
 -}
 +	int i, nr_msrs;
 +	struct perf_guest_switch_msr *msrs;
  
 -static int handle_vmcall(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_emulate_hypercall(vcpu);
 +	msrs = perf_guest_get_msrs(&nr_msrs);
 +
 +	if (!msrs)
 +		return;
 +
 +	for (i = 0; i < nr_msrs; i++)
 +		if (msrs[i].host == msrs[i].guest)
 +			clear_atomic_switch_msr(vmx, msrs[i].msr);
 +		else
 +			add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,
 +					msrs[i].host, false);
  }
  
 -static int handle_invd(struct kvm_vcpu *vcpu)
 +static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
  {
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, val);
 +	if (!vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
 +			      PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = true;
  }
  
 -static int handle_invlpg(struct kvm_vcpu *vcpu)
 +static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
  {
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 tscl;
 +	u32 delta_tsc;
  
 -	kvm_mmu_invlpg(vcpu, exit_qualification);
 -	return kvm_skip_emulated_instruction(vcpu);
 +	if (vmx->req_immediate_exit) {
 +		vmx_arm_hv_timer(vmx, 0);
 +		return;
 +	}
 +
 +	if (vmx->hv_deadline_tsc != -1) {
 +		tscl = rdtsc();
 +		if (vmx->hv_deadline_tsc > tscl)
 +			/* set_hv_timer ensures the delta fits in 32-bits */
 +			delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
 +				cpu_preemption_timer_multi);
 +		else
 +			delta_tsc = 0;
 +
 +		vmx_arm_hv_timer(vmx, delta_tsc);
 +		return;
 +	}
 +
 +	if (vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
 +				PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = false;
  }
  
 -static int handle_rdpmc(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
++=======
++static void __vmx_vcpu_run(struct kvm_vcpu *vcpu, struct vcpu_vmx *vmx)
+ {
 -	int err;
++	unsigned long evmcs_rsp;
+ 
 -	err = kvm_rdpmc(vcpu);
 -	return kvm_complete_insn_gp(vcpu, err);
 -}
++	vmx->__launched = vmx->loaded_vmcs->launched;
++
++	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
++		(unsigned long)&current_evmcs->host_rsp : 0;
++
++	if (static_branch_unlikely(&vmx_l1d_should_flush))
++		vmx_l1d_flush(vcpu);
++
++	asm(
++		/* Store host registers */
++		"push %%" _ASM_BP " \n\t"
++		"sub $%c[wordsize], %%" _ASM_SP "\n\t" /* placeholder for guest RCX */
++		"push %%" _ASM_CX " \n\t"
++		"sub $%c[wordsize], %%" _ASM_SP "\n\t" /* temporarily adjust RSP for CALL */
++		"cmp %%" _ASM_SP ", (%%" _ASM_DI ") \n\t"
++		"je 1f \n\t"
++		"mov %%" _ASM_SP ", (%%" _ASM_DI ") \n\t"
++		/* Avoid VMWRITE when Enlightened VMCS is in use */
++		"test %%" _ASM_SI ", %%" _ASM_SI " \n\t"
++		"jz 2f \n\t"
++		"mov %%" _ASM_SP ", (%%" _ASM_SI ") \n\t"
++		"jmp 1f \n\t"
++		"2: \n\t"
++		"mov $%c[HOST_RSP], %%" _ASM_DX " \n\t"
++		__ex("vmwrite %%" _ASM_SP ", %%" _ASM_DX) "\n\t"
++		"1: \n\t"
++		"add $%c[wordsize], %%" _ASM_SP "\n\t" /* un-adjust RSP */
+ 
 -static int handle_wbinvd(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_emulate_wbinvd(vcpu);
 -}
++		/* Reload cr2 if changed */
++		"mov %c[cr2](%%" _ASM_CX "), %%" _ASM_AX " \n\t"
++		"mov %%cr2, %%" _ASM_DX " \n\t"
++		"cmp %%" _ASM_AX ", %%" _ASM_DX " \n\t"
++		"je 3f \n\t"
++		"mov %%" _ASM_AX", %%cr2 \n\t"
++		"3: \n\t"
++		/* Check if vmlaunch or vmresume is needed */
++		"cmpb $0, %c[launched](%%" _ASM_CX ") \n\t"
++		/* Load guest registers.  Don't clobber flags. */
++		"mov %c[rax](%%" _ASM_CX "), %%" _ASM_AX " \n\t"
++		"mov %c[rbx](%%" _ASM_CX "), %%" _ASM_BX " \n\t"
++		"mov %c[rdx](%%" _ASM_CX "), %%" _ASM_DX " \n\t"
++		"mov %c[rsi](%%" _ASM_CX "), %%" _ASM_SI " \n\t"
++		"mov %c[rdi](%%" _ASM_CX "), %%" _ASM_DI " \n\t"
++		"mov %c[rbp](%%" _ASM_CX "), %%" _ASM_BP " \n\t"
++#ifdef CONFIG_X86_64
++		"mov %c[r8](%%" _ASM_CX "),  %%r8  \n\t"
++		"mov %c[r9](%%" _ASM_CX "),  %%r9  \n\t"
++		"mov %c[r10](%%" _ASM_CX "), %%r10 \n\t"
++		"mov %c[r11](%%" _ASM_CX "), %%r11 \n\t"
++		"mov %c[r12](%%" _ASM_CX "), %%r12 \n\t"
++		"mov %c[r13](%%" _ASM_CX "), %%r13 \n\t"
++		"mov %c[r14](%%" _ASM_CX "), %%r14 \n\t"
++		"mov %c[r15](%%" _ASM_CX "), %%r15 \n\t"
++#endif
++		/* Load guest RCX.  This kills the vmx_vcpu pointer! */
++		"mov %c[rcx](%%" _ASM_CX "), %%" _ASM_CX " \n\t"
+ 
 -static int handle_xsetbv(struct kvm_vcpu *vcpu)
 -{
 -	u64 new_bv = kvm_read_edx_eax(vcpu);
 -	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
++		/* Enter guest mode */
++		"call vmx_vmenter\n\t"
+ 
 -	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 -		return kvm_skip_emulated_instruction(vcpu);
 -	return 1;
 -}
++		/* Save guest's RCX to the stack placeholder (see above) */
++		"mov %%" _ASM_CX ", %c[wordsize](%%" _ASM_SP ") \n\t"
+ 
 -static int handle_xsaves(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 -}
++		/* Load host's RCX, i.e. the vmx_vcpu pointer */
++		"pop %%" _ASM_CX " \n\t"
+ 
 -static int handle_xrstors(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 -}
++		/* Set vmx->fail based on EFLAGS.{CF,ZF} */
++		"setbe %c[fail](%%" _ASM_CX ")\n\t"
+ 
 -static int handle_apic_access(struct kvm_vcpu *vcpu)
 -{
 -	if (likely(fasteoi)) {
 -		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -		int access_type, offset;
++		/* Save all guest registers, including RCX from the stack */
++		"mov %%" _ASM_AX ", %c[rax](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_BX ", %c[rbx](%%" _ASM_CX ") \n\t"
++		__ASM_SIZE(pop) " %c[rcx](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_DX ", %c[rdx](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_SI ", %c[rsi](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_DI ", %c[rdi](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_BP ", %c[rbp](%%" _ASM_CX ") \n\t"
++#ifdef CONFIG_X86_64
++		"mov %%r8,  %c[r8](%%" _ASM_CX ") \n\t"
++		"mov %%r9,  %c[r9](%%" _ASM_CX ") \n\t"
++		"mov %%r10, %c[r10](%%" _ASM_CX ") \n\t"
++		"mov %%r11, %c[r11](%%" _ASM_CX ") \n\t"
++		"mov %%r12, %c[r12](%%" _ASM_CX ") \n\t"
++		"mov %%r13, %c[r13](%%" _ASM_CX ") \n\t"
++		"mov %%r14, %c[r14](%%" _ASM_CX ") \n\t"
++		"mov %%r15, %c[r15](%%" _ASM_CX ") \n\t"
+ 
 -		access_type = exit_qualification & APIC_ACCESS_TYPE;
 -		offset = exit_qualification & APIC_ACCESS_OFFSET;
+ 		/*
 -		 * Sane guest uses MOV to write EOI, with written value
 -		 * not cared. So make a short-circuit here by avoiding
 -		 * heavy instruction emulation.
++		 * Clear all general purpose registers (except RSP, which is loaded by
++		 * the CPU during VM-Exit) to prevent speculative use of the guest's
++		 * values, even those that are saved/loaded via the stack.  In theory,
++		 * an L1 cache miss when restoring registers could lead to speculative
++		 * execution with the guest's values.  Zeroing XORs are dirt cheap,
++		 * i.e. the extra paranoia is essentially free.
+ 		 */
 -		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 -		    (offset == APIC_EOI)) {
 -			kvm_lapic_set_eoi(vcpu);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -	}
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
 -
 -static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	int vector = exit_qualification & 0xff;
 -
 -	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_set_eoi_accelerated(vcpu, vector);
 -	return 1;
 -}
 -
 -static int handle_apic_write(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	u32 offset = exit_qualification & 0xfff;
++		"xor %%r8d,  %%r8d \n\t"
++		"xor %%r9d,  %%r9d \n\t"
++		"xor %%r10d, %%r10d \n\t"
++		"xor %%r11d, %%r11d \n\t"
++		"xor %%r12d, %%r12d \n\t"
++		"xor %%r13d, %%r13d \n\t"
++		"xor %%r14d, %%r14d \n\t"
++		"xor %%r15d, %%r15d \n\t"
++#endif
++		"mov %%cr2, %%" _ASM_AX "   \n\t"
++		"mov %%" _ASM_AX ", %c[cr2](%%" _ASM_CX ") \n\t"
+ 
 -	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_write_nodecode(vcpu, offset);
 -	return 1;
++		"xor %%eax, %%eax \n\t"
++		"xor %%ebx, %%ebx \n\t"
++		"xor %%ecx, %%ecx \n\t"
++		"xor %%edx, %%edx \n\t"
++		"xor %%esi, %%esi \n\t"
++		"xor %%edi, %%edi \n\t"
++		"xor %%ebp, %%ebp \n\t"
++		"pop  %%" _ASM_BP " \n\t"
++	      : ASM_CALL_CONSTRAINT, "=D"((int){0}), "=S"((int){0})
++	      : "c"(vmx), "D"(&vmx->loaded_vmcs->host_state.rsp), "S"(evmcs_rsp),
++		[launched]"i"(offsetof(struct vcpu_vmx, __launched)),
++		[fail]"i"(offsetof(struct vcpu_vmx, fail)),
++		[HOST_RSP]"i"(HOST_RSP),
++		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
++		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
++		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
++		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
++		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
++		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
++		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
++#ifdef CONFIG_X86_64
++		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
++		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
++		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
++		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
++		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
++		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
++		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
++		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
++#endif
++		[cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),
++		[wordsize]"i"(sizeof(ulong))
++	      : "cc", "memory"
++#ifdef CONFIG_X86_64
++		, "rax", "rbx", "rdx"
++		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
++#else
++		, "eax", "ebx", "edx"
++#endif
++	      );
+ }
++STACK_FRAME_NON_STANDARD(__vmx_vcpu_run);
+ 
 -static int handle_task_switch(struct kvm_vcpu *vcpu)
++static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
++>>>>>>> 5a8781607e67 (KVM: nVMX: Cache host_rsp on a per-VMCS basis)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	unsigned long exit_qualification;
 -	bool has_error_code = false;
 -	u32 error_code = 0;
 -	u16 tss_selector;
 -	int reason, type, idt_v, idt_index;
 +	unsigned long cr3, cr4, evmcs_rsp;
  
 -	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 -	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 -	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
 +	/* Record the guest's net vcpu time for enforced NMI injections. */
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->entry_time = ktime_get();
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	/* Don't enter VMX if guest state is invalid, let the exit handler
 +	   start emulation until we arrive back to a valid state */
 +	if (vmx->emulation_required)
 +		return;
  
 -	reason = (u32)exit_qualification >> 30;
 -	if (reason == TASK_SWITCH_GATE && idt_v) {
 -		switch (type) {
 -		case INTR_TYPE_NMI_INTR:
 -			vcpu->arch.nmi_injected = false;
 -			vmx_set_nmi_mask(vcpu, true);
 -			break;
 -		case INTR_TYPE_EXT_INTR:
 -		case INTR_TYPE_SOFT_INTR:
 -			kvm_clear_interrupt_queue(vcpu);
 -			break;
 -		case INTR_TYPE_HARD_EXCEPTION:
 -			if (vmx->idt_vectoring_info &
 -			    VECTORING_INFO_DELIVER_CODE_MASK) {
 -				has_error_code = true;
 -				error_code =
 -					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 -			}
 -			/* fall through */
 -		case INTR_TYPE_SOFT_EXCEPTION:
 -			kvm_clear_exception_queue(vcpu);
 -			break;
 -		default:
 -			break;
 +	if (vmx->ple_window_dirty) {
 +		vmx->ple_window_dirty = false;
 +		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 +	}
 +
 +	if (vmx->nested.need_vmcs12_sync) {
 +		if (vmx->nested.hv_evmcs) {
 +			copy_vmcs12_to_enlightened(vmx);
 +			/* All fields are clean */
 +			vmx->nested.hv_evmcs->hv_clean_fields |=
 +				HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
 +		} else {
 +			copy_vmcs12_to_shadow(vmx);
  		}
 +		vmx->nested.need_vmcs12_sync = false;
  	}
 -	tss_selector = exit_qualification;
  
 -	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 -		       type != INTR_TYPE_EXT_INTR &&
 -		       type != INTR_TYPE_NMI_INTR))
 -		skip_emulated_instruction(vcpu);
 +	if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 +	if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
  
 -	if (kvm_task_switch(vcpu, tss_selector,
 -			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 -			    has_error_code, error_code) == EMULATE_FAIL) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -		vcpu->run->internal.ndata = 0;
 -		return 0;
 +	cr3 = __get_current_cr3_fast();
 +	if (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {
 +		vmcs_writel(HOST_CR3, cr3);
 +		vmx->loaded_vmcs->host_state.cr3 = cr3;
 +	}
 +
 +	cr4 = cr4_read_shadow();
 +	if (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {
 +		vmcs_writel(HOST_CR4, cr4);
 +		vmx->loaded_vmcs->host_state.cr4 = cr4;
  	}
  
 -	/*
 -	 * TODO: What about debug traps on tss switch?
 -	 *       Are we supposed to inject them and update dr6?
 -	 */
 +	/* When single-stepping over STI and MOV SS, we must clear the
 +	 * corresponding interruptibility bits in the guest state. Otherwise
 +	 * vmentry fails as it then expects bit 14 (BS) in pending debug
 +	 * exceptions being set, but that's not correct for the guest debugging
 +	 * case. */
 +	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 +		vmx_set_interrupt_shadow(vcpu, 0);
  
 -	return 1;
 -}
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
 +	    vcpu->arch.pkru != vmx->host_pkru)
 +		__write_pkru(vcpu->arch.pkru);
  
 -static int handle_ept_violation(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification;
 -	gpa_t gpa;
 -	u64 error_code;
 +	atomic_switch_perf_msrs(vmx);
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	vmx_update_hv_timer(vcpu);
  
  	/*
 -	 * EPT violation happened while executing iret from NMI,
 -	 * "blocked by NMI" bit has to be set before next VM entry.
 -	 * There are errata that may cause this bit to not be set:
 -	 * AAK134, BY25.
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
  	 */
 -	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 -			enable_vnmi &&
 -			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 -		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 -
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	trace_kvm_page_fault(gpa, exit_qualification);
 +	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
  
 -	/* Is it a read fault? */
 -	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 -		     ? PFERR_USER_MASK : 0;
 -	/* Is it a write fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 -		      ? PFERR_WRITE_MASK : 0;
 -	/* Is it a fetch fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 -		      ? PFERR_FETCH_MASK : 0;
 -	/* ept page table entry is present? */
 -	error_code |= (exit_qualification &
 -		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 -			EPT_VIOLATION_EXECUTABLE))
 -		      ? PFERR_PRESENT_MASK : 0;
 +	vmx->__launched = vmx->loaded_vmcs->launched;
  
 -	error_code |= (exit_qualification & 0x100) != 0 ?
 -	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 +	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
 +		(unsigned long)&current_evmcs->host_rsp : 0;
  
 -	vcpu->arch.exit_qualification = exit_qualification;
 -	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 -}
 +	if (static_branch_unlikely(&vmx_l1d_should_flush))
 +		vmx_l1d_flush(vcpu);
  
 -static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 -{
 -	gpa_t gpa;
 +	asm(
 +		/* Store host registers */
 +		"push %%" _ASM_DX "; push %%" _ASM_BP ";"
 +		"push %%" _ASM_CX " \n\t" /* placeholder for guest rcx */
 +		"push %%" _ASM_CX " \n\t"
 +		"cmp %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		"je 1f \n\t"
 +		"mov %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		/* Avoid VMWRITE when Enlightened VMCS is in use */
 +		"test %%" _ASM_SI ", %%" _ASM_SI " \n\t"
 +		"jz 2f \n\t"
 +		"mov %%" _ASM_SP ", (%%" _ASM_SI ") \n\t"
 +		"jmp 1f \n\t"
 +		"2: \n\t"
 +		__ex("vmwrite %%" _ASM_SP ", %%" _ASM_DX) "\n\t"
 +		"1: \n\t"
 +		/* Reload cr2 if changed */
 +		"mov %c[cr2](%0), %%" _ASM_AX " \n\t"
 +		"mov %%cr2, %%" _ASM_DX " \n\t"
 +		"cmp %%" _ASM_AX ", %%" _ASM_DX " \n\t"
 +		"je 3f \n\t"
 +		"mov %%" _ASM_AX", %%cr2 \n\t"
 +		"3: \n\t"
 +		/* Check if vmlaunch or vmresume is needed */
 +		"cmpl $0, %c[launched](%0) \n\t"
 +		/* Load guest registers.  Don't clobber flags. */
 +		"mov %c[rax](%0), %%" _ASM_AX " \n\t"
 +		"mov %c[rbx](%0), %%" _ASM_BX " \n\t"
 +		"mov %c[rdx](%0), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%0), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%0), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%0), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%0),  %%r8  \n\t"
 +		"mov %c[r9](%0),  %%r9  \n\t"
 +		"mov %c[r10](%0), %%r10 \n\t"
 +		"mov %c[r11](%0), %%r11 \n\t"
 +		"mov %c[r12](%0), %%r12 \n\t"
 +		"mov %c[r13](%0), %%r13 \n\t"
 +		"mov %c[r14](%0), %%r14 \n\t"
 +		"mov %c[r15](%0), %%r15 \n\t"
 +#endif
 +		"mov %c[rcx](%0), %%" _ASM_CX " \n\t" /* kills %0 (ecx) */
  
 -	/*
 -	 * A nested guest cannot optimize MMIO vmexits, because we have an
 -	 * nGPA here instead of the required GPA.
 -	 */
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	if (!is_guest_mode(vcpu) &&
 -	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 -		trace_kvm_fast_mmio(gpa);
 +		/* Enter guest mode */
 +		"jne 1f \n\t"
 +		__ex("vmlaunch") "\n\t"
 +		"jmp 2f \n\t"
 +		"1: " __ex("vmresume") "\n\t"
 +		"2: "
 +		/* Save guest registers, load host registers, keep flags */
 +		"mov %0, %c[wordsize](%%" _ASM_SP ") \n\t"
 +		"pop %0 \n\t"
 +		"setbe %c[fail](%0)\n\t"
 +		"mov %%" _ASM_AX ", %c[rax](%0) \n\t"
 +		"mov %%" _ASM_BX ", %c[rbx](%0) \n\t"
 +		__ASM_SIZE(pop) " %c[rcx](%0) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%0) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%0) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%0) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%0) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%0) \n\t"
 +		"mov %%r9,  %c[r9](%0) \n\t"
 +		"mov %%r10, %c[r10](%0) \n\t"
 +		"mov %%r11, %c[r11](%0) \n\t"
 +		"mov %%r12, %c[r12](%0) \n\t"
 +		"mov %%r13, %c[r13](%0) \n\t"
 +		"mov %%r14, %c[r14](%0) \n\t"
 +		"mov %%r15, %c[r15](%0) \n\t"
  		/*
 -		 * Doing kvm_skip_emulated_instruction() depends on undefined
 -		 * behavior: Intel's manual doesn't mandate
 -		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 -		 * occurs and while on real hardware it was observed to be set,
 -		 * other hypervisors (namely Hyper-V) don't set it, we end up
 -		 * advancing IP with some random value. Disable fast mmio when
 -		 * running nested and keep it for real hardware in hope that
 -		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 -		 */
 -		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 -			return kvm_skip_emulated_instruction(vcpu);
 -		else
 -			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 -								EMULATE_DONE;
 -	}
 -
 -	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 -}
 -
 -static int handle_nmi_window(struct kvm_vcpu *vcpu)
 -{
 -	WARN_ON_ONCE(!enable_vnmi);
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_NMI_PENDING);
 -	++vcpu->stat.nmi_window_exits;
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 -
 -	return 1;
 -}
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d,  %%r8d \n\t"
 +		"xor %%r9d,  %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"mov %%cr2, %%" _ASM_AX "   \n\t"
 +		"mov %%" _ASM_AX ", %c[cr2](%0) \n\t"
  
 -static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	enum emulation_result err = EMULATE_DONE;
 -	int ret = 1;
 -	u32 cpu_exec_ctrl;
 -	bool intr_window_requested;
 -	unsigned count = 130;
 +		"xor %%eax, %%eax \n\t"
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop  %%" _ASM_BP "; pop  %%" _ASM_DX " \n\t"
 +		".pushsection .rodata \n\t"
 +		".global vmx_return \n\t"
 +		"vmx_return: " _ASM_PTR " 2b \n\t"
 +		".popsection"
 +	      : : "c"(vmx), "d"((unsigned long)HOST_RSP), "S"(evmcs_rsp),
 +		[launched]"i"(offsetof(struct vcpu_vmx, __launched)),
 +		[fail]"i"(offsetof(struct vcpu_vmx, fail)),
 +		[host_rsp]"i"(offsetof(struct vcpu_vmx, host_rsp)),
 +		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
 +		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
 +#ifdef CONFIG_X86_64
 +		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
 +		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
 +		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
 +		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
 +		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
 +		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
 +		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
 +		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
 +#endif
 +		[cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),
 +		[wordsize]"i"(sizeof(ulong))
 +	      : "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rax", "rbx", "rdi"
 +		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
 +#else
 +		, "eax", "ebx", "edi"
 +#endif
 +	      );
  
  	/*
 -	 * We should never reach the point where we are emulating L2
 -	 * due to invalid guest state as that means we incorrectly
 -	 * allowed a nested VMEntry with an invalid vmcs12.
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
  	 */
 -	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
  
 -	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 -	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
 +	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
  
 -	while (vmx->emulation_required && count-- != 0) {
 -		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 -			return handle_interrupt_window(&vmx->vcpu);
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
  
 -		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
 -			return 1;
 +	/* All fields are clean at this point */
 +	if (static_branch_unlikely(&enable_evmcs))
 +		current_evmcs->hv_clean_fields |=
 +			HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
  
 -		err = kvm_emulate_instruction(vcpu, 0);
 +	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 +	if (vmx->host_debugctlmsr)
 +		update_debugctlmsr(vmx->host_debugctlmsr);
  
 -		if (err == EMULATE_USER_EXIT) {
 -			++vcpu->stat.mmio_exits;
 -			ret = 0;
 -			goto out;
 -		}
 +#ifndef CONFIG_X86_64
 +	/*
 +	 * The sysexit path does not restore ds/es, so we must set them to
 +	 * a reasonable value ourselves.
 +	 *
 +	 * We can't defer this to vmx_prepare_switch_to_host() since that
 +	 * function may be executed in interrupt context, which saves and
 +	 * restore segments around it, nullifying its effect.
 +	 */
 +	loadsegment(ds, __USER_DS);
 +	loadsegment(es, __USER_DS);
 +#endif
  
 -		if (err != EMULATE_DONE)
 -			goto emulation_error;
 +	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
 +				  | (1 << VCPU_EXREG_RFLAGS)
 +				  | (1 << VCPU_EXREG_PDPTR)
 +				  | (1 << VCPU_EXREG_SEGMENTS)
 +				  | (1 << VCPU_EXREG_CR3));
 +	vcpu->arch.regs_dirty = 0;
  
 -		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 -		    vcpu->arch.exception.pending)
 -			goto emulation_error;
 +	/*
 +	 * eager fpu is enabled if PKEY is supported and CR4 is switched
 +	 * back on host, so it is safe to read guest PKRU from current
 +	 * XSAVE.
 +	 */
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
 +		vcpu->arch.pkru = __read_pkru();
 +		if (vcpu->arch.pkru != vmx->host_pkru)
 +			__write_pkru(vmx->host_pkru);
 +	}
  
 -		if (vcpu->arch.halt_request) {
 -			vcpu->arch.halt_request = 0;
 -			ret = kvm_vcpu_halt(vcpu);
 -			goto out;
 -		}
 +	vmx->nested.nested_run_pending = 0;
 +	vmx->idt_vectoring_info = 0;
  
 -		if (signal_pending(current))
 -			goto out;
 -		if (need_resched())
 -			schedule();
 -	}
 +	vmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);
 +	if (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		return;
  
 -out:
 -	return ret;
 +	vmx->loaded_vmcs->launched = 1;
 +	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
  
 -emulation_error:
 -	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -	vcpu->run->internal.ndata = 0;
 -	return 0;
 +	vmx_complete_atomic_exit(vmx);
 +	vmx_recover_nmi_blocking(vmx);
 +	vmx_complete_interrupts(vmx);
  }
 +STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
  
 -static void grow_ple_window(struct kvm_vcpu *vcpu)
 +static struct kvm *vmx_vm_alloc(void)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 -
 -	vmx->ple_window = __grow_ple_window(old, ple_window,
 -					    ple_window_grow,
 -					    ple_window_max);
 -
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 +	return &kvm_vmx->kvm;
 +}
  
 -	trace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);
 +static void vmx_vm_free(struct kvm *kvm)
 +{
 +	vfree(to_kvm_vmx(kvm));
  }
  
 -static void shrink_ple_window(struct kvm_vcpu *vcpu)
 +static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 +	int cpu;
  
 -	vmx->ple_window = __shrink_ple_window(old, ple_window,
 -					      ple_window_shrink,
 -					      ple_window);
 +	if (vmx->loaded_vmcs == vmcs)
 +		return;
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	cpu = get_cpu();
 +	vmx_vcpu_put(vcpu);
 +	vmx->loaded_vmcs = vmcs;
 +	vmx_vcpu_load(vcpu, cpu);
 +	put_cpu();
  
 -	trace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);
 +	vm_entry_controls_reset_shadow(vmx);
 +	vm_exit_controls_reset_shadow(vmx);
 +	vmx_segment_cache_clear(vmx);
  }
  
  /*
* Unmerged path arch/x86/kvm/vmx/nested.c
* Unmerged path arch/x86/kvm/vmx/vmcs.h
* Unmerged path arch/x86/kvm/vmx/vmx.h
* Unmerged path arch/x86/kvm/vmx/nested.c
* Unmerged path arch/x86/kvm/vmx/vmcs.h
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/vmx/vmx.h
