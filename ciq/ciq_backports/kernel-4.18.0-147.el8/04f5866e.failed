coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

jira LE-1907
cve CVE-2019-3892
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit 04f5866e41fb70690e28397487d8bd8eea7d712a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/04f5866e.failed

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  "Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct"

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added "Fixes: 86039bd3b4e6"
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other "Fixes:" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the "mm" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 ("userfaultfd: add new syscall to provide memory externalization")
	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Reported-by: Jann Horn <jannh@google.com>
	Suggested-by: Oleg Nesterov <oleg@redhat.com>
	Acked-by: Peter Xu <peterx@redhat.com>
	Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
	Reviewed-by: Oleg Nesterov <oleg@redhat.com>
	Reviewed-by: Jann Horn <jannh@google.com>
	Acked-by: Jason Gunthorpe <jgg@mellanox.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 04f5866e41fb70690e28397487d8bd8eea7d712a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs_main.c
diff --cc drivers/infiniband/core/uverbs_main.c
index 495189811c27,f2e7ffe6fc54..000000000000
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@@ -817,13 -778,241 +817,106 @@@ static int ib_uverbs_mmap(struct file *
  		goto out;
  	}
  
 -	ret = ucontext->device->ops.mmap(ucontext, vma);
++<<<<<<< HEAD
 +	if (!file->ucontext)
 +		ret = -ENODEV;
 +	else
 +		ret = ib_dev->mmap(file->ucontext, vma);
  out:
  	srcu_read_unlock(&file->device->disassociate_srcu, srcu_key);
  	return ret;
 -}
 -
 -/*
 - * Each time we map IO memory into user space this keeps track of the mapping.
 - * When the device is hot-unplugged we 'zap' the mmaps in user space to point
 - * to the zero page and allow the hot unplug to proceed.
 - *
 - * This is necessary for cases like PCI physical hot unplug as the actual BAR
 - * memory may vanish after this and access to it from userspace could MCE.
 - *
 - * RDMA drivers supporting disassociation must have their user space designed
 - * to cope in some way with their IO pages going to the zero page.
 - */
 -struct rdma_umap_priv {
 -	struct vm_area_struct *vma;
 -	struct list_head list;
 -};
 -
 -static const struct vm_operations_struct rdma_umap_ops;
 -
 -static void rdma_umap_priv_init(struct rdma_umap_priv *priv,
 -				struct vm_area_struct *vma)
 -{
 -	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
 -
 -	priv->vma = vma;
 -	vma->vm_private_data = priv;
 -	vma->vm_ops = &rdma_umap_ops;
 -
 -	mutex_lock(&ufile->umap_lock);
 -	list_add(&priv->list, &ufile->umaps);
 -	mutex_unlock(&ufile->umap_lock);
 -}
 -
 -/*
 - * The VMA has been dup'd, initialize the vm_private_data with a new tracking
 - * struct
 - */
 -static void rdma_umap_open(struct vm_area_struct *vma)
 -{
 -	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
 -	struct rdma_umap_priv *opriv = vma->vm_private_data;
 -	struct rdma_umap_priv *priv;
 -
 -	if (!opriv)
 -		return;
 -
 -	/* We are racing with disassociation */
 -	if (!down_read_trylock(&ufile->hw_destroy_rwsem))
 -		goto out_zap;
 -	/*
 -	 * Disassociation already completed, the VMA should already be zapped.
 -	 */
 -	if (!ufile->ucontext)
 -		goto out_unlock;
 -
 -	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 -	if (!priv)
 -		goto out_unlock;
 -	rdma_umap_priv_init(priv, vma);
 -
 -	up_read(&ufile->hw_destroy_rwsem);
 -	return;
 -
 -out_unlock:
 -	up_read(&ufile->hw_destroy_rwsem);
 -out_zap:
 -	/*
 -	 * We can't allow the VMA to be created with the actual IO pages, that
 -	 * would break our API contract, and it can't be stopped at this
 -	 * point, so zap it.
 -	 */
 -	vma->vm_private_data = NULL;
 -	zap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);
 -}
 -
 -static void rdma_umap_close(struct vm_area_struct *vma)
 -{
 -	struct ib_uverbs_file *ufile = vma->vm_file->private_data;
 -	struct rdma_umap_priv *priv = vma->vm_private_data;
 -
 -	if (!priv)
 -		return;
 -
 -	/*
 -	 * The vma holds a reference on the struct file that created it, which
 -	 * in turn means that the ib_uverbs_file is guaranteed to exist at
 -	 * this point.
 -	 */
 -	mutex_lock(&ufile->umap_lock);
 -	list_del(&priv->list);
 -	mutex_unlock(&ufile->umap_lock);
 -	kfree(priv);
 -}
 -
 -static const struct vm_operations_struct rdma_umap_ops = {
 -	.open = rdma_umap_open,
 -	.close = rdma_umap_close,
 -};
 -
 -static struct rdma_umap_priv *rdma_user_mmap_pre(struct ib_ucontext *ucontext,
 -						 struct vm_area_struct *vma,
 -						 unsigned long size)
 -{
 -	struct ib_uverbs_file *ufile = ucontext->ufile;
 -	struct rdma_umap_priv *priv;
 -
 -	if (vma->vm_end - vma->vm_start != size)
 -		return ERR_PTR(-EINVAL);
 -
 -	/* Driver is using this wrong, must be called by ib_uverbs_mmap */
 -	if (WARN_ON(!vma->vm_file ||
 -		    vma->vm_file->private_data != ufile))
 -		return ERR_PTR(-EINVAL);
 -	lockdep_assert_held(&ufile->device->disassociate_srcu);
 -
 -	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 -	if (!priv)
 -		return ERR_PTR(-ENOMEM);
 -	return priv;
 -}
 -
 -/*
 - * Map IO memory into a process. This is to be called by drivers as part of
 - * their mmap() functions if they wish to send something like PCI-E BAR memory
 - * to userspace.
 - */
 -int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
 -		      unsigned long pfn, unsigned long size, pgprot_t prot)
 -{
 -	struct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);
 -
 -	if (IS_ERR(priv))
 -		return PTR_ERR(priv);
 -
 -	vma->vm_page_prot = prot;
 -	if (io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot)) {
 -		kfree(priv);
 -		return -EAGAIN;
 -	}
 -
++=======
+ 	rdma_umap_priv_init(priv, vma);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(rdma_user_mmap_io);
+ 
+ /*
+  * The page case is here for a slightly different reason, the driver expects
+  * to be able to free the page it is sharing to user space when it destroys
+  * its ucontext, which means we need to zap the user space references.
+  *
+  * We could handle this differently by providing an API to allocate a shared
+  * page and then only freeing the shared page when the last ufile is
+  * destroyed.
+  */
+ int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+ 			struct vm_area_struct *vma, struct page *page,
+ 			unsigned long size)
+ {
+ 	struct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);
+ 
+ 	if (IS_ERR(priv))
+ 		return PTR_ERR(priv);
+ 
+ 	if (remap_pfn_range(vma, vma->vm_start, page_to_pfn(page), size,
+ 			    vma->vm_page_prot)) {
+ 		kfree(priv);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	rdma_umap_priv_init(priv, vma);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(rdma_user_mmap_page);
+ 
+ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
+ {
+ 	struct rdma_umap_priv *priv, *next_priv;
+ 
+ 	lockdep_assert_held(&ufile->hw_destroy_rwsem);
+ 
+ 	while (1) {
+ 		struct mm_struct *mm = NULL;
+ 
+ 		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
+ 		mutex_lock(&ufile->umap_lock);
+ 		while (!list_empty(&ufile->umaps)) {
+ 			int ret;
+ 
+ 			priv = list_first_entry(&ufile->umaps,
+ 						struct rdma_umap_priv, list);
+ 			mm = priv->vma->vm_mm;
+ 			ret = mmget_not_zero(mm);
+ 			if (!ret) {
+ 				list_del_init(&priv->list);
+ 				mm = NULL;
+ 				continue;
+ 			}
+ 			break;
+ 		}
+ 		mutex_unlock(&ufile->umap_lock);
+ 		if (!mm)
+ 			return;
+ 
+ 		/*
+ 		 * The umap_lock is nested under mmap_sem since it used within
+ 		 * the vma_ops callbacks, so we have to clean the list one mm
+ 		 * at a time to get the lock ordering right. Typically there
+ 		 * will only be one mm, so no big deal.
+ 		 */
+ 		down_write(&mm->mmap_sem);
+ 		if (!mmget_still_valid(mm))
+ 			goto skip_mm;
+ 		mutex_lock(&ufile->umap_lock);
+ 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
+ 					  list) {
+ 			struct vm_area_struct *vma = priv->vma;
+ 
+ 			if (vma->vm_mm != mm)
+ 				continue;
+ 			list_del_init(&priv->list);
+ 
+ 			zap_vma_ptes(vma, vma->vm_start,
+ 				     vma->vm_end - vma->vm_start);
+ 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
+ 		}
+ 		mutex_unlock(&ufile->umap_lock);
+ 	skip_mm:
+ 		up_write(&mm->mmap_sem);
+ 		mmput(mm);
+ 	}
++>>>>>>> 04f5866e41fb (coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping)
  }
  
  /*
* Unmerged path drivers/infiniband/core/uverbs_main.c
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index dfd73a4616ce..21a3cd4372b4 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -1137,6 +1137,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 					count = -EINTR;
 					goto out_mm;
 				}
+				/*
+				 * Avoid to modify vma->vm_flags
+				 * without locked ops while the
+				 * coredump reads the vm_flags.
+				 */
+				if (!mmget_still_valid(mm)) {
+					/*
+					 * Silently return "count"
+					 * like if get_task_mm()
+					 * failed. FIXME: should this
+					 * function have returned
+					 * -ESRCH if get_task_mm()
+					 * failed like if
+					 * get_proc_task() fails?
+					 */
+					up_write(&mm->mmap_sem);
+					goto out_mm;
+				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index a4037ccdaa1d..df1e3d45aeb4 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -632,6 +632,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,
 
 		/* the various vma->vm_userfaultfd_ctx still points to it */
 		down_write(&mm->mmap_sem);
+		/* no task can run (and in turn coredump) yet */
+		VM_WARN_ON(!mmget_still_valid(mm));
 		for (vma = mm->mmap; vma; vma = vma->vm_next)
 			if (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {
 				vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
@@ -886,6 +888,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 	 * taking the mmap_sem for writing.
 	 */
 	down_write(&mm->mmap_sem);
+	if (!mmget_still_valid(mm))
+		goto skip_mm;
 	prev = NULL;
 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
 		cond_resched();
@@ -908,6 +912,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 		vma->vm_flags = new_flags;
 		vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 	}
+skip_mm:
 	up_write(&mm->mmap_sem);
 	mmput(mm);
 wakeup:
@@ -1336,6 +1341,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,
 		goto out;
 
 	down_write(&mm->mmap_sem);
+	if (!mmget_still_valid(mm))
+		goto out_unlock;
 	vma = find_vma_prev(mm, start, &prev);
 	if (!vma)
 		goto out_unlock;
@@ -1523,6 +1530,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,
 		goto out;
 
 	down_write(&mm->mmap_sem);
+	if (!mmget_still_valid(mm))
+		goto out_unlock;
 	vma = find_vma_prev(mm, start, &prev);
 	if (!vma)
 		goto out_unlock;
diff --git a/include/linux/sched/mm.h b/include/linux/sched/mm.h
index 44d356f5e47c..32c40b2d09d9 100644
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)
 		__mmdrop(mm);
 }
 
+/*
+ * This has to be called after a get_task_mm()/mmget_not_zero()
+ * followed by taking the mmap_sem for writing before modifying the
+ * vmas or anything the coredump pretends not to change from under it.
+ *
+ * NOTE: find_extend_vma() called from GUP context is the only place
+ * that can modify the "mm" (notably the vm_start/end) under mmap_sem
+ * for reading and outside the context of the process, so it is also
+ * the only case that holds the mmap_sem for reading that must call
+ * this function. Generally if the mmap_sem is hold for reading
+ * there's no need of this check after get_task_mm()/mmget_not_zero().
+ *
+ * This function can be obsoleted and the check can be removed, after
+ * the coredump code will hold the mmap_sem for writing before
+ * invoking the ->core_dump methods.
+ */
+static inline bool mmget_still_valid(struct mm_struct *mm)
+{
+	return likely(!mm->core_state);
+}
+
 /**
  * mmget() - Pin the address space associated with a &struct mm_struct.
  * @mm: The address space to pin.
diff --git a/mm/mmap.c b/mm/mmap.c
index f003facc1141..39e109399ba2 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -45,6 +45,7 @@
 #include <linux/moduleparam.h>
 #include <linux/pkeys.h>
 #include <linux/oom.h>
+#include <linux/sched/mm.h>
 
 #include <linux/uaccess.h>
 #include <asm/cacheflush.h>
@@ -2502,7 +2503,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)
 	vma = find_vma_prev(mm, addr, &prev);
 	if (vma && (vma->vm_start <= addr))
 		return vma;
-	if (!prev || expand_stack(prev, addr))
+	/* don't alter vm_end if the coredump is running */
+	if (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))
 		return NULL;
 	if (prev->vm_flags & VM_LOCKED)
 		populate_vma_page_range(prev, addr, prev->vm_end, NULL);
@@ -2528,6 +2530,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)
 		return vma;
 	if (!(vma->vm_flags & VM_GROWSDOWN))
 		return NULL;
+	/* don't alter vm_start if the coredump is running */
+	if (!mmget_still_valid(mm))
+		return NULL;
 	start = vma->vm_start;
 	if (expand_stack(vma, addr))
 		return NULL;
