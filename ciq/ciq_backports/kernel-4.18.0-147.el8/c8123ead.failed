bpf: Extend the sk_lookup() helper to XDP hookpoint.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
Rebuild_CHGLOG: - [net] bpf: Extend the sk_lookup() helper to XDP hookpoint (Yauheni Kaliuta) [1700846]
Rebuild_FUZZ: 99.03%
commit-author Nitin Hande <nitin.hande@gmail.com>
commit c8123ead13a5c92dc5fd15c0fdfe88eef41e6ac1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/c8123ead.failed

This patch proposes to extend the sk_lookup() BPF API to the
XDP hookpoint. The sk_lookup() helper supports a lookup
on incoming packet to find the corresponding socket that will
receive this packet. Current support for this BPF API is
at the tc hookpoint. This patch will extend this API at XDP
hookpoint. A XDP program can map the incoming packet to the
5-tuple parameter and invoke the API to find the corresponding
socket structure.

	Signed-off-by: Nitin Hande <Nitin.Hande@gmail.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit c8123ead13a5c92dc5fd15c0fdfe88eef41e6ac1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	net/core/filter.c
diff --cc include/uapi/linux/bpf.h
index 11269f3807ea,47d606d744cc..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -2118,6 -2161,106 +2118,109 @@@ union bpf_attr 
   *		the shared data.
   *	Return
   *		Pointer to the local storage area.
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_sk_select_reuseport(struct sk_reuseport_md *reuse, struct bpf_map *map, void *key, u64 flags)
+  *	Description
+  *		Select a SO_REUSEPORT sk from a	BPF_MAP_TYPE_REUSEPORT_ARRAY map
+  *		It checks the selected sk is matching the incoming
+  *		request in the skb.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * struct bpf_sock *bpf_sk_lookup_tcp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u32 netns, u64 flags)
+  *	Description
+  *		Look for TCP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is zero, then the socket lookup table in the
+  *		netns associated with the *ctx* will be used. For the TC hooks,
+  *		this in the netns of the device in the skb. For socket hooks,
+  *		this in the netns of the socket. If *netns* is non-zero, then
+  *		it specifies the ID of the netns relative to the netns
+  *		associated with the *ctx*.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * struct bpf_sock *bpf_sk_lookup_udp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u32 netns, u64 flags)
+  *	Description
+  *		Look for UDP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-NULL, released via **bpf_sk_release**\ ().
+  *
+  *		The *ctx* should point to the context of the program, such as
+  *		the skb or socket (depending on the hook in use). This is used
+  *		to determine the base network namespace for the lookup.
+  *
+  *		*tuple_size* must be one of:
+  *
+  *		**sizeof**\ (*tuple*\ **->ipv4**)
+  *			Look for an IPv4 socket.
+  *		**sizeof**\ (*tuple*\ **->ipv6**)
+  *			Look for an IPv6 socket.
+  *
+  *		If the *netns* is zero, then the socket lookup table in the
+  *		netns associated with the *ctx* will be used. For the TC hooks,
+  *		this in the netns of the device in the skb. For socket hooks,
+  *		this in the netns of the socket. If *netns* is non-zero, then
+  *		it specifies the ID of the netns relative to the netns
+  *		associated with the *ctx*.
+  *
+  *		All values for *flags* are reserved for future usage, and must
+  *		be left at zero.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to *struct bpf_sock*, or NULL in case of failure.
+  *		For sockets with reuseport option, *struct bpf_sock*
+  *		return is from reuse->socks[] using hash of the packet.
+  *
+  * int bpf_sk_release(struct bpf_sock *sk)
+  *	Description
+  *		Release the reference held by *sock*. *sock* must be a non-NULL
+  *		pointer that was returned from bpf_sk_lookup_xxx\ ().
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_msg_push_data(struct sk_buff *skb, u32 start, u32 len, u64 flags)
+  *	Description
+  *		For socket policies, insert *len* bytes into msg at offset
+  *		*start*.
+  *
+  *		If a program of type **BPF_PROG_TYPE_SK_MSG** is run on a
+  *		*msg* it may want to insert metadata or options into the msg.
+  *		This can later be read and used by any of the lower layer BPF
+  *		hooks.
+  *
+  *		This helper may fail if under memory pressure (a malloc
+  *		fails) in these cases BPF programs will get an appropriate
+  *		error and BPF programs will need to handle them.
+  *
+  *	Return
+  *		0 on success, or a negative error in case of failure.
++>>>>>>> c8123ead13a5 (bpf: Extend the sk_lookup() helper to XDP hookpoint.)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --cc net/core/filter.c
index ed8de8b22015,53d50fb75ea1..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -4757,6 -4843,208 +4757,211 @@@ static const struct bpf_func_proto bpf_
  };
  #endif /* CONFIG_IPV6_SEG6_BPF */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INET
+ static struct sock *sk_lookup(struct net *net, struct bpf_sock_tuple *tuple,
+ 			      int dif, int sdif, u8 family, u8 proto)
+ {
+ 	bool refcounted = false;
+ 	struct sock *sk = NULL;
+ 
+ 	if (family == AF_INET) {
+ 		__be32 src4 = tuple->ipv4.saddr;
+ 		__be32 dst4 = tuple->ipv4.daddr;
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					   src4, tuple->ipv4.sport,
+ 					   dst4, tuple->ipv4.dport,
+ 					   dif, sdif, &refcounted);
+ 		else
+ 			sk = __udp4_lib_lookup(net, src4, tuple->ipv4.sport,
+ 					       dst4, tuple->ipv4.dport,
+ 					       dif, sdif, &udp_table, NULL);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	} else {
+ 		struct in6_addr *src6 = (struct in6_addr *)&tuple->ipv6.saddr;
+ 		struct in6_addr *dst6 = (struct in6_addr *)&tuple->ipv6.daddr;
+ 		u16 hnum = ntohs(tuple->ipv6.dport);
+ 
+ 		if (proto == IPPROTO_TCP)
+ 			sk = __inet6_lookup(net, &tcp_hashinfo, NULL, 0,
+ 					    src6, tuple->ipv6.sport,
+ 					    dst6, hnum,
+ 					    dif, sdif, &refcounted);
+ 		else if (likely(ipv6_bpf_stub))
+ 			sk = ipv6_bpf_stub->udp6_lib_lookup(net,
+ 							    src6, tuple->ipv6.sport,
+ 							    dst6, hnum,
+ 							    dif, sdif,
+ 							    &udp_table, NULL);
+ #endif
+ 	}
+ 
+ 	if (unlikely(sk && !refcounted && !sock_flag(sk, SOCK_RCU_FREE))) {
+ 		WARN_ONCE(1, "Found non-RCU, unreferenced socket!");
+ 		sk = NULL;
+ 	}
+ 	return sk;
+ }
+ 
+ /* bpf_sk_lookup performs the core lookup for different types of sockets,
+  * taking a reference on the socket if it doesn't have the flag SOCK_RCU_FREE.
+  * Returns the socket as an 'unsigned long' to simplify the casting in the
+  * callers to satisfy BPF_CALL declarations.
+  */
+ static unsigned long
+ __bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 		struct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,
+ 		u64 flags)
+ {
+ 	struct sock *sk = NULL;
+ 	u8 family = AF_UNSPEC;
+ 	struct net *net;
+ 	int sdif;
+ 
+ 	family = len == sizeof(tuple->ipv4) ? AF_INET : AF_INET6;
+ 	if (unlikely(family == AF_UNSPEC || netns_id > U32_MAX || flags))
+ 		goto out;
+ 
+ 	if (family == AF_INET)
+ 		sdif = inet_sdif(skb);
+ 	else
+ 		sdif = inet6_sdif(skb);
+ 
+ 	if (netns_id) {
+ 		net = get_net_ns_by_id(caller_net, netns_id);
+ 		if (unlikely(!net))
+ 			goto out;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 		put_net(net);
+ 	} else {
+ 		net = caller_net;
+ 		sk = sk_lookup(net, tuple, ifindex, sdif, family, proto);
+ 	}
+ 
+ 	if (sk)
+ 		sk = sk_to_full_sk(sk);
+ out:
+ 	return (unsigned long) sk;
+ }
+ 
+ static unsigned long
+ bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,
+ 	      u8 proto, u64 netns_id, u64 flags)
+ {
+ 	struct net *caller_net;
+ 	int ifindex;
+ 
+ 	if (skb->dev) {
+ 		caller_net = dev_net(skb->dev);
+ 		ifindex = skb->dev->ifindex;
+ 	} else {
+ 		caller_net = sock_net(skb->sk);
+ 		ifindex = 0;
+ 	}
+ 
+ 	return __bpf_sk_lookup(skb, tuple, len, caller_net, ifindex,
+ 			      proto, netns_id, flags);
+ }
+ 
+ BPF_CALL_5(bpf_sk_lookup_tcp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_tcp_proto = {
+ 	.func		= bpf_sk_lookup_tcp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_sk_lookup_udp, struct sk_buff *, skb,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)
+ {
+ 	return bpf_sk_lookup(skb, tuple, len, IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_lookup_udp_proto = {
+ 	.func		= bpf_sk_lookup_udp,
+ 	.gpl_only	= false,
+ 	.pkt_access	= true,
+ 	.ret_type	= RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_MEM,
+ 	.arg3_type	= ARG_CONST_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_1(bpf_sk_release, struct sock *, sk)
+ {
+ 	if (!sock_flag(sk, SOCK_RCU_FREE))
+ 		sock_gen_put(sk);
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_sk_release_proto = {
+ 	.func		= bpf_sk_release,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_SOCKET,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_udp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return __bpf_sk_lookup(NULL, tuple, len, caller_net, ifindex,
+ 			      IPPROTO_UDP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_udp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_udp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_5(bpf_xdp_sk_lookup_tcp, struct xdp_buff *, ctx,
+ 	   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)
+ {
+ 	struct net *caller_net = dev_net(ctx->rxq->dev);
+ 	int ifindex = ctx->rxq->dev->ifindex;
+ 
+ 	return __bpf_sk_lookup(NULL, tuple, len, caller_net, ifindex,
+ 			      IPPROTO_TCP, netns_id, flags);
+ }
+ 
+ static const struct bpf_func_proto bpf_xdp_sk_lookup_tcp_proto = {
+ 	.func           = bpf_xdp_sk_lookup_tcp,
+ 	.gpl_only       = false,
+ 	.pkt_access     = true,
+ 	.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_PTR_TO_MEM,
+ 	.arg3_type      = ARG_CONST_SIZE,
+ 	.arg4_type      = ARG_ANYTHING,
+ 	.arg5_type      = ARG_ANYTHING,
+ };
+ #endif /* CONFIG_INET */
+ 
++>>>>>>> c8123ead13a5 (bpf: Extend the sk_lookup() helper to XDP hookpoint.)
  bool bpf_helper_changes_pkt_data(void *func)
  {
  	if (func == bpf_skb_vlan_push ||
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path net/core/filter.c
