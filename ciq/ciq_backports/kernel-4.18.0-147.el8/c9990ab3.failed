RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit c9990ab39b6e911003bab10a6da96e98ab1503a3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/c9990ab3.failed

This is the first step to make ODP use the owning_mm that is now part of
struct ib_umem.

Each ODP umem is linked to a single per_mm structure, which in turn, is
linked to a single mm, via the embedded mmu_notifier. This first patch
introduces the structure and reworks eveything to use it.

This also needs to introduce tgid into the ib_ucontext_per_mm, as
get_user_pages_remote() requires the originating task for statistics
tracking.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit c9990ab39b6e911003bab10a6da96e98ab1503a3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_odp.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/core/umem_odp.c
index 82d979c70f50,6bf3fc0c12a1..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -179,18 -180,20 +180,19 @@@ static int ib_umem_notifier_release_tra
  static void ib_umem_notifier_release(struct mmu_notifier *mn,
  				     struct mm_struct *mm)
  {
- 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
+ 	struct ib_ucontext_per_mm *per_mm =
+ 		container_of(mn, struct ib_ucontext_per_mm, mn);
  
- 	if (!context->invalidate_range)
+ 	if (!per_mm->context->invalidate_range)
  		return;
  
- 	ib_ucontext_notifier_start_account(context);
- 	down_read(&context->umem_rwsem);
- 	rbt_ib_umem_for_each_in_range(&context->umem_tree, 0,
+ 	ib_ucontext_notifier_start_account(per_mm);
+ 	down_read(&per_mm->umem_rwsem);
+ 	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0,
  				      ULLONG_MAX,
  				      ib_umem_notifier_release_trampoline,
 -				      true,
  				      NULL);
- 	up_read(&context->umem_rwsem);
+ 	up_read(&per_mm->umem_rwsem);
  }
  
  static int invalidate_page_trampoline(struct ib_umem_odp *item, u64 start,
@@@ -210,22 -213,32 +212,45 @@@ static int invalidate_range_start_tramp
  	return 0;
  }
  
 -static int ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
 +static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
  						    struct mm_struct *mm,
  						    unsigned long start,
 -						    unsigned long end,
 -						    bool blockable)
 +						    unsigned long end)
  {
++<<<<<<< HEAD
 +	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 +
 +	if (!context->invalidate_range)
 +		return;
 +
 +	ib_ucontext_notifier_start_account(context);
 +	down_read(&context->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&context->umem_tree, start,
 +				      end,
 +				      invalidate_range_start_trampoline, NULL);
 +	up_read(&context->umem_rwsem);
++=======
+ 	struct ib_ucontext_per_mm *per_mm =
+ 		container_of(mn, struct ib_ucontext_per_mm, mn);
+ 	int ret;
+ 
+ 	if (!per_mm->context->invalidate_range)
+ 		return 0;
+ 
+ 	if (blockable)
+ 		down_read(&per_mm->umem_rwsem);
+ 	else if (!down_read_trylock(&per_mm->umem_rwsem))
+ 		return -EAGAIN;
+ 
+ 	ib_ucontext_notifier_start_account(per_mm);
+ 	ret = rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start,
+ 				      end,
+ 				      invalidate_range_start_trampoline,
+ 				      blockable, NULL);
+ 	up_read(&per_mm->umem_rwsem);
+ 
+ 	return ret;
++>>>>>>> c9990ab39b6e (RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm)
  }
  
  static int invalidate_range_end_trampoline(struct ib_umem_odp *item, u64 start,
@@@ -240,17 -253,23 +265,32 @@@ static void ib_umem_notifier_invalidate
  						  unsigned long start,
  						  unsigned long end)
  {
- 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
+ 	struct ib_ucontext_per_mm *per_mm =
+ 		container_of(mn, struct ib_ucontext_per_mm, mn);
  
- 	if (!context->invalidate_range)
+ 	if (!per_mm->context->invalidate_range)
  		return;
  
++<<<<<<< HEAD
 +	down_read(&context->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&context->umem_tree, start,
 +				      end,
 +				      invalidate_range_end_trampoline, NULL);
 +	up_read(&context->umem_rwsem);
 +	ib_ucontext_notifier_end_account(context);
++=======
+ 	/*
+ 	 * TODO: we currently bail out if there is any sleepable work to be done
+ 	 * in ib_umem_notifier_invalidate_range_start so we shouldn't really block
+ 	 * here. But this is ugly and fragile.
+ 	 */
+ 	down_read(&per_mm->umem_rwsem);
+ 	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start,
+ 				      end,
+ 				      invalidate_range_end_trampoline, true, NULL);
+ 	up_read(&per_mm->umem_rwsem);
+ 	ib_ucontext_notifier_end_account(per_mm);
++>>>>>>> c9990ab39b6e (RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm)
  }
  
  static const struct mmu_notifier_ops ib_umem_notifiers = {
@@@ -276,6 -296,8 +317,11 @@@ struct ib_umem_odp *ib_alloc_odp_umem(s
  	umem->address    = addr;
  	umem->page_shift = PAGE_SHIFT;
  	umem->writable   = 1;
++<<<<<<< HEAD
++=======
+ 	umem->is_odp = 1;
+ 	odp_data->per_mm = per_mm = &context->per_mm;
++>>>>>>> c9990ab39b6e (RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm)
  
  	mutex_init(&odp_data->umem_mutex);
  	init_completion(&odp_data->notifier_completion);
@@@ -301,11 -323,9 +347,11 @@@
  		odp_data->mn_counters_active = true;
  	else
  		list_add(&odp_data->no_private_counters,
- 			 &context->no_private_counters);
- 	up_write(&context->umem_rwsem);
+ 			 &per_mm->no_private_counters);
+ 	up_write(&per_mm->umem_rwsem);
  
 +	umem->odp_data = odp_data;
 +
  	return odp_data;
  
  out_page_list:
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 4e6f586dbb7c,9982b5f4e598..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -368,15 -375,15 +375,19 @@@ fail
  static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,
  						u64 io_virt, size_t bcnt)
  {
- 	struct ib_ucontext *ctx = mr->ibmr.pd->uobject->context;
  	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);
  	struct ib_umem_odp *odp, *result = NULL;
 -	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
  	u64 addr = io_virt & MLX5_IMR_MTT_MASK;
  	int nentries = 0, start_idx = 0, ret;
  	struct mlx5_ib_mr *mtt;
  
++<<<<<<< HEAD
 +	mutex_lock(&mr->umem->odp_data->umem_mutex);
 +	odp = odp_lookup(ctx, addr, 1, mr);
++=======
+ 	mutex_lock(&odp_mr->umem_mutex);
+ 	odp = odp_lookup(addr, 1, mr);
++>>>>>>> c9990ab39b6e (RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm)
  
  	mlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",
  		    io_virt, bcnt, addr, odp);
@@@ -386,9 -393,10 +397,10 @@@ next_mr
  		if (nentries)
  			nentries++;
  	} else {
- 		odp = ib_alloc_odp_umem(ctx, addr, MLX5_IMR_MTT_SIZE);
+ 		odp = ib_alloc_odp_umem(odp_mr->umem.context, addr,
+ 					MLX5_IMR_MTT_SIZE);
  		if (IS_ERR(odp)) {
 -			mutex_unlock(&odp_mr->umem_mutex);
 +			mutex_unlock(&mr->umem->odp_data->umem_mutex);
  			return ERR_CAST(odp);
  		}
  
@@@ -485,12 -493,12 +497,19 @@@ static int mr_leaf_free(struct ib_umem_
  
  void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
  {
- 	struct ib_ucontext *ctx = imr->ibmr.pd->uobject->context;
+ 	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
  
++<<<<<<< HEAD
 +	down_read(&ctx->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&ctx->umem_tree, 0, ULLONG_MAX,
 +				      mr_leaf_free, imr);
 +	up_read(&ctx->umem_rwsem);
++=======
+ 	down_read(&per_mm->umem_rwsem);
+ 	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0, ULLONG_MAX,
+ 				      mr_leaf_free, true, imr);
+ 	up_read(&per_mm->umem_rwsem);
++>>>>>>> c9990ab39b6e (RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm)
  
  	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
  }
* Unmerged path drivers/infiniband/core/umem_odp.c
diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c
index 300532a4cde8..cc4af84e5936 100644
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -118,10 +118,11 @@ ssize_t ib_uverbs_get_context(struct ib_uverbs_file *file,
 	ucontext->closing = 0;
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	ucontext->umem_tree = RB_ROOT_CACHED;
-	init_rwsem(&ucontext->umem_rwsem);
-	ucontext->odp_mrs_count = 0;
-	INIT_LIST_HEAD(&ucontext->no_private_counters);
+	ucontext->per_mm.umem_tree = RB_ROOT_CACHED;
+	init_rwsem(&ucontext->per_mm.umem_rwsem);
+	ucontext->per_mm.odp_mrs_count = 0;
+	INIT_LIST_HEAD(&ucontext->per_mm.no_private_counters);
+	ucontext->per_mm.context = ucontext;
 
 	if (!(ib_dev->attrs.device_cap_flags & IB_DEVICE_ON_DEMAND_PAGING))
 		ucontext->invalidate_range = NULL;
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h
index 594840da09ca..63e1cc608d96 100644
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -44,6 +44,8 @@ struct umem_odp_node {
 
 struct ib_umem_odp {
 	struct ib_umem umem;
+	struct ib_ucontext_per_mm *per_mm;
+
 	/*
 	 * An array of the pages included in the on-demand paging umem.
 	 * Indices of pages that are currently not mapped into the device will
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6eada57914aa..ceaed1ac5b6e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1485,6 +1485,25 @@ struct ib_rdmacg_object {
 #endif
 };
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+struct ib_ucontext_per_mm {
+	struct ib_ucontext *context;
+
+	struct rb_root_cached umem_tree;
+	/*
+	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
+	 * mmu notifiers registration.
+	 */
+	struct rw_semaphore umem_rwsem;
+
+	struct mmu_notifier mn;
+	atomic_t notifier_count;
+	/* A list of umems that don't have private mmu notifier counters yet. */
+	struct list_head no_private_counters;
+	unsigned int odp_mrs_count;
+};
+#endif
+
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct ib_uverbs_file  *ufile;
@@ -1499,20 +1518,9 @@ struct ib_ucontext {
 
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	struct rb_root_cached   umem_tree;
-	/*
-	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
-	 * mmu notifiers registration.
-	 */
-	struct rw_semaphore	umem_rwsem;
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
-
-	struct mmu_notifier	mn;
-	atomic_t		notifier_count;
-	/* A list of umems that don't have private mmu notifier counters yet. */
-	struct list_head	no_private_counters;
-	int                     odp_mrs_count;
+	struct ib_ucontext_per_mm per_mm;
 #endif
 
 	struct ib_rdmacg_object	cg_obj;
