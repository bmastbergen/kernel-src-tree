net/mlx5e: Enhance flow counter scheme for offloaded TC eswitch rules

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Shahar Klein <shahark@mellanox.com>
commit f9392795e2e35449a7dca46574b1a86eace20e9c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f9392795.failed

Assign a counter dev attribute according to device capability and use
it for management of counters related to offloaded eswitch TC flows.

With upcoming support for uplink LAG, we have two HW rules per one
logical SW (TC) rule. Although the HW supports attaching one counter
to multiple rules, we are allocating counter per HW rule.

We need this separation for two reasons:

1. "flow eswitch" counter affinity HW require the counter to be
allocated on the device where the eswitch rule is set.

2. for some use-cases (multi-path routing) each HW flow relates to
different neighbour, hence our neigh update logic must have a per-rule
HW accountant in order to provide the proper feedback to the kernel.

	Signed-off-by: Shahar Klein <shahark@mellanox.com>
	Signed-off-by: Roi Dayan <roid@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit f9392795e2e35449a7dca46574b1a86eace20e9c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index c9ee89f4edb1,779ca3a43bec..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -868,36 -994,36 +868,36 @@@ mlx5e_tc_add_fdb_flow(struct mlx5e_pri
  	}
  
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {
- 		counter = mlx5_fc_create(esw->dev, true);
+ 		counter = mlx5_fc_create(attr->counter_dev, true);
  		if (IS_ERR(counter)) {
 -			err = PTR_ERR(counter);
 +			rule = ERR_CAST(counter);
  			goto err_create_counter;
  		}
  
  		attr->counter = counter;
  	}
  
 -	/* we get here if (1) there's no error or when
 +	/* we get here if (1) there's no error (rule being null) or when
  	 * (2) there's an encap action and we're on -EAGAIN (no valid neigh)
  	 */
 -	if (encap_err == -EAGAIN) {
 -		/* continue with goto slow path rule instead */
 -		struct mlx5_esw_flow_attr slow_attr;
 -
 -		flow->rule[0] = mlx5e_tc_offload_to_slow_path(esw, flow, &parse_attr->spec, &slow_attr);
 -	} else {
 -		flow->rule[0] = mlx5e_tc_offload_fdb_rules(esw, flow, &parse_attr->spec, attr);
 -	}
 -
 -	if (IS_ERR(flow->rule[0])) {
 -		err = PTR_ERR(flow->rule[0]);
 -		goto err_add_rule;
 +	if (rule != ERR_PTR(-EAGAIN)) {
 +		rule = mlx5_eswitch_add_offloaded_rule(esw, &parse_attr->spec, attr);
 +		if (IS_ERR(rule))
 +			goto err_add_rule;
 +
 +		if (attr->mirror_count) {
 +			flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, &parse_attr->spec, attr);
 +			if (IS_ERR(flow->rule[1]))
 +				goto err_fwd_rule;
 +		}
  	}
 +	return rule;
  
 -	return 0;
 -
 +err_fwd_rule:
 +	mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
 +	rule = flow->rule[1];
  err_add_rule:
- 	mlx5_fc_destroy(esw->dev, counter);
+ 	mlx5_fc_destroy(attr->counter_dev, counter);
  err_create_counter:
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
  		mlx5e_detach_mod_hdr(priv, flow);
@@@ -2803,47 -2725,264 +2803,190 @@@ int mlx5e_configure_flower(struct mlx5e
  	flow->flags = flow_flags;
  	flow->priv = priv;
  
++<<<<<<< HEAD
 +	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 +	if (err < 0)
 +		goto err_free;
 +
 +	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 +		err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
 +	} else {
 +		err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule[0] = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
++=======
+ 	*__flow = flow;
+ 	*__parse_attr = parse_attr;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ 	return err;
+ }
+ 
+ static int
+ __mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
+ 		     struct tc_cls_flower_offload *f,
+ 		     u16 flow_flags,
+ 		     struct net_device *filter_dev,
+ 		     struct mlx5_eswitch_rep *in_rep,
+ 		     struct mlx5_core_dev *in_mdev,
+ 		     struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_ESWITCH;
+ 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 	parse_attr->filter_dev = filter_dev;
+ 	flow->esw_attr->parse_attr = parse_attr;
+ 	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
+ 			       f, filter_dev);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->esw_attr->chain = f->common.chain_index;
+ 	flow->esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+ 	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->esw_attr->in_rep = in_rep;
+ 	flow->esw_attr->in_mdev = in_mdev;
+ 
+ 	if (MLX5_CAP_ESW(esw->dev, counter_eswitch_affinity) ==
+ 	    MLX5_COUNTER_SOURCE_ESWITCH)
+ 		flow->esw_attr->counter_dev = in_mdev;
+ 	else
+ 		flow->esw_attr->counter_dev = priv->mdev;
+ 
+ 	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int mlx5e_tc_add_fdb_peer_flow(struct tc_cls_flower_offload *f,
+ 				      struct mlx5e_tc_flow *flow)
+ {
+ 	struct mlx5e_priv *priv = flow->priv, *peer_priv;
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch, *peer_esw;
+ 	struct mlx5_devcom *devcom = priv->mdev->priv.devcom;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_rep_priv *peer_urpriv;
+ 	struct mlx5e_tc_flow *peer_flow;
+ 	struct mlx5_core_dev *in_mdev;
+ 	int err = 0;
+ 
+ 	peer_esw = mlx5_devcom_get_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+ 	if (!peer_esw)
+ 		return -ENODEV;
+ 
+ 	peer_urpriv = mlx5_eswitch_get_uplink_priv(peer_esw, REP_ETH);
+ 	peer_priv = netdev_priv(peer_urpriv->netdev);
+ 
+ 	/* in_mdev is assigned of which the packet originated from.
+ 	 * So packets redirected to uplink use the same mdev of the
+ 	 * original flow and packets redirected from uplink use the
+ 	 * peer mdev.
+ 	 */
+ 	if (flow->esw_attr->in_rep->vport == FDB_UPLINK_VPORT)
+ 		in_mdev = peer_priv->mdev;
+ 	else
+ 		in_mdev = priv->mdev;
+ 
+ 	parse_attr = flow->esw_attr->parse_attr;
+ 	err = __mlx5e_add_fdb_flow(peer_priv, f, flow->flags,
+ 				   parse_attr->filter_dev,
+ 				   flow->esw_attr->in_rep, in_mdev, &peer_flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	flow->peer_flow = peer_flow;
+ 	flow->flags |= MLX5E_TC_FLOW_DUP;
+ 	mutex_lock(&esw->offloads.peer_mutex);
+ 	list_add_tail(&flow->peer, &esw->offloads.peer_flows);
+ 	mutex_unlock(&esw->offloads.peer_mutex);
+ 
+ out:
+ 	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct net_device *filter_dev,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	struct mlx5_eswitch_rep *in_rep = rpriv->rep;
+ 	struct mlx5_core_dev *in_mdev = priv->mdev;
+ 	struct mlx5e_tc_flow *flow;
+ 	int err;
+ 
+ 	err = __mlx5e_add_fdb_flow(priv, f, flow_flags, filter_dev, in_rep,
+ 				   in_mdev, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (is_peer_flow_needed(flow)) {
+ 		err = mlx5e_tc_add_fdb_peer_flow(f, flow);
+ 		if (err) {
+ 			mlx5e_tc_del_fdb_flow(priv, flow);
+ 			goto out;
+ 		}
++>>>>>>> f9392795e2e3 (net/mlx5e: Enhance flow counter scheme for offloaded TC eswitch rules)
  	}
  
 -	*__flow = flow;
 -
 -	return 0;
 -
 -out:
 -	return err;
 -}
 -
 -static int
 -mlx5e_add_nic_flow(struct mlx5e_priv *priv,
 -		   struct tc_cls_flower_offload *f,
 -		   u16 flow_flags,
 -		   struct net_device *filter_dev,
 -		   struct mlx5e_tc_flow **__flow)
 -{
 -	struct netlink_ext_ack *extack = f->common.extack;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr;
 -	struct mlx5e_tc_flow *flow;
 -	int attr_size, err;
 -
 -	/* multi-chain not supported for NIC rules */
 -	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
 -		return -EOPNOTSUPP;
 -
 -	flow_flags |= MLX5E_TC_FLOW_NIC;
 -	attr_size  = sizeof(struct mlx5_nic_flow_attr);
 -	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
 -			       &parse_attr, &flow);
 -	if (err)
 -		goto out;
 -
 -	parse_attr->filter_dev = filter_dev;
 -	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
 -			       f, filter_dev);
 -	if (err)
 -		goto err_free;
 -
 -	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow, extack);
 -	if (err)
 -		goto err_free;
 -
 -	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
 -	if (err)
 -		goto err_free;
 -
 -	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 -	kvfree(parse_attr);
 -	*__flow = flow;
 -
 -	return 0;
 -
 -err_free:
 -	kfree(flow);
 -	kvfree(parse_attr);
 -out:
 -	return err;
 -}
 -
 -static int
 -mlx5e_tc_add_flow(struct mlx5e_priv *priv,
 -		  struct tc_cls_flower_offload *f,
 -		  int flags,
 -		  struct net_device *filter_dev,
 -		  struct mlx5e_tc_flow **flow)
 -{
 -	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	u16 flow_flags;
 -	int err;
 -
 -	get_flags(flags, &flow_flags);
 -
 -	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 -		return -EOPNOTSUPP;
 -
 -	if (esw && esw->mode == SRIOV_OFFLOADS)
 -		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
 -					 filter_dev, flow);
 -	else
 -		err = mlx5e_add_nic_flow(priv, f, flow_flags,
 -					 filter_dev, flow);
 -
 -	return err;
 -}
 -
 -int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 -			   struct tc_cls_flower_offload *f, int flags)
 -{
 -	struct netlink_ext_ack *extack = f->common.extack;
 -	struct rhashtable *tc_ht = get_tc_ht(priv);
 -	struct mlx5e_tc_flow *flow;
 -	int err = 0;
 -
 -	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
 -	if (flow) {
 -		NL_SET_ERR_MSG_MOD(extack,
 -				   "flow cookie already exists, ignoring");
 -		netdev_warn_once(priv->netdev,
 -				 "flow cookie %lx already exists, ignoring\n",
 -				 f->cookie);
 -		goto out;
 +	if (IS_ERR(flow->rule[0])) {
 +		err = PTR_ERR(flow->rule[0]);
 +		if (err != -EAGAIN)
 +			goto err_free;
  	}
  
 -	err = mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
 -	if (err)
 -		goto out;
 +	if (err != -EAGAIN)
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 +
 +	if (!(flow->flags & MLX5E_TC_FLOW_ESWITCH) ||
 +	    !(flow->esw_attr->action &
 +	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
 +		kvfree(parse_attr);
  
  	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 -	if (err)
 -		goto err_free;
 +	if (err) {
 +		mlx5e_tc_del_flow(priv, flow);
 +		kfree(flow);
 +	}
  
 -	return 0;
 +	return err;
  
  err_free:
 -	mlx5e_tc_del_flow(priv, flow);
 +	kvfree(parse_attr);
  	kfree(flow);
 -out:
  	return err;
  }
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index de691a49ed92..ed4da6cf655a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@ -285,6 +285,7 @@ struct mlx5_esw_flow_attr {
 	struct mlx5_eswitch_rep *out_rep[MLX5_MAX_FLOW_FWD_VPORTS];
 	struct mlx5_core_dev	*out_mdev[MLX5_MAX_FLOW_FWD_VPORTS];
 	struct mlx5_core_dev	*in_mdev;
+	struct mlx5_core_dev    *counter_dev;
 
 	int mirror_count;
 	int out_count;
