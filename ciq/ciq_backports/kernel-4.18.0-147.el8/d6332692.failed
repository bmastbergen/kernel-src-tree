mm/hibernation: Make hibernation handle unmapped pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Rick Edgecombe <rick.p.edgecombe@intel.com>
commit d63326928611600ad65baff54a70f53b02b3cdfe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/d6332692.failed

Make hibernate handle unmapped pages on the direct map when
CONFIG_ARCH_HAS_SET_ALIAS=y is set. These functions allow for setting pages
to invalid configurations, so now hibernate should check if the pages have
valid mappings and handle if they are unmapped when doing a hibernate
save operation.

Previously this checking was already done when CONFIG_DEBUG_PAGEALLOC=y
was configured. It does not appear to have a big hibernating performance
impact. The speed of the saving operation before this change was measured
as 819.02 MB/s, and after was measured at 813.32 MB/s.

Before:
[    4.670938] PM: Wrote 171996 kbytes in 0.21 seconds (819.02 MB/s)

After:
[    4.504714] PM: Wrote 178932 kbytes in 0.22 seconds (813.32 MB/s)

	Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Pavel Machek <pavel@ucw.cz>
	Cc: <akpm@linux-foundation.org>
	Cc: <ard.biesheuvel@linaro.org>
	Cc: <deneen.t.dock@intel.com>
	Cc: <kernel-hardening@lists.openwall.com>
	Cc: <kristen@linux.intel.com>
	Cc: <linux_dti@icloud.com>
	Cc: <will.deacon@arm.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Nadav Amit <nadav.amit@gmail.com>
	Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20190426001143.4983-16-namit@vmware.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d63326928611600ad65baff54a70f53b02b3cdfe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/pageattr.c
diff --cc arch/x86/mm/pageattr.c
index 4020d9779795,daf4d645e537..000000000000
--- a/arch/x86/mm/pageattr.c
+++ b/arch/x86/mm/pageattr.c
@@@ -2052,6 -2247,16 +2052,19 @@@ static int __set_pages_np(struct page *
  	return __change_page_attr_set_clr(&cpa, 0);
  }
  
++<<<<<<< HEAD
++=======
+ int set_direct_map_invalid_noflush(struct page *page)
+ {
+ 	return __set_pages_np(page, 1);
+ }
+ 
+ int set_direct_map_default_noflush(struct page *page)
+ {
+ 	return __set_pages_p(page, 1);
+ }
+ 
++>>>>>>> d63326928611 (mm/hibernation: Make hibernation handle unmapped pages)
  void __kernel_map_pages(struct page *page, int numpages, int enable)
  {
  	if (PageHighMem(page))
@@@ -2097,13 -2301,10 +2110,17 @@@ bool kernel_page_present(struct page *p
  	pte = lookup_address((unsigned long)page_address(page), &level);
  	return (pte_val(*pte) & _PAGE_PRESENT);
  }
- 
  #endif /* CONFIG_HIBERNATION */
  
++<<<<<<< HEAD
 +#endif /* CONFIG_DEBUG_PAGEALLOC */
 +
 +int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
 +			    unsigned numpages, unsigned long page_flags)
++=======
+ int __init kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
+ 				   unsigned numpages, unsigned long page_flags)
++>>>>>>> d63326928611 (mm/hibernation: Make hibernation handle unmapped pages)
  {
  	int retval = -EINVAL;
  
* Unmerged path arch/x86/mm/pageattr.c
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 26b4440159c9..fff4a27aa4a9 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2643,37 +2643,31 @@ static inline void kernel_poison_pages(struct page *page, int numpages,
 					int enable) { }
 #endif
 
-#ifdef CONFIG_DEBUG_PAGEALLOC
 extern bool _debug_pagealloc_enabled;
-extern void __kernel_map_pages(struct page *page, int numpages, int enable);
 
 static inline bool debug_pagealloc_enabled(void)
 {
-	return _debug_pagealloc_enabled;
+	return IS_ENABLED(CONFIG_DEBUG_PAGEALLOC) && _debug_pagealloc_enabled;
 }
 
+#if defined(CONFIG_DEBUG_PAGEALLOC) || defined(CONFIG_ARCH_HAS_SET_DIRECT_MAP)
+extern void __kernel_map_pages(struct page *page, int numpages, int enable);
+
 static inline void
 kernel_map_pages(struct page *page, int numpages, int enable)
 {
-	if (!debug_pagealloc_enabled())
-		return;
-
 	__kernel_map_pages(page, numpages, enable);
 }
 #ifdef CONFIG_HIBERNATION
 extern bool kernel_page_present(struct page *page);
 #endif	/* CONFIG_HIBERNATION */
-#else	/* CONFIG_DEBUG_PAGEALLOC */
+#else	/* CONFIG_DEBUG_PAGEALLOC || CONFIG_ARCH_HAS_SET_DIRECT_MAP */
 static inline void
 kernel_map_pages(struct page *page, int numpages, int enable) {}
 #ifdef CONFIG_HIBERNATION
 static inline bool kernel_page_present(struct page *page) { return true; }
 #endif	/* CONFIG_HIBERNATION */
-static inline bool debug_pagealloc_enabled(void)
-{
-	return false;
-}
-#endif	/* CONFIG_DEBUG_PAGEALLOC */
+#endif	/* CONFIG_DEBUG_PAGEALLOC || CONFIG_ARCH_HAS_SET_DIRECT_MAP */
 
 #ifdef __HAVE_ARCH_GATE_AREA
 extern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);
diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c
index b2180b5130a6..c6550dbba7e2 100644
--- a/kernel/power/snapshot.c
+++ b/kernel/power/snapshot.c
@@ -1338,8 +1338,9 @@ static inline void do_copy_page(long *dst, long *src)
  * safe_copy_page - Copy a page in a safe way.
  *
  * Check if the page we are going to copy is marked as present in the kernel
- * page tables (this always is the case if CONFIG_DEBUG_PAGEALLOC is not set
- * and in that case kernel_page_present() always returns 'true').
+ * page tables. This always is the case if CONFIG_DEBUG_PAGEALLOC or
+ * CONFIG_ARCH_HAS_SET_DIRECT_MAP is not set. In that case kernel_page_present()
+ * always returns 'true'.
  */
 static void safe_copy_page(void *dst, struct page *s_page)
 {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 950f713e9882..ccf08e5e1acf 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1057,7 +1057,9 @@ static __always_inline bool free_pages_prepare(struct page *page,
 	}
 	arch_free_page(page, order);
 	kernel_poison_pages(page, 1 << order, 0);
-	kernel_map_pages(page, 1 << order, 0);
+	if (debug_pagealloc_enabled())
+		kernel_map_pages(page, 1 << order, 0);
+
 	kasan_free_nondeferred_pages(page, order);
 
 	return true;
@@ -1926,7 +1928,8 @@ inline void post_alloc_hook(struct page *page, unsigned int order,
 	set_page_refcounted(page);
 
 	arch_alloc_page(page, order);
-	kernel_map_pages(page, 1 << order, 1);
+	if (debug_pagealloc_enabled())
+		kernel_map_pages(page, 1 << order, 1);
 	kasan_alloc_pages(page, order);
 	kernel_poison_pages(page, 1 << order, 1);
 	set_page_owner(page, order, gfp_flags);
