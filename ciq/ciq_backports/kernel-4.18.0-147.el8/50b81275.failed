scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Giridhar Malavali <gmalavali@marvell.com>
commit 50b812755e9766fa0a1a28533f4d11a34a5b813e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/50b81275.failed

When SGE buffer containing DIF information crosses 4G boundary, it results
in DMA error. This patch fixes this issue by calculating SGE buffer size
and if it crosses 4G boundary, driver will split it into multiple SGE
buffers to avoid DMA error.

	Signed-off-by: Giridhar Malavali <gmalavali@marvell.com>
	Signed-off-by: Himanshu Madhani <hmadhani@marvell.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 50b812755e9766fa0a1a28533f4d11a34a5b813e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_attr.c
#	drivers/scsi/qla2xxx/qla_gbl.h
diff --cc drivers/scsi/qla2xxx/qla_attr.c
index c8731568f9c4,8b4dd72011bf..000000000000
--- a/drivers/scsi/qla2xxx/qla_attr.c
+++ b/drivers/scsi/qla2xxx/qla_attr.c
@@@ -1513,7 -1632,448 +1513,452 @@@ qla2x00_max_speed_sup_show(struct devic
  	    ha->max_speed_sup ? "32Gps" : "16Gps");
  }
  
++<<<<<<< HEAD
 +static DEVICE_ATTR(driver_version, S_IRUGO, qla2x00_drvr_version_show, NULL);
++=======
+ /* ----- */
+ 
+ static ssize_t
+ qlini_mode_show(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int len = 0;
+ 
+ 	len += scnprintf(buf + len, PAGE_SIZE-len,
+ 	    "Supported options: enabled | disabled | dual | exclusive\n");
+ 
+ 	/* --- */
+ 	len += scnprintf(buf + len, PAGE_SIZE-len, "Current selection: ");
+ 
+ 	switch (vha->qlini_mode) {
+ 	case QLA2XXX_INI_MODE_EXCLUSIVE:
+ 		len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		    QLA2XXX_INI_MODE_STR_EXCLUSIVE);
+ 		break;
+ 	case QLA2XXX_INI_MODE_DISABLED:
+ 		len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		    QLA2XXX_INI_MODE_STR_DISABLED);
+ 		break;
+ 	case QLA2XXX_INI_MODE_ENABLED:
+ 		len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		    QLA2XXX_INI_MODE_STR_ENABLED);
+ 		break;
+ 	case QLA2XXX_INI_MODE_DUAL:
+ 		len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		    QLA2XXX_INI_MODE_STR_DUAL);
+ 		break;
+ 	}
+ 	len += scnprintf(buf + len, PAGE_SIZE-len, "\n");
+ 
+ 	return len;
+ }
+ 
+ static char *mode_to_str[] = {
+ 	"exclusive",
+ 	"disabled",
+ 	"enabled",
+ 	"dual",
+ };
+ 
+ #define NEED_EXCH_OFFLOAD(_exchg) ((_exchg) > FW_DEF_EXCHANGES_CNT)
+ static int qla_set_ini_mode(scsi_qla_host_t *vha, int op)
+ {
+ 	int rc = 0;
+ 	enum {
+ 		NO_ACTION,
+ 		MODE_CHANGE_ACCEPT,
+ 		MODE_CHANGE_NO_ACTION,
+ 		TARGET_STILL_ACTIVE,
+ 	};
+ 	int action = NO_ACTION;
+ 	int set_mode = 0;
+ 	u8  eo_toggle = 0;	/* exchange offload flipped */
+ 
+ 	switch (vha->qlini_mode) {
+ 	case QLA2XXX_INI_MODE_DISABLED:
+ 		switch (op) {
+ 		case QLA2XXX_INI_MODE_DISABLED:
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 				if (((vha->ql2xexchoffld !=
+ 				    vha->u_ql2xexchoffld) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld)) ||
+ 				    eo_toggle) {
+ 					/*
+ 					 * The number of exchange to be offload
+ 					 * was tweaked or offload option was
+ 					 * flipped
+ 					 */
+ 					action = MODE_CHANGE_ACCEPT;
+ 				} else {
+ 					action = MODE_CHANGE_NO_ACTION;
+ 				}
+ 			} else {
+ 				action = MODE_CHANGE_NO_ACTION;
+ 			}
+ 			break;
+ 		case QLA2XXX_INI_MODE_EXCLUSIVE:
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 				if (((vha->ql2xexchoffld !=
+ 				    vha->u_ql2xexchoffld) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld)) ||
+ 				    eo_toggle) {
+ 					/*
+ 					 * The number of exchange to be offload
+ 					 * was tweaked or offload option was
+ 					 * flipped
+ 					 */
+ 					action = MODE_CHANGE_ACCEPT;
+ 				} else {
+ 					action = MODE_CHANGE_NO_ACTION;
+ 				}
+ 			} else {
+ 				action = MODE_CHANGE_ACCEPT;
+ 			}
+ 			break;
+ 		case QLA2XXX_INI_MODE_DUAL:
+ 			action = MODE_CHANGE_ACCEPT;
+ 			/* active_mode is target only, reset it to dual */
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				set_mode = 1;
+ 				action = MODE_CHANGE_ACCEPT;
+ 			} else {
+ 				action = MODE_CHANGE_NO_ACTION;
+ 			}
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_ENABLED:
+ 			if (qla_tgt_mode_enabled(vha))
+ 				action = TARGET_STILL_ACTIVE;
+ 			else {
+ 				action = MODE_CHANGE_ACCEPT;
+ 				set_mode = 1;
+ 			}
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case QLA2XXX_INI_MODE_EXCLUSIVE:
+ 		switch (op) {
+ 		case QLA2XXX_INI_MODE_EXCLUSIVE:
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 				if (((vha->ql2xexchoffld !=
+ 				    vha->u_ql2xexchoffld) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld)) ||
+ 				    eo_toggle)
+ 					/*
+ 					 * The number of exchange to be offload
+ 					 * was tweaked or offload option was
+ 					 * flipped
+ 					 */
+ 					action = MODE_CHANGE_ACCEPT;
+ 				else
+ 					action = NO_ACTION;
+ 			} else
+ 				action = NO_ACTION;
+ 
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_DISABLED:
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 				if (((vha->ql2xexchoffld !=
+ 				      vha->u_ql2xexchoffld) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld)) ||
+ 				    eo_toggle)
+ 					action = MODE_CHANGE_ACCEPT;
+ 				else
+ 					action = MODE_CHANGE_NO_ACTION;
+ 			} else
+ 				action = MODE_CHANGE_NO_ACTION;
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_DUAL: /* exclusive -> dual */
+ 			if (qla_tgt_mode_enabled(vha)) {
+ 				action = MODE_CHANGE_ACCEPT;
+ 				set_mode = 1;
+ 			} else
+ 				action = MODE_CHANGE_ACCEPT;
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_ENABLED:
+ 			if (qla_tgt_mode_enabled(vha))
+ 				action = TARGET_STILL_ACTIVE;
+ 			else {
+ 				if (vha->hw->flags.fw_started)
+ 					action = MODE_CHANGE_NO_ACTION;
+ 				else
+ 					action = MODE_CHANGE_ACCEPT;
+ 			}
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case QLA2XXX_INI_MODE_ENABLED:
+ 		switch (op) {
+ 		case QLA2XXX_INI_MODE_ENABLED:
+ 			if (NEED_EXCH_OFFLOAD(vha->u_ql2xiniexchg) !=
+ 			    vha->hw->flags.exchoffld_enabled)
+ 				eo_toggle = 1;
+ 			if (((vha->ql2xiniexchg != vha->u_ql2xiniexchg) &&
+ 				NEED_EXCH_OFFLOAD(vha->u_ql2xiniexchg)) ||
+ 			    eo_toggle)
+ 				action = MODE_CHANGE_ACCEPT;
+ 			else
+ 				action = NO_ACTION;
+ 			break;
+ 		case QLA2XXX_INI_MODE_DUAL:
+ 		case QLA2XXX_INI_MODE_DISABLED:
+ 			action = MODE_CHANGE_ACCEPT;
+ 			break;
+ 		default:
+ 			action = MODE_CHANGE_NO_ACTION;
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case QLA2XXX_INI_MODE_DUAL:
+ 		switch (op) {
+ 		case QLA2XXX_INI_MODE_DUAL:
+ 			if (qla_tgt_mode_enabled(vha) ||
+ 			    qla_dual_mode_enabled(vha)) {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld +
+ 					vha->u_ql2xiniexchg) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 
+ 				if ((((vha->ql2xexchoffld +
+ 				       vha->ql2xiniexchg) !=
+ 				    (vha->u_ql2xiniexchg +
+ 				     vha->u_ql2xexchoffld)) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xiniexchg +
+ 					vha->u_ql2xexchoffld)) || eo_toggle)
+ 					action = MODE_CHANGE_ACCEPT;
+ 				else
+ 					action = NO_ACTION;
+ 			} else {
+ 				if (NEED_EXCH_OFFLOAD(vha->u_ql2xexchoffld +
+ 					vha->u_ql2xiniexchg) !=
+ 				    vha->hw->flags.exchoffld_enabled)
+ 					eo_toggle = 1;
+ 
+ 				if ((((vha->ql2xexchoffld + vha->ql2xiniexchg)
+ 				    != (vha->u_ql2xiniexchg +
+ 					vha->u_ql2xexchoffld)) &&
+ 				    NEED_EXCH_OFFLOAD(vha->u_ql2xiniexchg +
+ 					vha->u_ql2xexchoffld)) || eo_toggle)
+ 					action = MODE_CHANGE_NO_ACTION;
+ 				else
+ 					action = NO_ACTION;
+ 			}
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_DISABLED:
+ 			if (qla_tgt_mode_enabled(vha) ||
+ 			    qla_dual_mode_enabled(vha)) {
+ 				/* turning off initiator mode */
+ 				set_mode = 1;
+ 				action = MODE_CHANGE_ACCEPT;
+ 			} else {
+ 				action = MODE_CHANGE_NO_ACTION;
+ 			}
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_EXCLUSIVE:
+ 			if (qla_tgt_mode_enabled(vha) ||
+ 			    qla_dual_mode_enabled(vha)) {
+ 				set_mode = 1;
+ 				action = MODE_CHANGE_ACCEPT;
+ 			} else {
+ 				action = MODE_CHANGE_ACCEPT;
+ 			}
+ 			break;
+ 
+ 		case QLA2XXX_INI_MODE_ENABLED:
+ 			if (qla_tgt_mode_enabled(vha) ||
+ 			    qla_dual_mode_enabled(vha)) {
+ 				action = TARGET_STILL_ACTIVE;
+ 			} else {
+ 				action = MODE_CHANGE_ACCEPT;
+ 			}
+ 		}
+ 		break;
+ 	}
+ 
+ 	switch (action) {
+ 	case MODE_CHANGE_ACCEPT:
+ 		ql_log(ql_log_warn, vha, 0xffff,
+ 		    "Mode change accepted. From %s to %s, Tgt exchg %d|%d. ini exchg %d|%d\n",
+ 		    mode_to_str[vha->qlini_mode], mode_to_str[op],
+ 		    vha->ql2xexchoffld, vha->u_ql2xexchoffld,
+ 		    vha->ql2xiniexchg, vha->u_ql2xiniexchg);
+ 
+ 		vha->qlini_mode = op;
+ 		vha->ql2xexchoffld = vha->u_ql2xexchoffld;
+ 		vha->ql2xiniexchg = vha->u_ql2xiniexchg;
+ 		if (set_mode)
+ 			qlt_set_mode(vha);
+ 		vha->flags.online = 1;
+ 		set_bit(ISP_ABORT_NEEDED, &vha->dpc_flags);
+ 		break;
+ 
+ 	case MODE_CHANGE_NO_ACTION:
+ 		ql_log(ql_log_warn, vha, 0xffff,
+ 		    "Mode is set. No action taken. From %s to %s, Tgt exchg %d|%d. ini exchg %d|%d\n",
+ 		    mode_to_str[vha->qlini_mode], mode_to_str[op],
+ 		    vha->ql2xexchoffld, vha->u_ql2xexchoffld,
+ 		    vha->ql2xiniexchg, vha->u_ql2xiniexchg);
+ 		vha->qlini_mode = op;
+ 		vha->ql2xexchoffld = vha->u_ql2xexchoffld;
+ 		vha->ql2xiniexchg = vha->u_ql2xiniexchg;
+ 		break;
+ 
+ 	case TARGET_STILL_ACTIVE:
+ 		ql_log(ql_log_warn, vha, 0xffff,
+ 		    "Target Mode is active. Unable to change Mode.\n");
+ 		break;
+ 
+ 	case NO_ACTION:
+ 	default:
+ 		ql_log(ql_log_warn, vha, 0xffff,
+ 		    "Mode unchange. No action taken. %d|%d pct %d|%d.\n",
+ 		    vha->qlini_mode, op,
+ 		    vha->ql2xexchoffld, vha->u_ql2xexchoffld);
+ 		break;
+ 	}
+ 
+ 	return rc;
+ }
+ 
+ static ssize_t
+ qlini_mode_store(struct device *dev, struct device_attribute *attr,
+     const char *buf, size_t count)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int ini;
+ 
+ 	if (!buf)
+ 		return -EINVAL;
+ 
+ 	if (strncasecmp(QLA2XXX_INI_MODE_STR_EXCLUSIVE, buf,
+ 		strlen(QLA2XXX_INI_MODE_STR_EXCLUSIVE)) == 0)
+ 		ini = QLA2XXX_INI_MODE_EXCLUSIVE;
+ 	else if (strncasecmp(QLA2XXX_INI_MODE_STR_DISABLED, buf,
+ 		strlen(QLA2XXX_INI_MODE_STR_DISABLED)) == 0)
+ 		ini = QLA2XXX_INI_MODE_DISABLED;
+ 	else if (strncasecmp(QLA2XXX_INI_MODE_STR_ENABLED, buf,
+ 		  strlen(QLA2XXX_INI_MODE_STR_ENABLED)) == 0)
+ 		ini = QLA2XXX_INI_MODE_ENABLED;
+ 	else if (strncasecmp(QLA2XXX_INI_MODE_STR_DUAL, buf,
+ 		strlen(QLA2XXX_INI_MODE_STR_DUAL)) == 0)
+ 		ini = QLA2XXX_INI_MODE_DUAL;
+ 	else
+ 		return -EINVAL;
+ 
+ 	qla_set_ini_mode(vha, ini);
+ 	return strlen(buf);
+ }
+ 
+ static ssize_t
+ ql2xexchoffld_show(struct device *dev, struct device_attribute *attr,
+     char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int len = 0;
+ 
+ 	len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		"target exchange: new %d : current: %d\n\n",
+ 		vha->u_ql2xexchoffld, vha->ql2xexchoffld);
+ 
+ 	len += scnprintf(buf + len, PAGE_SIZE-len,
+ 	    "Please (re)set operating mode via \"/sys/class/scsi_host/host%ld/qlini_mode\" to load new setting.\n",
+ 	    vha->host_no);
+ 
+ 	return len;
+ }
+ 
+ static ssize_t
+ ql2xexchoffld_store(struct device *dev, struct device_attribute *attr,
+     const char *buf, size_t count)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int val = 0;
+ 
+ 	if (sscanf(buf, "%d", &val) != 1)
+ 		return -EINVAL;
+ 
+ 	if (val > FW_MAX_EXCHANGES_CNT)
+ 		val = FW_MAX_EXCHANGES_CNT;
+ 	else if (val < 0)
+ 		val = 0;
+ 
+ 	vha->u_ql2xexchoffld = val;
+ 	return strlen(buf);
+ }
+ 
+ static ssize_t
+ ql2xiniexchg_show(struct device *dev, struct device_attribute *attr,
+     char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int len = 0;
+ 
+ 	len += scnprintf(buf + len, PAGE_SIZE-len,
+ 		"target exchange: new %d : current: %d\n\n",
+ 		vha->u_ql2xiniexchg, vha->ql2xiniexchg);
+ 
+ 	len += scnprintf(buf + len, PAGE_SIZE-len,
+ 	    "Please (re)set operating mode via \"/sys/class/scsi_host/host%ld/qlini_mode\" to load new setting.\n",
+ 	    vha->host_no);
+ 
+ 	return len;
+ }
+ 
+ static ssize_t
+ ql2xiniexchg_store(struct device *dev, struct device_attribute *attr,
+     const char *buf, size_t count)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	int val = 0;
+ 
+ 	if (sscanf(buf, "%d", &val) != 1)
+ 		return -EINVAL;
+ 
+ 	if (val > FW_MAX_EXCHANGES_CNT)
+ 		val = FW_MAX_EXCHANGES_CNT;
+ 	else if (val < 0)
+ 		val = 0;
+ 
+ 	vha->u_ql2xiniexchg = val;
+ 	return strlen(buf);
+ }
+ 
+ static ssize_t
+ qla2x00_dif_bundle_statistics_show(struct device *dev,
+     struct device_attribute *attr, char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	struct qla_hw_data *ha = vha->hw;
+ 
+ 	return scnprintf(buf, PAGE_SIZE,
+ 	    "cross=%llu read=%llu write=%llu kalloc=%llu dma_alloc=%llu unusable=%u\n",
+ 	    ha->dif_bundle_crossed_pages, ha->dif_bundle_reads,
+ 	    ha->dif_bundle_writes, ha->dif_bundle_kallocs,
+ 	    ha->dif_bundle_dma_allocs, ha->pool.unusable.count);
+ }
+ 
+ static DEVICE_ATTR(driver_version, S_IRUGO, qla2x00_driver_version_show, NULL);
++>>>>>>> 50b812755e97 (scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary)
  static DEVICE_ATTR(fw_version, S_IRUGO, qla2x00_fw_version_show, NULL);
  static DEVICE_ATTR(serial_num, S_IRUGO, qla2x00_serial_num_show, NULL);
  static DEVICE_ATTR(isp_name, S_IRUGO, qla2x00_isp_name_show, NULL);
@@@ -1560,6 -2120,15 +2005,18 @@@ static DEVICE_ATTR(allow_cna_fw_dump, S
  static DEVICE_ATTR(pep_version, S_IRUGO, qla2x00_pep_version_show, NULL);
  static DEVICE_ATTR(min_link_speed, S_IRUGO, qla2x00_min_link_speed_show, NULL);
  static DEVICE_ATTR(max_speed_sup, S_IRUGO, qla2x00_max_speed_sup_show, NULL);
++<<<<<<< HEAD
++=======
+ static DEVICE_ATTR(zio_threshold, 0644,
+     qla_zio_threshold_show,
+     qla_zio_threshold_store);
+ static DEVICE_ATTR_RW(qlini_mode);
+ static DEVICE_ATTR_RW(ql2xexchoffld);
+ static DEVICE_ATTR_RW(ql2xiniexchg);
+ static DEVICE_ATTR(dif_bundle_statistics, 0444,
+     qla2x00_dif_bundle_statistics_show, NULL);
+ 
++>>>>>>> 50b812755e97 (scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary)
  
  struct device_attribute *qla2x00_host_attrs[] = {
  	&dev_attr_driver_version,
@@@ -1596,6 -2165,11 +2053,14 @@@
  	&dev_attr_pep_version,
  	&dev_attr_min_link_speed,
  	&dev_attr_max_speed_sup,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_zio_threshold,
+ 	&dev_attr_dif_bundle_statistics,
+ 	NULL, /* reserve for qlini_mode */
+ 	NULL, /* reserve for ql2xiniexchg */
+ 	NULL, /* reserve for ql2xexchoffld */
++>>>>>>> 50b812755e97 (scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary)
  	NULL,
  };
  
diff --cc drivers/scsi/qla2xxx/qla_gbl.h
index e792c390f32e,bcc17a7261e7..000000000000
--- a/drivers/scsi/qla2xxx/qla_gbl.h
+++ b/drivers/scsi/qla2xxx/qla_gbl.h
@@@ -159,6 -159,8 +159,11 @@@ extern int ql2xnvmeenable
  extern int ql2xautodetectsfp;
  extern int ql2xenablemsix;
  extern int qla2xuseresexchforels;
++<<<<<<< HEAD
++=======
+ extern int ql2xexlogins;
+ extern int ql2xdifbundlinginternalbuffers;
++>>>>>>> 50b812755e97 (scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary)
  
  extern int qla2x00_loop_reset(scsi_qla_host_t *);
  extern void qla2x00_abort_all_cmds(scsi_qla_host_t *, int);
* Unmerged path drivers/scsi/qla2xxx/qla_attr.c
diff --git a/drivers/scsi/qla2xxx/qla_def.h b/drivers/scsi/qla2xxx/qla_def.h
index d96e90214a6e..c75d37a81ebc 100644
--- a/drivers/scsi/qla2xxx/qla_def.h
+++ b/drivers/scsi/qla2xxx/qla_def.h
@@ -314,6 +314,7 @@ struct srb_cmd {
 #define SRB_CRC_PROT_DMA_VALID		BIT_4	/* DIF: prot DMA valid */
 #define SRB_CRC_CTX_DSD_VALID		BIT_5	/* DIF: dsd_list valid */
 #define SRB_WAKEUP_ON_COMP		BIT_6
+#define SRB_DIF_BUNDL_DMA_VALID		BIT_7   /* DIF: DMA list valid */
 
 /* To identify if a srb is of T10-CRC type. @sp => srb_t pointer */
 #define IS_PROT_IO(sp)	(sp->flags & SRB_CRC_CTX_DSD_VALID)
@@ -1888,6 +1889,13 @@ struct crc_context {
 	/* List of DMA context transfers */
 	struct list_head dsd_list;
 
+	/* List of DIF Bundling context DMA address */
+	struct list_head ldif_dsd_list;
+	u8 no_ldif_dsd;
+
+	struct list_head ldif_dma_hndl_list;
+	u32 dif_bundl_len;
+	u8 no_dif_bundl;
 	/* This structure should not exceed 512 bytes */
 };
 
@@ -4173,6 +4181,26 @@ struct qla_hw_data {
 	uint16_t min_link_speed;
 	uint16_t max_speed_sup;
 
+	/* DMA pool for the DIF bundling buffers */
+	struct dma_pool *dif_bundl_pool;
+	#define DIF_BUNDLING_DMA_POOL_SIZE  1024
+	struct {
+		struct {
+			struct list_head head;
+			uint count;
+		} good;
+		struct {
+			struct list_head head;
+			uint count;
+		} unusable;
+	} pool;
+
+	unsigned long long dif_bundle_crossed_pages;
+	unsigned long long dif_bundle_reads;
+	unsigned long long dif_bundle_writes;
+	unsigned long long dif_bundle_kallocs;
+	unsigned long long dif_bundle_dma_allocs;
+
 	atomic_t        nvme_active_aen_cnt;
 	uint16_t        nvme_last_rptd_aen;             /* Last recorded aen count */
 };
* Unmerged path drivers/scsi/qla2xxx/qla_gbl.h
diff --git a/drivers/scsi/qla2xxx/qla_iocb.c b/drivers/scsi/qla2xxx/qla_iocb.c
index e1298209664a..420d25605139 100644
--- a/drivers/scsi/qla2xxx/qla_iocb.c
+++ b/drivers/scsi/qla2xxx/qla_iocb.c
@@ -1098,88 +1098,300 @@ qla24xx_walk_and_build_sglist(struct qla_hw_data *ha, srb_t *sp, uint32_t *dsd,
 
 int
 qla24xx_walk_and_build_prot_sglist(struct qla_hw_data *ha, srb_t *sp,
-	uint32_t *dsd, uint16_t tot_dsds, struct qla_tc_param *tc)
+    uint32_t *cur_dsd, uint16_t tot_dsds, struct qla_tgt_cmd *tc)
 {
-	void *next_dsd;
-	uint8_t avail_dsds = 0;
-	uint32_t dsd_list_len;
-	struct dsd_dma *dsd_ptr;
+	struct dsd_dma *dsd_ptr = NULL, *dif_dsd, *nxt_dsd;
 	struct scatterlist *sg, *sgl;
-	int	i;
-	struct scsi_cmnd *cmd;
-	uint32_t *cur_dsd = dsd;
-	uint16_t used_dsds = tot_dsds;
+	struct crc_context *difctx = NULL;
 	struct scsi_qla_host *vha;
+	uint dsd_list_len;
+	uint avail_dsds = 0;
+	uint used_dsds = tot_dsds;
+	bool dif_local_dma_alloc = false;
+	bool direction_to_device = false;
+	int i;
 
 	if (sp) {
-		cmd = GET_CMD_SP(sp);
+		struct scsi_cmnd *cmd = GET_CMD_SP(sp);
 		sgl = scsi_prot_sglist(cmd);
 		vha = sp->vha;
+		difctx = sp->u.scmd.ctx;
+		direction_to_device = cmd->sc_data_direction == DMA_TO_DEVICE;
+		ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe021,
+		  "%s: scsi_cmnd: %p, crc_ctx: %p, sp: %p\n",
+			__func__, cmd, difctx, sp);
 	} else if (tc) {
 		vha = tc->vha;
 		sgl = tc->prot_sg;
+		difctx = tc->ctx;
+		direction_to_device = tc->dma_data_direction == DMA_TO_DEVICE;
 	} else {
 		BUG();
 		return 1;
 	}
 
-	ql_dbg(ql_dbg_tgt, vha, 0xe021,
-		"%s: enter\n", __func__);
-
-	for_each_sg(sgl, sg, tot_dsds, i) {
-		dma_addr_t	sle_dma;
-
-		/* Allocate additional continuation packets? */
-		if (avail_dsds == 0) {
-			avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
-						QLA_DSDS_PER_IOCB : used_dsds;
-			dsd_list_len = (avail_dsds + 1) * 12;
-			used_dsds -= avail_dsds;
-
-			/* allocate tracking DS */
-			dsd_ptr = kzalloc(sizeof(struct dsd_dma), GFP_ATOMIC);
-			if (!dsd_ptr)
-				return 1;
-
-			/* allocate new list */
-			dsd_ptr->dsd_addr = next_dsd =
-			    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
-				&dsd_ptr->dsd_list_dma);
-
-			if (!next_dsd) {
-				/*
-				 * Need to cleanup only this dsd_ptr, rest
-				 * will be done by sp_free_dma()
-				 */
-				kfree(dsd_ptr);
-				return 1;
+	ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe021,
+	    "%s: enter (write=%u)\n", __func__, direction_to_device);
+
+	/* if initiator doing write or target doing read */
+	if (direction_to_device) {
+		for_each_sg(sgl, sg, tot_dsds, i) {
+			dma_addr_t sle_phys = sg_phys(sg);
+
+			/* If SGE addr + len flips bits in upper 32-bits */
+			if (MSD(sle_phys + sg->length) ^ MSD(sle_phys)) {
+				ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe022,
+				    "%s: page boundary crossing (phys=%llx len=%x)\n",
+				    __func__, sle_phys, sg->length);
+
+				if (difctx) {
+					ha->dif_bundle_crossed_pages++;
+					dif_local_dma_alloc = true;
+				} else {
+					ql_dbg(ql_dbg_tgt + ql_dbg_verbose,
+					    vha, 0xe022,
+					    "%s: difctx pointer is NULL\n",
+					    __func__);
+				}
+				break;
+			}
+		}
+		ha->dif_bundle_writes++;
+	} else {
+		ha->dif_bundle_reads++;
+	}
+
+	if (ql2xdifbundlinginternalbuffers)
+		dif_local_dma_alloc = direction_to_device;
+
+	if (dif_local_dma_alloc) {
+		u32 track_difbundl_buf = 0;
+		u32 ldma_sg_len = 0;
+		u8 ldma_needed = 1;
+
+		difctx->no_dif_bundl = 0;
+		difctx->dif_bundl_len = 0;
+
+		/* Track DSD buffers */
+		INIT_LIST_HEAD(&difctx->ldif_dsd_list);
+		/* Track local DMA buffers */
+		INIT_LIST_HEAD(&difctx->ldif_dma_hndl_list);
+
+		for_each_sg(sgl, sg, tot_dsds, i) {
+			u32 sglen = sg_dma_len(sg);
+
+			ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe023,
+			    "%s: sg[%x] (phys=%llx sglen=%x) ldma_sg_len: %x dif_bundl_len: %x ldma_needed: %x\n",
+			    __func__, i, sg_phys(sg), sglen, ldma_sg_len,
+			    difctx->dif_bundl_len, ldma_needed);
+
+			while (sglen) {
+				u32 xfrlen = 0;
+
+				if (ldma_needed) {
+					/*
+					 * Allocate list item to store
+					 * the DMA buffers
+					 */
+					dsd_ptr = kzalloc(sizeof(*dsd_ptr),
+					    GFP_ATOMIC);
+					if (!dsd_ptr) {
+						ql_dbg(ql_dbg_tgt, vha, 0xe024,
+						    "%s: failed alloc dsd_ptr\n",
+						    __func__);
+						return 1;
+					}
+					ha->dif_bundle_kallocs++;
+
+					/* allocate dma buffer */
+					dsd_ptr->dsd_addr = dma_pool_alloc
+						(ha->dif_bundl_pool, GFP_ATOMIC,
+						 &dsd_ptr->dsd_list_dma);
+					if (!dsd_ptr->dsd_addr) {
+						ql_dbg(ql_dbg_tgt, vha, 0xe024,
+						    "%s: failed alloc ->dsd_ptr\n",
+						    __func__);
+						/*
+						 * need to cleanup only this
+						 * dsd_ptr rest will be done
+						 * by sp_free_dma()
+						 */
+						kfree(dsd_ptr);
+						ha->dif_bundle_kallocs--;
+						return 1;
+					}
+					ha->dif_bundle_dma_allocs++;
+					ldma_needed = 0;
+					difctx->no_dif_bundl++;
+					list_add_tail(&dsd_ptr->list,
+					    &difctx->ldif_dma_hndl_list);
+				}
+
+				/* xfrlen is min of dma pool size and sglen */
+				xfrlen = (sglen >
+				   (DIF_BUNDLING_DMA_POOL_SIZE - ldma_sg_len)) ?
+				    DIF_BUNDLING_DMA_POOL_SIZE - ldma_sg_len :
+				    sglen;
+
+				/* replace with local allocated dma buffer */
+				sg_pcopy_to_buffer(sgl, sg_nents(sgl),
+				    dsd_ptr->dsd_addr + ldma_sg_len, xfrlen,
+				    difctx->dif_bundl_len);
+				difctx->dif_bundl_len += xfrlen;
+				sglen -= xfrlen;
+				ldma_sg_len += xfrlen;
+				if (ldma_sg_len == DIF_BUNDLING_DMA_POOL_SIZE ||
+				    sg_is_last(sg)) {
+					ldma_needed = 1;
+					ldma_sg_len = 0;
+				}
 			}
+		}
 
-			if (sp) {
-				list_add_tail(&dsd_ptr->list,
-				    &((struct crc_context *)
-					    sp->u.scmd.ctx)->dsd_list);
+		track_difbundl_buf = used_dsds = difctx->no_dif_bundl;
+		ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe025,
+		    "dif_bundl_len=%x, no_dif_bundl=%x track_difbundl_buf: %x\n",
+		    difctx->dif_bundl_len, difctx->no_dif_bundl,
+		    track_difbundl_buf);
 
-				sp->flags |= SRB_CRC_CTX_DSD_VALID;
-			} else {
-				list_add_tail(&dsd_ptr->list,
-				    &(tc->ctx->dsd_list));
-				*tc->ctx_dsd_alloced = 1;
+		if (sp)
+			sp->flags |= SRB_DIF_BUNDL_DMA_VALID;
+		else
+			tc->prot_flags = DIF_BUNDL_DMA_VALID;
+
+		list_for_each_entry_safe(dif_dsd, nxt_dsd,
+		    &difctx->ldif_dma_hndl_list, list) {
+			u32 sglen = (difctx->dif_bundl_len >
+			    DIF_BUNDLING_DMA_POOL_SIZE) ?
+			    DIF_BUNDLING_DMA_POOL_SIZE : difctx->dif_bundl_len;
+
+			BUG_ON(track_difbundl_buf == 0);
+
+			/* Allocate additional continuation packets? */
+			if (avail_dsds == 0) {
+				ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha,
+				    0xe024,
+				    "%s: adding continuation iocb's\n",
+				    __func__);
+				avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
+				    QLA_DSDS_PER_IOCB : used_dsds;
+				dsd_list_len = (avail_dsds + 1) * 12;
+				used_dsds -= avail_dsds;
+
+				/* allocate tracking DS */
+				dsd_ptr = kzalloc(sizeof(*dsd_ptr), GFP_ATOMIC);
+				if (!dsd_ptr) {
+					ql_dbg(ql_dbg_tgt, vha, 0xe026,
+					    "%s: failed alloc dsd_ptr\n",
+					    __func__);
+					return 1;
+				}
+				ha->dif_bundle_kallocs++;
+
+				difctx->no_ldif_dsd++;
+				/* allocate new list */
+				dsd_ptr->dsd_addr =
+				    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
+					&dsd_ptr->dsd_list_dma);
+				if (!dsd_ptr->dsd_addr) {
+					ql_dbg(ql_dbg_tgt, vha, 0xe026,
+					    "%s: failed alloc ->dsd_addr\n",
+					    __func__);
+					/*
+					 * need to cleanup only this dsd_ptr
+					 *  rest will be done by sp_free_dma()
+					 */
+					kfree(dsd_ptr);
+					ha->dif_bundle_kallocs--;
+					return 1;
+				}
+				ha->dif_bundle_dma_allocs++;
+
+				if (sp) {
+					list_add_tail(&dsd_ptr->list,
+					    &difctx->ldif_dsd_list);
+					sp->flags |= SRB_CRC_CTX_DSD_VALID;
+				} else {
+					list_add_tail(&dsd_ptr->list,
+					    &difctx->ldif_dsd_list);
+					tc->ctx_dsd_alloced = 1;
+				}
+
+				/* add new list to cmd iocb or last list */
+				*cur_dsd++ =
+				    cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
+				*cur_dsd++ =
+				    cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
+				*cur_dsd++ = dsd_list_len;
+				cur_dsd = dsd_ptr->dsd_addr;
 			}
-
-			/* add new list to cmd iocb or last list */
-			*cur_dsd++ = cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
-			*cur_dsd++ = cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
-			*cur_dsd++ = dsd_list_len;
-			cur_dsd = (uint32_t *)next_dsd;
+			*cur_dsd++ = cpu_to_le32(LSD(dif_dsd->dsd_list_dma));
+			*cur_dsd++ = cpu_to_le32(MSD(dif_dsd->dsd_list_dma));
+			*cur_dsd++ = cpu_to_le32(sglen);
+			avail_dsds--;
+			difctx->dif_bundl_len -= sglen;
+			track_difbundl_buf--;
 		}
-		sle_dma = sg_dma_address(sg);
-
-		*cur_dsd++ = cpu_to_le32(LSD(sle_dma));
-		*cur_dsd++ = cpu_to_le32(MSD(sle_dma));
-		*cur_dsd++ = cpu_to_le32(sg_dma_len(sg));
 
-		avail_dsds--;
+		ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe026,
+		    "%s: no_ldif_dsd:%x, no_dif_bundl:%x\n", __func__,
+			difctx->no_ldif_dsd, difctx->no_dif_bundl);
+	} else {
+		for_each_sg(sgl, sg, tot_dsds, i) {
+			dma_addr_t sle_dma;
+
+			/* Allocate additional continuation packets? */
+			if (avail_dsds == 0) {
+				avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
+				    QLA_DSDS_PER_IOCB : used_dsds;
+				dsd_list_len = (avail_dsds + 1) * 12;
+				used_dsds -= avail_dsds;
+
+				/* allocate tracking DS */
+				dsd_ptr = kzalloc(sizeof(*dsd_ptr), GFP_ATOMIC);
+				if (!dsd_ptr) {
+					ql_dbg(ql_dbg_tgt + ql_dbg_verbose,
+					    vha, 0xe027,
+					    "%s: failed alloc dsd_dma...\n",
+					    __func__);
+					return 1;
+				}
+
+				/* allocate new list */
+				dsd_ptr->dsd_addr =
+				    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
+					&dsd_ptr->dsd_list_dma);
+				if (!dsd_ptr->dsd_addr) {
+					/* need to cleanup only this dsd_ptr */
+					/* rest will be done by sp_free_dma() */
+					kfree(dsd_ptr);
+					return 1;
+				}
+
+				if (sp) {
+					list_add_tail(&dsd_ptr->list,
+					    &difctx->dsd_list);
+					sp->flags |= SRB_CRC_CTX_DSD_VALID;
+				} else {
+					list_add_tail(&dsd_ptr->list,
+					    &difctx->dsd_list);
+					tc->ctx_dsd_alloced = 1;
+				}
+
+				/* add new list to cmd iocb or last list */
+				*cur_dsd++ =
+				    cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
+				*cur_dsd++ =
+				    cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
+				*cur_dsd++ = dsd_list_len;
+				cur_dsd = dsd_ptr->dsd_addr;
+			}
+			sle_dma = sg_dma_address(sg);
+			*cur_dsd++ = cpu_to_le32(LSD(sle_dma));
+			*cur_dsd++ = cpu_to_le32(MSD(sle_dma));
+			*cur_dsd++ = cpu_to_le32(sg_dma_len(sg));
+			avail_dsds--;
+		}
 	}
 	/* Null termination */
 	*cur_dsd++ = 0;
@@ -1187,7 +1399,6 @@ qla24xx_walk_and_build_prot_sglist(struct qla_hw_data *ha, srb_t *sp,
 	*cur_dsd++ = 0;
 	return 0;
 }
-
 /**
  * qla24xx_build_scsi_crc_2_iocbs() - Build IOCB command utilizing Command
  *							Type 6 IOCB types.
diff --git a/drivers/scsi/qla2xxx/qla_isr.c b/drivers/scsi/qla2xxx/qla_isr.c
index 45581b93e72f..6b5ce25725cf 100644
--- a/drivers/scsi/qla2xxx/qla_isr.c
+++ b/drivers/scsi/qla2xxx/qla_isr.c
@@ -2724,6 +2724,17 @@ qla2x00_status_entry(scsi_qla_host_t *vha, struct rsp_que *rsp, void *pkt)
 			    cp->device->vendor);
 		break;
 
+	case CS_DMA:
+		ql_log(ql_log_info, fcport->vha, 0x3022,
+		    "CS_DMA error: 0x%x-0x%x (0x%x) nexus=%ld:%d:%llu portid=%06x oxid=0x%x cdb=%10phN len=0x%x rsp_info=0x%x resid=0x%x fw_resid=0x%x sp=%p cp=%p.\n",
+		    comp_status, scsi_status, res, vha->host_no,
+		    cp->device->id, cp->device->lun, fcport->d_id.b24,
+		    ox_id, cp->cmnd, scsi_bufflen(cp), rsp_info_len,
+		    resid_len, fw_resid_len, sp, cp);
+		ql_dump_buffer(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe0ee,
+		    pkt, sizeof(*sts24));
+		res = DID_ERROR << 16;
+		break;
 	default:
 		res = DID_ERROR << 16;
 		break;
diff --git a/drivers/scsi/qla2xxx/qla_os.c b/drivers/scsi/qla2xxx/qla_os.c
index 566ba945c9c2..9bdf1eb2cad1 100644
--- a/drivers/scsi/qla2xxx/qla_os.c
+++ b/drivers/scsi/qla2xxx/qla_os.c
@@ -297,6 +297,13 @@ MODULE_PARM_DESC(ql2xprotguard, "Override choice of DIX checksum\n"
 		 "  1 -- Force T10 CRC\n"
 		 "  2 -- Force IP checksum\n");
 
+int ql2xdifbundlinginternalbuffers;
+module_param(ql2xdifbundlinginternalbuffers, int, 0644);
+MODULE_PARM_DESC(ql2xdifbundlinginternalbuffers,
+    "Force using internal buffers for DIF information\n"
+    "0 (Default). Based on check.\n"
+    "1 Force using internal buffers\n");
+
 /*
  * SCSI host template entry points
  */
@@ -817,7 +824,44 @@ qla2xxx_qpair_sp_free_dma(void *ptr)
 		ha->gbl_dsd_inuse -= ctx1->dsd_use_cnt;
 		ha->gbl_dsd_avail += ctx1->dsd_use_cnt;
 		mempool_free(ctx1, ha->ctx_mempool);
+		sp->flags &= ~SRB_FCP_CMND_DMA_VALID;
+	}
+	if (sp->flags & SRB_DIF_BUNDL_DMA_VALID) {
+		struct crc_context *difctx = sp->u.scmd.ctx;
+		struct dsd_dma *dif_dsd, *nxt_dsd;
+
+		list_for_each_entry_safe(dif_dsd, nxt_dsd,
+		    &difctx->ldif_dma_hndl_list, list) {
+			list_del(&dif_dsd->list);
+			dma_pool_free(ha->dif_bundl_pool, dif_dsd->dsd_addr,
+			    dif_dsd->dsd_list_dma);
+			kfree(dif_dsd);
+			difctx->no_dif_bundl--;
+		}
+
+		list_for_each_entry_safe(dif_dsd, nxt_dsd,
+		    &difctx->ldif_dsd_list, list) {
+			list_del(&dif_dsd->list);
+			dma_pool_free(ha->dl_dma_pool, dif_dsd->dsd_addr,
+			    dif_dsd->dsd_list_dma);
+			kfree(dif_dsd);
+			difctx->no_ldif_dsd--;
+		}
+
+		if (difctx->no_ldif_dsd) {
+			ql_dbg(ql_dbg_tgt+ql_dbg_verbose, sp->vha, 0xe022,
+			    "%s: difctx->no_ldif_dsd=%x\n",
+			    __func__, difctx->no_ldif_dsd);
+		}
+
+		if (difctx->no_dif_bundl) {
+			ql_dbg(ql_dbg_tgt+ql_dbg_verbose, sp->vha, 0xe022,
+			    "%s: difctx->no_dif_bundl=%x\n",
+			    __func__, difctx->no_dif_bundl);
+		}
+		sp->flags &= ~SRB_DIF_BUNDL_DMA_VALID;
 	}
+
 end:
 	CMD_SP(cmd) = NULL;
 	qla2xxx_rel_qpair_sp(sp->qpair, sp);
@@ -3989,9 +4033,86 @@ qla2x00_mem_alloc(struct qla_hw_data *ha, uint16_t req_len, uint16_t rsp_len,
 			    "Failed to allocate memory for fcp_cmnd_dma_pool.\n");
 			goto fail_dl_dma_pool;
 		}
+
+		if (ql2xenabledif) {
+			u64 bufsize = DIF_BUNDLING_DMA_POOL_SIZE;
+			struct dsd_dma *dsd, *nxt;
+			uint i;
+			/* Creata a DMA pool of buffers for DIF bundling */
+			ha->dif_bundl_pool = dma_pool_create(name,
+			    &ha->pdev->dev, DIF_BUNDLING_DMA_POOL_SIZE, 8, 0);
+			if (!ha->dif_bundl_pool) {
+				ql_dbg_pci(ql_dbg_init, ha->pdev, 0x0024,
+				    "%s: failed create dif_bundl_pool\n",
+				    __func__);
+				goto fail_dif_bundl_dma_pool;
+			}
+
+			INIT_LIST_HEAD(&ha->pool.good.head);
+			INIT_LIST_HEAD(&ha->pool.unusable.head);
+			ha->pool.good.count = 0;
+			ha->pool.unusable.count = 0;
+			for (i = 0; i < 128; i++) {
+				dsd = kzalloc(sizeof(*dsd), GFP_ATOMIC);
+				if (!dsd) {
+					ql_dbg_pci(ql_dbg_init, ha->pdev,
+					    0xe0ee, "%s: failed alloc dsd\n",
+					    __func__);
+					return 1;
+				}
+				ha->dif_bundle_kallocs++;
+
+				dsd->dsd_addr = dma_pool_alloc(
+				    ha->dif_bundl_pool, GFP_ATOMIC,
+				    &dsd->dsd_list_dma);
+				if (!dsd->dsd_addr) {
+					ql_dbg_pci(ql_dbg_init, ha->pdev,
+					    0xe0ee,
+					    "%s: failed alloc ->dsd_addr\n",
+					    __func__);
+					kfree(dsd);
+					ha->dif_bundle_kallocs--;
+					continue;
+				}
+				ha->dif_bundle_dma_allocs++;
+
+				/*
+				 * if DMA buffer crosses 4G boundary,
+				 * put it on bad list
+				 */
+				if (MSD(dsd->dsd_list_dma) ^
+				    MSD(dsd->dsd_list_dma + bufsize)) {
+					list_add_tail(&dsd->list,
+					    &ha->pool.unusable.head);
+					ha->pool.unusable.count++;
+				} else {
+					list_add_tail(&dsd->list,
+					    &ha->pool.good.head);
+					ha->pool.good.count++;
+				}
+			}
+
+			/* return the good ones back to the pool */
+			list_for_each_entry_safe(dsd, nxt,
+			    &ha->pool.good.head, list) {
+				list_del(&dsd->list);
+				dma_pool_free(ha->dif_bundl_pool,
+				    dsd->dsd_addr, dsd->dsd_list_dma);
+				ha->dif_bundle_dma_allocs--;
+				kfree(dsd);
+				ha->dif_bundle_kallocs--;
+			}
+
+			ql_dbg_pci(ql_dbg_init, ha->pdev, 0x0024,
+			    "%s: dif dma pool (good=%u unusable=%u)\n",
+			    __func__, ha->pool.good.count,
+			    ha->pool.unusable.count);
+		}
+
 		ql_dbg_pci(ql_dbg_init, ha->pdev, 0x0025,
-		    "dl_dma_pool=%p fcp_cmnd_dma_pool=%p.\n",
-		    ha->dl_dma_pool, ha->fcp_cmnd_dma_pool);
+		    "dl_dma_pool=%p fcp_cmnd_dma_pool=%p dif_bundl_pool=%p.\n",
+		    ha->dl_dma_pool, ha->fcp_cmnd_dma_pool,
+		    ha->dif_bundl_pool);
 	}
 
 	/* Allocate memory for SNS commands */
@@ -4156,6 +4277,24 @@ qla2x00_mem_alloc(struct qla_hw_data *ha, uint16_t req_len, uint16_t rsp_len,
 		dma_free_coherent(&ha->pdev->dev, sizeof(struct sns_cmd_pkt),
 		    ha->sns_cmd, ha->sns_cmd_dma);
 fail_dma_pool:
+	if (ql2xenabledif) {
+		struct dsd_dma *dsd, *nxt;
+
+		list_for_each_entry_safe(dsd, nxt, &ha->pool.unusable.head,
+		    list) {
+			list_del(&dsd->list);
+			dma_pool_free(ha->dif_bundl_pool, dsd->dsd_addr,
+			    dsd->dsd_list_dma);
+			ha->dif_bundle_dma_allocs--;
+			kfree(dsd);
+			ha->dif_bundle_kallocs--;
+			ha->pool.unusable.count--;
+		}
+		dma_pool_destroy(ha->dif_bundl_pool);
+		ha->dif_bundl_pool = NULL;
+	}
+
+fail_dif_bundl_dma_pool:
 	if (IS_QLA82XX(ha) || ql2xenabledif) {
 		dma_pool_destroy(ha->fcp_cmnd_dma_pool);
 		ha->fcp_cmnd_dma_pool = NULL;
@@ -4516,6 +4655,32 @@ qla2x00_mem_free(struct qla_hw_data *ha)
 
 	mempool_destroy(ha->ctx_mempool);
 
+	if (ql2xenabledif) {
+		struct dsd_dma *dsd, *nxt;
+
+		list_for_each_entry_safe(dsd, nxt, &ha->pool.unusable.head,
+					 list) {
+			list_del(&dsd->list);
+			dma_pool_free(ha->dif_bundl_pool, dsd->dsd_addr,
+				      dsd->dsd_list_dma);
+			ha->dif_bundle_dma_allocs--;
+			kfree(dsd);
+			ha->dif_bundle_kallocs--;
+			ha->pool.unusable.count--;
+		}
+		list_for_each_entry_safe(dsd, nxt, &ha->pool.good.head, list) {
+			list_del(&dsd->list);
+			dma_pool_free(ha->dif_bundl_pool, dsd->dsd_addr,
+				      dsd->dsd_list_dma);
+			ha->dif_bundle_dma_allocs--;
+			kfree(dsd);
+			ha->dif_bundle_kallocs--;
+		}
+	}
+
+	if (ha->dif_bundl_pool)
+		dma_pool_destroy(ha->dif_bundl_pool);
+
 	qlt_mem_free(ha);
 
 	if (ha->init_cb)
diff --git a/drivers/scsi/qla2xxx/qla_target.c b/drivers/scsi/qla2xxx/qla_target.c
index e58bc1675a89..08f7295e15ff 100644
--- a/drivers/scsi/qla2xxx/qla_target.c
+++ b/drivers/scsi/qla2xxx/qla_target.c
@@ -3134,7 +3134,7 @@ qlt_build_ctio_crc2_pkt(struct qla_qpair *qpair, struct qla_tgt_prm *prm)
 
 		cur_dsd = (uint32_t *) &crc_ctx_pkt->u.bundling.dif_address;
 		if (qla24xx_walk_and_build_prot_sglist(ha, NULL, cur_dsd,
-			prm->prot_seg_cnt, &tc))
+			prm->prot_seg_cnt, cmd))
 			goto crc_queuing_error;
 	}
 	return QLA_SUCCESS;
diff --git a/drivers/scsi/qla2xxx/qla_target.h b/drivers/scsi/qla2xxx/qla_target.h
index 6126281c3982..371c2e61ad30 100644
--- a/drivers/scsi/qla2xxx/qla_target.h
+++ b/drivers/scsi/qla2xxx/qla_target.h
@@ -936,6 +936,8 @@ struct qla_tgt_cmd {
 	uint64_t	lba;
 	uint16_t	a_guard, e_guard, a_app_tag, e_app_tag;
 	uint32_t	a_ref_tag, e_ref_tag;
+#define DIF_BUNDL_DMA_VALID 1
+	uint16_t prot_flags;
 
 	uint64_t jiffies_at_alloc;
 	uint64_t jiffies_at_free;
