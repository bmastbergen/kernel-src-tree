RDMA/mlx5: Embed into the code flow the ODP config option

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 13859d5df418ea535926e2b57c29d5161c522b9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/13859d5d.failed

Convert various places to more readable code, which embeds
CONFIG_INFINIBAND_ON_DEMAND_PAGING into the code flow.

	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 13859d5df418ea535926e2b57c29d5161c522b9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs_cmd.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mem.c
diff --cc drivers/infiniband/core/uverbs_cmd.c
index 300532a4cde8,d4f1a2ef5015..000000000000
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@@ -110,19 -230,12 +110,24 @@@ ssize_t ib_uverbs_get_context(struct ib
  	ucontext->cg_obj = cg_obj;
  	/* ufile is required when some objects are released */
  	ucontext->ufile = file;
 +	uverbs_initialize_ucontext(ucontext);
  
 -	ucontext->closing = false;
 -	ucontext->cleanup_retryable = false;
 +	rcu_read_lock();
 +	ucontext->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
 +	rcu_read_unlock();
 +	ucontext->closing = 0;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	ucontext->umem_tree = RB_ROOT_CACHED;
 +	init_rwsem(&ucontext->umem_rwsem);
 +	ucontext->odp_mrs_count = 0;
 +	INIT_LIST_HEAD(&ucontext->no_private_counters);
 +
++=======
+ 	mutex_init(&ucontext->per_mm_list_lock);
+ 	INIT_LIST_HEAD(&ucontext->per_mm_list);
++>>>>>>> 13859d5df418 (RDMA/mlx5: Embed into the code flow the ODP config option)
  	if (!(ib_dev->attrs.device_cap_flags & IB_DEVICE_ON_DEMAND_PAGING))
  		ucontext->invalidate_range = NULL;
  
diff --cc drivers/infiniband/hw/mlx5/main.c
index 9f7060c02bfd,11e9783cefcc..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1794,25 -1763,21 +1794,25 @@@ static struct ib_ucontext *mlx5_ib_allo
  	if (err)
  		goto out_sys_pages;
  
- #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
- 	context->ibucontext.invalidate_range = &mlx5_ib_invalidate_range;
- #endif
+ 	if (ibdev->attrs.device_cap_flags & IB_DEVICE_ON_DEMAND_PAGING)
+ 		context->ibucontext.invalidate_range =
+ 			&mlx5_ib_invalidate_range;
  
 +	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn);
 +	if (err)
 +		goto out_uars;
 +
  	if (req.flags & MLX5_IB_ALLOC_UCTX_DEVX) {
 -		err = mlx5_ib_devx_create(dev, true);
 -		if (err < 0)
 -			goto out_uars;
 -		context->devx_uid = err;
 -	}
 +		/* Block DEVX on Infiniband as of SELinux */
 +		if (mlx5_ib_port_link_layer(ibdev, 1) != IB_LINK_LAYER_ETHERNET) {
 +			err = -EPERM;
 +			goto out_td;
 +		}
  
 -	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn,
 -					     context->devx_uid);
 -	if (err)
 -		goto out_devx;
 +		err = mlx5_ib_devx_create(dev, context);
 +		if (err)
 +			goto out_td;
 +	}
  
  	if (MLX5_CAP_GEN(dev->mdev, dump_fill_mkey)) {
  		err = mlx5_cmd_dump_fill_mkey(dev->mdev, &dump_fill_mkey);
@@@ -1932,11 -1897,16 +1932,18 @@@ static int mlx5_ib_dealloc_ucontext(str
  	struct mlx5_ib_dev *dev = to_mdev(ibcontext->device);
  	struct mlx5_bfreg_info *bfregi;
  
++<<<<<<< HEAD
 +	if (context->devx_uid)
 +		mlx5_ib_devx_destroy(dev, context);
++=======
+ 	/* All umem's must be destroyed before destroying the ucontext. */
+ 	mutex_lock(&ibcontext->per_mm_list_lock);
+ 	WARN_ON(!list_empty(&ibcontext->per_mm_list));
+ 	mutex_unlock(&ibcontext->per_mm_list_lock);
++>>>>>>> 13859d5df418 (RDMA/mlx5: Embed into the code flow the ODP config option)
  
  	bfregi = &context->bfregi;
 -	mlx5_ib_dealloc_transport_domain(dev, context->tdn, context->devx_uid);
 -
 -	if (context->devx_uid)
 -		mlx5_ib_devx_destroy(dev, context->devx_uid);
 +	mlx5_ib_dealloc_transport_domain(dev, context->tdn);
  
  	deallocate_uars(dev, context);
  	kfree(bfregi->sys_pages);
@@@ -5459,9 -5720,11 +5466,17 @@@ static struct ib_counters *mlx5_ib_crea
  void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
  {
  	mlx5_ib_cleanup_multiport_master(dev);
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	cleanup_srcu_struct(&dev->mr_srcu);
 +#endif
++=======
+ 	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
+ 		cleanup_srcu_struct(&dev->mr_srcu);
+ 		drain_workqueue(dev->advise_mr_wq);
+ 		destroy_workqueue(dev->advise_mr_wq);
+ 	}
++>>>>>>> 13859d5df418 (RDMA/mlx5: Embed into the code flow the ODP config option)
  	kfree(dev->port);
  }
  
@@@ -5522,11 -5777,20 +5537,28 @@@ int mlx5_ib_stage_init_init(struct mlx5
  	spin_lock_init(&dev->memic.memic_lock);
  	dev->memic.dev = mdev;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	err = init_srcu_struct(&dev->mr_srcu);
 +	if (err)
 +		goto err_free_port;
 +#endif
++=======
+ 	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
+ 		dev->advise_mr_wq =
+ 			alloc_ordered_workqueue("mlx5_ib_advise_mr_wq", 0);
+ 		if (!dev->advise_mr_wq) {
+ 			err = -ENOMEM;
+ 			goto err_mp;
+ 		}
+ 
+ 		err = init_srcu_struct(&dev->mr_srcu);
+ 		if (err) {
+ 			destroy_workqueue(dev->advise_mr_wq);
+ 			goto err_mp;
+ 		}
+ 	}
++>>>>>>> 13859d5df418 (RDMA/mlx5: Embed into the code flow the ODP config option)
  
  	return 0;
  err_mp:
diff --cc drivers/infiniband/hw/mlx5/mem.c
index f3dbd75a0a96,9f90be296ee0..000000000000
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@@ -151,10 -149,8 +149,15 @@@ void __mlx5_ib_populate_pas(struct mlx5
  	int len;
  	struct scatterlist *sg;
  	int entry;
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	const bool odp = umem->odp_data != NULL;
 +
 +	if (odp) {
++=======
+ 
+ 	if (umem->is_odp) {
++>>>>>>> 13859d5df418 (RDMA/mlx5: Embed into the code flow the ODP config option)
  		WARN_ON(shift != 0);
  		WARN_ON(access_flags != (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE));
  
* Unmerged path drivers/infiniband/core/uverbs_cmd.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mem.c
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 7aa0bfa996ba..ee4f0bf1bdcd 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -71,10 +71,9 @@ static int destroy_mkey(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
 {
 	int err = mlx5_core_destroy_mkey(dev->mdev, &mr->mmkey);
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	/* Wait until all page fault handlers using the mr complete. */
-	synchronize_srcu(&dev->mr_srcu);
-#endif
+	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+		/* Wait until all page fault handlers using the mr complete. */
+		synchronize_srcu(&dev->mr_srcu);
 
 	return err;
 }
@@ -256,9 +255,8 @@ static void remove_keys(struct mlx5_ib_dev *dev, int c, int num)
 		mlx5_core_destroy_mkey(dev->mdev, &mr->mmkey);
 	}
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	synchronize_srcu(&dev->mr_srcu);
-#endif
+	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+		synchronize_srcu(&dev->mr_srcu);
 
 	list_for_each_entry_safe(mr, tmp_mr, &del_list, list) {
 		list_del(&mr->list);
@@ -1316,8 +1314,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 		    start, virt_addr, length, access_flags);
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	if (!start && length == U64_MAX) {
+	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && !start &&
+	    length == U64_MAX) {
 		if (!(access_flags & IB_ACCESS_ON_DEMAND) ||
 		    !(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
 			return ERR_PTR(-EINVAL);
@@ -1327,7 +1325,6 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 			return ERR_CAST(mr);
 		return &mr->ibmr;
 	}
-#endif
 
 	err = mr_umem_get(pd, start, length, access_flags, &umem, &npages,
 			   &page_shift, &ncont, &order);
@@ -1390,9 +1387,9 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 		}
 	}
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	mr->live = 1;
-#endif
+	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+		mr->live = 1;
+
 	return &mr->ibmr;
 error:
 	ib_umem_release(umem);
@@ -1507,9 +1504,8 @@ int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 		}
 
 		mr->allocated_from_cache = 0;
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-		mr->live = 1;
-#endif
+		if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+			mr->live = 1;
 	} else {
 		/*
 		 * Send a UMR WQE
diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h
index f617ea3b60a6..0acecca48afe 100644
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -87,13 +87,6 @@ static inline struct ib_umem_odp *to_ib_umem_odp(struct ib_umem *umem)
 	return container_of(umem, struct ib_umem_odp, umem);
 }
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-
-int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
-struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
-				      unsigned long addr, size_t size);
-void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
-
 /*
  * The lower 2 bits of the DMA address signal the R/W permissions for
  * the entry. To upgrade the permissions, provide the appropriate
@@ -107,6 +100,13 @@ void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
 
 #define ODP_DMA_ADDR_MASK (~(ODP_READ_ALLOWED_BIT | ODP_WRITE_ALLOWED_BIT))
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+
+int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
+struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
+				      unsigned long addr, size_t size);
+void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
+
 int ib_umem_odp_map_dma_pages(struct ib_umem_odp *umem_odp, u64 start_offset,
 			      u64 bcnt, u64 access_mask,
 			      unsigned long current_seq);
