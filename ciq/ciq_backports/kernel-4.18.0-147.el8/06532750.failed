dma-mapping: use dma_get_mask in dma_addressing_limited

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Eric Auger <eric.auger@redhat.com>
commit 06532750010e06dd4b6d69983773677df7fc5291
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/06532750.failed

We currently have cases where the dma_addressing_limited() gets
called with dma_mask unset. This causes a NULL pointer dereference.

Use dma_get_mask() accessor to prevent the crash.

Fixes: b866455423e0 ("dma-mapping: add a dma_addressing_limited helper")
	Signed-off-by: Eric Auger <eric.auger@redhat.com>
	Acked-by: Michael S. Tsirkin <mst@redhat.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 06532750010e06dd4b6d69983773677df7fc5291)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
diff --cc include/linux/dma-mapping.h
index 5f446033278a,f7d1eea32c78..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -678,15 -679,37 +678,36 @@@ static inline int dma_coerce_mask_and_c
  	return dma_set_mask_and_coherent(dev, mask);
  }
  
++<<<<<<< HEAD
 +#ifndef arch_setup_dma_ops
++=======
+ /**
+  * dma_addressing_limited - return if the device is addressing limited
+  * @dev:	device to check
+  *
+  * Return %true if the devices DMA mask is too small to address all memory in
+  * the system, else %false.  Lack of addressing bits is the prime reason for
+  * bounce buffering, but might not be the only one.
+  */
+ static inline bool dma_addressing_limited(struct device *dev)
+ {
+ 	return min_not_zero(dma_get_mask(dev), dev->bus_dma_mask) <
+ 			    dma_get_required_mask(dev);
+ }
+ 
+ #ifdef CONFIG_ARCH_HAS_SETUP_DMA_OPS
+ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+ 		const struct iommu_ops *iommu, bool coherent);
+ #else
++>>>>>>> 06532750010e (dma-mapping: use dma_get_mask in dma_addressing_limited)
  static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
 -		u64 size, const struct iommu_ops *iommu, bool coherent)
 -{
 -}
 -#endif /* CONFIG_ARCH_HAS_SETUP_DMA_OPS */
 +				      u64 size, const struct iommu_ops *iommu,
 +				      bool coherent) { }
 +#endif
  
 -#ifdef CONFIG_ARCH_HAS_TEARDOWN_DMA_OPS
 -void arch_teardown_dma_ops(struct device *dev);
 -#else
 -static inline void arch_teardown_dma_ops(struct device *dev)
 -{
 -}
 -#endif /* CONFIG_ARCH_HAS_TEARDOWN_DMA_OPS */
 +#ifndef arch_teardown_dma_ops
 +static inline void arch_teardown_dma_ops(struct device *dev) { }
 +#endif
  
  static inline unsigned int dma_get_max_seg_size(struct device *dev)
  {
* Unmerged path include/linux/dma-mapping.h
