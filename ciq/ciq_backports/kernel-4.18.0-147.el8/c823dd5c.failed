KVM: VMX: Fold __vmx_vcpu_run() back into vmx_vcpu_run()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit c823dd5c0f3fafa595ed51cc72c6e006efc20ad3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/c823dd5c.failed

...now that the code is no longer tagged with STACK_FRAME_NON_STANDARD.
Arguably, providing __vmx_vcpu_run() to break up vmx_vcpu_run() is
valuable on its own, but the previous split was purposely made as small
as possible to limit the effects STACK_FRAME_NON_STANDARD.  In other
words, the current split is now completely arbitrary and likely not the
most logical.

This also allows renaming ____vmx_vcpu_run() to __vmx_vcpu_run() in a
future patch.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c823dd5c0f3fafa595ed51cc72c6e006efc20ad3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 7955a1810f70,e61bb8da9767..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -7169,4388 -4571,851 +7169,4430 @@@ static int handle_triple_fault(struct k
  	return 0;
  }
  
 -static int handle_io(struct kvm_vcpu *vcpu)
 +static int handle_io(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification;
 +	int size, in, string;
 +	unsigned port;
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	string = (exit_qualification & 16) != 0;
 +
 +	++vcpu->stat.io_exits;
 +
 +	if (string)
 +		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +
 +	port = exit_qualification >> 16;
 +	size = (exit_qualification & 7) + 1;
 +	in = (exit_qualification & 8) != 0;
 +
 +	return kvm_fast_pio(vcpu, size, port, in);
 +}
 +
 +static void
 +vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +{
 +	/*
 +	 * Patch in the VMCALL instruction:
 +	 */
 +	hypercall[0] = 0x0f;
 +	hypercall[1] = 0x01;
 +	hypercall[2] = 0xc1;
 +}
 +
 +/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 +static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	if (is_guest_mode(vcpu)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		unsigned long orig_val = val;
 +
 +		/*
 +		 * We get here when L2 changed cr0 in a way that did not change
 +		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 +		 * but did change L0 shadowed bits. So we first calculate the
 +		 * effective cr0 value that L1 would like to write into the
 +		 * hardware. It consists of the L2-owned bits from the new
 +		 * value combined with the L1-owned bits from L1's guest_cr0.
 +		 */
 +		val = (val & ~vmcs12->cr0_guest_host_mask) |
 +			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
 +
 +		if (!nested_guest_cr0_valid(vcpu, val))
 +			return 1;
 +
 +		if (kvm_set_cr0(vcpu, val))
 +			return 1;
 +		vmcs_writel(CR0_READ_SHADOW, orig_val);
 +		return 0;
 +	} else {
 +		if (to_vmx(vcpu)->nested.vmxon &&
 +		    !nested_host_cr0_valid(vcpu, val))
 +			return 1;
 +
 +		return kvm_set_cr0(vcpu, val);
 +	}
 +}
 +
 +static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	if (is_guest_mode(vcpu)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		unsigned long orig_val = val;
 +
 +		/* analogously to handle_set_cr0 */
 +		val = (val & ~vmcs12->cr4_guest_host_mask) |
 +			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 +		if (kvm_set_cr4(vcpu, val))
 +			return 1;
 +		vmcs_writel(CR4_READ_SHADOW, orig_val);
 +		return 0;
 +	} else
 +		return kvm_set_cr4(vcpu, val);
 +}
 +
 +static int handle_desc(struct kvm_vcpu *vcpu)
 +{
 +	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
 +
 +static int handle_cr(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification, val;
 +	int cr;
 +	int reg;
 +	int err;
 +	int ret;
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	cr = exit_qualification & 15;
 +	reg = (exit_qualification >> 8) & 15;
 +	switch ((exit_qualification >> 4) & 3) {
 +	case 0: /* mov to cr */
 +		val = kvm_register_readl(vcpu, reg);
 +		trace_kvm_cr_write(cr, val);
 +		switch (cr) {
 +		case 0:
 +			err = handle_set_cr0(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 3:
 +			WARN_ON_ONCE(enable_unrestricted_guest);
 +			err = kvm_set_cr3(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 4:
 +			err = handle_set_cr4(vcpu, val);
 +			return kvm_complete_insn_gp(vcpu, err);
 +		case 8: {
 +				u8 cr8_prev = kvm_get_cr8(vcpu);
 +				u8 cr8 = (u8)val;
 +				err = kvm_set_cr8(vcpu, cr8);
 +				ret = kvm_complete_insn_gp(vcpu, err);
 +				if (lapic_in_kernel(vcpu))
 +					return ret;
 +				if (cr8_prev <= cr8)
 +					return ret;
 +				/*
 +				 * TODO: we might be squashing a
 +				 * KVM_GUESTDBG_SINGLESTEP-triggered
 +				 * KVM_EXIT_DEBUG here.
 +				 */
 +				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 +				return 0;
 +			}
 +		}
 +		break;
 +	case 2: /* clts */
 +		WARN_ONCE(1, "Guest should always own CR0.TS");
 +		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 +		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 +		return kvm_skip_emulated_instruction(vcpu);
 +	case 1: /*mov from cr*/
 +		switch (cr) {
 +		case 3:
 +			WARN_ON_ONCE(enable_unrestricted_guest);
 +			val = kvm_read_cr3(vcpu);
 +			kvm_register_write(vcpu, reg, val);
 +			trace_kvm_cr_read(cr, val);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		case 8:
 +			val = kvm_get_cr8(vcpu);
 +			kvm_register_write(vcpu, reg, val);
 +			trace_kvm_cr_read(cr, val);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		break;
 +	case 3: /* lmsw */
 +		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 +		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 +		kvm_lmsw(vcpu, val);
 +
 +		return kvm_skip_emulated_instruction(vcpu);
 +	default:
 +		break;
 +	}
 +	vcpu->run->exit_reason = 0;
 +	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 +	       (int)(exit_qualification >> 4) & 3, cr);
 +	return 0;
 +}
 +
 +static int handle_dr(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification;
 +	int dr, dr7, reg;
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 +
 +	/* First, if DR does not exist, trigger UD */
 +	if (!kvm_require_dr(vcpu, dr))
 +		return 1;
 +
 +	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 +	if (!kvm_require_cpl(vcpu, 0))
 +		return 1;
 +	dr7 = vmcs_readl(GUEST_DR7);
 +	if (dr7 & DR7_GD) {
 +		/*
 +		 * As the vm-exit takes precedence over the debug trap, we
 +		 * need to emulate the latter, either for the host or the
 +		 * guest debugging itself.
 +		 */
 +		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 +			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 +			vcpu->run->debug.arch.dr7 = dr7;
 +			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 +			vcpu->run->debug.arch.exception = DB_VECTOR;
 +			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 +			return 0;
 +		} else {
 +			vcpu->arch.dr6 &= ~15;
 +			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 +			kvm_queue_exception(vcpu, DB_VECTOR);
 +			return 1;
 +		}
 +	}
 +
 +	if (vcpu->guest_debug == 0) {
 +		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +				CPU_BASED_MOV_DR_EXITING);
 +
 +		/*
 +		 * No more DR vmexits; force a reload of the debug registers
 +		 * and reenter on this instruction.  The next vmexit will
 +		 * retrieve the full state of the debug registers.
 +		 */
 +		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 +		return 1;
 +	}
 +
 +	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 +	if (exit_qualification & TYPE_MOV_FROM_DR) {
 +		unsigned long val;
 +
 +		if (kvm_get_dr(vcpu, dr, &val))
 +			return 1;
 +		kvm_register_write(vcpu, reg, val);
 +	} else
 +		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 +			return 1;
 +
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 +{
 +	return vcpu->arch.dr6;
 +}
 +
 +static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +}
 +
 +static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +{
 +	get_debugreg(vcpu->arch.db[0], 0);
 +	get_debugreg(vcpu->arch.db[1], 1);
 +	get_debugreg(vcpu->arch.db[2], 2);
 +	get_debugreg(vcpu->arch.db[3], 3);
 +	get_debugreg(vcpu->arch.dr6, 6);
 +	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
 +
 +	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 +	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 +}
 +
 +static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 +{
 +	vmcs_writel(GUEST_DR7, val);
 +}
 +
 +static int handle_cpuid(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_cpuid(vcpu);
 +}
 +
 +static int handle_rdmsr(struct kvm_vcpu *vcpu)
 +{
 +	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 +	struct msr_data msr_info;
 +
 +	msr_info.index = ecx;
 +	msr_info.host_initiated = false;
 +	if (vmx_get_msr(vcpu, &msr_info)) {
 +		trace_kvm_msr_read_ex(ecx);
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	trace_kvm_msr_read(ecx, msr_info.data);
 +
 +	/* FIXME: handling of bits 32:63 of rax, rdx */
 +	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 +	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_wrmsr(struct kvm_vcpu *vcpu)
 +{
 +	struct msr_data msr;
 +	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 +	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 +		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
 +
 +	msr.data = data;
 +	msr.index = ecx;
 +	msr.host_initiated = false;
 +	if (kvm_set_msr(vcpu, &msr) != 0) {
 +		trace_kvm_msr_write_ex(ecx, data);
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	trace_kvm_msr_write(ecx, data);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +{
 +	kvm_apic_update_ppr(vcpu);
 +	return 1;
 +}
 +
 +static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 +{
 +	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +			CPU_BASED_VIRTUAL_INTR_PENDING);
 +
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +
 +	++vcpu->stat.irq_window_exits;
 +	return 1;
 +}
 +
 +static int handle_halt(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_halt(vcpu);
 +}
 +
 +static int handle_vmcall(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_hypercall(vcpu);
 +}
 +
 +static int handle_invd(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
 +
 +static int handle_invlpg(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +
 +	kvm_mmu_invlpg(vcpu, exit_qualification);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_rdpmc(struct kvm_vcpu *vcpu)
 +{
 +	int err;
 +
 +	err = kvm_rdpmc(vcpu);
 +	return kvm_complete_insn_gp(vcpu, err);
 +}
 +
 +static int handle_wbinvd(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_wbinvd(vcpu);
 +}
 +
 +static int handle_xsetbv(struct kvm_vcpu *vcpu)
 +{
 +	u64 new_bv = kvm_read_edx_eax(vcpu);
 +	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
 +
 +	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 +		return kvm_skip_emulated_instruction(vcpu);
 +	return 1;
 +}
 +
 +static int handle_xsaves(struct kvm_vcpu *vcpu)
 +{
 +	kvm_skip_emulated_instruction(vcpu);
 +	WARN(1, "this should never happen\n");
 +	return 1;
 +}
 +
 +static int handle_xrstors(struct kvm_vcpu *vcpu)
 +{
 +	kvm_skip_emulated_instruction(vcpu);
 +	WARN(1, "this should never happen\n");
 +	return 1;
 +}
 +
 +static int handle_apic_access(struct kvm_vcpu *vcpu)
 +{
 +	if (likely(fasteoi)) {
 +		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +		int access_type, offset;
 +
 +		access_type = exit_qualification & APIC_ACCESS_TYPE;
 +		offset = exit_qualification & APIC_ACCESS_OFFSET;
 +		/*
 +		 * Sane guest uses MOV to write EOI, with written value
 +		 * not cared. So make a short-circuit here by avoiding
 +		 * heavy instruction emulation.
 +		 */
 +		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 +		    (offset == APIC_EOI)) {
 +			kvm_lapic_set_eoi(vcpu);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +	}
 +	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +}
 +
 +static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	int vector = exit_qualification & 0xff;
 +
 +	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 +	kvm_apic_set_eoi_accelerated(vcpu, vector);
 +	return 1;
 +}
 +
 +static int handle_apic_write(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 offset = exit_qualification & 0xfff;
 +
 +	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 +	kvm_apic_write_nodecode(vcpu, offset);
 +	return 1;
 +}
 +
 +static int handle_task_switch(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned long exit_qualification;
 +	bool has_error_code = false;
 +	u32 error_code = 0;
 +	u16 tss_selector;
 +	int reason, type, idt_v, idt_index;
 +
 +	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 +	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 +	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +
 +	reason = (u32)exit_qualification >> 30;
 +	if (reason == TASK_SWITCH_GATE && idt_v) {
 +		switch (type) {
 +		case INTR_TYPE_NMI_INTR:
 +			vcpu->arch.nmi_injected = false;
 +			vmx_set_nmi_mask(vcpu, true);
 +			break;
 +		case INTR_TYPE_EXT_INTR:
 +		case INTR_TYPE_SOFT_INTR:
 +			kvm_clear_interrupt_queue(vcpu);
 +			break;
 +		case INTR_TYPE_HARD_EXCEPTION:
 +			if (vmx->idt_vectoring_info &
 +			    VECTORING_INFO_DELIVER_CODE_MASK) {
 +				has_error_code = true;
 +				error_code =
 +					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 +			}
 +			/* fall through */
 +		case INTR_TYPE_SOFT_EXCEPTION:
 +			kvm_clear_exception_queue(vcpu);
 +			break;
 +		default:
 +			break;
 +		}
 +	}
 +	tss_selector = exit_qualification;
 +
 +	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 +		       type != INTR_TYPE_EXT_INTR &&
 +		       type != INTR_TYPE_NMI_INTR))
 +		skip_emulated_instruction(vcpu);
 +
 +	if (kvm_task_switch(vcpu, tss_selector,
 +			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 +			    has_error_code, error_code) == EMULATE_FAIL) {
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 +		vcpu->run->internal.ndata = 0;
 +		return 0;
 +	}
 +
 +	/*
 +	 * TODO: What about debug traps on tss switch?
 +	 *       Are we supposed to inject them and update dr6?
 +	 */
 +
 +	return 1;
 +}
 +
 +static int handle_ept_violation(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification;
 +	gpa_t gpa;
 +	u64 error_code;
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +
 +	/*
 +	 * EPT violation happened while executing iret from NMI,
 +	 * "blocked by NMI" bit has to be set before next VM entry.
 +	 * There are errata that may cause this bit to not be set:
 +	 * AAK134, BY25.
 +	 */
 +	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			enable_vnmi &&
 +			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 +		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 +
 +	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +	trace_kvm_page_fault(gpa, exit_qualification);
 +
 +	/* Is it a read fault? */
 +	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 +		     ? PFERR_USER_MASK : 0;
 +	/* Is it a write fault? */
 +	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 +		      ? PFERR_WRITE_MASK : 0;
 +	/* Is it a fetch fault? */
 +	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 +		      ? PFERR_FETCH_MASK : 0;
 +	/* ept page table entry is present? */
 +	error_code |= (exit_qualification &
 +		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 +			EPT_VIOLATION_EXECUTABLE))
 +		      ? PFERR_PRESENT_MASK : 0;
 +
 +	error_code |= (exit_qualification & 0x100) != 0 ?
 +	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 +
 +	vcpu->arch.exit_qualification = exit_qualification;
 +	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 +}
 +
 +static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 +{
 +	gpa_t gpa;
 +
 +	/*
 +	 * A nested guest cannot optimize MMIO vmexits, because we have an
 +	 * nGPA here instead of the required GPA.
 +	 */
 +	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +	if (!is_guest_mode(vcpu) &&
 +	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 +		trace_kvm_fast_mmio(gpa);
 +		/*
 +		 * Doing kvm_skip_emulated_instruction() depends on undefined
 +		 * behavior: Intel's manual doesn't mandate
 +		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 +		 * occurs and while on real hardware it was observed to be set,
 +		 * other hypervisors (namely Hyper-V) don't set it, we end up
 +		 * advancing IP with some random value. Disable fast mmio when
 +		 * running nested and keep it for real hardware in hope that
 +		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 +		 */
 +		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 +			return kvm_skip_emulated_instruction(vcpu);
 +		else
 +			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 +								EMULATE_DONE;
 +	}
 +
 +	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 +}
 +
 +static int handle_nmi_window(struct kvm_vcpu *vcpu)
 +{
 +	WARN_ON_ONCE(!enable_vnmi);
 +	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 +			CPU_BASED_VIRTUAL_NMI_PENDING);
 +	++vcpu->stat.nmi_window_exits;
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +
 +	return 1;
 +}
 +
 +static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	enum emulation_result err = EMULATE_DONE;
 +	int ret = 1;
 +	u32 cpu_exec_ctrl;
 +	bool intr_window_requested;
 +	unsigned count = 130;
 +
 +	/*
 +	 * We should never reach the point where we are emulating L2
 +	 * due to invalid guest state as that means we incorrectly
 +	 * allowed a nested VMEntry with an invalid vmcs12.
 +	 */
 +	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
 +
 +	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 +	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
 +
 +	while (vmx->emulation_required && count-- != 0) {
 +		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 +			return handle_interrupt_window(&vmx->vcpu);
 +
 +		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
 +			return 1;
 +
 +		err = kvm_emulate_instruction(vcpu, 0);
 +
 +		if (err == EMULATE_USER_EXIT) {
 +			++vcpu->stat.mmio_exits;
 +			ret = 0;
 +			goto out;
 +		}
 +
 +		if (err != EMULATE_DONE)
 +			goto emulation_error;
 +
 +		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 +		    vcpu->arch.exception.pending)
 +			goto emulation_error;
 +
 +		if (vcpu->arch.halt_request) {
 +			vcpu->arch.halt_request = 0;
 +			ret = kvm_vcpu_halt(vcpu);
 +			goto out;
 +		}
 +
 +		if (signal_pending(current))
 +			goto out;
 +		if (need_resched())
 +			schedule();
 +	}
 +
 +out:
 +	return ret;
 +
 +emulation_error:
 +	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 +	vcpu->run->internal.ndata = 0;
 +	return 0;
 +}
 +
 +static void grow_ple_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int old = vmx->ple_window;
 +
 +	vmx->ple_window = __grow_ple_window(old, ple_window,
 +					    ple_window_grow,
 +					    ple_window_max);
 +
 +	if (vmx->ple_window != old)
 +		vmx->ple_window_dirty = true;
 +
 +	trace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);
 +}
 +
 +static void shrink_ple_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int old = vmx->ple_window;
 +
 +	vmx->ple_window = __shrink_ple_window(old, ple_window,
 +					      ple_window_shrink,
 +					      ple_window);
 +
 +	if (vmx->ple_window != old)
 +		vmx->ple_window_dirty = true;
 +
 +	trace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);
 +}
 +
 +/*
 + * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
 + */
 +static void wakeup_handler(void)
 +{
 +	struct kvm_vcpu *vcpu;
 +	int cpu = smp_processor_id();
 +
 +	spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 +	list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
 +			blocked_vcpu_list) {
 +		struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
 +
 +		if (pi_test_on(pi_desc) == 1)
 +			kvm_vcpu_kick(vcpu);
 +	}
 +	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 +}
 +
 +static void vmx_enable_tdp(void)
 +{
 +	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
 +		enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull,
 +		enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
 +		0ull, VMX_EPT_EXECUTABLE_MASK,
 +		cpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK,
 +		VMX_EPT_RWX_MASK, 0ull);
 +
 +	ept_set_mmio_spte_mask();
 +	kvm_enable_tdp();
 +}
 +
 +/*
 + * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
 + * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
 + */
 +static int handle_pause(struct kvm_vcpu *vcpu)
 +{
 +	if (!kvm_pause_in_guest(vcpu->kvm))
 +		grow_ple_window(vcpu);
 +
 +	/*
 +	 * Intel sdm vol3 ch-25.1.3 says: The "PAUSE-loop exiting"
 +	 * VM-execution control is ignored if CPL > 0. OTOH, KVM
 +	 * never set PAUSE_EXITING and just set PLE if supported,
 +	 * so the vcpu must be CPL=0 if it gets a PAUSE exit.
 +	 */
 +	kvm_vcpu_on_spin(vcpu, true);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_nop(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_mwait(struct kvm_vcpu *vcpu)
 +{
 +	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 +	return handle_nop(vcpu);
 +}
 +
 +static int handle_invalid_op(struct kvm_vcpu *vcpu)
 +{
 +	kvm_queue_exception(vcpu, UD_VECTOR);
 +	return 1;
 +}
 +
 +static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 +{
 +	return 1;
 +}
 +
 +static int handle_monitor(struct kvm_vcpu *vcpu)
 +{
 +	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 +	return handle_nop(vcpu);
 +}
 +
 +/*
 + * The following 3 functions, nested_vmx_succeed()/failValid()/failInvalid(),
 + * set the success or error code of an emulated VMX instruction, as specified
 + * by Vol 2B, VMX Instruction Reference, "Conventions".
 + */
 +static void nested_vmx_succeed(struct kvm_vcpu *vcpu)
 +{
 +	vmx_set_rflags(vcpu, vmx_get_rflags(vcpu)
 +			& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |
 +			    X86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_OF));
 +}
 +
 +static void nested_vmx_failInvalid(struct kvm_vcpu *vcpu)
 +{
 +	vmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)
 +			& ~(X86_EFLAGS_PF | X86_EFLAGS_AF | X86_EFLAGS_ZF |
 +			    X86_EFLAGS_SF | X86_EFLAGS_OF))
 +			| X86_EFLAGS_CF);
 +}
 +
 +static void nested_vmx_failValid(struct kvm_vcpu *vcpu,
 +					u32 vm_instruction_error)
 +{
 +	if (to_vmx(vcpu)->nested.current_vmptr == -1ull) {
 +		/*
 +		 * failValid writes the error number to the current VMCS, which
 +		 * can't be done there isn't a current VMCS.
 +		 */
 +		nested_vmx_failInvalid(vcpu);
 +		return;
 +	}
 +	vmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)
 +			& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |
 +			    X86_EFLAGS_SF | X86_EFLAGS_OF))
 +			| X86_EFLAGS_ZF);
 +	get_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;
 +	/*
 +	 * We don't need to force a shadow sync because
 +	 * VM_INSTRUCTION_ERROR is not shadowed
 +	 */
 +}
 +
 +static void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)
 +{
 +	/* TODO: not to reset guest simply here. */
 +	kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 +	pr_debug_ratelimited("kvm: nested vmx abort, indicator %d\n", indicator);
 +}
 +
 +static enum hrtimer_restart vmx_preemption_timer_fn(struct hrtimer *timer)
 +{
 +	struct vcpu_vmx *vmx =
 +		container_of(timer, struct vcpu_vmx, nested.preemption_timer);
 +
 +	vmx->nested.preemption_timer_expired = true;
 +	kvm_make_request(KVM_REQ_EVENT, &vmx->vcpu);
 +	kvm_vcpu_kick(&vmx->vcpu);
 +
 +	return HRTIMER_NORESTART;
 +}
 +
 +/*
 + * Decode the memory-address operand of a vmx instruction, as recorded on an
 + * exit caused by such an instruction (run by a guest hypervisor).
 + * On success, returns 0. When the operand is invalid, returns 1 and throws
 + * #UD or #GP.
 + */
 +static int get_vmx_mem_address(struct kvm_vcpu *vcpu,
 +				 unsigned long exit_qualification,
 +				 u32 vmx_instruction_info, bool wr, gva_t *ret)
 +{
 +	gva_t off;
 +	bool exn;
 +	struct kvm_segment s;
 +
 +	/*
 +	 * According to Vol. 3B, "Information for VM Exits Due to Instruction
 +	 * Execution", on an exit, vmx_instruction_info holds most of the
 +	 * addressing components of the operand. Only the displacement part
 +	 * is put in exit_qualification (see 3B, "Basic VM-Exit Information").
 +	 * For how an actual address is calculated from all these components,
 +	 * refer to Vol. 1, "Operand Addressing".
 +	 */
 +	int  scaling = vmx_instruction_info & 3;
 +	int  addr_size = (vmx_instruction_info >> 7) & 7;
 +	bool is_reg = vmx_instruction_info & (1u << 10);
 +	int  seg_reg = (vmx_instruction_info >> 15) & 7;
 +	int  index_reg = (vmx_instruction_info >> 18) & 0xf;
 +	bool index_is_valid = !(vmx_instruction_info & (1u << 22));
 +	int  base_reg       = (vmx_instruction_info >> 23) & 0xf;
 +	bool base_is_valid  = !(vmx_instruction_info & (1u << 27));
 +
 +	if (is_reg) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	/* Addr = segment_base + offset */
 +	/* offset = base + [index * scale] + displacement */
 +	off = exit_qualification; /* holds the displacement */
 +	if (base_is_valid)
 +		off += kvm_register_read(vcpu, base_reg);
 +	if (index_is_valid)
 +		off += kvm_register_read(vcpu, index_reg)<<scaling;
 +	vmx_get_segment(vcpu, &s, seg_reg);
 +	*ret = s.base + off;
 +
 +	if (addr_size == 1) /* 32 bit */
 +		*ret &= 0xffffffff;
 +
 +	/* Checks for #GP/#SS exceptions. */
 +	exn = false;
 +	if (is_long_mode(vcpu)) {
 +		/* Long mode: #GP(0)/#SS(0) if the memory address is in a
 +		 * non-canonical form. This is the only check on the memory
 +		 * destination for long mode!
 +		 */
 +		exn = is_noncanonical_address(*ret, vcpu);
 +	} else if (is_protmode(vcpu)) {
 +		/* Protected mode: apply checks for segment validity in the
 +		 * following order:
 +		 * - segment type check (#GP(0) may be thrown)
 +		 * - usability check (#GP(0)/#SS(0))
 +		 * - limit check (#GP(0)/#SS(0))
 +		 */
 +		if (wr)
 +			/* #GP(0) if the destination operand is located in a
 +			 * read-only data segment or any code segment.
 +			 */
 +			exn = ((s.type & 0xa) == 0 || (s.type & 8));
 +		else
 +			/* #GP(0) if the source operand is located in an
 +			 * execute-only code segment
 +			 */
 +			exn = ((s.type & 0xa) == 8);
 +		if (exn) {
 +			kvm_queue_exception_e(vcpu, GP_VECTOR, 0);
 +			return 1;
 +		}
 +		/* Protected mode: #GP(0)/#SS(0) if the segment is unusable.
 +		 */
 +		exn = (s.unusable != 0);
 +		/* Protected mode: #GP(0)/#SS(0) if the memory
 +		 * operand is outside the segment limit.
 +		 */
 +		exn = exn || (off + sizeof(u64) > s.limit);
 +	}
 +	if (exn) {
 +		kvm_queue_exception_e(vcpu,
 +				      seg_reg == VCPU_SREG_SS ?
 +						SS_VECTOR : GP_VECTOR,
 +				      0);
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer)
 +{
 +	gva_t gva;
 +	struct x86_exception e;
 +
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +			vmcs_read32(VMX_INSTRUCTION_INFO), false, &gva))
 +		return 1;
 +
 +	if (kvm_read_guest_virt(vcpu, gva, vmpointer, sizeof(*vmpointer), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +/*
 + * Allocate a shadow VMCS and associate it with the currently loaded
 + * VMCS, unless such a shadow VMCS already exists. The newly allocated
 + * VMCS is also VMCLEARed, so that it is ready for use.
 + */
 +static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct loaded_vmcs *loaded_vmcs = vmx->loaded_vmcs;
 +
 +	/*
 +	 * We should allocate a shadow vmcs for vmcs01 only when L1
 +	 * executes VMXON and free it when L1 executes VMXOFF.
 +	 * As it is invalid to execute VMXON twice, we shouldn't reach
 +	 * here when vmcs01 already have an allocated shadow vmcs.
 +	 */
 +	WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
 +
 +	if (!loaded_vmcs->shadow_vmcs) {
 +		loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
 +		if (loaded_vmcs->shadow_vmcs)
 +			vmcs_clear(loaded_vmcs->shadow_vmcs);
 +	}
 +	return loaded_vmcs->shadow_vmcs;
 +}
 +
 +static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int r;
 +
 +	r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
 +	if (r < 0)
 +		goto out_vmcs02;
 +
 +	vmx->nested.cached_vmcs12 = kmalloc(VMCS12_SIZE, GFP_KERNEL);
 +	if (!vmx->nested.cached_vmcs12)
 +		goto out_cached_vmcs12;
 +
 +	vmx->nested.cached_shadow_vmcs12 = kmalloc(VMCS12_SIZE, GFP_KERNEL);
 +	if (!vmx->nested.cached_shadow_vmcs12)
 +		goto out_cached_shadow_vmcs12;
 +
 +	if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
 +		goto out_shadow_vmcs;
 +
 +	hrtimer_init(&vmx->nested.preemption_timer, CLOCK_MONOTONIC,
 +		     HRTIMER_MODE_REL_PINNED);
 +	vmx->nested.preemption_timer.function = vmx_preemption_timer_fn;
 +
 +	vmx->nested.vpid02 = allocate_vpid();
 +
 +	vmx->nested.vmcs02_initialized = false;
 +	vmx->nested.vmxon = true;
 +	return 0;
 +
 +out_shadow_vmcs:
 +	kfree(vmx->nested.cached_shadow_vmcs12);
 +
 +out_cached_shadow_vmcs12:
 +	kfree(vmx->nested.cached_vmcs12);
 +
 +out_cached_vmcs12:
 +	free_loaded_vmcs(&vmx->nested.vmcs02);
 +
 +out_vmcs02:
 +	return -ENOMEM;
 +}
 +
 +/*
 + * Emulate the VMXON instruction.
 + * Currently, we just remember that VMX is active, and do not save or even
 + * inspect the argument to VMXON (the so-called "VMXON pointer") because we
 + * do not currently need to store anything in that guest-allocated memory
 + * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their
 + * argument is different from the VMXON pointer (which the spec says they do).
 + */
 +static int handle_vmon(struct kvm_vcpu *vcpu)
 +{
 +	int ret;
 +	gpa_t vmptr;
 +	struct page *page;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	const u64 VMXON_NEEDED_FEATURES = FEATURE_CONTROL_LOCKED
 +		| FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;
 +
 +	/*
 +	 * The Intel VMX Instruction Reference lists a bunch of bits that are
 +	 * prerequisite to running VMXON, most notably cr4.VMXE must be set to
 +	 * 1 (see vmx_set_cr4() for when we allow the guest to set this).
 +	 * Otherwise, we should fail with #UD.  But most faulting conditions
 +	 * have already been checked by hardware, prior to the VM-exit for
 +	 * VMXON.  We do test guest cr4.VMXE because processor CR4 always has
 +	 * that bit set to 1 in non-root mode.
 +	 */
 +	if (!kvm_read_cr4_bits(vcpu, X86_CR4_VMXE)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	/* CPL=0 must be checked manually. */
 +	if (vmx_get_cpl(vcpu)) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	if (vmx->nested.vmxon) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if ((vmx->msr_ia32_feature_control & VMXON_NEEDED_FEATURES)
 +			!= VMXON_NEEDED_FEATURES) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	if (nested_vmx_get_vmptr(vcpu, &vmptr))
 +		return 1;
 +
 +	/*
 +	 * SDM 3: 24.11.5
 +	 * The first 4 bytes of VMXON region contain the supported
 +	 * VMCS revision identifier
 +	 *
 +	 * Note - IA32_VMX_BASIC[48] will never be 1 for the nested case;
 +	 * which replaces physical address width with 32
 +	 */
 +	if (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {
 +		nested_vmx_failInvalid(vcpu);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	page = kvm_vcpu_gpa_to_page(vcpu, vmptr);
 +	if (is_error_page(page)) {
 +		nested_vmx_failInvalid(vcpu);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +	if (*(u32 *)kmap(page) != VMCS12_REVISION) {
 +		kunmap(page);
 +		kvm_release_page_clean(page);
 +		nested_vmx_failInvalid(vcpu);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +	kunmap(page);
 +	kvm_release_page_clean(page);
 +
 +	vmx->nested.vmxon_ptr = vmptr;
 +	ret = enter_vmx_operation(vcpu);
 +	if (ret)
 +		return ret;
 +
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +/*
 + * Intel's VMX Instruction Reference specifies a common set of prerequisites
 + * for running VMX instructions (except VMXON, whose prerequisites are
 + * slightly different). It also specifies what exception to inject otherwise.
 + * Note that many of these exceptions have priority over VM exits, so they
 + * don't have to be checked again here.
 + */
 +static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
 +{
 +	if (!to_vmx(vcpu)->nested.vmxon) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 0;
 +	}
 +
 +	if (vmx_get_cpl(vcpu)) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 0;
 +	}
 +
 +	return 1;
 +}
 +
 +static void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)
 +{
 +	vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL, SECONDARY_EXEC_SHADOW_VMCS);
 +	vmcs_write64(VMCS_LINK_POINTER, -1ull);
 +}
 +
 +static inline void nested_release_vmcs12(struct vcpu_vmx *vmx)
 +{
 +	if (vmx->nested.current_vmptr == -1ull)
 +		return;
 +
 +	if (enable_shadow_vmcs) {
 +		/* copy to memory all shadowed fields in case
 +		   they were modified */
 +		copy_shadow_to_vmcs12(vmx);
 +		vmx->nested.need_vmcs12_sync = false;
 +		vmx_disable_shadow_vmcs(vmx);
 +	}
 +	vmx->nested.posted_intr_nv = -1;
 +
 +	/* Flush VMCS12 to guest memory */
 +	kvm_vcpu_write_guest_page(&vmx->vcpu,
 +				  vmx->nested.current_vmptr >> PAGE_SHIFT,
 +				  vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
 +
 +	vmx->nested.current_vmptr = -1ull;
 +}
 +
 +/*
 + * Free whatever needs to be freed from vmx->nested when L1 goes down, or
 + * just stops using VMX.
 + */
 +static void free_nested(struct vcpu_vmx *vmx)
 +{
 +	if (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)
 +		return;
 +
 +	hrtimer_cancel(&vmx->nested.preemption_timer);
 +	vmx->nested.vmxon = false;
 +	vmx->nested.smm.vmxon = false;
 +	free_vpid(vmx->nested.vpid02);
 +	vmx->nested.posted_intr_nv = -1;
 +	vmx->nested.current_vmptr = -1ull;
 +	if (enable_shadow_vmcs) {
 +		vmx_disable_shadow_vmcs(vmx);
 +		vmcs_clear(vmx->vmcs01.shadow_vmcs);
 +		free_vmcs(vmx->vmcs01.shadow_vmcs);
 +		vmx->vmcs01.shadow_vmcs = NULL;
 +	}
 +	kfree(vmx->nested.cached_vmcs12);
 +	kfree(vmx->nested.cached_shadow_vmcs12);
 +	/* Unpin physical memory we referred to in the vmcs02 */
 +	if (vmx->nested.apic_access_page) {
 +		kvm_release_page_dirty(vmx->nested.apic_access_page);
 +		vmx->nested.apic_access_page = NULL;
 +	}
 +	if (vmx->nested.virtual_apic_page) {
 +		kvm_release_page_dirty(vmx->nested.virtual_apic_page);
 +		vmx->nested.virtual_apic_page = NULL;
 +	}
 +	if (vmx->nested.pi_desc_page) {
 +		kunmap(vmx->nested.pi_desc_page);
 +		kvm_release_page_dirty(vmx->nested.pi_desc_page);
 +		vmx->nested.pi_desc_page = NULL;
 +		vmx->nested.pi_desc = NULL;
 +	}
 +
 +	free_loaded_vmcs(&vmx->nested.vmcs02);
 +}
 +
 +/* Emulate the VMXOFF instruction */
 +static int handle_vmoff(struct kvm_vcpu *vcpu)
 +{
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +	free_nested(to_vmx(vcpu));
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +/* Emulate the VMCLEAR instruction */
 +static int handle_vmclear(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 zero = 0;
 +	gpa_t vmptr;
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	if (nested_vmx_get_vmptr(vcpu, &vmptr))
 +		return 1;
 +
 +	if (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if (vmptr == vmx->nested.vmxon_ptr) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if (vmptr == vmx->nested.current_vmptr)
 +		nested_release_vmcs12(vmx);
 +
 +	kvm_vcpu_write_guest(vcpu,
 +			vmptr + offsetof(struct vmcs12, launch_state),
 +			&zero, sizeof(zero));
 +
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);
 +
 +/* Emulate the VMLAUNCH instruction */
 +static int handle_vmlaunch(struct kvm_vcpu *vcpu)
 +{
 +	return nested_vmx_run(vcpu, true);
 +}
 +
 +/* Emulate the VMRESUME instruction */
 +static int handle_vmresume(struct kvm_vcpu *vcpu)
 +{
 +
 +	return nested_vmx_run(vcpu, false);
 +}
 +
 +/*
 + * Read a vmcs12 field. Since these can have varying lengths and we return
 + * one type, we chose the biggest type (u64) and zero-extend the return value
 + * to that size. Note that the caller, handle_vmread, might need to use only
 + * some of the bits we return here (e.g., on 32-bit guests, only 32 bits of
 + * 64-bit fields are to be returned).
 + */
 +static inline int vmcs12_read_any(struct vmcs12 *vmcs12,
 +				  unsigned long field, u64 *ret)
 +{
 +	short offset = vmcs_field_to_offset(field);
 +	char *p;
 +
 +	if (offset < 0)
 +		return offset;
 +
 +	p = (char *)vmcs12 + offset;
 +
 +	switch (vmcs_field_width(field)) {
 +	case VMCS_FIELD_WIDTH_NATURAL_WIDTH:
 +		*ret = *((natural_width *)p);
 +		return 0;
 +	case VMCS_FIELD_WIDTH_U16:
 +		*ret = *((u16 *)p);
 +		return 0;
 +	case VMCS_FIELD_WIDTH_U32:
 +		*ret = *((u32 *)p);
 +		return 0;
 +	case VMCS_FIELD_WIDTH_U64:
 +		*ret = *((u64 *)p);
 +		return 0;
 +	default:
 +		WARN_ON(1);
 +		return -ENOENT;
 +	}
 +}
 +
 +
 +static inline int vmcs12_write_any(struct vmcs12 *vmcs12,
 +				   unsigned long field, u64 field_value){
 +	short offset = vmcs_field_to_offset(field);
 +	char *p = (char *)vmcs12 + offset;
 +	if (offset < 0)
 +		return offset;
 +
 +	switch (vmcs_field_width(field)) {
 +	case VMCS_FIELD_WIDTH_U16:
 +		*(u16 *)p = field_value;
 +		return 0;
 +	case VMCS_FIELD_WIDTH_U32:
 +		*(u32 *)p = field_value;
 +		return 0;
 +	case VMCS_FIELD_WIDTH_U64:
 +		*(u64 *)p = field_value;
 +		return 0;
 +	case VMCS_FIELD_WIDTH_NATURAL_WIDTH:
 +		*(natural_width *)p = field_value;
 +		return 0;
 +	default:
 +		WARN_ON(1);
 +		return -ENOENT;
 +	}
 +
 +}
 +
 +static int copy_enlightened_to_vmcs12(struct vcpu_vmx *vmx)
 +{
 +	struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
 +	struct hv_enlightened_vmcs *evmcs = vmx->nested.hv_evmcs;
 +
 +	/* HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE */
 +	vmcs12->tpr_threshold = evmcs->tpr_threshold;
 +	vmcs12->guest_rip = evmcs->guest_rip;
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_BASIC))) {
 +		vmcs12->guest_rsp = evmcs->guest_rsp;
 +		vmcs12->guest_rflags = evmcs->guest_rflags;
 +		vmcs12->guest_interruptibility_info =
 +			evmcs->guest_interruptibility_info;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_PROC))) {
 +		vmcs12->cpu_based_vm_exec_control =
 +			evmcs->cpu_based_vm_exec_control;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_PROC))) {
 +		vmcs12->exception_bitmap = evmcs->exception_bitmap;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_ENTRY))) {
 +		vmcs12->vm_entry_controls = evmcs->vm_entry_controls;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_EVENT))) {
 +		vmcs12->vm_entry_intr_info_field =
 +			evmcs->vm_entry_intr_info_field;
 +		vmcs12->vm_entry_exception_error_code =
 +			evmcs->vm_entry_exception_error_code;
 +		vmcs12->vm_entry_instruction_len =
 +			evmcs->vm_entry_instruction_len;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_HOST_GRP1))) {
 +		vmcs12->host_ia32_pat = evmcs->host_ia32_pat;
 +		vmcs12->host_ia32_efer = evmcs->host_ia32_efer;
 +		vmcs12->host_cr0 = evmcs->host_cr0;
 +		vmcs12->host_cr3 = evmcs->host_cr3;
 +		vmcs12->host_cr4 = evmcs->host_cr4;
 +		vmcs12->host_ia32_sysenter_esp = evmcs->host_ia32_sysenter_esp;
 +		vmcs12->host_ia32_sysenter_eip = evmcs->host_ia32_sysenter_eip;
 +		vmcs12->host_rip = evmcs->host_rip;
 +		vmcs12->host_ia32_sysenter_cs = evmcs->host_ia32_sysenter_cs;
 +		vmcs12->host_es_selector = evmcs->host_es_selector;
 +		vmcs12->host_cs_selector = evmcs->host_cs_selector;
 +		vmcs12->host_ss_selector = evmcs->host_ss_selector;
 +		vmcs12->host_ds_selector = evmcs->host_ds_selector;
 +		vmcs12->host_fs_selector = evmcs->host_fs_selector;
 +		vmcs12->host_gs_selector = evmcs->host_gs_selector;
 +		vmcs12->host_tr_selector = evmcs->host_tr_selector;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_HOST_GRP1))) {
 +		vmcs12->pin_based_vm_exec_control =
 +			evmcs->pin_based_vm_exec_control;
 +		vmcs12->vm_exit_controls = evmcs->vm_exit_controls;
 +		vmcs12->secondary_vm_exec_control =
 +			evmcs->secondary_vm_exec_control;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_IO_BITMAP))) {
 +		vmcs12->io_bitmap_a = evmcs->io_bitmap_a;
 +		vmcs12->io_bitmap_b = evmcs->io_bitmap_b;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP))) {
 +		vmcs12->msr_bitmap = evmcs->msr_bitmap;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP2))) {
 +		vmcs12->guest_es_base = evmcs->guest_es_base;
 +		vmcs12->guest_cs_base = evmcs->guest_cs_base;
 +		vmcs12->guest_ss_base = evmcs->guest_ss_base;
 +		vmcs12->guest_ds_base = evmcs->guest_ds_base;
 +		vmcs12->guest_fs_base = evmcs->guest_fs_base;
 +		vmcs12->guest_gs_base = evmcs->guest_gs_base;
 +		vmcs12->guest_ldtr_base = evmcs->guest_ldtr_base;
 +		vmcs12->guest_tr_base = evmcs->guest_tr_base;
 +		vmcs12->guest_gdtr_base = evmcs->guest_gdtr_base;
 +		vmcs12->guest_idtr_base = evmcs->guest_idtr_base;
 +		vmcs12->guest_es_limit = evmcs->guest_es_limit;
 +		vmcs12->guest_cs_limit = evmcs->guest_cs_limit;
 +		vmcs12->guest_ss_limit = evmcs->guest_ss_limit;
 +		vmcs12->guest_ds_limit = evmcs->guest_ds_limit;
 +		vmcs12->guest_fs_limit = evmcs->guest_fs_limit;
 +		vmcs12->guest_gs_limit = evmcs->guest_gs_limit;
 +		vmcs12->guest_ldtr_limit = evmcs->guest_ldtr_limit;
 +		vmcs12->guest_tr_limit = evmcs->guest_tr_limit;
 +		vmcs12->guest_gdtr_limit = evmcs->guest_gdtr_limit;
 +		vmcs12->guest_idtr_limit = evmcs->guest_idtr_limit;
 +		vmcs12->guest_es_ar_bytes = evmcs->guest_es_ar_bytes;
 +		vmcs12->guest_cs_ar_bytes = evmcs->guest_cs_ar_bytes;
 +		vmcs12->guest_ss_ar_bytes = evmcs->guest_ss_ar_bytes;
 +		vmcs12->guest_ds_ar_bytes = evmcs->guest_ds_ar_bytes;
 +		vmcs12->guest_fs_ar_bytes = evmcs->guest_fs_ar_bytes;
 +		vmcs12->guest_gs_ar_bytes = evmcs->guest_gs_ar_bytes;
 +		vmcs12->guest_ldtr_ar_bytes = evmcs->guest_ldtr_ar_bytes;
 +		vmcs12->guest_tr_ar_bytes = evmcs->guest_tr_ar_bytes;
 +		vmcs12->guest_es_selector = evmcs->guest_es_selector;
 +		vmcs12->guest_cs_selector = evmcs->guest_cs_selector;
 +		vmcs12->guest_ss_selector = evmcs->guest_ss_selector;
 +		vmcs12->guest_ds_selector = evmcs->guest_ds_selector;
 +		vmcs12->guest_fs_selector = evmcs->guest_fs_selector;
 +		vmcs12->guest_gs_selector = evmcs->guest_gs_selector;
 +		vmcs12->guest_ldtr_selector = evmcs->guest_ldtr_selector;
 +		vmcs12->guest_tr_selector = evmcs->guest_tr_selector;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_GRP2))) {
 +		vmcs12->tsc_offset = evmcs->tsc_offset;
 +		vmcs12->virtual_apic_page_addr = evmcs->virtual_apic_page_addr;
 +		vmcs12->xss_exit_bitmap = evmcs->xss_exit_bitmap;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CRDR))) {
 +		vmcs12->cr0_guest_host_mask = evmcs->cr0_guest_host_mask;
 +		vmcs12->cr4_guest_host_mask = evmcs->cr4_guest_host_mask;
 +		vmcs12->cr0_read_shadow = evmcs->cr0_read_shadow;
 +		vmcs12->cr4_read_shadow = evmcs->cr4_read_shadow;
 +		vmcs12->guest_cr0 = evmcs->guest_cr0;
 +		vmcs12->guest_cr3 = evmcs->guest_cr3;
 +		vmcs12->guest_cr4 = evmcs->guest_cr4;
 +		vmcs12->guest_dr7 = evmcs->guest_dr7;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_HOST_POINTER))) {
 +		vmcs12->host_fs_base = evmcs->host_fs_base;
 +		vmcs12->host_gs_base = evmcs->host_gs_base;
 +		vmcs12->host_tr_base = evmcs->host_tr_base;
 +		vmcs12->host_gdtr_base = evmcs->host_gdtr_base;
 +		vmcs12->host_idtr_base = evmcs->host_idtr_base;
 +		vmcs12->host_rsp = evmcs->host_rsp;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_XLAT))) {
 +		vmcs12->ept_pointer = evmcs->ept_pointer;
 +		vmcs12->virtual_processor_id = evmcs->virtual_processor_id;
 +	}
 +
 +	if (unlikely(!(evmcs->hv_clean_fields &
 +		       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP1))) {
 +		vmcs12->vmcs_link_pointer = evmcs->vmcs_link_pointer;
 +		vmcs12->guest_ia32_debugctl = evmcs->guest_ia32_debugctl;
 +		vmcs12->guest_ia32_pat = evmcs->guest_ia32_pat;
 +		vmcs12->guest_ia32_efer = evmcs->guest_ia32_efer;
 +		vmcs12->guest_pdptr0 = evmcs->guest_pdptr0;
 +		vmcs12->guest_pdptr1 = evmcs->guest_pdptr1;
 +		vmcs12->guest_pdptr2 = evmcs->guest_pdptr2;
 +		vmcs12->guest_pdptr3 = evmcs->guest_pdptr3;
 +		vmcs12->guest_pending_dbg_exceptions =
 +			evmcs->guest_pending_dbg_exceptions;
 +		vmcs12->guest_sysenter_esp = evmcs->guest_sysenter_esp;
 +		vmcs12->guest_sysenter_eip = evmcs->guest_sysenter_eip;
 +		vmcs12->guest_bndcfgs = evmcs->guest_bndcfgs;
 +		vmcs12->guest_activity_state = evmcs->guest_activity_state;
 +		vmcs12->guest_sysenter_cs = evmcs->guest_sysenter_cs;
 +	}
 +
 +	/*
 +	 * Not used?
 +	 * vmcs12->vm_exit_msr_store_addr = evmcs->vm_exit_msr_store_addr;
 +	 * vmcs12->vm_exit_msr_load_addr = evmcs->vm_exit_msr_load_addr;
 +	 * vmcs12->vm_entry_msr_load_addr = evmcs->vm_entry_msr_load_addr;
 +	 * vmcs12->cr3_target_value0 = evmcs->cr3_target_value0;
 +	 * vmcs12->cr3_target_value1 = evmcs->cr3_target_value1;
 +	 * vmcs12->cr3_target_value2 = evmcs->cr3_target_value2;
 +	 * vmcs12->cr3_target_value3 = evmcs->cr3_target_value3;
 +	 * vmcs12->page_fault_error_code_mask =
 +	 *		evmcs->page_fault_error_code_mask;
 +	 * vmcs12->page_fault_error_code_match =
 +	 *		evmcs->page_fault_error_code_match;
 +	 * vmcs12->cr3_target_count = evmcs->cr3_target_count;
 +	 * vmcs12->vm_exit_msr_store_count = evmcs->vm_exit_msr_store_count;
 +	 * vmcs12->vm_exit_msr_load_count = evmcs->vm_exit_msr_load_count;
 +	 * vmcs12->vm_entry_msr_load_count = evmcs->vm_entry_msr_load_count;
 +	 */
 +
 +	/*
 +	 * Read only fields:
 +	 * vmcs12->guest_physical_address = evmcs->guest_physical_address;
 +	 * vmcs12->vm_instruction_error = evmcs->vm_instruction_error;
 +	 * vmcs12->vm_exit_reason = evmcs->vm_exit_reason;
 +	 * vmcs12->vm_exit_intr_info = evmcs->vm_exit_intr_info;
 +	 * vmcs12->vm_exit_intr_error_code = evmcs->vm_exit_intr_error_code;
 +	 * vmcs12->idt_vectoring_info_field = evmcs->idt_vectoring_info_field;
 +	 * vmcs12->idt_vectoring_error_code = evmcs->idt_vectoring_error_code;
 +	 * vmcs12->vm_exit_instruction_len = evmcs->vm_exit_instruction_len;
 +	 * vmcs12->vmx_instruction_info = evmcs->vmx_instruction_info;
 +	 * vmcs12->exit_qualification = evmcs->exit_qualification;
 +	 * vmcs12->guest_linear_address = evmcs->guest_linear_address;
 +	 *
 +	 * Not present in struct vmcs12:
 +	 * vmcs12->exit_io_instruction_ecx = evmcs->exit_io_instruction_ecx;
 +	 * vmcs12->exit_io_instruction_esi = evmcs->exit_io_instruction_esi;
 +	 * vmcs12->exit_io_instruction_edi = evmcs->exit_io_instruction_edi;
 +	 * vmcs12->exit_io_instruction_eip = evmcs->exit_io_instruction_eip;
 +	 */
 +
 +	return 0;
 +}
 +
 +static int copy_vmcs12_to_enlightened(struct vcpu_vmx *vmx)
 +{
 +	struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
 +	struct hv_enlightened_vmcs *evmcs = vmx->nested.hv_evmcs;
 +
 +	/*
 +	 * Should not be changed by KVM:
 +	 *
 +	 * evmcs->host_es_selector = vmcs12->host_es_selector;
 +	 * evmcs->host_cs_selector = vmcs12->host_cs_selector;
 +	 * evmcs->host_ss_selector = vmcs12->host_ss_selector;
 +	 * evmcs->host_ds_selector = vmcs12->host_ds_selector;
 +	 * evmcs->host_fs_selector = vmcs12->host_fs_selector;
 +	 * evmcs->host_gs_selector = vmcs12->host_gs_selector;
 +	 * evmcs->host_tr_selector = vmcs12->host_tr_selector;
 +	 * evmcs->host_ia32_pat = vmcs12->host_ia32_pat;
 +	 * evmcs->host_ia32_efer = vmcs12->host_ia32_efer;
 +	 * evmcs->host_cr0 = vmcs12->host_cr0;
 +	 * evmcs->host_cr3 = vmcs12->host_cr3;
 +	 * evmcs->host_cr4 = vmcs12->host_cr4;
 +	 * evmcs->host_ia32_sysenter_esp = vmcs12->host_ia32_sysenter_esp;
 +	 * evmcs->host_ia32_sysenter_eip = vmcs12->host_ia32_sysenter_eip;
 +	 * evmcs->host_rip = vmcs12->host_rip;
 +	 * evmcs->host_ia32_sysenter_cs = vmcs12->host_ia32_sysenter_cs;
 +	 * evmcs->host_fs_base = vmcs12->host_fs_base;
 +	 * evmcs->host_gs_base = vmcs12->host_gs_base;
 +	 * evmcs->host_tr_base = vmcs12->host_tr_base;
 +	 * evmcs->host_gdtr_base = vmcs12->host_gdtr_base;
 +	 * evmcs->host_idtr_base = vmcs12->host_idtr_base;
 +	 * evmcs->host_rsp = vmcs12->host_rsp;
 +	 * sync_vmcs12() doesn't read these:
 +	 * evmcs->io_bitmap_a = vmcs12->io_bitmap_a;
 +	 * evmcs->io_bitmap_b = vmcs12->io_bitmap_b;
 +	 * evmcs->msr_bitmap = vmcs12->msr_bitmap;
 +	 * evmcs->ept_pointer = vmcs12->ept_pointer;
 +	 * evmcs->xss_exit_bitmap = vmcs12->xss_exit_bitmap;
 +	 * evmcs->vm_exit_msr_store_addr = vmcs12->vm_exit_msr_store_addr;
 +	 * evmcs->vm_exit_msr_load_addr = vmcs12->vm_exit_msr_load_addr;
 +	 * evmcs->vm_entry_msr_load_addr = vmcs12->vm_entry_msr_load_addr;
 +	 * evmcs->cr3_target_value0 = vmcs12->cr3_target_value0;
 +	 * evmcs->cr3_target_value1 = vmcs12->cr3_target_value1;
 +	 * evmcs->cr3_target_value2 = vmcs12->cr3_target_value2;
 +	 * evmcs->cr3_target_value3 = vmcs12->cr3_target_value3;
 +	 * evmcs->tpr_threshold = vmcs12->tpr_threshold;
 +	 * evmcs->virtual_processor_id = vmcs12->virtual_processor_id;
 +	 * evmcs->exception_bitmap = vmcs12->exception_bitmap;
 +	 * evmcs->vmcs_link_pointer = vmcs12->vmcs_link_pointer;
 +	 * evmcs->pin_based_vm_exec_control = vmcs12->pin_based_vm_exec_control;
 +	 * evmcs->vm_exit_controls = vmcs12->vm_exit_controls;
 +	 * evmcs->secondary_vm_exec_control = vmcs12->secondary_vm_exec_control;
 +	 * evmcs->page_fault_error_code_mask =
 +	 *		vmcs12->page_fault_error_code_mask;
 +	 * evmcs->page_fault_error_code_match =
 +	 *		vmcs12->page_fault_error_code_match;
 +	 * evmcs->cr3_target_count = vmcs12->cr3_target_count;
 +	 * evmcs->virtual_apic_page_addr = vmcs12->virtual_apic_page_addr;
 +	 * evmcs->tsc_offset = vmcs12->tsc_offset;
 +	 * evmcs->guest_ia32_debugctl = vmcs12->guest_ia32_debugctl;
 +	 * evmcs->cr0_guest_host_mask = vmcs12->cr0_guest_host_mask;
 +	 * evmcs->cr4_guest_host_mask = vmcs12->cr4_guest_host_mask;
 +	 * evmcs->cr0_read_shadow = vmcs12->cr0_read_shadow;
 +	 * evmcs->cr4_read_shadow = vmcs12->cr4_read_shadow;
 +	 * evmcs->vm_exit_msr_store_count = vmcs12->vm_exit_msr_store_count;
 +	 * evmcs->vm_exit_msr_load_count = vmcs12->vm_exit_msr_load_count;
 +	 * evmcs->vm_entry_msr_load_count = vmcs12->vm_entry_msr_load_count;
 +	 *
 +	 * Not present in struct vmcs12:
 +	 * evmcs->exit_io_instruction_ecx = vmcs12->exit_io_instruction_ecx;
 +	 * evmcs->exit_io_instruction_esi = vmcs12->exit_io_instruction_esi;
 +	 * evmcs->exit_io_instruction_edi = vmcs12->exit_io_instruction_edi;
 +	 * evmcs->exit_io_instruction_eip = vmcs12->exit_io_instruction_eip;
 +	 */
 +
 +	evmcs->guest_es_selector = vmcs12->guest_es_selector;
 +	evmcs->guest_cs_selector = vmcs12->guest_cs_selector;
 +	evmcs->guest_ss_selector = vmcs12->guest_ss_selector;
 +	evmcs->guest_ds_selector = vmcs12->guest_ds_selector;
 +	evmcs->guest_fs_selector = vmcs12->guest_fs_selector;
 +	evmcs->guest_gs_selector = vmcs12->guest_gs_selector;
 +	evmcs->guest_ldtr_selector = vmcs12->guest_ldtr_selector;
 +	evmcs->guest_tr_selector = vmcs12->guest_tr_selector;
 +
 +	evmcs->guest_es_limit = vmcs12->guest_es_limit;
 +	evmcs->guest_cs_limit = vmcs12->guest_cs_limit;
 +	evmcs->guest_ss_limit = vmcs12->guest_ss_limit;
 +	evmcs->guest_ds_limit = vmcs12->guest_ds_limit;
 +	evmcs->guest_fs_limit = vmcs12->guest_fs_limit;
 +	evmcs->guest_gs_limit = vmcs12->guest_gs_limit;
 +	evmcs->guest_ldtr_limit = vmcs12->guest_ldtr_limit;
 +	evmcs->guest_tr_limit = vmcs12->guest_tr_limit;
 +	evmcs->guest_gdtr_limit = vmcs12->guest_gdtr_limit;
 +	evmcs->guest_idtr_limit = vmcs12->guest_idtr_limit;
 +
 +	evmcs->guest_es_ar_bytes = vmcs12->guest_es_ar_bytes;
 +	evmcs->guest_cs_ar_bytes = vmcs12->guest_cs_ar_bytes;
 +	evmcs->guest_ss_ar_bytes = vmcs12->guest_ss_ar_bytes;
 +	evmcs->guest_ds_ar_bytes = vmcs12->guest_ds_ar_bytes;
 +	evmcs->guest_fs_ar_bytes = vmcs12->guest_fs_ar_bytes;
 +	evmcs->guest_gs_ar_bytes = vmcs12->guest_gs_ar_bytes;
 +	evmcs->guest_ldtr_ar_bytes = vmcs12->guest_ldtr_ar_bytes;
 +	evmcs->guest_tr_ar_bytes = vmcs12->guest_tr_ar_bytes;
 +
 +	evmcs->guest_es_base = vmcs12->guest_es_base;
 +	evmcs->guest_cs_base = vmcs12->guest_cs_base;
 +	evmcs->guest_ss_base = vmcs12->guest_ss_base;
 +	evmcs->guest_ds_base = vmcs12->guest_ds_base;
 +	evmcs->guest_fs_base = vmcs12->guest_fs_base;
 +	evmcs->guest_gs_base = vmcs12->guest_gs_base;
 +	evmcs->guest_ldtr_base = vmcs12->guest_ldtr_base;
 +	evmcs->guest_tr_base = vmcs12->guest_tr_base;
 +	evmcs->guest_gdtr_base = vmcs12->guest_gdtr_base;
 +	evmcs->guest_idtr_base = vmcs12->guest_idtr_base;
 +
 +	evmcs->guest_ia32_pat = vmcs12->guest_ia32_pat;
 +	evmcs->guest_ia32_efer = vmcs12->guest_ia32_efer;
 +
 +	evmcs->guest_pdptr0 = vmcs12->guest_pdptr0;
 +	evmcs->guest_pdptr1 = vmcs12->guest_pdptr1;
 +	evmcs->guest_pdptr2 = vmcs12->guest_pdptr2;
 +	evmcs->guest_pdptr3 = vmcs12->guest_pdptr3;
 +
 +	evmcs->guest_pending_dbg_exceptions =
 +		vmcs12->guest_pending_dbg_exceptions;
 +	evmcs->guest_sysenter_esp = vmcs12->guest_sysenter_esp;
 +	evmcs->guest_sysenter_eip = vmcs12->guest_sysenter_eip;
 +
 +	evmcs->guest_activity_state = vmcs12->guest_activity_state;
 +	evmcs->guest_sysenter_cs = vmcs12->guest_sysenter_cs;
 +
 +	evmcs->guest_cr0 = vmcs12->guest_cr0;
 +	evmcs->guest_cr3 = vmcs12->guest_cr3;
 +	evmcs->guest_cr4 = vmcs12->guest_cr4;
 +	evmcs->guest_dr7 = vmcs12->guest_dr7;
 +
 +	evmcs->guest_physical_address = vmcs12->guest_physical_address;
 +
 +	evmcs->vm_instruction_error = vmcs12->vm_instruction_error;
 +	evmcs->vm_exit_reason = vmcs12->vm_exit_reason;
 +	evmcs->vm_exit_intr_info = vmcs12->vm_exit_intr_info;
 +	evmcs->vm_exit_intr_error_code = vmcs12->vm_exit_intr_error_code;
 +	evmcs->idt_vectoring_info_field = vmcs12->idt_vectoring_info_field;
 +	evmcs->idt_vectoring_error_code = vmcs12->idt_vectoring_error_code;
 +	evmcs->vm_exit_instruction_len = vmcs12->vm_exit_instruction_len;
 +	evmcs->vmx_instruction_info = vmcs12->vmx_instruction_info;
 +
 +	evmcs->exit_qualification = vmcs12->exit_qualification;
 +
 +	evmcs->guest_linear_address = vmcs12->guest_linear_address;
 +	evmcs->guest_rsp = vmcs12->guest_rsp;
 +	evmcs->guest_rflags = vmcs12->guest_rflags;
 +
 +	evmcs->guest_interruptibility_info =
 +		vmcs12->guest_interruptibility_info;
 +	evmcs->cpu_based_vm_exec_control = vmcs12->cpu_based_vm_exec_control;
 +	evmcs->vm_entry_controls = vmcs12->vm_entry_controls;
 +	evmcs->vm_entry_intr_info_field = vmcs12->vm_entry_intr_info_field;
 +	evmcs->vm_entry_exception_error_code =
 +		vmcs12->vm_entry_exception_error_code;
 +	evmcs->vm_entry_instruction_len = vmcs12->vm_entry_instruction_len;
 +
 +	evmcs->guest_rip = vmcs12->guest_rip;
 +
 +	evmcs->guest_bndcfgs = vmcs12->guest_bndcfgs;
 +
 +	return 0;
 +}
 +
 +/*
 + * Copy the writable VMCS shadow fields back to the VMCS12, in case
 + * they have been modified by the L1 guest. Note that the "read-only"
 + * VM-exit information fields are actually writable if the vCPU is
 + * configured to support "VMWRITE to any supported field in the VMCS."
 + */
 +static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 +{
 +	const u16 *fields[] = {
 +		shadow_read_write_fields,
 +		shadow_read_only_fields
 +	};
 +	const int max_fields[] = {
 +		max_shadow_read_write_fields,
 +		max_shadow_read_only_fields
 +	};
 +	int i, q;
 +	unsigned long field;
 +	u64 field_value;
 +	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
 +
 +	preempt_disable();
 +
 +	vmcs_load(shadow_vmcs);
 +
 +	for (q = 0; q < ARRAY_SIZE(fields); q++) {
 +		for (i = 0; i < max_fields[q]; i++) {
 +			field = fields[q][i];
 +			field_value = __vmcs_readl(field);
 +			vmcs12_write_any(get_vmcs12(&vmx->vcpu), field, field_value);
 +		}
 +		/*
 +		 * Skip the VM-exit information fields if they are read-only.
 +		 */
 +		if (!nested_cpu_has_vmwrite_any_field(&vmx->vcpu))
 +			break;
 +	}
 +
 +	vmcs_clear(shadow_vmcs);
 +	vmcs_load(vmx->loaded_vmcs->vmcs);
 +
 +	preempt_enable();
 +}
 +
 +static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 +{
 +	const u16 *fields[] = {
 +		shadow_read_write_fields,
 +		shadow_read_only_fields
 +	};
 +	const int max_fields[] = {
 +		max_shadow_read_write_fields,
 +		max_shadow_read_only_fields
 +	};
 +	int i, q;
 +	unsigned long field;
 +	u64 field_value = 0;
 +	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
 +
 +	vmcs_load(shadow_vmcs);
 +
 +	for (q = 0; q < ARRAY_SIZE(fields); q++) {
 +		for (i = 0; i < max_fields[q]; i++) {
 +			field = fields[q][i];
 +			vmcs12_read_any(get_vmcs12(&vmx->vcpu), field, &field_value);
 +			__vmcs_writel(field, field_value);
 +		}
 +	}
 +
 +	vmcs_clear(shadow_vmcs);
 +	vmcs_load(vmx->loaded_vmcs->vmcs);
 +}
 +
 +/*
 + * VMX instructions which assume a current vmcs12 (i.e., that VMPTRLD was
 + * used before) all generate the same failure when it is missing.
 + */
 +static int nested_vmx_check_vmcs12(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	if (vmx->nested.current_vmptr == -1ull) {
 +		nested_vmx_failInvalid(vcpu);
 +		return 0;
 +	}
 +	return 1;
 +}
 +
 +static int handle_vmread(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long field;
 +	u64 field_value;
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	gva_t gva = 0;
 +	struct vmcs12 *vmcs12;
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	if (!nested_vmx_check_vmcs12(vcpu))
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	if (!is_guest_mode(vcpu))
 +		vmcs12 = get_vmcs12(vcpu);
 +	else {
 +		/*
 +		 * When vmcs->vmcs_link_pointer is -1ull, any VMREAD
 +		 * to shadowed-field sets the ALU flags for VMfailInvalid.
 +		 */
 +		if (get_vmcs12(vcpu)->vmcs_link_pointer == -1ull) {
 +			nested_vmx_failInvalid(vcpu);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		vmcs12 = get_shadow_vmcs12(vcpu);
 +	}
 +
 +	/* Decode instruction info and find the field to read */
 +	field = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
 +	/* Read the field, zero-extended to a u64 field_value */
 +	if (vmcs12_read_any(vmcs12, field, &field_value) < 0) {
 +		nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +	/*
 +	 * Now copy part of this value to register or memory, as requested.
 +	 * Note that the number of bits actually copied is 32 or 64 depending
 +	 * on the guest's mode (32 or 64 bit), not on the given field's length.
 +	 */
 +	if (vmx_instruction_info & (1u << 10)) {
 +		kvm_register_writel(vcpu, (((vmx_instruction_info) >> 3) & 0xf),
 +			field_value);
 +	} else {
 +		if (get_vmx_mem_address(vcpu, exit_qualification,
 +				vmx_instruction_info, true, &gva))
 +			return 1;
 +		/* _system ok, nested_vmx_check_permission has verified cpl=0 */
 +		kvm_write_guest_virt_system(vcpu, gva, &field_value,
 +					    (is_long_mode(vcpu) ? 8 : 4), NULL);
 +	}
 +
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +
 +static int handle_vmwrite(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long field;
 +	gva_t gva;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +
 +	/* The value to write might be 32 or 64 bits, depending on L1's long
 +	 * mode, and eventually we need to write that into a field of several
 +	 * possible lengths. The code below first zero-extends the value to 64
 +	 * bit (field_value), and then copies only the appropriate number of
 +	 * bits into the vmcs12 field.
 +	 */
 +	u64 field_value = 0;
 +	struct x86_exception e;
 +	struct vmcs12 *vmcs12;
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	if (!nested_vmx_check_vmcs12(vcpu))
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	if (vmx_instruction_info & (1u << 10))
 +		field_value = kvm_register_readl(vcpu,
 +			(((vmx_instruction_info) >> 3) & 0xf));
 +	else {
 +		if (get_vmx_mem_address(vcpu, exit_qualification,
 +				vmx_instruction_info, false, &gva))
 +			return 1;
 +		if (kvm_read_guest_virt(vcpu, gva, &field_value,
 +					(is_64_bit_mode(vcpu) ? 8 : 4), &e)) {
 +			kvm_inject_page_fault(vcpu, &e);
 +			return 1;
 +		}
 +	}
 +
 +
 +	field = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
 +	/*
 +	 * If the vCPU supports "VMWRITE to any supported field in the
 +	 * VMCS," then the "read-only" fields are actually read/write.
 +	 */
 +	if (vmcs_field_readonly(field) &&
 +	    !nested_cpu_has_vmwrite_any_field(vcpu)) {
 +		nested_vmx_failValid(vcpu,
 +			VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if (!is_guest_mode(vcpu))
 +		vmcs12 = get_vmcs12(vcpu);
 +	else {
 +		/*
 +		 * When vmcs->vmcs_link_pointer is -1ull, any VMWRITE
 +		 * to shadowed-field sets the ALU flags for VMfailInvalid.
 +		 */
 +		if (get_vmcs12(vcpu)->vmcs_link_pointer == -1ull) {
 +			nested_vmx_failInvalid(vcpu);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		vmcs12 = get_shadow_vmcs12(vcpu);
 +
 +	}
 +
 +	if (vmcs12_write_any(vmcs12, field, field_value) < 0) {
 +		nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	/*
 +	 * Do not track vmcs12 dirty-state if in guest-mode
 +	 * as we actually dirty shadow vmcs12 instead of vmcs12.
 +	 */
 +	if (!is_guest_mode(vcpu)) {
 +		switch (field) {
 +#define SHADOW_FIELD_RW(x) case x:
 +#include "vmcs_shadow_fields.h"
 +			/*
 +			 * The fields that can be updated by L1 without a vmexit are
 +			 * always updated in the vmcs02, the others go down the slow
 +			 * path of prepare_vmcs02.
 +			 */
 +			break;
 +		default:
 +			vmx->nested.dirty_vmcs12 = true;
 +			break;
 +		}
 +	}
 +
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 +{
 +	vmx->nested.current_vmptr = vmptr;
 +	if (enable_shadow_vmcs) {
 +		vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 +			      SECONDARY_EXEC_SHADOW_VMCS);
 +		vmcs_write64(VMCS_LINK_POINTER,
 +			     __pa(vmx->vmcs01.shadow_vmcs));
 +		vmx->nested.need_vmcs12_sync = true;
 +	}
 +	vmx->nested.dirty_vmcs12 = true;
 +}
 +
 +/* Emulate the VMPTRLD instruction */
 +static int handle_vmptrld(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	gpa_t vmptr;
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	if (nested_vmx_get_vmptr(vcpu, &vmptr))
 +		return 1;
 +
 +	if (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if (vmptr == vmx->nested.vmxon_ptr) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	if (vmx->nested.current_vmptr != vmptr) {
 +		struct vmcs12 *new_vmcs12;
 +		struct page *page;
 +		page = kvm_vcpu_gpa_to_page(vcpu, vmptr);
 +		if (is_error_page(page)) {
 +			/*
 +			 * Reads from an unbacked page return all 1s,
 +			 * which means that the 32 bits located at the
 +			 * given physical address won't match the required
 +			 * VMCS12_REVISION identifier.
 +			 */
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		new_vmcs12 = kmap(page);
 +		if (new_vmcs12->hdr.revision_id != VMCS12_REVISION ||
 +		    (new_vmcs12->hdr.shadow_vmcs &&
 +		     !nested_cpu_has_vmx_shadow_vmcs(vcpu))) {
 +			kunmap(page);
 +			kvm_release_page_clean(page);
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +
 +		nested_release_vmcs12(vmx);
 +		/*
 +		 * Load VMCS12 from guest memory since it is not already
 +		 * cached.
 +		 */
 +		memcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);
 +		kunmap(page);
 +		kvm_release_page_clean(page);
 +
 +		set_current_vmptr(vmx, vmptr);
 +	}
 +
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +/* Emulate the VMPTRST instruction */
 +static int handle_vmptrst(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qual = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 instr_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	gpa_t current_vmptr = to_vmx(vcpu)->nested.current_vmptr;
 +	struct x86_exception e;
 +	gva_t gva;
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	if (get_vmx_mem_address(vcpu, exit_qual, instr_info, true, &gva))
 +		return 1;
 +	/* *_system ok, nested_vmx_check_permission has verified cpl=0 */
 +	if (kvm_write_guest_virt_system(vcpu, gva, (void *)&current_vmptr,
 +					sizeof(gpa_t), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +	nested_vmx_succeed(vcpu);
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +/* Emulate the INVEPT instruction */
 +static int handle_invept(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 vmx_instruction_info, types;
 +	unsigned long type;
 +	gva_t gva;
 +	struct x86_exception e;
 +	struct {
 +		u64 eptp, gpa;
 +	} operand;
 +
 +	if (!(vmx->nested.msrs.secondary_ctls_high &
 +	      SECONDARY_EXEC_ENABLE_EPT) ||
 +	    !(vmx->nested.msrs.ept_caps & VMX_EPT_INVEPT_BIT)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 +
 +	types = (vmx->nested.msrs.ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;
 +
 +	if (type >= 32 || !(types & (1 << type))) {
 +		nested_vmx_failValid(vcpu,
 +				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	/* According to the Intel VMX instruction reference, the memory
 +	 * operand is read even if it isn't needed (e.g., for type==global)
 +	 */
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +			vmx_instruction_info, false, &gva))
 +		return 1;
 +	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +
 +	switch (type) {
 +	case VMX_EPT_EXTENT_GLOBAL:
 +	/*
 +	 * TODO: track mappings and invalidate
 +	 * single context requests appropriately
 +	 */
 +	case VMX_EPT_EXTENT_CONTEXT:
 +		kvm_mmu_sync_roots(vcpu);
 +		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +		nested_vmx_succeed(vcpu);
 +		break;
 +	default:
 +		BUG_ON(1);
 +		break;
 +	}
 +
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static u16 nested_get_vpid02(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	return vmx->nested.vpid02 ? vmx->nested.vpid02 : vmx->vpid;
 +}
 +
 +static int handle_invvpid(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 vmx_instruction_info;
 +	unsigned long type, types;
 +	gva_t gva;
 +	struct x86_exception e;
 +	struct {
 +		u64 vpid;
 +		u64 gla;
 +	} operand;
 +	u16 vpid02;
 +
 +	if (!(vmx->nested.msrs.secondary_ctls_high &
 +	      SECONDARY_EXEC_ENABLE_VPID) ||
 +			!(vmx->nested.msrs.vpid_caps & VMX_VPID_INVVPID_BIT)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
 +
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 +
 +	types = (vmx->nested.msrs.vpid_caps &
 +			VMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;
 +
 +	if (type >= 32 || !(types & (1 << type))) {
 +		nested_vmx_failValid(vcpu,
 +			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	/* according to the intel vmx instruction reference, the memory
 +	 * operand is read even if it isn't needed (e.g., for type==global)
 +	 */
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +			vmx_instruction_info, false, &gva))
 +		return 1;
 +	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +	if (operand.vpid >> 16) {
 +		nested_vmx_failValid(vcpu,
 +			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	vpid02 = nested_get_vpid02(vcpu);
 +	switch (type) {
 +	case VMX_VPID_EXTENT_INDIVIDUAL_ADDR:
 +		if (!operand.vpid ||
 +		    is_noncanonical_address(operand.gla, vcpu)) {
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		if (cpu_has_vmx_invvpid_individual_addr()) {
 +			__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR,
 +				vpid02, operand.gla);
 +		} else
 +			__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	case VMX_VPID_EXTENT_SINGLE_CONTEXT:
 +	case VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:
 +		if (!operand.vpid) {
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	case VMX_VPID_EXTENT_ALL_CONTEXT:
 +		__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	default:
 +		WARN_ON_ONCE(1);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	nested_vmx_succeed(vcpu);
 +
 +	return kvm_skip_emulated_instruction(vcpu);
 +}
 +
 +static int handle_invpcid(struct kvm_vcpu *vcpu)
 +{
 +	u32 vmx_instruction_info;
 +	unsigned long type;
 +	bool pcid_enabled;
 +	gva_t gva;
 +	struct x86_exception e;
 +	unsigned i;
 +	unsigned long roots_to_free = 0;
 +	struct {
 +		u64 pcid;
 +		u64 gla;
 +	} operand;
 +
 +	if (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 +
 +	if (type > 3) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	/* According to the Intel instruction reference, the memory operand
 +	 * is read even if it isn't needed (e.g., for type==all)
 +	 */
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +				vmx_instruction_info, false, &gva))
 +		return 1;
 +
 +	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +
 +	if (operand.pcid >> 12 != 0) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
 +
 +	switch (type) {
 +	case INVPCID_TYPE_INDIV_ADDR:
 +		if ((!pcid_enabled && (operand.pcid != 0)) ||
 +		    is_noncanonical_address(operand.gla, vcpu)) {
 +			kvm_inject_gp(vcpu, 0);
 +			return 1;
 +		}
 +		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	case INVPCID_TYPE_SINGLE_CTXT:
 +		if (!pcid_enabled && (operand.pcid != 0)) {
 +			kvm_inject_gp(vcpu, 0);
 +			return 1;
 +		}
 +
 +		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
 +			kvm_mmu_sync_roots(vcpu);
 +			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +		}
 +
 +		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 +			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].cr3)
 +			    == operand.pcid)
 +				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
 +
 +		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);
 +		/*
 +		 * If neither the current cr3 nor any of the prev_roots use the
 +		 * given PCID, then nothing needs to be done here because a
 +		 * resync will happen anyway before switching to any other CR3.
 +		 */
 +
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	case INVPCID_TYPE_ALL_NON_GLOBAL:
 +		/*
 +		 * Currently, KVM doesn't mark global entries in the shadow
 +		 * page tables, so a non-global flush just degenerates to a
 +		 * global flush. If needed, we could optimize this later by
 +		 * keeping track of global entries in shadow page tables.
 +		 */
 +
 +		/* fall-through */
 +	case INVPCID_TYPE_ALL_INCL_GLOBAL:
 +		kvm_mmu_unload(vcpu);
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	default:
 +		BUG(); /* We have already checked above that type <= 3 */
 +	}
 +}
 +
 +static int handle_pml_full(struct kvm_vcpu *vcpu)
 +{
 +	unsigned long exit_qualification;
 +
 +	trace_kvm_pml_full(vcpu->vcpu_id);
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +
 +	/*
 +	 * PML buffer FULL happened while executing iret from NMI,
 +	 * "blocked by NMI" bit has to be set before next VM entry.
 +	 */
 +	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			enable_vnmi &&
 +			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 +		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				GUEST_INTR_STATE_NMI);
 +
 +	/*
 +	 * PML buffer already flushed at beginning of VMEXIT. Nothing to do
 +	 * here.., and there's no userspace involvement needed for PML.
 +	 */
 +	return 1;
 +}
 +
 +static int handle_preemption_timer(struct kvm_vcpu *vcpu)
 +{
 +	if (!to_vmx(vcpu)->req_immediate_exit)
 +		kvm_lapic_expired_hv_timer(vcpu);
 +	return 1;
 +}
 +
 +static bool valid_ept_address(struct kvm_vcpu *vcpu, u64 address)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int maxphyaddr = cpuid_maxphyaddr(vcpu);
 +
 +	/* Check for memory type validity */
 +	switch (address & VMX_EPTP_MT_MASK) {
 +	case VMX_EPTP_MT_UC:
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT))
 +			return false;
 +		break;
 +	case VMX_EPTP_MT_WB:
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT))
 +			return false;
 +		break;
 +	default:
 +		return false;
 +	}
 +
 +	/* only 4 levels page-walk length are valid */
 +	if ((address & VMX_EPTP_PWL_MASK) != VMX_EPTP_PWL_4)
 +		return false;
 +
 +	/* Reserved bits should not be set */
 +	if (address >> maxphyaddr || ((address >> 7) & 0x1f))
 +		return false;
 +
 +	/* AD, if set, should be supported */
 +	if (address & VMX_EPTP_AD_ENABLE_BIT) {
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT))
 +			return false;
 +	}
 +
 +	return true;
 +}
 +
 +static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 +				     struct vmcs12 *vmcs12)
 +{
 +	u32 index = vcpu->arch.regs[VCPU_REGS_RCX];
 +	u64 address;
 +	bool accessed_dirty;
 +	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
 +
 +	if (!nested_cpu_has_eptp_switching(vmcs12) ||
 +	    !nested_cpu_has_ept(vmcs12))
 +		return 1;
 +
 +	if (index >= VMFUNC_EPTP_ENTRIES)
 +		return 1;
 +
 +
 +	if (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,
 +				     &address, index * 8, 8))
 +		return 1;
 +
 +	accessed_dirty = !!(address & VMX_EPTP_AD_ENABLE_BIT);
 +
 +	/*
 +	 * If the (L2) guest does a vmfunc to the currently
 +	 * active ept pointer, we don't have to do anything else
 +	 */
 +	if (vmcs12->ept_pointer != address) {
 +		if (!valid_ept_address(vcpu, address))
 +			return 1;
 +
 +		kvm_mmu_unload(vcpu);
 +		mmu->ept_ad = accessed_dirty;
 +		mmu->mmu_role.base.ad_disabled = !accessed_dirty;
 +		vmcs12->ept_pointer = address;
 +		/*
 +		 * TODO: Check what's the correct approach in case
 +		 * mmu reload fails. Currently, we just let the next
 +		 * reload potentially fail
 +		 */
 +		kvm_mmu_reload(vcpu);
 +	}
 +
 +	return 0;
 +}
 +
 +static int handle_vmfunc(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct vmcs12 *vmcs12;
 +	u32 function = vcpu->arch.regs[VCPU_REGS_RAX];
 +
 +	/*
 +	 * VMFUNC is only supported for nested guests, but we always enable the
 +	 * secondary control for simplicity; for non-nested mode, fake that we
 +	 * didn't by injecting #UD.
 +	 */
 +	if (!is_guest_mode(vcpu)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	vmcs12 = get_vmcs12(vcpu);
 +	if ((vmcs12->vm_function_control & (1 << function)) == 0)
 +		goto fail;
 +
 +	switch (function) {
 +	case 0:
 +		if (nested_vmx_eptp_switching(vcpu, vmcs12))
 +			goto fail;
 +		break;
 +	default:
 +		goto fail;
 +	}
 +	return kvm_skip_emulated_instruction(vcpu);
 +
 +fail:
 +	nested_vmx_vmexit(vcpu, vmx->exit_reason,
 +			  vmcs_read32(VM_EXIT_INTR_INFO),
 +			  vmcs_readl(EXIT_QUALIFICATION));
 +	return 1;
 +}
 +
 +static int handle_encls(struct kvm_vcpu *vcpu)
 +{
 +	/*
 +	 * SGX virtualization is not yet supported.  There is no software
 +	 * enable bit for SGX, so we have to trap ENCLS and inject a #UD
 +	 * to prevent the guest from executing ENCLS.
 +	 */
 +	kvm_queue_exception(vcpu, UD_VECTOR);
 +	return 1;
 +}
 +
 +/*
 + * The exit handlers return 1 if the exit was handled fully and guest execution
 + * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
 + * to be done to userspace and return 0.
 + */
 +static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 +	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,
 +	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
 +	[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,
 +	[EXIT_REASON_NMI_WINDOW]	      = handle_nmi_window,
 +	[EXIT_REASON_IO_INSTRUCTION]          = handle_io,
 +	[EXIT_REASON_CR_ACCESS]               = handle_cr,
 +	[EXIT_REASON_DR_ACCESS]               = handle_dr,
 +	[EXIT_REASON_CPUID]                   = handle_cpuid,
 +	[EXIT_REASON_MSR_READ]                = handle_rdmsr,
 +	[EXIT_REASON_MSR_WRITE]               = handle_wrmsr,
 +	[EXIT_REASON_PENDING_INTERRUPT]       = handle_interrupt_window,
 +	[EXIT_REASON_HLT]                     = handle_halt,
 +	[EXIT_REASON_INVD]		      = handle_invd,
 +	[EXIT_REASON_INVLPG]		      = handle_invlpg,
 +	[EXIT_REASON_RDPMC]                   = handle_rdpmc,
 +	[EXIT_REASON_VMCALL]                  = handle_vmcall,
 +	[EXIT_REASON_VMCLEAR]	              = handle_vmclear,
 +	[EXIT_REASON_VMLAUNCH]                = handle_vmlaunch,
 +	[EXIT_REASON_VMPTRLD]                 = handle_vmptrld,
 +	[EXIT_REASON_VMPTRST]                 = handle_vmptrst,
 +	[EXIT_REASON_VMREAD]                  = handle_vmread,
 +	[EXIT_REASON_VMRESUME]                = handle_vmresume,
 +	[EXIT_REASON_VMWRITE]                 = handle_vmwrite,
 +	[EXIT_REASON_VMOFF]                   = handle_vmoff,
 +	[EXIT_REASON_VMON]                    = handle_vmon,
 +	[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,
 +	[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,
 +	[EXIT_REASON_APIC_WRITE]              = handle_apic_write,
 +	[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,
 +	[EXIT_REASON_WBINVD]                  = handle_wbinvd,
 +	[EXIT_REASON_XSETBV]                  = handle_xsetbv,
 +	[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,
 +	[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,
 +	[EXIT_REASON_GDTR_IDTR]		      = handle_desc,
 +	[EXIT_REASON_LDTR_TR]		      = handle_desc,
 +	[EXIT_REASON_EPT_VIOLATION]	      = handle_ept_violation,
 +	[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,
 +	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
 +	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
 +	[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,
 +	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
 +	[EXIT_REASON_INVEPT]                  = handle_invept,
 +	[EXIT_REASON_INVVPID]                 = handle_invvpid,
 +	[EXIT_REASON_RDRAND]                  = handle_invalid_op,
 +	[EXIT_REASON_RDSEED]                  = handle_invalid_op,
 +	[EXIT_REASON_XSAVES]                  = handle_xsaves,
 +	[EXIT_REASON_XRSTORS]                 = handle_xrstors,
 +	[EXIT_REASON_PML_FULL]		      = handle_pml_full,
 +	[EXIT_REASON_INVPCID]                 = handle_invpcid,
 +	[EXIT_REASON_VMFUNC]                  = handle_vmfunc,
 +	[EXIT_REASON_PREEMPTION_TIMER]	      = handle_preemption_timer,
 +	[EXIT_REASON_ENCLS]		      = handle_encls,
 +};
 +
 +static const int kvm_vmx_max_exit_handlers =
 +	ARRAY_SIZE(kvm_vmx_exit_handlers);
 +
 +static bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,
 +				       struct vmcs12 *vmcs12)
 +{
 +	unsigned long exit_qualification;
 +	gpa_t bitmap, last_bitmap;
 +	unsigned int port;
 +	int size;
 +	u8 b;
 +
 +	if (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))
 +		return nested_cpu_has(vmcs12, CPU_BASED_UNCOND_IO_EXITING);
 +
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +
 +	port = exit_qualification >> 16;
 +	size = (exit_qualification & 7) + 1;
 +
 +	last_bitmap = (gpa_t)-1;
 +	b = -1;
 +
 +	while (size > 0) {
 +		if (port < 0x8000)
 +			bitmap = vmcs12->io_bitmap_a;
 +		else if (port < 0x10000)
 +			bitmap = vmcs12->io_bitmap_b;
 +		else
 +			return true;
 +		bitmap += (port & 0x7fff) / 8;
 +
 +		if (last_bitmap != bitmap)
 +			if (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))
 +				return true;
 +		if (b & (1 << (port & 7)))
 +			return true;
 +
 +		port++;
 +		size--;
 +		last_bitmap = bitmap;
 +	}
 +
 +	return false;
 +}
 +
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle an MSR access access,
 + * rather than handle it ourselves in L0. I.e., check whether L1 expressed
 + * disinterest in the current event (read or write a specific MSR) by using an
 + * MSR bitmap. This may be the case even when L0 doesn't use MSR bitmaps.
 + */
 +static bool nested_vmx_exit_handled_msr(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12, u32 exit_reason)
 +{
 +	u32 msr_index = vcpu->arch.regs[VCPU_REGS_RCX];
 +	gpa_t bitmap;
 +
 +	if (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))
 +		return true;
 +
 +	/*
 +	 * The MSR_BITMAP page is divided into four 1024-byte bitmaps,
 +	 * for the four combinations of read/write and low/high MSR numbers.
 +	 * First we need to figure out which of the four to use:
 +	 */
 +	bitmap = vmcs12->msr_bitmap;
 +	if (exit_reason == EXIT_REASON_MSR_WRITE)
 +		bitmap += 2048;
 +	if (msr_index >= 0xc0000000) {
 +		msr_index -= 0xc0000000;
 +		bitmap += 1024;
 +	}
 +
 +	/* Then read the msr_index'th bit from this bitmap: */
 +	if (msr_index < 1024*8) {
 +		unsigned char b;
 +		if (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))
 +			return true;
 +		return 1 & (b >> (msr_index & 7));
 +	} else
 +		return true; /* let L1 handle the wrong parameter */
 +}
 +
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle a CR access exit,
 + * rather than handle it ourselves in L0. I.e., check if L1 wanted to
 + * intercept (via guest_host_mask etc.) the current event.
 + */
 +static bool nested_vmx_exit_handled_cr(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	int cr = exit_qualification & 15;
 +	int reg;
 +	unsigned long val;
 +
 +	switch ((exit_qualification >> 4) & 3) {
 +	case 0: /* mov to cr */
 +		reg = (exit_qualification >> 8) & 15;
 +		val = kvm_register_readl(vcpu, reg);
 +		switch (cr) {
 +		case 0:
 +			if (vmcs12->cr0_guest_host_mask &
 +			    (val ^ vmcs12->cr0_read_shadow))
 +				return true;
 +			break;
 +		case 3:
 +			if ((vmcs12->cr3_target_count >= 1 &&
 +					vmcs12->cr3_target_value0 == val) ||
 +				(vmcs12->cr3_target_count >= 2 &&
 +					vmcs12->cr3_target_value1 == val) ||
 +				(vmcs12->cr3_target_count >= 3 &&
 +					vmcs12->cr3_target_value2 == val) ||
 +				(vmcs12->cr3_target_count >= 4 &&
 +					vmcs12->cr3_target_value3 == val))
 +				return false;
 +			if (nested_cpu_has(vmcs12, CPU_BASED_CR3_LOAD_EXITING))
 +				return true;
 +			break;
 +		case 4:
 +			if (vmcs12->cr4_guest_host_mask &
 +			    (vmcs12->cr4_read_shadow ^ val))
 +				return true;
 +			break;
 +		case 8:
 +			if (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING))
 +				return true;
 +			break;
 +		}
 +		break;
 +	case 2: /* clts */
 +		if ((vmcs12->cr0_guest_host_mask & X86_CR0_TS) &&
 +		    (vmcs12->cr0_read_shadow & X86_CR0_TS))
 +			return true;
 +		break;
 +	case 1: /* mov from cr */
 +		switch (cr) {
 +		case 3:
 +			if (vmcs12->cpu_based_vm_exec_control &
 +			    CPU_BASED_CR3_STORE_EXITING)
 +				return true;
 +			break;
 +		case 8:
 +			if (vmcs12->cpu_based_vm_exec_control &
 +			    CPU_BASED_CR8_STORE_EXITING)
 +				return true;
 +			break;
 +		}
 +		break;
 +	case 3: /* lmsw */
 +		/*
 +		 * lmsw can change bits 1..3 of cr0, and only set bit 0 of
 +		 * cr0. Other attempted changes are ignored, with no exit.
 +		 */
 +		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 +		if (vmcs12->cr0_guest_host_mask & 0xe &
 +		    (val ^ vmcs12->cr0_read_shadow))
 +			return true;
 +		if ((vmcs12->cr0_guest_host_mask & 0x1) &&
 +		    !(vmcs12->cr0_read_shadow & 0x1) &&
 +		    (val & 0x1))
 +			return true;
 +		break;
 +	}
 +	return false;
 +}
 +
 +static bool nested_vmx_exit_handled_vmcs_access(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12, gpa_t bitmap)
 +{
 +	u32 vmx_instruction_info;
 +	unsigned long field;
 +	u8 b;
 +
 +	if (!nested_cpu_has_shadow_vmcs(vmcs12))
 +		return true;
 +
 +	/* Decode instruction info and find the field to access */
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	field = kvm_register_read(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
 +
 +	/* Out-of-range fields always cause a VM exit from L2 to L1 */
 +	if (field >> 15)
 +		return true;
 +
 +	if (kvm_vcpu_read_guest(vcpu, bitmap + field/8, &b, 1))
 +		return true;
 +
 +	return 1 & (b >> (field & 7));
 +}
 +
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle an exit, or 0 if we
 + * should handle it ourselves in L0 (and then continue L2). Only call this
 + * when in is_guest_mode (L2).
 + */
 +static bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)
 +{
 +	u32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +
 +	if (vmx->nested.nested_run_pending)
 +		return false;
 +
 +	if (unlikely(vmx->fail)) {
 +		pr_info_ratelimited("%s failed vm entry %x\n", __func__,
 +				    vmcs_read32(VM_INSTRUCTION_ERROR));
 +		return true;
 +	}
 +
 +	/*
 +	 * The host physical addresses of some pages of guest memory
 +	 * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC
 +	 * Page). The CPU may write to these pages via their host
 +	 * physical address while L2 is running, bypassing any
 +	 * address-translation-based dirty tracking (e.g. EPT write
 +	 * protection).
 +	 *
 +	 * Mark them dirty on every exit from L2 to prevent them from
 +	 * getting out of sync with dirty tracking.
 +	 */
 +	nested_mark_vmcs12_pages_dirty(vcpu);
 +
 +	trace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,
 +				vmcs_readl(EXIT_QUALIFICATION),
 +				vmx->idt_vectoring_info,
 +				intr_info,
 +				vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 +				KVM_ISA_VMX);
 +
 +	switch (exit_reason) {
 +	case EXIT_REASON_EXCEPTION_NMI:
 +		if (is_nmi(intr_info))
 +			return false;
 +		else if (is_page_fault(intr_info))
 +			return !vmx->vcpu.arch.apf.host_apf_reason && enable_ept;
 +		else if (is_debug(intr_info) &&
 +			 vcpu->guest_debug &
 +			 (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))
 +			return false;
 +		else if (is_breakpoint(intr_info) &&
 +			 vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 +			return false;
 +		return vmcs12->exception_bitmap &
 +				(1u << (intr_info & INTR_INFO_VECTOR_MASK));
 +	case EXIT_REASON_EXTERNAL_INTERRUPT:
 +		return false;
 +	case EXIT_REASON_TRIPLE_FAULT:
 +		return true;
 +	case EXIT_REASON_PENDING_INTERRUPT:
 +		return nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);
 +	case EXIT_REASON_NMI_WINDOW:
 +		return nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);
 +	case EXIT_REASON_TASK_SWITCH:
 +		return true;
 +	case EXIT_REASON_CPUID:
 +		return true;
 +	case EXIT_REASON_HLT:
 +		return nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);
 +	case EXIT_REASON_INVD:
 +		return true;
 +	case EXIT_REASON_INVLPG:
 +		return nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);
 +	case EXIT_REASON_RDPMC:
 +		return nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);
 +	case EXIT_REASON_RDRAND:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDRAND_EXITING);
 +	case EXIT_REASON_RDSEED:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDSEED_EXITING);
 +	case EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:
 +		return nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);
 +	case EXIT_REASON_VMREAD:
 +		return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,
 +			vmcs12->vmread_bitmap);
 +	case EXIT_REASON_VMWRITE:
 +		return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,
 +			vmcs12->vmwrite_bitmap);
 +	case EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:
 +	case EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:
 +	case EXIT_REASON_VMPTRST: case EXIT_REASON_VMRESUME:
 +	case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
 +	case EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:
 +		/*
 +		 * VMX instructions trap unconditionally. This allows L1 to
 +		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
 +		 */
 +		return true;
 +	case EXIT_REASON_CR_ACCESS:
 +		return nested_vmx_exit_handled_cr(vcpu, vmcs12);
 +	case EXIT_REASON_DR_ACCESS:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);
 +	case EXIT_REASON_IO_INSTRUCTION:
 +		return nested_vmx_exit_handled_io(vcpu, vmcs12);
 +	case EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);
 +	case EXIT_REASON_MSR_READ:
 +	case EXIT_REASON_MSR_WRITE:
 +		return nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);
 +	case EXIT_REASON_INVALID_STATE:
 +		return true;
 +	case EXIT_REASON_MWAIT_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);
 +	case EXIT_REASON_MONITOR_TRAP_FLAG:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);
 +	case EXIT_REASON_MONITOR_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);
 +	case EXIT_REASON_PAUSE_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||
 +			nested_cpu_has2(vmcs12,
 +				SECONDARY_EXEC_PAUSE_LOOP_EXITING);
 +	case EXIT_REASON_MCE_DURING_VMENTRY:
 +		return false;
 +	case EXIT_REASON_TPR_BELOW_THRESHOLD:
 +		return nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);
 +	case EXIT_REASON_APIC_ACCESS:
 +	case EXIT_REASON_APIC_WRITE:
 +	case EXIT_REASON_EOI_INDUCED:
 +		/*
 +		 * The controls for "virtualize APIC accesses," "APIC-
 +		 * register virtualization," and "virtual-interrupt
 +		 * delivery" only come from vmcs12.
 +		 */
 +		return true;
 +	case EXIT_REASON_EPT_VIOLATION:
 +		/*
 +		 * L0 always deals with the EPT violation. If nested EPT is
 +		 * used, and the nested mmu code discovers that the address is
 +		 * missing in the guest EPT table (EPT12), the EPT violation
 +		 * will be injected with nested_ept_inject_page_fault()
 +		 */
 +		return false;
 +	case EXIT_REASON_EPT_MISCONFIG:
 +		/*
 +		 * L2 never uses directly L1's EPT, but rather L0's own EPT
 +		 * table (shadow on EPT) or a merged EPT table that L0 built
 +		 * (EPT on EPT). So any problems with the structure of the
 +		 * table is L0's fault.
 +		 */
 +		return false;
 +	case EXIT_REASON_INVPCID:
 +		return
 +			nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_INVPCID) &&
 +			nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);
 +	case EXIT_REASON_WBINVD:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);
 +	case EXIT_REASON_XSETBV:
 +		return true;
 +	case EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:
 +		/*
 +		 * This should never happen, since it is not possible to
 +		 * set XSS to a non-zero value---neither in L1 nor in L2.
 +		 * If if it were, XSS would have to be checked against
 +		 * the XSS exit bitmap in vmcs12.
 +		 */
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);
 +	case EXIT_REASON_PREEMPTION_TIMER:
 +		return false;
 +	case EXIT_REASON_PML_FULL:
 +		/* We emulate PML support to L1. */
 +		return false;
 +	case EXIT_REASON_VMFUNC:
 +		/* VM functions are emulated through L2->L0 vmexits. */
 +		return false;
 +	case EXIT_REASON_ENCLS:
 +		/* SGX is never exposed to L1 */
 +		return false;
 +	default:
 +		return true;
 +	}
 +}
 +
 +static int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)
 +{
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +
 +	/*
 +	 * At this point, the exit interruption info in exit_intr_info
 +	 * is only valid for EXCEPTION_NMI exits.  For EXTERNAL_INTERRUPT
 +	 * we need to query the in-kernel LAPIC.
 +	 */
 +	WARN_ON(exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT);
 +	if ((exit_intr_info &
 +	     (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) ==
 +	    (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		vmcs12->vm_exit_intr_error_code =
 +			vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
 +	}
 +
 +	nested_vmx_vmexit(vcpu, exit_reason, exit_intr_info,
 +			  vmcs_readl(EXIT_QUALIFICATION));
 +	return 1;
 +}
 +
 +static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +{
 +	*info1 = vmcs_readl(EXIT_QUALIFICATION);
 +	*info2 = vmcs_read32(VM_EXIT_INTR_INFO);
 +}
 +
 +static void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)
 +{
 +	if (vmx->pml_pg) {
 +		__free_page(vmx->pml_pg);
 +		vmx->pml_pg = NULL;
 +	}
 +}
 +
 +static void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 *pml_buf;
 +	u16 pml_idx;
 +
 +	pml_idx = vmcs_read16(GUEST_PML_INDEX);
 +
 +	/* Do nothing if PML buffer is empty */
 +	if (pml_idx == (PML_ENTITY_NUM - 1))
 +		return;
 +
 +	/* PML index always points to next available PML buffer entity */
 +	if (pml_idx >= PML_ENTITY_NUM)
 +		pml_idx = 0;
 +	else
 +		pml_idx++;
 +
 +	pml_buf = page_address(vmx->pml_pg);
 +	for (; pml_idx < PML_ENTITY_NUM; pml_idx++) {
 +		u64 gpa;
 +
 +		gpa = pml_buf[pml_idx];
 +		WARN_ON(gpa & (PAGE_SIZE - 1));
 +		kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
 +	}
 +
 +	/* reset PML index */
 +	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
 +}
 +
 +/*
 + * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.
 + * Called before reporting dirty_bitmap to userspace.
 + */
 +static void kvm_flush_pml_buffers(struct kvm *kvm)
 +{
 +	int i;
 +	struct kvm_vcpu *vcpu;
 +	/*
 +	 * We only need to kick vcpu out of guest mode here, as PML buffer
 +	 * is flushed at beginning of all VMEXITs, and it's obvious that only
 +	 * vcpus running in guest are possible to have unflushed GPAs in PML
 +	 * buffer.
 +	 */
 +	kvm_for_each_vcpu(i, vcpu, kvm)
 +		kvm_vcpu_kick(vcpu);
 +}
 +
 +static void vmx_dump_sel(char *name, uint32_t sel)
 +{
 +	pr_err("%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
 +	       name, vmcs_read16(sel),
 +	       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),
 +	       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),
 +	       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));
 +}
 +
 +static void vmx_dump_dtsel(char *name, uint32_t limit)
 +{
 +	pr_err("%s                           limit=0x%08x, base=0x%016lx\n",
 +	       name, vmcs_read32(limit),
 +	       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
 +}
 +
 +static void dump_vmcs(void)
 +{
 +	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
 +	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
 +	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 +	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
 +	u32 secondary_exec_control = 0;
 +	unsigned long cr4 = vmcs_readl(GUEST_CR4);
 +	u64 efer = vmcs_read64(GUEST_IA32_EFER);
 +	int i, n;
 +
 +	if (cpu_has_secondary_exec_ctrls())
 +		secondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
 +
 +	pr_err("*** Guest State ***\n");
 +	pr_err("CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
 +	       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),
 +	       vmcs_readl(CR0_GUEST_HOST_MASK));
 +	pr_err("CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
 +	       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));
 +	pr_err("CR3 = 0x%016lx\n", vmcs_readl(GUEST_CR3));
 +	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&
 +	    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))
 +	{
 +		pr_err("PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\n",
 +		       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));
 +		pr_err("PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\n",
 +		       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));
 +	}
 +	pr_err("RSP = 0x%016lx  RIP = 0x%016lx\n",
 +	       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
 +	pr_err("RFLAGS=0x%08lx         DR7 = 0x%016lx\n",
 +	       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));
 +	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
 +	       vmcs_readl(GUEST_SYSENTER_ESP),
 +	       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));
 +	vmx_dump_sel("CS:  ", GUEST_CS_SELECTOR);
 +	vmx_dump_sel("DS:  ", GUEST_DS_SELECTOR);
 +	vmx_dump_sel("SS:  ", GUEST_SS_SELECTOR);
 +	vmx_dump_sel("ES:  ", GUEST_ES_SELECTOR);
 +	vmx_dump_sel("FS:  ", GUEST_FS_SELECTOR);
 +	vmx_dump_sel("GS:  ", GUEST_GS_SELECTOR);
 +	vmx_dump_dtsel("GDTR:", GUEST_GDTR_LIMIT);
 +	vmx_dump_sel("LDTR:", GUEST_LDTR_SELECTOR);
 +	vmx_dump_dtsel("IDTR:", GUEST_IDTR_LIMIT);
 +	vmx_dump_sel("TR:  ", GUEST_TR_SELECTOR);
 +	if ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||
 +	    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))
 +		pr_err("EFER =     0x%016llx  PAT = 0x%016llx\n",
 +		       efer, vmcs_read64(GUEST_IA32_PAT));
 +	pr_err("DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\n",
 +	       vmcs_read64(GUEST_IA32_DEBUGCTL),
 +	       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
 +	if (cpu_has_load_perf_global_ctrl() &&
 +	    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)
 +		pr_err("PerfGlobCtl = 0x%016llx\n",
 +		       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));
 +	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)
 +		pr_err("BndCfgS = 0x%016llx\n", vmcs_read64(GUEST_BNDCFGS));
 +	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
 +	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
 +	       vmcs_read32(GUEST_ACTIVITY_STATE));
 +	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 +		pr_err("InterruptStatus = %04x\n",
 +		       vmcs_read16(GUEST_INTR_STATUS));
 +
 +	pr_err("*** Host State ***\n");
 +	pr_err("RIP = 0x%016lx  RSP = 0x%016lx\n",
 +	       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));
 +	pr_err("CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\n",
 +	       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),
 +	       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),
 +	       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),
 +	       vmcs_read16(HOST_TR_SELECTOR));
 +	pr_err("FSBase=%016lx GSBase=%016lx TRBase=%016lx\n",
 +	       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),
 +	       vmcs_readl(HOST_TR_BASE));
 +	pr_err("GDTBase=%016lx IDTBase=%016lx\n",
 +	       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));
 +	pr_err("CR0=%016lx CR3=%016lx CR4=%016lx\n",
 +	       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),
 +	       vmcs_readl(HOST_CR4));
 +	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
 +	       vmcs_readl(HOST_IA32_SYSENTER_ESP),
 +	       vmcs_read32(HOST_IA32_SYSENTER_CS),
 +	       vmcs_readl(HOST_IA32_SYSENTER_EIP));
 +	if (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))
 +		pr_err("EFER = 0x%016llx  PAT = 0x%016llx\n",
 +		       vmcs_read64(HOST_IA32_EFER),
 +		       vmcs_read64(HOST_IA32_PAT));
 +	if (cpu_has_load_perf_global_ctrl() &&
 +	    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)
 +		pr_err("PerfGlobCtl = 0x%016llx\n",
 +		       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));
 +
 +	pr_err("*** Control State ***\n");
 +	pr_err("PinBased=%08x CPUBased=%08x SecondaryExec=%08x\n",
 +	       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);
 +	pr_err("EntryControls=%08x ExitControls=%08x\n", vmentry_ctl, vmexit_ctl);
 +	pr_err("ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\n",
 +	       vmcs_read32(EXCEPTION_BITMAP),
 +	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
 +	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
 +	pr_err("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",
 +	       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +	       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
 +	       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
 +	pr_err("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",
 +	       vmcs_read32(VM_EXIT_INTR_INFO),
 +	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 +	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
 +	pr_err("        reason=%08x qualification=%016lx\n",
 +	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
 +	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
 +	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
 +	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
 +	pr_err("TSC Offset = 0x%016llx\n", vmcs_read64(TSC_OFFSET));
 +	if (secondary_exec_control & SECONDARY_EXEC_TSC_SCALING)
 +		pr_err("TSC Multiplier = 0x%016llx\n",
 +		       vmcs_read64(TSC_MULTIPLIER));
 +	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)
 +		pr_err("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
 +	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
 +		pr_err("PostedIntrVec = 0x%02x\n", vmcs_read16(POSTED_INTR_NV));
 +	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))
 +		pr_err("EPT pointer = 0x%016llx\n", vmcs_read64(EPT_POINTER));
 +	n = vmcs_read32(CR3_TARGET_COUNT);
 +	for (i = 0; i + 1 < n; i += 4)
 +		pr_err("CR3 target%u=%016lx target%u=%016lx\n",
 +		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),
 +		       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));
 +	if (i < n)
 +		pr_err("CR3 target%u=%016lx\n",
 +		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));
 +	if (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)
 +		pr_err("PLE Gap=%08x Window=%08x\n",
 +		       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));
 +	if (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)
 +		pr_err("Virtual processor ID = 0x%04x\n",
 +		       vmcs_read16(VIRTUAL_PROCESSOR_ID));
 +}
 +
 +/*
 + * The guest has exited.  See if we can fix it or if we need userspace
 + * assistance.
 + */
 +static int vmx_handle_exit(struct kvm_vcpu *vcpu)
  {
 -	unsigned long exit_qualification;
 -	int size, in, string;
 -	unsigned port;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	string = (exit_qualification & 16) != 0;
 -
 -	++vcpu->stat.io_exits;
 -
 -	if (string)
 -		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -
 -	port = exit_qualification >> 16;
 -	size = (exit_qualification & 7) + 1;
 -	in = (exit_qualification & 8) != 0;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 exit_reason = vmx->exit_reason;
 +	u32 vectoring_info = vmx->idt_vectoring_info;
  
 -	return kvm_fast_pio(vcpu, size, port, in);
 -}
 +	trace_kvm_exit(exit_reason, vcpu, KVM_ISA_VMX);
  
 -static void
 -vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 -{
  	/*
 -	 * Patch in the VMCALL instruction:
 +	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more
 +	 * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before
 +	 * querying dirty_bitmap, we only need to kick all vcpus out of guest
 +	 * mode as if vcpus is in root mode, the PML buffer must has been
 +	 * flushed already.
  	 */
 -	hypercall[0] = 0x0f;
 -	hypercall[1] = 0x01;
 -	hypercall[2] = 0xc1;
 -}
 +	if (enable_pml)
 +		vmx_flush_pml_buffer(vcpu);
  
 -/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 -static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
 -{
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	/* If guest state is invalid, start emulating */
 +	if (vmx->emulation_required)
 +		return handle_invalid_guest_state(vcpu);
  
 -		/*
 -		 * We get here when L2 changed cr0 in a way that did not change
 -		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 -		 * but did change L0 shadowed bits. So we first calculate the
 -		 * effective cr0 value that L1 would like to write into the
 -		 * hardware. It consists of the L2-owned bits from the new
 -		 * value combined with the L1-owned bits from L1's guest_cr0.
 -		 */
 -		val = (val & ~vmcs12->cr0_guest_host_mask) |
 -			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
 +	if (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason))
 +		return nested_vmx_reflect_vmexit(vcpu, exit_reason);
  
 -		if (!nested_guest_cr0_valid(vcpu, val))
 -			return 1;
 +	if (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
 +		dump_vmcs();
 +		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		vcpu->run->fail_entry.hardware_entry_failure_reason
 +			= exit_reason;
 +		return 0;
 +	}
  
 -		if (kvm_set_cr0(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR0_READ_SHADOW, orig_val);
 +	if (unlikely(vmx->fail)) {
 +		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		vcpu->run->fail_entry.hardware_entry_failure_reason
 +			= vmcs_read32(VM_INSTRUCTION_ERROR);
  		return 0;
 -	} else {
 -		if (to_vmx(vcpu)->nested.vmxon &&
 -		    !nested_host_cr0_valid(vcpu, val))
 -			return 1;
 +	}
  
 -		return kvm_set_cr0(vcpu, val);
 +	/*
 +	 * Note:
 +	 * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by
 +	 * delivery event since it indicates guest is accessing MMIO.
 +	 * The vm-exit can be triggered again after return to guest that
 +	 * will cause infinite loop.
 +	 */
 +	if ((vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			(exit_reason != EXIT_REASON_EXCEPTION_NMI &&
 +			exit_reason != EXIT_REASON_EPT_VIOLATION &&
 +			exit_reason != EXIT_REASON_PML_FULL &&
 +			exit_reason != EXIT_REASON_TASK_SWITCH)) {
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;
 +		vcpu->run->internal.ndata = 3;
 +		vcpu->run->internal.data[0] = vectoring_info;
 +		vcpu->run->internal.data[1] = exit_reason;
 +		vcpu->run->internal.data[2] = vcpu->arch.exit_qualification;
 +		if (exit_reason == EXIT_REASON_EPT_MISCONFIG) {
 +			vcpu->run->internal.ndata++;
 +			vcpu->run->internal.data[3] =
 +				vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +		}
 +		return 0;
  	}
 -}
  
 -static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 -{
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked)) {
 +		if (vmx_interrupt_allowed(vcpu)) {
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
 +			   vcpu->arch.nmi_pending) {
 +			/*
 +			 * This CPU don't support us in finding the end of an
 +			 * NMI-blocked window if the guest runs with IRQs
 +			 * disabled. So we pull the trigger after 1 s of
 +			 * futile waiting, but inform the user about this.
 +			 */
 +			printk(KERN_WARNING "%s: Breaking out of NMI-blocked "
 +			       "state on VCPU %d after 1 s timeout\n",
 +			       __func__, vcpu->vcpu_id);
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		}
 +	}
  
 -		/* analogously to handle_set_cr0 */
 -		val = (val & ~vmcs12->cr4_guest_host_mask) |
 -			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 -		if (kvm_set_cr4(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR4_READ_SHADOW, orig_val);
 -		return 0;
 -	} else
 -		return kvm_set_cr4(vcpu, val);
 +	if (exit_reason < kvm_vmx_max_exit_handlers
 +	    && kvm_vmx_exit_handlers[exit_reason])
 +		return kvm_vmx_exit_handlers[exit_reason](vcpu);
 +	else {
 +		vcpu_unimpl(vcpu, "vmx: unexpected exit reason 0x%x\n",
 +				exit_reason);
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
  }
  
 -static int handle_desc(struct kvm_vcpu *vcpu)
 +/*
 + * Software based L1D cache flush which is used when microcode providing
 + * the cache control MSR is not loaded.
 + *
 + * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
 + * flush it is required to read in 64 KiB because the replacement algorithm
 + * is not exactly LRU. This could be sized at runtime via topology
 + * information but as all relevant affected CPUs have 32KiB L1D cache size
 + * there is no point in doing so.
 + */
 +static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
  {
 -	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
 +	int size = PAGE_SIZE << L1D_CACHE_ORDER;
  
 -static int handle_cr(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification, val;
 -	int cr;
 -	int reg;
 -	int err;
 -	int ret;
 +	/*
 +	 * This code is only executed when the the flush mode is 'cond' or
 +	 * 'always'
 +	 */
 +	if (static_branch_likely(&vmx_l1d_flush_cond)) {
 +		bool flush_l1d;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	cr = exit_qualification & 15;
 -	reg = (exit_qualification >> 8) & 15;
 -	switch ((exit_qualification >> 4) & 3) {
 -	case 0: /* mov to cr */
 -		val = kvm_register_readl(vcpu, reg);
 -		trace_kvm_cr_write(cr, val);
 -		switch (cr) {
 -		case 0:
 -			err = handle_set_cr0(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			err = kvm_set_cr3(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 4:
 -			err = handle_set_cr4(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 8: {
 -				u8 cr8_prev = kvm_get_cr8(vcpu);
 -				u8 cr8 = (u8)val;
 -				err = kvm_set_cr8(vcpu, cr8);
 -				ret = kvm_complete_insn_gp(vcpu, err);
 -				if (lapic_in_kernel(vcpu))
 -					return ret;
 -				if (cr8_prev <= cr8)
 -					return ret;
 -				/*
 -				 * TODO: we might be squashing a
 -				 * KVM_GUESTDBG_SINGLESTEP-triggered
 -				 * KVM_EXIT_DEBUG here.
 -				 */
 -				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 -				return 0;
 -			}
 -		}
 -		break;
 -	case 2: /* clts */
 -		WARN_ONCE(1, "Guest should always own CR0.TS");
 -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 -		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 -		return kvm_skip_emulated_instruction(vcpu);
 -	case 1: /*mov from cr*/
 -		switch (cr) {
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			val = kvm_read_cr3(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		case 8:
 -			val = kvm_get_cr8(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -		break;
 -	case 3: /* lmsw */
 -		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 -		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 -		kvm_lmsw(vcpu, val);
 +		/*
 +		 * Clear the per-vcpu flush bit, it gets set again
 +		 * either from vcpu_run() or from one of the unsafe
 +		 * VMEXIT handlers.
 +		 */
 +		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 +		vcpu->arch.l1tf_flush_l1d = false;
 +
 +		/*
 +		 * Clear the per-cpu flush bit, it gets set again from
 +		 * the interrupt handlers.
 +		 */
 +		flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
 +		kvm_clear_cpu_l1tf_flush_l1d();
  
 -		return kvm_skip_emulated_instruction(vcpu);
 -	default:
 -		break;
 +		if (!flush_l1d)
 +			return;
  	}
 -	vcpu->run->exit_reason = 0;
 -	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 -	       (int)(exit_qualification >> 4) & 3, cr);
 -	return 0;
 +
 +	vcpu->stat.l1d_flush++;
 +
 +	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 +		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 +		return;
 +	}
 +
 +	asm volatile(
 +		/* First ensure the pages are in the TLB */
 +		"xorl	%%eax, %%eax\n"
 +		".Lpopulate_tlb:\n\t"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$4096, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lpopulate_tlb\n\t"
 +		"xorl	%%eax, %%eax\n\t"
 +		"cpuid\n\t"
 +		/* Now fill the cache */
 +		"xorl	%%eax, %%eax\n"
 +		".Lfill_cache:\n"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$64, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lfill_cache\n\t"
 +		"lfence\n"
 +		:: [flush_pages] "r" (vmx_l1d_flush_pages),
 +		    [size] "r" (size)
 +		: "eax", "ebx", "ecx", "edx");
  }
  
 -static int handle_dr(struct kvm_vcpu *vcpu)
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
 -	unsigned long exit_qualification;
 -	int dr, dr7, reg;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
  
 -	/* First, if DR does not exist, trigger UD */
 -	if (!kvm_require_dr(vcpu, dr))
 -		return 1;
 +	if (is_guest_mode(vcpu) &&
 +		nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
 +		return;
  
 -	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 -	if (!kvm_require_cpl(vcpu, 0))
 -		return 1;
 -	dr7 = vmcs_readl(GUEST_DR7);
 -	if (dr7 & DR7_GD) {
 -		/*
 -		 * As the vm-exit takes precedence over the debug trap, we
 -		 * need to emulate the latter, either for the host or the
 -		 * guest debugging itself.
 -		 */
 -		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 -			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 -			vcpu->run->debug.arch.dr7 = dr7;
 -			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 -			vcpu->run->debug.arch.exception = DB_VECTOR;
 -			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 -			return 0;
 -		} else {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 -		}
 +	if (irr == -1 || tpr < irr) {
 +		vmcs_write32(TPR_THRESHOLD, 0);
 +		return;
  	}
  
 -	if (vcpu->guest_debug == 0) {
 -		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -				CPU_BASED_MOV_DR_EXITING);
 +	vmcs_write32(TPR_THRESHOLD, irr);
 +}
  
 -		/*
 -		 * No more DR vmexits; force a reload of the debug registers
 -		 * and reenter on this instruction.  The next vmexit will
 -		 * retrieve the full state of the debug registers.
 -		 */
 -		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 -		return 1;
 +static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	u32 sec_exec_control;
 +
 +	if (!lapic_in_kernel(vcpu))
 +		return;
 +
 +	if (!flexpriority_enabled &&
 +	    !cpu_has_vmx_virtualize_x2apic_mode())
 +		return;
 +
 +	/* Postpone execution until vmcs01 is the current VMCS. */
 +	if (is_guest_mode(vcpu)) {
 +		to_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;
 +		return;
  	}
  
 -	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 -	if (exit_qualification & TYPE_MOV_FROM_DR) {
 -		unsigned long val;
 +	sec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
 +	sec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +			      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
  
 -		if (kvm_get_dr(vcpu, dr, &val))
 -			return 1;
 -		kvm_register_write(vcpu, reg, val);
 -	} else
 -		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 -			return 1;
 +	switch (kvm_get_apic_mode(vcpu)) {
 +	case LAPIC_MODE_INVALID:
 +		WARN_ONCE(true, "Invalid local APIC state");
 +	case LAPIC_MODE_DISABLED:
 +		break;
 +	case LAPIC_MODE_XAPIC:
 +		if (flexpriority_enabled) {
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +			vmx_flush_tlb(vcpu, true);
 +		}
 +		break;
 +	case LAPIC_MODE_X2APIC:
 +		if (cpu_has_vmx_virtualize_x2apic_mode())
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
 +		break;
 +	}
 +	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);
  
 -	return kvm_skip_emulated_instruction(vcpu);
 +	vmx_update_msr_bitmap(vcpu);
  }
  
 -static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 +static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
  {
 -	return vcpu->arch.dr6;
 +	if (!is_guest_mode(vcpu)) {
 +		vmcs_write64(APIC_ACCESS_ADDR, hpa);
 +		vmx_flush_tlb(vcpu, true);
 +	}
  }
  
 -static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
  {
 +	u16 status;
 +	u8 old;
 +
 +	if (max_isr == -1)
 +		max_isr = 0;
 +
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = status >> 8;
 +	if (max_isr != old) {
 +		status &= 0xff;
 +		status |= max_isr << 8;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
  }
  
 -static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +static void vmx_set_rvi(int vector)
  {
 -	get_debugreg(vcpu->arch.db[0], 0);
 -	get_debugreg(vcpu->arch.db[1], 1);
 -	get_debugreg(vcpu->arch.db[2], 2);
 -	get_debugreg(vcpu->arch.db[3], 3);
 -	get_debugreg(vcpu->arch.dr6, 6);
 -	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
 +	u16 status;
 +	u8 old;
  
 -	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 +	if (vector == -1)
 +		vector = 0;
 +
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = (u8)status & 0xff;
 +	if ((u8)vector != old) {
 +		status &= ~0xff;
 +		status |= (u8)vector;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
  }
  
 -static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 +static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
  {
 -	vmcs_writel(GUEST_DR7, val);
 +	/*
 +	 * When running L2, updating RVI is only relevant when
 +	 * vmcs12 virtual-interrupt-delivery enabled.
 +	 * However, it can be enabled only when L1 also
 +	 * intercepts external-interrupts and in that case
 +	 * we should not update vmcs02 RVI but instead intercept
 +	 * interrupt. Therefore, do nothing when running L2.
 +	 */
 +	if (!is_guest_mode(vcpu))
 +		vmx_set_rvi(max_irr);
  }
  
 -static int handle_cpuid(struct kvm_vcpu *vcpu)
 +static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
  {
 -	return kvm_emulate_cpuid(vcpu);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int max_irr;
 +	bool max_irr_updated;
 +
 +	WARN_ON(!vcpu->arch.apicv_active);
 +	if (pi_test_on(&vmx->pi_desc)) {
 +		pi_clear_on(&vmx->pi_desc);
 +		/*
 +		 * IOMMU can write to PIR.ON, so the barrier matters even on UP.
 +		 * But on x86 this is just a compiler barrier anyway.
 +		 */
 +		smp_mb__after_atomic();
 +		max_irr_updated =
 +			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
 +
 +		/*
 +		 * If we are running L2 and L1 has a new pending interrupt
 +		 * which can be injected, we should re-evaluate
 +		 * what should be done with this new L1 interrupt.
 +		 * If L1 intercepts external-interrupts, we should
 +		 * exit from L2 to L1. Otherwise, interrupt should be
 +		 * delivered directly to L2.
 +		 */
 +		if (is_guest_mode(vcpu) && max_irr_updated) {
 +			if (nested_exit_on_intr(vcpu))
 +				kvm_vcpu_exiting_guest_mode(vcpu);
 +			else
 +				kvm_make_request(KVM_REQ_EVENT, vcpu);
 +		}
 +	} else {
 +		max_irr = kvm_lapic_find_highest_irr(vcpu);
 +	}
 +	vmx_hwapic_irr_update(vcpu, max_irr);
 +	return max_irr;
  }
  
 -static int handle_rdmsr(struct kvm_vcpu *vcpu)
 +static u8 vmx_has_apicv_interrupt(struct kvm_vcpu *vcpu)
  {
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	struct msr_data msr_info;
 +	u8 rvi = vmx_get_rvi();
 +	u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
  
 -	msr_info.index = ecx;
 -	msr_info.host_initiated = false;
 -	if (vmx_get_msr(vcpu, &msr_info)) {
 -		trace_kvm_msr_read_ex(ecx);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +	return ((rvi & 0xf0) > (vppr & 0xf0));
 +}
  
 -	trace_kvm_msr_read(ecx, msr_info.data);
 +static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
  
 -	/* FIXME: handling of bits 32:63 of rax, rdx */
 -	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 -	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 -	return kvm_skip_emulated_instruction(vcpu);
 +	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);
 +	vmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);
 +	vmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);
 +	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
  }
  
 -static int handle_wrmsr(struct kvm_vcpu *vcpu)
 +static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
  {
 -	struct msr_data msr;
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 -		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
 -
 -	msr.data = data;
 -	msr.index = ecx;
 -	msr.host_initiated = false;
 -	if (kvm_set_msr(vcpu, &msr) != 0) {
 -		trace_kvm_msr_write_ex(ecx, data);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -	trace_kvm_msr_write(ecx, data);
 -	return kvm_skip_emulated_instruction(vcpu);
 +	pi_clear_on(&vmx->pi_desc);
 +	memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
  }
  
 -static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
  {
 -	kvm_apic_update_ppr(vcpu);
 -	return 1;
 +	u32 exit_intr_info = 0;
 +	u16 basic_exit_reason = (u16)vmx->exit_reason;
 +
 +	if (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY
 +	      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))
 +		return;
 +
 +	if (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +	vmx->exit_intr_info = exit_intr_info;
 +
 +	/* if exit due to PF check for async PF */
 +	if (is_page_fault(exit_intr_info))
 +		vmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	/* Handle machine checks before interrupts are enabled */
 +	if (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||
 +	    is_machine_check(exit_intr_info))
 +		kvm_machine_check();
 +
 +	/* We need to handle NMIs before interrupts are enabled */
 +	if (is_nmi(exit_intr_info)) {
 +		kvm_before_interrupt(&vmx->vcpu);
 +		asm("int $2");
 +		kvm_after_interrupt(&vmx->vcpu);
 +	}
  }
  
 -static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 +static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
  {
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_INTR_PENDING);
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
  
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +	if ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))
 +			== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {
 +		unsigned int vector;
 +		unsigned long entry;
 +		gate_desc *desc;
 +		struct vcpu_vmx *vmx = to_vmx(vcpu);
 +#ifdef CONFIG_X86_64
 +		unsigned long tmp;
 +#endif
  
 -	++vcpu->stat.irq_window_exits;
 -	return 1;
 +		vector =  exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		desc = (gate_desc *)vmx->host_idt_base + vector;
 +		entry = gate_offset(desc);
 +		asm volatile(
 +#ifdef CONFIG_X86_64
 +			"mov %%" _ASM_SP ", %[sp]\n\t"
 +			"and $0xfffffffffffffff0, %%" _ASM_SP "\n\t"
 +			"push $%c[ss]\n\t"
 +			"push %[sp]\n\t"
 +#endif
 +			"pushf\n\t"
 +			__ASM_SIZE(push) " $%c[cs]\n\t"
 +			CALL_NOSPEC
 +			:
 +#ifdef CONFIG_X86_64
 +			[sp]"=&r"(tmp),
 +#endif
 +			ASM_CALL_CONSTRAINT
 +			:
 +			THUNK_TARGET(entry),
 +			[ss]"i"(__KERNEL_DS),
 +			[cs]"i"(__KERNEL_CS)
 +			);
 +	}
  }
 +STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
  
 -static int handle_halt(struct kvm_vcpu *vcpu)
 +static bool vmx_has_emulated_msr(int index)
  {
 -	return kvm_emulate_halt(vcpu);
 +	switch (index) {
 +	case MSR_IA32_SMBASE:
 +		/*
 +		 * We cannot do SMM unless we can run the guest in big
 +		 * real mode.
 +		 */
 +		return enable_unrestricted_guest || emulate_invalid_guest_state;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		/* This is AMD only.  */
 +		return false;
 +	default:
 +		return true;
 +	}
  }
  
 -static int handle_vmcall(struct kvm_vcpu *vcpu)
 +static bool vmx_mpx_supported(void)
  {
 -	return kvm_emulate_hypercall(vcpu);
 +	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 +		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
  }
  
 -static int handle_invd(struct kvm_vcpu *vcpu)
 +static bool vmx_xsaves_supported(void)
  {
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_XSAVES;
  }
  
 -static int handle_invlpg(struct kvm_vcpu *vcpu)
 +static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
  {
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	u32 exit_intr_info;
 +	bool unblock_nmi;
 +	u8 vector;
 +	bool idtv_info_valid;
  
 -	kvm_mmu_invlpg(vcpu, exit_qualification);
 -	return kvm_skip_emulated_instruction(vcpu);
 +	idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
 +
 +	if (enable_vnmi) {
 +		if (vmx->loaded_vmcs->nmi_known_unmasked)
 +			return;
 +		/*
 +		 * Can't use vmx->exit_intr_info since we're not sure what
 +		 * the exit reason is.
 +		 */
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +		unblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;
 +		vector = exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Re-set bit "block by NMI" before VM entry if vmexit caused by
 +		 * a guest IRET fault.
 +		 * SDM 3: 23.2.2 (September 2008)
 +		 * Bit 12 is undefined in any of the following cases:
 +		 *  If the VM exit sets the valid bit in the IDT-vectoring
 +		 *   information field.
 +		 *  If the VM exit is due to a double fault.
 +		 */
 +		if ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&
 +		    vector != DF_VECTOR && !idtv_info_valid)
 +			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				      GUEST_INTR_STATE_NMI);
 +		else
 +			vmx->loaded_vmcs->nmi_known_unmasked =
 +				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)
 +				  & GUEST_INTR_STATE_NMI);
 +	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->vnmi_blocked_time +=
 +			ktime_to_ns(ktime_sub(ktime_get(),
 +					      vmx->loaded_vmcs->entry_time));
  }
  
 -static int handle_rdpmc(struct kvm_vcpu *vcpu)
 +static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 +				      u32 idt_vectoring_info,
 +				      int instr_len_field,
 +				      int error_code_field)
  {
 -	int err;
 +	u8 vector;
 +	int type;
 +	bool idtv_info_valid;
  
 -	err = kvm_rdpmc(vcpu);
 -	return kvm_complete_insn_gp(vcpu, err);
 +	idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
 +
 +	vcpu->arch.nmi_injected = false;
 +	kvm_clear_exception_queue(vcpu);
 +	kvm_clear_interrupt_queue(vcpu);
 +
 +	if (!idtv_info_valid)
 +		return;
 +
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +
 +	vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
 +	type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
 +
 +	switch (type) {
 +	case INTR_TYPE_NMI_INTR:
 +		vcpu->arch.nmi_injected = true;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Clear bit "block by NMI" before VM entry if a NMI
 +		 * delivery faulted.
 +		 */
 +		vmx_set_nmi_mask(vcpu, false);
 +		break;
 +	case INTR_TYPE_SOFT_EXCEPTION:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_HARD_EXCEPTION:
 +		if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
 +			u32 err = vmcs_read32(error_code_field);
 +			kvm_requeue_exception_e(vcpu, vector, err);
 +		} else
 +			kvm_requeue_exception(vcpu, vector);
 +		break;
 +	case INTR_TYPE_SOFT_INTR:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_EXT_INTR:
 +		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 +		break;
 +	default:
 +		break;
 +	}
  }
  
 -static int handle_wbinvd(struct kvm_vcpu *vcpu)
 +static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
  {
 -	return kvm_emulate_wbinvd(vcpu);
 +	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 +				  VM_EXIT_INSTRUCTION_LEN,
 +				  IDT_VECTORING_ERROR_CODE);
  }
  
 -static int handle_xsetbv(struct kvm_vcpu *vcpu)
 +static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
  {
 -	u64 new_bv = kvm_read_edx_eax(vcpu);
 -	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
 +	__vmx_complete_interrupts(vcpu,
 +				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +				  VM_ENTRY_INSTRUCTION_LEN,
 +				  VM_ENTRY_EXCEPTION_ERROR_CODE);
  
 -	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 -		return kvm_skip_emulated_instruction(vcpu);
 -	return 1;
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
  }
  
 -static int handle_xsaves(struct kvm_vcpu *vcpu)
 +static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
  {
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 +	int i, nr_msrs;
 +	struct perf_guest_switch_msr *msrs;
 +
 +	msrs = perf_guest_get_msrs(&nr_msrs);
 +
 +	if (!msrs)
 +		return;
 +
 +	for (i = 0; i < nr_msrs; i++)
 +		if (msrs[i].host == msrs[i].guest)
 +			clear_atomic_switch_msr(vmx, msrs[i].msr);
 +		else
 +			add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,
 +					msrs[i].host, false);
  }
  
 -static int handle_xrstors(struct kvm_vcpu *vcpu)
 +static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
  {
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 +	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, val);
 +	if (!vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
 +			      PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = true;
  }
  
 -static int handle_apic_access(struct kvm_vcpu *vcpu)
 +static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
  {
 -	if (likely(fasteoi)) {
 -		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -		int access_type, offset;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 tscl;
 +	u32 delta_tsc;
  
 -		access_type = exit_qualification & APIC_ACCESS_TYPE;
 -		offset = exit_qualification & APIC_ACCESS_OFFSET;
 -		/*
 -		 * Sane guest uses MOV to write EOI, with written value
 -		 * not cared. So make a short-circuit here by avoiding
 -		 * heavy instruction emulation.
 -		 */
 -		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 -		    (offset == APIC_EOI)) {
 -			kvm_lapic_set_eoi(vcpu);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 +	if (vmx->req_immediate_exit) {
 +		vmx_arm_hv_timer(vmx, 0);
 +		return;
  	}
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
  
 -static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	int vector = exit_qualification & 0xff;
 +	if (vmx->hv_deadline_tsc != -1) {
 +		tscl = rdtsc();
 +		if (vmx->hv_deadline_tsc > tscl)
 +			/* set_hv_timer ensures the delta fits in 32-bits */
 +			delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
 +				cpu_preemption_timer_multi);
 +		else
 +			delta_tsc = 0;
  
 -	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_set_eoi_accelerated(vcpu, vector);
 -	return 1;
 +		vmx_arm_hv_timer(vmx, delta_tsc);
 +		return;
 +	}
 +
 +	if (vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
 +				PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = false;
  }
  
 -static int handle_apic_write(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
++=======
++void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
+ {
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	u32 offset = exit_qualification & 0xfff;
 -
 -	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_write_nodecode(vcpu, offset);
 -	return 1;
++	if (unlikely(host_rsp != vmx->loaded_vmcs->host_state.rsp)) {
++		vmx->loaded_vmcs->host_state.rsp = host_rsp;
++		vmcs_writel(HOST_RSP, host_rsp);
++	}
+ }
+ 
 -static int handle_task_switch(struct kvm_vcpu *vcpu)
++static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
++>>>>>>> c823dd5c0f3f (KVM: VMX: Fold __vmx_vcpu_run() back into vmx_vcpu_run())
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	unsigned long exit_qualification;
 -	bool has_error_code = false;
 -	u32 error_code = 0;
 -	u16 tss_selector;
 -	int reason, type, idt_v, idt_index;
 +	unsigned long cr3, cr4, evmcs_rsp;
  
 -	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 -	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 -	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
 +	/* Record the guest's net vcpu time for enforced NMI injections. */
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->entry_time = ktime_get();
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	/* Don't enter VMX if guest state is invalid, let the exit handler
 +	   start emulation until we arrive back to a valid state */
 +	if (vmx->emulation_required)
 +		return;
  
 -	reason = (u32)exit_qualification >> 30;
 -	if (reason == TASK_SWITCH_GATE && idt_v) {
 -		switch (type) {
 -		case INTR_TYPE_NMI_INTR:
 -			vcpu->arch.nmi_injected = false;
 -			vmx_set_nmi_mask(vcpu, true);
 -			break;
 -		case INTR_TYPE_EXT_INTR:
 -		case INTR_TYPE_SOFT_INTR:
 -			kvm_clear_interrupt_queue(vcpu);
 -			break;
 -		case INTR_TYPE_HARD_EXCEPTION:
 -			if (vmx->idt_vectoring_info &
 -			    VECTORING_INFO_DELIVER_CODE_MASK) {
 -				has_error_code = true;
 -				error_code =
 -					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 -			}
 -			/* fall through */
 -		case INTR_TYPE_SOFT_EXCEPTION:
 -			kvm_clear_exception_queue(vcpu);
 -			break;
 -		default:
 -			break;
 +	if (vmx->ple_window_dirty) {
 +		vmx->ple_window_dirty = false;
 +		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 +	}
 +
 +	if (vmx->nested.need_vmcs12_sync) {
 +		if (vmx->nested.hv_evmcs) {
 +			copy_vmcs12_to_enlightened(vmx);
 +			/* All fields are clean */
 +			vmx->nested.hv_evmcs->hv_clean_fields |=
 +				HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
 +		} else {
 +			copy_vmcs12_to_shadow(vmx);
  		}
 +		vmx->nested.need_vmcs12_sync = false;
  	}
 -	tss_selector = exit_qualification;
  
 -	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 -		       type != INTR_TYPE_EXT_INTR &&
 -		       type != INTR_TYPE_NMI_INTR))
 -		skip_emulated_instruction(vcpu);
 +	if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 +	if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
  
 -	if (kvm_task_switch(vcpu, tss_selector,
 -			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 -			    has_error_code, error_code) == EMULATE_FAIL) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -		vcpu->run->internal.ndata = 0;
 -		return 0;
 +	cr3 = __get_current_cr3_fast();
 +	if (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {
 +		vmcs_writel(HOST_CR3, cr3);
 +		vmx->loaded_vmcs->host_state.cr3 = cr3;
 +	}
 +
 +	cr4 = cr4_read_shadow();
 +	if (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {
 +		vmcs_writel(HOST_CR4, cr4);
 +		vmx->loaded_vmcs->host_state.cr4 = cr4;
  	}
  
 +	/* When single-stepping over STI and MOV SS, we must clear the
 +	 * corresponding interruptibility bits in the guest state. Otherwise
 +	 * vmentry fails as it then expects bit 14 (BS) in pending debug
 +	 * exceptions being set, but that's not correct for the guest debugging
 +	 * case. */
 +	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 +		vmx_set_interrupt_shadow(vcpu, 0);
 +
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
 +	    vcpu->arch.pkru != vmx->host_pkru)
 +		__write_pkru(vcpu->arch.pkru);
 +
 +	atomic_switch_perf_msrs(vmx);
 +
 +	vmx_update_hv_timer(vcpu);
 +
  	/*
 -	 * TODO: What about debug traps on tss switch?
 -	 *       Are we supposed to inject them and update dr6?
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
  	 */
 +	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
  
 -	return 1;
 -}
++<<<<<<< HEAD
 +	vmx->__launched = vmx->loaded_vmcs->launched;
  
 -static int handle_ept_violation(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification;
 -	gpa_t gpa;
 -	u64 error_code;
 +	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
 +		(unsigned long)&current_evmcs->host_rsp : 0;
 +
 +	if (static_branch_unlikely(&vmx_l1d_should_flush))
 +		vmx_l1d_flush(vcpu);
 +
 +	asm(
 +		/* Store host registers */
 +		"push %%" _ASM_DX "; push %%" _ASM_BP ";"
 +		"push %%" _ASM_CX " \n\t" /* placeholder for guest rcx */
 +		"push %%" _ASM_CX " \n\t"
 +		"cmp %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		"je 1f \n\t"
 +		"mov %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		/* Avoid VMWRITE when Enlightened VMCS is in use */
 +		"test %%" _ASM_SI ", %%" _ASM_SI " \n\t"
 +		"jz 2f \n\t"
 +		"mov %%" _ASM_SP ", (%%" _ASM_SI ") \n\t"
 +		"jmp 1f \n\t"
 +		"2: \n\t"
 +		__ex("vmwrite %%" _ASM_SP ", %%" _ASM_DX) "\n\t"
 +		"1: \n\t"
 +		/* Reload cr2 if changed */
 +		"mov %c[cr2](%0), %%" _ASM_AX " \n\t"
 +		"mov %%cr2, %%" _ASM_DX " \n\t"
 +		"cmp %%" _ASM_AX ", %%" _ASM_DX " \n\t"
 +		"je 3f \n\t"
 +		"mov %%" _ASM_AX", %%cr2 \n\t"
 +		"3: \n\t"
 +		/* Check if vmlaunch or vmresume is needed */
 +		"cmpl $0, %c[launched](%0) \n\t"
 +		/* Load guest registers.  Don't clobber flags. */
 +		"mov %c[rax](%0), %%" _ASM_AX " \n\t"
 +		"mov %c[rbx](%0), %%" _ASM_BX " \n\t"
 +		"mov %c[rdx](%0), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%0), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%0), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%0), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%0),  %%r8  \n\t"
 +		"mov %c[r9](%0),  %%r9  \n\t"
 +		"mov %c[r10](%0), %%r10 \n\t"
 +		"mov %c[r11](%0), %%r11 \n\t"
 +		"mov %c[r12](%0), %%r12 \n\t"
 +		"mov %c[r13](%0), %%r13 \n\t"
 +		"mov %c[r14](%0), %%r14 \n\t"
 +		"mov %c[r15](%0), %%r15 \n\t"
 +#endif
 +		"mov %c[rcx](%0), %%" _ASM_CX " \n\t" /* kills %0 (ecx) */
 +
 +		/* Enter guest mode */
 +		"jne 1f \n\t"
 +		__ex("vmlaunch") "\n\t"
 +		"jmp 2f \n\t"
 +		"1: " __ex("vmresume") "\n\t"
 +		"2: "
 +		/* Save guest registers, load host registers, keep flags */
 +		"mov %0, %c[wordsize](%%" _ASM_SP ") \n\t"
 +		"pop %0 \n\t"
 +		"setbe %c[fail](%0)\n\t"
 +		"mov %%" _ASM_AX ", %c[rax](%0) \n\t"
 +		"mov %%" _ASM_BX ", %c[rbx](%0) \n\t"
 +		__ASM_SIZE(pop) " %c[rcx](%0) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%0) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%0) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%0) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%0) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%0) \n\t"
 +		"mov %%r9,  %c[r9](%0) \n\t"
 +		"mov %%r10, %c[r10](%0) \n\t"
 +		"mov %%r11, %c[r11](%0) \n\t"
 +		"mov %%r12, %c[r12](%0) \n\t"
 +		"mov %%r13, %c[r13](%0) \n\t"
 +		"mov %%r14, %c[r14](%0) \n\t"
 +		"mov %%r15, %c[r15](%0) \n\t"
 +		/*
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d,  %%r8d \n\t"
 +		"xor %%r9d,  %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"mov %%cr2, %%" _ASM_AX "   \n\t"
 +		"mov %%" _ASM_AX ", %c[cr2](%0) \n\t"
 +
 +		"xor %%eax, %%eax \n\t"
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop  %%" _ASM_BP "; pop  %%" _ASM_DX " \n\t"
 +		".pushsection .rodata \n\t"
 +		".global vmx_return \n\t"
 +		"vmx_return: " _ASM_PTR " 2b \n\t"
 +		".popsection"
 +	      : : "c"(vmx), "d"((unsigned long)HOST_RSP), "S"(evmcs_rsp),
 +		[launched]"i"(offsetof(struct vcpu_vmx, __launched)),
 +		[fail]"i"(offsetof(struct vcpu_vmx, fail)),
 +		[host_rsp]"i"(offsetof(struct vcpu_vmx, host_rsp)),
 +		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
 +		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
 +#ifdef CONFIG_X86_64
 +		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
 +		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
 +		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
 +		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
 +		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
 +		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
 +		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
 +		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
 +#endif
 +		[cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),
 +		[wordsize]"i"(sizeof(ulong))
 +	      : "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rax", "rbx", "rdi"
 +		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
 +#else
 +		, "eax", "ebx", "edi"
 +#endif
 +	      );
++=======
++	if (static_branch_unlikely(&vmx_l1d_should_flush))
++		vmx_l1d_flush(vcpu);
++
++	if (vcpu->arch.cr2 != read_cr2())
++		write_cr2(vcpu->arch.cr2);
++
++	asm(
++		"call ____vmx_vcpu_run \n\t"
++	      : ASM_CALL_CONSTRAINT, "=b"(vmx->fail),
++#ifdef CONFIG_X86_64
++		"=D"((int){0}), "=S"((int){0})
++	      : "D"(vmx), "S"(&vcpu->arch.regs),
++#else
++		"=a"((int){0}), "=d"((int){0})
++	      : "a"(vmx), "d"(&vcpu->arch.regs),
++#endif
++		"b"(vmx->loaded_vmcs->launched)
++	      : "cc", "memory"
++#ifdef CONFIG_X86_64
++		, "rax", "rcx", "rdx"
++		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
++#else
++		, "ecx", "edi", "esi"
++#endif
++	      );
++
++	vcpu->arch.cr2 = read_cr2();
++>>>>>>> c823dd5c0f3f (KVM: VMX: Fold __vmx_vcpu_run() back into vmx_vcpu_run())
 +
 +	/*
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 */
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 +
 +	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
 +
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
 +
 +	/* All fields are clean at this point */
 +	if (static_branch_unlikely(&enable_evmcs))
 +		current_evmcs->hv_clean_fields |=
 +			HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
 +
 +	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 +	if (vmx->host_debugctlmsr)
 +		update_debugctlmsr(vmx->host_debugctlmsr);
 +
 +#ifndef CONFIG_X86_64
 +	/*
 +	 * The sysexit path does not restore ds/es, so we must set them to
 +	 * a reasonable value ourselves.
 +	 *
 +	 * We can't defer this to vmx_prepare_switch_to_host() since that
 +	 * function may be executed in interrupt context, which saves and
 +	 * restore segments around it, nullifying its effect.
 +	 */
 +	loadsegment(ds, __USER_DS);
 +	loadsegment(es, __USER_DS);
 +#endif
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
 +				  | (1 << VCPU_EXREG_RFLAGS)
 +				  | (1 << VCPU_EXREG_PDPTR)
 +				  | (1 << VCPU_EXREG_SEGMENTS)
 +				  | (1 << VCPU_EXREG_CR3));
 +	vcpu->arch.regs_dirty = 0;
  
  	/*
 -	 * EPT violation happened while executing iret from NMI,
 -	 * "blocked by NMI" bit has to be set before next VM entry.
 -	 * There are errata that may cause this bit to not be set:
 -	 * AAK134, BY25.
 +	 * eager fpu is enabled if PKEY is supported and CR4 is switched
 +	 * back on host, so it is safe to read guest PKRU from current
 +	 * XSAVE.
  	 */
 -	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 -			enable_vnmi &&
 -			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 -		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
 +		vcpu->arch.pkru = __read_pkru();
 +		if (vcpu->arch.pkru != vmx->host_pkru)
 +			__write_pkru(vmx->host_pkru);
 +	}
  
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	trace_kvm_page_fault(gpa, exit_qualification);
 +	vmx->nested.nested_run_pending = 0;
 +	vmx->idt_vectoring_info = 0;
  
 -	/* Is it a read fault? */
 -	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 -		     ? PFERR_USER_MASK : 0;
 -	/* Is it a write fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 -		      ? PFERR_WRITE_MASK : 0;
 -	/* Is it a fetch fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 -		      ? PFERR_FETCH_MASK : 0;
 -	/* ept page table entry is present? */
 -	error_code |= (exit_qualification &
 -		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 -			EPT_VIOLATION_EXECUTABLE))
 -		      ? PFERR_PRESENT_MASK : 0;
 +	vmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);
 +	if (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		return;
  
 -	error_code |= (exit_qualification & 0x100) != 0 ?
 -	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 +	vmx->loaded_vmcs->launched = 1;
 +	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
  
 -	vcpu->arch.exit_qualification = exit_qualification;
 -	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 +	vmx_complete_atomic_exit(vmx);
 +	vmx_recover_nmi_blocking(vmx);
 +	vmx_complete_interrupts(vmx);
  }
 +STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
  
 -static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 +static struct kvm *vmx_vm_alloc(void)
  {
 -	gpa_t gpa;
 -
 -	/*
 -	 * A nested guest cannot optimize MMIO vmexits, because we have an
 -	 * nGPA here instead of the required GPA.
 -	 */
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	if (!is_guest_mode(vcpu) &&
 -	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 -		trace_kvm_fast_mmio(gpa);
 -		/*
 -		 * Doing kvm_skip_emulated_instruction() depends on undefined
 -		 * behavior: Intel's manual doesn't mandate
 -		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 -		 * occurs and while on real hardware it was observed to be set,
 -		 * other hypervisors (namely Hyper-V) don't set it, we end up
 -		 * advancing IP with some random value. Disable fast mmio when
 -		 * running nested and keep it for real hardware in hope that
 -		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 -		 */
 -		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 -			return kvm_skip_emulated_instruction(vcpu);
 -		else
 -			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 -								EMULATE_DONE;
 -	}
 -
 -	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 +	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 +	return &kvm_vmx->kvm;
  }
  
 -static int handle_nmi_window(struct kvm_vcpu *vcpu)
 +static void vmx_vm_free(struct kvm *kvm)
  {
 -	WARN_ON_ONCE(!enable_vnmi);
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_NMI_PENDING);
 -	++vcpu->stat.nmi_window_exits;
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 -
 -	return 1;
 +	vfree(to_kvm_vmx(kvm));
  }
  
 -static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 +static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	enum emulation_result err = EMULATE_DONE;
 -	int ret = 1;
 -	u32 cpu_exec_ctrl;
 -	bool intr_window_requested;
 -	unsigned count = 130;
 +	int cpu;
  
 -	/*
 -	 * We should never reach the point where we are emulating L2
 -	 * due to invalid guest state as that means we incorrectly
 -	 * allowed a nested VMEntry with an invalid vmcs12.
 -	 */
 -	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
 +	if (vmx->loaded_vmcs == vmcs)
 +		return;
  
 -	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 -	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
 +	cpu = get_cpu();
 +	vmx_vcpu_put(vcpu);
 +	vmx->loaded_vmcs = vmcs;
 +	vmx_vcpu_load(vcpu, cpu);
 +	put_cpu();
  
 -	while (vmx->emulation_required && count-- != 0) {
 -		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 -			return handle_interrupt_window(&vmx->vcpu);
 +	vm_entry_controls_reset_shadow(vmx);
 +	vm_exit_controls_reset_shadow(vmx);
 +	vmx_segment_cache_clear(vmx);
 +}
  
 -		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
 -			return 1;
 +/*
 + * Ensure that the current vmcs of the logical processor is the
 + * vmcs01 of the vcpu before calling free_nested().
 + */
 +static void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)
 +{
 +       struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -		err = kvm_emulate_instruction(vcpu, 0);
 +       vcpu_load(vcpu);
 +       vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 +       free_nested(vmx);
 +       vcpu_put(vcpu);
 +}
  
 -		if (err == EMULATE_USER_EXIT) {
 -			++vcpu->stat.mmio_exits;
 -			ret = 0;
 -			goto out;
 -		}
 +static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -		if (err != EMULATE_DONE)
 -			goto emulation_error;
 +	if (enable_pml)
 +		vmx_destroy_pml_buffer(vmx);
 +	free_vpid(vmx->vpid);
 +	leave_guest_mode(vcpu);
 +	vmx_free_vcpu_nested(vcpu);
 +	free_loaded_vmcs(vmx->loaded_vmcs);
 +	kfree(vmx->guest_msrs);
 +	kvm_vcpu_uninit(vcpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +}
  
 -		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 -		    vcpu->arch.exception.pending)
 -			goto emulation_error;
 +static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 +{
 +	int err;
 +	struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
 +	unsigned long *msr_bitmap;
 +	int cpu;
  
 -		if (vcpu->arch.halt_request) {
 -			vcpu->arch.halt_request = 0;
 -			ret = kvm_vcpu_halt(vcpu);
 -			goto out;
 -		}
 +	if (!vmx)
 +		return ERR_PTR(-ENOMEM);
  
 -		if (signal_pending(current))
 -			goto out;
 -		if (need_resched())
 -			schedule();
 +	vmx->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache, GFP_KERNEL);
 +	if (!vmx->vcpu.arch.guest_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
 +		err = -ENOMEM;
 +		goto free_partial_vcpu;
  	}
  
 -out:
 -	return ret;
 -
 -emulation_error:
 -	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -	vcpu->run->internal.ndata = 0;
 -	return 0;
 -}
 -
 -static void grow_ple_window(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 -
 -	vmx->ple_window = __grow_ple_window(old, ple_window,
 -					    ple_window_grow,
 -					    ple_window_max);
 +	vmx->vpid = allocate_vpid();
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
 +	if (err)
 +		goto free_vcpu;
  
 -	trace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);
 -}
 +	err = -ENOMEM;
  
 -static void shrink_ple_window(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 +	/*
 +	 * If PML is turned on, failure on enabling PML just results in failure
 +	 * of creating the vcpu, therefore we can simplify PML logic (by
 +	 * avoiding dealing with cases, such as enabling PML partially on vcpus
 +	 * for the guest, etc.
 +	 */
 +	if (enable_pml) {
 +		vmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);
 +		if (!vmx->pml_pg)
 +			goto uninit_vcpu;
 +	}
  
 -	vmx->ple_window = __shrink_ple_window(old, ple_window,
 -					      ple_window_shrink,
 -					      ple_window);
 +	vmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);
 +	BUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])
 +		     > PAGE_SIZE);
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	if (!vmx->guest_msrs)
 +		goto free_pml;
  
 -	trace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);
 -}
 +	err = alloc_loaded_vmcs(&vmx->vmcs01);
 +	if (err < 0)
 +		goto free_msrs;
  
 -/*
 - * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
 - */
 -static void wakeup_handler(void)
 -{
 -	struct kvm_vcpu *vcpu;
 -	int cpu = smp_processor_id();
 +	msr_bitmap = vmx->vmcs01.msr_bitmap;
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_TSC, MSR_TYPE_R);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_FS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_GS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
 +	vmx->msr_bitmap_mode = 0;
  
 -	spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 -	list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
 -			blocked_vcpu_list) {
 -		struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
 +	vmx->loaded_vmcs = &vmx->vmcs01;
 +	cpu = get_cpu();
 +	vmx_vcpu_load(&vmx->vcpu, cpu);
 +	vmx->vcpu.cpu = cpu;
 +	vmx_vcpu_setup(vmx);
 +	vmx_vcpu_put(&vmx->vcpu);
 +	put_cpu();
 +	if (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {
 +		err = alloc_apic_access_page(kvm);
 +		if (err)
 +			goto free_vmcs;
 +	}
  
 -		if (pi_test_on(pi_desc) == 1)
 -			kvm_vcpu_kick(vcpu);
 +	if (enable_ept && !enable_unrestricted_guest) {
 +		err = init_rmode_identity_map(kvm);
 +		if (err)
 +			goto free_vmcs;
  	}
 -	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 -}
  
 -static void vmx_enable_tdp(void)
 -{
 -	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
 -		enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull,
 -		enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
 -		0ull, VMX_EPT_EXECUTABLE_MASK,
 -		cpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK,
 -		VMX_EPT_RWX_MASK, 0ull);
 +	if (nested)
 +		nested_vmx_setup_ctls_msrs(&vmx->nested.msrs,
 +					   vmx_capability.ept,
 +					   kvm_vcpu_apicv_active(&vmx->vcpu));
  
 -	ept_set_mmio_spte_mask();
 -	kvm_enable_tdp();
 -}
 +	vmx->nested.posted_intr_nv = -1;
 +	vmx->nested.current_vmptr = -1ull;
  
 -/*
 - * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
 - * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
 - */
 -static int handle_pause(struct kvm_vcpu *vcpu)
 -{
 -	if (!kvm_pause_in_guest(vcpu->kvm))
 -		grow_ple_window(vcpu);
 +	vmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;
  
  	/*
 -	 * Intel sdm vol3 ch-25.1.3 says: The "PAUSE-loop exiting"
 -	 * VM-execution control is ignored if CPL > 0. OTOH, KVM
 -	 * never set PAUSE_EXITING and just set PLE if supported,
 -	 * so the vcpu must be CPL=0 if it gets a PAUSE exit.
 +	 * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR
 +	 * or POSTED_INTR_WAKEUP_VECTOR.
  	 */
 -	kvm_vcpu_on_spin(vcpu, true);
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	vmx->pi_desc.nv = POSTED_INTR_VECTOR;
 +	vmx->pi_desc.sn = 1;
  
 -static int handle_nop(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	return &vmx->vcpu;
  
 -static int handle_mwait(struct kvm_vcpu *vcpu)
 -{
 -	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 -	return handle_nop(vcpu);
 +free_vmcs:
 +	free_loaded_vmcs(vmx->loaded_vmcs);
 +free_msrs:
 +	kfree(vmx->guest_msrs);
 +free_pml:
 +	vmx_destroy_pml_buffer(vmx);
 +uninit_vcpu:
 +	kvm_vcpu_uninit(&vmx->vcpu);
 +free_vcpu:
 +	free_vpid(vmx->vpid);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +free_partial_vcpu:
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +	return ERR_PTR(err);
  }
  
 -static int handle_invalid_op(struct kvm_vcpu *vcpu)
 -{
 -	kvm_queue_exception(vcpu, UD_VECTOR);
 -	return 1;
 -}
 +#define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 +#define L1TF_MSG_L1D "L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
  
 -static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 +static int vmx_vm_init(struct kvm *kvm)
  {
 -	return 1;
 +	spin_lock_init(&to_kvm_vmx(kvm)->ept_pointer_lock);
 +
 +	if (!ple_gap)
 +		kvm->arch.pause_in_guest = true;
 +
 +	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
 +		switch (l1tf_mitigation) {
 +		case L1TF_MITIGATION_OFF:
 +		case L1TF_MITIGATION_FLUSH_NOWARN:
 +			/* 'I explicitly don't care' is set */
 +			break;
 +		case L1TF_MITIGATION_FLUSH:
 +		case L1TF_MITIGATION_FLUSH_NOSMT:
 +		case L1TF_MITIGATION_FULL:
 +			/*
 +			 * Warn upon starting the first VM in a potentially
 +			 * insecure environment.
 +			 */
 +			if (sched_smt_active())
 +				pr_warn_once(L1TF_MSG_SMT);
 +			if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
 +				pr_warn_once(L1TF_MSG_L1D);
 +			break;
 +		case L1TF_MITIGATION_FULL_FORCE:
 +			/* Flush is enforced */
 +			break;
 +		}
 +	}
 +	return 0;
  }
  
 -static int handle_monitor(struct kvm_vcpu *vcpu)
 +static void __init vmx_check_processor_compat(void *rtn)
  {
 -	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 -	return handle_nop(vcpu);
 +	struct vmcs_config vmcs_conf;
 +	struct vmx_capability vmx_cap;
 +
 +	*(int *)rtn = 0;
 +	if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0)
 +		*(int *)rtn = -EIO;
 +	nested_vmx_setup_ctls_msrs(&vmcs_conf.nested, vmx_cap.ept, enable_apicv);
 +	if (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config)) != 0) {
 +		printk(KERN_ERR "kvm: CPU %d feature inconsistency!\n",
 +				smp_processor_id());
 +		*(int *)rtn = -EIO;
 +	}
  }
  
 -static int handle_invpcid(struct kvm_vcpu *vcpu)
 +static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
  {
 -	u32 vmx_instruction_info;
 -	unsigned long type;
 -	bool pcid_enabled;
 -	gva_t gva;
 -	struct x86_exception e;
 -	unsigned i;
 -	unsigned long roots_to_free = 0;
 -	struct {
 -		u64 pcid;
 -		u64 gla;
 -	} operand;
 +	u8 cache;
 +	u64 ipat = 0;
  
 -	if (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {
 -		kvm_queue_exception(vcpu, UD_VECTOR);
 -		return 1;
 +	/* For VT-d and EPT combination
 +	 * 1. MMIO: always map as UC
 +	 * 2. EPT with VT-d:
 +	 *   a. VT-d without snooping control feature: can't guarantee the
 +	 *	result, try to trust guest.
 +	 *   b. VT-d with snooping control feature: snooping control feature of
 +	 *	VT-d engine can guarantee the cache correctness. Just set it
 +	 *	to WB to keep consistent with host. So the same as item 3.
 +	 * 3. EPT without VT-d: always map as WB and set IPAT=1 to keep
 +	 *    consistent with host MTRR
 +	 */
 +	if (is_mmio) {
 +		cache = MTRR_TYPE_UNCACHABLE;
 +		goto exit;
  	}
  
 -	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 -	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 +	if (!kvm_arch_has_noncoherent_dma(vcpu->kvm)) {
 +		ipat = VMX_EPT_IPAT_BIT;
 +		cache = MTRR_TYPE_WRBACK;
 +		goto exit;
 +	}
  
 -	if (type > 3) {
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 +	if (kvm_read_cr0(vcpu) & X86_CR0_CD) {
 +		ipat = VMX_EPT_IPAT_BIT;
 +		if (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))
 +			cache = MTRR_TYPE_WRBACK;
 +		else
 +			cache = MTRR_TYPE_UNCACHABLE;
 +		goto exit;
  	}
  
 -	/* According to the Intel instruction reference, the memory operand
 -	 * is read even if it isn't needed (e.g., for type==all)
 -	 */
 -	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 -				vmx_instruction_info, false, &gva))
 -		return 1;
 +	cache = kvm_mtrr_get_guest_memory_type(vcpu, gfn);
  
 -	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 -		kvm_inject_page_fault(vcpu, &e);
 -		return 1;
 -	}
 +exit:
 +	return (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;
 +}
  
 -	if (operand.pcid >> 12 != 0) {
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +static int vmx_get_lpage_level(void)
 +{
 +	if (enable_ept && !cpu_has_vmx_ept_1g_page())
 +		return PT_DIRECTORY_LEVEL;
 +	else
 +		/* For shadow and EPT supported 1GB page */
 +		return PT_PDPE_LEVEL;
 +}
  
 -	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
 +static void vmcs_set_secondary_exec_control(u32 new_ctl)
 +{
 +	/*
 +	 * These bits in the secondary execution controls field
 +	 * are dynamic, the others are mostly based on the hypervisor
 +	 * architecture and the guest's CPUID.  Do not touch the
 +	 * dynamic bits.
 +	 */
 +	u32 mask =
 +		SECONDARY_EXEC_SHADOW_VMCS |
 +		SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 +		SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +		SECONDARY_EXEC_DESC;
  
 -	switch (type) {
 -	case INVPCID_TYPE_INDIV_ADDR:
 -		if ((!pcid_enabled && (operand.pcid != 0)) ||
 -		    is_noncanonical_address(operand.gla, vcpu)) {
 -			kvm_inject_gp(vcpu, 0);
 -			return 1;
 -		}
 -		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
 -		return kvm_skip_emulated_instruction(vcpu);
 +	u32 cur_ctl = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
  
 -	case INVPCID_TYPE_SINGLE_CTXT:
 -		if (!pcid_enabled && (operand.pcid != 0)) {
 -			kvm_inject_gp(vcpu, 0);
 -			return 1;
 -		}
 +	vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
 +		     (new_ctl & ~mask) | (cur_ctl & mask));
 +}
  
 -		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
 -			kvm_mmu_sync_roots(vcpu);
 -			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 -		}
 +/*
 + * Generate MSR_IA32_VMX_CR{0,4}_FIXED1 according to CPUID. Only set bits
 + * (indicating "allowed-1") if they are supported in the guest's CPUID.
 + */
 +static void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct kvm_cpuid_entry2 *entry;
  
 -		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 -			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].cr3)
 -			    == operand.pcid)
 -				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
 +	vmx->nested.msrs.cr0_fixed1 = 0xffffffff;
 +	vmx->nested.msrs.cr4_fixed1 = X86_CR4_PCE;
 +
 +#define cr4_fixed1_update(_cr4_mask, _reg, _cpuid_mask) do {		\
 +	if (entry && (entry->_reg & (_cpuid_mask)))			\
 +		vmx->nested.msrs.cr4_fixed1 |= (_cr4_mask);	\
 +} while (0)
  
 -		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);
 -		/*
 -		 * If neither the current cr3 nor any of the prev_roots use the
 -		 * given PCID, then nothing needs to be done here because a
 -		 * resync will happen anyway before switching to any other CR3.
 -		 */
 +	entry = kvm_find_cpuid_entry(vcpu, 0x1, 0);
 +	cr4_fixed1_update(X86_CR4_VME,        edx, bit(X86_FEATURE_VME));
 +	cr4_fixed1_update(X86_CR4_PVI,        edx, bit(X86_FEATURE_VME));
 +	cr4_fixed1_update(X86_CR4_TSD,        edx, bit(X86_FEATURE_TSC));
 +	cr4_fixed1_update(X86_CR4_DE,         edx, bit(X86_FEATURE_DE));
 +	cr4_fixed1_update(X86_CR4_PSE,        edx, bit(X86_FEATURE_PSE));
 +	cr4_fixed1_update(X86_CR4_PAE,        edx, bit(X86_FEATURE_PAE));
 +	cr4_fixed1_update(X86_CR4_MCE,        edx, bit(X86_FEATURE_MCE));
 +	cr4_fixed1_update(X86_CR4_PGE,        edx, bit(X86_FEATURE_PGE));
 +	cr4_fixed1_update(X86_CR4_OSFXSR,     edx, bit(X86_FEATURE_FXSR));
 +	cr4_fixed1_update(X86_CR4_OSXMMEXCPT, edx, bit(X86_FEATURE_XMM));
 +	cr4_fixed1_update(X86_CR4_VMXE,       ecx, bit(X86_FEATURE_VMX));
 +	cr4_fixed1_update(X86_CR4_SMXE,       ecx, bit(X86_FEATURE_SMX));
 +	cr4_fixed1_update(X86_CR4_PCIDE,      ecx, bit(X86_FEATURE_PCID));
 +	cr4_fixed1_update(X86_CR4_OSXSAVE,    ecx, bit(X86_FEATURE_XSAVE));
  
 -		return kvm_skip_emulated_instruction(vcpu);
 +	entry = kvm_find_cpuid_entry(vcpu, 0x7, 0);
 +	cr4_fixed1_update(X86_CR4_FSGSBASE,   ebx, bit(X86_FEATURE_FSGSBASE));
 +	cr4_fixed1_update(X86_CR4_SMEP,       ebx, bit(X86_FEATURE_SMEP));
 +	cr4_fixed1_update(X86_CR4_SMAP,       ebx, bit(X86_FEATURE_SMAP));
 +	cr4_fixed1_update(X86_CR4_PKE,        ecx, bit(X86_FEATURE_PKU));
 +	cr4_fixed1_update(X86_CR4_UMIP,       ecx, bit(X86_FEATURE_UMIP));
  
 -	case INVPCID_TYPE_ALL_NON_GLOBAL:
 -		/*
 -		 * Currently, KVM doesn't mark global entries in the shadow
 -		 * page tables, so a non-global flush just degenerates to a
 -		 * global flush. If needed, we could optimize this later by
 -		 * keeping track of global entries in shadow page tables.
 -		 */
 +#undef cr4_fixed1_update
 +}
  
 -		/* fall-through */
 -	case INVPCID_TYPE_ALL_INCL_GLOBAL:
 -		kvm_mmu_unload(vcpu);
 -		return kvm_skip_emulated_instruction(vcpu);
 +static void nested_vmx_entry_exit_ctls_update(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -	default:
 -		BUG(); /* We have already checked above that type <= 3 */
 +	if (kvm_mpx_supported()) {
 +		bool mpx_enabled = guest_cpuid_has(vcpu, X86_FEATURE_MPX);
 +
 +		if (mpx_enabled) {
 +			vmx->nested.msrs.entry_ctls_high |= VM_ENTRY_LOAD_BNDCFGS;
 +			vmx->nested.msrs.exit_ctls_high |= VM_EXIT_CLEAR_BNDCFGS;
 +		} else {
 +			vmx->nested.msrs.entry_ctls_high &= ~VM_ENTRY_LOAD_BNDCFGS;
 +			vmx->nested.msrs.exit_ctls_high &= ~VM_EXIT_CLEAR_BNDCFGS;
 +		}
  	}
  }
  
* Unmerged path arch/x86/kvm/vmx/vmx.c
