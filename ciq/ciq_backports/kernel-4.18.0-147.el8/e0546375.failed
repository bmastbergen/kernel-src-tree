mm, sched/numa: Remove remaining traces of NUMA rate-limiting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Srikar Dronamraju <srikar@linux.vnet.ibm.com>
commit e054637597ba36d3729ba6a3a3dd7aad8e2a3003
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/e0546375.failed

Remove the leftover pglist_data::numabalancing_migrate_lock and its
initialization, we stopped using this lock with:

  efaffc5e40ae ("mm, sched/numa: Remove rate-limiting of automatic NUMA balancing migration")

[ mingo: Rewrote the changelog. ]

	Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Linux-MM <linux-mm@kvack.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/1538824999-31230-1-git-send-email-srikar@linux.vnet.ibm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit e054637597ba36d3729ba6a3a3dd7aad8e2a3003)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
#	mm/page_alloc.c
diff --cc include/linux/mmzone.h
index 73525f33eaee,d4b0c79d2924..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -673,16 -668,6 +673,19 @@@ typedef struct pglist_data 
  	wait_queue_head_t kcompactd_wait;
  	struct task_struct *kcompactd;
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_NUMA_BALANCING
 +	/* Lock serializing the migrate rate limiting window */
 +	spinlock_t numabalancing_migrate_lock;
 +
 +	/* Rate limiting time interval */
 +	unsigned long numabalancing_migrate_next_window;
 +
 +	/* Number of pages migrated during the rate limiting time interval */
 +	unsigned long numabalancing_migrate_nr_pages;
 +#endif
++=======
++>>>>>>> e054637597ba (mm, sched/numa: Remove remaining traces of NUMA rate-limiting)
  	/*
  	 * This is a per-node reserve of pages that are not available
  	 * to userspace allocations.
diff --cc mm/page_alloc.c
index c76cd0f6c558,e2ef1c17942f..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -6218,6 -6193,72 +6218,75 @@@ static unsigned long __paginginit calc_
  	return PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ static void pgdat_init_split_queue(struct pglist_data *pgdat)
+ {
+ 	spin_lock_init(&pgdat->split_queue_lock);
+ 	INIT_LIST_HEAD(&pgdat->split_queue);
+ 	pgdat->split_queue_len = 0;
+ }
+ #else
+ static void pgdat_init_split_queue(struct pglist_data *pgdat) {}
+ #endif
+ 
+ #ifdef CONFIG_COMPACTION
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat)
+ {
+ 	init_waitqueue_head(&pgdat->kcompactd_wait);
+ }
+ #else
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}
+ #endif
+ 
+ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
+ {
+ 	pgdat_resize_init(pgdat);
+ 
+ 	pgdat_init_split_queue(pgdat);
+ 	pgdat_init_kcompactd(pgdat);
+ 
+ 	init_waitqueue_head(&pgdat->kswapd_wait);
+ 	init_waitqueue_head(&pgdat->pfmemalloc_wait);
+ 
+ 	pgdat_page_ext_init(pgdat);
+ 	spin_lock_init(&pgdat->lru_lock);
+ 	lruvec_init(node_lruvec(pgdat));
+ }
+ 
+ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,
+ 							unsigned long remaining_pages)
+ {
+ 	zone->managed_pages = remaining_pages;
+ 	zone_set_nid(zone, nid);
+ 	zone->name = zone_names[idx];
+ 	zone->zone_pgdat = NODE_DATA(nid);
+ 	spin_lock_init(&zone->lock);
+ 	zone_seqlock_init(zone);
+ 	zone_pcp_init(zone);
+ }
+ 
+ /*
+  * Set up the zone data structures
+  * - init pgdat internals
+  * - init all zones belonging to this node
+  *
+  * NOTE: this function is only called during memory hotplug
+  */
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ void __ref free_area_init_core_hotplug(int nid)
+ {
+ 	enum zone_type z;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 
+ 	pgdat_init_internals(pgdat);
+ 	for (z = 0; z < MAX_NR_ZONES; z++)
+ 		zone_init_internals(&pgdat->node_zones[z], z, nid, 0);
+ }
+ #endif
+ 
++>>>>>>> e054637597ba (mm, sched/numa: Remove remaining traces of NUMA rate-limiting)
  /*
   * Set up the zone data structures:
   *   - mark all pages reserved
* Unmerged path include/linux/mmzone.h
* Unmerged path mm/page_alloc.c
