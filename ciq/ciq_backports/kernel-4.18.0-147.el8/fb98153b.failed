IB/mlx5: Enforce DEVX privilege by firmware

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Yishai Hadas <yishaih@mellanox.com>
commit fb98153bbf28b627fe52f41e658ae39fa67d2684
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/fb98153b.failed

Enforce DEVX privilege by firmware, this enables future device
functionality without the need to make driver changes unless a new
privilege type will be introduced.

	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit fb98153bbf28b627fe52f41e658ae39fa67d2684)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/devx.c
index c7f8859c08ee,80053324dd31..000000000000
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@@ -24,28 -25,49 +24,44 @@@ struct devx_obj 
  	u32			dinbox[MLX5_MAX_DESTROY_INBOX_SIZE_DW];
  };
  
 -struct devx_umem {
 -	struct mlx5_core_dev		*mdev;
 -	struct ib_umem			*umem;
 -	u32				page_offset;
 -	int				page_shift;
 -	int				ncont;
 -	u32				dinlen;
 -	u32				dinbox[MLX5_ST_SZ_DW(general_obj_in_cmd_hdr)];
 -};
 -
 -struct devx_umem_reg_cmd {
 -	void				*in;
 -	u32				inlen;
 -	u32				out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)];
 -};
 -
 -static struct mlx5_ib_ucontext *
 -devx_ufile2uctx(const struct uverbs_attr_bundle *attrs)
 +static struct mlx5_ib_ucontext *devx_ufile2uctx(struct ib_uverbs_file *file)
  {
 -	return to_mucontext(ib_uverbs_get_ucontext(attrs));
 +	return to_mucontext(ib_uverbs_get_ucontext(file));
  }
  
++<<<<<<< HEAD
 +int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, struct mlx5_ib_ucontext *context)
++=======
+ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user)
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  {
  	u32 in[MLX5_ST_SZ_DW(create_uctx_in)] = {0};
  	u32 out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)] = {0};
  	u64 general_obj_types;
- 	void *hdr;
+ 	void *hdr, *uctx;
  	int err;
++<<<<<<< HEAD
++=======
+ 	u16 uid;
+ 	u32 cap = 0;
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  
  	hdr = MLX5_ADDR_OF(create_uctx_in, in, hdr);
+ 	uctx = MLX5_ADDR_OF(create_uctx_in, in, uctx);
  
  	general_obj_types = MLX5_CAP_GEN_64(dev->mdev, general_obj_types);
  	if (!(general_obj_types & MLX5_GENERAL_OBJ_TYPES_CAP_UCTX) ||
  	    !(general_obj_types & MLX5_GENERAL_OBJ_TYPES_CAP_UMEM))
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	if (!capable(CAP_NET_RAW))
 +		return -EPERM;
++=======
+ 	if (is_user && capable(CAP_NET_RAW) &&
+ 	    (MLX5_CAP_GEN(dev->mdev, uctx_cap) & MLX5_UCTX_CAP_RAW_TX))
+ 		cap |= MLX5_UCTX_CAP_RAW_TX;
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  
  	MLX5_SET(general_obj_in_cmd_hdr, hdr, opcode, MLX5_CMD_OP_CREATE_GENERAL_OBJECT);
  	MLX5_SET(general_obj_in_cmd_hdr, hdr, obj_type, MLX5_OBJ_TYPE_UCTX);
@@@ -122,6 -654,40 +139,43 @@@ static bool devx_is_general_cmd(const v
  
  	switch (opcode) {
  	case MLX5_CMD_OP_QUERY_HCA_CAP:
++<<<<<<< HEAD
++=======
+ 	case MLX5_CMD_OP_QUERY_HCA_VPORT_CONTEXT:
+ 		return true;
+ 	default:
+ 		return false;
+ 	}
+ }
+ 
+ static int devx_get_uid(struct mlx5_ib_ucontext *c, void *cmd_in)
+ {
+ 	if (devx_is_whitelist_cmd(cmd_in)) {
+ 		struct mlx5_ib_dev *dev;
+ 
+ 		if (c->devx_uid)
+ 			return c->devx_uid;
+ 
+ 		dev = to_mdev(c->ibucontext.device);
+ 		if (dev->devx_whitelist_uid)
+ 			return dev->devx_whitelist_uid;
+ 
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (!c->devx_uid)
+ 		return -EINVAL;
+ 
+ 	return c->devx_uid;
+ }
+ static bool devx_is_general_cmd(void *in)
+ {
+ 	u16 opcode = MLX5_GET(general_obj_in_cmd_hdr, in, opcode);
+ 
+ 	switch (opcode) {
+ 	case MLX5_CMD_OP_QUERY_HCA_CAP:
+ 	case MLX5_CMD_OP_QUERY_HCA_VPORT_CONTEXT:
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  	case MLX5_CMD_OP_QUERY_VPORT_STATE:
  	case MLX5_CMD_OP_QUERY_ADAPTER:
  	case MLX5_CMD_OP_QUERY_ISSI:
@@@ -428,47 -1073,374 +482,393 @@@ obj_free
  	return err;
  }
  
 -static int UVERBS_HANDLER(MLX5_IB_METHOD_DEVX_OBJ_MODIFY)(
 -	struct uverbs_attr_bundle *attrs)
 -{
 -	void *cmd_in = uverbs_attr_get_alloced_ptr(attrs, MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_IN);
 -	int cmd_out_len = uverbs_attr_get_len(attrs,
 -					MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_OUT);
 -	struct ib_uobject *uobj = uverbs_attr_get_uobject(attrs,
 -							  MLX5_IB_ATTR_DEVX_OBJ_MODIFY_HANDLE);
 -	struct mlx5_ib_ucontext *c = to_mucontext(uobj->context);
 -	struct mlx5_ib_dev *mdev = to_mdev(uobj->context->device);
 -	void *cmd_out;
 -	int err;
 -	int uid;
 -
 -	uid = devx_get_uid(c, cmd_in);
 -	if (uid < 0)
 -		return uid;
 -
 -	if (!devx_is_obj_modify_cmd(cmd_in))
 -		return -EINVAL;
 -
 -	if (!devx_is_valid_obj_id(uobj, cmd_in))
 -		return -EINVAL;
 -
 -	cmd_out = uverbs_zalloc(attrs, cmd_out_len);
 -	if (IS_ERR(cmd_out))
 -		return PTR_ERR(cmd_out);
 -
 +static DECLARE_UVERBS_NAMED_METHOD(MLX5_IB_METHOD_DEVX_OTHER,
 +	&UVERBS_ATTR_PTR_IN_SZ(MLX5_IB_ATTR_DEVX_OTHER_CMD_IN,
 +			       UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
 +			       UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY |
 +					UVERBS_ATTR_SPEC_F_MIN_SZ_OR_ZERO |
 +					UVERBS_ATTR_SPEC_F_ALLOC_AND_COPY)),
 +	&UVERBS_ATTR_PTR_OUT_SZ(MLX5_IB_ATTR_DEVX_OTHER_CMD_OUT,
 +				UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
 +				UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY |
 +					 UVERBS_ATTR_SPEC_F_MIN_SZ_OR_ZERO))
 +);
 +
 +static DECLARE_UVERBS_NAMED_METHOD(MLX5_IB_METHOD_DEVX_OBJ_CREATE,
 +	&UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_CREATE_HANDLE,
 +			 MLX5_IB_OBJECT_DEVX_OBJ,
 +			 UVERBS_ACCESS_NEW,
 +			 UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&UVERBS_ATTR_PTR_IN_SZ(MLX5_IB_ATTR_DEVX_OBJ_CREATE_CMD_IN,
 +			       UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
 +			       UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY |
 +					UVERBS_ATTR_SPEC_F_MIN_SZ_OR_ZERO |
 +					UVERBS_ATTR_SPEC_F_ALLOC_AND_COPY)),
 +	&UVERBS_ATTR_PTR_OUT_SZ(MLX5_IB_ATTR_DEVX_OBJ_CREATE_CMD_OUT,
 +				UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
 +				UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY |
 +					 UVERBS_ATTR_SPEC_F_MIN_SZ_OR_ZERO)));
 +
 +static DECLARE_UVERBS_NAMED_METHOD(MLX5_IB_METHOD_DEVX_OBJ_DESTROY,
 +	&UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_DESTROY_HANDLE,
 +			 MLX5_IB_OBJECT_DEVX_OBJ,
 +			 UVERBS_ACCESS_DESTROY,
 +			 UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)));
 +
 +static DECLARE_UVERBS_GLOBAL_METHODS(MLX5_IB_OBJECT_DEVX,
 +	&UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OTHER));
 +
 +static DECLARE_UVERBS_NAMED_OBJECT(MLX5_IB_OBJECT_DEVX_OBJ,
 +	&UVERBS_TYPE_ALLOC_IDR(0, devx_obj_cleanup),
 +		&UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_CREATE),
 +		&UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_DESTROY));
 +
++<<<<<<< HEAD
 +static DECLARE_UVERBS_OBJECT_TREE(devx_objects,
 +	&UVERBS_OBJECT(MLX5_IB_OBJECT_DEVX),
 +	&UVERBS_OBJECT(MLX5_IB_OBJECT_DEVX_OBJ));
++=======
+ 	MLX5_SET(general_obj_in_cmd_hdr, cmd_in, uid, uid);
+ 	devx_set_umem_valid(cmd_in);
+ 
+ 	err = mlx5_cmd_exec(mdev->mdev, cmd_in,
+ 			    uverbs_attr_get_len(attrs, MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_IN),
+ 			    cmd_out, cmd_out_len);
+ 	if (err)
+ 		return err;
+ 
+ 	return uverbs_copy_to(attrs, MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_OUT,
+ 			      cmd_out, cmd_out_len);
+ }
+ 
+ static int UVERBS_HANDLER(MLX5_IB_METHOD_DEVX_OBJ_QUERY)(
+ 	struct uverbs_attr_bundle *attrs)
+ {
+ 	void *cmd_in = uverbs_attr_get_alloced_ptr(attrs, MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_IN);
+ 	int cmd_out_len = uverbs_attr_get_len(attrs,
+ 					      MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_OUT);
+ 	struct ib_uobject *uobj = uverbs_attr_get_uobject(attrs,
+ 							  MLX5_IB_ATTR_DEVX_OBJ_QUERY_HANDLE);
+ 	struct mlx5_ib_ucontext *c = to_mucontext(uobj->context);
+ 	void *cmd_out;
+ 	int err;
+ 	int uid;
+ 	struct mlx5_ib_dev *mdev = to_mdev(uobj->context->device);
+ 
+ 	uid = devx_get_uid(c, cmd_in);
+ 	if (uid < 0)
+ 		return uid;
+ 
+ 	if (!devx_is_obj_query_cmd(cmd_in))
+ 		return -EINVAL;
+ 
+ 	if (!devx_is_valid_obj_id(uobj, cmd_in))
+ 		return -EINVAL;
+ 
+ 	cmd_out = uverbs_zalloc(attrs, cmd_out_len);
+ 	if (IS_ERR(cmd_out))
+ 		return PTR_ERR(cmd_out);
+ 
+ 	MLX5_SET(general_obj_in_cmd_hdr, cmd_in, uid, uid);
+ 	err = mlx5_cmd_exec(mdev->mdev, cmd_in,
+ 			    uverbs_attr_get_len(attrs, MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_IN),
+ 			    cmd_out, cmd_out_len);
+ 	if (err)
+ 		return err;
+ 
+ 	return uverbs_copy_to(attrs, MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_OUT,
+ 			      cmd_out, cmd_out_len);
+ }
+ 
+ static int devx_umem_get(struct mlx5_ib_dev *dev, struct ib_ucontext *ucontext,
+ 			 struct uverbs_attr_bundle *attrs,
+ 			 struct devx_umem *obj)
+ {
+ 	u64 addr;
+ 	size_t size;
+ 	u32 access;
+ 	int npages;
+ 	int err;
+ 	u32 page_mask;
+ 
+ 	if (uverbs_copy_from(&addr, attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_ADDR) ||
+ 	    uverbs_copy_from(&size, attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_LEN))
+ 		return -EFAULT;
+ 
+ 	err = uverbs_get_flags32(&access, attrs,
+ 				 MLX5_IB_ATTR_DEVX_UMEM_REG_ACCESS,
+ 				 IB_ACCESS_SUPPORTED);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ib_check_mr_access(access);
+ 	if (err)
+ 		return err;
+ 
+ 	obj->umem = ib_umem_get(ucontext, addr, size, access, 0);
+ 	if (IS_ERR(obj->umem))
+ 		return PTR_ERR(obj->umem);
+ 
+ 	mlx5_ib_cont_pages(obj->umem, obj->umem->address,
+ 			   MLX5_MKEY_PAGE_SHIFT_MASK, &npages,
+ 			   &obj->page_shift, &obj->ncont, NULL);
+ 
+ 	if (!npages) {
+ 		ib_umem_release(obj->umem);
+ 		return -EINVAL;
+ 	}
+ 
+ 	page_mask = (1 << obj->page_shift) - 1;
+ 	obj->page_offset = obj->umem->address & page_mask;
+ 
+ 	return 0;
+ }
+ 
+ static int devx_umem_reg_cmd_alloc(struct uverbs_attr_bundle *attrs,
+ 				   struct devx_umem *obj,
+ 				   struct devx_umem_reg_cmd *cmd)
+ {
+ 	cmd->inlen = MLX5_ST_SZ_BYTES(create_umem_in) +
+ 		    (MLX5_ST_SZ_BYTES(mtt) * obj->ncont);
+ 	cmd->in = uverbs_zalloc(attrs, cmd->inlen);
+ 	return PTR_ERR_OR_ZERO(cmd->in);
+ }
+ 
+ static void devx_umem_reg_cmd_build(struct mlx5_ib_dev *dev,
+ 				    struct devx_umem *obj,
+ 				    struct devx_umem_reg_cmd *cmd)
+ {
+ 	void *umem;
+ 	__be64 *mtt;
+ 
+ 	umem = MLX5_ADDR_OF(create_umem_in, cmd->in, umem);
+ 	mtt = (__be64 *)MLX5_ADDR_OF(umem, umem, mtt);
+ 
+ 	MLX5_SET(general_obj_in_cmd_hdr, cmd->in, opcode, MLX5_CMD_OP_CREATE_GENERAL_OBJECT);
+ 	MLX5_SET(general_obj_in_cmd_hdr, cmd->in, obj_type, MLX5_OBJ_TYPE_UMEM);
+ 	MLX5_SET64(umem, umem, num_of_mtt, obj->ncont);
+ 	MLX5_SET(umem, umem, log_page_size, obj->page_shift -
+ 					    MLX5_ADAPTER_PAGE_SHIFT);
+ 	MLX5_SET(umem, umem, page_offset, obj->page_offset);
+ 	mlx5_ib_populate_pas(dev, obj->umem, obj->page_shift, mtt,
+ 			     (obj->umem->writable ? MLX5_IB_MTT_WRITE : 0) |
+ 			     MLX5_IB_MTT_READ);
+ }
+ 
+ static int UVERBS_HANDLER(MLX5_IB_METHOD_DEVX_UMEM_REG)(
+ 	struct uverbs_attr_bundle *attrs)
+ {
+ 	struct devx_umem_reg_cmd cmd;
+ 	struct devx_umem *obj;
+ 	struct ib_uobject *uobj = uverbs_attr_get_uobject(
+ 		attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_HANDLE);
+ 	u32 obj_id;
+ 	struct mlx5_ib_ucontext *c = to_mucontext(uobj->context);
+ 	struct mlx5_ib_dev *dev = to_mdev(c->ibucontext.device);
+ 	int err;
+ 
+ 	if (!c->devx_uid)
+ 		return -EINVAL;
+ 
+ 	obj = kzalloc(sizeof(struct devx_umem), GFP_KERNEL);
+ 	if (!obj)
+ 		return -ENOMEM;
+ 
+ 	err = devx_umem_get(dev, &c->ibucontext, attrs, obj);
+ 	if (err)
+ 		goto err_obj_free;
+ 
+ 	err = devx_umem_reg_cmd_alloc(attrs, obj, &cmd);
+ 	if (err)
+ 		goto err_umem_release;
+ 
+ 	devx_umem_reg_cmd_build(dev, obj, &cmd);
+ 
+ 	MLX5_SET(general_obj_in_cmd_hdr, cmd.in, uid, c->devx_uid);
+ 	err = mlx5_cmd_exec(dev->mdev, cmd.in, cmd.inlen, cmd.out,
+ 			    sizeof(cmd.out));
+ 	if (err)
+ 		goto err_umem_release;
+ 
+ 	obj->mdev = dev->mdev;
+ 	uobj->object = obj;
+ 	devx_obj_build_destroy_cmd(cmd.in, cmd.out, obj->dinbox, &obj->dinlen, &obj_id);
+ 	err = uverbs_copy_to(attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_OUT_ID, &obj_id, sizeof(obj_id));
+ 	if (err)
+ 		goto err_umem_destroy;
+ 
+ 	return 0;
+ 
+ err_umem_destroy:
+ 	mlx5_cmd_exec(obj->mdev, obj->dinbox, obj->dinlen, cmd.out, sizeof(cmd.out));
+ err_umem_release:
+ 	ib_umem_release(obj->umem);
+ err_obj_free:
+ 	kfree(obj);
+ 	return err;
+ }
+ 
+ static int devx_umem_cleanup(struct ib_uobject *uobject,
+ 			     enum rdma_remove_reason why)
+ {
+ 	struct devx_umem *obj = uobject->object;
+ 	u32 out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)];
+ 	int err;
+ 
+ 	err = mlx5_cmd_exec(obj->mdev, obj->dinbox, obj->dinlen, out, sizeof(out));
+ 	if (ib_is_destroy_retryable(err, why, uobject))
+ 		return err;
+ 
+ 	ib_umem_release(obj->umem);
+ 	kfree(obj);
+ 	return 0;
+ }
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_UMEM_REG,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_UMEM_REG_HANDLE,
+ 			MLX5_IB_OBJECT_DEVX_UMEM,
+ 			UVERBS_ACCESS_NEW,
+ 			UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_DEVX_UMEM_REG_ADDR,
+ 			   UVERBS_ATTR_TYPE(u64),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_DEVX_UMEM_REG_LEN,
+ 			   UVERBS_ATTR_TYPE(u64),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_FLAGS_IN(MLX5_IB_ATTR_DEVX_UMEM_REG_ACCESS,
+ 			     enum ib_access_flags),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_DEVX_UMEM_REG_OUT_ID,
+ 			    UVERBS_ATTR_TYPE(u32),
+ 			    UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD_DESTROY(
+ 	MLX5_IB_METHOD_DEVX_UMEM_DEREG,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_UMEM_DEREG_HANDLE,
+ 			MLX5_IB_OBJECT_DEVX_UMEM,
+ 			UVERBS_ACCESS_DESTROY,
+ 			UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_QUERY_EQN,
+ 	UVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_DEVX_QUERY_EQN_USER_VEC,
+ 			   UVERBS_ATTR_TYPE(u32),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_DEVX_QUERY_EQN_DEV_EQN,
+ 			    UVERBS_ATTR_TYPE(u32),
+ 			    UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_QUERY_UAR,
+ 	UVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_DEVX_QUERY_UAR_USER_IDX,
+ 			   UVERBS_ATTR_TYPE(u32),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_DEVX_QUERY_UAR_DEV_IDX,
+ 			    UVERBS_ATTR_TYPE(u32),
+ 			    UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_OTHER,
+ 	UVERBS_ATTR_PTR_IN(
+ 		MLX5_IB_ATTR_DEVX_OTHER_CMD_IN,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
+ 		UA_MANDATORY,
+ 		UA_ALLOC_AND_COPY),
+ 	UVERBS_ATTR_PTR_OUT(
+ 		MLX5_IB_ATTR_DEVX_OTHER_CMD_OUT,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
+ 		UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_OBJ_CREATE,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_CREATE_HANDLE,
+ 			MLX5_IB_OBJECT_DEVX_OBJ,
+ 			UVERBS_ACCESS_NEW,
+ 			UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(
+ 		MLX5_IB_ATTR_DEVX_OBJ_CREATE_CMD_IN,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
+ 		UA_MANDATORY,
+ 		UA_ALLOC_AND_COPY),
+ 	UVERBS_ATTR_PTR_OUT(
+ 		MLX5_IB_ATTR_DEVX_OBJ_CREATE_CMD_OUT,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
+ 		UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD_DESTROY(
+ 	MLX5_IB_METHOD_DEVX_OBJ_DESTROY,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_DESTROY_HANDLE,
+ 			MLX5_IB_OBJECT_DEVX_OBJ,
+ 			UVERBS_ACCESS_DESTROY,
+ 			UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_OBJ_MODIFY,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_MODIFY_HANDLE,
+ 			UVERBS_IDR_ANY_OBJECT,
+ 			UVERBS_ACCESS_WRITE,
+ 			UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(
+ 		MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_IN,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
+ 		UA_MANDATORY,
+ 		UA_ALLOC_AND_COPY),
+ 	UVERBS_ATTR_PTR_OUT(
+ 		MLX5_IB_ATTR_DEVX_OBJ_MODIFY_CMD_OUT,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
+ 		UA_MANDATORY));
+ 
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	MLX5_IB_METHOD_DEVX_OBJ_QUERY,
+ 	UVERBS_ATTR_IDR(MLX5_IB_ATTR_DEVX_OBJ_QUERY_HANDLE,
+ 			UVERBS_IDR_ANY_OBJECT,
+ 			UVERBS_ACCESS_READ,
+ 			UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(
+ 		MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_IN,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_in_cmd_hdr)),
+ 		UA_MANDATORY,
+ 		UA_ALLOC_AND_COPY),
+ 	UVERBS_ATTR_PTR_OUT(
+ 		MLX5_IB_ATTR_DEVX_OBJ_QUERY_CMD_OUT,
+ 		UVERBS_ATTR_MIN_SIZE(MLX5_ST_SZ_BYTES(general_obj_out_cmd_hdr)),
+ 		UA_MANDATORY));
+ 
+ DECLARE_UVERBS_GLOBAL_METHODS(MLX5_IB_OBJECT_DEVX,
+ 			      &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OTHER),
+ 			      &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_QUERY_UAR),
+ 			      &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_QUERY_EQN));
+ 
+ DECLARE_UVERBS_NAMED_OBJECT(MLX5_IB_OBJECT_DEVX_OBJ,
+ 			    UVERBS_TYPE_ALLOC_IDR(devx_obj_cleanup),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_CREATE),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_DESTROY),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_MODIFY),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_OBJ_QUERY));
+ 
+ DECLARE_UVERBS_NAMED_OBJECT(MLX5_IB_OBJECT_DEVX_UMEM,
+ 			    UVERBS_TYPE_ALLOC_IDR(devx_umem_cleanup),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_UMEM_REG),
+ 			    &UVERBS_METHOD(MLX5_IB_METHOD_DEVX_UMEM_DEREG));
+ 
+ static bool devx_is_supported(struct ib_device *device)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(device);
+ 
+ 	return !dev->rep && MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &
+ 				    MLX5_GENERAL_OBJ_TYPES_CAP_UCTX;
+ }
+ 
+ const struct uapi_definition mlx5_ib_devx_defs[] = {
+ 	UAPI_DEF_CHAIN_OBJ_TREE_NAMED(
+ 		MLX5_IB_OBJECT_DEVX,
+ 		UAPI_DEF_IS_OBJ_SUPPORTED(devx_is_supported)),
+ 	UAPI_DEF_CHAIN_OBJ_TREE_NAMED(
+ 		MLX5_IB_OBJECT_DEVX_OBJ,
+ 		UAPI_DEF_IS_OBJ_SUPPORTED(devx_is_supported)),
+ 	UAPI_DEF_CHAIN_OBJ_TREE_NAMED(
+ 		MLX5_IB_OBJECT_DEVX_UMEM,
+ 		UAPI_DEF_IS_OBJ_SUPPORTED(devx_is_supported)),
+ 	{},
+ };
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
diff --cc drivers/infiniband/hw/mlx5/main.c
index 72ce00f38f1d,2b09e6896e5a..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1720,21 -1762,17 +1720,33 @@@ static struct ib_ucontext *mlx5_ib_allo
  	context->ibucontext.invalidate_range = &mlx5_ib_invalidate_range;
  #endif
  
++<<<<<<< HEAD
 +	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn);
++=======
+ 	if (req.flags & MLX5_IB_ALLOC_UCTX_DEVX) {
+ 		err = mlx5_ib_devx_create(dev, true);
+ 		if (err < 0)
+ 			goto out_uars;
+ 		context->devx_uid = err;
+ 	}
+ 
+ 	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn,
+ 					     context->devx_uid);
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  	if (err)
 -		goto out_devx;
 +		goto out_uars;
 +
 +	if (req.flags & MLX5_IB_ALLOC_UCTX_DEVX) {
 +		/* Block DEVX on Infiniband as of SELinux */
 +		if (mlx5_ib_port_link_layer(ibdev, 1) != IB_LINK_LAYER_ETHERNET) {
 +			err = -EPERM;
 +			goto out_td;
 +		}
 +
 +		err = mlx5_ib_devx_create(dev, context);
 +		if (err)
 +			goto out_td;
 +	}
  
  	if (MLX5_CAP_GEN(dev->mdev, dump_fill_mkey)) {
  		err = mlx5_cmd_dump_fill_mkey(dev->mdev, &dump_fill_mkey);
@@@ -5897,6 -6218,34 +5909,37 @@@ static void mlx5_ib_stage_rep_reg_clean
  	mlx5_ib_unregister_vport_reps(dev);
  }
  
++<<<<<<< HEAD
++=======
+ static int mlx5_ib_stage_dev_notifier_init(struct mlx5_ib_dev *dev)
+ {
+ 	dev->mdev_events.notifier_call = mlx5_ib_event;
+ 	mlx5_notifier_register(dev->mdev, &dev->mdev_events);
+ 	return 0;
+ }
+ 
+ static void mlx5_ib_stage_dev_notifier_cleanup(struct mlx5_ib_dev *dev)
+ {
+ 	mlx5_notifier_unregister(dev->mdev, &dev->mdev_events);
+ }
+ 
+ static int mlx5_ib_stage_devx_init(struct mlx5_ib_dev *dev)
+ {
+ 	int uid;
+ 
+ 	uid = mlx5_ib_devx_create(dev, false);
+ 	if (uid > 0)
+ 		dev->devx_whitelist_uid = uid;
+ 
+ 	return 0;
+ }
+ static void mlx5_ib_stage_devx_cleanup(struct mlx5_ib_dev *dev)
+ {
+ 	if (dev->devx_whitelist_uid)
+ 		mlx5_ib_devx_destroy(dev, dev->devx_whitelist_uid);
+ }
+ 
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  void __mlx5_ib_remove(struct mlx5_ib_dev *dev,
  		      const struct mlx5_ib_profile *profile,
  		      int stage)
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index ceae1f16ebc9,4d33965369cc..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -1214,16 -1268,34 +1214,47 @@@ void mlx5_ib_put_native_port_mdev(struc
  				  u8 port_num);
  
  #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
++<<<<<<< HEAD
 +int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 +			struct mlx5_ib_ucontext *context);
 +void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 +			  struct mlx5_ib_ucontext *context);
 +#else
 +static inline int
 +mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 +		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
 +static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 +					struct mlx5_ib_ucontext *context) {}
++=======
+ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);
+ void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
+ const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
+ extern const struct uapi_definition mlx5_ib_devx_defs[];
+ extern const struct uapi_definition mlx5_ib_flow_defs[];
+ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
+ 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+ 	struct mlx5_flow_act *flow_act, u32 counter_id,
+ 	void *cmd_in, int inlen, int dest_id, int dest_type);
+ bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
+ bool mlx5_ib_devx_is_flow_counter(void *obj, u32 *counter_id);
+ int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
+ void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
+ #else
+ static inline int
+ mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+ 			   bool is_user) { return -EOPNOTSUPP; }
+ static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
+ static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
+ 					     int *dest_type)
+ {
+ 	return false;
+ }
+ static inline void
+ mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction)
+ {
+ 	return;
+ };
++>>>>>>> fb98153bbf28 (IB/mlx5: Enforce DEVX privilege by firmware)
  #endif
  static inline void init_query_mad(struct ib_smp *mad)
  {
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
