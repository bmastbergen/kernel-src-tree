KVM/VMX: Add hv tlb range flush support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Lan Tianyu <Tianyu.Lan@microsoft.com>
commit 1f3a3e46cc49e81ab84d32710d3dae30697753a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/1f3a3e46.failed

This patch is to register tlb_remote_flush_with_range callback with
hv tlb range flush interface.

	Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 1f3a3e46cc49e81ab84d32710d3dae30697753a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index a6572b4b59f2,d1c818a877df..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -1444,125 -401,57 +1444,152 @@@ DEFINE_STATIC_KEY_FALSE(enable_evmcs)
  static bool __read_mostly enlightened_vmcs = true;
  module_param(enlightened_vmcs, bool, 0444);
  
 -/* check_ept_pointer() should be under protection of ept_pointer_lock. */
 -static void check_ept_pointer_match(struct kvm *kvm)
 +static inline void evmcs_write64(unsigned long field, u64 value)
  {
 -	struct kvm_vcpu *vcpu;
 -	u64 tmp_eptp = INVALID_PAGE;
 -	int i;
 +	u16 clean_field;
 +	int offset = get_evmcs_offset(field, &clean_field);
  
 -	kvm_for_each_vcpu(i, vcpu, kvm) {
 -		if (!VALID_PAGE(tmp_eptp)) {
 -			tmp_eptp = to_vmx(vcpu)->ept_pointer;
 -		} else if (tmp_eptp != to_vmx(vcpu)->ept_pointer) {
 -			to_kvm_vmx(kvm)->ept_pointers_match
 -				= EPT_POINTERS_MISMATCH;
 -			return;
 -		}
 -	}
 +	if (offset < 0)
 +		return;
  
 -	to_kvm_vmx(kvm)->ept_pointers_match = EPT_POINTERS_MATCH;
 +	*(u64 *)((char *)current_evmcs + offset) = value;
 +
 +	current_evmcs->hv_clean_fields &= ~clean_field;
  }
  
 -int kvm_fill_hv_flush_list_func(struct hv_guest_mapping_flush_list *flush,
 -		void *data)
 +static inline void evmcs_write32(unsigned long field, u32 value)
  {
 -	struct kvm_tlb_range *range = data;
 +	u16 clean_field;
 +	int offset = get_evmcs_offset(field, &clean_field);
  
 -	return hyperv_fill_flush_guest_mapping_list(flush, range->start_gfn,
 -			range->pages);
 +	if (offset < 0)
 +		return;
 +
 +	*(u32 *)((char *)current_evmcs + offset) = value;
 +	current_evmcs->hv_clean_fields &= ~clean_field;
  }
  
 -static inline int __hv_remote_flush_tlb_with_range(struct kvm *kvm,
 +static inline void evmcs_write16(unsigned long field, u16 value)
 +{
 +	u16 clean_field;
 +	int offset = get_evmcs_offset(field, &clean_field);
 +
 +	if (offset < 0)
 +		return;
 +
 +	*(u16 *)((char *)current_evmcs + offset) = value;
 +	current_evmcs->hv_clean_fields &= ~clean_field;
 +}
 +
 +static inline u64 evmcs_read64(unsigned long field)
 +{
 +	int offset = get_evmcs_offset(field, NULL);
 +
 +	if (offset < 0)
 +		return 0;
 +
 +	return *(u64 *)((char *)current_evmcs + offset);
 +}
 +
 +static inline u32 evmcs_read32(unsigned long field)
 +{
 +	int offset = get_evmcs_offset(field, NULL);
 +
 +	if (offset < 0)
 +		return 0;
 +
 +	return *(u32 *)((char *)current_evmcs + offset);
 +}
 +
 +static inline u16 evmcs_read16(unsigned long field)
 +{
 +	int offset = get_evmcs_offset(field, NULL);
 +
 +	if (offset < 0)
 +		return 0;
 +
 +	return *(u16 *)((char *)current_evmcs + offset);
 +}
 +
 +static inline void evmcs_touch_msr_bitmap(void)
 +{
 +	if (unlikely(!current_evmcs))
 +		return;
 +
 +	if (current_evmcs->hv_enlightenments_control.msr_bitmap)
 +		current_evmcs->hv_clean_fields &=
 +			~HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP;
 +}
 +
 +static void evmcs_load(u64 phys_addr)
 +{
 +	struct hv_vp_assist_page *vp_ap =
 +		hv_get_vp_assist_page(smp_processor_id());
 +
 +	vp_ap->current_nested_vmcs = phys_addr;
 +	vp_ap->enlighten_vmentry = 1;
 +}
 +
 +static void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)
 +{
 +	vmcs_conf->pin_based_exec_ctrl &= ~EVMCS1_UNSUPPORTED_PINCTRL;
 +	vmcs_conf->cpu_based_2nd_exec_ctrl &= ~EVMCS1_UNSUPPORTED_2NDEXEC;
 +
 +	vmcs_conf->vmexit_ctrl &= ~EVMCS1_UNSUPPORTED_VMEXIT_CTRL;
 +	vmcs_conf->vmentry_ctrl &= ~EVMCS1_UNSUPPORTED_VMENTRY_CTRL;
 +
 +}
 +
 +/* check_ept_pointer() should be under protection of ept_pointer_lock. */
 +static void check_ept_pointer_match(struct kvm *kvm)
 +{
 +	struct kvm_vcpu *vcpu;
 +	u64 tmp_eptp = INVALID_PAGE;
 +	int i;
 +
 +	kvm_for_each_vcpu(i, vcpu, kvm) {
 +		if (!VALID_PAGE(tmp_eptp)) {
 +			tmp_eptp = to_vmx(vcpu)->ept_pointer;
 +		} else if (tmp_eptp != to_vmx(vcpu)->ept_pointer) {
 +			to_kvm_vmx(kvm)->ept_pointers_match
 +				= EPT_POINTERS_MISMATCH;
 +			return;
 +		}
 +	}
 +
 +	to_kvm_vmx(kvm)->ept_pointers_match = EPT_POINTERS_MATCH;
 +}
 +
- static int vmx_hv_remote_flush_tlb(struct kvm *kvm)
++int kvm_fill_hv_flush_list_func(struct hv_guest_mapping_flush_list *flush,
++		void *data)
++{
++	struct kvm_tlb_range *range = data;
++
++	return hyperv_fill_flush_guest_mapping_list(flush, range->start_gfn,
++			range->pages);
++}
++
++static inline int __hv_remote_flush_tlb_with_range(struct kvm *kvm,
+ 		struct kvm_vcpu *vcpu, struct kvm_tlb_range *range)
+ {
+ 	u64 ept_pointer = to_vmx(vcpu)->ept_pointer;
+ 
+ 	/*
+ 	 * FLUSH_GUEST_PHYSICAL_ADDRESS_SPACE hypercall needs address
+ 	 * of the base of EPT PML4 table, strip off EPT configuration
+ 	 * information.
+ 	 */
+ 	if (range)
+ 		return hyperv_flush_guest_mapping_range(ept_pointer & PAGE_MASK,
+ 				kvm_fill_hv_flush_list_func, (void *)range);
+ 	else
+ 		return hyperv_flush_guest_mapping(ept_pointer & PAGE_MASK);
+ }
+ 
+ static int hv_remote_flush_tlb_with_range(struct kvm *kvm,
+ 		struct kvm_tlb_range *range)
  {
 -	struct kvm_vcpu *vcpu;
 -	int ret = -ENOTSUPP, i;
 +	int ret;
  
  	spin_lock(&to_kvm_vmx(kvm)->ept_pointer_lock);
  
@@@ -1570,142 -459,27 +1597,163 @@@
  		check_ept_pointer_match(kvm);
  
  	if (to_kvm_vmx(kvm)->ept_pointers_match != EPT_POINTERS_MATCH) {
++<<<<<<< HEAD
 +		ret = -ENOTSUPP;
 +		goto out;
 +	}
 +
 +	/*
 +	 * FLUSH_GUEST_PHYSICAL_ADDRESS_SPACE hypercall needs the address of the
 +	 * base of EPT PML4 table, strip off EPT configuration information.
 +	 */
 +	ret = hyperv_flush_guest_mapping(
 +			to_vmx(kvm_get_vcpu(kvm, 0))->ept_pointer & PAGE_MASK);
++=======
+ 		kvm_for_each_vcpu(i, vcpu, kvm) {
+ 			/* If ept_pointer is invalid pointer, bypass flush request. */
+ 			if (VALID_PAGE(to_vmx(vcpu)->ept_pointer))
+ 				ret |= __hv_remote_flush_tlb_with_range(
+ 					kvm, vcpu, range);
+ 		}
+ 	} else {
+ 		ret = __hv_remote_flush_tlb_with_range(kvm,
+ 				kvm_get_vcpu(kvm, 0), range);
+ 	}
++>>>>>>> 1f3a3e46cc49 (KVM/VMX: Add hv tlb range flush support)
  
 +out:
  	spin_unlock(&to_kvm_vmx(kvm)->ept_pointer_lock);
  	return ret;
  }
++<<<<<<< HEAD
 +#else /* !IS_ENABLED(CONFIG_HYPERV) */
 +static inline void evmcs_write64(unsigned long field, u64 value) {}
 +static inline void evmcs_write32(unsigned long field, u32 value) {}
 +static inline void evmcs_write16(unsigned long field, u16 value) {}
 +static inline u64 evmcs_read64(unsigned long field) { return 0; }
 +static inline u32 evmcs_read32(unsigned long field) { return 0; }
 +static inline u16 evmcs_read16(unsigned long field) { return 0; }
 +static inline void evmcs_load(u64 phys_addr) {}
 +static inline void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf) {}
 +static inline void evmcs_touch_msr_bitmap(void) {}
++=======
+ static int hv_remote_flush_tlb(struct kvm *kvm)
+ {
+ 	return hv_remote_flush_tlb_with_range(kvm, NULL);
+ }
+ 
++>>>>>>> 1f3a3e46cc49 (KVM/VMX: Add hv tlb range flush support)
  #endif /* IS_ENABLED(CONFIG_HYPERV) */
  
 +static inline bool is_exception_n(u32 intr_info, u8 vector)
 +{
 +	return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |
 +			     INTR_INFO_VALID_MASK)) ==
 +		(INTR_TYPE_HARD_EXCEPTION | vector | INTR_INFO_VALID_MASK);
 +}
 +
 +static inline bool is_debug(u32 intr_info)
 +{
 +	return is_exception_n(intr_info, DB_VECTOR);
 +}
 +
 +static inline bool is_breakpoint(u32 intr_info)
 +{
 +	return is_exception_n(intr_info, BP_VECTOR);
 +}
 +
 +static inline bool is_page_fault(u32 intr_info)
 +{
 +	return is_exception_n(intr_info, PF_VECTOR);
 +}
 +
 +static inline bool is_invalid_opcode(u32 intr_info)
 +{
 +	return is_exception_n(intr_info, UD_VECTOR);
 +}
 +
 +static inline bool is_gp_fault(u32 intr_info)
 +{
 +	return is_exception_n(intr_info, GP_VECTOR);
 +}
 +
 +static inline bool is_machine_check(u32 intr_info)
 +{
 +	return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |
 +			     INTR_INFO_VALID_MASK)) ==
 +		(INTR_TYPE_HARD_EXCEPTION | MC_VECTOR | INTR_INFO_VALID_MASK);
 +}
 +
 +/* Undocumented: icebp/int1 */
 +static inline bool is_icebp(u32 intr_info)
 +{
 +	return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))
 +		== (INTR_TYPE_PRIV_SW_EXCEPTION | INTR_INFO_VALID_MASK);
 +}
 +
 +static inline bool cpu_has_load_ia32_efer(void)
 +{
 +	return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_EFER) &&
 +	       (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_EFER);
 +}
 +
 +static inline bool cpu_has_load_perf_global_ctrl(void)
 +{
 +	return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&
 +	       (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
 +}
 +
 +static inline bool cpu_has_vmx_msr_bitmap(void)
 +{
 +	return vmcs_config.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS;
 +}
 +
 +static inline bool cpu_has_vmx_tpr_shadow(void)
 +{
 +	return vmcs_config.cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW;
 +}
 +
 +static inline bool cpu_need_tpr_shadow(struct kvm_vcpu *vcpu)
 +{
 +	return cpu_has_vmx_tpr_shadow() && lapic_in_kernel(vcpu);
 +}
 +
 +static inline bool cpu_has_secondary_exec_ctrls(void)
 +{
 +	return vmcs_config.cpu_based_exec_ctrl &
 +		CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
 +}
 +
 +static inline bool cpu_has_vmx_virtualize_apic_accesses(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +}
 +
 +static inline bool cpu_has_vmx_virtualize_x2apic_mode(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
 +}
 +
 +static inline bool cpu_has_vmx_apic_register_virt(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_APIC_REGISTER_VIRT;
 +}
 +
 +static inline bool cpu_has_vmx_virtual_intr_delivery(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
 +}
 +
 +static inline bool cpu_has_vmx_encls_vmexit(void)
 +{
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_ENCLS_EXITING;
 +}
 +
  /*
   * Comment's format: document - errata name - stepping - processor name.
   * Refer from
* Unmerged path arch/x86/kvm/vmx/vmx.c
