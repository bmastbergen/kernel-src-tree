IB/rdmavt: Fix loopback send with invalidate ordering

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Mike Marciniszyn <mike.marciniszyn@intel.com>
commit 38bbc9f0381550d1d227fc57afa08436e36b32fc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/38bbc9f0.failed

The IBTA spec notes:

o9-5.2.1: For any HCA which supports SEND with Invalidate, upon receiving
an IETH, the Invalidate operation must not take place until after the
normal transport header validation checks have been successfully
completed.

The rdmavt loopback code does the validation after the invalidate.

Fix by relocating the operation specific logic for all SEND variants until
after the validity checks.

	Cc: <stable@vger.kernel.org> #v4.20+
	Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
	Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 38bbc9f0381550d1d227fc57afa08436e36b32fc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/sw/rdmavt/qp.c
diff --cc drivers/infiniband/sw/rdmavt/qp.c
index 7b45580bb62e,337365806261..000000000000
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@@ -2476,3 -2669,443 +2476,446 @@@ void rvt_qp_iter(struct rvt_dev_info *r
  	rcu_read_unlock();
  }
  EXPORT_SYMBOL(rvt_qp_iter);
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * This should be called with s_lock held.
+  */
+ void rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
+ 		       enum ib_wc_status status)
+ {
+ 	u32 old_last, last;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);
+ 
+ 	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_OR_FLUSH_SEND))
+ 		return;
+ 
+ 	last = qp->s_last;
+ 	old_last = last;
+ 	trace_rvt_qp_send_completion(qp, wqe, last);
+ 	if (++last >= qp->s_size)
+ 		last = 0;
+ 	trace_rvt_qp_send_completion(qp, wqe, last);
+ 	qp->s_last = last;
+ 	/* See post_send() */
+ 	barrier();
+ 	rvt_put_swqe(wqe);
+ 	if (qp->ibqp.qp_type == IB_QPT_UD ||
+ 	    qp->ibqp.qp_type == IB_QPT_SMI ||
+ 	    qp->ibqp.qp_type == IB_QPT_GSI)
+ 		atomic_dec(&ibah_to_rvtah(wqe->ud_wr.ah)->refcount);
+ 
+ 	rvt_qp_swqe_complete(qp,
+ 			     wqe,
+ 			     rdi->wc_opcode[wqe->wr.opcode],
+ 			     status);
+ 
+ 	if (qp->s_acked == old_last)
+ 		qp->s_acked = last;
+ 	if (qp->s_cur == old_last)
+ 		qp->s_cur = last;
+ 	if (qp->s_tail == old_last)
+ 		qp->s_tail = last;
+ 	if (qp->state == IB_QPS_SQD && last == qp->s_cur)
+ 		qp->s_draining = 0;
+ }
+ EXPORT_SYMBOL(rvt_send_complete);
+ 
+ /**
+  * rvt_copy_sge - copy data to SGE memory
+  * @qp: associated QP
+  * @ss: the SGE state
+  * @data: the data to copy
+  * @length: the length of the data
+  * @release: boolean to release MR
+  * @copy_last: do a separate copy of the last 8 bytes
+  */
+ void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
+ 		  void *data, u32 length,
+ 		  bool release, bool copy_last)
+ {
+ 	struct rvt_sge *sge = &ss->sge;
+ 	int i;
+ 	bool in_last = false;
+ 	bool cacheless_copy = false;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);
+ 	struct rvt_wss *wss = rdi->wss;
+ 	unsigned int sge_copy_mode = rdi->dparms.sge_copy_mode;
+ 
+ 	if (sge_copy_mode == RVT_SGE_COPY_CACHELESS) {
+ 		cacheless_copy = length >= PAGE_SIZE;
+ 	} else if (sge_copy_mode == RVT_SGE_COPY_ADAPTIVE) {
+ 		if (length >= PAGE_SIZE) {
+ 			/*
+ 			 * NOTE: this *assumes*:
+ 			 * o The first vaddr is the dest.
+ 			 * o If multiple pages, then vaddr is sequential.
+ 			 */
+ 			wss_insert(wss, sge->vaddr);
+ 			if (length >= (2 * PAGE_SIZE))
+ 				wss_insert(wss, (sge->vaddr + PAGE_SIZE));
+ 
+ 			cacheless_copy = wss_exceeds_threshold(wss);
+ 		} else {
+ 			wss_advance_clean_counter(wss);
+ 		}
+ 	}
+ 
+ 	if (copy_last) {
+ 		if (length > 8) {
+ 			length -= 8;
+ 		} else {
+ 			copy_last = false;
+ 			in_last = true;
+ 		}
+ 	}
+ 
+ again:
+ 	while (length) {
+ 		u32 len = rvt_get_sge_length(sge, length);
+ 
+ 		WARN_ON_ONCE(len == 0);
+ 		if (unlikely(in_last)) {
+ 			/* enforce byte transfer ordering */
+ 			for (i = 0; i < len; i++)
+ 				((u8 *)sge->vaddr)[i] = ((u8 *)data)[i];
+ 		} else if (cacheless_copy) {
+ 			cacheless_memcpy(sge->vaddr, data, len);
+ 		} else {
+ 			memcpy(sge->vaddr, data, len);
+ 		}
+ 		rvt_update_sge(ss, len, release);
+ 		data += len;
+ 		length -= len;
+ 	}
+ 
+ 	if (copy_last) {
+ 		copy_last = false;
+ 		in_last = true;
+ 		length = 8;
+ 		goto again;
+ 	}
+ }
+ EXPORT_SYMBOL(rvt_copy_sge);
+ 
+ /**
+  * ruc_loopback - handle UC and RC loopback requests
+  * @sqp: the sending QP
+  *
+  * This is called from rvt_do_send() to forward a WQE addressed to the same HFI
+  * Note that although we are single threaded due to the send engine, we still
+  * have to protect against post_send().  We don't have to worry about
+  * receive interrupts since this is a connected protocol and all packets
+  * will pass through here.
+  */
+ void rvt_ruc_loopback(struct rvt_qp *sqp)
+ {
+ 	struct rvt_ibport *rvp =  NULL;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(sqp->ibqp.device);
+ 	struct rvt_qp *qp;
+ 	struct rvt_swqe *wqe;
+ 	struct rvt_sge *sge;
+ 	unsigned long flags;
+ 	struct ib_wc wc;
+ 	u64 sdata;
+ 	atomic64_t *maddr;
+ 	enum ib_wc_status send_status;
+ 	bool release;
+ 	int ret;
+ 	bool copy_last = false;
+ 	int local_ops = 0;
+ 
+ 	rcu_read_lock();
+ 	rvp = rdi->ports[sqp->port_num - 1];
+ 
+ 	/*
+ 	 * Note that we check the responder QP state after
+ 	 * checking the requester's state.
+ 	 */
+ 
+ 	qp = rvt_lookup_qpn(ib_to_rvt(sqp->ibqp.device), rvp,
+ 			    sqp->remote_qpn);
+ 
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 
+ 	/* Return if we are already busy processing a work request. */
+ 	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
+ 	    !(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_OR_FLUSH_SEND))
+ 		goto unlock;
+ 
+ 	sqp->s_flags |= RVT_S_BUSY;
+ 
+ again:
+ 	if (sqp->s_last == READ_ONCE(sqp->s_head))
+ 		goto clr_busy;
+ 	wqe = rvt_get_swqe_ptr(sqp, sqp->s_last);
+ 
+ 	/* Return if it is not OK to start a new work request. */
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_NEXT_SEND_OK)) {
+ 		if (!(ib_rvt_state_ops[sqp->state] & RVT_FLUSH_SEND))
+ 			goto clr_busy;
+ 		/* We are in the error state, flush the work request. */
+ 		send_status = IB_WC_WR_FLUSH_ERR;
+ 		goto flush_send;
+ 	}
+ 
+ 	/*
+ 	 * We can rely on the entry not changing without the s_lock
+ 	 * being held until we update s_last.
+ 	 * We increment s_cur to indicate s_last is in progress.
+ 	 */
+ 	if (sqp->s_last == sqp->s_cur) {
+ 		if (++sqp->s_cur >= sqp->s_size)
+ 			sqp->s_cur = 0;
+ 	}
+ 	spin_unlock_irqrestore(&sqp->s_lock, flags);
+ 
+ 	if (!qp || !(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) ||
+ 	    qp->ibqp.qp_type != sqp->ibqp.qp_type) {
+ 		rvp->n_pkt_drops++;
+ 		/*
+ 		 * For RC, the requester would timeout and retry so
+ 		 * shortcut the timeouts and just signal too many retries.
+ 		 */
+ 		if (sqp->ibqp.qp_type == IB_QPT_RC)
+ 			send_status = IB_WC_RETRY_EXC_ERR;
+ 		else
+ 			send_status = IB_WC_SUCCESS;
+ 		goto serr;
+ 	}
+ 
+ 	memset(&wc, 0, sizeof(wc));
+ 	send_status = IB_WC_SUCCESS;
+ 
+ 	release = true;
+ 	sqp->s_sge.sge = wqe->sg_list[0];
+ 	sqp->s_sge.sg_list = wqe->sg_list + 1;
+ 	sqp->s_sge.num_sge = wqe->wr.num_sge;
+ 	sqp->s_len = wqe->length;
+ 	switch (wqe->wr.opcode) {
+ 	case IB_WR_REG_MR:
+ 		goto send_comp;
+ 
+ 	case IB_WR_LOCAL_INV:
+ 		if (!(wqe->wr.send_flags & RVT_SEND_COMPLETION_ONLY)) {
+ 			if (rvt_invalidate_rkey(sqp,
+ 						wqe->wr.ex.invalidate_rkey))
+ 				send_status = IB_WC_LOC_PROT_ERR;
+ 			local_ops = 1;
+ 		}
+ 		goto send_comp;
+ 
+ 	case IB_WR_SEND_WITH_INV:
+ 	case IB_WR_SEND_WITH_IMM:
+ 	case IB_WR_SEND:
+ 		ret = rvt_get_rwqe(qp, false);
+ 		if (ret < 0)
+ 			goto op_err;
+ 		if (!ret)
+ 			goto rnr_nak;
+ 		if (wqe->length > qp->r_len)
+ 			goto inv_err;
+ 		switch (wqe->wr.opcode) {
+ 		case IB_WR_SEND_WITH_INV:
+ 			if (!rvt_invalidate_rkey(qp,
+ 						 wqe->wr.ex.invalidate_rkey)) {
+ 				wc.wc_flags = IB_WC_WITH_INVALIDATE;
+ 				wc.ex.invalidate_rkey =
+ 					wqe->wr.ex.invalidate_rkey;
+ 			}
+ 			break;
+ 		case IB_WR_SEND_WITH_IMM:
+ 			wc.wc_flags = IB_WC_WITH_IMM;
+ 			wc.ex.imm_data = wqe->wr.ex.imm_data;
+ 			break;
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case IB_WR_RDMA_WRITE_WITH_IMM:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))
+ 			goto inv_err;
+ 		wc.wc_flags = IB_WC_WITH_IMM;
+ 		wc.ex.imm_data = wqe->wr.ex.imm_data;
+ 		ret = rvt_get_rwqe(qp, true);
+ 		if (ret < 0)
+ 			goto op_err;
+ 		if (!ret)
+ 			goto rnr_nak;
+ 		/* skip copy_last set and qp_access_flags recheck */
+ 		goto do_write;
+ 	case IB_WR_RDMA_WRITE:
+ 		copy_last = rvt_is_user_qp(qp);
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))
+ 			goto inv_err;
+ do_write:
+ 		if (wqe->length == 0)
+ 			break;
+ 		if (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, wqe->length,
+ 					  wqe->rdma_wr.remote_addr,
+ 					  wqe->rdma_wr.rkey,
+ 					  IB_ACCESS_REMOTE_WRITE)))
+ 			goto acc_err;
+ 		qp->r_sge.sg_list = NULL;
+ 		qp->r_sge.num_sge = 1;
+ 		qp->r_sge.total_len = wqe->length;
+ 		break;
+ 
+ 	case IB_WR_RDMA_READ:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))
+ 			goto inv_err;
+ 		if (unlikely(!rvt_rkey_ok(qp, &sqp->s_sge.sge, wqe->length,
+ 					  wqe->rdma_wr.remote_addr,
+ 					  wqe->rdma_wr.rkey,
+ 					  IB_ACCESS_REMOTE_READ)))
+ 			goto acc_err;
+ 		release = false;
+ 		sqp->s_sge.sg_list = NULL;
+ 		sqp->s_sge.num_sge = 1;
+ 		qp->r_sge.sge = wqe->sg_list[0];
+ 		qp->r_sge.sg_list = wqe->sg_list + 1;
+ 		qp->r_sge.num_sge = wqe->wr.num_sge;
+ 		qp->r_sge.total_len = wqe->length;
+ 		break;
+ 
+ 	case IB_WR_ATOMIC_CMP_AND_SWP:
+ 	case IB_WR_ATOMIC_FETCH_AND_ADD:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))
+ 			goto inv_err;
+ 		if (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, sizeof(u64),
+ 					  wqe->atomic_wr.remote_addr,
+ 					  wqe->atomic_wr.rkey,
+ 					  IB_ACCESS_REMOTE_ATOMIC)))
+ 			goto acc_err;
+ 		/* Perform atomic OP and save result. */
+ 		maddr = (atomic64_t *)qp->r_sge.sge.vaddr;
+ 		sdata = wqe->atomic_wr.compare_add;
+ 		*(u64 *)sqp->s_sge.sge.vaddr =
+ 			(wqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) ?
+ 			(u64)atomic64_add_return(sdata, maddr) - sdata :
+ 			(u64)cmpxchg((u64 *)qp->r_sge.sge.vaddr,
+ 				      sdata, wqe->atomic_wr.swap);
+ 		rvt_put_mr(qp->r_sge.sge.mr);
+ 		qp->r_sge.num_sge = 0;
+ 		goto send_comp;
+ 
+ 	default:
+ 		send_status = IB_WC_LOC_QP_OP_ERR;
+ 		goto serr;
+ 	}
+ 
+ 	sge = &sqp->s_sge.sge;
+ 	while (sqp->s_len) {
+ 		u32 len = rvt_get_sge_length(sge, sqp->s_len);
+ 
+ 		WARN_ON_ONCE(len == 0);
+ 		rvt_copy_sge(qp, &qp->r_sge, sge->vaddr,
+ 			     len, release, copy_last);
+ 		rvt_update_sge(&sqp->s_sge, len, !release);
+ 		sqp->s_len -= len;
+ 	}
+ 	if (release)
+ 		rvt_put_ss(&qp->r_sge);
+ 
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
+ 		goto send_comp;
+ 
+ 	if (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)
+ 		wc.opcode = IB_WC_RECV_RDMA_WITH_IMM;
+ 	else
+ 		wc.opcode = IB_WC_RECV;
+ 	wc.wr_id = qp->r_wr_id;
+ 	wc.status = IB_WC_SUCCESS;
+ 	wc.byte_len = wqe->length;
+ 	wc.qp = &qp->ibqp;
+ 	wc.src_qp = qp->remote_qpn;
+ 	wc.slid = rdma_ah_get_dlid(&qp->remote_ah_attr) & U16_MAX;
+ 	wc.sl = rdma_ah_get_sl(&qp->remote_ah_attr);
+ 	wc.port_num = 1;
+ 	/* Signal completion event if the solicited bit is set. */
+ 	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
+ 		     wqe->wr.send_flags & IB_SEND_SOLICITED);
+ 
+ send_comp:
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	rvp->n_loop_pkts++;
+ flush_send:
+ 	sqp->s_rnr_retry = sqp->s_rnr_retry_cnt;
+ 	rvt_send_complete(sqp, wqe, send_status);
+ 	if (local_ops) {
+ 		atomic_dec(&sqp->local_ops_pending);
+ 		local_ops = 0;
+ 	}
+ 	goto again;
+ 
+ rnr_nak:
+ 	/* Handle RNR NAK */
+ 	if (qp->ibqp.qp_type == IB_QPT_UC)
+ 		goto send_comp;
+ 	rvp->n_rnr_naks++;
+ 	/*
+ 	 * Note: we don't need the s_lock held since the BUSY flag
+ 	 * makes this single threaded.
+ 	 */
+ 	if (sqp->s_rnr_retry == 0) {
+ 		send_status = IB_WC_RNR_RETRY_EXC_ERR;
+ 		goto serr;
+ 	}
+ 	if (sqp->s_rnr_retry_cnt < 7)
+ 		sqp->s_rnr_retry--;
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_RECV_OK))
+ 		goto clr_busy;
+ 	rvt_add_rnr_timer(sqp, qp->r_min_rnr_timer <<
+ 				IB_AETH_CREDIT_SHIFT);
+ 	goto clr_busy;
+ 
+ op_err:
+ 	send_status = IB_WC_REM_OP_ERR;
+ 	wc.status = IB_WC_LOC_QP_OP_ERR;
+ 	goto err;
+ 
+ inv_err:
+ 	send_status =
+ 		sqp->ibqp.qp_type == IB_QPT_RC ?
+ 			IB_WC_REM_INV_REQ_ERR :
+ 			IB_WC_SUCCESS;
+ 	wc.status = IB_WC_LOC_QP_OP_ERR;
+ 	goto err;
+ 
+ acc_err:
+ 	send_status = IB_WC_REM_ACCESS_ERR;
+ 	wc.status = IB_WC_LOC_PROT_ERR;
+ err:
+ 	/* responder goes to error state */
+ 	rvt_rc_error(qp, wc.status);
+ 
+ serr:
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	rvt_send_complete(sqp, wqe, send_status);
+ 	if (sqp->ibqp.qp_type == IB_QPT_RC) {
+ 		int lastwqe = rvt_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
+ 
+ 		sqp->s_flags &= ~RVT_S_BUSY;
+ 		spin_unlock_irqrestore(&sqp->s_lock, flags);
+ 		if (lastwqe) {
+ 			struct ib_event ev;
+ 
+ 			ev.device = sqp->ibqp.device;
+ 			ev.element.qp = &sqp->ibqp;
+ 			ev.event = IB_EVENT_QP_LAST_WQE_REACHED;
+ 			sqp->ibqp.event_handler(&ev, sqp->ibqp.qp_context);
+ 		}
+ 		goto done;
+ 	}
+ clr_busy:
+ 	sqp->s_flags &= ~RVT_S_BUSY;
+ unlock:
+ 	spin_unlock_irqrestore(&sqp->s_lock, flags);
+ done:
+ 	rcu_read_unlock();
+ }
+ EXPORT_SYMBOL(rvt_ruc_loopback);
++>>>>>>> 38bbc9f03815 (IB/rdmavt: Fix loopback send with invalidate ordering)
* Unmerged path drivers/infiniband/sw/rdmavt/qp.c
