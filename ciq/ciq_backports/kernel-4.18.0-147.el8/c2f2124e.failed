dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Christoph Hellwig <hch@lst.de>
commit c2f2124e0d447ad02a41a92361b3734366797680
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/c2f2124e.failed

Only call into arch_dma_alloc if we require an uncached mapping,
and remove the parisc code manually doing normal cached
DMA_ATTR_NON_CONSISTENT allocations.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Helge Deller <deller@gmx.de> # parisc
(cherry picked from commit c2f2124e0d447ad02a41a92361b3734366797680)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/parisc/kernel/pci-dma.c
#	kernel/dma/direct.c
diff --cc arch/parisc/kernel/pci-dma.c
index 6df07ce4f3c2,ca35d9a76e50..000000000000
--- a/arch/parisc/kernel/pci-dma.c
+++ b/arch/parisc/kernel/pci-dma.c
@@@ -395,8 -394,8 +395,13 @@@ pcxl_dma_init(void
  
  __initcall(pcxl_dma_init);
  
++<<<<<<< HEAD
 +static void *pa11_dma_alloc(struct device *dev, size_t size,
 +		dma_addr_t *dma_handle, gfp_t flag, unsigned long attrs)
++=======
+ void *arch_dma_alloc(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
++>>>>>>> c2f2124e0d44 (dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code)
  {
  	unsigned long vaddr;
  	unsigned long paddr;
@@@ -405,7 -407,7 +413,11 @@@
  	order = get_order(size);
  	size = 1 << (order + PAGE_SHIFT);
  	vaddr = pcxl_alloc_range(size);
++<<<<<<< HEAD
 +	paddr = __get_free_pages(flag, order);
++=======
+ 	paddr = __get_free_pages(gfp | __GFP_ZERO, order);
++>>>>>>> c2f2124e0d44 (dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code)
  	flush_kernel_dcache_range(paddr, size);
  	paddr = __pa(paddr);
  	map_uncached_pages(vaddr, size, paddr);
@@@ -422,12 -424,14 +434,13 @@@
  	return (void *)vaddr;
  }
  
 -void arch_dma_free(struct device *dev, size_t size, void *vaddr,
++<<<<<<< HEAD
 +static void pa11_dma_free(struct device *dev, size_t size, void *vaddr,
  		dma_addr_t dma_handle, unsigned long attrs)
  {
 -	int order = get_order(size);
 -
 -	WARN_ON_ONCE(boot_cpu_data.cpu_type != pcxl2 &&
 -		     boot_cpu_data.cpu_type != pcxl);
 +	int order;
  
 +	order = get_order(size);
  	size = 1 << (order + PAGE_SHIFT);
  	unmap_uncached_pages((unsigned long)vaddr, size);
  	pcxl_free_range((unsigned long)vaddr, size);
@@@ -559,53 -456,3 +572,68 @@@ static void pa11_dma_cache_sync(struct 
  {
  	flush_kernel_dcache_range((unsigned long)vaddr, size);
  }
 +
 +const struct dma_map_ops pcxl_dma_ops = {
 +	.alloc =		pa11_dma_alloc,
 +	.free =			pa11_dma_free,
 +	.map_page =		pa11_dma_map_page,
 +	.unmap_page =		pa11_dma_unmap_page,
 +	.map_sg =		pa11_dma_map_sg,
 +	.unmap_sg =		pa11_dma_unmap_sg,
 +	.sync_single_for_cpu =	pa11_dma_sync_single_for_cpu,
 +	.sync_single_for_device = pa11_dma_sync_single_for_device,
 +	.sync_sg_for_cpu =	pa11_dma_sync_sg_for_cpu,
 +	.sync_sg_for_device =	pa11_dma_sync_sg_for_device,
 +	.cache_sync =		pa11_dma_cache_sync,
 +};
 +
 +static void *pcx_dma_alloc(struct device *dev, size_t size,
 +		dma_addr_t *dma_handle, gfp_t flag, unsigned long attrs)
 +{
 +	void *addr;
 +
 +	if ((attrs & DMA_ATTR_NON_CONSISTENT) == 0)
 +		return NULL;
 +
 +	addr = (void *)__get_free_pages(flag, get_order(size));
 +	if (addr)
 +		*dma_handle = (dma_addr_t)virt_to_phys(addr);
 +
 +	return addr;
 +}
 +
 +static void pcx_dma_free(struct device *dev, size_t size, void *vaddr,
 +		dma_addr_t iova, unsigned long attrs)
 +{
 +	free_pages((unsigned long)vaddr, get_order(size));
 +	return;
++=======
++void arch_dma_free(struct device *dev, size_t size, void *vaddr,
++		dma_addr_t dma_handle, unsigned long attrs)
++{
++	int order = get_order(size);
++
++	WARN_ON_ONCE(boot_cpu_data.cpu_type != pcxl2 &&
++		     boot_cpu_data.cpu_type != pcxl);
++
++	size = 1 << (order + PAGE_SHIFT);
++	unmap_uncached_pages((unsigned long)vaddr, size);
++	pcxl_free_range((unsigned long)vaddr, size);
++
++	free_pages((unsigned long)__va(dma_handle), order);
++>>>>>>> c2f2124e0d44 (dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code)
 +}
 +
 +const struct dma_map_ops pcx_dma_ops = {
 +	.alloc =		pcx_dma_alloc,
 +	.free =			pcx_dma_free,
 +	.map_page =		pa11_dma_map_page,
 +	.unmap_page =		pa11_dma_unmap_page,
 +	.map_sg =		pa11_dma_map_sg,
 +	.unmap_sg =		pa11_dma_unmap_sg,
 +	.sync_single_for_cpu =	pa11_dma_sync_single_for_cpu,
 +	.sync_single_for_device = pa11_dma_sync_single_for_device,
 +	.sync_sg_for_cpu =	pa11_dma_sync_sg_for_cpu,
 +	.sync_sg_for_device =	pa11_dma_sync_sg_for_device,
 +	.cache_sync =		pa11_dma_cache_sync,
 +};
diff --cc kernel/dma/direct.c
index 26bb7b9a7670,fc354f4f490b..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -180,7 -190,8 +180,12 @@@ void dma_direct_free_pages(struct devic
  void *dma_direct_alloc(struct device *dev, size_t size,
  		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
  {
++<<<<<<< HEAD
 +	if (!dev_is_dma_coherent(dev))
++=======
+ 	if (!IS_ENABLED(CONFIG_ARCH_HAS_UNCACHED_SEGMENT) &&
+ 	    dma_alloc_need_uncached(dev, attrs))
++>>>>>>> c2f2124e0d44 (dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code)
  		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
  	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
  }
@@@ -188,7 -199,8 +193,12 @@@
  void dma_direct_free(struct device *dev, size_t size,
  		void *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)
  {
++<<<<<<< HEAD
 +	if (!dev_is_dma_coherent(dev))
++=======
+ 	if (!IS_ENABLED(CONFIG_ARCH_HAS_UNCACHED_SEGMENT) &&
+ 	    dma_alloc_need_uncached(dev, attrs))
++>>>>>>> c2f2124e0d44 (dma-direct: handle DMA_ATTR_NON_CONSISTENT in common code)
  		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
  	else
  		dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
* Unmerged path arch/parisc/kernel/pci-dma.c
* Unmerged path kernel/dma/direct.c
