KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation)

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Singh, Brijesh <brijesh.singh@amd.com>
commit 05d5a48635259e621ea26d01e8316c6feeb34190
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/05d5a486.failed

Errata#1096:

On a nested data page fault when CR.SMAP=1 and the guest data read
generates a SMAP violation, GuestInstrBytes field of the VMCB on a
VMEXIT will incorrectly return 0h instead the correct guest
instruction bytes .

Recommend Workaround:

To determine what instruction the guest was executing the hypervisor
will have to decode the instruction at the instruction pointer.

The recommended workaround can not be implemented for the SEV
guest because guest memory is encrypted with the guest specific key,
and instruction decoder will not be able to decode the instruction
bytes. If we hit this errata in the SEV guest then log the message
and request a guest shutdown.

	Reported-by: Venkatesh Srinivas <venkateshs@google.com>
	Cc: Jim Mattson <jmattson@google.com>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: "Radim Krčmář" <rkrcmar@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 05d5a48635259e621ea26d01e8316c6feeb34190)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/include/asm/kvm_host.h
index 4f4095c7b53d,5b03006c00be..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1165,6 -1188,12 +1165,15 @@@ struct kvm_x86_ops 
  	int (*mem_enc_unreg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
  
  	int (*get_msr_feature)(struct kvm_msr_entry *entry);
++<<<<<<< HEAD
++=======
+ 
+ 	int (*nested_enable_evmcs)(struct kvm_vcpu *vcpu,
+ 				   uint16_t *vmcs_version);
+ 	uint16_t (*nested_get_evmcs_version)(struct kvm_vcpu *vcpu);
+ 
+ 	bool (*need_emulation_on_page_fault)(struct kvm_vcpu *vcpu);
++>>>>>>> 05d5a4863525 (KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation))
  };
  
  struct kvm_arch_async_pf {
diff --cc arch/x86/kvm/svm.c
index 8d45833d65a9,426039285fd1..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -7080,6 -7085,49 +7080,52 @@@ failed
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static uint16_t nested_get_evmcs_version(struct kvm_vcpu *vcpu)
+ {
+ 	/* Not supported */
+ 	return 0;
+ }
+ 
+ static int nested_enable_evmcs(struct kvm_vcpu *vcpu,
+ 				   uint16_t *vmcs_version)
+ {
+ 	/* Intel-only feature */
+ 	return -ENODEV;
+ }
+ 
+ static bool svm_need_emulation_on_page_fault(struct kvm_vcpu *vcpu)
+ {
+ 	bool is_user, smap;
+ 
+ 	is_user = svm_get_cpl(vcpu) == 3;
+ 	smap = !kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
+ 
+ 	/*
+ 	 * Detect and workaround Errata 1096 Fam_17h_00_0Fh
+ 	 *
+ 	 * In non SEV guest, hypervisor will be able to read the guest
+ 	 * memory to decode the instruction pointer when insn_len is zero
+ 	 * so we return true to indicate that decoding is possible.
+ 	 *
+ 	 * But in the SEV guest, the guest memory is encrypted with the
+ 	 * guest specific key and hypervisor will not be able to decode the
+ 	 * instruction pointer so we will not able to workaround it. Lets
+ 	 * print the error and request to kill the guest.
+ 	 */
+ 	if (is_user && smap) {
+ 		if (!sev_guest(vcpu->kvm))
+ 			return true;
+ 
+ 		pr_err_ratelimited("KVM: Guest triggered AMD Erratum 1096\n");
+ 		kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
+ 	}
+ 
+ 	return false;
+ }
+ 
++>>>>>>> 05d5a4863525 (KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation))
  static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
  	.cpu_has_kvm_support = has_svm,
  	.disabled_by_bios = is_disabled,
@@@ -7209,6 -7258,11 +7255,14 @@@
  	.mem_enc_op = svm_mem_enc_op,
  	.mem_enc_reg_region = svm_register_enc_region,
  	.mem_enc_unreg_region = svm_unregister_enc_region,
++<<<<<<< HEAD
++=======
+ 
+ 	.nested_enable_evmcs = nested_enable_evmcs,
+ 	.nested_get_evmcs_version = nested_get_evmcs_version,
+ 
+ 	.need_emulation_on_page_fault = svm_need_emulation_on_page_fault,
++>>>>>>> 05d5a4863525 (KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation))
  };
  
  static int __init svm_init(void)
diff --cc arch/x86/kvm/vmx/vmx.c
index 7955a1810f70,6aa84e09217b..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -14623,237 -7409,8 +14623,242 @@@ static int enable_smi_window(struct kvm
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 +				struct kvm_nested_state __user *user_kvm_nested_state,
 +				u32 user_data_size)
 +{
 +	struct vcpu_vmx *vmx;
 +	struct vmcs12 *vmcs12;
 +	struct kvm_nested_state kvm_state = {
 +		.flags = 0,
 +		.format = 0,
 +		.size = sizeof(kvm_state),
 +		.vmx.vmxon_pa = -1ull,
 +		.vmx.vmcs_pa = -1ull,
 +	};
 +
 +	if (!vcpu)
 +		return kvm_state.size + 2 * VMCS12_SIZE;
 +
 +	vmx = to_vmx(vcpu);
 +	vmcs12 = get_vmcs12(vcpu);
 +
 +	/* FIXME: Enlightened VMCS is currently unsupported */
 +	if (vmx->nested.hv_evmcs)
 +		return -ENOTSUPP;
 +
 +	if (nested_vmx_allowed(vcpu) &&
 +	    (vmx->nested.vmxon || vmx->nested.smm.vmxon)) {
 +		kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
 +		kvm_state.vmx.vmcs_pa = vmx->nested.current_vmptr;
 +
 +		if (vmx->nested.current_vmptr != -1ull) {
 +			kvm_state.size += VMCS12_SIZE;
 +
 +			if (is_guest_mode(vcpu) &&
 +			    nested_cpu_has_shadow_vmcs(vmcs12) &&
 +			    vmcs12->vmcs_link_pointer != -1ull)
 +				kvm_state.size += VMCS12_SIZE;
 +		}
 +
 +		if (vmx->nested.smm.vmxon)
 +			kvm_state.vmx.smm.flags |= KVM_STATE_NESTED_SMM_VMXON;
 +
 +		if (vmx->nested.smm.guest_mode)
 +			kvm_state.vmx.smm.flags |= KVM_STATE_NESTED_SMM_GUEST_MODE;
 +
 +		if (is_guest_mode(vcpu)) {
 +			kvm_state.flags |= KVM_STATE_NESTED_GUEST_MODE;
 +
 +			if (vmx->nested.nested_run_pending)
 +				kvm_state.flags |= KVM_STATE_NESTED_RUN_PENDING;
 +		}
 +	}
 +
 +	if (user_data_size < kvm_state.size)
 +		goto out;
 +
 +	if (copy_to_user(user_kvm_nested_state, &kvm_state, sizeof(kvm_state)))
 +		return -EFAULT;
 +
 +	if (vmx->nested.current_vmptr == -1ull)
 +		goto out;
 +
 +	/*
 +	 * When running L2, the authoritative vmcs12 state is in the
 +	 * vmcs02. When running L1, the authoritative vmcs12 state is
 +	 * in the shadow vmcs linked to vmcs01, unless
 +	 * need_vmcs12_sync is set, in which case, the authoritative
 +	 * vmcs12 state is in the vmcs12 already.
 +	 */
 +	if (is_guest_mode(vcpu))
 +		sync_vmcs12(vcpu, vmcs12);
 +	else if (enable_shadow_vmcs && !vmx->nested.need_vmcs12_sync)
 +		copy_shadow_to_vmcs12(vmx);
 +
 +	if (copy_to_user(user_kvm_nested_state->data, vmcs12, sizeof(*vmcs12)))
 +		return -EFAULT;
 +
 +	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 +	    vmcs12->vmcs_link_pointer != -1ull) {
 +		if (copy_to_user(user_kvm_nested_state->data + VMCS12_SIZE,
 +				 get_shadow_vmcs12(vcpu), sizeof(*vmcs12)))
 +			return -EFAULT;
 +	}
 +
 +out:
 +	return kvm_state.size;
 +}
 +
 +static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 +				struct kvm_nested_state __user *user_kvm_nested_state,
 +				struct kvm_nested_state *kvm_state)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct vmcs12 *vmcs12;
 +	u32 exit_qual;
 +	int ret;
 +
 +	if (kvm_state->format != 0)
 +		return -EINVAL;
 +
 +	if (!nested_vmx_allowed(vcpu))
 +		return kvm_state->vmx.vmxon_pa == -1ull ? 0 : -EINVAL;
 +
 +	if (kvm_state->vmx.vmxon_pa == -1ull) {
 +		if (kvm_state->vmx.smm.flags)
 +			return -EINVAL;
 +
 +		if (kvm_state->vmx.vmcs_pa != -1ull)
 +			return -EINVAL;
 +
 +		vmx_leave_nested(vcpu);
 +		return 0;
 +	}
 +
 +	if (!page_address_valid(vcpu, kvm_state->vmx.vmxon_pa))
 +		return -EINVAL;
 +
 +	if ((kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE) &&
 +	    (kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))
 +		return -EINVAL;
 +
 +	if (kvm_state->vmx.smm.flags &
 +	    ~(KVM_STATE_NESTED_SMM_GUEST_MODE | KVM_STATE_NESTED_SMM_VMXON))
 +		return -EINVAL;
 +
 +	/*
 +	 * SMM temporarily disables VMX, so we cannot be in guest mode,
 +	 * nor can VMLAUNCH/VMRESUME be pending.  Outside SMM, SMM flags
 +	 * must be zero.
 +	 */
 +	if (is_smm(vcpu) ? kvm_state->flags : kvm_state->vmx.smm.flags)
 +		return -EINVAL;
 +
 +	if ((kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE) &&
 +	    !(kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_VMXON))
 +		return -EINVAL;
 +
 +	vmx_leave_nested(vcpu);
 +	if (kvm_state->vmx.vmxon_pa == -1ull)
 +		return 0;
 +
 +	vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
 +	ret = enter_vmx_operation(vcpu);
 +	if (ret)
 +		return ret;
 +
 +	/* Empty 'VMXON' state is permitted */
 +	if (kvm_state->size < sizeof(kvm_state) + sizeof(*vmcs12))
 +		return 0;
 +
 +	if (kvm_state->vmx.vmcs_pa == kvm_state->vmx.vmxon_pa ||
 +	    !page_address_valid(vcpu, kvm_state->vmx.vmcs_pa))
 +		return -EINVAL;
 +
 +	set_current_vmptr(vmx, kvm_state->vmx.vmcs_pa);
 +
 +	if (kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_VMXON) {
 +		vmx->nested.smm.vmxon = true;
 +		vmx->nested.vmxon = false;
 +
 +		if (kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE)
 +			vmx->nested.smm.guest_mode = true;
 +	}
 +
 +	vmcs12 = get_vmcs12(vcpu);
 +	if (copy_from_user(vmcs12, user_kvm_nested_state->data, sizeof(*vmcs12)))
 +		return -EFAULT;
 +
 +	if (vmcs12->hdr.revision_id != VMCS12_REVISION)
 +		return -EINVAL;
 +
 +	if (!(kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))
 +		return 0;
 +
 +	vmx->nested.nested_run_pending =
 +		!!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);
 +
 +	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 +	    vmcs12->vmcs_link_pointer != -1ull) {
 +		struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
 +		if (kvm_state->size < sizeof(kvm_state) + 2 * sizeof(*vmcs12))
 +			return -EINVAL;
 +
 +		if (copy_from_user(shadow_vmcs12,
 +				   user_kvm_nested_state->data + VMCS12_SIZE,
 +				   sizeof(*vmcs12)))
 +			return -EFAULT;
 +
 +		if (shadow_vmcs12->hdr.revision_id != VMCS12_REVISION ||
 +		    !shadow_vmcs12->hdr.shadow_vmcs)
 +			return -EINVAL;
 +	}
 +
 +	if (check_vmentry_prereqs(vcpu, vmcs12) ||
 +	    check_vmentry_postreqs(vcpu, vmcs12, &exit_qual))
 +		return -EINVAL;
 +
 +	vmx->nested.dirty_vmcs12 = true;
 +	ret = nested_vmx_enter_non_root_mode(vcpu, false);
 +	if (ret)
 +		return -EINVAL;
 +
 +	return 0;
 +}
 +
 +static __exit void nested_vmx_hardware_unsetup(void)
 +{
 +	int i;
 +
 +	if (enable_shadow_vmcs) {
 +		for (i = 0; i < VMX_BITMAP_NR; i++)
 +			free_page((unsigned long)vmx_bitmap[i]);
 +	}
 +}
 +
 +static __init int nested_vmx_hardware_setup(void)
 +{
 +	int i;
 +
 +	if (enable_shadow_vmcs) {
 +		for (i = 0; i < VMX_BITMAP_NR; i++) {
 +			vmx_bitmap[i] = (unsigned long *)
 +				__get_free_page(GFP_KERNEL);
 +			if (!vmx_bitmap[i]) {
 +				nested_vmx_hardware_unsetup();
 +				return -ENOMEM;
 +			}
 +		}
 +
 +		init_vmcs_shadow_fields();
 +	}
 +
++=======
+ static bool vmx_need_emulation_on_page_fault(struct kvm_vcpu *vcpu)
+ {
++>>>>>>> 05d5a4863525 (KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation))
  	return 0;
  }
  
@@@ -15156,6 -7710,13 +15161,16 @@@ static struct kvm_x86_ops vmx_x86_ops _
  	.pre_enter_smm = vmx_pre_enter_smm,
  	.pre_leave_smm = vmx_pre_leave_smm,
  	.enable_smi_window = enable_smi_window,
++<<<<<<< HEAD
++=======
+ 
+ 	.check_nested_events = NULL,
+ 	.get_nested_state = NULL,
+ 	.set_nested_state = NULL,
+ 	.get_vmcs12_pages = NULL,
+ 	.nested_enable_evmcs = NULL,
+ 	.need_emulation_on_page_fault = vmx_need_emulation_on_page_fault,
++>>>>>>> 05d5a4863525 (KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation))
  };
  
  static void vmx_cleanup_l1d_flush(void)
* Unmerged path arch/x86/include/asm/kvm_host.h
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 7df43ed1e670..73da4d7a97f8 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -5330,10 +5330,12 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 	 * This can happen if a guest gets a page-fault on data access but the HW
 	 * table walker is not able to read the instruction page (e.g instruction
 	 * page is not present in memory). In those cases we simply restart the
-	 * guest.
+	 * guest, with the exception of AMD Erratum 1096 which is unrecoverable.
 	 */
-	if (unlikely(insn && !insn_len))
-		return 1;
+	if (unlikely(insn && !insn_len)) {
+		if (!kvm_x86_ops->need_emulation_on_page_fault(vcpu))
+			return 1;
+	}
 
 	er = x86_emulate_instruction(vcpu, cr2, emulation_type, insn, insn_len);
 
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
