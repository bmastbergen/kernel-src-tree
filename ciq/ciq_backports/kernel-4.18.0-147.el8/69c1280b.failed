net/mlx5: Device events, Use async events chain

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 69c1280b1f3b9123bc5154b2062507abcc14c3ef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/69c1280b.failed

Move all the generic async events handling into new specific events
handling file events.c to keep eq.c file clean from concrete event logic
handling.

Use new API to register for NOTIFY_ANY to handle generic events and
dispatch allowed events to mlx5_core consumers (mlx5_ib and mlx5e)

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 69c1280b1f3b9123bc5154b2062507abcc14c3ef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/Makefile
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
#	include/linux/mlx5/driver.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/Makefile
index a094c09dfaa9,26afe0779a0c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@@ -14,8 -14,8 +14,13 @@@ obj-$(CONFIG_MLX5_CORE) += mlx5_core.
  mlx5_core-y :=	main.o cmd.o debugfs.o fw.o eq.o uar.o pagealloc.o \
  		health.o mcg.o cq.o srq.o alloc.o qp.o port.o mr.o pd.o \
  		mad.o transobj.o vport.o sriov.o fs_cmd.o fs_core.o \
++<<<<<<< HEAD
 +		fs_counters.o rl.o lag.o dev.o wq.o lib/gid.o  \
 +		diag/fs_tracepoint.o
++=======
+ 		fs_counters.o rl.o lag.o dev.o events.o wq.o lib/gid.o \
+ 		diag/fs_tracepoint.o diag/fw_tracer.o
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  
  #
  # Netdev basic
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index a997f6ba7cac,4aa39a1fe23f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -87,324 -108,6 +87,327 @@@ static int mlx5_cmd_destroy_eq(struct m
  	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
  }
  
++<<<<<<< HEAD
 +static struct mlx5_eqe *get_eqe(struct mlx5_eq *eq, u32 entry)
 +{
 +	return mlx5_buf_offset(&eq->buf, entry * MLX5_EQE_SIZE);
 +}
 +
 +static struct mlx5_eqe *next_eqe_sw(struct mlx5_eq *eq)
 +{
 +	struct mlx5_eqe *eqe = get_eqe(eq, eq->cons_index & (eq->nent - 1));
 +
 +	return ((eqe->owner & 1) ^ !!(eq->cons_index & eq->nent)) ? NULL : eqe;
 +}
 +
 +static const char *eqe_type_str(u8 type)
 +{
 +	switch (type) {
 +	case MLX5_EVENT_TYPE_COMP:
 +		return "MLX5_EVENT_TYPE_COMP";
 +	case MLX5_EVENT_TYPE_PATH_MIG:
 +		return "MLX5_EVENT_TYPE_PATH_MIG";
 +	case MLX5_EVENT_TYPE_COMM_EST:
 +		return "MLX5_EVENT_TYPE_COMM_EST";
 +	case MLX5_EVENT_TYPE_SQ_DRAINED:
 +		return "MLX5_EVENT_TYPE_SQ_DRAINED";
 +	case MLX5_EVENT_TYPE_SRQ_LAST_WQE:
 +		return "MLX5_EVENT_TYPE_SRQ_LAST_WQE";
 +	case MLX5_EVENT_TYPE_SRQ_RQ_LIMIT:
 +		return "MLX5_EVENT_TYPE_SRQ_RQ_LIMIT";
 +	case MLX5_EVENT_TYPE_CQ_ERROR:
 +		return "MLX5_EVENT_TYPE_CQ_ERROR";
 +	case MLX5_EVENT_TYPE_WQ_CATAS_ERROR:
 +		return "MLX5_EVENT_TYPE_WQ_CATAS_ERROR";
 +	case MLX5_EVENT_TYPE_PATH_MIG_FAILED:
 +		return "MLX5_EVENT_TYPE_PATH_MIG_FAILED";
 +	case MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR:
 +		return "MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR";
 +	case MLX5_EVENT_TYPE_WQ_ACCESS_ERROR:
 +		return "MLX5_EVENT_TYPE_WQ_ACCESS_ERROR";
 +	case MLX5_EVENT_TYPE_SRQ_CATAS_ERROR:
 +		return "MLX5_EVENT_TYPE_SRQ_CATAS_ERROR";
 +	case MLX5_EVENT_TYPE_INTERNAL_ERROR:
 +		return "MLX5_EVENT_TYPE_INTERNAL_ERROR";
 +	case MLX5_EVENT_TYPE_PORT_CHANGE:
 +		return "MLX5_EVENT_TYPE_PORT_CHANGE";
 +	case MLX5_EVENT_TYPE_GPIO_EVENT:
 +		return "MLX5_EVENT_TYPE_GPIO_EVENT";
 +	case MLX5_EVENT_TYPE_PORT_MODULE_EVENT:
 +		return "MLX5_EVENT_TYPE_PORT_MODULE_EVENT";
 +	case MLX5_EVENT_TYPE_TEMP_WARN_EVENT:
 +		return "MLX5_EVENT_TYPE_TEMP_WARN_EVENT";
 +	case MLX5_EVENT_TYPE_REMOTE_CONFIG:
 +		return "MLX5_EVENT_TYPE_REMOTE_CONFIG";
 +	case MLX5_EVENT_TYPE_DB_BF_CONGESTION:
 +		return "MLX5_EVENT_TYPE_DB_BF_CONGESTION";
 +	case MLX5_EVENT_TYPE_STALL_EVENT:
 +		return "MLX5_EVENT_TYPE_STALL_EVENT";
 +	case MLX5_EVENT_TYPE_CMD:
 +		return "MLX5_EVENT_TYPE_CMD";
 +	case MLX5_EVENT_TYPE_PAGE_REQUEST:
 +		return "MLX5_EVENT_TYPE_PAGE_REQUEST";
 +	case MLX5_EVENT_TYPE_PAGE_FAULT:
 +		return "MLX5_EVENT_TYPE_PAGE_FAULT";
 +	case MLX5_EVENT_TYPE_PPS_EVENT:
 +		return "MLX5_EVENT_TYPE_PPS_EVENT";
 +	case MLX5_EVENT_TYPE_NIC_VPORT_CHANGE:
 +		return "MLX5_EVENT_TYPE_NIC_VPORT_CHANGE";
 +	case MLX5_EVENT_TYPE_FPGA_ERROR:
 +		return "MLX5_EVENT_TYPE_FPGA_ERROR";
 +	case MLX5_EVENT_TYPE_FPGA_QP_ERROR:
 +		return "MLX5_EVENT_TYPE_FPGA_QP_ERROR";
 +	case MLX5_EVENT_TYPE_GENERAL_EVENT:
 +		return "MLX5_EVENT_TYPE_GENERAL_EVENT";
 +	default:
 +		return "Unrecognized event";
 +	}
 +}
 +
 +static enum mlx5_dev_event port_subtype_event(u8 subtype)
 +{
 +	switch (subtype) {
 +	case MLX5_PORT_CHANGE_SUBTYPE_DOWN:
 +		return MLX5_DEV_EVENT_PORT_DOWN;
 +	case MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:
 +		return MLX5_DEV_EVENT_PORT_UP;
 +	case MLX5_PORT_CHANGE_SUBTYPE_INITIALIZED:
 +		return MLX5_DEV_EVENT_PORT_INITIALIZED;
 +	case MLX5_PORT_CHANGE_SUBTYPE_LID:
 +		return MLX5_DEV_EVENT_LID_CHANGE;
 +	case MLX5_PORT_CHANGE_SUBTYPE_PKEY:
 +		return MLX5_DEV_EVENT_PKEY_CHANGE;
 +	case MLX5_PORT_CHANGE_SUBTYPE_GUID:
 +		return MLX5_DEV_EVENT_GUID_CHANGE;
 +	case MLX5_PORT_CHANGE_SUBTYPE_CLIENT_REREG:
 +		return MLX5_DEV_EVENT_CLIENT_REREG;
 +	}
 +	return -1;
 +}
 +
 +static void eq_update_ci(struct mlx5_eq *eq, int arm)
 +{
 +	__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);
 +	u32 val = (eq->cons_index & 0xffffff) | (eq->eqn << 24);
 +
 +	__raw_writel((__force u32)cpu_to_be32(val), addr);
 +	/* We still want ordering, just not swabbing, so add a barrier */
 +	mb();
 +}
 +
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +static void eqe_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_pagefault *pfault = container_of(work,
 +						     struct mlx5_pagefault,
 +						     work);
 +	struct mlx5_eq *eq = pfault->eq;
 +
 +	mlx5_core_page_fault(eq->dev, pfault);
 +	mempool_free(pfault, eq->pf_ctx.pool);
 +}
 +
 +static void eq_pf_process(struct mlx5_eq *eq)
 +{
 +	struct mlx5_core_dev *dev = eq->dev;
 +	struct mlx5_eqe_page_fault *pf_eqe;
 +	struct mlx5_pagefault *pfault;
 +	struct mlx5_eqe *eqe;
 +	int set_ci = 0;
 +
 +	while ((eqe = next_eqe_sw(eq))) {
 +		pfault = mempool_alloc(eq->pf_ctx.pool, GFP_ATOMIC);
 +		if (!pfault) {
 +			schedule_work(&eq->pf_ctx.work);
 +			break;
 +		}
 +
 +		dma_rmb();
 +		pf_eqe = &eqe->data.page_fault;
 +		pfault->event_subtype = eqe->sub_type;
 +		pfault->bytes_committed = be32_to_cpu(pf_eqe->bytes_committed);
 +
 +		mlx5_core_dbg(dev,
 +			      "PAGE_FAULT: subtype: 0x%02x, bytes_committed: 0x%06x\n",
 +			      eqe->sub_type, pfault->bytes_committed);
 +
 +		switch (eqe->sub_type) {
 +		case MLX5_PFAULT_SUBTYPE_RDMA:
 +			/* RDMA based event */
 +			pfault->type =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) >> 24;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) &
 +				MLX5_24BIT_MASK;
 +			pfault->rdma.r_key =
 +				be32_to_cpu(pf_eqe->rdma.r_key);
 +			pfault->rdma.packet_size =
 +				be16_to_cpu(pf_eqe->rdma.packet_length);
 +			pfault->rdma.rdma_op_len =
 +				be32_to_cpu(pf_eqe->rdma.rdma_op_len);
 +			pfault->rdma.rdma_va =
 +				be64_to_cpu(pf_eqe->rdma.rdma_va);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, r_key: 0x%08x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->rdma.r_key);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: rdma_op_len: 0x%08x, rdma_va: 0x%016llx\n",
 +				      pfault->rdma.rdma_op_len,
 +				      pfault->rdma.rdma_va);
 +			break;
 +
 +		case MLX5_PFAULT_SUBTYPE_WQE:
 +			/* WQE based event */
 +			pfault->type =
 +				(be32_to_cpu(pf_eqe->wqe.pftype_wq) >> 24) & 0x7;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->wqe.token);
 +			pfault->wqe.wq_num =
 +				be32_to_cpu(pf_eqe->wqe.pftype_wq) &
 +				MLX5_24BIT_MASK;
 +			pfault->wqe.wqe_index =
 +				be16_to_cpu(pf_eqe->wqe.wqe_index);
 +			pfault->wqe.packet_size =
 +				be16_to_cpu(pf_eqe->wqe.packet_length);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, wq_num: 0x%06x, wqe_index: 0x%04x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->wqe.wq_num,
 +				      pfault->wqe.wqe_index);
 +			break;
 +
 +		default:
 +			mlx5_core_warn(dev,
 +				       "Unsupported page fault event sub-type: 0x%02hhx\n",
 +				       eqe->sub_type);
 +			/* Unsupported page faults should still be
 +			 * resolved by the page fault handler
 +			 */
 +		}
 +
 +		pfault->eq = eq;
 +		INIT_WORK(&pfault->work, eqe_pf_action);
 +		queue_work(eq->pf_ctx.wq, &pfault->work);
 +
 +		++eq->cons_index;
 +		++set_ci;
 +
 +		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
 +			eq_update_ci(eq, 0);
 +			set_ci = 0;
 +		}
 +	}
 +
 +	eq_update_ci(eq, 1);
 +}
 +
 +static irqreturn_t mlx5_eq_pf_int(int irq, void *eq_ptr)
 +{
 +	struct mlx5_eq *eq = eq_ptr;
 +	unsigned long flags;
 +
 +	if (spin_trylock_irqsave(&eq->pf_ctx.lock, flags)) {
 +		eq_pf_process(eq);
 +		spin_unlock_irqrestore(&eq->pf_ctx.lock, flags);
 +	} else {
 +		schedule_work(&eq->pf_ctx.work);
 +	}
 +
 +	return IRQ_HANDLED;
 +}
 +
 +/* mempool_refill() was proposed but unfortunately wasn't accepted
 + * http://lkml.iu.edu/hypermail/linux/kernel/1512.1/05073.html
 + * Chip workaround.
 + */
 +static void mempool_refill(mempool_t *pool)
 +{
 +	while (pool->curr_nr < pool->min_nr)
 +		mempool_free(mempool_alloc(pool, GFP_KERNEL), pool);
 +}
 +
 +static void eq_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_eq *eq = container_of(work, struct mlx5_eq, pf_ctx.work);
 +
 +	mempool_refill(eq->pf_ctx.pool);
 +
 +	spin_lock_irq(&eq->pf_ctx.lock);
 +	eq_pf_process(eq);
 +	spin_unlock_irq(&eq->pf_ctx.lock);
 +}
 +
 +static int init_pf_ctx(struct mlx5_eq_pagefault *pf_ctx, const char *name)
 +{
 +	spin_lock_init(&pf_ctx->lock);
 +	INIT_WORK(&pf_ctx->work, eq_pf_action);
 +
 +	pf_ctx->wq = alloc_workqueue(name,
 +				     WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
 +				     MLX5_NUM_CMD_EQE);
 +	if (!pf_ctx->wq)
 +		return -ENOMEM;
 +
 +	pf_ctx->pool = mempool_create_kmalloc_pool
 +		(MLX5_NUM_PF_DRAIN, sizeof(struct mlx5_pagefault));
 +	if (!pf_ctx->pool)
 +		goto err_wq;
 +
 +	return 0;
 +err_wq:
 +	destroy_workqueue(pf_ctx->wq);
 +	return -ENOMEM;
 +}
 +
 +int mlx5_core_page_fault_resume(struct mlx5_core_dev *dev, u32 token,
 +				u32 wq_num, u8 type, int error)
 +{
 +	u32 out[MLX5_ST_SZ_DW(page_fault_resume_out)] = {0};
 +	u32 in[MLX5_ST_SZ_DW(page_fault_resume_in)]   = {0};
 +
 +	MLX5_SET(page_fault_resume_in, in, opcode,
 +		 MLX5_CMD_OP_PAGE_FAULT_RESUME);
 +	MLX5_SET(page_fault_resume_in, in, error, !!error);
 +	MLX5_SET(page_fault_resume_in, in, page_fault_type, type);
 +	MLX5_SET(page_fault_resume_in, in, wq_number, wq_num);
 +	MLX5_SET(page_fault_resume_in, in, token, token);
 +
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 +}
 +EXPORT_SYMBOL_GPL(mlx5_core_page_fault_resume);
 +#endif
 +
 +static void general_event_handler(struct mlx5_core_dev *dev,
 +				  struct mlx5_eqe *eqe)
 +{
 +	switch (eqe->sub_type) {
 +	case MLX5_GENERAL_SUBTYPE_DELAY_DROP_TIMEOUT:
 +		if (dev->event)
 +			dev->event(dev, MLX5_DEV_EVENT_DELAY_DROP_TIMEOUT, 0);
 +		break;
 +	default:
 +		mlx5_core_dbg(dev, "General event with unrecognized subtype: sub_type %d\n",
 +			      eqe->sub_type);
 +	}
 +}
 +
 +static void mlx5_temp_warning_event(struct mlx5_core_dev *dev,
 +				    struct mlx5_eqe *eqe)
 +{
 +	u64 value_lsb;
 +	u64 value_msb;
 +
 +	value_lsb = be64_to_cpu(eqe->data.temp_warning.sensor_warning_lsb);
 +	value_msb = be64_to_cpu(eqe->data.temp_warning.sensor_warning_msb);
 +
 +	mlx5_core_warn(dev,
 +		       "High temperature on sensors with bit set %llx %llx",
 +		       value_msb, value_lsb);
 +}
 +
++=======
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  /* caller must eventually call mlx5_cq_put on the returned cq */
  static struct mlx5_core_cq *mlx5_eq_cq_get(struct mlx5_eq *eq, u32 cqn)
  {
@@@ -605,6 -190,51 +608,54 @@@ u32 mlx5_eq_poll_irq_disabled(struct ml
  	return count_eqe;
  }
  
++<<<<<<< HEAD
++=======
+ static irqreturn_t mlx5_eq_async_int(int irq, void *eq_ptr)
+ {
+ 	struct mlx5_eq *eq = eq_ptr;
+ 	struct mlx5_eq_table *eqt;
+ 	struct mlx5_core_dev *dev;
+ 	struct mlx5_eqe *eqe;
+ 	int set_ci = 0;
+ 
+ 	dev = eq->dev;
+ 	eqt = dev->priv.eq_table;
+ 
+ 	while ((eqe = next_eqe_sw(eq))) {
+ 		/*
+ 		 * Make sure we read EQ entry contents after we've
+ 		 * checked the ownership bit.
+ 		 */
+ 		dma_rmb();
+ 
+ 		if (likely(eqe->type < MLX5_EVENT_TYPE_MAX))
+ 			atomic_notifier_call_chain(&eqt->nh[eqe->type], eqe->type, eqe);
+ 		else
+ 			mlx5_core_warn_once(dev, "notifier_call_chain is not setup for eqe: %d\n", eqe->type);
+ 
+ 		atomic_notifier_call_chain(&eqt->nh[MLX5_EVENT_TYPE_NOTIFY_ANY], eqe->type, eqe);
+ 
+ 		++eq->cons_index;
+ 		++set_ci;
+ 
+ 		/* The HCA will think the queue has overflowed if we
+ 		 * don't tell it we've been processing events.  We
+ 		 * create our EQs with MLX5_NUM_SPARE_EQE extra
+ 		 * entries, so we must update our consumer index at
+ 		 * least that often.
+ 		 */
+ 		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
+ 			eq_update_ci(eq, 0);
+ 			set_ci = 0;
+ 		}
+ 	}
+ 
+ 	eq_update_ci(eq, 1);
+ 
+ 	return IRQ_HANDLED;
+ }
+ 
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  static void init_eq_buf(struct mlx5_eq *eq)
  {
  	struct mlx5_eqe *eqe;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/main.c
index cfdc2f0b35a2,e56278ead4eb..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@@ -1004,9 -807,10 +1010,10 @@@ err_tables_cleanup
  	mlx5_cleanup_srq_table(dev);
  	mlx5_cleanup_qp_table(dev);
  	mlx5_cq_debugfs_cleanup(dev);
- 
+ err_events_cleanup:
+ 	mlx5_events_cleanup(dev);
  err_eq_cleanup:
 -	mlx5_eq_table_cleanup(dev);
 +	mlx5_eq_cleanup(dev);
  
  out:
  	return err;
@@@ -1026,7 -831,8 +1033,12 @@@ static void mlx5_cleanup_once(struct ml
  	mlx5_cleanup_srq_table(dev);
  	mlx5_cleanup_qp_table(dev);
  	mlx5_cq_debugfs_cleanup(dev);
++<<<<<<< HEAD
 +	mlx5_eq_cleanup(dev);
++=======
+ 	mlx5_events_cleanup(dev);
+ 	mlx5_eq_table_cleanup(dev);
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  }
  
  static int mlx5_load_one(struct mlx5_core_dev *dev, struct mlx5_priv *priv,
@@@ -1158,25 -952,22 +1170,32 @@@
  	if (IS_ERR(dev->priv.uar)) {
  		dev_err(&pdev->dev, "Failed allocating uar, aborting\n");
  		err = PTR_ERR(dev->priv.uar);
 -		goto err_get_uars;
 +		goto err_disable_msix;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_start_eqs(dev);
++=======
+ 	mlx5_events_start(dev);
+ 	mlx5_pagealloc_start(dev);
+ 
+ 	err = mlx5_eq_table_create(dev);
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
 +	if (err) {
 +		dev_err(&pdev->dev, "Failed to start pages and async EQs\n");
 +		goto err_put_uars;
 +	}
 +
 +	err = alloc_comp_eqs(dev);
  	if (err) {
 -		dev_err(&pdev->dev, "Failed to create EQs\n");
 -		goto err_eq_table;
 +		dev_err(&pdev->dev, "Failed to alloc completion EQs\n");
 +		goto err_stop_eqs;
  	}
  
 -	err = mlx5_fw_tracer_init(dev->tracer);
 +	err = mlx5_irq_set_affinity_hints(dev);
  	if (err) {
 -		dev_err(&pdev->dev, "Failed to init FW tracer\n");
 -		goto err_fw_tracer;
 +		dev_err(&pdev->dev, "Failed to alloc affinity hint cpumask\n");
 +		goto err_affinity_hints;
  	}
  
  	err = mlx5_fpga_device_start(dev);
@@@ -1247,21 -1038,17 +1266,27 @@@ err_ipsec_start
  	mlx5_fpga_device_stop(dev);
  
  err_fpga_start:
 -	mlx5_fw_tracer_cleanup(dev->tracer);
 +	mlx5_irq_clear_affinity_hints(dev);
  
 -err_fw_tracer:
 -	mlx5_eq_table_destroy(dev);
 +err_affinity_hints:
 +	free_comp_eqs(dev);
  
++<<<<<<< HEAD
 +err_stop_eqs:
 +	mlx5_stop_eqs(dev);
 +
 +err_put_uars:
++=======
+ err_eq_table:
+ 	mlx5_pagealloc_stop(dev);
+ 	mlx5_events_stop(dev);
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  	mlx5_put_uars_page(dev, priv->uar);
  
 -err_get_uars:
 +err_disable_msix:
 +	mlx5_free_irq_vectors(dev);
 +
 +err_cleanup_once:
  	if (boot)
  		mlx5_cleanup_once(dev);
  
@@@ -1318,11 -1102,11 +1343,19 @@@ static int mlx5_unload_one(struct mlx5_
  	mlx5_accel_ipsec_cleanup(dev);
  	mlx5_accel_tls_cleanup(dev);
  	mlx5_fpga_device_stop(dev);
++<<<<<<< HEAD
 +	mlx5_irq_clear_affinity_hints(dev);
 +	free_comp_eqs(dev);
 +	mlx5_stop_eqs(dev);
 +	mlx5_put_uars_page(dev, priv->uar);
 +	mlx5_free_irq_vectors(dev);
++=======
+ 	mlx5_fw_tracer_cleanup(dev->tracer);
+ 	mlx5_eq_table_destroy(dev);
+ 	mlx5_pagealloc_stop(dev);
+ 	mlx5_events_stop(dev);
+ 	mlx5_put_uars_page(dev, priv->uar);
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  	if (cleanup)
  		mlx5_cleanup_once(dev);
  	mlx5_stop_health_poll(dev, cleanup);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 0594d0961cb3,c70bd94e18d6..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@@ -100,9 -105,6 +100,12 @@@ int mlx5_cmd_fast_teardown_hca(struct m
  
  void mlx5_core_event(struct mlx5_core_dev *dev, enum mlx5_dev_event event,
  		     unsigned long param);
++<<<<<<< HEAD
 +void mlx5_core_page_fault(struct mlx5_core_dev *dev,
 +			  struct mlx5_pagefault *pfault);
 +void mlx5_port_module_event(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe);
++=======
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  void mlx5_enter_error_state(struct mlx5_core_dev *dev, bool force);
  void mlx5_disable_device(struct mlx5_core_dev *dev);
  void mlx5_recover_device(struct mlx5_core_dev *dev);
diff --cc include/linux/mlx5/driver.h
index 2b6e906af6bb,ba64ecf72478..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -608,37 -541,12 +609,12 @@@ struct mlx5_rl_table 
  	struct mlx5_rl_entry   *rl_entry;
  };
  
- enum port_module_event_status_type {
- 	MLX5_MODULE_STATUS_PLUGGED   = 0x1,
- 	MLX5_MODULE_STATUS_UNPLUGGED = 0x2,
- 	MLX5_MODULE_STATUS_ERROR     = 0x3,
- 	MLX5_MODULE_STATUS_NUM       = 0x3,
- };
- 
- enum  port_module_event_error_type {
- 	MLX5_MODULE_EVENT_ERROR_POWER_BUDGET_EXCEEDED,
- 	MLX5_MODULE_EVENT_ERROR_LONG_RANGE_FOR_NON_MLNX_CABLE_MODULE,
- 	MLX5_MODULE_EVENT_ERROR_BUS_STUCK,
- 	MLX5_MODULE_EVENT_ERROR_NO_EEPROM_RETRY_TIMEOUT,
- 	MLX5_MODULE_EVENT_ERROR_ENFORCE_PART_NUMBER_LIST,
- 	MLX5_MODULE_EVENT_ERROR_UNKNOWN_IDENTIFIER,
- 	MLX5_MODULE_EVENT_ERROR_HIGH_TEMPERATURE,
- 	MLX5_MODULE_EVENT_ERROR_BAD_CABLE,
- 	MLX5_MODULE_EVENT_ERROR_UNKNOWN,
- 	MLX5_MODULE_EVENT_ERROR_NUM,
- };
- 
- struct mlx5_port_module_event_stats {
- 	u64 status_counters[MLX5_MODULE_STATUS_NUM];
- 	u64 error_counters[MLX5_MODULE_EVENT_ERROR_NUM];
- };
- 
  struct mlx5_priv {
  	char			name[MLX5_MAX_NAME_LEN];
 -	struct mlx5_eq_table	*eq_table;
 +	struct mlx5_eq_table	eq_table;
 +	struct mlx5_irq_info	*irq_info;
  
  	/* pages stuff */
 -	struct mlx5_nb          pg_nb;
  	struct workqueue_struct *pg_wq;
  	struct rb_root		page_root;
  	int			fw_pages;
@@@ -692,15 -601,6 +669,18 @@@
  	struct mlx5_fc_stats		fc_stats;
  	struct mlx5_rl_table            rl_table;
  
++<<<<<<< HEAD
 +	struct mlx5_port_module_event_stats  pme_stats;
 +
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	void		      (*pfault)(struct mlx5_core_dev *dev,
 +					void *context,
 +					struct mlx5_pagefault *pfault);
 +	void		       *pfault_ctx;
 +	struct srcu_struct      pfault_srcu;
 +#endif
++=======
++>>>>>>> 69c1280b1f3b (net/mlx5: Device events, Use async events chain)
  	struct mlx5_bfreg_data		bfregs;
  	struct mlx5_uars_page	       *uar;
  };
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/Makefile
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
index e8e2b89f39fb..a6afdbdae3c6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#include "lib/mlx5.h"
 #include "en.h"
 #include "en_accel/ipsec.h"
 #include "en_accel/tls.h"
@@ -1110,15 +1111,17 @@ static int mlx5e_grp_pme_fill_strings(struct mlx5e_priv *priv, u8 *data,
 static int mlx5e_grp_pme_fill_stats(struct mlx5e_priv *priv, u64 *data,
 				    int idx)
 {
-	struct mlx5_priv *mlx5_priv = &priv->mdev->priv;
+	struct mlx5_pme_stats pme_stats;
 	int i;
 
+	mlx5_get_pme_stats(priv->mdev, &pme_stats);
+
 	for (i = 0; i < NUM_PME_STATUS_STATS; i++)
-		data[idx++] = MLX5E_READ_CTR64_CPU(mlx5_priv->pme_stats.status_counters,
+		data[idx++] = MLX5E_READ_CTR64_CPU(pme_stats.status_counters,
 						   mlx5e_pme_status_desc, i);
 
 	for (i = 0; i < NUM_PME_ERR_STATS; i++)
-		data[idx++] = MLX5E_READ_CTR64_CPU(mlx5_priv->pme_stats.error_counters,
+		data[idx++] = MLX5E_READ_CTR64_CPU(pme_stats.error_counters,
 						   mlx5e_pme_error_desc, i);
 
 	return idx;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/events.c b/drivers/net/ethernet/mellanox/mlx5/core/events.c
new file mode 100644
index 000000000000..d3ab86bd394b
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/events.c
@@ -0,0 +1,283 @@
+// SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
+// Copyright (c) 2018 Mellanox Technologies
+
+#include <linux/mlx5/driver.h>
+#include "mlx5_core.h"
+#include "lib/eq.h"
+#include "lib/mlx5.h"
+
+struct mlx5_events {
+	struct mlx5_nb        nb;
+	struct mlx5_core_dev *dev;
+
+	/* port module evetns stats */
+	struct mlx5_pme_stats pme_stats;
+};
+
+static const char *eqe_type_str(u8 type)
+{
+	switch (type) {
+	case MLX5_EVENT_TYPE_COMP:
+		return "MLX5_EVENT_TYPE_COMP";
+	case MLX5_EVENT_TYPE_PATH_MIG:
+		return "MLX5_EVENT_TYPE_PATH_MIG";
+	case MLX5_EVENT_TYPE_COMM_EST:
+		return "MLX5_EVENT_TYPE_COMM_EST";
+	case MLX5_EVENT_TYPE_SQ_DRAINED:
+		return "MLX5_EVENT_TYPE_SQ_DRAINED";
+	case MLX5_EVENT_TYPE_SRQ_LAST_WQE:
+		return "MLX5_EVENT_TYPE_SRQ_LAST_WQE";
+	case MLX5_EVENT_TYPE_SRQ_RQ_LIMIT:
+		return "MLX5_EVENT_TYPE_SRQ_RQ_LIMIT";
+	case MLX5_EVENT_TYPE_CQ_ERROR:
+		return "MLX5_EVENT_TYPE_CQ_ERROR";
+	case MLX5_EVENT_TYPE_WQ_CATAS_ERROR:
+		return "MLX5_EVENT_TYPE_WQ_CATAS_ERROR";
+	case MLX5_EVENT_TYPE_PATH_MIG_FAILED:
+		return "MLX5_EVENT_TYPE_PATH_MIG_FAILED";
+	case MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR:
+		return "MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR";
+	case MLX5_EVENT_TYPE_WQ_ACCESS_ERROR:
+		return "MLX5_EVENT_TYPE_WQ_ACCESS_ERROR";
+	case MLX5_EVENT_TYPE_SRQ_CATAS_ERROR:
+		return "MLX5_EVENT_TYPE_SRQ_CATAS_ERROR";
+	case MLX5_EVENT_TYPE_INTERNAL_ERROR:
+		return "MLX5_EVENT_TYPE_INTERNAL_ERROR";
+	case MLX5_EVENT_TYPE_PORT_CHANGE:
+		return "MLX5_EVENT_TYPE_PORT_CHANGE";
+	case MLX5_EVENT_TYPE_GPIO_EVENT:
+		return "MLX5_EVENT_TYPE_GPIO_EVENT";
+	case MLX5_EVENT_TYPE_PORT_MODULE_EVENT:
+		return "MLX5_EVENT_TYPE_PORT_MODULE_EVENT";
+	case MLX5_EVENT_TYPE_TEMP_WARN_EVENT:
+		return "MLX5_EVENT_TYPE_TEMP_WARN_EVENT";
+	case MLX5_EVENT_TYPE_REMOTE_CONFIG:
+		return "MLX5_EVENT_TYPE_REMOTE_CONFIG";
+	case MLX5_EVENT_TYPE_DB_BF_CONGESTION:
+		return "MLX5_EVENT_TYPE_DB_BF_CONGESTION";
+	case MLX5_EVENT_TYPE_STALL_EVENT:
+		return "MLX5_EVENT_TYPE_STALL_EVENT";
+	case MLX5_EVENT_TYPE_CMD:
+		return "MLX5_EVENT_TYPE_CMD";
+	case MLX5_EVENT_TYPE_PAGE_REQUEST:
+		return "MLX5_EVENT_TYPE_PAGE_REQUEST";
+	case MLX5_EVENT_TYPE_PAGE_FAULT:
+		return "MLX5_EVENT_TYPE_PAGE_FAULT";
+	case MLX5_EVENT_TYPE_PPS_EVENT:
+		return "MLX5_EVENT_TYPE_PPS_EVENT";
+	case MLX5_EVENT_TYPE_NIC_VPORT_CHANGE:
+		return "MLX5_EVENT_TYPE_NIC_VPORT_CHANGE";
+	case MLX5_EVENT_TYPE_FPGA_ERROR:
+		return "MLX5_EVENT_TYPE_FPGA_ERROR";
+	case MLX5_EVENT_TYPE_FPGA_QP_ERROR:
+		return "MLX5_EVENT_TYPE_FPGA_QP_ERROR";
+	case MLX5_EVENT_TYPE_GENERAL_EVENT:
+		return "MLX5_EVENT_TYPE_GENERAL_EVENT";
+	case MLX5_EVENT_TYPE_DEVICE_TRACER:
+		return "MLX5_EVENT_TYPE_DEVICE_TRACER";
+	default:
+		return "Unrecognized event";
+	}
+}
+
+static enum mlx5_dev_event port_subtype2dev(u8 subtype)
+{
+	switch (subtype) {
+	case MLX5_PORT_CHANGE_SUBTYPE_DOWN:
+		return MLX5_DEV_EVENT_PORT_DOWN;
+	case MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:
+		return MLX5_DEV_EVENT_PORT_UP;
+	case MLX5_PORT_CHANGE_SUBTYPE_INITIALIZED:
+		return MLX5_DEV_EVENT_PORT_INITIALIZED;
+	case MLX5_PORT_CHANGE_SUBTYPE_LID:
+		return MLX5_DEV_EVENT_LID_CHANGE;
+	case MLX5_PORT_CHANGE_SUBTYPE_PKEY:
+		return MLX5_DEV_EVENT_PKEY_CHANGE;
+	case MLX5_PORT_CHANGE_SUBTYPE_GUID:
+		return MLX5_DEV_EVENT_GUID_CHANGE;
+	case MLX5_PORT_CHANGE_SUBTYPE_CLIENT_REREG:
+		return MLX5_DEV_EVENT_CLIENT_REREG;
+	}
+	return -1;
+}
+
+static void temp_warning_event(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe)
+{
+	u64 value_lsb;
+	u64 value_msb;
+
+	value_lsb = be64_to_cpu(eqe->data.temp_warning.sensor_warning_lsb);
+	value_msb = be64_to_cpu(eqe->data.temp_warning.sensor_warning_msb);
+
+	mlx5_core_warn(dev,
+		       "High temperature on sensors with bit set %llx %llx",
+		       value_msb, value_lsb);
+}
+
+static const char *mlx5_pme_status[MLX5_MODULE_STATUS_NUM] = {
+	"Cable plugged",   /* MLX5_MODULE_STATUS_PLUGGED    = 0x1 */
+	"Cable unplugged", /* MLX5_MODULE_STATUS_UNPLUGGED  = 0x2 */
+	"Cable error",     /* MLX5_MODULE_STATUS_ERROR      = 0x3 */
+};
+
+static const char *mlx5_pme_error[MLX5_MODULE_EVENT_ERROR_NUM] = {
+	"Power budget exceeded",
+	"Long Range for non MLNX cable",
+	"Bus stuck(I2C or data shorted)",
+	"No EEPROM/retry timeout",
+	"Enforce part number list",
+	"Unknown identifier",
+	"High Temperature",
+	"Bad or shorted cable/module",
+	"Unknown status",
+};
+
+static void port_module_event(struct mlx5_events *events, struct mlx5_eqe *eqe)
+{
+	enum port_module_event_status_type module_status;
+	enum port_module_event_error_type error_type;
+	struct mlx5_eqe_port_module *module_event_eqe;
+	struct mlx5_core_dev *dev = events->dev;
+	u8 module_num;
+
+	module_event_eqe = &eqe->data.port_module;
+	module_num = module_event_eqe->module;
+	module_status = module_event_eqe->module_status &
+			PORT_MODULE_EVENT_MODULE_STATUS_MASK;
+	error_type = module_event_eqe->error_type &
+		     PORT_MODULE_EVENT_ERROR_TYPE_MASK;
+
+	if (module_status < MLX5_MODULE_STATUS_ERROR) {
+		events->pme_stats.status_counters[module_status - 1]++;
+	} else if (module_status == MLX5_MODULE_STATUS_ERROR) {
+		if (error_type >= MLX5_MODULE_EVENT_ERROR_UNKNOWN)
+			/* Unknown error type */
+			error_type = MLX5_MODULE_EVENT_ERROR_UNKNOWN;
+		events->pme_stats.error_counters[error_type]++;
+	}
+
+	if (!printk_ratelimit())
+		return;
+
+	if (module_status < MLX5_MODULE_STATUS_ERROR)
+		mlx5_core_info(dev,
+			       "Port module event: module %u, %s\n",
+			       module_num, mlx5_pme_status[module_status - 1]);
+
+	else if (module_status == MLX5_MODULE_STATUS_ERROR)
+		mlx5_core_info(dev,
+			       "Port module event[error]: module %u, %s, %s\n",
+			       module_num, mlx5_pme_status[module_status - 1],
+			       mlx5_pme_error[error_type]);
+}
+
+void mlx5_get_pme_stats(struct mlx5_core_dev *dev, struct mlx5_pme_stats *stats)
+{
+	*stats = dev->priv.events->pme_stats;
+}
+
+/* Event handler for the low level mlx5_core driver.
+ * This handler will process/filter _some_ events and sometimes dispatch
+ * the equivalent mlx5_dev_event to the HCA interfaces (mlx5_ib and mlx5e)
+ *
+ * Other Major feature specific events such as
+ * clock/eswitch/fpga/FW trace and many others, are handled elsewhere, with
+ * separate notifiers callbacks, specifically by those mlx5 components.
+ */
+static int events_notifier(struct notifier_block *nb,
+			   unsigned long type, void *data)
+{
+	bool dev_event_dispatch = false;
+	enum mlx5_dev_event dev_event;
+	unsigned long dev_event_data;
+
+	struct mlx5_eqe *eqe = data;
+	struct mlx5_events *events;
+	struct mlx5_core_dev *dev;
+	u8 port;
+
+	events = mlx5_nb_cof(nb, struct mlx5_events, nb);
+	dev = events->dev;
+
+	mlx5_core_dbg(dev, "Async eqe type %s, subtype (%d)\n",
+		      eqe_type_str(eqe->type), eqe->sub_type);
+	switch (eqe->type) {
+	case MLX5_EVENT_TYPE_PORT_CHANGE:
+		port = (eqe->data.port.port >> 4) & 0xf;
+		switch (eqe->sub_type) {
+		case MLX5_PORT_CHANGE_SUBTYPE_DOWN:
+		case MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:
+		case MLX5_PORT_CHANGE_SUBTYPE_LID:
+		case MLX5_PORT_CHANGE_SUBTYPE_PKEY:
+		case MLX5_PORT_CHANGE_SUBTYPE_GUID:
+		case MLX5_PORT_CHANGE_SUBTYPE_CLIENT_REREG:
+		case MLX5_PORT_CHANGE_SUBTYPE_INITIALIZED:
+			dev_event = port_subtype2dev(eqe->sub_type);
+			dev_event_data = (unsigned long)port;
+			dev_event_dispatch = true;
+			break;
+		default:
+			mlx5_core_warn(dev, "Port event with unrecognized subtype: port %d, sub_type %d\n",
+				       port, eqe->sub_type);
+		}
+		break;
+	case MLX5_EVENT_TYPE_GENERAL_EVENT:
+		switch (eqe->sub_type) {
+		case MLX5_GENERAL_SUBTYPE_DELAY_DROP_TIMEOUT:
+			dev_event = MLX5_DEV_EVENT_DELAY_DROP_TIMEOUT;
+			dev_event_data = 0;
+			dev_event_dispatch = true;
+			break;
+		default:
+			mlx5_core_dbg(dev, "General event with unrecognized subtype: sub_type %d\n",
+				      eqe->sub_type);
+		}
+		break;
+
+	case MLX5_EVENT_TYPE_PORT_MODULE_EVENT:
+		port_module_event(events, eqe);
+		break;
+	case MLX5_EVENT_TYPE_TEMP_WARN_EVENT:
+		temp_warning_event(dev, eqe);
+		break;
+	default:
+		return NOTIFY_DONE;
+	}
+
+	if (dev->event && dev_event_dispatch)
+		dev->event(dev, dev_event, dev_event_data);
+
+	return NOTIFY_OK;
+}
+
+int mlx5_events_init(struct mlx5_core_dev *dev)
+{
+	struct mlx5_events *events = kzalloc(sizeof(*events), GFP_KERNEL);
+
+	if (!events)
+		return -ENOMEM;
+
+	events->dev = dev;
+	dev->priv.events = events;
+	return 0;
+}
+
+void mlx5_events_cleanup(struct mlx5_core_dev *dev)
+{
+	kvfree(dev->priv.events);
+}
+
+void mlx5_events_start(struct mlx5_core_dev *dev)
+{
+	struct mlx5_events *events = dev->priv.events;
+
+	MLX5_NB_INIT(&events->nb, events_notifier, NOTIFY_ANY);
+	mlx5_eq_notifier_register(dev, &events->nb);
+}
+
+void mlx5_events_stop(struct mlx5_core_dev *dev)
+{
+	struct mlx5_events *events = dev->priv.events;
+
+	mlx5_eq_notifier_unregister(dev, &events->nb);
+}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/lib/mlx5.h b/drivers/net/ethernet/mellanox/mlx5/core/lib/mlx5.h
index 7550b1cc8c6a..23317e328b0b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/lib/mlx5.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lib/mlx5.h
@@ -33,6 +33,8 @@
 #ifndef __LIB_MLX5_H__
 #define __LIB_MLX5_H__
 
+#include "mlx5_core.h"
+
 void mlx5_init_reserved_gids(struct mlx5_core_dev *dev);
 void mlx5_cleanup_reserved_gids(struct mlx5_core_dev *dev);
 int  mlx5_core_reserve_gids(struct mlx5_core_dev *dev, unsigned int count);
@@ -40,4 +42,36 @@ void mlx5_core_unreserve_gids(struct mlx5_core_dev *dev, unsigned int count);
 int  mlx5_core_reserved_gid_alloc(struct mlx5_core_dev *dev, int *gid_index);
 void mlx5_core_reserved_gid_free(struct mlx5_core_dev *dev, int gid_index);
 
+/* TODO move to lib/events.h */
+
+#define PORT_MODULE_EVENT_MODULE_STATUS_MASK 0xF
+#define PORT_MODULE_EVENT_ERROR_TYPE_MASK    0xF
+
+enum port_module_event_status_type {
+	MLX5_MODULE_STATUS_PLUGGED   = 0x1,
+	MLX5_MODULE_STATUS_UNPLUGGED = 0x2,
+	MLX5_MODULE_STATUS_ERROR     = 0x3,
+	MLX5_MODULE_STATUS_NUM       = 0x3,
+};
+
+enum  port_module_event_error_type {
+	MLX5_MODULE_EVENT_ERROR_POWER_BUDGET_EXCEEDED,
+	MLX5_MODULE_EVENT_ERROR_LONG_RANGE_FOR_NON_MLNX_CABLE_MODULE,
+	MLX5_MODULE_EVENT_ERROR_BUS_STUCK,
+	MLX5_MODULE_EVENT_ERROR_NO_EEPROM_RETRY_TIMEOUT,
+	MLX5_MODULE_EVENT_ERROR_ENFORCE_PART_NUMBER_LIST,
+	MLX5_MODULE_EVENT_ERROR_UNKNOWN_IDENTIFIER,
+	MLX5_MODULE_EVENT_ERROR_HIGH_TEMPERATURE,
+	MLX5_MODULE_EVENT_ERROR_BAD_CABLE,
+	MLX5_MODULE_EVENT_ERROR_UNKNOWN,
+	MLX5_MODULE_EVENT_ERROR_NUM,
+};
+
+struct mlx5_pme_stats {
+	u64 status_counters[MLX5_MODULE_STATUS_NUM];
+	u64 error_counters[MLX5_MODULE_EVENT_ERROR_NUM];
+};
+
+void mlx5_get_pme_stats(struct mlx5_core_dev *dev, struct mlx5_pme_stats *stats);
+
 #endif
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/port.c b/drivers/net/ethernet/mellanox/mlx5/core/port.c
index 31a9cbd85689..2b82f35f4c35 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/port.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/port.c
@@ -915,63 +915,6 @@ void mlx5_query_port_fcs(struct mlx5_core_dev *mdev, bool *supported,
 	*enabled = !!(MLX5_GET(pcmr_reg, out, fcs_chk));
 }
 
-static const char *mlx5_pme_status[MLX5_MODULE_STATUS_NUM] = {
-	"Cable plugged",   /* MLX5_MODULE_STATUS_PLUGGED    = 0x1 */
-	"Cable unplugged", /* MLX5_MODULE_STATUS_UNPLUGGED  = 0x2 */
-	"Cable error",     /* MLX5_MODULE_STATUS_ERROR      = 0x3 */
-};
-
-static const char *mlx5_pme_error[MLX5_MODULE_EVENT_ERROR_NUM] = {
-	"Power budget exceeded",
-	"Long Range for non MLNX cable",
-	"Bus stuck(I2C or data shorted)",
-	"No EEPROM/retry timeout",
-	"Enforce part number list",
-	"Unknown identifier",
-	"High Temperature",
-	"Bad or shorted cable/module",
-	"Unknown status",
-};
-
-void mlx5_port_module_event(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe)
-{
-	enum port_module_event_status_type module_status;
-	enum port_module_event_error_type error_type;
-	struct mlx5_eqe_port_module *module_event_eqe;
-	struct mlx5_priv *priv = &dev->priv;
-	u8 module_num;
-
-	module_event_eqe = &eqe->data.port_module;
-	module_num = module_event_eqe->module;
-	module_status = module_event_eqe->module_status &
-			PORT_MODULE_EVENT_MODULE_STATUS_MASK;
-	error_type = module_event_eqe->error_type &
-		     PORT_MODULE_EVENT_ERROR_TYPE_MASK;
-
-	if (module_status < MLX5_MODULE_STATUS_ERROR) {
-		priv->pme_stats.status_counters[module_status - 1]++;
-	} else if (module_status == MLX5_MODULE_STATUS_ERROR) {
-		if (error_type >= MLX5_MODULE_EVENT_ERROR_UNKNOWN)
-			/* Unknown error type */
-			error_type = MLX5_MODULE_EVENT_ERROR_UNKNOWN;
-		priv->pme_stats.error_counters[error_type]++;
-	}
-
-	if (!printk_ratelimit())
-		return;
-
-	if (module_status < MLX5_MODULE_STATUS_ERROR)
-		mlx5_core_info(dev,
-			       "Port module event: module %u, %s\n",
-			       module_num, mlx5_pme_status[module_status - 1]);
-
-	else if (module_status == MLX5_MODULE_STATUS_ERROR)
-		mlx5_core_info(dev,
-			       "Port module event[error]: module %u, %s, %s\n",
-			       module_num, mlx5_pme_status[module_status - 1],
-			       mlx5_pme_error[error_type]);
-}
-
 int mlx5_query_mtpps(struct mlx5_core_dev *mdev, u32 *mtpps, u32 mtpps_size)
 {
 	u32 in[MLX5_ST_SZ_DW(mtpps_reg)] = {0};
* Unmerged path include/linux/mlx5/driver.h
diff --git a/include/linux/mlx5/port.h b/include/linux/mlx5/port.h
index 34aed6032f86..bf4bc01ffb0c 100644
--- a/include/linux/mlx5/port.h
+++ b/include/linux/mlx5/port.h
@@ -107,9 +107,6 @@ enum mlx5e_connector_type {
 
 #define MLX5E_PROT_MASK(link_mode) (1 << link_mode)
 
-#define PORT_MODULE_EVENT_MODULE_STATUS_MASK 0xF
-#define PORT_MODULE_EVENT_ERROR_TYPE_MASK         0xF
-
 int mlx5_set_port_caps(struct mlx5_core_dev *dev, u8 port_num, u32 caps);
 int mlx5_query_port_ptys(struct mlx5_core_dev *dev, u32 *ptys,
 			 int ptys_size, int proto_mask, u8 local_port);
