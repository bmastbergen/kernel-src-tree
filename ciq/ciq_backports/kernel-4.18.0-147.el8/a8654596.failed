locking/rwsem: Enable lock event counting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Waiman Long <longman@redhat.com>
commit a8654596f0371c2604c4d475422c48f4fc6a56c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/a8654596.failed

Add lock event counting calls so that we can track the number of lock
events happening in the rwsem code.

With CONFIG_LOCK_EVENT_COUNTS on and booting a 4-socket 112-thread x86-64
system, the rwsem counts after system bootup were as follows:

  rwsem_opt_fail=261
  rwsem_opt_wlock=50636
  rwsem_rlock=445
  rwsem_rlock_fail=0
  rwsem_rlock_fast=22
  rwsem_rtrylock=810144
  rwsem_sleep_reader=441
  rwsem_sleep_writer=310
  rwsem_wake_reader=355
  rwsem_wake_writer=2335
  rwsem_wlock=261
  rwsem_wlock_fail=0
  rwsem_wtrylock=20583

It can be seen that most of the lock acquisitions in the slowpath were
write-locks in the optimistic spinning code path with no sleeping at
all. For this system, over 97% of the locks are acquired via optimistic
spinning. It illustrates the importance of optimistic spinning in
improving the performance of rwsem.

	Signed-off-by: Waiman Long <longman@redhat.com>
	Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Acked-by: Davidlohr Bueso <dbueso@suse.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Will Deacon <will.deacon@arm.com>
Link: http://lkml.kernel.org/r/20190404174320.22416-11-longman@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a8654596f0371c2604c4d475422c48f4fc6a56c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	kernel/locking/rwsem-xadd.c
#	kernel/locking/rwsem.h
diff --cc arch/Kconfig
index f03b72644902,02cb4e6a3e38..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -977,4 -880,38 +977,37 @@@ config REFCOUNT_FUL
  	  against various use-after-free conditions that can be used in
  	  security flaw exploits.
  
++<<<<<<< HEAD
++=======
+ config HAVE_ARCH_COMPILER_H
+ 	bool
+ 	help
+ 	  An architecture can select this if it provides an
+ 	  asm/compiler.h header that should be included after
+ 	  linux/compiler-*.h in order to override macro definitions that those
+ 	  headers generally provide.
+ 
+ config HAVE_ARCH_PREL32_RELOCATIONS
+ 	bool
+ 	help
+ 	  May be selected by an architecture if it supports place-relative
+ 	  32-bit relocations, both in the toolchain and in the module loader,
+ 	  in which case relative references can be used in special sections
+ 	  for PCI fixup, initcalls etc which are only half the size on 64 bit
+ 	  architectures, and don't require runtime relocation on relocatable
+ 	  kernels.
+ 
+ config ARCH_USE_MEMREMAP_PROT
+ 	bool
+ 
+ config LOCK_EVENT_COUNTS
+ 	bool "Locking event counts collection"
+ 	depends on DEBUG_FS
+ 	---help---
+ 	  Enable light-weight counting of various locking related events
+ 	  in the system with minimal performance impact. This reduces
+ 	  the chance of application behavior change because of timing
+ 	  differences. The counts are reported via debugfs.
+ 
++>>>>>>> a8654596f037 (locking/rwsem: Enable lock event counting)
  source "kernel/gcov/Kconfig"
 -
 -source "scripts/gcc-plugins/Kconfig"
 -
 -endmenu
diff --cc kernel/locking/rwsem-xadd.c
index d663e90ae961,6b3ee9948bf1..000000000000
--- a/kernel/locking/rwsem-xadd.c
+++ b/kernel/locking/rwsem-xadd.c
@@@ -438,6 -439,8 +442,11 @@@ __rwsem_down_read_failed_common(struct 
  		 */
  		if (atomic_long_read(&sem->count) >= 0) {
  			raw_spin_unlock_irq(&sem->wait_lock);
++<<<<<<< HEAD
++=======
+ 			rwsem_set_reader_owned(sem);
+ 			lockevent_inc(rwsem_rlock_fast);
++>>>>>>> a8654596f037 (locking/rwsem: Enable lock event counting)
  			return sem;
  		}
  		adjustment += RWSEM_WAITING_BIAS;
diff --cc kernel/locking/rwsem.h
index bad2bca0268b,37db17890e36..000000000000
--- a/kernel/locking/rwsem.h
+++ b/kernel/locking/rwsem.h
@@@ -132,3 -160,144 +134,147 @@@ static inline void rwsem_clear_reader_o
  {
  }
  #endif
++<<<<<<< HEAD
++=======
+ 
+ extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
+ extern struct rw_semaphore *rwsem_down_read_failed_killable(struct rw_semaphore *sem);
+ extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
+ extern struct rw_semaphore *rwsem_down_write_failed_killable(struct rw_semaphore *sem);
+ extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem);
+ extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
+ 
+ /*
+  * lock for reading
+  */
+ static inline void __down_read(struct rw_semaphore *sem)
+ {
+ 	if (unlikely(atomic_long_inc_return_acquire(&sem->count) <= 0)) {
+ 		rwsem_down_read_failed(sem);
+ 		DEBUG_RWSEMS_WARN_ON(!((unsigned long)sem->owner &
+ 					RWSEM_READER_OWNED), sem);
+ 	} else {
+ 		rwsem_set_reader_owned(sem);
+ 	}
+ }
+ 
+ static inline int __down_read_killable(struct rw_semaphore *sem)
+ {
+ 	if (unlikely(atomic_long_inc_return_acquire(&sem->count) <= 0)) {
+ 		if (IS_ERR(rwsem_down_read_failed_killable(sem)))
+ 			return -EINTR;
+ 		DEBUG_RWSEMS_WARN_ON(!((unsigned long)sem->owner &
+ 					RWSEM_READER_OWNED), sem);
+ 	} else {
+ 		rwsem_set_reader_owned(sem);
+ 	}
+ 	return 0;
+ }
+ 
+ static inline int __down_read_trylock(struct rw_semaphore *sem)
+ {
+ 	/*
+ 	 * Optimize for the case when the rwsem is not locked at all.
+ 	 */
+ 	long tmp = RWSEM_UNLOCKED_VALUE;
+ 
+ 	lockevent_inc(rwsem_rtrylock);
+ 	do {
+ 		if (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp,
+ 					tmp + RWSEM_ACTIVE_READ_BIAS)) {
+ 			rwsem_set_reader_owned(sem);
+ 			return 1;
+ 		}
+ 	} while (tmp >= 0);
+ 	return 0;
+ }
+ 
+ /*
+  * lock for writing
+  */
+ static inline void __down_write(struct rw_semaphore *sem)
+ {
+ 	long tmp;
+ 
+ 	tmp = atomic_long_add_return_acquire(RWSEM_ACTIVE_WRITE_BIAS,
+ 					     &sem->count);
+ 	if (unlikely(tmp != RWSEM_ACTIVE_WRITE_BIAS))
+ 		rwsem_down_write_failed(sem);
+ 	rwsem_set_owner(sem);
+ }
+ 
+ static inline int __down_write_killable(struct rw_semaphore *sem)
+ {
+ 	long tmp;
+ 
+ 	tmp = atomic_long_add_return_acquire(RWSEM_ACTIVE_WRITE_BIAS,
+ 					     &sem->count);
+ 	if (unlikely(tmp != RWSEM_ACTIVE_WRITE_BIAS))
+ 		if (IS_ERR(rwsem_down_write_failed_killable(sem)))
+ 			return -EINTR;
+ 	rwsem_set_owner(sem);
+ 	return 0;
+ }
+ 
+ static inline int __down_write_trylock(struct rw_semaphore *sem)
+ {
+ 	long tmp;
+ 
+ 	lockevent_inc(rwsem_wtrylock);
+ 	tmp = atomic_long_cmpxchg_acquire(&sem->count, RWSEM_UNLOCKED_VALUE,
+ 		      RWSEM_ACTIVE_WRITE_BIAS);
+ 	if (tmp == RWSEM_UNLOCKED_VALUE) {
+ 		rwsem_set_owner(sem);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
+ /*
+  * unlock after reading
+  */
+ static inline void __up_read(struct rw_semaphore *sem)
+ {
+ 	long tmp;
+ 
+ 	DEBUG_RWSEMS_WARN_ON(!((unsigned long)sem->owner & RWSEM_READER_OWNED),
+ 				sem);
+ 	rwsem_clear_reader_owned(sem);
+ 	tmp = atomic_long_dec_return_release(&sem->count);
+ 	if (unlikely(tmp < -1 && (tmp & RWSEM_ACTIVE_MASK) == 0))
+ 		rwsem_wake(sem);
+ }
+ 
+ /*
+  * unlock after writing
+  */
+ static inline void __up_write(struct rw_semaphore *sem)
+ {
+ 	DEBUG_RWSEMS_WARN_ON(sem->owner != current, sem);
+ 	rwsem_clear_owner(sem);
+ 	if (unlikely(atomic_long_sub_return_release(RWSEM_ACTIVE_WRITE_BIAS,
+ 						    &sem->count) < 0))
+ 		rwsem_wake(sem);
+ }
+ 
+ /*
+  * downgrade write lock to read lock
+  */
+ static inline void __downgrade_write(struct rw_semaphore *sem)
+ {
+ 	long tmp;
+ 
+ 	/*
+ 	 * When downgrading from exclusive to shared ownership,
+ 	 * anything inside the write-locked region cannot leak
+ 	 * into the read side. In contrast, anything in the
+ 	 * read-locked region is ok to be re-ordered into the
+ 	 * write side. As such, rely on RELEASE semantics.
+ 	 */
+ 	DEBUG_RWSEMS_WARN_ON(sem->owner != current, sem);
+ 	tmp = atomic_long_add_return_release(-RWSEM_WAITING_BIAS, &sem->count);
+ 	rwsem_set_reader_owned(sem);
+ 	if (tmp < 0)
+ 		rwsem_downgrade_wake(sem);
+ }
++>>>>>>> a8654596f037 (locking/rwsem: Enable lock event counting)
* Unmerged path arch/Kconfig
diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index 8b4d2e180475..ad7668cfc9da 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -48,3 +48,20 @@ LOCK_EVENT(lock_use_node3)	/* # of locking ops that use 3rd percpu node */
 LOCK_EVENT(lock_use_node4)	/* # of locking ops that use 4th percpu node */
 LOCK_EVENT(lock_no_node)	/* # of locking ops w/o using percpu node    */
 #endif /* CONFIG_QUEUED_SPINLOCKS */
+
+/*
+ * Locking events for rwsem
+ */
+LOCK_EVENT(rwsem_sleep_reader)	/* # of reader sleeps			*/
+LOCK_EVENT(rwsem_sleep_writer)	/* # of writer sleeps			*/
+LOCK_EVENT(rwsem_wake_reader)	/* # of reader wakeups			*/
+LOCK_EVENT(rwsem_wake_writer)	/* # of writer wakeups			*/
+LOCK_EVENT(rwsem_opt_wlock)	/* # of write locks opt-spin acquired	*/
+LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
+LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
+LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
+LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/
+LOCK_EVENT(rwsem_rtrylock)	/* # of read trylock calls		*/
+LOCK_EVENT(rwsem_wlock)		/* # of write locks acquired		*/
+LOCK_EVENT(rwsem_wlock_fail)	/* # of failed write lock acquisitions	*/
+LOCK_EVENT(rwsem_wtrylock)	/* # of write trylock calls		*/
* Unmerged path kernel/locking/rwsem-xadd.c
* Unmerged path kernel/locking/rwsem.h
