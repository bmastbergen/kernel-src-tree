RDMA/mlx5: Initialize ib_device_ops struct

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Kamal Heib <kamalheib1@gmail.com>
commit 96458233ee73b6082f75c1a55c9a4ad6ea7913d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/96458233.failed

Initialize ib_device_ops with the supported operations using
ib_set_device_ops().

	Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 96458233ee73b6082f75c1a55c9a4ad6ea7913d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index 1177d9389ab8,47d9cd260846..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -5475,65 -5938,17 +5563,65 @@@ int mlx5_ib_stage_caps_init(struct mlx5
  		(1ull << IB_USER_VERBS_EX_CMD_CREATE_CQ)	|
  		(1ull << IB_USER_VERBS_EX_CMD_CREATE_QP)	|
  		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_QP)	|
- 		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_CQ);
+ 		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_CQ)	|
+ 		(1ull << IB_USER_VERBS_EX_CMD_CREATE_FLOW)	|
+ 		(1ull << IB_USER_VERBS_EX_CMD_DESTROY_FLOW);
  
++<<<<<<< HEAD
 +	dev->ib_dev.query_device	= mlx5_ib_query_device;
 +	dev->ib_dev.get_link_layer	= mlx5_ib_port_link_layer;
 +	dev->ib_dev.query_gid		= mlx5_ib_query_gid;
 +	dev->ib_dev.add_gid		= mlx5_ib_add_gid;
 +	dev->ib_dev.del_gid		= mlx5_ib_del_gid;
 +	dev->ib_dev.query_pkey		= mlx5_ib_query_pkey;
 +	dev->ib_dev.modify_device	= mlx5_ib_modify_device;
 +	dev->ib_dev.modify_port		= mlx5_ib_modify_port;
 +	dev->ib_dev.alloc_ucontext	= mlx5_ib_alloc_ucontext;
 +	dev->ib_dev.dealloc_ucontext	= mlx5_ib_dealloc_ucontext;
 +	dev->ib_dev.mmap		= mlx5_ib_mmap;
 +	dev->ib_dev.alloc_pd		= mlx5_ib_alloc_pd;
 +	dev->ib_dev.dealloc_pd		= mlx5_ib_dealloc_pd;
 +	dev->ib_dev.create_ah		= mlx5_ib_create_ah;
 +	dev->ib_dev.query_ah		= mlx5_ib_query_ah;
 +	dev->ib_dev.destroy_ah		= mlx5_ib_destroy_ah;
 +	dev->ib_dev.create_srq		= mlx5_ib_create_srq;
 +	dev->ib_dev.modify_srq		= mlx5_ib_modify_srq;
 +	dev->ib_dev.query_srq		= mlx5_ib_query_srq;
 +	dev->ib_dev.destroy_srq		= mlx5_ib_destroy_srq;
 +	dev->ib_dev.post_srq_recv	= mlx5_ib_post_srq_recv;
 +	dev->ib_dev.create_qp		= mlx5_ib_create_qp;
 +	dev->ib_dev.modify_qp		= mlx5_ib_modify_qp;
 +	dev->ib_dev.query_qp		= mlx5_ib_query_qp;
 +	dev->ib_dev.destroy_qp		= mlx5_ib_destroy_qp;
 +	dev->ib_dev.post_send		= mlx5_ib_post_send;
 +	dev->ib_dev.post_recv		= mlx5_ib_post_recv;
 +	dev->ib_dev.create_cq		= mlx5_ib_create_cq;
 +	dev->ib_dev.modify_cq		= mlx5_ib_modify_cq;
 +	dev->ib_dev.resize_cq		= mlx5_ib_resize_cq;
 +	dev->ib_dev.destroy_cq		= mlx5_ib_destroy_cq;
 +	dev->ib_dev.poll_cq		= mlx5_ib_poll_cq;
 +	dev->ib_dev.req_notify_cq	= mlx5_ib_arm_cq;
 +	dev->ib_dev.get_dma_mr		= mlx5_ib_get_dma_mr;
 +	dev->ib_dev.reg_user_mr		= mlx5_ib_reg_user_mr;
 +	dev->ib_dev.rereg_user_mr	= mlx5_ib_rereg_user_mr;
 +	dev->ib_dev.dereg_mr		= mlx5_ib_dereg_mr;
 +	dev->ib_dev.attach_mcast	= mlx5_ib_mcg_attach;
 +	dev->ib_dev.detach_mcast	= mlx5_ib_mcg_detach;
 +	dev->ib_dev.process_mad		= mlx5_ib_process_mad;
 +	dev->ib_dev.alloc_mr		= mlx5_ib_alloc_mr;
 +	dev->ib_dev.map_mr_sg		= mlx5_ib_map_mr_sg;
 +	dev->ib_dev.check_mr_status	= mlx5_ib_check_mr_status;
 +	dev->ib_dev.get_dev_fw_str      = get_dev_fw_str;
 +	dev->ib_dev.get_vector_affinity	= mlx5_ib_get_vector_affinity;
++=======
++>>>>>>> 96458233ee73 (RDMA/mlx5: Initialize ib_device_ops struct)
  	if (MLX5_CAP_GEN(mdev, ipoib_enhanced_offloads) &&
  	    IS_ENABLED(CONFIG_MLX5_CORE_IPOIB))
- 		dev->ib_dev.rdma_netdev_get_params = mlx5_ib_rn_get_params;
- 
- 	if (mlx5_core_is_pf(mdev)) {
- 		dev->ib_dev.get_vf_config	= mlx5_ib_get_vf_config;
- 		dev->ib_dev.set_vf_link_state	= mlx5_ib_set_vf_link_state;
- 		dev->ib_dev.get_vf_stats	= mlx5_ib_get_vf_stats;
- 		dev->ib_dev.set_vf_guid		= mlx5_ib_set_vf_guid;
- 	}
+ 		ib_set_device_ops(&dev->ib_dev,
+ 				  &mlx5_ib_dev_ipoib_enhanced_ops);
  
- 	dev->ib_dev.disassociate_ucontext = mlx5_ib_disassociate_ucontext;
+ 	if (mlx5_core_is_pf(mdev))
+ 		ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_sriov_ops);
  
  	dev->umr_fence = mlx5_get_umr_fence(MLX5_CAP_GEN(mdev, umr_fence));
  
@@@ -5549,29 -5963,21 +5636,29 @@@
  		dev->ib_dev.uverbs_cmd_mask |=
  			(1ull << IB_USER_VERBS_CMD_OPEN_XRCD) |
  			(1ull << IB_USER_VERBS_CMD_CLOSE_XRCD);
+ 		ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_xrc_ops);
  	}
  
- 	if (MLX5_CAP_DEV_MEM(mdev, memic)) {
- 		dev->ib_dev.alloc_dm = mlx5_ib_alloc_dm;
- 		dev->ib_dev.dealloc_dm = mlx5_ib_dealloc_dm;
- 		dev->ib_dev.reg_dm_mr = mlx5_ib_reg_dm_mr;
- 	}
+ 	if (MLX5_CAP_DEV_MEM(mdev, memic))
+ 		ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_dm_ops);
  
++<<<<<<< HEAD
 +	dev->ib_dev.create_flow	= mlx5_ib_create_flow;
 +	dev->ib_dev.destroy_flow = mlx5_ib_destroy_flow;
 +	dev->ib_dev.uverbs_ex_cmd_mask |=
 +			(1ull << IB_USER_VERBS_EX_CMD_CREATE_FLOW) |
 +			(1ull << IB_USER_VERBS_EX_CMD_DESTROY_FLOW);
 +	dev->ib_dev.create_flow_action_esp = mlx5_ib_create_flow_action_esp;
 +	dev->ib_dev.destroy_flow_action = mlx5_ib_destroy_flow_action;
 +	dev->ib_dev.modify_flow_action_esp = mlx5_ib_modify_flow_action_esp;
++=======
+ 	if (mlx5_accel_ipsec_device_caps(dev->mdev) &
+ 	    MLX5_ACCEL_IPSEC_CAP_DEVICE)
+ 		ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_flow_ipsec_ops);
++>>>>>>> 96458233ee73 (RDMA/mlx5: Initialize ib_device_ops struct)
  	dev->ib_dev.driver_id = RDMA_DRIVER_MLX5;
- 	dev->ib_dev.create_counters = mlx5_ib_create_counters;
- 	dev->ib_dev.destroy_counters = mlx5_ib_destroy_counters;
- 	dev->ib_dev.read_counters = mlx5_ib_read_counters;
+ 	ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_ops);
  
 -	if (IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS))
 -		dev->ib_dev.driver_def = mlx5_ib_defs;
 -
  	err = init_node_data(dev);
  	if (err)
  		return err;
@@@ -5600,9 -6012,18 +5693,22 @@@ int mlx5_ib_stage_rep_non_default_cb(st
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int mlx5_ib_stage_common_roce_init(struct mlx5_ib_dev *dev,
 +					  u8 port_num)
++=======
+ static const struct ib_device_ops mlx5_ib_dev_common_roce_ops = {
+ 	.create_rwq_ind_table = mlx5_ib_create_rwq_ind_table,
+ 	.create_wq = mlx5_ib_create_wq,
+ 	.destroy_rwq_ind_table = mlx5_ib_destroy_rwq_ind_table,
+ 	.destroy_wq = mlx5_ib_destroy_wq,
+ 	.get_netdev = mlx5_ib_get_netdev,
+ 	.modify_wq = mlx5_ib_modify_wq,
+ };
+ 
+ static int mlx5_ib_stage_common_roce_init(struct mlx5_ib_dev *dev)
++>>>>>>> 96458233ee73 (RDMA/mlx5: Initialize ib_device_ops struct)
  {
 -	u8 port_num;
  	int i;
  
  	for (i = 0; i < dev->num_ports; i++) {
@@@ -5624,7 -6038,10 +5723,8 @@@
  			(1ull << IB_USER_VERBS_EX_CMD_DESTROY_WQ) |
  			(1ull << IB_USER_VERBS_EX_CMD_CREATE_RWQ_IND_TBL) |
  			(1ull << IB_USER_VERBS_EX_CMD_DESTROY_RWQ_IND_TBL);
+ 	ib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_common_roce_ops);
  
 -	port_num = mlx5_core_native_port_num(dev->mdev) - 1;
 -
  	return mlx5_add_netdev_notifier(dev, port_num);
  }
  
@@@ -5719,6 -6132,16 +5819,19 @@@ static int mlx5_ib_stage_odp_init(struc
  	return mlx5_ib_odp_init_one(dev);
  }
  
++<<<<<<< HEAD
++=======
+ void mlx5_ib_stage_odp_cleanup(struct mlx5_ib_dev *dev)
+ {
+ 	mlx5_ib_odp_cleanup_one(dev);
+ }
+ 
+ static const struct ib_device_ops mlx5_ib_dev_hw_stats_ops = {
+ 	.alloc_hw_stats = mlx5_ib_alloc_hw_stats,
+ 	.get_hw_stats = mlx5_ib_get_hw_stats,
+ };
+ 
++>>>>>>> 96458233ee73 (RDMA/mlx5: Initialize ib_device_ops struct)
  int mlx5_ib_stage_counters_init(struct mlx5_ib_dev *dev)
  {
  	if (MLX5_CAP_GEN(dev->mdev, max_qp_cnt)) {
* Unmerged path drivers/infiniband/hw/mlx5/main.c
