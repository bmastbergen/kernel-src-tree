x86/kvm/mmu: fix switch between root and guest MMUs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit ad7dc69aeb23138cc23c406cac25003b97e8ee17
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/ad7dc69a.failed

Commit 14c07ad89f4d ("x86/kvm/mmu: introduce guest_mmu") brought one subtle
change: previously, when switching back from L2 to L1, we were resetting
MMU hooks (like mmu->get_cr3()) in kvm_init_mmu() called from
nested_vmx_load_cr3() and now we do that in nested_ept_uninit_mmu_context()
when we re-target vcpu->arch.mmu pointer.
The change itself looks logical: if nested_ept_init_mmu_context() changes
something than nested_ept_uninit_mmu_context() restores it back. There is,
however, one thing: the following call chain:

 nested_vmx_load_cr3()
  kvm_mmu_new_cr3()
    __kvm_mmu_new_cr3()
      fast_cr3_switch()
        cached_root_available()

now happens with MMU hooks pointing to the new MMU (root MMU in our case)
while previously it was happening with the old one. cached_root_available()
tries to stash current root but it is incorrect to read current CR3 with
mmu->get_cr3(), we need to use old_mmu->get_cr3() which in case we're
switching from L2 to L1 is guest_mmu. (BTW, in shadow page tables case this
is a non-issue because we don't switch MMU).

While we could've tried to guess that we're switching between MMUs and call
the right ->get_cr3() from cached_root_available() this seems to be overly
complicated. Instead, just stash the corresponding CR3 when setting
root_hpa and make cached_root_available() use the stashed value.

Fixes: 14c07ad89f4d ("x86/kvm/mmu: introduce guest_mmu")
	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Cc: stable@vger.kernel.org
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ad7dc69aeb23138cc23c406cac25003b97e8ee17)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 55a047693562,6e62ed3852ac..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -5426,6 -5485,81 +5433,84 @@@ void kvm_disable_tdp(void
  }
  EXPORT_SYMBOL_GPL(kvm_disable_tdp);
  
++<<<<<<< HEAD
++=======
+ static void free_mmu_pages(struct kvm_vcpu *vcpu)
+ {
+ 	free_page((unsigned long)vcpu->arch.mmu->pae_root);
+ 	free_page((unsigned long)vcpu->arch.mmu->lm_root);
+ }
+ 
+ static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
+ {
+ 	struct page *page;
+ 	int i;
+ 
+ 	if (tdp_enabled)
+ 		return 0;
+ 
+ 	/*
+ 	 * When emulating 32-bit mode, cr3 is only 32 bits even on x86_64.
+ 	 * Therefore we need to allocate shadow page tables in the first
+ 	 * 4GB of memory, which happens to fit the DMA32 zone.
+ 	 */
+ 	page = alloc_page(GFP_KERNEL | __GFP_DMA32);
+ 	if (!page)
+ 		return -ENOMEM;
+ 
+ 	vcpu->arch.mmu->pae_root = page_address(page);
+ 	for (i = 0; i < 4; ++i)
+ 		vcpu->arch.mmu->pae_root[i] = INVALID_PAGE;
+ 
+ 	return 0;
+ }
+ 
+ int kvm_mmu_create(struct kvm_vcpu *vcpu)
+ {
+ 	uint i;
+ 
+ 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ 
+ 	vcpu->arch.root_mmu.root_hpa = INVALID_PAGE;
+ 	vcpu->arch.root_mmu.root_cr3 = 0;
+ 	vcpu->arch.root_mmu.translate_gpa = translate_gpa;
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 		vcpu->arch.root_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
+ 
+ 	vcpu->arch.guest_mmu.root_hpa = INVALID_PAGE;
+ 	vcpu->arch.guest_mmu.root_cr3 = 0;
+ 	vcpu->arch.guest_mmu.translate_gpa = translate_gpa;
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 		vcpu->arch.guest_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
+ 
+ 	vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
+ 	return alloc_mmu_pages(vcpu);
+ }
+ 
+ static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
+ 			struct kvm_memory_slot *slot,
+ 			struct kvm_page_track_notifier_node *node)
+ {
+ 	kvm_mmu_invalidate_zap_all_pages(kvm);
+ }
+ 
+ void kvm_mmu_init_vm(struct kvm *kvm)
+ {
+ 	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;
+ 
+ 	node->track_write = kvm_mmu_pte_write;
+ 	node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
+ 	kvm_page_track_register_notifier(kvm, node);
+ }
+ 
+ void kvm_mmu_uninit_vm(struct kvm *kvm)
+ {
+ 	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;
+ 
+ 	kvm_page_track_unregister_notifier(kvm, node);
+ }
++>>>>>>> ad7dc69aeb23 (x86/kvm/mmu: fix switch between root and guest MMUs)
  
  /* The return value indicates if tlb flush on all vcpus is needed. */
  typedef bool (*slot_level_handler) (struct kvm *kvm, struct kvm_rmap_head *rmap_head);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4f4095c7b53d..fff0fd5fd63c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -379,6 +379,7 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
+	gpa_t root_cr3;
 	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;
* Unmerged path arch/x86/kvm/mmu.c
