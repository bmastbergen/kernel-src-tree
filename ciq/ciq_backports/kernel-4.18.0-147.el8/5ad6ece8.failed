KVM: VMX: Move vmx_vcpu_run()'s VM-Enter asm blob to a helper function

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 5ad6ece869d46c834976ce383ef200f9116881f8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/5ad6ece8.failed

...along with the function's STACK_FRAME_NON_STANDARD tag.  Moving the
asm blob results in a significantly smaller amount of code that is
marked with STACK_FRAME_NON_STANDARD, which makes it far less likely
that gcc will split the function and trigger a spurious objtool warning.
As a bonus, removing STACK_FRAME_NON_STANDARD from vmx_vcpu_run() allows
the bulk of code to be properly checked by objtool.

Because %rbp is not loaded via VMCS fields, vmx_vcpu_run() must manually
save/restore the host's RBP and load the guest's RBP prior to calling
vmx_vmenter().  Modifying %rbp triggers objtool's stack validation code,
and so vmx_vcpu_run() is tagged with STACK_FRAME_NON_STANDARD since it's
impossible to avoid modifying %rbp.

Unfortunately, vmx_vcpu_run() is also a gigantic function that gcc will
split into separate functions, e.g. so that pieces of the function can
be inlined.  Splitting the function means that the compiled Elf file
will contain one or more vmx_vcpu_run.part.* functions in addition to
a vmx_vcpu_run function.  Depending on where the function is split,
objtool may warn about a "call without frame pointer save/setup" in
vmx_vcpu_run.part.* since objtool's stack validation looks for exact
names when whitelisting functions tagged with STACK_FRAME_NON_STANDARD.

Up until recently, the undesirable function splitting was effectively
blocked because vmx_vcpu_run() was tagged with __noclone.  At the time,
__noclone had an unintended side effect that put vmx_vcpu_run() into a
separate optimization unit, which in turn prevented gcc from inlining
the function (or any of its own function calls) and thus eliminated gcc's
motivation to split the function.  Removing the __noclone attribute
allowed gcc to optimize vmx_vcpu_run(), exposing the objtool warning.

Kudos to Qian Cai for root causing that the fnsplit optimization is what
caused objtool to complain.

Fixes: 453eafbe65f7 ("KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines")
	Tested-by: Qian Cai <cai@lca.pw>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Reported-by: kbuild test robot <lkp@intel.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5ad6ece869d46c834976ce383ef200f9116881f8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 50f4aa38b1a2,99c898523c5e..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -10357,1068 -4504,1047 +10357,1144 @@@ static int vmx_handle_exit(struct kvm_v
  		return 0;
  	}
  
 -	if (is_page_fault(intr_info)) {
 -		cr2 = vmcs_readl(EXIT_QUALIFICATION);
 -		/* EPT won't cause page fault directly */
 -		WARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);
 -		return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked)) {
 +		if (vmx_interrupt_allowed(vcpu)) {
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
 +			   vcpu->arch.nmi_pending) {
 +			/*
 +			 * This CPU don't support us in finding the end of an
 +			 * NMI-blocked window if the guest runs with IRQs
 +			 * disabled. So we pull the trigger after 1 s of
 +			 * futile waiting, but inform the user about this.
 +			 */
 +			printk(KERN_WARNING "%s: Breaking out of NMI-blocked "
 +			       "state on VCPU %d after 1 s timeout\n",
 +			       __func__, vcpu->vcpu_id);
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		}
  	}
  
 -	ex_no = intr_info & INTR_INFO_VECTOR_MASK;
 +	if (exit_reason < kvm_vmx_max_exit_handlers
 +	    && kvm_vmx_exit_handlers[exit_reason])
 +		return kvm_vmx_exit_handlers[exit_reason](vcpu);
 +	else {
 +		vcpu_unimpl(vcpu, "vmx: unexpected exit reason 0x%x\n",
 +				exit_reason);
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +}
  
 -	if (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))
 -		return handle_rmode_exception(vcpu, ex_no, error_code);
 +/*
 + * Software based L1D cache flush which is used when microcode providing
 + * the cache control MSR is not loaded.
 + *
 + * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
 + * flush it is required to read in 64 KiB because the replacement algorithm
 + * is not exactly LRU. This could be sized at runtime via topology
 + * information but as all relevant affected CPUs have 32KiB L1D cache size
 + * there is no point in doing so.
 + */
 +static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 +{
 +	int size = PAGE_SIZE << L1D_CACHE_ORDER;
  
 -	switch (ex_no) {
 -	case AC_VECTOR:
 -		kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 -		return 1;
 -	case DB_VECTOR:
 -		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 -		if (!(vcpu->guest_debug &
 -		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= dr6 | DR6_RTM;
 -			if (is_icebp(intr_info))
 -				skip_emulated_instruction(vcpu);
 +	/*
 +	 * This code is only executed when the the flush mode is 'cond' or
 +	 * 'always'
 +	 */
 +	if (static_branch_likely(&vmx_l1d_flush_cond)) {
 +		bool flush_l1d;
  
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 -		}
 -		kvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;
 -		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 -		/* fall through */
 -	case BP_VECTOR:
  		/*
 -		 * Update instruction length as we may reinject #BP from
 -		 * user space while in guest debugging mode. Reading it for
 -		 * #DB as well causes no harm, it is not used in that case.
 +		 * Clear the per-vcpu flush bit, it gets set again
 +		 * either from vcpu_run() or from one of the unsafe
 +		 * VMEXIT handlers.
  		 */
 -		vmx->vcpu.arch.event_exit_inst_len =
 -			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 -		kvm_run->exit_reason = KVM_EXIT_DEBUG;
 -		rip = kvm_rip_read(vcpu);
 -		kvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;
 -		kvm_run->debug.arch.exception = ex_no;
 -		break;
 -	default:
 -		kvm_run->exit_reason = KVM_EXIT_EXCEPTION;
 -		kvm_run->ex.exception = ex_no;
 -		kvm_run->ex.error_code = error_code;
 -		break;
 +		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 +		vcpu->arch.l1tf_flush_l1d = false;
 +
 +		/*
 +		 * Clear the per-cpu flush bit, it gets set again from
 +		 * the interrupt handlers.
 +		 */
 +		flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
 +		kvm_clear_cpu_l1tf_flush_l1d();
 +
 +		if (!flush_l1d)
 +			return;
  	}
 -	return 0;
 -}
  
 -static int handle_external_interrupt(struct kvm_vcpu *vcpu)
 -{
 -	++vcpu->stat.irq_exits;
 -	return 1;
 -}
 +	vcpu->stat.l1d_flush++;
  
 -static int handle_triple_fault(struct kvm_vcpu *vcpu)
 -{
 -	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
 -	vcpu->mmio_needed = 0;
 -	return 0;
 +	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 +		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 +		return;
 +	}
 +
 +	asm volatile(
 +		/* First ensure the pages are in the TLB */
 +		"xorl	%%eax, %%eax\n"
 +		".Lpopulate_tlb:\n\t"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$4096, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lpopulate_tlb\n\t"
 +		"xorl	%%eax, %%eax\n\t"
 +		"cpuid\n\t"
 +		/* Now fill the cache */
 +		"xorl	%%eax, %%eax\n"
 +		".Lfill_cache:\n"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$64, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lfill_cache\n\t"
 +		"lfence\n"
 +		:: [flush_pages] "r" (vmx_l1d_flush_pages),
 +		    [size] "r" (size)
 +		: "eax", "ebx", "ecx", "edx");
  }
  
 -static int handle_io(struct kvm_vcpu *vcpu)
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
 -	unsigned long exit_qualification;
 -	int size, in, string;
 -	unsigned port;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	string = (exit_qualification & 16) != 0;
 -
 -	++vcpu->stat.io_exits;
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
  
 -	if (string)
 -		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	if (is_guest_mode(vcpu) &&
 +		nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
 +		return;
  
 -	port = exit_qualification >> 16;
 -	size = (exit_qualification & 7) + 1;
 -	in = (exit_qualification & 8) != 0;
 +	if (irr == -1 || tpr < irr) {
 +		vmcs_write32(TPR_THRESHOLD, 0);
 +		return;
 +	}
  
 -	return kvm_fast_pio(vcpu, size, port, in);
 +	vmcs_write32(TPR_THRESHOLD, irr);
  }
  
 -static void
 -vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
  {
 -	/*
 -	 * Patch in the VMCALL instruction:
 -	 */
 -	hypercall[0] = 0x0f;
 -	hypercall[1] = 0x01;
 -	hypercall[2] = 0xc1;
 -}
 +	u32 sec_exec_control;
  
 -/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 -static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
 -{
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	if (!lapic_in_kernel(vcpu))
 +		return;
  
 -		/*
 -		 * We get here when L2 changed cr0 in a way that did not change
 -		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 -		 * but did change L0 shadowed bits. So we first calculate the
 -		 * effective cr0 value that L1 would like to write into the
 -		 * hardware. It consists of the L2-owned bits from the new
 -		 * value combined with the L1-owned bits from L1's guest_cr0.
 -		 */
 -		val = (val & ~vmcs12->cr0_guest_host_mask) |
 -			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
 +	if (!flexpriority_enabled &&
 +	    !cpu_has_vmx_virtualize_x2apic_mode())
 +		return;
  
 -		if (!nested_guest_cr0_valid(vcpu, val))
 -			return 1;
 +	/* Postpone execution until vmcs01 is the current VMCS. */
 +	if (is_guest_mode(vcpu)) {
 +		to_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;
 +		return;
 +	}
  
 -		if (kvm_set_cr0(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR0_READ_SHADOW, orig_val);
 -		return 0;
 -	} else {
 -		if (to_vmx(vcpu)->nested.vmxon &&
 -		    !nested_host_cr0_valid(vcpu, val))
 -			return 1;
 +	sec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
 +	sec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +			      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
  
 -		return kvm_set_cr0(vcpu, val);
 +	switch (kvm_get_apic_mode(vcpu)) {
 +	case LAPIC_MODE_INVALID:
 +		WARN_ONCE(true, "Invalid local APIC state");
 +	case LAPIC_MODE_DISABLED:
 +		break;
 +	case LAPIC_MODE_XAPIC:
 +		if (flexpriority_enabled) {
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +			vmx_flush_tlb(vcpu, true);
 +		}
 +		break;
 +	case LAPIC_MODE_X2APIC:
 +		if (cpu_has_vmx_virtualize_x2apic_mode())
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
 +		break;
  	}
 +	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);
 +
 +	vmx_update_msr_bitmap(vcpu);
  }
  
 -static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 +static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
  {
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 -
 -		/* analogously to handle_set_cr0 */
 -		val = (val & ~vmcs12->cr4_guest_host_mask) |
 -			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 -		if (kvm_set_cr4(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR4_READ_SHADOW, orig_val);
 -		return 0;
 -	} else
 -		return kvm_set_cr4(vcpu, val);
 +	if (!is_guest_mode(vcpu)) {
 +		vmcs_write64(APIC_ACCESS_ADDR, hpa);
 +		vmx_flush_tlb(vcpu, true);
 +	}
  }
  
 -static int handle_desc(struct kvm_vcpu *vcpu)
 +static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
  {
 -	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	u16 status;
 +	u8 old;
 +
 +	if (max_isr == -1)
 +		max_isr = 0;
 +
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = status >> 8;
 +	if (max_isr != old) {
 +		status &= 0xff;
 +		status |= max_isr << 8;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
  }
  
 -static int handle_cr(struct kvm_vcpu *vcpu)
 +static void vmx_set_rvi(int vector)
  {
 -	unsigned long exit_qualification, val;
 -	int cr;
 -	int reg;
 -	int err;
 -	int ret;
 +	u16 status;
 +	u8 old;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	cr = exit_qualification & 15;
 -	reg = (exit_qualification >> 8) & 15;
 -	switch ((exit_qualification >> 4) & 3) {
 -	case 0: /* mov to cr */
 -		val = kvm_register_readl(vcpu, reg);
 -		trace_kvm_cr_write(cr, val);
 -		switch (cr) {
 -		case 0:
 -			err = handle_set_cr0(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			err = kvm_set_cr3(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 4:
 -			err = handle_set_cr4(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 8: {
 -				u8 cr8_prev = kvm_get_cr8(vcpu);
 -				u8 cr8 = (u8)val;
 -				err = kvm_set_cr8(vcpu, cr8);
 -				ret = kvm_complete_insn_gp(vcpu, err);
 -				if (lapic_in_kernel(vcpu))
 -					return ret;
 -				if (cr8_prev <= cr8)
 -					return ret;
 -				/*
 -				 * TODO: we might be squashing a
 -				 * KVM_GUESTDBG_SINGLESTEP-triggered
 -				 * KVM_EXIT_DEBUG here.
 -				 */
 -				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 -				return 0;
 -			}
 -		}
 -		break;
 -	case 2: /* clts */
 -		WARN_ONCE(1, "Guest should always own CR0.TS");
 -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 -		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 -		return kvm_skip_emulated_instruction(vcpu);
 -	case 1: /*mov from cr*/
 -		switch (cr) {
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			val = kvm_read_cr3(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		case 8:
 -			val = kvm_get_cr8(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -		break;
 -	case 3: /* lmsw */
 -		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 -		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 -		kvm_lmsw(vcpu, val);
 +	if (vector == -1)
 +		vector = 0;
  
 -		return kvm_skip_emulated_instruction(vcpu);
 -	default:
 -		break;
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = (u8)status & 0xff;
 +	if ((u8)vector != old) {
 +		status &= ~0xff;
 +		status |= (u8)vector;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
  	}
 -	vcpu->run->exit_reason = 0;
 -	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 -	       (int)(exit_qualification >> 4) & 3, cr);
 -	return 0;
  }
  
 -static int handle_dr(struct kvm_vcpu *vcpu)
 +static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
  {
 -	unsigned long exit_qualification;
 -	int dr, dr7, reg;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 +	/*
 +	 * When running L2, updating RVI is only relevant when
 +	 * vmcs12 virtual-interrupt-delivery enabled.
 +	 * However, it can be enabled only when L1 also
 +	 * intercepts external-interrupts and in that case
 +	 * we should not update vmcs02 RVI but instead intercept
 +	 * interrupt. Therefore, do nothing when running L2.
 +	 */
 +	if (!is_guest_mode(vcpu))
 +		vmx_set_rvi(max_irr);
 +}
  
 -	/* First, if DR does not exist, trigger UD */
 -	if (!kvm_require_dr(vcpu, dr))
 -		return 1;
 +static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int max_irr;
 +	bool max_irr_updated;
  
 -	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 -	if (!kvm_require_cpl(vcpu, 0))
 -		return 1;
 -	dr7 = vmcs_readl(GUEST_DR7);
 -	if (dr7 & DR7_GD) {
 +	WARN_ON(!vcpu->arch.apicv_active);
 +	if (pi_test_on(&vmx->pi_desc)) {
 +		pi_clear_on(&vmx->pi_desc);
  		/*
 -		 * As the vm-exit takes precedence over the debug trap, we
 -		 * need to emulate the latter, either for the host or the
 -		 * guest debugging itself.
 +		 * IOMMU can write to PIR.ON, so the barrier matters even on UP.
 +		 * But on x86 this is just a compiler barrier anyway.
  		 */
 -		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 -			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 -			vcpu->run->debug.arch.dr7 = dr7;
 -			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 -			vcpu->run->debug.arch.exception = DB_VECTOR;
 -			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 -			return 0;
 -		} else {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 -		}
 -	}
 -
 -	if (vcpu->guest_debug == 0) {
 -		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -				CPU_BASED_MOV_DR_EXITING);
 +		smp_mb__after_atomic();
 +		max_irr_updated =
 +			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
  
  		/*
 -		 * No more DR vmexits; force a reload of the debug registers
 -		 * and reenter on this instruction.  The next vmexit will
 -		 * retrieve the full state of the debug registers.
 +		 * If we are running L2 and L1 has a new pending interrupt
 +		 * which can be injected, we should re-evaluate
 +		 * what should be done with this new L1 interrupt.
 +		 * If L1 intercepts external-interrupts, we should
 +		 * exit from L2 to L1. Otherwise, interrupt should be
 +		 * delivered directly to L2.
  		 */
 -		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 -		return 1;
 +		if (is_guest_mode(vcpu) && max_irr_updated) {
 +			if (nested_exit_on_intr(vcpu))
 +				kvm_vcpu_exiting_guest_mode(vcpu);
 +			else
 +				kvm_make_request(KVM_REQ_EVENT, vcpu);
 +		}
 +	} else {
 +		max_irr = kvm_lapic_find_highest_irr(vcpu);
  	}
 +	vmx_hwapic_irr_update(vcpu, max_irr);
 +	return max_irr;
 +}
  
 -	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 -	if (exit_qualification & TYPE_MOV_FROM_DR) {
 -		unsigned long val;
 +static u8 vmx_has_apicv_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	u8 rvi = vmx_get_rvi();
 +	u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
  
 -		if (kvm_get_dr(vcpu, dr, &val))
 -			return 1;
 -		kvm_register_write(vcpu, reg, val);
 -	} else
 -		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 -			return 1;
 +	return ((rvi & 0xf0) > (vppr & 0xf0));
 +}
  
 -	return kvm_skip_emulated_instruction(vcpu);
 +static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);
 +	vmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);
 +	vmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);
 +	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
  }
  
 -static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 +static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
  {
 -	return vcpu->arch.dr6;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	pi_clear_on(&vmx->pi_desc);
 +	memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
  }
  
 -static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
  {
 +	u32 exit_intr_info = 0;
 +	u16 basic_exit_reason = (u16)vmx->exit_reason;
 +
 +	if (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY
 +	      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))
 +		return;
 +
 +	if (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +	vmx->exit_intr_info = exit_intr_info;
 +
 +	/* if exit due to PF check for async PF */
 +	if (is_page_fault(exit_intr_info))
 +		vmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	/* Handle machine checks before interrupts are enabled */
 +	if (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||
 +	    is_machine_check(exit_intr_info))
 +		kvm_machine_check();
 +
 +	/* We need to handle NMIs before interrupts are enabled */
 +	if (is_nmi(exit_intr_info)) {
 +		kvm_before_interrupt(&vmx->vcpu);
 +		asm("int $2");
 +		kvm_after_interrupt(&vmx->vcpu);
 +	}
  }
  
 -static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
  {
 -	get_debugreg(vcpu->arch.db[0], 0);
 -	get_debugreg(vcpu->arch.db[1], 1);
 -	get_debugreg(vcpu->arch.db[2], 2);
 -	get_debugreg(vcpu->arch.db[3], 3);
 -	get_debugreg(vcpu->arch.dr6, 6);
 -	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
  
 -	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 +	if ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))
 +			== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {
 +		unsigned int vector;
 +		unsigned long entry;
 +		gate_desc *desc;
 +		struct vcpu_vmx *vmx = to_vmx(vcpu);
 +#ifdef CONFIG_X86_64
 +		unsigned long tmp;
 +#endif
 +
 +		vector =  exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		desc = (gate_desc *)vmx->host_idt_base + vector;
 +		entry = gate_offset(desc);
 +		asm volatile(
 +#ifdef CONFIG_X86_64
 +			"mov %%" _ASM_SP ", %[sp]\n\t"
 +			"and $0xfffffffffffffff0, %%" _ASM_SP "\n\t"
 +			"push $%c[ss]\n\t"
 +			"push %[sp]\n\t"
 +#endif
 +			"pushf\n\t"
 +			__ASM_SIZE(push) " $%c[cs]\n\t"
 +			CALL_NOSPEC
 +			:
 +#ifdef CONFIG_X86_64
 +			[sp]"=&r"(tmp),
 +#endif
 +			ASM_CALL_CONSTRAINT
 +			:
 +			THUNK_TARGET(entry),
 +			[ss]"i"(__KERNEL_DS),
 +			[cs]"i"(__KERNEL_CS)
 +			);
 +	}
  }
 +STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
  
 -static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 +static bool vmx_has_emulated_msr(int index)
  {
 -	vmcs_writel(GUEST_DR7, val);
 +	switch (index) {
 +	case MSR_IA32_SMBASE:
 +		/*
 +		 * We cannot do SMM unless we can run the guest in big
 +		 * real mode.
 +		 */
 +		return enable_unrestricted_guest || emulate_invalid_guest_state;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		/* This is AMD only.  */
 +		return false;
 +	default:
 +		return true;
 +	}
  }
  
 -static int handle_cpuid(struct kvm_vcpu *vcpu)
 +static bool vmx_mpx_supported(void)
  {
 -	return kvm_emulate_cpuid(vcpu);
 +	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 +		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
  }
  
 -static int handle_rdmsr(struct kvm_vcpu *vcpu)
 +static bool vmx_xsaves_supported(void)
  {
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	struct msr_data msr_info;
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_XSAVES;
 +}
  
 -	msr_info.index = ecx;
 -	msr_info.host_initiated = false;
 -	if (vmx_get_msr(vcpu, &msr_info)) {
 -		trace_kvm_msr_read_ex(ecx);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 +{
 +	u32 exit_intr_info;
 +	bool unblock_nmi;
 +	u8 vector;
 +	bool idtv_info_valid;
  
 -	trace_kvm_msr_read(ecx, msr_info.data);
 +	idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
  
 -	/* FIXME: handling of bits 32:63 of rax, rdx */
 -	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 -	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 -	return kvm_skip_emulated_instruction(vcpu);
 +	if (enable_vnmi) {
 +		if (vmx->loaded_vmcs->nmi_known_unmasked)
 +			return;
 +		/*
 +		 * Can't use vmx->exit_intr_info since we're not sure what
 +		 * the exit reason is.
 +		 */
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +		unblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;
 +		vector = exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Re-set bit "block by NMI" before VM entry if vmexit caused by
 +		 * a guest IRET fault.
 +		 * SDM 3: 23.2.2 (September 2008)
 +		 * Bit 12 is undefined in any of the following cases:
 +		 *  If the VM exit sets the valid bit in the IDT-vectoring
 +		 *   information field.
 +		 *  If the VM exit is due to a double fault.
 +		 */
 +		if ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&
 +		    vector != DF_VECTOR && !idtv_info_valid)
 +			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				      GUEST_INTR_STATE_NMI);
 +		else
 +			vmx->loaded_vmcs->nmi_known_unmasked =
 +				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)
 +				  & GUEST_INTR_STATE_NMI);
 +	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->vnmi_blocked_time +=
 +			ktime_to_ns(ktime_sub(ktime_get(),
 +					      vmx->loaded_vmcs->entry_time));
  }
  
 -static int handle_wrmsr(struct kvm_vcpu *vcpu)
 +static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 +				      u32 idt_vectoring_info,
 +				      int instr_len_field,
 +				      int error_code_field)
  {
 -	struct msr_data msr;
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 -		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
 +	u8 vector;
 +	int type;
 +	bool idtv_info_valid;
  
 -	msr.data = data;
 -	msr.index = ecx;
 -	msr.host_initiated = false;
 -	if (kvm_set_msr(vcpu, &msr) != 0) {
 -		trace_kvm_msr_write_ex(ecx, data);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 +	idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
 +
 +	vcpu->arch.nmi_injected = false;
 +	kvm_clear_exception_queue(vcpu);
 +	kvm_clear_interrupt_queue(vcpu);
 +
 +	if (!idtv_info_valid)
 +		return;
 +
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +
 +	vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
 +	type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
 +
 +	switch (type) {
 +	case INTR_TYPE_NMI_INTR:
 +		vcpu->arch.nmi_injected = true;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Clear bit "block by NMI" before VM entry if a NMI
 +		 * delivery faulted.
 +		 */
 +		vmx_set_nmi_mask(vcpu, false);
 +		break;
 +	case INTR_TYPE_SOFT_EXCEPTION:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_HARD_EXCEPTION:
 +		if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
 +			u32 err = vmcs_read32(error_code_field);
 +			kvm_requeue_exception_e(vcpu, vector, err);
 +		} else
 +			kvm_requeue_exception(vcpu, vector);
 +		break;
 +	case INTR_TYPE_SOFT_INTR:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_EXT_INTR:
 +		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 +		break;
 +	default:
 +		break;
  	}
 -
 -	trace_kvm_msr_write(ecx, data);
 -	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
  {
 -	kvm_apic_update_ppr(vcpu);
 -	return 1;
 +	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 +				  VM_EXIT_INSTRUCTION_LEN,
 +				  IDT_VECTORING_ERROR_CODE);
  }
  
 -static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 +static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
  {
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_INTR_PENDING);
 -
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 -
 -	++vcpu->stat.irq_window_exits;
 -	return 1;
 -}
 +	__vmx_complete_interrupts(vcpu,
 +				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +				  VM_ENTRY_INSTRUCTION_LEN,
 +				  VM_ENTRY_EXCEPTION_ERROR_CODE);
  
 -static int handle_halt(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_emulate_halt(vcpu);
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
  }
  
 -static int handle_vmcall(struct kvm_vcpu *vcpu)
 +static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
  {
 -	return kvm_emulate_hypercall(vcpu);
 -}
 +	int i, nr_msrs;
 +	struct perf_guest_switch_msr *msrs;
  
 -static int handle_invd(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
 +	msrs = perf_guest_get_msrs(&nr_msrs);
  
 -static int handle_invlpg(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	if (!msrs)
 +		return;
  
 -	kvm_mmu_invlpg(vcpu, exit_qualification);
 -	return kvm_skip_emulated_instruction(vcpu);
 +	for (i = 0; i < nr_msrs; i++)
 +		if (msrs[i].host == msrs[i].guest)
 +			clear_atomic_switch_msr(vmx, msrs[i].msr);
 +		else
 +			add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,
 +					msrs[i].host, false);
  }
  
 -static int handle_rdpmc(struct kvm_vcpu *vcpu)
 +static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
  {
 -	int err;
 -
 -	err = kvm_rdpmc(vcpu);
 -	return kvm_complete_insn_gp(vcpu, err);
 +	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, val);
 +	if (!vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
 +			      PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = true;
  }
  
 -static int handle_wbinvd(struct kvm_vcpu *vcpu)
 +static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
  {
 -	return kvm_emulate_wbinvd(vcpu);
 -}
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 tscl;
 +	u32 delta_tsc;
  
 -static int handle_xsetbv(struct kvm_vcpu *vcpu)
 -{
 -	u64 new_bv = kvm_read_edx_eax(vcpu);
 -	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
 +	if (vmx->req_immediate_exit) {
 +		vmx_arm_hv_timer(vmx, 0);
 +		return;
 +	}
  
 -	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 -		return kvm_skip_emulated_instruction(vcpu);
 -	return 1;
 -}
 +	if (vmx->hv_deadline_tsc != -1) {
 +		tscl = rdtsc();
 +		if (vmx->hv_deadline_tsc > tscl)
 +			/* set_hv_timer ensures the delta fits in 32-bits */
 +			delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
 +				cpu_preemption_timer_multi);
 +		else
 +			delta_tsc = 0;
  
 -static int handle_xsaves(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 -}
 +		vmx_arm_hv_timer(vmx, delta_tsc);
 +		return;
 +	}
  
 -static int handle_xrstors(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 +	if (vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
 +				PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = false;
  }
  
 -static int handle_apic_access(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
  {
 -	if (likely(fasteoi)) {
 -		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -		int access_type, offset;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	unsigned long cr3, cr4, evmcs_rsp;
  
 -		access_type = exit_qualification & APIC_ACCESS_TYPE;
 -		offset = exit_qualification & APIC_ACCESS_OFFSET;
 -		/*
 -		 * Sane guest uses MOV to write EOI, with written value
 -		 * not cared. So make a short-circuit here by avoiding
 -		 * heavy instruction emulation.
 -		 */
 -		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 -		    (offset == APIC_EOI)) {
 -			kvm_lapic_set_eoi(vcpu);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -	}
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
 +	/* Record the guest's net vcpu time for enforced NMI injections. */
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->entry_time = ktime_get();
  
 -static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	int vector = exit_qualification & 0xff;
 +	/* Don't enter VMX if guest state is invalid, let the exit handler
 +	   start emulation until we arrive back to a valid state */
 +	if (vmx->emulation_required)
 +		return;
  
 -	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_set_eoi_accelerated(vcpu, vector);
 -	return 1;
 -}
 +	if (vmx->ple_window_dirty) {
 +		vmx->ple_window_dirty = false;
 +		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 +	}
  
 -static int handle_apic_write(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	u32 offset = exit_qualification & 0xfff;
 +	if (vmx->nested.need_vmcs12_sync) {
 +		if (vmx->nested.hv_evmcs) {
 +			copy_vmcs12_to_enlightened(vmx);
 +			/* All fields are clean */
 +			vmx->nested.hv_evmcs->hv_clean_fields |=
 +				HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
 +		} else {
 +			copy_vmcs12_to_shadow(vmx);
 +		}
 +		vmx->nested.need_vmcs12_sync = false;
 +	}
  
 -	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_write_nodecode(vcpu, offset);
 -	return 1;
 -}
 +	if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 +	if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
  
 -static int handle_task_switch(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	unsigned long exit_qualification;
 -	bool has_error_code = false;
 -	u32 error_code = 0;
 -	u16 tss_selector;
 -	int reason, type, idt_v, idt_index;
 +	cr3 = __get_current_cr3_fast();
 +	if (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {
 +		vmcs_writel(HOST_CR3, cr3);
 +		vmx->loaded_vmcs->host_state.cr3 = cr3;
 +	}
  
 -	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 -	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 -	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
 +	cr4 = cr4_read_shadow();
 +	if (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {
 +		vmcs_writel(HOST_CR4, cr4);
 +		vmx->loaded_vmcs->host_state.cr4 = cr4;
 +	}
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	/* When single-stepping over STI and MOV SS, we must clear the
 +	 * corresponding interruptibility bits in the guest state. Otherwise
 +	 * vmentry fails as it then expects bit 14 (BS) in pending debug
 +	 * exceptions being set, but that's not correct for the guest debugging
 +	 * case. */
 +	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 +		vmx_set_interrupt_shadow(vcpu, 0);
  
 -	reason = (u32)exit_qualification >> 30;
 -	if (reason == TASK_SWITCH_GATE && idt_v) {
 -		switch (type) {
 -		case INTR_TYPE_NMI_INTR:
 -			vcpu->arch.nmi_injected = false;
 -			vmx_set_nmi_mask(vcpu, true);
 -			break;
 -		case INTR_TYPE_EXT_INTR:
 -		case INTR_TYPE_SOFT_INTR:
 -			kvm_clear_interrupt_queue(vcpu);
 -			break;
 -		case INTR_TYPE_HARD_EXCEPTION:
 -			if (vmx->idt_vectoring_info &
 -			    VECTORING_INFO_DELIVER_CODE_MASK) {
 -				has_error_code = true;
 -				error_code =
 -					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 -			}
 -			/* fall through */
 -		case INTR_TYPE_SOFT_EXCEPTION:
 -			kvm_clear_exception_queue(vcpu);
 -			break;
 -		default:
 -			break;
 -		}
 -	}
 -	tss_selector = exit_qualification;
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
 +	    vcpu->arch.pkru != vmx->host_pkru)
 +		__write_pkru(vcpu->arch.pkru);
  
 -	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 -		       type != INTR_TYPE_EXT_INTR &&
 -		       type != INTR_TYPE_NMI_INTR))
 -		skip_emulated_instruction(vcpu);
 +	atomic_switch_perf_msrs(vmx);
  
 -	if (kvm_task_switch(vcpu, tss_selector,
 -			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 -			    has_error_code, error_code) == EMULATE_FAIL) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -		vcpu->run->internal.ndata = 0;
 -		return 0;
 -	}
 +	vmx_update_hv_timer(vcpu);
  
  	/*
 -	 * TODO: What about debug traps on tss switch?
 -	 *       Are we supposed to inject them and update dr6?
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
  	 */
 +	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
++=======
++static void __vmx_vcpu_run(struct kvm_vcpu *vcpu, struct vcpu_vmx *vmx)
++{
++	unsigned long evmcs_rsp;
++>>>>>>> 5ad6ece869d4 (KVM: VMX: Move vmx_vcpu_run()'s VM-Enter asm blob to a helper function)
  
 -	return 1;
 -}
 -
 -static int handle_ept_violation(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification;
 -	gpa_t gpa;
 -	u64 error_code;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	vmx->__launched = vmx->loaded_vmcs->launched;
  
 -	/*
 -	 * EPT violation happened while executing iret from NMI,
 -	 * "blocked by NMI" bit has to be set before next VM entry.
 -	 * There are errata that may cause this bit to not be set:
 -	 * AAK134, BY25.
 -	 */
 -	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 -			enable_vnmi &&
 -			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 -		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 +	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
 +		(unsigned long)&current_evmcs->host_rsp : 0;
  
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	trace_kvm_page_fault(gpa, exit_qualification);
 +	if (static_branch_unlikely(&vmx_l1d_should_flush))
 +		vmx_l1d_flush(vcpu);
  
 -	/* Is it a read fault? */
 -	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 -		     ? PFERR_USER_MASK : 0;
 -	/* Is it a write fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 -		      ? PFERR_WRITE_MASK : 0;
 -	/* Is it a fetch fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 -		      ? PFERR_FETCH_MASK : 0;
 -	/* ept page table entry is present? */
 -	error_code |= (exit_qualification &
 -		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 -			EPT_VIOLATION_EXECUTABLE))
 -		      ? PFERR_PRESENT_MASK : 0;
 +	asm(
 +		/* Store host registers */
 +		"push %%" _ASM_DX "; push %%" _ASM_BP ";"
 +		"push %%" _ASM_CX " \n\t" /* placeholder for guest rcx */
 +		"push %%" _ASM_CX " \n\t"
 +		"cmp %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		"je 1f \n\t"
 +		"mov %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		/* Avoid VMWRITE when Enlightened VMCS is in use */
 +		"test %%" _ASM_SI ", %%" _ASM_SI " \n\t"
 +		"jz 2f \n\t"
 +		"mov %%" _ASM_SP ", (%%" _ASM_SI ") \n\t"
 +		"jmp 1f \n\t"
 +		"2: \n\t"
 +		__ex("vmwrite %%" _ASM_SP ", %%" _ASM_DX) "\n\t"
 +		"1: \n\t"
 +		/* Reload cr2 if changed */
 +		"mov %c[cr2](%0), %%" _ASM_AX " \n\t"
 +		"mov %%cr2, %%" _ASM_DX " \n\t"
 +		"cmp %%" _ASM_AX ", %%" _ASM_DX " \n\t"
 +		"je 3f \n\t"
 +		"mov %%" _ASM_AX", %%cr2 \n\t"
 +		"3: \n\t"
 +		/* Check if vmlaunch or vmresume is needed */
 +		"cmpl $0, %c[launched](%0) \n\t"
 +		/* Load guest registers.  Don't clobber flags. */
 +		"mov %c[rax](%0), %%" _ASM_AX " \n\t"
 +		"mov %c[rbx](%0), %%" _ASM_BX " \n\t"
 +		"mov %c[rdx](%0), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%0), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%0), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%0), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%0),  %%r8  \n\t"
 +		"mov %c[r9](%0),  %%r9  \n\t"
 +		"mov %c[r10](%0), %%r10 \n\t"
 +		"mov %c[r11](%0), %%r11 \n\t"
 +		"mov %c[r12](%0), %%r12 \n\t"
 +		"mov %c[r13](%0), %%r13 \n\t"
 +		"mov %c[r14](%0), %%r14 \n\t"
 +		"mov %c[r15](%0), %%r15 \n\t"
 +#endif
 +		"mov %c[rcx](%0), %%" _ASM_CX " \n\t" /* kills %0 (ecx) */
  
 -	error_code |= (exit_qualification & 0x100) != 0 ?
 -	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 +		/* Enter guest mode */
 +		"jne 1f \n\t"
 +		__ex("vmlaunch") "\n\t"
 +		"jmp 2f \n\t"
 +		"1: " __ex("vmresume") "\n\t"
 +		"2: "
 +		/* Save guest registers, load host registers, keep flags */
 +		"mov %0, %c[wordsize](%%" _ASM_SP ") \n\t"
 +		"pop %0 \n\t"
 +		"setbe %c[fail](%0)\n\t"
 +		"mov %%" _ASM_AX ", %c[rax](%0) \n\t"
 +		"mov %%" _ASM_BX ", %c[rbx](%0) \n\t"
 +		__ASM_SIZE(pop) " %c[rcx](%0) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%0) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%0) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%0) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%0) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%0) \n\t"
 +		"mov %%r9,  %c[r9](%0) \n\t"
 +		"mov %%r10, %c[r10](%0) \n\t"
 +		"mov %%r11, %c[r11](%0) \n\t"
 +		"mov %%r12, %c[r12](%0) \n\t"
 +		"mov %%r13, %c[r13](%0) \n\t"
 +		"mov %%r14, %c[r14](%0) \n\t"
 +		"mov %%r15, %c[r15](%0) \n\t"
 +		/*
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d,  %%r8d \n\t"
 +		"xor %%r9d,  %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"mov %%cr2, %%" _ASM_AX "   \n\t"
 +		"mov %%" _ASM_AX ", %c[cr2](%0) \n\t"
  
 -	vcpu->arch.exit_qualification = exit_qualification;
 -	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 +		"xor %%eax, %%eax \n\t"
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop  %%" _ASM_BP "; pop  %%" _ASM_DX " \n\t"
 +		".pushsection .rodata \n\t"
 +		".global vmx_return \n\t"
 +		"vmx_return: " _ASM_PTR " 2b \n\t"
 +		".popsection"
 +	      : : "c"(vmx), "d"((unsigned long)HOST_RSP), "S"(evmcs_rsp),
 +		[launched]"i"(offsetof(struct vcpu_vmx, __launched)),
 +		[fail]"i"(offsetof(struct vcpu_vmx, fail)),
 +		[host_rsp]"i"(offsetof(struct vcpu_vmx, host_rsp)),
 +		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
 +		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
 +#ifdef CONFIG_X86_64
 +		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
 +		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
 +		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
 +		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
 +		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
 +		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
 +		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
 +		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
 +#endif
 +		[cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),
 +		[wordsize]"i"(sizeof(ulong))
 +	      : "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rax", "rbx", "rdi"
 +		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
 +#else
 +		, "eax", "ebx", "edi"
 +#endif
 +	      );
+ }
++STACK_FRAME_NON_STANDARD(__vmx_vcpu_run);
+ 
 -static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
++static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
+ {
 -	gpa_t gpa;
++	struct vcpu_vmx *vmx = to_vmx(vcpu);
++	unsigned long cr3, cr4;
+ 
 -	/*
 -	 * A nested guest cannot optimize MMIO vmexits, because we have an
 -	 * nGPA here instead of the required GPA.
 -	 */
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	if (!is_guest_mode(vcpu) &&
 -	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 -		trace_kvm_fast_mmio(gpa);
 -		/*
 -		 * Doing kvm_skip_emulated_instruction() depends on undefined
 -		 * behavior: Intel's manual doesn't mandate
 -		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 -		 * occurs and while on real hardware it was observed to be set,
 -		 * other hypervisors (namely Hyper-V) don't set it, we end up
 -		 * advancing IP with some random value. Disable fast mmio when
 -		 * running nested and keep it for real hardware in hope that
 -		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 -		 */
 -		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 -			return kvm_skip_emulated_instruction(vcpu);
 -		else
 -			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 -								EMULATE_DONE;
 -	}
++	/* Record the guest's net vcpu time for enforced NMI injections. */
++	if (unlikely(!enable_vnmi &&
++		     vmx->loaded_vmcs->soft_vnmi_blocked))
++		vmx->loaded_vmcs->entry_time = ktime_get();
+ 
 -	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 -}
++	/* Don't enter VMX if guest state is invalid, let the exit handler
++	   start emulation until we arrive back to a valid state */
++	if (vmx->emulation_required)
++		return;
+ 
 -static int handle_nmi_window(struct kvm_vcpu *vcpu)
 -{
 -	WARN_ON_ONCE(!enable_vnmi);
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_NMI_PENDING);
 -	++vcpu->stat.nmi_window_exits;
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
++	if (vmx->ple_window_dirty) {
++		vmx->ple_window_dirty = false;
++		vmcs_write32(PLE_WINDOW, vmx->ple_window);
++	}
+ 
 -	return 1;
 -}
++	if (vmx->nested.need_vmcs12_sync)
++		nested_sync_from_vmcs12(vcpu);
+ 
 -static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	enum emulation_result err = EMULATE_DONE;
 -	int ret = 1;
 -	u32 cpu_exec_ctrl;
 -	bool intr_window_requested;
 -	unsigned count = 130;
++	if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))
++		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
++	if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
++		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
+ 
 -	/*
 -	 * We should never reach the point where we are emulating L2
 -	 * due to invalid guest state as that means we incorrectly
 -	 * allowed a nested VMEntry with an invalid vmcs12.
 -	 */
 -	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
++	cr3 = __get_current_cr3_fast();
++	if (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {
++		vmcs_writel(HOST_CR3, cr3);
++		vmx->loaded_vmcs->host_state.cr3 = cr3;
++	}
+ 
 -	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 -	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
++	cr4 = cr4_read_shadow();
++	if (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {
++		vmcs_writel(HOST_CR4, cr4);
++		vmx->loaded_vmcs->host_state.cr4 = cr4;
++	}
+ 
 -	while (vmx->emulation_required && count-- != 0) {
 -		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 -			return handle_interrupt_window(&vmx->vcpu);
++	/* When single-stepping over STI and MOV SS, we must clear the
++	 * corresponding interruptibility bits in the guest state. Otherwise
++	 * vmentry fails as it then expects bit 14 (BS) in pending debug
++	 * exceptions being set, but that's not correct for the guest debugging
++	 * case. */
++	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
++		vmx_set_interrupt_shadow(vcpu, 0);
+ 
 -		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
 -			return 1;
++	if (static_cpu_has(X86_FEATURE_PKU) &&
++	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
++	    vcpu->arch.pkru != vmx->host_pkru)
++		__write_pkru(vcpu->arch.pkru);
+ 
 -		err = kvm_emulate_instruction(vcpu, 0);
++	pt_guest_enter(vmx);
+ 
 -		if (err == EMULATE_USER_EXIT) {
 -			++vcpu->stat.mmio_exits;
 -			ret = 0;
 -			goto out;
 -		}
++	atomic_switch_perf_msrs(vmx);
+ 
 -		if (err != EMULATE_DONE)
 -			goto emulation_error;
++	vmx_update_hv_timer(vcpu);
+ 
 -		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 -		    vcpu->arch.exception.pending)
 -			goto emulation_error;
++	/*
++	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
++	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
++	 * is no need to worry about the conditional branch over the wrmsr
++	 * being speculatively taken.
++	 */
++	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
+ 
 -		if (vcpu->arch.halt_request) {
 -			vcpu->arch.halt_request = 0;
 -			ret = kvm_vcpu_halt(vcpu);
 -			goto out;
 -		}
++	__vmx_vcpu_run(vcpu, vmx);
 +
 +	/*
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 */
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
  
 -		if (signal_pending(current))
 -			goto out;
 -		if (need_resched())
 -			schedule();
 -	}
 +	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
  
 -out:
 -	return ret;
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
  
 -emulation_error:
 -	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -	vcpu->run->internal.ndata = 0;
 -	return 0;
 -}
 +	/* All fields are clean at this point */
 +	if (static_branch_unlikely(&enable_evmcs))
 +		current_evmcs->hv_clean_fields |=
 +			HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
  
 -static void grow_ple_window(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 +	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 +	if (vmx->host_debugctlmsr)
 +		update_debugctlmsr(vmx->host_debugctlmsr);
  
 -	vmx->ple_window = __grow_ple_window(old, ple_window,
 -					    ple_window_grow,
 -					    ple_window_max);
 +#ifndef CONFIG_X86_64
 +	/*
 +	 * The sysexit path does not restore ds/es, so we must set them to
 +	 * a reasonable value ourselves.
 +	 *
 +	 * We can't defer this to vmx_prepare_switch_to_host() since that
 +	 * function may be executed in interrupt context, which saves and
 +	 * restore segments around it, nullifying its effect.
 +	 */
 +	loadsegment(ds, __USER_DS);
 +	loadsegment(es, __USER_DS);
 +#endif
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
 +				  | (1 << VCPU_EXREG_RFLAGS)
 +				  | (1 << VCPU_EXREG_PDPTR)
 +				  | (1 << VCPU_EXREG_SEGMENTS)
 +				  | (1 << VCPU_EXREG_CR3));
 +	vcpu->arch.regs_dirty = 0;
  
 -	trace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);
 -}
 +	/*
 +	 * eager fpu is enabled if PKEY is supported and CR4 is switched
 +	 * back on host, so it is safe to read guest PKRU from current
 +	 * XSAVE.
 +	 */
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
 +		vcpu->arch.pkru = __read_pkru();
 +		if (vcpu->arch.pkru != vmx->host_pkru)
 +			__write_pkru(vmx->host_pkru);
 +	}
  
 -static void shrink_ple_window(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 +	vmx->nested.nested_run_pending = 0;
 +	vmx->idt_vectoring_info = 0;
  
 -	vmx->ple_window = __shrink_ple_window(old, ple_window,
 -					      ple_window_shrink,
 -					      ple_window);
 +	vmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);
 +	if (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		return;
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	vmx->loaded_vmcs->launched = 1;
 +	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
  
 -	trace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);
 +	vmx_complete_atomic_exit(vmx);
 +	vmx_recover_nmi_blocking(vmx);
 +	vmx_complete_interrupts(vmx);
  }
- STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
  
 -/*
 - * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
 - */
 -static void wakeup_handler(void)
 +static struct kvm *vmx_vm_alloc(void)
  {
 -	struct kvm_vcpu *vcpu;
 -	int cpu = smp_processor_id();
 -
 -	spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 -	list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
 -			blocked_vcpu_list) {
 -		struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
 -
 -		if (pi_test_on(pi_desc) == 1)
 -			kvm_vcpu_kick(vcpu);
 -	}
 -	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 +	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 +	return &kvm_vmx->kvm;
  }
  
 -static void vmx_enable_tdp(void)
 +static void vmx_vm_free(struct kvm *kvm)
  {
 -	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
 -		enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull,
 -		enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
 -		0ull, VMX_EPT_EXECUTABLE_MASK,
 -		cpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK,
 -		VMX_EPT_RWX_MASK, 0ull);
 -
 -	ept_set_mmio_spte_mask();
 -	kvm_enable_tdp();
 +	vfree(to_kvm_vmx(kvm));
  }
  
 -/*
 - * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
 - * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
 - */
 -static int handle_pause(struct kvm_vcpu *vcpu)
 +static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  {
 -	if (!kvm_pause_in_guest(vcpu->kvm))
 -		grow_ple_window(vcpu);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int cpu;
  
 -	/*
 -	 * Intel sdm vol3 ch-25.1.3 says: The "PAUSE-loop exiting"
 -	 * VM-execution control is ignored if CPL > 0. OTOH, KVM
 -	 * never set PAUSE_EXITING and just set PLE if supported,
 -	 * so the vcpu must be CPL=0 if it gets a PAUSE exit.
 -	 */
 -	kvm_vcpu_on_spin(vcpu, true);
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	if (vmx->loaded_vmcs == vmcs)
 +		return;
  
 -static int handle_nop(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	cpu = get_cpu();
 +	vmx_vcpu_put(vcpu);
 +	vmx->loaded_vmcs = vmcs;
 +	vmx_vcpu_load(vcpu, cpu);
 +	put_cpu();
  
 -static int handle_mwait(struct kvm_vcpu *vcpu)
 -{
 -	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 -	return handle_nop(vcpu);
 +	vm_entry_controls_reset_shadow(vmx);
 +	vm_exit_controls_reset_shadow(vmx);
 +	vmx_segment_cache_clear(vmx);
  }
  
 -static int handle_invalid_op(struct kvm_vcpu *vcpu)
 +/*
 + * Ensure that the current vmcs of the logical processor is the
 + * vmcs01 of the vcpu before calling free_nested().
 + */
 +static void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)
  {
 -	kvm_queue_exception(vcpu, UD_VECTOR);
 -	return 1;
 -}
 +       struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 -{
 -	return 1;
 +       vcpu_load(vcpu);
 +       vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 +       free_nested(vmx);
 +       vcpu_put(vcpu);
  }
  
 -static int handle_monitor(struct kvm_vcpu *vcpu)
 +static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
  {
 -	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 -	return handle_nop(vcpu);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	if (enable_pml)
 +		vmx_destroy_pml_buffer(vmx);
 +	free_vpid(vmx->vpid);
 +	leave_guest_mode(vcpu);
 +	vmx_free_vcpu_nested(vcpu);
 +	free_loaded_vmcs(vmx->loaded_vmcs);
 +	kfree(vmx->guest_msrs);
 +	kvm_vcpu_uninit(vcpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
  }
  
 -static int handle_invpcid(struct kvm_vcpu *vcpu)
 +static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
  {
 -	u32 vmx_instruction_info;
 -	unsigned long type;
 -	bool pcid_enabled;
 -	gva_t gva;
 -	struct x86_exception e;
 -	unsigned i;
 -	unsigned long roots_to_free = 0;
 -	struct {
 -		u64 pcid;
 -		u64 gla;
 -	} operand;
 -
 -	if (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {
 -		kvm_queue_exception(vcpu, UD_VECTOR);
 -		return 1;
 -	}
 -
 -	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 -	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 -
 -	if (type > 3) {
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 -
 -	/* According to the Intel instruction reference, the memory operand
 -	 * is read even if it isn't needed (e.g., for type==all)
 -	 */
 -	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 -				vmx_instruction_info, false, &gva))
 -		return 1;
 +	int err;
 +	struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
 +	unsigned long *msr_bitmap;
 +	int cpu;
  
 -	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 -		kvm_inject_page_fault(vcpu, &e);
 -		return 1;
 -	}
 +	if (!vmx)
 +		return ERR_PTR(-ENOMEM);
  
 -	if (operand.pcid >> 12 != 0) {
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 +	vmx->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache, GFP_KERNEL);
 +	if (!vmx->vcpu.arch.guest_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
 +		err = -ENOMEM;
 +		goto free_partial_vcpu;
  	}
  
 -	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
 +	vmx->vpid = allocate_vpid();
  
 -	switch (type) {
 -	case INVPCID_TYPE_INDIV_ADDR:
 -		if ((!pcid_enabled && (operand.pcid != 0)) ||
 -		    is_noncanonical_address(operand.gla, vcpu)) {
 -			kvm_inject_gp(vcpu, 0);
 -			return 1;
 -		}
 -		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
 -		return kvm_skip_emulated_instruction(vcpu);
 +	err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
 +	if (err)
 +		goto free_vcpu;
  
 -	case INVPCID_TYPE_SINGLE_CTXT:
 -		if (!pcid_enabled && (operand.pcid != 0)) {
 -			kvm_inject_gp(vcpu, 0);
 -			return 1;
 -		}
 +	err = -ENOMEM;
  
 -		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
 -			kvm_mmu_sync_roots(vcpu);
 -			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 -		}
 +	/*
 +	 * If PML is turned on, failure on enabling PML just results in failure
 +	 * of creating the vcpu, therefore we can simplify PML logic (by
 +	 * avoiding dealing with cases, such as enabling PML partially on vcpus
 +	 * for the guest, etc.
 +	 */
 +	if (enable_pml) {
 +		vmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);
 +		if (!vmx->pml_pg)
 +			goto uninit_vcpu;
 +	}
  
 -		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 -			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].cr3)
 -			    == operand.pcid)
 -				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
 +	vmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);
 +	BUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])
 +		     > PAGE_SIZE);
  
 -		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);
 -		/*
 -		 * If neither the current cr3 nor any of the prev_roots use the
 -		 * given PCID, then nothing needs to be done here because a
 -		 * resync will happen anyway before switching to any other CR3.
 -		 */
 +	if (!vmx->guest_msrs)
 +		goto free_pml;
  
 -		return kvm_skip_emulated_instruction(vcpu);
 +	err = alloc_loaded_vmcs(&vmx->vmcs01);
 +	if (err < 0)
 +		goto free_msrs;
  
 -	case INVPCID_TYPE_ALL_NON_GLOBAL:
 -		/*
 -		 * Currently, KVM doesn't mark global entries in the shadow
 -		 * page tables, so a non-global flush just degenerates to a
 -		 * global flush. If needed, we could optimize this later by
 -		 * keeping track of global entries in shadow page tables.
 -		 */
 +	msr_bitmap = vmx->vmcs01.msr_bitmap;
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_TSC, MSR_TYPE_R);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_FS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_GS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
 +	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
 +	vmx->msr_bitmap_mode = 0;
  
 -		/* fall-through */
 -	case INVPCID_TYPE_ALL_INCL_GLOBAL:
 -		kvm_mmu_unload(vcpu);
 -		return kvm_skip_emulated_instruction(vcpu);
 +	vmx->loaded_vmcs = &vmx->vmcs01;
 +	cpu = get_cpu();
 +	vmx_vcpu_load(&vmx->vcpu, cpu);
 +	vmx->vcpu.cpu = cpu;
 +	vmx_vcpu_setup(vmx);
 +	vmx_vcpu_put(&vmx->vcpu);
 +	put_cpu();
 +	if (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {
 +		err = alloc_apic_access_page(kvm);
 +		if (err)
 +			goto free_vmcs;
 +	}
  
 -	default:
 -		BUG(); /* We have already checked above that type <= 3 */
 +	if (enable_ept && !enable_unrestricted_guest) {
 +		err = init_rmode_identity_map(kvm);
 +		if (err)
 +			goto free_vmcs;
  	}
 -}
  
 -static int handle_pml_full(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification;
 +	if (nested)
 +		nested_vmx_setup_ctls_msrs(&vmx->nested.msrs,
 +					   vmx_capability.ept,
 +					   kvm_vcpu_apicv_active(&vmx->vcpu));
  
 -	trace_kvm_pml_full(vcpu->vcpu_id);
 +	vmx->nested.posted_intr_nv = -1;
 +	vmx->nested.current_vmptr = -1ull;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	vmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;
  
  	/*
 -	 * PML buffer FULL happened while executing iret from NMI,
 -	 * "blocked by NMI" bit has to be set before next VM entry.
 +	 * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR
 +	 * or POSTED_INTR_WAKEUP_VECTOR.
  	 */
 -	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 -			enable_vnmi &&
 -			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 -		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 -				GUEST_INTR_STATE_NMI);
 +	vmx->pi_desc.nv = POSTED_INTR_VECTOR;
 +	vmx->pi_desc.sn = 1;
  
 -	/*
 -	 * PML buffer already flushed at beginning of VMEXIT. Nothing to do
 -	 * here.., and there's no userspace involvement needed for PML.
 -	 */
 -	return 1;
 -}
 +	return &vmx->vcpu;
  
 -static int handle_preemption_timer(struct kvm_vcpu *vcpu)
 -{
 -	if (!to_vmx(vcpu)->req_immediate_exit)
 -		kvm_lapic_expired_hv_timer(vcpu);
 -	return 1;
 +free_vmcs:
 +	free_loaded_vmcs(vmx->loaded_vmcs);
 +free_msrs:
 +	kfree(vmx->guest_msrs);
 +free_pml:
 +	vmx_destroy_pml_buffer(vmx);
 +uninit_vcpu:
 +	kvm_vcpu_uninit(&vmx->vcpu);
 +free_vcpu:
 +	free_vpid(vmx->vpid);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +free_partial_vcpu:
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +	return ERR_PTR(err);
  }
  
 -/*
 - * When nested=0, all VMX instruction VM Exits filter here.  The handlers
 - * are overwritten by nested_vmx_setup() when nested=1.
 - */
 -static int handle_vmx_instruction(struct kvm_vcpu *vcpu)
 -{
 -	kvm_queue_exception(vcpu, UD_VECTOR);
 -	return 1;
 -}
 +#define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 +#define L1TF_MSG_L1D "L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
  
 -static int handle_encls(struct kvm_vcpu *vcpu)
 +static int vmx_vm_init(struct kvm *kvm)
  {
 -	/*
 -	 * SGX virtualization is not yet supported.  There is no software
 -	 * enable bit for SGX, so we have to trap ENCLS and inject a #UD
 -	 * to prevent the guest from executing ENCLS.
 -	 */
 -	kvm_queue_exception(vcpu, UD_VECTOR);
 -	return 1;
 -}
 -
 -/*
 - * The exit handlers return 1 if the exit was handled fully and guest execution
 - * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
 - * to be done to userspace and return 0.
 - */
 -static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 -	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,
 -	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
 -	[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,
 -	[EXIT_REASON_NMI_WINDOW]	      = handle_nmi_window,
 -	[EXIT_REASON_IO_INSTRUCTION]          = handle_io,
 -	[EXIT_REASON_CR_ACCESS]               = handle_cr,
 -	[EXIT_REASON_DR_ACCESS]               = handle_dr,
 -	[EXIT_REASON_CPUID]                   = handle_cpuid,
 -	[EXIT_REASON_MSR_READ]                = handle_rdmsr,
 -	[EXIT_REASON_MSR_WRITE]               = handle_wrmsr,
 -	[EXIT_REASON_PENDING_INTERRUPT]       = handle_interrupt_window,
 -	[EXIT_REASON_HLT]                     = handle_halt,
 -	[EXIT_REASON_INVD]		      = handle_invd,
 -	[EXIT_REASON_INVLPG]		      = handle_invlpg,
 -	[EXIT_REASON_RDPMC]                   = handle_rdpmc,
 -	[EXIT_REASON_VMCALL]                  = handle_vmcall,
 -	[EXIT_REASON_VMCLEAR]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMLAUNCH]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMPTRLD]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMPTRST]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMREAD]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMRESUME]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMWRITE]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMOFF]		      = handle_vmx_instruction,
 -	[EXIT_REASON_VMON]		      = handle_vmx_instruction,
 -	[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,
 -	[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,
 -	[EXIT_REASON_APIC_WRITE]              = handle_apic_write,
 -	[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,
 -	[EXIT_REASON_WBINVD]                  = handle_wbinvd,
 -	[EXIT_REASON_XSETBV]                  = handle_xsetbv,
 -	[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,
 -	[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,
 -	[EXIT_REASON_GDTR_IDTR]		      = handle_desc,
 -	[EXIT_REASON_LDTR_TR]		      = handle_desc,
 -	[EXIT_REASON_EPT_VIOLATION]	      = handle_ept_violation,
 -	[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,
 -	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
 -	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
 -	[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,
 -	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
 -	[EXIT_REASON_INVEPT]                  = handle_vmx_instruction,
 -	[EXIT_REASON_INVVPID]                 = handle_vmx_instruction,
 -	[EXIT_REASON_RDRAND]                  = handle_invalid_op,
 -	[EXIT_REASON_RDSEED]                  = handle_invalid_op,
 -	[EXIT_REASON_XSAVES]                  = handle_xsaves,
 -	[EXIT_REASON_XRSTORS]                 = handle_xrstors,
 -	[EXIT_REASON_PML_FULL]		      = handle_pml_full,
 -	[EXIT_REASON_INVPCID]                 = handle_invpcid,
 -	[EXIT_REASON_VMFUNC]		      = handle_vmx_instruction,
 -	[EXIT_REASON_PREEMPTION_TIMER]	      = handle_preemption_timer,
 -	[EXIT_REASON_ENCLS]		      = handle_encls,
 -};
 +	spin_lock_init(&to_kvm_vmx(kvm)->ept_pointer_lock);
  
 -static const int kvm_vmx_max_exit_handlers =
 -	ARRAY_SIZE(kvm_vmx_exit_handlers);
 +	if (!ple_gap)
 +		kvm->arch.pause_in_guest = true;
  
 -static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 -{
 -	*info1 = vmcs_readl(EXIT_QUALIFICATION);
 -	*info2 = vmcs_read32(VM_EXIT_INTR_INFO);
 +	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
 +		switch (l1tf_mitigation) {
 +		case L1TF_MITIGATION_OFF:
 +		case L1TF_MITIGATION_FLUSH_NOWARN:
 +			/* 'I explicitly don't care' is set */
 +			break;
 +		case L1TF_MITIGATION_FLUSH:
 +		case L1TF_MITIGATION_FLUSH_NOSMT:
 +		case L1TF_MITIGATION_FULL:
 +			/*
 +			 * Warn upon starting the first VM in a potentially
 +			 * insecure environment.
 +			 */
 +			if (sched_smt_active())
 +				pr_warn_once(L1TF_MSG_SMT);
 +			if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
 +				pr_warn_once(L1TF_MSG_L1D);
 +			break;
 +		case L1TF_MITIGATION_FULL_FORCE:
 +			/* Flush is enforced */
 +			break;
 +		}
 +	}
 +	return 0;
  }
  
 -static void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)
 +static void __init vmx_check_processor_compat(void *rtn)
  {
 -	if (vmx->pml_pg) {
 -		__free_page(vmx->pml_pg);
 -		vmx->pml_pg = NULL;
 +	struct vmcs_config vmcs_conf;
 +	struct vmx_capability vmx_cap;
 +
 +	*(int *)rtn = 0;
 +	if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0)
 +		*(int *)rtn = -EIO;
 +	nested_vmx_setup_ctls_msrs(&vmcs_conf.nested, vmx_cap.ept, enable_apicv);
 +	if (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config)) != 0) {
 +		printk(KERN_ERR "kvm: CPU %d feature inconsistency!\n",
 +				smp_processor_id());
 +		*(int *)rtn = -EIO;
  	}
  }
  
* Unmerged path arch/x86/kvm/vmx/vmx.c
