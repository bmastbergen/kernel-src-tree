powerpc/powernv/npu: Add release_ownership hook

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 1b785611e1191751e1fb44ac3e89a0bd68ec7971
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/1b785611.failed

In order to make ATS work and translate addresses for arbitrary
LPID and PID, we need to program an NPU with LPID and allow PID wildcard
matching with a specific MSR mask.

This implements a helper to assign a GPU to LPAR and program the NPU
with a wildcard for PID and a helper to do clean-up. The helper takes
MSR (only DR/HV/PR/SF bits are allowed) to program them into NPU2 for
ATS checkout requests support.

This exports pnv_npu2_unmap_lpar_dev() as following patches will use it
from the VFIO driver.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 1b785611e1191751e1fb44ac3e89a0bd68ec7971)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/powernv/npu-dma.c
diff --cc arch/powerpc/platforms/powernv/npu-dma.c
index 7c8fc584b8ac,44f471959932..000000000000
--- a/arch/powerpc/platforms/powernv/npu-dma.c
+++ b/arch/powerpc/platforms/powernv/npu-dma.c
@@@ -352,11 -291,15 +352,12 @@@ void pnv_npu_try_dma_set_bypass(struct 
  	}
  }
  
 -#ifdef CONFIG_IOMMU_API
  /* Switch ownership from platform code to external user (e.g. VFIO) */
 -static void pnv_npu_take_ownership(struct iommu_table_group *table_group)
 +void pnv_npu_take_ownership(struct pnv_ioda_pe *npe)
  {
 -	struct pnv_ioda_pe *npe = container_of(table_group, struct pnv_ioda_pe,
 -			table_group);
  	struct pnv_phb *phb = npe->phb;
  	int64_t rc;
+ 	struct pci_dev *gpdev = NULL;
  
  	/*
  	 * Note: NPU has just a single TVE in the hardware which means that
@@@ -378,30 -321,315 +379,93 @@@
  		return;
  	}
  	pnv_pci_ioda2_tce_invalidate_entire(npe->phb, false);
+ 
+ 	get_gpu_pci_dev_and_pe(npe, &gpdev);
+ 	if (gpdev)
+ 		pnv_npu2_unmap_lpar_dev(gpdev);
  }
  
+ static void pnv_npu_release_ownership(struct iommu_table_group *table_group)
+ {
+ 	struct pnv_ioda_pe *npe = container_of(table_group, struct pnv_ioda_pe,
+ 			table_group);
+ 	struct pci_dev *gpdev = NULL;
+ 
+ 	get_gpu_pci_dev_and_pe(npe, &gpdev);
+ 	if (gpdev)
+ 		pnv_npu2_map_lpar_dev(gpdev, 0, MSR_DR | MSR_PR | MSR_HV);
+ }
+ 
++<<<<<<< HEAD
 +struct pnv_ioda_pe *pnv_pci_npu_setup_iommu(struct pnv_ioda_pe *npe)
++=======
+ static struct iommu_table_group_ops pnv_pci_npu_ops = {
+ 	.set_window = pnv_npu_set_window,
+ 	.unset_window = pnv_npu_unset_window,
+ 	.take_ownership = pnv_npu_take_ownership,
+ 	.release_ownership = pnv_npu_release_ownership,
+ };
+ #endif /* !CONFIG_IOMMU_API */
+ 
+ /*
+  * NPU2 ATS
+  */
+ /* Maximum possible number of ATSD MMIO registers per NPU */
+ #define NV_NMMU_ATSD_REGS 8
+ #define NV_NPU_MAX_PE_NUM	16
+ 
+ /*
+  * A compound NPU IOMMU group which might consist of 1 GPU + 2xNPUs (POWER8) or
+  * up to 3 x (GPU + 2xNPUs) (POWER9).
+  */
+ struct npu_comp {
+ 	struct iommu_table_group table_group;
+ 	int pe_num;
+ 	struct pnv_ioda_pe *pe[NV_NPU_MAX_PE_NUM];
+ };
+ 
+ /* An NPU descriptor, valid for POWER9 only */
+ struct npu {
+ 	int index;
+ 	__be64 *mmio_atsd_regs[NV_NMMU_ATSD_REGS];
+ 	unsigned int mmio_atsd_count;
+ 
+ 	/* Bitmask for MMIO register usage */
+ 	unsigned long mmio_atsd_usage;
+ 
+ 	/* Do we need to explicitly flush the nest mmu? */
+ 	bool nmmu_flush;
+ 
+ 	struct npu_comp npucomp;
+ };
+ 
+ #ifdef CONFIG_IOMMU_API
+ static long pnv_npu_peers_create_table_userspace(
+ 		struct iommu_table_group *table_group,
+ 		int num, __u32 page_shift, __u64 window_size, __u32 levels,
+ 		struct iommu_table **ptbl)
++>>>>>>> 1b785611e119 (powerpc/powernv/npu: Add release_ownership hook)
  {
 -	struct npu_comp *npucomp = container_of(table_group, struct npu_comp,
 -			table_group);
 -
 -	if (!npucomp->pe_num || !npucomp->pe[0] ||
 -			!npucomp->pe[0]->table_group.ops ||
 -			!npucomp->pe[0]->table_group.ops->create_table)
 -		return -EFAULT;
 -
 -	return npucomp->pe[0]->table_group.ops->create_table(
 -			&npucomp->pe[0]->table_group, num, page_shift,
 -			window_size, levels, ptbl);
 -}
 -
 -static long pnv_npu_peers_set_window(struct iommu_table_group *table_group,
 -		int num, struct iommu_table *tbl)
 -{
 -	int i, j;
 -	long ret = 0;
 -	struct npu_comp *npucomp = container_of(table_group, struct npu_comp,
 -			table_group);
 -
 -	for (i = 0; i < npucomp->pe_num; ++i) {
 -		struct pnv_ioda_pe *pe = npucomp->pe[i];
 -
 -		if (!pe->table_group.ops->set_window)
 -			continue;
 -
 -		ret = pe->table_group.ops->set_window(&pe->table_group,
 -				num, tbl);
 -		if (ret)
 -			break;
 -	}
 -
 -	if (ret) {
 -		for (j = 0; j < i; ++j) {
 -			struct pnv_ioda_pe *pe = npucomp->pe[j];
 -
 -			if (!pe->table_group.ops->unset_window)
 -				continue;
 -
 -			ret = pe->table_group.ops->unset_window(
 -					&pe->table_group, num);
 -			if (ret)
 -				break;
 -		}
 -	} else {
 -		table_group->tables[num] = iommu_tce_table_get(tbl);
 -	}
 -
 -	return ret;
 -}
 -
 -static long pnv_npu_peers_unset_window(struct iommu_table_group *table_group,
 -		int num)
 -{
 -	int i, j;
 -	long ret = 0;
 -	struct npu_comp *npucomp = container_of(table_group, struct npu_comp,
 -			table_group);
 -
 -	for (i = 0; i < npucomp->pe_num; ++i) {
 -		struct pnv_ioda_pe *pe = npucomp->pe[i];
 -
 -		WARN_ON(npucomp->table_group.tables[num] !=
 -				table_group->tables[num]);
 -		if (!npucomp->table_group.tables[num])
 -			continue;
 -
 -		if (!pe->table_group.ops->unset_window)
 -			continue;
 -
 -		ret = pe->table_group.ops->unset_window(&pe->table_group, num);
 -		if (ret)
 -			break;
 -	}
 -
 -	if (ret) {
 -		for (j = 0; j < i; ++j) {
 -			struct pnv_ioda_pe *pe = npucomp->pe[j];
 -
 -			if (!npucomp->table_group.tables[num])
 -				continue;
 -
 -			if (!pe->table_group.ops->set_window)
 -				continue;
 -
 -			ret = pe->table_group.ops->set_window(&pe->table_group,
 -					num, table_group->tables[num]);
 -			if (ret)
 -				break;
 -		}
 -	} else if (table_group->tables[num]) {
 -		iommu_tce_table_put(table_group->tables[num]);
 -		table_group->tables[num] = NULL;
 -	}
 -
 -	return ret;
 -}
 -
 -static void pnv_npu_peers_take_ownership(struct iommu_table_group *table_group)
 -{
 -	int i;
 -	struct npu_comp *npucomp = container_of(table_group, struct npu_comp,
 -			table_group);
 -
 -	for (i = 0; i < npucomp->pe_num; ++i) {
 -		struct pnv_ioda_pe *pe = npucomp->pe[i];
 -
 -		if (!pe->table_group.ops->take_ownership)
 -			continue;
 -		pe->table_group.ops->take_ownership(&pe->table_group);
 -	}
 -}
 -
 -static void pnv_npu_peers_release_ownership(
 -		struct iommu_table_group *table_group)
 -{
 -	int i;
 -	struct npu_comp *npucomp = container_of(table_group, struct npu_comp,
 -			table_group);
 -
 -	for (i = 0; i < npucomp->pe_num; ++i) {
 -		struct pnv_ioda_pe *pe = npucomp->pe[i];
 -
 -		if (!pe->table_group.ops->release_ownership)
 -			continue;
 -		pe->table_group.ops->release_ownership(&pe->table_group);
 -	}
 -}
 -
 -static struct iommu_table_group_ops pnv_npu_peers_ops = {
 -	.get_table_size = pnv_pci_ioda2_get_table_size,
 -	.create_table = pnv_npu_peers_create_table_userspace,
 -	.set_window = pnv_npu_peers_set_window,
 -	.unset_window = pnv_npu_peers_unset_window,
 -	.take_ownership = pnv_npu_peers_take_ownership,
 -	.release_ownership = pnv_npu_peers_release_ownership,
 -};
 -
 -static void pnv_comp_attach_table_group(struct npu_comp *npucomp,
 -		struct pnv_ioda_pe *pe)
 -{
 -	if (WARN_ON(npucomp->pe_num == NV_NPU_MAX_PE_NUM))
 -		return;
 -
 -	npucomp->pe[npucomp->pe_num] = pe;
 -	++npucomp->pe_num;
 -}
 -
 -struct iommu_table_group *pnv_try_setup_npu_table_group(struct pnv_ioda_pe *pe)
 -{
 -	struct iommu_table_group *table_group;
 -	struct npu_comp *npucomp;
 -	struct pci_dev *gpdev = NULL;
 -	struct pci_controller *hose;
 -	struct pci_dev *npdev = NULL;
 -
 -	list_for_each_entry(gpdev, &pe->pbus->devices, bus_list) {
 -		npdev = pnv_pci_get_npu_dev(gpdev, 0);
 -		if (npdev)
 -			break;
 -	}
 -
 -	if (!npdev)
 -		/* It is not an NPU attached device, skip */
 -		return NULL;
 -
 -	hose = pci_bus_to_host(npdev->bus);
 -
 -	if (hose->npu) {
 -		table_group = &hose->npu->npucomp.table_group;
 -
 -		if (!table_group->group) {
 -			table_group->ops = &pnv_npu_peers_ops;
 -			iommu_register_group(table_group,
 -					hose->global_number,
 -					pe->pe_number);
 -		}
 -	} else {
 -		/* Create a group for 1 GPU and attached NPUs for POWER8 */
 -		pe->npucomp = kzalloc(sizeof(pe->npucomp), GFP_KERNEL);
 -		table_group = &pe->npucomp->table_group;
 -		table_group->ops = &pnv_npu_peers_ops;
 -		iommu_register_group(table_group, hose->global_number,
 -				pe->pe_number);
 -	}
 -
 -	/* Steal capabilities from a GPU PE */
 -	table_group->max_dynamic_windows_supported =
 -		pe->table_group.max_dynamic_windows_supported;
 -	table_group->tce32_start = pe->table_group.tce32_start;
 -	table_group->tce32_size = pe->table_group.tce32_size;
 -	table_group->max_levels = pe->table_group.max_levels;
 -	if (!table_group->pgsizes)
 -		table_group->pgsizes = pe->table_group.pgsizes;
 -
 -	npucomp = container_of(table_group, struct npu_comp, table_group);
 -	pnv_comp_attach_table_group(npucomp, pe);
 -
 -	return table_group;
 -}
 -
 -struct iommu_table_group *pnv_npu_compound_attach(struct pnv_ioda_pe *pe)
 -{
 -	struct iommu_table_group *table_group;
 -	struct npu_comp *npucomp;
 -	struct pci_dev *gpdev = NULL;
 -	struct pci_dev *npdev;
 -	struct pnv_ioda_pe *gpe = get_gpu_pci_dev_and_pe(pe, &gpdev);
 +	struct pnv_phb *phb = npe->phb;
 +	struct pci_bus *pbus = phb->hose->bus;
 +	struct pci_dev *npdev, *gpdev = NULL, *gptmp;
 +	struct pnv_ioda_pe *gpe = get_gpu_pci_dev_and_pe(npe, &gpdev);
  
 -	WARN_ON(!(pe->flags & PNV_IODA_PE_DEV));
 -	if (!gpe)
 +	if (!gpe || !gpdev)
  		return NULL;
  
 -	/*
 -	 * IODA2 bridges get this set up from pci_controller_ops::setup_bridge
 -	 * but NPU bridges do not have this hook defined so we do it here.
 -	 * We do not setup other table group parameters as they won't be used
 -	 * anyway - NVLink bridges are subordinate PEs.
 -	 */
 -	pe->table_group.ops = &pnv_pci_npu_ops;
 +	list_for_each_entry(npdev, &pbus->devices, bus_list) {
 +		gptmp = pnv_pci_get_gpu_dev(npdev);
  
 -	table_group = iommu_group_get_iommudata(
 -			iommu_group_get(&gpdev->dev));
 -
 -	/*
 -	 * On P9 NPU PHB and PCI PHB support different page sizes,
 -	 * keep only matching. We expect here that NVLink bridge PE pgsizes is
 -	 * initialized by the caller.
 -	 */
 -	table_group->pgsizes &= pe->table_group.pgsizes;
 -	npucomp = container_of(table_group, struct npu_comp, table_group);
 -	pnv_comp_attach_table_group(npucomp, pe);
 -
 -	list_for_each_entry(npdev, &pe->phb->hose->bus->devices, bus_list) {
 -		struct pci_dev *gpdevtmp = pnv_pci_get_gpu_dev(npdev);
 -
 -		if (gpdevtmp != gpdev)
 +		if (gptmp != gpdev)
  			continue;
  
 -		iommu_add_device(table_group, &npdev->dev);
 +		pe_info(gpe, "Attached NPU %s\n", dev_name(&npdev->dev));
 +		iommu_group_add_device(gpe->table_group.group, &npdev->dev);
  	}
  
 -	return table_group;
 +	return gpe;
  }
 -#endif /* CONFIG_IOMMU_API */
  
  /* Maximum number of nvlinks per npu */
  #define NV_MAX_LINKS 6
@@@ -960,38 -1168,122 +1024,72 @@@ int pnv_npu2_init(struct pnv_phb *phb
  {
  	unsigned int i;
  	u64 mmio_atsd;
 +	struct device_node *dn;
 +	struct pci_dev *gpdev;
  	static int npu_index;
 -	struct npu *npu;
 -	int ret;
 -
 -	npu = kzalloc(sizeof(*npu), GFP_KERNEL);
 -	if (!npu)
 -		return -ENOMEM;
 -
 -	npu->nmmu_flush = of_property_read_bool(hose->dn, "ibm,nmmu-flush");
 +	uint64_t rc = 0;
 +
 +	phb->npu.nmmu_flush =
 +		of_property_read_bool(phb->hose->dn, "ibm,nmmu-flush");
 +	for_each_child_of_node(phb->hose->dn, dn) {
 +		gpdev = pnv_pci_get_gpu_dev(get_pci_dev(dn));
 +		if (gpdev) {
 +			rc = opal_npu_map_lpar(phb->opal_id,
 +				PCI_DEVID(gpdev->bus->number, gpdev->devfn),
 +				0, 0);
 +			if (rc)
 +				dev_err(&gpdev->dev,
 +					"Error %lld mapping device to LPAR\n",
 +					rc);
 +		}
 +	}
  
 -	for (i = 0; !of_property_read_u64_index(hose->dn, "ibm,mmio-atsd",
 +	for (i = 0; !of_property_read_u64_index(phb->hose->dn, "ibm,mmio-atsd",
  							i, &mmio_atsd); i++)
 -		npu->mmio_atsd_regs[i] = ioremap(mmio_atsd, 32);
 +		phb->npu.mmio_atsd_regs[i] = ioremap(mmio_atsd, 32);
  
 -	pr_info("NPU%d: Found %d MMIO ATSD registers", hose->global_number, i);
 -	npu->mmio_atsd_count = i;
 -	npu->mmio_atsd_usage = 0;
 +	pr_info("NPU%lld: Found %d MMIO ATSD registers", phb->opal_id, i);
 +	phb->npu.mmio_atsd_count = i;
 +	phb->npu.mmio_atsd_usage = 0;
  	npu_index++;
 -	if (WARN_ON(npu_index >= NV_MAX_NPUS)) {
 -		ret = -ENOSPC;
 -		goto fail_exit;
 -	}
 +	if (WARN_ON(npu_index >= NV_MAX_NPUS))
 +		return -ENOSPC;
  	max_npu2_index = npu_index;
 -	npu->index = npu_index;
 -	hose->npu = npu;
 +	phb->npu.index = npu_index;
  
  	return 0;
 -
 -fail_exit:
 -	for (i = 0; i < npu->mmio_atsd_count; ++i)
 -		iounmap(npu->mmio_atsd_regs[i]);
 -
 -	kfree(npu);
 -
 -	return ret;
 -}
 -
 -int pnv_npu2_map_lpar_dev(struct pci_dev *gpdev, unsigned int lparid,
 -		unsigned long msr)
 -{
 -	int ret;
 -	struct pci_dev *npdev = pnv_pci_get_npu_dev(gpdev, 0);
 -	struct pci_controller *hose;
 -	struct pnv_phb *nphb;
 -
 -	if (!npdev)
 -		return -ENODEV;
 -
 -	hose = pci_bus_to_host(npdev->bus);
 -	nphb = hose->private_data;
 -
 -	dev_dbg(&gpdev->dev, "Map LPAR opalid=%llu lparid=%u\n",
 -			nphb->opal_id, lparid);
 -	/*
 -	 * Currently we only support radix and non-zero LPCR only makes sense
 -	 * for hash tables so skiboot expects the LPCR parameter to be a zero.
 -	 */
 -	ret = opal_npu_map_lpar(nphb->opal_id,
 -			PCI_DEVID(gpdev->bus->number, gpdev->devfn), lparid,
 -			0 /* LPCR bits */);
 -	if (ret) {
 -		dev_err(&gpdev->dev, "Error %d mapping device to LPAR\n", ret);
 -		return ret;
 -	}
 -
 -	dev_dbg(&gpdev->dev, "init context opalid=%llu msr=%lx\n",
 -			nphb->opal_id, msr);
 -	ret = opal_npu_init_context(nphb->opal_id, 0/*__unused*/, msr,
 -			PCI_DEVID(gpdev->bus->number, gpdev->devfn));
 -	if (ret < 0)
 -		dev_err(&gpdev->dev, "Failed to init context: %d\n", ret);
 -	else
 -		ret = 0;
 -
 -	return 0;
 -}
 -EXPORT_SYMBOL_GPL(pnv_npu2_map_lpar_dev);
 -
 -void pnv_npu2_map_lpar(struct pnv_ioda_pe *gpe, unsigned long msr)
 -{
 -	struct pci_dev *gpdev;
 -
 -	list_for_each_entry(gpdev, &gpe->pbus->devices, bus_list)
 -		pnv_npu2_map_lpar_dev(gpdev, 0, msr);
  }
+ 
+ int pnv_npu2_unmap_lpar_dev(struct pci_dev *gpdev)
+ {
+ 	int ret;
+ 	struct pci_dev *npdev = pnv_pci_get_npu_dev(gpdev, 0);
+ 	struct pci_controller *hose;
+ 	struct pnv_phb *nphb;
+ 
+ 	if (!npdev)
+ 		return -ENODEV;
+ 
+ 	hose = pci_bus_to_host(npdev->bus);
+ 	nphb = hose->private_data;
+ 
+ 	dev_dbg(&gpdev->dev, "destroy context opalid=%llu\n",
+ 			nphb->opal_id);
+ 	ret = opal_npu_destroy_context(nphb->opal_id, 0/*__unused*/,
+ 			PCI_DEVID(gpdev->bus->number, gpdev->devfn));
+ 	if (ret < 0) {
+ 		dev_err(&gpdev->dev, "Failed to destroy context: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+ 	/* Set LPID to 0 anyway, just to be safe */
+ 	dev_dbg(&gpdev->dev, "Map LPAR opalid=%llu lparid=0\n", nphb->opal_id);
+ 	ret = opal_npu_map_lpar(nphb->opal_id,
+ 			PCI_DEVID(gpdev->bus->number, gpdev->devfn), 0 /*LPID*/,
+ 			0 /* LPCR bits */);
+ 	if (ret)
+ 		dev_err(&gpdev->dev, "Error %d mapping device to LPAR\n", ret);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(pnv_npu2_unmap_lpar_dev);
* Unmerged path arch/powerpc/platforms/powernv/npu-dma.c
