KVM: nVMX: Move vmcs12 code to dedicated files

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 609363cf81fcbd2c7fc93d1f920cef3a71154de8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/609363cf.failed

vmcs12 is the KVM-defined struct used to track a nested VMCS, e.g. a
VMCS created by L1 for L2.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 609363cf81fcbd2c7fc93d1f920cef3a71154de8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmcs.h
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 461e3698b5ee,ebe500f48bdf..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -16,49 -16,50 +16,60 @@@
   *
   */
  
 -#include <linux/frame.h>
 -#include <linux/highmem.h>
 -#include <linux/hrtimer.h>
 -#include <linux/kernel.h>
 +#include "irq.h"
 +#include "mmu.h"
 +#include "cpuid.h"
 +#include "lapic.h"
 +
  #include <linux/kvm_host.h>
  #include <linux/module.h>
 +#include <linux/kernel.h>
 +#include <linux/mm.h>
 +#include <linux/highmem.h>
 +#include <linux/sched.h>
 +#include <linux/sched/smt.h>
  #include <linux/moduleparam.h>
  #include <linux/mod_devicetable.h>
++<<<<<<< HEAD
 +#include <linux/trace_events.h>
++=======
+ #include <linux/mm.h>
+ #include <linux/sched.h>
++>>>>>>> 609363cf81fc (KVM: nVMX: Move vmcs12 code to dedicated files)
  #include <linux/slab.h>
  #include <linux/tboot.h>
 -#include <linux/trace_events.h>
 +#include <linux/hrtimer.h>
 +#include <linux/frame.h>
 +#include <linux/nospec.h>
 +#include "kvm_cache_regs.h"
 +#include "x86.h"
  
 -#include <asm/apic.h>
  #include <asm/asm.h>
  #include <asm/cpu.h>
 -#include <asm/debugreg.h>
 +#include <asm/io.h>
  #include <asm/desc.h>
 +#include <asm/vmx.h>
 +#include <asm/virtext.h>
 +#include <asm/mce.h>
  #include <asm/fpu/internal.h>
 -#include <asm/io.h>
 -#include <asm/irq_remapping.h>
 -#include <asm/kexec.h>
  #include <asm/perf_event.h>
 -#include <asm/mce.h>
 +#include <asm/debugreg.h>
 +#include <asm/kexec.h>
 +#include <asm/apic.h>
 +#include <asm/irq_remapping.h>
  #include <asm/mmu_context.h>
 -#include <asm/mshyperv.h>
  #include <asm/spec-ctrl.h>
 -#include <asm/virtext.h>
 -#include <asm/vmx.h>
 +#include <asm/mshyperv.h>
  
 -#include "capabilities.h"
 -#include "cpuid.h"
 -#include "evmcs.h"
 -#include "hyperv.h"
 -#include "irq.h"
 -#include "kvm_cache_regs.h"
 -#include "lapic.h"
 -#include "mmu.h"
 -#include "pmu.h"
  #include "trace.h"
++<<<<<<< HEAD
 +#include "pmu.h"
 +#include "vmx_evmcs.h"
++=======
+ #include "vmcs.h"
+ #include "vmcs12.h"
+ #include "x86.h"
++>>>>>>> 609363cf81fc (KVM: nVMX: Move vmcs12 code to dedicated files)
  
  #define __ex(x) __kvm_handle_fault_on_reboot(x)
  #define __ex_clear(x, reg) \
@@@ -419,396 -369,6 +430,399 @@@ struct shared_msr_entry 
  };
  
  /*
++<<<<<<< HEAD
 + * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a
 + * single nested guest (L2), hence the name vmcs12. Any VMX implementation has
 + * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is
 + * stored in guest memory specified by VMPTRLD, but is opaque to the guest,
 + * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.
 + * More than one of these structures may exist, if L1 runs multiple L2 guests.
 + * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the
 + * underlying hardware which will be used to run L2.
 + * This structure is packed to ensure that its layout is identical across
 + * machines (necessary for live migration).
 + *
 + * IMPORTANT: Changing the layout of existing fields in this structure
 + * will break save/restore compatibility with older kvm releases. When
 + * adding new fields, either use space in the reserved padding* arrays
 + * or add the new fields to the end of the structure.
 + */
 +typedef u64 natural_width;
 +struct __packed vmcs12 {
 +	/* According to the Intel spec, a VMCS region must start with the
 +	 * following two fields. Then follow implementation-specific data.
 +	 */
 +	struct vmcs_hdr hdr;
 +	u32 abort;
 +
 +	u32 launch_state; /* set to 0 by VMCLEAR, to 1 by VMLAUNCH */
 +	u32 padding[7]; /* room for future expansion */
 +
 +	u64 io_bitmap_a;
 +	u64 io_bitmap_b;
 +	u64 msr_bitmap;
 +	u64 vm_exit_msr_store_addr;
 +	u64 vm_exit_msr_load_addr;
 +	u64 vm_entry_msr_load_addr;
 +	u64 tsc_offset;
 +	u64 virtual_apic_page_addr;
 +	u64 apic_access_addr;
 +	u64 posted_intr_desc_addr;
 +	u64 ept_pointer;
 +	u64 eoi_exit_bitmap0;
 +	u64 eoi_exit_bitmap1;
 +	u64 eoi_exit_bitmap2;
 +	u64 eoi_exit_bitmap3;
 +	u64 xss_exit_bitmap;
 +	u64 guest_physical_address;
 +	u64 vmcs_link_pointer;
 +	u64 guest_ia32_debugctl;
 +	u64 guest_ia32_pat;
 +	u64 guest_ia32_efer;
 +	u64 guest_ia32_perf_global_ctrl;
 +	u64 guest_pdptr0;
 +	u64 guest_pdptr1;
 +	u64 guest_pdptr2;
 +	u64 guest_pdptr3;
 +	u64 guest_bndcfgs;
 +	u64 host_ia32_pat;
 +	u64 host_ia32_efer;
 +	u64 host_ia32_perf_global_ctrl;
 +	u64 vmread_bitmap;
 +	u64 vmwrite_bitmap;
 +	u64 vm_function_control;
 +	u64 eptp_list_address;
 +	u64 pml_address;
 +	u64 padding64[3]; /* room for future expansion */
 +	/*
 +	 * To allow migration of L1 (complete with its L2 guests) between
 +	 * machines of different natural widths (32 or 64 bit), we cannot have
 +	 * unsigned long fields with no explict size. We use u64 (aliased
 +	 * natural_width) instead. Luckily, x86 is little-endian.
 +	 */
 +	natural_width cr0_guest_host_mask;
 +	natural_width cr4_guest_host_mask;
 +	natural_width cr0_read_shadow;
 +	natural_width cr4_read_shadow;
 +	natural_width cr3_target_value0;
 +	natural_width cr3_target_value1;
 +	natural_width cr3_target_value2;
 +	natural_width cr3_target_value3;
 +	natural_width exit_qualification;
 +	natural_width guest_linear_address;
 +	natural_width guest_cr0;
 +	natural_width guest_cr3;
 +	natural_width guest_cr4;
 +	natural_width guest_es_base;
 +	natural_width guest_cs_base;
 +	natural_width guest_ss_base;
 +	natural_width guest_ds_base;
 +	natural_width guest_fs_base;
 +	natural_width guest_gs_base;
 +	natural_width guest_ldtr_base;
 +	natural_width guest_tr_base;
 +	natural_width guest_gdtr_base;
 +	natural_width guest_idtr_base;
 +	natural_width guest_dr7;
 +	natural_width guest_rsp;
 +	natural_width guest_rip;
 +	natural_width guest_rflags;
 +	natural_width guest_pending_dbg_exceptions;
 +	natural_width guest_sysenter_esp;
 +	natural_width guest_sysenter_eip;
 +	natural_width host_cr0;
 +	natural_width host_cr3;
 +	natural_width host_cr4;
 +	natural_width host_fs_base;
 +	natural_width host_gs_base;
 +	natural_width host_tr_base;
 +	natural_width host_gdtr_base;
 +	natural_width host_idtr_base;
 +	natural_width host_ia32_sysenter_esp;
 +	natural_width host_ia32_sysenter_eip;
 +	natural_width host_rsp;
 +	natural_width host_rip;
 +	natural_width paddingl[8]; /* room for future expansion */
 +	u32 pin_based_vm_exec_control;
 +	u32 cpu_based_vm_exec_control;
 +	u32 exception_bitmap;
 +	u32 page_fault_error_code_mask;
 +	u32 page_fault_error_code_match;
 +	u32 cr3_target_count;
 +	u32 vm_exit_controls;
 +	u32 vm_exit_msr_store_count;
 +	u32 vm_exit_msr_load_count;
 +	u32 vm_entry_controls;
 +	u32 vm_entry_msr_load_count;
 +	u32 vm_entry_intr_info_field;
 +	u32 vm_entry_exception_error_code;
 +	u32 vm_entry_instruction_len;
 +	u32 tpr_threshold;
 +	u32 secondary_vm_exec_control;
 +	u32 vm_instruction_error;
 +	u32 vm_exit_reason;
 +	u32 vm_exit_intr_info;
 +	u32 vm_exit_intr_error_code;
 +	u32 idt_vectoring_info_field;
 +	u32 idt_vectoring_error_code;
 +	u32 vm_exit_instruction_len;
 +	u32 vmx_instruction_info;
 +	u32 guest_es_limit;
 +	u32 guest_cs_limit;
 +	u32 guest_ss_limit;
 +	u32 guest_ds_limit;
 +	u32 guest_fs_limit;
 +	u32 guest_gs_limit;
 +	u32 guest_ldtr_limit;
 +	u32 guest_tr_limit;
 +	u32 guest_gdtr_limit;
 +	u32 guest_idtr_limit;
 +	u32 guest_es_ar_bytes;
 +	u32 guest_cs_ar_bytes;
 +	u32 guest_ss_ar_bytes;
 +	u32 guest_ds_ar_bytes;
 +	u32 guest_fs_ar_bytes;
 +	u32 guest_gs_ar_bytes;
 +	u32 guest_ldtr_ar_bytes;
 +	u32 guest_tr_ar_bytes;
 +	u32 guest_interruptibility_info;
 +	u32 guest_activity_state;
 +	u32 guest_sysenter_cs;
 +	u32 host_ia32_sysenter_cs;
 +	u32 vmx_preemption_timer_value;
 +	u32 padding32[7]; /* room for future expansion */
 +	u16 virtual_processor_id;
 +	u16 posted_intr_nv;
 +	u16 guest_es_selector;
 +	u16 guest_cs_selector;
 +	u16 guest_ss_selector;
 +	u16 guest_ds_selector;
 +	u16 guest_fs_selector;
 +	u16 guest_gs_selector;
 +	u16 guest_ldtr_selector;
 +	u16 guest_tr_selector;
 +	u16 guest_intr_status;
 +	u16 host_es_selector;
 +	u16 host_cs_selector;
 +	u16 host_ss_selector;
 +	u16 host_ds_selector;
 +	u16 host_fs_selector;
 +	u16 host_gs_selector;
 +	u16 host_tr_selector;
 +	u16 guest_pml_index;
 +};
 +
 +/*
 + * For save/restore compatibility, the vmcs12 field offsets must not change.
 + */
 +#define CHECK_OFFSET(field, loc)				\
 +	BUILD_BUG_ON_MSG(offsetof(struct vmcs12, field) != (loc),	\
 +		"Offset of " #field " in struct vmcs12 has changed.")
 +
 +static inline void vmx_check_vmcs12_offsets(void) {
 +	CHECK_OFFSET(hdr, 0);
 +	CHECK_OFFSET(abort, 4);
 +	CHECK_OFFSET(launch_state, 8);
 +	CHECK_OFFSET(io_bitmap_a, 40);
 +	CHECK_OFFSET(io_bitmap_b, 48);
 +	CHECK_OFFSET(msr_bitmap, 56);
 +	CHECK_OFFSET(vm_exit_msr_store_addr, 64);
 +	CHECK_OFFSET(vm_exit_msr_load_addr, 72);
 +	CHECK_OFFSET(vm_entry_msr_load_addr, 80);
 +	CHECK_OFFSET(tsc_offset, 88);
 +	CHECK_OFFSET(virtual_apic_page_addr, 96);
 +	CHECK_OFFSET(apic_access_addr, 104);
 +	CHECK_OFFSET(posted_intr_desc_addr, 112);
 +	CHECK_OFFSET(ept_pointer, 120);
 +	CHECK_OFFSET(eoi_exit_bitmap0, 128);
 +	CHECK_OFFSET(eoi_exit_bitmap1, 136);
 +	CHECK_OFFSET(eoi_exit_bitmap2, 144);
 +	CHECK_OFFSET(eoi_exit_bitmap3, 152);
 +	CHECK_OFFSET(xss_exit_bitmap, 160);
 +	CHECK_OFFSET(guest_physical_address, 168);
 +	CHECK_OFFSET(vmcs_link_pointer, 176);
 +	CHECK_OFFSET(guest_ia32_debugctl, 184);
 +	CHECK_OFFSET(guest_ia32_pat, 192);
 +	CHECK_OFFSET(guest_ia32_efer, 200);
 +	CHECK_OFFSET(guest_ia32_perf_global_ctrl, 208);
 +	CHECK_OFFSET(guest_pdptr0, 216);
 +	CHECK_OFFSET(guest_pdptr1, 224);
 +	CHECK_OFFSET(guest_pdptr2, 232);
 +	CHECK_OFFSET(guest_pdptr3, 240);
 +	CHECK_OFFSET(guest_bndcfgs, 248);
 +	CHECK_OFFSET(host_ia32_pat, 256);
 +	CHECK_OFFSET(host_ia32_efer, 264);
 +	CHECK_OFFSET(host_ia32_perf_global_ctrl, 272);
 +	CHECK_OFFSET(vmread_bitmap, 280);
 +	CHECK_OFFSET(vmwrite_bitmap, 288);
 +	CHECK_OFFSET(vm_function_control, 296);
 +	CHECK_OFFSET(eptp_list_address, 304);
 +	CHECK_OFFSET(pml_address, 312);
 +	CHECK_OFFSET(cr0_guest_host_mask, 344);
 +	CHECK_OFFSET(cr4_guest_host_mask, 352);
 +	CHECK_OFFSET(cr0_read_shadow, 360);
 +	CHECK_OFFSET(cr4_read_shadow, 368);
 +	CHECK_OFFSET(cr3_target_value0, 376);
 +	CHECK_OFFSET(cr3_target_value1, 384);
 +	CHECK_OFFSET(cr3_target_value2, 392);
 +	CHECK_OFFSET(cr3_target_value3, 400);
 +	CHECK_OFFSET(exit_qualification, 408);
 +	CHECK_OFFSET(guest_linear_address, 416);
 +	CHECK_OFFSET(guest_cr0, 424);
 +	CHECK_OFFSET(guest_cr3, 432);
 +	CHECK_OFFSET(guest_cr4, 440);
 +	CHECK_OFFSET(guest_es_base, 448);
 +	CHECK_OFFSET(guest_cs_base, 456);
 +	CHECK_OFFSET(guest_ss_base, 464);
 +	CHECK_OFFSET(guest_ds_base, 472);
 +	CHECK_OFFSET(guest_fs_base, 480);
 +	CHECK_OFFSET(guest_gs_base, 488);
 +	CHECK_OFFSET(guest_ldtr_base, 496);
 +	CHECK_OFFSET(guest_tr_base, 504);
 +	CHECK_OFFSET(guest_gdtr_base, 512);
 +	CHECK_OFFSET(guest_idtr_base, 520);
 +	CHECK_OFFSET(guest_dr7, 528);
 +	CHECK_OFFSET(guest_rsp, 536);
 +	CHECK_OFFSET(guest_rip, 544);
 +	CHECK_OFFSET(guest_rflags, 552);
 +	CHECK_OFFSET(guest_pending_dbg_exceptions, 560);
 +	CHECK_OFFSET(guest_sysenter_esp, 568);
 +	CHECK_OFFSET(guest_sysenter_eip, 576);
 +	CHECK_OFFSET(host_cr0, 584);
 +	CHECK_OFFSET(host_cr3, 592);
 +	CHECK_OFFSET(host_cr4, 600);
 +	CHECK_OFFSET(host_fs_base, 608);
 +	CHECK_OFFSET(host_gs_base, 616);
 +	CHECK_OFFSET(host_tr_base, 624);
 +	CHECK_OFFSET(host_gdtr_base, 632);
 +	CHECK_OFFSET(host_idtr_base, 640);
 +	CHECK_OFFSET(host_ia32_sysenter_esp, 648);
 +	CHECK_OFFSET(host_ia32_sysenter_eip, 656);
 +	CHECK_OFFSET(host_rsp, 664);
 +	CHECK_OFFSET(host_rip, 672);
 +	CHECK_OFFSET(pin_based_vm_exec_control, 744);
 +	CHECK_OFFSET(cpu_based_vm_exec_control, 748);
 +	CHECK_OFFSET(exception_bitmap, 752);
 +	CHECK_OFFSET(page_fault_error_code_mask, 756);
 +	CHECK_OFFSET(page_fault_error_code_match, 760);
 +	CHECK_OFFSET(cr3_target_count, 764);
 +	CHECK_OFFSET(vm_exit_controls, 768);
 +	CHECK_OFFSET(vm_exit_msr_store_count, 772);
 +	CHECK_OFFSET(vm_exit_msr_load_count, 776);
 +	CHECK_OFFSET(vm_entry_controls, 780);
 +	CHECK_OFFSET(vm_entry_msr_load_count, 784);
 +	CHECK_OFFSET(vm_entry_intr_info_field, 788);
 +	CHECK_OFFSET(vm_entry_exception_error_code, 792);
 +	CHECK_OFFSET(vm_entry_instruction_len, 796);
 +	CHECK_OFFSET(tpr_threshold, 800);
 +	CHECK_OFFSET(secondary_vm_exec_control, 804);
 +	CHECK_OFFSET(vm_instruction_error, 808);
 +	CHECK_OFFSET(vm_exit_reason, 812);
 +	CHECK_OFFSET(vm_exit_intr_info, 816);
 +	CHECK_OFFSET(vm_exit_intr_error_code, 820);
 +	CHECK_OFFSET(idt_vectoring_info_field, 824);
 +	CHECK_OFFSET(idt_vectoring_error_code, 828);
 +	CHECK_OFFSET(vm_exit_instruction_len, 832);
 +	CHECK_OFFSET(vmx_instruction_info, 836);
 +	CHECK_OFFSET(guest_es_limit, 840);
 +	CHECK_OFFSET(guest_cs_limit, 844);
 +	CHECK_OFFSET(guest_ss_limit, 848);
 +	CHECK_OFFSET(guest_ds_limit, 852);
 +	CHECK_OFFSET(guest_fs_limit, 856);
 +	CHECK_OFFSET(guest_gs_limit, 860);
 +	CHECK_OFFSET(guest_ldtr_limit, 864);
 +	CHECK_OFFSET(guest_tr_limit, 868);
 +	CHECK_OFFSET(guest_gdtr_limit, 872);
 +	CHECK_OFFSET(guest_idtr_limit, 876);
 +	CHECK_OFFSET(guest_es_ar_bytes, 880);
 +	CHECK_OFFSET(guest_cs_ar_bytes, 884);
 +	CHECK_OFFSET(guest_ss_ar_bytes, 888);
 +	CHECK_OFFSET(guest_ds_ar_bytes, 892);
 +	CHECK_OFFSET(guest_fs_ar_bytes, 896);
 +	CHECK_OFFSET(guest_gs_ar_bytes, 900);
 +	CHECK_OFFSET(guest_ldtr_ar_bytes, 904);
 +	CHECK_OFFSET(guest_tr_ar_bytes, 908);
 +	CHECK_OFFSET(guest_interruptibility_info, 912);
 +	CHECK_OFFSET(guest_activity_state, 916);
 +	CHECK_OFFSET(guest_sysenter_cs, 920);
 +	CHECK_OFFSET(host_ia32_sysenter_cs, 924);
 +	CHECK_OFFSET(vmx_preemption_timer_value, 928);
 +	CHECK_OFFSET(virtual_processor_id, 960);
 +	CHECK_OFFSET(posted_intr_nv, 962);
 +	CHECK_OFFSET(guest_es_selector, 964);
 +	CHECK_OFFSET(guest_cs_selector, 966);
 +	CHECK_OFFSET(guest_ss_selector, 968);
 +	CHECK_OFFSET(guest_ds_selector, 970);
 +	CHECK_OFFSET(guest_fs_selector, 972);
 +	CHECK_OFFSET(guest_gs_selector, 974);
 +	CHECK_OFFSET(guest_ldtr_selector, 976);
 +	CHECK_OFFSET(guest_tr_selector, 978);
 +	CHECK_OFFSET(guest_intr_status, 980);
 +	CHECK_OFFSET(host_es_selector, 982);
 +	CHECK_OFFSET(host_cs_selector, 984);
 +	CHECK_OFFSET(host_ss_selector, 986);
 +	CHECK_OFFSET(host_ds_selector, 988);
 +	CHECK_OFFSET(host_fs_selector, 990);
 +	CHECK_OFFSET(host_gs_selector, 992);
 +	CHECK_OFFSET(host_tr_selector, 994);
 +	CHECK_OFFSET(guest_pml_index, 996);
 +}
 +
 +/*
 + * VMCS12_REVISION is an arbitrary id that should be changed if the content or
 + * layout of struct vmcs12 is changed. MSR_IA32_VMX_BASIC returns this id, and
 + * VMPTRLD verifies that the VMCS region that L1 is loading contains this id.
 + *
 + * IMPORTANT: Changing this value will break save/restore compatibility with
 + * older kvm releases.
 + */
 +#define VMCS12_REVISION 0x11e57ed0
 +
 +/*
 + * VMCS12_SIZE is the number of bytes L1 should allocate for the VMXON region
 + * and any VMCS region. Although only sizeof(struct vmcs12) are used by the
 + * current implementation, 4K are reserved to avoid future complications.
 + */
 +#define VMCS12_SIZE 0x1000
 +
 +/*
 + * VMCS12_MAX_FIELD_INDEX is the highest index value used in any
 + * supported VMCS12 field encoding.
 + */
 +#define VMCS12_MAX_FIELD_INDEX 0x17
 +
 +struct nested_vmx_msrs {
 +	/*
 +	 * We only store the "true" versions of the VMX capability MSRs. We
 +	 * generate the "non-true" versions by setting the must-be-1 bits
 +	 * according to the SDM.
 +	 */
 +	u32 procbased_ctls_low;
 +	u32 procbased_ctls_high;
 +	u32 secondary_ctls_low;
 +	u32 secondary_ctls_high;
 +	u32 pinbased_ctls_low;
 +	u32 pinbased_ctls_high;
 +	u32 exit_ctls_low;
 +	u32 exit_ctls_high;
 +	u32 entry_ctls_low;
 +	u32 entry_ctls_high;
 +	u32 misc_low;
 +	u32 misc_high;
 +	u32 ept_caps;
 +	u32 vpid_caps;
 +	u64 basic;
 +	u64 cr0_fixed0;
 +	u64 cr0_fixed1;
 +	u64 cr4_fixed0;
 +	u64 cr4_fixed1;
 +	u64 vmcs_enum;
 +	u64 vmfunc_controls;
 +};
 +
 +/*
++=======
++>>>>>>> 609363cf81fc (KVM: nVMX: Move vmcs12 code to dedicated files)
   * The nested_vmx structure is part of vcpu_vmx, and holds information we need
   * for correct emulation of VMX (i.e., nested VMX) on this vcpu.
   */
* Unmerged path arch/x86/kvm/vmx/vmcs.h
diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index 13fd54de5449..79d97d837cf3 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -16,7 +16,7 @@ kvm-y			+= x86.o mmu.o emulate.o i8259.o irq.o lapic.o \
 			   i8254.o ioapic.o irq_comm.o cpuid.o pmu.o mtrr.o \
 			   hyperv.o page_track.o debugfs.o
 
-kvm-intel-y		+= vmx/vmx.o vmx/pmu_intel.o
+kvm-intel-y		+= vmx/vmx.o vmx/pmu_intel.o vmx/vmcs12.o
 kvm-amd-y		+= svm.o pmu_amd.o
 
 obj-$(CONFIG_KVM)	+= kvm.o
* Unmerged path arch/x86/kvm/vmx/vmcs.h
diff --git a/arch/x86/kvm/vmx/vmcs12.c b/arch/x86/kvm/vmx/vmcs12.c
new file mode 100644
index 000000000000..53dfb401316d
--- /dev/null
+++ b/arch/x86/kvm/vmx/vmcs12.c
@@ -0,0 +1,157 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#include "vmcs12.h"
+
+#define ROL16(val, n) ((u16)(((u16)(val) << (n)) | ((u16)(val) >> (16 - (n)))))
+#define VMCS12_OFFSET(x) offsetof(struct vmcs12, x)
+#define FIELD(number, name)	[ROL16(number, 6)] = VMCS12_OFFSET(name)
+#define FIELD64(number, name)						\
+	FIELD(number, name),						\
+	[ROL16(number##_HIGH, 6)] = VMCS12_OFFSET(name) + sizeof(u32)
+
+const unsigned short vmcs_field_to_offset_table[] = {
+	FIELD(VIRTUAL_PROCESSOR_ID, virtual_processor_id),
+	FIELD(POSTED_INTR_NV, posted_intr_nv),
+	FIELD(GUEST_ES_SELECTOR, guest_es_selector),
+	FIELD(GUEST_CS_SELECTOR, guest_cs_selector),
+	FIELD(GUEST_SS_SELECTOR, guest_ss_selector),
+	FIELD(GUEST_DS_SELECTOR, guest_ds_selector),
+	FIELD(GUEST_FS_SELECTOR, guest_fs_selector),
+	FIELD(GUEST_GS_SELECTOR, guest_gs_selector),
+	FIELD(GUEST_LDTR_SELECTOR, guest_ldtr_selector),
+	FIELD(GUEST_TR_SELECTOR, guest_tr_selector),
+	FIELD(GUEST_INTR_STATUS, guest_intr_status),
+	FIELD(GUEST_PML_INDEX, guest_pml_index),
+	FIELD(HOST_ES_SELECTOR, host_es_selector),
+	FIELD(HOST_CS_SELECTOR, host_cs_selector),
+	FIELD(HOST_SS_SELECTOR, host_ss_selector),
+	FIELD(HOST_DS_SELECTOR, host_ds_selector),
+	FIELD(HOST_FS_SELECTOR, host_fs_selector),
+	FIELD(HOST_GS_SELECTOR, host_gs_selector),
+	FIELD(HOST_TR_SELECTOR, host_tr_selector),
+	FIELD64(IO_BITMAP_A, io_bitmap_a),
+	FIELD64(IO_BITMAP_B, io_bitmap_b),
+	FIELD64(MSR_BITMAP, msr_bitmap),
+	FIELD64(VM_EXIT_MSR_STORE_ADDR, vm_exit_msr_store_addr),
+	FIELD64(VM_EXIT_MSR_LOAD_ADDR, vm_exit_msr_load_addr),
+	FIELD64(VM_ENTRY_MSR_LOAD_ADDR, vm_entry_msr_load_addr),
+	FIELD64(PML_ADDRESS, pml_address),
+	FIELD64(TSC_OFFSET, tsc_offset),
+	FIELD64(VIRTUAL_APIC_PAGE_ADDR, virtual_apic_page_addr),
+	FIELD64(APIC_ACCESS_ADDR, apic_access_addr),
+	FIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),
+	FIELD64(VM_FUNCTION_CONTROL, vm_function_control),
+	FIELD64(EPT_POINTER, ept_pointer),
+	FIELD64(EOI_EXIT_BITMAP0, eoi_exit_bitmap0),
+	FIELD64(EOI_EXIT_BITMAP1, eoi_exit_bitmap1),
+	FIELD64(EOI_EXIT_BITMAP2, eoi_exit_bitmap2),
+	FIELD64(EOI_EXIT_BITMAP3, eoi_exit_bitmap3),
+	FIELD64(EPTP_LIST_ADDRESS, eptp_list_address),
+	FIELD64(VMREAD_BITMAP, vmread_bitmap),
+	FIELD64(VMWRITE_BITMAP, vmwrite_bitmap),
+	FIELD64(XSS_EXIT_BITMAP, xss_exit_bitmap),
+	FIELD64(GUEST_PHYSICAL_ADDRESS, guest_physical_address),
+	FIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),
+	FIELD64(GUEST_IA32_DEBUGCTL, guest_ia32_debugctl),
+	FIELD64(GUEST_IA32_PAT, guest_ia32_pat),
+	FIELD64(GUEST_IA32_EFER, guest_ia32_efer),
+	FIELD64(GUEST_IA32_PERF_GLOBAL_CTRL, guest_ia32_perf_global_ctrl),
+	FIELD64(GUEST_PDPTR0, guest_pdptr0),
+	FIELD64(GUEST_PDPTR1, guest_pdptr1),
+	FIELD64(GUEST_PDPTR2, guest_pdptr2),
+	FIELD64(GUEST_PDPTR3, guest_pdptr3),
+	FIELD64(GUEST_BNDCFGS, guest_bndcfgs),
+	FIELD64(HOST_IA32_PAT, host_ia32_pat),
+	FIELD64(HOST_IA32_EFER, host_ia32_efer),
+	FIELD64(HOST_IA32_PERF_GLOBAL_CTRL, host_ia32_perf_global_ctrl),
+	FIELD(PIN_BASED_VM_EXEC_CONTROL, pin_based_vm_exec_control),
+	FIELD(CPU_BASED_VM_EXEC_CONTROL, cpu_based_vm_exec_control),
+	FIELD(EXCEPTION_BITMAP, exception_bitmap),
+	FIELD(PAGE_FAULT_ERROR_CODE_MASK, page_fault_error_code_mask),
+	FIELD(PAGE_FAULT_ERROR_CODE_MATCH, page_fault_error_code_match),
+	FIELD(CR3_TARGET_COUNT, cr3_target_count),
+	FIELD(VM_EXIT_CONTROLS, vm_exit_controls),
+	FIELD(VM_EXIT_MSR_STORE_COUNT, vm_exit_msr_store_count),
+	FIELD(VM_EXIT_MSR_LOAD_COUNT, vm_exit_msr_load_count),
+	FIELD(VM_ENTRY_CONTROLS, vm_entry_controls),
+	FIELD(VM_ENTRY_MSR_LOAD_COUNT, vm_entry_msr_load_count),
+	FIELD(VM_ENTRY_INTR_INFO_FIELD, vm_entry_intr_info_field),
+	FIELD(VM_ENTRY_EXCEPTION_ERROR_CODE, vm_entry_exception_error_code),
+	FIELD(VM_ENTRY_INSTRUCTION_LEN, vm_entry_instruction_len),
+	FIELD(TPR_THRESHOLD, tpr_threshold),
+	FIELD(SECONDARY_VM_EXEC_CONTROL, secondary_vm_exec_control),
+	FIELD(VM_INSTRUCTION_ERROR, vm_instruction_error),
+	FIELD(VM_EXIT_REASON, vm_exit_reason),
+	FIELD(VM_EXIT_INTR_INFO, vm_exit_intr_info),
+	FIELD(VM_EXIT_INTR_ERROR_CODE, vm_exit_intr_error_code),
+	FIELD(IDT_VECTORING_INFO_FIELD, idt_vectoring_info_field),
+	FIELD(IDT_VECTORING_ERROR_CODE, idt_vectoring_error_code),
+	FIELD(VM_EXIT_INSTRUCTION_LEN, vm_exit_instruction_len),
+	FIELD(VMX_INSTRUCTION_INFO, vmx_instruction_info),
+	FIELD(GUEST_ES_LIMIT, guest_es_limit),
+	FIELD(GUEST_CS_LIMIT, guest_cs_limit),
+	FIELD(GUEST_SS_LIMIT, guest_ss_limit),
+	FIELD(GUEST_DS_LIMIT, guest_ds_limit),
+	FIELD(GUEST_FS_LIMIT, guest_fs_limit),
+	FIELD(GUEST_GS_LIMIT, guest_gs_limit),
+	FIELD(GUEST_LDTR_LIMIT, guest_ldtr_limit),
+	FIELD(GUEST_TR_LIMIT, guest_tr_limit),
+	FIELD(GUEST_GDTR_LIMIT, guest_gdtr_limit),
+	FIELD(GUEST_IDTR_LIMIT, guest_idtr_limit),
+	FIELD(GUEST_ES_AR_BYTES, guest_es_ar_bytes),
+	FIELD(GUEST_CS_AR_BYTES, guest_cs_ar_bytes),
+	FIELD(GUEST_SS_AR_BYTES, guest_ss_ar_bytes),
+	FIELD(GUEST_DS_AR_BYTES, guest_ds_ar_bytes),
+	FIELD(GUEST_FS_AR_BYTES, guest_fs_ar_bytes),
+	FIELD(GUEST_GS_AR_BYTES, guest_gs_ar_bytes),
+	FIELD(GUEST_LDTR_AR_BYTES, guest_ldtr_ar_bytes),
+	FIELD(GUEST_TR_AR_BYTES, guest_tr_ar_bytes),
+	FIELD(GUEST_INTERRUPTIBILITY_INFO, guest_interruptibility_info),
+	FIELD(GUEST_ACTIVITY_STATE, guest_activity_state),
+	FIELD(GUEST_SYSENTER_CS, guest_sysenter_cs),
+	FIELD(HOST_IA32_SYSENTER_CS, host_ia32_sysenter_cs),
+	FIELD(VMX_PREEMPTION_TIMER_VALUE, vmx_preemption_timer_value),
+	FIELD(CR0_GUEST_HOST_MASK, cr0_guest_host_mask),
+	FIELD(CR4_GUEST_HOST_MASK, cr4_guest_host_mask),
+	FIELD(CR0_READ_SHADOW, cr0_read_shadow),
+	FIELD(CR4_READ_SHADOW, cr4_read_shadow),
+	FIELD(CR3_TARGET_VALUE0, cr3_target_value0),
+	FIELD(CR3_TARGET_VALUE1, cr3_target_value1),
+	FIELD(CR3_TARGET_VALUE2, cr3_target_value2),
+	FIELD(CR3_TARGET_VALUE3, cr3_target_value3),
+	FIELD(EXIT_QUALIFICATION, exit_qualification),
+	FIELD(GUEST_LINEAR_ADDRESS, guest_linear_address),
+	FIELD(GUEST_CR0, guest_cr0),
+	FIELD(GUEST_CR3, guest_cr3),
+	FIELD(GUEST_CR4, guest_cr4),
+	FIELD(GUEST_ES_BASE, guest_es_base),
+	FIELD(GUEST_CS_BASE, guest_cs_base),
+	FIELD(GUEST_SS_BASE, guest_ss_base),
+	FIELD(GUEST_DS_BASE, guest_ds_base),
+	FIELD(GUEST_FS_BASE, guest_fs_base),
+	FIELD(GUEST_GS_BASE, guest_gs_base),
+	FIELD(GUEST_LDTR_BASE, guest_ldtr_base),
+	FIELD(GUEST_TR_BASE, guest_tr_base),
+	FIELD(GUEST_GDTR_BASE, guest_gdtr_base),
+	FIELD(GUEST_IDTR_BASE, guest_idtr_base),
+	FIELD(GUEST_DR7, guest_dr7),
+	FIELD(GUEST_RSP, guest_rsp),
+	FIELD(GUEST_RIP, guest_rip),
+	FIELD(GUEST_RFLAGS, guest_rflags),
+	FIELD(GUEST_PENDING_DBG_EXCEPTIONS, guest_pending_dbg_exceptions),
+	FIELD(GUEST_SYSENTER_ESP, guest_sysenter_esp),
+	FIELD(GUEST_SYSENTER_EIP, guest_sysenter_eip),
+	FIELD(HOST_CR0, host_cr0),
+	FIELD(HOST_CR3, host_cr3),
+	FIELD(HOST_CR4, host_cr4),
+	FIELD(HOST_FS_BASE, host_fs_base),
+	FIELD(HOST_GS_BASE, host_gs_base),
+	FIELD(HOST_TR_BASE, host_tr_base),
+	FIELD(HOST_GDTR_BASE, host_gdtr_base),
+	FIELD(HOST_IDTR_BASE, host_idtr_base),
+	FIELD(HOST_IA32_SYSENTER_ESP, host_ia32_sysenter_esp),
+	FIELD(HOST_IA32_SYSENTER_EIP, host_ia32_sysenter_eip),
+	FIELD(HOST_RSP, host_rsp),
+	FIELD(HOST_RIP, host_rip),
+};
+const unsigned int nr_vmcs12_fields = ARRAY_SIZE(vmcs_field_to_offset_table);
diff --git a/arch/x86/kvm/vmx/vmcs12.h b/arch/x86/kvm/vmx/vmcs12.h
new file mode 100644
index 000000000000..3a742428ad17
--- /dev/null
+++ b/arch/x86/kvm/vmx/vmcs12.h
@@ -0,0 +1,462 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __KVM_X86_VMX_VMCS12_H
+#define __KVM_X86_VMX_VMCS12_H
+
+#include <linux/build_bug.h>
+
+#include "vmcs.h"
+
+/*
+ * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a
+ * single nested guest (L2), hence the name vmcs12. Any VMX implementation has
+ * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is
+ * stored in guest memory specified by VMPTRLD, but is opaque to the guest,
+ * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.
+ * More than one of these structures may exist, if L1 runs multiple L2 guests.
+ * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the
+ * underlying hardware which will be used to run L2.
+ * This structure is packed to ensure that its layout is identical across
+ * machines (necessary for live migration).
+ *
+ * IMPORTANT: Changing the layout of existing fields in this structure
+ * will break save/restore compatibility with older kvm releases. When
+ * adding new fields, either use space in the reserved padding* arrays
+ * or add the new fields to the end of the structure.
+ */
+typedef u64 natural_width;
+struct __packed vmcs12 {
+	/* According to the Intel spec, a VMCS region must start with the
+	 * following two fields. Then follow implementation-specific data.
+	 */
+	struct vmcs_hdr hdr;
+	u32 abort;
+
+	u32 launch_state; /* set to 0 by VMCLEAR, to 1 by VMLAUNCH */
+	u32 padding[7]; /* room for future expansion */
+
+	u64 io_bitmap_a;
+	u64 io_bitmap_b;
+	u64 msr_bitmap;
+	u64 vm_exit_msr_store_addr;
+	u64 vm_exit_msr_load_addr;
+	u64 vm_entry_msr_load_addr;
+	u64 tsc_offset;
+	u64 virtual_apic_page_addr;
+	u64 apic_access_addr;
+	u64 posted_intr_desc_addr;
+	u64 ept_pointer;
+	u64 eoi_exit_bitmap0;
+	u64 eoi_exit_bitmap1;
+	u64 eoi_exit_bitmap2;
+	u64 eoi_exit_bitmap3;
+	u64 xss_exit_bitmap;
+	u64 guest_physical_address;
+	u64 vmcs_link_pointer;
+	u64 guest_ia32_debugctl;
+	u64 guest_ia32_pat;
+	u64 guest_ia32_efer;
+	u64 guest_ia32_perf_global_ctrl;
+	u64 guest_pdptr0;
+	u64 guest_pdptr1;
+	u64 guest_pdptr2;
+	u64 guest_pdptr3;
+	u64 guest_bndcfgs;
+	u64 host_ia32_pat;
+	u64 host_ia32_efer;
+	u64 host_ia32_perf_global_ctrl;
+	u64 vmread_bitmap;
+	u64 vmwrite_bitmap;
+	u64 vm_function_control;
+	u64 eptp_list_address;
+	u64 pml_address;
+	u64 padding64[3]; /* room for future expansion */
+	/*
+	 * To allow migration of L1 (complete with its L2 guests) between
+	 * machines of different natural widths (32 or 64 bit), we cannot have
+	 * unsigned long fields with no explicit size. We use u64 (aliased
+	 * natural_width) instead. Luckily, x86 is little-endian.
+	 */
+	natural_width cr0_guest_host_mask;
+	natural_width cr4_guest_host_mask;
+	natural_width cr0_read_shadow;
+	natural_width cr4_read_shadow;
+	natural_width cr3_target_value0;
+	natural_width cr3_target_value1;
+	natural_width cr3_target_value2;
+	natural_width cr3_target_value3;
+	natural_width exit_qualification;
+	natural_width guest_linear_address;
+	natural_width guest_cr0;
+	natural_width guest_cr3;
+	natural_width guest_cr4;
+	natural_width guest_es_base;
+	natural_width guest_cs_base;
+	natural_width guest_ss_base;
+	natural_width guest_ds_base;
+	natural_width guest_fs_base;
+	natural_width guest_gs_base;
+	natural_width guest_ldtr_base;
+	natural_width guest_tr_base;
+	natural_width guest_gdtr_base;
+	natural_width guest_idtr_base;
+	natural_width guest_dr7;
+	natural_width guest_rsp;
+	natural_width guest_rip;
+	natural_width guest_rflags;
+	natural_width guest_pending_dbg_exceptions;
+	natural_width guest_sysenter_esp;
+	natural_width guest_sysenter_eip;
+	natural_width host_cr0;
+	natural_width host_cr3;
+	natural_width host_cr4;
+	natural_width host_fs_base;
+	natural_width host_gs_base;
+	natural_width host_tr_base;
+	natural_width host_gdtr_base;
+	natural_width host_idtr_base;
+	natural_width host_ia32_sysenter_esp;
+	natural_width host_ia32_sysenter_eip;
+	natural_width host_rsp;
+	natural_width host_rip;
+	natural_width paddingl[8]; /* room for future expansion */
+	u32 pin_based_vm_exec_control;
+	u32 cpu_based_vm_exec_control;
+	u32 exception_bitmap;
+	u32 page_fault_error_code_mask;
+	u32 page_fault_error_code_match;
+	u32 cr3_target_count;
+	u32 vm_exit_controls;
+	u32 vm_exit_msr_store_count;
+	u32 vm_exit_msr_load_count;
+	u32 vm_entry_controls;
+	u32 vm_entry_msr_load_count;
+	u32 vm_entry_intr_info_field;
+	u32 vm_entry_exception_error_code;
+	u32 vm_entry_instruction_len;
+	u32 tpr_threshold;
+	u32 secondary_vm_exec_control;
+	u32 vm_instruction_error;
+	u32 vm_exit_reason;
+	u32 vm_exit_intr_info;
+	u32 vm_exit_intr_error_code;
+	u32 idt_vectoring_info_field;
+	u32 idt_vectoring_error_code;
+	u32 vm_exit_instruction_len;
+	u32 vmx_instruction_info;
+	u32 guest_es_limit;
+	u32 guest_cs_limit;
+	u32 guest_ss_limit;
+	u32 guest_ds_limit;
+	u32 guest_fs_limit;
+	u32 guest_gs_limit;
+	u32 guest_ldtr_limit;
+	u32 guest_tr_limit;
+	u32 guest_gdtr_limit;
+	u32 guest_idtr_limit;
+	u32 guest_es_ar_bytes;
+	u32 guest_cs_ar_bytes;
+	u32 guest_ss_ar_bytes;
+	u32 guest_ds_ar_bytes;
+	u32 guest_fs_ar_bytes;
+	u32 guest_gs_ar_bytes;
+	u32 guest_ldtr_ar_bytes;
+	u32 guest_tr_ar_bytes;
+	u32 guest_interruptibility_info;
+	u32 guest_activity_state;
+	u32 guest_sysenter_cs;
+	u32 host_ia32_sysenter_cs;
+	u32 vmx_preemption_timer_value;
+	u32 padding32[7]; /* room for future expansion */
+	u16 virtual_processor_id;
+	u16 posted_intr_nv;
+	u16 guest_es_selector;
+	u16 guest_cs_selector;
+	u16 guest_ss_selector;
+	u16 guest_ds_selector;
+	u16 guest_fs_selector;
+	u16 guest_gs_selector;
+	u16 guest_ldtr_selector;
+	u16 guest_tr_selector;
+	u16 guest_intr_status;
+	u16 host_es_selector;
+	u16 host_cs_selector;
+	u16 host_ss_selector;
+	u16 host_ds_selector;
+	u16 host_fs_selector;
+	u16 host_gs_selector;
+	u16 host_tr_selector;
+	u16 guest_pml_index;
+};
+
+/*
+ * VMCS12_REVISION is an arbitrary id that should be changed if the content or
+ * layout of struct vmcs12 is changed. MSR_IA32_VMX_BASIC returns this id, and
+ * VMPTRLD verifies that the VMCS region that L1 is loading contains this id.
+ *
+ * IMPORTANT: Changing this value will break save/restore compatibility with
+ * older kvm releases.
+ */
+#define VMCS12_REVISION 0x11e57ed0
+
+/*
+ * VMCS12_SIZE is the number of bytes L1 should allocate for the VMXON region
+ * and any VMCS region. Although only sizeof(struct vmcs12) are used by the
+ * current implementation, 4K are reserved to avoid future complications.
+ */
+#define VMCS12_SIZE 0x1000
+
+/*
+ * VMCS12_MAX_FIELD_INDEX is the highest index value used in any
+ * supported VMCS12 field encoding.
+ */
+#define VMCS12_MAX_FIELD_INDEX 0x17
+
+/*
+ * For save/restore compatibility, the vmcs12 field offsets must not change.
+ */
+#define CHECK_OFFSET(field, loc)				\
+	BUILD_BUG_ON_MSG(offsetof(struct vmcs12, field) != (loc),	\
+		"Offset of " #field " in struct vmcs12 has changed.")
+
+static inline void vmx_check_vmcs12_offsets(void)
+{
+	CHECK_OFFSET(hdr, 0);
+	CHECK_OFFSET(abort, 4);
+	CHECK_OFFSET(launch_state, 8);
+	CHECK_OFFSET(io_bitmap_a, 40);
+	CHECK_OFFSET(io_bitmap_b, 48);
+	CHECK_OFFSET(msr_bitmap, 56);
+	CHECK_OFFSET(vm_exit_msr_store_addr, 64);
+	CHECK_OFFSET(vm_exit_msr_load_addr, 72);
+	CHECK_OFFSET(vm_entry_msr_load_addr, 80);
+	CHECK_OFFSET(tsc_offset, 88);
+	CHECK_OFFSET(virtual_apic_page_addr, 96);
+	CHECK_OFFSET(apic_access_addr, 104);
+	CHECK_OFFSET(posted_intr_desc_addr, 112);
+	CHECK_OFFSET(ept_pointer, 120);
+	CHECK_OFFSET(eoi_exit_bitmap0, 128);
+	CHECK_OFFSET(eoi_exit_bitmap1, 136);
+	CHECK_OFFSET(eoi_exit_bitmap2, 144);
+	CHECK_OFFSET(eoi_exit_bitmap3, 152);
+	CHECK_OFFSET(xss_exit_bitmap, 160);
+	CHECK_OFFSET(guest_physical_address, 168);
+	CHECK_OFFSET(vmcs_link_pointer, 176);
+	CHECK_OFFSET(guest_ia32_debugctl, 184);
+	CHECK_OFFSET(guest_ia32_pat, 192);
+	CHECK_OFFSET(guest_ia32_efer, 200);
+	CHECK_OFFSET(guest_ia32_perf_global_ctrl, 208);
+	CHECK_OFFSET(guest_pdptr0, 216);
+	CHECK_OFFSET(guest_pdptr1, 224);
+	CHECK_OFFSET(guest_pdptr2, 232);
+	CHECK_OFFSET(guest_pdptr3, 240);
+	CHECK_OFFSET(guest_bndcfgs, 248);
+	CHECK_OFFSET(host_ia32_pat, 256);
+	CHECK_OFFSET(host_ia32_efer, 264);
+	CHECK_OFFSET(host_ia32_perf_global_ctrl, 272);
+	CHECK_OFFSET(vmread_bitmap, 280);
+	CHECK_OFFSET(vmwrite_bitmap, 288);
+	CHECK_OFFSET(vm_function_control, 296);
+	CHECK_OFFSET(eptp_list_address, 304);
+	CHECK_OFFSET(pml_address, 312);
+	CHECK_OFFSET(cr0_guest_host_mask, 344);
+	CHECK_OFFSET(cr4_guest_host_mask, 352);
+	CHECK_OFFSET(cr0_read_shadow, 360);
+	CHECK_OFFSET(cr4_read_shadow, 368);
+	CHECK_OFFSET(cr3_target_value0, 376);
+	CHECK_OFFSET(cr3_target_value1, 384);
+	CHECK_OFFSET(cr3_target_value2, 392);
+	CHECK_OFFSET(cr3_target_value3, 400);
+	CHECK_OFFSET(exit_qualification, 408);
+	CHECK_OFFSET(guest_linear_address, 416);
+	CHECK_OFFSET(guest_cr0, 424);
+	CHECK_OFFSET(guest_cr3, 432);
+	CHECK_OFFSET(guest_cr4, 440);
+	CHECK_OFFSET(guest_es_base, 448);
+	CHECK_OFFSET(guest_cs_base, 456);
+	CHECK_OFFSET(guest_ss_base, 464);
+	CHECK_OFFSET(guest_ds_base, 472);
+	CHECK_OFFSET(guest_fs_base, 480);
+	CHECK_OFFSET(guest_gs_base, 488);
+	CHECK_OFFSET(guest_ldtr_base, 496);
+	CHECK_OFFSET(guest_tr_base, 504);
+	CHECK_OFFSET(guest_gdtr_base, 512);
+	CHECK_OFFSET(guest_idtr_base, 520);
+	CHECK_OFFSET(guest_dr7, 528);
+	CHECK_OFFSET(guest_rsp, 536);
+	CHECK_OFFSET(guest_rip, 544);
+	CHECK_OFFSET(guest_rflags, 552);
+	CHECK_OFFSET(guest_pending_dbg_exceptions, 560);
+	CHECK_OFFSET(guest_sysenter_esp, 568);
+	CHECK_OFFSET(guest_sysenter_eip, 576);
+	CHECK_OFFSET(host_cr0, 584);
+	CHECK_OFFSET(host_cr3, 592);
+	CHECK_OFFSET(host_cr4, 600);
+	CHECK_OFFSET(host_fs_base, 608);
+	CHECK_OFFSET(host_gs_base, 616);
+	CHECK_OFFSET(host_tr_base, 624);
+	CHECK_OFFSET(host_gdtr_base, 632);
+	CHECK_OFFSET(host_idtr_base, 640);
+	CHECK_OFFSET(host_ia32_sysenter_esp, 648);
+	CHECK_OFFSET(host_ia32_sysenter_eip, 656);
+	CHECK_OFFSET(host_rsp, 664);
+	CHECK_OFFSET(host_rip, 672);
+	CHECK_OFFSET(pin_based_vm_exec_control, 744);
+	CHECK_OFFSET(cpu_based_vm_exec_control, 748);
+	CHECK_OFFSET(exception_bitmap, 752);
+	CHECK_OFFSET(page_fault_error_code_mask, 756);
+	CHECK_OFFSET(page_fault_error_code_match, 760);
+	CHECK_OFFSET(cr3_target_count, 764);
+	CHECK_OFFSET(vm_exit_controls, 768);
+	CHECK_OFFSET(vm_exit_msr_store_count, 772);
+	CHECK_OFFSET(vm_exit_msr_load_count, 776);
+	CHECK_OFFSET(vm_entry_controls, 780);
+	CHECK_OFFSET(vm_entry_msr_load_count, 784);
+	CHECK_OFFSET(vm_entry_intr_info_field, 788);
+	CHECK_OFFSET(vm_entry_exception_error_code, 792);
+	CHECK_OFFSET(vm_entry_instruction_len, 796);
+	CHECK_OFFSET(tpr_threshold, 800);
+	CHECK_OFFSET(secondary_vm_exec_control, 804);
+	CHECK_OFFSET(vm_instruction_error, 808);
+	CHECK_OFFSET(vm_exit_reason, 812);
+	CHECK_OFFSET(vm_exit_intr_info, 816);
+	CHECK_OFFSET(vm_exit_intr_error_code, 820);
+	CHECK_OFFSET(idt_vectoring_info_field, 824);
+	CHECK_OFFSET(idt_vectoring_error_code, 828);
+	CHECK_OFFSET(vm_exit_instruction_len, 832);
+	CHECK_OFFSET(vmx_instruction_info, 836);
+	CHECK_OFFSET(guest_es_limit, 840);
+	CHECK_OFFSET(guest_cs_limit, 844);
+	CHECK_OFFSET(guest_ss_limit, 848);
+	CHECK_OFFSET(guest_ds_limit, 852);
+	CHECK_OFFSET(guest_fs_limit, 856);
+	CHECK_OFFSET(guest_gs_limit, 860);
+	CHECK_OFFSET(guest_ldtr_limit, 864);
+	CHECK_OFFSET(guest_tr_limit, 868);
+	CHECK_OFFSET(guest_gdtr_limit, 872);
+	CHECK_OFFSET(guest_idtr_limit, 876);
+	CHECK_OFFSET(guest_es_ar_bytes, 880);
+	CHECK_OFFSET(guest_cs_ar_bytes, 884);
+	CHECK_OFFSET(guest_ss_ar_bytes, 888);
+	CHECK_OFFSET(guest_ds_ar_bytes, 892);
+	CHECK_OFFSET(guest_fs_ar_bytes, 896);
+	CHECK_OFFSET(guest_gs_ar_bytes, 900);
+	CHECK_OFFSET(guest_ldtr_ar_bytes, 904);
+	CHECK_OFFSET(guest_tr_ar_bytes, 908);
+	CHECK_OFFSET(guest_interruptibility_info, 912);
+	CHECK_OFFSET(guest_activity_state, 916);
+	CHECK_OFFSET(guest_sysenter_cs, 920);
+	CHECK_OFFSET(host_ia32_sysenter_cs, 924);
+	CHECK_OFFSET(vmx_preemption_timer_value, 928);
+	CHECK_OFFSET(virtual_processor_id, 960);
+	CHECK_OFFSET(posted_intr_nv, 962);
+	CHECK_OFFSET(guest_es_selector, 964);
+	CHECK_OFFSET(guest_cs_selector, 966);
+	CHECK_OFFSET(guest_ss_selector, 968);
+	CHECK_OFFSET(guest_ds_selector, 970);
+	CHECK_OFFSET(guest_fs_selector, 972);
+	CHECK_OFFSET(guest_gs_selector, 974);
+	CHECK_OFFSET(guest_ldtr_selector, 976);
+	CHECK_OFFSET(guest_tr_selector, 978);
+	CHECK_OFFSET(guest_intr_status, 980);
+	CHECK_OFFSET(host_es_selector, 982);
+	CHECK_OFFSET(host_cs_selector, 984);
+	CHECK_OFFSET(host_ss_selector, 986);
+	CHECK_OFFSET(host_ds_selector, 988);
+	CHECK_OFFSET(host_fs_selector, 990);
+	CHECK_OFFSET(host_gs_selector, 992);
+	CHECK_OFFSET(host_tr_selector, 994);
+	CHECK_OFFSET(guest_pml_index, 996);
+}
+
+extern const unsigned short vmcs_field_to_offset_table[];
+extern const unsigned int nr_vmcs12_fields;
+
+#define ROL16(val, n) ((u16)(((u16)(val) << (n)) | ((u16)(val) >> (16 - (n)))))
+
+static inline short vmcs_field_to_offset(unsigned long field)
+{
+	unsigned short offset;
+	unsigned int index;
+
+	if (field >> 15)
+		return -ENOENT;
+
+	index = ROL16(field, 6);
+	if (index >= nr_vmcs12_fields)
+		return -ENOENT;
+
+	index = array_index_nospec(index, nr_vmcs12_fields);
+	offset = vmcs_field_to_offset_table[index];
+	if (offset == 0)
+		return -ENOENT;
+	return offset;
+}
+
+#undef ROL16
+
+/*
+ * Read a vmcs12 field. Since these can have varying lengths and we return
+ * one type, we chose the biggest type (u64) and zero-extend the return value
+ * to that size. Note that the caller, handle_vmread, might need to use only
+ * some of the bits we return here (e.g., on 32-bit guests, only 32 bits of
+ * 64-bit fields are to be returned).
+ */
+static inline int vmcs12_read_any(struct vmcs12 *vmcs12,
+				  unsigned long field, u64 *ret)
+{
+	short offset = vmcs_field_to_offset(field);
+	char *p;
+
+	if (offset < 0)
+		return offset;
+
+	p = (char *)vmcs12 + offset;
+
+	switch (vmcs_field_width(field)) {
+	case VMCS_FIELD_WIDTH_NATURAL_WIDTH:
+		*ret = *((natural_width *)p);
+		return 0;
+	case VMCS_FIELD_WIDTH_U16:
+		*ret = *((u16 *)p);
+		return 0;
+	case VMCS_FIELD_WIDTH_U32:
+		*ret = *((u32 *)p);
+		return 0;
+	case VMCS_FIELD_WIDTH_U64:
+		*ret = *((u64 *)p);
+		return 0;
+	default:
+		WARN_ON(1);
+		return -ENOENT;
+	}
+}
+
+static inline int vmcs12_write_any(struct vmcs12 *vmcs12,
+				   unsigned long field, u64 field_value){
+	short offset = vmcs_field_to_offset(field);
+	char *p = (char *)vmcs12 + offset;
+
+	if (offset < 0)
+		return offset;
+
+	switch (vmcs_field_width(field)) {
+	case VMCS_FIELD_WIDTH_U16:
+		*(u16 *)p = field_value;
+		return 0;
+	case VMCS_FIELD_WIDTH_U32:
+		*(u32 *)p = field_value;
+		return 0;
+	case VMCS_FIELD_WIDTH_U64:
+		*(u64 *)p = field_value;
+		return 0;
+	case VMCS_FIELD_WIDTH_NATURAL_WIDTH:
+		*(natural_width *)p = field_value;
+		return 0;
+	default:
+		WARN_ON(1);
+		return -ENOENT;
+	}
+
+}
+
+#endif /* __KVM_X86_VMX_VMCS12_H */
* Unmerged path arch/x86/kvm/vmx/vmx.c
