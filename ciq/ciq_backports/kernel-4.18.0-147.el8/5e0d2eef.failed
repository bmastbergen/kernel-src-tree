net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 5e0d2eef771ee78b092bf93d040eac02a0965fea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/5e0d2eef.failed

Add support for the HW feature of multi-packet WQE in XDP
xmit flow.

The conventional TX descriptor (WQE, Work Queue Element) serves
a single packet. Our HW has support for multi-packet WQE (MPWQE)
in which a single descriptor serves multiple TX packets.

This reduces both the PCI overhead and the CPU cycles wasted on
writing them.

In this patch we add support for the HW feature, which is supported
starting from ConnectX-5.

Performance:
Tested packet rate for UDP 64Byte multi-stream over ConnectX-5 NICs.
CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz

XDP_TX:
We see a huge gain on single port ConnectX-5, and reach the 100 Mpps
milestone.
* Single-port HCA:
	Before:   70 Mpps
	After:   100 Mpps (+42.8%)

* Dual-port HCA:
	Before: 51.7 Mpps
	After:  57.3 Mpps (+10.8%)

* In both cases we tested traffic on one port and for now On Dual-port HCAs
  we see only small gain, we are working to overcome this bottleneck, but
  for the moment only with experimental firmware on dual port HCAs we can
  reach the wanted numbers as seen on Single-port HCAs.

XDP_REDIRECT:
Redirect from (A) ConnectX-5 to (B) ConnectX-5.
Due to a setup limitation, (A) and (B) are on different NUMA nodes,
so absolute performance numbers are not optimal.
Note:
  Below is the transmit rate of (B), not the redirect rate of (A)
  which is in some cases higher.

* (B) is single-port:
	Before:   77 Mpps
	After:    90 Mpps (+16.8%)

* (B) is dual-port:
	Before:  61 Mpps
	After:   72 Mpps (+18%)

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 5e0d2eef771ee78b092bf93d040eac02a0965fea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 007808f3e87f,8f5545d317ba..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -408,6 -404,28 +408,31 @@@ struct mlx5e_xdp_info 
  	struct mlx5e_dma_info di;
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5e_xdp_info_fifo {
+ 	struct mlx5e_xdp_info *xi;
+ 	u32 *cc;
+ 	u32 *pc;
+ 	u32 mask;
+ };
+ 
+ struct mlx5e_xdp_wqe_info {
+ 	u8 num_wqebbs;
+ 	u8 num_ds;
+ };
+ 
+ struct mlx5e_xdp_mpwqe {
+ 	/* Current MPWQE session */
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u8                   ds_count;
+ 	u8                   max_ds_count;
+ };
+ 
+ struct mlx5e_xdpsq;
+ typedef bool (*mlx5e_fp_xmit_xdp_frame)(struct mlx5e_xdpsq*,
+ 					struct mlx5e_xdp_info*);
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
  struct mlx5e_xdpsq {
  	/* data path */
  
@@@ -416,16 -435,20 +441,24 @@@
  	bool                       redirect_flush;
  
  	/* dirtied @xmit */
++<<<<<<< HEAD
 +	u16                        pc ____cacheline_aligned_in_smp;
 +	bool                       doorbell;
++=======
+ 	u32                        xdpi_fifo_pc ____cacheline_aligned_in_smp;
+ 	u16                        pc;
+ 	struct mlx5_wqe_ctrl_seg   *doorbell_cseg;
+ 	struct mlx5e_xdp_mpwqe     mpwqe;
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
  
  	struct mlx5e_cq            cq;
  
  	/* read only */
  	struct mlx5_wq_cyc         wq;
  	struct mlx5e_xdpsq_stats  *stats;
+ 	mlx5e_fp_xmit_xdp_frame    xmit_xdp_frame;
  	struct {
 -		struct mlx5e_xdp_wqe_info *wqe_info;
 -		struct mlx5e_xdp_info_fifo xdpi_fifo;
 +		struct mlx5e_xdp_info     *xdpi;
  	} db;
  	void __iomem              *uar_map;
  	u32                        sqn;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
index 60fd47414ccc,3740177eed09..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
@@@ -243,3 -356,80 +334,83 @@@ void mlx5e_free_xdpsq_descs(struct mlx5
  	}
  }
  
++<<<<<<< HEAD
++=======
+ int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
+ 		   u32 flags)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	struct mlx5e_xdpsq *sq;
+ 	int drops = 0;
+ 	int sq_num;
+ 	int i;
+ 
+ 	if (unlikely(!test_bit(MLX5E_STATE_OPENED, &priv->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	sq_num = smp_processor_id();
+ 
+ 	if (unlikely(sq_num >= priv->channels.num))
+ 		return -ENXIO;
+ 
+ 	sq = &priv->channels.c[sq_num]->xdpsq;
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return -ENETDOWN;
+ 
+ 	for (i = 0; i < n; i++) {
+ 		struct xdp_frame *xdpf = frames[i];
+ 		struct mlx5e_xdp_info xdpi;
+ 
+ 		xdpi.dma_addr = dma_map_single(sq->pdev, xdpf->data, xdpf->len,
+ 					       DMA_TO_DEVICE);
+ 		if (unlikely(dma_mapping_error(sq->pdev, xdpi.dma_addr))) {
+ 			xdp_return_frame_rx_napi(xdpf);
+ 			drops++;
+ 			continue;
+ 		}
+ 
+ 		xdpi.xdpf = xdpf;
+ 
+ 		if (unlikely(!sq->xmit_xdp_frame(sq, &xdpi))) {
+ 			dma_unmap_single(sq->pdev, xdpi.dma_addr,
+ 					 xdpf->len, DMA_TO_DEVICE);
+ 			xdp_return_frame_rx_napi(xdpf);
+ 			drops++;
+ 		}
+ 	}
+ 
+ 	if (flags & XDP_XMIT_FLUSH) {
+ 		if (sq->mpwqe.wqe)
+ 			mlx5e_xdp_mpwqe_complete(sq);
+ 		mlx5e_xmit_xdp_doorbell(sq);
+ 	}
+ 
+ 	return n - drops;
+ }
+ 
+ void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5e_xdpsq *xdpsq = &rq->xdpsq;
+ 
+ 	if (xdpsq->mpwqe.wqe)
+ 		mlx5e_xdp_mpwqe_complete(xdpsq);
+ 
+ 	mlx5e_xmit_xdp_doorbell(xdpsq);
+ 
+ 	if (xdpsq->redirect_flush) {
+ 		xdp_do_flush_map();
+ 		xdpsq->redirect_flush = false;
+ 	}
+ }
+ 
+ void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw)
+ {
+ 	sq->xmit_xdp_frame = is_mpw ?
+ 		mlx5e_xmit_xdp_frame_mpwqe : mlx5e_xmit_xdp_frame;
+ }
+ 
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 81739aad0188,3a67cb3cd179..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@@ -42,20 -43,56 +43,59 @@@
  
  bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
  		      void *va, u16 *rx_headroom, u32 *len);
++<<<<<<< HEAD
 +bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 +void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 +
 +bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
++=======
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq, struct mlx5e_rq *rq);
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq, struct mlx5e_rq *rq);
+ void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw);
+ void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
+ int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
+ 		   u32 flags);
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
  
  static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
  {
 -	if (sq->doorbell_cseg) {
 -		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, sq->doorbell_cseg);
 -		sq->doorbell_cseg = NULL;
 -	}
 -}
 +	struct mlx5_wq_cyc *wq = &sq->wq;
 +	struct mlx5e_tx_wqe *wqe;
 +	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc - 1); /* last pi */
  
++<<<<<<< HEAD
 +	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
++=======
+ static inline void
+ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, dma_addr_t dma_addr, u16 dma_len)
+ {
+ 	struct mlx5e_xdp_mpwqe *session = &sq->mpwqe;
+ 	struct mlx5_wqe_data_seg *dseg =
+ 		(struct mlx5_wqe_data_seg *)session->wqe + session->ds_count++;
+ 
+ 	dseg->addr       = cpu_to_be64(dma_addr);
+ 	dseg->byte_count = cpu_to_be32(dma_len);
+ 	dseg->lkey       = sq->mkey_be;
+ }
+ 
+ static inline void mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq,
+ 					 struct mlx5e_tx_wqe **wqe)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+ 
+ 	*wqe = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	memset(*wqe, 0, sizeof(**wqe));
+ }
+ 
+ static inline void
+ mlx5e_xdpi_fifo_push(struct mlx5e_xdp_info_fifo *fifo,
+ 		     struct mlx5e_xdp_info *xi)
+ {
+ 	u32 i = (*fifo->pc)++ & fifo->mask;
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
  
 -	fifo->xi[i] = *xi;
 -}
 -
 -static inline struct mlx5e_xdp_info
 -mlx5e_xdpi_fifo_pop(struct mlx5e_xdp_info_fifo *fifo)
 -{
 -	return fifo->xi[(*fifo->cc)++ & fifo->mask];
 +	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
  }
  
  #endif
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index b4f6c14ea722,07b16e5f02bd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -1551,15 -1584,13 +1552,12 @@@ static void mlx5e_close_icosq(struct ml
  static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
  			    struct mlx5e_params *params,
  			    struct mlx5e_sq_param *param,
 -			    struct mlx5e_xdpsq *sq,
 -			    bool is_redirect)
 +			    struct mlx5e_xdpsq *sq)
  {
- 	unsigned int ds_cnt = MLX5E_XDP_TX_DS_COUNT;
  	struct mlx5e_create_sq_param csp = {};
- 	unsigned int inline_hdr_sz = 0;
  	int err;
- 	int i;
  
 -	err = mlx5e_alloc_xdpsq(c, params, param, sq, is_redirect);
 +	err = mlx5e_alloc_xdpsq(c, params, param, sq);
  	if (err)
  		return err;
  
@@@ -1573,23 -1604,35 +1571,49 @@@
  	if (err)
  		goto err_free_xdpsq;
  
- 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
- 		inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
- 		ds_cnt++;
- 	}
+ 	mlx5e_set_xmit_fp(sq, param->is_mpw);
  
++<<<<<<< HEAD
 +	/* Pre initialize fixed WQE fields */
 +	for (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {
 +		struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);
 +		struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 +		struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 +		struct mlx5_wqe_data_seg *dseg;
++=======
+ 	if (!param->is_mpw) {
+ 		unsigned int ds_cnt = MLX5E_XDP_TX_DS_COUNT;
+ 		unsigned int inline_hdr_sz = 0;
+ 		int i;
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
+ 
+ 		if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 			inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
+ 			ds_cnt++;
+ 		}
  
- 		cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
- 		eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
- 
++<<<<<<< HEAD
 +		dseg = (struct mlx5_wqe_data_seg *)cseg + (ds_cnt - 1);
 +		dseg->lkey = sq->mkey_be;
++=======
+ 		/* Pre initialize fixed WQE fields */
+ 		for (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {
+ 			struct mlx5e_xdp_wqe_info *wi  = &sq->db.wqe_info[i];
+ 			struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);
+ 			struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 			struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 			struct mlx5_wqe_data_seg *dseg;
+ 
+ 			cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
+ 			eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
+ 
+ 			dseg = (struct mlx5_wqe_data_seg *)cseg + (ds_cnt - 1);
+ 			dseg->lkey = sq->mkey_be;
+ 
+ 			wi->num_wqebbs = 1;
+ 			wi->num_ds     = 1;
+ 		}
++>>>>>>> 5e0d2eef771e (net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE)
  	}
  
  	return 0;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --git a/include/linux/mlx5/device.h b/include/linux/mlx5/device.h
index 142dacf6f5ff..823f35e03016 100644
--- a/include/linux/mlx5/device.h
+++ b/include/linux/mlx5/device.h
@@ -405,6 +405,7 @@ enum {
 	MLX5_OPCODE_ATOMIC_MASKED_FA	= 0x15,
 	MLX5_OPCODE_BIND_MW		= 0x18,
 	MLX5_OPCODE_CONFIG_CMD		= 0x1f,
+	MLX5_OPCODE_ENHANCED_MPSW	= 0x29,
 
 	MLX5_RECV_OPCODE_RDMA_WRITE_IMM	= 0x00,
 	MLX5_RECV_OPCODE_SEND		= 0x01,
