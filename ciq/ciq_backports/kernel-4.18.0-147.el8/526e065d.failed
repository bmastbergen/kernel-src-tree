arm64: add sysfs vulnerability show for speculative store bypass

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Jeremy Linton <jeremy.linton@arm.com>
commit 526e065dbca6df0b5a130b84b836b8b3c9f54e21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/526e065d.failed

Return status based on ssbd_state and __ssb_safe. If the
mitigation is disabled, or the firmware isn't responding then
return the expected machine state based on a whitelist of known
good cores.

Given a heterogeneous machine, the overall machine vulnerability
defaults to safe but is reset to unsafe when we miss the whitelist
and the firmware doesn't explicitly tell us the core is safe.
In order to make that work we delay transitioning to vulnerable
until we know the firmware isn't responding to avoid a case
where we miss the whitelist, but the firmware goes ahead and
reports the core is not vulnerable. If all the cores in the
machine have SSBS, then __ssb_safe will remain true.

	Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
	Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 526e065dbca6df0b5a130b84b836b8b3c9f54e21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/cpu_errata.c
diff --cc arch/arm64/kernel/cpu_errata.c
index fde6f87a8b7e,4bb0f7cad418..000000000000
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@@ -342,8 -386,19 +344,22 @@@ static bool has_ssbd_mitigation(const s
  
  	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
  
++<<<<<<< HEAD
++=======
+ 	if (this_cpu_has_cap(ARM64_SSBS)) {
+ 		required = false;
+ 		goto out_printmsg;
+ 	}
+ 
+ 	/* delay setting __ssb_safe until we get a firmware response */
+ 	if (is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list))
+ 		this_cpu_safe = true;
+ 
++>>>>>>> 526e065dbca6 (arm64: add sysfs vulnerability show for speculative store bypass)
  	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
  		ssbd_state = ARM64_SSBD_UNKNOWN;
+ 		if (!this_cpu_safe)
+ 			__ssb_safe = false;
  		return false;
  	}
  
@@@ -413,9 -474,33 +437,26 @@@
  		break;
  	}
  
 -out_printmsg:
 -	switch (ssbd_state) {
 -	case ARM64_SSBD_FORCE_DISABLE:
 -		pr_info_once("%s disabled from command-line\n", entry->desc);
 -		break;
 -
 -	case ARM64_SSBD_FORCE_ENABLE:
 -		pr_info_once("%s forced from command-line\n", entry->desc);
 -		break;
 -	}
 -
  	return required;
  }
++<<<<<<< HEAD
 +#endif	/* CONFIG_ARM64_SSBD */
++=======
+ 
+ /* known invulnerable cores */
+ static const struct midr_range arm64_ssb_cpus[] = {
+ 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
+ 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
+ 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
+ 	{},
+ };
+ 
+ static void __maybe_unused
+ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
+ {
+ 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCI, 0);
+ }
++>>>>>>> 526e065dbca6 (arm64: add sysfs vulnerability show for speculative store bypass)
  
  #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
  	.matches = is_affected_midr_range,			\
@@@ -681,7 -791,23 +722,8 @@@ const struct arm64_cpu_capabilities arm
  		.capability = ARM64_SSBD,
  		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
  		.matches = has_ssbd_mitigation,
+ 		.midr_range_list = arm64_ssb_cpus,
  	},
 -#ifdef CONFIG_ARM64_ERRATUM_1188873
 -	{
 -		/* Cortex-A76 r0p0 to r2p0 */
 -		.desc = "ARM erratum 1188873",
 -		.capability = ARM64_WORKAROUND_1188873,
 -		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
 -	},
 -#endif
 -#ifdef CONFIG_ARM64_ERRATUM_1165522
 -	{
 -		/* Cortex-A76 r0p0 to r2p0 */
 -		.desc = "ARM erratum 1165522",
 -		.capability = ARM64_WORKAROUND_1165522,
 -		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
 -	},
  #endif
  	{
  	}
@@@ -692,3 -818,32 +734,35 @@@ ssize_t cpu_show_spectre_v1(struct devi
  {
  	return sprintf(buf, "Mitigation: __user pointer sanitization\n");
  }
++<<<<<<< HEAD
++=======
+ 
+ ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr,
+ 		char *buf)
+ {
+ 	if (__spectrev2_safe)
+ 		return sprintf(buf, "Not affected\n");
+ 
+ 	if (__hardenbp_enab)
+ 		return sprintf(buf, "Mitigation: Branch predictor hardening\n");
+ 
+ 	return sprintf(buf, "Vulnerable\n");
+ }
+ 
+ ssize_t cpu_show_spec_store_bypass(struct device *dev,
+ 		struct device_attribute *attr, char *buf)
+ {
+ 	if (__ssb_safe)
+ 		return sprintf(buf, "Not affected\n");
+ 
+ 	switch (ssbd_state) {
+ 	case ARM64_SSBD_KERNEL:
+ 	case ARM64_SSBD_FORCE_ENABLE:
+ 		if (IS_ENABLED(CONFIG_ARM64_SSBD))
+ 			return sprintf(buf,
+ 			    "Mitigation: Speculative Store Bypass disabled via prctl\n");
+ 	}
+ 
+ 	return sprintf(buf, "Vulnerable\n");
+ }
++>>>>>>> 526e065dbca6 (arm64: add sysfs vulnerability show for speculative store bypass)
* Unmerged path arch/arm64/kernel/cpu_errata.c
