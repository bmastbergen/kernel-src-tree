scsi: qla2xxx: avoid printf format warning

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Arnd Bergmann <arnd@arndb.de>
commit 038d710fca5bb149d3af2e0b71f1284f8430a979
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/038d710f.failed

Depending on the target architecture and configuration, both phys_addr_t
and dma_addr_t may be smaller than 'long long', so we get a warning when
printing either of them using the %llx format string:

drivers/scsi/qla2xxx/qla_iocb.c: In function 'qla24xx_walk_and_build_prot_sglist':
drivers/scsi/qla2xxx/qla_iocb.c:1140:46: error: format '%llx' expects argument of type 'long long unsigned int', but argument 6 has type 'dma_addr_t' {aka 'unsigned int'} [-Werror=format=]
         "%s: page boundary crossing (phys=%llx len=%x)\n",
                                           ~~~^
                                           %x
         __func__, sle_phys, sg->length);
                   ~~~~~~~~
drivers/scsi/qla2xxx/qla_iocb.c:1180:29: error: format '%llx' expects argument of type 'long long unsigned int', but argument 7 has type 'dma_addr_t' {aka 'unsigned int'} [-Werror=format=]
        "%s: sg[%x] (phys=%llx sglen=%x) ldma_sg_len: %x dif_bundl_len: %x ldma_needed: %x\n",
                          ~~~^

There are special %pad and %pap format strings in Linux that we could use
here, but since the driver already does 64-bit arithmetic on the values,
using a plain 'u64' seems more consistent here.

Note: A possible related issue may be that the driver possibly checks the
wrong kind of overflow: when an IOMMU is in use, buffers that cross a
32-bit boundary in physical addresses would still be mapped into dma
addresses within the low 4GB space, so I suspect that we actually want to
check sg_dma_address() instead of sg_phys() here.

Fixes: 50b812755e97 ("scsi: qla2xxx: Fix DMA error when the DIF sg buffer crosses 4GB boundary")
	Signed-off-by: Arnd Bergmann <arnd@arndb.de>
	Acked-by: Himanshu Madhani <hmadhani@marvell.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 038d710fca5bb149d3af2e0b71f1284f8430a979)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_iocb.c
diff --cc drivers/scsi/qla2xxx/qla_iocb.c
index dd132658631c,456a41d2e2c6..000000000000
--- a/drivers/scsi/qla2xxx/qla_iocb.c
+++ b/drivers/scsi/qla2xxx/qla_iocb.c
@@@ -1119,63 -1126,268 +1119,154 @@@ qla24xx_walk_and_build_prot_sglist(stru
  		return 1;
  	}
  
 -	ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe021,
 -	    "%s: enter (write=%u)\n", __func__, direction_to_device);
 +	ql_dbg(ql_dbg_tgt, vha, 0xe021,
 +		"%s: enter\n", __func__);
  
++<<<<<<< HEAD
 +	for_each_sg(sgl, sg, tot_dsds, i) {
 +		dma_addr_t	sle_dma;
++=======
+ 	/* if initiator doing write or target doing read */
+ 	if (direction_to_device) {
+ 		for_each_sg(sgl, sg, tot_dsds, i) {
+ 			u64 sle_phys = sg_phys(sg);
++>>>>>>> 038d710fca5b (scsi: qla2xxx: avoid printf format warning)
  
 -			/* If SGE addr + len flips bits in upper 32-bits */
 -			if (MSD(sle_phys + sg->length) ^ MSD(sle_phys)) {
 -				ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe022,
 -				    "%s: page boundary crossing (phys=%llx len=%x)\n",
 -				    __func__, sle_phys, sg->length);
 -
 -				if (difctx) {
 -					ha->dif_bundle_crossed_pages++;
 -					dif_local_dma_alloc = true;
 -				} else {
 -					ql_dbg(ql_dbg_tgt + ql_dbg_verbose,
 -					    vha, 0xe022,
 -					    "%s: difctx pointer is NULL\n",
 -					    __func__);
 -				}
 -				break;
 +		/* Allocate additional continuation packets? */
 +		if (avail_dsds == 0) {
 +			avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
 +						QLA_DSDS_PER_IOCB : used_dsds;
 +			dsd_list_len = (avail_dsds + 1) * 12;
 +			used_dsds -= avail_dsds;
 +
 +			/* allocate tracking DS */
 +			dsd_ptr = kzalloc(sizeof(struct dsd_dma), GFP_ATOMIC);
 +			if (!dsd_ptr)
 +				return 1;
 +
 +			/* allocate new list */
 +			dsd_ptr->dsd_addr = next_dsd =
 +			    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
 +				&dsd_ptr->dsd_list_dma);
 +
 +			if (!next_dsd) {
 +				/*
 +				 * Need to cleanup only this dsd_ptr, rest
 +				 * will be done by sp_free_dma()
 +				 */
 +				kfree(dsd_ptr);
 +				return 1;
  			}
 -		}
 -		ha->dif_bundle_writes++;
 -	} else {
 -		ha->dif_bundle_reads++;
 -	}
  
 -	if (ql2xdifbundlinginternalbuffers)
 -		dif_local_dma_alloc = direction_to_device;
 +			if (sp) {
 +				list_add_tail(&dsd_ptr->list,
 +				    &((struct crc_context *)
 +					    sp->u.scmd.ctx)->dsd_list);
  
++<<<<<<< HEAD
 +				sp->flags |= SRB_CRC_CTX_DSD_VALID;
 +			} else {
 +				list_add_tail(&dsd_ptr->list,
 +				    &(tc->ctx->dsd_list));
 +				*tc->ctx_dsd_alloced = 1;
++=======
+ 	if (dif_local_dma_alloc) {
+ 		u32 track_difbundl_buf = 0;
+ 		u32 ldma_sg_len = 0;
+ 		u8 ldma_needed = 1;
+ 
+ 		difctx->no_dif_bundl = 0;
+ 		difctx->dif_bundl_len = 0;
+ 
+ 		/* Track DSD buffers */
+ 		INIT_LIST_HEAD(&difctx->ldif_dsd_list);
+ 		/* Track local DMA buffers */
+ 		INIT_LIST_HEAD(&difctx->ldif_dma_hndl_list);
+ 
+ 		for_each_sg(sgl, sg, tot_dsds, i) {
+ 			u32 sglen = sg_dma_len(sg);
+ 
+ 			ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe023,
+ 			    "%s: sg[%x] (phys=%llx sglen=%x) ldma_sg_len: %x dif_bundl_len: %x ldma_needed: %x\n",
+ 			    __func__, i, (u64)sg_phys(sg), sglen, ldma_sg_len,
+ 			    difctx->dif_bundl_len, ldma_needed);
+ 
+ 			while (sglen) {
+ 				u32 xfrlen = 0;
+ 
+ 				if (ldma_needed) {
+ 					/*
+ 					 * Allocate list item to store
+ 					 * the DMA buffers
+ 					 */
+ 					dsd_ptr = kzalloc(sizeof(*dsd_ptr),
+ 					    GFP_ATOMIC);
+ 					if (!dsd_ptr) {
+ 						ql_dbg(ql_dbg_tgt, vha, 0xe024,
+ 						    "%s: failed alloc dsd_ptr\n",
+ 						    __func__);
+ 						return 1;
+ 					}
+ 					ha->dif_bundle_kallocs++;
+ 
+ 					/* allocate dma buffer */
+ 					dsd_ptr->dsd_addr = dma_pool_alloc
+ 						(ha->dif_bundl_pool, GFP_ATOMIC,
+ 						 &dsd_ptr->dsd_list_dma);
+ 					if (!dsd_ptr->dsd_addr) {
+ 						ql_dbg(ql_dbg_tgt, vha, 0xe024,
+ 						    "%s: failed alloc ->dsd_ptr\n",
+ 						    __func__);
+ 						/*
+ 						 * need to cleanup only this
+ 						 * dsd_ptr rest will be done
+ 						 * by sp_free_dma()
+ 						 */
+ 						kfree(dsd_ptr);
+ 						ha->dif_bundle_kallocs--;
+ 						return 1;
+ 					}
+ 					ha->dif_bundle_dma_allocs++;
+ 					ldma_needed = 0;
+ 					difctx->no_dif_bundl++;
+ 					list_add_tail(&dsd_ptr->list,
+ 					    &difctx->ldif_dma_hndl_list);
+ 				}
+ 
+ 				/* xfrlen is min of dma pool size and sglen */
+ 				xfrlen = (sglen >
+ 				   (DIF_BUNDLING_DMA_POOL_SIZE - ldma_sg_len)) ?
+ 				    DIF_BUNDLING_DMA_POOL_SIZE - ldma_sg_len :
+ 				    sglen;
+ 
+ 				/* replace with local allocated dma buffer */
+ 				sg_pcopy_to_buffer(sgl, sg_nents(sgl),
+ 				    dsd_ptr->dsd_addr + ldma_sg_len, xfrlen,
+ 				    difctx->dif_bundl_len);
+ 				difctx->dif_bundl_len += xfrlen;
+ 				sglen -= xfrlen;
+ 				ldma_sg_len += xfrlen;
+ 				if (ldma_sg_len == DIF_BUNDLING_DMA_POOL_SIZE ||
+ 				    sg_is_last(sg)) {
+ 					ldma_needed = 1;
+ 					ldma_sg_len = 0;
+ 				}
++>>>>>>> 038d710fca5b (scsi: qla2xxx: avoid printf format warning)
  			}
 -		}
 -
 -		track_difbundl_buf = used_dsds = difctx->no_dif_bundl;
 -		ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe025,
 -		    "dif_bundl_len=%x, no_dif_bundl=%x track_difbundl_buf: %x\n",
 -		    difctx->dif_bundl_len, difctx->no_dif_bundl,
 -		    track_difbundl_buf);
 -
 -		if (sp)
 -			sp->flags |= SRB_DIF_BUNDL_DMA_VALID;
 -		else
 -			tc->prot_flags = DIF_BUNDL_DMA_VALID;
 -
 -		list_for_each_entry_safe(dif_dsd, nxt_dsd,
 -		    &difctx->ldif_dma_hndl_list, list) {
 -			u32 sglen = (difctx->dif_bundl_len >
 -			    DIF_BUNDLING_DMA_POOL_SIZE) ?
 -			    DIF_BUNDLING_DMA_POOL_SIZE : difctx->dif_bundl_len;
 -
 -			BUG_ON(track_difbundl_buf == 0);
 -
 -			/* Allocate additional continuation packets? */
 -			if (avail_dsds == 0) {
 -				ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha,
 -				    0xe024,
 -				    "%s: adding continuation iocb's\n",
 -				    __func__);
 -				avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
 -				    QLA_DSDS_PER_IOCB : used_dsds;
 -				dsd_list_len = (avail_dsds + 1) * 12;
 -				used_dsds -= avail_dsds;
 -
 -				/* allocate tracking DS */
 -				dsd_ptr = kzalloc(sizeof(*dsd_ptr), GFP_ATOMIC);
 -				if (!dsd_ptr) {
 -					ql_dbg(ql_dbg_tgt, vha, 0xe026,
 -					    "%s: failed alloc dsd_ptr\n",
 -					    __func__);
 -					return 1;
 -				}
 -				ha->dif_bundle_kallocs++;
 -
 -				difctx->no_ldif_dsd++;
 -				/* allocate new list */
 -				dsd_ptr->dsd_addr =
 -				    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
 -					&dsd_ptr->dsd_list_dma);
 -				if (!dsd_ptr->dsd_addr) {
 -					ql_dbg(ql_dbg_tgt, vha, 0xe026,
 -					    "%s: failed alloc ->dsd_addr\n",
 -					    __func__);
 -					/*
 -					 * need to cleanup only this dsd_ptr
 -					 *  rest will be done by sp_free_dma()
 -					 */
 -					kfree(dsd_ptr);
 -					ha->dif_bundle_kallocs--;
 -					return 1;
 -				}
 -				ha->dif_bundle_dma_allocs++;
  
 -				if (sp) {
 -					list_add_tail(&dsd_ptr->list,
 -					    &difctx->ldif_dsd_list);
 -					sp->flags |= SRB_CRC_CTX_DSD_VALID;
 -				} else {
 -					list_add_tail(&dsd_ptr->list,
 -					    &difctx->ldif_dsd_list);
 -					tc->ctx_dsd_alloced = 1;
 -				}
 -
 -				/* add new list to cmd iocb or last list */
 -				*cur_dsd++ =
 -				    cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
 -				*cur_dsd++ =
 -				    cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
 -				*cur_dsd++ = dsd_list_len;
 -				cur_dsd = dsd_ptr->dsd_addr;
 -			}
 -			*cur_dsd++ = cpu_to_le32(LSD(dif_dsd->dsd_list_dma));
 -			*cur_dsd++ = cpu_to_le32(MSD(dif_dsd->dsd_list_dma));
 -			*cur_dsd++ = cpu_to_le32(sglen);
 -			avail_dsds--;
 -			difctx->dif_bundl_len -= sglen;
 -			track_difbundl_buf--;
 +			/* add new list to cmd iocb or last list */
 +			*cur_dsd++ = cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
 +			*cur_dsd++ = cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
 +			*cur_dsd++ = dsd_list_len;
 +			cur_dsd = (uint32_t *)next_dsd;
  		}
 +		sle_dma = sg_dma_address(sg);
  
 -		ql_dbg(ql_dbg_tgt + ql_dbg_verbose, vha, 0xe026,
 -		    "%s: no_ldif_dsd:%x, no_dif_bundl:%x\n", __func__,
 -			difctx->no_ldif_dsd, difctx->no_dif_bundl);
 -	} else {
 -		for_each_sg(sgl, sg, tot_dsds, i) {
 -			dma_addr_t sle_dma;
 -
 -			/* Allocate additional continuation packets? */
 -			if (avail_dsds == 0) {
 -				avail_dsds = (used_dsds > QLA_DSDS_PER_IOCB) ?
 -				    QLA_DSDS_PER_IOCB : used_dsds;
 -				dsd_list_len = (avail_dsds + 1) * 12;
 -				used_dsds -= avail_dsds;
 -
 -				/* allocate tracking DS */
 -				dsd_ptr = kzalloc(sizeof(*dsd_ptr), GFP_ATOMIC);
 -				if (!dsd_ptr) {
 -					ql_dbg(ql_dbg_tgt + ql_dbg_verbose,
 -					    vha, 0xe027,
 -					    "%s: failed alloc dsd_dma...\n",
 -					    __func__);
 -					return 1;
 -				}
 -
 -				/* allocate new list */
 -				dsd_ptr->dsd_addr =
 -				    dma_pool_alloc(ha->dl_dma_pool, GFP_ATOMIC,
 -					&dsd_ptr->dsd_list_dma);
 -				if (!dsd_ptr->dsd_addr) {
 -					/* need to cleanup only this dsd_ptr */
 -					/* rest will be done by sp_free_dma() */
 -					kfree(dsd_ptr);
 -					return 1;
 -				}
 -
 -				if (sp) {
 -					list_add_tail(&dsd_ptr->list,
 -					    &difctx->dsd_list);
 -					sp->flags |= SRB_CRC_CTX_DSD_VALID;
 -				} else {
 -					list_add_tail(&dsd_ptr->list,
 -					    &difctx->dsd_list);
 -					tc->ctx_dsd_alloced = 1;
 -				}
 +		*cur_dsd++ = cpu_to_le32(LSD(sle_dma));
 +		*cur_dsd++ = cpu_to_le32(MSD(sle_dma));
 +		*cur_dsd++ = cpu_to_le32(sg_dma_len(sg));
  
 -				/* add new list to cmd iocb or last list */
 -				*cur_dsd++ =
 -				    cpu_to_le32(LSD(dsd_ptr->dsd_list_dma));
 -				*cur_dsd++ =
 -				    cpu_to_le32(MSD(dsd_ptr->dsd_list_dma));
 -				*cur_dsd++ = dsd_list_len;
 -				cur_dsd = dsd_ptr->dsd_addr;
 -			}
 -			sle_dma = sg_dma_address(sg);
 -			*cur_dsd++ = cpu_to_le32(LSD(sle_dma));
 -			*cur_dsd++ = cpu_to_le32(MSD(sle_dma));
 -			*cur_dsd++ = cpu_to_le32(sg_dma_len(sg));
 -			avail_dsds--;
 -		}
 +		avail_dsds--;
  	}
  	/* Null termination */
  	*cur_dsd++ = 0;
* Unmerged path drivers/scsi/qla2xxx/qla_iocb.c
