x86/resctrl: Initialize a new resource group with default MBA values

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Xiaochen Shen <xiaochen.shen@intel.com>
commit 47820e73f5b3a1fdb8ebd1219191edc96e0c85c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/47820e73.failed

Currently, when a new resource group is created, the allocation values
of the MBA resource are not initialized and remain meaningless data.

For example:

  mkdir /sys/fs/resctrl/p1
  cat /sys/fs/resctrl/p1/schemata
  MB:0=100;1=100

  echo "MB:0=10;1=20" > /sys/fs/resctrl/p1/schemata
  cat /sys/fs/resctrl/p1/schemata
  MB:0= 10;1= 20

  rmdir /sys/fs/resctrl/p1
  mkdir /sys/fs/resctrl/p2
  cat /sys/fs/resctrl/p2/schemata
  MB:0= 10;1= 20

Therefore, when the new group is created, it is reasonable to initialize
MBA resource with default values.

Initialize the MBA resource and cache resources in separate functions.

 [ bp: Add newlines between code blocks for better readability. ]

	Signed-off-by: Xiaochen Shen <xiaochen.shen@intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Fenghua Yu <fenghua.yu@intel.com>
	Reviewed-by: Reinette Chatre <reinette.chatre@intel.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: pei.p.jia@intel.com
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/1555499329-1170-3-git-send-email-xiaochen.shen@intel.com
(cherry picked from commit 47820e73f5b3a1fdb8ebd1219191edc96e0c85c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
diff --cc arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
index 1f3fa4aa5ccc,333c177a2471..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
+++ b/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
@@@ -2469,100 -2516,127 +2469,201 @@@ static void cbm_ensure_valid(u32 *_val
  	bitmap_clear(val, zero_bit, cbm_len - zero_bit);
  }
  
++<<<<<<< HEAD:arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
 +/**
 + * rdtgroup_init_alloc - Initialize the new RDT group's allocations
++=======
+ /*
+  * Initialize cache resources per RDT domain
+  *
+  * Set the RDT domain up to start off with all usable allocations. That is,
+  * all shareable and unused bits. All-zero CBM is invalid.
+  */
+ static int __init_one_rdt_domain(struct rdt_domain *d, struct rdt_resource *r,
+ 				 u32 closid)
+ {
+ 	struct rdt_resource *r_cdp = NULL;
+ 	struct rdt_domain *d_cdp = NULL;
+ 	u32 used_b = 0, unused_b = 0;
+ 	unsigned long tmp_cbm;
+ 	enum rdtgrp_mode mode;
+ 	u32 peer_ctl, *ctrl;
+ 	int i;
+ 
+ 	rdt_cdp_peer_get(r, d, &r_cdp, &d_cdp);
+ 	d->have_new_ctrl = false;
+ 	d->new_ctrl = r->cache.shareable_bits;
+ 	used_b = r->cache.shareable_bits;
+ 	ctrl = d->ctrl_val;
+ 	for (i = 0; i < closids_supported(); i++, ctrl++) {
+ 		if (closid_allocated(i) && i != closid) {
+ 			mode = rdtgroup_mode_by_closid(i);
+ 			if (mode == RDT_MODE_PSEUDO_LOCKSETUP)
+ 				break;
+ 			/*
+ 			 * If CDP is active include peer domain's
+ 			 * usage to ensure there is no overlap
+ 			 * with an exclusive group.
+ 			 */
+ 			if (d_cdp)
+ 				peer_ctl = d_cdp->ctrl_val[i];
+ 			else
+ 				peer_ctl = 0;
+ 			used_b |= *ctrl | peer_ctl;
+ 			if (mode == RDT_MODE_SHAREABLE)
+ 				d->new_ctrl |= *ctrl | peer_ctl;
+ 		}
+ 	}
+ 	if (d->plr && d->plr->cbm > 0)
+ 		used_b |= d->plr->cbm;
+ 	unused_b = used_b ^ (BIT_MASK(r->cache.cbm_len) - 1);
+ 	unused_b &= BIT_MASK(r->cache.cbm_len) - 1;
+ 	d->new_ctrl |= unused_b;
+ 	/*
+ 	 * Force the initial CBM to be valid, user can
+ 	 * modify the CBM based on system availability.
+ 	 */
+ 	cbm_ensure_valid(&d->new_ctrl, r);
+ 	/*
+ 	 * Assign the u32 CBM to an unsigned long to ensure that
+ 	 * bitmap_weight() does not access out-of-bound memory.
+ 	 */
+ 	tmp_cbm = d->new_ctrl;
+ 	if (bitmap_weight(&tmp_cbm, r->cache.cbm_len) < r->cache.min_cbm_bits) {
+ 		rdt_last_cmd_printf("No space on %s:%d\n", r->name, d->id);
+ 		return -ENOSPC;
+ 	}
+ 	d->have_new_ctrl = true;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Initialize cache resources with default values.
++>>>>>>> 47820e73f5b3 (x86/resctrl: Initialize a new resource group with default MBA values):arch/x86/kernel/cpu/resctrl/rdtgroup.c
   *
   * A new RDT group is being created on an allocation capable (CAT)
   * supporting system. Set this group up to start off with all usable
 - * allocations.
 + * allocations. That is, all shareable and unused bits.
   *
 - * If there are no more shareable bits available on any domain then
 - * the entire allocation will fail.
 + * All-zero CBM is invalid. If there are no more shareable bits available
 + * on any domain then the entire allocation will fail.
   */
- static int rdtgroup_init_alloc(struct rdtgroup *rdtgrp)
+ static int rdtgroup_init_cat(struct rdt_resource *r, u32 closid)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
 +	struct rdt_resource *r_cdp = NULL;
 +	struct rdt_domain *d_cdp = NULL;
 +	u32 used_b = 0, unused_b = 0;
 +	u32 closid = rdtgrp->closid;
 +	struct rdt_resource *r;
 +	unsigned long tmp_cbm;
 +	enum rdtgrp_mode mode;
++=======
++>>>>>>> 47820e73f5b3 (x86/resctrl: Initialize a new resource group with default MBA values):arch/x86/kernel/cpu/resctrl/rdtgroup.c
  	struct rdt_domain *d;
 -	int ret;
 +	u32 peer_ctl, *ctrl;
 +	int i, ret;
  
+ 	list_for_each_entry(d, &r->domains, list) {
+ 		ret = __init_one_rdt_domain(d, r, closid);
+ 		if (ret < 0)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /* Initialize MBA resource with default values. */
+ static void rdtgroup_init_mba(struct rdt_resource *r)
+ {
+ 	struct rdt_domain *d;
+ 
+ 	list_for_each_entry(d, &r->domains, list) {
+ 		d->new_ctrl = is_mba_sc(r) ? MBA_MAX_MBPS : r->default_ctrl;
+ 		d->have_new_ctrl = true;
+ 	}
+ }
+ 
+ /* Initialize the RDT group's allocations. */
+ static int rdtgroup_init_alloc(struct rdtgroup *rdtgrp)
+ {
+ 	struct rdt_resource *r;
+ 	int ret;
+ 
  	for_each_alloc_enabled_rdt_resource(r) {
++<<<<<<< HEAD:arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
 +		/*
 +		 * Only initialize default allocations for CBM cache
 +		 * resources
 +		 */
 +		if (r->rid == RDT_RESOURCE_MBA)
 +			continue;
 +		list_for_each_entry(d, &r->domains, list) {
 +			rdt_cdp_peer_get(r, d, &r_cdp, &d_cdp);
 +			d->have_new_ctrl = false;
 +			d->new_ctrl = r->cache.shareable_bits;
 +			used_b = r->cache.shareable_bits;
 +			ctrl = d->ctrl_val;
 +			for (i = 0; i < r->num_closid; i++, ctrl++) {
 +				if (closid_allocated(i) && i != closid) {
 +					mode = rdtgroup_mode_by_closid(i);
 +					if (mode == RDT_MODE_PSEUDO_LOCKSETUP)
 +						break;
 +					/*
 +					 * If CDP is active include peer
 +					 * domain's usage to ensure there
 +					 * is no overlap with an exclusive
 +					 * group.
 +					 */
 +					if (d_cdp)
 +						peer_ctl = d_cdp->ctrl_val[i];
 +					else
 +						peer_ctl = 0;
 +					used_b |= *ctrl | peer_ctl;
 +					if (mode == RDT_MODE_SHAREABLE)
 +						d->new_ctrl |= *ctrl | peer_ctl;
 +				}
 +			}
 +			if (d->plr && d->plr->cbm > 0)
 +				used_b |= d->plr->cbm;
 +			unused_b = used_b ^ (BIT_MASK(r->cache.cbm_len) - 1);
 +			unused_b &= BIT_MASK(r->cache.cbm_len) - 1;
 +			d->new_ctrl |= unused_b;
 +			/*
 +			 * Force the initial CBM to be valid, user can
 +			 * modify the CBM based on system availability.
 +			 */
 +			cbm_ensure_valid(&d->new_ctrl, r);
 +			/*
 +			 * Assign the u32 CBM to an unsigned long to ensure
 +			 * that bitmap_weight() does not access out-of-bound
 +			 * memory.
 +			 */
 +			tmp_cbm = d->new_ctrl;
 +			if (bitmap_weight(&tmp_cbm, r->cache.cbm_len) <
 +			    r->cache.min_cbm_bits) {
 +				rdt_last_cmd_printf("no space on %s:%d\n",
 +						    r->name, d->id);
 +				return -ENOSPC;
 +			}
 +			d->have_new_ctrl = true;
++=======
+ 		if (r->rid == RDT_RESOURCE_MBA) {
+ 			rdtgroup_init_mba(r);
+ 		} else {
+ 			ret = rdtgroup_init_cat(r, rdtgrp->closid);
+ 			if (ret < 0)
+ 				return ret;
++>>>>>>> 47820e73f5b3 (x86/resctrl: Initialize a new resource group with default MBA values):arch/x86/kernel/cpu/resctrl/rdtgroup.c
  		}
- 	}
  
- 	for_each_alloc_enabled_rdt_resource(r) {
- 		/*
- 		 * Only initialize default allocations for CBM cache
- 		 * resources
- 		 */
- 		if (r->rid == RDT_RESOURCE_MBA)
- 			continue;
  		ret = update_domains(r, rdtgrp->closid);
  		if (ret < 0) {
 -			rdt_last_cmd_puts("Failed to initialize allocations\n");
 +			rdt_last_cmd_puts("failed to initialize allocations\n");
  			return ret;
  		}
+ 
  	}
  
  	rdtgrp->mode = RDT_MODE_SHAREABLE;
diff --git a/arch/x86/kernel/cpu/intel_rdt_ctrlmondata.c b/arch/x86/kernel/cpu/intel_rdt_ctrlmondata.c
index 5a7134a37be0..3bc2ae7cf797 100644
--- a/arch/x86/kernel/cpu/intel_rdt_ctrlmondata.c
+++ b/arch/x86/kernel/cpu/intel_rdt_ctrlmondata.c
@@ -341,10 +341,10 @@ int update_domains(struct rdt_resource *r, int closid)
 	if (cpumask_empty(cpu_mask) || mba_sc)
 		goto done;
 	cpu = get_cpu();
-	/* Update CBM on this cpu if it's in cpu_mask. */
+	/* Update resource control msr on this CPU if it's in cpu_mask. */
 	if (cpumask_test_cpu(cpu, cpu_mask))
 		rdt_ctrl_update(&msr_param);
-	/* Update CBM on other cpus. */
+	/* Update resource control msr on other CPUs. */
 	smp_call_function_many(cpu_mask, rdt_ctrl_update, &msr_param, 1);
 	put_cpu();
 
* Unmerged path arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
