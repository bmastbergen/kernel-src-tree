net/mlx5: Fix compilation warning in eq.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 092ead48290b43afe5d548797c73b179dbaf6523
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/092ead48.failed

mlx5_eq_table_get_rmap is being used only when CONFIG_RFS_ACCEL is
enabled, this patch fixes the below warning when CONFIG_RFS_ACCEL is
disabled.

drivers/.../mlx5/core/eq.c:903:18: [-Werror=missing-prototypes]
error: no previous prototype for ‘mlx5_eq_table_get_rmap’

Fixes: f2f3df550139 ("net/mlx5: EQ, Privatize eq_table and friends")
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 092ead48290b43afe5d548797c73b179dbaf6523)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index a997f6ba7cac,46a747f7c162..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -921,16 -621,303 +921,307 @@@ void mlx5_stop_eqs(struct mlx5_core_de
  	if (err)
  		mlx5_core_err(dev, "failed to destroy command eq, err(%d)\n",
  			      err);
 -
 -	mlx5_eq_notifier_unregister(dev, &table->cq_err_nb);
  }
  
 -struct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev)
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen)
  {
 -	return &dev->priv.eq_table->async_eq;
 -}
 +	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {0};
  
++<<<<<<< HEAD
 +	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
 +	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
++=======
+ void mlx5_eq_synchronize_async_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->async_eq.irqn);
+ }
+ 
+ void mlx5_eq_synchronize_cmd_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->cmd_eq.irqn);
+ }
+ 
+ /* Generic EQ API for mlx5_core consumers
+  * Needed For RDMA ODP EQ for now
+  */
+ struct mlx5_eq *
+ mlx5_eq_create_generic(struct mlx5_core_dev *dev, const char *name,
+ 		       struct mlx5_eq_param *param)
+ {
+ 	struct mlx5_eq *eq = kvzalloc(sizeof(*eq), GFP_KERNEL);
+ 	int err;
+ 
+ 	if (!eq)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	err = create_async_eq(dev, name, eq, param);
+ 	if (err) {
+ 		kvfree(eq);
+ 		eq = ERR_PTR(err);
+ 	}
+ 
+ 	return eq;
+ }
+ EXPORT_SYMBOL(mlx5_eq_create_generic);
+ 
+ int mlx5_eq_destroy_generic(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
+ {
+ 	int err;
+ 
+ 	if (IS_ERR(eq))
+ 		return -EINVAL;
+ 
+ 	err = destroy_async_eq(dev, eq);
+ 	if (err)
+ 		goto out;
+ 
+ 	kvfree(eq);
+ out:
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_eq_destroy_generic);
+ 
+ struct mlx5_eqe *mlx5_eq_get_eqe(struct mlx5_eq *eq, u32 cc)
+ {
+ 	u32 ci = eq->cons_index + cc;
+ 	struct mlx5_eqe *eqe;
+ 
+ 	eqe = get_eqe(eq, ci & (eq->nent - 1));
+ 	eqe = ((eqe->owner & 1) ^ !!(ci & eq->nent)) ? NULL : eqe;
+ 	/* Make sure we read EQ entry contents after we've
+ 	 * checked the ownership bit.
+ 	 */
+ 	if (eqe)
+ 		dma_rmb();
+ 
+ 	return eqe;
+ }
+ EXPORT_SYMBOL(mlx5_eq_get_eqe);
+ 
+ void mlx5_eq_update_ci(struct mlx5_eq *eq, u32 cc, bool arm)
+ {
+ 	__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);
+ 	u32 val;
+ 
+ 	eq->cons_index += cc;
+ 	val = (eq->cons_index & 0xffffff) | (eq->eqn << 24);
+ 
+ 	__raw_writel((__force u32)cpu_to_be32(val), addr);
+ 	/* We still want ordering, just not swabbing, so add a barrier */
+ 	mb();
+ }
+ EXPORT_SYMBOL(mlx5_eq_update_ci);
+ 
+ /* Completion EQs */
+ 
+ static int set_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	if (!zalloc_cpumask_var(&irq_info->mask, GFP_KERNEL)) {
+ 		mlx5_core_warn(mdev, "zalloc_cpumask_var failed");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	cpumask_set_cpu(cpumask_local_spread(i, priv->numa_node),
+ 			irq_info->mask);
+ 
+ 	if (IS_ENABLED(CONFIG_SMP) &&
+ 	    irq_set_affinity_hint(irq, irq_info->mask))
+ 		mlx5_core_warn(mdev, "irq_set_affinity_hint failed, irq 0x%.4x", irq);
+ 
+ 	return 0;
+ }
+ 
+ static void clear_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	irq_set_affinity_hint(irq, NULL);
+ 	free_cpumask_var(irq_info->mask);
+ }
+ 
+ static int set_comp_irq_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++) {
+ 		err = set_comp_irq_affinity_hint(mdev, i);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return 0;
+ 
+ err_out:
+ 	for (i--; i >= 0; i--)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ 
+ 	return err;
+ }
+ 
+ static void clear_comp_irqs_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ }
+ 
+ static void destroy_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		list_del(&eq->list);
+ 		if (destroy_unmap_eq(dev, &eq->core))
+ 			mlx5_core_warn(dev, "failed to destroy comp EQ 0x%x\n",
+ 				       eq->core.eqn);
+ 		tasklet_disable(&eq->tasklet_ctx.task);
+ 		kfree(eq);
+ 	}
+ }
+ 
+ static int create_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	struct mlx5_eq_comp *eq;
+ 	int ncomp_vec;
+ 	int nent;
+ 	int err;
+ 	int i;
+ 
+ 	INIT_LIST_HEAD(&table->comp_eqs_list);
+ 	ncomp_vec = table->num_comp_vectors;
+ 	nent = MLX5_COMP_EQ_SIZE;
+ #ifdef CONFIG_RFS_ACCEL
+ 	table->rmap = alloc_irq_cpu_rmap(ncomp_vec);
+ 	if (!table->rmap)
+ 		return -ENOMEM;
+ #endif
+ 	for (i = 0; i < ncomp_vec; i++) {
+ 		int vecidx = i + MLX5_EQ_VEC_COMP_BASE;
+ 		struct mlx5_eq_param param = {};
+ 
+ 		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
+ 		if (!eq) {
+ 			err = -ENOMEM;
+ 			goto clean;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.list);
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.process_list);
+ 		spin_lock_init(&eq->tasklet_ctx.lock);
+ 		tasklet_init(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb,
+ 			     (unsigned long)&eq->tasklet_ctx);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 		irq_cpu_rmap_add(table->rmap, pci_irq_vector(dev->pdev, vecidx));
+ #endif
+ 		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
+ 		param = (struct mlx5_eq_param) {
+ 			.index = vecidx,
+ 			.mask = 0,
+ 			.nent = nent,
+ 			.context = &eq->core,
+ 			.handler = mlx5_eq_comp_int
+ 		};
+ 		err = create_map_eq(dev, &eq->core, name, &param);
+ 		if (err) {
+ 			kfree(eq);
+ 			goto clean;
+ 		}
+ 		mlx5_core_dbg(dev, "allocated completion EQN %d\n", eq->core.eqn);
+ 		/* add tail, to keep the list ordered, for mlx5_vector2eqn to work */
+ 		list_add_tail(&eq->list, &table->comp_eqs_list);
+ 	}
+ 
+ 	err = set_comp_irq_affinity_hints(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to alloc affinity hint cpumask\n");
+ 		goto clean;
+ 	}
+ 
+ 	return 0;
+ 
+ clean:
+ 	destroy_comp_eqs(dev);
+ 	return err;
+ }
+ 
+ int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
+ 		    unsigned int *irqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 	int err = -ENOENT;
+ 	int i = 0;
+ 
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		if (i++ == vector) {
+ 			*eqn = eq->core.eqn;
+ 			*irqn = eq->core.irqn;
+ 			err = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_vector2eqn);
+ 
+ unsigned int mlx5_comp_vectors_count(struct mlx5_core_dev *dev)
+ {
+ 	return dev->priv.eq_table->num_comp_vectors;
+ }
+ EXPORT_SYMBOL(mlx5_comp_vectors_count);
+ 
+ struct cpumask *
+ mlx5_comp_irq_get_affinity_mask(struct mlx5_core_dev *dev, int vector)
+ {
+ 	/* TODO: consider irq_get_affinity_mask(irq) */
+ 	return dev->priv.eq_table->irq_info[vector + MLX5_EQ_VEC_COMP_BASE].mask;
+ }
+ EXPORT_SYMBOL(mlx5_comp_irq_get_affinity_mask);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ struct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev)
+ {
+ 	return dev->priv.eq_table->rmap;
+ }
+ #endif
+ 
+ struct mlx5_eq_comp *mlx5_eqn2comp_eq(struct mlx5_core_dev *dev, int eqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq;
+ 
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list) {
+ 		if (eq->core.eqn == eqn)
+ 			return eq;
+ 	}
+ 
+ 	return ERR_PTR(-ENOENT);
++>>>>>>> 092ead48290b (net/mlx5: Fix compilation warning in eq.c)
  }
  
  /* This function should only be called after mlx5_cmd_force_teardown_hca */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
