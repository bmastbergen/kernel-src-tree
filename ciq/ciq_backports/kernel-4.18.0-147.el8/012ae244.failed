powerpc/perf: Trace imc PMU functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Anju T Sudhakar <anju@linux.vnet.ibm.com>
commit 012ae244845f19d5f6ca2a90426851bc5044a0dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/012ae244.failed

Add PMU functions to support trace-imc.

	Signed-off-by: Anju T Sudhakar <anju@linux.vnet.ibm.com>
	Reviewed-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 012ae244845f19d5f6ca2a90426851bc5044a0dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/perf/imc-pmu.c
diff --cc arch/powerpc/perf/imc-pmu.c
index 100bdd0c0fbd,31fa753e2eb2..000000000000
--- a/arch/powerpc/perf/imc-pmu.c
+++ b/arch/powerpc/perf/imc-pmu.c
@@@ -1068,6 -1079,235 +1087,238 @@@ static void thread_imc_event_del(struc
  	imc_event_update(event);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Allocate a page of memory for each cpu, and load LDBAR with 0.
+  */
+ static int trace_imc_mem_alloc(int cpu_id, int size)
+ {
+ 	u64 *local_mem = per_cpu(trace_imc_mem, cpu_id);
+ 	int phys_id = cpu_to_node(cpu_id), rc = 0;
+ 	int core_id = (cpu_id / threads_per_core);
+ 
+ 	if (!local_mem) {
+ 		local_mem = page_address(alloc_pages_node(phys_id,
+ 					GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE |
+ 					__GFP_NOWARN, get_order(size)));
+ 		if (!local_mem)
+ 			return -ENOMEM;
+ 		per_cpu(trace_imc_mem, cpu_id) = local_mem;
+ 
+ 		/* Initialise the counters for trace mode */
+ 		rc = opal_imc_counters_init(OPAL_IMC_COUNTERS_TRACE, __pa((void *)local_mem),
+ 					    get_hard_smp_processor_id(cpu_id));
+ 		if (rc) {
+ 			pr_info("IMC:opal init failed for trace imc\n");
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	/* Init the mutex, if not already */
+ 	trace_imc_refc[core_id].id = core_id;
+ 	mutex_init(&trace_imc_refc[core_id].lock);
+ 
+ 	mtspr(SPRN_LDBAR, 0);
+ 	return 0;
+ }
+ 
+ static int ppc_trace_imc_cpu_online(unsigned int cpu)
+ {
+ 	return trace_imc_mem_alloc(cpu, trace_imc_mem_size);
+ }
+ 
+ static int ppc_trace_imc_cpu_offline(unsigned int cpu)
+ {
+ 	mtspr(SPRN_LDBAR, 0);
+ 	return 0;
+ }
+ 
+ static int trace_imc_cpu_init(void)
+ {
+ 	return cpuhp_setup_state(CPUHP_AP_PERF_POWERPC_TRACE_IMC_ONLINE,
+ 			  "perf/powerpc/imc_trace:online",
+ 			  ppc_trace_imc_cpu_online,
+ 			  ppc_trace_imc_cpu_offline);
+ }
+ 
+ static u64 get_trace_imc_event_base_addr(void)
+ {
+ 	return (u64)per_cpu(trace_imc_mem, smp_processor_id());
+ }
+ 
+ /*
+  * Function to parse trace-imc data obtained
+  * and to prepare the perf sample.
+  */
+ static int trace_imc_prepare_sample(struct trace_imc_data *mem,
+ 				    struct perf_sample_data *data,
+ 				    u64 *prev_tb,
+ 				    struct perf_event_header *header,
+ 				    struct perf_event *event)
+ {
+ 	/* Sanity checks for a valid record */
+ 	if (be64_to_cpu(READ_ONCE(mem->tb1)) > *prev_tb)
+ 		*prev_tb = be64_to_cpu(READ_ONCE(mem->tb1));
+ 	else
+ 		return -EINVAL;
+ 
+ 	if ((be64_to_cpu(READ_ONCE(mem->tb1)) & IMC_TRACE_RECORD_TB1_MASK) !=
+ 			 be64_to_cpu(READ_ONCE(mem->tb2)))
+ 		return -EINVAL;
+ 
+ 	/* Prepare perf sample */
+ 	data->ip =  be64_to_cpu(READ_ONCE(mem->ip));
+ 	data->period = event->hw.last_period;
+ 
+ 	header->type = PERF_RECORD_SAMPLE;
+ 	header->size = sizeof(*header) + event->header_size;
+ 	header->misc = 0;
+ 
+ 	if (is_kernel_addr(data->ip))
+ 		header->misc |= PERF_RECORD_MISC_KERNEL;
+ 	else
+ 		header->misc |= PERF_RECORD_MISC_USER;
+ 
+ 	perf_event_header__init_id(header, data, event);
+ 
+ 	return 0;
+ }
+ 
+ static void dump_trace_imc_data(struct perf_event *event)
+ {
+ 	struct trace_imc_data *mem;
+ 	int i, ret;
+ 	u64 prev_tb = 0;
+ 
+ 	mem = (struct trace_imc_data *)get_trace_imc_event_base_addr();
+ 	for (i = 0; i < (trace_imc_mem_size / sizeof(struct trace_imc_data));
+ 		i++, mem++) {
+ 		struct perf_sample_data data;
+ 		struct perf_event_header header;
+ 
+ 		ret = trace_imc_prepare_sample(mem, &data, &prev_tb, &header, event);
+ 		if (ret) /* Exit, if not a valid record */
+ 			break;
+ 		else {
+ 			/* If this is a valid record, create the sample */
+ 			struct perf_output_handle handle;
+ 
+ 			if (perf_output_begin(&handle, event, header.size))
+ 				return;
+ 
+ 			perf_output_sample(&handle, &header, &data, event);
+ 			perf_output_end(&handle);
+ 		}
+ 	}
+ }
+ 
+ static int trace_imc_event_add(struct perf_event *event, int flags)
+ {
+ 	int core_id = smp_processor_id() / threads_per_core;
+ 	struct imc_pmu_ref *ref = NULL;
+ 	u64 local_mem, ldbar_value;
+ 
+ 	/* Set trace-imc bit in ldbar and load ldbar with per-thread memory address */
+ 	local_mem = get_trace_imc_event_base_addr();
+ 	ldbar_value = ((u64)local_mem & THREAD_IMC_LDBAR_MASK) | TRACE_IMC_ENABLE;
+ 
+ 	if (core_imc_refc)
+ 		ref = &core_imc_refc[core_id];
+ 	if (!ref) {
+ 		/* If core-imc is not enabled, use trace-imc reference count */
+ 		if (trace_imc_refc)
+ 			ref = &trace_imc_refc[core_id];
+ 		if (!ref)
+ 			return -EINVAL;
+ 	}
+ 	mtspr(SPRN_LDBAR, ldbar_value);
+ 	mutex_lock(&ref->lock);
+ 	if (ref->refc == 0) {
+ 		if (opal_imc_counters_start(OPAL_IMC_COUNTERS_TRACE,
+ 				get_hard_smp_processor_id(smp_processor_id()))) {
+ 			mutex_unlock(&ref->lock);
+ 			pr_err("trace-imc: Unable to start the counters for core %d\n", core_id);
+ 			mtspr(SPRN_LDBAR, 0);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	++ref->refc;
+ 	mutex_unlock(&ref->lock);
+ 
+ 	return 0;
+ }
+ 
+ static void trace_imc_event_read(struct perf_event *event)
+ {
+ 	return;
+ }
+ 
+ static void trace_imc_event_stop(struct perf_event *event, int flags)
+ {
+ 	u64 local_mem = get_trace_imc_event_base_addr();
+ 	dump_trace_imc_data(event);
+ 	memset((void *)local_mem, 0, sizeof(u64));
+ }
+ 
+ static void trace_imc_event_start(struct perf_event *event, int flags)
+ {
+ 	return;
+ }
+ 
+ static void trace_imc_event_del(struct perf_event *event, int flags)
+ {
+ 	int core_id = smp_processor_id() / threads_per_core;
+ 	struct imc_pmu_ref *ref = NULL;
+ 
+ 	if (core_imc_refc)
+ 		ref = &core_imc_refc[core_id];
+ 	if (!ref) {
+ 		/* If core-imc is not enabled, use trace-imc reference count */
+ 		if (trace_imc_refc)
+ 			ref = &trace_imc_refc[core_id];
+ 		if (!ref)
+ 			return;
+ 	}
+ 	mtspr(SPRN_LDBAR, 0);
+ 	mutex_lock(&ref->lock);
+ 	ref->refc--;
+ 	if (ref->refc == 0) {
+ 		if (opal_imc_counters_stop(OPAL_IMC_COUNTERS_TRACE,
+ 				get_hard_smp_processor_id(smp_processor_id()))) {
+ 			mutex_unlock(&ref->lock);
+ 			pr_err("trace-imc: Unable to stop the counters for core %d\n", core_id);
+ 			return;
+ 		}
+ 	} else if (ref->refc < 0) {
+ 		ref->refc = 0;
+ 	}
+ 	mutex_unlock(&ref->lock);
+ 	trace_imc_event_stop(event, flags);
+ }
+ 
+ static int trace_imc_event_init(struct perf_event *event)
+ {
+ 	struct task_struct *target;
+ 
+ 	if (event->attr.type != event->pmu->type)
+ 		return -ENOENT;
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EACCES;
+ 
+ 	/* Return if this is a couting event */
+ 	if (event->attr.sample_period == 0)
+ 		return -ENOENT;
+ 
+ 	event->hw.idx = -1;
+ 	target = event->hw.target;
+ 
+ 	event->pmu->task_ctx_nr = perf_hw_context;
+ 	return 0;
+ }
+ 
++>>>>>>> 012ae244845f (powerpc/perf: Trace imc PMU functions)
  /* update_pmu_ops : Populate the appropriate operations for "pmu" */
  static int update_pmu_ops(struct imc_pmu *pmu)
  {
* Unmerged path arch/powerpc/perf/imc-pmu.c
