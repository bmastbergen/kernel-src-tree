KVM: arm64: Support dirty page tracking for PUD hugepages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Punit Agrawal <punit.agrawal@arm.com>
commit 4ea5af53114091e23a8fc279f25637e6c4e892c6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/4ea5af53.failed

In preparation for creating PUD hugepages at stage 2, add support for
write protecting PUD hugepages when they are encountered. Write
protecting guest tables is used to track dirty pages when migrating
VMs.

Also, provide trivial implementations of required kvm_s2pud_* helpers
to allow sharing of code with arm32.

	Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
	Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Will Deacon <will.deacon@arm.com>
[ Replaced BUG() => WARN_ON() in arm32 pud helpers ]
	Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
(cherry picked from commit 4ea5af53114091e23a8fc279f25637e6c4e892c6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/kvm_mmu.h
diff --cc arch/arm/include/asm/kvm_mmu.h
index aa04390ce4a5,9fe6c30eb2fc..000000000000
--- a/arch/arm/include/asm/kvm_mmu.h
+++ b/arch/arm/include/asm/kvm_mmu.h
@@@ -82,6 -82,26 +82,29 @@@ void kvm_clear_hyp_idmap(void)
  #define kvm_mk_pud(pmdp)	__pud(__pa(pmdp) | PMD_TYPE_TABLE)
  #define kvm_mk_pgd(pudp)	({ BUILD_BUG(); 0; })
  
++<<<<<<< HEAD
++=======
+ #define kvm_pfn_pte(pfn, prot)	pfn_pte(pfn, prot)
+ #define kvm_pfn_pmd(pfn, prot)	pfn_pmd(pfn, prot)
+ 
+ #define kvm_pmd_mkhuge(pmd)	pmd_mkhuge(pmd)
+ 
+ /*
+  * The following kvm_*pud*() functions are provided strictly to allow
+  * sharing code with arm64. They should never be called in practice.
+  */
+ static inline void kvm_set_s2pud_readonly(pud_t *pud)
+ {
+ 	WARN_ON(1);
+ }
+ 
+ static inline bool kvm_s2pud_readonly(pud_t *pud)
+ {
+ 	WARN_ON(1);
+ 	return false;
+ }
+ 
++>>>>>>> 4ea5af531140 (KVM: arm64: Support dirty page tracking for PUD hugepages)
  static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
  {
  	pte_val(pte) |= L_PTE_S2_RDWR;
* Unmerged path arch/arm/include/asm/kvm_mmu.h
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 94acbfa0650c..775957d8bdc8 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -246,6 +246,16 @@ static inline bool kvm_s2pmd_exec(pmd_t *pmdp)
 	return !(READ_ONCE(pmd_val(*pmdp)) & PMD_S2_XN);
 }
 
+static inline void kvm_set_s2pud_readonly(pud_t *pudp)
+{
+	kvm_set_s2pte_readonly((pte_t *)pudp);
+}
+
+static inline bool kvm_s2pud_readonly(pud_t *pudp)
+{
+	return kvm_s2pte_readonly((pte_t *)pudp);
+}
+
 #define hyp_pte_table_empty(ptep) kvm_page_empty(ptep)
 
 #ifdef __PAGETABLE_PMD_FOLDED
diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c
index 22e88cffe24c..90c91d93dfb5 100644
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@ -1343,9 +1343,12 @@ static void  stage2_wp_puds(struct kvm *kvm, pgd_t *pgd,
 	do {
 		next = stage2_pud_addr_end(kvm, addr, end);
 		if (!stage2_pud_none(kvm, *pud)) {
-			/* TODO:PUD not supported, revisit later if supported */
-			BUG_ON(stage2_pud_huge(kvm, *pud));
-			stage2_wp_pmds(kvm, pud, addr, next);
+			if (stage2_pud_huge(kvm, *pud)) {
+				if (!kvm_s2pud_readonly(pud))
+					kvm_set_s2pud_readonly(pud);
+			} else {
+				stage2_wp_pmds(kvm, pud, addr, next);
+			}
 		}
 	} while (pud++, addr = next, addr != end);
 }
@@ -1388,7 +1391,7 @@ static void stage2_wp_range(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)
  *
  * Called to start logging dirty pages after memory region
  * KVM_MEM_LOG_DIRTY_PAGES operation is called. After this function returns
- * all present PMD and PTEs are write protected in the memory region.
+ * all present PUD, PMD and PTEs are write protected in the memory region.
  * Afterwards read of dirty page log can be called.
  *
  * Acquires kvm_mmu_lock. Called with kvm->slots_lock mutex acquired,
