x86/intel_rdt: Limit C-states dynamically when pseudo-locking active

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Reinette Chatre <reinette.chatre@intel.com>
commit 6fc0de37f663278af160e8e1f0c38b27e6c06206
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/6fc0de37.failed

Deeper C-states impact cache content through shrinking of the cache or
flushing entire cache to memory before reducing power to the cache.
Deeper C-states will thus negatively impact the pseudo-locked regions.

To avoid impacting pseudo-locked regions C-states are limited on
pseudo-locked region creation so that cores associated with the
pseudo-locked region are prevented from entering deeper C-states.
This is accomplished by requesting a CPU latency target which will
prevent the core from entering C6 across all supported platforms.

	Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: fenghua.yu@intel.com
	Cc: tony.luck@intel.com
	Cc: vikas.shivappa@linux.intel.com
	Cc: gavin.hindman@intel.com
	Cc: jithu.joseph@intel.com
	Cc: dave.hansen@intel.com
	Cc: hpa@zytor.com
Link: https://lkml.kernel.org/r/1ef4f99dd6ba12fa6fb44c5a1141e75f952b9cd9.1529706536.git.reinette.chatre@intel.com


(cherry picked from commit 6fc0de37f663278af160e8e1f0c38b27e6c06206)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/x86/intel_rdt_ui.txt
diff --cc Documentation/x86/intel_rdt_ui.txt
index a16aa2113840,acac30b67c62..000000000000
--- a/Documentation/x86/intel_rdt_ui.txt
+++ b/Documentation/x86/intel_rdt_ui.txt
@@@ -379,6 -421,170 +379,173 @@@ L3CODE:0=fffff;1=fffff;2=fffff;3=ffff
  L3DATA:0=fffff;1=fffff;2=3c0;3=fffff
  L3CODE:0=fffff;1=fffff;2=fffff;3=fffff
  
++<<<<<<< HEAD
++=======
+ Cache Pseudo-Locking
+ --------------------
+ CAT enables a user to specify the amount of cache space that an
+ application can fill. Cache pseudo-locking builds on the fact that a
+ CPU can still read and write data pre-allocated outside its current
+ allocated area on a cache hit. With cache pseudo-locking, data can be
+ preloaded into a reserved portion of cache that no application can
+ fill, and from that point on will only serve cache hits. The cache
+ pseudo-locked memory is made accessible to user space where an
+ application can map it into its virtual address space and thus have
+ a region of memory with reduced average read latency.
+ 
+ The creation of a cache pseudo-locked region is triggered by a request
+ from the user to do so that is accompanied by a schemata of the region
+ to be pseudo-locked. The cache pseudo-locked region is created as follows:
+ - Create a CAT allocation CLOSNEW with a CBM matching the schemata
+   from the user of the cache region that will contain the pseudo-locked
+   memory. This region must not overlap with any current CAT allocation/CLOS
+   on the system and no future overlap with this cache region is allowed
+   while the pseudo-locked region exists.
+ - Create a contiguous region of memory of the same size as the cache
+   region.
+ - Flush the cache, disable hardware prefetchers, disable preemption.
+ - Make CLOSNEW the active CLOS and touch the allocated memory to load
+   it into the cache.
+ - Set the previous CLOS as active.
+ - At this point the closid CLOSNEW can be released - the cache
+   pseudo-locked region is protected as long as its CBM does not appear in
+   any CAT allocation. Even though the cache pseudo-locked region will from
+   this point on not appear in any CBM of any CLOS an application running with
+   any CLOS will be able to access the memory in the pseudo-locked region since
+   the region continues to serve cache hits.
+ - The contiguous region of memory loaded into the cache is exposed to
+   user-space as a character device.
+ 
+ Cache pseudo-locking increases the probability that data will remain
+ in the cache via carefully configuring the CAT feature and controlling
+ application behavior. There is no guarantee that data is placed in
+ cache. Instructions like INVD, WBINVD, CLFLUSH, etc. can still evict
+ “locked” data from cache. Power management C-states may shrink or
+ power off cache. Deeper C-states will automatically be restricted on
+ pseudo-locked region creation.
+ 
+ It is required that an application using a pseudo-locked region runs
+ with affinity to the cores (or a subset of the cores) associated
+ with the cache on which the pseudo-locked region resides. A sanity check
+ within the code will not allow an application to map pseudo-locked memory
+ unless it runs with affinity to cores associated with the cache on which the
+ pseudo-locked region resides. The sanity check is only done during the
+ initial mmap() handling, there is no enforcement afterwards and the
+ application self needs to ensure it remains affine to the correct cores.
+ 
+ Pseudo-locking is accomplished in two stages:
+ 1) During the first stage the system administrator allocates a portion
+    of cache that should be dedicated to pseudo-locking. At this time an
+    equivalent portion of memory is allocated, loaded into allocated
+    cache portion, and exposed as a character device.
+ 2) During the second stage a user-space application maps (mmap()) the
+    pseudo-locked memory into its address space.
+ 
+ Cache Pseudo-Locking Interface
+ ------------------------------
+ A pseudo-locked region is created using the resctrl interface as follows:
+ 
+ 1) Create a new resource group by creating a new directory in /sys/fs/resctrl.
+ 2) Change the new resource group's mode to "pseudo-locksetup" by writing
+    "pseudo-locksetup" to the "mode" file.
+ 3) Write the schemata of the pseudo-locked region to the "schemata" file. All
+    bits within the schemata should be "unused" according to the "bit_usage"
+    file.
+ 
+ On successful pseudo-locked region creation the "mode" file will contain
+ "pseudo-locked" and a new character device with the same name as the resource
+ group will exist in /dev/pseudo_lock. This character device can be mmap()'ed
+ by user space in order to obtain access to the pseudo-locked memory region.
+ 
+ An example of cache pseudo-locked region creation and usage can be found below.
+ 
+ Cache Pseudo-Locking Debugging Interface
+ ---------------------------------------
+ The pseudo-locking debugging interface is enabled by default (if
+ CONFIG_DEBUG_FS is enabled) and can be found in /sys/kernel/debug/resctrl.
+ 
+ There is no explicit way for the kernel to test if a provided memory
+ location is present in the cache. The pseudo-locking debugging interface uses
+ the tracing infrastructure to provide two ways to measure cache residency of
+ the pseudo-locked region:
+ 1) Memory access latency using the pseudo_lock_mem_latency tracepoint. Data
+    from these measurements are best visualized using a hist trigger (see
+    example below). In this test the pseudo-locked region is traversed at
+    a stride of 32 bytes while hardware prefetchers and preemption
+    are disabled. This also provides a substitute visualization of cache
+    hits and misses.
+ 2) Cache hit and miss measurements using model specific precision counters if
+    available. Depending on the levels of cache on the system the pseudo_lock_l2
+    and pseudo_lock_l3 tracepoints are available.
+    WARNING: triggering this  measurement uses from two (for just L2
+    measurements) to four (for L2 and L3 measurements) precision counters on
+    the system, if any other measurements are in progress the counters and
+    their corresponding event registers will be clobbered.
+ 
+ When a pseudo-locked region is created a new debugfs directory is created for
+ it in debugfs as /sys/kernel/debug/resctrl/<newdir>. A single
+ write-only file, pseudo_lock_measure, is present in this directory. The
+ measurement on the pseudo-locked region depends on the number, 1 or 2,
+ written to this debugfs file. Since the measurements are recorded with the
+ tracing infrastructure the relevant tracepoints need to be enabled before the
+ measurement is triggered.
+ 
+ Example of latency debugging interface:
+ In this example a pseudo-locked region named "newlock" was created. Here is
+ how we can measure the latency in cycles of reading from this region and
+ visualize this data with a histogram that is available if CONFIG_HIST_TRIGGERS
+ is set:
+ # :> /sys/kernel/debug/tracing/trace
+ # echo 'hist:keys=latency' > /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_mem_latency/trigger
+ # echo 1 > /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_mem_latency/enable
+ # echo 1 > /sys/kernel/debug/resctrl/newlock/pseudo_lock_measure
+ # echo 0 > /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_mem_latency/enable
+ # cat /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_mem_latency/hist
+ 
+ # event histogram
+ #
+ # trigger info: hist:keys=latency:vals=hitcount:sort=hitcount:size=2048 [active]
+ #
+ 
+ { latency:        456 } hitcount:          1
+ { latency:         50 } hitcount:         83
+ { latency:         36 } hitcount:         96
+ { latency:         44 } hitcount:        174
+ { latency:         48 } hitcount:        195
+ { latency:         46 } hitcount:        262
+ { latency:         42 } hitcount:        693
+ { latency:         40 } hitcount:       3204
+ { latency:         38 } hitcount:       3484
+ 
+ Totals:
+     Hits: 8192
+     Entries: 9
+    Dropped: 0
+ 
+ Example of cache hits/misses debugging:
+ In this example a pseudo-locked region named "newlock" was created on the L2
+ cache of a platform. Here is how we can obtain details of the cache hits
+ and misses using the platform's precision counters.
+ 
+ # :> /sys/kernel/debug/tracing/trace
+ # echo 1 > /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_l2/enable
+ # echo 2 > /sys/kernel/debug/resctrl/newlock/pseudo_lock_measure
+ # echo 0 > /sys/kernel/debug/tracing/events/resctrl/pseudo_lock_l2/enable
+ # cat /sys/kernel/debug/tracing/trace
+ 
+ # tracer: nop
+ #
+ #                              _-----=> irqs-off
+ #                             / _----=> need-resched
+ #                            | / _---=> hardirq/softirq
+ #                            || / _--=> preempt-depth
+ #                            ||| /     delay
+ #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
+ #              | |       |   ||||       |         |
+  pseudo_lock_mea-1672  [002] ....  3132.860500: pseudo_lock_l2: hits=4097 miss=0
+ 
+ 
++>>>>>>> 6fc0de37f663 (x86/intel_rdt: Limit C-states dynamically when pseudo-locking active)
  Examples for RDT allocation usage:
  
  Example 1
* Unmerged path Documentation/x86/intel_rdt_ui.txt
diff --git a/arch/x86/kernel/cpu/intel_rdt.h b/arch/x86/kernel/cpu/intel_rdt.h
index b8e490a43290..2d9cbb9d7a58 100644
--- a/arch/x86/kernel/cpu/intel_rdt.h
+++ b/arch/x86/kernel/cpu/intel_rdt.h
@@ -142,6 +142,7 @@ struct mongroup {
  *			region
  * @debugfs_dir:	pointer to this region's directory in the debugfs
  *			filesystem
+ * @pm_reqs:		Power management QoS requests related to this region
  */
 struct pseudo_lock_region {
 	struct rdt_resource	*r;
@@ -155,6 +156,7 @@ struct pseudo_lock_region {
 	void			*kmem;
 	unsigned int		minor;
 	struct dentry		*debugfs_dir;
+	struct list_head	pm_reqs;
 };
 
 /**
diff --git a/arch/x86/kernel/cpu/intel_rdt_pseudo_lock.c b/arch/x86/kernel/cpu/intel_rdt_pseudo_lock.c
index dd1341557c9d..6e83f61552a5 100644
--- a/arch/x86/kernel/cpu/intel_rdt_pseudo_lock.c
+++ b/arch/x86/kernel/cpu/intel_rdt_pseudo_lock.c
@@ -17,6 +17,7 @@
 #include <linux/debugfs.h>
 #include <linux/kthread.h>
 #include <linux/mman.h>
+#include <linux/pm_qos.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
 
@@ -175,6 +176,76 @@ static struct rdtgroup *region_find_by_minor(unsigned int minor)
 	return rdtgrp_match;
 }
 
+/**
+ * pseudo_lock_pm_req - A power management QoS request list entry
+ * @list:	Entry within the @pm_reqs list for a pseudo-locked region
+ * @req:	PM QoS request
+ */
+struct pseudo_lock_pm_req {
+	struct list_head list;
+	struct dev_pm_qos_request req;
+};
+
+static void pseudo_lock_cstates_relax(struct pseudo_lock_region *plr)
+{
+	struct pseudo_lock_pm_req *pm_req, *next;
+
+	list_for_each_entry_safe(pm_req, next, &plr->pm_reqs, list) {
+		dev_pm_qos_remove_request(&pm_req->req);
+		list_del(&pm_req->list);
+		kfree(pm_req);
+	}
+}
+
+/**
+ * pseudo_lock_cstates_constrain - Restrict cores from entering C6
+ *
+ * To prevent the cache from being affected by power management entering
+ * C6 has to be avoided. This is accomplished by requesting a latency
+ * requirement lower than lowest C6 exit latency of all supported
+ * platforms as found in the cpuidle state tables in the intel_idle driver.
+ * At this time it is possible to do so with a single latency requirement
+ * for all supported platforms.
+ *
+ * Since Goldmont is supported, which is affected by X86_BUG_MONITOR,
+ * the ACPI latencies need to be considered while keeping in mind that C2
+ * may be set to map to deeper sleep states. In this case the latency
+ * requirement needs to prevent entering C2 also.
+ */
+static int pseudo_lock_cstates_constrain(struct pseudo_lock_region *plr)
+{
+	struct pseudo_lock_pm_req *pm_req;
+	int cpu;
+	int ret;
+
+	for_each_cpu(cpu, &plr->d->cpu_mask) {
+		pm_req = kzalloc(sizeof(*pm_req), GFP_KERNEL);
+		if (!pm_req) {
+			rdt_last_cmd_puts("fail allocating mem for PM QoS\n");
+			ret = -ENOMEM;
+			goto out_err;
+		}
+		ret = dev_pm_qos_add_request(get_cpu_device(cpu),
+					     &pm_req->req,
+					     DEV_PM_QOS_RESUME_LATENCY,
+					     30);
+		if (ret < 0) {
+			rdt_last_cmd_printf("fail to add latency req cpu%d\n",
+					    cpu);
+			kfree(pm_req);
+			ret = -1;
+			goto out_err;
+		}
+		list_add(&pm_req->list, &plr->pm_reqs);
+	}
+
+	return 0;
+
+out_err:
+	pseudo_lock_cstates_relax(plr);
+	return ret;
+}
+
 /**
  * pseudo_lock_region_init - Initialize pseudo-lock region information
  * @plr: pseudo-lock region
@@ -242,6 +313,7 @@ static int pseudo_lock_init(struct rdtgroup *rdtgrp)
 		return -ENOMEM;
 
 	init_waitqueue_head(&plr->lock_thread_wq);
+	INIT_LIST_HEAD(&plr->pm_reqs);
 	rdtgrp->plr = plr;
 	return 0;
 }
@@ -1135,6 +1207,12 @@ int rdtgroup_pseudo_lock_create(struct rdtgroup *rdtgrp)
 	if (ret < 0)
 		return ret;
 
+	ret = pseudo_lock_cstates_constrain(plr);
+	if (ret < 0) {
+		ret = -EINVAL;
+		goto out_region;
+	}
+
 	plr->thread_done = 0;
 
 	thread = kthread_create_on_node(pseudo_lock_fn, rdtgrp,
@@ -1143,7 +1221,7 @@ int rdtgroup_pseudo_lock_create(struct rdtgroup *rdtgrp)
 	if (IS_ERR(thread)) {
 		ret = PTR_ERR(thread);
 		rdt_last_cmd_printf("locking thread returned error %d\n", ret);
-		goto out_region;
+		goto out_cstates;
 	}
 
 	kthread_bind(thread, plr->cpu);
@@ -1161,7 +1239,7 @@ int rdtgroup_pseudo_lock_create(struct rdtgroup *rdtgrp)
 		 * empty pseudo-locking loop.
 		 */
 		rdt_last_cmd_puts("locking thread interrupted\n");
-		goto out_region;
+		goto out_cstates;
 	}
 
 	if (!IS_ERR_OR_NULL(debugfs_resctrl)) {
@@ -1222,6 +1300,8 @@ int rdtgroup_pseudo_lock_create(struct rdtgroup *rdtgrp)
 	pseudo_lock_minor_release(new_minor);
 out_debugfs:
 	debugfs_remove_recursive(plr->debugfs_dir);
+out_cstates:
+	pseudo_lock_cstates_relax(plr);
 out_region:
 	pseudo_lock_region_clear(plr);
 out:
@@ -1255,6 +1335,7 @@ void rdtgroup_pseudo_lock_remove(struct rdtgroup *rdtgrp)
 		goto free;
 	}
 
+	pseudo_lock_cstates_relax(plr);
 	debugfs_remove_recursive(rdtgrp->plr->debugfs_dir);
 	device_destroy(pseudo_lock_class, MKDEV(pseudo_lock_major, plr->minor));
 	pseudo_lock_minor_release(plr->minor);
