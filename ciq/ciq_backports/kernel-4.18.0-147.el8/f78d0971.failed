KVM: VMX: Don't save guest registers after VM-Fail

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit f78d0971b7bd5bf4373a1fac27f176af5d5594ed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/f78d0971.failed

A failed VM-Enter (obviously) didn't succeed, meaning the CPU never
executed an instrunction in guest mode and so can't have changed the
general purpose registers.

In addition to saving some instructions in the VM-Fail case, this also
provides a separate path entirely and thus an opportunity to propagate
the fail condition to vmx->fail via register without introducing undue
pain.  Using a register, as opposed to directly referencing vmx->fail,
eliminates the need to pass the offset of 'fail', which will simplify
moving the code to proper assembly in future patches.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f78d0971b7bd5bf4373a1fac27f176af5d5594ed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 44f839379d76,1dcd3f157a70..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -9259,1973 -3684,1561 +9259,2138 @@@ static u16 nested_get_vpid02(struct kvm
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -	if (is_guest_mode(vcpu) &&
 -	    vector == vmx->nested.posted_intr_nv) {
 -		/*
 -		 * If a posted intr is not recognized by hardware,
 -		 * we will accomplish it in the next vmentry.
 -		 */
 -		vmx->nested.pi_pending = true;
 -		kvm_make_request(KVM_REQ_EVENT, vcpu);
 -		/* the PIR and ON have been set by L1. */
 -		if (!kvm_vcpu_trigger_posted_interrupt(vcpu, true))
 -			kvm_vcpu_kick(vcpu);
 -		return 0;
 -	}
 -	return -1;
 +	return vmx->nested.vpid02 ? vmx->nested.vpid02 : vmx->vpid;
  }
 -/*
 - * Send interrupt to vcpu via posted interrupt way.
 - * 1. If target vcpu is running(non-root mode), send posted interrupt
 - * notification to vcpu and hardware will sync PIR to vIRR atomically.
 - * 2. If target vcpu isn't running(root mode), kick it to pick up the
 - * interrupt from PIR in next vmentry.
 - */
 -static void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 +
 +static int handle_invvpid(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int r;
 +	u32 vmx_instruction_info;
 +	unsigned long type, types;
 +	gva_t gva;
 +	struct x86_exception e;
 +	struct {
 +		u64 vpid;
 +		u64 gla;
 +	} operand;
 +	u16 vpid02;
  
 -	r = vmx_deliver_nested_posted_interrupt(vcpu, vector);
 -	if (!r)
 -		return;
 +	if (!(vmx->nested.msrs.secondary_ctls_high &
 +	      SECONDARY_EXEC_ENABLE_VPID) ||
 +			!(vmx->nested.msrs.vpid_caps & VMX_VPID_INVVPID_BIT)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
  
 -	if (pi_test_and_set_pir(vector, &vmx->pi_desc))
 -		return;
 +	if (!nested_vmx_check_permission(vcpu))
 +		return 1;
  
 -	/* If a previous notification has sent the IPI, nothing to do.  */
 -	if (pi_test_and_set_on(&vmx->pi_desc))
 -		return;
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
  
 -	if (!kvm_vcpu_trigger_posted_interrupt(vcpu, false))
 -		kvm_vcpu_kick(vcpu);
 +	types = (vmx->nested.msrs.vpid_caps &
 +			VMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;
 +
 +	if (type >= 32 || !(types & (1 << type))) {
 +		nested_vmx_failValid(vcpu,
 +			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	/* according to the intel vmx instruction reference, the memory
 +	 * operand is read even if it isn't needed (e.g., for type==global)
 +	 */
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +			vmx_instruction_info, false, &gva))
 +		return 1;
 +	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
 +	if (operand.vpid >> 16) {
 +		nested_vmx_failValid(vcpu,
 +			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	vpid02 = nested_get_vpid02(vcpu);
 +	switch (type) {
 +	case VMX_VPID_EXTENT_INDIVIDUAL_ADDR:
 +		if (!operand.vpid ||
 +		    is_noncanonical_address(operand.gla, vcpu)) {
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		if (cpu_has_vmx_invvpid_individual_addr()) {
 +			__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR,
 +				vpid02, operand.gla);
 +		} else
 +			__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	case VMX_VPID_EXTENT_SINGLE_CONTEXT:
 +	case VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:
 +		if (!operand.vpid) {
 +			nested_vmx_failValid(vcpu,
 +				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 +			return kvm_skip_emulated_instruction(vcpu);
 +		}
 +		__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	case VMX_VPID_EXTENT_ALL_CONTEXT:
 +		__vmx_flush_tlb(vcpu, vpid02, false);
 +		break;
 +	default:
 +		WARN_ON_ONCE(1);
 +		return kvm_skip_emulated_instruction(vcpu);
 +	}
 +
 +	nested_vmx_succeed(vcpu);
 +
 +	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -/*
 - * Set up the vmcs's constant host-state fields, i.e., host-state fields that
 - * will not change in the lifetime of the guest.
 - * Note that host-state that does change is set elsewhere. E.g., host-state
 - * that is set differently for each CPU is set in vmx_vcpu_load(), not here.
 - */
 -void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
 +static int handle_invpcid(struct kvm_vcpu *vcpu)
  {
 -	u32 low32, high32;
 -	unsigned long tmpl;
 -	struct desc_ptr dt;
 -	unsigned long cr0, cr3, cr4;
 +	u32 vmx_instruction_info;
 +	unsigned long type;
 +	bool pcid_enabled;
 +	gva_t gva;
 +	struct x86_exception e;
 +	unsigned i;
 +	unsigned long roots_to_free = 0;
 +	struct {
 +		u64 pcid;
 +		u64 gla;
 +	} operand;
  
 -	cr0 = read_cr0();
 -	WARN_ON(cr0 & X86_CR0_TS);
 -	vmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */
 +	if (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
  
 -	/*
 -	 * Save the most likely value for this task's CR3 in the VMCS.
 -	 * We can't use __get_current_cr3_fast() because we're not atomic.
 -	 */
 -	cr3 = __read_cr3();
 -	vmcs_writel(HOST_CR3, cr3);		/* 22.2.3  FIXME: shadow tables */
 -	vmx->loaded_vmcs->host_state.cr3 = cr3;
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
  
 -	/* Save the most likely value for this task's CR4 in the VMCS. */
 -	cr4 = cr4_read_shadow();
 -	vmcs_writel(HOST_CR4, cr4);			/* 22.2.3, 22.2.5 */
 -	vmx->loaded_vmcs->host_state.cr4 = cr4;
 +	if (type > 3) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
  
 -	vmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */
 -#ifdef CONFIG_X86_64
 -	/*
 -	 * Load null selectors, so we can avoid reloading them in
 -	 * vmx_prepare_switch_to_host(), in case userspace uses
 -	 * the null selectors too (the expected case).
 +	/* According to the Intel instruction reference, the memory operand
 +	 * is read even if it isn't needed (e.g., for type==all)
  	 */
 -	vmcs_write16(HOST_DS_SELECTOR, 0);
 -	vmcs_write16(HOST_ES_SELECTOR, 0);
 -#else
 -	vmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 -	vmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 -#endif
 -	vmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
 -	vmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */
 +	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 +				vmx_instruction_info, false, &gva))
 +		return 1;
  
 -	store_idt(&dt);
 -	vmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */
 -	vmx->host_idt_base = dt.address;
 +	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
 +		kvm_inject_page_fault(vcpu, &e);
 +		return 1;
 +	}
  
 -	vmcs_writel(HOST_RIP, (unsigned long)vmx_vmexit); /* 22.2.5 */
 +	if (operand.pcid >> 12 != 0) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
  
 -	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
 -	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
 -	rdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);
 -	vmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */
 +	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
  
 -	if (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {
 -		rdmsr(MSR_IA32_CR_PAT, low32, high32);
 -		vmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));
 -	}
 +	switch (type) {
 +	case INVPCID_TYPE_INDIV_ADDR:
 +		if ((!pcid_enabled && (operand.pcid != 0)) ||
 +		    is_noncanonical_address(operand.gla, vcpu)) {
 +			kvm_inject_gp(vcpu, 0);
 +			return 1;
 +		}
 +		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
 +		return kvm_skip_emulated_instruction(vcpu);
  
 -	if (cpu_has_load_ia32_efer())
 -		vmcs_write64(HOST_IA32_EFER, host_efer);
 -}
 +	case INVPCID_TYPE_SINGLE_CTXT:
 +		if (!pcid_enabled && (operand.pcid != 0)) {
 +			kvm_inject_gp(vcpu, 0);
 +			return 1;
 +		}
  
 -void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
 -{
 -	vmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;
 -	if (enable_ept)
 -		vmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;
 -	if (is_guest_mode(&vmx->vcpu))
 -		vmx->vcpu.arch.cr4_guest_owned_bits &=
 -			~get_vmcs12(&vmx->vcpu)->cr4_guest_host_mask;
 -	vmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);
 +		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
 +			kvm_mmu_sync_roots(vcpu);
 +			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +		}
 +
 +		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 +			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].cr3)
 +			    == operand.pcid)
 +				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
 +
 +		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);
 +		/*
 +		 * If neither the current cr3 nor any of the prev_roots use the
 +		 * given PCID, then nothing needs to be done here because a
 +		 * resync will happen anyway before switching to any other CR3.
 +		 */
 +
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	case INVPCID_TYPE_ALL_NON_GLOBAL:
 +		/*
 +		 * Currently, KVM doesn't mark global entries in the shadow
 +		 * page tables, so a non-global flush just degenerates to a
 +		 * global flush. If needed, we could optimize this later by
 +		 * keeping track of global entries in shadow page tables.
 +		 */
 +
 +		/* fall-through */
 +	case INVPCID_TYPE_ALL_INCL_GLOBAL:
 +		kvm_mmu_unload(vcpu);
 +		return kvm_skip_emulated_instruction(vcpu);
 +
 +	default:
 +		BUG(); /* We have already checked above that type <= 3 */
 +	}
  }
  
 -static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 +static int handle_pml_full(struct kvm_vcpu *vcpu)
  {
 -	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;
 +	unsigned long exit_qualification;
  
 -	if (!kvm_vcpu_apicv_active(&vmx->vcpu))
 -		pin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;
 +	trace_kvm_pml_full(vcpu->vcpu_id);
  
 -	if (!enable_vnmi)
 -		pin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
  
 -	/* Enable the preemption timer dynamically */
 -	pin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
 -	return pin_based_exec_ctrl;
 +	/*
 +	 * PML buffer FULL happened while executing iret from NMI,
 +	 * "blocked by NMI" bit has to be set before next VM entry.
 +	 */
 +	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			enable_vnmi &&
 +			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 +		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				GUEST_INTR_STATE_NMI);
 +
 +	/*
 +	 * PML buffer already flushed at beginning of VMEXIT. Nothing to do
 +	 * here.., and there's no userspace involvement needed for PML.
 +	 */
 +	return 1;
  }
  
 -static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 +static int handle_preemption_timer(struct kvm_vcpu *vcpu)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -
 -	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
 -	if (cpu_has_secondary_exec_ctrls()) {
 -		if (kvm_vcpu_apicv_active(vcpu))
 -			vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 -				      SECONDARY_EXEC_APIC_REGISTER_VIRT |
 -				      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 -		else
 -			vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,
 -					SECONDARY_EXEC_APIC_REGISTER_VIRT |
 -					SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 -	}
 -
 -	if (cpu_has_vmx_msr_bitmap())
 -		vmx_update_msr_bitmap(vcpu);
 +	if (!to_vmx(vcpu)->req_immediate_exit)
 +		kvm_lapic_expired_hv_timer(vcpu);
 +	return 1;
  }
  
 -u32 vmx_exec_control(struct vcpu_vmx *vmx)
 +static bool valid_ept_address(struct kvm_vcpu *vcpu, u64 address)
  {
 -	u32 exec_control = vmcs_config.cpu_based_exec_ctrl;
 -
 -	if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)
 -		exec_control &= ~CPU_BASED_MOV_DR_EXITING;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int maxphyaddr = cpuid_maxphyaddr(vcpu);
  
 -	if (!cpu_need_tpr_shadow(&vmx->vcpu)) {
 -		exec_control &= ~CPU_BASED_TPR_SHADOW;
 -#ifdef CONFIG_X86_64
 -		exec_control |= CPU_BASED_CR8_STORE_EXITING |
 -				CPU_BASED_CR8_LOAD_EXITING;
 -#endif
 +	/* Check for memory type validity */
 +	switch (address & VMX_EPTP_MT_MASK) {
 +	case VMX_EPTP_MT_UC:
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT))
 +			return false;
 +		break;
 +	case VMX_EPTP_MT_WB:
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT))
 +			return false;
 +		break;
 +	default:
 +		return false;
  	}
 -	if (!enable_ept)
 -		exec_control |= CPU_BASED_CR3_STORE_EXITING |
 -				CPU_BASED_CR3_LOAD_EXITING  |
 -				CPU_BASED_INVLPG_EXITING;
 -	if (kvm_mwait_in_guest(vmx->vcpu.kvm))
 -		exec_control &= ~(CPU_BASED_MWAIT_EXITING |
 -				CPU_BASED_MONITOR_EXITING);
 -	if (kvm_hlt_in_guest(vmx->vcpu.kvm))
 -		exec_control &= ~CPU_BASED_HLT_EXITING;
 -	return exec_control;
 -}
  
 +	/* only 4 levels page-walk length are valid */
 +	if ((address & VMX_EPTP_PWL_MASK) != VMX_EPTP_PWL_4)
 +		return false;
  
 -static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 -{
 -	struct kvm_vcpu *vcpu = &vmx->vcpu;
 -
 -	u32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;
 +	/* Reserved bits should not be set */
 +	if (address >> maxphyaddr || ((address >> 7) & 0x1f))
 +		return false;
  
 -	if (pt_mode == PT_MODE_SYSTEM)
 -		exec_control &= ~(SECONDARY_EXEC_PT_USE_GPA | SECONDARY_EXEC_PT_CONCEAL_VMX);
 -	if (!cpu_need_virtualize_apic_accesses(vcpu))
 -		exec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 -	if (vmx->vpid == 0)
 -		exec_control &= ~SECONDARY_EXEC_ENABLE_VPID;
 -	if (!enable_ept) {
 -		exec_control &= ~SECONDARY_EXEC_ENABLE_EPT;
 -		enable_unrestricted_guest = 0;
 +	/* AD, if set, should be supported */
 +	if (address & VMX_EPTP_AD_ENABLE_BIT) {
 +		if (!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT))
 +			return false;
  	}
 -	if (!enable_unrestricted_guest)
 -		exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;
 -	if (kvm_pause_in_guest(vmx->vcpu.kvm))
 -		exec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;
 -	if (!kvm_vcpu_apicv_active(vcpu))
 -		exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |
 -				  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 -	exec_control &= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
 -
 -	/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,
 -	 * in vmx_set_cr4.  */
 -	exec_control &= ~SECONDARY_EXEC_DESC;
 -
 -	/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD
 -	   (handle_vmptrld).
 -	   We can NOT enable shadow_vmcs here because we don't have yet
 -	   a current VMCS12
 -	*/
 -	exec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;
  
 -	if (!enable_pml)
 -		exec_control &= ~SECONDARY_EXEC_ENABLE_PML;
 +	return true;
 +}
  
 -	if (vmx_xsaves_supported()) {
 -		/* Exposing XSAVES only when XSAVE is exposed */
 -		bool xsaves_enabled =
 -			guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 -			guest_cpuid_has(vcpu, X86_FEATURE_XSAVES);
 +static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 +				     struct vmcs12 *vmcs12)
 +{
 +	u32 index = vcpu->arch.regs[VCPU_REGS_RCX];
 +	u64 address;
 +	bool accessed_dirty;
 +	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
  
 -		if (!xsaves_enabled)
 -			exec_control &= ~SECONDARY_EXEC_XSAVES;
 +	if (!nested_cpu_has_eptp_switching(vmcs12) ||
 +	    !nested_cpu_has_ept(vmcs12))
 +		return 1;
  
 -		if (nested) {
 -			if (xsaves_enabled)
 -				vmx->nested.msrs.secondary_ctls_high |=
 -					SECONDARY_EXEC_XSAVES;
 -			else
 -				vmx->nested.msrs.secondary_ctls_high &=
 -					~SECONDARY_EXEC_XSAVES;
 -		}
 -	}
 +	if (index >= VMFUNC_EPTP_ENTRIES)
 +		return 1;
  
 -	if (vmx_rdtscp_supported()) {
 -		bool rdtscp_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP);
 -		if (!rdtscp_enabled)
 -			exec_control &= ~SECONDARY_EXEC_RDTSCP;
  
 -		if (nested) {
 -			if (rdtscp_enabled)
 -				vmx->nested.msrs.secondary_ctls_high |=
 -					SECONDARY_EXEC_RDTSCP;
 -			else
 -				vmx->nested.msrs.secondary_ctls_high &=
 -					~SECONDARY_EXEC_RDTSCP;
 -		}
 -	}
 +	if (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,
 +				     &address, index * 8, 8))
 +		return 1;
  
 -	if (vmx_invpcid_supported()) {
 -		/* Exposing INVPCID only when PCID is exposed */
 -		bool invpcid_enabled =
 -			guest_cpuid_has(vcpu, X86_FEATURE_INVPCID) &&
 -			guest_cpuid_has(vcpu, X86_FEATURE_PCID);
 +	accessed_dirty = !!(address & VMX_EPTP_AD_ENABLE_BIT);
  
 -		if (!invpcid_enabled) {
 -			exec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;
 -			guest_cpuid_clear(vcpu, X86_FEATURE_INVPCID);
 -		}
 +	/*
 +	 * If the (L2) guest does a vmfunc to the currently
 +	 * active ept pointer, we don't have to do anything else
 +	 */
 +	if (vmcs12->ept_pointer != address) {
 +		if (!valid_ept_address(vcpu, address))
 +			return 1;
  
 -		if (nested) {
 -			if (invpcid_enabled)
 -				vmx->nested.msrs.secondary_ctls_high |=
 -					SECONDARY_EXEC_ENABLE_INVPCID;
 -			else
 -				vmx->nested.msrs.secondary_ctls_high &=
 -					~SECONDARY_EXEC_ENABLE_INVPCID;
 -		}
 +		kvm_mmu_unload(vcpu);
 +		mmu->ept_ad = accessed_dirty;
 +		mmu->mmu_role.base.ad_disabled = !accessed_dirty;
 +		vmcs12->ept_pointer = address;
 +		/*
 +		 * TODO: Check what's the correct approach in case
 +		 * mmu reload fails. Currently, we just let the next
 +		 * reload potentially fail
 +		 */
 +		kvm_mmu_reload(vcpu);
  	}
  
 -	if (vmx_rdrand_supported()) {
 -		bool rdrand_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDRAND);
 -		if (rdrand_enabled)
 -			exec_control &= ~SECONDARY_EXEC_RDRAND_EXITING;
 +	return 0;
 +}
  
 -		if (nested) {
 -			if (rdrand_enabled)
 -				vmx->nested.msrs.secondary_ctls_high |=
 -					SECONDARY_EXEC_RDRAND_EXITING;
 -			else
 -				vmx->nested.msrs.secondary_ctls_high &=
 -					~SECONDARY_EXEC_RDRAND_EXITING;
 -		}
 +static int handle_vmfunc(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	struct vmcs12 *vmcs12;
 +	u32 function = vcpu->arch.regs[VCPU_REGS_RAX];
 +
 +	/*
 +	 * VMFUNC is only supported for nested guests, but we always enable the
 +	 * secondary control for simplicity; for non-nested mode, fake that we
 +	 * didn't by injecting #UD.
 +	 */
 +	if (!is_guest_mode(vcpu)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
  	}
  
 -	if (vmx_rdseed_supported()) {
 -		bool rdseed_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDSEED);
 -		if (rdseed_enabled)
 -			exec_control &= ~SECONDARY_EXEC_RDSEED_EXITING;
 +	vmcs12 = get_vmcs12(vcpu);
 +	if ((vmcs12->vm_function_control & (1 << function)) == 0)
 +		goto fail;
  
 -		if (nested) {
 -			if (rdseed_enabled)
 -				vmx->nested.msrs.secondary_ctls_high |=
 -					SECONDARY_EXEC_RDSEED_EXITING;
 -			else
 -				vmx->nested.msrs.secondary_ctls_high &=
 -					~SECONDARY_EXEC_RDSEED_EXITING;
 -		}
 +	switch (function) {
 +	case 0:
 +		if (nested_vmx_eptp_switching(vcpu, vmcs12))
 +			goto fail;
 +		break;
 +	default:
 +		goto fail;
  	}
 +	return kvm_skip_emulated_instruction(vcpu);
  
 -	vmx->secondary_exec_control = exec_control;
 +fail:
 +	nested_vmx_vmexit(vcpu, vmx->exit_reason,
 +			  vmcs_read32(VM_EXIT_INTR_INFO),
 +			  vmcs_readl(EXIT_QUALIFICATION));
 +	return 1;
  }
  
 -static void ept_set_mmio_spte_mask(void)
 +static int handle_encls(struct kvm_vcpu *vcpu)
  {
  	/*
 -	 * EPT Misconfigurations can be generated if the value of bits 2:0
 -	 * of an EPT paging-structure entry is 110b (write/execute).
 +	 * SGX virtualization is not yet supported.  There is no software
 +	 * enable bit for SGX, so we have to trap ENCLS and inject a #UD
 +	 * to prevent the guest from executing ENCLS.
  	 */
 -	kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,
 -				   VMX_EPT_MISCONFIG_WX_VALUE);
 +	kvm_queue_exception(vcpu, UD_VECTOR);
 +	return 1;
  }
  
 -#define VMX_XSS_EXIT_BITMAP 0
 +/*
 + * The exit handlers return 1 if the exit was handled fully and guest execution
 + * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
 + * to be done to userspace and return 0.
 + */
 +static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 +	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,
 +	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
 +	[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,
 +	[EXIT_REASON_NMI_WINDOW]	      = handle_nmi_window,
 +	[EXIT_REASON_IO_INSTRUCTION]          = handle_io,
 +	[EXIT_REASON_CR_ACCESS]               = handle_cr,
 +	[EXIT_REASON_DR_ACCESS]               = handle_dr,
 +	[EXIT_REASON_CPUID]                   = handle_cpuid,
 +	[EXIT_REASON_MSR_READ]                = handle_rdmsr,
 +	[EXIT_REASON_MSR_WRITE]               = handle_wrmsr,
 +	[EXIT_REASON_PENDING_INTERRUPT]       = handle_interrupt_window,
 +	[EXIT_REASON_HLT]                     = handle_halt,
 +	[EXIT_REASON_INVD]		      = handle_invd,
 +	[EXIT_REASON_INVLPG]		      = handle_invlpg,
 +	[EXIT_REASON_RDPMC]                   = handle_rdpmc,
 +	[EXIT_REASON_VMCALL]                  = handle_vmcall,
 +	[EXIT_REASON_VMCLEAR]	              = handle_vmclear,
 +	[EXIT_REASON_VMLAUNCH]                = handle_vmlaunch,
 +	[EXIT_REASON_VMPTRLD]                 = handle_vmptrld,
 +	[EXIT_REASON_VMPTRST]                 = handle_vmptrst,
 +	[EXIT_REASON_VMREAD]                  = handle_vmread,
 +	[EXIT_REASON_VMRESUME]                = handle_vmresume,
 +	[EXIT_REASON_VMWRITE]                 = handle_vmwrite,
 +	[EXIT_REASON_VMOFF]                   = handle_vmoff,
 +	[EXIT_REASON_VMON]                    = handle_vmon,
 +	[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,
 +	[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,
 +	[EXIT_REASON_APIC_WRITE]              = handle_apic_write,
 +	[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,
 +	[EXIT_REASON_WBINVD]                  = handle_wbinvd,
 +	[EXIT_REASON_XSETBV]                  = handle_xsetbv,
 +	[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,
 +	[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,
 +	[EXIT_REASON_GDTR_IDTR]		      = handle_desc,
 +	[EXIT_REASON_LDTR_TR]		      = handle_desc,
 +	[EXIT_REASON_EPT_VIOLATION]	      = handle_ept_violation,
 +	[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,
 +	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
 +	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
 +	[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,
 +	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
 +	[EXIT_REASON_INVEPT]                  = handle_invept,
 +	[EXIT_REASON_INVVPID]                 = handle_invvpid,
 +	[EXIT_REASON_RDRAND]                  = handle_invalid_op,
 +	[EXIT_REASON_RDSEED]                  = handle_invalid_op,
 +	[EXIT_REASON_XSAVES]                  = handle_xsaves,
 +	[EXIT_REASON_XRSTORS]                 = handle_xrstors,
 +	[EXIT_REASON_PML_FULL]		      = handle_pml_full,
 +	[EXIT_REASON_INVPCID]                 = handle_invpcid,
 +	[EXIT_REASON_VMFUNC]                  = handle_vmfunc,
 +	[EXIT_REASON_PREEMPTION_TIMER]	      = handle_preemption_timer,
 +	[EXIT_REASON_ENCLS]		      = handle_encls,
 +};
 +
 +static const int kvm_vmx_max_exit_handlers =
 +	ARRAY_SIZE(kvm_vmx_exit_handlers);
  
 -/*
 - * Sets up the vmcs for emulated real mode.
 - */
 -static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 +static bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,
 +				       struct vmcs12 *vmcs12)
  {
 -	int i;
 -
 -	if (nested)
 -		nested_vmx_vcpu_setup();
 -
 -	if (cpu_has_vmx_msr_bitmap())
 -		vmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));
 -
 -	vmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */
 +	unsigned long exit_qualification;
 +	gpa_t bitmap, last_bitmap;
 +	unsigned int port;
 +	int size;
 +	u8 b;
  
 -	/* Control */
 -	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
 -	vmx->hv_deadline_tsc = -1;
 +	if (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))
 +		return nested_cpu_has(vmcs12, CPU_BASED_UNCOND_IO_EXITING);
  
 -	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL, vmx_exec_control(vmx));
 +	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
  
 -	if (cpu_has_secondary_exec_ctrls()) {
 -		vmx_compute_secondary_exec_control(vmx);
 -		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
 -			     vmx->secondary_exec_control);
 -	}
 +	port = exit_qualification >> 16;
 +	size = (exit_qualification & 7) + 1;
  
 -	if (kvm_vcpu_apicv_active(&vmx->vcpu)) {
 -		vmcs_write64(EOI_EXIT_BITMAP0, 0);
 -		vmcs_write64(EOI_EXIT_BITMAP1, 0);
 -		vmcs_write64(EOI_EXIT_BITMAP2, 0);
 -		vmcs_write64(EOI_EXIT_BITMAP3, 0);
 +	last_bitmap = (gpa_t)-1;
 +	b = -1;
  
 -		vmcs_write16(GUEST_INTR_STATUS, 0);
 +	while (size > 0) {
 +		if (port < 0x8000)
 +			bitmap = vmcs12->io_bitmap_a;
 +		else if (port < 0x10000)
 +			bitmap = vmcs12->io_bitmap_b;
 +		else
 +			return true;
 +		bitmap += (port & 0x7fff) / 8;
  
 -		vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
 -		vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
 -	}
 +		if (last_bitmap != bitmap)
 +			if (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))
 +				return true;
 +		if (b & (1 << (port & 7)))
 +			return true;
  
 -	if (!kvm_pause_in_guest(vmx->vcpu.kvm)) {
 -		vmcs_write32(PLE_GAP, ple_gap);
 -		vmx->ple_window = ple_window;
 -		vmx->ple_window_dirty = true;
 +		port++;
 +		size--;
 +		last_bitmap = bitmap;
  	}
  
 -	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
 -	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
 -	vmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */
 -
 -	vmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */
 -	vmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */
 -	vmx_set_constant_host_state(vmx);
 -	vmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */
 -	vmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */
 +	return false;
 +}
  
 -	if (cpu_has_vmx_vmfunc())
 -		vmcs_write64(VM_FUNCTION_CONTROL, 0);
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle an MSR access access,
 + * rather than handle it ourselves in L0. I.e., check whether L1 expressed
 + * disinterest in the current event (read or write a specific MSR) by using an
 + * MSR bitmap. This may be the case even when L0 doesn't use MSR bitmaps.
 + */
 +static bool nested_vmx_exit_handled_msr(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12, u32 exit_reason)
 +{
 +	u32 msr_index = vcpu->arch.regs[VCPU_REGS_RCX];
 +	gpa_t bitmap;
  
 -	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
 -	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
 -	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
 -	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
 -	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
 +	if (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))
 +		return true;
  
 -	if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)
 -		vmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);
 +	/*
 +	 * The MSR_BITMAP page is divided into four 1024-byte bitmaps,
 +	 * for the four combinations of read/write and low/high MSR numbers.
 +	 * First we need to figure out which of the four to use:
 +	 */
 +	bitmap = vmcs12->msr_bitmap;
 +	if (exit_reason == EXIT_REASON_MSR_WRITE)
 +		bitmap += 2048;
 +	if (msr_index >= 0xc0000000) {
 +		msr_index -= 0xc0000000;
 +		bitmap += 1024;
 +	}
 +
 +	/* Then read the msr_index'th bit from this bitmap: */
 +	if (msr_index < 1024*8) {
 +		unsigned char b;
 +		if (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))
 +			return true;
 +		return 1 & (b >> (msr_index & 7));
 +	} else
 +		return true; /* let L1 handle the wrong parameter */
 +}
  
 -	for (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i) {
 -		u32 index = vmx_msr_index[i];
 -		u32 data_low, data_high;
 -		int j = vmx->nmsrs;
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle a CR access exit,
 + * rather than handle it ourselves in L0. I.e., check if L1 wanted to
 + * intercept (via guest_host_mask etc.) the current event.
 + */
 +static bool nested_vmx_exit_handled_cr(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12)
 +{
 +	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	int cr = exit_qualification & 15;
 +	int reg;
 +	unsigned long val;
  
 -		if (rdmsr_safe(index, &data_low, &data_high) < 0)
 -			continue;
 -		if (wrmsr_safe(index, data_low, data_high) < 0)
 -			continue;
 -		vmx->guest_msrs[j].index = i;
 -		vmx->guest_msrs[j].data = 0;
 -		vmx->guest_msrs[j].mask = -1ull;
 -		++vmx->nmsrs;
 +	switch ((exit_qualification >> 4) & 3) {
 +	case 0: /* mov to cr */
 +		reg = (exit_qualification >> 8) & 15;
 +		val = kvm_register_readl(vcpu, reg);
 +		switch (cr) {
 +		case 0:
 +			if (vmcs12->cr0_guest_host_mask &
 +			    (val ^ vmcs12->cr0_read_shadow))
 +				return true;
 +			break;
 +		case 3:
 +			if ((vmcs12->cr3_target_count >= 1 &&
 +					vmcs12->cr3_target_value0 == val) ||
 +				(vmcs12->cr3_target_count >= 2 &&
 +					vmcs12->cr3_target_value1 == val) ||
 +				(vmcs12->cr3_target_count >= 3 &&
 +					vmcs12->cr3_target_value2 == val) ||
 +				(vmcs12->cr3_target_count >= 4 &&
 +					vmcs12->cr3_target_value3 == val))
 +				return false;
 +			if (nested_cpu_has(vmcs12, CPU_BASED_CR3_LOAD_EXITING))
 +				return true;
 +			break;
 +		case 4:
 +			if (vmcs12->cr4_guest_host_mask &
 +			    (vmcs12->cr4_read_shadow ^ val))
 +				return true;
 +			break;
 +		case 8:
 +			if (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING))
 +				return true;
 +			break;
 +		}
 +		break;
 +	case 2: /* clts */
 +		if ((vmcs12->cr0_guest_host_mask & X86_CR0_TS) &&
 +		    (vmcs12->cr0_read_shadow & X86_CR0_TS))
 +			return true;
 +		break;
 +	case 1: /* mov from cr */
 +		switch (cr) {
 +		case 3:
 +			if (vmcs12->cpu_based_vm_exec_control &
 +			    CPU_BASED_CR3_STORE_EXITING)
 +				return true;
 +			break;
 +		case 8:
 +			if (vmcs12->cpu_based_vm_exec_control &
 +			    CPU_BASED_CR8_STORE_EXITING)
 +				return true;
 +			break;
 +		}
 +		break;
 +	case 3: /* lmsw */
 +		/*
 +		 * lmsw can change bits 1..3 of cr0, and only set bit 0 of
 +		 * cr0. Other attempted changes are ignored, with no exit.
 +		 */
 +		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 +		if (vmcs12->cr0_guest_host_mask & 0xe &
 +		    (val ^ vmcs12->cr0_read_shadow))
 +			return true;
 +		if ((vmcs12->cr0_guest_host_mask & 0x1) &&
 +		    !(vmcs12->cr0_read_shadow & 0x1) &&
 +		    (val & 0x1))
 +			return true;
 +		break;
  	}
 +	return false;
 +}
  
 -	vmx->arch_capabilities = kvm_get_arch_capabilities();
 -
 -	vm_exit_controls_init(vmx, vmx_vmexit_ctrl());
 -
 -	/* 22.2.1, 20.8.1 */
 -	vm_entry_controls_init(vmx, vmx_vmentry_ctrl());
 -
 -	vmx->vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;
 -	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
 +static bool nested_vmx_exit_handled_vmcs_access(struct kvm_vcpu *vcpu,
 +	struct vmcs12 *vmcs12, gpa_t bitmap)
 +{
 +	u32 vmx_instruction_info;
 +	unsigned long field;
 +	u8 b;
  
 -	set_cr4_guest_host_mask(vmx);
 +	if (!nested_cpu_has_shadow_vmcs(vmcs12))
 +		return true;
  
 -	if (vmx_xsaves_supported())
 -		vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);
 +	/* Decode instruction info and find the field to access */
 +	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 +	field = kvm_register_read(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
  
 -	if (enable_pml) {
 -		vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));
 -		vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
 -	}
 +	/* Out-of-range fields always cause a VM exit from L2 to L1 */
 +	if (field >> 15)
 +		return true;
  
 -	if (cpu_has_vmx_encls_vmexit())
 -		vmcs_write64(ENCLS_EXITING_BITMAP, -1ull);
 +	if (kvm_vcpu_read_guest(vcpu, bitmap + field/8, &b, 1))
 +		return true;
  
 -	if (pt_mode == PT_MODE_HOST_GUEST) {
 -		memset(&vmx->pt_desc, 0, sizeof(vmx->pt_desc));
 -		/* Bit[6~0] are forced to 1, writes are ignored. */
 -		vmx->pt_desc.guest.output_mask = 0x7F;
 -		vmcs_write64(GUEST_IA32_RTIT_CTL, 0);
 -	}
 +	return 1 & (b >> (field & 7));
  }
  
 -static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 +/*
 + * Return 1 if we should exit from L2 to L1 to handle an exit, or 0 if we
 + * should handle it ourselves in L0 (and then continue L2). Only call this
 + * when in is_guest_mode (L2).
 + */
 +static bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)
  {
 +	u32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct msr_data apic_base_msr;
 -	u64 cr0;
 -
 -	vmx->rmode.vm86_active = 0;
 -	vmx->spec_ctrl = 0;
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
  
 -	vcpu->arch.microcode_version = 0x100000000ULL;
 -	vmx->vcpu.arch.regs[VCPU_REGS_RDX] = get_rdx_init_val();
 -	kvm_set_cr8(vcpu, 0);
 +	if (vmx->nested.nested_run_pending)
 +		return false;
  
 -	if (!init_event) {
 -		apic_base_msr.data = APIC_DEFAULT_PHYS_BASE |
 -				     MSR_IA32_APICBASE_ENABLE;
 -		if (kvm_vcpu_is_reset_bsp(vcpu))
 -			apic_base_msr.data |= MSR_IA32_APICBASE_BSP;
 -		apic_base_msr.host_initiated = true;
 -		kvm_set_apic_base(vcpu, &apic_base_msr);
 +	if (unlikely(vmx->fail)) {
 +		pr_info_ratelimited("%s failed vm entry %x\n", __func__,
 +				    vmcs_read32(VM_INSTRUCTION_ERROR));
 +		return true;
  	}
  
 -	vmx_segment_cache_clear(vmx);
 -
 -	seg_setup(VCPU_SREG_CS);
 -	vmcs_write16(GUEST_CS_SELECTOR, 0xf000);
 -	vmcs_writel(GUEST_CS_BASE, 0xffff0000ul);
 -
 -	seg_setup(VCPU_SREG_DS);
 -	seg_setup(VCPU_SREG_ES);
 -	seg_setup(VCPU_SREG_FS);
 -	seg_setup(VCPU_SREG_GS);
 -	seg_setup(VCPU_SREG_SS);
 -
 -	vmcs_write16(GUEST_TR_SELECTOR, 0);
 -	vmcs_writel(GUEST_TR_BASE, 0);
 -	vmcs_write32(GUEST_TR_LIMIT, 0xffff);
 -	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);
 -
 -	vmcs_write16(GUEST_LDTR_SELECTOR, 0);
 -	vmcs_writel(GUEST_LDTR_BASE, 0);
 -	vmcs_write32(GUEST_LDTR_LIMIT, 0xffff);
 -	vmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);
 -
 -	if (!init_event) {
 -		vmcs_write32(GUEST_SYSENTER_CS, 0);
 -		vmcs_writel(GUEST_SYSENTER_ESP, 0);
 -		vmcs_writel(GUEST_SYSENTER_EIP, 0);
 -		vmcs_write64(GUEST_IA32_DEBUGCTL, 0);
 +	/*
 +	 * The host physical addresses of some pages of guest memory
 +	 * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC
 +	 * Page). The CPU may write to these pages via their host
 +	 * physical address while L2 is running, bypassing any
 +	 * address-translation-based dirty tracking (e.g. EPT write
 +	 * protection).
 +	 *
 +	 * Mark them dirty on every exit from L2 to prevent them from
 +	 * getting out of sync with dirty tracking.
 +	 */
 +	nested_mark_vmcs12_pages_dirty(vcpu);
 +
 +	trace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,
 +				vmcs_readl(EXIT_QUALIFICATION),
 +				vmx->idt_vectoring_info,
 +				intr_info,
 +				vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 +				KVM_ISA_VMX);
 +
 +	switch (exit_reason) {
 +	case EXIT_REASON_EXCEPTION_NMI:
 +		if (is_nmi(intr_info))
 +			return false;
 +		else if (is_page_fault(intr_info))
 +			return !vmx->vcpu.arch.apf.host_apf_reason && enable_ept;
 +		else if (is_debug(intr_info) &&
 +			 vcpu->guest_debug &
 +			 (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))
 +			return false;
 +		else if (is_breakpoint(intr_info) &&
 +			 vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 +			return false;
 +		return vmcs12->exception_bitmap &
 +				(1u << (intr_info & INTR_INFO_VECTOR_MASK));
 +	case EXIT_REASON_EXTERNAL_INTERRUPT:
 +		return false;
 +	case EXIT_REASON_TRIPLE_FAULT:
 +		return true;
 +	case EXIT_REASON_PENDING_INTERRUPT:
 +		return nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);
 +	case EXIT_REASON_NMI_WINDOW:
 +		return nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);
 +	case EXIT_REASON_TASK_SWITCH:
 +		return true;
 +	case EXIT_REASON_CPUID:
 +		return true;
 +	case EXIT_REASON_HLT:
 +		return nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);
 +	case EXIT_REASON_INVD:
 +		return true;
 +	case EXIT_REASON_INVLPG:
 +		return nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);
 +	case EXIT_REASON_RDPMC:
 +		return nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);
 +	case EXIT_REASON_RDRAND:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDRAND_EXITING);
 +	case EXIT_REASON_RDSEED:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDSEED_EXITING);
 +	case EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:
 +		return nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);
 +	case EXIT_REASON_VMREAD:
 +		return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,
 +			vmcs12->vmread_bitmap);
 +	case EXIT_REASON_VMWRITE:
 +		return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,
 +			vmcs12->vmwrite_bitmap);
 +	case EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:
 +	case EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:
 +	case EXIT_REASON_VMPTRST: case EXIT_REASON_VMRESUME:
 +	case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
 +	case EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:
 +		/*
 +		 * VMX instructions trap unconditionally. This allows L1 to
 +		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
 +		 */
 +		return true;
 +	case EXIT_REASON_CR_ACCESS:
 +		return nested_vmx_exit_handled_cr(vcpu, vmcs12);
 +	case EXIT_REASON_DR_ACCESS:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);
 +	case EXIT_REASON_IO_INSTRUCTION:
 +		return nested_vmx_exit_handled_io(vcpu, vmcs12);
 +	case EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);
 +	case EXIT_REASON_MSR_READ:
 +	case EXIT_REASON_MSR_WRITE:
 +		return nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);
 +	case EXIT_REASON_INVALID_STATE:
 +		return true;
 +	case EXIT_REASON_MWAIT_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);
 +	case EXIT_REASON_MONITOR_TRAP_FLAG:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);
 +	case EXIT_REASON_MONITOR_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);
 +	case EXIT_REASON_PAUSE_INSTRUCTION:
 +		return nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||
 +			nested_cpu_has2(vmcs12,
 +				SECONDARY_EXEC_PAUSE_LOOP_EXITING);
 +	case EXIT_REASON_MCE_DURING_VMENTRY:
 +		return false;
 +	case EXIT_REASON_TPR_BELOW_THRESHOLD:
 +		return nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);
 +	case EXIT_REASON_APIC_ACCESS:
 +	case EXIT_REASON_APIC_WRITE:
 +	case EXIT_REASON_EOI_INDUCED:
 +		/*
 +		 * The controls for "virtualize APIC accesses," "APIC-
 +		 * register virtualization," and "virtual-interrupt
 +		 * delivery" only come from vmcs12.
 +		 */
 +		return true;
 +	case EXIT_REASON_EPT_VIOLATION:
 +		/*
 +		 * L0 always deals with the EPT violation. If nested EPT is
 +		 * used, and the nested mmu code discovers that the address is
 +		 * missing in the guest EPT table (EPT12), the EPT violation
 +		 * will be injected with nested_ept_inject_page_fault()
 +		 */
 +		return false;
 +	case EXIT_REASON_EPT_MISCONFIG:
 +		/*
 +		 * L2 never uses directly L1's EPT, but rather L0's own EPT
 +		 * table (shadow on EPT) or a merged EPT table that L0 built
 +		 * (EPT on EPT). So any problems with the structure of the
 +		 * table is L0's fault.
 +		 */
 +		return false;
 +	case EXIT_REASON_INVPCID:
 +		return
 +			nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_INVPCID) &&
 +			nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);
 +	case EXIT_REASON_WBINVD:
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);
 +	case EXIT_REASON_XSETBV:
 +		return true;
 +	case EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:
 +		/*
 +		 * This should never happen, since it is not possible to
 +		 * set XSS to a non-zero value---neither in L1 nor in L2.
 +		 * If if it were, XSS would have to be checked against
 +		 * the XSS exit bitmap in vmcs12.
 +		 */
 +		return nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);
 +	case EXIT_REASON_PREEMPTION_TIMER:
 +		return false;
 +	case EXIT_REASON_PML_FULL:
 +		/* We emulate PML support to L1. */
 +		return false;
 +	case EXIT_REASON_VMFUNC:
 +		/* VM functions are emulated through L2->L0 vmexits. */
 +		return false;
 +	case EXIT_REASON_ENCLS:
 +		/* SGX is never exposed to L1 */
 +		return false;
 +	default:
 +		return true;
  	}
 +}
  
 -	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
 -	kvm_rip_write(vcpu, 0xfff0);
 -
 -	vmcs_writel(GUEST_GDTR_BASE, 0);
 -	vmcs_write32(GUEST_GDTR_LIMIT, 0xffff);
 -
 -	vmcs_writel(GUEST_IDTR_BASE, 0);
 -	vmcs_write32(GUEST_IDTR_LIMIT, 0xffff);
 +static int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)
 +{
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
  
 -	vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);
 -	vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);
 -	vmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);
 -	if (kvm_mpx_supported())
 -		vmcs_write64(GUEST_BNDCFGS, 0);
 +	/*
 +	 * At this point, the exit interruption info in exit_intr_info
 +	 * is only valid for EXCEPTION_NMI exits.  For EXTERNAL_INTERRUPT
 +	 * we need to query the in-kernel LAPIC.
 +	 */
 +	WARN_ON(exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT);
 +	if ((exit_intr_info &
 +	     (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) ==
 +	    (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) {
 +		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 +		vmcs12->vm_exit_intr_error_code =
 +			vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
 +	}
  
 -	setup_msrs(vmx);
 +	nested_vmx_vmexit(vcpu, exit_reason, exit_intr_info,
 +			  vmcs_readl(EXIT_QUALIFICATION));
 +	return 1;
 +}
  
 -	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */
 +static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +{
 +	*info1 = vmcs_readl(EXIT_QUALIFICATION);
 +	*info2 = vmcs_read32(VM_EXIT_INTR_INFO);
 +}
  
 -	if (cpu_has_vmx_tpr_shadow() && !init_event) {
 -		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
 -		if (cpu_need_tpr_shadow(vcpu))
 -			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
 -				     __pa(vcpu->arch.apic->regs));
 -		vmcs_write32(TPR_THRESHOLD, 0);
 +static void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)
 +{
 +	if (vmx->pml_pg) {
 +		__free_page(vmx->pml_pg);
 +		vmx->pml_pg = NULL;
  	}
 +}
  
 -	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 +static void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 *pml_buf;
 +	u16 pml_idx;
  
 -	if (vmx->vpid != 0)
 -		vmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);
 +	pml_idx = vmcs_read16(GUEST_PML_INDEX);
  
 -	cr0 = X86_CR0_NW | X86_CR0_CD | X86_CR0_ET;
 -	vmx->vcpu.arch.cr0 = cr0;
 -	vmx_set_cr0(vcpu, cr0); /* enter rmode */
 -	vmx_set_cr4(vcpu, 0);
 -	vmx_set_efer(vcpu, 0);
 +	/* Do nothing if PML buffer is empty */
 +	if (pml_idx == (PML_ENTITY_NUM - 1))
 +		return;
  
 -	update_exception_bitmap(vcpu);
 +	/* PML index always points to next available PML buffer entity */
 +	if (pml_idx >= PML_ENTITY_NUM)
 +		pml_idx = 0;
 +	else
 +		pml_idx++;
  
 -	vpid_sync_context(vmx->vpid);
 -	if (init_event)
 -		vmx_clear_hlt(vcpu);
 +	pml_buf = page_address(vmx->pml_pg);
 +	for (; pml_idx < PML_ENTITY_NUM; pml_idx++) {
 +		u64 gpa;
 +
 +		gpa = pml_buf[pml_idx];
 +		WARN_ON(gpa & (PAGE_SIZE - 1));
 +		kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
 +	}
 +
 +	/* reset PML index */
 +	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
  }
  
 -static void enable_irq_window(struct kvm_vcpu *vcpu)
 +/*
 + * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.
 + * Called before reporting dirty_bitmap to userspace.
 + */
 +static void kvm_flush_pml_buffers(struct kvm *kvm)
  {
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 -		      CPU_BASED_VIRTUAL_INTR_PENDING);
 +	int i;
 +	struct kvm_vcpu *vcpu;
 +	/*
 +	 * We only need to kick vcpu out of guest mode here, as PML buffer
 +	 * is flushed at beginning of all VMEXITs, and it's obvious that only
 +	 * vcpus running in guest are possible to have unflushed GPAs in PML
 +	 * buffer.
 +	 */
 +	kvm_for_each_vcpu(i, vcpu, kvm)
 +		kvm_vcpu_kick(vcpu);
  }
  
 -static void enable_nmi_window(struct kvm_vcpu *vcpu)
 +static void vmx_dump_sel(char *name, uint32_t sel)
  {
 -	if (!enable_vnmi ||
 -	    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {
 -		enable_irq_window(vcpu);
 -		return;
 -	}
 +	pr_err("%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
 +	       name, vmcs_read16(sel),
 +	       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),
 +	       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),
 +	       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));
 +}
  
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 -		      CPU_BASED_VIRTUAL_NMI_PENDING);
 +static void vmx_dump_dtsel(char *name, uint32_t limit)
 +{
 +	pr_err("%s                           limit=0x%08x, base=0x%016lx\n",
 +	       name, vmcs_read32(limit),
 +	       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
  }
  
 -static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 +static void dump_vmcs(void)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	uint32_t intr;
 -	int irq = vcpu->arch.interrupt.nr;
 +	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
 +	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
 +	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 +	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
 +	u32 secondary_exec_control = 0;
 +	unsigned long cr4 = vmcs_readl(GUEST_CR4);
 +	u64 efer = vmcs_read64(GUEST_IA32_EFER);
 +	int i, n;
  
 -	trace_kvm_inj_virq(irq);
 +	if (cpu_has_secondary_exec_ctrls())
 +		secondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
  
 -	++vcpu->stat.irq_injections;
 -	if (vmx->rmode.vm86_active) {
 -		int inc_eip = 0;
 -		if (vcpu->arch.interrupt.soft)
 -			inc_eip = vcpu->arch.event_exit_inst_len;
 -		if (kvm_inject_realmode_interrupt(vcpu, irq, inc_eip) != EMULATE_DONE)
 -			kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 -		return;
 +	pr_err("*** Guest State ***\n");
 +	pr_err("CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
 +	       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),
 +	       vmcs_readl(CR0_GUEST_HOST_MASK));
 +	pr_err("CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
 +	       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));
 +	pr_err("CR3 = 0x%016lx\n", vmcs_readl(GUEST_CR3));
 +	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&
 +	    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))
 +	{
 +		pr_err("PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\n",
 +		       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));
 +		pr_err("PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\n",
 +		       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));
  	}
 -	intr = irq | INTR_INFO_VALID_MASK;
 -	if (vcpu->arch.interrupt.soft) {
 -		intr |= INTR_TYPE_SOFT_INTR;
 -		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,
 -			     vmx->vcpu.arch.event_exit_inst_len);
 -	} else
 -		intr |= INTR_TYPE_EXT_INTR;
 -	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
 +	pr_err("RSP = 0x%016lx  RIP = 0x%016lx\n",
 +	       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
 +	pr_err("RFLAGS=0x%08lx         DR7 = 0x%016lx\n",
 +	       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));
 +	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
 +	       vmcs_readl(GUEST_SYSENTER_ESP),
 +	       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));
 +	vmx_dump_sel("CS:  ", GUEST_CS_SELECTOR);
 +	vmx_dump_sel("DS:  ", GUEST_DS_SELECTOR);
 +	vmx_dump_sel("SS:  ", GUEST_SS_SELECTOR);
 +	vmx_dump_sel("ES:  ", GUEST_ES_SELECTOR);
 +	vmx_dump_sel("FS:  ", GUEST_FS_SELECTOR);
 +	vmx_dump_sel("GS:  ", GUEST_GS_SELECTOR);
 +	vmx_dump_dtsel("GDTR:", GUEST_GDTR_LIMIT);
 +	vmx_dump_sel("LDTR:", GUEST_LDTR_SELECTOR);
 +	vmx_dump_dtsel("IDTR:", GUEST_IDTR_LIMIT);
 +	vmx_dump_sel("TR:  ", GUEST_TR_SELECTOR);
 +	if ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||
 +	    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))
 +		pr_err("EFER =     0x%016llx  PAT = 0x%016llx\n",
 +		       efer, vmcs_read64(GUEST_IA32_PAT));
 +	pr_err("DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\n",
 +	       vmcs_read64(GUEST_IA32_DEBUGCTL),
 +	       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
 +	if (cpu_has_load_perf_global_ctrl() &&
 +	    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)
 +		pr_err("PerfGlobCtl = 0x%016llx\n",
 +		       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));
 +	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)
 +		pr_err("BndCfgS = 0x%016llx\n", vmcs_read64(GUEST_BNDCFGS));
 +	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
 +	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
 +	       vmcs_read32(GUEST_ACTIVITY_STATE));
 +	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 +		pr_err("InterruptStatus = %04x\n",
 +		       vmcs_read16(GUEST_INTR_STATUS));
  
 -	vmx_clear_hlt(vcpu);
 +	pr_err("*** Host State ***\n");
 +	pr_err("RIP = 0x%016lx  RSP = 0x%016lx\n",
 +	       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));
 +	pr_err("CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\n",
 +	       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),
 +	       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),
 +	       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),
 +	       vmcs_read16(HOST_TR_SELECTOR));
 +	pr_err("FSBase=%016lx GSBase=%016lx TRBase=%016lx\n",
 +	       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),
 +	       vmcs_readl(HOST_TR_BASE));
 +	pr_err("GDTBase=%016lx IDTBase=%016lx\n",
 +	       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));
 +	pr_err("CR0=%016lx CR3=%016lx CR4=%016lx\n",
 +	       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),
 +	       vmcs_readl(HOST_CR4));
 +	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
 +	       vmcs_readl(HOST_IA32_SYSENTER_ESP),
 +	       vmcs_read32(HOST_IA32_SYSENTER_CS),
 +	       vmcs_readl(HOST_IA32_SYSENTER_EIP));
 +	if (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))
 +		pr_err("EFER = 0x%016llx  PAT = 0x%016llx\n",
 +		       vmcs_read64(HOST_IA32_EFER),
 +		       vmcs_read64(HOST_IA32_PAT));
 +	if (cpu_has_load_perf_global_ctrl() &&
 +	    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)
 +		pr_err("PerfGlobCtl = 0x%016llx\n",
 +		       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));
 +
 +	pr_err("*** Control State ***\n");
 +	pr_err("PinBased=%08x CPUBased=%08x SecondaryExec=%08x\n",
 +	       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);
 +	pr_err("EntryControls=%08x ExitControls=%08x\n", vmentry_ctl, vmexit_ctl);
 +	pr_err("ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\n",
 +	       vmcs_read32(EXCEPTION_BITMAP),
 +	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
 +	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
 +	pr_err("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",
 +	       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +	       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
 +	       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
 +	pr_err("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",
 +	       vmcs_read32(VM_EXIT_INTR_INFO),
 +	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 +	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
 +	pr_err("        reason=%08x qualification=%016lx\n",
 +	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
 +	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
 +	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
 +	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
 +	pr_err("TSC Offset = 0x%016llx\n", vmcs_read64(TSC_OFFSET));
 +	if (secondary_exec_control & SECONDARY_EXEC_TSC_SCALING)
 +		pr_err("TSC Multiplier = 0x%016llx\n",
 +		       vmcs_read64(TSC_MULTIPLIER));
 +	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)
 +		pr_err("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
 +	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
 +		pr_err("PostedIntrVec = 0x%02x\n", vmcs_read16(POSTED_INTR_NV));
 +	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))
 +		pr_err("EPT pointer = 0x%016llx\n", vmcs_read64(EPT_POINTER));
 +	n = vmcs_read32(CR3_TARGET_COUNT);
 +	for (i = 0; i + 1 < n; i += 4)
 +		pr_err("CR3 target%u=%016lx target%u=%016lx\n",
 +		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),
 +		       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));
 +	if (i < n)
 +		pr_err("CR3 target%u=%016lx\n",
 +		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));
 +	if (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)
 +		pr_err("PLE Gap=%08x Window=%08x\n",
 +		       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));
 +	if (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)
 +		pr_err("Virtual processor ID = 0x%04x\n",
 +		       vmcs_read16(VIRTUAL_PROCESSOR_ID));
  }
  
 -static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 +/*
 + * The guest has exited.  See if we can fix it or if we need userspace
 + * assistance.
 + */
 +static int vmx_handle_exit(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u32 exit_reason = vmx->exit_reason;
 +	u32 vectoring_info = vmx->idt_vectoring_info;
  
 -	if (!enable_vnmi) {
 -		/*
 -		 * Tracking the NMI-blocked state in software is built upon
 -		 * finding the next open IRQ window. This, in turn, depends on
 -		 * well-behaving guests: They have to keep IRQs disabled at
 -		 * least as long as the NMI handler runs. Otherwise we may
 -		 * cause NMI nesting, maybe breaking the guest. But as this is
 -		 * highly unlikely, we can live with the residual risk.
 -		 */
 -		vmx->loaded_vmcs->soft_vnmi_blocked = 1;
 -		vmx->loaded_vmcs->vnmi_blocked_time = 0;
 -	}
 -
 -	++vcpu->stat.nmi_injections;
 -	vmx->loaded_vmcs->nmi_known_unmasked = false;
 -
 -	if (vmx->rmode.vm86_active) {
 -		if (kvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0) != EMULATE_DONE)
 -			kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);
 -		return;
 -	}
 -
 -	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 -			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 -
 -	vmx_clear_hlt(vcpu);
 -}
 +	trace_kvm_exit(exit_reason, vcpu, KVM_ISA_VMX);
  
 -bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	bool masked;
 +	/*
 +	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more
 +	 * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before
 +	 * querying dirty_bitmap, we only need to kick all vcpus out of guest
 +	 * mode as if vcpus is in root mode, the PML buffer must has been
 +	 * flushed already.
 +	 */
 +	if (enable_pml)
 +		vmx_flush_pml_buffer(vcpu);
  
 -	if (!enable_vnmi)
 -		return vmx->loaded_vmcs->soft_vnmi_blocked;
 -	if (vmx->loaded_vmcs->nmi_known_unmasked)
 -		return false;
 -	masked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;
 -	vmx->loaded_vmcs->nmi_known_unmasked = !masked;
 -	return masked;
 -}
 +	/* If guest state is invalid, start emulating */
 +	if (vmx->emulation_required)
 +		return handle_invalid_guest_state(vcpu);
  
 -void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	if (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason))
 +		return nested_vmx_reflect_vmexit(vcpu, exit_reason);
  
 -	if (!enable_vnmi) {
 -		if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {
 -			vmx->loaded_vmcs->soft_vnmi_blocked = masked;
 -			vmx->loaded_vmcs->vnmi_blocked_time = 0;
 -		}
 -	} else {
 -		vmx->loaded_vmcs->nmi_known_unmasked = !masked;
 -		if (masked)
 -			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 -				      GUEST_INTR_STATE_NMI);
 -		else
 -			vmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,
 -					GUEST_INTR_STATE_NMI);
 +	if (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
 +		dump_vmcs();
 +		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		vcpu->run->fail_entry.hardware_entry_failure_reason
 +			= exit_reason;
 +		return 0;
  	}
 -}
  
 -static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
 -{
 -	if (to_vmx(vcpu)->nested.nested_run_pending)
 +	if (unlikely(vmx->fail)) {
 +		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		vcpu->run->fail_entry.hardware_entry_failure_reason
 +			= vmcs_read32(VM_INSTRUCTION_ERROR);
  		return 0;
 +	}
  
 -	if (!enable_vnmi &&
 -	    to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)
 +	/*
 +	 * Note:
 +	 * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by
 +	 * delivery event since it indicates guest is accessing MMIO.
 +	 * The vm-exit can be triggered again after return to guest that
 +	 * will cause infinite loop.
 +	 */
 +	if ((vectoring_info & VECTORING_INFO_VALID_MASK) &&
 +			(exit_reason != EXIT_REASON_EXCEPTION_NMI &&
 +			exit_reason != EXIT_REASON_EPT_VIOLATION &&
 +			exit_reason != EXIT_REASON_PML_FULL &&
 +			exit_reason != EXIT_REASON_TASK_SWITCH)) {
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;
 +		vcpu->run->internal.ndata = 3;
 +		vcpu->run->internal.data[0] = vectoring_info;
 +		vcpu->run->internal.data[1] = exit_reason;
 +		vcpu->run->internal.data[2] = vcpu->arch.exit_qualification;
 +		if (exit_reason == EXIT_REASON_EPT_MISCONFIG) {
 +			vcpu->run->internal.ndata++;
 +			vcpu->run->internal.data[3] =
 +				vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 +		}
  		return 0;
 +	}
  
 -	return	!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &
 -		  (GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI
 -		   | GUEST_INTR_STATE_NMI));
 -}
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked)) {
 +		if (vmx_interrupt_allowed(vcpu)) {
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
 +			   vcpu->arch.nmi_pending) {
 +			/*
 +			 * This CPU don't support us in finding the end of an
 +			 * NMI-blocked window if the guest runs with IRQs
 +			 * disabled. So we pull the trigger after 1 s of
 +			 * futile waiting, but inform the user about this.
 +			 */
 +			printk(KERN_WARNING "%s: Breaking out of NMI-blocked "
 +			       "state on VCPU %d after 1 s timeout\n",
 +			       __func__, vcpu->vcpu_id);
 +			vmx->loaded_vmcs->soft_vnmi_blocked = 0;
 +		}
 +	}
  
 -static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 -{
 -	return (!to_vmx(vcpu)->nested.nested_run_pending &&
 -		vmcs_readl(GUEST_RFLAGS) & X86_EFLAGS_IF) &&
 -		!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &
 -			(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
 +	if (exit_reason < kvm_vmx_max_exit_handlers
 +	    && kvm_vmx_exit_handlers[exit_reason])
 +		return kvm_vmx_exit_handlers[exit_reason](vcpu);
 +	else {
 +		vcpu_unimpl(vcpu, "vmx: unexpected exit reason 0x%x\n",
 +				exit_reason);
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
  }
  
 -static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +/*
 + * Software based L1D cache flush which is used when microcode providing
 + * the cache control MSR is not loaded.
 + *
 + * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
 + * flush it is required to read in 64 KiB because the replacement algorithm
 + * is not exactly LRU. This could be sized at runtime via topology
 + * information but as all relevant affected CPUs have 32KiB L1D cache size
 + * there is no point in doing so.
 + */
 +static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
  {
 -	int ret;
 -
 -	if (enable_unrestricted_guest)
 -		return 0;
 +	int size = PAGE_SIZE << L1D_CACHE_ORDER;
  
 -	ret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
 -				    PAGE_SIZE * 3);
 -	if (ret)
 -		return ret;
 -	to_kvm_vmx(kvm)->tss_addr = addr;
 -	return init_rmode_tss(kvm);
 -}
 +	/*
 +	 * This code is only executed when the the flush mode is 'cond' or
 +	 * 'always'
 +	 */
 +	if (static_branch_likely(&vmx_l1d_flush_cond)) {
 +		bool flush_l1d;
  
 -static int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 -{
 -	to_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;
 -	return 0;
 -}
 +		/*
 +		 * Clear the per-vcpu flush bit, it gets set again
 +		 * either from vcpu_run() or from one of the unsafe
 +		 * VMEXIT handlers.
 +		 */
 +		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 +		vcpu->arch.l1tf_flush_l1d = false;
  
 -static bool rmode_exception(struct kvm_vcpu *vcpu, int vec)
 -{
 -	switch (vec) {
 -	case BP_VECTOR:
  		/*
 -		 * Update instruction length as we may reinject the exception
 -		 * from user space while in guest debugging mode.
 +		 * Clear the per-cpu flush bit, it gets set again from
 +		 * the interrupt handlers.
  		 */
 -		to_vmx(vcpu)->vcpu.arch.event_exit_inst_len =
 -			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 -		if (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 -			return false;
 -		/* fall through */
 -	case DB_VECTOR:
 -		if (vcpu->guest_debug &
 -			(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))
 -			return false;
 -		/* fall through */
 -	case DE_VECTOR:
 -	case OF_VECTOR:
 -	case BR_VECTOR:
 -	case UD_VECTOR:
 -	case DF_VECTOR:
 -	case SS_VECTOR:
 -	case GP_VECTOR:
 -	case MF_VECTOR:
 -		return true;
 -	break;
 -	}
 -	return false;
 -}
 +		flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
 +		kvm_clear_cpu_l1tf_flush_l1d();
  
 -static int handle_rmode_exception(struct kvm_vcpu *vcpu,
 -				  int vec, u32 err_code)
 -{
 -	/*
 -	 * Instruction with address size override prefix opcode 0x67
 -	 * Cause the #SS fault with 0 error code in VM86 mode.
 -	 */
 -	if (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {
 -		if (kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE) {
 -			if (vcpu->arch.halt_request) {
 -				vcpu->arch.halt_request = 0;
 -				return kvm_vcpu_halt(vcpu);
 -			}
 -			return 1;
 -		}
 -		return 0;
 +		if (!flush_l1d)
 +			return;
  	}
  
 -	/*
 -	 * Forward all other exceptions that are valid in real mode.
 -	 * FIXME: Breaks guest debugging in real mode, needs to be fixed with
 -	 *        the required debugging infrastructure rework.
 -	 */
 -	kvm_queue_exception(vcpu, vec);
 -	return 1;
 -}
 -
 -/*
 - * Trigger machine check on the host. We assume all the MSRs are already set up
 - * by the CPU and that we still run on the same CPU as the MCE occurred on.
 - * We pass a fake environment to the machine check handler because we want
 - * the guest to be always treated like user space, no matter what context
 - * it used internally.
 - */
 -static void kvm_machine_check(void)
 -{
 -#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_64)
 -	struct pt_regs regs = {
 -		.cs = 3, /* Fake ring 3 no matter what the guest ran on */
 -		.flags = X86_EFLAGS_IF,
 -	};
 +	vcpu->stat.l1d_flush++;
  
 -	do_machine_check(&regs, 0);
 -#endif
 -}
 +	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 +		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 +		return;
 +	}
  
 -static int handle_machine_check(struct kvm_vcpu *vcpu)
 -{
 -	/* already handled by vcpu_run */
 -	return 1;
 +	asm volatile(
 +		/* First ensure the pages are in the TLB */
 +		"xorl	%%eax, %%eax\n"
 +		".Lpopulate_tlb:\n\t"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$4096, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lpopulate_tlb\n\t"
 +		"xorl	%%eax, %%eax\n\t"
 +		"cpuid\n\t"
 +		/* Now fill the cache */
 +		"xorl	%%eax, %%eax\n"
 +		".Lfill_cache:\n"
 +		"movzbl	(%[flush_pages], %%" _ASM_AX "), %%ecx\n\t"
 +		"addl	$64, %%eax\n\t"
 +		"cmpl	%%eax, %[size]\n\t"
 +		"jne	.Lfill_cache\n\t"
 +		"lfence\n"
 +		:: [flush_pages] "r" (vmx_l1d_flush_pages),
 +		    [size] "r" (size)
 +		: "eax", "ebx", "ecx", "edx");
  }
  
 -static int handle_exception(struct kvm_vcpu *vcpu)
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	struct kvm_run *kvm_run = vcpu->run;
 -	u32 intr_info, ex_no, error_code;
 -	unsigned long cr2, rip, dr6;
 -	u32 vect_info;
 -	enum emulation_result er;
 -
 -	vect_info = vmx->idt_vectoring_info;
 -	intr_info = vmx->exit_intr_info;
 +	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
  
 -	if (is_machine_check(intr_info))
 -		return handle_machine_check(vcpu);
 +	if (is_guest_mode(vcpu) &&
 +		nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
 +		return;
  
 -	if (is_nmi(intr_info))
 -		return 1;  /* already handled by vmx_vcpu_run() */
 +	if (irr == -1 || tpr < irr) {
 +		vmcs_write32(TPR_THRESHOLD, 0);
 +		return;
 +	}
  
 -	if (is_invalid_opcode(intr_info))
 -		return handle_ud(vcpu);
 +	vmcs_write32(TPR_THRESHOLD, irr);
 +}
  
 -	error_code = 0;
 -	if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
 -		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
 +static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	u32 sec_exec_control;
  
 -	if (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {
 -		WARN_ON_ONCE(!enable_vmware_backdoor);
 -		er = kvm_emulate_instruction(vcpu,
 -			EMULTYPE_VMWARE | EMULTYPE_NO_UD_ON_FAIL);
 -		if (er == EMULATE_USER_EXIT)
 -			return 0;
 -		else if (er != EMULATE_DONE)
 -			kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 -		return 1;
 -	}
 +	if (!lapic_in_kernel(vcpu))
 +		return;
  
 -	/*
 -	 * The #PF with PFEC.RSVD = 1 indicates the guest is accessing
 -	 * MMIO, it is better to report an internal error.
 -	 * See the comments in vmx_handle_exit.
 -	 */
 -	if ((vect_info & VECTORING_INFO_VALID_MASK) &&
 -	    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;
 -		vcpu->run->internal.ndata = 3;
 -		vcpu->run->internal.data[0] = vect_info;
 -		vcpu->run->internal.data[1] = intr_info;
 -		vcpu->run->internal.data[2] = error_code;
 -		return 0;
 -	}
 +	if (!flexpriority_enabled &&
 +	    !cpu_has_vmx_virtualize_x2apic_mode())
 +		return;
  
 -	if (is_page_fault(intr_info)) {
 -		cr2 = vmcs_readl(EXIT_QUALIFICATION);
 -		/* EPT won't cause page fault directly */
 -		WARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);
 -		return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
 +	/* Postpone execution until vmcs01 is the current VMCS. */
 +	if (is_guest_mode(vcpu)) {
 +		to_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;
 +		return;
  	}
  
 -	ex_no = intr_info & INTR_INFO_VECTOR_MASK;
 -
 -	if (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))
 -		return handle_rmode_exception(vcpu, ex_no, error_code);
 -
 -	switch (ex_no) {
 -	case AC_VECTOR:
 -		kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 -		return 1;
 -	case DB_VECTOR:
 -		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 -		if (!(vcpu->guest_debug &
 -		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= dr6 | DR6_RTM;
 -			if (is_icebp(intr_info))
 -				skip_emulated_instruction(vcpu);
 +	sec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
 +	sec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
 +			      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
  
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 +	switch (kvm_get_apic_mode(vcpu)) {
 +	case LAPIC_MODE_INVALID:
 +		WARN_ONCE(true, "Invalid local APIC state");
 +	case LAPIC_MODE_DISABLED:
 +		break;
 +	case LAPIC_MODE_XAPIC:
 +		if (flexpriority_enabled) {
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
 +			vmx_flush_tlb(vcpu, true);
  		}
 -		kvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;
 -		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 -		/* fall through */
 -	case BP_VECTOR:
 -		/*
 -		 * Update instruction length as we may reinject #BP from
 -		 * user space while in guest debugging mode. Reading it for
 -		 * #DB as well causes no harm, it is not used in that case.
 -		 */
 -		vmx->vcpu.arch.event_exit_inst_len =
 -			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 -		kvm_run->exit_reason = KVM_EXIT_DEBUG;
 -		rip = kvm_rip_read(vcpu);
 -		kvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;
 -		kvm_run->debug.arch.exception = ex_no;
  		break;
 -	default:
 -		kvm_run->exit_reason = KVM_EXIT_EXCEPTION;
 -		kvm_run->ex.exception = ex_no;
 -		kvm_run->ex.error_code = error_code;
 +	case LAPIC_MODE_X2APIC:
 +		if (cpu_has_vmx_virtualize_x2apic_mode())
 +			sec_exec_control |=
 +				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
  		break;
  	}
 -	return 0;
 -}
 +	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);
  
 -static int handle_external_interrupt(struct kvm_vcpu *vcpu)
 -{
 -	++vcpu->stat.irq_exits;
 -	return 1;
 +	vmx_update_msr_bitmap(vcpu);
  }
  
 -static int handle_triple_fault(struct kvm_vcpu *vcpu)
 +static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
  {
 -	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
 -	vcpu->mmio_needed = 0;
 -	return 0;
 +	if (!is_guest_mode(vcpu)) {
 +		vmcs_write64(APIC_ACCESS_ADDR, hpa);
 +		vmx_flush_tlb(vcpu, true);
 +	}
  }
  
 -static int handle_io(struct kvm_vcpu *vcpu)
 +static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
  {
 -	unsigned long exit_qualification;
 -	int size, in, string;
 -	unsigned port;
 +	u16 status;
 +	u8 old;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	string = (exit_qualification & 16) != 0;
 +	if (max_isr == -1)
 +		max_isr = 0;
  
 -	++vcpu->stat.io_exits;
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = status >> 8;
 +	if (max_isr != old) {
 +		status &= 0xff;
 +		status |= max_isr << 8;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
 +}
  
 -	if (string)
 -		return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +static void vmx_set_rvi(int vector)
 +{
 +	u16 status;
 +	u8 old;
  
 -	port = exit_qualification >> 16;
 -	size = (exit_qualification & 7) + 1;
 -	in = (exit_qualification & 8) != 0;
 +	if (vector == -1)
 +		vector = 0;
  
 -	return kvm_fast_pio(vcpu, size, port, in);
 +	status = vmcs_read16(GUEST_INTR_STATUS);
 +	old = (u8)status & 0xff;
 +	if ((u8)vector != old) {
 +		status &= ~0xff;
 +		status |= (u8)vector;
 +		vmcs_write16(GUEST_INTR_STATUS, status);
 +	}
  }
  
 -static void
 -vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
  {
  	/*
 -	 * Patch in the VMCALL instruction:
 +	 * When running L2, updating RVI is only relevant when
 +	 * vmcs12 virtual-interrupt-delivery enabled.
 +	 * However, it can be enabled only when L1 also
 +	 * intercepts external-interrupts and in that case
 +	 * we should not update vmcs02 RVI but instead intercept
 +	 * interrupt. Therefore, do nothing when running L2.
  	 */
 -	hypercall[0] = 0x0f;
 -	hypercall[1] = 0x01;
 -	hypercall[2] = 0xc1;
 +	if (!is_guest_mode(vcpu))
 +		vmx_set_rvi(max_irr);
  }
  
 -/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
 -static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
 +static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
  {
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	int max_irr;
 +	bool max_irr_updated;
  
 +	WARN_ON(!vcpu->arch.apicv_active);
 +	if (pi_test_on(&vmx->pi_desc)) {
 +		pi_clear_on(&vmx->pi_desc);
  		/*
 -		 * We get here when L2 changed cr0 in a way that did not change
 -		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),
 -		 * but did change L0 shadowed bits. So we first calculate the
 -		 * effective cr0 value that L1 would like to write into the
 -		 * hardware. It consists of the L2-owned bits from the new
 -		 * value combined with the L1-owned bits from L1's guest_cr0.
 +		 * IOMMU can write to PIR.ON, so the barrier matters even on UP.
 +		 * But on x86 this is just a compiler barrier anyway.
  		 */
 -		val = (val & ~vmcs12->cr0_guest_host_mask) |
 -			(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);
 -
 -		if (!nested_guest_cr0_valid(vcpu, val))
 -			return 1;
 +		smp_mb__after_atomic();
 +		max_irr_updated =
 +			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
  
 -		if (kvm_set_cr0(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR0_READ_SHADOW, orig_val);
 -		return 0;
 +		/*
 +		 * If we are running L2 and L1 has a new pending interrupt
 +		 * which can be injected, we should re-evaluate
 +		 * what should be done with this new L1 interrupt.
 +		 * If L1 intercepts external-interrupts, we should
 +		 * exit from L2 to L1. Otherwise, interrupt should be
 +		 * delivered directly to L2.
 +		 */
 +		if (is_guest_mode(vcpu) && max_irr_updated) {
 +			if (nested_exit_on_intr(vcpu))
 +				kvm_vcpu_exiting_guest_mode(vcpu);
 +			else
 +				kvm_make_request(KVM_REQ_EVENT, vcpu);
 +		}
  	} else {
 -		if (to_vmx(vcpu)->nested.vmxon &&
 -		    !nested_host_cr0_valid(vcpu, val))
 -			return 1;
 -
 -		return kvm_set_cr0(vcpu, val);
 +		max_irr = kvm_lapic_find_highest_irr(vcpu);
  	}
 +	vmx_hwapic_irr_update(vcpu, max_irr);
 +	return max_irr;
  }
  
 -static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 +static u8 vmx_has_apicv_interrupt(struct kvm_vcpu *vcpu)
  {
 -	if (is_guest_mode(vcpu)) {
 -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 -		unsigned long orig_val = val;
 +	u8 rvi = vmx_get_rvi();
 +	u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
  
 -		/* analogously to handle_set_cr0 */
 -		val = (val & ~vmcs12->cr4_guest_host_mask) |
 -			(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);
 -		if (kvm_set_cr4(vcpu, val))
 -			return 1;
 -		vmcs_writel(CR4_READ_SHADOW, orig_val);
 -		return 0;
 -	} else
 -		return kvm_set_cr4(vcpu, val);
 +	return ((rvi & 0xf0) > (vppr & 0xf0));
  }
  
 -static int handle_desc(struct kvm_vcpu *vcpu)
 +static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
  {
 -	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);
 +	vmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);
 +	vmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);
 +	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
  }
  
 -static int handle_cr(struct kvm_vcpu *vcpu)
 +static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
  {
 -	unsigned long exit_qualification, val;
 -	int cr;
 -	int reg;
 -	int err;
 -	int ret;
 -
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	cr = exit_qualification & 15;
 -	reg = (exit_qualification >> 8) & 15;
 -	switch ((exit_qualification >> 4) & 3) {
 -	case 0: /* mov to cr */
 -		val = kvm_register_readl(vcpu, reg);
 -		trace_kvm_cr_write(cr, val);
 -		switch (cr) {
 -		case 0:
 -			err = handle_set_cr0(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			err = kvm_set_cr3(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 4:
 -			err = handle_set_cr4(vcpu, val);
 -			return kvm_complete_insn_gp(vcpu, err);
 -		case 8: {
 -				u8 cr8_prev = kvm_get_cr8(vcpu);
 -				u8 cr8 = (u8)val;
 -				err = kvm_set_cr8(vcpu, cr8);
 -				ret = kvm_complete_insn_gp(vcpu, err);
 -				if (lapic_in_kernel(vcpu))
 -					return ret;
 -				if (cr8_prev <= cr8)
 -					return ret;
 -				/*
 -				 * TODO: we might be squashing a
 -				 * KVM_GUESTDBG_SINGLESTEP-triggered
 -				 * KVM_EXIT_DEBUG here.
 -				 */
 -				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;
 -				return 0;
 -			}
 -		}
 -		break;
 -	case 2: /* clts */
 -		WARN_ONCE(1, "Guest should always own CR0.TS");
 -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));
 -		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 -		return kvm_skip_emulated_instruction(vcpu);
 -	case 1: /*mov from cr*/
 -		switch (cr) {
 -		case 3:
 -			WARN_ON_ONCE(enable_unrestricted_guest);
 -			val = kvm_read_cr3(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		case 8:
 -			val = kvm_get_cr8(vcpu);
 -			kvm_register_write(vcpu, reg, val);
 -			trace_kvm_cr_read(cr, val);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -		break;
 -	case 3: /* lmsw */
 -		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;
 -		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);
 -		kvm_lmsw(vcpu, val);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
  
 -		return kvm_skip_emulated_instruction(vcpu);
 -	default:
 -		break;
 -	}
 -	vcpu->run->exit_reason = 0;
 -	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",
 -	       (int)(exit_qualification >> 4) & 3, cr);
 -	return 0;
 +	pi_clear_on(&vmx->pi_desc);
 +	memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
  }
  
 -static int handle_dr(struct kvm_vcpu *vcpu)
 +static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
  {
 -	unsigned long exit_qualification;
 -	int dr, dr7, reg;
 +	u32 exit_intr_info = 0;
 +	u16 basic_exit_reason = (u16)vmx->exit_reason;
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 +	if (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY
 +	      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))
 +		return;
  
 -	/* First, if DR does not exist, trigger UD */
 -	if (!kvm_require_dr(vcpu, dr))
 -		return 1;
 +	if (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +	vmx->exit_intr_info = exit_intr_info;
  
 -	/* Do not handle if the CPL > 0, will trigger GP on re-entry */
 -	if (!kvm_require_cpl(vcpu, 0))
 -		return 1;
 -	dr7 = vmcs_readl(GUEST_DR7);
 -	if (dr7 & DR7_GD) {
 -		/*
 -		 * As the vm-exit takes precedence over the debug trap, we
 -		 * need to emulate the latter, either for the host or the
 -		 * guest debugging itself.
 -		 */
 -		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 -			vcpu->run->debug.arch.dr6 = vcpu->arch.dr6;
 -			vcpu->run->debug.arch.dr7 = dr7;
 -			vcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 -			vcpu->run->debug.arch.exception = DB_VECTOR;
 -			vcpu->run->exit_reason = KVM_EXIT_DEBUG;
 -			return 0;
 -		} else {
 -			vcpu->arch.dr6 &= ~15;
 -			vcpu->arch.dr6 |= DR6_BD | DR6_RTM;
 -			kvm_queue_exception(vcpu, DB_VECTOR);
 -			return 1;
 -		}
 +	/* if exit due to PF check for async PF */
 +	if (is_page_fault(exit_intr_info))
 +		vmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	/* Handle machine checks before interrupts are enabled */
 +	if (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||
 +	    is_machine_check(exit_intr_info))
 +		kvm_machine_check();
 +
 +	/* We need to handle NMIs before interrupts are enabled */
 +	if (is_nmi(exit_intr_info)) {
 +		kvm_before_interrupt(&vmx->vcpu);
 +		asm("int $2");
 +		kvm_after_interrupt(&vmx->vcpu);
  	}
 +}
  
 -	if (vcpu->guest_debug == 0) {
 -		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -				CPU_BASED_MOV_DR_EXITING);
 +static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 +{
 +	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +
 +	if ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))
 +			== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {
 +		unsigned int vector;
 +		unsigned long entry;
 +		gate_desc *desc;
 +		struct vcpu_vmx *vmx = to_vmx(vcpu);
 +#ifdef CONFIG_X86_64
 +		unsigned long tmp;
 +#endif
 +
 +		vector =  exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		desc = (gate_desc *)vmx->host_idt_base + vector;
 +		entry = gate_offset(desc);
 +		asm volatile(
 +#ifdef CONFIG_X86_64
 +			"mov %%" _ASM_SP ", %[sp]\n\t"
 +			"and $0xfffffffffffffff0, %%" _ASM_SP "\n\t"
 +			"push $%c[ss]\n\t"
 +			"push %[sp]\n\t"
 +#endif
 +			"pushf\n\t"
 +			__ASM_SIZE(push) " $%c[cs]\n\t"
 +			CALL_NOSPEC
 +			:
 +#ifdef CONFIG_X86_64
 +			[sp]"=&r"(tmp),
 +#endif
 +			ASM_CALL_CONSTRAINT
 +			:
 +			THUNK_TARGET(entry),
 +			[ss]"i"(__KERNEL_DS),
 +			[cs]"i"(__KERNEL_CS)
 +			);
 +	}
 +}
 +STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
  
 +static bool vmx_has_emulated_msr(int index)
 +{
 +	switch (index) {
 +	case MSR_IA32_SMBASE:
  		/*
 -		 * No more DR vmexits; force a reload of the debug registers
 -		 * and reenter on this instruction.  The next vmexit will
 -		 * retrieve the full state of the debug registers.
 +		 * We cannot do SMM unless we can run the guest in big
 +		 * real mode.
  		 */
 -		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 -		return 1;
 +		return enable_unrestricted_guest || emulate_invalid_guest_state;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		/* This is AMD only.  */
 +		return false;
 +	default:
 +		return true;
  	}
 -
 -	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 -	if (exit_qualification & TYPE_MOV_FROM_DR) {
 -		unsigned long val;
 -
 -		if (kvm_get_dr(vcpu, dr, &val))
 -			return 1;
 -		kvm_register_write(vcpu, reg, val);
 -	} else
 -		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 -			return 1;
 -
 -	return kvm_skip_emulated_instruction(vcpu);
  }
  
 -static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 +static bool vmx_mpx_supported(void)
  {
 -	return vcpu->arch.dr6;
 +	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 +		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
  }
  
 -static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 +static bool vmx_xsaves_supported(void)
  {
 +	return vmcs_config.cpu_based_2nd_exec_ctrl &
 +		SECONDARY_EXEC_XSAVES;
  }
  
 -static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 +static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
  {
 -	get_debugreg(vcpu->arch.db[0], 0);
 -	get_debugreg(vcpu->arch.db[1], 1);
 -	get_debugreg(vcpu->arch.db[2], 2);
 -	get_debugreg(vcpu->arch.db[3], 3);
 -	get_debugreg(vcpu->arch.dr6, 6);
 -	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);
 +	u32 exit_intr_info;
 +	bool unblock_nmi;
 +	u8 vector;
 +	bool idtv_info_valid;
  
 -	vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
 -	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 -}
 +	idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
  
 -static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 -{
 -	vmcs_writel(GUEST_DR7, val);
 +	if (enable_vnmi) {
 +		if (vmx->loaded_vmcs->nmi_known_unmasked)
 +			return;
 +		/*
 +		 * Can't use vmx->exit_intr_info since we're not sure what
 +		 * the exit reason is.
 +		 */
 +		exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
 +		unblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;
 +		vector = exit_intr_info & INTR_INFO_VECTOR_MASK;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Re-set bit "block by NMI" before VM entry if vmexit caused by
 +		 * a guest IRET fault.
 +		 * SDM 3: 23.2.2 (September 2008)
 +		 * Bit 12 is undefined in any of the following cases:
 +		 *  If the VM exit sets the valid bit in the IDT-vectoring
 +		 *   information field.
 +		 *  If the VM exit is due to a double fault.
 +		 */
 +		if ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&
 +		    vector != DF_VECTOR && !idtv_info_valid)
 +			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,
 +				      GUEST_INTR_STATE_NMI);
 +		else
 +			vmx->loaded_vmcs->nmi_known_unmasked =
 +				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)
 +				  & GUEST_INTR_STATE_NMI);
 +	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->vnmi_blocked_time +=
 +			ktime_to_ns(ktime_sub(ktime_get(),
 +					      vmx->loaded_vmcs->entry_time));
  }
  
 -static int handle_cpuid(struct kvm_vcpu *vcpu)
 +static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 +				      u32 idt_vectoring_info,
 +				      int instr_len_field,
 +				      int error_code_field)
  {
 -	return kvm_emulate_cpuid(vcpu);
 -}
 +	u8 vector;
 +	int type;
 +	bool idtv_info_valid;
  
 -static int handle_rdmsr(struct kvm_vcpu *vcpu)
 -{
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	struct msr_data msr_info;
 +	idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
  
 -	msr_info.index = ecx;
 -	msr_info.host_initiated = false;
 -	if (vmx_get_msr(vcpu, &msr_info)) {
 -		trace_kvm_msr_read_ex(ecx);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 -	}
 +	vcpu->arch.nmi_injected = false;
 +	kvm_clear_exception_queue(vcpu);
 +	kvm_clear_interrupt_queue(vcpu);
  
 -	trace_kvm_msr_read(ecx, msr_info.data);
 +	if (!idtv_info_valid)
 +		return;
  
 -	/* FIXME: handling of bits 32:63 of rax, rdx */
 -	vcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;
 -	vcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
  
 -static int handle_wrmsr(struct kvm_vcpu *vcpu)
 -{
 -	struct msr_data msr;
 -	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
 -	u64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)
 -		| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);
 +	vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
 +	type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
  
 -	msr.data = data;
 -	msr.index = ecx;
 -	msr.host_initiated = false;
 -	if (kvm_set_msr(vcpu, &msr) != 0) {
 -		trace_kvm_msr_write_ex(ecx, data);
 -		kvm_inject_gp(vcpu, 0);
 -		return 1;
 +	switch (type) {
 +	case INTR_TYPE_NMI_INTR:
 +		vcpu->arch.nmi_injected = true;
 +		/*
 +		 * SDM 3: 27.7.1.2 (September 2008)
 +		 * Clear bit "block by NMI" before VM entry if a NMI
 +		 * delivery faulted.
 +		 */
 +		vmx_set_nmi_mask(vcpu, false);
 +		break;
 +	case INTR_TYPE_SOFT_EXCEPTION:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_HARD_EXCEPTION:
 +		if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
 +			u32 err = vmcs_read32(error_code_field);
 +			kvm_requeue_exception_e(vcpu, vector, err);
 +		} else
 +			kvm_requeue_exception(vcpu, vector);
 +		break;
 +	case INTR_TYPE_SOFT_INTR:
 +		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 +		/* fall through */
 +	case INTR_TYPE_EXT_INTR:
 +		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 +		break;
 +	default:
 +		break;
  	}
 +}
  
 -	trace_kvm_msr_write(ecx, data);
 -	return kvm_skip_emulated_instruction(vcpu);
 +static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 +{
 +	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 +				  VM_EXIT_INSTRUCTION_LEN,
 +				  IDT_VECTORING_ERROR_CODE);
  }
  
 -static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 +static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
  {
 -	kvm_apic_update_ppr(vcpu);
 -	return 1;
 +	__vmx_complete_interrupts(vcpu,
 +				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 +				  VM_ENTRY_INSTRUCTION_LEN,
 +				  VM_ENTRY_EXCEPTION_ERROR_CODE);
 +
 +	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
  }
  
 -static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 +static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
  {
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_INTR_PENDING);
 +	int i, nr_msrs;
 +	struct perf_guest_switch_msr *msrs;
  
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +	msrs = perf_guest_get_msrs(&nr_msrs);
  
 -	++vcpu->stat.irq_window_exits;
 -	return 1;
 +	if (!msrs)
 +		return;
 +
 +	for (i = 0; i < nr_msrs; i++)
 +		if (msrs[i].host == msrs[i].guest)
 +			clear_atomic_switch_msr(vmx, msrs[i].msr);
 +		else
 +			add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,
 +					msrs[i].host, false);
  }
  
 -static int handle_halt(struct kvm_vcpu *vcpu)
 +static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
  {
 -	return kvm_emulate_halt(vcpu);
 +	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, val);
 +	if (!vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
 +			      PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = true;
  }
  
 -static int handle_vmcall(struct kvm_vcpu *vcpu)
 +static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
  {
 -	return kvm_emulate_hypercall(vcpu);
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +	u64 tscl;
 +	u32 delta_tsc;
 +
 +	if (vmx->req_immediate_exit) {
 +		vmx_arm_hv_timer(vmx, 0);
 +		return;
 +	}
 +
 +	if (vmx->hv_deadline_tsc != -1) {
 +		tscl = rdtsc();
 +		if (vmx->hv_deadline_tsc > tscl)
 +			/* set_hv_timer ensures the delta fits in 32-bits */
 +			delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
 +				cpu_preemption_timer_multi);
 +		else
 +			delta_tsc = 0;
 +
 +		vmx_arm_hv_timer(vmx, delta_tsc);
 +		return;
 +	}
 +
 +	if (vmx->loaded_vmcs->hv_timer_armed)
 +		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
 +				PIN_BASED_VMX_PREEMPTION_TIMER);
 +	vmx->loaded_vmcs->hv_timer_armed = false;
  }
  
 -static int handle_invd(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
++=======
++void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
+ {
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
++	if (unlikely(host_rsp != vmx->loaded_vmcs->host_state.rsp)) {
++		vmx->loaded_vmcs->host_state.rsp = host_rsp;
++		vmcs_writel(HOST_RSP, host_rsp);
++	}
+ }
+ 
 -static int handle_invlpg(struct kvm_vcpu *vcpu)
++static void __vmx_vcpu_run(struct kvm_vcpu *vcpu, struct vcpu_vmx *vmx)
+ {
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
++	if (static_branch_unlikely(&vmx_l1d_should_flush))
++		vmx_l1d_flush(vcpu);
+ 
 -	kvm_mmu_invlpg(vcpu, exit_qualification);
 -	return kvm_skip_emulated_instruction(vcpu);
 -}
++	if (vcpu->arch.cr2 != read_cr2())
++		write_cr2(vcpu->arch.cr2);
++
++	asm(
++		/* Store host registers */
++		"push %%" _ASM_BP " \n\t"
++		"push %%" _ASM_ARG1 " \n\t"
+ 
 -static int handle_rdpmc(struct kvm_vcpu *vcpu)
 -{
 -	int err;
++		/* Adjust RSP to account for the CALL to vmx_vmenter(). */
++		"lea -%c[wordsize](%%" _ASM_SP "), %%" _ASM_ARG2 " \n\t"
++		"call vmx_update_host_rsp \n\t"
+ 
 -	err = kvm_rdpmc(vcpu);
 -	return kvm_complete_insn_gp(vcpu, err);
 -}
++		/* Load the vcpu_vmx pointer to RCX. */
++		"mov (%%" _ASM_SP "), %%" _ASM_CX " \n\t"
+ 
 -static int handle_wbinvd(struct kvm_vcpu *vcpu)
 -{
 -	return kvm_emulate_wbinvd(vcpu);
 -}
++		/* Check if vmlaunch or vmresume is needed */
++		"cmpb $0, %%bl \n\t"
+ 
 -static int handle_xsetbv(struct kvm_vcpu *vcpu)
 -{
 -	u64 new_bv = kvm_read_edx_eax(vcpu);
 -	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
++		/* Load guest registers.  Don't clobber flags. */
++		"mov %c[rax](%%" _ASM_CX "), %%" _ASM_AX " \n\t"
++		"mov %c[rbx](%%" _ASM_CX "), %%" _ASM_BX " \n\t"
++		"mov %c[rdx](%%" _ASM_CX "), %%" _ASM_DX " \n\t"
++		"mov %c[rsi](%%" _ASM_CX "), %%" _ASM_SI " \n\t"
++		"mov %c[rdi](%%" _ASM_CX "), %%" _ASM_DI " \n\t"
++		"mov %c[rbp](%%" _ASM_CX "), %%" _ASM_BP " \n\t"
++#ifdef CONFIG_X86_64
++		"mov %c[r8](%%" _ASM_CX "),  %%r8  \n\t"
++		"mov %c[r9](%%" _ASM_CX "),  %%r9  \n\t"
++		"mov %c[r10](%%" _ASM_CX "), %%r10 \n\t"
++		"mov %c[r11](%%" _ASM_CX "), %%r11 \n\t"
++		"mov %c[r12](%%" _ASM_CX "), %%r12 \n\t"
++		"mov %c[r13](%%" _ASM_CX "), %%r13 \n\t"
++		"mov %c[r14](%%" _ASM_CX "), %%r14 \n\t"
++		"mov %c[r15](%%" _ASM_CX "), %%r15 \n\t"
++#endif
++		/* Load guest RCX.  This kills the vmx_vcpu pointer! */
++		"mov %c[rcx](%%" _ASM_CX "), %%" _ASM_CX " \n\t"
+ 
 -	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 -		return kvm_skip_emulated_instruction(vcpu);
 -	return 1;
 -}
++		/* Enter guest mode */
++		"call vmx_vmenter\n\t"
++		"jbe 2f \n\t"
+ 
 -static int handle_xsaves(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 -}
++		/* Temporarily save guest's RCX. */
++		"push %%" _ASM_CX " \n\t"
+ 
 -static int handle_xrstors(struct kvm_vcpu *vcpu)
 -{
 -	kvm_skip_emulated_instruction(vcpu);
 -	WARN(1, "this should never happen\n");
 -	return 1;
 -}
++		/* Reload the vcpu_vmx pointer to RCX. */
++		"mov %c[wordsize](%%" _ASM_SP "), %%" _ASM_CX " \n\t"
+ 
 -static int handle_apic_access(struct kvm_vcpu *vcpu)
 -{
 -	if (likely(fasteoi)) {
 -		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -		int access_type, offset;
++		/* Save all guest registers, including RCX from the stack */
++		"mov %%" _ASM_AX ", %c[rax](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_BX ", %c[rbx](%%" _ASM_CX ") \n\t"
++		__ASM_SIZE(pop) " %c[rcx](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_DX ", %c[rdx](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_SI ", %c[rsi](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_DI ", %c[rdi](%%" _ASM_CX ") \n\t"
++		"mov %%" _ASM_BP ", %c[rbp](%%" _ASM_CX ") \n\t"
++#ifdef CONFIG_X86_64
++		"mov %%r8,  %c[r8](%%" _ASM_CX ") \n\t"
++		"mov %%r9,  %c[r9](%%" _ASM_CX ") \n\t"
++		"mov %%r10, %c[r10](%%" _ASM_CX ") \n\t"
++		"mov %%r11, %c[r11](%%" _ASM_CX ") \n\t"
++		"mov %%r12, %c[r12](%%" _ASM_CX ") \n\t"
++		"mov %%r13, %c[r13](%%" _ASM_CX ") \n\t"
++		"mov %%r14, %c[r14](%%" _ASM_CX ") \n\t"
++		"mov %%r15, %c[r15](%%" _ASM_CX ") \n\t"
++#endif
++
++		/* Clear EBX to indicate VM-Exit (as opposed to VM-Fail). */
++		"xor %%ebx, %%ebx \n\t"
+ 
 -		access_type = exit_qualification & APIC_ACCESS_TYPE;
 -		offset = exit_qualification & APIC_ACCESS_OFFSET;
+ 		/*
 -		 * Sane guest uses MOV to write EOI, with written value
 -		 * not cared. So make a short-circuit here by avoiding
 -		 * heavy instruction emulation.
++		 * Clear all general purpose registers except RSP and RBX to prevent
++		 * speculative use of the guest's values, even those that are reloaded
++		 * via the stack.  In theory, an L1 cache miss when restoring registers
++		 * could lead to speculative execution with the guest's values.
++		 * Zeroing XORs are dirt cheap, i.e. the extra paranoia is essentially
++		 * free.  RSP and RBX are exempt as RSP is restored by hardware during
++		 * VM-Exit and RBX is explicitly loaded with 0 or 1 to "return" VM-Fail.
+ 		 */
 -		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&
 -		    (offset == APIC_EOI)) {
 -			kvm_lapic_set_eoi(vcpu);
 -			return kvm_skip_emulated_instruction(vcpu);
 -		}
 -	}
 -	return kvm_emulate_instruction(vcpu, 0) == EMULATE_DONE;
 -}
++		"1: \n\t"
++#ifdef CONFIG_X86_64
++		"xor %%r8d,  %%r8d \n\t"
++		"xor %%r9d,  %%r9d \n\t"
++		"xor %%r10d, %%r10d \n\t"
++		"xor %%r11d, %%r11d \n\t"
++		"xor %%r12d, %%r12d \n\t"
++		"xor %%r13d, %%r13d \n\t"
++		"xor %%r14d, %%r14d \n\t"
++		"xor %%r15d, %%r15d \n\t"
++#endif
++		"xor %%eax, %%eax \n\t"
++		"xor %%ecx, %%ecx \n\t"
++		"xor %%edx, %%edx \n\t"
++		"xor %%esi, %%esi \n\t"
++		"xor %%edi, %%edi \n\t"
++		"xor %%ebp, %%ebp \n\t"
+ 
 -static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	int vector = exit_qualification & 0xff;
++		/* "POP" the vcpu_vmx pointer. */
++		"add $%c[wordsize], %%" _ASM_SP " \n\t"
++		"pop  %%" _ASM_BP " \n\t"
++		"jmp 3f \n\t"
+ 
 -	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_set_eoi_accelerated(vcpu, vector);
 -	return 1;
 -}
++		/* VM-Fail.  Out-of-line to avoid a taken Jcc after VM-Exit. */
++		"2: \n\t"
++		"mov $1, %%ebx \n\t"
++		"jmp 1b \n\t"
++		"3: \n\t"
+ 
 -static int handle_apic_write(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 -	u32 offset = exit_qualification & 0xfff;
++	      : ASM_CALL_CONSTRAINT, "=b"(vmx->fail),
++#ifdef CONFIG_X86_64
++		"=D"((int){0})
++	      : "D"(vmx),
++#else
++		"=a"((int){0})
++	      : "a"(vmx),
++#endif
++		"b"(vmx->loaded_vmcs->launched),
++		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
++		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
++		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
++		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
++		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
++		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
++		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
++#ifdef CONFIG_X86_64
++		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
++		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
++		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
++		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
++		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
++		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
++		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
++		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
++#endif
++		[wordsize]"i"(sizeof(ulong))
++	      : "cc", "memory"
++#ifdef CONFIG_X86_64
++		, "rax", "rcx", "rdx", "rsi"
++		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
++#else
++		, "ecx", "edx", "edi", "esi"
++#endif
++	      );
+ 
 -	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 -	kvm_apic_write_nodecode(vcpu, offset);
 -	return 1;
++	vcpu->arch.cr2 = read_cr2();
+ }
++STACK_FRAME_NON_STANDARD(__vmx_vcpu_run);
+ 
 -static int handle_task_switch(struct kvm_vcpu *vcpu)
++static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
++>>>>>>> f78d0971b7bd (KVM: VMX: Don't save guest registers after VM-Fail)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	unsigned long exit_qualification;
 -	bool has_error_code = false;
 -	u32 error_code = 0;
 -	u16 tss_selector;
 -	int reason, type, idt_v, idt_index;
 +	unsigned long cr3, cr4, evmcs_rsp;
  
 -	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
 -	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
 -	type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
 +	/* Record the guest's net vcpu time for enforced NMI injections. */
 +	if (unlikely(!enable_vnmi &&
 +		     vmx->loaded_vmcs->soft_vnmi_blocked))
 +		vmx->loaded_vmcs->entry_time = ktime_get();
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	/* Don't enter VMX if guest state is invalid, let the exit handler
 +	   start emulation until we arrive back to a valid state */
 +	if (vmx->emulation_required)
 +		return;
  
 -	reason = (u32)exit_qualification >> 30;
 -	if (reason == TASK_SWITCH_GATE && idt_v) {
 -		switch (type) {
 -		case INTR_TYPE_NMI_INTR:
 -			vcpu->arch.nmi_injected = false;
 -			vmx_set_nmi_mask(vcpu, true);
 -			break;
 -		case INTR_TYPE_EXT_INTR:
 -		case INTR_TYPE_SOFT_INTR:
 -			kvm_clear_interrupt_queue(vcpu);
 -			break;
 -		case INTR_TYPE_HARD_EXCEPTION:
 -			if (vmx->idt_vectoring_info &
 -			    VECTORING_INFO_DELIVER_CODE_MASK) {
 -				has_error_code = true;
 -				error_code =
 -					vmcs_read32(IDT_VECTORING_ERROR_CODE);
 -			}
 -			/* fall through */
 -		case INTR_TYPE_SOFT_EXCEPTION:
 -			kvm_clear_exception_queue(vcpu);
 -			break;
 -		default:
 -			break;
 +	if (vmx->ple_window_dirty) {
 +		vmx->ple_window_dirty = false;
 +		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 +	}
 +
 +	if (vmx->nested.need_vmcs12_sync) {
 +		if (vmx->nested.hv_evmcs) {
 +			copy_vmcs12_to_enlightened(vmx);
 +			/* All fields are clean */
 +			vmx->nested.hv_evmcs->hv_clean_fields |=
 +				HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
 +		} else {
 +			copy_vmcs12_to_shadow(vmx);
  		}
 +		vmx->nested.need_vmcs12_sync = false;
  	}
 -	tss_selector = exit_qualification;
  
 -	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&
 -		       type != INTR_TYPE_EXT_INTR &&
 -		       type != INTR_TYPE_NMI_INTR))
 -		skip_emulated_instruction(vcpu);
 +	if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 +	if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 +		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
  
 -	if (kvm_task_switch(vcpu, tss_selector,
 -			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 -			    has_error_code, error_code) == EMULATE_FAIL) {
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -		vcpu->run->internal.ndata = 0;
 -		return 0;
 +	cr3 = __get_current_cr3_fast();
 +	if (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {
 +		vmcs_writel(HOST_CR3, cr3);
 +		vmx->loaded_vmcs->host_state.cr3 = cr3;
  	}
  
 -	/*
 -	 * TODO: What about debug traps on tss switch?
 -	 *       Are we supposed to inject them and update dr6?
 -	 */
 +	cr4 = cr4_read_shadow();
 +	if (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {
 +		vmcs_writel(HOST_CR4, cr4);
 +		vmx->loaded_vmcs->host_state.cr4 = cr4;
 +	}
  
 -	return 1;
 -}
 +	/* When single-stepping over STI and MOV SS, we must clear the
 +	 * corresponding interruptibility bits in the guest state. Otherwise
 +	 * vmentry fails as it then expects bit 14 (BS) in pending debug
 +	 * exceptions being set, but that's not correct for the guest debugging
 +	 * case. */
 +	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 +		vmx_set_interrupt_shadow(vcpu, 0);
  
 -static int handle_ept_violation(struct kvm_vcpu *vcpu)
 -{
 -	unsigned long exit_qualification;
 -	gpa_t gpa;
 -	u64 error_code;
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
 +	    vcpu->arch.pkru != vmx->host_pkru)
 +		__write_pkru(vcpu->arch.pkru);
  
 -	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 +	atomic_switch_perf_msrs(vmx);
 +
 +	vmx_update_hv_timer(vcpu);
  
  	/*
 -	 * EPT violation happened while executing iret from NMI,
 -	 * "blocked by NMI" bit has to be set before next VM entry.
 -	 * There are errata that may cause this bit to not be set:
 -	 * AAK134, BY25.
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
  	 */
 -	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
 -			enable_vnmi &&
 -			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 -		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 -
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	trace_kvm_page_fault(gpa, exit_qualification);
 +	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
  
 -	/* Is it a read fault? */
 -	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 -		     ? PFERR_USER_MASK : 0;
 -	/* Is it a write fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
 -		      ? PFERR_WRITE_MASK : 0;
 -	/* Is it a fetch fault? */
 -	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
 -		      ? PFERR_FETCH_MASK : 0;
 -	/* ept page table entry is present? */
 -	error_code |= (exit_qualification &
 -		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
 -			EPT_VIOLATION_EXECUTABLE))
 -		      ? PFERR_PRESENT_MASK : 0;
 +	vmx->__launched = vmx->loaded_vmcs->launched;
  
 -	error_code |= (exit_qualification & 0x100) != 0 ?
 -	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 +	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
 +		(unsigned long)&current_evmcs->host_rsp : 0;
  
 -	vcpu->arch.exit_qualification = exit_qualification;
 -	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 -}
 +	if (static_branch_unlikely(&vmx_l1d_should_flush))
 +		vmx_l1d_flush(vcpu);
  
 -static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 -{
 -	gpa_t gpa;
 +	asm(
 +		/* Store host registers */
 +		"push %%" _ASM_DX "; push %%" _ASM_BP ";"
 +		"push %%" _ASM_CX " \n\t" /* placeholder for guest rcx */
 +		"push %%" _ASM_CX " \n\t"
 +		"cmp %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		"je 1f \n\t"
 +		"mov %%" _ASM_SP ", %c[host_rsp](%0) \n\t"
 +		/* Avoid VMWRITE when Enlightened VMCS is in use */
 +		"test %%" _ASM_SI ", %%" _ASM_SI " \n\t"
 +		"jz 2f \n\t"
 +		"mov %%" _ASM_SP ", (%%" _ASM_SI ") \n\t"
 +		"jmp 1f \n\t"
 +		"2: \n\t"
 +		__ex("vmwrite %%" _ASM_SP ", %%" _ASM_DX) "\n\t"
 +		"1: \n\t"
 +		/* Reload cr2 if changed */
 +		"mov %c[cr2](%0), %%" _ASM_AX " \n\t"
 +		"mov %%cr2, %%" _ASM_DX " \n\t"
 +		"cmp %%" _ASM_AX ", %%" _ASM_DX " \n\t"
 +		"je 3f \n\t"
 +		"mov %%" _ASM_AX", %%cr2 \n\t"
 +		"3: \n\t"
 +		/* Check if vmlaunch or vmresume is needed */
 +		"cmpl $0, %c[launched](%0) \n\t"
 +		/* Load guest registers.  Don't clobber flags. */
 +		"mov %c[rax](%0), %%" _ASM_AX " \n\t"
 +		"mov %c[rbx](%0), %%" _ASM_BX " \n\t"
 +		"mov %c[rdx](%0), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%0), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%0), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%0), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%0),  %%r8  \n\t"
 +		"mov %c[r9](%0),  %%r9  \n\t"
 +		"mov %c[r10](%0), %%r10 \n\t"
 +		"mov %c[r11](%0), %%r11 \n\t"
 +		"mov %c[r12](%0), %%r12 \n\t"
 +		"mov %c[r13](%0), %%r13 \n\t"
 +		"mov %c[r14](%0), %%r14 \n\t"
 +		"mov %c[r15](%0), %%r15 \n\t"
 +#endif
 +		"mov %c[rcx](%0), %%" _ASM_CX " \n\t" /* kills %0 (ecx) */
  
 -	/*
 -	 * A nested guest cannot optimize MMIO vmexits, because we have an
 -	 * nGPA here instead of the required GPA.
 -	 */
 -	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 -	if (!is_guest_mode(vcpu) &&
 -	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 -		trace_kvm_fast_mmio(gpa);
 +		/* Enter guest mode */
 +		"jne 1f \n\t"
 +		__ex("vmlaunch") "\n\t"
 +		"jmp 2f \n\t"
 +		"1: " __ex("vmresume") "\n\t"
 +		"2: "
 +		/* Save guest registers, load host registers, keep flags */
 +		"mov %0, %c[wordsize](%%" _ASM_SP ") \n\t"
 +		"pop %0 \n\t"
 +		"setbe %c[fail](%0)\n\t"
 +		"mov %%" _ASM_AX ", %c[rax](%0) \n\t"
 +		"mov %%" _ASM_BX ", %c[rbx](%0) \n\t"
 +		__ASM_SIZE(pop) " %c[rcx](%0) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%0) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%0) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%0) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%0) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%0) \n\t"
 +		"mov %%r9,  %c[r9](%0) \n\t"
 +		"mov %%r10, %c[r10](%0) \n\t"
 +		"mov %%r11, %c[r11](%0) \n\t"
 +		"mov %%r12, %c[r12](%0) \n\t"
 +		"mov %%r13, %c[r13](%0) \n\t"
 +		"mov %%r14, %c[r14](%0) \n\t"
 +		"mov %%r15, %c[r15](%0) \n\t"
  		/*
 -		 * Doing kvm_skip_emulated_instruction() depends on undefined
 -		 * behavior: Intel's manual doesn't mandate
 -		 * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG
 -		 * occurs and while on real hardware it was observed to be set,
 -		 * other hypervisors (namely Hyper-V) don't set it, we end up
 -		 * advancing IP with some random value. Disable fast mmio when
 -		 * running nested and keep it for real hardware in hope that
 -		 * VM_EXIT_INSTRUCTION_LEN will always be set correctly.
 -		 */
 -		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
 -			return kvm_skip_emulated_instruction(vcpu);
 -		else
 -			return kvm_emulate_instruction(vcpu, EMULTYPE_SKIP) ==
 -								EMULATE_DONE;
 -	}
 -
 -	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 -}
 -
 -static int handle_nmi_window(struct kvm_vcpu *vcpu)
 -{
 -	WARN_ON_ONCE(!enable_vnmi);
 -	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 -			CPU_BASED_VIRTUAL_NMI_PENDING);
 -	++vcpu->stat.nmi_window_exits;
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 -
 -	return 1;
 -}
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d,  %%r8d \n\t"
 +		"xor %%r9d,  %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"mov %%cr2, %%" _ASM_AX "   \n\t"
 +		"mov %%" _ASM_AX ", %c[cr2](%0) \n\t"
  
 -static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	enum emulation_result err = EMULATE_DONE;
 -	int ret = 1;
 -	u32 cpu_exec_ctrl;
 -	bool intr_window_requested;
 -	unsigned count = 130;
 +		"xor %%eax, %%eax \n\t"
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop  %%" _ASM_BP "; pop  %%" _ASM_DX " \n\t"
 +		".pushsection .rodata \n\t"
 +		".global vmx_return \n\t"
 +		"vmx_return: " _ASM_PTR " 2b \n\t"
 +		".popsection"
 +	      : : "c"(vmx), "d"((unsigned long)HOST_RSP), "S"(evmcs_rsp),
 +		[launched]"i"(offsetof(struct vcpu_vmx, __launched)),
 +		[fail]"i"(offsetof(struct vcpu_vmx, fail)),
 +		[host_rsp]"i"(offsetof(struct vcpu_vmx, host_rsp)),
 +		[rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),
 +		[rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		[rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		[rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		[rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		[rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		[rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),
 +#ifdef CONFIG_X86_64
 +		[r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),
 +		[r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),
 +		[r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),
 +		[r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),
 +		[r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),
 +		[r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),
 +		[r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),
 +		[r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),
 +#endif
 +		[cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),
 +		[wordsize]"i"(sizeof(ulong))
 +	      : "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rax", "rbx", "rdi"
 +		, "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
 +#else
 +		, "eax", "ebx", "edi"
 +#endif
 +	      );
  
  	/*
 -	 * We should never reach the point where we are emulating L2
 -	 * due to invalid guest state as that means we incorrectly
 -	 * allowed a nested VMEntry with an invalid vmcs12.
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
  	 */
 -	WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
  
 -	cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
 -	intr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;
 +	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
  
 -	while (vmx->emulation_required && count-- != 0) {
 -		if (intr_window_requested && vmx_interrupt_allowed(vcpu))
 -			return handle_interrupt_window(&vmx->vcpu);
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
  
 -		if (kvm_test_request(KVM_REQ_EVENT, vcpu))
 -			return 1;
 +	/* All fields are clean at this point */
 +	if (static_branch_unlikely(&enable_evmcs))
 +		current_evmcs->hv_clean_fields |=
 +			HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;
  
 -		err = kvm_emulate_instruction(vcpu, 0);
 +	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 +	if (vmx->host_debugctlmsr)
 +		update_debugctlmsr(vmx->host_debugctlmsr);
  
 -		if (err == EMULATE_USER_EXIT) {
 -			++vcpu->stat.mmio_exits;
 -			ret = 0;
 -			goto out;
 -		}
 +#ifndef CONFIG_X86_64
 +	/*
 +	 * The sysexit path does not restore ds/es, so we must set them to
 +	 * a reasonable value ourselves.
 +	 *
 +	 * We can't defer this to vmx_prepare_switch_to_host() since that
 +	 * function may be executed in interrupt context, which saves and
 +	 * restore segments around it, nullifying its effect.
 +	 */
 +	loadsegment(ds, __USER_DS);
 +	loadsegment(es, __USER_DS);
 +#endif
  
 -		if (err != EMULATE_DONE)
 -			goto emulation_error;
 +	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
 +				  | (1 << VCPU_EXREG_RFLAGS)
 +				  | (1 << VCPU_EXREG_PDPTR)
 +				  | (1 << VCPU_EXREG_SEGMENTS)
 +				  | (1 << VCPU_EXREG_CR3));
 +	vcpu->arch.regs_dirty = 0;
  
 -		if (vmx->emulation_required && !vmx->rmode.vm86_active &&
 -		    vcpu->arch.exception.pending)
 -			goto emulation_error;
 +	/*
 +	 * eager fpu is enabled if PKEY is supported and CR4 is switched
 +	 * back on host, so it is safe to read guest PKRU from current
 +	 * XSAVE.
 +	 */
 +	if (static_cpu_has(X86_FEATURE_PKU) &&
 +	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
 +		vcpu->arch.pkru = __read_pkru();
 +		if (vcpu->arch.pkru != vmx->host_pkru)
 +			__write_pkru(vmx->host_pkru);
 +	}
  
 -		if (vcpu->arch.halt_request) {
 -			vcpu->arch.halt_request = 0;
 -			ret = kvm_vcpu_halt(vcpu);
 -			goto out;
 -		}
 +	vmx->nested.nested_run_pending = 0;
 +	vmx->idt_vectoring_info = 0;
  
 -		if (signal_pending(current))
 -			goto out;
 -		if (need_resched())
 -			schedule();
 -	}
 +	vmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);
 +	if (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 +		return;
  
 -out:
 -	return ret;
 +	vmx->loaded_vmcs->launched = 1;
 +	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
  
 -emulation_error:
 -	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 -	vcpu->run->internal.ndata = 0;
 -	return 0;
 +	vmx_complete_atomic_exit(vmx);
 +	vmx_recover_nmi_blocking(vmx);
 +	vmx_complete_interrupts(vmx);
  }
 +STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
  
 -static void grow_ple_window(struct kvm_vcpu *vcpu)
 +static struct kvm *vmx_vm_alloc(void)
  {
 -	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 -
 -	vmx->ple_window = __grow_ple_window(old, ple_window,
 -					    ple_window_grow,
 -					    ple_window_max);
 -
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 +	return &kvm_vmx->kvm;
 +}
  
 -	trace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);
 +static void vmx_vm_free(struct kvm *kvm)
 +{
 +	vfree(to_kvm_vmx(kvm));
  }
  
 -static void shrink_ple_window(struct kvm_vcpu *vcpu)
 +static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
 -	int old = vmx->ple_window;
 +	int cpu;
  
 -	vmx->ple_window = __shrink_ple_window(old, ple_window,
 -					      ple_window_shrink,
 -					      ple_window);
 +	if (vmx->loaded_vmcs == vmcs)
 +		return;
  
 -	if (vmx->ple_window != old)
 -		vmx->ple_window_dirty = true;
 +	cpu = get_cpu();
 +	vmx_vcpu_put(vcpu);
 +	vmx->loaded_vmcs = vmcs;
 +	vmx_vcpu_load(vcpu, cpu);
 +	put_cpu();
  
 -	trace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);
 +	vm_entry_controls_reset_shadow(vmx);
 +	vm_exit_controls_reset_shadow(vmx);
 +	vmx_segment_cache_clear(vmx);
  }
  
  /*
* Unmerged path arch/x86/kvm/vmx/vmx.c
