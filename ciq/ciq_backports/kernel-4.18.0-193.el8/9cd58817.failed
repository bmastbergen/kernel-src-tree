RDMA/devices: Remove the lock around remove_client_context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 9cd5881719e9555cae300ec8b389eda3c8101339
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/9cd58817.failed

Due to the complexity of client->remove() callbacks it is desirable to not
hold any locks while calling them. Remove the last one by tracking only
the highest client ID and running backwards from there over the xarray.

Since the only purpose of that lock was to protect the linked list, we can
drop the lock.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
Link: https://lore.kernel.org/r/20190731081841.32345-3-leon@kernel.org
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 9cd5881719e9555cae300ec8b389eda3c8101339)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/device.c
index ec96a7b1c811,ea8661a00651..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -65,28 -60,145 +65,66 @@@ struct workqueue_struct *ib_comp_unboun
  struct workqueue_struct *ib_wq;
  EXPORT_SYMBOL_GPL(ib_wq);
  
++<<<<<<< HEAD
 +/* The device_list and clients contain devices and clients after their
 + * registration has completed, and the devices and clients are removed
 + * during unregistration. */
 +static LIST_HEAD(device_list);
 +static LIST_HEAD(client_list);
++=======
+ /*
+  * Each of the three rwsem locks (devices, clients, client_data) protects the
+  * xarray of the same name. Specifically it allows the caller to assert that
+  * the MARK will/will not be changing under the lock, and for devices and
+  * clients, that the value in the xarray is still a valid pointer. Change of
+  * the MARK is linked to the object state, so holding the lock and testing the
+  * MARK also asserts that the contained object is in a certain state.
+  *
+  * This is used to build a two stage register/unregister flow where objects
+  * can continue to be in the xarray even though they are still in progress to
+  * register/unregister.
+  *
+  * The xarray itself provides additional locking, and restartable iteration,
+  * which is also relied on.
+  *
+  * Locks should not be nested, with the exception of client_data, which is
+  * allowed to nest under the read side of the other two locks.
+  *
+  * The devices_rwsem also protects the device name list, any change or
+  * assignment of device name must also hold the write side to guarantee unique
+  * names.
+  */
+ 
+ /*
+  * devices contains devices that have had their names assigned. The
+  * devices may not be registered. Users that care about the registration
+  * status need to call ib_device_try_get() on the device to ensure it is
+  * registered, and keep it registered, for the required duration.
+  *
+  */
+ static DEFINE_XARRAY_FLAGS(devices, XA_FLAGS_ALLOC);
+ static DECLARE_RWSEM(devices_rwsem);
+ #define DEVICE_REGISTERED XA_MARK_1
+ 
+ static u32 highest_client_id;
++>>>>>>> 9cd5881719e9 (RDMA/devices: Remove the lock around remove_client_context)
  #define CLIENT_REGISTERED XA_MARK_1
  static DEFINE_XARRAY_FLAGS(clients, XA_FLAGS_ALLOC);
 -static DECLARE_RWSEM(clients_rwsem);
 -
 -static void ib_client_put(struct ib_client *client)
 -{
 -	if (refcount_dec_and_test(&client->uses))
 -		complete(&client->uses_zero);
 -}
 -
 -/*
 - * If client_data is registered then the corresponding client must also still
 - * be registered.
 - */
 -#define CLIENT_DATA_REGISTERED XA_MARK_1
 -
 -/**
 - * struct rdma_dev_net - rdma net namespace metadata for a net
 - * @net:	Pointer to owner net namespace
 - * @id:		xarray id to identify the net namespace.
 - */
 -struct rdma_dev_net {
 -	possible_net_t net;
 -	u32 id;
 -};
 -
 -static unsigned int rdma_dev_net_id;
  
  /*
 - * A list of net namespaces is maintained in an xarray. This is necessary
 - * because we can't get the locking right using the existing net ns list. We
 - * would require a init_net callback after the list is updated.
 - */
 -static DEFINE_XARRAY_FLAGS(rdma_nets, XA_FLAGS_ALLOC);
 -/*
 - * rwsem to protect accessing the rdma_nets xarray entries.
 - */
 -static DECLARE_RWSEM(rdma_nets_rwsem);
 -
 -bool ib_devices_shared_netns = true;
 -module_param_named(netns_mode, ib_devices_shared_netns, bool, 0444);
 -MODULE_PARM_DESC(netns_mode,
 -		 "Share device among net namespaces; default=1 (shared)");
 -/**
 - * rdma_dev_access_netns() - Return whether a rdma device can be accessed
 - *			     from a specified net namespace or not.
 - * @device:	Pointer to rdma device which needs to be checked
 - * @net:	Pointer to net namesapce for which access to be checked
 + * device_mutex and lists_rwsem protect access to both device_list and
 + * clients.  device_mutex protects writer access by device and client
 + * registration / de-registration.  lists_rwsem protects reader access to
 + * these lists.  Iterators of these lists must lock it for read, while updates
 + * to the lists must be done with a write lock. A special case is when the
 + * device_mutex is locked. In this case locking the lists for read access is
 + * not necessary as the device_mutex implies it.
   *
 - * rdma_dev_access_netns() - Return whether a rdma device can be accessed
 - *			     from a specified net namespace or not. When
 - *			     rdma device is in shared mode, it ignores the
 - *			     net namespace. When rdma device is exclusive
 - *			     to a net namespace, rdma device net namespace is
 - *			     checked against the specified one.
 + * lists_rwsem also protects access to the client data list.
   */
 -bool rdma_dev_access_netns(const struct ib_device *dev, const struct net *net)
 -{
 -	return (ib_devices_shared_netns ||
 -		net_eq(read_pnet(&dev->coredev.rdma_net), net));
 -}
 -EXPORT_SYMBOL(rdma_dev_access_netns);
 +static DEFINE_MUTEX(device_mutex);
 +static DECLARE_RWSEM(lists_rwsem);
  
 -/*
 - * xarray has this behavior where it won't iterate over NULL values stored in
 - * allocated arrays.  So we need our own iterator to see all values stored in
 - * the array. This does the same thing as xa_for_each except that it also
 - * returns NULL valued entries if the array is allocating. Simplified to only
 - * work on simple xarrays.
 - */
 -static void *xan_find_marked(struct xarray *xa, unsigned long *indexp,
 -			     xa_mark_t filter)
 -{
 -	XA_STATE(xas, xa, *indexp);
 -	void *entry;
 -
 -	rcu_read_lock();
 -	do {
 -		entry = xas_find_marked(&xas, ULONG_MAX, filter);
 -		if (xa_is_zero(entry))
 -			break;
 -	} while (xas_retry(&xas, entry));
 -	rcu_read_unlock();
 -
 -	if (entry) {
 -		*indexp = xas.xa_index;
 -		if (xa_is_zero(entry))
 -			return NULL;
 -		return entry;
 -	}
 -	return XA_ERROR(-ENOENT);
 -}
 -#define xan_for_each_marked(xa, index, entry, filter)                          \
 -	for (index = 0, entry = xan_find_marked(xa, &(index), filter);         \
 -	     !xa_is_err(entry);                                                \
 -	     (index)++, entry = xan_find_marked(xa, &(index), filter))
 -
 -/* RCU hash table mapping netdevice pointers to struct ib_port_data */
 -static DEFINE_SPINLOCK(ndev_hash_lock);
 -static DECLARE_HASHTABLE(ndev_hash, 5);
 -
 -static void free_netdevs(struct ib_device *ib_dev);
 -static void ib_unregister_work(struct work_struct *work);
 -static void __ib_unregister_device(struct ib_device *device);
  static int ib_security_change(struct notifier_block *nb, unsigned long event,
  			      void *lsm_data);
  static void ib_policy_change_task(struct work_struct *work);
@@@ -564,141 -1235,446 +602,225 @@@ static int setup_device(struct ib_devic
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void disable_device(struct ib_device *device)
+ {
+ 	u32 cid;
+ 
+ 	WARN_ON(!refcount_read(&device->refcount));
+ 
+ 	down_write(&devices_rwsem);
+ 	xa_clear_mark(&devices, device->index, DEVICE_REGISTERED);
+ 	up_write(&devices_rwsem);
+ 
+ 	/*
+ 	 * Remove clients in LIFO order, see assign_client_id. This could be
+ 	 * more efficient if xarray learns to reverse iterate. Since no new
+ 	 * clients can be added to this ib_device past this point we only need
+ 	 * the maximum possible client_id value here.
+ 	 */
+ 	down_read(&clients_rwsem);
+ 	cid = highest_client_id;
+ 	up_read(&clients_rwsem);
+ 	while (cid) {
+ 		cid--;
+ 		remove_client_context(device, cid);
+ 	}
+ 
+ 	/* Pairs with refcount_set in enable_device */
+ 	ib_device_put(device);
+ 	wait_for_completion(&device->unreg_completion);
+ 
+ 	/*
+ 	 * compat devices must be removed after device refcount drops to zero.
+ 	 * Otherwise init_net() may add more compatdevs after removing compat
+ 	 * devices and before device is disabled.
+ 	 */
+ 	remove_compat_devs(device);
+ }
+ 
+ /*
+  * An enabled device is visible to all clients and to all the public facing
+  * APIs that return a device pointer. This always returns with a new get, even
+  * if it fails.
+  */
+ static int enable_device_and_get(struct ib_device *device)
+ {
+ 	struct ib_client *client;
+ 	unsigned long index;
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * One ref belongs to the xa and the other belongs to this
+ 	 * thread. This is needed to guard against parallel unregistration.
+ 	 */
+ 	refcount_set(&device->refcount, 2);
+ 	down_write(&devices_rwsem);
+ 	xa_set_mark(&devices, device->index, DEVICE_REGISTERED);
+ 
+ 	/*
+ 	 * By using downgrade_write() we ensure that no other thread can clear
+ 	 * DEVICE_REGISTERED while we are completing the client setup.
+ 	 */
+ 	downgrade_write(&devices_rwsem);
+ 
+ 	if (device->ops.enable_driver) {
+ 		ret = device->ops.enable_driver(device);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	down_read(&clients_rwsem);
+ 	xa_for_each_marked (&clients, index, client, CLIENT_REGISTERED) {
+ 		ret = add_client_context(device, client);
+ 		if (ret)
+ 			break;
+ 	}
+ 	up_read(&clients_rwsem);
+ 	if (!ret)
+ 		ret = add_compat_devs(device);
+ out:
+ 	up_read(&devices_rwsem);
+ 	return ret;
+ }
+ 
++>>>>>>> 9cd5881719e9 (RDMA/devices: Remove the lock around remove_client_context)
  /**
 - * ib_register_device - Register an IB device with IB core
 - * @device:Device to register
 - *
 - * Low-level drivers use ib_register_device() to register their
 - * devices with the IB core.  All registered clients will receive a
 - * callback for each device that is added. @device must be allocated
 - * with ib_alloc_device().
 - *
 - * If the driver uses ops.dealloc_driver and calls any ib_unregister_device()
 - * asynchronously then the device pointer may become freed as soon as this
 - * function returns.
 - */
 -int ib_register_device(struct ib_device *device, const char *name)
 -{
 -	int ret;
 -
 -	ret = assign_name(device, name);
 -	if (ret)
 -		return ret;
 -
 -	ret = setup_device(device);
 -	if (ret)
 -		return ret;
 -
 -	ret = ib_cache_setup_one(device);
 -	if (ret) {
 -		dev_warn(&device->dev,
 -			 "Couldn't set up InfiniBand P_Key/GID cache\n");
 -		return ret;
 -	}
 -
 -	ib_device_register_rdmacg(device);
 -
 -	rdma_counter_init(device);
 -
 -	/*
 -	 * Ensure that ADD uevent is not fired because it
 -	 * is too early amd device is not initialized yet.
 -	 */
 -	dev_set_uevent_suppress(&device->dev, true);
 -	ret = device_add(&device->dev);
 -	if (ret)
 -		goto cg_cleanup;
 -
 -	ret = ib_device_register_sysfs(device);
 -	if (ret) {
 -		dev_warn(&device->dev,
 -			 "Couldn't register device with driver model\n");
 -		goto dev_cleanup;
 -	}
 -
 -	ret = enable_device_and_get(device);
 -	dev_set_uevent_suppress(&device->dev, false);
 -	/* Mark for userspace that device is ready */
 -	kobject_uevent(&device->dev.kobj, KOBJ_ADD);
 -	if (ret) {
 -		void (*dealloc_fn)(struct ib_device *);
 -
 -		/*
 -		 * If we hit this error flow then we don't want to
 -		 * automatically dealloc the device since the caller is
 -		 * expected to call ib_dealloc_device() after
 -		 * ib_register_device() fails. This is tricky due to the
 -		 * possibility for a parallel unregistration along with this
 -		 * error flow. Since we have a refcount here we know any
 -		 * parallel flow is stopped in disable_device and will see the
 -		 * NULL pointers, causing the responsibility to
 -		 * ib_dealloc_device() to revert back to this thread.
 -		 */
 -		dealloc_fn = device->ops.dealloc_driver;
 -		device->ops.dealloc_driver = NULL;
 -		ib_device_put(device);
 -		__ib_unregister_device(device);
 -		device->ops.dealloc_driver = dealloc_fn;
 -		return ret;
 -	}
 -	ib_device_put(device);
 -
 -	return 0;
 -
 -dev_cleanup:
 -	device_del(&device->dev);
 -cg_cleanup:
 -	dev_set_uevent_suppress(&device->dev, false);
 -	ib_device_unregister_rdmacg(device);
 -	ib_cache_cleanup_one(device);
 -	return ret;
 -}
 -EXPORT_SYMBOL(ib_register_device);
 -
 -/* Callers must hold a get on the device. */
 -static void __ib_unregister_device(struct ib_device *ib_dev)
 -{
 -	/*
 -	 * We have a registration lock so that all the calls to unregister are
 -	 * fully fenced, once any unregister returns the device is truely
 -	 * unregistered even if multiple callers are unregistering it at the
 -	 * same time. This also interacts with the registration flow and
 -	 * provides sane semantics if register and unregister are racing.
 -	 */
 -	mutex_lock(&ib_dev->unregistration_lock);
 -	if (!refcount_read(&ib_dev->refcount))
 -		goto out;
 -
 -	disable_device(ib_dev);
 -
 -	/* Expedite removing unregistered pointers from the hash table */
 -	free_netdevs(ib_dev);
 -
 -	ib_device_unregister_sysfs(ib_dev);
 -	device_del(&ib_dev->dev);
 -	ib_device_unregister_rdmacg(ib_dev);
 -	ib_cache_cleanup_one(ib_dev);
 -
 -	/*
 -	 * Drivers using the new flow may not call ib_dealloc_device except
 -	 * in error unwind prior to registration success.
 -	 */
 -	if (ib_dev->ops.dealloc_driver) {
 -		WARN_ON(kref_read(&ib_dev->dev.kobj.kref) <= 1);
 -		ib_dealloc_device(ib_dev);
 -	}
 -out:
 -	mutex_unlock(&ib_dev->unregistration_lock);
 -}
 -
 -/**
 - * ib_unregister_device - Unregister an IB device
 - * @device: The device to unregister
 - *
 - * Unregister an IB device.  All clients will receive a remove callback.
 - *
 - * Callers should call this routine only once, and protect against races with
 - * registration. Typically it should only be called as part of a remove
 - * callback in an implementation of driver core's struct device_driver and
 - * related.
 - *
 - * If ops.dealloc_driver is used then ib_dev will be freed upon return from
 - * this function.
 - */
 -void ib_unregister_device(struct ib_device *ib_dev)
 -{
 -	get_device(&ib_dev->dev);
 -	__ib_unregister_device(ib_dev);
 -	put_device(&ib_dev->dev);
 -}
 -EXPORT_SYMBOL(ib_unregister_device);
 -
 -/**
 - * ib_unregister_device_and_put - Unregister a device while holding a 'get'
 - * device: The device to unregister
 - *
 - * This is the same as ib_unregister_device(), except it includes an internal
 - * ib_device_put() that should match a 'get' obtained by the caller.
 - *
 - * It is safe to call this routine concurrently from multiple threads while
 - * holding the 'get'. When the function returns the device is fully
 - * unregistered.
 - *
 - * Drivers using this flow MUST use the driver_unregister callback to clean up
 - * their resources associated with the device and dealloc it.
 - */
 -void ib_unregister_device_and_put(struct ib_device *ib_dev)
 -{
 -	WARN_ON(!ib_dev->ops.dealloc_driver);
 -	get_device(&ib_dev->dev);
 -	ib_device_put(ib_dev);
 -	__ib_unregister_device(ib_dev);
 -	put_device(&ib_dev->dev);
 -}
 -EXPORT_SYMBOL(ib_unregister_device_and_put);
 -
 -/**
 - * ib_unregister_driver - Unregister all IB devices for a driver
 - * @driver_id: The driver to unregister
 - *
 - * This implements a fence for device unregistration. It only returns once all
 - * devices associated with the driver_id have fully completed their
 - * unregistration and returned from ib_unregister_device*().
 - *
 - * If device's are not yet unregistered it goes ahead and starts unregistering
 - * them.
 - *
 - * This does not block creation of new devices with the given driver_id, that
 - * is the responsibility of the caller.
 - */
 -void ib_unregister_driver(enum rdma_driver_id driver_id)
 -{
 -	struct ib_device *ib_dev;
 -	unsigned long index;
 -
 -	down_read(&devices_rwsem);
 -	xa_for_each (&devices, index, ib_dev) {
 -		if (ib_dev->ops.driver_id != driver_id)
 -			continue;
 -
 -		get_device(&ib_dev->dev);
 -		up_read(&devices_rwsem);
 -
 -		WARN_ON(!ib_dev->ops.dealloc_driver);
 -		__ib_unregister_device(ib_dev);
 -
 -		put_device(&ib_dev->dev);
 -		down_read(&devices_rwsem);
 -	}
 -	up_read(&devices_rwsem);
 -}
 -EXPORT_SYMBOL(ib_unregister_driver);
 -
 -static void ib_unregister_work(struct work_struct *work)
 -{
 -	struct ib_device *ib_dev =
 -		container_of(work, struct ib_device, unregistration_work);
 -
 -	__ib_unregister_device(ib_dev);
 -	put_device(&ib_dev->dev);
 -}
 -
 -/**
 - * ib_unregister_device_queued - Unregister a device using a work queue
 - * device: The device to unregister
 - *
 - * This schedules an asynchronous unregistration using a WQ for the device. A
 - * driver should use this to avoid holding locks while doing unregistration,
 - * such as holding the RTNL lock.
 - *
 - * Drivers using this API must use ib_unregister_driver before module unload
 - * to ensure that all scheduled unregistrations have completed.
 - */
 -void ib_unregister_device_queued(struct ib_device *ib_dev)
 -{
 -	WARN_ON(!refcount_read(&ib_dev->refcount));
 -	WARN_ON(!ib_dev->ops.dealloc_driver);
 -	get_device(&ib_dev->dev);
 -	if (!queue_work(system_unbound_wq, &ib_dev->unregistration_work))
 -		put_device(&ib_dev->dev);
 -}
 -EXPORT_SYMBOL(ib_unregister_device_queued);
 -
 -/*
 - * The caller must pass in a device that has the kref held and the refcount
 - * released. If the device is in cur_net and still registered then it is moved
 - * into net.
 + * ib_register_device - Register an IB device with IB core
 + * @device:Device to register
 + *
 + * Low-level drivers use ib_register_device() to register their
 + * devices with the IB core.  All registered clients will receive a
 + * callback for each device that is added. @device must be allocated
 + * with ib_alloc_device().
   */
 -static int rdma_dev_change_netns(struct ib_device *device, struct net *cur_net,
 -				 struct net *net)
 +int ib_register_device(struct ib_device *device, const char *name)
  {
 -	int ret2 = -EINVAL;
  	int ret;
 +	struct ib_client *client;
 +	unsigned long index;
  
 -	mutex_lock(&device->unregistration_lock);
 +	setup_dma_device(device);
  
 -	/*
 -	 * If a device not under ib_device_get() or if the unregistration_lock
 -	 * is not held, the namespace can be changed, or it can be unregistered.
 -	 * Check again under the lock.
 -	 */
 -	if (refcount_read(&device->refcount) == 0 ||
 -	    !net_eq(cur_net, read_pnet(&device->coredev.rdma_net))) {
 -		ret = -ENODEV;
 +	mutex_lock(&device_mutex);
 +
 +	if (strchr(name, '%')) {
 +		ret = alloc_name(device, name);
 +		if (ret)
 +			goto out;
 +	} else {
 +		ret = dev_set_name(&device->dev, name);
 +		if (ret)
 +			goto out;
 +	}
 +	if (__ib_device_get_by_name(dev_name(&device->dev))) {
 +		ret = -ENFILE;
  		goto out;
  	}
 +	strlcpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);
  
 -	kobject_uevent(&device->dev.kobj, KOBJ_REMOVE);
 -	disable_device(device);
 +	ret = setup_device(device);
 +	if (ret)
 +		goto out;
  
 -	/*
 -	 * At this point no one can be using the device, so it is safe to
 -	 * change the namespace.
 -	 */
 -	write_pnet(&device->coredev.rdma_net, net);
 +	ret = ib_cache_setup_one(device);
 +	if (ret) {
 +		dev_warn(&device->dev,
 +			 "Couldn't set up InfiniBand P_Key/GID cache\n");
 +		goto out;
 +	}
  
 -	down_read(&devices_rwsem);
 -	/*
 -	 * Currently rdma devices are system wide unique. So the device name
 -	 * is guaranteed free in the new namespace. Publish the new namespace
 -	 * at the sysfs level.
 -	 */
 -	ret = device_rename(&device->dev, dev_name(&device->dev));
 -	up_read(&devices_rwsem);
 +	device->index = __dev_new_index();
 +
 +	ret = ib_device_register_rdmacg(device);
  	if (ret) {
  		dev_warn(&device->dev,
 -			 "%s: Couldn't rename device after namespace change\n",
 -			 __func__);
 -		/* Try and put things back and re-enable the device */
 -		write_pnet(&device->coredev.rdma_net, cur_net);
 +			 "Couldn't register device with rdma cgroup\n");
 +		goto dev_cleanup;
  	}
  
 -	ret2 = enable_device_and_get(device);
 -	if (ret2) {
 -		/*
 -		 * This shouldn't really happen, but if it does, let the user
 -		 * retry at later point. So don't disable the device.
 -		 */
 +	ret = ib_device_register_sysfs(device);
 +	if (ret) {
  		dev_warn(&device->dev,
 -			 "%s: Couldn't re-enable device after namespace change\n",
 -			 __func__);
 +			 "Couldn't register device with driver model\n");
 +		goto cg_cleanup;
  	}
 -	kobject_uevent(&device->dev.kobj, KOBJ_ADD);
  
 -	ib_device_put(device);
 +	refcount_set(&device->refcount, 1);
 +
 +	xa_for_each_marked (&clients, index, client, CLIENT_REGISTERED)
 +		if (!add_client_context(device, client) && client->add)
 +			client->add(device);
 +
 +	down_write(&lists_rwsem);
 +	list_add_tail(&device->core_list, &device_list);
 +	up_write(&lists_rwsem);
 +	mutex_unlock(&device_mutex);
 +	return 0;
 +
 +cg_cleanup:
 +	ib_device_unregister_rdmacg(device);
 +dev_cleanup:
 +	ib_cache_cleanup_one(device);
  out:
 -	mutex_unlock(&device->unregistration_lock);
 -	if (ret)
 -		return ret;
 -	return ret2;
 +	mutex_unlock(&device_mutex);
 +	return ret;
  }
 +EXPORT_SYMBOL(ib_register_device);
  
 -int ib_device_set_netns_put(struct sk_buff *skb,
 -			    struct ib_device *dev, u32 ns_fd)
 +/**
 + * ib_unregister_device - Unregister an IB device
 + * @device:Device to unregister
 + *
 + * Unregister an IB device.  All clients will receive a remove callback.
 + */
 +void ib_unregister_device(struct ib_device *device)
  {
 -	struct net *net;
 -	int ret;
 -
 -	net = get_net_ns_by_fd(ns_fd);
 -	if (IS_ERR(net)) {
 -		ret = PTR_ERR(net);
 -		goto net_err;
 -	}
 -
 -	if (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN)) {
 -		ret = -EPERM;
 -		goto ns_err;
 -	}
 +	struct ib_client_data *context, *tmp;
 +	unsigned long flags;
  
  	/*
 -	 * Currently supported only for those providers which support
 -	 * disassociation and don't do port specific sysfs init. Once a
 -	 * port_cleanup infrastructure is implemented, this limitation will be
 -	 * removed.
 +	 * Wait for all netlink command callers to finish working on the
 +	 * device.
  	 */
 -	if (!dev->ops.disassociate_ucontext || dev->ops.init_port ||
 -	    ib_devices_shared_netns) {
 -		ret = -EOPNOTSUPP;
 -		goto ns_err;
 +	ib_device_put(device);
 +	wait_for_completion(&device->unreg_completion);
 +
 +	mutex_lock(&device_mutex);
 +
 +	down_write(&lists_rwsem);
 +	list_del(&device->core_list);
 +	write_lock_irq(&device->client_data_lock);
 +	list_for_each_entry(context, &device->client_data_list, list)
 +		context->going_down = true;
 +	write_unlock_irq(&device->client_data_lock);
 +	downgrade_write(&lists_rwsem);
 +
 +	list_for_each_entry(context, &device->client_data_list, list) {
 +		if (context->client->remove)
 +			context->client->remove(device, context->data);
  	}
 +	up_read(&lists_rwsem);
  
 -	get_device(&dev->dev);
 -	ib_device_put(dev);
 -	ret = rdma_dev_change_netns(dev, current->nsproxy->net_ns, net);
 -	put_device(&dev->dev);
 +	ib_device_unregister_sysfs(device);
 +	ib_device_unregister_rdmacg(device);
  
 -	put_net(net);
 -	return ret;
 +	mutex_unlock(&device_mutex);
  
 -ns_err:
 -	put_net(net);
 -net_err:
 -	ib_device_put(dev);
 -	return ret;
 -}
 +	ib_cache_cleanup_one(device);
  
 -static struct pernet_operations rdma_dev_net_ops = {
 -	.init = rdma_dev_init_net,
 -	.exit = rdma_dev_exit_net,
 -	.id = &rdma_dev_net_id,
 -	.size = sizeof(struct rdma_dev_net),
 -};
 +	down_write(&lists_rwsem);
 +	write_lock_irqsave(&device->client_data_lock, flags);
 +	list_for_each_entry_safe(context, tmp, &device->client_data_list,
 +				 list) {
 +		list_del(&context->list);
 +		kfree(context);
 +	}
 +	write_unlock_irqrestore(&device->client_data_lock, flags);
 +	up_write(&lists_rwsem);
 +}
 +EXPORT_SYMBOL(ib_unregister_device);
  
  static int assign_client_id(struct ib_client *client)
  {
@@@ -707,22 -1683,19 +829,31 @@@
  	/*
  	 * The add/remove callbacks must be called in FIFO/LIFO order. To
  	 * achieve this we assign client_ids so they are sorted in
- 	 * registration order, and retain a linked list we can reverse iterate
- 	 * to get the LIFO order. The extra linked list can go away if xarray
- 	 * learns to reverse iterate.
+ 	 * registration order.
  	 */
++<<<<<<< HEAD
 +	if (list_empty(&client_list))
 +		client->client_id = 0;
 +	else
 +		client->client_id =
 +			list_last_entry(&client_list, struct ib_client, list)
 +				->client_id;
 +	ret = xa_alloc(&clients, &client->client_id, INT_MAX, client,
 +		       GFP_KERNEL);
 +	if (ret)
 +		goto out;
 +
++=======
+ 	client->client_id = highest_client_id;
+ 	ret = xa_insert(&clients, client->client_id, client, GFP_KERNEL);
+ 	if (ret)
+ 		goto out;
+ 
+ 	highest_client_id++;
+ 	xa_set_mark(&clients, client->client_id, CLIENT_REGISTERED);
+ 
++>>>>>>> 9cd5881719e9 (RDMA/devices: Remove the lock around remove_client_context)
  out:
 -	up_write(&clients_rwsem);
  	return ret;
  }
  
@@@ -775,53 -1758,37 +916,62 @@@ EXPORT_SYMBOL(ib_register_client)
   */
  void ib_unregister_client(struct ib_client *client)
  {
 +	struct ib_client_data *context;
  	struct ib_device *device;
 -	unsigned long index;
  
 -	down_write(&clients_rwsem);
 -	ib_client_put(client);
 +	mutex_lock(&device_mutex);
 +
 +	down_write(&lists_rwsem);
  	xa_clear_mark(&clients, client->client_id, CLIENT_REGISTERED);
 -	up_write(&clients_rwsem);
 +	up_write(&lists_rwsem);
  
 -	/* We do not want to have locks while calling client->remove() */
 -	rcu_read_lock();
 -	xa_for_each (&devices, index, device) {
 -		if (!ib_device_try_get(device))
 -			continue;
 -		rcu_read_unlock();
 +	list_for_each_entry(device, &device_list, core_list) {
 +		struct ib_client_data *found_context = NULL;
 +
 +		down_write(&lists_rwsem);
 +		write_lock_irq(&device->client_data_lock);
 +		list_for_each_entry(context, &device->client_data_list, list)
 +			if (context->client == client) {
 +				context->going_down = true;
 +				found_context = context;
 +				break;
 +			}
 +		write_unlock_irq(&device->client_data_lock);
 +		up_write(&lists_rwsem);
  
 -		remove_client_context(device, client->client_id);
 +		if (client->remove)
 +			client->remove(device, found_context ?
 +					       found_context->data : NULL);
 +
 +		if (!found_context) {
 +			dev_warn(&device->dev,
 +				 "No client context found for %s\n",
 +				 client->name);
 +			continue;
 +		}
  
 -		ib_device_put(device);
 -		rcu_read_lock();
 +		down_write(&lists_rwsem);
 +		write_lock_irq(&device->client_data_lock);
 +		list_del(&found_context->list);
 +		write_unlock_irq(&device->client_data_lock);
 +		up_write(&lists_rwsem);
 +		kfree(found_context);
  	}
 -	rcu_read_unlock();
  
++<<<<<<< HEAD
 +	down_write(&lists_rwsem);
 +	list_del(&client->list);
 +	xa_erase(&clients, client->client_id);
 +	up_write(&lists_rwsem);
 +	mutex_unlock(&device_mutex);
++=======
+ 	/*
+ 	 * remove_client_context() is not a fence, it can return even though a
+ 	 * removal is ongoing. Wait until all removals are completed.
+ 	 */
+ 	wait_for_completion(&client->uses_zero);
+ 	remove_client_id(client);
++>>>>>>> 9cd5881719e9 (RDMA/devices: Remove the lock around remove_client_context)
  }
  EXPORT_SYMBOL(ib_unregister_client);
  
diff --cc include/rdma/ib_verbs.h
index 9a1c4c9437dd,4f225175cb91..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -2529,7 -2647,9 +2529,13 @@@ struct ib_client 
  			const union ib_gid *gid,
  			const struct sockaddr *addr,
  			void *client_data);
++<<<<<<< HEAD
 +	struct list_head list;
++=======
+ 
+ 	refcount_t uses;
+ 	struct completion uses_zero;
++>>>>>>> 9cd5881719e9 (RDMA/devices: Remove the lock around remove_client_context)
  	u32 client_id;
  
  	/* kverbs are not required by the client */
* Unmerged path drivers/infiniband/core/device.c
* Unmerged path include/rdma/ib_verbs.h
