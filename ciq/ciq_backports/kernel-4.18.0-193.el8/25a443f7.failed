net: sched: allow indirect blocks to bind to clsact in TC

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] sched: allow indirect blocks to bind to clsact in TC (Ivan Vecera) [1789862]
Rebuild_FUZZ: 95.41%
commit-author John Hurley <john.hurley@netronome.com>
commit 25a443f74bcff2c4d506a39eae62fc15ad7c618a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/25a443f7.failed

When a device is bound to a clsact qdisc, bind events are triggered to
registered drivers for both ingress and egress. However, if a driver
registers to such a device using the indirect block routines then it is
assumed that it is only interested in ingress offload and so only replays
ingress bind/unbind messages.

The NFP driver supports the offload of some egress filters when
registering to a block with qdisc of type clsact. However, on unregister,
if the block is still active, it will not receive an unbind egress
notification which can prevent proper cleanup of other registered
callbacks.

Modify the indirect block callback command in TC to send messages of
ingress and/or egress bind depending on the qdisc in use. NFP currently
supports egress offload for TC flower offload so the changes are only
added to TC.

Fixes: 4d12ba42787b ("nfp: flower: allow offloading of matches on 'internal' ports")
	Signed-off-by: John Hurley <john.hurley@netronome.com>
	Acked-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 25a443f74bcff2c4d506a39eae62fc15ad7c618a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_api.c
diff --cc net/sched/cls_api.c
index 36ca1f338da1,3c335fd9bcb0..000000000000
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@@ -559,9 -623,39 +559,43 @@@ static void tcf_chain_flush(struct tcf_
  	}
  }
  
++<<<<<<< HEAD
 +static struct tcf_block *tc_dev_ingress_block(struct net_device *dev)
++=======
+ static int tcf_block_setup(struct tcf_block *block,
+ 			   struct flow_block_offload *bo);
+ 
+ static void tc_indr_block_cmd(struct net_device *dev, struct tcf_block *block,
+ 			      flow_indr_block_bind_cb_t *cb, void *cb_priv,
+ 			      enum flow_block_command command, bool ingress)
+ {
+ 	struct flow_block_offload bo = {
+ 		.command	= command,
+ 		.binder_type	= ingress ?
+ 				  FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS :
+ 				  FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS,
+ 		.net		= dev_net(dev),
+ 		.block_shared	= tcf_block_non_null_shared(block),
+ 	};
+ 	INIT_LIST_HEAD(&bo.cb_list);
+ 
+ 	if (!block)
+ 		return;
+ 
+ 	bo.block = &block->flow_block;
+ 
+ 	down_write(&block->cb_lock);
+ 	cb(dev, cb_priv, TC_SETUP_BLOCK, &bo);
+ 
+ 	tcf_block_setup(block, &bo);
+ 	up_write(&block->cb_lock);
+ }
+ 
+ static struct tcf_block *tc_dev_block(struct net_device *dev, bool ingress)
++>>>>>>> 25a443f74bcf (net: sched: allow indirect blocks to bind to clsact in TC)
  {
  	const struct Qdisc_class_ops *cops;
+ 	const struct Qdisc_ops *ops;
  	struct Qdisc *qdisc;
  
  	if (!dev_ingress_queue(dev))
@@@ -578,217 -679,37 +619,234 @@@
  	if (!cops->tcf_block)
  		return NULL;
  
- 	return cops->tcf_block(qdisc, TC_H_MIN_INGRESS, NULL);
+ 	return cops->tcf_block(qdisc,
+ 			       ingress ? TC_H_MIN_INGRESS : TC_H_MIN_EGRESS,
+ 			       NULL);
  }
  
++<<<<<<< HEAD
 +static struct rhashtable indr_setup_block_ht;
 +
 +struct tc_indr_block_dev {
 +	struct rhash_head ht_node;
 +	struct net_device *dev;
 +	unsigned int refcnt;
 +	struct list_head cb_list;
 +	struct tcf_block *block;
 +};
 +
 +struct tc_indr_block_cb {
 +	struct list_head list;
 +	void *cb_priv;
 +	tc_indr_block_bind_cb_t *cb;
 +	void *cb_ident;
 +};
 +
 +static const struct rhashtable_params tc_indr_setup_block_ht_params = {
 +	.key_offset	= offsetof(struct tc_indr_block_dev, dev),
 +	.head_offset	= offsetof(struct tc_indr_block_dev, ht_node),
 +	.key_len	= sizeof(struct net_device *),
 +};
 +
 +static struct tc_indr_block_dev *
 +tc_indr_block_dev_lookup(struct net_device *dev)
 +{
 +	return rhashtable_lookup_fast(&indr_setup_block_ht, &dev,
 +				      tc_indr_setup_block_ht_params);
++=======
+ static void tc_indr_block_get_and_cmd(struct net_device *dev,
+ 				      flow_indr_block_bind_cb_t *cb,
+ 				      void *cb_priv,
+ 				      enum flow_block_command command)
+ {
+ 	struct tcf_block *block;
+ 
+ 	block = tc_dev_block(dev, true);
+ 	tc_indr_block_cmd(dev, block, cb, cb_priv, command, true);
+ 
+ 	block = tc_dev_block(dev, false);
+ 	tc_indr_block_cmd(dev, block, cb, cb_priv, command, false);
++>>>>>>> 25a443f74bcf (net: sched: allow indirect blocks to bind to clsact in TC)
 +}
 +
 +static struct tc_indr_block_dev *tc_indr_block_dev_get(struct net_device *dev)
 +{
 +	struct tc_indr_block_dev *indr_dev;
 +
 +	indr_dev = tc_indr_block_dev_lookup(dev);
 +	if (indr_dev)
 +		goto inc_ref;
 +
 +	indr_dev = kzalloc(sizeof(*indr_dev), GFP_KERNEL);
 +	if (!indr_dev)
 +		return NULL;
 +
 +	INIT_LIST_HEAD(&indr_dev->cb_list);
 +	indr_dev->dev = dev;
 +	indr_dev->block = tc_dev_ingress_block(dev);
 +	if (rhashtable_insert_fast(&indr_setup_block_ht, &indr_dev->ht_node,
 +				   tc_indr_setup_block_ht_params)) {
 +		kfree(indr_dev);
 +		return NULL;
 +	}
 +
 +inc_ref:
 +	indr_dev->refcnt++;
 +	return indr_dev;
 +}
 +
 +static void tc_indr_block_dev_put(struct tc_indr_block_dev *indr_dev)
 +{
 +	if (--indr_dev->refcnt)
 +		return;
 +
 +	rhashtable_remove_fast(&indr_setup_block_ht, &indr_dev->ht_node,
 +			       tc_indr_setup_block_ht_params);
 +	kfree(indr_dev);
 +}
 +
 +static struct tc_indr_block_cb *
 +tc_indr_block_cb_lookup(struct tc_indr_block_dev *indr_dev,
 +			tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct tc_indr_block_cb *indr_block_cb;
 +
 +	list_for_each_entry(indr_block_cb, &indr_dev->cb_list, list)
 +		if (indr_block_cb->cb == cb &&
 +		    indr_block_cb->cb_ident == cb_ident)
 +			return indr_block_cb;
 +	return NULL;
 +}
 +
 +static struct tc_indr_block_cb *
 +tc_indr_block_cb_add(struct tc_indr_block_dev *indr_dev, void *cb_priv,
 +		     tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct tc_indr_block_cb *indr_block_cb;
 +
 +	indr_block_cb = tc_indr_block_cb_lookup(indr_dev, cb, cb_ident);
 +	if (indr_block_cb)
 +		return ERR_PTR(-EEXIST);
 +
 +	indr_block_cb = kzalloc(sizeof(*indr_block_cb), GFP_KERNEL);
 +	if (!indr_block_cb)
 +		return ERR_PTR(-ENOMEM);
 +
 +	indr_block_cb->cb_priv = cb_priv;
 +	indr_block_cb->cb = cb;
 +	indr_block_cb->cb_ident = cb_ident;
 +	list_add(&indr_block_cb->list, &indr_dev->cb_list);
 +
 +	return indr_block_cb;
 +}
 +
 +static void tc_indr_block_cb_del(struct tc_indr_block_cb *indr_block_cb)
 +{
 +	list_del(&indr_block_cb->list);
 +	kfree(indr_block_cb);
  }
  
 -static void tc_indr_block_call(struct tcf_block *block,
 -			       struct net_device *dev,
 +static int tcf_block_setup(struct tcf_block *block,
 +			   struct flow_block_offload *bo);
 +
 +static void tc_indr_block_ing_cmd(struct tc_indr_block_dev *indr_dev,
 +				  struct tc_indr_block_cb *indr_block_cb,
 +				  enum tc_block_command command)
 +{
 +	struct tc_block_offload bo = {
 +		.command	= command,
 +		.binder_type	= TCF_BLOCK_BINDER_TYPE_CLSACT_INGRESS,
 +		.block		= indr_dev->block,
 +	};
 +	INIT_LIST_HEAD(&bo.cb_list);
 +
 +	if (!indr_dev->block)
 +		return;
 +
 +	indr_block_cb->cb(indr_dev->dev, indr_block_cb->cb_priv, TC_SETUP_BLOCK,
 +			  &bo);
 +	tcf_block_setup(indr_dev->block, &bo);
 +}
 +
 +int __tc_indr_block_cb_register(struct net_device *dev, void *cb_priv,
 +				tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct tc_indr_block_cb *indr_block_cb;
 +	struct tc_indr_block_dev *indr_dev;
 +	int err;
 +
 +	indr_dev = tc_indr_block_dev_get(dev);
 +	if (!indr_dev)
 +		return -ENOMEM;
 +
 +	indr_block_cb = tc_indr_block_cb_add(indr_dev, cb_priv, cb, cb_ident);
 +	err = PTR_ERR_OR_ZERO(indr_block_cb);
 +	if (err)
 +		goto err_dev_put;
 +
 +	tc_indr_block_ing_cmd(indr_dev, indr_block_cb, TC_BLOCK_BIND);
 +	return 0;
 +
 +err_dev_put:
 +	tc_indr_block_dev_put(indr_dev);
 +	return err;
 +}
 +EXPORT_SYMBOL_GPL(__tc_indr_block_cb_register);
 +
 +int tc_indr_block_cb_register(struct net_device *dev, void *cb_priv,
 +			      tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	int err;
 +
 +	rtnl_lock();
 +	err = __tc_indr_block_cb_register(dev, cb_priv, cb, cb_ident);
 +	rtnl_unlock();
 +
 +	return err;
 +}
 +EXPORT_SYMBOL_GPL(tc_indr_block_cb_register);
 +
 +void __tc_indr_block_cb_unregister(struct net_device *dev,
 +				   tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct tc_indr_block_cb *indr_block_cb;
 +	struct tc_indr_block_dev *indr_dev;
 +
 +	indr_dev = tc_indr_block_dev_lookup(dev);
 +	if (!indr_dev)
 +		return;
 +
 +	indr_block_cb = tc_indr_block_cb_lookup(indr_dev, cb, cb_ident);
 +	if (!indr_block_cb)
 +		return;
 +
 +	/* Send unbind message if required to free any block cbs. */
 +	tc_indr_block_ing_cmd(indr_dev, indr_block_cb, TC_BLOCK_UNBIND);
 +	tc_indr_block_cb_del(indr_block_cb);
 +	tc_indr_block_dev_put(indr_dev);
 +}
 +EXPORT_SYMBOL_GPL(__tc_indr_block_cb_unregister);
 +
 +void tc_indr_block_cb_unregister(struct net_device *dev,
 +				 tc_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	rtnl_lock();
 +	__tc_indr_block_cb_unregister(dev, cb, cb_ident);
 +	rtnl_unlock();
 +}
 +EXPORT_SYMBOL_GPL(tc_indr_block_cb_unregister);
 +
 +static void tc_indr_block_call(struct tcf_block *block, struct net_device *dev,
  			       struct tcf_block_ext_info *ei,
 -			       enum flow_block_command command,
 +			       enum tc_block_command command,
  			       struct netlink_ext_ack *extack)
  {
 -	struct flow_block_offload bo = {
 +	struct tc_indr_block_cb *indr_block_cb;
 +	struct tc_indr_block_dev *indr_dev;
 +	struct tc_block_offload bo = {
  		.command	= command,
  		.binder_type	= ei->binder_type,
 -		.net		= dev_net(dev),
 -		.block		= &block->flow_block,
 -		.block_shared	= tcf_block_shared(block),
 +		.block		= block,
  		.extack		= extack,
  	};
  	INIT_LIST_HEAD(&bo.cb_list);
@@@ -3446,6 -3640,11 +3504,14 @@@ static struct pernet_operations tcf_net
  	.size = sizeof(struct tcf_net),
  };
  
++<<<<<<< HEAD
++=======
+ static struct flow_indr_block_entry block_entry = {
+ 	.cb = tc_indr_block_get_and_cmd,
+ 	.list = LIST_HEAD_INIT(block_entry.list),
+ };
+ 
++>>>>>>> 25a443f74bcf (net: sched: allow indirect blocks to bind to clsact in TC)
  static int __init tc_filter_init(void)
  {
  	int err;
@@@ -3458,10 -3657,7 +3524,14 @@@
  	if (err)
  		goto err_register_pernet_subsys;
  
++<<<<<<< HEAD
 +	err = rhashtable_init(&indr_setup_block_ht,
 +			      &tc_indr_setup_block_ht_params);
 +	if (err)
 +		goto err_rhash_setup_block_ht;
++=======
+ 	flow_indr_add_block_cb(&block_entry);
++>>>>>>> 25a443f74bcf (net: sched: allow indirect blocks to bind to clsact in TC)
  
  	rtnl_register(PF_UNSPEC, RTM_NEWTFILTER, tc_new_tfilter, NULL,
  		      RTNL_FLAG_DOIT_UNLOCKED);
* Unmerged path net/sched/cls_api.c
