mm/hmm: allow to mirror vma of a file on a DAX backed filesystem

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] hmm: allow to mirror vma of a file on a DAX backed filesystem (Jerome Glisse) [1498655 1597758]
Rebuild_FUZZ: 97.60%
commit-author Jérôme Glisse <jglisse@redhat.com>
commit 992de9a8b7511673156df7d2bb1039dea3b2f7f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/992de9a8.failed

HMM mirror is a device driver helpers to mirror range of virtual address.
It means that the process jobs running on the device can access the same
virtual address as the CPU threads of that process.  This patch adds
support for mirroring mapping of file that are on a DAX block device (ie
range of virtual address that is an mmap of a file in a filesystem on a
DAX block device).  There is no reason to not support such case when
mirroring virtual address on a device.

Note that unlike GUP code we do not take page reference hence when we
back-off we have nothing to undo.

[jglisse@redhat.com: move THP and hugetlbfs code path behind #if KCONFIG]
  Link: http://lkml.kernel.org/r/20190422163741.13029-1-jglisse@redhat.com
Link: http://lkml.kernel.org/r/20190403193318.16478-10-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 992de9a8b7511673156df7d2bb1039dea3b2f7f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 4c052ccc4e21,b1c9b05bf26f..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -660,6 -759,140 +710,143 @@@ again
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int hmm_vma_walk_pud(pud_t *pudp,
+ 			    unsigned long start,
+ 			    unsigned long end,
+ 			    struct mm_walk *walk)
+ {
+ 	struct hmm_vma_walk *hmm_vma_walk = walk->private;
+ 	struct hmm_range *range = hmm_vma_walk->range;
+ 	unsigned long addr = start, next;
+ 	pmd_t *pmdp;
+ 	pud_t pud;
+ 	int ret;
+ 
+ again:
+ 	pud = READ_ONCE(*pudp);
+ 	if (pud_none(pud))
+ 		return hmm_vma_walk_hole(start, end, walk);
+ 
+ 	if (pud_huge(pud) && pud_devmap(pud)) {
+ 		unsigned long i, npages, pfn;
+ 		uint64_t *pfns, cpu_flags;
+ 		bool fault, write_fault;
+ 
+ 		if (!pud_present(pud))
+ 			return hmm_vma_walk_hole(start, end, walk);
+ 
+ 		i = (addr - range->start) >> PAGE_SHIFT;
+ 		npages = (end - addr) >> PAGE_SHIFT;
+ 		pfns = &range->pfns[i];
+ 
+ 		cpu_flags = pud_to_hmm_pfn_flags(range, pud);
+ 		hmm_range_need_fault(hmm_vma_walk, pfns, npages,
+ 				     cpu_flags, &fault, &write_fault);
+ 		if (fault || write_fault)
+ 			return hmm_vma_walk_hole_(addr, end, fault,
+ 						write_fault, walk);
+ 
+ #ifdef CONFIG_HUGETLB_PAGE
+ 		pfn = pud_pfn(pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 		for (i = 0; i < npages; ++i, ++pfn) {
+ 			hmm_vma_walk->pgmap = get_dev_pagemap(pfn,
+ 					      hmm_vma_walk->pgmap);
+ 			if (unlikely(!hmm_vma_walk->pgmap))
+ 				return -EBUSY;
+ 			pfns[i] = hmm_pfn_from_pfn(range, pfn) | cpu_flags;
+ 		}
+ 		if (hmm_vma_walk->pgmap) {
+ 			put_dev_pagemap(hmm_vma_walk->pgmap);
+ 			hmm_vma_walk->pgmap = NULL;
+ 		}
+ 		hmm_vma_walk->last = end;
+ 		return 0;
+ #else
+ 		return -EINVAL;
+ #endif
+ 	}
+ 
+ 	split_huge_pud(walk->vma, pudp, addr);
+ 	if (pud_none(*pudp))
+ 		goto again;
+ 
+ 	pmdp = pmd_offset(pudp, addr);
+ 	do {
+ 		next = pmd_addr_end(addr, end);
+ 		ret = hmm_vma_walk_pmd(pmdp, addr, next, walk);
+ 		if (ret)
+ 			return ret;
+ 	} while (pmdp++, addr = next, addr != end);
+ 
+ 	return 0;
+ }
+ 
+ static int hmm_vma_walk_hugetlb_entry(pte_t *pte, unsigned long hmask,
+ 				      unsigned long start, unsigned long end,
+ 				      struct mm_walk *walk)
+ {
+ #ifdef CONFIG_HUGETLB_PAGE
+ 	unsigned long addr = start, i, pfn, mask, size, pfn_inc;
+ 	struct hmm_vma_walk *hmm_vma_walk = walk->private;
+ 	struct hmm_range *range = hmm_vma_walk->range;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	struct hstate *h = hstate_vma(vma);
+ 	uint64_t orig_pfn, cpu_flags;
+ 	bool fault, write_fault;
+ 	spinlock_t *ptl;
+ 	pte_t entry;
+ 	int ret = 0;
+ 
+ 	size = 1UL << huge_page_shift(h);
+ 	mask = size - 1;
+ 	if (range->page_shift != PAGE_SHIFT) {
+ 		/* Make sure we are looking at full page. */
+ 		if (start & mask)
+ 			return -EINVAL;
+ 		if (end < (start + size))
+ 			return -EINVAL;
+ 		pfn_inc = size >> PAGE_SHIFT;
+ 	} else {
+ 		pfn_inc = 1;
+ 		size = PAGE_SIZE;
+ 	}
+ 
+ 
+ 	ptl = huge_pte_lock(hstate_vma(walk->vma), walk->mm, pte);
+ 	entry = huge_ptep_get(pte);
+ 
+ 	i = (start - range->start) >> range->page_shift;
+ 	orig_pfn = range->pfns[i];
+ 	range->pfns[i] = range->values[HMM_PFN_NONE];
+ 	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
+ 	fault = write_fault = false;
+ 	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
+ 			   &fault, &write_fault);
+ 	if (fault || write_fault) {
+ 		ret = -ENOENT;
+ 		goto unlock;
+ 	}
+ 
+ 	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
+ 	for (; addr < end; addr += size, i++, pfn += pfn_inc)
+ 		range->pfns[i] = hmm_pfn_from_pfn(range, pfn) | cpu_flags;
+ 	hmm_vma_walk->last = end;
+ 
+ unlock:
+ 	spin_unlock(ptl);
+ 
+ 	if (ret == -ENOENT)
+ 		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
+ 
+ 	return ret;
+ #else /* CONFIG_HUGETLB_PAGE */
+ 	return -EINVAL;
+ #endif
+ }
+ 
++>>>>>>> 992de9a8b751 (mm/hmm: allow to mirror vma of a file on a DAX backed filesystem)
  static void hmm_pfns_clear(struct hmm_range *range,
  			   uint64_t *pfns,
  			   unsigned long addr,
@@@ -669,279 -902,286 +856,382 @@@
  		*pfns = range->values[HMM_PFN_NONE];
  }
  
- static void hmm_pfns_special(struct hmm_range *range)
- {
- 	unsigned long addr = range->start, i = 0;
- 
- 	for (; addr < range->end; addr += PAGE_SIZE, i++)
- 		range->pfns[i] = range->values[HMM_PFN_SPECIAL];
- }
- 
  /*
 - * hmm_range_register() - start tracking change to CPU page table over a range
 - * @range: range
 - * @mm: the mm struct for the range of virtual address
 - * @start: start virtual address (inclusive)
 - * @end: end virtual address (exclusive)
 - * @page_shift: expect page shift for the range
 - * Returns 0 on success, -EFAULT if the address space is no longer valid
 + * hmm_vma_get_pfns() - snapshot CPU page table for a range of virtual addresses
 + * @range: range being snapshotted
 + * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          vma permission, 0 success
   *
 - * Track updates to the CPU page table see include/linux/hmm.h
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See hmm_vma_range_done() for further
 + * information.
 + *
 + * The range struct is initialized here. It tracks the CPU page table, but only
 + * if the function returns success (0), in which case the caller must then call
 + * hmm_vma_range_done() to stop CPU page table update tracking on this range.
 + *
 + * NOT CALLING hmm_vma_range_done() IF FUNCTION RETURNS 0 WILL LEAD TO SERIOUS
 + * MEMORY CORRUPTION ! YOU HAVE BEEN WARNED !
   */
 -int hmm_range_register(struct hmm_range *range,
 -		       struct mm_struct *mm,
 -		       unsigned long start,
 -		       unsigned long end,
 -		       unsigned page_shift)
 +int hmm_vma_get_pfns(struct hmm_range *range)
  {
 -	unsigned long mask = ((1UL << page_shift) - 1UL);
 -
 -	range->valid = false;
 -	range->hmm = NULL;
 +	struct vm_area_struct *vma = range->vma;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct mm_walk mm_walk;
 +	struct hmm *hmm;
  
 -	if ((start & mask) || (end & mask))
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
  		return -EINVAL;
 -	if (start >= end)
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
  		return -EINVAL;
  
 -	range->page_shift = page_shift;
 -	range->start = start;
 -	range->end = end;
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm)
 +		return -ENOMEM;
 +	/* Caller must have registered a mirror, via hmm_mirror_register() ! */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
  
 -	range->hmm = hmm_get_or_create(mm);
 -	if (!range->hmm)
 -		return -EFAULT;
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (range->hmm->mm == NULL || range->hmm->dead) {
 -		hmm_put(range->hmm);
 -		return -EFAULT;
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
  	}
  
  	/* Initialize range to track CPU page table update */
 -	mutex_lock(&range->hmm->lock);
 -
 -	list_add_rcu(&range->list, &range->hmm->ranges);
 -
 -	/*
 -	 * If there are any concurrent notifiers we have to wait for them for
 -	 * the range to be valid (see hmm_range_wait_until_valid()).
 -	 */
 -	if (!range->hmm->notifiers)
 -		range->valid = true;
 -	mutex_unlock(&range->hmm->lock);
 -
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = false;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	walk_page_range(range->start, range->end, &mm_walk);
  	return 0;
  }
 -EXPORT_SYMBOL(hmm_range_register);
 +EXPORT_SYMBOL(hmm_vma_get_pfns);
  
  /*
 - * hmm_range_unregister() - stop tracking change to CPU page table over a range
 - * @range: range
 + * hmm_vma_range_done() - stop tracking change to CPU page table over a range
 + * @range: range being tracked
 + * Returns: false if range data has been invalidated, true otherwise
   *
   * Range struct is used to track updates to the CPU page table after a call to
 - * hmm_range_register(). See include/linux/hmm.h for how to use it.
 - */
 -void hmm_range_unregister(struct hmm_range *range)
 -{
 -	/* Sanity check this really should not happen. */
 -	if (range->hmm == NULL || range->end <= range->start)
 -		return;
 -
 -	mutex_lock(&range->hmm->lock);
 -	list_del_rcu(&range->list);
 -	mutex_unlock(&range->hmm->lock);
 -
 -	/* Drop reference taken by hmm_range_register() */
 -	range->valid = false;
 -	hmm_put(range->hmm);
 -	range->hmm = NULL;
 -}
 -EXPORT_SYMBOL(hmm_range_unregister);
 -
 -/*
 - * hmm_range_snapshot() - snapshot CPU page table for a range
 - * @range: range
 - * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 - *          permission (for instance asking for write and range is read only),
 - *          -EAGAIN if you need to retry, -EFAULT invalid (ie either no valid
 - *          vma or it is illegal to access that range), number of valid pages
 - *          in range->pfns[] (from range start address).
 + * either hmm_vma_get_pfns() or hmm_vma_fault(). Once the device driver is done
 + * using the data,  or wants to lock updates to the data it got from those
 + * functions, it must call the hmm_vma_range_done() function, which will then
 + * stop tracking CPU page table updates.
   *
 - * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 - * validity is tracked by range struct. See in include/linux/hmm.h for example
 - * on how to use.
 + * Note that device driver must still implement general CPU page table update
 + * tracking either by using hmm_mirror (see hmm_mirror_register()) or by using
 + * the mmu_notifier API directly.
 + *
 + * CPU page table update tracking done through hmm_range is only temporary and
 + * to be used while trying to duplicate CPU page table contents for a range of
 + * virtual addresses.
 + *
 + * There are two ways to use this :
 + * again:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   trans = device_build_page_table_update_transaction(pfns);
 + *   device_page_table_lock();
 + *   if (!hmm_vma_range_done(range)) {
 + *     device_page_table_unlock();
 + *     goto again;
 + *   }
 + *   device_commit_transaction(trans);
 + *   device_page_table_unlock();
 + *
 + * Or:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   device_page_table_lock();
 + *   hmm_vma_range_done(range);
 + *   device_update_page_table(range->pfns);
 + *   device_page_table_unlock();
   */
 -long hmm_range_snapshot(struct hmm_range *range)
 +bool hmm_vma_range_done(struct hmm_range *range)
  {
 -	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 -	unsigned long start = range->start, end;
 -	struct hmm_vma_walk hmm_vma_walk;
 -	struct hmm *hmm = range->hmm;
 -	struct vm_area_struct *vma;
 -	struct mm_walk mm_walk;
 +	unsigned long npages = (range->end - range->start) >> PAGE_SHIFT;
 +	struct hmm *hmm;
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	if (range->end <= range->start) {
 +		BUG();
 +		return false;
 +	}
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid)
 -			return -EAGAIN;
 +	hmm = hmm_register(range->vma->vm_mm);
 +	if (!hmm) {
 +		memset(range->pfns, 0, sizeof(*range->pfns) * npages);
 +		return false;
 +	}
  
 -		vma = find_vma(hmm->mm, start);
 -		if (vma == NULL || (vma->vm_flags & device_vma))
 -			return -EFAULT;
 +	spin_lock(&hmm->lock);
 +	list_del_rcu(&range->list);
 +	spin_unlock(&hmm->lock);
  
++<<<<<<< HEAD
 +	return range->valid;
++=======
+ 		if (is_vm_hugetlb_page(vma)) {
+ 			struct hstate *h = hstate_vma(vma);
+ 
+ 			if (huge_page_shift(h) != range->page_shift &&
+ 			    range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		} else {
+ 			if (range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.pgmap = NULL;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = false;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pud_entry = hmm_vma_walk_pud;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
+ 
+ 		walk_page_range(start, end, &mm_walk);
+ 		start = end;
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> 992de9a8b751 (mm/hmm: allow to mirror vma of a file on a DAX backed filesystem)
  }
 -EXPORT_SYMBOL(hmm_range_snapshot);
 +EXPORT_SYMBOL(hmm_vma_range_done);
  
  /*
 - * hmm_range_fault() - try to fault some address in a virtual address range
 + * hmm_vma_fault() - try to fault some address in a virtual address range
   * @range: range being faulted
   * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
 - * Returns: number of valid pages in range->pfns[] (from range start
 - *          address). This may be zero. If the return value is negative,
 - *          then one of the following values may be returned:
 - *
 - *           -EINVAL  invalid arguments or mm or virtual address are in an
 - *                    invalid vma (for instance device file vma).
 - *           -ENOMEM: Out of memory.
 - *           -EPERM:  Invalid permission (for instance asking for write and
 - *                    range is read only).
 - *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
 - *                    happens if block argument is false.
 - *           -EBUSY:  If the the range is being invalidated and you should wait
 - *                    for invalidation to finish.
 - *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
 - *                    that range), number of valid pages in range->pfns[] (from
 - *                    range start address).
 + * Returns: 0 success, error otherwise (-EAGAIN means mmap_sem have been drop)
   *
   * This is similar to a regular CPU page fault except that it will not trigger
 - * any memory migration if the memory being faulted is not accessible by CPUs
 - * and caller does not ask for migration.
 + * any memory migration if the memory being faulted is not accessible by CPUs.
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
 + *
 + * Expected use pattern:
 + * retry:
 + *   down_read(&mm->mmap_sem);
 + *   // Find vma and address device wants to fault, initialize hmm_pfn_t
 + *   // array accordingly
 + *   ret = hmm_vma_fault(range, write, block);
 + *   switch (ret) {
 + *   case -EAGAIN:
 + *     hmm_vma_range_done(range);
 + *     // You might want to rate limit or yield to play nicely, you may
 + *     // also commit any valid pfn in the array assuming that you are
 + *     // getting true from hmm_vma_range_monitor_end()
 + *     goto retry;
 + *   case 0:
 + *     break;
 + *   case -ENOMEM:
 + *   case -EINVAL:
 + *   case -EPERM:
 + *   default:
 + *     // Handle error !
 + *     up_read(&mm->mmap_sem)
 + *     return;
 + *   }
 + *   // Take device driver lock that serialize device page table update
 + *   driver_lock_device_page_table_update();
 + *   hmm_vma_range_done(range);
 + *   // Commit pfns we got from hmm_vma_fault()
 + *   driver_unlock_device_page_table_update();
 + *   up_read(&mm->mmap_sem)
 + *
 + * YOU MUST CALL hmm_vma_range_done() AFTER THIS FUNCTION RETURN SUCCESS (0)
 + * BEFORE FREEING THE range struct OR YOU WILL HAVE SERIOUS MEMORY CORRUPTION !
 + *
 + * YOU HAVE BEEN WARNED !
   */
 -long hmm_range_fault(struct hmm_range *range, bool block)
 +int hmm_vma_fault(struct hmm_range *range, bool block)
  {
 -	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 -	unsigned long start = range->start, end;
 +	struct vm_area_struct *vma = range->vma;
 +	unsigned long start = range->start;
  	struct hmm_vma_walk hmm_vma_walk;
 -	struct hmm *hmm = range->hmm;
 -	struct vm_area_struct *vma;
  	struct mm_walk mm_walk;
 +	struct hmm *hmm;
  	int ret;
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid) {
 -			up_read(&hmm->mm->mmap_sem);
 -			return -EAGAIN;
 -		}
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm) {
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -ENOMEM;
 +	}
 +	/* Caller must have registered a mirror using hmm_mirror_register() */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
  
 -		vma = find_vma(hmm->mm, start);
 -		if (vma == NULL || (vma->vm_flags & device_vma))
 -			return -EFAULT;
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
 +
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
 +	}
  
 +	/* Initialize range to track CPU page table update */
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = true;
 +	hmm_vma_walk.block = block;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +	hmm_vma_walk.last = range->start;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	do {
 +		ret = walk_page_range(start, range->end, &mm_walk);
 +		start = hmm_vma_walk.last;
 +	} while (ret == -EAGAIN);
 +
 +	if (ret) {
 +		unsigned long i;
 +
++<<<<<<< HEAD
 +		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		hmm_pfns_clear(range, &range->pfns[i], hmm_vma_walk.last,
 +			       range->end);
 +		hmm_vma_range_done(range);
 +	}
 +	return ret;
++=======
+ 		if (is_vm_hugetlb_page(vma)) {
+ 			if (huge_page_shift(hstate_vma(vma)) !=
+ 			    range->page_shift &&
+ 			    range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		} else {
+ 			if (range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.pgmap = NULL;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = true;
+ 		hmm_vma_walk.block = block;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pud_entry = hmm_vma_walk_pud;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
+ 
+ 		do {
+ 			ret = walk_page_range(start, end, &mm_walk);
+ 			start = hmm_vma_walk.last;
+ 
+ 			/* Keep trying while the range is valid. */
+ 		} while (ret == -EBUSY && range->valid);
+ 
+ 		if (ret) {
+ 			unsigned long i;
+ 
+ 			i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
+ 			hmm_pfns_clear(range, &range->pfns[i],
+ 				hmm_vma_walk.last, range->end);
+ 			return ret;
+ 		}
+ 		start = end;
+ 
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> 992de9a8b751 (mm/hmm: allow to mirror vma of a file on a DAX backed filesystem)
  }
 -EXPORT_SYMBOL(hmm_range_fault);
 +EXPORT_SYMBOL(hmm_vma_fault);
  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
  
* Unmerged path mm/hmm.c
