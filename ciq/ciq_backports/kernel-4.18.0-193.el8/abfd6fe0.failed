iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [iommu] io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page() (Jerry Snitselaar) [1729845]
Rebuild_FUZZ: 95.16%
commit-author Will Deacon <will@kernel.org>
commit abfd6fe0cd535d31ee83b668be6eb59ce6a8469d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/abfd6fe0.failed

The ->tlb_add_flush() callback in the io-pgtable API now looks a bit
silly:

  - It takes a size and a granule, which are always the same
  - It takes a 'bool leaf', which is always true
  - It only ever flushes a single page

With that in mind, replace it with an optional ->tlb_add_page() callback
that drops the useless parameters.

	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit abfd6fe0cd535d31ee83b668be6eb59ce6a8469d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/panfrost/panfrost_mmu.c
#	drivers/iommu/arm-smmu-v3.c
#	drivers/iommu/arm-smmu.c
#	drivers/iommu/ipmmu-vmsa.c
#	drivers/iommu/msm_iommu.c
#	drivers/iommu/mtk_iommu.c
#	drivers/iommu/qcom_iommu.c
diff --cc drivers/iommu/arm-smmu-v3.c
index c08a34136024,98c90a1b4b22..000000000000
--- a/drivers/iommu/arm-smmu-v3.c
+++ b/drivers/iommu/arm-smmu-v3.c
@@@ -1598,9 -1603,37 +1598,43 @@@ static void arm_smmu_tlb_inv_range_nosy
  	} while (size -= granule);
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops arm_smmu_gather_ops = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
++=======
+ static void arm_smmu_tlb_inv_page_nosync(unsigned long iova, size_t granule,
+ 					 void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 
+ 	arm_smmu_tlb_inv_range_nosync(iova, size, granule, false, cookie);
+ 	arm_smmu_cmdq_issue_sync(smmu);
+ }
+ 
+ static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 
+ 	arm_smmu_tlb_inv_range_nosync(iova, size, granule, true, cookie);
+ 	arm_smmu_cmdq_issue_sync(smmu);
+ }
+ 
+ static const struct iommu_flush_ops arm_smmu_flush_ops = {
+ 	.tlb_flush_all	= arm_smmu_tlb_inv_context,
+ 	.tlb_flush_walk = arm_smmu_tlb_inv_walk,
+ 	.tlb_flush_leaf = arm_smmu_tlb_inv_leaf,
+ 	.tlb_add_page	= arm_smmu_tlb_inv_page_nosync,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	.tlb_sync	= arm_smmu_tlb_sync,
  };
  
diff --cc drivers/iommu/arm-smmu.c
index 58f85e7c33c2,f056164a94b0..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -244,7 -257,7 +250,11 @@@ struct arm_smmu_flush_ops 
  struct arm_smmu_domain {
  	struct arm_smmu_device		*smmu;
  	struct io_pgtable_ops		*pgtbl_ops;
++<<<<<<< HEAD
 +	const struct iommu_gather_ops	*tlb_ops;
++=======
+ 	const struct arm_smmu_flush_ops	*flush_ops;
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	struct arm_smmu_cfg		cfg;
  	enum arm_smmu_domain_stage	stage;
  	bool				non_strict;
@@@ -540,22 -553,66 +550,85 @@@ static void arm_smmu_tlb_inv_vmid_nosyn
  	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops arm_smmu_s1_tlb_ops = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_context,
 +};
 +
 +static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v2 = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_context,
 +};
 +
 +static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v1 = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_vmid_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_vmid,
++=======
+ static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, size, granule, false, cookie);
+ 	ops->tlb.tlb_sync(cookie);
+ }
+ 
+ static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, size, granule, true, cookie);
+ 	ops->tlb.tlb_sync(cookie);
+ }
+ 
+ static void arm_smmu_tlb_add_page(unsigned long iova, size_t granule,
+ 				  void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct arm_smmu_flush_ops arm_smmu_s1_tlb_ops = {
+ 	.tlb = {
+ 		.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 		.tlb_sync	= arm_smmu_tlb_sync_context,
+ 	},
+ 	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
+ };
+ 
+ static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
+ 	.tlb = {
+ 		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 		.tlb_sync	= arm_smmu_tlb_sync_context,
+ 	},
+ 	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
+ };
+ 
+ static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v1 = {
+ 	.tlb = {
+ 		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 		.tlb_sync	= arm_smmu_tlb_sync_vmid,
+ 	},
+ 	.tlb_inv_range		= arm_smmu_tlb_inv_vmid_nosync,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  };
  
  static irqreturn_t arm_smmu_context_fault(int irq, void *dev)
diff --cc drivers/iommu/ipmmu-vmsa.c
index c842cf9ce2c3,c4da271af90e..000000000000
--- a/drivers/iommu/ipmmu-vmsa.c
+++ b/drivers/iommu/ipmmu-vmsa.c
@@@ -349,15 -361,16 +349,28 @@@ static void ipmmu_tlb_flush_all(void *c
  	ipmmu_tlb_invalidate(domain);
  }
  
++<<<<<<< HEAD
 +static void ipmmu_tlb_add_flush(unsigned long iova, size_t size,
 +				size_t granule, bool leaf, void *cookie)
 +{
 +	/* The hardware doesn't support selective TLB flush. */
 +}
 +
 +static const struct iommu_gather_ops ipmmu_gather_ops = {
 +	.tlb_flush_all = ipmmu_tlb_flush_all,
 +	.tlb_add_flush = ipmmu_tlb_add_flush,
++=======
+ static void ipmmu_tlb_flush(unsigned long iova, size_t size,
+ 				size_t granule, void *cookie)
+ {
+ 	ipmmu_tlb_flush_all(cookie);
+ }
+ 
+ static const struct iommu_flush_ops ipmmu_flush_ops = {
+ 	.tlb_flush_all = ipmmu_tlb_flush_all,
+ 	.tlb_flush_walk = ipmmu_tlb_flush,
+ 	.tlb_flush_leaf = ipmmu_tlb_flush,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	.tlb_sync = ipmmu_tlb_flush_all,
  };
  
diff --cc drivers/iommu/msm_iommu.c
index bc2cef99bb30,2cd83295a841..000000000000
--- a/drivers/iommu/msm_iommu.c
+++ b/drivers/iommu/msm_iommu.c
@@@ -189,9 -178,30 +189,36 @@@ static void __flush_iotlb_sync(void *co
  	 */
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops msm_iommu_gather_ops = {
 +	.tlb_flush_all = __flush_iotlb,
 +	.tlb_add_flush = __flush_iotlb_range,
++=======
+ static void __flush_iotlb_walk(unsigned long iova, size_t size,
+ 			       size_t granule, void *cookie)
+ {
+ 	__flush_iotlb_range(iova, size, granule, false, cookie);
+ 	__flush_iotlb_sync(cookie);
+ }
+ 
+ static void __flush_iotlb_leaf(unsigned long iova, size_t size,
+ 			       size_t granule, void *cookie)
+ {
+ 	__flush_iotlb_range(iova, size, granule, true, cookie);
+ 	__flush_iotlb_sync(cookie);
+ }
+ 
+ static void __flush_iotlb_page(unsigned long iova, size_t granule, void *cookie)
+ {
+ 	__flush_iotlb_range(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops msm_iommu_flush_ops = {
+ 	.tlb_flush_all = __flush_iotlb,
+ 	.tlb_flush_walk = __flush_iotlb_walk,
+ 	.tlb_flush_leaf = __flush_iotlb_leaf,
+ 	.tlb_add_page = __flush_iotlb_page,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	.tlb_sync = __flush_iotlb_sync,
  };
  
diff --cc drivers/iommu/mtk_iommu.c
index f9f69f7111a9,a0b4b4dc4b90..000000000000
--- a/drivers/iommu/mtk_iommu.c
+++ b/drivers/iommu/mtk_iommu.c
@@@ -196,9 -188,31 +196,37 @@@ static void mtk_iommu_tlb_sync(void *co
  	}
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops mtk_iommu_gather_ops = {
 +	.tlb_flush_all = mtk_iommu_tlb_flush_all,
 +	.tlb_add_flush = mtk_iommu_tlb_add_flush_nosync,
++=======
+ static void mtk_iommu_tlb_flush_walk(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, size, granule, false, cookie);
+ 	mtk_iommu_tlb_sync(cookie);
+ }
+ 
+ static void mtk_iommu_tlb_flush_leaf(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, size, granule, true, cookie);
+ 	mtk_iommu_tlb_sync(cookie);
+ }
+ 
+ static void mtk_iommu_tlb_flush_page_nosync(unsigned long iova, size_t granule,
+ 					    void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops mtk_iommu_flush_ops = {
+ 	.tlb_flush_all = mtk_iommu_tlb_flush_all,
+ 	.tlb_flush_walk = mtk_iommu_tlb_flush_walk,
+ 	.tlb_flush_leaf = mtk_iommu_tlb_flush_leaf,
+ 	.tlb_add_page = mtk_iommu_tlb_flush_page_nosync,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	.tlb_sync = mtk_iommu_tlb_sync,
  };
  
diff --cc drivers/iommu/qcom_iommu.c
index 5fa9507289ec,7d8411dee4cf..000000000000
--- a/drivers/iommu/qcom_iommu.c
+++ b/drivers/iommu/qcom_iommu.c
@@@ -175,9 -164,31 +175,37 @@@ static void qcom_iommu_tlb_inv_range_no
  	}
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops qcom_gather_ops = {
 +	.tlb_flush_all	= qcom_iommu_tlb_inv_context,
 +	.tlb_add_flush	= qcom_iommu_tlb_inv_range_nosync,
++=======
+ static void qcom_iommu_tlb_flush_walk(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, size, granule, false, cookie);
+ 	qcom_iommu_tlb_sync(cookie);
+ }
+ 
+ static void qcom_iommu_tlb_flush_leaf(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, size, granule, true, cookie);
+ 	qcom_iommu_tlb_sync(cookie);
+ }
+ 
+ static void qcom_iommu_tlb_add_page(unsigned long iova, size_t granule,
+ 				    void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops qcom_flush_ops = {
+ 	.tlb_flush_all	= qcom_iommu_tlb_inv_context,
+ 	.tlb_flush_walk = qcom_iommu_tlb_flush_walk,
+ 	.tlb_flush_leaf = qcom_iommu_tlb_flush_leaf,
+ 	.tlb_add_page	= qcom_iommu_tlb_add_page,
++>>>>>>> abfd6fe0cd53 (iommu/io-pgtable: Replace ->tlb_add_flush() with ->tlb_add_page())
  	.tlb_sync	= qcom_iommu_tlb_sync,
  };
  
* Unmerged path drivers/gpu/drm/panfrost/panfrost_mmu.c
* Unmerged path drivers/gpu/drm/panfrost/panfrost_mmu.c
* Unmerged path drivers/iommu/arm-smmu-v3.c
* Unmerged path drivers/iommu/arm-smmu.c
diff --git a/drivers/iommu/io-pgtable-arm-v7s.c b/drivers/iommu/io-pgtable-arm-v7s.c
index a0032d1cb4ef..95daa9abe2d3 100644
--- a/drivers/iommu/io-pgtable-arm-v7s.c
+++ b/drivers/iommu/io-pgtable-arm-v7s.c
@@ -595,7 +595,7 @@ static size_t arm_v7s_split_blk_unmap(struct arm_v7s_io_pgtable *data,
 		return __arm_v7s_unmap(data, iova, size, 2, tablep);
 	}
 
-	io_pgtable_tlb_add_flush(&data->iop, iova, size, size, true);
+	io_pgtable_tlb_add_page(&data->iop, iova, size);
 	return size;
 }
 
@@ -658,8 +658,7 @@ static size_t __arm_v7s_unmap(struct arm_v7s_io_pgtable *data,
 				 */
 				smp_wmb();
 			} else {
-				io_pgtable_tlb_add_flush(iop, iova, blk_size,
-							 blk_size, true);
+				io_pgtable_tlb_add_page(iop, iova, blk_size);
 			}
 			iova += blk_size;
 		}
@@ -820,10 +819,9 @@ static void dummy_tlb_flush(unsigned long iova, size_t size, size_t granule,
 	WARN_ON(!(size & cfg_cookie->pgsize_bitmap));
 }
 
-static void dummy_tlb_add_flush(unsigned long iova, size_t size,
-				size_t granule, bool leaf, void *cookie)
+static void dummy_tlb_add_page(unsigned long iova, size_t granule, void *cookie)
 {
-	dummy_tlb_flush(iova, size, granule, cookie);
+	dummy_tlb_flush(iova, granule, granule, cookie);
 }
 
 static void dummy_tlb_sync(void *cookie)
@@ -835,7 +833,7 @@ static const struct iommu_gather_ops dummy_tlb_ops = {
 	.tlb_flush_all	= dummy_tlb_flush_all,
 	.tlb_flush_walk	= dummy_tlb_flush,
 	.tlb_flush_leaf	= dummy_tlb_flush,
-	.tlb_add_flush	= dummy_tlb_add_flush,
+	.tlb_add_page	= dummy_tlb_add_page,
 	.tlb_sync	= dummy_tlb_sync,
 };
 
diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c
index df587d77518c..db71b57d31b9 100644
--- a/drivers/iommu/io-pgtable-arm.c
+++ b/drivers/iommu/io-pgtable-arm.c
@@ -596,7 +596,7 @@ static size_t arm_lpae_split_blk_unmap(struct arm_lpae_io_pgtable *data,
 
 		tablep = iopte_deref(pte, data);
 	} else if (unmap_idx >= 0) {
-		io_pgtable_tlb_add_flush(&data->iop, iova, size, size, true);
+		io_pgtable_tlb_add_page(&data->iop, iova, size);
 		return size;
 	}
 
@@ -637,7 +637,7 @@ static size_t __arm_lpae_unmap(struct arm_lpae_io_pgtable *data,
 			 */
 			smp_wmb();
 		} else {
-			io_pgtable_tlb_add_flush(iop, iova, size, size, true);
+			io_pgtable_tlb_add_page(iop, iova, size);
 		}
 
 		return size;
@@ -1118,10 +1118,9 @@ static void dummy_tlb_flush(unsigned long iova, size_t size, size_t granule,
 	WARN_ON(!(size & cfg_cookie->pgsize_bitmap));
 }
 
-static void dummy_tlb_add_flush(unsigned long iova, size_t size,
-				size_t granule, bool leaf, void *cookie)
+static void dummy_tlb_add_page(unsigned long iova, size_t granule, void *cookie)
 {
-	dummy_tlb_flush(iova, size, granule, cookie);
+	dummy_tlb_flush(iova, granule, granule, cookie);
 }
 
 static void dummy_tlb_sync(void *cookie)
@@ -1133,7 +1132,7 @@ static const struct iommu_gather_ops dummy_tlb_ops __initconst = {
 	.tlb_flush_all	= dummy_tlb_flush_all,
 	.tlb_flush_walk	= dummy_tlb_flush,
 	.tlb_flush_leaf	= dummy_tlb_flush,
-	.tlb_add_flush	= dummy_tlb_add_flush,
+	.tlb_add_page	= dummy_tlb_add_page,
 	.tlb_sync	= dummy_tlb_sync,
 };
 
* Unmerged path drivers/iommu/ipmmu-vmsa.c
* Unmerged path drivers/iommu/msm_iommu.c
* Unmerged path drivers/iommu/mtk_iommu.c
* Unmerged path drivers/iommu/qcom_iommu.c
diff --git a/include/linux/io-pgtable.h b/include/linux/io-pgtable.h
index 7f040c22b7fb..b6a08233bbf6 100644
--- a/include/linux/io-pgtable.h
+++ b/include/linux/io-pgtable.h
@@ -25,12 +25,11 @@ enum io_pgtable_fmt {
  *                  address range.
  * @tlb_flush_leaf: Synchronously invalidate all leaf TLB state for a virtual
  *                  address range.
- * @tlb_add_flush:  Optional callback to queue up leaf TLB invalidation for a
- *                  virtual address range.  This function exists purely as an
- *                  optimisation for IOMMUs that cannot batch TLB invalidation
- *                  operations efficiently and are therefore better suited to
- *                  issuing them early rather than deferring them until
- *                  iommu_tlb_sync().
+ * @tlb_add_page:   Optional callback to queue up leaf TLB invalidation for a
+ *                  single page. This function exists purely as an optimisation
+ *                  for IOMMUs that cannot batch TLB invalidation operations
+ *                  efficiently and are therefore better suited to issuing them
+ *                  early rather than deferring them until iommu_tlb_sync().
  * @tlb_sync:       Ensure any queued TLB invalidation has taken effect, and
  *                  any corresponding page table updates are visible to the
  *                  IOMMU.
@@ -44,8 +43,7 @@ struct iommu_gather_ops {
 			       void *cookie);
 	void (*tlb_flush_leaf)(unsigned long iova, size_t size, size_t granule,
 			       void *cookie);
-	void (*tlb_add_flush)(unsigned long iova, size_t size, size_t granule,
-			      bool leaf, void *cookie);
+	void (*tlb_add_page)(unsigned long iova, size_t granule, void *cookie);
 	void (*tlb_sync)(void *cookie);
 };
 
@@ -212,10 +210,12 @@ io_pgtable_tlb_flush_leaf(struct io_pgtable *iop, unsigned long iova,
 	iop->cfg.tlb->tlb_flush_leaf(iova, size, granule, iop->cookie);
 }
 
-static inline void io_pgtable_tlb_add_flush(struct io_pgtable *iop,
-		unsigned long iova, size_t size, size_t granule, bool leaf)
+static inline void
+io_pgtable_tlb_add_page(struct io_pgtable *iop, unsigned long iova,
+			size_t granule)
 {
-	iop->cfg.tlb->tlb_add_flush(iova, size, granule, leaf, iop->cookie);
+	if (iop->cfg.tlb->tlb_add_page)
+		iop->cfg.tlb->tlb_add_page(iova, granule, iop->cookie);
 }
 
 static inline void io_pgtable_tlb_sync(struct io_pgtable *iop)
