RDMA/counter: Add set/clear per-port auto mode support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Mark Zhang <markz@mellanox.com>
commit 413d3347503bc39e17577eaf16451fd492a68558
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/413d3347.failed

Add an API to support set/clear per-port auto mode.

	Signed-off-by: Mark Zhang <markz@mellanox.com>
	Reviewed-by: Majd Dibbiny <majd@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 413d3347503bc39e17577eaf16451fd492a68558)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
#	include/rdma/ib_verbs.h
#	include/rdma/rdma_counter.h
diff --cc drivers/infiniband/core/device.c
index ec96a7b1c811,6579865e4866..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -44,8 -45,11 +44,9 @@@
  #include <rdma/rdma_netlink.h>
  #include <rdma/ib_addr.h>
  #include <rdma/ib_cache.h>
+ #include <rdma/rdma_counter.h>
  
  #include "core_priv.h"
 -#include "restrack.h"
  
  MODULE_AUTHOR("Roland Dreier");
  MODULE_DESCRIPTION("core kernel InfiniBand API");
@@@ -251,12 -488,20 +252,27 @@@ static void ib_device_release(struct de
  {
  	struct ib_device *dev = container_of(device, struct ib_device, dev);
  
 -	free_netdevs(dev);
  	WARN_ON(refcount_read(&dev->refcount));
++<<<<<<< HEAD
 +	ib_cache_release_one(dev);
 +	ib_security_release_port_pkey_list(dev);
 +	kfree(dev->port_pkey_list);
 +	kfree(dev->port_immutable);
 +	kfree(dev);
++=======
+ 	if (dev->port_data) {
+ 		ib_cache_release_one(dev);
+ 		ib_security_release_port_pkey_list(dev);
+ 		rdma_counter_release(dev);
+ 		kfree_rcu(container_of(dev->port_data, struct ib_port_data_rcu,
+ 				       pdata[0]),
+ 			  rcu_head);
+ 	}
+ 
+ 	xa_destroy(&dev->compat_devs);
+ 	xa_destroy(&dev->client_data);
+ 	kfree_rcu(dev, rcu_head);
++>>>>>>> 413d3347503b (RDMA/counter: Add set/clear per-port auto mode support)
  }
  
  static int ib_device_uevent(struct device *device,
@@@ -609,96 -1549,101 +625,109 @@@ int ib_register_device(struct ib_devic
  		goto out;
  	}
  
 -	kobject_uevent(&device->dev.kobj, KOBJ_REMOVE);
 -	disable_device(device);
 +	device->index = __dev_new_index();
  
 -	/*
 -	 * At this point no one can be using the device, so it is safe to
 -	 * change the namespace.
 -	 */
 -	write_pnet(&device->coredev.rdma_net, net);
++<<<<<<< HEAD
 +	ret = ib_device_register_rdmacg(device);
 +	if (ret) {
 +		dev_warn(&device->dev,
 +			 "Couldn't register device with rdma cgroup\n");
 +		goto dev_cleanup;
 +	}
++=======
++	rdma_counter_init(device);
+ 
 -	down_read(&devices_rwsem);
+ 	/*
 -	 * Currently rdma devices are system wide unique. So the device name
 -	 * is guaranteed free in the new namespace. Publish the new namespace
 -	 * at the sysfs level.
++	 * Ensure that ADD uevent is not fired because it
++	 * is too early amd device is not initialized yet.
+ 	 */
 -	ret = device_rename(&device->dev, dev_name(&device->dev));
 -	up_read(&devices_rwsem);
++	dev_set_uevent_suppress(&device->dev, true);
++	ret = device_add(&device->dev);
++	if (ret)
++		goto cg_cleanup;
++>>>>>>> 413d3347503b (RDMA/counter: Add set/clear per-port auto mode support)
 +
 +	ret = ib_device_register_sysfs(device);
  	if (ret) {
  		dev_warn(&device->dev,
 -			 "%s: Couldn't rename device after namespace change\n",
 -			 __func__);
 -		/* Try and put things back and re-enable the device */
 -		write_pnet(&device->coredev.rdma_net, cur_net);
 +			 "Couldn't register device with driver model\n");
 +		goto cg_cleanup;
  	}
  
 -	ret2 = enable_device_and_get(device);
 -	if (ret2) {
 -		/*
 -		 * This shouldn't really happen, but if it does, let the user
 -		 * retry at later point. So don't disable the device.
 -		 */
 -		dev_warn(&device->dev,
 -			 "%s: Couldn't re-enable device after namespace change\n",
 -			 __func__);
 -	}
 -	kobject_uevent(&device->dev.kobj, KOBJ_ADD);
 +	refcount_set(&device->refcount, 1);
  
 -	ib_device_put(device);
 +	xa_for_each_marked (&clients, index, client, CLIENT_REGISTERED)
 +		if (!add_client_context(device, client) && client->add)
 +			client->add(device);
 +
 +	down_write(&lists_rwsem);
 +	list_add_tail(&device->core_list, &device_list);
 +	up_write(&lists_rwsem);
 +	mutex_unlock(&device_mutex);
 +	return 0;
 +
 +cg_cleanup:
 +	ib_device_unregister_rdmacg(device);
 +dev_cleanup:
 +	ib_cache_cleanup_one(device);
  out:
 -	mutex_unlock(&device->unregistration_lock);
 -	if (ret)
 -		return ret;
 -	return ret2;
 +	mutex_unlock(&device_mutex);
 +	return ret;
  }
 +EXPORT_SYMBOL(ib_register_device);
  
 -int ib_device_set_netns_put(struct sk_buff *skb,
 -			    struct ib_device *dev, u32 ns_fd)
 +/**
 + * ib_unregister_device - Unregister an IB device
 + * @device:Device to unregister
 + *
 + * Unregister an IB device.  All clients will receive a remove callback.
 + */
 +void ib_unregister_device(struct ib_device *device)
  {
 -	struct net *net;
 -	int ret;
 -
 -	net = get_net_ns_by_fd(ns_fd);
 -	if (IS_ERR(net)) {
 -		ret = PTR_ERR(net);
 -		goto net_err;
 -	}
 -
 -	if (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN)) {
 -		ret = -EPERM;
 -		goto ns_err;
 -	}
 +	struct ib_client_data *context, *tmp;
 +	unsigned long flags;
  
  	/*
 -	 * Currently supported only for those providers which support
 -	 * disassociation and don't do port specific sysfs init. Once a
 -	 * port_cleanup infrastructure is implemented, this limitation will be
 -	 * removed.
 +	 * Wait for all netlink command callers to finish working on the
 +	 * device.
  	 */
 -	if (!dev->ops.disassociate_ucontext || dev->ops.init_port ||
 -	    ib_devices_shared_netns) {
 -		ret = -EOPNOTSUPP;
 -		goto ns_err;
 +	ib_device_put(device);
 +	wait_for_completion(&device->unreg_completion);
 +
 +	mutex_lock(&device_mutex);
 +
 +	down_write(&lists_rwsem);
 +	list_del(&device->core_list);
 +	write_lock_irq(&device->client_data_lock);
 +	list_for_each_entry(context, &device->client_data_list, list)
 +		context->going_down = true;
 +	write_unlock_irq(&device->client_data_lock);
 +	downgrade_write(&lists_rwsem);
 +
 +	list_for_each_entry(context, &device->client_data_list, list) {
 +		if (context->client->remove)
 +			context->client->remove(device, context->data);
  	}
 +	up_read(&lists_rwsem);
  
 -	get_device(&dev->dev);
 -	ib_device_put(dev);
 -	ret = rdma_dev_change_netns(dev, current->nsproxy->net_ns, net);
 -	put_device(&dev->dev);
 +	ib_device_unregister_sysfs(device);
 +	ib_device_unregister_rdmacg(device);
  
 -	put_net(net);
 -	return ret;
 +	mutex_unlock(&device_mutex);
  
 -ns_err:
 -	put_net(net);
 -net_err:
 -	ib_device_put(dev);
 -	return ret;
 -}
 +	ib_cache_cleanup_one(device);
  
 -static struct pernet_operations rdma_dev_net_ops = {
 -	.init = rdma_dev_init_net,
 -	.exit = rdma_dev_exit_net,
 -	.id = &rdma_dev_net_id,
 -	.size = sizeof(struct rdma_dev_net),
 -};
 +	down_write(&lists_rwsem);
 +	write_lock_irqsave(&device->client_data_lock, flags);
 +	list_for_each_entry_safe(context, tmp, &device->client_data_list,
 +				 list) {
 +		list_del(&context->list);
 +		kfree(context);
 +	}
 +	write_unlock_irqrestore(&device->client_data_lock, flags);
 +	up_write(&lists_rwsem);
 +}
 +EXPORT_SYMBOL(ib_unregister_device);
  
  static int assign_client_id(struct ib_client *client)
  {
diff --cc include/rdma/ib_verbs.h
index 9a1c4c9437dd,3d19c056fbc0..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -2094,6 -2107,22 +2095,25 @@@ struct ib_port_immutable 
  	u32                           max_mad_size;
  };
  
++<<<<<<< HEAD
++=======
+ struct ib_port_data {
+ 	struct ib_device *ib_dev;
+ 
+ 	struct ib_port_immutable immutable;
+ 
+ 	spinlock_t pkey_list_lock;
+ 	struct list_head pkey_list;
+ 
+ 	struct ib_port_cache cache;
+ 
+ 	spinlock_t netdev_lock;
+ 	struct net_device __rcu *netdev;
+ 	struct hlist_node ndev_hash_link;
+ 	struct rdma_port_counter port_counter;
+ };
+ 
++>>>>>>> 413d3347503b (RDMA/counter: Add set/clear per-port auto mode support)
  /* rdma netdev type - specifies protocol type */
  enum rdma_netdev_t {
  	RDMA_NETDEV_OPA_VNIC,
* Unmerged path include/rdma/rdma_counter.h
diff --git a/drivers/infiniband/core/Makefile b/drivers/infiniband/core/Makefile
index 69dee36e0e89..b4a9c9c91c33 100644
--- a/drivers/infiniband/core/Makefile
+++ b/drivers/infiniband/core/Makefile
@@ -12,7 +12,7 @@ ib_core-y :=			packer.o ud_header.o verbs.o cq.o rw.o sysfs.o \
 				device.o fmr_pool.o cache.o netlink.o \
 				roce_gid_mgmt.o mr_pool.o addr.o sa_query.o \
 				multicast.o mad.o smi.o agent.o mad_rmpp.o \
-				nldev.o restrack.o
+				nldev.o restrack.o counters.o
 
 ib_core-$(CONFIG_SECURITY_INFINIBAND) += security.o
 ib_core-$(CONFIG_INFINIBAND_USER_MEM) += umem.o
diff --git a/drivers/infiniband/core/counters.c b/drivers/infiniband/core/counters.c
new file mode 100644
index 000000000000..6167914fba06
--- /dev/null
+++ b/drivers/infiniband/core/counters.c
@@ -0,0 +1,74 @@
+// SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
+/*
+ * Copyright (c) 2019 Mellanox Technologies. All rights reserved.
+ */
+#include <rdma/ib_verbs.h>
+#include <rdma/rdma_counter.h>
+
+#include "core_priv.h"
+#include "restrack.h"
+
+#define ALL_AUTO_MODE_MASKS (RDMA_COUNTER_MASK_QP_TYPE)
+
+static int __counter_set_mode(struct rdma_counter_mode *curr,
+			      enum rdma_nl_counter_mode new_mode,
+			      enum rdma_nl_counter_mask new_mask)
+{
+	if ((new_mode == RDMA_COUNTER_MODE_AUTO) &&
+	    ((new_mask & (~ALL_AUTO_MODE_MASKS)) ||
+	     (curr->mode != RDMA_COUNTER_MODE_NONE)))
+		return -EINVAL;
+
+	curr->mode = new_mode;
+	curr->mask = new_mask;
+	return 0;
+}
+
+/**
+ * rdma_counter_set_auto_mode() - Turn on/off per-port auto mode
+ *
+ * When @on is true, the @mask must be set
+ */
+int rdma_counter_set_auto_mode(struct ib_device *dev, u8 port,
+			       bool on, enum rdma_nl_counter_mask mask)
+{
+	struct rdma_port_counter *port_counter;
+	int ret;
+
+	port_counter = &dev->port_data[port].port_counter;
+	mutex_lock(&port_counter->lock);
+	if (on) {
+		ret = __counter_set_mode(&port_counter->mode,
+					 RDMA_COUNTER_MODE_AUTO, mask);
+	} else {
+		if (port_counter->mode.mode != RDMA_COUNTER_MODE_AUTO) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = __counter_set_mode(&port_counter->mode,
+					 RDMA_COUNTER_MODE_NONE, 0);
+	}
+
+out:
+	mutex_unlock(&port_counter->lock);
+	return ret;
+}
+
+void rdma_counter_init(struct ib_device *dev)
+{
+	struct rdma_port_counter *port_counter;
+	u32 port;
+
+	if (!dev->ops.alloc_hw_stats || !dev->port_data)
+		return;
+
+	rdma_for_each_port(dev, port) {
+		port_counter = &dev->port_data[port].port_counter;
+		port_counter->mode.mode = RDMA_COUNTER_MODE_NONE;
+		mutex_init(&port_counter->lock);
+	}
+}
+
+void rdma_counter_release(struct ib_device *dev)
+{
+}
* Unmerged path drivers/infiniband/core/device.c
* Unmerged path include/rdma/ib_verbs.h
* Unmerged path include/rdma/rdma_counter.h
diff --git a/include/uapi/rdma/rdma_netlink.h b/include/uapi/rdma/rdma_netlink.h
index 213452ef94a5..ac352abe3761 100644
--- a/include/uapi/rdma/rdma_netlink.h
+++ b/include/uapi/rdma/rdma_netlink.h
@@ -462,4 +462,30 @@ enum rdma_nldev_attr {
 	 */
 	RDMA_NLDEV_ATTR_MAX
 };
+
+/*
+ * Supported counter bind modes. All modes are mutual-exclusive.
+ */
+enum rdma_nl_counter_mode {
+	RDMA_COUNTER_MODE_NONE,
+
+	/*
+	 * A qp is bound with a counter automatically during initialization
+	 * based on the auto mode (e.g., qp type, ...)
+	 */
+	RDMA_COUNTER_MODE_AUTO,
+
+	/*
+	 * Always the end
+	 */
+	RDMA_COUNTER_MODE_MAX,
+};
+
+/*
+ * Supported criteria in counter auto mode.
+ * Currently only "qp type" is supported
+ */
+enum rdma_nl_counter_mask {
+	RDMA_COUNTER_MASK_QP_TYPE = 1,
+};
 #endif /* _UAPI_RDMA_NETLINK_H */
