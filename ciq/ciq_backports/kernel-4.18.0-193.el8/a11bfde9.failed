iommu/vt-d: Do deferred attachment in iommu_need_mapping()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Joerg Roedel <jroedel@suse.de>
commit a11bfde9c77df1fd350ea27169ab921f511bf5d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/a11bfde9.failed

The attachment of deferred devices needs to happen before the check
whether the device is identity mapped or not. Otherwise the check will
return wrong results, cause warnings boot failures in kdump kernels, like

	WARNING: CPU: 0 PID: 318 at ../drivers/iommu/intel-iommu.c:592 domain_get_iommu+0x61/0x70

	[...]

	 Call Trace:
	  __intel_map_single+0x55/0x190
	  intel_alloc_coherent+0xac/0x110
	  dmam_alloc_attrs+0x50/0xa0
	  ahci_port_start+0xfb/0x1f0 [libahci]
	  ata_host_start.part.39+0x104/0x1e0 [libata]

With the earlier check the kdump boot succeeds and a crashdump is written.

Fixes: 1ee0186b9a12 ("iommu/vt-d: Refactor find_domain() helper")
	Cc: stable@vger.kernel.org # v5.5
	Reviewed-by: Jerry Snitselaar <jsnitsel@redhat.com>
	Acked-by: Lu Baolu <baolu.lu@linux.intel.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit a11bfde9c77df1fd350ea27169ab921f511bf5d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index 1431de3bfba0,723f615c6e84..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -2385,6 -2528,22 +2385,24 @@@ static struct dmar_domain *find_domain(
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static void do_deferred_attach(struct device *dev)
+ {
+ 	struct iommu_domain *domain;
+ 
+ 	dev->archdata.iommu = NULL;
+ 	domain = iommu_get_domain_for_dev(dev);
+ 	if (domain)
+ 		intel_iommu_attach_device(domain, dev);
+ }
+ 
+ static struct dmar_domain *deferred_attach_domain(struct device *dev)
+ {
+ 	return find_domain(dev);
+ }
+ 
++>>>>>>> a11bfde9c77d (iommu/vt-d: Do deferred attachment in iommu_need_mapping())
  static inline struct device_domain_info *
  dmar_search_domain_by_dev_info(int segment, int bus, int devfn)
  {
@@@ -3589,47 -3585,49 +3607,56 @@@ out
  }
  
  /* Check if the dev needs to go through non-identity map and unmap process.*/
 -static bool iommu_need_mapping(struct device *dev)
 +static int iommu_no_mapping(struct device *dev)
  {
 -	int ret;
 +	int found;
  
  	if (iommu_dummy(dev))
 -		return false;
 +		return 1;
  
++<<<<<<< HEAD
 +	if (!iommu_identity_mapping)
 +		return 0;
++=======
+ 	if (unlikely(attach_deferred(dev)))
+ 		do_deferred_attach(dev);
+ 
+ 	ret = identity_mapping(dev);
+ 	if (ret) {
+ 		u64 dma_mask = *dev->dma_mask;
++>>>>>>> a11bfde9c77d (iommu/vt-d: Do deferred attachment in iommu_need_mapping())
  
 -		if (dev->coherent_dma_mask && dev->coherent_dma_mask < dma_mask)
 -			dma_mask = dev->coherent_dma_mask;
 -
 -		if (dma_mask >= dma_direct_get_required_mask(dev))
 -			return false;
 -
 +	found = identity_mapping(dev);
 +	if (found) {
 +		if (iommu_should_identity_map(dev, 0))
 +			return 1;
 +		else {
 +			/*
 +			 * 32 bit DMA is removed from si_domain and fall back
 +			 * to non-identity mapping.
 +			 */
 +			dmar_remove_one_dev_info(si_domain, dev);
 +			pr_info("32bit %s uses non-identity mapping\n",
 +				dev_name(dev));
 +			return 0;
 +		}
 +	} else {
  		/*
 -		 * 32 bit DMA is removed from si_domain and fall back to
 -		 * non-identity mapping.
 +		 * In case of a detached 64 bit DMA device from vm, the device
 +		 * is put into si_domain for identity mapping.
  		 */
 -		dmar_remove_one_dev_info(dev);
 -		ret = iommu_request_dma_domain_for_dev(dev);
 -		if (ret) {
 -			struct iommu_domain *domain;
 -			struct dmar_domain *dmar_domain;
 -
 -			domain = iommu_get_domain_for_dev(dev);
 -			if (domain) {
 -				dmar_domain = to_dmar_domain(domain);
 -				dmar_domain->flags |= DOMAIN_FLAG_LOSE_CHILDREN;
 +		if (iommu_should_identity_map(dev, 0)) {
 +			int ret;
 +			ret = domain_add_dev_info(si_domain, dev);
 +			if (!ret) {
 +				pr_info("64bit %s uses identity mapping\n",
 +					dev_name(dev));
 +				return 1;
  			}
 -			dmar_remove_one_dev_info(dev);
 -			get_private_domain_for_dev(dev);
  		}
 -
 -		dev_info(dev, "32bit DMA uses non-identity mapping\n");
  	}
  
 -	return true;
 +	return 0;
  }
  
  static dma_addr_t __intel_map_single(struct device *dev, phys_addr_t paddr,
@@@ -3915,8 -3921,264 +3942,267 @@@ static const struct dma_map_ops intel_d
  	.map_page = intel_map_page,
  	.unmap_page = intel_unmap_page,
  	.map_resource = intel_map_resource,
 -	.unmap_resource = intel_unmap_resource,
 +	.unmap_resource = intel_unmap_page,
  	.dma_supported = dma_direct_supported,
++<<<<<<< HEAD
++=======
+ 	.mmap = dma_common_mmap,
+ 	.get_sgtable = dma_common_get_sgtable,
+ 	.get_required_mask = intel_get_required_mask,
+ };
+ 
+ static void
+ bounce_sync_single(struct device *dev, dma_addr_t addr, size_t size,
+ 		   enum dma_data_direction dir, enum dma_sync_target target)
+ {
+ 	struct dmar_domain *domain;
+ 	phys_addr_t tlb_addr;
+ 
+ 	domain = find_domain(dev);
+ 	if (WARN_ON(!domain))
+ 		return;
+ 
+ 	tlb_addr = intel_iommu_iova_to_phys(&domain->domain, addr);
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_sync_single(dev, tlb_addr, size, dir, target);
+ }
+ 
+ static dma_addr_t
+ bounce_map_single(struct device *dev, phys_addr_t paddr, size_t size,
+ 		  enum dma_data_direction dir, unsigned long attrs,
+ 		  u64 dma_mask)
+ {
+ 	size_t aligned_size = ALIGN(size, VTD_PAGE_SIZE);
+ 	struct dmar_domain *domain;
+ 	struct intel_iommu *iommu;
+ 	unsigned long iova_pfn;
+ 	unsigned long nrpages;
+ 	phys_addr_t tlb_addr;
+ 	int prot = 0;
+ 	int ret;
+ 
+ 	if (unlikely(attach_deferred(dev)))
+ 		do_deferred_attach(dev);
+ 
+ 	domain = deferred_attach_domain(dev);
+ 
+ 	if (WARN_ON(dir == DMA_NONE || !domain))
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	iommu = domain_get_iommu(domain);
+ 	if (WARN_ON(!iommu))
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	nrpages = aligned_nrpages(0, size);
+ 	iova_pfn = intel_alloc_iova(dev, domain,
+ 				    dma_to_mm_pfn(nrpages), dma_mask);
+ 	if (!iova_pfn)
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	/*
+ 	 * Check if DMAR supports zero-length reads on write only
+ 	 * mappings..
+ 	 */
+ 	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL ||
+ 			!cap_zlr(iommu->cap))
+ 		prot |= DMA_PTE_READ;
+ 	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
+ 		prot |= DMA_PTE_WRITE;
+ 
+ 	/*
+ 	 * If both the physical buffer start address and size are
+ 	 * page aligned, we don't need to use a bounce page.
+ 	 */
+ 	if (!IS_ALIGNED(paddr | size, VTD_PAGE_SIZE)) {
+ 		tlb_addr = swiotlb_tbl_map_single(dev,
+ 				__phys_to_dma(dev, io_tlb_start),
+ 				paddr, size, aligned_size, dir, attrs);
+ 		if (tlb_addr == DMA_MAPPING_ERROR) {
+ 			goto swiotlb_error;
+ 		} else {
+ 			/* Cleanup the padding area. */
+ 			void *padding_start = phys_to_virt(tlb_addr);
+ 			size_t padding_size = aligned_size;
+ 
+ 			if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
+ 			    (dir == DMA_TO_DEVICE ||
+ 			     dir == DMA_BIDIRECTIONAL)) {
+ 				padding_start += size;
+ 				padding_size -= size;
+ 			}
+ 
+ 			memset(padding_start, 0, padding_size);
+ 		}
+ 	} else {
+ 		tlb_addr = paddr;
+ 	}
+ 
+ 	ret = domain_pfn_mapping(domain, mm_to_dma_pfn(iova_pfn),
+ 				 tlb_addr >> VTD_PAGE_SHIFT, nrpages, prot);
+ 	if (ret)
+ 		goto mapping_error;
+ 
+ 	trace_bounce_map_single(dev, iova_pfn << PAGE_SHIFT, paddr, size);
+ 
+ 	return (phys_addr_t)iova_pfn << PAGE_SHIFT;
+ 
+ mapping_error:
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ 					 aligned_size, dir, attrs);
+ swiotlb_error:
+ 	free_iova_fast(&domain->iovad, iova_pfn, dma_to_mm_pfn(nrpages));
+ 	dev_err(dev, "Device bounce map: %zx@%llx dir %d --- failed\n",
+ 		size, (unsigned long long)paddr, dir);
+ 
+ 	return DMA_MAPPING_ERROR;
+ }
+ 
+ static void
+ bounce_unmap_single(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		    enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	size_t aligned_size = ALIGN(size, VTD_PAGE_SIZE);
+ 	struct dmar_domain *domain;
+ 	phys_addr_t tlb_addr;
+ 
+ 	domain = find_domain(dev);
+ 	if (WARN_ON(!domain))
+ 		return;
+ 
+ 	tlb_addr = intel_iommu_iova_to_phys(&domain->domain, dev_addr);
+ 	if (WARN_ON(!tlb_addr))
+ 		return;
+ 
+ 	intel_unmap(dev, dev_addr, size);
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ 					 aligned_size, dir, attrs);
+ 
+ 	trace_bounce_unmap_single(dev, dev_addr, size);
+ }
+ 
+ static dma_addr_t
+ bounce_map_page(struct device *dev, struct page *page, unsigned long offset,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return bounce_map_single(dev, page_to_phys(page) + offset,
+ 				 size, dir, attrs, *dev->dma_mask);
+ }
+ 
+ static dma_addr_t
+ bounce_map_resource(struct device *dev, phys_addr_t phys_addr, size_t size,
+ 		    enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return bounce_map_single(dev, phys_addr, size,
+ 				 dir, attrs, *dev->dma_mask);
+ }
+ 
+ static void
+ bounce_unmap_page(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		  enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	bounce_unmap_single(dev, dev_addr, size, dir, attrs);
+ }
+ 
+ static void
+ bounce_unmap_resource(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		      enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	bounce_unmap_single(dev, dev_addr, size, dir, attrs);
+ }
+ 
+ static void
+ bounce_unmap_sg(struct device *dev, struct scatterlist *sglist, int nelems,
+ 		enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_unmap_page(dev, sg->dma_address,
+ 				  sg_dma_len(sg), dir, attrs);
+ }
+ 
+ static int
+ bounce_map_sg(struct device *dev, struct scatterlist *sglist, int nelems,
+ 	      enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	int i;
+ 	struct scatterlist *sg;
+ 
+ 	for_each_sg(sglist, sg, nelems, i) {
+ 		sg->dma_address = bounce_map_page(dev, sg_page(sg),
+ 						  sg->offset, sg->length,
+ 						  dir, attrs);
+ 		if (sg->dma_address == DMA_MAPPING_ERROR)
+ 			goto out_unmap;
+ 		sg_dma_len(sg) = sg->length;
+ 	}
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		trace_bounce_map_sg(dev, i + 1, nelems, sg);
+ 
+ 	return nelems;
+ 
+ out_unmap:
+ 	bounce_unmap_sg(dev, sglist, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
+ 	return 0;
+ }
+ 
+ static void
+ bounce_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+ 			   size_t size, enum dma_data_direction dir)
+ {
+ 	bounce_sync_single(dev, addr, size, dir, SYNC_FOR_CPU);
+ }
+ 
+ static void
+ bounce_sync_single_for_device(struct device *dev, dma_addr_t addr,
+ 			      size_t size, enum dma_data_direction dir)
+ {
+ 	bounce_sync_single(dev, addr, size, dir, SYNC_FOR_DEVICE);
+ }
+ 
+ static void
+ bounce_sync_sg_for_cpu(struct device *dev, struct scatterlist *sglist,
+ 		       int nelems, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_sync_single(dev, sg_dma_address(sg),
+ 				   sg_dma_len(sg), dir, SYNC_FOR_CPU);
+ }
+ 
+ static void
+ bounce_sync_sg_for_device(struct device *dev, struct scatterlist *sglist,
+ 			  int nelems, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_sync_single(dev, sg_dma_address(sg),
+ 				   sg_dma_len(sg), dir, SYNC_FOR_DEVICE);
+ }
+ 
+ static const struct dma_map_ops bounce_dma_ops = {
+ 	.alloc			= intel_alloc_coherent,
+ 	.free			= intel_free_coherent,
+ 	.map_sg			= bounce_map_sg,
+ 	.unmap_sg		= bounce_unmap_sg,
+ 	.map_page		= bounce_map_page,
+ 	.unmap_page		= bounce_unmap_page,
+ 	.sync_single_for_cpu	= bounce_sync_single_for_cpu,
+ 	.sync_single_for_device	= bounce_sync_single_for_device,
+ 	.sync_sg_for_cpu	= bounce_sync_sg_for_cpu,
+ 	.sync_sg_for_device	= bounce_sync_sg_for_device,
+ 	.map_resource		= bounce_map_resource,
+ 	.unmap_resource		= bounce_unmap_resource,
+ 	.dma_supported		= dma_direct_supported,
++>>>>>>> a11bfde9c77d (iommu/vt-d: Do deferred attachment in iommu_need_mapping())
  };
  
  static inline int iommu_domain_cache_init(void)
* Unmerged path drivers/iommu/intel-iommu.c
