locking/lockdep: Reuse lock chains that have been freed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Bart Van Assche <bvanassche@acm.org>
commit de4643a77356a77bce73f64275b125b4b71a69cf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/de4643a7.failed

A previous patch introduced a lock chain leak. Fix that leak by reusing
lock chains that have been freed.

	Signed-off-by: Bart Van Assche <bvanassche@acm.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Johannes Berg <johannes@sipsolutions.net>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Waiman Long <longman@redhat.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: johannes.berg@intel.com
	Cc: tj@kernel.org
Link: https://lkml.kernel.org/r/20190214230058.196511-16-bvanassche@acm.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit de4643a77356a77bce73f64275b125b4b71a69cf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index daaed4296d60,0bb204464afe..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -284,11 -281,42 +284,44 @@@ static inline void lock_release_holdtim
  #endif
  
  /*
 - * We keep a global list of all lock classes. The list is only accessed with
 - * the lockdep spinlock lock held. free_lock_classes is a list with free
 - * elements. These elements are linked together by the lock_entry member in
 - * struct lock_class.
 + * We keep a global list of all lock classes. The list only grows,
 + * never shrinks. The list is only accessed with the lockdep
 + * spinlock lock held.
   */
  LIST_HEAD(all_lock_classes);
++<<<<<<< HEAD
++=======
+ static LIST_HEAD(free_lock_classes);
+ 
+ /**
+  * struct pending_free - information about data structures about to be freed
+  * @zapped: Head of a list with struct lock_class elements.
+  * @lock_chains_being_freed: Bitmap that indicates which lock_chains[] elements
+  *	are about to be freed.
+  */
+ struct pending_free {
+ 	struct list_head zapped;
+ 	DECLARE_BITMAP(lock_chains_being_freed, MAX_LOCKDEP_CHAINS);
+ };
+ 
+ /**
+  * struct delayed_free - data structures used for delayed freeing
+  *
+  * A data structure for delayed freeing of data structures that may be
+  * accessed by RCU readers at the time these were freed.
+  *
+  * @rcu_head:  Used to schedule an RCU callback for freeing data structures.
+  * @index:     Index of @pf to which freed data structures are added.
+  * @scheduled: Whether or not an RCU callback has been scheduled.
+  * @pf:        Array with information about data structures about to be freed.
+  */
+ static struct delayed_free {
+ 	struct rcu_head		rcu_head;
+ 	int			index;
+ 	int			scheduled;
+ 	struct pending_free	pf[2];
+ } delayed_free;
++>>>>>>> de4643a77356 (locking/lockdep: Reuse lock chains that have been freed)
  
  /*
   * The lockdep classes are in a hash-table as well, for fast lookup:
@@@ -4159,6 -4218,81 +4200,84 @@@ void lockdep_reset(void
  	raw_local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
++=======
+ /* Remove a class from a lock chain. Must be called with the graph lock held. */
+ static void remove_class_from_lock_chain(struct pending_free *pf,
+ 					 struct lock_chain *chain,
+ 					 struct lock_class *class)
+ {
+ #ifdef CONFIG_PROVE_LOCKING
+ 	struct lock_chain *new_chain;
+ 	u64 chain_key;
+ 	int i;
+ 
+ 	for (i = chain->base; i < chain->base + chain->depth; i++) {
+ 		if (chain_hlocks[i] != class - lock_classes)
+ 			continue;
+ 		/* The code below leaks one chain_hlock[] entry. */
+ 		if (--chain->depth > 0)
+ 			memmove(&chain_hlocks[i], &chain_hlocks[i + 1],
+ 				(chain->base + chain->depth - i) *
+ 				sizeof(chain_hlocks[0]));
+ 		/*
+ 		 * Each lock class occurs at most once in a lock chain so once
+ 		 * we found a match we can break out of this loop.
+ 		 */
+ 		goto recalc;
+ 	}
+ 	/* Since the chain has not been modified, return. */
+ 	return;
+ 
+ recalc:
+ 	chain_key = 0;
+ 	for (i = chain->base; i < chain->base + chain->depth; i++)
+ 		chain_key = iterate_chain_key(chain_key, chain_hlocks[i] + 1);
+ 	if (chain->depth && chain->chain_key == chain_key)
+ 		return;
+ 	/* Overwrite the chain key for concurrent RCU readers. */
+ 	WRITE_ONCE(chain->chain_key, chain_key);
+ 	/*
+ 	 * Note: calling hlist_del_rcu() from inside a
+ 	 * hlist_for_each_entry_rcu() loop is safe.
+ 	 */
+ 	hlist_del_rcu(&chain->entry);
+ 	__set_bit(chain - lock_chains, pf->lock_chains_being_freed);
+ 	if (chain->depth == 0)
+ 		return;
+ 	/*
+ 	 * If the modified lock chain matches an existing lock chain, drop
+ 	 * the modified lock chain.
+ 	 */
+ 	if (lookup_chain_cache(chain_key))
+ 		return;
+ 	new_chain = alloc_lock_chain();
+ 	if (WARN_ON_ONCE(!new_chain)) {
+ 		debug_locks_off();
+ 		return;
+ 	}
+ 	*new_chain = *chain;
+ 	hlist_add_head_rcu(&new_chain->entry, chainhashentry(chain_key));
+ #endif
+ }
+ 
+ /* Must be called with the graph lock held. */
+ static void remove_class_from_lock_chains(struct pending_free *pf,
+ 					  struct lock_class *class)
+ {
+ 	struct lock_chain *chain;
+ 	struct hlist_head *head;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
+ 		head = chainhash_table + i;
+ 		hlist_for_each_entry_rcu(chain, head, entry) {
+ 			remove_class_from_lock_chain(pf, chain, class);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> de4643a77356 (locking/lockdep: Reuse lock chains that have been freed)
  /*
   * Remove all references to a lock class. The caller must hold the graph lock.
   */
@@@ -4171,22 -4307,41 +4290,40 @@@ static void zap_class(struct lock_clas
  	 * Remove all dependencies this lock is
  	 * involved in:
  	 */
 -	for_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
 -		entry = list_entries + i;
 +	for (i = 0, entry = list_entries; i < nr_list_entries; i++, entry++) {
  		if (entry->class != class && entry->links_to != class)
  			continue;
 -		__clear_bit(i, list_entries_in_use);
 -		nr_list_entries--;
  		list_del_rcu(&entry->entry);
 +		/* Clear .class and .links_to to avoid double removal. */
 +		WRITE_ONCE(entry->class, NULL);
 +		WRITE_ONCE(entry->links_to, NULL);
  	}
 -	if (list_empty(&class->locks_after) &&
 -	    list_empty(&class->locks_before)) {
 -		list_move_tail(&class->lock_entry, &pf->zapped);
 -		hlist_del_rcu(&class->hash_entry);
 -		WRITE_ONCE(class->key, NULL);
 -		WRITE_ONCE(class->name, NULL);
 -		nr_lock_classes--;
 -	} else {
 -		WARN_ONCE(true, "%s() failed for class %s\n", __func__,
 -			  class->name);
 -	}
 +	/*
 +	 * Unhash the class and remove it from the all_lock_classes list:
 +	 */
 +	hlist_del_rcu(&class->hash_entry);
 +	list_del(&class->lock_entry);
  
++<<<<<<< HEAD
 +	RCU_INIT_POINTER(class->key, NULL);
 +	RCU_INIT_POINTER(class->name, NULL);
++=======
+ 	remove_class_from_lock_chains(pf, class);
+ }
+ 
+ static void reinit_class(struct lock_class *class)
+ {
+ 	void *const p = class;
+ 	const unsigned int offset = offsetof(struct lock_class, key);
+ 
+ 	WARN_ON_ONCE(!class->lock_entry.next);
+ 	WARN_ON_ONCE(!list_empty(&class->locks_after));
+ 	WARN_ON_ONCE(!list_empty(&class->locks_before));
+ 	memset(p + offset, 0, sizeof(*class) - offset);
+ 	WARN_ON_ONCE(!class->lock_entry.next);
+ 	WARN_ON_ONCE(!list_empty(&class->locks_after));
+ 	WARN_ON_ONCE(!list_empty(&class->locks_before));
++>>>>>>> de4643a77356 (locking/lockdep: Reuse lock chains that have been freed)
  }
  
  static inline int within(const void *addr, void *start, unsigned long size)
@@@ -4194,7 -4349,93 +4331,97 @@@
  	return addr >= start && addr < start + size;
  }
  
++<<<<<<< HEAD
 +static void __lockdep_free_key_range(void *start, unsigned long size)
++=======
+ static bool inside_selftest(void)
+ {
+ 	return current == lockdep_selftest_task_struct;
+ }
+ 
+ /* The caller must hold the graph lock. */
+ static struct pending_free *get_pending_free(void)
+ {
+ 	return delayed_free.pf + delayed_free.index;
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *cb);
+ 
+ /*
+  * Schedule an RCU callback if no RCU callback is pending. Must be called with
+  * the graph lock held.
+  */
+ static void call_rcu_zapped(struct pending_free *pf)
+ {
+ 	WARN_ON_ONCE(inside_selftest());
+ 
+ 	if (list_empty(&pf->zapped))
+ 		return;
+ 
+ 	if (delayed_free.scheduled)
+ 		return;
+ 
+ 	delayed_free.scheduled = true;
+ 
+ 	WARN_ON_ONCE(delayed_free.pf + delayed_free.index != pf);
+ 	delayed_free.index ^= 1;
+ 
+ 	call_rcu(&delayed_free.rcu_head, free_zapped_rcu);
+ }
+ 
+ /* The caller must hold the graph lock. May be called from RCU context. */
+ static void __free_zapped_classes(struct pending_free *pf)
+ {
+ 	struct lock_class *class;
+ 
+ 	list_for_each_entry(class, &pf->zapped, lock_entry)
+ 		reinit_class(class);
+ 
+ 	list_splice_init(&pf->zapped, &free_lock_classes);
+ 
+ #ifdef CONFIG_PROVE_LOCKING
+ 	bitmap_andnot(lock_chains_in_use, lock_chains_in_use,
+ 		      pf->lock_chains_being_freed, ARRAY_SIZE(lock_chains));
+ 	bitmap_clear(pf->lock_chains_being_freed, 0, ARRAY_SIZE(lock_chains));
+ #endif
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *ch)
+ {
+ 	struct pending_free *pf;
+ 	unsigned long flags;
+ 
+ 	if (WARN_ON_ONCE(ch != &delayed_free.rcu_head))
+ 		return;
+ 
+ 	raw_local_irq_save(flags);
+ 	if (!graph_lock())
+ 		goto out_irq;
+ 
+ 	/* closed head */
+ 	pf = delayed_free.pf + (delayed_free.index ^ 1);
+ 	__free_zapped_classes(pf);
+ 	delayed_free.scheduled = false;
+ 
+ 	/*
+ 	 * If there's anything on the open list, close and start a new callback.
+ 	 */
+ 	call_rcu_zapped(delayed_free.pf + delayed_free.index);
+ 
+ 	graph_unlock();
+ out_irq:
+ 	raw_local_irq_restore(flags);
+ }
+ 
+ /*
+  * Remove all lock classes from the class hash table and from the
+  * all_lock_classes list whose key or name is in the address range [start,
+  * start + size). Move these lock classes to the zapped_classes list. Must
+  * be called with the graph lock held.
+  */
+ static void __lockdep_free_key_range(struct pending_free *pf, void *start,
+ 				     unsigned long size)
++>>>>>>> de4643a77356 (locking/lockdep: Reuse lock chains that have been freed)
  {
  	struct lock_class *class;
  	struct hlist_head *head;
* Unmerged path kernel/locking/lockdep.c
