sched/fair: Clean-up update_sg_lb_stats parameters

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Quentin Perret <quentin.perret@arm.com>
commit 630246a06ae2a7a12d1fce85f1e5681032982791
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/630246a0.failed

In preparation for the introduction of a new root domain flag which can
be set during load balance (the 'overutilized' flag), clean-up the set
of parameters passed to update_sg_lb_stats(). More specifically, the
'local_group' and 'local_idx' parameters can be removed since they can
easily be reconstructed from within the function.

While at it, transform the 'overload' parameter into a flag stored in
the 'sg_status' parameter hence facilitating the definition of new flags
when needed.

	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Suggested-by: Valentin Schneider <valentin.schneider@arm.com>
	Signed-off-by: Quentin Perret <quentin.perret@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: adharmap@codeaurora.org
	Cc: chris.redpath@arm.com
	Cc: currojerez@riseup.net
	Cc: dietmar.eggemann@arm.com
	Cc: edubezval@gmail.com
	Cc: gregkh@linuxfoundation.org
	Cc: javi.merino@kernel.org
	Cc: joel@joelfernandes.org
	Cc: juri.lelli@redhat.com
	Cc: morten.rasmussen@arm.com
	Cc: patrick.bellasi@arm.com
	Cc: pkondeti@codeaurora.org
	Cc: rjw@rjwysocki.net
	Cc: skannan@codeaurora.org
	Cc: smuckle@google.com
	Cc: srinivas.pandruvada@linux.intel.com
	Cc: thara.gopinath@linaro.org
	Cc: tkjos@google.com
	Cc: vincent.guittot@linaro.org
	Cc: viresh.kumar@linaro.org
Link: https://lkml.kernel.org/r/20181203095628.11858-12-quentin.perret@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 630246a06ae2a7a12d1fce85f1e5681032982791)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/sched.h
diff --cc kernel/sched/sched.h
index 2f0ee3f2ef1e,d4d984846924..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -712,6 -710,15 +712,18 @@@ static inline bool sched_asym_prefer(in
  	return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
  }
  
++<<<<<<< HEAD
++=======
+ struct perf_domain {
+ 	struct em_perf_domain *em_pd;
+ 	struct perf_domain *next;
+ 	struct rcu_head rcu;
+ };
+ 
+ /* Scheduling group status flags */
+ #define SG_OVERLOAD		0x1 /* More than one runnable task on a CPU. */
+ 
++>>>>>>> 630246a06ae2 (sched/fair: Clean-up update_sg_lb_stats parameters)
  /*
   * We add the notion of a root-domain which will be used to define per-domain
   * variables. Each exclusive cpuset essentially defines an island domain by
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 64d8aacf573c..c0eb8503d279 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -7966,16 +7966,16 @@ static bool update_nohz_stats(struct rq *rq, bool force)
  * update_sg_lb_stats - Update sched_group's statistics for load balancing.
  * @env: The load balancing environment.
  * @group: sched_group whose statistics are to be updated.
- * @load_idx: Load index of sched_domain of this_cpu for load calc.
- * @local_group: Does group contain this_cpu.
  * @sgs: variable to hold the statistics for this group.
- * @overload: Indicate pullable load (e.g. >1 runnable task).
+ * @sg_status: Holds flag indicating the status of the sched_group
  */
 static inline void update_sg_lb_stats(struct lb_env *env,
-			struct sched_group *group, int load_idx,
-			int local_group, struct sg_lb_stats *sgs,
-			bool *overload)
+				      struct sched_group *group,
+				      struct sg_lb_stats *sgs,
+				      int *sg_status)
 {
+	int local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(group));
+	int load_idx = get_sd_load_idx(env->sd, env->idle);
 	unsigned long load;
 	int i, nr_running;
 
@@ -7999,7 +7999,7 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 
 		nr_running = rq->nr_running;
 		if (nr_running > 1)
-			*overload = true;
+			*sg_status |= SG_OVERLOAD;
 
 #ifdef CONFIG_NUMA_BALANCING
 		sgs->nr_numa_running += rq->nr_numa_running;
@@ -8015,7 +8015,7 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 		if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
 		    sgs->group_misfit_task_load < rq->misfit_task_load) {
 			sgs->group_misfit_task_load = rq->misfit_task_load;
-			*overload = 1;
+			*sg_status |= SG_OVERLOAD;
 		}
 	}
 
@@ -8160,17 +8160,14 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 	struct sched_group *sg = env->sd->groups;
 	struct sg_lb_stats *local = &sds->local_stat;
 	struct sg_lb_stats tmp_sgs;
-	int load_idx;
-	bool overload = false;
 	bool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;
+	int sg_status = 0;
 
 #ifdef CONFIG_NO_HZ_COMMON
 	if (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))
 		env->flags |= LBF_NOHZ_STATS;
 #endif
 
-	load_idx = get_sd_load_idx(env->sd, env->idle);
-
 	do {
 		struct sg_lb_stats *sgs = &tmp_sgs;
 		int local_group;
@@ -8185,8 +8182,7 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 				update_group_capacity(env->sd, env->dst_cpu);
 		}
 
-		update_sg_lb_stats(env, sg, load_idx, local_group, sgs,
-						&overload);
+		update_sg_lb_stats(env, sg, sgs, &sg_status);
 
 		if (local_group)
 			goto next_group;
@@ -8236,8 +8232,7 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 
 	if (!env->sd->parent) {
 		/* update overload indicator if we are at root domain */
-		if (READ_ONCE(env->dst_rq->rd->overload) != overload)
-			WRITE_ONCE(env->dst_rq->rd->overload, overload);
+		WRITE_ONCE(env->dst_rq->rd->overload, sg_status & SG_OVERLOAD);
 	}
 }
 
* Unmerged path kernel/sched/sched.h
