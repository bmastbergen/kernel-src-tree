xprtrdma: Reduce context switching due to Local Invalidation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Chuck Lever <chuck.lever@oracle.com>
commit d8099feda4833bab96b1bf312e9e6aad6b771570
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/d8099fed.failed

Since commit ba69cd122ece ("xprtrdma: Remove support for FMR memory
registration"), FRWR is the only supported memory registration mode.

We can take advantage of the asynchronous nature of FRWR's LOCAL_INV
Work Requests to get rid of the completion wait by having the
LOCAL_INV completion handler take care of DMA unmapping MRs and
waking the upper layer RPC waiter.

This eliminates two context switches when local invalidation is
necessary. As a side benefit, we will no longer need the per-xprt
deferred completion work queue.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit d8099feda4833bab96b1bf312e9e6aad6b771570)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/frwr_ops.c
#	net/sunrpc/xprtrdma/rpc_rdma.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/frwr_ops.c
index 4f4311900f93,0b6dad7580a1..000000000000
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@@ -521,18 -490,64 +521,26 @@@ void frwr_reminv(struct rpcrdma_rep *re
  		}
  }
  
 -static void __frwr_release_mr(struct ib_wc *wc, struct rpcrdma_mr *mr)
 -{
 -	if (wc->status != IB_WC_SUCCESS)
 -		rpcrdma_mr_recycle(mr);
 -	else
 -		rpcrdma_mr_unmap_and_put(mr);
 -}
 -
 -/**
 - * frwr_wc_localinv - Invoked by RDMA provider for a LOCAL_INV WC
 - * @cq:	completion queue (ignored)
 - * @wc:	completed WR
 - *
 - */
 -static void frwr_wc_localinv(struct ib_cq *cq, struct ib_wc *wc)
 -{
 -	struct ib_cqe *cqe = wc->wr_cqe;
 -	struct rpcrdma_frwr *frwr =
 -		container_of(cqe, struct rpcrdma_frwr, fr_cqe);
 -	struct rpcrdma_mr *mr = container_of(frwr, struct rpcrdma_mr, frwr);
 -
 -	/* WARNING: Only wr_cqe and status are reliable at this point */
 -	trace_xprtrdma_wc_li(wc, frwr);
 -	__frwr_release_mr(wc, mr);
 -}
 -
 -/**
 - * frwr_wc_localinv_wake - Invoked by RDMA provider for a LOCAL_INV WC
 - * @cq:	completion queue (ignored)
 - * @wc:	completed WR
 - *
 - * Awaken anyone waiting for an MR to finish being fenced.
 - */
 -static void frwr_wc_localinv_wake(struct ib_cq *cq, struct ib_wc *wc)
 -{
 -	struct ib_cqe *cqe = wc->wr_cqe;
 -	struct rpcrdma_frwr *frwr =
 -		container_of(cqe, struct rpcrdma_frwr, fr_cqe);
 -	struct rpcrdma_mr *mr = container_of(frwr, struct rpcrdma_mr, frwr);
 -
 -	/* WARNING: Only wr_cqe and status are reliable at this point */
 -	trace_xprtrdma_wc_li_wake(wc, frwr);
 -	complete(&frwr->fr_linv_done);
 -	__frwr_release_mr(wc, mr);
 -}
 -
  /**
   * frwr_unmap_sync - invalidate memory regions that were registered for @req
 - * @r_xprt: controlling transport instance
 - * @req: rpcrdma_req with a non-empty list of MRs to process
 + * @r_xprt: controlling transport
 + * @mrs: list of MRs to process
 + *
++<<<<<<< HEAD
 + * Sleeps until it is safe for the host CPU to access the
 + * previously mapped memory regions.
   *
 + * Caller ensures that @mrs is not empty before the call. This
 + * function empties the list.
++=======
+  * Sleeps until it is safe for the host CPU to access the previously mapped
+  * memory regions. This guarantees that registered MRs are properly fenced
+  * from the server before the RPC consumer accesses the data in them. It
+  * also ensures proper Send flow control: waking the next RPC waits until
+  * this RPC has relinquished all its Send Queue entries.
++>>>>>>> d8099feda483 (xprtrdma: Reduce context switching due to Local Invalidation)
   */
 -void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 +void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct list_head *mrs)
  {
  	struct ib_send_wr *first, **prev, *last;
  	const struct ib_send_wr *bad_wr;
diff --cc net/sunrpc/xprtrdma/rpc_rdma.c
index 2469b4d91dbb,33b6e6a03f68..000000000000
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@@ -1264,24 -1268,15 +1264,34 @@@ out_badheader
  	goto out;
  }
  
- void rpcrdma_release_rqst(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
+ /* Ensure that any DMA mapped pages associated with
+  * the Send of the RPC Call have been unmapped before
+  * allowing the RPC to complete. This protects argument
+  * memory not controlled by the RPC client from being
+  * re-used before we're done with it.
+  */
+ static void rpcrdma_release_tx(struct rpcrdma_xprt *r_xprt,
+ 			       struct rpcrdma_req *req)
  {
++<<<<<<< HEAD
 +	/* Invalidate and unmap the data payloads before waking
 +	 * the waiting application. This guarantees the memory
 +	 * regions are properly fenced from the server before the
 +	 * application accesses the data. It also ensures proper
 +	 * send flow control: waking the next RPC waits until this
 +	 * RPC has relinquished all its Send Queue entries.
 +	 */
 +	if (!list_empty(&req->rl_registered))
 +		frwr_unmap_sync(r_xprt, &req->rl_registered);
 +
 +	/* Ensure that any DMA mapped pages associated with
 +	 * the Send of the RPC Call have been unmapped before
 +	 * allowing the RPC to complete. This protects argument
 +	 * memory not controlled by the RPC client from being
 +	 * re-used before we're done with it.
 +	 */
++=======
++>>>>>>> d8099feda483 (xprtrdma: Reduce context switching due to Local Invalidation)
  	if (test_bit(RPCRDMA_REQ_F_TX_RESOURCES, &req->rl_flags)) {
  		r_xprt->rx_stats.reply_waits_for_send++;
  		out_of_line_wait_on_bit(&req->rl_flags,
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index cc5a576b0242,e465221c9c96..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -237,18 -239,12 +236,23 @@@ struct rpcrdma_sendctx 
   * An external memory region is any buffer or page that is registered
   * on the fly (ie, not pre-registered).
   */
++<<<<<<< HEAD
 +enum rpcrdma_frwr_state {
 +	FRWR_IS_INVALID,	/* ready to be used */
 +	FRWR_IS_VALID,		/* in use */
 +	FRWR_FLUSHED_FR,	/* flushed FASTREG WR */
 +	FRWR_FLUSHED_LI,	/* flushed LOCALINV WR */
 +};
 +
++=======
+ struct rpcrdma_req;
++>>>>>>> d8099feda483 (xprtrdma: Reduce context switching due to Local Invalidation)
  struct rpcrdma_frwr {
  	struct ib_mr			*fr_mr;
  	struct ib_cqe			fr_cqe;
 +	enum rpcrdma_frwr_state		fr_state;
  	struct completion		fr_linv_done;
+ 	struct rpcrdma_req		*fr_req;
  	union {
  		struct ib_reg_wr	fr_regwr;
  		struct ib_send_wr	fr_invwr;
@@@ -586,8 -560,8 +589,13 @@@ struct rpcrdma_mr_seg *frwr_map(struct 
  				struct rpcrdma_mr **mr);
  int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
  void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
++<<<<<<< HEAD
 +void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt,
 +		     struct list_head *mrs);
++=======
+ void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
+ void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
++>>>>>>> d8099feda483 (xprtrdma: Reduce context switching due to Local Invalidation)
  
  /*
   * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
diff --git a/include/trace/events/rpcrdma.h b/include/trace/events/rpcrdma.h
index bfa903f6b855..71fd3119f0ca 100644
--- a/include/trace/events/rpcrdma.h
+++ b/include/trace/events/rpcrdma.h
@@ -691,6 +691,7 @@ TRACE_EVENT(xprtrdma_wc_receive,
 DEFINE_FRWR_DONE_EVENT(xprtrdma_wc_fastreg);
 DEFINE_FRWR_DONE_EVENT(xprtrdma_wc_li);
 DEFINE_FRWR_DONE_EVENT(xprtrdma_wc_li_wake);
+DEFINE_FRWR_DONE_EVENT(xprtrdma_wc_li_done);
 
 TRACE_EVENT(xprtrdma_frwr_alloc,
 	TP_PROTO(
* Unmerged path net/sunrpc/xprtrdma/frwr_ops.c
* Unmerged path net/sunrpc/xprtrdma/rpc_rdma.c
diff --git a/net/sunrpc/xprtrdma/verbs.c b/net/sunrpc/xprtrdma/verbs.c
index a585b191fe98..c78daa1e5bb1 100644
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@ -89,14 +89,12 @@ static void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
  */
 static void rpcrdma_xprt_drain(struct rpcrdma_xprt *r_xprt)
 {
-	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 
 	/* Flush Receives, then wait for deferred Reply work
 	 * to complete.
 	 */
 	ib_drain_rq(ia->ri_id->qp);
-	drain_workqueue(buf->rb_completion_wq);
 
 	/* Deferred Reply processing might have scheduled
 	 * local invalidations.
@@ -1054,7 +1052,6 @@ static bool rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt, bool temp)
 
 	rep->rr_cqe.done = rpcrdma_wc_receive;
 	rep->rr_rxprt = r_xprt;
-	INIT_WORK(&rep->rr_work, rpcrdma_deferred_completion);
 	rep->rr_recv_wr.next = NULL;
 	rep->rr_recv_wr.wr_cqe = &rep->rr_cqe;
 	rep->rr_recv_wr.sg_list = &rep->rr_rdmabuf->rg_iov;
@@ -1111,15 +1108,6 @@ rpcrdma_buffer_create(struct rpcrdma_xprt *r_xprt)
 	if (rc)
 		goto out;
 
-	buf->rb_completion_wq = alloc_workqueue("rpcrdma-%s",
-						WQ_MEM_RECLAIM | WQ_HIGHPRI,
-						0,
-			r_xprt->rx_xprt.address_strings[RPC_DISPLAY_ADDR]);
-	if (!buf->rb_completion_wq) {
-		rc = -ENOMEM;
-		goto out;
-	}
-
 	return 0;
 out:
 	rpcrdma_buffer_destroy(buf);
@@ -1193,11 +1181,6 @@ rpcrdma_buffer_destroy(struct rpcrdma_buffer *buf)
 {
 	cancel_delayed_work_sync(&buf->rb_refresh_worker);
 
-	if (buf->rb_completion_wq) {
-		destroy_workqueue(buf->rb_completion_wq);
-		buf->rb_completion_wq = NULL;
-	}
-
 	rpcrdma_sendctxs_destroy(buf);
 
 	while (!list_empty(&buf->rb_recv_bufs)) {
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
