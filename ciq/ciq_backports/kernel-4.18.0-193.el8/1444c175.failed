net: sched: copy tunnel info when setting flow_action entry->tunnel

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] sched: copy tunnel info when setting flow_action entry->tunnel (Ivan Vecera) [1739606]
Rebuild_FUZZ: 96.12%
commit-author Vlad Buslov <vladbu@mellanox.com>
commit 1444c175a37443d3f6d3db825df050741452c3c3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1444c175.failed

In order to remove dependency on rtnl lock, modify tc_setup_flow_action()
to copy tunnel info, instead of just saving pointer to tunnel_key action
tunnel info. This is necessary to prevent concurrent action overwrite from
releasing tunnel info while it is being used by rtnl-unlocked driver.

Implement helper tcf_tunnel_info_copy() that is used to copy tunnel info
with all its options to dynamically allocated memory block. Modify
tc_cleanup_flow_action() to free dynamically allocated tunnel info.

	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Acked-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1444c175a37443d3f6d3db825df050741452c3c3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_api.c
diff --cc net/sched/cls_api.c
index 081ac3d0fb50,671ca905dbb5..000000000000
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@@ -3266,13 -3072,228 +3266,203 @@@ int tc_setup_cb_call(struct tcf_block *
  	}
  	return ok_count;
  }
 -
 -int tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
 -		     void *type_data, bool err_stop, bool rtnl_held)
 -{
 -	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
 -	int ok_count;
 -
 -retry:
 -	if (take_rtnl)
 -		rtnl_lock();
 -	down_read(&block->cb_lock);
 -	/* Need to obtain rtnl lock if block is bound to devs that require it.
 -	 * In block bind code cb_lock is obtained while holding rtnl, so we must
 -	 * obtain the locks in same order here.
 -	 */
 -	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
 -		up_read(&block->cb_lock);
 -		take_rtnl = true;
 -		goto retry;
 -	}
 -
 -	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
 -
 -	up_read(&block->cb_lock);
 -	if (take_rtnl)
 -		rtnl_unlock();
 -	return ok_count;
 -}
  EXPORT_SYMBOL(tc_setup_cb_call);
  
++<<<<<<< HEAD
++=======
+ /* Non-destructive filter add. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offloads counter. On failure,
+  * previously offloaded filter is considered to be intact and offloads counter
+  * is not decremented.
+  */
+ 
+ int tc_setup_cb_add(struct tcf_block *block, struct tcf_proto *tp,
+ 		    enum tc_setup_type type, void *type_data, bool err_stop,
+ 		    u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags,
+ 					  ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_add);
+ 
+ /* Destructive filter replace. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offload counter. On failure,
+  * previously offloaded filter is considered to be destroyed and offload counter
+  * is decremented.
+  */
+ 
+ int tc_setup_cb_replace(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *old_flags, unsigned int *old_in_hw_count,
+ 			u32 *new_flags, unsigned int *new_in_hw_count,
+ 			bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, old_in_hw_count, old_flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, new_in_hw_count,
+ 					  new_flags, ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_replace);
+ 
+ /* Destroy filter and decrement block offload counter, if filter was previously
+  * offloaded.
+  */
+ 
+ int tc_setup_cb_destroy(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, in_hw_count, flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_destroy);
+ 
+ int tc_setup_cb_reoffload(struct tcf_block *block, struct tcf_proto *tp,
+ 			  bool add, flow_setup_cb_t *cb,
+ 			  enum tc_setup_type type, void *type_data,
+ 			  void *cb_priv, u32 *flags, unsigned int *in_hw_count)
+ {
+ 	int err = cb(type, type_data, cb_priv);
+ 
+ 	if (err) {
+ 		if (add && tc_skip_sw(*flags))
+ 			return err;
+ 	} else {
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags, 1,
+ 					  add);
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_reoffload);
+ 
+ void tc_cleanup_flow_action(struct flow_action *flow_action)
+ {
+ 	struct flow_action_entry *entry;
+ 	int i;
+ 
+ 	flow_action_for_each(i, entry, flow_action) {
+ 		switch (entry->id) {
+ 		case FLOW_ACTION_REDIRECT:
+ 		case FLOW_ACTION_MIRRED:
+ 		case FLOW_ACTION_REDIRECT_INGRESS:
+ 		case FLOW_ACTION_MIRRED_INGRESS:
+ 			if (entry->dev)
+ 				dev_put(entry->dev);
+ 			break;
+ 		case FLOW_ACTION_TUNNEL_ENCAP:
+ 			kfree(entry->tunnel);
+ 			break;
+ 		default:
+ 			break;
+ 		}
+ 	}
+ }
+ EXPORT_SYMBOL(tc_cleanup_flow_action);
+ 
++>>>>>>> 1444c175a374 (net: sched: copy tunnel info when setting flow_action entry->tunnel)
  int tc_setup_flow_action(struct flow_action *flow_action,
 -			 const struct tcf_exts *exts, bool rtnl_held)
 +			 const struct tcf_exts *exts)
  {
  	const struct tc_action *act;
 -	int i, j, k, err = 0;
 +	int i, j, k;
  
  	if (!exts)
  		return 0;
diff --git a/include/net/tc_act/tc_tunnel_key.h b/include/net/tc_act/tc_tunnel_key.h
index 23d5b8b19f3e..0a6f09ab9761 100644
--- a/include/net/tc_act/tc_tunnel_key.h
+++ b/include/net/tc_act/tc_tunnel_key.h
@@ -63,4 +63,21 @@ static inline struct ip_tunnel_info *tcf_tunnel_info(const struct tc_action *a)
 	return NULL;
 #endif
 }
+
+static inline struct ip_tunnel_info *
+tcf_tunnel_info_copy(const struct tc_action *a)
+{
+#ifdef CONFIG_NET_CLS_ACT
+	struct ip_tunnel_info *tun = tcf_tunnel_info(a);
+
+	if (tun) {
+		size_t tun_size = sizeof(*tun) + tun->options_len;
+		struct ip_tunnel_info *tun_copy = kmemdup(tun, tun_size,
+							  GFP_KERNEL);
+
+		return tun_copy;
+	}
+#endif
+	return NULL;
+}
 #endif /* __NET_TC_TUNNEL_KEY_H */
* Unmerged path net/sched/cls_api.c
