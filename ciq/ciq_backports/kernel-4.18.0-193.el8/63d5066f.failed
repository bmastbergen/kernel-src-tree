mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping)

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping) (Jerome Glisse) [1498655 1597758]
Rebuild_FUZZ: 97.60%
commit-author Jérôme Glisse <jglisse@redhat.com>
commit 63d5066f6e5a1713d0247ef38f0add545408896b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/63d5066f.failed

HMM mirror is a device driver helpers to mirror range of virtual address.
It means that the process jobs running on the device can access the same
virtual address as the CPU threads of that process.  This patch adds
support for hugetlbfs mapping (ie range of virtual address that are mmap
of a hugetlbfs).

[rcampbell@nvidia.com: fix initial PFN for hugetlbfs pages]
  Link: http://lkml.kernel.org/r/20190419233536.8080-1-rcampbell@nvidia.com
Link: http://lkml.kernel.org/r/20190403193318.16478-9-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Signed-off-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 63d5066f6e5a1713d0247ef38f0add545408896b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index e51dc3e6210f,e5834082de60..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -148,11 -179,66 +148,72 @@@ struct hmm_range 
  	uint64_t		*pfns;
  	const uint64_t		*flags;
  	const uint64_t		*values;
++<<<<<<< HEAD
++=======
+ 	uint64_t		default_flags;
+ 	uint64_t		pfn_flags_mask;
+ 	uint8_t			page_shift;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  	uint8_t			pfn_shift;
  	bool			valid;
  };
  
  /*
++<<<<<<< HEAD
++=======
+  * hmm_range_page_shift() - return the page shift for the range
+  * @range: range being queried
+  * Returns: page shift (page size = 1 << page shift) for the range
+  */
+ static inline unsigned hmm_range_page_shift(const struct hmm_range *range)
+ {
+ 	return range->page_shift;
+ }
+ 
+ /*
+  * hmm_range_page_size() - return the page size for the range
+  * @range: range being queried
+  * Returns: page size for the range in bytes
+  */
+ static inline unsigned long hmm_range_page_size(const struct hmm_range *range)
+ {
+ 	return 1UL << hmm_range_page_shift(range);
+ }
+ 
+ /*
+  * hmm_range_wait_until_valid() - wait for range to be valid
+  * @range: range affected by invalidation to wait on
+  * @timeout: time out for wait in ms (ie abort wait after that period of time)
+  * Returns: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_wait_until_valid(struct hmm_range *range,
+ 					      unsigned long timeout)
+ {
+ 	/* Check if mm is dead ? */
+ 	if (range->hmm == NULL || range->hmm->dead || range->hmm->mm == NULL) {
+ 		range->valid = false;
+ 		return false;
+ 	}
+ 	if (range->valid)
+ 		return true;
+ 	wait_event_timeout(range->hmm->wq, range->valid || range->hmm->dead,
+ 			   msecs_to_jiffies(timeout));
+ 	/* Return current valid status just in case we get lucky */
+ 	return range->valid;
+ }
+ 
+ /*
+  * hmm_range_valid() - test if a range is valid or not
+  * @range: range
+  * Returns: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_valid(struct hmm_range *range)
+ {
+ 	return range->valid;
+ }
+ 
+ /*
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
   * hmm_pfn_to_page() - return struct page pointed to by a valid HMM pfn
   * @range: range use to decode HMM pfn value
   * @pfn: HMM pfn value to get corresponding struct page from
@@@ -354,41 -440,78 +415,109 @@@ void hmm_mirror_unregister(struct hmm_m
  
  
  /*
 - * Please see Documentation/vm/hmm.rst for how to use the range API.
 + * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device
 + * driver lock that serializes device page table updates, then call
 + * hmm_vma_range_done(), to check if the snapshot is still valid. The same
 + * device driver page table update lock must also be used in the
 + * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page
 + * table invalidation serializes on it.
 + *
 + * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL
 + * hmm_vma_get_pfns() WITHOUT ERROR !
 + *
 + * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !
   */
++<<<<<<< HEAD
 +int hmm_vma_get_pfns(struct hmm_range *range);
 +bool hmm_vma_range_done(struct hmm_range *range);
 +
++=======
+ int hmm_range_register(struct hmm_range *range,
+ 		       struct mm_struct *mm,
+ 		       unsigned long start,
+ 		       unsigned long end,
+ 		       unsigned page_shift);
+ void hmm_range_unregister(struct hmm_range *range);
+ long hmm_range_snapshot(struct hmm_range *range);
+ long hmm_range_fault(struct hmm_range *range, bool block);
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  
  /*
 - * HMM_RANGE_DEFAULT_TIMEOUT - default timeout (ms) when waiting for a range
 + * Fault memory on behalf of device driver. Unlike handle_mm_fault(), this will
 + * not migrate any device memory back to system memory. The HMM pfn array will
 + * be updated with the fault result and current snapshot of the CPU page table
 + * for the range.
   *
 - * When waiting for mmu notifiers we need some kind of time out otherwise we
 - * could potentialy wait for ever, 1000ms ie 1s sounds like a long time to
 - * wait already.
 + * The mmap_sem must be taken in read mode before entering and it might be
 + * dropped by the function if the block argument is false. In that case, the
 + * function returns -EAGAIN.
 + *
 + * Return value does not reflect if the fault was successful for every single
 + * address or not. Therefore, the caller must to inspect the HMM pfn array to
 + * determine fault status for each address.
 + *
 + * Trying to fault inside an invalid vma will result in -EINVAL.
 + *
 + * See the function description in mm/hmm.c for further documentation.
   */
++<<<<<<< HEAD
 +int hmm_vma_fault(struct hmm_range *range, bool block);
++=======
+ #define HMM_RANGE_DEFAULT_TIMEOUT 1000
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline bool hmm_vma_range_done(struct hmm_range *range)
+ {
+ 	bool ret = hmm_range_valid(range);
+ 
+ 	hmm_range_unregister(range);
+ 	return ret;
+ }
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline int hmm_vma_fault(struct hmm_range *range, bool block)
+ {
+ 	long ret;
+ 
+ 	/*
+ 	 * With the old API the driver must set each individual entries with
+ 	 * the requested flags (valid, write, ...). So here we set the mask to
+ 	 * keep intact the entries provided by the driver and zero out the
+ 	 * default_flags.
+ 	 */
+ 	range->default_flags = 0;
+ 	range->pfn_flags_mask = -1UL;
+ 
+ 	ret = hmm_range_register(range, range->vma->vm_mm,
+ 				 range->start, range->end,
+ 				 PAGE_SHIFT);
+ 	if (ret)
+ 		return (int)ret;
+ 
+ 	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
+ 		/*
+ 		 * The mmap_sem was taken by driver we release it here and
+ 		 * returns -EAGAIN which correspond to mmap_sem have been
+ 		 * drop in the old API.
+ 		 */
+ 		up_read(&range->vma->vm_mm->mmap_sem);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	ret = hmm_range_fault(range, block);
+ 	if (ret <= 0) {
+ 		if (ret == -EBUSY || !ret) {
+ 			/* Same as above  drop mmap_sem to match old API. */
+ 			up_read(&range->vma->vm_mm->mmap_sem);
+ 			ret = -EBUSY;
+ 		} else if (ret == -EAGAIN)
+ 			ret = -EBUSY;
+ 		hmm_range_unregister(range);
+ 		return ret;
+ 	}
+ 	return 0;
+ }
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  
  /* Below are for HMM internal use only! Not to be used by device driver! */
  void hmm_mm_destroy(struct mm_struct *mm);
diff --cc mm/hmm.c
index 4c052ccc4e21,52e40be56dc7..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -678,270 -790,293 +743,462 @@@ static void hmm_pfns_special(struct hmm
  }
  
  /*
++<<<<<<< HEAD
 + * hmm_vma_get_pfns() - snapshot CPU page table for a range of virtual addresses
 + * @range: range being snapshotted
 + * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          vma permission, 0 success
++=======
+  * hmm_range_register() - start tracking change to CPU page table over a range
+  * @range: range
+  * @mm: the mm struct for the range of virtual address
+  * @start: start virtual address (inclusive)
+  * @end: end virtual address (exclusive)
+  * @page_shift: expect page shift for the range
+  * Returns 0 on success, -EFAULT if the address space is no longer valid
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
 + *
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See hmm_vma_range_done() for further
 + * information.
   *
 - * Track updates to the CPU page table see include/linux/hmm.h
 + * The range struct is initialized here. It tracks the CPU page table, but only
 + * if the function returns success (0), in which case the caller must then call
 + * hmm_vma_range_done() to stop CPU page table update tracking on this range.
 + *
 + * NOT CALLING hmm_vma_range_done() IF FUNCTION RETURNS 0 WILL LEAD TO SERIOUS
 + * MEMORY CORRUPTION ! YOU HAVE BEEN WARNED !
   */
++<<<<<<< HEAD
 +int hmm_vma_get_pfns(struct hmm_range *range)
 +{
 +	struct vm_area_struct *vma = range->vma;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct mm_walk mm_walk;
 +	struct hmm *hmm;
 +
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
 +
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm)
 +		return -ENOMEM;
 +	/* Caller must have registered a mirror, via hmm_mirror_register() ! */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
++=======
+ int hmm_range_register(struct hmm_range *range,
+ 		       struct mm_struct *mm,
+ 		       unsigned long start,
+ 		       unsigned long end,
+ 		       unsigned page_shift)
+ {
+ 	unsigned long mask = ((1UL << page_shift) - 1UL);
+ 
+ 	range->valid = false;
+ 	range->hmm = NULL;
+ 
+ 	if ((start & mask) || (end & mask))
+ 		return -EINVAL;
+ 	if (start >= end)
+ 		return -EINVAL;
+ 
+ 	range->page_shift = page_shift;
+ 	range->start = start;
+ 	range->end = end;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  
 -	range->hmm = hmm_get_or_create(mm);
 -	if (!range->hmm)
 -		return -EFAULT;
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (range->hmm->mm == NULL || range->hmm->dead) {
 -		hmm_put(range->hmm);
 -		return -EFAULT;
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
  	}
  
  	/* Initialize range to track CPU page table update */
 -	mutex_lock(&range->hmm->lock);
 -
 -	list_add_rcu(&range->list, &range->hmm->ranges);
 -
 -	/*
 -	 * If there are any concurrent notifiers we have to wait for them for
 -	 * the range to be valid (see hmm_range_wait_until_valid()).
 -	 */
 -	if (!range->hmm->notifiers)
 -		range->valid = true;
 -	mutex_unlock(&range->hmm->lock);
 -
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = false;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	walk_page_range(range->start, range->end, &mm_walk);
  	return 0;
  }
 -EXPORT_SYMBOL(hmm_range_register);
 +EXPORT_SYMBOL(hmm_vma_get_pfns);
  
  /*
 - * hmm_range_unregister() - stop tracking change to CPU page table over a range
 - * @range: range
 + * hmm_vma_range_done() - stop tracking change to CPU page table over a range
 + * @range: range being tracked
 + * Returns: false if range data has been invalidated, true otherwise
   *
   * Range struct is used to track updates to the CPU page table after a call to
 - * hmm_range_register(). See include/linux/hmm.h for how to use it.
 - */
 -void hmm_range_unregister(struct hmm_range *range)
 -{
 -	/* Sanity check this really should not happen. */
 -	if (range->hmm == NULL || range->end <= range->start)
 -		return;
 -
 -	mutex_lock(&range->hmm->lock);
 -	list_del_rcu(&range->list);
 -	mutex_unlock(&range->hmm->lock);
 -
 -	/* Drop reference taken by hmm_range_register() */
 -	range->valid = false;
 -	hmm_put(range->hmm);
 -	range->hmm = NULL;
 -}
 -EXPORT_SYMBOL(hmm_range_unregister);
 -
 -/*
 - * hmm_range_snapshot() - snapshot CPU page table for a range
 - * @range: range
 - * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 - *          permission (for instance asking for write and range is read only),
 - *          -EAGAIN if you need to retry, -EFAULT invalid (ie either no valid
 - *          vma or it is illegal to access that range), number of valid pages
 - *          in range->pfns[] (from range start address).
 + * either hmm_vma_get_pfns() or hmm_vma_fault(). Once the device driver is done
 + * using the data,  or wants to lock updates to the data it got from those
 + * functions, it must call the hmm_vma_range_done() function, which will then
 + * stop tracking CPU page table updates.
   *
 - * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 - * validity is tracked by range struct. See in include/linux/hmm.h for example
 - * on how to use.
 + * Note that device driver must still implement general CPU page table update
 + * tracking either by using hmm_mirror (see hmm_mirror_register()) or by using
 + * the mmu_notifier API directly.
 + *
 + * CPU page table update tracking done through hmm_range is only temporary and
 + * to be used while trying to duplicate CPU page table contents for a range of
 + * virtual addresses.
 + *
 + * There are two ways to use this :
 + * again:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   trans = device_build_page_table_update_transaction(pfns);
 + *   device_page_table_lock();
 + *   if (!hmm_vma_range_done(range)) {
 + *     device_page_table_unlock();
 + *     goto again;
 + *   }
 + *   device_commit_transaction(trans);
 + *   device_page_table_unlock();
 + *
 + * Or:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   device_page_table_lock();
 + *   hmm_vma_range_done(range);
 + *   device_update_page_table(range->pfns);
 + *   device_page_table_unlock();
   */
 -long hmm_range_snapshot(struct hmm_range *range)
 +bool hmm_vma_range_done(struct hmm_range *range)
  {
++<<<<<<< HEAD
 +	unsigned long npages = (range->end - range->start) >> PAGE_SHIFT;
 +	struct hmm *hmm;
++=======
+ 	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
+ 	unsigned long start = range->start, end;
+ 	struct hmm_vma_walk hmm_vma_walk;
+ 	struct hmm *hmm = range->hmm;
+ 	struct vm_area_struct *vma;
+ 	struct mm_walk mm_walk;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	if (range->end <= range->start) {
 +		BUG();
 +		return false;
 +	}
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid)
 -			return -EAGAIN;
 +	hmm = hmm_register(range->vma->vm_mm);
 +	if (!hmm) {
 +		memset(range->pfns, 0, sizeof(*range->pfns) * npages);
 +		return false;
 +	}
  
++<<<<<<< HEAD
 +	spin_lock(&hmm->lock);
 +	list_del_rcu(&range->list);
 +	spin_unlock(&hmm->lock);
 +
 +	return range->valid;
++=======
+ 		vma = find_vma(hmm->mm, start);
+ 		if (vma == NULL || (vma->vm_flags & device_vma))
+ 			return -EFAULT;
+ 
+ 		/* FIXME support dax */
+ 		if (vma_is_dax(vma)) {
+ 			hmm_pfns_special(range);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (is_vm_hugetlb_page(vma)) {
+ 			struct hstate *h = hstate_vma(vma);
+ 
+ 			if (huge_page_shift(h) != range->page_shift &&
+ 			    range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		} else {
+ 			if (range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = false;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
+ 
+ 		walk_page_range(start, end, &mm_walk);
+ 		start = end;
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  }
 -EXPORT_SYMBOL(hmm_range_snapshot);
 +EXPORT_SYMBOL(hmm_vma_range_done);
  
  /*
 - * hmm_range_fault() - try to fault some address in a virtual address range
 + * hmm_vma_fault() - try to fault some address in a virtual address range
   * @range: range being faulted
   * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
++<<<<<<< HEAD
 + * Returns: 0 success, error otherwise (-EAGAIN means mmap_sem have been drop)
++=======
+  * Returns: number of valid pages in range->pfns[] (from range start
+  *          address). This may be zero. If the return value is negative,
+  *          then one of the following values may be returned:
+  *
+  *           -EINVAL  invalid arguments or mm or virtual address are in an
+  *                    invalid vma (for instance device file vma).
+  *           -ENOMEM: Out of memory.
+  *           -EPERM:  Invalid permission (for instance asking for write and
+  *                    range is read only).
+  *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
+  *                    happens if block argument is false.
+  *           -EBUSY:  If the the range is being invalidated and you should wait
+  *                    for invalidation to finish.
+  *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
+  *                    that range), number of valid pages in range->pfns[] (from
+  *                    range start address).
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
   *
   * This is similar to a regular CPU page fault except that it will not trigger
 - * any memory migration if the memory being faulted is not accessible by CPUs
 - * and caller does not ask for migration.
 + * any memory migration if the memory being faulted is not accessible by CPUs.
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
 + *
 + * Expected use pattern:
 + * retry:
 + *   down_read(&mm->mmap_sem);
 + *   // Find vma and address device wants to fault, initialize hmm_pfn_t
 + *   // array accordingly
 + *   ret = hmm_vma_fault(range, write, block);
 + *   switch (ret) {
 + *   case -EAGAIN:
 + *     hmm_vma_range_done(range);
 + *     // You might want to rate limit or yield to play nicely, you may
 + *     // also commit any valid pfn in the array assuming that you are
 + *     // getting true from hmm_vma_range_monitor_end()
 + *     goto retry;
 + *   case 0:
 + *     break;
 + *   case -ENOMEM:
 + *   case -EINVAL:
 + *   case -EPERM:
 + *   default:
 + *     // Handle error !
 + *     up_read(&mm->mmap_sem)
 + *     return;
 + *   }
 + *   // Take device driver lock that serialize device page table update
 + *   driver_lock_device_page_table_update();
 + *   hmm_vma_range_done(range);
 + *   // Commit pfns we got from hmm_vma_fault()
 + *   driver_unlock_device_page_table_update();
 + *   up_read(&mm->mmap_sem)
 + *
 + * YOU MUST CALL hmm_vma_range_done() AFTER THIS FUNCTION RETURN SUCCESS (0)
 + * BEFORE FREEING THE range struct OR YOU WILL HAVE SERIOUS MEMORY CORRUPTION !
 + *
 + * YOU HAVE BEEN WARNED !
   */
 -long hmm_range_fault(struct hmm_range *range, bool block)
 +int hmm_vma_fault(struct hmm_range *range, bool block)
  {
++<<<<<<< HEAD
 +	struct vm_area_struct *vma = range->vma;
 +	unsigned long start = range->start;
++=======
+ 	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
+ 	unsigned long start = range->start, end;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  	struct hmm_vma_walk hmm_vma_walk;
 -	struct hmm *hmm = range->hmm;
 -	struct vm_area_struct *vma;
  	struct mm_walk mm_walk;
 +	struct hmm *hmm;
  	int ret;
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid) {
 -			up_read(&hmm->mm->mmap_sem);
 -			return -EAGAIN;
 -		}
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm) {
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -ENOMEM;
 +	}
 +	/* Caller must have registered a mirror using hmm_mirror_register() */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
  
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
 +
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
 +	}
 +
 +	/* Initialize range to track CPU page table update */
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = true;
 +	hmm_vma_walk.block = block;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +	hmm_vma_walk.last = range->start;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	do {
 +		ret = walk_page_range(start, range->end, &mm_walk);
 +		start = hmm_vma_walk.last;
 +	} while (ret == -EAGAIN);
 +
++<<<<<<< HEAD
 +	if (ret) {
 +		unsigned long i;
 +
 +		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		hmm_pfns_clear(range, &range->pfns[i], hmm_vma_walk.last,
 +			       range->end);
 +		hmm_vma_range_done(range);
 +	}
 +	return ret;
++=======
+ 		vma = find_vma(hmm->mm, start);
+ 		if (vma == NULL || (vma->vm_flags & device_vma))
+ 			return -EFAULT;
+ 
+ 		/* FIXME support dax */
+ 		if (vma_is_dax(vma)) {
+ 			hmm_pfns_special(range);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (is_vm_hugetlb_page(vma)) {
+ 			if (huge_page_shift(hstate_vma(vma)) !=
+ 			    range->page_shift &&
+ 			    range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		} else {
+ 			if (range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = true;
+ 		hmm_vma_walk.block = block;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
+ 
+ 		do {
+ 			ret = walk_page_range(start, end, &mm_walk);
+ 			start = hmm_vma_walk.last;
+ 
+ 			/* Keep trying while the range is valid. */
+ 		} while (ret == -EBUSY && range->valid);
+ 
+ 		if (ret) {
+ 			unsigned long i;
+ 
+ 			i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
+ 			hmm_pfns_clear(range, &range->pfns[i],
+ 				hmm_vma_walk.last, range->end);
+ 			return ret;
+ 		}
+ 		start = end;
+ 
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> 63d5066f6e5a (mm/hmm: mirror hugetlbfs (snapshoting, faulting and DMA mapping))
  }
 -EXPORT_SYMBOL(hmm_range_fault);
 +EXPORT_SYMBOL(hmm_vma_fault);
  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
  
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
