net: core: support XDP generic on stacked devices.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] core: support XDP generic on stacked devices. (Jiri Benc) [1749814]
Rebuild_FUZZ: 94.74%
commit-author Stephen Hemminger <stephen@networkplumber.org>
commit 458bf2f224f04a513b0be972f8708e78ee2c986e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/458bf2f2.failed

When a device is stacked like (team, bonding, failsafe or netvsc) the
XDP generic program for the parent device was not called.

Move the call to XDP generic inside __netif_receive_skb_core where
it can be done multiple times for stacked case.

Fixes: d445516966dc ("net: xdp: support xdp generic on virtual devices")
	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 458bf2f224f04a513b0be972f8708e78ee2c986e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/dev.c
diff --cc net/core/dev.c
index db5e30e658df,cc2a4e257324..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -4476,25 -4502,8 +4476,8 @@@ static int netif_rx_internal(struct sk_
  
  	trace_netif_rx(skb);
  
- 	if (static_branch_unlikely(&generic_xdp_needed_key)) {
- 		int ret;
- 
- 		preempt_disable();
- 		rcu_read_lock();
- 		ret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);
- 		rcu_read_unlock();
- 		preempt_enable();
- 
- 		/* Consider XDP consuming the packet a success from
- 		 * the netdev point of view we do not want to count
- 		 * this as an error.
- 		 */
- 		if (ret != XDP_PASS)
- 			return NET_RX_SUCCESS;
- 	}
- 
  #ifdef CONFIG_RPS
 -	if (static_branch_unlikely(&rps_needed)) {
 +	if (static_key_false(&rps_needed)) {
  		struct rps_dev_flow voidflow, *rflow = &voidflow;
  		int cpu;
  
@@@ -5041,22 -5173,9 +5036,9 @@@ static int netif_receive_skb_internal(s
  	if (skb_defer_rx_timestamp(skb))
  		return NET_RX_SUCCESS;
  
- 	if (static_branch_unlikely(&generic_xdp_needed_key)) {
- 		int ret;
- 
- 		preempt_disable();
- 		rcu_read_lock();
- 		ret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);
- 		rcu_read_unlock();
- 		preempt_enable();
- 
- 		if (ret != XDP_PASS)
- 			return NET_RX_DROP;
- 	}
- 
  	rcu_read_lock();
  #ifdef CONFIG_RPS
 -	if (static_branch_unlikely(&rps_needed)) {
 +	if (static_key_false(&rps_needed)) {
  		struct rps_dev_flow voidflow, *rflow = &voidflow;
  		int cpu = get_rps_cpu(skb->dev, skb, &rflow);
  
@@@ -5072,6 -5191,39 +5054,42 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void netif_receive_skb_list_internal(struct list_head *head)
+ {
+ 	struct sk_buff *skb, *next;
+ 	struct list_head sublist;
+ 
+ 	INIT_LIST_HEAD(&sublist);
+ 	list_for_each_entry_safe(skb, next, head, list) {
+ 		net_timestamp_check(netdev_tstamp_prequeue, skb);
+ 		skb_list_del_init(skb);
+ 		if (!skb_defer_rx_timestamp(skb))
+ 			list_add_tail(&skb->list, &sublist);
+ 	}
+ 	list_splice_init(&sublist, head);
+ 
+ 	rcu_read_lock();
+ #ifdef CONFIG_RPS
+ 	if (static_branch_unlikely(&rps_needed)) {
+ 		list_for_each_entry_safe(skb, next, head, list) {
+ 			struct rps_dev_flow voidflow, *rflow = &voidflow;
+ 			int cpu = get_rps_cpu(skb->dev, skb, &rflow);
+ 
+ 			if (cpu >= 0) {
+ 				/* Will be handled, remove from list */
+ 				skb_list_del_init(skb);
+ 				enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
+ 			}
+ 		}
+ 	}
+ #endif
+ 	__netif_receive_skb_list(head);
+ 	rcu_read_unlock();
+ }
+ 
++>>>>>>> 458bf2f224f0 (net: core: support XDP generic on stacked devices.)
  /**
   *	netif_receive_skb - process receive buffer from network
   *	@skb: buffer to process
* Unmerged path net/core/dev.c
