KVM: Boost vCPUs that are delivering interrupts

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Wanpeng Li <wanpengli@tencent.com>
commit d73eb57b80b98ae147e4e6a7d9877c2ba175f972
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/d73eb57b.failed

Inspired by commit 9cac38dd5d (KVM/s390: Set preempted flag during
vcpu wakeup and interrupt delivery), we want to also boost not just
lock holders but also vCPUs that are delivering interrupts. Most
smp_call_function_many calls are synchronous, so the IPI target vCPUs
are also good yield candidates.  This patch introduces vcpu->ready to
boost vCPUs during wakeup and interrupt delivery time; unlike s390 we do
not reuse vcpu->preempted so that voluntarily preempted vCPUs are taken
into account by kvm_vcpu_on_spin, but vmx_vcpu_pi_put is not affected
(VT-d PI handles voluntary preemption separately, in pi_pre_block).

Testing on 80 HT 2 socket Xeon Skylake server, with 80 vCPUs VM 80GB RAM:
ebizzy -M

            vanilla     boosting    improved
1VM          21443       23520         9%
2VM           2800        8000       180%
3VM           1800        3100        72%

Testing on my Haswell desktop 8 HT, with 8 vCPUs VM 8GB RAM, two VMs,
one running ebizzy -M, the other running 'stress --cpu 2':

w/ boosting + w/o pv sched yield(vanilla)

            vanilla     boosting   improved
              1570         4000      155%

w/ boosting + w/ pv sched yield(vanilla)

            vanilla     boosting   improved
              1844         5157      179%

w/o boosting, perf top in VM:

 72.33%  [kernel]       [k] smp_call_function_many
  4.22%  [kernel]       [k] call_function_i
  3.71%  [kernel]       [k] async_page_fault

w/ boosting, perf top in VM:

 38.43%  [kernel]       [k] smp_call_function_many
  6.31%  [kernel]       [k] async_page_fault
  6.13%  libc-2.23.so   [.] __memcpy_avx_unaligned
  4.88%  [kernel]       [k] call_function_interrupt

	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Paul Mackerras <paulus@ozlabs.org>
	Cc: Marc Zyngier <maz@kernel.org>
	Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d73eb57b80b98ae147e4e6a7d9877c2ba175f972)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kvm/interrupt.c
#	virt/kvm/kvm_main.c
diff --cc arch/s390/kvm/interrupt.c
index 93796dc89f55,26f8bf4a22a7..000000000000
--- a/arch/s390/kvm/interrupt.c
+++ b/arch/s390/kvm/interrupt.c
@@@ -1240,8 -1240,8 +1240,13 @@@ void kvm_s390_vcpu_wakeup(struct kvm_vc
  		 * The vcpu gave up the cpu voluntarily, mark it as a good
  		 * yield-candidate.
  		 */
++<<<<<<< HEAD
 +		vcpu->preempted = true;
 +		swake_up(&vcpu->wq);
++=======
+ 		vcpu->ready = true;
+ 		swake_up_one(&vcpu->wq);
++>>>>>>> d73eb57b80b9 (KVM: Boost vCPUs that are delivering interrupts)
  		vcpu->stat.halt_wakeup++;
  	}
  	/*
diff --cc virt/kvm/kvm_main.c
index 76b5f2660625,887f3b0c2b60..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -2388,7 -2387,8 +2389,12 @@@ bool kvm_vcpu_wake_up(struct kvm_vcpu *
  
  	wqp = kvm_arch_vcpu_wq(vcpu);
  	if (swq_has_sleeper(wqp)) {
++<<<<<<< HEAD
 +		swake_up(wqp);
++=======
+ 		swake_up_one(wqp);
+ 		WRITE_ONCE(vcpu->ready, true);
++>>>>>>> d73eb57b80b9 (KVM: Boost vCPUs that are delivering interrupts)
  		++vcpu->stat.halt_wakeup;
  		return true;
  	}
* Unmerged path arch/s390/kvm/interrupt.c
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8009ba9d1138..cc93aaa9c0d7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -321,6 +321,7 @@ struct kvm_vcpu {
 	} spin_loop;
 #endif
 	bool preempted;
+	bool ready;
 	struct kvm_vcpu_arch arch;
 	struct dentry *debugfs_dentry;
 };
* Unmerged path virt/kvm/kvm_main.c
