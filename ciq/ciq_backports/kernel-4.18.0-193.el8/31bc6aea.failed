sched/fair: Optimize update_blocked_averages()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit 31bc6aeaab1d1de8959b67edbed5c7a4b3cdbe7c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/31bc6aea.failed

Removing a cfs_rq from rq->leaf_cfs_rq_list can break the parent/child
ordering of the list when it will be added back. In order to remove an
empty and fully decayed cfs_rq, we must remove its children too, so they
will be added back in the right order next time.

With a normal decay of PELT, a parent will be empty and fully decayed
if all children are empty and fully decayed too. In such a case, we just
have to ensure that the whole branch will be added when a new task is
enqueued. This is default behavior since :

  commit f6783319737f ("sched/fair: Fix insertion in rq->leaf_cfs_rq_list")

In case of throttling, the PELT of throttled cfs_rq will not be updated
whereas the parent will. This breaks the assumption made above unless we
remove the children of a cfs_rq that is throttled. Then, they will be
added back when unthrottled and a sched_entity will be enqueued.

As throttled cfs_rq are now removed from the list, we can remove the
associated test in update_blocked_averages().

	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: sargun@sargun.me
	Cc: tj@kernel.org
	Cc: xiexiuqi@huawei.com
	Cc: xiezhipeng1@huawei.com
Link: https://lkml.kernel.org/r/1549469662-13614-2-git-send-email-vincent.guittot@linaro.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 31bc6aeaab1d1de8959b67edbed5c7a4b3cdbe7c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 5c4ac0d448ba,027f8e1b5b66..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -7704,14 -7717,10 +7724,18 @@@ static void update_blocked_averages(in
  	 * Iterates the task_group tree in a bottom up fashion, see
  	 * list_add_leaf_cfs_rq() for details.
  	 */
 -	for_each_leaf_cfs_rq(rq, cfs_rq) {
 +	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {
  		struct sched_entity *se;
  
++<<<<<<< HEAD
 +		/* throttled entities do not contribute to load */
 +		if (throttled_hierarchy(cfs_rq))
 +			continue;
 +
 +		if (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))
++=======
+ 		if (update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq))
++>>>>>>> 31bc6aeaab1d (sched/fair: Optimize update_blocked_averages())
  			update_tg_load_avg(cfs_rq, 0);
  
  		/* Propagate pending load changes to the parent, if any: */
* Unmerged path kernel/sched/fair.c
