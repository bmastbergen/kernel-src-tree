RDMA: Convert CQ allocations to be under core responsibility

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit e39afe3d6dbd908d8fd189571a3c1561088a86c2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/e39afe3d.failed

Ensure that CQ is allocated and freed by IB/core and not by drivers.

	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Acked-by: Gal Pressman <galpress@amazon.com>
	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Tested-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit e39afe3d6dbd908d8fd189571a3c1561088a86c2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/cq.c
#	drivers/infiniband/core/device.c
#	drivers/infiniband/core/uverbs_cmd.c
#	drivers/infiniband/core/uverbs_std_types_cq.c
#	drivers/infiniband/core/verbs.c
#	drivers/infiniband/hw/bnxt_re/ib_verbs.c
#	drivers/infiniband/hw/bnxt_re/ib_verbs.h
#	drivers/infiniband/hw/bnxt_re/main.c
#	drivers/infiniband/hw/cxgb3/iwch_provider.c
#	drivers/infiniband/hw/cxgb4/cq.c
#	drivers/infiniband/hw/cxgb4/iw_cxgb4.h
#	drivers/infiniband/hw/cxgb4/provider.c
#	drivers/infiniband/hw/efa/efa.h
#	drivers/infiniband/hw/efa/efa_verbs.c
#	drivers/infiniband/hw/hns/hns_roce_cq.c
#	drivers/infiniband/hw/hns/hns_roce_device.h
#	drivers/infiniband/hw/hns/hns_roce_hw_v1.c
#	drivers/infiniband/hw/hns/hns_roce_main.c
#	drivers/infiniband/hw/i40iw/i40iw_verbs.c
#	drivers/infiniband/hw/mlx4/cq.c
#	drivers/infiniband/hw/mlx4/main.c
#	drivers/infiniband/hw/mlx4/mlx4_ib.h
#	drivers/infiniband/hw/mlx5/cq.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mthca/mthca_provider.c
#	drivers/infiniband/hw/nes/nes_verbs.c
#	drivers/infiniband/hw/ocrdma/ocrdma_main.c
#	drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
#	drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
#	drivers/infiniband/hw/qedr/main.c
#	drivers/infiniband/hw/qedr/verbs.c
#	drivers/infiniband/hw/qedr/verbs.h
#	drivers/infiniband/hw/usnic/usnic_ib_main.c
#	drivers/infiniband/hw/usnic/usnic_ib_verbs.c
#	drivers/infiniband/hw/usnic/usnic_ib_verbs.h
#	drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c
#	drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
#	drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
#	drivers/infiniband/sw/rdmavt/cq.c
#	drivers/infiniband/sw/rdmavt/cq.h
#	drivers/infiniband/sw/rdmavt/vt.c
#	drivers/infiniband/sw/rxe/rxe_verbs.c
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/cq.c
index d0ebdb43d295,3b9412c69565..000000000000
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@@ -145,13 -147,11 +145,17 @@@ struct ib_cq *__ib_alloc_cq(struct ib_d
  	struct ib_cq *cq;
  	int ret = -ENOMEM;
  
++<<<<<<< HEAD
 +	cq = dev->ops.create_cq(dev, &cq_attr, NULL, NULL);
 +	if (IS_ERR(cq))
 +		return cq;
++=======
+ 	cq = rdma_zalloc_drv_obj(dev, ib_cq);
+ 	if (!cq)
+ 		return ERR_PTR(ret);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  
  	cq->device = dev;
- 	cq->uobject = NULL;
- 	cq->event_handler = NULL;
  	cq->cq_context = private;
  	cq->poll_ctx = poll_ctx;
  	atomic_set(&cq->usecnt, 0);
@@@ -189,14 -194,16 +198,22 @@@
  
  	return cq;
  
+ out_destroy_cq:
+ 	rdma_restrack_del(&cq->res);
+ 	cq->device->ops.destroy_cq(cq, udata);
  out_free_wc:
  	kfree(cq->wc);
++<<<<<<< HEAD
 +	rdma_restrack_del(&cq->res);
 +out_destroy_cq:
 +	cq->device->ops.destroy_cq(cq);
++=======
+ out_free_cq:
+ 	kfree(cq);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	return ERR_PTR(ret);
  }
 -EXPORT_SYMBOL(__ib_alloc_cq_user);
 +EXPORT_SYMBOL(__ib_alloc_cq);
  
  /**
   * ib_free_cq_user - free a completion queue
@@@ -223,9 -229,9 +240,14 @@@ void ib_free_cq(struct ib_cq *cq
  		WARN_ON_ONCE(1);
  	}
  
- 	kfree(cq->wc);
  	rdma_restrack_del(&cq->res);
++<<<<<<< HEAD
 +	ret = cq->device->ops.destroy_cq(cq);
 +	WARN_ON_ONCE(ret);
++=======
+ 	cq->device->ops.destroy_cq(cq, udata);
+ 	kfree(cq->wc);
+ 	kfree(cq);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
 -EXPORT_SYMBOL(ib_free_cq_user);
 +EXPORT_SYMBOL(ib_free_cq);
diff --cc drivers/infiniband/core/device.c
index ec96a7b1c811,abb169f31d0f..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -1357,6 -2429,12 +1357,15 @@@ void ib_set_device_ops(struct ib_devic
  	SET_DEVICE_OP(dev_ops, set_vf_guid);
  	SET_DEVICE_OP(dev_ops, set_vf_link_state);
  	SET_DEVICE_OP(dev_ops, unmap_fmr);
++<<<<<<< HEAD
++=======
+ 
+ 	SET_OBJ_SIZE(dev_ops, ib_ah);
+ 	SET_OBJ_SIZE(dev_ops, ib_cq);
+ 	SET_OBJ_SIZE(dev_ops, ib_pd);
+ 	SET_OBJ_SIZE(dev_ops, ib_srq);
+ 	SET_OBJ_SIZE(dev_ops, ib_ucontext);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  EXPORT_SYMBOL(ib_set_device_ops);
  
diff --cc drivers/infiniband/core/uverbs_cmd.c
index a2f5dc1a0474,5c00d9a5698a..000000000000
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@@ -999,13 -1010,11 +999,18 @@@ static struct ib_ucq_object *create_cq(
  	attr.comp_vector = cmd->comp_vector;
  	attr.flags = cmd->flags;
  
++<<<<<<< HEAD
 +	cq = ib_dev->ops.create_cq(ib_dev, &attr, obj->uobject.context,
 +				   &attrs->driver_udata);
 +	if (IS_ERR(cq)) {
 +		ret = PTR_ERR(cq);
++=======
+ 	cq = rdma_zalloc_drv_obj(ib_dev, ib_cq);
+ 	if (!cq) {
+ 		ret = -ENOMEM;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		goto err_file;
  	}
- 
  	cq->device        = ib_dev;
  	cq->uobject       = &obj->uobject;
  	cq->comp_handler  = ib_uverbs_comp_handler;
@@@ -1032,8 -1045,10 +1041,15 @@@
  	return obj;
  
  err_cb:
++<<<<<<< HEAD
 +	ib_destroy_cq_user(cq, uverbs_get_cleared_udata(attrs));
 +
++=======
+ 	ib_destroy_cq(cq);
+ 	cq = NULL;
+ err_free:
+ 	kfree(cq);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  err_file:
  	if (ev_file)
  		ib_uverbs_release_ucq(attrs->ufile, ev_file, obj);
diff --cc drivers/infiniband/core/uverbs_std_types_cq.c
index 54bfc31bf2b0,06b8c7d017b7..000000000000
--- a/drivers/infiniband/core/uverbs_std_types_cq.c
+++ b/drivers/infiniband/core/uverbs_std_types_cq.c
@@@ -110,10 -111,9 +110,16 @@@ static int UVERBS_HANDLER(UVERBS_METHOD
  	INIT_LIST_HEAD(&obj->comp_list);
  	INIT_LIST_HEAD(&obj->async_list);
  
++<<<<<<< HEAD
 +	cq = ib_dev->ops.create_cq(ib_dev, &attr, obj->uobject.context,
 +				   &attrs->driver_udata);
 +	if (IS_ERR(cq)) {
 +		ret = PTR_ERR(cq);
++=======
+ 	cq = rdma_zalloc_drv_obj(ib_dev, ib_cq);
+ 	if (!cq) {
+ 		ret = -ENOMEM;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		goto err_event_file;
  	}
  
@@@ -135,8 -140,10 +146,15 @@@
  
  	return 0;
  err_cq:
++<<<<<<< HEAD
 +	ib_destroy_cq_user(cq, uverbs_get_cleared_udata(attrs));
 +
++=======
+ 	ib_destroy_cq(cq);
+ 	cq = NULL;
+ err_free:
+ 	kfree(cq);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  err_event_file:
  	if (ev_file)
  		uverbs_uobject_put(ev_file_uobj);
diff --cc drivers/infiniband/core/verbs.c
index 8ef3c69dc3da,585e100706aa..000000000000
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@@ -1905,21 -1916,28 +1905,32 @@@ struct ib_cq *__ib_create_cq(struct ib_
  			     const char *caller)
  {
  	struct ib_cq *cq;
+ 	int ret;
  
++<<<<<<< HEAD
 +	cq = device->ops.create_cq(device, cq_attr, NULL, NULL);
- 
- 	if (!IS_ERR(cq)) {
- 		cq->device        = device;
- 		cq->uobject       = NULL;
- 		cq->comp_handler  = comp_handler;
- 		cq->event_handler = event_handler;
- 		cq->cq_context    = cq_context;
- 		atomic_set(&cq->usecnt, 0);
- 		cq->res.type = RDMA_RESTRACK_CQ;
- 		rdma_restrack_set_task(&cq->res, caller);
- 		rdma_restrack_kadd(&cq->res);
++=======
+ 	cq = rdma_zalloc_drv_obj(device, ib_cq);
+ 	if (!cq)
+ 		return ERR_PTR(-ENOMEM);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
+ 
+ 	cq->device = device;
+ 	cq->uobject = NULL;
+ 	cq->comp_handler = comp_handler;
+ 	cq->event_handler = event_handler;
+ 	cq->cq_context = cq_context;
+ 	atomic_set(&cq->usecnt, 0);
+ 	cq->res.type = RDMA_RESTRACK_CQ;
+ 	rdma_restrack_set_task(&cq->res, caller);
+ 
+ 	ret = device->ops.create_cq(cq, cq_attr, NULL);
+ 	if (ret) {
+ 		kfree(cq);
+ 		return ERR_PTR(ret);
  	}
  
+ 	rdma_restrack_kadd(&cq->res);
  	return cq;
  }
  EXPORT_SYMBOL(__ib_create_cq);
@@@ -1938,9 -1956,11 +1949,15 @@@ int ib_destroy_cq(struct ib_cq *cq
  		return -EBUSY;
  
  	rdma_restrack_del(&cq->res);
++<<<<<<< HEAD
 +	return cq->device->ops.destroy_cq(cq);
++=======
+ 	cq->device->ops.destroy_cq(cq, udata);
+ 	kfree(cq);
+ 	return 0;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
 -EXPORT_SYMBOL(ib_destroy_cq_user);
 +EXPORT_SYMBOL(ib_destroy_cq);
  
  int ib_resize_cq(struct ib_cq *cq, int cqe)
  {
diff --cc drivers/infiniband/hw/bnxt_re/ib_verbs.c
index 9361cce3c120,44cc5f19df3b..000000000000
--- a/drivers/infiniband/hw/bnxt_re/ib_verbs.c
+++ b/drivers/infiniband/hw/bnxt_re/ib_verbs.c
@@@ -2579,19 -2534,14 +2579,26 @@@ int bnxt_re_destroy_cq(struct ib_cq *ib
  	atomic_dec(&rdev->cq_count);
  	nq->budget--;
  	kfree(cq->cql);
++<<<<<<< HEAD
 +	kfree(cq);
 +
 +	return 0;
 +}
 +
 +struct ib_cq *bnxt_re_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata)
++=======
+ }
+ 
+ int bnxt_re_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
- 	struct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);
+ 	struct bnxt_re_dev *rdev = to_bnxt_re_dev(ibcq->device, ibdev);
  	struct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;
- 	struct bnxt_re_cq *cq = NULL;
+ 	struct bnxt_re_cq *cq = container_of(ibcq, struct bnxt_re_cq, ib_cq);
  	int rc, entries;
  	int cqe = attr->cqe;
  	struct bnxt_qplib_nq *nq = NULL;
@@@ -2685,10 -2629,10 +2689,10 @@@
  		}
  	}
  
- 	return &cq->ib_cq;
+ 	return 0;
  
  c2fail:
 -	if (udata)
 +	if (context)
  		ib_umem_release(cq->umem);
  fail:
  	kfree(cq->cql);
diff --cc drivers/infiniband/hw/bnxt_re/ib_verbs.h
index fb4998da40ee,31662b1ee35a..000000000000
--- a/drivers/infiniband/hw/bnxt_re/ib_verbs.h
+++ b/drivers/infiniband/hw/bnxt_re/ib_verbs.h
@@@ -194,11 -190,9 +194,17 @@@ int bnxt_re_post_send(struct ib_qp *qp
  		      const struct ib_send_wr **bad_send_wr);
  int bnxt_re_post_recv(struct ib_qp *qp, const struct ib_recv_wr *recv_wr,
  		      const struct ib_recv_wr **bad_recv_wr);
++<<<<<<< HEAD
 +struct ib_cq *bnxt_re_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata);
 +int bnxt_re_destroy_cq(struct ib_cq *cq);
++=======
+ int bnxt_re_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata);
+ void bnxt_re_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int bnxt_re_poll_cq(struct ib_cq *cq, int num_entries, struct ib_wc *wc);
  int bnxt_re_req_notify_cq(struct ib_cq *cq, enum ib_cq_notify_flags flags);
  struct ib_mr *bnxt_re_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
diff --cc drivers/infiniband/hw/bnxt_re/main.c
index 1f64cd1c51a2,029babe713f3..000000000000
--- a/drivers/infiniband/hw/bnxt_re/main.c
+++ b/drivers/infiniband/hw/bnxt_re/main.c
@@@ -636,6 -640,11 +636,14 @@@ static const struct ib_device_ops bnxt_
  	.query_srq = bnxt_re_query_srq,
  	.reg_user_mr = bnxt_re_reg_user_mr,
  	.req_notify_cq = bnxt_re_req_notify_cq,
++<<<<<<< HEAD
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, bnxt_re_ah, ib_ah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, bnxt_re_cq, ib_cq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, bnxt_re_pd, ib_pd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, bnxt_re_srq, ib_srq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, bnxt_re_ucontext, ib_uctx),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static int bnxt_re_register_ib(struct bnxt_re_dev *rdev)
diff --cc drivers/infiniband/hw/cxgb3/iwch_provider.c
index ba5689180499,acba96f289cc..000000000000
--- a/drivers/infiniband/hw/cxgb3/iwch_provider.c
+++ b/drivers/infiniband/hw/cxgb3/iwch_provider.c
@@@ -103,40 -100,30 +103,50 @@@ static int iwch_destroy_cq(struct ib_c
  	wait_event(chp->wait, !atomic_read(&chp->refcnt));
  
  	cxio_destroy_cq(&chp->rhp->rdev, &chp->cq);
++<<<<<<< HEAD
 +	kfree(chp);
 +	return 0;
 +}
 +
 +static struct ib_cq *iwch_create_cq(struct ib_device *ibdev,
 +				    const struct ib_cq_init_attr *attr,
 +				    struct ib_ucontext *ib_context,
 +				    struct ib_udata *udata)
++=======
+ }
+ 
+ static int iwch_create_cq(struct ib_cq *ibcq,
+ 			  const struct ib_cq_init_attr *attr,
+ 			  struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
- 	struct iwch_dev *rhp;
- 	struct iwch_cq *chp;
+ 	struct iwch_dev *rhp = to_iwch_dev(ibcq->device);
+ 	struct iwch_cq *chp = to_iwch_cq(ibcq);
  	struct iwch_create_cq_resp uresp;
  	struct iwch_create_cq_req ureq;
 +	struct iwch_ucontext *ucontext = NULL;
  	static int warned;
  	size_t resplen;
  
  	pr_debug("%s ib_dev %p entries %d\n", __func__, ibdev, entries);
  	if (attr->flags)
- 		return ERR_PTR(-EINVAL);
- 
- 	rhp = to_iwch_dev(ibdev);
- 	chp = kzalloc(sizeof(*chp), GFP_KERNEL);
- 	if (!chp)
- 		return ERR_PTR(-ENOMEM);
+ 		return -EINVAL;
  
 -	if (udata) {
 +	if (ib_context) {
 +		ucontext = to_iwch_ucontext(ib_context);
  		if (!t3a_device(rhp)) {
++<<<<<<< HEAD
 +			if (ib_copy_from_udata(&ureq, udata, sizeof (ureq))) {
 +				kfree(chp);
 +				return ERR_PTR(-EFAULT);
 +			}
++=======
+ 			if (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))
+ 				return  -EFAULT;
+ 
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  			chp->user_rptr_addr = (u32 __user *)(unsigned long)ureq.user_rptr_addr;
  		}
  	}
@@@ -157,29 -144,29 +167,39 @@@
  	entries = roundup_pow_of_two(entries);
  	chp->cq.size_log2 = ilog2(entries);
  
++<<<<<<< HEAD
 +	if (cxio_create_cq(&rhp->rdev, &chp->cq, !ucontext)) {
 +		kfree(chp);
 +		return ERR_PTR(-ENOMEM);
 +	}
++=======
+ 	if (cxio_create_cq(&rhp->rdev, &chp->cq, !udata))
+ 		return -ENOMEM;
+ 
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	chp->rhp = rhp;
  	chp->ibcq.cqe = 1 << chp->cq.size_log2;
  	spin_lock_init(&chp->lock);
  	spin_lock_init(&chp->comp_handler_lock);
  	atomic_set(&chp->refcnt, 1);
  	init_waitqueue_head(&chp->wait);
 -	if (xa_store_irq(&rhp->cqs, chp->cq.cqid, chp, GFP_KERNEL)) {
 +	if (insert_handle(rhp, &rhp->cqidr, chp, chp->cq.cqid)) {
  		cxio_destroy_cq(&chp->rhp->rdev, &chp->cq);
- 		kfree(chp);
- 		return ERR_PTR(-ENOMEM);
+ 		return -ENOMEM;
  	}
  
 -	if (udata) {
 +	if (ucontext) {
  		struct iwch_mm_entry *mm;
 -		struct iwch_ucontext *ucontext = rdma_udata_to_drv_context(
 -			udata, struct iwch_ucontext, ibucontext);
  
 -		mm = kmalloc(sizeof(*mm), GFP_KERNEL);
 +		mm = kmalloc(sizeof *mm, GFP_KERNEL);
  		if (!mm) {
++<<<<<<< HEAD
 +			iwch_destroy_cq(&chp->ibcq);
 +			return ERR_PTR(-ENOMEM);
++=======
+ 			iwch_destroy_cq(&chp->ibcq, udata);
+ 			return -ENOMEM;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		}
  		uresp.cqid = chp->cq.cqid;
  		uresp.size_log2 = chp->cq.size_log2;
@@@ -204,84 -191,17 +224,89 @@@
  		}
  		if (ib_copy_to_udata(udata, &uresp, resplen)) {
  			kfree(mm);
++<<<<<<< HEAD
 +			iwch_destroy_cq(&chp->ibcq);
 +			return ERR_PTR(-EFAULT);
++=======
+ 			iwch_destroy_cq(&chp->ibcq, udata);
+ 			return -EFAULT;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		}
  		insert_mmap(ucontext, mm);
  	}
  	pr_debug("created cqid 0x%0x chp %p size 0x%0x, dma_addr %pad\n",
  		 chp->cq.cqid, chp, (1 << chp->cq.size_log2),
  		 &chp->cq.dma_addr);
- 	return &chp->ibcq;
+ 	return 0;
  }
  
 +static int iwch_resize_cq(struct ib_cq *cq, int cqe, struct ib_udata *udata)
 +{
 +#ifdef notyet
 +	struct iwch_cq *chp = to_iwch_cq(cq);
 +	struct t3_cq oldcq, newcq;
 +	int ret;
 +
 +	pr_debug("%s ib_cq %p cqe %d\n", __func__, cq, cqe);
 +
 +	/* We don't downsize... */
 +	if (cqe <= cq->cqe)
 +		return 0;
 +
 +	/* create new t3_cq with new size */
 +	cqe = roundup_pow_of_two(cqe+1);
 +	newcq.size_log2 = ilog2(cqe);
 +
 +	/* Dont allow resize to less than the current wce count */
 +	if (cqe < Q_COUNT(chp->cq.rptr, chp->cq.wptr)) {
 +		return -ENOMEM;
 +	}
 +
 +	/* Quiesce all QPs using this CQ */
 +	ret = iwch_quiesce_qps(chp);
 +	if (ret) {
 +		return ret;
 +	}
 +
 +	ret = cxio_create_cq(&chp->rhp->rdev, &newcq);
 +	if (ret) {
 +		return ret;
 +	}
 +
 +	/* copy CQEs */
 +	memcpy(newcq.queue, chp->cq.queue, (1 << chp->cq.size_log2) *
 +				        sizeof(struct t3_cqe));
 +
 +	/* old iwch_qp gets new t3_cq but keeps old cqid */
 +	oldcq = chp->cq;
 +	chp->cq = newcq;
 +	chp->cq.cqid = oldcq.cqid;
 +
 +	/* resize new t3_cq to update the HW context */
 +	ret = cxio_resize_cq(&chp->rhp->rdev, &chp->cq);
 +	if (ret) {
 +		chp->cq = oldcq;
 +		return ret;
 +	}
 +	chp->ibcq.cqe = (1<<chp->cq.size_log2) - 1;
 +
 +	/* destroy old t3_cq */
 +	oldcq.cqid = newcq.cqid;
 +	ret = cxio_destroy_cq(&chp->rhp->rdev, &oldcq);
 +	if (ret) {
 +		pr_err("%s - cxio_destroy_cq failed %d\n", __func__, ret);
 +	}
 +
 +	/* add user hooks here */
 +
 +	/* resume qps */
 +	ret = iwch_resume_qps(chp);
 +	return ret;
 +#else
 +	return -ENOSYS;
 +#endif
 +}
 +
  static int iwch_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags)
  {
  	struct iwch_dev *rhp;
@@@ -1345,7 -1268,9 +1370,13 @@@ static const struct ib_device_ops iwch_
  	.query_port = iwch_query_port,
  	.reg_user_mr = iwch_reg_user_mr,
  	.req_notify_cq = iwch_arm_cq,
++<<<<<<< HEAD
 +	.resize_cq = iwch_resize_cq,
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, iwch_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, iwch_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, iwch_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  int iwch_register_device(struct iwch_dev *dev)
diff --cc drivers/infiniband/hw/cxgb4/cq.c
index 724da8610e91,3cc4d3331a3f..000000000000
--- a/drivers/infiniband/hw/cxgb4/cq.c
+++ b/drivers/infiniband/hw/cxgb4/cq.c
@@@ -986,37 -986,32 +986,43 @@@ int c4iw_destroy_cq(struct ib_cq *ib_cq
  		   ucontext ? &ucontext->uctx : &chp->cq.rdev->uctx,
  		   chp->destroy_skb, chp->wr_waitp);
  	c4iw_put_wr_wait(chp->wr_waitp);
++<<<<<<< HEAD
 +	kfree(chp);
 +	return 0;
 +}
 +
 +struct ib_cq *c4iw_create_cq(struct ib_device *ibdev,
 +			     const struct ib_cq_init_attr *attr,
 +			     struct ib_ucontext *ib_context,
 +			     struct ib_udata *udata)
++=======
+ }
+ 
+ int c4iw_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		   struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	int vector = attr->comp_vector;
- 	struct c4iw_dev *rhp;
- 	struct c4iw_cq *chp;
+ 	struct c4iw_dev *rhp = to_c4iw_dev(ibcq->device);
+ 	struct c4iw_cq *chp = to_c4iw_cq(ibcq);
  	struct c4iw_create_cq ucmd;
  	struct c4iw_create_cq_resp uresp;
 +	struct c4iw_ucontext *ucontext = NULL;
  	int ret, wr_len;
  	size_t memsize, hwentries;
  	struct c4iw_mm_entry *mm, *mm2;
  
  	pr_debug("ib_dev %p entries %d\n", ibdev, entries);
  	if (attr->flags)
- 		return ERR_PTR(-EINVAL);
- 
- 	rhp = to_c4iw_dev(ibdev);
+ 		return -EINVAL;
  
  	if (vector >= rhp->rdev.lldi.nciq)
- 		return ERR_PTR(-EINVAL);
+ 		return -EINVAL;
  
 -	if (udata) {
 +	if (ib_context) {
 +		ucontext = to_c4iw_ucontext(ib_context);
  		if (udata->inlen < sizeof(ucmd))
  			ucontext->is_32b_cqe = 1;
  	}
diff --cc drivers/infiniband/hw/cxgb4/iw_cxgb4.h
index 7dffb68092fc,7d06b0f8d49a..000000000000
--- a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
+++ b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
@@@ -1047,12 -991,10 +1047,19 @@@ struct ib_mr *c4iw_reg_user_mr(struct i
  					   u64 length, u64 virt, int acc,
  					   struct ib_udata *udata);
  struct ib_mr *c4iw_get_dma_mr(struct ib_pd *pd, int acc);
++<<<<<<< HEAD
 +int c4iw_dereg_mr(struct ib_mr *ib_mr);
 +int c4iw_destroy_cq(struct ib_cq *ib_cq);
 +struct ib_cq *c4iw_create_cq(struct ib_device *ibdev,
 +			     const struct ib_cq_init_attr *attr,
 +			     struct ib_ucontext *ib_context,
 +			     struct ib_udata *udata);
++=======
+ int c4iw_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata);
+ void c4iw_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata);
+ int c4iw_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		   struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int c4iw_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
  int c4iw_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *attr,
  		    enum ib_srq_attr_mask srq_attr_mask,
diff --cc drivers/infiniband/hw/cxgb4/provider.c
index e03b14a450bc,5e59c5708729..000000000000
--- a/drivers/infiniband/hw/cxgb4/provider.c
+++ b/drivers/infiniband/hw/cxgb4/provider.c
@@@ -539,6 -536,10 +539,13 @@@ static const struct ib_device_ops c4iw_
  	.query_qp = c4iw_ib_query_qp,
  	.reg_user_mr = c4iw_reg_user_mr,
  	.req_notify_cq = c4iw_arm_cq,
++<<<<<<< HEAD
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, c4iw_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, c4iw_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, c4iw_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, c4iw_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static int set_netdevs(struct ib_device *ib_dev, struct c4iw_rdev *rdev)
diff --cc drivers/infiniband/hw/efa/efa.h
index 14a36546985b,119f8efec564..000000000000
--- a/drivers/infiniband/hw/efa/efa.h
+++ b/drivers/infiniband/hw/efa/efa.h
@@@ -134,10 -134,9 +134,16 @@@ int efa_destroy_qp(struct ib_qp *ibqp, 
  struct ib_qp *efa_create_qp(struct ib_pd *ibpd,
  			    struct ib_qp_init_attr *init_attr,
  			    struct ib_udata *udata);
++<<<<<<< HEAD
 +int efa_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);
 +struct ib_cq *efa_create_cq(struct ib_device *ibdev,
 +			    const struct ib_cq_init_attr *attr,
 +			    struct ib_udata *udata);
++=======
+ void efa_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);
+ int efa_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		  struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  struct ib_mr *efa_reg_mr(struct ib_pd *ibpd, u64 start, u64 length,
  			 u64 virt_addr, int access_flags,
  			 struct ib_udata *udata);
diff --cc drivers/infiniband/hw/efa/efa_verbs.c
index 2187063e7c29,a9372c9e4b30..000000000000
--- a/drivers/infiniband/hw/efa/efa_verbs.c
+++ b/drivers/infiniband/hw/efa/efa_verbs.c
@@@ -868,15 -856,9 +868,18 @@@ int efa_destroy_cq(struct ib_cq *ibcq, 
  		  "Destroy cq[%d] virt[0x%p] freed: size[%lu], dma[%pad]\n",
  		  cq->cq_idx, cq->cpu_addr, cq->size, &cq->dma_addr);
  
 -	efa_destroy_cq_idx(dev, cq->cq_idx);
 +	err = efa_destroy_cq_idx(dev, cq->cq_idx);
 +	if (err)
 +		return err;
 +
  	dma_unmap_single(&dev->pdev->dev, cq->dma_addr, cq->size,
  			 DMA_FROM_DEVICE);
++<<<<<<< HEAD
 +
 +	kfree(cq);
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static int cq_mmap_entries_setup(struct efa_dev *dev, struct efa_cq *cq,
diff --cc drivers/infiniband/hw/hns/hns_roce_cq.c
index 3a485f50fede,7e198c9ffbfe..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_cq.c
+++ b/drivers/infiniband/hw/hns/hns_roce_cq.c
@@@ -307,12 -299,11 +307,18 @@@ static void hns_roce_ib_free_cq_buf(str
  			  &buf->hr_buf);
  }
  
++<<<<<<< HEAD
 +struct ib_cq *hns_roce_ib_create_cq(struct ib_device *ib_dev,
 +				    const struct ib_cq_init_attr *attr,
 +				    struct ib_ucontext *context,
 +				    struct ib_udata *udata)
++=======
+ int hns_roce_ib_create_cq(struct ib_cq *ib_cq,
+ 			  const struct ib_cq_init_attr *attr,
+ 			  struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
- 	struct hns_roce_dev *hr_dev = to_hr_dev(ib_dev);
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ib_cq->device);
  	struct device *dev = hr_dev->dev;
  	struct hns_roce_ib_create_cq ucmd;
  	struct hns_roce_ib_create_cq_resp resp = {};
@@@ -455,33 -442,30 +456,36 @@@ int hns_roce_ib_destroy_cq(struct ib_c
  {
  	struct hns_roce_dev *hr_dev = to_hr_dev(ib_cq->device);
  	struct hns_roce_cq *hr_cq = to_hr_cq(ib_cq);
 +	int ret = 0;
  
  	if (hr_dev->hw->destroy_cq) {
 -		hr_dev->hw->destroy_cq(ib_cq, udata);
 -		return;
 -	}
 -
 -	hns_roce_free_cq(hr_dev, hr_cq);
 -	hns_roce_mtt_cleanup(hr_dev, &hr_cq->hr_buf.hr_mtt);
 -
 -	if (udata) {
 -		ib_umem_release(hr_cq->umem);
 -
 -		if (hr_cq->db_en == 1)
 -			hns_roce_db_unmap_user(rdma_udata_to_drv_context(
 -						       udata,
 -						       struct hns_roce_ucontext,
 -						       ibucontext),
 -					       &hr_cq->db);
 +		ret = hr_dev->hw->destroy_cq(ib_cq);
  	} else {
 -		/* Free the buff of stored cq */
 -		hns_roce_ib_free_cq_buf(hr_dev, &hr_cq->hr_buf, ib_cq->cqe);
 -		if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB)
 -			hns_roce_free_db(hr_dev, &hr_cq->db);
 +		hns_roce_free_cq(hr_dev, hr_cq);
 +		hns_roce_mtt_cleanup(hr_dev, &hr_cq->hr_buf.hr_mtt);
 +
 +		if (ib_cq->uobject) {
 +			ib_umem_release(hr_cq->umem);
 +
 +			if (hr_cq->db_en == 1)
 +				hns_roce_db_unmap_user(
 +					to_hr_ucontext(ib_cq->uobject->context),
 +					&hr_cq->db);
 +		} else {
 +			/* Free the buff of stored cq */
 +			hns_roce_ib_free_cq_buf(hr_dev, &hr_cq->hr_buf,
 +						ib_cq->cqe);
 +			if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB)
 +				hns_roce_free_db(hr_dev, &hr_cq->db);
 +		}
 +
 +		kfree(hr_cq);
  	}
++<<<<<<< HEAD
 +
 +	return ret;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  EXPORT_SYMBOL_GPL(hns_roce_ib_destroy_cq);
  
diff --cc drivers/infiniband/hw/hns/hns_roce_device.h
index 46aeda06cfd9,303ea7c614a8..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@@ -1012,15 -1205,15 +1012,21 @@@ void hns_roce_release_range_qp(struct h
  __be32 send_ieth(const struct ib_send_wr *wr);
  int to_hr_qp_type(int qp_type);
  
++<<<<<<< HEAD
 +struct ib_cq *hns_roce_ib_create_cq(struct ib_device *ib_dev,
 +				    const struct ib_cq_init_attr *attr,
 +				    struct ib_ucontext *context,
 +				    struct ib_udata *udata);
++=======
+ int hns_roce_ib_create_cq(struct ib_cq *ib_cq,
+ 			  const struct ib_cq_init_attr *attr,
+ 			  struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  
 -void hns_roce_ib_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata);
 +int hns_roce_ib_destroy_cq(struct ib_cq *ib_cq);
  void hns_roce_free_cq(struct hns_roce_dev *hr_dev, struct hns_roce_cq *hr_cq);
  
 -int hns_roce_db_map_user(struct hns_roce_ucontext *context,
 -			 struct ib_udata *udata, unsigned long virt,
 +int hns_roce_db_map_user(struct hns_roce_ucontext *context, unsigned long virt,
  			 struct hns_roce_db *db);
  void hns_roce_db_unmap_user(struct hns_roce_ucontext *context,
  			    struct hns_roce_db *db);
diff --cc drivers/infiniband/hw/hns/hns_roce_hw_v1.c
index ce18a158aa0b,c899879da222..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
@@@ -716,8 -717,8 +716,12 @@@ static int hns_roce_v1_rsv_lp_qp(struc
  	union ib_gid dgid;
  	u64 subnet_prefix;
  	int attr_mask = 0;
++<<<<<<< HEAD
++=======
+ 	int ret;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	int i, j;
 +	int ret;
  	u8 queue_en[HNS_ROCE_V1_RESV_QP] = { 0 };
  	u8 phy_port;
  	u8 port = 0;
@@@ -729,10 -730,16 +733,22 @@@
  	/* Reserved cq for loop qp */
  	cq_init_attr.cqe		= HNS_ROCE_MIN_WQE_NUM * 2;
  	cq_init_attr.comp_vector	= 0;
++<<<<<<< HEAD
 +	cq = hns_roce_ib_create_cq(&hr_dev->ib_dev, &cq_init_attr, NULL, NULL);
 +	if (IS_ERR(cq)) {
 +		dev_err(dev, "Create cq for reserved loop qp failed!");
++=======
+ 
+ 	ibdev = &hr_dev->ib_dev;
+ 	cq = rdma_zalloc_drv_obj(ibdev, ib_cq);
+ 	if (!cq)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		return -ENOMEM;
+ 
+ 	ret = hns_roce_ib_create_cq(cq, &cq_init_attr, NULL);
+ 	if (ret) {
+ 		dev_err(dev, "Create cq for reserved loop qp failed!");
+ 		goto alloc_cq_failed;
  	}
  	free_mr->mr_free_cq = to_hr_cq(cq);
  	free_mr->mr_free_cq->ib_cq.device		= &hr_dev->ib_dev;
@@@ -742,12 -749,15 +758,22 @@@
  	free_mr->mr_free_cq->ib_cq.cq_context		= NULL;
  	atomic_set(&free_mr->mr_free_cq->ib_cq.usecnt, 0);
  
++<<<<<<< HEAD
 +	pd = hns_roce_alloc_pd(&hr_dev->ib_dev, NULL, NULL);
 +	if (IS_ERR(pd)) {
 +		dev_err(dev, "Create pd for reserved loop qp failed!");
 +		ret = -ENOMEM;
++=======
+ 	pd = rdma_zalloc_drv_obj(ibdev, ib_pd);
+ 	if (!pd)
+ 		goto alloc_mem_failed;
+ 
+ 	pd->device  = ibdev;
+ 	ret = hns_roce_alloc_pd(pd, NULL);
+ 	if (ret)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		goto alloc_pd_failed;
 -
 +	}
  	free_mr->mr_free_pd = to_hr_pd(pd);
  	free_mr->mr_free_pd->ibpd.device  = &hr_dev->ib_dev;
  	free_mr->mr_free_pd->ibpd.uobject = NULL;
@@@ -854,13 -864,15 +880,22 @@@ create_lp_qp_failed
  			dev_err(dev, "Destroy qp %d for mr free failed!\n", i);
  	}
  
 -	hns_roce_dealloc_pd(pd, NULL);
 +	if (hns_roce_dealloc_pd(pd))
 +		dev_err(dev, "Destroy pd for create_lp_qp failed!\n");
  
  alloc_pd_failed:
++<<<<<<< HEAD
 +	if (hns_roce_ib_destroy_cq(cq))
 +		dev_err(dev, "Destroy cq for create_lp_qp failed!\n");
 +
++=======
+ 	kfree(pd);
+ 
+ alloc_mem_failed:
+ 	hns_roce_ib_destroy_cq(cq, NULL);
+ alloc_cq_failed:
+ 	kfree(cq);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	return ret;
  }
  
@@@ -887,13 -899,9 +922,19 @@@ static void hns_roce_v1_release_lp_qp(s
  				i, ret);
  	}
  
++<<<<<<< HEAD
 +	ret = hns_roce_ib_destroy_cq(&free_mr->mr_free_cq->ib_cq);
 +	if (ret)
 +		dev_err(dev, "Destroy cq for mr_free failed(%d)!\n", ret);
 +
 +	ret = hns_roce_dealloc_pd(&free_mr->mr_free_pd->ibpd);
 +	if (ret)
 +		dev_err(dev, "Destroy pd for mr_free failed(%d)!\n", ret);
++=======
+ 	hns_roce_ib_destroy_cq(&free_mr->mr_free_cq->ib_cq, NULL);
+ 	kfree(&free_mr->mr_free_cq->ib_cq);
+ 	hns_roce_dealloc_pd(&free_mr->mr_free_pd->ibpd, NULL);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static int hns_roce_db_init(struct hns_roce_dev *hr_dev)
@@@ -4040,10 -3701,6 +4081,13 @@@ static int hns_roce_v1_destroy_cq(struc
  		cq_buf_size = (ibcq->cqe + 1) * hr_dev->caps.cq_entry_sz;
  		hns_roce_buf_free(hr_dev, cq_buf_size, &hr_cq->hr_buf.hr_buf);
  	}
++<<<<<<< HEAD
 +
 +	kfree(hr_cq);
 +
 +	return ret;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static void set_eq_cons_index_v1(struct hns_roce_eq *eq, int req_not)
diff --cc drivers/infiniband/hw/hns/hns_roce_main.c
index e6509f248a6a,3e45b119b0eb..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_main.c
+++ b/drivers/infiniband/hw/hns/hns_roce_main.c
@@@ -505,6 -444,11 +505,14 @@@ static const struct ib_device_ops hns_r
  	.query_pkey = hns_roce_query_pkey,
  	.query_port = hns_roce_query_port,
  	.reg_user_mr = hns_roce_reg_user_mr,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, hns_roce_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, hns_roce_cq, ib_cq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, hns_roce_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, hns_roce_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops hns_roce_dev_mr_ops = {
diff --cc drivers/infiniband/hw/i40iw/i40iw_verbs.c
index fab854e906ed,3100b0c31b0a..000000000000
--- a/drivers/infiniband/hw/i40iw/i40iw_verbs.c
+++ b/drivers/infiniband/hw/i40iw/i40iw_verbs.c
@@@ -1106,25 -1075,22 +1106,31 @@@ static int i40iw_destroy_cq(struct ib_c
  	cq = &iwcq->sc_cq;
  	i40iw_cq_wq_destroy(iwdev, cq);
  	cq_free_resources(iwdev, iwcq);
- 	kfree(iwcq);
  	i40iw_rem_devusecount(iwdev);
 +	return 0;
  }
  
  /**
   * i40iw_create_cq - create cq
-  * @ibdev: device pointer from stack
+  * @ibcq: CQ allocated
   * @attr: attributes for cq
 + * @context: user context created during alloc
   * @udata: user data
   */
++<<<<<<< HEAD
 +static struct ib_cq *i40iw_create_cq(struct ib_device *ibdev,
 +				     const struct ib_cq_init_attr *attr,
 +				     struct ib_ucontext *context,
 +				     struct ib_udata *udata)
++=======
+ static int i40iw_create_cq(struct ib_cq *ibcq,
+ 			   const struct ib_cq_init_attr *attr,
+ 			   struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	struct i40iw_device *iwdev = to_iwdev(ibdev);
- 	struct i40iw_cq *iwcq;
+ 	struct i40iw_cq *iwcq = to_iwcq(ibcq);
  	struct i40iw_pbl *iwpbl;
  	u32 cq_num = 0;
  	struct i40iw_sc_cq *cq;
@@@ -2710,6 -2684,9 +2708,12 @@@ static const struct ib_device_ops i40iw
  	.query_qp = i40iw_query_qp,
  	.reg_user_mr = i40iw_reg_user_mr,
  	.req_notify_cq = i40iw_req_notify_cq,
++<<<<<<< HEAD
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, i40iw_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, i40iw_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, i40iw_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  /**
diff --cc drivers/infiniband/hw/mlx4/cq.c
index 3b6507196091,72f238ddafb5..000000000000
--- a/drivers/infiniband/hw/mlx4/cq.c
+++ b/drivers/infiniband/hw/mlx4/cq.c
@@@ -171,11 -172,10 +171,17 @@@ err_buf
  }
  
  #define CQ_CREATE_FLAGS_SUPPORTED IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION
++<<<<<<< HEAD
 +struct ib_cq *mlx4_ib_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata)
++=======
+ int mlx4_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	int vector = attr->comp_vector;
  	struct mlx4_ib_dev *dev = to_mdev(ibdev);
@@@ -183,16 -183,14 +189,12 @@@
  	struct mlx4_uar *uar;
  	void *buf_addr;
  	int err;
 -	struct mlx4_ib_ucontext *context = rdma_udata_to_drv_context(
 -		udata, struct mlx4_ib_ucontext, ibucontext);
  
  	if (entries < 1 || entries > dev->dev->caps.max_cqes)
- 		return ERR_PTR(-EINVAL);
+ 		return -EINVAL;
  
  	if (attr->flags & ~CQ_CREATE_FLAGS_SUPPORTED)
- 		return ERR_PTR(-EINVAL);
- 
- 	cq = kzalloc(sizeof(*cq), GFP_KERNEL);
- 	if (!cq)
- 		return ERR_PTR(-ENOMEM);
+ 		return -EINVAL;
  
  	entries      = roundup_pow_of_two(entries + 1);
  	cq->ibcq.cqe = entries - 1;
@@@ -287,13 -283,10 +289,10 @@@ err_mtt
  		mlx4_ib_free_cq_buf(dev, &cq->buf, cq->ibcq.cqe);
  
  err_db:
 -	if (!udata)
 +	if (!context)
  		mlx4_db_free(dev->dev, &cq->db);
- 
  err_cq:
- 	kfree(cq);
- 
- 	return ERR_PTR(err);
+ 	return err;
  }
  
  static int mlx4_alloc_resize_buf(struct mlx4_ib_dev *dev, struct mlx4_ib_cq *cq,
@@@ -501,10 -499,6 +500,13 @@@ int mlx4_ib_destroy_cq(struct ib_cq *cq
  		mlx4_ib_free_cq_buf(dev, &mcq->buf, cq->cqe);
  		mlx4_db_free(dev->dev, &mcq->db);
  	}
++<<<<<<< HEAD
 +
 +	kfree(mcq);
 +
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static void dump_cqe(void *cqe)
diff --cc drivers/infiniband/hw/mlx4/main.c
index d66002a31000,8790101facb7..000000000000
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@@ -2580,6 -2563,12 +2580,15 @@@ static const struct ib_device_ops mlx4_
  	.req_notify_cq = mlx4_ib_arm_cq,
  	.rereg_user_mr = mlx4_ib_rereg_user_mr,
  	.resize_cq = mlx4_ib_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, mlx4_ib_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, mlx4_ib_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, mlx4_ib_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, mlx4_ib_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, mlx4_ib_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops mlx4_ib_dev_wq_ops = {
diff --cc drivers/infiniband/hw/mlx4/mlx4_ib.h
index 4c774304cc63,81b3d85e5167..000000000000
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@@ -744,11 -743,9 +744,17 @@@ int mlx4_ib_map_mr_sg(struct ib_mr *ibm
  		      unsigned int *sg_offset);
  int mlx4_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
  int mlx4_ib_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata);
++<<<<<<< HEAD
 +struct ib_cq *mlx4_ib_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata);
 +int mlx4_ib_destroy_cq(struct ib_cq *cq);
++=======
+ int mlx4_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata);
+ void mlx4_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int mlx4_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
  int mlx4_ib_arm_cq(struct ib_cq *cq, enum ib_cq_notify_flags flags);
  void __mlx4_ib_cq_clean(struct mlx4_ib_cq *cq, u32 qpn, struct mlx4_ib_srq *srq);
diff --cc drivers/infiniband/hw/mlx5/cq.c
index 6bfa574fa013,07b73df0e1a3..000000000000
--- a/drivers/infiniband/hw/mlx5/cq.c
+++ b/drivers/infiniband/hw/mlx5/cq.c
@@@ -881,11 -884,10 +881,17 @@@ static void notify_soft_wc_handler(stru
  	cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);
  }
  
++<<<<<<< HEAD
 +struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata)
++=======
+ int mlx5_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	int vector = attr->comp_vector;
  	struct mlx5_ib_dev *dev = to_mdev(ibdev);
@@@ -923,11 -921,11 +925,11 @@@
  	INIT_LIST_HEAD(&cq->list_send_qp);
  	INIT_LIST_HEAD(&cq->list_recv_qp);
  
 -	if (udata) {
 -		err = create_cq_user(dev, udata, cq, entries, &cqb, &cqe_size,
 -				     &index, &inlen);
 +	if (context) {
 +		err = create_cq_user(dev, udata, context, cq, entries,
 +				     &cqb, &cqe_size, &index, &inlen);
  		if (err)
- 			goto err_create;
+ 			return err;
  	} else {
  		cqe_size = cache_line_size() == 128 ? 128 : 64;
  		err = create_cq_kernel(dev, cq, entries, cqe_size, &cqb,
@@@ -985,36 -983,23 +987,35 @@@ err_cmd
  
  err_cqb:
  	kvfree(cqb);
 -	if (udata)
 -		destroy_cq_user(cq, udata);
 +	if (context)
 +		destroy_cq_user(cq, context);
  	else
  		destroy_cq_kernel(dev, cq);
- 
- err_create:
- 	kfree(cq);
- 
- 	return ERR_PTR(err);
+ 	return err;
  }
  
 -void mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata)
 +
 +int mlx5_ib_destroy_cq(struct ib_cq *cq)
  {
  	struct mlx5_ib_dev *dev = to_mdev(cq->device);
  	struct mlx5_ib_cq *mcq = to_mcq(cq);
 +	struct ib_ucontext *context = NULL;
 +
 +	if (cq->uobject)
 +		context = cq->uobject->context;
  
  	mlx5_core_destroy_cq(dev->mdev, &mcq->mcq);
 -	if (udata)
 -		destroy_cq_user(mcq, udata);
 +	if (context)
 +		destroy_cq_user(mcq, context);
  	else
  		destroy_cq_kernel(dev, mcq);
++<<<<<<< HEAD
 +
 +	kfree(mcq);
 +
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static int is_equal_rsn(struct mlx5_cqe64 *cqe64, u32 rsn)
diff --cc drivers/infiniband/hw/mlx5/main.c
index 9937619aa9d5,99eb4a8b0b0d..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -4747,19 -4887,24 +4747,34 @@@ static int create_dev_resources(struct 
  	devr->p0->uobject = NULL;
  	atomic_set(&devr->p0->usecnt, 0);
  
++<<<<<<< HEAD
 +	devr->c0 = mlx5_ib_create_cq(&dev->ib_dev, &cq_attr, NULL, NULL);
 +	if (IS_ERR(devr->c0)) {
 +		ret = PTR_ERR(devr->c0);
++=======
+ 	ret = mlx5_ib_alloc_pd(devr->p0, NULL);
+ 	if (ret)
+ 		goto error0;
+ 
+ 	devr->c0 = rdma_zalloc_drv_obj(ibdev, ib_cq);
+ 	if (!devr->c0) {
+ 		ret = -ENOMEM;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		goto error1;
  	}
- 	devr->c0->device        = &dev->ib_dev;
- 	devr->c0->uobject       = NULL;
- 	devr->c0->comp_handler  = NULL;
- 	devr->c0->event_handler = NULL;
- 	devr->c0->cq_context    = NULL;
+ 
+ 	devr->c0->device = &dev->ib_dev;
  	atomic_set(&devr->c0->usecnt, 0);
  
++<<<<<<< HEAD
 +	devr->x0 = mlx5_ib_alloc_xrcd(&dev->ib_dev, NULL, NULL);
++=======
+ 	ret = mlx5_ib_create_cq(devr->c0, &cq_attr, NULL);
+ 	if (ret)
+ 		goto err_create_cq;
+ 
+ 	devr->x0 = mlx5_ib_alloc_xrcd(&dev->ib_dev, NULL);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	if (IS_ERR(devr->x0)) {
  		ret = PTR_ERR(devr->x0);
  		goto error2;
@@@ -4833,35 -4983,44 +4848,54 @@@
  
  	return 0;
  
 -error6:
 -	kfree(devr->s1);
  error5:
 -	mlx5_ib_destroy_srq(devr->s0, NULL);
 -err_create:
 -	kfree(devr->s0);
 +	mlx5_ib_destroy_srq(devr->s0);
  error4:
 -	mlx5_ib_dealloc_xrcd(devr->x1, NULL);
 +	mlx5_ib_dealloc_xrcd(devr->x1);
  error3:
 -	mlx5_ib_dealloc_xrcd(devr->x0, NULL);
 +	mlx5_ib_dealloc_xrcd(devr->x0);
  error2:
++<<<<<<< HEAD
 +	mlx5_ib_destroy_cq(devr->c0);
++=======
+ 	mlx5_ib_destroy_cq(devr->c0, NULL);
+ err_create_cq:
+ 	kfree(devr->c0);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  error1:
 -	mlx5_ib_dealloc_pd(devr->p0, NULL);
 +	mlx5_ib_dealloc_pd(devr->p0);
  error0:
 -	kfree(devr->p0);
  	return ret;
  }
  
  static void destroy_dev_resources(struct mlx5_ib_resources *devr)
  {
 +	struct mlx5_ib_dev *dev =
 +		container_of(devr, struct mlx5_ib_dev, devr);
  	int port;
  
++<<<<<<< HEAD
 +	mlx5_ib_destroy_srq(devr->s1);
 +	mlx5_ib_destroy_srq(devr->s0);
 +	mlx5_ib_dealloc_xrcd(devr->x0);
 +	mlx5_ib_dealloc_xrcd(devr->x1);
 +	mlx5_ib_destroy_cq(devr->c0);
 +	mlx5_ib_dealloc_pd(devr->p0);
++=======
+ 	mlx5_ib_destroy_srq(devr->s1, NULL);
+ 	kfree(devr->s1);
+ 	mlx5_ib_destroy_srq(devr->s0, NULL);
+ 	kfree(devr->s0);
+ 	mlx5_ib_dealloc_xrcd(devr->x0, NULL);
+ 	mlx5_ib_dealloc_xrcd(devr->x1, NULL);
+ 	mlx5_ib_destroy_cq(devr->c0, NULL);
+ 	kfree(devr->c0);
+ 	mlx5_ib_dealloc_pd(devr->p0, NULL);
+ 	kfree(devr->p0);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  
  	/* Make sure no change P_Key work items are still executing */
 -	for (port = 0; port < ARRAY_SIZE(devr->ports); ++port)
 +	for (port = 0; port < dev->num_ports; ++port)
  		cancel_work_sync(&devr->ports[port].pkey_change_work);
  }
  
@@@ -5979,6 -6184,12 +6013,15 @@@ static const struct ib_device_ops mlx5_
  	.req_notify_cq = mlx5_ib_arm_cq,
  	.rereg_user_mr = mlx5_ib_rereg_user_mr,
  	.resize_cq = mlx5_ib_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, mlx5_ib_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, mlx5_ib_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, mlx5_ib_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, mlx5_ib_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, mlx5_ib_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops mlx5_ib_dev_flow_ipsec_ops = {
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1b44b1ade284,f2ad0372d38d..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -1080,14 -1109,15 +1080,26 @@@ int mlx5_ib_post_send(struct ib_qp *ibq
  		      const struct ib_send_wr **bad_wr);
  int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
  		      const struct ib_recv_wr **bad_wr);
++<<<<<<< HEAD
 +int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 +			  void *buffer, u32 length,
 +			  struct mlx5_ib_qp_base *base);
 +struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 +				const struct ib_cq_init_attr *attr,
 +				struct ib_ucontext *context,
 +				struct ib_udata *udata);
 +int mlx5_ib_destroy_cq(struct ib_cq *cq);
++=======
+ int mlx5_ib_read_user_wqe_sq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+ 			     int buflen, size_t *bc);
+ int mlx5_ib_read_user_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+ 			     int buflen, size_t *bc);
+ int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
+ 			      void *buffer, int buflen, size_t *bc);
+ int mlx5_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		      struct ib_udata *udata);
+ void mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
  int mlx5_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
  int mlx5_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
diff --cc drivers/infiniband/hw/mthca/mthca_provider.c
index f4ce72da939d,efd4e3d13ae2..000000000000
--- a/drivers/infiniband/hw/mthca/mthca_provider.c
+++ b/drivers/infiniband/hw/mthca/mthca_provider.c
@@@ -650,47 -601,45 +650,56 @@@ static int mthca_destroy_qp(struct ib_q
  	return 0;
  }
  
++<<<<<<< HEAD
 +static struct ib_cq *mthca_create_cq(struct ib_device *ibdev,
 +				     const struct ib_cq_init_attr *attr,
 +				     struct ib_ucontext *context,
 +				     struct ib_udata *udata)
++=======
+ static int mthca_create_cq(struct ib_cq *ibcq,
+ 			   const struct ib_cq_init_attr *attr,
+ 			   struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	struct mthca_create_cq ucmd;
  	struct mthca_cq *cq;
  	int nent;
  	int err;
 -	struct mthca_ucontext *context = rdma_udata_to_drv_context(
 -		udata, struct mthca_ucontext, ibucontext);
  
  	if (attr->flags)
- 		return ERR_PTR(-EINVAL);
+ 		return -EINVAL;
  
  	if (entries < 1 || entries > to_mdev(ibdev)->limits.max_cqes)
- 		return ERR_PTR(-EINVAL);
+ 		return -EINVAL;
  
++<<<<<<< HEAD
 +	if (context) {
 +		if (ib_copy_from_udata(&ucmd, udata, sizeof ucmd))
 +			return ERR_PTR(-EFAULT);
++=======
+ 	if (udata) {
+ 		if (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd)))
+ 			return -EFAULT;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  
 -		err = mthca_map_user_db(to_mdev(ibdev), &context->uar,
 -					context->db_tab, ucmd.set_db_index,
 -					ucmd.set_db_page);
 +		err = mthca_map_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,
 +					to_mucontext(context)->db_tab,
 +					ucmd.set_db_index, ucmd.set_db_page);
  		if (err)
- 			return ERR_PTR(err);
+ 			return err;
  
 -		err = mthca_map_user_db(to_mdev(ibdev), &context->uar,
 -					context->db_tab, ucmd.arm_db_index,
 -					ucmd.arm_db_page);
 +		err = mthca_map_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,
 +					to_mucontext(context)->db_tab,
 +					ucmd.arm_db_index, ucmd.arm_db_page);
  		if (err)
  			goto err_unmap_set;
  	}
  
- 	cq = kzalloc(sizeof(*cq), GFP_KERNEL);
- 	if (!cq) {
- 		err = -ENOMEM;
- 		goto err_unmap_arm;
- 	}
+ 	cq = to_mcq(ibcq);
  
 -	if (udata) {
 +	if (context) {
  		cq->buf.mr.ibmr.lkey = ucmd.lkey;
  		cq->set_ci_db_index  = ucmd.set_db_index;
  		cq->arm_db_index     = ucmd.arm_db_index;
@@@ -699,37 -648,33 +708,34 @@@
  	for (nent = 1; nent <= entries; nent <<= 1)
  		; /* nothing */
  
 -	err = mthca_init_cq(to_mdev(ibdev), nent, context,
 -			    udata ? ucmd.pdn : to_mdev(ibdev)->driver_pd.pd_num,
 +	err = mthca_init_cq(to_mdev(ibdev), nent,
 +			    context ? to_mucontext(context) : NULL,
 +			    context ? ucmd.pdn : to_mdev(ibdev)->driver_pd.pd_num,
  			    cq);
  	if (err)
- 		goto err_free;
+ 		goto err_unmap_arm;
  
 -	if (udata && ib_copy_to_udata(udata, &cq->cqn, sizeof(__u32))) {
 +	if (context && ib_copy_to_udata(udata, &cq->cqn, sizeof (__u32))) {
  		mthca_free_cq(to_mdev(ibdev), cq);
  		err = -EFAULT;
- 		goto err_free;
+ 		goto err_unmap_arm;
  	}
  
  	cq->resize_buf = NULL;
  
- 	return &cq->ibcq;
- 
- err_free:
- 	kfree(cq);
+ 	return 0;
  
  err_unmap_arm:
 -	if (udata)
 -		mthca_unmap_user_db(to_mdev(ibdev), &context->uar,
 -				    context->db_tab, ucmd.arm_db_index);
 +	if (context)
 +		mthca_unmap_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,
 +				    to_mucontext(context)->db_tab, ucmd.arm_db_index);
  
  err_unmap_set:
 -	if (udata)
 -		mthca_unmap_user_db(to_mdev(ibdev), &context->uar,
 -				    context->db_tab, ucmd.set_db_index);
 +	if (context)
 +		mthca_unmap_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,
 +				    to_mucontext(context)->db_tab, ucmd.set_db_index);
  
- 	return ERR_PTR(err);
+ 	return err;
  }
  
  static int mthca_alloc_resize_buf(struct mthca_dev *dev, struct mthca_cq *cq,
@@@ -866,9 -817,6 +872,12 @@@ static int mthca_destroy_cq(struct ib_c
  				    to_mcq(cq)->set_ci_db_index);
  	}
  	mthca_free_cq(to_mdev(cq->device), to_mcq(cq));
++<<<<<<< HEAD
 +	kfree(cq);
 +
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static inline u32 convert_access(int acc)
@@@ -1224,6 -1178,11 +1233,14 @@@ static const struct ib_device_ops mthca
  	.query_qp = mthca_query_qp,
  	.reg_user_mr = mthca_reg_user_mr,
  	.resize_cq = mthca_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, mthca_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, mthca_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, mthca_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, mthca_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops mthca_dev_arbel_srq_ops = {
diff --cc drivers/infiniband/hw/nes/nes_verbs.c
index fc87bfde3c68,0420203820f6..000000000000
--- a/drivers/infiniband/hw/nes/nes_verbs.c
+++ b/drivers/infiniband/hw/nes/nes_verbs.c
@@@ -1393,11 -1374,11 +1393,12 @@@ static int nes_destroy_qp(struct ib_qp 
  /**
   * nes_create_cq
   */
- static struct ib_cq *nes_create_cq(struct ib_device *ibdev,
+ static int nes_create_cq(struct ib_cq *ibcq,
  				   const struct ib_cq_init_attr *attr,
 +				   struct ib_ucontext *context,
  				   struct ib_udata *udata)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	u64 u64temp;
  	struct nes_vnic *nesvnic = to_nesvnic(ibdev);
@@@ -1440,13 -1414,14 +1434,17 @@@
  	nescq->hw_cq.cq_number = cq_num;
  	nescq->ibcq.cqe = nescq->hw_cq.cq_size - 1;
  
 -	if (udata) {
 -		struct nes_ucontext *nes_ucontext = rdma_udata_to_drv_context(
 -			udata, struct nes_ucontext, ibucontext);
  
++<<<<<<< HEAD
 +	if (context) {
 +		nes_ucontext = to_nesucontext(context);
 +		if (ib_copy_from_udata(&req, udata, sizeof (struct nes_create_cq_req))) {
++=======
+ 		if (ib_copy_from_udata(&req, udata,
+ 				       sizeof(struct nes_create_cq_req))) {
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  			nes_free_resource(nesadapter, nesadapter->allocated_cqs, cq_num);
- 			kfree(nescq);
- 			return ERR_PTR(-EFAULT);
+ 			return -EFAULT;
  		}
  		nesvnic->mcrq_ucontext = nes_ucontext;
  		nes_ucontext->mcrqf = req.mcrqf;
@@@ -1729,9 -1682,6 +1715,12 @@@ static int nes_destroy_cq(struct ib_cq 
  	if (nescq->cq_mem_size)
  		pci_free_consistent(nesdev->pcidev, nescq->cq_mem_size,
  				    nescq->hw_cq.cq_vbase, nescq->hw_cq.cq_pbase);
++<<<<<<< HEAD
 +	kfree(nescq);
 +
 +	return ret;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  /**
@@@ -3600,6 -3564,9 +3589,12 @@@ static const struct ib_device_ops nes_d
  	.query_qp = nes_query_qp,
  	.reg_user_mr = nes_reg_user_mr,
  	.req_notify_cq = nes_req_notify_cq,
++<<<<<<< HEAD
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, nes_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, nes_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, nes_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  /**
diff --cc drivers/infiniband/hw/ocrdma/ocrdma_main.c
index f315478642e2,c15cfc6cef81..000000000000
--- a/drivers/infiniband/hw/ocrdma/ocrdma_main.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_main.c
@@@ -176,6 -180,11 +176,14 @@@ static const struct ib_device_ops ocrdm
  	.reg_user_mr = ocrdma_reg_user_mr,
  	.req_notify_cq = ocrdma_arm_cq,
  	.resize_cq = ocrdma_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, ocrdma_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, ocrdma_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, ocrdma_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, ocrdma_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops ocrdma_dev_srq_ops = {
diff --cc drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
index 9c19475e0282,10b35edb286b..000000000000
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
@@@ -987,15 -977,15 +987,21 @@@ err
  	return status;
  }
  
++<<<<<<< HEAD
 +struct ib_cq *ocrdma_create_cq(struct ib_device *ibdev,
 +			       const struct ib_cq_init_attr *attr,
 +			       struct ib_ucontext *ib_ctx,
 +			       struct ib_udata *udata)
++=======
+ int ocrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		     struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
- 	struct ocrdma_cq *cq;
+ 	struct ocrdma_cq *cq = get_ocrdma_cq(ibcq);
  	struct ocrdma_dev *dev = get_ocrdma_dev(ibdev);
 -	struct ocrdma_ucontext *uctx = rdma_udata_to_drv_context(
 -		udata, struct ocrdma_ucontext, ibucontext);
 +	struct ocrdma_ucontext *uctx = NULL;
  	u16 pd_id = 0;
  	int status;
  	struct ocrdma_create_cq_ureq ureq;
@@@ -1017,18 -1004,15 +1020,26 @@@
  	INIT_LIST_HEAD(&cq->sq_head);
  	INIT_LIST_HEAD(&cq->rq_head);
  
 -	if (udata)
 +	if (ib_ctx) {
 +		uctx = get_ocrdma_ucontext(ib_ctx);
  		pd_id = uctx->cntxt_pd->id;
 +	}
  
  	status = ocrdma_mbx_create_cq(dev, cq, entries, ureq.dpp_cq, pd_id);
++<<<<<<< HEAD
 +	if (status) {
 +		kfree(cq);
 +		return ERR_PTR(status);
 +	}
 +	if (ib_ctx) {
 +		status = ocrdma_copy_cq_uresp(dev, cq, udata, ib_ctx);
++=======
+ 	if (status)
+ 		return status;
+ 
+ 	if (udata) {
+ 		status = ocrdma_copy_cq_uresp(dev, cq, udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		if (status)
  			goto ctx_err;
  	}
@@@ -1108,9 -1090,6 +1118,12 @@@ int ocrdma_destroy_cq(struct ib_cq *ibc
  				ocrdma_get_db_addr(dev, pdid),
  				dev->nic_info.db_page_size);
  	}
++<<<<<<< HEAD
 +
 +	kfree(cq);
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static int ocrdma_add_qpn_map(struct ocrdma_dev *dev, struct ocrdma_qp *qp)
diff --cc drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
index 0421f3e3c922,32488da1b752..000000000000
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
@@@ -69,16 -68,13 +69,21 @@@ int ocrdma_dealloc_ucontext(struct ib_u
  
  int ocrdma_mmap(struct ib_ucontext *, struct vm_area_struct *vma);
  
 -int ocrdma_alloc_pd(struct ib_pd *pd, struct ib_udata *udata);
 -void ocrdma_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata);
 +struct ib_pd *ocrdma_alloc_pd(struct ib_device *,
 +			      struct ib_ucontext *, struct ib_udata *);
 +int ocrdma_dealloc_pd(struct ib_pd *pd);
  
++<<<<<<< HEAD
 +struct ib_cq *ocrdma_create_cq(struct ib_device *ibdev,
 +			       const struct ib_cq_init_attr *attr,
 +			       struct ib_ucontext *ib_ctx,
 +			       struct ib_udata *udata);
++=======
+ int ocrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		     struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int ocrdma_resize_cq(struct ib_cq *, int cqe, struct ib_udata *);
 -void ocrdma_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);
 +int ocrdma_destroy_cq(struct ib_cq *);
  
  struct ib_qp *ocrdma_create_qp(struct ib_pd *,
  			       struct ib_qp_init_attr *attrs,
diff --cc drivers/infiniband/hw/qedr/main.c
index 65953b3f1f4f,a0a7ba0a5af4..000000000000
--- a/drivers/infiniband/hw/qedr/main.c
+++ b/drivers/infiniband/hw/qedr/main.c
@@@ -223,6 -222,12 +223,15 @@@ static const struct ib_device_ops qedr_
  	.reg_user_mr = qedr_reg_user_mr,
  	.req_notify_cq = qedr_arm_cq,
  	.resize_cq = qedr_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, qedr_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, qedr_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, qedr_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, qedr_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, qedr_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static int qedr_register_device(struct qedr_dev *dev)
diff --cc drivers/infiniband/hw/qedr/verbs.c
index 26243f439c76,3fc7a4e901c3..000000000000
--- a/drivers/infiniband/hw/qedr/verbs.c
+++ b/drivers/infiniband/hw/qedr/verbs.c
@@@ -838,11 -806,12 +838,20 @@@ int qedr_arm_cq(struct ib_cq *ibcq, enu
  	return 0;
  }
  
++<<<<<<< HEAD
 +struct ib_cq *qedr_create_cq(struct ib_device *ibdev,
 +			     const struct ib_cq_init_attr *attr,
 +			     struct ib_ucontext *ib_ctx, struct ib_udata *udata)
 +{
 +	struct qedr_ucontext *ctx = get_qedr_ucontext(ib_ctx);
++=======
+ int qedr_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		   struct ib_udata *udata)
+ {
+ 	struct ib_device *ibdev = ibcq->device;
+ 	struct qedr_ucontext *ctx = rdma_udata_to_drv_context(
+ 		udata, struct qedr_ucontext, ibucontext);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	struct qed_rdma_destroy_cq_out_params destroy_oparams;
  	struct qed_rdma_destroy_cq_in_params destroy_iparams;
  	struct qedr_dev *dev = get_qedr_dev(ibdev);
@@@ -1001,16 -963,13 +1004,16 @@@ int qedr_destroy_cq(struct ib_cq *ibcq
  
  	/* GSIs CQs are handled by driver, so they don't exist in the FW */
  	if (cq->cq_type == QEDR_CQ_TYPE_GSI)
- 		goto done;
+ 		return;
  
  	iparams.icid = cq->icid;
 -	dev->ops->rdma_destroy_cq(dev->rdma_ctx, &iparams, &oparams);
 +	rc = dev->ops->rdma_destroy_cq(dev->rdma_ctx, &iparams, &oparams);
 +	if (rc)
 +		return rc;
 +
  	dev->ops->common->chain_free(dev->cdev, &cq->pbl);
  
 -	if (udata) {
 +	if (ibcq->uobject && ibcq->uobject->context) {
  		qedr_free_pbl(dev, &cq->q.pbl_info, cq->q.pbl_tbl);
  		ib_umem_release(cq->q.umem);
  	}
@@@ -1046,19 -1002,6 +1049,22 @@@
  	 * Since the destroy CQ ramrod has also been received on the EQ we can
  	 * be certain that there's no event handler in process.
  	 */
++<<<<<<< HEAD
 +done:
 +	cq->sig = ~cq->sig;
 +
 +	kfree(cq);
 +
 +	return 0;
 +
 +err:
 +	DP_ERR(dev,
 +	       "CQ %p (icid=%d) not freed, expecting %d ints but got %d ints\n",
 +	       cq, cq->icid, oparams.num_cq_notif, cq->cnq_notif);
 +
 +	return -EINVAL;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  static inline int get_gid_info_from_table(struct ib_qp *ibqp,
diff --cc drivers/infiniband/hw/qedr/verbs.h
index 0e5fb4e82418,9aaa90283d6e..000000000000
--- a/drivers/infiniband/hw/qedr/verbs.h
+++ b/drivers/infiniband/hw/qedr/verbs.h
@@@ -43,20 -43,17 +43,25 @@@ int qedr_iw_query_gid(struct ib_device 
  
  int qedr_query_pkey(struct ib_device *, u8 port, u16 index, u16 *pkey);
  
 -int qedr_alloc_ucontext(struct ib_ucontext *uctx, struct ib_udata *udata);
 -void qedr_dealloc_ucontext(struct ib_ucontext *uctx);
 +struct ib_ucontext *qedr_alloc_ucontext(struct ib_device *, struct ib_udata *);
 +int qedr_dealloc_ucontext(struct ib_ucontext *);
  
  int qedr_mmap(struct ib_ucontext *, struct vm_area_struct *vma);
 -int qedr_alloc_pd(struct ib_pd *pd, struct ib_udata *udata);
 -void qedr_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata);
 +struct ib_pd *qedr_alloc_pd(struct ib_device *,
 +			    struct ib_ucontext *, struct ib_udata *);
 +int qedr_dealloc_pd(struct ib_pd *pd);
  
++<<<<<<< HEAD
 +struct ib_cq *qedr_create_cq(struct ib_device *ibdev,
 +			     const struct ib_cq_init_attr *attr,
 +			     struct ib_ucontext *ib_ctx,
 +			     struct ib_udata *udata);
++=======
+ int qedr_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		   struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int qedr_resize_cq(struct ib_cq *, int cqe, struct ib_udata *);
 -void qedr_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);
 +int qedr_destroy_cq(struct ib_cq *);
  int qedr_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
  struct ib_qp *qedr_create_qp(struct ib_pd *, struct ib_qp_init_attr *attrs,
  			     struct ib_udata *);
diff --cc drivers/infiniband/hw/usnic/usnic_ib_main.c
index 6b56f4f2222a,6ae5ce007fed..000000000000
--- a/drivers/infiniband/hw/usnic/usnic_ib_main.c
+++ b/drivers/infiniband/hw/usnic/usnic_ib_main.c
@@@ -343,7 -353,9 +343,13 @@@ static const struct ib_device_ops usnic
  	.query_port = usnic_ib_query_port,
  	.query_qp = usnic_ib_query_qp,
  	.reg_user_mr = usnic_ib_reg_mr,
++<<<<<<< HEAD
 +	.req_notify_cq = usnic_ib_req_notify_cq,
++=======
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, usnic_ib_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, usnic_ib_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, usnic_ib_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  /* Start of PF discovery section */
diff --cc drivers/infiniband/hw/usnic/usnic_ib_verbs.c
index 432e6f6599fa,eeb07b245ef9..000000000000
--- a/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
+++ b/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
@@@ -611,29 -587,18 +611,31 @@@ out_unlock
  	return status;
  }
  
++<<<<<<< HEAD
 +struct ib_cq *usnic_ib_create_cq(struct ib_device *ibdev,
 +				 const struct ib_cq_init_attr *attr,
 +				 struct ib_ucontext *context,
 +				 struct ib_udata *udata)
++=======
+ int usnic_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		       struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
- 	struct ib_cq *cq;
- 
- 	usnic_dbg("\n");
  	if (attr->flags)
- 		return ERR_PTR(-EINVAL);
- 
- 	cq = kzalloc(sizeof(*cq), GFP_KERNEL);
- 	if (!cq)
- 		return ERR_PTR(-EBUSY);
+ 		return -EINVAL;
  
- 	return cq;
+ 	return 0;
  }
  
 -void usnic_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata)
 +int usnic_ib_destroy_cq(struct ib_cq *cq)
  {
++<<<<<<< HEAD
 +	usnic_dbg("\n");
 +	kfree(cq);
 +	return 0;
++=======
+ 	return;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  struct ib_mr *usnic_ib_reg_mr(struct ib_pd *pd, u64 start, u64 length,
diff --cc drivers/infiniband/hw/usnic/usnic_ib_verbs.h
index e33144261b9a,2aedf78c13cf..000000000000
--- a/drivers/infiniband/hw/usnic/usnic_ib_verbs.h
+++ b/drivers/infiniband/hw/usnic/usnic_ib_verbs.h
@@@ -58,14 -55,12 +58,20 @@@ int usnic_ib_dealloc_pd(struct ib_pd *p
  struct ib_qp *usnic_ib_create_qp(struct ib_pd *pd,
  					struct ib_qp_init_attr *init_attr,
  					struct ib_udata *udata);
 -int usnic_ib_destroy_qp(struct ib_qp *qp, struct ib_udata *udata);
 +int usnic_ib_destroy_qp(struct ib_qp *qp);
  int usnic_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
  				int attr_mask, struct ib_udata *udata);
++<<<<<<< HEAD
 +struct ib_cq *usnic_ib_create_cq(struct ib_device *ibdev,
 +				 const struct ib_cq_init_attr *attr,
 +				 struct ib_ucontext *context,
 +				 struct ib_udata *udata);
 +int usnic_ib_destroy_cq(struct ib_cq *cq);
++=======
+ int usnic_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		       struct ib_udata *udata);
+ void usnic_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  struct ib_mr *usnic_ib_reg_mr(struct ib_pd *pd, u64 start, u64 length,
  				u64 virt_addr, int access_flags,
  				struct ib_udata *udata);
diff --cc drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c
index 0f004c737620,38573fc0a9bf..000000000000
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c
@@@ -91,22 -92,19 +91,27 @@@ int pvrdma_req_notify_cq(struct ib_cq *
  
  /**
   * pvrdma_create_cq - create completion queue
-  * @ibdev: the device
+  * @ibcq: Allocated CQ
   * @attr: completion queue attributes
 + * @context: user context
   * @udata: user data
   *
-  * @return: ib_cq completion queue pointer on success,
-  *          otherwise returns negative errno.
+  * @return: 0 on success
   */
++<<<<<<< HEAD
 +struct ib_cq *pvrdma_create_cq(struct ib_device *ibdev,
 +			       const struct ib_cq_init_attr *attr,
 +			       struct ib_ucontext *context,
 +			       struct ib_udata *udata)
++=======
+ int pvrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		     struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	int entries = attr->cqe;
  	struct pvrdma_dev *dev = to_vdev(ibdev);
- 	struct pvrdma_cq *cq;
+ 	struct pvrdma_cq *cq = to_vcq(ibcq);
  	int ret;
  	int npages;
  	unsigned long flags;
@@@ -114,26 -112,22 +119,20 @@@
  	union pvrdma_cmd_resp rsp;
  	struct pvrdma_cmd_create_cq *cmd = &req.create_cq;
  	struct pvrdma_cmd_create_cq_resp *resp = &rsp.create_cq_resp;
- 	struct pvrdma_create_cq_resp cq_resp = {0};
+ 	struct pvrdma_create_cq_resp cq_resp = {};
  	struct pvrdma_create_cq ucmd;
 -	struct pvrdma_ucontext *context = rdma_udata_to_drv_context(
 -		udata, struct pvrdma_ucontext, ibucontext);
  
  	BUILD_BUG_ON(sizeof(struct pvrdma_cqe) != 64);
  
  	entries = roundup_pow_of_two(entries);
  	if (entries < 1 || entries > dev->dsr->caps.max_cqe)
- 		return ERR_PTR(-EINVAL);
+ 		return -EINVAL;
  
  	if (!atomic_add_unless(&dev->num_cqs, 1, dev->dsr->caps.max_cq))
- 		return ERR_PTR(-ENOMEM);
- 
- 	cq = kzalloc(sizeof(*cq), GFP_KERNEL);
- 	if (!cq) {
- 		atomic_dec(&dev->num_cqs);
- 		return ERR_PTR(-ENOMEM);
- 	}
+ 		return -ENOMEM;
  
  	cq->ibcq.cqe = entries;
 -	cq->is_kernel = !udata;
 +	cq->is_kernel = !context;
  
  	if (!cq->is_kernel) {
  		if (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {
@@@ -210,8 -203,8 +209,13 @@@
  		if (ib_copy_to_udata(udata, &cq_resp, sizeof(cq_resp))) {
  			dev_warn(&dev->pdev->dev,
  				 "failed to copy back udata\n");
++<<<<<<< HEAD
 +			pvrdma_destroy_cq(&cq->ibcq);
 +			return ERR_PTR(-EINVAL);
++=======
+ 			pvrdma_destroy_cq(&cq->ibcq, udata);
+ 			return -EINVAL;
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		}
  	}
  
diff --cc drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
index 02c7ba35823c,e580ae9cc55a..000000000000
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
@@@ -176,6 -180,11 +176,14 @@@ static const struct ib_device_ops pvrdm
  	.query_qp = pvrdma_query_qp,
  	.reg_user_mr = pvrdma_reg_user_mr,
  	.req_notify_cq = pvrdma_req_notify_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, pvrdma_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, pvrdma_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, pvrdma_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, pvrdma_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static const struct ib_device_ops pvrdma_dev_srq_ops = {
diff --cc drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
index f7f758d60110,e4a48f5c0c85..000000000000
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
@@@ -407,25 -404,22 +407,31 @@@ struct ib_mr *pvrdma_get_dma_mr(struct 
  struct ib_mr *pvrdma_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
  				 u64 virt_addr, int access_flags,
  				 struct ib_udata *udata);
 -int pvrdma_dereg_mr(struct ib_mr *mr, struct ib_udata *udata);
 +int pvrdma_dereg_mr(struct ib_mr *mr);
  struct ib_mr *pvrdma_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 -			      u32 max_num_sg, struct ib_udata *udata);
 +			      u32 max_num_sg);
  int pvrdma_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,
  		     int sg_nents, unsigned int *sg_offset);
++<<<<<<< HEAD
 +struct ib_cq *pvrdma_create_cq(struct ib_device *ibdev,
 +			       const struct ib_cq_init_attr *attr,
 +			       struct ib_ucontext *context,
 +			       struct ib_udata *udata);
 +int pvrdma_destroy_cq(struct ib_cq *cq);
++=======
+ int pvrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		     struct ib_udata *udata);
+ void pvrdma_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int pvrdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
  int pvrdma_req_notify_cq(struct ib_cq *cq, enum ib_cq_notify_flags flags);
 -int pvrdma_create_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr, u32 flags,
 -		     struct ib_udata *udata);
 -void pvrdma_destroy_ah(struct ib_ah *ah, u32 flags);
 +struct ib_ah *pvrdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
 +			       u32 flags, struct ib_udata *udata);
 +int pvrdma_destroy_ah(struct ib_ah *ah, u32 flags);
  
 -int pvrdma_create_srq(struct ib_srq *srq, struct ib_srq_init_attr *init_attr,
 -		      struct ib_udata *udata);
 +struct ib_srq *pvrdma_create_srq(struct ib_pd *pd,
 +				 struct ib_srq_init_attr *init_attr,
 +				 struct ib_udata *udata);
  int pvrdma_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
  		      enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
  int pvrdma_query_srq(struct ib_srq *srq, struct ib_srq_attr *srq_attr);
diff --cc drivers/infiniband/sw/rdmavt/cq.c
index 4f1544ad4aff,b46714a92b7a..000000000000
--- a/drivers/infiniband/sw/rdmavt/cq.c
+++ b/drivers/infiniband/sw/rdmavt/cq.c
@@@ -166,25 -166,21 +166,29 @@@ static void send_complete(struct work_s
  
  /**
   * rvt_create_cq - create a completion queue
-  * @ibdev: the device this completion queue is attached to
+  * @ibcq: Allocated CQ
   * @attr: creation attributes
 + * @context: unused by the QLogic_IB driver
   * @udata: user data for libibverbs.so
   *
   * Called by ib_create_cq() in the generic verbs code.
   *
-  * Return: pointer to the completion queue or negative errno values
-  * for failure.
+  * Return: 0 on success
   */
++<<<<<<< HEAD
 +struct ib_cq *rvt_create_cq(struct ib_device *ibdev,
 +			    const struct ib_cq_init_attr *attr,
 +			    struct ib_ucontext *context,
 +			    struct ib_udata *udata)
++=======
+ int rvt_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		  struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
+ 	struct ib_device *ibdev = ibcq->device;
  	struct rvt_dev_info *rdi = ib_to_rvt(ibdev);
- 	struct rvt_cq *cq;
+ 	struct rvt_cq *cq = container_of(ibcq, struct rvt_cq, ibcq);
  	struct rvt_cq_wc *wc;
- 	struct ib_cq *ret;
  	u32 sz;
  	unsigned int entries = attr->cqe;
  	int comp_vector = attr->comp_vector;
@@@ -230,11 -219,9 +227,15 @@@
  	 * See rvt_mmap() for details.
  	 */
  	if (udata && udata->outlen >= sizeof(__u64)) {
++<<<<<<< HEAD
 +		int err;
 +
 +		cq->ip = rvt_create_mmap_info(rdi, sz, context, wc);
++=======
+ 		cq->ip = rvt_create_mmap_info(rdi, sz, udata, wc);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  		if (!cq->ip) {
- 			ret = ERR_PTR(-ENOMEM);
+ 			err = -ENOMEM;
  			goto bail_wc;
  		}
  
@@@ -317,9 -296,6 +311,12 @@@ int rvt_destroy_cq(struct ib_cq *ibcq
  		kref_put(&cq->ip->ref, rvt_release_mmap_info);
  	else
  		vfree(cq->queue);
++<<<<<<< HEAD
 +	kfree(cq);
 +
 +	return 0;
++=======
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  }
  
  /**
diff --cc drivers/infiniband/sw/rdmavt/cq.h
index 72184b1c176b,5e26a2eb19a4..000000000000
--- a/drivers/infiniband/sw/rdmavt/cq.h
+++ b/drivers/infiniband/sw/rdmavt/cq.h
@@@ -51,11 -51,9 +51,17 @@@
  #include <rdma/rdma_vt.h>
  #include <rdma/rdmavt_cq.h>
  
++<<<<<<< HEAD
 +struct ib_cq *rvt_create_cq(struct ib_device *ibdev,
 +			    const struct ib_cq_init_attr *attr,
 +			    struct ib_ucontext *context,
 +			    struct ib_udata *udata);
 +int rvt_destroy_cq(struct ib_cq *ibcq);
++=======
+ int rvt_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 		  struct ib_udata *udata);
+ void rvt_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  int rvt_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify_flags);
  int rvt_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata);
  int rvt_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry);
diff --cc drivers/infiniband/sw/rdmavt/vt.c
index b3f0c5578925,18da1e1ea979..000000000000
--- a/drivers/infiniband/sw/rdmavt/vt.c
+++ b/drivers/infiniband/sw/rdmavt/vt.c
@@@ -436,6 -427,12 +436,15 @@@ static const struct ib_device_ops rvt_d
  	.req_notify_cq = rvt_req_notify_cq,
  	.resize_cq = rvt_resize_cq,
  	.unmap_fmr = rvt_unmap_fmr,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, rvt_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, rvt_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, rvt_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, rvt_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, rvt_ucontext, ibucontext),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
  static noinline int check_support(struct rvt_dev_info *rdi, int verb)
diff --cc drivers/infiniband/sw/rxe/rxe_verbs.c
index f3188f269481,4ebdfcf4d33e..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@@ -801,14 -778,13 +801,20 @@@ err1
  	return err;
  }
  
++<<<<<<< HEAD
 +static struct ib_cq *rxe_create_cq(struct ib_device *dev,
 +				   const struct ib_cq_init_attr *attr,
 +				   struct ib_ucontext *context,
 +				   struct ib_udata *udata)
++=======
+ static int rxe_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+ 			 struct ib_udata *udata)
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  {
  	int err;
+ 	struct ib_device *dev = ibcq->device;
  	struct rxe_dev *rxe = to_rdev(dev);
- 	struct rxe_cq *cq;
+ 	struct rxe_cq *cq = to_rcq(ibcq);
  	struct rxe_create_cq_resp __user *uresp = NULL;
  
  	if (udata) {
@@@ -822,28 -798,17 +828,17 @@@
  
  	err = rxe_cq_chk_attr(rxe, NULL, attr->cqe, attr->comp_vector);
  	if (err)
- 		goto err1;
- 
- 	cq = rxe_alloc(&rxe->cq_pool);
- 	if (!cq) {
- 		err = -ENOMEM;
- 		goto err1;
- 	}
+ 		return err;
  
 -	err = rxe_cq_from_init(rxe, cq, attr->cqe, attr->comp_vector, udata,
 -			       uresp);
 +	err = rxe_cq_from_init(rxe, cq, attr->cqe, attr->comp_vector,
 +			       context, uresp);
  	if (err)
- 		goto err2;
- 
- 	return &cq->ibcq;
+ 		return err;
  
- err2:
- 	rxe_drop_ref(cq);
- err1:
- 	return ERR_PTR(err);
+ 	return rxe_add_to_pool(&rxe->cq_pool, &cq->pelem);
  }
  
 -static void rxe_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)
 +static int rxe_destroy_cq(struct ib_cq *ibcq)
  {
  	struct rxe_cq *cq = to_rcq(ibcq);
  
@@@ -1169,9 -1147,15 +1164,18 @@@ static const struct ib_device_ops rxe_d
  	.reg_user_mr = rxe_reg_user_mr,
  	.req_notify_cq = rxe_req_notify_cq,
  	.resize_cq = rxe_resize_cq,
++<<<<<<< HEAD
++=======
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_ah, rxe_ah, ibah),
+ 	INIT_RDMA_OBJ_SIZE(ib_cq, rxe_cq, ibcq),
+ 	INIT_RDMA_OBJ_SIZE(ib_pd, rxe_pd, ibpd),
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, rxe_srq, ibsrq),
+ 	INIT_RDMA_OBJ_SIZE(ib_ucontext, rxe_ucontext, ibuc),
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
 -int rxe_register_device(struct rxe_dev *rxe, const char *ibdev_name)
 +int rxe_register_device(struct rxe_dev *rxe)
  {
  	int err;
  	struct ib_device *dev = &rxe->ib_dev;
diff --cc include/rdma/ib_verbs.h
index 714b9b5e3733,f357e03a85a6..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -2417,13 -2457,11 +2417,19 @@@ struct ib_device_ops 
  			 int qp_attr_mask, struct ib_udata *udata);
  	int (*query_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
  			int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr);
++<<<<<<< HEAD
 +	int (*destroy_qp)(struct ib_qp *qp);
 +	struct ib_cq *(*create_cq)(struct ib_device *device,
 +				   const struct ib_cq_init_attr *attr,
 +				   struct ib_ucontext *context,
 +				   struct ib_udata *udata);
++=======
+ 	int (*destroy_qp)(struct ib_qp *qp, struct ib_udata *udata);
+ 	int (*create_cq)(struct ib_cq *cq, const struct ib_cq_init_attr *attr,
+ 			 struct ib_udata *udata);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
 -	void (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
 +	int (*destroy_cq)(struct ib_cq *cq);
  	int (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);
  	struct ib_mr *(*get_dma_mr)(struct ib_pd *pd, int mr_access_flags);
  	struct ib_mr *(*reg_user_mr)(struct ib_pd *pd, u64 start, u64 length,
@@@ -2532,8 -2569,55 +2538,46 @@@
  	 */
  	int (*init_port)(struct ib_device *device, u8 port_num,
  			 struct kobject *port_sysfs);
++<<<<<<< HEAD
++=======
+ 	/**
+ 	 * Allows rdma drivers to add their own restrack attributes.
+ 	 */
+ 	int (*fill_res_entry)(struct sk_buff *msg,
+ 			      struct rdma_restrack_entry *entry);
+ 
+ 	/* Device lifecycle callbacks */
+ 	/*
+ 	 * Called after the device becomes registered, before clients are
+ 	 * attached
+ 	 */
+ 	int (*enable_driver)(struct ib_device *dev);
+ 	/*
+ 	 * This is called as part of ib_dealloc_device().
+ 	 */
+ 	void (*dealloc_driver)(struct ib_device *dev);
+ 
+ 	/* iWarp CM callbacks */
+ 	void (*iw_add_ref)(struct ib_qp *qp);
+ 	void (*iw_rem_ref)(struct ib_qp *qp);
+ 	struct ib_qp *(*iw_get_qp)(struct ib_device *device, int qpn);
+ 	int (*iw_connect)(struct iw_cm_id *cm_id,
+ 			  struct iw_cm_conn_param *conn_param);
+ 	int (*iw_accept)(struct iw_cm_id *cm_id,
+ 			 struct iw_cm_conn_param *conn_param);
+ 	int (*iw_reject)(struct iw_cm_id *cm_id, const void *pdata,
+ 			 u8 pdata_len);
+ 	int (*iw_create_listen)(struct iw_cm_id *cm_id, int backlog);
+ 	int (*iw_destroy_listen)(struct iw_cm_id *cm_id);
+ 
+ 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
+ 	DECLARE_RDMA_OBJ_SIZE(ib_cq);
+ 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
+ 	DECLARE_RDMA_OBJ_SIZE(ib_srq);
+ 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
++>>>>>>> e39afe3d6dbd (RDMA: Convert CQ allocations to be under core responsibility)
  };
  
 -struct ib_core_device {
 -	/* device must be the first element in structure until,
 -	 * union of ib_core_device and device exists in ib_device.
 -	 */
 -	struct device dev;
 -	possible_net_t rdma_net;
 -	struct kobject *ports_kobj;
 -	struct list_head port_list;
 -	struct ib_device *owner; /* reach back to owner ib_device */
 -};
 -
 -struct rdma_restrack_root;
  struct ib_device {
  	/* Do not access @dma_device directly from ULP nor from HW drivers. */
  	struct device                *dma_device;
* Unmerged path drivers/infiniband/core/cq.c
* Unmerged path drivers/infiniband/core/device.c
* Unmerged path drivers/infiniband/core/uverbs_cmd.c
* Unmerged path drivers/infiniband/core/uverbs_std_types_cq.c
* Unmerged path drivers/infiniband/core/verbs.c
* Unmerged path drivers/infiniband/hw/bnxt_re/ib_verbs.c
* Unmerged path drivers/infiniband/hw/bnxt_re/ib_verbs.h
* Unmerged path drivers/infiniband/hw/bnxt_re/main.c
* Unmerged path drivers/infiniband/hw/cxgb3/iwch_provider.c
* Unmerged path drivers/infiniband/hw/cxgb4/cq.c
* Unmerged path drivers/infiniband/hw/cxgb4/iw_cxgb4.h
* Unmerged path drivers/infiniband/hw/cxgb4/provider.c
* Unmerged path drivers/infiniband/hw/efa/efa.h
diff --git a/drivers/infiniband/hw/efa/efa_main.c b/drivers/infiniband/hw/efa/efa_main.c
index 44152ed1ee86..f1cfb56e3673 100644
--- a/drivers/infiniband/hw/efa/efa_main.c
+++ b/drivers/infiniband/hw/efa/efa_main.c
@@ -220,6 +220,7 @@ static const struct ib_device_ops efa_dev_ops = {
 	.reg_user_mr = efa_reg_mr,
 
 	INIT_RDMA_OBJ_SIZE(ib_ah, efa_ah, ibah),
+	INIT_RDMA_OBJ_SIZE(ib_cq, efa_cq, ibcq),
 	INIT_RDMA_OBJ_SIZE(ib_pd, efa_pd, ibpd),
 	INIT_RDMA_OBJ_SIZE(ib_ucontext, efa_ucontext, ibucontext),
 };
* Unmerged path drivers/infiniband/hw/efa/efa_verbs.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_cq.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_device.h
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v1.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_main.c
* Unmerged path drivers/infiniband/hw/i40iw/i40iw_verbs.c
* Unmerged path drivers/infiniband/hw/mlx4/cq.c
* Unmerged path drivers/infiniband/hw/mlx4/main.c
* Unmerged path drivers/infiniband/hw/mlx4/mlx4_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/cq.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mthca/mthca_provider.c
* Unmerged path drivers/infiniband/hw/nes/nes_verbs.c
* Unmerged path drivers/infiniband/hw/ocrdma/ocrdma_main.c
* Unmerged path drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
* Unmerged path drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
* Unmerged path drivers/infiniband/hw/qedr/main.c
* Unmerged path drivers/infiniband/hw/qedr/verbs.c
* Unmerged path drivers/infiniband/hw/qedr/verbs.h
diff --git a/drivers/infiniband/hw/usnic/usnic_ib.h b/drivers/infiniband/hw/usnic/usnic_ib.h
index 1461be06940c..b38f8839dfad 100644
--- a/drivers/infiniband/hw/usnic/usnic_ib.h
+++ b/drivers/infiniband/hw/usnic/usnic_ib.h
@@ -61,6 +61,10 @@ struct usnic_ib_pd {
 	struct usnic_uiom_pd		*umem_pd;
 };
 
+struct usnic_ib_cq {
+	struct ib_cq			ibcq;
+};
+
 struct usnic_ib_mr {
 	struct ib_mr			ibmr;
 	struct usnic_uiom_reg		*umem;
* Unmerged path drivers/infiniband/hw/usnic/usnic_ib_main.c
* Unmerged path drivers/infiniband/hw/usnic/usnic_ib_verbs.c
* Unmerged path drivers/infiniband/hw/usnic/usnic_ib_verbs.h
* Unmerged path drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c
* Unmerged path drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
* Unmerged path drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
* Unmerged path drivers/infiniband/sw/rdmavt/cq.c
* Unmerged path drivers/infiniband/sw/rdmavt/cq.h
* Unmerged path drivers/infiniband/sw/rdmavt/vt.c
diff --git a/drivers/infiniband/sw/rxe/rxe_pool.c b/drivers/infiniband/sw/rxe/rxe_pool.c
index b5c91df22047..97dea6d596ee 100644
--- a/drivers/infiniband/sw/rxe/rxe_pool.c
+++ b/drivers/infiniband/sw/rxe/rxe_pool.c
@@ -70,6 +70,7 @@ struct rxe_type_info rxe_type_info[RXE_NUM_TYPES] = {
 	[RXE_TYPE_CQ] = {
 		.name		= "rxe-cq",
 		.size		= sizeof(struct rxe_cq),
+		.flags          = RXE_POOL_NO_ALLOC,
 		.cleanup	= rxe_cq_cleanup,
 	},
 	[RXE_TYPE_MR] = {
* Unmerged path drivers/infiniband/sw/rxe/rxe_verbs.c
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 74e04801d34d..b6783a11c980 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -85,8 +85,8 @@ struct rxe_cqe {
 };
 
 struct rxe_cq {
-	struct rxe_pool_entry	pelem;
 	struct ib_cq		ibcq;
+	struct rxe_pool_entry	pelem;
 	struct rxe_queue	*queue;
 	spinlock_t		cq_lock;
 	u8			notify;
* Unmerged path include/rdma/ib_verbs.h
