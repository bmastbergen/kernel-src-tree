mm/hmm: add default fault flags to avoid the need to pre-fill pfns arrays

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jérôme Glisse <jglisse@redhat.com>
commit 023a019a9b4e90b9df8ed5be591787b5c914d74f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/023a019a.failed

The HMM mirror API can be use in two fashions.  The first one where the
HMM user coalesce multiple page faults into one request and set flags per
pfns for of those faults.  The second one where the HMM user want to
pre-fault a range with specific flags.  For the latter one it is a waste
to have the user pre-fill the pfn arrays with a default flags value.

This patch adds a default flags value allowing user to set them for a
range without having to pre-fill the pfn array.

Link: http://lkml.kernel.org/r/20190403193318.16478-8-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 023a019a9b4e90b9df8ed5be591787b5c914d74f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
diff --cc include/linux/hmm.h
index e51dc3e6210f,dee2f8953b2e..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -354,41 -419,76 +358,97 @@@ void hmm_mirror_unregister(struct hmm_m
  
  
  /*
 - * Please see Documentation/vm/hmm.rst for how to use the range API.
 + * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device
 + * driver lock that serializes device page table updates, then call
 + * hmm_vma_range_done(), to check if the snapshot is still valid. The same
 + * device driver page table update lock must also be used in the
 + * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page
 + * table invalidation serializes on it.
 + *
 + * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL
 + * hmm_vma_get_pfns() WITHOUT ERROR !
 + *
 + * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !
   */
 -int hmm_range_register(struct hmm_range *range,
 -		       struct mm_struct *mm,
 -		       unsigned long start,
 -		       unsigned long end);
 -void hmm_range_unregister(struct hmm_range *range);
 -long hmm_range_snapshot(struct hmm_range *range);
 -long hmm_range_fault(struct hmm_range *range, bool block);
 +int hmm_vma_get_pfns(struct hmm_range *range);
 +bool hmm_vma_range_done(struct hmm_range *range);
 +
  
  /*
 - * HMM_RANGE_DEFAULT_TIMEOUT - default timeout (ms) when waiting for a range
 + * Fault memory on behalf of device driver. Unlike handle_mm_fault(), this will
 + * not migrate any device memory back to system memory. The HMM pfn array will
 + * be updated with the fault result and current snapshot of the CPU page table
 + * for the range.
 + *
 + * The mmap_sem must be taken in read mode before entering and it might be
 + * dropped by the function if the block argument is false. In that case, the
 + * function returns -EAGAIN.
   *
 - * When waiting for mmu notifiers we need some kind of time out otherwise we
 - * could potentialy wait for ever, 1000ms ie 1s sounds like a long time to
 - * wait already.
 + * Return value does not reflect if the fault was successful for every single
 + * address or not. Therefore, the caller must to inspect the HMM pfn array to
 + * determine fault status for each address.
 + *
 + * Trying to fault inside an invalid vma will result in -EINVAL.
 + *
 + * See the function description in mm/hmm.c for further documentation.
   */
++<<<<<<< HEAD
 +int hmm_vma_fault(struct hmm_range *range, bool block);
++=======
+ #define HMM_RANGE_DEFAULT_TIMEOUT 1000
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline bool hmm_vma_range_done(struct hmm_range *range)
+ {
+ 	bool ret = hmm_range_valid(range);
+ 
+ 	hmm_range_unregister(range);
+ 	return ret;
+ }
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline int hmm_vma_fault(struct hmm_range *range, bool block)
+ {
+ 	long ret;
+ 
+ 	/*
+ 	 * With the old API the driver must set each individual entries with
+ 	 * the requested flags (valid, write, ...). So here we set the mask to
+ 	 * keep intact the entries provided by the driver and zero out the
+ 	 * default_flags.
+ 	 */
+ 	range->default_flags = 0;
+ 	range->pfn_flags_mask = -1UL;
+ 
+ 	ret = hmm_range_register(range, range->vma->vm_mm,
+ 				 range->start, range->end);
+ 	if (ret)
+ 		return (int)ret;
+ 
+ 	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
+ 		/*
+ 		 * The mmap_sem was taken by driver we release it here and
+ 		 * returns -EAGAIN which correspond to mmap_sem have been
+ 		 * drop in the old API.
+ 		 */
+ 		up_read(&range->vma->vm_mm->mmap_sem);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	ret = hmm_range_fault(range, block);
+ 	if (ret <= 0) {
+ 		if (ret == -EBUSY || !ret) {
+ 			/* Same as above  drop mmap_sem to match old API. */
+ 			up_read(&range->vma->vm_mm->mmap_sem);
+ 			ret = -EBUSY;
+ 		} else if (ret == -EAGAIN)
+ 			ret = -EBUSY;
+ 		hmm_range_unregister(range);
+ 		return ret;
+ 	}
+ 	return 0;
+ }
++>>>>>>> 023a019a9b4e (mm/hmm: add default fault flags to avoid the need to pre-fill pfns arrays)
  
  /* Below are for HMM internal use only! Not to be used by device driver! */
  void hmm_mm_destroy(struct mm_struct *mm);
diff --git a/Documentation/vm/hmm.rst b/Documentation/vm/hmm.rst
index cdf3911582c8..82166ae78df5 100644
--- a/Documentation/vm/hmm.rst
+++ b/Documentation/vm/hmm.rst
@@ -255,6 +255,41 @@ report commands as executed is serialized (there is no point in doing this
 concurrently).
 
 
+Leverage default_flags and pfn_flags_mask
+=========================================
+
+The hmm_range struct has 2 fields default_flags and pfn_flags_mask that allows
+to set fault or snapshot policy for a whole range instead of having to set them
+for each entries in the range.
+
+For instance if the device flags for device entries are:
+    VALID (1 << 63)
+    WRITE (1 << 62)
+
+Now let say that device driver wants to fault with at least read a range then
+it does set:
+    range->default_flags = (1 << 63)
+    range->pfn_flags_mask = 0;
+
+and calls hmm_range_fault() as described above. This will fill fault all page
+in the range with at least read permission.
+
+Now let say driver wants to do the same except for one page in the range for
+which its want to have write. Now driver set:
+    range->default_flags = (1 << 63);
+    range->pfn_flags_mask = (1 << 62);
+    range->pfns[index_of_write] = (1 << 62);
+
+With this HMM will fault in all page with at least read (ie valid) and for the
+address == range->start + (index_of_write << PAGE_SHIFT) it will fault with
+write permission ie if the CPU pte does not have write permission set then HMM
+will call handle_mm_fault().
+
+Note that HMM will populate the pfns array with write permission for any entry
+that have write permission within the CPU pte no matter what are the values set
+in default_flags or pfn_flags_mask.
+
+
 Represent and manage device memory from core kernel point of view
 =================================================================
 
* Unmerged path include/linux/hmm.h
diff --git a/mm/hmm.c b/mm/hmm.c
index 4c052ccc4e21..519ec19778c6 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -389,6 +389,18 @@ static inline void hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
 	if (!hmm_vma_walk->fault)
 		return;
 
+	/*
+	 * So we not only consider the individual per page request we also
+	 * consider the default flags requested for the range. The API can
+	 * be use in 2 fashions. The first one where the HMM user coalesce
+	 * multiple page fault into one request and set flags per pfns for
+	 * of those faults. The second one where the HMM user want to pre-
+	 * fault a range with specific flags. For the latter one it is a
+	 * waste to have the user pre-fill the pfn arrays with a default
+	 * flags value.
+	 */
+	pfns = (pfns & range->pfn_flags_mask) | range->default_flags;
+
 	/* We aren't ask to do anything ... */
 	if (!(pfns & range->flags[HMM_PFN_VALID]))
 		return;
