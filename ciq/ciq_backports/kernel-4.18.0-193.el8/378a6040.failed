mm/hmm: Remove duplicate condition test before wait_event_timeout

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 378a60406415bd20ec6e845a3d6883d460656537
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/378a6040.failed

The wait_event_timeout macro already tests the condition as its first
action, so there is no reason to open code another version of this, all
that does is skip the might_sleep() debugging in common cases, which is
not helpful.

Further, based on prior patches, we can now simplify the required condition
test:
 - If range is valid memory then so is range->hmm
 - If hmm_release() has run then range->valid is set to false
   at the same time as dead, so no reason to check both.
 - A valid hmm has a valid hmm->mm.

Allowing the return value of wait_event_timeout() (along with its internal
barriers) to compute the result of the function.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: John Hubbard <jhubbard@nvidia.com>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Philip Yang <Philip.Yang@amd.com>
(cherry picked from commit 378a60406415bd20ec6e845a3d6883d460656537)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
diff --cc include/linux/hmm.h
index 2f68a486cc0d,26e7c477490c..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -153,26 -181,69 +153,76 @@@ struct hmm_range 
  };
  
  /*
++<<<<<<< HEAD
 + * hmm_pfn_to_page() - return struct page pointed to by a valid HMM pfn
 + * @range: range use to decode HMM pfn value
 + * @pfn: HMM pfn value to get corresponding struct page from
 + * Returns: struct page pointer if pfn is a valid HMM pfn, NULL otherwise
++=======
+  * hmm_range_page_shift() - return the page shift for the range
+  * @range: range being queried
+  * Return: page shift (page size = 1 << page shift) for the range
+  */
+ static inline unsigned hmm_range_page_shift(const struct hmm_range *range)
+ {
+ 	return range->page_shift;
+ }
+ 
+ /*
+  * hmm_range_page_size() - return the page size for the range
+  * @range: range being queried
+  * Return: page size for the range in bytes
+  */
+ static inline unsigned long hmm_range_page_size(const struct hmm_range *range)
+ {
+ 	return 1UL << hmm_range_page_shift(range);
+ }
+ 
+ /*
+  * hmm_range_wait_until_valid() - wait for range to be valid
+  * @range: range affected by invalidation to wait on
+  * @timeout: time out for wait in ms (ie abort wait after that period of time)
+  * Return: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_wait_until_valid(struct hmm_range *range,
+ 					      unsigned long timeout)
+ {
+ 	return wait_event_timeout(range->hmm->wq, range->valid,
+ 				  msecs_to_jiffies(timeout)) != 0;
+ }
+ 
+ /*
+  * hmm_range_valid() - test if a range is valid or not
+  * @range: range
+  * Return: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_valid(struct hmm_range *range)
+ {
+ 	return range->valid;
+ }
+ 
+ /*
+  * hmm_device_entry_to_page() - return struct page pointed to by a device entry
+  * @range: range use to decode device entry value
+  * @entry: device entry value to get corresponding struct page from
+  * Return: struct page pointer if entry is a valid, NULL otherwise
++>>>>>>> 378a60406415 (mm/hmm: Remove duplicate condition test before wait_event_timeout)
   *
 - * If the device entry is valid (ie valid flag set) then return the struct page
 - * matching the entry value. Otherwise return NULL.
 + * If the HMM pfn is valid (ie valid flag set) then return the struct page
 + * matching the pfn value stored in the HMM pfn. Otherwise return NULL.
   */
 -static inline struct page *hmm_device_entry_to_page(const struct hmm_range *range,
 -						    uint64_t entry)
 +static inline struct page *hmm_pfn_to_page(const struct hmm_range *range,
 +					   uint64_t pfn)
  {
 -	if (entry == range->values[HMM_PFN_NONE])
 +	if (pfn == range->values[HMM_PFN_NONE])
  		return NULL;
 -	if (entry == range->values[HMM_PFN_ERROR])
 +	if (pfn == range->values[HMM_PFN_ERROR])
  		return NULL;
 -	if (entry == range->values[HMM_PFN_SPECIAL])
 +	if (pfn == range->values[HMM_PFN_SPECIAL])
  		return NULL;
 -	if (!(entry & range->flags[HMM_PFN_VALID]))
 +	if (!(pfn & range->flags[HMM_PFN_VALID]))
  		return NULL;
 -	return pfn_to_page(entry >> range->pfn_shift);
 +	return pfn_to_page(pfn >> range->pfn_shift);
  }
  
  /*
* Unmerged path include/linux/hmm.h
