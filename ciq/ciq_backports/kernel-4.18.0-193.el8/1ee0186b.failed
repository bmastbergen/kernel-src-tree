iommu/vt-d: Refactor find_domain() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Lu Baolu <baolu.lu@linux.intel.com>
commit 1ee0186b9a128a872887e16e2d1520ea37a95dc4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1ee0186b.failed

Current find_domain() helper checks and does the deferred domain
attachment and return the domain in use. This isn't always the
use case for the callers. Some callers only want to retrieve the
current domain in use.

This refactors find_domain() into two helpers: 1) find_domain()
only returns the domain in use; 2) deferred_attach_domain() does
the deferred domain attachment if required and return the domain
in use.

	Cc: Ashok Raj <ashok.raj@intel.com>
	Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
	Cc: Kevin Tian <kevin.tian@intel.com>
	Signed-off-by: Lu Baolu <baolu.lu@linux.intel.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 1ee0186b9a128a872887e16e2d1520ea37a95dc4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index 1431de3bfba0,e70cc1c5055f..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -2378,11 -2424,30 +2374,37 @@@ static struct dmar_domain *find_domain(
  {
  	struct device_domain_info *info;
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely(dev->archdata.iommu == DEFER_DEVICE_DOMAIN_INFO ||
+ 		     dev->archdata.iommu == DUMMY_DEVICE_DOMAIN_INFO))
+ 		return NULL;
+ 
++>>>>>>> 1ee0186b9a12 (iommu/vt-d: Refactor find_domain() helper)
  	/* No lock here, assumes no domain exit in normal case */
  	info = dev->archdata.iommu;
  	if (likely(info))
  		return info->domain;
++<<<<<<< HEAD
 +	return NULL;
++=======
+ 
+ 	return NULL;
+ }
+ 
+ static struct dmar_domain *deferred_attach_domain(struct device *dev)
+ {
+ 	if (unlikely(dev->archdata.iommu == DEFER_DEVICE_DOMAIN_INFO)) {
+ 		struct iommu_domain *domain;
+ 
+ 		dev->archdata.iommu = NULL;
+ 		domain = iommu_get_domain_for_dev(dev);
+ 		if (domain)
+ 			intel_iommu_attach_device(domain, dev);
+ 	}
+ 
+ 	return find_domain(dev);
++>>>>>>> 1ee0186b9a12 (iommu/vt-d: Refactor find_domain() helper)
  }
  
  static inline struct device_domain_info *
@@@ -3645,10 -3517,7 +3667,14 @@@ static dma_addr_t __intel_map_single(st
  
  	BUG_ON(dir == DMA_NONE);
  
++<<<<<<< HEAD
 +	if (iommu_no_mapping(dev))
 +		return paddr;
 +
 +	domain = get_valid_domain_for_dev(dev);
++=======
+ 	domain = deferred_attach_domain(dev);
++>>>>>>> 1ee0186b9a12 (iommu/vt-d: Refactor find_domain() helper)
  	if (!domain)
  		return DMA_MAPPING_ERROR;
  
@@@ -3864,10 -3734,10 +3890,14 @@@ static int intel_map_sg(struct device *
  	struct intel_iommu *iommu;
  
  	BUG_ON(dir == DMA_NONE);
 -	if (!iommu_need_mapping(dev))
 -		return dma_direct_map_sg(dev, sglist, nelems, dir, attrs);
 +	if (iommu_no_mapping(dev))
 +		return intel_nontranslate_map_sg(dev, sglist, nelems, dir);
  
++<<<<<<< HEAD
 +	domain = get_valid_domain_for_dev(dev);
++=======
+ 	domain = deferred_attach_domain(dev);
++>>>>>>> 1ee0186b9a12 (iommu/vt-d: Refactor find_domain() helper)
  	if (!domain)
  		return 0;
  
@@@ -3915,8 -3788,256 +3945,259 @@@ static const struct dma_map_ops intel_d
  	.map_page = intel_map_page,
  	.unmap_page = intel_unmap_page,
  	.map_resource = intel_map_resource,
 -	.unmap_resource = intel_unmap_resource,
 +	.unmap_resource = intel_unmap_page,
  	.dma_supported = dma_direct_supported,
++<<<<<<< HEAD
++=======
+ 	.mmap = dma_common_mmap,
+ 	.get_sgtable = dma_common_get_sgtable,
+ };
+ 
+ static void
+ bounce_sync_single(struct device *dev, dma_addr_t addr, size_t size,
+ 		   enum dma_data_direction dir, enum dma_sync_target target)
+ {
+ 	struct dmar_domain *domain;
+ 	phys_addr_t tlb_addr;
+ 
+ 	domain = find_domain(dev);
+ 	if (WARN_ON(!domain))
+ 		return;
+ 
+ 	tlb_addr = intel_iommu_iova_to_phys(&domain->domain, addr);
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_sync_single(dev, tlb_addr, size, dir, target);
+ }
+ 
+ static dma_addr_t
+ bounce_map_single(struct device *dev, phys_addr_t paddr, size_t size,
+ 		  enum dma_data_direction dir, unsigned long attrs,
+ 		  u64 dma_mask)
+ {
+ 	size_t aligned_size = ALIGN(size, VTD_PAGE_SIZE);
+ 	struct dmar_domain *domain;
+ 	struct intel_iommu *iommu;
+ 	unsigned long iova_pfn;
+ 	unsigned long nrpages;
+ 	phys_addr_t tlb_addr;
+ 	int prot = 0;
+ 	int ret;
+ 
+ 	domain = deferred_attach_domain(dev);
+ 	if (WARN_ON(dir == DMA_NONE || !domain))
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	iommu = domain_get_iommu(domain);
+ 	if (WARN_ON(!iommu))
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	nrpages = aligned_nrpages(0, size);
+ 	iova_pfn = intel_alloc_iova(dev, domain,
+ 				    dma_to_mm_pfn(nrpages), dma_mask);
+ 	if (!iova_pfn)
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	/*
+ 	 * Check if DMAR supports zero-length reads on write only
+ 	 * mappings..
+ 	 */
+ 	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL ||
+ 			!cap_zlr(iommu->cap))
+ 		prot |= DMA_PTE_READ;
+ 	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
+ 		prot |= DMA_PTE_WRITE;
+ 
+ 	/*
+ 	 * If both the physical buffer start address and size are
+ 	 * page aligned, we don't need to use a bounce page.
+ 	 */
+ 	if (!IS_ALIGNED(paddr | size, VTD_PAGE_SIZE)) {
+ 		tlb_addr = swiotlb_tbl_map_single(dev,
+ 				__phys_to_dma(dev, io_tlb_start),
+ 				paddr, size, aligned_size, dir, attrs);
+ 		if (tlb_addr == DMA_MAPPING_ERROR) {
+ 			goto swiotlb_error;
+ 		} else {
+ 			/* Cleanup the padding area. */
+ 			void *padding_start = phys_to_virt(tlb_addr);
+ 			size_t padding_size = aligned_size;
+ 
+ 			if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
+ 			    (dir == DMA_TO_DEVICE ||
+ 			     dir == DMA_BIDIRECTIONAL)) {
+ 				padding_start += size;
+ 				padding_size -= size;
+ 			}
+ 
+ 			memset(padding_start, 0, padding_size);
+ 		}
+ 	} else {
+ 		tlb_addr = paddr;
+ 	}
+ 
+ 	ret = domain_pfn_mapping(domain, mm_to_dma_pfn(iova_pfn),
+ 				 tlb_addr >> VTD_PAGE_SHIFT, nrpages, prot);
+ 	if (ret)
+ 		goto mapping_error;
+ 
+ 	trace_bounce_map_single(dev, iova_pfn << PAGE_SHIFT, paddr, size);
+ 
+ 	return (phys_addr_t)iova_pfn << PAGE_SHIFT;
+ 
+ mapping_error:
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ 					 aligned_size, dir, attrs);
+ swiotlb_error:
+ 	free_iova_fast(&domain->iovad, iova_pfn, dma_to_mm_pfn(nrpages));
+ 	dev_err(dev, "Device bounce map: %zx@%llx dir %d --- failed\n",
+ 		size, (unsigned long long)paddr, dir);
+ 
+ 	return DMA_MAPPING_ERROR;
+ }
+ 
+ static void
+ bounce_unmap_single(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		    enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	size_t aligned_size = ALIGN(size, VTD_PAGE_SIZE);
+ 	struct dmar_domain *domain;
+ 	phys_addr_t tlb_addr;
+ 
+ 	domain = find_domain(dev);
+ 	if (WARN_ON(!domain))
+ 		return;
+ 
+ 	tlb_addr = intel_iommu_iova_to_phys(&domain->domain, dev_addr);
+ 	if (WARN_ON(!tlb_addr))
+ 		return;
+ 
+ 	intel_unmap(dev, dev_addr, size);
+ 	if (is_swiotlb_buffer(tlb_addr))
+ 		swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ 					 aligned_size, dir, attrs);
+ 
+ 	trace_bounce_unmap_single(dev, dev_addr, size);
+ }
+ 
+ static dma_addr_t
+ bounce_map_page(struct device *dev, struct page *page, unsigned long offset,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return bounce_map_single(dev, page_to_phys(page) + offset,
+ 				 size, dir, attrs, *dev->dma_mask);
+ }
+ 
+ static dma_addr_t
+ bounce_map_resource(struct device *dev, phys_addr_t phys_addr, size_t size,
+ 		    enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return bounce_map_single(dev, phys_addr, size,
+ 				 dir, attrs, *dev->dma_mask);
+ }
+ 
+ static void
+ bounce_unmap_page(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		  enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	bounce_unmap_single(dev, dev_addr, size, dir, attrs);
+ }
+ 
+ static void
+ bounce_unmap_resource(struct device *dev, dma_addr_t dev_addr, size_t size,
+ 		      enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	bounce_unmap_single(dev, dev_addr, size, dir, attrs);
+ }
+ 
+ static void
+ bounce_unmap_sg(struct device *dev, struct scatterlist *sglist, int nelems,
+ 		enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_unmap_page(dev, sg->dma_address,
+ 				  sg_dma_len(sg), dir, attrs);
+ }
+ 
+ static int
+ bounce_map_sg(struct device *dev, struct scatterlist *sglist, int nelems,
+ 	      enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	int i;
+ 	struct scatterlist *sg;
+ 
+ 	for_each_sg(sglist, sg, nelems, i) {
+ 		sg->dma_address = bounce_map_page(dev, sg_page(sg),
+ 						  sg->offset, sg->length,
+ 						  dir, attrs);
+ 		if (sg->dma_address == DMA_MAPPING_ERROR)
+ 			goto out_unmap;
+ 		sg_dma_len(sg) = sg->length;
+ 	}
+ 
+ 	return nelems;
+ 
+ out_unmap:
+ 	bounce_unmap_sg(dev, sglist, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
+ 	return 0;
+ }
+ 
+ static void
+ bounce_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+ 			   size_t size, enum dma_data_direction dir)
+ {
+ 	bounce_sync_single(dev, addr, size, dir, SYNC_FOR_CPU);
+ }
+ 
+ static void
+ bounce_sync_single_for_device(struct device *dev, dma_addr_t addr,
+ 			      size_t size, enum dma_data_direction dir)
+ {
+ 	bounce_sync_single(dev, addr, size, dir, SYNC_FOR_DEVICE);
+ }
+ 
+ static void
+ bounce_sync_sg_for_cpu(struct device *dev, struct scatterlist *sglist,
+ 		       int nelems, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_sync_single(dev, sg_dma_address(sg),
+ 				   sg_dma_len(sg), dir, SYNC_FOR_CPU);
+ }
+ 
+ static void
+ bounce_sync_sg_for_device(struct device *dev, struct scatterlist *sglist,
+ 			  int nelems, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sglist, sg, nelems, i)
+ 		bounce_sync_single(dev, sg_dma_address(sg),
+ 				   sg_dma_len(sg), dir, SYNC_FOR_DEVICE);
+ }
+ 
+ static const struct dma_map_ops bounce_dma_ops = {
+ 	.alloc			= intel_alloc_coherent,
+ 	.free			= intel_free_coherent,
+ 	.map_sg			= bounce_map_sg,
+ 	.unmap_sg		= bounce_unmap_sg,
+ 	.map_page		= bounce_map_page,
+ 	.unmap_page		= bounce_unmap_page,
+ 	.sync_single_for_cpu	= bounce_sync_single_for_cpu,
+ 	.sync_single_for_device	= bounce_sync_single_for_device,
+ 	.sync_sg_for_cpu	= bounce_sync_sg_for_cpu,
+ 	.sync_sg_for_device	= bounce_sync_sg_for_device,
+ 	.map_resource		= bounce_map_resource,
+ 	.unmap_resource		= bounce_unmap_resource,
+ 	.dma_supported		= dma_direct_supported,
++>>>>>>> 1ee0186b9a12 (iommu/vt-d: Refactor find_domain() helper)
  };
  
  static inline int iommu_domain_cache_init(void)
* Unmerged path drivers/iommu/intel-iommu.c
