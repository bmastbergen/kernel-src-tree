net: sched: notify classifier on successful offload add/delete

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] sched: notify classifier on successful offload add/delete (Ivan Vecera) [1739606]
Rebuild_FUZZ: 95.80%
commit-author Vlad Buslov <vladbu@mellanox.com>
commit a449a3e77a85fc8b31fef7238451dc87af8ff1af
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/a449a3e7.failed

To remove dependency on rtnl lock, extend classifier ops with new
ops->hw_add() and ops->hw_del() callbacks. Call them from cls API while
holding cb_lock every time filter if successfully added to or deleted from
hardware.

Implement the new API in flower classifier. Use it to manage hw_filters
list under cb_lock protection, instead of relying on rtnl lock to
synchronize with concurrent fl_reoffload() call.

	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Acked-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a449a3e77a85fc8b31fef7238451dc87af8ff1af)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_api.c
#	net/sched/cls_flower.c
diff --cc net/sched/cls_api.c
index 081ac3d0fb50,8b807e75fae2..000000000000
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@@ -3266,8 -3066,134 +3266,126 @@@ int tc_setup_cb_call(struct tcf_block *
  	}
  	return ok_count;
  }
 -
 -int tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
 -		     void *type_data, bool err_stop, bool rtnl_held)
 -{
 -	int ok_count;
 -
 -	down_read(&block->cb_lock);
 -	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
 -	up_read(&block->cb_lock);
 -	return ok_count;
 -}
  EXPORT_SYMBOL(tc_setup_cb_call);
  
++<<<<<<< HEAD
++=======
+ /* Non-destructive filter add. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offloads counter. On failure,
+  * previously offloaded filter is considered to be intact and offloads counter
+  * is not decremented.
+  */
+ 
+ int tc_setup_cb_add(struct tcf_block *block, struct tcf_proto *tp,
+ 		    enum tc_setup_type type, void *type_data, bool err_stop,
+ 		    u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	int ok_count;
+ 
+ 	down_read(&block->cb_lock);
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags,
+ 					  ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_add);
+ 
+ /* Destructive filter replace. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offload counter. On failure,
+  * previously offloaded filter is considered to be destroyed and offload counter
+  * is decremented.
+  */
+ 
+ int tc_setup_cb_replace(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *old_flags, unsigned int *old_in_hw_count,
+ 			u32 *new_flags, unsigned int *new_in_hw_count,
+ 			bool rtnl_held)
+ {
+ 	int ok_count;
+ 
+ 	down_read(&block->cb_lock);
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, old_in_hw_count, old_flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, new_in_hw_count,
+ 					  new_flags, ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_replace);
+ 
+ /* Destroy filter and decrement block offload counter, if filter was previously
+  * offloaded.
+  */
+ 
+ int tc_setup_cb_destroy(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	int ok_count;
+ 
+ 	down_read(&block->cb_lock);
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, in_hw_count, flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	up_read(&block->cb_lock);
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_destroy);
+ 
+ int tc_setup_cb_reoffload(struct tcf_block *block, struct tcf_proto *tp,
+ 			  bool add, flow_setup_cb_t *cb,
+ 			  enum tc_setup_type type, void *type_data,
+ 			  void *cb_priv, u32 *flags, unsigned int *in_hw_count)
+ {
+ 	int err = cb(type, type_data, cb_priv);
+ 
+ 	if (err) {
+ 		if (add && tc_skip_sw(*flags))
+ 			return err;
+ 	} else {
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags, 1,
+ 					  add);
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_reoffload);
+ 
++>>>>>>> a449a3e77a85 (net: sched: notify classifier on successful offload add/delete)
  int tc_setup_flow_action(struct flow_action *flow_action,
  			 const struct tcf_exts *exts)
  {
diff --cc net/sched/cls_flower.c
index 3eb4f57be10d,5cb694469b51..000000000000
--- a/net/sched/cls_flower.c
+++ b/net/sched/cls_flower.c
@@@ -384,34 -407,45 +384,45 @@@ static void fl_destroy_filter_work(stru
  }
  
  static void fl_hw_destroy_filter(struct tcf_proto *tp, struct cls_fl_filter *f,
 -				 bool rtnl_held, struct netlink_ext_ack *extack)
 +				 struct netlink_ext_ack *extack)
  {
 +	struct tc_cls_flower_offload cls_flower = {};
  	struct tcf_block *block = tp->chain->block;
 -	struct flow_cls_offload cls_flower = {};
 -
 -	if (!rtnl_held)
 -		rtnl_lock();
  
  	tc_cls_common_offload_init(&cls_flower.common, tp, f->flags, extack);
 -	cls_flower.command = FLOW_CLS_DESTROY;
 +	cls_flower.command = TC_CLSFLOWER_DESTROY;
  	cls_flower.cookie = (unsigned long) f;
  
++<<<<<<< HEAD
 +	tc_setup_cb_call(block, TC_SETUP_CLSFLOWER, &cls_flower, false);
 +	tcf_block_offload_dec(block, &f->flags);
++=======
+ 	tc_setup_cb_destroy(block, tp, TC_SETUP_CLSFLOWER, &cls_flower, false,
+ 			    &f->flags, &f->in_hw_count, true);
+ 
+ 	if (!rtnl_held)
+ 		rtnl_unlock();
++>>>>>>> a449a3e77a85 (net: sched: notify classifier on successful offload add/delete)
  }
  
  static int fl_hw_replace_filter(struct tcf_proto *tp,
 -				struct cls_fl_filter *f, bool rtnl_held,
 +				struct cls_fl_filter *f,
  				struct netlink_ext_ack *extack)
  {
++<<<<<<< HEAD
 +	struct tc_cls_flower_offload cls_flower = {};
++=======
++>>>>>>> a449a3e77a85 (net: sched: notify classifier on successful offload add/delete)
  	struct tcf_block *block = tp->chain->block;
 -	struct flow_cls_offload cls_flower = {};
  	bool skip_sw = tc_skip_sw(f->flags);
 -	int err = 0;
 -
 -	if (!rtnl_held)
 -		rtnl_lock();
 +	int err;
  
  	cls_flower.rule = flow_rule_alloc(tcf_exts_num_actions(&f->exts));
 -	if (!cls_flower.rule) {
 -		err = -ENOMEM;
 -		goto errout;
 -	}
 +	if (!cls_flower.rule)
 +		return -ENOMEM;
  
  	tc_cls_common_offload_init(&cls_flower.common, tp, f->flags, extack);
 -	cls_flower.command = FLOW_CLS_REPLACE;
 +	cls_flower.command = TC_CLSFLOWER_REPLACE;
  	cls_flower.cookie = (unsigned long) f;
  	cls_flower.rule->match.dissector = &f->mask->dissector;
  	cls_flower.rule->match.mask = &f->mask->key;
@@@ -421,37 -455,45 +432,45 @@@
  	err = tc_setup_flow_action(&cls_flower.rule->action, &f->exts);
  	if (err) {
  		kfree(cls_flower.rule);
 -		if (skip_sw)
 +		if (skip_sw) {
  			NL_SET_ERR_MSG_MOD(extack, "Failed to setup flow action");
 -		else
 -			err = 0;
 -		goto errout;
 +			return err;
 +		}
 +		return 0;
  	}
  
 -	err = tc_setup_cb_add(block, tp, TC_SETUP_CLSFLOWER, &cls_flower,
 -			      skip_sw, &f->flags, &f->in_hw_count, true);
 +	err = tc_setup_cb_call(block, TC_SETUP_CLSFLOWER, &cls_flower, skip_sw);
  	kfree(cls_flower.rule);
  
 -	if (err) {
 -		fl_hw_destroy_filter(tp, f, true, NULL);
 -		goto errout;
 +	if (err < 0) {
 +		fl_hw_destroy_filter(tp, f, NULL);
 +		return err;
 +	} else if (err > 0) {
 +		f->in_hw_count = err;
 +		tcf_block_offload_inc(block, &f->flags);
  	}
  
 -	if (skip_sw && !(f->flags & TCA_CLS_FLAGS_IN_HW)) {
 -		err = -EINVAL;
 -		goto errout;
 -	}
 +	if (skip_sw && !(f->flags & TCA_CLS_FLAGS_IN_HW))
 +		return -EINVAL;
  
++<<<<<<< HEAD
 +	return 0;
++=======
+ errout:
+ 	if (!rtnl_held)
+ 		rtnl_unlock();
+ 
+ 	return err;
++>>>>>>> a449a3e77a85 (net: sched: notify classifier on successful offload add/delete)
  }
  
 -static void fl_hw_update_stats(struct tcf_proto *tp, struct cls_fl_filter *f,
 -			       bool rtnl_held)
 +static void fl_hw_update_stats(struct tcf_proto *tp, struct cls_fl_filter *f)
  {
 +	struct tc_cls_flower_offload cls_flower = {};
  	struct tcf_block *block = tp->chain->block;
 -	struct flow_cls_offload cls_flower = {};
 -
 -	if (!rtnl_held)
 -		rtnl_lock();
  
  	tc_cls_common_offload_init(&cls_flower.common, tp, f->flags, NULL);
 -	cls_flower.command = FLOW_CLS_STATS;
 +	cls_flower.command = TC_CLSFLOWER_STATS;
  	cls_flower.cookie = (unsigned long) f;
  	cls_flower.classid = f->res.classid;
  
diff --git a/include/net/sch_generic.h b/include/net/sch_generic.h
index 13911b10ed82..8937e73e60a4 100644
--- a/include/net/sch_generic.h
+++ b/include/net/sch_generic.h
@@ -315,6 +315,10 @@ struct tcf_proto_ops {
 	int			(*reoffload)(struct tcf_proto *tp, bool add,
 					     tc_setup_cb_t *cb, void *cb_priv,
 					     struct netlink_ext_ack *extack);
+	void			(*hw_add)(struct tcf_proto *tp,
+					  void *type_data);
+	void			(*hw_del)(struct tcf_proto *tp,
+					  void *type_data);
 	void			(*bind_class)(void *, u32, unsigned long);
 	void *			(*tmplt_create)(struct net *net,
 						struct tcf_chain *chain,
* Unmerged path net/sched/cls_api.c
* Unmerged path net/sched/cls_flower.c
