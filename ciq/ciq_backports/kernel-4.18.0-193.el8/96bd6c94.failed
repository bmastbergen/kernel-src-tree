s390/qeth: add BQL support for IQD devices

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Julian Wiedmann <jwi@linux.ibm.com>
commit 96bd6c94bdf9de38b0fa0ec679fe40013f1c4576
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/96bd6c94.failed

Each TX buffer may contain multiple skbs. So just accumulate the sent
byte count in the buffer struct, and later use the same count when
completing the buffer.

	Signed-off-by: Julian Wiedmann <jwi@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 96bd6c94bdf9de38b0fa0ec679fe40013f1c4576)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/net/qeth_core_main.c
diff --cc drivers/s390/net/qeth_core_main.c
index a0c5702815af,4c7c7d320c9c..000000000000
--- a/drivers/s390/net/qeth_core_main.c
+++ b/drivers/s390/net/qeth_core_main.c
@@@ -1217,9 -1140,9 +1217,10 @@@ static void qeth_clear_output_buffer(st
  		buf->is_header[i] = 0;
  	}
  
 -	qeth_scrub_qdio_buffer(buf->buffer, queue->max_elements);
 +	qeth_scrub_qdio_buffer(buf->buffer,
 +			       QETH_MAX_BUFFER_ELEMENTS(queue->card));
  	buf->next_element_to_fill = 0;
+ 	buf->bytes = 0;
  	atomic_set(&buf->state, QETH_QDIO_BUF_EMPTY);
  }
  
@@@ -2784,14 -2665,16 +2785,27 @@@ int qeth_init_qdio_queues(struct qeth_c
  
  	/* outbound queue */
  	for (i = 0; i < card->qdio.no_out_queues; ++i) {
++<<<<<<< HEAD
 +		qdio_reset_buffers(card->qdio.out_qs[i]->qdio_bufs,
 +				   QDIO_MAX_BUFFERS_PER_Q);
 +		card->qdio.out_qs[i]->next_buf_to_fill = 0;
 +		card->qdio.out_qs[i]->do_pack = 0;
 +		atomic_set(&card->qdio.out_qs[i]->used_buffers, 0);
 +		atomic_set(&card->qdio.out_qs[i]->set_pci_flags_count, 0);
 +		atomic_set(&card->qdio.out_qs[i]->state,
 +			   QETH_OUT_Q_UNLOCKED);
++=======
+ 		struct qeth_qdio_out_q *queue = card->qdio.out_qs[i];
+ 
+ 		qdio_reset_buffers(queue->qdio_bufs, QDIO_MAX_BUFFERS_PER_Q);
+ 		queue->max_elements = QETH_MAX_BUFFER_ELEMENTS(card);
+ 		queue->next_buf_to_fill = 0;
+ 		queue->do_pack = 0;
+ 		atomic_set(&queue->used_buffers, 0);
+ 		atomic_set(&queue->set_pci_flags_count, 0);
+ 		atomic_set(&queue->state, QETH_OUT_Q_UNLOCKED);
+ 		netdev_tx_reset_queue(netdev_get_tx_queue(card->dev, i));
++>>>>>>> 96bd6c94bdf9 (s390/qeth: add BQL support for IQD devices)
  	}
  	return 0;
  }
@@@ -3946,15 -3792,35 +3960,40 @@@ static int qeth_do_send_packet_fast(str
  {
  	int index = queue->next_buf_to_fill;
  	struct qeth_qdio_out_buffer *buffer = queue->bufs[index];
++<<<<<<< HEAD
++=======
+ 	unsigned int bytes = qdisc_pkt_len(skb);
+ 	struct netdev_queue *txq;
+ 	bool stopped = false;
++>>>>>>> 96bd6c94bdf9 (s390/qeth: add BQL support for IQD devices)
  
 -	/* Just a sanity check, the wake/stop logic should ensure that we always
 -	 * get a free buffer.
 +	/*
 +	 * check if buffer is empty to make sure that we do not 'overtake'
 +	 * ourselves and try to fill a buffer that is already primed
  	 */
  	if (atomic_read(&buffer->state) != QETH_QDIO_BUF_EMPTY)
  		return -EBUSY;
++<<<<<<< HEAD
 +	qeth_fill_buffer(queue, buffer, skb, hdr, offset, hd_len);
++=======
+ 
+ 	txq = netdev_get_tx_queue(queue->card->dev, skb_get_queue_mapping(skb));
+ 
+ 	if (atomic_inc_return(&queue->used_buffers) >= QDIO_MAX_BUFFERS_PER_Q) {
+ 		/* If a TX completion happens right _here_ and misses to wake
+ 		 * the txq, then our re-check below will catch the race.
+ 		 */
+ 		QETH_TXQ_STAT_INC(queue, stopped);
+ 		netif_tx_stop_queue(txq);
+ 		stopped = true;
+ 	}
+ 
+ 	qeth_fill_buffer(queue, buffer, skb, hdr, offset, hd_len, stopped);
+ 	netdev_tx_sent_queue(txq, bytes);
+ 	buffer->bytes += bytes;
+ 
++>>>>>>> 96bd6c94bdf9 (s390/qeth: add BQL support for IQD devices)
  	qeth_flush_buffers(queue, index, 1);
 -
 -	if (stopped && !qeth_out_queue_is_full(queue))
 -		netif_tx_start_queue(txq);
  	return 0;
  }
  
@@@ -5336,6 -5145,107 +5375,110 @@@ out
  }
  EXPORT_SYMBOL_GPL(qeth_poll);
  
++<<<<<<< HEAD
++=======
+ static void qeth_iqd_tx_complete(struct qeth_qdio_out_q *queue,
+ 				 unsigned int bidx, bool error, int budget)
+ {
+ 	struct qeth_qdio_out_buffer *buffer = queue->bufs[bidx];
+ 	u8 sflags = buffer->buffer->element[15].sflags;
+ 	struct qeth_card *card = queue->card;
+ 
+ 	if (queue->bufstates && (queue->bufstates[bidx].flags &
+ 				 QDIO_OUTBUF_STATE_FLAG_PENDING)) {
+ 		WARN_ON_ONCE(card->options.cq != QETH_CQ_ENABLED);
+ 
+ 		if (atomic_cmpxchg(&buffer->state, QETH_QDIO_BUF_PRIMED,
+ 						   QETH_QDIO_BUF_PENDING) ==
+ 		    QETH_QDIO_BUF_PRIMED)
+ 			qeth_notify_skbs(queue, buffer, TX_NOTIFY_PENDING);
+ 
+ 		QETH_CARD_TEXT_(card, 5, "pel%u", bidx);
+ 
+ 		/* prepare the queue slot for re-use: */
+ 		qeth_scrub_qdio_buffer(buffer->buffer, queue->max_elements);
+ 		if (qeth_init_qdio_out_buf(queue, bidx)) {
+ 			QETH_CARD_TEXT(card, 2, "outofbuf");
+ 			qeth_schedule_recovery(card);
+ 		}
+ 
+ 		return;
+ 	}
+ 
+ 	if (card->options.cq == QETH_CQ_ENABLED)
+ 		qeth_notify_skbs(queue, buffer,
+ 				 qeth_compute_cq_notification(sflags, 0));
+ 	qeth_clear_output_buffer(queue, buffer, error, budget);
+ }
+ 
+ static int qeth_tx_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct qeth_qdio_out_q *queue = qeth_napi_to_out_queue(napi);
+ 	unsigned int queue_no = queue->queue_no;
+ 	struct qeth_card *card = queue->card;
+ 	struct net_device *dev = card->dev;
+ 	unsigned int work_done = 0;
+ 	struct netdev_queue *txq;
+ 
+ 	txq = netdev_get_tx_queue(dev, qeth_iqd_translate_txq(dev, queue_no));
+ 
+ 	while (1) {
+ 		unsigned int start, error, i;
+ 		unsigned int packets = 0;
+ 		unsigned int bytes = 0;
+ 		int completed;
+ 
+ 		if (qeth_out_queue_is_empty(queue)) {
+ 			napi_complete(napi);
+ 			return 0;
+ 		}
+ 
+ 		/* Give the CPU a breather: */
+ 		if (work_done >= QDIO_MAX_BUFFERS_PER_Q) {
+ 			QETH_TXQ_STAT_INC(queue, completion_yield);
+ 			if (napi_complete_done(napi, 0))
+ 				napi_schedule(napi);
+ 			return 0;
+ 		}
+ 
+ 		completed = qdio_inspect_queue(CARD_DDEV(card), queue_no, false,
+ 					       &start, &error);
+ 		if (completed <= 0) {
+ 			/* Ensure we see TX completion for pending work: */
+ 			if (napi_complete_done(napi, 0))
+ 				qeth_tx_arm_timer(queue);
+ 			return 0;
+ 		}
+ 
+ 		for (i = start; i < start + completed; i++) {
+ 			struct qeth_qdio_out_buffer *buffer;
+ 			unsigned int bidx = QDIO_BUFNR(i);
+ 
+ 			buffer = queue->bufs[bidx];
+ 			packets += skb_queue_len(&buffer->skb_list);
+ 			bytes += buffer->bytes;
+ 
+ 			qeth_handle_send_error(card, buffer, error);
+ 			qeth_iqd_tx_complete(queue, bidx, error, budget);
+ 			qeth_cleanup_handled_pending(queue, bidx, false);
+ 		}
+ 
+ 		netdev_tx_completed_queue(txq, packets, bytes);
+ 		atomic_sub(completed, &queue->used_buffers);
+ 		work_done += completed;
+ 
+ 		/* xmit may have observed the full-condition, but not yet
+ 		 * stopped the txq. In which case the code below won't trigger.
+ 		 * So before returning, xmit will re-check the txq's fill level
+ 		 * and wake it up if needed.
+ 		 */
+ 		if (netif_tx_queue_stopped(txq) &&
+ 		    !qeth_out_queue_is_full(queue))
+ 			netif_tx_wake_queue(txq);
+ 	}
+ }
+ 
++>>>>>>> 96bd6c94bdf9 (s390/qeth: add BQL support for IQD devices)
  static int qeth_setassparms_inspect_rc(struct qeth_ipa_cmd *cmd)
  {
  	if (!cmd->hdr.return_code)
diff --git a/drivers/s390/net/qeth_core.h b/drivers/s390/net/qeth_core.h
index dcbcb3b4d38f..f987f4f4432c 100644
--- a/drivers/s390/net/qeth_core.h
+++ b/drivers/s390/net/qeth_core.h
@@ -446,6 +446,7 @@ struct qeth_qdio_out_buffer {
 	struct qdio_buffer *buffer;
 	atomic_t state;
 	int next_element_to_fill;
+	unsigned int bytes;
 	struct sk_buff_head skb_list;
 	int is_header[QDIO_MAX_ELEMENTS_PER_BUFFER];
 
* Unmerged path drivers/s390/net/qeth_core_main.c
