sched/fair: Replace source_load() & target_load() with weighted_cpuload()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dietmar Eggemann <dietmar.eggemann@arm.com>
commit 1c1b8a7b03ef50f80f5d0c871ee261c04a6c967e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1c1b8a7b.failed

With LB_BIAS disabled, source_load() & target_load() return
weighted_cpuload(). Replace both with calls to weighted_cpuload().

The function to obtain the load index (sd->*_idx) for an sd,
get_sd_load_idx(), can be removed as well.

Finally, get rid of the sched feature LB_BIAS.

	Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Rik van Riel <riel@surriel.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Morten Rasmussen <morten.rasmussen@arm.com>
	Cc: Patrick Bellasi <patrick.bellasi@arm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Quentin Perret <quentin.perret@arm.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Valentin Schneider <valentin.schneider@arm.com>
	Cc: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lkml.kernel.org/r/20190527062116.11512-3-dietmar.eggemann@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1c1b8a7b03ef50f80f5d0c871ee261c04a6c967e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 555548f14de2,5b9691e5ea59..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -7933,17 -7911,14 +7860,20 @@@ static bool update_nohz_stats(struct r
   * update_sg_lb_stats - Update sched_group's statistics for load balancing.
   * @env: The load balancing environment.
   * @group: sched_group whose statistics are to be updated.
 + * @load_idx: Load index of sched_domain of this_cpu for load calc.
 + * @local_group: Does group contain this_cpu.
   * @sgs: variable to hold the statistics for this group.
 - * @sg_status: Holds flag indicating the status of the sched_group
 + * @overload: Indicate pullable load (e.g. >1 runnable task).
   */
  static inline void update_sg_lb_stats(struct lb_env *env,
 -				      struct sched_group *group,
 -				      struct sg_lb_stats *sgs,
 -				      int *sg_status)
 +			struct sched_group *group, int load_idx,
 +			int local_group, struct sg_lb_stats *sgs,
 +			bool *overload)
  {
++<<<<<<< HEAD
 +	unsigned long load;
++=======
++>>>>>>> 1c1b8a7b03ef (sched/fair: Replace source_load() & target_load() with weighted_cpuload())
  	int i, nr_running;
  
  	memset(sgs, 0, sizeof(*sgs));
* Unmerged path kernel/sched/fair.c
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 858589b83377..2410db5e9a35 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -39,7 +39,6 @@ SCHED_FEAT(WAKEUP_PREEMPTION, true)
 
 SCHED_FEAT(HRTICK, false)
 SCHED_FEAT(DOUBLE_TICK, false)
-SCHED_FEAT(LB_BIAS, false)
 
 /*
  * Decrement CPU capacity based on time not spent running tasks
