xfs: add kmem_alloc_io()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dave Chinner <dchinner@redhat.com>
commit f8f9ee479439c1be9e33c4404912a2a112c46200
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f8f9ee47.failed

Memory we use to submit for IO needs strict alignment to the
underlying driver contraints. Worst case, this is 512 bytes. Given
that all allocations for IO are always a power of 2 multiple of 512
bytes, the kernel heap provides natural alignment for objects of
these sizes and that suffices.

Until, of course, memory debugging of some kind is turned on (e.g.
red zones, poisoning, KASAN) and then the alignment of the heap
objects is thrown out the window. Then we get weird IO errors and
data corruption problems because drivers don't validate alignment
and do the wrong thing when passed unaligned memory buffers in bios.

TO fix this, introduce kmem_alloc_io(), which will guaranteeat least
512 byte alignment of buffers for IO, even if memory debugging
options are turned on. It is assumed that the minimum allocation
size will be 512 bytes, and that sizes will be power of 2 mulitples
of 512 bytes.

Use this everywhere we allocate buffers for IO.

This no longer fails with log recovery errors when KASAN is enabled
due to the brd driver not handling unaligned memory buffers:

# mkfs.xfs -f /dev/ram0 ; mount /dev/ram0 /mnt/test

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit f8f9ee479439c1be9e33c4404912a2a112c46200)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/kmem.c
#	fs/xfs/xfs_log.c
#	fs/xfs/xfs_log_recover.c
#	fs/xfs/xfs_trace.h
diff --cc fs/xfs/kmem.c
index fdd9d6ede25c,da031b93e182..000000000000
--- a/fs/xfs/kmem.c
+++ b/fs/xfs/kmem.c
@@@ -38,19 -43,8 +46,22 @@@ __kmem_vmalloc(size_t size, xfs_km_flag
  {
  	unsigned nofs_flag = 0;
  	void	*ptr;
- 	gfp_t	lflags;
+ 	gfp_t	lflags = kmem_flags_convert(flags);
  
++<<<<<<< HEAD
 +	ptr = kmem_alloc(size, flags | KM_MAYFAIL);
 +	if (ptr)
 +		return ptr;
 +
 +	/*
 +	 * __vmalloc() will allocate data pages and auxillary structures (e.g.
 +	 * pagetables) with GFP_KERNEL, yet we may be under GFP_NOFS context
 +	 * here. Hence we need to tell memory reclaim that we are in such a
 +	 * context via PF_MEMALLOC_NOFS to prevent memory reclaim re-entering
 +	 * the filesystem here and potentially deadlocking.
 +	 */
++=======
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  	if (flags & KM_NOFS)
  		nofs_flag = memalloc_nofs_save();
  
diff --cc fs/xfs/xfs_log.c
index 2466b0f5b6c4,3e0a105b6cc9..000000000000
--- a/fs/xfs/xfs_log.c
+++ b/fs/xfs/xfs_log.c
@@@ -1516,29 -1402,23 +1516,45 @@@ xlog_alloc_log
  	 * xlog_in_core_t in xfs_log_priv.h for details.
  	 */
  	ASSERT(log->l_iclog_size >= 4096);
++<<<<<<< HEAD
 +	for (i=0; i < log->l_iclog_bufs; i++) {
 +		*iclogp = kmem_zalloc(sizeof(xlog_in_core_t), KM_MAYFAIL);
 +		if (!*iclogp)
++=======
+ 	for (i = 0; i < log->l_iclog_bufs; i++) {
+ 		int align_mask = xfs_buftarg_dma_alignment(mp->m_logdev_targp);
+ 		size_t bvec_size = howmany(log->l_iclog_size, PAGE_SIZE) *
+ 				sizeof(struct bio_vec);
+ 
+ 		iclog = kmem_zalloc(sizeof(*iclog) + bvec_size, KM_MAYFAIL);
+ 		if (!iclog)
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  			goto out_free_iclog;
  
 -		*iclogp = iclog;
 +		iclog = *iclogp;
  		iclog->ic_prev = prev_iclog;
  		prev_iclog = iclog;
  
++<<<<<<< HEAD
 +		bp = xfs_buf_get_uncached(mp->m_logdev_targp,
 +					  BTOBB(log->l_iclog_size),
 +					  XBF_NO_IOACCT);
 +		if (!bp)
++=======
+ 		iclog->ic_data = kmem_alloc_io(log->l_iclog_size, align_mask,
+ 						KM_MAYFAIL);
+ 		if (!iclog->ic_data)
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  			goto out_free_iclog;
 +
 +		ASSERT(xfs_buf_islocked(bp));
 +		xfs_buf_unlock(bp);
 +
 +		/* use high priority wq for log I/O completion */
 +		bp->b_ioend_wq = mp->m_log_workqueue;
 +		bp->b_iodone = xlog_iodone;
 +		iclog->ic_bp = bp;
 +		iclog->ic_data = bp->b_addr;
  #ifdef DEBUG
  		log->l_iclog_bak[i] = &iclog->ic_header;
  #endif
diff --cc fs/xfs/xfs_log_recover.c
index 9329f5adbfbe,f05c6c99c4f3..000000000000
--- a/fs/xfs/xfs_log_recover.c
+++ b/fs/xfs/xfs_log_recover.c
@@@ -101,7 -97,7 +101,11 @@@ xlog_get_bp
  	struct xlog	*log,
  	int		nbblks)
  {
++<<<<<<< HEAD
 +	struct xfs_buf	*bp;
++=======
+ 	int align_mask = xfs_buftarg_dma_alignment(log->l_targ);
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  
  	/*
  	 * Pass log block 0 since we don't have an addr yet, buffer will be
@@@ -133,18 -127,7 +137,22 @@@
  	if (nbblks > 1 && log->l_sectBBsize > 1)
  		nbblks += log->l_sectBBsize;
  	nbblks = round_up(nbblks, log->l_sectBBsize);
++<<<<<<< HEAD
 +
 +	bp = xfs_buf_get_uncached(log->l_mp->m_logdev_targp, nbblks, 0);
 +	if (bp)
 +		xfs_buf_unlock(bp);
 +	return bp;
 +}
 +
 +STATIC void
 +xlog_put_bp(
 +	xfs_buf_t	*bp)
 +{
 +	xfs_buf_free(bp);
++=======
+ 	return kmem_alloc_io(BBTOB(nbblks), align_mask, KM_MAYFAIL);
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  }
  
  /*
diff --cc fs/xfs/xfs_trace.h
index a8c32206e147,eaae275ed430..000000000000
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@@ -3360,8 -3365,250 +3360,252 @@@ DEFINE_TRANS_EVENT(xfs_trans_dup)
  DEFINE_TRANS_EVENT(xfs_trans_free);
  DEFINE_TRANS_EVENT(xfs_trans_roll);
  DEFINE_TRANS_EVENT(xfs_trans_add_item);
 -DEFINE_TRANS_EVENT(xfs_trans_commit_items);
  DEFINE_TRANS_EVENT(xfs_trans_free_items);
  
++<<<<<<< HEAD
++=======
+ TRACE_EVENT(xfs_iunlink_update_bucket,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, unsigned int bucket,
+ 		 xfs_agino_t old_ptr, xfs_agino_t new_ptr),
+ 	TP_ARGS(mp, agno, bucket, old_ptr, new_ptr),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(unsigned int, bucket)
+ 		__field(xfs_agino_t, old_ptr)
+ 		__field(xfs_agino_t, new_ptr)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->bucket = bucket;
+ 		__entry->old_ptr = old_ptr;
+ 		__entry->new_ptr = new_ptr;
+ 	),
+ 	TP_printk("dev %d:%d agno %u bucket %u old 0x%x new 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno,
+ 		  __entry->bucket,
+ 		  __entry->old_ptr,
+ 		  __entry->new_ptr)
+ );
+ 
+ TRACE_EVENT(xfs_iunlink_update_dinode,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, xfs_agino_t agino,
+ 		 xfs_agino_t old_ptr, xfs_agino_t new_ptr),
+ 	TP_ARGS(mp, agno, agino, old_ptr, new_ptr),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, agino)
+ 		__field(xfs_agino_t, old_ptr)
+ 		__field(xfs_agino_t, new_ptr)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->agino = agino;
+ 		__entry->old_ptr = old_ptr;
+ 		__entry->new_ptr = new_ptr;
+ 	),
+ 	TP_printk("dev %d:%d agno %u agino 0x%x old 0x%x new 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno,
+ 		  __entry->agino,
+ 		  __entry->old_ptr,
+ 		  __entry->new_ptr)
+ );
+ 
+ DECLARE_EVENT_CLASS(xfs_ag_inode_class,
+ 	TP_PROTO(struct xfs_inode *ip),
+ 	TP_ARGS(ip),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, agino)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = VFS_I(ip)->i_sb->s_dev;
+ 		__entry->agno = XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino);
+ 		__entry->agino = XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino);
+ 	),
+ 	TP_printk("dev %d:%d agno %u agino %u",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno, __entry->agino)
+ )
+ 
+ #define DEFINE_AGINODE_EVENT(name) \
+ DEFINE_EVENT(xfs_ag_inode_class, name, \
+ 	TP_PROTO(struct xfs_inode *ip), \
+ 	TP_ARGS(ip))
+ DEFINE_AGINODE_EVENT(xfs_iunlink);
+ DEFINE_AGINODE_EVENT(xfs_iunlink_remove);
+ DEFINE_AG_EVENT(xfs_iunlink_map_prev_fallback);
+ 
+ DECLARE_EVENT_CLASS(xfs_fs_corrupt_class,
+ 	TP_PROTO(struct xfs_mount *mp, unsigned int flags),
+ 	TP_ARGS(mp, flags),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(unsigned int, flags)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->flags = flags;
+ 	),
+ 	TP_printk("dev %d:%d flags 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->flags)
+ );
+ #define DEFINE_FS_CORRUPT_EVENT(name)	\
+ DEFINE_EVENT(xfs_fs_corrupt_class, name,	\
+ 	TP_PROTO(struct xfs_mount *mp, unsigned int flags), \
+ 	TP_ARGS(mp, flags))
+ DEFINE_FS_CORRUPT_EVENT(xfs_fs_mark_sick);
+ DEFINE_FS_CORRUPT_EVENT(xfs_fs_mark_healthy);
+ DEFINE_FS_CORRUPT_EVENT(xfs_fs_unfixed_corruption);
+ DEFINE_FS_CORRUPT_EVENT(xfs_rt_mark_sick);
+ DEFINE_FS_CORRUPT_EVENT(xfs_rt_mark_healthy);
+ DEFINE_FS_CORRUPT_EVENT(xfs_rt_unfixed_corruption);
+ 
+ DECLARE_EVENT_CLASS(xfs_ag_corrupt_class,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, unsigned int flags),
+ 	TP_ARGS(mp, agno, flags),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(unsigned int, flags)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->flags = flags;
+ 	),
+ 	TP_printk("dev %d:%d agno %u flags 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno, __entry->flags)
+ );
+ #define DEFINE_AG_CORRUPT_EVENT(name)	\
+ DEFINE_EVENT(xfs_ag_corrupt_class, name,	\
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, \
+ 		 unsigned int flags), \
+ 	TP_ARGS(mp, agno, flags))
+ DEFINE_AG_CORRUPT_EVENT(xfs_ag_mark_sick);
+ DEFINE_AG_CORRUPT_EVENT(xfs_ag_mark_healthy);
+ DEFINE_AG_CORRUPT_EVENT(xfs_ag_unfixed_corruption);
+ 
+ DECLARE_EVENT_CLASS(xfs_inode_corrupt_class,
+ 	TP_PROTO(struct xfs_inode *ip, unsigned int flags),
+ 	TP_ARGS(ip, flags),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_ino_t, ino)
+ 		__field(unsigned int, flags)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = ip->i_mount->m_super->s_dev;
+ 		__entry->ino = ip->i_ino;
+ 		__entry->flags = flags;
+ 	),
+ 	TP_printk("dev %d:%d ino 0x%llx flags 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->ino, __entry->flags)
+ );
+ #define DEFINE_INODE_CORRUPT_EVENT(name)	\
+ DEFINE_EVENT(xfs_inode_corrupt_class, name,	\
+ 	TP_PROTO(struct xfs_inode *ip, unsigned int flags), \
+ 	TP_ARGS(ip, flags))
+ DEFINE_INODE_CORRUPT_EVENT(xfs_inode_mark_sick);
+ DEFINE_INODE_CORRUPT_EVENT(xfs_inode_mark_healthy);
+ 
+ TRACE_EVENT(xfs_iwalk_ag,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno,
+ 		 xfs_agino_t startino),
+ 	TP_ARGS(mp, agno, startino),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, startino)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->startino = startino;
+ 	),
+ 	TP_printk("dev %d:%d agno %d startino %u",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->agno,
+ 		  __entry->startino)
+ )
+ 
+ TRACE_EVENT(xfs_iwalk_ag_rec,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno,
+ 		 struct xfs_inobt_rec_incore *irec),
+ 	TP_ARGS(mp, agno, irec),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, startino)
+ 		__field(uint64_t, freemask)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->startino = irec->ir_startino;
+ 		__entry->freemask = irec->ir_free;
+ 	),
+ 	TP_printk("dev %d:%d agno %d startino %u freemask 0x%llx",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->agno,
+ 		  __entry->startino, __entry->freemask)
+ )
+ 
+ TRACE_EVENT(xfs_pwork_init,
+ 	TP_PROTO(struct xfs_mount *mp, unsigned int nr_threads, pid_t pid),
+ 	TP_ARGS(mp, nr_threads, pid),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(unsigned int, nr_threads)
+ 		__field(pid_t, pid)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->nr_threads = nr_threads;
+ 		__entry->pid = pid;
+ 	),
+ 	TP_printk("dev %d:%d nr_threads %u pid %u",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->nr_threads, __entry->pid)
+ )
+ 
+ DECLARE_EVENT_CLASS(xfs_kmem_class,
+ 	TP_PROTO(ssize_t size, int flags, unsigned long caller_ip),
+ 	TP_ARGS(size, flags, caller_ip),
+ 	TP_STRUCT__entry(
+ 		__field(ssize_t, size)
+ 		__field(int, flags)
+ 		__field(unsigned long, caller_ip)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->size = size;
+ 		__entry->flags = flags;
+ 		__entry->caller_ip = caller_ip;
+ 	),
+ 	TP_printk("size %zd flags 0x%x caller %pS",
+ 		  __entry->size,
+ 		  __entry->flags,
+ 		  (char *)__entry->caller_ip)
+ )
+ 
+ #define DEFINE_KMEM_EVENT(name) \
+ DEFINE_EVENT(xfs_kmem_class, name, \
+ 	TP_PROTO(ssize_t size, int flags, unsigned long caller_ip), \
+ 	TP_ARGS(size, flags, caller_ip))
+ DEFINE_KMEM_EVENT(kmem_alloc);
+ DEFINE_KMEM_EVENT(kmem_alloc_io);
+ DEFINE_KMEM_EVENT(kmem_alloc_large);
+ DEFINE_KMEM_EVENT(kmem_realloc);
+ DEFINE_KMEM_EVENT(kmem_zone_alloc);
+ 
++>>>>>>> f8f9ee479439 (xfs: add kmem_alloc_io())
  #endif /* _TRACE_XFS_H */
  
  #undef TRACE_INCLUDE_PATH
* Unmerged path fs/xfs/kmem.c
diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 267655acd426..7e2605341c16 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -59,6 +59,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 }
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
+extern void *kmem_alloc_io(size_t size, int align_mask, xfs_km_flags_t flags);
 extern void *kmem_alloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, xfs_km_flags_t);
 static inline void  kmem_free(const void *ptr)
diff --git a/fs/xfs/xfs_buf.c b/fs/xfs/xfs_buf.c
index ade6ec28e1c9..d62798319f73 100644
--- a/fs/xfs/xfs_buf.c
+++ b/fs/xfs/xfs_buf.c
@@ -368,7 +368,8 @@ xfs_buf_allocate_memory(
 	 */
 	size = BBTOB(bp->b_length);
 	if (size < PAGE_SIZE) {
-		bp->b_addr = kmem_alloc(size, KM_NOFS);
+		int align_mask = xfs_buftarg_dma_alignment(bp->b_target);
+		bp->b_addr = kmem_alloc_io(size, align_mask, KM_NOFS);
 		if (!bp->b_addr) {
 			/* low memory - use alloc_page loop instead */
 			goto use_alloc_page;
@@ -383,7 +384,7 @@ xfs_buf_allocate_memory(
 		}
 		bp->b_offset = offset_in_page(bp->b_addr);
 		bp->b_pages = bp->b_page_array;
-		bp->b_pages[0] = virt_to_page(bp->b_addr);
+		bp->b_pages[0] = kmem_to_page(bp->b_addr);
 		bp->b_page_count = 1;
 		bp->b_flags |= _XBF_KMEM;
 		return 0;
* Unmerged path fs/xfs/xfs_log.c
* Unmerged path fs/xfs/xfs_log_recover.c
* Unmerged path fs/xfs/xfs_trace.h
