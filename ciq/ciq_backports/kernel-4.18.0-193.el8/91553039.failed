Documentation: Kill all references to mmiowb()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Will Deacon <will.deacon@arm.com>
commit 915530396c788d75c40f200edd67b56ac363c728
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/91553039.failed

The guarantees provided by mmiowb() are now provided implicitly by
spin_unlock(), so remove all references to this most confusing of
barriers from our Documentation.

Good riddance.

	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 915530396c788d75c40f200edd67b56ac363c728)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/memory-barriers.txt
diff --cc Documentation/memory-barriers.txt
index 7068b910f7ec,3522f0cc772f..000000000000
--- a/Documentation/memory-barriers.txt
+++ b/Documentation/memory-barriers.txt
@@@ -2532,17 -2447,10 +2448,24 @@@ the device to malfunction
  
  Inside of the Linux kernel, I/O should be done through the appropriate accessor
  routines - such as inb() or writel() - which know how to make such accesses
++<<<<<<< HEAD
 +appropriately sequential.  Whilst this, for the most part, renders the explicit
 +use of memory barriers unnecessary, there are a couple of situations where they
 +might be needed:
 +
 + (1) On some systems, I/O stores are not strongly ordered across all CPUs, and
 +     so for _all_ general drivers locks should be used and mmiowb() must be
 +     issued prior to unlocking the critical section.
 +
 + (2) If the accessor functions are used to refer to an I/O memory window with
 +     relaxed memory access properties, then _mandatory_ memory barriers are
 +     required to enforce ordering.
++=======
+ appropriately sequential.  While this, for the most part, renders the explicit
+ use of memory barriers unnecessary, if the accessor functions are used to refer
+ to an I/O memory window with relaxed memory access properties, then _mandatory_
+ memory barriers are required to enforce ordering.
++>>>>>>> 915530396c78 (Documentation: Kill all references to mmiowb())
  
  See Documentation/driver-api/device-io.rst for more information.
  
diff --git a/Documentation/driver-api/device-io.rst b/Documentation/driver-api/device-io.rst
index b00b23903078..0e389378f71d 100644
--- a/Documentation/driver-api/device-io.rst
+++ b/Documentation/driver-api/device-io.rst
@@ -103,51 +103,6 @@ continuing execution::
         ha->flags.ints_enabled = 0;
     }
 
-In addition to write posting, on some large multiprocessing systems
-(e.g. SGI Challenge, Origin and Altix machines) posted writes won't be
-strongly ordered coming from different CPUs. Thus it's important to
-properly protect parts of your driver that do memory-mapped writes with
-locks and use the :c:func:`mmiowb()` to make sure they arrive in the
-order intended. Issuing a regular readX() will also ensure write ordering,
-but should only be used when the 
-driver has to be sure that the write has actually arrived at the device
-(not that it's simply ordered with respect to other writes), since a
-full readX() is a relatively expensive operation.
-
-Generally, one should use :c:func:`mmiowb()` prior to releasing a spinlock
-that protects regions using :c:func:`writeb()` or similar functions that
-aren't surrounded by readb() calls, which will ensure ordering
-and flushing. The following pseudocode illustrates what might occur if
-write ordering isn't guaranteed via :c:func:`mmiowb()` or one of the
-readX() functions::
-
-    CPU A:  spin_lock_irqsave(&dev_lock, flags)
-    CPU A:  ...
-    CPU A:  writel(newval, ring_ptr);
-    CPU A:  spin_unlock_irqrestore(&dev_lock, flags)
-            ...
-    CPU B:  spin_lock_irqsave(&dev_lock, flags)
-    CPU B:  writel(newval2, ring_ptr);
-    CPU B:  ...
-    CPU B:  spin_unlock_irqrestore(&dev_lock, flags)
-
-In the case above, newval2 could be written to ring_ptr before newval.
-Fixing it is easy though::
-
-    CPU A:  spin_lock_irqsave(&dev_lock, flags)
-    CPU A:  ...
-    CPU A:  writel(newval, ring_ptr);
-    CPU A:  mmiowb(); /* ensure no other writes beat us to the device */
-    CPU A:  spin_unlock_irqrestore(&dev_lock, flags)
-            ...
-    CPU B:  spin_lock_irqsave(&dev_lock, flags)
-    CPU B:  writel(newval2, ring_ptr);
-    CPU B:  ...
-    CPU B:  mmiowb();
-    CPU B:  spin_unlock_irqrestore(&dev_lock, flags)
-
-See tg3.c for a real world example of how to use :c:func:`mmiowb()`
-
 PCI ordering rules also guarantee that PIO read responses arrive after any
 outstanding DMA writes from that bus, since for some devices the result of
 a readb() call may signal to the driver that a DMA transaction is
diff --git a/Documentation/driver-api/pci/p2pdma.rst b/Documentation/driver-api/pci/p2pdma.rst
index 6d85b5a2598d..44deb52beeb4 100644
--- a/Documentation/driver-api/pci/p2pdma.rst
+++ b/Documentation/driver-api/pci/p2pdma.rst
@@ -132,10 +132,6 @@ precludes passing these pages to userspace.
 P2P memory is also technically IO memory but should never have any side
 effects behind it. Thus, the order of loads and stores should not be important
 and ioreadX(), iowriteX() and friends should not be necessary.
-However, as the memory is not cache coherent, if access ever needs to
-be protected by a spinlock then :c:func:`mmiowb()` must be used before
-unlocking the lock. (See ACQUIRES VS I/O ACCESSES in
-Documentation/memory-barriers.txt)
 
 
 P2P DMA Support Library
* Unmerged path Documentation/memory-barriers.txt
