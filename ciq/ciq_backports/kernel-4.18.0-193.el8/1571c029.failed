dax: Fix xarray entry association for mixed mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jan Kara <jack@suse.cz>
commit 1571c029a2ff289683ddb0a32253850363bcb8a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1571c029.failed

When inserting entry into xarray, we store mapping and index in
corresponding struct pages for memory error handling. When it happened
that one process was mapping file at PMD granularity while another
process at PTE granularity, we could wrongly deassociate PMD range and
then reassociate PTE range leaving the rest of struct pages in PMD range
without mapping information which could later cause missed notifications
about memory errors. Fix the problem by calling the association /
deassociation code if and only if we are really going to update the
xarray (deassociating and associating zero or empty entries is just
no-op so there's no reason to complicate the code with trying to avoid
the calls for these cases).

	Cc: <stable@vger.kernel.org>
Fixes: d2c997c0f145 ("fs, dax: use page->mapping to warn if truncate...")
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 1571c029a2ff289683ddb0a32253850363bcb8a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 7752d3866711,9fd908f3df32..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -879,19 -720,19 +879,24 @@@ static void *dax_insert_entry(struct ad
  		/* we are replacing a zero page with block mapping */
  		if (dax_is_pmd_entry(entry))
  			unmap_mapping_pages(mapping, index & ~PG_PMD_COLOUR,
 -					PG_PMD_NR, false);
 +							PG_PMD_NR, false);
  		else /* pte entry */
 -			unmap_mapping_pages(mapping, index, 1, false);
 +			unmap_mapping_pages(mapping, vmf->pgoff, 1, false);
  	}
  
++<<<<<<< HEAD
 +	xa_lock_irq(pages);
 +	new_entry = dax_make_locked(pfn, flags);
 +	if (dax_entry_size(entry) != dax_entry_size(new_entry)) {
++=======
+ 	xas_reset(xas);
+ 	xas_lock_irq(xas);
+ 	if (dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {
+ 		void *old;
+ 
++>>>>>>> 1571c029a2ff (dax: Fix xarray entry association for mixed mappings)
  		dax_disassociate_entry(entry, mapping, false);
  		dax_associate_entry(new_entry, mapping, vmf->vma, vmf->address);
- 	}
- 
- 	if (dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {
  		/*
  		 * Only swap our new entry into the page cache if the current
  		 * entry is a zero page or an empty entry.  If a normal PTE or
@@@ -900,15 -741,12 +905,21 @@@
  		 * existing entry is a PMD, we will just leave the PMD in the
  		 * tree and dirty it if necessary.
  		 */
++<<<<<<< HEAD
 +		struct radix_tree_node *node;
 +		void **slot;
 +		void *ret;
 +
 +		ret = __radix_tree_lookup(pages, index, &node, &slot);
 +		WARN_ON_ONCE(ret != entry);
 +		__radix_tree_replace(pages, node, slot,
 +				     new_entry, NULL);
++=======
+ 		old = dax_lock_entry(xas, new_entry);
+ 		WARN_ON_ONCE(old != xa_mk_value(xa_to_value(entry) |
+ 					DAX_LOCKED));
++>>>>>>> 1571c029a2ff (dax: Fix xarray entry association for mixed mappings)
  		entry = new_entry;
 -	} else {
 -		xas_load(xas);	/* Walk the xa_state */
  	}
  
  	if (dirty)
* Unmerged path fs/dax.c
