powerpc/shared: Use static key to detect shared processor

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Srikar Dronamraju <srikar@linux.vnet.ibm.com>
commit 656c21d6af5d9279dd7b51ca7a4a71008127044b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/656c21d6.failed

With the static key shared processor available, is_shared_processor()
can return without having to query the lppaca structure.

	Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Acked-by: Phil Auld <pauld@redhat.com>
	Acked-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20191213035036.6913-2-mpe@ellerman.id.au
(cherry picked from commit 656c21d6af5d9279dd7b51ca7a4a71008127044b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/spinlock.h
diff --cc arch/powerpc/include/asm/spinlock.h
index 2122adbd7c9b,1b55fc08f853..000000000000
--- a/arch/powerpc/include/asm/spinlock.h
+++ b/arch/powerpc/include/asm/spinlock.h
@@@ -121,18 -103,40 +121,46 @@@ static inline int arch_spin_trylock(arc
  
  #if defined(CONFIG_PPC_SPLPAR)
  /* We only yield to the hypervisor if we are in shared processor mode */
 -void splpar_spin_yield(arch_spinlock_t *lock);
 -void splpar_rw_yield(arch_rwlock_t *lock);
 +#define SHARED_PROCESSOR (lppaca_shared_proc(local_paca->lppaca_ptr))
 +extern void __spin_yield(arch_spinlock_t *lock);
 +extern void __rw_yield(arch_rwlock_t *lock);
  #else /* SPLPAR */
 -static inline void splpar_spin_yield(arch_spinlock_t *lock) {};
 -static inline void splpar_rw_yield(arch_rwlock_t *lock) {};
 +#define __spin_yield(x)	barrier()
 +#define __rw_yield(x)	barrier()
 +#define SHARED_PROCESSOR	0
  #endif
  
++<<<<<<< HEAD
++=======
+ static inline bool is_shared_processor(void)
+ {
+ #ifdef CONFIG_PPC_SPLPAR
+ 	return static_branch_unlikely(&shared_processor);
+ #else
+ 	return false;
+ #endif
+ }
+ 
+ static inline void spin_yield(arch_spinlock_t *lock)
+ {
+ 	if (is_shared_processor())
+ 		splpar_spin_yield(lock);
+ 	else
+ 		barrier();
+ }
+ 
+ static inline void rw_yield(arch_rwlock_t *lock)
+ {
+ 	if (is_shared_processor())
+ 		splpar_rw_yield(lock);
+ 	else
+ 		barrier();
+ }
+ 
++>>>>>>> 656c21d6af5d (powerpc/shared: Use static key to detect shared processor)
  static inline void arch_spin_lock(arch_spinlock_t *lock)
  {
 +	CLEAR_IO_SYNC;
  	while (1) {
  		if (likely(__arch_spin_trylock(lock) == 0))
  			break;
* Unmerged path arch/powerpc/include/asm/spinlock.h
