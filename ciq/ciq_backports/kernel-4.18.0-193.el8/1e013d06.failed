arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Will Deacon <will.deacon@arm.com>
commit 1e013d06120cbf67e771848fc5d98174c33b078a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1e013d06.failed

Open-coding the pointer-auth HWCAPs is a mess and can be avoided by
reusing the multi-cap logic from the CPU errata framework.

Move the multi_entry_cap_matches code to cpufeature.h and reuse it for
the pointer auth HWCAPs.

	Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 1e013d06120cbf67e771848fc5d98174c33b078a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/cpu_errata.c
#	arch/arm64/kernel/cpufeature.c
diff --cc arch/arm64/kernel/cpu_errata.c
index feeb8962a8d8,ff2fda3a98e1..000000000000
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@@ -539,99 -507,26 +539,103 @@@ static const struct midr_range arm64_ss
  	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
  	CAP_MIDR_RANGE_LIST(midr_list)
  
++<<<<<<< HEAD
 +/* Track overall mitigation state. We are only mitigated if all cores are ok */
 +static bool __hardenbp_enab = true;
 +static bool __spectrev2_safe = true;
 +
 +/*
 + * Generic helper for handling capabilties with multiple (match,enable) pairs
 + * of call backs, sharing the same capability bit.
 + * Iterate over each entry to see if at least one matches.
 + */
 +static bool __maybe_unused
 +multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry, int scope)
 +{
 +	const struct arm64_cpu_capabilities *caps;
 +
 +	for (caps = entry->match_list; caps->matches; caps++)
 +		if (caps->matches(caps, scope))
 +			return true;
 +
 +	return false;
 +}
 +
 +/*
 + * Take appropriate action for all matching entries in the shared capability
 + * entry.
 + */
 +static void __maybe_unused
 +multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
 +{
 +	const struct arm64_cpu_capabilities *caps;
 +
 +	for (caps = entry->match_list; caps->matches; caps++)
 +		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
 +		    caps->cpu_enable)
 +			caps->cpu_enable(caps);
 +}
++=======
+ #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
++>>>>>>> 1e013d06120c (arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches)
  
  /*
 - * List of CPUs where we need to issue a psci call to
 - * harden the branch predictor.
 + * List of CPUs that do not need any Spectre-v2 mitigation at all.
   */
 -static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 -	MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
 -	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
 -	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
 -	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 -	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 -	MIDR_ALL_VERSIONS(MIDR_NVIDIA_DENVER),
 -	{},
 +static const struct midr_range spectre_v2_safe_list[] = {
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 +	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
 +	{ /* sentinel */ }
  };
  
 -#endif
 +/*
 + * Track overall bp hardening for all heterogeneous cores in the machine.
 + * We are only considered "safe" if all booted cores are known safe.
 + */
 +static bool __maybe_unused
 +check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 +{
 +	int need_wa;
 +
 +	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 +
 +	/* If the CPU has CSV2 set, we're safe */
 +	if (cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64PFR0_EL1),
 +						 ID_AA64PFR0_CSV2_SHIFT))
 +		return false;
 +
 +	/* Alternatively, we have a list of unaffected CPUs */
 +	if (is_midr_in_range_list(read_cpuid_id(), spectre_v2_safe_list))
 +		return false;
 +
 +	/* Fallback to firmware detection */
 +	need_wa = detect_harden_bp_fw();
 +	if (!need_wa)
 +		return false;
 +
 +	__spectrev2_safe = false;
 +
 +	if (!IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR)) {
 +		pr_warn_once("spectrev2 mitigation disabled by kernel configuration\n");
 +		__hardenbp_enab = false;
 +		return false;
 +	}
 +
 +	/* forced off */
 +	if (__nospectre_v2 || cpu_mitigations_off()) {
 +		pr_info_once("spectrev2 mitigation disabled by command line option\n");
 +		__hardenbp_enab = false;
 +		return false;
 +	}
 +
 +	if (need_wa < 0) {
 +		pr_warn_once("ARM_SMCCC_ARCH_WORKAROUND_1 missing from firmware\n");
 +		__hardenbp_enab = false;
 +	}
 +
 +	return (need_wa > 0);
 +}
  
  #ifdef CONFIG_HARDEN_EL2_VECTORS
  
diff --cc arch/arm64/kernel/cpufeature.c
index ea14666781d7,7ea0b2f20262..000000000000
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@@ -1162,6 -1168,36 +1162,39 @@@ static void cpu_enable_ssbs(const struc
  }
  #endif /* CONFIG_ARM64_SSBD */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_ARM64_PAN
+ static void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
+ {
+ 	/*
+ 	 * We modify PSTATE. This won't work from irq context as the PSTATE
+ 	 * is discarded once we return from the exception.
+ 	 */
+ 	WARN_ON_ONCE(in_interrupt());
+ 
+ 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_SPAN, 0);
+ 	asm(SET_PSTATE_PAN(1));
+ }
+ #endif /* CONFIG_ARM64_PAN */
+ 
+ #ifdef CONFIG_ARM64_RAS_EXTN
+ static void cpu_clear_disr(const struct arm64_cpu_capabilities *__unused)
+ {
+ 	/* Firmware may have left a deferred SError in this register. */
+ 	write_sysreg_s(0, SYS_DISR_EL1);
+ }
+ #endif /* CONFIG_ARM64_RAS_EXTN */
+ 
+ #ifdef CONFIG_ARM64_PTR_AUTH
+ static void cpu_enable_address_auth(struct arm64_cpu_capabilities const *cap)
+ {
+ 	sysreg_clear_set(sctlr_el1, 0, SCTLR_ELx_ENIA | SCTLR_ELx_ENIB |
+ 				       SCTLR_ELx_ENDA | SCTLR_ELx_ENDB);
+ }
+ #endif /* CONFIG_ARM64_PTR_AUTH */
+ 
++>>>>>>> 1e013d06120c (arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches)
  static const struct arm64_cpu_capabilities arm64_features[] = {
  	{
  		.desc = "GIC system register CPU interface",
@@@ -1431,6 -1561,10 +1503,13 @@@ static const struct arm64_cpu_capabilit
  	HWCAP_CAP(SYS_ID_AA64PFR0_EL1, ID_AA64PFR0_SVE_SHIFT, FTR_UNSIGNED, ID_AA64PFR0_SVE, CAP_HWCAP, HWCAP_SVE),
  #endif
  	HWCAP_CAP(SYS_ID_AA64PFR1_EL1, ID_AA64PFR1_SSBS_SHIFT, FTR_UNSIGNED, ID_AA64PFR1_SSBS_PSTATE_INSNS, CAP_HWCAP, HWCAP_SSBS),
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_ARM64_PTR_AUTH
+ 	HWCAP_MULTI_CAP(ptr_auth_hwcap_addr_matches, CAP_HWCAP, HWCAP_PACA),
+ 	HWCAP_MULTI_CAP(ptr_auth_hwcap_gen_matches, CAP_HWCAP, HWCAP_PACG),
+ #endif
++>>>>>>> 1e013d06120c (arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches)
  	{},
  };
  
diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7c0459c391c4..150f47958d55 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -321,19 +321,20 @@ struct arm64_cpu_capabilities {
 			bool sign;
 			unsigned long hwcap;
 		};
-		/*
-		 * A list of "matches/cpu_enable" pair for the same
-		 * "capability" of the same "type" as described by the parent.
-		 * Only matches(), cpu_enable() and fields relevant to these
-		 * methods are significant in the list. The cpu_enable is
-		 * invoked only if the corresponding entry "matches()".
-		 * However, if a cpu_enable() method is associated
-		 * with multiple matches(), care should be taken that either
-		 * the match criteria are mutually exclusive, or that the
-		 * method is robust against being called multiple times.
-		 */
-		const struct arm64_cpu_capabilities *match_list;
 	};
+
+	/*
+	 * An optional list of "matches/cpu_enable" pair for the same
+	 * "capability" of the same "type" as described by the parent.
+	 * Only matches(), cpu_enable() and fields relevant to these
+	 * methods are significant in the list. The cpu_enable is
+	 * invoked only if the corresponding entry "matches()".
+	 * However, if a cpu_enable() method is associated
+	 * with multiple matches(), care should be taken that either
+	 * the match criteria are mutually exclusive, or that the
+	 * method is robust against being called multiple times.
+	 */
+	const struct arm64_cpu_capabilities *match_list;
 };
 
 static inline int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
@@ -353,6 +354,39 @@ cpucap_late_cpu_permitted(const struct arm64_cpu_capabilities *cap)
 	return !!(cap->type & ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU);
 }
 
+/*
+ * Generic helper for handling capabilties with multiple (match,enable) pairs
+ * of call backs, sharing the same capability bit.
+ * Iterate over each entry to see if at least one matches.
+ */
+static inline bool
+cpucap_multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
+			       int scope)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, scope))
+			return true;
+
+	return false;
+}
+
+/*
+ * Take appropriate action for all matching entries in the shared capability
+ * entry.
+ */
+static inline void
+cpucap_multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
+		    caps->cpu_enable)
+			caps->cpu_enable(caps);
+}
+
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;
@@ -473,7 +507,6 @@ static inline bool id_aa64pfr0_sve(u64 pfr0)
 void __init setup_cpu_features(void);
 void check_local_cpu_capabilities(void);
 
-
 u64 read_sanitised_ftr_reg(u32 id);
 
 static inline bool cpu_supports_mixed_endian_el0(void)
* Unmerged path arch/arm64/kernel/cpu_errata.c
* Unmerged path arch/arm64/kernel/cpufeature.c
