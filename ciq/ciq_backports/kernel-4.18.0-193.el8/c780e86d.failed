blktrace: Protect q->blk_trace with RCU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jan Kara <jack@suse.cz>
commit c780e86dd48ef6467a1146cf7d0fe1e05a635039
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/c780e86d.failed

KASAN is reporting that __blk_add_trace() has a use-after-free issue
when accessing q->blk_trace. Indeed the switching of block tracing (and
thus eventual freeing of q->blk_trace) is completely unsynchronized with
the currently running tracing and thus it can happen that the blk_trace
structure is being freed just while __blk_add_trace() works on it.
Protect accesses to q->blk_trace by RCU during tracing and make sure we
wait for the end of RCU grace period when shutting down tracing. Luckily
that is rare enough event that we can afford that. Note that postponing
the freeing of blk_trace to an RCU callback should better be avoided as
it could have unexpected user visible side-effects as debugfs files
would be still existing for a short while block tracing has been shut
down.

Link: https://bugzilla.kernel.org/show_bug.cgi?id=205711
CC: stable@vger.kernel.org
	Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Tested-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Bart Van Assche <bvanassche@acm.org>
	Reported-by: Tristan Madani <tristmd@gmail.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c780e86dd48ef6467a1146cf7d0fe1e05a635039)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/trace/blktrace.c
diff --cc kernel/trace/blktrace.c
index c8eca3615faf,4560878f0bac..000000000000
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@@ -769,23 -753,23 +772,25 @@@ void blk_trace_shutdown(struct request_
  }
  
  #ifdef CONFIG_BLK_CGROUP
 -static u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
 +static union kernfs_node_id *
 +blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  
+ 	/* We don't use the 'bt' value here except as an optimization... */
+ 	bt = rcu_dereference_protected(q->blk_trace, 1);
  	if (!bt || !(blk_tracer_flags.val & TRACE_BLK_OPT_CGROUP))
 -		return 0;
 +		return NULL;
  
  	if (!bio->bi_blkg)
 -		return 0;
 -	return cgroup_id(bio_blkcg(bio)->css.cgroup);
 +		return NULL;
 +	return cgroup_get_kernfs_id(bio_blkcg(bio)->css.cgroup);
  }
  #else
 -u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
 +static union kernfs_node_id *
 +blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
  {
 -	return 0;
 +	return NULL;
  }
  #endif
  
@@@ -815,13 -799,16 +820,17 @@@ blk_trace_request_get_cgid(struct reque
   *
   **/
  static void blk_add_trace_rq(struct request *rq, int error,
 -			     unsigned int nr_bytes, u32 what, u64 cgid)
 +			     unsigned int nr_bytes, u32 what,
 +			     union kernfs_node_id *cgid)
  {
- 	struct blk_trace *bt = rq->q->blk_trace;
+ 	struct blk_trace *bt;
  
- 	if (likely(!bt))
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(rq->q->blk_trace);
+ 	if (likely(!bt)) {
+ 		rcu_read_unlock();
  		return;
+ 	}
  
  	if (blk_rq_is_passthrough(rq))
  		what |= BLK_TC_ACT(BLK_TC_PC);
@@@ -927,11 -920,14 +942,18 @@@ static void blk_add_trace_getrq(void *i
  	if (bio)
  		blk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);
  	else {
- 		struct blk_trace *bt = q->blk_trace;
+ 		struct blk_trace *bt;
  
+ 		rcu_read_lock();
+ 		bt = rcu_dereference(q->blk_trace);
  		if (bt)
  			__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,
++<<<<<<< HEAD
 +					NULL, NULL);
++=======
+ 					NULL, 0);
+ 		rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  	}
  }
  
@@@ -943,20 -939,26 +965,34 @@@ static void blk_add_trace_sleeprq(void 
  	if (bio)
  		blk_add_trace_bio(q, bio, BLK_TA_SLEEPRQ, 0);
  	else {
- 		struct blk_trace *bt = q->blk_trace;
+ 		struct blk_trace *bt;
  
+ 		rcu_read_lock();
+ 		bt = rcu_dereference(q->blk_trace);
  		if (bt)
  			__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,
++<<<<<<< HEAD
 +					0, 0, NULL, NULL);
++=======
+ 					0, 0, NULL, 0);
+ 		rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  	}
  }
  
  static void blk_add_trace_plug(void *ignore, struct request_queue *q)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
  	if (bt)
++<<<<<<< HEAD
 +		__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, NULL);
++=======
+ 		__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  static void blk_add_trace_unplug(void *ignore, struct request_queue *q,
@@@ -973,8 -977,9 +1011,9 @@@
  		else
  			what = BLK_TA_UNPLUG_TIMER;
  
 -		__blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);
 +		__blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, NULL);
  	}
+ 	rcu_read_unlock();
  }
  
  static void blk_add_trace_split(void *ignore,
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fa087fdb5961..2c914710bc25 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -536,7 +536,7 @@ struct request_queue {
 	unsigned int		sg_reserved_size;
 	int			node;
 #ifdef CONFIG_BLK_DEV_IO_TRACE
-	struct blk_trace	*blk_trace;
+	struct blk_trace __rcu	*blk_trace;
 	struct mutex		blk_trace_mutex;
 #endif
 	/*
diff --git a/include/linux/blktrace_api.h b/include/linux/blktrace_api.h
index 8804753805ac..aeebb874e67f 100644
--- a/include/linux/blktrace_api.h
+++ b/include/linux/blktrace_api.h
@@ -51,9 +51,13 @@ void __trace_note_message(struct blk_trace *, struct blkcg *blkcg, const char *f
  **/
 #define blk_add_cgroup_trace_msg(q, cg, fmt, ...)			\
 	do {								\
-		struct blk_trace *bt = (q)->blk_trace;			\
+		struct blk_trace *bt;					\
+									\
+		rcu_read_lock();					\
+		bt = rcu_dereference((q)->blk_trace);			\
 		if (unlikely(bt))					\
 			__trace_note_message(bt, cg, fmt, ##__VA_ARGS__);\
+		rcu_read_unlock();					\
 	} while (0)
 #define blk_add_trace_msg(q, fmt, ...)					\
 	blk_add_cgroup_trace_msg(q, NULL, fmt, ##__VA_ARGS__)
@@ -61,10 +65,14 @@ void __trace_note_message(struct blk_trace *, struct blkcg *blkcg, const char *f
 
 static inline bool blk_trace_note_message_enabled(struct request_queue *q)
 {
-	struct blk_trace *bt = q->blk_trace;
-	if (likely(!bt))
-		return false;
-	return bt->act_mask & BLK_TC_NOTIFY;
+	struct blk_trace *bt;
+	bool ret;
+
+	rcu_read_lock();
+	bt = rcu_dereference(q->blk_trace);
+	ret = bt && (bt->act_mask & BLK_TC_NOTIFY);
+	rcu_read_unlock();
+	return ret;
 }
 
 extern void blk_add_driver_data(struct request_queue *q, struct request *rq,
* Unmerged path kernel/trace/blktrace.c
