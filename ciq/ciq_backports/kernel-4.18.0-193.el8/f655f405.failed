mm/percpu: add checks for the return value of memblock_alloc*()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit f655f40537916d4b1d6d1a023a778697c75a4fe2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f655f405.failed

Add panic() calls if memblock_alloc() returns NULL.

The panic() format duplicates the one used by memblock itself and in
order to avoid explosion with long parameters list replace open coded
allocation size calculations with a local variable.

Link: http://lkml.kernel.org/r/1548057848-15136-17-git-send-email-rppt@linux.ibm.com
	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Dennis Zhou <dennis@kernel.org>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Guo Ren <ren_guo@c-sky.com>				[c-sky]
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Juergen Gross <jgross@suse.com>			[Xen]
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Paul Burton <paul.burton@mips.com>
	Cc: Petr Mladek <pmladek@suse.com>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Rob Herring <robh+dt@kernel.org>
	Cc: Rob Herring <robh@kernel.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Stafford Horne <shorne@gmail.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f655f40537916d4b1d6d1a023a778697c75a4fe2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 7c41be93b260,3f9fb3086a9b..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -1101,9 -1102,12 +1102,18 @@@ static struct pcpu_chunk * __init pcpu_
  	region_size = ALIGN(start_offset + map_size, lcm_align);
  
  	/* allocate chunk */
++<<<<<<< HEAD
 +	chunk = memblock_virt_alloc(sizeof(struct pcpu_chunk) +
 +				    BITS_TO_LONGS(region_size >> PAGE_SHIFT),
 +				    0);
++=======
+ 	alloc_size = sizeof(struct pcpu_chunk) +
+ 		BITS_TO_LONGS(region_size >> PAGE_SHIFT);
+ 	chunk = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  	INIT_LIST_HEAD(&chunk->list);
  
@@@ -1114,12 -1118,25 +1124,34 @@@
  	chunk->nr_pages = region_size >> PAGE_SHIFT;
  	region_bits = pcpu_chunk_map_bits(chunk);
  
++<<<<<<< HEAD
 +	chunk->alloc_map = memblock_virt_alloc(BITS_TO_LONGS(region_bits) *
 +					       sizeof(chunk->alloc_map[0]), 0);
 +	chunk->bound_map = memblock_virt_alloc(BITS_TO_LONGS(region_bits + 1) *
 +					       sizeof(chunk->bound_map[0]), 0);
 +	chunk->md_blocks = memblock_virt_alloc(pcpu_chunk_nr_blocks(chunk) *
 +					       sizeof(chunk->md_blocks[0]), 0);
++=======
+ 	alloc_size = BITS_TO_LONGS(region_bits) * sizeof(chunk->alloc_map[0]);
+ 	chunk->alloc_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->alloc_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size =
+ 		BITS_TO_LONGS(region_bits + 1) * sizeof(chunk->bound_map[0]);
+ 	chunk->bound_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->bound_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = pcpu_chunk_nr_blocks(chunk) * sizeof(chunk->md_blocks[0]);
+ 	chunk->md_blocks = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->md_blocks)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  	pcpu_init_md_blocks(chunk);
  
  	/* manage populated page bitmap */
@@@ -2075,12 -2093,29 +2108,38 @@@ int __init pcpu_setup_first_chunk(cons
  	PCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);
  
  	/* process group information and build config tables accordingly */
++<<<<<<< HEAD
 +	group_offsets = memblock_virt_alloc(ai->nr_groups *
 +					     sizeof(group_offsets[0]), 0);
 +	group_sizes = memblock_virt_alloc(ai->nr_groups *
 +					   sizeof(group_sizes[0]), 0);
 +	unit_map = memblock_virt_alloc(nr_cpu_ids * sizeof(unit_map[0]), 0);
 +	unit_off = memblock_virt_alloc(nr_cpu_ids * sizeof(unit_off[0]), 0);
++=======
+ 	alloc_size = ai->nr_groups * sizeof(group_offsets[0]);
+ 	group_offsets = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!group_offsets)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = ai->nr_groups * sizeof(group_sizes[0]);
+ 	group_sizes = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!group_sizes)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = nr_cpu_ids * sizeof(unit_map[0]);
+ 	unit_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!unit_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = nr_cpu_ids * sizeof(unit_off[0]);
+ 	unit_off = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!unit_off)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  	for (cpu = 0; cpu < nr_cpu_ids; cpu++)
  		unit_map[cpu] = UINT_MAX;
@@@ -2144,8 -2179,11 +2203,16 @@@
  	 * empty chunks.
  	 */
  	pcpu_nr_slots = __pcpu_size_to_slot(pcpu_unit_size) + 2;
++<<<<<<< HEAD
 +	pcpu_slot = memblock_virt_alloc(
 +			pcpu_nr_slots * sizeof(pcpu_slot[0]), 0);
++=======
+ 	pcpu_slot = memblock_alloc(pcpu_nr_slots * sizeof(pcpu_slot[0]),
+ 				   SMP_CACHE_BYTES);
+ 	if (!pcpu_slot)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      pcpu_nr_slots * sizeof(pcpu_slot[0]));
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  	for (i = 0; i < pcpu_nr_slots; i++)
  		INIT_LIST_HEAD(&pcpu_slot[i]);
  
@@@ -2599,7 -2637,10 +2666,14 @@@ int __init pcpu_page_first_chunk(size_
  	/* unaligned allocations can't be freed, round up to page size */
  	pages_size = PFN_ALIGN(unit_pages * num_possible_cpus() *
  			       sizeof(pages[0]));
++<<<<<<< HEAD
 +	pages = memblock_virt_alloc(pages_size, 0);
++=======
+ 	pages = memblock_alloc(pages_size, SMP_CACHE_BYTES);
+ 	if (!pages)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      pages_size);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  	/* allocate pages */
  	j = 0;
* Unmerged path mm/percpu.c
