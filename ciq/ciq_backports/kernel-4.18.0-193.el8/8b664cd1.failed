s390/qeth: use IQD Multi-Write

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Julian Wiedmann <jwi@linux.ibm.com>
commit 8b664cd127a1e3777e23c8aaa96ba52ef741bb55
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/8b664cd1.failed

For IQD devices with Multi-Write support, we can defer the queue-flush
further and transmit multiple IO buffers with a single TX doorbell.
The same-target restriction still applies.

	Signed-off-by: Julian Wiedmann <jwi@linux.ibm.com>
	Reviewed-by: Alexandra Winter <wintera@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8b664cd127a1e3777e23c8aaa96ba52ef741bb55)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/net/qeth_core.h
#	drivers/s390/net/qeth_core_main.c
diff --cc drivers/s390/net/qeth_core.h
index dcbcb3b4d38f,d08154502b15..000000000000
--- a/drivers/s390/net/qeth_core.h
+++ b/drivers/s390/net/qeth_core.h
@@@ -480,8 -528,38 +480,17 @@@ struct qeth_qdio_out_q 
  	atomic_t used_buffers;
  	/* indicates whether PCI flag must be set (or if one is outstanding) */
  	atomic_t set_pci_flags_count;
++<<<<<<< HEAD
++=======
+ 	struct napi_struct napi;
+ 	struct timer_list timer;
+ 	struct qeth_hdr *prev_hdr;
+ 	u8 bulk_start;
+ 	u8 bulk_count;
+ 	u8 bulk_max;
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  };
  
 -#define qeth_for_each_output_queue(card, q, i)		\
 -	for (i = 0; i < card->qdio.no_out_queues &&	\
 -		    (q = card->qdio.out_qs[i]); i++)
 -
 -#define	qeth_napi_to_out_queue(n) container_of(n, struct qeth_qdio_out_q, napi)
 -
 -static inline void qeth_tx_arm_timer(struct qeth_qdio_out_q *queue)
 -{
 -	if (timer_pending(&queue->timer))
 -		return;
 -	mod_timer(&queue->timer, usecs_to_jiffies(QETH_TX_TIMER_USECS) +
 -				 jiffies);
 -}
 -
 -static inline bool qeth_out_queue_is_full(struct qeth_qdio_out_q *queue)
 -{
 -	return atomic_read(&queue->used_buffers) >= QDIO_MAX_BUFFERS_PER_Q;
 -}
 -
 -static inline bool qeth_out_queue_is_empty(struct qeth_qdio_out_q *queue)
 -{
 -	return atomic_read(&queue->used_buffers) == 0;
 -}
 -
  struct qeth_qdio_info {
  	atomic_t state;
  	/* input */
@@@ -821,6 -871,22 +830,25 @@@ static inline bool qeth_netdev_is_regis
  	return dev->netdev_ops != NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static inline u16 qeth_iqd_translate_txq(struct net_device *dev, u16 txq)
+ {
+ 	if (txq == QETH_IQD_MCAST_TXQ)
+ 		return dev->num_tx_queues - 1;
+ 	if (txq == dev->num_tx_queues - 1)
+ 		return QETH_IQD_MCAST_TXQ;
+ 	return txq;
+ }
+ 
+ static inline bool qeth_iqd_is_mcast_queue(struct qeth_card *card,
+ 					   struct qeth_qdio_out_q *queue)
+ {
+ 	return qeth_iqd_translate_txq(card->dev, queue->queue_no) ==
+ 	       QETH_IQD_MCAST_TXQ;
+ }
+ 
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  static inline void qeth_scrub_qdio_buffer(struct qdio_buffer *buf,
  					  unsigned int elements)
  {
diff --cc drivers/s390/net/qeth_core_main.c
index a0c5702815af,62054afd73cc..000000000000
--- a/drivers/s390/net/qeth_core_main.c
+++ b/drivers/s390/net/qeth_core_main.c
@@@ -2784,14 -2677,20 +2796,31 @@@ int qeth_init_qdio_queues(struct qeth_c
  
  	/* outbound queue */
  	for (i = 0; i < card->qdio.no_out_queues; ++i) {
++<<<<<<< HEAD
 +		qdio_reset_buffers(card->qdio.out_qs[i]->qdio_bufs,
 +				   QDIO_MAX_BUFFERS_PER_Q);
 +		card->qdio.out_qs[i]->next_buf_to_fill = 0;
 +		card->qdio.out_qs[i]->do_pack = 0;
 +		atomic_set(&card->qdio.out_qs[i]->used_buffers, 0);
 +		atomic_set(&card->qdio.out_qs[i]->set_pci_flags_count, 0);
 +		atomic_set(&card->qdio.out_qs[i]->state,
 +			   QETH_OUT_Q_UNLOCKED);
++=======
+ 		struct qeth_qdio_out_q *queue = card->qdio.out_qs[i];
+ 
+ 		qdio_reset_buffers(queue->qdio_bufs, QDIO_MAX_BUFFERS_PER_Q);
+ 		queue->max_elements = QETH_MAX_BUFFER_ELEMENTS(card);
+ 		queue->next_buf_to_fill = 0;
+ 		queue->do_pack = 0;
+ 		queue->prev_hdr = NULL;
+ 		queue->bulk_start = 0;
+ 		queue->bulk_count = 0;
+ 		queue->bulk_max = qeth_tx_select_bulk_max(card, queue);
+ 		atomic_set(&queue->used_buffers, 0);
+ 		atomic_set(&queue->set_pci_flags_count, 0);
+ 		atomic_set(&queue->state, QETH_OUT_Q_UNLOCKED);
+ 		netdev_tx_reset_queue(netdev_get_tx_queue(card->dev, i));
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  	}
  	return 0;
  }
@@@ -3433,8 -3328,15 +3462,20 @@@ static void qeth_flush_buffers(struct q
  		qeth_schedule_recovery(queue->card);
  		return;
  	}
++<<<<<<< HEAD
 +	if (queue->card->options.performance_stats)
 +		queue->card->perf_stats.bufs_sent += count;
++=======
+ }
+ 
+ static void qeth_flush_queue(struct qeth_qdio_out_q *queue)
+ {
+ 	qeth_flush_buffers(queue, queue->bulk_start, queue->bulk_count);
+ 
+ 	queue->bulk_start = QDIO_BUFNR(queue->bulk_start + queue->bulk_count);
+ 	queue->prev_hdr = NULL;
+ 	queue->bulk_count = 0;
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  }
  
  static void qeth_check_outbound_queue(struct qeth_qdio_out_q *queue)
@@@ -3824,9 -3694,32 +3865,38 @@@ check_layout
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void __qeth_fill_buffer(struct sk_buff *skb,
 +			       struct qeth_qdio_out_buffer *buf,
 +			       bool is_first_elem, unsigned int offset)
++=======
+ static bool qeth_iqd_may_bulk(struct qeth_qdio_out_q *queue,
+ 			      struct sk_buff *curr_skb,
+ 			      struct qeth_hdr *curr_hdr)
+ {
+ 	struct qeth_qdio_out_buffer *buffer = queue->bufs[queue->bulk_start];
+ 	struct qeth_hdr *prev_hdr = queue->prev_hdr;
+ 
+ 	if (!prev_hdr)
+ 		return true;
+ 
+ 	/* All packets must have the same target: */
+ 	if (curr_hdr->hdr.l2.id == QETH_HEADER_TYPE_LAYER2) {
+ 		struct sk_buff *prev_skb = skb_peek(&buffer->skb_list);
+ 
+ 		return ether_addr_equal(eth_hdr(prev_skb)->h_dest,
+ 					eth_hdr(curr_skb)->h_dest) &&
+ 		       qeth_l2_same_vlan(&prev_hdr->hdr.l2, &curr_hdr->hdr.l2);
+ 	}
+ 
+ 	return qeth_l3_same_next_hop(&prev_hdr->hdr.l3, &curr_hdr->hdr.l3) &&
+ 	       qeth_l3_iqd_same_vlan(&prev_hdr->hdr.l3, &curr_hdr->hdr.l3);
+ }
+ 
+ static unsigned int __qeth_fill_buffer(struct sk_buff *skb,
+ 				       struct qeth_qdio_out_buffer *buf,
+ 				       bool is_first_elem, unsigned int offset)
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  {
  	struct qdio_buffer *buffer = buf->buffer;
  	int element = buf->next_element_to_fill;
@@@ -3918,43 -3810,83 +3988,115 @@@ static int qeth_fill_buffer(struct qeth
  		buf->next_element_to_fill++;
  	}
  
 -	return __qeth_fill_buffer(skb, buf, is_first_elem, offset);
 +	__qeth_fill_buffer(skb, buf, is_first_elem, offset);
 +
 +	if (!queue->do_pack) {
 +		QETH_CARD_TEXT(queue->card, 6, "fillbfnp");
 +	} else {
 +		QETH_CARD_TEXT(queue->card, 6, "fillbfpa");
 +		if (queue->card->options.performance_stats)
 +			queue->card->perf_stats.skbs_sent_pack++;
 +
 +		/* If the buffer still has free elements, keep using it. */
 +		if (buf->next_element_to_fill <
 +		    QETH_MAX_BUFFER_ELEMENTS(queue->card))
 +			return 0;
 +	}
 +
 +	/* flush out the buffer */
 +	atomic_set(&buf->state, QETH_QDIO_BUF_PRIMED);
 +	queue->next_buf_to_fill = (queue->next_buf_to_fill + 1) %
 +				  QDIO_MAX_BUFFERS_PER_Q;
 +	return 1;
  }
  
 -static int __qeth_xmit(struct qeth_card *card, struct qeth_qdio_out_q *queue,
 -		       struct sk_buff *skb, unsigned int elements,
 -		       struct qeth_hdr *hdr, unsigned int offset,
 -		       unsigned int hd_len)
 +static int qeth_do_send_packet_fast(struct qeth_qdio_out_q *queue,
 +				    struct sk_buff *skb, struct qeth_hdr *hdr,
 +				    unsigned int offset, unsigned int hd_len)
  {
++<<<<<<< HEAD
 +	int index = queue->next_buf_to_fill;
 +	struct qeth_qdio_out_buffer *buffer = queue->bufs[index];
 +
 +	/*
 +	 * check if buffer is empty to make sure that we do not 'overtake'
 +	 * ourselves and try to fill a buffer that is already primed
 +	 */
 +	if (atomic_read(&buffer->state) != QETH_QDIO_BUF_EMPTY)
 +		return -EBUSY;
 +	qeth_fill_buffer(queue, buffer, skb, hdr, offset, hd_len);
 +	qeth_flush_buffers(queue, index, 1);
++=======
+ 	unsigned int bytes = qdisc_pkt_len(skb);
+ 	struct qeth_qdio_out_buffer *buffer;
+ 	unsigned int next_element;
+ 	struct netdev_queue *txq;
+ 	bool stopped = false;
+ 	bool flush;
+ 
+ 	buffer = queue->bufs[QDIO_BUFNR(queue->bulk_start + queue->bulk_count)];
+ 	txq = netdev_get_tx_queue(card->dev, skb_get_queue_mapping(skb));
+ 
+ 	/* Just a sanity check, the wake/stop logic should ensure that we always
+ 	 * get a free buffer.
+ 	 */
+ 	if (atomic_read(&buffer->state) != QETH_QDIO_BUF_EMPTY)
+ 		return -EBUSY;
+ 
+ 	flush = !qeth_iqd_may_bulk(queue, skb, hdr);
+ 
+ 	if (flush ||
+ 	    (buffer->next_element_to_fill + elements > queue->max_elements)) {
+ 		if (buffer->next_element_to_fill > 0) {
+ 			atomic_set(&buffer->state, QETH_QDIO_BUF_PRIMED);
+ 			queue->bulk_count++;
+ 		}
+ 
+ 		if (queue->bulk_count >= queue->bulk_max)
+ 			flush = true;
+ 
+ 		if (flush)
+ 			qeth_flush_queue(queue);
+ 
+ 		buffer = queue->bufs[QDIO_BUFNR(queue->bulk_start +
+ 						queue->bulk_count)];
+ 
+ 		/* Sanity-check again: */
+ 		if (atomic_read(&buffer->state) != QETH_QDIO_BUF_EMPTY)
+ 			return -EBUSY;
+ 	}
+ 
+ 	if (buffer->next_element_to_fill == 0 &&
+ 	    atomic_inc_return(&queue->used_buffers) >= QDIO_MAX_BUFFERS_PER_Q) {
+ 		/* If a TX completion happens right _here_ and misses to wake
+ 		 * the txq, then our re-check below will catch the race.
+ 		 */
+ 		QETH_TXQ_STAT_INC(queue, stopped);
+ 		netif_tx_stop_queue(txq);
+ 		stopped = true;
+ 	}
+ 
+ 	next_element = qeth_fill_buffer(buffer, skb, hdr, offset, hd_len);
+ 	buffer->bytes += bytes;
+ 	queue->prev_hdr = hdr;
+ 
+ 	flush = __netdev_tx_sent_queue(txq, bytes,
+ 				       !stopped && netdev_xmit_more());
+ 
+ 	if (flush || next_element >= queue->max_elements) {
+ 		atomic_set(&buffer->state, QETH_QDIO_BUF_PRIMED);
+ 		queue->bulk_count++;
+ 
+ 		if (queue->bulk_count >= queue->bulk_max)
+ 			flush = true;
+ 
+ 		if (flush)
+ 			qeth_flush_queue(queue);
+ 	}
+ 
+ 	if (stopped && !qeth_out_queue_is_full(queue))
+ 		netif_tx_start_queue(txq);
++>>>>>>> 8b664cd127a1 (s390/qeth: use IQD Multi-Write)
  	return 0;
  }
  
* Unmerged path drivers/s390/net/qeth_core.h
* Unmerged path drivers/s390/net/qeth_core_main.c
