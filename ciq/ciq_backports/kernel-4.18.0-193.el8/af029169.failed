vfio/type1: Check reserved region conflict and update iova list

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [vfio] type1: Check reserved region conflict and update iova list (Auger Eric) [1704597]
Rebuild_FUZZ: 95.87%
commit-author Shameer Kolothum <shameerali.kolothum.thodi@huawei.com>
commit af029169b8fdae31064624d60b5469a3da95ad32
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/af029169.failed

This retrieves the reserved regions associated with dev group and
checks for conflicts with any existing dma mappings. Also update
the iova list excluding the reserved regions.

Reserved regions with type IOMMU_RESV_DIRECT_RELAXABLE are
excluded from above checks as they are considered as directly
mapped regions which are known to be relaxable.

	Signed-off-by: Shameer Kolothum <shameerali.kolothum.thodi@huawei.com>
	Reviewed-by: Eric Auger <eric.auger@redhat.com>
	Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
(cherry picked from commit af029169b8fdae31064624d60b5469a3da95ad32)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vfio/vfio_iommu_type1.c
diff --cc drivers/vfio/vfio_iommu_type1.c
index 7c23a77fd289,a3c9794ccf83..000000000000
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@@ -1340,6 -1299,324 +1340,327 @@@ static bool vfio_iommu_has_sw_msi(struc
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static struct device *vfio_mdev_get_iommu_device(struct device *dev)
+ {
+ 	struct device *(*fn)(struct device *dev);
+ 	struct device *iommu_device;
+ 
+ 	fn = symbol_get(mdev_get_iommu_device);
+ 	if (fn) {
+ 		iommu_device = fn(dev);
+ 		symbol_put(mdev_get_iommu_device);
+ 
+ 		return iommu_device;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static int vfio_mdev_attach_domain(struct device *dev, void *data)
+ {
+ 	struct iommu_domain *domain = data;
+ 	struct device *iommu_device;
+ 
+ 	iommu_device = vfio_mdev_get_iommu_device(dev);
+ 	if (iommu_device) {
+ 		if (iommu_dev_feature_enabled(iommu_device, IOMMU_DEV_FEAT_AUX))
+ 			return iommu_aux_attach_device(domain, iommu_device);
+ 		else
+ 			return iommu_attach_device(domain, iommu_device);
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static int vfio_mdev_detach_domain(struct device *dev, void *data)
+ {
+ 	struct iommu_domain *domain = data;
+ 	struct device *iommu_device;
+ 
+ 	iommu_device = vfio_mdev_get_iommu_device(dev);
+ 	if (iommu_device) {
+ 		if (iommu_dev_feature_enabled(iommu_device, IOMMU_DEV_FEAT_AUX))
+ 			iommu_aux_detach_device(domain, iommu_device);
+ 		else
+ 			iommu_detach_device(domain, iommu_device);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int vfio_iommu_attach_group(struct vfio_domain *domain,
+ 				   struct vfio_group *group)
+ {
+ 	if (group->mdev_group)
+ 		return iommu_group_for_each_dev(group->iommu_group,
+ 						domain->domain,
+ 						vfio_mdev_attach_domain);
+ 	else
+ 		return iommu_attach_group(domain->domain, group->iommu_group);
+ }
+ 
+ static void vfio_iommu_detach_group(struct vfio_domain *domain,
+ 				    struct vfio_group *group)
+ {
+ 	if (group->mdev_group)
+ 		iommu_group_for_each_dev(group->iommu_group, domain->domain,
+ 					 vfio_mdev_detach_domain);
+ 	else
+ 		iommu_detach_group(domain->domain, group->iommu_group);
+ }
+ 
+ static bool vfio_bus_is_mdev(struct bus_type *bus)
+ {
+ 	struct bus_type *mdev_bus;
+ 	bool ret = false;
+ 
+ 	mdev_bus = symbol_get(mdev_bus_type);
+ 	if (mdev_bus) {
+ 		ret = (bus == mdev_bus);
+ 		symbol_put(mdev_bus_type);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int vfio_mdev_iommu_device(struct device *dev, void *data)
+ {
+ 	struct device **old = data, *new;
+ 
+ 	new = vfio_mdev_get_iommu_device(dev);
+ 	if (!new || (*old && *old != new))
+ 		return -EINVAL;
+ 
+ 	*old = new;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * This is a helper function to insert an address range to iova list.
+  * The list is initially created with a single entry corresponding to
+  * the IOMMU domain geometry to which the device group is attached.
+  * The list aperture gets modified when a new domain is added to the
+  * container if the new aperture doesn't conflict with the current one
+  * or with any existing dma mappings. The list is also modified to
+  * exclude any reserved regions associated with the device group.
+  */
+ static int vfio_iommu_iova_insert(struct list_head *head,
+ 				  dma_addr_t start, dma_addr_t end)
+ {
+ 	struct vfio_iova *region;
+ 
+ 	region = kmalloc(sizeof(*region), GFP_KERNEL);
+ 	if (!region)
+ 		return -ENOMEM;
+ 
+ 	INIT_LIST_HEAD(&region->list);
+ 	region->start = start;
+ 	region->end = end;
+ 
+ 	list_add_tail(&region->list, head);
+ 	return 0;
+ }
+ 
+ /*
+  * Check the new iommu aperture conflicts with existing aper or with any
+  * existing dma mappings.
+  */
+ static bool vfio_iommu_aper_conflict(struct vfio_iommu *iommu,
+ 				     dma_addr_t start, dma_addr_t end)
+ {
+ 	struct vfio_iova *first, *last;
+ 	struct list_head *iova = &iommu->iova_list;
+ 
+ 	if (list_empty(iova))
+ 		return false;
+ 
+ 	/* Disjoint sets, return conflict */
+ 	first = list_first_entry(iova, struct vfio_iova, list);
+ 	last = list_last_entry(iova, struct vfio_iova, list);
+ 	if (start > last->end || end < first->start)
+ 		return true;
+ 
+ 	/* Check for any existing dma mappings below the new start */
+ 	if (start > first->start) {
+ 		if (vfio_find_dma(iommu, first->start, start - first->start))
+ 			return true;
+ 	}
+ 
+ 	/* Check for any existing dma mappings beyond the new end */
+ 	if (end < last->end) {
+ 		if (vfio_find_dma(iommu, end + 1, last->end - end))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Resize iommu iova aperture window. This is called only if the new
+  * aperture has no conflict with existing aperture and dma mappings.
+  */
+ static int vfio_iommu_aper_resize(struct list_head *iova,
+ 				  dma_addr_t start, dma_addr_t end)
+ {
+ 	struct vfio_iova *node, *next;
+ 
+ 	if (list_empty(iova))
+ 		return vfio_iommu_iova_insert(iova, start, end);
+ 
+ 	/* Adjust iova list start */
+ 	list_for_each_entry_safe(node, next, iova, list) {
+ 		if (start < node->start)
+ 			break;
+ 		if (start >= node->start && start < node->end) {
+ 			node->start = start;
+ 			break;
+ 		}
+ 		/* Delete nodes before new start */
+ 		list_del(&node->list);
+ 		kfree(node);
+ 	}
+ 
+ 	/* Adjust iova list end */
+ 	list_for_each_entry_safe(node, next, iova, list) {
+ 		if (end > node->end)
+ 			continue;
+ 		if (end > node->start && end <= node->end) {
+ 			node->end = end;
+ 			continue;
+ 		}
+ 		/* Delete nodes after new end */
+ 		list_del(&node->list);
+ 		kfree(node);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Check reserved region conflicts with existing dma mappings
+  */
+ static bool vfio_iommu_resv_conflict(struct vfio_iommu *iommu,
+ 				     struct list_head *resv_regions)
+ {
+ 	struct iommu_resv_region *region;
+ 
+ 	/* Check for conflict with existing dma mappings */
+ 	list_for_each_entry(region, resv_regions, list) {
+ 		if (region->type == IOMMU_RESV_DIRECT_RELAXABLE)
+ 			continue;
+ 
+ 		if (vfio_find_dma(iommu, region->start, region->length))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Check iova region overlap with  reserved regions and
+  * exclude them from the iommu iova range
+  */
+ static int vfio_iommu_resv_exclude(struct list_head *iova,
+ 				   struct list_head *resv_regions)
+ {
+ 	struct iommu_resv_region *resv;
+ 	struct vfio_iova *n, *next;
+ 
+ 	list_for_each_entry(resv, resv_regions, list) {
+ 		phys_addr_t start, end;
+ 
+ 		if (resv->type == IOMMU_RESV_DIRECT_RELAXABLE)
+ 			continue;
+ 
+ 		start = resv->start;
+ 		end = resv->start + resv->length - 1;
+ 
+ 		list_for_each_entry_safe(n, next, iova, list) {
+ 			int ret = 0;
+ 
+ 			/* No overlap */
+ 			if (start > n->end || end < n->start)
+ 				continue;
+ 			/*
+ 			 * Insert a new node if current node overlaps with the
+ 			 * reserve region to exlude that from valid iova range.
+ 			 * Note that, new node is inserted before the current
+ 			 * node and finally the current node is deleted keeping
+ 			 * the list updated and sorted.
+ 			 */
+ 			if (start > n->start)
+ 				ret = vfio_iommu_iova_insert(&n->list, n->start,
+ 							     start - 1);
+ 			if (!ret && end < n->end)
+ 				ret = vfio_iommu_iova_insert(&n->list, end + 1,
+ 							     n->end);
+ 			if (ret)
+ 				return ret;
+ 
+ 			list_del(&n->list);
+ 			kfree(n);
+ 		}
+ 	}
+ 
+ 	if (list_empty(iova))
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static void vfio_iommu_resv_free(struct list_head *resv_regions)
+ {
+ 	struct iommu_resv_region *n, *next;
+ 
+ 	list_for_each_entry_safe(n, next, resv_regions, list) {
+ 		list_del(&n->list);
+ 		kfree(n);
+ 	}
+ }
+ 
+ static void vfio_iommu_iova_free(struct list_head *iova)
+ {
+ 	struct vfio_iova *n, *next;
+ 
+ 	list_for_each_entry_safe(n, next, iova, list) {
+ 		list_del(&n->list);
+ 		kfree(n);
+ 	}
+ }
+ 
+ static int vfio_iommu_iova_get_copy(struct vfio_iommu *iommu,
+ 				    struct list_head *iova_copy)
+ {
+ 	struct list_head *iova = &iommu->iova_list;
+ 	struct vfio_iova *n;
+ 	int ret;
+ 
+ 	list_for_each_entry(n, iova, list) {
+ 		ret = vfio_iommu_iova_insert(iova_copy, n->start, n->end);
+ 		if (ret)
+ 			goto out_free;
+ 	}
+ 
+ 	return 0;
+ 
+ out_free:
+ 	vfio_iommu_iova_free(iova_copy);
+ 	return ret;
+ }
+ 
+ static void vfio_iommu_iova_insert_copy(struct vfio_iommu *iommu,
+ 					struct list_head *iova_copy)
+ {
+ 	struct list_head *iova = &iommu->iova_list;
+ 
+ 	vfio_iommu_iova_free(iova);
+ 
+ 	list_splice_tail(iova_copy, iova);
+ }
++>>>>>>> af029169b8fd (vfio/type1: Check reserved region conflict and update iova list)
  static int vfio_iommu_type1_attach_group(void *iommu_data,
  					 struct iommu_group *iommu_group)
  {
@@@ -1350,6 -1627,9 +1671,12 @@@
  	int ret;
  	bool resv_msi, msi_remap;
  	phys_addr_t resv_msi_base;
++<<<<<<< HEAD
++=======
+ 	struct iommu_domain_geometry geo;
+ 	LIST_HEAD(iova_copy);
+ 	LIST_HEAD(group_resv_regions);
++>>>>>>> af029169b8fd (vfio/type1: Check reserved region conflict and update iova list)
  
  	mutex_lock(&iommu->lock);
  
@@@ -1419,6 -1706,42 +1746,45 @@@
  	if (ret)
  		goto out_domain;
  
++<<<<<<< HEAD
++=======
+ 	/* Get aperture info */
+ 	iommu_domain_get_attr(domain->domain, DOMAIN_ATTR_GEOMETRY, &geo);
+ 
+ 	if (vfio_iommu_aper_conflict(iommu, geo.aperture_start,
+ 				     geo.aperture_end)) {
+ 		ret = -EINVAL;
+ 		goto out_detach;
+ 	}
+ 
+ 	ret = iommu_get_group_resv_regions(iommu_group, &group_resv_regions);
+ 	if (ret)
+ 		goto out_detach;
+ 
+ 	if (vfio_iommu_resv_conflict(iommu, &group_resv_regions)) {
+ 		ret = -EINVAL;
+ 		goto out_detach;
+ 	}
+ 
+ 	/*
+ 	 * We don't want to work on the original iova list as the list
+ 	 * gets modified and in case of failure we have to retain the
+ 	 * original list. Get a copy here.
+ 	 */
+ 	ret = vfio_iommu_iova_get_copy(iommu, &iova_copy);
+ 	if (ret)
+ 		goto out_detach;
+ 
+ 	ret = vfio_iommu_aper_resize(&iova_copy, geo.aperture_start,
+ 				     geo.aperture_end);
+ 	if (ret)
+ 		goto out_detach;
+ 
+ 	ret = vfio_iommu_resv_exclude(&iova_copy, &group_resv_regions);
+ 	if (ret)
+ 		goto out_detach;
+ 
++>>>>>>> af029169b8fd (vfio/type1: Check reserved region conflict and update iova list)
  	resv_msi = vfio_iommu_has_sw_msi(iommu_group, &resv_msi_base);
  
  	INIT_LIST_HEAD(&domain->group_list);
@@@ -1476,15 -1798,20 +1842,21 @@@
  	}
  
  	list_add(&domain->next, &iommu->domain_list);
 -done:
 -	/* Delete the old one and insert new iova list */
 -	vfio_iommu_iova_insert_copy(iommu, &iova_copy);
 +
  	mutex_unlock(&iommu->lock);
+ 	vfio_iommu_resv_free(&group_resv_regions);
  
  	return 0;
  
  out_detach:
 -	vfio_iommu_detach_group(domain, group);
 +	iommu_detach_group(domain->domain, iommu_group);
  out_domain:
  	iommu_domain_free(domain->domain);
++<<<<<<< HEAD
++=======
+ 	vfio_iommu_iova_free(&iova_copy);
+ 	vfio_iommu_resv_free(&group_resv_regions);
++>>>>>>> af029169b8fd (vfio/type1: Check reserved region conflict and update iova list)
  out_free:
  	kfree(domain);
  	kfree(group);
* Unmerged path drivers/vfio/vfio_iommu_type1.c
