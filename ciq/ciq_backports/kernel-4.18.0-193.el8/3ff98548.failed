drm/amdgpu: Export function to flush TLB of specific vm hub

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Oak Zeng <Oak.Zeng@amd.com>
commit 3ff985485b29693376bb470a40b7aba4394a189b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/3ff98548.failed

This is for kfd to reuse amdgpu TLB invalidation function.
On gfx10, kfd only needs to flush TLB on gfx hub but not
on mm hub. So export a function for KFD flush TLB only on
specific hub.

	Signed-off-by: Oak Zeng <Oak.Zeng@amd.com>
	Reviewed-by: Christian Konig <christian.koenig@amd.com>
	Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
(cherry picked from commit 3ff985485b29693376bb470a40b7aba4394a189b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
#	drivers/gpu/drm/amd/amdgpu/gfx_v10_0.c
#	drivers/gpu/drm/amd/amdgpu/gmc_v10_0.c
#	drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c
diff --cc drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
index 5c51d4910650,e262f2ac07a3..000000000000
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
@@@ -730,9 -639,76 +730,51 @@@ static uint16_t get_atc_vmid_pasid_mapp
  	return reg & ATC_VMID0_PASID_MAPPING__PASID_MASK;
  }
  
 -static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid,
 -			uint32_t flush_type)
 -{
 -	signed long r;
 -	uint32_t seq;
 -	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
 -
 -	spin_lock(&adev->gfx.kiq.ring_lock);
 -	amdgpu_ring_alloc(ring, 12); /* fence + invalidate_tlbs package*/
 -	amdgpu_ring_write(ring, PACKET3(PACKET3_INVALIDATE_TLBS, 0));
 -	amdgpu_ring_write(ring,
 -			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
 -			PACKET3_INVALIDATE_TLBS_ALL_HUB(1) |
 -			PACKET3_INVALIDATE_TLBS_PASID(pasid) |
 -			PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(flush_type));
 -	amdgpu_fence_emit_polling(ring, &seq);
 -	amdgpu_ring_commit(ring);
 -	spin_unlock(&adev->gfx.kiq.ring_lock);
 -
 -	r = amdgpu_fence_wait_polling(ring, seq, adev->usec_timeout);
 -	if (r < 1) {
 -		DRM_ERROR("wait for kiq fence error: %ld.\n", r);
 -		return -ETIME;
 -	}
 -
 -	return 0;
 -}
 -
 -int kgd_gfx_v9_invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 +static void write_vmid_invalidate_request(struct kgd_dev *kgd, uint8_t vmid)
  {
  	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
++<<<<<<< HEAD
++=======
+ 	int vmid, i;
+ 	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
+ 	uint32_t flush_type = 0;
+ 
+ 	if (adev->in_gpu_reset)
+ 		return -EIO;
+ 	if (adev->gmc.xgmi.num_physical_nodes &&
+ 		adev->asic_type == CHIP_VEGA20)
+ 		flush_type = 2;
+ 
+ 	if (ring->sched.ready)
+ 		return invalidate_tlbs_with_kiq(adev, pasid, flush_type);
+ 
+ 	for (vmid = 0; vmid < 16; vmid++) {
+ 		if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid))
+ 			continue;
+ 		if (kgd_gfx_v9_get_atc_vmid_pasid_mapping_valid(kgd, vmid)) {
+ 			if (kgd_gfx_v9_get_atc_vmid_pasid_mapping_pasid(kgd, vmid)
+ 				== pasid) {
+ 				for (i = 0; i < adev->num_vmhubs; i++)
+ 					amdgpu_gmc_flush_gpu_tlb(adev, vmid,
+ 								i, flush_type);
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int kgd_gfx_v9_invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
+ {
+ 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
+ 	int i;
+ 
+ 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
+ 		pr_err("non kfd vmid %d\n", vmid);
+ 		return 0;
+ 	}
++>>>>>>> 3ff985485b29 (drm/amdgpu: Export function to flush TLB of specific vm hub)
  
  	/* Use legacy mode tlb invalidation.
  	 *
@@@ -749,32 -725,8 +791,37 @@@
  	 * TODO 2: support range-based invalidation, requires kfg2kgd
  	 * interface change
  	 */
++<<<<<<< HEAD
 +	amdgpu_gmc_flush_gpu_tlb(adev, vmid, 0);
 +}
 +
 +static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
 +{
 +	signed long r;
 +	uint32_t seq;
 +	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
 +
 +	spin_lock(&adev->gfx.kiq.ring_lock);
 +	amdgpu_ring_alloc(ring, 12); /* fence + invalidate_tlbs package*/
 +	amdgpu_ring_write(ring, PACKET3(PACKET3_INVALIDATE_TLBS, 0));
 +	amdgpu_ring_write(ring,
 +			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
 +			PACKET3_INVALIDATE_TLBS_ALL_HUB(1) |
 +			PACKET3_INVALIDATE_TLBS_PASID(pasid) |
 +			PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(0)); /* legacy */
 +	amdgpu_fence_emit_polling(ring, &seq);
 +	amdgpu_ring_commit(ring);
 +	spin_unlock(&adev->gfx.kiq.ring_lock);
 +
 +	r = amdgpu_fence_wait_polling(ring, seq, adev->usec_timeout);
 +	if (r < 1) {
 +		DRM_ERROR("wait for kiq fence error: %ld.\n", r);
 +		return -ETIME;
 +	}
++=======
+ 	for (i = 0; i < adev->num_vmhubs; i++)
+ 		amdgpu_gmc_flush_gpu_tlb(adev, vmid, i, 0);
++>>>>>>> 3ff985485b29 (drm/amdgpu: Export function to flush TLB of specific vm hub)
  
  	return 0;
  }
diff --cc drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c
index c0446af00cdd,7da355bf6d89..000000000000
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c
@@@ -391,44 -453,45 +391,51 @@@ static uint32_t gmc_v9_0_get_invalidate
   *
   * Flush the TLB for the requested page table using certain type.
   */
- static void gmc_v9_0_flush_gpu_tlb(struct amdgpu_device *adev,
- 				uint32_t vmid, uint32_t flush_type)
+ static void gmc_v9_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,
+ 					uint32_t vmhub, uint32_t flush_type)
  {
  	const unsigned eng = 17;
- 	unsigned i, j;
+ 	u32 j, tmp;
+ 	struct amdgpu_vmhub *hub;
  
++<<<<<<< HEAD
 +	for (i = 0; i < AMDGPU_MAX_VMHUBS; ++i) {
 +		struct amdgpu_vmhub *hub = &adev->vmhub[i];
 +		u32 tmp = gmc_v9_0_get_invalidate_req(vmid, flush_type);
++=======
+ 	BUG_ON(vmhub >= adev->num_vmhubs);
++>>>>>>> 3ff985485b29 (drm/amdgpu: Export function to flush TLB of specific vm hub)
  
- 		/* This is necessary for a HW workaround under SRIOV as well
- 		 * as GFXOFF under bare metal
- 		 */
- 		if (adev->gfx.kiq.ring.sched.ready &&
- 		    (amdgpu_sriov_runtime(adev) || !amdgpu_sriov_vf(adev)) &&
- 		    !adev->in_gpu_reset) {
- 			uint32_t req = hub->vm_inv_eng0_req + eng;
- 			uint32_t ack = hub->vm_inv_eng0_ack + eng;
- 
- 			amdgpu_virt_kiq_reg_write_reg_wait(adev, req, ack, tmp,
- 							   1 << vmid);
- 			continue;
- 		}
+ 	hub = &adev->vmhub[vmhub];
+ 	tmp = gmc_v9_0_get_invalidate_req(vmid, flush_type);
  
- 		spin_lock(&adev->gmc.invalidate_lock);
- 		WREG32_NO_KIQ(hub->vm_inv_eng0_req + eng, tmp);
- 		for (j = 0; j < adev->usec_timeout; j++) {
- 			tmp = RREG32_NO_KIQ(hub->vm_inv_eng0_ack + eng);
- 			if (tmp & (1 << vmid))
- 				break;
- 			udelay(1);
- 		}
- 		spin_unlock(&adev->gmc.invalidate_lock);
- 		if (j < adev->usec_timeout)
- 			continue;
+ 	/* This is necessary for a HW workaround under SRIOV as well
+ 	 * as GFXOFF under bare metal
+ 	 */
+ 	if (adev->gfx.kiq.ring.sched.ready &&
+ 			(amdgpu_sriov_runtime(adev) || !amdgpu_sriov_vf(adev)) &&
+ 			!adev->in_gpu_reset) {
+ 		uint32_t req = hub->vm_inv_eng0_req + eng;
+ 		uint32_t ack = hub->vm_inv_eng0_ack + eng;
+ 
+ 		amdgpu_virt_kiq_reg_write_reg_wait(adev, req, ack, tmp,
+ 				1 << vmid);
+ 		return;
+ 	}
  
- 		DRM_ERROR("Timeout waiting for VM flush ACK!\n");
+ 	spin_lock(&adev->gmc.invalidate_lock);
+ 	WREG32_NO_KIQ(hub->vm_inv_eng0_req + eng, tmp);
+ 	for (j = 0; j < adev->usec_timeout; j++) {
+ 		tmp = RREG32_NO_KIQ(hub->vm_inv_eng0_ack + eng);
+ 		if (tmp & (1 << vmid))
+ 			break;
+ 		udelay(1);
  	}
+ 	spin_unlock(&adev->gmc.invalidate_lock);
+ 	if (j < adev->usec_timeout)
+ 		return;
+ 
+ 	DRM_ERROR("Timeout waiting for VM flush ACK!\n");
  }
  
  static uint64_t gmc_v9_0_emit_flush_gpu_tlb(struct amdgpu_ring *ring,
@@@ -1147,8 -1350,13 +1154,18 @@@ static int gmc_v9_0_gart_enable(struct 
  		value = true;
  
  	gfxhub_v1_0_set_fault_enable_default(adev, value);
++<<<<<<< HEAD
 +	mmhub_v1_0_set_fault_enable_default(adev, value);
 +	gmc_v9_0_flush_gpu_tlb(adev, 0, 0);
++=======
+ 	if (adev->asic_type == CHIP_ARCTURUS)
+ 		mmhub_v9_4_set_fault_enable_default(adev, value);
+ 	else
+ 		mmhub_v1_0_set_fault_enable_default(adev, value);
+ 
+ 	for (i = 0; i < adev->num_vmhubs; ++i)
+ 		gmc_v9_0_flush_gpu_tlb(adev, 0, i, 0);
++>>>>>>> 3ff985485b29 (drm/amdgpu: Export function to flush TLB of specific vm hub)
  
  	DRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",
  		 (unsigned)(adev->gmc.gart_size >> 20),
* Unmerged path drivers/gpu/drm/amd/amdgpu/gfx_v10_0.c
* Unmerged path drivers/gpu/drm/amd/amdgpu/gmc_v10_0.c
* Unmerged path drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c
index 6d11e1721147..a67ffff6098c 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c
@@ -248,7 +248,9 @@ int amdgpu_gart_unbind(struct amdgpu_device *adev, uint64_t offset,
 	}
 	mb();
 	amdgpu_asic_flush_hdp(adev, NULL);
-	amdgpu_gmc_flush_gpu_tlb(adev, 0, 0);
+	for (i = 0; i < adev->num_vmhubs; i++)
+		amdgpu_gmc_flush_gpu_tlb(adev, 0, i, 0);
+
 	return 0;
 }
 
@@ -309,7 +311,7 @@ int amdgpu_gart_bind(struct amdgpu_device *adev, uint64_t offset,
 #ifdef CONFIG_DRM_AMDGPU_GART_DEBUGFS
 	unsigned i,t,p;
 #endif
-	int r;
+	int r, i;
 
 	if (!adev->gart.ready) {
 		WARN(1, "trying to bind memory to uninitialized GART !\n");
@@ -333,7 +335,8 @@ int amdgpu_gart_bind(struct amdgpu_device *adev, uint64_t offset,
 
 	mb();
 	amdgpu_asic_flush_hdp(adev, NULL);
-	amdgpu_gmc_flush_gpu_tlb(adev, 0, 0);
+	for (i = 0; i < adev->num_vmhubs; i++)
+		amdgpu_gmc_flush_gpu_tlb(adev, 0, i, 0);
 	return 0;
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gmc.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_gmc.h
index 81e6070d255b..dd1017f5b9b0 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gmc.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gmc.h
@@ -63,8 +63,8 @@ struct amdgpu_vmhub {
  */
 struct amdgpu_gmc_funcs {
 	/* flush the vm tlb via mmio */
-	void (*flush_gpu_tlb)(struct amdgpu_device *adev,
-			      uint32_t vmid, uint32_t flush_type);
+	void (*flush_gpu_tlb)(struct amdgpu_device *adev, uint32_t vmid,
+				uint32_t vmhub, uint32_t flush_type);
 	/* flush the vm tlb via ring */
 	uint64_t (*emit_flush_gpu_tlb)(struct amdgpu_ring *ring, unsigned vmid,
 				       uint64_t pd_addr);
@@ -152,7 +152,7 @@ struct amdgpu_gmc {
 	struct amdgpu_xgmi xgmi;
 };
 
-#define amdgpu_gmc_flush_gpu_tlb(adev, vmid, type) (adev)->gmc.gmc_funcs->flush_gpu_tlb((adev), (vmid), (type))
+#define amdgpu_gmc_flush_gpu_tlb(adev, vmid, vmhub, type) ((adev)->gmc.gmc_funcs->flush_gpu_tlb((adev), (vmid), (vmhub), (type)))
 #define amdgpu_gmc_emit_flush_gpu_tlb(r, vmid, addr) (r)->adev->gmc.gmc_funcs->emit_flush_gpu_tlb((r), (vmid), (addr))
 #define amdgpu_gmc_emit_pasid_mapping(r, vmid, pasid) (r)->adev->gmc.gmc_funcs->emit_pasid_mapping((r), (vmid), (pasid))
 #define amdgpu_gmc_set_pte_pde(adev, pt, idx, addr, flags) (adev)->gmc.gmc_funcs->set_pte_pde((adev), (pt), (idx), (addr), (flags))
* Unmerged path drivers/gpu/drm/amd/amdgpu/gfx_v10_0.c
* Unmerged path drivers/gpu/drm/amd/amdgpu/gmc_v10_0.c
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v6_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v6_0.c
index 9fc3296592fe..a080ffef6b32 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v6_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v6_0.c
@@ -359,8 +359,8 @@ static int gmc_v6_0_mc_init(struct amdgpu_device *adev)
 	return 0;
 }
 
-static void gmc_v6_0_flush_gpu_tlb(struct amdgpu_device *adev,
-				uint32_t vmid, uint32_t flush_type)
+static void gmc_v6_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,
+					uint32_t vmhub, uint32_t flush_type)
 {
 	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
 }
@@ -582,7 +582,7 @@ static int gmc_v6_0_gart_enable(struct amdgpu_device *adev)
 	else
 		gmc_v6_0_set_fault_enable_default(adev, true);
 
-	gmc_v6_0_flush_gpu_tlb(adev, 0, 0);
+	gmc_v6_0_flush_gpu_tlb(adev, 0, 0, 0);
 	dev_info(adev->dev, "PCIE GART of %uM enabled (table at 0x%016llX).\n",
 		 (unsigned)(adev->gmc.gart_size >> 20),
 		 (unsigned long long)table_addr);
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
index 761dcfb2fec0..53a3fba3449f 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c
@@ -430,8 +430,8 @@ static int gmc_v7_0_mc_init(struct amdgpu_device *adev)
  *
  * Flush the TLB for the requested page table (CIK).
  */
-static void gmc_v7_0_flush_gpu_tlb(struct amdgpu_device *adev,
-				uint32_t vmid, uint32_t flush_type)
+static void gmc_v7_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,
+					uint32_t vmhub, uint32_t flush_type)
 {
 	/* bits 0-15 are the VM contexts0-15 */
 	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
@@ -699,7 +699,7 @@ static int gmc_v7_0_gart_enable(struct amdgpu_device *adev)
 		WREG32(mmCHUB_CONTROL, tmp);
 	}
 
-	gmc_v7_0_flush_gpu_tlb(adev, 0, 0);
+	gmc_v7_0_flush_gpu_tlb(adev, 0, 0, 0);
 	DRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",
 		 (unsigned)(adev->gmc.gart_size >> 20),
 		 (unsigned long long)table_addr);
diff --git a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
index 34440672f938..7c76851beca4 100644
--- a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
+++ b/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c
@@ -632,8 +632,8 @@ static int gmc_v8_0_mc_init(struct amdgpu_device *adev)
  *
  * Flush the TLB for the requested page table (CIK).
  */
-static void gmc_v8_0_flush_gpu_tlb(struct amdgpu_device *adev,
-				uint32_t vmid, uint32_t flush_type)
+static void gmc_v8_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,
+					uint32_t vmhub, uint32_t flush_type)
 {
 	/* bits 0-15 are the VM contexts0-15 */
 	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
@@ -942,7 +942,7 @@ static int gmc_v8_0_gart_enable(struct amdgpu_device *adev)
 	else
 		gmc_v8_0_set_fault_enable_default(adev, true);
 
-	gmc_v8_0_flush_gpu_tlb(adev, 0, 0);
+	gmc_v8_0_flush_gpu_tlb(adev, 0, 0, 0);
 	DRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",
 		 (unsigned)(adev->gmc.gart_size >> 20),
 		 (unsigned long long)table_addr);
* Unmerged path drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c
