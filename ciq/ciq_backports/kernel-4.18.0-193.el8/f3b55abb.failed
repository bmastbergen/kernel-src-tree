selftests: bpf: adjust several test_verifier helpers for insn insertion

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jiong Wang <jiong.wang@netronome.com>
commit f3b55abb6d5a522228e136c3bc4a9a716d5d8a54
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f3b55abb.failed

- bpf_fill_ld_abs_vlan_push_pop:
    Prevent zext happens inside PUSH_CNT loop. This could happen because
    of BPF_LD_ABS (32-bit def) + BPF_JMP (64-bit use), or BPF_LD_ABS +
    EXIT (64-bit use of R0). So, change BPF_JMP to BPF_JMP32 and redefine
    R0 at exit path to cut off the data-flow from inside the loop.

  - bpf_fill_jump_around_ld_abs:
    Jump range is limited to 16 bit. every ld_abs is replaced by 6 insns,
    but on arches like arm, ppc etc, there will be one BPF_ZEXT inserted
    to extend the error value of the inlined ld_abs sequence which then
    contains 7 insns. so, set the dividend to 7 so the testcase could
    work on all arches.

  - bpf_fill_scale1/bpf_fill_scale2:
    Both contains ~1M BPF_ALU32_IMM which will trigger ~1M insn patcher
    call because of hi32 randomization later when BPF_F_TEST_RND_HI32 is
    set for bpf selftests. Insn patcher is not efficient that 1M call to
    it will hang computer. So , change to BPF_ALU64_IMM to avoid hi32
    randomization.

	Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit f3b55abb6d5a522228e136c3bc4a9a716d5d8a54)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/bpf/test_verifier.c
diff --cc tools/testing/selftests/bpf/test_verifier.c
index 96071fae318d,fa9b5bfe5d9f..000000000000
--- a/tools/testing/selftests/bpf/test_verifier.c
+++ b/tools/testing/selftests/bpf/test_verifier.c
@@@ -153,15 -163,25 +154,29 @@@ loop
  	if (++k < 5)
  		goto loop;
  
- 	for (; i < len - 1; i++)
- 		insn[i] = BPF_ALU32_IMM(BPF_MOV, BPF_REG_0, 0xbef);
+ 	for (; i < len - 3; i++)
+ 		insn[i] = BPF_ALU64_IMM(BPF_MOV, BPF_REG_0, 0xbef);
+ 	insn[len - 3] = BPF_JMP_A(1);
+ 	/* error label */
+ 	insn[len - 2] = BPF_MOV32_IMM(BPF_REG_0, 0);
  	insn[len - 1] = BPF_EXIT_INSN();
 -	self->prog_len = len;
  }
  
  static void bpf_fill_jump_around_ld_abs(struct bpf_test *self)
  {
++<<<<<<< HEAD
 +	struct bpf_insn *insn = self->insns;
 +	unsigned int len = BPF_MAXINSNS;
++=======
+ 	struct bpf_insn *insn = self->fill_insns;
+ 	/* jump range is limited to 16 bit. every ld_abs is replaced by 6 insns,
+ 	 * but on arches like arm, ppc etc, there will be one BPF_ZEXT inserted
+ 	 * to extend the error value of the inlined ld_abs sequence which then
+ 	 * contains 7 insns. so, set the dividend to 7 so the testcase could
+ 	 * work on all arches.
+ 	 */
+ 	unsigned int len = (1 << 15) / 7;
++>>>>>>> f3b55abb6d5a (selftests: bpf: adjust several test_verifier helpers for insn insertion)
  	int i = 0;
  
  	insn[i++] = BPF_MOV64_REG(BPF_REG_6, BPF_REG_1);
* Unmerged path tools/testing/selftests/bpf/test_verifier.c
