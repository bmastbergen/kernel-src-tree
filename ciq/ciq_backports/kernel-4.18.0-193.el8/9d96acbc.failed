SUNRPC: Add a bvec array to struct xdr_buf for use with iovec_iter()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Trond Myklebust <trond.myklebust@hammerspace.com>
commit 9d96acbc7f376dc1ffcedca0c349dd3389187a38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/9d96acbc.failed

Add a bvec array to struct xdr_buf, and have the client allocate it
when we need to receive data into pages.

	Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
(cherry picked from commit 9d96acbc7f376dc1ffcedca0c349dd3389187a38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sunrpc/xprt.h
#	net/sunrpc/clnt.c
#	net/sunrpc/xprt.c
diff --cc include/linux/sunrpc/xprt.h
index e7472a422479,a4ab4f8d9140..000000000000
--- a/include/linux/sunrpc/xprt.h
+++ b/include/linux/sunrpc/xprt.h
@@@ -134,7 -141,8 +134,12 @@@ struct rpc_xprt_ops 
  	void		(*connect)(struct rpc_xprt *xprt, struct rpc_task *task);
  	int		(*buf_alloc)(struct rpc_task *task);
  	void		(*buf_free)(struct rpc_task *task);
++<<<<<<< HEAD
 +	int		(*send_request)(struct rpc_rqst *req, struct rpc_task *task);
++=======
+ 	void		(*prepare_request)(struct rpc_rqst *req);
+ 	int		(*send_request)(struct rpc_rqst *req);
++>>>>>>> 9d96acbc7f37 (SUNRPC: Add a bvec array to struct xdr_buf for use with iovec_iter())
  	void		(*set_retrans_timeout)(struct rpc_task *task);
  	void		(*timer)(struct rpc_xprt *xprt, struct rpc_task *task);
  	void		(*release_request)(struct rpc_task *task);
@@@ -332,7 -344,12 +337,8 @@@ int			xprt_reserve_xprt_cong(struct rpc
  void			xprt_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task);
  void			xprt_free_slot(struct rpc_xprt *xprt,
  				       struct rpc_rqst *req);
+ void			xprt_request_prepare(struct rpc_rqst *req);
  bool			xprt_prepare_transmit(struct rpc_task *task);
 -void			xprt_request_enqueue_transmit(struct rpc_task *task);
 -void			xprt_request_enqueue_receive(struct rpc_task *task);
 -void			xprt_request_wait_receive(struct rpc_task *task);
 -bool			xprt_request_need_retransmit(struct rpc_task *task);
  void			xprt_transmit(struct rpc_task *task);
  void			xprt_end_transmit(struct rpc_task *task);
  int			xprt_adjust_timeout(struct rpc_rqst *req);
diff --cc net/sunrpc/clnt.c
index d6ca4ed0e072,ae3b8145da35..000000000000
--- a/net/sunrpc/clnt.c
+++ b/net/sunrpc/clnt.c
@@@ -1760,7 -1754,35 +1760,39 @@@ rpc_xdr_encode(struct rpc_task *task
  	task->tk_status = rpcauth_wrap_req(task, encode, req, p,
  			task->tk_msg.rpc_argp);
  	if (task->tk_status == 0)
++<<<<<<< HEAD
 +		set_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);
++=======
+ 		xprt_request_prepare(req);
+ }
+ 
+ /*
+  * 3.	Encode arguments of an RPC call
+  */
+ static void
+ call_encode(struct rpc_task *task)
+ {
+ 	if (!rpc_task_need_encode(task))
+ 		goto out;
+ 	/* Encode here so that rpcsec_gss can use correct sequence number. */
+ 	rpc_xdr_encode(task);
+ 	/* Did the encode result in an error condition? */
+ 	if (task->tk_status != 0) {
+ 		/* Was the error nonfatal? */
+ 		if (task->tk_status == -EAGAIN || task->tk_status == -ENOMEM)
+ 			rpc_delay(task, HZ >> 4);
+ 		else
+ 			rpc_exit(task, task->tk_status);
+ 		return;
+ 	}
+ 
+ 	/* Add task to reply queue before transmission to avoid races */
+ 	if (rpc_reply_expected(task))
+ 		xprt_request_enqueue_receive(task);
+ 	xprt_request_enqueue_transmit(task);
+ out:
+ 	task->tk_action = call_bind;
++>>>>>>> 9d96acbc7f37 (SUNRPC: Add a bvec array to struct xdr_buf for use with iovec_iter())
  }
  
  /*
diff --cc net/sunrpc/xprt.c
index 4f9f6015515a,7ee9f1e996db..000000000000
--- a/net/sunrpc/xprt.c
+++ b/net/sunrpc/xprt.c
@@@ -946,6 -1126,172 +946,175 @@@ static void xprt_timer(struct rpc_task 
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * xprt_request_wait_receive - wait for the reply to an RPC request
+  * @task: RPC task about to send a request
+  *
+  */
+ void xprt_request_wait_receive(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	if (!test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))
+ 		return;
+ 	/*
+ 	 * Sleep on the pending queue if we're expecting a reply.
+ 	 * The spinlock ensures atomicity between the test of
+ 	 * req->rq_reply_bytes_recvd, and the call to rpc_sleep_on().
+ 	 */
+ 	spin_lock(&xprt->queue_lock);
+ 	if (test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate)) {
+ 		xprt->ops->set_retrans_timeout(task);
+ 		rpc_sleep_on(&xprt->pending, task, xprt_timer);
+ 		/*
+ 		 * Send an extra queue wakeup call if the
+ 		 * connection was dropped in case the call to
+ 		 * rpc_sleep_on() raced.
+ 		 */
+ 		if (xprt_request_retransmit_after_disconnect(task))
+ 			rpc_wake_up_queued_task_set_status(&xprt->pending,
+ 					task, -ENOTCONN);
+ 	}
+ 	spin_unlock(&xprt->queue_lock);
+ }
+ 
+ static bool
+ xprt_request_need_enqueue_transmit(struct rpc_task *task, struct rpc_rqst *req)
+ {
+ 	return !test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);
+ }
+ 
+ /**
+  * xprt_request_enqueue_transmit - queue a task for transmission
+  * @task: pointer to rpc_task
+  *
+  * Add a task to the transmission queue.
+  */
+ void
+ xprt_request_enqueue_transmit(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *pos, *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	if (xprt_request_need_enqueue_transmit(task, req)) {
+ 		spin_lock(&xprt->queue_lock);
+ 		/*
+ 		 * Requests that carry congestion control credits are added
+ 		 * to the head of the list to avoid starvation issues.
+ 		 */
+ 		if (req->rq_cong) {
+ 			xprt_clear_congestion_window_wait(xprt);
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_cong)
+ 					continue;
+ 				/* Note: req is added _before_ pos */
+ 				list_add_tail(&req->rq_xmit, &pos->rq_xmit);
+ 				INIT_LIST_HEAD(&req->rq_xmit2);
+ 				goto out;
+ 			}
+ 		} else if (RPC_IS_SWAPPER(task)) {
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_cong || pos->rq_bytes_sent)
+ 					continue;
+ 				if (RPC_IS_SWAPPER(pos->rq_task))
+ 					continue;
+ 				/* Note: req is added _before_ pos */
+ 				list_add_tail(&req->rq_xmit, &pos->rq_xmit);
+ 				INIT_LIST_HEAD(&req->rq_xmit2);
+ 				goto out;
+ 			}
+ 		} else {
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_task->tk_owner != task->tk_owner)
+ 					continue;
+ 				list_add_tail(&req->rq_xmit2, &pos->rq_xmit2);
+ 				INIT_LIST_HEAD(&req->rq_xmit);
+ 				goto out;
+ 			}
+ 		}
+ 		list_add_tail(&req->rq_xmit, &xprt->xmit_queue);
+ 		INIT_LIST_HEAD(&req->rq_xmit2);
+ out:
+ 		set_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);
+ 		spin_unlock(&xprt->queue_lock);
+ 	}
+ }
+ 
+ /**
+  * xprt_request_dequeue_transmit_locked - remove a task from the transmission queue
+  * @task: pointer to rpc_task
+  *
+  * Remove a task from the transmission queue
+  * Caller must hold xprt->queue_lock
+  */
+ static void
+ xprt_request_dequeue_transmit_locked(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 
+ 	if (!test_and_clear_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))
+ 		return;
+ 	if (!list_empty(&req->rq_xmit)) {
+ 		list_del(&req->rq_xmit);
+ 		if (!list_empty(&req->rq_xmit2)) {
+ 			struct rpc_rqst *next = list_first_entry(&req->rq_xmit2,
+ 					struct rpc_rqst, rq_xmit2);
+ 			list_del(&req->rq_xmit2);
+ 			list_add_tail(&next->rq_xmit, &next->rq_xprt->xmit_queue);
+ 		}
+ 	} else
+ 		list_del(&req->rq_xmit2);
+ }
+ 
+ /**
+  * xprt_request_dequeue_transmit - remove a task from the transmission queue
+  * @task: pointer to rpc_task
+  *
+  * Remove a task from the transmission queue
+  */
+ static void
+ xprt_request_dequeue_transmit(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	spin_lock(&xprt->queue_lock);
+ 	xprt_request_dequeue_transmit_locked(task);
+ 	spin_unlock(&xprt->queue_lock);
+ }
+ 
+ /**
+  * xprt_request_prepare - prepare an encoded request for transport
+  * @req: pointer to rpc_rqst
+  *
+  * Calls into the transport layer to do whatever is needed to prepare
+  * the request for transmission or receive.
+  */
+ void
+ xprt_request_prepare(struct rpc_rqst *req)
+ {
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	if (xprt->ops->prepare_request)
+ 		xprt->ops->prepare_request(req);
+ }
+ 
+ /**
+  * xprt_request_need_retransmit - Test if a task needs retransmission
+  * @task: pointer to rpc_task
+  *
+  * Test for whether a connection breakage requires the task to retransmit
+  */
+ bool
+ xprt_request_need_retransmit(struct rpc_task *task)
+ {
+ 	return xprt_request_retransmit_after_disconnect(task);
+ }
+ 
+ /**
++>>>>>>> 9d96acbc7f37 (SUNRPC: Add a bvec array to struct xdr_buf for use with iovec_iter())
   * xprt_prepare_transmit - reserve the transport before sending a request
   * @task: RPC task about to send a request
   *
diff --git a/include/linux/sunrpc/xdr.h b/include/linux/sunrpc/xdr.h
index 431829233392..745587132a87 100644
--- a/include/linux/sunrpc/xdr.h
+++ b/include/linux/sunrpc/xdr.h
@@ -18,6 +18,7 @@
 #include <asm/unaligned.h>
 #include <linux/scatterlist.h>
 
+struct bio_vec;
 struct rpc_rqst;
 
 /*
@@ -52,6 +53,7 @@ struct xdr_buf {
 	struct kvec	head[1],	/* RPC header + non-page data */
 			tail[1];	/* Appended after page data */
 
+	struct bio_vec	*bvec;
 	struct page **	pages;		/* Array of pages */
 	unsigned int	page_base,	/* Start of page data */
 			page_len,	/* Length of page data */
@@ -70,6 +72,8 @@ xdr_buf_init(struct xdr_buf *buf, void *start, size_t len)
 	buf->head[0].iov_base = start;
 	buf->head[0].iov_len = len;
 	buf->tail[0].iov_len = 0;
+	buf->bvec = NULL;
+	buf->pages = NULL;
 	buf->page_len = 0;
 	buf->flags = 0;
 	buf->len = 0;
@@ -116,6 +120,9 @@ __be32 *xdr_decode_netobj(__be32 *p, struct xdr_netobj *);
 void	xdr_inline_pages(struct xdr_buf *, unsigned int,
 			 struct page **, unsigned int, unsigned int);
 void	xdr_terminate_string(struct xdr_buf *, const u32);
+size_t	xdr_buf_pagecount(struct xdr_buf *buf);
+int	xdr_alloc_bvec(struct xdr_buf *buf, gfp_t gfp);
+void	xdr_free_bvec(struct xdr_buf *buf);
 
 static inline __be32 *xdr_encode_array(__be32 *p, const void *s, unsigned int len)
 {
* Unmerged path include/linux/sunrpc/xprt.h
* Unmerged path net/sunrpc/clnt.c
diff --git a/net/sunrpc/xdr.c b/net/sunrpc/xdr.c
index 30afbd236656..2bbb8d38d2bf 100644
--- a/net/sunrpc/xdr.c
+++ b/net/sunrpc/xdr.c
@@ -15,6 +15,7 @@
 #include <linux/errno.h>
 #include <linux/sunrpc/xdr.h>
 #include <linux/sunrpc/msg_prot.h>
+#include <linux/bvec.h>
 
 /*
  * XDR functions for basic NFS types
@@ -128,6 +129,39 @@ xdr_terminate_string(struct xdr_buf *buf, const u32 len)
 }
 EXPORT_SYMBOL_GPL(xdr_terminate_string);
 
+size_t
+xdr_buf_pagecount(struct xdr_buf *buf)
+{
+	if (!buf->page_len)
+		return 0;
+	return (buf->page_base + buf->page_len + PAGE_SIZE - 1) >> PAGE_SHIFT;
+}
+
+int
+xdr_alloc_bvec(struct xdr_buf *buf, gfp_t gfp)
+{
+	size_t i, n = xdr_buf_pagecount(buf);
+
+	if (n != 0 && buf->bvec == NULL) {
+		buf->bvec = kmalloc_array(n, sizeof(buf->bvec[0]), gfp);
+		if (!buf->bvec)
+			return -ENOMEM;
+		for (i = 0; i < n; i++) {
+			buf->bvec[i].bv_page = buf->pages[i];
+			buf->bvec[i].bv_len = PAGE_SIZE;
+			buf->bvec[i].bv_offset = 0;
+		}
+	}
+	return 0;
+}
+
+void
+xdr_free_bvec(struct xdr_buf *buf)
+{
+	kfree(buf->bvec);
+	buf->bvec = NULL;
+}
+
 void
 xdr_inline_pages(struct xdr_buf *xdr, unsigned int offset,
 		 struct page **pages, unsigned int base, unsigned int len)
* Unmerged path net/sunrpc/xprt.c
