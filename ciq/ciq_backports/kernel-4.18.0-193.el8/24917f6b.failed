memremap: provide an optional internal refcount in struct dev_pagemap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 24917f6b1041a73993178920656e13364f847995
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/24917f6b.failed

Provide an internal refcounting logic if no ->ref field is provided
in the pagemap passed into devm_memremap_pages so that callers don't
have to reinvent it poorly.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Tested-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 24917f6b1041a73993178920656e13364f847995)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memremap.h
#	kernel/memremap.c
#	tools/testing/nvdimm/test/iomap.c
diff --cc include/linux/memremap.h
index ae6713454b27,f8a5b2a19945..000000000000
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@@ -112,25 -95,34 +112,32 @@@ typedef void (*dev_page_free_t)(struct 
   * @altmap: pre-allocated/reserved memory for vmemmap allocations
   * @res: physical address range covered by @ref
   * @ref: reference count that pins the devm_memremap_pages() mapping
++<<<<<<< HEAD
 + * @kill: callback to transition @ref to the dead state
++=======
+  * @internal_ref: internal reference if @ref is not provided by the caller
+  * @done: completion for @internal_ref
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
   * @dev: host device of the mapping for debug
   * @data: private data pointer for page_free()
   * @type: memory type: see MEMORY_* in memory_hotplug.h
 - * @flags: PGMAP_* flags to specify defailed behavior
 - * @ops: method table
   */
  struct dev_pagemap {
 +	dev_page_fault_t page_fault;
 +	dev_page_free_t page_free;
  	struct vmem_altmap altmap;
 +	bool altmap_valid;
  	struct resource res;
  	struct percpu_ref *ref;
+ 	struct percpu_ref internal_ref;
+ 	struct completion done;
  	struct device *dev;
 +	void *data;
  	enum memory_type type;
 -	unsigned int flags;
  	u64 pci_p2pdma_bus_offset;
 -	const struct dev_pagemap_ops *ops;
 +	RH_KABI_EXTEND(void (*kill)(struct percpu_ref *ref))
  };
  
 -static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
 -{
 -	if (pgmap->flags & PGMAP_ALTMAP_VALID)
 -		return &pgmap->altmap;
 -	return NULL;
 -}
 -
  #ifdef CONFIG_ZONE_DEVICE
  void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap);
  void devm_memunmap_pages(struct device *dev, struct dev_pagemap *pgmap);
diff --cc kernel/memremap.c
index 794888559eb7,bea6f887adad..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -17,70 -16,39 +17,76 @@@ static RADIX_TREE(pgmap_radix, GFP_KERN
  #define SECTION_MASK ~((1UL << PA_SECTION_SHIFT) - 1)
  #define SECTION_SIZE (1UL << PA_SECTION_SHIFT)
  
 -#ifdef CONFIG_DEV_PAGEMAP_OPS
 -DEFINE_STATIC_KEY_FALSE(devmap_managed_key);
 -EXPORT_SYMBOL(devmap_managed_key);
 -static atomic_t devmap_managed_enable;
 +static unsigned long order_at(struct resource *res, unsigned long pgoff)
 +{
 +	unsigned long phys_pgoff = PHYS_PFN(res->start) + pgoff;
 +	unsigned long nr_pages, mask;
 +
 +	nr_pages = PHYS_PFN(resource_size(res));
 +	if (nr_pages == pgoff)
 +		return ULONG_MAX;
 +
 +	/*
 +	 * What is the largest aligned power-of-2 range available from
 +	 * this resource pgoff to the end of the resource range,
 +	 * considering the alignment of the current pgoff?
 +	 */
 +	mask = phys_pgoff | rounddown_pow_of_two(nr_pages - pgoff);
 +	if (!mask)
 +		return ULONG_MAX;
 +
 +	return find_first_bit(&mask, BITS_PER_LONG);
 +}
 +
 +#define foreach_order_pgoff(res, order, pgoff) \
 +	for (pgoff = 0, order = order_at((res), pgoff); order < ULONG_MAX; \
 +			pgoff += 1UL << order, order = order_at((res), pgoff))
  
 -static void devmap_managed_enable_put(void *data)
 +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)
 +int device_private_entry_fault(struct vm_area_struct *vma,
 +		       unsigned long addr,
 +		       swp_entry_t entry,
 +		       unsigned int flags,
 +		       pmd_t *pmdp)
  {
 -	if (atomic_dec_and_test(&devmap_managed_enable))
 -		static_branch_disable(&devmap_managed_key);
++<<<<<<< HEAD
 +	struct page *page = device_private_entry_to_page(entry);
 +
 +	/*
 +	 * The page_fault() callback must migrate page back to system memory
 +	 * so that CPU can access it. This might fail for various reasons
 +	 * (device issue, device was unsafely unplugged, ...). When such
 +	 * error conditions happen, the callback must return VM_FAULT_SIGBUS.
 +	 *
 +	 * Note that because memory cgroup charges are accounted to the device
 +	 * memory, this should never fail because of memory restrictions (but
 +	 * allocation of regular system page might still fail because we are
 +	 * out of memory).
 +	 *
 +	 * There is a more in-depth description of what that callback can and
 +	 * cannot do, in include/linux/memremap.h
 +	 */
 +	return page->pgmap->page_fault(vma, addr, page, flags, pmdp);
  }
 +#endif /* CONFIG_DEVICE_PRIVATE */
  
 -static int devmap_managed_enable_get(struct device *dev, struct dev_pagemap *pgmap)
 +static void pgmap_radix_release(struct resource *res, unsigned long end_pgoff)
  {
 +	unsigned long pgoff, order;
 +
 +	mutex_lock(&pgmap_lock);
 +	foreach_order_pgoff(res, order, pgoff) {
 +		if (pgoff >= end_pgoff)
 +			break;
 +		radix_tree_delete(&pgmap_radix, PHYS_PFN(res->start) + pgoff);
++=======
+ 	if (!pgmap->ops || !pgmap->ops->page_free) {
+ 		WARN(1, "Missing page_free method\n");
+ 		return -EINVAL;
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
  	}
 +	mutex_unlock(&pgmap_lock);
  
 -	if (atomic_inc_return(&devmap_managed_enable) == 1)
 -		static_branch_enable(&devmap_managed_key);
 -	return devm_add_action_or_reset(dev, devmap_managed_enable_put, NULL);
 -}
 -#else
 -static int devmap_managed_enable_get(struct device *dev, struct dev_pagemap *pgmap)
 -{
 -	return -EINVAL;
 -}
 -#endif /* CONFIG_DEV_PAGEMAP_OPS */
 -
 -static void pgmap_array_delete(struct resource *res)
 -{
 -	xa_store_range(&pgmap_array, PHYS_PFN(res->start), PHYS_PFN(res->end),
 -			NULL, GFP_KERNEL);
  	synchronize_rcu();
  }
  
@@@ -122,9 -102,10 +146,16 @@@ static void devm_memremap_pages_release
  	unsigned long pfn;
  	int nid;
  
++<<<<<<< HEAD
 +	pgmap->kill(pgmap->ref);
 +	for_each_device_pfn(pfn, pgmap)
 +		put_page(pfn_to_page(pfn));
++=======
+ 	dev_pagemap_kill(pgmap);
+ 	for_each_device_pfn(pfn, pgmap)
+ 		put_page(pfn_to_page(pfn));
+ 	dev_pagemap_cleanup(pgmap);
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
  
  	/* pages are dead and unused, undo the arch mapping */
  	align_start = res->start & ~(SECTION_SIZE - 1);
@@@ -157,13 -146,14 +196,22 @@@ static void dev_pagemap_percpu_release(
   * @pgmap: pointer to a struct dev_pagemap
   *
   * Notes:
++<<<<<<< HEAD
 + * 1/ At a minimum the res, ref and type members of @pgmap must be initialized
++=======
+  * 1/ At a minimum the res and type members of @pgmap must be initialized
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
   *    by the caller before passing it to this function
   *
 - * 2/ The altmap field may optionally be initialized, in which case
 - *    PGMAP_ALTMAP_VALID must be set in pgmap->flags.
 + * 2/ The altmap field may optionally be initialized, in which case altmap_valid
 + *    must be set to true
   *
++<<<<<<< HEAD
 + * 3/ pgmap->ref must be 'live' on entry and will be killed at
++=======
+  * 3/ The ref field may optionally be provided, in which pgmap->ref must be
+  *    'live' on entry and will be killed and reaped at
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
   *    devm_memremap_pages_release() time, or if this routine fails.
   *
   * 4/ res is expected to be a host memory range that could feasibly be
@@@ -173,16 -163,67 +221,68 @@@
  void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap)
  {
  	resource_size_t align_start, align_size, align_end;
 +	struct vmem_altmap *altmap = pgmap->altmap_valid ?
 +			&pgmap->altmap : NULL;
  	struct resource *res = &pgmap->res;
  	struct dev_pagemap *conflict_pgmap;
 -	struct mhp_restrictions restrictions = {
 -		/*
 -		 * We do not want any optional features only our own memmap
 -		*/
 -		.altmap = pgmap_altmap(pgmap),
 -	};
  	pgprot_t pgprot = PAGE_KERNEL;
 +	unsigned long pgoff, order;
  	int error, nid, is_ram;
 -	bool need_devmap_managed = true;
  
++<<<<<<< HEAD
 +	if (!pgmap->ref || !pgmap->kill)
 +		return ERR_PTR(-EINVAL);
++=======
+ 	switch (pgmap->type) {
+ 	case MEMORY_DEVICE_PRIVATE:
+ 		if (!IS_ENABLED(CONFIG_DEVICE_PRIVATE)) {
+ 			WARN(1, "Device private memory not supported\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 		if (!pgmap->ops || !pgmap->ops->migrate_to_ram) {
+ 			WARN(1, "Missing migrate_to_ram method\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 		break;
+ 	case MEMORY_DEVICE_FS_DAX:
+ 		if (!IS_ENABLED(CONFIG_ZONE_DEVICE) ||
+ 		    IS_ENABLED(CONFIG_FS_DAX_LIMITED)) {
+ 			WARN(1, "File system DAX not supported\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 		break;
+ 	case MEMORY_DEVICE_DEVDAX:
+ 	case MEMORY_DEVICE_PCI_P2PDMA:
+ 		need_devmap_managed = false;
+ 		break;
+ 	default:
+ 		WARN(1, "Invalid pgmap type %d\n", pgmap->type);
+ 		break;
+ 	}
+ 
+ 	if (!pgmap->ref) {
+ 		if (pgmap->ops && (pgmap->ops->kill || pgmap->ops->cleanup))
+ 			return ERR_PTR(-EINVAL);
+ 
+ 		init_completion(&pgmap->done);
+ 		error = percpu_ref_init(&pgmap->internal_ref,
+ 				dev_pagemap_percpu_release, 0, GFP_KERNEL);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 		pgmap->ref = &pgmap->internal_ref;
+ 	} else {
+ 		if (!pgmap->ops || !pgmap->ops->kill || !pgmap->ops->cleanup) {
+ 			WARN(1, "Missing reference count teardown definition\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 	}
+ 
+ 	if (need_devmap_managed) {
+ 		error = devmap_managed_enable_get(dev, pgmap);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 	}
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
  
  	align_start = res->start & ~(SECTION_SIZE - 1);
  	align_size = ALIGN(res->start + resource_size(res), SECTION_SIZE)
@@@ -299,10 -332,10 +399,17 @@@
   err_kasan:
  	untrack_pfn(NULL, PHYS_PFN(align_start), align_size);
   err_pfn_remap:
++<<<<<<< HEAD
 + err_radix:
 +	pgmap_radix_release(res, pgoff);
 + err_pgmap:
 +	pgmap->kill(pgmap->ref);
++=======
+ 	pgmap_array_delete(res);
+  err_array:
+ 	dev_pagemap_kill(pgmap);
+ 	dev_pagemap_cleanup(pgmap);
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
  	return ERR_PTR(error);
  }
  EXPORT_SYMBOL_GPL(devm_memremap_pages);
diff --cc tools/testing/nvdimm/test/iomap.c
index 280015c22598,cd040b5abffe..000000000000
--- a/tools/testing/nvdimm/test/iomap.c
+++ b/tools/testing/nvdimm/test/iomap.c
@@@ -100,7 -100,27 +100,31 @@@ static void nfit_test_kill(void *_pgmap
  {
  	struct dev_pagemap *pgmap = _pgmap;
  
++<<<<<<< HEAD
 +	pgmap->kill(pgmap->ref);
++=======
+ 	WARN_ON(!pgmap || !pgmap->ref);
+ 
+ 	if (pgmap->ops && pgmap->ops->kill)
+ 		pgmap->ops->kill(pgmap);
+ 	else
+ 		percpu_ref_kill(pgmap->ref);
+ 
+ 	if (pgmap->ops && pgmap->ops->cleanup) {
+ 		pgmap->ops->cleanup(pgmap);
+ 	} else {
+ 		wait_for_completion(&pgmap->done);
+ 		percpu_ref_exit(pgmap->ref);
+ 	}
+ }
+ 
+ static void dev_pagemap_percpu_release(struct percpu_ref *ref)
+ {
+ 	struct dev_pagemap *pgmap =
+ 		container_of(ref, struct dev_pagemap, internal_ref);
+ 
+ 	complete(&pgmap->done);
++>>>>>>> 24917f6b1041 (memremap: provide an optional internal refcount in struct dev_pagemap)
  }
  
  void *__wrap_devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap)
* Unmerged path include/linux/memremap.h
* Unmerged path kernel/memremap.c
* Unmerged path tools/testing/nvdimm/test/iomap.c
