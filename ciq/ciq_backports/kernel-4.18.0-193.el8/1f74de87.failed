sched/toplogy: Introduce the 'sched_energy_present' static key

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Quentin Perret <quentin.perret@arm.com>
commit 1f74de8798c93ce14801cc4e772603e51c841c33
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1f74de87.failed

In order to make sure Energy Aware Scheduling (EAS) will not impact
systems where no Energy Model is available, introduce a static key
guarding the access to EAS code. Since EAS is enabled on a
per-root-domain basis, the static key is enabled when at least one root
domain meets all conditions for EAS.

	Signed-off-by: Quentin Perret <quentin.perret@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: adharmap@codeaurora.org
	Cc: chris.redpath@arm.com
	Cc: currojerez@riseup.net
	Cc: dietmar.eggemann@arm.com
	Cc: edubezval@gmail.com
	Cc: gregkh@linuxfoundation.org
	Cc: javi.merino@kernel.org
	Cc: joel@joelfernandes.org
	Cc: juri.lelli@redhat.com
	Cc: morten.rasmussen@arm.com
	Cc: patrick.bellasi@arm.com
	Cc: pkondeti@codeaurora.org
	Cc: rjw@rjwysocki.net
	Cc: skannan@codeaurora.org
	Cc: smuckle@google.com
	Cc: srinivas.pandruvada@linux.intel.com
	Cc: thara.gopinath@linaro.org
	Cc: tkjos@google.com
	Cc: valentin.schneider@arm.com
	Cc: vincent.guittot@linaro.org
	Cc: viresh.kumar@linaro.org
Link: https://lkml.kernel.org/r/20181203095628.11858-10-quentin.perret@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1f74de8798c93ce14801cc4e772603e51c841c33)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/sched.h
#	kernel/sched/topology.c
diff --cc kernel/sched/sched.h
index 2f0ee3f2ef1e,2b3cf356e958..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -2301,3 -2290,13 +2301,16 @@@ unsigned long scale_irq_capacity(unsign
  	return util;
  }
  #endif
++<<<<<<< HEAD
++=======
+ 
+ #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+ #define perf_domain_span(pd) (to_cpumask(((pd)->em_pd->cpus)))
+ #else
+ #define perf_domain_span(pd) NULL
+ #endif
+ 
+ #ifdef CONFIG_SMP
+ extern struct static_key_false sched_energy_present;
+ #endif
++>>>>>>> 1f74de8798c9 (sched/toplogy: Introduce the 'sched_energy_present' static key)
diff --cc kernel/sched/topology.c
index 7e6a83289382,3f35ba1d8fde..000000000000
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@@ -201,6 -201,199 +201,202 @@@ sd_parent_degenerate(struct sched_domai
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ DEFINE_STATIC_KEY_FALSE(sched_energy_present);
+ #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+ DEFINE_MUTEX(sched_energy_mutex);
+ bool sched_energy_update;
+ 
+ static void free_pd(struct perf_domain *pd)
+ {
+ 	struct perf_domain *tmp;
+ 
+ 	while (pd) {
+ 		tmp = pd->next;
+ 		kfree(pd);
+ 		pd = tmp;
+ 	}
+ }
+ 
+ static struct perf_domain *find_pd(struct perf_domain *pd, int cpu)
+ {
+ 	while (pd) {
+ 		if (cpumask_test_cpu(cpu, perf_domain_span(pd)))
+ 			return pd;
+ 		pd = pd->next;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static struct perf_domain *pd_init(int cpu)
+ {
+ 	struct em_perf_domain *obj = em_cpu_get(cpu);
+ 	struct perf_domain *pd;
+ 
+ 	if (!obj) {
+ 		if (sched_debug())
+ 			pr_info("%s: no EM found for CPU%d\n", __func__, cpu);
+ 		return NULL;
+ 	}
+ 
+ 	pd = kzalloc(sizeof(*pd), GFP_KERNEL);
+ 	if (!pd)
+ 		return NULL;
+ 	pd->em_pd = obj;
+ 
+ 	return pd;
+ }
+ 
+ static void perf_domain_debug(const struct cpumask *cpu_map,
+ 						struct perf_domain *pd)
+ {
+ 	if (!sched_debug() || !pd)
+ 		return;
+ 
+ 	printk(KERN_DEBUG "root_domain %*pbl:", cpumask_pr_args(cpu_map));
+ 
+ 	while (pd) {
+ 		printk(KERN_CONT " pd%d:{ cpus=%*pbl nr_cstate=%d }",
+ 				cpumask_first(perf_domain_span(pd)),
+ 				cpumask_pr_args(perf_domain_span(pd)),
+ 				em_pd_nr_cap_states(pd->em_pd));
+ 		pd = pd->next;
+ 	}
+ 
+ 	printk(KERN_CONT "\n");
+ }
+ 
+ static void destroy_perf_domain_rcu(struct rcu_head *rp)
+ {
+ 	struct perf_domain *pd;
+ 
+ 	pd = container_of(rp, struct perf_domain, rcu);
+ 	free_pd(pd);
+ }
+ 
+ static void sched_energy_set(bool has_eas)
+ {
+ 	if (!has_eas && static_branch_unlikely(&sched_energy_present)) {
+ 		if (sched_debug())
+ 			pr_info("%s: stopping EAS\n", __func__);
+ 		static_branch_disable_cpuslocked(&sched_energy_present);
+ 	} else if (has_eas && !static_branch_unlikely(&sched_energy_present)) {
+ 		if (sched_debug())
+ 			pr_info("%s: starting EAS\n", __func__);
+ 		static_branch_enable_cpuslocked(&sched_energy_present);
+ 	}
+ }
+ 
+ /*
+  * EAS can be used on a root domain if it meets all the following conditions:
+  *    1. an Energy Model (EM) is available;
+  *    2. the SD_ASYM_CPUCAPACITY flag is set in the sched_domain hierarchy.
+  *    3. the EM complexity is low enough to keep scheduling overheads low;
+  *    4. schedutil is driving the frequency of all CPUs of the rd;
+  *
+  * The complexity of the Energy Model is defined as:
+  *
+  *              C = nr_pd * (nr_cpus + nr_cs)
+  *
+  * with parameters defined as:
+  *  - nr_pd:    the number of performance domains
+  *  - nr_cpus:  the number of CPUs
+  *  - nr_cs:    the sum of the number of capacity states of all performance
+  *              domains (for example, on a system with 2 performance domains,
+  *              with 10 capacity states each, nr_cs = 2 * 10 = 20).
+  *
+  * It is generally not a good idea to use such a model in the wake-up path on
+  * very complex platforms because of the associated scheduling overheads. The
+  * arbitrary constraint below prevents that. It makes EAS usable up to 16 CPUs
+  * with per-CPU DVFS and less than 8 capacity states each, for example.
+  */
+ #define EM_MAX_COMPLEXITY 2048
+ 
+ extern struct cpufreq_governor schedutil_gov;
+ static bool build_perf_domains(const struct cpumask *cpu_map)
+ {
+ 	int i, nr_pd = 0, nr_cs = 0, nr_cpus = cpumask_weight(cpu_map);
+ 	struct perf_domain *pd = NULL, *tmp;
+ 	int cpu = cpumask_first(cpu_map);
+ 	struct root_domain *rd = cpu_rq(cpu)->rd;
+ 	struct cpufreq_policy *policy;
+ 	struct cpufreq_governor *gov;
+ 
+ 	/* EAS is enabled for asymmetric CPU capacity topologies. */
+ 	if (!per_cpu(sd_asym_cpucapacity, cpu)) {
+ 		if (sched_debug()) {
+ 			pr_info("rd %*pbl: CPUs do not have asymmetric capacities\n",
+ 					cpumask_pr_args(cpu_map));
+ 		}
+ 		goto free;
+ 	}
+ 
+ 	for_each_cpu(i, cpu_map) {
+ 		/* Skip already covered CPUs. */
+ 		if (find_pd(pd, i))
+ 			continue;
+ 
+ 		/* Do not attempt EAS if schedutil is not being used. */
+ 		policy = cpufreq_cpu_get(i);
+ 		if (!policy)
+ 			goto free;
+ 		gov = policy->governor;
+ 		cpufreq_cpu_put(policy);
+ 		if (gov != &schedutil_gov) {
+ 			if (rd->pd)
+ 				pr_warn("rd %*pbl: Disabling EAS, schedutil is mandatory\n",
+ 						cpumask_pr_args(cpu_map));
+ 			goto free;
+ 		}
+ 
+ 		/* Create the new pd and add it to the local list. */
+ 		tmp = pd_init(i);
+ 		if (!tmp)
+ 			goto free;
+ 		tmp->next = pd;
+ 		pd = tmp;
+ 
+ 		/*
+ 		 * Count performance domains and capacity states for the
+ 		 * complexity check.
+ 		 */
+ 		nr_pd++;
+ 		nr_cs += em_pd_nr_cap_states(pd->em_pd);
+ 	}
+ 
+ 	/* Bail out if the Energy Model complexity is too high. */
+ 	if (nr_pd * (nr_cs + nr_cpus) > EM_MAX_COMPLEXITY) {
+ 		WARN(1, "rd %*pbl: Failed to start EAS, EM complexity is too high\n",
+ 						cpumask_pr_args(cpu_map));
+ 		goto free;
+ 	}
+ 
+ 	perf_domain_debug(cpu_map, pd);
+ 
+ 	/* Attach the new list of performance domains to the root domain. */
+ 	tmp = rd->pd;
+ 	rcu_assign_pointer(rd->pd, pd);
+ 	if (tmp)
+ 		call_rcu(&tmp->rcu, destroy_perf_domain_rcu);
+ 
+ 	return !!pd;
+ 
+ free:
+ 	free_pd(pd);
+ 	tmp = rd->pd;
+ 	rcu_assign_pointer(rd->pd, NULL);
+ 	if (tmp)
+ 		call_rcu(&tmp->rcu, destroy_perf_domain_rcu);
+ 
+ 	return false;
+ }
+ #else
+ static void free_pd(struct perf_domain *pd) { }
+ #endif /* CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL*/
+ 
++>>>>>>> 1f74de8798c9 (sched/toplogy: Introduce the 'sched_energy_present' static key)
  static void free_rootdomain(struct rcu_head *rcu)
  {
  	struct root_domain *rd = container_of(rcu, struct root_domain, rcu);
@@@ -1994,6 -2189,24 +2191,27 @@@ match2
  		;
  	}
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+ 	/* Build perf. domains: */
+ 	for (i = 0; i < ndoms_new; i++) {
+ 		for (j = 0; j < n && !sched_energy_update; j++) {
+ 			if (cpumask_equal(doms_new[i], doms_cur[j]) &&
+ 			    cpu_rq(cpumask_first(doms_cur[j]))->rd->pd) {
+ 				has_eas = true;
+ 				goto match3;
+ 			}
+ 		}
+ 		/* No match - add perf. domains for a new rd */
+ 		has_eas |= build_perf_domains(doms_new[i]);
+ match3:
+ 		;
+ 	}
+ 	sched_energy_set(has_eas);
+ #endif
+ 
++>>>>>>> 1f74de8798c9 (sched/toplogy: Introduce the 'sched_energy_present' static key)
  	/* Remember the new sched domains: */
  	if (doms_cur != &fallback_doms)
  		free_sched_domains(doms_cur, ndoms_cur);
* Unmerged path kernel/sched/sched.h
* Unmerged path kernel/sched/topology.c
