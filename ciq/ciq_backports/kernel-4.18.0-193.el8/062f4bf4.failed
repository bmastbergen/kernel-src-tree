net/mlx5: E-Switch, Consolidate eswitch function number of VFs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [netdrv] mlx5: E-Switch, Consolidate eswitch function number of VFs (Alaa Hleihel) [1724327 1724336]
Rebuild_FUZZ: 96.67%
commit-author Bodong Wang <bodong@mellanox.com>
commit 062f4bf4aab5c6bb62bea59cda59d5c64f23ba29
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/062f4bf4.failed

Enabled number of VFs is key for eswich manager to do flow steering
initialization and vport configurations. However, the number of
enabled VFs may come from two sources as below.

PF: num of VFs is provided by enabled SR-IOV of itself.
ECPF: num of VFs is provided by enabled SR-IOV from its peer PF. And
      SR-IOV can't be enabled from ECPF itself.

Current driver handles the two cases in different stages and passing
the number of enabled VFs among a large scope of internal functions.
It is usually hard to find out where is the real number of VFs from
due to layers of argument pass-in.

This patch consolidated that number from the entry point of doing
eswitch setup, and maintained a copy so that eswitch functions can
refer to it directly.

Eswitch driver shall always use this number when referring to enabled
number of VFs, don't use other numbers such as from SR-IOV.

	Signed-off-by: Bodong Wang <bodong@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 062f4bf4aab5c6bb62bea59cda59d5c64f23ba29)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
#	drivers/net/ethernet/mellanox/mlx5/core/sriov.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
index f4a0b22e3987,b4f96f04a18b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
@@@ -1688,8 -1718,19 +1688,14 @@@ static int eswitch_vport_event(struct n
  /* Public E-Switch API */
  #define ESW_ALLOWED(esw) ((esw) && MLX5_ESWITCH_MANAGER((esw)->dev))
  
++<<<<<<< HEAD
 +int mlx5_eswitch_enable_sriov(struct mlx5_eswitch *esw, int nvfs, int mode)
 +{
++=======
+ int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode)
+ {
+ 	struct mlx5_vport *vport;
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	int err;
  	int i, enabled_events;
  
@@@ -1700,13 -1741,11 +1706,16 @@@
  	}
  
  	if (!MLX5_CAP_ESW_INGRESS_ACL(esw->dev, ft_support))
 -		esw_warn(esw->dev, "ingress ACL is not supported by FW\n");
 +		esw_warn(esw->dev, "E-Switch ingress ACL is not supported by FW\n");
  
  	if (!MLX5_CAP_ESW_EGRESS_ACL(esw->dev, ft_support))
 -		esw_warn(esw->dev, "engress ACL is not supported by FW\n");
 +		esw_warn(esw->dev, "E-Switch engress ACL is not supported by FW\n");
 +
++<<<<<<< HEAD
 +	esw_info(esw->dev, "E-Switch enable SRIOV: nvfs(%d) mode (%d)\n", nvfs, mode);
  
++=======
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	esw->mode = mode;
  
  	mlx5_lag_update(esw->dev);
@@@ -1718,7 -1757,7 +1727,11 @@@
  	} else {
  		mlx5_reload_interface(esw->dev, MLX5_INTERFACE_PROTOCOL_ETH);
  		mlx5_reload_interface(esw->dev, MLX5_INTERFACE_PROTOCOL_IB);
++<<<<<<< HEAD
 +		err = esw_offloads_init(esw, nvfs + MLX5_SPECIAL_VPORTS);
++=======
+ 		err = esw_offloads_init(esw);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	}
  
  	if (err)
@@@ -1732,17 -1771,31 +1745,42 @@@
  	 * 1. L2 table (MPFS) is programmed by PF/VF representors netdevs set_rx_mode
  	 * 2. FDB/Eswitch is programmed by user space tools
  	 */
 -	enabled_events = (mode == MLX5_ESWITCH_LEGACY) ? SRIOV_VPORT_EVENTS : 0;
 +	enabled_events = (mode == SRIOV_LEGACY) ? SRIOV_VPORT_EVENTS : 0;
 +	for (i = 0; i <= nvfs; i++)
 +		esw_enable_vport(esw, i, enabled_events);
  
++<<<<<<< HEAD
 +	if (mode == SRIOV_LEGACY) {
++=======
+ 	/* Enable PF vport */
+ 	vport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_PF);
+ 	esw_enable_vport(esw, vport, enabled_events);
+ 
+ 	/* Enable ECPF vports */
+ 	if (mlx5_ecpf_vport_exists(esw->dev)) {
+ 		vport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_ECPF);
+ 		esw_enable_vport(esw, vport, enabled_events);
+ 	}
+ 
+ 	/* Enable VF vports */
+ 	mlx5_esw_for_each_vf_vport(esw, i, vport, esw->esw_funcs.num_vfs)
+ 		esw_enable_vport(esw, vport, enabled_events);
+ 
+ 	if (mode == MLX5_ESWITCH_LEGACY) {
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  		MLX5_NB_INIT(&esw->nb, eswitch_vport_event, NIC_VPORT_CHANGE);
  		mlx5_eq_notifier_register(esw->dev, &esw->nb);
  	}
  
++<<<<<<< HEAD
 +	esw_info(esw->dev, "SRIOV enabled: active vports(%d)\n",
 +		 esw->enabled_vports);
++=======
+ 	esw_info(esw->dev, "Enable: mode(%s), nvfs(%d), active vports(%d)\n",
+ 		 mode == MLX5_ESWITCH_LEGACY ? "LEGACY" : "OFFLOADS",
+ 		 esw->esw_funcs.num_vfs, esw->enabled_vports);
+ 
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	return 0;
  
  abort:
@@@ -1756,27 -1809,27 +1794,33 @@@
  	return err;
  }
  
 -void mlx5_eswitch_disable(struct mlx5_eswitch *esw)
 +void mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw)
  {
  	struct esw_mc_addr *mc_promisc;
 -	struct mlx5_vport *vport;
  	int old_mode;
 +	int nvports;
  	int i;
  
 -	if (!ESW_ALLOWED(esw) || esw->mode == MLX5_ESWITCH_NONE)
 +	if (!ESW_ALLOWED(esw) || esw->mode == SRIOV_NONE)
  		return;
  
++<<<<<<< HEAD
 +	esw_info(esw->dev, "disable SRIOV: active vports(%d) mode(%d)\n",
 +		 esw->enabled_vports, esw->mode);
++=======
+ 	esw_info(esw->dev, "Disable: mode(%s), nvfs(%d), active vports(%d)\n",
+ 		 esw->mode == MLX5_ESWITCH_LEGACY ? "LEGACY" : "OFFLOADS",
+ 		 esw->esw_funcs.num_vfs, esw->enabled_vports);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  
  	mc_promisc = &esw->mc_promisc;
 +	nvports = esw->enabled_vports;
  
 -	if (esw->mode == MLX5_ESWITCH_LEGACY)
 +	if (esw->mode == SRIOV_LEGACY)
  		mlx5_eq_notifier_unregister(esw->dev, &esw->nb);
  
 -	mlx5_esw_for_all_vports(esw, i, vport)
 -		esw_disable_vport(esw, vport);
 +	for (i = 0; i < esw->total_vports; i++)
 +		esw_disable_vport(esw, i);
  
  	if (mc_promisc && mc_promisc->uplink_rule)
  		mlx5_del_flow_rules(mc_promisc->uplink_rule);
@@@ -2465,6 -2504,24 +2509,24 @@@ bool mlx5_esw_lag_prereq(struct mlx5_co
  bool mlx5_esw_multipath_prereq(struct mlx5_core_dev *dev0,
  			       struct mlx5_core_dev *dev1)
  {
 -	return (dev0->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS &&
 -		dev1->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS);
 +	return (dev0->priv.eswitch->mode == SRIOV_OFFLOADS &&
 +		dev1->priv.eswitch->mode == SRIOV_OFFLOADS);
  }
+ 
+ void mlx5_eswitch_update_num_of_vfs(struct mlx5_eswitch *esw, const int num_vfs)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(query_esw_functions_out)] = {};
+ 	int err;
+ 
+ 	WARN_ON_ONCE(esw->mode != MLX5_ESWITCH_NONE);
+ 
+ 	if (!mlx5_core_is_ecpf_esw_manager(esw->dev)) {
+ 		esw->esw_funcs.num_vfs = num_vfs;
+ 		return;
+ 	}
+ 
+ 	err = mlx5_esw_query_functions(esw->dev, out, sizeof(out));
+ 	if (!err)
+ 		esw->esw_funcs.num_vfs = MLX5_GET(query_esw_functions_out, out,
+ 						  host_params_context.host_num_of_vfs);
+ }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index af9a875f1cf1,744352baf434..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@@ -209,10 -226,12 +209,15 @@@ struct mlx5_eswitch 
  	struct mlx5_esw_offload offloads;
  	int                     mode;
  	int                     nvports;
 -	u16                     manager_vport;
 -	struct mlx5_esw_functions esw_funcs;
  };
  
++<<<<<<< HEAD
 +void esw_offloads_cleanup(struct mlx5_eswitch *esw, int nvports);
 +int esw_offloads_init(struct mlx5_eswitch *esw, int nvports);
++=======
+ void esw_offloads_cleanup(struct mlx5_eswitch *esw);
+ int esw_offloads_init(struct mlx5_eswitch *esw);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  void esw_offloads_cleanup_reps(struct mlx5_eswitch *esw);
  int esw_offloads_init_reps(struct mlx5_eswitch *esw);
  void esw_vport_cleanup_ingress_rules(struct mlx5_eswitch *esw,
@@@ -231,8 -252,8 +236,13 @@@ void esw_vport_disable_ingress_acl(stru
  /* E-Switch API */
  int mlx5_eswitch_init(struct mlx5_core_dev *dev);
  void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw);
++<<<<<<< HEAD
 +int mlx5_eswitch_enable_sriov(struct mlx5_eswitch *esw, int nvfs, int mode);
 +void mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw);
++=======
+ int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode);
+ void mlx5_eswitch_disable(struct mlx5_eswitch *esw);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  int mlx5_eswitch_set_vport_mac(struct mlx5_eswitch *esw,
  			       int vport, u8 mac[ETH_ALEN]);
  int mlx5_eswitch_set_vport_state(struct mlx5_eswitch *esw,
@@@ -395,14 -418,129 +405,73 @@@ bool mlx5_esw_multipath_prereq(struct m
  /* TODO: This mlx5e_tc function shouldn't be called by eswitch */
  void mlx5e_tc_clean_fdb_peer_flows(struct mlx5_eswitch *esw);
  
++<<<<<<< HEAD
++=======
+ /* The vport getter/iterator are only valid after esw->total_vports
+  * and vport->vport are initialized in mlx5_eswitch_init.
+  */
+ #define mlx5_esw_for_all_vports(esw, i, vport)		\
+ 	for ((i) = MLX5_VPORT_PF;			\
+ 	     (vport) = &(esw)->vports[i],		\
+ 	     (i) < (esw)->total_vports; (i)++)
+ 
+ #define mlx5_esw_for_each_vf_vport(esw, i, vport, nvfs)	\
+ 	for ((i) = MLX5_VPORT_FIRST_VF;			\
+ 	     (vport) = &(esw)->vports[(i)],		\
+ 	     (i) <= (nvfs); (i)++)
+ 
+ #define mlx5_esw_for_each_vf_vport_reverse(esw, i, vport, nvfs)	\
+ 	for ((i) = (nvfs);					\
+ 	     (vport) = &(esw)->vports[(i)],			\
+ 	     (i) >= MLX5_VPORT_FIRST_VF; (i)--)
+ 
+ /* The rep getter/iterator are only valid after esw->total_vports
+  * and vport->vport are initialized in mlx5_eswitch_init.
+  */
+ #define mlx5_esw_for_all_reps(esw, i, rep)			\
+ 	for ((i) = MLX5_VPORT_PF;				\
+ 	     (rep) = &(esw)->offloads.vport_reps[i],		\
+ 	     (i) < (esw)->total_vports; (i)++)
+ 
+ #define mlx5_esw_for_each_vf_rep(esw, i, rep, nvfs)		\
+ 	for ((i) = MLX5_VPORT_FIRST_VF;				\
+ 	     (rep) = &(esw)->offloads.vport_reps[i],		\
+ 	     (i) <= (nvfs); (i)++)
+ 
+ #define mlx5_esw_for_each_vf_rep_reverse(esw, i, rep, nvfs)	\
+ 	for ((i) = (nvfs);					\
+ 	     (rep) = &(esw)->offloads.vport_reps[i],		\
+ 	     (i) >= MLX5_VPORT_FIRST_VF; (i)--)
+ 
+ #define mlx5_esw_for_each_vf_vport_num(esw, vport, nvfs)	\
+ 	for ((vport) = MLX5_VPORT_FIRST_VF; (vport) <= (nvfs); (vport)++)
+ 
+ #define mlx5_esw_for_each_vf_vport_num_reverse(esw, vport, nvfs)	\
+ 	for ((vport) = (nvfs); (vport) >= MLX5_VPORT_FIRST_VF; (vport)--)
+ 
+ struct mlx5_vport *__must_check
+ mlx5_eswitch_get_vport(struct mlx5_eswitch *esw, u16 vport_num);
+ 
+ bool mlx5_eswitch_is_vf_vport(const struct mlx5_eswitch *esw, u16 vport_num);
+ 
+ void mlx5_eswitch_update_num_of_vfs(struct mlx5_eswitch *esw, const int num_vfs);
+ 
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  #else  /* CONFIG_MLX5_ESWITCH */
  /* eswitch API stubs */
  static inline int  mlx5_eswitch_init(struct mlx5_core_dev *dev) { return 0; }
  static inline void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw) {}
++<<<<<<< HEAD
 +static inline int  mlx5_eswitch_enable_sriov(struct mlx5_eswitch *esw, int nvfs, int mode) { return 0; }
 +static inline void mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw) {}
++=======
+ static inline int  mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode) { return 0; }
+ static inline void mlx5_eswitch_disable(struct mlx5_eswitch *esw) {}
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  static inline bool mlx5_esw_lag_prereq(struct mlx5_core_dev *dev0, struct mlx5_core_dev *dev1) { return true; }
 -static inline bool mlx5_eswitch_is_funcs_handler(struct mlx5_core_dev *dev) { return false; }
 -static inline int
 -mlx5_esw_query_functions(struct mlx5_core_dev *dev, u32 *out, int outlen)
 -{
 -	return -EOPNOTSUPP;
 -}
  
+ static inline void mlx5_eswitch_update_num_of_vfs(struct mlx5_eswitch *esw, const int num_vfs) {}
+ 
  #define FDB_MAX_CHAIN 1
  #define FDB_SLOW_PATH_CHAIN (FDB_MAX_CHAIN + 1)
  #define FDB_MAX_PRIO 1
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 2a02050a09e7,8010e4eaba9a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -1167,21 -1356,22 +1167,31 @@@ out
  static int esw_offloads_start(struct mlx5_eswitch *esw,
  			      struct netlink_ext_ack *extack)
  {
- 	int err, err1, num_vfs = esw->dev->priv.sriov.num_vfs;
+ 	int err, err1;
  
 -	if (esw->mode != MLX5_ESWITCH_LEGACY &&
 +	if (esw->mode != SRIOV_LEGACY &&
  	    !mlx5_core_is_ecpf_esw_manager(esw->dev)) {
  		NL_SET_ERR_MSG_MOD(extack,
  				   "Can't set offloads mode, SRIOV legacy not enabled");
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	mlx5_eswitch_disable_sriov(esw);
 +	err = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_OFFLOADS);
 +	if (err) {
 +		NL_SET_ERR_MSG_MOD(extack,
 +				   "Failed setting eswitch to offloads");
 +		err1 = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_LEGACY);
++=======
+ 	mlx5_eswitch_disable(esw);
+ 	mlx5_eswitch_update_num_of_vfs(esw, esw->dev->priv.sriov.num_vfs);
+ 	err = mlx5_eswitch_enable(esw, MLX5_ESWITCH_OFFLOADS);
+ 	if (err) {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "Failed setting eswitch to offloads");
+ 		err1 = mlx5_eswitch_enable(esw, MLX5_ESWITCH_LEGACY);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  		if (err1) {
  			NL_SET_ERR_MSG_MOD(extack,
  					   "Failed setting eswitch back to legacy");
@@@ -1254,24 -1463,63 +1263,60 @@@ static void esw_offloads_unload_reps(st
  	u8 rep_type = NUM_REP_TYPES;
  
  	while (rep_type-- > 0)
 -		__unload_reps_vf_vport(esw, nvports, rep_type);
 +		esw_offloads_unload_reps_type(esw, nvports, rep_type);
  }
  
++<<<<<<< HEAD
 +static int esw_offloads_load_reps_type(struct mlx5_eswitch *esw, int nvports,
 +				       u8 rep_type)
++=======
+ static void __unload_reps_all_vport(struct mlx5_eswitch *esw, u8 rep_type)
+ {
+ 	__unload_reps_vf_vport(esw, esw->esw_funcs.num_vfs, rep_type);
+ 
+ 	/* Special vports must be the last to unload. */
+ 	__unload_reps_special_vport(esw, rep_type);
+ }
+ 
+ static void esw_offloads_unload_all_reps(struct mlx5_eswitch *esw)
+ {
+ 	u8 rep_type = NUM_REP_TYPES;
+ 
+ 	while (rep_type-- > 0)
+ 		__unload_reps_all_vport(esw, rep_type);
+ }
+ 
+ static int __esw_offloads_load_rep(struct mlx5_eswitch *esw,
+ 				   struct mlx5_eswitch_rep *rep, u8 rep_type)
+ {
+ 	int err = 0;
+ 
+ 	if (atomic_cmpxchg(&rep->rep_data[rep_type].state,
+ 			   REP_REGISTERED, REP_LOADED) == REP_REGISTERED) {
+ 		err = esw->offloads.rep_ops[rep_type]->load(esw->dev, rep);
+ 		if (err)
+ 			atomic_set(&rep->rep_data[rep_type].state,
+ 				   REP_REGISTERED);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int __load_reps_special_vport(struct mlx5_eswitch *esw, u8 rep_type)
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  {
  	struct mlx5_eswitch_rep *rep;
 +	int vport;
  	int err;
  
 -	rep = mlx5_eswitch_get_rep(esw, MLX5_VPORT_UPLINK);
 -	err = __esw_offloads_load_rep(esw, rep, rep_type);
 -	if (err)
 -		return err;
 +	for (vport = 0; vport < nvports; vport++) {
 +		rep = &esw->offloads.vport_reps[vport];
 +		if (!rep->rep_if[rep_type].valid)
 +			continue;
  
 -	if (mlx5_core_is_ecpf_esw_manager(esw->dev)) {
 -		rep = mlx5_eswitch_get_rep(esw, MLX5_VPORT_PF);
 -		err = __esw_offloads_load_rep(esw, rep, rep_type);
 +		err = rep->rep_if[rep_type].load(esw->dev, rep);
  		if (err)
 -			goto err_pf;
 -	}
 -
 -	if (mlx5_ecpf_vport_exists(esw->dev)) {
 -		rep = mlx5_eswitch_get_rep(esw, MLX5_VPORT_ECPF);
 -		err = __esw_offloads_load_rep(esw, rep, rep_type);
 -		if (err)
 -			goto err_ecpf;
 +			goto err_reps;
  	}
  
  	return 0;
@@@ -1281,7 -1536,46 +1326,50 @@@ err_reps
  	return err;
  }
  
++<<<<<<< HEAD
 +static int esw_offloads_load_reps(struct mlx5_eswitch *esw, int nvports)
++=======
+ static int __load_reps_vf_vport(struct mlx5_eswitch *esw, int nvports,
+ 				u8 rep_type)
+ {
+ 	struct mlx5_eswitch_rep *rep;
+ 	int err, i;
+ 
+ 	mlx5_esw_for_each_vf_rep(esw, i, rep, nvports) {
+ 		err = __esw_offloads_load_rep(esw, rep, rep_type);
+ 		if (err)
+ 			goto err_vf;
+ 	}
+ 
+ 	return 0;
+ 
+ err_vf:
+ 	__unload_reps_vf_vport(esw, --i, rep_type);
+ 	return err;
+ }
+ 
+ static int __load_reps_all_vport(struct mlx5_eswitch *esw, u8 rep_type)
+ {
+ 	int err;
+ 
+ 	/* Special vports must be loaded first, uplink rep creates mdev resource. */
+ 	err = __load_reps_special_vport(esw, rep_type);
+ 	if (err)
+ 		return err;
+ 
+ 	err = __load_reps_vf_vport(esw, esw->esw_funcs.num_vfs, rep_type);
+ 	if (err)
+ 		goto err_vfs;
+ 
+ 	return 0;
+ 
+ err_vfs:
+ 	__unload_reps_special_vport(esw, rep_type);
+ 	return err;
+ }
+ 
+ static int esw_offloads_load_vf_reps(struct mlx5_eswitch *esw, int nvports)
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  {
  	u8 rep_type = 0;
  	int err;
@@@ -1296,7 -1590,26 +1384,30 @@@
  
  err_reps:
  	while (rep_type-- > 0)
++<<<<<<< HEAD
 +		esw_offloads_unload_reps_type(esw, nvports, rep_type);
++=======
+ 		__unload_reps_vf_vport(esw, nvports, rep_type);
+ 	return err;
+ }
+ 
+ static int esw_offloads_load_all_reps(struct mlx5_eswitch *esw)
+ {
+ 	u8 rep_type = 0;
+ 	int err;
+ 
+ 	for (rep_type = 0; rep_type < NUM_REP_TYPES; rep_type++) {
+ 		err = __load_reps_all_vport(esw, rep_type);
+ 		if (err)
+ 			goto err_reps;
+ 	}
+ 
+ 	return err;
+ 
+ err_reps:
+ 	while (rep_type-- > 0)
+ 		__unload_reps_all_vport(esw, rep_type);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	return err;
  }
  
@@@ -1548,34 -1982,42 +1659,41 @@@ err_ingress
  	return err;
  }
  
 -static void esw_destroy_offloads_acl_tables(struct mlx5_eswitch *esw)
 +static void esw_prio_tag_acls_cleanup(struct mlx5_eswitch *esw)
  {
 -	struct mlx5_vport *vport;
  	int i;
  
 -	mlx5_esw_for_all_vports(esw, i, vport) {
 -		esw_vport_disable_egress_acl(esw, vport);
 -		esw_vport_disable_ingress_acl(esw, vport);
 +	mlx5_esw_for_each_vf_vport(esw, i, esw->nvports) {
 +		esw_vport_disable_egress_acl(esw, &esw->vports[i]);
 +		esw_vport_disable_ingress_acl(esw, &esw->vports[i]);
  	}
 -
 -	esw->flags &= ~MLX5_ESWITCH_VPORT_MATCH_METADATA;
  }
  
- static int esw_offloads_steering_init(struct mlx5_eswitch *esw, int nvports)
+ static int esw_offloads_steering_init(struct mlx5_eswitch *esw)
  {
+ 	int num_vfs = esw->esw_funcs.num_vfs;
+ 	int total_vports;
  	int err;
  
+ 	if (mlx5_core_is_ecpf_esw_manager(esw->dev))
+ 		total_vports = esw->total_vports;
+ 	else
+ 		total_vports = num_vfs + MLX5_SPECIAL_VPORTS(esw->dev);
+ 
  	memset(&esw->fdb_table.offloads, 0, sizeof(struct offloads_fdb));
  	mutex_init(&esw->fdb_table.offloads.fdb_prio_lock);
  
 -	err = esw_create_offloads_acl_tables(esw);
 -	if (err)
 -		return err;
 +	if (MLX5_CAP_GEN(esw->dev, prio_tag_required)) {
 +		err = esw_prio_tag_acls_config(esw, nvports);
 +		if (err)
 +			return err;
 +	}
  
- 	err = esw_create_offloads_fdb_tables(esw, nvports);
+ 	err = esw_create_offloads_fdb_tables(esw, total_vports);
  	if (err)
 -		goto create_fdb_err;
 +		return err;
  
- 	err = esw_create_offloads_table(esw, nvports);
+ 	err = esw_create_offloads_table(esw, total_vports);
  	if (err)
  		goto create_ft_err;
  
@@@ -1599,25 -2044,110 +1717,128 @@@ static void esw_offloads_steering_clean
  	esw_destroy_vport_rx_group(esw);
  	esw_destroy_offloads_table(esw);
  	esw_destroy_offloads_fdb_tables(esw);
 -	esw_destroy_offloads_acl_tables(esw);
 +	if (MLX5_CAP_GEN(esw->dev, prio_tag_required))
 +		esw_prio_tag_acls_cleanup(esw);
  }
  
++<<<<<<< HEAD
 +int esw_offloads_init(struct mlx5_eswitch *esw, int nvports)
 +{
 +	int err;
 +
 +	mutex_init(&esw->fdb_table.offloads.fdb_prio_lock);
 +
 +	err = esw_offloads_steering_init(esw, nvports);
 +	if (err)
 +		return err;
 +
 +	err = esw_offloads_load_reps(esw, nvports);
++=======
+ static void esw_functions_changed_event_handler(struct work_struct *work)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(query_esw_functions_out)] = {};
+ 	struct mlx5_host_work *host_work;
+ 	struct mlx5_eswitch *esw;
+ 	bool host_pf_disabled;
+ 	u16 num_vfs = 0;
+ 	int err;
+ 
+ 	host_work = container_of(work, struct mlx5_host_work, work);
+ 	esw = host_work->esw;
+ 
+ 	err = mlx5_esw_query_functions(esw->dev, out, sizeof(out));
+ 	num_vfs = MLX5_GET(query_esw_functions_out, out,
+ 			   host_params_context.host_num_of_vfs);
+ 	host_pf_disabled = MLX5_GET(query_esw_functions_out, out,
+ 				    host_params_context.host_pf_disabled);
+ 	if (err || host_pf_disabled || num_vfs == esw->esw_funcs.num_vfs)
+ 		goto out;
+ 
+ 	/* Number of VFs can only change from "0 to x" or "x to 0". */
+ 	if (esw->esw_funcs.num_vfs > 0) {
+ 		esw_offloads_unload_vf_reps(esw, esw->esw_funcs.num_vfs);
+ 	} else {
+ 		err = esw_offloads_load_vf_reps(esw, num_vfs);
+ 
+ 		if (err)
+ 			goto out;
+ 	}
+ 
+ 	esw->esw_funcs.num_vfs = num_vfs;
+ 
+ out:
+ 	kfree(host_work);
+ }
+ 
+ 
+ static int
+ esw_functions_changed_event(struct notifier_block *nb, unsigned long type, void *data)
+ {
+ 	struct mlx5_esw_functions *esw_funcs;
+ 	struct mlx5_host_work *host_work;
+ 	struct mlx5_eswitch *esw;
+ 
+ 	host_work = kzalloc(sizeof(*host_work), GFP_ATOMIC);
+ 	if (!host_work)
+ 		return NOTIFY_DONE;
+ 
+ 	esw_funcs = mlx5_nb_cof(nb, struct mlx5_esw_functions, nb);
+ 	esw = container_of(esw_funcs, struct mlx5_eswitch, esw_funcs);
+ 
+ 	host_work->esw = esw;
+ 
+ 	INIT_WORK(&host_work->work, esw_functions_changed_event_handler);
+ 	queue_work(esw->work_queue, &host_work->work);
+ 
+ 	return NOTIFY_OK;
+ }
+ 
+ static void esw_functions_changed_event_init(struct mlx5_eswitch *esw)
+ {
+ 	if (mlx5_eswitch_is_funcs_handler(esw->dev)) {
+ 		MLX5_NB_INIT(&esw->esw_funcs.nb, esw_functions_changed_event,
+ 			     ESW_FUNCTIONS_CHANGED);
+ 		mlx5_eq_notifier_register(esw->dev, &esw->esw_funcs.nb);
+ 	}
+ }
+ 
+ static void esw_functions_changed_event_cleanup(struct mlx5_eswitch *esw)
+ {
+ 	if (!mlx5_eswitch_is_funcs_handler(esw->dev))
+ 		return;
+ 
+ 	mlx5_eq_notifier_unregister(esw->dev, &esw->esw_funcs.nb);
+ 	flush_workqueue(esw->work_queue);
+ }
+ 
+ int esw_offloads_init(struct mlx5_eswitch *esw)
+ {
+ 	int err;
+ 
+ 	err = esw_offloads_steering_init(esw);
+ 	if (err)
+ 		return err;
+ 
+ 	if (mlx5_eswitch_vport_match_metadata_enabled(esw)) {
+ 		err = mlx5_eswitch_enable_passing_vport_metadata(esw);
+ 		if (err)
+ 			goto err_vport_metadata;
+ 	}
+ 
+ 	err = esw_offloads_load_all_reps(esw);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	if (err)
  		goto err_reps;
  
  	esw_offloads_devcom_init(esw);
++<<<<<<< HEAD
++=======
+ 
+ 	esw_functions_changed_event_init(esw);
+ 
+ 	mlx5_rdma_enable_roce(esw->dev);
+ 
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	return 0;
  
  err_reps:
@@@ -1628,13 -2161,13 +1849,21 @@@
  static int esw_offloads_stop(struct mlx5_eswitch *esw,
  			     struct netlink_ext_ack *extack)
  {
- 	int err, err1, num_vfs = esw->dev->priv.sriov.num_vfs;
+ 	int err, err1;
  
++<<<<<<< HEAD
 +	mlx5_eswitch_disable_sriov(esw);
 +	err = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_LEGACY);
 +	if (err) {
 +		NL_SET_ERR_MSG_MOD(extack, "Failed setting eswitch to legacy");
 +		err1 = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_OFFLOADS);
++=======
+ 	mlx5_eswitch_disable(esw);
+ 	err = mlx5_eswitch_enable(esw, MLX5_ESWITCH_LEGACY);
+ 	if (err) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Failed setting eswitch to legacy");
+ 		err1 = mlx5_eswitch_enable(esw, MLX5_ESWITCH_OFFLOADS);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  		if (err1) {
  			NL_SET_ERR_MSG_MOD(extack,
  					   "Failed setting eswitch back to offloads");
@@@ -1644,10 -2177,14 +1873,16 @@@
  	return err;
  }
  
 -void esw_offloads_cleanup(struct mlx5_eswitch *esw)
 +void esw_offloads_cleanup(struct mlx5_eswitch *esw, int nvports)
  {
 -	esw_functions_changed_event_cleanup(esw);
 -	mlx5_rdma_disable_roce(esw->dev);
  	esw_offloads_devcom_cleanup(esw);
++<<<<<<< HEAD
 +	esw_offloads_unload_reps(esw, nvports);
++=======
+ 	esw_offloads_unload_all_reps(esw);
+ 	if (mlx5_eswitch_vport_match_metadata_enabled(esw))
+ 		mlx5_eswitch_disable_passing_vport_metadata(esw);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	esw_offloads_steering_cleanup(esw);
  }
  
@@@ -1952,39 -2489,34 +2187,47 @@@ int mlx5_devlink_eswitch_encap_mode_get
  	return 0;
  }
  
 -void mlx5_eswitch_register_vport_reps(struct mlx5_eswitch *esw,
 -				      const struct mlx5_eswitch_rep_ops *ops,
 -				      u8 rep_type)
 +void mlx5_eswitch_register_vport_rep(struct mlx5_eswitch *esw,
 +				     int vport_index,
 +				     struct mlx5_eswitch_rep_if *__rep_if,
 +				     u8 rep_type)
  {
 -	struct mlx5_eswitch_rep_data *rep_data;
 -	struct mlx5_eswitch_rep *rep;
 -	int i;
 +	struct mlx5_esw_offload *offloads = &esw->offloads;
 +	struct mlx5_eswitch_rep_if *rep_if;
  
 -	esw->offloads.rep_ops[rep_type] = ops;
 -	mlx5_esw_for_all_reps(esw, i, rep) {
 -		rep_data = &rep->rep_data[rep_type];
 -		atomic_set(&rep_data->state, REP_REGISTERED);
 -	}
 +	rep_if = &offloads->vport_reps[vport_index].rep_if[rep_type];
 +
 +	rep_if->load   = __rep_if->load;
 +	rep_if->unload = __rep_if->unload;
 +	rep_if->get_proto_dev = __rep_if->get_proto_dev;
 +	rep_if->priv = __rep_if->priv;
 +
 +	rep_if->valid = true;
  }
 -EXPORT_SYMBOL(mlx5_eswitch_register_vport_reps);
 +EXPORT_SYMBOL(mlx5_eswitch_register_vport_rep);
  
 -void mlx5_eswitch_unregister_vport_reps(struct mlx5_eswitch *esw, u8 rep_type)
 +void mlx5_eswitch_unregister_vport_rep(struct mlx5_eswitch *esw,
 +				       int vport_index, u8 rep_type)
  {
++<<<<<<< HEAD
 +	struct mlx5_esw_offload *offloads = &esw->offloads;
++=======
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	struct mlx5_eswitch_rep *rep;
 -	int i;
  
++<<<<<<< HEAD
 +	rep = &offloads->vport_reps[vport_index];
++=======
+ 	if (esw->mode == MLX5_ESWITCH_OFFLOADS)
+ 		__unload_reps_all_vport(esw, rep_type);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
 +
 +	if (esw->mode == SRIOV_OFFLOADS && esw->vports[vport_index].enabled)
 +		rep->rep_if[rep_type].unload(rep);
  
 -	mlx5_esw_for_all_reps(esw, i, rep)
 -		atomic_set(&rep->rep_data[rep_type].state, REP_UNREGISTERED);
 +	rep->rep_if[rep_type].valid = false;
  }
 -EXPORT_SYMBOL(mlx5_eswitch_unregister_vport_reps);
 +EXPORT_SYMBOL(mlx5_eswitch_unregister_vport_rep);
  
  void *mlx5_eswitch_get_uplink_priv(struct mlx5_eswitch *esw, u8 rep_type)
  {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/sriov.c
index a249b3c3843d,547d0be9025e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@@ -84,7 -77,8 +84,12 @@@ static int mlx5_device_enable_sriov(str
  	if (!MLX5_ESWITCH_MANAGER(dev))
  		goto enable_vfs_hca;
  
++<<<<<<< HEAD
 +	err = mlx5_eswitch_enable_sriov(dev->priv.eswitch, num_vfs, SRIOV_LEGACY);
++=======
+ 	mlx5_eswitch_update_num_of_vfs(dev->priv.eswitch, num_vfs);
+ 	err = mlx5_eswitch_enable(dev->priv.eswitch, MLX5_ESWITCH_LEGACY);
++>>>>>>> 062f4bf4aab5 (net/mlx5: E-Switch, Consolidate eswitch function number of VFs)
  	if (err) {
  		mlx5_core_warn(dev,
  			       "failed to enable eswitch SRIOV (%d)\n", err);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/sriov.c
