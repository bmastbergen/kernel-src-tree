mm, swap: fix race between swapoff and some swap operations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Huang Ying <ying.huang@intel.com>
commit eb085574a7526c4375965c5fbf7e5b0c19cdd336
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/eb085574.failed

When swapin is performed, after getting the swap entry information from
the page table, system will swap in the swap entry, without any lock held
to prevent the swap device from being swapoff.  This may cause the race
like below,

CPU 1				CPU 2
-----				-----
				do_swap_page
				  swapin_readahead
				    __read_swap_cache_async
swapoff				      swapcache_prepare
  p->swap_map = NULL		        __swap_duplicate
					  p->swap_map[?] /* !!! NULL pointer access */

Because swapoff is usually done when system shutdown only, the race may
not hit many people in practice.  But it is still a race need to be fixed.

To fix the race, get_swap_device() is added to check whether the specified
swap entry is valid in its swap device.  If so, it will keep the swap
entry valid via preventing the swap device from being swapoff, until
put_swap_device() is called.

Because swapoff() is very rare code path, to make the normal path runs as
fast as possible, rcu_read_lock/unlock() and synchronize_rcu() instead of
reference count is used to implement get/put_swap_device().  >From
get_swap_device() to put_swap_device(), RCU reader side is locked, so
synchronize_rcu() in swapoff() will wait until put_swap_device() is
called.

In addition to swap_map, cluster_info, etc.  data structure in the struct
swap_info_struct, the swap cache radix tree will be freed after swapoff,
so this patch fixes the race between swap cache looking up and swapoff
too.

Races between some other swap cache usages and swapoff are fixed too via
calling synchronize_rcu() between clearing PageSwapCache() and freeing
swap cache data structure.

Another possible method to fix this is to use preempt_off() +
stop_machine() to prevent the swap device from being swapoff when its data
structure is being accessed.  The overhead in hot-path of both methods is
similar.  The advantages of RCU based method are,

1. stop_machine() may disturb the normal execution code path on other
   CPUs.

2. File cache uses RCU to protect its radix tree.  If the similar
   mechanism is used for swap cache too, it is easier to share code
   between them.

3. RCU is used to protect swap cache in total_swapcache_pages() and
   exit_swap_address_space() already.  The two mechanisms can be
   merged to simplify the logic.

Link: http://lkml.kernel.org/r/20190522015423.14418-1-ying.huang@intel.com
Fixes: 235b62176712 ("mm/swap: add cluster lock")
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
Not-nacked-by: Hugh Dickins <hughd@google.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Jérôme Glisse <jglisse@redhat.com>
	Cc: Yang Shi <yang.shi@linux.alibaba.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Dave Jiang <dave.jiang@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit eb085574a7526c4375965c5fbf7e5b0c19cdd336)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swap.h
#	mm/swapfile.c
diff --cc include/linux/swap.h
index cdca8da805db,6358a6185634..000000000000
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@@ -170,13 -169,15 +170,25 @@@ enum 
  	SWP_SOLIDSTATE	= (1 << 4),	/* blkdev seeks are cheap */
  	SWP_CONTINUED	= (1 << 5),	/* swap_map has count continuation */
  	SWP_BLKDEV	= (1 << 6),	/* its a block device */
++<<<<<<< HEAD
 +	SWP_FILE	= (1 << 7),	/* set after swap_activate success */
 +	SWP_AREA_DISCARD = (1 << 8),	/* single-time swap area discards */
 +	SWP_PAGE_DISCARD = (1 << 9),	/* freed swap page-cluster discards */
 +	SWP_STABLE_WRITES = (1 << 10),	/* no overwrite PG_writeback pages */
 +	SWP_SYNCHRONOUS_IO = (1 << 11),	/* synchronous IO is efficient */
 +					/* add others here before... */
 +	SWP_SCANNING	= (1 << 12),	/* refcount in scan_swap_map */
++=======
+ 	SWP_ACTIVATED	= (1 << 7),	/* set after swap_activate success */
+ 	SWP_FS		= (1 << 8),	/* swap file goes through fs */
+ 	SWP_AREA_DISCARD = (1 << 9),	/* single-time swap area discards */
+ 	SWP_PAGE_DISCARD = (1 << 10),	/* freed swap page-cluster discards */
+ 	SWP_STABLE_WRITES = (1 << 11),	/* no overwrite PG_writeback pages */
+ 	SWP_SYNCHRONOUS_IO = (1 << 12),	/* synchronous IO is efficient */
+ 	SWP_VALID	= (1 << 13),	/* swap is valid to be operated on? */
+ 					/* add others here before... */
+ 	SWP_SCANNING	= (1 << 14),	/* refcount in scan_swap_map */
++>>>>>>> eb085574a752 (mm, swap: fix race between swapoff and some swap operations)
  };
  
  #define SWAP_CLUSTER_MAX 32UL
diff --cc mm/swapfile.c
index a53b57c54471,dbab16ddefa6..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -1156,7 -1183,83 +1155,84 @@@ static unsigned char __swap_entry_free(
  	usage = count | has_cache;
  	p->swap_map[offset] = usage ? : SWAP_HAS_CACHE;
  
++<<<<<<< HEAD
++=======
+ 	return usage;
+ }
+ 
+ /*
+  * Check whether swap entry is valid in the swap device.  If so,
+  * return pointer to swap_info_struct, and keep the swap entry valid
+  * via preventing the swap device from being swapoff, until
+  * put_swap_device() is called.  Otherwise return NULL.
+  *
+  * The entirety of the RCU read critical section must come before the
+  * return from or after the call to synchronize_rcu() in
+  * enable_swap_info() or swapoff().  So if "si->flags & SWP_VALID" is
+  * true, the si->map, si->cluster_info, etc. must be valid in the
+  * critical section.
+  *
+  * Notice that swapoff or swapoff+swapon can still happen before the
+  * rcu_read_lock() in get_swap_device() or after the rcu_read_unlock()
+  * in put_swap_device() if there isn't any other way to prevent
+  * swapoff, such as page lock, page table lock, etc.  The caller must
+  * be prepared for that.  For example, the following situation is
+  * possible.
+  *
+  *   CPU1				CPU2
+  *   do_swap_page()
+  *     ...				swapoff+swapon
+  *     __read_swap_cache_async()
+  *       swapcache_prepare()
+  *         __swap_duplicate()
+  *           // check swap_map
+  *     // verify PTE not changed
+  *
+  * In __swap_duplicate(), the swap_map need to be checked before
+  * changing partly because the specified swap entry may be for another
+  * swap device which has been swapoff.  And in do_swap_page(), after
+  * the page is read from the swap device, the PTE is verified not
+  * changed with the page table locked to check whether the swap device
+  * has been swapoff or swapoff+swapon.
+  */
+ struct swap_info_struct *get_swap_device(swp_entry_t entry)
+ {
+ 	struct swap_info_struct *si;
+ 	unsigned long offset;
+ 
+ 	if (!entry.val)
+ 		goto out;
+ 	si = swp_swap_info(entry);
+ 	if (!si)
+ 		goto bad_nofile;
+ 
+ 	rcu_read_lock();
+ 	if (!(si->flags & SWP_VALID))
+ 		goto unlock_out;
+ 	offset = swp_offset(entry);
+ 	if (offset >= si->max)
+ 		goto unlock_out;
+ 
+ 	return si;
+ bad_nofile:
+ 	pr_err("%s: %s%08lx\n", __func__, Bad_file, entry.val);
+ out:
+ 	return NULL;
+ unlock_out:
+ 	rcu_read_unlock();
+ 	return NULL;
+ }
+ 
+ static unsigned char __swap_entry_free(struct swap_info_struct *p,
+ 				       swp_entry_t entry, unsigned char usage)
+ {
+ 	struct swap_cluster_info *ci;
+ 	unsigned long offset = swp_offset(entry);
+ 
+ 	ci = lock_cluster_or_swap_info(p, offset);
+ 	usage = __swap_entry_free_locked(p, offset, usage);
++>>>>>>> eb085574a752 (mm, swap: fix race between swapoff and some swap operations)
  	unlock_cluster_or_swap_info(p, ci);
 -	if (!usage)
 -		free_swap_slot(entry);
  
  	return usage;
  }
* Unmerged path include/linux/swap.h
diff --git a/mm/memory.c b/mm/memory.c
index d4172eb72cef..8c14a914b156 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3041,7 +3041,7 @@ int do_swap_page(struct vm_fault *vmf)
 		struct swap_info_struct *si = swp_swap_info(entry);
 
 		if (si->flags & SWP_SYNCHRONOUS_IO &&
-				__swap_count(si, entry) == 1) {
+				__swap_count(entry) == 1) {
 			/* skip swapcache */
 			page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,
 							vmf->address);
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 0d6a7f268d2e..b50c9b2ec2b4 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -334,8 +334,13 @@ struct page *lookup_swap_cache(swp_entry_t entry, struct vm_area_struct *vma,
 			       unsigned long addr)
 {
 	struct page *page;
+	struct swap_info_struct *si;
 
+	si = get_swap_device(entry);
+	if (!si)
+		return NULL;
 	page = find_get_page(swap_address_space(entry), swp_offset(entry));
+	put_swap_device(si);
 
 	INC_CACHE_INFO(find_total);
 	if (page) {
@@ -378,8 +383,8 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 			struct vm_area_struct *vma, unsigned long addr,
 			bool *new_page_allocated)
 {
-	struct page *found_page, *new_page = NULL;
-	struct address_space *swapper_space = swap_address_space(entry);
+	struct page *found_page = NULL, *new_page = NULL;
+	struct swap_info_struct *si;
 	int err;
 	*new_page_allocated = false;
 
@@ -389,7 +394,12 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 		 * called after lookup_swap_cache() failed, re-calling
 		 * that would confuse statistics.
 		 */
-		found_page = find_get_page(swapper_space, swp_offset(entry));
+		si = get_swap_device(entry);
+		if (!si)
+			break;
+		found_page = find_get_page(swap_address_space(entry),
+					   swp_offset(entry));
+		put_swap_device(si);
 		if (found_page)
 			break;
 
* Unmerged path mm/swapfile.c
