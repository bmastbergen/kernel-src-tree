RDMA/core: Fix error code in stat_get_doit_qp()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dan Carpenter <dan.carpenter@oracle.com>
commit 932727c55653c1d7838d0ecd0cdce4393be156e0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/932727c5.failed

We need to set the error codes on these paths.  Currently the only
possible error code is -EMSGSIZE so that's what the patch uses.

Fixes: 83c2c1fcbd08 ("RDMA/nldev: Allow get counter mode through RDMA netlink")
	Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
Link: https://lore.kernel.org/r/20190809101311.GA17867@mwanda
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 932727c55653c1d7838d0ecd0cdce4393be156e0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/nldev.c
diff --cc drivers/infiniband/core/nldev.c
index efccd8e0fb77,87d40d1ecdde..000000000000
--- a/drivers/infiniband/core/nldev.c
+++ b/drivers/infiniband/core/nldev.c
@@@ -1089,6 -1413,617 +1089,620 @@@ RES_GET_FUNCS(cm_id, RDMA_RESTRACK_CM_I
  RES_GET_FUNCS(cq, RDMA_RESTRACK_CQ);
  RES_GET_FUNCS(pd, RDMA_RESTRACK_PD);
  RES_GET_FUNCS(mr, RDMA_RESTRACK_MR);
++<<<<<<< HEAD
++=======
+ RES_GET_FUNCS(counter, RDMA_RESTRACK_COUNTER);
+ 
+ static LIST_HEAD(link_ops);
+ static DECLARE_RWSEM(link_ops_rwsem);
+ 
+ static const struct rdma_link_ops *link_ops_get(const char *type)
+ {
+ 	const struct rdma_link_ops *ops;
+ 
+ 	list_for_each_entry(ops, &link_ops, list) {
+ 		if (!strcmp(ops->type, type))
+ 			goto out;
+ 	}
+ 	ops = NULL;
+ out:
+ 	return ops;
+ }
+ 
+ void rdma_link_register(struct rdma_link_ops *ops)
+ {
+ 	down_write(&link_ops_rwsem);
+ 	if (WARN_ON_ONCE(link_ops_get(ops->type)))
+ 		goto out;
+ 	list_add(&ops->list, &link_ops);
+ out:
+ 	up_write(&link_ops_rwsem);
+ }
+ EXPORT_SYMBOL(rdma_link_register);
+ 
+ void rdma_link_unregister(struct rdma_link_ops *ops)
+ {
+ 	down_write(&link_ops_rwsem);
+ 	list_del(&ops->list);
+ 	up_write(&link_ops_rwsem);
+ }
+ EXPORT_SYMBOL(rdma_link_unregister);
+ 
+ static int nldev_newlink(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			  struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	char ibdev_name[IB_DEVICE_NAME_MAX];
+ 	const struct rdma_link_ops *ops;
+ 	char ndev_name[IFNAMSIZ];
+ 	struct net_device *ndev;
+ 	char type[IFNAMSIZ];
+ 	int err;
+ 
+ 	err = nlmsg_parse_deprecated(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 				     nldev_policy, extack);
+ 	if (err || !tb[RDMA_NLDEV_ATTR_DEV_NAME] ||
+ 	    !tb[RDMA_NLDEV_ATTR_LINK_TYPE] || !tb[RDMA_NLDEV_ATTR_NDEV_NAME])
+ 		return -EINVAL;
+ 
+ 	nla_strlcpy(ibdev_name, tb[RDMA_NLDEV_ATTR_DEV_NAME],
+ 		    sizeof(ibdev_name));
+ 	if (strchr(ibdev_name, '%'))
+ 		return -EINVAL;
+ 
+ 	nla_strlcpy(type, tb[RDMA_NLDEV_ATTR_LINK_TYPE], sizeof(type));
+ 	nla_strlcpy(ndev_name, tb[RDMA_NLDEV_ATTR_NDEV_NAME],
+ 		    sizeof(ndev_name));
+ 
+ 	ndev = dev_get_by_name(sock_net(skb->sk), ndev_name);
+ 	if (!ndev)
+ 		return -ENODEV;
+ 
+ 	down_read(&link_ops_rwsem);
+ 	ops = link_ops_get(type);
+ #ifdef CONFIG_MODULES
+ 	if (!ops) {
+ 		up_read(&link_ops_rwsem);
+ 		request_module("rdma-link-%s", type);
+ 		down_read(&link_ops_rwsem);
+ 		ops = link_ops_get(type);
+ 	}
+ #endif
+ 	err = ops ? ops->newlink(ibdev_name, ndev) : -EINVAL;
+ 	up_read(&link_ops_rwsem);
+ 	dev_put(ndev);
+ 
+ 	return err;
+ }
+ 
+ static int nldev_dellink(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			  struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	struct ib_device *device;
+ 	u32 index;
+ 	int err;
+ 
+ 	err = nlmsg_parse_deprecated(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 				     nldev_policy, extack);
+ 	if (err || !tb[RDMA_NLDEV_ATTR_DEV_INDEX])
+ 		return -EINVAL;
+ 
+ 	index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 	device = ib_device_get_by_index(sock_net(skb->sk), index);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	if (!(device->attrs.device_cap_flags & IB_DEVICE_ALLOW_USER_UNREG)) {
+ 		ib_device_put(device);
+ 		return -EINVAL;
+ 	}
+ 
+ 	ib_unregister_device_and_put(device);
+ 	return 0;
+ }
+ 
+ static int nldev_get_chardev(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			     struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	char client_name[RDMA_NLDEV_ATTR_CHARDEV_TYPE_SIZE];
+ 	struct ib_client_nl_info data = {};
+ 	struct ib_device *ibdev = NULL;
+ 	struct sk_buff *msg;
+ 	u32 index;
+ 	int err;
+ 
+ 	err = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1, nldev_policy,
+ 			  extack);
+ 	if (err || !tb[RDMA_NLDEV_ATTR_CHARDEV_TYPE])
+ 		return -EINVAL;
+ 
+ 	nla_strlcpy(client_name, tb[RDMA_NLDEV_ATTR_CHARDEV_TYPE],
+ 		    sizeof(client_name));
+ 
+ 	if (tb[RDMA_NLDEV_ATTR_DEV_INDEX]) {
+ 		index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 		ibdev = ib_device_get_by_index(sock_net(skb->sk), index);
+ 		if (!ibdev)
+ 			return -EINVAL;
+ 
+ 		if (tb[RDMA_NLDEV_ATTR_PORT_INDEX]) {
+ 			data.port = nla_get_u32(tb[RDMA_NLDEV_ATTR_PORT_INDEX]);
+ 			if (!rdma_is_port_valid(ibdev, data.port)) {
+ 				err = -EINVAL;
+ 				goto out_put;
+ 			}
+ 		} else {
+ 			data.port = -1;
+ 		}
+ 	} else if (tb[RDMA_NLDEV_ATTR_PORT_INDEX]) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg) {
+ 		err = -ENOMEM;
+ 		goto out_put;
+ 	}
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_GET_CHARDEV),
+ 			0, 0);
+ 
+ 	data.nl_msg = msg;
+ 	err = ib_get_client_nl_info(ibdev, client_name, &data);
+ 	if (err)
+ 		goto out_nlmsg;
+ 
+ 	err = nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_CHARDEV,
+ 				huge_encode_dev(data.cdev->devt),
+ 				RDMA_NLDEV_ATTR_PAD);
+ 	if (err)
+ 		goto out_data;
+ 	err = nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_CHARDEV_ABI, data.abi,
+ 				RDMA_NLDEV_ATTR_PAD);
+ 	if (err)
+ 		goto out_data;
+ 	if (nla_put_string(msg, RDMA_NLDEV_ATTR_CHARDEV_NAME,
+ 			   dev_name(data.cdev))) {
+ 		err = -EMSGSIZE;
+ 		goto out_data;
+ 	}
+ 
+ 	nlmsg_end(msg, nlh);
+ 	put_device(data.cdev);
+ 	if (ibdev)
+ 		ib_device_put(ibdev);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ 
+ out_data:
+ 	put_device(data.cdev);
+ out_nlmsg:
+ 	nlmsg_free(msg);
+ out_put:
+ 	if (ibdev)
+ 		ib_device_put(ibdev);
+ 	return err;
+ }
+ 
+ static int nldev_sys_get_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			      struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	struct sk_buff *msg;
+ 	int err;
+ 
+ 	err = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, extack);
+ 	if (err)
+ 		return err;
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg)
+ 		return -ENOMEM;
+ 
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_SYS_GET),
+ 			0, 0);
+ 
+ 	err = nla_put_u8(msg, RDMA_NLDEV_SYS_ATTR_NETNS_MODE,
+ 			 (u8)ib_devices_shared_netns);
+ 	if (err) {
+ 		nlmsg_free(msg);
+ 		return err;
+ 	}
+ 	nlmsg_end(msg, nlh);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ }
+ 
+ static int nldev_set_sys_set_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 				  struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	u8 enable;
+ 	int err;
+ 
+ 	err = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, extack);
+ 	if (err || !tb[RDMA_NLDEV_SYS_ATTR_NETNS_MODE])
+ 		return -EINVAL;
+ 
+ 	enable = nla_get_u8(tb[RDMA_NLDEV_SYS_ATTR_NETNS_MODE]);
+ 	/* Only 0 and 1 are supported */
+ 	if (enable > 1)
+ 		return -EINVAL;
+ 
+ 	err = rdma_compatdev_set(enable);
+ 	return err;
+ }
+ 
+ static int nldev_stat_set_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			       struct netlink_ext_ack *extack)
+ {
+ 	u32 index, port, mode, mask = 0, qpn, cntn = 0;
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	struct ib_device *device;
+ 	struct sk_buff *msg;
+ 	int ret;
+ 
+ 	ret = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, extack);
+ 	/* Currently only counter for QP is supported */
+ 	if (ret || !tb[RDMA_NLDEV_ATTR_STAT_RES] ||
+ 	    !tb[RDMA_NLDEV_ATTR_DEV_INDEX] ||
+ 	    !tb[RDMA_NLDEV_ATTR_PORT_INDEX] || !tb[RDMA_NLDEV_ATTR_STAT_MODE])
+ 		return -EINVAL;
+ 
+ 	if (nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_RES]) != RDMA_NLDEV_ATTR_RES_QP)
+ 		return -EINVAL;
+ 
+ 	index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 	device = ib_device_get_by_index(sock_net(skb->sk), index);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	port = nla_get_u32(tb[RDMA_NLDEV_ATTR_PORT_INDEX]);
+ 	if (!rdma_is_port_valid(device, port)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_STAT_SET),
+ 			0, 0);
+ 
+ 	mode = nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_MODE]);
+ 	if (mode == RDMA_COUNTER_MODE_AUTO) {
+ 		if (tb[RDMA_NLDEV_ATTR_STAT_AUTO_MODE_MASK])
+ 			mask = nla_get_u32(
+ 				tb[RDMA_NLDEV_ATTR_STAT_AUTO_MODE_MASK]);
+ 
+ 		ret = rdma_counter_set_auto_mode(device, port,
+ 						 mask ? true : false, mask);
+ 		if (ret)
+ 			goto err_msg;
+ 	} else {
+ 		qpn = nla_get_u32(tb[RDMA_NLDEV_ATTR_RES_LQPN]);
+ 		if (tb[RDMA_NLDEV_ATTR_STAT_COUNTER_ID]) {
+ 			cntn = nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_COUNTER_ID]);
+ 			ret = rdma_counter_bind_qpn(device, port, qpn, cntn);
+ 		} else {
+ 			ret = rdma_counter_bind_qpn_alloc(device, port,
+ 							  qpn, &cntn);
+ 		}
+ 		if (ret)
+ 			goto err_msg;
+ 
+ 		if (fill_nldev_handle(msg, device) ||
+ 		    nla_put_u32(msg, RDMA_NLDEV_ATTR_PORT_INDEX, port) ||
+ 		    nla_put_u32(msg, RDMA_NLDEV_ATTR_STAT_COUNTER_ID, cntn) ||
+ 		    nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_LQPN, qpn)) {
+ 			ret = -EMSGSIZE;
+ 			goto err_fill;
+ 		}
+ 	}
+ 
+ 	nlmsg_end(msg, nlh);
+ 	ib_device_put(device);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ 
+ err_fill:
+ 	rdma_counter_unbind_qpn(device, port, qpn, cntn);
+ err_msg:
+ 	nlmsg_free(msg);
+ err:
+ 	ib_device_put(device);
+ 	return ret;
+ }
+ 
+ static int nldev_stat_del_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			       struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	struct ib_device *device;
+ 	struct sk_buff *msg;
+ 	u32 index, port, qpn, cntn;
+ 	int ret;
+ 
+ 	ret = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, extack);
+ 	if (ret || !tb[RDMA_NLDEV_ATTR_STAT_RES] ||
+ 	    !tb[RDMA_NLDEV_ATTR_DEV_INDEX] || !tb[RDMA_NLDEV_ATTR_PORT_INDEX] ||
+ 	    !tb[RDMA_NLDEV_ATTR_STAT_COUNTER_ID] ||
+ 	    !tb[RDMA_NLDEV_ATTR_RES_LQPN])
+ 		return -EINVAL;
+ 
+ 	if (nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_RES]) != RDMA_NLDEV_ATTR_RES_QP)
+ 		return -EINVAL;
+ 
+ 	index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 	device = ib_device_get_by_index(sock_net(skb->sk), index);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	port = nla_get_u32(tb[RDMA_NLDEV_ATTR_PORT_INDEX]);
+ 	if (!rdma_is_port_valid(device, port)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_STAT_SET),
+ 			0, 0);
+ 
+ 	cntn = nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_COUNTER_ID]);
+ 	qpn = nla_get_u32(tb[RDMA_NLDEV_ATTR_RES_LQPN]);
+ 	ret = rdma_counter_unbind_qpn(device, port, qpn, cntn);
+ 	if (ret)
+ 		goto err_unbind;
+ 
+ 	if (fill_nldev_handle(msg, device) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_PORT_INDEX, port) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_STAT_COUNTER_ID, cntn) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_LQPN, qpn)) {
+ 		ret = -EMSGSIZE;
+ 		goto err_fill;
+ 	}
+ 
+ 	nlmsg_end(msg, nlh);
+ 	ib_device_put(device);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ 
+ err_fill:
+ 	rdma_counter_bind_qpn(device, port, qpn, cntn);
+ err_unbind:
+ 	nlmsg_free(msg);
+ err:
+ 	ib_device_put(device);
+ 	return ret;
+ }
+ 
+ static int stat_get_doit_default_counter(struct sk_buff *skb,
+ 					 struct nlmsghdr *nlh,
+ 					 struct netlink_ext_ack *extack,
+ 					 struct nlattr *tb[])
+ {
+ 	struct rdma_hw_stats *stats;
+ 	struct nlattr *table_attr;
+ 	struct ib_device *device;
+ 	int ret, num_cnts, i;
+ 	struct sk_buff *msg;
+ 	u32 index, port;
+ 	u64 v;
+ 
+ 	if (!tb[RDMA_NLDEV_ATTR_DEV_INDEX] || !tb[RDMA_NLDEV_ATTR_PORT_INDEX])
+ 		return -EINVAL;
+ 
+ 	index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 	device = ib_device_get_by_index(sock_net(skb->sk), index);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	if (!device->ops.alloc_hw_stats || !device->ops.get_hw_stats) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	port = nla_get_u32(tb[RDMA_NLDEV_ATTR_PORT_INDEX]);
+ 	if (!rdma_is_port_valid(device, port)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_STAT_GET),
+ 			0, 0);
+ 
+ 	if (fill_nldev_handle(msg, device) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_PORT_INDEX, port)) {
+ 		ret = -EMSGSIZE;
+ 		goto err_msg;
+ 	}
+ 
+ 	stats = device->port_data ? device->port_data[port].hw_stats : NULL;
+ 	if (stats == NULL) {
+ 		ret = -EINVAL;
+ 		goto err_msg;
+ 	}
+ 	mutex_lock(&stats->lock);
+ 
+ 	num_cnts = device->ops.get_hw_stats(device, stats, port, 0);
+ 	if (num_cnts < 0) {
+ 		ret = -EINVAL;
+ 		goto err_stats;
+ 	}
+ 
+ 	table_attr = nla_nest_start(msg, RDMA_NLDEV_ATTR_STAT_HWCOUNTERS);
+ 	if (!table_attr) {
+ 		ret = -EMSGSIZE;
+ 		goto err_stats;
+ 	}
+ 	for (i = 0; i < num_cnts; i++) {
+ 		v = stats->value[i] +
+ 			rdma_counter_get_hwstat_value(device, port, i);
+ 		if (fill_stat_hwcounter_entry(msg, stats->names[i], v)) {
+ 			ret = -EMSGSIZE;
+ 			goto err_table;
+ 		}
+ 	}
+ 	nla_nest_end(msg, table_attr);
+ 
+ 	mutex_unlock(&stats->lock);
+ 	nlmsg_end(msg, nlh);
+ 	ib_device_put(device);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ 
+ err_table:
+ 	nla_nest_cancel(msg, table_attr);
+ err_stats:
+ 	mutex_unlock(&stats->lock);
+ err_msg:
+ 	nlmsg_free(msg);
+ err:
+ 	ib_device_put(device);
+ 	return ret;
+ }
+ 
+ static int stat_get_doit_qp(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			    struct netlink_ext_ack *extack, struct nlattr *tb[])
+ 
+ {
+ 	static enum rdma_nl_counter_mode mode;
+ 	static enum rdma_nl_counter_mask mask;
+ 	struct ib_device *device;
+ 	struct sk_buff *msg;
+ 	u32 index, port;
+ 	int ret;
+ 
+ 	if (tb[RDMA_NLDEV_ATTR_STAT_COUNTER_ID])
+ 		return nldev_res_get_counter_doit(skb, nlh, extack);
+ 
+ 	if (!tb[RDMA_NLDEV_ATTR_STAT_MODE] ||
+ 	    !tb[RDMA_NLDEV_ATTR_DEV_INDEX] || !tb[RDMA_NLDEV_ATTR_PORT_INDEX])
+ 		return -EINVAL;
+ 
+ 	index = nla_get_u32(tb[RDMA_NLDEV_ATTR_DEV_INDEX]);
+ 	device = ib_device_get_by_index(sock_net(skb->sk), index);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	port = nla_get_u32(tb[RDMA_NLDEV_ATTR_PORT_INDEX]);
+ 	if (!rdma_is_port_valid(device, port)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+ 	if (!msg) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	nlh = nlmsg_put(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq,
+ 			RDMA_NL_GET_TYPE(RDMA_NL_NLDEV,
+ 					 RDMA_NLDEV_CMD_STAT_GET),
+ 			0, 0);
+ 
+ 	ret = rdma_counter_get_mode(device, port, &mode, &mask);
+ 	if (ret)
+ 		goto err_msg;
+ 
+ 	if (fill_nldev_handle(msg, device) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_PORT_INDEX, port) ||
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_STAT_MODE, mode)) {
+ 		ret = -EMSGSIZE;
+ 		goto err_msg;
+ 	}
+ 
+ 	if ((mode == RDMA_COUNTER_MODE_AUTO) &&
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_STAT_AUTO_MODE_MASK, mask)) {
+ 		ret = -EMSGSIZE;
+ 		goto err_msg;
+ 	}
+ 
+ 	nlmsg_end(msg, nlh);
+ 	ib_device_put(device);
+ 	return rdma_nl_unicast(msg, NETLINK_CB(skb).portid);
+ 
+ err_msg:
+ 	nlmsg_free(msg);
+ err:
+ 	ib_device_put(device);
+ 	return ret;
+ }
+ 
+ static int nldev_stat_get_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			       struct netlink_ext_ack *extack)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	int ret;
+ 
+ 	ret = nlmsg_parse(nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, extack);
+ 	if (ret)
+ 		return -EINVAL;
+ 
+ 	if (!tb[RDMA_NLDEV_ATTR_STAT_RES])
+ 		return stat_get_doit_default_counter(skb, nlh, extack, tb);
+ 
+ 	switch (nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_RES])) {
+ 	case RDMA_NLDEV_ATTR_RES_QP:
+ 		ret = stat_get_doit_qp(skb, nlh, extack, tb);
+ 		break;
+ 
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int nldev_stat_get_dumpit(struct sk_buff *skb,
+ 				 struct netlink_callback *cb)
+ {
+ 	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
+ 	int ret;
+ 
+ 	ret = nlmsg_parse(cb->nlh, 0, tb, RDMA_NLDEV_ATTR_MAX - 1,
+ 			  nldev_policy, NULL);
+ 	if (ret || !tb[RDMA_NLDEV_ATTR_STAT_RES])
+ 		return -EINVAL;
+ 
+ 	switch (nla_get_u32(tb[RDMA_NLDEV_ATTR_STAT_RES])) {
+ 	case RDMA_NLDEV_ATTR_RES_QP:
+ 		ret = nldev_res_get_counter_dumpit(skb, cb);
+ 		break;
+ 
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
++>>>>>>> 932727c55653 (RDMA/core: Fix error code in stat_get_doit_qp())
  
  static const struct rdma_nl_cbs nldev_cb_table[RDMA_NLDEV_NUM_OPS] = {
  	[RDMA_NLDEV_CMD_GET] = {
* Unmerged path drivers/infiniband/core/nldev.c
