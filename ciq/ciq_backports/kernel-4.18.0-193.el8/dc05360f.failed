net: convert rps_needed and rfs_needed to new static branch api

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] convert rps_needed and rfs_needed to new static branch api (Jiri Benc) [1749814]
Rebuild_FUZZ: 95.87%
commit-author Eric Dumazet <edumazet@google.com>
commit dc05360fee660a9dbe59824b3f7896534210432b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/dc05360f.failed

We prefer static_branch_unlikely() over static_key_false() these days.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
	Acked-by: Willem de Bruijn <willemb@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit dc05360fee660a9dbe59824b3f7896534210432b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/dev.c
diff --cc net/core/dev.c
index db5e30e658df,9ca2d3abfd1a..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -5072,6 -5195,55 +5072,58 @@@ static int netif_receive_skb_internal(s
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void netif_receive_skb_list_internal(struct list_head *head)
+ {
+ 	struct bpf_prog *xdp_prog = NULL;
+ 	struct sk_buff *skb, *next;
+ 	struct list_head sublist;
+ 
+ 	INIT_LIST_HEAD(&sublist);
+ 	list_for_each_entry_safe(skb, next, head, list) {
+ 		net_timestamp_check(netdev_tstamp_prequeue, skb);
+ 		skb_list_del_init(skb);
+ 		if (!skb_defer_rx_timestamp(skb))
+ 			list_add_tail(&skb->list, &sublist);
+ 	}
+ 	list_splice_init(&sublist, head);
+ 
+ 	if (static_branch_unlikely(&generic_xdp_needed_key)) {
+ 		preempt_disable();
+ 		rcu_read_lock();
+ 		list_for_each_entry_safe(skb, next, head, list) {
+ 			xdp_prog = rcu_dereference(skb->dev->xdp_prog);
+ 			skb_list_del_init(skb);
+ 			if (do_xdp_generic(xdp_prog, skb) == XDP_PASS)
+ 				list_add_tail(&skb->list, &sublist);
+ 		}
+ 		rcu_read_unlock();
+ 		preempt_enable();
+ 		/* Put passed packets back on main list */
+ 		list_splice_init(&sublist, head);
+ 	}
+ 
+ 	rcu_read_lock();
+ #ifdef CONFIG_RPS
+ 	if (static_branch_unlikely(&rps_needed)) {
+ 		list_for_each_entry_safe(skb, next, head, list) {
+ 			struct rps_dev_flow voidflow, *rflow = &voidflow;
+ 			int cpu = get_rps_cpu(skb->dev, skb, &rflow);
+ 
+ 			if (cpu >= 0) {
+ 				/* Will be handled, remove from list */
+ 				skb_list_del_init(skb);
+ 				enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
+ 			}
+ 		}
+ 	}
+ #endif
+ 	__netif_receive_skb_list(head);
+ 	rcu_read_unlock();
+ }
+ 
++>>>>>>> dc05360fee66 (net: convert rps_needed and rfs_needed to new static branch api)
  /**
   *	netif_receive_skb - process receive buffer from network
   *	@skb: buffer to process
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 3271c040b424..2d071e5a9642 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -1037,7 +1037,7 @@ static int tun_net_close(struct net_device *dev)
 static void tun_automq_xmit(struct tun_struct *tun, struct sk_buff *skb)
 {
 #ifdef CONFIG_RPS
-	if (tun->numqueues == 1 && static_key_false(&rps_needed)) {
+	if (tun->numqueues == 1 && static_branch_unlikely(&rps_needed)) {
 		/* Select queue was not called for the skbuff, so we extract the
 		 * RPS hash and save it into the flow_table here.
 		 */
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 4c743add611a..80fc08c95ff1 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -196,8 +196,8 @@ struct net_device_stats {
 
 #ifdef CONFIG_RPS
 #include <linux/static_key.h>
-extern struct static_key rps_needed;
-extern struct static_key rfs_needed;
+extern struct static_key_false rps_needed;
+extern struct static_key_false rfs_needed;
 #endif
 
 struct neighbour;
diff --git a/include/net/sock.h b/include/net/sock.h
index d1434f1d108d..7a7d6dcbbac2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -980,7 +980,7 @@ static inline void sock_rps_record_flow_hash(__u32 hash)
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
 #ifdef CONFIG_RPS
-	if (static_key_false(&rfs_needed)) {
+	if (static_branch_unlikely(&rfs_needed)) {
 		/* Reading sk->sk_rxhash might incur an expensive cache line
 		 * miss.
 		 *
* Unmerged path net/core/dev.c
diff --git a/net/core/net-sysfs.c b/net/core/net-sysfs.c
index 0d64330074ff..9d89a48d8bc5 100644
--- a/net/core/net-sysfs.c
+++ b/net/core/net-sysfs.c
@@ -765,9 +765,9 @@ static ssize_t store_rps_map(struct netdev_rx_queue *queue,
 	rcu_assign_pointer(queue->rps_map, map);
 
 	if (map)
-		static_key_slow_inc(&rps_needed);
+		static_branch_inc(&rps_needed);
 	if (old_map)
-		static_key_slow_dec(&rps_needed);
+		static_branch_dec(&rps_needed);
 
 	mutex_unlock(&rps_map_mutex);
 
diff --git a/net/core/sysctl_net_core.c b/net/core/sysctl_net_core.c
index d67ec17f2cc8..6651f3648482 100644
--- a/net/core/sysctl_net_core.c
+++ b/net/core/sysctl_net_core.c
@@ -86,12 +86,12 @@ static int rps_sock_flow_sysctl(struct ctl_table *table, int write,
 		if (sock_table != orig_sock_table) {
 			rcu_assign_pointer(rps_sock_flow_table, sock_table);
 			if (sock_table) {
-				static_key_slow_inc(&rps_needed);
-				static_key_slow_inc(&rfs_needed);
+				static_branch_inc(&rps_needed);
+				static_branch_inc(&rfs_needed);
 			}
 			if (orig_sock_table) {
-				static_key_slow_dec(&rps_needed);
-				static_key_slow_dec(&rfs_needed);
+				static_branch_dec(&rps_needed);
+				static_branch_dec(&rfs_needed);
 				synchronize_rcu();
 				vfree(orig_sock_table);
 			}
