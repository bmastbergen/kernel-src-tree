locking/lockdep: Check data structure consistency

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Bart Van Assche <bvanassche@acm.org>
commit b526b2e39a53b312f5a6867ce57824247aa0ce8b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/b526b2e3.failed

Debugging lockdep data structure inconsistencies is challenging. Add
code that verifies data structure consistency at runtime. That code is
disabled by default because it is very CPU intensive.

	Signed-off-by: Bart Van Assche <bvanassche@acm.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Johannes Berg <johannes@sipsolutions.net>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Waiman Long <longman@redhat.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: johannes.berg@intel.com
	Cc: tj@kernel.org
Link: https://lkml.kernel.org/r/20190214230058.196511-17-bvanassche@acm.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b526b2e39a53b312f5a6867ce57824247aa0ce8b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index daaed4296d60,630be9ac6253..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -739,8 -777,171 +741,170 @@@ static bool assign_lock_key(struct lock
  	return true;
  }
  
+ /* Check whether element @e occurs in list @h */
+ static bool in_list(struct list_head *e, struct list_head *h)
+ {
+ 	struct list_head *f;
+ 
+ 	list_for_each(f, h) {
+ 		if (e == f)
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Check whether entry @e occurs in any of the locks_after or locks_before
+  * lists.
+  */
+ static bool in_any_class_list(struct list_head *e)
+ {
+ 	struct lock_class *class;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (in_list(e, &class->locks_after) ||
+ 		    in_list(e, &class->locks_before))
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool class_lock_list_valid(struct lock_class *c, struct list_head *h)
+ {
+ 	struct lock_list *e;
+ 
+ 	list_for_each_entry(e, h, entry) {
+ 		if (e->links_to != c) {
+ 			printk(KERN_INFO "class %s: mismatch for lock entry %ld; class %s <> %s",
+ 			       c->name ? : "(?)",
+ 			       (unsigned long)(e - list_entries),
+ 			       e->links_to && e->links_to->name ?
+ 			       e->links_to->name : "(?)",
+ 			       e->class && e->class->name ? e->class->name :
+ 			       "(?)");
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ static u16 chain_hlocks[];
+ 
+ static bool check_lock_chain_key(struct lock_chain *chain)
+ {
+ #ifdef CONFIG_PROVE_LOCKING
+ 	u64 chain_key = 0;
+ 	int i;
+ 
+ 	for (i = chain->base; i < chain->base + chain->depth; i++)
+ 		chain_key = iterate_chain_key(chain_key, chain_hlocks[i] + 1);
+ 	/*
+ 	 * The 'unsigned long long' casts avoid that a compiler warning
+ 	 * is reported when building tools/lib/lockdep.
+ 	 */
+ 	if (chain->chain_key != chain_key)
+ 		printk(KERN_INFO "chain %lld: key %#llx <> %#llx\n",
+ 		       (unsigned long long)(chain - lock_chains),
+ 		       (unsigned long long)chain->chain_key,
+ 		       (unsigned long long)chain_key);
+ 	return chain->chain_key == chain_key;
+ #else
+ 	return true;
+ #endif
+ }
+ 
+ static bool in_any_zapped_class_list(struct lock_class *class)
+ {
+ 	struct pending_free *pf;
+ 	int i;
+ 
+ 	for (i = 0, pf = delayed_free.pf; i < ARRAY_SIZE(delayed_free.pf);
+ 	     i++, pf++)
+ 		if (in_list(&class->lock_entry, &pf->zapped))
+ 			return true;
+ 
+ 	return false;
+ }
+ 
+ static bool check_data_structures(void)
+ {
+ 	struct lock_class *class;
+ 	struct lock_chain *chain;
+ 	struct hlist_head *head;
+ 	struct lock_list *e;
+ 	int i;
+ 
+ 	/* Check whether all classes occur in a lock list. */
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (!in_list(&class->lock_entry, &all_lock_classes) &&
+ 		    !in_list(&class->lock_entry, &free_lock_classes) &&
+ 		    !in_any_zapped_class_list(class)) {
+ 			printk(KERN_INFO "class %px/%s is not in any class list\n",
+ 			       class, class->name ? : "(?)");
+ 			return false;
+ 			return false;
+ 		}
+ 	}
+ 
+ 	/* Check whether all classes have valid lock lists. */
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (!class_lock_list_valid(class, &class->locks_before))
+ 			return false;
+ 		if (!class_lock_list_valid(class, &class->locks_after))
+ 			return false;
+ 	}
+ 
+ 	/* Check the chain_key of all lock chains. */
+ 	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
+ 		head = chainhash_table + i;
+ 		hlist_for_each_entry_rcu(chain, head, entry) {
+ 			if (!check_lock_chain_key(chain))
+ 				return false;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Check whether all list entries that are in use occur in a class
+ 	 * lock list.
+ 	 */
+ 	for_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
+ 		e = list_entries + i;
+ 		if (!in_any_class_list(&e->entry)) {
+ 			printk(KERN_INFO "list entry %d is not in any class list; class %s <> %s\n",
+ 			       (unsigned int)(e - list_entries),
+ 			       e->class->name ? : "(?)",
+ 			       e->links_to->name ? : "(?)");
+ 			return false;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Check whether all list entries that are not in use do not occur in
+ 	 * a class lock list.
+ 	 */
+ 	for_each_clear_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
+ 		e = list_entries + i;
+ 		if (in_any_class_list(&e->entry)) {
+ 			printk(KERN_INFO "list entry %d occurs in a class list; class %s <> %s\n",
+ 			       (unsigned int)(e - list_entries),
+ 			       e->class && e->class->name ? e->class->name :
+ 			       "(?)",
+ 			       e->links_to && e->links_to->name ?
+ 			       e->links_to->name : "(?)");
+ 			return false;
+ 		}
+ 	}
+ 
+ 	return true;
+ }
+ 
  /*
 - * Initialize the lock_classes[] array elements, the free_lock_classes list
 - * and also the delayed_free structure.
 + * Initialize the lock_classes[] array elements.
   */
  static void init_data_structures_once(void)
  {
@@@ -4194,7 -4513,96 +4358,100 @@@ static inline int within(const void *ad
  	return addr >= start && addr < start + size;
  }
  
++<<<<<<< HEAD
 +static void __lockdep_free_key_range(void *start, unsigned long size)
++=======
+ static bool inside_selftest(void)
+ {
+ 	return current == lockdep_selftest_task_struct;
+ }
+ 
+ /* The caller must hold the graph lock. */
+ static struct pending_free *get_pending_free(void)
+ {
+ 	return delayed_free.pf + delayed_free.index;
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *cb);
+ 
+ /*
+  * Schedule an RCU callback if no RCU callback is pending. Must be called with
+  * the graph lock held.
+  */
+ static void call_rcu_zapped(struct pending_free *pf)
+ {
+ 	WARN_ON_ONCE(inside_selftest());
+ 
+ 	if (list_empty(&pf->zapped))
+ 		return;
+ 
+ 	if (delayed_free.scheduled)
+ 		return;
+ 
+ 	delayed_free.scheduled = true;
+ 
+ 	WARN_ON_ONCE(delayed_free.pf + delayed_free.index != pf);
+ 	delayed_free.index ^= 1;
+ 
+ 	call_rcu(&delayed_free.rcu_head, free_zapped_rcu);
+ }
+ 
+ /* The caller must hold the graph lock. May be called from RCU context. */
+ static void __free_zapped_classes(struct pending_free *pf)
+ {
+ 	struct lock_class *class;
+ 
+ 	if (check_data_structure_consistency)
+ 		WARN_ON_ONCE(!check_data_structures());
+ 
+ 	list_for_each_entry(class, &pf->zapped, lock_entry)
+ 		reinit_class(class);
+ 
+ 	list_splice_init(&pf->zapped, &free_lock_classes);
+ 
+ #ifdef CONFIG_PROVE_LOCKING
+ 	bitmap_andnot(lock_chains_in_use, lock_chains_in_use,
+ 		      pf->lock_chains_being_freed, ARRAY_SIZE(lock_chains));
+ 	bitmap_clear(pf->lock_chains_being_freed, 0, ARRAY_SIZE(lock_chains));
+ #endif
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *ch)
+ {
+ 	struct pending_free *pf;
+ 	unsigned long flags;
+ 
+ 	if (WARN_ON_ONCE(ch != &delayed_free.rcu_head))
+ 		return;
+ 
+ 	raw_local_irq_save(flags);
+ 	if (!graph_lock())
+ 		goto out_irq;
+ 
+ 	/* closed head */
+ 	pf = delayed_free.pf + (delayed_free.index ^ 1);
+ 	__free_zapped_classes(pf);
+ 	delayed_free.scheduled = false;
+ 
+ 	/*
+ 	 * If there's anything on the open list, close and start a new callback.
+ 	 */
+ 	call_rcu_zapped(delayed_free.pf + delayed_free.index);
+ 
+ 	graph_unlock();
+ out_irq:
+ 	raw_local_irq_restore(flags);
+ }
+ 
+ /*
+  * Remove all lock classes from the class hash table and from the
+  * all_lock_classes list whose key or name is in the address range [start,
+  * start + size). Move these lock classes to the zapped_classes list. Must
+  * be called with the graph lock held.
+  */
+ static void __lockdep_free_key_range(struct pending_free *pf, void *start,
+ 				     unsigned long size)
++>>>>>>> b526b2e39a53 (locking/lockdep: Check data structure consistency)
  {
  	struct lock_class *class;
  	struct hlist_head *head;
* Unmerged path kernel/locking/lockdep.c
