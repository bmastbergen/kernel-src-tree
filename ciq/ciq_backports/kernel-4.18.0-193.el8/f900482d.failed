mm/migrate.c: cleanup expected_page_refs()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] migrate.c: cleanup expected_page_refs() (Don Dutile) [1754737]
Rebuild_FUZZ: 96.30%
commit-author Jan Kara <jack@suse.cz>
commit f900482da560941f978b0d36660e96f48ea78752
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f900482d.failed

Andrea has noted that page migration code propagates page_mapping(page)
through the whole migration stack down to migrate_page() function so it
seems stupid to then use page_mapping(page) in expected_page_refs()
instead of passed down 'mapping' argument.  I agree so let's make
expected_page_refs() more in line with the rest of the migration stack.

Link: http://lkml.kernel.org/r/20190207112314.24872-1-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
	Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f900482da560941f978b0d36660e96f48ea78752)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/migrate.c
diff --cc mm/migrate.c
index d35a94faeed0,ac6f4939bb59..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -372,55 -374,21 +372,70 @@@ unlock
  }
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_BLOCK
 +/* Returns true if all buffers are successfully locked */
 +static bool buffer_migrate_lock_buffers(struct buffer_head *head,
 +							enum migrate_mode mode)
++=======
+ static int expected_page_refs(struct address_space *mapping, struct page *page)
++>>>>>>> f900482da560 (mm/migrate.c: cleanup expected_page_refs())
  {
 -	int expected_count = 1;
 +	struct buffer_head *bh = head;
  
++<<<<<<< HEAD
 +	/* Simple case, sync compaction */
 +	if (mode != MIGRATE_ASYNC) {
 +		do {
 +			get_bh(bh);
 +			lock_buffer(bh);
 +			bh = bh->b_this_page;
++=======
+ 	/*
+ 	 * Device public or private pages have an extra refcount as they are
+ 	 * ZONE_DEVICE pages.
+ 	 */
+ 	expected_count += is_device_private_page(page);
+ 	expected_count += is_device_public_page(page);
+ 	if (mapping)
+ 		expected_count += hpage_nr_pages(page) + page_has_private(page);
++>>>>>>> f900482da560 (mm/migrate.c: cleanup expected_page_refs())
 +
 +		} while (bh != head);
 +
 +		return true;
 +	}
 +
 +	/* async case, we cannot block on lock_buffer so use trylock_buffer */
 +	do {
 +		get_bh(bh);
 +		if (!trylock_buffer(bh)) {
 +			/*
 +			 * We failed to lock the buffer and cannot stall in
 +			 * async migration. Release the taken locks
 +			 */
 +			struct buffer_head *failed_bh = bh;
 +			put_bh(failed_bh);
 +			bh = head;
 +			while (bh != failed_bh) {
 +				unlock_buffer(bh);
 +				put_bh(bh);
 +				bh = bh->b_this_page;
 +			}
 +			return false;
 +		}
  
 -	return expected_count;
 +		bh = bh->b_this_page;
 +	} while (bh != head);
 +	return true;
 +}
 +#else
 +static inline bool buffer_migrate_lock_buffers(struct buffer_head *head,
 +							enum migrate_mode mode)
 +{
 +	return true;
  }
 +#endif /* CONFIG_BLOCK */
  
  /*
   * Replace the page in the mapping.
@@@ -431,21 -399,13 +446,25 @@@
   * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.
   */
  int migrate_page_move_mapping(struct address_space *mapping,
 -		struct page *newpage, struct page *page, enum migrate_mode mode,
 +		struct page *newpage, struct page *page,
 +		struct buffer_head *head, enum migrate_mode mode,
  		int extra_count)
  {
 -	XA_STATE(xas, &mapping->i_pages, page_index(page));
  	struct zone *oldzone, *newzone;
  	int dirty;
++<<<<<<< HEAD
 +	int expected_count = 1 + extra_count;
 +	void **pslot;
 +
 +	/*
 +	 * Device public or private pages have an extra refcount as they are
 +	 * ZONE_DEVICE pages.
 +	 */
 +	expected_count += is_device_private_page(page);
 +	expected_count += is_device_public_page(page);
++=======
+ 	int expected_count = expected_page_refs(mapping, page) + extra_count;
++>>>>>>> f900482da560 (mm/migrate.c: cleanup expected_page_refs())
  
  	if (!mapping) {
  		/* Anonymous page without mapping */
@@@ -782,20 -749,45 +801,28 @@@ int buffer_migrate_page(struct address_
  	if (!page_has_buffers(page))
  		return migrate_page(mapping, newpage, page, mode);
  
++<<<<<<< HEAD
++=======
+ 	/* Check whether page does not have extra refs before we do more work */
+ 	expected_count = expected_page_refs(mapping, page);
+ 	if (page_count(page) != expected_count)
+ 		return -EAGAIN;
+ 
++>>>>>>> f900482da560 (mm/migrate.c: cleanup expected_page_refs())
  	head = page_buffers(page);
 -	if (!buffer_migrate_lock_buffers(head, mode))
 -		return -EAGAIN;
  
 -	if (check_refs) {
 -		bool busy;
 -		bool invalidated = false;
 +	rc = migrate_page_move_mapping(mapping, newpage, page, head, mode, 0);
  
 -recheck_buffers:
 -		busy = false;
 -		spin_lock(&mapping->private_lock);
 -		bh = head;
 -		do {
 -			if (atomic_read(&bh->b_count)) {
 -				busy = true;
 -				break;
 -			}
 -			bh = bh->b_this_page;
 -		} while (bh != head);
 -		spin_unlock(&mapping->private_lock);
 -		if (busy) {
 -			if (invalidated) {
 -				rc = -EAGAIN;
 -				goto unlock_buffers;
 -			}
 -			invalidate_bh_lrus();
 -			invalidated = true;
 -			goto recheck_buffers;
 -		}
 -	}
 -
 -	rc = migrate_page_move_mapping(mapping, newpage, page, mode, 0);
  	if (rc != MIGRATEPAGE_SUCCESS)
 -		goto unlock_buffers;
 +		return rc;
 +
 +	/*
 +	 * In the async case, migrate_page_move_mapping locked the buffers
 +	 * with an IRQ-safe spinlock held. In the sync case, the buffers
 +	 * need to be locked now
 +	 */
 +	if (mode != MIGRATE_ASYNC)
 +		BUG_ON(!buffer_migrate_lock_buffers(head, mode));
  
  	ClearPagePrivate(page);
  	set_page_private(newpage, page_private(page));
* Unmerged path mm/migrate.c
