RDMA/devices: Re-organize device.c locking

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 921eab1143aadf976a42cac4605b4d35159b355d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/921eab11.failed

The locking here started out with a single lock that covered everything
and then has lately veered into crazy town.

The fundamental problem is that several places need to iterate over a
linked list, but also need to drop their locks to avoid deadlock during
client callbacks.

xarray's restartable iteration offers a simple solution to the
problem. Once all the lists are xarrays we can drop locks in the places
that need that and rely on xarray to provide consistency and locking for
the data structure.

The resulting simplification is that each of the three lists has a
dedicated rwsem that must be held when working with the list it
covers. One data structure is no longer covered by multiple locks.

The sleeping semaphore is selected because the read side generally needs
to be held over something sleeping, and using RCU reader locking in those
cases is overkill.

In the process this simplifies the entire registration/unregistration flow
to be the expected list of setups and the reversed list of matching
teardowns, and the registration lock 'refcount' can now be revised to be
released after the ULPs are removed, providing a very sane semantic for
this feature.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 921eab1143aadf976a42cac4605b4d35159b355d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/device.c
index 67ef68b75ec7,3325be4f91a5..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -65,27 -55,83 +64,105 @@@ struct workqueue_struct *ib_comp_unboun
  struct workqueue_struct *ib_wq;
  EXPORT_SYMBOL_GPL(ib_wq);
  
++<<<<<<< HEAD
 +/* The device_list and clients contain devices and clients after their
 + * registration has completed, and the devices and clients are removed
 + * during unregistration. */
 +static LIST_HEAD(device_list);
++=======
+ /*
+  * Each of the three rwsem locks (devices, clients, client_data) protects the
+  * xarray of the same name. Specifically it allows the caller to assert that
+  * the MARK will/will not be changing under the lock, and for devices and
+  * clients, that the value in the xarray is still a valid pointer. Change of
+  * the MARK is linked to the object state, so holding the lock and testing the
+  * MARK also asserts that the contained object is in a certain state.
+  *
+  * This is used to build a two stage register/unregister flow where objects
+  * can continue to be in the xarray even though they are still in progress to
+  * register/unregister.
+  *
+  * The xarray itself provides additional locking, and restartable iteration,
+  * which is also relied on.
+  *
+  * Locks should not be nested, with the exception of client_data, which is
+  * allowed to nest under the read side of the other two locks.
+  *
+  * The devices_rwsem also protects the device name list, any change or
+  * assignment of device name must also hold the write side to guarantee unique
+  * names.
+  */
+ 
+ /*
+  * devices contains devices that have had their names assigned. The
+  * devices may not be registered. Users that care about the registration
+  * status need to call ib_device_try_get() on the device to ensure it is
+  * registered, and keep it registered, for the required duration.
+  *
+  */
+ static DEFINE_XARRAY_FLAGS(devices, XA_FLAGS_ALLOC);
+ static DECLARE_RWSEM(devices_rwsem);
+ #define DEVICE_REGISTERED XA_MARK_1
+ 
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  static LIST_HEAD(client_list);
  #define CLIENT_REGISTERED XA_MARK_1
  static DEFINE_XARRAY_FLAGS(clients, XA_FLAGS_ALLOC);
+ static DECLARE_RWSEM(clients_rwsem);
  
  /*
++<<<<<<< HEAD
 + * device_mutex and lists_rwsem protect access to both device_list and
 + * clients.  device_mutex protects writer access by device and client
 + * registration / de-registration.  lists_rwsem protects reader access to
 + * these lists.  Iterators of these lists must lock it for read, while updates
 + * to the lists must be done with a write lock. A special case is when the
 + * device_mutex is locked. In this case locking the lists for read access is
 + * not necessary as the device_mutex implies it.
 + *
 + * lists_rwsem also protects access to the client data list.
 + */
 +static DEFINE_MUTEX(device_mutex);
 +static DECLARE_RWSEM(lists_rwsem);
++=======
+  * If client_data is registered then the corresponding client must also still
+  * be registered.
+  */
+ #define CLIENT_DATA_REGISTERED XA_MARK_1
+ /*
+  * xarray has this behavior where it won't iterate over NULL values stored in
+  * allocated arrays.  So we need our own iterator to see all values stored in
+  * the array. This does the same thing as xa_for_each except that it also
+  * returns NULL valued entries if the array is allocating. Simplified to only
+  * work on simple xarrays.
+  */
+ static void *xan_find_marked(struct xarray *xa, unsigned long *indexp,
+ 			     xa_mark_t filter)
+ {
+ 	XA_STATE(xas, xa, *indexp);
+ 	void *entry;
+ 
+ 	rcu_read_lock();
+ 	do {
+ 		entry = xas_find_marked(&xas, ULONG_MAX, filter);
+ 		if (xa_is_zero(entry))
+ 			break;
+ 	} while (xas_retry(&xas, entry));
+ 	rcu_read_unlock();
+ 
+ 	if (entry) {
+ 		*indexp = xas.xa_index;
+ 		if (xa_is_zero(entry))
+ 			return NULL;
+ 		return entry;
+ 	}
+ 	return XA_ERROR(-ENOENT);
+ }
+ #define xan_for_each_marked(xa, index, entry, filter)                          \
+ 	for (index = 0, entry = xan_find_marked(xa, &(index), filter);         \
+ 	     !xa_is_err(entry);                                                \
+ 	     (index)++, entry = xan_find_marked(xa, &(index), filter))
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
  static int ib_security_change(struct notifier_block *nb, unsigned long event,
  			      void *lsm_data);
@@@ -154,8 -189,8 +231,13 @@@ struct ib_device *ib_device_get_by_inde
  {
  	struct ib_device *device;
  
++<<<<<<< HEAD
 +	down_read(&lists_rwsem);
 +	device = __ib_device_get_by_index(index);
++=======
+ 	down_read(&devices_rwsem);
+ 	device = xa_load(&devices, index);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  	if (device) {
  		if (!ib_device_try_get(device))
  			device = NULL;
@@@ -220,8 -257,9 +302,9 @@@ static int alloc_name(struct ib_device 
  	int rc;
  	int i;
  
+ 	lockdep_assert_held_exclusive(&devices_rwsem);
  	ida_init(&inuse);
 -	xa_for_each (&devices, index, device) {
 +	list_for_each_entry(device, &device_list, core_list) {
  		char buf[IB_DEVICE_NAME_MAX];
  
  		if (sscanf(dev_name(&device->dev), name, &i) != 1)
@@@ -306,8 -345,12 +389,17 @@@ struct ib_device *_ib_alloc_device(size
  
  	INIT_LIST_HEAD(&device->event_handler_list);
  	spin_lock_init(&device->event_handler_lock);
++<<<<<<< HEAD
 +	rwlock_init(&device->client_data_lock);
 +	INIT_LIST_HEAD(&device->client_data_list);
++=======
+ 	/*
+ 	 * client_data needs to be alloc because we don't want our mark to be
+ 	 * destroyed if the user stores NULL in the client data.
+ 	 */
+ 	xa_init_flags(&device->client_data, XA_FLAGS_ALLOC);
+ 	init_rwsem(&device->client_data_rwsem);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  	INIT_LIST_HEAD(&device->port_list);
  	init_completion(&device->unreg_completion);
  
@@@ -330,28 -373,86 +422,108 @@@ void ib_dealloc_device(struct ib_devic
  }
  EXPORT_SYMBOL(ib_dealloc_device);
  
- static int add_client_context(struct ib_device *device, struct ib_client *client)
+ /*
+  * add_client_context() and remove_client_context() must be safe against
+  * parallel calls on the same device - registration/unregistration of both the
+  * device and client can be occurring in parallel.
+  *
+  * The routines need to be a fence, any caller must not return until the add
+  * or remove is fully completed.
+  */
+ static int add_client_context(struct ib_device *device,
+ 			      struct ib_client *client)
  {
++<<<<<<< HEAD
 +	struct ib_client_data *context;
++=======
+ 	int ret = 0;
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
  	if (!device->kverbs_provider && !client->no_kverbs_req)
- 		return -EOPNOTSUPP;
+ 		return 0;
  
++<<<<<<< HEAD
 +	context = kmalloc(sizeof(*context), GFP_KERNEL);
 +	if (!context)
 +		return -ENOMEM;
 +
 +	context->client = client;
 +	context->data   = NULL;
 +	context->going_down = false;
 +
 +	down_write(&lists_rwsem);
 +	write_lock_irq(&device->client_data_lock);
 +	list_add(&context->list, &device->client_data_list);
 +	write_unlock_irq(&device->client_data_lock);
 +	up_write(&lists_rwsem);
 +
 +	return 0;
++=======
+ 	down_write(&device->client_data_rwsem);
+ 	/*
+ 	 * Another caller to add_client_context got here first and has already
+ 	 * completely initialized context.
+ 	 */
+ 	if (xa_get_mark(&device->client_data, client->client_id,
+ 		    CLIENT_DATA_REGISTERED))
+ 		goto out;
+ 
+ 	ret = xa_err(xa_store(&device->client_data, client->client_id, NULL,
+ 			      GFP_KERNEL));
+ 	if (ret)
+ 		goto out;
+ 	downgrade_write(&device->client_data_rwsem);
+ 	if (client->add)
+ 		client->add(device);
+ 
+ 	/* Readers shall not see a client until add has been completed */
+ 	xa_set_mark(&device->client_data, client->client_id,
+ 		    CLIENT_DATA_REGISTERED);
+ 	up_read(&device->client_data_rwsem);
+ 	return 0;
+ 
+ out:
+ 	up_write(&device->client_data_rwsem);
+ 	return ret;
+ }
+ 
+ static void remove_client_context(struct ib_device *device,
+ 				  unsigned int client_id)
+ {
+ 	struct ib_client *client;
+ 	void *client_data;
+ 
+ 	down_write(&device->client_data_rwsem);
+ 	if (!xa_get_mark(&device->client_data, client_id,
+ 			 CLIENT_DATA_REGISTERED)) {
+ 		up_write(&device->client_data_rwsem);
+ 		return;
+ 	}
+ 	client_data = xa_load(&device->client_data, client_id);
+ 	xa_clear_mark(&device->client_data, client_id, CLIENT_DATA_REGISTERED);
+ 	client = xa_load(&clients, client_id);
+ 	downgrade_write(&device->client_data_rwsem);
+ 
+ 	/*
+ 	 * Notice we cannot be holding any exclusive locks when calling the
+ 	 * remove callback as the remove callback can recurse back into any
+ 	 * public functions in this module and thus try for any locks those
+ 	 * functions take.
+ 	 *
+ 	 * For this reason clients and drivers should not call the
+ 	 * unregistration functions will holdling any locks.
+ 	 *
+ 	 * It tempting to drop the client_data_rwsem too, but this is required
+ 	 * to ensure that unregister_client does not return until all clients
+ 	 * are completely unregistered, which is required to avoid module
+ 	 * unloading races.
+ 	 */
+ 	if (client->remove)
+ 		client->remove(device, client_data);
+ 
+ 	xa_erase(&device->client_data, client_id);
+ 	up_read(&device->client_data_rwsem);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  }
  
  static int verify_immutable(const struct ib_device *dev, u8 port)
@@@ -428,9 -529,10 +600,14 @@@ static int setup_port_pkey_list(struct 
  static void ib_policy_change_task(struct work_struct *work)
  {
  	struct ib_device *dev;
 -	unsigned long index;
  
++<<<<<<< HEAD
 +	down_read(&lists_rwsem);
 +	list_for_each_entry(dev, &device_list, core_list) {
++=======
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  		int i;
  
  		for (i = rdma_start_port(dev); i <= rdma_end_port(dev); i++) {
@@@ -461,28 -563,53 +638,68 @@@ static int ib_security_change(struct no
  	return NOTIFY_OK;
  }
  
 -/*
 - * Assign the unique string device name and the unique device index.
 +/**
 + *	__dev_new_index	-	allocate an device index
 + *
 + *	Returns a suitable unique value for a new device interface
 + *	number.  It assumes that there are less than 2^32-1 ib devices
 + *	will be present in the system.
   */
 -static int assign_name(struct ib_device *device, const char *name)
 +static u32 __dev_new_index(void)
  {
 -	static u32 last_id;
 -	int ret;
 +	/*
 +	 * The device index to allow stable naming.
 +	 * Similar to struct net -> ifindex.
 +	 */
 +	static u32 index;
  
++<<<<<<< HEAD
 +	for (;;) {
 +		if (!(++index))
 +			index = 1;
++=======
+ 	down_write(&devices_rwsem);
+ 	/* Assign a unique name to the device */
+ 	if (strchr(name, '%'))
+ 		ret = alloc_name(device, name);
+ 	else
+ 		ret = dev_set_name(&device->dev, name);
+ 	if (ret)
+ 		goto out;
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
 -	if (__ib_device_get_by_name(dev_name(&device->dev))) {
 -		ret = -ENFILE;
 -		goto out;
 +		if (!__ib_device_get_by_index(index))
 +			return index;
  	}
++<<<<<<< HEAD
++=======
+ 	strlcpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);
+ 
+ 	/* Cyclically allocate a user visible ID for the device */
+ 	device->index = last_id;
+ 	ret = xa_alloc(&devices, &device->index, INT_MAX, device, GFP_KERNEL);
+ 	if (ret == -ENOSPC) {
+ 		device->index = 0;
+ 		ret = xa_alloc(&devices, &device->index, INT_MAX, device,
+ 			       GFP_KERNEL);
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	last_id = device->index + 1;
+ 
+ 	ret = 0;
+ 
+ out:
+ 	up_write(&devices_rwsem);
+ 	return ret;
+ }
+ 
+ static void release_name(struct ib_device *device)
+ {
+ 	down_write(&devices_rwsem);
+ 	xa_erase(&devices, device->index);
+ 	up_write(&devices_rwsem);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  }
  
  static void setup_dma_device(struct ib_device *device)
@@@ -565,27 -747,10 +837,27 @@@ static int enable_device(struct ib_devi
  int ib_register_device(struct ib_device *device, const char *name)
  {
  	int ret;
- 	struct ib_client *client;
- 	unsigned long index;
- 
- 	setup_dma_device(device);
- 
- 	mutex_lock(&device_mutex);
  
++<<<<<<< HEAD
 +	if (strchr(name, '%')) {
 +		ret = alloc_name(device, name);
 +		if (ret)
 +			goto out;
 +	} else {
 +		ret = dev_set_name(&device->dev, name);
 +		if (ret)
 +			goto out;
 +	}
 +	if (__ib_device_get_by_name(dev_name(&device->dev))) {
 +		ret = -ENFILE;
 +		goto out;
 +	}
 +	strlcpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);
++=======
+ 	ret = assign_name(device, name);
+ 	if (ret)
+ 		return ret;
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
  	ret = setup_device(device);
  	if (ret)
@@@ -614,24 -772,19 +886,31 @@@
  		goto cg_cleanup;
  	}
  
- 	refcount_set(&device->refcount, 1);
+ 	ret = enable_device(device);
+ 	if (ret)
+ 		goto sysfs_cleanup;
  
++<<<<<<< HEAD
 +	xa_for_each_marked (&clients, index, client, CLIENT_REGISTERED)
 +		if (!add_client_context(device, client) && client->add)
 +			client->add(device);
 +
 +	down_write(&lists_rwsem);
 +	list_add_tail(&device->core_list, &device_list);
 +	up_write(&lists_rwsem);
 +	mutex_unlock(&device_mutex);
++=======
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  	return 0;
  
+ sysfs_cleanup:
+ 	ib_device_unregister_sysfs(device);
  cg_cleanup:
  	ib_device_unregister_rdmacg(device);
 +dev_cleanup:
  	ib_cache_cleanup_one(device);
  out:
- 	mutex_unlock(&device_mutex);
+ 	release_name(device);
  	return ret;
  }
  EXPORT_SYMBOL(ib_register_device);
@@@ -644,48 -797,11 +923,56 @@@
   */
  void ib_unregister_device(struct ib_device *device)
  {
++<<<<<<< HEAD
 +	struct ib_client_data *context, *tmp;
 +	unsigned long flags;
 +
 +	/*
 +	 * Wait for all netlink command callers to finish working on the
 +	 * device.
 +	 */
 +	ib_device_put(device);
 +	wait_for_completion(&device->unreg_completion);
 +
 +	mutex_lock(&device_mutex);
 +
 +	down_write(&lists_rwsem);
 +	list_del(&device->core_list);
 +	write_lock_irq(&device->client_data_lock);
 +	list_for_each_entry(context, &device->client_data_list, list)
 +		context->going_down = true;
 +	write_unlock_irq(&device->client_data_lock);
 +	downgrade_write(&lists_rwsem);
 +
 +	list_for_each_entry(context, &device->client_data_list, list) {
 +		if (context->client->remove)
 +			context->client->remove(device, context->data);
 +	}
 +	up_read(&lists_rwsem);
 +
 +	ib_device_unregister_sysfs(device);
 +	ib_device_unregister_rdmacg(device);
 +
 +	mutex_unlock(&device_mutex);
 +
 +	ib_cache_cleanup_one(device);
 +
 +	down_write(&lists_rwsem);
 +	write_lock_irqsave(&device->client_data_lock, flags);
 +	list_for_each_entry_safe(context, tmp, &device->client_data_list,
 +				 list) {
 +		list_del(&context->list);
 +		kfree(context);
 +	}
 +	write_unlock_irqrestore(&device->client_data_lock, flags);
 +	up_write(&lists_rwsem);
++=======
+ 	disable_device(device);
+ 	ib_device_unregister_sysfs(device);
+ 	ib_device_unregister_rdmacg(device);
+ 	ib_cache_cleanup_one(device);
+ 	release_name(device);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  }
  EXPORT_SYMBOL(ib_unregister_device);
  
@@@ -731,25 -852,23 +1023,36 @@@ out
  int ib_register_client(struct ib_client *client)
  {
  	struct ib_device *device;
 -	unsigned long index;
  	int ret;
  
- 	mutex_lock(&device_mutex);
  	ret = assign_client_id(client);
- 	if (ret) {
- 		mutex_unlock(&device_mutex);
+ 	if (ret)
  		return ret;
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, device, DEVICE_REGISTERED) {
+ 		ret = add_client_context(device, client);
+ 		if (ret) {
+ 			up_read(&devices_rwsem);
+ 			ib_unregister_client(client);
+ 			return ret;
+ 		}
  	}
++<<<<<<< HEAD
 +
 +	list_for_each_entry(device, &device_list, core_list)
 +		if (!add_client_context(device, client) && client->add)
 +			client->add(device);
 +
 +	down_write(&lists_rwsem);
 +	xa_set_mark(&clients, client->client_id, CLIENT_REGISTERED);
 +	up_write(&lists_rwsem);
 +
 +	mutex_unlock(&device_mutex);
 +
++=======
+ 	up_read(&devices_rwsem);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  	return 0;
  }
  EXPORT_SYMBOL(ib_register_client);
@@@ -764,53 -886,25 +1070,62 @@@
   */
  void ib_unregister_client(struct ib_client *client)
  {
 +	struct ib_client_data *context;
  	struct ib_device *device;
 -	unsigned long index;
  
- 	mutex_lock(&device_mutex);
- 
- 	down_write(&lists_rwsem);
+ 	down_write(&clients_rwsem);
  	xa_clear_mark(&clients, client->client_id, CLIENT_REGISTERED);
- 	up_write(&lists_rwsem);
+ 	up_write(&clients_rwsem);
+ 	/*
+ 	 * Every device still known must be serialized to make sure we are
+ 	 * done with the client callbacks before we return.
+ 	 */
+ 	down_read(&devices_rwsem);
+ 	xa_for_each (&devices, index, device)
+ 		remove_client_context(device, client->client_id);
+ 	up_read(&devices_rwsem);
  
++<<<<<<< HEAD
 +	list_for_each_entry(device, &device_list, core_list) {
 +		struct ib_client_data *found_context = NULL;
 +
 +		down_write(&lists_rwsem);
 +		write_lock_irq(&device->client_data_lock);
 +		list_for_each_entry(context, &device->client_data_list, list)
 +			if (context->client == client) {
 +				context->going_down = true;
 +				found_context = context;
 +				break;
 +			}
 +		write_unlock_irq(&device->client_data_lock);
 +		up_write(&lists_rwsem);
 +
 +		if (client->remove)
 +			client->remove(device, found_context ?
 +					       found_context->data : NULL);
 +
 +		if (!found_context) {
 +			dev_warn(&device->dev,
 +				 "No client context found for %s\n",
 +				 client->name);
 +			continue;
 +		}
 +
 +		down_write(&lists_rwsem);
 +		write_lock_irq(&device->client_data_lock);
 +		list_del(&found_context->list);
 +		write_unlock_irq(&device->client_data_lock);
 +		up_write(&lists_rwsem);
 +		kfree(found_context);
 +	}
 +
 +	down_write(&lists_rwsem);
++=======
+ 	down_write(&clients_rwsem);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  	list_del(&client->list);
  	xa_erase(&clients, client->client_id);
- 	up_write(&lists_rwsem);
- 	mutex_unlock(&device_mutex);
+ 	up_write(&clients_rwsem);
  }
  EXPORT_SYMBOL(ib_unregister_client);
  
@@@ -1024,11 -1087,12 +1339,16 @@@ void ib_enum_all_roce_netdevs(roce_netd
  			      void *cookie)
  {
  	struct ib_device *dev;
 -	unsigned long index;
  
++<<<<<<< HEAD
 +	down_read(&lists_rwsem);
 +	list_for_each_entry(dev, &device_list, core_list)
++=======
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED)
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  		ib_enum_roce_netdev(dev, filter, filter_cookie, cb, cookie);
- 	up_read(&lists_rwsem);
+ 	up_read(&devices_rwsem);
  }
  
  /**
@@@ -1044,8 -1109,8 +1364,13 @@@ int ib_enum_all_devs(nldev_callback nld
  	unsigned int idx = 0;
  	int ret = 0;
  
++<<<<<<< HEAD
 +	down_read(&lists_rwsem);
 +	list_for_each_entry(dev, &device_list, core_list) {
++=======
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  		ret = nldev_cb(dev, skb, cb, idx);
  		if (ret)
  			break;
@@@ -1223,24 -1289,24 +1548,34 @@@ struct net_device *ib_get_net_dev_by_pa
  	if (!rdma_protocol_ib(dev, port))
  		return NULL;
  
++<<<<<<< HEAD
 +	down_read(&lists_rwsem);
 +
 +	list_for_each_entry(context, &dev->client_data_list, list) {
 +		struct ib_client *client = context->client;
++=======
+ 	/*
+ 	 * Holding the read side guarantees that the client will not become
+ 	 * unregistered while we are calling get_net_dev_by_params()
+ 	 */
+ 	down_read(&dev->client_data_rwsem);
+ 	xan_for_each_marked (&dev->client_data, index, client_data,
+ 			     CLIENT_DATA_REGISTERED) {
+ 		struct ib_client *client = xa_load(&clients, index);
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
 -		if (!client || !client->get_net_dev_by_params)
 +		if (context->going_down)
  			continue;
  
 -		net_dev = client->get_net_dev_by_params(dev, port, pkey, gid,
 -							addr, client_data);
 -		if (net_dev)
 -			break;
 +		if (client->get_net_dev_by_params) {
 +			net_dev = client->get_net_dev_by_params(dev, port, pkey,
 +								gid, addr,
 +								context->data);
 +			if (net_dev)
 +				break;
 +		}
  	}
- 
- 	up_read(&lists_rwsem);
+ 	up_read(&dev->client_data_rwsem);
  
  	return net_dev;
  }
diff --cc include/rdma/ib_verbs.h
index 2ff74f11eec0,135fab2c016c..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -2536,12 -2542,8 +2536,17 @@@ struct ib_device 
  	struct list_head              event_handler_list;
  	spinlock_t                    event_handler_lock;
  
++<<<<<<< HEAD
 +	rwlock_t			client_data_lock;
 +	struct list_head              core_list;
 +	/* Access to the client_data_list is protected by the client_data_lock
 +	 * rwlock and the lists_rwsem read-write semaphore
 +	 */
 +	struct list_head              client_data_list;
++=======
+ 	struct rw_semaphore	      client_data_rwsem;
+ 	struct xarray                 client_data;
++>>>>>>> 921eab1143aa (RDMA/devices: Re-organize device.c locking)
  
  	struct ib_cache               cache;
  	/**
* Unmerged path drivers/infiniband/core/device.c
* Unmerged path include/rdma/ib_verbs.h
