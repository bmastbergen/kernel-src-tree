mm/memory_hotplug: don't free usage map when removing a re-added early section

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author David Hildenbrand <david@redhat.com>
commit 8068df3b60373c390198f660574ea14c8098de57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/8068df3b.failed

When we remove an early section, we don't free the usage map, as the
usage maps of other sections are placed into the same page.  Once the
section is removed, it is no longer an early section (especially, the
memmap is freed).  When we re-add that section, the usage map is reused,
however, it is no longer an early section.  When removing that section
again, we try to kfree() a usage map that was allocated during early
boot - bad.

Let's check against PageReserved() to see if we are dealing with an
usage map that was allocated during boot.  We could also check against
!(PageSlab(usage_page) || PageCompound(usage_page)), but PageReserved() is
cleaner.

Can be triggered using memtrace under ppc64/powernv:

  $ mount -t debugfs none /sys/kernel/debug/
  $ echo 0x20000000 > /sys/kernel/debug/powerpc/memtrace/enable
  $ echo 0x20000000 > /sys/kernel/debug/powerpc/memtrace/enable
   ------------[ cut here ]------------
   kernel BUG at mm/slub.c:3969!
   Oops: Exception in kernel mode, sig: 5 [#1]
   LE PAGE_SIZE=3D64K MMU=3DHash SMP NR_CPUS=3D2048 NUMA PowerNV
   Modules linked in:
   CPU: 0 PID: 154 Comm: sh Not tainted 5.5.0-rc2-next-20191216-00005-g0be1dba7b7c0 #61
   NIP kfree+0x338/0x3b0
   LR section_deactivate+0x138/0x200
   Call Trace:
     section_deactivate+0x138/0x200
     __remove_pages+0x114/0x150
     arch_remove_memory+0x3c/0x160
     try_remove_memory+0x114/0x1a0
     __remove_memory+0x20/0x40
     memtrace_enable_set+0x254/0x850
     simple_attr_write+0x138/0x160
     full_proxy_write+0x8c/0x110
     __vfs_write+0x38/0x70
     vfs_write+0x11c/0x2a0
     ksys_write+0x84/0x140
     system_call+0x5c/0x68
   ---[ end trace 4b053cbd84e0db62 ]---

The first invocation will offline+remove memory blocks.  The second
invocation will first add+online them again, in order to offline+remove
them again (usually we are lucky and the exact same memory blocks will
get "reallocated").

Tested on powernv with boot memory: The usage map will not get freed.
Tested on x86-64 with DIMMs: The usage map will get freed.

Using Dynamic Memory under a Power DLAPR can trigger it easily.

Triggering removal (I assume after previously removed+re-added) of
memory from the HMC GUI can crash the kernel with the same call trace
and is fixed by this patch.

Link: http://lkml.kernel.org/r/20191217104637.5509-1-david@redhat.com
Fixes: 326e1b8f83a4 ("mm/sparsemem: introduce a SECTION_IS_EARLY flag")
	Signed-off-by: David Hildenbrand <david@redhat.com>
	Tested-by: Pingfan Liu <piliu@redhat.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Oscar Salvador <osalvador@suse.de>
	Cc: Michal Hocko <mhocko@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8068df3b60373c390198f660574ea14c8098de57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/sparse.c
diff --cc mm/sparse.c
index 02b5de3161a3,3822ecbd8a1f..000000000000
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@@ -653,13 -734,129 +653,132 @@@ static void free_map_bootmem(struct pag
  			put_page_bootmem(page);
  	}
  }
 +#endif /* CONFIG_MEMORY_HOTREMOVE */
  #endif /* CONFIG_SPARSEMEM_VMEMMAP */
  
++<<<<<<< HEAD
++=======
+ static void section_deactivate(unsigned long pfn, unsigned long nr_pages,
+ 		struct vmem_altmap *altmap)
+ {
+ 	DECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };
+ 	DECLARE_BITMAP(tmp, SUBSECTIONS_PER_SECTION) = { 0 };
+ 	struct mem_section *ms = __pfn_to_section(pfn);
+ 	bool section_is_early = early_section(ms);
+ 	struct page *memmap = NULL;
+ 	unsigned long *subsection_map = ms->usage
+ 		? &ms->usage->subsection_map[0] : NULL;
+ 
+ 	subsection_mask_set(map, pfn, nr_pages);
+ 	if (subsection_map)
+ 		bitmap_and(tmp, map, subsection_map, SUBSECTIONS_PER_SECTION);
+ 
+ 	if (WARN(!subsection_map || !bitmap_equal(tmp, map, SUBSECTIONS_PER_SECTION),
+ 				"section already deactivated (%#lx + %ld)\n",
+ 				pfn, nr_pages))
+ 		return;
+ 
+ 	/*
+ 	 * There are 3 cases to handle across two configurations
+ 	 * (SPARSEMEM_VMEMMAP={y,n}):
+ 	 *
+ 	 * 1/ deactivation of a partial hot-added section (only possible
+ 	 * in the SPARSEMEM_VMEMMAP=y case).
+ 	 *    a/ section was present at memory init
+ 	 *    b/ section was hot-added post memory init
+ 	 * 2/ deactivation of a complete hot-added section
+ 	 * 3/ deactivation of a complete section from memory init
+ 	 *
+ 	 * For 1/, when subsection_map does not empty we will not be
+ 	 * freeing the usage map, but still need to free the vmemmap
+ 	 * range.
+ 	 *
+ 	 * For 2/ and 3/ the SPARSEMEM_VMEMMAP={y,n} cases are unified
+ 	 */
+ 	bitmap_xor(subsection_map, map, subsection_map, SUBSECTIONS_PER_SECTION);
+ 	if (bitmap_empty(subsection_map, SUBSECTIONS_PER_SECTION)) {
+ 		unsigned long section_nr = pfn_to_section_nr(pfn);
+ 
+ 		/*
+ 		 * When removing an early section, the usage map is kept (as the
+ 		 * usage maps of other sections fall into the same page). It
+ 		 * will be re-used when re-adding the section - which is then no
+ 		 * longer an early section. If the usage map is PageReserved, it
+ 		 * was allocated during boot.
+ 		 */
+ 		if (!PageReserved(virt_to_page(ms->usage))) {
+ 			kfree(ms->usage);
+ 			ms->usage = NULL;
+ 		}
+ 		memmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);
+ 		ms->section_mem_map = sparse_encode_mem_map(NULL, section_nr);
+ 	}
+ 
+ 	if (section_is_early && memmap)
+ 		free_map_bootmem(memmap);
+ 	else
+ 		depopulate_section_memmap(pfn, nr_pages, altmap);
+ }
+ 
+ static struct page * __meminit section_activate(int nid, unsigned long pfn,
+ 		unsigned long nr_pages, struct vmem_altmap *altmap)
+ {
+ 	DECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };
+ 	struct mem_section *ms = __pfn_to_section(pfn);
+ 	struct mem_section_usage *usage = NULL;
+ 	unsigned long *subsection_map;
+ 	struct page *memmap;
+ 	int rc = 0;
+ 
+ 	subsection_mask_set(map, pfn, nr_pages);
+ 
+ 	if (!ms->usage) {
+ 		usage = kzalloc(mem_section_usage_size(), GFP_KERNEL);
+ 		if (!usage)
+ 			return ERR_PTR(-ENOMEM);
+ 		ms->usage = usage;
+ 	}
+ 	subsection_map = &ms->usage->subsection_map[0];
+ 
+ 	if (bitmap_empty(map, SUBSECTIONS_PER_SECTION))
+ 		rc = -EINVAL;
+ 	else if (bitmap_intersects(map, subsection_map, SUBSECTIONS_PER_SECTION))
+ 		rc = -EEXIST;
+ 	else
+ 		bitmap_or(subsection_map, map, subsection_map,
+ 				SUBSECTIONS_PER_SECTION);
+ 
+ 	if (rc) {
+ 		if (usage)
+ 			ms->usage = NULL;
+ 		kfree(usage);
+ 		return ERR_PTR(rc);
+ 	}
+ 
+ 	/*
+ 	 * The early init code does not consider partially populated
+ 	 * initial sections, it simply assumes that memory will never be
+ 	 * referenced.  If we hot-add memory into such a section then we
+ 	 * do not need to populate the memmap and can simply reuse what
+ 	 * is already there.
+ 	 */
+ 	if (nr_pages < PAGES_PER_SECTION && early_section(ms))
+ 		return pfn_to_page(pfn);
+ 
+ 	memmap = populate_section_memmap(pfn, nr_pages, nid, altmap);
+ 	if (!memmap) {
+ 		section_deactivate(pfn, nr_pages, altmap);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	return memmap;
+ }
+ 
++>>>>>>> 8068df3b6037 (mm/memory_hotplug: don't free usage map when removing a re-added early section)
  /**
 - * sparse_add_section - add a memory section, or populate an existing one
 + * sparse_add_one_section - add a memory section
   * @nid: The node to add section on
   * @start_pfn: start pfn of the memory range
 - * @nr_pages: number of pfns to add in the section
   * @altmap: device page map
   *
   * This is only intended for hotplug.
* Unmerged path mm/sparse.c
