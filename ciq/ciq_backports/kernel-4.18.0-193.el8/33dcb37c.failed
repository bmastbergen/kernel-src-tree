dma-mapping: fix page attributes for dma_mmap_*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 33dcb37cef741294b481f4d889a465b8091f11bf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/33dcb37c.failed

All the way back to introducing dma_common_mmap we've defaulted to mark
the pages as uncached.  But this is wrong for DMA coherent devices.
Later on DMA_ATTR_WRITE_COMBINE also got incorrect treatment as that
flag is only treated special on the alloc side for non-coherent devices.

Introduce a new dma_pgprot helper that deals with the check for coherent
devices so that only the remapping cases ever reach arch_dma_mmap_pgprot
and we thus ensure no aliasing of page attributes happens, which makes
the powerpc version of arch_dma_mmap_pgprot obsolete and simplifies the
remaining ones.

Note that this means arch_dma_mmap_pgprot is a bit misnamed now, but
we'll phase it out soon.

Fixes: 64ccc9c033c6 ("common: dma-mapping: add support for generic dma_mmap_* calls")
	Reported-by: Shawn Anastasio <shawn@anastas.io>
	Reported-by: Gavin Li <git@thegavinli.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64
(cherry picked from commit 33dcb37cef741294b481f4d889a465b8091f11bf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/mm/dma-mapping.c
#	arch/powerpc/Kconfig
#	arch/powerpc/kernel/Makefile
diff --cc arch/arm/mm/dma-mapping.c
index 121c6c3ba9e0,d42557ee69c2..000000000000
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@@ -2400,4 -2377,48 +2400,49 @@@ void arch_teardown_dma_ops(struct devic
  		return;
  
  	arm_teardown_iommu_dma_ops(dev);
 -	/* Let arch_setup_dma_ops() start again from scratch upon re-probe */
 -	set_dma_ops(dev, NULL);
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_SWIOTLB
+ void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_cpu_to_dev(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ void arch_sync_dma_for_cpu(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_dev_to_cpu(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
+ 		dma_addr_t dma_addr)
+ {
+ 	return dma_to_pfn(dev, dma_addr);
+ }
+ 
+ pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
+ 		unsigned long attrs)
+ {
+ 	return __get_dma_pgprot(attrs, prot);
+ }
+ 
+ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t gfp, unsigned long attrs)
+ {
+ 	return __dma_alloc(dev, size, dma_handle, gfp,
+ 			   __get_dma_pgprot(attrs, PAGE_KERNEL), false,
+ 			   attrs, __builtin_return_address(0));
+ }
+ 
+ void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	__arm_dma_free(dev, size, cpu_addr, dma_handle, attrs, false);
+ }
+ #endif /* CONFIG_SWIOTLB */
++>>>>>>> 33dcb37cef74 (dma-mapping: fix page attributes for dma_mmap_*)
diff --cc arch/powerpc/Kconfig
index 40466f86cda5,d8dcd8820369..000000000000
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@@ -121,8 -118,9 +121,11 @@@ config PP
  	#
  	# Please keep this list sorted alphabetically.
  	#
 -	select ARCH_32BIT_OFF_T if PPC32
 -	select ARCH_HAS_DEBUG_VIRTUAL
  	select ARCH_HAS_DEVMEM_IS_ALLOWED
++<<<<<<< HEAD
 +	select ARCH_HAS_DMA_SET_COHERENT_MASK
++=======
++>>>>>>> 33dcb37cef74 (dma-mapping: fix page attributes for dma_mmap_*)
  	select ARCH_HAS_ELF_RANDOMIZE
  	select ARCH_HAS_FORTIFY_SOURCE
  	select ARCH_HAS_GCOV_PROFILE_ALL
diff --cc arch/powerpc/kernel/Makefile
index 3b66f2c19c84,56dfa7a2a6f2..000000000000
--- a/arch/powerpc/kernel/Makefile
+++ b/arch/powerpc/kernel/Makefile
@@@ -33,7 -48,7 +33,11 @@@ obj-y				:= cputable.o ptrace.o syscall
  				   process.o systbl.o idle.o \
  				   signal.o sysfs.o cacheinfo.o time.o \
  				   prom.o traps.o setup-common.o \
++<<<<<<< HEAD
 +				   udbg.o misc.o io.o dma.o misc_$(BITS).o \
++=======
+ 				   udbg.o misc.o io.o misc_$(BITS).o \
++>>>>>>> 33dcb37cef74 (dma-mapping: fix page attributes for dma_mmap_*)
  				   of_platform.o prom_parse.o
  obj-$(CONFIG_PPC64)		+= setup_64.o sys_ppc32.o \
  				   signal_64.o ptrace32.o \
* Unmerged path arch/arm/mm/dma-mapping.c
diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c
index d612522a8464..fa63a9f40045 100644
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@ -37,9 +37,7 @@
 pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
 		unsigned long attrs)
 {
-	if (!dev_is_dma_coherent(dev) || (attrs & DMA_ATTR_WRITE_COMBINE))
-		return pgprot_writecombine(prot);
-	return prot;
+	return pgprot_writecombine(prot);
 }
 
 void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
* Unmerged path arch/powerpc/Kconfig
* Unmerged path arch/powerpc/kernel/Makefile
diff --git a/drivers/iommu/dma-iommu.c b/drivers/iommu/dma-iommu.c
index aec74b314ab1..55d008b1a456 100644
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@ -583,7 +583,7 @@ static void *iommu_dma_alloc_remap(struct device *dev, size_t size,
 	struct iova_domain *iovad = &cookie->iovad;
 	bool coherent = dev_is_dma_coherent(dev);
 	int ioprot = dma_info_to_prot(DMA_BIDIRECTIONAL, coherent, attrs);
-	pgprot_t prot = arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs);
+	pgprot_t prot = dma_pgprot(dev, PAGE_KERNEL, attrs);
 	unsigned int count, min_size, alloc_sizes = domain->pgsize_bitmap;
 	struct page **pages;
 	struct sg_table sgt;
@@ -987,7 +987,7 @@ static void *iommu_dma_alloc_pages(struct device *dev, size_t size,
 		return NULL;
 
 	if (IS_ENABLED(CONFIG_DMA_REMAP) && (!coherent || PageHighMem(page))) {
-		pgprot_t prot = arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs);
+		pgprot_t prot = dma_pgprot(dev, PAGE_KERNEL, attrs);
 
 		cpu_addr = dma_common_contiguous_remap(page, alloc_size,
 				VM_USERMAP, prot, __builtin_return_address(0));
@@ -1047,7 +1047,7 @@ static int iommu_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 	unsigned long pfn, off = vma->vm_pgoff;
 	int ret;
 
-	vma->vm_page_prot = arch_dma_mmap_pgprot(dev, vma->vm_page_prot, attrs);
+	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
 
 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
 		return ret;
diff --git a/include/linux/dma-noncoherent.h b/include/linux/dma-noncoherent.h
index 53ee36ecdf37..029424e95b2b 100644
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@ -42,13 +42,18 @@ void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
 		dma_addr_t dma_addr);
-
-#ifdef CONFIG_ARCH_HAS_DMA_MMAP_PGPROT
 pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
 		unsigned long attrs);
+
+#ifdef CONFIG_MMU
+pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs);
 #else
-# define arch_dma_mmap_pgprot(dev, prot, attrs)	pgprot_noncached(prot)
-#endif
+static inline pgprot_t dma_pgprot(struct device *dev, pgprot_t prot,
+		unsigned long attrs)
+{
+	return prot;	/* no protection bits supported without page tables */
+}
+#endif /* CONFIG_MMU */
 
 #ifdef CONFIG_DMA_NONCOHERENT_CACHE_SYNC
 void arch_dma_cache_sync(struct device *dev, void *vaddr, size_t size,
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index b5dcfbca5c27..2bd82e6307e7 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -150,6 +150,23 @@ int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
 }
 EXPORT_SYMBOL(dma_get_sgtable_attrs);
 
+#ifdef CONFIG_MMU
+/*
+ * Return the page attributes used for mapping dma_alloc_* memory, either in
+ * kernel space if remapping is needed, or to userspace through dma_mmap_*.
+ */
+pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs)
+{
+	if (dev_is_dma_coherent(dev) ||
+	    (IS_ENABLED(CONFIG_DMA_NONCOHERENT_CACHE_SYNC) &&
+             (attrs & DMA_ATTR_NON_CONSISTENT)))
+		return prot;
+	if (IS_ENABLED(CONFIG_ARCH_HAS_DMA_MMAP_PGPROT))
+		return arch_dma_mmap_pgprot(dev, prot, attrs);
+	return pgprot_noncached(prot);
+}
+#endif /* CONFIG_MMU */
+
 /*
  * Create userspace mapping for the DMA-coherent memory.
  */
@@ -164,7 +181,7 @@ int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 	unsigned long pfn;
 	int ret = -ENXIO;
 
-	vma->vm_page_prot = arch_dma_mmap_pgprot(dev, vma->vm_page_prot, attrs);
+	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
 
 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
 		return ret;
diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index a594aec07882..ffe78f0b2fe4 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -218,7 +218,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 
 	/* create a coherent mapping */
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
-			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
+			dma_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
 	if (!ret) {
 		__dma_direct_free_pages(dev, size, page);
