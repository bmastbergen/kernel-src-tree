RDMA/core: Introduce a helper function to change net namespace of rdma device

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Parav Pandit <parav@mellanox.com>
commit decbc7a6b0073f55b200d80a3ecf5a5e205edd06
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/decbc7a6.failed

Introduce a helper function that changes rdma device's net namespace which
performs mini disable/enable sequence to have device visible only in
assigned net namespace.

Device unregistration, device rename and device change net namespace
may be invoked concurrently.

(a) device unregistration needs to wait if a device change (rename or net
    namespace change) operation is in progress.
(b) device net namespace change should not proceed if the unregistration
    has started.
(c) while one cpu is changing device net namespace, other cpu should not
    be able to rename or change net namespace.

To address above concurrency,
(a) Use unreg_mutex to synchronize between ib_unregister_device() and net
    namespace change operation
(b) In cases where unregister_device() has started unregistration before
    change_netns got chance to acquire unreg_mutex, validate the refcount
    - if it dropped to zero, abort the net namespace change operation.

Finally use the helper function to change net namespace of ib device to
move the device back to init_net when such net is deleted.

	Signed-off-by: Parav Pandit <parav@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit decbc7a6b0073f55b200d80a3ecf5a5e205edd06)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
diff --cc drivers/infiniband/core/device.c
index ec96a7b1c811,7fe4f8b880ee..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -96,6 -201,15 +96,18 @@@ static struct notifier_block ibdev_lsm_
  	.notifier_call = ib_security_change,
  };
  
++<<<<<<< HEAD
++=======
+ static int rdma_dev_change_netns(struct ib_device *device, struct net *cur_net,
+ 				 struct net *net);
+ 
+ /* Pointer to the RCU head at the start of the ib_port_data array */
+ struct ib_port_data_rcu {
+ 	struct rcu_head rcu_head;
+ 	struct ib_port_data pdata[];
+ };
+ 
++>>>>>>> decbc7a6b007 (RDMA/core: Introduce a helper function to change net namespace of rdma device)
  static int ib_device_check_mandatory(struct ib_device *device)
  {
  #define IB_MANDATORY_FUNC(x) { offsetof(struct ib_device_ops, x), #x }
@@@ -469,28 -755,315 +481,311 @@@ static int ib_security_change(struct no
  	return NOTIFY_OK;
  }
  
++<<<<<<< HEAD
 +/**
 + *	__dev_new_index	-	allocate an device index
 + *
 + *	Returns a suitable unique value for a new device interface
 + *	number.  It assumes that there are less than 2^32-1 ib devices
 + *	will be present in the system.
++=======
+ static void compatdev_release(struct device *dev)
+ {
+ 	struct ib_core_device *cdev =
+ 		container_of(dev, struct ib_core_device, dev);
+ 
+ 	kfree(cdev);
+ }
+ 
+ static int add_one_compat_dev(struct ib_device *device,
+ 			      struct rdma_dev_net *rnet)
+ {
+ 	struct ib_core_device *cdev;
+ 	int ret;
+ 
+ 	lockdep_assert_held(&rdma_nets_rwsem);
+ 	if (!ib_devices_shared_netns)
+ 		return 0;
+ 
+ 	/*
+ 	 * Create and add compat device in all namespaces other than where it
+ 	 * is currently bound to.
+ 	 */
+ 	if (net_eq(read_pnet(&rnet->net),
+ 		   read_pnet(&device->coredev.rdma_net)))
+ 		return 0;
+ 
+ 	/*
+ 	 * The first of init_net() or ib_register_device() to take the
+ 	 * compat_devs_mutex wins and gets to add the device. Others will wait
+ 	 * for completion here.
+ 	 */
+ 	mutex_lock(&device->compat_devs_mutex);
+ 	cdev = xa_load(&device->compat_devs, rnet->id);
+ 	if (cdev) {
+ 		ret = 0;
+ 		goto done;
+ 	}
+ 	ret = xa_reserve(&device->compat_devs, rnet->id, GFP_KERNEL);
+ 	if (ret)
+ 		goto done;
+ 
+ 	cdev = kzalloc(sizeof(*cdev), GFP_KERNEL);
+ 	if (!cdev) {
+ 		ret = -ENOMEM;
+ 		goto cdev_err;
+ 	}
+ 
+ 	cdev->dev.parent = device->dev.parent;
+ 	rdma_init_coredev(cdev, device, read_pnet(&rnet->net));
+ 	cdev->dev.release = compatdev_release;
+ 	dev_set_name(&cdev->dev, "%s", dev_name(&device->dev));
+ 
+ 	ret = device_add(&cdev->dev);
+ 	if (ret)
+ 		goto add_err;
+ 	ret = ib_setup_port_attrs(cdev, false);
+ 	if (ret)
+ 		goto port_err;
+ 
+ 	ret = xa_err(xa_store(&device->compat_devs, rnet->id,
+ 			      cdev, GFP_KERNEL));
+ 	if (ret)
+ 		goto insert_err;
+ 
+ 	mutex_unlock(&device->compat_devs_mutex);
+ 	return 0;
+ 
+ insert_err:
+ 	ib_free_port_attrs(cdev);
+ port_err:
+ 	device_del(&cdev->dev);
+ add_err:
+ 	put_device(&cdev->dev);
+ cdev_err:
+ 	xa_release(&device->compat_devs, rnet->id);
+ done:
+ 	mutex_unlock(&device->compat_devs_mutex);
+ 	return ret;
+ }
+ 
+ static void remove_one_compat_dev(struct ib_device *device, u32 id)
+ {
+ 	struct ib_core_device *cdev;
+ 
+ 	mutex_lock(&device->compat_devs_mutex);
+ 	cdev = xa_erase(&device->compat_devs, id);
+ 	mutex_unlock(&device->compat_devs_mutex);
+ 	if (cdev) {
+ 		ib_free_port_attrs(cdev);
+ 		device_del(&cdev->dev);
+ 		put_device(&cdev->dev);
+ 	}
+ }
+ 
+ static void remove_compat_devs(struct ib_device *device)
+ {
+ 	struct ib_core_device *cdev;
+ 	unsigned long index;
+ 
+ 	xa_for_each (&device->compat_devs, index, cdev)
+ 		remove_one_compat_dev(device, index);
+ }
+ 
+ static int add_compat_devs(struct ib_device *device)
+ {
+ 	struct rdma_dev_net *rnet;
+ 	unsigned long index;
+ 	int ret = 0;
+ 
+ 	lockdep_assert_held(&devices_rwsem);
+ 
+ 	down_read(&rdma_nets_rwsem);
+ 	xa_for_each (&rdma_nets, index, rnet) {
+ 		ret = add_one_compat_dev(device, rnet);
+ 		if (ret)
+ 			break;
+ 	}
+ 	up_read(&rdma_nets_rwsem);
+ 	return ret;
+ }
+ 
+ static void remove_all_compat_devs(void)
+ {
+ 	struct ib_compat_device *cdev;
+ 	struct ib_device *dev;
+ 	unsigned long index;
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each (&devices, index, dev) {
+ 		unsigned long c_index = 0;
+ 
+ 		/* Hold nets_rwsem so that any other thread modifying this
+ 		 * system param can sync with this thread.
+ 		 */
+ 		down_read(&rdma_nets_rwsem);
+ 		xa_for_each (&dev->compat_devs, c_index, cdev)
+ 			remove_one_compat_dev(dev, c_index);
+ 		up_read(&rdma_nets_rwsem);
+ 	}
+ 	up_read(&devices_rwsem);
+ }
+ 
+ static int add_all_compat_devs(void)
+ {
+ 	struct rdma_dev_net *rnet;
+ 	struct ib_device *dev;
+ 	unsigned long index;
+ 	int ret = 0;
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {
+ 		unsigned long net_index = 0;
+ 
+ 		/* Hold nets_rwsem so that any other thread modifying this
+ 		 * system param can sync with this thread.
+ 		 */
+ 		down_read(&rdma_nets_rwsem);
+ 		xa_for_each (&rdma_nets, net_index, rnet) {
+ 			ret = add_one_compat_dev(dev, rnet);
+ 			if (ret)
+ 				break;
+ 		}
+ 		up_read(&rdma_nets_rwsem);
+ 	}
+ 	up_read(&devices_rwsem);
+ 	if (ret)
+ 		remove_all_compat_devs();
+ 	return ret;
+ }
+ 
+ int rdma_compatdev_set(u8 enable)
+ {
+ 	struct rdma_dev_net *rnet;
+ 	unsigned long index;
+ 	int ret = 0;
+ 
+ 	down_write(&rdma_nets_rwsem);
+ 	if (ib_devices_shared_netns == enable) {
+ 		up_write(&rdma_nets_rwsem);
+ 		return 0;
+ 	}
+ 
+ 	/* enable/disable of compat devices is not supported
+ 	 * when more than default init_net exists.
+ 	 */
+ 	xa_for_each (&rdma_nets, index, rnet) {
+ 		ret++;
+ 		break;
+ 	}
+ 	if (!ret)
+ 		ib_devices_shared_netns = enable;
+ 	up_write(&rdma_nets_rwsem);
+ 	if (ret)
+ 		return -EBUSY;
+ 
+ 	if (enable)
+ 		ret = add_all_compat_devs();
+ 	else
+ 		remove_all_compat_devs();
+ 	return ret;
+ }
+ 
+ static void rdma_dev_exit_net(struct net *net)
+ {
+ 	struct rdma_dev_net *rnet = net_generic(net, rdma_dev_net_id);
+ 	struct ib_device *dev;
+ 	unsigned long index;
+ 	int ret;
+ 
+ 	down_write(&rdma_nets_rwsem);
+ 	/*
+ 	 * Prevent the ID from being re-used and hide the id from xa_for_each.
+ 	 */
+ 	ret = xa_err(xa_store(&rdma_nets, rnet->id, NULL, GFP_KERNEL));
+ 	WARN_ON(ret);
+ 	up_write(&rdma_nets_rwsem);
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each (&devices, index, dev) {
+ 		get_device(&dev->dev);
+ 		/*
+ 		 * Release the devices_rwsem so that pontentially blocking
+ 		 * device_del, doesn't hold the devices_rwsem for too long.
+ 		 */
+ 		up_read(&devices_rwsem);
+ 
+ 		remove_one_compat_dev(dev, rnet->id);
+ 
+ 		/*
+ 		 * If the real device is in the NS then move it back to init.
+ 		 */
+ 		rdma_dev_change_netns(dev, net, &init_net);
+ 
+ 		put_device(&dev->dev);
+ 		down_read(&devices_rwsem);
+ 	}
+ 	up_read(&devices_rwsem);
+ 
+ 	xa_erase(&rdma_nets, rnet->id);
+ }
+ 
+ static __net_init int rdma_dev_init_net(struct net *net)
+ {
+ 	struct rdma_dev_net *rnet = net_generic(net, rdma_dev_net_id);
+ 	unsigned long index;
+ 	struct ib_device *dev;
+ 	int ret;
+ 
+ 	/* No need to create any compat devices in default init_net. */
+ 	if (net_eq(net, &init_net))
+ 		return 0;
+ 
+ 	write_pnet(&rnet->net, net);
+ 
+ 	ret = xa_alloc(&rdma_nets, &rnet->id, rnet, xa_limit_32b, GFP_KERNEL);
+ 	if (ret)
+ 		return ret;
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {
+ 		/* Hold nets_rwsem so that netlink command cannot change
+ 		 * system configuration for device sharing mode.
+ 		 */
+ 		down_read(&rdma_nets_rwsem);
+ 		ret = add_one_compat_dev(dev, rnet);
+ 		up_read(&rdma_nets_rwsem);
+ 		if (ret)
+ 			break;
+ 	}
+ 	up_read(&devices_rwsem);
+ 
+ 	if (ret)
+ 		rdma_dev_exit_net(net);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Assign the unique string device name and the unique device index. This is
+  * undone by ib_dealloc_device.
++>>>>>>> decbc7a6b007 (RDMA/core: Introduce a helper function to change net namespace of rdma device)
   */
 -static int assign_name(struct ib_device *device, const char *name)
 +static u32 __dev_new_index(void)
  {
 -	static u32 last_id;
 -	int ret;
 +	/*
 +	 * The device index to allow stable naming.
 +	 * Similar to struct net -> ifindex.
 +	 */
 +	static u32 index;
  
 -	down_write(&devices_rwsem);
 -	/* Assign a unique name to the device */
 -	if (strchr(name, '%'))
 -		ret = alloc_name(device, name);
 -	else
 -		ret = dev_set_name(&device->dev, name);
 -	if (ret)
 -		goto out;
 +	for (;;) {
 +		if (!(++index))
 +			index = 1;
  
 -	if (__ib_device_get_by_name(dev_name(&device->dev))) {
 -		ret = -ENFILE;
 -		goto out;
 +		if (!__ib_device_get_by_index(index))
 +			return index;
  	}
 -	strlcpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);
 -
 -	ret = xa_alloc_cyclic(&devices, &device->index, device, xa_limit_31b,
 -			&last_id, GFP_KERNEL);
 -	if (ret > 0)
 -		ret = 0;
 -
 -out:
 -	up_write(&devices_rwsem);
 -	return ret;
  }
  
  static void setup_dma_device(struct ib_device *device)
@@@ -700,6 -1347,171 +995,174 @@@ void ib_unregister_device(struct ib_dev
  }
  EXPORT_SYMBOL(ib_unregister_device);
  
++<<<<<<< HEAD
++=======
+ /**
+  * ib_unregister_device_and_put - Unregister a device while holding a 'get'
+  * device: The device to unregister
+  *
+  * This is the same as ib_unregister_device(), except it includes an internal
+  * ib_device_put() that should match a 'get' obtained by the caller.
+  *
+  * It is safe to call this routine concurrently from multiple threads while
+  * holding the 'get'. When the function returns the device is fully
+  * unregistered.
+  *
+  * Drivers using this flow MUST use the driver_unregister callback to clean up
+  * their resources associated with the device and dealloc it.
+  */
+ void ib_unregister_device_and_put(struct ib_device *ib_dev)
+ {
+ 	WARN_ON(!ib_dev->ops.dealloc_driver);
+ 	get_device(&ib_dev->dev);
+ 	ib_device_put(ib_dev);
+ 	__ib_unregister_device(ib_dev);
+ 	put_device(&ib_dev->dev);
+ }
+ EXPORT_SYMBOL(ib_unregister_device_and_put);
+ 
+ /**
+  * ib_unregister_driver - Unregister all IB devices for a driver
+  * @driver_id: The driver to unregister
+  *
+  * This implements a fence for device unregistration. It only returns once all
+  * devices associated with the driver_id have fully completed their
+  * unregistration and returned from ib_unregister_device*().
+  *
+  * If device's are not yet unregistered it goes ahead and starts unregistering
+  * them.
+  *
+  * This does not block creation of new devices with the given driver_id, that
+  * is the responsibility of the caller.
+  */
+ void ib_unregister_driver(enum rdma_driver_id driver_id)
+ {
+ 	struct ib_device *ib_dev;
+ 	unsigned long index;
+ 
+ 	down_read(&devices_rwsem);
+ 	xa_for_each (&devices, index, ib_dev) {
+ 		if (ib_dev->driver_id != driver_id)
+ 			continue;
+ 
+ 		get_device(&ib_dev->dev);
+ 		up_read(&devices_rwsem);
+ 
+ 		WARN_ON(!ib_dev->ops.dealloc_driver);
+ 		__ib_unregister_device(ib_dev);
+ 
+ 		put_device(&ib_dev->dev);
+ 		down_read(&devices_rwsem);
+ 	}
+ 	up_read(&devices_rwsem);
+ }
+ EXPORT_SYMBOL(ib_unregister_driver);
+ 
+ static void ib_unregister_work(struct work_struct *work)
+ {
+ 	struct ib_device *ib_dev =
+ 		container_of(work, struct ib_device, unregistration_work);
+ 
+ 	__ib_unregister_device(ib_dev);
+ 	put_device(&ib_dev->dev);
+ }
+ 
+ /**
+  * ib_unregister_device_queued - Unregister a device using a work queue
+  * device: The device to unregister
+  *
+  * This schedules an asynchronous unregistration using a WQ for the device. A
+  * driver should use this to avoid holding locks while doing unregistration,
+  * such as holding the RTNL lock.
+  *
+  * Drivers using this API must use ib_unregister_driver before module unload
+  * to ensure that all scheduled unregistrations have completed.
+  */
+ void ib_unregister_device_queued(struct ib_device *ib_dev)
+ {
+ 	WARN_ON(!refcount_read(&ib_dev->refcount));
+ 	WARN_ON(!ib_dev->ops.dealloc_driver);
+ 	get_device(&ib_dev->dev);
+ 	if (!queue_work(system_unbound_wq, &ib_dev->unregistration_work))
+ 		put_device(&ib_dev->dev);
+ }
+ EXPORT_SYMBOL(ib_unregister_device_queued);
+ 
+ /*
+  * The caller must pass in a device that has the kref held and the refcount
+  * released. If the device is in cur_net and still registered then it is moved
+  * into net.
+  */
+ static int rdma_dev_change_netns(struct ib_device *device, struct net *cur_net,
+ 				 struct net *net)
+ {
+ 	int ret2 = -EINVAL;
+ 	int ret;
+ 
+ 	mutex_lock(&device->unregistration_lock);
+ 
+ 	/*
+ 	 * If a device not under ib_device_get() or the unregistration_lock
+ 	 * the namespace can be changed, or it can be unregistered. Check
+ 	 * again under the lock.
+ 	 */
+ 	if (refcount_read(&device->refcount) == 0 ||
+ 	    !net_eq(cur_net, read_pnet(&device->coredev.rdma_net))) {
+ 		ret = -ENODEV;
+ 		goto out;
+ 	}
+ 
+ 	kobject_uevent(&device->dev.kobj, KOBJ_REMOVE);
+ 	disable_device(device);
+ 
+ 	/*
+ 	 * At this point no one can be using the device, so it is safe to
+ 	 * change the namespace.
+ 	 */
+ 	write_pnet(&device->coredev.rdma_net, net);
+ 
+ 	/*
+ 	 * Currently rdma devices are system wide unique. So the device name
+ 	 * is guaranteed free in the new namespace. Publish the new namespace
+ 	 * at the sysfs level.
+ 	 */
+ 	down_read(&devices_rwsem);
+ 	ret = device_rename(&device->dev, dev_name(&device->dev));
+ 	up_read(&devices_rwsem);
+ 	if (ret) {
+ 		dev_warn(&device->dev,
+ 			 "%s: Couldn't rename device after namespace change\n",
+ 			 __func__);
+ 		/* Try and put things back and re-enable the device */
+ 		write_pnet(&device->coredev.rdma_net, cur_net);
+ 	}
+ 
+ 	ret2 = enable_device_and_get(device);
+ 	if (ret2)
+ 		/*
+ 		 * This shouldn't really happen, but if it does, let the user
+ 		 * retry at later point. So don't disable the device.
+ 		 */
+ 		dev_warn(&device->dev,
+ 			 "%s: Couldn't re-enable device after namespace change\n",
+ 			 __func__);
+ 	kobject_uevent(&device->dev.kobj, KOBJ_ADD);
+ 	ib_device_put(device);
+ out:
+ 	mutex_unlock(&device->unregistration_lock);
+ 	if (ret)
+ 		return ret;
+ 	return ret2;
+ }
+ 
+ static struct pernet_operations rdma_dev_net_ops = {
+ 	.init = rdma_dev_init_net,
+ 	.exit = rdma_dev_exit_net,
+ 	.id = &rdma_dev_net_id,
+ 	.size = sizeof(struct rdma_dev_net),
+ };
+ 
++>>>>>>> decbc7a6b007 (RDMA/core: Introduce a helper function to change net namespace of rdma device)
  static int assign_client_id(struct ib_client *client)
  {
  	int ret;
* Unmerged path drivers/infiniband/core/device.c
