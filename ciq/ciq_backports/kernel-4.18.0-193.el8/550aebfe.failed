SUNRPC: Allow AF_LOCAL sockets to use the generic stream receive

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Trond Myklebust <trond.myklebust@hammerspace.com>
commit 550aebfe1c573518c35ae85d6ffbdc2d44c92703
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/550aebfe.failed

	Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
(cherry picked from commit 550aebfe1c573518c35ae85d6ffbdc2d44c92703)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtsock.c
diff --cc net/sunrpc/xprtsock.c
index b3c742fe8073,90d4c92177b7..000000000000
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@@ -325,6 -325,362 +325,365 @@@ static void xs_free_peer_addresses(stru
  		}
  }
  
++<<<<<<< HEAD
++=======
+ static size_t
+ xs_alloc_sparse_pages(struct xdr_buf *buf, size_t want, gfp_t gfp)
+ {
+ 	size_t i,n;
+ 
+ 	if (!(buf->flags & XDRBUF_SPARSE_PAGES))
+ 		return want;
+ 	if (want > buf->page_len)
+ 		want = buf->page_len;
+ 	n = (buf->page_base + want + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 	for (i = 0; i < n; i++) {
+ 		if (buf->pages[i])
+ 			continue;
+ 		buf->bvec[i].bv_page = buf->pages[i] = alloc_page(gfp);
+ 		if (!buf->pages[i]) {
+ 			buf->page_len = (i * PAGE_SIZE) - buf->page_base;
+ 			return buf->page_len;
+ 		}
+ 	}
+ 	return want;
+ }
+ 
+ static ssize_t
+ xs_sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags, size_t seek)
+ {
+ 	ssize_t ret;
+ 	if (seek != 0)
+ 		iov_iter_advance(&msg->msg_iter, seek);
+ 	ret = sock_recvmsg(sock, msg, flags);
+ 	return ret > 0 ? ret + seek : ret;
+ }
+ 
+ static ssize_t
+ xs_read_kvec(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct kvec *kvec, size_t count, size_t seek)
+ {
+ 	iov_iter_kvec(&msg->msg_iter, READ | ITER_KVEC, kvec, 1, count);
+ 	return xs_sock_recvmsg(sock, msg, flags, seek);
+ }
+ 
+ static ssize_t
+ xs_read_bvec(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct bio_vec *bvec, unsigned long nr, size_t count,
+ 		size_t seek)
+ {
+ 	iov_iter_bvec(&msg->msg_iter, READ | ITER_BVEC, bvec, nr, count);
+ 	return xs_sock_recvmsg(sock, msg, flags, seek);
+ }
+ 
+ static ssize_t
+ xs_read_discard(struct socket *sock, struct msghdr *msg, int flags,
+ 		size_t count)
+ {
+ 	struct kvec kvec = { 0 };
+ 	return xs_read_kvec(sock, msg, flags | MSG_TRUNC, &kvec, count, 0);
+ }
+ 
+ static ssize_t
+ xs_read_xdr_buf(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct xdr_buf *buf, size_t count, size_t seek, size_t *read)
+ {
+ 	size_t want, seek_init = seek, offset = 0;
+ 	ssize_t ret;
+ 
+ 	if (seek < buf->head[0].iov_len) {
+ 		want = min_t(size_t, count, buf->head[0].iov_len);
+ 		ret = xs_read_kvec(sock, msg, flags, &buf->head[0], want, seek);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		offset += ret;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto eagain;
+ 		seek = 0;
+ 	} else {
+ 		seek -= buf->head[0].iov_len;
+ 		offset += buf->head[0].iov_len;
+ 	}
+ 	if (seek < buf->page_len) {
+ 		want = xs_alloc_sparse_pages(buf,
+ 				min_t(size_t, count - offset, buf->page_len),
+ 				GFP_NOWAIT);
+ 		ret = xs_read_bvec(sock, msg, flags, buf->bvec,
+ 				xdr_buf_pagecount(buf),
+ 				want + buf->page_base,
+ 				seek + buf->page_base);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		offset += ret - buf->page_base;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto eagain;
+ 		seek = 0;
+ 	} else {
+ 		seek -= buf->page_len;
+ 		offset += buf->page_len;
+ 	}
+ 	if (seek < buf->tail[0].iov_len) {
+ 		want = min_t(size_t, count - offset, buf->tail[0].iov_len);
+ 		ret = xs_read_kvec(sock, msg, flags, &buf->tail[0], want, seek);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		offset += ret;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto eagain;
+ 	} else
+ 		offset += buf->tail[0].iov_len;
+ 	ret = -EMSGSIZE;
+ 	msg->msg_flags |= MSG_TRUNC;
+ out:
+ 	*read = offset - seek_init;
+ 	return ret;
+ eagain:
+ 	ret = -EAGAIN;
+ 	goto out;
+ sock_err:
+ 	offset += seek;
+ 	goto out;
+ }
+ 
+ static void
+ xs_read_header(struct sock_xprt *transport, struct xdr_buf *buf)
+ {
+ 	if (!transport->recv.copied) {
+ 		if (buf->head[0].iov_len >= transport->recv.offset)
+ 			memcpy(buf->head[0].iov_base,
+ 					&transport->recv.xid,
+ 					transport->recv.offset);
+ 		transport->recv.copied = transport->recv.offset;
+ 	}
+ }
+ 
+ static bool
+ xs_read_stream_request_done(struct sock_xprt *transport)
+ {
+ 	return transport->recv.fraghdr & cpu_to_be32(RPC_LAST_STREAM_FRAGMENT);
+ }
+ 
+ static ssize_t
+ xs_read_stream_request(struct sock_xprt *transport, struct msghdr *msg,
+ 		int flags, struct rpc_rqst *req)
+ {
+ 	struct xdr_buf *buf = &req->rq_private_buf;
+ 	size_t want, read;
+ 	ssize_t ret;
+ 
+ 	xs_read_header(transport, buf);
+ 
+ 	want = transport->recv.len - transport->recv.offset;
+ 	ret = xs_read_xdr_buf(transport->sock, msg, flags, buf,
+ 			transport->recv.copied + want, transport->recv.copied,
+ 			&read);
+ 	transport->recv.offset += read;
+ 	transport->recv.copied += read;
+ 	if (transport->recv.offset == transport->recv.len) {
+ 		if (xs_read_stream_request_done(transport))
+ 			msg->msg_flags |= MSG_EOR;
+ 		return transport->recv.copied;
+ 	}
+ 
+ 	switch (ret) {
+ 	case -EMSGSIZE:
+ 		return transport->recv.copied;
+ 	case 0:
+ 		return -ESHUTDOWN;
+ 	default:
+ 		if (ret < 0)
+ 			return ret;
+ 	}
+ 	return -EAGAIN;
+ }
+ 
+ static size_t
+ xs_read_stream_headersize(bool isfrag)
+ {
+ 	if (isfrag)
+ 		return sizeof(__be32);
+ 	return 3 * sizeof(__be32);
+ }
+ 
+ static ssize_t
+ xs_read_stream_header(struct sock_xprt *transport, struct msghdr *msg,
+ 		int flags, size_t want, size_t seek)
+ {
+ 	struct kvec kvec = {
+ 		.iov_base = &transport->recv.fraghdr,
+ 		.iov_len = want,
+ 	};
+ 	return xs_read_kvec(transport->sock, msg, flags, &kvec, want, seek);
+ }
+ 
+ #if defined(CONFIG_SUNRPC_BACKCHANNEL)
+ static ssize_t
+ xs_read_stream_call(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	struct rpc_xprt *xprt = &transport->xprt;
+ 	struct rpc_rqst *req;
+ 	ssize_t ret;
+ 
+ 	/* Look up and lock the request corresponding to the given XID */
+ 	req = xprt_lookup_bc_request(xprt, transport->recv.xid);
+ 	if (!req) {
+ 		printk(KERN_WARNING "Callback slot table overflowed\n");
+ 		return -ESHUTDOWN;
+ 	}
+ 
+ 	ret = xs_read_stream_request(transport, msg, flags, req);
+ 	if (msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 		xprt_complete_bc_request(req, ret);
+ 
+ 	return ret;
+ }
+ #else /* CONFIG_SUNRPC_BACKCHANNEL */
+ static ssize_t
+ xs_read_stream_call(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	return -ESHUTDOWN;
+ }
+ #endif /* CONFIG_SUNRPC_BACKCHANNEL */
+ 
+ static ssize_t
+ xs_read_stream_reply(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	struct rpc_xprt *xprt = &transport->xprt;
+ 	struct rpc_rqst *req;
+ 	ssize_t ret = 0;
+ 
+ 	/* Look up and lock the request corresponding to the given XID */
+ 	spin_lock(&xprt->queue_lock);
+ 	req = xprt_lookup_rqst(xprt, transport->recv.xid);
+ 	if (!req) {
+ 		msg->msg_flags |= MSG_TRUNC;
+ 		goto out;
+ 	}
+ 	xprt_pin_rqst(req);
+ 	spin_unlock(&xprt->queue_lock);
+ 
+ 	ret = xs_read_stream_request(transport, msg, flags, req);
+ 
+ 	spin_lock(&xprt->queue_lock);
+ 	if (msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 		xprt_complete_rqst(req->rq_task, ret);
+ 	xprt_unpin_rqst(req);
+ out:
+ 	spin_unlock(&xprt->queue_lock);
+ 	return ret;
+ }
+ 
+ static ssize_t
+ xs_read_stream(struct sock_xprt *transport, int flags)
+ {
+ 	struct msghdr msg = { 0 };
+ 	size_t want, read = 0;
+ 	ssize_t ret = 0;
+ 
+ 	if (transport->recv.len == 0) {
+ 		want = xs_read_stream_headersize(transport->recv.copied != 0);
+ 		ret = xs_read_stream_header(transport, &msg, flags, want,
+ 				transport->recv.offset);
+ 		if (ret <= 0)
+ 			goto out_err;
+ 		transport->recv.offset = ret;
+ 		if (ret != want) {
+ 			ret = -EAGAIN;
+ 			goto out_err;
+ 		}
+ 		transport->recv.len = be32_to_cpu(transport->recv.fraghdr) &
+ 			RPC_FRAGMENT_SIZE_MASK;
+ 		transport->recv.offset -= sizeof(transport->recv.fraghdr);
+ 		read = ret;
+ 	}
+ 
+ 	switch (be32_to_cpu(transport->recv.calldir)) {
+ 	case RPC_CALL:
+ 		ret = xs_read_stream_call(transport, &msg, flags);
+ 		break;
+ 	case RPC_REPLY:
+ 		ret = xs_read_stream_reply(transport, &msg, flags);
+ 	}
+ 	if (msg.msg_flags & MSG_TRUNC) {
+ 		transport->recv.calldir = cpu_to_be32(-1);
+ 		transport->recv.copied = -1;
+ 	}
+ 	if (ret < 0)
+ 		goto out_err;
+ 	read += ret;
+ 	if (transport->recv.offset < transport->recv.len) {
+ 		ret = xs_read_discard(transport->sock, &msg, flags,
+ 				transport->recv.len - transport->recv.offset);
+ 		if (ret <= 0)
+ 			goto out_err;
+ 		transport->recv.offset += ret;
+ 		read += ret;
+ 		if (transport->recv.offset != transport->recv.len)
+ 			return -EAGAIN;
+ 	}
+ 	if (xs_read_stream_request_done(transport)) {
+ 		trace_xs_stream_read_request(transport);
+ 		transport->recv.copied = 0;
+ 	}
+ 	transport->recv.offset = 0;
+ 	transport->recv.len = 0;
+ 	return read;
+ out_err:
+ 	switch (ret) {
+ 	case 0:
+ 	case -ESHUTDOWN:
+ 		xprt_force_disconnect(&transport->xprt);
+ 		return -ESHUTDOWN;
+ 	}
+ 	return ret;
+ }
+ 
+ static void xs_stream_data_receive(struct sock_xprt *transport)
+ {
+ 	size_t read = 0;
+ 	ssize_t ret = 0;
+ 
+ 	mutex_lock(&transport->recv_mutex);
+ 	if (transport->sock == NULL)
+ 		goto out;
+ 	clear_bit(XPRT_SOCK_DATA_READY, &transport->sock_state);
+ 	for (;;) {
+ 		ret = xs_read_stream(transport, MSG_DONTWAIT);
+ 		if (ret <= 0)
+ 			break;
+ 		read += ret;
+ 		cond_resched();
+ 	}
+ out:
+ 	mutex_unlock(&transport->recv_mutex);
+ 	trace_xs_stream_read_data(&transport->xprt, ret, read);
+ }
+ 
+ static void xs_stream_data_receive_workfn(struct work_struct *work)
+ {
+ 	struct sock_xprt *transport =
+ 		container_of(work, struct sock_xprt, recv_worker);
+ 	xs_stream_data_receive(transport);
+ }
+ 
+ static void
+ xs_stream_reset_connect(struct sock_xprt *transport)
+ {
+ 	transport->recv.offset = 0;
+ 	transport->recv.len = 0;
+ 	transport->recv.copied = 0;
+ 	transport->xmit.offset = 0;
+ 	transport->xprt.stat.connect_count++;
+ 	transport->xprt.stat.connect_start = jiffies;
+ }
+ 
++>>>>>>> 550aebfe1c57 (SUNRPC: Allow AF_LOCAL sockets to use the generic stream receive)
  #define XS_SENDMSG_FLAGS	(MSG_DONTWAIT | MSG_NOSIGNAL)
  
  static int xs_send_kvec(struct socket *sock, struct sockaddr *addr, int addrlen, struct kvec *vec, unsigned int base, int more)
@@@ -931,114 -1277,6 +1290,117 @@@ static void xs_destroy(struct rpc_xprt 
  	module_put(THIS_MODULE);
  }
  
++<<<<<<< HEAD
 +static int xs_local_copy_to_xdr(struct xdr_buf *xdr, struct sk_buff *skb)
 +{
 +	struct xdr_skb_reader desc = {
 +		.skb		= skb,
 +		.offset		= sizeof(rpc_fraghdr),
 +		.count		= skb->len - sizeof(rpc_fraghdr),
 +	};
 +
 +	if (xdr_partial_copy_from_skb(xdr, 0, &desc, xdr_skb_read_bits) < 0)
 +		return -1;
 +	if (desc.count)
 +		return -1;
 +	return 0;
 +}
 +
 +/**
 + * xs_local_data_read_skb
 + * @xprt: transport
 + * @sk: socket
 + * @skb: skbuff
 + *
 + * Currently this assumes we can read the whole reply in a single gulp.
 + */
 +static void xs_local_data_read_skb(struct rpc_xprt *xprt,
 +		struct sock *sk,
 +		struct sk_buff *skb)
 +{
 +	struct rpc_task *task;
 +	struct rpc_rqst *rovr;
 +	int repsize, copied;
 +	u32 _xid;
 +	__be32 *xp;
 +
 +	repsize = skb->len - sizeof(rpc_fraghdr);
 +	if (repsize < 4) {
 +		dprintk("RPC:       impossible RPC reply size %d\n", repsize);
 +		return;
 +	}
 +
 +	/* Copy the XID from the skb... */
 +	xp = skb_header_pointer(skb, sizeof(rpc_fraghdr), sizeof(_xid), &_xid);
 +	if (xp == NULL)
 +		return;
 +
 +	/* Look up and lock the request corresponding to the given XID */
 +	spin_lock(&xprt->recv_lock);
 +	rovr = xprt_lookup_rqst(xprt, *xp);
 +	if (!rovr)
 +		goto out_unlock;
 +	xprt_pin_rqst(rovr);
 +	spin_unlock(&xprt->recv_lock);
 +	task = rovr->rq_task;
 +
 +	copied = rovr->rq_private_buf.buflen;
 +	if (copied > repsize)
 +		copied = repsize;
 +
 +	if (xs_local_copy_to_xdr(&rovr->rq_private_buf, skb)) {
 +		dprintk("RPC:       sk_buff copy failed\n");
 +		spin_lock(&xprt->recv_lock);
 +		goto out_unpin;
 +	}
 +
 +	spin_lock(&xprt->recv_lock);
 +	xprt_complete_rqst(task, copied);
 +out_unpin:
 +	xprt_unpin_rqst(rovr);
 + out_unlock:
 +	spin_unlock(&xprt->recv_lock);
 +}
 +
 +static void xs_local_data_receive(struct sock_xprt *transport)
 +{
 +	struct sk_buff *skb;
 +	struct sock *sk;
 +	int err;
 +
 +restart:
 +	mutex_lock(&transport->recv_mutex);
 +	sk = transport->inet;
 +	if (sk == NULL)
 +		goto out;
 +	for (;;) {
 +		skb = skb_recv_datagram(sk, 0, 1, &err);
 +		if (skb != NULL) {
 +			xs_local_data_read_skb(&transport->xprt, sk, skb);
 +			skb_free_datagram(sk, skb);
 +			continue;
 +		}
 +		if (!test_and_clear_bit(XPRT_SOCK_DATA_READY, &transport->sock_state))
 +			break;
 +		if (need_resched()) {
 +			mutex_unlock(&transport->recv_mutex);
 +			cond_resched();
 +			goto restart;
 +		}
 +	}
 +out:
 +	mutex_unlock(&transport->recv_mutex);
 +}
 +
 +static void xs_local_data_receive_workfn(struct work_struct *work)
 +{
 +	struct sock_xprt *transport =
 +		container_of(work, struct sock_xprt, recv_worker);
 +	xs_local_data_receive(transport);
 +}
 +
++=======
++>>>>>>> 550aebfe1c57 (SUNRPC: Allow AF_LOCAL sockets to use the generic stream receive)
  /**
   * xs_udp_data_read_skb - receive callback for UDP sockets
   * @xprt: transport
@@@ -2032,9 -1877,8 +2394,12 @@@ static int xs_local_finish_connecting(s
  		write_unlock_bh(&sk->sk_callback_lock);
  	}
  
- 	transport->xmit.offset = 0;
+ 	xs_stream_reset_connect(transport);
  
++<<<<<<< HEAD
 +	/* Tell the socket layer to start connecting... */
++=======
++>>>>>>> 550aebfe1c57 (SUNRPC: Allow AF_LOCAL sockets to use the generic stream receive)
  	return kernel_connect(sock, xs_addr(xprt), xprt->addrlen, 0);
  }
  
@@@ -2389,11 -2235,7 +2754,15 @@@ static int xs_tcp_finish_connecting(str
  	xs_set_memalloc(xprt);
  
  	/* Reset TCP record info */
++<<<<<<< HEAD
 +	transport->recv.offset = 0;
 +	transport->recv.len = 0;
 +	transport->recv.copied = 0;
 +	transport->recv.flags = TCP_RCV_COPY_FRAGHDR | TCP_RCV_COPY_XID;
 +	transport->xmit.offset = 0;
++=======
+ 	xs_stream_reset_connect(transport);
++>>>>>>> 550aebfe1c57 (SUNRPC: Allow AF_LOCAL sockets to use the generic stream receive)
  
  	/* Tell the socket layer to start connecting... */
  	set_bit(XPRT_SOCK_CONNECTING, &transport->sock_state);
diff --git a/include/linux/sunrpc/xdr.h b/include/linux/sunrpc/xdr.h
index 431829233392..9dc9dde533cf 100644
--- a/include/linux/sunrpc/xdr.h
+++ b/include/linux/sunrpc/xdr.h
@@ -178,7 +178,6 @@ struct xdr_skb_reader {
 
 typedef size_t (*xdr_skb_read_actor)(struct xdr_skb_reader *desc, void *to, size_t len);
 
-size_t xdr_skb_read_bits(struct xdr_skb_reader *desc, void *to, size_t len);
 extern int csum_partial_copy_to_xdr(struct xdr_buf *, struct sk_buff *);
 extern ssize_t xdr_partial_copy_from_skb(struct xdr_buf *, unsigned int,
 		struct xdr_skb_reader *, xdr_skb_read_actor);
diff --git a/net/sunrpc/socklib.c b/net/sunrpc/socklib.c
index 08f00a98151f..0e7c0dee7578 100644
--- a/net/sunrpc/socklib.c
+++ b/net/sunrpc/socklib.c
@@ -26,7 +26,8 @@
  * Possibly called several times to iterate over an sk_buff and copy
  * data out of it.
  */
-size_t xdr_skb_read_bits(struct xdr_skb_reader *desc, void *to, size_t len)
+static size_t
+xdr_skb_read_bits(struct xdr_skb_reader *desc, void *to, size_t len)
 {
 	if (len > desc->count)
 		len = desc->count;
@@ -36,7 +37,6 @@ size_t xdr_skb_read_bits(struct xdr_skb_reader *desc, void *to, size_t len)
 	desc->offset += len;
 	return len;
 }
-EXPORT_SYMBOL_GPL(xdr_skb_read_bits);
 
 /**
  * xdr_skb_read_and_csum_bits - copy and checksum from skb to buffer
* Unmerged path net/sunrpc/xprtsock.c
