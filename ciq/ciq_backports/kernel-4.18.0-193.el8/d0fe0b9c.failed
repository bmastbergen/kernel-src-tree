sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dietmar Eggemann <dietmar.eggemann@arm.com>
commit d0fe0b9c45c144e4ac60cf7f07f7e8ae86d3536d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/d0fe0b9c.failed

Since commit:

  d03266910a53 ("sched/fair: Fix task group initialization")

the utilization of a sched entity representing a task group is no longer
initialized to any other value than 0. So post_init_entity_util_avg() is
only used for tasks, not for sched_entities.

Make this clear by calling it with a task_struct pointer argument which
also eliminates the entity_is_task(se) if condition in the fork path and
get rid of the stale comment in remove_entity_load_avg() accordingly.

	Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Morten Rasmussen <morten.rasmussen@arm.com>
	Cc: Patrick Bellasi <patrick.bellasi@arm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Quentin Perret <quentin.perret@arm.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Valentin Schneider <valentin.schneider@arm.com>
	Cc: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lkml.kernel.org/r/20190122162501.12000-1-dietmar.eggemann@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d0fe0b9c45c144e4ac60cf7f07f7e8ae86d3536d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index ea2f2b2bf238,58edbbdeb661..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -780,22 -779,19 +781,38 @@@ void post_init_entity_util_avg(struct t
  		}
  	}
  
++<<<<<<< HEAD
 +	if (entity_is_task(se)) {
 +		struct task_struct *p = task_of(se);
 +		if (p->sched_class != &fair_sched_class) {
 +			/*
 +			 * For !fair tasks do:
 +			 *
 +			update_cfs_rq_load_avg(now, cfs_rq);
 +			attach_entity_load_avg(cfs_rq, se, 0);
 +			switched_from_fair(rq, p);
 +			 *
 +			 * such that the next switched_to_fair() has the
 +			 * expected state.
 +			 */
 +			se->avg.last_update_time = cfs_rq_clock_task(cfs_rq);
 +			return;
 +		}
++=======
+ 	if (p->sched_class != &fair_sched_class) {
+ 		/*
+ 		 * For !fair tasks do:
+ 		 *
+ 		update_cfs_rq_load_avg(now, cfs_rq);
+ 		attach_entity_load_avg(cfs_rq, se, 0);
+ 		switched_from_fair(rq, p);
+ 		 *
+ 		 * such that the next switched_to_fair() has the
+ 		 * expected state.
+ 		 */
+ 		se->avg.last_update_time = cfs_rq_clock_pelt(cfs_rq);
+ 		return;
++>>>>>>> d0fe0b9c45c1 (sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument)
  	}
  
  	attach_entity_cfs_rq(se);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index e0bccbcd0342..c8056fae775b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2409,7 +2409,7 @@ void wake_up_new_task(struct task_struct *p)
 #endif
 	rq = __task_rq_lock(p, &rf);
 	update_rq_clock(rq);
-	post_init_entity_util_avg(&p->se);
+	post_init_entity_util_avg(p);
 
 	activate_task(rq, p, ENQUEUE_NOCLOCK);
 	p->on_rq = TASK_ON_RQ_QUEUED;
* Unmerged path kernel/sched/fair.c
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2f0ee3f2ef1e..4b2e90620641 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1785,7 +1785,7 @@ extern void init_dl_rq_bw_ratio(struct dl_rq *dl_rq);
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
-extern void post_init_entity_util_avg(struct sched_entity *se);
+extern void post_init_entity_util_avg(struct task_struct *p);
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(struct rq *rq);
