drm/ttm: return -EBUSY on pipelining with no_gpu_wait (v2)

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Christian König <christian.koenig@amd.com>
commit 3084cf46cf8110826a42de8c8ef30e8fa48974c2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/3084cf46.failed

Setting the no_gpu_wait flag means that the allocate BO must be available
immediately and we can't wait for any GPU operation to finish.

v2: squash in mem leak fix, rebase

	Signed-off-by: Christian König <christian.koenig@amd.com>
	Acked-by: Felix Kuehling <Felix.Kuehling@amd.com>
	Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
(cherry picked from commit 3084cf46cf8110826a42de8c8ef30e8fa48974c2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/ttm/ttm_bo.c
diff --cc drivers/gpu/drm/ttm/ttm_bo.c
index 1a01669b159a,6394e0c5cc02..000000000000
--- a/drivers/gpu/drm/ttm/ttm_bo.c
+++ b/drivers/gpu/drm/ttm/ttm_bo.c
@@@ -871,19 -935,22 +872,33 @@@ static int ttm_bo_add_move_fence(struc
  	fence = dma_fence_get(man->move);
  	spin_unlock(&man->move_lock);
  
++<<<<<<< HEAD
 +	if (fence) {
 +		reservation_object_add_shared_fence(bo->resv, fence);
 +
 +		ret = reservation_object_reserve_shared(bo->resv, 1);
 +		if (unlikely(ret)) {
 +			dma_fence_put(fence);
 +			return ret;
 +		}
++=======
+ 	if (!fence)
+ 		return 0;
+ 
+ 	if (no_wait_gpu)
+ 		return -EBUSY;
++>>>>>>> 3084cf46cf81 (drm/ttm: return -EBUSY on pipelining with no_gpu_wait (v2))
+ 
+ 	dma_resv_add_shared_fence(bo->base.resv, fence);
  
- 		dma_fence_put(bo->moving);
- 		bo->moving = fence;
+ 	ret = dma_resv_reserve_shared(bo->base.resv, 1);
+ 	if (unlikely(ret)) {
+ 		dma_fence_put(fence);
+ 		return ret;
  	}
  
+ 	dma_fence_put(bo->moving);
+ 	bo->moving = fence;
  	return 0;
  }
  
@@@ -911,8 -980,8 +926,13 @@@ static int ttm_bo_mem_force_space(struc
  		if (unlikely(ret != 0))
  			return ret;
  	} while (1);
++<<<<<<< HEAD
 +	mem->mem_type = mem_type;
 +	return ttm_bo_add_move_fence(bo, man, mem);
++=======
+ 
+ 	return ttm_bo_add_move_fence(bo, man, mem, ctx->no_wait_gpu);
++>>>>>>> 3084cf46cf81 (drm/ttm: return -EBUSY on pipelining with no_gpu_wait (v2))
  }
  
  static uint32_t ttm_bo_select_caching(struct ttm_mem_type_manager *man,
@@@ -988,53 -1105,38 +1008,67 @@@ int ttm_bo_mem_space(struct ttm_buffer_
  	mem->mm_node = NULL;
  	for (i = 0; i < placement->num_placement; ++i) {
  		const struct ttm_place *place = &placement->placement[i];
 -		struct ttm_mem_type_manager *man;
  
 -		ret = ttm_bo_mem_placement(bo, place, mem, ctx);
 -		if (ret == -EBUSY)
 -			continue;
 +		ret = ttm_mem_type_from_place(place, &mem_type);
  		if (ret)
 -			goto error;
 +			return ret;
 +		man = &bdev->man[mem_type];
 +		if (!man->has_type || !man->use_type)
 +			continue;
 +
 +		type_ok = ttm_bo_mt_compatible(man, mem_type, place,
 +						&cur_flags);
 +
 +		if (!type_ok)
 +			continue;
  
  		type_found = true;
 -		mem->mm_node = NULL;
 -		if (mem->mem_type == TTM_PL_SYSTEM)
 -			return 0;
 +		cur_flags = ttm_bo_select_caching(man, bo->mem.placement,
 +						  cur_flags);
 +		/*
 +		 * Use the access and other non-mapping-related flag bits from
 +		 * the memory placement flags to the current flags
 +		 */
 +		ttm_flag_masked(&cur_flags, place->flags,
 +				~TTM_PL_MASK_MEMTYPE);
 +
 +		if (mem_type == TTM_PL_SYSTEM)
 +			break;
  
 -		man = &bdev->man[mem->mem_type];
  		ret = (*man->func->get_node)(man, bo, place, mem);
  		if (unlikely(ret))
 -			goto error;
 +			return ret;
  
++<<<<<<< HEAD
 +		if (mem->mm_node) {
 +			ret = ttm_bo_add_move_fence(bo, man, mem);
 +			if (unlikely(ret)) {
 +				(*man->func->put_node)(man, mem);
 +				return ret;
 +			}
 +			break;
++=======
+ 		if (!mem->mm_node)
+ 			continue;
+ 
+ 		ret = ttm_bo_add_move_fence(bo, man, mem, ctx->no_wait_gpu);
+ 		if (unlikely(ret)) {
+ 			(*man->func->put_node)(man, mem);
+ 			if (ret == -EBUSY)
+ 				continue;
+ 
+ 			goto error;
++>>>>>>> 3084cf46cf81 (drm/ttm: return -EBUSY on pipelining with no_gpu_wait (v2))
  		}
+ 		return 0;
  	}
  
 +	if ((type_ok && (mem_type == TTM_PL_SYSTEM)) || mem->mm_node) {
 +		mem->mem_type = mem_type;
 +		mem->placement = cur_flags;
 +		return 0;
 +	}
 +
  	for (i = 0; i < placement->num_busy_placement; ++i) {
  		const struct ttm_place *place = &placement->busy_placement[i];
  
* Unmerged path drivers/gpu/drm/ttm/ttm_bo.c
