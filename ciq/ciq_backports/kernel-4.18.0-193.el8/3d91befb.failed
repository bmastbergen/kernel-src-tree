arm64: KVM: Enable !VHE support for :G/:H perf event modifiers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [arm64] kvm: Enable !VHE support for :G/:H perf event modifiers (Auger Eric) [1749501]
Rebuild_FUZZ: 94.02%
commit-author Andrew Murray <andrew.murray@arm.com>
commit 3d91befbb3a0fcec6e1eebde45c8074b88cc9441
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/3d91befb.failed

Enable/disable event counters as appropriate when entering and exiting
the guest to enable support for guest or host only event counting.

For both VHE and non-VHE we switch the counters between host/guest at
EL2.

The PMU may be on when we change which counters are enabled however
we avoid adding an isb as we instead rely on existing context
synchronisation events: the eret to enter the guest (__guest_enter)
and eret in kvm_call_hyp for __kvm_vcpu_run_nvhe on returning.

	Signed-off-by: Andrew Murray <andrew.murray@arm.com>
	Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
(cherry picked from commit 3d91befbb3a0fcec6e1eebde45c8074b88cc9441)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/kvm_host.h
#	arch/arm64/kvm/hyp/switch.c
#	arch/arm64/kvm/pmu.c
diff --cc arch/arm64/include/asm/kvm_host.h
index 90427baf5f4c,645d74c705d6..000000000000
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@@ -553,6 -588,15 +553,18 @@@ static inline int kvm_arch_vcpu_run_pid
  {
  	return kvm_arch_vcpu_run_map_fp(vcpu);
  }
++<<<<<<< HEAD
++=======
+ 
+ void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr);
+ void kvm_clr_pmu_events(u32 clr);
+ 
+ void __pmu_switch_to_host(struct kvm_cpu_context *host_ctxt);
+ bool __pmu_switch_to_guest(struct kvm_cpu_context *host_ctxt);
+ #else
+ static inline void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr) {}
+ static inline void kvm_clr_pmu_events(u32 clr) {}
++>>>>>>> 3d91befbb3a0 (arm64: KVM: Enable !VHE support for :G/:H perf event modifiers)
  #endif
  
  static inline void kvm_arm_vhe_guest_enter(void)
diff --cc arch/arm64/kvm/hyp/switch.c
index 818267feda20,22b4c335e0b2..000000000000
--- a/arch/arm64/kvm/hyp/switch.c
+++ b/arch/arm64/kvm/hyp/switch.c
@@@ -565,8 -566,20 +565,9 @@@ int __hyp_text __kvm_vcpu_run_nvhe(stru
  {
  	struct kvm_cpu_context *host_ctxt;
  	struct kvm_cpu_context *guest_ctxt;
+ 	bool pmu_switch_needed;
  	u64 exit_code;
  
 -	/*
 -	 * Having IRQs masked via PMR when entering the guest means the GIC
 -	 * will not signal the CPU of interrupts of lower priority, and the
 -	 * only way to get out will be via guest exceptions.
 -	 * Naturally, we want to avoid this.
 -	 */
 -	if (system_uses_irq_prio_masking()) {
 -		gic_write_pmr(GIC_PRIO_IRQON);
 -		dsb(sy);
 -	}
 -
  	vcpu = kern_hyp_va(vcpu);
  
  	host_ctxt = kern_hyp_va(vcpu->arch.host_cpu_context);
@@@ -619,6 -634,13 +622,16 @@@
  	 */
  	__debug_switch_to_host(vcpu);
  
++<<<<<<< HEAD
++=======
+ 	if (pmu_switch_needed)
+ 		__pmu_switch_to_host(host_ctxt);
+ 
+ 	/* Returning to host will clear PSR.I, remask PMR if needed */
+ 	if (system_uses_irq_prio_masking())
+ 		gic_write_pmr(GIC_PRIO_IRQOFF);
+ 
++>>>>>>> 3d91befbb3a0 (arm64: KVM: Enable !VHE support for :G/:H perf event modifiers)
  	return exit_code;
  }
  
* Unmerged path arch/arm64/kvm/pmu.c
* Unmerged path arch/arm64/include/asm/kvm_host.h
* Unmerged path arch/arm64/kvm/hyp/switch.c
* Unmerged path arch/arm64/kvm/pmu.c
