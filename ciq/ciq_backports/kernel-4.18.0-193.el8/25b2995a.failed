mm: remove MEMORY_DEVICE_PUBLIC support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] remove MEMORY_DEVICE_PUBLIC support (Don Dutile) [1754737]
Rebuild_FUZZ: 94.59%
commit-author Christoph Hellwig <hch@lst.de>
commit 25b2995a35b609119cf96f6b62eccd56c0234c7d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/25b2995a.failed

The code hasn't been used since it was added to the tree, and doesn't
appear to actually be usable.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Tested-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 25b2995a35b609119cf96f6b62eccd56c0234c7d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	include/linux/ioport.h
#	mm/hmm.c
#	mm/migrate.c
diff --cc include/linux/hmm.h
index 2f68a486cc0d,44a5ac738bb5..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -570,27 -748,7 +567,31 @@@ static inline unsigned long hmm_devmem_
  {
  	return page->hmm_data;
  }
++<<<<<<< HEAD
 +
 +
 +/*
 + * struct hmm_device - fake device to hang device memory onto
 + *
 + * @device: device struct
 + * @minor: device minor number
 + */
 +struct hmm_device {
 +	struct device		device;
 +	unsigned int		minor;
 +};
 +
 +/*
 + * A device driver that wants to handle multiple devices memory through a
 + * single fake device can use hmm_device to do so. This is purely a helper and
 + * it is not strictly needed, in order to make use of any HMM functionality.
 + */
 +struct hmm_device *hmm_device_new(void *drvdata);
 +void hmm_device_put(struct hmm_device *hmm_device);
 +#endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
++=======
+ #endif /* CONFIG_DEVICE_PRIVATE */
++>>>>>>> 25b2995a35b6 (mm: remove MEMORY_DEVICE_PUBLIC support)
  #else /* IS_ENABLED(CONFIG_HMM) */
  static inline void hmm_mm_destroy(struct mm_struct *mm) {}
  static inline void hmm_mm_init(struct mm_struct *mm) {}
diff --cc include/linux/ioport.h
index d61a807f81e5,dd961882bc74..000000000000
--- a/include/linux/ioport.h
+++ b/include/linux/ioport.h
@@@ -139,16 -132,6 +139,19 @@@ enum 
  	IORES_DESC_PERSISTENT_MEMORY		= 4,
  	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,
  	IORES_DESC_DEVICE_PRIVATE_MEMORY	= 6,
++<<<<<<< HEAD
 +	IORES_DESC_DEVICE_PUBLIC_MEMORY		= 7,
 +	IORES_DESC_RESERVED			= 8,
 +};
 +
 +/*
 + * Flags controlling ioremap() behavior.
 + */
 +enum {
 +	IORES_MAP_SYSTEM_RAM		= BIT(0),
 +	IORES_MAP_ENCRYPTED		= BIT(1),
++=======
++>>>>>>> 25b2995a35b6 (mm: remove MEMORY_DEVICE_PUBLIC support)
  };
  
  /* helpers to define resources */
diff --cc mm/hmm.c
index 91b885757871,376159a769fb..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -1094,138 -1478,4 +1094,142 @@@ struct hmm_devmem *hmm_devmem_add(cons
  	return devmem;
  }
  EXPORT_SYMBOL_GPL(hmm_devmem_add);
++<<<<<<< HEAD
 +
 +struct hmm_devmem *hmm_devmem_add_resource(const struct hmm_devmem_ops *ops,
 +					   struct device *device,
 +					   struct resource *res)
 +{
 +	struct hmm_devmem *devmem;
 +	void *result;
 +	int ret;
 +
 +	if (res->desc != IORES_DESC_DEVICE_PUBLIC_MEMORY)
 +		return ERR_PTR(-EINVAL);
 +
 +	dev_pagemap_get_ops();
 +
 +	devmem = devm_kzalloc(device, sizeof(*devmem), GFP_KERNEL);
 +	if (!devmem)
 +		return ERR_PTR(-ENOMEM);
 +
 +	init_completion(&devmem->completion);
 +	devmem->pfn_first = -1UL;
 +	devmem->pfn_last = -1UL;
 +	devmem->resource = res;
 +	devmem->device = device;
 +	devmem->ops = ops;
 +
 +	ret = percpu_ref_init(&devmem->ref, &hmm_devmem_ref_release,
 +			      0, GFP_KERNEL);
 +	if (ret)
 +		return ERR_PTR(ret);
 +
 +	ret = devm_add_action_or_reset(device, hmm_devmem_ref_exit,
 +			&devmem->ref);
 +	if (ret)
 +		return ERR_PTR(ret);
 +
 +	devmem->pfn_first = devmem->resource->start >> PAGE_SHIFT;
 +	devmem->pfn_last = devmem->pfn_first +
 +			   (resource_size(devmem->resource) >> PAGE_SHIFT);
 +
 +	devmem->pagemap.type = MEMORY_DEVICE_PUBLIC;
 +	devmem->pagemap.res = *devmem->resource;
 +	devmem->pagemap.page_fault = hmm_devmem_fault;
 +	devmem->pagemap.page_free = hmm_devmem_free;
 +	devmem->pagemap.altmap_valid = false;
 +	devmem->pagemap.ref = &devmem->ref;
 +	devmem->pagemap.data = devmem;
 +	devmem->pagemap.kill = hmm_devmem_ref_kill;
 +
 +	result = devm_memremap_pages(devmem->device, &devmem->pagemap);
 +	if (IS_ERR(result))
 +		return result;
 +	return devmem;
 +}
 +EXPORT_SYMBOL_GPL(hmm_devmem_add_resource);
 +
 +/*
 + * A device driver that wants to handle multiple devices memory through a
 + * single fake device can use hmm_device to do so. This is purely a helper
 + * and it is not needed to make use of any HMM functionality.
 + */
 +#define HMM_DEVICE_MAX 256
 +
 +static DECLARE_BITMAP(hmm_device_mask, HMM_DEVICE_MAX);
 +static DEFINE_SPINLOCK(hmm_device_lock);
 +static struct class *hmm_device_class;
 +static dev_t hmm_device_devt;
 +
 +static void hmm_device_release(struct device *device)
 +{
 +	struct hmm_device *hmm_device;
 +
 +	hmm_device = container_of(device, struct hmm_device, device);
 +	spin_lock(&hmm_device_lock);
 +	clear_bit(hmm_device->minor, hmm_device_mask);
 +	spin_unlock(&hmm_device_lock);
 +
 +	kfree(hmm_device);
 +}
 +
 +struct hmm_device *hmm_device_new(void *drvdata)
 +{
 +	struct hmm_device *hmm_device;
 +
 +	hmm_device = kzalloc(sizeof(*hmm_device), GFP_KERNEL);
 +	if (!hmm_device)
 +		return ERR_PTR(-ENOMEM);
 +
 +	spin_lock(&hmm_device_lock);
 +	hmm_device->minor = find_first_zero_bit(hmm_device_mask, HMM_DEVICE_MAX);
 +	if (hmm_device->minor >= HMM_DEVICE_MAX) {
 +		spin_unlock(&hmm_device_lock);
 +		kfree(hmm_device);
 +		return ERR_PTR(-EBUSY);
 +	}
 +	set_bit(hmm_device->minor, hmm_device_mask);
 +	spin_unlock(&hmm_device_lock);
 +
 +	dev_set_name(&hmm_device->device, "hmm_device%d", hmm_device->minor);
 +	hmm_device->device.devt = MKDEV(MAJOR(hmm_device_devt),
 +					hmm_device->minor);
 +	hmm_device->device.release = hmm_device_release;
 +	dev_set_drvdata(&hmm_device->device, drvdata);
 +	hmm_device->device.class = hmm_device_class;
 +	device_initialize(&hmm_device->device);
 +
 +	return hmm_device;
 +}
 +EXPORT_SYMBOL(hmm_device_new);
 +
 +void hmm_device_put(struct hmm_device *hmm_device)
 +{
 +	put_device(&hmm_device->device);
 +}
 +EXPORT_SYMBOL(hmm_device_put);
 +
 +static int __init hmm_init(void)
 +{
 +	int ret;
 +
 +	ret = alloc_chrdev_region(&hmm_device_devt, 0,
 +				  HMM_DEVICE_MAX,
 +				  "hmm_device");
 +	if (ret)
 +		return ret;
 +
 +	hmm_device_class = class_create(THIS_MODULE, "hmm_device");
 +	if (IS_ERR(hmm_device_class)) {
 +		unregister_chrdev_region(hmm_device_devt, HMM_DEVICE_MAX);
 +		return PTR_ERR(hmm_device_class);
 +	}
 +	return 0;
 +}
 +
 +device_initcall(hmm_init);
 +#endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
++=======
+ #endif /* CONFIG_DEVICE_PRIVATE  */
++>>>>>>> 25b2995a35b6 (mm: remove MEMORY_DEVICE_PUBLIC support)
diff --cc mm/migrate.c
index d35a94faeed0,78d45e184457..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -372,55 -370,20 +370,65 @@@ unlock
  }
  #endif
  
 -static int expected_page_refs(struct address_space *mapping, struct page *page)
 +#ifdef CONFIG_BLOCK
 +/* Returns true if all buffers are successfully locked */
 +static bool buffer_migrate_lock_buffers(struct buffer_head *head,
 +							enum migrate_mode mode)
  {
 -	int expected_count = 1;
 +	struct buffer_head *bh = head;
  
++<<<<<<< HEAD
 +	/* Simple case, sync compaction */
 +	if (mode != MIGRATE_ASYNC) {
 +		do {
 +			get_bh(bh);
 +			lock_buffer(bh);
 +			bh = bh->b_this_page;
++=======
+ 	/*
+ 	 * Device public or private pages have an extra refcount as they are
+ 	 * ZONE_DEVICE pages.
+ 	 */
+ 	expected_count += is_device_private_page(page);
+ 	if (mapping)
+ 		expected_count += hpage_nr_pages(page) + page_has_private(page);
++>>>>>>> 25b2995a35b6 (mm: remove MEMORY_DEVICE_PUBLIC support)
  
 -	return expected_count;
 +		} while (bh != head);
 +
 +		return true;
 +	}
 +
 +	/* async case, we cannot block on lock_buffer so use trylock_buffer */
 +	do {
 +		get_bh(bh);
 +		if (!trylock_buffer(bh)) {
 +			/*
 +			 * We failed to lock the buffer and cannot stall in
 +			 * async migration. Release the taken locks
 +			 */
 +			struct buffer_head *failed_bh = bh;
 +			put_bh(failed_bh);
 +			bh = head;
 +			while (bh != failed_bh) {
 +				unlock_buffer(bh);
 +				put_bh(bh);
 +				bh = bh->b_this_page;
 +			}
 +			return false;
 +		}
 +
 +		bh = bh->b_this_page;
 +	} while (bh != head);
 +	return true;
 +}
 +#else
 +static inline bool buffer_migrate_lock_buffers(struct buffer_head *head,
 +							enum migrate_mode mode)
 +{
 +	return true;
  }
 +#endif /* CONFIG_BLOCK */
  
  /*
   * Replace the page in the mapping.
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 8ea06abc73be..85cefc505c86 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -1272,7 +1272,7 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,
 		if (pm->show_pfn)
 			frame = pte_pfn(pte);
 		flags |= PM_PRESENT;
-		page = _vm_normal_page(vma, addr, pte, true);
+		page = vm_normal_page(vma, addr, pte);
 		if (pte_soft_dirty(pte))
 			flags |= PM_SOFT_DIRTY;
 	} else if (is_swap_pte(pte)) {
* Unmerged path include/linux/hmm.h
* Unmerged path include/linux/ioport.h
diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index ae6713454b27..824eb2fbab5a 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -40,13 +40,6 @@ struct vmem_altmap {
  * A more complete discussion of unaddressable memory may be found in
  * include/linux/hmm.h and Documentation/vm/hmm.rst.
  *
- * MEMORY_DEVICE_PUBLIC:
- * Device memory that is cache coherent from device and CPU point of view. This
- * is use on platform that have an advance system bus (like CAPI or CCIX). A
- * driver can hotplug the device memory using ZONE_DEVICE and with that memory
- * type. Any page of a process can be migrated to such memory. However no one
- * should be allow to pin such memory so that it can always be evicted.
- *
  * MEMORY_DEVICE_FS_DAX:
  * Host memory that has similar access semantics as System RAM i.e. DMA
  * coherent and supports page pinning. In support of coordinating page
@@ -61,7 +54,6 @@ struct vmem_altmap {
  */
 enum memory_type {
 	MEMORY_DEVICE_PRIVATE = 1,
-	MEMORY_DEVICE_PUBLIC,
 	MEMORY_DEVICE_FS_DAX,
 	MEMORY_DEVICE_PCI_P2PDMA,
 };
diff --git a/include/linux/mm.h b/include/linux/mm.h
index fc33f44a60f1..25f4fedb2d41 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -876,7 +876,6 @@ static inline bool put_devmap_managed_page(struct page *page)
 		return false;
 	switch (page->pgmap->type) {
 	case MEMORY_DEVICE_PRIVATE:
-	case MEMORY_DEVICE_PUBLIC:
 	case MEMORY_DEVICE_FS_DAX:
 		__put_devmap_managed_page(page);
 		return true;
@@ -892,12 +891,6 @@ static inline bool is_device_private_page(const struct page *page)
 		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
 }
 
-static inline bool is_device_public_page(const struct page *page)
-{
-	return is_zone_device_page(page) &&
-		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
-}
-
 #ifdef CONFIG_PCI_P2PDMA
 static inline bool is_pci_p2pdma_page(const struct page *page)
 {
@@ -930,11 +923,6 @@ static inline bool is_device_private_page(const struct page *page)
 	return false;
 }
 
-static inline bool is_device_public_page(const struct page *page)
-{
-	return false;
-}
-
 static inline bool is_pci_p2pdma_page(const struct page *page)
 {
 	return false;
@@ -1346,10 +1334,8 @@ struct zap_details {
 	pgoff_t last_index;			/* Highest page->index to unmap */
 };
 
-struct page *_vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
-			     pte_t pte, bool with_public_device);
-#define vm_normal_page(vma, addr, pte) _vm_normal_page(vma, addr, pte, false)
-
+struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
+			     pte_t pte);
 struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
 				pmd_t pmd);
 
diff --git a/mm/Kconfig b/mm/Kconfig
index cf4b52aa39e1..f5f6971b5883 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -721,17 +721,6 @@ config DEVICE_PRIVATE
 	  memory; i.e., memory that is only accessible from the device (or
 	  group of devices). You likely also want to select HMM_MIRROR.
 
-config DEVICE_PUBLIC
-	bool "Addressable device memory (like GPU memory)"
-	depends on ARCH_HAS_HMM
-	select HMM
-	select DEV_PAGEMAP_OPS
-
-	help
-	  Allows creation of struct pages to represent addressable device
-	  memory; i.e., memory that is accessible from both the device and
-	  the CPU
-
 config FRAME_VECTOR
 	bool
 
diff --git a/mm/gup.c b/mm/gup.c
index 538b120f2280..23222b3387c0 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -604,13 +604,6 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,
 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
 			goto unmap;
 		*page = pte_page(*pte);
-
-		/*
-		 * This should never happen (a device public page in the gate
-		 * area).
-		 */
-		if (is_device_public_page(*page))
-			goto unmap;
 	}
 	if (unlikely(!try_get_page(*page))) {
 		ret = -ENOMEM;
* Unmerged path mm/hmm.c
diff --git a/mm/madvise.c b/mm/madvise.c
index 6cb1ca93e290..4f76df2dbfb5 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -354,7 +354,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 			continue;
 		}
 
-		page = _vm_normal_page(vma, addr, ptent, true);
+		page = vm_normal_page(vma, addr, ptent);
 		if (!page)
 			continue;
 
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 9ff06aed2718..8bd333d97910 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -4721,7 +4721,7 @@ enum mc_target_type {
 static struct page *mc_handle_present_pte(struct vm_area_struct *vma,
 						unsigned long addr, pte_t ptent)
 {
-	struct page *page = _vm_normal_page(vma, addr, ptent, true);
+	struct page *page = vm_normal_page(vma, addr, ptent);
 
 	if (!page || !page_mapped(page))
 		return NULL;
@@ -4922,8 +4922,8 @@ static int mem_cgroup_move_account(struct page *page,
  *   2(MC_TARGET_SWAP): if the swap entry corresponding to this pte is a
  *     target for charge migration. if @target is not NULL, the entry is stored
  *     in target->ent.
- *   3(MC_TARGET_DEVICE): like MC_TARGET_PAGE  but page is MEMORY_DEVICE_PUBLIC
- *     or MEMORY_DEVICE_PRIVATE (so ZONE_DEVICE page and thus not on the lru).
+ *   3(MC_TARGET_DEVICE): like MC_TARGET_PAGE  but page is MEMORY_DEVICE_PRIVATE
+ *     (so ZONE_DEVICE page and thus not on the lru).
  *     For now we such page is charge like a regular page would be as for all
  *     intent and purposes it is just special memory taking the place of a
  *     regular page.
@@ -4957,8 +4957,7 @@ static enum mc_target_type get_mctgt_type(struct vm_area_struct *vma,
 		 */
 		if (page->mem_cgroup == mc.from) {
 			ret = MC_TARGET_PAGE;
-			if (is_device_private_page(page) ||
-			    is_device_public_page(page))
+			if (is_device_private_page(page))
 				ret = MC_TARGET_DEVICE;
 			if (target)
 				target->page = page;
@@ -5029,8 +5028,8 @@ static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,
 	if (ptl) {
 		/*
 		 * Note their can not be MC_TARGET_DEVICE for now as we do not
-		 * support transparent huge page with MEMORY_DEVICE_PUBLIC or
-		 * MEMORY_DEVICE_PRIVATE but this might change.
+		 * support transparent huge page with MEMORY_DEVICE_PRIVATE but
+		 * this might change.
 		 */
 		if (get_mctgt_type_thp(vma, addr, *pmd, NULL) == MC_TARGET_PAGE)
 			mc.precharge += HPAGE_PMD_NR;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index f5ec8e5dc464..1a8f1471c642 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -1177,16 +1177,12 @@ static int memory_failure_dev_pagemap(unsigned long pfn, int flags,
 		goto unlock;
 	}
 
-	switch (pgmap->type) {
-	case MEMORY_DEVICE_PRIVATE:
-	case MEMORY_DEVICE_PUBLIC:
+	if (pgmap->type == MEMORY_DEVICE_PRIVATE) {
 		/*
 		 * TODO: Handle HMM pages which may need coordination
 		 * with device-side memory.
 		 */
 		goto unlock;
-	default:
-		break;
 	}
 
 	/*
diff --git a/mm/memory.c b/mm/memory.c
index 041d24985cf6..e4afe86e7085 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -816,8 +816,8 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
  * PFNMAP mappings in order to support COWable mappings.
  *
  */
-struct page *_vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
-			     pte_t pte, bool with_public_device)
+struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
+			    pte_t pte)
 {
 	unsigned long pfn = pte_pfn(pte);
 
@@ -830,29 +830,6 @@ struct page *_vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 			return NULL;
 		if (is_zero_pfn(pfn))
 			return NULL;
-
-		/*
-		 * Device public pages are special pages (they are ZONE_DEVICE
-		 * pages but different from persistent memory). They behave
-		 * allmost like normal pages. The difference is that they are
-		 * not on the lru and thus should never be involve with any-
-		 * thing that involve lru manipulation (mlock, numa balancing,
-		 * ...).
-		 *
-		 * This is why we still want to return NULL for such page from
-		 * vm_normal_page() so that we do not have to special case all
-		 * call site of vm_normal_page().
-		 */
-		if (likely(pfn <= highest_memmap_pfn)) {
-			struct page *page = pfn_to_page(pfn);
-
-			if (is_device_public_page(page)) {
-				if (with_public_device)
-					return page;
-				return NULL;
-			}
-		}
-
 		if (pte_devmap(pte))
 			return NULL;
 
@@ -1042,17 +1019,6 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		rss[mm_counter(page)]++;
 	} else if (pte_devmap(pte)) {
 		page = pte_page(pte);
-
-		/*
-		 * Cache coherent device memory behave like regular page and
-		 * not like persistent memory page. For more informations see
-		 * MEMORY_DEVICE_CACHE_COHERENT in memory_hotplug.h
-		 */
-		if (is_device_public_page(page)) {
-			get_page(page);
-			page_dup_rmap(page, false);
-			rss[mm_counter(page)]++;
-		}
 	}
 
 out_set_pte:
@@ -1308,7 +1274,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 		if (pte_present(ptent)) {
 			struct page *page;
 
-			page = _vm_normal_page(vma, addr, ptent, true);
+			page = vm_normal_page(vma, addr, ptent);
 			if (unlikely(details) && page) {
 				/*
 				 * unmap_shared_mapping_pages() wants to
* Unmerged path mm/migrate.c
diff --git a/mm/swap.c b/mm/swap.c
index 586bc0cfcd75..f2098f8772b3 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -736,17 +736,6 @@ void release_pages(struct page **pages, int nr)
 		if (is_huge_zero_page(page))
 			continue;
 
-		/* Device public page can not be huge page */
-		if (is_device_public_page(page)) {
-			if (locked_pgdat) {
-				spin_unlock_irqrestore(&locked_pgdat->lru_lock,
-						       flags);
-				locked_pgdat = NULL;
-			}
-			put_devmap_managed_page(page);
-			continue;
-		}
-
 		page = compound_head(page);
 		if (!put_page_testzero(page))
 			continue;
