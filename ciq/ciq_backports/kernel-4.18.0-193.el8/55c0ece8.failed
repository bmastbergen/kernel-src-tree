mm/hmm: add a helper function that fault pages and map them to a device

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] hmm: add a helper function that fault pages and map them to a device (Jerome Glisse) [1498655 1597758]
Rebuild_FUZZ: 97.84%
commit-author Jérôme Glisse <jglisse@redhat.com>
commit 55c0ece82ac6ad018a71465d332847dce023eeb3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/55c0ece8.failed

This is a all in one helper that fault pages in a range and map them to a
device so that every single device driver do not have to re-implement this
common pattern.

This is taken from ODP RDMA in preparation of ODP RDMA convertion.  It
will be use by nouveau and other drivers.

[jglisse@redhat.com: Was using wrong field and wrong enum]
  Link: http://lkml.kernel.org/r/20190409175340.26614-1-jglisse@redhat.com
Link: http://lkml.kernel.org/r/20190403193318.16478-12-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Cc: Ralph Campbell <rcampbell@nvidia.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 55c0ece82ac6ad018a71465d332847dce023eeb3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index 2f68a486cc0d,f81fe2c0f343..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -378,41 -464,87 +378,61 @@@ static inline bool hmm_mirror_mm_is_ali
  
  
  /*
 - * Please see Documentation/vm/hmm.rst for how to use the range API.
 + * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device
 + * driver lock that serializes device page table updates, then call
 + * hmm_vma_range_done(), to check if the snapshot is still valid. The same
 + * device driver page table update lock must also be used in the
 + * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page
 + * table invalidation serializes on it.
 + *
 + * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL
 + * hmm_vma_get_pfns() WITHOUT ERROR !
 + *
 + * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !
   */
++<<<<<<< HEAD
 +int hmm_vma_get_pfns(struct hmm_range *range);
 +bool hmm_vma_range_done(struct hmm_range *range);
 +
++=======
+ int hmm_range_register(struct hmm_range *range,
+ 		       struct mm_struct *mm,
+ 		       unsigned long start,
+ 		       unsigned long end,
+ 		       unsigned page_shift);
+ void hmm_range_unregister(struct hmm_range *range);
+ long hmm_range_snapshot(struct hmm_range *range);
+ long hmm_range_fault(struct hmm_range *range, bool block);
+ long hmm_range_dma_map(struct hmm_range *range,
+ 		       struct device *device,
+ 		       dma_addr_t *daddrs,
+ 		       bool block);
+ long hmm_range_dma_unmap(struct hmm_range *range,
+ 			 struct vm_area_struct *vma,
+ 			 struct device *device,
+ 			 dma_addr_t *daddrs,
+ 			 bool dirty);
++>>>>>>> 55c0ece82ac6 (mm/hmm: add a helper function that fault pages and map them to a device)
  
  /*
 - * HMM_RANGE_DEFAULT_TIMEOUT - default timeout (ms) when waiting for a range
 + * Fault memory on behalf of device driver. Unlike handle_mm_fault(), this will
 + * not migrate any device memory back to system memory. The HMM pfn array will
 + * be updated with the fault result and current snapshot of the CPU page table
 + * for the range.
 + *
 + * The mmap_sem must be taken in read mode before entering and it might be
 + * dropped by the function if the block argument is false. In that case, the
 + * function returns -EAGAIN.
   *
 - * When waiting for mmu notifiers we need some kind of time out otherwise we
 - * could potentialy wait for ever, 1000ms ie 1s sounds like a long time to
 - * wait already.
 + * Return value does not reflect if the fault was successful for every single
 + * address or not. Therefore, the caller must to inspect the HMM pfn array to
 + * determine fault status for each address.
 + *
 + * Trying to fault inside an invalid vma will result in -EINVAL.
 + *
 + * See the function description in mm/hmm.c for further documentation.
   */
 -#define HMM_RANGE_DEFAULT_TIMEOUT 1000
 -
 -/* This is a temporary helper to avoid merge conflict between trees. */
 -static inline bool hmm_vma_range_done(struct hmm_range *range)
 -{
 -	bool ret = hmm_range_valid(range);
 -
 -	hmm_range_unregister(range);
 -	return ret;
 -}
 -
 -/* This is a temporary helper to avoid merge conflict between trees. */
 -static inline int hmm_vma_fault(struct hmm_range *range, bool block)
 -{
 -	long ret;
 -
 -	/*
 -	 * With the old API the driver must set each individual entries with
 -	 * the requested flags (valid, write, ...). So here we set the mask to
 -	 * keep intact the entries provided by the driver and zero out the
 -	 * default_flags.
 -	 */
 -	range->default_flags = 0;
 -	range->pfn_flags_mask = -1UL;
 -
 -	ret = hmm_range_register(range, range->vma->vm_mm,
 -				 range->start, range->end,
 -				 PAGE_SHIFT);
 -	if (ret)
 -		return (int)ret;
 -
 -	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
 -		/*
 -		 * The mmap_sem was taken by driver we release it here and
 -		 * returns -EAGAIN which correspond to mmap_sem have been
 -		 * drop in the old API.
 -		 */
 -		up_read(&range->vma->vm_mm->mmap_sem);
 -		return -EAGAIN;
 -	}
 -
 -	ret = hmm_range_fault(range, block);
 -	if (ret <= 0) {
 -		if (ret == -EBUSY || !ret) {
 -			/* Same as above  drop mmap_sem to match old API. */
 -			up_read(&range->vma->vm_mm->mmap_sem);
 -			ret = -EBUSY;
 -		} else if (ret == -EAGAIN)
 -			ret = -EBUSY;
 -		hmm_range_unregister(range);
 -		return ret;
 -	}
 -	return 0;
 -}
 +int hmm_vma_fault(struct hmm_range *range, bool block);
  
  /* Below are for HMM internal use only! Not to be used by device driver! */
  void hmm_mm_destroy(struct mm_struct *mm);
diff --cc mm/hmm.c
index 4c052ccc4e21,95fa7abb9d67..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -828,120 -1095,245 +829,275 @@@ EXPORT_SYMBOL(hmm_vma_range_done)
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
 + *
 + * Expected use pattern:
 + * retry:
 + *   down_read(&mm->mmap_sem);
 + *   // Find vma and address device wants to fault, initialize hmm_pfn_t
 + *   // array accordingly
 + *   ret = hmm_vma_fault(range, write, block);
 + *   switch (ret) {
 + *   case -EAGAIN:
 + *     hmm_vma_range_done(range);
 + *     // You might want to rate limit or yield to play nicely, you may
 + *     // also commit any valid pfn in the array assuming that you are
 + *     // getting true from hmm_vma_range_monitor_end()
 + *     goto retry;
 + *   case 0:
 + *     break;
 + *   case -ENOMEM:
 + *   case -EINVAL:
 + *   case -EPERM:
 + *   default:
 + *     // Handle error !
 + *     up_read(&mm->mmap_sem)
 + *     return;
 + *   }
 + *   // Take device driver lock that serialize device page table update
 + *   driver_lock_device_page_table_update();
 + *   hmm_vma_range_done(range);
 + *   // Commit pfns we got from hmm_vma_fault()
 + *   driver_unlock_device_page_table_update();
 + *   up_read(&mm->mmap_sem)
 + *
 + * YOU MUST CALL hmm_vma_range_done() AFTER THIS FUNCTION RETURN SUCCESS (0)
 + * BEFORE FREEING THE range struct OR YOU WILL HAVE SERIOUS MEMORY CORRUPTION !
 + *
 + * YOU HAVE BEEN WARNED !
   */
 -long hmm_range_fault(struct hmm_range *range, bool block)
 +int hmm_vma_fault(struct hmm_range *range, bool block)
  {
 -	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 -	unsigned long start = range->start, end;
 +	struct vm_area_struct *vma = range->vma;
 +	unsigned long start = range->start;
  	struct hmm_vma_walk hmm_vma_walk;
 -	struct hmm *hmm = range->hmm;
 -	struct vm_area_struct *vma;
  	struct mm_walk mm_walk;
 +	struct hmm *hmm;
  	int ret;
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid) {
 -			up_read(&hmm->mm->mmap_sem);
 -			return -EAGAIN;
 -		}
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm) {
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -ENOMEM;
 +	}
 +	/* Caller must have registered a mirror using hmm_mirror_register() */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
  
 -		vma = find_vma(hmm->mm, start);
 -		if (vma == NULL || (vma->vm_flags & device_vma))
 -			return -EFAULT;
 -
 -		if (is_vm_hugetlb_page(vma)) {
 -			if (huge_page_shift(hstate_vma(vma)) !=
 -			    range->page_shift &&
 -			    range->page_shift != PAGE_SHIFT)
 -				return -EINVAL;
 -		} else {
 -			if (range->page_shift != PAGE_SHIFT)
 -				return -EINVAL;
 -		}
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
  
 -		if (!(vma->vm_flags & VM_READ)) {
 -			/*
 -			 * If vma do not allow read access, then assume that it
 -			 * does not allow write access, either. HMM does not
 -			 * support architecture that allow write without read.
 -			 */
 -			hmm_pfns_clear(range, range->pfns,
 -				range->start, range->end);
 -			return -EPERM;
 -		}
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
 +	}
  
 -		range->vma = vma;
 -		hmm_vma_walk.pgmap = NULL;
 -		hmm_vma_walk.last = start;
 -		hmm_vma_walk.fault = true;
 -		hmm_vma_walk.block = block;
 -		hmm_vma_walk.range = range;
 -		mm_walk.private = &hmm_vma_walk;
 -		end = min(range->end, vma->vm_end);
 -
 -		mm_walk.vma = vma;
 -		mm_walk.mm = vma->vm_mm;
 -		mm_walk.pte_entry = NULL;
 -		mm_walk.test_walk = NULL;
 -		mm_walk.hugetlb_entry = NULL;
 -		mm_walk.pud_entry = hmm_vma_walk_pud;
 -		mm_walk.pmd_entry = hmm_vma_walk_pmd;
 -		mm_walk.pte_hole = hmm_vma_walk_hole;
 -		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
 -
 -		do {
 -			ret = walk_page_range(start, end, &mm_walk);
 -			start = hmm_vma_walk.last;
 -
 -			/* Keep trying while the range is valid. */
 -		} while (ret == -EBUSY && range->valid);
 -
 -		if (ret) {
 -			unsigned long i;
 -
 -			i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 -			hmm_pfns_clear(range, &range->pfns[i],
 -				hmm_vma_walk.last, range->end);
 -			return ret;
 -		}
 -		start = end;
 +	/* Initialize range to track CPU page table update */
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = true;
 +	hmm_vma_walk.block = block;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +	hmm_vma_walk.last = range->start;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
  
 -	} while (start < range->end);
 +	do {
 +		ret = walk_page_range(start, range->end, &mm_walk);
 +		start = hmm_vma_walk.last;
 +	} while (ret == -EAGAIN);
 +
 +	if (ret) {
 +		unsigned long i;
  
 -	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		hmm_pfns_clear(range, &range->pfns[i], hmm_vma_walk.last,
 +			       range->end);
 +		hmm_vma_range_done(range);
 +	}
 +	return ret;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(hmm_vma_fault);
++=======
+ EXPORT_SYMBOL(hmm_range_fault);
+ 
+ /**
+  * hmm_range_dma_map() - hmm_range_fault() and dma map page all in one.
+  * @range: range being faulted
+  * @device: device against to dma map page to
+  * @daddrs: dma address of mapped pages
+  * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
+  * Returns: number of pages mapped on success, -EAGAIN if mmap_sem have been
+  *          drop and you need to try again, some other error value otherwise
+  *
+  * Note same usage pattern as hmm_range_fault().
+  */
+ long hmm_range_dma_map(struct hmm_range *range,
+ 		       struct device *device,
+ 		       dma_addr_t *daddrs,
+ 		       bool block)
+ {
+ 	unsigned long i, npages, mapped;
+ 	long ret;
+ 
+ 	ret = hmm_range_fault(range, block);
+ 	if (ret <= 0)
+ 		return ret ? ret : -EBUSY;
+ 
+ 	npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	for (i = 0, mapped = 0; i < npages; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		/*
+ 		 * FIXME need to update DMA API to provide invalid DMA address
+ 		 * value instead of a function to test dma address value. This
+ 		 * would remove lot of dumb code duplicated accross many arch.
+ 		 *
+ 		 * For now setting it to 0 here is good enough as the pfns[]
+ 		 * value is what is use to check what is valid and what isn't.
+ 		 */
+ 		daddrs[i] = 0;
+ 
+ 		page = hmm_pfn_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		/* Check if range is being invalidated */
+ 		if (!range->valid) {
+ 			ret = -EBUSY;
+ 			goto unmap;
+ 		}
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE])
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 		daddrs[i] = dma_map_page(device, page, 0, PAGE_SIZE, dir);
+ 		if (dma_mapping_error(device, daddrs[i])) {
+ 			ret = -EFAULT;
+ 			goto unmap;
+ 		}
+ 
+ 		mapped++;
+ 	}
+ 
+ 	return mapped;
+ 
+ unmap:
+ 	for (npages = i, i = 0; (i < npages) && mapped; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		page = hmm_pfn_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		if (dma_mapping_error(device, daddrs[i]))
+ 			continue;
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE])
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 		dma_unmap_page(device, daddrs[i], PAGE_SIZE, dir);
+ 		mapped--;
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL(hmm_range_dma_map);
+ 
+ /**
+  * hmm_range_dma_unmap() - unmap range of that was map with hmm_range_dma_map()
+  * @range: range being unmapped
+  * @vma: the vma against which the range (optional)
+  * @device: device against which dma map was done
+  * @daddrs: dma address of mapped pages
+  * @dirty: dirty page if it had the write flag set
+  * Returns: number of page unmapped on success, -EINVAL otherwise
+  *
+  * Note that caller MUST abide by mmu notifier or use HMM mirror and abide
+  * to the sync_cpu_device_pagetables() callback so that it is safe here to
+  * call set_page_dirty(). Caller must also take appropriate locks to avoid
+  * concurrent mmu notifier or sync_cpu_device_pagetables() to make progress.
+  */
+ long hmm_range_dma_unmap(struct hmm_range *range,
+ 			 struct vm_area_struct *vma,
+ 			 struct device *device,
+ 			 dma_addr_t *daddrs,
+ 			 bool dirty)
+ {
+ 	unsigned long i, npages;
+ 	long cpages = 0;
+ 
+ 	/* Sanity check. */
+ 	if (range->end <= range->start)
+ 		return -EINVAL;
+ 	if (!daddrs)
+ 		return -EINVAL;
+ 	if (!range->pfns)
+ 		return -EINVAL;
+ 
+ 	npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	for (i = 0; i < npages; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		page = hmm_pfn_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE]) {
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 			/*
+ 			 * See comments in function description on why it is
+ 			 * safe here to call set_page_dirty()
+ 			 */
+ 			if (dirty)
+ 				set_page_dirty(page);
+ 		}
+ 
+ 		/* Unmap and clear pfns/dma address */
+ 		dma_unmap_page(device, daddrs[i], PAGE_SIZE, dir);
+ 		range->pfns[i] = range->values[HMM_PFN_NONE];
+ 		/* FIXME see comments in hmm_vma_dma_map() */
+ 		daddrs[i] = 0;
+ 		cpages++;
+ 	}
+ 
+ 	return cpages;
+ }
+ EXPORT_SYMBOL(hmm_range_dma_unmap);
++>>>>>>> 55c0ece82ac6 (mm/hmm: add a helper function that fault pages and map them to a device)
  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
  
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
