memremap: lift the devmap_enable manipulation into devm_memremap_pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Christoph Hellwig <hch@lst.de>
commit f6a55e1a3fe6b3bb294a80a05437fcf86488d819
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f6a55e1a.failed

Just check if there is a ->page_free operation set and take care of the
static key enable, as well as the put using device managed resources.
Also check that a ->page_free is provided for the pgmaps types that
require it, and check for a valid type as well while we are at it.

Note that this also fixes the fact that hmm never called
dev_pagemap_put_ops and thus would leave the slow path enabled forever,
even after a device driver unload or disable.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Tested-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit f6a55e1a3fe6b3bb294a80a05437fcf86488d819)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvdimm/pmem.c
#	kernel/memremap.c
diff --cc drivers/nvdimm/pmem.c
index d9d845077b8b,48767171a4df..000000000000
--- a/drivers/nvdimm/pmem.c
+++ b/drivers/nvdimm/pmem.c
@@@ -334,26 -334,16 +334,38 @@@ static void pmem_release_disk(void *__p
  	put_disk(pmem->disk);
  }
  
++<<<<<<< HEAD
 +static void pmem_release_pgmap_ops(void *__pgmap)
 +{
 +	dev_pagemap_put_ops();
 +}
 +
 +static void fsdax_pagefree(struct page *page, void *data)
++=======
+ static void pmem_pagemap_page_free(struct page *page, void *data)
++>>>>>>> f6a55e1a3fe6 (memremap: lift the devmap_enable manipulation into devm_memremap_pages)
  {
  	wake_up_var(&page->_refcount);
  }
  
++<<<<<<< HEAD
 +static int setup_pagemap_fsdax(struct device *dev, struct dev_pagemap *pgmap)
 +{
 +	dev_pagemap_get_ops();
 +	if (devm_add_action_or_reset(dev, pmem_release_pgmap_ops, pgmap))
 +		return -ENOMEM;
 +	pgmap->type = MEMORY_DEVICE_FS_DAX;
 +	pgmap->page_free = fsdax_pagefree;
 +
 +	return 0;
 +}
++=======
+ static const struct dev_pagemap_ops fsdax_pagemap_ops = {
+ 	.page_free		= pmem_pagemap_page_free,
+ 	.kill			= pmem_pagemap_kill,
+ 	.cleanup		= pmem_pagemap_cleanup,
+ };
++>>>>>>> f6a55e1a3fe6 (memremap: lift the devmap_enable manipulation into devm_memremap_pages)
  
  static int pmem_attach_disk(struct device *dev,
  		struct nd_namespace_common *ndns)
@@@ -407,15 -397,11 +419,15 @@@
  	if (!q)
  		return -ENOMEM;
  
 +	if (devm_add_action_or_reset(dev, pmem_release_queue, q))
 +		return -ENOMEM;
 +
  	pmem->pfn_flags = PFN_DEV;
  	pmem->pgmap.ref = &q->q_usage_counter;
 +	pmem->pgmap.kill = pmem_freeze_queue;
  	if (is_nd_pfn(dev)) {
- 		if (setup_pagemap_fsdax(dev, &pmem->pgmap))
- 			return -ENOMEM;
+ 		pmem->pgmap.type = MEMORY_DEVICE_FS_DAX;
+ 		pmem->pgmap.ops = &fsdax_pagemap_ops;
  		addr = devm_memremap_pages(dev, &pmem->pgmap);
  		pfn_sb = nd_pfn->pfn_sb;
  		pmem->data_offset = le64_to_cpu(pfn_sb->dataoff);
diff --cc kernel/memremap.c
index 794888559eb7,3219a4c91d07..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -17,33 -17,37 +17,64 @@@ static RADIX_TREE(pgmap_radix, GFP_KERN
  #define SECTION_MASK ~((1UL << PA_SECTION_SHIFT) - 1)
  #define SECTION_SIZE (1UL << PA_SECTION_SHIFT)
  
++<<<<<<< HEAD
 +static unsigned long order_at(struct resource *res, unsigned long pgoff)
 +{
 +	unsigned long phys_pgoff = PHYS_PFN(res->start) + pgoff;
 +	unsigned long nr_pages, mask;
 +
 +	nr_pages = PHYS_PFN(resource_size(res));
 +	if (nr_pages == pgoff)
 +		return ULONG_MAX;
 +
 +	/*
 +	 * What is the largest aligned power-of-2 range available from
 +	 * this resource pgoff to the end of the resource range,
 +	 * considering the alignment of the current pgoff?
 +	 */
 +	mask = phys_pgoff | rounddown_pow_of_two(nr_pages - pgoff);
 +	if (!mask)
 +		return ULONG_MAX;
 +
 +	return find_first_bit(&mask, BITS_PER_LONG);
 +}
 +
 +#define foreach_order_pgoff(res, order, pgoff) \
 +	for (pgoff = 0, order = order_at((res), pgoff); order < ULONG_MAX; \
 +			pgoff += 1UL << order, order = order_at((res), pgoff))
++=======
+ #ifdef CONFIG_DEV_PAGEMAP_OPS
+ DEFINE_STATIC_KEY_FALSE(devmap_managed_key);
+ EXPORT_SYMBOL(devmap_managed_key);
+ static atomic_t devmap_managed_enable;
+ 
+ static void devmap_managed_enable_put(void *data)
+ {
+ 	if (atomic_dec_and_test(&devmap_managed_enable))
+ 		static_branch_disable(&devmap_managed_key);
+ }
+ 
+ static int devmap_managed_enable_get(struct device *dev, struct dev_pagemap *pgmap)
+ {
+ 	if (!pgmap->ops->page_free) {
+ 		WARN(1, "Missing page_free method\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (atomic_inc_return(&devmap_managed_enable) == 1)
+ 		static_branch_enable(&devmap_managed_key);
+ 	return devm_add_action_or_reset(dev, devmap_managed_enable_put, NULL);
+ }
+ #else
+ static int devmap_managed_enable_get(struct device *dev, struct dev_pagemap *pgmap)
+ {
+ 	return -EINVAL;
+ }
+ #endif /* CONFIG_DEV_PAGEMAP_OPS */
++>>>>>>> f6a55e1a3fe6 (memremap: lift the devmap_enable manipulation into devm_memremap_pages)
  
  #if IS_ENABLED(CONFIG_DEVICE_PRIVATE)
 -vm_fault_t device_private_entry_fault(struct vm_area_struct *vma,
 +int device_private_entry_fault(struct vm_area_struct *vma,
  		       unsigned long addr,
  		       swp_entry_t entry,
  		       unsigned int flags,
@@@ -177,13 -177,51 +208,49 @@@ void *devm_memremap_pages(struct devic
  			&pgmap->altmap : NULL;
  	struct resource *res = &pgmap->res;
  	struct dev_pagemap *conflict_pgmap;
 -	struct mhp_restrictions restrictions = {
 -		/*
 -		 * We do not want any optional features only our own memmap
 -		*/
 -		.altmap = altmap,
 -	};
  	pgprot_t pgprot = PAGE_KERNEL;
 +	unsigned long pgoff, order;
  	int error, nid, is_ram;
+ 	bool need_devmap_managed = true;
  
++<<<<<<< HEAD
 +	if (!pgmap->ref || !pgmap->kill)
++=======
+ 	switch (pgmap->type) {
+ 	case MEMORY_DEVICE_PRIVATE:
+ 		if (!IS_ENABLED(CONFIG_DEVICE_PRIVATE)) {
+ 			WARN(1, "Device private memory not supported\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 		break;
+ 	case MEMORY_DEVICE_FS_DAX:
+ 		if (!IS_ENABLED(CONFIG_ZONE_DEVICE) ||
+ 		    IS_ENABLED(CONFIG_FS_DAX_LIMITED)) {
+ 			WARN(1, "File system DAX not supported\n");
+ 			return ERR_PTR(-EINVAL);
+ 		}
+ 		break;
+ 	case MEMORY_DEVICE_DEVDAX:
+ 	case MEMORY_DEVICE_PCI_P2PDMA:
+ 		need_devmap_managed = false;
+ 		break;
+ 	default:
+ 		WARN(1, "Invalid pgmap type %d\n", pgmap->type);
+ 		break;
+ 	}
+ 
+ 	if (!pgmap->ref || !pgmap->ops || !pgmap->ops->kill ||
+ 	    !pgmap->ops->cleanup) {
+ 		WARN(1, "Missing reference count teardown definition\n");
++>>>>>>> f6a55e1a3fe6 (memremap: lift the devmap_enable manipulation into devm_memremap_pages)
  		return ERR_PTR(-EINVAL);
 -	}
  
+ 	if (need_devmap_managed) {
+ 		error = devmap_managed_enable_get(dev, pgmap);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 	}
+ 
  	align_start = res->start & ~(SECTION_SIZE - 1);
  	align_size = ALIGN(res->start + resource_size(res), SECTION_SIZE)
  		- align_start;
* Unmerged path drivers/nvdimm/pmem.c
diff --git a/include/linux/mm.h b/include/linux/mm.h
index fc33f44a60f1..9bce367720d3 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -864,8 +864,6 @@ static inline bool is_zone_device_page(const struct page *page)
 #endif
 
 #ifdef CONFIG_DEV_PAGEMAP_OPS
-void dev_pagemap_get_ops(void);
-void dev_pagemap_put_ops(void);
 void __put_devmap_managed_page(struct page *page);
 DECLARE_STATIC_KEY_FALSE(devmap_managed_key);
 static inline bool put_devmap_managed_page(struct page *page)
@@ -912,14 +910,6 @@ static inline bool is_pci_p2pdma_page(const struct page *page)
 #endif /* CONFIG_PCI_P2PDMA */
 
 #else /* CONFIG_DEV_PAGEMAP_OPS */
-static inline void dev_pagemap_get_ops(void)
-{
-}
-
-static inline void dev_pagemap_put_ops(void)
-{
-}
-
 static inline bool put_devmap_managed_page(struct page *page)
 {
 	return false;
* Unmerged path kernel/memremap.c
diff --git a/mm/hmm.c b/mm/hmm.c
index 91b885757871..29fec34fadfe 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -1027,8 +1027,6 @@ struct hmm_devmem *hmm_devmem_add(const struct hmm_devmem_ops *ops,
 	void *result;
 	int ret;
 
-	dev_pagemap_get_ops();
-
 	devmem = devm_kzalloc(device, sizeof(*devmem), GFP_KERNEL);
 	if (!devmem)
 		return ERR_PTR(-ENOMEM);
