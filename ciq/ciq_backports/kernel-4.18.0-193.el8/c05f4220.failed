blk-mq: remove blk_mq_put_ctx()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Bart Van Assche <bvanassche@acm.org>
commit c05f42206f4de12b6807270fc669b45472f1bdb7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/c05f4220.failed

No code that occurs between blk_mq_get_ctx() and blk_mq_put_ctx() depends
on preemption being disabled for its correctness. Since removing the CPU
preemption calls does not measurably affect performance, simplify the
blk-mq code by removing the blk_mq_put_ctx() function and also by not
disabling preemption in blk_mq_get_ctx().

	Cc: Hannes Reinecke <hare@suse.com>
	Cc: Omar Sandoval <osandov@fb.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Bart Van Assche <bvanassche@acm.org>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c05f42206f4de12b6807270fc669b45472f1bdb7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-sched.c
#	block/blk-mq.c
diff --cc block/blk-mq-sched.c
index 6e6649fd2ba3,c9d183d6c499..000000000000
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@@ -325,10 -330,8 +325,15 @@@ bool __blk_mq_sched_bio_merge(struct re
  	bool ret = false;
  	enum hctx_type type;
  
++<<<<<<< HEAD
 +	if (e && e->type->ops.bio_merge) {
 +		blk_mq_put_ctx(ctx);
 +		return e->type->ops.bio_merge(hctx, bio);
 +	}
++=======
+ 	if (e && e->type->ops.bio_merge)
+ 		return e->type->ops.bio_merge(hctx, bio, nr_segs);
++>>>>>>> c05f42206f4d (blk-mq: remove blk_mq_put_ctx())
  
  	type = hctx->type;
  	if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
diff --cc block/blk-mq.c
index 49d6c08f6c9f,0cb1b152f320..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1974,8 -1973,7 +1970,12 @@@ static blk_qc_t blk_mq_make_request(str
  
  	plug = current->plug;
  	if (unlikely(is_flush_fua)) {
++<<<<<<< HEAD
 +		blk_mq_put_ctx(data.ctx);
 +		blk_mq_bio_to_request(rq, bio);
++=======
+ 		blk_mq_bio_to_request(rq, bio, nr_segs);
++>>>>>>> c05f42206f4d (blk-mq: remove blk_mq_put_ctx())
  
  		/* bypass scheduler for flush rq */
  		blk_insert_flush(rq);
@@@ -1988,8 -1986,7 +1988,12 @@@
  		unsigned int request_count = plug->rq_count;
  		struct request *last = NULL;
  
++<<<<<<< HEAD
 +		blk_mq_put_ctx(data.ctx);
 +		blk_mq_bio_to_request(rq, bio);
++=======
+ 		blk_mq_bio_to_request(rq, bio, nr_segs);
++>>>>>>> c05f42206f4d (blk-mq: remove blk_mq_put_ctx())
  
  		if (!request_count)
  			trace_block_plug(q);
@@@ -2032,12 -2027,10 +2034,19 @@@
  		}
  	} else if ((q->nr_hw_queues > 1 && is_sync) || (!q->elevator &&
  			!data.hctx->dispatch_busy)) {
++<<<<<<< HEAD
 +		blk_mq_put_ctx(data.ctx);
 +		blk_mq_bio_to_request(rq, bio);
 +		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
 +	} else {
 +		blk_mq_put_ctx(data.ctx);
 +		blk_mq_bio_to_request(rq, bio);
++=======
+ 		blk_mq_bio_to_request(rq, bio, nr_segs);
+ 		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ 	} else {
+ 		blk_mq_bio_to_request(rq, bio, nr_segs);
++>>>>>>> c05f42206f4d (blk-mq: remove blk_mq_put_ctx())
  		blk_mq_sched_insert_request(rq, false, true, true);
  	}
  
* Unmerged path block/blk-mq-sched.c
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 2e1809bfdb25..7e7d12daa483 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -112,7 +112,6 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	struct sbq_wait_state *ws;
 	DEFINE_SBQ_WAIT(wait);
 	unsigned int tag_offset;
-	bool drop_ctx;
 	int tag;
 
 	if (data->flags & BLK_MQ_REQ_RESERVED) {
@@ -135,7 +134,6 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		return BLK_MQ_TAG_FAIL;
 
 	ws = bt_wait_ptr(bt, data->hctx);
-	drop_ctx = data->ctx == NULL;
 	do {
 		struct sbitmap_queue *bt_prev;
 
@@ -160,9 +158,6 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		if (tag != -1)
 			break;
 
-		if (data->ctx)
-			blk_mq_put_ctx(data->ctx);
-
 		bt_prev = bt;
 		io_schedule();
 
@@ -188,9 +183,6 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		ws = bt_wait_ptr(bt, data->hctx);
 	} while (1);
 
-	if (drop_ctx && data->ctx)
-		blk_mq_put_ctx(data->ctx);
-
 	sbitmap_finish_wait(bt, ws, &wait);
 
 found_tag:
* Unmerged path block/blk-mq.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index ea15f6e6fc87..52f1e3400c61 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -154,12 +154,7 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
  */
 static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
 {
-	return __blk_mq_get_ctx(q, get_cpu());
-}
-
-static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
-{
-	put_cpu();
+	return __blk_mq_get_ctx(q, raw_smp_processor_id());
 }
 
 struct blk_mq_alloc_data {
diff --git a/block/kyber-iosched.c b/block/kyber-iosched.c
index ec6a04e01bc1..0ba37268217a 100644
--- a/block/kyber-iosched.c
+++ b/block/kyber-iosched.c
@@ -585,7 +585,6 @@ static bool kyber_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio)
 	spin_lock(&kcq->lock);
 	merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio);
 	spin_unlock(&kcq->lock);
-	blk_mq_put_ctx(ctx);
 
 	return merged;
 }
