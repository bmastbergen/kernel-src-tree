net: pass net_device argument to the eth_get_headlen

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] pass net_device argument to the eth_get_headlen (Jiri Benc) [1749814]
Rebuild_FUZZ: 94.95%
commit-author Stanislav Fomichev <sdf@google.com>
commit c43f1255b866b423d2381f77eaa2cbc64a9c49aa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/c43f1255.failed

Update all users of eth_get_headlen to pass network device, fetch
network namespace from it and pass it down to the flow dissector.
This commit is a noop until administrator inserts BPF flow dissector
program.

	Cc: Maxim Krasnyansky <maxk@qti.qualcomm.com>
	Cc: Saeed Mahameed <saeedm@mellanox.com>
	Cc: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
	Cc: intel-wired-lan@lists.osuosl.org
	Cc: Yisen Zhuang <yisen.zhuang@huawei.com>
	Cc: Salil Mehta <salil.mehta@huawei.com>
	Cc: Michael Chan <michael.chan@broadcom.com>
	Cc: Igor Russkikh <igor.russkikh@aquantia.com>
	Signed-off-by: Stanislav Fomichev <sdf@google.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit c43f1255b866b423d2381f77eaa2cbc64a9c49aa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
#	include/linux/etherdevice.h
#	net/ethernet/eth.c
diff --cc drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
index 528c3c77850d,5f7b51c6ee91..000000000000
--- a/drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
+++ b/drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
@@@ -2301,32 -2574,151 +2301,69 @@@ static int hns3_handle_rx_bd(struct hns
  			put_page(desc_cb->priv);
  
  		ring_ptr_move_fw(ring, next_to_clean);
++<<<<<<< HEAD
++=======
+ 		return 0;
+ 	}
+ 	u64_stats_update_begin(&ring->syncp);
+ 	ring->stats.seg_pkt_cnt++;
+ 	u64_stats_update_end(&ring->syncp);
+ 
+ 	ring->pull_len = eth_get_headlen(netdev, va, HNS3_RX_HEAD_SIZE);
+ 	__skb_put(skb, ring->pull_len);
+ 	hns3_nic_reuse_page(skb, ring->frag_num++, ring, ring->pull_len,
+ 			    desc_cb);
+ 	ring_ptr_move_fw(ring, next_to_clean);
+ 
+ 	return HNS3_NEED_ADD_FRAG;
+ }
+ 
+ static int hns3_add_frag(struct hns3_enet_ring *ring, struct hns3_desc *desc,
+ 			 struct sk_buff **out_skb, bool pending)
+ {
+ 	struct sk_buff *skb = *out_skb;
+ 	struct sk_buff *head_skb = *out_skb;
+ 	struct sk_buff *new_skb;
+ 	struct hns3_desc_cb *desc_cb;
+ 	struct hns3_desc *pre_desc;
+ 	u32 bd_base_info;
+ 	int pre_bd;
+ 
+ 	/* if there is pending bd, the SW param next_to_clean has moved
+ 	 * to next and the next is NULL
+ 	 */
+ 	if (pending) {
+ 		pre_bd = (ring->next_to_clean - 1 + ring->desc_num) %
+ 			ring->desc_num;
+ 		pre_desc = &ring->desc[pre_bd];
+ 		bd_base_info = le32_to_cpu(pre_desc->rx.bd_base_info);
++>>>>>>> c43f1255b866 (net: pass net_device argument to the eth_get_headlen)
  	} else {
 -		bd_base_info = le32_to_cpu(desc->rx.bd_base_info);
 -	}
 +		u64_stats_update_begin(&ring->syncp);
 +		ring->stats.seg_pkt_cnt++;
 +		u64_stats_update_end(&ring->syncp);
  
 -	while (!(bd_base_info & BIT(HNS3_RXD_FE_B))) {
 -		desc = &ring->desc[ring->next_to_clean];
 -		desc_cb = &ring->desc_cb[ring->next_to_clean];
 -		bd_base_info = le32_to_cpu(desc->rx.bd_base_info);
 -		/* make sure HW write desc complete */
 -		dma_rmb();
 -		if (!(bd_base_info & BIT(HNS3_RXD_VLD_B)))
 -			return -ENXIO;
 -
 -		if (unlikely(ring->frag_num >= MAX_SKB_FRAGS)) {
 -			new_skb = napi_alloc_skb(&ring->tqp_vector->napi,
 -						 HNS3_RX_HEAD_SIZE);
 -			if (unlikely(!new_skb)) {
 -				netdev_err(ring->tqp->handle->kinfo.netdev,
 -					   "alloc rx skb frag fail\n");
 -				return -ENXIO;
 -			}
 -			ring->frag_num = 0;
 -
 -			if (ring->tail_skb) {
 -				ring->tail_skb->next = new_skb;
 -				ring->tail_skb = new_skb;
 -			} else {
 -				skb_shinfo(skb)->frag_list = new_skb;
 -				ring->tail_skb = new_skb;
 -			}
 -		}
 +		pull_len = eth_get_headlen(va, HNS3_RX_HEAD_SIZE);
  
 -		if (ring->tail_skb) {
 -			head_skb->truesize += hnae3_buf_size(ring);
 -			head_skb->data_len += le16_to_cpu(desc->rx.size);
 -			head_skb->len += le16_to_cpu(desc->rx.size);
 -			skb = ring->tail_skb;
 -		}
 +		memcpy(__skb_put(skb, pull_len), va,
 +		       ALIGN(pull_len, sizeof(long)));
  
 -		hns3_nic_reuse_page(skb, ring->frag_num++, ring, 0, desc_cb);
 +		hns3_nic_reuse_page(skb, 0, ring, pull_len, desc_cb);
  		ring_ptr_move_fw(ring, next_to_clean);
 -		ring->pending_buf++;
 -	}
 -
 -	return 0;
 -}
 -
 -static int hns3_set_gro_and_checksum(struct hns3_enet_ring *ring,
 -				     struct sk_buff *skb, u32 l234info,
 -				     u32 bd_base_info)
 -{
 -	u16 gro_count;
 -	u32 l3_type;
  
 -	gro_count = hnae3_get_field(l234info, HNS3_RXD_GRO_COUNT_M,
 -				    HNS3_RXD_GRO_COUNT_S);
 -	/* if there is no HW GRO, do not set gro params */
 -	if (!gro_count) {
 -		hns3_rx_checksum(ring, skb, l234info, bd_base_info);
 -		return 0;
 +		while (!hnae3_get_bit(bd_base_info, HNS3_RXD_FE_B)) {
 +			desc = &ring->desc[ring->next_to_clean];
 +			desc_cb = &ring->desc_cb[ring->next_to_clean];
 +			bd_base_info = le32_to_cpu(desc->rx.bd_base_info);
 +			hns3_nic_reuse_page(skb, bnum, ring, 0, desc_cb);
 +			ring_ptr_move_fw(ring, next_to_clean);
 +			bnum++;
 +		}
  	}
  
 -	NAPI_GRO_CB(skb)->count = gro_count;
 -
 -	l3_type = hnae3_get_field(l234info, HNS3_RXD_L3ID_M,
 -				  HNS3_RXD_L3ID_S);
 -	if (l3_type == HNS3_L3_TYPE_IPV4)
 -		skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
 -	else if (l3_type == HNS3_L3_TYPE_IPV6)
 -		skb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;
 -	else
 -		return -EFAULT;
 -
 -	skb_shinfo(skb)->gso_size = hnae3_get_field(bd_base_info,
 -						    HNS3_RXD_GRO_SIZE_M,
 -						    HNS3_RXD_GRO_SIZE_S);
 +	*out_bnum = bnum;
  
 -	return  hns3_gro_complete(skb);
 -}
 -
 -static void hns3_set_rx_skb_rss_type(struct hns3_enet_ring *ring,
 -				     struct sk_buff *skb)
 -{
 -	struct hnae3_handle *handle = ring->tqp->handle;
 -	enum pkt_hash_types rss_type;
 -	struct hns3_desc *desc;
 -	int last_bd;
 -
 -	/* When driver handle the rss type, ring->next_to_clean indicates the
 -	 * first descriptor of next packet, need -1 here.
 -	 */
 -	last_bd = (ring->next_to_clean - 1 + ring->desc_num) % ring->desc_num;
 -	desc = &ring->desc[last_bd];
 -
 -	if (le32_to_cpu(desc->rx.rss_hash))
 -		rss_type = handle->kinfo.rss_type;
 -	else
 -		rss_type = PKT_HASH_TYPE_NONE;
 -
 -	skb_set_hash(skb, le32_to_cpu(desc->rx.rss_hash), rss_type);
 -}
 -
 -static int hns3_handle_bdinfo(struct hns3_enet_ring *ring, struct sk_buff *skb,
 -			      struct hns3_desc *desc)
 -{
 -	struct net_device *netdev = ring->tqp->handle->kinfo.netdev;
 -	u32 bd_base_info = le32_to_cpu(desc->rx.bd_base_info);
 -	u32 l234info = le32_to_cpu(desc->rx.l234_info);
 -	enum hns3_pkt_l2t_type l2_frame_type;
 -	unsigned int len;
 -	int ret;
 +	l234info = le32_to_cpu(desc->rx.l234_info);
  
  	/* Based on hw strategy, the tag offloaded will be stored at
  	 * ot_vlan_tag in two layer tag case, and stored at vlan_tag
diff --cc include/linux/etherdevice.h
index 79563840c295,c6c1930e28a0..000000000000
--- a/include/linux/etherdevice.h
+++ b/include/linux/etherdevice.h
@@@ -32,7 -32,8 +32,12 @@@
  struct device;
  int eth_platform_get_mac_address(struct device *dev, u8 *mac_addr);
  unsigned char *arch_get_platform_mac_address(void);
++<<<<<<< HEAD
 +u32 eth_get_headlen(void *data, unsigned int max_len);
++=======
+ int nvmem_get_mac_address(struct device *dev, void *addrbuf);
+ u32 eth_get_headlen(const struct net_device *dev, void *data, unsigned int len);
++>>>>>>> c43f1255b866 (net: pass net_device argument to the eth_get_headlen)
  __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);
  extern const struct header_ops eth_header_ops;
  
diff --cc net/ethernet/eth.c
index ee28440f57c5,0f9863dc4d44..000000000000
--- a/net/ethernet/eth.c
+++ b/net/ethernet/eth.c
@@@ -135,8 -137,9 +136,14 @@@ u32 eth_get_headlen(const struct net_de
  		return len;
  
  	/* parse any remaining L2/L3 headers, check for L4 */
++<<<<<<< HEAD
 +	if (!skb_flow_dissect_flow_keys_basic(NULL, &keys, data, eth->h_proto,
 +					      sizeof(*eth), len, flags))
++=======
+ 	if (!skb_flow_dissect_flow_keys_basic(dev_net(dev), NULL, &keys, data,
+ 					      eth->h_proto, sizeof(*eth),
+ 					      len, flags))
++>>>>>>> c43f1255b866 (net: pass net_device argument to the eth_get_headlen)
  		return max_t(u32, keys.control.thoff, sizeof(*eth));
  
  	/* parse for any L4 headers */
diff --git a/drivers/net/ethernet/aquantia/atlantic/aq_ring.c b/drivers/net/ethernet/aquantia/atlantic/aq_ring.c
index c63439e0b136..2a7b91ed17c5 100644
--- a/drivers/net/ethernet/aquantia/atlantic/aq_ring.c
+++ b/drivers/net/ethernet/aquantia/atlantic/aq_ring.c
@@ -364,7 +364,8 @@ int aq_ring_rx_clean(struct aq_ring_s *self,
 
 			hdr_len = buff->len;
 			if (hdr_len > AQ_CFG_RX_HDR_SIZE)
-				hdr_len = eth_get_headlen(aq_buf_vaddr(&buff->rxdata),
+				hdr_len = eth_get_headlen(skb->dev,
+							  aq_buf_vaddr(&buff->rxdata),
 							  AQ_CFG_RX_HDR_SIZE);
 
 			memcpy(__skb_put(skb, hdr_len), aq_buf_vaddr(&buff->rxdata),
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt.c b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
index 94501af8bebc..297983b0da51 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
@@ -917,7 +917,7 @@ static struct sk_buff *bnxt_rx_page_skb(struct bnxt *bp,
 			     DMA_ATTR_WEAK_ORDERING);
 
 	if (unlikely(!payload))
-		payload = eth_get_headlen(data_ptr, len);
+		payload = eth_get_headlen(bp->dev, data_ptr, len);
 
 	skb = napi_alloc_skb(&rxr->bnapi->napi, payload);
 	if (!skb) {
diff --git a/drivers/net/ethernet/hisilicon/hns/hns_enet.c b/drivers/net/ethernet/hisilicon/hns/hns_enet.c
index 727d12f3423d..1ef259ec565f 100644
--- a/drivers/net/ethernet/hisilicon/hns/hns_enet.c
+++ b/drivers/net/ethernet/hisilicon/hns/hns_enet.c
@@ -598,7 +598,7 @@ static int hns_nic_poll_rx_skb(struct hns_nic_ring_data *ring_data,
 	} else {
 		ring->stats.seg_pkt_cnt++;
 
-		pull_len = eth_get_headlen(va, HNS_RX_HEAD_SIZE);
+		pull_len = eth_get_headlen(ndev, va, HNS_RX_HEAD_SIZE);
 		memcpy(__skb_put(skb, pull_len), va,
 		       ALIGN(pull_len, sizeof(long)));
 
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
diff --git a/drivers/net/ethernet/intel/fm10k/fm10k_main.c b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
index 2325cee76211..b4d970e44163 100644
--- a/drivers/net/ethernet/intel/fm10k/fm10k_main.c
+++ b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
@@ -280,7 +280,7 @@ static bool fm10k_add_rx_frag(struct fm10k_rx_buffer *rx_buffer,
 	/* we need the header to contain the greater of either ETH_HLEN or
 	 * 60 bytes if the skb->len is less than 60 for skb_pad.
 	 */
-	pull_len = eth_get_headlen(va, FM10K_RX_HDR_LEN);
+	pull_len = eth_get_headlen(skb->dev, va, FM10K_RX_HDR_LEN);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, pull_len), va, ALIGN(pull_len, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 1a95223c9f99..e1931701cd7e 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -2035,7 +2035,8 @@ static struct sk_buff *i40e_construct_skb(struct i40e_ring *rx_ring,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > I40E_RX_HDR_SIZE)
-		headlen = eth_get_headlen(xdp->data, I40E_RX_HDR_SIZE);
+		headlen = eth_get_headlen(skb->dev, xdp->data,
+					  I40E_RX_HDR_SIZE);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), xdp->data,
diff --git a/drivers/net/ethernet/intel/iavf/iavf_txrx.c b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
index b64187753ad6..cf8be63a8a4f 100644
--- a/drivers/net/ethernet/intel/iavf/iavf_txrx.c
+++ b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
@@ -1315,7 +1315,7 @@ static struct sk_buff *iavf_construct_skb(struct iavf_ring *rx_ring,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > IAVF_RX_HDR_SIZE)
-		headlen = eth_get_headlen(va, IAVF_RX_HDR_SIZE);
+		headlen = eth_get_headlen(skb->dev, va, IAVF_RX_HDR_SIZE);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), va, ALIGN(headlen, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.c b/drivers/net/ethernet/intel/ice/ice_txrx.c
index efade31167b5..8d5dcfe607a4 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@ -712,7 +712,7 @@ ice_construct_skb(struct ice_ring *rx_ring, struct ice_rx_buf *rx_buf,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > ICE_RX_HDR_SIZE)
-		headlen = eth_get_headlen(va, ICE_RX_HDR_SIZE);
+		headlen = eth_get_headlen(skb->dev, va, ICE_RX_HDR_SIZE);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), va, ALIGN(headlen, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index db0d62038082..cc977d6b65f0 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -8053,7 +8053,7 @@ static struct sk_buff *igb_construct_skb(struct igb_ring *rx_ring,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > IGB_RX_HDR_LEN)
-		headlen = eth_get_headlen(va, IGB_RX_HDR_LEN);
+		headlen = eth_get_headlen(skb->dev, va, IGB_RX_HDR_LEN);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), va, ALIGN(headlen, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/igc/igc_main.c b/drivers/net/ethernet/intel/igc/igc_main.c
index 5cb082494942..25b9a2e95c6a 100644
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@ -1396,7 +1396,7 @@ static struct sk_buff *igc_construct_skb(struct igc_ring *rx_ring,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > IGC_RX_HDR_LEN)
-		headlen = eth_get_headlen(va, IGC_RX_HDR_LEN);
+		headlen = eth_get_headlen(skb->dev, va, IGC_RX_HDR_LEN);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), va, ALIGN(headlen, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 676942919522..70f39cc93e90 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -1800,7 +1800,7 @@ static void ixgbe_pull_tail(struct ixgbe_ring *rx_ring,
 	 * we need the header to contain the greater of either ETH_HLEN or
 	 * 60 bytes if the skb->len is less than 60 for skb_pad.
 	 */
-	pull_len = eth_get_headlen(va, IXGBE_RX_HDR_SIZE);
+	pull_len = eth_get_headlen(skb->dev, va, IXGBE_RX_HDR_SIZE);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 94e1702782f2..571fd8ce54f6 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -896,7 +896,8 @@ struct sk_buff *ixgbevf_construct_skb(struct ixgbevf_ring *rx_ring,
 	/* Determine available headroom for copy */
 	headlen = size;
 	if (headlen > IXGBEVF_RX_HDR_SIZE)
-		headlen = eth_get_headlen(xdp->data, IXGBEVF_RX_HDR_SIZE);
+		headlen = eth_get_headlen(skb->dev, xdp->data,
+					  IXGBEVF_RX_HDR_SIZE);
 
 	/* align pull length to size of long to optimize memcpy performance */
 	memcpy(__skb_put(skb, headlen), xdp->data,
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index fb6f7ea3e525..57a3705ce679 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -167,7 +167,7 @@ static inline u16 mlx5e_calc_min_inline(enum mlx5_inline_modes mode,
 	case MLX5_INLINE_MODE_NONE:
 		return 0;
 	case MLX5_INLINE_MODE_TCP_UDP:
-		hlen = eth_get_headlen(skb->data, skb_headlen(skb));
+		hlen = eth_get_headlen(skb->dev, skb->data, skb_headlen(skb));
 		if (hlen == ETH_HLEN && !skb_vlan_tag_present(skb))
 			hlen += VLAN_HLEN;
 		break;
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 3271c040b424..acbf302002b1 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -2005,7 +2005,8 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 
 	if (frags) {
 		/* Exercise flow dissector code path. */
-		u32 headlen = eth_get_headlen(skb->data, skb_headlen(skb));
+		u32 headlen = eth_get_headlen(tun->dev, skb->data,
+					      skb_headlen(skb));
 
 		if (unlikely(headlen > skb_headlen(skb))) {
 			this_cpu_inc(tun->pcpu_stats->rx_dropped);
* Unmerged path include/linux/etherdevice.h
* Unmerged path net/ethernet/eth.c
