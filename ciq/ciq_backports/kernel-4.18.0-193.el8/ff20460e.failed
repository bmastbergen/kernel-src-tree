tools, bpf: Synchronise BPF UAPI header with tools

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [tools] bpf: synchronise BPF UAPI header with tools (Yauheni Kaliuta) [1747615]
Rebuild_FUZZ: 92.47%
commit-author Quentin Monnet <quentin@isovalent.com>
commit ff20460e94af5d11ebffd9d97c1eaa00e520ecbe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/ff20460e.failed

Synchronise the bpf.h header under tools, to report the fixes recently
brought to the documentation for the BPF helpers.

	Signed-off-by: Quentin Monnet <quentin@isovalent.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20200511161536.29853-5-quentin@isovalent.com
(cherry picked from commit ff20460e94af5d11ebffd9d97c1eaa00e520ecbe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/include/uapi/linux/bpf.h
diff --cc tools/include/uapi/linux/bpf.h
index a8d76994a40e,bfb31c1be219..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -525,26 -670,21 +525,38 @@@ union bpf_attr 
   * 	Return
   * 		0 on success, or a negative error in case of failure.
   *
 - * int bpf_probe_read(void *dst, u32 size, const void *unsafe_ptr)
 + * int bpf_map_push_elem(struct bpf_map *map, const void *value, u64 flags)
 + * 	Description
 + * 		Push an element *value* in *map*. *flags* is one of:
 + *
 + * 		**BPF_EXIST**
 + * 		If the queue/stack is full, the oldest element is removed to
 + * 		make room for this.
 + * 	Return
 + * 		0 on success, or a negative error in case of failure.
 + *
 + * int bpf_probe_read(void *dst, u32 size, const void *src)
   * 	Description
   * 		For tracing programs, safely attempt to read *size* bytes from
++<<<<<<< HEAD
 + * 		address *src* and store the data in *dst*.
++=======
+  * 		kernel space address *unsafe_ptr* and store the data in *dst*.
+  *
+  * 		Generally, use **bpf_probe_read_user**\ () or
+  * 		**bpf_probe_read_kernel**\ () instead.
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 	Return
   * 		0 on success, or a negative error in case of failure.
   *
   * u64 bpf_ktime_get_ns(void)
   * 	Description
   * 		Return the time elapsed since system boot, in nanoseconds.
++<<<<<<< HEAD
++=======
+  * 		Does not include time the system was suspended.
+  * 		See: **clock_gettime**\ (**CLOCK_MONOTONIC**)
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 	Return
   * 		Current *ktime*.
   *
@@@ -1398,45 -1540,14 +1410,54 @@@
   * 	Return
   * 		0 on success, or a negative error in case of failure.
   *
 - * int bpf_probe_read_str(void *dst, u32 size, const void *unsafe_ptr)
 + * int bpf_probe_read_str(void *dst, int size, const void *unsafe_ptr)
   * 	Description
++<<<<<<< HEAD
 + * 		Copy a NUL terminated string from an unsafe address
 + * 		*unsafe_ptr* to *dst*. The *size* should include the
 + * 		terminating NUL byte. In case the string length is smaller than
 + * 		*size*, the target is not padded with further NUL bytes. If the
 + * 		string length is larger than *size*, just *size*-1 bytes are
 + * 		copied and the last byte is set to NUL.
 + *
 + * 		On success, the length of the copied string is returned. This
 + * 		makes this helper useful in tracing programs for reading
 + * 		strings, and more importantly to get its length at runtime. See
 + * 		the following snippet:
 + *
 + * 		::
 + *
 + * 			SEC("kprobe/sys_open")
 + * 			void bpf_sys_open(struct pt_regs *ctx)
 + * 			{
 + * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
 + * 			        int res = bpf_probe_read_str(buf, sizeof(buf),
 + * 				                             ctx->di);
 + *
 + * 				// Consume buf, for example push it to
 + * 				// userspace via bpf_perf_event_output(); we
 + * 				// can use res (the string length) as event
 + * 				// size, after checking its boundaries.
 + * 			}
 + *
 + * 		In comparison, using **bpf_probe_read()** helper here instead
 + * 		to read the string would require to estimate the length at
 + * 		compile time, and would often result in copying more memory
 + * 		than necessary.
 + *
 + * 		Another useful use case is when parsing individual process
 + * 		arguments or individual environment variables navigating
 + * 		*current*\ **->mm->arg_start** and *current*\
 + * 		**->mm->env_start**: using this helper and the return value,
 + * 		one can quickly iterate at the right offset of the memory area.
++=======
+  * 		Copy a NUL terminated string from an unsafe kernel address
+  * 		*unsafe_ptr* to *dst*. See **bpf_probe_read_kernel_str**\ () for
+  * 		more details.
+  *
+  * 		Generally, use **bpf_probe_read_user_str**\ () or
+  * 		**bpf_probe_read_kernel_str**\ () instead.
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 	Return
   * 		On success, the strictly positive length of the string,
   * 		including the trailing NUL character. On error, a negative
@@@ -1464,8 -1575,8 +1485,13 @@@
   *
   * u64 bpf_get_socket_cookie(struct bpf_sock_ops *ctx)
   * 	Description
++<<<<<<< HEAD
 + * 		Equivalent to bpf_get_socket_cookie() helper that accepts
 + * 		*skb*, but gets socket from **struct bpf_sock_ops** contex.
++=======
+  * 		Equivalent to **bpf_get_socket_cookie**\ () helper that accepts
+  * 		*skb*, but gets socket from **struct bpf_sock_ops** context.
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 	Return
   * 		A 8-byte long non-decreasing number.
   *
@@@ -1492,6 -1603,12 +1518,15 @@@
   * 		must be specified, see **setsockopt(2)** for more information.
   * 		The option value of length *optlen* is pointed by *optval*.
   *
++<<<<<<< HEAD
++=======
+  * 		*bpf_socket* should be one of the following:
+  *
+  * 		* **struct bpf_sock_ops** for **BPF_PROG_TYPE_SOCK_OPS**.
+  * 		* **struct bpf_sock_addr** for **BPF_CGROUP_INET4_CONNECT**
+  * 		  and **BPF_CGROUP_INET6_CONNECT**.
+  *
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 		This helper actually implements a subset of **setsockopt()**.
   * 		It supports the following *level*\ s:
   *
@@@ -1552,19 -1673,17 +1587,24 @@@
   *
   * 		The lower two bits of *flags* are used as the return code if
   * 		the map lookup fails. This is so that the return value can be
-  * 		one of the XDP program return codes up to XDP_TX, as chosen by
-  * 		the caller. Any higher bits in the *flags* argument must be
+  * 		one of the XDP program return codes up to **XDP_TX**, as chosen
+  * 		by the caller. Any higher bits in the *flags* argument must be
   * 		unset.
   *
++<<<<<<< HEAD
 + * 		When used to redirect packets to net devices, this helper
 + * 		provides a high performance increase over **bpf_redirect**\ ().
 + * 		This is due to various implementation details of the underlying
 + * 		mechanisms, one of which is the fact that **bpf_redirect_map**\
 + * 		() tries to send packet as a "bulk" to the device.
++=======
+  * 		See also **bpf_redirect**\ (), which only supports redirecting
+  * 		to an ifindex, but doesn't require a map to do so.
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 	Return
 - * 		**XDP_REDIRECT** on success, or the value of the two lower bits
 - * 		of the *flags* argument on error.
 + * 		**XDP_REDIRECT** on success, or **XDP_ABORTED** on error.
   *
 - * int bpf_sk_redirect_map(struct sk_buff *skb, struct bpf_map *map, u32 key, u64 flags)
 + * int bpf_sk_redirect_map(struct bpf_map *map, u32 key, u64 flags)
   * 	Description
   * 		Redirect the packet to the socket referenced by *map* (of type
   * 		**BPF_MAP_TYPE_SOCKMAP**) at index *key*. Both ingress and
@@@ -1693,6 -1812,12 +1733,15 @@@
   * 		The retrieved value is stored in the structure pointed by
   * 		*opval* and of length *optlen*.
   *
++<<<<<<< HEAD
++=======
+  * 		*bpf_socket* should be one of the following:
+  *
+  * 		* **struct bpf_sock_ops** for **BPF_PROG_TYPE_SOCK_OPS**.
+  * 		* **struct bpf_sock_addr** for **BPF_CGROUP_INET4_CONNECT**
+  * 		  and **BPF_CGROUP_INET6_CONNECT**.
+  *
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   * 		This helper actually implements a subset of **getsockopt()**.
   * 		It supports the following *level*\ s:
   *
@@@ -2382,6 -2528,599 +2431,602 @@@
   *		"**y**".
   *	Return
   *		0
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_spin_lock(struct bpf_spin_lock *lock)
+  *	Description
+  *		Acquire a spinlock represented by the pointer *lock*, which is
+  *		stored as part of a value of a map. Taking the lock allows to
+  *		safely update the rest of the fields in that value. The
+  *		spinlock can (and must) later be released with a call to
+  *		**bpf_spin_unlock**\ (\ *lock*\ ).
+  *
+  *		Spinlocks in BPF programs come with a number of restrictions
+  *		and constraints:
+  *
+  *		* **bpf_spin_lock** objects are only allowed inside maps of
+  *		  types **BPF_MAP_TYPE_HASH** and **BPF_MAP_TYPE_ARRAY** (this
+  *		  list could be extended in the future).
+  *		* BTF description of the map is mandatory.
+  *		* The BPF program can take ONE lock at a time, since taking two
+  *		  or more could cause dead locks.
+  *		* Only one **struct bpf_spin_lock** is allowed per map element.
+  *		* When the lock is taken, calls (either BPF to BPF or helpers)
+  *		  are not allowed.
+  *		* The **BPF_LD_ABS** and **BPF_LD_IND** instructions are not
+  *		  allowed inside a spinlock-ed region.
+  *		* The BPF program MUST call **bpf_spin_unlock**\ () to release
+  *		  the lock, on all execution paths, before it returns.
+  *		* The BPF program can access **struct bpf_spin_lock** only via
+  *		  the **bpf_spin_lock**\ () and **bpf_spin_unlock**\ ()
+  *		  helpers. Loading or storing data into the **struct
+  *		  bpf_spin_lock** *lock*\ **;** field of a map is not allowed.
+  *		* To use the **bpf_spin_lock**\ () helper, the BTF description
+  *		  of the map value must be a struct and have **struct
+  *		  bpf_spin_lock** *anyname*\ **;** field at the top level.
+  *		  Nested lock inside another struct is not allowed.
+  *		* The **struct bpf_spin_lock** *lock* field in a map value must
+  *		  be aligned on a multiple of 4 bytes in that value.
+  *		* Syscall with command **BPF_MAP_LOOKUP_ELEM** does not copy
+  *		  the **bpf_spin_lock** field to user space.
+  *		* Syscall with command **BPF_MAP_UPDATE_ELEM**, or update from
+  *		  a BPF program, do not update the **bpf_spin_lock** field.
+  *		* **bpf_spin_lock** cannot be on the stack or inside a
+  *		  networking packet (it can only be inside of a map values).
+  *		* **bpf_spin_lock** is available to root only.
+  *		* Tracing programs and socket filter programs cannot use
+  *		  **bpf_spin_lock**\ () due to insufficient preemption checks
+  *		  (but this may change in the future).
+  *		* **bpf_spin_lock** is not allowed in inner maps of map-in-map.
+  *	Return
+  *		0
+  *
+  * int bpf_spin_unlock(struct bpf_spin_lock *lock)
+  *	Description
+  *		Release the *lock* previously locked by a call to
+  *		**bpf_spin_lock**\ (\ *lock*\ ).
+  *	Return
+  *		0
+  *
+  * struct bpf_sock *bpf_sk_fullsock(struct bpf_sock *sk)
+  *	Description
+  *		This helper gets a **struct bpf_sock** pointer such
+  *		that all the fields in this **bpf_sock** can be accessed.
+  *	Return
+  *		A **struct bpf_sock** pointer on success, or **NULL** in
+  *		case of failure.
+  *
+  * struct bpf_tcp_sock *bpf_tcp_sock(struct bpf_sock *sk)
+  *	Description
+  *		This helper gets a **struct bpf_tcp_sock** pointer from a
+  *		**struct bpf_sock** pointer.
+  *	Return
+  *		A **struct bpf_tcp_sock** pointer on success, or **NULL** in
+  *		case of failure.
+  *
+  * int bpf_skb_ecn_set_ce(struct sk_buff *skb)
+  *	Description
+  *		Set ECN (Explicit Congestion Notification) field of IP header
+  *		to **CE** (Congestion Encountered) if current value is **ECT**
+  *		(ECN Capable Transport). Otherwise, do nothing. Works with IPv6
+  *		and IPv4.
+  *	Return
+  *		1 if the **CE** flag is set (either by the current helper call
+  *		or because it was already present), 0 if it is not set.
+  *
+  * struct bpf_sock *bpf_get_listener_sock(struct bpf_sock *sk)
+  *	Description
+  *		Return a **struct bpf_sock** pointer in **TCP_LISTEN** state.
+  *		**bpf_sk_release**\ () is unnecessary and not allowed.
+  *	Return
+  *		A **struct bpf_sock** pointer on success, or **NULL** in
+  *		case of failure.
+  *
+  * struct bpf_sock *bpf_skc_lookup_tcp(void *ctx, struct bpf_sock_tuple *tuple, u32 tuple_size, u64 netns, u64 flags)
+  *	Description
+  *		Look for TCP socket matching *tuple*, optionally in a child
+  *		network namespace *netns*. The return value must be checked,
+  *		and if non-**NULL**, released via **bpf_sk_release**\ ().
+  *
+  *		This function is identical to **bpf_sk_lookup_tcp**\ (), except
+  *		that it also returns timewait or request sockets. Use
+  *		**bpf_sk_fullsock**\ () or **bpf_tcp_sock**\ () to access the
+  *		full structure.
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		**CONFIG_NET** configuration option.
+  *	Return
+  *		Pointer to **struct bpf_sock**, or **NULL** in case of failure.
+  *		For sockets with reuseport option, the **struct bpf_sock**
+  *		result is from *reuse*\ **->socks**\ [] using the hash of the
+  *		tuple.
+  *
+  * int bpf_tcp_check_syncookie(struct bpf_sock *sk, void *iph, u32 iph_len, struct tcphdr *th, u32 th_len)
+  * 	Description
+  * 		Check whether *iph* and *th* contain a valid SYN cookie ACK for
+  * 		the listening socket in *sk*.
+  *
+  * 		*iph* points to the start of the IPv4 or IPv6 header, while
+  * 		*iph_len* contains **sizeof**\ (**struct iphdr**) or
+  * 		**sizeof**\ (**struct ip6hdr**).
+  *
+  * 		*th* points to the start of the TCP header, while *th_len*
+  * 		contains **sizeof**\ (**struct tcphdr**).
+  * 	Return
+  * 		0 if *iph* and *th* are a valid SYN cookie ACK, or a negative
+  * 		error otherwise.
+  *
+  * int bpf_sysctl_get_name(struct bpf_sysctl *ctx, char *buf, size_t buf_len, u64 flags)
+  *	Description
+  *		Get name of sysctl in /proc/sys/ and copy it into provided by
+  *		program buffer *buf* of size *buf_len*.
+  *
+  *		The buffer is always NUL terminated, unless it's zero-sized.
+  *
+  *		If *flags* is zero, full name (e.g. "net/ipv4/tcp_mem") is
+  *		copied. Use **BPF_F_SYSCTL_BASE_NAME** flag to copy base name
+  *		only (e.g. "tcp_mem").
+  *	Return
+  *		Number of character copied (not including the trailing NUL).
+  *
+  *		**-E2BIG** if the buffer wasn't big enough (*buf* will contain
+  *		truncated name in this case).
+  *
+  * int bpf_sysctl_get_current_value(struct bpf_sysctl *ctx, char *buf, size_t buf_len)
+  *	Description
+  *		Get current value of sysctl as it is presented in /proc/sys
+  *		(incl. newline, etc), and copy it as a string into provided
+  *		by program buffer *buf* of size *buf_len*.
+  *
+  *		The whole value is copied, no matter what file position user
+  *		space issued e.g. sys_read at.
+  *
+  *		The buffer is always NUL terminated, unless it's zero-sized.
+  *	Return
+  *		Number of character copied (not including the trailing NUL).
+  *
+  *		**-E2BIG** if the buffer wasn't big enough (*buf* will contain
+  *		truncated name in this case).
+  *
+  *		**-EINVAL** if current value was unavailable, e.g. because
+  *		sysctl is uninitialized and read returns -EIO for it.
+  *
+  * int bpf_sysctl_get_new_value(struct bpf_sysctl *ctx, char *buf, size_t buf_len)
+  *	Description
+  *		Get new value being written by user space to sysctl (before
+  *		the actual write happens) and copy it as a string into
+  *		provided by program buffer *buf* of size *buf_len*.
+  *
+  *		User space may write new value at file position > 0.
+  *
+  *		The buffer is always NUL terminated, unless it's zero-sized.
+  *	Return
+  *		Number of character copied (not including the trailing NUL).
+  *
+  *		**-E2BIG** if the buffer wasn't big enough (*buf* will contain
+  *		truncated name in this case).
+  *
+  *		**-EINVAL** if sysctl is being read.
+  *
+  * int bpf_sysctl_set_new_value(struct bpf_sysctl *ctx, const char *buf, size_t buf_len)
+  *	Description
+  *		Override new value being written by user space to sysctl with
+  *		value provided by program in buffer *buf* of size *buf_len*.
+  *
+  *		*buf* should contain a string in same form as provided by user
+  *		space on sysctl write.
+  *
+  *		User space may write new value at file position > 0. To override
+  *		the whole sysctl value file position should be set to zero.
+  *	Return
+  *		0 on success.
+  *
+  *		**-E2BIG** if the *buf_len* is too big.
+  *
+  *		**-EINVAL** if sysctl is being read.
+  *
+  * int bpf_strtol(const char *buf, size_t buf_len, u64 flags, long *res)
+  *	Description
+  *		Convert the initial part of the string from buffer *buf* of
+  *		size *buf_len* to a long integer according to the given base
+  *		and save the result in *res*.
+  *
+  *		The string may begin with an arbitrary amount of white space
+  *		(as determined by **isspace**\ (3)) followed by a single
+  *		optional '**-**' sign.
+  *
+  *		Five least significant bits of *flags* encode base, other bits
+  *		are currently unused.
+  *
+  *		Base must be either 8, 10, 16 or 0 to detect it automatically
+  *		similar to user space **strtol**\ (3).
+  *	Return
+  *		Number of characters consumed on success. Must be positive but
+  *		no more than *buf_len*.
+  *
+  *		**-EINVAL** if no valid digits were found or unsupported base
+  *		was provided.
+  *
+  *		**-ERANGE** if resulting value was out of range.
+  *
+  * int bpf_strtoul(const char *buf, size_t buf_len, u64 flags, unsigned long *res)
+  *	Description
+  *		Convert the initial part of the string from buffer *buf* of
+  *		size *buf_len* to an unsigned long integer according to the
+  *		given base and save the result in *res*.
+  *
+  *		The string may begin with an arbitrary amount of white space
+  *		(as determined by **isspace**\ (3)).
+  *
+  *		Five least significant bits of *flags* encode base, other bits
+  *		are currently unused.
+  *
+  *		Base must be either 8, 10, 16 or 0 to detect it automatically
+  *		similar to user space **strtoul**\ (3).
+  *	Return
+  *		Number of characters consumed on success. Must be positive but
+  *		no more than *buf_len*.
+  *
+  *		**-EINVAL** if no valid digits were found or unsupported base
+  *		was provided.
+  *
+  *		**-ERANGE** if resulting value was out of range.
+  *
+  * void *bpf_sk_storage_get(struct bpf_map *map, struct bpf_sock *sk, void *value, u64 flags)
+  *	Description
+  *		Get a bpf-local-storage from a *sk*.
+  *
+  *		Logically, it could be thought of getting the value from
+  *		a *map* with *sk* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *sk*) except this
+  *		helper enforces the key must be a full socket and the map must
+  *		be a **BPF_MAP_TYPE_SK_STORAGE** also.
+  *
+  *		Underneath, the value is stored locally at *sk* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf-local-storages residing at *sk*.
+  *
+  *		An optional *flags* (**BPF_SK_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf-local-storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_SK_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf-local-storage.  If *value* is
+  *		**NULL**, the new bpf-local-storage will be zero initialized.
+  *	Return
+  *		A bpf-local-storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf-local-storage.
+  *
+  * int bpf_sk_storage_delete(struct bpf_map *map, struct bpf_sock *sk)
+  *	Description
+  *		Delete a bpf-local-storage from a *sk*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf-local-storage cannot be found.
+  *
+  * int bpf_send_signal(u32 sig)
+  *	Description
+  *		Send signal *sig* to the process of the current task.
+  *		The signal may be delivered to any of this process's threads.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * s64 bpf_tcp_gen_syncookie(struct bpf_sock *sk, void *iph, u32 iph_len, struct tcphdr *th, u32 th_len)
+  *	Description
+  *		Try to issue a SYN cookie for the packet with corresponding
+  *		IP/TCP headers, *iph* and *th*, on the listening socket in *sk*.
+  *
+  *		*iph* points to the start of the IPv4 or IPv6 header, while
+  *		*iph_len* contains **sizeof**\ (**struct iphdr**) or
+  *		**sizeof**\ (**struct ip6hdr**).
+  *
+  *		*th* points to the start of the TCP header, while *th_len*
+  *		contains the length of the TCP header.
+  *	Return
+  *		On success, lower 32 bits hold the generated SYN cookie in
+  *		followed by 16 bits which hold the MSS value for that cookie,
+  *		and the top 16 bits are unused.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** SYN cookie cannot be issued due to error
+  *
+  *		**-ENOENT** SYN cookie should not be issued (no SYN flood)
+  *
+  *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
+  *
+  *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user**\ () helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with **bpf_probe_read_user_str**\ () apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string, including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct **tcp_sock**.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (**struct perf_branch_entry**) associated to *ctx*
+  *		and store it in the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of **sizeof**\ (**struct perf_branch_entry**\ ).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
+  *
+  * int bpf_get_ns_current_pid_tgid(u64 dev, u64 ino, struct bpf_pidns_info *nsdata, u32 size)
+  *	Description
+  *		Returns 0 on success, values for *pid* and *tgid* as seen from the current
+  *		*namespace* will be returned in *nsdata*.
+  *	Return
+  *		0 on success, or one of the following in case of failure:
+  *
+  *		**-EINVAL** if dev and inum supplied don't match dev_t and inode number
+  *              with nsfs of current task, or if dev conversion to dev_t lost high bits.
+  *
+  *		**-ENOENT** if pidns does not exists for the current task.
+  *
+  * int bpf_xdp_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  *	Description
+  *		Write raw *data* blob into a special BPF perf event held by
+  *		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  *		event must have the following attributes: **PERF_SAMPLE_RAW**
+  *		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  *		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  *		The *flags* are used to indicate the index in *map* for which
+  *		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  *		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  *		to indicate that the index of the current CPU core should be
+  *		used.
+  *
+  *		The value to write, of *size*, is passed through eBPF stack and
+  *		pointed by *data*.
+  *
+  *		*ctx* is a pointer to in-kernel struct xdp_buff.
+  *
+  *		This helper is similar to **bpf_perf_eventoutput**\ () but
+  *		restricted to raw_tracepoint bpf programs.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * u64 bpf_get_netns_cookie(void *ctx)
+  * 	Description
+  * 		Retrieve the cookie (generated by the kernel) of the network
+  * 		namespace the input *ctx* is associated with. The network
+  * 		namespace cookie remains stable for its lifetime and provides
+  * 		a global identifier that can be assumed unique. If *ctx* is
+  * 		NULL, then the helper returns the cookie for the initial
+  * 		network namespace. The cookie itself is very similar to that
+  * 		of **bpf_get_socket_cookie**\ () helper, but for network
+  * 		namespaces instead of sockets.
+  * 	Return
+  * 		A 8-byte long opaque number.
+  *
+  * u64 bpf_get_current_ancestor_cgroup_id(int ancestor_level)
+  * 	Description
+  * 		Return id of cgroup v2 that is ancestor of the cgroup associated
+  * 		with the current task at the *ancestor_level*. The root cgroup
+  * 		is at *ancestor_level* zero and each step down the hierarchy
+  * 		increments the level. If *ancestor_level* == level of cgroup
+  * 		associated with the current task, then return value will be the
+  * 		same as that of **bpf_get_current_cgroup_id**\ ().
+  *
+  * 		The helper is useful to implement policies based on cgroups
+  * 		that are upper in hierarchy than immediate cgroup associated
+  * 		with the current task.
+  *
+  * 		The format of returned id and helper limitations are same as in
+  * 		**bpf_get_current_cgroup_id**\ ().
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * int bpf_sk_assign(struct sk_buff *skb, struct bpf_sock *sk, u64 flags)
+  *	Description
+  *		Assign the *sk* to the *skb*. When combined with appropriate
+  *		routing configuration to receive the packet towards the socket,
+  *		will cause *skb* to be delivered to the specified socket.
+  *		Subsequent redirection of *skb* via  **bpf_redirect**\ (),
+  *		**bpf_clone_redirect**\ () or other methods outside of BPF may
+  *		interfere with successful delivery to the socket.
+  *
+  *		This operation is only valid from TC ingress path.
+  *
+  *		The *flags* argument must be zero.
+  *	Return
+  *		0 on success, or a negative error in case of failure:
+  *
+  *		**-EINVAL** if specified *flags* are not supported.
+  *
+  *		**-ENOENT** if the socket is unavailable for assignment.
+  *
+  *		**-ENETUNREACH** if the socket is unreachable (wrong netns).
+  *
+  *		**-EOPNOTSUPP** if the operation is not supported, for example
+  *		a call from outside of TC ingress.
+  *
+  *		**-ESOCKTNOSUPPORT** if the socket type is not supported
+  *		(reuseport).
+  *
+  * u64 bpf_ktime_get_boot_ns(void)
+  * 	Description
+  * 		Return the time elapsed since system boot, in nanoseconds.
+  * 		Does include the time the system was suspended.
+  * 		See: **clock_gettime**\ (**CLOCK_BOOTTIME**)
+  * 	Return
+  * 		Current *ktime*.
+  *
+  * int bpf_seq_printf(struct seq_file *m, const char *fmt, u32 fmt_size, const void *data, u32 data_len)
+  * 	Description
+  * 		**bpf_seq_printf**\ () uses seq_file **seq_printf**\ () to print
+  * 		out the format string.
+  * 		The *m* represents the seq_file. The *fmt* and *fmt_size* are for
+  * 		the format string itself. The *data* and *data_len* are format string
+  * 		arguments. The *data* are a **u64** array and corresponding format string
+  * 		values are stored in the array. For strings and pointers where pointees
+  * 		are accessed, only the pointer values are stored in the *data* array.
+  * 		The *data_len* is the size of *data* in bytes.
+  *
+  *		Formats **%s**, **%p{i,I}{4,6}** requires to read kernel memory.
+  *		Reading kernel memory may fail due to either invalid address or
+  *		valid address but requiring a major memory fault. If reading kernel memory
+  *		fails, the string for **%s** will be an empty string, and the ip
+  *		address for **%p{i,I}{4,6}** will be 0. Not returning error to
+  *		bpf program is consistent with what **bpf_trace_printk**\ () does for now.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure:
+  *
+  *		**-EBUSY** if per-CPU memory copy buffer is busy, can try again
+  *		by returning 1 from bpf program.
+  *
+  *		**-EINVAL** if arguments are invalid, or if *fmt* is invalid/unsupported.
+  *
+  *		**-E2BIG** if *fmt* contains too many format specifiers.
+  *
+  *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
+  *
+  * int bpf_seq_write(struct seq_file *m, const void *data, u32 len)
+  * 	Description
+  * 		**bpf_seq_write**\ () uses seq_file **seq_write**\ () to write the data.
+  * 		The *m* represents the seq_file. The *data* and *len* represent the
+  * 		data to write in bytes.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure:
+  *
+  *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
++>>>>>>> ff20460e94af (tools, bpf: Synchronise BPF UAPI header with tools)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
* Unmerged path tools/include/uapi/linux/bpf.h
