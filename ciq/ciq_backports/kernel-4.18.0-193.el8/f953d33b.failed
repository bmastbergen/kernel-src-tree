net/tls: add kernel-driven TLS RX resync

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] tls: add kernel-driven TLS RX resync (Sabrina Dubroca) [1760375]
Rebuild_FUZZ: 94.74%
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit f953d33ba1225d68cf8790b4706d8c4410b15926
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/f953d33b.failed

TLS offload device may lose sync with the TCP stream if packets
arrive out of order.  Drivers can currently request a resync at
a specific TCP sequence number.  When a record is found starting
at that sequence number kernel will inform the device of the
corresponding record number.

This requires the device to constantly scan the stream for a
known pattern (constant bytes of the header) after sync is lost.

This patch adds an alternative approach which is entirely under
the control of the kernel.  Kernel tracks records it had to fully
decrypt, even though TLS socket is in TLS_HW mode.  If multiple
records did not have any decrypted parts - it's a pretty strong
indication that the device is out of sync.

We choose the min number of fully encrypted records to be 2,
which should hopefully be more than will get retransmitted at
a time.

After kernel decides the device is out of sync it schedules a
resync request.  If the TCP socket is empty the resync gets
performed immediately.  If socket is not empty we leave the
record parser to resync when next record comes.

Before resync in message parser we peek at the TCP socket and
don't attempt the sync if the socket already has some of the
next record queued.

On resync failure (encrypted data continues to flow in) we
retry with exponential backoff, up to once every 128 records
(with a 16k record thats at most once every 2M of data).

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f953d33ba1225d68cf8790b4706d8c4410b15926)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/networking/tls-offload.rst
#	include/net/tls.h
#	net/tls/tls_device.c
#	net/tls/tls_sw.c
diff --cc include/net/tls.h
index 654ae56c8ae2,28eca6a3b615..000000000000
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@@ -293,6 -286,31 +293,34 @@@ struct tls_context 
  	refcount_t refcount;
  };
  
++<<<<<<< HEAD
++=======
+ enum tls_offload_ctx_dir {
+ 	TLS_OFFLOAD_CTX_DIR_RX,
+ 	TLS_OFFLOAD_CTX_DIR_TX,
+ };
+ 
+ struct tlsdev_ops {
+ 	int (*tls_dev_add)(struct net_device *netdev, struct sock *sk,
+ 			   enum tls_offload_ctx_dir direction,
+ 			   struct tls_crypto_info *crypto_info,
+ 			   u32 start_offload_tcp_sn);
+ 	void (*tls_dev_del)(struct net_device *netdev,
+ 			    struct tls_context *ctx,
+ 			    enum tls_offload_ctx_dir direction);
+ 	void (*tls_dev_resync_rx)(struct net_device *netdev,
+ 				  struct sock *sk, u32 seq, u8 *rcd_sn);
+ };
+ 
+ enum tls_offload_sync_type {
+ 	TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ = 0,
+ 	TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT = 1,
+ };
+ 
+ #define TLS_DEVICE_RESYNC_NH_START_IVAL		2
+ #define TLS_DEVICE_RESYNC_NH_MAX_IVAL		128
+ 
++>>>>>>> f953d33ba122 (net/tls: add kernel-driven TLS RX resync)
  struct tls_offload_context_rx {
  	/* sw must be the first member of tls_offload_context_rx */
  	struct tls_sw_context_rx sw;
@@@ -584,6 -638,6 +634,10 @@@ int tls_sw_fallback_init(struct sock *s
  int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
  
  void tls_device_offload_cleanup_rx(struct sock *sk);
++<<<<<<< HEAD
 +void handle_device_resync(struct sock *sk, u32 seq, u64 rcd_sn);
++=======
+ void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
++>>>>>>> f953d33ba122 (net/tls: add kernel-driven TLS RX resync)
  
  #endif /* _TLS_OFFLOAD_H */
diff --cc net/tls/tls_device.c
index 330d54f9f11e,477c869c69c8..000000000000
--- a/net/tls/tls_device.c
+++ b/net/tls/tls_device.c
@@@ -563,7 -563,7 +563,11 @@@ static void tls_device_resync_rx(struc
  	clear_bit_unlock(TLS_RX_SYNC_RUNNING, &tls_ctx->flags);
  }
  
++<<<<<<< HEAD
 +void handle_device_resync(struct sock *sk, u32 seq, u64 rcd_sn)
++=======
+ void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq)
++>>>>>>> f953d33ba122 (net/tls: add kernel-driven TLS RX resync)
  {
  	struct tls_context *tls_ctx = tls_get_ctx(sk);
  	struct tls_offload_context_rx *rx_ctx;
@@@ -574,15 -576,84 +580,90 @@@
  	if (tls_ctx->rx_conf != TLS_HW)
  		return;
  
+ 	prot = &tls_ctx->prot_info;
  	rx_ctx = tls_offload_ctx_rx(tls_ctx);
- 	resync_req = atomic64_read(&rx_ctx->resync_req);
- 	req_seq = resync_req >> 32;
- 	seq += TLS_HEADER_SIZE - 1;
- 	is_req_pending = resync_req;
+ 	memcpy(rcd_sn, tls_ctx->rx.rec_seq, prot->rec_seq_size);
  
++<<<<<<< HEAD
 +	if (unlikely(is_req_pending) && req_seq == seq &&
 +	    atomic64_try_cmpxchg(&rx_ctx->resync_req, &resync_req, 0))
 +		tls_device_resync_rx(tls_ctx, sk, seq, rcd_sn);
++=======
+ 	switch (rx_ctx->resync_type) {
+ 	case TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ:
+ 		resync_req = atomic64_read(&rx_ctx->resync_req);
+ 		req_seq = resync_req >> 32;
+ 		seq += TLS_HEADER_SIZE - 1;
+ 		is_req_pending = resync_req;
+ 
+ 		if (likely(!is_req_pending) || req_seq != seq ||
+ 		    !atomic64_try_cmpxchg(&rx_ctx->resync_req, &resync_req, 0))
+ 			return;
+ 		break;
+ 	case TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT:
+ 		if (likely(!rx_ctx->resync_nh_do_now))
+ 			return;
+ 
+ 		/* head of next rec is already in, note that the sock_inq will
+ 		 * include the currently parsed message when called from parser
+ 		 */
+ 		if (tcp_inq(sk) > rcd_len)
+ 			return;
+ 
+ 		rx_ctx->resync_nh_do_now = 0;
+ 		seq += rcd_len;
+ 		tls_bigint_increment(rcd_sn, prot->rec_seq_size);
+ 		break;
+ 	}
+ 
+ 	tls_device_resync_rx(tls_ctx, sk, seq, rcd_sn);
+ }
+ 
+ static void tls_device_core_ctrl_rx_resync(struct tls_context *tls_ctx,
+ 					   struct tls_offload_context_rx *ctx,
+ 					   struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct strp_msg *rxm;
+ 
+ 	/* device will request resyncs by itself based on stream scan */
+ 	if (ctx->resync_type != TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT)
+ 		return;
+ 	/* already scheduled */
+ 	if (ctx->resync_nh_do_now)
+ 		return;
+ 	/* seen decrypted fragments since last fully-failed record */
+ 	if (ctx->resync_nh_reset) {
+ 		ctx->resync_nh_reset = 0;
+ 		ctx->resync_nh.decrypted_failed = 1;
+ 		ctx->resync_nh.decrypted_tgt = TLS_DEVICE_RESYNC_NH_START_IVAL;
+ 		return;
+ 	}
+ 
+ 	if (++ctx->resync_nh.decrypted_failed <= ctx->resync_nh.decrypted_tgt)
+ 		return;
+ 
+ 	/* doing resync, bump the next target in case it fails */
+ 	if (ctx->resync_nh.decrypted_tgt < TLS_DEVICE_RESYNC_NH_MAX_IVAL)
+ 		ctx->resync_nh.decrypted_tgt *= 2;
+ 	else
+ 		ctx->resync_nh.decrypted_tgt += TLS_DEVICE_RESYNC_NH_MAX_IVAL;
+ 
+ 	rxm = strp_msg(skb);
+ 
+ 	/* head of next rec is already in, parser will sync for us */
+ 	if (tcp_inq(sk) > rxm->full_len) {
+ 		ctx->resync_nh_do_now = 1;
+ 	} else {
+ 		struct tls_prot_info *prot = &tls_ctx->prot_info;
+ 		u8 rcd_sn[TLS_MAX_REC_SEQ_SIZE];
+ 
+ 		memcpy(rcd_sn, tls_ctx->rx.rec_seq, prot->rec_seq_size);
+ 		tls_bigint_increment(rcd_sn, prot->rec_seq_size);
+ 
+ 		tls_device_resync_rx(tls_ctx, sk, tcp_sk(sk)->copied_seq,
+ 				     rcd_sn);
+ 	}
++>>>>>>> f953d33ba122 (net/tls: add kernel-driven TLS RX resync)
  }
  
  static int tls_device_reencrypt(struct sock *sk, struct sk_buff *skb)
diff --cc net/tls/tls_sw.c
index ce27aa31184f,533eaa4826e5..000000000000
--- a/net/tls/tls_sw.c
+++ b/net/tls/tls_sw.c
@@@ -2015,8 -2015,8 +2015,13 @@@ static int tls_read_size(struct strpars
  		goto read_failure;
  	}
  #ifdef CONFIG_TLS_DEVICE
++<<<<<<< HEAD
 +	handle_device_resync(strp->sk, TCP_SKB_CB(skb)->seq + rxm->offset,
 +			     *(u64*)tls_ctx->rx.rec_seq);
++=======
+ 	tls_device_rx_resync_new_rec(strp->sk, data_len + TLS_HEADER_SIZE,
+ 				     TCP_SKB_CB(skb)->seq + rxm->offset);
++>>>>>>> f953d33ba122 (net/tls: add kernel-driven TLS RX resync)
  #endif
  	return data_len + TLS_HEADER_SIZE;
  
* Unmerged path Documentation/networking/tls-offload.rst
* Unmerged path Documentation/networking/tls-offload.rst
* Unmerged path include/net/tls.h
* Unmerged path net/tls/tls_device.c
* Unmerged path net/tls/tls_sw.c
