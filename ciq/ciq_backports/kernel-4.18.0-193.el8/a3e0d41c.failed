mm/hmm: improve driver API to work and wait over a range

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] hmm: improve driver API to work and wait over a range (Jerome Glisse) [1498655 1597758]
Rebuild_FUZZ: 97.25%
commit-author Jérôme Glisse <jglisse@redhat.com>
commit a3e0d41c2b1f86b483b202d642140d8b86d677ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/a3e0d41c.failed

A common use case for HMM mirror is user trying to mirror a range and
before they could program the hardware it get invalidated by some core mm
event.  Instead of having user re-try right away to mirror the range
provide a completion mechanism for them to wait for any active
invalidation affecting the range.

This also changes how hmm_range_snapshot() and hmm_range_fault() works by
not relying on vma so that we can drop the mmap_sem when waiting and
lookup the vma again on retry.

Link: http://lkml.kernel.org/r/20190403193318.16478-7-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a3e0d41c2b1f86b483b202d642140d8b86d677ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/vm/hmm.rst
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc Documentation/vm/hmm.rst
index cdf3911582c8,945d5fb6d14a..000000000000
--- a/Documentation/vm/hmm.rst
+++ b/Documentation/vm/hmm.rst
@@@ -220,19 -210,50 +220,53 @@@ respect in order to keep things properl
   {
        struct hmm_range range;
        ...
++<<<<<<< HEAD
 + again:
 +      ret = hmm_vma_get_pfns(vma, &range, start, end, pfns);
 +      if (ret)
++=======
+ 
+       range.start = ...;
+       range.end = ...;
+       range.pfns = ...;
+       range.flags = ...;
+       range.values = ...;
+       range.pfn_shift = ...;
+       hmm_range_register(&range);
+ 
+       /*
+        * Just wait for range to be valid, safe to ignore return value as we
+        * will use the return value of hmm_range_snapshot() below under the
+        * mmap_sem to ascertain the validity of the range.
+        */
+       hmm_range_wait_until_valid(&range, TIMEOUT_IN_MSEC);
+ 
+  again:
+       down_read(&mm->mmap_sem);
+       ret = hmm_range_snapshot(&range);
+       if (ret) {
+           up_read(&mm->mmap_sem);
+           if (ret == -EAGAIN) {
+             /*
+              * No need to check hmm_range_wait_until_valid() return value
+              * on retry we will get proper error with hmm_range_snapshot()
+              */
+             hmm_range_wait_until_valid(&range, TIMEOUT_IN_MSEC);
+             goto again;
+           }
+           hmm_mirror_unregister(&range);
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
            return ret;
 -      }
        take_lock(driver->update);
-       if (!hmm_vma_range_done(vma, &range)) {
+       if (!range.valid) {
            release_lock(driver->update);
 -          up_read(&mm->mmap_sem);
            goto again;
        }
  
        // Use pfns array content to update device page table
  
+       hmm_mirror_unregister(&range);
        release_lock(driver->update);
 -      up_read(&mm->mmap_sem);
        return 0;
   }
  
diff --cc include/linux/hmm.h
index e51dc3e6210f,ec4bfa91648f..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -354,41 -415,67 +412,89 @@@ void hmm_mirror_unregister(struct hmm_m
  
  
  /*
++<<<<<<< HEAD
 + * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device
 + * driver lock that serializes device page table updates, then call
 + * hmm_vma_range_done(), to check if the snapshot is still valid. The same
 + * device driver page table update lock must also be used in the
 + * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page
 + * table invalidation serializes on it.
 + *
 + * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL
 + * hmm_vma_get_pfns() WITHOUT ERROR !
 + *
 + * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !
 + */
 +int hmm_vma_get_pfns(struct hmm_range *range);
 +bool hmm_vma_range_done(struct hmm_range *range);
 +
++=======
+  * Please see Documentation/vm/hmm.rst for how to use the range API.
+  */
+ int hmm_range_register(struct hmm_range *range,
+ 		       struct mm_struct *mm,
+ 		       unsigned long start,
+ 		       unsigned long end);
+ void hmm_range_unregister(struct hmm_range *range);
+ long hmm_range_snapshot(struct hmm_range *range);
+ long hmm_range_fault(struct hmm_range *range, bool block);
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  
  /*
-  * Fault memory on behalf of device driver. Unlike handle_mm_fault(), this will
-  * not migrate any device memory back to system memory. The HMM pfn array will
-  * be updated with the fault result and current snapshot of the CPU page table
-  * for the range.
-  *
-  * The mmap_sem must be taken in read mode before entering and it might be
-  * dropped by the function if the block argument is false. In that case, the
-  * function returns -EAGAIN.
-  *
-  * Return value does not reflect if the fault was successful for every single
-  * address or not. Therefore, the caller must to inspect the HMM pfn array to
-  * determine fault status for each address.
-  *
-  * Trying to fault inside an invalid vma will result in -EINVAL.
+  * HMM_RANGE_DEFAULT_TIMEOUT - default timeout (ms) when waiting for a range
   *
-  * See the function description in mm/hmm.c for further documentation.
+  * When waiting for mmu notifiers we need some kind of time out otherwise we
+  * could potentialy wait for ever, 1000ms ie 1s sounds like a long time to
+  * wait already.
   */
++<<<<<<< HEAD
 +int hmm_vma_fault(struct hmm_range *range, bool block);
++=======
+ #define HMM_RANGE_DEFAULT_TIMEOUT 1000
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline bool hmm_vma_range_done(struct hmm_range *range)
+ {
+ 	bool ret = hmm_range_valid(range);
+ 
+ 	hmm_range_unregister(range);
+ 	return ret;
+ }
+ 
+ /* This is a temporary helper to avoid merge conflict between trees. */
+ static inline int hmm_vma_fault(struct hmm_range *range, bool block)
+ {
+ 	long ret;
+ 
+ 	ret = hmm_range_register(range, range->vma->vm_mm,
+ 				 range->start, range->end);
+ 	if (ret)
+ 		return (int)ret;
+ 
+ 	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
+ 		/*
+ 		 * The mmap_sem was taken by driver we release it here and
+ 		 * returns -EAGAIN which correspond to mmap_sem have been
+ 		 * drop in the old API.
+ 		 */
+ 		up_read(&range->vma->vm_mm->mmap_sem);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	ret = hmm_range_fault(range, block);
+ 	if (ret <= 0) {
+ 		if (ret == -EBUSY || !ret) {
+ 			/* Same as above  drop mmap_sem to match old API. */
+ 			up_read(&range->vma->vm_mm->mmap_sem);
+ 			ret = -EBUSY;
+ 		} else if (ret == -EAGAIN)
+ 			ret = -EBUSY;
+ 		hmm_range_unregister(range);
+ 		return ret;
+ 	}
+ 	return 0;
+ }
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  
  /* Below are for HMM internal use only! Not to be used by device driver! */
  void hmm_mm_destroy(struct mm_struct *mm);
diff --cc mm/hmm.c
index 4c052ccc4e21,3e07f32b94f8..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -38,57 -38,48 +38,80 @@@
  #if IS_ENABLED(CONFIG_HMM_MIRROR)
  static const struct mmu_notifier_ops hmm_mmu_notifier_ops;
  
++<<<<<<< HEAD
 +/*
 + * struct hmm - HMM per mm struct
 + *
 + * @mm: mm struct this HMM struct is bound to
 + * @lock: lock protecting ranges list
 + * @sequence: we track updates to the CPU page table with a sequence number
 + * @ranges: list of range being snapshotted
 + * @mirrors: list of mirrors for this mm
 + * @mmu_notifier: mmu notifier to track updates to CPU page table
 + * @mirrors_sem: read/write semaphore protecting the mirrors list
 + */
 +struct hmm {
 +	struct mm_struct	*mm;
 +	spinlock_t		lock;
 +	atomic_t		sequence;
 +	struct list_head	ranges;
 +	struct list_head	mirrors;
 +	struct mmu_notifier	mmu_notifier;
 +	struct rw_semaphore	mirrors_sem;
 +};
 +
 +/*
 + * hmm_register - register HMM against an mm (HMM internal)
++=======
+ static inline struct hmm *mm_get_hmm(struct mm_struct *mm)
+ {
+ 	struct hmm *hmm = READ_ONCE(mm->hmm);
+ 
+ 	if (hmm && kref_get_unless_zero(&hmm->kref))
+ 		return hmm;
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * hmm_get_or_create - register HMM against an mm (HMM internal)
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
   *
   * @mm: mm struct to attach to
 - * Returns: returns an HMM object, either by referencing the existing
 - *          (per-process) object, or by creating a new one.
   *
 - * This is not intended to be used directly by device drivers. If mm already
 - * has an HMM struct then it get a reference on it and returns it. Otherwise
 - * it allocates an HMM struct, initializes it, associate it with the mm and
 - * returns it.
 + * This is not intended to be used directly by device drivers. It allocates an
 + * HMM struct if mm does not have one, and initializes it.
   */
 -static struct hmm *hmm_get_or_create(struct mm_struct *mm)
 +static struct hmm *hmm_register(struct mm_struct *mm)
  {
 -	struct hmm *hmm = mm_get_hmm(mm);
 +	struct hmm *hmm = READ_ONCE(mm->hmm);
  	bool cleanup = false;
  
 +	/*
 +	 * The hmm struct can only be freed once the mm_struct goes away,
 +	 * hence we should always have pre-allocated an new hmm struct
 +	 * above.
 +	 */
  	if (hmm)
  		return hmm;
  
  	hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);
  	if (!hmm)
  		return NULL;
+ 	init_waitqueue_head(&hmm->wq);
  	INIT_LIST_HEAD(&hmm->mirrors);
  	init_rwsem(&hmm->mirrors_sem);
 +	atomic_set(&hmm->sequence, 0);
  	hmm->mmu_notifier.ops = NULL;
  	INIT_LIST_HEAD(&hmm->ranges);
++<<<<<<< HEAD
 +	spin_lock_init(&hmm->lock);
++=======
+ 	mutex_init(&hmm->lock);
+ 	kref_init(&hmm->kref);
+ 	hmm->notifiers = 0;
+ 	hmm->dead = false;
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  	hmm->mm = mm;
  
  	spin_lock(&mm->page_table_lock);
@@@ -121,45 -112,60 +144,72 @@@ error
  	return NULL;
  }
  
 -static void hmm_free(struct kref *kref)
 -{
 -	struct hmm *hmm = container_of(kref, struct hmm, kref);
 -	struct mm_struct *mm = hmm->mm;
 -
 -	mmu_notifier_unregister_no_release(&hmm->mmu_notifier, mm);
 -
 -	spin_lock(&mm->page_table_lock);
 -	if (mm->hmm == hmm)
 -		mm->hmm = NULL;
 -	spin_unlock(&mm->page_table_lock);
 -
 -	kfree(hmm);
 -}
 -
 -static inline void hmm_put(struct hmm *hmm)
 +void hmm_mm_destroy(struct mm_struct *mm)
  {
 -	kref_put(&hmm->kref, hmm_free);
++<<<<<<< HEAD
 +	kfree(mm->hmm);
  }
  
 -void hmm_mm_destroy(struct mm_struct *mm)
 -{
 +static int hmm_invalidate_range(struct hmm *hmm,
 +				const struct hmm_update *update)
++=======
+ 	struct hmm *hmm;
+ 
+ 	spin_lock(&mm->page_table_lock);
+ 	hmm = mm_get_hmm(mm);
+ 	mm->hmm = NULL;
+ 	if (hmm) {
+ 		hmm->mm = NULL;
+ 		hmm->dead = true;
+ 		spin_unlock(&mm->page_table_lock);
+ 		hmm_put(hmm);
+ 		return;
+ 	}
+ 
+ 	spin_unlock(&mm->page_table_lock);
+ }
+ 
+ static void hmm_release(struct mmu_notifier *mn, struct mm_struct *mm)
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  {
+ 	struct hmm *hmm = mm_get_hmm(mm);
  	struct hmm_mirror *mirror;
  	struct hmm_range *range;
  
- 	spin_lock(&hmm->lock);
- 	list_for_each_entry(range, &hmm->ranges, list) {
- 		if (update->end < range->start || update->start >= range->end)
- 			continue;
+ 	/* Report this HMM as dying. */
+ 	hmm->dead = true;
  
+ 	/* Wake-up everyone waiting on any range. */
+ 	mutex_lock(&hmm->lock);
+ 	list_for_each_entry(range, &hmm->ranges, list) {
  		range->valid = false;
  	}
++<<<<<<< HEAD
 +	spin_unlock(&hmm->lock);
 +
 +	down_read(&hmm->mirrors_sem);
 +	list_for_each_entry(mirror, &hmm->mirrors, list) {
 +		int ret;
 +
 +		ret = mirror->ops->sync_cpu_device_pagetables(mirror, update);
 +		if (!update->blockable && ret == -EAGAIN) {
 +			up_read(&hmm->mirrors_sem);
 +			return -EAGAIN;
 +		}
 +	}
 +	up_read(&hmm->mirrors_sem);
 +
 +	return 0;
 +}
 +
 +static void hmm_release(struct mmu_notifier *mn, struct mm_struct *mm)
 +{
 +	struct hmm_mirror *mirror;
 +	struct hmm *hmm = mm->hmm;
++=======
+ 	wake_up_all(&hmm->wq);
+ 	mutex_unlock(&hmm->lock);
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  
  	down_write(&hmm->mirrors_sem);
  	mirror = list_first_entry_or_null(&hmm->mirrors, struct hmm_mirror,
@@@ -180,35 -186,86 +230,114 @@@
  						  struct hmm_mirror, list);
  	}
  	up_write(&hmm->mirrors_sem);
 +}
  
 -	hmm_put(hmm);
++<<<<<<< HEAD
 +static void hmm_invalidate_range_start(struct mmu_notifier *mn,
 +				       struct mm_struct *mm,
 +				       unsigned long start,
 +				       unsigned long end)
 +{
 +	struct hmm *hmm = mm->hmm;
 +
 +	VM_BUG_ON(!hmm);
 +
 +	atomic_inc(&hmm->sequence);
  }
  
 +static void hmm_invalidate_range_end(struct mmu_notifier *mn,
 +				     struct mm_struct *mm,
 +				     unsigned long start,
 +				     unsigned long end)
 +{
 +	struct hmm_update update;
 +	struct hmm *hmm = mm->hmm;
 +
 +	VM_BUG_ON(!hmm);
 +
 +	update.start = start;
 +	update.end = end;
 +	update.event = HMM_UPDATE_INVALIDATE;
 +	update.blockable = true;
 +	hmm_invalidate_range(hmm, &update);
++=======
+ static int hmm_invalidate_range_start(struct mmu_notifier *mn,
+ 			const struct mmu_notifier_range *nrange)
+ {
+ 	struct hmm *hmm = mm_get_hmm(nrange->mm);
+ 	struct hmm_mirror *mirror;
+ 	struct hmm_update update;
+ 	struct hmm_range *range;
+ 	int ret = 0;
+ 
+ 	VM_BUG_ON(!hmm);
+ 
+ 	update.start = nrange->start;
+ 	update.end = nrange->end;
+ 	update.event = HMM_UPDATE_INVALIDATE;
+ 	update.blockable = nrange->blockable;
+ 
+ 	if (nrange->blockable)
+ 		mutex_lock(&hmm->lock);
+ 	else if (!mutex_trylock(&hmm->lock)) {
+ 		ret = -EAGAIN;
+ 		goto out;
+ 	}
+ 	hmm->notifiers++;
+ 	list_for_each_entry(range, &hmm->ranges, list) {
+ 		if (update.end < range->start || update.start >= range->end)
+ 			continue;
+ 
+ 		range->valid = false;
+ 	}
+ 	mutex_unlock(&hmm->lock);
+ 
+ 	if (nrange->blockable)
+ 		down_read(&hmm->mirrors_sem);
+ 	else if (!down_read_trylock(&hmm->mirrors_sem)) {
+ 		ret = -EAGAIN;
+ 		goto out;
+ 	}
+ 	list_for_each_entry(mirror, &hmm->mirrors, list) {
+ 		int ret;
+ 
+ 		ret = mirror->ops->sync_cpu_device_pagetables(mirror, &update);
+ 		if (!update.blockable && ret == -EAGAIN) {
+ 			up_read(&hmm->mirrors_sem);
+ 			ret = -EAGAIN;
+ 			goto out;
+ 		}
+ 	}
+ 	up_read(&hmm->mirrors_sem);
+ 
+ out:
+ 	hmm_put(hmm);
+ 	return ret;
+ }
+ 
+ static void hmm_invalidate_range_end(struct mmu_notifier *mn,
+ 			const struct mmu_notifier_range *nrange)
+ {
+ 	struct hmm *hmm = mm_get_hmm(nrange->mm);
+ 
+ 	VM_BUG_ON(!hmm);
+ 
+ 	mutex_lock(&hmm->lock);
+ 	hmm->notifiers--;
+ 	if (!hmm->notifiers) {
+ 		struct hmm_range *range;
+ 
+ 		list_for_each_entry(range, &hmm->ranges, list) {
+ 			if (range->valid)
+ 				continue;
+ 			range->valid = true;
+ 		}
+ 		wake_up_all(&hmm->wq);
+ 	}
+ 	mutex_unlock(&hmm->lock);
+ 
+ 	hmm_put(hmm);
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  }
  
  static const struct mmu_notifier_ops hmm_mmu_notifier_ops = {
@@@ -678,270 -713,263 +807,489 @@@ static void hmm_pfns_special(struct hmm
  }
  
  /*
++<<<<<<< HEAD
 + * hmm_vma_get_pfns() - snapshot CPU page table for a range of virtual addresses
 + * @range: range being snapshotted
 + * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          vma permission, 0 success
 + *
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See hmm_vma_range_done() for further
 + * information.
 + *
 + * The range struct is initialized here. It tracks the CPU page table, but only
 + * if the function returns success (0), in which case the caller must then call
 + * hmm_vma_range_done() to stop CPU page table update tracking on this range.
 + *
 + * NOT CALLING hmm_vma_range_done() IF FUNCTION RETURNS 0 WILL LEAD TO SERIOUS
 + * MEMORY CORRUPTION ! YOU HAVE BEEN WARNED !
 + */
 +int hmm_vma_get_pfns(struct hmm_range *range)
 +{
 +	struct vm_area_struct *vma = range->vma;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct mm_walk mm_walk;
 +	struct hmm *hmm;
 +
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
 +
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm)
 +		return -ENOMEM;
 +	/* Caller must have registered a mirror, via hmm_mirror_register() ! */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
 +
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
 +
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
++=======
+  * hmm_range_register() - start tracking change to CPU page table over a range
+  * @range: range
+  * @mm: the mm struct for the range of virtual address
+  * @start: start virtual address (inclusive)
+  * @end: end virtual address (exclusive)
+  * Returns 0 on success, -EFAULT if the address space is no longer valid
+  *
+  * Track updates to the CPU page table see include/linux/hmm.h
+  */
+ int hmm_range_register(struct hmm_range *range,
+ 		       struct mm_struct *mm,
+ 		       unsigned long start,
+ 		       unsigned long end)
+ {
+ 	range->start = start & PAGE_MASK;
+ 	range->end = end & PAGE_MASK;
+ 	range->valid = false;
+ 	range->hmm = NULL;
+ 
+ 	if (range->start >= range->end)
+ 		return -EINVAL;
+ 
+ 	range->start = start;
+ 	range->end = end;
+ 
+ 	range->hmm = hmm_get_or_create(mm);
+ 	if (!range->hmm)
+ 		return -EFAULT;
+ 
+ 	/* Check if hmm_mm_destroy() was call. */
+ 	if (range->hmm->mm == NULL || range->hmm->dead) {
+ 		hmm_put(range->hmm);
+ 		return -EFAULT;
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  	}
  
  	/* Initialize range to track CPU page table update */
- 	spin_lock(&hmm->lock);
- 	range->valid = true;
- 	list_add_rcu(&range->list, &hmm->ranges);
- 	spin_unlock(&hmm->lock);
+ 	mutex_lock(&range->hmm->lock);
  
++<<<<<<< HEAD
 +	hmm_vma_walk.fault = false;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	walk_page_range(range->start, range->end, &mm_walk);
++	return 0;
++=======
+ 	list_add_rcu(&range->list, &range->hmm->ranges);
+ 
+ 	/*
+ 	 * If there are any concurrent notifiers we have to wait for them for
+ 	 * the range to be valid (see hmm_range_wait_until_valid()).
+ 	 */
+ 	if (!range->hmm->notifiers)
+ 		range->valid = true;
+ 	mutex_unlock(&range->hmm->lock);
+ 
  	return 0;
  }
+ EXPORT_SYMBOL(hmm_range_register);
+ 
+ /*
+  * hmm_range_unregister() - stop tracking change to CPU page table over a range
+  * @range: range
+  *
+  * Range struct is used to track updates to the CPU page table after a call to
+  * hmm_range_register(). See include/linux/hmm.h for how to use it.
+  */
+ void hmm_range_unregister(struct hmm_range *range)
+ {
+ 	/* Sanity check this really should not happen. */
+ 	if (range->hmm == NULL || range->end <= range->start)
+ 		return;
+ 
+ 	mutex_lock(&range->hmm->lock);
+ 	list_del_rcu(&range->list);
+ 	mutex_unlock(&range->hmm->lock);
+ 
+ 	/* Drop reference taken by hmm_range_register() */
+ 	range->valid = false;
+ 	hmm_put(range->hmm);
+ 	range->hmm = NULL;
+ }
+ EXPORT_SYMBOL(hmm_range_unregister);
+ 
+ /*
+  * hmm_range_snapshot() - snapshot CPU page table for a range
+  * @range: range
+  * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
+  *          permission (for instance asking for write and range is read only),
+  *          -EAGAIN if you need to retry, -EFAULT invalid (ie either no valid
+  *          vma or it is illegal to access that range), number of valid pages
+  *          in range->pfns[] (from range start address).
+  *
+  * This snapshots the CPU page table for a range of virtual addresses. Snapshot
+  * validity is tracked by range struct. See in include/linux/hmm.h for example
+  * on how to use.
+  */
+ long hmm_range_snapshot(struct hmm_range *range)
+ {
+ 	unsigned long start = range->start, end;
+ 	struct hmm_vma_walk hmm_vma_walk;
+ 	struct hmm *hmm = range->hmm;
+ 	struct vm_area_struct *vma;
+ 	struct mm_walk mm_walk;
+ 
+ 	/* Check if hmm_mm_destroy() was call. */
+ 	if (hmm->mm == NULL || hmm->dead)
+ 		return -EFAULT;
+ 
+ 	do {
+ 		/* If range is no longer valid force retry. */
+ 		if (!range->valid)
+ 			return -EAGAIN;
+ 
+ 		vma = find_vma(hmm->mm, start);
+ 		if (vma == NULL || (vma->vm_flags & VM_SPECIAL))
+ 			return -EFAULT;
+ 
+ 		/* FIXME support hugetlb fs/dax */
+ 		if (is_vm_hugetlb_page(vma) || vma_is_dax(vma)) {
+ 			hmm_pfns_special(range);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = false;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 
+ 		walk_page_range(start, end, &mm_walk);
+ 		start = end;
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
++}
 +EXPORT_SYMBOL(hmm_vma_get_pfns);
 +
 +/*
++<<<<<<< HEAD
 + * hmm_vma_range_done() - stop tracking change to CPU page table over a range
 + * @range: range being tracked
 + * Returns: false if range data has been invalidated, true otherwise
 + *
 + * Range struct is used to track updates to the CPU page table after a call to
 + * either hmm_vma_get_pfns() or hmm_vma_fault(). Once the device driver is done
 + * using the data,  or wants to lock updates to the data it got from those
 + * functions, it must call the hmm_vma_range_done() function, which will then
 + * stop tracking CPU page table updates.
 + *
 + * Note that device driver must still implement general CPU page table update
 + * tracking either by using hmm_mirror (see hmm_mirror_register()) or by using
 + * the mmu_notifier API directly.
 + *
 + * CPU page table update tracking done through hmm_range is only temporary and
 + * to be used while trying to duplicate CPU page table contents for a range of
 + * virtual addresses.
 + *
 + * There are two ways to use this :
 + * again:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   trans = device_build_page_table_update_transaction(pfns);
 + *   device_page_table_lock();
 + *   if (!hmm_vma_range_done(range)) {
 + *     device_page_table_unlock();
 + *     goto again;
 + *   }
 + *   device_commit_transaction(trans);
 + *   device_page_table_unlock();
 + *
 + * Or:
 + *   hmm_vma_get_pfns(range); or hmm_vma_fault(...);
 + *   device_page_table_lock();
 + *   hmm_vma_range_done(range);
 + *   device_update_page_table(range->pfns);
 + *   device_page_table_unlock();
 + */
 +bool hmm_vma_range_done(struct hmm_range *range)
 +{
 +	unsigned long npages = (range->end - range->start) >> PAGE_SHIFT;
 +	struct hmm *hmm;
 +
 +	if (range->end <= range->start) {
 +		BUG();
 +		return false;
 +	}
 +
 +	hmm = hmm_register(range->vma->vm_mm);
 +	if (!hmm) {
 +		memset(range->pfns, 0, sizeof(*range->pfns) * npages);
 +		return false;
 +	}
 +
 +	spin_lock(&hmm->lock);
 +	list_del_rcu(&range->list);
 +	spin_unlock(&hmm->lock);
 +
 +	return range->valid;
  }
 -EXPORT_SYMBOL(hmm_range_snapshot);
 +EXPORT_SYMBOL(hmm_vma_range_done);
  
  /*
 + * hmm_vma_fault() - try to fault some address in a virtual address range
++=======
+  * hmm_range_fault() - try to fault some address in a virtual address range
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
   * @range: range being faulted
   * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
 - * Returns: number of valid pages in range->pfns[] (from range start
 - *          address). This may be zero. If the return value is negative,
 - *          then one of the following values may be returned:
 - *
 - *           -EINVAL  invalid arguments or mm or virtual address are in an
 - *                    invalid vma (ie either hugetlbfs or device file vma).
 - *           -ENOMEM: Out of memory.
 - *           -EPERM:  Invalid permission (for instance asking for write and
 - *                    range is read only).
 - *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
 - *                    happens if block argument is false.
 - *           -EBUSY:  If the the range is being invalidated and you should wait
 - *                    for invalidation to finish.
 - *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
 - *                    that range), number of valid pages in range->pfns[] (from
 - *                    range start address).
 + * Returns: 0 success, error otherwise (-EAGAIN means mmap_sem have been drop)
   *
   * This is similar to a regular CPU page fault except that it will not trigger
 - * any memory migration if the memory being faulted is not accessible by CPUs
 - * and caller does not ask for migration.
 + * any memory migration if the memory being faulted is not accessible by CPUs.
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
 + *
 + * Expected use pattern:
 + * retry:
 + *   down_read(&mm->mmap_sem);
 + *   // Find vma and address device wants to fault, initialize hmm_pfn_t
 + *   // array accordingly
 + *   ret = hmm_vma_fault(range, write, block);
 + *   switch (ret) {
 + *   case -EAGAIN:
 + *     hmm_vma_range_done(range);
 + *     // You might want to rate limit or yield to play nicely, you may
 + *     // also commit any valid pfn in the array assuming that you are
 + *     // getting true from hmm_vma_range_monitor_end()
 + *     goto retry;
 + *   case 0:
 + *     break;
 + *   case -ENOMEM:
 + *   case -EINVAL:
 + *   case -EPERM:
 + *   default:
 + *     // Handle error !
 + *     up_read(&mm->mmap_sem)
 + *     return;
 + *   }
 + *   // Take device driver lock that serialize device page table update
 + *   driver_lock_device_page_table_update();
 + *   hmm_vma_range_done(range);
 + *   // Commit pfns we got from hmm_vma_fault()
 + *   driver_unlock_device_page_table_update();
 + *   up_read(&mm->mmap_sem)
 + *
 + * YOU MUST CALL hmm_vma_range_done() AFTER THIS FUNCTION RETURN SUCCESS (0)
 + * BEFORE FREEING THE range struct OR YOU WILL HAVE SERIOUS MEMORY CORRUPTION !
 + *
 + * YOU HAVE BEEN WARNED !
   */
 -long hmm_range_fault(struct hmm_range *range, bool block)
 +int hmm_vma_fault(struct hmm_range *range, bool block)
  {
- 	struct vm_area_struct *vma = range->vma;
- 	unsigned long start = range->start;
+ 	unsigned long start = range->start, end;
  	struct hmm_vma_walk hmm_vma_walk;
+ 	struct hmm *hmm = range->hmm;
+ 	struct vm_area_struct *vma;
  	struct mm_walk mm_walk;
- 	struct hmm *hmm;
  	int ret;
  
++<<<<<<< HEAD
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
 +
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm) {
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -ENOMEM;
 +	}
 +	/* Caller must have registered a mirror using hmm_mirror_register() */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
 +
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
 +
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
 +	}
 +
 +	/* Initialize range to track CPU page table update */
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = true;
 +	hmm_vma_walk.block = block;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +	hmm_vma_walk.last = range->start;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	do {
 +		ret = walk_page_range(start, range->end, &mm_walk);
 +		start = hmm_vma_walk.last;
 +	} while (ret == -EAGAIN);
++=======
+ 	/* Check if hmm_mm_destroy() was call. */
+ 	if (hmm->mm == NULL || hmm->dead)
+ 		return -EFAULT;
+ 
+ 	do {
+ 		/* If range is no longer valid force retry. */
+ 		if (!range->valid) {
+ 			up_read(&hmm->mm->mmap_sem);
+ 			return -EAGAIN;
+ 		}
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  
- 	if (ret) {
- 		unsigned long i;
+ 		vma = find_vma(hmm->mm, start);
+ 		if (vma == NULL || (vma->vm_flags & VM_SPECIAL))
+ 			return -EFAULT;
  
++<<<<<<< HEAD
 +		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		hmm_pfns_clear(range, &range->pfns[i], hmm_vma_walk.last,
 +			       range->end);
 +		hmm_vma_range_done(range);
 +	}
 +	return ret;
++=======
+ 		/* FIXME support hugetlb fs/dax */
+ 		if (is_vm_hugetlb_page(vma) || vma_is_dax(vma)) {
+ 			hmm_pfns_special(range);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = true;
+ 		hmm_vma_walk.block = block;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 
+ 		do {
+ 			ret = walk_page_range(start, end, &mm_walk);
+ 			start = hmm_vma_walk.last;
+ 
+ 			/* Keep trying while the range is valid. */
+ 		} while (ret == -EBUSY && range->valid);
+ 
+ 		if (ret) {
+ 			unsigned long i;
+ 
+ 			i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
+ 			hmm_pfns_clear(range, &range->pfns[i],
+ 				hmm_vma_walk.last, range->end);
+ 			return ret;
+ 		}
+ 		start = end;
+ 
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
++>>>>>>> a3e0d41c2b1f (mm/hmm: improve driver API to work and wait over a range)
  }
 -EXPORT_SYMBOL(hmm_range_fault);
 +EXPORT_SYMBOL(hmm_vma_fault);
  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
  
* Unmerged path Documentation/vm/hmm.rst
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
