selftests: kvm: consolidate VMX support checks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 9143613ef0ba9f88d2fef9038930637a0909d35a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/9143613e.failed

vmx_* tests require VMX and three of them implement the same check. Move it
to vmx library.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 9143613ef0ba9f88d2fef9038930637a0909d35a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/kvm/include/x86_64/vmx.h
#	tools/testing/selftests/kvm/lib/x86_64/vmx.c
diff --cc tools/testing/selftests/kvm/include/x86_64/vmx.h
index c9bd935b939c,f52e0ba84fed..000000000000
--- a/tools/testing/selftests/kvm/include/x86_64/vmx.h
+++ b/tools/testing/selftests/kvm/include/x86_64/vmx.h
@@@ -578,4 -580,16 +578,19 @@@ bool prepare_for_vmx_operation(struct v
  void prepare_vmcs(struct vmx_pages *vmx, void *guest_rip, void *guest_rsp);
  bool load_vmcs(struct vmx_pages *vmx);
  
++<<<<<<< HEAD
++=======
+ void nested_vmx_check_supported(void);
+ 
+ void nested_pg_map(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 		   uint64_t nested_paddr, uint64_t paddr, uint32_t eptp_memslot);
+ void nested_map(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 		 uint64_t nested_paddr, uint64_t paddr, uint64_t size,
+ 		 uint32_t eptp_memslot);
+ void nested_map_memslot(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 			uint32_t memslot, uint32_t eptp_memslot);
+ void prepare_eptp(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 		  uint32_t eptp_memslot);
+ 
++>>>>>>> 9143613ef0ba (selftests: kvm: consolidate VMX support checks)
  #endif /* SELFTEST_KVM_VMX_H */
diff --cc tools/testing/selftests/kvm/lib/x86_64/vmx.c
index a95dcf858f36,f6ec97b7eaef..000000000000
--- a/tools/testing/selftests/kvm/lib/x86_64/vmx.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/vmx.c
@@@ -328,3 -375,162 +328,165 @@@ void prepare_vmcs(struct vmx_pages *vmx
  	init_vmcs_host_state();
  	init_vmcs_guest_state(guest_rip, guest_rsp);
  }
++<<<<<<< HEAD
++=======
+ 
+ void nested_vmx_check_supported(void)
+ {
+ 	struct kvm_cpuid_entry2 *entry = kvm_get_supported_cpuid_entry(1);
+ 
+ 	if (!(entry->ecx & CPUID_VMX)) {
+ 		fprintf(stderr, "nested VMX not enabled, skipping test\n");
+ 		exit(KSFT_SKIP);
+ 	}
+ }
+ 
+ void nested_pg_map(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 	 	   uint64_t nested_paddr, uint64_t paddr, uint32_t eptp_memslot)
+ {
+ 	uint16_t index[4];
+ 	struct eptPageTableEntry *pml4e;
+ 
+ 	TEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, "Attempt to use "
+ 		    "unknown or unsupported guest mode, mode: 0x%x", vm->mode);
+ 
+ 	TEST_ASSERT((nested_paddr % vm->page_size) == 0,
+ 		    "Nested physical address not on page boundary,\n"
+ 		    "  nested_paddr: 0x%lx vm->page_size: 0x%x",
+ 		    nested_paddr, vm->page_size);
+ 	TEST_ASSERT((nested_paddr >> vm->page_shift) <= vm->max_gfn,
+ 		    "Physical address beyond beyond maximum supported,\n"
+ 		    "  nested_paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
+ 		    paddr, vm->max_gfn, vm->page_size);
+ 	TEST_ASSERT((paddr % vm->page_size) == 0,
+ 		    "Physical address not on page boundary,\n"
+ 		    "  paddr: 0x%lx vm->page_size: 0x%x",
+ 		    paddr, vm->page_size);
+ 	TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
+ 		    "Physical address beyond beyond maximum supported,\n"
+ 		    "  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
+ 		    paddr, vm->max_gfn, vm->page_size);
+ 
+ 	index[0] = (nested_paddr >> 12) & 0x1ffu;
+ 	index[1] = (nested_paddr >> 21) & 0x1ffu;
+ 	index[2] = (nested_paddr >> 30) & 0x1ffu;
+ 	index[3] = (nested_paddr >> 39) & 0x1ffu;
+ 
+ 	/* Allocate page directory pointer table if not present. */
+ 	pml4e = vmx->eptp_hva;
+ 	if (!pml4e[index[3]].readable) {
+ 		pml4e[index[3]].address = vm_phy_page_alloc(vm,
+ 			  KVM_EPT_PAGE_TABLE_MIN_PADDR, eptp_memslot)
+ 			>> vm->page_shift;
+ 		pml4e[index[3]].writable = true;
+ 		pml4e[index[3]].readable = true;
+ 		pml4e[index[3]].executable = true;
+ 	}
+ 
+ 	/* Allocate page directory table if not present. */
+ 	struct eptPageTableEntry *pdpe;
+ 	pdpe = addr_gpa2hva(vm, pml4e[index[3]].address * vm->page_size);
+ 	if (!pdpe[index[2]].readable) {
+ 		pdpe[index[2]].address = vm_phy_page_alloc(vm,
+ 			  KVM_EPT_PAGE_TABLE_MIN_PADDR, eptp_memslot)
+ 			>> vm->page_shift;
+ 		pdpe[index[2]].writable = true;
+ 		pdpe[index[2]].readable = true;
+ 		pdpe[index[2]].executable = true;
+ 	}
+ 
+ 	/* Allocate page table if not present. */
+ 	struct eptPageTableEntry *pde;
+ 	pde = addr_gpa2hva(vm, pdpe[index[2]].address * vm->page_size);
+ 	if (!pde[index[1]].readable) {
+ 		pde[index[1]].address = vm_phy_page_alloc(vm,
+ 			  KVM_EPT_PAGE_TABLE_MIN_PADDR, eptp_memslot)
+ 			>> vm->page_shift;
+ 		pde[index[1]].writable = true;
+ 		pde[index[1]].readable = true;
+ 		pde[index[1]].executable = true;
+ 	}
+ 
+ 	/* Fill in page table entry. */
+ 	struct eptPageTableEntry *pte;
+ 	pte = addr_gpa2hva(vm, pde[index[1]].address * vm->page_size);
+ 	pte[index[0]].address = paddr >> vm->page_shift;
+ 	pte[index[0]].writable = true;
+ 	pte[index[0]].readable = true;
+ 	pte[index[0]].executable = true;
+ 
+ 	/*
+ 	 * For now mark these as accessed and dirty because the only
+ 	 * testcase we have needs that.  Can be reconsidered later.
+ 	 */
+ 	pte[index[0]].accessed = true;
+ 	pte[index[0]].dirty = true;
+ }
+ 
+ /*
+  * Map a range of EPT guest physical addresses to the VM's physical address
+  *
+  * Input Args:
+  *   vm - Virtual Machine
+  *   nested_paddr - Nested guest physical address to map
+  *   paddr - VM Physical Address
+  *   size - The size of the range to map
+  *   eptp_memslot - Memory region slot for new virtual translation tables
+  *
+  * Output Args: None
+  *
+  * Return: None
+  *
+  * Within the VM given by vm, creates a nested guest translation for the
+  * page range starting at nested_paddr to the page range starting at paddr.
+  */
+ void nested_map(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 		uint64_t nested_paddr, uint64_t paddr, uint64_t size,
+ 		uint32_t eptp_memslot)
+ {
+ 	size_t page_size = vm->page_size;
+ 	size_t npages = size / page_size;
+ 
+ 	TEST_ASSERT(nested_paddr + size > nested_paddr, "Vaddr overflow");
+ 	TEST_ASSERT(paddr + size > paddr, "Paddr overflow");
+ 
+ 	while (npages--) {
+ 		nested_pg_map(vmx, vm, nested_paddr, paddr, eptp_memslot);
+ 		nested_paddr += page_size;
+ 		paddr += page_size;
+ 	}
+ }
+ 
+ /* Prepare an identity extended page table that maps all the
+  * physical pages in VM.
+  */
+ void nested_map_memslot(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 			uint32_t memslot, uint32_t eptp_memslot)
+ {
+ 	sparsebit_idx_t i, last;
+ 	struct userspace_mem_region *region =
+ 		memslot2region(vm, memslot);
+ 
+ 	i = (region->region.guest_phys_addr >> vm->page_shift) - 1;
+ 	last = i + (region->region.memory_size >> vm->page_shift);
+ 	for (;;) {
+ 		i = sparsebit_next_clear(region->unused_phy_pages, i);
+ 		if (i > last)
+ 			break;
+ 
+ 		nested_map(vmx, vm,
+ 			   (uint64_t)i << vm->page_shift,
+ 			   (uint64_t)i << vm->page_shift,
+ 			   1 << vm->page_shift,
+ 			   eptp_memslot);
+ 	}
+ }
+ 
+ void prepare_eptp(struct vmx_pages *vmx, struct kvm_vm *vm,
+ 		  uint32_t eptp_memslot)
+ {
+ 	vmx->eptp = (void *)vm_vaddr_alloc(vm, getpagesize(), 0x10000, 0, 0);
+ 	vmx->eptp_hva = addr_gva2hva(vm, (uintptr_t)vmx->eptp);
+ 	vmx->eptp_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->eptp);
+ }
++>>>>>>> 9143613ef0ba (selftests: kvm: consolidate VMX support checks)
* Unmerged path tools/testing/selftests/kvm/include/x86_64/vmx.h
* Unmerged path tools/testing/selftests/kvm/lib/x86_64/vmx.c
diff --git a/tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c b/tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c
index 97182b47b10c..b084d270998d 100644
--- a/tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c
+++ b/tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c
@@ -54,12 +54,8 @@ static void l1_guest_code(struct vmx_pages *vmx_pages)
 int main(int argc, char *argv[])
 {
 	vm_vaddr_t vmx_pages_gva;
-	struct kvm_cpuid_entry2 *entry = kvm_get_supported_cpuid_entry(1);
 
-	if (!(entry->ecx & CPUID_VMX)) {
-		fprintf(stderr, "nested VMX not enabled, skipping test\n");
-		exit(KSFT_SKIP);
-	}
+	nested_vmx_check_supported();
 
 	vm = vm_create_default(VCPU_ID, 0, (void *) l1_guest_code);
 	vcpu_set_cpuid(vm, VCPU_ID, kvm_get_supported_cpuid());
diff --git a/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c b/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
index f44269d12c03..057a5e7eaf09 100644
--- a/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
+++ b/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
@@ -225,7 +225,6 @@ int main(int argc, char *argv[])
 {
 	struct kvm_vm *vm;
 	struct kvm_nested_state state;
-	struct kvm_cpuid_entry2 *entry = kvm_get_supported_cpuid_entry(1);
 
 	have_evmcs = kvm_check_cap(KVM_CAP_HYPERV_ENLIGHTENED_VMCS);
 
@@ -238,10 +237,7 @@ int main(int argc, char *argv[])
 	 * AMD currently does not implement set_nested_state, so for now we
 	 * just early out.
 	 */
-	if (!(entry->ecx & CPUID_VMX)) {
-		fprintf(stderr, "nested VMX not enabled, skipping test\n");
-		exit(KSFT_SKIP);
-	}
+	nested_vmx_check_supported();
 
 	vm = vm_create_default(VCPU_ID, 0, 0);
 
diff --git a/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c b/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
index 6d37a3173956..b6afaef615a3 100644
--- a/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
+++ b/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
@@ -130,12 +130,8 @@ static void report(int64_t val)
 int main(int argc, char *argv[])
 {
 	vm_vaddr_t vmx_pages_gva;
-	struct kvm_cpuid_entry2 *entry = kvm_get_supported_cpuid_entry(1);
 
-	if (!(entry->ecx & CPUID_VMX)) {
-		fprintf(stderr, "nested VMX not enabled, skipping test\n");
-		exit(KSFT_SKIP);
-	}
+	nested_vmx_check_supported();
 
 	vm = vm_create_default(VCPU_ID, 0, (void *) l1_guest_code);
 	vcpu_set_cpuid(vm, VCPU_ID, kvm_get_supported_cpuid());
