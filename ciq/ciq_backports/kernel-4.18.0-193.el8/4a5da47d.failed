net: sched: take reference to psample group in flow_action infra

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [net] sched: take reference to psample group in flow_action infra (Ivan Vecera) [1739606]
Rebuild_FUZZ: 95.93%
commit-author Vlad Buslov <vladbu@mellanox.com>
commit 4a5da47d5cb6aba3c26a5cc0dddfb2d577e851e9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/4a5da47d.failed

With recent patch set that removed rtnl lock dependency from cls hardware
offload API rtnl lock is only taken when reading action data and can be
released after action-specific data is parsed into intermediate
representation. However, sample action psample group is passed by pointer
without obtaining reference to it first, which makes it possible to
concurrently overwrite the action and deallocate object pointed by
psample_group pointer after rtnl lock is released but before driver
finished using the pointer.

To prevent such race condition, obtain reference to psample group while it
is used by flow_action infra. Extend psample API with function
psample_group_take() that increments psample group reference counter.
Extend struct tc_action_ops with new get_psample_group() API. Implement the
API for action sample using psample_group_take() and already existing
psample_group_put() as a destructor. Use it in tc_setup_flow_action() to
take reference to psample group pointed to by entry->sample.psample_group
and release it in tc_cleanup_flow_action().

Disable bh when taking psample_groups_lock. The lock is now taken while
holding action tcf_lock that is used by data path and requires bh to be
disabled, so doing the same for psample_groups_lock is necessary to
preserve SOFTIRQ-irq-safety.

Fixes: 918190f50eb6 ("net: sched: flower: don't take rtnl lock for cls hw offloads API")
	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Acked-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 4a5da47d5cb6aba3c26a5cc0dddfb2d577e851e9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_api.c
diff --cc net/sched/cls_api.c
index 081ac3d0fb50,60d44b14750a..000000000000
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@@ -3266,13 -3084,261 +3266,236 @@@ int tc_setup_cb_call(struct tcf_block *
  	}
  	return ok_count;
  }
 -
 -int tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
 -		     void *type_data, bool err_stop, bool rtnl_held)
 -{
 -	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
 -	int ok_count;
 -
 -retry:
 -	if (take_rtnl)
 -		rtnl_lock();
 -	down_read(&block->cb_lock);
 -	/* Need to obtain rtnl lock if block is bound to devs that require it.
 -	 * In block bind code cb_lock is obtained while holding rtnl, so we must
 -	 * obtain the locks in same order here.
 -	 */
 -	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
 -		up_read(&block->cb_lock);
 -		take_rtnl = true;
 -		goto retry;
 -	}
 -
 -	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
 -
 -	up_read(&block->cb_lock);
 -	if (take_rtnl)
 -		rtnl_unlock();
 -	return ok_count;
 -}
  EXPORT_SYMBOL(tc_setup_cb_call);
  
++<<<<<<< HEAD
++=======
+ /* Non-destructive filter add. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offloads counter. On failure,
+  * previously offloaded filter is considered to be intact and offloads counter
+  * is not decremented.
+  */
+ 
+ int tc_setup_cb_add(struct tcf_block *block, struct tcf_proto *tp,
+ 		    enum tc_setup_type type, void *type_data, bool err_stop,
+ 		    u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags,
+ 					  ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_add);
+ 
+ /* Destructive filter replace. If filter that wasn't already in hardware is
+  * successfully offloaded, increment block offload counter. On failure,
+  * previously offloaded filter is considered to be destroyed and offload counter
+  * is decremented.
+  */
+ 
+ int tc_setup_cb_replace(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *old_flags, unsigned int *old_in_hw_count,
+ 			u32 *new_flags, unsigned int *new_in_hw_count,
+ 			bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	/* Make sure all netdevs sharing this block are offload-capable. */
+ 	if (block->nooffloaddevcnt && err_stop) {
+ 		ok_count = -EOPNOTSUPP;
+ 		goto err_unlock;
+ 	}
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, old_in_hw_count, old_flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 	if (ok_count < 0)
+ 		goto err_unlock;
+ 
+ 	if (tp->ops->hw_add)
+ 		tp->ops->hw_add(tp, type_data);
+ 	if (ok_count > 0)
+ 		tc_cls_offload_cnt_update(block, tp, new_in_hw_count,
+ 					  new_flags, ok_count, true);
+ err_unlock:
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_replace);
+ 
+ /* Destroy filter and decrement block offload counter, if filter was previously
+  * offloaded.
+  */
+ 
+ int tc_setup_cb_destroy(struct tcf_block *block, struct tcf_proto *tp,
+ 			enum tc_setup_type type, void *type_data, bool err_stop,
+ 			u32 *flags, unsigned int *in_hw_count, bool rtnl_held)
+ {
+ 	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
+ 	int ok_count;
+ 
+ retry:
+ 	if (take_rtnl)
+ 		rtnl_lock();
+ 	down_read(&block->cb_lock);
+ 	/* Need to obtain rtnl lock if block is bound to devs that require it.
+ 	 * In block bind code cb_lock is obtained while holding rtnl, so we must
+ 	 * obtain the locks in same order here.
+ 	 */
+ 	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
+ 		up_read(&block->cb_lock);
+ 		take_rtnl = true;
+ 		goto retry;
+ 	}
+ 
+ 	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);
+ 
+ 	tc_cls_offload_cnt_reset(block, tp, in_hw_count, flags);
+ 	if (tp->ops->hw_del)
+ 		tp->ops->hw_del(tp, type_data);
+ 
+ 	up_read(&block->cb_lock);
+ 	if (take_rtnl)
+ 		rtnl_unlock();
+ 	return ok_count < 0 ? ok_count : 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_destroy);
+ 
+ int tc_setup_cb_reoffload(struct tcf_block *block, struct tcf_proto *tp,
+ 			  bool add, flow_setup_cb_t *cb,
+ 			  enum tc_setup_type type, void *type_data,
+ 			  void *cb_priv, u32 *flags, unsigned int *in_hw_count)
+ {
+ 	int err = cb(type, type_data, cb_priv);
+ 
+ 	if (err) {
+ 		if (add && tc_skip_sw(*flags))
+ 			return err;
+ 	} else {
+ 		tc_cls_offload_cnt_update(block, tp, in_hw_count, flags, 1,
+ 					  add);
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tc_setup_cb_reoffload);
+ 
+ void tc_cleanup_flow_action(struct flow_action *flow_action)
+ {
+ 	struct flow_action_entry *entry;
+ 	int i;
+ 
+ 	flow_action_for_each(i, entry, flow_action)
+ 		if (entry->destructor)
+ 			entry->destructor(entry->destructor_priv);
+ }
+ EXPORT_SYMBOL(tc_cleanup_flow_action);
+ 
+ static void tcf_mirred_put_dev(void *priv)
+ {
+ 	struct net_device *dev = priv;
+ 
+ 	dev_put(dev);
+ }
+ 
+ static void tcf_mirred_get_dev(struct flow_action_entry *entry,
+ 			       const struct tc_action *act)
+ {
+ 	entry->dev = tcf_mirred_dev(act);
+ 	if (!entry->dev)
+ 		return;
+ 	dev_hold(entry->dev);
+ 	entry->destructor = tcf_mirred_put_dev;
+ 	entry->destructor_priv = entry->dev;
+ }
+ 
+ static void tcf_tunnel_encap_put_tunnel(void *priv)
+ {
+ 	struct ip_tunnel_info *tunnel = priv;
+ 
+ 	kfree(tunnel);
+ }
+ 
+ static int tcf_tunnel_encap_get_tunnel(struct flow_action_entry *entry,
+ 				       const struct tc_action *act)
+ {
+ 	entry->tunnel = tcf_tunnel_info_copy(act);
+ 	if (!entry->tunnel)
+ 		return -ENOMEM;
+ 	entry->destructor = tcf_tunnel_encap_put_tunnel;
+ 	entry->destructor_priv = entry->tunnel;
+ 	return 0;
+ }
+ 
+ static void tcf_sample_get_group(struct flow_action_entry *entry,
+ 				 const struct tc_action *act)
+ {
+ #ifdef CONFIG_NET_CLS_ACT
+ 	entry->sample.psample_group =
+ 		act->ops->get_psample_group(act, &entry->destructor);
+ 	entry->destructor_priv = entry->sample.psample_group;
+ #endif
+ }
+ 
++>>>>>>> 4a5da47d5cb6 (net: sched: take reference to psample group in flow_action infra)
  int tc_setup_flow_action(struct flow_action *flow_action,
 -			 const struct tcf_exts *exts, bool rtnl_held)
 +			 const struct tcf_exts *exts)
  {
  	const struct tc_action *act;
 -	int i, j, k, err = 0;
 +	int i, j, k;
  
  	if (!exts)
  		return 0;
diff --git a/include/net/act_api.h b/include/net/act_api.h
index c61a1bf4e3de..1b46f202a643 100644
--- a/include/net/act_api.h
+++ b/include/net/act_api.h
@@ -77,6 +77,8 @@ static inline void tcf_tm_dump(struct tcf_t *dtm, const struct tcf_t *stm)
 #define ACT_P_CREATED 1
 #define ACT_P_DELETED 1
 
+typedef void (*tc_action_priv_destructor)(void *priv);
+
 struct tc_action_ops {
 	struct list_head head;
 	char    kind[IFNAMSIZ];
@@ -100,6 +102,9 @@ struct tc_action_ops {
 	size_t  (*get_fill_size)(const struct tc_action *act);
 	struct net_device *(*get_dev)(const struct tc_action *a);
 	void	(*put_dev)(struct net_device *dev);
+	struct psample_group *
+	(*get_psample_group)(const struct tc_action *a,
+			     tc_action_priv_destructor *destructor);
 };
 
 struct tc_action_net {
diff --git a/include/net/psample.h b/include/net/psample.h
index 6b578ce69cd8..68ae16bb0a4a 100644
--- a/include/net/psample.h
+++ b/include/net/psample.h
@@ -15,6 +15,7 @@ struct psample_group {
 };
 
 struct psample_group *psample_group_get(struct net *net, u32 group_num);
+void psample_group_take(struct psample_group *group);
 void psample_group_put(struct psample_group *group);
 
 #if IS_ENABLED(CONFIG_PSAMPLE)
diff --git a/include/net/tc_act/tc_sample.h b/include/net/tc_act/tc_sample.h
index b4fce0fae645..b5d76305e854 100644
--- a/include/net/tc_act/tc_sample.h
+++ b/include/net/tc_act/tc_sample.h
@@ -41,10 +41,4 @@ static inline int tcf_sample_trunc_size(const struct tc_action *a)
 	return to_sample(a)->trunc_size;
 }
 
-static inline struct psample_group *
-tcf_sample_psample_group(const struct tc_action *a)
-{
-	return rcu_dereference_rtnl(to_sample(a)->psample_group);
-}
-
 #endif /* __NET_TC_SAMPLE_H */
diff --git a/net/psample/psample.c b/net/psample/psample.c
index 4cea353221da..9e6b41b6da53 100644
--- a/net/psample/psample.c
+++ b/net/psample/psample.c
@@ -76,7 +76,7 @@ static int psample_nl_cmd_get_group_dumpit(struct sk_buff *msg,
 	int idx = 0;
 	int err;
 
-	spin_lock(&psample_groups_lock);
+	spin_lock_bh(&psample_groups_lock);
 	list_for_each_entry(group, &psample_groups_list, list) {
 		if (!net_eq(group->net, sock_net(msg->sk)))
 			continue;
@@ -92,7 +92,7 @@ static int psample_nl_cmd_get_group_dumpit(struct sk_buff *msg,
 		idx++;
 	}
 
-	spin_unlock(&psample_groups_lock);
+	spin_unlock_bh(&psample_groups_lock);
 	cb->args[0] = idx;
 	return msg->len;
 }
@@ -174,7 +174,7 @@ struct psample_group *psample_group_get(struct net *net, u32 group_num)
 {
 	struct psample_group *group;
 
-	spin_lock(&psample_groups_lock);
+	spin_lock_bh(&psample_groups_lock);
 
 	group = psample_group_lookup(net, group_num);
 	if (!group) {
@@ -185,19 +185,27 @@ struct psample_group *psample_group_get(struct net *net, u32 group_num)
 	group->refcount++;
 
 out:
-	spin_unlock(&psample_groups_lock);
+	spin_unlock_bh(&psample_groups_lock);
 	return group;
 }
 EXPORT_SYMBOL_GPL(psample_group_get);
 
+void psample_group_take(struct psample_group *group)
+{
+	spin_lock_bh(&psample_groups_lock);
+	group->refcount++;
+	spin_unlock_bh(&psample_groups_lock);
+}
+EXPORT_SYMBOL_GPL(psample_group_take);
+
 void psample_group_put(struct psample_group *group)
 {
-	spin_lock(&psample_groups_lock);
+	spin_lock_bh(&psample_groups_lock);
 
 	if (--group->refcount == 0)
 		psample_group_destroy(group);
 
-	spin_unlock(&psample_groups_lock);
+	spin_unlock_bh(&psample_groups_lock);
 }
 EXPORT_SYMBOL_GPL(psample_group_put);
 
diff --git a/net/sched/act_sample.c b/net/sched/act_sample.c
index 820d3c6ae8f1..c122ad5d75b6 100644
--- a/net/sched/act_sample.c
+++ b/net/sched/act_sample.c
@@ -255,6 +255,32 @@ static int tcf_sample_search(struct net *net, struct tc_action **a, u32 index)
 	return tcf_idr_search(tn, a, index);
 }
 
+static void tcf_psample_group_put(void *priv)
+{
+	struct psample_group *group = priv;
+
+	psample_group_put(group);
+}
+
+static struct psample_group *
+tcf_sample_get_group(const struct tc_action *a,
+		     tc_action_priv_destructor *destructor)
+{
+	struct tcf_sample *s = to_sample(a);
+	struct psample_group *group;
+
+	spin_lock_bh(&s->tcf_lock);
+	group = rcu_dereference_protected(s->psample_group,
+					  lockdep_is_held(&s->tcf_lock));
+	if (group) {
+		psample_group_take(group);
+		*destructor = tcf_psample_group_put;
+	}
+	spin_unlock_bh(&s->tcf_lock);
+
+	return group;
+}
+
 static struct tc_action_ops act_sample_ops = {
 	.kind	  = "sample",
 	.id	  = TCA_ID_SAMPLE,
@@ -265,6 +291,7 @@ static struct tc_action_ops act_sample_ops = {
 	.cleanup  = tcf_sample_cleanup,
 	.walk	  = tcf_sample_walker,
 	.lookup	  = tcf_sample_search,
+	.get_psample_group = tcf_sample_get_group,
 	.size	  = sizeof(struct tcf_sample),
 };
 
* Unmerged path net/sched/cls_api.c
