RDMA/mlx5: Move default representors SQ steering to rule to modify QP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Mark Bloch <markb@mellanox.com>
commit d5ed8ac34cefc678d0633bfb88d0e20523ba3068
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/d5ed8ac3.failed

Currently the steering for SQs created on representors is done on
creation, once we move to representors as ports of an IB device we need
the port argument which is given only at the modify QP stage, adjust the
code appropriately.

	Signed-off-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit d5ed8ac34cefc678d0633bfb88d0e20523ba3068)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/ib_rep.c
#	drivers/infiniband/hw/mlx5/ib_rep.h
diff --cc drivers/infiniband/hw/mlx5/ib_rep.c
index 95ac97af6166,d3988f6ae2ae..000000000000
--- a/drivers/infiniband/hw/mlx5/ib_rep.c
+++ b/drivers/infiniband/hw/mlx5/ib_rep.c
@@@ -183,22 -146,21 +183,34 @@@ struct mlx5_eswitch_rep *mlx5_ib_vport_
  	return mlx5_eswitch_vport_rep(esw, vport);
  }
  
- int create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
- 			      struct mlx5_ib_sq *sq)
+ struct mlx5_flow_handle *create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
+ 						   struct mlx5_ib_sq *sq,
+ 						   u16 port)
  {
- 	struct mlx5_flow_handle *flow_rule;
  	struct mlx5_eswitch *esw = dev->mdev->priv.eswitch;
+ 	struct mlx5_eswitch_rep *rep;
  
++<<<<<<< HEAD
 +	if (!dev->rep)
 +		return 0;
 +
 +	flow_rule =
 +		mlx5_eswitch_add_send_to_vport_rule(esw,
 +						    dev->rep->vport,
 +						    sq->base.mqp.qpn);
 +	if (IS_ERR(flow_rule))
 +		return PTR_ERR(flow_rule);
 +	sq->flow_rule = flow_rule;
++=======
+ 	if (!dev->is_rep || !port)
+ 		return NULL;
  
- 	return 0;
+ 	if (!dev->port[port - 1].rep)
+ 		return ERR_PTR(-EINVAL);
++>>>>>>> d5ed8ac34cef (RDMA/mlx5: Move default representors SQ steering to rule to modify QP)
+ 
+ 	rep = dev->port[port - 1].rep;
+ 
+ 	return mlx5_eswitch_add_send_to_vport_rule(esw, rep->vport,
+ 						   sq->base.mqp.qpn);
  }
diff --cc drivers/infiniband/hw/mlx5/ib_rep.h
index 2ba73636a2fb,1d9778da8a50..000000000000
--- a/drivers/infiniband/hw/mlx5/ib_rep.h
+++ b/drivers/infiniband/hw/mlx5/ib_rep.h
@@@ -16,10 -18,11 +16,18 @@@ struct mlx5_ib_dev *mlx5_ib_get_rep_ibd
  struct mlx5_ib_dev *mlx5_ib_get_uplink_ibdev(struct mlx5_eswitch *esw);
  struct mlx5_eswitch_rep *mlx5_ib_vport_rep(struct mlx5_eswitch *esw,
  					   int vport_index);
++<<<<<<< HEAD
 +void mlx5_ib_register_vport_reps(struct mlx5_ib_dev *dev);
 +void mlx5_ib_unregister_vport_reps(struct mlx5_ib_dev *dev);
 +int create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
 +			      struct mlx5_ib_sq *sq);
++=======
+ void mlx5_ib_register_vport_reps(struct mlx5_core_dev *mdev);
+ void mlx5_ib_unregister_vport_reps(struct mlx5_core_dev *mdev);
+ struct mlx5_flow_handle *create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
+ 						   struct mlx5_ib_sq *sq,
+ 						   u16 port);
++>>>>>>> d5ed8ac34cef (RDMA/mlx5: Move default representors SQ steering to rule to modify QP)
  struct net_device *mlx5_ib_get_rep_netdev(struct mlx5_eswitch *esw,
  					  int vport_index);
  #else /* CONFIG_MLX5_ESWITCH */
@@@ -48,12 -51,14 +56,21 @@@ struct mlx5_eswitch_rep *mlx5_ib_vport_
  	return NULL;
  }
  
++<<<<<<< HEAD
 +static inline void mlx5_ib_register_vport_reps(struct mlx5_ib_dev *dev) {}
 +static inline void mlx5_ib_unregister_vport_reps(struct mlx5_ib_dev *dev) {}
 +static inline int create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
 +					    struct mlx5_ib_sq *sq)
++=======
+ static inline void mlx5_ib_register_vport_reps(struct mlx5_core_dev *mdev) {}
+ static inline void mlx5_ib_unregister_vport_reps(struct mlx5_core_dev *mdev) {}
+ static inline
+ struct mlx5_flow_handle *create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
+ 						   struct mlx5_ib_sq *sq,
+ 						   u16 port)
++>>>>>>> d5ed8ac34cef (RDMA/mlx5: Move default representors SQ steering to rule to modify QP)
  {
- 	return 0;
+ 	return NULL;
  }
  
  static inline
* Unmerged path drivers/infiniband/hw/mlx5/ib_rep.c
* Unmerged path drivers/infiniband/hw/mlx5/ib_rep.h
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index 83a4d3984747..6dc89c58f513 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -92,6 +92,7 @@ struct mlx5_modify_raw_qp_param {
 	struct mlx5_rate_limit rl;
 
 	u8 rq_q_ctr_id;
+	u16 port;
 };
 
 static void get_cqs(enum ib_qp_type qp_type,
@@ -1206,11 +1207,11 @@ static void destroy_raw_packet_qp_tis(struct mlx5_ib_dev *dev,
 	mlx5_cmd_destroy_tis(dev->mdev, sq->tisn, to_mpd(pd)->uid);
 }
 
-static void destroy_flow_rule_vport_sq(struct mlx5_ib_dev *dev,
-				       struct mlx5_ib_sq *sq)
+static void destroy_flow_rule_vport_sq(struct mlx5_ib_sq *sq)
 {
 	if (sq->flow_rule)
 		mlx5_del_flow_rules(sq->flow_rule);
+	sq->flow_rule = NULL;
 }
 
 static int create_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
@@ -1277,15 +1278,8 @@ static int create_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
 	if (err)
 		goto err_umem;
 
-	err = create_flow_rule_vport_sq(dev, sq);
-	if (err)
-		goto err_flow;
-
 	return 0;
 
-err_flow:
-	mlx5_core_destroy_sq_tracked(dev->mdev, &sq->base.mqp);
-
 err_umem:
 	ib_umem_release(sq->ubuffer.umem);
 	sq->ubuffer.umem = NULL;
@@ -1296,7 +1290,7 @@ static int create_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
 static void destroy_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
 				     struct mlx5_ib_sq *sq)
 {
-	destroy_flow_rule_vport_sq(dev, sq);
+	destroy_flow_rule_vport_sq(sq);
 	mlx5_core_destroy_sq_tracked(dev->mdev, &sq->base.mqp);
 	ib_umem_release(sq->ubuffer.umem);
 }
@@ -3260,6 +3254,8 @@ static int modify_raw_packet_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 	}
 
 	if (modify_sq) {
+		struct mlx5_flow_handle *flow_rule;
+
 		if (tx_affinity) {
 			err = modify_raw_packet_tx_affinity(dev->mdev, sq,
 							    tx_affinity,
@@ -3268,8 +3264,25 @@ static int modify_raw_packet_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 				return err;
 		}
 
-		return modify_raw_packet_qp_sq(dev->mdev, sq, sq_state,
-					       raw_qp_param, qp->ibqp.pd);
+		flow_rule = create_flow_rule_vport_sq(dev, sq,
+						      raw_qp_param->port);
+		if (IS_ERR(flow_rule))
+			return err;
+
+		err = modify_raw_packet_qp_sq(dev->mdev, sq, sq_state,
+					      raw_qp_param, qp->ibqp.pd);
+		if (err) {
+			if (flow_rule)
+				mlx5_del_flow_rules(flow_rule);
+			return err;
+		}
+
+		if (flow_rule) {
+			destroy_flow_rule_vport_sq(sq);
+			sq->flow_rule = flow_rule;
+		}
+
+		return err;
 	}
 
 	return 0;
@@ -3551,6 +3564,9 @@ static int __mlx5_ib_modify_qp(struct ib_qp *ibqp,
 			raw_qp_param.set_mask |= MLX5_RAW_QP_MOD_SET_RQ_Q_CTR_ID;
 		}
 
+		if (attr_mask & IB_QP_PORT)
+			raw_qp_param.port = attr->port_num;
+
 		if (attr_mask & IB_QP_RATE_LIMIT) {
 			raw_qp_param.rl.rate = attr->rate_limit;
 
