sched/fair: Rename weighted_cpuload() to cpu_runnable_load()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dietmar Eggemann <dietmar.eggemann@arm.com>
commit a3df067974c52df936f548ed218120f623c4c560
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/a3df0679.failed

The term 'weighted' is not needed since there is no 'unweighted' load.
Instead use the term 'runnable' to distinguish 'runnable' load
(avg.runnable_load_avg) used in load balance from load (avg.load_avg)
which is the sum of 'runnable' and 'blocked' load.

	Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Morten Rasmussen <morten.rasmussen@arm.com>
	Cc: Patrick Bellasi <patrick.bellasi@arm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Quentin Perret <quentin.perret@arm.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Valentin Schneider <valentin.schneider@arm.com>
	Cc: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lkml.kernel.org/r/57f27a7f-2775-d832-e965-0f4d51bb1954@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a3df067974c52df936f548ed218120f623c4c560)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 304e6f44708c,3bdcd3c718bc..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -1457,9 -1485,7 +1457,13 @@@ bool should_numa_migrate_memory(struct 
  	       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;
  }
  
++<<<<<<< HEAD
 +static unsigned long weighted_cpuload(struct rq *rq);
 +static unsigned long source_load(int cpu, int type);
 +static unsigned long target_load(int cpu, int type);
++=======
+ static unsigned long cpu_runnable_load(struct rq *rq);
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  
  /* Cached statistics for all CPUs within a node */
  struct numa_stats {
@@@ -5284,8 -5366,7 +5288,12 @@@ static struct 
  
  #endif /* CONFIG_NO_HZ_COMMON */
  
++<<<<<<< HEAD
 +/* Used instead of source_load when we know the type == 0 */
 +static unsigned long weighted_cpuload(struct rq *rq)
++=======
+ static unsigned long cpu_runnable_load(struct rq *rq)
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  {
  	return cfs_rq_runnable_load_avg(&rq->cfs);
  }
@@@ -5430,7 -5478,7 +5438,11 @@@ wake_affine_weight(struct sched_domain 
  	s64 this_eff_load, prev_eff_load;
  	unsigned long task_load;
  
++<<<<<<< HEAD
 +	this_eff_load = target_load(this_cpu, sd->wake_idx);
++=======
+ 	this_eff_load = cpu_runnable_load(cpu_rq(this_cpu));
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  
  	if (sync) {
  		unsigned long current_load = task_h_load(current);
@@@ -5448,7 -5496,7 +5460,11 @@@
  		this_eff_load *= 100;
  	this_eff_load *= capacity_of(prev_cpu);
  
++<<<<<<< HEAD
 +	prev_eff_load = source_load(prev_cpu, sd->wake_idx);
++=======
+ 	prev_eff_load = cpu_runnable_load(cpu_rq(prev_cpu));
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  	prev_eff_load -= task_load;
  	if (sched_feat(WA_BIAS))
  		prev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;
@@@ -5540,12 -5584,7 +5556,16 @@@ find_idlest_group(struct sched_domain *
  		max_spare_cap = 0;
  
  		for_each_cpu(i, sched_group_span(group)) {
++<<<<<<< HEAD
 +			/* Bias balancing toward CPUs of our domain */
 +			if (local_group)
 +				load = source_load(i, load_idx);
 +			else
 +				load = target_load(i, load_idx);
 +
++=======
+ 			load = cpu_runnable_load(cpu_rq(i));
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  			runnable_load += load;
  
  			avg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);
@@@ -7959,13 -7969,7 +7979,17 @@@ static inline void update_sg_lb_stats(s
  		if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
  			env->flags |= LBF_NOHZ_AGAIN;
  
++<<<<<<< HEAD
 +		/* Bias balancing toward CPUs of our domain: */
 +		if (local_group)
 +			load = target_load(i, load_idx);
 +		else
 +			load = source_load(i, load_idx);
 +
 +		sgs->group_load += load;
++=======
+ 		sgs->group_load += cpu_runnable_load(rq);
++>>>>>>> a3df067974c5 (sched/fair: Rename weighted_cpuload() to cpu_runnable_load())
  		sgs->group_util += cpu_util(i);
  		sgs->sum_nr_running += rq->cfs.h_nr_running;
  
* Unmerged path kernel/sched/fair.c
