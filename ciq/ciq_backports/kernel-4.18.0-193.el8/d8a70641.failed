dax: Use non-exclusive wait in wait_entry_unlocked()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Dan Williams <dan.j.williams@intel.com>
commit d8a706414af4827fc0b4b1c0c631c607351938b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/d8a70641.failed

get_unlocked_entry() uses an exclusive wait because it is guaranteed to
eventually obtain the lock and follow on with an unlock+wakeup cycle.
The wait_entry_unlocked() path does not have the same guarantee. Rather
than open-code an extra wakeup, just switch to a non-exclusive wait.

	Cc: Jan Kara <jack@suse.cz>
	Cc: Matthew Wilcox <willy@infradead.org>
	Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit d8a706414af4827fc0b4b1c0c631c607351938b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index b89ab8b5e700,042d3b31b413..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -243,6 -232,32 +243,35 @@@ static void *get_unlocked_entry(struct 
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * The only thing keeping the address space around is the i_pages lock
+  * (it's cycled in clear_inode() after removing the entries from i_pages)
+  * After we call xas_unlock_irq(), we cannot touch xas->xa.
+  */
+ static void wait_entry_unlocked(struct xa_state *xas, void *entry)
+ {
+ 	struct wait_exceptional_entry_queue ewait;
+ 	wait_queue_head_t *wq;
+ 
+ 	init_wait(&ewait.wait);
+ 	ewait.wait.func = wake_exceptional_entry_func;
+ 
+ 	wq = dax_entry_waitqueue(xas, entry, &ewait.key);
+ 	/*
+ 	 * Unlike get_unlocked_entry() there is no guarantee that this
+ 	 * path ever successfully retrieves an unlocked entry before an
+ 	 * inode dies. Perform a non-exclusive wait in case this path
+ 	 * never successfully performs its own wake up.
+ 	 */
+ 	prepare_to_wait(wq, &ewait.wait, TASK_UNINTERRUPTIBLE);
+ 	xas_unlock_irq(xas);
+ 	schedule();
+ 	finish_wait(wq, &ewait.wait);
+ }
+ 
++>>>>>>> d8a706414af4 (dax: Use non-exclusive wait in wait_entry_unlocked())
  static void put_unlocked_entry(struct xa_state *xas, void *entry)
  {
  	/* If we were the only waiter woken, wake the next one */
* Unmerged path fs/dax.c
