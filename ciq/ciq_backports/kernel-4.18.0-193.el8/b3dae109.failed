sched/swait: Rename to exclusive

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit b3dae109fa89d67334bf3349babab3ad9b6f233f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/b3dae109.failed

Since swait basically implemented exclusive waits only, make sure
the API reflects that.

  $ git grep -l -e "\<swake_up\>"
		-e "\<swait_event[^ (]*"
		-e "\<prepare_to_swait\>" | while read file;
    do
	sed -i -e 's/\<swake_up\>/&_one/g'
	       -e 's/\<swait_event[^ (]*/&_exclusive/g'
	       -e 's/\<prepare_to_swait\>/&_exclusive/g' $file;
    done

With a few manual touch-ups.

	Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: bigeasy@linutronix.de
	Cc: oleg@redhat.com
	Cc: paulmck@linux.vnet.ibm.com
	Cc: pbonzini@redhat.com
Link: https://lkml.kernel.org/r/20180612083909.261946548@infradead.org

(cherry picked from commit b3dae109fa89d67334bf3349babab3ad9b6f233f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
#	kernel/rcu/tree_exp.h
#	kernel/rcu/tree_plugin.h
#	virt/kvm/arm/psci.c
diff --cc kernel/rcu/tree.c
index 7efe9866f53c,91f888d3b23a..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -1543,18 -1715,19 +1543,22 @@@ static bool rcu_future_gp_cleanup(struc
  }
  
  /*
 - * Awaken the grace-period kthread for the specified flavor of RCU.
 - * Don't do a self-awaken, and don't bother awakening when there is
 - * nothing for the grace-period kthread to do (as in several CPUs
 - * raced to awaken, and we lost), and finally don't try to awaken
 - * a kthread that has not yet been created.
 + * Awaken the grace-period kthread.  Don't do a self-awaken, and don't
 + * bother awakening when there is nothing for the grace-period kthread
 + * to do (as in several CPUs raced to awaken, and we lost), and finally
 + * don't try to awaken a kthread that has not yet been created.
   */
 -static void rcu_gp_kthread_wake(struct rcu_state *rsp)
 +static void rcu_gp_kthread_wake(void)
  {
 -	if (current == rsp->gp_kthread ||
 -	    !READ_ONCE(rsp->gp_flags) ||
 -	    !rsp->gp_kthread)
 +	if (current == rcu_state.gp_kthread ||
 +	    !READ_ONCE(rcu_state.gp_flags) ||
 +	    !rcu_state.gp_kthread)
  		return;
++<<<<<<< HEAD
 +	swake_up(&rcu_state.gp_wq);
++=======
+ 	swake_up_one(&rsp->gp_wq);
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  }
  
  /*
@@@ -1863,15 -2002,15 +1867,15 @@@ static bool rcu_gp_init(void
  }
  
  /*
-  * Helper function for swait_event_idle() wakeup at force-quiescent-state
+  * Helper function for swait_event_idle_exclusive() wakeup at force-quiescent-state
   * time.
   */
 -static bool rcu_gp_fqs_check_wake(struct rcu_state *rsp, int *gfp)
 +static bool rcu_gp_fqs_check_wake(int *gfp)
  {
 -	struct rcu_node *rnp = rcu_get_root(rsp);
 +	struct rcu_node *rnp = rcu_get_root();
  
  	/* Someone like call_rcu() requested a force-quiescent-state scan. */
 -	*gfp = READ_ONCE(rsp->gp_flags);
 +	*gfp = READ_ONCE(rcu_state.gp_flags);
  	if (*gfp & RCU_GP_FLAG_FQS)
  		return true;
  
@@@ -2067,32 -2140,93 +2071,105 @@@ static int __noreturn rcu_gp_kthread(vo
  
  		/* Handle grace-period start. */
  		for (;;) {
 -			trace_rcu_grace_period(rsp->name,
 -					       READ_ONCE(rsp->gpnum),
 +			trace_rcu_grace_period(rcu_state.name,
 +					       READ_ONCE(rcu_state.gp_seq),
  					       TPS("reqwait"));
++<<<<<<< HEAD
 +			rcu_state.gp_state = RCU_GP_WAIT_GPS;
 +			swait_event_idle(rcu_state.gp_wq,
 +					 READ_ONCE(rcu_state.gp_flags) &
 +					 RCU_GP_FLAG_INIT);
 +			rcu_state.gp_state = RCU_GP_DONE_GPS;
++=======
+ 			rsp->gp_state = RCU_GP_WAIT_GPS;
+ 			swait_event_idle_exclusive(rsp->gp_wq, READ_ONCE(rsp->gp_flags) &
+ 						     RCU_GP_FLAG_INIT);
+ 			rsp->gp_state = RCU_GP_DONE_GPS;
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  			/* Locking provides needed memory barrier. */
 -			if (rcu_gp_init(rsp))
 +			if (rcu_gp_init())
  				break;
  			cond_resched_tasks_rcu_qs();
 -			WRITE_ONCE(rsp->gp_activity, jiffies);
 +			WRITE_ONCE(rcu_state.gp_activity, jiffies);
  			WARN_ON(signal_pending(current));
 -			trace_rcu_grace_period(rsp->name,
 -					       READ_ONCE(rsp->gpnum),
 +			trace_rcu_grace_period(rcu_state.name,
 +					       READ_ONCE(rcu_state.gp_seq),
  					       TPS("reqwaitsig"));
  		}
  
  		/* Handle quiescent-state forcing. */
++<<<<<<< HEAD
 +		rcu_gp_fqs_loop();
++=======
+ 		first_gp_fqs = true;
+ 		j = jiffies_till_first_fqs;
+ 		if (j > HZ) {
+ 			j = HZ;
+ 			jiffies_till_first_fqs = HZ;
+ 		}
+ 		ret = 0;
+ 		for (;;) {
+ 			if (!ret) {
+ 				rsp->jiffies_force_qs = jiffies + j;
+ 				WRITE_ONCE(rsp->jiffies_kick_kthreads,
+ 					   jiffies + 3 * j);
+ 			}
+ 			trace_rcu_grace_period(rsp->name,
+ 					       READ_ONCE(rsp->gpnum),
+ 					       TPS("fqswait"));
+ 			rsp->gp_state = RCU_GP_WAIT_FQS;
+ 			ret = swait_event_idle_timeout_exclusive(rsp->gp_wq,
+ 					rcu_gp_fqs_check_wake(rsp, &gf), j);
+ 			rsp->gp_state = RCU_GP_DOING_FQS;
+ 			/* Locking provides needed memory barriers. */
+ 			/* If grace period done, leave loop. */
+ 			if (!READ_ONCE(rnp->qsmask) &&
+ 			    !rcu_preempt_blocked_readers_cgp(rnp))
+ 				break;
+ 			/* If time for quiescent-state forcing, do it. */
+ 			if (ULONG_CMP_GE(jiffies, rsp->jiffies_force_qs) ||
+ 			    (gf & RCU_GP_FLAG_FQS)) {
+ 				trace_rcu_grace_period(rsp->name,
+ 						       READ_ONCE(rsp->gpnum),
+ 						       TPS("fqsstart"));
+ 				rcu_gp_fqs(rsp, first_gp_fqs);
+ 				first_gp_fqs = false;
+ 				trace_rcu_grace_period(rsp->name,
+ 						       READ_ONCE(rsp->gpnum),
+ 						       TPS("fqsend"));
+ 				cond_resched_tasks_rcu_qs();
+ 				WRITE_ONCE(rsp->gp_activity, jiffies);
+ 				ret = 0; /* Force full wait till next FQS. */
+ 				j = jiffies_till_next_fqs;
+ 				if (j > HZ) {
+ 					j = HZ;
+ 					jiffies_till_next_fqs = HZ;
+ 				} else if (j < 1) {
+ 					j = 1;
+ 					jiffies_till_next_fqs = 1;
+ 				}
+ 			} else {
+ 				/* Deal with stray signal. */
+ 				cond_resched_tasks_rcu_qs();
+ 				WRITE_ONCE(rsp->gp_activity, jiffies);
+ 				WARN_ON(signal_pending(current));
+ 				trace_rcu_grace_period(rsp->name,
+ 						       READ_ONCE(rsp->gpnum),
+ 						       TPS("fqswaitsig"));
+ 				ret = 1; /* Keep old FQS timing. */
+ 				j = jiffies;
+ 				if (time_after(jiffies, rsp->jiffies_force_qs))
+ 					j = 1;
+ 				else
+ 					j = rsp->jiffies_force_qs - j;
+ 			}
+ 		}
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  
  		/* Handle grace-period end. */
 -		rsp->gp_state = RCU_GP_CLEANUP;
 -		rcu_gp_cleanup(rsp);
 -		rsp->gp_state = RCU_GP_CLEANED;
 +		rcu_state.gp_state = RCU_GP_CLEANUP;
 +		rcu_gp_cleanup();
 +		rcu_state.gp_state = RCU_GP_CLEANED;
  	}
  }
  
diff --cc kernel/rcu/tree_exp.h
index 87c05b0fcf59,d428cc1064c8..000000000000
--- a/kernel/rcu/tree_exp.h
+++ b/kernel/rcu/tree_exp.h
@@@ -212,7 -212,7 +212,11 @@@ static void __rcu_report_exp_rnp(struc
  			raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  			if (wake) {
  				smp_mb(); /* EGP done before wake_up(). */
++<<<<<<< HEAD
 +				swake_up(&rcu_state.expedited_wq);
++=======
+ 				swake_up_one(&rsp->expedited_wq);
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  			}
  			break;
  		}
@@@ -481,8 -518,8 +485,13 @@@ static void synchronize_sched_expedited
  	jiffies_start = jiffies;
  
  	for (;;) {
++<<<<<<< HEAD
 +		ret = swait_event_timeout(
 +				rcu_state.expedited_wq,
++=======
+ 		ret = swait_event_timeout_exclusive(
+ 				rsp->expedited_wq,
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  				sync_rcu_preempt_exp_done_unlocked(rnp_root),
  				jiffies_stall);
  		if (ret > 0 || sync_rcu_preempt_exp_done_unlocked(rnp_root))
diff --cc kernel/rcu/tree_plugin.h
index 1a59447c6208,ad53d133f709..000000000000
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@@ -2170,9 -2082,9 +2170,15 @@@ static void rcu_nocb_wait_gp(struct rcu
  	 */
  	trace_rcu_this_gp(rnp, rdp, c, TPS("StartWait"));
  	for (;;) {
++<<<<<<< HEAD
 +		swait_event_interruptible(
 +			rnp->nocb_gp_wq[rcu_seq_ctr(c) & 0x1],
 +			(d = rcu_seq_done(&rnp->gp_seq, c)));
++=======
+ 		swait_event_interruptible_exclusive(
+ 			rnp->nocb_gp_wq[c & 0x1],
+ 			(d = ULONG_CMP_GE(READ_ONCE(rnp->completed), c)));
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  		if (likely(d))
  			break;
  		WARN_ON(signal_pending(current));
@@@ -2198,8 -2110,8 +2204,13 @@@ wait_again
  
  	/* Wait for callbacks to appear. */
  	if (!rcu_nocb_poll) {
++<<<<<<< HEAD
 +		trace_rcu_nocb_wake(rcu_state.name, my_rdp->cpu, TPS("Sleep"));
 +		swait_event_interruptible(my_rdp->nocb_wq,
++=======
+ 		trace_rcu_nocb_wake(my_rdp->rsp->name, my_rdp->cpu, TPS("Sleep"));
+ 		swait_event_interruptible_exclusive(my_rdp->nocb_wq,
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  				!READ_ONCE(my_rdp->nocb_leader_sleep));
  		raw_spin_lock_irqsave(&my_rdp->nocb_lock, flags);
  		my_rdp->nocb_leader_sleep = true;
@@@ -2280,8 -2192,8 +2291,13 @@@
  static void nocb_follower_wait(struct rcu_data *rdp)
  {
  	for (;;) {
++<<<<<<< HEAD
 +		trace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS("FollowerSleep"));
 +		swait_event_interruptible(rdp->nocb_wq,
++=======
+ 		trace_rcu_nocb_wake(rdp->rsp->name, rdp->cpu, TPS("FollowerSleep"));
+ 		swait_event_interruptible_exclusive(rdp->nocb_wq,
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  					 READ_ONCE(rdp->nocb_follower_head));
  		if (smp_load_acquire(&rdp->nocb_follower_head)) {
  			/* ^^^ Ensure CB invocation follows _head test. */
diff --cc virt/kvm/arm/psci.c
index 34d08ee63747,9b73d3ad918a..000000000000
--- a/virt/kvm/arm/psci.c
+++ b/virt/kvm/arm/psci.c
@@@ -139,19 -150,12 +139,24 @@@ static unsigned long kvm_psci_vcpu_on(s
  	 * NOTE: We always update r0 (or x0) because for PSCI v0.1
  	 * the general puspose registers are undefined upon CPU_ON.
  	 */
 -	smccc_set_retval(vcpu, context_id, 0, 0, 0);
 -	vcpu->arch.power_off = false;
 -	smp_mb();		/* Make sure the above is visible */
 +	reset_state->r0 = smccc_get_arg3(source_vcpu);
 +
++<<<<<<< HEAD
 +	WRITE_ONCE(reset_state->reset, true);
 +	kvm_make_request(KVM_REQ_VCPU_RESET, vcpu);
  
 +	/*
 +	 * Make sure the reset request is observed if the change to
 +	 * power_state is observed.
 +	 */
 +	smp_wmb();
 +
 +	vcpu->arch.power_off = false;
 +	kvm_vcpu_wake_up(vcpu);
++=======
+ 	wq = kvm_arch_vcpu_wq(vcpu);
+ 	swake_up_one(wq);
++>>>>>>> b3dae109fa89 (sched/swait: Rename to exclusive)
  
  	return PSCI_RET_SUCCESS;
  }
diff --git a/arch/mips/kvm/mips.c b/arch/mips/kvm/mips.c
index 4a436e80bd87..e3f7606bdbb4 100644
--- a/arch/mips/kvm/mips.c
+++ b/arch/mips/kvm/mips.c
@@ -515,7 +515,7 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 	dvcpu->arch.wait = 0;
 
 	if (swq_has_sleeper(&dvcpu->wq))
-		swake_up(&dvcpu->wq);
+		swake_up_one(&dvcpu->wq);
 
 	return 0;
 }
@@ -1230,7 +1230,7 @@ static void kvm_mips_comparecount_func(unsigned long data)
 
 	vcpu->arch.wait = 0;
 	if (swq_has_sleeper(&vcpu->wq))
-		swake_up(&vcpu->wq);
+		swake_up_one(&vcpu->wq);
 }
 
 /* low level hrtimer wake routine */
diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
index 20b2fe1036e8..25116613def2 100644
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@ -236,7 +236,7 @@ static void kvmppc_fast_vcpu_kick_hv(struct kvm_vcpu *vcpu)
 
 	wqp = kvm_arch_vcpu_wq(vcpu);
 	if (swq_has_sleeper(wqp)) {
-		swake_up(wqp);
+		swake_up_one(wqp);
 		++vcpu->stat.halt_wakeup;
 	}
 
@@ -3764,7 +3764,7 @@ static void kvmppc_vcore_blocked(struct kvmppc_vcore *vc)
 		}
 	}
 
-	prepare_to_swait(&vc->wq, &wait, TASK_INTERRUPTIBLE);
+	prepare_to_swait_exclusive(&vc->wq, &wait, TASK_INTERRUPTIBLE);
 
 	if (kvmppc_vcore_check_block(vc)) {
 		finish_swait(&vc->wq, &wait);
@@ -3892,7 +3892,7 @@ static int kvmppc_run_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)
 			kvmppc_start_thread(vcpu, vc);
 			trace_kvm_guest_enter(vcpu);
 		} else if (vc->vcore_state == VCORE_SLEEPING) {
-			swake_up(&vc->wq);
+			swake_up_one(&vc->wq);
 		}
 
 	}
diff --git a/arch/s390/kvm/interrupt.c b/arch/s390/kvm/interrupt.c
index 93796dc89f55..b2b144cdf7a2 100644
--- a/arch/s390/kvm/interrupt.c
+++ b/arch/s390/kvm/interrupt.c
@@ -1241,7 +1241,7 @@ void kvm_s390_vcpu_wakeup(struct kvm_vcpu *vcpu)
 		 * yield-candidate.
 		 */
 		vcpu->preempted = true;
-		swake_up(&vcpu->wq);
+		swake_up_one(&vcpu->wq);
 		vcpu->stat.halt_wakeup++;
 	}
 	/*
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d7a53617ce5d..7f89d609095a 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -145,7 +145,7 @@ void kvm_async_pf_task_wait(u32 token, int interrupt_kernel)
 
 	for (;;) {
 		if (!n.halted)
-			prepare_to_swait(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
+			prepare_to_swait_exclusive(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
 		if (hlist_unhashed(&n.link))
 			break;
 
@@ -179,7 +179,7 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 	if (n->halted)
 		smp_send_reschedule(n->cpu);
 	else if (swq_has_sleeper(&n->wq))
-		swake_up(&n->wq);
+		swake_up_one(&n->wq);
 }
 
 static void apf_task_wake_all(void)
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index d10e97e42449..d7c4e09894d3 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1452,7 +1452,7 @@ static void apic_timer_expired(struct kvm_lapic *apic)
 	 * using swait_active() is safe.
 	 */
 	if (swait_active(q))
-		swake_up(q);
+		swake_up_one(q);
 
 	if (apic_lvtt_tscdeadline(apic))
 		ktimer->expired_tscdeadline = ktimer->tscdeadline;
diff --git a/include/linux/swait.h b/include/linux/swait.h
index dd032061112d..73e06e9986d4 100644
--- a/include/linux/swait.h
+++ b/include/linux/swait.h
@@ -16,7 +16,7 @@
  * wait-queues, but the semantics are actually completely different, and
  * every single user we have ever had has been buggy (or pointless).
  *
- * A "swake_up()" only wakes up _one_ waiter, which is not at all what
+ * A "swake_up_one()" only wakes up _one_ waiter, which is not at all what
  * "wake_up()" does, and has led to problems. In other cases, it has
  * been fine, because there's only ever one waiter (kvm), but in that
  * case gthe whole "simple" wait-queue is just pointless to begin with,
@@ -115,7 +115,7 @@ extern void __init_swait_queue_head(struct swait_queue_head *q, const char *name
  *      CPU0 - waker                    CPU1 - waiter
  *
  *                                      for (;;) {
- *      @cond = true;                     prepare_to_swait(&wq_head, &wait, state);
+ *      @cond = true;                     prepare_to_swait_exclusive(&wq_head, &wait, state);
  *      smp_mb();                         // smp_mb() from set_current_state()
  *      if (swait_active(wq_head))        if (@cond)
  *        wake_up(wq_head);                      break;
@@ -157,11 +157,11 @@ static inline bool swq_has_sleeper(struct swait_queue_head *wq)
 	return swait_active(wq);
 }
 
-extern void swake_up(struct swait_queue_head *q);
+extern void swake_up_one(struct swait_queue_head *q);
 extern void swake_up_all(struct swait_queue_head *q);
 extern void swake_up_locked(struct swait_queue_head *q);
 
-extern void prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait, int state);
+extern void prepare_to_swait_exclusive(struct swait_queue_head *q, struct swait_queue *wait, int state);
 extern long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait, int state);
 
 extern void __finish_swait(struct swait_queue_head *q, struct swait_queue *wait);
@@ -196,7 +196,7 @@ __out:	__ret;								\
 	(void)___swait_event(wq, condition, TASK_UNINTERRUPTIBLE, 0,	\
 			    schedule())
 
-#define swait_event(wq, condition)					\
+#define swait_event_exclusive(wq, condition)				\
 do {									\
 	if (condition)							\
 		break;							\
@@ -208,7 +208,7 @@ do {									\
 		      TASK_UNINTERRUPTIBLE, timeout,			\
 		      __ret = schedule_timeout(__ret))
 
-#define swait_event_timeout(wq, condition, timeout)			\
+#define swait_event_timeout_exclusive(wq, condition, timeout)		\
 ({									\
 	long __ret = timeout;						\
 	if (!___wait_cond_timeout(condition))				\
@@ -220,7 +220,7 @@ do {									\
 	___swait_event(wq, condition, TASK_INTERRUPTIBLE, 0,		\
 		      schedule())
 
-#define swait_event_interruptible(wq, condition)			\
+#define swait_event_interruptible_exclusive(wq, condition)		\
 ({									\
 	int __ret = 0;							\
 	if (!(condition))						\
@@ -233,7 +233,7 @@ do {									\
 		      TASK_INTERRUPTIBLE, timeout,			\
 		      __ret = schedule_timeout(__ret))
 
-#define swait_event_interruptible_timeout(wq, condition, timeout)	\
+#define swait_event_interruptible_timeout_exclusive(wq, condition, timeout)\
 ({									\
 	long __ret = timeout;						\
 	if (!___wait_cond_timeout(condition))				\
@@ -246,7 +246,7 @@ do {									\
 	(void)___swait_event(wq, condition, TASK_IDLE, 0, schedule())
 
 /**
- * swait_event_idle - wait without system load contribution
+ * swait_event_idle_exclusive - wait without system load contribution
  * @wq: the waitqueue to wait on
  * @condition: a C expression for the event to wait for
  *
@@ -257,7 +257,7 @@ do {									\
  * condition and doesn't want to contribute to system load. Signals are
  * ignored.
  */
-#define swait_event_idle(wq, condition)					\
+#define swait_event_idle_exclusive(wq, condition)			\
 do {									\
 	if (condition)							\
 		break;							\
@@ -270,7 +270,7 @@ do {									\
 		       __ret = schedule_timeout(__ret))
 
 /**
- * swait_event_idle_timeout - wait up to timeout without load contribution
+ * swait_event_idle_timeout_exclusive - wait up to timeout without load contribution
  * @wq: the waitqueue to wait on
  * @condition: a C expression for the event to wait for
  * @timeout: timeout at which we'll give up in jiffies
@@ -288,7 +288,7 @@ do {									\
  * or the remaining jiffies (at least 1) if the @condition evaluated
  * to %true before the @timeout elapsed.
  */
-#define swait_event_idle_timeout(wq, condition, timeout)		\
+#define swait_event_idle_timeout_exclusive(wq, condition, timeout)	\
 ({									\
 	long __ret = timeout;						\
 	if (!___wait_cond_timeout(condition))				\
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index aeb9ff8a8c39..bafccf4fe56d 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -92,7 +92,7 @@ static void s2idle_enter(void)
 	/* Push all the CPUs into the idle loop. */
 	wake_up_all_idle_cpus();
 	/* Make the current CPU wait so it can enter the idle loop too. */
-	swait_event(s2idle_wait_head,
+	swait_event_exclusive(s2idle_wait_head,
 		    s2idle_state == S2IDLE_STATE_WAKE);
 
 	cpuidle_pause();
@@ -160,7 +160,7 @@ void s2idle_wake(void)
 	raw_spin_lock_irqsave(&s2idle_lock, flags);
 	if (s2idle_state > S2IDLE_STATE_NONE) {
 		s2idle_state = S2IDLE_STATE_WAKE;
-		swake_up(&s2idle_wait_head);
+		swake_up_one(&s2idle_wait_head);
 	}
 	raw_spin_unlock_irqrestore(&s2idle_lock, flags);
 }
diff --git a/kernel/rcu/srcutiny.c b/kernel/rcu/srcutiny.c
index 5902ac6b5822..b46e6683f8c9 100644
--- a/kernel/rcu/srcutiny.c
+++ b/kernel/rcu/srcutiny.c
@@ -113,7 +113,7 @@ void __srcu_read_unlock(struct srcu_struct *sp, int idx)
 
 	WRITE_ONCE(sp->srcu_lock_nesting[idx], newval);
 	if (!newval && READ_ONCE(sp->srcu_gp_waiting))
-		swake_up(&sp->srcu_wq);
+		swake_up_one(&sp->srcu_wq);
 }
 EXPORT_SYMBOL_GPL(__srcu_read_unlock);
 
@@ -143,7 +143,7 @@ void srcu_drive_gp(struct work_struct *wp)
 	idx = sp->srcu_idx;
 	WRITE_ONCE(sp->srcu_idx, !sp->srcu_idx);
 	WRITE_ONCE(sp->srcu_gp_waiting, true);  /* srcu_read_unlock() wakes! */
-	swait_event(sp->srcu_wq, !READ_ONCE(sp->srcu_lock_nesting[idx]));
+	swait_event_exclusive(sp->srcu_wq, !READ_ONCE(sp->srcu_lock_nesting[idx]));
 	WRITE_ONCE(sp->srcu_gp_waiting, false); /* srcu_read_unlock() cheap. */
 
 	/* Invoke the callbacks we removed above. */
* Unmerged path kernel/rcu/tree.c
* Unmerged path kernel/rcu/tree_exp.h
* Unmerged path kernel/rcu/tree_plugin.h
diff --git a/kernel/sched/swait.c b/kernel/sched/swait.c
index 66890de93ee5..66b59ac77c22 100644
--- a/kernel/sched/swait.c
+++ b/kernel/sched/swait.c
@@ -32,7 +32,7 @@ void swake_up_locked(struct swait_queue_head *q)
 }
 EXPORT_SYMBOL(swake_up_locked);
 
-void swake_up(struct swait_queue_head *q)
+void swake_up_one(struct swait_queue_head *q)
 {
 	unsigned long flags;
 
@@ -40,7 +40,7 @@ void swake_up(struct swait_queue_head *q)
 	swake_up_locked(q);
 	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL(swake_up);
+EXPORT_SYMBOL(swake_up_one);
 
 /*
  * Does not allow usage from IRQ disabled, since we must be able to
@@ -76,7 +76,7 @@ static void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *w
 		list_add_tail(&wait->task_list, &q->task_list);
 }
 
-void prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait, int state)
+void prepare_to_swait_exclusive(struct swait_queue_head *q, struct swait_queue *wait, int state)
 {
 	unsigned long flags;
 
@@ -85,7 +85,7 @@ void prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait, int
 	set_current_state(state);
 	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL(prepare_to_swait);
+EXPORT_SYMBOL(prepare_to_swait_exclusive);
 
 long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait, int state)
 {
@@ -95,7 +95,7 @@ long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait
 	raw_spin_lock_irqsave(&q->lock, flags);
 	if (unlikely(signal_pending_state(state, current))) {
 		/*
-		 * See prepare_to_wait_event(). TL;DR, subsequent swake_up()
+		 * See prepare_to_wait_event(). TL;DR, subsequent swake_up_one()
 		 * must not see us.
 		 */
 		list_del_init(&wait->task_list);
diff --git a/virt/kvm/arm/arm.c b/virt/kvm/arm/arm.c
index 466b21390611..1669459d20fb 100644
--- a/virt/kvm/arm/arm.c
+++ b/virt/kvm/arm/arm.c
@@ -599,7 +599,7 @@ void kvm_arm_resume_guest(struct kvm *kvm)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		vcpu->arch.pause = false;
-		swake_up(kvm_arch_vcpu_wq(vcpu));
+		swake_up_one(kvm_arch_vcpu_wq(vcpu));
 	}
 }
 
@@ -607,7 +607,7 @@ static void vcpu_req_sleep(struct kvm_vcpu *vcpu)
 {
 	struct swait_queue_head *wq = kvm_arch_vcpu_wq(vcpu);
 
-	swait_event_interruptible(*wq, ((!vcpu->arch.power_off) &&
+	swait_event_interruptible_exclusive(*wq, ((!vcpu->arch.power_off) &&
 				       (!vcpu->arch.pause)));
 
 	if (vcpu->arch.power_off || vcpu->arch.pause) {
* Unmerged path virt/kvm/arm/psci.c
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index a402df565e9e..110cbe3f74f8 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -107,7 +107,7 @@ static void async_pf_execute(struct work_struct *work)
 	trace_kvm_async_pf_completed(addr, gva);
 
 	if (swq_has_sleeper(&vcpu->wq))
-		swake_up(&vcpu->wq);
+		swake_up_one(&vcpu->wq);
 
 	mmput(mm);
 	kvm_put_kvm(vcpu->kvm);
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index bb74649109b6..d4d7400447f6 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2276,7 +2276,7 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 	kvm_arch_vcpu_blocking(vcpu);
 
 	for (;;) {
-		prepare_to_swait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
+		prepare_to_swait_exclusive(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
 
 		if (kvm_vcpu_check_block(vcpu) < 0)
 			break;
@@ -2318,7 +2318,7 @@ bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu)
 
 	wqp = kvm_arch_vcpu_wq(vcpu);
 	if (swq_has_sleeper(wqp)) {
-		swake_up(wqp);
+		swake_up_one(wqp);
 		++vcpu->stat.halt_wakeup;
 		return true;
 	}
