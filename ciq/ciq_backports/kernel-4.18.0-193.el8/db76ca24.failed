net/mlx5e: Allow concurrent creation of hairpin entries

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [netdrv] mlx5e: Allow concurrent creation of hairpin entries (Alaa Hleihel) [1663231 1724336]
Rebuild_FUZZ: 96.23%
commit-author Vlad Buslov <vladbu@mellanox.com>
commit db76ca2424fe28923aaec5e2187e886b025a914c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/db76ca24.failed

Hairpin entries creation is fully synchronized by hairpin_tbl_lock. In
order to allow concurrent initialization of mlx5e_hairpin structure
instances and provisioning of hairpin entries to hardware, extend
mlx5e_hairpin_entry with 'res_ready' completion. Move call to
mlx5e_hairpin_create() out of hairpin_tbl_lock critical section. Modify
code that attaches new flows to existing hpe to wait for 'res_ready'
completion before using the hpe. Insert hpe to hairpin table before
provisioning it to hardware and modify all users of hairpin table to verify
that hpe was fully initialized by checking hpe->hp pointer (and to wait for
'res_ready' completion, if necessary).

Modify dead peer update event handling function to save hpe's to temporary
list with their reference counter incremented. Wait for completion of hpe's
in temporary list and update their 'peer_gone' flag outside of
hairpin_tbl_lock critical section.

	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Reviewed-by: Roi Dayan <roid@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit db76ca2424fe28923aaec5e2187e886b025a914c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index c033e0d4fc59,b6a91e3054c0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -38,7 -38,8 +38,12 @@@
  #include <linux/mlx5/fs.h>
  #include <linux/mlx5/device.h>
  #include <linux/rhashtable.h>
++<<<<<<< HEAD
 +#include <net/switchdev.h>
++=======
+ #include <linux/refcount.h>
+ #include <linux/completion.h>
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  #include <net/tc_act/tc_mirred.h>
  #include <net/tc_act/tc_vlan.h>
  #include <net/tc_act/tc_tunnel_key.h>
@@@ -158,12 -163,20 +163,21 @@@ struct mlx5e_hairpin_entry 
  	/* a node of a hash table which keeps all the  hairpin entries */
  	struct hlist_node hairpin_hlist;
  
 -	/* protects flows list */
 -	spinlock_t flows_lock;
  	/* flows sharing the same hairpin */
  	struct list_head flows;
+ 	/* hpe's that were not fully initialized when dead peer update event
+ 	 * function traversed them.
+ 	 */
+ 	struct list_head dead_peer_wait_list;
  
  	u16 peer_vhca_id;
  	u8 prio;
  	struct mlx5e_hairpin *hp;
++<<<<<<< HEAD
++=======
+ 	refcount_t refcnt;
+ 	struct completion res_ready;
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  };
  
  struct mod_hdr_key {
@@@ -563,6 -654,26 +577,29 @@@ static struct mlx5e_hairpin_entry *mlx5
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static void mlx5e_hairpin_put(struct mlx5e_priv *priv,
+ 			      struct mlx5e_hairpin_entry *hpe)
+ {
+ 	/* no more hairpin flows for us, release the hairpin pair */
+ 	if (!refcount_dec_and_mutex_lock(&hpe->refcnt, &priv->fs.tc.hairpin_tbl_lock))
+ 		return;
+ 	hash_del(&hpe->hairpin_hlist);
+ 	mutex_unlock(&priv->fs.tc.hairpin_tbl_lock);
+ 
+ 	if (!IS_ERR_OR_NULL(hpe->hp)) {
+ 		netdev_dbg(priv->netdev, "del hairpin: peer %s\n",
+ 			   dev_name(hpe->hp->pair->peer_mdev->device));
+ 
+ 		mlx5e_hairpin_destroy(hpe->hp);
+ 	}
+ 
+ 	WARN_ON(!list_empty(&hpe->flows));
+ 	kfree(hpe);
+ }
+ 
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  #define UNKNOWN_MATCH_PRIO 8
  
  static int mlx5e_hairpin_get_prio(struct mlx5e_priv *priv,
@@@ -628,17 -739,37 +665,42 @@@ static int mlx5e_hairpin_flow_add(struc
  				     extack);
  	if (err)
  		return err;
 -
 -	mutex_lock(&priv->fs.tc.hairpin_tbl_lock);
  	hpe = mlx5e_hairpin_get(priv, peer_id, match_prio);
- 	if (hpe)
+ 	if (hpe) {
+ 		mutex_unlock(&priv->fs.tc.hairpin_tbl_lock);
+ 		wait_for_completion(&hpe->res_ready);
+ 
+ 		if (IS_ERR(hpe->hp)) {
+ 			err = -EREMOTEIO;
+ 			goto out_err;
+ 		}
  		goto attach_flow;
+ 	}
  
  	hpe = kzalloc(sizeof(*hpe), GFP_KERNEL);
++<<<<<<< HEAD
 +	if (!hpe)
 +		return -ENOMEM;
++=======
+ 	if (!hpe) {
+ 		mutex_unlock(&priv->fs.tc.hairpin_tbl_lock);
+ 		return -ENOMEM;
+ 	}
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  
 -	spin_lock_init(&hpe->flows_lock);
  	INIT_LIST_HEAD(&hpe->flows);
+ 	INIT_LIST_HEAD(&hpe->dead_peer_wait_list);
  	hpe->peer_vhca_id = peer_id;
  	hpe->prio = match_prio;
++<<<<<<< HEAD
++=======
+ 	refcount_set(&hpe->refcnt, 1);
+ 	init_completion(&hpe->res_ready);
+ 
+ 	hash_add(priv->fs.tc.hairpin_tbl, &hpe->hairpin_hlist,
+ 		 hash_hairpin_info(peer_id, match_prio));
+ 	mutex_unlock(&priv->fs.tc.hairpin_tbl_lock);
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  
  	params.log_data_size = 15;
  	params.log_data_size = min_t(u8, params.log_data_size,
@@@ -666,26 -799,27 +730,33 @@@
  	}
  
  	netdev_dbg(priv->netdev, "add hairpin: tirn %x rqn %x peer %s sqn %x prio %d (log) data %d packets %d\n",
 -		   hp->tirn, hp->pair->rqn[0],
 -		   dev_name(hp->pair->peer_mdev->device),
 +		   hp->tirn, hp->pair->rqn[0], hp->pair->peer_mdev->priv.name,
  		   hp->pair->sqn[0], match_prio, params.log_data_size, params.log_num_packets);
  
- 	hpe->hp = hp;
- 	hash_add(priv->fs.tc.hairpin_tbl, &hpe->hairpin_hlist,
- 		 hash_hairpin_info(peer_id, match_prio));
- 
  attach_flow:
  	if (hpe->hp->num_channels > 1) {
 -		flow_flag_set(flow, HAIRPIN_RSS);
 +		flow->flags |= MLX5E_TC_FLOW_HAIRPIN_RSS;
  		flow->nic_attr->hairpin_ft = hpe->hp->ttc.ft.t;
  	} else {
  		flow->nic_attr->hairpin_tirn = hpe->hp->tirn;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	flow->hpe = hpe;
+ 	spin_lock(&hpe->flows_lock);
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  	list_add(&flow->hairpin, &hpe->flows);
 -	spin_unlock(&hpe->flows_lock);
  
  	return 0;
  
++<<<<<<< HEAD
 +create_hairpin_err:
 +	kfree(hpe);
++=======
+ out_err:
+ 	mlx5e_hairpin_put(priv, hpe);
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  	return err;
  }
  
@@@ -3422,9 -3796,18 +3494,24 @@@ static void mlx5e_tc_hairpin_update_dea
  
  	peer_vhca_id = MLX5_CAP_GEN(peer_mdev, vhca_id);
  
++<<<<<<< HEAD
 +	hash_for_each(priv->fs.tc.hairpin_tbl, bkt, hpe, hairpin_hlist) {
 +		if (hpe->peer_vhca_id == peer_vhca_id)
 +			hpe->hp->pair->peer_gone = true;
++=======
+ 	mutex_lock(&priv->fs.tc.hairpin_tbl_lock);
+ 	hash_for_each(priv->fs.tc.hairpin_tbl, bkt, hpe, hairpin_hlist)
+ 		if (refcount_inc_not_zero(&hpe->refcnt))
+ 			list_add(&hpe->dead_peer_wait_list, &init_wait_list);
+ 	mutex_unlock(&priv->fs.tc.hairpin_tbl_lock);
+ 
+ 	list_for_each_entry_safe(hpe, tmp, &init_wait_list, dead_peer_wait_list) {
+ 		wait_for_completion(&hpe->res_ready);
+ 		if (!IS_ERR_OR_NULL(hpe->hp) && hpe->peer_vhca_id == peer_vhca_id)
+ 			hpe->hp->pair->peer_gone = true;
+ 
+ 		mlx5e_hairpin_put(priv, hpe);
++>>>>>>> db76ca2424fe (net/mlx5e: Allow concurrent creation of hairpin entries)
  	}
  }
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
