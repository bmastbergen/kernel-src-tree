sched/topology: Introduce a sysctl for Energy Aware Scheduling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Quentin Perret <quentin.perret@arm.com>
commit 8d5d0cfb63cbcb4005e19a332b31d687b1d01e58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/8d5d0cfb.failed

In its current state, Energy Aware Scheduling (EAS) starts automatically
on asymmetric platforms having an Energy Model (EM). However, there are
users who want to have an EM (for thermal management for example), but
don't want EAS with it.

In order to let users disable EAS explicitly, introduce a new sysctl
called 'sched_energy_aware'. It is enabled by default so that EAS can
start automatically on platforms where it makes sense. Flipping it to 0
rebuilds the scheduling domains and disables EAS.

	Signed-off-by: Quentin Perret <quentin.perret@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: adharmap@codeaurora.org
	Cc: chris.redpath@arm.com
	Cc: currojerez@riseup.net
	Cc: dietmar.eggemann@arm.com
	Cc: edubezval@gmail.com
	Cc: gregkh@linuxfoundation.org
	Cc: javi.merino@kernel.org
	Cc: joel@joelfernandes.org
	Cc: juri.lelli@redhat.com
	Cc: morten.rasmussen@arm.com
	Cc: patrick.bellasi@arm.com
	Cc: pkondeti@codeaurora.org
	Cc: rjw@rjwysocki.net
	Cc: skannan@codeaurora.org
	Cc: smuckle@google.com
	Cc: srinivas.pandruvada@linux.intel.com
	Cc: thara.gopinath@linaro.org
	Cc: tkjos@google.com
	Cc: valentin.schneider@arm.com
	Cc: vincent.guittot@linaro.org
	Cc: viresh.kumar@linaro.org
Link: https://lkml.kernel.org/r/20181203095628.11858-11-quentin.perret@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 8d5d0cfb63cbcb4005e19a332b31d687b1d01e58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/topology.c
diff --cc kernel/sched/topology.c
index 7e6a83289382,50c3fc316c54..000000000000
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@@ -201,6 -201,228 +201,231 @@@ sd_parent_degenerate(struct sched_domai
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ DEFINE_STATIC_KEY_FALSE(sched_energy_present);
+ #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+ unsigned int sysctl_sched_energy_aware = 1;
+ DEFINE_MUTEX(sched_energy_mutex);
+ bool sched_energy_update;
+ 
+ #ifdef CONFIG_PROC_SYSCTL
+ int sched_energy_aware_handler(struct ctl_table *table, int write,
+ 			 void __user *buffer, size_t *lenp, loff_t *ppos)
+ {
+ 	int ret, state;
+ 
+ 	if (write && !capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
+ 	if (!ret && write) {
+ 		state = static_branch_unlikely(&sched_energy_present);
+ 		if (state != sysctl_sched_energy_aware) {
+ 			mutex_lock(&sched_energy_mutex);
+ 			sched_energy_update = 1;
+ 			rebuild_sched_domains();
+ 			sched_energy_update = 0;
+ 			mutex_unlock(&sched_energy_mutex);
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ #endif
+ 
+ static void free_pd(struct perf_domain *pd)
+ {
+ 	struct perf_domain *tmp;
+ 
+ 	while (pd) {
+ 		tmp = pd->next;
+ 		kfree(pd);
+ 		pd = tmp;
+ 	}
+ }
+ 
+ static struct perf_domain *find_pd(struct perf_domain *pd, int cpu)
+ {
+ 	while (pd) {
+ 		if (cpumask_test_cpu(cpu, perf_domain_span(pd)))
+ 			return pd;
+ 		pd = pd->next;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static struct perf_domain *pd_init(int cpu)
+ {
+ 	struct em_perf_domain *obj = em_cpu_get(cpu);
+ 	struct perf_domain *pd;
+ 
+ 	if (!obj) {
+ 		if (sched_debug())
+ 			pr_info("%s: no EM found for CPU%d\n", __func__, cpu);
+ 		return NULL;
+ 	}
+ 
+ 	pd = kzalloc(sizeof(*pd), GFP_KERNEL);
+ 	if (!pd)
+ 		return NULL;
+ 	pd->em_pd = obj;
+ 
+ 	return pd;
+ }
+ 
+ static void perf_domain_debug(const struct cpumask *cpu_map,
+ 						struct perf_domain *pd)
+ {
+ 	if (!sched_debug() || !pd)
+ 		return;
+ 
+ 	printk(KERN_DEBUG "root_domain %*pbl:", cpumask_pr_args(cpu_map));
+ 
+ 	while (pd) {
+ 		printk(KERN_CONT " pd%d:{ cpus=%*pbl nr_cstate=%d }",
+ 				cpumask_first(perf_domain_span(pd)),
+ 				cpumask_pr_args(perf_domain_span(pd)),
+ 				em_pd_nr_cap_states(pd->em_pd));
+ 		pd = pd->next;
+ 	}
+ 
+ 	printk(KERN_CONT "\n");
+ }
+ 
+ static void destroy_perf_domain_rcu(struct rcu_head *rp)
+ {
+ 	struct perf_domain *pd;
+ 
+ 	pd = container_of(rp, struct perf_domain, rcu);
+ 	free_pd(pd);
+ }
+ 
+ static void sched_energy_set(bool has_eas)
+ {
+ 	if (!has_eas && static_branch_unlikely(&sched_energy_present)) {
+ 		if (sched_debug())
+ 			pr_info("%s: stopping EAS\n", __func__);
+ 		static_branch_disable_cpuslocked(&sched_energy_present);
+ 	} else if (has_eas && !static_branch_unlikely(&sched_energy_present)) {
+ 		if (sched_debug())
+ 			pr_info("%s: starting EAS\n", __func__);
+ 		static_branch_enable_cpuslocked(&sched_energy_present);
+ 	}
+ }
+ 
+ /*
+  * EAS can be used on a root domain if it meets all the following conditions:
+  *    1. an Energy Model (EM) is available;
+  *    2. the SD_ASYM_CPUCAPACITY flag is set in the sched_domain hierarchy.
+  *    3. the EM complexity is low enough to keep scheduling overheads low;
+  *    4. schedutil is driving the frequency of all CPUs of the rd;
+  *
+  * The complexity of the Energy Model is defined as:
+  *
+  *              C = nr_pd * (nr_cpus + nr_cs)
+  *
+  * with parameters defined as:
+  *  - nr_pd:    the number of performance domains
+  *  - nr_cpus:  the number of CPUs
+  *  - nr_cs:    the sum of the number of capacity states of all performance
+  *              domains (for example, on a system with 2 performance domains,
+  *              with 10 capacity states each, nr_cs = 2 * 10 = 20).
+  *
+  * It is generally not a good idea to use such a model in the wake-up path on
+  * very complex platforms because of the associated scheduling overheads. The
+  * arbitrary constraint below prevents that. It makes EAS usable up to 16 CPUs
+  * with per-CPU DVFS and less than 8 capacity states each, for example.
+  */
+ #define EM_MAX_COMPLEXITY 2048
+ 
+ extern struct cpufreq_governor schedutil_gov;
+ static bool build_perf_domains(const struct cpumask *cpu_map)
+ {
+ 	int i, nr_pd = 0, nr_cs = 0, nr_cpus = cpumask_weight(cpu_map);
+ 	struct perf_domain *pd = NULL, *tmp;
+ 	int cpu = cpumask_first(cpu_map);
+ 	struct root_domain *rd = cpu_rq(cpu)->rd;
+ 	struct cpufreq_policy *policy;
+ 	struct cpufreq_governor *gov;
+ 
+ 	if (!sysctl_sched_energy_aware)
+ 		goto free;
+ 
+ 	/* EAS is enabled for asymmetric CPU capacity topologies. */
+ 	if (!per_cpu(sd_asym_cpucapacity, cpu)) {
+ 		if (sched_debug()) {
+ 			pr_info("rd %*pbl: CPUs do not have asymmetric capacities\n",
+ 					cpumask_pr_args(cpu_map));
+ 		}
+ 		goto free;
+ 	}
+ 
+ 	for_each_cpu(i, cpu_map) {
+ 		/* Skip already covered CPUs. */
+ 		if (find_pd(pd, i))
+ 			continue;
+ 
+ 		/* Do not attempt EAS if schedutil is not being used. */
+ 		policy = cpufreq_cpu_get(i);
+ 		if (!policy)
+ 			goto free;
+ 		gov = policy->governor;
+ 		cpufreq_cpu_put(policy);
+ 		if (gov != &schedutil_gov) {
+ 			if (rd->pd)
+ 				pr_warn("rd %*pbl: Disabling EAS, schedutil is mandatory\n",
+ 						cpumask_pr_args(cpu_map));
+ 			goto free;
+ 		}
+ 
+ 		/* Create the new pd and add it to the local list. */
+ 		tmp = pd_init(i);
+ 		if (!tmp)
+ 			goto free;
+ 		tmp->next = pd;
+ 		pd = tmp;
+ 
+ 		/*
+ 		 * Count performance domains and capacity states for the
+ 		 * complexity check.
+ 		 */
+ 		nr_pd++;
+ 		nr_cs += em_pd_nr_cap_states(pd->em_pd);
+ 	}
+ 
+ 	/* Bail out if the Energy Model complexity is too high. */
+ 	if (nr_pd * (nr_cs + nr_cpus) > EM_MAX_COMPLEXITY) {
+ 		WARN(1, "rd %*pbl: Failed to start EAS, EM complexity is too high\n",
+ 						cpumask_pr_args(cpu_map));
+ 		goto free;
+ 	}
+ 
+ 	perf_domain_debug(cpu_map, pd);
+ 
+ 	/* Attach the new list of performance domains to the root domain. */
+ 	tmp = rd->pd;
+ 	rcu_assign_pointer(rd->pd, pd);
+ 	if (tmp)
+ 		call_rcu(&tmp->rcu, destroy_perf_domain_rcu);
+ 
+ 	return !!pd;
+ 
+ free:
+ 	free_pd(pd);
+ 	tmp = rd->pd;
+ 	rcu_assign_pointer(rd->pd, NULL);
+ 	if (tmp)
+ 		call_rcu(&tmp->rcu, destroy_perf_domain_rcu);
+ 
+ 	return false;
+ }
+ #else
+ static void free_pd(struct perf_domain *pd) { }
+ #endif /* CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL*/
+ 
++>>>>>>> 8d5d0cfb63cb (sched/topology: Introduce a sysctl for Energy Aware Scheduling)
  static void free_rootdomain(struct rcu_head *rcu)
  {
  	struct root_domain *rd = container_of(rcu, struct root_domain, rcu);
diff --git a/Documentation/sysctl/kernel.txt b/Documentation/sysctl/kernel.txt
index 444ecace8667..4185d661f184 100644
--- a/Documentation/sysctl/kernel.txt
+++ b/Documentation/sysctl/kernel.txt
@@ -77,6 +77,7 @@ show up in /proc/sys/kernel:
 - reboot-cmd                  [ SPARC only ]
 - rtsig-max
 - rtsig-nr
+- sched_energy_aware
 - seccomp/                    ==> Documentation/userspace-api/seccomp_filter.rst
 - sem
 - sem_next_id		      [ sysv ipc ]
@@ -859,6 +860,17 @@ rtsig-nr shows the number of RT signals currently queued.
 
 ==============================================================
 
+sched_energy_aware:
+
+Enables/disables Energy Aware Scheduling (EAS). EAS starts
+automatically on platforms where it can run (that is,
+platforms with asymmetric CPU topologies and having an Energy
+Model available). If your platform happens to meet the
+requirements for EAS but you do not want to use it, change
+this value to 0.
+
+==============================================================
+
 sched_schedstats:
 
 Enables/disables scheduler statistics. Enabling this feature
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 913488d828cb..68e900f1c94c 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -82,4 +82,11 @@ extern int sysctl_schedstats(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp,
 				 loff_t *ppos);
 
+#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+extern unsigned int sysctl_sched_energy_aware;
+extern int sched_energy_aware_handler(struct ctl_table *table, int write,
+				 void __user *buffer, size_t *lenp,
+				 loff_t *ppos);
+#endif
+
 #endif /* _LINUX_SCHED_SYSCTL_H */
* Unmerged path kernel/sched/topology.c
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 0cdce22f2d79..db40fb7b8660 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -467,6 +467,17 @@ static struct ctl_table kern_table[] = {
 		.extra1		= &one,
 	},
 #endif
+#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+	{
+		.procname	= "sched_energy_aware",
+		.data		= &sysctl_sched_energy_aware,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= sched_energy_aware_handler,
+		.extra1		= &zero,
+		.extra2		= &one,
+	},
+#endif
 #ifdef CONFIG_PROVE_LOCKING
 	{
 		.procname	= "prove_locking",
