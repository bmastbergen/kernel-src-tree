SUNRPC: Initiate a connection close on an ESHUTDOWN error in stream receive

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Trond Myklebust <trond.myklebust@hammerspace.com>
commit 5f52a9d429b8ba8f9d669730adf16bc534eb74ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/5f52a9d4.failed

If the client stream receive code receives an ESHUTDOWN error either
because the server closed the connection, or because it sent a
callback which cannot be processed, then we should shut down
the connection.

	Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
(cherry picked from commit 5f52a9d429b8ba8f9d669730adf16bc534eb74ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtsock.c
diff --cc net/sunrpc/xprtsock.c
index 3865a12ff7a6,a3aadc04808f..000000000000
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@@ -321,42 -323,425 +321,462 @@@ static void xs_free_peer_addresses(stru
  		}
  }
  
++<<<<<<< HEAD
++=======
+ static size_t
+ xs_alloc_sparse_pages(struct xdr_buf *buf, size_t want, gfp_t gfp)
+ {
+ 	size_t i,n;
+ 
+ 	if (!want || !(buf->flags & XDRBUF_SPARSE_PAGES))
+ 		return want;
+ 	n = (buf->page_base + want + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 	for (i = 0; i < n; i++) {
+ 		if (buf->pages[i])
+ 			continue;
+ 		buf->bvec[i].bv_page = buf->pages[i] = alloc_page(gfp);
+ 		if (!buf->pages[i]) {
+ 			i *= PAGE_SIZE;
+ 			return i > buf->page_base ? i - buf->page_base : 0;
+ 		}
+ 	}
+ 	return want;
+ }
+ 
+ static ssize_t
+ xs_sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags, size_t seek)
+ {
+ 	ssize_t ret;
+ 	if (seek != 0)
+ 		iov_iter_advance(&msg->msg_iter, seek);
+ 	ret = sock_recvmsg(sock, msg, flags);
+ 	return ret > 0 ? ret + seek : ret;
+ }
+ 
+ static ssize_t
+ xs_read_kvec(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct kvec *kvec, size_t count, size_t seek)
+ {
+ 	iov_iter_kvec(&msg->msg_iter, READ, kvec, 1, count);
+ 	return xs_sock_recvmsg(sock, msg, flags, seek);
+ }
+ 
+ static ssize_t
+ xs_read_bvec(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct bio_vec *bvec, unsigned long nr, size_t count,
+ 		size_t seek)
+ {
+ 	iov_iter_bvec(&msg->msg_iter, READ, bvec, nr, count);
+ 	return xs_sock_recvmsg(sock, msg, flags, seek);
+ }
+ 
+ static ssize_t
+ xs_read_discard(struct socket *sock, struct msghdr *msg, int flags,
+ 		size_t count)
+ {
+ 	iov_iter_discard(&msg->msg_iter, READ, count);
+ 	return sock_recvmsg(sock, msg, flags);
+ }
+ 
+ #if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
+ static void
+ xs_flush_bvec(const struct bio_vec *bvec, size_t count, size_t seek)
+ {
+ 	struct bvec_iter bi = {
+ 		.bi_size = count,
+ 	};
+ 	struct bio_vec bv;
+ 
+ 	bvec_iter_advance(bvec, &bi, seek & PAGE_MASK);
+ 	for_each_bvec(bv, bvec, bi, bi)
+ 		flush_dcache_page(bv.bv_page);
+ }
+ #else
+ static inline void
+ xs_flush_bvec(const struct bio_vec *bvec, size_t count, size_t seek)
+ {
+ }
+ #endif
+ 
+ static ssize_t
+ xs_read_xdr_buf(struct socket *sock, struct msghdr *msg, int flags,
+ 		struct xdr_buf *buf, size_t count, size_t seek, size_t *read)
+ {
+ 	size_t want, seek_init = seek, offset = 0;
+ 	ssize_t ret;
+ 
+ 	want = min_t(size_t, count, buf->head[0].iov_len);
+ 	if (seek < want) {
+ 		ret = xs_read_kvec(sock, msg, flags, &buf->head[0], want, seek);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		offset += ret;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto out;
+ 		seek = 0;
+ 	} else {
+ 		seek -= want;
+ 		offset += want;
+ 	}
+ 
+ 	want = xs_alloc_sparse_pages(buf,
+ 			min_t(size_t, count - offset, buf->page_len),
+ 			GFP_NOWAIT);
+ 	if (seek < want) {
+ 		ret = xs_read_bvec(sock, msg, flags, buf->bvec,
+ 				xdr_buf_pagecount(buf),
+ 				want + buf->page_base,
+ 				seek + buf->page_base);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		xs_flush_bvec(buf->bvec, ret, seek + buf->page_base);
+ 		offset += ret - buf->page_base;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto out;
+ 		seek = 0;
+ 	} else {
+ 		seek -= want;
+ 		offset += want;
+ 	}
+ 
+ 	want = min_t(size_t, count - offset, buf->tail[0].iov_len);
+ 	if (seek < want) {
+ 		ret = xs_read_kvec(sock, msg, flags, &buf->tail[0], want, seek);
+ 		if (ret <= 0)
+ 			goto sock_err;
+ 		offset += ret;
+ 		if (offset == count || msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 			goto out;
+ 		if (ret != want)
+ 			goto out;
+ 	} else
+ 		offset = seek_init;
+ 	ret = -EMSGSIZE;
+ out:
+ 	*read = offset - seek_init;
+ 	return ret;
+ sock_err:
+ 	offset += seek;
+ 	goto out;
+ }
+ 
+ static void
+ xs_read_header(struct sock_xprt *transport, struct xdr_buf *buf)
+ {
+ 	if (!transport->recv.copied) {
+ 		if (buf->head[0].iov_len >= transport->recv.offset)
+ 			memcpy(buf->head[0].iov_base,
+ 					&transport->recv.xid,
+ 					transport->recv.offset);
+ 		transport->recv.copied = transport->recv.offset;
+ 	}
+ }
+ 
+ static bool
+ xs_read_stream_request_done(struct sock_xprt *transport)
+ {
+ 	return transport->recv.fraghdr & cpu_to_be32(RPC_LAST_STREAM_FRAGMENT);
+ }
+ 
+ static void
+ xs_read_stream_check_eor(struct sock_xprt *transport,
+ 		struct msghdr *msg)
+ {
+ 	if (xs_read_stream_request_done(transport))
+ 		msg->msg_flags |= MSG_EOR;
+ }
+ 
+ static ssize_t
+ xs_read_stream_request(struct sock_xprt *transport, struct msghdr *msg,
+ 		int flags, struct rpc_rqst *req)
+ {
+ 	struct xdr_buf *buf = &req->rq_private_buf;
+ 	size_t want, read;
+ 	ssize_t ret;
+ 
+ 	xs_read_header(transport, buf);
+ 
+ 	want = transport->recv.len - transport->recv.offset;
+ 	if (want != 0) {
+ 		ret = xs_read_xdr_buf(transport->sock, msg, flags, buf,
+ 				transport->recv.copied + want,
+ 				transport->recv.copied,
+ 				&read);
+ 		transport->recv.offset += read;
+ 		transport->recv.copied += read;
+ 	}
+ 
+ 	if (transport->recv.offset == transport->recv.len)
+ 		xs_read_stream_check_eor(transport, msg);
+ 
+ 	if (want == 0)
+ 		return 0;
+ 
+ 	switch (ret) {
+ 	default:
+ 		break;
+ 	case -EFAULT:
+ 	case -EMSGSIZE:
+ 		msg->msg_flags |= MSG_TRUNC;
+ 		return read;
+ 	case 0:
+ 		return -ESHUTDOWN;
+ 	}
+ 	return ret < 0 ? ret : read;
+ }
+ 
+ static size_t
+ xs_read_stream_headersize(bool isfrag)
+ {
+ 	if (isfrag)
+ 		return sizeof(__be32);
+ 	return 3 * sizeof(__be32);
+ }
+ 
+ static ssize_t
+ xs_read_stream_header(struct sock_xprt *transport, struct msghdr *msg,
+ 		int flags, size_t want, size_t seek)
+ {
+ 	struct kvec kvec = {
+ 		.iov_base = &transport->recv.fraghdr,
+ 		.iov_len = want,
+ 	};
+ 	return xs_read_kvec(transport->sock, msg, flags, &kvec, want, seek);
+ }
+ 
+ #if defined(CONFIG_SUNRPC_BACKCHANNEL)
+ static ssize_t
+ xs_read_stream_call(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	struct rpc_xprt *xprt = &transport->xprt;
+ 	struct rpc_rqst *req;
+ 	ssize_t ret;
+ 
+ 	/* Look up and lock the request corresponding to the given XID */
+ 	req = xprt_lookup_bc_request(xprt, transport->recv.xid);
+ 	if (!req) {
+ 		printk(KERN_WARNING "Callback slot table overflowed\n");
+ 		return -ESHUTDOWN;
+ 	}
+ 
+ 	ret = xs_read_stream_request(transport, msg, flags, req);
+ 	if (msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 		xprt_complete_bc_request(req, transport->recv.copied);
+ 
+ 	return ret;
+ }
+ #else /* CONFIG_SUNRPC_BACKCHANNEL */
+ static ssize_t
+ xs_read_stream_call(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	return -ESHUTDOWN;
+ }
+ #endif /* CONFIG_SUNRPC_BACKCHANNEL */
+ 
+ static ssize_t
+ xs_read_stream_reply(struct sock_xprt *transport, struct msghdr *msg, int flags)
+ {
+ 	struct rpc_xprt *xprt = &transport->xprt;
+ 	struct rpc_rqst *req;
+ 	ssize_t ret = 0;
+ 
+ 	/* Look up and lock the request corresponding to the given XID */
+ 	spin_lock(&xprt->queue_lock);
+ 	req = xprt_lookup_rqst(xprt, transport->recv.xid);
+ 	if (!req) {
+ 		msg->msg_flags |= MSG_TRUNC;
+ 		goto out;
+ 	}
+ 	xprt_pin_rqst(req);
+ 	spin_unlock(&xprt->queue_lock);
+ 
+ 	ret = xs_read_stream_request(transport, msg, flags, req);
+ 
+ 	spin_lock(&xprt->queue_lock);
+ 	if (msg->msg_flags & (MSG_EOR|MSG_TRUNC))
+ 		xprt_complete_rqst(req->rq_task, transport->recv.copied);
+ 	xprt_unpin_rqst(req);
+ out:
+ 	spin_unlock(&xprt->queue_lock);
+ 	return ret;
+ }
+ 
+ static ssize_t
+ xs_read_stream(struct sock_xprt *transport, int flags)
+ {
+ 	struct msghdr msg = { 0 };
+ 	size_t want, read = 0;
+ 	ssize_t ret = 0;
+ 
+ 	if (transport->recv.len == 0) {
+ 		want = xs_read_stream_headersize(transport->recv.copied != 0);
+ 		ret = xs_read_stream_header(transport, &msg, flags, want,
+ 				transport->recv.offset);
+ 		if (ret <= 0)
+ 			goto out_err;
+ 		transport->recv.offset = ret;
+ 		if (transport->recv.offset != want)
+ 			return transport->recv.offset;
+ 		transport->recv.len = be32_to_cpu(transport->recv.fraghdr) &
+ 			RPC_FRAGMENT_SIZE_MASK;
+ 		transport->recv.offset -= sizeof(transport->recv.fraghdr);
+ 		read = ret;
+ 	}
+ 
+ 	switch (be32_to_cpu(transport->recv.calldir)) {
+ 	default:
+ 		msg.msg_flags |= MSG_TRUNC;
+ 		break;
+ 	case RPC_CALL:
+ 		ret = xs_read_stream_call(transport, &msg, flags);
+ 		break;
+ 	case RPC_REPLY:
+ 		ret = xs_read_stream_reply(transport, &msg, flags);
+ 	}
+ 	if (msg.msg_flags & MSG_TRUNC) {
+ 		transport->recv.calldir = cpu_to_be32(-1);
+ 		transport->recv.copied = -1;
+ 	}
+ 	if (ret < 0)
+ 		goto out_err;
+ 	read += ret;
+ 	if (transport->recv.offset < transport->recv.len) {
+ 		if (!(msg.msg_flags & MSG_TRUNC))
+ 			return read;
+ 		msg.msg_flags = 0;
+ 		ret = xs_read_discard(transport->sock, &msg, flags,
+ 				transport->recv.len - transport->recv.offset);
+ 		if (ret <= 0)
+ 			goto out_err;
+ 		transport->recv.offset += ret;
+ 		read += ret;
+ 		if (transport->recv.offset != transport->recv.len)
+ 			return read;
+ 	}
+ 	if (xs_read_stream_request_done(transport)) {
+ 		trace_xs_stream_read_request(transport);
+ 		transport->recv.copied = 0;
+ 	}
+ 	transport->recv.offset = 0;
+ 	transport->recv.len = 0;
+ 	return read;
+ out_err:
+ 	return ret != 0 ? ret : -ESHUTDOWN;
+ }
+ 
+ static __poll_t xs_poll_socket(struct sock_xprt *transport)
+ {
+ 	return transport->sock->ops->poll(NULL, transport->sock, NULL);
+ }
+ 
+ static bool xs_poll_socket_readable(struct sock_xprt *transport)
+ {
+ 	__poll_t events = xs_poll_socket(transport);
+ 
+ 	return (events & (EPOLLIN | EPOLLRDNORM)) && !(events & EPOLLRDHUP);
+ }
+ 
+ static void xs_poll_check_readable(struct sock_xprt *transport)
+ {
+ 
+ 	clear_bit(XPRT_SOCK_DATA_READY, &transport->sock_state);
+ 	if (!xs_poll_socket_readable(transport))
+ 		return;
+ 	if (!test_and_set_bit(XPRT_SOCK_DATA_READY, &transport->sock_state))
+ 		queue_work(xprtiod_workqueue, &transport->recv_worker);
+ }
+ 
+ static void xs_stream_data_receive(struct sock_xprt *transport)
+ {
+ 	size_t read = 0;
+ 	ssize_t ret = 0;
+ 
+ 	mutex_lock(&transport->recv_mutex);
+ 	if (transport->sock == NULL)
+ 		goto out;
+ 	for (;;) {
+ 		ret = xs_read_stream(transport, MSG_DONTWAIT);
+ 		if (ret < 0)
+ 			break;
+ 		read += ret;
+ 		cond_resched();
+ 	}
+ 	if (ret == -ESHUTDOWN)
+ 		kernel_sock_shutdown(transport->sock, SHUT_RDWR);
+ 	else
+ 		xs_poll_check_readable(transport);
+ out:
+ 	mutex_unlock(&transport->recv_mutex);
+ 	trace_xs_stream_read_data(&transport->xprt, ret, read);
+ }
+ 
+ static void xs_stream_data_receive_workfn(struct work_struct *work)
+ {
+ 	struct sock_xprt *transport =
+ 		container_of(work, struct sock_xprt, recv_worker);
+ 	unsigned int pflags = memalloc_nofs_save();
+ 
+ 	xs_stream_data_receive(transport);
+ 	memalloc_nofs_restore(pflags);
+ }
+ 
+ static void
+ xs_stream_reset_connect(struct sock_xprt *transport)
+ {
+ 	transport->recv.offset = 0;
+ 	transport->recv.len = 0;
+ 	transport->recv.copied = 0;
+ 	transport->xmit.offset = 0;
+ }
+ 
+ static void
+ xs_stream_start_connect(struct sock_xprt *transport)
+ {
+ 	transport->xprt.stat.connect_count++;
+ 	transport->xprt.stat.connect_start = jiffies;
+ }
+ 
++>>>>>>> 5f52a9d429b8 (SUNRPC: Initiate a connection close on an ESHUTDOWN error in stream receive)
  #define XS_SENDMSG_FLAGS	(MSG_DONTWAIT | MSG_NOSIGNAL)
  
 +/* Common case:
 + *  - stream transport
 + *  - sending from byte 0 of the message
 + *  - the message is wholly contained in @xdr's head iovec
 + */
 +static int xs_send_rm_and_kvec(struct socket *sock, struct xdr_buf *xdr,
 +			       unsigned int remainder)
 +{
 +	struct msghdr msg = {
 +		.msg_flags	= XS_SENDMSG_FLAGS | (remainder ? MSG_MORE : 0)
 +	};
 +	rpc_fraghdr marker = cpu_to_be32(RPC_LAST_STREAM_FRAGMENT |
 +					 (u32)xdr->len);
 +	struct kvec iov[2] = {
 +		{
 +			.iov_base	= &marker,
 +			.iov_len	= sizeof(marker)
 +		},
 +		{
 +			.iov_base	= xdr->head[0].iov_base,
 +			.iov_len	= xdr->head[0].iov_len
 +		},
 +	};
 +	int ret;
 +
 +	ret = kernel_sendmsg(sock, &msg, iov, 2,
 +			     iov[0].iov_len + iov[1].iov_len);
 +	if (ret < 0)
 +		return ret;
 +	if (ret < iov[0].iov_len)
 +		return -EPIPE;
 +	return ret - iov[0].iov_len;
 +}
 +
  static int xs_send_kvec(struct socket *sock, struct sockaddr *addr, int addrlen, struct kvec *vec, unsigned int base, int more)
  {
  	struct msghdr msg = {
* Unmerged path net/sunrpc/xprtsock.c
