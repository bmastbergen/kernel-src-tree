locking/lockdep: Zap lock classes even with lock debugging disabled

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Bart Van Assche <bvanassche@acm.org>
commit 90c1cba2b3b3851c151229f61801919b2904d437
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/90c1cba2.failed

The following commit:

  a0b0fd53e1e6 ("locking/lockdep: Free lock classes that are no longer in use")

changed the behavior of lockdep_free_key_range() from
unconditionally zapping lock classes into only zapping lock classes if
debug_lock == true. Not zapping lock classes if debug_lock == false leaves
dangling pointers in several lockdep datastructures, e.g. lock_class::name
in the all_lock_classes list.

The shell command "cat /proc/lockdep" causes the kernel to iterate the
all_lock_classes list. Hence the "unable to handle kernel paging request" cash
that Shenghui encountered by running cat /proc/lockdep.

Since the new behavior can cause cat /proc/lockdep to crash, restore the
pre-v5.1 behavior.

This patch avoids that cat /proc/lockdep triggers the following crash
with debug_lock == false:

  BUG: unable to handle kernel paging request at fffffbfff40ca448
  RIP: 0010:__asan_load1+0x28/0x50
  Call Trace:
   string+0xac/0x180
   vsnprintf+0x23e/0x820
   seq_vprintf+0x82/0xc0
   seq_printf+0x92/0xb0
   print_name+0x34/0xb0
   l_show+0x184/0x200
   seq_read+0x59e/0x6c0
   proc_reg_read+0x11f/0x170
   __vfs_read+0x4d/0x90
   vfs_read+0xc5/0x1f0
   ksys_read+0xab/0x130
   __x64_sys_read+0x43/0x50
   do_syscall_64+0x71/0x210
   entry_SYSCALL_64_after_hwframe+0x49/0xbe

	Reported-by: shenghui <shhuiw@foxmail.com>
	Signed-off-by: Bart Van Assche <bvanassche@acm.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Waiman Long <longman@redhat.com>
	Cc: Will Deacon <will.deacon@arm.com>
Fixes: a0b0fd53e1e6 ("locking/lockdep: Free lock classes that are no longer in use") # v5.1-rc1.
Link: https://lkml.kernel.org/r/20190403233552.124673-1-bvanassche@acm.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 90c1cba2b3b3851c151229f61801919b2904d437)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index aed5c03ce876,e16766ff184b..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -4205,7 -4626,95 +4205,99 @@@ static inline int within(const void *ad
  	return addr >= start && addr < start + size;
  }
  
++<<<<<<< HEAD
 +static void __lockdep_free_key_range(void *start, unsigned long size)
++=======
+ static bool inside_selftest(void)
+ {
+ 	return current == lockdep_selftest_task_struct;
+ }
+ 
+ /* The caller must hold the graph lock. */
+ static struct pending_free *get_pending_free(void)
+ {
+ 	return delayed_free.pf + delayed_free.index;
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *cb);
+ 
+ /*
+  * Schedule an RCU callback if no RCU callback is pending. Must be called with
+  * the graph lock held.
+  */
+ static void call_rcu_zapped(struct pending_free *pf)
+ {
+ 	WARN_ON_ONCE(inside_selftest());
+ 
+ 	if (list_empty(&pf->zapped))
+ 		return;
+ 
+ 	if (delayed_free.scheduled)
+ 		return;
+ 
+ 	delayed_free.scheduled = true;
+ 
+ 	WARN_ON_ONCE(delayed_free.pf + delayed_free.index != pf);
+ 	delayed_free.index ^= 1;
+ 
+ 	call_rcu(&delayed_free.rcu_head, free_zapped_rcu);
+ }
+ 
+ /* The caller must hold the graph lock. May be called from RCU context. */
+ static void __free_zapped_classes(struct pending_free *pf)
+ {
+ 	struct lock_class *class;
+ 
+ 	check_data_structures();
+ 
+ 	list_for_each_entry(class, &pf->zapped, lock_entry)
+ 		reinit_class(class);
+ 
+ 	list_splice_init(&pf->zapped, &free_lock_classes);
+ 
+ #ifdef CONFIG_PROVE_LOCKING
+ 	bitmap_andnot(lock_chains_in_use, lock_chains_in_use,
+ 		      pf->lock_chains_being_freed, ARRAY_SIZE(lock_chains));
+ 	bitmap_clear(pf->lock_chains_being_freed, 0, ARRAY_SIZE(lock_chains));
+ #endif
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *ch)
+ {
+ 	struct pending_free *pf;
+ 	unsigned long flags;
+ 
+ 	if (WARN_ON_ONCE(ch != &delayed_free.rcu_head))
+ 		return;
+ 
+ 	raw_local_irq_save(flags);
+ 	arch_spin_lock(&lockdep_lock);
+ 	current->lockdep_recursion = 1;
+ 
+ 	/* closed head */
+ 	pf = delayed_free.pf + (delayed_free.index ^ 1);
+ 	__free_zapped_classes(pf);
+ 	delayed_free.scheduled = false;
+ 
+ 	/*
+ 	 * If there's anything on the open list, close and start a new callback.
+ 	 */
+ 	call_rcu_zapped(delayed_free.pf + delayed_free.index);
+ 
+ 	current->lockdep_recursion = 0;
+ 	arch_spin_unlock(&lockdep_lock);
+ 	raw_local_irq_restore(flags);
+ }
+ 
+ /*
+  * Remove all lock classes from the class hash table and from the
+  * all_lock_classes list whose key or name is in the address range [start,
+  * start + size). Move these lock classes to the zapped_classes list. Must
+  * be called with the graph lock held.
+  */
+ static void __lockdep_free_key_range(struct pending_free *pf, void *start,
+ 				     unsigned long size)
++>>>>>>> 90c1cba2b3b3 (locking/lockdep: Zap lock classes even with lock debugging disabled)
  {
  	struct lock_class *class;
  	struct hlist_head *head;
@@@ -4231,18 -4740,21 +4323,27 @@@
   * guaranteed nobody will look up these exact classes -- they're properly dead
   * but still allocated.
   */
 -static void lockdep_free_key_range_reg(void *start, unsigned long size)
 +void lockdep_free_key_range(void *start, unsigned long size)
  {
 -	struct pending_free *pf;
  	unsigned long flags;
- 	int locked;
  
  	init_data_structures_once();
  
  	raw_local_irq_save(flags);
++<<<<<<< HEAD
 +	locked = graph_lock();
 +	__lockdep_free_key_range(start, size);
 +	if (locked)
 +		graph_unlock();
++=======
+ 	arch_spin_lock(&lockdep_lock);
+ 	current->lockdep_recursion = 1;
+ 	pf = get_pending_free();
+ 	__lockdep_free_key_range(pf, start, size);
+ 	call_rcu_zapped(pf);
+ 	current->lockdep_recursion = 0;
+ 	arch_spin_unlock(&lockdep_lock);
++>>>>>>> 90c1cba2b3b3 (locking/lockdep: Zap lock classes even with lock debugging disabled)
  	raw_local_irq_restore(flags);
  
  	/*
@@@ -4310,17 -4850,86 +4411,35 @@@ void lockdep_reset_lock(struct lockdep_
  	unsigned long flags;
  	int locked;
  
 -	raw_local_irq_save(flags);
 -	locked = graph_lock();
 -	if (!locked)
 -		goto out_irq;
 -
 -	pf = get_pending_free();
 -	__lockdep_reset_lock(pf, lock);
 -	call_rcu_zapped(pf);
 -
 -	graph_unlock();
 -out_irq:
 -	raw_local_irq_restore(flags);
 -}
 -
 -/*
 - * Reset a lock. Does not sleep. Ignores debug_locks. Must only be used by the
 - * lockdep selftests.
 - */
 -static void lockdep_reset_lock_imm(struct lockdep_map *lock)
 -{
 -	struct pending_free *pf = delayed_free.pf;
 -	unsigned long flags;
 -
 -	raw_local_irq_save(flags);
 -	arch_spin_lock(&lockdep_lock);
 -	__lockdep_reset_lock(pf, lock);
 -	__free_zapped_classes(pf);
 -	arch_spin_unlock(&lockdep_lock);
 -	raw_local_irq_restore(flags);
 -}
 -
 -void lockdep_reset_lock(struct lockdep_map *lock)
 -{
  	init_data_structures_once();
  
 -	if (inside_selftest())
 -		lockdep_reset_lock_imm(lock);
 -	else
 -		lockdep_reset_lock_reg(lock);
 -}
 -
 -/* Unregister a dynamically allocated key. */
 -void lockdep_unregister_key(struct lock_class_key *key)
 -{
 -	struct hlist_head *hash_head = keyhashentry(key);
 -	struct lock_class_key *k;
 -	struct pending_free *pf;
 -	unsigned long flags;
 -	bool found = false;
 -
 -	might_sleep();
 -
 -	if (WARN_ON_ONCE(static_obj(key)))
 -		return;
 -
  	raw_local_irq_save(flags);
++<<<<<<< HEAD
 +	locked = graph_lock();
 +	__lockdep_reset_lock(lock);
 +	if (locked)
 +		graph_unlock();
++=======
+ 	arch_spin_lock(&lockdep_lock);
+ 	current->lockdep_recursion = 1;
+ 	pf = get_pending_free();
+ 	hlist_for_each_entry_rcu(k, hash_head, hash_entry) {
+ 		if (k == key) {
+ 			hlist_del_rcu(&k->hash_entry);
+ 			found = true;
+ 			break;
+ 		}
+ 	}
+ 	WARN_ON_ONCE(!found);
+ 	__lockdep_free_key_range(pf, key, 1);
+ 	call_rcu_zapped(pf);
+ 	current->lockdep_recursion = 0;
+ 	arch_spin_unlock(&lockdep_lock);
++>>>>>>> 90c1cba2b3b3 (locking/lockdep: Zap lock classes even with lock debugging disabled)
  	raw_local_irq_restore(flags);
 -
 -	/* Wait until is_dynamic_key() has finished accessing k->hash_entry. */
 -	synchronize_rcu();
  }
 -EXPORT_SYMBOL_GPL(lockdep_unregister_key);
  
 -void __init lockdep_init(void)
 +void __init lockdep_info(void)
  {
  	printk("Lock dependency validator: Copyright (c) 2006 Red Hat, Inc., Ingo Molnar\n");
  
* Unmerged path kernel/locking/lockdep.c
