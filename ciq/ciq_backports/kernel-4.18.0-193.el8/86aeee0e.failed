SUNRPC: Enqueue swapper tagged RPCs at the head of the transmit queue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Trond Myklebust <trond.myklebust@hammerspace.com>
commit 86aeee0eb6c316e75d3b5d148177d4c01b81c977
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/86aeee0e.failed

Avoid memory starvation by giving RPCs that are tagged with the
RPC_TASK_SWAPPER flag the highest priority.

	Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
(cherry picked from commit 86aeee0eb6c316e75d3b5d148177d4c01b81c977)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprt.c
diff --cc net/sunrpc/xprt.c
index eacde9249456,d7585458dfe4..000000000000
--- a/net/sunrpc/xprt.c
+++ b/net/sunrpc/xprt.c
@@@ -956,6 -1046,156 +956,159 @@@ static void xprt_timer(struct rpc_task 
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * xprt_request_wait_receive - wait for the reply to an RPC request
+  * @task: RPC task about to send a request
+  *
+  */
+ void xprt_request_wait_receive(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	if (!test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))
+ 		return;
+ 	/*
+ 	 * Sleep on the pending queue if we're expecting a reply.
+ 	 * The spinlock ensures atomicity between the test of
+ 	 * req->rq_reply_bytes_recvd, and the call to rpc_sleep_on().
+ 	 */
+ 	spin_lock(&xprt->queue_lock);
+ 	if (test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate)) {
+ 		xprt->ops->set_retrans_timeout(task);
+ 		rpc_sleep_on(&xprt->pending, task, xprt_timer);
+ 		/*
+ 		 * Send an extra queue wakeup call if the
+ 		 * connection was dropped in case the call to
+ 		 * rpc_sleep_on() raced.
+ 		 */
+ 		if (xprt_request_retransmit_after_disconnect(task))
+ 			rpc_wake_up_queued_task_set_status(&xprt->pending,
+ 					task, -ENOTCONN);
+ 	}
+ 	spin_unlock(&xprt->queue_lock);
+ }
+ 
+ static bool
+ xprt_request_need_enqueue_transmit(struct rpc_task *task, struct rpc_rqst *req)
+ {
+ 	return !test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);
+ }
+ 
+ /**
+  * xprt_request_enqueue_transmit - queue a task for transmission
+  * @task: pointer to rpc_task
+  *
+  * Add a task to the transmission queue.
+  */
+ void
+ xprt_request_enqueue_transmit(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *pos, *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	if (xprt_request_need_enqueue_transmit(task, req)) {
+ 		spin_lock(&xprt->queue_lock);
+ 		/*
+ 		 * Requests that carry congestion control credits are added
+ 		 * to the head of the list to avoid starvation issues.
+ 		 */
+ 		if (req->rq_cong) {
+ 			xprt_clear_congestion_window_wait(xprt);
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_cong)
+ 					continue;
+ 				/* Note: req is added _before_ pos */
+ 				list_add_tail(&req->rq_xmit, &pos->rq_xmit);
+ 				INIT_LIST_HEAD(&req->rq_xmit2);
+ 				goto out;
+ 			}
+ 		} else if (RPC_IS_SWAPPER(task)) {
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_cong || pos->rq_bytes_sent)
+ 					continue;
+ 				if (RPC_IS_SWAPPER(pos->rq_task))
+ 					continue;
+ 				/* Note: req is added _before_ pos */
+ 				list_add_tail(&req->rq_xmit, &pos->rq_xmit);
+ 				INIT_LIST_HEAD(&req->rq_xmit2);
+ 				goto out;
+ 			}
+ 		} else {
+ 			list_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {
+ 				if (pos->rq_task->tk_owner != task->tk_owner)
+ 					continue;
+ 				list_add_tail(&req->rq_xmit2, &pos->rq_xmit2);
+ 				INIT_LIST_HEAD(&req->rq_xmit);
+ 				goto out;
+ 			}
+ 		}
+ 		list_add_tail(&req->rq_xmit, &xprt->xmit_queue);
+ 		INIT_LIST_HEAD(&req->rq_xmit2);
+ out:
+ 		set_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);
+ 		spin_unlock(&xprt->queue_lock);
+ 	}
+ }
+ 
+ /**
+  * xprt_request_dequeue_transmit_locked - remove a task from the transmission queue
+  * @task: pointer to rpc_task
+  *
+  * Remove a task from the transmission queue
+  * Caller must hold xprt->queue_lock
+  */
+ static void
+ xprt_request_dequeue_transmit_locked(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 
+ 	if (!test_and_clear_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))
+ 		return;
+ 	if (!list_empty(&req->rq_xmit)) {
+ 		list_del(&req->rq_xmit);
+ 		if (!list_empty(&req->rq_xmit2)) {
+ 			struct rpc_rqst *next = list_first_entry(&req->rq_xmit2,
+ 					struct rpc_rqst, rq_xmit2);
+ 			list_del(&req->rq_xmit2);
+ 			list_add_tail(&next->rq_xmit, &next->rq_xprt->xmit_queue);
+ 		}
+ 	} else
+ 		list_del(&req->rq_xmit2);
+ }
+ 
+ /**
+  * xprt_request_dequeue_transmit - remove a task from the transmission queue
+  * @task: pointer to rpc_task
+  *
+  * Remove a task from the transmission queue
+  */
+ static void
+ xprt_request_dequeue_transmit(struct rpc_task *task)
+ {
+ 	struct rpc_rqst *req = task->tk_rqstp;
+ 	struct rpc_xprt *xprt = req->rq_xprt;
+ 
+ 	spin_lock(&xprt->queue_lock);
+ 	xprt_request_dequeue_transmit_locked(task);
+ 	spin_unlock(&xprt->queue_lock);
+ }
+ 
+ /**
+  * xprt_request_need_retransmit - Test if a task needs retransmission
+  * @task: pointer to rpc_task
+  *
+  * Test for whether a connection breakage requires the task to retransmit
+  */
+ bool
+ xprt_request_need_retransmit(struct rpc_task *task)
+ {
+ 	return xprt_request_retransmit_after_disconnect(task);
+ }
+ 
+ /**
++>>>>>>> 86aeee0eb6c3 (SUNRPC: Enqueue swapper tagged RPCs at the head of the transmit queue)
   * xprt_prepare_transmit - reserve the transport before sending a request
   * @task: RPC task about to send a request
   *
* Unmerged path net/sunrpc/xprt.c
