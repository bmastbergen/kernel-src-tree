iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [iommu] io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page() (Jerry Snitselaar) [1729845]
Rebuild_FUZZ: 95.38%
commit-author Will Deacon <will@kernel.org>
commit 3951c41af4a65ba418e6b1b973d398552bedb84f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/3951c41a.failed

With all the pieces in place, we can finally propagate the
iommu_iotlb_gather structure from the call to unmap() down to the IOMMU
drivers' implementation of ->tlb_add_page(). Currently everybody ignores
it, but the machinery is now there to defer invalidation.

	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 3951c41af4a65ba418e6b1b973d398552bedb84f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu-v3.c
#	drivers/iommu/arm-smmu.c
#	drivers/iommu/io-pgtable-arm-v7s.c
#	drivers/iommu/io-pgtable-arm.c
#	drivers/iommu/msm_iommu.c
#	drivers/iommu/mtk_iommu.c
#	drivers/iommu/qcom_iommu.c
#	include/linux/io-pgtable.h
diff --cc drivers/iommu/arm-smmu-v3.c
index c08a34136024,d1ebc7103065..000000000000
--- a/drivers/iommu/arm-smmu-v3.c
+++ b/drivers/iommu/arm-smmu-v3.c
@@@ -1598,10 -1596,38 +1598,41 @@@ static void arm_smmu_tlb_inv_range_nosy
  	} while (size -= granule);
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops arm_smmu_gather_ops = {
++=======
+ static void arm_smmu_tlb_inv_page_nosync(struct iommu_iotlb_gather *gather,
+ 					 unsigned long iova, size_t granule,
+ 					 void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 
+ 	arm_smmu_tlb_inv_range_nosync(iova, size, granule, false, cookie);
+ 	arm_smmu_cmdq_issue_sync(smmu);
+ }
+ 
+ static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 
+ 	arm_smmu_tlb_inv_range_nosync(iova, size, granule, true, cookie);
+ 	arm_smmu_cmdq_issue_sync(smmu);
+ }
+ 
+ static const struct iommu_flush_ops arm_smmu_flush_ops = {
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  	.tlb_flush_all	= arm_smmu_tlb_inv_context,
 -	.tlb_flush_walk = arm_smmu_tlb_inv_walk,
 -	.tlb_flush_leaf = arm_smmu_tlb_inv_leaf,
 -	.tlb_add_page	= arm_smmu_tlb_inv_page_nosync,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync,
  };
  
  /* IOMMU API */
diff --cc drivers/iommu/arm-smmu.c
index 58f85e7c33c2,5598d0ff71a8..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -540,22 -554,67 +540,64 @@@ static void arm_smmu_tlb_inv_vmid_nosyn
  	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops arm_smmu_s1_tlb_ops = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_context,
++=======
+ static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, size, granule, false, cookie);
+ 	ops->tlb_sync(cookie);
+ }
+ 
+ static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
+ 				  size_t granule, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, size, granule, true, cookie);
+ 	ops->tlb_sync(cookie);
+ }
+ 
+ static void arm_smmu_tlb_add_page(struct iommu_iotlb_gather *gather,
+ 				  unsigned long iova, size_t granule,
+ 				  void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 
+ 	ops->tlb_inv_range(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct arm_smmu_flush_ops arm_smmu_s1_tlb_ops = {
+ 	.tlb = {
+ 		.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 	},
+ 	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
+ 	.tlb_sync		= arm_smmu_tlb_sync_context,
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  };
  
 -static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
 -	.tlb = {
 -		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 -		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 -		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 -		.tlb_add_page	= arm_smmu_tlb_add_page,
 -	},
 -	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
 -	.tlb_sync		= arm_smmu_tlb_sync_context,
 +static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v2 = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_context,
  };
  
 -static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v1 = {
 -	.tlb = {
 -		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 -		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 -		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 -		.tlb_add_page	= arm_smmu_tlb_add_page,
 -	},
 -	.tlb_inv_range		= arm_smmu_tlb_inv_vmid_nosync,
 -	.tlb_sync		= arm_smmu_tlb_sync_vmid,
 +static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v1 = {
 +	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +	.tlb_add_flush	= arm_smmu_tlb_inv_vmid_nosync,
 +	.tlb_sync	= arm_smmu_tlb_sync_vmid,
  };
  
  static irqreturn_t arm_smmu_context_fault(int irq, void *dev)
diff --cc drivers/iommu/io-pgtable-arm-v7s.c
index a0032d1cb4ef,18e7d212c7de..000000000000
--- a/drivers/iommu/io-pgtable-arm-v7s.c
+++ b/drivers/iommu/io-pgtable-arm-v7s.c
@@@ -592,10 -583,10 +594,14 @@@ static size_t arm_v7s_split_blk_unmap(s
  			return 0;
  
  		tablep = iopte_deref(pte, 1);
- 		return __arm_v7s_unmap(data, iova, size, 2, tablep);
+ 		return __arm_v7s_unmap(data, gather, iova, size, 2, tablep);
  	}
  
++<<<<<<< HEAD
 +	io_pgtable_tlb_add_flush(&data->iop, iova, size, size, true);
++=======
+ 	io_pgtable_tlb_add_page(&data->iop, gather, iova, size);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  	return size;
  }
  
@@@ -658,8 -650,7 +665,12 @@@ static size_t __arm_v7s_unmap(struct ar
  				 */
  				smp_wmb();
  			} else {
++<<<<<<< HEAD
 +				io_pgtable_tlb_add_flush(iop, iova, blk_size,
 +							 blk_size, true);
++=======
+ 				io_pgtable_tlb_add_page(iop, gather, iova, blk_size);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  			}
  			iova += blk_size;
  		}
@@@ -820,18 -812,13 +832,23 @@@ static void dummy_tlb_flush(unsigned lo
  	WARN_ON(!(size & cfg_cookie->pgsize_bitmap));
  }
  
++<<<<<<< HEAD
 +static void dummy_tlb_add_flush(unsigned long iova, size_t size,
 +				size_t granule, bool leaf, void *cookie)
++=======
+ static void dummy_tlb_add_page(struct iommu_iotlb_gather *gather,
+ 			       unsigned long iova, size_t granule, void *cookie)
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  {
 -	dummy_tlb_flush(iova, granule, granule, cookie);
 +	dummy_tlb_flush(iova, size, granule, cookie);
 +}
 +
 +static void dummy_tlb_sync(void *cookie)
 +{
 +	WARN_ON(cookie != cfg_cookie);
  }
  
 -static const struct iommu_flush_ops dummy_tlb_ops = {
 +static const struct iommu_gather_ops dummy_tlb_ops = {
  	.tlb_flush_all	= dummy_tlb_flush_all,
  	.tlb_flush_walk	= dummy_tlb_flush,
  	.tlb_flush_leaf	= dummy_tlb_flush,
diff --cc drivers/iommu/io-pgtable-arm.c
index df587d77518c,4c91359057c5..000000000000
--- a/drivers/iommu/io-pgtable-arm.c
+++ b/drivers/iommu/io-pgtable-arm.c
@@@ -596,7 -585,7 +600,11 @@@ static size_t arm_lpae_split_blk_unmap(
  
  		tablep = iopte_deref(pte, data);
  	} else if (unmap_idx >= 0) {
++<<<<<<< HEAD
 +		io_pgtable_tlb_add_flush(&data->iop, iova, size, size, true);
++=======
+ 		io_pgtable_tlb_add_page(&data->iop, gather, iova, size);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  		return size;
  	}
  
@@@ -637,7 -627,7 +646,11 @@@ static size_t __arm_lpae_unmap(struct a
  			 */
  			smp_wmb();
  		} else {
++<<<<<<< HEAD
 +			io_pgtable_tlb_add_flush(iop, iova, size, size, true);
++=======
+ 			io_pgtable_tlb_add_page(iop, gather, iova, size);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  		}
  
  		return size;
@@@ -1118,18 -1079,13 +1131,23 @@@ static void dummy_tlb_flush(unsigned lo
  	WARN_ON(!(size & cfg_cookie->pgsize_bitmap));
  }
  
++<<<<<<< HEAD
 +static void dummy_tlb_add_flush(unsigned long iova, size_t size,
 +				size_t granule, bool leaf, void *cookie)
++=======
+ static void dummy_tlb_add_page(struct iommu_iotlb_gather *gather,
+ 			       unsigned long iova, size_t granule, void *cookie)
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  {
 -	dummy_tlb_flush(iova, granule, granule, cookie);
 +	dummy_tlb_flush(iova, size, granule, cookie);
 +}
 +
 +static void dummy_tlb_sync(void *cookie)
 +{
 +	WARN_ON(cookie != cfg_cookie);
  }
  
 -static const struct iommu_flush_ops dummy_tlb_ops __initconst = {
 +static const struct iommu_gather_ops dummy_tlb_ops __initconst = {
  	.tlb_flush_all	= dummy_tlb_flush_all,
  	.tlb_flush_walk	= dummy_tlb_flush,
  	.tlb_flush_leaf	= dummy_tlb_flush,
diff --cc drivers/iommu/msm_iommu.c
index bc2cef99bb30,4c0be5b75c28..000000000000
--- a/drivers/iommu/msm_iommu.c
+++ b/drivers/iommu/msm_iommu.c
@@@ -179,20 -168,29 +179,36 @@@ fail
  	return;
  }
  
 -static void __flush_iotlb_walk(unsigned long iova, size_t size,
 -			       size_t granule, void *cookie)
 +static void __flush_iotlb_sync(void *cookie)
  {
 -	__flush_iotlb_range(iova, size, granule, false, cookie);
 +	/*
 +	 * Nothing is needed here, the barrier to guarantee
 +	 * completion of the tlb sync operation is implicitly
 +	 * taken care when the iommu client does a writel before
 +	 * kick starting the other master.
 +	 */
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops msm_iommu_gather_ops = {
++=======
+ static void __flush_iotlb_leaf(unsigned long iova, size_t size,
+ 			       size_t granule, void *cookie)
+ {
+ 	__flush_iotlb_range(iova, size, granule, true, cookie);
+ }
+ 
+ static void __flush_iotlb_page(struct iommu_iotlb_gather *gather,
+ 			       unsigned long iova, size_t granule, void *cookie)
+ {
+ 	__flush_iotlb_range(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops msm_iommu_flush_ops = {
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  	.tlb_flush_all = __flush_iotlb,
 -	.tlb_flush_walk = __flush_iotlb_walk,
 -	.tlb_flush_leaf = __flush_iotlb_leaf,
 -	.tlb_add_page = __flush_iotlb_page,
 +	.tlb_add_flush = __flush_iotlb_range,
 +	.tlb_sync = __flush_iotlb_sync,
  };
  
  static int msm_iommu_alloc_ctx(unsigned long *map, int start, int end)
diff --cc drivers/iommu/mtk_iommu.c
index f9f69f7111a9,0827d51936fa..000000000000
--- a/drivers/iommu/mtk_iommu.c
+++ b/drivers/iommu/mtk_iommu.c
@@@ -196,10 -188,32 +196,35 @@@ static void mtk_iommu_tlb_sync(void *co
  	}
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops mtk_iommu_gather_ops = {
++=======
+ static void mtk_iommu_tlb_flush_walk(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, size, granule, false, cookie);
+ 	mtk_iommu_tlb_sync(cookie);
+ }
+ 
+ static void mtk_iommu_tlb_flush_leaf(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, size, granule, true, cookie);
+ 	mtk_iommu_tlb_sync(cookie);
+ }
+ 
+ static void mtk_iommu_tlb_flush_page_nosync(struct iommu_iotlb_gather *gather,
+ 					    unsigned long iova, size_t granule,
+ 					    void *cookie)
+ {
+ 	mtk_iommu_tlb_add_flush_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops mtk_iommu_flush_ops = {
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  	.tlb_flush_all = mtk_iommu_tlb_flush_all,
 -	.tlb_flush_walk = mtk_iommu_tlb_flush_walk,
 -	.tlb_flush_leaf = mtk_iommu_tlb_flush_leaf,
 -	.tlb_add_page = mtk_iommu_tlb_flush_page_nosync,
 +	.tlb_add_flush = mtk_iommu_tlb_add_flush_nosync,
 +	.tlb_sync = mtk_iommu_tlb_sync,
  };
  
  static irqreturn_t mtk_iommu_isr(int irq, void *dev_id)
diff --cc drivers/iommu/qcom_iommu.c
index 5fa9507289ec,eac760cdbb28..000000000000
--- a/drivers/iommu/qcom_iommu.c
+++ b/drivers/iommu/qcom_iommu.c
@@@ -175,10 -164,32 +175,35 @@@ static void qcom_iommu_tlb_inv_range_no
  	}
  }
  
++<<<<<<< HEAD
 +static const struct iommu_gather_ops qcom_gather_ops = {
++=======
+ static void qcom_iommu_tlb_flush_walk(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, size, granule, false, cookie);
+ 	qcom_iommu_tlb_sync(cookie);
+ }
+ 
+ static void qcom_iommu_tlb_flush_leaf(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, size, granule, true, cookie);
+ 	qcom_iommu_tlb_sync(cookie);
+ }
+ 
+ static void qcom_iommu_tlb_add_page(struct iommu_iotlb_gather *gather,
+ 				    unsigned long iova, size_t granule,
+ 				    void *cookie)
+ {
+ 	qcom_iommu_tlb_inv_range_nosync(iova, granule, granule, true, cookie);
+ }
+ 
+ static const struct iommu_flush_ops qcom_flush_ops = {
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  	.tlb_flush_all	= qcom_iommu_tlb_inv_context,
 -	.tlb_flush_walk = qcom_iommu_tlb_flush_walk,
 -	.tlb_flush_leaf = qcom_iommu_tlb_flush_leaf,
 -	.tlb_add_page	= qcom_iommu_tlb_add_page,
 +	.tlb_add_flush	= qcom_iommu_tlb_inv_range_nosync,
 +	.tlb_sync	= qcom_iommu_tlb_sync,
  };
  
  static irqreturn_t qcom_iommu_fault(int irq, void *dev)
diff --cc include/linux/io-pgtable.h
index 7f040c22b7fb,6b1b8be3ebec..000000000000
--- a/include/linux/io-pgtable.h
+++ b/include/linux/io-pgtable.h
@@@ -25,15 -27,11 +25,23 @@@ enum io_pgtable_fmt 
   *                  address range.
   * @tlb_flush_leaf: Synchronously invalidate all leaf TLB state for a virtual
   *                  address range.
++<<<<<<< HEAD
 + * @tlb_add_flush:  Optional callback to queue up leaf TLB invalidation for a
 + *                  virtual address range.  This function exists purely as an
 + *                  optimisation for IOMMUs that cannot batch TLB invalidation
 + *                  operations efficiently and are therefore better suited to
 + *                  issuing them early rather than deferring them until
 + *                  iommu_tlb_sync().
 + * @tlb_sync:       Ensure any queued TLB invalidation has taken effect, and
 + *                  any corresponding page table updates are visible to the
 + *                  IOMMU.
++=======
+  * @tlb_add_page:   Optional callback to queue up leaf TLB invalidation for a
+  *                  single page.  IOMMUs that cannot batch TLB invalidation
+  *                  operations efficiently will typically issue them here, but
+  *                  others may decide to update the iommu_iotlb_gather structure
+  *                  and defer the invalidation until iommu_tlb_sync() instead.
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
   *
   * Note that these can all be called in atomic context and must therefore
   * not block.
@@@ -44,9 -42,8 +52,14 @@@ struct iommu_gather_ops 
  			       void *cookie);
  	void (*tlb_flush_leaf)(unsigned long iova, size_t size, size_t granule,
  			       void *cookie);
++<<<<<<< HEAD
 +	void (*tlb_add_flush)(unsigned long iova, size_t size, size_t granule,
 +			      bool leaf, void *cookie);
 +	void (*tlb_sync)(void *cookie);
++=======
+ 	void (*tlb_add_page)(struct iommu_iotlb_gather *gather,
+ 			     unsigned long iova, size_t granule, void *cookie);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  };
  
  /**
@@@ -212,15 -209,13 +225,25 @@@ io_pgtable_tlb_flush_leaf(struct io_pgt
  	iop->cfg.tlb->tlb_flush_leaf(iova, size, granule, iop->cookie);
  }
  
++<<<<<<< HEAD
 +static inline void io_pgtable_tlb_add_flush(struct io_pgtable *iop,
 +		unsigned long iova, size_t size, size_t granule, bool leaf)
 +{
 +	iop->cfg.tlb->tlb_add_flush(iova, size, granule, leaf, iop->cookie);
 +}
 +
 +static inline void io_pgtable_tlb_sync(struct io_pgtable *iop)
 +{
 +	iop->cfg.tlb->tlb_sync(iop->cookie);
++=======
+ static inline void
+ io_pgtable_tlb_add_page(struct io_pgtable *iop,
+ 			struct iommu_iotlb_gather * gather, unsigned long iova,
+ 			size_t granule)
+ {
+ 	if (iop->cfg.tlb->tlb_add_page)
+ 		iop->cfg.tlb->tlb_add_page(gather, iova, granule, iop->cookie);
++>>>>>>> 3951c41af4a6 (iommu/io-pgtable: Pass struct iommu_iotlb_gather to ->tlb_add_page())
  }
  
  /**
* Unmerged path drivers/iommu/arm-smmu-v3.c
* Unmerged path drivers/iommu/arm-smmu.c
* Unmerged path drivers/iommu/io-pgtable-arm-v7s.c
* Unmerged path drivers/iommu/io-pgtable-arm.c
* Unmerged path drivers/iommu/msm_iommu.c
* Unmerged path drivers/iommu/mtk_iommu.c
* Unmerged path drivers/iommu/qcom_iommu.c
* Unmerged path include/linux/io-pgtable.h
