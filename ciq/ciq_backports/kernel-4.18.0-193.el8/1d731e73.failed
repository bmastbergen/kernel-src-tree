x86/fpu: Add a fastpath to __fpu__restore_sig()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit 1d731e731c4cd7cbd3b1aa295f0932e7610da82f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/1d731e73.failed

The previous commits refactor the restoration of the FPU registers so
that they can be loaded from in-kernel memory. This overhead can be
avoided if the load can be performed without a pagefault.

Attempt to restore FPU registers by invoking
copy_user_to_fpregs_zeroing(). If it fails try the slowpath which can
handle pagefaults.

 [ bp: Add a comment over the fastpath to be able to find one's way
   around the function. ]

	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Dave Hansen <dave.hansen@intel.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jann Horn <jannh@google.com>
	Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
	Cc: kvm ML <kvm@vger.kernel.org>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/20190403164156.19645-25-bigeasy@linutronix.de
(cherry picked from commit 1d731e731c4cd7cbd3b1aa295f0932e7610da82f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/signal.c
diff --cc arch/x86/kernel/fpu/signal.c
index ef1568522109,a1bd7be70206..000000000000
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@@ -304,78 -306,101 +304,108 @@@ static int __fpu__restore_sig(void __us
  		}
  	}
  
 -	/*
 -	 * The current state of the FPU registers does not matter. By setting
 -	 * TIF_NEED_FPU_LOAD unconditionally it is ensured that the our xstate
 -	 * is not modified on context switch and that the xstate is considered
 -	 * to be loaded again on return to userland (overriding last_cpu avoids
 -	 * the optimisation).
 -	 */
 -	set_thread_flag(TIF_NEED_FPU_LOAD);
 -	__fpu_invalidate_fpregs_state(fpu);
 -
 -	if ((unsigned long)buf_fx % 64)
 -		fx_only = 1;
 -	/*
 -	 * For 32-bit frames with fxstate, copy the fxstate so it can be
 -	 * reconstructed later.
 -	 */
  	if (ia32_fxstate) {
++<<<<<<< HEAD
 +		/*
 +		 * For 32-bit frames with fxstate, copy the user state to the
 +		 * thread's fpu state, reconstruct fxstate from the fsave
 +		 * header. Validate and sanitize the copied state.
 +		 */
 +		struct user_i387_ia32_struct env;
 +		int err = 0;
 +
 +		/*
 +		 * Drop the current fpu which clears fpu->initialized. This ensures
 +		 * that any context-switch during the copy of the new state,
 +		 * avoids the intermediate state from getting restored/saved.
 +		 * Thus avoiding the new restored state from getting corrupted.
 +		 * We will be ready to restore/save the state only after
 +		 * fpu->initialized is again set.
 +		 */
 +		fpu__drop(fpu);
++=======
+ 		ret = __copy_from_user(&env, buf, sizeof(env));
+ 		if (ret)
+ 			goto err_out;
+ 		envp = &env;
+ 	} else {
+ 		/*
+ 		 * Attempt to restore the FPU registers directly from user
+ 		 * memory. For that to succeed, the user access cannot cause
+ 		 * page faults. If it does, fall back to the slow path below,
+ 		 * going through the kernel buffer with the enabled pagefault
+ 		 * handler.
+ 		 */
+ 		fpregs_lock();
+ 		pagefault_disable();
+ 		ret = copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only);
+ 		pagefault_enable();
+ 		if (!ret) {
+ 			fpregs_mark_activate();
+ 			fpregs_unlock();
+ 			return 0;
+ 		}
+ 		fpregs_unlock();
+ 	}
+ 
+ 
+ 	if (use_xsave() && !fx_only) {
+ 		u64 init_bv = xfeatures_mask & ~xfeatures;
++>>>>>>> 1d731e731c4c (x86/fpu: Add a fastpath to __fpu__restore_sig())
  
  		if (using_compacted_format()) {
 -			ret = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
 +			err = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
  		} else {
 -			ret = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);
 +			err = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);
  
 -			if (!ret && state_size > offsetof(struct xregs_state, header))
 -				ret = validate_xstate_header(&fpu->state.xsave.header);
 +			if (!err && state_size > offsetof(struct xregs_state, header))
 +				err = validate_xstate_header(&fpu->state.xsave.header);
  		}
 -		if (ret)
 -			goto err_out;
 -
 -		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
  
 -		fpregs_lock();
 -		if (unlikely(init_bv))
 -			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 -		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, xfeatures);
 -
 -	} else if (use_fxsr()) {
 -		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
 -		if (ret) {
 -			ret = -EFAULT;
 -			goto err_out;
 +		if (err || __copy_from_user(&env, buf, sizeof(env))) {
 +			fpstate_init(&fpu->state);
 +			trace_x86_fpu_init_state(fpu);
 +			err = -1;
 +		} else {
 +			sanitize_restored_xstate(tsk, &env, xfeatures, fx_only);
  		}
  
 -		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
 +		fpu->initialized = 1;
 +		preempt_disable();
 +		fpu__restore(fpu);
 +		preempt_enable();
  
 -		fpregs_lock();
 -		if (use_xsave()) {
 -			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
 -			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 -		}
 -
 -		ret = copy_kernel_to_fxregs_err(&fpu->state.fxsave);
 +		return err;
  	} else {
 -		ret = __copy_from_user(&fpu->state.fsave, buf_fx, state_size);
 -		if (ret)
 -			goto err_out;
 +		int ret;
  
 -		fpregs_lock();
 -		ret = copy_kernel_to_fregs_err(&fpu->state.fsave);
 +		/*
 +		 * For 64-bit frames and 32-bit fsave frames, restore the user
 +		 * state to the registers directly (with exceptions handled).
 +		 */
 +		if (use_xsave()) {
 +			if ((unsigned long)buf_fx % 64 || fx_only) {
 +				u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
 +				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 +				ret = copy_user_to_fxregs(buf_fx);
 +			} else {
 +				u64 init_bv = xfeatures_mask & ~xfeatures;
 +				if (unlikely(init_bv))
 +					copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 +				ret = copy_user_to_xregs(buf_fx, xfeatures);
 +			}
 +		} else if (use_fxsr()) {
 +			ret = copy_user_to_fxregs(buf_fx);
 +		} else
 +			ret = copy_user_to_fregs(buf_fx);
 +
 +		if (ret) {
 +			fpu__clear(fpu);
 +			return -1;
 +		}
  	}
 -	if (!ret)
 -		fpregs_mark_activate();
 -	fpregs_unlock();
  
 -err_out:
 -	if (ret)
 -		fpu__clear(fpu);
 -	return ret;
 +	return 0;
  }
  
  static inline int xstate_sigframe_size(void)
* Unmerged path arch/x86/kernel/fpu/signal.c
