locking/lockdep: Add module_param to enable consistency checks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 72dcd505e8585857397207f28782c8ba55e553b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/72dcd505.failed

And move the whole lot under CONFIG_DEBUG_LOCKDEP.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 72dcd505e8585857397207f28782c8ba55e553b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index aed5c03ce876,fda370ca529c..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -750,8 -789,193 +750,198 @@@ static bool assign_lock_key(struct lock
  	return true;
  }
  
++<<<<<<< HEAD
 +/*
 + * Initialize the lock_classes[] array elements.
++=======
+ #ifdef CONFIG_DEBUG_LOCKDEP
+ 
+ /* Check whether element @e occurs in list @h */
+ static bool in_list(struct list_head *e, struct list_head *h)
+ {
+ 	struct list_head *f;
+ 
+ 	list_for_each(f, h) {
+ 		if (e == f)
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Check whether entry @e occurs in any of the locks_after or locks_before
+  * lists.
+  */
+ static bool in_any_class_list(struct list_head *e)
+ {
+ 	struct lock_class *class;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (in_list(e, &class->locks_after) ||
+ 		    in_list(e, &class->locks_before))
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool class_lock_list_valid(struct lock_class *c, struct list_head *h)
+ {
+ 	struct lock_list *e;
+ 
+ 	list_for_each_entry(e, h, entry) {
+ 		if (e->links_to != c) {
+ 			printk(KERN_INFO "class %s: mismatch for lock entry %ld; class %s <> %s",
+ 			       c->name ? : "(?)",
+ 			       (unsigned long)(e - list_entries),
+ 			       e->links_to && e->links_to->name ?
+ 			       e->links_to->name : "(?)",
+ 			       e->class && e->class->name ? e->class->name :
+ 			       "(?)");
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ static u16 chain_hlocks[];
+ 
+ static bool check_lock_chain_key(struct lock_chain *chain)
+ {
+ #ifdef CONFIG_PROVE_LOCKING
+ 	u64 chain_key = 0;
+ 	int i;
+ 
+ 	for (i = chain->base; i < chain->base + chain->depth; i++)
+ 		chain_key = iterate_chain_key(chain_key, chain_hlocks[i] + 1);
+ 	/*
+ 	 * The 'unsigned long long' casts avoid that a compiler warning
+ 	 * is reported when building tools/lib/lockdep.
+ 	 */
+ 	if (chain->chain_key != chain_key) {
+ 		printk(KERN_INFO "chain %lld: key %#llx <> %#llx\n",
+ 		       (unsigned long long)(chain - lock_chains),
+ 		       (unsigned long long)chain->chain_key,
+ 		       (unsigned long long)chain_key);
+ 		return false;
+ 	}
+ #endif
+ 	return true;
+ }
+ 
+ static bool in_any_zapped_class_list(struct lock_class *class)
+ {
+ 	struct pending_free *pf;
+ 	int i;
+ 
+ 	for (i = 0, pf = delayed_free.pf; i < ARRAY_SIZE(delayed_free.pf); i++, pf++) {
+ 		if (in_list(&class->lock_entry, &pf->zapped))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static bool __check_data_structures(void)
+ {
+ 	struct lock_class *class;
+ 	struct lock_chain *chain;
+ 	struct hlist_head *head;
+ 	struct lock_list *e;
+ 	int i;
+ 
+ 	/* Check whether all classes occur in a lock list. */
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (!in_list(&class->lock_entry, &all_lock_classes) &&
+ 		    !in_list(&class->lock_entry, &free_lock_classes) &&
+ 		    !in_any_zapped_class_list(class)) {
+ 			printk(KERN_INFO "class %px/%s is not in any class list\n",
+ 			       class, class->name ? : "(?)");
+ 			return false;
+ 		}
+ 	}
+ 
+ 	/* Check whether all classes have valid lock lists. */
+ 	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+ 		class = &lock_classes[i];
+ 		if (!class_lock_list_valid(class, &class->locks_before))
+ 			return false;
+ 		if (!class_lock_list_valid(class, &class->locks_after))
+ 			return false;
+ 	}
+ 
+ 	/* Check the chain_key of all lock chains. */
+ 	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
+ 		head = chainhash_table + i;
+ 		hlist_for_each_entry_rcu(chain, head, entry) {
+ 			if (!check_lock_chain_key(chain))
+ 				return false;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Check whether all list entries that are in use occur in a class
+ 	 * lock list.
+ 	 */
+ 	for_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
+ 		e = list_entries + i;
+ 		if (!in_any_class_list(&e->entry)) {
+ 			printk(KERN_INFO "list entry %d is not in any class list; class %s <> %s\n",
+ 			       (unsigned int)(e - list_entries),
+ 			       e->class->name ? : "(?)",
+ 			       e->links_to->name ? : "(?)");
+ 			return false;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Check whether all list entries that are not in use do not occur in
+ 	 * a class lock list.
+ 	 */
+ 	for_each_clear_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
+ 		e = list_entries + i;
+ 		if (in_any_class_list(&e->entry)) {
+ 			printk(KERN_INFO "list entry %d occurs in a class list; class %s <> %s\n",
+ 			       (unsigned int)(e - list_entries),
+ 			       e->class && e->class->name ? e->class->name :
+ 			       "(?)",
+ 			       e->links_to && e->links_to->name ?
+ 			       e->links_to->name : "(?)");
+ 			return false;
+ 		}
+ 	}
+ 
+ 	return true;
+ }
+ 
+ int check_consistency = 0;
+ module_param(check_consistency, int, 0644);
+ 
+ static void check_data_structures(void)
+ {
+ 	static bool once = false;
+ 
+ 	if (check_consistency && !once) {
+ 		if (!__check_data_structures()) {
+ 			once = true;
+ 			WARN_ON(once);
+ 		}
+ 	}
+ }
+ 
+ #else /* CONFIG_DEBUG_LOCKDEP */
+ 
+ static inline void check_data_structures(void) { }
+ 
+ #endif /* CONFIG_DEBUG_LOCKDEP */
+ 
+ /*
+  * Initialize the lock_classes[] array elements, the free_lock_classes list
+  * and also the delayed_free structure.
++>>>>>>> 72dcd505e858 (locking/lockdep: Add module_param to enable consistency checks)
   */
  static void init_data_structures_once(void)
  {
@@@ -4170,6 -4480,82 +4360,85 @@@ void lockdep_reset(void
  	raw_local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
++=======
+ /* Remove a class from a lock chain. Must be called with the graph lock held. */
+ static void remove_class_from_lock_chain(struct pending_free *pf,
+ 					 struct lock_chain *chain,
+ 					 struct lock_class *class)
+ {
+ #ifdef CONFIG_PROVE_LOCKING
+ 	struct lock_chain *new_chain;
+ 	u64 chain_key;
+ 	int i;
+ 
+ 	for (i = chain->base; i < chain->base + chain->depth; i++) {
+ 		if (chain_hlocks[i] != class - lock_classes)
+ 			continue;
+ 		/* The code below leaks one chain_hlock[] entry. */
+ 		if (--chain->depth > 0) {
+ 			memmove(&chain_hlocks[i], &chain_hlocks[i + 1],
+ 				(chain->base + chain->depth - i) *
+ 				sizeof(chain_hlocks[0]));
+ 		}
+ 		/*
+ 		 * Each lock class occurs at most once in a lock chain so once
+ 		 * we found a match we can break out of this loop.
+ 		 */
+ 		goto recalc;
+ 	}
+ 	/* Since the chain has not been modified, return. */
+ 	return;
+ 
+ recalc:
+ 	chain_key = 0;
+ 	for (i = chain->base; i < chain->base + chain->depth; i++)
+ 		chain_key = iterate_chain_key(chain_key, chain_hlocks[i] + 1);
+ 	if (chain->depth && chain->chain_key == chain_key)
+ 		return;
+ 	/* Overwrite the chain key for concurrent RCU readers. */
+ 	WRITE_ONCE(chain->chain_key, chain_key);
+ 	/*
+ 	 * Note: calling hlist_del_rcu() from inside a
+ 	 * hlist_for_each_entry_rcu() loop is safe.
+ 	 */
+ 	hlist_del_rcu(&chain->entry);
+ 	__set_bit(chain - lock_chains, pf->lock_chains_being_freed);
+ 	if (chain->depth == 0)
+ 		return;
+ 	/*
+ 	 * If the modified lock chain matches an existing lock chain, drop
+ 	 * the modified lock chain.
+ 	 */
+ 	if (lookup_chain_cache(chain_key))
+ 		return;
+ 	new_chain = alloc_lock_chain();
+ 	if (WARN_ON_ONCE(!new_chain)) {
+ 		debug_locks_off();
+ 		return;
+ 	}
+ 	*new_chain = *chain;
+ 	hlist_add_head_rcu(&new_chain->entry, chainhashentry(chain_key));
+ #endif
+ }
+ 
+ /* Must be called with the graph lock held. */
+ static void remove_class_from_lock_chains(struct pending_free *pf,
+ 					  struct lock_class *class)
+ {
+ 	struct lock_chain *chain;
+ 	struct hlist_head *head;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
+ 		head = chainhash_table + i;
+ 		hlist_for_each_entry_rcu(chain, head, entry) {
+ 			remove_class_from_lock_chain(pf, chain, class);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 72dcd505e858 (locking/lockdep: Add module_param to enable consistency checks)
  /*
   * Remove all references to a lock class. The caller must hold the graph lock.
   */
@@@ -4205,7 -4612,95 +4474,99 @@@ static inline int within(const void *ad
  	return addr >= start && addr < start + size;
  }
  
++<<<<<<< HEAD
 +static void __lockdep_free_key_range(void *start, unsigned long size)
++=======
+ static bool inside_selftest(void)
+ {
+ 	return current == lockdep_selftest_task_struct;
+ }
+ 
+ /* The caller must hold the graph lock. */
+ static struct pending_free *get_pending_free(void)
+ {
+ 	return delayed_free.pf + delayed_free.index;
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *cb);
+ 
+ /*
+  * Schedule an RCU callback if no RCU callback is pending. Must be called with
+  * the graph lock held.
+  */
+ static void call_rcu_zapped(struct pending_free *pf)
+ {
+ 	WARN_ON_ONCE(inside_selftest());
+ 
+ 	if (list_empty(&pf->zapped))
+ 		return;
+ 
+ 	if (delayed_free.scheduled)
+ 		return;
+ 
+ 	delayed_free.scheduled = true;
+ 
+ 	WARN_ON_ONCE(delayed_free.pf + delayed_free.index != pf);
+ 	delayed_free.index ^= 1;
+ 
+ 	call_rcu(&delayed_free.rcu_head, free_zapped_rcu);
+ }
+ 
+ /* The caller must hold the graph lock. May be called from RCU context. */
+ static void __free_zapped_classes(struct pending_free *pf)
+ {
+ 	struct lock_class *class;
+ 
+ 	check_data_structures();
+ 
+ 	list_for_each_entry(class, &pf->zapped, lock_entry)
+ 		reinit_class(class);
+ 
+ 	list_splice_init(&pf->zapped, &free_lock_classes);
+ 
+ #ifdef CONFIG_PROVE_LOCKING
+ 	bitmap_andnot(lock_chains_in_use, lock_chains_in_use,
+ 		      pf->lock_chains_being_freed, ARRAY_SIZE(lock_chains));
+ 	bitmap_clear(pf->lock_chains_being_freed, 0, ARRAY_SIZE(lock_chains));
+ #endif
+ }
+ 
+ static void free_zapped_rcu(struct rcu_head *ch)
+ {
+ 	struct pending_free *pf;
+ 	unsigned long flags;
+ 
+ 	if (WARN_ON_ONCE(ch != &delayed_free.rcu_head))
+ 		return;
+ 
+ 	raw_local_irq_save(flags);
+ 	if (!graph_lock())
+ 		goto out_irq;
+ 
+ 	/* closed head */
+ 	pf = delayed_free.pf + (delayed_free.index ^ 1);
+ 	__free_zapped_classes(pf);
+ 	delayed_free.scheduled = false;
+ 
+ 	/*
+ 	 * If there's anything on the open list, close and start a new callback.
+ 	 */
+ 	call_rcu_zapped(delayed_free.pf + delayed_free.index);
+ 
+ 	graph_unlock();
+ out_irq:
+ 	raw_local_irq_restore(flags);
+ }
+ 
+ /*
+  * Remove all lock classes from the class hash table and from the
+  * all_lock_classes list whose key or name is in the address range [start,
+  * start + size). Move these lock classes to the zapped_classes list. Must
+  * be called with the graph lock held.
+  */
+ static void __lockdep_free_key_range(struct pending_free *pf, void *start,
+ 				     unsigned long size)
++>>>>>>> 72dcd505e858 (locking/lockdep: Add module_param to enable consistency checks)
  {
  	struct lock_class *class;
  	struct hlist_head *head;
* Unmerged path kernel/locking/lockdep.c
