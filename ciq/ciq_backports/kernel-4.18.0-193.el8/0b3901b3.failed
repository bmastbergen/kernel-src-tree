mm: migration: factor out code to compute expected number of page references

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
Rebuild_CHGLOG: - [mm] migration: factor out code to compute expected number of page references (Don Dutile) [1754737]
Rebuild_FUZZ: 97.30%
commit-author Jan Kara <jack@suse.cz>
commit 0b3901b38d9d916f634e903ce7cd2a8ddd5b1559
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/0b3901b3.failed

Patch series "mm: migrate: Fix page migration stalls for blkdev pages".

This patchset deals with page migration stalls that were reported by our
customer due to a block device page that had a bufferhead that was in the
bh LRU cache.

The patchset modifies the page migration code so that bufferheads are
completely handled inside buffer_migrate_page() and then provides a new
migration helper for pages with buffer heads that is safe to use even for
block device pages and that also deals with bh lrus.

This patch (of 6):

Factor out function to compute number of expected page references in
migrate_page_move_mapping().  Note that we move hpage_nr_pages() and
page_has_private() checks from under xas_lock_irq() however this is safe
since we hold page lock.

[jack@suse.cz: fix expected_page_refs()]
  Link: http://lkml.kernel.org/r/20181217131710.GB8611@quack2.suse.cz
Link: http://lkml.kernel.org/r/20181211172143.7358-2-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Cc: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0b3901b38d9d916f634e903ce7cd2a8ddd5b1559)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/migrate.c
diff --cc mm/migrate.c
index d35a94faeed0,94c9ebf1f33e..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -435,17 -453,10 +451,21 @@@ int migrate_page_move_mapping(struct ad
  		struct buffer_head *head, enum migrate_mode mode,
  		int extra_count)
  {
 -	XA_STATE(xas, &mapping->i_pages, page_index(page));
  	struct zone *oldzone, *newzone;
  	int dirty;
++<<<<<<< HEAD
 +	int expected_count = 1 + extra_count;
 +	void **pslot;
 +
 +	/*
 +	 * Device public or private pages have an extra refcount as they are
 +	 * ZONE_DEVICE pages.
 +	 */
 +	expected_count += is_device_private_page(page);
 +	expected_count += is_device_public_page(page);
++=======
+ 	int expected_count = expected_page_refs(page) + extra_count;
++>>>>>>> 0b3901b38d9d (mm: migration: factor out code to compute expected number of page references)
  
  	if (!mapping) {
  		/* Anonymous page without mapping */
@@@ -464,16 -475,9 +484,22 @@@
  	oldzone = page_zone(page);
  	newzone = page_zone(newpage);
  
++<<<<<<< HEAD
 +	xa_lock_irq(&mapping->i_pages);
 +
 +	pslot = radix_tree_lookup_slot(&mapping->i_pages,
 + 					page_index(page));
 +
 +	expected_count += hpage_nr_pages(page) + page_has_private(page);
 +	if (page_count(page) != expected_count ||
 +		radix_tree_deref_slot_protected(pslot,
 +					&mapping->i_pages.xa_lock) != page) {
 +		xa_unlock_irq(&mapping->i_pages);
++=======
+ 	xas_lock_irq(&xas);
+ 	if (page_count(page) != expected_count || xas_load(&xas) != page) {
+ 		xas_unlock_irq(&xas);
++>>>>>>> 0b3901b38d9d (mm: migration: factor out code to compute expected number of page references)
  		return -EAGAIN;
  	}
  
* Unmerged path mm/migrate.c
