xfs: cache unlinked pointers in an rhashtable

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Darrick J. Wong <darrick.wong@oracle.com>
commit 9b2471797942a5947664818cfe2c6de93b43f37a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/9b247179.failed

Use a rhashtable to cache the unlinked list incore.  This should speed
up unlinked processing considerably when there are a lot of inodes on
the unlinked list because iunlink_remove no longer has to traverse an
entire bucket list to find which inode points to the one being removed.

The incore list structure records "X.next_unlinked = Y" relations, with
the rhashtable using Y to index the records.  This makes finding the
inode X that points to a inode Y very quick.  If our cache fails to find
anything we can always fall back on the old method.

FWIW this drastically reduces the amount of time it takes to remove
inodes from the unlinked list.  I wrote a program to open a lot of
O_TMPFILE files and then close them in the same order, which takes
a very long time if we have to traverse the unlinked lists.  With the
ptach, I see:

+ /d/t/tmpfile/tmpfile
Opened 193531 files in 6.33s.
Closed 193531 files in 5.86s

real    0m12.192s
user    0m0.064s
sys     0m11.619s
+ cd /
+ umount /mnt

real    0m0.050s
user    0m0.004s
sys     0m0.030s

And without the patch:

+ /d/t/tmpfile/tmpfile
Opened 193588 files in 6.35s.
Closed 193588 files in 751.61s

real    12m38.853s
user    0m0.084s
sys     12m34.470s
+ cd /
+ umount /mnt

real    0m0.086s
user    0m0.000s
sys     0m0.060s

	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
(cherry picked from commit 9b2471797942a5947664818cfe2c6de93b43f37a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_inode.c
#	fs/xfs/xfs_trace.h
diff --cc fs/xfs/xfs_inode.c
index e66f940da2c6,9aaa3143a277..000000000000
--- a/fs/xfs/xfs_inode.c
+++ b/fs/xfs/xfs_inode.c
@@@ -1907,8 -1907,335 +1907,340 @@@ xfs_inactive
  }
  
  /*
++<<<<<<< HEAD
 + * This is called when the inode's link count has gone to 0 or we are creating
 + * a tmpfile via O_TMPFILE.  The inode @ip must have nlink == 0.
++=======
+  * In-Core Unlinked List Lookups
+  * =============================
+  *
+  * Every inode is supposed to be reachable from some other piece of metadata
+  * with the exception of the root directory.  Inodes with a connection to a
+  * file descriptor but not linked from anywhere in the on-disk directory tree
+  * are collectively known as unlinked inodes, though the filesystem itself
+  * maintains links to these inodes so that on-disk metadata are consistent.
+  *
+  * XFS implements a per-AG on-disk hash table of unlinked inodes.  The AGI
+  * header contains a number of buckets that point to an inode, and each inode
+  * record has a pointer to the next inode in the hash chain.  This
+  * singly-linked list causes scaling problems in the iunlink remove function
+  * because we must walk that list to find the inode that points to the inode
+  * being removed from the unlinked hash bucket list.
+  *
+  * What if we modelled the unlinked list as a collection of records capturing
+  * "X.next_unlinked = Y" relations?  If we indexed those records on Y, we'd
+  * have a fast way to look up unlinked list predecessors, which avoids the
+  * slow list walk.  That's exactly what we do here (in-core) with a per-AG
+  * rhashtable.
+  *
+  * Because this is a backref cache, we ignore operational failures since the
+  * iunlink code can fall back to the slow bucket walk.  The only errors that
+  * should bubble out are for obviously incorrect situations.
+  *
+  * All users of the backref cache MUST hold the AGI buffer lock to serialize
+  * access or have otherwise provided for concurrency control.
+  */
+ 
+ /* Capture a "X.next_unlinked = Y" relationship. */
+ struct xfs_iunlink {
+ 	struct rhash_head	iu_rhash_head;
+ 	xfs_agino_t		iu_agino;		/* X */
+ 	xfs_agino_t		iu_next_unlinked;	/* Y */
+ };
+ 
+ /* Unlinked list predecessor lookup hashtable construction */
+ static int
+ xfs_iunlink_obj_cmpfn(
+ 	struct rhashtable_compare_arg	*arg,
+ 	const void			*obj)
+ {
+ 	const xfs_agino_t		*key = arg->key;
+ 	const struct xfs_iunlink	*iu = obj;
+ 
+ 	if (iu->iu_next_unlinked != *key)
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static const struct rhashtable_params xfs_iunlink_hash_params = {
+ 	.min_size		= XFS_AGI_UNLINKED_BUCKETS,
+ 	.key_len		= sizeof(xfs_agino_t),
+ 	.key_offset		= offsetof(struct xfs_iunlink,
+ 					   iu_next_unlinked),
+ 	.head_offset		= offsetof(struct xfs_iunlink, iu_rhash_head),
+ 	.automatic_shrinking	= true,
+ 	.obj_cmpfn		= xfs_iunlink_obj_cmpfn,
+ };
+ 
+ /*
+  * Return X, where X.next_unlinked == @agino.  Returns NULLAGINO if no such
+  * relation is found.
+  */
+ static xfs_agino_t
+ xfs_iunlink_lookup_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		agino)
+ {
+ 	struct xfs_iunlink	*iu;
+ 
+ 	iu = rhashtable_lookup_fast(&pag->pagi_unlinked_hash, &agino,
+ 			xfs_iunlink_hash_params);
+ 	return iu ? iu->iu_agino : NULLAGINO;
+ }
+ 
+ /*
+  * Take ownership of an iunlink cache entry and insert it into the hash table.
+  * If successful, the entry will be owned by the cache; if not, it is freed.
+  * Either way, the caller does not own @iu after this call.
+  */
+ static int
+ xfs_iunlink_insert_backref(
+ 	struct xfs_perag	*pag,
+ 	struct xfs_iunlink	*iu)
+ {
+ 	int			error;
+ 
+ 	error = rhashtable_insert_fast(&pag->pagi_unlinked_hash,
+ 			&iu->iu_rhash_head, xfs_iunlink_hash_params);
+ 	/*
+ 	 * Fail loudly if there already was an entry because that's a sign of
+ 	 * corruption of in-memory data.  Also fail loudly if we see an error
+ 	 * code we didn't anticipate from the rhashtable code.  Currently we
+ 	 * only anticipate ENOMEM.
+ 	 */
+ 	if (error) {
+ 		WARN(error != -ENOMEM, "iunlink cache insert error %d", error);
+ 		kmem_free(iu);
+ 	}
+ 	/*
+ 	 * Absorb any runtime errors that aren't a result of corruption because
+ 	 * this is a cache and we can always fall back to bucket list scanning.
+ 	 */
+ 	if (error != 0 && error != -EEXIST)
+ 		error = 0;
+ 	return error;
+ }
+ 
+ /* Remember that @prev_agino.next_unlinked = @this_agino. */
+ static int
+ xfs_iunlink_add_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		prev_agino,
+ 	xfs_agino_t		this_agino)
+ {
+ 	struct xfs_iunlink	*iu;
+ 
+ 	if (XFS_TEST_ERROR(false, pag->pag_mount, XFS_ERRTAG_IUNLINK_FALLBACK))
+ 		return 0;
+ 
+ 	iu = kmem_zalloc(sizeof(*iu), KM_SLEEP | KM_NOFS);
+ 	iu->iu_agino = prev_agino;
+ 	iu->iu_next_unlinked = this_agino;
+ 
+ 	return xfs_iunlink_insert_backref(pag, iu);
+ }
+ 
+ /*
+  * Replace X.next_unlinked = @agino with X.next_unlinked = @next_unlinked.
+  * If @next_unlinked is NULLAGINO, we drop the backref and exit.  If there
+  * wasn't any such entry then we don't bother.
+  */
+ static int
+ xfs_iunlink_change_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		agino,
+ 	xfs_agino_t		next_unlinked)
+ {
+ 	struct xfs_iunlink	*iu;
+ 	int			error;
+ 
+ 	/* Look up the old entry; if there wasn't one then exit. */
+ 	iu = rhashtable_lookup_fast(&pag->pagi_unlinked_hash, &agino,
+ 			xfs_iunlink_hash_params);
+ 	if (!iu)
+ 		return 0;
+ 
+ 	/*
+ 	 * Remove the entry.  This shouldn't ever return an error, but if we
+ 	 * couldn't remove the old entry we don't want to add it again to the
+ 	 * hash table, and if the entry disappeared on us then someone's
+ 	 * violated the locking rules and we need to fail loudly.  Either way
+ 	 * we cannot remove the inode because internal state is or would have
+ 	 * been corrupt.
+ 	 */
+ 	error = rhashtable_remove_fast(&pag->pagi_unlinked_hash,
+ 			&iu->iu_rhash_head, xfs_iunlink_hash_params);
+ 	if (error)
+ 		return error;
+ 
+ 	/* If there is no new next entry just free our item and return. */
+ 	if (next_unlinked == NULLAGINO) {
+ 		kmem_free(iu);
+ 		return 0;
+ 	}
+ 
+ 	/* Update the entry and re-add it to the hash table. */
+ 	iu->iu_next_unlinked = next_unlinked;
+ 	return xfs_iunlink_insert_backref(pag, iu);
+ }
+ 
+ /* Set up the in-core predecessor structures. */
+ int
+ xfs_iunlink_init(
+ 	struct xfs_perag	*pag)
+ {
+ 	return rhashtable_init(&pag->pagi_unlinked_hash,
+ 			&xfs_iunlink_hash_params);
+ }
+ 
+ /* Free the in-core predecessor structures. */
+ static void
+ xfs_iunlink_free_item(
+ 	void			*ptr,
+ 	void			*arg)
+ {
+ 	struct xfs_iunlink	*iu = ptr;
+ 	bool			*freed_anything = arg;
+ 
+ 	*freed_anything = true;
+ 	kmem_free(iu);
+ }
+ 
+ void
+ xfs_iunlink_destroy(
+ 	struct xfs_perag	*pag)
+ {
+ 	bool			freed_anything = false;
+ 
+ 	rhashtable_free_and_destroy(&pag->pagi_unlinked_hash,
+ 			xfs_iunlink_free_item, &freed_anything);
+ 
+ 	ASSERT(freed_anything == false || XFS_FORCED_SHUTDOWN(pag->pag_mount));
+ }
+ 
+ /*
+  * Point the AGI unlinked bucket at an inode and log the results.  The caller
+  * is responsible for validating the old value.
+  */
+ STATIC int
+ xfs_iunlink_update_bucket(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	struct xfs_buf		*agibp,
+ 	unsigned int		bucket_index,
+ 	xfs_agino_t		new_agino)
+ {
+ 	struct xfs_agi		*agi = XFS_BUF_TO_AGI(agibp);
+ 	xfs_agino_t		old_value;
+ 	int			offset;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(tp->t_mountp, agno, new_agino));
+ 
+ 	old_value = be32_to_cpu(agi->agi_unlinked[bucket_index]);
+ 	trace_xfs_iunlink_update_bucket(tp->t_mountp, agno, bucket_index,
+ 			old_value, new_agino);
+ 
+ 	/*
+ 	 * We should never find the head of the list already set to the value
+ 	 * passed in because either we're adding or removing ourselves from the
+ 	 * head of the list.
+ 	 */
+ 	if (old_value == new_agino)
+ 		return -EFSCORRUPTED;
+ 
+ 	agi->agi_unlinked[bucket_index] = cpu_to_be32(new_agino);
+ 	offset = offsetof(struct xfs_agi, agi_unlinked) +
+ 			(sizeof(xfs_agino_t) * bucket_index);
+ 	xfs_trans_log_buf(tp, agibp, offset, offset + sizeof(xfs_agino_t) - 1);
+ 	return 0;
+ }
+ 
+ /* Set an on-disk inode's next_unlinked pointer. */
+ STATIC void
+ xfs_iunlink_update_dinode(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		agino,
+ 	struct xfs_buf		*ibp,
+ 	struct xfs_dinode	*dip,
+ 	struct xfs_imap		*imap,
+ 	xfs_agino_t		next_agino)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	int			offset;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(mp, agno, next_agino));
+ 
+ 	trace_xfs_iunlink_update_dinode(mp, agno, agino,
+ 			be32_to_cpu(dip->di_next_unlinked), next_agino);
+ 
+ 	dip->di_next_unlinked = cpu_to_be32(next_agino);
+ 	offset = imap->im_boffset +
+ 			offsetof(struct xfs_dinode, di_next_unlinked);
+ 
+ 	/* need to recalc the inode CRC if appropriate */
+ 	xfs_dinode_calc_crc(mp, dip);
+ 	xfs_trans_inode_buf(tp, ibp);
+ 	xfs_trans_log_buf(tp, ibp, offset, offset + sizeof(xfs_agino_t) - 1);
+ 	xfs_inobp_check(mp, ibp);
+ }
+ 
+ /* Set an in-core inode's unlinked pointer and return the old value. */
+ STATIC int
+ xfs_iunlink_update_inode(
+ 	struct xfs_trans	*tp,
+ 	struct xfs_inode	*ip,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		next_agino,
+ 	xfs_agino_t		*old_next_agino)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	struct xfs_dinode	*dip;
+ 	struct xfs_buf		*ibp;
+ 	xfs_agino_t		old_value;
+ 	int			error;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(mp, agno, next_agino));
+ 
+ 	error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &ibp, 0, 0);
+ 	if (error)
+ 		return error;
+ 
+ 	/* Make sure the old pointer isn't garbage. */
+ 	old_value = be32_to_cpu(dip->di_next_unlinked);
+ 	if (!xfs_verify_agino_or_null(mp, agno, old_value)) {
+ 		error = -EFSCORRUPTED;
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Since we're updating a linked list, we should never find that the
+ 	 * current pointer is the same as the new value, unless we're
+ 	 * terminating the list.
+ 	 */
+ 	*old_next_agino = old_value;
+ 	if (old_value == next_agino) {
+ 		if (next_agino != NULLAGINO)
+ 			error = -EFSCORRUPTED;
+ 		goto out;
+ 	}
+ 
+ 	/* Ok, update the new pointer. */
+ 	xfs_iunlink_update_dinode(tp, agno, XFS_INO_TO_AGINO(mp, ip->i_ino),
+ 			ibp, dip, &ip->i_imap, next_agino);
+ 	return 0;
+ out:
+ 	xfs_trans_brelse(tp, ibp);
+ 	return error;
+ }
+ 
+ /*
+  * This is called when the inode's link count goes to 0 or we are creating a
+  * tmpfile via O_TMPFILE. In the case of a tmpfile, @ignore_linkcount will be
+  * set to true as the link count is dropped to zero by the VFS after we've
+  * created the file successfully, so we have to add it to the unlinked list
+  * while the link count is non-zero.
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
   *
   * We place the on-disk inode on a list in the AGI.  It will be pulled from this
   * list when the inode is freed.
@@@ -1950,14 -2274,126 +2282,136 @@@ xfs_iunlink
  		return -EFSCORRUPTED;
  
  	if (next_agino != NULLAGINO) {
++<<<<<<< HEAD
++=======
+ 		struct xfs_perag	*pag;
+ 		xfs_agino_t		old_agino;
+ 
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  		/*
 -		 * There is already another inode in the bucket, so point this
 -		 * inode to the current head of the list.
 +		 * There is already another inode in the bucket we need
 +		 * to add ourselves to.  Add us at the front of the list.
 +		 * Here we put the head pointer into our next pointer,
 +		 * and then we fall through to point the head at us.
  		 */
++<<<<<<< HEAD
 +		error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &ibp,
 +				       0, 0);
++=======
+ 		error = xfs_iunlink_update_inode(tp, ip, agno, next_agino,
+ 				&old_agino);
+ 		if (error)
+ 			return error;
+ 		ASSERT(old_agino == NULLAGINO);
+ 
+ 		/*
+ 		 * agino has been unlinked, add a backref from the next inode
+ 		 * back to agino.
+ 		 */
+ 		pag = xfs_perag_get(mp, agno);
+ 		error = xfs_iunlink_add_backref(pag, agino, next_agino);
+ 		xfs_perag_put(pag);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	/* Point the head of the list to point to this inode. */
+ 	return xfs_iunlink_update_bucket(tp, agno, agibp, bucket_index, agino);
+ }
+ 
+ /* Return the imap, dinode pointer, and buffer for an inode. */
+ STATIC int
+ xfs_iunlink_map_ino(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		agino,
+ 	struct xfs_imap		*imap,
+ 	struct xfs_dinode	**dipp,
+ 	struct xfs_buf		**bpp)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	int			error;
+ 
+ 	imap->im_blkno = 0;
+ 	error = xfs_imap(mp, tp, XFS_AGINO_TO_INO(mp, agno, agino), imap, 0);
+ 	if (error) {
+ 		xfs_warn(mp, "%s: xfs_imap returned error %d.",
+ 				__func__, error);
+ 		return error;
+ 	}
+ 
+ 	error = xfs_imap_to_bp(mp, tp, imap, dipp, bpp, 0, 0);
+ 	if (error) {
+ 		xfs_warn(mp, "%s: xfs_imap_to_bp returned error %d.",
+ 				__func__, error);
+ 		return error;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Walk the unlinked chain from @head_agino until we find the inode that
+  * points to @target_agino.  Return the inode number, map, dinode pointer,
+  * and inode cluster buffer of that inode as @agino, @imap, @dipp, and @bpp.
+  *
+  * @tp, @pag, @head_agino, and @target_agino are input parameters.
+  * @agino, @imap, @dipp, and @bpp are all output parameters.
+  *
+  * Do not call this function if @target_agino is the head of the list.
+  */
+ STATIC int
+ xfs_iunlink_map_prev(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		head_agino,
+ 	xfs_agino_t		target_agino,
+ 	xfs_agino_t		*agino,
+ 	struct xfs_imap		*imap,
+ 	struct xfs_dinode	**dipp,
+ 	struct xfs_buf		**bpp,
+ 	struct xfs_perag	*pag)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	xfs_agino_t		next_agino;
+ 	int			error;
+ 
+ 	ASSERT(head_agino != target_agino);
+ 	*bpp = NULL;
+ 
+ 	/* See if our backref cache can find it faster. */
+ 	*agino = xfs_iunlink_lookup_backref(pag, target_agino);
+ 	if (*agino != NULLAGINO) {
+ 		error = xfs_iunlink_map_ino(tp, agno, *agino, imap, dipp, bpp);
+ 		if (error)
+ 			return error;
+ 
+ 		if (be32_to_cpu((*dipp)->di_next_unlinked) == target_agino)
+ 			return 0;
+ 
+ 		/*
+ 		 * If we get here the cache contents were corrupt, so drop the
+ 		 * buffer and fall back to walking the bucket list.
+ 		 */
+ 		xfs_trans_brelse(tp, *bpp);
+ 		*bpp = NULL;
+ 		WARN_ON_ONCE(1);
+ 	}
+ 
+ 	trace_xfs_iunlink_map_prev_fallback(mp, agno);
+ 
+ 	/* Otherwise, walk the entire bucket until we find it. */
+ 	next_agino = head_agino;
+ 	while (next_agino != target_agino) {
+ 		xfs_agino_t	unlinked_agino;
+ 
+ 		if (*bpp)
+ 			xfs_trans_brelse(tp, *bpp);
+ 
+ 		*agino = next_agino;
+ 		error = xfs_iunlink_map_ino(tp, agno, next_agino, imap, dipp,
+ 				bpp);
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  		if (error)
  			return error;
  
@@@ -1997,12 -2426,10 +2451,16 @@@ xfs_iunlink_remove
  {
  	struct xfs_mount	*mp = tp->t_mountp;
  	struct xfs_agi		*agi;
 +	struct xfs_dinode	*dip;
  	struct xfs_buf		*agibp;
 +	struct xfs_buf		*ibp;
  	struct xfs_buf		*last_ibp;
  	struct xfs_dinode	*last_dip = NULL;
++<<<<<<< HEAD
 +	xfs_ino_t		next_ino;
++=======
+ 	struct xfs_perag	*pag = NULL;
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  	xfs_agnumber_t		agno = XFS_INO_TO_AGNO(mp, ip->i_ino);
  	xfs_agino_t		agino = XFS_INO_TO_AGINO(mp, ip->i_ino);
  	xfs_agino_t		next_agino;
@@@ -2028,134 -2456,71 +2486,199 @@@
  		return -EFSCORRUPTED;
  	}
  
++<<<<<<< HEAD
 +	if (next_agino == agino) {
 +		/*
 +		 * We're at the head of the list.  Get the inode's on-disk
 +		 * buffer to see if there is anyone after us on the list.
 +		 * Only modify our next pointer if it is not already NULLAGINO.
 +		 * This saves us the overhead of dealing with the buffer when
 +		 * there is no need to change it.
 +		 */
 +		error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &ibp,
 +				       0, 0);
 +		if (error) {
 +			xfs_warn(mp, "%s: xfs_imap_to_bp returned error %d.",
 +				__func__, error);
 +			return error;
 +		}
 +		next_agino = be32_to_cpu(dip->di_next_unlinked);
 +		ASSERT(next_agino != 0);
 +		if (next_agino != NULLAGINO) {
 +			dip->di_next_unlinked = cpu_to_be32(NULLAGINO);
 +			offset = ip->i_imap.im_boffset +
 +				offsetof(xfs_dinode_t, di_next_unlinked);
 +
 +			/* need to recalc the inode CRC if appropriate */
 +			xfs_dinode_calc_crc(mp, dip);
 +
 +			xfs_trans_inode_buf(tp, ibp);
 +			xfs_trans_log_buf(tp, ibp, offset,
 +					  (offset + sizeof(xfs_agino_t) - 1));
 +			xfs_inobp_check(mp, ibp);
 +		} else {
 +			xfs_trans_brelse(tp, ibp);
 +		}
 +		/*
 +		 * Point the bucket head pointer at the next inode.
 +		 */
 +		ASSERT(next_agino != 0);
 +		ASSERT(next_agino != agino);
 +		agi->agi_unlinked[bucket_index] = cpu_to_be32(next_agino);
 +		offset = offsetof(xfs_agi_t, agi_unlinked) +
 +			(sizeof(xfs_agino_t) * bucket_index);
 +		xfs_trans_log_buf(tp, agibp, offset,
 +				  (offset + sizeof(xfs_agino_t) - 1));
++=======
+ 	/*
+ 	 * Set our inode's next_unlinked pointer to NULL and then return
+ 	 * the old pointer value so that we can update whatever was previous
+ 	 * to us in the list to point to whatever was next in the list.
+ 	 */
+ 	error = xfs_iunlink_update_inode(tp, ip, agno, NULLAGINO, &next_agino);
+ 	if (error)
+ 		return error;
+ 
+ 	/*
+ 	 * If there was a backref pointing from the next inode back to this
+ 	 * one, remove it because we've removed this inode from the list.
+ 	 *
+ 	 * Later, if this inode was in the middle of the list we'll update
+ 	 * this inode's backref to point from the next inode.
+ 	 */
+ 	if (next_agino != NULLAGINO) {
+ 		pag = xfs_perag_get(mp, agno);
+ 		error = xfs_iunlink_change_backref(pag, next_agino,
+ 				NULLAGINO);
+ 		if (error)
+ 			goto out;
+ 	}
+ 
+ 	if (head_agino == agino) {
+ 		/* Point the head of the list to the next unlinked inode. */
+ 		error = xfs_iunlink_update_bucket(tp, agno, agibp, bucket_index,
+ 				next_agino);
+ 		if (error)
+ 			goto out;
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  	} else {
 -		struct xfs_imap	imap;
 -		xfs_agino_t	prev_agino;
 +		/*
 +		 * We need to search the list for the inode being freed.
 +		 */
 +		last_ibp = NULL;
 +		while (next_agino != agino) {
 +			struct xfs_imap	imap;
 +
++<<<<<<< HEAD
 +			if (last_ibp)
 +				xfs_trans_brelse(tp, last_ibp);
 +
 +			imap.im_blkno = 0;
 +			next_ino = XFS_AGINO_TO_INO(mp, agno, next_agino);
 +
 +			error = xfs_imap(mp, tp, next_ino, &imap, 0);
 +			if (error) {
 +				xfs_warn(mp,
 +	"%s: xfs_imap returned error %d.",
 +					 __func__, error);
 +				return error;
 +			}
 +
 +			error = xfs_imap_to_bp(mp, tp, &imap, &last_dip,
 +					       &last_ibp, 0, 0);
 +			if (error) {
 +				xfs_warn(mp,
 +	"%s: xfs_imap_to_bp returned error %d.",
 +					__func__, error);
 +				return error;
 +			}
  
 +			last_offset = imap.im_boffset;
 +			next_agino = be32_to_cpu(last_dip->di_next_unlinked);
 +			if (!xfs_verify_agino(mp, agno, next_agino)) {
 +				XFS_CORRUPTION_ERROR(__func__,
 +						XFS_ERRLEVEL_LOW, mp,
 +						last_dip, sizeof(*last_dip));
 +				return -EFSCORRUPTED;
 +			}
 +		}
 +
 +		/*
 +		 * Now last_ibp points to the buffer previous to us on the
 +		 * unlinked list.  Pull us from the list.
 +		 */
 +		error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &ibp,
 +				       0, 0);
 +		if (error) {
 +			xfs_warn(mp, "%s: xfs_imap_to_bp(2) returned error %d.",
 +				__func__, error);
 +			return error;
 +		}
 +		next_agino = be32_to_cpu(dip->di_next_unlinked);
 +		ASSERT(next_agino != 0);
 +		ASSERT(next_agino != agino);
 +		if (next_agino != NULLAGINO) {
 +			dip->di_next_unlinked = cpu_to_be32(NULLAGINO);
 +			offset = ip->i_imap.im_boffset +
 +				offsetof(xfs_dinode_t, di_next_unlinked);
 +
 +			/* need to recalc the inode CRC if appropriate */
 +			xfs_dinode_calc_crc(mp, dip);
 +
 +			xfs_trans_inode_buf(tp, ibp);
 +			xfs_trans_log_buf(tp, ibp, offset,
 +					  (offset + sizeof(xfs_agino_t) - 1));
 +			xfs_inobp_check(mp, ibp);
 +		} else {
 +			xfs_trans_brelse(tp, ibp);
 +		}
 +		/*
 +		 * Point the previous inode on the list to the next inode.
 +		 */
 +		last_dip->di_next_unlinked = cpu_to_be32(next_agino);
 +		ASSERT(next_agino != 0);
 +		offset = last_offset + offsetof(xfs_dinode_t, di_next_unlinked);
 +
 +		/* need to recalc the inode CRC if appropriate */
 +		xfs_dinode_calc_crc(mp, last_dip);
 +
 +		xfs_trans_inode_buf(tp, last_ibp);
 +		xfs_trans_log_buf(tp, last_ibp, offset,
 +				  (offset + sizeof(xfs_agino_t) - 1));
 +		xfs_inobp_check(mp, last_ibp);
++=======
+ 		if (!pag)
+ 			pag = xfs_perag_get(mp, agno);
+ 
+ 		/* We need to search the list for the inode being freed. */
+ 		error = xfs_iunlink_map_prev(tp, agno, head_agino, agino,
+ 				&prev_agino, &imap, &last_dip, &last_ibp,
+ 				pag);
+ 		if (error)
+ 			goto out;
+ 
+ 		/* Point the previous inode on the list to the next inode. */
+ 		xfs_iunlink_update_dinode(tp, agno, prev_agino, last_ibp,
+ 				last_dip, &imap, next_agino);
+ 
+ 		/*
+ 		 * Now we deal with the backref for this inode.  If this inode
+ 		 * pointed at a real inode, change the backref that pointed to
+ 		 * us to point to our old next.  If this inode was the end of
+ 		 * the list, delete the backref that pointed to us.  Note that
+ 		 * change_backref takes care of deleting the backref if
+ 		 * next_agino is NULLAGINO.
+ 		 */
+ 		error = xfs_iunlink_change_backref(pag, agino, next_agino);
+ 		if (error)
+ 			goto out;
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  	}
- 	return 0;
+ 
+ out:
+ 	if (pag)
+ 		xfs_perag_put(pag);
+ 	return error;
  }
  
  /*
diff --cc fs/xfs/xfs_trace.h
index 6fcc893dfc91,c83ce022a355..000000000000
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@@ -3371,6 -3371,84 +3371,87 @@@ DEFINE_TRANS_EVENT(xfs_trans_roll)
  DEFINE_TRANS_EVENT(xfs_trans_add_item);
  DEFINE_TRANS_EVENT(xfs_trans_free_items);
  
++<<<<<<< HEAD
++=======
+ TRACE_EVENT(xfs_iunlink_update_bucket,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, unsigned int bucket,
+ 		 xfs_agino_t old_ptr, xfs_agino_t new_ptr),
+ 	TP_ARGS(mp, agno, bucket, old_ptr, new_ptr),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(unsigned int, bucket)
+ 		__field(xfs_agino_t, old_ptr)
+ 		__field(xfs_agino_t, new_ptr)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->bucket = bucket;
+ 		__entry->old_ptr = old_ptr;
+ 		__entry->new_ptr = new_ptr;
+ 	),
+ 	TP_printk("dev %d:%d agno %u bucket %u old 0x%x new 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno,
+ 		  __entry->bucket,
+ 		  __entry->old_ptr,
+ 		  __entry->new_ptr)
+ );
+ 
+ TRACE_EVENT(xfs_iunlink_update_dinode,
+ 	TP_PROTO(struct xfs_mount *mp, xfs_agnumber_t agno, xfs_agino_t agino,
+ 		 xfs_agino_t old_ptr, xfs_agino_t new_ptr),
+ 	TP_ARGS(mp, agno, agino, old_ptr, new_ptr),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, agino)
+ 		__field(xfs_agino_t, old_ptr)
+ 		__field(xfs_agino_t, new_ptr)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = mp->m_super->s_dev;
+ 		__entry->agno = agno;
+ 		__entry->agino = agino;
+ 		__entry->old_ptr = old_ptr;
+ 		__entry->new_ptr = new_ptr;
+ 	),
+ 	TP_printk("dev %d:%d agno %u agino 0x%x old 0x%x new 0x%x",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno,
+ 		  __entry->agino,
+ 		  __entry->old_ptr,
+ 		  __entry->new_ptr)
+ );
+ 
+ DECLARE_EVENT_CLASS(xfs_ag_inode_class,
+ 	TP_PROTO(struct xfs_inode *ip),
+ 	TP_ARGS(ip),
+ 	TP_STRUCT__entry(
+ 		__field(dev_t, dev)
+ 		__field(xfs_agnumber_t, agno)
+ 		__field(xfs_agino_t, agino)
+ 	),
+ 	TP_fast_assign(
+ 		__entry->dev = VFS_I(ip)->i_sb->s_dev;
+ 		__entry->agno = XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino);
+ 		__entry->agino = XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino);
+ 	),
+ 	TP_printk("dev %d:%d agno %u agino %u",
+ 		  MAJOR(__entry->dev), MINOR(__entry->dev),
+ 		  __entry->agno, __entry->agino)
+ )
+ 
+ #define DEFINE_AGINODE_EVENT(name) \
+ DEFINE_EVENT(xfs_ag_inode_class, name, \
+ 	TP_PROTO(struct xfs_inode *ip), \
+ 	TP_ARGS(ip))
+ DEFINE_AGINODE_EVENT(xfs_iunlink);
+ DEFINE_AGINODE_EVENT(xfs_iunlink_remove);
+ DEFINE_AG_EVENT(xfs_iunlink_map_prev_fallback);
+ 
++>>>>>>> 9b2471797942 (xfs: cache unlinked pointers in an rhashtable)
  #endif /* _TRACE_XFS_H */
  
  #undef TRACE_INCLUDE_PATH
diff --git a/fs/xfs/libxfs/xfs_errortag.h b/fs/xfs/libxfs/xfs_errortag.h
index 66077a105cbb..79e6c4fb1d8a 100644
--- a/fs/xfs/libxfs/xfs_errortag.h
+++ b/fs/xfs/libxfs/xfs_errortag.h
@@ -54,7 +54,8 @@
 #define XFS_ERRTAG_BUF_LRU_REF				31
 #define XFS_ERRTAG_FORCE_SCRUB_REPAIR			32
 #define XFS_ERRTAG_FORCE_SUMMARY_RECALC			33
-#define XFS_ERRTAG_MAX					34
+#define XFS_ERRTAG_IUNLINK_FALLBACK			34
+#define XFS_ERRTAG_MAX					35
 
 /*
  * Random factors for above tags, 1 means always, 2 means 1/2 time, etc.
@@ -93,5 +94,6 @@
 #define XFS_RANDOM_BUF_LRU_REF				2
 #define XFS_RANDOM_FORCE_SCRUB_REPAIR			1
 #define XFS_RANDOM_FORCE_SUMMARY_RECALC			1
+#define XFS_RANDOM_IUNLINK_FALLBACK			(XFS_RANDOM_DEFAULT/10)
 
 #endif /* __XFS_ERRORTAG_H_ */
diff --git a/fs/xfs/xfs_error.c b/fs/xfs/xfs_error.c
index 57a85410a8c6..a1e177f66404 100644
--- a/fs/xfs/xfs_error.c
+++ b/fs/xfs/xfs_error.c
@@ -51,6 +51,7 @@ static unsigned int xfs_errortag_random_default[] = {
 	XFS_RANDOM_BUF_LRU_REF,
 	XFS_RANDOM_FORCE_SCRUB_REPAIR,
 	XFS_RANDOM_FORCE_SUMMARY_RECALC,
+	XFS_RANDOM_IUNLINK_FALLBACK,
 };
 
 struct xfs_errortag_attr {
@@ -159,6 +160,7 @@ XFS_ERRORTAG_ATTR_RW(log_item_pin,	XFS_ERRTAG_LOG_ITEM_PIN);
 XFS_ERRORTAG_ATTR_RW(buf_lru_ref,	XFS_ERRTAG_BUF_LRU_REF);
 XFS_ERRORTAG_ATTR_RW(force_repair,	XFS_ERRTAG_FORCE_SCRUB_REPAIR);
 XFS_ERRORTAG_ATTR_RW(bad_summary,	XFS_ERRTAG_FORCE_SUMMARY_RECALC);
+XFS_ERRORTAG_ATTR_RW(iunlink_fallback,	XFS_ERRTAG_IUNLINK_FALLBACK);
 
 static struct attribute *xfs_errortag_attrs[] = {
 	XFS_ERRORTAG_ATTR_LIST(noerror),
@@ -195,6 +197,7 @@ static struct attribute *xfs_errortag_attrs[] = {
 	XFS_ERRORTAG_ATTR_LIST(buf_lru_ref),
 	XFS_ERRORTAG_ATTR_LIST(force_repair),
 	XFS_ERRORTAG_ATTR_LIST(bad_summary),
+	XFS_ERRORTAG_ATTR_LIST(iunlink_fallback),
 	NULL,
 };
 
* Unmerged path fs/xfs/xfs_inode.c
diff --git a/fs/xfs/xfs_inode.h b/fs/xfs/xfs_inode.h
index be2014520155..e62074a5257c 100644
--- a/fs/xfs/xfs_inode.h
+++ b/fs/xfs/xfs_inode.h
@@ -500,4 +500,7 @@ extern struct kmem_zone	*xfs_inode_zone;
 
 bool xfs_inode_verify_forks(struct xfs_inode *ip);
 
+int xfs_iunlink_init(struct xfs_perag *pag);
+void xfs_iunlink_destroy(struct xfs_perag *pag);
+
 #endif	/* __XFS_INODE_H__ */
diff --git a/fs/xfs/xfs_mount.c b/fs/xfs/xfs_mount.c
index b4d8c318be3c..fd63b0b1307c 100644
--- a/fs/xfs/xfs_mount.c
+++ b/fs/xfs/xfs_mount.c
@@ -149,6 +149,7 @@ xfs_free_perag(
 		spin_unlock(&mp->m_perag_lock);
 		ASSERT(pag);
 		ASSERT(atomic_read(&pag->pag_ref) == 0);
+		xfs_iunlink_destroy(pag);
 		xfs_buf_hash_destroy(pag);
 		mutex_destroy(&pag->pag_ici_reclaim_lock);
 		call_rcu(&pag->rcu_head, __xfs_free_perag);
@@ -227,6 +228,9 @@ xfs_initialize_perag(
 		/* first new pag is fully initialized */
 		if (first_initialised == NULLAGNUMBER)
 			first_initialised = index;
+		error = xfs_iunlink_init(pag);
+		if (error)
+			goto out_hash_destroy;
 	}
 
 	index = xfs_set_inode_alloc(mp, agcount);
@@ -249,6 +253,7 @@ xfs_initialize_perag(
 		if (!pag)
 			break;
 		xfs_buf_hash_destroy(pag);
+		xfs_iunlink_destroy(pag);
 		mutex_destroy(&pag->pag_ici_reclaim_lock);
 		kmem_free(pag);
 	}
diff --git a/fs/xfs/xfs_mount.h b/fs/xfs/xfs_mount.h
index b2620bad94e8..864ecf27aa75 100644
--- a/fs/xfs/xfs_mount.h
+++ b/fs/xfs/xfs_mount.h
@@ -396,6 +396,13 @@ typedef struct xfs_perag {
 
 	/* reference count */
 	uint8_t			pagf_refcount_level;
+
+	/*
+	 * Unlinked inode information.  This incore information reflects
+	 * data stored in the AGI, so callers must hold the AGI buffer lock
+	 * or have some other means to control concurrency.
+	 */
+	struct rhashtable	pagi_unlinked_hash;
 } xfs_perag_t;
 
 static inline struct xfs_ag_resv *
* Unmerged path fs/xfs/xfs_trace.h
