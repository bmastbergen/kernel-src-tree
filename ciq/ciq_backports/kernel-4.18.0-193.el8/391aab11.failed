mm/hmm: convert various hmm_pfn_* to device_entry which is a better name

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Jérôme Glisse <jglisse@redhat.com>
commit 391aab11e93f36c421abeab62526954d08ac3eed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/391aab11.failed

Convert hmm_pfn_* to device_entry_* as here we are dealing with device
driver specific entry format and hmm provide helpers to allow differents
components (including HMM) to create/parse device entry.

We keep wrapper with the old name so that we can convert driver to use the
new API in stages in each device driver tree.  This will get remove once
all driver are converted.

Link: http://lkml.kernel.org/r/20190403193318.16478-13-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Cc: Ralph Campbell <rcampbell@nvidia.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 391aab11e93f36c421abeab62526954d08ac3eed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index 2f68a486cc0d,51ec27a84668..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -153,26 -187,78 +153,85 @@@ struct hmm_range 
  };
  
  /*
++<<<<<<< HEAD
 + * hmm_pfn_to_page() - return struct page pointed to by a valid HMM pfn
 + * @range: range use to decode HMM pfn value
 + * @pfn: HMM pfn value to get corresponding struct page from
 + * Returns: struct page pointer if pfn is a valid HMM pfn, NULL otherwise
++=======
+  * hmm_range_page_shift() - return the page shift for the range
+  * @range: range being queried
+  * Returns: page shift (page size = 1 << page shift) for the range
+  */
+ static inline unsigned hmm_range_page_shift(const struct hmm_range *range)
+ {
+ 	return range->page_shift;
+ }
+ 
+ /*
+  * hmm_range_page_size() - return the page size for the range
+  * @range: range being queried
+  * Returns: page size for the range in bytes
+  */
+ static inline unsigned long hmm_range_page_size(const struct hmm_range *range)
+ {
+ 	return 1UL << hmm_range_page_shift(range);
+ }
+ 
+ /*
+  * hmm_range_wait_until_valid() - wait for range to be valid
+  * @range: range affected by invalidation to wait on
+  * @timeout: time out for wait in ms (ie abort wait after that period of time)
+  * Returns: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_wait_until_valid(struct hmm_range *range,
+ 					      unsigned long timeout)
+ {
+ 	/* Check if mm is dead ? */
+ 	if (range->hmm == NULL || range->hmm->dead || range->hmm->mm == NULL) {
+ 		range->valid = false;
+ 		return false;
+ 	}
+ 	if (range->valid)
+ 		return true;
+ 	wait_event_timeout(range->hmm->wq, range->valid || range->hmm->dead,
+ 			   msecs_to_jiffies(timeout));
+ 	/* Return current valid status just in case we get lucky */
+ 	return range->valid;
+ }
+ 
+ /*
+  * hmm_range_valid() - test if a range is valid or not
+  * @range: range
+  * Returns: true if the range is valid, false otherwise.
+  */
+ static inline bool hmm_range_valid(struct hmm_range *range)
+ {
+ 	return range->valid;
+ }
+ 
+ /*
+  * hmm_device_entry_to_page() - return struct page pointed to by a device entry
+  * @range: range use to decode device entry value
+  * @entry: device entry value to get corresponding struct page from
+  * Returns: struct page pointer if entry is a valid, NULL otherwise
++>>>>>>> 391aab11e93f (mm/hmm: convert various hmm_pfn_* to device_entry which is a better name)
   *
-  * If the HMM pfn is valid (ie valid flag set) then return the struct page
-  * matching the pfn value stored in the HMM pfn. Otherwise return NULL.
+  * If the device entry is valid (ie valid flag set) then return the struct page
+  * matching the entry value. Otherwise return NULL.
   */
- static inline struct page *hmm_pfn_to_page(const struct hmm_range *range,
- 					   uint64_t pfn)
+ static inline struct page *hmm_device_entry_to_page(const struct hmm_range *range,
+ 						    uint64_t entry)
  {
- 	if (pfn == range->values[HMM_PFN_NONE])
+ 	if (entry == range->values[HMM_PFN_NONE])
  		return NULL;
- 	if (pfn == range->values[HMM_PFN_ERROR])
+ 	if (entry == range->values[HMM_PFN_ERROR])
  		return NULL;
- 	if (pfn == range->values[HMM_PFN_SPECIAL])
+ 	if (entry == range->values[HMM_PFN_SPECIAL])
  		return NULL;
- 	if (!(pfn & range->flags[HMM_PFN_VALID]))
+ 	if (!(entry & range->flags[HMM_PFN_VALID]))
  		return NULL;
- 	return pfn_to_page(pfn >> range->pfn_shift);
+ 	return pfn_to_page(entry >> range->pfn_shift);
  }
  
  /*
diff --cc mm/hmm.c
index 4c052ccc4e21,44a238642b1d..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -479,10 -536,25 +479,26 @@@ static int hmm_vma_handle_pmd(struct mm
  		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
  
  	pfn = pmd_pfn(pmd) + pte_index(addr);
++<<<<<<< HEAD
 +	for (i = 0; addr < end; addr += PAGE_SIZE, i++, pfn++)
 +		pfns[i] = hmm_pfn_from_pfn(range, pfn) | cpu_flags;
++=======
+ 	for (i = 0; addr < end; addr += PAGE_SIZE, i++, pfn++) {
+ 		if (pmd_devmap(pmd)) {
+ 			hmm_vma_walk->pgmap = get_dev_pagemap(pfn,
+ 					      hmm_vma_walk->pgmap);
+ 			if (unlikely(!hmm_vma_walk->pgmap))
+ 				return -EBUSY;
+ 		}
+ 		pfns[i] = hmm_device_entry_from_pfn(range, pfn) | cpu_flags;
+ 	}
+ 	if (hmm_vma_walk->pgmap) {
+ 		put_dev_pagemap(hmm_vma_walk->pgmap);
+ 		hmm_vma_walk->pgmap = NULL;
+ 	}
++>>>>>>> 391aab11e93f (mm/hmm: convert various hmm_pfn_* to device_entry which is a better name)
  	hmm_vma_walk->last = end;
  	return 0;
 -#else
 -	/* If THP is not enabled then we should never reach that code ! */
 -	return -EINVAL;
 -#endif
  }
  
  static inline uint64_t pte_to_hmm_pfn_flags(struct hmm_range *range, pte_t pte)
@@@ -563,7 -640,17 +580,21 @@@ static int hmm_vma_handle_pte(struct mm
  	if (fault || write_fault)
  		goto fault;
  
++<<<<<<< HEAD
 +	*pfn = hmm_pfn_from_pfn(range, pte_pfn(pte)) | cpu_flags;
++=======
+ 	if (pte_devmap(pte)) {
+ 		hmm_vma_walk->pgmap = get_dev_pagemap(pte_pfn(pte),
+ 					      hmm_vma_walk->pgmap);
+ 		if (unlikely(!hmm_vma_walk->pgmap))
+ 			return -EBUSY;
+ 	} else if (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL) && pte_special(pte)) {
+ 		*pfn = range->values[HMM_PFN_SPECIAL];
+ 		return -EFAULT;
+ 	}
+ 
+ 	*pfn = hmm_device_entry_from_pfn(range, pte_pfn(pte)) | cpu_flags;
++>>>>>>> 391aab11e93f (mm/hmm: convert various hmm_pfn_* to device_entry which is a better name)
  	return 0;
  
  fault:
@@@ -660,6 -761,142 +691,145 @@@ again
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int hmm_vma_walk_pud(pud_t *pudp,
+ 			    unsigned long start,
+ 			    unsigned long end,
+ 			    struct mm_walk *walk)
+ {
+ 	struct hmm_vma_walk *hmm_vma_walk = walk->private;
+ 	struct hmm_range *range = hmm_vma_walk->range;
+ 	unsigned long addr = start, next;
+ 	pmd_t *pmdp;
+ 	pud_t pud;
+ 	int ret;
+ 
+ again:
+ 	pud = READ_ONCE(*pudp);
+ 	if (pud_none(pud))
+ 		return hmm_vma_walk_hole(start, end, walk);
+ 
+ 	if (pud_huge(pud) && pud_devmap(pud)) {
+ 		unsigned long i, npages, pfn;
+ 		uint64_t *pfns, cpu_flags;
+ 		bool fault, write_fault;
+ 
+ 		if (!pud_present(pud))
+ 			return hmm_vma_walk_hole(start, end, walk);
+ 
+ 		i = (addr - range->start) >> PAGE_SHIFT;
+ 		npages = (end - addr) >> PAGE_SHIFT;
+ 		pfns = &range->pfns[i];
+ 
+ 		cpu_flags = pud_to_hmm_pfn_flags(range, pud);
+ 		hmm_range_need_fault(hmm_vma_walk, pfns, npages,
+ 				     cpu_flags, &fault, &write_fault);
+ 		if (fault || write_fault)
+ 			return hmm_vma_walk_hole_(addr, end, fault,
+ 						write_fault, walk);
+ 
+ #ifdef CONFIG_HUGETLB_PAGE
+ 		pfn = pud_pfn(pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 		for (i = 0; i < npages; ++i, ++pfn) {
+ 			hmm_vma_walk->pgmap = get_dev_pagemap(pfn,
+ 					      hmm_vma_walk->pgmap);
+ 			if (unlikely(!hmm_vma_walk->pgmap))
+ 				return -EBUSY;
+ 			pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
+ 				  cpu_flags;
+ 		}
+ 		if (hmm_vma_walk->pgmap) {
+ 			put_dev_pagemap(hmm_vma_walk->pgmap);
+ 			hmm_vma_walk->pgmap = NULL;
+ 		}
+ 		hmm_vma_walk->last = end;
+ 		return 0;
+ #else
+ 		return -EINVAL;
+ #endif
+ 	}
+ 
+ 	split_huge_pud(walk->vma, pudp, addr);
+ 	if (pud_none(*pudp))
+ 		goto again;
+ 
+ 	pmdp = pmd_offset(pudp, addr);
+ 	do {
+ 		next = pmd_addr_end(addr, end);
+ 		ret = hmm_vma_walk_pmd(pmdp, addr, next, walk);
+ 		if (ret)
+ 			return ret;
+ 	} while (pmdp++, addr = next, addr != end);
+ 
+ 	return 0;
+ }
+ 
+ static int hmm_vma_walk_hugetlb_entry(pte_t *pte, unsigned long hmask,
+ 				      unsigned long start, unsigned long end,
+ 				      struct mm_walk *walk)
+ {
+ #ifdef CONFIG_HUGETLB_PAGE
+ 	unsigned long addr = start, i, pfn, mask, size, pfn_inc;
+ 	struct hmm_vma_walk *hmm_vma_walk = walk->private;
+ 	struct hmm_range *range = hmm_vma_walk->range;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	struct hstate *h = hstate_vma(vma);
+ 	uint64_t orig_pfn, cpu_flags;
+ 	bool fault, write_fault;
+ 	spinlock_t *ptl;
+ 	pte_t entry;
+ 	int ret = 0;
+ 
+ 	size = 1UL << huge_page_shift(h);
+ 	mask = size - 1;
+ 	if (range->page_shift != PAGE_SHIFT) {
+ 		/* Make sure we are looking at full page. */
+ 		if (start & mask)
+ 			return -EINVAL;
+ 		if (end < (start + size))
+ 			return -EINVAL;
+ 		pfn_inc = size >> PAGE_SHIFT;
+ 	} else {
+ 		pfn_inc = 1;
+ 		size = PAGE_SIZE;
+ 	}
+ 
+ 
+ 	ptl = huge_pte_lock(hstate_vma(walk->vma), walk->mm, pte);
+ 	entry = huge_ptep_get(pte);
+ 
+ 	i = (start - range->start) >> range->page_shift;
+ 	orig_pfn = range->pfns[i];
+ 	range->pfns[i] = range->values[HMM_PFN_NONE];
+ 	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
+ 	fault = write_fault = false;
+ 	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
+ 			   &fault, &write_fault);
+ 	if (fault || write_fault) {
+ 		ret = -ENOENT;
+ 		goto unlock;
+ 	}
+ 
+ 	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
+ 	for (; addr < end; addr += size, i++, pfn += pfn_inc)
+ 		range->pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
+ 				 cpu_flags;
+ 	hmm_vma_walk->last = end;
+ 
+ unlock:
+ 	spin_unlock(ptl);
+ 
+ 	if (ret == -ENOENT)
+ 		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
+ 
+ 	return ret;
+ #else /* CONFIG_HUGETLB_PAGE */
+ 	return -EINVAL;
+ #endif
+ }
+ 
++>>>>>>> 391aab11e93f (mm/hmm: convert various hmm_pfn_* to device_entry which is a better name)
  static void hmm_pfns_clear(struct hmm_range *range,
  			   uint64_t *pfns,
  			   unsigned long addr,
@@@ -828,120 -1098,245 +998,336 @@@ EXPORT_SYMBOL(hmm_vma_range_done)
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
 + *
 + * Expected use pattern:
 + * retry:
 + *   down_read(&mm->mmap_sem);
 + *   // Find vma and address device wants to fault, initialize hmm_pfn_t
 + *   // array accordingly
 + *   ret = hmm_vma_fault(range, write, block);
 + *   switch (ret) {
 + *   case -EAGAIN:
 + *     hmm_vma_range_done(range);
 + *     // You might want to rate limit or yield to play nicely, you may
 + *     // also commit any valid pfn in the array assuming that you are
 + *     // getting true from hmm_vma_range_monitor_end()
 + *     goto retry;
 + *   case 0:
 + *     break;
 + *   case -ENOMEM:
 + *   case -EINVAL:
 + *   case -EPERM:
 + *   default:
 + *     // Handle error !
 + *     up_read(&mm->mmap_sem)
 + *     return;
 + *   }
 + *   // Take device driver lock that serialize device page table update
 + *   driver_lock_device_page_table_update();
 + *   hmm_vma_range_done(range);
 + *   // Commit pfns we got from hmm_vma_fault()
 + *   driver_unlock_device_page_table_update();
 + *   up_read(&mm->mmap_sem)
 + *
 + * YOU MUST CALL hmm_vma_range_done() AFTER THIS FUNCTION RETURN SUCCESS (0)
 + * BEFORE FREEING THE range struct OR YOU WILL HAVE SERIOUS MEMORY CORRUPTION !
 + *
 + * YOU HAVE BEEN WARNED !
   */
 -long hmm_range_fault(struct hmm_range *range, bool block)
 +int hmm_vma_fault(struct hmm_range *range, bool block)
  {
 -	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 -	unsigned long start = range->start, end;
 +	struct vm_area_struct *vma = range->vma;
 +	unsigned long start = range->start;
  	struct hmm_vma_walk hmm_vma_walk;
 -	struct hmm *hmm = range->hmm;
 -	struct vm_area_struct *vma;
  	struct mm_walk mm_walk;
 +	struct hmm *hmm;
  	int ret;
  
 -	/* Check if hmm_mm_destroy() was call. */
 -	if (hmm->mm == NULL || hmm->dead)
 -		return -EFAULT;
 +	/* Sanity check, this really should not happen ! */
 +	if (range->start < vma->vm_start || range->start >= vma->vm_end)
 +		return -EINVAL;
 +	if (range->end < vma->vm_start || range->end > vma->vm_end)
 +		return -EINVAL;
  
 -	do {
 -		/* If range is no longer valid force retry. */
 -		if (!range->valid) {
 -			up_read(&hmm->mm->mmap_sem);
 -			return -EAGAIN;
 -		}
 +	hmm = hmm_register(vma->vm_mm);
 +	if (!hmm) {
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -ENOMEM;
 +	}
 +	/* Caller must have registered a mirror using hmm_mirror_register() */
 +	if (!hmm->mmu_notifier.ops)
 +		return -EINVAL;
  
 -		vma = find_vma(hmm->mm, start);
 -		if (vma == NULL || (vma->vm_flags & device_vma))
 -			return -EFAULT;
 +	/* FIXME support hugetlb fs */
 +	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
 +			vma_is_dax(vma)) {
 +		hmm_pfns_special(range);
 +		return -EINVAL;
 +	}
  
 +	if (!(vma->vm_flags & VM_READ)) {
 +		/*
 +		 * If vma do not allow read access, then assume that it does
 +		 * not allow write access, either. Architecture that allow
 +		 * write without read access are not supported by HMM, because
 +		 * operations such has atomic access would not work.
 +		 */
 +		hmm_pfns_clear(range, range->pfns, range->start, range->end);
 +		return -EPERM;
 +	}
 +
 +	/* Initialize range to track CPU page table update */
 +	spin_lock(&hmm->lock);
 +	range->valid = true;
 +	list_add_rcu(&range->list, &hmm->ranges);
 +	spin_unlock(&hmm->lock);
 +
 +	hmm_vma_walk.fault = true;
 +	hmm_vma_walk.block = block;
 +	hmm_vma_walk.range = range;
 +	mm_walk.private = &hmm_vma_walk;
 +	hmm_vma_walk.last = range->start;
 +
 +	mm_walk.vma = vma;
 +	mm_walk.mm = vma->vm_mm;
 +	mm_walk.pte_entry = NULL;
 +	mm_walk.test_walk = NULL;
 +	mm_walk.hugetlb_entry = NULL;
 +	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 +	mm_walk.pte_hole = hmm_vma_walk_hole;
 +
 +	do {
 +		ret = walk_page_range(start, range->end, &mm_walk);
 +		start = hmm_vma_walk.last;
 +	} while (ret == -EAGAIN);
 +
 +	if (ret) {
 +		unsigned long i;
 +
++<<<<<<< HEAD
 +		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +		hmm_pfns_clear(range, &range->pfns[i], hmm_vma_walk.last,
 +			       range->end);
 +		hmm_vma_range_done(range);
 +	}
 +	return ret;
 +}
 +EXPORT_SYMBOL(hmm_vma_fault);
++=======
+ 		if (is_vm_hugetlb_page(vma)) {
+ 			if (huge_page_shift(hstate_vma(vma)) !=
+ 			    range->page_shift &&
+ 			    range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		} else {
+ 			if (range->page_shift != PAGE_SHIFT)
+ 				return -EINVAL;
+ 		}
+ 
+ 		if (!(vma->vm_flags & VM_READ)) {
+ 			/*
+ 			 * If vma do not allow read access, then assume that it
+ 			 * does not allow write access, either. HMM does not
+ 			 * support architecture that allow write without read.
+ 			 */
+ 			hmm_pfns_clear(range, range->pfns,
+ 				range->start, range->end);
+ 			return -EPERM;
+ 		}
+ 
+ 		range->vma = vma;
+ 		hmm_vma_walk.pgmap = NULL;
+ 		hmm_vma_walk.last = start;
+ 		hmm_vma_walk.fault = true;
+ 		hmm_vma_walk.block = block;
+ 		hmm_vma_walk.range = range;
+ 		mm_walk.private = &hmm_vma_walk;
+ 		end = min(range->end, vma->vm_end);
+ 
+ 		mm_walk.vma = vma;
+ 		mm_walk.mm = vma->vm_mm;
+ 		mm_walk.pte_entry = NULL;
+ 		mm_walk.test_walk = NULL;
+ 		mm_walk.hugetlb_entry = NULL;
+ 		mm_walk.pud_entry = hmm_vma_walk_pud;
+ 		mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 		mm_walk.pte_hole = hmm_vma_walk_hole;
+ 		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
+ 
+ 		do {
+ 			ret = walk_page_range(start, end, &mm_walk);
+ 			start = hmm_vma_walk.last;
+ 
+ 			/* Keep trying while the range is valid. */
+ 		} while (ret == -EBUSY && range->valid);
+ 
+ 		if (ret) {
+ 			unsigned long i;
+ 
+ 			i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
+ 			hmm_pfns_clear(range, &range->pfns[i],
+ 				hmm_vma_walk.last, range->end);
+ 			return ret;
+ 		}
+ 		start = end;
+ 
+ 	} while (start < range->end);
+ 
+ 	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
+ }
+ EXPORT_SYMBOL(hmm_range_fault);
+ 
+ /**
+  * hmm_range_dma_map() - hmm_range_fault() and dma map page all in one.
+  * @range: range being faulted
+  * @device: device against to dma map page to
+  * @daddrs: dma address of mapped pages
+  * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
+  * Returns: number of pages mapped on success, -EAGAIN if mmap_sem have been
+  *          drop and you need to try again, some other error value otherwise
+  *
+  * Note same usage pattern as hmm_range_fault().
+  */
+ long hmm_range_dma_map(struct hmm_range *range,
+ 		       struct device *device,
+ 		       dma_addr_t *daddrs,
+ 		       bool block)
+ {
+ 	unsigned long i, npages, mapped;
+ 	long ret;
+ 
+ 	ret = hmm_range_fault(range, block);
+ 	if (ret <= 0)
+ 		return ret ? ret : -EBUSY;
+ 
+ 	npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	for (i = 0, mapped = 0; i < npages; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		/*
+ 		 * FIXME need to update DMA API to provide invalid DMA address
+ 		 * value instead of a function to test dma address value. This
+ 		 * would remove lot of dumb code duplicated accross many arch.
+ 		 *
+ 		 * For now setting it to 0 here is good enough as the pfns[]
+ 		 * value is what is use to check what is valid and what isn't.
+ 		 */
+ 		daddrs[i] = 0;
+ 
+ 		page = hmm_device_entry_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		/* Check if range is being invalidated */
+ 		if (!range->valid) {
+ 			ret = -EBUSY;
+ 			goto unmap;
+ 		}
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE])
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 		daddrs[i] = dma_map_page(device, page, 0, PAGE_SIZE, dir);
+ 		if (dma_mapping_error(device, daddrs[i])) {
+ 			ret = -EFAULT;
+ 			goto unmap;
+ 		}
+ 
+ 		mapped++;
+ 	}
+ 
+ 	return mapped;
+ 
+ unmap:
+ 	for (npages = i, i = 0; (i < npages) && mapped; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		page = hmm_device_entry_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		if (dma_mapping_error(device, daddrs[i]))
+ 			continue;
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE])
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 		dma_unmap_page(device, daddrs[i], PAGE_SIZE, dir);
+ 		mapped--;
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL(hmm_range_dma_map);
+ 
+ /**
+  * hmm_range_dma_unmap() - unmap range of that was map with hmm_range_dma_map()
+  * @range: range being unmapped
+  * @vma: the vma against which the range (optional)
+  * @device: device against which dma map was done
+  * @daddrs: dma address of mapped pages
+  * @dirty: dirty page if it had the write flag set
+  * Returns: number of page unmapped on success, -EINVAL otherwise
+  *
+  * Note that caller MUST abide by mmu notifier or use HMM mirror and abide
+  * to the sync_cpu_device_pagetables() callback so that it is safe here to
+  * call set_page_dirty(). Caller must also take appropriate locks to avoid
+  * concurrent mmu notifier or sync_cpu_device_pagetables() to make progress.
+  */
+ long hmm_range_dma_unmap(struct hmm_range *range,
+ 			 struct vm_area_struct *vma,
+ 			 struct device *device,
+ 			 dma_addr_t *daddrs,
+ 			 bool dirty)
+ {
+ 	unsigned long i, npages;
+ 	long cpages = 0;
+ 
+ 	/* Sanity check. */
+ 	if (range->end <= range->start)
+ 		return -EINVAL;
+ 	if (!daddrs)
+ 		return -EINVAL;
+ 	if (!range->pfns)
+ 		return -EINVAL;
+ 
+ 	npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	for (i = 0; i < npages; ++i) {
+ 		enum dma_data_direction dir = DMA_TO_DEVICE;
+ 		struct page *page;
+ 
+ 		page = hmm_device_entry_to_page(range, range->pfns[i]);
+ 		if (page == NULL)
+ 			continue;
+ 
+ 		/* If it is read and write than map bi-directional. */
+ 		if (range->pfns[i] & range->flags[HMM_PFN_WRITE]) {
+ 			dir = DMA_BIDIRECTIONAL;
+ 
+ 			/*
+ 			 * See comments in function description on why it is
+ 			 * safe here to call set_page_dirty()
+ 			 */
+ 			if (dirty)
+ 				set_page_dirty(page);
+ 		}
+ 
+ 		/* Unmap and clear pfns/dma address */
+ 		dma_unmap_page(device, daddrs[i], PAGE_SIZE, dir);
+ 		range->pfns[i] = range->values[HMM_PFN_NONE];
+ 		/* FIXME see comments in hmm_vma_dma_map() */
+ 		daddrs[i] = 0;
+ 		cpages++;
+ 	}
+ 
+ 	return cpages;
+ }
+ EXPORT_SYMBOL(hmm_range_dma_unmap);
++>>>>>>> 391aab11e93f (mm/hmm: convert various hmm_pfn_* to device_entry which is a better name)
  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
  
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
