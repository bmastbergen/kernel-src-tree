KVM: x86/mmu: Take slots_lock when using kvm_mmu_zap_all_fast()

jira LE-1907
cve CVE-2018-12207
Rebuild_History Non-Buildable kernel-4.18.0-193.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit ed69a6cb700880d052a0d085ff2e5bfc108ce238
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-193.el8/ed69a6cb.failed

Acquire the per-VM slots_lock when zapping all shadow pages as part of
toggling nx_huge_pages.  The fast zap algorithm relies on exclusivity
(via slots_lock) to identify obsolete vs. valid shadow pages, because it
uses a single bit for its generation number. Holding slots_lock also
obviates the need to acquire a read lock on the VM's srcu.

Failing to take slots_lock when toggling nx_huge_pages allows multiple
instances of kvm_mmu_zap_all_fast() to run concurrently, as the other
user, KVM_SET_USER_MEMORY_REGION, does not take the global kvm_lock.
(kvm_mmu_zap_all_fast() does take kvm->mmu_lock, but it can be
temporarily dropped by kvm_zap_obsolete_pages(), so it is not enough
to enforce exclusivity).

Concurrent fast zap instances causes obsolete shadow pages to be
incorrectly identified as valid due to the single bit generation number
wrapping, which results in stale shadow pages being left in KVM's MMU
and leads to all sorts of undesirable behavior.
The bug is easily confirmed by running with CONFIG_PROVE_LOCKING and
toggling nx_huge_pages via its module param.

Note, until commit 4ae5acbc4936 ("KVM: x86/mmu: Take slots_lock when
using kvm_mmu_zap_all_fast()", 2019-11-13) the fast zap algorithm used
an ulong-sized generation instead of relying on exclusivity for
correctness, but all callers except the recently added set_nx_huge_pages()
needed to hold slots_lock anyways.  Therefore, this patch does not have
to be backported to stable kernels.

Given that toggling nx_huge_pages is by no means a fast path, force it
to conform to the current approach instead of reintroducing the previous
generation count.

Fixes: b8e8c8303ff28 ("kvm: mmu: ITLB_MULTIHIT mitigation", but NOT FOR STABLE)
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ed69a6cb700880d052a0d085ff2e5bfc108ce238)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 0afcce3fd133,2ce9da58611e..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -6137,6 -6255,52 +6137,55 @@@ static void kvm_set_mmio_spte_mask(void
  	kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
  }
  
++<<<<<<< HEAD
++=======
+ static bool get_nx_auto_mode(void)
+ {
+ 	/* Return true when CPU has the bug, and mitigations are ON */
+ 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
+ }
+ 
+ static void __set_nx_huge_pages(bool val)
+ {
+ 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ }
+ 
+ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
+ {
+ 	bool old_val = nx_huge_pages;
+ 	bool new_val;
+ 
+ 	/* In "auto" mode deploy workaround only if CPU has the bug. */
+ 	if (sysfs_streq(val, "off"))
+ 		new_val = 0;
+ 	else if (sysfs_streq(val, "force"))
+ 		new_val = 1;
+ 	else if (sysfs_streq(val, "auto"))
+ 		new_val = get_nx_auto_mode();
+ 	else if (strtobool(val, &new_val) < 0)
+ 		return -EINVAL;
+ 
+ 	__set_nx_huge_pages(new_val);
+ 
+ 	if (new_val != old_val) {
+ 		struct kvm *kvm;
+ 
+ 		mutex_lock(&kvm_lock);
+ 
+ 		list_for_each_entry(kvm, &vm_list, vm_list) {
+ 			mutex_lock(&kvm->slots_lock);
+ 			kvm_mmu_zap_all_fast(kvm);
+ 			mutex_unlock(&kvm->slots_lock);
+ 
+ 			wake_up_process(kvm->arch.nx_lpage_recovery_thread);
+ 		}
+ 		mutex_unlock(&kvm_lock);
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> ed69a6cb7008 (KVM: x86/mmu: Take slots_lock when using kvm_mmu_zap_all_fast())
  int kvm_mmu_module_init(void)
  {
  	int ret = -ENOMEM;
* Unmerged path arch/x86/kvm/mmu.c
