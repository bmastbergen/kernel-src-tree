sched: Queue RT tasks to head when prio drops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.24.1.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 81a44c5441d7f7d2c3dc9105f4d65ad0d5818617
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.24.1.el7/81a44c54.failed

The following scenario does not work correctly:

Runqueue of CPUx contains two runnable and pinned tasks:

 T1: SCHED_FIFO, prio 80
 T2: SCHED_FIFO, prio 80

T1 is on the cpu and executes the following syscalls (classic priority
ceiling scenario):

 sys_sched_setscheduler(pid(T1), SCHED_FIFO, .prio = 90);
 ...
 sys_sched_setscheduler(pid(T1), SCHED_FIFO, .prio = 80);
 ...

Now T1 gets preempted by T3 (SCHED_FIFO, prio 95). After T3 goes back
to sleep the scheduler picks T2. Surprise!

The same happens w/o actual preemption when T1 is forced into the
scheduler due to a sporadic NEED_RESCHED event. The scheduler invokes
pick_next_task() which returns T2. So T1 gets preempted and scheduled
out.

This happens because sched_setscheduler() dequeues T1 from the prio 90
list and then enqueues it on the tail of the prio 80 list behind T2.
This violates the POSIX spec and surprises user space which relies on
the guarantee that SCHED_FIFO tasks are not scheduled out unless they
give the CPU up voluntarily or are preempted by a higher priority
task. In the latter case the preempted task must get back on the CPU
after the preempting task schedules out again.

We fixed a similar issue already in commit 60db48c (sched: Queue a
deboosted task to the head of the RT prio queue). The same treatment
is necessary for sched_setscheduler(). So enqueue to head of the prio
bucket list if the priority of the task is lowered.

It might be possible that existing user space relies on the current
behaviour, but it can be considered highly unlikely due to the corner
case nature of the application scenario.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1391803122-4425-6-git-send-email-bigeasy@linutronix.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 81a44c5441d7f7d2c3dc9105f4d65ad0d5818617)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 71c102c7b0a3,9c2fcbf9a266..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -4869,21 -3442,18 +4869,31 @@@ change
  
  	if (running)
  		p->sched_class->set_curr_task(rq);
++<<<<<<< HEAD
 +	if (queued)
 +		enqueue_task(rq, p, ENQUEUE_RESTORE);
++=======
+ 	if (on_rq) {
+ 		/*
+ 		 * We enqueue to tail when the priority of a task is
+ 		 * increased (user space view).
+ 		 */
+ 		enqueue_task(rq, p, oldprio <= p->prio ? ENQUEUE_HEAD : 0);
+ 	}
++>>>>>>> 81a44c5441d7 (sched: Queue RT tasks to head when prio drops)
  
  	check_class_changed(rq, p, prev_class, oldprio);
 +	preempt_disable(); /* avoid rq from going away on us */
  	task_rq_unlock(rq, p, &flags);
  
 -	rt_mutex_adjust_pi(p);
 +	if (pi)
 +		rt_mutex_adjust_pi(p);
 +
 +	/*
 +	 * Run balance callbacks after we've adjusted the PI chain.
 +	 */
 +	balance_callback(rq);
 +	preempt_enable();
  
  	return 0;
  }
* Unmerged path kernel/sched/core.c
