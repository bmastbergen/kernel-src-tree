arm64: KVM: Enable Common Not Private translations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Vladimir Murzin <vladimir.murzin@arm.com>
commit ab510027dc4dbd1eeb611a34b0cda8b21fcde492
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/ab510027.failed

We rely on cpufeature framework to detect and enable CNP so for KVM we
need to patch hyp to set CNP bit just before TTBR0_EL2 gets written.

For the guest we encode CNP bit while building vttbr, so we don't need
to bother with that in a world switch.

	Reviewed-by: James Morse <james.morse@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <marc.zyngier@arm.com>
	Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit ab510027dc4dbd1eeb611a34b0cda8b21fcde492)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/kvm_mmu.h
#	arch/arm64/include/asm/kvm_arm.h
#	arch/arm64/include/asm/kvm_mmu.h
diff --cc arch/arm/include/asm/kvm_mmu.h
index 5ad1a54f98dc,847f01fa429d..000000000000
--- a/arch/arm/include/asm/kvm_mmu.h
+++ b/arch/arm/include/asm/kvm_mmu.h
@@@ -358,7 -355,10 +358,14 @@@ static inline int hyp_map_aux_data(void
  
  #define kvm_phys_to_vttbr(addr)		(addr)
  
++<<<<<<< HEAD
 +static inline void kvm_set_ipa_limit(void) {}
++=======
+ static inline bool kvm_cpu_has_cnp(void)
+ {
+ 	return false;
+ }
++>>>>>>> ab510027dc4d (arm64: KVM: Enable Common Not Private translations)
  
  #endif	/* !__ASSEMBLY__ */
  
diff --cc arch/arm64/include/asm/kvm_arm.h
index 130697f925a3,b476bc46f0ab..000000000000
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@@ -142,127 -145,38 +142,132 @@@
  #define VTCR_EL2_COMMON_BITS	(VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
  				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_RES1)
  
 -#ifdef CONFIG_ARM64_64K_PAGES
  /*
 - * Stage2 translation configuration:
 - * 64kB pages (TG0 = 1)
 - * 2 level page tables (SL = 1)
 + * VTCR_EL2:SL0 indicates the entry level for Stage2 translation.
 + * Interestingly, it depends on the page size.
 + * See D.10.2.121, VTCR_EL2, in ARM DDI 0487C.a
 + *
 + *	-----------------------------------------
 + *	| Entry level		|  4K  | 16K/64K |
 + *	------------------------------------------
 + *	| Level: 0		|  2   |   -     |
 + *	------------------------------------------
 + *	| Level: 1		|  1   |   2     |
 + *	------------------------------------------
 + *	| Level: 2		|  0   |   1     |
 + *	------------------------------------------
 + *	| Level: 3		|  -   |   0     |
 + *	------------------------------------------
 + *
 + * The table roughly translates to :
 + *
 + *	SL0(PAGE_SIZE, Entry_level) = TGRAN_SL0_BASE - Entry_Level
 + *
 + * Where TGRAN_SL0_BASE is a magic number depending on the page size:
 + * 	TGRAN_SL0_BASE(4K) = 2
 + *	TGRAN_SL0_BASE(16K) = 3
 + *	TGRAN_SL0_BASE(64K) = 3
 + * provided we take care of ruling out the unsupported cases and
 + * Entry_Level = 4 - Number_of_levels.
 + *
   */
 -#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SL0_LVL1)
 -#define VTTBR_X_TGRAN_MAGIC		38
 +#ifdef CONFIG_ARM64_64K_PAGES
 +
 +#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_64K
 +#define VTCR_EL2_TGRAN_SL0_BASE		3UL
 +
  #elif defined(CONFIG_ARM64_16K_PAGES)
 -/*
 - * Stage2 translation configuration:
 - * 16kB pages (TG0 = 2)
 - * 2 level page tables (SL = 1)
 - */
 -#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_16K | VTCR_EL2_SL0_LVL1)
 -#define VTTBR_X_TGRAN_MAGIC		42
 +
 +#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_16K
 +#define VTCR_EL2_TGRAN_SL0_BASE		3UL
 +
  #else	/* 4K */
 -/*
 - * Stage2 translation configuration:
 - * 4kB pages (TG0 = 0)
 - * 3 level page tables (SL = 1)
 - */
 -#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SL0_LVL1)
 -#define VTTBR_X_TGRAN_MAGIC		37
 +
 +#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_4K
 +#define VTCR_EL2_TGRAN_SL0_BASE		2UL
 +
  #endif
  
 -#define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
 -#define VTTBR_X				(VTTBR_X_TGRAN_MAGIC - VTCR_EL2_T0SZ_IPA)
 +#define VTCR_EL2_LVLS_TO_SL0(levels)	\
 +	((VTCR_EL2_TGRAN_SL0_BASE - (4 - (levels))) << VTCR_EL2_SL0_SHIFT)
 +#define VTCR_EL2_SL0_TO_LVLS(sl0)	\
 +	((sl0) + 4 - VTCR_EL2_TGRAN_SL0_BASE)
 +#define VTCR_EL2_LVLS(vtcr)		\
 +	VTCR_EL2_SL0_TO_LVLS(((vtcr) & VTCR_EL2_SL0_MASK) >> VTCR_EL2_SL0_SHIFT)
 +
 +#define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN)
 +#define VTCR_EL2_IPA(vtcr)		(64 - ((vtcr) & VTCR_EL2_T0SZ_MASK))
 +
 +/*
 + * ARM VMSAv8-64 defines an algorithm for finding the translation table
 + * descriptors in section D4.2.8 in ARM DDI 0487C.a.
 + *
 + * The algorithm defines the expectations on the translation table
 + * addresses for each level, based on PAGE_SIZE, entry level
 + * and the translation table size (T0SZ). The variable "x" in the
 + * algorithm determines the alignment of a table base address at a given
 + * level and thus determines the alignment of VTTBR:BADDR for stage2
 + * page table entry level.
 + * Since the number of bits resolved at the entry level could vary
 + * depending on the T0SZ, the value of "x" is defined based on a
 + * Magic constant for a given PAGE_SIZE and Entry Level. The
 + * intermediate levels must be always aligned to the PAGE_SIZE (i.e,
 + * x = PAGE_SHIFT).
 + *
 + * The value of "x" for entry level is calculated as :
 + *    x = Magic_N - T0SZ
 + *
 + * where Magic_N is an integer depending on the page size and the entry
 + * level of the page table as below:
 + *
 + *	--------------------------------------------
 + *	| Entry level		|  4K    16K   64K |
 + *	--------------------------------------------
 + *	| Level: 0 (4 levels)	| 28   |  -  |  -  |
 + *	--------------------------------------------
 + *	| Level: 1 (3 levels)	| 37   | 31  | 25  |
 + *	--------------------------------------------
 + *	| Level: 2 (2 levels)	| 46   | 42  | 38  |
 + *	--------------------------------------------
 + *	| Level: 3 (1 level)	| -    | 53  | 51  |
 + *	--------------------------------------------
 + *
 + * We have a magic formula for the Magic_N below:
 + *
 + *  Magic_N(PAGE_SIZE, Level) = 64 - ((PAGE_SHIFT - 3) * Number_of_levels)
 + *
 + * where Number_of_levels = (4 - Level). We are only interested in the
 + * value for Entry_Level for the stage2 page table.
 + *
 + * So, given that T0SZ = (64 - IPA_SHIFT), we can compute 'x' as follows:
 + *
 + *	x = (64 - ((PAGE_SHIFT - 3) * Number_of_levels)) - (64 - IPA_SHIFT)
 + *	  = IPA_SHIFT - ((PAGE_SHIFT - 3) * Number of levels)
 + *
 + * Here is one way to explain the Magic Formula:
 + *
 + *  x = log2(Size_of_Entry_Level_Table)
 + *
 + * Since, we can resolve (PAGE_SHIFT - 3) bits at each level, and another
 + * PAGE_SHIFT bits in the PTE, we have :
 + *
 + *  Bits_Entry_level = IPA_SHIFT - ((PAGE_SHIFT - 3) * (n - 1) + PAGE_SHIFT)
 + *		     = IPA_SHIFT - (PAGE_SHIFT - 3) * n - 3
 + *  where n = number of levels, and since each pointer is 8bytes, we have:
 + *
 + *  x = Bits_Entry_Level + 3
 + *    = IPA_SHIFT - (PAGE_SHIFT - 3) * n
 + *
 + * The only constraint here is that, we have to find the number of page table
 + * levels for a given IPA size (which we do, see stage2_pt_levels())
 + */
 +#define ARM64_VTTBR_X(ipa, levels)	((ipa) - ((levels) * (PAGE_SHIFT - 3)))
  
++<<<<<<< HEAD
++=======
+ #define VTTBR_CNP_BIT     (UL(1))
+ #define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_X)
++>>>>>>> ab510027dc4d (arm64: KVM: Enable Common Not Private translations)
  #define VTTBR_VMID_SHIFT  (UL(48))
  #define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
  
diff --cc arch/arm64/include/asm/kvm_mmu.h
index 8bd2fca8ec98,64337afbf124..000000000000
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@@ -504,28 -517,9 +504,34 @@@ static inline int hyp_map_aux_data(void
  
  #define kvm_phys_to_vttbr(addr)		phys_to_ttbr(addr)
  
++<<<<<<< HEAD
 +/*
 + * Get the magic number 'x' for VTTBR:BADDR of this KVM instance.
 + * With v8.2 LVA extensions, 'x' should be a minimum of 6 with
 + * 52bit IPS.
 + */
 +static inline int arm64_vttbr_x(u32 ipa_shift, u32 levels)
 +{
 +	int x = ARM64_VTTBR_X(ipa_shift, levels);
 +
 +	return (IS_ENABLED(CONFIG_ARM64_PA_BITS_52) && x < 6) ? 6 : x;
 +}
 +
 +static inline u64 vttbr_baddr_mask(u32 ipa_shift, u32 levels)
 +{
 +	unsigned int x = arm64_vttbr_x(ipa_shift, levels);
 +
 +	return GENMASK_ULL(PHYS_MASK_SHIFT - 1, x);
 +}
 +
 +static inline u64 kvm_vttbr_baddr_mask(struct kvm *kvm)
 +{
 +	return vttbr_baddr_mask(kvm_phys_shift(kvm), kvm_stage2_levels(kvm));
++=======
+ static inline bool kvm_cpu_has_cnp(void)
+ {
+ 	return system_supports_cnp();
++>>>>>>> ab510027dc4d (arm64: KVM: Enable Common Not Private translations)
  }
  
  #endif /* __ASSEMBLY__ */
diff --git a/arch/arm/include/asm/kvm_arm.h b/arch/arm/include/asm/kvm_arm.h
index c3f1f9b304b7..b95f8d0d9f17 100644
--- a/arch/arm/include/asm/kvm_arm.h
+++ b/arch/arm/include/asm/kvm_arm.h
@@ -160,6 +160,7 @@
 #else
 #define VTTBR_X		(5 - KVM_T0SZ)
 #endif
+#define VTTBR_CNP_BIT     _AC(1, UL)
 #define VTTBR_BADDR_MASK  (((_AC(1, ULL) << (40 - VTTBR_X)) - 1) << VTTBR_X)
 #define VTTBR_VMID_SHIFT  _AC(48, ULL)
 #define VTTBR_VMID_MASK(size)	(_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
* Unmerged path arch/arm/include/asm/kvm_mmu.h
* Unmerged path arch/arm64/include/asm/kvm_arm.h
* Unmerged path arch/arm64/include/asm/kvm_mmu.h
diff --git a/arch/arm64/kvm/hyp-init.S b/arch/arm64/kvm/hyp-init.S
index ea9225160786..4576b86a5579 100644
--- a/arch/arm64/kvm/hyp-init.S
+++ b/arch/arm64/kvm/hyp-init.S
@@ -65,6 +65,9 @@ __do_hyp_init:
 	b.lo	__kvm_handle_stub_hvc
 
 	phys_to_ttbr x4, x0
+alternative_if ARM64_HAS_CNP
+	orr	x4, x4, #TTBR_CNP_BIT
+alternative_else_nop_endif
 	msr	ttbr0_el2, x4
 
 	mrs	x4, tcr_el1
diff --git a/virt/kvm/arm/arm.c b/virt/kvm/arm/arm.c
index 0cb26049fff3..83578c2a4558 100644
--- a/virt/kvm/arm/arm.c
+++ b/virt/kvm/arm/arm.c
@@ -497,7 +497,7 @@ static bool need_new_vmid_gen(struct kvm *kvm)
 static void update_vttbr(struct kvm *kvm)
 {
 	phys_addr_t pgd_phys;
-	u64 vmid;
+	u64 vmid, cnp = kvm_cpu_has_cnp() ? VTTBR_CNP_BIT : 0;
 	bool new_gen;
 
 	read_lock(&kvm_vmid_lock);
@@ -547,7 +547,7 @@ static void update_vttbr(struct kvm *kvm)
 	pgd_phys = virt_to_phys(kvm->arch.pgd);
 	BUG_ON(pgd_phys & ~kvm_vttbr_baddr_mask(kvm));
 	vmid = ((u64)(kvm->arch.vmid) << VTTBR_VMID_SHIFT) & VTTBR_VMID_MASK(kvm_vmid_bits);
-	kvm->arch.vttbr = kvm_phys_to_vttbr(pgd_phys) | vmid;
+	kvm->arch.vttbr = kvm_phys_to_vttbr(pgd_phys) | vmid | cnp;
 
 	write_unlock(&kvm_vmid_lock);
 }
