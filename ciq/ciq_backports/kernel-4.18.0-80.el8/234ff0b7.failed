KVM: PPC: Book3S HV: Fix race between kvm_unmap_hva_range and MMU mode switch

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul Mackerras <paulus@ozlabs.org>
commit 234ff0b729ad882d20f7996591a964965647addf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/234ff0b7.failed

Testing has revealed an occasional crash which appears to be caused
by a race between kvmppc_switch_mmu_to_hpt and kvm_unmap_hva_range_hv.
The symptom is a NULL pointer dereference in __find_linux_pte() called
from kvm_unmap_radix() with kvm->arch.pgtable == NULL.

Looking at kvmppc_switch_mmu_to_hpt(), it does indeed clear
kvm->arch.pgtable (via kvmppc_free_radix()) before setting
kvm->arch.radix to NULL, and there is nothing to prevent
kvm_unmap_hva_range_hv() or the other MMU callback functions from
being called concurrently with kvmppc_switch_mmu_to_hpt() or
kvmppc_switch_mmu_to_radix().

This patch therefore adds calls to spin_lock/unlock on the kvm->mmu_lock
around the assignments to kvm->arch.radix, and makes sure that the
partition-scoped radix tree or HPT is only freed after changing
kvm->arch.radix.

This also takes the kvm->mmu_lock in kvmppc_rmap_reset() to make sure
that the clearing of each rmap array (one per memslot) doesn't happen
concurrently with use of the array in the kvm_unmap_hva_range_hv()
or the other MMU callbacks.

Fixes: 18c3640cefc7 ("KVM: PPC: Book3S HV: Add infrastructure for running HPT guests on radix host")
	Cc: stable@vger.kernel.org # v4.15+
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit 234ff0b729ad882d20f7996591a964965647addf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv.c
diff --cc arch/powerpc/kvm/book3s_hv.c
index 5275029b47c2,ab43306c4ea1..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -3849,10 -4530,14 +3849,19 @@@ static int kvmppc_hv_setup_htab_rma(str
  /* Must be called with kvm->lock held and mmu_ready = 0 and no vcpus running */
  int kvmppc_switch_mmu_to_hpt(struct kvm *kvm)
  {
 -	if (nesting_enabled(kvm))
 +	if (kvm->arch.nested_enable) {
 +		kvm->arch.nested_enable = false;
  		kvmhv_release_all_nested(kvm);
++<<<<<<< HEAD
 +	}
++=======
+ 	kvmppc_rmap_reset(kvm);
+ 	kvm->arch.process_table = 0;
+ 	/* Mutual exclusion with kvm_unmap_hva_range etc. */
+ 	spin_lock(&kvm->mmu_lock);
+ 	kvm->arch.radix = 0;
+ 	spin_unlock(&kvm->mmu_lock);
++>>>>>>> 234ff0b729ad (KVM: PPC: Book3S HV: Fix race between kvm_unmap_hva_range and MMU mode switch)
  	kvmppc_free_radix(kvm);
  	kvmppc_update_lpcr(kvm, LPCR_VPM1,
  			   LPCR_VPM1 | LPCR_UPRT | LPCR_GTSE | LPCR_HR);
@@@ -3874,7 -4560,6 +3884,10 @@@ int kvmppc_switch_mmu_to_radix(struct k
  	kvmppc_free_hpt(&kvm->arch.hpt);
  	kvmppc_update_lpcr(kvm, LPCR_UPRT | LPCR_GTSE | LPCR_HR,
  			   LPCR_VPM1 | LPCR_UPRT | LPCR_GTSE | LPCR_HR);
++<<<<<<< HEAD
 +	kvm->arch.radix = 1;
++=======
++>>>>>>> 234ff0b729ad (KVM: PPC: Book3S HV: Fix race between kvm_unmap_hva_range and MMU mode switch)
  	return 0;
  }
  
diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index 4c08f42f6406..9ec04f862509 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -745,12 +745,15 @@ void kvmppc_rmap_reset(struct kvm *kvm)
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	slots = kvm_memslots(kvm);
 	kvm_for_each_memslot(memslot, slots) {
+		/* Mutual exclusion with kvm_unmap_hva_range etc. */
+		spin_lock(&kvm->mmu_lock);
 		/*
 		 * This assumes it is acceptable to lose reference and
 		 * change bits across a reset.
 		 */
 		memset(memslot->arch.rmap, 0,
 		       memslot->npages * sizeof(*memslot->arch.rmap));
+		spin_unlock(&kvm->mmu_lock);
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
 }
* Unmerged path arch/powerpc/kvm/book3s_hv.c
