rcu: Inline _rcu_barrier() into its sole remaining caller

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit dd46a7882c2c2006201e053ebf5e9ad761860cc0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/dd46a788.failed

Because rcu_barrier() is a one-line wrapper function for _rcu_barrier()
and because nothing else calls _rcu_barrier(), this commit inlines
_rcu_barrier() into rcu_barrier().

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit dd46a7882c2c2006201e053ebf5e9ad761860cc0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 19ff444b5603,ce16b8da2c6f..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -3011,33 -2996,31 +3011,51 @@@ static void rcu_barrier_trace(const cha
   */
  static void rcu_barrier_callback(struct rcu_head *rhp)
  {
++<<<<<<< HEAD
 +	struct rcu_data *rdp = container_of(rhp, struct rcu_data, barrier_head);
 +	struct rcu_state *rsp = rdp->rsp;
 +
 +	if (atomic_dec_and_test(&rsp->barrier_cpu_count)) {
 +		_rcu_barrier_trace(TPS("LastCB"), -1, rsp->barrier_sequence);
 +		complete(&rsp->barrier_completion);
 +	} else {
 +		_rcu_barrier_trace(TPS("CB"), -1, rsp->barrier_sequence);
++=======
+ 	if (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {
+ 		rcu_barrier_trace(TPS("LastCB"), -1,
+ 				   rcu_state.barrier_sequence);
+ 		complete(&rcu_state.barrier_completion);
+ 	} else {
+ 		rcu_barrier_trace(TPS("CB"), -1, rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  	}
  }
  
  /*
   * Called with preemption disabled, and from cross-cpu IRQ context.
   */
 -static void rcu_barrier_func(void *unused)
 +static void rcu_barrier_func(void *type)
  {
 +	struct rcu_state *rsp = type;
  	struct rcu_data *rdp = raw_cpu_ptr(&rcu_data);
  
++<<<<<<< HEAD
 +	_rcu_barrier_trace(TPS("IRQ"), -1, rsp->barrier_sequence);
++=======
+ 	rcu_barrier_trace(TPS("IRQ"), -1, rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  	rdp->barrier_head.func = rcu_barrier_callback;
  	debug_rcu_head_queue(&rdp->barrier_head);
  	if (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head, 0)) {
 -		atomic_inc(&rcu_state.barrier_cpu_count);
 +		atomic_inc(&rsp->barrier_cpu_count);
  	} else {
  		debug_rcu_head_unqueue(&rdp->barrier_head);
++<<<<<<< HEAD
 +		_rcu_barrier_trace(TPS("IRQNQ"), -1, rsp->barrier_sequence);
++=======
+ 		rcu_barrier_trace(TPS("IRQNQ"), -1,
+ 				   rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  	}
  }
  
@@@ -3046,25 -3036,25 +3071,36 @@@ void rcu_barrier(void
  {
  	int cpu;
  	struct rcu_data *rdp;
 -	unsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);
 +	struct rcu_state *rsp = &rcu_state;
 +	unsigned long s = rcu_seq_snap(&rsp->barrier_sequence);
  
- 	_rcu_barrier_trace(TPS("Begin"), -1, s);
+ 	rcu_barrier_trace(TPS("Begin"), -1, s);
  
  	/* Take mutex to serialize concurrent rcu_barrier() requests. */
 -	mutex_lock(&rcu_state.barrier_mutex);
 +	mutex_lock(&rsp->barrier_mutex);
  
  	/* Did someone else do our work for us? */
++<<<<<<< HEAD
 +	if (rcu_seq_done(&rsp->barrier_sequence, s)) {
 +		_rcu_barrier_trace(TPS("EarlyExit"), -1, rsp->barrier_sequence);
++=======
+ 	if (rcu_seq_done(&rcu_state.barrier_sequence, s)) {
+ 		rcu_barrier_trace(TPS("EarlyExit"), -1,
+ 				   rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  		smp_mb(); /* caller's subsequent code after above check. */
 -		mutex_unlock(&rcu_state.barrier_mutex);
 +		mutex_unlock(&rsp->barrier_mutex);
  		return;
  	}
  
  	/* Mark the start of the barrier operation. */
++<<<<<<< HEAD
 +	rcu_seq_start(&rsp->barrier_sequence);
 +	_rcu_barrier_trace(TPS("Inc1"), -1, rsp->barrier_sequence);
++=======
+ 	rcu_seq_start(&rcu_state.barrier_sequence);
+ 	rcu_barrier_trace(TPS("Inc1"), -1, rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  
  	/*
  	 * Initialize the count to one rather than to zero in order to
@@@ -3087,23 -3077,23 +3123,40 @@@
  		rdp = per_cpu_ptr(&rcu_data, cpu);
  		if (rcu_is_nocb_cpu(cpu)) {
  			if (!rcu_nocb_cpu_needs_barrier(cpu)) {
++<<<<<<< HEAD
 +				_rcu_barrier_trace(TPS("OfflineNoCB"), cpu,
 +						   rsp->barrier_sequence);
 +			} else {
 +				_rcu_barrier_trace(TPS("OnlineNoCB"), cpu,
 +						   rsp->barrier_sequence);
++=======
+ 				rcu_barrier_trace(TPS("OfflineNoCB"), cpu,
+ 						   rcu_state.barrier_sequence);
+ 			} else {
+ 				rcu_barrier_trace(TPS("OnlineNoCB"), cpu,
+ 						   rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  				smp_mb__before_atomic();
 -				atomic_inc(&rcu_state.barrier_cpu_count);
 +				atomic_inc(&rsp->barrier_cpu_count);
  				__call_rcu(&rdp->barrier_head,
  					   rcu_barrier_callback, cpu, 0);
  			}
  		} else if (rcu_segcblist_n_cbs(&rdp->cblist)) {
++<<<<<<< HEAD
 +			_rcu_barrier_trace(TPS("OnlineQ"), cpu,
 +					   rsp->barrier_sequence);
 +			smp_call_function_single(cpu, rcu_barrier_func, rsp, 1);
 +		} else {
 +			_rcu_barrier_trace(TPS("OnlineNQ"), cpu,
 +					   rsp->barrier_sequence);
++=======
+ 			rcu_barrier_trace(TPS("OnlineQ"), cpu,
+ 					   rcu_state.barrier_sequence);
+ 			smp_call_function_single(cpu, rcu_barrier_func, NULL, 1);
+ 		} else {
+ 			rcu_barrier_trace(TPS("OnlineNQ"), cpu,
+ 					   rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  		}
  	}
  	put_online_cpus();
@@@ -3112,32 -3102,19 +3165,24 @@@
  	 * Now that we have an rcu_barrier_callback() callback on each
  	 * CPU, and thus each counted, remove the initial count.
  	 */
 -	if (atomic_dec_and_test(&rcu_state.barrier_cpu_count))
 -		complete(&rcu_state.barrier_completion);
 +	if (atomic_dec_and_test(&rsp->barrier_cpu_count))
 +		complete(&rsp->barrier_completion);
  
  	/* Wait for all rcu_barrier_callback() callbacks to be invoked. */
 -	wait_for_completion(&rcu_state.barrier_completion);
 +	wait_for_completion(&rsp->barrier_completion);
  
  	/* Mark the end of the barrier operation. */
++<<<<<<< HEAD
 +	_rcu_barrier_trace(TPS("Inc2"), -1, rsp->barrier_sequence);
 +	rcu_seq_end(&rsp->barrier_sequence);
++=======
+ 	rcu_barrier_trace(TPS("Inc2"), -1, rcu_state.barrier_sequence);
+ 	rcu_seq_end(&rcu_state.barrier_sequence);
++>>>>>>> dd46a7882c2c (rcu: Inline _rcu_barrier() into its sole remaining caller)
  
  	/* Other rcu_barrier() invocations can now safely proceed. */
 -	mutex_unlock(&rcu_state.barrier_mutex);
 +	mutex_unlock(&rsp->barrier_mutex);
  }
- 
- /**
-  * rcu_barrier - Wait until all in-flight call_rcu() callbacks complete.
-  *
-  * Note that this primitive does not necessarily wait for an RCU grace period
-  * to complete.  For example, if there are no RCU callbacks queued anywhere
-  * in the system, then rcu_barrier() is within its rights to return
-  * immediately, without waiting for anything, much less an RCU grace period.
-  */
- void rcu_barrier(void)
- {
- 	_rcu_barrier();
- }
  EXPORT_SYMBOL_GPL(rcu_barrier);
  
  /*
diff --git a/include/trace/events/rcu.h b/include/trace/events/rcu.h
index a8d07feff6a0..175e0bce22bd 100644
--- a/include/trace/events/rcu.h
+++ b/include/trace/events/rcu.h
@@ -705,20 +705,20 @@ TRACE_EVENT(rcu_torture_read,
 );
 
 /*
- * Tracepoint for _rcu_barrier() execution.  The string "s" describes
- * the _rcu_barrier phase:
- *	"Begin": _rcu_barrier() started.
- *	"EarlyExit": _rcu_barrier() piggybacked, thus early exit.
- *	"Inc1": _rcu_barrier() piggyback check counter incremented.
- *	"OfflineNoCB": _rcu_barrier() found callback on never-online CPU
- *	"OnlineNoCB": _rcu_barrier() found online no-CBs CPU.
- *	"OnlineQ": _rcu_barrier() found online CPU with callbacks.
- *	"OnlineNQ": _rcu_barrier() found online CPU, no callbacks.
+ * Tracepoint for rcu_barrier() execution.  The string "s" describes
+ * the rcu_barrier phase:
+ *	"Begin": rcu_barrier() started.
+ *	"EarlyExit": rcu_barrier() piggybacked, thus early exit.
+ *	"Inc1": rcu_barrier() piggyback check counter incremented.
+ *	"OfflineNoCB": rcu_barrier() found callback on never-online CPU
+ *	"OnlineNoCB": rcu_barrier() found online no-CBs CPU.
+ *	"OnlineQ": rcu_barrier() found online CPU with callbacks.
+ *	"OnlineNQ": rcu_barrier() found online CPU, no callbacks.
  *	"IRQ": An rcu_barrier_callback() callback posted on remote CPU.
  *	"IRQNQ": An rcu_barrier_callback() callback found no callbacks.
  *	"CB": An rcu_barrier_callback() invoked a callback, not the last.
  *	"LastCB": An rcu_barrier_callback() invoked the last callback.
- *	"Inc2": _rcu_barrier() piggyback check counter incremented.
+ *	"Inc2": rcu_barrier() piggyback check counter incremented.
  * The "cpu" argument is the CPU or -1 if meaningless, the "cnt" argument
  * is the count of remaining callbacks, and "done" is the piggybacking count.
  */
* Unmerged path kernel/rcu/tree.c
diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7c6033d71e9d..e13597cbb4d3 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -223,7 +223,7 @@ struct rcu_data {
 					/* Grace period that needs help */
 					/*  from cond_resched(). */
 
-	/* 5) _rcu_barrier(), OOM callbacks, and expediting. */
+	/* 5) rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
@@ -330,7 +330,7 @@ struct rcu_state {
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
 	struct completion barrier_completion;	/* Wake at barrier end. */
 	unsigned long barrier_sequence;		/* ++ at start and end of */
-						/*  _rcu_barrier(). */
+						/*  rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 
 	struct mutex exp_mutex;			/* Serialize expedited GP. */
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 05c430d4a698..e0aef9bacb2d 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -2009,7 +2009,7 @@ static bool rcu_nocb_cpu_needs_barrier(int cpu)
 	 * There needs to be a barrier before this function is called,
 	 * but associated with a prior determination that no more
 	 * callbacks would be posted.  In the worst case, the first
-	 * barrier in _rcu_barrier() suffices (but the caller cannot
+	 * barrier in rcu_barrier() suffices (but the caller cannot
 	 * necessarily rely on this, not a substitute for the caller
 	 * getting the concurrency design right!).  There must also be
 	 * a barrier between the following load an posting of a callback
