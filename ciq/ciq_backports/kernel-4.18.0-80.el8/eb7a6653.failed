rcu: Eliminate initialization-time use of rsp

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit eb7a6653887b540a81d1b91ee0fc68b604da9386
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/eb7a6653.failed

Now that there is only one rcu_state structure, there is less point in
maintaining a pointer to it.  This commit therefore replaces rsp with
&rcu_state in rcu_cpu_starting() and rcu_init_one().

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit eb7a6653887b540a81d1b91ee0fc68b604da9386)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index a0aff61f4663,5e3a3001a50d..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -3486,34 -3455,31 +3486,59 @@@ void rcu_cpu_starting(unsigned int cpu
  	unsigned long oldmask;
  	struct rcu_data *rdp;
  	struct rcu_node *rnp;
++<<<<<<< HEAD
 +	struct rcu_state *rsp;
++=======
++>>>>>>> eb7a6653887b (rcu: Eliminate initialization-time use of rsp)
  
  	if (per_cpu(rcu_cpu_started, cpu))
  		return;
  
  	per_cpu(rcu_cpu_started, cpu) = 1;
  
++<<<<<<< HEAD
 +	for_each_rcu_flavor(rsp) {
 +		rdp = per_cpu_ptr(&rcu_data, cpu);
 +		rnp = rdp->mynode;
 +		mask = rdp->grpmask;
 +		raw_spin_lock_irqsave_rcu_node(rnp, flags);
 +		rnp->qsmaskinitnext |= mask;
 +		oldmask = rnp->expmaskinitnext;
 +		rnp->expmaskinitnext |= mask;
 +		oldmask ^= rnp->expmaskinitnext;
 +		nbits = bitmap_weight(&oldmask, BITS_PER_LONG);
 +		/* Allow lockless access for expedited grace periods. */
 +		smp_store_release(&rsp->ncpus, rsp->ncpus + nbits); /* ^^^ */
 +		rcu_gpnum_ovf(rnp, rdp); /* Offline-induced counter wrap? */
 +		rdp->rcu_onl_gp_seq = READ_ONCE(rsp->gp_seq);
 +		rdp->rcu_onl_gp_flags = READ_ONCE(rsp->gp_flags);
 +		if (rnp->qsmask & mask) { /* RCU waiting on incoming CPU? */
 +			/* Report QS -after- changing ->qsmaskinitnext! */
 +			rcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);
 +		} else {
 +			raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
 +		}
++=======
+ 	rdp = per_cpu_ptr(&rcu_data, cpu);
+ 	rnp = rdp->mynode;
+ 	mask = rdp->grpmask;
+ 	raw_spin_lock_irqsave_rcu_node(rnp, flags);
+ 	rnp->qsmaskinitnext |= mask;
+ 	oldmask = rnp->expmaskinitnext;
+ 	rnp->expmaskinitnext |= mask;
+ 	oldmask ^= rnp->expmaskinitnext;
+ 	nbits = bitmap_weight(&oldmask, BITS_PER_LONG);
+ 	/* Allow lockless access for expedited grace periods. */
+ 	smp_store_release(&rcu_state.ncpus, rcu_state.ncpus + nbits); /* ^^^ */
+ 	rcu_gpnum_ovf(rnp, rdp); /* Offline-induced counter wrap? */
+ 	rdp->rcu_onl_gp_seq = READ_ONCE(rcu_state.gp_seq);
+ 	rdp->rcu_onl_gp_flags = READ_ONCE(rcu_state.gp_flags);
+ 	if (rnp->qsmask & mask) { /* RCU waiting on incoming CPU? */
+ 		/* Report QS -after- changing ->qsmaskinitnext! */
+ 		rcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);
+ 	} else {
+ 		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
++>>>>>>> eb7a6653887b (rcu: Eliminate initialization-time use of rsp)
  	}
  	smp_mb(); /* Ensure RCU read-side usage follows above initialization. */
  }
@@@ -3759,9 -3722,9 +3784,15 @@@ static void __init rcu_init_one(void
  		}
  	}
  
++<<<<<<< HEAD
 +	init_swait_queue_head(&rsp->gp_wq);
 +	init_swait_queue_head(&rsp->expedited_wq);
 +	rnp = rcu_first_leaf_node(rsp);
++=======
+ 	init_swait_queue_head(&rcu_state.gp_wq);
+ 	init_swait_queue_head(&rcu_state.expedited_wq);
+ 	rnp = rcu_first_leaf_node();
++>>>>>>> eb7a6653887b (rcu: Eliminate initialization-time use of rsp)
  	for_each_possible_cpu(i) {
  		while (i > rnp->grphi)
  			rnp++;
* Unmerged path kernel/rcu/tree.c
