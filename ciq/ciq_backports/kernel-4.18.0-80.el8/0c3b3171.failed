dma-mapping: move the arm64 noncoherent alloc/free support to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 0c3b3171ceccb8830c2bb5adff1b4e9b204c1450
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/0c3b3171.failed

The arm64 codebase to implement coherent dma allocation for architectures
with non-coherent DMA is a good start for a generic implementation, given
that is uses the generic remap helpers, provides the atomic pool for
allocations that can't sleep and still is realtively simple and well
tested.  Move it to kernel/dma and allow architectures to opt into it
using a config symbol.  Architectures just need to provide a new
arch_dma_prep_coherent helper to writeback an invalidate the caches
for any memory that gets remapped for uncached access.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Will Deacon <will.deacon@arm.com>
	Reviewed-by: Robin Murphy <robin.murphy@arm.com>
(cherry picked from commit 0c3b3171ceccb8830c2bb5adff1b4e9b204c1450)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/Kconfig
#	arch/arm64/mm/dma-mapping.c
#	kernel/dma/Kconfig
#	kernel/dma/remap.c
diff --cc arch/arm64/Kconfig
index 5b4fd63f9948,2e645ea693ea..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -75,8 -79,10 +75,12 @@@ config ARM6
  	select CLONE_BACKWARDS
  	select COMMON_CLK
  	select CPU_PM if (SUSPEND || CPU_IDLE)
 -	select CRC32
  	select DCACHE_WORD_ACCESS
  	select DMA_DIRECT_OPS
++<<<<<<< HEAD
++=======
+ 	select DMA_DIRECT_REMAP
++>>>>>>> 0c3b3171cecc (dma-mapping: move the arm64 noncoherent alloc/free support to common code)
  	select EDAC_SUPPORT
  	select FRAME_POINTER
  	select GENERIC_ALLOCATOR
diff --cc arch/arm64/mm/dma-mapping.c
index f9ad9f019915,e2e7e5d0f94e..000000000000
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@@ -32,167 -33,39 +32,194 @@@
  
  #include <asm/cacheflush.h>
  
 -pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
 -		unsigned long attrs)
++<<<<<<< HEAD
 +static int swiotlb __ro_after_init;
 +
 +static pgprot_t __get_dma_pgprot(unsigned long attrs, pgprot_t prot,
 +				 bool coherent)
  {
 -	if (!dev_is_dma_coherent(dev) || (attrs & DMA_ATTR_WRITE_COMBINE))
 +	if (!coherent || (attrs & DMA_ATTR_WRITE_COMBINE))
  		return pgprot_writecombine(prot);
  	return prot;
  }
  
 -void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
 -		size_t size, enum dma_data_direction dir)
 +static struct gen_pool *atomic_pool __ro_after_init;
 +
 +#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
 +static size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;
 +
 +static int __init early_coherent_pool(char *p)
 +{
 +	atomic_pool_size = memparse(p, &p);
 +	return 0;
 +}
 +early_param("coherent_pool", early_coherent_pool);
 +
 +static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)
 +{
 +	unsigned long val;
 +	void *ptr = NULL;
 +
 +	if (!atomic_pool) {
 +		WARN(1, "coherent pool not initialised!\n");
 +		return NULL;
 +	}
 +
 +	val = gen_pool_alloc(atomic_pool, size);
 +	if (val) {
 +		phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);
 +
 +		*ret_page = phys_to_page(phys);
 +		ptr = (void *)val;
 +		memset(ptr, 0, size);
 +	}
 +
 +	return ptr;
 +}
 +
 +static bool __in_atomic_pool(void *start, size_t size)
 +{
 +	return addr_in_gen_pool(atomic_pool, (unsigned long)start, size);
 +}
 +
 +static int __free_from_pool(void *start, size_t size)
 +{
 +	if (!__in_atomic_pool(start, size))
 +		return 0;
 +
 +	gen_pool_free(atomic_pool, (unsigned long)start, size);
 +
 +	return 1;
 +}
 +
 +static void *__dma_alloc(struct device *dev, size_t size,
 +			 dma_addr_t *dma_handle, gfp_t flags,
 +			 unsigned long attrs)
 +{
 +	struct page *page;
 +	void *ptr, *coherent_ptr;
 +	bool coherent = is_device_dma_coherent(dev);
 +	pgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL, false);
 +
 +	size = PAGE_ALIGN(size);
 +
 +	if (!coherent && !gfpflags_allow_blocking(flags)) {
 +		struct page *page = NULL;
 +		void *addr = __alloc_from_pool(size, &page, flags);
 +
 +		if (addr)
 +			*dma_handle = phys_to_dma(dev, page_to_phys(page));
 +
 +		return addr;
 +	}
 +
 +	ptr = dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
 +	if (!ptr)
 +		goto no_mem;
 +
 +	/* no need for non-cacheable mapping if coherent */
 +	if (coherent)
 +		return ptr;
 +
 +	/* remove any dirty cache lines on the kernel alias */
 +	__dma_flush_area(ptr, size);
 +
 +	/* create a coherent mapping */
 +	page = virt_to_page(ptr);
 +	coherent_ptr = dma_common_contiguous_remap(page, size, VM_USERMAP,
 +						   prot, __builtin_return_address(0));
 +	if (!coherent_ptr)
 +		goto no_map;
 +
 +	return coherent_ptr;
 +
 +no_map:
 +	dma_direct_free_pages(dev, size, ptr, *dma_handle, attrs);
 +no_mem:
 +	return NULL;
 +}
 +
 +static void __dma_free(struct device *dev, size_t size,
 +		       void *vaddr, dma_addr_t dma_handle,
 +		       unsigned long attrs)
 +{
 +	void *swiotlb_addr = phys_to_virt(dma_to_phys(dev, dma_handle));
 +
 +	size = PAGE_ALIGN(size);
 +
 +	if (!is_device_dma_coherent(dev)) {
 +		if (__free_from_pool(vaddr, size))
 +			return;
 +		vunmap(vaddr);
 +	}
 +	dma_direct_free_pages(dev, size, swiotlb_addr, dma_handle, attrs);
 +}
 +
 +static dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,
 +				     unsigned long offset, size_t size,
 +				     enum dma_data_direction dir,
 +				     unsigned long attrs)
 +{
 +	dma_addr_t dev_addr;
 +
 +	dev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);
 +
 +	return dev_addr;
 +}
 +
 +
 +static void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,
 +				 size_t size, enum dma_data_direction dir,
 +				 unsigned long attrs)
++=======
++pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
++		unsigned long attrs)
++>>>>>>> 0c3b3171cecc (dma-mapping: move the arm64 noncoherent alloc/free support to common code)
  {
 -	__dma_map_area(phys_to_virt(paddr), size, dir);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);
 +	swiotlb_unmap_page(dev, dev_addr, size, dir, attrs);
  }
  
 +static int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,
 +				  int nelems, enum dma_data_direction dir,
 +				  unsigned long attrs)
 +{
 +	struct scatterlist *sg;
 +	int i, ret;
 +
++<<<<<<< HEAD
 +	ret = swiotlb_map_sg_attrs(dev, sgl, nelems, dir, attrs);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		for_each_sg(sgl, sg, ret, i)
 +			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),
 +				       sg->length, dir);
++=======
+ void arch_sync_dma_for_cpu(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_unmap_area(phys_to_virt(paddr), size, dir);
+ }
+ 
+ void arch_dma_prep_coherent(struct page *page, size_t size)
+ {
+ 	__dma_flush_area(page_address(page), size);
+ }
+ 
+ #ifdef CONFIG_IOMMU_DMA
+ static int __swiotlb_get_sgtable_page(struct sg_table *sgt,
+ 				      struct page *page, size_t size)
+ {
+ 	int ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
++>>>>>>> 0c3b3171cecc (dma-mapping: move the arm64 noncoherent alloc/free support to common code)
  
  	return ret;
  }
@@@ -276,136 -87,8 +303,75 @@@ static int __swiotlb_mmap_pfn(struct vm
  
  	return ret;
  }
 -#endif /* CONFIG_IOMMU_DMA */
 +
 +static int __swiotlb_mmap(struct device *dev,
 +			  struct vm_area_struct *vma,
 +			  void *cpu_addr, dma_addr_t dma_addr, size_t size,
 +			  unsigned long attrs)
 +{
 +	int ret;
 +	unsigned long pfn = dma_to_phys(dev, dma_addr) >> PAGE_SHIFT;
 +
 +	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot,
 +					     is_device_dma_coherent(dev));
 +
 +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
 +		return ret;
 +
 +	return __swiotlb_mmap_pfn(vma, pfn, size);
 +}
 +
 +static int __swiotlb_get_sgtable_page(struct sg_table *sgt,
 +				      struct page *page, size_t size)
 +{
 +	int ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
 +
 +	if (!ret)
 +		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
 +
 +	return ret;
 +}
 +
 +static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				 void *cpu_addr, dma_addr_t handle, size_t size,
 +				 unsigned long attrs)
 +{
 +	struct page *page = phys_to_page(dma_to_phys(dev, handle));
 +
 +	return __swiotlb_get_sgtable_page(sgt, page, size);
 +}
 +
 +static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)
 +{
 +	if (swiotlb)
 +		return swiotlb_dma_supported(hwdev, mask);
 +	return 1;
 +}
 +
 +static int __swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t addr)
 +{
 +	if (swiotlb)
 +		return dma_direct_mapping_error(hwdev, addr);
 +	return 0;
 +}
 +
 +static const struct dma_map_ops arm64_swiotlb_dma_ops = {
 +	.alloc = __dma_alloc,
 +	.free = __dma_free,
 +	.mmap = __swiotlb_mmap,
 +	.get_sgtable = __swiotlb_get_sgtable,
 +	.map_page = __swiotlb_map_page,
 +	.unmap_page = __swiotlb_unmap_page,
 +	.map_sg = __swiotlb_map_sg_attrs,
 +	.unmap_sg = __swiotlb_unmap_sg_attrs,
 +	.sync_single_for_cpu = __swiotlb_sync_single_for_cpu,
 +	.sync_single_for_device = __swiotlb_sync_single_for_device,
 +	.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,
 +	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
 +	.dma_supported = __swiotlb_dma_supported,
 +	.mapping_error = __swiotlb_dma_mapping_error,
 +};
  
- static int __init atomic_pool_init(void)
- {
- 	pgprot_t prot = __pgprot(PROT_NORMAL_NC);
- 	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
- 	struct page *page;
- 	void *addr;
- 	unsigned int pool_size_order = get_order(atomic_pool_size);
- 
- 	if (dev_get_cma_area(NULL))
- 		page = dma_alloc_from_contiguous(NULL, nr_pages,
- 						 pool_size_order, false);
- 	else
- 		page = alloc_pages(GFP_DMA32, pool_size_order);
- 
- 	if (page) {
- 		int ret;
- 		void *page_addr = page_address(page);
- 
- 		memset(page_addr, 0, atomic_pool_size);
- 		__dma_flush_area(page_addr, atomic_pool_size);
- 
- 		atomic_pool = gen_pool_create(PAGE_SHIFT, -1);
- 		if (!atomic_pool)
- 			goto free_page;
- 
- 		addr = dma_common_contiguous_remap(page, atomic_pool_size,
- 					VM_USERMAP, prot, atomic_pool_init);
- 
- 		if (!addr)
- 			goto destroy_genpool;
- 
- 		ret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,
- 					page_to_phys(page),
- 					atomic_pool_size, -1);
- 		if (ret)
- 			goto remove_mapping;
- 
- 		gen_pool_set_algo(atomic_pool,
- 				  gen_pool_first_fit_order_align,
- 				  NULL);
- 
- 		pr_info("DMA: preallocated %zu KiB pool for atomic allocations\n",
- 			atomic_pool_size / 1024);
- 		return 0;
- 	}
- 	goto out;
- 
- remove_mapping:
- 	dma_common_free_remap(addr, atomic_pool_size, VM_USERMAP);
- destroy_genpool:
- 	gen_pool_destroy(atomic_pool);
- 	atomic_pool = NULL;
- free_page:
- 	if (!dma_release_from_contiguous(NULL, page, nr_pages))
- 		__free_pages(page, pool_size_order);
- out:
- 	pr_err("DMA: failed to allocate %zu KiB pool for atomic coherent allocation\n",
- 		atomic_pool_size / 1024);
- 	return -ENOMEM;
- }
- 
  /********************************************
   * The following APIs are for dummy DMA ops *
   ********************************************/
diff --cc kernel/dma/Kconfig
index 1b1d63b3634b,41c3b1df70eb..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -51,3 -51,12 +51,15 @@@ config SWIOTL
  	bool
  	select DMA_DIRECT_OPS
  	select NEED_DMA_MAP_STATE
++<<<<<<< HEAD
++=======
+ 
+ config DMA_REMAP
+ 	depends on MMU
+ 	bool
+ 
+ config DMA_DIRECT_REMAP
+ 	bool
+ 	depends on DMA_DIRECT_OPS
+ 	select DMA_REMAP
++>>>>>>> 0c3b3171cecc (dma-mapping: move the arm64 noncoherent alloc/free support to common code)
* Unmerged path kernel/dma/remap.c
* Unmerged path arch/arm64/Kconfig
* Unmerged path arch/arm64/mm/dma-mapping.c
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 7ed018372d80..3dc77b9417a2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -455,6 +455,11 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 			const void *caller);
 void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
 
+int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
+bool dma_in_atomic_pool(void *start, size_t size);
+void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
+bool dma_free_from_pool(void *start, size_t size);
+
 /**
  * dma_mmap_attrs - map a coherent DMA allocation into user space
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
diff --git a/include/linux/dma-noncoherent.h b/include/linux/dma-noncoherent.h
index a0aa00cc909d..d0befca7d379 100644
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@ -52,4 +52,6 @@ static inline void arch_sync_dma_for_cpu_all(struct device *dev)
 }
 #endif /* CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL */
 
+void arch_dma_prep_coherent(struct page *page, size_t size);
+
 #endif /* _LINUX_DMA_NONCOHERENT_H */
* Unmerged path kernel/dma/Kconfig
* Unmerged path kernel/dma/remap.c
