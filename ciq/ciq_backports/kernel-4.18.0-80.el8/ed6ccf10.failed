dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit ed6ccf10f24bdfc1955bc8b976ddedc370fc3869
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/ed6ccf10.failed

This avoids link failures in drivers using the DMA API, when they
are compiled for user mode Linux with CONFIG_COMPILE_TEST=y.

Fixes: 356da6d0cd ("dma-mapping: bypass indirect calls for dma-direct")
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit ed6ccf10f24bdfc1955bc8b976ddedc370fc3869)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
diff --cc include/linux/dma-mapping.h
index df61c9f6069e,cef2127e1d70..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -195,8 -194,72 +195,77 @@@ static inline int dma_mmap_from_global_
  }
  #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_HAS_DMA
 +#include <asm/dma-mapping.h>
++=======
+ static inline bool dma_is_direct(const struct dma_map_ops *ops)
+ {
+ 	return likely(!ops);
+ }
+ 
+ /*
+  * All the dma_direct_* declarations are here just for the indirect call bypass,
+  * and must not be used directly drivers!
+  */
+ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
+ 		unsigned long offset, size_t size, enum dma_data_direction dir,
+ 		unsigned long attrs);
+ int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+ 		enum dma_data_direction dir, unsigned long attrs);
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+ void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+ #else
+ static inline void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ }
+ #endif
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs);
+ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs);
+ void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+ void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+ #else
+ static inline void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline void dma_direct_unmap_sg(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ }
+ static inline void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ }
+ #endif
+ 
+ #ifdef CONFIG_HAS_DMA
+ #include <asm/dma-mapping.h>
+ 
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
  {
  	if (dev && dev->dma_ops)
@@@ -209,23 -272,10 +278,30 @@@ static inline void set_dma_ops(struct d
  {
  	dev->dma_ops = dma_ops;
  }
++<<<<<<< HEAD
 +#else
 +/*
 + * Define the dma api to allow compilation of dma dependent code.
 + * Code that depends on the dma-mapping API needs to set 'depends on HAS_DMA'
 + * in its Kconfig, unless it already depends on <something> || COMPILE_TEST,
 + * where <something> guarantuees the availability of the dma-mapping API.
 + */
 +static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 +{
 +	return NULL;
 +}
 +#endif
 +
 +static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 +					      size_t size,
 +					      enum dma_data_direction dir,
 +					      unsigned long attrs)
++=======
+ 
+ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+ 		struct page *page, size_t offset, size_t size,
+ 		enum dma_data_direction dir, unsigned long attrs)
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
  	dma_addr_t addr;
@@@ -405,18 -431,178 +467,180 @@@ dma_sync_sg_for_device(struct device *d
  
  }
  
++<<<<<<< HEAD
++=======
+ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+ {
+ 	debug_dma_mapping_error(dev, dma_addr);
+ 
+ 	if (dma_addr == DMA_MAPPING_ERROR)
+ 		return -ENOMEM;
+ 	return 0;
+ }
+ 
+ void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t flag, unsigned long attrs);
+ void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs);
+ void *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t gfp, unsigned long attrs);
+ void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
+ 		dma_addr_t dma_handle);
+ void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+ 		enum dma_data_direction dir);
+ int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs);
+ int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs);
+ int dma_supported(struct device *dev, u64 mask);
+ int dma_set_mask(struct device *dev, u64 mask);
+ int dma_set_coherent_mask(struct device *dev, u64 mask);
+ u64 dma_get_required_mask(struct device *dev);
+ #else /* CONFIG_HAS_DMA */
+ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+ 		struct page *page, size_t offset, size_t size,
+ 		enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return DMA_MAPPING_ERROR;
+ }
+ static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return 0;
+ }
+ static inline void dma_unmap_sg_attrs(struct device *dev,
+ 		struct scatterlist *sg, int nents, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ }
+ static inline dma_addr_t dma_map_resource(struct device *dev,
+ 		phys_addr_t phys_addr, size_t size, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ 	return DMA_MAPPING_ERROR;
+ }
+ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+ {
+ }
+ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+ {
+ 	return -ENOMEM;
+ }
+ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t flag, unsigned long attrs)
+ {
+ 	return NULL;
+ }
+ static void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ }
+ static inline void *dmam_alloc_attrs(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
+ {
+ 	return NULL;
+ }
+ static inline void dmam_free_coherent(struct device *dev, size_t size,
+ 		void *vaddr, dma_addr_t dma_handle)
+ {
+ }
+ static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ }
+ static inline int dma_get_sgtable_attrs(struct device *dev,
+ 		struct sg_table *sgt, void *cpu_addr, dma_addr_t dma_addr,
+ 		size_t size, unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ static inline int dma_supported(struct device *dev, u64 mask)
+ {
+ 	return 0;
+ }
+ static inline int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	return -EIO;
+ }
+ static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
+ {
+ 	return -EIO;
+ }
+ static inline u64 dma_get_required_mask(struct device *dev)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_HAS_DMA */
+ 
+ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	debug_dma_map_single(dev, ptr, size);
+ 	return dma_map_page_attrs(dev, virt_to_page(ptr), offset_in_page(ptr),
+ 			size, dir, attrs);
+ }
+ 
+ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return dma_unmap_page_attrs(dev, addr, size, dir, attrs);
+ }
+ 
+ static inline void dma_sync_single_range_for_cpu(struct device *dev,
+ 		dma_addr_t addr, unsigned long offset, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ 	return dma_sync_single_for_cpu(dev, addr + offset, size, dir);
+ }
+ 
+ static inline void dma_sync_single_range_for_device(struct device *dev,
+ 		dma_addr_t addr, unsigned long offset, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ 	return dma_sync_single_for_device(dev, addr + offset, size, dir);
+ }
+ 
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
  #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
  #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
  #define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
  #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
  #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
- 
- void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
- 		enum dma_data_direction dir);
+ #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
+ #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
  
  extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 -		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 -		unsigned long attrs);
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
  
  void *dma_common_contiguous_remap(struct page *page, size_t size,
  			unsigned long vm_flags,
@@@ -427,108 -613,15 +651,114 @@@ void *dma_common_pages_remap(struct pag
  			const void *caller);
  void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
  
 -int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
 -bool dma_in_atomic_pool(void *start, size_t size);
 -void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 -bool dma_free_from_pool(void *start, size_t size);
 +/**
 + * dma_mmap_attrs - map a coherent DMA allocation into user space
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @vma: vm_area_struct describing requested user mapping
 + * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 + * @handle: device-view address returned from dma_alloc_attrs
 + * @size: size of memory originally requested in dma_alloc_attrs
 + * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 + *
 + * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
 + * into user space.  The coherent DMA buffer must not be freed by the
 + * driver until the user space mapping has been released.
 + */
 +static inline int
 +dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 +	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	BUG_ON(!ops);
 +	if (ops->mmap)
 +		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 +	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
 +}
 +
++<<<<<<< HEAD
 +#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
  
++=======
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  int
  dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,
  		dma_addr_t dma_addr, size_t size, unsigned long attrs);
  
++<<<<<<< HEAD
 +static inline int
 +dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 +		      dma_addr_t dma_addr, size_t size,
 +		      unsigned long attrs)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	BUG_ON(!ops);
 +	if (ops->get_sgtable)
 +		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 +					attrs);
 +	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 +			attrs);
 +}
 +
 +#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 +
 +#ifndef arch_dma_alloc_attrs
 +#define arch_dma_alloc_attrs(dev)	(true)
 +#endif
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				       dma_addr_t *dma_handle, gfp_t flag,
 +				       unsigned long attrs)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	void *cpu_addr;
 +
 +	BUG_ON(!ops);
 +	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
 +
 +	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 +		return cpu_addr;
 +
 +	/* let the implementation decide on the zone to allocate from: */
 +	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
 +
 +	if (!arch_dma_alloc_attrs(&dev))
 +		return NULL;
 +	if (!ops->alloc)
 +		return NULL;
 +
 +	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
 +	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
 +	return cpu_addr;
 +}
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				     void *cpu_addr, dma_addr_t dma_handle,
 +				     unsigned long attrs)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	BUG_ON(!ops);
 +
 +	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
 +		return;
 +	/*
 +	 * On non-coherent platforms which implement DMA-coherent buffers via
 +	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
 +	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
 +	 * sleep on some machines, and b) an indication that the driver is
 +	 * probably misusing the coherent API anyway.
 +	 */
 +	WARN_ON(irqs_disabled());
 +
 +	if (!ops->free || !cpu_addr)
 +		return;
 +
 +	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
 +	ops->free(dev, size, cpu_addr, dma_handle, attrs);
 +}
 +
++=======
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  static inline void *dma_alloc_coherent(struct device *dev, size_t size,
  		dma_addr_t *dma_handle, gfp_t gfp)
  {
@@@ -543,44 -636,6 +773,47 @@@ static inline void dma_free_coherent(st
  	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
  }
  
++<<<<<<< HEAD
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	debug_dma_mapping_error(dev, dma_addr);
 +
 +	if (dma_addr == DMA_MAPPING_ERROR)
 +		return -ENOMEM;
 +	return 0;
 +}
 +
 +static inline void dma_check_mask(struct device *dev, u64 mask)
 +{
 +	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
 +		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	if (!ops)
 +		return 0;
 +	if (!ops->dma_supported)
 +		return 1;
 +	return ops->dma_supported(dev, mask);
 +}
 +
 +#ifndef HAVE_ARCH_DMA_SET_MASK
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	dma_check_mask(dev, mask);
 +
 +	*dev->dma_mask = mask;
 +	return 0;
 +}
 +#endif
++=======
++>>>>>>> ed6ccf10f24b (dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA)
  
  static inline u64 dma_get_mask(struct device *dev)
  {
* Unmerged path include/linux/dma-mapping.h
