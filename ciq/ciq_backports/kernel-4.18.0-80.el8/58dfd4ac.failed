dma-direct: improve addressability error reporting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 58dfd4ac022037c6a562e92fc6d2a778819b2162
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/58dfd4ac.failed

Only report report a DMA addressability report once to avoid spewing the
kernel log with repeated message.  Also provide a stack trace to make it
easy to find the actual caller that caused the problem.

Last but not least move the actual check into the fast path and only
leave the error reporting in a helper.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Tony Luck <tony.luck@intel.com>
(cherry picked from commit 58dfd4ac022037c6a562e92fc6d2a778819b2162)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index a0ae9e05e04e,edb24f94ea1e..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -27,37 -30,70 +27,44 @@@ static inline bool force_dma_unencrypte
  	return sev_active();
  }
  
- static bool
- check_addr(struct device *dev, dma_addr_t dma_addr, size_t size,
- 		const char *caller)
+ static void report_addr(struct device *dev, dma_addr_t dma_addr, size_t size)
  {
++<<<<<<< HEAD
 +	if (unlikely(dev && !dma_capable(dev, dma_addr, size))) {
 +		if (!dev->dma_mask) {
 +			dev_err(dev,
 +				"%s: call on device without dma_mask\n",
 +				caller);
 +			return false;
 +		}
 +
 +		if (*dev->dma_mask >= DMA_BIT_MASK(32)) {
 +			dev_err(dev,
 +				"%s: overflow %pad+%zu of device mask %llx\n",
 +				caller, &dma_addr, size, *dev->dma_mask);
 +		}
 +		return false;
++=======
+ 	if (!dev->dma_mask) {
+ 		dev_err_once(dev, "DMA map on device without dma_mask\n");
+ 	} else if (*dev->dma_mask >= DMA_BIT_MASK(32) || dev->bus_dma_mask) {
+ 		dev_err_once(dev,
+ 			"overflow %pad+%zu of DMA mask %llx bus mask %llx\n",
+ 			&dma_addr, size, *dev->dma_mask, dev->bus_dma_mask);
++>>>>>>> 58dfd4ac0220 (dma-direct: improve addressability error reporting)
  	}
- 	return true;
+ 	WARN_ON_ONCE(1);
  }
  
 -static inline dma_addr_t phys_to_dma_direct(struct device *dev,
 -		phys_addr_t phys)
 -{
 -	if (force_dma_unencrypted())
 -		return __phys_to_dma(dev, phys);
 -	return phys_to_dma(dev, phys);
 -}
 -
 -u64 dma_direct_get_required_mask(struct device *dev)
 -{
 -	u64 max_dma = phys_to_dma_direct(dev, (max_pfn - 1) << PAGE_SHIFT);
 -
 -	if (dev->bus_dma_mask && dev->bus_dma_mask < max_dma)
 -		max_dma = dev->bus_dma_mask;
 -
 -	return (1ULL << (fls64(max_dma) - 1)) * 2 - 1;
 -}
 -
 -static gfp_t __dma_direct_optimal_gfp_mask(struct device *dev, u64 dma_mask,
 -		u64 *phys_mask)
 -{
 -	if (dev->bus_dma_mask && dev->bus_dma_mask < dma_mask)
 -		dma_mask = dev->bus_dma_mask;
 -
 -	if (force_dma_unencrypted())
 -		*phys_mask = __dma_to_phys(dev, dma_mask);
 -	else
 -		*phys_mask = dma_to_phys(dev, dma_mask);
 -
 -	/*
 -	 * Optimistically try the zone that the physical address mask falls
 -	 * into first.  If that returns memory that isn't actually addressable
 -	 * we will fallback to the next lower zone and try again.
 -	 *
 -	 * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding
 -	 * zones.
 -	 */
 -	if (*phys_mask <= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))
 -		return GFP_DMA;
 -	if (*phys_mask <= DMA_BIT_MASK(32))
 -		return GFP_DMA32;
 -	return 0;
 -}
 -
  static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
  {
 -	return phys_to_dma_direct(dev, phys) + size - 1 <=
 -			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_mask);
 +	dma_addr_t addr = force_dma_unencrypted() ?
 +		__phys_to_dma(dev, phys) : phys_to_dma(dev, phys);
 +	return addr + size - 1 <= dev->coherent_dma_mask;
  }
  
 -struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 +void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +		gfp_t gfp, unsigned long attrs)
  {
  	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
  	int page_order = get_order(size);
@@@ -141,10 -274,16 +148,20 @@@ dma_addr_t dma_direct_map_page(struct d
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs)
  {
 -	phys_addr_t phys = page_to_phys(page) + offset;
 -	dma_addr_t dma_addr = phys_to_dma(dev, phys);
 +	dma_addr_t dma_addr = phys_to_dma(dev, page_to_phys(page)) + offset;
  
++<<<<<<< HEAD
 +	if (!check_addr(dev, dma_addr, size, __func__))
 +		return DIRECT_MAPPING_ERROR;
++=======
+ 	if (unlikely(dev && !dma_capable(dev, dma_addr, size))) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+ 		dma_direct_sync_single_for_device(dev, dma_addr, size, dir);
++>>>>>>> 58dfd4ac0220 (dma-direct: improve addressability error reporting)
  	return dma_addr;
  }
  
* Unmerged path kernel/dma/direct.c
