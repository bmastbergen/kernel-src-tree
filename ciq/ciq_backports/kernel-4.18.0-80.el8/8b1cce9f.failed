dma-mapping: fix inverted logic in dma_supported

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Thierry Reding <treding@nvidia.com>
commit 8b1cce9f5832a8eda17d37a3c49fb7dd2d650f46
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/8b1cce9f.failed

The cleanup in commit 356da6d0cde3 ("dma-mapping: bypass indirect calls
for dma-direct") accidentally inverted the logic in the check for the
presence of a ->dma_supported() callback. Switch this back to the way it
was to prevent a crash on boot.

Fixes: 356da6d0cde3 ("dma-mapping: bypass indirect calls for dma-direct")
	Signed-off-by: Thierry Reding <treding@nvidia.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8b1cce9f5832a8eda17d37a3c49fb7dd2d650f46)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/mapping.c
diff --cc kernel/dma/mapping.c
index 4f53f830671a,d7c34d2d1ba5..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -242,99 -258,184 +242,218 @@@ int dma_common_mmap(struct device *dev
  	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
  		return ret;
  
 -	if (off >= count || user_count > count - off)
 -		return -ENXIO;
 +	if (off < count && user_count <= (count - off))
 +		ret = remap_pfn_range(vma, vma->vm_start,
 +				      page_to_pfn(virt_to_page(cpu_addr)) + off,
 +				      user_count << PAGE_SHIFT,
 +				      vma->vm_page_prot);
 +#endif	/* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
  
 -	if (!dev_is_dma_coherent(dev)) {
 -		if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_COHERENT_TO_PFN))
 -			return -ENXIO;
 -		pfn = arch_dma_coherent_to_pfn(dev, cpu_addr, dma_addr);
 -	} else {
 -		pfn = page_to_pfn(virt_to_page(cpu_addr));
 +	return ret;
 +}
 +EXPORT_SYMBOL(dma_common_mmap);
 +
 +#ifdef CONFIG_MMU
 +static struct vm_struct *__dma_common_pages_remap(struct page **pages,
 +			size_t size, unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
 +{
 +	struct vm_struct *area;
 +
 +	area = get_vm_area_caller(size, vm_flags, caller);
 +	if (!area)
 +		return NULL;
 +
 +	if (map_vm_area(area, prot, pages)) {
 +		vunmap(area->addr);
 +		return NULL;
  	}
  
 -	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
 -			user_count << PAGE_SHIFT, vma->vm_page_prot);
 -#else
 -	return -ENXIO;
 -#endif /* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
 +	return area;
  }
  
 -/**
 - * dma_mmap_attrs - map a coherent DMA allocation into user space
 - * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 - * @vma: vm_area_struct describing requested user mapping
 - * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 - * @dma_addr: device-view address returned from dma_alloc_attrs
 - * @size: size of memory originally requested in dma_alloc_attrs
 - * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 - *
 - * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user
 - * space.  The coherent DMA buffer must not be freed by the driver until the
 - * user space mapping has been released.
 +/*
 + * remaps an array of PAGE_SIZE pages into another vm_area
 + * Cannot be used in non-sleeping contexts
   */
 -int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
 -		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 -		unsigned long attrs)
 +void *dma_common_pages_remap(struct page **pages, size_t size,
 +			unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
  {
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	struct vm_struct *area;
  
 -	if (!dma_is_direct(ops) && ops->mmap)
 -		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 -	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +	if (!area)
 +		return NULL;
 +
 +	area->pages = pages;
 +
 +	return area->addr;
  }
 -EXPORT_SYMBOL(dma_mmap_attrs);
  
 -#ifndef ARCH_HAS_DMA_GET_REQUIRED_MASK
 -static u64 dma_default_get_required_mask(struct device *dev)
 +/*
 + * remaps an allocated contiguous region into another vm_area.
 + * Cannot be used in non-sleeping contexts
 + */
 +
 +void *dma_common_contiguous_remap(struct page *page, size_t size,
 +			unsigned long vm_flags,
 +			pgprot_t prot, const void *caller)
  {
 -	u32 low_totalram = ((max_pfn - 1) << PAGE_SHIFT);
 -	u32 high_totalram = ((max_pfn - 1) >> (32 - PAGE_SHIFT));
 -	u64 mask;
 -
 -	if (!high_totalram) {
 -		/* convert to mask just covering totalram */
 -		low_totalram = (1 << (fls(low_totalram) - 1));
 -		low_totalram += low_totalram - 1;
 -		mask = low_totalram;
 -	} else {
 -		high_totalram = (1 << (fls(high_totalram) - 1));
 -		high_totalram += high_totalram - 1;
 -		mask = (((u64)high_totalram) << 32) + 0xffffffff;
 +	int i;
 +	struct page **pages;
 +	struct vm_struct *area;
 +
 +	pages = kmalloc(sizeof(struct page *) << get_order(size), GFP_KERNEL);
 +	if (!pages)
 +		return NULL;
 +
 +	for (i = 0; i < (size >> PAGE_SHIFT); i++)
 +		pages[i] = nth_page(page, i);
 +
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +
 +	kfree(pages);
 +
 +	if (!area)
 +		return NULL;
 +	return area->addr;
 +}
 +
 +/*
 + * unmaps a range previously mapped by dma_common_*_remap
 + */
 +void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
 +{
 +	struct vm_struct *area = find_vm_area(cpu_addr);
 +
 +	if (!area || (area->flags & vm_flags) != vm_flags) {
 +		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 +		return;
  	}
 -	return mask;
 +
 +	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 +	vunmap(cpu_addr);
  }
++<<<<<<< HEAD
++=======
+ 
+ u64 dma_get_required_mask(struct device *dev)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_get_required_mask(dev);
+ 	if (ops->get_required_mask)
+ 		return ops->get_required_mask(dev);
+ 	return dma_default_get_required_mask(dev);
+ }
+ EXPORT_SYMBOL_GPL(dma_get_required_mask);
+ #endif
+ 
+ #ifndef arch_dma_alloc_attrs
+ #define arch_dma_alloc_attrs(dev)	(true)
+ #endif
+ 
+ void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t flag, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	void *cpu_addr;
+ 
+ 	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
+ 
+ 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
+ 		return cpu_addr;
+ 
+ 	/* let the implementation decide on the zone to allocate from: */
+ 	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
+ 
+ 	if (!arch_dma_alloc_attrs(&dev))
+ 		return NULL;
+ 
+ 	if (dma_is_direct(ops))
+ 		cpu_addr = dma_direct_alloc(dev, size, dma_handle, flag, attrs);
+ 	else if (ops->alloc)
+ 		cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+ 	else
+ 		return NULL;
+ 
+ 	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+ 	return cpu_addr;
+ }
+ EXPORT_SYMBOL(dma_alloc_attrs);
+ 
+ void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
+ 		return;
+ 	/*
+ 	 * On non-coherent platforms which implement DMA-coherent buffers via
+ 	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
+ 	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
+ 	 * sleep on some machines, and b) an indication that the driver is
+ 	 * probably misusing the coherent API anyway.
+ 	 */
+ 	WARN_ON(irqs_disabled());
+ 
+ 	if (!cpu_addr)
+ 		return;
+ 
+ 	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+ 	if (dma_is_direct(ops))
+ 		dma_direct_free(dev, size, cpu_addr, dma_handle, attrs);
+ 	else if (ops->free)
+ 		ops->free(dev, size, cpu_addr, dma_handle, attrs);
+ }
+ EXPORT_SYMBOL(dma_free_attrs);
+ 
+ static inline void dma_check_mask(struct device *dev, u64 mask)
+ {
+ 	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
+ 		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
+ }
+ 
+ int dma_supported(struct device *dev, u64 mask)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_supported(dev, mask);
+ 	if (!ops->dma_supported)
+ 		return 1;
+ 	return ops->dma_supported(dev, mask);
+ }
+ EXPORT_SYMBOL(dma_supported);
+ 
+ #ifndef HAVE_ARCH_DMA_SET_MASK
+ int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dev->dma_mask || !dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	*dev->dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_mask);
+ #endif
+ 
+ #ifndef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
+ int dma_set_coherent_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	dev->coherent_dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_coherent_mask);
++>>>>>>> 8b1cce9f5832 (dma-mapping: fix inverted logic in dma_supported)
  #endif
  
  void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
* Unmerged path kernel/dma/mapping.c
