arm64: fix warnings without CONFIG_IOMMU_DMA

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit f62717551b2b7d72fc2a3975539117d350bad84d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/f6271755.failed

__swiotlb_get_sgtable_page and __swiotlb_mmap_pfn are not only misnamed
but also only used if CONFIG_IOMMU_DMA is set.  Just add a simple ifdef
for now, given that we plan to remove them entirely for the next merge
window.

	Reported-by: Florian Fainelli <f.fainelli@gmail.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit f62717551b2b7d72fc2a3975539117d350bad84d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/dma-mapping.c
diff --cc arch/arm64/mm/dma-mapping.c
index f9ad9f019915,a3ac26284845..000000000000
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@@ -138,61 -123,51 +138,78 @@@ no_mem
  	return NULL;
  }
  
 -void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 -		dma_addr_t dma_handle, unsigned long attrs)
 +static void __dma_free(struct device *dev, size_t size,
 +		       void *vaddr, dma_addr_t dma_handle,
 +		       unsigned long attrs)
  {
 -	if (!__free_from_pool(vaddr, PAGE_ALIGN(size))) {
 -		void *kaddr = phys_to_virt(dma_to_phys(dev, dma_handle));
 +	void *swiotlb_addr = phys_to_virt(dma_to_phys(dev, dma_handle));
  
 +	size = PAGE_ALIGN(size);
 +
 +	if (!is_device_dma_coherent(dev)) {
 +		if (__free_from_pool(vaddr, size))
 +			return;
  		vunmap(vaddr);
 -		dma_direct_free_pages(dev, size, kaddr, dma_handle, attrs);
  	}
 +	dma_direct_free_pages(dev, size, swiotlb_addr, dma_handle, attrs);
  }
  
 -long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
 -		dma_addr_t dma_addr)
 +static dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,
 +				     unsigned long offset, size_t size,
 +				     enum dma_data_direction dir,
 +				     unsigned long attrs)
  {
 -	return __phys_to_pfn(dma_to_phys(dev, dma_addr));
 -}
 +	dma_addr_t dev_addr;
  
 -pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
 -		unsigned long attrs)
 -{
 -	if (!dev_is_dma_coherent(dev) || (attrs & DMA_ATTR_WRITE_COMBINE))
 -		return pgprot_writecombine(prot);
 -	return prot;
 +	dev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);
 +
 +	return dev_addr;
  }
  
 -void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
 -		size_t size, enum dma_data_direction dir)
 +
 +static void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,
 +				 size_t size, enum dma_data_direction dir,
 +				 unsigned long attrs)
  {
 -	__dma_map_area(phys_to_virt(paddr), size, dir);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);
 +	swiotlb_unmap_page(dev, dev_addr, size, dir, attrs);
  }
  
 +static int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,
 +				  int nelems, enum dma_data_direction dir,
 +				  unsigned long attrs)
 +{
 +	struct scatterlist *sg;
 +	int i, ret;
 +
++<<<<<<< HEAD
 +	ret = swiotlb_map_sg_attrs(dev, sgl, nelems, dir, attrs);
 +	if (!is_device_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		for_each_sg(sgl, sg, ret, i)
 +			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),
 +				       sg->length, dir);
++=======
+ void arch_sync_dma_for_cpu(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_unmap_area(phys_to_virt(paddr), size, dir);
+ }
+ 
+ #ifdef CONFIG_IOMMU_DMA
+ static int __swiotlb_get_sgtable_page(struct sg_table *sgt,
+ 				      struct page *page, size_t size)
+ {
+ 	int ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
++>>>>>>> f62717551b2b (arm64: fix warnings without CONFIG_IOMMU_DMA)
  
  	return ret;
  }
@@@ -276,75 -189,8 +293,76 @@@ static int __swiotlb_mmap_pfn(struct vm
  
  	return ret;
  }
+ #endif /* CONFIG_IOMMU_DMA */
  
 +static int __swiotlb_mmap(struct device *dev,
 +			  struct vm_area_struct *vma,
 +			  void *cpu_addr, dma_addr_t dma_addr, size_t size,
 +			  unsigned long attrs)
 +{
 +	int ret;
 +	unsigned long pfn = dma_to_phys(dev, dma_addr) >> PAGE_SHIFT;
 +
 +	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot,
 +					     is_device_dma_coherent(dev));
 +
 +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
 +		return ret;
 +
 +	return __swiotlb_mmap_pfn(vma, pfn, size);
 +}
 +
 +static int __swiotlb_get_sgtable_page(struct sg_table *sgt,
 +				      struct page *page, size_t size)
 +{
 +	int ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
 +
 +	if (!ret)
 +		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
 +
 +	return ret;
 +}
 +
 +static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				 void *cpu_addr, dma_addr_t handle, size_t size,
 +				 unsigned long attrs)
 +{
 +	struct page *page = phys_to_page(dma_to_phys(dev, handle));
 +
 +	return __swiotlb_get_sgtable_page(sgt, page, size);
 +}
 +
 +static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)
 +{
 +	if (swiotlb)
 +		return swiotlb_dma_supported(hwdev, mask);
 +	return 1;
 +}
 +
 +static int __swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t addr)
 +{
 +	if (swiotlb)
 +		return dma_direct_mapping_error(hwdev, addr);
 +	return 0;
 +}
 +
 +static const struct dma_map_ops arm64_swiotlb_dma_ops = {
 +	.alloc = __dma_alloc,
 +	.free = __dma_free,
 +	.mmap = __swiotlb_mmap,
 +	.get_sgtable = __swiotlb_get_sgtable,
 +	.map_page = __swiotlb_map_page,
 +	.unmap_page = __swiotlb_unmap_page,
 +	.map_sg = __swiotlb_map_sg_attrs,
 +	.unmap_sg = __swiotlb_unmap_sg_attrs,
 +	.sync_single_for_cpu = __swiotlb_sync_single_for_cpu,
 +	.sync_single_for_device = __swiotlb_sync_single_for_device,
 +	.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,
 +	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
 +	.dma_supported = __swiotlb_dma_supported,
 +	.mapping_error = __swiotlb_dma_mapping_error,
 +};
 +
  static int __init atomic_pool_init(void)
  {
  	pgprot_t prot = __pgprot(PROT_NORMAL_NC);
* Unmerged path arch/arm64/mm/dma-mapping.c
