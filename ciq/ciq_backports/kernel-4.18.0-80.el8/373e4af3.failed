block: remove queue_lockdep_assert_held

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 373e4af34ec13c17a6b80227c7d5d3719122eb77
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/373e4af3.failed

The only remaining user unconditionally drops and reacquires the lock,
which means we really don't need any additional (conditional) annotation.

	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 373e4af34ec13c17a6b80227c7d5d3719122eb77)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk.h
diff --cc block/blk.h
index fc4461de2d5b,027a0ccc175e..000000000000
--- a/block/blk.h
+++ b/block/blk.h
@@@ -41,79 -35,10 +41,84 @@@ extern struct kmem_cache *blk_requestq_
  extern struct kobj_type blk_queue_ktype;
  extern struct ida blk_queue_ida;
  
++<<<<<<< HEAD
 +/*
 + * @q->queue_lock is set while a queue is being initialized. Since we know
 + * that no other threads access the queue object before @q->queue_lock has
 + * been set, it is safe to manipulate queue flags without holding the
 + * queue_lock if @q->queue_lock == NULL. See also blk_alloc_queue_node() and
 + * blk_init_allocated_queue().
 + */
 +static inline void queue_lockdep_assert_held(struct request_queue *q)
 +{
 +	if (q->queue_lock)
 +		lockdep_assert_held(q->queue_lock);
 +}
 +
 +static inline void queue_flag_set_unlocked(unsigned int flag,
 +					   struct request_queue *q)
++=======
+ static inline struct blk_flush_queue *
+ blk_get_flush_queue(struct request_queue *q, struct blk_mq_ctx *ctx)
++>>>>>>> 373e4af34ec1 (block: remove queue_lockdep_assert_held)
 +{
 +	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
 +	    kref_read(&q->kobj.kref))
 +		lockdep_assert_held(q->queue_lock);
 +	__set_bit(flag, &q->queue_flags);
 +}
 +
 +static inline void queue_flag_clear_unlocked(unsigned int flag,
 +					     struct request_queue *q)
  {
 -	return blk_mq_map_queue(q, REQ_OP_FLUSH, ctx->cpu)->fq;
 +	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
 +	    kref_read(&q->kobj.kref))
 +		lockdep_assert_held(q->queue_lock);
 +	__clear_bit(flag, &q->queue_flags);
 +}
 +
 +static inline int queue_flag_test_and_clear(unsigned int flag,
 +					    struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +
 +	if (test_bit(flag, &q->queue_flags)) {
 +		__clear_bit(flag, &q->queue_flags);
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static inline int queue_flag_test_and_set(unsigned int flag,
 +					  struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +
 +	if (!test_bit(flag, &q->queue_flags)) {
 +		__set_bit(flag, &q->queue_flags);
 +		return 0;
 +	}
 +
 +	return 1;
 +}
 +
 +static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +	__set_bit(flag, &q->queue_flags);
 +}
 +
 +static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +	__clear_bit(flag, &q->queue_flags);
 +}
 +
 +static inline struct blk_flush_queue *blk_get_flush_queue(
 +		struct request_queue *q, struct blk_mq_ctx *ctx)
 +{
 +	return blk_mq_map_queue(q, ctx->cpu)->fq;
  }
  
  static inline void __blk_get_queue(struct request_queue *q)
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index 07f1b008e5ab..1e3f53c57d54 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2351,7 +2351,6 @@ void blk_throtl_drain(struct request_queue *q)
 	struct bio *bio;
 	int rw;
 
-	queue_lockdep_assert_held(q);
 	rcu_read_lock();
 
 	/*
* Unmerged path block/blk.h
