dma-direct: provide page based alloc/free helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit b18814e767a445534ab9ccba02e82a31208f85d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/b18814e7.failed

Some architectures support remapping highmem into DMA coherent
allocations.  To use the common code for them we need variants of
dma_direct_{alloc,free}_pages that do not use kernel virtual addresses.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Robin Murphy <robin.murphy@arm.com>
(cherry picked from commit b18814e767a445534ab9ccba02e82a31208f85d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-direct.h
#	kernel/dma/direct.c
diff --cc include/linux/dma-direct.h
index 861a4e72abde,61b78f934f64..000000000000
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@@ -61,6 -63,13 +61,16 @@@ void *dma_direct_alloc(struct device *d
  		gfp_t gfp, unsigned long attrs);
  void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
  		dma_addr_t dma_addr, unsigned long attrs);
++<<<<<<< HEAD
++=======
+ void *dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_addr, unsigned long attrs);
+ struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+ void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
++>>>>>>> b18814e767a4 (dma-direct: provide page based alloc/free helpers)
  dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs);
diff --cc kernel/dma/direct.c
index a0ae9e05e04e,680287779b0a..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -49,20 -53,63 +49,29 @@@ check_addr(struct device *dev, dma_addr
  	return true;
  }
  
 -static inline dma_addr_t phys_to_dma_direct(struct device *dev,
 -		phys_addr_t phys)
 -{
 -	if (force_dma_unencrypted())
 -		return __phys_to_dma(dev, phys);
 -	return phys_to_dma(dev, phys);
 -}
 -
 -u64 dma_direct_get_required_mask(struct device *dev)
 -{
 -	u64 max_dma = phys_to_dma_direct(dev, (max_pfn - 1) << PAGE_SHIFT);
 -
 -	if (dev->bus_dma_mask && dev->bus_dma_mask < max_dma)
 -		max_dma = dev->bus_dma_mask;
 -
 -	return (1ULL << (fls64(max_dma) - 1)) * 2 - 1;
 -}
 -
 -static gfp_t __dma_direct_optimal_gfp_mask(struct device *dev, u64 dma_mask,
 -		u64 *phys_mask)
 -{
 -	if (dev->bus_dma_mask && dev->bus_dma_mask < dma_mask)
 -		dma_mask = dev->bus_dma_mask;
 -
 -	if (force_dma_unencrypted())
 -		*phys_mask = __dma_to_phys(dev, dma_mask);
 -	else
 -		*phys_mask = dma_to_phys(dev, dma_mask);
 -
 -	/*
 -	 * Optimistically try the zone that the physical address mask falls
 -	 * into first.  If that returns memory that isn't actually addressable
 -	 * we will fallback to the next lower zone and try again.
 -	 *
 -	 * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding
 -	 * zones.
 -	 */
 -	if (*phys_mask <= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))
 -		return GFP_DMA;
 -	if (*phys_mask <= DMA_BIT_MASK(32))
 -		return GFP_DMA32;
 -	return 0;
 -}
 -
  static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
  {
 -	return phys_to_dma_direct(dev, phys) + size - 1 <=
 -			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_mask);
 +	dma_addr_t addr = force_dma_unencrypted() ?
 +		__phys_to_dma(dev, phys) : phys_to_dma(dev, phys);
 +	return addr + size - 1 <= dev->coherent_dma_mask;
  }
  
++<<<<<<< HEAD
 +void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +		gfp_t gfp, unsigned long attrs)
++=======
+ struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
++>>>>>>> b18814e767a4 (dma-direct: provide page based alloc/free helpers)
  {
  	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
  	int page_order = get_order(size);
  	struct page *page = NULL;
++<<<<<<< HEAD
 +	void *ret;
++=======
+ 	u64 phys_mask;
++>>>>>>> b18814e767a4 (dma-direct: provide page based alloc/free helpers)
  
  	if (attrs & DMA_ATTR_NO_WARN)
  		gfp |= __GFP_NOWARN;
@@@ -121,22 -173,102 +141,32 @@@ void *dma_direct_alloc_pages(struct dev
  	return ret;
  }
  
++<<<<<<< HEAD
 +/*
 + * NOTE: this function must never look at the dma_addr argument, because we want
 + * to be able to use it as a helper for iommu implementations as well.
 + */
 +void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
- 		dma_addr_t dma_addr, unsigned long attrs)
++=======
+ void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page)
  {
  	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 
+ 	if (!dma_release_from_contiguous(dev, page, count))
+ 		__free_pages(page, get_order(size));
+ }
+ 
+ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
++>>>>>>> b18814e767a4 (dma-direct: provide page based alloc/free helpers)
+ 		dma_addr_t dma_addr, unsigned long attrs)
+ {
  	unsigned int page_order = get_order(size);
  
  	if (force_dma_unencrypted())
  		set_memory_encrypted((unsigned long)cpu_addr, 1 << page_order);
- 	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))
- 		free_pages((unsigned long)cpu_addr, page_order);
+ 	__dma_direct_free_pages(dev, size, virt_to_page(cpu_addr));
  }
  
 -void *dma_direct_alloc(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 -{
 -	if (!dev_is_dma_coherent(dev))
 -		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
 -	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
 -}
 -
 -void dma_direct_free(struct device *dev, size_t size,
 -		void *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)
 -{
 -	if (!dev_is_dma_coherent(dev))
 -		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
 -	else
 -		dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
 -}
 -
 -static void dma_direct_sync_single_for_device(struct device *dev,
 -		dma_addr_t addr, size_t size, enum dma_data_direction dir)
 -{
 -	if (dev_is_dma_coherent(dev))
 -		return;
 -	arch_sync_dma_for_device(dev, dma_to_phys(dev, addr), size, dir);
 -}
 -
 -static void dma_direct_sync_sg_for_device(struct device *dev,
 -		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
 -{
 -	struct scatterlist *sg;
 -	int i;
 -
 -	if (dev_is_dma_coherent(dev))
 -		return;
 -
 -	for_each_sg(sgl, sg, nents, i)
 -		arch_sync_dma_for_device(dev, sg_phys(sg), sg->length, dir);
 -}
 -
 -#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
 -    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
 -static void dma_direct_sync_single_for_cpu(struct device *dev,
 -		dma_addr_t addr, size_t size, enum dma_data_direction dir)
 -{
 -	if (dev_is_dma_coherent(dev))
 -		return;
 -	arch_sync_dma_for_cpu(dev, dma_to_phys(dev, addr), size, dir);
 -	arch_sync_dma_for_cpu_all(dev);
 -}
 -
 -static void dma_direct_sync_sg_for_cpu(struct device *dev,
 -		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
 -{
 -	struct scatterlist *sg;
 -	int i;
 -
 -	if (dev_is_dma_coherent(dev))
 -		return;
 -
 -	for_each_sg(sgl, sg, nents, i)
 -		arch_sync_dma_for_cpu(dev, sg_phys(sg), sg->length, dir);
 -	arch_sync_dma_for_cpu_all(dev);
 -}
 -
 -static void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
 -		size_t size, enum dma_data_direction dir, unsigned long attrs)
 -{
 -	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 -		dma_direct_sync_single_for_cpu(dev, addr, size, dir);
 -}
 -
 -static void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
 -		int nents, enum dma_data_direction dir, unsigned long attrs)
 -{
 -	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 -		dma_direct_sync_sg_for_cpu(dev, sgl, nents, dir);
 -}
 -#endif
 -
  dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs)
* Unmerged path include/linux/dma-direct.h
* Unmerged path kernel/dma/direct.c
