rcu: Switch urgent quiescent-state requests to rcu_data structure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit 2dba13f0b6c2b26ff371b8927ac58d20a7d94713
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/2dba13f0.failed

This commit removes ->rcu_need_heavy_qs and ->rcu_urgent_qs from the
rcu_dynticks structure and updates the code to access them from the
rcu_data structure.

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit 2dba13f0b6c2b26ff371b8927ac58d20a7d94713)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
#	kernel/rcu/tree.h
diff --cc kernel/rcu/tree.c
index 65383db6b433,7ec0ba885273..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -1072,25 -1071,21 +1072,31 @@@ static int rcu_implicit_dynticks_qs(str
  
  	/*
  	 * A CPU running for an extended time within the kernel can
 -	 * delay RCU grace periods: (1) At age jiffies_to_sched_qs,
 -	 * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set
 -	 * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the
 -	 * unsynchronized assignments to the per-CPU rcu_need_heavy_qs
 -	 * variable are safe because the assignments are repeated if this
 -	 * CPU failed to pass through a quiescent state.  This code
 -	 * also checks .jiffies_resched in case jiffies_to_sched_qs
 -	 * is set way high.
 +	 * delay RCU grace periods.  When the CPU is in NO_HZ_FULL mode,
 +	 * even context-switching back and forth between a pair of
 +	 * in-kernel CPU-bound tasks cannot advance grace periods.
 +	 * So if the grace period is old enough, make the CPU pay attention.
 +	 * Note that the unsynchronized assignments to the per-CPU
 +	 * rcu_need_heavy_qs variable are safe.  Yes, setting of
 +	 * bits can be lost, but they will be set again on the next
 +	 * force-quiescent-state pass.  So lost bit sets do not result
 +	 * in incorrect behavior, merely in a grace period lasting
 +	 * a few jiffies longer than it might otherwise.  Because
 +	 * there are at most four threads involved, and because the
 +	 * updates are only once every few jiffies, the probability of
 +	 * lossage (and thus of slight grace-period extension) is
 +	 * quite low.
  	 */
++<<<<<<< HEAD
 +	rnhqp = &per_cpu(rcu_dynticks.rcu_need_heavy_qs, rdp->cpu);
++=======
+ 	jtsq = READ_ONCE(jiffies_to_sched_qs);
+ 	ruqp = per_cpu_ptr(&rcu_data.rcu_urgent_qs, rdp->cpu);
+ 	rnhqp = &per_cpu(rcu_data.rcu_need_heavy_qs, rdp->cpu);
++>>>>>>> 2dba13f0b6c2 (rcu: Switch urgent quiescent-state requests to rcu_data structure)
  	if (!READ_ONCE(*rnhqp) &&
 -	    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||
 -	     time_after(jiffies, rcu_state.jiffies_resched))) {
 +	    (time_after(jiffies, rdp->rsp->gp_start + jtsq) ||
 +	     time_after(jiffies, rdp->rsp->jiffies_resched))) {
  		WRITE_ONCE(*rnhqp, true);
  		/* Store rcu_need_heavy_qs before rcu_urgent_qs. */
  		smp_store_release(ruqp, true);
@@@ -2480,7 -2497,16 +2486,20 @@@ static void rcu_do_batch(struct rcu_dat
  void rcu_check_callbacks(int user)
  {
  	trace_rcu_utilization(TPS("Start scheduler-tick"));
++<<<<<<< HEAD
 +	increment_cpu_stall_ticks();
++=======
+ 	raw_cpu_inc(rcu_data.ticks_this_gp);
+ 	/* The load-acquire pairs with the store-release setting to true. */
+ 	if (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {
+ 		/* Idle and userspace execution already are quiescent states. */
+ 		if (!rcu_is_cpu_rrupt_from_idle() && !user) {
+ 			set_tsk_need_resched(current);
+ 			set_preempt_need_resched();
+ 		}
+ 		__this_cpu_write(rcu_data.rcu_urgent_qs, false);
+ 	}
++>>>>>>> 2dba13f0b6c2 (rcu: Switch urgent quiescent-state requests to rcu_data structure)
  	rcu_flavor_check_callbacks(user);
  	if (rcu_pending())
  		invoke_rcu_core();
diff --cc kernel/rcu/tree.h
index 971069b043e9,4c31066ddb94..000000000000
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@@ -41,21 -41,6 +41,24 @@@ struct rcu_dynticks 
  	long dynticks_nesting;      /* Track process nesting level. */
  	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
  	atomic_t dynticks;	    /* Even value for idle, else odd. */
++<<<<<<< HEAD
 +	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 +	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 +	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 +#ifdef CONFIG_RCU_FAST_NO_HZ
 +	bool all_lazy;		    /* Are all CPU's CBs lazy? */
 +	unsigned long nonlazy_posted;
 +				    /* # times non-lazy CBs posted to CPU. */
 +	unsigned long nonlazy_posted_snap;
 +				    /* idle-period nonlazy_posted snapshot. */
 +	unsigned long last_accelerate;
 +				    /* Last jiffy CBs were accelerated. */
 +	unsigned long last_advance_all;
 +				    /* Last jiffy CBs were all advanced. */
 +	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
 +#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
++=======
++>>>>>>> 2dba13f0b6c2 (rcu: Switch urgent quiescent-state requests to rcu_data structure)
  };
  
  /* Communicate arguments to a workqueue handler. */
* Unmerged path kernel/rcu/tree.c
* Unmerged path kernel/rcu/tree.h
diff --git a/kernel/rcu/tree_exp.h b/kernel/rcu/tree_exp.h
index d5f722f2b4cd..56e29ddfb865 100644
--- a/kernel/rcu/tree_exp.h
+++ b/kernel/rcu/tree_exp.h
@@ -790,7 +790,7 @@ static void sync_sched_exp_handler(void *unused)
 	}
 	__this_cpu_write(rcu_data.cpu_no_qs.b.exp, true);
 	/* Store .exp before .rcu_urgent_qs. */
-	smp_store_release(this_cpu_ptr(&rcu_dynticks.rcu_urgent_qs), true);
+	smp_store_release(this_cpu_ptr(&rcu_data.rcu_urgent_qs), true);
 	set_tsk_need_resched(current);
 	set_preempt_need_resched();
 }
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 6d38152e918f..99480176964d 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -966,17 +966,17 @@ void rcu_all_qs(void)
 {
 	unsigned long flags;
 
-	if (!raw_cpu_read(rcu_dynticks.rcu_urgent_qs))
+	if (!raw_cpu_read(rcu_data.rcu_urgent_qs))
 		return;
 	preempt_disable();
 	/* Load rcu_urgent_qs before other flags. */
-	if (!smp_load_acquire(this_cpu_ptr(&rcu_dynticks.rcu_urgent_qs))) {
+	if (!smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {
 		preempt_enable();
 		return;
 	}
-	this_cpu_write(rcu_dynticks.rcu_urgent_qs, false);
+	this_cpu_write(rcu_data.rcu_urgent_qs, false);
 	barrier(); /* Avoid RCU read-side critical sections leaking down. */
-	if (unlikely(raw_cpu_read(rcu_dynticks.rcu_need_heavy_qs))) {
+	if (unlikely(raw_cpu_read(rcu_data.rcu_need_heavy_qs))) {
 		local_irq_save(flags);
 		rcu_momentary_dyntick_idle();
 		local_irq_restore(flags);
@@ -998,10 +998,10 @@ void rcu_note_context_switch(bool preempt)
 	trace_rcu_utilization(TPS("Start context switch"));
 	rcu_qs();
 	/* Load rcu_urgent_qs before other flags. */
-	if (!smp_load_acquire(this_cpu_ptr(&rcu_dynticks.rcu_urgent_qs)))
+	if (!smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs)))
 		goto out;
-	this_cpu_write(rcu_dynticks.rcu_urgent_qs, false);
-	if (unlikely(raw_cpu_read(rcu_dynticks.rcu_need_heavy_qs)))
+	this_cpu_write(rcu_data.rcu_urgent_qs, false);
+	if (unlikely(raw_cpu_read(rcu_data.rcu_need_heavy_qs)))
 		rcu_momentary_dyntick_idle();
 	this_cpu_inc(rcu_dynticks.rcu_qs_ctr);
 	if (!preempt)
