dma-mapping: move various slow path functions out of line

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 7249c1a52df9967cd23550f3dc24fb6ca43cdc6a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/7249c1a5.failed

There is no need to have all setup and coherent allocation / freeing
routines inline.  Move them out of line to keep the implemeation
nicely encapsulated and save some kernel text size.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Tony Luck <tony.luck@intel.com>
(cherry picked from commit 7249c1a52df9967cd23550f3dc24fb6ca43cdc6a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
#	kernel/dma/mapping.c
diff --cc include/linux/dma-mapping.h
index 763c2c215404,0bbce52606c2..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -435,30 -435,14 +435,41 @@@ void *dma_common_pages_remap(struct pag
  			const void *caller);
  void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
  
++<<<<<<< HEAD
 +/**
 + * dma_mmap_attrs - map a coherent DMA allocation into user space
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @vma: vm_area_struct describing requested user mapping
 + * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 + * @handle: device-view address returned from dma_alloc_attrs
 + * @size: size of memory originally requested in dma_alloc_attrs
 + * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 + *
 + * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
 + * into user space.  The coherent DMA buffer must not be freed by the
 + * driver until the user space mapping has been released.
 + */
 +static inline int
 +dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 +	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
 +{
 +	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	BUG_ON(!ops);
 +	if (ops->mmap)
 +		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 +	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
 +}
 +
++=======
+ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
+ bool dma_in_atomic_pool(void *start, size_t size);
+ void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
+ bool dma_free_from_pool(void *start, size_t size);
+ 
+ int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs);
++>>>>>>> 7249c1a52df9 (dma-mapping: move various slow path functions out of line)
  #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
  
  int
diff --cc kernel/dma/mapping.c
index e100ec904c99,176ae3e08916..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -242,97 -257,176 +255,222 @@@ int dma_common_mmap(struct device *dev
  	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
  		return ret;
  
 -	if (off >= count || user_count > count - off)
 -		return -ENXIO;
 -
 -	if (!dev_is_dma_coherent(dev)) {
 -		if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_COHERENT_TO_PFN))
 -			return -ENXIO;
 -		pfn = arch_dma_coherent_to_pfn(dev, cpu_addr, dma_addr);
 -	} else {
 -		pfn = page_to_pfn(virt_to_page(cpu_addr));
 -	}
 +	if (off < count && user_count <= (count - off))
 +		ret = remap_pfn_range(vma, vma->vm_start,
 +				      page_to_pfn(virt_to_page(cpu_addr)) + off,
 +				      user_count << PAGE_SHIFT,
 +				      vma->vm_page_prot);
 +#endif	/* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
  
 -	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
 -			user_count << PAGE_SHIFT, vma->vm_page_prot);
 -#else
 -	return -ENXIO;
 -#endif /* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
 +	return ret;
  }
- EXPORT_SYMBOL(dma_common_mmap);
+ 
+ /**
+  * dma_mmap_attrs - map a coherent DMA allocation into user space
+  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+  * @vma: vm_area_struct describing requested user mapping
+  * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
+  * @dma_addr: device-view address returned from dma_alloc_attrs
+  * @size: size of memory originally requested in dma_alloc_attrs
+  * @attrs: attributes of mapping properties requested in dma_alloc_attrs
+  *
+  * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user
+  * space.  The coherent DMA buffer must not be freed by the driver until the
+  * user space mapping has been released.
+  */
+ int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	BUG_ON(!ops);
+ 	if (ops->mmap)
+ 		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+ 	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+ }
+ EXPORT_SYMBOL(dma_mmap_attrs);
  
 -#ifndef ARCH_HAS_DMA_GET_REQUIRED_MASK
 -static u64 dma_default_get_required_mask(struct device *dev)
 +#ifdef CONFIG_MMU
 +static struct vm_struct *__dma_common_pages_remap(struct page **pages,
 +			size_t size, unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
  {
 -	u32 low_totalram = ((max_pfn - 1) << PAGE_SHIFT);
 -	u32 high_totalram = ((max_pfn - 1) >> (32 - PAGE_SHIFT));
 -	u64 mask;
 -
 -	if (!high_totalram) {
 -		/* convert to mask just covering totalram */
 -		low_totalram = (1 << (fls(low_totalram) - 1));
 -		low_totalram += low_totalram - 1;
 -		mask = low_totalram;
 -	} else {
 -		high_totalram = (1 << (fls(high_totalram) - 1));
 -		high_totalram += high_totalram - 1;
 -		mask = (((u64)high_totalram) << 32) + 0xffffffff;
 +	struct vm_struct *area;
 +
 +	area = get_vm_area_caller(size, vm_flags, caller);
 +	if (!area)
 +		return NULL;
 +
 +	if (map_vm_area(area, prot, pages)) {
 +		vunmap(area->addr);
 +		return NULL;
  	}
 -	return mask;
 +
 +	return area;
  }
  
 -u64 dma_get_required_mask(struct device *dev)
 +/*
 + * remaps an array of PAGE_SIZE pages into another vm_area
 + * Cannot be used in non-sleeping contexts
 + */
 +void *dma_common_pages_remap(struct page **pages, size_t size,
 +			unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
  {
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 +	struct vm_struct *area;
 +
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +	if (!area)
 +		return NULL;
  
 -	if (ops->get_required_mask)
 -		return ops->get_required_mask(dev);
 -	return dma_default_get_required_mask(dev);
 +	area->pages = pages;
 +
 +	return area->addr;
  }
 -EXPORT_SYMBOL_GPL(dma_get_required_mask);
 -#endif
  
++<<<<<<< HEAD
 +/*
 + * remaps an allocated contiguous region into another vm_area.
 + * Cannot be used in non-sleeping contexts
 + */
 +
 +void *dma_common_contiguous_remap(struct page *page, size_t size,
 +			unsigned long vm_flags,
 +			pgprot_t prot, const void *caller)
 +{
 +	int i;
 +	struct page **pages;
 +	struct vm_struct *area;
 +
 +	pages = kmalloc(sizeof(struct page *) << get_order(size), GFP_KERNEL);
 +	if (!pages)
 +		return NULL;
 +
 +	for (i = 0; i < (size >> PAGE_SHIFT); i++)
 +		pages[i] = nth_page(page, i);
 +
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +
 +	kfree(pages);
 +
 +	if (!area)
 +		return NULL;
 +	return area->addr;
 +}
 +
 +/*
 + * unmaps a range previously mapped by dma_common_*_remap
 + */
 +void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
 +{
 +	struct vm_struct *area = find_vm_area(cpu_addr);
 +
 +	if (!area || (area->flags & vm_flags) != vm_flags) {
 +		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 +		return;
 +	}
 +
 +	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 +	vunmap(cpu_addr);
 +}
++=======
+ #ifndef arch_dma_alloc_attrs
+ #define arch_dma_alloc_attrs(dev)	(true)
+ #endif
+ 
+ void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t flag, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	void *cpu_addr;
+ 
+ 	BUG_ON(!ops);
+ 	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
+ 
+ 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
+ 		return cpu_addr;
+ 
+ 	/* let the implementation decide on the zone to allocate from: */
+ 	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
+ 
+ 	if (!arch_dma_alloc_attrs(&dev))
+ 		return NULL;
+ 	if (!ops->alloc)
+ 		return NULL;
+ 
+ 	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+ 	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+ 	return cpu_addr;
+ }
+ EXPORT_SYMBOL(dma_alloc_attrs);
+ 
+ void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!ops);
+ 
+ 	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
+ 		return;
+ 	/*
+ 	 * On non-coherent platforms which implement DMA-coherent buffers via
+ 	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
+ 	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
+ 	 * sleep on some machines, and b) an indication that the driver is
+ 	 * probably misusing the coherent API anyway.
+ 	 */
+ 	WARN_ON(irqs_disabled());
+ 
+ 	if (!ops->free || !cpu_addr)
+ 		return;
+ 
+ 	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+ 	ops->free(dev, size, cpu_addr, dma_handle, attrs);
+ }
+ EXPORT_SYMBOL(dma_free_attrs);
+ 
+ static inline void dma_check_mask(struct device *dev, u64 mask)
+ {
+ 	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
+ 		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
+ }
+ 
+ int dma_supported(struct device *dev, u64 mask)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (!ops)
+ 		return 0;
+ 	if (!ops->dma_supported)
+ 		return 1;
+ 	return ops->dma_supported(dev, mask);
+ }
+ EXPORT_SYMBOL(dma_supported);
+ 
+ #ifndef HAVE_ARCH_DMA_SET_MASK
+ int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dev->dma_mask || !dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	*dev->dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_mask);
+ #endif
+ 
+ #ifndef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
+ int dma_set_coherent_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	dev->coherent_dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_coherent_mask);
++>>>>>>> 7249c1a52df9 (dma-mapping: move various slow path functions out of line)
  #endif
diff --git a/arch/powerpc/include/asm/dma-mapping.h b/arch/powerpc/include/asm/dma-mapping.h
index 8fa394520af6..5201f2b7838c 100644
--- a/arch/powerpc/include/asm/dma-mapping.h
+++ b/arch/powerpc/include/asm/dma-mapping.h
@@ -108,7 +108,6 @@ static inline void set_dma_offset(struct device *dev, dma_addr_t off)
 }
 
 #define HAVE_ARCH_DMA_SET_MASK 1
-extern int dma_set_mask(struct device *dev, u64 dma_mask);
 
 extern u64 __dma_get_required_mask(struct device *dev);
 
* Unmerged path include/linux/dma-mapping.h
* Unmerged path kernel/dma/mapping.c
