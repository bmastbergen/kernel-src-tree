KVM: PPC: Book3S HV: Implement H_TLB_INVALIDATE hcall

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Suraj Jitindar Singh <sjitindarsingh@gmail.com>
commit e3b6b4661527e821ffbe3db83952fdb1e6e47c49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/e3b6b466.failed

When running a nested (L2) guest the guest (L1) hypervisor will use
the H_TLB_INVALIDATE hcall when it needs to change the partition
scoped page tables or the partition table which it manages.  It will
use this hcall in the situations where it would use a partition-scoped
tlbie instruction if it were running in hypervisor mode.

The H_TLB_INVALIDATE hcall can invalidate different scopes:

Invalidate TLB for a given target address:
- This invalidates a single L2 -> L1 pte
- We need to invalidate any L2 -> L0 shadow_pgtable ptes which map the L2
  address space which is being invalidated. This is because a single
  L2 -> L1 pte may have been mapped with more than one pte in the
  L2 -> L0 page tables.

Invalidate the entire TLB for a given LPID or for all LPIDs:
- Invalidate the entire shadow_pgtable for a given nested guest, or
  for all nested guests.

Invalidate the PWC (page walk cache) for a given LPID or for all LPIDs:
- We don't cache the PWC, so nothing to do.

Invalidate the entire TLB, PWC and partition table for a given/all LPIDs:
- Here we re-read the partition table entry and remove the nested state
  for any nested guest for which the first doubleword of the partition
  table entry is now zero.

The H_TLB_INVALIDATE hcall takes as parameters the tlbie instruction
word (of which only the RIC, PRS and R fields are used), the rS value
(giving the lpid, where required) and the rB value (giving the IS, AP
and EPN values).

[paulus@ozlabs.org - adapted to having the partition table in guest
memory, added the H_TLB_INVALIDATE implementation, removed tlbie
instruction emulation, reworded the commit message.]

	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit e3b6b4661527e821ffbe3db83952fdb1e6e47c49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/kvm_book3s.h
#	arch/powerpc/kvm/book3s_hv_nested.c
diff --cc arch/powerpc/include/asm/kvm_book3s.h
index 43f212e38b89,09f8e9ba69bc..000000000000
--- a/arch/powerpc/include/asm/kvm_book3s.h
+++ b/arch/powerpc/include/asm/kvm_book3s.h
@@@ -280,6 -300,14 +280,17 @@@ void kvmhv_vm_nested_init(struct kvm *k
  long kvmhv_set_partition_table(struct kvm_vcpu *vcpu);
  void kvmhv_set_ptbl_entry(unsigned int lpid, u64 dw0, u64 dw1);
  void kvmhv_release_all_nested(struct kvm *kvm);
++<<<<<<< HEAD
++=======
+ long kvmhv_enter_nested_guest(struct kvm_vcpu *vcpu);
+ long kvmhv_do_nested_tlbie(struct kvm_vcpu *vcpu);
+ int kvmhv_run_single_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu,
+ 			  u64 time_limit, unsigned long lpcr);
+ void kvmhv_save_hv_regs(struct kvm_vcpu *vcpu, struct hv_guest_state *hr);
+ void kvmhv_restore_hv_return_state(struct kvm_vcpu *vcpu,
+ 				   struct hv_guest_state *hr);
+ long int kvmhv_nested_page_fault(struct kvm_vcpu *vcpu);
++>>>>>>> e3b6b4661527 (KVM: PPC: Book3S HV: Implement H_TLB_INVALIDATE hcall)
  
  void kvmppc_giveup_fac(struct kvm_vcpu *vcpu, ulong fac);
  
diff --cc arch/powerpc/kvm/book3s_hv_nested.c
index 327826248314,c83c13d707e2..000000000000
--- a/arch/powerpc/kvm/book3s_hv_nested.c
+++ b/arch/powerpc/kvm/book3s_hv_nested.c
@@@ -239,8 -481,19 +239,8 @@@ void kvmhv_release_all_nested(struct kv
  }
  
  /* caller must hold gp->tlb_lock */
- void kvmhv_flush_nested(struct kvm_nested_guest *gp)
+ static void kvmhv_flush_nested(struct kvm_nested_guest *gp)
  {
 -	struct kvm *kvm = gp->l1_host;
 -
 -	spin_lock(&kvm->mmu_lock);
 -	kvmppc_free_pgtable_radix(kvm, gp->shadow_pgtable, gp->shadow_lpid);
 -	spin_unlock(&kvm->mmu_lock);
 -	radix__flush_tlb_lpid(gp->shadow_lpid);
  	kvmhv_update_ptbl_cache(gp);
  	if (gp->l1_gr_to_hr == 0)
  		kvmhv_remove_nested(gp);
@@@ -299,3 -552,636 +299,639 @@@ void kvmhv_put_nested(struct kvm_nested
  	if (ref == 0)
  		kvmhv_release_nested(gp);
  }
++<<<<<<< HEAD
++=======
+ 
+ static struct kvm_nested_guest *kvmhv_find_nested(struct kvm *kvm, int lpid)
+ {
+ 	if (lpid > kvm->arch.max_nested_lpid)
+ 		return NULL;
+ 	return kvm->arch.nested_guests[lpid];
+ }
+ 
+ static inline bool kvmhv_n_rmap_is_equal(u64 rmap_1, u64 rmap_2)
+ {
+ 	return !((rmap_1 ^ rmap_2) & (RMAP_NESTED_LPID_MASK |
+ 				       RMAP_NESTED_GPA_MASK));
+ }
+ 
+ void kvmhv_insert_nest_rmap(struct kvm *kvm, unsigned long *rmapp,
+ 			    struct rmap_nested **n_rmap)
+ {
+ 	struct llist_node *entry = ((struct llist_head *) rmapp)->first;
+ 	struct rmap_nested *cursor;
+ 	u64 rmap, new_rmap = (*n_rmap)->rmap;
+ 
+ 	/* Are there any existing entries? */
+ 	if (!(*rmapp)) {
+ 		/* No -> use the rmap as a single entry */
+ 		*rmapp = new_rmap | RMAP_NESTED_IS_SINGLE_ENTRY;
+ 		return;
+ 	}
+ 
+ 	/* Do any entries match what we're trying to insert? */
+ 	for_each_nest_rmap_safe(cursor, entry, &rmap) {
+ 		if (kvmhv_n_rmap_is_equal(rmap, new_rmap))
+ 			return;
+ 	}
+ 
+ 	/* Do we need to create a list or just add the new entry? */
+ 	rmap = *rmapp;
+ 	if (rmap & RMAP_NESTED_IS_SINGLE_ENTRY) /* Not previously a list */
+ 		*rmapp = 0UL;
+ 	llist_add(&((*n_rmap)->list), (struct llist_head *) rmapp);
+ 	if (rmap & RMAP_NESTED_IS_SINGLE_ENTRY) /* Not previously a list */
+ 		(*n_rmap)->list.next = (struct llist_node *) rmap;
+ 
+ 	/* Set NULL so not freed by caller */
+ 	*n_rmap = NULL;
+ }
+ 
+ static void kvmhv_remove_nest_rmap(struct kvm *kvm, u64 n_rmap,
+ 				   unsigned long hpa, unsigned long mask)
+ {
+ 	struct kvm_nested_guest *gp;
+ 	unsigned long gpa;
+ 	unsigned int shift, lpid;
+ 	pte_t *ptep;
+ 
+ 	gpa = n_rmap & RMAP_NESTED_GPA_MASK;
+ 	lpid = (n_rmap & RMAP_NESTED_LPID_MASK) >> RMAP_NESTED_LPID_SHIFT;
+ 	gp = kvmhv_find_nested(kvm, lpid);
+ 	if (!gp)
+ 		return;
+ 
+ 	/* Find and invalidate the pte */
+ 	ptep = __find_linux_pte(gp->shadow_pgtable, gpa, NULL, &shift);
+ 	/* Don't spuriously invalidate ptes if the pfn has changed */
+ 	if (ptep && pte_present(*ptep) && ((pte_val(*ptep) & mask) == hpa))
+ 		kvmppc_unmap_pte(kvm, ptep, gpa, shift, NULL, gp->shadow_lpid);
+ }
+ 
+ static void kvmhv_remove_nest_rmap_list(struct kvm *kvm, unsigned long *rmapp,
+ 					unsigned long hpa, unsigned long mask)
+ {
+ 	struct llist_node *entry = llist_del_all((struct llist_head *) rmapp);
+ 	struct rmap_nested *cursor;
+ 	unsigned long rmap;
+ 
+ 	for_each_nest_rmap_safe(cursor, entry, &rmap) {
+ 		kvmhv_remove_nest_rmap(kvm, rmap, hpa, mask);
+ 		kfree(cursor);
+ 	}
+ }
+ 
+ /* called with kvm->mmu_lock held */
+ void kvmhv_remove_nest_rmap_range(struct kvm *kvm,
+ 				  struct kvm_memory_slot *memslot,
+ 				  unsigned long gpa, unsigned long hpa,
+ 				  unsigned long nbytes)
+ {
+ 	unsigned long gfn, end_gfn;
+ 	unsigned long addr_mask;
+ 
+ 	if (!memslot)
+ 		return;
+ 	gfn = (gpa >> PAGE_SHIFT) - memslot->base_gfn;
+ 	end_gfn = gfn + (nbytes >> PAGE_SHIFT);
+ 
+ 	addr_mask = PTE_RPN_MASK & ~(nbytes - 1);
+ 	hpa &= addr_mask;
+ 
+ 	for (; gfn < end_gfn; gfn++) {
+ 		unsigned long *rmap = &memslot->arch.rmap[gfn];
+ 		kvmhv_remove_nest_rmap_list(kvm, rmap, hpa, addr_mask);
+ 	}
+ }
+ 
+ static void kvmhv_free_memslot_nest_rmap(struct kvm_memory_slot *free)
+ {
+ 	unsigned long page;
+ 
+ 	for (page = 0; page < free->npages; page++) {
+ 		unsigned long rmap, *rmapp = &free->arch.rmap[page];
+ 		struct rmap_nested *cursor;
+ 		struct llist_node *entry;
+ 
+ 		entry = llist_del_all((struct llist_head *) rmapp);
+ 		for_each_nest_rmap_safe(cursor, entry, &rmap)
+ 			kfree(cursor);
+ 	}
+ }
+ 
+ static bool kvmhv_invalidate_shadow_pte(struct kvm_vcpu *vcpu,
+ 					struct kvm_nested_guest *gp,
+ 					long gpa, int *shift_ret)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	bool ret = false;
+ 	pte_t *ptep;
+ 	int shift;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	ptep = __find_linux_pte(gp->shadow_pgtable, gpa, NULL, &shift);
+ 	if (!shift)
+ 		shift = PAGE_SHIFT;
+ 	if (ptep && pte_present(*ptep)) {
+ 		kvmppc_unmap_pte(kvm, ptep, gpa, shift, NULL, gp->shadow_lpid);
+ 		ret = true;
+ 	}
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	if (shift_ret)
+ 		*shift_ret = shift;
+ 	return ret;
+ }
+ 
+ static inline int get_ric(unsigned int instr)
+ {
+ 	return (instr >> 18) & 0x3;
+ }
+ 
+ static inline int get_prs(unsigned int instr)
+ {
+ 	return (instr >> 17) & 0x1;
+ }
+ 
+ static inline int get_r(unsigned int instr)
+ {
+ 	return (instr >> 16) & 0x1;
+ }
+ 
+ static inline int get_lpid(unsigned long r_val)
+ {
+ 	return r_val & 0xffffffff;
+ }
+ 
+ static inline int get_is(unsigned long r_val)
+ {
+ 	return (r_val >> 10) & 0x3;
+ }
+ 
+ static inline int get_ap(unsigned long r_val)
+ {
+ 	return (r_val >> 5) & 0x7;
+ }
+ 
+ static inline long get_epn(unsigned long r_val)
+ {
+ 	return r_val >> 12;
+ }
+ 
+ static int kvmhv_emulate_tlbie_tlb_addr(struct kvm_vcpu *vcpu, int lpid,
+ 					int ap, long epn)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_nested_guest *gp;
+ 	long npages;
+ 	int shift, shadow_shift;
+ 	unsigned long addr;
+ 
+ 	shift = ap_to_shift(ap);
+ 	addr = epn << 12;
+ 	if (shift < 0)
+ 		/* Invalid ap encoding */
+ 		return -EINVAL;
+ 
+ 	addr &= ~((1UL << shift) - 1);
+ 	npages = 1UL << (shift - PAGE_SHIFT);
+ 
+ 	gp = kvmhv_get_nested(kvm, lpid, false);
+ 	if (!gp) /* No such guest -> nothing to do */
+ 		return 0;
+ 	mutex_lock(&gp->tlb_lock);
+ 
+ 	/* There may be more than one host page backing this single guest pte */
+ 	do {
+ 		kvmhv_invalidate_shadow_pte(vcpu, gp, addr, &shadow_shift);
+ 
+ 		npages -= 1UL << (shadow_shift - PAGE_SHIFT);
+ 		addr += 1UL << shadow_shift;
+ 	} while (npages > 0);
+ 
+ 	mutex_unlock(&gp->tlb_lock);
+ 	kvmhv_put_nested(gp);
+ 	return 0;
+ }
+ 
+ static void kvmhv_emulate_tlbie_lpid(struct kvm_vcpu *vcpu,
+ 				     struct kvm_nested_guest *gp, int ric)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 
+ 	mutex_lock(&gp->tlb_lock);
+ 	switch (ric) {
+ 	case 0:
+ 		/* Invalidate TLB */
+ 		spin_lock(&kvm->mmu_lock);
+ 		kvmppc_free_pgtable_radix(kvm, gp->shadow_pgtable,
+ 					  gp->shadow_lpid);
+ 		radix__flush_tlb_lpid(gp->shadow_lpid);
+ 		spin_unlock(&kvm->mmu_lock);
+ 		break;
+ 	case 1:
+ 		/*
+ 		 * Invalidate PWC
+ 		 * We don't cache this -> nothing to do
+ 		 */
+ 		break;
+ 	case 2:
+ 		/* Invalidate TLB, PWC and caching of partition table entries */
+ 		kvmhv_flush_nested(gp);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	mutex_unlock(&gp->tlb_lock);
+ }
+ 
+ static void kvmhv_emulate_tlbie_all_lpid(struct kvm_vcpu *vcpu, int ric)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_nested_guest *gp;
+ 	int i;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	for (i = 0; i <= kvm->arch.max_nested_lpid; i++) {
+ 		gp = kvm->arch.nested_guests[i];
+ 		if (gp) {
+ 			spin_unlock(&kvm->mmu_lock);
+ 			kvmhv_emulate_tlbie_lpid(vcpu, gp, ric);
+ 			spin_lock(&kvm->mmu_lock);
+ 		}
+ 	}
+ 	spin_unlock(&kvm->mmu_lock);
+ }
+ 
+ static int kvmhv_emulate_priv_tlbie(struct kvm_vcpu *vcpu, unsigned int instr,
+ 				    unsigned long rsval, unsigned long rbval)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_nested_guest *gp;
+ 	int r, ric, prs, is, ap;
+ 	int lpid;
+ 	long epn;
+ 	int ret = 0;
+ 
+ 	ric = get_ric(instr);
+ 	prs = get_prs(instr);
+ 	r = get_r(instr);
+ 	lpid = get_lpid(rsval);
+ 	is = get_is(rbval);
+ 
+ 	/*
+ 	 * These cases are invalid and are not handled:
+ 	 * r   != 1 -> Only radix supported
+ 	 * prs == 1 -> Not HV privileged
+ 	 * ric == 3 -> No cluster bombs for radix
+ 	 * is  == 1 -> Partition scoped translations not associated with pid
+ 	 * (!is) && (ric == 1 || ric == 2) -> Not supported by ISA
+ 	 */
+ 	if ((!r) || (prs) || (ric == 3) || (is == 1) ||
+ 	    ((!is) && (ric == 1 || ric == 2)))
+ 		return -EINVAL;
+ 
+ 	switch (is) {
+ 	case 0:
+ 		/*
+ 		 * We know ric == 0
+ 		 * Invalidate TLB for a given target address
+ 		 */
+ 		epn = get_epn(rbval);
+ 		ap = get_ap(rbval);
+ 		ret = kvmhv_emulate_tlbie_tlb_addr(vcpu, lpid, ap, epn);
+ 		break;
+ 	case 2:
+ 		/* Invalidate matching LPID */
+ 		gp = kvmhv_get_nested(kvm, lpid, false);
+ 		if (gp) {
+ 			kvmhv_emulate_tlbie_lpid(vcpu, gp, ric);
+ 			kvmhv_put_nested(gp);
+ 		}
+ 		break;
+ 	case 3:
+ 		/* Invalidate ALL LPIDs */
+ 		kvmhv_emulate_tlbie_all_lpid(vcpu, ric);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * This handles the H_TLB_INVALIDATE hcall.
+  * Parameters are (r4) tlbie instruction code, (r5) rS contents,
+  * (r6) rB contents.
+  */
+ long kvmhv_do_nested_tlbie(struct kvm_vcpu *vcpu)
+ {
+ 	int ret;
+ 
+ 	ret = kvmhv_emulate_priv_tlbie(vcpu, kvmppc_get_gpr(vcpu, 4),
+ 			kvmppc_get_gpr(vcpu, 5), kvmppc_get_gpr(vcpu, 6));
+ 	if (ret)
+ 		return H_PARAMETER;
+ 	return H_SUCCESS;
+ }
+ 
+ /* Used to convert a nested guest real address to a L1 guest real address */
+ static int kvmhv_translate_addr_nested(struct kvm_vcpu *vcpu,
+ 				       struct kvm_nested_guest *gp,
+ 				       unsigned long n_gpa, unsigned long dsisr,
+ 				       struct kvmppc_pte *gpte_p)
+ {
+ 	u64 fault_addr, flags = dsisr & DSISR_ISSTORE;
+ 	int ret;
+ 
+ 	ret = kvmppc_mmu_walk_radix_tree(vcpu, n_gpa, gpte_p, gp->l1_gr_to_hr,
+ 					 &fault_addr);
+ 
+ 	if (ret) {
+ 		/* We didn't find a pte */
+ 		if (ret == -EINVAL) {
+ 			/* Unsupported mmu config */
+ 			flags |= DSISR_UNSUPP_MMU;
+ 		} else if (ret == -ENOENT) {
+ 			/* No translation found */
+ 			flags |= DSISR_NOHPTE;
+ 		} else if (ret == -EFAULT) {
+ 			/* Couldn't access L1 real address */
+ 			flags |= DSISR_PRTABLE_FAULT;
+ 			vcpu->arch.fault_gpa = fault_addr;
+ 		} else {
+ 			/* Unknown error */
+ 			return ret;
+ 		}
+ 		goto forward_to_l1;
+ 	} else {
+ 		/* We found a pte -> check permissions */
+ 		if (dsisr & DSISR_ISSTORE) {
+ 			/* Can we write? */
+ 			if (!gpte_p->may_write) {
+ 				flags |= DSISR_PROTFAULT;
+ 				goto forward_to_l1;
+ 			}
+ 		} else if (vcpu->arch.trap == BOOK3S_INTERRUPT_H_INST_STORAGE) {
+ 			/* Can we execute? */
+ 			if (!gpte_p->may_execute) {
+ 				flags |= SRR1_ISI_N_OR_G;
+ 				goto forward_to_l1;
+ 			}
+ 		} else {
+ 			/* Can we read? */
+ 			if (!gpte_p->may_read && !gpte_p->may_write) {
+ 				flags |= DSISR_PROTFAULT;
+ 				goto forward_to_l1;
+ 			}
+ 		}
+ 	}
+ 
+ 	return 0;
+ 
+ forward_to_l1:
+ 	vcpu->arch.fault_dsisr = flags;
+ 	if (vcpu->arch.trap == BOOK3S_INTERRUPT_H_INST_STORAGE) {
+ 		vcpu->arch.shregs.msr &= ~0x783f0000ul;
+ 		vcpu->arch.shregs.msr |= flags;
+ 	}
+ 	return RESUME_HOST;
+ }
+ 
+ static long kvmhv_handle_nested_set_rc(struct kvm_vcpu *vcpu,
+ 				       struct kvm_nested_guest *gp,
+ 				       unsigned long n_gpa,
+ 				       struct kvmppc_pte gpte,
+ 				       unsigned long dsisr)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	bool writing = !!(dsisr & DSISR_ISSTORE);
+ 	u64 pgflags;
+ 	bool ret;
+ 
+ 	/* Are the rc bits set in the L1 partition scoped pte? */
+ 	pgflags = _PAGE_ACCESSED;
+ 	if (writing)
+ 		pgflags |= _PAGE_DIRTY;
+ 	if (pgflags & ~gpte.rc)
+ 		return RESUME_HOST;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	/* Set the rc bit in the pte of our (L0) pgtable for the L1 guest */
+ 	ret = kvmppc_hv_handle_set_rc(kvm, kvm->arch.pgtable, writing,
+ 				     gpte.raddr, kvm->arch.lpid);
+ 	spin_unlock(&kvm->mmu_lock);
+ 	if (!ret)
+ 		return -EINVAL;
+ 
+ 	/* Set the rc bit in the pte of the shadow_pgtable for the nest guest */
+ 	ret = kvmppc_hv_handle_set_rc(kvm, gp->shadow_pgtable, writing, n_gpa,
+ 				      gp->shadow_lpid);
+ 	if (!ret)
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ static inline int kvmppc_radix_level_to_shift(int level)
+ {
+ 	switch (level) {
+ 	case 2:
+ 		return PUD_SHIFT;
+ 	case 1:
+ 		return PMD_SHIFT;
+ 	default:
+ 		return PAGE_SHIFT;
+ 	}
+ }
+ 
+ static inline int kvmppc_radix_shift_to_level(int shift)
+ {
+ 	if (shift == PUD_SHIFT)
+ 		return 2;
+ 	if (shift == PMD_SHIFT)
+ 		return 1;
+ 	if (shift == PAGE_SHIFT)
+ 		return 0;
+ 	WARN_ON_ONCE(1);
+ 	return 0;
+ }
+ 
+ /* called with gp->tlb_lock held */
+ static long int __kvmhv_nested_page_fault(struct kvm_vcpu *vcpu,
+ 					  struct kvm_nested_guest *gp)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_memory_slot *memslot;
+ 	struct rmap_nested *n_rmap;
+ 	struct kvmppc_pte gpte;
+ 	pte_t pte, *pte_p;
+ 	unsigned long mmu_seq;
+ 	unsigned long dsisr = vcpu->arch.fault_dsisr;
+ 	unsigned long ea = vcpu->arch.fault_dar;
+ 	unsigned long *rmapp;
+ 	unsigned long n_gpa, gpa, gfn, perm = 0UL;
+ 	unsigned int shift, l1_shift, level;
+ 	bool writing = !!(dsisr & DSISR_ISSTORE);
+ 	bool kvm_ro = false;
+ 	long int ret;
+ 
+ 	if (!gp->l1_gr_to_hr) {
+ 		kvmhv_update_ptbl_cache(gp);
+ 		if (!gp->l1_gr_to_hr)
+ 			return RESUME_HOST;
+ 	}
+ 
+ 	/* Convert the nested guest real address into a L1 guest real address */
+ 
+ 	n_gpa = vcpu->arch.fault_gpa & ~0xF000000000000FFFULL;
+ 	if (!(dsisr & DSISR_PRTABLE_FAULT))
+ 		n_gpa |= ea & 0xFFF;
+ 	ret = kvmhv_translate_addr_nested(vcpu, gp, n_gpa, dsisr, &gpte);
+ 
+ 	/*
+ 	 * If the hardware found a translation but we don't now have a usable
+ 	 * translation in the l1 partition-scoped tree, remove the shadow pte
+ 	 * and let the guest retry.
+ 	 */
+ 	if (ret == RESUME_HOST &&
+ 	    (dsisr & (DSISR_PROTFAULT | DSISR_BADACCESS | DSISR_NOEXEC_OR_G |
+ 		      DSISR_BAD_COPYPASTE)))
+ 		goto inval;
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Failed to set the reference/change bits */
+ 	if (dsisr & DSISR_SET_RC) {
+ 		ret = kvmhv_handle_nested_set_rc(vcpu, gp, n_gpa, gpte, dsisr);
+ 		if (ret == RESUME_HOST)
+ 			return ret;
+ 		if (ret)
+ 			goto inval;
+ 		dsisr &= ~DSISR_SET_RC;
+ 		if (!(dsisr & (DSISR_BAD_FAULT_64S | DSISR_NOHPTE |
+ 			       DSISR_PROTFAULT)))
+ 			return RESUME_GUEST;
+ 	}
+ 
+ 	/*
+ 	 * We took an HISI or HDSI while we were running a nested guest which
+ 	 * means we have no partition scoped translation for that. This means
+ 	 * we need to insert a pte for the mapping into our shadow_pgtable.
+ 	 */
+ 
+ 	l1_shift = gpte.page_shift;
+ 	if (l1_shift < PAGE_SHIFT) {
+ 		/* We don't support l1 using a page size smaller than our own */
+ 		pr_err("KVM: L1 guest page shift (%d) less than our own (%d)\n",
+ 			l1_shift, PAGE_SHIFT);
+ 		return -EINVAL;
+ 	}
+ 	gpa = gpte.raddr;
+ 	gfn = gpa >> PAGE_SHIFT;
+ 
+ 	/* 1. Get the corresponding host memslot */
+ 
+ 	memslot = gfn_to_memslot(kvm, gfn);
+ 	if (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID)) {
+ 		if (dsisr & (DSISR_PRTABLE_FAULT | DSISR_BADACCESS)) {
+ 			/* unusual error -> reflect to the guest as a DSI */
+ 			kvmppc_core_queue_data_storage(vcpu, ea, dsisr);
+ 			return RESUME_GUEST;
+ 		}
+ 		/* passthrough of emulated MMIO case... */
+ 		pr_err("emulated MMIO passthrough?\n");
+ 		return -EINVAL;
+ 	}
+ 	if (memslot->flags & KVM_MEM_READONLY) {
+ 		if (writing) {
+ 			/* Give the guest a DSI */
+ 			kvmppc_core_queue_data_storage(vcpu, ea,
+ 					DSISR_ISSTORE | DSISR_PROTFAULT);
+ 			return RESUME_GUEST;
+ 		}
+ 		kvm_ro = true;
+ 	}
+ 
+ 	/* 2. Find the host pte for this L1 guest real address */
+ 
+ 	/* Used to check for invalidations in progress */
+ 	mmu_seq = kvm->mmu_notifier_seq;
+ 	smp_rmb();
+ 
+ 	/* See if can find translation in our partition scoped tables for L1 */
+ 	pte = __pte(0);
+ 	spin_lock(&kvm->mmu_lock);
+ 	pte_p = __find_linux_pte(kvm->arch.pgtable, gpa, NULL, &shift);
+ 	if (!shift)
+ 		shift = PAGE_SHIFT;
+ 	if (pte_p)
+ 		pte = *pte_p;
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	if (!pte_present(pte) || (writing && !(pte_val(pte) & _PAGE_WRITE))) {
+ 		/* No suitable pte found -> try to insert a mapping */
+ 		ret = kvmppc_book3s_instantiate_page(vcpu, gpa, memslot,
+ 					writing, kvm_ro, &pte, &level);
+ 		if (ret == -EAGAIN)
+ 			return RESUME_GUEST;
+ 		else if (ret)
+ 			return ret;
+ 		shift = kvmppc_radix_level_to_shift(level);
+ 	}
+ 
+ 	/* 3. Compute the pte we need to insert for nest_gpa -> host r_addr */
+ 
+ 	/* The permissions is the combination of the host and l1 guest ptes */
+ 	perm |= gpte.may_read ? 0UL : _PAGE_READ;
+ 	perm |= gpte.may_write ? 0UL : _PAGE_WRITE;
+ 	perm |= gpte.may_execute ? 0UL : _PAGE_EXEC;
+ 	pte = __pte(pte_val(pte) & ~perm);
+ 
+ 	/* What size pte can we insert? */
+ 	if (shift > l1_shift) {
+ 		u64 mask;
+ 		unsigned int actual_shift = PAGE_SHIFT;
+ 		if (PMD_SHIFT < l1_shift)
+ 			actual_shift = PMD_SHIFT;
+ 		mask = (1UL << shift) - (1UL << actual_shift);
+ 		pte = __pte(pte_val(pte) | (gpa & mask));
+ 		shift = actual_shift;
+ 	}
+ 	level = kvmppc_radix_shift_to_level(shift);
+ 	n_gpa &= ~((1UL << shift) - 1);
+ 
+ 	/* 4. Insert the pte into our shadow_pgtable */
+ 
+ 	n_rmap = kzalloc(sizeof(*n_rmap), GFP_KERNEL);
+ 	if (!n_rmap)
+ 		return RESUME_GUEST; /* Let the guest try again */
+ 	n_rmap->rmap = (n_gpa & RMAP_NESTED_GPA_MASK) |
+ 		(((unsigned long) gp->l1_lpid) << RMAP_NESTED_LPID_SHIFT);
+ 	rmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];
+ 	ret = kvmppc_create_pte(kvm, gp->shadow_pgtable, pte, n_gpa, level,
+ 				mmu_seq, gp->shadow_lpid, rmapp, &n_rmap);
+ 	if (n_rmap)
+ 		kfree(n_rmap);
+ 	if (ret == -EAGAIN)
+ 		ret = RESUME_GUEST;	/* Let the guest try again */
+ 
+ 	return ret;
+ 
+  inval:
+ 	kvmhv_invalidate_shadow_pte(vcpu, gp, n_gpa, NULL);
+ 	return RESUME_GUEST;
+ }
+ 
+ long int kvmhv_nested_page_fault(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_nested_guest *gp = vcpu->arch.nested;
+ 	long int ret;
+ 
+ 	mutex_lock(&gp->tlb_lock);
+ 	ret = __kvmhv_nested_page_fault(vcpu, gp);
+ 	mutex_unlock(&gp->tlb_lock);
+ 	return ret;
+ }
++>>>>>>> e3b6b4661527 (KVM: PPC: Book3S HV: Implement H_TLB_INVALIDATE hcall)
diff --git a/arch/powerpc/include/asm/book3s/64/mmu-hash.h b/arch/powerpc/include/asm/book3s/64/mmu-hash.h
index 50ed64fba4ae..d2b3cea55d6a 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu-hash.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu-hash.h
@@ -203,6 +203,18 @@ static inline unsigned int mmu_psize_to_shift(unsigned int mmu_psize)
 	BUG();
 }
 
+static inline unsigned int ap_to_shift(unsigned long ap)
+{
+	int psize;
+
+	for (psize = 0; psize < MMU_PAGE_COUNT; psize++) {
+		if (mmu_psize_defs[psize].ap == ap)
+			return mmu_psize_defs[psize].shift;
+	}
+
+	return -1;
+}
+
 static inline unsigned long get_sllp_encoding(int psize)
 {
 	unsigned long sllp;
* Unmerged path arch/powerpc/include/asm/kvm_book3s.h
diff --git a/arch/powerpc/include/asm/ppc-opcode.h b/arch/powerpc/include/asm/ppc-opcode.h
index 4436887bc415..a686febbe601 100644
--- a/arch/powerpc/include/asm/ppc-opcode.h
+++ b/arch/powerpc/include/asm/ppc-opcode.h
@@ -105,6 +105,7 @@
 #define OP_31_XOP_LHZUX     311
 #define OP_31_XOP_MSGSNDP   142
 #define OP_31_XOP_MSGCLRP   174
+#define OP_31_XOP_TLBIE     306
 #define OP_31_XOP_MFSPR     339
 #define OP_31_XOP_LWAX      341
 #define OP_31_XOP_LHAX      343
diff --git a/arch/powerpc/kvm/book3s_emulate.c b/arch/powerpc/kvm/book3s_emulate.c
index 2654df220d05..8c7e933e942e 100644
--- a/arch/powerpc/kvm/book3s_emulate.c
+++ b/arch/powerpc/kvm/book3s_emulate.c
@@ -36,7 +36,6 @@
 #define OP_31_XOP_MTSR		210
 #define OP_31_XOP_MTSRIN	242
 #define OP_31_XOP_TLBIEL	274
-#define OP_31_XOP_TLBIE		306
 /* Opcode is officially reserved, reuse it as sc 1 when sc 1 doesn't trap */
 #define OP_31_XOP_FAKE_SC1	308
 #define OP_31_XOP_SLBMTE	402
diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
index de29c24e5f3d..ec5cc0234dd8 100644
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@ -949,6 +949,9 @@ int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu)
 		break;
 	case H_TLB_INVALIDATE:
 		ret = H_FUNCTION;
+		if (!vcpu->kvm->arch.nested_enable)
+			break;
+		ret = kvmhv_do_nested_tlbie(vcpu);
 		break;
 
 	default:
* Unmerged path arch/powerpc/kvm/book3s_hv_nested.c
