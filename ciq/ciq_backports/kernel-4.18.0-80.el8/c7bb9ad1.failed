block: get rid of q->softirq_done_fn()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit c7bb9ad1744ea14e61e5fff99ee5282709b0c9d9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/c7bb9ad1.failed

With the legacy path gone, all we do is funnel it through the
mq_ops->complete() operation.

	Tested-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c7bb9ad1744ea14e61e5fff99ee5282709b0c9d9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-settings.c
#	include/linux/blkdev.h
diff --cc block/blk-settings.c
index 82b31fb2888c,cca83590a1dc..000000000000
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@@ -20,46 -20,6 +20,49 @@@ EXPORT_SYMBOL(blk_max_low_pfn)
  
  unsigned long blk_max_pfn;
  
++<<<<<<< HEAD
 +/**
 + * blk_queue_prep_rq - set a prepare_request function for queue
 + * @q:		queue
 + * @pfn:	prepare_request function
 + *
 + * It's possible for a queue to register a prepare_request callback which
 + * is invoked before the request is handed to the request_fn. The goal of
 + * the function is to prepare a request for I/O, it can be used to build a
 + * cdb from the request data for instance.
 + *
 + */
 +void blk_queue_prep_rq(struct request_queue *q, prep_rq_fn *pfn)
 +{
 +	q->prep_rq_fn = pfn;
 +}
 +EXPORT_SYMBOL(blk_queue_prep_rq);
 +
 +/**
 + * blk_queue_unprep_rq - set an unprepare_request function for queue
 + * @q:		queue
 + * @ufn:	unprepare_request function
 + *
 + * It's possible for a queue to register an unprepare_request callback
 + * which is invoked before the request is finally completed. The goal
 + * of the function is to deallocate any data that was allocated in the
 + * prepare_request callback.
 + *
 + */
 +void blk_queue_unprep_rq(struct request_queue *q, unprep_rq_fn *ufn)
 +{
 +	q->unprep_rq_fn = ufn;
 +}
 +EXPORT_SYMBOL(blk_queue_unprep_rq);
 +
 +void blk_queue_softirq_done(struct request_queue *q, softirq_done_fn *fn)
 +{
 +	q->softirq_done_fn = fn;
 +}
 +EXPORT_SYMBOL(blk_queue_softirq_done);
 +
++=======
++>>>>>>> c7bb9ad1744e (block: get rid of q->softirq_done_fn())
  void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
  {
  	q->rq_timeout = timeout;
diff --cc include/linux/blkdev.h
index 0f09a73a4b12,d4104844d6bb..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -309,18 -286,11 +309,17 @@@ static inline unsigned short req_get_io
  
  struct blk_queue_ctx;
  
 +typedef void (request_fn_proc) (struct request_queue *q);
  typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
  typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
 +typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 +typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
  
  struct bio_vec;
- typedef void (softirq_done_fn)(struct request *);
  typedef int (dma_drain_needed_fn)(struct request *);
 +typedef int (bsg_job_fn) (struct bsg_job *);
 +typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
 +typedef void (exit_rq_fn)(struct request_queue *, struct request *);
  
  enum blk_eh_timer_return {
  	BLK_EH_DONE,		/* drivers has completed the command */
@@@ -440,28 -404,9 +439,31 @@@ struct request_queue 
  	struct blk_queue_stats	*stats;
  	struct rq_qos		*rq_qos;
  
 +	/*
 +	 * If blkcg is not used, @q->root_rl serves all requests.  If blkcg
 +	 * is used, root blkg allocates from @q->root_rl and all other
 +	 * blkgs from their own blkg->rl.  Which one to use should be
 +	 * determined using bio_request_list().
 +	 */
 +	struct request_list	root_rl;
 +
 +	request_fn_proc		*request_fn;
  	make_request_fn		*make_request_fn;
  	poll_q_fn		*poll_fn;
++<<<<<<< HEAD
 +	prep_rq_fn		*prep_rq_fn;
 +	unprep_rq_fn		*unprep_rq_fn;
 +	softirq_done_fn		*softirq_done_fn;
 +	rq_timed_out_fn		*rq_timed_out_fn;
++=======
++>>>>>>> c7bb9ad1744e (block: get rid of q->softirq_done_fn())
  	dma_drain_needed_fn	*dma_drain_needed;
 +	/* Called just after a request is allocated */
 +	init_rq_fn		*init_rq_fn;
 +	/* Called just before a request is freed */
 +	exit_rq_fn		*exit_rq_fn;
 +	/* Called from inside blk_get_request() */
 +	void (*initialize_rq_fn)(struct request *rq);
  
  	const struct blk_mq_ops	*mq_ops;
  
@@@ -1220,12 -1109,8 +1222,15 @@@ extern int blk_queue_dma_drain(struct r
  			       void *buf, unsigned int size);
  extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
  extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 +extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 +extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
  extern void blk_queue_dma_alignment(struct request_queue *, int);
  extern void blk_queue_update_dma_alignment(struct request_queue *, int);
++<<<<<<< HEAD
 +extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 +extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
++=======
++>>>>>>> c7bb9ad1744e (block: get rid of q->softirq_done_fn())
  extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
  extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
  extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
diff --git a/block/blk-mq.c b/block/blk-mq.c
index bd5c1e40de64..d86e8841d8db 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -550,13 +550,15 @@ EXPORT_SYMBOL(blk_mq_end_request);
 static void __blk_mq_complete_request_remote(void *data)
 {
 	struct request *rq = data;
+	struct request_queue *q = rq->q;
 
-	rq->q->softirq_done_fn(rq);
+	q->mq_ops->complete(rq);
 }
 
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
+	struct request_queue *q = rq->q;
 	bool shared = false;
 	int cpu;
 
@@ -572,18 +574,18 @@ static void __blk_mq_complete_request(struct request *rq)
 	 * So complete IO reqeust in softirq context in case of single queue
 	 * for not degrading IO performance by irqsoff latency.
 	 */
-	if (rq->q->nr_hw_queues == 1) {
+	if (q->nr_hw_queues == 1) {
 		__blk_complete_request(rq);
 		return;
 	}
 
-	if (!test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags)) {
-		rq->q->softirq_done_fn(rq);
+	if (!test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
+		q->mq_ops->complete(rq);
 		return;
 	}
 
 	cpu = get_cpu();
-	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))
+	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
 		shared = cpus_share_cache(cpu, ctx->cpu);
 
 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
@@ -592,7 +594,7 @@ static void __blk_mq_complete_request(struct request *rq)
 		rq->csd.flags = 0;
 		smp_call_function_single_async(ctx->cpu, &rq->csd);
 	} else {
-		rq->q->softirq_done_fn(rq);
+		q->mq_ops->complete(rq);
 	}
 	put_cpu();
 }
@@ -2711,9 +2713,6 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	 */
 	q->poll_nsec = -1;
 
-	if (set->ops->complete)
-		blk_queue_softirq_done(q, set->ops->complete);
-
 	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
 	blk_mq_add_queue_tag_set(set, q);
 	blk_mq_map_swqueue(q);
* Unmerged path block/blk-settings.c
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 8ca0f6caf174..727d64436ec4 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -34,7 +34,7 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 
 		rq = list_entry(local_list.next, struct request, ipi_list);
 		list_del_init(&rq->ipi_list);
-		rq->q->softirq_done_fn(rq);
+		rq->q->mq_ops->complete(rq);
 	}
 }
 
@@ -102,7 +102,7 @@ void __blk_complete_request(struct request *req)
 	unsigned long flags;
 	bool shared = false;
 
-	BUG_ON(!q->softirq_done_fn);
+	BUG_ON(!q->mq_ops->complete);
 
 	local_irq_save(flags);
 	cpu = smp_processor_id();
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5c8418ebbfd6..9dd574e5436a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -115,6 +115,7 @@ typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
 typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 typedef bool (busy_fn)(struct request_queue *);
+typedef void (complete_fn)(struct request *);
 
 
 struct blk_mq_ops {
@@ -142,7 +143,7 @@ struct blk_mq_ops {
 	 */
 	poll_fn			*poll;
 
-	softirq_done_fn		*complete;
+	complete_fn		*complete;
 
 	/*
 	 * Called when the block layer side of a hardware queue has been
* Unmerged path include/linux/blkdev.h
