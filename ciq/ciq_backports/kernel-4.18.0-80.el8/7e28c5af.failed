rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit 7e28c5af4ef6b539334aa5de40feca0c041c94df
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/7e28c5af.failed

The ->rcu_qs_ctr counter was intended to allow providing a lightweight
report of a quiescent state to all RCU flavors.  But now that there is
only one flavor of RCU in any one running kernel, there is no point in
having this feature.  This commit therefore removes the ->rcu_qs_ctr
field from the rcu_dynticks structure and the ->rcu_qs_ctr_snap field
from the rcu_data structure.  This results in the "rqc" option to the
rcu_fqs trace event no longer being used, so this commit also removes the
"rqc" description from the header comment.

While in the neighborhood, this commit also causes the forward-progress
request .rcu_need_heavy_qs be set one jiffies_till_sched_qs interval
later in the grace period than the first setting of .rcu_urgent_qs.

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit 7e28c5af4ef6b539334aa5de40feca0c041c94df)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 19ff444b5603,bc42c600027c..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -1030,28 -1018,9 +1030,31 @@@ static int rcu_implicit_dynticks_qs(str
  		return 1;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Has this CPU encountered a cond_resched() since the beginning
 +	 * of the grace period?  For this to be the case, the CPU has to
 +	 * have noticed the current grace period.  This might not be the
 +	 * case for nohz_full CPUs looping in the kernel.
 +	 */
 +	jtsq = jiffies_till_sched_qs;
 +	ruqp = per_cpu_ptr(&rcu_dynticks.rcu_urgent_qs, rdp->cpu);
 +	if (time_after(jiffies, rdp->rsp->gp_start + jtsq) &&
 +	    READ_ONCE(rdp->rcu_qs_ctr_snap) != per_cpu(rcu_dynticks.rcu_qs_ctr, rdp->cpu) &&
 +	    rcu_seq_current(&rdp->gp_seq) == rnp->gp_seq && !rdp->gpwrap) {
 +		trace_rcu_fqs(rdp->rsp->name, rdp->gp_seq, rdp->cpu, TPS("rqc"));
 +		rcu_gpnum_ovf(rnp, rdp);
 +		return 1;
 +	} else if (time_after(jiffies, rdp->rsp->gp_start + jtsq)) {
 +		/* Load rcu_qs_ctr before store to rcu_urgent_qs. */
 +		smp_store_release(ruqp, true);
 +	}
 +
++=======
++>>>>>>> 7e28c5af4ef6 (rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure)
  	/* If waiting too long on an offline CPU, complain. */
  	if (!(rdp->grpmask & rcu_rnp_online_cpus(rnp)) &&
 -	    time_after(jiffies, rcu_state.gp_start + HZ)) {
 +	    time_after(jiffies, rdp->rsp->gp_start + HZ)) {
  		bool onl;
  		struct rcu_node *rnp1;
  
@@@ -1072,29 -1041,27 +1075,36 @@@
  
  	/*
  	 * A CPU running for an extended time within the kernel can
- 	 * delay RCU grace periods.  When the CPU is in NO_HZ_FULL mode,
- 	 * even context-switching back and forth between a pair of
- 	 * in-kernel CPU-bound tasks cannot advance grace periods.
- 	 * So if the grace period is old enough, make the CPU pay attention.
- 	 * Note that the unsynchronized assignments to the per-CPU
- 	 * rcu_need_heavy_qs variable are safe.  Yes, setting of
- 	 * bits can be lost, but they will be set again on the next
- 	 * force-quiescent-state pass.  So lost bit sets do not result
- 	 * in incorrect behavior, merely in a grace period lasting
- 	 * a few jiffies longer than it might otherwise.  Because
- 	 * there are at most four threads involved, and because the
- 	 * updates are only once every few jiffies, the probability of
- 	 * lossage (and thus of slight grace-period extension) is
- 	 * quite low.
+ 	 * delay RCU grace periods: (1) At age jiffies_till_sched_qs,
+ 	 * set .rcu_urgent_qs, (2) At age 2*jiffies_till_sched_qs, set
+ 	 * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the
+ 	 * unsynchronized assignments to the per-CPU rcu_need_heavy_qs
+ 	 * variable are safe because the assignments are repeated if this
+ 	 * CPU failed to pass through a quiescent state.  This code
+ 	 * also checks .jiffies_resched in case jiffies_till_sched_qs
+ 	 * is set way high.
  	 */
+ 	jtsq = jiffies_till_sched_qs;
+ 	ruqp = per_cpu_ptr(&rcu_dynticks.rcu_urgent_qs, rdp->cpu);
  	rnhqp = &per_cpu(rcu_dynticks.rcu_need_heavy_qs, rdp->cpu);
  	if (!READ_ONCE(*rnhqp) &&
++<<<<<<< HEAD
 +	    (time_after(jiffies, rdp->rsp->gp_start + jtsq) ||
 +	     time_after(jiffies, rdp->rsp->jiffies_resched))) {
 +		WRITE_ONCE(*rnhqp, true);
 +		/* Store rcu_need_heavy_qs before rcu_urgent_qs. */
 +		smp_store_release(ruqp, true);
 +		rdp->rsp->jiffies_resched += jtsq; /* Re-enable beating. */
++=======
+ 	    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||
+ 	     time_after(jiffies, rcu_state.jiffies_resched))) {
+ 		WRITE_ONCE(*rnhqp, true);
+ 		/* Store rcu_need_heavy_qs before rcu_urgent_qs. */
+ 		smp_store_release(ruqp, true);
+ 		rcu_state.jiffies_resched += jtsq; /* Re-enable beating. */
+ 	} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {
+ 		WRITE_ONCE(*ruqp, true);
++>>>>>>> 7e28c5af4ef6 (rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure)
  	}
  
  	/*
@@@ -1103,7 -1070,7 +1113,11 @@@
  	 * see if the CPU is getting hammered with interrupts, but only
  	 * once per grace period, just to keep the IPIs down to a dull roar.
  	 */
++<<<<<<< HEAD
 +	if (jiffies - rdp->rsp->gp_start > rcu_jiffies_till_stall_check() / 2) {
++=======
+ 	if (time_after(jiffies, rcu_state.jiffies_resched)) {
++>>>>>>> 7e28c5af4ef6 (rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure)
  		resched_cpu(rdp->cpu);
  		if (IS_ENABLED(CONFIG_IRQ_WORK) &&
  		    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&
@@@ -1684,10 -1645,9 +1698,9 @@@ static bool __note_gp_changes(struct rc
  		 * set up to detect a quiescent state, otherwise don't
  		 * go looking for one.
  		 */
 -		trace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS("cpustart"));
 +		trace_rcu_grace_period(rsp->name, rnp->gp_seq, TPS("cpustart"));
  		need_gp = !!(rnp->qsmask & rdp->grpmask);
  		rdp->cpu_no_qs.b.norm = need_gp;
- 		rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_dynticks.rcu_qs_ctr);
  		rdp->core_needs_qs = need_gp;
  		zero_cpu_stall_ticks(rdp);
  	}
diff --git a/include/trace/events/rcu.h b/include/trace/events/rcu.h
index a8d07feff6a0..35b08e5fba51 100644
--- a/include/trace/events/rcu.h
+++ b/include/trace/events/rcu.h
@@ -393,9 +393,8 @@ TRACE_EVENT(rcu_quiescent_state_report,
  * Tracepoint for quiescent states detected by force_quiescent_state().
  * These trace events include the type of RCU, the grace-period number
  * that was blocked by the CPU, the CPU itself, and the type of quiescent
- * state, which can be "dti" for dyntick-idle mode, "kick" when kicking
- * a CPU that has been in dyntick-idle mode for too long, or "rqc" if the
- * CPU got a quiescent state via its rcu_qs_ctr.
+ * state, which can be "dti" for dyntick-idle mode or "kick" when kicking
+ * a CPU that has been in dyntick-idle mode for too long.
  */
 TRACE_EVENT(rcu_fqs,
 
* Unmerged path kernel/rcu/tree.c
diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7c6033d71e9d..5eccf15cdcac 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -42,7 +42,6 @@ struct rcu_dynticks {
 	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
-	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	bool all_lazy;		    /* Are all CPU's CBs lazy? */
@@ -189,8 +188,6 @@ struct rcu_data {
 	/* 1) quiescent-state and grace-period handling : */
 	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
 	unsigned long	gp_seq_needed;	/* Track rsp->rcu_gp_seq_needed ctr. */
-	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
-					/*  for rcu_all_qs() invocations. */
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 05c430d4a698..5dfb3de013e3 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -979,9 +979,7 @@ void rcu_all_qs(void)
 		rcu_momentary_dyntick_idle();
 		local_irq_restore(flags);
 	}
-	if (unlikely(raw_cpu_read(rcu_data.cpu_no_qs.b.exp)))
-		rcu_qs();
-	this_cpu_inc(rcu_dynticks.rcu_qs_ctr);
+	rcu_qs();
 	barrier(); /* Avoid RCU read-side critical sections leaking up. */
 	preempt_enable();
 }
@@ -1001,7 +999,6 @@ void rcu_note_context_switch(bool preempt)
 	this_cpu_write(rcu_dynticks.rcu_urgent_qs, false);
 	if (unlikely(raw_cpu_read(rcu_dynticks.rcu_need_heavy_qs)))
 		rcu_momentary_dyntick_idle();
-	this_cpu_inc(rcu_dynticks.rcu_qs_ctr);
 	if (!preempt)
 		rcu_tasks_qs(current);
 out:
