block: delete part_round_stats and switch to less precise counting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit 5b18b5a737600fd20ba2045f320d5926ebbf341a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/5b18b5a7.failed

We want to convert to per-cpu in_flight counters.

The function part_round_stats needs the in_flight counter every jiffy, it
would be too costly to sum all the percpu variables every jiffy, so it
must be deleted. part_round_stats is used to calculate two counters -
time_in_queue and io_ticks.

time_in_queue can be calculated without part_round_stats, by adding the
duration of the I/O when the I/O ends (the value is almost as exact as the
previously calculated value, except that time for in-progress I/Os is not
counted).

io_ticks can be approximated by increasing the value when I/O is started
or ended and the jiffies value has changed. If the I/Os take less than a
jiffy, the value is as exact as the previously calculated value. If the
I/Os take more than a jiffy, io_ticks can drift behind the previously
calculated value.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5b18b5a737600fd20ba2045f320d5926ebbf341a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/bio.c
#	block/blk-core.c
#	block/blk-merge.c
#	block/genhd.c
#	block/partition-generic.c
#	include/linux/genhd.h
diff --cc block/bio.c
index 8d70c85b6b8f,036e3f0cc736..000000000000
--- a/block/bio.c
+++ b/block/bio.c
@@@ -1667,11 -1684,12 +1683,19 @@@ void generic_start_io_acct(struct reque
  			   unsigned long sectors, struct hd_struct *part)
  {
  	const int sgrp = op_stat_group(op);
 +	int cpu = part_stat_lock();
  
++<<<<<<< HEAD
 +	part_round_stats(q, cpu, part);
 +	part_stat_inc(cpu, part, ios[sgrp]);
 +	part_stat_add(cpu, part, sectors[sgrp], sectors);
++=======
+ 	part_stat_lock();
+ 
+ 	update_io_ticks(part, jiffies);
+ 	part_stat_inc(part, ios[sgrp]);
+ 	part_stat_add(part, sectors[sgrp], sectors);
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  	part_inc_in_flight(q, part, op_is_write(op));
  
  	part_stat_unlock();
@@@ -1681,12 -1699,15 +1705,21 @@@ EXPORT_SYMBOL(generic_start_io_acct)
  void generic_end_io_acct(struct request_queue *q, int req_op,
  			 struct hd_struct *part, unsigned long start_time)
  {
- 	unsigned long duration = jiffies - start_time;
+ 	unsigned long now = jiffies;
+ 	unsigned long duration = now - start_time;
  	const int sgrp = op_stat_group(req_op);
 +	int cpu = part_stat_lock();
  
++<<<<<<< HEAD
 +	part_stat_add(cpu, part, nsecs[sgrp], jiffies_to_nsecs(duration));
 +	part_round_stats(q, cpu, part);
++=======
+ 	part_stat_lock();
+ 
+ 	update_io_ticks(part, now);
+ 	part_stat_add(part, nsecs[sgrp], jiffies_to_nsecs(duration));
+ 	part_stat_add(part, time_in_queue, duration);
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  	part_dec_in_flight(q, part, op_is_write(req_op));
  
  	part_stat_unlock();
diff --cc block/blk-core.c
index d7c519b2554b,268d2b8e9843..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -1602,153 -584,9 +1602,156 @@@ struct request *blk_get_request(struct 
  }
  EXPORT_SYMBOL(blk_get_request);
  
++<<<<<<< HEAD
 +/**
 + * blk_requeue_request - put a request back on queue
 + * @q:		request queue where request should be inserted
 + * @rq:		request to be inserted
 + *
 + * Description:
 + *    Drivers often keep queueing requests until the hardware cannot accept
 + *    more, when that condition happens we need to put the request back
 + *    on the queue. Must be called with queue lock held.
 + */
 +void blk_requeue_request(struct request_queue *q, struct request *rq)
 +{
 +	lockdep_assert_held(q->queue_lock);
 +	WARN_ON_ONCE(q->mq_ops);
 +
 +	blk_delete_timer(rq);
 +	blk_clear_rq_complete(rq);
 +	trace_block_rq_requeue(q, rq);
 +	rq_qos_requeue(q, rq);
 +
 +	BUG_ON(blk_queued_rq(rq));
 +
 +	elv_requeue_request(q, rq);
 +}
 +EXPORT_SYMBOL(blk_requeue_request);
 +
 +static void add_acct_request(struct request_queue *q, struct request *rq,
 +			     int where)
 +{
 +	blk_account_io_start(rq, true);
 +	__elv_add_request(q, rq, where);
 +}
 +
 +static void part_round_stats_single(struct request_queue *q, int cpu,
 +				    struct hd_struct *part, unsigned long now,
 +				    unsigned int inflight)
 +{
 +	if (inflight) {
 +		__part_stat_add(cpu, part, time_in_queue,
 +				inflight * (now - part->stamp));
 +		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
 +	}
 +	part->stamp = now;
 +}
 +
 +/**
 + * part_round_stats() - Round off the performance stats on a struct disk_stats.
 + * @q: target block queue
 + * @cpu: cpu number for stats access
 + * @part: target partition
 + *
 + * The average IO queue length and utilisation statistics are maintained
 + * by observing the current state of the queue length and the amount of
 + * time it has been in this state for.
 + *
 + * Normally, that accounting is done on IO completion, but that can result
 + * in more than a second's worth of IO being accounted for within any one
 + * second, leading to >100% utilisation.  To deal with that, we call this
 + * function to do a round-off before returning the results when reading
 + * /proc/diskstats.  This accounts immediately for all queue usage up to
 + * the current jiffies and restarts the counters again.
 + */
 +void part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part)
 +{
 +	struct hd_struct *part2 = NULL;
 +	unsigned long now = jiffies;
 +	unsigned int inflight[2];
 +	int stats = 0;
 +
 +	if (part->stamp != now)
 +		stats |= 1;
 +
 +	if (part->partno) {
 +		part2 = &part_to_disk(part)->part0;
 +		if (part2->stamp != now)
 +			stats |= 2;
 +	}
 +
 +	if (!stats)
 +		return;
 +
 +	part_in_flight(q, part, inflight);
 +
 +	if (stats & 2)
 +		part_round_stats_single(q, cpu, part2, now, inflight[1]);
 +	if (stats & 1)
 +		part_round_stats_single(q, cpu, part, now, inflight[0]);
 +}
 +EXPORT_SYMBOL_GPL(part_round_stats);
 +
 +void __blk_put_request(struct request_queue *q, struct request *req)
 +{
 +	req_flags_t rq_flags = req->rq_flags;
 +
 +	if (unlikely(!q))
 +		return;
 +
 +	if (q->mq_ops) {
 +		blk_mq_free_request(req);
 +		return;
 +	}
 +
 +	lockdep_assert_held(q->queue_lock);
 +
 +	blk_req_zone_write_unlock(req);
 +	blk_pm_put_request(req);
 +	blk_pm_mark_last_busy(req);
 +
 +	elv_completed_request(q, req);
 +
 +	/* this is a bio leak */
 +	WARN_ON(req->bio != NULL);
 +
 +	rq_qos_done(q, req);
 +
 +	/*
 +	 * Request may not have originated from ll_rw_blk. if not,
 +	 * it didn't come out of our reserved rq pools
 +	 */
 +	if (rq_flags & RQF_ALLOCED) {
 +		struct request_list *rl = blk_rq_rl(req);
 +		bool sync = op_is_sync(req->cmd_flags);
 +
 +		BUG_ON(!list_empty(&req->queuelist));
 +		BUG_ON(ELV_ON_HASH(req));
 +
 +		blk_free_request(rl, req);
 +		freed_request(rl, sync, rq_flags);
 +		blk_put_rl(rl);
 +		blk_queue_exit(q);
 +	}
 +}
 +EXPORT_SYMBOL_GPL(__blk_put_request);
 +
++=======
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  void blk_put_request(struct request *req)
  {
 -	blk_mq_free_request(req);
 +	struct request_queue *q = req->q;
 +
 +	if (q->mq_ops)
 +		blk_mq_free_request(req);
 +	else {
 +		unsigned long flags;
 +
 +		spin_lock_irqsave(q->queue_lock, flags);
 +		__blk_put_request(q, req);
 +		spin_unlock_irqrestore(q->queue_lock, flags);
 +	}
  }
  EXPORT_SYMBOL(blk_put_request);
  
@@@ -2688,14 -1323,14 +2691,21 @@@ void blk_account_io_done(struct reques
  	if (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {
  		const int sgrp = op_stat_group(req_op(req));
  		struct hd_struct *part;
 +		int cpu;
  
 -		part_stat_lock();
 +		cpu = part_stat_lock();
  		part = req->part;
  
++<<<<<<< HEAD
 +		part_stat_inc(cpu, part, ios[sgrp]);
 +		part_stat_add(cpu, part, nsecs[sgrp], now - req->start_time_ns);
 +		part_round_stats(req->q, cpu, part);
++=======
+ 		update_io_ticks(part, jiffies);
+ 		part_stat_inc(part, ios[sgrp]);
+ 		part_stat_add(part, nsecs[sgrp], now - req->start_time_ns);
+ 		part_stat_add(part, time_in_queue, nsecs_to_jiffies64(now - req->start_time_ns));
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  		part_dec_in_flight(req->q, part, rq_data_dir(req));
  
  		hd_struct_put(part);
@@@ -2731,7 -1365,6 +2741,10 @@@ void blk_account_io_start(struct reques
  			part = &rq->rq_disk->part0;
  			hd_struct_get(part);
  		}
++<<<<<<< HEAD
 +		part_round_stats(rq->q, cpu, part);
++=======
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  		part_inc_in_flight(rq->q, part, rw);
  		rq->part = part;
  	}
diff --cc block/blk-merge.c
index 8a4a6209453b,9da5629d0887..000000000000
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@@ -685,12 -685,10 +685,15 @@@ static void blk_account_io_merge(struc
  {
  	if (blk_do_io_stat(req)) {
  		struct hd_struct *part;
 +		int cpu;
  
 -		part_stat_lock();
 +		cpu = part_stat_lock();
  		part = req->part;
  
++<<<<<<< HEAD
 +		part_round_stats(req->q, cpu, part);
++=======
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  		part_dec_in_flight(req->q, part, rq_data_dir(req));
  
  		hd_struct_put(part);
diff --cc block/genhd.c
index be5bab20b2ab,cdf174d7d329..000000000000
--- a/block/genhd.c
+++ b/block/genhd.c
@@@ -1329,9 -1337,6 +1329,12 @@@ static int diskstats_show(struct seq_fi
  
  	disk_part_iter_init(&piter, gp, DISK_PITER_INCL_EMPTY_PART0);
  	while ((hd = disk_part_iter_next(&piter))) {
++<<<<<<< HEAD
 +		cpu = part_stat_lock();
 +		part_round_stats(gp->queue, cpu, hd);
 +		part_stat_unlock();
++=======
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  		part_in_flight(gp->queue, hd, inflight);
  		seq_printf(seqf, "%4d %7d %s "
  			   "%lu %lu %lu %u "
diff --cc block/partition-generic.c
index 5f8db5c5140f,42d6138ac876..000000000000
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@@ -121,11 -121,7 +121,14 @@@ ssize_t part_stat_show(struct device *d
  	struct hd_struct *p = dev_to_part(dev);
  	struct request_queue *q = part_to_disk(p)->queue;
  	unsigned int inflight[2];
 -
 +	int cpu;
 +
++<<<<<<< HEAD
 +	cpu = part_stat_lock();
 +	part_round_stats(q, cpu, p);
 +	part_stat_unlock();
++=======
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  	part_in_flight(q, p, inflight);
  	return sprintf(buf,
  		"%8lu %8lu %8llu %8u "
diff --cc include/linux/genhd.h
index f767293b00e6,838c2a7a40c5..000000000000
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@@ -398,14 -398,14 +398,18 @@@ static inline void free_part_info(struc
  	kfree(part->info);
  }
  
++<<<<<<< HEAD
 +/* block/blk-core.c */
 +extern void part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part);
++=======
+ void update_io_ticks(struct hd_struct *part, unsigned long now);
++>>>>>>> 5b18b5a73760 (block: delete part_round_stats and switch to less precise counting)
  
  /* block/genhd.c */
 -extern void device_add_disk(struct device *parent, struct gendisk *disk,
 -			    const struct attribute_group **groups);
 +extern void device_add_disk(struct device *parent, struct gendisk *disk);
  static inline void add_disk(struct gendisk *disk)
  {
 -	device_add_disk(NULL, disk, NULL);
 +	device_add_disk(NULL, disk);
  }
  extern void device_add_disk_no_queue_reg(struct device *parent, struct gendisk *disk);
  static inline void add_disk_no_queue_reg(struct gendisk *disk)
* Unmerged path block/bio.c
* Unmerged path block/blk-core.c
* Unmerged path block/blk-merge.c
* Unmerged path block/genhd.c
* Unmerged path block/partition-generic.c
* Unmerged path include/linux/genhd.h
