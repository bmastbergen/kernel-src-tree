rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit dc5a4f2932f18568bb9d8cdbe2139a8ddbc28bb8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/dc5a4f29.failed

This commit move ->dynticks from the rcu_dynticks structure to the
rcu_data structure, replacing the field of the same name.  It also updates
the code to access ->dynticks from the rcu_data structure and to use the
rcu_data structure rather than following to now-gone ->dynticks field
to the now-gone rcu_dynticks structure.  While in the area, this commit
also fixes up comments.

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit dc5a4f2932f18568bb9d8cdbe2139a8ddbc28bb8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
#	kernel/rcu/tree.h
#	kernel/rcu/tree_plugin.h
diff --cc kernel/rcu/tree.c
index 65383db6b433,32f500fb24d3..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -72,7 -73,21 +72,25 @@@
  
  /* Data structures. */
  
++<<<<<<< HEAD
 +static DEFINE_PER_CPU_SHARED_ALIGNED(struct rcu_data, rcu_data);
++=======
+ /*
+  * Steal a bit from the bottom of ->dynticks for idle entry/exit
+  * control.  Initially this is for TLB flushing.
+  */
+ #define RCU_DYNTICK_CTRL_MASK 0x1
+ #define RCU_DYNTICK_CTRL_CTR  (RCU_DYNTICK_CTRL_MASK + 1)
+ #ifndef rcu_eqs_special_exit
+ #define rcu_eqs_special_exit() do { } while (0)
+ #endif
+ 
+ static DEFINE_PER_CPU_SHARED_ALIGNED(struct rcu_data, rcu_data) = {
+ 	.dynticks_nesting = 1,
+ 	.dynticks_nmi_nesting = DYNTICK_IRQ_NONIDLE,
+ 	.dynticks = ATOMIC_INIT(RCU_DYNTICK_CTRL_CTR),
+ };
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  struct rcu_state rcu_state = {
  	.level = { &rcu_state.node[0] },
  	.gp_state = RCU_GP_IDLE,
@@@ -201,22 -214,6 +219,25 @@@ void rcu_softirq_qs(void
  }
  
  /*
++<<<<<<< HEAD
 + * Steal a bit from the bottom of ->dynticks for idle entry/exit
 + * control.  Initially this is for TLB flushing.
 + */
 +#define RCU_DYNTICK_CTRL_MASK 0x1
 +#define RCU_DYNTICK_CTRL_CTR  (RCU_DYNTICK_CTRL_MASK + 1)
 +#ifndef rcu_eqs_special_exit
 +#define rcu_eqs_special_exit() do { } while (0)
 +#endif
 +
 +static DEFINE_PER_CPU(struct rcu_dynticks, rcu_dynticks) = {
 +	.dynticks_nesting = 1,
 +	.dynticks_nmi_nesting = DYNTICK_IRQ_NONIDLE,
 +	.dynticks = ATOMIC_INIT(RCU_DYNTICK_CTRL_CTR),
 +};
 +
 +/*
++=======
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
   * Record entry into an extended quiescent state.  This is only to be
   * called when not already in an extended quiescent state.
   */
@@@ -360,11 -357,11 +381,16 @@@ bool rcu_eqs_special_set(int cpu
   */
  static void __maybe_unused rcu_momentary_dyntick_idle(void)
  {
- 	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
  	int special;
  
++<<<<<<< HEAD
 +	raw_cpu_write(rcu_dynticks.rcu_need_heavy_qs, false);
 +	special = atomic_add_return(2 * RCU_DYNTICK_CTRL_CTR, &rdtp->dynticks);
++=======
+ 	raw_cpu_write(rcu_data.rcu_need_heavy_qs, false);
+ 	special = atomic_add_return(2 * RCU_DYNTICK_CTRL_CTR,
+ 				    &this_cpu_ptr(&rcu_data)->dynticks);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	/* It is illegal to call this from idle state. */
  	WARN_ON_ONCE(!(special & RCU_DYNTICK_CTRL_CTR));
  	rcu_preempt_deferred_qs(current);
@@@ -546,30 -569,25 +572,41 @@@ static struct rcu_node *rcu_get_root(vo
   */
  static void rcu_eqs_enter(bool user)
  {
++<<<<<<< HEAD
 +	struct rcu_state *rsp;
 +	struct rcu_data *rdp;
 +	struct rcu_dynticks *rdtp;
 +
 +	rdtp = this_cpu_ptr(&rcu_dynticks);
 +	WARN_ON_ONCE(rdtp->dynticks_nmi_nesting != DYNTICK_IRQ_NONIDLE);
 +	WRITE_ONCE(rdtp->dynticks_nmi_nesting, 0);
++=======
+ 	struct rcu_data *rdp = this_cpu_ptr(&rcu_data);
+ 
+ 	WARN_ON_ONCE(rdp->dynticks_nmi_nesting != DYNTICK_IRQ_NONIDLE);
+ 	WRITE_ONCE(rdp->dynticks_nmi_nesting, 0);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
 -		     rdp->dynticks_nesting == 0);
 -	if (rdp->dynticks_nesting != 1) {
 -		rdp->dynticks_nesting--;
 +		     rdtp->dynticks_nesting == 0);
 +	if (rdtp->dynticks_nesting != 1) {
 +		rdtp->dynticks_nesting--;
  		return;
  	}
  
  	lockdep_assert_irqs_disabled();
++<<<<<<< HEAD
 +	trace_rcu_dyntick(TPS("Start"), rdtp->dynticks_nesting, 0, rdtp->dynticks);
++=======
+ 	trace_rcu_dyntick(TPS("Start"), rdp->dynticks_nesting, 0, rdp->dynticks);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));
 -	rdp = this_cpu_ptr(&rcu_data);
 -	do_nocb_deferred_wakeup(rdp);
 +	for_each_rcu_flavor(rsp) {
 +		rdp = this_cpu_ptr(&rcu_data);
 +		do_nocb_deferred_wakeup(rdp);
 +	}
  	rcu_prepare_for_idle();
  	rcu_preempt_deferred_qs(current);
 -	WRITE_ONCE(rdp->dynticks_nesting, 0); /* Avoid irq-access tearing. */
 +	WRITE_ONCE(rdtp->dynticks_nesting, 0); /* Avoid irq-access tearing. */
  	rcu_dynticks_eqs_enter();
  	rcu_dynticks_task_enter();
  }
@@@ -612,7 -630,7 +649,11 @@@ void rcu_user_enter(void
  
  /*
   * If we are returning from the outermost NMI handler that interrupted an
++<<<<<<< HEAD
 + * RCU-idle period, update rdtp->dynticks and rdtp->dynticks_nmi_nesting
++=======
+  * RCU-idle period, update rdp->dynticks and rdp->dynticks_nmi_nesting
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
   * to let the RCU grace-period handling know that the CPU is back to
   * being RCU-idle.
   *
@@@ -621,7 -639,7 +662,11 @@@
   */
  static __always_inline void rcu_nmi_exit_common(bool irq)
  {
++<<<<<<< HEAD
 +	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
++=======
+ 	struct rcu_data *rdp = this_cpu_ptr(&rcu_data);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  
  	/*
  	 * Check for ->dynticks_nmi_nesting underflow and bad ->dynticks.
@@@ -635,16 -653,16 +680,28 @@@
  	 * If the nesting level is not 1, the CPU wasn't RCU-idle, so
  	 * leave it in non-RCU-idle state.
  	 */
++<<<<<<< HEAD
 +	if (rdtp->dynticks_nmi_nesting != 1) {
 +		trace_rcu_dyntick(TPS("--="), rdtp->dynticks_nmi_nesting, rdtp->dynticks_nmi_nesting - 2, rdtp->dynticks);
 +		WRITE_ONCE(rdtp->dynticks_nmi_nesting, /* No store tearing. */
 +			   rdtp->dynticks_nmi_nesting - 2);
++=======
+ 	if (rdp->dynticks_nmi_nesting != 1) {
+ 		trace_rcu_dyntick(TPS("--="), rdp->dynticks_nmi_nesting, rdp->dynticks_nmi_nesting - 2, rdp->dynticks);
+ 		WRITE_ONCE(rdp->dynticks_nmi_nesting, /* No store tearing. */
+ 			   rdp->dynticks_nmi_nesting - 2);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  		return;
  	}
  
  	/* This NMI interrupted an RCU-idle CPU, restore RCU-idleness. */
++<<<<<<< HEAD
 +	trace_rcu_dyntick(TPS("Startirq"), rdtp->dynticks_nmi_nesting, 0, rdtp->dynticks);
 +	WRITE_ONCE(rdtp->dynticks_nmi_nesting, 0); /* Avoid store tearing. */
++=======
+ 	trace_rcu_dyntick(TPS("Startirq"), rdp->dynticks_nmi_nesting, 0, rdp->dynticks);
+ 	WRITE_ONCE(rdp->dynticks_nmi_nesting, 0); /* Avoid store tearing. */
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  
  	if (irq)
  		rcu_prepare_for_idle();
@@@ -717,25 -735,25 +774,38 @@@ void rcu_irq_exit_irqson(void
   */
  static void rcu_eqs_exit(bool user)
  {
++<<<<<<< HEAD
 +	struct rcu_dynticks *rdtp;
 +	long oldval;
 +
 +	lockdep_assert_irqs_disabled();
 +	rdtp = this_cpu_ptr(&rcu_dynticks);
 +	oldval = rdtp->dynticks_nesting;
++=======
+ 	struct rcu_data *rdp;
+ 	long oldval;
+ 
+ 	lockdep_assert_irqs_disabled();
+ 	rdp = this_cpu_ptr(&rcu_data);
+ 	oldval = rdp->dynticks_nesting;
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  	if (oldval) {
 -		rdp->dynticks_nesting++;
 +		rdtp->dynticks_nesting++;
  		return;
  	}
  	rcu_dynticks_task_exit();
  	rcu_dynticks_eqs_exit();
  	rcu_cleanup_after_idle();
++<<<<<<< HEAD
 +	trace_rcu_dyntick(TPS("End"), rdtp->dynticks_nesting, 1, rdtp->dynticks);
++=======
+ 	trace_rcu_dyntick(TPS("End"), rdp->dynticks_nesting, 1, rdp->dynticks);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));
 -	WRITE_ONCE(rdp->dynticks_nesting, 1);
 -	WARN_ON_ONCE(rdp->dynticks_nmi_nesting);
 -	WRITE_ONCE(rdp->dynticks_nmi_nesting, DYNTICK_IRQ_NONIDLE);
 +	WRITE_ONCE(rdtp->dynticks_nesting, 1);
 +	WARN_ON_ONCE(rdtp->dynticks_nmi_nesting);
 +	WRITE_ONCE(rdtp->dynticks_nmi_nesting, DYNTICK_IRQ_NONIDLE);
  }
  
  /**
@@@ -776,8 -794,8 +846,13 @@@ void rcu_user_exit(void
   * rcu_nmi_enter_common - inform RCU of entry to NMI context
   * @irq: Is this call from rcu_irq_enter?
   *
++<<<<<<< HEAD
 + * If the CPU was idle from RCU's viewpoint, update rdtp->dynticks and
 + * rdtp->dynticks_nmi_nesting to let the RCU grace-period handling know
++=======
+  * If the CPU was idle from RCU's viewpoint, update rdp->dynticks and
+  * rdp->dynticks_nmi_nesting to let the RCU grace-period handling know
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
   * that the CPU is active.  This implementation permits nested NMIs, as
   * long as the nesting level does not overflow an int.  (You will probably
   * run out of stack space first.)
@@@ -787,7 -805,7 +862,11 @@@
   */
  static __always_inline void rcu_nmi_enter_common(bool irq)
  {
++<<<<<<< HEAD
 +	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
++=======
+ 	struct rcu_data *rdp = this_cpu_ptr(&rcu_data);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	long incby = 2;
  
  	/* Complain about underflow. */
@@@ -814,10 -832,10 +893,17 @@@
  		incby = 1;
  	}
  	trace_rcu_dyntick(incby == 1 ? TPS("Endirq") : TPS("++="),
++<<<<<<< HEAD
 +			  rdtp->dynticks_nmi_nesting,
 +			  rdtp->dynticks_nmi_nesting + incby, rdtp->dynticks);
 +	WRITE_ONCE(rdtp->dynticks_nmi_nesting, /* Prevent store tearing. */
 +		   rdtp->dynticks_nmi_nesting + incby);
++=======
+ 			  rdp->dynticks_nmi_nesting,
+ 			  rdp->dynticks_nmi_nesting + incby, rdp->dynticks);
+ 	WRITE_ONCE(rdp->dynticks_nmi_nesting, /* Prevent store tearing. */
+ 		   rdp->dynticks_nmi_nesting + incby);
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	barrier();
  }
  
@@@ -972,9 -986,9 +1058,9 @@@ static void rcu_gpnum_ovf(struct rcu_no
   */
  static int dyntick_save_progress_counter(struct rcu_data *rdp)
  {
- 	rdp->dynticks_snap = rcu_dynticks_snap(rdp->dynticks);
+ 	rdp->dynticks_snap = rcu_dynticks_snap(rdp);
  	if (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {
 -		trace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS("dti"));
 +		trace_rcu_fqs(rdp->rsp->name, rdp->gp_seq, rdp->cpu, TPS("dti"));
  		rcu_gpnum_ovf(rdp->mynode, rdp);
  		return 1;
  	}
@@@ -1023,8 -1037,8 +1109,13 @@@ static int rcu_implicit_dynticks_qs(str
  	 * read-side critical section that started before the beginning
  	 * of the current RCU grace period.
  	 */
++<<<<<<< HEAD
 +	if (rcu_dynticks_in_eqs_since(rdp->dynticks, rdp->dynticks_snap)) {
 +		trace_rcu_fqs(rdp->rsp->name, rdp->gp_seq, rdp->cpu, TPS("dti"));
++=======
+ 	if (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {
+ 		trace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS("dti"));
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  		rdp->dynticks_fqs++;
  		rcu_gpnum_ovf(rnp, rdp);
  		return 1;
@@@ -3181,9 -3189,8 +3272,14 @@@ rcu_boot_init_percpu_data(int cpu
  
  	/* Set up local state, ensuring consistent view of global state. */
  	rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
++<<<<<<< HEAD
 +	rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
 +	WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != 1);
 +	WARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(rdp->dynticks)));
++=======
+ 	WARN_ON_ONCE(rdp->dynticks_nesting != 1);
+ 	WARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(rdp)));
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	rdp->rcu_ofl_gp_seq = rcu_state.gp_seq;
  	rdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;
  	rdp->rcu_onl_gp_seq = rcu_state.gp_seq;
diff --cc kernel/rcu/tree.h
index 971069b043e9,af8681fec23b..000000000000
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@@ -34,30 -34,6 +34,33 @@@
  
  #include "rcu_segcblist.h"
  
++<<<<<<< HEAD
 +/*
 + * Dynticks per-CPU state.
 + */
 +struct rcu_dynticks {
 +	long dynticks_nesting;      /* Track process nesting level. */
 +	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 +	atomic_t dynticks;	    /* Even value for idle, else odd. */
 +	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 +	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 +	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 +#ifdef CONFIG_RCU_FAST_NO_HZ
 +	bool all_lazy;		    /* Are all CPU's CBs lazy? */
 +	unsigned long nonlazy_posted;
 +				    /* # times non-lazy CBs posted to CPU. */
 +	unsigned long nonlazy_posted_snap;
 +				    /* idle-period nonlazy_posted snapshot. */
 +	unsigned long last_accelerate;
 +				    /* Last jiffy CBs were accelerated. */
 +	unsigned long last_advance_all;
 +				    /* Last jiffy CBs were all advanced. */
 +	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
 +#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 +};
 +
++=======
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  /* Communicate arguments to a workqueue handler. */
  struct rcu_exp_work {
  	smp_call_func_t rew_func;
diff --cc kernel/rcu/tree_plugin.h
index 6d38152e918f,05915e536336..000000000000
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@@ -1842,8 -1829,8 +1841,13 @@@ static void print_cpu_stall_info(int cp
  			rdp->rcu_iw_pending ? (int)min(delta, 9UL) + '0' :
  				"!."[!delta],
  	       ticks_value, ticks_title,
++<<<<<<< HEAD
 +	       rcu_dynticks_snap(rdtp) & 0xfff,
 +	       rdtp->dynticks_nesting, rdtp->dynticks_nmi_nesting,
++=======
+ 	       rcu_dynticks_snap(rdp) & 0xfff,
+ 	       rdp->dynticks_nesting, rdp->dynticks_nmi_nesting,
++>>>>>>> dc5a4f2932f1 (rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks)
  	       rdp->softirq_snap, kstat_softirqs_cpu(RCU_SOFTIRQ, cpu),
  	       READ_ONCE(rcu_state.n_force_qs) - rcu_state.n_force_qs_gpstart,
  	       fast_no_hz);
* Unmerged path kernel/rcu/tree.c
* Unmerged path kernel/rcu/tree.h
diff --git a/kernel/rcu/tree_exp.h b/kernel/rcu/tree_exp.h
index d5f722f2b4cd..c9aedab3a600 100644
--- a/kernel/rcu/tree_exp.h
+++ b/kernel/rcu/tree_exp.h
@@ -362,14 +362,13 @@ static void sync_rcu_exp_select_node_cpus(struct work_struct *wp)
 	for_each_leaf_node_cpu_mask(rnp, cpu, rnp->expmask) {
 		unsigned long mask = leaf_node_cpu_bit(rnp, cpu);
 		struct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);
-		struct rcu_dynticks *rdtp = per_cpu_ptr(&rcu_dynticks, cpu);
 		int snap;
 
 		if (raw_smp_processor_id() == cpu ||
 		    !(rnp->qsmaskinitnext & mask)) {
 			mask_ofl_test |= mask;
 		} else {
-			snap = rcu_dynticks_snap(rdtp);
+			snap = rcu_dynticks_snap(rdp);
 			if (rcu_dynticks_in_eqs(snap))
 				mask_ofl_test |= mask;
 			else
@@ -395,8 +394,7 @@ static void sync_rcu_exp_select_node_cpus(struct work_struct *wp)
 		if (!(mask_ofl_ipi & mask))
 			continue;
 retry_ipi:
-		if (rcu_dynticks_in_eqs_since(rdp->dynticks,
-					      rdp->exp_dynticks_snap)) {
+		if (rcu_dynticks_in_eqs_since(rdp, rdp->exp_dynticks_snap)) {
 			mask_ofl_test |= mask;
 			continue;
 		}
* Unmerged path kernel/rcu/tree_plugin.h
