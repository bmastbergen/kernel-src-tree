RDMA/hns: Move all prints out of irq handle

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author liuyixian <liuyixian@huawei.com>
commit b00a92c8f2cae4ea8c84ddae9b9ba5daeb9f327f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/b00a92c8.failed

It will trigger unnecessary interrupts caused by time out if prints inside
aeq handle under some configurations.  Thus, move all prints out of aeq
handle to work queue.

	Signed-off-by: liuyixian <liuyixian@huawei.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit b00a92c8f2cae4ea8c84ddae9b9ba5daeb9f327f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_device.h
#	drivers/infiniband/hw/hns/hns_roce_hw_v2.c
diff --cc drivers/infiniband/hw/hns/hns_roce_device.h
index dddf1b89c3b6,cfb88a41b28f..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@@ -727,6 -734,15 +727,18 @@@ struct hns_roce_caps 
  	u64		flags;
  };
  
++<<<<<<< HEAD
++=======
+ struct hns_roce_work {
+ 	struct hns_roce_dev *hr_dev;
+ 	struct work_struct work;
+ 	u32 qpn;
+ 	u32 cqn;
+ 	int event_type;
+ 	int sub_type;
+ };
+ 
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  struct hns_roce_hw {
  	int (*reset)(struct hns_roce_dev *hr_dev, bool enable);
  	int (*cmq_init)(struct hns_roce_dev *hr_dev);
diff --cc drivers/infiniband/hw/hns/hns_roce_hw_v2.c
index eb4d2122b67e,7ccf3774eb71..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
@@@ -3886,6 -3959,166 +3886,169 @@@ static int hns_roce_v2_modify_cq(struc
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void hns_roce_set_qps_to_err(struct hns_roce_dev *hr_dev, u32 qpn)
+ {
+ 	struct hns_roce_qp *hr_qp;
+ 	struct ib_qp_attr attr;
+ 	int attr_mask;
+ 	int ret;
+ 
+ 	hr_qp = __hns_roce_qp_lookup(hr_dev, qpn);
+ 	if (!hr_qp) {
+ 		dev_warn(hr_dev->dev, "no hr_qp can be found!\n");
+ 		return;
+ 	}
+ 
+ 	if (hr_qp->ibqp.uobject) {
+ 		if (hr_qp->sdb_en == 1) {
+ 			hr_qp->sq.head = *(int *)(hr_qp->sdb.virt_addr);
+ 			hr_qp->rq.head = *(int *)(hr_qp->rdb.virt_addr);
+ 		} else {
+ 			dev_warn(hr_dev->dev, "flush cqe is unsupported in userspace!\n");
+ 			return;
+ 		}
+ 	}
+ 
+ 	attr_mask = IB_QP_STATE;
+ 	attr.qp_state = IB_QPS_ERR;
+ 	ret = hns_roce_v2_modify_qp(&hr_qp->ibqp, &attr, attr_mask,
+ 				    hr_qp->state, IB_QPS_ERR);
+ 	if (ret)
+ 		dev_err(hr_dev->dev, "failed to modify qp %d to err state.\n",
+ 			qpn);
+ }
+ 
+ static void hns_roce_irq_work_handle(struct work_struct *work)
+ {
+ 	struct hns_roce_work *irq_work =
+ 				container_of(work, struct hns_roce_work, work);
+ 	struct device *dev = irq_work->hr_dev->dev;
+ 	u32 qpn = irq_work->qpn;
+ 	u32 cqn = irq_work->cqn;
+ 
+ 	switch (irq_work->event_type) {
+ 	case HNS_ROCE_EVENT_TYPE_PATH_MIG:
+ 		dev_info(dev, "Path migrated succeeded.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_PATH_MIG_FAILED:
+ 		dev_warn(dev, "Path migration failed.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_COMM_EST:
+ 		dev_info(dev, "Communication established.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_SQ_DRAINED:
+ 		dev_warn(dev, "Send queue drained.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:
+ 		dev_err(dev, "Local work queue catastrophic error.\n");
+ 		hns_roce_set_qps_to_err(irq_work->hr_dev, qpn);
+ 		switch (irq_work->sub_type) {
+ 		case HNS_ROCE_LWQCE_QPC_ERROR:
+ 			dev_err(dev, "QP %d, QPC error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LWQCE_MTU_ERROR:
+ 			dev_err(dev, "QP %d, MTU error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LWQCE_WQE_BA_ADDR_ERROR:
+ 			dev_err(dev, "QP %d, WQE BA addr error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LWQCE_WQE_ADDR_ERROR:
+ 			dev_err(dev, "QP %d, WQE addr error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LWQCE_SQ_WQE_SHIFT_ERROR:
+ 			dev_err(dev, "QP %d, WQE shift error.\n", qpn);
+ 			break;
+ 		default:
+ 			dev_err(dev, "Unhandled sub_event type %d.\n",
+ 				irq_work->sub_type);
+ 			break;
+ 		}
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:
+ 		dev_err(dev, "Invalid request local work queue error.\n");
+ 		hns_roce_set_qps_to_err(irq_work->hr_dev, qpn);
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:
+ 		dev_err(dev, "Local access violation work queue error.\n");
+ 		hns_roce_set_qps_to_err(irq_work->hr_dev, qpn);
+ 		switch (irq_work->sub_type) {
+ 		case HNS_ROCE_LAVWQE_R_KEY_VIOLATION:
+ 			dev_err(dev, "QP %d, R_key violation.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_LENGTH_ERROR:
+ 			dev_err(dev, "QP %d, length error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_VA_ERROR:
+ 			dev_err(dev, "QP %d, VA error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_PD_ERROR:
+ 			dev_err(dev, "QP %d, PD error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_RW_ACC_ERROR:
+ 			dev_err(dev, "QP %d, rw acc error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_KEY_STATE_ERROR:
+ 			dev_err(dev, "QP %d, key state error.\n", qpn);
+ 			break;
+ 		case HNS_ROCE_LAVWQE_MR_OPERATION_ERROR:
+ 			dev_err(dev, "QP %d, MR operation error.\n", qpn);
+ 			break;
+ 		default:
+ 			dev_err(dev, "Unhandled sub_event type %d.\n",
+ 				irq_work->sub_type);
+ 			break;
+ 		}
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:
+ 		dev_warn(dev, "SRQ limit reach.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:
+ 		dev_warn(dev, "SRQ last wqe reach.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_SRQ_CATAS_ERROR:
+ 		dev_err(dev, "SRQ catas error.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:
+ 		dev_err(dev, "CQ 0x%x access err.\n", cqn);
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:
+ 		dev_warn(dev, "CQ 0x%x overflow\n", cqn);
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_DB_OVERFLOW:
+ 		dev_warn(dev, "DB overflow.\n");
+ 		break;
+ 	case HNS_ROCE_EVENT_TYPE_FLR:
+ 		dev_warn(dev, "Function level reset.\n");
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	kfree(irq_work);
+ }
+ 
+ static void hns_roce_v2_init_irq_work(struct hns_roce_dev *hr_dev,
+ 				      struct hns_roce_eq *eq,
+ 				      u32 qpn, u32 cqn)
+ {
+ 	struct hns_roce_work *irq_work;
+ 
+ 	irq_work = kzalloc(sizeof(struct hns_roce_work), GFP_ATOMIC);
+ 	if (!irq_work)
+ 		return;
+ 
+ 	INIT_WORK(&(irq_work->work), hns_roce_irq_work_handle);
+ 	irq_work->hr_dev = hr_dev;
+ 	irq_work->qpn = qpn;
+ 	irq_work->cqn = cqn;
+ 	irq_work->event_type = eq->event_type;
+ 	irq_work->sub_type = eq->sub_type;
+ 	queue_work(hr_dev->irq_workq, &(irq_work->work));
+ }
+ 
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  static void set_eq_cons_index_v2(struct hns_roce_eq *eq)
  {
  	u32 doorbell[2];
@@@ -3917,134 -4150,6 +4080,137 @@@
  	hns_roce_write64_k(doorbell, eq->doorbell);
  }
  
++<<<<<<< HEAD
 +static void hns_roce_v2_wq_catas_err_handle(struct hns_roce_dev *hr_dev,
 +						  struct hns_roce_aeqe *aeqe,
 +						  u32 qpn)
 +{
 +	struct device *dev = hr_dev->dev;
 +	int sub_type;
 +
 +	dev_warn(dev, "Local work queue catastrophic error.\n");
 +	sub_type = roce_get_field(aeqe->asyn, HNS_ROCE_V2_AEQE_SUB_TYPE_M,
 +				  HNS_ROCE_V2_AEQE_SUB_TYPE_S);
 +	switch (sub_type) {
 +	case HNS_ROCE_LWQCE_QPC_ERROR:
 +		dev_warn(dev, "QP %d, QPC error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LWQCE_MTU_ERROR:
 +		dev_warn(dev, "QP %d, MTU error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LWQCE_WQE_BA_ADDR_ERROR:
 +		dev_warn(dev, "QP %d, WQE BA addr error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LWQCE_WQE_ADDR_ERROR:
 +		dev_warn(dev, "QP %d, WQE addr error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LWQCE_SQ_WQE_SHIFT_ERROR:
 +		dev_warn(dev, "QP %d, WQE shift error.\n", qpn);
 +		break;
 +	default:
 +		dev_err(dev, "Unhandled sub_event type %d.\n", sub_type);
 +		break;
 +	}
 +}
 +
 +static void hns_roce_v2_local_wq_access_err_handle(struct hns_roce_dev *hr_dev,
 +					    struct hns_roce_aeqe *aeqe, u32 qpn)
 +{
 +	struct device *dev = hr_dev->dev;
 +	int sub_type;
 +
 +	dev_warn(dev, "Local access violation work queue error.\n");
 +	sub_type = roce_get_field(aeqe->asyn, HNS_ROCE_V2_AEQE_SUB_TYPE_M,
 +				  HNS_ROCE_V2_AEQE_SUB_TYPE_S);
 +	switch (sub_type) {
 +	case HNS_ROCE_LAVWQE_R_KEY_VIOLATION:
 +		dev_warn(dev, "QP %d, R_key violation.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_LENGTH_ERROR:
 +		dev_warn(dev, "QP %d, length error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_VA_ERROR:
 +		dev_warn(dev, "QP %d, VA error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_PD_ERROR:
 +		dev_err(dev, "QP %d, PD error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_RW_ACC_ERROR:
 +		dev_warn(dev, "QP %d, rw acc error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_KEY_STATE_ERROR:
 +		dev_warn(dev, "QP %d, key state error.\n", qpn);
 +		break;
 +	case HNS_ROCE_LAVWQE_MR_OPERATION_ERROR:
 +		dev_warn(dev, "QP %d, MR operation error.\n", qpn);
 +		break;
 +	default:
 +		dev_err(dev, "Unhandled sub_event type %d.\n", sub_type);
 +		break;
 +	}
 +}
 +
 +static void hns_roce_v2_qp_err_handle(struct hns_roce_dev *hr_dev,
 +				      struct hns_roce_aeqe *aeqe,
 +				      int event_type)
 +{
 +	struct device *dev = hr_dev->dev;
 +	u32 qpn;
 +
 +	qpn = roce_get_field(aeqe->event.qp_event.qp,
 +			     HNS_ROCE_V2_AEQE_EVENT_QUEUE_NUM_M,
 +			     HNS_ROCE_V2_AEQE_EVENT_QUEUE_NUM_S);
 +
 +	switch (event_type) {
 +	case HNS_ROCE_EVENT_TYPE_COMM_EST:
 +		dev_warn(dev, "Communication established.\n");
 +		break;
 +	case HNS_ROCE_EVENT_TYPE_SQ_DRAINED:
 +		dev_warn(dev, "Send queue drained.\n");
 +		break;
 +	case HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:
 +		hns_roce_v2_wq_catas_err_handle(hr_dev, aeqe, qpn);
 +		break;
 +	case HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:
 +		dev_warn(dev, "Invalid request local work queue error.\n");
 +		break;
 +	case HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:
 +		hns_roce_v2_local_wq_access_err_handle(hr_dev, aeqe, qpn);
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	hns_roce_qp_event(hr_dev, qpn, event_type);
 +}
 +
 +static void hns_roce_v2_cq_err_handle(struct hns_roce_dev *hr_dev,
 +				      struct hns_roce_aeqe *aeqe,
 +				      int event_type)
 +{
 +	struct device *dev = hr_dev->dev;
 +	u32 cqn;
 +
 +	cqn = roce_get_field(aeqe->event.cq_event.cq,
 +			     HNS_ROCE_V2_AEQE_EVENT_QUEUE_NUM_M,
 +			     HNS_ROCE_V2_AEQE_EVENT_QUEUE_NUM_S);
 +
 +	switch (event_type) {
 +	case HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:
 +		dev_warn(dev, "CQ 0x%x access err.\n", cqn);
 +		break;
 +	case HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:
 +		dev_warn(dev, "CQ 0x%x overflow\n", cqn);
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	hns_roce_cq_event(hr_dev, cqn, event_type);
 +}
 +
++=======
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  static struct hns_roce_aeqe *get_aeqe_v2(struct hns_roce_eq *eq, u32 entry)
  {
  	u32 buf_chk_sz;
@@@ -4118,7 -4231,7 +4280,11 @@@ static int hns_roce_v2_aeq_int(struct h
  		case HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:
  		case HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:
  		case HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:
++<<<<<<< HEAD
 +			hns_roce_v2_qp_err_handle(hr_dev, aeqe, event_type);
++=======
+ 			hns_roce_qp_event(hr_dev, qpn, event_type);
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  			break;
  		case HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:
  		case HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:
@@@ -4126,11 -4239,9 +4292,13 @@@
  			break;
  		case HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:
  		case HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:
++<<<<<<< HEAD
 +			hns_roce_v2_cq_err_handle(hr_dev, aeqe, event_type);
++=======
+ 			hns_roce_cq_event(hr_dev, cqn, event_type);
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  			break;
  		case HNS_ROCE_EVENT_TYPE_DB_OVERFLOW:
- 			dev_warn(dev, "DB overflow.\n");
  			break;
  		case HNS_ROCE_EVENT_TYPE_MB:
  			hns_roce_cmd_event(hr_dev,
@@@ -4157,6 -4268,7 +4323,10 @@@
  			dev_warn(dev, "cons_index overflow, set back to 0.\n");
  			eq->cons_index = 0;
  		}
++<<<<<<< HEAD
++=======
+ 		hns_roce_v2_init_irq_work(hr_dev, eq, qpn, cqn);
++>>>>>>> b00a92c8f2ca (RDMA/hns: Move all prints out of irq handle)
  	}
  
  	set_eq_cons_index_v2(eq);
* Unmerged path drivers/infiniband/hw/hns/hns_roce_device.h
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v2.c
