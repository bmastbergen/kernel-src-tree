x86/KVM/VMX: Use MSR save list for IA32_FLUSH_CMD if required

jira LE-1907
cve CVE-2018-3646
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 390d975e0c4e60ce70d4157e0dd91ede37824603
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/390d975e.failed

If the L1D flush module parameter is set to 'always' and the IA32_FLUSH_CMD
MSR is available, optimize the VMENTER code with the MSR save list.

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 390d975e0c4e60ce70d4157e0dd91ede37824603)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index d9e0ef7725c0,eb7c207a3bc3..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -9950,6 -9604,77 +9966,80 @@@ static int vmx_handle_exit(struct kvm_v
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Software based L1D cache flush which is used when microcode providing
+  * the cache control MSR is not loaded.
+  *
+  * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
+  * flush it is required to read in 64 KiB because the replacement algorithm
+  * is not exactly LRU. This could be sized at runtime via topology
+  * information but as all relevant affected CPUs have 32KiB L1D cache size
+  * there is no point in doing so.
+  */
+ #define L1D_CACHE_ORDER 4
+ static void *vmx_l1d_flush_pages;
+ 
+ static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
+ {
+ 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
+ 	bool always;
+ 
+ 	/*
+ 	 * This code is only executed when:
+ 	 * - the flush mode is 'cond'
+ 	 * - the flush mode is 'always' and the flush MSR is not
+ 	 *   available
+ 	 *
+ 	 * If the CPU has the flush MSR then clear the flush bit because
+ 	 * 'always' mode is handled via the MSR save list.
+ 	 *
+ 	 * If the MSR is not avaibable then act depending on the mitigation
+ 	 * mode: If 'flush always', keep the flush bit set, otherwise clear
+ 	 * it.
+ 	 *
+ 	 * The flush bit gets set again either from vcpu_run() or from one
+ 	 * of the unsafe VMEXIT handlers.
+ 	 */
+ 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D))
+ 		always = false;
+ 	else
+ 		always = vmentry_l1d_flush == VMENTER_L1D_FLUSH_ALWAYS;
+ 
+ 	vcpu->arch.l1tf_flush_l1d = always;
+ 
+ 	vcpu->stat.l1d_flush++;
+ 
+ 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
+ 		return;
+ 	}
+ 
+ 	asm volatile(
+ 		/* First ensure the pages are in the TLB */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lpopulate_tlb:\n\t"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$4096, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lpopulate_tlb\n\t"
+ 		"xorl	%%eax, %%eax\n\t"
+ 		"cpuid\n\t"
+ 		/* Now fill the cache */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lfill_cache:\n"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$64, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lfill_cache\n\t"
+ 		"lfence\n"
+ 		:: [empty_zp] "r" (vmx_l1d_flush_pages),
+ 		    [size] "r" (size)
+ 		: "eax", "ebx", "ecx", "edx");
+ }
+ 
++>>>>>>> 390d975e0c4e (x86/KVM/VMX: Use MSR save list for IA32_FLUSH_CMD if required)
  static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
  	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@@ -13877,6 -13231,34 +13967,37 @@@ static struct kvm_x86_ops vmx_x86_ops _
  	.enable_smi_window = enable_smi_window,
  };
  
++<<<<<<< HEAD
++=======
+ static int __init vmx_setup_l1d_flush(void)
+ {
+ 	struct page *page;
+ 
+ 	if (vmentry_l1d_flush == VMENTER_L1D_FLUSH_NEVER ||
+ 	    !boot_cpu_has_bug(X86_BUG_L1TF) ||
+ 	    vmx_l1d_use_msr_save_list())
+ 		return 0;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
+ 		if (!page)
+ 			return -ENOMEM;
+ 		vmx_l1d_flush_pages = page_address(page);
+ 	}
+ 
+ 	static_branch_enable(&vmx_l1d_should_flush);
+ 	return 0;
+ }
+ 
+ static void vmx_free_l1d_flush_pages(void)
+ {
+ 	if (vmx_l1d_flush_pages) {
+ 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ 		vmx_l1d_flush_pages = NULL;
+ 	}
+ }
+ 
++>>>>>>> 390d975e0c4e (x86/KVM/VMX: Use MSR save list for IA32_FLUSH_CMD if required)
  static int __init vmx_init(void)
  {
  	int r;
* Unmerged path arch/x86/kvm/vmx.c
