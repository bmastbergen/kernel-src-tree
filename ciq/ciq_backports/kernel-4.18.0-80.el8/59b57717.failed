blkcg: delay blkg destruction until after writeback has finished

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
Rebuild_CHGLOG: - [block] blkcg: delay blkg destruction until after writeback has (Ming Lei) [1634332]
Rebuild_FUZZ: 92.44%
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 59b57717fff8b562825d9d25e0180ad7e8048ca9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/59b57717.failed

Currently, blkcg destruction relies on a sequence of events:
  1. Destruction starts. blkcg_css_offline() is called and blkgs
     release their reference to the blkcg. This immediately destroys
     the cgwbs (writeback).
  2. With blkgs giving up their reference, the blkcg ref count should
     become zero and eventually call blkcg_css_free() which finally
     frees the blkcg.

Jiufei Xue reported that there is a race between blkcg_bio_issue_check()
and cgroup_rmdir(). To remedy this, blkg destruction becomes contingent
on the completion of all writeback associated with the blkcg. A count of
the number of cgwbs is maintained and once that goes to zero, blkg
destruction can follow. This should prevent premature blkg destruction
related to writeback.

The new process for blkcg cleanup is as follows:
  1. Destruction starts. blkcg_css_offline() is called which offlines
     writeback. Blkg destruction is delayed on the cgwb_refcnt count to
     avoid punting potentially large amounts of outstanding writeback
     to root while maintaining any ongoing policies. Here, the base
     cgwb_refcnt is put back.
  2. When the cgwb_refcnt becomes zero, blkcg_destroy_blkgs() is called
     and handles destruction of blkgs. This is where the css reference
     held by each blkg is released.
  3. Once the blkcg ref count goes to zero, blkcg_css_free() is called.
     This finally frees the blkg.

It seems in the past blk-throttle didn't do the most understandable
things with taking data from a blkg while associating with current. So,
the simplification and unification of what blk-throttle is doing caused
this.

Fixes: 08e18eab0c579 ("block: add bi_blkg to the bio for cgroups")
	Reviewed-by: Josef Bacik <josef@toxicpanda.com>
	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Cc: Jiufei Xue <jiufei.xue@linux.alibaba.com>
	Cc: Joseph Qi <joseph.qi@linux.alibaba.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Josef Bacik <josef@toxicpanda.com>
	Cc: Jens Axboe <axboe@kernel.dk>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 59b57717fff8b562825d9d25e0180ad7e8048ca9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-cgroup.c
diff --cc block/blk-cgroup.c
index 694595b29b8f,c19f9078da1e..000000000000
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@@ -1057,55 -1067,39 +1078,79 @@@ static struct cftype blkcg_legacy_files
   * blkcg_css_offline - cgroup css_offline callback
   * @css: css of interest
   *
++<<<<<<< HEAD
 + * This function is called when @css is about to go away and responsible
 + * for offlining all blkgs pd and killing all wbs associated with @css.
 + * blkgs pd offline should be done while holding both q and blkcg locks.
 + * As blkcg lock is nested inside q lock, this function performs reverse
 + * double lock dancing.
 + *
 + * This is the blkcg counterpart of ioc_release_fn().
++=======
+  * This function is called when @css is about to go away.  Here the cgwbs are
+  * offlined first and only once writeback associated with the blkcg has
+  * finished do we start step 2 (see above).
++>>>>>>> 59b57717fff8 (blkcg: delay blkg destruction until after writeback has finished)
   */
  static void blkcg_css_offline(struct cgroup_subsys_state *css)
  {
  	struct blkcg *blkcg = css_to_blkcg(css);
 +	struct blkcg_gq *blkg;
  
+ 	/* this prevents anyone from attaching or migrating to this blkcg */
+ 	wb_blkcg_offline(blkcg);
+ 
+ 	/* put the base cgwb reference allowing step 2 to be triggered */
+ 	blkcg_cgwb_put(blkcg);
+ }
+ 
+ /**
+  * blkcg_destroy_blkgs - responsible for shooting down blkgs
+  * @blkcg: blkcg of interest
+  *
+  * blkgs should be removed while holding both q and blkcg locks.  As blkcg lock
+  * is nested inside q lock, this function performs reverse double lock dancing.
+  * Destroying the blkgs releases the reference held on the blkcg's css allowing
+  * blkcg_css_free to eventually be called.
+  *
+  * This is the blkcg counterpart of ioc_release_fn().
+  */
+ void blkcg_destroy_blkgs(struct blkcg *blkcg)
+ {
  	spin_lock_irq(&blkcg->lock);
  
 +	hlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {
 +		struct request_queue *q = blkg->q;
 +
 +		if (spin_trylock(q->queue_lock)) {
 +			blkg_pd_offline(blkg);
 +			spin_unlock(q->queue_lock);
 +		} else {
 +			spin_unlock_irq(&blkcg->lock);
 +			cpu_relax();
 +			spin_lock_irq(&blkcg->lock);
 +		}
 +	}
 +
 +	spin_unlock_irq(&blkcg->lock);
- 
- 	wb_blkcg_offline(blkcg);
 +}
 +
 +/**
 + * blkcg_destroy_all_blkgs - destroy all blkgs associated with a blkcg
 + * @blkcg: blkcg of interest
 + *
 + * This function is called when blkcg css is about to free and responsible for
 + * destroying all blkgs associated with @blkcg.
 + * blkgs should be removed while holding both q and blkcg locks. As blkcg lock
 + * is nested inside q lock, this function performs reverse double lock dancing.
 + */
 +static void blkcg_destroy_all_blkgs(struct blkcg *blkcg)
 +{
 +	spin_lock_irq(&blkcg->lock);
  	while (!hlist_empty(&blkcg->blkg_list)) {
  		struct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,
 -						struct blkcg_gq, blkcg_node);
 +						    struct blkcg_gq,
 +						    blkcg_node);
  		struct request_queue *q = blkg->q;
  
  		if (spin_trylock(q->queue_lock)) {
* Unmerged path block/blk-cgroup.c
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index 1361cfc9b878..f9e400494884 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -56,6 +56,7 @@ struct blkcg {
 	struct list_head		all_blkcgs_node;
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct list_head		cgwb_list;
+	refcount_t			cgwb_refcnt;
 #endif
 };
 
@@ -393,6 +394,49 @@ static inline struct blkcg *cpd_to_blkcg(struct blkcg_policy_data *cpd)
 	return cpd ? cpd->blkcg : NULL;
 }
 
+extern void blkcg_destroy_blkgs(struct blkcg *blkcg);
+
+#ifdef CONFIG_CGROUP_WRITEBACK
+
+/**
+ * blkcg_cgwb_get - get a reference for blkcg->cgwb_list
+ * @blkcg: blkcg of interest
+ *
+ * This is used to track the number of active wb's related to a blkcg.
+ */
+static inline void blkcg_cgwb_get(struct blkcg *blkcg)
+{
+	refcount_inc(&blkcg->cgwb_refcnt);
+}
+
+/**
+ * blkcg_cgwb_put - put a reference for @blkcg->cgwb_list
+ * @blkcg: blkcg of interest
+ *
+ * This is used to track the number of active wb's related to a blkcg.
+ * When this count goes to zero, all active wb has finished so the
+ * blkcg can continue destruction by calling blkcg_destroy_blkgs().
+ * This work may occur in cgwb_release_workfn() on the cgwb_release
+ * workqueue.
+ */
+static inline void blkcg_cgwb_put(struct blkcg *blkcg)
+{
+	if (refcount_dec_and_test(&blkcg->cgwb_refcnt))
+		blkcg_destroy_blkgs(blkcg);
+}
+
+#else
+
+static inline void blkcg_cgwb_get(struct blkcg *blkcg) { }
+
+static inline void blkcg_cgwb_put(struct blkcg *blkcg)
+{
+	/* wb isn't being accounted, so trigger destruction right away */
+	blkcg_destroy_blkgs(blkcg);
+}
+
+#endif
+
 /**
  * blkg_path - format cgroup path of blkg
  * @blkg: blkg of interest
diff --git a/mm/backing-dev.c b/mm/backing-dev.c
index 2e5d3df0853d..dbae14986e04 100644
--- a/mm/backing-dev.c
+++ b/mm/backing-dev.c
@@ -494,6 +494,7 @@ static void cgwb_release_workfn(struct work_struct *work)
 {
 	struct bdi_writeback *wb = container_of(work, struct bdi_writeback,
 						release_work);
+	struct blkcg *blkcg = css_to_blkcg(wb->blkcg_css);
 
 	mutex_lock(&wb->bdi->cgwb_release_mutex);
 	wb_shutdown(wb);
@@ -502,6 +503,9 @@ static void cgwb_release_workfn(struct work_struct *work)
 	css_put(wb->blkcg_css);
 	mutex_unlock(&wb->bdi->cgwb_release_mutex);
 
+	/* triggers blkg destruction if cgwb_refcnt becomes zero */
+	blkcg_cgwb_put(blkcg);
+
 	fprop_local_destroy_percpu(&wb->memcg_completions);
 	percpu_ref_exit(&wb->refcnt);
 	wb_exit(wb);
@@ -600,6 +604,7 @@ static int cgwb_create(struct backing_dev_info *bdi,
 			list_add_tail_rcu(&wb->bdi_node, &bdi->wb_list);
 			list_add(&wb->memcg_node, memcg_cgwb_list);
 			list_add(&wb->blkcg_node, blkcg_cgwb_list);
+			blkcg_cgwb_get(blkcg);
 			css_get(memcg_css);
 			css_get(blkcg_css);
 		}
