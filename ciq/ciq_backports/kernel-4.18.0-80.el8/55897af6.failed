dma-direct: merge swiotlb_dma_ops into the dma_direct code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 55897af63091ebc2c3f239c6a6666f748113ac50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/55897af6.failed

While the dma-direct code is (relatively) clean and simple we actually
have to use the swiotlb ops for the mapping on many architectures due
to devices with addressing limits.  Instead of keeping two
implementations around this commit allows the dma-direct
implementation to call the swiotlb bounce buffering functions and
thus share the guts of the mapping implementation.  This also
simplified the dma-mapping setup on a few architectures where we
don't have to differenciate which implementation to use.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Tony Luck <tony.luck@intel.com>
(cherry picked from commit 55897af63091ebc2c3f239c6a6666f748113ac50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/dma-mapping.c
#	arch/mips/include/asm/dma-mapping.h
#	arch/powerpc/kernel/dma-swiotlb.c
#	kernel/dma/direct.c
#	kernel/dma/swiotlb.c
diff --cc arch/arm64/mm/dma-mapping.c
index f5a38a0d7e17,ab1e417204d0..000000000000
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@@ -872,9 -463,9 +872,13 @@@ void arch_setup_dma_ops(struct device *
  			const struct iommu_ops *iommu, bool coherent)
  {
  	if (!dev->dma_ops)
++<<<<<<< HEAD
 +		dev->dma_ops = &arm64_swiotlb_dma_ops;
++=======
+ 		dev->dma_ops = &dma_direct_ops;
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  
 -	dev->dma_coherent = coherent;
 +	dev->archdata.dma_coherent = coherent;
  	__iommu_setup_dma_ops(dev, dma_base, size, iommu);
  
  #ifdef CONFIG_XEN
diff --cc arch/mips/include/asm/dma-mapping.h
index 886e75a383f2,69f914667f3e..000000000000
--- a/arch/mips/include/asm/dma-mapping.h
+++ b/arch/mips/include/asm/dma-mapping.h
@@@ -14,7 -8,11 +14,15 @@@ extern const struct dma_map_ops *mips_d
  
  static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
  {
++<<<<<<< HEAD
 +	return mips_dma_map_ops;
++=======
+ #if defined(CONFIG_MACH_JAZZ)
+ 	return &jazz_dma_ops;
+ #else
+ 	return &dma_direct_ops;
+ #endif
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  }
  
  #define arch_setup_dma_ops arch_setup_dma_ops
diff --cc arch/powerpc/kernel/dma-swiotlb.c
index 5fc335f4d9cd,430a7d0aa2cb..000000000000
--- a/arch/powerpc/kernel/dma-swiotlb.c
+++ b/arch/powerpc/kernel/dma-swiotlb.c
@@@ -50,16 -50,15 +50,25 @@@ const struct dma_map_ops powerpc_swiotl
  	.alloc = __dma_nommu_alloc_coherent,
  	.free = __dma_nommu_free_coherent,
  	.mmap = dma_nommu_mmap_coherent,
- 	.map_sg = swiotlb_map_sg_attrs,
- 	.unmap_sg = swiotlb_unmap_sg_attrs,
+ 	.map_sg = dma_direct_map_sg,
+ 	.unmap_sg = dma_direct_unmap_sg,
  	.dma_supported = swiotlb_dma_supported,
++<<<<<<< HEAD
 +	.map_page = swiotlb_map_page,
 +	.unmap_page = swiotlb_unmap_page,
 +	.sync_single_for_cpu = swiotlb_sync_single_for_cpu,
 +	.sync_single_for_device = swiotlb_sync_single_for_device,
 +	.sync_sg_for_cpu = swiotlb_sync_sg_for_cpu,
 +	.sync_sg_for_device = swiotlb_sync_sg_for_device,
 +	.mapping_error = dma_direct_mapping_error,
++=======
+ 	.map_page = dma_direct_map_page,
+ 	.unmap_page = dma_direct_unmap_page,
+ 	.sync_single_for_cpu = dma_direct_sync_single_for_cpu,
+ 	.sync_single_for_device = dma_direct_sync_single_for_device,
+ 	.sync_sg_for_cpu = dma_direct_sync_sg_for_cpu,
+ 	.sync_sg_for_device = dma_direct_sync_sg_for_device,
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  	.get_required_mask = swiotlb_powerpc_get_required,
  };
  
diff --cc kernel/dma/direct.c
index a0ae9e05e04e,85d8286a0ba2..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -8,8 -10,10 +8,9 @@@
  #include <linux/dma-direct.h>
  #include <linux/scatterlist.h>
  #include <linux/dma-contiguous.h>
 -#include <linux/dma-noncoherent.h>
  #include <linux/pfn.h>
  #include <linux/set_memory.h>
+ #include <linux/swiotlb.h>
  
  /*
   * Most architectures use ZONE_DMA for the first 16 Megabytes, but
@@@ -133,18 -190,145 +134,153 @@@ void dma_direct_free(struct device *dev
  
  	if (force_dma_unencrypted())
  		set_memory_encrypted((unsigned long)cpu_addr, 1 << page_order);
 -	__dma_direct_free_pages(dev, size, virt_to_page(cpu_addr));
 +	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))
 +		free_pages((unsigned long)cpu_addr, page_order);
  }
  
++<<<<<<< HEAD
++=======
+ void *dma_direct_alloc(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
+ {
+ 	if (!dev_is_dma_coherent(dev))
+ 		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
+ 	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
+ }
+ 
+ void dma_direct_free(struct device *dev, size_t size,
+ 		void *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)
+ {
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
+ 	else
+ 		dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
+ }
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ 	phys_addr_t paddr = dma_to_phys(dev, addr);
+ 
+ 	if (unlikely(is_swiotlb_buffer(paddr)))
+ 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ 
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_sync_dma_for_device(dev, paddr, size, dir);
+ }
+ 
+ void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i) {
+ 		if (unlikely(is_swiotlb_buffer(sg_phys(sg))))
+ 			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length,
+ 					dir, SYNC_FOR_DEVICE);
+ 
+ 		if (!dev_is_dma_coherent(dev))
+ 			arch_sync_dma_for_device(dev, sg_phys(sg), sg->length,
+ 					dir);
+ 	}
+ }
+ #endif
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ 	phys_addr_t paddr = dma_to_phys(dev, addr);
+ 
+ 	if (!dev_is_dma_coherent(dev)) {
+ 		arch_sync_dma_for_cpu(dev, paddr, size, dir);
+ 		arch_sync_dma_for_cpu_all(dev);
+ 	}
+ 
+ 	if (unlikely(is_swiotlb_buffer(paddr)))
+ 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ }
+ 
+ void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i) {
+ 		if (!dev_is_dma_coherent(dev))
+ 			arch_sync_dma_for_cpu(dev, sg_phys(sg), sg->length, dir);
+ 	
+ 		if (unlikely(is_swiotlb_buffer(sg_phys(sg))))
+ 			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length, dir,
+ 					SYNC_FOR_CPU);
+ 	}
+ 
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_sync_dma_for_cpu_all(dev);
+ }
+ 
+ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	phys_addr_t phys = dma_to_phys(dev, addr);
+ 
+ 	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+ 		dma_direct_sync_single_for_cpu(dev, addr, size, dir);
+ 
+ 	if (unlikely(is_swiotlb_buffer(phys)))
+ 		swiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);
+ }
+ 
+ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i)
+ 		dma_direct_unmap_page(dev, sg->dma_address, sg_dma_len(sg), dir,
+ 			     attrs);
+ }
+ #else
+ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ #endif
+ 
+ static inline bool dma_direct_possible(struct device *dev, dma_addr_t dma_addr,
+ 		size_t size)
+ {
+ 	return swiotlb_force != SWIOTLB_FORCE &&
+ 		(!dev || dma_capable(dev, dma_addr, size));
+ }
+ 
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs)
  {
 -	phys_addr_t phys = page_to_phys(page) + offset;
 -	dma_addr_t dma_addr = phys_to_dma(dev, phys);
 +	dma_addr_t dma_addr = phys_to_dma(dev, page_to_phys(page)) + offset;
  
++<<<<<<< HEAD
 +	if (!check_addr(dev, dma_addr, size, __func__))
 +		return DIRECT_MAPPING_ERROR;
++=======
+ 	if (unlikely(!dma_direct_possible(dev, dma_addr, size)) &&
+ 	    !swiotlb_map(dev, &phys, &dma_addr, size, dir, attrs)) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	if (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+ 		arch_sync_dma_for_device(dev, phys, size, dir);
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  	return dma_addr;
  }
  
@@@ -155,44 -339,38 +291,55 @@@ int dma_direct_map_sg(struct device *de
  	struct scatterlist *sg;
  
  	for_each_sg(sgl, sg, nents, i) {
++<<<<<<< HEAD
 +		BUG_ON(!sg_page(sg));
 +
 +		sg_dma_address(sg) = phys_to_dma(dev, sg_phys(sg));
 +		if (!check_addr(dev, sg_dma_address(sg), sg->length, __func__))
 +			return 0;
++=======
+ 		sg->dma_address = dma_direct_map_page(dev, sg_page(sg),
+ 				sg->offset, sg->length, dir, attrs);
+ 		if (sg->dma_address == DMA_MAPPING_ERROR)
+ 			goto out_unmap;
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  		sg_dma_len(sg) = sg->length;
  	}
  
  	return nents;
+ 
+ out_unmap:
+ 	dma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
+ 	return 0;
  }
  
 -/*
 - * Because 32-bit DMA masks are so common we expect every architecture to be
 - * able to satisfy them - either by not supporting more physical memory, or by
 - * providing a ZONE_DMA32.  If neither is the case, the architecture needs to
 - * use an IOMMU instead of the direct mapping.
 - */
  int dma_direct_supported(struct device *dev, u64 mask)
  {
 -	u64 min_mask;
 -
 -	if (IS_ENABLED(CONFIG_ZONE_DMA))
 -		min_mask = DMA_BIT_MASK(ARCH_ZONE_DMA_BITS);
 -	else
 -		min_mask = DMA_BIT_MASK(32);
 -
 -	min_mask = min_t(u64, min_mask, (max_pfn - 1) << PAGE_SHIFT);
 +#ifdef CONFIG_ZONE_DMA
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(ARCH_ZONE_DMA_BITS)))
 +		return 0;
 +#else
 +	/*
 +	 * Because 32-bit DMA masks are so common we expect every architecture
 +	 * to be able to satisfy them - either by not supporting more physical
 +	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the
 +	 * architecture needs to use an IOMMU instead of the direct mapping.
 +	 */
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(32)))
 +		return 0;
 +#endif
 +	/*
 +	 * Upstream PCI/PCIe bridges or SoC interconnects may not carry
 +	 * as many DMA address bits as the device itself supports.
 +	 */
 +	if (dev->bus_dma_mask && mask > dev->bus_dma_mask)
 +		return 0;
 +	return 1;
 +}
  
 -	return mask >= phys_to_dma(dev, min_mask);
 +int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return dma_addr == DIRECT_MAPPING_ERROR;
  }
  
  const struct dma_map_ops dma_direct_ops = {
@@@ -200,7 -378,21 +347,24 @@@
  	.free			= dma_direct_free,
  	.map_page		= dma_direct_map_page,
  	.map_sg			= dma_direct_map_sg,
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_SWIOTLB)
+ 	.sync_single_for_device	= dma_direct_sync_single_for_device,
+ 	.sync_sg_for_device	= dma_direct_sync_sg_for_device,
+ #endif
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+     defined(CONFIG_SWIOTLB)
+ 	.sync_single_for_cpu	= dma_direct_sync_single_for_cpu,
+ 	.sync_sg_for_cpu	= dma_direct_sync_sg_for_cpu,
+ 	.unmap_page		= dma_direct_unmap_page,
+ 	.unmap_sg		= dma_direct_unmap_sg,
+ #endif
+ 	.get_required_mask	= dma_direct_get_required_mask,
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  	.dma_supported		= dma_direct_supported,
 -	.cache_sync		= arch_dma_cache_sync,
 +	.mapping_error		= dma_direct_mapping_error,
  };
  EXPORT_SYMBOL(dma_direct_ops);
diff --cc kernel/dma/swiotlb.c
index 5fb3413ff1ee,d6361776dc5c..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -631,229 -629,24 +629,245 @@@ bool swiotlb_map(struct device *dev, ph
  	if (unlikely(swiotlb_force == SWIOTLB_NO_FORCE)) {
  		dev_warn_ratelimited(dev,
  			"Cannot do DMA to address %pa\n", phys);
++<<<<<<< HEAD
 +		return DIRECT_MAPPING_ERROR;
++=======
+ 		return false;
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  	}
  
  	/* Oh well, have to allocate and map a bounce buffer. */
  	*phys = swiotlb_tbl_map_single(dev, __phys_to_dma(dev, io_tlb_start),
  			*phys, size, dir, attrs);
++<<<<<<< HEAD
 +	if (*phys == SWIOTLB_MAP_ERROR)
 +		return DIRECT_MAPPING_ERROR;
++=======
+ 	if (*phys == DMA_MAPPING_ERROR)
+ 		return false;
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  
  	/* Ensure that the address returned is DMA'ble */
- 	dma_addr = __phys_to_dma(dev, *phys);
- 	if (unlikely(!dma_capable(dev, dma_addr, size))) {
+ 	*dma_addr = __phys_to_dma(dev, *phys);
+ 	if (unlikely(!dma_capable(dev, *dma_addr, size))) {
  		swiotlb_tbl_unmap_single(dev, *phys, size, dir,
  			attrs | DMA_ATTR_SKIP_CPU_SYNC);
++<<<<<<< HEAD
 +		return DIRECT_MAPPING_ERROR;
 +	}
 +
 +	return dma_addr;
 +}
 +
 +/*
 + * Map a single buffer of the indicated size for DMA in streaming mode.  The
 + * physical address to use is returned.
 + *
 + * Once the device is given the dma address, the device owns this memory until
 + * either swiotlb_unmap_page or swiotlb_dma_sync_single is performed.
 + */
 +dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,
 +			    unsigned long offset, size_t size,
 +			    enum dma_data_direction dir,
 +			    unsigned long attrs)
 +{
 +	phys_addr_t phys = page_to_phys(page) + offset;
 +	dma_addr_t dev_addr = phys_to_dma(dev, phys);
 +
 +	BUG_ON(dir == DMA_NONE);
 +	/*
 +	 * If the address happens to be in the device's DMA window,
 +	 * we can safely return the device addr and not worry about bounce
 +	 * buffering it.
 +	 */
 +	if (!dma_capable(dev, dev_addr, size) ||
 +	    swiotlb_force == SWIOTLB_FORCE) {
 +		trace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);
 +		dev_addr = swiotlb_bounce_page(dev, &phys, size, dir, attrs);
 +	}
 +
 +	if (!dev_is_dma_coherent(dev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0 &&
 +	    dev_addr != DIRECT_MAPPING_ERROR)
 +		arch_sync_dma_for_device(dev, phys, size, dir);
 +
 +	return dev_addr;
 +}
 +
 +/*
 + * Unmap a single streaming mode DMA translation.  The dma_addr and size must
 + * match what was provided for in a previous swiotlb_map_page call.  All
 + * other usages are undefined.
 + *
 + * After this call, reads by the cpu to the buffer are guaranteed to see
 + * whatever the device wrote there.
 + */
 +void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 +			size_t size, enum dma_data_direction dir,
 +			unsigned long attrs)
 +{
 +	phys_addr_t paddr = dma_to_phys(hwdev, dev_addr);
 +
 +	BUG_ON(dir == DMA_NONE);
 +
 +	if (!dev_is_dma_coherent(hwdev) &&
 +	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
 +		arch_sync_dma_for_cpu(hwdev, paddr, size, dir);
 +
 +	if (is_swiotlb_buffer(paddr)) {
 +		swiotlb_tbl_unmap_single(hwdev, paddr, size, dir, attrs);
 +		return;
 +	}
 +
 +	if (dir != DMA_FROM_DEVICE)
 +		return;
 +
 +	/*
 +	 * phys_to_virt doesn't work with hihgmem page but we could
 +	 * call dma_mark_clean() with hihgmem page here. However, we
 +	 * are fine since dma_mark_clean() is null on POWERPC. We can
 +	 * make dma_mark_clean() take a physical address if necessary.
 +	 */
 +	dma_mark_clean(phys_to_virt(paddr), size);
 +}
 +
 +/*
 + * Make physical memory consistent for a single streaming mode DMA translation
 + * after a transfer.
 + *
 + * If you perform a swiotlb_map_page() but wish to interrogate the buffer
 + * using the cpu, yet do not wish to teardown the dma mapping, you must
 + * call this function before doing so.  At the next point you give the dma
 + * address back to the card, you must first perform a
 + * swiotlb_dma_sync_for_device, and then the device again owns the buffer
 + */
 +static void
 +swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,
 +		    size_t size, enum dma_data_direction dir,
 +		    enum dma_sync_target target)
 +{
 +	phys_addr_t paddr = dma_to_phys(hwdev, dev_addr);
 +
 +	BUG_ON(dir == DMA_NONE);
 +
 +	if (!dev_is_dma_coherent(hwdev) && target == SYNC_FOR_CPU)
 +		arch_sync_dma_for_cpu(hwdev, paddr, size, dir);
 +
 +	if (is_swiotlb_buffer(paddr))
 +		swiotlb_tbl_sync_single(hwdev, paddr, size, dir, target);
 +
 +	if (!dev_is_dma_coherent(hwdev) && target == SYNC_FOR_DEVICE)
 +		arch_sync_dma_for_device(hwdev, paddr, size, dir);
 +
 +	if (!is_swiotlb_buffer(paddr) && dir == DMA_FROM_DEVICE)
 +		dma_mark_clean(phys_to_virt(paddr), size);
 +}
 +
 +void
 +swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
 +			    size_t size, enum dma_data_direction dir)
 +{
 +	swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_CPU);
 +}
 +
 +void
 +swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,
 +			       size_t size, enum dma_data_direction dir)
 +{
 +	swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_DEVICE);
 +}
 +
 +/*
 + * Map a set of buffers described by scatterlist in streaming mode for DMA.
 + * This is the scatter-gather version of the above swiotlb_map_page
 + * interface.  Here the scatter gather list elements are each tagged with the
 + * appropriate dma address and length.  They are obtained via
 + * sg_dma_{address,length}(SG).
 + *
 + * Device ownership issues as mentioned above for swiotlb_map_page are the
 + * same here.
 + */
 +int
 +swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl, int nelems,
 +		     enum dma_data_direction dir, unsigned long attrs)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	for_each_sg(sgl, sg, nelems, i) {
 +		sg->dma_address = swiotlb_map_page(dev, sg_page(sg), sg->offset,
 +				sg->length, dir, attrs);
 +		if (sg->dma_address == DIRECT_MAPPING_ERROR)
 +			goto out_error;
 +		sg_dma_len(sg) = sg->length;
 +	}
 +
 +	return nelems;
 +
 +out_error:
 +	swiotlb_unmap_sg_attrs(dev, sgl, i, dir,
 +			attrs | DMA_ATTR_SKIP_CPU_SYNC);
 +	sg_dma_len(sgl) = 0;
 +	return 0;
 +}
 +
 +/*
 + * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
 + * concerning calls here are the same as for swiotlb_unmap_page() above.
 + */
 +void
 +swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
 +		       int nelems, enum dma_data_direction dir,
 +		       unsigned long attrs)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(dir == DMA_NONE);
 +
 +	for_each_sg(sgl, sg, nelems, i)
 +		swiotlb_unmap_page(hwdev, sg->dma_address, sg_dma_len(sg), dir,
 +			     attrs);
 +}
 +
 +/*
 + * Make physical memory consistent for a set of streaming mode DMA translations
 + * after a transfer.
 + *
 + * The same as swiotlb_sync_single_* but for a scatter-gather list, same rules
 + * and usage.
 + */
 +static void
 +swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,
 +		int nelems, enum dma_data_direction dir,
 +		enum dma_sync_target target)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	for_each_sg(sgl, sg, nelems, i)
 +		swiotlb_sync_single(hwdev, sg->dma_address,
 +				    sg_dma_len(sg), dir, target);
 +}
 +
 +void
 +swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 +			int nelems, enum dma_data_direction dir)
 +{
 +	swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_CPU);
 +}
 +
 +void
 +swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 +			   int nelems, enum dma_data_direction dir)
 +{
 +	swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_DEVICE);
++=======
+ 		return false;
+ 	}
+ 
+ 	return true;
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
  }
  
  /*
@@@ -867,19 -660,3 +881,22 @@@ swiotlb_dma_supported(struct device *hw
  {
  	return __phys_to_dma(hwdev, io_tlb_end - 1) <= mask;
  }
++<<<<<<< HEAD
 +
 +const struct dma_map_ops swiotlb_dma_ops = {
 +	.mapping_error		= dma_direct_mapping_error,
 +	.alloc			= dma_direct_alloc,
 +	.free			= dma_direct_free,
 +	.sync_single_for_cpu	= swiotlb_sync_single_for_cpu,
 +	.sync_single_for_device	= swiotlb_sync_single_for_device,
 +	.sync_sg_for_cpu	= swiotlb_sync_sg_for_cpu,
 +	.sync_sg_for_device	= swiotlb_sync_sg_for_device,
 +	.map_sg			= swiotlb_map_sg_attrs,
 +	.unmap_sg		= swiotlb_unmap_sg_attrs,
 +	.map_page		= swiotlb_map_page,
 +	.unmap_page		= swiotlb_unmap_page,
 +	.dma_supported		= dma_direct_supported,
 +};
 +EXPORT_SYMBOL(swiotlb_dma_ops);
++=======
++>>>>>>> 55897af63091 (dma-direct: merge swiotlb_dma_ops into the dma_direct code)
* Unmerged path arch/arm64/mm/dma-mapping.c
diff --git a/arch/ia64/hp/common/hwsw_iommu.c b/arch/ia64/hp/common/hwsw_iommu.c
index 58969039bed2..f40ca499b246 100644
--- a/arch/ia64/hp/common/hwsw_iommu.c
+++ b/arch/ia64/hp/common/hwsw_iommu.c
@@ -38,7 +38,7 @@ static inline int use_swiotlb(struct device *dev)
 const struct dma_map_ops *hwsw_dma_get_ops(struct device *dev)
 {
 	if (use_swiotlb(dev))
-		return &swiotlb_dma_ops;
+		return &dma_direct_ops;
 	return &sba_dma_ops;
 }
 EXPORT_SYMBOL(hwsw_dma_get_ops);
diff --git a/arch/ia64/hp/common/sba_iommu.c b/arch/ia64/hp/common/sba_iommu.c
index ee5b652d320a..5f94bca69b96 100644
--- a/arch/ia64/hp/common/sba_iommu.c
+++ b/arch/ia64/hp/common/sba_iommu.c
@@ -2080,8 +2080,6 @@ static int __init acpi_sba_ioc_init_acpi(void)
 /* This has to run before acpi_scan_init(). */
 arch_initcall(acpi_sba_ioc_init_acpi);
 
-extern const struct dma_map_ops swiotlb_dma_ops;
-
 static int __init
 sba_init(void)
 {
@@ -2095,7 +2093,7 @@ sba_init(void)
 	 * a successful kdump kernel boot is to use the swiotlb.
 	 */
 	if (is_kdump_kernel()) {
-		dma_ops = &swiotlb_dma_ops;
+		dma_ops = &dma_direct_ops;
 		if (swiotlb_late_init_with_default_size(64 * (1<<20)) != 0)
 			panic("Unable to initialize software I/O TLB:"
 				  " Try machvec=dig boot option");
@@ -2117,7 +2115,7 @@ sba_init(void)
 		 * If we didn't find something sba_iommu can claim, we
 		 * need to setup the swiotlb and switch to the dig machvec.
 		 */
-		dma_ops = &swiotlb_dma_ops;
+		dma_ops = &dma_direct_ops;
 		if (swiotlb_late_init_with_default_size(64 * (1<<20)) != 0)
 			panic("Unable to find SBA IOMMU or initialize "
 			      "software I/O TLB: Try machvec=dig boot option");
diff --git a/arch/ia64/kernel/dma-mapping.c b/arch/ia64/kernel/dma-mapping.c
index 7a471d8d67d4..667a0026030b 100644
--- a/arch/ia64/kernel/dma-mapping.c
+++ b/arch/ia64/kernel/dma-mapping.c
@@ -18,7 +18,7 @@ EXPORT_SYMBOL(dma_get_ops);
 #ifdef CONFIG_SWIOTLB
 void __init swiotlb_dma_init(void)
 {
-	dma_ops = &swiotlb_dma_ops;
+	dma_ops = &dma_direct_ops;
 	swiotlb_init(1);
 }
 #endif
* Unmerged path arch/mips/include/asm/dma-mapping.h
* Unmerged path arch/powerpc/kernel/dma-swiotlb.c
diff --git a/arch/riscv/include/asm/dma-mapping.h b/arch/riscv/include/asm/dma-mapping.h
deleted file mode 100644
index 8facc1c8fa05..000000000000
--- a/arch/riscv/include/asm/dma-mapping.h
+++ /dev/null
@@ -1,15 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-#ifndef _RISCV_ASM_DMA_MAPPING_H
-#define _RISCV_ASM_DMA_MAPPING_H 1
-
-#ifdef CONFIG_SWIOTLB
-#include <linux/swiotlb.h>
-static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
-{
-	return &swiotlb_dma_ops;
-}
-#else
-#include <asm-generic/dma-mapping.h>
-#endif /* CONFIG_SWIOTLB */
-
-#endif /* _RISCV_ASM_DMA_MAPPING_H */
diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c
index 661583662430..e6cfc800ac23 100644
--- a/arch/x86/kernel/pci-swiotlb.c
+++ b/arch/x86/kernel/pci-swiotlb.c
@@ -64,10 +64,8 @@ IOMMU_INIT(pci_swiotlb_detect_4gb,
 
 void __init pci_swiotlb_init(void)
 {
-	if (swiotlb) {
+	if (swiotlb)
 		swiotlb_init(0);
-		dma_ops = &swiotlb_dma_ops;
-	}
 }
 
 void __init pci_swiotlb_late_init(void)
diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c
index b2de398d1fd3..d7a7ed6dceb3 100644
--- a/arch/x86/mm/mem_encrypt.c
+++ b/arch/x86/mm/mem_encrypt.c
@@ -356,13 +356,6 @@ void __init mem_encrypt_init(void)
 	/* Call into SWIOTLB to update the SWIOTLB DMA buffers */
 	swiotlb_update_mem_attributes();
 
-	/*
-	 * With SEV, DMA operations cannot use encryption, we need to use
-	 * SWIOTLB to bounce buffer DMA operation.
-	 */
-	if (sev_active())
-		dma_ops = &swiotlb_dma_ops;
-
 	/*
 	 * With SEV, we need to unroll the rep string I/O instructions.
 	 */
diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c
index 7a5bafb76d77..3cdafea55ab6 100644
--- a/arch/x86/pci/sta2x11-fixup.c
+++ b/arch/x86/pci/sta2x11-fixup.c
@@ -168,7 +168,6 @@ static void sta2x11_setup_pdev(struct pci_dev *pdev)
 		return;
 	pci_set_consistent_dma_mask(pdev, STA2X11_AMBA_SIZE - 1);
 	pci_set_dma_mask(pdev, STA2X11_AMBA_SIZE - 1);
-	pdev->dev.dma_ops = &swiotlb_dma_ops;
 	pdev->dev.archdata.is_sta2x11 = true;
 
 	/* We must enable all devices as master, for audio DMA to work */
diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 861a4e72abde..183b818f0716 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -64,8 +64,20 @@ void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
 dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir,
 		unsigned long attrs);
+void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+		int nents, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
 int dma_direct_supported(struct device *dev, u64 mask);
 int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr);
 #endif /* _LINUX_DMA_DIRECT_H */
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index a387b59640a4..978fc5f193a9 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -16,8 +16,6 @@ enum swiotlb_force {
 	SWIOTLB_NO_FORCE,	/* swiotlb=noforce */
 };
 
-extern enum swiotlb_force swiotlb_force;
-
 /*
  * Maximum allowable number of contiguous slabs to map,
  * must be a power of 2.  What is the appropriate value ?
@@ -65,56 +63,44 @@ extern void swiotlb_tbl_sync_single(struct device *hwdev,
 				    size_t size, enum dma_data_direction dir,
 				    enum dma_sync_target target);
 
-/* Accessory functions. */
-
-extern dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,
-				   unsigned long offset, size_t size,
-				   enum dma_data_direction dir,
-				   unsigned long attrs);
-extern void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
-			       size_t size, enum dma_data_direction dir,
-			       unsigned long attrs);
-
-extern int
-swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,
-		     enum dma_data_direction dir,
-		     unsigned long attrs);
-
-extern void
-swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
-		       int nelems, enum dma_data_direction dir,
-		       unsigned long attrs);
-
-extern void
-swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
-			    size_t size, enum dma_data_direction dir);
-
-extern void
-swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
-			int nelems, enum dma_data_direction dir);
-
-extern void
-swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,
-			       size_t size, enum dma_data_direction dir);
-
-extern void
-swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
-			   int nelems, enum dma_data_direction dir);
-
 extern int
 swiotlb_dma_supported(struct device *hwdev, u64 mask);
 
 #ifdef CONFIG_SWIOTLB
-extern void __init swiotlb_exit(void);
+extern enum swiotlb_force swiotlb_force;
+extern phys_addr_t io_tlb_start, io_tlb_end;
+
+static inline bool is_swiotlb_buffer(phys_addr_t paddr)
+{
+	return paddr >= io_tlb_start && paddr < io_tlb_end;
+}
+
+bool swiotlb_map(struct device *dev, phys_addr_t *phys, dma_addr_t *dma_addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
+void __init swiotlb_exit(void);
 unsigned int swiotlb_max_segment(void);
 #else
-static inline void swiotlb_exit(void) { }
-static inline unsigned int swiotlb_max_segment(void) { return 0; }
-#endif
+#define swiotlb_force SWIOTLB_NO_FORCE
+static inline bool is_swiotlb_buffer(phys_addr_t paddr)
+{
+	return false;
+}
+static inline bool swiotlb_map(struct device *dev, phys_addr_t *phys,
+		dma_addr_t *dma_addr, size_t size, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+	return false;
+}
+static inline void swiotlb_exit(void)
+{
+}
+static inline unsigned int swiotlb_max_segment(void)
+{
+	return 0;
+}
+#endif /* CONFIG_SWIOTLB */
 
 extern void swiotlb_print_info(void);
 extern void swiotlb_set_max_segment(unsigned int);
 
-extern const struct dma_map_ops swiotlb_dma_ops;
-
 #endif /* __LINUX_SWIOTLB_H */
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/swiotlb.c
