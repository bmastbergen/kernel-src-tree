intel-iommu: mark intel_dma_ops static

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 02b4da5f84d157f6a27e05ca017bdb74bcf01bee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/02b4da5f.failed

ia64 currently explicitly assigns it to dma_ops, but that same work is
already done by intel_iommu_init a little later, so we can remove the
duplicate assignment and mark the variable static.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Tony Luck <tony.luck@intel.com>
(cherry picked from commit 02b4da5f84d157f6a27e05ca017bdb74bcf01bee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/kernel/pci-dma.c
diff --cc arch/ia64/kernel/pci-dma.c
index b5df084c0af4,fe988c49f01c..000000000000
--- a/arch/ia64/kernel/pci-dma.c
+++ b/arch/ia64/kernel/pci-dma.c
@@@ -42,56 -35,8 +40,59 @@@ static int __init pci_iommu_init(void
  /* Must execute after PCI subsystem */
  fs_initcall(pci_iommu_init);
  
 +void pci_iommu_shutdown(void)
 +{
 +	return;
 +}
 +
 +void __init
 +iommu_dma_init(void)
 +{
 +	return;
 +}
 +
 +int iommu_dma_supported(struct device *dev, u64 mask)
 +{
 +	/* Copied from i386. Doesn't make much sense, because it will
 +	   only work for pci_alloc_coherent.
 +	   The caller just has to use GFP_DMA in this case. */
 +	if (mask < DMA_BIT_MASK(24))
 +		return 0;
 +
 +	/* Tell the device to use SAC when IOMMU force is on.  This
 +	   allows the driver to use cheaper accesses in some cases.
 +
 +	   Problem with this is that if we overflow the IOMMU area and
 +	   return DAC as fallback address the device may not handle it
 +	   correctly.
 +
 +	   As a special case some controllers have a 39bit address
 +	   mode that is as efficient as 32bit (aic79xx). Don't force
 +	   SAC for these.  Assume all masks <= 40 bits are of this
 +	   type. Normally this doesn't make any difference, but gives
 +	   more gentle handling of IOMMU overflow. */
 +	if (iommu_sac_force && (mask >= DMA_BIT_MASK(40))) {
 +		dev_info(dev, "Force SAC with mask %llx\n", mask);
 +		return 0;
 +	}
 +
 +	return 1;
 +}
 +EXPORT_SYMBOL(iommu_dma_supported);
 +
  void __init pci_iommu_alloc(void)
  {
++<<<<<<< HEAD
 +	dma_ops = &intel_dma_ops;
 +
 +	intel_dma_ops.sync_single_for_cpu = machvec_dma_sync_single;
 +	intel_dma_ops.sync_sg_for_cpu = machvec_dma_sync_sg;
 +	intel_dma_ops.sync_single_for_device = machvec_dma_sync_single;
 +	intel_dma_ops.sync_sg_for_device = machvec_dma_sync_sg;
 +	intel_dma_ops.dma_supported = iommu_dma_supported;
 +
++=======
++>>>>>>> 02b4da5f84d1 (intel-iommu: mark intel_dma_ops static)
  	/*
  	 * The order of these functions is important for
  	 * fall-back/fail-over reasons
* Unmerged path arch/ia64/kernel/pci-dma.c
diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c
index 16675eadec1f..c30c8820cc3d 100644
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@ -3890,7 +3890,7 @@ static int intel_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return !dma_addr;
 }
 
-const struct dma_map_ops intel_dma_ops = {
+static const struct dma_map_ops intel_dma_ops = {
 	.alloc = intel_alloc_coherent,
 	.free = intel_free_coherent,
 	.map_sg = intel_map_sg,
