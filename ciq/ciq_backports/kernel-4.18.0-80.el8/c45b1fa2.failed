nvme-pci: fix nvme_setup_irqs()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Ming Lei <ming.lei@redhat.com>
commit c45b1fa2433c65e44bdf48f513cb37289f3116b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/c45b1fa2.failed

When -ENOSPC is returned from pci_alloc_irq_vectors_affinity(),
we still try to allocate multiple irq vectors again, so irq queues
covers the admin queue actually. But we don't consider that, then
number of the allocated irq vector may be same with sum of
io_queues[HCTX_TYPE_DEFAULT] and io_queues[HCTX_TYPE_READ], this way
is obviously wrong, and finally breaks nvme_pci_map_queues(), and
warning from pci_irq_get_affinity() is triggered.

IRQ queues should cover admin queues, this patch makes this
point explicitely in nvme_calc_io_queues().

We got severl boot failure internal report on aarch64, so please
consider to fix it in v4.20.

Fixes: 6451fe73fa0f ("nvme: fix irq vs io_queue calculations")
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Tested-by: fin4478 <fin4478@hotmail.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c45b1fa2433c65e44bdf48f513cb37289f3116b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/pci.c
diff --cc drivers/nvme/host/pci.c
index e63d03795df8,89f9dd72135a..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -2038,56 -2041,40 +2038,83 @@@ static int nvme_setup_host_mem(struct n
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void nvme_calc_io_queues(struct nvme_dev *dev, unsigned int nr_io_queues)
++=======
+ /* irq_queues covers admin queue */
+ static void nvme_calc_io_queues(struct nvme_dev *dev, unsigned int irq_queues)
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  {
  	unsigned int this_w_queues = write_queues;
 +	unsigned int this_p_queues = poll_queues;
  
+ 	WARN_ON(!irq_queues);
+ 
  	/*
- 	 * Setup read/write queue split
+ 	 * Setup read/write queue split, assign admin queue one independent
+ 	 * irq vector if irq_queues is > 1.
  	 */
++<<<<<<< HEAD
 +	if (nr_io_queues == 1) {
 +		dev->io_queues[NVMEQ_TYPE_READ] = 1;
 +		dev->io_queues[NVMEQ_TYPE_WRITE] = 0;
 +		dev->io_queues[NVMEQ_TYPE_POLL] = 0;
++=======
+ 	if (irq_queues <= 2) {
+ 		dev->io_queues[HCTX_TYPE_DEFAULT] = 1;
+ 		dev->io_queues[HCTX_TYPE_READ] = 0;
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  		return;
  	}
  
 +	/*
 +	 * Configure number of poll queues, if set
 +	 */
 +	if (this_p_queues) {
 +		/*
 +		 * We need at least one queue left. With just one queue, we'll
 +		 * have a single shared read/write set.
 +		 */
 +		if (this_p_queues >= nr_io_queues) {
 +			this_w_queues = 0;
 +			this_p_queues = nr_io_queues - 1;
 +		}
 +
 +		dev->io_queues[NVMEQ_TYPE_POLL] = this_p_queues;
 +		nr_io_queues -= this_p_queues;
 +	} else
 +		dev->io_queues[NVMEQ_TYPE_POLL] = 0;
 +
  	/*
  	 * If 'write_queues' is set, ensure it leaves room for at least
- 	 * one read queue
+ 	 * one read queue and one admin queue
  	 */
++<<<<<<< HEAD
 +	if (this_w_queues >= nr_io_queues)
 +		this_w_queues = nr_io_queues - 1;
++=======
+ 	if (this_w_queues >= irq_queues)
+ 		this_w_queues = irq_queues - 2;
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  
  	/*
  	 * If 'write_queues' is set to zero, reads and writes will share
  	 * a queue set.
  	 */
  	if (!this_w_queues) {
++<<<<<<< HEAD
 +		dev->io_queues[NVMEQ_TYPE_WRITE] = 0;
 +		dev->io_queues[NVMEQ_TYPE_READ] = nr_io_queues;
 +	} else {
 +		dev->io_queues[NVMEQ_TYPE_WRITE] = this_w_queues;
 +		dev->io_queues[NVMEQ_TYPE_READ] = nr_io_queues - this_w_queues;
++=======
+ 		dev->io_queues[HCTX_TYPE_DEFAULT] = irq_queues - 1;
+ 		dev->io_queues[HCTX_TYPE_READ] = 0;
+ 	} else {
+ 		dev->io_queues[HCTX_TYPE_DEFAULT] = this_w_queues;
+ 		dev->io_queues[HCTX_TYPE_READ] = irq_queues - this_w_queues - 1;
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  	}
  }
  
@@@ -2101,6 -2088,20 +2128,23 @@@ static int nvme_setup_irqs(struct nvme_
  		.sets = irq_sets,
  	};
  	int result = 0;
++<<<<<<< HEAD
++=======
+ 	unsigned int irq_queues, this_p_queues;
+ 
+ 	/*
+ 	 * Poll queues don't need interrupts, but we need at least one IO
+ 	 * queue left over for non-polled IO.
+ 	 */
+ 	this_p_queues = poll_queues;
+ 	if (this_p_queues >= nr_io_queues) {
+ 		this_p_queues = nr_io_queues - 1;
+ 		irq_queues = 1;
+ 	} else {
+ 		irq_queues = nr_io_queues - this_p_queues + 1;
+ 	}
+ 	dev->io_queues[HCTX_TYPE_POLL] = this_p_queues;
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  
  	/*
  	 * For irq sets, we have to ask for minvec == maxvec. This passes
@@@ -2118,12 -2119,13 +2162,18 @@@
  		 * If we got a failure and we're down to asking for just
  		 * 1 + 1 queues, just ask for a single vector. We'll share
  		 * that between the single IO queue and the admin queue.
+ 		 * Otherwise, we assign one independent vector to admin queue.
  		 */
++<<<<<<< HEAD
 +		if (!(result < 0 && nr_io_queues == 1))
 +			nr_io_queues = irq_sets[0] + irq_sets[1] + 1;
++=======
+ 		if (irq_queues > 1)
+ 			irq_queues = irq_sets[0] + irq_sets[1] + 1;
++>>>>>>> c45b1fa2433c (nvme-pci: fix nvme_setup_irqs())
  
 -		result = pci_alloc_irq_vectors_affinity(pdev, irq_queues,
 -				irq_queues,
 +		result = pci_alloc_irq_vectors_affinity(pdev, nr_io_queues,
 +				nr_io_queues,
  				PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
  
  		/*
* Unmerged path drivers/nvme/host/pci.c
