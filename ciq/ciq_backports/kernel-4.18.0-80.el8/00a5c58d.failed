KVM: PPC: Make iommu_table::it_userspace big endian

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 00a5c58d9499bd0c290b57205f43a70f2e69d3f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/00a5c58d.failed

We are going to reuse multilevel TCE code for the userspace copy of
the TCE table and since it is big endian, let's make the copy big endian
too.

	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Acked-by: Paul Mackerras <paulus@ozlabs.org>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 00a5c58d9499bd0c290b57205f43a70f2e69d3f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vfio/vfio_iommu_spapr_tce.c
diff --cc drivers/vfio/vfio_iommu_spapr_tce.c
index 7cd63b0c1a46,8ab124a67311..000000000000
--- a/drivers/vfio/vfio_iommu_spapr_tce.c
+++ b/drivers/vfio/vfio_iommu_spapr_tce.c
@@@ -487,11 -487,11 +487,16 @@@ static void tce_iommu_unuse_page_v2(str
  	if (!pua)
  		return;
  
++<<<<<<< HEAD
 +	ret = tce_iommu_prereg_ua_to_hpa(container, *pua, tbl->it_page_shift,
 +			&hpa, &mem);
++=======
+ 	ret = tce_iommu_prereg_ua_to_hpa(container, be64_to_cpu(*pua),
+ 			IOMMU_PAGE_SIZE(tbl), &hpa, &mem);
++>>>>>>> 00a5c58d9499 (KVM: PPC: Make iommu_table::it_userspace big endian)
  	if (ret)
- 		pr_debug("%s: tce %lx at #%lx was not cached, ret=%d\n",
- 				__func__, *pua, entry, ret);
+ 		pr_debug("%s: tce %llx at #%lx was not cached, ret=%d\n",
+ 				__func__, be64_to_cpu(*pua), entry, ret);
  	if (mem)
  		mm_iommu_mapped_dec(mem);
  
@@@ -607,11 -607,10 +612,10 @@@ static long tce_iommu_build_v2(struct t
  
  	for (i = 0; i < pages; ++i) {
  		struct mm_iommu_table_group_mem_t *mem = NULL;
- 		unsigned long *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl,
- 				entry + i);
+ 		__be64 *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry + i);
  
  		ret = tce_iommu_prereg_ua_to_hpa(container,
 -				tce, IOMMU_PAGE_SIZE(tbl), &hpa, &mem);
 +				tce, tbl->it_page_shift, &hpa, &mem);
  		if (ret)
  			break;
  
diff --git a/arch/powerpc/include/asm/iommu.h b/arch/powerpc/include/asm/iommu.h
index 20febe0b7f32..803ac70ecedf 100644
--- a/arch/powerpc/include/asm/iommu.h
+++ b/arch/powerpc/include/asm/iommu.h
@@ -117,7 +117,7 @@ struct iommu_table {
 	unsigned long *it_map;       /* A simple allocation bitmap for now */
 	unsigned long  it_page_shift;/* table iommu page size */
 	struct list_head it_group_list;/* List of iommu_table_group_link */
-	unsigned long *it_userspace; /* userspace view of the table */
+	__be64 *it_userspace; /* userspace view of the table */
 	struct iommu_table_ops *it_ops;
 	struct kref    it_kref;
 };
diff --git a/arch/powerpc/kvm/book3s_64_vio.c b/arch/powerpc/kvm/book3s_64_vio.c
index 8167ce8e0cdd..6f34eddba88c 100644
--- a/arch/powerpc/kvm/book3s_64_vio.c
+++ b/arch/powerpc/kvm/book3s_64_vio.c
@@ -377,19 +377,19 @@ static long kvmppc_tce_iommu_mapped_dec(struct kvm *kvm,
 {
 	struct mm_iommu_table_group_mem_t *mem = NULL;
 	const unsigned long pgsize = 1ULL << tbl->it_page_shift;
-	unsigned long *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
+	__be64 *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
 
 	if (!pua)
 		/* it_userspace allocation might be delayed */
 		return H_TOO_HARD;
 
-	mem = mm_iommu_lookup(kvm->mm, *pua, pgsize);
+	mem = mm_iommu_lookup(kvm->mm, be64_to_cpu(*pua), pgsize);
 	if (!mem)
 		return H_TOO_HARD;
 
 	mm_iommu_mapped_dec(mem);
 
-	*pua = 0;
+	*pua = cpu_to_be64(0);
 
 	return H_SUCCESS;
 }
@@ -436,7 +436,8 @@ long kvmppc_tce_iommu_do_map(struct kvm *kvm, struct iommu_table *tbl,
 		enum dma_data_direction dir)
 {
 	long ret;
-	unsigned long hpa, *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
+	unsigned long hpa;
+	__be64 *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
 	struct mm_iommu_table_group_mem_t *mem;
 
 	if (!pua)
@@ -463,7 +464,7 @@ long kvmppc_tce_iommu_do_map(struct kvm *kvm, struct iommu_table *tbl,
 	if (dir != DMA_NONE)
 		kvmppc_tce_iommu_mapped_dec(kvm, tbl, entry);
 
-	*pua = ua;
+	*pua = cpu_to_be64(ua);
 
 	return 0;
 }
diff --git a/arch/powerpc/kvm/book3s_64_vio_hv.c b/arch/powerpc/kvm/book3s_64_vio_hv.c
index 5b298f5a1a14..841aef714929 100644
--- a/arch/powerpc/kvm/book3s_64_vio_hv.c
+++ b/arch/powerpc/kvm/book3s_64_vio_hv.c
@@ -200,7 +200,7 @@ static long kvmppc_rm_tce_iommu_mapped_dec(struct kvm *kvm,
 {
 	struct mm_iommu_table_group_mem_t *mem = NULL;
 	const unsigned long pgsize = 1ULL << tbl->it_page_shift;
-	unsigned long *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
+	__be64 *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
 
 	if (!pua)
 		/* it_userspace allocation might be delayed */
@@ -210,13 +210,13 @@ static long kvmppc_rm_tce_iommu_mapped_dec(struct kvm *kvm,
 	if (WARN_ON_ONCE_RM(!pua))
 		return H_HARDWARE;
 
-	mem = mm_iommu_lookup_rm(kvm->mm, *pua, pgsize);
+	mem = mm_iommu_lookup_rm(kvm->mm, be64_to_cpu(*pua), pgsize);
 	if (!mem)
 		return H_TOO_HARD;
 
 	mm_iommu_mapped_dec(mem);
 
-	*pua = 0;
+	*pua = cpu_to_be64(0);
 
 	return H_SUCCESS;
 }
@@ -268,7 +268,7 @@ static long kvmppc_rm_tce_iommu_do_map(struct kvm *kvm, struct iommu_table *tbl,
 {
 	long ret;
 	unsigned long hpa = 0;
-	unsigned long *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
+	__be64 *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry);
 	struct mm_iommu_table_group_mem_t *mem;
 
 	if (!pua)
@@ -303,7 +303,7 @@ static long kvmppc_rm_tce_iommu_do_map(struct kvm *kvm, struct iommu_table *tbl,
 	if (dir != DMA_NONE)
 		kvmppc_rm_tce_iommu_mapped_dec(kvm, tbl, entry);
 
-	*pua = ua;
+	*pua = cpu_to_be64(ua);
 
 	return 0;
 }
* Unmerged path drivers/vfio/vfio_iommu_spapr_tce.c
