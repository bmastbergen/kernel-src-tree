blk-mq: ensure mq_ops ->poll() is entered at least once

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit aa61bec30eca11816789dc25c2090366b0ccfaf8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/aa61bec3.failed

Right now we immediately bail if need_resched() is true, but
we need to do at least one loop in case we have entries waiting.
So just invert the need_resched() check, putting it at the
bottom of the loop.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit aa61bec30eca11816789dc25c2090366b0ccfaf8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 52f967bfb7fc,ba3c7b6476b7..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -3361,7 -3349,56 +3361,60 @@@ static bool blk_mq_poll(struct request_
  			return false;
  	}
  
++<<<<<<< HEAD
 +	return __blk_mq_poll(hctx, rq);
++=======
+ 	return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ }
+ 
+ static int blk_mq_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	long state;
+ 
+ 	if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ 		return 0;
+ 
+ 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
+ 
+ 	/*
+ 	 * If we sleep, have the caller restart the poll loop to reset
+ 	 * the state. Like for the other success return cases, the
+ 	 * caller is responsible for checking if the IO completed. If
+ 	 * the IO isn't complete, we'll get called again and will go
+ 	 * straight to the busy poll loop.
+ 	 */
+ 	if (blk_mq_poll_hybrid(q, hctx, cookie))
+ 		return 1;
+ 
+ 	hctx->poll_considered++;
+ 
+ 	state = current->state;
+ 	do {
+ 		int ret;
+ 
+ 		hctx->poll_invoked++;
+ 
+ 		ret = q->mq_ops->poll(hctx);
+ 		if (ret > 0) {
+ 			hctx->poll_success++;
+ 			__set_current_state(TASK_RUNNING);
+ 			return ret;
+ 		}
+ 
+ 		if (signal_pending_state(state, current))
+ 			__set_current_state(TASK_RUNNING);
+ 
+ 		if (current->state == TASK_RUNNING)
+ 			return 1;
+ 		if (ret < 0 || !spin)
+ 			break;
+ 		cpu_relax();
+ 	} while (!need_resched());
+ 
+ 	__set_current_state(TASK_RUNNING);
+ 	return 0;
++>>>>>>> aa61bec30eca (blk-mq: ensure mq_ops ->poll() is entered at least once)
  }
  
  unsigned int blk_mq_rq_cpu(struct request *rq)
* Unmerged path block/blk-mq.c
