dma-direct: remove the mapping_error dma_map_ops method

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit b0cbeae4944924640bf550b75487729a20204c14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/b0cbeae4.failed

The dma-direct code already returns (~(dma_addr_t)0x0) on mapping
failures, so we can switch over to returning DMA_MAPPING_ERROR and let
the core dma-mapping code handle the rest.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b0cbeae4944924640bf550b75487729a20204c14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index a0ae9e05e04e,308f88a750c8..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -141,10 -285,14 +141,17 @@@ dma_addr_t dma_direct_map_page(struct d
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs)
  {
 -	phys_addr_t phys = page_to_phys(page) + offset;
 -	dma_addr_t dma_addr = phys_to_dma(dev, phys);
 +	dma_addr_t dma_addr = phys_to_dma(dev, page_to_phys(page)) + offset;
  
  	if (!check_addr(dev, dma_addr, size, __func__))
++<<<<<<< HEAD
 +		return DIRECT_MAPPING_ERROR;
++=======
+ 		return DMA_MAPPING_ERROR;
+ 
+ 	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+ 		dma_direct_sync_single_for_device(dev, dma_addr, size, dir);
++>>>>>>> b0cbeae49449 (dma-direct: remove the mapping_error dma_map_ops method)
  	return dma_addr;
  }
  
@@@ -166,41 -314,46 +173,40 @@@ int dma_direct_map_sg(struct device *de
  	return nents;
  }
  
 -/*
 - * Because 32-bit DMA masks are so common we expect every architecture to be
 - * able to satisfy them - either by not supporting more physical memory, or by
 - * providing a ZONE_DMA32.  If neither is the case, the architecture needs to
 - * use an IOMMU instead of the direct mapping.
 - */
  int dma_direct_supported(struct device *dev, u64 mask)
  {
 -	u64 min_mask;
 -
 -	if (IS_ENABLED(CONFIG_ZONE_DMA))
 -		min_mask = DMA_BIT_MASK(ARCH_ZONE_DMA_BITS);
 -	else
 -		min_mask = DMA_BIT_MASK(32);
 -
 -	min_mask = min_t(u64, min_mask, (max_pfn - 1) << PAGE_SHIFT);
 -
 -	return mask >= phys_to_dma(dev, min_mask);
 +#ifdef CONFIG_ZONE_DMA
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(ARCH_ZONE_DMA_BITS)))
 +		return 0;
 +#else
 +	/*
 +	 * Because 32-bit DMA masks are so common we expect every architecture
 +	 * to be able to satisfy them - either by not supporting more physical
 +	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the
 +	 * architecture needs to use an IOMMU instead of the direct mapping.
 +	 */
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(32)))
 +		return 0;
 +#endif
 +	/*
 +	 * Upstream PCI/PCIe bridges or SoC interconnects may not carry
 +	 * as many DMA address bits as the device itself supports.
 +	 */
 +	if (dev->bus_dma_mask && mask > dev->bus_dma_mask)
 +		return 0;
 +	return 1;
  }
  
- int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr)
- {
- 	return dma_addr == DIRECT_MAPPING_ERROR;
- }
- 
  const struct dma_map_ops dma_direct_ops = {
  	.alloc			= dma_direct_alloc,
  	.free			= dma_direct_free,
  	.map_page		= dma_direct_map_page,
  	.map_sg			= dma_direct_map_sg,
 -#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE)
 -	.sync_single_for_device	= dma_direct_sync_single_for_device,
 -	.sync_sg_for_device	= dma_direct_sync_sg_for_device,
 -#endif
 -#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
 -    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
 -	.sync_single_for_cpu	= dma_direct_sync_single_for_cpu,
 -	.sync_sg_for_cpu	= dma_direct_sync_sg_for_cpu,
 -	.unmap_page		= dma_direct_unmap_page,
 -	.unmap_sg		= dma_direct_unmap_sg,
 -#endif
 -	.get_required_mask	= dma_direct_get_required_mask,
  	.dma_supported		= dma_direct_supported,
++<<<<<<< HEAD
 +	.mapping_error		= dma_direct_mapping_error,
++=======
+ 	.cache_sync		= arch_dma_cache_sync,
++>>>>>>> b0cbeae49449 (dma-direct: remove the mapping_error dma_map_ops method)
  };
  EXPORT_SYMBOL(dma_direct_ops);
diff --git a/arch/powerpc/kernel/dma-swiotlb.c b/arch/powerpc/kernel/dma-swiotlb.c
index 5fc335f4d9cd..3d8df2cf8be9 100644
--- a/arch/powerpc/kernel/dma-swiotlb.c
+++ b/arch/powerpc/kernel/dma-swiotlb.c
@@ -59,7 +59,6 @@ const struct dma_map_ops powerpc_swiotlb_dma_ops = {
 	.sync_single_for_device = swiotlb_sync_single_for_device,
 	.sync_sg_for_cpu = swiotlb_sync_sg_for_cpu,
 	.sync_sg_for_device = swiotlb_sync_sg_for_device,
-	.mapping_error = dma_direct_mapping_error,
 	.get_required_mask = swiotlb_powerpc_get_required,
 };
 
diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 861a4e72abde..7c58792c3481 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -5,8 +5,6 @@
 #include <linux/dma-mapping.h>
 #include <linux/mem_encrypt.h>
 
-#define DIRECT_MAPPING_ERROR		(~(dma_addr_t)0)
-
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else
@@ -67,5 +65,4 @@ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
-int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr);
 #endif /* _LINUX_DMA_DIRECT_H */
* Unmerged path kernel/dma/direct.c
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 5fb3413ff1ee..cc49e20ea6f2 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -631,21 +631,21 @@ static dma_addr_t swiotlb_bounce_page(struct device *dev, phys_addr_t *phys,
 	if (unlikely(swiotlb_force == SWIOTLB_NO_FORCE)) {
 		dev_warn_ratelimited(dev,
 			"Cannot do DMA to address %pa\n", phys);
-		return DIRECT_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 	}
 
 	/* Oh well, have to allocate and map a bounce buffer. */
 	*phys = swiotlb_tbl_map_single(dev, __phys_to_dma(dev, io_tlb_start),
 			*phys, size, dir, attrs);
 	if (*phys == SWIOTLB_MAP_ERROR)
-		return DIRECT_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 
 	/* Ensure that the address returned is DMA'ble */
 	dma_addr = __phys_to_dma(dev, *phys);
 	if (unlikely(!dma_capable(dev, dma_addr, size))) {
 		swiotlb_tbl_unmap_single(dev, *phys, size, dir,
 			attrs | DMA_ATTR_SKIP_CPU_SYNC);
-		return DIRECT_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 	}
 
 	return dma_addr;
@@ -680,7 +680,7 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,
 
 	if (!dev_is_dma_coherent(dev) &&
 	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0 &&
-	    dev_addr != DIRECT_MAPPING_ERROR)
+	    dev_addr != DMA_MAPPING_ERROR)
 		arch_sync_dma_for_device(dev, phys, size, dir);
 
 	return dev_addr;
@@ -789,7 +789,7 @@ swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl, int nelems,
 	for_each_sg(sgl, sg, nelems, i) {
 		sg->dma_address = swiotlb_map_page(dev, sg_page(sg), sg->offset,
 				sg->length, dir, attrs);
-		if (sg->dma_address == DIRECT_MAPPING_ERROR)
+		if (sg->dma_address == DMA_MAPPING_ERROR)
 			goto out_error;
 		sg_dma_len(sg) = sg->length;
 	}
@@ -869,7 +869,6 @@ swiotlb_dma_supported(struct device *hwdev, u64 mask)
 }
 
 const struct dma_map_ops swiotlb_dma_ops = {
-	.mapping_error		= dma_direct_mapping_error,
 	.alloc			= dma_direct_alloc,
 	.free			= dma_direct_free,
 	.sync_single_for_cpu	= swiotlb_sync_single_for_cpu,
