KVM: PPC: Book3S HV: Nested guest entry via hypercall

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul Mackerras <paulus@ozlabs.org>
commit 360cae313702cdd0b90f82c261a8302fecef030a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/360cae31.failed

This adds a new hypercall, H_ENTER_NESTED, which is used by a nested
hypervisor to enter one of its nested guests.  The hypercall supplies
register values in two structs.  Those values are copied by the level 0
(L0) hypervisor (the one which is running in hypervisor mode) into the
vcpu struct of the L1 guest, and then the guest is run until an
interrupt or error occurs which needs to be reported to L1 via the
hypercall return value.

Currently this assumes that the L0 and L1 hypervisors are the same
endianness, and the structs passed as arguments are in native
endianness.  If they are of different endianness, the version number
check will fail and the hcall will be rejected.

Nested hypervisors do not support indep_threads_mode=N, so this adds
code to print a warning message if the administrator has set
indep_threads_mode=N, and treat it as Y.

	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 360cae313702cdd0b90f82c261a8302fecef030a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv.c
diff --cc arch/powerpc/kvm/book3s_hv.c
index c786bc608f50,7a2f344af4f8..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -3096,6 -3201,293 +3201,296 @@@ static noinline void kvmppc_run_core(st
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Load up hypervisor-mode registers on P9.
+  */
+ static int kvmhv_load_hv_regs_and_go(struct kvm_vcpu *vcpu, u64 time_limit,
+ 				     unsigned long lpcr)
+ {
+ 	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+ 	s64 hdec;
+ 	u64 tb, purr, spurr;
+ 	int trap;
+ 	unsigned long host_hfscr = mfspr(SPRN_HFSCR);
+ 	unsigned long host_ciabr = mfspr(SPRN_CIABR);
+ 	unsigned long host_dawr = mfspr(SPRN_DAWR);
+ 	unsigned long host_dawrx = mfspr(SPRN_DAWRX);
+ 	unsigned long host_psscr = mfspr(SPRN_PSSCR);
+ 	unsigned long host_pidr = mfspr(SPRN_PID);
+ 
+ 	hdec = time_limit - mftb();
+ 	if (hdec < 0)
+ 		return BOOK3S_INTERRUPT_HV_DECREMENTER;
+ 	mtspr(SPRN_HDEC, hdec);
+ 
+ 	if (vc->tb_offset) {
+ 		u64 new_tb = mftb() + vc->tb_offset;
+ 		mtspr(SPRN_TBU40, new_tb);
+ 		tb = mftb();
+ 		if ((tb & 0xffffff) < (new_tb & 0xffffff))
+ 			mtspr(SPRN_TBU40, new_tb + 0x1000000);
+ 		vc->tb_offset_applied = vc->tb_offset;
+ 	}
+ 
+ 	if (vc->pcr)
+ 		mtspr(SPRN_PCR, vc->pcr);
+ 	mtspr(SPRN_DPDES, vc->dpdes);
+ 	mtspr(SPRN_VTB, vc->vtb);
+ 
+ 	local_paca->kvm_hstate.host_purr = mfspr(SPRN_PURR);
+ 	local_paca->kvm_hstate.host_spurr = mfspr(SPRN_SPURR);
+ 	mtspr(SPRN_PURR, vcpu->arch.purr);
+ 	mtspr(SPRN_SPURR, vcpu->arch.spurr);
+ 
+ 	if (cpu_has_feature(CPU_FTR_DAWR)) {
+ 		mtspr(SPRN_DAWR, vcpu->arch.dawr);
+ 		mtspr(SPRN_DAWRX, vcpu->arch.dawrx);
+ 	}
+ 	mtspr(SPRN_CIABR, vcpu->arch.ciabr);
+ 	mtspr(SPRN_IC, vcpu->arch.ic);
+ 	mtspr(SPRN_PID, vcpu->arch.pid);
+ 
+ 	mtspr(SPRN_PSSCR, vcpu->arch.psscr | PSSCR_EC |
+ 	      (local_paca->kvm_hstate.fake_suspend << PSSCR_FAKE_SUSPEND_LG));
+ 
+ 	mtspr(SPRN_HFSCR, vcpu->arch.hfscr);
+ 
+ 	mtspr(SPRN_SPRG0, vcpu->arch.shregs.sprg0);
+ 	mtspr(SPRN_SPRG1, vcpu->arch.shregs.sprg1);
+ 	mtspr(SPRN_SPRG2, vcpu->arch.shregs.sprg2);
+ 	mtspr(SPRN_SPRG3, vcpu->arch.shregs.sprg3);
+ 
+ 	mtspr(SPRN_AMOR, ~0UL);
+ 
+ 	mtspr(SPRN_LPCR, lpcr);
+ 	isync();
+ 
+ 	kvmppc_xive_push_vcpu(vcpu);
+ 
+ 	mtspr(SPRN_SRR0, vcpu->arch.shregs.srr0);
+ 	mtspr(SPRN_SRR1, vcpu->arch.shregs.srr1);
+ 
+ 	trap = __kvmhv_vcpu_entry_p9(vcpu);
+ 
+ 	/* Advance host PURR/SPURR by the amount used by guest */
+ 	purr = mfspr(SPRN_PURR);
+ 	spurr = mfspr(SPRN_SPURR);
+ 	mtspr(SPRN_PURR, local_paca->kvm_hstate.host_purr +
+ 	      purr - vcpu->arch.purr);
+ 	mtspr(SPRN_SPURR, local_paca->kvm_hstate.host_spurr +
+ 	      spurr - vcpu->arch.spurr);
+ 	vcpu->arch.purr = purr;
+ 	vcpu->arch.spurr = spurr;
+ 
+ 	vcpu->arch.ic = mfspr(SPRN_IC);
+ 	vcpu->arch.pid = mfspr(SPRN_PID);
+ 	vcpu->arch.psscr = mfspr(SPRN_PSSCR) & PSSCR_GUEST_VIS;
+ 
+ 	vcpu->arch.shregs.sprg0 = mfspr(SPRN_SPRG0);
+ 	vcpu->arch.shregs.sprg1 = mfspr(SPRN_SPRG1);
+ 	vcpu->arch.shregs.sprg2 = mfspr(SPRN_SPRG2);
+ 	vcpu->arch.shregs.sprg3 = mfspr(SPRN_SPRG3);
+ 
+ 	mtspr(SPRN_PSSCR, host_psscr);
+ 	mtspr(SPRN_HFSCR, host_hfscr);
+ 	mtspr(SPRN_CIABR, host_ciabr);
+ 	mtspr(SPRN_DAWR, host_dawr);
+ 	mtspr(SPRN_DAWRX, host_dawrx);
+ 	mtspr(SPRN_PID, host_pidr);
+ 
+ 	/*
+ 	 * Since this is radix, do a eieio; tlbsync; ptesync sequence in
+ 	 * case we interrupted the guest between a tlbie and a ptesync.
+ 	 */
+ 	asm volatile("eieio; tlbsync; ptesync");
+ 
+ 	mtspr(SPRN_LPID, vcpu->kvm->arch.host_lpid);	/* restore host LPID */
+ 	isync();
+ 
+ 	vc->dpdes = mfspr(SPRN_DPDES);
+ 	vc->vtb = mfspr(SPRN_VTB);
+ 	mtspr(SPRN_DPDES, 0);
+ 	if (vc->pcr)
+ 		mtspr(SPRN_PCR, 0);
+ 
+ 	if (vc->tb_offset_applied) {
+ 		u64 new_tb = mftb() - vc->tb_offset_applied;
+ 		mtspr(SPRN_TBU40, new_tb);
+ 		tb = mftb();
+ 		if ((tb & 0xffffff) < (new_tb & 0xffffff))
+ 			mtspr(SPRN_TBU40, new_tb + 0x1000000);
+ 		vc->tb_offset_applied = 0;
+ 	}
+ 
+ 	mtspr(SPRN_HDEC, 0x7fffffff);
+ 	mtspr(SPRN_LPCR, vcpu->kvm->arch.host_lpcr);
+ 
+ 	return trap;
+ }
+ 
+ /*
+  * Virtual-mode guest entry for POWER9 and later when the host and
+  * guest are both using the radix MMU.  The LPIDR has already been set.
+  */
+ int kvmhv_p9_guest_entry(struct kvm_vcpu *vcpu, u64 time_limit,
+ 			 unsigned long lpcr)
+ {
+ 	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+ 	unsigned long host_dscr = mfspr(SPRN_DSCR);
+ 	unsigned long host_tidr = mfspr(SPRN_TIDR);
+ 	unsigned long host_iamr = mfspr(SPRN_IAMR);
+ 	s64 dec;
+ 	u64 tb;
+ 	int trap, save_pmu;
+ 
+ 	dec = mfspr(SPRN_DEC);
+ 	tb = mftb();
+ 	if (dec < 512)
+ 		return BOOK3S_INTERRUPT_HV_DECREMENTER;
+ 	local_paca->kvm_hstate.dec_expires = dec + tb;
+ 	if (local_paca->kvm_hstate.dec_expires < time_limit)
+ 		time_limit = local_paca->kvm_hstate.dec_expires;
+ 
+ 	vcpu->arch.ceded = 0;
+ 
+ 	kvmhv_save_host_pmu();		/* saves it to PACA kvm_hstate */
+ 
+ 	kvmppc_subcore_enter_guest();
+ 
+ 	vc->entry_exit_map = 1;
+ 	vc->in_guest = 1;
+ 
+ 	if (vcpu->arch.vpa.pinned_addr) {
+ 		struct lppaca *lp = vcpu->arch.vpa.pinned_addr;
+ 		u32 yield_count = be32_to_cpu(lp->yield_count) + 1;
+ 		lp->yield_count = cpu_to_be32(yield_count);
+ 		vcpu->arch.vpa.dirty = 1;
+ 	}
+ 
+ 	if (cpu_has_feature(CPU_FTR_TM) ||
+ 	    cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST))
+ 		kvmppc_restore_tm_hv(vcpu, vcpu->arch.shregs.msr, true);
+ 
+ 	kvmhv_load_guest_pmu(vcpu);
+ 
+ 	msr_check_and_set(MSR_FP | MSR_VEC | MSR_VSX);
+ 	load_fp_state(&vcpu->arch.fp);
+ #ifdef CONFIG_ALTIVEC
+ 	load_vr_state(&vcpu->arch.vr);
+ #endif
+ 
+ 	mtspr(SPRN_DSCR, vcpu->arch.dscr);
+ 	mtspr(SPRN_IAMR, vcpu->arch.iamr);
+ 	mtspr(SPRN_PSPB, vcpu->arch.pspb);
+ 	mtspr(SPRN_FSCR, vcpu->arch.fscr);
+ 	mtspr(SPRN_TAR, vcpu->arch.tar);
+ 	mtspr(SPRN_EBBHR, vcpu->arch.ebbhr);
+ 	mtspr(SPRN_EBBRR, vcpu->arch.ebbrr);
+ 	mtspr(SPRN_BESCR, vcpu->arch.bescr);
+ 	mtspr(SPRN_WORT, vcpu->arch.wort);
+ 	mtspr(SPRN_TIDR, vcpu->arch.tid);
+ 	mtspr(SPRN_DAR, vcpu->arch.shregs.dar);
+ 	mtspr(SPRN_DSISR, vcpu->arch.shregs.dsisr);
+ 	mtspr(SPRN_AMR, vcpu->arch.amr);
+ 	mtspr(SPRN_UAMOR, vcpu->arch.uamor);
+ 
+ 	if (!(vcpu->arch.ctrl & 1))
+ 		mtspr(SPRN_CTRLT, mfspr(SPRN_CTRLF) & ~1);
+ 
+ 	mtspr(SPRN_DEC, vcpu->arch.dec_expires - mftb());
+ 
+ 	if (kvmhv_on_pseries()) {
+ 		/* call our hypervisor to load up HV regs and go */
+ 		struct hv_guest_state hvregs;
+ 
+ 		kvmhv_save_hv_regs(vcpu, &hvregs);
+ 		hvregs.lpcr = lpcr;
+ 		vcpu->arch.regs.msr = vcpu->arch.shregs.msr;
+ 		hvregs.version = HV_GUEST_STATE_VERSION;
+ 		if (vcpu->arch.nested) {
+ 			hvregs.lpid = vcpu->arch.nested->shadow_lpid;
+ 			hvregs.vcpu_token = vcpu->arch.nested_vcpu_id;
+ 		} else {
+ 			hvregs.lpid = vcpu->kvm->arch.lpid;
+ 			hvregs.vcpu_token = vcpu->vcpu_id;
+ 		}
+ 		hvregs.hdec_expiry = time_limit;
+ 		trap = plpar_hcall_norets(H_ENTER_NESTED, __pa(&hvregs),
+ 					  __pa(&vcpu->arch.regs));
+ 		kvmhv_restore_hv_return_state(vcpu, &hvregs);
+ 		vcpu->arch.shregs.msr = vcpu->arch.regs.msr;
+ 		vcpu->arch.shregs.dar = mfspr(SPRN_DAR);
+ 		vcpu->arch.shregs.dsisr = mfspr(SPRN_DSISR);
+ 	} else {
+ 		trap = kvmhv_load_hv_regs_and_go(vcpu, time_limit, lpcr);
+ 	}
+ 
+ 	vcpu->arch.slb_max = 0;
+ 	dec = mfspr(SPRN_DEC);
+ 	tb = mftb();
+ 	vcpu->arch.dec_expires = dec + tb;
+ 	vcpu->cpu = -1;
+ 	vcpu->arch.thread_cpu = -1;
+ 	vcpu->arch.ctrl = mfspr(SPRN_CTRLF);
+ 
+ 	vcpu->arch.iamr = mfspr(SPRN_IAMR);
+ 	vcpu->arch.pspb = mfspr(SPRN_PSPB);
+ 	vcpu->arch.fscr = mfspr(SPRN_FSCR);
+ 	vcpu->arch.tar = mfspr(SPRN_TAR);
+ 	vcpu->arch.ebbhr = mfspr(SPRN_EBBHR);
+ 	vcpu->arch.ebbrr = mfspr(SPRN_EBBRR);
+ 	vcpu->arch.bescr = mfspr(SPRN_BESCR);
+ 	vcpu->arch.wort = mfspr(SPRN_WORT);
+ 	vcpu->arch.tid = mfspr(SPRN_TIDR);
+ 	vcpu->arch.amr = mfspr(SPRN_AMR);
+ 	vcpu->arch.uamor = mfspr(SPRN_UAMOR);
+ 	vcpu->arch.dscr = mfspr(SPRN_DSCR);
+ 
+ 	mtspr(SPRN_PSPB, 0);
+ 	mtspr(SPRN_WORT, 0);
+ 	mtspr(SPRN_AMR, 0);
+ 	mtspr(SPRN_UAMOR, 0);
+ 	mtspr(SPRN_DSCR, host_dscr);
+ 	mtspr(SPRN_TIDR, host_tidr);
+ 	mtspr(SPRN_IAMR, host_iamr);
+ 	mtspr(SPRN_PSPB, 0);
+ 
+ 	msr_check_and_set(MSR_FP | MSR_VEC | MSR_VSX);
+ 	store_fp_state(&vcpu->arch.fp);
+ #ifdef CONFIG_ALTIVEC
+ 	store_vr_state(&vcpu->arch.vr);
+ #endif
+ 
+ 	if (cpu_has_feature(CPU_FTR_TM) ||
+ 	    cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST))
+ 		kvmppc_save_tm_hv(vcpu, vcpu->arch.shregs.msr, true);
+ 
+ 	save_pmu = 1;
+ 	if (vcpu->arch.vpa.pinned_addr) {
+ 		struct lppaca *lp = vcpu->arch.vpa.pinned_addr;
+ 		u32 yield_count = be32_to_cpu(lp->yield_count) + 1;
+ 		lp->yield_count = cpu_to_be32(yield_count);
+ 		vcpu->arch.vpa.dirty = 1;
+ 		save_pmu = lp->pmcregs_in_use;
+ 	}
+ 
+ 	kvmhv_save_guest_pmu(vcpu, save_pmu);
+ 
+ 	vc->entry_exit_map = 0x101;
+ 	vc->in_guest = 0;
+ 
+ 	mtspr(SPRN_DEC, local_paca->kvm_hstate.dec_expires - mftb());
+ 
+ 	kvmhv_load_host_pmu();
+ 
+ 	kvmppc_subcore_exit_guest();
+ 
+ 	return trap;
+ }
+ 
+ /*
++>>>>>>> 360cae313702 (KVM: PPC: Book3S HV: Nested guest entry via hypercall)
   * Wait for some other vcpu thread to execute us, and
   * wake us up when we need to handle something in the host.
   */
@@@ -3421,6 -3818,184 +3821,187 @@@ static int kvmppc_run_vcpu(struct kvm_r
  	return vcpu->arch.ret;
  }
  
++<<<<<<< HEAD
++=======
+ int kvmhv_run_single_vcpu(struct kvm_run *kvm_run,
+ 			  struct kvm_vcpu *vcpu, u64 time_limit,
+ 			  unsigned long lpcr)
+ {
+ 	int trap, r, pcpu, pcpu0;
+ 	int srcu_idx;
+ 	struct kvmppc_vcore *vc;
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_nested_guest *nested = vcpu->arch.nested;
+ 	unsigned long lpid;
+ 
+ 	trace_kvmppc_run_vcpu_enter(vcpu);
+ 
+ 	kvm_run->exit_reason = 0;
+ 	vcpu->arch.ret = RESUME_GUEST;
+ 	vcpu->arch.trap = 0;
+ 
+ 	vc = vcpu->arch.vcore;
+ 	vcpu->arch.ceded = 0;
+ 	vcpu->arch.run_task = current;
+ 	vcpu->arch.kvm_run = kvm_run;
+ 	vcpu->arch.stolen_logged = vcore_stolen_time(vc, mftb());
+ 	vcpu->arch.state = KVMPPC_VCPU_RUNNABLE;
+ 	vcpu->arch.busy_preempt = TB_NIL;
+ 	vcpu->arch.last_inst = KVM_INST_FETCH_FAILED;
+ 	vc->runnable_threads[0] = vcpu;
+ 	vc->n_runnable = 1;
+ 	vc->runner = vcpu;
+ 
+ 	/* See if the MMU is ready to go */
+ 	if (!kvm->arch.mmu_ready)
+ 		kvmhv_setup_mmu(vcpu);
+ 
+ 	if (need_resched())
+ 		cond_resched();
+ 
+ 	kvmppc_update_vpas(vcpu);
+ 
+ 	init_vcore_to_run(vc);
+ 	vc->preempt_tb = TB_NIL;
+ 
+ 	preempt_disable();
+ 	pcpu = smp_processor_id();
+ 	vc->pcpu = pcpu;
+ 	kvmppc_prepare_radix_vcpu(vcpu, pcpu);
+ 
+ 	local_irq_disable();
+ 	hard_irq_disable();
+ 	if (signal_pending(current))
+ 		goto sigpend;
+ 	if (lazy_irq_pending() || need_resched() || !kvm->arch.mmu_ready)
+ 		goto out;
+ 
+ 	if (!nested) {
+ 		kvmppc_core_prepare_to_enter(vcpu);
+ 		if (vcpu->arch.doorbell_request) {
+ 			vc->dpdes = 1;
+ 			smp_wmb();
+ 			vcpu->arch.doorbell_request = 0;
+ 		}
+ 		if (test_bit(BOOK3S_IRQPRIO_EXTERNAL,
+ 			     &vcpu->arch.pending_exceptions))
+ 			lpcr |= LPCR_MER;
+ 	} else if (vcpu->arch.pending_exceptions ||
+ 		   vcpu->arch.doorbell_request ||
+ 		   xive_interrupt_pending(vcpu)) {
+ 		vcpu->arch.ret = RESUME_HOST;
+ 		goto out;
+ 	}
+ 
+ 	kvmppc_clear_host_core(pcpu);
+ 
+ 	local_paca->kvm_hstate.tid = 0;
+ 	local_paca->kvm_hstate.napping = 0;
+ 	local_paca->kvm_hstate.kvm_split_mode = NULL;
+ 	kvmppc_start_thread(vcpu, vc);
+ 	kvmppc_create_dtl_entry(vcpu, vc);
+ 	trace_kvm_guest_enter(vcpu);
+ 
+ 	vc->vcore_state = VCORE_RUNNING;
+ 	trace_kvmppc_run_core(vc, 0);
+ 
+ 	lpid = vc->kvm->arch.lpid;
+ 	if (nested)
+ 		lpid = nested->shadow_lpid;
+ 	mtspr(SPRN_LPID, lpid);
+ 	isync();
+ 
+ 	/* See comment above in kvmppc_run_core() about this */
+ 	pcpu0 = pcpu;
+ 	if (cpu_has_feature(CPU_FTR_ARCH_300))
+ 		pcpu0 &= ~0x3UL;
+ 
+ 	if (cpumask_test_cpu(pcpu0, &kvm->arch.need_tlb_flush)) {
+ 		radix__local_flush_tlb_lpid_guest(lpid);
+ 		/* Clear the bit after the TLB flush */
+ 		cpumask_clear_cpu(pcpu0, &kvm->arch.need_tlb_flush);
+ 	}
+ 
+ 	trace_hardirqs_on();
+ 	guest_enter_irqoff();
+ 
+ 	srcu_idx = srcu_read_lock(&kvm->srcu);
+ 
+ 	this_cpu_disable_ftrace();
+ 
+ 	trap = kvmhv_p9_guest_entry(vcpu, time_limit, lpcr);
+ 	vcpu->arch.trap = trap;
+ 
+ 	this_cpu_enable_ftrace();
+ 
+ 	srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 
+ 	mtspr(SPRN_LPID, kvm->arch.host_lpid);
+ 	isync();
+ 
+ 	trace_hardirqs_off();
+ 	set_irq_happened(trap);
+ 
+ 	kvmppc_set_host_core(pcpu);
+ 
+ 	local_irq_enable();
+ 	guest_exit();
+ 
+ 	cpumask_clear_cpu(pcpu, &kvm->arch.cpu_in_guest);
+ 
+ 	preempt_enable();
+ 
+ 	/* cancel pending decrementer exception if DEC is now positive */
+ 	if (get_tb() < vcpu->arch.dec_expires && kvmppc_core_pending_dec(vcpu))
+ 		kvmppc_core_dequeue_dec(vcpu);
+ 
+ 	trace_kvm_guest_exit(vcpu);
+ 	r = RESUME_GUEST;
+ 	if (trap) {
+ 		if (!nested)
+ 			r = kvmppc_handle_exit_hv(kvm_run, vcpu, current);
+ 		else
+ 			r = kvmppc_handle_nested_exit(vcpu);
+ 	}
+ 	vcpu->arch.ret = r;
+ 
+ 	if (is_kvmppc_resume_guest(r) && vcpu->arch.ceded &&
+ 	    !kvmppc_vcpu_woken(vcpu)) {
+ 		kvmppc_set_timer(vcpu);
+ 		while (vcpu->arch.ceded && !kvmppc_vcpu_woken(vcpu)) {
+ 			if (signal_pending(current)) {
+ 				vcpu->stat.signal_exits++;
+ 				kvm_run->exit_reason = KVM_EXIT_INTR;
+ 				vcpu->arch.ret = -EINTR;
+ 				break;
+ 			}
+ 			spin_lock(&vc->lock);
+ 			kvmppc_vcore_blocked(vc);
+ 			spin_unlock(&vc->lock);
+ 		}
+ 	}
+ 	vcpu->arch.ceded = 0;
+ 
+ 	vc->vcore_state = VCORE_INACTIVE;
+ 	trace_kvmppc_run_core(vc, 1);
+ 
+  done:
+ 	kvmppc_remove_runnable(vc, vcpu);
+ 	trace_kvmppc_run_vcpu_exit(vcpu, kvm_run);
+ 
+ 	return vcpu->arch.ret;
+ 
+  sigpend:
+ 	vcpu->stat.signal_exits++;
+ 	kvm_run->exit_reason = KVM_EXIT_INTR;
+ 	vcpu->arch.ret = -EINTR;
+  out:
+ 	local_irq_enable();
+ 	preempt_enable();
+ 	goto done;
+ }
+ 
++>>>>>>> 360cae313702 (KVM: PPC: Book3S HV: Nested guest entry via hypercall)
  static int kvmppc_vcpu_run_hv(struct kvm_run *run, struct kvm_vcpu *vcpu)
  {
  	int r;
@@@ -3496,7 -4071,11 +4077,15 @@@
  	vcpu->arch.state = KVMPPC_VCPU_BUSY_IN_HOST;
  
  	do {
++<<<<<<< HEAD
 +		r = kvmppc_run_vcpu(run, vcpu);
++=======
+ 		if (kvm->arch.threads_indep && kvm_is_radix(kvm))
+ 			r = kvmhv_run_single_vcpu(run, vcpu, ~(u64)0,
+ 						  vcpu->arch.vcore->lpcr);
+ 		else
+ 			r = kvmppc_run_vcpu(run, vcpu);
++>>>>>>> 360cae313702 (KVM: PPC: Book3S HV: Nested guest entry via hypercall)
  
  		if (run->exit_reason == KVM_EXIT_PAPR_HCALL &&
  		    !(vcpu->arch.shregs.msr & MSR_PR)) {
diff --git a/arch/powerpc/include/asm/hvcall.h b/arch/powerpc/include/asm/hvcall.h
index 6c74d2af2a3a..40a79c551ffe 100644
--- a/arch/powerpc/include/asm/hvcall.h
+++ b/arch/powerpc/include/asm/hvcall.h
@@ -464,6 +464,42 @@ struct h_cpu_char_result {
 	u64 behaviour;
 };
 
+/* Register state for entering a nested guest with H_ENTER_NESTED */
+struct hv_guest_state {
+	u64 version;		/* version of this structure layout */
+	u32 lpid;
+	u32 vcpu_token;
+	/* These registers are hypervisor privileged (at least for writing) */
+	u64 lpcr;
+	u64 pcr;
+	u64 amor;
+	u64 dpdes;
+	u64 hfscr;
+	s64 tb_offset;
+	u64 dawr0;
+	u64 dawrx0;
+	u64 ciabr;
+	u64 hdec_expiry;
+	u64 purr;
+	u64 spurr;
+	u64 ic;
+	u64 vtb;
+	u64 hdar;
+	u64 hdsisr;
+	u64 heir;
+	u64 asdr;
+	/* These are OS privileged but need to be set late in guest entry */
+	u64 srr0;
+	u64 srr1;
+	u64 sprg[4];
+	u64 pidr;
+	u64 cfar;
+	u64 ppr;
+};
+
+/* Latest version of hv_guest_state structure */
+#define HV_GUEST_STATE_VERSION	1
+
 #endif /* __ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_HVCALL_H */
diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h
index 43f212e38b89..093fd700da32 100644
--- a/arch/powerpc/include/asm/kvm_book3s.h
+++ b/arch/powerpc/include/asm/kvm_book3s.h
@@ -280,6 +280,13 @@ void kvmhv_vm_nested_init(struct kvm *kvm);
 long kvmhv_set_partition_table(struct kvm_vcpu *vcpu);
 void kvmhv_set_ptbl_entry(unsigned int lpid, u64 dw0, u64 dw1);
 void kvmhv_release_all_nested(struct kvm *kvm);
+long kvmhv_enter_nested_guest(struct kvm_vcpu *vcpu);
+int kvmhv_run_single_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu,
+			  u64 time_limit, unsigned long lpcr);
+void kvmhv_save_hv_regs(struct kvm_vcpu *vcpu, struct hv_guest_state *hr);
+void kvmhv_restore_hv_return_state(struct kvm_vcpu *vcpu,
+				   struct hv_guest_state *hr);
+long int kvmhv_nested_page_fault(struct kvm_vcpu *vcpu);
 
 void kvmppc_giveup_fac(struct kvm_vcpu *vcpu, ulong fac);
 
diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c35d4f2c4d90..ceb9f20a0b24 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -95,6 +95,7 @@ struct dtl_entry;
 
 struct kvmppc_vcpu_book3s;
 struct kvmppc_book3s_shadow_vcpu;
+struct kvm_nested_guest;
 
 struct kvm_vm_stat {
 	ulong remote_tlb_flush;
@@ -786,6 +787,10 @@ struct kvm_vcpu_arch {
 	u32 emul_inst;
 
 	u32 online;
+
+	/* For support of nested guests */
+	struct kvm_nested_guest *nested;
+	u32 nested_vcpu_id;
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING
diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 7c3738d890e8..d0abcbbdc700 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -503,6 +503,7 @@ int main(void)
 	OFFSET(VCPU_VPA, kvm_vcpu, arch.vpa.pinned_addr);
 	OFFSET(VCPU_VPA_DIRTY, kvm_vcpu, arch.vpa.dirty);
 	OFFSET(VCPU_HEIR, kvm_vcpu, arch.emul_inst);
+	OFFSET(VCPU_NESTED, kvm_vcpu, arch.nested);
 	OFFSET(VCPU_CPU, kvm_vcpu, cpu);
 	OFFSET(VCPU_THREAD_CPU, kvm_vcpu, arch.thread_cpu);
 #endif
* Unmerged path arch/powerpc/kvm/book3s_hv.c
diff --git a/arch/powerpc/kvm/book3s_hv_nested.c b/arch/powerpc/kvm/book3s_hv_nested.c
index 327826248314..5b9fd7c8d931 100644
--- a/arch/powerpc/kvm/book3s_hv_nested.c
+++ b/arch/powerpc/kvm/book3s_hv_nested.c
@@ -20,6 +20,231 @@ static struct patb_entry *pseries_partition_tb;
 
 static void kvmhv_update_ptbl_cache(struct kvm_nested_guest *gp);
 
+void kvmhv_save_hv_regs(struct kvm_vcpu *vcpu, struct hv_guest_state *hr)
+{
+	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+
+	hr->pcr = vc->pcr;
+	hr->dpdes = vc->dpdes;
+	hr->hfscr = vcpu->arch.hfscr;
+	hr->tb_offset = vc->tb_offset;
+	hr->dawr0 = vcpu->arch.dawr;
+	hr->dawrx0 = vcpu->arch.dawrx;
+	hr->ciabr = vcpu->arch.ciabr;
+	hr->purr = vcpu->arch.purr;
+	hr->spurr = vcpu->arch.spurr;
+	hr->ic = vcpu->arch.ic;
+	hr->vtb = vc->vtb;
+	hr->srr0 = vcpu->arch.shregs.srr0;
+	hr->srr1 = vcpu->arch.shregs.srr1;
+	hr->sprg[0] = vcpu->arch.shregs.sprg0;
+	hr->sprg[1] = vcpu->arch.shregs.sprg1;
+	hr->sprg[2] = vcpu->arch.shregs.sprg2;
+	hr->sprg[3] = vcpu->arch.shregs.sprg3;
+	hr->pidr = vcpu->arch.pid;
+	hr->cfar = vcpu->arch.cfar;
+	hr->ppr = vcpu->arch.ppr;
+}
+
+static void save_hv_return_state(struct kvm_vcpu *vcpu, int trap,
+				 struct hv_guest_state *hr)
+{
+	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+
+	hr->dpdes = vc->dpdes;
+	hr->hfscr = vcpu->arch.hfscr;
+	hr->purr = vcpu->arch.purr;
+	hr->spurr = vcpu->arch.spurr;
+	hr->ic = vcpu->arch.ic;
+	hr->vtb = vc->vtb;
+	hr->srr0 = vcpu->arch.shregs.srr0;
+	hr->srr1 = vcpu->arch.shregs.srr1;
+	hr->sprg[0] = vcpu->arch.shregs.sprg0;
+	hr->sprg[1] = vcpu->arch.shregs.sprg1;
+	hr->sprg[2] = vcpu->arch.shregs.sprg2;
+	hr->sprg[3] = vcpu->arch.shregs.sprg3;
+	hr->pidr = vcpu->arch.pid;
+	hr->cfar = vcpu->arch.cfar;
+	hr->ppr = vcpu->arch.ppr;
+	switch (trap) {
+	case BOOK3S_INTERRUPT_H_DATA_STORAGE:
+		hr->hdar = vcpu->arch.fault_dar;
+		hr->hdsisr = vcpu->arch.fault_dsisr;
+		hr->asdr = vcpu->arch.fault_gpa;
+		break;
+	case BOOK3S_INTERRUPT_H_INST_STORAGE:
+		hr->asdr = vcpu->arch.fault_gpa;
+		break;
+	case BOOK3S_INTERRUPT_H_EMUL_ASSIST:
+		hr->heir = vcpu->arch.emul_inst;
+		break;
+	}
+}
+
+static void restore_hv_regs(struct kvm_vcpu *vcpu, struct hv_guest_state *hr)
+{
+	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+
+	vc->pcr = hr->pcr;
+	vc->dpdes = hr->dpdes;
+	vcpu->arch.hfscr = hr->hfscr;
+	vcpu->arch.dawr = hr->dawr0;
+	vcpu->arch.dawrx = hr->dawrx0;
+	vcpu->arch.ciabr = hr->ciabr;
+	vcpu->arch.purr = hr->purr;
+	vcpu->arch.spurr = hr->spurr;
+	vcpu->arch.ic = hr->ic;
+	vc->vtb = hr->vtb;
+	vcpu->arch.shregs.srr0 = hr->srr0;
+	vcpu->arch.shregs.srr1 = hr->srr1;
+	vcpu->arch.shregs.sprg0 = hr->sprg[0];
+	vcpu->arch.shregs.sprg1 = hr->sprg[1];
+	vcpu->arch.shregs.sprg2 = hr->sprg[2];
+	vcpu->arch.shregs.sprg3 = hr->sprg[3];
+	vcpu->arch.pid = hr->pidr;
+	vcpu->arch.cfar = hr->cfar;
+	vcpu->arch.ppr = hr->ppr;
+}
+
+void kvmhv_restore_hv_return_state(struct kvm_vcpu *vcpu,
+				   struct hv_guest_state *hr)
+{
+	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+
+	vc->dpdes = hr->dpdes;
+	vcpu->arch.hfscr = hr->hfscr;
+	vcpu->arch.purr = hr->purr;
+	vcpu->arch.spurr = hr->spurr;
+	vcpu->arch.ic = hr->ic;
+	vc->vtb = hr->vtb;
+	vcpu->arch.fault_dar = hr->hdar;
+	vcpu->arch.fault_dsisr = hr->hdsisr;
+	vcpu->arch.fault_gpa = hr->asdr;
+	vcpu->arch.emul_inst = hr->heir;
+	vcpu->arch.shregs.srr0 = hr->srr0;
+	vcpu->arch.shregs.srr1 = hr->srr1;
+	vcpu->arch.shregs.sprg0 = hr->sprg[0];
+	vcpu->arch.shregs.sprg1 = hr->sprg[1];
+	vcpu->arch.shregs.sprg2 = hr->sprg[2];
+	vcpu->arch.shregs.sprg3 = hr->sprg[3];
+	vcpu->arch.pid = hr->pidr;
+	vcpu->arch.cfar = hr->cfar;
+	vcpu->arch.ppr = hr->ppr;
+}
+
+long kvmhv_enter_nested_guest(struct kvm_vcpu *vcpu)
+{
+	long int err, r;
+	struct kvm_nested_guest *l2;
+	struct pt_regs l2_regs, saved_l1_regs;
+	struct hv_guest_state l2_hv, saved_l1_hv;
+	struct kvmppc_vcore *vc = vcpu->arch.vcore;
+	u64 hv_ptr, regs_ptr;
+	u64 hdec_exp;
+	s64 delta_purr, delta_spurr, delta_ic, delta_vtb;
+	u64 mask;
+	unsigned long lpcr;
+
+	if (vcpu->kvm->arch.l1_ptcr == 0)
+		return H_NOT_AVAILABLE;
+
+	/* copy parameters in */
+	hv_ptr = kvmppc_get_gpr(vcpu, 4);
+	err = kvm_vcpu_read_guest(vcpu, hv_ptr, &l2_hv,
+				  sizeof(struct hv_guest_state));
+	if (err)
+		return H_PARAMETER;
+	if (l2_hv.version != HV_GUEST_STATE_VERSION)
+		return H_P2;
+
+	regs_ptr = kvmppc_get_gpr(vcpu, 5);
+	err = kvm_vcpu_read_guest(vcpu, regs_ptr, &l2_regs,
+				  sizeof(struct pt_regs));
+	if (err)
+		return H_PARAMETER;
+
+	/* translate lpid */
+	l2 = kvmhv_get_nested(vcpu->kvm, l2_hv.lpid, true);
+	if (!l2)
+		return H_PARAMETER;
+	if (!l2->l1_gr_to_hr) {
+		mutex_lock(&l2->tlb_lock);
+		kvmhv_update_ptbl_cache(l2);
+		mutex_unlock(&l2->tlb_lock);
+	}
+
+	/* save l1 values of things */
+	vcpu->arch.regs.msr = vcpu->arch.shregs.msr;
+	saved_l1_regs = vcpu->arch.regs;
+	kvmhv_save_hv_regs(vcpu, &saved_l1_hv);
+
+	/* convert TB values/offsets to host (L0) values */
+	hdec_exp = l2_hv.hdec_expiry - vc->tb_offset;
+	vc->tb_offset += l2_hv.tb_offset;
+
+	/* set L1 state to L2 state */
+	vcpu->arch.nested = l2;
+	vcpu->arch.nested_vcpu_id = l2_hv.vcpu_token;
+	vcpu->arch.regs = l2_regs;
+	vcpu->arch.shregs.msr = vcpu->arch.regs.msr;
+	mask = LPCR_DPFD | LPCR_ILE | LPCR_TC | LPCR_AIL | LPCR_LD |
+		LPCR_LPES | LPCR_MER;
+	lpcr = (vc->lpcr & ~mask) | (l2_hv.lpcr & mask);
+	restore_hv_regs(vcpu, &l2_hv);
+
+	vcpu->arch.ret = RESUME_GUEST;
+	vcpu->arch.trap = 0;
+	do {
+		if (mftb() >= hdec_exp) {
+			vcpu->arch.trap = BOOK3S_INTERRUPT_HV_DECREMENTER;
+			r = RESUME_HOST;
+			break;
+		}
+		r = kvmhv_run_single_vcpu(vcpu->arch.kvm_run, vcpu, hdec_exp,
+					  lpcr);
+	} while (is_kvmppc_resume_guest(r));
+
+	/* save L2 state for return */
+	l2_regs = vcpu->arch.regs;
+	l2_regs.msr = vcpu->arch.shregs.msr;
+	delta_purr = vcpu->arch.purr - l2_hv.purr;
+	delta_spurr = vcpu->arch.spurr - l2_hv.spurr;
+	delta_ic = vcpu->arch.ic - l2_hv.ic;
+	delta_vtb = vc->vtb - l2_hv.vtb;
+	save_hv_return_state(vcpu, vcpu->arch.trap, &l2_hv);
+
+	/* restore L1 state */
+	vcpu->arch.nested = NULL;
+	vcpu->arch.regs = saved_l1_regs;
+	vcpu->arch.shregs.msr = saved_l1_regs.msr & ~MSR_TS_MASK;
+	/* set L1 MSR TS field according to L2 transaction state */
+	if (l2_regs.msr & MSR_TS_MASK)
+		vcpu->arch.shregs.msr |= MSR_TS_S;
+	vc->tb_offset = saved_l1_hv.tb_offset;
+	restore_hv_regs(vcpu, &saved_l1_hv);
+	vcpu->arch.purr += delta_purr;
+	vcpu->arch.spurr += delta_spurr;
+	vcpu->arch.ic += delta_ic;
+	vc->vtb += delta_vtb;
+
+	kvmhv_put_nested(l2);
+
+	/* copy l2_hv_state and regs back to guest */
+	err = kvm_vcpu_write_guest(vcpu, hv_ptr, &l2_hv,
+				   sizeof(struct hv_guest_state));
+	if (err)
+		return H_AUTHORITY;
+	err = kvm_vcpu_write_guest(vcpu, regs_ptr, &l2_regs,
+				   sizeof(struct pt_regs));
+	if (err)
+		return H_AUTHORITY;
+
+	if (r == -EINTR)
+		return H_INTERRUPT;
+
+	return vcpu->arch.trap;
+}
+
 long kvmhv_nested_init(void)
 {
 	long int ptb_order;
@@ -299,3 +524,8 @@ void kvmhv_put_nested(struct kvm_nested_guest *gp)
 	if (ref == 0)
 		kvmhv_release_nested(gp);
 }
+
+long kvmhv_nested_page_fault(struct kvm_vcpu *vcpu)
+{
+	return RESUME_HOST;
+}
diff --git a/arch/powerpc/kvm/book3s_hv_rmhandlers.S b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
index ec859f4f230a..5446309fac41 100644
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@ -1387,6 +1387,10 @@ END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)
 BEGIN_FTR_SECTION
 	PPC_MSGSYNC
 	lwsync
+	/* always exit if we're running a nested guest */
+	ld	r0, VCPU_NESTED(r9)
+	cmpdi	r0, 0
+	bne	guest_exit_cont
 END_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)
 	lbz	r0, HSTATE_HOST_IPI(r13)
 	cmpwi	r0, 0
@@ -2257,6 +2261,10 @@ hcall_try_real_mode:
 	andi.	r0,r11,MSR_PR
 	/* sc 1 from userspace - reflect to guest syscall */
 	bne	sc_1_fast_return
+	/* sc 1 from nested guest - give it to L1 to handle */
+	ld	r0, VCPU_NESTED(r9)
+	cmpdi	r0, 0
+	bne	guest_exit_cont
 	clrrdi	r3,r3,2
 	cmpldi	r3,hcall_real_table_end - hcall_real_table
 	bge	guest_exit_cont
