rcu: Remove rsp parameter from rcu_node tree accessor macros

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Paul E. McKenney <paulmck@linux.vnet.ibm.com>
commit aedf4ba984168ab5b96898a03bfdb51d07194776
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/aedf4ba9.failed

There now is only one rcu_state structure in a given build of the Linux
kernel, so there is no need to pass it as a parameter to RCU's rcu_node
tree's accessor macros.  This commit therefore removes the rsp parameter
from those macros in kernel/rcu/rcu.h, and removes some now-unused rsp
local variables while in the area.

	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
(cherry picked from commit aedf4ba984168ab5b96898a03bfdb51d07194776)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree_exp.h
diff --cc kernel/rcu/tree_exp.h
index f0673eb3c045,060bdb45cd95..000000000000
--- a/kernel/rcu/tree_exp.h
+++ b/kernel/rcu/tree_exp.h
@@@ -97,7 -97,7 +97,11 @@@ static void sync_exp_reset_tree_hotplug
  	 * Each pass through the following loop propagates newly onlined
  	 * CPUs for the current rcu_node structure up the rcu_node tree.
  	 */
++<<<<<<< HEAD
 +	rcu_for_each_leaf_node(rsp, rnp) {
++=======
+ 	rcu_for_each_leaf_node(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  		raw_spin_lock_irqsave_rcu_node(rnp, flags);
  		if (rnp->expmaskinit == rnp->expmaskinitnext) {
  			raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
@@@ -140,8 -140,8 +144,13 @@@ static void __maybe_unused sync_exp_res
  	unsigned long flags;
  	struct rcu_node *rnp;
  
++<<<<<<< HEAD
 +	sync_exp_reset_tree_hotplug(rsp);
 +	rcu_for_each_node_breadth_first(rsp, rnp) {
++=======
+ 	sync_exp_reset_tree_hotplug();
+ 	rcu_for_each_node_breadth_first(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  		raw_spin_lock_irqsave_rcu_node(rnp, flags);
  		WARN_ON_ONCE(rnp->expmask);
  		rnp->expmask = rnp->expmaskinit;
@@@ -436,20 -433,19 +445,28 @@@ static void sync_rcu_exp_select_cpus(st
  	int cpu;
  	struct rcu_node *rnp;
  
 -	trace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS("reset"));
 -	sync_exp_reset_tree();
 -	trace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS("select"));
 +	trace_rcu_exp_grace_period(rsp->name, rcu_exp_gp_seq_endval(rsp), TPS("reset"));
 +	sync_exp_reset_tree(rsp);
 +	trace_rcu_exp_grace_period(rsp->name, rcu_exp_gp_seq_endval(rsp), TPS("select"));
  
  	/* Schedule work for each leaf rcu_node structure. */
++<<<<<<< HEAD
 +	rcu_for_each_leaf_node(rsp, rnp) {
++=======
+ 	rcu_for_each_leaf_node(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  		rnp->exp_need_flush = false;
  		if (!READ_ONCE(rnp->expmask))
  			continue; /* Avoid early boot non-existent wq. */
  		rnp->rew.rew_func = func;
 +		rnp->rew.rew_rsp = rsp;
  		if (!READ_ONCE(rcu_par_gp_wq) ||
  		    rcu_scheduler_active != RCU_SCHEDULER_RUNNING ||
++<<<<<<< HEAD
 +		    rcu_is_last_leaf_node(rsp, rnp)) {
++=======
+ 		    rcu_is_last_leaf_node(rnp)) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  			/* No workqueues yet or last leaf, do direct call. */
  			sync_rcu_exp_select_node_cpus(&rnp->rew.rew_work);
  			continue;
@@@ -466,7 -462,7 +483,11 @@@
  	}
  
  	/* Wait for workqueue jobs (if any) to complete. */
++<<<<<<< HEAD
 +	rcu_for_each_leaf_node(rsp, rnp)
++=======
+ 	rcu_for_each_leaf_node(rnp)
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  		if (rnp->exp_need_flush)
  			flush_work(&rnp->rew.rew_work);
  }
@@@ -498,9 -494,9 +519,13 @@@ static void synchronize_sched_expedited
  			continue;
  		panic_on_rcu_stall();
  		pr_err("INFO: %s detected expedited stalls on CPUs/tasks: {",
 -		       rcu_state.name);
 +		       rsp->name);
  		ndetected = 0;
++<<<<<<< HEAD
 +		rcu_for_each_leaf_node(rsp, rnp) {
++=======
+ 		rcu_for_each_leaf_node(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  			ndetected += rcu_print_task_exp_stall(rnp);
  			for_each_leaf_node_possible_cpu(rnp, cpu) {
  				struct rcu_data *rdp;
@@@ -521,7 -517,7 +546,11 @@@
  			rnp_root->expmask, ".T"[!!rnp_root->exp_tasks]);
  		if (ndetected) {
  			pr_err("blocking rcu_node structures:");
++<<<<<<< HEAD
 +			rcu_for_each_node_breadth_first(rsp, rnp) {
++=======
+ 			rcu_for_each_node_breadth_first(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  				if (rnp == rnp_root)
  					continue; /* printed unconditionally */
  				if (sync_rcu_preempt_exp_done_unlocked(rnp))
@@@ -533,7 -529,7 +562,11 @@@
  			}
  			pr_cont("\n");
  		}
++<<<<<<< HEAD
 +		rcu_for_each_leaf_node(rsp, rnp) {
++=======
+ 		rcu_for_each_leaf_node(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  			for_each_leaf_node_possible_cpu(rnp, cpu) {
  				mask = leaf_node_cpu_bit(rnp, cpu);
  				if (!(rnp->expmask & mask))
@@@ -563,9 -559,9 +596,13 @@@ static void rcu_exp_wait_wake(struct rc
  	 * Switch over to wakeup mode, allowing the next GP, but -only- the
  	 * next GP, to proceed.
  	 */
 -	mutex_lock(&rcu_state.exp_wake_mutex);
 +	mutex_lock(&rsp->exp_wake_mutex);
  
++<<<<<<< HEAD
 +	rcu_for_each_node_breadth_first(rsp, rnp) {
++=======
+ 	rcu_for_each_node_breadth_first(rnp) {
++>>>>>>> aedf4ba98416 (rcu: Remove rsp parameter from rcu_node tree accessor macros)
  		if (ULONG_CMP_LT(READ_ONCE(rnp->exp_seq_rq), s)) {
  			spin_lock(&rnp->exp_lock);
  			/* Recheck, avoid hang in case someone just arrived. */
diff --git a/kernel/rcu/rcu.h b/kernel/rcu/rcu.h
index 4d04683c31b2..2bb77fddc11f 100644
--- a/kernel/rcu/rcu.h
+++ b/kernel/rcu/rcu.h
@@ -329,29 +329,23 @@ static inline void rcu_init_levelspread(int *levelspread, const int *levelcnt)
 }
 
 /* Returns first leaf rcu_node of the specified RCU flavor. */
-#define rcu_first_leaf_node(rsp) ((rsp)->level[rcu_num_lvls - 1])
+#define rcu_first_leaf_node() (rcu_state.level[rcu_num_lvls - 1])
 
 /* Is this rcu_node a leaf? */
 #define rcu_is_leaf_node(rnp) ((rnp)->level == rcu_num_lvls - 1)
 
 /* Is this rcu_node the last leaf? */
-#define rcu_is_last_leaf_node(rsp, rnp) ((rnp) == &(rsp)->node[rcu_num_nodes - 1])
+#define rcu_is_last_leaf_node(rnp) ((rnp) == &rcu_state.node[rcu_num_nodes - 1])
 
 /*
- * Do a full breadth-first scan of the rcu_node structures for the
+ * Do a full breadth-first scan of the {s,}rcu_node structures for the
  * specified rcu_state structure.
  */
-#define rcu_for_each_node_breadth_first(rsp, rnp) \
-	for ((rnp) = &(rsp)->node[0]; \
-	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
-
-/*
- * Do a breadth-first scan of the non-leaf rcu_node structures for the
- * specified rcu_state structure.  Note that if there is a singleton
- * rcu_node tree with but one rcu_node structure, this loop is a no-op.
- */
-#define rcu_for_each_nonleaf_node_breadth_first(rsp, rnp) \
-	for ((rnp) = &(rsp)->node[0]; !rcu_is_leaf_node(rsp, rnp); (rnp)++)
+#define srcu_for_each_node_breadth_first(sp, rnp) \
+	for ((rnp) = &(sp)->node[0]; \
+	     (rnp) < &(sp)->node[rcu_num_nodes]; (rnp)++)
+#define rcu_for_each_node_breadth_first(rnp) \
+	srcu_for_each_node_breadth_first(&rcu_state, rnp)
 
 /*
  * Scan the leaves of the rcu_node hierarchy for the specified rcu_state
@@ -359,9 +353,9 @@ static inline void rcu_init_levelspread(int *levelspread, const int *levelcnt)
  * one rcu_node structure, this loop -will- visit the rcu_node structure.
  * It is still a leaf node, even if it is also the root node.
  */
-#define rcu_for_each_leaf_node(rsp, rnp) \
-	for ((rnp) = rcu_first_leaf_node(rsp); \
-	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
+#define rcu_for_each_leaf_node(rnp) \
+	for ((rnp) = rcu_first_leaf_node(); \
+	     (rnp) < &rcu_state.node[rcu_num_nodes]; (rnp)++)
 
 /*
  * Iterate over all possible CPUs in a leaf RCU node.
diff --git a/kernel/rcu/srcutree.c b/kernel/rcu/srcutree.c
index 6c9866a854b1..2042080cd38b 100644
--- a/kernel/rcu/srcutree.c
+++ b/kernel/rcu/srcutree.c
@@ -105,7 +105,7 @@ static void init_srcu_struct_nodes(struct srcu_struct *sp, bool is_static)
 	rcu_init_levelspread(levelspread, num_rcu_lvl);
 
 	/* Each pass through this loop initializes one srcu_node structure. */
-	rcu_for_each_node_breadth_first(sp, snp) {
+	srcu_for_each_node_breadth_first(sp, snp) {
 		spin_lock_init(&ACCESS_PRIVATE(snp, lock));
 		WARN_ON_ONCE(ARRAY_SIZE(snp->srcu_have_cbs) !=
 			     ARRAY_SIZE(snp->srcu_data_have_cbs));
@@ -561,7 +561,7 @@ static void srcu_gp_end(struct srcu_struct *sp)
 
 	/* Initiate callback invocation as needed. */
 	idx = rcu_seq_ctr(gpseq) % ARRAY_SIZE(snp->srcu_have_cbs);
-	rcu_for_each_node_breadth_first(sp, snp) {
+	srcu_for_each_node_breadth_first(sp, snp) {
 		spin_lock_irq_rcu_node(snp);
 		cbs = false;
 		last_lvl = snp >= sp->level[rcu_num_lvls - 1];
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index f33d62c3dd9d..e56fba1ffe57 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -573,7 +573,7 @@ void show_rcu_gp_kthreads(void)
 	for_each_rcu_flavor(rsp) {
 		pr_info("%s: wait state: %d ->state: %#lx\n",
 			rsp->name, rsp->gp_state, rsp->gp_kthread->state);
-		rcu_for_each_node_breadth_first(rsp, rnp) {
+		rcu_for_each_node_breadth_first(rnp) {
 			if (ULONG_CMP_GE(rsp->gp_seq, rnp->gp_seq_needed))
 				continue;
 			pr_info("\trcu_node %d:%d ->gp_seq %lu ->gp_seq_needed %lu\n",
@@ -1276,7 +1276,7 @@ static void rcu_dump_cpu_stacks(void)
 	unsigned long flags;
 	struct rcu_node *rnp;
 
-	rcu_for_each_leaf_node(&rcu_state, rnp) {
+	rcu_for_each_leaf_node(rnp) {
 		raw_spin_lock_irqsave_rcu_node(rnp, flags);
 		for_each_leaf_node_possible_cpu(rnp, cpu)
 			if (rnp->qsmask & leaf_node_cpu_bit(rnp, cpu))
@@ -1336,7 +1336,7 @@ static void print_other_cpu_stall(unsigned long gp_seq)
 	 */
 	pr_err("INFO: %s detected stalls on CPUs/tasks:", rsp->name);
 	print_cpu_stall_info_begin();
-	rcu_for_each_leaf_node(rsp, rnp) {
+	rcu_for_each_leaf_node(rnp) {
 		raw_spin_lock_irqsave_rcu_node(rnp, flags);
 		ndetected += rcu_print_task_stall(rnp);
 		if (rnp->qsmask != 0) {
@@ -1872,7 +1872,7 @@ static bool rcu_gp_init(struct rcu_state *rsp)
 	 * will handle subsequent offline CPUs.
 	 */
 	rsp->gp_state = RCU_GP_ONOFF;
-	rcu_for_each_leaf_node(rsp, rnp) {
+	rcu_for_each_leaf_node(rnp) {
 		spin_lock(&rsp->ofl_lock);
 		raw_spin_lock_irq_rcu_node(rnp);
 		if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
@@ -1932,7 +1932,7 @@ static bool rcu_gp_init(struct rcu_state *rsp)
 	 * process finishes, because this kthread handles both.
 	 */
 	rsp->gp_state = RCU_GP_INIT;
-	rcu_for_each_node_breadth_first(rsp, rnp) {
+	rcu_for_each_node_breadth_first(rnp) {
 		rcu_gp_slow(gp_init_delay);
 		raw_spin_lock_irqsave_rcu_node(rnp, flags);
 		rdp = this_cpu_ptr(&rcu_data);
@@ -2043,7 +2043,7 @@ static void rcu_gp_cleanup(struct rcu_state *rsp)
 	 */
 	new_gp_seq = rsp->gp_seq;
 	rcu_seq_end(&new_gp_seq);
-	rcu_for_each_node_breadth_first(rsp, rnp) {
+	rcu_for_each_node_breadth_first(rnp) {
 		raw_spin_lock_irq_rcu_node(rnp);
 		if (WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)))
 			dump_blkd_tasks(rnp, 10);
@@ -2603,9 +2603,8 @@ static void force_qs_rnp(int (*f)(struct rcu_data *rsp))
 	unsigned long flags;
 	unsigned long mask;
 	struct rcu_node *rnp;
-	struct rcu_state *rsp = &rcu_state;
 
-	rcu_for_each_leaf_node(rsp, rnp) {
+	rcu_for_each_leaf_node(rnp) {
 		cond_resched_tasks_rcu_qs();
 		mask = 0;
 		raw_spin_lock_irqsave_rcu_node(rnp, flags);
@@ -3775,7 +3774,7 @@ static void __init rcu_init_one(void)
 
 	init_swait_queue_head(&rsp->gp_wq);
 	init_swait_queue_head(&rsp->expedited_wq);
-	rnp = rcu_first_leaf_node(rsp);
+	rnp = rcu_first_leaf_node();
 	for_each_possible_cpu(i) {
 		while (i > rnp->grphi)
 			rnp++;
@@ -3875,7 +3874,7 @@ static void __init rcu_dump_rcu_node_tree(void)
 
 	pr_info("rcu_node tree layout dump\n");
 	pr_info(" ");
-	rcu_for_each_node_breadth_first(&rcu_state, rnp) {
+	rcu_for_each_node_breadth_first(rnp) {
 		if (rnp->level != level) {
 			pr_cont("\n");
 			pr_info(" ");
* Unmerged path kernel/rcu/tree_exp.h
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index ce4f2d2872ce..7c708b95ad59 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -688,7 +688,7 @@ static void rcu_print_detail_task_stall(void)
 	struct rcu_node *rnp = rcu_get_root();
 
 	rcu_print_detail_task_stall_rnp(rnp);
-	rcu_for_each_leaf_node(&rcu_state, rnp)
+	rcu_for_each_leaf_node(rnp)
 		rcu_print_detail_task_stall_rnp(rnp);
 }
 
@@ -1428,7 +1428,7 @@ static void __init rcu_spawn_boost_kthreads(void)
 	for_each_possible_cpu(cpu)
 		per_cpu(rcu_cpu_has_work, cpu) = 0;
 	BUG_ON(smpboot_register_percpu_thread(&rcu_cpu_thread_spec));
-	rcu_for_each_leaf_node(&rcu_state, rnp)
+	rcu_for_each_leaf_node(rnp)
 		(void)rcu_spawn_one_boost_kthread(rnp);
 }
 
