arm64: mm: Introduce MAX_USER_VA_BITS definition

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Will Deacon <will.deacon@arm.com>
commit 9b31cf493ffa40914e02998381993116e574c651
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/9b31cf49.failed

With the introduction of 52-bit virtual addressing for userspace, we are
now in a position where the virtual addressing capability of userspace
may exceed that of the kernel. Consequently, the VA_BITS definition
cannot be used blindly, since it reflects only the size of kernel
virtual addresses.

This patch introduces MAX_USER_VA_BITS which is either VA_BITS or 52
depending on whether 52-bit virtual addressing has been configured at
build time, removing a few places where the 52 is open-coded based on
explicit CONFIG_ guards.

	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 9b31cf493ffa40914e02998381993116e574c651)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/pgtable-hwdef.h
#	arch/arm64/include/asm/processor.h
diff --cc arch/arm64/include/asm/pgtable-hwdef.h
index fd208eac9f2a,54a37660b8c9..000000000000
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@@ -80,7 -80,7 +80,11 @@@
  #define PGDIR_SHIFT		ARM64_HW_PGTABLE_LEVEL_SHIFT(4 - CONFIG_PGTABLE_LEVELS)
  #define PGDIR_SIZE		(_AC(1, UL) << PGDIR_SHIFT)
  #define PGDIR_MASK		(~(PGDIR_SIZE-1))
++<<<<<<< HEAD
 +#define PTRS_PER_PGD		(1 << (VA_BITS - PGDIR_SHIFT))
++=======
+ #define PTRS_PER_PGD		(1 << (MAX_USER_VA_BITS - PGDIR_SHIFT))
++>>>>>>> 9b31cf493ffa (arm64: mm: Introduce MAX_USER_VA_BITS definition)
  
  /*
   * Section address mask and size definitions.
diff --cc arch/arm64/include/asm/processor.h
index 516c433ecbba,bbecc6fe3e5b..000000000000
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@@ -20,20 -20,17 +20,32 @@@
  #define __ASM_PROCESSOR_H
  
  #define KERNEL_DS		UL(-1)
++<<<<<<< HEAD
 +#ifdef CONFIG_ARM64_52BIT_VA
 +#define USER_DS			((UL(1) << 52) - 1)
 +#else
 +#define USER_DS			((UL(1) << VA_BITS) - 1)
 +#endif /* CONFIG_ARM64_52BIT_VA */
++=======
+ #define USER_DS			((UL(1) << MAX_USER_VA_BITS) - 1)
+ 
+ /*
+  * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is
+  * no point in shifting all network buffers by 2 bytes just to make some IP
+  * header fields appear aligned in memory, potentially sacrificing some DMA
+  * performance on some platforms.
+  */
+ #define NET_IP_ALIGN	0
++>>>>>>> 9b31cf493ffa (arm64: mm: Introduce MAX_USER_VA_BITS definition)
  
  #ifndef __ASSEMBLY__
 +
 +/*
 + * Default implementation of macro that returns current
 + * instruction pointer ("program counter").
 + */
 +#define current_text_addr() ({ __label__ _l; _l: &&_l;})
 +
  #ifdef __KERNEL__
  
  #include <linux/build_bug.h>
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 49d99214f43c..474f93f5fdb5 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -73,6 +73,12 @@
 #define KERNEL_START      _text
 #define KERNEL_END        _end
 
+#ifdef CONFIG_ARM64_USER_VA_BITS_52
+#define MAX_USER_VA_BITS	52
+#else
+#define MAX_USER_VA_BITS	VA_BITS
+#endif
+
 /*
  * KASAN requires 1/8th of the kernel virtual address space for the shadow
  * region. KASAN can bloat the stack significantly, so double the (minimum)
* Unmerged path arch/arm64/include/asm/pgtable-hwdef.h
* Unmerged path arch/arm64/include/asm/processor.h
