dma-mapping: bypass indirect calls for dma-direct

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 356da6d0cde3323236977fce54c1f9612a742036
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/356da6d0.failed

Avoid expensive indirect calls in the fast path DMA mapping
operations by directly calling the dma_direct_* ops if we are using
the directly mapped DMA operations.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Tested-by: Tony Luck <tony.luck@intel.com>
(cherry picked from commit 356da6d0cde3323236977fce54c1f9612a742036)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/mm/cache.c
#	arch/arm64/mm/dma-mapping.c
#	arch/ia64/hp/common/hwsw_iommu.c
#	arch/ia64/hp/common/sba_iommu.c
#	arch/ia64/kernel/dma-mapping.c
#	arch/mips/include/asm/dma-mapping.h
#	arch/parisc/kernel/setup.c
#	arch/sparc/include/asm/dma-mapping.h
#	include/asm-generic/dma-mapping.h
#	include/linux/dma-direct.h
#	include/linux/dma-mapping.h
#	kernel/dma/direct.c
#	kernel/dma/mapping.c
diff --cc arch/arc/mm/cache.c
index 25c631942500,e188bb3ede53..000000000000
--- a/arch/arc/mm/cache.c
+++ b/arch/arc/mm/cache.c
@@@ -1277,6 -1277,12 +1277,15 @@@ void __init arc_cache_init_master(void
  		__dma_cache_inv = __dma_cache_inv_l1;
  		__dma_cache_wback = __dma_cache_wback_l1;
  	}
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * In case of IOC (say IOC+SLC case), pointers above could still be set
+ 	 * but end up not being relevant as the first function in chain is not
+ 	 * called at all for devices using coherent DMA.
+ 	 *     arch_sync_dma_for_cpu() -> dma_cache_*() -> __dma_cache_*()
+ 	 */
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  }
  
  void __ref arc_cache_init(void)
diff --cc arch/arm64/mm/dma-mapping.c
index f5a38a0d7e17,95eda81e3f2d..000000000000
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@@ -871,10 -462,7 +871,14 @@@ static void __iommu_setup_dma_ops(struc
  void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
  			const struct iommu_ops *iommu, bool coherent)
  {
++<<<<<<< HEAD
 +	if (!dev->dma_ops)
 +		dev->dma_ops = &arm64_swiotlb_dma_ops;
 +
 +	dev->archdata.dma_coherent = coherent;
++=======
+ 	dev->dma_coherent = coherent;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  	__iommu_setup_dma_ops(dev, dma_base, size, iommu);
  
  #ifdef CONFIG_XEN
diff --cc arch/ia64/hp/common/hwsw_iommu.c
index 58969039bed2,8840ed97712f..000000000000
--- a/arch/ia64/hp/common/hwsw_iommu.c
+++ b/arch/ia64/hp/common/hwsw_iommu.c
@@@ -38,7 -38,7 +38,11 @@@ static inline int use_swiotlb(struct de
  const struct dma_map_ops *hwsw_dma_get_ops(struct device *dev)
  {
  	if (use_swiotlb(dev))
++<<<<<<< HEAD
 +		return &swiotlb_dma_ops;
++=======
+ 		return NULL;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  	return &sba_dma_ops;
  }
  EXPORT_SYMBOL(hwsw_dma_get_ops);
diff --cc arch/ia64/hp/common/sba_iommu.c
index ee5b652d320a,5a361e51cb1e..000000000000
--- a/arch/ia64/hp/common/sba_iommu.c
+++ b/arch/ia64/hp/common/sba_iommu.c
@@@ -2095,7 -2078,7 +2095,11 @@@ sba_init(void
  	 * a successful kdump kernel boot is to use the swiotlb.
  	 */
  	if (is_kdump_kernel()) {
++<<<<<<< HEAD
 +		dma_ops = &swiotlb_dma_ops;
++=======
+ 		dma_ops = NULL;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  		if (swiotlb_late_init_with_default_size(64 * (1<<20)) != 0)
  			panic("Unable to initialize software I/O TLB:"
  				  " Try machvec=dig boot option");
@@@ -2117,7 -2100,7 +2121,11 @@@
  		 * If we didn't find something sba_iommu can claim, we
  		 * need to setup the swiotlb and switch to the dig machvec.
  		 */
++<<<<<<< HEAD
 +		dma_ops = &swiotlb_dma_ops;
++=======
+ 		dma_ops = NULL;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  		if (swiotlb_late_init_with_default_size(64 * (1<<20)) != 0)
  			panic("Unable to find SBA IOMMU or initialize "
  			      "software I/O TLB: Try machvec=dig boot option");
diff --cc arch/ia64/kernel/dma-mapping.c
index 7a471d8d67d4,ad7d9963de34..000000000000
--- a/arch/ia64/kernel/dma-mapping.c
+++ b/arch/ia64/kernel/dma-mapping.c
@@@ -16,9 -16,26 +16,12 @@@ const struct dma_map_ops *dma_get_ops(s
  EXPORT_SYMBOL(dma_get_ops);
  
  #ifdef CONFIG_SWIOTLB
 -void *arch_dma_alloc(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 -{
 -	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
 -}
 -
 -void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
 -		dma_addr_t dma_addr, unsigned long attrs)
 -{
 -	dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
 -}
 -
 -long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
 -		dma_addr_t dma_addr)
 -{
 -	return page_to_pfn(virt_to_page(cpu_addr));
 -}
 -
  void __init swiotlb_dma_init(void)
  {
++<<<<<<< HEAD
 +	dma_ops = &swiotlb_dma_ops;
++=======
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  	swiotlb_init(1);
  }
  #endif
diff --cc arch/mips/include/asm/dma-mapping.h
index 886e75a383f2,20dfaad3a55d..000000000000
--- a/arch/mips/include/asm/dma-mapping.h
+++ b/arch/mips/include/asm/dma-mapping.h
@@@ -14,7 -8,11 +14,15 @@@ extern const struct dma_map_ops *mips_d
  
  static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
  {
++<<<<<<< HEAD
 +	return mips_dma_map_ops;
++=======
+ #if defined(CONFIG_MACH_JAZZ)
+ 	return &jazz_dma_ops;
+ #else
+ 	return NULL;
+ #endif
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  }
  
  #define arch_setup_dma_ops arch_setup_dma_ops
diff --cc arch/parisc/kernel/setup.c
index 8d3a7b80ac42,54818cd78bd0..000000000000
--- a/arch/parisc/kernel/setup.c
+++ b/arch/parisc/kernel/setup.c
@@@ -97,14 -97,8 +97,17 @@@ void __init dma_ops_init(void
  		panic(	"PA-RISC Linux currently only supports machines that conform to\n"
  			"the PA-RISC 1.1 or 2.0 architecture specification.\n");
  
 +	case pcxs:
 +	case pcxt:
 +		hppa_dma_ops = &pcx_dma_ops;
 +		break;
  	case pcxl2:
  		pa7300lc_init();
++<<<<<<< HEAD
 +	case pcxl: /* falls through */
 +		hppa_dma_ops = &pcxl_dma_ops;
++=======
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  		break;
  	default:
  		break;
diff --cc arch/sparc/include/asm/dma-mapping.h
index 12ae33daf52f,ed32845bd2d2..000000000000
--- a/arch/sparc/include/asm/dma-mapping.h
+++ b/arch/sparc/include/asm/dma-mapping.h
@@@ -15,11 -12,11 +15,19 @@@ static inline const struct dma_map_ops 
  {
  #ifdef CONFIG_SPARC_LEON
  	if (sparc_cpu_model == sparc_leon)
++<<<<<<< HEAD
 +		return &pci32_dma_ops;
 +#endif
 +#if defined(CONFIG_SPARC32) && defined(CONFIG_PCI)
 +	if (bus == &pci_bus_type)
 +		return &pci32_dma_ops;
++=======
+ 		return NULL;
+ #endif
+ #if defined(CONFIG_SPARC32) && defined(CONFIG_PCI)
+ 	if (bus == &pci_bus_type)
+ 		return NULL;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  #endif
  	return dma_ops;
  }
diff --cc include/asm-generic/dma-mapping.h
index ad2868263867,c13f46109e88..000000000000
--- a/include/asm-generic/dma-mapping.h
+++ b/include/asm-generic/dma-mapping.h
@@@ -4,16 -4,7 +4,20 @@@
  
  static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
  {
++<<<<<<< HEAD
 +	/*
 +	 * Use the non-coherent ops if available.  If an architecture wants a
 +	 * more fine-grained selection of operations it will have to implement
 +	 * get_arch_dma_ops itself or use the per-device dma_ops.
 +	 */
 +#ifdef CONFIG_DMA_NONCOHERENT_OPS
 +	return &dma_noncoherent_ops;
 +#else
 +	return &dma_direct_ops;
 +#endif
++=======
+ 	return NULL;
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  }
  
  #endif /* _ASM_GENERIC_DMA_MAPPING_H */
diff --cc include/linux/dma-direct.h
index 861a4e72abde,b7338702592a..000000000000
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@@ -61,11 -53,12 +61,21 @@@ void *dma_direct_alloc(struct device *d
  		gfp_t gfp, unsigned long attrs);
  void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
  		dma_addr_t dma_addr, unsigned long attrs);
++<<<<<<< HEAD
 +dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 +		unsigned long offset, size_t size, enum dma_data_direction dir,
 +		unsigned long attrs);
 +int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 +		enum dma_data_direction dir, unsigned long attrs);
++=======
+ void *dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_addr, unsigned long attrs);
+ struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+ void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  int dma_direct_supported(struct device *dev, u64 mask);
 +int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr);
  #endif /* _LINUX_DMA_DIRECT_H */
diff --cc include/linux/dma-mapping.h
index 49e1e473ffe7,f422aec0f53c..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -134,9 -134,8 +134,12 @@@ struct dma_map_ops 
  
  #define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
  
++<<<<<<< HEAD
 +extern const struct dma_map_ops dma_direct_ops;
 +extern const struct dma_map_ops dma_noncoherent_ops;
++=======
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  extern const struct dma_map_ops dma_virt_ops;
 -extern const struct dma_map_ops dma_dummy_ops;
  
  #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
  
diff --cc kernel/dma/direct.c
index a0ae9e05e04e,79da61b49fa4..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -133,20 -190,149 +133,147 @@@ void dma_direct_free(struct device *dev
  
  	if (force_dma_unencrypted())
  		set_memory_encrypted((unsigned long)cpu_addr, 1 << page_order);
++<<<<<<< HEAD
 +	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))
 +		free_pages((unsigned long)cpu_addr, page_order);
++=======
+ 	__dma_direct_free_pages(dev, size, virt_to_page(cpu_addr));
+ }
+ 
+ void *dma_direct_alloc(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
+ {
+ 	if (!dev_is_dma_coherent(dev))
+ 		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
+ 	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
+ }
+ 
+ void dma_direct_free(struct device *dev, size_t size,
+ 		void *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)
+ {
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
+ 	else
+ 		dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
+ }
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ 	phys_addr_t paddr = dma_to_phys(dev, addr);
+ 
+ 	if (unlikely(is_swiotlb_buffer(paddr)))
+ 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ 
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_sync_dma_for_device(dev, paddr, size, dir);
+ }
+ EXPORT_SYMBOL(dma_direct_sync_single_for_device);
+ 
+ void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i) {
+ 		if (unlikely(is_swiotlb_buffer(sg_phys(sg))))
+ 			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length,
+ 					dir, SYNC_FOR_DEVICE);
+ 
+ 		if (!dev_is_dma_coherent(dev))
+ 			arch_sync_dma_for_device(dev, sg_phys(sg), sg->length,
+ 					dir);
+ 	}
+ }
+ EXPORT_SYMBOL(dma_direct_sync_sg_for_device);
+ #endif
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ 	phys_addr_t paddr = dma_to_phys(dev, addr);
+ 
+ 	if (!dev_is_dma_coherent(dev)) {
+ 		arch_sync_dma_for_cpu(dev, paddr, size, dir);
+ 		arch_sync_dma_for_cpu_all(dev);
+ 	}
+ 
+ 	if (unlikely(is_swiotlb_buffer(paddr)))
+ 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ }
+ EXPORT_SYMBOL(dma_direct_sync_single_for_cpu);
+ 
+ void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i) {
+ 		if (!dev_is_dma_coherent(dev))
+ 			arch_sync_dma_for_cpu(dev, sg_phys(sg), sg->length, dir);
+ 	
+ 		if (unlikely(is_swiotlb_buffer(sg_phys(sg))))
+ 			swiotlb_tbl_sync_single(dev, sg_phys(sg), sg->length, dir,
+ 					SYNC_FOR_CPU);
+ 	}
+ 
+ 	if (!dev_is_dma_coherent(dev))
+ 		arch_sync_dma_for_cpu_all(dev);
+ }
+ EXPORT_SYMBOL(dma_direct_sync_sg_for_cpu);
+ 
+ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	phys_addr_t phys = dma_to_phys(dev, addr);
+ 
+ 	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+ 		dma_direct_sync_single_for_cpu(dev, addr, size, dir);
+ 
+ 	if (unlikely(is_swiotlb_buffer(phys)))
+ 		swiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);
+ }
+ EXPORT_SYMBOL(dma_direct_unmap_page);
+ 
+ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	for_each_sg(sgl, sg, nents, i)
+ 		dma_direct_unmap_page(dev, sg->dma_address, sg_dma_len(sg), dir,
+ 			     attrs);
+ }
+ EXPORT_SYMBOL(dma_direct_unmap_sg);
+ #endif
+ 
+ static inline bool dma_direct_possible(struct device *dev, dma_addr_t dma_addr,
+ 		size_t size)
+ {
+ 	return swiotlb_force != SWIOTLB_FORCE &&
+ 		(!dev || dma_capable(dev, dma_addr, size));
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  }
  
  dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
  		unsigned long offset, size_t size, enum dma_data_direction dir,
  		unsigned long attrs)
  {
 -	phys_addr_t phys = page_to_phys(page) + offset;
 -	dma_addr_t dma_addr = phys_to_dma(dev, phys);
 -
 -	if (unlikely(!dma_direct_possible(dev, dma_addr, size)) &&
 -	    !swiotlb_map(dev, &phys, &dma_addr, size, dir, attrs)) {
 -		report_addr(dev, dma_addr, size);
 -		return DMA_MAPPING_ERROR;
 -	}
 +	dma_addr_t dma_addr = phys_to_dma(dev, page_to_phys(page)) + offset;
  
 -	if (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 -		arch_sync_dma_for_device(dev, phys, size, dir);
 +	if (!check_addr(dev, dma_addr, size, __func__))
 +		return DIRECT_MAPPING_ERROR;
  	return dma_addr;
  }
+ EXPORT_SYMBOL(dma_direct_map_page);
  
  int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
  		enum dma_data_direction dir, unsigned long attrs)
@@@ -164,43 -349,29 +291,47 @@@
  	}
  
  	return nents;
 -
 -out_unmap:
 -	dma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
 -	return 0;
  }
+ EXPORT_SYMBOL(dma_direct_map_sg);
  
 -/*
 - * Because 32-bit DMA masks are so common we expect every architecture to be
 - * able to satisfy them - either by not supporting more physical memory, or by
 - * providing a ZONE_DMA32.  If neither is the case, the architecture needs to
 - * use an IOMMU instead of the direct mapping.
 - */
  int dma_direct_supported(struct device *dev, u64 mask)
  {
 -	u64 min_mask;
 -
 -	if (IS_ENABLED(CONFIG_ZONE_DMA))
 -		min_mask = DMA_BIT_MASK(ARCH_ZONE_DMA_BITS);
 -	else
 -		min_mask = DMA_BIT_MASK(32);
 -
 -	min_mask = min_t(u64, min_mask, (max_pfn - 1) << PAGE_SHIFT);
 +#ifdef CONFIG_ZONE_DMA
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(ARCH_ZONE_DMA_BITS)))
 +		return 0;
 +#else
 +	/*
 +	 * Because 32-bit DMA masks are so common we expect every architecture
 +	 * to be able to satisfy them - either by not supporting more physical
 +	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the
 +	 * architecture needs to use an IOMMU instead of the direct mapping.
 +	 */
 +	if (mask < phys_to_dma(dev, DMA_BIT_MASK(32)))
 +		return 0;
 +#endif
 +	/*
 +	 * Upstream PCI/PCIe bridges or SoC interconnects may not carry
 +	 * as many DMA address bits as the device itself supports.
 +	 */
 +	if (dev->bus_dma_mask && mask > dev->bus_dma_mask)
 +		return 0;
 +	return 1;
 +}
  
 -	return mask >= phys_to_dma(dev, min_mask);
 +int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return dma_addr == DIRECT_MAPPING_ERROR;
  }
++<<<<<<< HEAD
 +
 +const struct dma_map_ops dma_direct_ops = {
 +	.alloc			= dma_direct_alloc,
 +	.free			= dma_direct_free,
 +	.map_page		= dma_direct_map_page,
 +	.map_sg			= dma_direct_map_sg,
 +	.dma_supported		= dma_direct_supported,
 +	.mapping_error		= dma_direct_mapping_error,
 +};
 +EXPORT_SYMBOL(dma_direct_ops);
++=======
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
diff --cc kernel/dma/mapping.c
index 4f53f830671a,fc84c81029d9..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -5,9 -5,10 +5,14 @@@
   * Copyright (c) 2006  SUSE Linux Products GmbH
   * Copyright (c) 2006  Tejun Heo <teheo@suse.de>
   */
 -#include <linux/memblock.h> /* for max_pfn */
 +
  #include <linux/acpi.h>
++<<<<<<< HEAD
 +#include <linux/dma-mapping.h>
++=======
+ #include <linux/dma-direct.h>
+ #include <linux/dma-noncoherent.h>
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  #include <linux/export.h>
  #include <linux/gfp.h>
  #include <linux/of_device.h>
@@@ -223,7 -224,20 +228,24 @@@ int dma_common_get_sgtable(struct devic
  		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
  	return ret;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(dma_common_get_sgtable);
++=======
+ 
+ int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (!dma_is_direct(ops) && ops->get_sgtable)
+ 		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+ 					attrs);
+ 	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+ 			attrs);
+ }
+ EXPORT_SYMBOL(dma_get_sgtable_attrs);
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  
  /*
   * Create userspace mapping for the DMA-coherent memory.
@@@ -242,99 -258,184 +264,226 @@@ int dma_common_mmap(struct device *dev
  	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
  		return ret;
  
 -	if (off >= count || user_count > count - off)
 -		return -ENXIO;
 +	if (off < count && user_count <= (count - off))
 +		ret = remap_pfn_range(vma, vma->vm_start,
 +				      page_to_pfn(virt_to_page(cpu_addr)) + off,
 +				      user_count << PAGE_SHIFT,
 +				      vma->vm_page_prot);
 +#endif	/* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
  
 -	if (!dev_is_dma_coherent(dev)) {
 -		if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_COHERENT_TO_PFN))
 -			return -ENXIO;
 -		pfn = arch_dma_coherent_to_pfn(dev, cpu_addr, dma_addr);
 -	} else {
 -		pfn = page_to_pfn(virt_to_page(cpu_addr));
 +	return ret;
 +}
 +EXPORT_SYMBOL(dma_common_mmap);
 +
 +#ifdef CONFIG_MMU
 +static struct vm_struct *__dma_common_pages_remap(struct page **pages,
 +			size_t size, unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
 +{
 +	struct vm_struct *area;
 +
 +	area = get_vm_area_caller(size, vm_flags, caller);
 +	if (!area)
 +		return NULL;
 +
 +	if (map_vm_area(area, prot, pages)) {
 +		vunmap(area->addr);
 +		return NULL;
  	}
  
 -	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
 -			user_count << PAGE_SHIFT, vma->vm_page_prot);
 -#else
 -	return -ENXIO;
 -#endif /* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
 +	return area;
  }
  
 -/**
 - * dma_mmap_attrs - map a coherent DMA allocation into user space
 - * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 - * @vma: vm_area_struct describing requested user mapping
 - * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 - * @dma_addr: device-view address returned from dma_alloc_attrs
 - * @size: size of memory originally requested in dma_alloc_attrs
 - * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 - *
 - * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user
 - * space.  The coherent DMA buffer must not be freed by the driver until the
 - * user space mapping has been released.
 +/*
 + * remaps an array of PAGE_SIZE pages into another vm_area
 + * Cannot be used in non-sleeping contexts
   */
 -int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
 -		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 -		unsigned long attrs)
 +void *dma_common_pages_remap(struct page **pages, size_t size,
 +			unsigned long vm_flags, pgprot_t prot,
 +			const void *caller)
  {
++<<<<<<< HEAD
 +	struct vm_struct *area;
 +
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +	if (!area)
 +		return NULL;
 +
 +	area->pages = pages;
 +
 +	return area->addr;
++=======
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (!dma_is_direct(ops) && ops->mmap)
+ 		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+ 	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  }
 -EXPORT_SYMBOL(dma_mmap_attrs);
  
 -#ifndef ARCH_HAS_DMA_GET_REQUIRED_MASK
 -static u64 dma_default_get_required_mask(struct device *dev)
 +/*
 + * remaps an allocated contiguous region into another vm_area.
 + * Cannot be used in non-sleeping contexts
 + */
 +
 +void *dma_common_contiguous_remap(struct page *page, size_t size,
 +			unsigned long vm_flags,
 +			pgprot_t prot, const void *caller)
  {
 -	u32 low_totalram = ((max_pfn - 1) << PAGE_SHIFT);
 -	u32 high_totalram = ((max_pfn - 1) >> (32 - PAGE_SHIFT));
 -	u64 mask;
 -
 -	if (!high_totalram) {
 -		/* convert to mask just covering totalram */
 -		low_totalram = (1 << (fls(low_totalram) - 1));
 -		low_totalram += low_totalram - 1;
 -		mask = low_totalram;
 -	} else {
 -		high_totalram = (1 << (fls(high_totalram) - 1));
 -		high_totalram += high_totalram - 1;
 -		mask = (((u64)high_totalram) << 32) + 0xffffffff;
 +	int i;
 +	struct page **pages;
 +	struct vm_struct *area;
 +
 +	pages = kmalloc(sizeof(struct page *) << get_order(size), GFP_KERNEL);
 +	if (!pages)
 +		return NULL;
 +
 +	for (i = 0; i < (size >> PAGE_SHIFT); i++)
 +		pages[i] = nth_page(page, i);
 +
 +	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
 +
 +	kfree(pages);
 +
 +	if (!area)
 +		return NULL;
 +	return area->addr;
 +}
 +
 +/*
 + * unmaps a range previously mapped by dma_common_*_remap
 + */
 +void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
 +{
 +	struct vm_struct *area = find_vm_area(cpu_addr);
 +
 +	if (!area || (area->flags & vm_flags) != vm_flags) {
 +		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 +		return;
  	}
 -	return mask;
 +
 +	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 +	vunmap(cpu_addr);
  }
++<<<<<<< HEAD
++=======
+ 
+ u64 dma_get_required_mask(struct device *dev)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_get_required_mask(dev);
+ 	if (ops->get_required_mask)
+ 		return ops->get_required_mask(dev);
+ 	return dma_default_get_required_mask(dev);
+ }
+ EXPORT_SYMBOL_GPL(dma_get_required_mask);
+ #endif
+ 
+ #ifndef arch_dma_alloc_attrs
+ #define arch_dma_alloc_attrs(dev)	(true)
+ #endif
+ 
+ void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t flag, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	void *cpu_addr;
+ 
+ 	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
+ 
+ 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
+ 		return cpu_addr;
+ 
+ 	/* let the implementation decide on the zone to allocate from: */
+ 	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
+ 
+ 	if (!arch_dma_alloc_attrs(&dev))
+ 		return NULL;
+ 
+ 	if (dma_is_direct(ops))
+ 		cpu_addr = dma_direct_alloc(dev, size, dma_handle, flag, attrs);
+ 	else if (ops->alloc)
+ 		cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+ 	else
+ 		return NULL;
+ 
+ 	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+ 	return cpu_addr;
+ }
+ EXPORT_SYMBOL(dma_alloc_attrs);
+ 
+ void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
+ 		return;
+ 	/*
+ 	 * On non-coherent platforms which implement DMA-coherent buffers via
+ 	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
+ 	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
+ 	 * sleep on some machines, and b) an indication that the driver is
+ 	 * probably misusing the coherent API anyway.
+ 	 */
+ 	WARN_ON(irqs_disabled());
+ 
+ 	if (!cpu_addr)
+ 		return;
+ 
+ 	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+ 	if (dma_is_direct(ops))
+ 		dma_direct_free(dev, size, cpu_addr, dma_handle, attrs);
+ 	else if (ops->free)
+ 		ops->free(dev, size, cpu_addr, dma_handle, attrs);
+ }
+ EXPORT_SYMBOL(dma_free_attrs);
+ 
+ static inline void dma_check_mask(struct device *dev, u64 mask)
+ {
+ 	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
+ 		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
+ }
+ 
+ int dma_supported(struct device *dev, u64 mask)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_supported(dev, mask);
+ 	if (ops->dma_supported)
+ 		return 1;
+ 	return ops->dma_supported(dev, mask);
+ }
+ EXPORT_SYMBOL(dma_supported);
+ 
+ #ifndef HAVE_ARCH_DMA_SET_MASK
+ int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dev->dma_mask || !dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	*dev->dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_mask);
+ #endif
+ 
+ #ifndef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
+ int dma_set_coherent_mask(struct device *dev, u64 mask)
+ {
+ 	if (!dma_supported(dev, mask))
+ 		return -EIO;
+ 
+ 	dma_check_mask(dev, mask);
+ 	dev->coherent_dma_mask = mask;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dma_set_coherent_mask);
++>>>>>>> 356da6d0cde3 (dma-mapping: bypass indirect calls for dma-direct)
  #endif
  
  void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
diff --git a/arch/alpha/include/asm/dma-mapping.h b/arch/alpha/include/asm/dma-mapping.h
index 8beeafd4f68e..0ee6a5c99b16 100644
--- a/arch/alpha/include/asm/dma-mapping.h
+++ b/arch/alpha/include/asm/dma-mapping.h
@@ -7,7 +7,7 @@ extern const struct dma_map_ops alpha_pci_ops;
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
 #ifdef CONFIG_ALPHA_JENSEN
-	return &dma_direct_ops;
+	return NULL;
 #else
 	return &alpha_pci_ops;
 #endif
* Unmerged path arch/arc/mm/cache.c
diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 8436f6ade57d..74ef7650bc2b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -18,7 +18,7 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : &dma_direct_ops;
+	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : NULL;
 }
 
 #ifdef __arch_page_to_dma
diff --git a/arch/arm/mm/dma-mapping-nommu.c b/arch/arm/mm/dma-mapping-nommu.c
index f448a0663b10..274bb5231db2 100644
--- a/arch/arm/mm/dma-mapping-nommu.c
+++ b/arch/arm/mm/dma-mapping-nommu.c
@@ -22,7 +22,7 @@
 #include "dma.h"
 
 /*
- *  dma_direct_ops is used if
+ *  The generic direct mapping code is used if
  *   - MMU/MPU is off
  *   - cpu is v7m w/o cache support
  *   - device is coherent
@@ -208,16 +208,9 @@ const struct dma_map_ops arm_nommu_dma_ops = {
 };
 EXPORT_SYMBOL(arm_nommu_dma_ops);
 
-static const struct dma_map_ops *arm_nommu_get_dma_map_ops(bool coherent)
-{
-	return coherent ? &dma_direct_ops : &arm_nommu_dma_ops;
-}
-
 void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 			const struct iommu_ops *iommu, bool coherent)
 {
-	const struct dma_map_ops *dma_ops;
-
 	if (IS_ENABLED(CONFIG_CPU_V7M)) {
 		/*
 		 * Cache support for v7m is optional, so can be treated as
@@ -233,9 +226,8 @@ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 		dev->archdata.dma_coherent = (get_cr() & CR_M) ? coherent : true;
 	}
 
-	dma_ops = arm_nommu_get_dma_map_ops(dev->archdata.dma_coherent);
-
-	set_dma_ops(dev, dma_ops);
+	if (!dev->archdata.dma_coherent)
+		set_dma_ops(dev, &arm_nommu_dma_ops);
 }
 
 void arch_teardown_dma_ops(struct device *dev)
* Unmerged path arch/arm64/mm/dma-mapping.c
* Unmerged path arch/ia64/hp/common/hwsw_iommu.c
* Unmerged path arch/ia64/hp/common/sba_iommu.c
* Unmerged path arch/ia64/kernel/dma-mapping.c
* Unmerged path arch/mips/include/asm/dma-mapping.h
* Unmerged path arch/parisc/kernel/setup.c
* Unmerged path arch/sparc/include/asm/dma-mapping.h
diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c
index acfd04121da3..5fa468d36d3a 100644
--- a/arch/x86/kernel/pci-dma.c
+++ b/arch/x86/kernel/pci-dma.c
@@ -17,7 +17,7 @@
 
 static bool disable_dac_quirk __read_mostly;
 
-const struct dma_map_ops *dma_ops = &dma_direct_ops;
+const struct dma_map_ops *dma_ops;
 EXPORT_SYMBOL(dma_ops);
 
 #ifdef CONFIG_IOMMU_DEBUG
diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_drv.c b/drivers/gpu/drm/vmwgfx/vmwgfx_drv.c
index 09cc721160c4..c918b1a330b1 100644
--- a/drivers/gpu/drm/vmwgfx/vmwgfx_drv.c
+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_drv.c
@@ -560,7 +560,7 @@ static int vmw_dma_select_mode(struct vmw_private *dev_priv)
 
 	dev_priv->map_mode = vmw_dma_map_populate;
 
-	if (dma_ops->sync_single_for_cpu)
+	if (dma_ops && dma_ops->sync_single_for_cpu)
 		dev_priv->map_mode = vmw_dma_alloc_coherent;
 #ifdef CONFIG_SWIOTLB
 	if (swiotlb_nr_tbl() == 0)
diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c
index 1639e138bdd4..75ec30b1a4c9 100644
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@ -2188,7 +2188,7 @@ static int amd_iommu_add_device(struct device *dev)
 				dev_name(dev));
 
 		iommu_ignore_device(dev);
-		dev->dma_ops = &dma_direct_ops;
+		dev->dma_ops = NULL;
 		goto out;
 	}
 	init_iommu_group(dev);
@@ -2774,17 +2774,6 @@ int __init amd_iommu_init_dma_ops(void)
 	swiotlb        = (iommu_pass_through || sme_me_mask) ? 1 : 0;
 	iommu_detected = 1;
 
-	/*
-	 * In case we don't initialize SWIOTLB (actually the common case
-	 * when AMD IOMMU is enabled and SME is not active), make sure there
-	 * are global dma_ops set as a fall-back for devices not handled by
-	 * this driver (for example non-PCI devices). When SME is active,
-	 * make sure that swiotlb variable remains set so the global dma_ops
-	 * continue to be SWIOTLB.
-	 */
-	if (!swiotlb)
-		dma_ops = &dma_direct_ops;
-
 	if (amd_iommu_unmap_flush)
 		pr_info("AMD-Vi: IO/TLB flush on unmap enabled\n");
 	else
* Unmerged path include/asm-generic/dma-mapping.h
* Unmerged path include/linux/dma-direct.h
* Unmerged path include/linux/dma-mapping.h
diff --git a/include/linux/dma-noncoherent.h b/include/linux/dma-noncoherent.h
index a0aa00cc909d..8bf2395a537f 100644
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@ -21,7 +21,10 @@ int arch_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 void arch_dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 		enum dma_data_direction direction);
 #else
-#define arch_dma_cache_sync NULL
+static inline void arch_dma_cache_sync(struct device *dev, void *vaddr,
+		size_t size, enum dma_data_direction direction)
+{
+}
 #endif /* CONFIG_DMA_NONCOHERENT_CACHE_SYNC */
 
 #ifdef CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/mapping.c
