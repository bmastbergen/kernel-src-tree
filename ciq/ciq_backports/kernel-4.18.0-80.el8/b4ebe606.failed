dma-direct: implement complete bus_dma_mask handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Christoph Hellwig <hch@lst.de>
commit b4ebe6063204da58e48600b810a97c29ae9e5d12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/b4ebe606.failed

Instead of rejecting devices with a too small bus_dma_mask we can handle
by taking the bus dma_mask into account for allocations and bounce
buffering decisions.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit b4ebe6063204da58e48600b810a97c29ae9e5d12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index 2ba3b52698ea,60c433b880e0..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -49,15 -55,51 +50,59 @@@ check_addr(struct device *dev, dma_addr
  	return true;
  }
  
++<<<<<<< HEAD
 +static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
 +{
 +	dma_addr_t addr = force_dma_unencrypted() ?
 +		__phys_to_dma(dev, phys) : phys_to_dma(dev, phys);
 +	return addr + size - 1 <= dev->coherent_dma_mask;
++=======
+ static inline dma_addr_t phys_to_dma_direct(struct device *dev,
+ 		phys_addr_t phys)
+ {
+ 	if (force_dma_unencrypted())
+ 		return __phys_to_dma(dev, phys);
+ 	return phys_to_dma(dev, phys);
+ }
+ 
+ u64 dma_direct_get_required_mask(struct device *dev)
+ {
+ 	u64 max_dma = phys_to_dma_direct(dev, (max_pfn - 1) << PAGE_SHIFT);
+ 
+ 	if (dev->bus_dma_mask && dev->bus_dma_mask < max_dma)
+ 		max_dma = dev->bus_dma_mask;
+ 
+ 	return (1ULL << (fls64(max_dma) - 1)) * 2 - 1;
+ }
+ 
+ static gfp_t __dma_direct_optimal_gfp_mask(struct device *dev, u64 dma_mask,
+ 		u64 *phys_mask)
+ {
+ 	if (dev->bus_dma_mask && dev->bus_dma_mask < dma_mask)
+ 		dma_mask = dev->bus_dma_mask;
+ 
+ 	if (force_dma_unencrypted())
+ 		*phys_mask = __dma_to_phys(dev, dma_mask);
+ 	else
+ 		*phys_mask = dma_to_phys(dev, dma_mask);
+ 
+ 	/* GFP_DMA32 and GFP_DMA are no ops without the corresponding zones: */
+ 	if (*phys_mask <= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))
+ 		return GFP_DMA;
+ 	if (*phys_mask <= DMA_BIT_MASK(32))
+ 		return GFP_DMA32;
+ 	return 0;
+ }
+ 
+ static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
+ {
+ 	return phys_to_dma_direct(dev, phys) + size - 1 <=
+ 			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_mask);
++>>>>>>> b4ebe6063204 (dma-direct: implement complete bus_dma_mask handling)
  }
  
 -void *dma_direct_alloc_pages(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 +void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +		gfp_t gfp, unsigned long attrs)
  {
  	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
  	int page_order = get_order(size);
diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 38d0cd3bc850..d7baa2940c42 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -29,7 +29,8 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 	if (!dev->dma_mask)
 		return false;
 
-	return addr + size - 1 <= *dev->dma_mask;
+	return addr + size - 1 <=
+		min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
* Unmerged path kernel/dma/direct.c
