x86/speculation: Prepare for conditional IBPB in switch_mm()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 4c71a2b6fd7e42814aa68a6dec88abf3b42ea573
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/4c71a2b6.failed

The IBPB speculation barrier is issued from switch_mm() when the kernel
switches to a user space task with a different mm than the user space task
which ran last on the same CPU.

An additional optimization is to avoid IBPB when the incoming task can be
ptraced by the outgoing task. This optimization only works when switching
directly between two user space tasks. When switching from a kernel task to
a user space task the optimization fails because the previous task cannot
be accessed anymore. So for quite some scenarios the optimization is just
adding overhead.

The upcoming conditional IBPB support will issue IBPB only for user space
tasks which have the TIF_SPEC_IB bit set. This requires to handle the
following cases:

  1) Switch from a user space task (potential attacker) which has
     TIF_SPEC_IB set to a user space task (potential victim) which has
     TIF_SPEC_IB not set.

  2) Switch from a user space task (potential attacker) which has
     TIF_SPEC_IB not set to a user space task (potential victim) which has
     TIF_SPEC_IB set.

This needs to be optimized for the case where the IBPB can be avoided when
only kernel threads ran in between user space tasks which belong to the
same process.

The current check whether two tasks belong to the same context is using the
tasks context id. While correct, it's simpler to use the mm pointer because
it allows to mangle the TIF_SPEC_IB bit into it. The context id based
mechanism requires extra storage, which creates worse code.

When a task is scheduled out its TIF_SPEC_IB bit is mangled as bit 0 into
the per CPU storage which is used to track the last user space mm which was
running on a CPU. This bit can be used together with the TIF_SPEC_IB bit of
the incoming task to make the decision whether IBPB needs to be issued or
not to cover the two cases above.

As conditional IBPB is going to be the default, remove the dubious ptrace
check for the IBPB always case and simply issue IBPB always when the
process changes.

Move the storage to a different place in the struct as the original one
created a hole.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: David Woodhouse <dwmw@amazon.co.uk>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Casey Schaufler <casey.schaufler@intel.com>
	Cc: Asit Mallick <asit.k.mallick@intel.com>
	Cc: Arjan van de Ven <arjan@linux.intel.com>
	Cc: Jon Masters <jcm@redhat.com>
	Cc: Waiman Long <longman9394@gmail.com>
	Cc: Greg KH <gregkh@linuxfoundation.org>
	Cc: Dave Stewart <david.c.stewart@intel.com>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: stable@vger.kernel.org
Link: https://lkml.kernel.org/r/20181125185005.466447057@linutronix.de

(cherry picked from commit 4c71a2b6fd7e42814aa68a6dec88abf3b42ea573)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/mm/tlb.c
diff --cc arch/x86/include/asm/nospec-branch.h
index c202a64edd95,d4d35baf0430..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -303,6 -311,10 +303,13 @@@ do {									
  	preempt_enable();						\
  } while (0)
  
++<<<<<<< HEAD
++=======
+ DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+ DECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+ DECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+ 
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  #endif /* __ASSEMBLY__ */
  
  /*
diff --cc arch/x86/kernel/cpu/bugs.c
index 85d19b8cdb86,7c946a9af947..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -51,6 -54,13 +51,16 @@@ static u64 __ro_after_init x86_spec_ctr
  u64 __ro_after_init x86_amd_ls_cfg_base;
  u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
  
++<<<<<<< HEAD
++=======
+ /* Control conditional STIPB in switch_to() */
+ DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+ /* Control conditional IBPB in switch_mm() */
+ DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+ /* Control unconditional IBPB in switch_mm() */
+ DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+ 
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  void __init check_bugs(void)
  {
  	identify_boot_cpu();
@@@ -255,6 -241,131 +265,134 @@@ static inline bool match_option(const c
  	return len == arglen && !strncmp(arg, opt, len);
  }
  
++<<<<<<< HEAD
++=======
+ /* The kernel command line selection for spectre v2 */
+ enum spectre_v2_mitigation_cmd {
+ 	SPECTRE_V2_CMD_NONE,
+ 	SPECTRE_V2_CMD_AUTO,
+ 	SPECTRE_V2_CMD_FORCE,
+ 	SPECTRE_V2_CMD_RETPOLINE,
+ 	SPECTRE_V2_CMD_RETPOLINE_GENERIC,
+ 	SPECTRE_V2_CMD_RETPOLINE_AMD,
+ };
+ 
+ enum spectre_v2_user_cmd {
+ 	SPECTRE_V2_USER_CMD_NONE,
+ 	SPECTRE_V2_USER_CMD_AUTO,
+ 	SPECTRE_V2_USER_CMD_FORCE,
+ };
+ 
+ static const char * const spectre_v2_user_strings[] = {
+ 	[SPECTRE_V2_USER_NONE]		= "User space: Vulnerable",
+ 	[SPECTRE_V2_USER_STRICT]	= "User space: Mitigation: STIBP protection",
+ };
+ 
+ static const struct {
+ 	const char			*option;
+ 	enum spectre_v2_user_cmd	cmd;
+ 	bool				secure;
+ } v2_user_options[] __initdata = {
+ 	{ "auto",	SPECTRE_V2_USER_CMD_AUTO,	false },
+ 	{ "off",	SPECTRE_V2_USER_CMD_NONE,	false },
+ 	{ "on",		SPECTRE_V2_USER_CMD_FORCE,	true  },
+ };
+ 
+ static void __init spec_v2_user_print_cond(const char *reason, bool secure)
+ {
+ 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)
+ 		pr_info("spectre_v2_user=%s forced on command line.\n", reason);
+ }
+ 
+ static enum spectre_v2_user_cmd __init
+ spectre_v2_parse_user_cmdline(enum spectre_v2_mitigation_cmd v2_cmd)
+ {
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	switch (v2_cmd) {
+ 	case SPECTRE_V2_CMD_NONE:
+ 		return SPECTRE_V2_USER_CMD_NONE;
+ 	case SPECTRE_V2_CMD_FORCE:
+ 		return SPECTRE_V2_USER_CMD_FORCE;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	ret = cmdline_find_option(boot_command_line, "spectre_v2_user",
+ 				  arg, sizeof(arg));
+ 	if (ret < 0)
+ 		return SPECTRE_V2_USER_CMD_AUTO;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(v2_user_options); i++) {
+ 		if (match_option(arg, ret, v2_user_options[i].option)) {
+ 			spec_v2_user_print_cond(v2_user_options[i].option,
+ 						v2_user_options[i].secure);
+ 			return v2_user_options[i].cmd;
+ 		}
+ 	}
+ 
+ 	pr_err("Unknown user space protection option (%s). Switching to AUTO select\n", arg);
+ 	return SPECTRE_V2_USER_CMD_AUTO;
+ }
+ 
+ static void __init
+ spectre_v2_user_select_mitigation(enum spectre_v2_mitigation_cmd v2_cmd)
+ {
+ 	enum spectre_v2_user_mitigation mode = SPECTRE_V2_USER_NONE;
+ 	bool smt_possible = IS_ENABLED(CONFIG_SMP);
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_STIBP))
+ 		return;
+ 
+ 	if (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||
+ 	    cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ 		smt_possible = false;
+ 
+ 	switch (spectre_v2_parse_user_cmdline(v2_cmd)) {
+ 	case SPECTRE_V2_USER_CMD_AUTO:
+ 	case SPECTRE_V2_USER_CMD_NONE:
+ 		goto set_mode;
+ 	case SPECTRE_V2_USER_CMD_FORCE:
+ 		mode = SPECTRE_V2_USER_STRICT;
+ 		break;
+ 	}
+ 
+ 	/* Initialize Indirect Branch Prediction Barrier */
+ 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+ 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+ 
+ 		switch (mode) {
+ 		case SPECTRE_V2_USER_STRICT:
+ 			static_branch_enable(&switch_mm_always_ibpb);
+ 			break;
+ 		default:
+ 			break;
+ 		}
+ 
+ 		pr_info("mitigation: Enabling %s Indirect Branch Prediction Barrier\n",
+ 			mode == SPECTRE_V2_USER_STRICT ? "always-on" : "conditional");
+ 	}
+ 
+ 	/* If enhanced IBRS is enabled no STIPB required */
+ 	if (spectre_v2_enabled == SPECTRE_V2_IBRS_ENHANCED)
+ 		return;
+ 
+ set_mode:
+ 	spectre_v2_user = mode;
+ 	/* Only print the STIBP mode when SMT possible */
+ 	if (smt_possible)
+ 		pr_info("%s\n", spectre_v2_user_strings[mode]);
+ }
+ 
+ static const char * const spectre_v2_strings[] = {
+ 	[SPECTRE_V2_NONE]			= "Vulnerable",
+ 	[SPECTRE_V2_RETPOLINE_GENERIC]		= "Mitigation: Full generic retpoline",
+ 	[SPECTRE_V2_RETPOLINE_AMD]		= "Mitigation: Full AMD retpoline",
+ 	[SPECTRE_V2_IBRS_ENHANCED]		= "Mitigation: Enhanced IBRS",
+ };
+ 
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  static const struct {
  	const char *option;
  	enum spectre_v2_mitigation_cmd cmd;
@@@ -721,6 -918,68 +859,71 @@@ static void __init l1tf_select_mitigati
  
  #ifdef CONFIG_SYSFS
  
++<<<<<<< HEAD
++=======
+ #define L1TF_DEFAULT_MSG "Mitigation: PTE Inversion"
+ 
+ #if IS_ENABLED(CONFIG_KVM_INTEL)
+ static const char * const l1tf_vmx_states[] = {
+ 	[VMENTER_L1D_FLUSH_AUTO]		= "auto",
+ 	[VMENTER_L1D_FLUSH_NEVER]		= "vulnerable",
+ 	[VMENTER_L1D_FLUSH_COND]		= "conditional cache flushes",
+ 	[VMENTER_L1D_FLUSH_ALWAYS]		= "cache flushes",
+ 	[VMENTER_L1D_FLUSH_EPT_DISABLED]	= "EPT disabled",
+ 	[VMENTER_L1D_FLUSH_NOT_REQUIRED]	= "flush not necessary"
+ };
+ 
+ static ssize_t l1tf_show_state(char *buf)
+ {
+ 	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+ 		return sprintf(buf, "%s\n", L1TF_DEFAULT_MSG);
+ 
+ 	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+ 	    (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+ 	     sched_smt_active())) {
+ 		return sprintf(buf, "%s; VMX: %s\n", L1TF_DEFAULT_MSG,
+ 			       l1tf_vmx_states[l1tf_vmx_mitigation]);
+ 	}
+ 
+ 	return sprintf(buf, "%s; VMX: %s, SMT %s\n", L1TF_DEFAULT_MSG,
+ 		       l1tf_vmx_states[l1tf_vmx_mitigation],
+ 		       sched_smt_active() ? "vulnerable" : "disabled");
+ }
+ #else
+ static ssize_t l1tf_show_state(char *buf)
+ {
+ 	return sprintf(buf, "%s\n", L1TF_DEFAULT_MSG);
+ }
+ #endif
+ 
+ static char *stibp_state(void)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_IBRS_ENHANCED)
+ 		return "";
+ 
+ 	switch (spectre_v2_user) {
+ 	case SPECTRE_V2_USER_NONE:
+ 		return ", STIBP: disabled";
+ 	case SPECTRE_V2_USER_STRICT:
+ 		return ", STIBP: forced";
+ 	}
+ 	return "";
+ }
+ 
+ static char *ibpb_state(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+ 		switch (spectre_v2_user) {
+ 		case SPECTRE_V2_USER_NONE:
+ 			return ", IBPB: disabled";
+ 		case SPECTRE_V2_USER_STRICT:
+ 			return ", IBPB: always-on";
+ 		}
+ 	}
+ 	return "";
+ }
+ 
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
  			       char *buf, unsigned int bug)
  {
diff --cc arch/x86/mm/tlb.c
index 80c33a856e3a,03b6b4c2238d..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -264,12 -340,33 +339,15 @@@ void switch_mm_irqs_off(struct mm_struc
  				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
  			cpumask_set_cpu(cpu, mm_cpumask(next));
  
 -		/*
 -		 * If the CPU is not in lazy TLB mode, we are just switching
 -		 * from one thread in a process to another thread in the same
 -		 * process. No TLB flush required.
 -		 */
 -		if (!was_lazy)
 -			return;
 -
 -		/*
 -		 * Read the tlb_gen to check whether a flush is needed.
 -		 * If the TLB is up to date, just use it.
 -		 * The barrier synchronizes with the tlb_gen increment in
 -		 * the TLB shootdown code.
 -		 */
 -		smp_mb();
 -		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
 -		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
 -				next_tlb_gen)
 -			return;
 -
 -		/*
 -		 * TLB contents went out of date while we were in lazy
 -		 * mode. Fall through to the TLB switching code below.
 -		 */
 -		new_asid = prev_asid;
 -		need_flush = true;
 +		return;
  	} else {
++<<<<<<< HEAD
 +		u16 new_asid;
 +		bool need_flush;
 +		u64 last_ctx_id = this_cpu_read(cpu_tlbstate.last_ctx_id);
 +
++=======
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  		/*
  		 * Avoid user/user BTB poisoning by flushing the branch
  		 * predictor when switching between processes. This stops
@@@ -310,46 -406,40 +380,59 @@@
  		/* Let nmi_uaccess_okay() know that we're changing CR3. */
  		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
  		barrier();
 -	}
  
 -	if (need_flush) {
 -		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
 -		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
 -		load_new_mm_cr3(next->pgd, new_asid, true);
 +		if (need_flush) {
 +			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
 +			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
 +			load_new_mm_cr3(next->pgd, new_asid, true);
 +
 +			/*
 +			 * NB: This gets called via leave_mm() in the idle path
 +			 * where RCU functions differently.  Tracing normally
 +			 * uses RCU, so we need to use the _rcuidle variant.
 +			 *
 +			 * (There is no good reason for this.  The idle code should
 +			 *  be rearranged to call this before rcu_idle_enter().)
 +			 */
 +			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 +		} else {
 +			/* The new ASID is already up to date. */
 +			load_new_mm_cr3(next->pgd, new_asid, false);
 +
 +			/* See above wrt _rcuidle. */
 +			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
 +		}
  
  		/*
 -		 * NB: This gets called via leave_mm() in the idle path
 -		 * where RCU functions differently.  Tracing normally
 -		 * uses RCU, so we need to use the _rcuidle variant.
 -		 *
 -		 * (There is no good reason for this.  The idle code should
 -		 *  be rearranged to call this before rcu_idle_enter().)
 +		 * Record last user mm's context id, so we can avoid
 +		 * flushing branch buffer with IBPB if we switch back
 +		 * to the same user.
  		 */
 -		trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 -	} else {
 -		/* The new ASID is already up to date. */
 -		load_new_mm_cr3(next->pgd, new_asid, false);
 +		if (next != &init_mm)
 +			this_cpu_write(cpu_tlbstate.last_ctx_id, next->context.ctx_id);
  
 -		/* See above wrt _rcuidle. */
 -		trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
 +		/* Make sure we write CR3 before loaded_mm. */
 +		barrier();
 +
 +		this_cpu_write(cpu_tlbstate.loaded_mm, next);
 +		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
  	}
  
++<<<<<<< HEAD
 +	load_mm_cr4(next);
 +	switch_ldt(real_prev, next);
++=======
+ 	/* Make sure we write CR3 before loaded_mm. */
+ 	barrier();
+ 
+ 	this_cpu_write(cpu_tlbstate.loaded_mm, next);
+ 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
+ 
+ 	if (next != real_prev) {
+ 		load_mm_cr4(next);
+ 		switch_ldt(real_prev, next);
+ 	}
++>>>>>>> 4c71a2b6fd7e (x86/speculation: Prepare for conditional IBPB in switch_mm())
  }
  
  /*
* Unmerged path arch/x86/include/asm/nospec-branch.h
diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h
index 0af97e51e609..1bbc43077583 100644
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -185,10 +185,14 @@ struct tlb_state {
 
 #define LOADED_MM_SWITCHING ((struct mm_struct *)1)
 
+	/* Last user mm for optimizing IBPB */
+	union {
+		struct mm_struct	*last_user_mm;
+		unsigned long		last_user_mm_ibpb;
+	};
+
 	u16 loaded_mm_asid;
 	u16 next_asid;
-	/* last user mm's ctx id */
-	u64 last_ctx_id;
 
 	/*
 	 * We can be in one of several states:
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/mm/tlb.c
