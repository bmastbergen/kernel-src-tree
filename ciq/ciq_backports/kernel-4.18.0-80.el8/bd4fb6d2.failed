arm64: Add support for SB barrier and patch in over DSB; ISB sequences

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Will Deacon <will.deacon@arm.com>
commit bd4fb6d270bc423a9a4098108784f7f9254c4e6d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/bd4fb6d2.failed

We currently use a DSB; ISB sequence to inhibit speculation in set_fs().
Whilst this works for current CPUs, future CPUs may implement a new SB
barrier instruction which acts as an architected speculation barrier.

On CPUs that support it, patch in an SB; NOP sequence over the DSB; ISB
sequence and advertise the presence of the new instruction to userspace.

	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit bd4fb6d270bc423a9a4098108784f7f9254c4e6d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/cpucaps.h
#	arch/arm64/include/uapi/asm/hwcap.h
#	arch/arm64/kernel/cpufeature.c
#	arch/arm64/kernel/cpuinfo.c
diff --cc arch/arm64/include/asm/cpucaps.h
index 8a699c708fc9,b7f0709a21af..000000000000
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@@ -49,7 -49,13 +49,18 @@@
  #define ARM64_HAS_CACHE_DIC			28
  #define ARM64_HW_DBM				29
  #define ARM64_SSBD				30
++<<<<<<< HEAD
 +
 +#define ARM64_NCAPS				31
++=======
+ #define ARM64_MISMATCHED_CACHE_TYPE		31
+ #define ARM64_HAS_STAGE2_FWB			32
+ #define ARM64_HAS_CRC32				33
+ #define ARM64_SSBS				34
+ #define ARM64_WORKAROUND_1188873		35
+ #define ARM64_HAS_SB				36
+ 
+ #define ARM64_NCAPS				37
++>>>>>>> bd4fb6d270bc (arm64: Add support for SB barrier and patch in over DSB; ISB sequences)
  
  #endif /* __ASM_CPUCAPS_H */
diff --cc arch/arm64/include/uapi/asm/hwcap.h
index 17c65c8f33cb,7784f7cba16c..000000000000
--- a/arch/arm64/include/uapi/asm/hwcap.h
+++ b/arch/arm64/include/uapi/asm/hwcap.h
@@@ -48,5 -48,7 +48,10 @@@
  #define HWCAP_USCAT		(1 << 25)
  #define HWCAP_ILRCPC		(1 << 26)
  #define HWCAP_FLAGM		(1 << 27)
++<<<<<<< HEAD
++=======
+ #define HWCAP_SSBS		(1 << 28)
+ #define HWCAP_SB		(1 << 29)
++>>>>>>> bd4fb6d270bc (arm64: Add support for SB barrier and patch in over DSB; ISB sequences)
  
  #endif /* _UAPI__ASM_HWCAP_H */
diff --cc arch/arm64/kernel/cpufeature.c
index c6d80743f4ed,e6467e64ee91..000000000000
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@@ -1202,6 -1364,51 +1203,54 @@@ static const struct arm64_cpu_capabilit
  		.cpu_enable = cpu_enable_hw_dbm,
  	},
  #endif
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_ARM64_SSBD
+ 	{
+ 		.desc = "CRC32 instructions",
+ 		.capability = ARM64_HAS_CRC32,
+ 		.type = ARM64_CPUCAP_SYSTEM_FEATURE,
+ 		.matches = has_cpuid_feature,
+ 		.sys_reg = SYS_ID_AA64ISAR0_EL1,
+ 		.field_pos = ID_AA64ISAR0_CRC32_SHIFT,
+ 		.min_field_value = 1,
+ 	},
+ 	{
+ 		.desc = "Speculative Store Bypassing Safe (SSBS)",
+ 		.capability = ARM64_SSBS,
+ 		.type = ARM64_CPUCAP_WEAK_LOCAL_CPU_FEATURE,
+ 		.matches = has_cpuid_feature,
+ 		.sys_reg = SYS_ID_AA64PFR1_EL1,
+ 		.field_pos = ID_AA64PFR1_SSBS_SHIFT,
+ 		.sign = FTR_UNSIGNED,
+ 		.min_field_value = ID_AA64PFR1_SSBS_PSTATE_ONLY,
+ 		.cpu_enable = cpu_enable_ssbs,
+ 	},
+ #endif
+ #ifdef CONFIG_ARM64_CNP
+ 	{
+ 		.desc = "Common not Private translations",
+ 		.capability = ARM64_HAS_CNP,
+ 		.type = ARM64_CPUCAP_SYSTEM_FEATURE,
+ 		.matches = has_useable_cnp,
+ 		.sys_reg = SYS_ID_AA64MMFR2_EL1,
+ 		.sign = FTR_UNSIGNED,
+ 		.field_pos = ID_AA64MMFR2_CNP_SHIFT,
+ 		.min_field_value = 1,
+ 		.cpu_enable = cpu_enable_cnp,
+ 	},
+ #endif
+ 	{
+ 		.desc = "Speculation barrier (SB)",
+ 		.capability = ARM64_HAS_SB,
+ 		.type = ARM64_CPUCAP_SYSTEM_FEATURE,
+ 		.matches = has_cpuid_feature,
+ 		.sys_reg = SYS_ID_AA64ISAR1_EL1,
+ 		.field_pos = ID_AA64ISAR1_SB_SHIFT,
+ 		.sign = FTR_UNSIGNED,
+ 		.min_field_value = 1,
+ 	},
++>>>>>>> bd4fb6d270bc (arm64: Add support for SB barrier and patch in over DSB; ISB sequences)
  	{},
  };
  
diff --cc arch/arm64/kernel/cpuinfo.c
index e9ab7b3ed317,7cb0b08ab0a7..000000000000
--- a/arch/arm64/kernel/cpuinfo.c
+++ b/arch/arm64/kernel/cpuinfo.c
@@@ -81,6 -81,8 +81,11 @@@ static const char *const hwcap_str[] = 
  	"uscat",
  	"ilrcpc",
  	"flagm",
++<<<<<<< HEAD
++=======
+ 	"ssbs",
+ 	"sb",
++>>>>>>> bd4fb6d270bc (arm64: Add support for SB barrier and patch in over DSB; ISB sequences)
  	NULL
  };
  
diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 0bcc98dbba56..07574d99145f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -122,6 +122,19 @@
 	hint	#20
 	.endm
 
+/*
+ * Speculation barrier
+ */
+	.macro	sb
+alternative_if_not ARM64_HAS_SB
+	dsb	nsh
+	isb
+alternative_else
+	SB_BARRIER_INSN
+	nop
+alternative_endif
+	.endm
+
 /*
  * Sanitise a 64-bit bounded index wrt speculation, returning zero if out
  * of bounds.
diff --git a/arch/arm64/include/asm/barrier.h b/arch/arm64/include/asm/barrier.h
index 822a9192c551..f66bb04fdf2d 100644
--- a/arch/arm64/include/asm/barrier.h
+++ b/arch/arm64/include/asm/barrier.h
@@ -34,6 +34,10 @@
 #define psb_csync()	asm volatile("hint #17" : : : "memory")
 #define csdb()		asm volatile("hint #20" : : : "memory")
 
+#define spec_bar()	asm volatile(ALTERNATIVE("dsb nsh\nisb\n",		\
+						 SB_BARRIER_INSN"nop\n",	\
+						 ARM64_HAS_SB))
+
 #define mb()		dsb(sy)
 #define rmb()		dsb(ld)
 #define wmb()		dsb(st)
* Unmerged path arch/arm64/include/asm/cpucaps.h
diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6c400b628e30..e67315411175 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -92,6 +92,11 @@
 #define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
 				      (!!x)<<8 | 0x1f)
 
+#define __SYS_BARRIER_INSN(CRm, op2, Rt) \
+	__emit_inst(0xd5000000 | sys_insn(0, 3, 3, (CRm), (op2)) | ((Rt) & 0x1f))
+
+#define SB_BARRIER_INSN			__SYS_BARRIER_INSN(0, 7, 31)
+
 #define SYS_DC_ISW			sys_insn(1, 0, 7, 6, 2)
 #define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)
 #define SYS_DC_CISW			sys_insn(1, 0, 7, 14, 2)
@@ -512,6 +517,7 @@
 #define ID_AA64ISAR0_AES_SHIFT		4
 
 /* id_aa64isar1 */
+#define ID_AA64ISAR1_SB_SHIFT		36
 #define ID_AA64ISAR1_LRCPC_SHIFT	20
 #define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12
diff --git a/arch/arm64/include/asm/uaccess.h b/arch/arm64/include/asm/uaccess.h
index e66b0fca99c2..3c3bf4171f3b 100644
--- a/arch/arm64/include/asm/uaccess.h
+++ b/arch/arm64/include/asm/uaccess.h
@@ -46,8 +46,7 @@ static inline void set_fs(mm_segment_t fs)
 	 * Prevent a mispredicted conditional call to set_fs from forwarding
 	 * the wrong address limit to access_ok under speculation.
 	 */
-	dsb(nsh);
-	isb();
+	spec_bar();
 
 	/* On user-mode return, check fs is correct */
 	set_thread_flag(TIF_FSCHECK);
* Unmerged path arch/arm64/include/uapi/asm/hwcap.h
* Unmerged path arch/arm64/kernel/cpufeature.c
* Unmerged path arch/arm64/kernel/cpuinfo.c
