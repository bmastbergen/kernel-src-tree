blk-mq: cache request hardware queue mapping

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit ea4f995ee8b8f0578b3319949f2edd5d812fdb0a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/ea4f995e.failed

We call blk_mq_map_queue() a lot, at least two times for each
request per IO, sometimes more. Since we now have an indirect
call as well in that function. cache the mapping so we don't
have to re-call blk_mq_map_queue() for the same request
multiple times.

	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ea4f995ee8b8f0578b3319949f2edd5d812fdb0a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-flush.c
#	block/blk-mq-debugfs.c
#	block/blk-mq-sched.c
#	block/blk-mq-tag.c
#	block/blk-mq.c
#	block/blk-mq.h
diff --cc block/blk-flush.c
index 248fe78c2b9b,c53197dcdd70..000000000000
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@@ -215,7 -215,7 +215,11 @@@ static void flush_end_io(struct reques
  
  	/* release the tag's ownership to the req cloned from */
  	spin_lock_irqsave(&fq->mq_flush_lock, flags);
++<<<<<<< HEAD
 +	hctx = blk_mq_map_queue(q, flush_rq->mq_ctx->cpu);
++=======
+ 	hctx = flush_rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  	if (!q->elevator) {
  		blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
  		flush_rq->tag = -1;
@@@ -301,8 -301,7 +305,12 @@@ static void blk_kick_flush(struct reque
  	if (!q->elevator) {
  		fq->orig_rq = first_rq;
  		flush_rq->tag = first_rq->tag;
++<<<<<<< HEAD
 +		hctx = blk_mq_map_queue(q, first_rq->mq_ctx->cpu);
 +		blk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);
++=======
+ 		blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  	} else {
  		flush_rq->internal_tag = first_rq->internal_tag;
  	}
@@@ -324,8 -323,6 +332,11 @@@ static void mq_flush_data_end_io(struc
  	unsigned long flags;
  	struct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);
  
++<<<<<<< HEAD
 +	hctx = blk_mq_map_queue(q, ctx->cpu);
 +
++=======
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  	if (q->elevator) {
  		WARN_ON(rq->tag < 0);
  		blk_mq_put_driver_tag_hctx(hctx, rq);
diff --cc block/blk-mq-debugfs.c
index 18bc46ce4264,cde19be36135..000000000000
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@@ -443,7 -428,7 +443,11 @@@ static void hctx_show_busy_rq(struct re
  {
  	const struct show_busy_params *params = data;
  
++<<<<<<< HEAD
 +	if (blk_mq_map_queue(rq->q, rq->mq_ctx->cpu) == params->hctx)
++=======
+ 	if (rq->mq_hctx == params->hctx)
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  		__blk_mq_debugfs_rq_show(params->m,
  					 list_entry_rq(&rq->queuelist));
  }
diff --cc block/blk-mq-sched.c
index 51ff587cddb8,641df3f00632..000000000000
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@@ -442,7 -366,7 +442,11 @@@ void blk_mq_sched_insert_request(struc
  	struct request_queue *q = rq->q;
  	struct elevator_queue *e = q->elevator;
  	struct blk_mq_ctx *ctx = rq->mq_ctx;
++<<<<<<< HEAD
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
++=======
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  
  	/* flush rq in flush machinery need to be dispatched directly */
  	if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
@@@ -475,11 -399,17 +479,21 @@@ void blk_mq_sched_insert_requests(struc
  				  struct blk_mq_ctx *ctx,
  				  struct list_head *list, bool run_queue_async)
  {
 -	struct blk_mq_hw_ctx *hctx;
 -	struct elevator_queue *e;
 -	struct request *rq;
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 +	struct elevator_queue *e = hctx->queue->elevator;
  
++<<<<<<< HEAD
 +	if (e && e->type->ops.mq.insert_requests)
 +		e->type->ops.mq.insert_requests(hctx, list, false);
++=======
+ 	/* For list inserts, requests better be on the same hw queue */
+ 	rq = list_first_entry(list, struct request, queuelist);
+ 	hctx = rq->mq_hctx;
+ 
+ 	e = hctx->queue->elevator;
+ 	if (e && e->type->ops.insert_requests)
+ 		e->type->ops.insert_requests(hctx, list, false);
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  	else {
  		/*
  		 * try to issue requests directly if the hw queue isn't
diff --cc block/blk-mq-tag.c
index d730bdcbb46e,fb836d818b80..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -460,14 -527,7 +460,18 @@@ int blk_mq_tag_update_depth(struct blk_
   */
  u32 blk_mq_unique_tag(struct request *rq)
  {
++<<<<<<< HEAD
 +	struct request_queue *q = rq->q;
 +	struct blk_mq_hw_ctx *hctx;
 +	int hwq = 0;
 +
 +	hctx = blk_mq_map_queue(q, rq->mq_ctx->cpu);
 +	hwq = hctx->queue_num;
 +
 +	return (hwq << BLK_MQ_UNIQUE_TAG_BITS) |
++=======
+ 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  		(rq->tag & BLK_MQ_UNIQUE_TAG_MASK);
  }
  EXPORT_SYMBOL(blk_mq_unique_tag);
diff --cc block/blk-mq.c
index e9efa8a78a58,6b2859d3ad23..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -470,7 -473,7 +471,11 @@@ static void __blk_mq_free_request(struc
  {
  	struct request_queue *q = rq->q;
  	struct blk_mq_ctx *ctx = rq->mq_ctx;
++<<<<<<< HEAD
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
++=======
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  	const int sched_tag = rq->internal_tag;
  
  	blk_pm_mark_last_busy(rq);
@@@ -487,11 -491,11 +493,15 @@@ void blk_mq_free_request(struct reques
  	struct request_queue *q = rq->q;
  	struct elevator_queue *e = q->elevator;
  	struct blk_mq_ctx *ctx = rq->mq_ctx;
++<<<<<<< HEAD
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
++=======
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  
  	if (rq->rq_flags & RQF_ELVPRIV) {
 -		if (e && e->type->ops.finish_request)
 -			e->type->ops.finish_request(rq);
 +		if (e && e->type->ops.mq.finish_request)
 +			e->type->ops.mq.finish_request(rq);
  		if (rq->elv.icq) {
  			put_io_context(rq->elv.icq->ioc);
  			rq->elv.icq = NULL;
@@@ -979,8 -985,9 +989,12 @@@ bool blk_mq_get_driver_tag(struct reque
  {
  	struct blk_mq_alloc_data data = {
  		.q = rq->q,
++<<<<<<< HEAD
 +		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
++=======
+ 		.hctx = rq->mq_hctx,
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  		.flags = BLK_MQ_REQ_NOWAIT,
 -		.cmd_flags = rq->cmd_flags,
  	};
  	bool shared;
  
@@@ -1144,7 -1151,7 +1158,11 @@@ bool blk_mq_dispatch_rq_list(struct req
  
  		rq = list_first_entry(list, struct request, queuelist);
  
++<<<<<<< HEAD
 +		hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
++=======
+ 		hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
  			break;
  
@@@ -1574,8 -1581,7 +1592,12 @@@ void __blk_mq_insert_request(struct blk
   */
  void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
  {
++<<<<<<< HEAD
 +	struct blk_mq_ctx *ctx = rq->mq_ctx;
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
++=======
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  
  	spin_lock(&hctx->lock);
  	list_add_tail(&rq->queuelist, &hctx->dispatch);
@@@ -1784,8 -1790,7 +1806,12 @@@ blk_status_t blk_mq_request_issue_direc
  	blk_status_t ret;
  	int srcu_idx;
  	blk_qc_t unused_cookie;
++<<<<<<< HEAD
 +	struct blk_mq_ctx *ctx = rq->mq_ctx;
 +	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
++=======
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  
  	hctx_lock(hctx, &srcu_idx);
  	ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true);
@@@ -1910,8 -1915,7 +1936,12 @@@ static blk_qc_t blk_mq_make_request(str
  		blk_mq_put_ctx(data.ctx);
  
  		if (same_queue_rq) {
++<<<<<<< HEAD
 +			data.hctx = blk_mq_map_queue(q,
 +					same_queue_rq->mq_ctx->cpu);
++=======
+ 			data.hctx = same_queue_rq->mq_hctx;
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  			blk_mq_try_issue_directly(data.hctx, same_queue_rq,
  					&cookie);
  		}
diff --cc block/blk-mq.h
index d9facfb9ca51,facb6e9ddce4..000000000000
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@@ -207,10 -226,7 +207,12 @@@ static inline void blk_mq_put_driver_ta
  	if (rq->tag == -1 || rq->internal_tag == -1)
  		return;
  
++<<<<<<< HEAD
 +	hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
 +	__blk_mq_put_driver_tag(hctx, rq);
++=======
+ 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
++>>>>>>> ea4f995ee8b8 (blk-mq: cache request hardware queue mapping)
  }
  
  static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
* Unmerged path block/blk-flush.c
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-tag.c
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-mq.h
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f866412f46bc..0d5bf600a130 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -148,6 +148,7 @@ enum mq_rq_state {
 struct request {
 	struct request_queue *q;
 	struct blk_mq_ctx *mq_ctx;
+	struct blk_mq_hw_ctx *mq_hctx;
 
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
