x86/ioremap: Add an ioremap_encrypted() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] ioremap: Add an ioremap_encrypted() helper (Lianbo Jiang) [1718348]
Rebuild_FUZZ: 95.45%
commit-author Lianbo Jiang <lijiang@redhat.com>
commit c3a7a61c192ec350330128edb13db33a9bc0ace1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/c3a7a61c.failed

When SME is enabled, the memory is encrypted in the first kernel. In
this case, SME also needs to be enabled in the kdump kernel, and we have
to remap the old memory with the memory encryption mask.

The case of concern here is if SME is active in the first kernel,
and it is active too in the kdump kernel. There are four cases to be
considered:

a. dump vmcore
   It is encrypted in the first kernel, and needs be read out in the
   kdump kernel.

b. crash notes
   When dumping vmcore, the people usually need to read useful
   information from notes, and the notes is also encrypted.

c. iommu device table
   It's encrypted in the first kernel, kdump kernel needs to access its
   content to analyze and get information it needs.

d. mmio of AMD iommu
   not encrypted in both kernels

Add a new bool parameter @encrypted to __ioremap_caller(). If set,
memory will be remapped with the SME mask.

Add a new function ioremap_encrypted() to explicitly pass in a true
value for @encrypted. Use ioremap_encrypted() for the above a, b, c
cases.

 [ bp: cleanup commit message, extern defs in io.h and drop forgotten
   include. ]

	Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: kexec@lists.infradead.org
	Cc: tglx@linutronix.de
	Cc: mingo@redhat.com
	Cc: hpa@zytor.com
	Cc: akpm@linux-foundation.org
	Cc: dan.j.williams@intel.com
	Cc: bhelgaas@google.com
	Cc: baiyaowei@cmss.chinamobile.com
	Cc: tiwai@suse.de
	Cc: brijesh.singh@amd.com
	Cc: dyoung@redhat.com
	Cc: bhe@redhat.com
	Cc: jroedel@suse.de
Link: https://lkml.kernel.org/r/20180927071954.29615-2-lijiang@redhat.com
(cherry picked from commit c3a7a61c192ec350330128edb13db33a9bc0ace1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/io.h
#	arch/x86/mm/ioremap.c
diff --cc arch/x86/include/asm/io.h
index 772a9eaedf5b,6df53efcecfd..000000000000
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@@ -161,6 -179,21 +161,24 @@@ static inline unsigned int isa_virt_to_
  #define virt_to_bus virt_to_phys
  #define bus_to_virt phys_to_virt
  
++<<<<<<< HEAD
++=======
+ /*
+  * The default ioremap() behavior is non-cached; if you need something
+  * else, you probably want one of the following.
+  */
+ extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+ #define ioremap_nocache ioremap_nocache
+ extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
+ #define ioremap_uc ioremap_uc
+ extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
+ #define ioremap_cache ioremap_cache
+ extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size, unsigned long prot_val);
+ #define ioremap_prot ioremap_prot
+ extern void __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long size);
+ #define ioremap_encrypted ioremap_encrypted
+ 
++>>>>>>> c3a7a61c192e (x86/ioremap: Add an ioremap_encrypted() helper)
  /**
   * ioremap     -   map bus memory into CPU space
   * @offset:    bus address of the memory
diff --cc arch/x86/mm/ioremap.c
index 94e769457935,24e0920a9b25..000000000000
--- a/arch/x86/mm/ioremap.c
+++ b/arch/x86/mm/ioremap.c
@@@ -203,7 -200,7 +204,11 @@@ static void __iomem *__ioremap_caller(r
  	 * resulting mapping.
  	 */
  	prot = PAGE_KERNEL_IO;
++<<<<<<< HEAD
 +	if (sev_active() && mem_flags.name_other)
++=======
+ 	if ((sev_active() && mem_flags.desc_other) || encrypted)
++>>>>>>> c3a7a61c192e (x86/ioremap: Add an ioremap_encrypted() helper)
  		prot = pgprot_encrypted(prot);
  
  	switch (pcm) {
@@@ -295,6 -297,39 +300,42 @@@ void __iomem *ioremap_nocache(resource_
  EXPORT_SYMBOL(ioremap_nocache);
  
  /**
++<<<<<<< HEAD
++=======
+  * ioremap_uc     -   map bus memory into CPU space as strongly uncachable
+  * @phys_addr:    bus address of the memory
+  * @size:      size of the resource to map
+  *
+  * ioremap_uc performs a platform specific sequence of operations to
+  * make bus memory CPU accessible via the readb/readw/readl/writeb/
+  * writew/writel functions and the other mmio helpers. The returned
+  * address is not guaranteed to be usable directly as a virtual
+  * address.
+  *
+  * This version of ioremap ensures that the memory is marked with a strong
+  * preference as completely uncachable on the CPU when possible. For non-PAT
+  * systems this ends up setting page-attribute flags PCD=1, PWT=1. For PAT
+  * systems this will set the PAT entry for the pages as strong UC.  This call
+  * will honor existing caching rules from things like the PCI bus. Note that
+  * there are other caches and buffers on many busses. In particular driver
+  * authors should read up on PCI writes.
+  *
+  * It's useful if some control registers are in such an area and
+  * write combining or read caching is not desirable:
+  *
+  * Must be freed with iounmap.
+  */
+ void __iomem *ioremap_uc(resource_size_t phys_addr, unsigned long size)
+ {
+ 	enum page_cache_mode pcm = _PAGE_CACHE_MODE_UC;
+ 
+ 	return __ioremap_caller(phys_addr, size, pcm,
+ 				__builtin_return_address(0), false);
+ }
+ EXPORT_SYMBOL_GPL(ioremap_uc);
+ 
+ /**
++>>>>>>> c3a7a61c192e (x86/ioremap: Add an ioremap_encrypted() helper)
   * ioremap_wc	-	map memory into CPU space write combined
   * @phys_addr:	bus address of the memory
   * @size:	size of the resource to map
@@@ -306,14 -341,35 +347,46 @@@
   */
  void __iomem *ioremap_wc(resource_size_t phys_addr, unsigned long size)
  {
++<<<<<<< HEAD
 +	if (pat_enabled)
 +		return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WC,
 +					__builtin_return_address(0));
 +	else
 +		return ioremap_nocache(phys_addr, size);
 +}
 +EXPORT_SYMBOL(ioremap_wc);
 +
++=======
+ 	return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WC,
+ 					__builtin_return_address(0), false);
+ }
+ EXPORT_SYMBOL(ioremap_wc);
+ 
+ /**
+  * ioremap_wt	-	map memory into CPU space write through
+  * @phys_addr:	bus address of the memory
+  * @size:	size of the resource to map
+  *
+  * This version of ioremap ensures that the memory is marked write through.
+  * Write through stores data into memory while keeping the cache up-to-date.
+  *
+  * Must be freed with iounmap.
+  */
+ void __iomem *ioremap_wt(resource_size_t phys_addr, unsigned long size)
+ {
+ 	return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WT,
+ 					__builtin_return_address(0), false);
+ }
+ EXPORT_SYMBOL(ioremap_wt);
+ 
+ void __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long size)
+ {
+ 	return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
+ 				__builtin_return_address(0), true);
+ }
+ EXPORT_SYMBOL(ioremap_encrypted);
+ 
++>>>>>>> c3a7a61c192e (x86/ioremap: Add an ioremap_encrypted() helper)
  void __iomem *ioremap_cache(resource_size_t phys_addr, unsigned long size)
  {
  	return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
* Unmerged path arch/x86/include/asm/io.h
* Unmerged path arch/x86/mm/ioremap.c
