x86/kvm/vmx: Add MDS protection when L1D Flush is not active

jira LE-1907
cve CVE-2019-11091
cve CVE-2018-12130
cve CVE-2018-12127
cve CVE-2018-12126
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm/vmx: Add MDS protection when L1D Flush is not active (Waiman Long) [1709296 1690358 1690348 1690335] {CVE-2018-12126 CVE-2018-12127 CVE-2018-12130 CVE-2019-11091}
Rebuild_FUZZ: 96.55%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 650b68a0622f933444a6d66936abb3103029413b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/650b68a0.failed

CPUs which are affected by L1TF and MDS mitigate MDS with the L1D Flush on
VMENTER when updated microcode is installed.

If a CPU is not affected by L1TF or if the L1D Flush is not in use, then
MDS mitigation needs to be invoked explicitly.

For these cases, follow the host mitigation state and invoke the MDS
mitigation before VMENTER.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Jon Masters <jcm@redhat.com>
	Tested-by: Jon Masters <jcm@redhat.com>
(cherry picked from commit 650b68a0622f933444a6d66936abb3103029413b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kernel/cpu/bugs.c
index 6ec1d2da76d1,29ed8e8dfee2..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -22,15 -27,45 +22,47 @@@
  #include <asm/paravirt.h>
  #include <asm/alternative.h>
  #include <asm/pgtable.h>
 -#include <asm/set_memory.h>
 -#include <asm/intel-family.h>
 -#include <asm/e820/api.h>
 -#include <asm/hypervisor.h>
 -
 -#include "cpu.h"
 +#include <asm/cacheflush.h>
 +#include <asm/spec_ctrl.h>
 +#include <linux/prctl.h>
  
  static void __init spectre_v2_select_mitigation(void);
 -static void __init ssb_select_mitigation(void);
 +static void __init ssb_parse_cmdline(void);
 +void ssb_select_mitigation(void);
  static void __init l1tf_select_mitigation(void);
++<<<<<<< HEAD
 +extern void spec_ctrl_save_msr(void);
++=======
+ 
+ /* The base value of the SPEC_CTRL MSR that always has to be preserved. */
+ u64 x86_spec_ctrl_base;
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
+ static DEFINE_MUTEX(spec_ctrl_mutex);
+ 
+ /*
+  * The vendor and possibly platform specific bits which can be modified in
+  * x86_spec_ctrl_base.
+  */
+ static u64 __ro_after_init x86_spec_ctrl_mask = SPEC_CTRL_IBRS;
+ 
+ /*
+  * AMD specific MSR info for Speculative Store Bypass control.
+  * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().
+  */
+ u64 __ro_after_init x86_amd_ls_cfg_base;
+ u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
+ 
+ /* Control conditional STIBP in switch_to() */
+ DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+ /* Control conditional IBPB in switch_mm() */
+ DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+ /* Control unconditional IBPB in switch_mm() */
+ DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+ 
+ /* Control MDS CPU buffer clear before returning to user space */
+ DEFINE_STATIC_KEY_FALSE(mds_user_clear);
+ EXPORT_SYMBOL_GPL(mds_user_clear);
++>>>>>>> 650b68a0622f (x86/kvm/vmx: Add MDS protection when L1D Flush is not active)
  
  void __init check_bugs(void)
  {
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
