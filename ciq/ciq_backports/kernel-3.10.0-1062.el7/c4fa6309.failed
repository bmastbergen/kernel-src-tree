mm, swap: fix swap readahead marking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] swap: fix swap readahead marking (Rafael Aquini) [1485248]
Rebuild_FUZZ: 94.12%
commit-author Huang Ying <ying.huang@intel.com>
commit c4fa63092f216737b60c789968371d9960a598e5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/c4fa6309.failed

In the original implementation, it is possible that the existing pages
in the swap cache (not newly readahead) could be marked as the readahead
pages.  This will cause the statistics of swap readahead be wrong and
influence the swap readahead algorithm too.

This is fixed via marking a page as the readahead page only if it is
newly allocated and read from the disk.

When testing with linpack, after the fixing the swap readahead hit rate
increased from ~66% to ~86%.

Link: http://lkml.kernel.org/r/20170807054038.1843-3-ying.huang@intel.com
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Fengguang Wu <fengguang.wu@intel.com>
	Cc: Tim Chen <tim.c.chen@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c4fa63092f216737b60c789968371d9960a598e5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap_state.c
diff --cc mm/swap_state.c
index bbce650716cb,a901afe9da61..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -461,12 -493,18 +461,16 @@@ struct page *swapin_readahead(swp_entry
  			struct vm_area_struct *vma, unsigned long addr)
  {
  	struct page *page;
 -	unsigned long entry_offset = swp_offset(entry);
 -	unsigned long offset = entry_offset;
 +	unsigned long offset = swp_offset(entry);
  	unsigned long start_offset, end_offset;
 -	unsigned long mask;
 +	unsigned long mask = (1UL << page_cluster) - 1;
 +	struct swap_info_struct *si = swp_swap_info(entry);
  	struct blk_plug plug;
++<<<<<<< HEAD
++=======
+ 	bool do_poll = true, page_allocated;
++>>>>>>> c4fa63092f21 (mm, swap: fix swap readahead marking)
  
 -	mask = swapin_nr_pages(offset) - 1;
 -	if (!mask)
 -		goto skip;
 -
 -	do_poll = false;
  	/* Read a page_cluster sized and aligned cluster around offset. */
  	start_offset = offset & ~mask;
  	end_offset = offset | mask;
@@@ -478,11 -514,20 +482,28 @@@
  	blk_start_plug(&plug);
  	for (offset = start_offset; offset <= end_offset ; offset++) {
  		/* Ok, do the async read-ahead now */
++<<<<<<< HEAD
 +		page = read_swap_cache_async(swp_entry(swp_type(entry), offset),
 +						gfp_mask, vma, addr);
 +		if (!page)
 +			continue;
 +		page_cache_release(page);
++=======
+ 		page = __read_swap_cache_async(
+ 			swp_entry(swp_type(entry), offset),
+ 			gfp_mask, vma, addr, &page_allocated);
+ 		if (!page)
+ 			continue;
+ 		if (page_allocated) {
+ 			swap_readpage(page, false);
+ 			if (offset != entry_offset &&
+ 			    likely(!PageTransCompound(page))) {
+ 				SetPageReadahead(page);
+ 				count_vm_event(SWAP_RA);
+ 			}
+ 		}
+ 		put_page(page);
++>>>>>>> c4fa63092f21 (mm, swap: fix swap readahead marking)
  	}
  	blk_finish_plug(&plug);
  
* Unmerged path mm/swap_state.c
