perf stat: Use perf_evsel__is_clocki() for clock events

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Ravi Bangoria <ravi.bangoria@linux.ibm.com>
commit eb08d006054e7e374592068919e32579988602d4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/eb08d006.failed

We already have function to check if a given event is either
SW_CPU_CLOCK or SW_TASK_CLOCK. Utilize it.

	Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
	Acked-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Anton Blanchard <anton@samba.org>
	Cc: Jin Yao <yao.jin@linux.intel.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Thomas Richter <tmricht@linux.vnet.ibm.com>
	Cc: yuzhoujian@didichuxing.com
Link: http://lkml.kernel.org/r/20181115095533.16930-1-ravi.bangoria@linux.ibm.com
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit eb08d006054e7e374592068919e32579988602d4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/util/stat-shadow.c
diff --cc tools/perf/util/stat-shadow.c
index b29877d9b2f4,f0a8cec55c47..000000000000
--- a/tools/perf/util/stat-shadow.c
+++ b/tools/perf/util/stat-shadow.c
@@@ -228,52 -212,59 +228,57 @@@ void perf_stat__update_shadow_stats(str
  
  	count *= counter->scale;
  
++<<<<<<< HEAD
 +	if (perf_evsel__match(counter, SOFTWARE, SW_TASK_CLOCK) ||
 +	    perf_evsel__match(counter, SOFTWARE, SW_CPU_CLOCK))
 +		update_stats(&runtime_nsecs_stats[cpu], count);
++=======
+ 	if (perf_evsel__is_clock(counter))
+ 		update_runtime_stat(st, STAT_NSECS, 0, cpu, count);
++>>>>>>> eb08d006054e (perf stat: Use perf_evsel__is_clocki() for clock events)
  	else if (perf_evsel__match(counter, HARDWARE, HW_CPU_CYCLES))
 -		update_runtime_stat(st, STAT_CYCLES, ctx, cpu, count);
 +		update_stats(&runtime_cycles_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, CYCLES_IN_TX))
 -		update_runtime_stat(st, STAT_CYCLES_IN_TX, ctx, cpu, count);
 +		update_stats(&runtime_cycles_in_tx_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TRANSACTION_START))
 -		update_runtime_stat(st, STAT_TRANSACTION, ctx, cpu, count);
 +		update_stats(&runtime_transaction_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, ELISION_START))
 -		update_runtime_stat(st, STAT_ELISION, ctx, cpu, count);
 +		update_stats(&runtime_elision_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_TOTAL_SLOTS))
 -		update_runtime_stat(st, STAT_TOPDOWN_TOTAL_SLOTS,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_total_slots[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_SLOTS_ISSUED))
 -		update_runtime_stat(st, STAT_TOPDOWN_SLOTS_ISSUED,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_slots_issued[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_SLOTS_RETIRED))
 -		update_runtime_stat(st, STAT_TOPDOWN_SLOTS_RETIRED,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_slots_retired[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_FETCH_BUBBLES))
 -		update_runtime_stat(st, STAT_TOPDOWN_FETCH_BUBBLES,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_fetch_bubbles[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_RECOVERY_BUBBLES))
 -		update_runtime_stat(st, STAT_TOPDOWN_RECOVERY_BUBBLES,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_recovery_bubbles[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_STALLED_CYCLES_FRONTEND))
 -		update_runtime_stat(st, STAT_STALLED_CYCLES_FRONT,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_stalled_cycles_front_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_STALLED_CYCLES_BACKEND))
 -		update_runtime_stat(st, STAT_STALLED_CYCLES_BACK,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_stalled_cycles_back_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_BRANCH_INSTRUCTIONS))
 -		update_runtime_stat(st, STAT_BRANCHES, ctx, cpu, count);
 +		update_stats(&runtime_branches_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_CACHE_REFERENCES))
 -		update_runtime_stat(st, STAT_CACHEREFS, ctx, cpu, count);
 +		update_stats(&runtime_cacherefs_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_L1D))
 -		update_runtime_stat(st, STAT_L1_DCACHE, ctx, cpu, count);
 +		update_stats(&runtime_l1_dcache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_L1I))
 -		update_runtime_stat(st, STAT_L1_ICACHE, ctx, cpu, count);
 +		update_stats(&runtime_ll_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_LL))
 -		update_runtime_stat(st, STAT_LL_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_ll_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_DTLB))
 -		update_runtime_stat(st, STAT_DTLB_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_dtlb_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_ITLB))
 -		update_runtime_stat(st, STAT_ITLB_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_itlb_cache_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, SMI_NUM))
 -		update_runtime_stat(st, STAT_SMI_NUM, ctx, cpu, count);
 +		update_stats(&runtime_smi_num_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, APERF))
 -		update_runtime_stat(st, STAT_APERF, ctx, cpu, count);
 +		update_stats(&runtime_aperf_stats[ctx][cpu], count);
  
  	if (counter->collect_stat) {
 -		struct saved_value *v = saved_value_lookup(counter, cpu, true,
 -							   STAT_NONE, 0, st);
 +		struct saved_value *v = saved_value_lookup(counter, cpu, true);
  		update_stats(&v->stats, count);
  	}
  }
* Unmerged path tools/perf/util/stat-shadow.c
