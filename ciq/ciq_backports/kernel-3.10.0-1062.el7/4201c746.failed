svcrdma: Introduce svc_rdma_send_ctxt

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 4201c7464753827803366b40e82eb050c04ebdef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/4201c746.failed

svc_rdma_op_ctxt's are pre-allocated and maintained on a per-xprt
free list. This eliminates the overhead of calling kmalloc / kfree,
both of which grab a globally shared lock that disables interrupts.
Introduce a replacement to svc_rdma_op_ctxt's that is built
especially for the svcrdma Send path.

Subsequent patches will take advantage of this new structure by
allocating real resources which are then cached in these objects.
The allocations are freed when the transport is torn down.

I've renamed the structure so that static type checking can be used
to ensure that uses of op_ctxt and send_ctxt are not confused. As an
additional clean up, structure fields are renamed to conform with
kernel coding conventions.

Additional clean ups:
- Handle svc_rdma_send_ctxt_get allocation failure at each call
  site, rather than pre-allocating and hoping we guessed correctly
- All send_ctxt_put call-sites request page freeing, so remove
  the @free_pages argument
- All send_ctxt_put call-sites unmap SGEs, so fold that into
  svc_rdma_send_ctxt_put

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit 4201c7464753827803366b40e82eb050c04ebdef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/svc_rdma_sendto.c
#	net/sunrpc/xprtrdma/svc_rdma_transport.c
diff --cc net/sunrpc/xprtrdma/svc_rdma_sendto.c
index 00a497c70989,b286d6a6e429..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_sendto.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
@@@ -302,41 -480,11 +480,41 @@@ static u32 svc_rdma_get_inv_rkey(__be3
  	return be32_to_cpup(p);
  }
  
 +/* ib_dma_map_page() is used here because svc_rdma_dma_unmap()
 + * is used during completion to DMA-unmap this memory, and
 + * it uses ib_dma_unmap_page() exclusively.
 + */
 +static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,
 +				struct svc_rdma_op_ctxt *ctxt,
 +				unsigned int sge_no,
 +				unsigned char *base,
 +				unsigned int len)
 +{
 +	unsigned long offset = (unsigned long)base & ~PAGE_MASK;
 +	struct ib_device *dev = rdma->sc_cm_id->device;
 +	dma_addr_t dma_addr;
 +
 +	dma_addr = ib_dma_map_page(dev, virt_to_page(base),
 +				   offset, len, DMA_TO_DEVICE);
 +	if (ib_dma_mapping_error(dev, dma_addr))
 +		goto out_maperr;
 +
 +	ctxt->sge[sge_no].addr = dma_addr;
 +	ctxt->sge[sge_no].length = len;
 +	ctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;
 +	svc_rdma_count_mappings(rdma, ctxt);
 +	return 0;
 +
 +out_maperr:
 +	pr_err("svcrdma: failed to map buffer\n");
 +	return -EIO;
 +}
 +
  static int svc_rdma_dma_map_page(struct svcxprt_rdma *rdma,
- 				 struct svc_rdma_op_ctxt *ctxt,
+ 				 struct svc_rdma_send_ctxt *ctxt,
  				 unsigned int sge_no,
  				 struct page *page,
 -				 unsigned long offset,
 +				 unsigned int offset,
  				 unsigned int len)
  {
  	struct ib_device *dev = rdma->sc_cm_id->device;
@@@ -346,10 -494,9 +524,16 @@@
  	if (ib_dma_mapping_error(dev, dma_addr))
  		goto out_maperr;
  
++<<<<<<< HEAD
 +	ctxt->sge[sge_no].addr = dma_addr;
 +	ctxt->sge[sge_no].length = len;
 +	ctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;
 +	svc_rdma_count_mappings(rdma, ctxt);
++=======
+ 	ctxt->sc_sges[sge_no].addr = dma_addr;
+ 	ctxt->sc_sges[sge_no].length = len;
+ 	ctxt->sc_send_wr.num_sge++;
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  	return 0;
  
  out_maperr:
@@@ -357,6 -504,19 +541,22 @@@
  	return -EIO;
  }
  
++<<<<<<< HEAD
++=======
+ /* ib_dma_map_page() is used here because svc_rdma_dma_unmap()
+  * handles DMA-unmap and it uses ib_dma_unmap_page() exclusively.
+  */
+ static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,
+ 				struct svc_rdma_send_ctxt *ctxt,
+ 				unsigned int sge_no,
+ 				unsigned char *base,
+ 				unsigned int len)
+ {
+ 	return svc_rdma_dma_map_page(rdma, ctxt, sge_no, virt_to_page(base),
+ 				     offset_in_page(base), len);
+ }
+ 
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  /**
   * svc_rdma_map_reply_hdr - DMA map the transport header buffer
   * @rdma: controlling transport
@@@ -385,10 -544,11 +584,10 @@@ int svc_rdma_map_reply_hdr(struct svcxp
   * Returns zero on success, or a negative errno on failure.
   */
  static int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
- 				  struct svc_rdma_op_ctxt *ctxt,
+ 				  struct svc_rdma_send_ctxt *ctxt,
  				  struct xdr_buf *xdr, __be32 *wr_lst)
  {
 -	unsigned int len, sge_no, remaining;
 -	unsigned long page_off;
 +	unsigned int len, sge_no, remaining, page_off;
  	struct page **ppages;
  	unsigned char *base;
  	u32 xdr_pad;
diff --cc net/sunrpc/xprtrdma/svc_rdma_transport.c
index c28ee7409658,3de81735a6cc..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_transport.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_transport.c
@@@ -157,118 -158,6 +158,121 @@@ static void svc_rdma_bc_free(struct svc
  }
  #endif	/* CONFIG_SUNRPC_BACKCHANNEL */
  
++<<<<<<< HEAD
 +static struct svc_rdma_op_ctxt *alloc_ctxt(struct svcxprt_rdma *xprt,
 +					   gfp_t flags)
 +{
 +	struct svc_rdma_op_ctxt *ctxt;
 +
 +	ctxt = kmalloc(sizeof(*ctxt), flags);
 +	if (ctxt) {
 +		ctxt->xprt = xprt;
 +		INIT_LIST_HEAD(&ctxt->list);
 +	}
 +	return ctxt;
 +}
 +
 +static bool svc_rdma_prealloc_ctxts(struct svcxprt_rdma *xprt)
 +{
 +	unsigned int i;
 +
 +	/* Each RPC/RDMA credit can consume one Receive and
 +	 * one Send WQE at the same time.
 +	 */
 +	i = xprt->sc_sq_depth + xprt->sc_rq_depth;
 +
 +	while (i--) {
 +		struct svc_rdma_op_ctxt *ctxt;
 +
 +		ctxt = alloc_ctxt(xprt, GFP_KERNEL);
 +		if (!ctxt) {
 +			dprintk("svcrdma: No memory for RDMA ctxt\n");
 +			return false;
 +		}
 +		list_add(&ctxt->list, &xprt->sc_ctxts);
 +	}
 +	return true;
 +}
 +
 +struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *xprt)
 +{
 +	struct svc_rdma_op_ctxt *ctxt = NULL;
 +
 +	spin_lock(&xprt->sc_ctxt_lock);
 +	xprt->sc_ctxt_used++;
 +	if (list_empty(&xprt->sc_ctxts))
 +		goto out_empty;
 +
 +	ctxt = list_first_entry(&xprt->sc_ctxts,
 +				struct svc_rdma_op_ctxt, list);
 +	list_del(&ctxt->list);
 +	spin_unlock(&xprt->sc_ctxt_lock);
 +
 +out:
 +	ctxt->count = 0;
 +	ctxt->mapped_sges = 0;
 +	return ctxt;
 +
 +out_empty:
 +	/* Either pre-allocation missed the mark, or send
 +	 * queue accounting is broken.
 +	 */
 +	spin_unlock(&xprt->sc_ctxt_lock);
 +
 +	ctxt = alloc_ctxt(xprt, GFP_NOIO);
 +	if (ctxt)
 +		goto out;
 +
 +	spin_lock(&xprt->sc_ctxt_lock);
 +	xprt->sc_ctxt_used--;
 +	spin_unlock(&xprt->sc_ctxt_lock);
 +	WARN_ONCE(1, "svcrdma: empty RDMA ctxt list?\n");
 +	return NULL;
 +}
 +
 +void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt)
 +{
 +	struct svcxprt_rdma *xprt = ctxt->xprt;
 +	struct ib_device *device = xprt->sc_cm_id->device;
 +	unsigned int i;
 +
 +	for (i = 0; i < ctxt->mapped_sges; i++)
 +		ib_dma_unmap_page(device,
 +				  ctxt->sge[i].addr,
 +				  ctxt->sge[i].length,
 +				  ctxt->direction);
 +	ctxt->mapped_sges = 0;
 +}
 +
 +void svc_rdma_put_context(struct svc_rdma_op_ctxt *ctxt, int free_pages)
 +{
 +	struct svcxprt_rdma *xprt = ctxt->xprt;
 +	int i;
 +
 +	if (free_pages)
 +		for (i = 0; i < ctxt->count; i++)
 +			put_page(ctxt->pages[i]);
 +
 +	spin_lock(&xprt->sc_ctxt_lock);
 +	xprt->sc_ctxt_used--;
 +	list_add(&ctxt->list, &xprt->sc_ctxts);
 +	spin_unlock(&xprt->sc_ctxt_lock);
 +}
 +
 +static void svc_rdma_destroy_ctxts(struct svcxprt_rdma *xprt)
 +{
 +	while (!list_empty(&xprt->sc_ctxts)) {
 +		struct svc_rdma_op_ctxt *ctxt;
 +
 +		ctxt = list_first_entry(&xprt->sc_ctxts,
 +					struct svc_rdma_op_ctxt, list);
 +		list_del(&ctxt->list);
 +		kfree(ctxt);
 +	}
 +}
 +
++=======
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  /* QP event handler */
  static void qp_event_handler(struct ib_event *event, void *context)
  {
@@@ -296,89 -185,8 +300,94 @@@
  	}
  }
  
++<<<<<<< HEAD
 +/**
 + * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC
 + * @cq:        completion queue
 + * @wc:        completed WR
 + *
 + */
 +static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)
 +{
 +	struct svcxprt_rdma *xprt = cq->cq_context;
 +	struct ib_cqe *cqe = wc->wr_cqe;
 +	struct svc_rdma_op_ctxt *ctxt;
 +
 +	trace_svcrdma_wc_receive(wc);
 +
 +	/* WARNING: Only wc->wr_cqe and wc->status are reliable */
 +	ctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);
 +	svc_rdma_unmap_dma(ctxt);
 +
 +	if (wc->status != IB_WC_SUCCESS)
 +		goto flushed;
 +
 +	/* All wc fields are now known to be valid */
 +	ctxt->byte_len = wc->byte_len;
 +	spin_lock(&xprt->sc_rq_dto_lock);
 +	list_add_tail(&ctxt->list, &xprt->sc_rq_dto_q);
 +	spin_unlock(&xprt->sc_rq_dto_lock);
 +
 +	svc_rdma_post_recv(xprt);
 +
 +	set_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);
 +	if (test_bit(RDMAXPRT_CONN_PENDING, &xprt->sc_flags))
 +		goto out;
 +	goto out_enqueue;
 +
 +flushed:
 +	if (wc->status != IB_WC_WR_FLUSH_ERR)
 +		pr_err("svcrdma: Recv: %s (%u/0x%x)\n",
 +		       ib_wc_status_msg(wc->status),
 +		       wc->status, wc->vendor_err);
 +	set_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);
 +	svc_rdma_put_context(ctxt, 1);
 +
 +out_enqueue:
 +	svc_xprt_enqueue(&xprt->sc_xprt);
 +out:
 +	svc_xprt_put(&xprt->sc_xprt);
 +}
 +
 +/**
 + * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC
 + * @cq:        completion queue
 + * @wc:        completed WR
 + *
 + */
 +void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)
 +{
 +	struct svcxprt_rdma *xprt = cq->cq_context;
 +	struct ib_cqe *cqe = wc->wr_cqe;
 +	struct svc_rdma_op_ctxt *ctxt;
 +
 +	trace_svcrdma_wc_send(wc);
 +
 +	atomic_inc(&xprt->sc_sq_avail);
 +	wake_up(&xprt->sc_send_wait);
 +
 +	ctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);
 +	svc_rdma_unmap_dma(ctxt);
 +	svc_rdma_put_context(ctxt, 1);
 +
 +	if (unlikely(wc->status != IB_WC_SUCCESS)) {
 +		set_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);
 +		svc_xprt_enqueue(&xprt->sc_xprt);
 +		if (wc->status != IB_WC_WR_FLUSH_ERR)
 +			pr_err("svcrdma: Send: %s (%u/0x%x)\n",
 +			       ib_wc_status_msg(wc->status),
 +			       wc->status, wc->vendor_err);
 +	}
 +
 +	svc_xprt_put(&xprt->sc_xprt);
 +}
 +
 +static struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,
 +					     int listener)
++=======
+ static struct svcxprt_rdma *svc_rdma_create_xprt(struct svc_serv *serv,
+ 						 struct net *net)
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  {
  	struct svcxprt_rdma *cma_xprt = kzalloc(sizeof *cma_xprt, GFP_KERNEL);
  
@@@ -388,13 -198,15 +397,23 @@@
  	INIT_LIST_HEAD(&cma_xprt->sc_accept_q);
  	INIT_LIST_HEAD(&cma_xprt->sc_rq_dto_q);
  	INIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);
++<<<<<<< HEAD
 +	INIT_LIST_HEAD(&cma_xprt->sc_ctxts);
++=======
+ 	INIT_LIST_HEAD(&cma_xprt->sc_send_ctxts);
+ 	INIT_LIST_HEAD(&cma_xprt->sc_recv_ctxts);
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  	INIT_LIST_HEAD(&cma_xprt->sc_rw_ctxts);
  	init_waitqueue_head(&cma_xprt->sc_send_wait);
  
  	spin_lock_init(&cma_xprt->sc_lock);
  	spin_lock_init(&cma_xprt->sc_rq_dto_lock);
++<<<<<<< HEAD
 +	spin_lock_init(&cma_xprt->sc_ctxt_lock);
++=======
+ 	spin_lock_init(&cma_xprt->sc_send_lock);
+ 	spin_lock_init(&cma_xprt->sc_recv_lock);
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  	spin_lock_init(&cma_xprt->sc_rw_ctxt_lock);
  
  	/*
@@@ -907,26 -649,8 +923,21 @@@ static void __svc_rdma_free(struct work
  		pr_err("svcrdma: sc_xprt still in use? (%d)\n",
  		       kref_read(&xprt->xpt_ref));
  
 -	svc_rdma_flush_recv_queues(rdma);
 +	while (!list_empty(&rdma->sc_read_complete_q)) {
 +		struct svc_rdma_op_ctxt *ctxt;
 +		ctxt = list_first_entry(&rdma->sc_read_complete_q,
 +					struct svc_rdma_op_ctxt, list);
 +		list_del(&ctxt->list);
 +		svc_rdma_put_context(ctxt, 1);
 +	}
 +	while (!list_empty(&rdma->sc_rq_dto_q)) {
 +		struct svc_rdma_op_ctxt *ctxt;
 +		ctxt = list_first_entry(&rdma->sc_rq_dto_q,
 +					struct svc_rdma_op_ctxt, list);
 +		list_del(&ctxt->list);
 +		svc_rdma_put_context(ctxt, 1);
 +	}
  
- 	/* Warn if we leaked a resource or under-referenced */
- 	if (rdma->sc_ctxt_used != 0)
- 		pr_err("svcrdma: ctxt still in use? (%d)\n",
- 		       rdma->sc_ctxt_used);
- 
  	/* Final put of backchannel client transport */
  	if (xprt->xpt_bc_xprt) {
  		xprt_put(xprt->xpt_bc_xprt);
@@@ -934,7 -658,8 +945,12 @@@
  	}
  
  	svc_rdma_destroy_rw_ctxts(rdma);
++<<<<<<< HEAD
 +	svc_rdma_destroy_ctxts(rdma);
++=======
+ 	svc_rdma_send_ctxts_destroy(rdma);
+ 	svc_rdma_recv_ctxts_destroy(rdma);
++>>>>>>> 4201c7464753 (svcrdma: Introduce svc_rdma_send_ctxt)
  
  	/* Destroy the QP if present (not a listener) */
  	if (rdma->sc_qp && !IS_ERR(rdma->sc_qp))
diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index e1e3c42bedad..fbacaf6f43fc 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -110,8 +110,8 @@ struct svcxprt_rdma {
 
 	struct ib_pd         *sc_pd;
 
-	spinlock_t	     sc_ctxt_lock;
-	struct list_head     sc_ctxts;
+	spinlock_t	     sc_send_lock;
+	struct list_head     sc_send_ctxts;
 	int		     sc_ctxt_used;
 	spinlock_t	     sc_rw_ctxt_lock;
 	struct list_head     sc_rw_ctxts;
@@ -149,6 +149,19 @@ static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 	ctxt->mapped_sges++;
 }
 
+enum {
+	RPCRDMA_MAX_SGES	= 1 + (RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE),
+};
+
+struct svc_rdma_send_ctxt {
+	struct list_head	sc_list;
+	struct ib_send_wr	sc_send_wr;
+	struct ib_cqe		sc_cqe;
+	int			sc_page_count;
+	struct page		*sc_pages[RPCSVC_MAXPAGES];
+	struct ib_sge		sc_sges[RPCRDMA_MAX_SGES];
+};
+
 /* svc_rdma_backchannel.c */
 extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    __be32 *rdma_resp,
@@ -169,24 +182,22 @@ extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
 				     struct xdr_buf *xdr);
 
 /* svc_rdma_sendto.c */
+extern void svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma);
+extern struct svc_rdma_send_ctxt *
+		svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma);
+extern void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
+				   struct svc_rdma_send_ctxt *ctxt);
+extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
 extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
-				  struct svc_rdma_op_ctxt *ctxt,
+				  struct svc_rdma_send_ctxt *ctxt,
 				  __be32 *rdma_resp, unsigned int len);
 extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
-				 struct svc_rdma_op_ctxt *ctxt,
+				 struct svc_rdma_send_ctxt *ctxt,
 				 u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */
-extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);
-extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
-extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
-extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
-extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
 extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);
diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 855b0740ad11..33319cc6e559 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2015 Oracle.  All rights reserved.
+ * Copyright (c) 2015-2018 Oracle.  All rights reserved.
  *
  * Support for backward direction RPCs on RPC/RDMA (server-side).
  */
@@ -116,10 +116,14 @@ out_notfound:
 static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 			      struct rpc_rqst *rqst)
 {
-	struct svc_rdma_op_ctxt *ctxt;
+	struct svc_rdma_send_ctxt *ctxt;
 	int ret;
 
-	ctxt = svc_rdma_get_context(rdma);
+	ctxt = svc_rdma_send_ctxt_get(rdma);
+	if (!ctxt) {
+		ret = -ENOMEM;
+		goto out_err;
+	}
 
 	/* rpcrdma_bc_send_request builds the transport header and
 	 * the backchannel RPC message in the same buffer. Thus only
@@ -143,8 +147,7 @@ out_err:
 	return ret;
 
 out_unmap:
-	svc_rdma_unmap_dma(ctxt);
-	svc_rdma_put_context(ctxt, 1);
+	svc_rdma_send_ctxt_put(rdma, ctxt);
 	ret = -EIO;
 	goto out_err;
 }
diff --git a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 2eed6e104513..caa847fc77f4 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@ -392,7 +392,7 @@ static void rdma_read_complete(struct svc_rqst *rqstp,
 static void svc_rdma_send_error(struct svcxprt_rdma *xprt,
 				__be32 *rdma_argp, int status)
 {
-	struct svc_rdma_op_ctxt *ctxt;
+	struct svc_rdma_send_ctxt *ctxt;
 	__be32 *p, *err_msgp;
 	unsigned int length;
 	struct page *page;
@@ -422,7 +422,10 @@ static void svc_rdma_send_error(struct svcxprt_rdma *xprt,
 	length = (unsigned long)p - (unsigned long)err_msgp;
 
 	/* Map transport header; no RPC message payload */
-	ctxt = svc_rdma_get_context(xprt);
+	ctxt = svc_rdma_send_ctxt_get(xprt);
+	if (!ctxt)
+		return;
+
 	ret = svc_rdma_map_reply_hdr(xprt, ctxt, err_msgp, length);
 	if (ret) {
 		dprintk("svcrdma: Error %d mapping send for protocol error\n",
@@ -431,10 +434,8 @@ static void svc_rdma_send_error(struct svcxprt_rdma *xprt,
 	}
 
 	ret = svc_rdma_post_send_wr(xprt, ctxt, 0);
-	if (ret) {
-		svc_rdma_unmap_dma(ctxt);
-		svc_rdma_put_context(ctxt, 1);
-	}
+	if (ret)
+		svc_rdma_send_ctxt_put(xprt, ctxt);
 }
 
 /* By convention, backchannel calls arrive via rdma_msg type
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_sendto.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_transport.c
