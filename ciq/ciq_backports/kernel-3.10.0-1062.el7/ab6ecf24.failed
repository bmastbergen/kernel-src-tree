mm: /proc/pid/pagemap: hide swap entries from unprivileged users

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Huang Ying <ying.huang@intel.com>
commit ab6ecf247a9321e3180e021a6a60164dee53ab2e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ab6ecf24.failed

In commit ab676b7d6fbf ("pagemap: do not leak physical addresses to
non-privileged userspace"), the /proc/PID/pagemap is restricted to be
readable only by CAP_SYS_ADMIN to address some security issue.

In commit 1c90308e7a77 ("pagemap: hide physical addresses from
non-privileged users"), the restriction is relieved to make
/proc/PID/pagemap readable, but hide the physical addresses for
non-privileged users.

But the swap entries are readable for non-privileged users too.  This
has some security issues.  For example, for page under migrating, the
swap entry has physical address information.  So, in this patch, the
swap entries are hided for non-privileged users too.

Link: http://lkml.kernel.org/r/20180508012745.7238-1-ying.huang@intel.com
Fixes: 1c90308e7a77 ("pagemap: hide physical addresses from non-privileged users")
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Suggested-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Andrei Vagin <avagin@openvz.org>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: Daniel Colascione <dancol@google.com>
	Cc: Zi Yan <zi.yan@cs.rutgers.edu>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ab6ecf247a9321e3180e021a6a60164dee53ab2e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
diff --cc fs/proc/task_mmu.c
index bf81d9f4a02e,597969db9e90..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -1096,81 -1257,92 +1096,136 @@@ static void pte_to_pagemap_entry(pagema
  	} else if (is_swap_pte(pte)) {
  		swp_entry_t entry;
  		if (pte_swp_soft_dirty(pte))
 -			flags |= PM_SOFT_DIRTY;
 +			flags2 |= __PM_SOFT_DIRTY;
  		entry = pte_to_swp_entry(pte);
++<<<<<<< HEAD
 +		frame = swp_type(entry) |
 +			(swp_offset(entry) << MAX_SWAPFILES_SHIFT);
 +		flags = PM_SWAP;
++=======
+ 		if (pm->show_pfn)
+ 			frame = swp_type(entry) |
+ 				(swp_offset(entry) << MAX_SWAPFILES_SHIFT);
+ 		flags |= PM_SWAP;
++>>>>>>> ab6ecf247a93 (mm: /proc/pid/pagemap: hide swap entries from unprivileged users)
  		if (is_migration_entry(entry))
  			page = migration_entry_to_page(entry);
 -
 -		if (is_device_private_entry(entry))
 -			page = device_private_entry_to_page(entry);
 +		else if (is_hmm_entry(entry))
 +			page = hmm_entry_to_page(entry);
 +	} else {
 +		if (vma->vm_flags & VM_SOFTDIRTY)
 +			flags2 |= __PM_SOFT_DIRTY;
 +		*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, flags2));
 +		return;
  	}
  
  	if (page && !PageAnon(page))
  		flags |= PM_FILE;
 -	if (page && page_mapcount(page) == 1)
 -		flags |= PM_MMAP_EXCLUSIVE;
 -	if (vma->vm_flags & VM_SOFTDIRTY)
 -		flags |= PM_SOFT_DIRTY;
 +	if ((vma->vm_flags & VM_SOFTDIRTY))
 +		flags2 |= __PM_SOFT_DIRTY;
  
 -	return make_pme(frame, flags);
 +	*pme = make_pme(PM_PFRAME(frame) | PM_STATUS2(pm->v2, flags2) | flags);
  }
  
 -static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,
 -			     struct mm_walk *walk)
 -{
 -	struct vm_area_struct *vma = walk->vma;
 -	struct pagemapread *pm = walk->private;
 -	spinlock_t *ptl;
 -	pte_t *pte, *orig_pte;
 -	int err = 0;
 -
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
++<<<<<<< HEAD
 +static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,
 +		pmd_t pmd, int offset, int pmd_flags2)
 +{
 +	/*
 +	 * Currently pmd for thp is always present because thp can not be
 +	 * swapped-out, migrated, or HWPOISONed (split in such cases instead.)
 +	 * This if-check is just to prepare for future implementation.
 +	 */
 +	if (pmd_present(pmd))
 +		*pme = make_pme(PM_PFRAME(pmd_pfn(pmd) + offset)
 +				| PM_STATUS2(pm->v2, pmd_flags2) | PM_PRESENT);
 +	else
 +		*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, pmd_flags2));
 +}
 +#else
 +static inline void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,
 +		pmd_t pmd, int offset, int pmd_flags2)
 +{
 +}
++=======
+ 	ptl = pmd_trans_huge_lock(pmdp, vma);
+ 	if (ptl) {
+ 		u64 flags = 0, frame = 0;
+ 		pmd_t pmd = *pmdp;
+ 		struct page *page = NULL;
+ 
+ 		if (vma->vm_flags & VM_SOFTDIRTY)
+ 			flags |= PM_SOFT_DIRTY;
+ 
+ 		if (pmd_present(pmd)) {
+ 			page = pmd_page(pmd);
+ 
+ 			flags |= PM_PRESENT;
+ 			if (pmd_soft_dirty(pmd))
+ 				flags |= PM_SOFT_DIRTY;
+ 			if (pm->show_pfn)
+ 				frame = pmd_pfn(pmd) +
+ 					((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 		}
+ #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+ 		else if (is_swap_pmd(pmd)) {
+ 			swp_entry_t entry = pmd_to_swp_entry(pmd);
+ 			unsigned long offset;
+ 
+ 			if (pm->show_pfn) {
+ 				offset = swp_offset(entry) +
+ 					((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 				frame = swp_type(entry) |
+ 					(offset << MAX_SWAPFILES_SHIFT);
+ 			}
+ 			flags |= PM_SWAP;
+ 			if (pmd_swp_soft_dirty(pmd))
+ 				flags |= PM_SOFT_DIRTY;
+ 			VM_BUG_ON(!is_pmd_migration_entry(pmd));
+ 			page = migration_entry_to_page(entry);
+ 		}
++>>>>>>> ab6ecf247a93 (mm: /proc/pid/pagemap: hide swap entries from unprivileged users)
  #endif
  
 -		if (page && page_mapcount(page) == 1)
 -			flags |= PM_MMAP_EXCLUSIVE;
 +static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
 +			     struct mm_walk *walk)
 +{
 +	struct vm_area_struct *vma;
 +	struct pagemapread *pm = walk->private;
 +	spinlock_t *ptl;
 +	pte_t *pte;
 +	int err = 0;
 +
 +	/* find the first VMA at or above 'addr' */
 +	vma = find_vma(walk->mm, addr);
 +	if (vma && pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 +		int pmd_flags2;
 +
 +		if ((vma->vm_flags & VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))
 +			pmd_flags2 = __PM_SOFT_DIRTY;
 +		else
 +			pmd_flags2 = 0;
  
  		for (; addr != end; addr += PAGE_SIZE) {
 -			pagemap_entry_t pme = make_pme(frame, flags);
 +			unsigned long offset;
 +			pagemap_entry_t pme;
  
 +			offset = (addr & ~PAGEMAP_WALK_MASK) >>
 +					PAGE_SHIFT;
 +			thp_pmd_to_pagemap_entry(&pme, pm, *pmd, offset, pmd_flags2);
  			err = add_to_pagemap(addr, &pme, pm);
  			if (err)
  				break;
++<<<<<<< HEAD
++=======
+ 			if (pm->show_pfn) {
+ 				if (flags & PM_PRESENT)
+ 					frame++;
+ 				else if (flags & PM_SWAP)
+ 					frame += (1 << MAX_SWAPFILES_SHIFT);
+ 			}
++>>>>>>> ab6ecf247a93 (mm: /proc/pid/pagemap: hide swap entries from unprivileged users)
  		}
  		spin_unlock(ptl);
  		return err;
* Unmerged path fs/proc/task_mmu.c
