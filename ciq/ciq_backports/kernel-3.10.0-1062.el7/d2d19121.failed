IB/mlx5: Set uid as part of TD commands

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Yishai Hadas <yishaih@mellanox.com>
commit d2d19121ae2f4bc4e818dd770c1746deadf14093
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d2d19121.failed

Set uid as part of TD commands so that the firmware can
manage the TD object in a secured way.

	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit d2d19121ae2f4bc4e818dd770c1746deadf14093)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/cmd.c
#	drivers/infiniband/hw/mlx5/cmd.h
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/cmd.c
index 20fa395e65e0,ca060a2e2b36..000000000000
--- a/drivers/infiniband/hw/mlx5/cmd.c
+++ b/drivers/infiniband/hw/mlx5/cmd.c
@@@ -81,3 -81,248 +81,251 @@@ int mlx5_cmd_modify_cong_params(struct 
  
  	return mlx5_cmd_exec(dev, in, in_size, out, sizeof(out));
  }
++<<<<<<< HEAD
++=======
+ 
+ int mlx5_cmd_alloc_memic(struct mlx5_memic *memic, phys_addr_t *addr,
+ 			  u64 length, u32 alignment)
+ {
+ 	struct mlx5_core_dev *dev = memic->dev;
+ 	u64 num_memic_hw_pages = MLX5_CAP_DEV_MEM(dev, memic_bar_size)
+ 					>> PAGE_SHIFT;
+ 	u64 hw_start_addr = MLX5_CAP64_DEV_MEM(dev, memic_bar_start_addr);
+ 	u32 max_alignment = MLX5_CAP_DEV_MEM(dev, log_max_memic_addr_alignment);
+ 	u32 num_pages = DIV_ROUND_UP(length, PAGE_SIZE);
+ 	u32 out[MLX5_ST_SZ_DW(alloc_memic_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(alloc_memic_in)] = {};
+ 	u32 mlx5_alignment;
+ 	u64 page_idx = 0;
+ 	int ret = 0;
+ 
+ 	if (!length || (length & MLX5_MEMIC_ALLOC_SIZE_MASK))
+ 		return -EINVAL;
+ 
+ 	/* mlx5 device sets alignment as 64*2^driver_value
+ 	 * so normalizing is needed.
+ 	 */
+ 	mlx5_alignment = (alignment < MLX5_MEMIC_BASE_ALIGN) ? 0 :
+ 			 alignment - MLX5_MEMIC_BASE_ALIGN;
+ 	if (mlx5_alignment > max_alignment)
+ 		return -EINVAL;
+ 
+ 	MLX5_SET(alloc_memic_in, in, opcode, MLX5_CMD_OP_ALLOC_MEMIC);
+ 	MLX5_SET(alloc_memic_in, in, range_size, num_pages * PAGE_SIZE);
+ 	MLX5_SET(alloc_memic_in, in, memic_size, length);
+ 	MLX5_SET(alloc_memic_in, in, log_memic_addr_alignment,
+ 		 mlx5_alignment);
+ 
+ 	while (page_idx < num_memic_hw_pages) {
+ 		spin_lock(&memic->memic_lock);
+ 		page_idx = bitmap_find_next_zero_area(memic->memic_alloc_pages,
+ 						      num_memic_hw_pages,
+ 						      page_idx,
+ 						      num_pages, 0);
+ 
+ 		if (page_idx < num_memic_hw_pages)
+ 			bitmap_set(memic->memic_alloc_pages,
+ 				   page_idx, num_pages);
+ 
+ 		spin_unlock(&memic->memic_lock);
+ 
+ 		if (page_idx >= num_memic_hw_pages)
+ 			break;
+ 
+ 		MLX5_SET64(alloc_memic_in, in, range_start_addr,
+ 			   hw_start_addr + (page_idx * PAGE_SIZE));
+ 
+ 		ret = mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ 		if (ret) {
+ 			spin_lock(&memic->memic_lock);
+ 			bitmap_clear(memic->memic_alloc_pages,
+ 				     page_idx, num_pages);
+ 			spin_unlock(&memic->memic_lock);
+ 
+ 			if (ret == -EAGAIN) {
+ 				page_idx++;
+ 				continue;
+ 			}
+ 
+ 			return ret;
+ 		}
+ 
+ 		*addr = pci_resource_start(dev->pdev, 0) +
+ 			MLX5_GET64(alloc_memic_out, out, memic_start_addr);
+ 
+ 		return 0;
+ 	}
+ 
+ 	return -ENOMEM;
+ }
+ 
+ int mlx5_cmd_dealloc_memic(struct mlx5_memic *memic, u64 addr, u64 length)
+ {
+ 	struct mlx5_core_dev *dev = memic->dev;
+ 	u64 hw_start_addr = MLX5_CAP64_DEV_MEM(dev, memic_bar_start_addr);
+ 	u32 num_pages = DIV_ROUND_UP(length, PAGE_SIZE);
+ 	u32 out[MLX5_ST_SZ_DW(dealloc_memic_out)] = {0};
+ 	u32 in[MLX5_ST_SZ_DW(dealloc_memic_in)] = {0};
+ 	u64 start_page_idx;
+ 	int err;
+ 
+ 	addr -= pci_resource_start(dev->pdev, 0);
+ 	start_page_idx = (addr - hw_start_addr) >> PAGE_SHIFT;
+ 
+ 	MLX5_SET(dealloc_memic_in, in, opcode, MLX5_CMD_OP_DEALLOC_MEMIC);
+ 	MLX5_SET64(dealloc_memic_in, in, memic_start_addr, addr);
+ 	MLX5_SET(dealloc_memic_in, in, memic_size, length);
+ 
+ 	err =  mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ 
+ 	if (!err) {
+ 		spin_lock(&memic->memic_lock);
+ 		bitmap_clear(memic->memic_alloc_pages,
+ 			     start_page_idx, num_pages);
+ 		spin_unlock(&memic->memic_lock);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ int mlx5_cmd_query_ext_ppcnt_counters(struct mlx5_core_dev *dev, void *out)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(ppcnt_reg)] = {};
+ 	int sz = MLX5_ST_SZ_BYTES(ppcnt_reg);
+ 
+ 	MLX5_SET(ppcnt_reg, in, local_port, 1);
+ 
+ 	MLX5_SET(ppcnt_reg, in, grp, MLX5_ETHERNET_EXTENDED_COUNTERS_GROUP);
+ 	return  mlx5_core_access_reg(dev, in, sz, out, sz, MLX5_REG_PPCNT,
+ 				     0, 0);
+ }
+ 
+ void mlx5_cmd_destroy_tir(struct mlx5_core_dev *dev, u32 tirn, u16 uid)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(destroy_tir_in)]   = {};
+ 	u32 out[MLX5_ST_SZ_DW(destroy_tir_out)] = {};
+ 
+ 	MLX5_SET(destroy_tir_in, in, opcode, MLX5_CMD_OP_DESTROY_TIR);
+ 	MLX5_SET(destroy_tir_in, in, tirn, tirn);
+ 	MLX5_SET(destroy_tir_in, in, uid, uid);
+ 	mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ void mlx5_cmd_destroy_tis(struct mlx5_core_dev *dev, u32 tisn, u16 uid)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(destroy_tis_in)]   = {0};
+ 	u32 out[MLX5_ST_SZ_DW(destroy_tis_out)] = {0};
+ 
+ 	MLX5_SET(destroy_tis_in, in, opcode, MLX5_CMD_OP_DESTROY_TIS);
+ 	MLX5_SET(destroy_tis_in, in, tisn, tisn);
+ 	MLX5_SET(destroy_tis_in, in, uid, uid);
+ 	mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ void mlx5_cmd_destroy_rqt(struct mlx5_core_dev *dev, u32 rqtn, u16 uid)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(destroy_rqt_in)]   = {};
+ 	u32 out[MLX5_ST_SZ_DW(destroy_rqt_out)] = {};
+ 
+ 	MLX5_SET(destroy_rqt_in, in, opcode, MLX5_CMD_OP_DESTROY_RQT);
+ 	MLX5_SET(destroy_rqt_in, in, rqtn, rqtn);
+ 	MLX5_SET(destroy_rqt_in, in, uid, uid);
+ 	mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ int mlx5_cmd_alloc_transport_domain(struct mlx5_core_dev *dev, u32 *tdn,
+ 				    u16 uid)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(alloc_transport_domain_in)]   = {0};
+ 	u32 out[MLX5_ST_SZ_DW(alloc_transport_domain_out)] = {0};
+ 	int err;
+ 
+ 	MLX5_SET(alloc_transport_domain_in, in, opcode,
+ 		 MLX5_CMD_OP_ALLOC_TRANSPORT_DOMAIN);
+ 
+ 	err = mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ 	if (!err)
+ 		*tdn = MLX5_GET(alloc_transport_domain_out, out,
+ 				transport_domain);
+ 
+ 	return err;
+ }
+ 
+ void mlx5_cmd_dealloc_transport_domain(struct mlx5_core_dev *dev, u32 tdn,
+ 				       u16 uid)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(dealloc_transport_domain_in)]   = {0};
+ 	u32 out[MLX5_ST_SZ_DW(dealloc_transport_domain_out)] = {0};
+ 
+ 	MLX5_SET(dealloc_transport_domain_in, in, opcode,
+ 		 MLX5_CMD_OP_DEALLOC_TRANSPORT_DOMAIN);
+ 	MLX5_SET(dealloc_transport_domain_in, in, transport_domain, tdn);
+ 	mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ void mlx5_cmd_dealloc_pd(struct mlx5_core_dev *dev, u32 pdn, u16 uid)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(dealloc_pd_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(dealloc_pd_in)]   = {};
+ 
+ 	MLX5_SET(dealloc_pd_in, in, opcode, MLX5_CMD_OP_DEALLOC_PD);
+ 	MLX5_SET(dealloc_pd_in, in, pd, pdn);
+ 	MLX5_SET(dealloc_pd_in, in, uid, uid);
+ 	mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ int mlx5_cmd_attach_mcg(struct mlx5_core_dev *dev, union ib_gid *mgid,
+ 			u32 qpn, u16 uid)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(attach_to_mcg_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(attach_to_mcg_in)]   = {};
+ 	void *gid;
+ 
+ 	MLX5_SET(attach_to_mcg_in, in, opcode, MLX5_CMD_OP_ATTACH_TO_MCG);
+ 	MLX5_SET(attach_to_mcg_in, in, qpn, qpn);
+ 	MLX5_SET(attach_to_mcg_in, in, uid, uid);
+ 	gid = MLX5_ADDR_OF(attach_to_mcg_in, in, multicast_gid);
+ 	memcpy(gid, mgid, sizeof(*mgid));
+ 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ int mlx5_cmd_detach_mcg(struct mlx5_core_dev *dev, union ib_gid *mgid,
+ 			u32 qpn, u16 uid)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(detach_from_mcg_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(detach_from_mcg_in)]   = {};
+ 	void *gid;
+ 
+ 	MLX5_SET(detach_from_mcg_in, in, opcode, MLX5_CMD_OP_DETACH_FROM_MCG);
+ 	MLX5_SET(detach_from_mcg_in, in, qpn, qpn);
+ 	MLX5_SET(detach_from_mcg_in, in, uid, uid);
+ 	gid = MLX5_ADDR_OF(detach_from_mcg_in, in, multicast_gid);
+ 	memcpy(gid, mgid, sizeof(*mgid));
+ 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
+ 
+ int mlx5_cmd_xrcd_alloc(struct mlx5_core_dev *dev, u32 *xrcdn, u16 uid)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(alloc_xrcd_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(alloc_xrcd_in)]   = {};
+ 	int err;
+ 
+ 	MLX5_SET(alloc_xrcd_in, in, opcode, MLX5_CMD_OP_ALLOC_XRCD);
+ 	MLX5_SET(alloc_xrcd_in, in, uid, uid);
+ 	err = mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ 	if (!err)
+ 		*xrcdn = MLX5_GET(alloc_xrcd_out, out, xrcd);
+ 	return err;
+ }
+ 
+ int mlx5_cmd_xrcd_dealloc(struct mlx5_core_dev *dev, u32 xrcdn, u16 uid)
+ {
+ 	u32 out[MLX5_ST_SZ_DW(dealloc_xrcd_out)] = {};
+ 	u32 in[MLX5_ST_SZ_DW(dealloc_xrcd_in)]   = {};
+ 
+ 	MLX5_SET(dealloc_xrcd_in, in, opcode, MLX5_CMD_OP_DEALLOC_XRCD);
+ 	MLX5_SET(dealloc_xrcd_in, in, xrcd, xrcdn);
+ 	MLX5_SET(dealloc_xrcd_in, in, uid, uid);
+ 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+ }
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
diff --cc drivers/infiniband/hw/mlx5/cmd.h
index 802f939e52b9,c03c56455534..000000000000
--- a/drivers/infiniband/hw/mlx5/cmd.h
+++ b/drivers/infiniband/hw/mlx5/cmd.h
@@@ -40,6 -41,24 +40,26 @@@ int mlx5_cmd_dump_fill_mkey(struct mlx5
  int mlx5_cmd_null_mkey(struct mlx5_core_dev *dev, u32 *null_mkey);
  int mlx5_cmd_query_cong_params(struct mlx5_core_dev *dev, int cong_point,
  			       void *out, int out_size);
 -int mlx5_cmd_query_ext_ppcnt_counters(struct mlx5_core_dev *dev, void *out);
  int mlx5_cmd_modify_cong_params(struct mlx5_core_dev *mdev,
  				void *in, int in_size);
++<<<<<<< HEAD
++=======
+ int mlx5_cmd_alloc_memic(struct mlx5_memic *memic, phys_addr_t *addr,
+ 			 u64 length, u32 alignment);
+ int mlx5_cmd_dealloc_memic(struct mlx5_memic *memic, u64 addr, u64 length);
+ void mlx5_cmd_dealloc_pd(struct mlx5_core_dev *dev, u32 pdn, u16 uid);
+ void mlx5_cmd_destroy_tir(struct mlx5_core_dev *dev, u32 tirn, u16 uid);
+ void mlx5_cmd_destroy_tis(struct mlx5_core_dev *dev, u32 tisn, u16 uid);
+ void mlx5_cmd_destroy_rqt(struct mlx5_core_dev *dev, u32 rqtn, u16 uid);
+ int mlx5_cmd_alloc_transport_domain(struct mlx5_core_dev *dev, u32 *tdn,
+ 				    u16 uid);
+ void mlx5_cmd_dealloc_transport_domain(struct mlx5_core_dev *dev, u32 tdn,
+ 				       u16 uid);
+ int mlx5_cmd_attach_mcg(struct mlx5_core_dev *dev, union ib_gid *mgid,
+ 			u32 qpn, u16 uid);
+ int mlx5_cmd_detach_mcg(struct mlx5_core_dev *dev, union ib_gid *mgid,
+ 			u32 qpn, u16 uid);
+ int mlx5_cmd_xrcd_alloc(struct mlx5_core_dev *dev, u32 *xrcdn, u16 uid);
+ int mlx5_cmd_xrcd_dealloc(struct mlx5_core_dev *dev, u32 xrcdn, u16 uid);
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
  #endif /* MLX5_IB_CMD_H */
diff --cc drivers/infiniband/hw/mlx5/main.c
index b529cb4f76e1,91693e1a3731..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1588,7 -1571,50 +1588,54 @@@ static void deallocate_uars(struct mlx5
  			mlx5_cmd_free_uar(dev->mdev, bfregi->sys_pages[i]);
  }
  
++<<<<<<< HEAD
 +static int mlx5_ib_alloc_transport_domain(struct mlx5_ib_dev *dev, u32 *tdn)
++=======
+ int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)
+ {
+ 	int err = 0;
+ 
+ 	mutex_lock(&dev->lb.mutex);
+ 	if (td)
+ 		dev->lb.user_td++;
+ 	if (qp)
+ 		dev->lb.qps++;
+ 
+ 	if (dev->lb.user_td == 2 ||
+ 	    dev->lb.qps == 1) {
+ 		if (!dev->lb.enabled) {
+ 			err = mlx5_nic_vport_update_local_lb(dev->mdev, true);
+ 			dev->lb.enabled = true;
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&dev->lb.mutex);
+ 
+ 	return err;
+ }
+ 
+ void mlx5_ib_disable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)
+ {
+ 	mutex_lock(&dev->lb.mutex);
+ 	if (td)
+ 		dev->lb.user_td--;
+ 	if (qp)
+ 		dev->lb.qps--;
+ 
+ 	if (dev->lb.user_td == 1 &&
+ 	    dev->lb.qps == 0) {
+ 		if (dev->lb.enabled) {
+ 			mlx5_nic_vport_update_local_lb(dev->mdev, false);
+ 			dev->lb.enabled = false;
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&dev->lb.mutex);
+ }
+ 
+ static int mlx5_ib_alloc_transport_domain(struct mlx5_ib_dev *dev, u32 *tdn,
+ 					  u16 uid)
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
  {
  	int err;
  
@@@ -1604,17 -1630,11 +1651,18 @@@
  	     !MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))
  		return err;
  
 -	return mlx5_ib_enable_lb(dev, true, false);
 +	mutex_lock(&dev->lb_mutex);
 +	dev->user_td++;
 +
 +	if (dev->user_td == 2)
 +		err = mlx5_nic_vport_update_local_lb(dev->mdev, true);
 +
 +	mutex_unlock(&dev->lb_mutex);
 +	return err;
  }
  
- static void mlx5_ib_dealloc_transport_domain(struct mlx5_ib_dev *dev, u32 tdn)
+ static void mlx5_ib_dealloc_transport_domain(struct mlx5_ib_dev *dev, u32 tdn,
+ 					     u16 uid)
  {
  	if (!MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
  		return;
@@@ -1744,9 -1758,22 +1792,28 @@@ static struct ib_ucontext *mlx5_ib_allo
  	context->ibucontext.invalidate_range = &mlx5_ib_invalidate_range;
  #endif
  
++<<<<<<< HEAD
 +	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn);
 +	if (err)
 +		goto out_uars;
++=======
+ 	if (req.flags & MLX5_IB_ALLOC_UCTX_DEVX) {
+ 		/* Block DEVX on Infiniband as of SELinux */
+ 		if (mlx5_ib_port_link_layer(ibdev, 1) != IB_LINK_LAYER_ETHERNET) {
+ 			err = -EPERM;
+ 			goto out_uars;
+ 		}
+ 
+ 		err = mlx5_ib_devx_create(dev, context);
+ 		if (err)
+ 			goto out_uars;
+ 	}
+ 
+ 	err = mlx5_ib_alloc_transport_domain(dev, &context->tdn,
+ 					     context->devx_uid);
+ 	if (err)
+ 		goto out_devx;
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
  
  	if (MLX5_CAP_GEN(dev->mdev, dump_fill_mkey)) {
  		err = mlx5_cmd_dump_fill_mkey(dev->mdev, &dump_fill_mkey);
@@@ -1831,10 -1856,21 +1898,18 @@@
  	context->lib_caps = req.lib_caps;
  	print_lib_caps(dev, context->lib_caps);
  
 -	if (mlx5_lag_is_active(dev->mdev)) {
 -		u8 port = mlx5_core_native_port_num(dev->mdev);
 -
 -		atomic_set(&context->tx_port_affinity,
 -			   atomic_add_return(
 -				   1, &dev->roce[port].tx_port_affinity));
 -	}
 -
  	return &context->ibucontext;
  
++<<<<<<< HEAD
 +out_td:
 +	mlx5_ib_dealloc_transport_domain(dev, context->tdn);
++=======
+ out_mdev:
+ 	mlx5_ib_dealloc_transport_domain(dev, context->tdn, context->devx_uid);
+ out_devx:
+ 	if (req.flags & MLX5_IB_ALLOC_UCTX_DEVX)
+ 		mlx5_ib_devx_destroy(dev, context);
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
  
  out_uars:
  	deallocate_uars(dev, context);
@@@ -1857,8 -1893,18 +1932,23 @@@ static int mlx5_ib_dealloc_ucontext(str
  	struct mlx5_ib_dev *dev = to_mdev(ibcontext->device);
  	struct mlx5_bfreg_info *bfregi;
  
++<<<<<<< HEAD
 +	bfregi = &context->bfregi;
 +	mlx5_ib_dealloc_transport_domain(dev, context->tdn);
++=======
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	/* All umem's must be destroyed before destroying the ucontext. */
+ 	mutex_lock(&ibcontext->per_mm_list_lock);
+ 	WARN_ON(!list_empty(&ibcontext->per_mm_list));
+ 	mutex_unlock(&ibcontext->per_mm_list_lock);
+ #endif
+ 
+ 	bfregi = &context->bfregi;
+ 	mlx5_ib_dealloc_transport_domain(dev, context->tdn, context->devx_uid);
+ 
+ 	if (context->devx_uid)
+ 		mlx5_ib_devx_destroy(dev, context);
++>>>>>>> d2d19121ae2f (IB/mlx5: Set uid as part of TD commands)
  
  	deallocate_uars(dev, context);
  	kfree(bfregi->sys_pages);
* Unmerged path drivers/infiniband/hw/mlx5/cmd.c
* Unmerged path drivers/infiniband/hw/mlx5/cmd.h
* Unmerged path drivers/infiniband/hw/mlx5/main.c
