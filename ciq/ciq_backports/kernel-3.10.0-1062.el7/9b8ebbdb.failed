KVM: x86: extend usage of RET_MMIO_PF_* constants

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 9b8ebbdb74b5ad76b9dfd8b101af17839174b126
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/9b8ebbdb.failed

The x86 MMU if full of code that returns 0 and 1 for retry/emulate.  Use
the existing RET_MMIO_PF_RETRY/RET_MMIO_PF_EMULATE enum, renaming it to
drop the MMIO part.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 9b8ebbdb74b5ad76b9dfd8b101af17839174b126)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/paging_tmpl.h
diff --cc arch/x86/kvm/mmu.c
index 890092e57ed7,0b481cc9c725..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2795,13 -2808,13 +2809,23 @@@ done
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 +			 unsigned pte_access, int write_fault, int *emulate,
 +			 int level, gfn_t gfn, kvm_pfn_t pfn, bool speculative,
 +			 bool host_writable)
 +{
 +	int was_rmapped = 0;
 +	int rmap_count;
++=======
+ static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep, unsigned pte_access,
+ 			int write_fault, int level, gfn_t gfn, kvm_pfn_t pfn,
+ 		       	bool speculative, bool host_writable)
+ {
+ 	int was_rmapped = 0;
+ 	int rmap_count;
+ 	int ret = RET_PF_RETRY;
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  
  	pgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,
  		 *sptep, write_fault, gfn);
@@@ -2831,12 -2844,12 +2855,21 @@@
  	if (set_spte(vcpu, sptep, pte_access, level, gfn, pfn, speculative,
  	      true, host_writable)) {
  		if (write_fault)
++<<<<<<< HEAD
 +			*emulate = 1;
 +		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +	}
 +
 +	if (unlikely(is_mmio_spte(*sptep) && emulate))
 +		*emulate = 1;
++=======
+ 			ret = RET_PF_EMULATE;
+ 		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
+ 	}
+ 
+ 	if (unlikely(is_mmio_spte(*sptep)))
+ 		ret = RET_PF_EMULATE;
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  
  	pgprintk("%s: setting spte %llx\n", __func__, *sptep);
  	pgprintk("instantiating %s PTE (%s) at %llx (%llx) addr %p\n",
@@@ -2855,6 -2868,8 +2888,11 @@@
  	}
  
  	kvm_release_pfn_clean(pfn);
++<<<<<<< HEAD
++=======
+ 
+ 	return ret;
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  }
  
  static kvm_pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
@@@ -4861,12 -4906,18 +4882,16 @@@ int kvm_mmu_page_fault(struct kvm_vcpu 
  {
  	int r, emulation_type = EMULTYPE_RETRY;
  	enum emulation_result er;
 -	bool direct = vcpu->arch.mmu.direct_map;
 -
 -	/* With shadow page tables, fault_address contains a GVA or nGPA.  */
 -	if (vcpu->arch.mmu.direct_map) {
 -		vcpu->arch.gpa_available = true;
 -		vcpu->arch.gpa_val = cr2;
 -	}
 +	bool direct = vcpu->arch.mmu.direct_map || mmu_is_nested(vcpu);
  
++<<<<<<< HEAD
 +	vcpu->arch.l1tf_flush_l1d = true;
++=======
+ 	r = RET_PF_INVALID;
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  	if (unlikely(error_code & PFERR_RSVD_MASK)) {
  		r = handle_mmio_page_fault(vcpu, cr2, direct);
- 		if (r == RET_MMIO_PF_EMULATE) {
+ 		if (r == RET_PF_EMULATE) {
  			emulation_type = 0;
  			goto emulate;
  		}
diff --cc arch/x86/kvm/paging_tmpl.h
index 54dd3c60caf1,5abae72266b7..000000000000
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@@ -593,7 -593,7 +593,11 @@@ static int FNAME(fetch)(struct kvm_vcp
  	struct kvm_mmu_page *sp = NULL;
  	struct kvm_shadow_walk_iterator it;
  	unsigned direct_access, access = gw->pt_access;
++<<<<<<< HEAD
 +	int top_level, emulate = 0;
++=======
+ 	int top_level, ret;
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  
  	direct_access = gw->pte_access;
  
@@@ -659,11 -659,11 +663,16 @@@
  	}
  
  	clear_sp_write_flooding_count(it.sptep);
++<<<<<<< HEAD
 +	mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault, &emulate,
 +		     it.level, gw->gfn, pfn, prefault, map_writable);
++=======
+ 	ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
+ 			   it.level, gw->gfn, pfn, prefault, map_writable);
++>>>>>>> 9b8ebbdb74b5 (KVM: x86: extend usage of RET_MMIO_PF_* constants)
  	FNAME(pte_prefetch)(vcpu, gw, it.sptep);
  
- 	return emulate;
+ 	return ret;
  
  out_gpte_changed:
  	kvm_release_pfn_clean(pfn);
@@@ -789,10 -789,9 +798,10 @@@ static int FNAME(page_fault)(struct kvm
  
  	if (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,
  			 &map_writable))
- 		return 0;
+ 		return RET_PF_RETRY;
  
 -	if (handle_abnormal_pfn(vcpu, addr, walker.gfn, pfn, walker.pte_access, &r))
 +	if (handle_abnormal_pfn(vcpu, mmu_is_nested(vcpu) ? 0 : addr,
 +				walker.gfn, pfn, walker.pte_access, &r))
  		return r;
  
  	/*
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/paging_tmpl.h
