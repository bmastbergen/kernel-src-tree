RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit 41b4deeaa123e62e1037af7a0be547af2e0e05f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/41b4deea.failed

These two structures are linked together, use the container_of pattern
instead of a double allocation to make the code simpler and easier to
follow.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 41b4deeaa123e62e1037af7a0be547af2e0e05f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
#	drivers/infiniband/core/umem_odp.c
diff --cc drivers/infiniband/core/umem.c
index 097e8522c0fc,88b9b88f90e1..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -117,25 -124,23 +124,41 @@@ struct ib_umem *ib_umem_get(struct ib_u
  	umem->address    = addr;
  	umem->page_shift = PAGE_SHIFT;
  	umem->writable   = ib_access_writable(access);
+ 	umem->owning_mm = mm = current->mm;
+ 	mmgrab(mm);
  
  	if (access & IB_ACCESS_ON_DEMAND) {
++<<<<<<< HEAD
 +		ret = ib_umem_odp_get(context, umem, access);
 +		if (ret) {
 +			kfree(umem);
 +			return ERR_PTR(ret);
 +		}
 +		return umem;
 +	}
 +
 +	umem->odp_data = NULL;
 +
++=======
+ 		ret = ib_umem_odp_get(to_ib_umem_odp(umem), access);
+ 		if (ret)
+ 			goto umem_kfree;
+ 		return umem;
+ 	}
+ 
++>>>>>>> 41b4deeaa123 (RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem)
  	/* We assume the memory is from hugetlb until proved otherwise */
  	umem->hugetlb   = 1;
  
  	page_list = (struct page **) __get_free_page(GFP_KERNEL);
  	if (!page_list) {
++<<<<<<< HEAD
 +		kfree(umem);
 +		return ERR_PTR(-ENOMEM);
++=======
+ 		ret = -ENOMEM;
+ 		goto umem_kfree;
++>>>>>>> 41b4deeaa123 (RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem)
  	}
  
  	/*
@@@ -221,12 -231,25 +244,34 @@@ out
  	if (vma_list)
  		free_page((unsigned long) vma_list);
  	free_page((unsigned long) page_list);
++<<<<<<< HEAD
 +
 +	return ret < 0 ? ERR_PTR(ret) : umem;
 +}
 +EXPORT_SYMBOL(ib_umem_get);
 +
 +static void ib_umem_account(struct work_struct *work)
++=======
+ umem_kfree:
+ 	if (ret) {
+ 		mmdrop(umem->owning_mm);
+ 		kfree(umem);
+ 	}
+ 	return ret ? ERR_PTR(ret) : umem;
+ }
+ EXPORT_SYMBOL(ib_umem_get);
+ 
+ static void __ib_umem_release_tail(struct ib_umem *umem)
+ {
+ 	mmdrop(umem->owning_mm);
+ 	if (umem->odp_data)
+ 		kfree(to_ib_umem_odp(umem));
+ 	else
+ 		kfree(umem);
+ }
+ 
+ static void ib_umem_release_defer(struct work_struct *work)
++>>>>>>> 41b4deeaa123 (RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem)
  {
  	struct ib_umem *umem = container_of(work, struct ib_umem, work);
  
diff --cc drivers/infiniband/core/umem_odp.c
index 505862826f1c,900fdedfe910..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -359,28 -370,23 +351,35 @@@ int ib_umem_odp_get(struct ib_umem_odp 
  		goto out_mm;
  	}
  
- 	umem->odp_data = kzalloc(sizeof(*umem->odp_data), GFP_KERNEL);
- 	if (!umem->odp_data) {
- 		ret_val = -ENOMEM;
- 		goto out_mm;
- 	}
- 	umem->odp_data->umem = umem;
- 
- 	mutex_init(&umem->odp_data->umem_mutex);
+ 	mutex_init(&umem_odp->umem_mutex);
  
- 	init_completion(&umem->odp_data->notifier_completion);
+ 	init_completion(&umem_odp->notifier_completion);
  
  	if (ib_umem_num_pages(umem)) {
++<<<<<<< HEAD
 +		umem->odp_data->page_list = vzalloc(ib_umem_num_pages(umem) *
 +					    sizeof(*umem->odp_data->page_list));
 +		if (!umem->odp_data->page_list) {
++=======
+ 		umem_odp->page_list =
+ 			vzalloc(array_size(sizeof(*umem_odp->page_list),
+ 					   ib_umem_num_pages(umem)));
+ 		if (!umem_odp->page_list) {
++>>>>>>> 41b4deeaa123 (RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem)
  			ret_val = -ENOMEM;
- 			goto out_odp_data;
+ 			goto out_mm;
  		}
  
++<<<<<<< HEAD
 +		umem->odp_data->dma_list = vzalloc(ib_umem_num_pages(umem) *
 +					  sizeof(*umem->odp_data->dma_list));
 +		if (!umem->odp_data->dma_list) {
++=======
+ 		umem_odp->dma_list =
+ 			vzalloc(array_size(sizeof(*umem_odp->dma_list),
+ 					   ib_umem_num_pages(umem)));
+ 		if (!umem_odp->dma_list) {
++>>>>>>> 41b4deeaa123 (RDMA/umem: Make ib_umem_odp into a sub structure of ib_umem)
  			ret_val = -ENOMEM;
  			goto out_page_list;
  		}
* Unmerged path drivers/infiniband/core/umem.c
* Unmerged path drivers/infiniband/core/umem_odp.c
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index 45dd3865a782..190ccc55b330 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -64,7 +64,7 @@ static int check_parent(struct ib_umem_odp *odp,
 static struct ib_umem_odp *odp_next(struct ib_umem_odp *odp)
 {
 	struct mlx5_ib_mr *mr = odp->private, *parent = mr->parent;
-	struct ib_ucontext *ctx = odp->umem->context;
+	struct ib_ucontext *ctx = odp->umem.context;
 	struct rb_node *rb;
 
 	down_read(&ctx->umem_rwsem);
@@ -102,7 +102,7 @@ static struct ib_umem_odp *odp_lookup(struct ib_ucontext *ctx,
 		if (!rb)
 			goto not_found;
 		odp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);
-		if (ib_umem_start(odp->umem) > start + length)
+		if (ib_umem_start(&odp->umem) > start + length)
 			goto not_found;
 	}
 not_found:
@@ -137,7 +137,7 @@ void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 	for (i = 0; i < nentries; i++, pklm++) {
 		pklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);
 		va = (offset + i) * MLX5_IMR_MTT_SIZE;
-		if (odp && odp->umem->address == va) {
+		if (odp && odp->umem.address == va) {
 			struct mlx5_ib_mr *mtt = odp->private;
 
 			pklm->key = cpu_to_be32(mtt->ibmr.lkey);
@@ -153,13 +153,13 @@ void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 static void mr_leaf_free_action(struct work_struct *work)
 {
 	struct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);
-	int idx = ib_umem_start(odp->umem) >> MLX5_IMR_MTT_SHIFT;
+	int idx = ib_umem_start(&odp->umem) >> MLX5_IMR_MTT_SHIFT;
 	struct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;
 
 	mr->parent = NULL;
 	synchronize_srcu(&mr->dev->mr_srcu);
 
-	ib_umem_release(odp->umem);
+	ib_umem_release(&odp->umem);
 	if (imr->live)
 		mlx5_ib_update_xlt(imr, idx, 1, 0,
 				   MLX5_IB_UPD_XLT_INDIRECT |
@@ -185,7 +185,7 @@ void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 		pr_err("invalidation called on NULL umem or non-ODP umem\n");
 		return;
 	}
-	umem = umem_odp->umem;
+	umem = &umem_odp->umem;
 
 	mr = umem_odp->private;
 
@@ -392,16 +392,16 @@ next_mr:
 			return ERR_CAST(odp);
 		}
 
-		mtt = implicit_mr_alloc(mr->ibmr.pd, odp->umem, 0,
+		mtt = implicit_mr_alloc(mr->ibmr.pd, &odp->umem, 0,
 					mr->access_flags);
 		if (IS_ERR(mtt)) {
 			mutex_unlock(&mr->umem->odp_data->umem_mutex);
-			ib_umem_release(odp->umem);
+			ib_umem_release(&odp->umem);
 			return ERR_CAST(mtt);
 		}
 
 		odp->private = mtt;
-		mtt->umem = odp->umem;
+		mtt->umem = &odp->umem;
 		mtt->mmkey.iova = addr;
 		mtt->parent = mr;
 		INIT_WORK(&odp->work, mr_leaf_free_action);
@@ -418,7 +418,7 @@ next_mr:
 	addr += MLX5_IMR_MTT_SIZE;
 	if (unlikely(addr < io_virt + bcnt)) {
 		odp = odp_next(odp);
-		if (odp && odp->umem->address != addr)
+		if (odp && odp->umem.address != addr)
 			odp = NULL;
 		goto next_mr;
 	}
@@ -465,7 +465,7 @@ static int mr_leaf_free(struct ib_umem_odp *umem_odp, u64 start, u64 end,
 			void *cookie)
 {
 	struct mlx5_ib_mr *mr = umem_odp->private, *imr = cookie;
-	struct ib_umem *umem = umem_odp->umem;
+	struct ib_umem *umem = &umem_odp->umem;
 
 	if (mr->parent != imr)
 		return 0;
@@ -518,7 +518,7 @@ static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
 	}
 
 next_mr:
-	size = min_t(size_t, bcnt, ib_umem_end(odp->umem) - io_virt);
+	size = min_t(size_t, bcnt, ib_umem_end(&odp->umem) - io_virt);
 
 	page_shift = mr->umem->page_shift;
 	page_mask = ~(BIT(page_shift) - 1);
@@ -577,7 +577,7 @@ next_mr:
 
 		io_virt += size;
 		next = odp_next(odp);
-		if (unlikely(!next || next->umem->address != io_virt)) {
+		if (unlikely(!next || next->umem.address != io_virt)) {
 			mlx5_ib_dbg(dev, "next implicit leaf removed at 0x%llx. got %p\n",
 				    io_virt, next);
 			return -EAGAIN;
diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h
index cbe16c505673..7774b42e3ead 100644
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -43,6 +43,7 @@ struct umem_odp_node {
 };
 
 struct ib_umem_odp {
+	struct ib_umem umem;
 	/*
 	 * An array of the pages included in the on-demand paging umem.
 	 * Indices of pages that are currently not mapped into the device will
@@ -72,7 +73,6 @@ struct ib_umem_odp {
 	/* A linked list of umems that don't have private mmu notifier
 	 * counters yet. */
 	struct list_head no_private_counters;
-	struct ib_umem		*umem;
 
 	/* Tree tracking */
 	struct umem_odp_node	interval_tree;
@@ -84,13 +84,12 @@ struct ib_umem_odp {
 
 static inline struct ib_umem_odp *to_ib_umem_odp(struct ib_umem *umem)
 {
-	return umem->odp_data;
+	return container_of(umem, struct ib_umem_odp, umem);
 }
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 
-int ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem,
-		    int access);
+int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
 struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
 				      unsigned long addr, size_t size);
 void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
@@ -156,9 +155,7 @@ static inline int ib_umem_mmu_notifier_retry(struct ib_umem_odp *umem_odp,
 
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
-static inline int ib_umem_odp_get(struct ib_ucontext *context,
-				  struct ib_umem *umem,
-				  int access)
+static inline int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access)
 {
 	return -EINVAL;
 }
