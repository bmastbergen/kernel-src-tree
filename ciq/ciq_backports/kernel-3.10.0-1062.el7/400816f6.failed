perf/x86/intel: Implement support for TSX Force Abort

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Peter Zijlstra (Intel) <peterz@infradead.org>
commit 400816f60c543153656ac74eaf7f36f6b7202378
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/400816f6.failed

Skylake (and later) will receive a microcode update to address a TSX
errata. This microcode will, on execution of a TSX instruction
(speculative or not) use (clobber) PMC3. This update will also provide
a new MSR to change this behaviour along with a CPUID bit to enumerate
the presence of this new MSR.

When the MSR gets set; the microcode will no longer use PMC3 but will
Force Abort every TSX transaction (upon executing COMMIT).

When TSX Force Abort (TFA) is allowed (default); the MSR gets set when
PMC3 gets scheduled and cleared when, after scheduling, PMC3 is
unused.

When TFA is not allowed; clear PMC3 from all constraints such that it
will not get used.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 400816f60c543153656ac74eaf7f36f6b7202378)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/intel/core.c
diff --cc arch/x86/events/intel/core.c
index f0d6acdd6b07,dadb8f7e5a0d..000000000000
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@@ -1936,6 -1999,51 +1936,54 @@@ static void intel_pmu_nhm_enable_all(in
  	intel_pmu_enable_all(added);
  }
  
++<<<<<<< HEAD
++=======
+ static void intel_set_tfa(struct cpu_hw_events *cpuc, bool on)
+ {
+ 	u64 val = on ? MSR_TFA_RTM_FORCE_ABORT : 0;
+ 
+ 	if (cpuc->tfa_shadow != val) {
+ 		cpuc->tfa_shadow = val;
+ 		wrmsrl(MSR_TSX_FORCE_ABORT, val);
+ 	}
+ }
+ 
+ static void intel_tfa_commit_scheduling(struct cpu_hw_events *cpuc, int idx, int cntr)
+ {
+ 	/*
+ 	 * We're going to use PMC3, make sure TFA is set before we touch it.
+ 	 */
+ 	if (cntr == 3 && !cpuc->is_fake)
+ 		intel_set_tfa(cpuc, true);
+ }
+ 
+ static void intel_tfa_pmu_enable_all(int added)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	/*
+ 	 * If we find PMC3 is no longer used when we enable the PMU, we can
+ 	 * clear TFA.
+ 	 */
+ 	if (!test_bit(3, cpuc->active_mask))
+ 		intel_set_tfa(cpuc, false);
+ 
+ 	intel_pmu_enable_all(added);
+ }
+ 
+ static void enable_counter_freeze(void)
+ {
+ 	update_debugctlmsr(get_debugctlmsr() |
+ 			DEBUGCTLMSR_FREEZE_PERFMON_ON_PMI);
+ }
+ 
+ static void disable_counter_freeze(void)
+ {
+ 	update_debugctlmsr(get_debugctlmsr() &
+ 			~DEBUGCTLMSR_FREEZE_PERFMON_ON_PMI);
+ }
+ 
++>>>>>>> 400816f60c54 (perf/x86/intel: Implement support for TSX Force Abort)
  static inline u64 intel_pmu_get_status(void)
  {
  	u64 status;
@@@ -3267,13 -3501,15 +3335,15 @@@ static int intel_pmu_cpu_prepare(int cp
  			goto err;
  	}
  
- 	if (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {
+ 	if (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA)) {
  		size_t sz = X86_PMC_IDX_MAX * sizeof(struct event_constraint);
  
 -		cpuc->constraint_list = kzalloc_node(sz, GFP_KERNEL, cpu_to_node(cpu));
 +		cpuc->constraint_list = kzalloc(sz, GFP_KERNEL);
  		if (!cpuc->constraint_list)
  			goto err_shared_regs;
+ 	}
  
+ 	if (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {
  		cpuc->excl_cntrs = allocate_excl_cntrs(cpu);
  		if (!cpuc->excl_cntrs)
  			goto err_constraint_list;
@@@ -4279,9 -4634,20 +4353,18 @@@ __init int intel_pmu_init(void
  			hsw_format_attr : nhm_format_attr;
  		extra_attr = merge_attr(extra_attr, skl_format_attr);
  		to_free = extra_attr;
 -		x86_pmu.cpu_events = hsw_events_attrs;
 -		mem_attr = hsw_mem_events_attrs;
 -		tsx_attr = hsw_tsx_events_attrs;
 +		x86_pmu.cpu_events = get_hsw_events_attrs();
  		intel_pmu_pebs_data_source_skl(
  			boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X);
+ 
+ 		if (boot_cpu_has(X86_FEATURE_TSX_FORCE_ABORT)) {
+ 			x86_pmu.flags |= PMU_FL_TFA;
+ 			x86_pmu.get_event_constraints = tfa_get_event_constraints;
+ 			x86_pmu.enable_all = intel_tfa_pmu_enable_all;
+ 			x86_pmu.commit_scheduling = intel_tfa_commit_scheduling;
+ 			intel_pmu_attrs[1] = &dev_attr_allow_tsx_force_abort.attr.attr;
+ 		}
+ 
  		pr_cont("Skylake events, ");
  		name = "skylake";
  		break;
* Unmerged path arch/x86/events/intel/core.c
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 35c6a27208b6..78e74d1fc4f4 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -253,6 +253,11 @@ struct cpu_hw_events {
 	struct intel_excl_cntrs		*excl_cntrs;
 	int excl_thread_id; /* 0 or 1 */
 
+	/*
+	 * SKL TSX_FORCE_ABORT shadow
+	 */
+	u64				tfa_shadow;
+
 	/*
 	 * AMD specific bits
 	 */
@@ -682,6 +687,7 @@ do {									\
 #define PMU_FL_EXCL_CNTRS	0x4 /* has exclusive counter requirements  */
 #define PMU_FL_EXCL_ENABLED	0x8 /* exclusive counter active */
 #define PMU_FL_PEBS_ALL		0x10 /* all events are valid PEBS events */
+#define PMU_FL_TFA		0x20 /* deal with TSX force abort */
 
 #define EVENT_VAR(_id)  event_attr_##_id
 #define EVENT_PTR(_id) &event_attr_##_id.attr.attr
