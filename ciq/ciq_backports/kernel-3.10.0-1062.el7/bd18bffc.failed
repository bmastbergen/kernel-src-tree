KVM: nVMX: restore host state in nested_vmx_vmexit for VMFail

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit bd18bffca35397214ae68d85cf7203aca25c3c1d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/bd18bffc.failed

A VMEnter that VMFails (as opposed to VMExits) does not touch host
state beyond registers that are explicitly noted in the VMFail path,
e.g. EFLAGS.  Host state does not need to be loaded because VMFail
is only signaled for consistency checks that occur before the CPU
starts to load guest state, i.e. there is no need to restore any
state as nothing has been modified.  But in the case where a VMFail
is detected by hardware and not by KVM (due to deferring consistency
checks to hardware), KVM has already loaded some amount of guest
state.  Luckily, "loaded" only means loaded to KVM's software model,
i.e. vmcs01 has not been modified.  So, unwind our software model to
the pre-VMEntry host state.

Not restoring host state in this VMFail path leads to a variety of
failures because we end up with stale data in vcpu->arch, e.g. CR0,
CR4, EFER, etc... will all be out of sync relative to vmcs01.  Any
significant delta in the stale data is all but guaranteed to crash
L1, e.g. emulation of SMEP, SMAP, UMIP, WP, etc... will be wrong.

An alternative to this "soft" reload would be to load host state from
vmcs12 as if we triggered a VMExit (as opposed to VMFail), but that is
wildly inconsistent with respect to the VMX architecture, e.g. an L1
VMM with separate VMExit and VMFail paths would explode.

Note that this approach does not mean KVM is 100% accurate with
respect to VMX hardware behavior, even at an architectural level
(the exact order of consistency checks is microarchitecture specific).
But 100% emulation accuracy isn't the goal (with this patch), rather
the goal is to be consistent in the information delivered to L1, e.g.
a VMExit should not fall-through VMENTER, and a VMFail should not jump
to HOST_RIP.

This technically reverts commit "5af4157388ad (KVM: nVMX: Fix mmu
context after VMLAUNCH/VMRESUME failure)", but retains the core
aspects of that patch, just in an open coded form due to the need to
pull state from vmcs01 instead of vmcs12.  Restoring host state
resolves a variety of issues introduced by commit "4f350c6dbcb9
(kvm: nVMX: Handle deferred early VMLAUNCH/VMRESUME failure properly)",
which remedied the incorrect behavior of treating VMFail like VMExit
but in doing so neglected to restore arch state that had been modified
prior to attempting nested VMEnter.

A sample failure that occurs due to stale vcpu.arch state is a fault
of some form while emulating an LGDT (due to emulated UMIP) from L1
after a failed VMEntry to L3, in this case when running the KVM unit
test test_tpr_threshold_values in L1.  L0 also hits a WARN in this
case due to a stale arch.cr4.UMIP.

L1:
  BUG: unable to handle kernel paging request at ffffc90000663b9e
  PGD 276512067 P4D 276512067 PUD 276513067 PMD 274efa067 PTE 8000000271de2163
  Oops: 0009 [#1] SMP
  CPU: 5 PID: 12495 Comm: qemu-system-x86 Tainted: G        W         4.18.0-rc2+ #2
  Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015
  RIP: 0010:native_load_gdt+0x0/0x10

  ...

  Call Trace:
   load_fixmap_gdt+0x22/0x30
   __vmx_load_host_state+0x10e/0x1c0 [kvm_intel]
   vmx_switch_vmcs+0x2d/0x50 [kvm_intel]
   nested_vmx_vmexit+0x222/0x9c0 [kvm_intel]
   vmx_handle_exit+0x246/0x15a0 [kvm_intel]
   kvm_arch_vcpu_ioctl_run+0x850/0x1830 [kvm]
   kvm_vcpu_ioctl+0x3a1/0x5c0 [kvm]
   do_vfs_ioctl+0x9f/0x600
   ksys_ioctl+0x66/0x70
   __x64_sys_ioctl+0x16/0x20
   do_syscall_64+0x4f/0x100
   entry_SYSCALL_64_after_hwframe+0x44/0xa9

L0:
  WARNING: CPU: 2 PID: 3529 at arch/x86/kvm/vmx.c:6618 handle_desc+0x28/0x30 [kvm_intel]
  ...
  CPU: 2 PID: 3529 Comm: qemu-system-x86 Not tainted 4.17.2-coffee+ #76
  Hardware name: Intel Corporation Kabylake Client platform/KBL S
  RIP: 0010:handle_desc+0x28/0x30 [kvm_intel]

  ...

  Call Trace:
   kvm_arch_vcpu_ioctl_run+0x863/0x1840 [kvm]
   kvm_vcpu_ioctl+0x3a1/0x5c0 [kvm]
   do_vfs_ioctl+0x9f/0x5e0
   ksys_ioctl+0x66/0x70
   __x64_sys_ioctl+0x16/0x20
   do_syscall_64+0x49/0xf0
   entry_SYSCALL_64_after_hwframe+0x44/0xa9

Fixes: 5af4157388ad (KVM: nVMX: Fix mmu context after VMLAUNCH/VMRESUME failure)
Fixes: 4f350c6dbcb9 (kvm: nVMX: Handle deferred early VMLAUNCH/VMRESUME failure properly)
	Cc: Jim Mattson <jmattson@google.com>
	Cc: Krish Sadhukhan <krish.sadhukhan@oracle.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
	Cc: Wanpeng Li <wanpeng.li@hotmail.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit bd18bffca35397214ae68d85cf7203aca25c3c1d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 5501a1957de2,8efce8e0a468..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -11225,28 -13197,35 +11225,41 @@@ static void load_vmcs12_host_state(stru
  	vmx_set_cr4(vcpu, vmcs12->host_cr4);
  
  	nested_ept_uninit_mmu_context(vcpu);
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Only PDPTE load can fail as the value of cr3 was checked on entry and
+ 	 * couldn't have changed.
+ 	 */
+ 	if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, &entry_failure_code))
+ 		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+ 
+ 	if (!enable_ept)
+ 		vcpu->arch.walk_mmu->inject_page_fault = kvm_inject_page_fault;
++>>>>>>> bd18bffca353 (KVM: nVMX: restore host state in nested_vmx_vmexit for VMFail)
  
  	/*
 -	 * If vmcs01 doesn't use VPID, CPU flushes TLB on every
 -	 * VMEntry/VMExit. Thus, no need to flush TLB.
 -	 *
 -	 * If vmcs12 doesn't use VPID, L1 expects TLB to be
 -	 * flushed on every VMEntry/VMExit.
 -	 *
 -	 * Otherwise, we can preserve TLB entries as long as we are
 -	 * able to tag L1 TLB entries differently than L2 TLB entries.
 -	 *
 -	 * If vmcs12 uses EPT, we need to execute this flush on EPTP01
 -	 * and therefore we request the TLB flush to happen only after VMCS EPTP
 -	 * has been set by KVM_REQ_LOAD_CR3.
 +	 * Only PDPTE load can fail as the value of cr3 was checked on entry and
 +	 * couldn't have changed.
  	 */
 -	if (enable_vpid &&
 -	    (!nested_cpu_has_vpid(vmcs12) || !nested_has_guest_tlb_tag(vcpu))) {
 -		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +	if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, &entry_failure_code))
 +		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
 +
 +	if (!enable_ept)
 +		vcpu->arch.walk_mmu->inject_page_fault = kvm_inject_page_fault;
 +
 +	if (enable_vpid) {
 +		/*
 +		 * Trivially support vpid by letting L2s share their parent
 +		 * L1's vpid. TODO: move to a more elaborate solution, giving
 +		 * each L2 its own vpid and exposing the vpid feature to L1.
 +		 */
 +		vmx_flush_tlb(vcpu);
  	}
 +	/* Restore posted intr vector. */
 +	if (nested_cpu_has_posted_intr(vmcs12))
 +		vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
  
  	vmcs_write32(GUEST_SYSENTER_CS, vmcs12->host_ia32_sysenter_cs);
  	vmcs_writel(GUEST_SYSENTER_ESP, vmcs12->host_ia32_sysenter_esp);
@@@ -11440,6 -13541,62 +11587,65 @@@ static void nested_vmx_vmexit(struct kv
  
  	/* in case we halted in L2 */
  	vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
++<<<<<<< HEAD
++=======
+ 
+ 	if (likely(!vmx->fail)) {
+ 		/*
+ 		 * TODO: SDM says that with acknowledge interrupt on
+ 		 * exit, bit 31 of the VM-exit interrupt information
+ 		 * (valid interrupt) is always set to 1 on
+ 		 * EXIT_REASON_EXTERNAL_INTERRUPT, so we shouldn't
+ 		 * need kvm_cpu_has_interrupt().  See the commit
+ 		 * message for details.
+ 		 */
+ 		if (nested_exit_intr_ack_set(vcpu) &&
+ 		    exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&
+ 		    kvm_cpu_has_interrupt(vcpu)) {
+ 			int irq = kvm_cpu_get_interrupt(vcpu);
+ 			WARN_ON(irq < 0);
+ 			vmcs12->vm_exit_intr_info = irq |
+ 				INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;
+ 		}
+ 
+ 		if (exit_reason != -1)
+ 			trace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,
+ 						       vmcs12->exit_qualification,
+ 						       vmcs12->idt_vectoring_info_field,
+ 						       vmcs12->vm_exit_intr_info,
+ 						       vmcs12->vm_exit_intr_error_code,
+ 						       KVM_ISA_VMX);
+ 
+ 		load_vmcs12_host_state(vcpu, vmcs12);
+ 
+ 		return;
+ 	}
+ 	
+ 	/*
+ 	 * After an early L2 VM-entry failure, we're now back
+ 	 * in L1 which thinks it just finished a VMLAUNCH or
+ 	 * VMRESUME instruction, so we need to set the failure
+ 	 * flag and the VM-instruction error field of the VMCS
+ 	 * accordingly.
+ 	 */
+ 	nested_vmx_failValid(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+ 
+ 	/*
+ 	 * Restore L1's host state to KVM's software model.  We're here
+ 	 * because a consistency check was caught by hardware, which
+ 	 * means some amount of guest state has been propagated to KVM's
+ 	 * model and needs to be unwound to the host's state.
+ 	 */
+ 	nested_vmx_restore_host_state(vcpu);
+ 
+ 	/*
+ 	 * The emulated instruction was already skipped in
+ 	 * nested_vmx_run, but the updated RIP was never
+ 	 * written back to the vmcs01.
+ 	 */
+ 	skip_emulated_instruction(vcpu);
+ 	vmx->fail = 0;
++>>>>>>> bd18bffca353 (KVM: nVMX: restore host state in nested_vmx_vmexit for VMFail)
  }
  
  /*
* Unmerged path arch/x86/kvm/vmx.c
