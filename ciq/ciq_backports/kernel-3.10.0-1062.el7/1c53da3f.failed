kvm: x86: Support resetting the MMU context without resetting roots

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Junaid Shahid <junaids@google.com>
commit 1c53da3fa3a333eb15ee5a154700e75d135c21c8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/1c53da3f.failed

This adds support for re-initializing the MMU context in a different
mode while preserving the active root_hpa and the prev_root.

	Signed-off-by: Junaid Shahid <junaids@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 1c53da3fa3a333eb15ee5a154700e75d135c21c8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index db86a346eaea,3d822a97dfa1..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3909,7 -4032,6 +3909,10 @@@ static void nonpaging_init_context(stru
  	context->update_pte = nonpaging_update_pte;
  	context->root_level = 0;
  	context->shadow_root_level = PT32E_ROOT_LEVEL;
++<<<<<<< HEAD
 +	context->root_hpa = INVALID_PAGE;
++=======
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  	context->direct_map = true;
  	context->nx = false;
  }
@@@ -4383,7 -4566,6 +4386,10 @@@ static void paging64_init_context_commo
  	context->invlpg = paging64_invlpg;
  	context->update_pte = paging64_update_pte;
  	context->shadow_root_level = level;
++<<<<<<< HEAD
 +	context->root_hpa = INVALID_PAGE;
++=======
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  	context->direct_map = false;
  }
  
@@@ -4410,7 -4595,6 +4416,10 @@@ static void paging32_init_context(struc
  	context->invlpg = paging32_invlpg;
  	context->update_pte = paging32_update_pte;
  	context->shadow_root_level = PT32E_ROOT_LEVEL;
++<<<<<<< HEAD
 +	context->root_hpa = INVALID_PAGE;
++=======
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  	context->direct_map = false;
  }
  
@@@ -4432,8 -4629,7 +4441,12 @@@ static void init_kvm_tdp_mmu(struct kvm
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
  	context->update_pte = nonpaging_update_pte;
++<<<<<<< HEAD
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
 +	context->root_hpa = INVALID_PAGE;
++=======
+ 	context->shadow_root_level = kvm_x86_ops->get_tdp_level(vcpu);
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  	context->direct_map = true;
  	context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
  	context->get_cr3 = get_cr3;
@@@ -4467,14 -4664,37 +4480,12 @@@
  	reset_tdp_shadow_zero_bits_mask(vcpu, context);
  }
  
 -static union kvm_mmu_page_role
 -kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu)
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
  {
 -	union kvm_mmu_page_role role = {0};
  	bool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
  	bool smap = kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
 -
 -	role.nxe = is_nx(vcpu);
 -	role.cr4_pae = !!is_pae(vcpu);
 -	role.cr0_wp  = is_write_protection(vcpu);
 -	role.smep_andnot_wp = smep && !is_write_protection(vcpu);
 -	role.smap_andnot_wp = smap && !is_write_protection(vcpu);
 -	role.guest_mode = is_guest_mode(vcpu);
 -	role.smm = is_smm(vcpu);
 -	role.direct = !is_paging(vcpu);
 -	role.access = ACC_ALL;
 -
 -	if (!is_long_mode(vcpu))
 -		role.level = PT32E_ROOT_LEVEL;
 -	else if (is_la57_mode(vcpu))
 -		role.level = PT64_ROOT_5LEVEL;
 -	else
 -		role.level = PT64_ROOT_4LEVEL;
 -
 -	return role;
 -}
 -
 -void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 -{
  	struct kvm_mmu *context = &vcpu->arch.mmu;
  
- 	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
- 
  	if (!is_paging(vcpu))
  		nonpaging_init_context(vcpu, context);
  	else if (is_long_mode(vcpu))
@@@ -4501,10 -4728,10 +4512,14 @@@ void kvm_init_shadow_ept_mmu(struct kvm
  			     bool accessed_dirty)
  {
  	struct kvm_mmu *context = &vcpu->arch.mmu;
 -	union kvm_mmu_page_role root_page_role =
 -		kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty);
  
++<<<<<<< HEAD
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
 +
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
++=======
+ 	context->shadow_root_level = PT64_ROOT_4LEVEL;
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  
  	context->nx = true;
  	context->ept_ad = accessed_dirty;
@@@ -4513,11 -4740,9 +4528,15 @@@
  	context->sync_page = ept_sync_page;
  	context->invlpg = ept_invlpg;
  	context->update_pte = ept_update_pte;
++<<<<<<< HEAD
 +	context->root_level = context->shadow_root_level;
 +	context->root_hpa = INVALID_PAGE;
++=======
+ 	context->root_level = PT64_ROOT_4LEVEL;
++>>>>>>> 1c53da3fa3a3 (kvm: x86: Support resetting the MMU context without resetting roots)
  	context->direct_map = false;
 -	context->base_role.word = root_page_role.word & mmu_base_role_mask.word;
 +	context->base_role.ad_disabled = !accessed_dirty;
 +	context->base_role.guest_mode = 1;
  	update_permission_bitmask(vcpu, context, true);
  	update_pkru_bitmask(vcpu, context, true);
  	update_last_nonleaf_level(vcpu, context);
@@@ -4586,7 -4819,17 +4610,8 @@@ void kvm_init_mmu(struct kvm_vcpu *vcpu
  	else
  		init_kvm_softmmu(vcpu);
  }
+ EXPORT_SYMBOL_GPL(kvm_init_mmu);
  
 -static union kvm_mmu_page_role
 -kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
 -{
 -	if (tdp_enabled)
 -		return kvm_calc_tdp_mmu_root_page_role(vcpu);
 -	else
 -		return kvm_calc_shadow_mmu_root_page_role(vcpu);
 -}
 -
  void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
  {
  	kvm_mmu_unload(vcpu);
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 3d1af6f75377..e6f58d4f750b 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -56,6 +56,7 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value);
 void
 reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
 
+void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots);
 void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu);
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     bool accessed_dirty);
