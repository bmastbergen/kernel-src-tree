bpf, s390: fix potential memleak when later bpf_jit_prog fails

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit f605ce5eb26ac934fb8106d75d46a2c875a2bf23
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f605ce5e.failed

If we would ever fail in the bpf_jit_prog() pass that writes the
actual insns to the image after we got header via bpf_jit_binary_alloc()
then we also need to make sure to free it through bpf_jit_binary_free()
again when bailing out. Given we had prior bpf_jit_prog() passes to
initially probe for clobbered registers, program size and to fill in
addrs arrray for jump targets, this is more of a theoretical one,
but at least make sure this doesn't break with future changes.

Fixes: 054623105728 ("s390/bpf: Add s390x eBPF JIT compiler backend")
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit f605ce5eb26ac934fb8106d75d46a2c875a2bf23)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/net/bpf_jit_comp.c
diff --cc arch/s390/net/bpf_jit_comp.c
index 15be4f92e665,5f0234ec8038..000000000000
--- a/arch/s390/net/bpf_jit_comp.c
+++ b/arch/s390/net/bpf_jit_comp.c
@@@ -698,187 -1076,233 +698,207 @@@ call_fn:	/* lg %r1,<d(function)>(%r13) 
  		/* j <exit> */
  		EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
  		break;
 -	/*
 -	 * Branch relative (number of skipped instructions) to offset on
 -	 * condition.
 -	 *
 -	 * Condition code to mask mapping:
 -	 *
 -	 * CC | Description	   | Mask
 -	 * ------------------------------
 -	 * 0  | Operands equal	   |	8
 -	 * 1  | First operand low  |	4
 -	 * 2  | First operand high |	2
 -	 * 3  | Unused		   |	1
 -	 *
 -	 * For s390x relative branches: ip = ip + off_bytes
 -	 * For BPF relative branches:	insn = insn + off_insns + 1
 -	 *
 -	 * For example for s390x with offset 0 we jump to the branch
 -	 * instruction itself (loop) and for BPF with offset 0 we
 -	 * branch to the instruction behind the branch.
 -	 */
 -	case BPF_JMP | BPF_JA: /* if (true) */
 -		mask = 0xf000; /* j */
 -		goto branch_oc;
 -	case BPF_JMP | BPF_JSGT | BPF_K: /* ((s64) dst > (s64) imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSLT | BPF_K: /* ((s64) dst < (s64) imm) */
 -		mask = 0x4000; /* jl */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSGE | BPF_K: /* ((s64) dst >= (s64) imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSLE | BPF_K: /* ((s64) dst <= (s64) imm) */
 -		mask = 0xc000; /* jle */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JGT | BPF_K: /* (dst_reg > imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JLT | BPF_K: /* (dst_reg < imm) */
 -		mask = 0x4000; /* jl */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JGE | BPF_K: /* (dst_reg >= imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JLE | BPF_K: /* (dst_reg <= imm) */
 -		mask = 0xc000; /* jle */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JNE | BPF_K: /* (dst_reg != imm) */
 -		mask = 0x7000; /* jne */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JEQ | BPF_K: /* (dst_reg == imm) */
 -		mask = 0x8000; /* je */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JSET | BPF_K: /* (dst_reg & imm) */
 -		mask = 0x7000; /* jnz */
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* ngr %w1,%dst */
 -		EMIT4(0xb9800000, REG_W1, dst_reg);
 -		goto branch_oc;
 -
 -	case BPF_JMP | BPF_JSGT | BPF_X: /* ((s64) dst > (s64) src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSLT | BPF_X: /* ((s64) dst < (s64) src) */
 -		mask = 0x4000; /* jl */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSGE | BPF_X: /* ((s64) dst >= (s64) src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSLE | BPF_X: /* ((s64) dst <= (s64) src) */
 -		mask = 0xc000; /* jle */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JGT | BPF_X: /* (dst > src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JLT | BPF_X: /* (dst < src) */
 -		mask = 0x4000; /* jl */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JGE | BPF_X: /* (dst >= src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JLE | BPF_X: /* (dst <= src) */
 -		mask = 0xc000; /* jle */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JNE | BPF_X: /* (dst != src) */
 -		mask = 0x7000; /* jne */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JEQ | BPF_X: /* (dst == src) */
 -		mask = 0x8000; /* je */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JSET | BPF_X: /* (dst & src) */
 -		mask = 0x7000; /* jnz */
 -		/* ngrk %w1,%dst,%src */
 -		EMIT4_RRF(0xb9e40000, REG_W1, dst_reg, src_reg);
 -		goto branch_oc;
 -branch_ks:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* cgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_ku:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* clgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_xs:
 -		/* cgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_xu:
 -		/* clgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, src_reg, i, off, mask);
 +	case BPF_S_ST: /* mem[K] = A */
 +		jit->seen |= SEEN_MEM;
 +		/* st %r5,<K>(%r15) */
 +		EMIT4_DISP(0x5050f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_STX: /* mem[K] = X : mov %ebx,off8(%rbp) */
 +		jit->seen |= SEEN_XREG | SEEN_MEM;
 +		/* st %r12,<K>(%r15) */
 +		EMIT4_DISP(0x50c0f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_ANC_PROTOCOL: /* A = ntohs(skb->protocol); */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(protocol)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, protocol));
 +		break;
 +	case BPF_S_ANC_IFINDEX:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->ifindex */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* l %r5,<d(ifindex)>(%r1) */
 +		EMIT4_DISP(0x58501000, offsetof(struct net_device, ifindex));
 +		break;
 +	case BPF_S_ANC_MARK: /* A = skb->mark */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +		/* l %r5,<d(mark)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, mark));
 +		break;
 +	case BPF_S_ANC_QUEUE: /* A = skb->queue_mapping */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(queue_mapping)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, queue_mapping));
 +		break;
 +	case BPF_S_ANC_HATYPE:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->type */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(type)>(%r1) */
 +		EMIT4_DISP(0xbf531000, offsetof(struct net_device, type));
 +		break;
 +	case BPF_S_ANC_RXHASH: /* A = skb->hash */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +		/* l %r5,<d(hash)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, hash));
 +		break;
 +	case BPF_S_ANC_VLAN_TAG:
 +	case BPF_S_ANC_VLAN_TAG_PRESENT:
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(vlan_tci)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, vlan_tci));
 +		if (filter->code == BPF_S_ANC_VLAN_TAG) {
 +			/* nill %r5,0xefff */
 +			EMIT4_IMM(0xa5570000, ~VLAN_TAG_PRESENT);
 +		} else {
 +			/* nill %r5,0x1000 */
 +			EMIT4_IMM(0xa5570000, VLAN_TAG_PRESENT);
 +			/* srl %r5,12 */
 +			EMIT4_DISP(0x88500000, 12);
 +		}
  		break;
 -branch_oc:
 -		/* brc mask,jmp_off (branch instruction needs 4 bytes) */
 -		jmp_off = addrs[i + off + 1] - (addrs[i + 1] - 4);
 -		EMIT4_PCREL(0xa7040000 | mask << 8, jmp_off);
 +	case BPF_S_ANC_CPU: /* A = smp_processor_id() */
 +#ifdef CONFIG_SMP
 +		/* l %r5,<d(cpu_nr)> */
 +		EMIT4_DISP(0x58500000, offsetof(struct _lowcore, cpu_nr));
 +#else
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +#endif
  		break;
  	default: /* too complex, give up */
 -		pr_err("Unknown opcode %02x\n", insn->code);
 -		return -1;
 -	}
 -	return insn_count;
 -}
 -
 -/*
 - * Compile eBPF program into s390x code
 - */
 -static int bpf_jit_prog(struct bpf_jit *jit, struct bpf_prog *fp)
 -{
 -	int i, insn_count;
 -
 -	jit->lit = jit->lit_start;
 -	jit->prg = 0;
 -
 -	bpf_jit_prologue(jit, fp->aux->stack_depth);
 -	for (i = 0; i < fp->len; i += insn_count) {
 -		insn_count = bpf_jit_insn(jit, fp, i);
 -		if (insn_count < 0)
 -			return -1;
 -		/* Next instruction address */
 -		jit->addrs[i + insn_count] = jit->prg;
 +		goto out;
  	}
 -	bpf_jit_epilogue(jit, fp->aux->stack_depth);
 -
 -	jit->lit_start = jit->prg;
 -	jit->size = jit->lit;
 -	jit->size_prg = jit->prg;
 +	addrs[i] = jit->prg - jit->start;
  	return 0;
 +out:
 +	return -1;
  }
  
 -/*
 - * Compile eBPF program "fp"
 - */
 -struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *fp)
 +void bpf_jit_compile(struct sk_filter *fp)
  {
 -	struct bpf_prog *tmp, *orig_fp = fp;
 -	struct bpf_binary_header *header;
 -	bool tmp_blinded = false;
 -	struct bpf_jit jit;
 -	int pass;
 -
 -	if (!fp->jit_requested)
 -		return orig_fp;
 -
 -	tmp = bpf_jit_blind_constants(fp);
 -	/*
 -	 * If blinding was requested and we failed during blinding,
 -	 * we must fall back to the interpreter.
 -	 */
 -	if (IS_ERR(tmp))
 -		return orig_fp;
 -	if (tmp != fp) {
 -		tmp_blinded = true;
 -		fp = tmp;
 -	}
 -
 -	memset(&jit, 0, sizeof(jit));
 -	jit.addrs = kcalloc(fp->len + 1, sizeof(*jit.addrs), GFP_KERNEL);
 -	if (jit.addrs == NULL) {
 -		fp = orig_fp;
 -		goto out;
 -	}
 -	/*
 -	 * Three initial passes:
 -	 *   - 1/2: Determine clobbered registers
 -	 *   - 3:   Calculate program size and addrs arrray
 -	 */
 -	for (pass = 1; pass <= 3; pass++) {
 -		if (bpf_jit_prog(&jit, fp)) {
 -			fp = orig_fp;
 -			goto free_addrs;
 +	unsigned long size, prg_len, lit_len;
 +	struct bpf_jit jit, cjit;
 +	unsigned int *addrs;
 +	int pass, i;
 +
 +	if (!bpf_jit_enable)
 +		return;
 +	addrs = kcalloc(fp->len, sizeof(*addrs), GFP_KERNEL);
 +	if (addrs == NULL)
 +		return;
 +	memset(&jit, 0, sizeof(cjit));
 +	memset(&cjit, 0, sizeof(cjit));
 +
 +	for (pass = 0; pass < 10; pass++) {
 +		jit.prg = jit.start;
 +		jit.lit = jit.mid;
 +
 +		bpf_jit_prologue(&jit);
 +		bpf_jit_noleaks(&jit, fp->insns);
 +		for (i = 0; i < fp->len; i++) {
 +			if (bpf_jit_insn(&jit, fp->insns + i, addrs, i,
 +					 i == fp->len - 1))
 +				goto out;
 +		}
++<<<<<<< HEAD
 +		bpf_jit_epilogue(&jit);
 +		if (jit.start) {
 +			WARN_ON(jit.prg > cjit.prg || jit.lit > cjit.lit);
 +			if (memcmp(&jit, &cjit, sizeof(jit)) == 0)
 +				break;
 +		} else if (jit.prg == cjit.prg && jit.lit == cjit.lit) {
 +			prg_len = jit.prg - jit.start;
 +			lit_len = jit.lit - jit.mid;
 +			size = max_t(unsigned long, prg_len + lit_len,
 +				     sizeof(struct work_struct));
 +			if (size >= BPF_SIZE_MAX)
 +				goto out;
 +			jit.start = module_alloc(size);
 +			if (!jit.start)
 +				goto out;
 +			jit.prg = jit.mid = jit.start + prg_len;
 +			jit.lit = jit.end = jit.start + prg_len + lit_len;
 +			jit.base_ip += (unsigned long) jit.start;
 +			jit.exit_ip += (unsigned long) jit.start;
 +			jit.ret0_ip += (unsigned long) jit.start;
  		}
 +		cjit = jit;
++=======
+ 	}
+ 	/*
+ 	 * Final pass: Allocate and generate program
+ 	 */
+ 	if (jit.size >= BPF_SIZE_MAX) {
+ 		fp = orig_fp;
+ 		goto free_addrs;
+ 	}
+ 	header = bpf_jit_binary_alloc(jit.size, &jit.prg_buf, 2, jit_fill_hole);
+ 	if (!header) {
+ 		fp = orig_fp;
+ 		goto free_addrs;
+ 	}
+ 	if (bpf_jit_prog(&jit, fp)) {
+ 		bpf_jit_binary_free(header);
+ 		fp = orig_fp;
+ 		goto free_addrs;
++>>>>>>> f605ce5eb26a (bpf, s390: fix potential memleak when later bpf_jit_prog fails)
  	}
  	if (bpf_jit_enable > 1) {
 -		bpf_jit_dump(fp->len, jit.size, pass, jit.prg_buf);
 -		print_fn_code(jit.prg_buf, jit.size_prg);
 +		pr_err("flen=%d proglen=%lu pass=%d image=%p\n",
 +		       fp->len, jit.end - jit.start, pass, jit.start);
 +		if (jit.start) {
 +			printk(KERN_ERR "JIT code:\n");
 +			print_fn_code(jit.start, jit.mid - jit.start);
 +			print_hex_dump(KERN_ERR, "JIT literals:\n",
 +				       DUMP_PREFIX_ADDRESS, 16, 1,
 +				       jit.mid, jit.end - jit.mid, false);
 +		}
  	}
 -	bpf_jit_binary_lock_ro(header);
 -	fp->bpf_func = (void *) jit.prg_buf;
 -	fp->jited = 1;
 -	fp->jited_len = jit.size;
 -free_addrs:
 -	kfree(jit.addrs);
 +	if (jit.start)
 +		fp->bpf_func = (void *) jit.start;
  out:
 -	if (tmp_blinded)
 -		bpf_jit_prog_release_other(fp, fp == orig_fp ?
 -					   tmp : orig_fp);
 -	return fp;
 +	kfree(addrs);
 +}
 +
 +static void jit_free_defer(struct work_struct *arg)
 +{
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	struct work_struct *work;
 +
 +	if (fp->bpf_func == sk_run_filter)
 +		return;
 +	work = (struct work_struct *)fp->bpf_func;
 +	INIT_WORK(work, jit_free_defer);
 +	schedule_work(work);
  }
* Unmerged path arch/s390/net/bpf_jit_comp.c
