x86/speculation: Move arch_smt_update() call to after mitigation decisions

jira LE-1907
cve CVE-2019-11091
cve CVE-2018-12130
cve CVE-2018-12127
cve CVE-2018-12126
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] speculation: Move arch_smt_update() call to after mitigation decisions (Waiman Long) [1709296 1690358 1690348 1690335] {CVE-2018-12126 CVE-2018-12127 CVE-2018-12130 CVE-2019-11091}
Rebuild_FUZZ: 97.22%
commit-author Josh Poimboeuf <jpoimboe@redhat.com>
commit 7c3658b20194a5b3209a143f63bc9c643c6a3ae2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/7c3658b2.failed

arch_smt_update() now has a dependency on both Spectre v2 and MDS
mitigations.  Move its initial call to after all the mitigation decisions
have been made.

	Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Tyler Hicks <tyhicks@canonical.com>
	Acked-by: Jiri Kosina <jkosina@suse.cz>

(cherry picked from commit 7c3658b20194a5b3209a143f63bc9c643c6a3ae2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/bugs.c
diff --cc arch/x86/kernel/cpu/bugs.c
index 6ec1d2da76d1,3f934ffef8cf..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -65,6 -109,10 +65,8 @@@ void __init check_bugs(void
  
  	l1tf_select_mitigation();
  
 -	mds_select_mitigation();
 -
+ 	arch_smt_update();
+ 
  #ifdef CONFIG_X86_32
  	/*
  	 * Check whether we are able to run this kernel safely on SMP.
@@@ -199,52 -565,165 +201,128 @@@ void __spectre_v2_select_mitigation(voi
  	case SPECTRE_V2_CMD_FORCE:
  	case SPECTRE_V2_CMD_AUTO:
  		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
 -			mode = SPECTRE_V2_IBRS_ENHANCED;
 -			/* Force it so VMEXIT will restore correctly */
 -			x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
 -			wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -			goto specv2_set_mode;
 +			spec_ctrl_enable_ibrs_enhanced();
 +			return;
  		}
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
 -		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_AMD:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_amd;
 -		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_generic;
  		break;
 +
  	case SPECTRE_V2_CMD_RETPOLINE:
++<<<<<<< HEAD
 +		spec_ctrl_enable_retpoline();
++=======
+ 		if (IS_ENABLED(CONFIG_RETPOLINE))
+ 			goto retpoline_auto;
+ 		break;
+ 	}
+ 	pr_err("Spectre mitigation: kernel not compiled with retpoline; no mitigation available!");
+ 	return;
+ 
+ retpoline_auto:
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||
+ 	    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) {
+ 	retpoline_amd:
+ 		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {
+ 			pr_err("Spectre mitigation: LFENCE not serializing, switching to generic retpoline\n");
+ 			goto retpoline_generic;
+ 		}
+ 		mode = SPECTRE_V2_RETPOLINE_AMD;
+ 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);
+ 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ 	} else {
+ 	retpoline_generic:
+ 		mode = SPECTRE_V2_RETPOLINE_GENERIC;
+ 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ 	}
+ 
+ specv2_set_mode:
+ 	spectre_v2_enabled = mode;
+ 	pr_info("%s\n", spectre_v2_strings[mode]);
+ 
+ 	/*
+ 	 * If spectre v2 protection has been enabled, unconditionally fill
+ 	 * RSB during a context switch; this protects against two independent
+ 	 * issues:
+ 	 *
+ 	 *	- RSB underflow (and switch to BTB) on Skylake+
+ 	 *	- SpectreRSB variant of spectre v2 on X86_BUG_SPECTRE_V2 CPUs
+ 	 */
+ 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+ 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
+ 
+ 	/*
+ 	 * Retpoline means the kernel is safe because it has no indirect
+ 	 * branches. Enhanced IBRS protects firmware too, so, enable restricted
+ 	 * speculation around firmware calls only when Enhanced IBRS isn't
+ 	 * supported.
+ 	 *
+ 	 * Use "mode" to check Enhanced IBRS instead of boot_cpu_has(), because
+ 	 * the user might select retpoline on the kernel command line and if
+ 	 * the CPU supports Enhanced IBRS, kernel might un-intentionally not
+ 	 * enable IBRS around firmware calls.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_IBRS) && mode != SPECTRE_V2_IBRS_ENHANCED) {
+ 		setup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);
+ 		pr_info("Enabling Restricted Speculation for firmware calls\n");
+ 	}
+ 
+ 	/* Set up IBPB and STIBP depending on the general spectre V2 command */
+ 	spectre_v2_user_select_mitigation(cmd);
+ }
+ 
+ static void update_stibp_msr(void * __unused)
+ {
+ 	wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ }
+ 
+ /* Update x86_spec_ctrl_base in case SMT state changed. */
+ static void update_stibp_strict(void)
+ {
+ 	u64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;
+ 
+ 	if (sched_smt_active())
+ 		mask |= SPEC_CTRL_STIBP;
+ 
+ 	if (mask == x86_spec_ctrl_base)
++>>>>>>> 7c3658b20194 (x86/speculation: Move arch_smt_update() call to after mitigation decisions)
  		return;
  
 -	pr_info("Update user space SMT mitigation: STIBP %s\n",
 -		mask & SPEC_CTRL_STIBP ? "always-on" : "off");
 -	x86_spec_ctrl_base = mask;
 -	on_each_cpu(update_stibp_msr, NULL, 1);
 -}
 +	case SPECTRE_V2_CMD_IBRS:
 +		if (spec_ctrl_force_enable_ibrs())
 +			return;
 +		break;
  
 -/* Update the static key controlling the evaluation of TIF_SPEC_IB */
 -static void update_indir_branch_cond(void)
 -{
 -	if (sched_smt_active())
 -		static_branch_enable(&switch_to_cond_stibp);
 -	else
 -		static_branch_disable(&switch_to_cond_stibp);
 -}
 +	case SPECTRE_V2_CMD_IBRS_ALWAYS:
 +		if (spec_ctrl_enable_ibrs_always() ||
 +		    spec_ctrl_force_enable_ibp_disabled())
 +			return;
 +		break;
  
 -/* Update the static key controlling the MDS CPU buffer clear in idle */
 -static void update_mds_branch_idle(void)
 -{
 -	/*
 -	 * Enable the idle clearing if SMT is active on CPUs which are
 -	 * affected only by MSBDS and not any other MDS variant.
 -	 *
 -	 * The other variants cannot be mitigated when SMT is enabled, so
 -	 * clearing the buffers on idle just to prevent the Store Buffer
 -	 * repartitioning leak would be a window dressing exercise.
 -	 */
 -	if (!boot_cpu_has_bug(X86_BUG_MSBDS_ONLY))
 -		return;
 +	case SPECTRE_V2_CMD_RETPOLINE_IBRS_USER:
 +		if (spec_ctrl_enable_retpoline_ibrs_user())
 +			return;
 +		break;
 +	}
  
 -	if (sched_smt_active())
 -		static_branch_enable(&mds_idle_clear);
 -	else
 -		static_branch_disable(&mds_idle_clear);
 -}
 +	if (spec_ctrl_cond_enable_ibrs(full_retpoline))
 +		return;
  
 -void arch_smt_update(void)
 -{
 -	/* Enhanced IBRS implies STIBP. No update required. */
 -	if (spectre_v2_enabled == SPECTRE_V2_IBRS_ENHANCED)
 +	if (spec_ctrl_cond_enable_ibp_disabled())
  		return;
  
 -	mutex_lock(&spec_ctrl_mutex);
 +	spec_ctrl_enable_retpoline();
 +}
  
 -	switch (spectre_v2_user) {
 -	case SPECTRE_V2_USER_NONE:
 -		break;
 -	case SPECTRE_V2_USER_STRICT:
 -	case SPECTRE_V2_USER_STRICT_PREFERRED:
 -		update_stibp_strict();
 -		break;
 -	case SPECTRE_V2_USER_PRCTL:
 -	case SPECTRE_V2_USER_SECCOMP:
 -		update_indir_branch_cond();
 -		break;
 -	}
 +void spectre_v2_print_mitigation(void)
 +{
  
 -	switch (mds_mitigation) {
 -	case MDS_MITIGATION_FULL:
 -	case MDS_MITIGATION_VMWERV:
 -		update_mds_branch_idle();
 -		break;
 -	case MDS_MITIGATION_OFF:
 -		break;
 -	}
 +	pr_info("%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
 +}
  
 -	mutex_unlock(&spec_ctrl_mutex);
 +static void __init spectre_v2_select_mitigation(void)
 +{
 +	spectre_v2_cmd = spectre_v2_parse_cmdline();
 +	__spectre_v2_select_mitigation();
 +	spectre_v2_print_mitigation();
  }
  
  #undef pr_fmt
* Unmerged path arch/x86/kernel/cpu/bugs.c
