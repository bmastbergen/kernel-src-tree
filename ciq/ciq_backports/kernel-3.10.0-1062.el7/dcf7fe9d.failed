userfaultfd: shmem: UFFDIO_COPY: set the page dirty if VM_WRITE is not set

jira LE-1907
cve CVE-2018-18397
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit dcf7fe9d89763a28e0f43975b422ff141fe79e43
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/dcf7fe9d.failed

Set the page dirty if VM_WRITE is not set because in such case the pte
won't be marked dirty and the page would be reclaimed without writepage
(i.e.  swapout in the shmem case).

This was found by source review.  Most apps (certainly including QEMU)
only use UFFDIO_COPY on PROT_READ|PROT_WRITE mappings or the app can't
modify the memory in the first place.  This is for correctness and it
could help the non cooperative use case to avoid unexpected data loss.

Link: http://lkml.kernel.org/r/20181126173452.26955-6-aarcange@redhat.com
	Reviewed-by: Hugh Dickins <hughd@google.com>
	Cc: stable@vger.kernel.org
Fixes: 4c27fe4c4c84 ("userfaultfd: shmem: add shmem_mcopy_atomic_pte for userfaultfd support")
	Reported-by: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
	Cc: Jann Horn <jannh@google.com>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Peter Xu <peterx@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit dcf7fe9d89763a28e0f43975b422ff141fe79e43)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/shmem.c
diff --cc mm/shmem.c
index 5d44b78288a6,16a3d7044c52..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -1628,9 -2272,27 +1628,19 @@@ static int shmem_mfill_atomic_pte(struc
  	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
  	if (dst_vma->vm_flags & VM_WRITE)
  		_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));
+ 	else {
+ 		/*
+ 		 * We don't set the pte dirty if the vma has no
+ 		 * VM_WRITE permission, so mark the page dirty or it
+ 		 * could be freed from under us. We could do it
+ 		 * unconditionally before unlock_page(), but doing it
+ 		 * only if VM_WRITE is not set is faster.
+ 		 */
+ 		set_page_dirty(page);
+ 	}
  
 -	dst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);
 -
 -	ret = -EFAULT;
 -	max_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 -	if (unlikely(offset >= max_off))
 -		goto out_release_uncharge_unlock;
 -
  	ret = -EEXIST;
 +	dst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);
  	if (!pte_none(*dst_pte))
  		goto out_release_uncharge_unlock;
  
@@@ -1655,8 -2317,10 +1665,13 @@@ out
  	return ret;
  out_release_uncharge_unlock:
  	pte_unmap_unlock(dst_pte, ptl);
++<<<<<<< HEAD
++=======
+ 	ClearPageDirty(page);
+ 	delete_from_page_cache(page);
++>>>>>>> dcf7fe9d8976 (userfaultfd: shmem: UFFDIO_COPY: set the page dirty if VM_WRITE is not set)
  out_release_uncharge:
 -	mem_cgroup_cancel_charge(page, memcg, false);
 +	mem_cgroup_uncharge_cache_page(page);
  out_release:
  	unlock_page(page);
  	put_page(page);
* Unmerged path mm/shmem.c
