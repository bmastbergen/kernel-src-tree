svcrdma: Persistently allocate and DMA-map Receive buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 3316f0631139c87631f2652c118da1a0354bd40d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/3316f063.failed

The current Receive path uses an array of pages which are allocated
and DMA mapped when each Receive WR is posted, and then handed off
to the upper layer in rqstp::rq_arg. The page flip releases unused
pages in the rq_pages pagelist. This mechanism introduces a
significant amount of overhead.

So instead, kmalloc the Receive buffer, and leave it DMA-mapped
while the transport remains connected. This confers a number of
benefits:

* Each Receive WR requires only one receive SGE, no matter how large
  the inline threshold is. This helps the server-side NFS/RDMA
  transport operate on less capable RDMA devices.

* The Receive buffer is left allocated and mapped all the time. This
  relieves svc_rdma_post_recv from the overhead of allocating and
  DMA-mapping a fresh buffer.

* svc_rdma_wc_receive no longer has to DMA unmap the Receive buffer.
  It has to DMA sync only the number of bytes that were received.

* svc_rdma_build_arg_xdr no longer has to free a page in rq_pages
  for each page in the Receive buffer, making it a constant-time
  function.

* The Receive buffer is now plugged directly into the rq_arg's
  head[0].iov_vec, and can be larger than a page without spilling
  over into rq_arg's page list. This enables simplification of
  the RDMA Read path in subsequent patches.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit 3316f0631139c87631f2652c118da1a0354bd40d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sunrpc/svc_rdma.h
#	net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
#	net/sunrpc/xprtrdma/svc_rdma_rw.c
diff --cc include/linux/sunrpc/svc_rdma.h
index 88da0c9bd7b1,01baabfb863b..000000000000
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@@ -142,6 -144,19 +142,22 @@@ struct svcxprt_rdma 
  
  #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
  
++<<<<<<< HEAD
++=======
+ struct svc_rdma_recv_ctxt {
+ 	struct list_head	rc_list;
+ 	struct ib_recv_wr	rc_recv_wr;
+ 	struct ib_cqe		rc_cqe;
+ 	struct ib_sge		rc_recv_sge;
+ 	void			*rc_recv_buf;
+ 	struct xdr_buf		rc_arg;
+ 	u32			rc_byte_len;
+ 	unsigned int		rc_page_count;
+ 	unsigned int		rc_hdr_count;
+ 	struct page		*rc_pages[RPCSVC_MAXPAGES];
+ };
+ 
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  /* Track DMA maps for this transport and context */
  static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
  					   struct svc_rdma_op_ctxt *ctxt)
diff --cc net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 330d542fd96e,d4ccd1c0142c..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@@ -108,61 -108,247 +108,301 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
++<<<<<<< HEAD
 +/*
 + * Replace the pages in the rq_argpages array with the pages from the SGE in
 + * the RDMA_RECV completion. The SGL should contain full pages up until the
 + * last one.
 + */
++=======
+ static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc);
+ 
+ static inline struct svc_rdma_recv_ctxt *
+ svc_rdma_next_recv_ctxt(struct list_head *list)
+ {
+ 	return list_first_entry_or_null(list, struct svc_rdma_recv_ctxt,
+ 					rc_list);
+ }
+ 
+ static struct svc_rdma_recv_ctxt *
+ svc_rdma_recv_ctxt_alloc(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 	dma_addr_t addr;
+ 	void *buffer;
+ 
+ 	ctxt = kmalloc(sizeof(*ctxt), GFP_KERNEL);
+ 	if (!ctxt)
+ 		goto fail0;
+ 	buffer = kmalloc(rdma->sc_max_req_size, GFP_KERNEL);
+ 	if (!buffer)
+ 		goto fail1;
+ 	addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
+ 				 rdma->sc_max_req_size, DMA_FROM_DEVICE);
+ 	if (ib_dma_mapping_error(rdma->sc_pd->device, addr))
+ 		goto fail2;
+ 
+ 	ctxt->rc_recv_wr.next = NULL;
+ 	ctxt->rc_recv_wr.wr_cqe = &ctxt->rc_cqe;
+ 	ctxt->rc_recv_wr.sg_list = &ctxt->rc_recv_sge;
+ 	ctxt->rc_recv_wr.num_sge = 1;
+ 	ctxt->rc_cqe.done = svc_rdma_wc_receive;
+ 	ctxt->rc_recv_sge.addr = addr;
+ 	ctxt->rc_recv_sge.length = rdma->sc_max_req_size;
+ 	ctxt->rc_recv_sge.lkey = rdma->sc_pd->local_dma_lkey;
+ 	ctxt->rc_recv_buf = buffer;
+ 	return ctxt;
+ 
+ fail2:
+ 	kfree(buffer);
+ fail1:
+ 	kfree(ctxt);
+ fail0:
+ 	return NULL;
+ }
+ 
+ /**
+  * svc_rdma_recv_ctxts_destroy - Release all recv_ctxt's for an xprt
+  * @rdma: svcxprt_rdma being torn down
+  *
+  */
+ void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts))) {
+ 		list_del(&ctxt->rc_list);
+ 		ib_dma_unmap_single(rdma->sc_pd->device,
+ 				    ctxt->rc_recv_sge.addr,
+ 				    ctxt->rc_recv_sge.length,
+ 				    DMA_FROM_DEVICE);
+ 		kfree(ctxt->rc_recv_buf);
+ 		kfree(ctxt);
+ 	}
+ }
+ 
+ static struct svc_rdma_recv_ctxt *
+ svc_rdma_recv_ctxt_get(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	spin_lock(&rdma->sc_recv_lock);
+ 	ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts);
+ 	if (!ctxt)
+ 		goto out_empty;
+ 	list_del(&ctxt->rc_list);
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ out:
+ 	ctxt->rc_page_count = 0;
+ 	return ctxt;
+ 
+ out_empty:
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ 	ctxt = svc_rdma_recv_ctxt_alloc(rdma);
+ 	if (!ctxt)
+ 		return NULL;
+ 	goto out;
+ }
+ 
+ /**
+  * svc_rdma_recv_ctxt_put - Return recv_ctxt to free list
+  * @rdma: controlling svcxprt_rdma
+  * @ctxt: object to return to the free list
+  *
+  */
+ void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
+ 			    struct svc_rdma_recv_ctxt *ctxt)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ctxt->rc_page_count; i++)
+ 		put_page(ctxt->rc_pages[i]);
+ 	spin_lock(&rdma->sc_recv_lock);
+ 	list_add(&ctxt->rc_list, &rdma->sc_recv_ctxts);
+ 	spin_unlock(&rdma->sc_recv_lock);
+ }
+ 
+ static int svc_rdma_post_recv(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 	struct ib_recv_wr *bad_recv_wr;
+ 	int ret;
+ 
+ 	ctxt = svc_rdma_recv_ctxt_get(rdma);
+ 	if (!ctxt)
+ 		return -ENOMEM;
+ 
+ 	svc_xprt_get(&rdma->sc_xprt);
+ 	ret = ib_post_recv(rdma->sc_qp, &ctxt->rc_recv_wr, &bad_recv_wr);
+ 	trace_svcrdma_post_recv(&ctxt->rc_recv_wr, ret);
+ 	if (ret)
+ 		goto err_post;
+ 	return 0;
+ 
+ err_post:
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	svc_xprt_put(&rdma->sc_xprt);
+ 	return ret;
+ }
+ 
+ /**
+  * svc_rdma_post_recvs - Post initial set of Recv WRs
+  * @rdma: fresh svcxprt_rdma
+  *
+  * Returns true if successful, otherwise false.
+  */
+ bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma)
+ {
+ 	unsigned int i;
+ 	int ret;
+ 
+ 	for (i = 0; i < rdma->sc_max_requests; i++) {
+ 		ret = svc_rdma_post_recv(rdma);
+ 		if (ret) {
+ 			pr_err("svcrdma: failure posting recv buffers: %d\n",
+ 			       ret);
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ /**
+  * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC
+  * @cq: Completion Queue context
+  * @wc: Work Completion object
+  *
+  * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
+  * the Receive completion handler could be running.
+  */
+ static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct svcxprt_rdma *rdma = cq->cq_context;
+ 	struct ib_cqe *cqe = wc->wr_cqe;
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	trace_svcrdma_wc_receive(wc);
+ 
+ 	/* WARNING: Only wc->wr_cqe and wc->status are reliable */
+ 	ctxt = container_of(cqe, struct svc_rdma_recv_ctxt, rc_cqe);
+ 
+ 	if (wc->status != IB_WC_SUCCESS)
+ 		goto flushed;
+ 
+ 	if (svc_rdma_post_recv(rdma))
+ 		goto post_err;
+ 
+ 	/* All wc fields are now known to be valid */
+ 	ctxt->rc_byte_len = wc->byte_len;
+ 	ib_dma_sync_single_for_cpu(rdma->sc_pd->device,
+ 				   ctxt->rc_recv_sge.addr,
+ 				   wc->byte_len, DMA_FROM_DEVICE);
+ 
+ 	spin_lock(&rdma->sc_rq_dto_lock);
+ 	list_add_tail(&ctxt->rc_list, &rdma->sc_rq_dto_q);
+ 	spin_unlock(&rdma->sc_rq_dto_lock);
+ 	set_bit(XPT_DATA, &rdma->sc_xprt.xpt_flags);
+ 	if (!test_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags))
+ 		svc_xprt_enqueue(&rdma->sc_xprt);
+ 	goto out;
+ 
+ flushed:
+ 	if (wc->status != IB_WC_WR_FLUSH_ERR)
+ 		pr_err("svcrdma: Recv: %s (%u/0x%x)\n",
+ 		       ib_wc_status_msg(wc->status),
+ 		       wc->status, wc->vendor_err);
+ post_err:
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 	svc_xprt_enqueue(&rdma->sc_xprt);
+ out:
+ 	svc_xprt_put(&rdma->sc_xprt);
+ }
+ 
+ /**
+  * svc_rdma_flush_recv_queues - Drain pending Receive work
+  * @rdma: svcxprt_rdma being shut down
+  *
+  */
+ void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_read_complete_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_rq_dto_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ }
+ 
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  static void svc_rdma_build_arg_xdr(struct svc_rqst *rqstp,
 -				   struct svc_rdma_recv_ctxt *ctxt)
 +				   struct svc_rdma_op_ctxt *ctxt)
  {
- 	struct page *page;
- 	int sge_no;
- 	u32 len;
+ 	struct xdr_buf *arg = &rqstp->rq_arg;
  
++<<<<<<< HEAD
 +	/* The reply path assumes the Call's transport header resides
 +	 * in rqstp->rq_pages[0].
 +	 */
 +	page = ctxt->pages[0];
 +	put_page(rqstp->rq_pages[0]);
 +	rqstp->rq_pages[0] = page;
 +
 +	/* Set up the XDR head */
 +	rqstp->rq_arg.head[0].iov_base = page_address(page);
 +	rqstp->rq_arg.head[0].iov_len =
 +		min_t(size_t, ctxt->byte_len, ctxt->sge[0].length);
 +	rqstp->rq_arg.len = ctxt->byte_len;
 +	rqstp->rq_arg.buflen = ctxt->byte_len;
 +
 +	/* Compute bytes past head in the SGL */
 +	len = ctxt->byte_len - rqstp->rq_arg.head[0].iov_len;
 +
 +	/* If data remains, store it in the pagelist */
 +	rqstp->rq_arg.page_len = len;
 +	rqstp->rq_arg.page_base = 0;
 +
 +	sge_no = 1;
 +	while (len && sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no];
 +		put_page(rqstp->rq_pages[sge_no]);
 +		rqstp->rq_pages[sge_no] = page;
 +		len -= min_t(u32, len, ctxt->sge[sge_no].length);
 +		sge_no++;
 +	}
 +	rqstp->rq_respages = &rqstp->rq_pages[sge_no];
 +	rqstp->rq_next_page = rqstp->rq_respages + 1;
 +
 +	/* If not all pages were used from the SGL, free the remaining ones */
 +	len = sge_no;
 +	while (sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no++];
 +		put_page(page);
 +	}
 +	ctxt->count = len;
 +
 +	/* Set up tail */
 +	rqstp->rq_arg.tail[0].iov_base = NULL;
 +	rqstp->rq_arg.tail[0].iov_len = 0;
++=======
+ 	arg->head[0].iov_base = ctxt->rc_recv_buf;
+ 	arg->head[0].iov_len = ctxt->rc_byte_len;
+ 	arg->tail[0].iov_base = NULL;
+ 	arg->tail[0].iov_len = 0;
+ 	arg->page_len = 0;
+ 	arg->page_base = 0;
+ 	arg->buflen = ctxt->rc_byte_len;
+ 	arg->len = ctxt->rc_byte_len;
+ 
+ 	rqstp->rq_respages = &rqstp->rq_pages[0];
+ 	rqstp->rq_next_page = rqstp->rq_respages + 1;
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  }
  
  /* This accommodates the largest possible Write chunk,
diff --cc net/sunrpc/xprtrdma/svc_rdma_rw.c
index 506b9ec4883d,ce3ea8419704..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_rw.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_rw.c
@@@ -714,12 -715,9 +714,15 @@@ static int svc_rdma_build_normal_read_c
  					    struct svc_rdma_read_info *info,
  					    __be32 *p)
  {
 -	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 +	struct svc_rdma_op_ctxt *head = info->ri_readctxt;
  	int ret;
  
++<<<<<<< HEAD
 +	info->ri_pageno = head->hdr_count;
 +	info->ri_pageoff = 0;
 +
++=======
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  	ret = svc_rdma_build_read_chunk(rqstp, info, p);
  	if (ret < 0)
  		goto out;
@@@ -771,35 -771,25 +776,48 @@@ static int svc_rdma_build_pz_read_chunk
  					struct svc_rdma_read_info *info,
  					__be32 *p)
  {
 -	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 +	struct svc_rdma_op_ctxt *head = info->ri_readctxt;
  	int ret;
  
++<<<<<<< HEAD
 +	info->ri_pageno = head->hdr_count - 1;
 +	info->ri_pageoff = offset_in_page(head->byte_len);
 +
++=======
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  	ret = svc_rdma_build_read_chunk(rqstp, info, p);
  	if (ret < 0)
  		goto out;
  
  	trace_svcrdma_encode_pzr(info->ri_chunklen);
  
 -	head->rc_arg.len += info->ri_chunklen;
 -	head->rc_arg.buflen += info->ri_chunklen;
 +	head->arg.len += info->ri_chunklen;
 +	head->arg.buflen += info->ri_chunklen;
  
++<<<<<<< HEAD
 +	if (head->arg.buflen <= head->sge[0].length) {
 +		/* Transport header and RPC message fit entirely
 +		 * in page where head iovec resides.
 +		 */
 +		head->arg.head[0].iov_len = info->ri_chunklen;
 +	} else {
 +		/* Transport header and part of RPC message reside
 +		 * in the head iovec's page.
 +		 */
 +		head->arg.head[0].iov_len =
 +				head->sge[0].length - head->byte_len;
 +		head->arg.page_len =
 +				info->ri_chunklen - head->arg.head[0].iov_len;
 +	}
++=======
+ 	head->rc_hdr_count = 1;
+ 	head->rc_arg.head[0].iov_base = page_address(head->rc_pages[0]);
+ 	head->rc_arg.head[0].iov_len = min_t(size_t, PAGE_SIZE,
+ 					     info->ri_chunklen);
+ 
+ 	head->rc_arg.page_len = info->ri_chunklen -
+ 				head->rc_arg.head[0].iov_len;
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  
  out:
  	return ret;
@@@ -830,17 -820,16 +848,27 @@@ int svc_rdma_recv_read_chunk(struct svc
  	int ret;
  
  	/* The request (with page list) is constructed in
 -	 * head->rc_arg. Pages involved with RDMA Read I/O are
 +	 * head->arg. Pages involved with RDMA Read I/O are
  	 * transferred there.
  	 */
++<<<<<<< HEAD
 +	head->hdr_count = head->count;
 +	head->arg.head[0] = rqstp->rq_arg.head[0];
 +	head->arg.tail[0] = rqstp->rq_arg.tail[0];
 +	head->arg.pages = head->pages;
 +	head->arg.page_base = 0;
 +	head->arg.page_len = 0;
 +	head->arg.len = rqstp->rq_arg.len;
 +	head->arg.buflen = rqstp->rq_arg.buflen;
++=======
+ 	head->rc_arg.head[0] = rqstp->rq_arg.head[0];
+ 	head->rc_arg.tail[0] = rqstp->rq_arg.tail[0];
+ 	head->rc_arg.pages = head->rc_pages;
+ 	head->rc_arg.page_base = 0;
+ 	head->rc_arg.page_len = 0;
+ 	head->rc_arg.len = rqstp->rq_arg.len;
+ 	head->rc_arg.buflen = rqstp->rq_arg.buflen;
++>>>>>>> 3316f0631139 (svcrdma: Persistently allocate and DMA-map Receive buffers)
  
  	info = svc_rdma_read_info_alloc(rdma);
  	if (!info)
* Unmerged path include/linux/sunrpc/svc_rdma.h
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_rw.c
diff --git a/net/sunrpc/xprtrdma/svc_rdma_sendto.c b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
index fed28de78d37..1b7a0c973947 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_sendto.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
@@ -628,10 +628,7 @@ int svc_rdma_sendto(struct svc_rqst *rqstp)
 	struct page *res_page;
 	int ret;
 
-	/* Find the call's chunk lists to decide how to send the reply.
-	 * Receive places the Call's xprt header at the start of page 0.
-	 */
-	rdma_argp = page_address(rqstp->rq_pages[0]);
+	rdma_argp = rctxt->rc_recv_buf;
 	svc_rdma_get_write_arrays(rdma_argp, &wr_lst, &rp_ch);
 
 	/* Create the RDMA response header. xprt->xpt_mutex,
diff --git a/net/sunrpc/xprtrdma/svc_rdma_transport.c b/net/sunrpc/xprtrdma/svc_rdma_transport.c
index c28ee7409658..6315b83675e9 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_transport.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_transport.c
@@ -778,7 +778,7 @@ static struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)
 	qp_attr.cap.max_send_wr = newxprt->sc_sq_depth - ctxts;
 	qp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;
 	qp_attr.cap.max_send_sge = newxprt->sc_max_sge;
-	qp_attr.cap.max_recv_sge = newxprt->sc_max_sge;
+	qp_attr.cap.max_recv_sge = 1;
 	qp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
 	qp_attr.qp_type = IB_QPT_RC;
 	qp_attr.send_cq = newxprt->sc_sq_cq;
