IB/mlx5: Enable driver uapi commands for flow steering

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Yishai Hadas <yishaih@mellanox.com>
commit cb80fb189270e7b2c32fa470d40e951852614eb2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/cb80fb18.failed

Expose the mlx5 flow steering parsing trees, exposing the functionality to
user space.

	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit cb80fb189270e7b2c32fa470d40e951852614eb2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/flow.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 71eefd2f6d5f,a26ab69f3741..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -4776,7 -5516,129 +4776,133 @@@ static void mlx5_ib_cleanup_multiport_m
  	mlx5_nic_vport_disable_roce(dev->mdev);
  }
  
++<<<<<<< HEAD
 +static void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++=======
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_dm,
+ 	UVERBS_OBJECT_DM,
+ 	UVERBS_METHOD_DM_ALLOC,
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,
+ 			    UVERBS_ATTR_TYPE(u64),
+ 			    UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_PAGE_INDEX,
+ 			    UVERBS_ATTR_TYPE(u16),
+ 			    UA_MANDATORY));
+ 
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_flow_action,
+ 	UVERBS_OBJECT_FLOW_ACTION,
+ 	UVERBS_METHOD_FLOW_ACTION_ESP_CREATE,
+ 	UVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_CREATE_FLOW_ACTION_FLAGS,
+ 			   UVERBS_ATTR_TYPE(u64),
+ 			   UA_MANDATORY));
+ 
+ #define NUM_TREES	5
+ static int populate_specs_root(struct mlx5_ib_dev *dev)
+ {
+ 	const struct uverbs_object_tree_def *default_root[NUM_TREES + 1] = {
+ 		uverbs_default_get_objects()};
+ 	size_t num_trees = 1;
+ 
+ 	if (mlx5_accel_ipsec_device_caps(dev->mdev) & MLX5_ACCEL_IPSEC_CAP_DEVICE &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = &mlx5_ib_flow_action;
+ 
+ 	if (MLX5_CAP_DEV_MEM(dev->mdev, memic) &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = &mlx5_ib_dm;
+ 
+ 	if (MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &
+ 			    MLX5_GENERAL_OBJ_TYPES_CAP_UCTX &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = mlx5_ib_get_devx_tree();
+ 
+ 	num_trees += mlx5_ib_get_flow_trees(default_root + num_trees);
+ 
+ 	dev->ib_dev.driver_specs_root =
+ 		uverbs_alloc_spec_tree(num_trees, default_root);
+ 
+ 	return PTR_ERR_OR_ZERO(dev->ib_dev.driver_specs_root);
+ }
+ 
+ static void depopulate_specs_root(struct mlx5_ib_dev *dev)
+ {
+ 	uverbs_free_spec_tree(dev->ib_dev.driver_specs_root);
+ }
+ 
+ static int mlx5_ib_read_counters(struct ib_counters *counters,
+ 				 struct ib_counters_read_attr *read_attr,
+ 				 struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 	struct mlx5_read_counters_attr mread_attr = {};
+ 	struct mlx5_ib_flow_counters_desc *desc;
+ 	int ret, i;
+ 
+ 	mutex_lock(&mcounters->mcntrs_mutex);
+ 	if (mcounters->cntrs_max_index > read_attr->ncounters) {
+ 		ret = -EINVAL;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.out = kcalloc(mcounters->counters_num, sizeof(u64),
+ 				 GFP_KERNEL);
+ 	if (!mread_attr.out) {
+ 		ret = -ENOMEM;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.hw_cntrs_hndl = mcounters->hw_cntrs_hndl;
+ 	mread_attr.flags = read_attr->flags;
+ 	ret = mcounters->read_counters(counters->device, &mread_attr);
+ 	if (ret)
+ 		goto err_read;
+ 
+ 	/* do the pass over the counters data array to assign according to the
+ 	 * descriptions and indexing pairs
+ 	 */
+ 	desc = mcounters->counters_data;
+ 	for (i = 0; i < mcounters->ncounters; i++)
+ 		read_attr->counters_buff[desc[i].index] += mread_attr.out[desc[i].description];
+ 
+ err_read:
+ 	kfree(mread_attr.out);
+ err_bound:
+ 	mutex_unlock(&mcounters->mcntrs_mutex);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_destroy_counters(struct ib_counters *counters)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 
+ 	counters_clear_description(counters);
+ 	if (mcounters->hw_cntrs_hndl)
+ 		mlx5_fc_destroy(to_mdev(counters->device)->mdev,
+ 				mcounters->hw_cntrs_hndl);
+ 
+ 	kfree(mcounters);
+ 
+ 	return 0;
+ }
+ 
+ static struct ib_counters *mlx5_ib_create_counters(struct ib_device *device,
+ 						   struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters;
+ 
+ 	mcounters = kzalloc(sizeof(*mcounters), GFP_KERNEL);
+ 	if (!mcounters)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mutex_init(&mcounters->mcntrs_mutex);
+ 
+ 	return &mcounters->ibcntrs;
+ }
+ 
+ void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++>>>>>>> cb80fb189270 (IB/mlx5: Enable driver uapi commands for flow steering)
  {
  	mlx5_ib_cleanup_multiport_master(dev);
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 71b1991d86b3,462505c8fa25..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -1092,6 -1226,36 +1092,39 @@@ struct mlx5_core_dev *mlx5_ib_get_nativ
  void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
  				  u8 port_num);
  
++<<<<<<< HEAD
++=======
+ #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
+ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+ 			struct mlx5_ib_ucontext *context);
+ void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
+ 			  struct mlx5_ib_ucontext *context);
+ const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
+ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
+ 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+ 	void *cmd_in, int inlen, int dest_id, int dest_type);
+ bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
+ int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
+ #else
+ static inline int
+ mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+ 		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
+ static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
+ 					struct mlx5_ib_ucontext *context) {}
+ static inline const struct uverbs_object_tree_def *
+ mlx5_ib_get_devx_tree(void) { return NULL; }
+ static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
+ 					     int *dest_type)
+ {
+ 	return false;
+ }
+ static inline int
+ mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root)
+ {
+ 	return 0;
+ }
+ #endif
++>>>>>>> cb80fb189270 (IB/mlx5: Enable driver uapi commands for flow steering)
  static inline void init_query_mad(struct ib_smp *mad)
  {
  	mad->base_version  = 1;
* Unmerged path drivers/infiniband/hw/mlx5/flow.c
* Unmerged path drivers/infiniband/hw/mlx5/flow.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
