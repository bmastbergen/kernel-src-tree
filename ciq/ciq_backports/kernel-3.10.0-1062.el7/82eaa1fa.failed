net/mlx5e: FPGA, fix Innova IPsec TX offload data path performance

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: FPGA, fix Innova IPsec TX offload data path performance (Alaa Hleihel) [1688717]
Rebuild_FUZZ: 96.88%
commit-author Raed Salem <raeds@mellanox.com>
commit 82eaa1fa0448da1852d7b80832e67e80a08dcc27
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/82eaa1fa.failed

At Innova IPsec TX offload data path a special software parser metadata
is used to pass some packet attributes to the hardware, this metadata
is passed using the Ethernet control segment of a WQE (a HW descriptor)
header.

The cited commit might nullify this header, hence the metadata is lost,
this caused a significant performance drop during hw offloading
operation.

Fix by restoring the metadata at the Ethernet control segment in case
it was nullified.

Fixes: 37fdffb217a4 ("net/mlx5: WQ, fixes for fragmented WQ buffers API")
	Signed-off-by: Raed Salem <raeds@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 82eaa1fa0448da1852d7b80832e67e80a08dcc27)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 1ad1904dca2e,0e55cd1f2e98..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -316,32 -333,75 +316,85 @@@ mlx5e_txwqe_complete(struct mlx5e_txqs
  
  	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
  		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 -}
  
 -#define INL_HDR_START_SZ (sizeof(((struct mlx5_wqe_eth_seg *)NULL)->inline_hdr.start))
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.wqe_info[pi].skb = NULL;
 +		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
 +		sq->stats->nop++;
 +	}
 +}
  
 -netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 -			  struct mlx5e_tx_wqe *wqe, u16 pi)
 +static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 +				 struct mlx5e_tx_wqe *wqe, u16 pi)
  {
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5_wqe_ctrl_seg *cseg;
 -	struct mlx5_wqe_eth_seg  *eseg;
 -	struct mlx5_wqe_data_seg *dseg;
 -	struct mlx5e_tx_wqe_info *wi;
 +	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
 +
 +	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 +	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
  
  	struct mlx5e_sq_stats *stats = sq->stats;
 -	u16 headlen, ihs, contig_wqebbs_room;
 -	u16 ds_cnt, ds_cnt_inl = 0;
 -	u8 num_wqebbs, opcode;
 -	u32 num_bytes;
 +	unsigned char *skb_data = skb->data;
 +	unsigned int skb_len = skb->len;
 +	u8  opcode = MLX5_OPCODE_SEND;
 +	unsigned int num_bytes;
  	int num_dma;
++<<<<<<< HEAD
 +	u16 headlen;
 +	u16 ds_cnt;
 +	u16 ihs;
++=======
+ 	__be16 mss;
+ 
+ 	/* Calc ihs and ds cnt, no writes to wqe yet */
+ 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
+ 	if (skb_is_gso(skb)) {
+ 		opcode    = MLX5_OPCODE_LSO;
+ 		mss       = cpu_to_be16(skb_shinfo(skb)->gso_size);
+ 		ihs       = mlx5e_tx_get_gso_ihs(sq, skb);
+ 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 		stats->packets += skb_shinfo(skb)->gso_segs;
+ 	} else {
+ 		opcode    = MLX5_OPCODE_SEND;
+ 		mss       = 0;
+ 		ihs       = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
+ 		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
+ 		stats->packets++;
+ 	}
+ 
+ 	stats->bytes     += num_bytes;
+ 	stats->xmit_more += skb->xmit_more;
+ 
+ 	headlen = skb->len - ihs - skb->data_len;
+ 	ds_cnt += !!headlen;
+ 	ds_cnt += skb_shinfo(skb)->nr_frags;
+ 
+ 	if (ihs) {
+ 		ihs += !!skb_vlan_tag_present(skb) * VLAN_HLEN;
+ 
+ 		ds_cnt_inl = DIV_ROUND_UP(ihs - INL_HDR_START_SZ, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	contig_wqebbs_room = mlx5_wq_cyc_get_contig_wqebbs(wq, pi);
+ 	if (unlikely(contig_wqebbs_room < num_wqebbs)) {
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		struct mlx5_wqe_eth_seg cur_eth = wqe->eth;
+ #endif
+ 		mlx5e_fill_sq_frag_edge(sq, wq, pi, contig_wqebbs_room);
+ 		mlx5e_sq_fetch_wqe(sq, &wqe, &pi);
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		wqe->eth = cur_eth;
+ #endif
+ 	}
+ 
+ 	/* fill wqe */
+ 	wi   = &sq->db.wqe_info[pi];
+ 	cseg = &wqe->ctrl;
+ 	eseg = &wqe->eth;
+ 	dseg =  wqe->data;
++>>>>>>> 82eaa1fa0448 (net/mlx5e: FPGA, fix Innova IPsec TX offload data path performance)
  
  	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
