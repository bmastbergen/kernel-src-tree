Revert "mm: numa: defer TLB flush for THP migration as long as possible"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Nadav Amit <namit@vmware.com>
commit a9b802500ebbff1544519a2969323b719dac21f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/a9b80250.failed

While deferring TLB flushes is a good practice, the reverted patch
caused pending TLB flushes to be checked while the page-table lock is
not taken.  As a result, in architectures with weak memory model (PPC),
Linux may miss a memory-barrier, miss the fact TLB flushes are pending,
and cause (in theory) a memory corruption.

Since the alternative of using smp_mb__after_unlock_lock() was
considered a bit open-coded, and the performance impact is expected to
be small, the previous patch is reverted.

This reverts b0943d61b8fa ("mm: numa: defer TLB flush for THP migration
as long as possible").

Link: http://lkml.kernel.org/r/20170802000818.4760-4-namit@vmware.com
	Signed-off-by: Nadav Amit <namit@vmware.com>
	Suggested-by: Mel Gorman <mgorman@suse.de>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Acked-by: Rik van Riel <riel@redhat.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jeff Dike <jdike@addtoit.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Nadav Amit <nadav.amit@gmail.com>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a9b802500ebbff1544519a2969323b719dac21f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/migrate.c
diff --cc mm/migrate.c
index dc35415df847,d68a41da6abb..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -1812,13 -1937,11 +1812,16 @@@ int migrate_misplaced_transhuge_page(st
  		put_page(new_page);
  		goto out_fail;
  	}
++<<<<<<< HEAD
 +
 +	if (tlb_flush_pending(mm))
 +		flush_tlb_range(vma, mmun_start, mmun_end);
++=======
++>>>>>>> a9b802500ebb (Revert "mm: numa: defer TLB flush for THP migration as long as possible")
  
  	/* Prepare a page as a migration target */
 -	__SetPageLocked(new_page);
 -	if (PageSwapBacked(page))
 -		__SetPageSwapBacked(new_page);
 +	__set_page_locked(new_page);
 +	SetPageSwapBacked(new_page);
  
  	/* anon mapping, we can simply copy page->mapping to the new page: */
  	new_page->mapping = page->mapping;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 5d69a0fbfd2c..32c2086a6593 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1719,6 +1719,13 @@ int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd)
 		goto clear_pmdnuma;
 	}
 
+	/*
+	 * The page_table_lock above provides a memory barrier
+	 * with change_protection_range.
+	 */
+	if (mm_tlb_flush_pending(vma->vm_mm))
+		flush_tlb_range(vma, haddr, haddr + HPAGE_PMD_SIZE);
+
 	/*
 	 * Migrate the THP to the requested node, returns with page unlocked
 	 * and pmd_numa cleared.
* Unmerged path mm/migrate.c
