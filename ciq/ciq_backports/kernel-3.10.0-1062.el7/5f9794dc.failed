RDMA/ucontext: Add a core API for mmaping driver IO memory

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit 5f9794dc94f59ad1eb821724a8ae1f8e803ea188
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5f9794dc.failed

To support disassociation and PCI hot unplug, we have to track all the
VMAs that refer to the device IO memory. When disassociation occurs the
VMAs have to be revised to point to the zero page, not the IO memory, to
allow the physical HW to be unplugged.

The three drivers supporting this implemented three different versions
of this algorithm, all leaving something to be desired. This new common
implementation has a few differences from the driver versions:

- Track all VMAs, including splitting/truncating/etc. Tie the lifetime of
  the private data allocation to the lifetime of the vma. This avoids any
  tricks with setting vm_ops which Linus didn't like. (see link)
- Support multiple mms, and support properly tracking mmaps triggered by
  processes other than the one first opening the uverbs fd. This makes
  fork behavior of disassociation enabled drivers the same as fork support
  in normal drivers.
- Don't use crazy get_task stuff.
- Simplify the approach for to racing between vm_ops close and
  disassociation, fixing the related bugs most of the driver
  implementations had. Since we are in core code the tracking list can be
  placed in struct ib_uverbs_ufile, which has a lifetime strictly longer
  than any VMAs created by mmap on the uverbs FD.

Link: https://www.spinics.net/lists/stable/msg248747.html
Link: https://lkml.kernel.org/r/CA+55aFxJTV_g46AQPoPXen-UPiqR1HGMZictt7VpC-SMFbm3Cw@mail.gmail.com
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 5f9794dc94f59ad1eb821724a8ae1f8e803ea188)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/rdma_core.c
#	drivers/infiniband/core/rdma_core.h
#	drivers/infiniband/core/uverbs.h
#	drivers/infiniband/core/uverbs_main.c
diff --cc drivers/infiniband/core/rdma_core.c
index 586f179a9de6,06d31fe56677..000000000000
--- a/drivers/infiniband/core/rdma_core.c
+++ b/drivers/infiniband/core/rdma_core.c
@@@ -623,105 -768,186 +623,128 @@@ const struct uverbs_obj_type_class uver
  	 */
  	.needs_kfree_rcu = true,
  };
 -EXPORT_SYMBOL(uverbs_idr_class);
  
 -void uverbs_close_fd(struct file *f)
 +static void _uverbs_close_fd(struct ib_uobject_file *uobj_file)
  {
 -	struct ib_uobject *uobj = f->private_data;
 -	struct ib_uverbs_file *ufile = uobj->ufile;
 +	struct ib_ucontext *ucontext;
 +	struct ib_uverbs_file *ufile = uobj_file->ufile;
 +	int ret;
  
 -	if (down_read_trylock(&ufile->hw_destroy_rwsem)) {
 -		/*
 -		 * lookup_get_fd_uobject holds the kref on the struct file any
 -		 * time a FD uobj is locked, which prevents this release
 -		 * method from being invoked. Meaning we can always get the
 -		 * write lock here, or we have a kernel bug.
 -		 */
 -		WARN_ON(uverbs_try_lock_object(uobj, UVERBS_LOOKUP_WRITE));
 -		uverbs_destroy_uobject(uobj, RDMA_REMOVE_CLOSE);
 -		up_read(&ufile->hw_destroy_rwsem);
 -	}
 +	mutex_lock(&uobj_file->ufile->cleanup_mutex);
  
 -	/* Matches the get in alloc_begin_fd_uobject */
 -	kref_put(&ufile->ref, ib_uverbs_release_file);
 +	/* uobject was either already cleaned up or is cleaned up right now anyway */
 +	if (!uobj_file->uobj.context ||
 +	    !down_read_trylock(&uobj_file->uobj.context->cleanup_rwsem))
 +		goto unlock;
  
 -	/* Pairs with filp->private_data in alloc_begin_fd_uobject */
 -	uverbs_uobject_put(uobj);
 +	ucontext = uobj_file->uobj.context;
 +	ret = _rdma_remove_commit_uobject(&uobj_file->uobj, RDMA_REMOVE_CLOSE);
 +	up_read(&ucontext->cleanup_rwsem);
 +	if (ret)
 +		pr_warn("uverbs: unable to clean up uobject file in uverbs_close_fd.\n");
 +unlock:
 +	mutex_unlock(&ufile->cleanup_mutex);
 +}
 +
 +void uverbs_close_fd(struct file *f)
 +{
 +	struct ib_uobject_file *uobj_file = f->private_data;
 +	struct kref *uverbs_file_ref = &uobj_file->ufile->ref;
 +
 +	_uverbs_close_fd(uobj_file);
 +	uverbs_uobject_put(&uobj_file->uobj);
 +	kref_put(uverbs_file_ref, ib_uverbs_release_file);
  }
  
 -static void ufile_disassociate_ucontext(struct ib_ucontext *ibcontext)
 +void uverbs_cleanup_ucontext(struct ib_ucontext *ucontext, bool device_removed)
  {
 -	struct ib_device *ib_dev = ibcontext->device;
 -	struct task_struct *owning_process  = NULL;
 -	struct mm_struct   *owning_mm       = NULL;
 -
 -	owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID);
 -	if (!owning_process)
 -		return;
 -
 -	owning_mm = get_task_mm(owning_process);
 -	if (!owning_mm) {
 -		pr_info("no mm, disassociate ucontext is pending task termination\n");
 -		while (1) {
 -			put_task_struct(owning_process);
 -			usleep_range(1000, 2000);
 -			owning_process = get_pid_task(ibcontext->tgid,
 -						      PIDTYPE_PID);
 -			if (!owning_process ||
 -			    owning_process->state == TASK_DEAD) {
 -				pr_info("disassociate ucontext done, task was terminated\n");
 -				/* in case task was dead need to release the
 -				 * task struct.
 +	enum rdma_remove_reason reason = device_removed ?
 +		RDMA_REMOVE_DRIVER_REMOVE : RDMA_REMOVE_CLOSE;
 +	unsigned int cur_order = 0;
 +
 +	ucontext->cleanup_reason = reason;
 +	/*
 +	 * Waits for all remove_commit and alloc_commit to finish. Logically, We
 +	 * want to hold this forever as the context is going to be destroyed,
 +	 * but we'll release it since it causes a "held lock freed" BUG message.
 +	 */
 +	down_write(&ucontext->cleanup_rwsem);
 +
 +	while (!list_empty(&ucontext->uobjects)) {
 +		struct ib_uobject *obj, *next_obj;
 +		unsigned int next_order = UINT_MAX;
 +
 +		/*
 +		 * This shouldn't run while executing other commands on this
 +		 * context. Thus, the only thing we should take care of is
 +		 * releasing a FD while traversing this list. The FD could be
 +		 * closed and released from the _release fop of this FD.
 +		 * In order to mitigate this, we add a lock.
 +		 * We take and release the lock per order traversal in order
 +		 * to let other threads (which might still use the FDs) chance
 +		 * to run.
 +		 */
 +		mutex_lock(&ucontext->uobjects_lock);
 +		list_for_each_entry_safe(obj, next_obj, &ucontext->uobjects,
 +					 list) {
 +			if (obj->type->destroy_order == cur_order) {
 +				int ret;
 +
 +				/*
 +				 * if we hit this WARN_ON, that means we are
 +				 * racing with a lookup_get.
  				 */
 -				if (owning_process)
 -					put_task_struct(owning_process);
 -				return;
 +				WARN_ON(uverbs_try_lock_object(obj, true));
 +				ret = obj->type->type_class->remove_commit(obj,
 +									   reason);
 +				list_del(&obj->list);
 +				if (ret)
 +					pr_warn("ib_uverbs: failed to remove uobject id %d order %u\n",
 +						obj->id, cur_order);
 +				/* put the ref we took when we created the object */
 +				uverbs_uobject_put(obj);
 +			} else {
 +				next_order = min(next_order,
 +						 obj->type->destroy_order);
  			}
  		}
 +		mutex_unlock(&ucontext->uobjects_lock);
 +		cur_order = next_order;
  	}
 -
 -	down_write(&owning_mm->mmap_sem);
 -	ib_dev->disassociate_ucontext(ibcontext);
 -	up_write(&owning_mm->mmap_sem);
 -	mmput(owning_mm);
 -	put_task_struct(owning_process);
 +	up_write(&ucontext->cleanup_rwsem);
  }
  
 -/*
 - * Drop the ucontext off the ufile and completely disconnect it from the
 - * ib_device
 - */
 -static void ufile_destroy_ucontext(struct ib_uverbs_file *ufile,
 -				   enum rdma_remove_reason reason)
 +void uverbs_initialize_ucontext(struct ib_ucontext *ucontext)
  {
++<<<<<<< HEAD
 +	ucontext->cleanup_reason = 0;
 +	mutex_init(&ucontext->uobjects_lock);
 +	INIT_LIST_HEAD(&ucontext->uobjects);
 +	init_rwsem(&ucontext->cleanup_rwsem);
++=======
+ 	struct ib_ucontext *ucontext = ufile->ucontext;
+ 	int ret;
+ 
+ 	if (reason == RDMA_REMOVE_DRIVER_REMOVE) {
+ 		uverbs_user_mmap_disassociate(ufile);
+ 		ufile_disassociate_ucontext(ucontext);
+ 	}
+ 
+ 	put_pid(ucontext->tgid);
+ 	ib_rdmacg_uncharge(&ucontext->cg_obj, ucontext->device,
+ 			   RDMACG_RESOURCE_HCA_HANDLE);
+ 
+ 	/*
+ 	 * FIXME: Drivers are not permitted to fail dealloc_ucontext, remove
+ 	 * the error return.
+ 	 */
+ 	ret = ucontext->device->dealloc_ucontext(ucontext);
+ 	WARN_ON(ret);
+ 
+ 	ufile->ucontext = NULL;
++>>>>>>> 5f9794dc94f5 (RDMA/ucontext: Add a core API for mmaping driver IO memory)
  }
 -
 -static int __uverbs_cleanup_ufile(struct ib_uverbs_file *ufile,
 -				  enum rdma_remove_reason reason)
 -{
 -	struct ib_uobject *obj, *next_obj;
 -	int ret = -EINVAL;
 -
 -	/*
 -	 * This shouldn't run while executing other commands on this
 -	 * context. Thus, the only thing we should take care of is
 -	 * releasing a FD while traversing this list. The FD could be
 -	 * closed and released from the _release fop of this FD.
 -	 * In order to mitigate this, we add a lock.
 -	 * We take and release the lock per traversal in order to let
 -	 * other threads (which might still use the FDs) chance to run.
 -	 */
 -	list_for_each_entry_safe(obj, next_obj, &ufile->uobjects, list) {
 -		/*
 -		 * if we hit this WARN_ON, that means we are
 -		 * racing with a lookup_get.
 -		 */
 -		WARN_ON(uverbs_try_lock_object(obj, UVERBS_LOOKUP_WRITE));
 -		if (!uverbs_destroy_uobject(obj, reason))
 -			ret = 0;
 -		else
 -			atomic_set(&obj->usecnt, 0);
 -	}
 -	return ret;
 -}
 -
 -/*
 - * Destroy the uncontext and every uobject associated with it. If called with
 - * reason != RDMA_REMOVE_CLOSE this will not return until the destruction has
 - * been completed and ufile->ucontext is NULL.
 - *
 - * This is internally locked and can be called in parallel from multiple
 - * contexts.
 - */
 -void uverbs_destroy_ufile_hw(struct ib_uverbs_file *ufile,
 -			     enum rdma_remove_reason reason)
 -{
 -	if (reason == RDMA_REMOVE_CLOSE) {
 -		/*
 -		 * During destruction we might trigger something that
 -		 * synchronously calls release on any file descriptor. For
 -		 * this reason all paths that come from file_operations
 -		 * release must use try_lock. They can progress knowing that
 -		 * there is an ongoing uverbs_destroy_ufile_hw that will clean
 -		 * up the driver resources.
 -		 */
 -		if (!mutex_trylock(&ufile->ucontext_lock))
 -			return;
 -
 -	} else {
 -		mutex_lock(&ufile->ucontext_lock);
 -	}
 -
 -	down_write(&ufile->hw_destroy_rwsem);
 -
 -	/*
 -	 * If a ucontext was never created then we can't have any uobjects to
 -	 * cleanup, nothing to do.
 -	 */
 -	if (!ufile->ucontext)
 -		goto done;
 -
 -	ufile->ucontext->closing = true;
 -	ufile->ucontext->cleanup_retryable = true;
 -	while (!list_empty(&ufile->uobjects))
 -		if (__uverbs_cleanup_ufile(ufile, reason)) {
 -			/*
 -			 * No entry was cleaned-up successfully during this
 -			 * iteration
 -			 */
 -			break;
 -		}
 -
 -	ufile->ucontext->cleanup_retryable = false;
 -	if (!list_empty(&ufile->uobjects))
 -		__uverbs_cleanup_ufile(ufile, reason);
 -
 -	ufile_destroy_ucontext(ufile, reason);
 -
 -done:
 -	up_write(&ufile->hw_destroy_rwsem);
 -	mutex_unlock(&ufile->ucontext_lock);
 -}
 -
 + 
  const struct uverbs_obj_type_class uverbs_fd_class = {
  	.alloc_begin = alloc_begin_fd_uobject,
  	.lookup_get = lookup_get_fd_uobject,
diff --cc drivers/infiniband/core/rdma_core.h
index a243cc2a59f7,4886d2bba7c7..000000000000
--- a/drivers/infiniband/core/rdma_core.h
+++ b/drivers/infiniband/core/rdma_core.h
@@@ -113,4 -106,60 +113,63 @@@ int uverbs_finalize_object(struct ib_uo
  			   enum uverbs_obj_access access,
  			   bool commit);
  
++<<<<<<< HEAD
++=======
+ void setup_ufile_idr_uobject(struct ib_uverbs_file *ufile);
+ void release_ufile_idr_uobject(struct ib_uverbs_file *ufile);
+ 
+ /*
+  * This is the runtime description of the uverbs API, used by the syscall
+  * machinery to validate and dispatch calls.
+  */
+ 
+ /*
+  * Depending on ID the slot pointer in the radix tree points at one of these
+  * structs.
+  */
+ struct uverbs_api_object {
+ 	const struct uverbs_obj_type *type_attrs;
+ 	const struct uverbs_obj_type_class *type_class;
+ };
+ 
+ struct uverbs_api_ioctl_method {
+ 	int (__rcu *handler)(struct ib_uverbs_file *ufile,
+ 			     struct uverbs_attr_bundle *ctx);
+ 	DECLARE_BITMAP(attr_mandatory, UVERBS_API_ATTR_BKEY_LEN);
+ 	u16 bundle_size;
+ 	u8 use_stack:1;
+ 	u8 driver_method:1;
+ 	u8 key_bitmap_len;
+ 	u8 destroy_bkey;
+ };
+ 
+ struct uverbs_api_attr {
+ 	struct uverbs_attr_spec spec;
+ };
+ 
+ struct uverbs_api_object;
+ struct uverbs_api {
+ 	/* radix tree contains struct uverbs_api_* pointers */
+ 	struct radix_tree_root radix;
+ 	enum rdma_driver_id driver_id;
+ };
+ 
+ static inline const struct uverbs_api_object *
+ uapi_get_object(struct uverbs_api *uapi, u16 object_id)
+ {
+ 	return radix_tree_lookup(&uapi->radix, uapi_key_obj(object_id));
+ }
+ 
+ char *uapi_key_format(char *S, unsigned int key);
+ struct uverbs_api *uverbs_alloc_api(
+ 	const struct uverbs_object_tree_def *const *driver_specs,
+ 	enum rdma_driver_id driver_id);
+ void uverbs_disassociate_api_pre(struct ib_uverbs_device *uverbs_dev);
+ void uverbs_disassociate_api(struct uverbs_api *uapi);
+ void uverbs_destroy_api(struct uverbs_api *uapi);
+ void uapi_compute_bundle_size(struct uverbs_api_ioctl_method *method_elm,
+ 			      unsigned int num_attrs);
+ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile);
+ 
++>>>>>>> 5f9794dc94f5 (RDMA/ucontext: Add a core API for mmaping driver IO memory)
  #endif /* RDMA_CORE_H */
diff --cc drivers/infiniband/core/uverbs.h
index f227bb6565ad,c97935a0c7c6..000000000000
--- a/drivers/infiniband/core/uverbs.h
+++ b/drivers/infiniband/core/uverbs.h
@@@ -144,6 -148,22 +144,25 @@@ struct ib_uverbs_file 
  	struct ib_uverbs_async_event_file       *async_file;
  	struct list_head			list;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * To access the uobjects list hw_destroy_rwsem must be held for write
+ 	 * OR hw_destroy_rwsem held for read AND uobjects_lock held.
+ 	 * hw_destroy_rwsem should be called across any destruction of the HW
+ 	 * object of an associated uobject.
+ 	 */
+ 	struct rw_semaphore	hw_destroy_rwsem;
+ 	spinlock_t		uobjects_lock;
+ 	struct list_head	uobjects;
+ 
+ 	struct mutex umap_lock;
+ 	struct list_head umaps;
+ 
+ 	u64 uverbs_cmd_mask;
+ 	u64 uverbs_ex_cmd_mask;
+ 
++>>>>>>> 5f9794dc94f5 (RDMA/ucontext: Add a core API for mmaping driver IO memory)
  	struct idr		idr;
  	/* spinlock protects write access to idr */
  	spinlock_t		idr_lock;
diff --cc drivers/infiniband/core/uverbs_main.c
index 8cc3600913c7,8d56773aac56..000000000000
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@@ -47,8 -45,9 +47,9 @@@
  #include <linux/cdev.h>
  #include <linux/anon_inodes.h>
  #include <linux/slab.h>
+ #include <linux/sched/mm.h>
  
 -#include <linux/uaccess.h>
 +#include <asm/uaccess.h>
  
  #include <rdma/ib.h>
  #include <rdma/uverbs_std_types.h>
@@@ -883,16 -1087,16 +1104,26 @@@ static int ib_uverbs_open(struct inode 
  	}
  
  	file->device	 = dev;
 +	spin_lock_init(&file->idr_lock);
 +	idr_init(&file->idr);
 +	file->ucontext	 = NULL;
 +	file->async_file = NULL;
  	kref_init(&file->ref);
++<<<<<<< HEAD
 +	mutex_init(&file->mutex);
 +	mutex_init(&file->cleanup_mutex);
++=======
+ 	mutex_init(&file->ucontext_lock);
+ 
+ 	spin_lock_init(&file->uobjects_lock);
+ 	INIT_LIST_HEAD(&file->uobjects);
+ 	init_rwsem(&file->hw_destroy_rwsem);
+ 	mutex_init(&file->umap_lock);
+ 	INIT_LIST_HEAD(&file->umaps);
++>>>>>>> 5f9794dc94f5 (RDMA/ucontext: Add a core API for mmaping driver IO memory)
  
  	filp->private_data = file;
 +	kobject_get(&dev->kobj);
  	list_add_tail(&file->list, &dev->uverbs_file_list);
  	mutex_unlock(&dev->lists_mutex);
  	srcu_read_unlock(&dev->disassociate_srcu, srcu_key);
* Unmerged path drivers/infiniband/core/rdma_core.c
* Unmerged path drivers/infiniband/core/rdma_core.h
* Unmerged path drivers/infiniband/core/uverbs.h
* Unmerged path drivers/infiniband/core/uverbs_main.c
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1eeaa385508f..892910752c23 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2555,6 +2555,28 @@ void *ib_get_client_data(struct ib_device *device, struct ib_client *client);
 void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
 			 void *data);
 
+#if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
+int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
+		      unsigned long pfn, unsigned long size, pgprot_t prot);
+int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+			struct vm_area_struct *vma, struct page *page,
+			unsigned long size);
+#else
+static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
+				    struct vm_area_struct *vma,
+				    unsigned long pfn, unsigned long size,
+				    pgprot_t prot)
+{
+	return -EINVAL;
+}
+static inline int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+				struct vm_area_struct *vma, struct page *page,
+				unsigned long size)
+{
+	return -EINVAL;
+}
+#endif
+
 static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)
 {
 	return copy_from_user(dest, udata->inbuf, len) ? -EFAULT : 0;
