tuntap: fix multiqueue rx

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Matthew Cover <werekraken@gmail.com>
commit 8ebebcba559a1bfbaec7bbda64feb9870b9c58da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/8ebebcba.failed

When writing packets to a descriptor associated with a combined queue, the
packets should end up on that queue.

Before this change all packets written to any descriptor associated with a
tap interface end up on rx-0, even when the descriptor is associated with a
different queue.

The rx traffic can be generated by either of the following.
  1. a simple tap program which spins up multiple queues and writes packets
     to each of the file descriptors
  2. tx from a qemu vm with a tap multiqueue netdev

The queue for rx traffic can be observed by either of the following (done
on the hypervisor in the qemu case).
  1. a simple netmap program which opens and reads from per-queue
     descriptors
  2. configuring RPS and doing per-cpu captures with rxtxcpu

Alternatively, if you printk() the return value of skb_get_rx_queue() just
before each instance of netif_receive_skb() in tun.c, you will get 65535
for every skb.

Calling skb_record_rx_queue() to set the rx queue to the queue_index fixes
the association between descriptor and rx queue.

	Signed-off-by: Matthew Cover <matthew.cover@stackpath.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8ebebcba559a1bfbaec7bbda64feb9870b9c58da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/tun.c
diff --cc drivers/net/tun.c
index 35cc09f13a24,e244f5d7512a..000000000000
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@@ -1128,143 -1526,200 +1128,147 @@@ static struct sk_buff *tun_alloc_skb(st
  	return skb;
  }
  
 -static void tun_rx_batched(struct tun_struct *tun, struct tun_file *tfile,
 -			   struct sk_buff *skb, int more)
 +/* set skb frags from iovec, this can move to core network code for reuse */
 +static int zerocopy_sg_from_iovec(struct sk_buff *skb, const struct iovec *from,
 +				  int offset, size_t count)
  {
 -	struct sk_buff_head *queue = &tfile->sk.sk_write_queue;
 -	struct sk_buff_head process_queue;
 -	u32 rx_batched = tun->rx_batched;
 -	bool rcv = false;
 +	int len = iov_length(from, count) - offset;
 +	int copy = skb_headlen(skb);
 +	int size, offset1 = 0;
 +	int i = 0;
  
 -	if (!rx_batched || (!more && skb_queue_empty(queue))) {
 -		local_bh_disable();
 -		skb_record_rx_queue(skb, tfile->queue_index);
 -		netif_receive_skb(skb);
 -		local_bh_enable();
 -		return;
 +	/* Skip over from offset */
 +	while (count && (offset >= from->iov_len)) {
 +		offset -= from->iov_len;
 +		++from;
 +		--count;
  	}
  
 -	spin_lock(&queue->lock);
 -	if (!more || skb_queue_len(queue) == rx_batched) {
 -		__skb_queue_head_init(&process_queue);
 -		skb_queue_splice_tail_init(queue, &process_queue);
 -		rcv = true;
 -	} else {
 -		__skb_queue_tail(queue, skb);
 +	/* copy up to skb headlen */
 +	while (count && (copy > 0)) {
 +		size = min_t(unsigned int, copy, from->iov_len - offset);
 +		if (copy_from_user(skb->data + offset1, from->iov_base + offset,
 +				   size))
 +			return -EFAULT;
 +		if (copy > size) {
 +			++from;
 +			--count;
 +			offset = 0;
 +		} else
 +			offset += size;
 +		copy -= size;
 +		offset1 += size;
  	}
 -	spin_unlock(&queue->lock);
  
 -	if (rcv) {
 -		struct sk_buff *nskb;
 +	if (len == offset1)
 +		return 0;
  
 -		local_bh_disable();
 -		while ((nskb = __skb_dequeue(&process_queue))) {
 -			skb_record_rx_queue(nskb, tfile->queue_index);
 -			netif_receive_skb(nskb);
 +	while (count--) {
 +		struct page *page[MAX_SKB_FRAGS];
 +		int num_pages;
 +		unsigned long base;
 +		unsigned long truesize;
 +
 +		len = from->iov_len - offset;
 +		if (!len) {
 +			offset = 0;
 +			++from;
 +			continue;
  		}
 -		skb_record_rx_queue(skb, tfile->queue_index);
 -		netif_receive_skb(skb);
 -		local_bh_enable();
 +		base = (unsigned long)from->iov_base + offset;
 +		size = ((base & ~PAGE_MASK) + len + ~PAGE_MASK) >> PAGE_SHIFT;
 +		if (i + size > MAX_SKB_FRAGS)
 +			return -EMSGSIZE;
 +		num_pages = get_user_pages_fast(base, size, 0, &page[i]);
 +		if (num_pages != size) {
 +			int j;
 +
 +			for (j = 0; j < num_pages; j++)
 +				put_page(page[i + j]);
 +			return -EFAULT;
 +		}
 +		truesize = size * PAGE_SIZE;
 +		skb->data_len += len;
 +		skb->len += len;
 +		skb->truesize += truesize;
 +		atomic_add(truesize, &skb->sk->sk_wmem_alloc);
 +		while (len) {
 +			int off = base & ~PAGE_MASK;
 +			int size = min_t(int, len, PAGE_SIZE - off);
 +			__skb_fill_page_desc(skb, i, page[i], off, size);
 +			skb_shinfo(skb)->nr_frags++;
 +			/* increase sk_wmem_alloc */
 +			base += size;
 +			len -= size;
 +			i++;
 +		}
 +		offset = 0;
 +		++from;
  	}
 +	return 0;
  }
  
 -static bool tun_can_build_skb(struct tun_struct *tun, struct tun_file *tfile,
 -			      int len, int noblock, bool zerocopy)
 -{
 -	if ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)
 -		return false;
 -
 -	if (tfile->socket.sk->sk_sndbuf != INT_MAX)
 -		return false;
 -
 -	if (!noblock)
 -		return false;
 -
 -	if (zerocopy)
 -		return false;
 -
 -	if (SKB_DATA_ALIGN(len + TUN_RX_PAD) +
 -	    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) > PAGE_SIZE)
 -		return false;
 -
 -	return true;
 -}
 -
 -static struct sk_buff *__tun_build_skb(struct page_frag *alloc_frag, char *buf,
 -				       int buflen, int len, int pad)
 +static unsigned long iov_pages(const struct iovec *iv, int offset,
 +			       unsigned long nr_segs)
  {
 -	struct sk_buff *skb = build_skb(buf, buflen);
 -
 -	if (!skb)
 -		return ERR_PTR(-ENOMEM);
 -
 -	skb_reserve(skb, pad);
 -	skb_put(skb, len);
 -
 -	get_page(alloc_frag->page);
 -	alloc_frag->offset += buflen;
 -
 -	return skb;
 -}
 +	unsigned long seg, base;
 +	int pages = 0, len, size;
  
 -static int tun_xdp_act(struct tun_struct *tun, struct bpf_prog *xdp_prog,
 -		       struct xdp_buff *xdp, u32 act)
 -{
 -	int err;
 +	while (nr_segs && (offset >= iv->iov_len)) {
 +		offset -= iv->iov_len;
 +		++iv;
 +		--nr_segs;
 +	}
  
 -	switch (act) {
 -	case XDP_REDIRECT:
 -		err = xdp_do_redirect(tun->dev, xdp, xdp_prog);
 -		if (err)
 -			return err;
 -		break;
 -	case XDP_TX:
 -		err = tun_xdp_tx(tun->dev, xdp);
 -		if (err < 0)
 -			return err;
 -		break;
 -	case XDP_PASS:
 -		break;
 -	default:
 -		bpf_warn_invalid_xdp_action(act);
 -		/* fall through */
 -	case XDP_ABORTED:
 -		trace_xdp_exception(tun->dev, xdp_prog, act);
 -		/* fall through */
 -	case XDP_DROP:
 -		this_cpu_inc(tun->pcpu_stats->rx_dropped);
 -		break;
 +	for (seg = 0; seg < nr_segs; seg++) {
 +		base = (unsigned long)iv[seg].iov_base + offset;
 +		len = iv[seg].iov_len - offset;
 +		size = ((base & ~PAGE_MASK) + len + ~PAGE_MASK) >> PAGE_SHIFT;
 +		pages += size;
 +		offset = 0;
  	}
  
 -	return act;
 +	return pages;
  }
  
 -static struct sk_buff *tun_build_skb(struct tun_struct *tun,
 -				     struct tun_file *tfile,
 -				     struct iov_iter *from,
 -				     struct virtio_net_hdr *hdr,
 -				     int len, int *skb_xdp)
 +static void tun_rx_batched(struct tun_struct *tun, struct tun_file *tfile,
 +			   struct sk_buff *skb, int more)
  {
 -	struct page_frag *alloc_frag = &current->task_frag;
 -	struct bpf_prog *xdp_prog;
 -	int buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 -	char *buf;
 -	size_t copied;
 -	int pad = TUN_RX_PAD;
 -	int err = 0;
 -
 -	rcu_read_lock();
 -	xdp_prog = rcu_dereference(tun->xdp_prog);
 -	if (xdp_prog)
 -		pad += XDP_PACKET_HEADROOM;
 -	buflen += SKB_DATA_ALIGN(len + pad);
 -	rcu_read_unlock();
 -
 -	alloc_frag->offset = ALIGN((u64)alloc_frag->offset, SMP_CACHE_BYTES);
 -	if (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))
 -		return ERR_PTR(-ENOMEM);
 -
 -	buf = (char *)page_address(alloc_frag->page) + alloc_frag->offset;
 -	copied = copy_page_from_iter(alloc_frag->page,
 -				     alloc_frag->offset + pad,
 -				     len, from);
 -	if (copied != len)
 -		return ERR_PTR(-EFAULT);
 +	struct sk_buff_head *queue = &tfile->sk.sk_write_queue;
 +	struct sk_buff_head process_queue;
 +	u32 rx_batched = tun->rx_batched;
 +	bool rcv = false;
  
 -	/* There's a small window that XDP may be set after the check
 -	 * of xdp_prog above, this should be rare and for simplicity
 -	 * we do XDP on skb in case the headroom is not enough.
 -	 */
 -	if (hdr->gso_type || !xdp_prog) {
 -		*skb_xdp = 1;
 -		return __tun_build_skb(alloc_frag, buf, buflen, len, pad);
 +	if (!rx_batched || (!more && skb_queue_empty(queue))) {
 +		local_bh_disable();
++		skb_record_rx_queue(skb, tfile->queue_index);
 +		netif_receive_skb(skb);
 +		local_bh_enable();
 +		return;
  	}
  
 -	*skb_xdp = 0;
 -
 -	local_bh_disable();
 -	rcu_read_lock();
 -	xdp_prog = rcu_dereference(tun->xdp_prog);
 -	if (xdp_prog) {
 -		struct xdp_buff xdp;
 -		u32 act;
 -
 -		xdp.data_hard_start = buf;
 -		xdp.data = buf + pad;
 -		xdp_set_data_meta_invalid(&xdp);
 -		xdp.data_end = xdp.data + len;
 -		xdp.rxq = &tfile->xdp_rxq;
 -
 -		act = bpf_prog_run_xdp(xdp_prog, &xdp);
 -		if (act == XDP_REDIRECT || act == XDP_TX) {
 -			get_page(alloc_frag->page);
 -			alloc_frag->offset += buflen;
 -		}
 -		err = tun_xdp_act(tun, xdp_prog, &xdp, act);
 -		if (err < 0)
 -			goto err_xdp;
 -		if (err == XDP_REDIRECT)
 -			xdp_do_flush_map();
 -		if (err != XDP_PASS)
 -			goto out;
 -
 -		pad = xdp.data - xdp.data_hard_start;
 -		len = xdp.data_end - xdp.data;
 +	spin_lock(&queue->lock);
 +	if (!more || skb_queue_len(queue) == rx_batched) {
 +		__skb_queue_head_init(&process_queue);
 +		skb_queue_splice_tail_init(queue, &process_queue);
 +		rcv = true;
 +	} else {
 +		__skb_queue_tail(queue, skb);
  	}
 -	rcu_read_unlock();
 -	local_bh_enable();
 +	spin_unlock(&queue->lock);
  
 -	return __tun_build_skb(alloc_frag, buf, buflen, len, pad);
 +	if (rcv) {
 +		struct sk_buff *nskb;
  
 -err_xdp:
 -	put_page(alloc_frag->page);
 -out:
 -	rcu_read_unlock();
 -	local_bh_enable();
 -	return NULL;
 +		local_bh_disable();
- 		while ((nskb = __skb_dequeue(&process_queue)))
++		while ((nskb = __skb_dequeue(&process_queue))) {
++			skb_record_rx_queue(nskb, tfile->queue_index);
 +			netif_receive_skb(nskb);
++		}
++		skb_record_rx_queue(skb, tfile->queue_index);
 +		netif_receive_skb(skb);
 +		local_bh_enable();
 +	}
  }
  
  /* Get packet from user space buffer */
@@@ -1815,12 -2381,104 +1819,105 @@@ static void tun_sock_write_space(struc
  	kill_fasync(&tfile->fasync, SIGIO, POLL_OUT);
  }
  
 -static int tun_xdp_one(struct tun_struct *tun,
 -		       struct tun_file *tfile,
 -		       struct xdp_buff *xdp, int *flush)
 +static int tun_sendmsg(struct kiocb *iocb, struct socket *sock,
 +		       struct msghdr *m, size_t total_len)
  {
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	struct tun_xdp_hdr *hdr = xdp->data_hard_start;
+ 	struct virtio_net_hdr *gso = &hdr->gso;
+ 	struct tun_pcpu_stats *stats;
+ 	struct bpf_prog *xdp_prog;
+ 	struct sk_buff *skb = NULL;
+ 	u32 rxhash = 0, act;
+ 	int buflen = hdr->buflen;
+ 	int err = 0;
+ 	bool skb_xdp = false;
+ 
+ 	xdp_prog = rcu_dereference(tun->xdp_prog);
+ 	if (xdp_prog) {
+ 		if (gso->gso_type) {
+ 			skb_xdp = true;
+ 			goto build;
+ 		}
+ 		xdp_set_data_meta_invalid(xdp);
+ 		xdp->rxq = &tfile->xdp_rxq;
+ 
+ 		act = bpf_prog_run_xdp(xdp_prog, xdp);
+ 		err = tun_xdp_act(tun, xdp_prog, xdp, act);
+ 		if (err < 0) {
+ 			put_page(virt_to_head_page(xdp->data));
+ 			return err;
+ 		}
+ 
+ 		switch (err) {
+ 		case XDP_REDIRECT:
+ 			*flush = true;
+ 			/* fall through */
+ 		case XDP_TX:
+ 			return 0;
+ 		case XDP_PASS:
+ 			break;
+ 		default:
+ 			put_page(virt_to_head_page(xdp->data));
+ 			return 0;
+ 		}
+ 	}
+ 
+ build:
+ 	skb = build_skb(xdp->data_hard_start, buflen);
+ 	if (!skb) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+ 	skb_put(skb, xdp->data_end - xdp->data);
+ 
+ 	if (virtio_net_hdr_to_skb(skb, gso, tun_is_little_endian(tun))) {
+ 		this_cpu_inc(tun->pcpu_stats->rx_frame_errors);
+ 		kfree_skb(skb);
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	skb->protocol = eth_type_trans(skb, tun->dev);
+ 	skb_reset_network_header(skb);
+ 	skb_probe_transport_header(skb, 0);
+ 
+ 	if (skb_xdp) {
+ 		err = do_xdp_generic(xdp_prog, skb);
+ 		if (err != XDP_PASS)
+ 			goto out;
+ 	}
+ 
+ 	if (!rcu_dereference(tun->steering_prog))
+ 		rxhash = __skb_get_hash_symmetric(skb);
+ 
+ 	skb_record_rx_queue(skb, tfile->queue_index);
+ 	netif_receive_skb(skb);
+ 
+ 	stats = get_cpu_ptr(tun->pcpu_stats);
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->rx_packets++;
+ 	stats->rx_bytes += skb->len;
+ 	u64_stats_update_end(&stats->syncp);
+ 	put_cpu_ptr(stats);
+ 
+ 	if (rxhash)
+ 		tun_flow_update(tun, rxhash, tfile);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static int tun_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)
+ {
+ 	int ret, i;
++>>>>>>> 8ebebcba559a (tuntap: fix multiqueue rx)
  	struct tun_file *tfile = container_of(sock, struct tun_file, socket);
 -	struct tun_struct *tun = tun_get(tfile);
 -	struct tun_msg_ctl *ctl = m->msg_control;
 -	struct xdp_buff *xdp;
 +	struct tun_struct *tun = __tun_get(tfile);
  
  	if (!tun)
  		return -EBADFD;
* Unmerged path drivers/net/tun.c
