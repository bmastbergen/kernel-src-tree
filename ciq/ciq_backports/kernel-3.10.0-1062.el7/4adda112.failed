RDMA/mlx5: Enable decap and packet reformat on flow tables

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Mark Bloch <markb@mellanox.com>
commit 4adda1122c490e042d4bcb920900f796fc9423e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/4adda112.failed

If NIC RX flow tables support decap operation, enable it on creation,
This allows to perform decapsulation of tunnelled packets by steering
rules. If NIC TX flow tables support reformat operation, enable it on
creation.

We don't enable those capabilities on representors as the E-Switch should
handle packet modification (can be configured via TC) and as current
hardware can't handle both FDB and NIC flow tables with decap/packet
reformat support.

	Signed-off-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 4adda1122c490e042d4bcb920900f796fc9423e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index b6df6ac3e140,ce9afa0dd983..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -2889,6 -3038,27 +2889,30 @@@ enum flow_table_type 
  
  #define MLX5_FS_MAX_TYPES	 6
  #define MLX5_FS_MAX_ENTRIES	 BIT(16)
++<<<<<<< HEAD
++=======
+ 
+ static struct mlx5_ib_flow_prio *_get_prio(struct mlx5_flow_namespace *ns,
+ 					   struct mlx5_ib_flow_prio *prio,
+ 					   int priority,
+ 					   int num_entries, int num_groups,
+ 					   u32 flags)
+ {
+ 	struct mlx5_flow_table *ft;
+ 
+ 	ft = mlx5_create_auto_grouped_flow_table(ns, priority,
+ 						 num_entries,
+ 						 num_groups,
+ 						 0, flags);
+ 	if (IS_ERR(ft))
+ 		return ERR_CAST(ft);
+ 
+ 	prio->flow_table = ft;
+ 	prio->refcount = 0;
+ 	return prio;
+ }
+ 
++>>>>>>> 4adda1122c49 (RDMA/mlx5: Enable decap and packet reformat on flow tables)
  static struct mlx5_ib_flow_prio *get_flow_table(struct mlx5_ib_dev *dev,
  						struct ib_flow_attr *flow_attr,
  						enum flow_table_type ft_type)
@@@ -2900,8 -3070,8 +2924,9 @@@
  	int max_table_size;
  	int num_entries;
  	int num_groups;
+ 	u32 flags = 0;
  	int priority;
 +	int err = 0;
  
  	max_table_size = BIT(MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev,
  						       log_max_ft_size));
@@@ -2914,13 -3084,25 +2939,33 @@@
  		else
  			priority = ib_prio_to_core_prio(flow_attr->priority,
  							dont_trap);
++<<<<<<< HEAD
 +		ns = mlx5_get_flow_namespace(dev->mdev,
 +					     ft_type == MLX5_IB_FT_TX ?
 +					     MLX5_FLOW_NAMESPACE_EGRESS :
 +					     MLX5_FLOW_NAMESPACE_BYPASS);
++=======
+ 		if (ft_type == MLX5_IB_FT_RX) {
+ 			fn_type = MLX5_FLOW_NAMESPACE_BYPASS;
+ 			prio = &dev->flow_db->prios[priority];
+ 			if (!dev->rep &&
+ 			    MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev, decap))
+ 				flags |= MLX5_FLOW_TABLE_TUNNEL_EN_DECAP;
+ 		} else {
+ 			max_table_size =
+ 				BIT(MLX5_CAP_FLOWTABLE_NIC_TX(dev->mdev,
+ 							      log_max_ft_size));
+ 			fn_type = MLX5_FLOW_NAMESPACE_EGRESS;
+ 			prio = &dev->flow_db->egress_prios[priority];
+ 			if (!dev->rep &&
+ 			    MLX5_CAP_FLOWTABLE_NIC_TX(dev->mdev, reformat))
+ 				flags |= MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT;
+ 		}
+ 		ns = mlx5_get_flow_namespace(dev->mdev, fn_type);
++>>>>>>> 4adda1122c49 (RDMA/mlx5: Enable decap and packet reformat on flow tables)
  		num_entries = MLX5_FS_MAX_ENTRIES;
  		num_groups = MLX5_FS_MAX_TYPES;
 +		prio = &dev->flow_db.prios[priority];
  	} else if (flow_attr->type == IB_FLOW_ATTR_ALL_DEFAULT ||
  		   flow_attr->type == IB_FLOW_ATTR_MC_DEFAULT) {
  		ns = mlx5_get_flow_namespace(dev->mdev,
@@@ -2951,21 -3133,11 +2996,27 @@@
  		return ERR_PTR(-ENOMEM);
  
  	ft = prio->flow_table;
++<<<<<<< HEAD
 +	if (!ft) {
 +		ft = mlx5_create_auto_grouped_flow_table(ns, priority,
 +							 num_entries,
 +							 num_groups,
 +							 0, 0);
++=======
+ 	if (!ft)
+ 		return _get_prio(ns, prio, priority, num_entries, num_groups,
+ 				 flags);
++>>>>>>> 4adda1122c49 (RDMA/mlx5: Enable decap and packet reformat on flow tables)
  
 -	return prio;
 +		if (!IS_ERR(ft)) {
 +			prio->refcount = 0;
 +			prio->flow_table = ft;
 +		} else {
 +			err = PTR_ERR(ft);
 +		}
 +	}
 +
 +	return err ? ERR_PTR(err) : prio;
  }
  
  static void set_underlay_qp(struct mlx5_ib_dev *dev,
@@@ -3333,6 -3692,355 +3384,358 @@@ unlock
  	return ERR_PTR(err);
  }
  
++<<<<<<< HEAD
++=======
+ static struct mlx5_ib_flow_prio *_get_flow_table(struct mlx5_ib_dev *dev,
+ 						 int priority, bool mcast)
+ {
+ 	int max_table_size;
+ 	struct mlx5_flow_namespace *ns = NULL;
+ 	struct mlx5_ib_flow_prio *prio;
+ 
+ 	max_table_size = BIT(MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev,
+ 			     log_max_ft_size));
+ 	if (max_table_size < MLX5_FS_MAX_ENTRIES)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (mcast)
+ 		priority = MLX5_IB_FLOW_MCAST_PRIO;
+ 	else
+ 		priority = ib_prio_to_core_prio(priority, false);
+ 
+ 	ns = mlx5_get_flow_namespace(dev->mdev, MLX5_FLOW_NAMESPACE_BYPASS);
+ 	if (!ns)
+ 		return ERR_PTR(-ENOTSUPP);
+ 
+ 	prio = &dev->flow_db->prios[priority];
+ 
+ 	if (prio->flow_table)
+ 		return prio;
+ 
+ 	return _get_prio(ns, prio, priority, MLX5_FS_MAX_ENTRIES,
+ 			 MLX5_FS_MAX_TYPES, 0);
+ }
+ 
+ static struct mlx5_ib_flow_handler *
+ _create_raw_flow_rule(struct mlx5_ib_dev *dev,
+ 		      struct mlx5_ib_flow_prio *ft_prio,
+ 		      struct mlx5_flow_destination *dst,
+ 		      struct mlx5_ib_flow_matcher  *fs_matcher,
+ 		      void *cmd_in, int inlen)
+ {
+ 	struct mlx5_ib_flow_handler *handler;
+ 	struct mlx5_flow_act flow_act = {.flow_tag = MLX5_FS_DEFAULT_FLOW_TAG};
+ 	struct mlx5_flow_spec *spec;
+ 	struct mlx5_flow_table *ft = ft_prio->flow_table;
+ 	int err = 0;
+ 
+ 	spec = kvzalloc(sizeof(*spec), GFP_KERNEL);
+ 	handler = kzalloc(sizeof(*handler), GFP_KERNEL);
+ 	if (!handler || !spec) {
+ 		err = -ENOMEM;
+ 		goto free;
+ 	}
+ 
+ 	INIT_LIST_HEAD(&handler->list);
+ 
+ 	memcpy(spec->match_value, cmd_in, inlen);
+ 	memcpy(spec->match_criteria, fs_matcher->matcher_mask.match_params,
+ 	       fs_matcher->mask_len);
+ 	spec->match_criteria_enable = fs_matcher->match_criteria_enable;
+ 
+ 	flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	handler->rule = mlx5_add_flow_rules(ft, spec,
+ 					    &flow_act, dst, 1);
+ 
+ 	if (IS_ERR(handler->rule)) {
+ 		err = PTR_ERR(handler->rule);
+ 		goto free;
+ 	}
+ 
+ 	ft_prio->refcount++;
+ 	handler->prio = ft_prio;
+ 	handler->dev = dev;
+ 	ft_prio->flow_table = ft;
+ 
+ free:
+ 	if (err)
+ 		kfree(handler);
+ 	kvfree(spec);
+ 	return err ? ERR_PTR(err) : handler;
+ }
+ 
+ static bool raw_fs_is_multicast(struct mlx5_ib_flow_matcher *fs_matcher,
+ 				void *match_v)
+ {
+ 	void *match_c;
+ 	void *match_v_set_lyr_2_4, *match_c_set_lyr_2_4;
+ 	void *dmac, *dmac_mask;
+ 	void *ipv4, *ipv4_mask;
+ 
+ 	if (!(fs_matcher->match_criteria_enable &
+ 	      (1 << MATCH_CRITERIA_ENABLE_OUTER_BIT)))
+ 		return false;
+ 
+ 	match_c = fs_matcher->matcher_mask.match_params;
+ 	match_v_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_v,
+ 					   outer_headers);
+ 	match_c_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_c,
+ 					   outer_headers);
+ 
+ 	dmac = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dmac_47_16);
+ 	dmac_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dmac_47_16);
+ 
+ 	if (is_multicast_ether_addr(dmac) &&
+ 	    is_multicast_ether_addr(dmac_mask))
+ 		return true;
+ 
+ 	ipv4 = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	ipv4_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	if (ipv4_is_multicast(*(__be32 *)(ipv4)) &&
+ 	    ipv4_is_multicast(*(__be32 *)(ipv4_mask)))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ struct mlx5_ib_flow_handler *
+ mlx5_ib_raw_fs_rule_add(struct mlx5_ib_dev *dev,
+ 			struct mlx5_ib_flow_matcher *fs_matcher,
+ 			void *cmd_in, int inlen, int dest_id,
+ 			int dest_type)
+ {
+ 	struct mlx5_flow_destination *dst;
+ 	struct mlx5_ib_flow_prio *ft_prio;
+ 	int priority = fs_matcher->priority;
+ 	struct mlx5_ib_flow_handler *handler;
+ 	bool mcast;
+ 	int err;
+ 
+ 	if (fs_matcher->flow_type != MLX5_IB_FLOW_TYPE_NORMAL)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	if (fs_matcher->priority > MLX5_IB_FLOW_LAST_PRIO)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	dst = kzalloc(sizeof(*dst), GFP_KERNEL);
+ 	if (!dst)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mcast = raw_fs_is_multicast(fs_matcher, cmd_in);
+ 	mutex_lock(&dev->flow_db->lock);
+ 
+ 	ft_prio = _get_flow_table(dev, priority, mcast);
+ 	if (IS_ERR(ft_prio)) {
+ 		err = PTR_ERR(ft_prio);
+ 		goto unlock;
+ 	}
+ 
+ 	if (dest_type == MLX5_FLOW_DESTINATION_TYPE_TIR) {
+ 		dst->type = dest_type;
+ 		dst->tir_num = dest_id;
+ 	} else {
+ 		dst->type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE_NUM;
+ 		dst->ft_num = dest_id;
+ 	}
+ 
+ 	handler = _create_raw_flow_rule(dev, ft_prio, dst, fs_matcher, cmd_in,
+ 					inlen);
+ 
+ 	if (IS_ERR(handler)) {
+ 		err = PTR_ERR(handler);
+ 		goto destroy_ft;
+ 	}
+ 
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	atomic_inc(&fs_matcher->usecnt);
+ 	handler->flow_matcher = fs_matcher;
+ 
+ 	kfree(dst);
+ 
+ 	return handler;
+ 
+ destroy_ft:
+ 	put_flow_table(dev, ft_prio, false);
+ unlock:
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	kfree(dst);
+ 
+ 	return ERR_PTR(err);
+ }
+ 
+ static u32 mlx5_ib_flow_action_flags_to_accel_xfrm_flags(u32 mlx5_flags)
+ {
+ 	u32 flags = 0;
+ 
+ 	if (mlx5_flags & MLX5_IB_UAPI_FLOW_ACTION_FLAGS_REQUIRE_METADATA)
+ 		flags |= MLX5_ACCEL_XFRM_FLAG_REQUIRE_METADATA;
+ 
+ 	return flags;
+ }
+ 
+ #define MLX5_FLOW_ACTION_ESP_CREATE_LAST_SUPPORTED	MLX5_IB_UAPI_FLOW_ACTION_FLAGS_REQUIRE_METADATA
+ static struct ib_flow_action *
+ mlx5_ib_create_flow_action_esp(struct ib_device *device,
+ 			       const struct ib_flow_action_attrs_esp *attr,
+ 			       struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_dev *mdev = to_mdev(device);
+ 	struct ib_uverbs_flow_action_esp_keymat_aes_gcm *aes_gcm;
+ 	struct mlx5_accel_esp_xfrm_attrs accel_attrs = {};
+ 	struct mlx5_ib_flow_action *action;
+ 	u64 action_flags;
+ 	u64 flags;
+ 	int err = 0;
+ 
+ 	err = uverbs_get_flags64(
+ 		&action_flags, attrs, MLX5_IB_ATTR_CREATE_FLOW_ACTION_FLAGS,
+ 		((MLX5_FLOW_ACTION_ESP_CREATE_LAST_SUPPORTED << 1) - 1));
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	flags = mlx5_ib_flow_action_flags_to_accel_xfrm_flags(action_flags);
+ 
+ 	/* We current only support a subset of the standard features. Only a
+ 	 * keymat of type AES_GCM, with icv_len == 16, iv_algo == SEQ and esn
+ 	 * (with overlap). Full offload mode isn't supported.
+ 	 */
+ 	if (!attr->keymat || attr->replay || attr->encap ||
+ 	    attr->spi || attr->seq || attr->tfc_pad ||
+ 	    attr->hard_limit_pkts ||
+ 	    (attr->flags & ~(IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			     IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ENCRYPT)))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	if (attr->keymat->protocol !=
+ 	    IB_UVERBS_FLOW_ACTION_ESP_KEYMAT_AES_GCM)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	aes_gcm = &attr->keymat->keymat.aes_gcm;
+ 
+ 	if (aes_gcm->icv_len != 16 ||
+ 	    aes_gcm->iv_algo != IB_UVERBS_FLOW_ACTION_IV_ALGO_SEQ)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	action = kmalloc(sizeof(*action), GFP_KERNEL);
+ 	if (!action)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	action->esp_aes_gcm.ib_flags = attr->flags;
+ 	memcpy(&accel_attrs.keymat.aes_gcm.aes_key, &aes_gcm->aes_key,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.aes_key));
+ 	accel_attrs.keymat.aes_gcm.key_len = aes_gcm->key_len * 8;
+ 	memcpy(&accel_attrs.keymat.aes_gcm.salt, &aes_gcm->salt,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.salt));
+ 	memcpy(&accel_attrs.keymat.aes_gcm.seq_iv, &aes_gcm->iv,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.seq_iv));
+ 	accel_attrs.keymat.aes_gcm.icv_len = aes_gcm->icv_len * 8;
+ 	accel_attrs.keymat.aes_gcm.iv_algo = MLX5_ACCEL_ESP_AES_GCM_IV_ALGO_SEQ;
+ 	accel_attrs.keymat_type = MLX5_ACCEL_ESP_KEYMAT_AES_GCM;
+ 
+ 	accel_attrs.esn = attr->esn;
+ 	if (attr->flags & IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_TRIGGERED;
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ENCRYPT)
+ 		accel_attrs.action |= MLX5_ACCEL_ESP_ACTION_ENCRYPT;
+ 
+ 	action->esp_aes_gcm.ctx =
+ 		mlx5_accel_esp_create_xfrm(mdev->mdev, &accel_attrs, flags);
+ 	if (IS_ERR(action->esp_aes_gcm.ctx)) {
+ 		err = PTR_ERR(action->esp_aes_gcm.ctx);
+ 		goto err_parse;
+ 	}
+ 
+ 	action->esp_aes_gcm.ib_flags = attr->flags;
+ 
+ 	return &action->ib_action;
+ 
+ err_parse:
+ 	kfree(action);
+ 	return ERR_PTR(err);
+ }
+ 
+ static int
+ mlx5_ib_modify_flow_action_esp(struct ib_flow_action *action,
+ 			       const struct ib_flow_action_attrs_esp *attr,
+ 			       struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_flow_action *maction = to_mflow_act(action);
+ 	struct mlx5_accel_esp_xfrm_attrs accel_attrs;
+ 	int err = 0;
+ 
+ 	if (attr->keymat || attr->replay || attr->encap ||
+ 	    attr->spi || attr->seq || attr->tfc_pad ||
+ 	    attr->hard_limit_pkts ||
+ 	    (attr->flags & ~(IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			     IB_FLOW_ACTION_ESP_FLAGS_MOD_ESP_ATTRS |
+ 			     IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)))
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Only the ESN value or the MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP can
+ 	 * be modified.
+ 	 */
+ 	if (!(maction->esp_aes_gcm.ib_flags &
+ 	      IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED) &&
+ 	    attr->flags & (IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			   IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW))
+ 		return -EINVAL;
+ 
+ 	memcpy(&accel_attrs, &maction->esp_aes_gcm.ctx->attrs,
+ 	       sizeof(accel_attrs));
+ 
+ 	accel_attrs.esn = attr->esn;
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 	else
+ 		accel_attrs.flags &= ~MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 
+ 	err = mlx5_accel_esp_modify_xfrm(maction->esp_aes_gcm.ctx,
+ 					 &accel_attrs);
+ 	if (err)
+ 		return err;
+ 
+ 	maction->esp_aes_gcm.ib_flags &=
+ 		~IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW;
+ 	maction->esp_aes_gcm.ib_flags |=
+ 		attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5_ib_destroy_flow_action(struct ib_flow_action *action)
+ {
+ 	struct mlx5_ib_flow_action *maction = to_mflow_act(action);
+ 
+ 	switch (action->type) {
+ 	case IB_FLOW_ACTION_ESP:
+ 		/*
+ 		 * We only support aes_gcm by now, so we implicitly know this is
+ 		 * the underline crypto.
+ 		 */
+ 		mlx5_accel_esp_destroy_xfrm(maction->esp_aes_gcm.ctx);
+ 		break;
+ 	case IB_FLOW_ACTION_UNSPECIFIED:
+ 		mlx5_ib_destroy_flow_action_raw(maction);
+ 		break;
+ 	default:
+ 		WARN_ON(true);
+ 		break;
+ 	}
+ 
+ 	kfree(maction);
+ 	return 0;
+ }
+ 
++>>>>>>> 4adda1122c49 (RDMA/mlx5: Enable decap and packet reformat on flow tables)
  static int mlx5_ib_mcg_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
  	struct mlx5_ib_dev *dev = to_mdev(ibqp->device);
* Unmerged path drivers/infiniband/hw/mlx5/main.c
