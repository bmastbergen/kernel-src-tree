RDMA/nldev: Add explicit pad attribute

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Steve Wise <swise@opengridcomputing.com>
commit 25a0ad85156a7b697d4340560fff0d25a3b19243
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/25a0ad85.failed

Add a specific RDMA_NLDEV_ATTR_PAD attribute to be used for 64b
attribute padding.  To preserve the ABI, make this attribute equal to
RDMA_NLDEV_ATTR_UNSPEC, which has a value of 0, because that has been
used up until now as the pad attribute.

Change all the previous use of 0 as the pad with this
new enum.

	Signed-off-by: Steve Wise <swise@opengridcomputing.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 25a0ad85156a7b697d4340560fff0d25a3b19243)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/nldev.c
diff --cc drivers/infiniband/core/nldev.c
index 67368b6847cb,6b0c1eb71ea0..000000000000
--- a/drivers/infiniband/core/nldev.c
+++ b/drivers/infiniband/core/nldev.c
@@@ -284,7 -399,116 +287,120 @@@ out
  	return -EMSGSIZE;
  }
  
++<<<<<<< HEAD
 +static int nldev_get_doit(struct sk_buff *skb, struct nlmsghdr *nlh)
++=======
+ static int fill_res_cq_entry(struct sk_buff *msg, struct netlink_callback *cb,
+ 			     struct rdma_restrack_entry *res, uint32_t port)
+ {
+ 	struct ib_cq *cq = container_of(res, struct ib_cq, res);
+ 	struct nlattr *entry_attr;
+ 
+ 	entry_attr = nla_nest_start(msg, RDMA_NLDEV_ATTR_RES_CQ_ENTRY);
+ 	if (!entry_attr)
+ 		goto out;
+ 
+ 	if (nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_CQE, cq->cqe))
+ 		goto err;
+ 	if (nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_RES_USECNT,
+ 			      atomic_read(&cq->usecnt), RDMA_NLDEV_ATTR_PAD))
+ 		goto err;
+ 
+ 	/* Poll context is only valid for kernel CQs */
+ 	if (rdma_is_kernel_res(res) &&
+ 	    nla_put_u8(msg, RDMA_NLDEV_ATTR_RES_POLL_CTX, cq->poll_ctx))
+ 		goto err;
+ 
+ 	if (fill_res_name_pid(msg, res))
+ 		goto err;
+ 
+ 	nla_nest_end(msg, entry_attr);
+ 	return 0;
+ 
+ err:
+ 	nla_nest_cancel(msg, entry_attr);
+ out:
+ 	return -EMSGSIZE;
+ }
+ 
+ static int fill_res_mr_entry(struct sk_buff *msg, struct netlink_callback *cb,
+ 			     struct rdma_restrack_entry *res, uint32_t port)
+ {
+ 	struct ib_mr *mr = container_of(res, struct ib_mr, res);
+ 	struct nlattr *entry_attr;
+ 
+ 	entry_attr = nla_nest_start(msg, RDMA_NLDEV_ATTR_RES_MR_ENTRY);
+ 	if (!entry_attr)
+ 		goto out;
+ 
+ 	if (netlink_capable(cb->skb, CAP_NET_ADMIN)) {
+ 		if (nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_RKEY, mr->rkey))
+ 			goto err;
+ 		if (nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_LKEY, mr->lkey))
+ 			goto err;
+ 		if (nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_RES_IOVA,
+ 				      mr->iova, RDMA_NLDEV_ATTR_PAD))
+ 			goto err;
+ 	}
+ 
+ 	if (nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_RES_MRLEN, mr->length,
+ 			      RDMA_NLDEV_ATTR_PAD))
+ 		goto err;
+ 
+ 	if (fill_res_name_pid(msg, res))
+ 		goto err;
+ 
+ 	nla_nest_end(msg, entry_attr);
+ 	return 0;
+ 
+ err:
+ 	nla_nest_cancel(msg, entry_attr);
+ out:
+ 	return -EMSGSIZE;
+ }
+ 
+ static int fill_res_pd_entry(struct sk_buff *msg, struct netlink_callback *cb,
+ 			     struct rdma_restrack_entry *res, uint32_t port)
+ {
+ 	struct ib_pd *pd = container_of(res, struct ib_pd, res);
+ 	struct nlattr *entry_attr;
+ 
+ 	entry_attr = nla_nest_start(msg, RDMA_NLDEV_ATTR_RES_PD_ENTRY);
+ 	if (!entry_attr)
+ 		goto out;
+ 
+ 	if (netlink_capable(cb->skb, CAP_NET_ADMIN)) {
+ 		if (nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_LOCAL_DMA_LKEY,
+ 				pd->local_dma_lkey))
+ 			goto err;
+ 		if ((pd->flags & IB_PD_UNSAFE_GLOBAL_RKEY) &&
+ 		    nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_UNSAFE_GLOBAL_RKEY,
+ 				pd->unsafe_global_rkey))
+ 			goto err;
+ 	}
+ 	if (nla_put_u64_64bit(msg, RDMA_NLDEV_ATTR_RES_USECNT,
+ 			      atomic_read(&pd->usecnt), RDMA_NLDEV_ATTR_PAD))
+ 		goto err;
+ 	if ((pd->flags & IB_PD_UNSAFE_GLOBAL_RKEY) &&
+ 	    nla_put_u32(msg, RDMA_NLDEV_ATTR_RES_UNSAFE_GLOBAL_RKEY,
+ 			pd->unsafe_global_rkey))
+ 		goto err;
+ 
+ 	if (fill_res_name_pid(msg, res))
+ 		goto err;
+ 
+ 	nla_nest_end(msg, entry_attr);
+ 	return 0;
+ 
+ err:
+ 	nla_nest_cancel(msg, entry_attr);
+ out:
+ 	return -EMSGSIZE;
+ }
+ 
+ static int nldev_get_doit(struct sk_buff *skb, struct nlmsghdr *nlh,
+ 			  struct netlink_ext_ack *extack)
++>>>>>>> 25a0ad85156a (RDMA/nldev: Add explicit pad attribute)
  {
  	struct nlattr *tb[RDMA_NLDEV_ATTR_MAX];
  	struct ib_device *device;
* Unmerged path drivers/infiniband/core/nldev.c
diff --git a/include/uapi/rdma/rdma_netlink.h b/include/uapi/rdma/rdma_netlink.h
index b90c8cd4c303..a257c0c198ff 100644
--- a/include/uapi/rdma/rdma_netlink.h
+++ b/include/uapi/rdma/rdma_netlink.h
@@ -244,6 +244,9 @@ enum rdma_nldev_attr {
 	/* don't change the order or add anything between, this is ABI! */
 	RDMA_NLDEV_ATTR_UNSPEC,
 
+	/* Pad attribute for 64b alignment */
+	RDMA_NLDEV_ATTR_PAD = RDMA_NLDEV_ATTR_UNSPEC,
+
 	/* Identifier for ib_device */
 	RDMA_NLDEV_ATTR_DEV_INDEX,		/* u32 */
 
