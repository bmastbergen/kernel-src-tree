mm, percpu: add support for __GFP_NOWARN flag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] percpu: add support for __GFP_NOWARN flag (Jiri Olsa) [1690683]
Rebuild_FUZZ: 95.35%
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 0ea7eeec24be5f04ae80d68f5b1ea3a11f49de2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/0ea7eeec.failed

Add an option for pcpu_alloc() to support __GFP_NOWARN flag.
Currently, we always throw a warning when size or alignment
is unsupported (and also dump stack on failed allocation
requests). The warning itself is harmless since we return
NULL anyway for any failed request, which callers are
required to handle anyway. However, it becomes harmful when
panic_on_warn is set.

The rationale for the WARN() in pcpu_alloc() is that it can
be tracked when larger than supported allocation requests are
made such that allocations limits can be tweaked if warranted.
This makes sense for in-kernel users, however, there are users
of pcpu allocator where allocation size is derived from user
space requests, e.g. when creating BPF maps. In these cases,
the requests should fail gracefully without throwing a splat.

The current work-around was to check allocation size against
the upper limit of PCPU_MIN_UNIT_SIZE from call-sites for
bailing out prior to a call to pcpu_alloc() in order to
avoid throwing the WARN(). This is bad in multiple ways since
PCPU_MIN_UNIT_SIZE is an implementation detail, and having
the checks on call-sites only complicates the code for no
good reason. Thus, lets fix it generically by supporting the
__GFP_NOWARN flag that users can then use with calling the
__alloc_percpu_gfp() helper instead.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 0ea7eeec24be5f04ae80d68f5b1ea3a11f49de2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index b15513d4a101,a0e0c82c1e4c..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -873,24 -1344,28 +877,35 @@@ static void __percpu *pcpu_alloc(size_
  	static int warn_limit = 10;
  	struct pcpu_chunk *chunk;
  	const char *err;
++<<<<<<< HEAD
 +	bool is_atomic = !(gfp & GFP_KERNEL);
 +	int occ_pages = 0;
 +	int slot, off, new_alloc, cpu, ret;
++=======
+ 	int slot, off, cpu, ret;
++>>>>>>> 0ea7eeec24be (mm, percpu: add support for __GFP_NOWARN flag)
  	unsigned long flags;
  	void __percpu *ptr;
 -	size_t bits, bit_align;
  
  	/*
 -	 * There is now a minimum allocation size of PCPU_MIN_ALLOC_SIZE,
 -	 * therefore alignment must be a minimum of that many bytes.
 -	 * An allocation may have internal fragmentation from rounding up
 -	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.
 +	 * We want the lowest bit of offset available for in-use/free
 +	 * indicator, so force >= 16bit alignment and make size even.
  	 */
 -	if (unlikely(align < PCPU_MIN_ALLOC_SIZE))
 -		align = PCPU_MIN_ALLOC_SIZE;
 +	if (unlikely(align < 2))
 +		align = 2;
  
 -	size = ALIGN(size, PCPU_MIN_ALLOC_SIZE);
 -	bits = size >> PCPU_MIN_ALLOC_SHIFT;
 -	bit_align = align >> PCPU_MIN_ALLOC_SHIFT;
 +	size = ALIGN(size, 2);
  
++<<<<<<< HEAD
 +	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {
 +		WARN(true, "illegal size (%zu) or align (%zu) for "
 +		     "percpu allocation\n", size, align);
++=======
+ 	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||
+ 		     !is_power_of_2(align))) {
+ 		WARN(do_warn, "illegal size (%zu) or align (%zu) for percpu allocation\n",
+ 		     size, align);
++>>>>>>> 0ea7eeec24be (mm, percpu: add support for __GFP_NOWARN flag)
  		return NULL;
  	}
  
@@@ -1029,12 -1483,14 +1044,20 @@@ area_found
  fail_unlock:
  	spin_unlock_irqrestore(&pcpu_lock, flags);
  fail:
++<<<<<<< HEAD
 +	if (!is_atomic && warn_limit) {
 +		pr_warning("PERCPU: allocation failed, size=%zu align=%zu atomic=%d, %s\n",
 +			   size, align, is_atomic, err);
++=======
+ 	trace_percpu_alloc_percpu_fail(reserved, is_atomic, size, align);
+ 
+ 	if (!is_atomic && do_warn && warn_limit) {
+ 		pr_warn("allocation failed, size=%zu align=%zu atomic=%d, %s\n",
+ 			size, align, is_atomic, err);
++>>>>>>> 0ea7eeec24be (mm, percpu: add support for __GFP_NOWARN flag)
  		dump_stack();
  		if (!--warn_limit)
 -			pr_info("limit reached, disable warning\n");
 +			pr_info("PERCPU: limit reached, disable warning\n");
  	}
  	if (is_atomic) {
  		/* see the flag handling in pcpu_blance_workfn() */
* Unmerged path mm/percpu.c
