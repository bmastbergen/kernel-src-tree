RDMA/umem: Handle a half-complete start/end sequence

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit be7a57b41ad824dbc59d1ffa91160ee73f2999ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/be7a57b4.failed

mmu_notifier_unregister() can race between a invalidate_start/end and
cause the invalidate_end to be skipped. This causes an imbalance in the
locking, which lockdep complains about.

This is not actually a bug, as we immediately kfree the memory holding the
lock, but it simple enough to fix.

Mark when the notifier is being destroyed and abort the start callback.
This can be done under the lock we already obtained, and can re-purpose
the invalidate_range test we already have.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit be7a57b41ad824dbc59d1ffa91160ee73f2999ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_odp.c
#	include/rdma/ib_umem_odp.h
diff --cc drivers/infiniband/core/umem_odp.c
index 505862826f1c,d7b6422b9611..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -177,18 -126,15 +177,27 @@@ static int ib_umem_notifier_release_tra
  static void ib_umem_notifier_release(struct mmu_notifier *mn,
  				     struct mm_struct *mm)
  {
 -	struct ib_ucontext_per_mm *per_mm =
 -		container_of(mn, struct ib_ucontext_per_mm, mn);
 +	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
  
++<<<<<<< HEAD
 +	if (!context->invalidate_range)
 +		return;
 +
 +	ib_ucontext_notifier_start_account(context);
 +	down_read(&context->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&context->umem_tree, 0,
 +				      ULLONG_MAX,
 +				      ib_umem_notifier_release_trampoline,
 +				      NULL);
 +	up_read(&context->umem_rwsem);
++=======
+ 	down_read(&per_mm->umem_rwsem);
+ 	if (per_mm->active)
+ 		rbt_ib_umem_for_each_in_range(
+ 			&per_mm->umem_tree, 0, ULLONG_MAX,
+ 			ib_umem_notifier_release_trampoline, true, NULL);
+ 	up_read(&per_mm->umem_rwsem);
++>>>>>>> be7a57b41ad8 (RDMA/umem: Handle a half-complete start/end sequence)
  }
  
  static int invalidate_page_trampoline(struct ib_umem_odp *item, u64 start,
@@@ -208,22 -154,33 +217,46 @@@ static int invalidate_range_start_tramp
  	return 0;
  }
  
 -static int ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
 +static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
  						    struct mm_struct *mm,
  						    unsigned long start,
 -						    unsigned long end,
 -						    bool blockable)
 +						    unsigned long end)
  {
++<<<<<<< HEAD
 +	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 +
 +	if (!context->invalidate_range)
 +		return;
 +
 +	ib_ucontext_notifier_start_account(context);
 +	down_read(&context->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&context->umem_tree, start,
 +				      end,
 +				      invalidate_range_start_trampoline, NULL);
 +	up_read(&context->umem_rwsem);
++=======
+ 	struct ib_ucontext_per_mm *per_mm =
+ 		container_of(mn, struct ib_ucontext_per_mm, mn);
+ 
+ 	if (blockable)
+ 		down_read(&per_mm->umem_rwsem);
+ 	else if (!down_read_trylock(&per_mm->umem_rwsem))
+ 		return -EAGAIN;
+ 
+ 	if (!per_mm->active) {
+ 		up_read(&per_mm->umem_rwsem);
+ 		/*
+ 		 * At this point active is permanently set and visible to this
+ 		 * CPU without a lock, that fact is relied on to skip the unlock
+ 		 * in range_end.
+ 		 */
+ 		return 0;
+ 	}
+ 
+ 	return rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start, end,
+ 					     invalidate_range_start_trampoline,
+ 					     blockable, NULL);
++>>>>>>> be7a57b41ad8 (RDMA/umem: Handle a half-complete start/end sequence)
  }
  
  static int invalidate_range_end_trampoline(struct ib_umem_odp *item, u64 start,
@@@ -238,17 -195,16 +271,21 @@@ static void ib_umem_notifier_invalidate
  						  unsigned long start,
  						  unsigned long end)
  {
 -	struct ib_ucontext_per_mm *per_mm =
 -		container_of(mn, struct ib_ucontext_per_mm, mn);
 +	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
  
++<<<<<<< HEAD
 +	if (!context->invalidate_range)
++=======
+ 	if (unlikely(!per_mm->active))
++>>>>>>> be7a57b41ad8 (RDMA/umem: Handle a half-complete start/end sequence)
  		return;
  
 -	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start,
 +	down_read(&context->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&context->umem_tree, start,
  				      end,
 -				      invalidate_range_end_trampoline, true, NULL);
 -	up_read(&per_mm->umem_rwsem);
 +				      invalidate_range_end_trampoline, NULL);
 +	up_read(&context->umem_rwsem);
 +	ib_ucontext_notifier_end_account(context);
  }
  
  static const struct mmu_notifier_ops ib_umem_notifiers = {
@@@ -257,11 -213,138 +294,141 @@@
  	.invalidate_range_end       = ib_umem_notifier_invalidate_range_end,
  };
  
++<<<<<<< HEAD
 +struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
++=======
+ static void add_umem_to_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_insert(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static void remove_umem_from_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_remove(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 	complete_all(&umem_odp->notifier_completion);
+ 
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static struct ib_ucontext_per_mm *alloc_per_mm(struct ib_ucontext *ctx,
+ 					       struct mm_struct *mm)
+ {
+ 	struct ib_ucontext_per_mm *per_mm;
+ 	int ret;
+ 
+ 	per_mm = kzalloc(sizeof(*per_mm), GFP_KERNEL);
+ 	if (!per_mm)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	per_mm->context = ctx;
+ 	per_mm->mm = mm;
+ 	per_mm->umem_tree = RB_ROOT_CACHED;
+ 	init_rwsem(&per_mm->umem_rwsem);
+ 	per_mm->active = ctx->invalidate_range;
+ 
+ 	rcu_read_lock();
+ 	per_mm->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
+ 	rcu_read_unlock();
+ 
+ 	WARN_ON(mm != current->mm);
+ 
+ 	per_mm->mn.ops = &ib_umem_notifiers;
+ 	ret = mmu_notifier_register(&per_mm->mn, per_mm->mm);
+ 	if (ret) {
+ 		dev_err(&ctx->device->dev,
+ 			"Failed to register mmu_notifier %d\n", ret);
+ 		goto out_pid;
+ 	}
+ 
+ 	list_add(&per_mm->ucontext_list, &ctx->per_mm_list);
+ 	return per_mm;
+ 
+ out_pid:
+ 	put_pid(per_mm->tgid);
+ 	kfree(per_mm);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int get_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	struct ib_ucontext_per_mm *per_mm;
+ 
+ 	/*
+ 	 * Generally speaking we expect only one or two per_mm in this list,
+ 	 * so no reason to optimize this search today.
+ 	 */
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	list_for_each_entry(per_mm, &ctx->per_mm_list, ucontext_list) {
+ 		if (per_mm->mm == umem_odp->umem.owning_mm)
+ 			goto found;
+ 	}
+ 
+ 	per_mm = alloc_per_mm(ctx, umem_odp->umem.owning_mm);
+ 	if (IS_ERR(per_mm)) {
+ 		mutex_unlock(&ctx->per_mm_list_lock);
+ 		return PTR_ERR(per_mm);
+ 	}
+ 
+ found:
+ 	umem_odp->per_mm = per_mm;
+ 	per_mm->odp_mrs_count++;
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	return 0;
+ }
+ 
+ void put_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	bool need_free;
+ 
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	umem_odp->per_mm = NULL;
+ 	per_mm->odp_mrs_count--;
+ 	need_free = per_mm->odp_mrs_count == 0;
+ 	if (need_free)
+ 		list_del(&per_mm->ucontext_list);
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	if (!need_free)
+ 		return;
+ 
+ 	/*
+ 	 * NOTE! mmu_notifier_unregister() can happen between a start/end
+ 	 * callback, resulting in an start/end, and thus an unbalanced
+ 	 * lock. This doesn't really matter to us since we are about to kfree
+ 	 * the memory that holds the lock, however LOCKDEP doesn't like this.
+ 	 */
+ 	down_write(&per_mm->umem_rwsem);
+ 	per_mm->active = false;
+ 	up_write(&per_mm->umem_rwsem);
+ 
+ 	mmu_notifier_unregister(&per_mm->mn, per_mm->mm);
+ 	put_pid(per_mm->tgid);
+ 	kfree(per_mm);
+ }
+ 
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext_per_mm *per_mm,
++>>>>>>> be7a57b41ad8 (RDMA/umem: Handle a half-complete start/end sequence)
  				      unsigned long addr, size_t size)
  {
 -	struct ib_ucontext *ctx = per_mm->context;
 -	struct ib_umem_odp *odp_data;
  	struct ib_umem *umem;
 +	struct ib_umem_odp *odp_data;
  	int pages = size >> PAGE_SHIFT;
  	int ret;
  
diff --cc include/rdma/ib_umem_odp.h
index cbe16c505673,ec05c82ead7a..000000000000
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@@ -89,9 -85,24 +89,30 @@@ static inline struct ib_umem_odp *to_ib
  
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
  
++<<<<<<< HEAD
 +int ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem,
 +		    int access);
 +struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
++=======
+ struct ib_ucontext_per_mm {
+ 	struct ib_ucontext *context;
+ 	struct mm_struct *mm;
+ 	struct pid *tgid;
+ 	bool active;
+ 
+ 	struct rb_root_cached umem_tree;
+ 	/* Protects umem_tree */
+ 	struct rw_semaphore umem_rwsem;
+ 
+ 	struct mmu_notifier mn;
+ 	unsigned int odp_mrs_count;
+ 
+ 	struct list_head ucontext_list;
+ };
+ 
+ int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext_per_mm *per_mm,
++>>>>>>> be7a57b41ad8 (RDMA/umem: Handle a half-complete start/end sequence)
  				      unsigned long addr, size_t size);
  void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
  
* Unmerged path drivers/infiniband/core/umem_odp.c
* Unmerged path include/rdma/ib_umem_odp.h
