dax: remove VM_MIXEDMAP for fsdax and device dax

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Dave Jiang <dave.jiang@intel.com>
commit e1fb4a0864958fac2fb1b23f9f4562a9f90e3e8f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/e1fb4a08.failed

This patch is reworked from an earlier patch that Dan has posted:
https://patchwork.kernel.org/patch/10131727/

VM_MIXEDMAP is used by dax to direct mm paths like vm_normal_page() that
the memory page it is dealing with is not typical memory from the linear
map.  The get_user_pages_fast() path, since it does not resolve the vma,
is already using {pte,pmd}_devmap() as a stand-in for VM_MIXEDMAP, so we
use that as a VM_MIXEDMAP replacement in some locations.  In the cases
where there is no pte to consult we fallback to using vma_is_dax() to
detect the VM_MIXEDMAP special case.

Now that we have explicit driver pfn_t-flag opt-in/opt-out for
get_user_pages() support for DAX we can stop setting VM_MIXEDMAP.  This
also means we no longer need to worry about safely manipulating vm_flags
in a future where we support dynamically changing the dax mode of a
file.

DAX should also now be supported with madvise_behavior(), vma_merge(),
and copy_page_range().

This patch has been tested against ndctl unit test.  It has also been
tested against xfstests commit: 625515d using fake pmem created by
memmap and no additional issues have been observed.

Link: http://lkml.kernel.org/r/152847720311.55924.16999195879201817653.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Acked-by: Dan Williams <dan.j.williams@intel.com>
	Cc: Jan Kara <jack@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e1fb4a0864958fac2fb1b23f9f4562a9f90e3e8f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/device.c
#	fs/ext2/file.c
#	fs/ext4/file.c
#	fs/xfs/xfs_file.c
#	mm/hmm.c
#	mm/memory.c
diff --cc drivers/dax/device.c
index a4b33e636d9f,0a2acd7993f0..000000000000
--- a/drivers/dax/device.c
+++ b/drivers/dax/device.c
@@@ -490,9 -474,7 +490,13 @@@ static int dax_mmap(struct file *filp, 
  		return rc;
  
  	vma->vm_ops = &dax_vm_ops;
++<<<<<<< HEAD
 +	vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 +	vma->vm_flags2 |= VM_PFN_MKWRITE | VM_HUGE_FAULT |
 +		VM_SPLIT | VM_PAGESIZE;
++=======
+ 	vma->vm_flags |= VM_HUGEPAGE;
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	return 0;
  }
  
diff --cc fs/ext2/file.c
index f758e8c98ab9,28b2609f25c1..000000000000
--- a/fs/ext2/file.c
+++ b/fs/ext2/file.c
@@@ -94,8 -126,6 +94,11 @@@ static int ext2_file_mmap(struct file *
  
  	file_accessed(file);
  	vma->vm_ops = &ext2_dax_vm_ops;
++<<<<<<< HEAD
 +	vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 +	vma->vm_flags2 |= VM_PFN_MKWRITE | VM_HUGE_FAULT;
++=======
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	return 0;
  }
  #else
diff --cc fs/ext4/file.c
index d6569374d80b,69d65d49837b..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -364,8 -374,7 +364,12 @@@ static int ext4_file_mmap(struct file *
  	file_accessed(file);
  	if (IS_DAX(file_inode(file))) {
  		vma->vm_ops = &ext4_dax_vm_ops;
++<<<<<<< HEAD
 +		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 +		vma->vm_flags2 |= VM_PFN_MKWRITE | VM_HUGE_FAULT;
++=======
+ 		vma->vm_flags |= VM_HUGEPAGE;
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	} else {
  		vma->vm_ops = &ext4_file_vm_ops;
  	}
diff --cc fs/xfs/xfs_file.c
index 374d446488ef,5eaef2c17293..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1276,8 -1169,7 +1276,12 @@@ xfs_file_mmap
  	file_accessed(filp);
  	vma->vm_ops = &xfs_file_vm_ops;
  	if (IS_DAX(file_inode(filp)))
++<<<<<<< HEAD
 +		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 +	vma->vm_flags2 |= VM_PFN_MKWRITE | VM_HUGE_FAULT;
++=======
+ 		vma->vm_flags |= VM_HUGEPAGE;
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	return 0;
  }
  
diff --cc mm/hmm.c
index 8c246333b466,f40e8add84b5..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -511,10 -675,25 +511,31 @@@ int hmm_vma_get_pfns(struct vm_area_str
  	if (!hmm->mmu_notifier.ops)
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	/* FIXME support hugetlb fs */
+ 	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
+ 			vma_is_dax(vma)) {
+ 		hmm_pfns_special(range);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!(vma->vm_flags & VM_READ)) {
+ 		/*
+ 		 * If vma do not allow read access, then assume that it does
+ 		 * not allow write access, either. Architecture that allow
+ 		 * write without read access are not supported by HMM, because
+ 		 * operations such has atomic access would not work.
+ 		 */
+ 		hmm_pfns_clear(range, range->pfns, range->start, range->end);
+ 		return -EPERM;
+ 	}
+ 
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	/* Initialize range to track CPU page table update */
 +	range->start = start;
 +	range->pfns = pfns;
 +	range->end = end;
  	spin_lock(&hmm->lock);
  	range->valid = true;
  	list_add_rcu(&range->list, &hmm->ranges);
@@@ -679,10 -849,25 +700,31 @@@ int hmm_vma_fault(struct vm_area_struc
  	if (!hmm->mmu_notifier.ops)
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	/* FIXME support hugetlb fs */
+ 	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
+ 			vma_is_dax(vma)) {
+ 		hmm_pfns_special(range);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!(vma->vm_flags & VM_READ)) {
+ 		/*
+ 		 * If vma do not allow read access, then assume that it does
+ 		 * not allow write access, either. Architecture that allow
+ 		 * write without read access are not supported by HMM, because
+ 		 * operations such has atomic access would not work.
+ 		 */
+ 		hmm_pfns_clear(range, range->pfns, range->start, range->end);
+ 		return -EPERM;
+ 	}
+ 
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  	/* Initialize range to track CPU page table update */
 +	range->start = start;
 +	range->pfns = pfns;
 +	range->end = end;
  	spin_lock(&hmm->lock);
  	range->valid = true;
  	list_add_rcu(&range->list, &hmm->ranges);
diff --cc mm/memory.c
index 992044080678,7c3bd119fcca..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -783,13 -828,42 +783,45 @@@ struct page *vm_normal_page(struct vm_a
  {
  	unsigned long pfn = pte_pfn(pte);
  
 -	if (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {
 +	if (HAVE_PTE_SPECIAL) {
  		if (likely(!pte_special(pte)))
  			goto check_pfn;
 -		if (vma->vm_ops && vma->vm_ops->find_special_page)
 -			return vma->vm_ops->find_special_page(vma, addr);
  		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))
  			return NULL;
++<<<<<<< HEAD
 +		if (!is_zero_pfn(pfn))
 +			print_bad_pte(vma, addr, pte, NULL);
++=======
+ 		if (is_zero_pfn(pfn))
+ 			return NULL;
+ 
+ 		/*
+ 		 * Device public pages are special pages (they are ZONE_DEVICE
+ 		 * pages but different from persistent memory). They behave
+ 		 * allmost like normal pages. The difference is that they are
+ 		 * not on the lru and thus should never be involve with any-
+ 		 * thing that involve lru manipulation (mlock, numa balancing,
+ 		 * ...).
+ 		 *
+ 		 * This is why we still want to return NULL for such page from
+ 		 * vm_normal_page() so that we do not have to special case all
+ 		 * call site of vm_normal_page().
+ 		 */
+ 		if (likely(pfn <= highest_memmap_pfn)) {
+ 			struct page *page = pfn_to_page(pfn);
+ 
+ 			if (is_device_public_page(page)) {
+ 				if (with_public_device)
+ 					return page;
+ 				return NULL;
+ 			}
+ 		}
+ 
+ 		if (pte_devmap(pte))
+ 			return NULL;
+ 
+ 		print_bad_pte(vma, addr, pte, NULL);
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  		return NULL;
  	}
  
@@@ -826,6 -901,48 +858,51 @@@ out
  	return pfn_to_page(pfn);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
+ 				pmd_t pmd)
+ {
+ 	unsigned long pfn = pmd_pfn(pmd);
+ 
+ 	/*
+ 	 * There is no pmd_special() but there may be special pmds, e.g.
+ 	 * in a direct-access (dax) mapping, so let's just replicate the
+ 	 * !CONFIG_ARCH_HAS_PTE_SPECIAL case from vm_normal_page() here.
+ 	 */
+ 	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
+ 		if (vma->vm_flags & VM_MIXEDMAP) {
+ 			if (!pfn_valid(pfn))
+ 				return NULL;
+ 			goto out;
+ 		} else {
+ 			unsigned long off;
+ 			off = (addr - vma->vm_start) >> PAGE_SHIFT;
+ 			if (pfn == vma->vm_pgoff + off)
+ 				return NULL;
+ 			if (!is_cow_mapping(vma->vm_flags))
+ 				return NULL;
+ 		}
+ 	}
+ 
+ 	if (pmd_devmap(pmd))
+ 		return NULL;
+ 	if (is_zero_pfn(pfn))
+ 		return NULL;
+ 	if (unlikely(pfn > highest_memmap_pfn))
+ 		return NULL;
+ 
+ 	/*
+ 	 * NOTE! We still have PageReserved() pages in the page tables.
+ 	 * eg. VDSO mappings can cause them to exist.
+ 	 */
+ out:
+ 	return pfn_to_page(pfn);
+ }
+ #endif
+ 
++>>>>>>> e1fb4a086495 (dax: remove VM_MIXEDMAP for fsdax and device dax)
  /*
   * copy one vm_area from one task to the other. Assumes the page tables
   * already present in the new task to be cleared in the whole range
* Unmerged path drivers/dax/device.c
* Unmerged path fs/ext2/file.c
* Unmerged path fs/ext4/file.c
* Unmerged path fs/xfs/xfs_file.c
* Unmerged path mm/hmm.c
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 5f95a9b1998d..813eb21f469d 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -937,11 +937,11 @@ int vmf_insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,
 	 * but we need to be consistent with PTEs and architectures that
 	 * can't support a 'special' bit.
 	 */
-	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
+	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) &&
+			!pfn_t_devmap(pfn));
 	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
 						(VM_PFNMAP|VM_MIXEDMAP));
 	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
-	BUG_ON(!pfn_t_devmap(pfn));
 
 	if (addr < vma->vm_start || addr >= vma->vm_end)
 		return VM_FAULT_SIGBUS;
diff --git a/mm/ksm.c b/mm/ksm.c
index aaa161fc9a0d..cdaf5ba8a5e1 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -2360,6 +2360,9 @@ int ksm_madvise(struct vm_area_struct *vma, unsigned long start,
 				 VM_HUGETLB | VM_NONLINEAR | VM_MIXEDMAP))
 			return 0;		/* just ignore the advice */
 
+		if (vma_is_dax(vma))
+			return 0;
+
 #ifdef VM_SAO
 		if (*vm_flags & VM_SAO)
 			return 0;
* Unmerged path mm/memory.c
diff --git a/mm/migrate.c b/mm/migrate.c
index dc35415df847..5be3c2a0c122 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2660,7 +2660,8 @@ int migrate_vma(const struct migrate_vma_ops *ops,
 	/* Sanity check the arguments */
 	start &= PAGE_MASK;
 	end &= PAGE_MASK;
-	if (!vma || is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL))
+	if (!vma || is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL) ||
+			vma_is_dax(vma))
 		return -EINVAL;
 	if (start < vma->vm_start || start >= vma->vm_end)
 		return -EINVAL;
diff --git a/mm/mlock.c b/mm/mlock.c
index a6e7b7820b31..893e70c9bba6 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -507,7 +507,8 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
 	vm_flags_t old_flags = vma->vm_flags;
 
 	if (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||
-	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))
+	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm) ||
+	    vma_is_dax(vma))
 		/* don't set VM_LOCKED or VM_LOCKONFAULT and don't count */
 		goto out;
 
diff --git a/mm/mmap.c b/mm/mmap.c
index f258137dd11a..01682551d1bc 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1838,11 +1838,12 @@ out:
 
 	vm_stat_account(mm, vm_flags, file, len >> PAGE_SHIFT);
 	if (vm_flags & VM_LOCKED) {
-		if (!((vm_flags & VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
-					vma == get_gate_vma(current->mm)))
-			mm->locked_vm += (len >> PAGE_SHIFT);
-		else
+		if ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||
+					is_vm_hugetlb_page(vma) ||
+					vma == get_gate_vma(current->mm))
 			vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
+		else
+			mm->locked_vm += (len >> PAGE_SHIFT);
 	}
 
 	if (file)
