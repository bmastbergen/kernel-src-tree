sched/fair: Fix bandwidth timer clock drift condition

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Xunlei Pang <xlpang@linux.alibaba.com>
commit 512ac999d2755d2b7109e996a76b6fb8b888631d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/512ac999.failed

I noticed that cgroup task groups constantly get throttled even
if they have low CPU usage, this causes some jitters on the response
time to some of our business containers when enabling CPU quotas.

It's very simple to reproduce:

  mkdir /sys/fs/cgroup/cpu/test
  cd /sys/fs/cgroup/cpu/test
  echo 100000 > cpu.cfs_quota_us
  echo $$ > tasks

then repeat:

  cat cpu.stat | grep nr_throttled  # nr_throttled will increase steadily

After some analysis, we found that cfs_rq::runtime_remaining will
be cleared by expire_cfs_rq_runtime() due to two equal but stale
"cfs_{b|q}->runtime_expires" after period timer is re-armed.

The current condition to judge clock drift in expire_cfs_rq_runtime()
is wrong, the two runtime_expires are actually the same when clock
drift happens, so this condtion can never hit. The orginal design was
correctly done by this commit:

  a9cf55b28610 ("sched: Expire invalid runtime")

... but was changed to be the current implementation due to its locking bug.

This patch introduces another way, it adds a new field in both structures
cfs_rq and cfs_bandwidth to record the expiration update sequence, and
uses them to figure out if clock drift happens (true if they are equal).

	Signed-off-by: Xunlei Pang <xlpang@linux.alibaba.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Ben Segall <bsegall@google.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Fixes: 51f2176d74ac ("sched/fair: Fix unlocked reads of some cfs_b->quota/period")
Link: http://lkml.kernel.org/r/20180620101834.24455-1-xlpang@linux.alibaba.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 512ac999d2755d2b7109e996a76b6fb8b888631d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/sched.h
diff --cc kernel/sched/sched.h
index 7969a5cf1ec6,c7742dcc136c..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -227,19 -328,24 +227,35 @@@ extern struct list_head task_groups
  
  struct cfs_bandwidth {
  #ifdef CONFIG_CFS_BANDWIDTH
++<<<<<<< HEAD
 +	raw_spinlock_t lock;
 +	ktime_t period;
 +	u64 quota, runtime;
 +	s64 hierarchal_quota;
 +	u64 runtime_expires;
 +
 +	int idle, timer_active;
 +	struct hrtimer period_timer, slack_timer;
 +	struct list_head throttled_cfs_rq;
++=======
+ 	raw_spinlock_t		lock;
+ 	ktime_t			period;
+ 	u64			quota;
+ 	u64			runtime;
+ 	s64			hierarchical_quota;
+ 	u64			runtime_expires;
+ 	int			expires_seq;
+ 
+ 	short			idle;
+ 	short			period_active;
+ 	struct hrtimer		period_timer;
+ 	struct hrtimer		slack_timer;
+ 	struct list_head	throttled_cfs_rq;
++>>>>>>> 512ac999d275 (sched/fair: Fix bandwidth timer clock drift condition)
  
 -	/* Statistics: */
 -	int			nr_periods;
 -	int			nr_throttled;
 -	u64			throttled_time;
 +	/* statistics */
 +	int nr_periods, nr_throttled;
 +	u64 throttled_time;
  #endif
  };
  
@@@ -424,27 -543,26 +440,34 @@@ struct cfs_rq 
  	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
  	 * (like users, containers etc.)
  	 *
 -	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
 -	 * This list is used during load balance.
 +	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
 +	 * list is used during load balance.
  	 */
 -	int			on_list;
 -	struct list_head	leaf_cfs_rq_list;
 -	struct task_group	*tg;	/* group that "owns" this runqueue */
 +	int on_list;
 +	struct list_head leaf_cfs_rq_list;
 +	struct task_group *tg;	/* group that "owns" this runqueue */
  
  #ifdef CONFIG_CFS_BANDWIDTH
++<<<<<<< HEAD
 +	int runtime_enabled;
 +	u64 runtime_expires;
 +	s64 runtime_remaining;
++=======
+ 	int			runtime_enabled;
+ 	int			expires_seq;
+ 	u64			runtime_expires;
+ 	s64			runtime_remaining;
++>>>>>>> 512ac999d275 (sched/fair: Fix bandwidth timer clock drift condition)
  
 -	u64			throttled_clock;
 -	u64			throttled_clock_task;
 -	u64			throttled_clock_task_time;
 -	int			throttled;
 -	int			throttle_count;
 -	struct list_head	throttled_list;
 +	u64 throttled_clock, throttled_clock_task;
 +	u64 throttled_clock_task_time;
 +	int throttled, throttle_count;
 +	struct list_head throttled_list;
  #endif /* CONFIG_CFS_BANDWIDTH */
 +#ifdef CONFIG_SMP
 +	RH_KABI_EXTEND(u64 last_h_load_update)
 +	RH_KABI_EXTEND(struct sched_entity *h_load_next)
 +#endif
  #endif /* CONFIG_FAIR_GROUP_SCHED */
  };
  
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c019c17cc2ca..b45a94f67ea6 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3206,6 +3206,7 @@ void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)
 	now = sched_clock_cpu(smp_processor_id());
 	cfs_b->runtime = cfs_b->quota;
 	cfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);
+	cfs_b->expires_seq++;
 }
 
 static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)
@@ -3228,6 +3229,7 @@ static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 	struct task_group *tg = cfs_rq->tg;
 	struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);
 	u64 amount = 0, min_amount, expires;
+	int expires_seq;
 
 	/* note: this is a positive sum as runtime_remaining <= 0 */
 	min_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;
@@ -3253,6 +3255,7 @@ static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 			cfs_b->idle = 0;
 		}
 	}
+	expires_seq = cfs_b->expires_seq;
 	expires = cfs_b->runtime_expires;
 	raw_spin_unlock(&cfs_b->lock);
 
@@ -3262,8 +3265,10 @@ static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 	 * spread between our sched_clock and the one on which runtime was
 	 * issued.
 	 */
-	if ((s64)(expires - cfs_rq->runtime_expires) > 0)
+	if (cfs_rq->expires_seq != expires_seq) {
+		cfs_rq->expires_seq = expires_seq;
 		cfs_rq->runtime_expires = expires;
+	}
 
 	return cfs_rq->runtime_remaining > 0;
 }
@@ -3289,12 +3294,9 @@ static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 	 * has not truly expired.
 	 *
 	 * Fortunately we can check determine whether this the case by checking
-	 * whether the global deadline has advanced. It is valid to compare
-	 * cfs_b->runtime_expires without any locks since we only care about
-	 * exact equality, so a partial write will still work.
+	 * whether the global deadline(cfs_b->expires_seq) has advanced.
 	 */
-
-	if (cfs_rq->runtime_expires != cfs_b->runtime_expires) {
+	if (cfs_rq->expires_seq == cfs_b->expires_seq) {
 		/* extend local deadline, drift is bounded above by 2 ticks */
 		cfs_rq->runtime_expires += TICK_NSEC;
 	} else {
* Unmerged path kernel/sched/sched.h
