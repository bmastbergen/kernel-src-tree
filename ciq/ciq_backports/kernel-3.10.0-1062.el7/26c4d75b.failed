x86/speculation: Rename SSBD update functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] speculation: Rename SSBD update functions (Waiman Long) [1671826]
Rebuild_FUZZ: 95.35%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 26c4d75b234040c11728a8acb796b3a85ba7507c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/26c4d75b.failed

During context switch, the SSBD bit in SPEC_CTRL MSR is updated according
to changes of the TIF_SSBD flag in the current and next running task.

Currently, only the bit controlling speculative store bypass disable in
SPEC_CTRL MSR is updated and the related update functions all have
"speculative_store" or "ssb" in their names.

For enhanced mitigation control other bits in SPEC_CTRL MSR need to be
updated as well, which makes the SSB names inadequate.

Rename the "speculative_store*" functions to a more generic name. No
functional change.

	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: David Woodhouse <dwmw@amazon.co.uk>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Casey Schaufler <casey.schaufler@intel.com>
	Cc: Asit Mallick <asit.k.mallick@intel.com>
	Cc: Arjan van de Ven <arjan@linux.intel.com>
	Cc: Jon Masters <jcm@redhat.com>
	Cc: Waiman Long <longman9394@gmail.com>
	Cc: Greg KH <gregkh@linuxfoundation.org>
	Cc: Dave Stewart <david.c.stewart@intel.com>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: stable@vger.kernel.org
Link: https://lkml.kernel.org/r/20181125185004.058866968@linutronix.de



(cherry picked from commit 26c4d75b234040c11728a8acb796b3a85ba7507c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/spec-ctrl.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/process.c
diff --cc arch/x86/kernel/cpu/bugs.c
index 12a489be7869,a723af0c4400..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -101,7 -123,89 +101,93 @@@ void __init check_bugs(void
  #endif
  }
  
++<<<<<<< HEAD
 +void x86_amd_ssbd_enable(void)
++=======
+ /* The kernel command line selection */
+ enum spectre_v2_mitigation_cmd {
+ 	SPECTRE_V2_CMD_NONE,
+ 	SPECTRE_V2_CMD_AUTO,
+ 	SPECTRE_V2_CMD_FORCE,
+ 	SPECTRE_V2_CMD_RETPOLINE,
+ 	SPECTRE_V2_CMD_RETPOLINE_GENERIC,
+ 	SPECTRE_V2_CMD_RETPOLINE_AMD,
+ };
+ 
+ static const char *spectre_v2_strings[] = {
+ 	[SPECTRE_V2_NONE]			= "Vulnerable",
+ 	[SPECTRE_V2_RETPOLINE_GENERIC]		= "Mitigation: Full generic retpoline",
+ 	[SPECTRE_V2_RETPOLINE_AMD]		= "Mitigation: Full AMD retpoline",
+ 	[SPECTRE_V2_IBRS_ENHANCED]		= "Mitigation: Enhanced IBRS",
+ };
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Spectre V2 : " fmt
+ 
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
+ void
+ x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
+ {
+ 	u64 msrval, guestval, hostval = x86_spec_ctrl_base;
+ 	struct thread_info *ti = current_thread_info();
+ 
+ 	/* Is MSR_SPEC_CTRL implemented ? */
+ 	if (static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL)) {
+ 		/*
+ 		 * Restrict guest_spec_ctrl to supported values. Clear the
+ 		 * modifiable bits in the host base value and or the
+ 		 * modifiable bits from the guest value.
+ 		 */
+ 		guestval = hostval & ~x86_spec_ctrl_mask;
+ 		guestval |= guest_spec_ctrl & x86_spec_ctrl_mask;
+ 
+ 		/* SSBD controlled in MSR_SPEC_CTRL */
+ 		if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||
+ 		    static_cpu_has(X86_FEATURE_AMD_SSBD))
+ 			hostval |= ssbd_tif_to_spec_ctrl(ti->flags);
+ 
+ 		if (hostval != guestval) {
+ 			msrval = setguest ? guestval : hostval;
+ 			wrmsrl(MSR_IA32_SPEC_CTRL, msrval);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If SSBD is not handled in MSR_SPEC_CTRL on AMD, update
+ 	 * MSR_AMD64_L2_CFG or MSR_VIRT_SPEC_CTRL if supported.
+ 	 */
+ 	if (!static_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&
+ 	    !static_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		return;
+ 
+ 	/*
+ 	 * If the host has SSBD mitigation enabled, force it in the host's
+ 	 * virtual MSR value. If its not permanently enabled, evaluate
+ 	 * current's TIF_SSBD thread flag.
+ 	 */
+ 	if (static_cpu_has(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE))
+ 		hostval = SPEC_CTRL_SSBD;
+ 	else
+ 		hostval = ssbd_tif_to_spec_ctrl(ti->flags);
+ 
+ 	/* Sanitize the guest value */
+ 	guestval = guest_virt_spec_ctrl & SPEC_CTRL_SSBD;
+ 
+ 	if (hostval != guestval) {
+ 		unsigned long tif;
+ 
+ 		tif = setguest ? ssbd_spec_ctrl_to_tif(guestval) :
+ 				 ssbd_spec_ctrl_to_tif(hostval);
+ 
+ 		speculation_ctrl_update(tif);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(x86_virt_spec_ctrl);
+ 
+ static void x86_amd_ssb_disable(void)
++>>>>>>> 26c4d75b2340 (x86/speculation: Rename SSBD update functions)
  {
  	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;
  
diff --cc arch/x86/kernel/process.c
index 5ad9710f0312,8aa49604f9ae..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -315,32 -395,29 +315,32 @@@ static __always_inline void amd_set_ssb
  	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
  }
  
- static __always_inline void intel_set_ssb_state(unsigned long tifn)
+ static __always_inline void spec_ctrl_update_msr(unsigned long tifn)
  {
 -	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
 -
 -	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 +	spec_ctrl_set_ssbd(ssbd_tif_to_spec_ctrl(tifn));
 +	wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl_pcp.entry64));
  }
  
- static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+ static __always_inline void __speculation_ctrl_update(unsigned long tifn)
  {
 +	if (!static_key_false(&ssbd_userset_key))
 +		return;	/* Don't do anything if not user settable */
 +
  	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
  		amd_set_ssb_virt_state(tifn);
  	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
  		amd_set_core_ssb_state(tifn);
  	else
- 		intel_set_ssb_state(tifn);
+ 		spec_ctrl_update_msr(tifn);
  }
  
- void speculative_store_bypass_update(unsigned long tif)
+ void speculation_ctrl_update(unsigned long tif)
  {
  	preempt_disable();
- 	__speculative_store_bypass_update(tif);
+ 	__speculation_ctrl_update(tif);
  	preempt_enable();
  }
 +EXPORT_SYMBOL_GPL(speculative_store_bypass_update);
  
  void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
  		      struct tss_struct *tss)
@@@ -350,44 -428,31 +350,49 @@@
  	prev = &prev_p->thread;
  	next = &next_p->thread;
  
 -	tifn = READ_ONCE(task_thread_info(next_p)->flags);
 -	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
 -	switch_to_bitmap(tss, prev, next, tifp, tifn);
 +	if (test_tsk_thread_flag(prev_p, TIF_BLOCKSTEP) ^
 +	    test_tsk_thread_flag(next_p, TIF_BLOCKSTEP)) {
 +		unsigned long debugctl = get_debugctlmsr();
  
 -	propagate_user_return_notify(prev_p, next_p);
 -
 -	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&
 -	    arch_has_block_step()) {
 -		unsigned long debugctl, msk;
 -
 -		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
  		debugctl &= ~DEBUGCTLMSR_BTF;
 -		msk = tifn & _TIF_BLOCKSTEP;
 -		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;
 -		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 +		if (test_tsk_thread_flag(next_p, TIF_BLOCKSTEP))
 +			debugctl |= DEBUGCTLMSR_BTF;
 +
 +		update_debugctlmsr(debugctl);
  	}
  
 -	if ((tifp ^ tifn) & _TIF_NOTSC)
 -		cr4_toggle_bits_irqsoff(X86_CR4_TSD);
 +	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
 +	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
 +		/* prev and next are different */
 +		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
 +			hard_disable_TSC();
 +		else
 +			hard_enable_TSC();
 +	}
  
 -	if ((tifp ^ tifn) & _TIF_NOCPUID)
 -		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 +	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
 +		/*
 +		 * Copy the relevant range of the IO bitmap.
 +		 * Normally this is 128 bytes or less:
 +		 */
 +		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
 +		       max(prev->io_bitmap_max, next->io_bitmap_max));
 +	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
 +		/*
 +		 * Clear any possible leftover bits:
 +		 */
 +		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 +	}
 +	propagate_user_return_notify(prev_p, next_p);
  
++<<<<<<< HEAD
 +	if (test_tsk_thread_flag(prev_p, TIF_SSBD) ^
 +	    test_tsk_thread_flag(next_p, TIF_SSBD))
 +		__speculative_store_bypass_update(task_thread_info(next_p)->flags);
++=======
+ 	if ((tifp ^ tifn) & _TIF_SSBD)
+ 		__speculation_ctrl_update(tifn);
++>>>>>>> 26c4d75b2340 (x86/speculation: Rename SSBD update functions)
  }
  
  /*
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/process.c
