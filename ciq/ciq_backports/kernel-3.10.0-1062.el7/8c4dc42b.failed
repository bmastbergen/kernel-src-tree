net/mlx5e: Support multiple encapsulations for a TC flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Support multiple encapsulations for a TC flow (Alaa Hleihel) [1642383 1642498]
Rebuild_FUZZ: 96.30%
commit-author Eli Britstein <elibr@mellanox.com>
commit 8c4dc42bf6e4ffeda49cf5e26bfc991b548fc0aa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/8c4dc42b.failed

Currently a flow is associated with a single encap structure. The FW
extended destination features enables the driver to associate a flow
with multiple encap instances.

Change the encap id field from a flow scope to a per destination value
in the flow attributes struct. Use the encaps array to associate a flow
table entry with multiple encap entries.

Update the neigh logic to offload only if all encapsulations used in a
flow are connected, and un-offload upon the first one disconnected.

Note that the driver can now support up to two encap destinations.

	Signed-off-by: Eli Britstein <elibr@mellanox.com>
	Reviewed-by: Oz Shlomo <ozsh@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 8c4dc42bf6e4ffeda49cf5e26bfc991b548fc0aa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index fbb4f1b36627,0921213561cb..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -811,35 -849,133 +811,82 @@@ static int mlx5e_attach_encap(struct ml
  			      struct ip_tunnel_info *tun_info,
  			      struct net_device *mirred_dev,
  			      struct net_device **encap_dev,
++<<<<<<< HEAD
 +			      struct mlx5e_tc_flow *flow);
++=======
+ 			      struct mlx5e_tc_flow *flow,
+ 			      struct netlink_ext_ack *extack,
+ 			      int out_index);
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  
  static struct mlx5_flow_handle *
 -mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
 -			   struct mlx5e_tc_flow *flow,
 -			   struct mlx5_flow_spec *spec,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	rule = mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
 -	if (IS_ERR(rule))
 -		return rule;
 -
 -	if (attr->split_count) {
 -		flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, spec, attr);
 -		if (IS_ERR(flow->rule[1])) {
 -			mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
 -			return flow->rule[1];
 -		}
 -	}
 -
 -	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
 -			     struct mlx5e_tc_flow *flow,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 -
 -	if (attr->split_count)
 -		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 -
 -	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 -}
 -
 -static struct mlx5_flow_handle *
 -mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct mlx5_flow_spec *spec,
 -			      struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
 -	slow_attr->split_count = 0,
 -	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN,
 -
 -	rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, slow_attr);
 -	if (!IS_ERR(rule))
 -		flow->flags |= MLX5E_TC_FLOW_SLOW;
 -
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
 -				  struct mlx5e_tc_flow *flow,
 -				  struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
 -	flow->flags &= ~MLX5E_TC_FLOW_SLOW;
 -}
 -
 -static int
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
  		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	u32 max_chain = mlx5_eswitch_get_chain_range(esw);
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 -	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
  	struct net_device *out_dev, *encap_dev = NULL;
 -	struct mlx5_fc *counter = NULL;
 +	struct mlx5_flow_handle *rule = NULL;
  	struct mlx5e_rep_priv *rpriv;
  	struct mlx5e_priv *out_priv;
++<<<<<<< HEAD
 +	int err;
 +
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP) {
 +		out_dev = __dev_get_by_index(dev_net(priv->netdev),
 +					     attr->parse_attr->mirred_ifindex);
 +		err = mlx5e_attach_encap(priv, &parse_attr->tun_info,
 +					 out_dev, &encap_dev, flow);
 +		if (err) {
 +			rule = ERR_PTR(err);
 +			if (err != -EAGAIN)
 +				goto err_attach_encap;
 +		}
++=======
+ 	int err = 0, encap_err = 0;
+ 	int out_index;
+ 
+ 	/* if prios are not supported, keep the old behaviour of using same prio
+ 	 * for all offloaded rules.
+ 	 */
+ 	if (!mlx5_eswitch_prios_supported(esw))
+ 		attr->prio = 1;
+ 
+ 	if (attr->chain > max_chain) {
+ 		NL_SET_ERR_MSG(extack, "Requested chain is out of supported range");
+ 		err = -EOPNOTSUPP;
+ 		goto err_max_prio_chain;
+ 	}
+ 
+ 	if (attr->prio > max_prio) {
+ 		NL_SET_ERR_MSG(extack, "Requested priority is out of supported range");
+ 		err = -EOPNOTSUPP;
+ 		goto err_max_prio_chain;
+ 	}
+ 
+ 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {
+ 		int mirred_ifindex;
+ 
+ 		if (!(attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP))
+ 			continue;
+ 
+ 		mirred_ifindex = attr->parse_attr->mirred_ifindex[out_index];
+ 		out_dev = __dev_get_by_index(dev_net(priv->netdev),
+ 					     mirred_ifindex);
+ 		err = mlx5e_attach_encap(priv,
+ 					 &parse_attr->tun_info[out_index],
+ 					 out_dev, &encap_dev, flow,
+ 					 extack, out_index);
+ 		if (err && err != -EAGAIN)
+ 			goto err_attach_encap;
+ 		if (err == -EAGAIN)
+ 			encap_err = err;
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  		out_priv = netdev_priv(encap_dev);
  		rpriv = out_priv->ppriv;
 -		attr->dests[out_index].rep = rpriv->rep;
 -		attr->dests[out_index].mdev = out_priv->mdev;
 +		attr->out_rep = rpriv->rep;
 +		attr->out_mdev = out_priv->mdev;
  	}
  
  	err = mlx5_eswitch_add_vlan_action(esw, attr);
@@@ -873,10 -1026,12 +920,16 @@@ err_add_rule
  err_mod_hdr:
  	mlx5_eswitch_del_vlan_action(esw, attr);
  err_add_vlan:
++<<<<<<< HEAD
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP)
 +		mlx5e_detach_encap(priv, flow);
++=======
+ 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++)
+ 		if (attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP)
+ 			mlx5e_detach_encap(priv, flow, out_index);
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  err_attach_encap:
 -err_max_prio_chain:
 -	return err;
 +	return rule;
  }
  
  static void mlx5e_tc_del_fdb_flow(struct mlx5e_priv *priv,
@@@ -892,10 -1051,10 +945,17 @@@
  
  	mlx5_eswitch_del_vlan_action(esw, attr);
  
++<<<<<<< HEAD
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP) {
 +		mlx5e_detach_encap(priv, flow);
 +		kvfree(attr->parse_attr);
 +	}
++=======
+ 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++)
+ 		if (attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP)
+ 			mlx5e_detach_encap(priv, flow, out_index);
+ 	kvfree(attr->parse_attr);
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
  		mlx5e_detach_mod_hdr(priv, flow);
@@@ -920,12 -1087,35 +980,44 @@@ void mlx5e_tc_encap_flows_add(struct ml
  	e->flags |= MLX5_ENCAP_ENTRY_VALID;
  	mlx5e_rep_queue_neigh_stats_work(priv);
  
++<<<<<<< HEAD
 +	list_for_each_entry(flow, &e->flows, encap) {
 +		esw_attr = flow->esw_attr;
 +		esw_attr->encap_id = e->encap_id;
 +		flow->rule = mlx5_eswitch_add_offloaded_rule(esw, &esw_attr->parse_attr->spec, esw_attr);
 +		if (IS_ERR(flow->rule)) {
 +			err = PTR_ERR(flow->rule);
++=======
+ 	list_for_each_entry(efi, &e->flows, list) {
+ 		bool all_flow_encaps_valid = true;
+ 		int i;
+ 
+ 		flow = container_of(efi, struct mlx5e_tc_flow, encaps[efi->index]);
+ 		esw_attr = flow->esw_attr;
+ 		spec = &esw_attr->parse_attr->spec;
+ 
+ 		esw_attr->dests[efi->index].encap_id = e->encap_id;
+ 		esw_attr->dests[efi->index].flags |= MLX5_ESW_DEST_ENCAP_VALID;
+ 		/* Flow can be associated with multiple encap entries.
+ 		 * Before offloading the flow verify that all of them have
+ 		 * a valid neighbour.
+ 		 */
+ 		for (i = 0; i < MLX5_MAX_FLOW_FWD_VPORTS; i++) {
+ 			if (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP))
+ 				continue;
+ 			if (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP_VALID)) {
+ 				all_flow_encaps_valid = false;
+ 				break;
+ 			}
+ 		}
+ 		/* Do not offload flows with unresolved neighbors */
+ 		if (!all_flow_encaps_valid)
+ 			continue;
+ 		/* update from slow path rule to encap rule */
+ 		rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, esw_attr);
+ 		if (IS_ERR(rule)) {
+ 			err = PTR_ERR(rule);
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  			mlx5_core_warn(priv->mdev, "Failed to update cached encapsulation flow, %d\n",
  				       err);
  			continue;
@@@ -938,13 -1131,32 +1030,30 @@@ void mlx5e_tc_encap_flows_del(struct ml
  			      struct mlx5e_encap_entry *e)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	struct mlx5_esw_flow_attr slow_attr;
 -	struct mlx5_flow_handle *rule;
 -	struct mlx5_flow_spec *spec;
 -	struct encap_flow_item *efi;
  	struct mlx5e_tc_flow *flow;
 -	int err;
  
++<<<<<<< HEAD
 +	list_for_each_entry(flow, &e->flows, encap) {
 +		if (flow->flags & MLX5E_TC_FLOW_OFFLOADED) {
 +			flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 +			mlx5_eswitch_del_offloaded_rule(esw, flow->rule, flow->esw_attr);
++=======
+ 	list_for_each_entry(efi, &e->flows, list) {
+ 		flow = container_of(efi, struct mlx5e_tc_flow, encaps[efi->index]);
+ 		spec = &flow->esw_attr->parse_attr->spec;
+ 
+ 		/* update from encap rule to slow path rule */
+ 		rule = mlx5e_tc_offload_to_slow_path(esw, flow, spec, &slow_attr);
+ 		/* mark the flow's encap dest as non-valid */
+ 		flow->esw_attr->dests[efi->index].flags &= ~MLX5_ESW_DEST_ENCAP_VALID;
+ 
+ 		if (IS_ERR(rule)) {
+ 			err = PTR_ERR(rule);
+ 			mlx5_core_warn(priv->mdev, "Failed to update slow path (encap) flow, %d\n",
+ 				       err);
+ 			continue;
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  		}
 -
 -		mlx5e_tc_unoffload_fdb_rules(esw, flow, flow->esw_attr);
 -		flow->flags |= MLX5E_TC_FLOW_OFFLOADED; /* was unset when fast path rule removed */
 -		flow->rule[0] = rule;
  	}
  
  	if (e->flags & MLX5_ENCAP_ENTRY_VALID) {
@@@ -1006,11 -1229,11 +1115,17 @@@ void mlx5e_tc_update_neigh_used_value(s
  }
  
  static void mlx5e_detach_encap(struct mlx5e_priv *priv,
- 			       struct mlx5e_tc_flow *flow)
+ 			       struct mlx5e_tc_flow *flow, int out_index)
  {
++<<<<<<< HEAD
 +	struct list_head *next = flow->encap.next;
 +
 +	list_del(&flow->encap);
++=======
+ 	struct list_head *next = flow->encaps[out_index].list.next;
+ 
+ 	list_del(&flow->encaps[out_index].list);
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  	if (list_empty(next)) {
  		struct mlx5e_encap_entry *e;
  
@@@ -2494,14 -2345,12 +2609,20 @@@ static int mlx5e_attach_encap(struct ml
  			      struct ip_tunnel_info *tun_info,
  			      struct net_device *mirred_dev,
  			      struct net_device **encap_dev,
++<<<<<<< HEAD
 +			      struct mlx5e_tc_flow *flow)
++=======
+ 			      struct mlx5e_tc_flow *flow,
+ 			      struct netlink_ext_ack *extack,
+ 			      int out_index)
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5e_rep_priv *uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw,
 +									   REP_ETH);
 +	struct net_device *up_dev = uplink_rpriv->netdev;
  	unsigned short family = ip_tunnel_info_af(tun_info);
 +	struct mlx5e_priv *up_priv = netdev_priv(up_dev);
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
  	struct ip_tunnel_key *key = &tun_info->key;
  	struct mlx5e_encap_entry *e;
@@@ -2563,12 -2394,15 +2684,19 @@@ vxlan_encap_offload_err
  	hash_add_rcu(esw->offloads.encap_tbl, &e->encap_hlist, hash_key);
  
  attach_flow:
++<<<<<<< HEAD
 +	list_add(&flow->encap, &e->flows);
++=======
+ 	list_add(&flow->encaps[out_index].list, &e->flows);
+ 	flow->encaps[out_index].index = out_index;
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  	*encap_dev = e->out_dev;
- 	if (e->flags & MLX5_ENCAP_ENTRY_VALID)
- 		attr->encap_id = e->encap_id;
- 	else
+ 	if (e->flags & MLX5_ENCAP_ENTRY_VALID) {
+ 		attr->dests[out_index].encap_id = e->encap_id;
+ 		attr->dests[out_index].flags |= MLX5_ESW_DEST_ENCAP_VALID;
+ 	} else {
  		err = -EAGAIN;
+ 	}
  
  	return err;
  
@@@ -2622,29 -2500,68 +2750,36 @@@ static int parse_tc_fdb_actions(struct 
  			continue;
  		}
  
 -		if (is_tcf_csum(a)) {
 -			if (csum_offload_supported(priv, action,
 -						   tcf_csum_update_flags(a),
 -						   extack))
 -				continue;
 -
 -			return -EOPNOTSUPP;
 -		}
 -
 -		if (is_tcf_mirred_egress_redirect(a) || is_tcf_mirred_egress_mirror(a)) {
 -			struct mlx5e_priv *out_priv;
 +		if (is_tcf_mirred_egress_redirect(a)) {
  			struct net_device *out_dev;
 +			struct mlx5e_priv *out_priv;
  
  			out_dev = tcf_mirred_dev(a);
 -			if (!out_dev) {
 -				/* out_dev is NULL when filters with
 -				 * non-existing mirred device are replayed to
 -				 * the driver.
 -				 */
 -				return -EINVAL;
 -			}
  
 -			if (attr->out_count >= MLX5_MAX_FLOW_FWD_VPORTS) {
 -				NL_SET_ERR_MSG_MOD(extack,
 -						   "can't support more output ports, can't offload forwarding");
 -				pr_err("can't support more than %d output ports, can't offload forwarding\n",
 -				       attr->out_count);
 -				return -EOPNOTSUPP;
 -			}
 -
 -			action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 -				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
  			if (switchdev_port_same_parent_id(priv->netdev,
 -							  out_dev) ||
 -			    is_merged_eswitch_dev(priv, out_dev)) {
 +							  out_dev)) {
 +				attr->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 +					MLX5_FLOW_CONTEXT_ACTION_COUNT;
  				out_priv = netdev_priv(out_dev);
  				rpriv = out_priv->ppriv;
 -				attr->dests[attr->out_count].rep = rpriv->rep;
 -				attr->dests[attr->out_count].mdev = out_priv->mdev;
 -				attr->out_count++;
 +				attr->out_rep = rpriv->rep;
 +				attr->out_mdev = out_priv->mdev;
  			} else if (encap) {
++<<<<<<< HEAD
 +				parse_attr->mirred_ifindex = out_dev->ifindex;
 +				parse_attr->tun_info = *info;
++=======
+ 				parse_attr->mirred_ifindex[attr->out_count] =
+ 					out_dev->ifindex;
+ 				parse_attr->tun_info[attr->out_count] = *info;
+ 				encap = false;
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  				attr->parse_attr = parse_attr;
 -				attr->dests[attr->out_count].flags |=
 -					MLX5_ESW_DEST_ENCAP;
 -				attr->out_count++;
 -				/* attr->dests[].rep is resolved when we
 -				 * handle encap
 -				 */
 -			} else if (parse_attr->filter_dev != priv->netdev) {
 -				/* All mlx5 devices are called to configure
 -				 * high level device filters. Therefore, the
 -				 * *attempt* to  install a filter on invalid
 -				 * eswitch should not trigger an explicit error
 -				 */
 -				return -EINVAL;
 +				attr->action |= MLX5_FLOW_CONTEXT_ACTION_ENCAP |
 +					MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 +					MLX5_FLOW_CONTEXT_ACTION_COUNT;
 +				/* attr->out_rep is resolved when we handle encap */
  			} else {
 -				NL_SET_ERR_MSG_MOD(extack,
 -						   "devices are not on same switch HW, can't offload forwarding");
  				pr_err("devices %s %s not on same switch HW, can't offload forwarding\n",
  				       priv->netdev->name, out_dev->name);
  				return -EINVAL;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index 21bc97b70ed9,87c9dea9bccf..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@@ -241,20 -278,39 +241,40 @@@ enum mlx5_flow_match_level 
  	MLX5_MATCH_L4	= MLX5_INLINE_MODE_TCP_UDP,
  };
  
++<<<<<<< HEAD
++=======
+ /* current maximum for flow based vport multicasting */
+ #define MLX5_MAX_FLOW_FWD_VPORTS 2
+ 
+ enum {
+ 	MLX5_ESW_DEST_ENCAP         = BIT(0),
+ 	MLX5_ESW_DEST_ENCAP_VALID   = BIT(1),
+ };
+ 
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  struct mlx5_esw_flow_attr {
  	struct mlx5_eswitch_rep *in_rep;
 +	struct mlx5_eswitch_rep *out_rep;
 +	struct mlx5_core_dev	*out_mdev;
  	struct mlx5_core_dev	*in_mdev;
  
 -	int split_count;
 -	int out_count;
 -
  	int	action;
 -	__be16	vlan_proto[MLX5_FS_VLAN_DEPTH];
 -	u16	vlan_vid[MLX5_FS_VLAN_DEPTH];
 -	u8	vlan_prio[MLX5_FS_VLAN_DEPTH];
 -	u8	total_vlan;
 +	__be16	vlan_proto;
 +	u16	vlan_vid;
 +	u8	vlan_prio;
  	bool	vlan_handled;
++<<<<<<< HEAD
 +	u32	encap_id;
++=======
+ 	struct {
+ 		u32 flags;
+ 		struct mlx5_eswitch_rep *rep;
+ 		struct mlx5_core_dev *mdev;
+ 		u32 encap_id;
+ 	} dests[MLX5_MAX_FLOW_FWD_VPORTS];
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  	u32	mod_hdr_id;
  	u8	match_level;
 -	struct mlx5_fc *counter;
 -	u32	chain;
 -	u16	prio;
 -	u32	dest_chain;
  	struct mlx5e_tc_flow_parse_attr *parse_attr;
  };
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 84864631953e,bde1fb8c284b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -70,23 -107,41 +70,56 @@@ mlx5_eswitch_add_offloaded_rule(struct 
  	}
  
  	if (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_FWD_DEST) {
++<<<<<<< HEAD
 +		dest[i].type = MLX5_FLOW_DESTINATION_TYPE_VPORT;
 +		dest[i].vport.num = attr->out_rep->vport;
 +		if (MLX5_CAP_ESW(esw->dev, merged_eswitch)) {
 +			dest[i].vport.vhca_id =
 +				MLX5_CAP_GEN(attr->out_mdev, vhca_id);
 +			dest[i].vport.vhca_id_valid = 1;
++=======
+ 		if (attr->dest_chain) {
+ 			struct mlx5_flow_table *ft;
+ 
+ 			ft = esw_get_prio_table(esw, attr->dest_chain, 1, 0);
+ 			if (IS_ERR(ft)) {
+ 				rule = ERR_CAST(ft);
+ 				goto err_create_goto_table;
+ 			}
+ 
+ 			dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 			dest[i].ft = ft;
+ 			i++;
+ 		} else {
+ 			for (j = attr->split_count; j < attr->out_count; j++) {
+ 				dest[i].type = MLX5_FLOW_DESTINATION_TYPE_VPORT;
+ 				dest[i].vport.num = attr->dests[j].rep->vport;
+ 				dest[i].vport.vhca_id =
+ 					MLX5_CAP_GEN(attr->dests[j].mdev, vhca_id);
+ 				if (MLX5_CAP_ESW(esw->dev, merged_eswitch))
+ 					dest[i].vport.flags |=
+ 						MLX5_FLOW_DEST_VPORT_VHCA_ID;
+ 				if (attr->dests[j].flags & MLX5_ESW_DEST_ENCAP) {
+ 					flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 					flow_act.reformat_id = attr->dests[j].encap_id;
+ 					dest[i].vport.flags |= MLX5_FLOW_DEST_VPORT_REFORMAT_ID;
+ 					dest[i].vport.reformat_id =
+ 						attr->dests[j].encap_id;
+ 				}
+ 				i++;
+ 			}
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  		}
 +		i++;
  	}
  	if (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {
 +		counter = mlx5_fc_create(esw->dev, true);
 +		if (IS_ERR(counter)) {
 +			rule = ERR_CAST(counter);
 +			goto err_counter_alloc;
 +		}
  		dest[i].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;
 -		dest[i].counter_id = mlx5_fc_id(attr->counter);
 +		dest[i].counter = counter;
  		i++;
  	}
  
@@@ -134,6 -194,105 +167,108 @@@ err_counter_alloc
  	return rule;
  }
  
++<<<<<<< HEAD
++=======
+ struct mlx5_flow_handle *
+ mlx5_eswitch_add_fwd_rule(struct mlx5_eswitch *esw,
+ 			  struct mlx5_flow_spec *spec,
+ 			  struct mlx5_esw_flow_attr *attr)
+ {
+ 	struct mlx5_flow_destination dest[MLX5_MAX_FLOW_FWD_VPORTS + 1] = {};
+ 	struct mlx5_flow_act flow_act = { .flags = FLOW_ACT_NO_APPEND, };
+ 	struct mlx5_flow_table *fast_fdb;
+ 	struct mlx5_flow_table *fwd_fdb;
+ 	struct mlx5_flow_handle *rule;
+ 	void *misc;
+ 	int i;
+ 
+ 	fast_fdb = esw_get_prio_table(esw, attr->chain, attr->prio, 0);
+ 	if (IS_ERR(fast_fdb)) {
+ 		rule = ERR_CAST(fast_fdb);
+ 		goto err_get_fast;
+ 	}
+ 
+ 	fwd_fdb = esw_get_prio_table(esw, attr->chain, attr->prio, 1);
+ 	if (IS_ERR(fwd_fdb)) {
+ 		rule = ERR_CAST(fwd_fdb);
+ 		goto err_get_fwd;
+ 	}
+ 
+ 	flow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	for (i = 0; i < attr->split_count; i++) {
+ 		dest[i].type = MLX5_FLOW_DESTINATION_TYPE_VPORT;
+ 		dest[i].vport.num = attr->dests[i].rep->vport;
+ 		dest[i].vport.vhca_id =
+ 			MLX5_CAP_GEN(attr->dests[i].mdev, vhca_id);
+ 		if (MLX5_CAP_ESW(esw->dev, merged_eswitch))
+ 			dest[i].vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;
+ 		if (attr->dests[i].flags & MLX5_ESW_DEST_ENCAP) {
+ 			dest[i].vport.flags |= MLX5_FLOW_DEST_VPORT_REFORMAT_ID;
+ 			dest[i].vport.reformat_id = attr->dests[i].encap_id;
+ 		}
+ 	}
+ 	dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 	dest[i].ft = fwd_fdb,
+ 	i++;
+ 
+ 	misc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);
+ 	MLX5_SET(fte_match_set_misc, misc, source_port, attr->in_rep->vport);
+ 
+ 	if (MLX5_CAP_ESW(esw->dev, merged_eswitch))
+ 		MLX5_SET(fte_match_set_misc, misc,
+ 			 source_eswitch_owner_vhca_id,
+ 			 MLX5_CAP_GEN(attr->in_mdev, vhca_id));
+ 
+ 	misc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);
+ 	MLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);
+ 	if (MLX5_CAP_ESW(esw->dev, merged_eswitch))
+ 		MLX5_SET_TO_ONES(fte_match_set_misc, misc,
+ 				 source_eswitch_owner_vhca_id);
+ 
+ 	if (attr->match_level == MLX5_MATCH_NONE)
+ 		spec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;
+ 	else
+ 		spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS |
+ 					      MLX5_MATCH_MISC_PARAMETERS;
+ 
+ 	rule = mlx5_add_flow_rules(fast_fdb, spec, &flow_act, dest, i);
+ 
+ 	if (IS_ERR(rule))
+ 		goto add_err;
+ 
+ 	esw->offloads.num_flows++;
+ 
+ 	return rule;
+ add_err:
+ 	esw_put_prio_table(esw, attr->chain, attr->prio, 1);
+ err_get_fwd:
+ 	esw_put_prio_table(esw, attr->chain, attr->prio, 0);
+ err_get_fast:
+ 	return rule;
+ }
+ 
+ static void
+ __mlx5_eswitch_del_rule(struct mlx5_eswitch *esw,
+ 			struct mlx5_flow_handle *rule,
+ 			struct mlx5_esw_flow_attr *attr,
+ 			bool fwd_rule)
+ {
+ 	bool split = (attr->split_count > 0);
+ 
+ 	mlx5_del_flow_rules(rule);
+ 	esw->offloads.num_flows--;
+ 
+ 	if (fwd_rule)  {
+ 		esw_put_prio_table(esw, attr->chain, attr->prio, 1);
+ 		esw_put_prio_table(esw, attr->chain, attr->prio, 0);
+ 	} else {
+ 		esw_put_prio_table(esw, attr->chain, attr->prio, !!split);
+ 		if (attr->dest_chain)
+ 			esw_put_prio_table(esw, attr->dest_chain, 1, 0);
+ 	}
+ }
+ 
++>>>>>>> 8c4dc42bf6e4 (net/mlx5e: Support multiple encapsulations for a TC flow)
  void
  mlx5_eswitch_del_offloaded_rule(struct mlx5_eswitch *esw,
  				struct mlx5_flow_handle *rule,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
