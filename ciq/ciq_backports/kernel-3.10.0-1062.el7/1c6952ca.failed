nfp: flower: generate merge flow rule

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author John Hurley <john.hurley@netronome.com>
commit 1c6952ca587d54512f79ba78cb20092c598a7385
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/1c6952ca.failed

When combining 2 sub_flows to a single 'merge flow' (assuming the merge is
valid), the merge flow should contain the same match fields as sub_flow 1
with actions derived from a combination of sub_flows 1 and 2. This action
list should have all actions from sub_flow 1 with the exception of the
output action that triggered the 'implicit recirculation' by sending to
an internal port, followed by all actions of sub_flow 2. Any pre-actions
in either sub_flow should feature at the start of the action list.

Add code to generate a new merge flow and populate the match and actions
fields based on the sub_flows. The offloading of the flow is left to
future patches.

	Signed-off-by: John Hurley <john.hurley@netronome.com>
	Signed-off-by: Simon Horman <simon.horman@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1c6952ca587d54512f79ba78cb20092c598a7385)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/flower/main.h
#	drivers/net/ethernet/netronome/nfp/flower/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/flower/main.h
index 90cc96d4eae4,df49cf9d73b3..000000000000
--- a/drivers/net/ethernet/netronome/nfp/flower/main.h
+++ b/drivers/net/ethernet/netronome/nfp/flower/main.h
@@@ -246,16 -263,33 +246,30 @@@ struct nfp_fl_stats_frame 
  	__be64 stats_cookie;
  };
  
 -static inline bool
 -nfp_flower_internal_port_can_offload(struct nfp_app *app,
 -				     struct net_device *netdev)
 +static inline unsigned long nfp_flower_fl_key(unsigned long tc_flower_cookie)
  {
 -	struct nfp_flower_priv *app_priv = app->priv;
 -
 -	if (!(app_priv->flower_ext_feats & NFP_FL_FEATS_FLOW_MERGE))
 -		return false;
 -	if (!netdev->rtnl_link_ops)
 -		return false;
 -	if (!strcmp(netdev->rtnl_link_ops->kind, "openvswitch"))
 -		return true;
 -
 -	return false;
 +#if BITS_PER_LONG == 64
 +	return tc_flower_cookie * NFP_FLOWER_GOLDEN_RATIO_64;
 +#else
 +	return tc_flower_cookie * NFP_FLOWER_GOLDEN_RATIO_32;
 +#endif
  }
  
++<<<<<<< HEAD
 +int nfp_flower_metadata_init(struct nfp_app *app);
++=======
+ /* The address of the merged flow acts as its cookie.
+  * Cookies supplied to us by TC flower are also addresses to allocated
+  * memory and thus this scheme should not generate any collisions.
+  */
+ static inline bool nfp_flower_is_merge_flow(struct nfp_fl_payload *flow_pay)
+ {
+ 	return flow_pay->tc_flower_cookie == (unsigned long)flow_pay;
+ }
+ 
+ int nfp_flower_metadata_init(struct nfp_app *app, u64 host_ctx_count,
+ 			     unsigned int host_ctx_split);
++>>>>>>> 1c6952ca587d (nfp: flower: generate merge flow rule)
  void nfp_flower_metadata_cleanup(struct nfp_app *app);
  
  int nfp_flower_setup_tc(struct nfp_app *app, struct net_device *netdev,
diff --cc drivers/net/ethernet/netronome/nfp/flower/offload.c
index 050fdefa9950,1e329667249d..000000000000
--- a/drivers/net/ethernet/netronome/nfp/flower/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/flower/offload.c
@@@ -457,6 -410,370 +457,373 @@@ err_free_flow
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nfp_flower_update_merge_with_actions(struct nfp_fl_payload *flow,
+ 				     struct nfp_flower_merge_check *merge,
+ 				     u8 *last_act_id, int *act_out)
+ {
+ 	struct nfp_fl_set_ipv6_tc_hl_fl *ipv6_tc_hl_fl;
+ 	struct nfp_fl_set_ip4_ttl_tos *ipv4_ttl_tos;
+ 	struct nfp_fl_set_ip4_addrs *ipv4_add;
+ 	struct nfp_fl_set_ipv6_addr *ipv6_add;
+ 	struct nfp_fl_push_vlan *push_vlan;
+ 	struct nfp_fl_set_tport *tport;
+ 	struct nfp_fl_set_eth *eth;
+ 	struct nfp_fl_act_head *a;
+ 	unsigned int act_off = 0;
+ 	u8 act_id = 0;
+ 	u8 *ports;
+ 	int i;
+ 
+ 	while (act_off < flow->meta.act_len) {
+ 		a = (struct nfp_fl_act_head *)&flow->action_data[act_off];
+ 		act_id = a->jump_id;
+ 
+ 		switch (act_id) {
+ 		case NFP_FL_ACTION_OPCODE_OUTPUT:
+ 			if (act_out)
+ 				(*act_out)++;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_PUSH_VLAN:
+ 			push_vlan = (struct nfp_fl_push_vlan *)a;
+ 			if (push_vlan->vlan_tci)
+ 				merge->tci = cpu_to_be16(0xffff);
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_POP_VLAN:
+ 			merge->tci = cpu_to_be16(0);
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV4_TUNNEL:
+ 			/* New tunnel header means l2 to l4 can be matched. */
+ 			eth_broadcast_addr(&merge->l2.mac_dst[0]);
+ 			eth_broadcast_addr(&merge->l2.mac_src[0]);
+ 			memset(&merge->l4, 0xff,
+ 			       sizeof(struct nfp_flower_tp_ports));
+ 			memset(&merge->ipv4, 0xff,
+ 			       sizeof(struct nfp_flower_ipv4));
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_ETHERNET:
+ 			eth = (struct nfp_fl_set_eth *)a;
+ 			for (i = 0; i < ETH_ALEN; i++)
+ 				merge->l2.mac_dst[i] |= eth->eth_addr_mask[i];
+ 			for (i = 0; i < ETH_ALEN; i++)
+ 				merge->l2.mac_src[i] |=
+ 					eth->eth_addr_mask[ETH_ALEN + i];
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV4_ADDRS:
+ 			ipv4_add = (struct nfp_fl_set_ip4_addrs *)a;
+ 			merge->ipv4.ipv4_src |= ipv4_add->ipv4_src_mask;
+ 			merge->ipv4.ipv4_dst |= ipv4_add->ipv4_dst_mask;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV4_TTL_TOS:
+ 			ipv4_ttl_tos = (struct nfp_fl_set_ip4_ttl_tos *)a;
+ 			merge->ipv4.ip_ext.ttl |= ipv4_ttl_tos->ipv4_ttl_mask;
+ 			merge->ipv4.ip_ext.tos |= ipv4_ttl_tos->ipv4_tos_mask;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV6_SRC:
+ 			ipv6_add = (struct nfp_fl_set_ipv6_addr *)a;
+ 			for (i = 0; i < 4; i++)
+ 				merge->ipv6.ipv6_src.in6_u.u6_addr32[i] |=
+ 					ipv6_add->ipv6[i].mask;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV6_DST:
+ 			ipv6_add = (struct nfp_fl_set_ipv6_addr *)a;
+ 			for (i = 0; i < 4; i++)
+ 				merge->ipv6.ipv6_dst.in6_u.u6_addr32[i] |=
+ 					ipv6_add->ipv6[i].mask;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_IPV6_TC_HL_FL:
+ 			ipv6_tc_hl_fl = (struct nfp_fl_set_ipv6_tc_hl_fl *)a;
+ 			merge->ipv6.ip_ext.ttl |=
+ 				ipv6_tc_hl_fl->ipv6_hop_limit_mask;
+ 			merge->ipv6.ip_ext.tos |= ipv6_tc_hl_fl->ipv6_tc_mask;
+ 			merge->ipv6.ipv6_flow_label_exthdr |=
+ 				ipv6_tc_hl_fl->ipv6_label_mask;
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_SET_UDP:
+ 		case NFP_FL_ACTION_OPCODE_SET_TCP:
+ 			tport = (struct nfp_fl_set_tport *)a;
+ 			ports = (u8 *)&merge->l4.port_src;
+ 			for (i = 0; i < 4; i++)
+ 				ports[i] |= tport->tp_port_mask[i];
+ 			break;
+ 		case NFP_FL_ACTION_OPCODE_PRE_TUNNEL:
+ 		case NFP_FL_ACTION_OPCODE_PRE_LAG:
+ 		case NFP_FL_ACTION_OPCODE_PUSH_GENEVE:
+ 			break;
+ 		default:
+ 			return -EOPNOTSUPP;
+ 		}
+ 
+ 		act_off += a->len_lw << NFP_FL_LW_SIZ;
+ 	}
+ 
+ 	if (last_act_id)
+ 		*last_act_id = act_id;
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_flower_populate_merge_match(struct nfp_fl_payload *flow,
+ 				struct nfp_flower_merge_check *merge,
+ 				bool extra_fields)
+ {
+ 	struct nfp_flower_meta_tci *meta_tci;
+ 	u8 *mask = flow->mask_data;
+ 	u8 key_layer, match_size;
+ 
+ 	memset(merge, 0, sizeof(struct nfp_flower_merge_check));
+ 
+ 	meta_tci = (struct nfp_flower_meta_tci *)mask;
+ 	key_layer = meta_tci->nfp_flow_key_layer;
+ 
+ 	if (key_layer & ~NFP_FLOWER_MERGE_FIELDS && !extra_fields)
+ 		return -EOPNOTSUPP;
+ 
+ 	merge->tci = meta_tci->tci;
+ 	mask += sizeof(struct nfp_flower_meta_tci);
+ 
+ 	if (key_layer & NFP_FLOWER_LAYER_EXT_META)
+ 		mask += sizeof(struct nfp_flower_ext_meta);
+ 
+ 	mask += sizeof(struct nfp_flower_in_port);
+ 
+ 	if (key_layer & NFP_FLOWER_LAYER_MAC) {
+ 		match_size = sizeof(struct nfp_flower_mac_mpls);
+ 		memcpy(&merge->l2, mask, match_size);
+ 		mask += match_size;
+ 	}
+ 
+ 	if (key_layer & NFP_FLOWER_LAYER_TP) {
+ 		match_size = sizeof(struct nfp_flower_tp_ports);
+ 		memcpy(&merge->l4, mask, match_size);
+ 		mask += match_size;
+ 	}
+ 
+ 	if (key_layer & NFP_FLOWER_LAYER_IPV4) {
+ 		match_size = sizeof(struct nfp_flower_ipv4);
+ 		memcpy(&merge->ipv4, mask, match_size);
+ 	}
+ 
+ 	if (key_layer & NFP_FLOWER_LAYER_IPV6) {
+ 		match_size = sizeof(struct nfp_flower_ipv6);
+ 		memcpy(&merge->ipv6, mask, match_size);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_flower_can_merge(struct nfp_fl_payload *sub_flow1,
+ 		     struct nfp_fl_payload *sub_flow2)
+ {
+ 	/* Two flows can be merged if sub_flow2 only matches on bits that are
+ 	 * either matched by sub_flow1 or set by a sub_flow1 action. This
+ 	 * ensures that every packet that hits sub_flow1 and recirculates is
+ 	 * guaranteed to hit sub_flow2.
+ 	 */
+ 	struct nfp_flower_merge_check sub_flow1_merge, sub_flow2_merge;
+ 	int err, act_out = 0;
+ 	u8 last_act_id = 0;
+ 
+ 	err = nfp_flower_populate_merge_match(sub_flow1, &sub_flow1_merge,
+ 					      true);
+ 	if (err)
+ 		return err;
+ 
+ 	err = nfp_flower_populate_merge_match(sub_flow2, &sub_flow2_merge,
+ 					      false);
+ 	if (err)
+ 		return err;
+ 
+ 	err = nfp_flower_update_merge_with_actions(sub_flow1, &sub_flow1_merge,
+ 						   &last_act_id, &act_out);
+ 	if (err)
+ 		return err;
+ 
+ 	/* Must only be 1 output action and it must be the last in sequence. */
+ 	if (act_out != 1 || last_act_id != NFP_FL_ACTION_OPCODE_OUTPUT)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Reject merge if sub_flow2 matches on something that is not matched
+ 	 * on or set in an action by sub_flow1.
+ 	 */
+ 	err = bitmap_andnot(sub_flow2_merge.vals, sub_flow2_merge.vals,
+ 			    sub_flow1_merge.vals,
+ 			    sizeof(struct nfp_flower_merge_check) * 8);
+ 	if (err)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static unsigned int
+ nfp_flower_copy_pre_actions(char *act_dst, char *act_src, int len,
+ 			    bool *tunnel_act)
+ {
+ 	unsigned int act_off = 0, act_len;
+ 	struct nfp_fl_act_head *a;
+ 	u8 act_id = 0;
+ 
+ 	while (act_off < len) {
+ 		a = (struct nfp_fl_act_head *)&act_src[act_off];
+ 		act_len = a->len_lw << NFP_FL_LW_SIZ;
+ 		act_id = a->jump_id;
+ 
+ 		switch (act_id) {
+ 		case NFP_FL_ACTION_OPCODE_PRE_TUNNEL:
+ 			if (tunnel_act)
+ 				*tunnel_act = true;
+ 		case NFP_FL_ACTION_OPCODE_PRE_LAG:
+ 			memcpy(act_dst + act_off, act_src + act_off, act_len);
+ 			break;
+ 		default:
+ 			return act_off;
+ 		}
+ 
+ 		act_off += act_len;
+ 	}
+ 
+ 	return act_off;
+ }
+ 
+ static int nfp_fl_verify_post_tun_acts(char *acts, int len)
+ {
+ 	struct nfp_fl_act_head *a;
+ 	unsigned int act_off = 0;
+ 
+ 	while (act_off < len) {
+ 		a = (struct nfp_fl_act_head *)&acts[act_off];
+ 		if (a->jump_id != NFP_FL_ACTION_OPCODE_OUTPUT)
+ 			return -EOPNOTSUPP;
+ 
+ 		act_off += a->len_lw << NFP_FL_LW_SIZ;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_flower_merge_action(struct nfp_fl_payload *sub_flow1,
+ 			struct nfp_fl_payload *sub_flow2,
+ 			struct nfp_fl_payload *merge_flow)
+ {
+ 	unsigned int sub1_act_len, sub2_act_len, pre_off1, pre_off2;
+ 	bool tunnel_act = false;
+ 	char *merge_act;
+ 	int err;
+ 
+ 	/* The last action of sub_flow1 must be output - do not merge this. */
+ 	sub1_act_len = sub_flow1->meta.act_len - sizeof(struct nfp_fl_output);
+ 	sub2_act_len = sub_flow2->meta.act_len;
+ 
+ 	if (!sub2_act_len)
+ 		return -EINVAL;
+ 
+ 	if (sub1_act_len + sub2_act_len > NFP_FL_MAX_A_SIZ)
+ 		return -EINVAL;
+ 
+ 	/* A shortcut can only be applied if there is a single action. */
+ 	if (sub1_act_len)
+ 		merge_flow->meta.shortcut = cpu_to_be32(NFP_FL_SC_ACT_NULL);
+ 	else
+ 		merge_flow->meta.shortcut = sub_flow2->meta.shortcut;
+ 
+ 	merge_flow->meta.act_len = sub1_act_len + sub2_act_len;
+ 	merge_act = merge_flow->action_data;
+ 
+ 	/* Copy any pre-actions to the start of merge flow action list. */
+ 	pre_off1 = nfp_flower_copy_pre_actions(merge_act,
+ 					       sub_flow1->action_data,
+ 					       sub1_act_len, &tunnel_act);
+ 	merge_act += pre_off1;
+ 	sub1_act_len -= pre_off1;
+ 	pre_off2 = nfp_flower_copy_pre_actions(merge_act,
+ 					       sub_flow2->action_data,
+ 					       sub2_act_len, NULL);
+ 	merge_act += pre_off2;
+ 	sub2_act_len -= pre_off2;
+ 
+ 	/* FW does a tunnel push when egressing, therefore, if sub_flow 1 pushes
+ 	 * a tunnel, sub_flow 2 can only have output actions for a valid merge.
+ 	 */
+ 	if (tunnel_act) {
+ 		char *post_tun_acts = &sub_flow2->action_data[pre_off2];
+ 
+ 		err = nfp_fl_verify_post_tun_acts(post_tun_acts, sub2_act_len);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	/* Copy remaining actions from sub_flows 1 and 2. */
+ 	memcpy(merge_act, sub_flow1->action_data + pre_off1, sub1_act_len);
+ 	merge_act += sub1_act_len;
+ 	memcpy(merge_act, sub_flow2->action_data + pre_off2, sub2_act_len);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * nfp_flower_merge_offloaded_flows() - Merge 2 existing flows to single flow.
+  * @app:	Pointer to the APP handle
+  * @sub_flow1:	Initial flow matched to produce merge hint
+  * @sub_flow2:	Post recirculation flow matched in merge hint
+  *
+  * Combines 2 flows (if valid) to a single flow, removing the initial from hw
+  * and offloading the new, merged flow.
+  *
+  * Return: negative value on error, 0 in success.
+  */
+ int nfp_flower_merge_offloaded_flows(struct nfp_app *app,
+ 				     struct nfp_fl_payload *sub_flow1,
+ 				     struct nfp_fl_payload *sub_flow2)
+ {
+ 	struct nfp_fl_payload *merge_flow;
+ 	struct nfp_fl_key_ls merge_key_ls;
+ 	int err;
+ 
+ 	ASSERT_RTNL();
+ 
+ 	if (sub_flow1 == sub_flow2 ||
+ 	    nfp_flower_is_merge_flow(sub_flow1) ||
+ 	    nfp_flower_is_merge_flow(sub_flow2))
+ 		return -EINVAL;
+ 
+ 	err = nfp_flower_can_merge(sub_flow1, sub_flow2);
+ 	if (err)
+ 		return err;
+ 
+ 	merge_key_ls.key_size = sub_flow1->meta.key_len;
+ 
+ 	merge_flow = nfp_flower_allocate_new(&merge_key_ls);
+ 	if (!merge_flow)
+ 		return -ENOMEM;
+ 
+ 	merge_flow->tc_flower_cookie = (unsigned long)merge_flow;
+ 	merge_flow->ingress_dev = sub_flow1->ingress_dev;
+ 
+ 	memcpy(merge_flow->unmasked_data, sub_flow1->unmasked_data,
+ 	       sub_flow1->meta.key_len);
+ 	memcpy(merge_flow->mask_data, sub_flow1->mask_data,
+ 	       sub_flow1->meta.mask_len);
+ 
+ 	err = nfp_flower_merge_action(sub_flow1, sub_flow2, merge_flow);
+ 	if (err)
+ 		goto err_destroy_merge_flow;
+ 
+ 	err = -EOPNOTSUPP;
+ 
+ err_destroy_merge_flow:
+ 	kfree(merge_flow->action_data);
+ 	kfree(merge_flow->mask_data);
+ 	kfree(merge_flow->unmasked_data);
+ 	kfree(merge_flow);
+ 	return err;
+ }
+ 
++>>>>>>> 1c6952ca587d (nfp: flower: generate merge flow rule)
  /**
   * nfp_flower_add_offload() - Adds a new flow to hardware.
   * @app:	Pointer to the APP handle
* Unmerged path drivers/net/ethernet/netronome/nfp/flower/main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/flower/offload.c
