x86/kvm/mmu: reset MMU context when 32-bit guest switches PAE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm/mmu: reset MMU context when 32-bit guest switches PAE (Vitaly Kuznetsov) [1703797]
Rebuild_FUZZ: 96.61%
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 0699c64a4be6e4a6137240379a1f82c752e663d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/0699c64a.failed

Commit 47c42e6b4192 ("KVM: x86: fix handling of role.cr4_pae and rename it
to 'gpte_size'") introduced a regression: 32-bit PAE guests stopped
working. The issue appears to be: when guest switches (enables) PAE we need
to re-initialize MMU context (set context->root_level, do
reset_rsvds_bits_mask(), ...) but init_kvm_tdp_mmu() doesn't do that
because we threw away is_pae(vcpu) flag from mmu role. Restore it to
kvm_mmu_extended_role (as we now don't need it in base role) to fix
the issue.

Fixes: 47c42e6b4192 ("KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size'")
	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 0699c64a4be6e4a6137240379a1f82c752e663d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/include/asm/kvm_host.h
index 86bec63d5f8a,c79abe7ca093..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -246,6 -283,36 +246,39 @@@ union kvm_mmu_page_role 
  	};
  };
  
++<<<<<<< HEAD
++=======
+ union kvm_mmu_extended_role {
+ /*
+  * This structure complements kvm_mmu_page_role caching everything needed for
+  * MMU configuration. If nothing in both these structures changed, MMU
+  * re-configuration can be skipped. @valid bit is set on first usage so we don't
+  * treat all-zero structure as valid data.
+  */
+ 	u32 word;
+ 	struct {
+ 		unsigned int valid:1;
+ 		unsigned int execonly:1;
+ 		unsigned int cr0_pg:1;
+ 		unsigned int cr4_pae:1;
+ 		unsigned int cr4_pse:1;
+ 		unsigned int cr4_pke:1;
+ 		unsigned int cr4_smap:1;
+ 		unsigned int cr4_smep:1;
+ 		unsigned int cr4_la57:1;
+ 		unsigned int maxphyaddr:6;
+ 	};
+ };
+ 
+ union kvm_mmu_role {
+ 	u64 as_u64;
+ 	struct {
+ 		union kvm_mmu_page_role base;
+ 		union kvm_mmu_extended_role ext;
+ 	};
+ };
+ 
++>>>>>>> 0699c64a4be6 (x86/kvm/mmu: reset MMU context when 32-bit guest switches PAE)
  struct kvm_rmap_head {
  	unsigned long val;
  };
diff --cc arch/x86/kvm/mmu.c
index d16e6f650c80,d9c7b45d231f..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -4420,14 -4776,67 +4420,67 @@@ static void paging32E_init_context(stru
  	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
  }
  
++<<<<<<< HEAD
++=======
+ static union kvm_mmu_extended_role kvm_calc_mmu_role_ext(struct kvm_vcpu *vcpu)
+ {
+ 	union kvm_mmu_extended_role ext = {0};
+ 
+ 	ext.cr0_pg = !!is_paging(vcpu);
+ 	ext.cr4_pae = !!is_pae(vcpu);
+ 	ext.cr4_smep = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
+ 	ext.cr4_smap = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
+ 	ext.cr4_pse = !!is_pse(vcpu);
+ 	ext.cr4_pke = !!kvm_read_cr4_bits(vcpu, X86_CR4_PKE);
+ 	ext.cr4_la57 = !!kvm_read_cr4_bits(vcpu, X86_CR4_LA57);
+ 	ext.maxphyaddr = cpuid_maxphyaddr(vcpu);
+ 
+ 	ext.valid = 1;
+ 
+ 	return ext;
+ }
+ 
+ static union kvm_mmu_role kvm_calc_mmu_role_common(struct kvm_vcpu *vcpu,
+ 						   bool base_only)
+ {
+ 	union kvm_mmu_role role = {0};
+ 
+ 	role.base.access = ACC_ALL;
+ 	role.base.nxe = !!is_nx(vcpu);
+ 	role.base.cr0_wp = is_write_protection(vcpu);
+ 	role.base.smm = is_smm(vcpu);
+ 	role.base.guest_mode = is_guest_mode(vcpu);
+ 
+ 	if (base_only)
+ 		return role;
+ 
+ 	role.ext = kvm_calc_mmu_role_ext(vcpu);
+ 
+ 	return role;
+ }
+ 
+ static union kvm_mmu_role
+ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
+ {
+ 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ 
+ 	role.base.ad_disabled = (shadow_accessed_mask == 0);
+ 	role.base.level = kvm_x86_ops->get_tdp_level(vcpu);
+ 	role.base.direct = true;
+ 	role.base.gpte_is_8_bytes = true;
+ 
+ 	return role;
+ }
+ 
++>>>>>>> 0699c64a4be6 (x86/kvm/mmu: reset MMU context when 32-bit guest switches PAE)
  static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *context = vcpu->arch.mmu;
 -	union kvm_mmu_role new_role =
 -		kvm_calc_tdp_mmu_root_page_role(vcpu, false);
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
  
 -	new_role.base.word &= mmu_base_role_mask.word;
 -	if (new_role.as_u64 == context->mmu_role.as_u64)
 -		return;
 -
 -	context->mmu_role.as_u64 = new_role.as_u64;
 +	context->base_role.word = 0;
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
 +	context->base_role.ad_disabled = (shadow_accessed_mask == 0);
  	context->page_fault = tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/mmu.c
