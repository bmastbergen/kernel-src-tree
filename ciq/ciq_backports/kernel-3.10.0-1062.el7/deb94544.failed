pagemap: switch to the new format and do some cleanup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
commit deb945441b9408d6cd15751f5232eeca9f50a5a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/deb94544.failed

This patch removes page-shift bits (scheduled to remove since 3.11) and
completes migration to the new bit layout.  Also it cleans messy macro.

	Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Mark Williamson <mwilliamson@undo-software.com>
	Tested-by:  Mark Williamson <mwilliamson@undo-software.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit deb945441b9408d6cd15751f5232eeca9f50a5a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
diff --cc fs/proc/task_mmu.c
index bf81d9f4a02e,41c0a0a500f7..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -1100,16 -1027,9 +1063,19 @@@ static pagemap_entry_t pte_to_pagemap_e
  		entry = pte_to_swp_entry(pte);
  		frame = swp_type(entry) |
  			(swp_offset(entry) << MAX_SWAPFILES_SHIFT);
- 		flags = PM_SWAP;
+ 		flags |= PM_SWAP;
  		if (is_migration_entry(entry))
  			page = migration_entry_to_page(entry);
++<<<<<<< HEAD
 +		else if (is_hmm_entry(entry))
 +			page = hmm_entry_to_page(entry);
 +	} else {
 +		if (vma->vm_flags & VM_SOFTDIRTY)
 +			flags2 |= __PM_SOFT_DIRTY;
 +		*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, flags2));
 +		return;
++=======
++>>>>>>> deb945441b94 (pagemap: switch to the new format and do some cleanup)
  	}
  
  	if (page && !PageAnon(page))
@@@ -1145,21 -1069,17 +1115,24 @@@ static pagemap_entry_t thp_pmd_to_pagem
  static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
  			     struct mm_walk *walk)
  {
 -	struct vm_area_struct *vma = walk->vma;
 +	struct vm_area_struct *vma;
  	struct pagemapread *pm = walk->private;
  	spinlock_t *ptl;
 -	pte_t *pte, *orig_pte;
 +	pte_t *pte;
  	int err = 0;
  
++<<<<<<< HEAD
 +	/* find the first VMA at or above 'addr' */
 +	vma = find_vma(walk->mm, addr);
 +	if (vma && pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 +		int pmd_flags2;
++=======
+ 	if (pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
+ 		u64 flags = 0;
++>>>>>>> deb945441b94 (pagemap: switch to the new format and do some cleanup)
  
  		if ((vma->vm_flags & VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))
- 			pmd_flags2 = __PM_SOFT_DIRTY;
- 		else
- 			pmd_flags2 = 0;
+ 			flags |= PM_SOFT_DIRTY;
  
  		for (; addr != end; addr += PAGE_SIZE) {
  			unsigned long offset;
@@@ -1179,51 -1099,20 +1152,57 @@@
  	if (pmd_trans_unstable(pmd))
  		return 0;
  
 -	/*
 -	 * We can assume that @vma always points to a valid one and @end never
 -	 * goes beyond vma->vm_end.
 -	 */
 -	orig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
 -	for (; addr < end; pte++, addr += PAGE_SIZE) {
 -		pagemap_entry_t pme;
 +	while (1) {
 +		/* End of address space hole, which we mark as non-present. */
 +		unsigned long hole_end;
  
++<<<<<<< HEAD
 +		if (vma)
 +			hole_end = min(end, vma->vm_start);
 +		else
 +			hole_end = end;
 +
 +		for (; addr < hole_end; addr += PAGE_SIZE) {
 +			pagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm->v2));
 +
 +			err = add_to_pagemap(addr, &pme, pm);
 +			if (err)
 +				return err;
 +		}
 +
 +		if (!vma || vma->vm_start >= end)
++=======
+ 		pme = pte_to_pagemap_entry(pm, vma, addr, *pte);
+ 		err = add_to_pagemap(addr, &pme, pm);
+ 		if (err)
++>>>>>>> deb945441b94 (pagemap: switch to the new format and do some cleanup)
  			break;
 +		/*
 +		 * We can't possibly be in a hugetlb VMA. In general,
 +		 * for a mm_walk with a pmd_entry and a hugetlb_entry,
 +		 * the pmd_entry can only be called on addresses in a
 +		 * hugetlb if the walk starts in a non-hugetlb VMA and
 +		 * spans a hugepage VMA. Since pagemap_read walks are
 +		 * PMD-sized and PMD-aligned, this will never be true.
 +		 */
 +		BUG_ON(is_vm_hugetlb_page(vma));
 +
 +		/* Addresses in the VMA. */
 +		for (; addr < min(end, vma->vm_end); addr += PAGE_SIZE) {
 +			pagemap_entry_t pme;
 +			pte = pte_offset_map(pmd, addr);
 +			pte_to_pagemap_entry(&pme, pm, vma, addr, *pte);
 +			pte_unmap(pte);
 +			err = add_to_pagemap(addr, &pme, pm);
 +			if (err)
 +				return err;
 +		}
 +
 +		if (addr == end)
 +			break;
 +
 +		vma = find_vma(walk->mm, addr);
  	}
 -	pte_unmap_unlock(orig_pte, ptl);
  
  	cond_resched();
  
@@@ -1249,18 -1139,13 +1229,23 @@@ static int pagemap_hugetlb_range(pte_t 
  				 struct mm_walk *walk)
  {
  	struct pagemapread *pm = walk->private;
 -	struct vm_area_struct *vma = walk->vma;
 +	struct vm_area_struct *vma;
  	int err = 0;
- 	int flags2;
+ 	u64 flags = 0;
  	pagemap_entry_t pme;
  
++<<<<<<< HEAD
 +	vma = find_vma(walk->mm, addr);
 +	WARN_ON_ONCE(!vma);
 +
 +	if (vma && (vma->vm_flags & VM_SOFTDIRTY))
 +		flags2 = __PM_SOFT_DIRTY;
 +	else
 +		flags2 = 0;
++=======
+ 	if (vma->vm_flags & VM_SOFTDIRTY)
+ 		flags |= PM_SOFT_DIRTY;
++>>>>>>> deb945441b94 (pagemap: switch to the new format and do some cleanup)
  
  	for (; addr != end; addr += PAGE_SIZE) {
  		int offset = (addr & ~hmask) >> PAGE_SHIFT;
* Unmerged path fs/proc/task_mmu.c
diff --git a/tools/vm/page-types.c b/tools/vm/page-types.c
index cc32f303d504..ca4d3176bc14 100644
--- a/tools/vm/page-types.c
+++ b/tools/vm/page-types.c
@@ -51,23 +51,14 @@
  * pagemap kernel ABI bits
  */
 
-#define PM_ENTRY_BYTES      sizeof(uint64_t)
-#define PM_STATUS_BITS      3
-#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)
-#define PM_STATUS_MASK      (((1LL << PM_STATUS_BITS) - 1) << PM_STATUS_OFFSET)
-#define PM_STATUS(nr)       (((nr) << PM_STATUS_OFFSET) & PM_STATUS_MASK)
-#define PM_PSHIFT_BITS      6
-#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)
-#define PM_PSHIFT_MASK      (((1LL << PM_PSHIFT_BITS) - 1) << PM_PSHIFT_OFFSET)
-#define __PM_PSHIFT(x)      (((uint64_t) (x) << PM_PSHIFT_OFFSET) & PM_PSHIFT_MASK)
-#define PM_PFRAME_MASK      ((1LL << PM_PSHIFT_OFFSET) - 1)
-#define PM_PFRAME(x)        ((x) & PM_PFRAME_MASK)
-
-#define __PM_SOFT_DIRTY      (1LL)
-#define PM_PRESENT          PM_STATUS(4LL)
-#define PM_SWAP             PM_STATUS(2LL)
-#define PM_SOFT_DIRTY       __PM_PSHIFT(__PM_SOFT_DIRTY)
-
+#define PM_ENTRY_BYTES		8
+#define PM_PFRAME_BITS		55
+#define PM_PFRAME_MASK		((1LL << PM_PFRAME_BITS) - 1)
+#define PM_PFRAME(x)		((x) & PM_PFRAME_MASK)
+#define PM_SOFT_DIRTY		(1ULL << 55)
+#define PM_FILE			(1ULL << 61)
+#define PM_SWAP			(1ULL << 62)
+#define PM_PRESENT		(1ULL << 63)
 
 /*
  * kernel page flags
