svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 25fd86eca11c26bad2aede6dd4709ff58f89c7cb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/25fd86ec.failed

Receive buffers are always the same size, but each Send WR has a
variable number of SGEs, based on the contents of the xdr_buf being
sent.

While assembling a Send WR, keep track of the number of SGEs so that
we don't exceed the device's maximum, or walk off the end of the
Send SGE array.

For now the Send path just fails if it exceeds the maximum.

The current logic in svc_rdma_accept bases the maximum number of
Send SGEs on the largest NFS request that can be sent or received.
In the transport layer, the limit is actually based on the
capabilities of the underlying device, not on properties of the
Upper Layer Protocol.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit 25fd86eca11c26bad2aede6dd4709ff58f89c7cb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sunrpc/svc_rdma.h
#	net/sunrpc/xprtrdma/svc_rdma_sendto.c
#	net/sunrpc/xprtrdma/svc_rdma_transport.c
diff --cc include/linux/sunrpc/svc_rdma.h
index e1e3c42bedad,bfb8824e31e1..000000000000
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@@ -142,12 -144,29 +142,38 @@@ struct svcxprt_rdma 
  
  #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
  
++<<<<<<< HEAD
 +/* Track DMA maps for this transport and context */
 +static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 +					   struct svc_rdma_op_ctxt *ctxt)
 +{
 +	ctxt->mapped_sges++;
 +}
++=======
+ struct svc_rdma_recv_ctxt {
+ 	struct list_head	rc_list;
+ 	struct ib_recv_wr	rc_recv_wr;
+ 	struct ib_cqe		rc_cqe;
+ 	struct ib_sge		rc_recv_sge;
+ 	void			*rc_recv_buf;
+ 	struct xdr_buf		rc_arg;
+ 	bool			rc_temp;
+ 	u32			rc_byte_len;
+ 	unsigned int		rc_page_count;
+ 	unsigned int		rc_hdr_count;
+ 	struct page		*rc_pages[RPCSVC_MAXPAGES];
+ };
+ 
+ struct svc_rdma_send_ctxt {
+ 	struct list_head	sc_list;
+ 	struct ib_send_wr	sc_send_wr;
+ 	struct ib_cqe		sc_cqe;
+ 	int			sc_page_count;
+ 	int			sc_cur_sge_no;
+ 	struct page		*sc_pages[RPCSVC_MAXPAGES];
+ 	struct ib_sge		sc_sges[];
+ };
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  
  /* svc_rdma_backchannel.c */
  extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
diff --cc net/sunrpc/xprtrdma/svc_rdma_sendto.c
index 00a497c70989,53d8db6bfaf2..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_sendto.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
@@@ -114,6 -114,187 +114,190 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
++<<<<<<< HEAD
++=======
+ static void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc);
+ 
+ static inline struct svc_rdma_send_ctxt *
+ svc_rdma_next_send_ctxt(struct list_head *list)
+ {
+ 	return list_first_entry_or_null(list, struct svc_rdma_send_ctxt,
+ 					sc_list);
+ }
+ 
+ static struct svc_rdma_send_ctxt *
+ svc_rdma_send_ctxt_alloc(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 	size_t size;
+ 	int i;
+ 
+ 	size = sizeof(*ctxt);
+ 	size += rdma->sc_max_send_sges * sizeof(struct ib_sge);
+ 	ctxt = kmalloc(size, GFP_KERNEL);
+ 	if (!ctxt)
+ 		return NULL;
+ 
+ 	ctxt->sc_cqe.done = svc_rdma_wc_send;
+ 	ctxt->sc_send_wr.next = NULL;
+ 	ctxt->sc_send_wr.wr_cqe = &ctxt->sc_cqe;
+ 	ctxt->sc_send_wr.sg_list = ctxt->sc_sges;
+ 	ctxt->sc_send_wr.send_flags = IB_SEND_SIGNALED;
+ 	for (i = 0; i < rdma->sc_max_send_sges; i++)
+ 		ctxt->sc_sges[i].lkey = rdma->sc_pd->local_dma_lkey;
+ 	return ctxt;
+ }
+ 
+ /**
+  * svc_rdma_send_ctxts_destroy - Release all send_ctxt's for an xprt
+  * @rdma: svcxprt_rdma being torn down
+  *
+  */
+ void svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_send_ctxt(&rdma->sc_send_ctxts))) {
+ 		list_del(&ctxt->sc_list);
+ 		kfree(ctxt);
+ 	}
+ }
+ 
+ /**
+  * svc_rdma_send_ctxt_get - Get a free send_ctxt
+  * @rdma: controlling svcxprt_rdma
+  *
+  * Returns a ready-to-use send_ctxt, or NULL if none are
+  * available and a fresh one cannot be allocated.
+  */
+ struct svc_rdma_send_ctxt *svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	spin_lock(&rdma->sc_send_lock);
+ 	ctxt = svc_rdma_next_send_ctxt(&rdma->sc_send_ctxts);
+ 	if (!ctxt)
+ 		goto out_empty;
+ 	list_del(&ctxt->sc_list);
+ 	spin_unlock(&rdma->sc_send_lock);
+ 
+ out:
+ 	ctxt->sc_send_wr.num_sge = 0;
+ 	ctxt->sc_page_count = 0;
+ 	return ctxt;
+ 
+ out_empty:
+ 	spin_unlock(&rdma->sc_send_lock);
+ 	ctxt = svc_rdma_send_ctxt_alloc(rdma);
+ 	if (!ctxt)
+ 		return NULL;
+ 	goto out;
+ }
+ 
+ /**
+  * svc_rdma_send_ctxt_put - Return send_ctxt to free list
+  * @rdma: controlling svcxprt_rdma
+  * @ctxt: object to return to the free list
+  *
+  * Pages left in sc_pages are DMA unmapped and released.
+  */
+ void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
+ 			    struct svc_rdma_send_ctxt *ctxt)
+ {
+ 	struct ib_device *device = rdma->sc_cm_id->device;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ctxt->sc_send_wr.num_sge; i++)
+ 		ib_dma_unmap_page(device,
+ 				  ctxt->sc_sges[i].addr,
+ 				  ctxt->sc_sges[i].length,
+ 				  DMA_TO_DEVICE);
+ 
+ 	for (i = 0; i < ctxt->sc_page_count; ++i)
+ 		put_page(ctxt->sc_pages[i]);
+ 
+ 	spin_lock(&rdma->sc_send_lock);
+ 	list_add(&ctxt->sc_list, &rdma->sc_send_ctxts);
+ 	spin_unlock(&rdma->sc_send_lock);
+ }
+ 
+ /**
+  * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC
+  * @cq: Completion Queue context
+  * @wc: Work Completion object
+  *
+  * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
+  * the Send completion handler could be running.
+  */
+ static void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct svcxprt_rdma *rdma = cq->cq_context;
+ 	struct ib_cqe *cqe = wc->wr_cqe;
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	trace_svcrdma_wc_send(wc);
+ 
+ 	atomic_inc(&rdma->sc_sq_avail);
+ 	wake_up(&rdma->sc_send_wait);
+ 
+ 	ctxt = container_of(cqe, struct svc_rdma_send_ctxt, sc_cqe);
+ 	svc_rdma_send_ctxt_put(rdma, ctxt);
+ 
+ 	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+ 		set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 		svc_xprt_enqueue(&rdma->sc_xprt);
+ 		if (wc->status != IB_WC_WR_FLUSH_ERR)
+ 			pr_err("svcrdma: Send: %s (%u/0x%x)\n",
+ 			       ib_wc_status_msg(wc->status),
+ 			       wc->status, wc->vendor_err);
+ 	}
+ 
+ 	svc_xprt_put(&rdma->sc_xprt);
+ }
+ 
+ int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr)
+ {
+ 	struct ib_send_wr *bad_wr, *n_wr;
+ 	int wr_count;
+ 	int i;
+ 	int ret;
+ 
+ 	wr_count = 1;
+ 	for (n_wr = wr->next; n_wr; n_wr = n_wr->next)
+ 		wr_count++;
+ 
+ 	/* If the SQ is full, wait until an SQ entry is available */
+ 	while (1) {
+ 		if ((atomic_sub_return(wr_count, &rdma->sc_sq_avail) < 0)) {
+ 			atomic_inc(&rdma_stat_sq_starve);
+ 			trace_svcrdma_sq_full(rdma);
+ 			atomic_add(wr_count, &rdma->sc_sq_avail);
+ 			wait_event(rdma->sc_send_wait,
+ 				   atomic_read(&rdma->sc_sq_avail) > wr_count);
+ 			if (test_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags))
+ 				return -ENOTCONN;
+ 			trace_svcrdma_sq_retry(rdma);
+ 			continue;
+ 		}
+ 		/* Take a transport ref for each WR posted */
+ 		for (i = 0; i < wr_count; i++)
+ 			svc_xprt_get(&rdma->sc_xprt);
+ 
+ 		/* Bump used SQ WR count and post */
+ 		ret = ib_post_send(rdma->sc_qp, wr, &bad_wr);
+ 		trace_svcrdma_post_send(wr, ret);
+ 		if (ret) {
+ 			set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 			for (i = 0; i < wr_count; i++)
+ 				svc_xprt_put(&rdma->sc_xprt);
+ 			wake_up(&rdma->sc_send_wait);
+ 		}
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  static u32 xdr_padsize(u32 len)
  {
  	return (len & 3) ? (4 - (len & 3)) : 0;
@@@ -302,41 -483,10 +486,45 @@@ static u32 svc_rdma_get_inv_rkey(__be3
  	return be32_to_cpup(p);
  }
  
 +/* ib_dma_map_page() is used here because svc_rdma_dma_unmap()
 + * is used during completion to DMA-unmap this memory, and
 + * it uses ib_dma_unmap_page() exclusively.
 + */
 +static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,
 +				struct svc_rdma_op_ctxt *ctxt,
 +				unsigned int sge_no,
 +				unsigned char *base,
 +				unsigned int len)
 +{
 +	unsigned long offset = (unsigned long)base & ~PAGE_MASK;
 +	struct ib_device *dev = rdma->sc_cm_id->device;
 +	dma_addr_t dma_addr;
 +
 +	dma_addr = ib_dma_map_page(dev, virt_to_page(base),
 +				   offset, len, DMA_TO_DEVICE);
 +	if (ib_dma_mapping_error(dev, dma_addr))
 +		goto out_maperr;
 +
 +	ctxt->sge[sge_no].addr = dma_addr;
 +	ctxt->sge[sge_no].length = len;
 +	ctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;
 +	svc_rdma_count_mappings(rdma, ctxt);
 +	return 0;
 +
 +out_maperr:
 +	pr_err("svcrdma: failed to map buffer\n");
 +	return -EIO;
 +}
 +
  static int svc_rdma_dma_map_page(struct svcxprt_rdma *rdma,
++<<<<<<< HEAD
 +				 struct svc_rdma_op_ctxt *ctxt,
 +				 unsigned int sge_no,
++=======
+ 				 struct svc_rdma_send_ctxt *ctxt,
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  				 struct page *page,
 -				 unsigned long offset,
 +				 unsigned int offset,
  				 unsigned int len)
  {
  	struct ib_device *dev = rdma->sc_cm_id->device;
@@@ -346,10 -496,9 +534,16 @@@
  	if (ib_dma_mapping_error(dev, dma_addr))
  		goto out_maperr;
  
++<<<<<<< HEAD
 +	ctxt->sge[sge_no].addr = dma_addr;
 +	ctxt->sge[sge_no].length = len;
 +	ctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;
 +	svc_rdma_count_mappings(rdma, ctxt);
++=======
+ 	ctxt->sc_sges[ctxt->sc_cur_sge_no].addr = dma_addr;
+ 	ctxt->sc_sges[ctxt->sc_cur_sge_no].length = len;
+ 	ctxt->sc_send_wr.num_sge++;
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  	return 0;
  
  out_maperr:
@@@ -357,6 -506,18 +551,21 @@@
  	return -EIO;
  }
  
++<<<<<<< HEAD
++=======
+ /* ib_dma_map_page() is used here because svc_rdma_dma_unmap()
+  * handles DMA-unmap and it uses ib_dma_unmap_page() exclusively.
+  */
+ static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,
+ 				struct svc_rdma_send_ctxt *ctxt,
+ 				unsigned char *base,
+ 				unsigned int len)
+ {
+ 	return svc_rdma_dma_map_page(rdma, ctxt, virt_to_page(base),
+ 				     offset_in_page(base), len);
+ }
+ 
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  /**
   * svc_rdma_map_reply_hdr - DMA map the transport header buffer
   * @rdma: controlling transport
@@@ -373,10 -534,10 +582,17 @@@ int svc_rdma_map_reply_hdr(struct svcxp
  			   __be32 *rdma_resp,
  			   unsigned int len)
  {
++<<<<<<< HEAD
 +	ctxt->direction = DMA_TO_DEVICE;
 +	ctxt->pages[0] = virt_to_page(rdma_resp);
 +	ctxt->count = 1;
 +	return svc_rdma_dma_map_page(rdma, ctxt, 0, ctxt->pages[0], 0, len);
++=======
+ 	ctxt->sc_pages[0] = virt_to_page(rdma_resp);
+ 	ctxt->sc_page_count++;
+ 	ctxt->sc_cur_sge_no = 0;
+ 	return svc_rdma_dma_map_page(rdma, ctxt, ctxt->sc_pages[0], 0, len);
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  }
  
  /* Load the xdr_buf into the ctxt's sge array, and DMA map each
@@@ -385,10 -546,11 +601,15 @@@
   * Returns zero on success, or a negative errno on failure.
   */
  static int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
 -				  struct svc_rdma_send_ctxt *ctxt,
 +				  struct svc_rdma_op_ctxt *ctxt,
  				  struct xdr_buf *xdr, __be32 *wr_lst)
  {
++<<<<<<< HEAD
 +	unsigned int len, sge_no, remaining, page_off;
++=======
+ 	unsigned int len, remaining;
+ 	unsigned long page_off;
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  	struct page **ppages;
  	unsigned char *base;
  	u32 xdr_pad;
diff --cc net/sunrpc/xprtrdma/svc_rdma_transport.c
index c28ee7409658,e9535a66bab0..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_transport.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_transport.c
@@@ -776,9 -529,9 +781,15 @@@ static struct svc_xprt *svc_rdma_accept
  	qp_attr.port_num = newxprt->sc_port_num;
  	qp_attr.cap.max_rdma_ctxs = ctxts;
  	qp_attr.cap.max_send_wr = newxprt->sc_sq_depth - ctxts;
++<<<<<<< HEAD
 +	qp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;
 +	qp_attr.cap.max_send_sge = newxprt->sc_max_sge;
 +	qp_attr.cap.max_recv_sge = newxprt->sc_max_sge;
++=======
+ 	qp_attr.cap.max_recv_wr = rq_depth;
+ 	qp_attr.cap.max_send_sge = newxprt->sc_max_send_sges;
+ 	qp_attr.cap.max_recv_sge = 1;
++>>>>>>> 25fd86eca11c (svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt)
  	qp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
  	qp_attr.qp_type = IB_QPT_RC;
  	qp_attr.send_cq = newxprt->sc_sq_cq;
* Unmerged path include/linux/sunrpc/svc_rdma.h
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_sendto.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_transport.c
