io: define stronger ordering for the default writeX() implementation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Sinan Kaya <okaya@codeaurora.org>
commit 755bd04aaf4bf7c49ce8aad2677ea4d14271fc46
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/755bd04a.failed

The default implementation of mapping writeX() to __raw_writeX() is wrong.
writeX() has stronger ordering semantics. Compiler is allowed to reorder
memory writes against __raw_writeX().

Use the previously defined __io_aw() and __io_bw() macros to harden
code generation according to architecture support.

	Signed-off-by: Sinan Kaya <okaya@codeaurora.org>
	Signed-off-by: Arnd Bergmann <arnd@arndb.de>
(cherry picked from commit 755bd04aaf4bf7c49ce8aad2677ea4d14271fc46)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/asm-generic/io.h
diff --cc include/asm-generic/io.h
index 66a3fecbf792,964725e4f459..000000000000
--- a/include/asm-generic/io.h
+++ b/include/asm-generic/io.h
@@@ -52,7 -101,156 +52,160 @@@ static inline u32 __raw_readl(const vol
  }
  #endif
  
++<<<<<<< HEAD
 +#define readb __raw_readb
++=======
+ #ifdef CONFIG_64BIT
+ #ifndef __raw_readq
+ #define __raw_readq __raw_readq
+ static inline u64 __raw_readq(const volatile void __iomem *addr)
+ {
+ 	return *(const volatile u64 __force *)addr;
+ }
+ #endif
+ #endif /* CONFIG_64BIT */
+ 
+ #ifndef __raw_writeb
+ #define __raw_writeb __raw_writeb
+ static inline void __raw_writeb(u8 value, volatile void __iomem *addr)
+ {
+ 	*(volatile u8 __force *)addr = value;
+ }
+ #endif
+ 
+ #ifndef __raw_writew
+ #define __raw_writew __raw_writew
+ static inline void __raw_writew(u16 value, volatile void __iomem *addr)
+ {
+ 	*(volatile u16 __force *)addr = value;
+ }
+ #endif
+ 
+ #ifndef __raw_writel
+ #define __raw_writel __raw_writel
+ static inline void __raw_writel(u32 value, volatile void __iomem *addr)
+ {
+ 	*(volatile u32 __force *)addr = value;
+ }
+ #endif
+ 
+ #ifdef CONFIG_64BIT
+ #ifndef __raw_writeq
+ #define __raw_writeq __raw_writeq
+ static inline void __raw_writeq(u64 value, volatile void __iomem *addr)
+ {
+ 	*(volatile u64 __force *)addr = value;
+ }
+ #endif
+ #endif /* CONFIG_64BIT */
+ 
+ /*
+  * {read,write}{b,w,l,q}() access little endian memory and return result in
+  * native endianness.
+  */
+ 
+ #ifndef readb
+ #define readb readb
+ static inline u8 readb(const volatile void __iomem *addr)
+ {
+ 	u8 val;
+ 
+ 	__io_br();
+ 	val = __raw_readb(addr);
+ 	__io_ar();
+ 	return val;
+ }
+ #endif
+ 
+ #ifndef readw
+ #define readw readw
+ static inline u16 readw(const volatile void __iomem *addr)
+ {
+ 	u16 val;
+ 
+ 	__io_br();
+ 	val = __le16_to_cpu(__raw_readw(addr));
+ 	__io_ar();
+ 	return val;
+ }
+ #endif
+ 
+ #ifndef readl
+ #define readl readl
+ static inline u32 readl(const volatile void __iomem *addr)
+ {
+ 	u32 val;
+ 
+ 	__io_br();
+ 	val = __le32_to_cpu(__raw_readl(addr));
+ 	__io_ar();
+ 	return val;
+ }
+ #endif
+ 
+ #ifdef CONFIG_64BIT
+ #ifndef readq
+ #define readq readq
+ static inline u64 readq(const volatile void __iomem *addr)
+ {
+ 	u64 val;
+ 
+ 	__io_br();
+ 	val = __le64_to_cpu(__raw_readq(addr));
+ 	__io_ar();
+ 	return val;
+ }
+ #endif
+ #endif /* CONFIG_64BIT */
+ 
+ #ifndef writeb
+ #define writeb writeb
+ static inline void writeb(u8 value, volatile void __iomem *addr)
+ {
+ 	__io_bw();
+ 	__raw_writeb(value, addr);
+ 	__io_aw();
+ }
+ #endif
+ 
+ #ifndef writew
+ #define writew writew
+ static inline void writew(u16 value, volatile void __iomem *addr)
+ {
+ 	__io_bw();
+ 	__raw_writew(cpu_to_le16(value), addr);
+ 	__io_aw();
+ }
+ #endif
+ 
+ #ifndef writel
+ #define writel writel
+ static inline void writel(u32 value, volatile void __iomem *addr)
+ {
+ 	__io_bw();
+ 	__raw_writel(__cpu_to_le32(value), addr);
+ 	__io_aw();
+ }
+ #endif
+ 
+ #ifdef CONFIG_64BIT
+ #ifndef writeq
+ #define writeq writeq
+ static inline void writeq(u64 value, volatile void __iomem *addr)
+ {
+ 	__io_bw();
+ 	__raw_writeq(__cpu_to_le64(value), addr);
+ 	__io_aw();
+ }
+ #endif
+ #endif /* CONFIG_64BIT */
+ 
+ /*
+  * {read,write}{b,w,l,q}_relaxed() are like the regular version, but
+  * are not guaranteed to provide ordering against spinlocks or memory
+  * accesses.
+  */
++>>>>>>> 755bd04aaf4b (io: define stronger ordering for the default writeX() implementation)
  #ifndef readb_relaxed
  #define readb_relaxed readb
  #endif
* Unmerged path include/asm-generic/io.h
