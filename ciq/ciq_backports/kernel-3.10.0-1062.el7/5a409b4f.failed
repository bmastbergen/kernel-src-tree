MD: fix lock contention for flush bios

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [md] fix lock contention for flush bios (Nigel Croxon) [1636944 1527943 1576466 1599780]
Rebuild_FUZZ: 94.44%
commit-author Xiao Ni <xni@redhat.com>
commit 5a409b4f56d50b212334f338cb8465d65550cd85
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5a409b4f.failed

There is a lock contention when there are many processes which send flush bios
to md device. eg. Create many lvs on one raid device and mkfs.xfs on each lv.

Now it just can handle flush request sequentially. It needs to wait mddev->flush_bio
to be NULL, otherwise get mddev->lock.

This patch remove mddev->flush_bio and handle flush bio asynchronously.
I did a test with command dbench -s 128 -t 300. This is the test result:

=================Without the patch============================
 Operation                Count    AvgLat    MaxLat
 --------------------------------------------------
 Flush                    11165   167.595  5879.560
 Close                   107469     1.391  2231.094
 LockX                      384     0.003     0.019
 Rename                    5944     2.141  1856.001
 ReadX                   208121     0.003     0.074
 WriteX                   98259  1925.402 15204.895
 Unlink                   25198    13.264  3457.268
 UnlockX                    384     0.001     0.009
 FIND_FIRST               47111     0.012     0.076
 SET_FILE_INFORMATION     12966     0.007     0.065
 QUERY_FILE_INFORMATION   27921     0.004     0.085
 QUERY_PATH_INFORMATION  124650     0.005     5.766
 QUERY_FS_INFORMATION     22519     0.003     0.053
 NTCreateX               141086     4.291  2502.812

Throughput 3.7181 MB/sec (sync open)  128 clients  128 procs  max_latency=15204.905 ms

=================With the patch============================
 Operation                Count    AvgLat    MaxLat
 --------------------------------------------------
 Flush                     4500   174.134   406.398
 Close                    48195     0.060   467.062
 LockX                      256     0.003     0.029
 Rename                    2324     0.026     0.360
 ReadX                    78846     0.004     0.504
 WriteX                   66832   562.775  1467.037
 Unlink                    5516     3.665  1141.740
 UnlockX                    256     0.002     0.019
 FIND_FIRST               16428     0.015     0.313
 SET_FILE_INFORMATION      6400     0.009     0.520
 QUERY_FILE_INFORMATION   17865     0.003     0.089
 QUERY_PATH_INFORMATION   47060     0.078   416.299
 QUERY_FS_INFORMATION      7024     0.004     0.032
 NTCreateX                55921     0.854  1141.452

Throughput 11.744 MB/sec (sync open)  128 clients  128 procs  max_latency=1467.041 ms

The test is done on raid1 disk with two rotational disks

V5: V4 is more complicated than the version with memory pool. So revert to the memory pool
version

V4: use address of fbio to do hash to choose free flush info.
V3:
Shaohua suggests mempool is overkill. In v3 it allocs memory during creating raid device
and uses a simple bitmap to record which resource is free.

Fix a bug from v2. It should set flush_pending to 1 at first.

V2:
Neil pointed out two problems. One is counting error problem and another is return value
when allocat memory fails.
1. counting error problem
This isn't safe.  It is only safe to call rdev_dec_pending() on rdevs
that you previously called
                          atomic_inc(&rdev->nr_pending);
If an rdev was added to the list between the start and end of the flush,
this will do something bad.

Now it doesn't use bio_chain. It uses specified call back function for each
flush bio.
2. Returned on IO error when kmalloc fails is wrong.
I use mempool suggested by Neil in V2
3. Fixed some places pointed by Guoqing

	Suggested-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Xiao Ni <xni@redhat.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 5a409b4f56d50b212334f338cb8465d65550cd85)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/md.c
#	drivers/md/md.h
diff --cc drivers/md/md.c
index c487b2c6f765,6b4e2f29fe4e..000000000000
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@@ -430,11 -430,25 +448,30 @@@ static int md_mergeable_bvec(struct req
  /*
   * Generic flush handling for md
   */
++<<<<<<< HEAD
 +
 +static void md_end_flush(struct bio *bio, int err)
++=======
+ static void submit_flushes(struct work_struct *ws)
++>>>>>>> 5a409b4f56d5 (MD: fix lock contention for flush bios)
  {
- 	struct md_rdev *rdev = bio->bi_private;
- 	struct mddev *mddev = rdev->mddev;
+ 	struct flush_info *fi = container_of(ws, struct flush_info, flush_work);
+ 	struct mddev *mddev = fi->mddev;
+ 	struct bio *bio = fi->bio;
+ 
+ 	bio->bi_opf &= ~REQ_PREFLUSH;
+ 	md_handle_request(mddev, bio);
+ 
+ 	mempool_free(fi, mddev->flush_pool);
+ }
+ 
+ static void md_end_flush(struct bio *fbio)
+ {
+ 	struct flush_bio *fb = fbio->bi_private;
+ 	struct md_rdev *rdev = fb->rdev;
+ 	struct flush_info *fi = fb->fi;
+ 	struct bio *bio = fi->bio;
+ 	struct mddev *mddev = fi->mddev;
  
  	rdev_dec_pending(rdev, mddev);
  
@@@ -466,55 -490,35 +513,66 @@@ void md_flush_request(struct mddev *mdd
  			atomic_inc(&rdev->nr_pending);
  			atomic_inc(&rdev->nr_pending);
  			rcu_read_unlock();
+ 
+ 			fb = mempool_alloc(mddev->flush_bio_pool, GFP_NOIO);
+ 			fb->fi = fi;
+ 			fb->rdev = rdev;
+ 
  			bi = bio_alloc_mddev(GFP_NOIO, 0, mddev);
++<<<<<<< HEAD
 +			bi->bi_end_io = md_end_flush;
 +			bi->bi_private = rdev;
 +			bi->bi_bdev = rdev->bdev;
 +			atomic_inc(&mddev->flush_pending);
 +			submit_bio(WRITE_FLUSH, bi);
++=======
+ 			bio_set_dev(bi, rdev->bdev);
+ 			bi->bi_end_io = md_end_flush;
+ 			bi->bi_private = fb;
+ 			bi->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
+ 
+ 			atomic_inc(&fi->flush_pending);
+ 			submit_bio(bi);
+ 
++>>>>>>> 5a409b4f56d5 (MD: fix lock contention for flush bios)
  			rcu_read_lock();
  			rdev_dec_pending(rdev, mddev);
  		}
  	rcu_read_unlock();
- 	if (atomic_dec_and_test(&mddev->flush_pending))
- 		queue_work(md_wq, &mddev->flush_work);
- }
  
++<<<<<<< HEAD
 +static void md_submit_flush_data(struct work_struct *ws)
 +{
 +	struct mddev *mddev = container_of(ws, struct mddev, flush_work);
 +	struct bio *bio = mddev->flush_bio;
 +
 +	/*
 +	 * must reset flush_bio before calling into md_handle_request to avoid a
 +	 * deadlock, because other bios passed md_handle_request suspend check
 +	 * could wait for this and below md_handle_request could wait for those
 +	 * bios because of suspend check
 +	 */
 +	mddev->flush_bio = NULL;
 +	wake_up(&mddev->sb_wait);
 +
 +	if (bio->bi_size == 0)
 +		/* an empty barrier - all done */
 +		bio_endio(bio, 0);
 +	else {
 +		bio->bi_rw &= ~REQ_FLUSH;
 +		md_handle_request(mddev, bio);
++=======
+ 	if (atomic_dec_and_test(&fi->flush_pending)) {
+ 		if (bio->bi_iter.bi_size == 0)
+ 			/* an empty barrier - all done */
+ 			bio_endio(bio);
+ 		else {
+ 			INIT_WORK(&fi->flush_work, submit_flushes);
+ 			queue_work(md_wq, &fi->flush_work);
+ 		}
++>>>>>>> 5a409b4f56d5 (MD: fix lock contention for flush bios)
  	}
  }
- 
- void md_flush_request(struct mddev *mddev, struct bio *bio)
- {
- 	spin_lock_irq(&mddev->lock);
- 	wait_event_lock_irq(mddev->sb_wait,
- 			    !mddev->flush_bio,
- 			    mddev->lock);
- 	mddev->flush_bio = bio;
- 	spin_unlock_irq(&mddev->lock);
- 
- 	INIT_WORK(&mddev->flush_work, submit_flushes);
- 	queue_work(md_wq, &mddev->flush_work);
- }
  EXPORT_SYMBOL(md_flush_request);
  
  static inline struct mddev *mddev_get(struct mddev *mddev)
@@@ -5812,7 -5933,14 +5893,18 @@@ void md_stop(struct mddev *mddev
  	 * This is called from dm-raid
  	 */
  	__md_stop(mddev);
++<<<<<<< HEAD
 +	bitmap_destroy(mddev);
++=======
+ 	if (mddev->flush_bio_pool) {
+ 		mempool_destroy(mddev->flush_bio_pool);
+ 		mddev->flush_bio_pool = NULL;
+ 	}
+ 	if (mddev->flush_pool) {
+ 		mempool_destroy(mddev->flush_pool);
+ 		mddev->flush_pool = NULL;
+ 	}
++>>>>>>> 5a409b4f56d5 (MD: fix lock contention for flush bios)
  	if (mddev->bio_set) {
  		bioset_free(mddev->bio_set);
  		mddev->bio_set = NULL;
diff --cc drivers/md/md.h
index 94c72c13bbab,ffcb1ae217fe..000000000000
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@@ -481,15 -470,11 +494,20 @@@ struct mddev 
  						   * metadata and bitmap writes
  						   */
  
++<<<<<<< HEAD
 +	/* Generic flush handling.
 +	 * The last to finish preflush schedules a worker to submit
 +	 * the rest of the request (without the REQ_FLUSH flag).
 +	 */
 +	struct bio *flush_bio;
 +	atomic_t flush_pending;
 +	struct work_struct flush_work;
++=======
+ 	mempool_t			*flush_pool;
+ 	mempool_t			*flush_bio_pool;
++>>>>>>> 5a409b4f56d5 (MD: fix lock contention for flush bios)
  	struct work_struct event_work;	/* used by dm to report failure event */
  	void (*sync_super)(struct mddev *mddev, struct md_rdev *rdev);
 -	struct md_cluster_info		*cluster_info;
  	unsigned int			good_device_nr;	/* good device num within cluster raid */
  
  	bool	has_superblocks:1;
* Unmerged path drivers/md/md.c
* Unmerged path drivers/md/md.h
