perf stat: Fix shadow stats for clock events

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Ravi Bangoria <ravi.bangoria@linux.ibm.com>
commit 57ddf09173c1e7d0511ead8924675c7198e56545
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/57ddf091.failed

Commit 0aa802a79469 ("perf stat: Get rid of extra clock display
function") introduced scale and unit for clock events. Thus,
perf_stat__update_shadow_stats() now saves scaled values of clock events
in msecs, instead of original nsecs. But while calculating values of
shadow stats we still consider clock event values in nsecs. This results
in a wrong shadow stat values. Ex,

  # ./perf stat -e task-clock,cycles ls
    <SNIP>
              2.60 msec task-clock:u    #    0.877 CPUs utilized
         2,430,564      cycles:u        # 1215282.000 GHz

Fix this by saving original nsec values for clock events in
perf_stat__update_shadow_stats(). After patch:

  # ./perf stat -e task-clock,cycles ls
    <SNIP>
              3.14 msec task-clock:u    #    0.839 CPUs utilized
         3,094,528      cycles:u        #    0.985 GHz

	Suggested-by: Jiri Olsa <jolsa@redhat.com>
	Reported-by: Anton Blanchard <anton@samba.org>
	Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
	Reviewed-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Jin Yao <yao.jin@linux.intel.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Thomas Richter <tmricht@linux.vnet.ibm.com>
	Cc: yuzhoujian@didichuxing.com
Fixes: 0aa802a79469 ("perf stat: Get rid of extra clock display function")
Link: http://lkml.kernel.org/r/20181116042843.24067-1-ravi.bangoria@linux.ibm.com
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 57ddf09173c1e7d0511ead8924675c7198e56545)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/util/stat-shadow.c
diff --cc tools/perf/util/stat-shadow.c
index b29877d9b2f4,3c22c58b3e90..000000000000
--- a/tools/perf/util/stat-shadow.c
+++ b/tools/perf/util/stat-shadow.c
@@@ -222,58 -206,66 +222,64 @@@ void perf_stat__reset_shadow_stats(void
   * instruction rates, etc:
   */
  void perf_stat__update_shadow_stats(struct perf_evsel *counter, u64 count,
 -				    int cpu, struct runtime_stat *st)
 +				    int cpu)
  {
  	int ctx = evsel_context(counter);
+ 	u64 count_ns = count;
  
  	count *= counter->scale;
  
++<<<<<<< HEAD
 +	if (perf_evsel__match(counter, SOFTWARE, SW_TASK_CLOCK) ||
 +	    perf_evsel__match(counter, SOFTWARE, SW_CPU_CLOCK))
 +		update_stats(&runtime_nsecs_stats[cpu], count);
++=======
+ 	if (perf_evsel__is_clock(counter))
+ 		update_runtime_stat(st, STAT_NSECS, 0, cpu, count_ns);
++>>>>>>> 57ddf09173c1 (perf stat: Fix shadow stats for clock events)
  	else if (perf_evsel__match(counter, HARDWARE, HW_CPU_CYCLES))
 -		update_runtime_stat(st, STAT_CYCLES, ctx, cpu, count);
 +		update_stats(&runtime_cycles_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, CYCLES_IN_TX))
 -		update_runtime_stat(st, STAT_CYCLES_IN_TX, ctx, cpu, count);
 +		update_stats(&runtime_cycles_in_tx_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TRANSACTION_START))
 -		update_runtime_stat(st, STAT_TRANSACTION, ctx, cpu, count);
 +		update_stats(&runtime_transaction_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, ELISION_START))
 -		update_runtime_stat(st, STAT_ELISION, ctx, cpu, count);
 +		update_stats(&runtime_elision_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_TOTAL_SLOTS))
 -		update_runtime_stat(st, STAT_TOPDOWN_TOTAL_SLOTS,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_total_slots[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_SLOTS_ISSUED))
 -		update_runtime_stat(st, STAT_TOPDOWN_SLOTS_ISSUED,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_slots_issued[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_SLOTS_RETIRED))
 -		update_runtime_stat(st, STAT_TOPDOWN_SLOTS_RETIRED,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_slots_retired[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_FETCH_BUBBLES))
 -		update_runtime_stat(st, STAT_TOPDOWN_FETCH_BUBBLES,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_fetch_bubbles[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, TOPDOWN_RECOVERY_BUBBLES))
 -		update_runtime_stat(st, STAT_TOPDOWN_RECOVERY_BUBBLES,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_topdown_recovery_bubbles[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_STALLED_CYCLES_FRONTEND))
 -		update_runtime_stat(st, STAT_STALLED_CYCLES_FRONT,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_stalled_cycles_front_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_STALLED_CYCLES_BACKEND))
 -		update_runtime_stat(st, STAT_STALLED_CYCLES_BACK,
 -				    ctx, cpu, count);
 +		update_stats(&runtime_stalled_cycles_back_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_BRANCH_INSTRUCTIONS))
 -		update_runtime_stat(st, STAT_BRANCHES, ctx, cpu, count);
 +		update_stats(&runtime_branches_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HARDWARE, HW_CACHE_REFERENCES))
 -		update_runtime_stat(st, STAT_CACHEREFS, ctx, cpu, count);
 +		update_stats(&runtime_cacherefs_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_L1D))
 -		update_runtime_stat(st, STAT_L1_DCACHE, ctx, cpu, count);
 +		update_stats(&runtime_l1_dcache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_L1I))
 -		update_runtime_stat(st, STAT_L1_ICACHE, ctx, cpu, count);
 +		update_stats(&runtime_ll_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_LL))
 -		update_runtime_stat(st, STAT_LL_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_ll_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_DTLB))
 -		update_runtime_stat(st, STAT_DTLB_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_dtlb_cache_stats[ctx][cpu], count);
  	else if (perf_evsel__match(counter, HW_CACHE, HW_CACHE_ITLB))
 -		update_runtime_stat(st, STAT_ITLB_CACHE, ctx, cpu, count);
 +		update_stats(&runtime_itlb_cache_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, SMI_NUM))
 -		update_runtime_stat(st, STAT_SMI_NUM, ctx, cpu, count);
 +		update_stats(&runtime_smi_num_stats[ctx][cpu], count);
  	else if (perf_stat_evsel__is(counter, APERF))
 -		update_runtime_stat(st, STAT_APERF, ctx, cpu, count);
 +		update_stats(&runtime_aperf_stats[ctx][cpu], count);
  
  	if (counter->collect_stat) {
 -		struct saved_value *v = saved_value_lookup(counter, cpu, true,
 -							   STAT_NONE, 0, st);
 +		struct saved_value *v = saved_value_lookup(counter, cpu, true);
  		update_stats(&v->stats, count);
  	}
  }
* Unmerged path tools/perf/util/stat-shadow.c
