x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm/mmu: make vcpu->mmu a pointer to the current MMU (Vitaly Kuznetsov) [1565739 1497611]
Rebuild_FUZZ: 96.30%
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 44dd3ffa7bb31126e0fc4f6f30398546eb494388
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/44dd3ffa.failed

As a preparation to full MMU split between L1 and L2 make vcpu->arch.mmu
a pointer to the currently used mmu. For now, this is always
vcpu->arch.root_mmu. No functional change.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
	Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
(cherry picked from commit 44dd3ffa7bb31126e0fc4f6f30398546eb494388)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/mmu.h
#	arch/x86/kvm/mmu_audit.c
#	arch/x86/kvm/paging_tmpl.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/mmu.c
index e7052d2c946d,dac9e977c703..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2099,12 -2180,8 +2099,17 @@@ static void kvm_mmu_commit_zap_page(str
  static bool __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
  			    struct list_head *invalid_list)
  {
++<<<<<<< HEAD
 +	if (sp->role.cr4_pae != !!is_pae(vcpu)) {
 +		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
 +		return false;
 +	}
 +
 +	if (vcpu->arch.mmu.sync_page(vcpu, sp) == 0) {
++=======
+ 	if (sp->role.cr4_pae != !!is_pae(vcpu)
+ 	    || vcpu->arch.mmu->sync_page(vcpu, sp) == 0) {
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
  		return false;
  	}
@@@ -2292,9 -2372,10 +2297,9 @@@ static struct kvm_mmu_page *kvm_mmu_get
  	struct kvm_mmu_page *sp;
  	bool need_sync = false;
  	bool flush = false;
 -	int collisions = 0;
  	LIST_HEAD(invalid_list);
  
- 	role = vcpu->arch.mmu.base_role;
+ 	role = vcpu->arch.mmu->base_role;
  	role.level = level;
  	role.direct = direct;
  	if (role.direct)
@@@ -2365,21 -2448,31 +2370,39 @@@
  	return sp;
  }
  
 -static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 -					struct kvm_vcpu *vcpu, hpa_t root,
 -					u64 addr)
 +static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 +			     struct kvm_vcpu *vcpu, u64 addr)
  {
  	iterator->addr = addr;
++<<<<<<< HEAD
 +	iterator->shadow_addr = vcpu->arch.mmu.root_hpa;
 +	iterator->level = vcpu->arch.mmu.shadow_root_level;
 +
 +	if (iterator->level == PT64_ROOT_LEVEL &&
 +	    vcpu->arch.mmu.root_level < PT64_ROOT_LEVEL &&
 +	    !vcpu->arch.mmu.direct_map)
 +		--iterator->level;
 +
 +	if (iterator->level == PT32E_ROOT_LEVEL) {
++=======
+ 	iterator->shadow_addr = root;
+ 	iterator->level = vcpu->arch.mmu->shadow_root_level;
+ 
+ 	if (iterator->level == PT64_ROOT_4LEVEL &&
+ 	    vcpu->arch.mmu->root_level < PT64_ROOT_4LEVEL &&
+ 	    !vcpu->arch.mmu->direct_map)
+ 		--iterator->level;
+ 
+ 	if (iterator->level == PT32E_ROOT_LEVEL) {
+ 		/*
+ 		 * prev_root is currently only used for 64-bit hosts. So only
+ 		 * the active root_hpa is valid here.
+ 		 */
+ 		BUG_ON(root != vcpu->arch.mmu->root_hpa);
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		iterator->shadow_addr
- 			= vcpu->arch.mmu.pae_root[(addr >> 30) & 3];
+ 			= vcpu->arch.mmu->pae_root[(addr >> 30) & 3];
  		iterator->shadow_addr &= PT64_BASE_ADDR_MASK;
  		--iterator->level;
  		if (!iterator->shadow_addr)
@@@ -2387,6 -2480,13 +2410,16 @@@
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
+ 			     struct kvm_vcpu *vcpu, u64 addr)
+ {
+ 	shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
+ 				    addr);
+ }
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
  {
  	if (iterator->level < PT_PAGE_TABLE_LEVEL)
@@@ -3319,34 -3460,44 +3352,39 @@@ static int nonpaging_map(struct kvm_vcp
  out_unlock:
  	spin_unlock(&vcpu->kvm->mmu_lock);
  	kvm_release_pfn_clean(pfn);
 -	return RET_PF_RETRY;
 +	return 0;
  }
  
 -static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 -			       struct list_head *invalid_list)
 -{
 -	struct kvm_mmu_page *sp;
 -
 -	if (!VALID_PAGE(*root_hpa))
 -		return;
 -
 -	sp = page_header(*root_hpa & PT64_BASE_ADDR_MASK);
 -	--sp->root_count;
 -	if (!sp->root_count && sp->role.invalid)
 -		kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
  
 -	*root_hpa = INVALID_PAGE;
 -}
 -
 -/* roots_to_free must be some combination of the KVM_MMU_ROOT_* flags */
 -void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, ulong roots_to_free)
 +static void mmu_free_roots(struct kvm_vcpu *vcpu)
  {
  	int i;
 +	struct kvm_mmu_page *sp;
  	LIST_HEAD(invalid_list);
++<<<<<<< HEAD
++=======
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 	bool free_active_root = roots_to_free & KVM_MMU_ROOT_CURRENT;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  
 -	BUILD_BUG_ON(KVM_MMU_NUM_PREV_ROOTS >= BITS_PER_LONG);
 +	if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
 +		return;
  
 -	/* Before acquiring the MMU lock, see if we need to do any real work. */
 -	if (!(free_active_root && VALID_PAGE(mmu->root_hpa))) {
 -		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 -			if ((roots_to_free & KVM_MMU_ROOT_PREVIOUS(i)) &&
 -			    VALID_PAGE(mmu->prev_roots[i].hpa))
 -				break;
 +	if (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL &&
 +	    (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL ||
 +	     vcpu->arch.mmu.direct_map)) {
 +		hpa_t root = vcpu->arch.mmu.root_hpa;
  
 -		if (i == KVM_MMU_NUM_PREV_ROOTS)
 -			return;
 +		spin_lock(&vcpu->kvm->mmu_lock);
 +		sp = page_header(root);
 +		--sp->root_count;
 +		if (!sp->root_count && sp->role.invalid) {
 +			kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
 +			kvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);
 +		}
 +		spin_unlock(&vcpu->kvm->mmu_lock);
 +		vcpu->arch.mmu.root_hpa = INVALID_PAGE;
 +		return;
  	}
  
  	spin_lock(&vcpu->kvm->mmu_lock);
@@@ -3385,19 -3544,20 +3423,28 @@@ static int mmu_alloc_direct_roots(struc
  	struct kvm_mmu_page *sp;
  	unsigned i;
  
++<<<<<<< HEAD
 +	if (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL) {
++=======
+ 	if (vcpu->arch.mmu->shadow_root_level >= PT64_ROOT_4LEVEL) {
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		spin_lock(&vcpu->kvm->mmu_lock);
  		if(make_mmu_pages_available(vcpu) < 0) {
  			spin_unlock(&vcpu->kvm->mmu_lock);
 -			return -ENOSPC;
 +			return 1;
  		}
++<<<<<<< HEAD
 +		sp = kvm_mmu_get_page(vcpu, 0, 0, PT64_ROOT_LEVEL, 1, ACC_ALL);
++=======
+ 		sp = kvm_mmu_get_page(vcpu, 0, 0,
+ 				vcpu->arch.mmu->shadow_root_level, 1, ACC_ALL);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		++sp->root_count;
  		spin_unlock(&vcpu->kvm->mmu_lock);
- 		vcpu->arch.mmu.root_hpa = __pa(sp->spt);
- 	} else if (vcpu->arch.mmu.shadow_root_level == PT32E_ROOT_LEVEL) {
+ 		vcpu->arch.mmu->root_hpa = __pa(sp->spt);
+ 	} else if (vcpu->arch.mmu->shadow_root_level == PT32E_ROOT_LEVEL) {
  		for (i = 0; i < 4; ++i) {
- 			hpa_t root = vcpu->arch.mmu.pae_root[i];
+ 			hpa_t root = vcpu->arch.mmu->pae_root[i];
  
  			MMU_WARN_ON(VALID_PAGE(root));
  			spin_lock(&vcpu->kvm->mmu_lock);
@@@ -3435,18 -3595,18 +3482,28 @@@ static int mmu_alloc_shadow_roots(struc
  	 * Do we shadow a long mode page table? If so we need to
  	 * write-protect the guests page table root.
  	 */
++<<<<<<< HEAD
 +	if (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL) {
 +		hpa_t root = vcpu->arch.mmu.root_hpa;
++=======
+ 	if (vcpu->arch.mmu->root_level >= PT64_ROOT_4LEVEL) {
+ 		hpa_t root = vcpu->arch.mmu->root_hpa;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  
  		MMU_WARN_ON(VALID_PAGE(root));
  
  		spin_lock(&vcpu->kvm->mmu_lock);
  		if (make_mmu_pages_available(vcpu) < 0) {
  			spin_unlock(&vcpu->kvm->mmu_lock);
 -			return -ENOSPC;
 +			return 1;
  		}
++<<<<<<< HEAD
 +		sp = kvm_mmu_get_page(vcpu, root_gfn, 0, PT64_ROOT_LEVEL,
 +				      0, ACC_ALL);
++=======
+ 		sp = kvm_mmu_get_page(vcpu, root_gfn, 0,
+ 				vcpu->arch.mmu->shadow_root_level, 0, ACC_ALL);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		root = __pa(sp->spt);
  		++sp->root_count;
  		spin_unlock(&vcpu->kvm->mmu_lock);
@@@ -3460,7 -3620,7 +3517,11 @@@
  	 * the shadow page table may be a PAE or a long mode page table.
  	 */
  	pm_mask = PT_PRESENT_MASK;
++<<<<<<< HEAD
 +	if (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL)
++=======
+ 	if (vcpu->arch.mmu->shadow_root_level == PT64_ROOT_4LEVEL)
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		pm_mask |= PT_ACCESSED_MASK | PT_WRITABLE_MASK | PT_USER_MASK;
  
  	for (i = 0; i < 4; ++i) {
@@@ -3496,8 -3656,8 +3557,13 @@@
  	 * If we shadow a 32 bit page table with a long mode page
  	 * table we enter this path.
  	 */
++<<<<<<< HEAD
 +	if (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL) {
 +		if (vcpu->arch.mmu.lm_root == NULL) {
++=======
+ 	if (vcpu->arch.mmu->shadow_root_level == PT64_ROOT_4LEVEL) {
+ 		if (vcpu->arch.mmu->lm_root == NULL) {
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  			/*
  			 * The additional page necessary for this is only
  			 * allocated on demand.
@@@ -3540,16 -3700,40 +3606,22 @@@ static void mmu_sync_roots(struct kvm_v
  		return;
  
  	vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
++<<<<<<< HEAD
 +	kvm_mmu_audit(vcpu, AUDIT_PRE_SYNC);
 +	if (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL) {
 +		hpa_t root = vcpu->arch.mmu.root_hpa;
++=======
+ 
+ 	if (vcpu->arch.mmu->root_level >= PT64_ROOT_4LEVEL) {
+ 		hpa_t root = vcpu->arch.mmu->root_hpa;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		sp = page_header(root);
 -
 -		/*
 -		 * Even if another CPU was marking the SP as unsync-ed
 -		 * simultaneously, any guest page table changes are not
 -		 * guaranteed to be visible anyway until this VCPU issues a TLB
 -		 * flush strictly after those changes are made. We only need to
 -		 * ensure that the other CPU sets these flags before any actual
 -		 * changes to the page tables are made. The comments in
 -		 * mmu_need_write_protect() describe what could go wrong if this
 -		 * requirement isn't satisfied.
 -		 */
 -		if (!smp_load_acquire(&sp->unsync) &&
 -		    !smp_load_acquire(&sp->unsync_children))
 -			return;
 -
 -		spin_lock(&vcpu->kvm->mmu_lock);
 -		kvm_mmu_audit(vcpu, AUDIT_PRE_SYNC);
 -
  		mmu_sync_children(vcpu, sp);
 -
  		kvm_mmu_audit(vcpu, AUDIT_POST_SYNC);
 -		spin_unlock(&vcpu->kvm->mmu_lock);
  		return;
  	}
 -
 -	spin_lock(&vcpu->kvm->mmu_lock);
 -	kvm_mmu_audit(vcpu, AUDIT_PRE_SYNC);
 -
  	for (i = 0; i < 4; ++i) {
- 		hpa_t root = vcpu->arch.mmu.pae_root[i];
+ 		hpa_t root = vcpu->arch.mmu->pae_root[i];
  
  		if (root && VALID_PAGE(root)) {
  			root &= PT64_BASE_ADDR_MASK;
@@@ -3849,10 -4050,10 +3921,10 @@@ static int tdp_page_fault(struct kvm_vc
  	int write = error_code & PFERR_WRITE_MASK;
  	bool map_writable;
  
- 	MMU_WARN_ON(!VALID_PAGE(vcpu->arch.mmu.root_hpa));
+ 	MMU_WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa));
  
  	if (page_fault_handle_page_track(vcpu, error_code, gfn))
 -		return RET_PF_EMULATE;
 +		return 1;
  
  	r = mmu_topup_memory_caches(vcpu);
  	if (r)
@@@ -3914,11 -4113,103 +3986,100 @@@ static void nonpaging_init_context(stru
  	context->nx = false;
  }
  
 -/*
 - * Find out if a previously cached root matching the new CR3/role is available.
 - * The current root is also inserted into the cache.
 - * If a matching root was found, it is assigned to kvm_mmu->root_hpa and true is
 - * returned.
 - * Otherwise, the LRU root from the cache is assigned to kvm_mmu->root_hpa and
 - * false is returned. This root should now be freed by the caller.
 - */
 -static bool cached_root_available(struct kvm_vcpu *vcpu, gpa_t new_cr3,
 -				  union kvm_mmu_page_role new_role)
 +void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	mmu_free_roots(vcpu);
 +}
 +
++=======
+ 	uint i;
+ 	struct kvm_mmu_root_info root;
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 
+ 	root.cr3 = mmu->get_cr3(vcpu);
+ 	root.hpa = mmu->root_hpa;
+ 
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {
+ 		swap(root, mmu->prev_roots[i]);
+ 
+ 		if (new_cr3 == root.cr3 && VALID_PAGE(root.hpa) &&
+ 		    page_header(root.hpa) != NULL &&
+ 		    new_role.word == page_header(root.hpa)->role.word)
+ 			break;
+ 	}
+ 
+ 	mmu->root_hpa = root.hpa;
+ 
+ 	return i < KVM_MMU_NUM_PREV_ROOTS;
+ }
+ 
+ static bool fast_cr3_switch(struct kvm_vcpu *vcpu, gpa_t new_cr3,
+ 			    union kvm_mmu_page_role new_role,
+ 			    bool skip_tlb_flush)
+ {
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 
+ 	/*
+ 	 * For now, limit the fast switch to 64-bit hosts+VMs in order to avoid
+ 	 * having to deal with PDPTEs. We may add support for 32-bit hosts/VMs
+ 	 * later if necessary.
+ 	 */
+ 	if (mmu->shadow_root_level >= PT64_ROOT_4LEVEL &&
+ 	    mmu->root_level >= PT64_ROOT_4LEVEL) {
+ 		if (mmu_check_root(vcpu, new_cr3 >> PAGE_SHIFT))
+ 			return false;
+ 
+ 		if (cached_root_available(vcpu, new_cr3, new_role)) {
+ 			/*
+ 			 * It is possible that the cached previous root page is
+ 			 * obsolete because of a change in the MMU
+ 			 * generation number. However, that is accompanied by
+ 			 * KVM_REQ_MMU_RELOAD, which will free the root that we
+ 			 * have set here and allocate a new one.
+ 			 */
+ 
+ 			kvm_make_request(KVM_REQ_LOAD_CR3, vcpu);
+ 			if (!skip_tlb_flush) {
+ 				kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
+ 				kvm_x86_ops->tlb_flush(vcpu, true);
+ 			}
+ 
+ 			/*
+ 			 * The last MMIO access's GVA and GPA are cached in the
+ 			 * VCPU. When switching to a new CR3, that GVA->GPA
+ 			 * mapping may no longer be valid. So clear any cached
+ 			 * MMIO info even when we don't need to sync the shadow
+ 			 * page tables.
+ 			 */
+ 			vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ 
+ 			__clear_sp_write_flooding_count(
+ 				page_header(mmu->root_hpa));
+ 
+ 			return true;
+ 		}
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void __kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3,
+ 			      union kvm_mmu_page_role new_role,
+ 			      bool skip_tlb_flush)
+ {
+ 	if (!fast_cr3_switch(vcpu, new_cr3, new_role, skip_tlb_flush))
+ 		kvm_mmu_free_roots(vcpu, KVM_MMU_ROOT_CURRENT);
+ }
+ 
+ void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush)
+ {
+ 	__kvm_mmu_new_cr3(vcpu, new_cr3, kvm_mmu_calc_root_page_role(vcpu),
+ 			  skip_tlb_flush);
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_new_cr3);
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  static unsigned long get_cr3(struct kvm_vcpu *vcpu)
  {
  	return kvm_read_cr3(vcpu);
@@@ -4420,14 -4722,27 +4581,14 @@@ static void paging32E_init_context(stru
  	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
  }
  
 -static union kvm_mmu_page_role
 -kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu)
 -{
 -	union kvm_mmu_page_role role = {0};
 -
 -	role.guest_mode = is_guest_mode(vcpu);
 -	role.smm = is_smm(vcpu);
 -	role.ad_disabled = (shadow_accessed_mask == 0);
 -	role.level = kvm_x86_ops->get_tdp_level(vcpu);
 -	role.direct = true;
 -	role.access = ACC_ALL;
 -
 -	return role;
 -}
 -
  static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
  {
- 	struct kvm_mmu *context = &vcpu->arch.mmu;
+ 	struct kvm_mmu *context = vcpu->arch.mmu;
  
 -	context->base_role.word = mmu_base_role_mask.word &
 -				  kvm_calc_tdp_mmu_root_page_role(vcpu).word;
 +	context->base_role.word = 0;
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
 +	context->base_role.ad_disabled = (shadow_accessed_mask == 0);
  	context->page_fault = tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
@@@ -4467,13 -4782,36 +4628,17 @@@
  	reset_tdp_shadow_zero_bits_mask(vcpu, context);
  }
  
 -static union kvm_mmu_page_role
 -kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu)
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
  {
 -	union kvm_mmu_page_role role = {0};
++<<<<<<< HEAD
  	bool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
  	bool smap = kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
 -
 -	role.nxe = is_nx(vcpu);
 -	role.cr4_pae = !!is_pae(vcpu);
 -	role.cr0_wp  = is_write_protection(vcpu);
 -	role.smep_andnot_wp = smep && !is_write_protection(vcpu);
 -	role.smap_andnot_wp = smap && !is_write_protection(vcpu);
 -	role.guest_mode = is_guest_mode(vcpu);
 -	role.smm = is_smm(vcpu);
 -	role.direct = !is_paging(vcpu);
 -	role.access = ACC_ALL;
 -
 -	if (!is_long_mode(vcpu))
 -		role.level = PT32E_ROOT_LEVEL;
 -	else if (is_la57_mode(vcpu))
 -		role.level = PT64_ROOT_5LEVEL;
 -	else
 -		role.level = PT64_ROOT_4LEVEL;
 -
 -	return role;
 -}
 -
 -void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 -{
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
++=======
+ 	struct kvm_mmu *context = vcpu->arch.mmu;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
 +
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
  
  	if (!is_paging(vcpu))
  		nonpaging_init_context(vcpu, context);
@@@ -4497,14 -4828,29 +4662,37 @@@
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
  
++<<<<<<< HEAD
++=======
+ static union kvm_mmu_page_role
+ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty)
+ {
+ 	union kvm_mmu_page_role role = vcpu->arch.mmu->base_role;
+ 
+ 	role.level = PT64_ROOT_4LEVEL;
+ 	role.direct = false;
+ 	role.ad_disabled = !accessed_dirty;
+ 	role.guest_mode = true;
+ 	role.access = ACC_ALL;
+ 
+ 	return role;
+ }
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 -			     bool accessed_dirty, gpa_t new_eptp)
 +			     bool accessed_dirty)
  {
++<<<<<<< HEAD
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
++=======
+ 	struct kvm_mmu *context = vcpu->arch.mmu;
+ 	union kvm_mmu_page_role root_page_role =
+ 		kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  
 -	__kvm_mmu_new_cr3(vcpu, new_eptp, root_page_role, false);
 -	context->shadow_root_level = PT64_ROOT_4LEVEL;
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
 +
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
  
  	context->nx = true;
  	context->ept_ad = accessed_dirty;
@@@ -4546,10 -4890,12 +4734,19 @@@ static void init_kvm_nested_mmu(struct 
  	g_context->inject_page_fault = kvm_inject_page_fault;
  
  	/*
++<<<<<<< HEAD
 +	 * Note that arch.mmu.gva_to_gpa translates l2_gva to l1_gpa. The
 +	 * translation of l2_gpa to l1_gpa addresses is done using the
 +	 * arch.nested_mmu.gva_to_gpa function. Basically the gva_to_gpa
 +	 * functions between mmu and nested_mmu are swapped.
++=======
+ 	 * Note that arch.mmu->gva_to_gpa translates l2_gpa to l1_gpa using
+ 	 * L1's nested page tables (e.g. EPT12). The nested translation
+ 	 * of l2_gva to l1_gpa is done by arch.nested_mmu.gva_to_gpa using
+ 	 * L2's page tables as the first level of translation and L1's
+ 	 * nested page tables as the second level of translation. Basically
+ 	 * the gva_to_gpa functions between mmu and nested_mmu are swapped.
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	 */
  	if (!is_paging(vcpu)) {
  		g_context->nx = false;
@@@ -4577,8 -4924,17 +4774,20 @@@
  	update_last_nonleaf_level(vcpu, g_context);
  }
  
 -void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
 +static void init_kvm_mmu(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
++=======
+ 	if (reset_roots) {
+ 		uint i;
+ 
+ 		vcpu->arch.mmu->root_hpa = INVALID_PAGE;
+ 
+ 		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 			vcpu->arch.mmu->prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
+ 	}
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	if (mmu_is_nested(vcpu))
  		init_kvm_nested_mmu(vcpu);
  	else if (tdp_enabled)
@@@ -4614,8 -4980,8 +4823,13 @@@ EXPORT_SYMBOL_GPL(kvm_mmu_load)
  
  void kvm_mmu_unload(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	mmu_free_roots(vcpu);
 +	WARN_ON(VALID_PAGE(vcpu->arch.mmu.root_hpa));
++=======
+ 	kvm_mmu_free_roots(vcpu, KVM_MMU_ROOTS_ALL);
+ 	WARN_ON(VALID_PAGE(vcpu->arch.mmu->root_hpa));
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_unload);
  
@@@ -4819,8 -5175,8 +5033,13 @@@ static void kvm_mmu_pte_write(struct kv
  			entry = *spte;
  			mmu_page_zap_pte(vcpu->kvm, sp, spte);
  			if (gentry &&
++<<<<<<< HEAD
 +			      !((sp->role.word ^ vcpu->arch.mmu.base_role.word)
 +			      & mask.word) && rmap_can_add(vcpu))
++=======
+ 			      !((sp->role.word ^ vcpu->arch.mmu->base_role.word)
+ 			      & mmu_base_role_mask.word) && rmap_can_add(vcpu))
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  				mmu_pte_write_new_pte(vcpu, sp, spte, &gentry);
  			if (need_remote_flush(entry, *spte))
  				remote_flush = true;
@@@ -4871,30 -5227,34 +5090,54 @@@ static int make_mmu_pages_available(str
  int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
  		       void *insn, int insn_len)
  {
 -	int r, emulation_type = 0;
 +	int r, emulation_type = EMULTYPE_RETRY;
  	enum emulation_result er;
++<<<<<<< HEAD
 +	bool direct = vcpu->arch.mmu.direct_map || mmu_is_nested(vcpu);
 +
 +	vcpu->arch.l1tf_flush_l1d = true;
++=======
+ 	bool direct = vcpu->arch.mmu->direct_map;
+ 
+ 	/* With shadow page tables, fault_address contains a GVA or nGPA.  */
+ 	if (vcpu->arch.mmu->direct_map) {
+ 		vcpu->arch.gpa_available = true;
+ 		vcpu->arch.gpa_val = cr2;
+ 	}
+ 
+ 	r = RET_PF_INVALID;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	if (unlikely(error_code & PFERR_RSVD_MASK)) {
  		r = handle_mmio_page_fault(vcpu, cr2, direct);
 -		if (r == RET_PF_EMULATE)
 +		if (r == RET_MMIO_PF_EMULATE) {
 +			emulation_type = 0;
  			goto emulate;
 +		}
 +		if (r == RET_MMIO_PF_RETRY)
 +			return 1;
 +		if (r < 0)
 +			return r;
 +		/* Must be RET_MMIO_PF_INVALID.  */
  	}
  
++<<<<<<< HEAD
 +	r = vcpu->arch.mmu.page_fault(vcpu, cr2, lower_32_bits(error_code),
 +				      false);
++=======
+ 	if (r == RET_PF_INVALID) {
+ 		r = vcpu->arch.mmu->page_fault(vcpu, cr2,
+ 					       lower_32_bits(error_code),
+ 					       false);
+ 		WARN_ON(r == RET_PF_INVALID);
+ 	}
+ 
+ 	if (r == RET_PF_RETRY)
+ 		return 1;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	if (r < 0)
  		return r;
 +	if (!r)
 +		return 1;
  
  	/*
  	 * Before emulating the instruction, check if the error code
@@@ -4930,12 -5311,67 +5173,75 @@@ EXPORT_SYMBOL_GPL(kvm_mmu_page_fault)
  
  void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
  {
++<<<<<<< HEAD
 +	vcpu->arch.mmu.invlpg(vcpu, gva);
 +	kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
++=======
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 	int i;
+ 
+ 	/* INVLPG on a * non-canonical address is a NOP according to the SDM.  */
+ 	if (is_noncanonical_address(gva, vcpu))
+ 		return;
+ 
+ 	mmu->invlpg(vcpu, gva, mmu->root_hpa);
+ 
+ 	/*
+ 	 * INVLPG is required to invalidate any global mappings for the VA,
+ 	 * irrespective of PCID. Since it would take us roughly similar amount
+ 	 * of work to determine whether any of the prev_root mappings of the VA
+ 	 * is marked global, or to just sync it blindly, so we might as well
+ 	 * just always sync it.
+ 	 *
+ 	 * Mappings not reachable via the current cr3 or the prev_roots will be
+ 	 * synced when switching to that cr3, so nothing needs to be done here
+ 	 * for them.
+ 	 */
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 		if (VALID_PAGE(mmu->prev_roots[i].hpa))
+ 			mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);
+ 
+ 	kvm_x86_ops->tlb_flush_gva(vcpu, gva);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	++vcpu->stat.invlpg;
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);
  
++<<<<<<< HEAD
++=======
+ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
+ {
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 	bool tlb_flush = false;
+ 	uint i;
+ 
+ 	if (pcid == kvm_get_active_pcid(vcpu)) {
+ 		mmu->invlpg(vcpu, gva, mmu->root_hpa);
+ 		tlb_flush = true;
+ 	}
+ 
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {
+ 		if (VALID_PAGE(mmu->prev_roots[i].hpa) &&
+ 		    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].cr3)) {
+ 			mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);
+ 			tlb_flush = true;
+ 		}
+ 	}
+ 
+ 	if (tlb_flush)
+ 		kvm_x86_ops->tlb_flush_gva(vcpu, gva);
+ 
+ 	++vcpu->stat.invlpg;
+ 
+ 	/*
+ 	 * Mappings not reachable via the current cr3 or the prev_roots will be
+ 	 * synced when switching to that cr3, so nothing needs to be done here
+ 	 * for them.
+ 	 */
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_invpcid_gva);
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  void kvm_enable_tdp(void)
  {
  	tdp_enabled = true;
@@@ -4980,19 -5416,30 +5286,33 @@@ static int alloc_mmu_pages(struct kvm_v
  
  int kvm_mmu_create(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	vcpu->arch.walk_mmu = &vcpu->arch.mmu;
 +	vcpu->arch.mmu.root_hpa = INVALID_PAGE;
 +	vcpu->arch.mmu.translate_gpa = translate_gpa;
 +	vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
 +
++=======
+ 	uint i;
+ 
+ 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ 	vcpu->arch.root_mmu.root_hpa = INVALID_PAGE;
+ 	vcpu->arch.root_mmu.translate_gpa = translate_gpa;
+ 	vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
+ 
+ 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 		vcpu->arch.root_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	return alloc_mmu_pages(vcpu);
  }
  
  void kvm_mmu_setup(struct kvm_vcpu *vcpu)
  {
- 	MMU_WARN_ON(VALID_PAGE(vcpu->arch.mmu.root_hpa));
+ 	MMU_WARN_ON(VALID_PAGE(vcpu->arch.mmu->root_hpa));
  
 -	/*
 -	 * kvm_mmu_setup() is called only on vCPU initialization.  
 -	 * Therefore, no need to reset mmu roots as they are not yet
 -	 * initialized.
 -	 */
 -	kvm_init_mmu(vcpu, false);
 +	init_kvm_mmu(vcpu);
  }
  
  static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
diff --cc arch/x86/kvm/mmu.h
index 3d1af6f75377,c7b333147c4a..000000000000
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@@ -78,6 -81,27 +78,30 @@@ static inline int kvm_mmu_reload(struc
  	return kvm_mmu_load(vcpu);
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long kvm_get_pcid(struct kvm_vcpu *vcpu, gpa_t cr3)
+ {
+ 	BUILD_BUG_ON((X86_CR3_PCID_MASK & PAGE_MASK) != 0);
+ 
+ 	return kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE)
+ 	       ? cr3 & X86_CR3_PCID_MASK
+ 	       : 0;
+ }
+ 
+ static inline unsigned long kvm_get_active_pcid(struct kvm_vcpu *vcpu)
+ {
+ 	return kvm_get_pcid(vcpu, kvm_read_cr3(vcpu));
+ }
+ 
+ static inline void kvm_mmu_load_cr3(struct kvm_vcpu *vcpu)
+ {
+ 	if (VALID_PAGE(vcpu->arch.mmu->root_hpa))
+ 		vcpu->arch.mmu->set_cr3(vcpu, vcpu->arch.mmu->root_hpa |
+ 					      kvm_get_active_pcid(vcpu));
+ }
+ 
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  /*
   * Currently, we have two sorts of write-protection, a) the first one
   * write-protects guest page to sync the guest modification, b) another one is
diff --cc arch/x86/kvm/mmu_audit.c
index 12fc217d85fa,abac7e208853..000000000000
--- a/arch/x86/kvm/mmu_audit.c
+++ b/arch/x86/kvm/mmu_audit.c
@@@ -59,14 -59,14 +59,22 @@@ static void mmu_spte_walk(struct kvm_vc
  	int i;
  	struct kvm_mmu_page *sp;
  
- 	if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
+ 	if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
  		return;
  
++<<<<<<< HEAD
 +	if (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL) {
 +		hpa_t root = vcpu->arch.mmu.root_hpa;
 +
 +		sp = page_header(root);
 +		__mmu_spte_walk(vcpu, sp, fn, PT64_ROOT_LEVEL);
++=======
+ 	if (vcpu->arch.mmu->root_level >= PT64_ROOT_4LEVEL) {
+ 		hpa_t root = vcpu->arch.mmu->root_hpa;
+ 
+ 		sp = page_header(root);
+ 		__mmu_spte_walk(vcpu, sp, fn, vcpu->arch.mmu->root_level);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  		return;
  	}
  
diff --cc arch/x86/kvm/paging_tmpl.h
index b281446d6b8c,7cf2185b7eb5..000000000000
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@@ -508,8 -509,8 +509,13 @@@ FNAME(prefetch_gpte)(struct kvm_vcpu *v
  	pgprintk("%s: gpte %llx spte %p\n", __func__, (u64)gpte, spte);
  
  	gfn = gpte_to_gfn(gpte);
++<<<<<<< HEAD
 +	pte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);
 +	FNAME(protect_clean_gpte)(&vcpu->arch.mmu, &pte_access, gpte);
++=======
+ 	pte_access = sp->role.access & FNAME(gpte_access)(gpte);
+ 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	pfn = pte_prefetch_gfn_to_pfn(vcpu, gfn,
  			no_dirty_log && (pte_access & ACC_WRITE_MASK));
  	if (is_error_pfn(pfn))
@@@ -1003,8 -1004,8 +1009,13 @@@ static int FNAME(sync_page)(struct kvm_
  
  		gfn = gpte_to_gfn(gpte);
  		pte_access = sp->role.access;
++<<<<<<< HEAD
 +		pte_access &= FNAME(gpte_access)(vcpu, gpte);
 +		FNAME(protect_clean_gpte)(&vcpu->arch.mmu, &pte_access, gpte);
++=======
+ 		pte_access &= FNAME(gpte_access)(gpte);
+ 		FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  
  		if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access,
  		      &nr_present))
diff --cc arch/x86/kvm/svm.c
index 2abd6b8af3db,2936c63bcc2f..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -2585,12 -2918,12 +2585,21 @@@ static void nested_svm_init_mmu_context
  {
  	WARN_ON(mmu_is_nested(vcpu));
  	kvm_init_shadow_mmu(vcpu);
++<<<<<<< HEAD
 +	vcpu->arch.mmu.set_cr3           = nested_svm_set_tdp_cr3;
 +	vcpu->arch.mmu.get_cr3           = nested_svm_get_tdp_cr3;
 +	vcpu->arch.mmu.get_pdptr         = nested_svm_get_tdp_pdptr;
 +	vcpu->arch.mmu.inject_page_fault = nested_svm_inject_npf_exit;
 +	vcpu->arch.mmu.shadow_root_level = get_npt_level();
 +	reset_shadow_zero_bits_mask(vcpu, &vcpu->arch.mmu);
++=======
+ 	vcpu->arch.mmu->set_cr3           = nested_svm_set_tdp_cr3;
+ 	vcpu->arch.mmu->get_cr3           = nested_svm_get_tdp_cr3;
+ 	vcpu->arch.mmu->get_pdptr         = nested_svm_get_tdp_pdptr;
+ 	vcpu->arch.mmu->inject_page_fault = nested_svm_inject_npf_exit;
+ 	vcpu->arch.mmu->shadow_root_level = get_npt_level(vcpu);
+ 	reset_shadow_zero_bits_mask(vcpu, vcpu->arch.mmu);
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
  }
  
diff --cc arch/x86/kvm/vmx.c
index 5501a1957de2,d243eba62340..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -4088,12 -5107,14 +4088,18 @@@ static void exit_lmode(struct kvm_vcpu 
  
  #endif
  
 -static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 -				bool invalidate_gpa)
 +static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid)
  {
++<<<<<<< HEAD
 +	if (enable_ept) {
 +		if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
++=======
+ 	if (enable_ept && (invalidate_gpa || !enable_vpid)) {
+ 		if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  			return;
- 		ept_sync_context(construct_eptp(vcpu, vcpu->arch.mmu.root_hpa));
+ 		ept_sync_context(construct_eptp(vcpu,
+ 						vcpu->arch.mmu->root_hpa));
  	} else {
  		vpid_sync_context(vpid);
  	}
@@@ -7897,11 -9074,84 +7903,89 @@@ static int handle_invvpid(struct kvm_vc
  		return 1;
  	}
  
 -	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 -	type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
 +	__vmx_flush_tlb(vcpu, vmx->nested.vpid02);
 +	nested_vmx_succeed(vcpu);
  
++<<<<<<< HEAD
 +	skip_emulated_instruction(vcpu);
 +	return 1;
++=======
+ 	if (type > 3) {
+ 		kvm_inject_gp(vcpu, 0);
+ 		return 1;
+ 	}
+ 
+ 	/* According to the Intel instruction reference, the memory operand
+ 	 * is read even if it isn't needed (e.g., for type==all)
+ 	 */
+ 	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
+ 				vmx_instruction_info, false, &gva))
+ 		return 1;
+ 
+ 	if (kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e)) {
+ 		kvm_inject_page_fault(vcpu, &e);
+ 		return 1;
+ 	}
+ 
+ 	if (operand.pcid >> 12 != 0) {
+ 		kvm_inject_gp(vcpu, 0);
+ 		return 1;
+ 	}
+ 
+ 	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
+ 
+ 	switch (type) {
+ 	case INVPCID_TYPE_INDIV_ADDR:
+ 		if ((!pcid_enabled && (operand.pcid != 0)) ||
+ 		    is_noncanonical_address(operand.gla, vcpu)) {
+ 			kvm_inject_gp(vcpu, 0);
+ 			return 1;
+ 		}
+ 		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	case INVPCID_TYPE_SINGLE_CTXT:
+ 		if (!pcid_enabled && (operand.pcid != 0)) {
+ 			kvm_inject_gp(vcpu, 0);
+ 			return 1;
+ 		}
+ 
+ 		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
+ 			kvm_mmu_sync_roots(vcpu);
+ 			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
+ 		}
+ 
+ 		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].cr3)
+ 			    == operand.pcid)
+ 				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
+ 
+ 		kvm_mmu_free_roots(vcpu, roots_to_free);
+ 		/*
+ 		 * If neither the current cr3 nor any of the prev_roots use the
+ 		 * given PCID, then nothing needs to be done here because a
+ 		 * resync will happen anyway before switching to any other CR3.
+ 		 */
+ 
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	case INVPCID_TYPE_ALL_NON_GLOBAL:
+ 		/*
+ 		 * Currently, KVM doesn't mark global entries in the shadow
+ 		 * page tables, so a non-global flush just degenerates to a
+ 		 * global flush. If needed, we could optimize this later by
+ 		 * keeping track of global entries in shadow page tables.
+ 		 */
+ 
+ 		/* fall-through */
+ 	case INVPCID_TYPE_ALL_INCL_GLOBAL:
+ 		kvm_mmu_unload(vcpu);
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	default:
+ 		BUG(); /* We have already checked above that type <= 3 */
+ 	}
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  }
  
  static int handle_pml_full(struct kvm_vcpu *vcpu)
@@@ -9704,23 -11296,20 +9788,31 @@@ static unsigned long nested_ept_get_cr3
  	return get_vmcs12(vcpu)->ept_pointer;
  }
  
 -static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 +static int nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
  {
  	WARN_ON(mmu_is_nested(vcpu));
 +	if (!valid_ept_address(vcpu, nested_ept_get_cr3(vcpu)))
 +		return 1;
  
 +	kvm_mmu_unload(vcpu);
  	kvm_init_shadow_ept_mmu(vcpu,
 -			to_vmx(vcpu)->nested.msrs.ept_caps &
 +			to_vmx(vcpu)->nested.nested_vmx_ept_caps &
  			VMX_EPT_EXECUTE_ONLY_BIT,
++<<<<<<< HEAD
 +			nested_ept_ad_enabled(vcpu));
 +	vcpu->arch.mmu.set_cr3           = vmx_set_cr3;
 +	vcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;
 +	vcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;
++=======
+ 			nested_ept_ad_enabled(vcpu),
+ 			nested_ept_get_cr3(vcpu));
+ 	vcpu->arch.mmu->set_cr3           = vmx_set_cr3;
+ 	vcpu->arch.mmu->get_cr3           = nested_ept_get_cr3;
+ 	vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
++>>>>>>> 44dd3ffa7bb3 (x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU)
  
  	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 +	return 0;
  }
  
  static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 86bec63d5f8a..1e5507d5f929 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -459,7 +459,10 @@ struct kvm_vcpu_arch {
 	 * the paging mode of the l1 guest. This context is always used to
 	 * handle faults.
 	 */
-	struct kvm_mmu mmu;
+	struct kvm_mmu *mmu;
+
+	/* Non-nested MMU for L1 */
+	struct kvm_mmu root_mmu;
 
 	/*
 	 * Paging state of an L2 guest (used for nested npt)
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/mmu.h
* Unmerged path arch/x86/kvm/mmu_audit.c
* Unmerged path arch/x86/kvm/paging_tmpl.h
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index bc19a3517278..e922db26ec41 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -460,7 +460,7 @@ static bool kvm_propagate_fault(struct kvm_vcpu *vcpu, struct x86_exception *fau
 	if (mmu_is_nested(vcpu) && !fault->nested_page_fault)
 		vcpu->arch.nested_mmu.inject_page_fault(vcpu, fault);
 	else
-		vcpu->arch.mmu.inject_page_fault(vcpu, fault);
+		vcpu->arch.mmu->inject_page_fault(vcpu, fault);
 
 	return fault->nested_page_fault;
 }
@@ -559,7 +559,7 @@ int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3)
 	for (i = 0; i < ARRAY_SIZE(pdpte); ++i) {
 		if ((pdpte[i] & PT_PRESENT_MASK) &&
 		    (pdpte[i] &
-		     vcpu->arch.mmu.guest_rsvd_check.rsvd_bits_mask[0][2])) {
+		     vcpu->arch.mmu->guest_rsvd_check.rsvd_bits_mask[0][2])) {
 			ret = 0;
 			goto out;
 		}
@@ -4245,7 +4245,7 @@ gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 
 	/* NPT walks are always user-walks */
 	access |= PFERR_USER_MASK;
-	t_gpa  = vcpu->arch.mmu.gva_to_gpa(vcpu, gpa, access, exception);
+	t_gpa  = vcpu->arch.mmu->gva_to_gpa(vcpu, gpa, access, exception);
 
 	return t_gpa;
 }
@@ -5276,7 +5276,7 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t cr2,
 	if (emulation_type & EMULTYPE_NO_REEXECUTE)
 		return false;
 
-	if (!vcpu->arch.mmu.direct_map) {
+	if (!vcpu->arch.mmu->direct_map) {
 		/*
 		 * Write permission should be allowed since only
 		 * write access need to be emulated.
@@ -5309,7 +5309,7 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t cr2,
 	kvm_release_pfn_clean(pfn);
 
 	/* The instructions are well-emulated on direct mmu. */
-	if (vcpu->arch.mmu.direct_map) {
+	if (vcpu->arch.mmu->direct_map) {
 		unsigned int indirect_shadow_pages;
 
 		spin_lock(&vcpu->kvm->mmu_lock);
@@ -5373,7 +5373,7 @@ static bool retry_instruction(struct x86_emulate_ctxt *ctxt,
 	vcpu->arch.last_retry_eip = ctxt->eip;
 	vcpu->arch.last_retry_addr = cr2;
 
-	if (!vcpu->arch.mmu.direct_map)
+	if (!vcpu->arch.mmu->direct_map)
 		gpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2, NULL);
 
 	kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
@@ -8347,7 +8347,7 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
 
-	if ((vcpu->arch.mmu.direct_map != work->arch.direct_map) ||
+	if ((vcpu->arch.mmu->direct_map != work->arch.direct_map) ||
 	      work->wakeup_all)
 		return;
 
@@ -8355,11 +8355,11 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 	if (unlikely(r))
 		return;
 
-	if (!vcpu->arch.mmu.direct_map &&
-	      work->arch.cr3 != vcpu->arch.mmu.get_cr3(vcpu))
+	if (!vcpu->arch.mmu->direct_map &&
+	      work->arch.cr3 != vcpu->arch.mmu->get_cr3(vcpu))
 		return;
 
-	vcpu->arch.mmu.page_fault(vcpu, work->gva, 0, true);
+	vcpu->arch.mmu->page_fault(vcpu, work->gva, 0, true);
 }
 
 static inline u32 kvm_async_pf_hash_fn(gfn_t gfn)
