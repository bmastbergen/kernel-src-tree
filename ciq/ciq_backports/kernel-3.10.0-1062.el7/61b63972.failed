mm, swap: use page-cluster as max window of VMA based swap readahead

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] swap: use page-cluster as max window of VMA based swap readahead (Rafael Aquini) [1485248]
Rebuild_FUZZ: 96.97%
commit-author Huang Ying <ying.huang@intel.com>
commit 61b639723be5a9fc4812d5d85cb769589afa5a38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/61b63972.failed

When the VMA based swap readahead was introduced, a new knob

  /sys/kernel/mm/swap/vma_ra_max_order

was added as the max window of VMA swap readahead.  This is to make it
possible to use different max window for VMA based readahead and
original physical readahead.  But Minchan Kim pointed out that this will
cause a regression because setting page-cluster sysctl to zero cannot
disable swap readahead with the change.

To fix the regression, the page-cluster sysctl is used as the max window
of both the VMA based swap readahead and original physical swap
readahead.  If more fine grained control is needed in the future, more
knobs can be added as the subordinate knobs of the page-cluster sysctl.

The vma_ra_max_order knob is deleted.  Because the knob was introduced
in v4.14-rc1, and this patch is targeting being merged before v4.14
releasing, there should be no existing users of this newly added ABI.

Link: http://lkml.kernel.org/r/20171011070847.16003-1-ying.huang@intel.com
Fixes: ec560175c0b6fce ("mm, swap: VMA based swap readahead")
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Reported-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Fengguang Wu <fengguang.wu@intel.com>
	Cc: Tim Chen <tim.c.chen@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 61b639723be5a9fc4812d5d85cb769589afa5a38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/ABI/testing/sysfs-kernel-mm-swap
#	mm/swap_state.c
diff --cc mm/swap_state.c
index bbce650716cb,05b6803f0cce..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -40,8 -37,28 +40,30 @@@ struct backing_dev_info swap_backing_de
  
  struct address_space *swapper_spaces[MAX_SWAPFILES];
  static unsigned int nr_swapper_spaces[MAX_SWAPFILES];
++<<<<<<< HEAD
++=======
+ bool swap_vma_readahead = true;
+ 
+ #define SWAP_RA_WIN_SHIFT	(PAGE_SHIFT / 2)
+ #define SWAP_RA_HITS_MASK	((1UL << SWAP_RA_WIN_SHIFT) - 1)
+ #define SWAP_RA_HITS_MAX	SWAP_RA_HITS_MASK
+ #define SWAP_RA_WIN_MASK	(~PAGE_MASK & ~SWAP_RA_HITS_MASK)
+ 
+ #define SWAP_RA_HITS(v)		((v) & SWAP_RA_HITS_MASK)
+ #define SWAP_RA_WIN(v)		(((v) & SWAP_RA_WIN_MASK) >> SWAP_RA_WIN_SHIFT)
+ #define SWAP_RA_ADDR(v)		((v) & PAGE_MASK)
+ 
+ #define SWAP_RA_VAL(addr, win, hits)				\
+ 	(((addr) & PAGE_MASK) |					\
+ 	 (((win) << SWAP_RA_WIN_SHIFT) & SWAP_RA_WIN_MASK) |	\
+ 	 ((hits) & SWAP_RA_HITS_MASK))
+ 
+ /* Initial readahead hits is 4 to start up with a small window */
+ #define GET_SWAP_RA_VAL(vma)					\
+ 	(atomic_long_read(&(vma)->swap_readahead_info) ? : 4)
++>>>>>>> 61b639723be5 (mm, swap: use page-cluster as max window of VMA based swap readahead)
  
  #define INC_CACHE_INFO(x)	do { swap_cache_info.x++; } while (0)
 -#define ADD_CACHE_INFO(x, nr)	do { swap_cache_info.x += (nr); } while (0)
  
  static struct {
  	unsigned long add_total;
@@@ -527,3 -631,187 +549,190 @@@ void exit_swap_address_space(unsigned i
  	synchronize_rcu();
  	kvfree(spaces);
  }
++<<<<<<< HEAD
++=======
+ 
+ static inline void swap_ra_clamp_pfn(struct vm_area_struct *vma,
+ 				     unsigned long faddr,
+ 				     unsigned long lpfn,
+ 				     unsigned long rpfn,
+ 				     unsigned long *start,
+ 				     unsigned long *end)
+ {
+ 	*start = max3(lpfn, PFN_DOWN(vma->vm_start),
+ 		      PFN_DOWN(faddr & PMD_MASK));
+ 	*end = min3(rpfn, PFN_DOWN(vma->vm_end),
+ 		    PFN_DOWN((faddr & PMD_MASK) + PMD_SIZE));
+ }
+ 
+ struct page *swap_readahead_detect(struct vm_fault *vmf,
+ 				   struct vma_swap_readahead *swap_ra)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	unsigned long swap_ra_info;
+ 	struct page *page;
+ 	swp_entry_t entry;
+ 	unsigned long faddr, pfn, fpfn;
+ 	unsigned long start, end;
+ 	pte_t *pte;
+ 	unsigned int max_win, hits, prev_win, win, left;
+ #ifndef CONFIG_64BIT
+ 	pte_t *tpte;
+ #endif
+ 
+ 	max_win = 1 << min_t(unsigned int, READ_ONCE(page_cluster),
+ 			     SWAP_RA_ORDER_CEILING);
+ 	if (max_win == 1) {
+ 		swap_ra->win = 1;
+ 		return NULL;
+ 	}
+ 
+ 	faddr = vmf->address;
+ 	entry = pte_to_swp_entry(vmf->orig_pte);
+ 	if ((unlikely(non_swap_entry(entry))))
+ 		return NULL;
+ 	page = lookup_swap_cache(entry, vma, faddr);
+ 	if (page)
+ 		return page;
+ 
+ 	fpfn = PFN_DOWN(faddr);
+ 	swap_ra_info = GET_SWAP_RA_VAL(vma);
+ 	pfn = PFN_DOWN(SWAP_RA_ADDR(swap_ra_info));
+ 	prev_win = SWAP_RA_WIN(swap_ra_info);
+ 	hits = SWAP_RA_HITS(swap_ra_info);
+ 	swap_ra->win = win = __swapin_nr_pages(pfn, fpfn, hits,
+ 					       max_win, prev_win);
+ 	atomic_long_set(&vma->swap_readahead_info,
+ 			SWAP_RA_VAL(faddr, win, 0));
+ 
+ 	if (win == 1)
+ 		return NULL;
+ 
+ 	/* Copy the PTEs because the page table may be unmapped */
+ 	if (fpfn == pfn + 1)
+ 		swap_ra_clamp_pfn(vma, faddr, fpfn, fpfn + win, &start, &end);
+ 	else if (pfn == fpfn + 1)
+ 		swap_ra_clamp_pfn(vma, faddr, fpfn - win + 1, fpfn + 1,
+ 				  &start, &end);
+ 	else {
+ 		left = (win - 1) / 2;
+ 		swap_ra_clamp_pfn(vma, faddr, fpfn - left, fpfn + win - left,
+ 				  &start, &end);
+ 	}
+ 	swap_ra->nr_pte = end - start;
+ 	swap_ra->offset = fpfn - start;
+ 	pte = vmf->pte - swap_ra->offset;
+ #ifdef CONFIG_64BIT
+ 	swap_ra->ptes = pte;
+ #else
+ 	tpte = swap_ra->ptes;
+ 	for (pfn = start; pfn != end; pfn++)
+ 		*tpte++ = *pte++;
+ #endif
+ 
+ 	return NULL;
+ }
+ 
+ struct page *do_swap_page_readahead(swp_entry_t fentry, gfp_t gfp_mask,
+ 				    struct vm_fault *vmf,
+ 				    struct vma_swap_readahead *swap_ra)
+ {
+ 	struct blk_plug plug;
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct page *page;
+ 	pte_t *pte, pentry;
+ 	swp_entry_t entry;
+ 	unsigned int i;
+ 	bool page_allocated;
+ 
+ 	if (swap_ra->win == 1)
+ 		goto skip;
+ 
+ 	blk_start_plug(&plug);
+ 	for (i = 0, pte = swap_ra->ptes; i < swap_ra->nr_pte;
+ 	     i++, pte++) {
+ 		pentry = *pte;
+ 		if (pte_none(pentry))
+ 			continue;
+ 		if (pte_present(pentry))
+ 			continue;
+ 		entry = pte_to_swp_entry(pentry);
+ 		if (unlikely(non_swap_entry(entry)))
+ 			continue;
+ 		page = __read_swap_cache_async(entry, gfp_mask, vma,
+ 					       vmf->address, &page_allocated);
+ 		if (!page)
+ 			continue;
+ 		if (page_allocated) {
+ 			swap_readpage(page, false);
+ 			if (i != swap_ra->offset &&
+ 			    likely(!PageTransCompound(page))) {
+ 				SetPageReadahead(page);
+ 				count_vm_event(SWAP_RA);
+ 			}
+ 		}
+ 		put_page(page);
+ 	}
+ 	blk_finish_plug(&plug);
+ 	lru_add_drain();
+ skip:
+ 	return read_swap_cache_async(fentry, gfp_mask, vma, vmf->address,
+ 				     swap_ra->win == 1);
+ }
+ 
+ #ifdef CONFIG_SYSFS
+ static ssize_t vma_ra_enabled_show(struct kobject *kobj,
+ 				     struct kobj_attribute *attr, char *buf)
+ {
+ 	return sprintf(buf, "%s\n", swap_vma_readahead ? "true" : "false");
+ }
+ static ssize_t vma_ra_enabled_store(struct kobject *kobj,
+ 				      struct kobj_attribute *attr,
+ 				      const char *buf, size_t count)
+ {
+ 	if (!strncmp(buf, "true", 4) || !strncmp(buf, "1", 1))
+ 		swap_vma_readahead = true;
+ 	else if (!strncmp(buf, "false", 5) || !strncmp(buf, "0", 1))
+ 		swap_vma_readahead = false;
+ 	else
+ 		return -EINVAL;
+ 
+ 	return count;
+ }
+ static struct kobj_attribute vma_ra_enabled_attr =
+ 	__ATTR(vma_ra_enabled, 0644, vma_ra_enabled_show,
+ 	       vma_ra_enabled_store);
+ 
+ static struct attribute *swap_attrs[] = {
+ 	&vma_ra_enabled_attr.attr,
+ 	NULL,
+ };
+ 
+ static struct attribute_group swap_attr_group = {
+ 	.attrs = swap_attrs,
+ };
+ 
+ static int __init swap_init_sysfs(void)
+ {
+ 	int err;
+ 	struct kobject *swap_kobj;
+ 
+ 	swap_kobj = kobject_create_and_add("swap", mm_kobj);
+ 	if (!swap_kobj) {
+ 		pr_err("failed to create swap kobject\n");
+ 		return -ENOMEM;
+ 	}
+ 	err = sysfs_create_group(swap_kobj, &swap_attr_group);
+ 	if (err) {
+ 		pr_err("failed to register swap group\n");
+ 		goto delete_obj;
+ 	}
+ 	return 0;
+ 
+ delete_obj:
+ 	kobject_put(swap_kobj);
+ 	return err;
+ }
+ subsys_initcall(swap_init_sysfs);
+ #endif
++>>>>>>> 61b639723be5 (mm, swap: use page-cluster as max window of VMA based swap readahead)
* Unmerged path Documentation/ABI/testing/sysfs-kernel-mm-swap
* Unmerged path Documentation/ABI/testing/sysfs-kernel-mm-swap
* Unmerged path mm/swap_state.c
