locking/rwsem: Prevent decrement of reader count before increment

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Waiman Long <longman@redhat.com>
commit a9e9bcb45b1525ba7aea26ed9441e8632aeeda58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/a9e9bcb4.failed

During my rwsem testing, it was found that after a down_read(), the
reader count may occasionally become 0 or even negative. Consequently,
a writer may steal the lock at that time and execute with the reader
in parallel thus breaking the mutual exclusion guarantee of the write
lock. In other words, both readers and writer can become rwsem owners
simultaneously.

The current reader wakeup code does it in one pass to clear waiter->task
and put them into wake_q before fully incrementing the reader count.
Once waiter->task is cleared, the corresponding reader may see it,
finish the critical section and do unlock to decrement the count before
the count is incremented. This is not a problem if there is only one
reader to wake up as the count has been pre-incremented by 1.  It is
a problem if there are more than one readers to be woken up and writer
can steal the lock.

The wakeup was actually done in 2 passes before the following v4.9 commit:

  70800c3c0cc5 ("locking/rwsem: Scan the wait_list for readers only once")

To fix this problem, the wakeup is now done in two passes
again. In the first pass, we collect the readers and count them.
The reader count is then fully incremented. In the second pass, the
waiter->task is then cleared and they are put into wake_q to be woken
up later.

	Signed-off-by: Waiman Long <longman@redhat.com>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: huang ying <huang.ying.caritas@gmail.com>
Fixes: 70800c3c0cc5 ("locking/rwsem: Scan the wait_list for readers only once")
Link: http://lkml.kernel.org/r/20190428212557.13482-2-longman@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a9e9bcb45b1525ba7aea26ed9441e8632aeeda58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rwsem.c
diff --cc lib/rwsem.c
index dcbfa1eac830,0b1f77957240..000000000000
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@@ -125,8 -128,9 +125,9 @@@ static void __rwsem_mark_wake(struct rw
  			      enum rwsem_wake_type wake_type,
  			      struct wake_q_head *wake_q)
  {
 -	struct rwsem_waiter *waiter, *tmp;
 +	struct rwsem_waiter *waiter;
  	long oldcount, woken = 0, adjustment = 0;
+ 	struct list_head wlist;
  
  	/*
  	 * Take a peek at the queue head waiter such that we can determine
@@@ -184,88 -189,57 +185,127 @@@
  	 * of the queue. We know that woken will be at least 1 as we accounted
  	 * for above. Note we increment the 'active part' of the count by the
  	 * number of readers before waking any processes up.
+ 	 *
+ 	 * We have to do wakeup in 2 passes to prevent the possibility that
+ 	 * the reader count may be decremented before it is incremented. It
+ 	 * is because the to-be-woken waiter may not have slept yet. So it
+ 	 * may see waiter->task got cleared, finish its critical section and
+ 	 * do an unlock before the reader count increment.
+ 	 *
+ 	 * 1) Collect the read-waiters in a separate list, count them and
+ 	 *    fully increment the reader count in rwsem.
+ 	 * 2) For each waiters in the new list, clear waiter->task and
+ 	 *    put them into wake_q to be woken up later.
  	 */
++<<<<<<< HEAD:lib/rwsem.c
 +	while (!slist_empty(&sem->wait_list)) {
 +		struct task_struct *tsk;
 +
 +		waiter = list_entry(sem->wait_list.next, typeof(*waiter), list);
 +
++=======
+ 	list_for_each_entry(waiter, &sem->wait_list, list) {
++>>>>>>> a9e9bcb45b15 (locking/rwsem: Prevent decrement of reader count before increment):kernel/locking/rwsem-xadd.c
  		if (waiter->type == RWSEM_WAITING_FOR_WRITE)
  			break;
  
  		woken++;
- 		tsk = waiter->task;
+ 	}
+ 	list_cut_before(&wlist, &sem->wait_list, &waiter->list);
  
++<<<<<<< HEAD:lib/rwsem.c
 +		wake_q_add(wake_q, tsk);
 +		slist_del(&waiter->list, &sem->wait_list);
++=======
+ 	adjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;
+ 	lockevent_cond_inc(rwsem_wake_reader, woken);
+ 	if (list_empty(&sem->wait_list)) {
+ 		/* hit end of list above */
+ 		adjustment -= RWSEM_WAITING_BIAS;
+ 	}
+ 
+ 	if (adjustment)
+ 		atomic_long_add(adjustment, &sem->count);
+ 
+ 	/* 2nd pass */
+ 	list_for_each_entry_safe(waiter, tmp, &wlist, list) {
+ 		struct task_struct *tsk;
+ 
+ 		tsk = waiter->task;
+ 		get_task_struct(tsk);
+ 
++>>>>>>> a9e9bcb45b15 (locking/rwsem: Prevent decrement of reader count before increment):kernel/locking/rwsem-xadd.c
  		/*
 -		 * Ensure calling get_task_struct() before setting the reader
 +		 * Ensure that the last operation is setting the reader
  		 * waiter to nil such that rwsem_down_read_failed() cannot
  		 * race with do_exit() by always holding a reference count
  		 * to the task to wakeup.
  		 */
  		smp_store_release(&waiter->task, NULL);
 -		/*
 -		 * Ensure issuing the wakeup (either by us or someone else)
 -		 * after setting the reader waiter to nil.
 -		 */
 -		wake_q_add_safe(wake_q, tsk);
  	}
++<<<<<<< HEAD:lib/rwsem.c
 +
 +	adjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;
 +	if (slist_empty(&sem->wait_list)) {
 +		/* hit end of list above */
 +		adjustment -= RWSEM_WAITING_BIAS;
 +	}
 +
 +	if (adjustment)
 +		atomic_long_add(adjustment, &sem->count);
++=======
++>>>>>>> a9e9bcb45b15 (locking/rwsem: Prevent decrement of reader count before increment):kernel/locking/rwsem-xadd.c
 +}
 +
 +/*
 + * Wait for the read lock to be granted
 + */
 +__visible
 +struct rw_semaphore __sched *rwsem_down_read_failed(struct rw_semaphore *sem)
 +{
 +	long count, adjustment = -RWSEM_ACTIVE_READ_BIAS;
 +	struct rwsem_waiter waiter;
 +	struct task_struct *tsk = current;
 +	WAKE_Q(wake_q);
 +
 +	/* set up my own style of waitqueue */
 +	waiter.task = tsk;
 +	waiter.type = RWSEM_WAITING_FOR_READ;
 +
 +	raw_spin_lock_irq(&sem->wait_lock);
 +	if (slist_empty(&sem->wait_list))
 +		adjustment += RWSEM_WAITING_BIAS;
 +	slist_add_tail(&waiter.list, &sem->wait_list);
 +
 +	/* we're now waiting on the lock, but no longer actively locking */
 +	count = atomic_long_add_return(adjustment, &sem->count);
 +
 +	/*
 +	 * If there are no active locks, wake the front queued process(es).
 +	 *
 +	 * If there are no writers and we are first in the queue,
 +	 * wake our own waiter to join the existing active readers !
 +	 */
 +	if (count == RWSEM_WAITING_BIAS ||
 +	    (count > RWSEM_WAITING_BIAS &&
 +	     adjustment != -RWSEM_ACTIVE_READ_BIAS))
 +		__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);
 +
 +	raw_spin_unlock_irq(&sem->wait_lock);
 +	wake_up_q(&wake_q);
 +
 +	/* wait to be given the lock */
 +	while (true) {
 +		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 +		if (!waiter.task)
 +			break;
 +		schedule();
 +	}
 +
 +	__set_task_state(tsk, TASK_RUNNING);
 +	return sem;
  }
 +EXPORT_SYMBOL(rwsem_down_read_failed);
  
  /*
   * This function must be called with the sem->wait_lock held to prevent
* Unmerged path lib/rwsem.c
