mm: update mmu_gather range correctly

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
commit b5bc66b713108710e341bb164f8ffbc11896706e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/b5bc66b7.failed

We use __tlb_adjust_range to update range convered by mmu_gather struct.
We later use the 'start' and 'end' to do a mmu_notifier_invalidate_range
in tlb_flush_mmu_tlbonly().  Update the 'end' correctly in
__tlb_adjust_range so that we call mmu_notifier_invalidate_range with
the correct range values.

Wrt tlbflush, this should not have any impact, because a flush with
correct start address will flush tlb mapping for the range.

Also add comment w.r.t updating the range when we free pagetable pages.
For now we don't support a range based page table cache flush.

Link: http://lkml.kernel.org/r/20161026084839.27299-3-aneesh.kumar@linux.vnet.ibm.com
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b5bc66b713108710e341bb164f8ffbc11896706e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/asm-generic/tlb.h
diff --cc include/asm-generic/tlb.h
index c3b25d4ce7d7,dba727becd5f..000000000000
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@@ -116,7 -121,48 +116,52 @@@ void tlb_gather_mmu(struct mmu_gather *
  void tlb_flush_mmu(struct mmu_gather *tlb);
  void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start,
  							unsigned long end);
++<<<<<<< HEAD
 +int __tlb_remove_page(struct mmu_gather *tlb, struct page *page);
++=======
+ extern bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,
+ 				   int page_size);
+ 
+ static inline void __tlb_adjust_range(struct mmu_gather *tlb,
+ 				      unsigned long address,
+ 				      unsigned int range_size)
+ {
+ 	tlb->start = min(tlb->start, address);
+ 	tlb->end = max(tlb->end, address + range_size);
+ 	/*
+ 	 * Track the last address with which we adjusted the range. This
+ 	 * will be used later to adjust again after a mmu_flush due to
+ 	 * failed __tlb_remove_page
+ 	 */
+ 	tlb->addr = address;
+ }
+ 
+ static inline void __tlb_reset_range(struct mmu_gather *tlb)
+ {
+ 	if (tlb->fullmm) {
+ 		tlb->start = tlb->end = ~0;
+ 	} else {
+ 		tlb->start = TASK_SIZE;
+ 		tlb->end = 0;
+ 	}
+ }
+ 
+ static inline void tlb_remove_page_size(struct mmu_gather *tlb,
+ 					struct page *page, int page_size)
+ {
+ 	if (__tlb_remove_page_size(tlb, page, page_size)) {
+ 		tlb_flush_mmu(tlb);
+ 		tlb->page_size = page_size;
+ 		__tlb_adjust_range(tlb, tlb->addr, page_size);
+ 		__tlb_remove_page_size(tlb, page, page_size);
+ 	}
+ }
+ 
+ static bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
+ {
+ 	return __tlb_remove_page_size(tlb, page, PAGE_SIZE);
+ }
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  
  /* tlb_remove_page
   *	Similar to __tlb_remove_page but will call tlb_flush_mmu() itself when
@@@ -124,10 -170,43 +169,47 @@@
   */
  static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
  {
 -	return tlb_remove_page_size(tlb, page, PAGE_SIZE);
 +	if (!__tlb_remove_page(tlb, page))
 +		tlb_flush_mmu(tlb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool __tlb_remove_pte_page(struct mmu_gather *tlb, struct page *page)
+ {
+ 	/* active->nr should be zero when we call this */
+ 	VM_BUG_ON_PAGE(tlb->active->nr, page);
+ 	tlb->page_size = PAGE_SIZE;
+ 	__tlb_adjust_range(tlb, tlb->addr, PAGE_SIZE);
+ 	return __tlb_remove_page(tlb, page);
+ }
+ 
+ /*
+  * In the case of tlb vma handling, we can optimise these away in the
+  * case where we're doing a full MM flush.  When we're doing a munmap,
+  * the vmas are adjusted to only cover the region to be torn down.
+  */
+ #ifndef tlb_start_vma
+ #define tlb_start_vma(tlb, vma) do { } while (0)
+ #endif
+ 
+ #define __tlb_end_vma(tlb, vma)					\
+ 	do {							\
+ 		if (!tlb->fullmm && tlb->end) {			\
+ 			tlb_flush(tlb);				\
+ 			__tlb_reset_range(tlb);			\
+ 		}						\
+ 	} while (0)
+ 
+ #ifndef tlb_end_vma
+ #define tlb_end_vma	__tlb_end_vma
+ #endif
+ 
+ #ifndef __tlb_remove_tlb_entry
+ #define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)
+ #endif
+ 
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  /**
   * tlb_remove_tlb_entry - remember a pte unmapping for later tlb invalidation.
   *
@@@ -137,7 -216,7 +219,11 @@@
   */
  #define tlb_remove_tlb_entry(tlb, ptep, address)		\
  	do {							\
++<<<<<<< HEAD
 +		tlb->need_flush = 1;				\
++=======
+ 		__tlb_adjust_range(tlb, address, PAGE_SIZE);	\
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  		__tlb_remove_tlb_entry(tlb, ptep, address);	\
  	} while (0)
  
@@@ -149,43 -228,47 +235,81 @@@
  #define __tlb_remove_pmd_tlb_entry(tlb, pmdp, address) do {} while (0)
  #endif
  
++<<<<<<< HEAD
 +#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)		\
 +	do {							\
 +		tlb->need_flush = 1;				\
 +		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);	\
 +	} while (0)
 +
 +/**
 + * tlb_remove_pud_tlb_entry - remember a pud mapping for later tlb
 + * invalidation. This is a nop so far, because only x86 needs it.
 + */
 +#ifndef __tlb_remove_pud_tlb_entry
 +#define __tlb_remove_pud_tlb_entry(tlb, pudp, address) do {} while (0)
 +#endif
 +
 +#define tlb_remove_pud_tlb_entry(tlb, pudp, address)			\
 +	do {								\
 +		tlb->need_flush = 1;					\
 +		__tlb_remove_pud_tlb_entry(tlb, pudp, address);		\
 +	} while (0)
 +
 +#define pte_free_tlb(tlb, ptep, address)			\
 +	do {							\
 +		tlb->need_flush = 1;				\
++=======
+ #define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)			\
+ 	do {								\
+ 		__tlb_adjust_range(tlb, address, HPAGE_PMD_SIZE);	\
+ 		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);		\
+ 	} while (0)
+ 
+ /*
+  * For things like page tables caches (ie caching addresses "inside" the
+  * page tables, like x86 does), for legacy reasons, flushing an
+  * individual page had better flush the page table caches behind it. This
+  * is definitely how x86 works, for example. And if you have an
+  * architected non-legacy page table cache (which I'm not aware of
+  * anybody actually doing), you're going to have some architecturally
+  * explicit flushing for that, likely *separate* from a regular TLB entry
+  * flush, and thus you'd need more than just some range expansion..
+  *
+  * So if we ever find an architecture
+  * that would want something that odd, I think it is up to that
+  * architecture to do its own odd thing, not cause pain for others
+  * http://lkml.kernel.org/r/CA+55aFzBggoXtNXQeng5d_mRoDnaMBE5Y+URs+PHR67nUpMtaw@mail.gmail.com
+  *
+  * For now w.r.t page table cache, mark the range_size as PAGE_SIZE
+  */
+ 
+ #define pte_free_tlb(tlb, ptep, address)			\
+ 	do {							\
+ 		__tlb_adjust_range(tlb, address, PAGE_SIZE);	\
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  		__pte_free_tlb(tlb, ptep, address);		\
  	} while (0)
  
  #ifndef __ARCH_HAS_4LEVEL_HACK
  #define pud_free_tlb(tlb, pudp, address)			\
  	do {							\
++<<<<<<< HEAD
 +		tlb->need_flush = 1;				\
++=======
+ 		__tlb_adjust_range(tlb, address, PAGE_SIZE);	\
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  		__pud_free_tlb(tlb, pudp, address);		\
  	} while (0)
  #endif
  
  #define pmd_free_tlb(tlb, pmdp, address)			\
  	do {							\
++<<<<<<< HEAD
 +		tlb->need_flush = 1;				\
++=======
+ 		__tlb_adjust_range(tlb, address, PAGE_SIZE);	\
++>>>>>>> b5bc66b71310 (mm: update mmu_gather range correctly)
  		__pmd_free_tlb(tlb, pmdp, address);		\
  	} while (0)
  
* Unmerged path include/asm-generic/tlb.h
