perf/x86/intel: Export mem events only if there's PEBS support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jiri Olsa <jolsa@redhat.com>
commit d4ae552982de39417d17f823df1f06b1cbc3686c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d4ae5529.failed

Memory events depends on PEBS support and access to LDLAT MSR, but we
display them in /sys/devices/cpu/events even if the CPU does not
provide those, like for KVM guests.

That brings the false assumption that those events should be
available, while they fail event to open.

Separating the mem-* events attributes and merging them with
cpu_events only if there's PEBS support detected.

We could also check if LDLAT MSR is available, but the PEBS check
seems to cover the need now.

	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
Link: http://lkml.kernel.org/r/20180906135748.GC9577@krava
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d4ae552982de39417d17f823df1f06b1cbc3686c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/intel/core.c
diff --cc arch/x86/events/intel/core.c
index 65e18eccc087,0fb8659b20d8..000000000000
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@@ -248,7 -247,48 +248,23 @@@ static struct attribute *nhm_mem_events
  	NULL,
  };
  
 -/*
 - * topdown events for Intel Core CPUs.
 - *
 - * The events are all in slots, which is a free slot in a 4 wide
 - * pipeline. Some events are already reported in slots, for cycle
 - * events we multiply by the pipeline width (4).
 - *
 - * With Hyper Threading on, topdown metrics are either summed or averaged
 - * between the threads of a core: (count_t0 + count_t1).
 - *
 - * For the average case the metric is always scaled to pipeline width,
 - * so we use factor 2 ((count_t0 + count_t1) / 2 * 4)
 - */
 -
 -EVENT_ATTR_STR_HT(topdown-total-slots, td_total_slots,
 -	"event=0x3c,umask=0x0",			/* cpu_clk_unhalted.thread */
 -	"event=0x3c,umask=0x0,any=1");		/* cpu_clk_unhalted.thread_any */
 -EVENT_ATTR_STR_HT(topdown-total-slots.scale, td_total_slots_scale, "4", "2");
 -EVENT_ATTR_STR(topdown-slots-issued, td_slots_issued,
 -	"event=0xe,umask=0x1");			/* uops_issued.any */
 -EVENT_ATTR_STR(topdown-slots-retired, td_slots_retired,
 -	"event=0xc2,umask=0x2");		/* uops_retired.retire_slots */
 -EVENT_ATTR_STR(topdown-fetch-bubbles, td_fetch_bubbles,
 -	"event=0x9c,umask=0x1");		/* idq_uops_not_delivered_core */
 -EVENT_ATTR_STR_HT(topdown-recovery-bubbles, td_recovery_bubbles,
 -	"event=0xd,umask=0x3,cmask=1",		/* int_misc.recovery_cycles */
 -	"event=0xd,umask=0x3,cmask=1,any=1");	/* int_misc.recovery_cycles_any */
 -EVENT_ATTR_STR_HT(topdown-recovery-bubbles.scale, td_recovery_bubbles_scale,
 -	"4", "2");
 -
  static struct attribute *snb_events_attrs[] = {
++<<<<<<< HEAD
++	EVENT_PTR(mem_ld_snb),
++	EVENT_PTR(mem_st_snb),
++=======
+ 	EVENT_PTR(td_slots_issued),
+ 	EVENT_PTR(td_slots_retired),
+ 	EVENT_PTR(td_fetch_bubbles),
+ 	EVENT_PTR(td_total_slots),
+ 	EVENT_PTR(td_total_slots_scale),
+ 	EVENT_PTR(td_recovery_bubbles),
+ 	EVENT_PTR(td_recovery_bubbles_scale),
++>>>>>>> d4ae552982de (perf/x86/intel: Export mem events only if there's PEBS support)
+ 	NULL,
+ };
+ 
+ static struct attribute *snb_mem_events_attrs[] = {
  	EVENT_PTR(mem_ld_snb),
  	EVENT_PTR(mem_st_snb),
  	NULL,
@@@ -3692,8 -3916,13 +3708,18 @@@ EVENT_ATTR_STR(cycles-t,	cycles_t,	"eve
  EVENT_ATTR_STR(cycles-ct,	cycles_ct,	"event=0x3c,in_tx=1,in_tx_cp=1");
  
  static struct attribute *hsw_events_attrs[] = {
++<<<<<<< HEAD
 +	EVENT_PTR(mem_ld_hsw),
 +	EVENT_PTR(mem_st_hsw),
++=======
+ 	EVENT_PTR(td_slots_issued),
+ 	EVENT_PTR(td_slots_retired),
+ 	EVENT_PTR(td_fetch_bubbles),
+ 	EVENT_PTR(td_total_slots),
+ 	EVENT_PTR(td_total_slots_scale),
+ 	EVENT_PTR(td_recovery_bubbles),
+ 	EVENT_PTR(td_recovery_bubbles_scale),
++>>>>>>> d4ae552982de (perf/x86/intel: Export mem events only if there's PEBS support)
  	NULL
  };
  
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 063beaee49b9..381679214962 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1617,9 +1617,9 @@ __init struct attribute **merge_attr(struct attribute **a, struct attribute **b)
 	struct attribute **new;
 	int j, i;
 
-	for (j = 0; a[j]; j++)
+	for (j = 0; a && a[j]; j++)
 		;
-	for (i = 0; b[i]; i++)
+	for (i = 0; b && b[i]; i++)
 		j++;
 	j++;
 
@@ -1628,9 +1628,9 @@ __init struct attribute **merge_attr(struct attribute **a, struct attribute **b)
 		return NULL;
 
 	j = 0;
-	for (i = 0; a[i]; i++)
+	for (i = 0; a && a[i]; i++)
 		new[j++] = a[i];
-	for (i = 0; b[i]; i++)
+	for (i = 0; b && b[i]; i++)
 		new[j++] = b[i];
 	new[j] = NULL;
 
* Unmerged path arch/x86/events/intel/core.c
