PCI: Remove pci_unmap_addr() wrappers for DMA API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 18b01b16e8bae9cd227909f6e6d2783d74855f65
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/18b01b16.failed

Only some of these were still used by the cxgb4 driver, and that despite
the fact that the driver otherwise uses the generic DMA API.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
(cherry picked from commit 18b01b16e8bae9cd227909f6e6d2783d74855f65)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/cxgb4/qp.c
#	include/linux/pci-dma.h
diff --cc drivers/infiniband/hw/cxgb4/qp.c
index 4106eed1b8fb,62d6f197ec0b..000000000000
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@@ -2104,3 -2489,359 +2104,362 @@@ int c4iw_ib_query_qp(struct ib_qp *ibqp
  	init_attr->sq_sig_type = qhp->sq_sig_all ? IB_SIGNAL_ALL_WR : 0;
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void free_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,
+ 			   struct c4iw_wr_wait *wr_waitp)
+ {
+ 	struct c4iw_rdev *rdev = &srq->rhp->rdev;
+ 	struct sk_buff *skb = srq->destroy_skb;
+ 	struct t4_srq *wq = &srq->wq;
+ 	struct fw_ri_res_wr *res_wr;
+ 	struct fw_ri_res *res;
+ 	int wr_len;
+ 
+ 	wr_len = sizeof(*res_wr) + sizeof(*res);
+ 	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+ 
+ 	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
+ 	memset(res_wr, 0, wr_len);
+ 	res_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |
+ 			FW_RI_RES_WR_NRES_V(1) |
+ 			FW_WR_COMPL_F);
+ 	res_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));
+ 	res_wr->cookie = (uintptr_t)wr_waitp;
+ 	res = res_wr->res;
+ 	res->u.srq.restype = FW_RI_RES_TYPE_SRQ;
+ 	res->u.srq.op = FW_RI_RES_OP_RESET;
+ 	res->u.srq.srqid = cpu_to_be32(srq->idx);
+ 	res->u.srq.eqid = cpu_to_be32(wq->qid);
+ 
+ 	c4iw_init_wr_wait(wr_waitp);
+ 	c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);
+ 
+ 	dma_free_coherent(&rdev->lldi.pdev->dev,
+ 			  wq->memsize, wq->queue,
+ 			dma_unmap_addr(wq, mapping));
+ 	c4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);
+ 	kfree(wq->sw_rq);
+ 	c4iw_put_qpid(rdev, wq->qid, uctx);
+ }
+ 
+ static int alloc_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,
+ 			   struct c4iw_wr_wait *wr_waitp)
+ {
+ 	struct c4iw_rdev *rdev = &srq->rhp->rdev;
+ 	int user = (uctx != &rdev->uctx);
+ 	struct t4_srq *wq = &srq->wq;
+ 	struct fw_ri_res_wr *res_wr;
+ 	struct fw_ri_res *res;
+ 	struct sk_buff *skb;
+ 	int wr_len;
+ 	int eqsize;
+ 	int ret = -ENOMEM;
+ 
+ 	wq->qid = c4iw_get_qpid(rdev, uctx);
+ 	if (!wq->qid)
+ 		goto err;
+ 
+ 	if (!user) {
+ 		wq->sw_rq = kcalloc(wq->size, sizeof(*wq->sw_rq),
+ 				    GFP_KERNEL);
+ 		if (!wq->sw_rq)
+ 			goto err_put_qpid;
+ 		wq->pending_wrs = kcalloc(srq->wq.size,
+ 					  sizeof(*srq->wq.pending_wrs),
+ 					  GFP_KERNEL);
+ 		if (!wq->pending_wrs)
+ 			goto err_free_sw_rq;
+ 	}
+ 
+ 	wq->rqt_size = wq->size;
+ 	wq->rqt_hwaddr = c4iw_rqtpool_alloc(rdev, wq->rqt_size);
+ 	if (!wq->rqt_hwaddr)
+ 		goto err_free_pending_wrs;
+ 	wq->rqt_abs_idx = (wq->rqt_hwaddr - rdev->lldi.vr->rq.start) >>
+ 		T4_RQT_ENTRY_SHIFT;
+ 
+ 	wq->queue = dma_alloc_coherent(&rdev->lldi.pdev->dev,
+ 				       wq->memsize, &wq->dma_addr,
+ 			GFP_KERNEL);
+ 	if (!wq->queue)
+ 		goto err_free_rqtpool;
+ 
+ 	memset(wq->queue, 0, wq->memsize);
+ 	dma_unmap_addr_set(wq, mapping, wq->dma_addr);
+ 
+ 	wq->bar2_va = c4iw_bar2_addrs(rdev, wq->qid, T4_BAR2_QTYPE_EGRESS,
+ 				      &wq->bar2_qid,
+ 			user ? &wq->bar2_pa : NULL);
+ 
+ 	/*
+ 	 * User mode must have bar2 access.
+ 	 */
+ 
+ 	if (user && !wq->bar2_va) {
+ 		pr_warn(MOD "%s: srqid %u not in BAR2 range.\n",
+ 			pci_name(rdev->lldi.pdev), wq->qid);
+ 		ret = -EINVAL;
+ 		goto err_free_queue;
+ 	}
+ 
+ 	/* build fw_ri_res_wr */
+ 	wr_len = sizeof(*res_wr) + sizeof(*res);
+ 
+ 	skb = alloc_skb(wr_len, GFP_KERNEL | __GFP_NOFAIL);
+ 	if (!skb)
+ 		goto err_free_queue;
+ 	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+ 
+ 	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
+ 	memset(res_wr, 0, wr_len);
+ 	res_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |
+ 			FW_RI_RES_WR_NRES_V(1) |
+ 			FW_WR_COMPL_F);
+ 	res_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));
+ 	res_wr->cookie = (uintptr_t)wr_waitp;
+ 	res = res_wr->res;
+ 	res->u.srq.restype = FW_RI_RES_TYPE_SRQ;
+ 	res->u.srq.op = FW_RI_RES_OP_WRITE;
+ 
+ 	/*
+ 	 * eqsize is the number of 64B entries plus the status page size.
+ 	 */
+ 	eqsize = wq->size * T4_RQ_NUM_SLOTS +
+ 		rdev->hw_queue.t4_eq_status_entries;
+ 	res->u.srq.eqid = cpu_to_be32(wq->qid);
+ 	res->u.srq.fetchszm_to_iqid =
+ 						/* no host cidx updates */
+ 		cpu_to_be32(FW_RI_RES_WR_HOSTFCMODE_V(0) |
+ 		FW_RI_RES_WR_CPRIO_V(0) |       /* don't keep in chip cache */
+ 		FW_RI_RES_WR_PCIECHN_V(0) |     /* set by uP at ri_init time */
+ 		FW_RI_RES_WR_FETCHRO_V(0));     /* relaxed_ordering */
+ 	res->u.srq.dcaen_to_eqsize =
+ 		cpu_to_be32(FW_RI_RES_WR_DCAEN_V(0) |
+ 		FW_RI_RES_WR_DCACPU_V(0) |
+ 		FW_RI_RES_WR_FBMIN_V(2) |
+ 		FW_RI_RES_WR_FBMAX_V(3) |
+ 		FW_RI_RES_WR_CIDXFTHRESHO_V(0) |
+ 		FW_RI_RES_WR_CIDXFTHRESH_V(0) |
+ 		FW_RI_RES_WR_EQSIZE_V(eqsize));
+ 	res->u.srq.eqaddr = cpu_to_be64(wq->dma_addr);
+ 	res->u.srq.srqid = cpu_to_be32(srq->idx);
+ 	res->u.srq.pdid = cpu_to_be32(srq->pdid);
+ 	res->u.srq.hwsrqsize = cpu_to_be32(wq->rqt_size);
+ 	res->u.srq.hwsrqaddr = cpu_to_be32(wq->rqt_hwaddr -
+ 			rdev->lldi.vr->rq.start);
+ 
+ 	c4iw_init_wr_wait(wr_waitp);
+ 
+ 	ret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, wq->qid, __func__);
+ 	if (ret)
+ 		goto err_free_queue;
+ 
+ 	pr_debug("%s srq %u eqid %u pdid %u queue va %p pa 0x%llx\n"
+ 			" bar2_addr %p rqt addr 0x%x size %d\n",
+ 			__func__, srq->idx, wq->qid, srq->pdid, wq->queue,
+ 			(u64)virt_to_phys(wq->queue), wq->bar2_va,
+ 			wq->rqt_hwaddr, wq->rqt_size);
+ 
+ 	return 0;
+ err_free_queue:
+ 	dma_free_coherent(&rdev->lldi.pdev->dev,
+ 			  wq->memsize, wq->queue,
+ 			dma_unmap_addr(wq, mapping));
+ err_free_rqtpool:
+ 	c4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);
+ err_free_pending_wrs:
+ 	if (!user)
+ 		kfree(wq->pending_wrs);
+ err_free_sw_rq:
+ 	if (!user)
+ 		kfree(wq->sw_rq);
+ err_put_qpid:
+ 	c4iw_put_qpid(rdev, wq->qid, uctx);
+ err:
+ 	return ret;
+ }
+ 
+ void c4iw_copy_wr_to_srq(struct t4_srq *srq, union t4_recv_wr *wqe, u8 len16)
+ {
+ 	u64 *src, *dst;
+ 
+ 	src = (u64 *)wqe;
+ 	dst = (u64 *)((u8 *)srq->queue + srq->wq_pidx * T4_EQ_ENTRY_SIZE);
+ 	while (len16) {
+ 		*dst++ = *src++;
+ 		if (dst >= (u64 *)&srq->queue[srq->size])
+ 			dst = (u64 *)srq->queue;
+ 		*dst++ = *src++;
+ 		if (dst >= (u64 *)&srq->queue[srq->size])
+ 			dst = (u64 *)srq->queue;
+ 		len16--;
+ 	}
+ }
+ 
+ struct ib_srq *c4iw_create_srq(struct ib_pd *pd, struct ib_srq_init_attr *attrs,
+ 			       struct ib_udata *udata)
+ {
+ 	struct c4iw_dev *rhp;
+ 	struct c4iw_srq *srq;
+ 	struct c4iw_pd *php;
+ 	struct c4iw_create_srq_resp uresp;
+ 	struct c4iw_ucontext *ucontext;
+ 	struct c4iw_mm_entry *srq_key_mm, *srq_db_key_mm;
+ 	int rqsize;
+ 	int ret;
+ 	int wr_len;
+ 
+ 	pr_debug("%s ib_pd %p\n", __func__, pd);
+ 
+ 	php = to_c4iw_pd(pd);
+ 	rhp = php->rhp;
+ 
+ 	if (!rhp->rdev.lldi.vr->srq.size)
+ 		return ERR_PTR(-EINVAL);
+ 	if (attrs->attr.max_wr > rhp->rdev.hw_queue.t4_max_rq_size)
+ 		return ERR_PTR(-E2BIG);
+ 	if (attrs->attr.max_sge > T4_MAX_RECV_SGE)
+ 		return ERR_PTR(-E2BIG);
+ 
+ 	/*
+ 	 * SRQ RQT and RQ must be a power of 2 and at least 16 deep.
+ 	 */
+ 	rqsize = attrs->attr.max_wr + 1;
+ 	rqsize = roundup_pow_of_two(max_t(u16, rqsize, 16));
+ 
+ 	ucontext = pd->uobject ? to_c4iw_ucontext(pd->uobject->context) : NULL;
+ 
+ 	srq = kzalloc(sizeof(*srq), GFP_KERNEL);
+ 	if (!srq)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	srq->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);
+ 	if (!srq->wr_waitp) {
+ 		ret = -ENOMEM;
+ 		goto err_free_srq;
+ 	}
+ 
+ 	srq->idx = c4iw_alloc_srq_idx(&rhp->rdev);
+ 	if (srq->idx < 0) {
+ 		ret = -ENOMEM;
+ 		goto err_free_wr_wait;
+ 	}
+ 
+ 	wr_len = sizeof(struct fw_ri_res_wr) + sizeof(struct fw_ri_res);
+ 	srq->destroy_skb = alloc_skb(wr_len, GFP_KERNEL);
+ 	if (!srq->destroy_skb) {
+ 		ret = -ENOMEM;
+ 		goto err_free_srq_idx;
+ 	}
+ 
+ 	srq->rhp = rhp;
+ 	srq->pdid = php->pdid;
+ 
+ 	srq->wq.size = rqsize;
+ 	srq->wq.memsize =
+ 		(rqsize + rhp->rdev.hw_queue.t4_eq_status_entries) *
+ 		sizeof(*srq->wq.queue);
+ 	if (ucontext)
+ 		srq->wq.memsize = roundup(srq->wq.memsize, PAGE_SIZE);
+ 
+ 	ret = alloc_srq_queue(srq, ucontext ? &ucontext->uctx :
+ 			&rhp->rdev.uctx, srq->wr_waitp);
+ 	if (ret)
+ 		goto err_free_skb;
+ 	attrs->attr.max_wr = rqsize - 1;
+ 
+ 	if (CHELSIO_CHIP_VERSION(rhp->rdev.lldi.adapter_type) > CHELSIO_T6)
+ 		srq->flags = T4_SRQ_LIMIT_SUPPORT;
+ 
+ 	ret = insert_handle(rhp, &rhp->qpidr, srq, srq->wq.qid);
+ 	if (ret)
+ 		goto err_free_queue;
+ 
+ 	if (udata) {
+ 		srq_key_mm = kmalloc(sizeof(*srq_key_mm), GFP_KERNEL);
+ 		if (!srq_key_mm) {
+ 			ret = -ENOMEM;
+ 			goto err_remove_handle;
+ 		}
+ 		srq_db_key_mm = kmalloc(sizeof(*srq_db_key_mm), GFP_KERNEL);
+ 		if (!srq_db_key_mm) {
+ 			ret = -ENOMEM;
+ 			goto err_free_srq_key_mm;
+ 		}
+ 		memset(&uresp, 0, sizeof(uresp));
+ 		uresp.flags = srq->flags;
+ 		uresp.qid_mask = rhp->rdev.qpmask;
+ 		uresp.srqid = srq->wq.qid;
+ 		uresp.srq_size = srq->wq.size;
+ 		uresp.srq_memsize = srq->wq.memsize;
+ 		uresp.rqt_abs_idx = srq->wq.rqt_abs_idx;
+ 		spin_lock(&ucontext->mmap_lock);
+ 		uresp.srq_key = ucontext->key;
+ 		ucontext->key += PAGE_SIZE;
+ 		uresp.srq_db_gts_key = ucontext->key;
+ 		ucontext->key += PAGE_SIZE;
+ 		spin_unlock(&ucontext->mmap_lock);
+ 		ret = ib_copy_to_udata(udata, &uresp, sizeof(uresp));
+ 		if (ret)
+ 			goto err_free_srq_db_key_mm;
+ 		srq_key_mm->key = uresp.srq_key;
+ 		srq_key_mm->addr = virt_to_phys(srq->wq.queue);
+ 		srq_key_mm->len = PAGE_ALIGN(srq->wq.memsize);
+ 		insert_mmap(ucontext, srq_key_mm);
+ 		srq_db_key_mm->key = uresp.srq_db_gts_key;
+ 		srq_db_key_mm->addr = (u64)(unsigned long)srq->wq.bar2_pa;
+ 		srq_db_key_mm->len = PAGE_SIZE;
+ 		insert_mmap(ucontext, srq_db_key_mm);
+ 	}
+ 
+ 	pr_debug("%s srq qid %u idx %u size %u memsize %lu num_entries %u\n",
+ 		 __func__, srq->wq.qid, srq->idx, srq->wq.size,
+ 			(unsigned long)srq->wq.memsize, attrs->attr.max_wr);
+ 
+ 	spin_lock_init(&srq->lock);
+ 	return &srq->ibsrq;
+ err_free_srq_db_key_mm:
+ 	kfree(srq_db_key_mm);
+ err_free_srq_key_mm:
+ 	kfree(srq_key_mm);
+ err_remove_handle:
+ 	remove_handle(rhp, &rhp->qpidr, srq->wq.qid);
+ err_free_queue:
+ 	free_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,
+ 		       srq->wr_waitp);
+ err_free_skb:
+ 	if (srq->destroy_skb)
+ 		kfree_skb(srq->destroy_skb);
+ err_free_srq_idx:
+ 	c4iw_free_srq_idx(&rhp->rdev, srq->idx);
+ err_free_wr_wait:
+ 	c4iw_put_wr_wait(srq->wr_waitp);
+ err_free_srq:
+ 	kfree(srq);
+ 	return ERR_PTR(ret);
+ }
+ 
+ int c4iw_destroy_srq(struct ib_srq *ibsrq)
+ {
+ 	struct c4iw_dev *rhp;
+ 	struct c4iw_srq *srq;
+ 	struct c4iw_ucontext *ucontext;
+ 
+ 	srq = to_c4iw_srq(ibsrq);
+ 	rhp = srq->rhp;
+ 
+ 	pr_debug("%s id %d\n", __func__, srq->wq.qid);
+ 
+ 	remove_handle(rhp, &rhp->qpidr, srq->wq.qid);
+ 	ucontext = ibsrq->uobject ?
+ 		to_c4iw_ucontext(ibsrq->uobject->context) : NULL;
+ 	free_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,
+ 		       srq->wr_waitp);
+ 	c4iw_free_srq_idx(&rhp->rdev, srq->idx);
+ 	c4iw_put_wr_wait(srq->wr_waitp);
+ 	kfree(srq);
+ 	return 0;
+ }
++>>>>>>> 18b01b16e8ba (PCI: Remove pci_unmap_addr() wrappers for DMA API)
* Unmerged path include/linux/pci-dma.h
* Unmerged path drivers/infiniband/hw/cxgb4/qp.c
diff --git a/drivers/infiniband/hw/cxgb4/t4.h b/drivers/infiniband/hw/cxgb4/t4.h
index 29a4dd5053f2..4969ae79fe09 100644
--- a/drivers/infiniband/hw/cxgb4/t4.h
+++ b/drivers/infiniband/hw/cxgb4/t4.h
@@ -379,7 +379,7 @@ struct t4_srq_pending_wr {
 struct t4_srq {
 	union t4_recv_wr *queue;
 	dma_addr_t dma_addr;
-	DECLARE_PCI_UNMAP_ADDR(mapping);
+	DEFINE_DMA_UNMAP_ADDR(mapping);
 	struct t4_swrqe *sw_rq;
 	void __iomem *bar2_va;
 	u64 bar2_pa;
* Unmerged path include/linux/pci-dma.h
diff --git a/include/linux/pci.h b/include/linux/pci.h
index c15ba17e49e7..707e990dc92c 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -1286,7 +1286,6 @@ int pci_set_vga_state(struct pci_dev *pdev, bool decode,
 
 /* kmem_cache style wrapper around pci_alloc_consistent() */
 
-#include <linux/pci-dma.h>
 #include <linux/dmapool.h>
 
 #define	pci_pool dma_pool
