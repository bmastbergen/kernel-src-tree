memcg, slab: cleanup memcg cache creation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Vladimir Davydov <vdavydov@parallels.com>
commit 5722d094ad2b56fa2c1cb3adaf40071a55bbf242
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5722d094.failed

This patch cleans up the memcg cache creation path as follows:

- Move memcg cache name creation to a separate function to be called
  from kmem_cache_create_memcg().  This allows us to get rid of the mutex
  protecting the temporary buffer used for the name formatting, because
  the whole cache creation path is protected by the slab_mutex.

- Get rid of memcg_create_kmem_cache().  This function serves as a proxy
  to kmem_cache_create_memcg().  After separating the cache name creation
  path, it would be reduced to a function call, so let's inline it.

	Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: Glauber Costa <glommer@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5722d094ad2b56fa2c1cb3adaf40071a55bbf242)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
#	mm/slab_common.c
diff --cc mm/memcontrol.c
index c2be36e1df50,32c7342df4bf..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -3375,80 -3321,15 +3398,84 @@@ void mem_cgroup_destroy_cache(struct km
  	schedule_work(&cachep->memcg_params->destroy);
  }
  
++<<<<<<< HEAD
 +/*
 + * This lock protects updaters, not readers. We want readers to be as fast as
 + * they can, and they will either see NULL or a valid cache value. Our model
 + * allow them to see NULL, in which case the root memcg will be selected.
 + *
 + * We need this lock because multiple allocations to the same cache from a non
 + * will span more than one worker. Only one of them can create the cache.
 + */
 +static DEFINE_MUTEX(memcg_cache_mutex);
 +
 +/*
 + * Called with memcg_cache_mutex held
 + */
 +static struct kmem_cache *kmem_cache_dup(struct mem_cgroup *memcg,
 +					 struct kmem_cache *s)
 +{
 +	struct kmem_cache *new;
 +	static char *tmp_name = NULL;
 +
 +	lockdep_assert_held(&memcg_cache_mutex);
 +
 +	/*
 +	 * kmem_cache_create_memcg duplicates the given name and
 +	 * cgroup_name for this name requires RCU context.
 +	 * This static temporary buffer is used to prevent from
 +	 * pointless shortliving allocation.
 +	 */
 +	if (!tmp_name) {
 +		tmp_name = kmalloc(PATH_MAX, GFP_KERNEL);
 +		if (!tmp_name)
 +			return NULL;
 +	}
 +
 +	rcu_read_lock();
 +	snprintf(tmp_name, PATH_MAX, "%s(%d:%s)", s->name,
 +			 memcg_cache_id(memcg), cgroup_name(memcg->css.cgroup));
 +	rcu_read_unlock();
 +
 +	new = kmem_cache_create_memcg(memcg, tmp_name, s->object_size, s->align,
 +				      (s->flags & ~SLAB_PANIC), s->ctor, s);
 +
 +	if (new)
 +		new->allocflags |= __GFP_KMEMCG;
 +
 +	return new;
 +}
 +
 +static struct kmem_cache *memcg_create_kmem_cache(struct mem_cgroup *memcg,
 +						  struct kmem_cache *cachep)
 +{
 +	struct kmem_cache *new_cachep;
 +
 +	BUG_ON(!memcg_can_account_kmem(memcg));
 +
 +	mutex_lock(&memcg_cache_mutex);
 +
 +	new_cachep = kmem_cache_dup(memcg, cachep);
 +	if (new_cachep == NULL) {
 +		new_cachep = cachep;
 +		goto out;
 +	}
 +
 +	mem_cgroup_get(memcg);
 +out:
 +	mutex_unlock(&memcg_cache_mutex);
 +	return new_cachep;
 +}
 +
 +static DEFINE_MUTEX(memcg_limit_mutex);
 +
 +int __kmem_cache_destroy_memcg_children(struct kmem_cache *s)
++=======
+ void kmem_cache_destroy_memcg_children(struct kmem_cache *s)
++>>>>>>> 5722d094ad2b (memcg, slab: cleanup memcg cache creation)
  {
  	struct kmem_cache *c;
 -	int i;
 -
 -	if (!s->memcg_params)
 -		return;
 -	if (!s->memcg_params->is_root_cache)
 -		return;
 +	int i, failed = 0;
  
  	/*
  	 * If the cache is being destroyed, we trust that there is no one else
@@@ -3481,20 -3363,10 +3508,14 @@@
  		c->memcg_params->dead = false;
  		cancel_work_sync(&c->memcg_params->destroy);
  		kmem_cache_destroy(c);
 +
 +		if (cache_from_memcg(s, i))
 +			failed++;
  	}
 -	mutex_unlock(&activate_kmem_mutex);
 +	mutex_unlock(&memcg_limit_mutex);
 +	return failed;
  }
  
- struct create_work {
- 	struct mem_cgroup *memcg;
- 	struct kmem_cache *cachep;
- 	struct work_struct work;
- };
- 
  static void mem_cgroup_destroy_all_caches(struct mem_cgroup *memcg)
  {
  	struct kmem_cache *cachep;
@@@ -3512,14 -3384,25 +3533,32 @@@
  	mutex_unlock(&memcg->slab_caches_mutex);
  }
  
+ struct create_work {
+ 	struct mem_cgroup *memcg;
+ 	struct kmem_cache *cachep;
+ 	struct work_struct work;
+ };
+ 
  static void memcg_create_cache_work_func(struct work_struct *w)
  {
- 	struct create_work *cw;
+ 	struct create_work *cw = container_of(w, struct create_work, work);
+ 	struct mem_cgroup *memcg = cw->memcg;
+ 	struct kmem_cache *cachep = cw->cachep;
+ 	struct kmem_cache *new;
  
++<<<<<<< HEAD
 +	cw = container_of(w, struct create_work, work);
 +	memcg_create_kmem_cache(cw->memcg, cw->cachep);
 +	/* Drop the reference gotten when we enqueued. */
 +	css_put(&cw->memcg->css);
++=======
+ 	new = kmem_cache_create_memcg(memcg, cachep->name,
+ 			cachep->object_size, cachep->align,
+ 			cachep->flags & ~SLAB_PANIC, cachep->ctor, cachep);
+ 	if (new)
+ 		new->allocflags |= __GFP_KMEMCG;
+ 	css_put(&memcg->css);
++>>>>>>> 5722d094ad2b (memcg, slab: cleanup memcg cache creation)
  	kfree(cw);
  }
  
diff --cc mm/slab_common.c
index 84ed8dc5b2d8,11857abf7057..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -243,43 -203,38 +243,52 @@@ kmem_cache_create_memcg(struct mem_cgro
  	if (!memcg) {
  		s = __kmem_cache_alias(name, size, align, flags, ctor);
  		if (s)
 -			goto out_unlock;
 +			goto out_locked;
  	}
  
 -	err = -ENOMEM;
  	s = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);
 -	if (!s)
 -		goto out_unlock;
 +	if (s) {
 +		s->object_size = s->size = size;
 +		s->align = calculate_alignment(flags, align, size);
 +		s->ctor = ctor;
  
 -	s->object_size = s->size = size;
 -	s->align = calculate_alignment(flags, align, size);
 -	s->ctor = ctor;
 +		err = memcg_alloc_cache_params(memcg, s, parent_cache);
 +		if (err) {
 +			kmem_cache_free(kmem_cache, s);
 +			goto out_locked;
 +		}
  
++<<<<<<< HEAD
 +		s->name = kstrdup(name, GFP_KERNEL);
 +		if (!s->name) {
 +			memcg_free_cache_params(s);
 +			kmem_cache_free(kmem_cache, s);
 +			err = -ENOMEM;
 +			goto out_locked;
 +		}
++=======
+ 	if (memcg)
+ 		s->name = memcg_create_cache_name(memcg, parent_cache);
+ 	else
+ 		s->name = kstrdup(name, GFP_KERNEL);
+ 	if (!s->name)
+ 		goto out_free_cache;
++>>>>>>> 5722d094ad2b (memcg, slab: cleanup memcg cache creation)
  
 -	err = memcg_alloc_cache_params(memcg, s, parent_cache);
 -	if (err)
 -		goto out_free_cache;
 -
 -	err = __kmem_cache_create(s, flags);
 -	if (err)
 -		goto out_free_cache;
 -
 -	s->refcount = 1;
 -	list_add(&s->list, &slab_caches);
 -	memcg_register_cache(s);
 +		err = __kmem_cache_create(s, flags);
 +		if (!err) {
 +			s->refcount = 1;
 +			list_add(&s->list, &slab_caches);
 +			memcg_register_cache(s);
 +		} else {
 +			memcg_free_cache_params(s);
 +			kfree(s->name);
 +			kmem_cache_free(kmem_cache, s);
 +		}
 +	} else
 +		err = -ENOMEM;
  
 -out_unlock:
 +out_locked:
  	mutex_unlock(&slab_mutex);
  	put_online_cpus();
  
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 5cb1a6d66642..6155c6c73cef 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -484,6 +484,9 @@ void __memcg_kmem_commit_charge(struct page *page,
 void __memcg_kmem_uncharge_pages(struct page *page, int order);
 
 int memcg_cache_id(struct mem_cgroup *memcg);
+
+char *memcg_create_cache_name(struct mem_cgroup *memcg,
+			      struct kmem_cache *root_cache);
 int memcg_alloc_cache_params(struct mem_cgroup *memcg, struct kmem_cache *s,
 			     struct kmem_cache *root_cache);
 void memcg_free_cache_params(struct kmem_cache *s);
@@ -627,6 +630,12 @@ static inline int memcg_cache_id(struct mem_cgroup *memcg)
 	return -1;
 }
 
+static inline char *memcg_create_cache_name(struct mem_cgroup *memcg,
+					    struct kmem_cache *root_cache)
+{
+	return NULL;
+}
+
 static inline int memcg_alloc_cache_params(struct mem_cgroup *memcg,
 		struct kmem_cache *s, struct kmem_cache *root_cache)
 {
* Unmerged path mm/memcontrol.c
* Unmerged path mm/slab_common.c
