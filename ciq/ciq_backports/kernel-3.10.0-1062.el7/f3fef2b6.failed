i40e: Remove umem from VSI

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jan Sokolowski <jan.sokolowski@intel.com>
commit f3fef2b6e1cc802fdf3e2dd543447a8ce15568a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f3fef2b6.failed

As current implementation of netdev already contains and provides
umems for us, we no longer have the need to contain these
structures in i40e_vsi.

Refactor the code to operate on netdev-provided umems.

	Signed-off-by: Jan Sokolowski <jan.sokolowski@intel.com>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit f3fef2b6e1cc802fdf3e2dd543447a8ce15568a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_xsk.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_xsk.c
index bd590f9fa93a,96d849460d9b..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_xsk.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
@@@ -140,6 -77,8 +77,11 @@@ static void i40e_xsk_umem_dma_unmap(str
  static int i40e_xsk_umem_enable(struct i40e_vsi *vsi, struct xdp_umem *umem,
  				u16 qid)
  {
++<<<<<<< HEAD
++=======
+ 	struct net_device *netdev = vsi->netdev;
+ 	struct xdp_umem_fq_reuse *reuseq;
++>>>>>>> f3fef2b6e1cc (i40e: Remove umem from VSI)
  	bool if_running;
  	int err;
  
@@@ -149,13 -88,16 +91,10 @@@
  	if (qid >= vsi->num_queue_pairs)
  		return -EINVAL;
  
- 	if (vsi->xsk_umems) {
- 		if (qid >= vsi->num_xsk_umems)
- 			return -EINVAL;
- 		if (vsi->xsk_umems[qid])
- 			return -EBUSY;
- 	}
+ 	if (qid >= netdev->real_num_rx_queues ||
+ 	    qid >= netdev->real_num_tx_queues)
+ 		return -EINVAL;
  
 -	reuseq = xsk_reuseq_prepare(vsi->rx_rings[0]->count);
 -	if (!reuseq)
 -		return -ENOMEM;
 -
 -	xsk_reuseq_free(xsk_reuseq_swap(umem, reuseq));
 -
  	err = i40e_xsk_umem_dma_map(vsi, umem);
  	if (err)
  		return err;
@@@ -651,3 -648,238 +583,241 @@@ int i40e_clean_rx_irq_zc(struct i40e_ri
  	return failure ? budget : (int)total_rx_packets;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * i40e_xmit_zc - Performs zero-copy Tx AF_XDP
+  * @xdp_ring: XDP Tx ring
+  * @budget: NAPI budget
+  *
+  * Returns true if the work is finished.
+  **/
+ static bool i40e_xmit_zc(struct i40e_ring *xdp_ring, unsigned int budget)
+ {
+ 	struct i40e_tx_desc *tx_desc = NULL;
+ 	struct i40e_tx_buffer *tx_bi;
+ 	bool work_done = true;
+ 	dma_addr_t dma;
+ 	u32 len;
+ 
+ 	while (budget-- > 0) {
+ 		if (!unlikely(I40E_DESC_UNUSED(xdp_ring))) {
+ 			xdp_ring->tx_stats.tx_busy++;
+ 			work_done = false;
+ 			break;
+ 		}
+ 
+ 		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &dma, &len))
+ 			break;
+ 
+ 		dma_sync_single_for_device(xdp_ring->dev, dma, len,
+ 					   DMA_BIDIRECTIONAL);
+ 
+ 		tx_bi = &xdp_ring->tx_bi[xdp_ring->next_to_use];
+ 		tx_bi->bytecount = len;
+ 
+ 		tx_desc = I40E_TX_DESC(xdp_ring, xdp_ring->next_to_use);
+ 		tx_desc->buffer_addr = cpu_to_le64(dma);
+ 		tx_desc->cmd_type_offset_bsz =
+ 			build_ctob(I40E_TX_DESC_CMD_ICRC
+ 				   | I40E_TX_DESC_CMD_EOP,
+ 				   0, len, 0);
+ 
+ 		xdp_ring->next_to_use++;
+ 		if (xdp_ring->next_to_use == xdp_ring->count)
+ 			xdp_ring->next_to_use = 0;
+ 	}
+ 
+ 	if (tx_desc) {
+ 		/* Request an interrupt for the last frame and bump tail ptr. */
+ 		tx_desc->cmd_type_offset_bsz |= (I40E_TX_DESC_CMD_RS <<
+ 						 I40E_TXD_QW1_CMD_SHIFT);
+ 		i40e_xdp_ring_update_tail(xdp_ring);
+ 
+ 		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
+ 	}
+ 
+ 	return !!budget && work_done;
+ }
+ 
+ /**
+  * i40e_clean_xdp_tx_buffer - Frees and unmaps an XDP Tx entry
+  * @tx_ring: XDP Tx ring
+  * @tx_bi: Tx buffer info to clean
+  **/
+ static void i40e_clean_xdp_tx_buffer(struct i40e_ring *tx_ring,
+ 				     struct i40e_tx_buffer *tx_bi)
+ {
+ 	xdp_return_frame(tx_bi->xdpf);
+ 	dma_unmap_single(tx_ring->dev,
+ 			 dma_unmap_addr(tx_bi, dma),
+ 			 dma_unmap_len(tx_bi, len), DMA_TO_DEVICE);
+ 	dma_unmap_len_set(tx_bi, len, 0);
+ }
+ 
+ /**
+  * i40e_clean_xdp_tx_irq - Completes AF_XDP entries, and cleans XDP entries
+  * @tx_ring: XDP Tx ring
+  * @tx_bi: Tx buffer info to clean
+  *
+  * Returns true if cleanup/tranmission is done.
+  **/
+ bool i40e_clean_xdp_tx_irq(struct i40e_vsi *vsi,
+ 			   struct i40e_ring *tx_ring, int napi_budget)
+ {
+ 	unsigned int ntc, total_bytes = 0, budget = vsi->work_limit;
+ 	u32 i, completed_frames, frames_ready, xsk_frames = 0;
+ 	struct xdp_umem *umem = tx_ring->xsk_umem;
+ 	u32 head_idx = i40e_get_head(tx_ring);
+ 	bool work_done = true, xmit_done;
+ 	struct i40e_tx_buffer *tx_bi;
+ 
+ 	if (head_idx < tx_ring->next_to_clean)
+ 		head_idx += tx_ring->count;
+ 	frames_ready = head_idx - tx_ring->next_to_clean;
+ 
+ 	if (frames_ready == 0) {
+ 		goto out_xmit;
+ 	} else if (frames_ready > budget) {
+ 		completed_frames = budget;
+ 		work_done = false;
+ 	} else {
+ 		completed_frames = frames_ready;
+ 	}
+ 
+ 	ntc = tx_ring->next_to_clean;
+ 
+ 	for (i = 0; i < completed_frames; i++) {
+ 		tx_bi = &tx_ring->tx_bi[ntc];
+ 
+ 		if (tx_bi->xdpf)
+ 			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
+ 		else
+ 			xsk_frames++;
+ 
+ 		tx_bi->xdpf = NULL;
+ 		total_bytes += tx_bi->bytecount;
+ 
+ 		if (++ntc >= tx_ring->count)
+ 			ntc = 0;
+ 	}
+ 
+ 	tx_ring->next_to_clean += completed_frames;
+ 	if (unlikely(tx_ring->next_to_clean >= tx_ring->count))
+ 		tx_ring->next_to_clean -= tx_ring->count;
+ 
+ 	if (xsk_frames)
+ 		xsk_umem_complete_tx(umem, xsk_frames);
+ 
+ 	i40e_arm_wb(tx_ring, vsi, budget);
+ 	i40e_update_tx_stats(tx_ring, completed_frames, total_bytes);
+ 
+ out_xmit:
+ 	xmit_done = i40e_xmit_zc(tx_ring, budget);
+ 
+ 	return work_done && xmit_done;
+ }
+ 
+ /**
+  * i40e_xsk_async_xmit - Implements the ndo_xsk_async_xmit
+  * @dev: the netdevice
+  * @queue_id: queue id to wake up
+  *
+  * Returns <0 for errors, 0 otherwise.
+  **/
+ int i40e_xsk_async_xmit(struct net_device *dev, u32 queue_id)
+ {
+ 	struct i40e_netdev_priv *np = netdev_priv(dev);
+ 	struct i40e_vsi *vsi = np->vsi;
+ 	struct i40e_ring *ring;
+ 
+ 	if (test_bit(__I40E_VSI_DOWN, vsi->state))
+ 		return -ENETDOWN;
+ 
+ 	if (!i40e_enabled_xdp_vsi(vsi))
+ 		return -ENXIO;
+ 
+ 	if (queue_id >= vsi->num_queue_pairs)
+ 		return -ENXIO;
+ 
+ 	if (!vsi->xdp_rings[queue_id]->xsk_umem)
+ 		return -ENXIO;
+ 
+ 	ring = vsi->xdp_rings[queue_id];
+ 
+ 	/* The idea here is that if NAPI is running, mark a miss, so
+ 	 * it will run again. If not, trigger an interrupt and
+ 	 * schedule the NAPI from interrupt context. If NAPI would be
+ 	 * scheduled here, the interrupt affinity would not be
+ 	 * honored.
+ 	 */
+ 	if (!napi_if_scheduled_mark_missed(&ring->q_vector->napi))
+ 		i40e_force_wb(vsi, ring->q_vector);
+ 
+ 	return 0;
+ }
+ 
+ void i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring)
+ {
+ 	u16 i;
+ 
+ 	for (i = 0; i < rx_ring->count; i++) {
+ 		struct i40e_rx_buffer *rx_bi = &rx_ring->rx_bi[i];
+ 
+ 		if (!rx_bi->addr)
+ 			continue;
+ 
+ 		xsk_umem_fq_reuse(rx_ring->xsk_umem, rx_bi->handle);
+ 		rx_bi->addr = NULL;
+ 	}
+ }
+ 
+ /**
+  * i40e_xsk_clean_xdp_ring - Clean the XDP Tx ring on shutdown
+  * @xdp_ring: XDP Tx ring
+  **/
+ void i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring)
+ {
+ 	u16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;
+ 	struct xdp_umem *umem = tx_ring->xsk_umem;
+ 	struct i40e_tx_buffer *tx_bi;
+ 	u32 xsk_frames = 0;
+ 
+ 	while (ntc != ntu) {
+ 		tx_bi = &tx_ring->tx_bi[ntc];
+ 
+ 		if (tx_bi->xdpf)
+ 			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
+ 		else
+ 			xsk_frames++;
+ 
+ 		tx_bi->xdpf = NULL;
+ 
+ 		ntc++;
+ 		if (ntc >= tx_ring->count)
+ 			ntc = 0;
+ 	}
+ 
+ 	if (xsk_frames)
+ 		xsk_umem_complete_tx(umem, xsk_frames);
+ }
+ 
+ /**
+  * i40e_xsk_any_rx_ring_enabled - Checks if Rx rings have AF_XDP UMEM attached
+  * @vsi: vsi
+  *
+  * Returns true if any of the Rx rings has an AF_XDP UMEM attached
+  **/
+ bool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi)
+ {
+ 	struct net_device *netdev = vsi->netdev;
+ 	int i;
+ 
+ 	for (i = 0; i < vsi->num_queue_pairs; i++) {
+ 		if (xdp_get_umem_from_qid(netdev, i))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
++>>>>>>> f3fef2b6e1cc (i40e: Remove umem from VSI)
diff --git a/drivers/net/ethernet/intel/i40e/i40e.h b/drivers/net/ethernet/intel/i40e/i40e.h
index 8de9085bba9e..7f41fd422ee1 100644
--- a/drivers/net/ethernet/intel/i40e/i40e.h
+++ b/drivers/net/ethernet/intel/i40e/i40e.h
@@ -34,6 +34,7 @@
 #include <net/pkt_cls.h>
 #include <net/tc_act/tc_gact.h>
 #include <net/tc_act/tc_mirred.h>
+#include <net/xdp_sock.h>
 #include "i40e_type.h"
 #include "i40e_prototype.h"
 #include "i40e_client.h"
@@ -787,11 +788,6 @@ struct i40e_vsi {
 
 	/* VSI specific handlers */
 	irqreturn_t (*irq_handler)(int irq, void *data);
-
-	/* AF_XDP zero-copy */
-	struct xdp_umem **xsk_umems;
-	u16 num_xsk_umems_used;
-	u16 num_xsk_umems;
 } ____cacheline_internodealigned_in_smp;
 
 struct i40e_netdev_priv {
@@ -1104,10 +1100,10 @@ static inline struct xdp_umem *i40e_xsk_umem(struct i40e_ring *ring)
 	if (ring_is_xdp(ring))
 		qid -= ring->vsi->alloc_queue_pairs;
 
-	if (!ring->vsi->xsk_umems || !ring->vsi->xsk_umems[qid] || !xdp_on)
+	if (!xdp_on)
 		return NULL;
 
-	return ring->vsi->xsk_umems[qid];
+	return xdp_get_umem_from_qid(ring->vsi->netdev, qid);
 }
 
 int i40e_create_queue_channel(struct i40e_vsi *vsi, struct i40e_channel *ch);
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_xsk.c
