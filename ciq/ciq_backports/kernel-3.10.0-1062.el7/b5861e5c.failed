KVM: nVMX: Fix loss of pending IRQ/NMI before entering L2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Liran Alon <liran.alon@oracle.com>
commit b5861e5cf2fcf83031ea3e26b0a69d887adf7d21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/b5861e5c.failed

Consider the case L1 had a IRQ/NMI event until it executed
VMLAUNCH/VMRESUME which wasn't delivered because it was disallowed
(e.g. interrupts disabled). When L1 executes VMLAUNCH/VMRESUME,
L0 needs to evaluate if this pending event should cause an exit from
L2 to L1 or delivered directly to L2 (e.g. In case L1 don't intercept
EXTERNAL_INTERRUPT).

Usually this would be handled by L0 requesting a IRQ/NMI window
by setting VMCS accordingly. However, this setting was done on
VMCS01 and now VMCS02 is active instead. Thus, when L1 executes
VMLAUNCH/VMRESUME we force L0 to perform pending event evaluation by
requesting a KVM_REQ_EVENT.

Note that above scenario exists when L1 KVM is about to enter L2 but
requests an "immediate-exit". As in this case, L1 will
disable-interrupts and then send a self-IPI before entering L2.

	Reviewed-by: Nikita Leshchenko <nikita.leshchenko@oracle.com>
Co-developed-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Liran Alon <liran.alon@oracle.com>
	Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
(cherry picked from commit b5861e5cf2fcf83031ea3e26b0a69d887adf7d21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 5501a1957de2,533a327372c8..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -10736,8 -12535,12 +10736,17 @@@ static int enter_vmx_non_root_mode(stru
  {
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
  	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
++<<<<<<< HEAD
 +	u32 msr_entry_idx;
 +	u32 exit_qual;
++=======
+ 	bool from_vmentry = !!exit_qual;
+ 	u32 dummy_exit_qual;
+ 	u32 vmcs01_cpu_exec_ctrl;
+ 	int r = 0;
++>>>>>>> b5861e5cf2fc (KVM: nVMX: Fix loss of pending IRQ/NMI before entering L2)
+ 
+ 	vmcs01_cpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
  
  	enter_guest_mode(vcpu);
  
@@@ -10747,27 -12550,52 +10756,46 @@@
  	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
  	vmx_segment_cache_clear(vmx);
  
 -	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)
 -		vcpu->arch.tsc_offset += vmcs12->tsc_offset;
 -
 -	r = EXIT_REASON_INVALID_STATE;
 -	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry ? exit_qual : &dummy_exit_qual))
 -		goto fail;
 +	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &exit_qual)) {
 +		leave_guest_mode(vcpu);
 +		vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 +		nested_vmx_entry_failure(vcpu, vmcs12,
 +					 EXIT_REASON_INVALID_STATE, exit_qual);
 +		return 1;
 +	}
  
 -	if (from_vmentry) {
 -		nested_get_vmcs12_pages(vcpu);
 +	nested_get_vmcs12_pages(vcpu, vmcs12);
  
 -		r = EXIT_REASON_MSR_LOAD_FAIL;
 -		*exit_qual = nested_vmx_load_msr(vcpu,
 -	     					 vmcs12->vm_entry_msr_load_addr,
 -					      	 vmcs12->vm_entry_msr_load_count);
 -		if (*exit_qual)
 -			goto fail;
 -	} else {
 -		/*
 -		 * The MMU is not initialized to point at the right entities yet and
 -		 * "get pages" would need to read data from the guest (i.e. we will
 -		 * need to perform gpa to hpa translation). Request a call
 -		 * to nested_get_vmcs12_pages before the next VM-entry.  The MSRs
 -		 * have already been set at vmentry time and should not be reset.
 -		 */
 -		kvm_make_request(KVM_REQ_GET_VMCS12_PAGES, vcpu);
 +	msr_entry_idx = nested_vmx_load_msr(vcpu,
 +					    vmcs12->vm_entry_msr_load_addr,
 +					    vmcs12->vm_entry_msr_load_count);
 +	if (msr_entry_idx) {
 +		leave_guest_mode(vcpu);
 +		vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 +		nested_vmx_entry_failure(vcpu, vmcs12,
 +				EXIT_REASON_MSR_LOAD_FAIL, msr_entry_idx);
 +		return 1;
  	}
  
+ 	/*
+ 	 * If L1 had a pending IRQ/NMI until it executed
+ 	 * VMLAUNCH/VMRESUME which wasn't delivered because it was
+ 	 * disallowed (e.g. interrupts disabled), L0 needs to
+ 	 * evaluate if this pending event should cause an exit from L2
+ 	 * to L1 or delivered directly to L2 (e.g. In case L1 don't
+ 	 * intercept EXTERNAL_INTERRUPT).
+ 	 *
+ 	 * Usually this would be handled by L0 requesting a
+ 	 * IRQ/NMI window by setting VMCS accordingly. However,
+ 	 * this setting was done on VMCS01 and now VMCS02 is active
+ 	 * instead. Thus, we force L0 to perform pending event
+ 	 * evaluation by requesting a KVM_REQ_EVENT.
+ 	 */
+ 	if (vmcs01_cpu_exec_ctrl &
+ 		(CPU_BASED_VIRTUAL_INTR_PENDING | CPU_BASED_VIRTUAL_NMI_PENDING)) {
+ 		kvm_make_request(KVM_REQ_EVENT, vcpu);
+ 	}
+ 
  	/*
  	 * Note no nested_vmx_succeed or nested_vmx_fail here. At this point
  	 * we are no longer running L1, and VMLAUNCH/VMRESUME has not yet
* Unmerged path arch/x86/kvm/vmx.c
