net/mlx5: EQ, Privatize eq_table and friends

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5: EQ, Privatize eq_table and friends (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 95.24%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit f2f3df5501391bc784c8462dc97d989c2194fb74
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f2f3df55.failed

Move unnecessary EQ table structures and declaration from the
public include/linux/mlx5/driver.h into the private area of mlx5_core
and into eq.c/eq.h.

Introduce new mlx5 EQ APIs:

mlx5_comp_vectors_count(dev);
mlx5_comp_irq_get_affinity_mask(dev, vector);

And use them from mlx5_ib or mlx5e netdevice instead of direct access to
mlx5_core internal structures.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit f2f3df5501391bc784c8462dc97d989c2194fb74)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
#	include/linux/mlx5/driver.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 8f90751c4e18,6fbc0cba1bac..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -4597,7 -5337,7 +4597,11 @@@ mlx5_ib_get_vector_affinity(struct ib_d
  {
  	struct mlx5_ib_dev *dev = to_mdev(ibdev);
  
++<<<<<<< HEAD
 +	return mlx5_get_vector_affinity(dev->mdev, comp_vector);
++=======
+ 	return mlx5_comp_irq_get_affinity_mask(dev->mdev, comp_vector);
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  }
  
  /* The mlx5_ib_multiport_mutex should be held when calling this function */
@@@ -4855,11 -5701,9 +4859,10 @@@ static int mlx5_ib_stage_init_init(stru
  	dev->ib_dev.node_type		= RDMA_NODE_IB_CA;
  	dev->ib_dev.local_dma_lkey	= 0 /* not supported for now */;
  	dev->ib_dev.phys_port_cnt	= dev->num_ports;
- 	dev->ib_dev.num_comp_vectors    =
- 		dev->mdev->priv.eq_table.num_comp_vectors;
+ 	dev->ib_dev.num_comp_vectors    = mlx5_comp_vectors_count(mdev);
  	dev->ib_dev.dev.parent		= &mdev->pdev->dev;
  
 +	mutex_init(&dev->flow_db.lock);
  	mutex_init(&dev->cap_mask_mutex);
  	INIT_LIST_HEAD(&dev->qp_list);
  	spin_lock_init(&dev->reset_flow_resource_lock);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 48dde39aa588,c23caade31bf..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -51,7 -45,11 +51,15 @@@
  #include "en_accel/tls.h"
  #include "accel/ipsec.h"
  #include "accel/tls.h"
++<<<<<<< HEAD
 +#include "vxlan.h"
++=======
+ #include "lib/vxlan.h"
+ #include "lib/clock.h"
+ #include "en/port.h"
+ #include "en/xdp.h"
+ #include "lib/eq.h"
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  
  struct mlx5e_rq_param {
  	u32			rqc[MLX5_ST_SZ_DW(rqc)];
@@@ -1475,11 -1759,6 +1483,14 @@@ static void mlx5e_close_cq(struct mlx5e
  	mlx5e_free_cq(cq);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_get_cpu(struct mlx5e_priv *priv, int ix)
 +{
 +	return cpumask_first(priv->mdev->priv.irq_info[ix].mask);
 +}
 +
++=======
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  static int mlx5e_open_tx_cqs(struct mlx5e_channel *c,
  			     struct mlx5e_params *params,
  			     struct mlx5e_channel_param *cparam)
@@@ -4544,13 -4927,53 +4555,56 @@@ static const struct mlx5e_profile mlx5e
  
  /* mlx5e generic netdev management API (move to en_common.c) */
  
++<<<<<<< HEAD
++=======
+ /* mlx5e_netdev_init/cleanup must be called from profile->init/cleanup callbacks */
+ int mlx5e_netdev_init(struct net_device *netdev,
+ 		      struct mlx5e_priv *priv,
+ 		      struct mlx5_core_dev *mdev,
+ 		      const struct mlx5e_profile *profile,
+ 		      void *ppriv)
+ {
+ 	/* priv init */
+ 	priv->mdev        = mdev;
+ 	priv->netdev      = netdev;
+ 	priv->profile     = profile;
+ 	priv->ppriv       = ppriv;
+ 	priv->msglevel    = MLX5E_MSG_LEVEL;
+ 	priv->max_opened_tc = 1;
+ 
+ 	mutex_init(&priv->state_lock);
+ 	INIT_WORK(&priv->update_carrier_work, mlx5e_update_carrier_work);
+ 	INIT_WORK(&priv->set_rx_mode_work, mlx5e_set_rx_mode_work);
+ 	INIT_WORK(&priv->tx_timeout_work, mlx5e_tx_timeout_work);
+ 	INIT_WORK(&priv->update_stats_work, mlx5e_update_stats_work);
+ 
+ 	priv->wq = create_singlethread_workqueue("mlx5e");
+ 	if (!priv->wq)
+ 		return -ENOMEM;
+ 
+ 	/* netdev init */
+ 	netif_carrier_off(netdev);
+ 
+ #ifdef CONFIG_MLX5_EN_ARFS
+ 	netdev->rx_cpu_rmap =  mlx5_eq_table_get_rmap(mdev);
+ #endif
+ 
+ 	return 0;
+ }
+ 
+ void mlx5e_netdev_cleanup(struct net_device *netdev, struct mlx5e_priv *priv)
+ {
+ 	destroy_workqueue(priv->wq);
+ }
+ 
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
  				       const struct mlx5e_profile *profile,
 -				       int nch,
  				       void *ppriv)
  {
 +	int nch = profile->max_nch(mdev);
  	struct net_device *netdev;
 -	int err;
 +	struct mlx5e_priv *priv;
  
  	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
  				    nch * profile->max_tc,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index 9a3af24e5f23,32ce20221c44..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -34,9 -34,14 +34,10 @@@
  #include <linux/module.h>
  #include <linux/mlx5/driver.h>
  #include <linux/mlx5/cmd.h>
 -#ifdef CONFIG_RFS_ACCEL
 -#include <linux/cpu_rmap.h>
 -#endif
  #include "mlx5_core.h"
+ #include "lib/eq.h"
  #include "fpga/core.h"
  #include "eswitch.h"
 -#include "lib/clock.h"
  #include "diag/fw_tracer.h"
  
  enum {
@@@ -689,7 -716,7 +711,11 @@@ mlx5_create_map_eq(struct mlx5_core_de
  	if (err)
  		goto err_in;
  
++<<<<<<< HEAD
 +	snprintf(priv->irq_info[vecidx].name, MLX5_MAX_IRQ_NAME, "%s@pci:%s",
++=======
+ 	snprintf(eq_table->irq_info[vecidx].name, MLX5_MAX_IRQ_NAME, "%s@pci:%s",
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  		 name, pci_name(dev->pdev));
  
  	eq->eqn = MLX5_GET(create_eq_out, out, eq_number);
@@@ -697,7 -724,7 +723,11 @@@
  	eq->dev = dev;
  	eq->doorbell = priv->uar->map + MLX5_EQ_DOORBEL_OFFSET;
  	err = request_irq(eq->irqn, handler, 0,
++<<<<<<< HEAD
 +			  priv->irq_info[vecidx].name, eq);
++=======
+ 			  eq_table->irq_info[vecidx].name, eq);
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  	if (err)
  		goto err_eq;
  
@@@ -810,14 -846,17 +849,15 @@@ int mlx5_eq_table_init(struct mlx5_core
  	return err;
  }
  
- void mlx5_eq_cleanup(struct mlx5_core_dev *dev)
+ void mlx5_eq_table_cleanup(struct mlx5_core_dev *dev)
  {
  	mlx5_eq_debugfs_cleanup(dev);
+ 	kvfree(dev->priv.eq_table);
  }
  
 -/* Async EQs */
 -
 -static int create_async_eqs(struct mlx5_core_dev *dev)
 +int mlx5_start_eqs(struct mlx5_core_dev *dev)
  {
- 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
  	u64 async_event_mask = MLX5_ASYNC_EVENT_MASK;
  	int err;
  
@@@ -907,9 -946,9 +947,9 @@@ err1
  	return err;
  }
  
 -static void destroy_async_eqs(struct mlx5_core_dev *dev)
 +void mlx5_stop_eqs(struct mlx5_core_dev *dev)
  {
- 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
  	int err;
  
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@@ -938,12 -977,312 +978,323 @@@
  			      err);
  }
  
++<<<<<<< HEAD
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen)
 +{
 +	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {0};
 +
 +	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
 +	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
++=======
+ struct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev)
+ {
+ 	return &dev->priv.eq_table->async_eq;
+ }
+ 
+ /* Completion EQs */
+ 
+ static int set_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	if (!zalloc_cpumask_var(&irq_info->mask, GFP_KERNEL)) {
+ 		mlx5_core_warn(mdev, "zalloc_cpumask_var failed");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	cpumask_set_cpu(cpumask_local_spread(i, priv->numa_node),
+ 			irq_info->mask);
+ 
+ 	if (IS_ENABLED(CONFIG_SMP) &&
+ 	    irq_set_affinity_hint(irq, irq_info->mask))
+ 		mlx5_core_warn(mdev, "irq_set_affinity_hint failed, irq 0x%.4x", irq);
+ 
+ 	return 0;
+ }
+ 
+ static void clear_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	irq_set_affinity_hint(irq, NULL);
+ 	free_cpumask_var(irq_info->mask);
+ }
+ 
+ static int set_comp_irq_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++) {
+ 		err = set_comp_irq_affinity_hint(mdev, i);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return 0;
+ 
+ err_out:
+ 	for (i--; i >= 0; i--)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ 
+ 	return err;
+ }
+ 
+ static void clear_comp_irqs_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ }
+ 
+ static void destroy_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq *eq, *n;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		list_del(&eq->list);
+ 		if (mlx5_destroy_unmap_eq(dev, eq))
+ 			mlx5_core_warn(dev, "failed to destroy EQ 0x%x\n",
+ 				       eq->eqn);
+ 		kfree(eq);
+ 	}
+ }
+ 
+ static int create_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	struct mlx5_eq *eq;
+ 	int ncomp_vec;
+ 	int nent;
+ 	int err;
+ 	int i;
+ 
+ 	INIT_LIST_HEAD(&table->comp_eqs_list);
+ 	ncomp_vec = table->num_comp_vectors;
+ 	nent = MLX5_COMP_EQ_SIZE;
+ #ifdef CONFIG_RFS_ACCEL
+ 	table->rmap = alloc_irq_cpu_rmap(ncomp_vec);
+ 	if (!table->rmap)
+ 		return -ENOMEM;
+ #endif
+ 	for (i = 0; i < ncomp_vec; i++) {
+ 		int vecidx = i + MLX5_EQ_VEC_COMP_BASE;
+ 
+ 		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
+ 		if (!eq) {
+ 			err = -ENOMEM;
+ 			goto clean;
+ 		}
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 		irq_cpu_rmap_add(table->rmap, pci_irq_vector(dev->pdev, vecidx));
+ #endif
+ 		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
+ 		err = mlx5_create_map_eq(dev, eq, vecidx, nent, 0,
+ 					 name, MLX5_EQ_TYPE_COMP);
+ 		if (err) {
+ 			kfree(eq);
+ 			goto clean;
+ 		}
+ 		mlx5_core_dbg(dev, "allocated completion EQN %d\n", eq->eqn);
+ 		/* add tail, to keep the list ordered, for mlx5_vector2eqn to work */
+ 		list_add_tail(&eq->list, &table->comp_eqs_list);
+ 	}
+ 
+ 	err = set_comp_irq_affinity_hints(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to alloc affinity hint cpumask\n");
+ 		goto clean;
+ 	}
+ 
+ 	return 0;
+ 
+ clean:
+ 	destroy_comp_eqs(dev);
+ 	return err;
+ }
+ 
+ int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
+ 		    unsigned int *irqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq *eq, *n;
+ 	int err = -ENOENT;
+ 	int i = 0;
+ 
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		if (i++ == vector) {
+ 			*eqn = eq->eqn;
+ 			*irqn = eq->irqn;
+ 			err = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_vector2eqn);
+ 
+ unsigned int mlx5_comp_vectors_count(struct mlx5_core_dev *dev)
+ {
+ 	return dev->priv.eq_table->num_comp_vectors;
+ }
+ EXPORT_SYMBOL(mlx5_comp_vectors_count);
+ 
+ struct cpumask *
+ mlx5_comp_irq_get_affinity_mask(struct mlx5_core_dev *dev, int vector)
+ {
+ 	/* TODO: consider irq_get_affinity_mask(irq) */
+ 	return dev->priv.eq_table->irq_info[vector + MLX5_EQ_VEC_COMP_BASE].mask;
+ }
+ EXPORT_SYMBOL(mlx5_comp_irq_get_affinity_mask);
+ 
+ struct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev)
+ {
+ #ifdef CONFIG_RFS_ACCEL
+ 	return dev->priv.eq_table->rmap;
+ #else
+ 	return NULL;
+ #endif
+ }
+ 
+ struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq *eq;
+ 
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list) {
+ 		if (eq->eqn == eqn)
+ 			return eq;
+ 	}
+ 
+ 	return ERR_PTR(-ENOENT);
+ }
+ 
+ /* This function should only be called after mlx5_cmd_force_teardown_hca */
+ void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq *eq;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list)
+ 		free_irq(eq->irqn, eq);
+ 
+ 	free_irq(table->pages_eq.irqn, &table->pages_eq);
+ 	free_irq(table->async_eq.irqn, &table->async_eq);
+ 	free_irq(table->cmd_eq.irqn, &table->cmd_eq);
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	if (MLX5_CAP_GEN(dev, pg))
+ 		free_irq(table->pfault_eq.irqn, &table->pfault_eq);
+ #endif
+ 	pci_free_irq_vectors(dev->pdev);
+ }
+ 
+ static int alloc_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 	struct mlx5_eq_table *table = priv->eq_table;
+ 	int num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?
+ 		      MLX5_CAP_GEN(dev, max_num_eqs) :
+ 		      1 << MLX5_CAP_GEN(dev, log_max_eq);
+ 	int nvec;
+ 	int err;
+ 
+ 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
+ 	       MLX5_EQ_VEC_COMP_BASE;
+ 	nvec = min_t(int, nvec, num_eqs);
+ 	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
+ 		return -ENOMEM;
+ 
+ 	table->irq_info = kcalloc(nvec, sizeof(*table->irq_info), GFP_KERNEL);
+ 	if (!table->irq_info)
+ 		return -ENOMEM;
+ 
+ 	nvec = pci_alloc_irq_vectors(dev->pdev, MLX5_EQ_VEC_COMP_BASE + 1,
+ 				     nvec, PCI_IRQ_MSIX);
+ 	if (nvec < 0) {
+ 		err = nvec;
+ 		goto err_free_irq_info;
+ 	}
+ 
+ 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+ 
+ 	return 0;
+ 
+ err_free_irq_info:
+ 	kfree(table->irq_info);
+ 	return err;
+ }
+ 
+ static void free_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 
+ 	pci_free_irq_vectors(dev->pdev);
+ 	kfree(priv->eq_table->irq_info);
+ }
+ 
+ int mlx5_eq_table_create(struct mlx5_core_dev *dev)
+ {
+ 	int err;
+ 
+ 	err = alloc_irq_vectors(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "alloc irq vectors failed\n");
+ 		return err;
+ 	}
+ 
+ 	err = create_async_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create async EQs\n");
+ 		goto err_async_eqs;
+ 	}
+ 
+ 	err = create_comp_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create completion EQs\n");
+ 		goto err_comp_eqs;
+ 	}
+ 
+ 	return 0;
+ err_comp_eqs:
+ 	destroy_async_eqs(dev);
+ err_async_eqs:
+ 	free_irq_vectors(dev);
+ 	return err;
+ }
+ 
+ void mlx5_eq_table_destroy(struct mlx5_core_dev *dev)
+ {
+ 	destroy_comp_eqs(dev);
+ 	destroy_async_eqs(dev);
+ 	free_irq_vectors(dev);
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 2ac07968015d,4d39adcfb0eb..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@@ -121,21 -124,6 +121,24 @@@ int mlx5_destroy_scheduling_element_cmd
  int mlx5_wait_for_vf_pages(struct mlx5_core_dev *dev);
  u64 mlx5_read_internal_timer(struct mlx5_core_dev *dev);
  
++<<<<<<< HEAD
 +int mlx5_eq_init(struct mlx5_core_dev *dev);
 +void mlx5_eq_cleanup(struct mlx5_core_dev *dev);
 +int mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
 +		       int nent, u64 mask, const char *name,
 +		       enum mlx5_eq_type type);
 +int mlx5_destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
 +int mlx5_eq_add_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
 +int mlx5_eq_del_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen);
 +int mlx5_start_eqs(struct mlx5_core_dev *dev);
 +void mlx5_stop_eqs(struct mlx5_core_dev *dev);
 +struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
 +u32 mlx5_eq_poll_irq_disabled(struct mlx5_eq *eq);
 +void mlx5_cq_tasklet_cb(unsigned long data);
++=======
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  void mlx5_cmd_comp_handler(struct mlx5_core_dev *dev, u64 vec, bool forced);
  int mlx5_debug_eq_add(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
  void mlx5_debug_eq_remove(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
diff --cc include/linux/mlx5/driver.h
index af4fa7465c37,4d6246cb6c19..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -464,19 -417,9 +409,22 @@@ struct mlx5_core_srq 
  
  	atomic_t		refcount;
  	struct completion	free;
 -	u16		uid;
  };
  
++<<<<<<< HEAD
 +struct mlx5_eq_table {
 +	struct list_head	comp_eqs_list;
 +	struct mlx5_eq		pages_eq;
 +	struct mlx5_eq		async_eq;
 +	struct mlx5_eq		cmd_eq;
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	struct mlx5_eq		pfault_eq;
 +#endif
 +	int			num_comp_vectors;
 +};
 +
++=======
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  struct mlx5_uars_page {
  	void __iomem	       *map;
  	bool			wc;
@@@ -630,8 -569,7 +574,12 @@@ struct mlx5_port_module_event_stats 
  
  struct mlx5_priv {
  	char			name[MLX5_MAX_NAME_LEN];
++<<<<<<< HEAD
 +	struct mlx5_eq_table	eq_table;
 +	struct mlx5_irq_info	*irq_info;
++=======
+ 	struct mlx5_eq_table	*eq_table;
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  
  	/* pages stuff */
  	struct workqueue_struct *pg_wq;
@@@ -1273,31 -1228,4 +1224,34 @@@ enum 
  	MLX5_TRIGGERED_CMD_COMP = (u64)1 << 32,
  };
  
++<<<<<<< HEAD
 +static inline const struct cpumask *
 +mlx5_get_vector_affinity(struct mlx5_core_dev *dev, int vector)
 +{
 +/* calling irq_to_desc will result to undefinded irq_desc symbol */
 +#ifndef CONFIG_GENERIC_HARDIRQS
 +	return cpu_possible_mask;
 +#else
 +	const struct cpumask *mask;
 +	struct irq_desc *desc;
 +	unsigned int irq;
 +	int eqn;
 +	int err;
 +
 +	err = mlx5_vector2eqn(dev, MLX5_EQ_VEC_COMP_BASE + vector, &eqn, &irq);
 +	if (err)
 +		return NULL;
 +
 +	desc = irq_to_desc(irq);
 +#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK
 +	mask = irq_data_get_effective_affinity_mask(&desc->irq_data);
 +#else
 +	mask = desc->irq_data.affinity;
 +#endif
 +	return mask;
 +#endif
 +}
 +
++=======
++>>>>>>> f2f3df550139 (net/mlx5: EQ, Privatize eq_table and friends)
  #endif /* MLX5_DRIVER_H */
* Unmerged path drivers/infiniband/hw/mlx5/main.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/cq.c b/drivers/net/ethernet/mellanox/mlx5/core/cq.c
index 4b85abb5c9f7..6e55d2f37c6d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/cq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cq.c
@@ -38,6 +38,7 @@
 #include <rdma/ib_verbs.h>
 #include <linux/mlx5/cq.h>
 #include "mlx5_core.h"
+#include "lib/eq.h"
 
 #define TASKLET_MAX_TIME 2
 #define TASKLET_MAX_TIME_JIFFIES msecs_to_jiffies(TASKLET_MAX_TIME)
@@ -124,7 +125,7 @@ int mlx5_core_create_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq,
 		goto err_cmd;
 
 	/* Add to async EQ CQ tree to recv async events */
-	err = mlx5_eq_add_cq(&dev->priv.eq_table.async_eq, cq);
+	err = mlx5_eq_add_cq(mlx5_get_async_eq(dev), cq);
 	if (err)
 		goto err_cq_add;
 
@@ -157,7 +158,7 @@ int mlx5_core_destroy_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq)
 	u32 in[MLX5_ST_SZ_DW(destroy_cq_in)] = {0};
 	int err;
 
-	err = mlx5_eq_del_cq(&dev->priv.eq_table.async_eq, cq);
+	err = mlx5_eq_del_cq(mlx5_get_async_eq(dev), cq);
 	if (err)
 		return err;
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
index 7ecadb501743..347918406ff8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
@@ -36,6 +36,7 @@
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/driver.h>
 #include "mlx5_core.h"
+#include "lib/eq.h"
 
 enum {
 	QP_PID,
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4a9c2819d01c..f5b4c5b03b57 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -178,8 +178,7 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 {
 	return is_kdump_kernel() ?
 		MLX5E_MIN_NUM_CHANNELS :
-		min_t(int, mdev->priv.eq_table.num_comp_vectors,
-		      MLX5E_MAX_NUM_CHANNELS);
+		min_t(int, mlx5_comp_vectors_count(mdev), MLX5E_MAX_NUM_CHANNELS);
 }
 
 struct mlx5e_tx_wqe {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
index 8798f4084ee6..750ac094d980 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
@@ -38,6 +38,7 @@
 #include "mlx5_core.h"
 #include "eswitch.h"
 #include "fs_core.h"
+#include "lib/eq.h"
 
 #define UPLINK_VPORT 0xFFFF
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/health.c b/drivers/net/ethernet/mellanox/mlx5/core/health.c
index b565dc1876d3..7197e7a92d4c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/health.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/health.c
@@ -38,6 +38,7 @@
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cmd.h>
 #include "mlx5_core.h"
+#include "lib/eq.h"
 
 enum {
 	MLX5_HEALTH_POLL_INTERVAL	= 2 * HZ,
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h b/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
new file mode 100644
index 000000000000..48ee37797b3f
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
@@ -0,0 +1,77 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB */
+/* Copyright (c) 2018 Mellanox Technologies */
+
+#ifndef __LIB_MLX5_EQ_H__
+#define __LIB_MLX5_EQ_H__
+#include <linux/mlx5/driver.h>
+
+#define MLX5_MAX_IRQ_NAME       (32)
+
+enum {
+	MLX5_EQ_VEC_PAGES	 = 0,
+	MLX5_EQ_VEC_CMD		 = 1,
+	MLX5_EQ_VEC_ASYNC	 = 2,
+	MLX5_EQ_VEC_PFAULT	 = 3,
+	MLX5_EQ_VEC_COMP_BASE,
+};
+
+struct mlx5_eq_tasklet {
+	struct list_head      list;
+	struct list_head      process_list;
+	struct tasklet_struct task;
+	spinlock_t            lock; /* lock completion tasklet list */
+};
+
+struct mlx5_eq_pagefault {
+	struct work_struct       work;
+	spinlock_t               lock; /* Pagefaults spinlock */
+	struct workqueue_struct  *wq;
+	mempool_t                *pool;
+};
+
+struct mlx5_cq_table {
+	spinlock_t              lock;	/* protect radix tree */
+	struct radix_tree_root  tree;
+};
+
+struct mlx5_eq {
+	struct mlx5_core_dev    *dev;
+	struct mlx5_cq_table    cq_table;
+	__be32 __iomem	        *doorbell;
+	u32                     cons_index;
+	struct mlx5_frag_buf    buf;
+	int                     size;
+	unsigned int            irqn;
+	u8                      eqn;
+	int                     nent;
+	struct list_head        list;
+	struct mlx5_rsc_debug   *dbg;
+	enum mlx5_eq_type       type;
+	union {
+		struct mlx5_eq_tasklet   tasklet_ctx;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+		struct mlx5_eq_pagefault pf_ctx;
+#endif
+	};
+};
+
+int mlx5_eq_table_init(struct mlx5_core_dev *dev);
+void mlx5_eq_table_cleanup(struct mlx5_core_dev *dev);
+int mlx5_eq_table_create(struct mlx5_core_dev *dev);
+void mlx5_eq_table_destroy(struct mlx5_core_dev *dev);
+int mlx5_eq_add_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
+int mlx5_eq_del_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
+struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
+struct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev);
+u32 mlx5_eq_poll_irq_disabled(struct mlx5_eq *eq);
+void mlx5_cq_tasklet_cb(unsigned long data);
+struct cpumask *mlx5_eq_comp_cpumask(struct mlx5_core_dev *dev, int ix);
+
+/* This function should only be called after mlx5_cmd_force_teardown_hca */
+void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev);
+
+#ifdef CONFIG_RFS_ACCEL
+struct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev);
+#endif
+
+#endif
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/main.c b/drivers/net/ethernet/mellanox/mlx5/core/main.c
index be9913773fa9..22654f217465 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -54,6 +54,7 @@
 #include <net/devlink.h>
 #include "mlx5_core.h"
 #include "fs_core.h"
+#include "lib/eq.h"
 #include "lib/mpfs.h"
 #include "eswitch.h"
 #include "lib/mlx5.h"
@@ -962,7 +963,7 @@ static int mlx5_init_once(struct mlx5_core_dev *dev, struct mlx5_priv *priv)
 		goto out;
 	}
 
-	err = mlx5_eq_init(dev);
+	err = mlx5_eq_table_init(dev);
 	if (err) {
 		dev_err(&pdev->dev, "failed to initialize eq\n");
 		goto out;
@@ -1033,7 +1034,7 @@ err_tables_cleanup:
 	mlx5_cq_debugfs_cleanup(dev);
 
 err_eq_cleanup:
-	mlx5_eq_cleanup(dev);
+	mlx5_eq_table_cleanup(dev);
 
 out:
 	return err;
@@ -1053,7 +1054,7 @@ static void mlx5_cleanup_once(struct mlx5_core_dev *dev)
 	mlx5_cleanup_srq_table(dev);
 	mlx5_cleanup_qp_table(dev);
 	mlx5_cq_debugfs_cleanup(dev);
-	mlx5_eq_cleanup(dev);
+	mlx5_eq_table_cleanup(dev);
 }
 
 static int mlx5_load_one(struct mlx5_core_dev *dev, struct mlx5_priv *priv,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
* Unmerged path include/linux/mlx5/driver.h
