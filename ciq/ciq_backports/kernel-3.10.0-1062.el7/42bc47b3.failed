treewide: Use array_size() in vmalloc()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Kees Cook <keescook@chromium.org>
commit 42bc47b35320e0e587a88e437e18f80f9c5bcbb2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/42bc47b3.failed

The vmalloc() function has no 2-factor argument form, so multiplication
factors need to be wrapped in array_size(). This patch replaces cases of:

        vmalloc(a * b)

with:
        vmalloc(array_size(a, b))

as well as handling cases of:

        vmalloc(a * b * c)

with:

        vmalloc(array3_size(a, b, c))

This does, however, attempt to ignore constant size factors like:

        vmalloc(4 * 1024)

though any constants defined via macros get caught up in the conversion.

Any factors with a sizeof() of "unsigned char", "char", and "u8" were
dropped, since they're redundant.

The Coccinelle script used for this was:

// Fix redundant parens around sizeof().
@@
type TYPE;
expression THING, E;
@@

(
  vmalloc(
-	(sizeof(TYPE)) * E
+	sizeof(TYPE) * E
  , ...)
|
  vmalloc(
-	(sizeof(THING)) * E
+	sizeof(THING) * E
  , ...)
)

// Drop single-byte sizes and redundant parens.
@@
expression COUNT;
typedef u8;
typedef __u8;
@@

(
  vmalloc(
-	sizeof(u8) * (COUNT)
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(__u8) * (COUNT)
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(char) * (COUNT)
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(unsigned char) * (COUNT)
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(u8) * COUNT
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(__u8) * COUNT
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(char) * COUNT
+	COUNT
  , ...)
|
  vmalloc(
-	sizeof(unsigned char) * COUNT
+	COUNT
  , ...)
)

// 2-factor product with sizeof(type/expression) and identifier or constant.
@@
type TYPE;
expression THING;
identifier COUNT_ID;
constant COUNT_CONST;
@@

(
  vmalloc(
-	sizeof(TYPE) * (COUNT_ID)
+	array_size(COUNT_ID, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * COUNT_ID
+	array_size(COUNT_ID, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * (COUNT_CONST)
+	array_size(COUNT_CONST, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * COUNT_CONST
+	array_size(COUNT_CONST, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(THING) * (COUNT_ID)
+	array_size(COUNT_ID, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * COUNT_ID
+	array_size(COUNT_ID, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * (COUNT_CONST)
+	array_size(COUNT_CONST, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * COUNT_CONST
+	array_size(COUNT_CONST, sizeof(THING))
  , ...)
)

// 2-factor product, only identifiers.
@@
identifier SIZE, COUNT;
@@

  vmalloc(
-	SIZE * COUNT
+	array_size(COUNT, SIZE)
  , ...)

// 3-factor product with 1 sizeof(type) or sizeof(expression), with
// redundant parens removed.
@@
expression THING;
identifier STRIDE, COUNT;
type TYPE;
@@

(
  vmalloc(
-	sizeof(TYPE) * (COUNT) * (STRIDE)
+	array3_size(COUNT, STRIDE, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * (COUNT) * STRIDE
+	array3_size(COUNT, STRIDE, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * COUNT * (STRIDE)
+	array3_size(COUNT, STRIDE, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(TYPE) * COUNT * STRIDE
+	array3_size(COUNT, STRIDE, sizeof(TYPE))
  , ...)
|
  vmalloc(
-	sizeof(THING) * (COUNT) * (STRIDE)
+	array3_size(COUNT, STRIDE, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * (COUNT) * STRIDE
+	array3_size(COUNT, STRIDE, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * COUNT * (STRIDE)
+	array3_size(COUNT, STRIDE, sizeof(THING))
  , ...)
|
  vmalloc(
-	sizeof(THING) * COUNT * STRIDE
+	array3_size(COUNT, STRIDE, sizeof(THING))
  , ...)
)

// 3-factor product with 2 sizeof(variable), with redundant parens removed.
@@
expression THING1, THING2;
identifier COUNT;
type TYPE1, TYPE2;
@@

(
  vmalloc(
-	sizeof(TYPE1) * sizeof(TYPE2) * COUNT
+	array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
  , ...)
|
  vmalloc(
-	sizeof(TYPE1) * sizeof(THING2) * (COUNT)
+	array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
  , ...)
|
  vmalloc(
-	sizeof(THING1) * sizeof(THING2) * COUNT
+	array3_size(COUNT, sizeof(THING1), sizeof(THING2))
  , ...)
|
  vmalloc(
-	sizeof(THING1) * sizeof(THING2) * (COUNT)
+	array3_size(COUNT, sizeof(THING1), sizeof(THING2))
  , ...)
|
  vmalloc(
-	sizeof(TYPE1) * sizeof(THING2) * COUNT
+	array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
  , ...)
|
  vmalloc(
-	sizeof(TYPE1) * sizeof(THING2) * (COUNT)
+	array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
  , ...)
)

// 3-factor product, only identifiers, with redundant parens removed.
@@
identifier STRIDE, SIZE, COUNT;
@@

(
  vmalloc(
-	(COUNT) * STRIDE * SIZE
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	COUNT * (STRIDE) * SIZE
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	COUNT * STRIDE * (SIZE)
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	(COUNT) * (STRIDE) * SIZE
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	COUNT * (STRIDE) * (SIZE)
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	(COUNT) * STRIDE * (SIZE)
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	(COUNT) * (STRIDE) * (SIZE)
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
|
  vmalloc(
-	COUNT * STRIDE * SIZE
+	array3_size(COUNT, STRIDE, SIZE)
  , ...)
)

// Any remaining multi-factor products, first at least 3-factor products
// when they're not all constants...
@@
expression E1, E2, E3;
constant C1, C2, C3;
@@

(
  vmalloc(C1 * C2 * C3, ...)
|
  vmalloc(
-	E1 * E2 * E3
+	array3_size(E1, E2, E3)
  , ...)
)

// And then all remaining 2 factors products when they're not all constants.
@@
expression E1, E2;
constant C1, C2;
@@

(
  vmalloc(C1 * C2, ...)
|
  vmalloc(
-	E1 * E2
+	array_size(E1, E2)
  , ...)
)

	Signed-off-by: Kees Cook <keescook@chromium.org>
(cherry picked from commit 42bc47b35320e0e587a88e437e18f80f9c5bcbb2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kvm/gaccess.c
#	arch/s390/kvm/kvm-s390.c
#	drivers/base/firmware_loader/fallback.c
#	drivers/lightnvm/pblk-gc.c
#	drivers/media/pci/meye/meye.c
#	drivers/media/pci/pt1/pt1.c
#	drivers/net/wireless/ath/ath5k/debug.c
#	drivers/net/wireless/marvell/mwifiex/cfg80211.c
#	drivers/rapidio/devices/rio_mport_cdev.c
#	drivers/staging/android/ion/ion_heap.c
#	drivers/staging/greybus/camera.c
#	drivers/staging/rts5208/ms.c
#	drivers/staging/rts5208/rtsx_chip.c
#	drivers/video/xen-fbfront.c
#	fs/binfmt_elf.c
#	fs/cifs/misc.c
#	kernel/cgroup/cgroup-v1.c
#	kernel/rcu/rcutorture.c
#	kernel/trace/tracing_map.c
#	mm/percpu-stats.c
diff --cc arch/s390/kvm/kvm-s390.c
index 875286d3fac7,3f6625c64341..000000000000
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@@ -205,6 -685,1083 +205,1086 @@@ static int kvm_vm_ioctl_enable_cap(stru
  	return r;
  }
  
++<<<<<<< HEAD
++=======
+ static int kvm_s390_get_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_MEM_LIMIT_SIZE:
+ 		ret = 0;
+ 		VM_EVENT(kvm, 3, "QUERY: max guest memory: %lu bytes",
+ 			 kvm->arch.mem_limit);
+ 		if (put_user(kvm->arch.mem_limit, (u64 __user *)attr->addr))
+ 			ret = -EFAULT;
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 	unsigned int idx;
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_MEM_ENABLE_CMMA:
+ 		ret = -ENXIO;
+ 		if (!sclp.has_cmma)
+ 			break;
+ 
+ 		ret = -EBUSY;
+ 		VM_EVENT(kvm, 3, "%s", "ENABLE: CMMA support");
+ 		mutex_lock(&kvm->lock);
+ 		if (!kvm->created_vcpus) {
+ 			kvm->arch.use_cmma = 1;
+ 			/* Not compatible with cmma. */
+ 			kvm->arch.use_pfmfi = 0;
+ 			ret = 0;
+ 		}
+ 		mutex_unlock(&kvm->lock);
+ 		break;
+ 	case KVM_S390_VM_MEM_CLR_CMMA:
+ 		ret = -ENXIO;
+ 		if (!sclp.has_cmma)
+ 			break;
+ 		ret = -EINVAL;
+ 		if (!kvm->arch.use_cmma)
+ 			break;
+ 
+ 		VM_EVENT(kvm, 3, "%s", "RESET: CMMA states");
+ 		mutex_lock(&kvm->lock);
+ 		idx = srcu_read_lock(&kvm->srcu);
+ 		s390_reset_cmma(kvm->arch.gmap->mm);
+ 		srcu_read_unlock(&kvm->srcu, idx);
+ 		mutex_unlock(&kvm->lock);
+ 		ret = 0;
+ 		break;
+ 	case KVM_S390_VM_MEM_LIMIT_SIZE: {
+ 		unsigned long new_limit;
+ 
+ 		if (kvm_is_ucontrol(kvm))
+ 			return -EINVAL;
+ 
+ 		if (get_user(new_limit, (u64 __user *)attr->addr))
+ 			return -EFAULT;
+ 
+ 		if (kvm->arch.mem_limit != KVM_S390_NO_MEM_LIMIT &&
+ 		    new_limit > kvm->arch.mem_limit)
+ 			return -E2BIG;
+ 
+ 		if (!new_limit)
+ 			return -EINVAL;
+ 
+ 		/* gmap_create takes last usable address */
+ 		if (new_limit != KVM_S390_NO_MEM_LIMIT)
+ 			new_limit -= 1;
+ 
+ 		ret = -EBUSY;
+ 		mutex_lock(&kvm->lock);
+ 		if (!kvm->created_vcpus) {
+ 			/* gmap_create will round the limit up */
+ 			struct gmap *new = gmap_create(current->mm, new_limit);
+ 
+ 			if (!new) {
+ 				ret = -ENOMEM;
+ 			} else {
+ 				gmap_remove(kvm->arch.gmap);
+ 				new->private = kvm;
+ 				kvm->arch.gmap = new;
+ 				ret = 0;
+ 			}
+ 		}
+ 		mutex_unlock(&kvm->lock);
+ 		VM_EVENT(kvm, 3, "SET: max guest address: %lu", new_limit);
+ 		VM_EVENT(kvm, 3, "New guest asce: 0x%pK",
+ 			 (void *) kvm->arch.gmap->asce);
+ 		break;
+ 	}
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu);
+ 
+ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	int i;
+ 
+ 	if (!test_kvm_facility(kvm, 76))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
+ 		get_random_bytes(
+ 			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
+ 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+ 		kvm->arch.crypto.aes_kw = 1;
+ 		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
+ 		break;
+ 	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
+ 		get_random_bytes(
+ 			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
+ 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
+ 		kvm->arch.crypto.dea_kw = 1;
+ 		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
+ 		break;
+ 	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
+ 		kvm->arch.crypto.aes_kw = 0;
+ 		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
+ 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+ 		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
+ 		break;
+ 	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
+ 		kvm->arch.crypto.dea_kw = 0;
+ 		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
+ 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
+ 		VM_EVENT(kvm, 3, "%s", "DISABLE: DEA keywrapping support");
+ 		break;
+ 	default:
+ 		mutex_unlock(&kvm->lock);
+ 		return -ENXIO;
+ 	}
+ 
+ 	kvm_for_each_vcpu(i, vcpu, kvm) {
+ 		kvm_s390_vcpu_crypto_setup(vcpu);
+ 		exit_sie(vcpu);
+ 	}
+ 	mutex_unlock(&kvm->lock);
+ 	return 0;
+ }
+ 
+ static void kvm_s390_sync_request_broadcast(struct kvm *kvm, int req)
+ {
+ 	int cx;
+ 	struct kvm_vcpu *vcpu;
+ 
+ 	kvm_for_each_vcpu(cx, vcpu, kvm)
+ 		kvm_s390_sync_request(req, vcpu);
+ }
+ 
+ /*
+  * Must be called with kvm->srcu held to avoid races on memslots, and with
+  * kvm->slots_lock to avoid races with ourselves and kvm_s390_vm_stop_migration.
+  */
+ static int kvm_s390_vm_start_migration(struct kvm *kvm)
+ {
+ 	struct kvm_s390_migration_state *mgs;
+ 	struct kvm_memory_slot *ms;
+ 	/* should be the only one */
+ 	struct kvm_memslots *slots;
+ 	unsigned long ram_pages;
+ 	int slotnr;
+ 
+ 	/* migration mode already enabled */
+ 	if (kvm->arch.migration_state)
+ 		return 0;
+ 
+ 	slots = kvm_memslots(kvm);
+ 	if (!slots || !slots->used_slots)
+ 		return -EINVAL;
+ 
+ 	mgs = kzalloc(sizeof(*mgs), GFP_KERNEL);
+ 	if (!mgs)
+ 		return -ENOMEM;
+ 	kvm->arch.migration_state = mgs;
+ 
+ 	if (kvm->arch.use_cmma) {
+ 		/*
+ 		 * Get the first slot. They are reverse sorted by base_gfn, so
+ 		 * the first slot is also the one at the end of the address
+ 		 * space. We have verified above that at least one slot is
+ 		 * present.
+ 		 */
+ 		ms = slots->memslots;
+ 		/* round up so we only use full longs */
+ 		ram_pages = roundup(ms->base_gfn + ms->npages, BITS_PER_LONG);
+ 		/* allocate enough bytes to store all the bits */
+ 		mgs->pgste_bitmap = vmalloc(ram_pages / 8);
+ 		if (!mgs->pgste_bitmap) {
+ 			kfree(mgs);
+ 			kvm->arch.migration_state = NULL;
+ 			return -ENOMEM;
+ 		}
+ 
+ 		mgs->bitmap_size = ram_pages;
+ 		atomic64_set(&mgs->dirty_pages, ram_pages);
+ 		/* mark all the pages in active slots as dirty */
+ 		for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
+ 			ms = slots->memslots + slotnr;
+ 			bitmap_set(mgs->pgste_bitmap, ms->base_gfn, ms->npages);
+ 		}
+ 
+ 		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_START_MIGRATION);
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * Must be called with kvm->slots_lock to avoid races with ourselves and
+  * kvm_s390_vm_start_migration.
+  */
+ static int kvm_s390_vm_stop_migration(struct kvm *kvm)
+ {
+ 	struct kvm_s390_migration_state *mgs;
+ 
+ 	/* migration mode already disabled */
+ 	if (!kvm->arch.migration_state)
+ 		return 0;
+ 	mgs = kvm->arch.migration_state;
+ 	kvm->arch.migration_state = NULL;
+ 
+ 	if (kvm->arch.use_cmma) {
+ 		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_STOP_MIGRATION);
+ 		/* We have to wait for the essa emulation to finish */
+ 		synchronize_srcu(&kvm->srcu);
+ 		vfree(mgs->pgste_bitmap);
+ 	}
+ 	kfree(mgs);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_vm_set_migration(struct kvm *kvm,
+ 				     struct kvm_device_attr *attr)
+ {
+ 	int res = -ENXIO;
+ 
+ 	mutex_lock(&kvm->slots_lock);
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_MIGRATION_START:
+ 		res = kvm_s390_vm_start_migration(kvm);
+ 		break;
+ 	case KVM_S390_VM_MIGRATION_STOP:
+ 		res = kvm_s390_vm_stop_migration(kvm);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	mutex_unlock(&kvm->slots_lock);
+ 
+ 	return res;
+ }
+ 
+ static int kvm_s390_vm_get_migration(struct kvm *kvm,
+ 				     struct kvm_device_attr *attr)
+ {
+ 	u64 mig = (kvm->arch.migration_state != NULL);
+ 
+ 	if (attr->attr != KVM_S390_VM_MIGRATION_STATUS)
+ 		return -ENXIO;
+ 
+ 	if (copy_to_user((void __user *)attr->addr, &mig, sizeof(mig)))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ static int kvm_s390_set_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_tod_clock gtod;
+ 
+ 	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
+ 		return -EFAULT;
+ 
+ 	if (!test_kvm_facility(kvm, 139) && gtod.epoch_idx)
+ 		return -EINVAL;
+ 	kvm_s390_set_tod_clock(kvm, &gtod);
+ 
+ 	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x, TOD base: 0x%llx",
+ 		gtod.epoch_idx, gtod.tod);
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	u8 gtod_high;
+ 
+ 	if (copy_from_user(&gtod_high, (void __user *)attr->addr,
+ 					   sizeof(gtod_high)))
+ 		return -EFAULT;
+ 
+ 	if (gtod_high != 0)
+ 		return -EINVAL;
+ 	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x", gtod_high);
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_tod_clock gtod = { 0 };
+ 
+ 	if (copy_from_user(&gtod.tod, (void __user *)attr->addr,
+ 			   sizeof(gtod.tod)))
+ 		return -EFAULT;
+ 
+ 	kvm_s390_set_tod_clock(kvm, &gtod);
+ 	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx", gtod.tod);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	if (attr->flags)
+ 		return -EINVAL;
+ 
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_TOD_EXT:
+ 		ret = kvm_s390_set_tod_ext(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD_HIGH:
+ 		ret = kvm_s390_set_tod_high(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD_LOW:
+ 		ret = kvm_s390_set_tod_low(kvm, attr);
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static void kvm_s390_get_tod_clock_ext(struct kvm *kvm,
+ 					struct kvm_s390_vm_tod_clock *gtod)
+ {
+ 	struct kvm_s390_tod_clock_ext htod;
+ 
+ 	preempt_disable();
+ 
+ 	get_tod_clock_ext((char *)&htod);
+ 
+ 	gtod->tod = htod.tod + kvm->arch.epoch;
+ 	gtod->epoch_idx = htod.epoch_idx + kvm->arch.epdx;
+ 
+ 	if (gtod->tod < htod.tod)
+ 		gtod->epoch_idx += 1;
+ 
+ 	preempt_enable();
+ }
+ 
+ static int kvm_s390_get_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_tod_clock gtod;
+ 
+ 	memset(&gtod, 0, sizeof(gtod));
+ 
+ 	if (test_kvm_facility(kvm, 139))
+ 		kvm_s390_get_tod_clock_ext(kvm, &gtod);
+ 	else
+ 		gtod.tod = kvm_s390_get_tod_clock_fast(kvm);
+ 
+ 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
+ 		return -EFAULT;
+ 
+ 	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x, TOD base: 0x%llx",
+ 		gtod.epoch_idx, gtod.tod);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	u8 gtod_high = 0;
+ 
+ 	if (copy_to_user((void __user *)attr->addr, &gtod_high,
+ 					 sizeof(gtod_high)))
+ 		return -EFAULT;
+ 	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x", gtod_high);
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	u64 gtod;
+ 
+ 	gtod = kvm_s390_get_tod_clock_fast(kvm);
+ 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
+ 		return -EFAULT;
+ 	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx", gtod);
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	if (attr->flags)
+ 		return -EINVAL;
+ 
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_TOD_EXT:
+ 		ret = kvm_s390_get_tod_ext(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD_HIGH:
+ 		ret = kvm_s390_get_tod_high(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD_LOW:
+ 		ret = kvm_s390_get_tod_low(kvm, attr);
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_processor *proc;
+ 	u16 lowest_ibc, unblocked_ibc;
+ 	int ret = 0;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	if (kvm->created_vcpus) {
+ 		ret = -EBUSY;
+ 		goto out;
+ 	}
+ 	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
+ 	if (!proc) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 	if (!copy_from_user(proc, (void __user *)attr->addr,
+ 			    sizeof(*proc))) {
+ 		kvm->arch.model.cpuid = proc->cpuid;
+ 		lowest_ibc = sclp.ibc >> 16 & 0xfff;
+ 		unblocked_ibc = sclp.ibc & 0xfff;
+ 		if (lowest_ibc && proc->ibc) {
+ 			if (proc->ibc > unblocked_ibc)
+ 				kvm->arch.model.ibc = unblocked_ibc;
+ 			else if (proc->ibc < lowest_ibc)
+ 				kvm->arch.model.ibc = lowest_ibc;
+ 			else
+ 				kvm->arch.model.ibc = proc->ibc;
+ 		}
+ 		memcpy(kvm->arch.model.fac_list, proc->fac_list,
+ 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
+ 		VM_EVENT(kvm, 3, "SET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
+ 			 kvm->arch.model.ibc,
+ 			 kvm->arch.model.cpuid);
+ 		VM_EVENT(kvm, 3, "SET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
+ 			 kvm->arch.model.fac_list[0],
+ 			 kvm->arch.model.fac_list[1],
+ 			 kvm->arch.model.fac_list[2]);
+ 	} else
+ 		ret = -EFAULT;
+ 	kfree(proc);
+ out:
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ static int kvm_s390_set_processor_feat(struct kvm *kvm,
+ 				       struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_feat data;
+ 
+ 	if (copy_from_user(&data, (void __user *)attr->addr, sizeof(data)))
+ 		return -EFAULT;
+ 	if (!bitmap_subset((unsigned long *) data.feat,
+ 			   kvm_s390_available_cpu_feat,
+ 			   KVM_S390_VM_CPU_FEAT_NR_BITS))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	if (kvm->created_vcpus) {
+ 		mutex_unlock(&kvm->lock);
+ 		return -EBUSY;
+ 	}
+ 	bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
+ 		    KVM_S390_VM_CPU_FEAT_NR_BITS);
+ 	mutex_unlock(&kvm->lock);
+ 	VM_EVENT(kvm, 3, "SET: guest feat: 0x%16.16llx.0x%16.16llx.0x%16.16llx",
+ 			 data.feat[0],
+ 			 data.feat[1],
+ 			 data.feat[2]);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
+ 					  struct kvm_device_attr *attr)
+ {
+ 	/*
+ 	 * Once supported by kernel + hw, we have to store the subfunctions
+ 	 * in kvm->arch and remember that user space configured them.
+ 	 */
+ 	return -ENXIO;
+ }
+ 
+ static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret = -ENXIO;
+ 
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_CPU_PROCESSOR:
+ 		ret = kvm_s390_set_processor(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+ 		ret = kvm_s390_set_processor_feat(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
+ 		ret = kvm_s390_set_processor_subfunc(kvm, attr);
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_processor *proc;
+ 	int ret = 0;
+ 
+ 	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
+ 	if (!proc) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 	proc->cpuid = kvm->arch.model.cpuid;
+ 	proc->ibc = kvm->arch.model.ibc;
+ 	memcpy(&proc->fac_list, kvm->arch.model.fac_list,
+ 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+ 	VM_EVENT(kvm, 3, "GET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
+ 		 kvm->arch.model.ibc,
+ 		 kvm->arch.model.cpuid);
+ 	VM_EVENT(kvm, 3, "GET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
+ 		 kvm->arch.model.fac_list[0],
+ 		 kvm->arch.model.fac_list[1],
+ 		 kvm->arch.model.fac_list[2]);
+ 	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
+ 		ret = -EFAULT;
+ 	kfree(proc);
+ out:
+ 	return ret;
+ }
+ 
+ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_machine *mach;
+ 	int ret = 0;
+ 
+ 	mach = kzalloc(sizeof(*mach), GFP_KERNEL);
+ 	if (!mach) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 	get_cpu_id((struct cpuid *) &mach->cpuid);
+ 	mach->ibc = sclp.ibc;
+ 	memcpy(&mach->fac_mask, kvm->arch.model.fac_mask,
+ 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+ 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
+ 	       sizeof(S390_lowcore.stfle_fac_list));
+ 	VM_EVENT(kvm, 3, "GET: host ibc:  0x%4.4x, host cpuid:  0x%16.16llx",
+ 		 kvm->arch.model.ibc,
+ 		 kvm->arch.model.cpuid);
+ 	VM_EVENT(kvm, 3, "GET: host facmask:  0x%16.16llx.%16.16llx.%16.16llx",
+ 		 mach->fac_mask[0],
+ 		 mach->fac_mask[1],
+ 		 mach->fac_mask[2]);
+ 	VM_EVENT(kvm, 3, "GET: host faclist:  0x%16.16llx.%16.16llx.%16.16llx",
+ 		 mach->fac_list[0],
+ 		 mach->fac_list[1],
+ 		 mach->fac_list[2]);
+ 	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
+ 		ret = -EFAULT;
+ 	kfree(mach);
+ out:
+ 	return ret;
+ }
+ 
+ static int kvm_s390_get_processor_feat(struct kvm *kvm,
+ 				       struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_feat data;
+ 
+ 	bitmap_copy((unsigned long *) data.feat, kvm->arch.cpu_feat,
+ 		    KVM_S390_VM_CPU_FEAT_NR_BITS);
+ 	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
+ 		return -EFAULT;
+ 	VM_EVENT(kvm, 3, "GET: guest feat: 0x%16.16llx.0x%16.16llx.0x%16.16llx",
+ 			 data.feat[0],
+ 			 data.feat[1],
+ 			 data.feat[2]);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_get_machine_feat(struct kvm *kvm,
+ 				     struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_vm_cpu_feat data;
+ 
+ 	bitmap_copy((unsigned long *) data.feat,
+ 		    kvm_s390_available_cpu_feat,
+ 		    KVM_S390_VM_CPU_FEAT_NR_BITS);
+ 	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
+ 		return -EFAULT;
+ 	VM_EVENT(kvm, 3, "GET: host feat:  0x%16.16llx.0x%16.16llx.0x%16.16llx",
+ 			 data.feat[0],
+ 			 data.feat[1],
+ 			 data.feat[2]);
+ 	return 0;
+ }
+ 
+ static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
+ 					  struct kvm_device_attr *attr)
+ {
+ 	/*
+ 	 * Once we can actually configure subfunctions (kernel + hw support),
+ 	 * we have to check if they were already set by user space, if so copy
+ 	 * them from kvm->arch.
+ 	 */
+ 	return -ENXIO;
+ }
+ 
+ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
+ 					struct kvm_device_attr *attr)
+ {
+ 	if (copy_to_user((void __user *)attr->addr, &kvm_s390_available_subfunc,
+ 	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret = -ENXIO;
+ 
+ 	switch (attr->attr) {
+ 	case KVM_S390_VM_CPU_PROCESSOR:
+ 		ret = kvm_s390_get_processor(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_MACHINE:
+ 		ret = kvm_s390_get_machine(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+ 		ret = kvm_s390_get_processor_feat(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_MACHINE_FEAT:
+ 		ret = kvm_s390_get_machine_feat(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
+ 		ret = kvm_s390_get_processor_subfunc(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
+ 		ret = kvm_s390_get_machine_subfunc(kvm, attr);
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	switch (attr->group) {
+ 	case KVM_S390_VM_MEM_CTRL:
+ 		ret = kvm_s390_set_mem_control(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD:
+ 		ret = kvm_s390_set_tod(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_MODEL:
+ 		ret = kvm_s390_set_cpu_model(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CRYPTO:
+ 		ret = kvm_s390_vm_set_crypto(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_MIGRATION:
+ 		ret = kvm_s390_vm_set_migration(kvm, attr);
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	switch (attr->group) {
+ 	case KVM_S390_VM_MEM_CTRL:
+ 		ret = kvm_s390_get_mem_control(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_TOD:
+ 		ret = kvm_s390_get_tod(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_CPU_MODEL:
+ 		ret = kvm_s390_get_cpu_model(kvm, attr);
+ 		break;
+ 	case KVM_S390_VM_MIGRATION:
+ 		ret = kvm_s390_vm_get_migration(kvm, attr);
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+ {
+ 	int ret;
+ 
+ 	switch (attr->group) {
+ 	case KVM_S390_VM_MEM_CTRL:
+ 		switch (attr->attr) {
+ 		case KVM_S390_VM_MEM_ENABLE_CMMA:
+ 		case KVM_S390_VM_MEM_CLR_CMMA:
+ 			ret = sclp.has_cmma ? 0 : -ENXIO;
+ 			break;
+ 		case KVM_S390_VM_MEM_LIMIT_SIZE:
+ 			ret = 0;
+ 			break;
+ 		default:
+ 			ret = -ENXIO;
+ 			break;
+ 		}
+ 		break;
+ 	case KVM_S390_VM_TOD:
+ 		switch (attr->attr) {
+ 		case KVM_S390_VM_TOD_LOW:
+ 		case KVM_S390_VM_TOD_HIGH:
+ 			ret = 0;
+ 			break;
+ 		default:
+ 			ret = -ENXIO;
+ 			break;
+ 		}
+ 		break;
+ 	case KVM_S390_VM_CPU_MODEL:
+ 		switch (attr->attr) {
+ 		case KVM_S390_VM_CPU_PROCESSOR:
+ 		case KVM_S390_VM_CPU_MACHINE:
+ 		case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+ 		case KVM_S390_VM_CPU_MACHINE_FEAT:
+ 		case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
+ 			ret = 0;
+ 			break;
+ 		/* configuring subfunctions is not supported yet */
+ 		case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
+ 		default:
+ 			ret = -ENXIO;
+ 			break;
+ 		}
+ 		break;
+ 	case KVM_S390_VM_CRYPTO:
+ 		switch (attr->attr) {
+ 		case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
+ 		case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
+ 		case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
+ 		case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
+ 			ret = 0;
+ 			break;
+ 		default:
+ 			ret = -ENXIO;
+ 			break;
+ 		}
+ 		break;
+ 	case KVM_S390_VM_MIGRATION:
+ 		ret = 0;
+ 		break;
+ 	default:
+ 		ret = -ENXIO;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
+ {
+ 	uint8_t *keys;
+ 	uint64_t hva;
+ 	int srcu_idx, i, r = 0;
+ 
+ 	if (args->flags != 0)
+ 		return -EINVAL;
+ 
+ 	/* Is this guest using storage keys? */
+ 	if (!mm_use_skey(current->mm))
+ 		return KVM_S390_GET_SKEYS_NONE;
+ 
+ 	/* Enforce sane limit on memory allocation */
+ 	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
+ 		return -EINVAL;
+ 
+ 	keys = kvmalloc_array(args->count, sizeof(uint8_t), GFP_KERNEL);
+ 	if (!keys)
+ 		return -ENOMEM;
+ 
+ 	down_read(&current->mm->mmap_sem);
+ 	srcu_idx = srcu_read_lock(&kvm->srcu);
+ 	for (i = 0; i < args->count; i++) {
+ 		hva = gfn_to_hva(kvm, args->start_gfn + i);
+ 		if (kvm_is_error_hva(hva)) {
+ 			r = -EFAULT;
+ 			break;
+ 		}
+ 
+ 		r = get_guest_storage_key(current->mm, hva, &keys[i]);
+ 		if (r)
+ 			break;
+ 	}
+ 	srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	up_read(&current->mm->mmap_sem);
+ 
+ 	if (!r) {
+ 		r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
+ 				 sizeof(uint8_t) * args->count);
+ 		if (r)
+ 			r = -EFAULT;
+ 	}
+ 
+ 	kvfree(keys);
+ 	return r;
+ }
+ 
+ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
+ {
+ 	uint8_t *keys;
+ 	uint64_t hva;
+ 	int srcu_idx, i, r = 0;
+ 
+ 	if (args->flags != 0)
+ 		return -EINVAL;
+ 
+ 	/* Enforce sane limit on memory allocation */
+ 	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
+ 		return -EINVAL;
+ 
+ 	keys = kvmalloc_array(args->count, sizeof(uint8_t), GFP_KERNEL);
+ 	if (!keys)
+ 		return -ENOMEM;
+ 
+ 	r = copy_from_user(keys, (uint8_t __user *)args->skeydata_addr,
+ 			   sizeof(uint8_t) * args->count);
+ 	if (r) {
+ 		r = -EFAULT;
+ 		goto out;
+ 	}
+ 
+ 	/* Enable storage key handling for the guest */
+ 	r = s390_enable_skey();
+ 	if (r)
+ 		goto out;
+ 
+ 	down_read(&current->mm->mmap_sem);
+ 	srcu_idx = srcu_read_lock(&kvm->srcu);
+ 	for (i = 0; i < args->count; i++) {
+ 		hva = gfn_to_hva(kvm, args->start_gfn + i);
+ 		if (kvm_is_error_hva(hva)) {
+ 			r = -EFAULT;
+ 			break;
+ 		}
+ 
+ 		/* Lowest order bit is reserved */
+ 		if (keys[i] & 0x01) {
+ 			r = -EINVAL;
+ 			break;
+ 		}
+ 
+ 		r = set_guest_storage_key(current->mm, hva, keys[i], 0);
+ 		if (r)
+ 			break;
+ 	}
+ 	srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	up_read(&current->mm->mmap_sem);
+ out:
+ 	kvfree(keys);
+ 	return r;
+ }
+ 
+ /*
+  * Base address and length must be sent at the start of each block, therefore
+  * it's cheaper to send some clean data, as long as it's less than the size of
+  * two longs.
+  */
+ #define KVM_S390_MAX_BIT_DISTANCE (2 * sizeof(void *))
+ /* for consistency */
+ #define KVM_S390_CMMA_SIZE_MAX ((u32)KVM_S390_SKEYS_MAX)
+ 
+ /*
+  * This function searches for the next page with dirty CMMA attributes, and
+  * saves the attributes in the buffer up to either the end of the buffer or
+  * until a block of at least KVM_S390_MAX_BIT_DISTANCE clean bits is found;
+  * no trailing clean bytes are saved.
+  * In case no dirty bits were found, or if CMMA was not enabled or used, the
+  * output buffer will indicate 0 as length.
+  */
+ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
+ 				  struct kvm_s390_cmma_log *args)
+ {
+ 	struct kvm_s390_migration_state *s = kvm->arch.migration_state;
+ 	unsigned long bufsize, hva, pgstev, i, next, cur;
+ 	int srcu_idx, peek, r = 0, rr;
+ 	u8 *res;
+ 
+ 	cur = args->start_gfn;
+ 	i = next = pgstev = 0;
+ 
+ 	if (unlikely(!kvm->arch.use_cmma))
+ 		return -ENXIO;
+ 	/* Invalid/unsupported flags were specified */
+ 	if (args->flags & ~KVM_S390_CMMA_PEEK)
+ 		return -EINVAL;
+ 	/* Migration mode query, and we are not doing a migration */
+ 	peek = !!(args->flags & KVM_S390_CMMA_PEEK);
+ 	if (!peek && !s)
+ 		return -EINVAL;
+ 	/* CMMA is disabled or was not used, or the buffer has length zero */
+ 	bufsize = min(args->count, KVM_S390_CMMA_SIZE_MAX);
+ 	if (!bufsize || !kvm->mm->context.uses_cmm) {
+ 		memset(args, 0, sizeof(*args));
+ 		return 0;
+ 	}
+ 
+ 	if (!peek) {
+ 		/* We are not peeking, and there are no dirty pages */
+ 		if (!atomic64_read(&s->dirty_pages)) {
+ 			memset(args, 0, sizeof(*args));
+ 			return 0;
+ 		}
+ 		cur = find_next_bit(s->pgste_bitmap, s->bitmap_size,
+ 				    args->start_gfn);
+ 		if (cur >= s->bitmap_size)	/* nothing found, loop back */
+ 			cur = find_next_bit(s->pgste_bitmap, s->bitmap_size, 0);
+ 		if (cur >= s->bitmap_size) {	/* again! (very unlikely) */
+ 			memset(args, 0, sizeof(*args));
+ 			return 0;
+ 		}
+ 		next = find_next_bit(s->pgste_bitmap, s->bitmap_size, cur + 1);
+ 	}
+ 
+ 	res = vmalloc(bufsize);
+ 	if (!res)
+ 		return -ENOMEM;
+ 
+ 	args->start_gfn = cur;
+ 
+ 	down_read(&kvm->mm->mmap_sem);
+ 	srcu_idx = srcu_read_lock(&kvm->srcu);
+ 	while (i < bufsize) {
+ 		hva = gfn_to_hva(kvm, cur);
+ 		if (kvm_is_error_hva(hva)) {
+ 			r = -EFAULT;
+ 			break;
+ 		}
+ 		/* decrement only if we actually flipped the bit to 0 */
+ 		if (!peek && test_and_clear_bit(cur, s->pgste_bitmap))
+ 			atomic64_dec(&s->dirty_pages);
+ 		r = get_pgste(kvm->mm, hva, &pgstev);
+ 		if (r < 0)
+ 			pgstev = 0;
+ 		/* save the value */
+ 		res[i++] = (pgstev >> 24) & 0x43;
+ 		/*
+ 		 * if the next bit is too far away, stop.
+ 		 * if we reached the previous "next", find the next one
+ 		 */
+ 		if (!peek) {
+ 			if (next > cur + KVM_S390_MAX_BIT_DISTANCE)
+ 				break;
+ 			if (cur == next)
+ 				next = find_next_bit(s->pgste_bitmap,
+ 						     s->bitmap_size, cur + 1);
+ 		/* reached the end of the bitmap or of the buffer, stop */
+ 			if ((next >= s->bitmap_size) ||
+ 			    (next >= args->start_gfn + bufsize))
+ 				break;
+ 		}
+ 		cur++;
+ 	}
+ 	srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	up_read(&kvm->mm->mmap_sem);
+ 	args->count = i;
+ 	args->remaining = s ? atomic64_read(&s->dirty_pages) : 0;
+ 
+ 	rr = copy_to_user((void __user *)args->values, res, args->count);
+ 	if (rr)
+ 		r = -EFAULT;
+ 
+ 	vfree(res);
+ 	return r;
+ }
+ 
+ /*
+  * This function sets the CMMA attributes for the given pages. If the input
+  * buffer has zero length, no action is taken, otherwise the attributes are
+  * set and the mm->context.uses_cmm flag is set.
+  */
+ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
+ 				  const struct kvm_s390_cmma_log *args)
+ {
+ 	unsigned long hva, mask, pgstev, i;
+ 	uint8_t *bits;
+ 	int srcu_idx, r = 0;
+ 
+ 	mask = args->mask;
+ 
+ 	if (!kvm->arch.use_cmma)
+ 		return -ENXIO;
+ 	/* invalid/unsupported flags */
+ 	if (args->flags != 0)
+ 		return -EINVAL;
+ 	/* Enforce sane limit on memory allocation */
+ 	if (args->count > KVM_S390_CMMA_SIZE_MAX)
+ 		return -EINVAL;
+ 	/* Nothing to do */
+ 	if (args->count == 0)
+ 		return 0;
+ 
+ 	bits = vmalloc(array_size(sizeof(*bits), args->count));
+ 	if (!bits)
+ 		return -ENOMEM;
+ 
+ 	r = copy_from_user(bits, (void __user *)args->values, args->count);
+ 	if (r) {
+ 		r = -EFAULT;
+ 		goto out;
+ 	}
+ 
+ 	down_read(&kvm->mm->mmap_sem);
+ 	srcu_idx = srcu_read_lock(&kvm->srcu);
+ 	for (i = 0; i < args->count; i++) {
+ 		hva = gfn_to_hva(kvm, args->start_gfn + i);
+ 		if (kvm_is_error_hva(hva)) {
+ 			r = -EFAULT;
+ 			break;
+ 		}
+ 
+ 		pgstev = bits[i];
+ 		pgstev = pgstev << 24;
+ 		mask &= _PGSTE_GPS_USAGE_MASK | _PGSTE_GPS_NODAT;
+ 		set_pgste_bits(kvm->mm, hva, mask, pgstev);
+ 	}
+ 	srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	up_read(&kvm->mm->mmap_sem);
+ 
+ 	if (!kvm->mm->context.uses_cmm) {
+ 		down_write(&kvm->mm->mmap_sem);
+ 		kvm->mm->context.uses_cmm = 1;
+ 		up_write(&kvm->mm->mmap_sem);
+ 	}
+ out:
+ 	vfree(bits);
+ 	return r;
+ }
+ 
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  long kvm_arch_vm_ioctl(struct file *filp,
  		       unsigned int ioctl, unsigned long arg)
  {
diff --cc drivers/media/pci/meye/meye.c
index 2381b05432e6,8001d3e9134e..000000000000
--- a/drivers/media/pci/meye/meye.c
+++ b/drivers/media/pci/meye/meye.c
@@@ -1638,17 -1624,10 +1638,22 @@@ static int meye_probe(struct pci_dev *p
  	}
  	ret = -ENOMEM;
  	meye.mchip_dev = pcidev;
 +	meye.vdev = video_device_alloc();
 +	if (!meye.vdev) {
 +		v4l2_err(v4l2_dev, "video_device_alloc() failed!\n");
 +		goto outnotdev;
 +	}
  
++<<<<<<< HEAD
 +	meye.grab_temp = vmalloc(MCHIP_NB_PAGES_MJPEG * PAGE_SIZE);
 +	if (!meye.grab_temp) {
 +		v4l2_err(v4l2_dev, "grab buffer allocation failed\n");
++=======
+ 	meye.grab_temp = vmalloc(array_size(PAGE_SIZE, MCHIP_NB_PAGES_MJPEG));
+ 	if (!meye.grab_temp)
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  		goto outvmalloc;
 +	}
  
  	spin_lock_init(&meye.grabq_lock);
  	if (kfifo_alloc(&meye.grabq, sizeof(int) * MEYE_MAX_BUFNBRS,
diff --cc drivers/media/pci/pt1/pt1.c
index e9211086df49,fda969a85684..000000000000
--- a/drivers/media/pci/pt1/pt1.c
+++ b/drivers/media/pci/pt1/pt1.c
@@@ -450,7 -612,10 +450,14 @@@ static int pt1_init_tables(struct pt1 *
  	int i, ret;
  	u32 first_pfn, pfn;
  
++<<<<<<< HEAD
 +	tables = vmalloc(sizeof(struct pt1_table) * pt1_nr_tables);
++=======
+ 	if (!pt1_nr_tables)
+ 		return 0;
+ 
+ 	tables = vmalloc(array_size(pt1_nr_tables, sizeof(struct pt1_table)));
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  	if (tables == NULL)
  		return -ENOMEM;
  
diff --cc drivers/net/wireless/ath/ath5k/debug.c
index b8d031ae63c2,e01faf641288..000000000000
--- a/drivers/net/wireless/ath/ath5k/debug.c
+++ b/drivers/net/wireless/ath/ath5k/debug.c
@@@ -894,6 -895,103 +894,106 @@@ static const struct file_operations fop
  	.llseek = default_llseek,
  };
  
++<<<<<<< HEAD
++=======
+ /* debugfs: eeprom */
+ 
+ struct eeprom_private {
+ 	u16 *buf;
+ 	int len;
+ };
+ 
+ static int open_file_eeprom(struct inode *inode, struct file *file)
+ {
+ 	struct eeprom_private *ep;
+ 	struct ath5k_hw *ah = inode->i_private;
+ 	bool res;
+ 	int i, ret;
+ 	u32 eesize;	/* NB: in 16-bit words */
+ 	u16 val, *buf;
+ 
+ 	/* Get eeprom size */
+ 
+ 	res = ath5k_hw_nvram_read(ah, AR5K_EEPROM_SIZE_UPPER, &val);
+ 	if (!res)
+ 		return -EACCES;
+ 
+ 	if (val == 0) {
+ 		eesize = AR5K_EEPROM_INFO_MAX + AR5K_EEPROM_INFO_BASE;
+ 	} else {
+ 		eesize = (val & AR5K_EEPROM_SIZE_UPPER_MASK) <<
+ 			AR5K_EEPROM_SIZE_ENDLOC_SHIFT;
+ 		ath5k_hw_nvram_read(ah, AR5K_EEPROM_SIZE_LOWER, &val);
+ 		eesize = eesize | val;
+ 	}
+ 
+ 	if (eesize > 4096)
+ 		return -EINVAL;
+ 
+ 	/* Create buffer and read in eeprom */
+ 
+ 	buf = vmalloc(array_size(eesize, 2));
+ 	if (!buf) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	for (i = 0; i < eesize; ++i) {
+ 		if (!ath5k_hw_nvram_read(ah, i, &val)) {
+ 			ret = -EIO;
+ 			goto freebuf;
+ 		}
+ 		buf[i] = val;
+ 	}
+ 
+ 	/* Create private struct and assign to file */
+ 
+ 	ep = kmalloc(sizeof(*ep), GFP_KERNEL);
+ 	if (!ep) {
+ 		ret = -ENOMEM;
+ 		goto freebuf;
+ 	}
+ 
+ 	ep->buf = buf;
+ 	ep->len = eesize * 2;
+ 
+ 	file->private_data = (void *)ep;
+ 
+ 	return 0;
+ 
+ freebuf:
+ 	vfree(buf);
+ err:
+ 	return ret;
+ 
+ }
+ 
+ static ssize_t read_file_eeprom(struct file *file, char __user *user_buf,
+ 				   size_t count, loff_t *ppos)
+ {
+ 	struct eeprom_private *ep = file->private_data;
+ 
+ 	return simple_read_from_buffer(user_buf, count, ppos, ep->buf, ep->len);
+ }
+ 
+ static int release_file_eeprom(struct inode *inode, struct file *file)
+ {
+ 	struct eeprom_private *ep = file->private_data;
+ 
+ 	vfree(ep->buf);
+ 	kfree(ep);
+ 
+ 	return 0;
+ }
+ 
+ static const struct file_operations fops_eeprom = {
+ 	.open = open_file_eeprom,
+ 	.read = read_file_eeprom,
+ 	.release = release_file_eeprom,
+ 	.owner = THIS_MODULE,
+ };
+ 
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  
  void
  ath5k_debug_init_device(struct ath5k_hw *ah)
diff --cc drivers/net/wireless/marvell/mwifiex/cfg80211.c
index 699e6ffdc68a,4b5ae9098504..000000000000
--- a/drivers/net/wireless/marvell/mwifiex/cfg80211.c
+++ b/drivers/net/wireless/marvell/mwifiex/cfg80211.c
@@@ -4206,9 -4238,12 +4206,18 @@@ int mwifiex_init_channel_scan_gap(struc
  	if (adapter->config_bands & BAND_A)
  		n_channels_a = mwifiex_band_5ghz.n_channels;
  
++<<<<<<< HEAD
 +	adapter->num_in_chan_stats = n_channels_bg + n_channels_a;
 +	adapter->chan_stats = vmalloc(sizeof(*adapter->chan_stats) *
 +				      adapter->num_in_chan_stats);
++=======
+ 	/* allocate twice the number total channels, since the driver issues an
+ 	 * additional active scan request for hidden SSIDs on passive channels.
+ 	 */
+ 	adapter->num_in_chan_stats = 2 * (n_channels_bg + n_channels_a);
+ 	adapter->chan_stats = vmalloc(array_size(sizeof(*adapter->chan_stats),
+ 						 adapter->num_in_chan_stats));
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  
  	if (!adapter->chan_stats)
  		return -ENOMEM;
diff --cc drivers/video/xen-fbfront.c
index 4b2d3ab870f3,6a4bbc9e1fb0..000000000000
--- a/drivers/video/xen-fbfront.c
+++ b/drivers/video/xen-fbfront.c
@@@ -402,8 -412,8 +402,13 @@@ static int xenfb_probe(struct xenbus_de
  
  	info->nr_pages = (fb_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
  
++<<<<<<< HEAD:drivers/video/xen-fbfront.c
 +	info->mfns = vmalloc(sizeof(unsigned long) * info->nr_pages);
 +	if (!info->mfns)
++=======
+ 	info->gfns = vmalloc(array_size(sizeof(unsigned long), info->nr_pages));
+ 	if (!info->gfns)
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc()):drivers/video/fbdev/xen-fbfront.c
  		goto error_nomem;
  
  	/* set up shared page */
diff --cc fs/binfmt_elf.c
index 97d77c0438c2,070b6184642d..000000000000
--- a/fs/binfmt_elf.c
+++ b/fs/binfmt_elf.c
@@@ -2167,7 -2292,22 +2167,26 @@@ static int elf_core_dump(struct coredum
  
  	dataoff = offset = roundup(offset, ELF_EXEC_PAGESIZE);
  
++<<<<<<< HEAD
 +	offset += elf_core_vma_data_size(gate_vma, cprm->mm_flags);
++=======
+ 	if (segs - 1 > ULONG_MAX / sizeof(*vma_filesz))
+ 		goto end_coredump;
+ 	vma_filesz = vmalloc(array_size(sizeof(*vma_filesz), (segs - 1)));
+ 	if (!vma_filesz)
+ 		goto end_coredump;
+ 
+ 	for (i = 0, vma = first_vma(current, gate_vma); vma != NULL;
+ 			vma = next_vma(vma, gate_vma)) {
+ 		unsigned long dump_size;
+ 
+ 		dump_size = vma_dump_size(vma, cprm->mm_flags);
+ 		vma_filesz[i++] = dump_size;
+ 		vma_data_size += dump_size;
+ 	}
+ 
+ 	offset += vma_data_size;
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
  	offset += elf_core_extra_data_size();
  	e_shoff = offset;
  
diff --cc fs/cifs/misc.c
index b168bf05b740,af29ade195c0..000000000000
--- a/fs/cifs/misc.c
+++ b/fs/cifs/misc.c
@@@ -734,3 -732,193 +734,196 @@@ parse_DFS_referrals_exit
  	}
  	return rc;
  }
++<<<<<<< HEAD
++=======
+ 
+ struct cifs_aio_ctx *
+ cifs_aio_ctx_alloc(void)
+ {
+ 	struct cifs_aio_ctx *ctx;
+ 
+ 	ctx = kzalloc(sizeof(struct cifs_aio_ctx), GFP_KERNEL);
+ 	if (!ctx)
+ 		return NULL;
+ 
+ 	INIT_LIST_HEAD(&ctx->list);
+ 	mutex_init(&ctx->aio_mutex);
+ 	init_completion(&ctx->done);
+ 	kref_init(&ctx->refcount);
+ 	return ctx;
+ }
+ 
+ void
+ cifs_aio_ctx_release(struct kref *refcount)
+ {
+ 	struct cifs_aio_ctx *ctx = container_of(refcount,
+ 					struct cifs_aio_ctx, refcount);
+ 
+ 	cifsFileInfo_put(ctx->cfile);
+ 	kvfree(ctx->bv);
+ 	kfree(ctx);
+ }
+ 
+ #define CIFS_AIO_KMALLOC_LIMIT (1024 * 1024)
+ 
+ int
+ setup_aio_ctx_iter(struct cifs_aio_ctx *ctx, struct iov_iter *iter, int rw)
+ {
+ 	ssize_t rc;
+ 	unsigned int cur_npages;
+ 	unsigned int npages = 0;
+ 	unsigned int i;
+ 	size_t len;
+ 	size_t count = iov_iter_count(iter);
+ 	unsigned int saved_len;
+ 	size_t start;
+ 	unsigned int max_pages = iov_iter_npages(iter, INT_MAX);
+ 	struct page **pages = NULL;
+ 	struct bio_vec *bv = NULL;
+ 
+ 	if (iter->type & ITER_KVEC) {
+ 		memcpy(&ctx->iter, iter, sizeof(struct iov_iter));
+ 		ctx->len = count;
+ 		iov_iter_advance(iter, count);
+ 		return 0;
+ 	}
+ 
+ 	if (max_pages * sizeof(struct bio_vec) <= CIFS_AIO_KMALLOC_LIMIT)
+ 		bv = kmalloc_array(max_pages, sizeof(struct bio_vec),
+ 				   GFP_KERNEL);
+ 
+ 	if (!bv) {
+ 		bv = vmalloc(array_size(max_pages, sizeof(struct bio_vec)));
+ 		if (!bv)
+ 			return -ENOMEM;
+ 	}
+ 
+ 	if (max_pages * sizeof(struct page *) <= CIFS_AIO_KMALLOC_LIMIT)
+ 		pages = kmalloc_array(max_pages, sizeof(struct page *),
+ 				      GFP_KERNEL);
+ 
+ 	if (!pages) {
+ 		pages = vmalloc(array_size(max_pages, sizeof(struct page *)));
+ 		if (!pages) {
+ 			kvfree(bv);
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
+ 	saved_len = count;
+ 
+ 	while (count && npages < max_pages) {
+ 		rc = iov_iter_get_pages(iter, pages, count, max_pages, &start);
+ 		if (rc < 0) {
+ 			cifs_dbg(VFS, "couldn't get user pages (rc=%zd)\n", rc);
+ 			break;
+ 		}
+ 
+ 		if (rc > count) {
+ 			cifs_dbg(VFS, "get pages rc=%zd more than %zu\n", rc,
+ 				 count);
+ 			break;
+ 		}
+ 
+ 		iov_iter_advance(iter, rc);
+ 		count -= rc;
+ 		rc += start;
+ 		cur_npages = DIV_ROUND_UP(rc, PAGE_SIZE);
+ 
+ 		if (npages + cur_npages > max_pages) {
+ 			cifs_dbg(VFS, "out of vec array capacity (%u vs %u)\n",
+ 				 npages + cur_npages, max_pages);
+ 			break;
+ 		}
+ 
+ 		for (i = 0; i < cur_npages; i++) {
+ 			len = rc > PAGE_SIZE ? PAGE_SIZE : rc;
+ 			bv[npages + i].bv_page = pages[i];
+ 			bv[npages + i].bv_offset = start;
+ 			bv[npages + i].bv_len = len - start;
+ 			rc -= len;
+ 			start = 0;
+ 		}
+ 
+ 		npages += cur_npages;
+ 	}
+ 
+ 	kvfree(pages);
+ 	ctx->bv = bv;
+ 	ctx->len = saved_len - count;
+ 	ctx->npages = npages;
+ 	iov_iter_bvec(&ctx->iter, ITER_BVEC | rw, ctx->bv, npages, ctx->len);
+ 	return 0;
+ }
+ 
+ /**
+  * cifs_alloc_hash - allocate hash and hash context together
+  *
+  * The caller has to make sure @sdesc is initialized to either NULL or
+  * a valid context. Both can be freed via cifs_free_hash().
+  */
+ int
+ cifs_alloc_hash(const char *name,
+ 		struct crypto_shash **shash, struct sdesc **sdesc)
+ {
+ 	int rc = 0;
+ 	size_t size;
+ 
+ 	if (*sdesc != NULL)
+ 		return 0;
+ 
+ 	*shash = crypto_alloc_shash(name, 0, 0);
+ 	if (IS_ERR(*shash)) {
+ 		cifs_dbg(VFS, "could not allocate crypto %s\n", name);
+ 		rc = PTR_ERR(*shash);
+ 		*shash = NULL;
+ 		*sdesc = NULL;
+ 		return rc;
+ 	}
+ 
+ 	size = sizeof(struct shash_desc) + crypto_shash_descsize(*shash);
+ 	*sdesc = kmalloc(size, GFP_KERNEL);
+ 	if (*sdesc == NULL) {
+ 		cifs_dbg(VFS, "no memory left to allocate crypto %s\n", name);
+ 		crypto_free_shash(*shash);
+ 		*shash = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	(*sdesc)->shash.tfm = *shash;
+ 	(*sdesc)->shash.flags = 0x0;
+ 	return 0;
+ }
+ 
+ /**
+  * cifs_free_hash - free hash and hash context together
+  *
+  * Freeing a NULL hash or context is safe.
+  */
+ void
+ cifs_free_hash(struct crypto_shash **shash, struct sdesc **sdesc)
+ {
+ 	kfree(*sdesc);
+ 	*sdesc = NULL;
+ 	if (*shash)
+ 		crypto_free_shash(*shash);
+ 	*shash = NULL;
+ }
+ 
+ /**
+  * rqst_page_get_length - obtain the length and offset for a page in smb_rqst
+  * Input: rqst - a smb_rqst, page - a page index for rqst
+  * Output: *len - the length for this page, *offset - the offset for this page
+  */
+ void rqst_page_get_length(struct smb_rqst *rqst, unsigned int page,
+ 				unsigned int *len, unsigned int *offset)
+ {
+ 	*len = rqst->rq_pagesz;
+ 	*offset = (page == 0) ? rqst->rq_offset : 0;
+ 
+ 	if (rqst->rq_npages == 1 || page == rqst->rq_npages-1)
+ 		*len = rqst->rq_tailsz;
+ 	else if (page == 0)
+ 		*len = rqst->rq_pagesz - rqst->rq_offset;
+ }
++>>>>>>> 42bc47b35320 (treewide: Use array_size() in vmalloc())
* Unmerged path arch/s390/kvm/gaccess.c
* Unmerged path drivers/base/firmware_loader/fallback.c
* Unmerged path drivers/lightnvm/pblk-gc.c
* Unmerged path drivers/rapidio/devices/rio_mport_cdev.c
* Unmerged path drivers/staging/android/ion/ion_heap.c
* Unmerged path drivers/staging/greybus/camera.c
* Unmerged path drivers/staging/rts5208/ms.c
* Unmerged path drivers/staging/rts5208/rtsx_chip.c
* Unmerged path kernel/cgroup/cgroup-v1.c
* Unmerged path kernel/rcu/rcutorture.c
* Unmerged path kernel/trace/tracing_map.c
* Unmerged path mm/percpu-stats.c
diff --git a/arch/powerpc/kernel/rtasd.c b/arch/powerpc/kernel/rtasd.c
index e6650fdef96c..521745651fce 100644
--- a/arch/powerpc/kernel/rtasd.c
+++ b/arch/powerpc/kernel/rtasd.c
@@ -554,7 +554,8 @@ static int __init rtas_event_scan_init(void)
 	rtas_error_log_max = rtas_get_error_log_max();
 	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
 
-	rtas_log_buf = vmalloc(rtas_error_log_buffer_max*LOG_NUMBER);
+	rtas_log_buf = vmalloc(array_size(LOG_NUMBER,
+					  rtas_error_log_buffer_max));
 	if (!rtas_log_buf) {
 		printk(KERN_ERR "rtasd: no memory\n");
 		return -ENOMEM;
diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index 0b6a5ab229f6..6b00bc06b8a9 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -110,7 +110,7 @@ int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)
 	npte = 1ul << (order - 4);
 
 	/* Allocate reverse map array */
-	rev = vmalloc(sizeof(struct revmap_entry) * npte);
+	rev = vmalloc(array_size(npte, sizeof(struct revmap_entry)));
 	if (!rev) {
 		pr_err("kvmppc_allocate_hpt: Couldn't alloc reverse map array\n");
 		if (cma)
diff --git a/arch/s390/hypfs/hypfs_diag.c b/arch/s390/hypfs/hypfs_diag.c
index c9aee42e32e6..0e47023be9d9 100644
--- a/arch/s390/hypfs/hypfs_diag.c
+++ b/arch/s390/hypfs/hypfs_diag.c
@@ -238,7 +238,7 @@ static void *page_align_ptr(void *ptr)
 static void *diag204_alloc_vbuf(int pages)
 {
 	/* The buffer has to be page aligned! */
-	diag204_buf_vmalloc = vmalloc(PAGE_SIZE * (pages + 1));
+	diag204_buf_vmalloc = vmalloc(array_size(PAGE_SIZE, (pages + 1)));
 	if (!diag204_buf_vmalloc)
 		return ERR_PTR(-ENOMEM);
 	diag204_buf = page_align_ptr(diag204_buf_vmalloc);
diff --git a/arch/s390/kernel/module.c b/arch/s390/kernel/module.c
index 9cf7bdf5d962..525cb7ec3020 100644
--- a/arch/s390/kernel/module.c
+++ b/arch/s390/kernel/module.c
@@ -141,8 +141,8 @@ int module_frob_arch_sections(Elf_Ehdr *hdr, Elf_Shdr *sechdrs,
 
 	/* Allocate one syminfo structure per symbol. */
 	me->arch.nsyms = symtab->sh_size / sizeof(Elf_Sym);
-	me->arch.syminfo = vmalloc(me->arch.nsyms *
-				   sizeof(struct mod_arch_syminfo));
+	me->arch.syminfo = vmalloc(array_size(sizeof(struct mod_arch_syminfo),
+					      me->arch.nsyms));
 	if (!me->arch.syminfo)
 		return -ENOMEM;
 	symbols = (void *) hdr + symtab->sh_offset;
diff --git a/arch/s390/kernel/sthyi.c b/arch/s390/kernel/sthyi.c
index 12981e197f01..5a02d5581751 100644
--- a/arch/s390/kernel/sthyi.c
+++ b/arch/s390/kernel/sthyi.c
@@ -318,7 +318,7 @@ static void fill_diag(struct sthyi_sctns *sctns)
 	if (pages <= 0)
 		return;
 
-	diag204_buf = vmalloc(PAGE_SIZE * pages);
+	diag204_buf = vmalloc(array_size(pages, PAGE_SIZE));
 	if (!diag204_buf)
 		return;
 
* Unmerged path arch/s390/kvm/gaccess.c
* Unmerged path arch/s390/kvm/kvm-s390.c
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index af94ebee7c2e..b9b03b24062b 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -193,8 +193,9 @@ int kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,
 		goto out;
 	r = -ENOMEM;
 	if (cpuid->nent) {
-		cpuid_entries = vmalloc(sizeof(struct kvm_cpuid_entry) *
-					cpuid->nent);
+		cpuid_entries =
+			vmalloc(array_size(sizeof(struct kvm_cpuid_entry),
+					   cpuid->nent));
 		if (!cpuid_entries)
 			goto out;
 		r = -EFAULT;
* Unmerged path drivers/base/firmware_loader/fallback.c
diff --git a/drivers/dma/ipu/ipu_idmac.c b/drivers/dma/ipu/ipu_idmac.c
index d39c2cd0795d..dce8bb54b160 100644
--- a/drivers/dma/ipu/ipu_idmac.c
+++ b/drivers/dma/ipu/ipu_idmac.c
@@ -910,7 +910,8 @@ out:
 /* Called with ichan->chan_mutex held */
 static int idmac_desc_alloc(struct idmac_channel *ichan, int n)
 {
-	struct idmac_tx_desc *desc = vmalloc(n * sizeof(struct idmac_tx_desc));
+	struct idmac_tx_desc *desc =
+		vmalloc(array_size(n, sizeof(struct idmac_tx_desc)));
 	struct idmac *idmac = to_idmac(ichan->dma_chan.device);
 
 	if (!desc)
diff --git a/drivers/gpu/drm/drm_memory.c b/drivers/gpu/drm/drm_memory.c
index 3c54044214db..d69e4fc1ee77 100644
--- a/drivers/gpu/drm/drm_memory.c
+++ b/drivers/gpu/drm/drm_memory.c
@@ -80,7 +80,7 @@ static void *agp_remap(unsigned long offset, unsigned long size,
 	 * page-table instead (that's probably faster anyhow...).
 	 */
 	/* note: use vmalloc() because num_pages could be large... */
-	page_map = vmalloc(num_pages * sizeof(struct page *));
+	page_map = vmalloc(array_size(num_pages, sizeof(struct page *)));
 	if (!page_map)
 		return NULL;
 
diff --git a/drivers/gpu/drm/nouveau/nv84_fence.c b/drivers/gpu/drm/nouveau/nv84_fence.c
index 090664899247..e721bb2163a0 100644
--- a/drivers/gpu/drm/nouveau/nv84_fence.c
+++ b/drivers/gpu/drm/nouveau/nv84_fence.c
@@ -141,7 +141,7 @@ nv84_fence_suspend(struct nouveau_drm *drm)
 	struct nv84_fence_priv *priv = drm->fence;
 	int i;
 
-	priv->suspend = vmalloc(drm->chan.nr * sizeof(u32));
+	priv->suspend = vmalloc(array_size(sizeof(u32), drm->chan.nr));
 	if (priv->suspend) {
 		for (i = 0; i < drm->chan.nr; i++)
 			priv->suspend[i] = nouveau_bo_rd32(priv->bo, i*4);
diff --git a/drivers/gpu/drm/qxl/qxl_fb.c b/drivers/gpu/drm/qxl/qxl_fb.c
index 1a2343b0f4c0..69ae683baecc 100644
--- a/drivers/gpu/drm/qxl/qxl_fb.c
+++ b/drivers/gpu/drm/qxl/qxl_fb.c
@@ -241,7 +241,7 @@ static int qxlfb_create(struct qxl_fbdev *qfbdev,
 	DRM_DEBUG_DRIVER("%dx%d %d\n", mode_cmd.width,
 			 mode_cmd.height, mode_cmd.pitches[0]);
 
-	shadow = vmalloc(mode_cmd.pitches[0] * mode_cmd.height);
+	shadow = vmalloc(array_size(mode_cmd.pitches[0], mode_cmd.height));
 	/* TODO: what's the usual response to memory allocation errors? */
 	BUG_ON(!shadow);
 	DRM_DEBUG_DRIVER("surface0 at gpu offset %lld, mmap_offset %lld (virt %p, shadow %p)\n",
diff --git a/drivers/gpu/drm/radeon/radeon_gart.c b/drivers/gpu/drm/radeon/radeon_gart.c
index c4777c8d0312..a3c143e85685 100644
--- a/drivers/gpu/drm/radeon/radeon_gart.c
+++ b/drivers/gpu/drm/radeon/radeon_gart.c
@@ -349,8 +349,8 @@ int radeon_gart_init(struct radeon_device *rdev)
 		radeon_gart_fini(rdev);
 		return -ENOMEM;
 	}
-	rdev->gart.pages_entry = vmalloc(sizeof(uint64_t) *
-					 rdev->gart.num_gpu_pages);
+	rdev->gart.pages_entry = vmalloc(array_size(sizeof(uint64_t),
+						    rdev->gart.num_gpu_pages));
 	if (rdev->gart.pages_entry == NULL) {
 		radeon_gart_fini(rdev);
 		return -ENOMEM;
diff --git a/drivers/gpu/drm/selftests/test-drm_mm.c b/drivers/gpu/drm/selftests/test-drm_mm.c
index 7cc935d7b7aa..e408acb56356 100644
--- a/drivers/gpu/drm/selftests/test-drm_mm.c
+++ b/drivers/gpu/drm/selftests/test-drm_mm.c
@@ -579,7 +579,7 @@ static int __igt_insert(unsigned int count, u64 size, bool replace)
 	DRM_MM_BUG_ON(!size);
 
 	ret = -ENOMEM;
-	nodes = vmalloc(count * sizeof(*nodes));
+	nodes = vmalloc(array_size(count, sizeof(*nodes)));
 	if (!nodes)
 		goto err;
 
diff --git a/drivers/iommu/tegra-gart.c b/drivers/iommu/tegra-gart.c
index 108c0e9c24d9..4de678315642 100644
--- a/drivers/iommu/tegra-gart.c
+++ b/drivers/iommu/tegra-gart.c
@@ -388,7 +388,7 @@ static int tegra_gart_probe(struct platform_device *pdev)
 	gart->iovmm_base = (dma_addr_t)res_remap->start;
 	gart->page_count = (resource_size(res_remap) >> GART_PAGE_SHIFT);
 
-	gart->savedata = vmalloc(sizeof(u32) * gart->page_count);
+	gart->savedata = vmalloc(array_size(sizeof(u32), gart->page_count));
 	if (!gart->savedata) {
 		dev_err(dev, "failed to allocate context save area\n");
 		err = -ENOMEM;
diff --git a/drivers/isdn/i4l/isdn_bsdcomp.c b/drivers/isdn/i4l/isdn_bsdcomp.c
index 8837ac5a492d..a3b475701c2e 100644
--- a/drivers/isdn/i4l/isdn_bsdcomp.c
+++ b/drivers/isdn/i4l/isdn_bsdcomp.c
@@ -340,7 +340,7 @@ static void *bsd_alloc(struct isdn_ppp_comp_data *data)
 	 * Allocate space for the dictionary. This may be more than one page in
 	 * length.
 	 */
-	db->dict = vmalloc(hsize * sizeof(struct bsd_dict));
+	db->dict = vmalloc(array_size(hsize, sizeof(struct bsd_dict)));
 	if (!db->dict) {
 		bsd_free(db);
 		return NULL;
@@ -353,7 +353,8 @@ static void *bsd_alloc(struct isdn_ppp_comp_data *data)
 	if (!decomp)
 		db->lens = NULL;
 	else {
-		db->lens = vmalloc((maxmaxcode + 1) * sizeof(db->lens[0]));
+		db->lens = vmalloc(array_size(sizeof(db->lens[0]),
+					      maxmaxcode + 1));
 		if (!db->lens) {
 			bsd_free(db);
 			return (NULL);
* Unmerged path drivers/lightnvm/pblk-gc.c
diff --git a/drivers/md/bcache/sysfs.c b/drivers/md/bcache/sysfs.c
index e9bd6c0cca5b..8d2434c11aef 100644
--- a/drivers/md/bcache/sysfs.c
+++ b/drivers/md/bcache/sysfs.c
@@ -679,7 +679,8 @@ SHOW(__bch_cache)
 		uint16_t q[nq], *p, *cached;
 		ssize_t ret;
 
-		cached = p = vmalloc(ca->sb.nbuckets * sizeof(uint16_t));
+		cached = p = vmalloc(array_size(sizeof(uint16_t),
+						ca->sb.nbuckets));
 		if (!p)
 			return -ENOMEM;
 
diff --git a/drivers/md/dm-cache-policy-smq.c b/drivers/md/dm-cache-policy-smq.c
index 4ab23d0075f6..4d69b6f4129e 100644
--- a/drivers/md/dm-cache-policy-smq.c
+++ b/drivers/md/dm-cache-policy-smq.c
@@ -588,7 +588,7 @@ static int h_init(struct smq_hash_table *ht, struct entry_space *es, unsigned nr
 	nr_buckets = roundup_pow_of_two(max(nr_entries / 4u, 16u));
 	ht->hash_bits = __ffs(nr_buckets);
 
-	ht->buckets = vmalloc(sizeof(*ht->buckets) * nr_buckets);
+	ht->buckets = vmalloc(array_size(nr_buckets, sizeof(*ht->buckets)));
 	if (!ht->buckets)
 		return -ENOMEM;
 
diff --git a/drivers/md/dm-region-hash.c b/drivers/md/dm-region-hash.c
index a1f57656fea5..e2cfab218f5b 100644
--- a/drivers/md/dm-region-hash.c
+++ b/drivers/md/dm-region-hash.c
@@ -200,7 +200,7 @@ struct dm_region_hash *dm_region_hash_create(
 	rh->shift = RH_HASH_SHIFT;
 	rh->prime = RH_HASH_MULT;
 
-	rh->buckets = vmalloc(nr_buckets * sizeof(*rh->buckets));
+	rh->buckets = vmalloc(array_size(nr_buckets, sizeof(*rh->buckets)));
 	if (!rh->buckets) {
 		DMERR("unable to allocate region hash bucket memory");
 		kfree(rh);
diff --git a/drivers/md/dm-switch.c b/drivers/md/dm-switch.c
index 5d25f8347758..2f55f958f3a2 100644
--- a/drivers/md/dm-switch.c
+++ b/drivers/md/dm-switch.c
@@ -114,7 +114,8 @@ static int alloc_region_table(struct dm_target *ti, unsigned nr_paths)
 		return -EINVAL;
 	}
 
-	sctx->region_table = vmalloc(nr_slots * sizeof(region_table_slot_t));
+	sctx->region_table = vmalloc(array_size(nr_slots,
+						sizeof(region_table_slot_t)));
 	if (!sctx->region_table) {
 		ti->error = "Cannot allocate region table";
 		return -ENOMEM;
diff --git a/drivers/md/dm-thin.c b/drivers/md/dm-thin.c
index d9ae4b3b51d5..e8a148b05143 100644
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@ -3091,7 +3091,9 @@ static struct pool *pool_create(struct mapped_device *pool_md,
 		goto bad_mapping_pool;
 	}
 
-	pool->cell_sort_array = vmalloc(sizeof(*pool->cell_sort_array) * CELL_SORT_ARRAY_SIZE);
+	pool->cell_sort_array =
+		vmalloc(array_size(CELL_SORT_ARRAY_SIZE,
+				   sizeof(*pool->cell_sort_array)));
 	if (!pool->cell_sort_array) {
 		*error = "Error allocating cell sort array";
 		err_p = ERR_PTR(-ENOMEM);
diff --git a/drivers/media/dvb-core/dmxdev.c b/drivers/media/dvb-core/dmxdev.c
index 0b4616b87195..16efc5940220 100644
--- a/drivers/media/dvb-core/dmxdev.c
+++ b/drivers/media/dvb-core/dmxdev.c
@@ -1224,7 +1224,8 @@ int dvb_dmxdev_init(struct dmxdev *dmxdev, struct dvb_adapter *dvb_adapter)
 	if (dmxdev->demux->open(dmxdev->demux) < 0)
 		return -EUSERS;
 
-	dmxdev->filter = vmalloc(dmxdev->filternum * sizeof(struct dmxdev_filter));
+	dmxdev->filter = vmalloc(array_size(sizeof(struct dmxdev_filter),
+					    dmxdev->filternum));
 	if (!dmxdev->filter)
 		return -ENOMEM;
 
diff --git a/drivers/media/dvb-core/dvb_demux.c b/drivers/media/dvb-core/dvb_demux.c
index 3485655fa082..a91d83d65a10 100644
--- a/drivers/media/dvb-core/dvb_demux.c
+++ b/drivers/media/dvb-core/dvb_demux.c
@@ -1242,12 +1242,14 @@ int dvb_dmx_init(struct dvb_demux *dvbdemux)
 
 	dvbdemux->cnt_storage = NULL;
 	dvbdemux->users = 0;
-	dvbdemux->filter = vmalloc(dvbdemux->filternum * sizeof(struct dvb_demux_filter));
+	dvbdemux->filter = vmalloc(array_size(sizeof(struct dvb_demux_filter),
+					      dvbdemux->filternum));
 
 	if (!dvbdemux->filter)
 		return -ENOMEM;
 
-	dvbdemux->feed = vmalloc(dvbdemux->feednum * sizeof(struct dvb_demux_feed));
+	dvbdemux->feed = vmalloc(array_size(sizeof(struct dvb_demux_feed),
+					    dvbdemux->feednum));
 	if (!dvbdemux->feed) {
 		vfree(dvbdemux->filter);
 		dvbdemux->filter = NULL;
* Unmerged path drivers/media/pci/meye/meye.c
* Unmerged path drivers/media/pci/pt1/pt1.c
diff --git a/drivers/media/pci/ttpci/av7110_ipack.c b/drivers/media/pci/ttpci/av7110_ipack.c
index 699ef8b5b99a..accaeb05a24f 100644
--- a/drivers/media/pci/ttpci/av7110_ipack.c
+++ b/drivers/media/pci/ttpci/av7110_ipack.c
@@ -23,7 +23,7 @@ void av7110_ipack_reset(struct ipack *p)
 int av7110_ipack_init(struct ipack *p, int size,
 		      void (*func)(u8 *buf, int size, void *priv))
 {
-	if (!(p->buf = vmalloc(size*sizeof(u8)))) {
+	if (!(p->buf = vmalloc(size))) {
 		printk(KERN_WARNING "Couldn't allocate memory for ipack\n");
 		return -ENOMEM;
 	}
diff --git a/drivers/media/pci/zoran/zoran_driver.c b/drivers/media/pci/zoran/zoran_driver.c
index d133c30c3fdc..e721d0b4fc87 100644
--- a/drivers/media/pci/zoran/zoran_driver.c
+++ b/drivers/media/pci/zoran/zoran_driver.c
@@ -1234,7 +1234,8 @@ static int setup_window(struct zoran_fh *fh,
 		}
 	} else if (clipcount) {
 		/* write our own bitmap from the clips */
-		vcp = vmalloc(sizeof(struct v4l2_clip) * (clipcount + 4));
+		vcp = vmalloc(array_size(sizeof(struct v4l2_clip),
+					 clipcount + 4));
 		if (vcp == NULL) {
 			dprintk(1,
 				KERN_ERR
diff --git a/drivers/media/platform/soc_camera/soc_camera.c b/drivers/media/platform/soc_camera/soc_camera.c
index 3a4efbdc7668..68c9bc4a7ce0 100644
--- a/drivers/media/platform/soc_camera/soc_camera.c
+++ b/drivers/media/platform/soc_camera/soc_camera.c
@@ -417,7 +417,8 @@ static int soc_camera_init_user_formats(struct soc_camera_device *icd)
 		return -ENXIO;
 
 	icd->user_formats =
-		vmalloc(fmts * sizeof(struct soc_camera_format_xlate));
+		vmalloc(array_size(fmts,
+				   sizeof(struct soc_camera_format_xlate)));
 	if (!icd->user_formats)
 		return -ENOMEM;
 
diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c
index f18f766aeb59..91ade88202d0 100644
--- a/drivers/media/v4l2-core/videobuf-dma-sg.c
+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c
@@ -100,7 +100,7 @@ static struct scatterlist *videobuf_pages_to_sg(struct page **pages,
 
 	if (NULL == pages[0])
 		return NULL;
-	sglist = vmalloc(nr_pages * sizeof(*sglist));
+	sglist = vmalloc(array_size(nr_pages, sizeof(*sglist)));
 	if (NULL == sglist)
 		return NULL;
 	sg_init_table(sglist, nr_pages);
diff --git a/drivers/mtd/ftl.c b/drivers/mtd/ftl.c
index 19d637266fcd..6267791478c1 100644
--- a/drivers/mtd/ftl.c
+++ b/drivers/mtd/ftl.c
@@ -269,7 +269,7 @@ static int build_maps(partition_t *part)
 
     /* Set up virtual page map */
     blocks = le32_to_cpu(header.FormattedSize) >> header.BlockSize;
-    part->VirtualBlockMap = vmalloc(blocks * sizeof(uint32_t));
+    part->VirtualBlockMap = vmalloc(array_size(blocks, sizeof(uint32_t)));
     if (!part->VirtualBlockMap)
 	    goto out_XferInfo;
 
diff --git a/drivers/mtd/mtdoops.c b/drivers/mtd/mtdoops.c
index 97bb8f6304d4..400b040dbb09 100644
--- a/drivers/mtd/mtdoops.c
+++ b/drivers/mtd/mtdoops.c
@@ -350,8 +350,10 @@ static void mtdoops_notify_add(struct mtd_info *mtd)
 	}
 
 	/* oops_page_used is a bit field */
-	cxt->oops_page_used = vmalloc(DIV_ROUND_UP(mtdoops_pages,
-			BITS_PER_LONG) * sizeof(unsigned long));
+	cxt->oops_page_used =
+		vmalloc(array_size(sizeof(unsigned long),
+				   DIV_ROUND_UP(mtdoops_pages,
+						BITS_PER_LONG)));
 	if (!cxt->oops_page_used) {
 		printk(KERN_ERR "mtdoops: could not allocate page array\n");
 		return;
diff --git a/drivers/mtd/mtdswap.c b/drivers/mtd/mtdswap.c
index c92f0f6bc130..d75f6b98d4df 100644
--- a/drivers/mtd/mtdswap.c
+++ b/drivers/mtd/mtdswap.c
@@ -1364,11 +1364,11 @@ static int mtdswap_init(struct mtdswap_dev *d, unsigned int eblocks,
 	for (i = 0; i < MTDSWAP_TREE_CNT; i++)
 		d->trees[i].root = RB_ROOT;
 
-	d->page_data = vmalloc(sizeof(int)*pages);
+	d->page_data = vmalloc(array_size(pages, sizeof(int)));
 	if (!d->page_data)
 		goto page_data_fail;
 
-	d->revmap = vmalloc(sizeof(int)*blocks);
+	d->revmap = vmalloc(array_size(blocks, sizeof(int)));
 	if (!d->revmap)
 		goto revmap_fail;
 
diff --git a/drivers/mtd/nand/nandsim.c b/drivers/mtd/nand/nandsim.c
index cb38f3d94218..fe84acc9eeed 100644
--- a/drivers/mtd/nand/nandsim.c
+++ b/drivers/mtd/nand/nandsim.c
@@ -602,7 +602,7 @@ static int alloc_device(struct nandsim *ns)
 		return 0;
 	}
 
-	ns->pages = vmalloc(ns->geom.pgnum * sizeof(union ns_mem));
+	ns->pages = vmalloc(array_size(sizeof(union ns_mem), ns->geom.pgnum));
 	if (!ns->pages) {
 		NS_ERR("alloc_device: unable to allocate page array\n");
 		return -ENOMEM;
diff --git a/drivers/mtd/rfd_ftl.c b/drivers/mtd/rfd_ftl.c
index 233b946e5d66..8f8181c09dfe 100644
--- a/drivers/mtd/rfd_ftl.c
+++ b/drivers/mtd/rfd_ftl.c
@@ -189,7 +189,8 @@ static int scan_header(struct partition *part)
 	if (!part->blocks)
 		goto err;
 
-	part->sector_map = vmalloc(part->sector_count * sizeof(u_long));
+	part->sector_map = vmalloc(array_size(sizeof(u_long),
+					      part->sector_count));
 	if (!part->sector_map) {
 		printk(KERN_ERR PREFIX "'%s': unable to allocate memory for "
 			"sector map", part->mbd.mtd->name);
diff --git a/drivers/net/ethernet/cavium/liquidio/request_manager.c b/drivers/net/ethernet/cavium/liquidio/request_manager.c
index b1270355b0b1..1f2e75da28f8 100644
--- a/drivers/net/ethernet/cavium/liquidio/request_manager.c
+++ b/drivers/net/ethernet/cavium/liquidio/request_manager.c
@@ -98,8 +98,9 @@ int octeon_init_instr_queue(struct octeon_device *oct,
 	iq->request_list = vmalloc_node((sizeof(*iq->request_list) * num_descs),
 					       numa_node);
 	if (!iq->request_list)
-		iq->request_list = vmalloc(sizeof(*iq->request_list) *
-						  num_descs);
+		iq->request_list =
+			vmalloc(array_size(num_descs,
+					   sizeof(*iq->request_list)));
 	if (!iq->request_list) {
 		lio_dma_free(oct, q_size, iq->base_addr, iq->base_addr_dma);
 		dev_err(&oct->pci_dev->dev, "Alloc failed for IQ[%d] nr free list\n",
diff --git a/drivers/net/ethernet/intel/fm10k/fm10k_ethtool.c b/drivers/net/ethernet/intel/fm10k/fm10k_ethtool.c
index 8c646b221d6c..ac4f4fb5520a 100644
--- a/drivers/net/ethernet/intel/fm10k/fm10k_ethtool.c
+++ b/drivers/net/ethernet/intel/fm10k/fm10k_ethtool.c
@@ -575,7 +575,7 @@ static int fm10k_set_ringparam(struct net_device *netdev,
 
 	/* allocate temporary buffer to store rings in */
 	i = max_t(int, interface->num_tx_queues, interface->num_rx_queues);
-	temp_ring = vmalloc(i * sizeof(struct fm10k_ring));
+	temp_ring = vmalloc(array_size(i, sizeof(struct fm10k_ring)));
 
 	if (!temp_ring) {
 		err = -ENOMEM;
diff --git a/drivers/net/ethernet/intel/igb/igb_ethtool.c b/drivers/net/ethernet/intel/igb/igb_ethtool.c
index 37583fd057f6..b57727e85dc1 100644
--- a/drivers/net/ethernet/intel/igb/igb_ethtool.c
+++ b/drivers/net/ethernet/intel/igb/igb_ethtool.c
@@ -902,11 +902,11 @@ static int igb_set_ringparam(struct net_device *netdev,
 	}
 
 	if (adapter->num_tx_queues > adapter->num_rx_queues)
-		temp_ring = vmalloc(adapter->num_tx_queues *
-				    sizeof(struct igb_ring));
+		temp_ring = vmalloc(array_size(sizeof(struct igb_ring),
+					       adapter->num_tx_queues));
 	else
-		temp_ring = vmalloc(adapter->num_rx_queues *
-				    sizeof(struct igb_ring));
+		temp_ring = vmalloc(array_size(sizeof(struct igb_ring),
+					       adapter->num_rx_queues));
 
 	if (!temp_ring) {
 		err = -ENOMEM;
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
index 188dde2ea18f..36d28376d3d6 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
@@ -1073,7 +1073,7 @@ static int ixgbe_set_ringparam(struct net_device *netdev,
 	/* allocate temporary buffer to store rings in */
 	i = max_t(int, adapter->num_tx_queues + adapter->num_xdp_queues,
 		  adapter->num_rx_queues);
-	temp_ring = vmalloc(i * sizeof(struct ixgbe_ring));
+	temp_ring = vmalloc(array_size(i, sizeof(struct ixgbe_ring)));
 
 	if (!temp_ring) {
 		err = -ENOMEM;
diff --git a/drivers/net/ethernet/intel/ixgbevf/ethtool.c b/drivers/net/ethernet/intel/ixgbevf/ethtool.c
index 8e7d6c6f5c92..01b6c910208c 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ethtool.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ethtool.c
@@ -305,8 +305,9 @@ static int ixgbevf_set_ringparam(struct net_device *netdev,
 	}
 
 	if (new_tx_count != adapter->tx_ring_count) {
-		tx_ring = vmalloc((adapter->num_tx_queues +
-				   adapter->num_xdp_queues) * sizeof(*tx_ring));
+		tx_ring = vmalloc(array_size(sizeof(*tx_ring),
+					     adapter->num_tx_queues +
+						adapter->num_xdp_queues));
 		if (!tx_ring) {
 			err = -ENOMEM;
 			goto clear_reset;
@@ -350,7 +351,8 @@ static int ixgbevf_set_ringparam(struct net_device *netdev,
 	}
 
 	if (new_rx_count != adapter->rx_ring_count) {
-		rx_ring = vmalloc(adapter->num_rx_queues * sizeof(*rx_ring));
+		rx_ring = vmalloc(array_size(sizeof(*rx_ring),
+					     adapter->num_rx_queues));
 		if (!rx_ring) {
 			err = -ENOMEM;
 			goto clear_reset;
diff --git a/drivers/net/ethernet/netronome/nfp/flower/metadata.c b/drivers/net/ethernet/netronome/nfp/flower/metadata.c
index bb425b17c6a7..5fcc4b44bf0f 100644
--- a/drivers/net/ethernet/netronome/nfp/flower/metadata.c
+++ b/drivers/net/ethernet/netronome/nfp/flower/metadata.c
@@ -414,7 +414,8 @@ int nfp_flower_metadata_init(struct nfp_app *app)
 
 	/* Init ring buffer and unallocated stats_ids. */
 	priv->stats_ids.free_list.buf =
-		vmalloc(NFP_FL_STATS_ENTRY_RS * NFP_FL_STATS_ELEM_RS);
+		vmalloc(array_size(NFP_FL_STATS_ELEM_RS,
+				   NFP_FL_STATS_ENTRY_RS));
 	if (!priv->stats_ids.free_list.buf)
 		goto err_free_last_used;
 
diff --git a/drivers/net/ppp/bsd_comp.c b/drivers/net/ppp/bsd_comp.c
index a9b759add187..61fedb23d3cf 100644
--- a/drivers/net/ppp/bsd_comp.c
+++ b/drivers/net/ppp/bsd_comp.c
@@ -406,7 +406,7 @@ static void *bsd_alloc (unsigned char *options, int opt_len, int decomp)
  * Allocate space for the dictionary. This may be more than one page in
  * length.
  */
-    db->dict = vmalloc(hsize * sizeof(struct bsd_dict));
+    db->dict = vmalloc(array_size(hsize, sizeof(struct bsd_dict)));
     if (!db->dict)
       {
 	bsd_free (db);
@@ -425,7 +425,7 @@ static void *bsd_alloc (unsigned char *options, int opt_len, int decomp)
  */
     else
       {
-        db->lens = vmalloc((maxmaxcode + 1) * sizeof(db->lens[0]));
+        db->lens = vmalloc(array_size(sizeof(db->lens[0]), (maxmaxcode + 1)));
 	if (!db->lens)
 	  {
 	    bsd_free (db);
* Unmerged path drivers/net/wireless/ath/ath5k/debug.c
* Unmerged path drivers/net/wireless/marvell/mwifiex/cfg80211.c
diff --git a/drivers/oprofile/event_buffer.c b/drivers/oprofile/event_buffer.c
index c0cc4e7ff023..b2caee06ff5a 100644
--- a/drivers/oprofile/event_buffer.c
+++ b/drivers/oprofile/event_buffer.c
@@ -91,7 +91,7 @@ int alloc_event_buffer(void)
 		return -EINVAL;
 
 	buffer_pos = 0;
-	event_buffer = vmalloc(sizeof(unsigned long) * buffer_size);
+	event_buffer = vmalloc(array_size(buffer_size, sizeof(unsigned long)));
 	if (!event_buffer)
 		return -ENOMEM;
 
* Unmerged path drivers/rapidio/devices/rio_mport_cdev.c
diff --git a/drivers/scsi/fnic/fnic_debugfs.c b/drivers/scsi/fnic/fnic_debugfs.c
index 8e6e41dc0ce2..d9f8104cc15f 100644
--- a/drivers/scsi/fnic/fnic_debugfs.c
+++ b/drivers/scsi/fnic/fnic_debugfs.c
@@ -250,8 +250,8 @@ static int fnic_trace_debugfs_open(struct inode *inode,
 		return -ENOMEM;
 
 	if (*rdata_ptr == fc_trc_flag->fnic_trace) {
-		fnic_dbg_prt->buffer = vmalloc(3 *
-					(trace_max_pages * PAGE_SIZE));
+		fnic_dbg_prt->buffer = vmalloc(array3_size(3, trace_max_pages,
+							   PAGE_SIZE));
 		if (!fnic_dbg_prt->buffer) {
 			kfree(fnic_dbg_prt);
 			return -ENOMEM;
@@ -261,7 +261,8 @@ static int fnic_trace_debugfs_open(struct inode *inode,
 		fnic_dbg_prt->buffer_len = fnic_get_trace_data(fnic_dbg_prt);
 	} else {
 		fnic_dbg_prt->buffer =
-			vmalloc(3 * (fnic_fc_trace_max_pages * PAGE_SIZE));
+			vmalloc(array3_size(3, fnic_fc_trace_max_pages,
+					    PAGE_SIZE));
 		if (!fnic_dbg_prt->buffer) {
 			kfree(fnic_dbg_prt);
 			return -ENOMEM;
diff --git a/drivers/scsi/fnic/fnic_trace.c b/drivers/scsi/fnic/fnic_trace.c
index 914eef257a12..6f8da732bca0 100644
--- a/drivers/scsi/fnic/fnic_trace.c
+++ b/drivers/scsi/fnic/fnic_trace.c
@@ -476,8 +476,9 @@ int fnic_trace_buf_init(void)
 	}
 	memset((void *)fnic_trace_buf_p, 0, (trace_max_pages * PAGE_SIZE));
 
-	fnic_trace_entries.page_offset = vmalloc(fnic_max_trace_entries *
-						  sizeof(unsigned long));
+	fnic_trace_entries.page_offset =
+		vmalloc(array_size(fnic_max_trace_entries,
+				   sizeof(unsigned long)));
 	if (!fnic_trace_entries.page_offset) {
 		printk(KERN_ERR PFX "Failed to allocate memory for"
 				  " page_offset\n");
@@ -554,8 +555,9 @@ int fnic_fc_trace_init(void)
 
 	fc_trace_max_entries = (fnic_fc_trace_max_pages * PAGE_SIZE)/
 				FC_TRC_SIZE_BYTES;
-	fnic_fc_ctlr_trace_buf_p = (unsigned long)vmalloc(
-					fnic_fc_trace_max_pages * PAGE_SIZE);
+	fnic_fc_ctlr_trace_buf_p =
+		(unsigned long)vmalloc(array_size(PAGE_SIZE,
+						  fnic_fc_trace_max_pages));
 	if (!fnic_fc_ctlr_trace_buf_p) {
 		pr_err("fnic: Failed to allocate memory for "
 		       "FC Control Trace Buf\n");
@@ -567,8 +569,9 @@ int fnic_fc_trace_init(void)
 			fnic_fc_trace_max_pages * PAGE_SIZE);
 
 	/* Allocate memory for page offset */
-	fc_trace_entries.page_offset = vmalloc(fc_trace_max_entries *
-						sizeof(unsigned long));
+	fc_trace_entries.page_offset =
+		vmalloc(array_size(fc_trace_max_entries,
+				   sizeof(unsigned long)));
 	if (!fc_trace_entries.page_offset) {
 		pr_err("fnic:Failed to allocate memory for page_offset\n");
 		if (fnic_fc_ctlr_trace_buf_p) {
diff --git a/drivers/scsi/ipr.c b/drivers/scsi/ipr.c
index a0d821d6f5ec..1bbb2343b815 100644
--- a/drivers/scsi/ipr.c
+++ b/drivers/scsi/ipr.c
@@ -4402,9 +4402,11 @@ static int ipr_alloc_dump(struct ipr_ioa_cfg *ioa_cfg)
 	}
 
 	if (ioa_cfg->sis64)
-		ioa_data = vmalloc(IPR_FMT3_MAX_NUM_DUMP_PAGES * sizeof(__be32 *));
+		ioa_data = vmalloc(array_size(IPR_FMT3_MAX_NUM_DUMP_PAGES,
+					      sizeof(__be32 *)));
 	else
-		ioa_data = vmalloc(IPR_FMT2_MAX_NUM_DUMP_PAGES * sizeof(__be32 *));
+		ioa_data = vmalloc(array_size(IPR_FMT2_MAX_NUM_DUMP_PAGES,
+					      sizeof(__be32 *)));
 
 	if (!ioa_data) {
 		ipr_err("Dump memory allocation failed\n");
diff --git a/drivers/scsi/osst.c b/drivers/scsi/osst.c
index 5da60596e5e0..776a48e9a777 100644
--- a/drivers/scsi/osst.c
+++ b/drivers/scsi/osst.c
@@ -1485,7 +1485,7 @@ static int osst_read_back_buffer_and_rewrite(struct osst_tape * STp, struct osst
 	int			dbg              = debugging;
 #endif
 
-	if ((buffer = vmalloc((nframes + 1) * OS_DATA_SIZE)) == NULL)
+	if ((buffer = vmalloc(array_size((nframes + 1), OS_DATA_SIZE))) == NULL)
 		return (-EIO);
 
 	printk(KERN_INFO "%s:I: Reading back %d frames from drive buffer%s\n",
diff --git a/drivers/scsi/scsi_debug.c b/drivers/scsi/scsi_debug.c
index 7f28c0f429a8..c8eccbe24e54 100644
--- a/drivers/scsi/scsi_debug.c
+++ b/drivers/scsi/scsi_debug.c
@@ -3393,7 +3393,8 @@ static int __init scsi_debug_init(void)
 		}
 
 		map_size = lba_to_map_index(sdebug_store_sectors - 1) + 1;
-		map_storep = vmalloc(BITS_TO_LONGS(map_size) * sizeof(long));
+		map_storep = vmalloc(array_size(sizeof(long),
+						BITS_TO_LONGS(map_size)));
 
 		printk(KERN_INFO "scsi_debug_init: %lu provisioning blocks\n",
 		       map_size);
* Unmerged path drivers/staging/android/ion/ion_heap.c
* Unmerged path drivers/staging/greybus/camera.c
* Unmerged path drivers/staging/rts5208/ms.c
* Unmerged path drivers/staging/rts5208/rtsx_chip.c
diff --git a/drivers/usb/misc/sisusbvga/sisusb_con.c b/drivers/usb/misc/sisusbvga/sisusb_con.c
index 73f7bde78e11..dc8433a45104 100644
--- a/drivers/usb/misc/sisusbvga/sisusb_con.c
+++ b/drivers/usb/misc/sisusbvga/sisusb_con.c
@@ -1243,7 +1243,7 @@ sisusbcon_font_set(struct vc_data *c, struct console_font *font,
 	}
 
 	if (!sisusb->font_backup)
-		sisusb->font_backup = vmalloc(charcount * 32);
+		sisusb->font_backup = vmalloc(array_size(charcount, 32));
 
 	if (sisusb->font_backup) {
 		memcpy(sisusb->font_backup, font->data, charcount * 32);
* Unmerged path drivers/video/xen-fbfront.c
* Unmerged path fs/binfmt_elf.c
* Unmerged path fs/cifs/misc.c
diff --git a/fs/dlm/lockspace.c b/fs/dlm/lockspace.c
index 88556dc0458e..e711dad69541 100644
--- a/fs/dlm/lockspace.c
+++ b/fs/dlm/lockspace.c
@@ -497,7 +497,7 @@ static int new_lockspace(const char *name, const char *cluster,
 	size = dlm_config.ci_rsbtbl_size;
 	ls->ls_rsbtbl_size = size;
 
-	ls->ls_rsbtbl = vmalloc(sizeof(struct dlm_rsbtable) * size);
+	ls->ls_rsbtbl = vmalloc(array_size(size, sizeof(struct dlm_rsbtable)));
 	if (!ls->ls_rsbtbl)
 		goto out_lsfree;
 	for (i = 0; i < size; i++) {
diff --git a/fs/reiserfs/bitmap.c b/fs/reiserfs/bitmap.c
index a98b7740a0fc..b29ddcbbc83b 100644
--- a/fs/reiserfs/bitmap.c
+++ b/fs/reiserfs/bitmap.c
@@ -1360,7 +1360,7 @@ int reiserfs_init_bitmap_cache(struct super_block *sb)
 	struct reiserfs_bitmap_info *bitmap;
 	unsigned int bmap_nr = reiserfs_bmap_count(sb);
 
-	bitmap = vmalloc(sizeof(*bitmap) * bmap_nr);
+	bitmap = vmalloc(array_size(bmap_nr, sizeof(*bitmap)));
 	if (bitmap == NULL)
 		return -ENOMEM;
 
diff --git a/fs/ubifs/lpt.c b/fs/ubifs/lpt.c
index d46b19ec1815..e4c4073f3e03 100644
--- a/fs/ubifs/lpt.c
+++ b/fs/ubifs/lpt.c
@@ -632,7 +632,8 @@ int ubifs_create_dflt_lpt(struct ubifs_info *c, int *main_lebs, int lpt_first,
 	pnode = kzalloc(sizeof(struct ubifs_pnode), GFP_KERNEL);
 	nnode = kzalloc(sizeof(struct ubifs_nnode), GFP_KERNEL);
 	buf = vmalloc(c->leb_size);
-	ltab = vmalloc(sizeof(struct ubifs_lpt_lprops) * c->lpt_lebs);
+	ltab = vmalloc(array_size(sizeof(struct ubifs_lpt_lprops),
+				  c->lpt_lebs));
 	if (!pnode || !nnode || !buf || !ltab || !lsave) {
 		err = -ENOMEM;
 		goto out;
@@ -1629,7 +1630,8 @@ static int lpt_init_rd(struct ubifs_info *c)
 {
 	int err, i;
 
-	c->ltab = vmalloc(sizeof(struct ubifs_lpt_lprops) * c->lpt_lebs);
+	c->ltab = vmalloc(array_size(sizeof(struct ubifs_lpt_lprops),
+				     c->lpt_lebs));
 	if (!c->ltab)
 		return -ENOMEM;
 
@@ -1691,7 +1693,8 @@ static int lpt_init_wr(struct ubifs_info *c)
 {
 	int err, i;
 
-	c->ltab_cmt = vmalloc(sizeof(struct ubifs_lpt_lprops) * c->lpt_lebs);
+	c->ltab_cmt = vmalloc(array_size(sizeof(struct ubifs_lpt_lprops),
+					 c->lpt_lebs));
 	if (!c->ltab_cmt)
 		return -ENOMEM;
 
* Unmerged path kernel/cgroup/cgroup-v1.c
diff --git a/kernel/power/swap.c b/kernel/power/swap.c
index 8c9a4819f798..b5308387eadd 100644
--- a/kernel/power/swap.c
+++ b/kernel/power/swap.c
@@ -602,7 +602,7 @@ static int save_image_lzo(struct swap_map_handle *handle,
 		goto out_clean;
 	}
 
-	data = vmalloc(sizeof(*data) * nr_threads);
+	data = vmalloc(array_size(nr_threads, sizeof(*data)));
 	if (!data) {
 		printk(KERN_ERR "PM: Failed to allocate LZO data\n");
 		ret = -ENOMEM;
@@ -1086,14 +1086,14 @@ static int load_image_lzo(struct swap_map_handle *handle,
 	nr_threads = num_online_cpus() - 1;
 	nr_threads = clamp_val(nr_threads, 1, LZO_THREADS);
 
-	page = vmalloc(sizeof(*page) * LZO_MAX_RD_PAGES);
+	page = vmalloc(array_size(LZO_MAX_RD_PAGES, sizeof(*page)));
 	if (!page) {
 		printk(KERN_ERR "PM: Failed to allocate LZO page\n");
 		ret = -ENOMEM;
 		goto out_clean;
 	}
 
-	data = vmalloc(sizeof(*data) * nr_threads);
+	data = vmalloc(array_size(nr_threads, sizeof(*data)));
 	if (!data) {
 		printk(KERN_ERR "PM: Failed to allocate LZO data\n");
 		ret = -ENOMEM;
* Unmerged path kernel/rcu/rcutorture.c
* Unmerged path kernel/trace/tracing_map.c
* Unmerged path mm/percpu-stats.c
diff --git a/net/bridge/netfilter/ebtables.c b/net/bridge/netfilter/ebtables.c
index 71f90ec4f774..7f552ef423d8 100644
--- a/net/bridge/netfilter/ebtables.c
+++ b/net/bridge/netfilter/ebtables.c
@@ -900,12 +900,13 @@ static int translate_table(struct net *net, const char *name,
 		/* this will get free'd in do_replace()/ebt_register_table()
 		   if an error occurs */
 		newinfo->chainstack =
-			vmalloc(nr_cpu_ids * sizeof(*(newinfo->chainstack)));
+			vmalloc(array_size(nr_cpu_ids,
+					   sizeof(*(newinfo->chainstack))));
 		if (!newinfo->chainstack)
 			return -ENOMEM;
 		for_each_possible_cpu(i) {
 			newinfo->chainstack[i] =
-			  vmalloc(udc_cnt * sizeof(*(newinfo->chainstack[0])));
+			  vmalloc(array_size(udc_cnt, sizeof(*(newinfo->chainstack[0]))));
 			if (!newinfo->chainstack[i]) {
 				while (i)
 					vfree(newinfo->chainstack[--i]);
@@ -915,7 +916,7 @@ static int translate_table(struct net *net, const char *name,
 			}
 		}
 
-		cl_s = vmalloc(udc_cnt * sizeof(*cl_s));
+		cl_s = vmalloc(array_size(udc_cnt, sizeof(*cl_s)));
 		if (!cl_s)
 			return -ENOMEM;
 		i = 0; /* the i'th udc */
@@ -1299,7 +1300,7 @@ static int do_update_counters(struct net *net, const char *name,
 	if (num_counters == 0)
 		return -EINVAL;
 
-	tmp = vmalloc(num_counters * sizeof(*tmp));
+	tmp = vmalloc(array_size(num_counters, sizeof(*tmp)));
 	if (!tmp)
 		return -ENOMEM;
 
@@ -1420,7 +1421,7 @@ static int copy_counters_to_user(struct ebt_table *t,
 		return -EINVAL;
 	}
 
-	counterstmp = vmalloc(nentries * sizeof(*counterstmp));
+	counterstmp = vmalloc(array_size(nentries, sizeof(*counterstmp)));
 	if (!counterstmp)
 		return -ENOMEM;
 
diff --git a/net/netfilter/ipvs/ip_vs_conn.c b/net/netfilter/ipvs/ip_vs_conn.c
index f014f470366e..d7ffcbbe882a 100644
--- a/net/netfilter/ipvs/ip_vs_conn.c
+++ b/net/netfilter/ipvs/ip_vs_conn.c
@@ -1367,7 +1367,8 @@ int __init ip_vs_conn_init(void)
 	/*
 	 * Allocate the connection hash table and initialize its list heads
 	 */
-	ip_vs_conn_tab = vmalloc(ip_vs_conn_tab_size * sizeof(*ip_vs_conn_tab));
+	ip_vs_conn_tab = vmalloc(array_size(ip_vs_conn_tab_size,
+					    sizeof(*ip_vs_conn_tab)));
 	if (!ip_vs_conn_tab)
 		return -ENOMEM;
 
diff --git a/sound/core/seq/seq_memory.c b/sound/core/seq/seq_memory.c
index f769c631d6a3..ba24bdd8f965 100644
--- a/sound/core/seq/seq_memory.c
+++ b/sound/core/seq/seq_memory.c
@@ -388,7 +388,8 @@ int snd_seq_pool_init(struct snd_seq_pool *pool)
 	if (snd_BUG_ON(!pool))
 		return -EINVAL;
 
-	cellptr = vmalloc(sizeof(struct snd_seq_event_cell) * pool->size);
+	cellptr = vmalloc(array_size(sizeof(struct snd_seq_event_cell),
+				     pool->size));
 	if (!cellptr)
 		return -ENOMEM;
 
diff --git a/sound/pci/cs46xx/dsp_spos.c b/sound/pci/cs46xx/dsp_spos.c
index f03bbd0eb027..7a2fc1bdd2f8 100644
--- a/sound/pci/cs46xx/dsp_spos.c
+++ b/sound/pci/cs46xx/dsp_spos.c
@@ -240,8 +240,9 @@ struct dsp_spos_instance *cs46xx_dsp_spos_create (struct snd_cs46xx * chip)
 		return NULL;
 
 	/* better to use vmalloc for this big table */
-	ins->symbol_table.symbols = vmalloc(sizeof(struct dsp_symbol_entry) *
-					    DSP_MAX_SYMBOLS);
+	ins->symbol_table.symbols =
+		vmalloc(array_size(DSP_MAX_SYMBOLS,
+				   sizeof(struct dsp_symbol_entry)));
 	ins->code.data = kmalloc(DSP_CODE_BYTE_SIZE, GFP_KERNEL);
 	ins->modules = kmalloc(sizeof(struct dsp_module_desc) * DSP_MAX_MODULES, GFP_KERNEL);
 	if (!ins->symbol_table.symbols || !ins->code.data || !ins->modules) {
diff --git a/sound/pci/emu10k1/emu10k1_main.c b/sound/pci/emu10k1/emu10k1_main.c
index 18267de3a269..61f85ff91cd9 100644
--- a/sound/pci/emu10k1/emu10k1_main.c
+++ b/sound/pci/emu10k1/emu10k1_main.c
@@ -1941,9 +1941,10 @@ int snd_emu10k1_create(struct snd_card *card,
 		(unsigned long)emu->ptb_pages.addr,
 		(unsigned long)(emu->ptb_pages.addr + emu->ptb_pages.bytes));
 
-	emu->page_ptr_table = vmalloc(emu->max_cache_pages * sizeof(void *));
-	emu->page_addr_table = vmalloc(emu->max_cache_pages *
-				       sizeof(unsigned long));
+	emu->page_ptr_table = vmalloc(array_size(sizeof(void *),
+						 emu->max_cache_pages));
+	emu->page_addr_table = vmalloc(array_size(sizeof(unsigned long),
+						  emu->max_cache_pages));
 	if (emu->page_ptr_table == NULL || emu->page_addr_table == NULL) {
 		err = -ENOMEM;
 		goto error;
@@ -2099,7 +2100,7 @@ static int alloc_pm_buffer(struct snd_emu10k1 *emu)
 	size = ARRAY_SIZE(saved_regs);
 	if (emu->audigy)
 		size += ARRAY_SIZE(saved_regs_audigy);
-	emu->saved_ptr = vmalloc(4 * NUM_G * size);
+	emu->saved_ptr = vmalloc(array3_size(4, NUM_G, size));
 	if (!emu->saved_ptr)
 		return -ENOMEM;
 	if (snd_emu10k1_efx_alloc_pm_buffer(emu) < 0)
diff --git a/sound/pci/emu10k1/emufx.c b/sound/pci/emu10k1/emufx.c
index 71331b65fe91..91781aa022b8 100644
--- a/sound/pci/emu10k1/emufx.c
+++ b/sound/pci/emu10k1/emufx.c
@@ -2698,7 +2698,7 @@ int snd_emu10k1_efx_alloc_pm_buffer(struct snd_emu10k1 *emu)
 	if (! emu->tram_val_saved || ! emu->tram_addr_saved)
 		return -ENOMEM;
 	len = emu->audigy ? 2 * 1024 : 2 * 512;
-	emu->saved_icode = vmalloc(len * 4);
+	emu->saved_icode = vmalloc(array_size(len, 4));
 	if (! emu->saved_icode)
 		return -ENOMEM;
 	return 0;
diff --git a/sound/pci/emu10k1/p16v.c b/sound/pci/emu10k1/p16v.c
index 91bc9ab6a942..672017cac4c7 100644
--- a/sound/pci/emu10k1/p16v.c
+++ b/sound/pci/emu10k1/p16v.c
@@ -871,7 +871,7 @@ int snd_p16v_mixer(struct snd_emu10k1 *emu)
 
 int snd_p16v_alloc_pm_buffer(struct snd_emu10k1 *emu)
 {
-	emu->p16v_saved = vmalloc(NUM_CHS * 4 * 0x80);
+	emu->p16v_saved = vmalloc(array_size(NUM_CHS * 4, 0x80));
 	if (! emu->p16v_saved)
 		return -ENOMEM;
 	return 0;
diff --git a/sound/pci/maestro3.c b/sound/pci/maestro3.c
index 4576d3bad3c1..62962178a9d7 100644
--- a/sound/pci/maestro3.c
+++ b/sound/pci/maestro3.c
@@ -2657,7 +2657,10 @@ snd_m3_create(struct snd_card *card, struct pci_dev *pci,
 	chip->irq = pci->irq;
 
 #ifdef CONFIG_PM_SLEEP
-	chip->suspend_mem = vmalloc(sizeof(u16) * (REV_B_CODE_MEMORY_LENGTH + REV_B_DATA_MEMORY_LENGTH));
+	chip->suspend_mem =
+		vmalloc(array_size(sizeof(u16),
+				   REV_B_CODE_MEMORY_LENGTH +
+					REV_B_DATA_MEMORY_LENGTH));
 	if (chip->suspend_mem == NULL)
 		dev_warn(card->dev, "can't allocate apm buffer\n");
 #endif
diff --git a/sound/pci/trident/trident_main.c b/sound/pci/trident/trident_main.c
index fdeea03b4fa7..5523e193d556 100644
--- a/sound/pci/trident/trident_main.c
+++ b/sound/pci/trident/trident_main.c
@@ -3362,7 +3362,9 @@ static int snd_trident_tlb_alloc(struct snd_trident *trident)
 	trident->tlb.entries = (__le32 *)ALIGN((unsigned long)trident->tlb.buffer.area, SNDRV_TRIDENT_MAX_PAGES * 4);
 	trident->tlb.entries_dmaaddr = ALIGN(trident->tlb.buffer.addr, SNDRV_TRIDENT_MAX_PAGES * 4);
 	/* allocate shadow TLB page table (virtual addresses) */
-	trident->tlb.shadow_entries = vmalloc(SNDRV_TRIDENT_MAX_PAGES*sizeof(unsigned long));
+	trident->tlb.shadow_entries =
+		vmalloc(array_size(SNDRV_TRIDENT_MAX_PAGES,
+				   sizeof(unsigned long)));
 	if (!trident->tlb.shadow_entries)
 		return -ENOMEM;
 
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 365126b76acd..425300065d79 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2991,7 +2991,8 @@ static long kvm_vm_ioctl(struct file *filp,
 			goto out;
 		if (routing.nr) {
 			r = -ENOMEM;
-			entries = vmalloc(routing.nr * sizeof(*entries));
+			entries = vmalloc(array_size(sizeof(*entries),
+						     routing.nr));
 			if (!entries)
 				goto out;
 			r = -EFAULT;
