{net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [infiniband] {net, ib}/mlx5: Move Page fault EQ and ODP logic to RDMA (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 99.10%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit d5d284b829a6eb7127df24d1bd3896a698981e62
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d5d284b8.failed

Use the new generic EQ API to move all ODP RDMA data structures and logic
form mlx5 core driver into mlx5_ib driver.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Acked-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit d5d284b829a6eb7127df24d1bd3896a698981e62)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
#	drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
#	include/linux/mlx5/driver.h
#	include/linux/mlx5/eq.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 8f90751c4e18,fcf4a0328a90..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -5152,7 -6040,12 +5152,16 @@@ static int mlx5_ib_stage_odp_init(struc
  	return mlx5_ib_odp_init_one(dev);
  }
  
++<<<<<<< HEAD
 +static int mlx5_ib_stage_counters_init(struct mlx5_ib_dev *dev)
++=======
+ void mlx5_ib_stage_odp_cleanup(struct mlx5_ib_dev *dev)
+ {
+ 	mlx5_ib_odp_cleanup_one(dev);
+ }
+ 
+ int mlx5_ib_stage_counters_init(struct mlx5_ib_dev *dev)
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  {
  	if (MLX5_CAP_GEN(dev->mdev, max_qp_cnt)) {
  		dev->ib_dev.get_hw_stats	= mlx5_ib_get_hw_stats;
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3c57e5c58ad3,27999fd32356..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -766,8 -813,85 +766,87 @@@ struct mlx5_ib_multiport_info 
  	bool unaffiliate;
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_ib_flow_action {
+ 	struct ib_flow_action		ib_action;
+ 	union {
+ 		struct {
+ 			u64			    ib_flags;
+ 			struct mlx5_accel_esp_xfrm *ctx;
+ 		} esp_aes_gcm;
+ 		struct {
+ 			struct mlx5_ib_dev *dev;
+ 			u32 sub_type;
+ 			u32 action_id;
+ 		} flow_action_raw;
+ 	};
+ };
+ 
+ struct mlx5_memic {
+ 	struct mlx5_core_dev *dev;
+ 	spinlock_t		memic_lock;
+ 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
+ };
+ 
+ struct mlx5_read_counters_attr {
+ 	struct mlx5_fc *hw_cntrs_hndl;
+ 	u64 *out;
+ 	u32 flags;
+ };
+ 
+ enum mlx5_ib_counters_type {
+ 	MLX5_IB_COUNTERS_FLOW,
+ };
+ 
+ struct mlx5_ib_mcounters {
+ 	struct ib_counters ibcntrs;
+ 	enum mlx5_ib_counters_type type;
+ 	/* number of counters supported for this counters type */
+ 	u32 counters_num;
+ 	struct mlx5_fc *hw_cntrs_hndl;
+ 	/* read function for this counters type */
+ 	int (*read_counters)(struct ib_device *ibdev,
+ 			     struct mlx5_read_counters_attr *read_attr);
+ 	/* max index set as part of create_flow */
+ 	u32 cntrs_max_index;
+ 	/* number of counters data entries (<description,index> pair) */
+ 	u32 ncounters;
+ 	/* counters data array for descriptions and indexes */
+ 	struct mlx5_ib_flow_counters_desc *counters_data;
+ 	/* protects access to mcounters internal data */
+ 	struct mutex mcntrs_mutex;
+ };
+ 
+ static inline struct mlx5_ib_mcounters *
+ to_mcounters(struct ib_counters *ibcntrs)
+ {
+ 	return container_of(ibcntrs, struct mlx5_ib_mcounters, ibcntrs);
+ }
+ 
+ int parse_flow_flow_action(struct mlx5_ib_flow_action *maction,
+ 			   bool is_egress,
+ 			   struct mlx5_flow_act *action);
+ struct mlx5_ib_lb_state {
+ 	/* protect the user_td */
+ 	struct mutex		mutex;
+ 	u32			user_td;
+ 	int			qps;
+ 	bool			enabled;
+ };
+ 
+ struct mlx5_ib_pf_eq {
+ 	struct mlx5_ib_dev *dev;
+ 	struct mlx5_eq *core;
+ 	struct work_struct work;
+ 	spinlock_t lock; /* Pagefaults spinlock */
+ 	struct workqueue_struct *wq;
+ 	mempool_t *pool;
+ };
+ 
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  struct mlx5_ib_dev {
  	struct ib_device		ib_dev;
 -	const struct uverbs_object_tree_def *driver_trees[7];
  	struct mlx5_core_dev		*mdev;
  	struct mlx5_roce		roce[MLX5_MAX_PORTS];
  	int				num_ports;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index e502527fbe4d,895401609c63..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -51,16 -57,29 +51,32 @@@ enum 
  };
  
  enum {
- 	MLX5_NUM_SPARE_EQE	= 0x80,
- 	MLX5_NUM_ASYNC_EQE	= 0x1000,
- 	MLX5_NUM_CMD_EQE	= 32,
- 	MLX5_NUM_PF_DRAIN	= 64,
+ 	MLX5_EQ_DOORBEL_OFFSET	= 0x40,
  };
  
- enum {
- 	MLX5_EQ_DOORBEL_OFFSET	= 0x40,
++<<<<<<< HEAD
++=======
+ struct mlx5_irq_info {
+ 	cpumask_var_t mask;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	void *context; /* dev_id provided to request_irq */
  };
  
+ struct mlx5_eq_table {
+ 	struct list_head        comp_eqs_list;
+ 	struct mlx5_eq          pages_eq;
+ 	struct mlx5_eq          async_eq;
+ 	struct mlx5_eq	        cmd_eq;
+ 
+ 	struct mutex            lock; /* sync async eqs creations */
+ 	int			num_comp_vectors;
+ 	struct mlx5_irq_info	*irq_info;
+ #ifdef CONFIG_RFS_ACCEL
+ 	struct cpu_rmap         *rmap;
+ #endif
+ };
+ 
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  #define MLX5_ASYNC_EVENT_MASK ((1ull << MLX5_EVENT_TYPE_PATH_MIG)	    | \
  			       (1ull << MLX5_EVENT_TYPE_COMM_EST)	    | \
  			       (1ull << MLX5_EVENT_TYPE_SQ_DRAINED)	    | \
@@@ -193,189 -212,6 +209,192 @@@ static void eq_update_ci(struct mlx5_e
  	mb();
  }
  
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +static void eqe_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_pagefault *pfault = container_of(work,
 +						     struct mlx5_pagefault,
 +						     work);
 +	struct mlx5_eq *eq = pfault->eq;
 +
 +	mlx5_core_page_fault(eq->dev, pfault);
 +	mempool_free(pfault, eq->pf_ctx.pool);
 +}
 +
 +static void eq_pf_process(struct mlx5_eq *eq)
 +{
 +	struct mlx5_core_dev *dev = eq->dev;
 +	struct mlx5_eqe_page_fault *pf_eqe;
 +	struct mlx5_pagefault *pfault;
 +	struct mlx5_eqe *eqe;
 +	int set_ci = 0;
 +
 +	while ((eqe = next_eqe_sw(eq))) {
 +		pfault = mempool_alloc(eq->pf_ctx.pool, GFP_ATOMIC);
 +		if (!pfault) {
 +			schedule_work(&eq->pf_ctx.work);
 +			break;
 +		}
 +
 +		dma_rmb();
 +		pf_eqe = &eqe->data.page_fault;
 +		pfault->event_subtype = eqe->sub_type;
 +		pfault->bytes_committed = be32_to_cpu(pf_eqe->bytes_committed);
 +
 +		mlx5_core_dbg(dev,
 +			      "PAGE_FAULT: subtype: 0x%02x, bytes_committed: 0x%06x\n",
 +			      eqe->sub_type, pfault->bytes_committed);
 +
 +		switch (eqe->sub_type) {
 +		case MLX5_PFAULT_SUBTYPE_RDMA:
 +			/* RDMA based event */
 +			pfault->type =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) >> 24;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->rdma.pftype_token) &
 +				MLX5_24BIT_MASK;
 +			pfault->rdma.r_key =
 +				be32_to_cpu(pf_eqe->rdma.r_key);
 +			pfault->rdma.packet_size =
 +				be16_to_cpu(pf_eqe->rdma.packet_length);
 +			pfault->rdma.rdma_op_len =
 +				be32_to_cpu(pf_eqe->rdma.rdma_op_len);
 +			pfault->rdma.rdma_va =
 +				be64_to_cpu(pf_eqe->rdma.rdma_va);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, r_key: 0x%08x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->rdma.r_key);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: rdma_op_len: 0x%08x, rdma_va: 0x%016llx\n",
 +				      pfault->rdma.rdma_op_len,
 +				      pfault->rdma.rdma_va);
 +			break;
 +
 +		case MLX5_PFAULT_SUBTYPE_WQE:
 +			/* WQE based event */
 +			pfault->type =
 +				(be32_to_cpu(pf_eqe->wqe.pftype_wq) >> 24) & 0x7;
 +			pfault->token =
 +				be32_to_cpu(pf_eqe->wqe.token);
 +			pfault->wqe.wq_num =
 +				be32_to_cpu(pf_eqe->wqe.pftype_wq) &
 +				MLX5_24BIT_MASK;
 +			pfault->wqe.wqe_index =
 +				be16_to_cpu(pf_eqe->wqe.wqe_index);
 +			pfault->wqe.packet_size =
 +				be16_to_cpu(pf_eqe->wqe.packet_length);
 +			mlx5_core_dbg(dev,
 +				      "PAGE_FAULT: type:0x%x, token: 0x%06x, wq_num: 0x%06x, wqe_index: 0x%04x\n",
 +				      pfault->type, pfault->token,
 +				      pfault->wqe.wq_num,
 +				      pfault->wqe.wqe_index);
 +			break;
 +
 +		default:
 +			mlx5_core_warn(dev,
 +				       "Unsupported page fault event sub-type: 0x%02hhx\n",
 +				       eqe->sub_type);
 +			/* Unsupported page faults should still be
 +			 * resolved by the page fault handler
 +			 */
 +		}
 +
 +		pfault->eq = eq;
 +		INIT_WORK(&pfault->work, eqe_pf_action);
 +		queue_work(eq->pf_ctx.wq, &pfault->work);
 +
 +		++eq->cons_index;
 +		++set_ci;
 +
 +		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
 +			eq_update_ci(eq, 0);
 +			set_ci = 0;
 +		}
 +	}
 +
 +	eq_update_ci(eq, 1);
 +}
 +
 +static irqreturn_t mlx5_eq_pf_int(int irq, void *eq_ptr)
 +{
 +	struct mlx5_eq *eq = eq_ptr;
 +	unsigned long flags;
 +
 +	if (spin_trylock_irqsave(&eq->pf_ctx.lock, flags)) {
 +		eq_pf_process(eq);
 +		spin_unlock_irqrestore(&eq->pf_ctx.lock, flags);
 +	} else {
 +		schedule_work(&eq->pf_ctx.work);
 +	}
 +
 +	return IRQ_HANDLED;
 +}
 +
 +/* mempool_refill() was proposed but unfortunately wasn't accepted
 + * http://lkml.iu.edu/hypermail/linux/kernel/1512.1/05073.html
 + * Chip workaround.
 + */
 +static void mempool_refill(mempool_t *pool)
 +{
 +	while (pool->curr_nr < pool->min_nr)
 +		mempool_free(mempool_alloc(pool, GFP_KERNEL), pool);
 +}
 +
 +static void eq_pf_action(struct work_struct *work)
 +{
 +	struct mlx5_eq *eq = container_of(work, struct mlx5_eq, pf_ctx.work);
 +
 +	mempool_refill(eq->pf_ctx.pool);
 +
 +	spin_lock_irq(&eq->pf_ctx.lock);
 +	eq_pf_process(eq);
 +	spin_unlock_irq(&eq->pf_ctx.lock);
 +}
 +
 +static int init_pf_ctx(struct mlx5_eq_pagefault *pf_ctx, const char *name)
 +{
 +	spin_lock_init(&pf_ctx->lock);
 +	INIT_WORK(&pf_ctx->work, eq_pf_action);
 +
 +	pf_ctx->wq = alloc_workqueue(name,
 +				     WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
 +				     MLX5_NUM_CMD_EQE);
 +	if (!pf_ctx->wq)
 +		return -ENOMEM;
 +
 +	pf_ctx->pool = mempool_create_kmalloc_pool
 +		(MLX5_NUM_PF_DRAIN, sizeof(struct mlx5_pagefault));
 +	if (!pf_ctx->pool)
 +		goto err_wq;
 +
 +	return 0;
 +err_wq:
 +	destroy_workqueue(pf_ctx->wq);
 +	return -ENOMEM;
 +}
 +
 +int mlx5_core_page_fault_resume(struct mlx5_core_dev *dev, u32 token,
 +				u32 wq_num, u8 type, int error)
 +{
 +	u32 out[MLX5_ST_SZ_DW(page_fault_resume_out)] = {0};
 +	u32 in[MLX5_ST_SZ_DW(page_fault_resume_in)]   = {0};
 +
 +	MLX5_SET(page_fault_resume_in, in, opcode,
 +		 MLX5_CMD_OP_PAGE_FAULT_RESUME);
 +	MLX5_SET(page_fault_resume_in, in, error, !!error);
 +	MLX5_SET(page_fault_resume_in, in, page_fault_type, type);
 +	MLX5_SET(page_fault_resume_in, in, wq_number, wq_num);
 +	MLX5_SET(page_fault_resume_in, in, token, token);
 +
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 +}
 +EXPORT_SYMBOL_GPL(mlx5_core_page_fault_resume);
 +#endif
 +
++=======
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  static void general_event_handler(struct mlx5_core_dev *dev,
  				  struct mlx5_eqe *eqe)
  {
@@@ -877,30 -788,10 +896,34 @@@ int mlx5_start_eqs(struct mlx5_core_de
  		goto err2;
  	}
  
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	if (MLX5_CAP_GEN(dev, pg)) {
 +		err = mlx5_create_map_eq(dev, &table->pfault_eq,
 +					 MLX5_EQ_VEC_PFAULT,
 +					 MLX5_NUM_ASYNC_EQE,
 +					 1 << MLX5_EVENT_TYPE_PAGE_FAULT,
 +					 "mlx5_page_fault_eq",
 +					 MLX5_EQ_TYPE_PF);
 +		if (err) {
 +			mlx5_core_warn(dev, "failed to create page fault EQ %d\n",
 +				       err);
 +			goto err3;
 +		}
 +	}
 +
 +	return err;
 +err3:
 +	mlx5_destroy_unmap_eq(dev, &table->pages_eq);
 +#else
  	return err;
 +#endif
++=======
++	return err;
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  
  err2:
 -	destroy_async_eq(dev, &table->async_eq);
 +	mlx5_destroy_unmap_eq(dev, &table->async_eq);
  
  err1:
  	mlx5_cmd_use_polling(dev);
@@@ -908,21 -799,12 +931,25 @@@
  	return err;
  }
  
 -static void destroy_async_eqs(struct mlx5_core_dev *dev)
 +void mlx5_stop_eqs(struct mlx5_core_dev *dev)
  {
 -	struct mlx5_eq_table *table = dev->priv.eq_table;
 +	struct mlx5_eq_table *table = &dev->priv.eq_table;
  	int err;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	if (MLX5_CAP_GEN(dev, pg)) {
 +		err = mlx5_destroy_unmap_eq(dev, &table->pfault_eq);
 +		if (err)
 +			mlx5_core_err(dev, "failed to destroy page fault eq, err(%d)\n",
 +				      err);
 +	}
 +#endif
 +
 +	err = mlx5_destroy_unmap_eq(dev, &table->pages_eq);
++=======
+ 	err = destroy_async_eq(dev, &table->pages_eq);
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  	if (err)
  		mlx5_core_err(dev, "failed to destroy pages eq, err(%d)\n",
  			      err);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 2ac07968015d,21727d9eeb84..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@@ -94,13 -95,13 +94,16 @@@ int mlx5_query_hca_caps(struct mlx5_cor
  int mlx5_query_board_id(struct mlx5_core_dev *dev);
  int mlx5_cmd_init_hca(struct mlx5_core_dev *dev, uint32_t *sw_owner_id);
  int mlx5_cmd_teardown_hca(struct mlx5_core_dev *dev);
 -int mlx5_cmd_force_teardown_hca(struct mlx5_core_dev *dev);
 -int mlx5_cmd_fast_teardown_hca(struct mlx5_core_dev *dev);
 -
  void mlx5_core_event(struct mlx5_core_dev *dev, enum mlx5_dev_event event,
  		     unsigned long param);
++<<<<<<< HEAD
 +void mlx5_core_page_fault(struct mlx5_core_dev *dev,
 +			  struct mlx5_pagefault *pfault);
 +void mlx5_pps_event(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe);
++=======
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  void mlx5_port_module_event(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe);
 -void mlx5_enter_error_state(struct mlx5_core_dev *dev, bool force);
 +void mlx5_enter_error_state(struct mlx5_core_dev *dev);
  void mlx5_disable_device(struct mlx5_core_dev *dev);
  void mlx5_recover_device(struct mlx5_core_dev *dev);
  int mlx5_sriov_init(struct mlx5_core_dev *dev);
diff --cc include/linux/mlx5/driver.h
index af4fa7465c37,f41e6713df10..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -580,7 -510,7 +580,11 @@@ struct mlx5_fc_stats 
  struct mlx5_mpfs;
  struct mlx5_eswitch;
  struct mlx5_lag;
++<<<<<<< HEAD
 +struct mlx5_pagefault;
++=======
+ struct mlx5_eq_table;
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  
  struct mlx5_rate_limit {
  	u32			rate;
@@@ -720,44 -642,6 +717,47 @@@ enum mlx5_pagefault_type_flags 
  	MLX5_PFAULT_RDMA      = 1 << 2,
  };
  
++<<<<<<< HEAD
 +/* Contains the details of a pagefault. */
 +struct mlx5_pagefault {
 +	u32			bytes_committed;
 +	u32			token;
 +	u8			event_subtype;
 +	u8			type;
 +	union {
 +		/* Initiator or send message responder pagefault details. */
 +		struct {
 +			/* Received packet size, only valid for responders. */
 +			u32	packet_size;
 +			/*
 +			 * Number of resource holding WQE, depends on type.
 +			 */
 +			u32	wq_num;
 +			/*
 +			 * WQE index. Refers to either the send queue or
 +			 * receive queue, according to event_subtype.
 +			 */
 +			u16	wqe_index;
 +		} wqe;
 +		/* RDMA responder pagefault details */
 +		struct {
 +			u32	r_key;
 +			/*
 +			 * Received packet size, minimal size page fault
 +			 * resolution required for forward progress.
 +			 */
 +			u32	packet_size;
 +			u32	rdma_op_len;
 +			u64	rdma_va;
 +		} rdma;
 +	};
 +
 +	struct mlx5_eq	       *eq;
 +	struct work_struct	work;
 +};
 +
++=======
++>>>>>>> d5d284b829a6 ({net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA)
  struct mlx5_td {
  	struct list_head tirs_list;
  	u32              tdn;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
* Unmerged path include/linux/mlx5/eq.h
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index ab528aa48815..e590095611c3 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -37,6 +37,46 @@
 #include "mlx5_ib.h"
 #include "cmd.h"
 
+#include <linux/mlx5/eq.h>
+
+/* Contains the details of a pagefault. */
+struct mlx5_pagefault {
+	u32			bytes_committed;
+	u32			token;
+	u8			event_subtype;
+	u8			type;
+	union {
+		/* Initiator or send message responder pagefault details. */
+		struct {
+			/* Received packet size, only valid for responders. */
+			u32	packet_size;
+			/*
+			 * Number of resource holding WQE, depends on type.
+			 */
+			u32	wq_num;
+			/*
+			 * WQE index. Refers to either the send queue or
+			 * receive queue, according to event_subtype.
+			 */
+			u16	wqe_index;
+		} wqe;
+		/* RDMA responder pagefault details */
+		struct {
+			u32	r_key;
+			/*
+			 * Received packet size, minimal size page fault
+			 * resolution required for forward progress.
+			 */
+			u32	packet_size;
+			u32	rdma_op_len;
+			u64	rdma_va;
+		} rdma;
+	};
+
+	struct mlx5_ib_pf_eq	*eq;
+	struct work_struct	work;
+};
+
 #define MAX_PREFETCH_LEN (4*1024*1024U)
 
 /* Timeout in ms to wait for an active mmu notifier to complete when handling
@@ -297,14 +337,20 @@ static void mlx5_ib_page_fault_resume(struct mlx5_ib_dev *dev,
 {
 	int wq_num = pfault->event_subtype == MLX5_PFAULT_SUBTYPE_WQE ?
 		     pfault->wqe.wq_num : pfault->token;
-	int ret = mlx5_core_page_fault_resume(dev->mdev,
-					      pfault->token,
-					      wq_num,
-					      pfault->type,
-					      error);
-	if (ret)
-		mlx5_ib_err(dev, "Failed to resolve the page fault on WQ 0x%x\n",
-			    wq_num);
+	u32 out[MLX5_ST_SZ_DW(page_fault_resume_out)] = { };
+	u32 in[MLX5_ST_SZ_DW(page_fault_resume_in)]   = { };
+	int err;
+
+	MLX5_SET(page_fault_resume_in, in, opcode, MLX5_CMD_OP_PAGE_FAULT_RESUME);
+	MLX5_SET(page_fault_resume_in, in, page_fault_type, pfault->type);
+	MLX5_SET(page_fault_resume_in, in, token, pfault->token);
+	MLX5_SET(page_fault_resume_in, in, wq_number, wq_num);
+	MLX5_SET(page_fault_resume_in, in, error, !!error);
+
+	err = mlx5_cmd_exec(dev->mdev, in, sizeof(in), out, sizeof(out));
+	if (err)
+		mlx5_ib_err(dev, "Failed to resolve the page fault on WQ 0x%x err %d\n",
+			    wq_num, err);
 }
 
 static struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,
@@ -1169,10 +1215,8 @@ static void mlx5_ib_mr_rdma_pfault_handler(struct mlx5_ib_dev *dev,
 	}
 }
 
-void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
-		    struct mlx5_pagefault *pfault)
+static void mlx5_ib_pfault(struct mlx5_ib_dev *dev, struct mlx5_pagefault *pfault)
 {
-	struct mlx5_ib_dev *dev = context;
 	u8 event_subtype = pfault->event_subtype;
 
 	switch (event_subtype) {
@@ -1189,6 +1233,203 @@ void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
 	}
 }
 
+static void mlx5_ib_eqe_pf_action(struct work_struct *work)
+{
+	struct mlx5_pagefault *pfault = container_of(work,
+						     struct mlx5_pagefault,
+						     work);
+	struct mlx5_ib_pf_eq *eq = pfault->eq;
+
+	mlx5_ib_pfault(eq->dev, pfault);
+	mempool_free(pfault, eq->pool);
+}
+
+static void mlx5_ib_eq_pf_process(struct mlx5_ib_pf_eq *eq)
+{
+	struct mlx5_eqe_page_fault *pf_eqe;
+	struct mlx5_pagefault *pfault;
+	struct mlx5_eqe *eqe;
+	int cc = 0;
+
+	while ((eqe = mlx5_eq_get_eqe(eq->core, cc))) {
+		pfault = mempool_alloc(eq->pool, GFP_ATOMIC);
+		if (!pfault) {
+			schedule_work(&eq->work);
+			break;
+		}
+
+		pf_eqe = &eqe->data.page_fault;
+		pfault->event_subtype = eqe->sub_type;
+		pfault->bytes_committed = be32_to_cpu(pf_eqe->bytes_committed);
+
+		mlx5_ib_dbg(eq->dev,
+			    "PAGE_FAULT: subtype: 0x%02x, bytes_committed: 0x%06x\n",
+			    eqe->sub_type, pfault->bytes_committed);
+
+		switch (eqe->sub_type) {
+		case MLX5_PFAULT_SUBTYPE_RDMA:
+			/* RDMA based event */
+			pfault->type =
+				be32_to_cpu(pf_eqe->rdma.pftype_token) >> 24;
+			pfault->token =
+				be32_to_cpu(pf_eqe->rdma.pftype_token) &
+				MLX5_24BIT_MASK;
+			pfault->rdma.r_key =
+				be32_to_cpu(pf_eqe->rdma.r_key);
+			pfault->rdma.packet_size =
+				be16_to_cpu(pf_eqe->rdma.packet_length);
+			pfault->rdma.rdma_op_len =
+				be32_to_cpu(pf_eqe->rdma.rdma_op_len);
+			pfault->rdma.rdma_va =
+				be64_to_cpu(pf_eqe->rdma.rdma_va);
+			mlx5_ib_dbg(eq->dev,
+				    "PAGE_FAULT: type:0x%x, token: 0x%06x, r_key: 0x%08x\n",
+				    pfault->type, pfault->token,
+				    pfault->rdma.r_key);
+			mlx5_ib_dbg(eq->dev,
+				    "PAGE_FAULT: rdma_op_len: 0x%08x, rdma_va: 0x%016llx\n",
+				    pfault->rdma.rdma_op_len,
+				    pfault->rdma.rdma_va);
+			break;
+
+		case MLX5_PFAULT_SUBTYPE_WQE:
+			/* WQE based event */
+			pfault->type =
+				(be32_to_cpu(pf_eqe->wqe.pftype_wq) >> 24) & 0x7;
+			pfault->token =
+				be32_to_cpu(pf_eqe->wqe.token);
+			pfault->wqe.wq_num =
+				be32_to_cpu(pf_eqe->wqe.pftype_wq) &
+				MLX5_24BIT_MASK;
+			pfault->wqe.wqe_index =
+				be16_to_cpu(pf_eqe->wqe.wqe_index);
+			pfault->wqe.packet_size =
+				be16_to_cpu(pf_eqe->wqe.packet_length);
+			mlx5_ib_dbg(eq->dev,
+				    "PAGE_FAULT: type:0x%x, token: 0x%06x, wq_num: 0x%06x, wqe_index: 0x%04x\n",
+				    pfault->type, pfault->token,
+				    pfault->wqe.wq_num,
+				    pfault->wqe.wqe_index);
+			break;
+
+		default:
+			mlx5_ib_warn(eq->dev,
+				     "Unsupported page fault event sub-type: 0x%02hhx\n",
+				     eqe->sub_type);
+			/* Unsupported page faults should still be
+			 * resolved by the page fault handler
+			 */
+		}
+
+		pfault->eq = eq;
+		INIT_WORK(&pfault->work, mlx5_ib_eqe_pf_action);
+		queue_work(eq->wq, &pfault->work);
+
+		cc = mlx5_eq_update_cc(eq->core, ++cc);
+	}
+
+	mlx5_eq_update_ci(eq->core, cc, 1);
+}
+
+static irqreturn_t mlx5_ib_eq_pf_int(int irq, void *eq_ptr)
+{
+	struct mlx5_ib_pf_eq *eq = eq_ptr;
+	unsigned long flags;
+
+	if (spin_trylock_irqsave(&eq->lock, flags)) {
+		mlx5_ib_eq_pf_process(eq);
+		spin_unlock_irqrestore(&eq->lock, flags);
+	} else {
+		schedule_work(&eq->work);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/* mempool_refill() was proposed but unfortunately wasn't accepted
+ * http://lkml.iu.edu/hypermail/linux/kernel/1512.1/05073.html
+ * Cheap workaround.
+ */
+static void mempool_refill(mempool_t *pool)
+{
+	while (pool->curr_nr < pool->min_nr)
+		mempool_free(mempool_alloc(pool, GFP_KERNEL), pool);
+}
+
+static void mlx5_ib_eq_pf_action(struct work_struct *work)
+{
+	struct mlx5_ib_pf_eq *eq =
+		container_of(work, struct mlx5_ib_pf_eq, work);
+
+	mempool_refill(eq->pool);
+
+	spin_lock_irq(&eq->lock);
+	mlx5_ib_eq_pf_process(eq);
+	spin_unlock_irq(&eq->lock);
+}
+
+enum {
+	MLX5_IB_NUM_PF_EQE	= 0x1000,
+	MLX5_IB_NUM_PF_DRAIN	= 64,
+};
+
+static int
+mlx5_ib_create_pf_eq(struct mlx5_ib_dev *dev, struct mlx5_ib_pf_eq *eq)
+{
+	struct mlx5_eq_param param = {};
+	int err;
+
+	INIT_WORK(&eq->work, mlx5_ib_eq_pf_action);
+	spin_lock_init(&eq->lock);
+	eq->dev = dev;
+
+	eq->pool = mempool_create_kmalloc_pool(MLX5_IB_NUM_PF_DRAIN,
+					       sizeof(struct mlx5_pagefault));
+	if (!eq->pool)
+		return -ENOMEM;
+
+	eq->wq = alloc_workqueue("mlx5_ib_page_fault",
+				 WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
+				 MLX5_NUM_CMD_EQE);
+	if (!eq->wq) {
+		err = -ENOMEM;
+		goto err_mempool;
+	}
+
+	param = (struct mlx5_eq_param) {
+		.index = MLX5_EQ_PFAULT_IDX,
+		.mask = 1 << MLX5_EVENT_TYPE_PAGE_FAULT,
+		.nent = MLX5_IB_NUM_PF_EQE,
+		.context = eq,
+		.handler = mlx5_ib_eq_pf_int
+	};
+	eq->core = mlx5_eq_create_generic(dev->mdev, "mlx5_ib_page_fault_eq", &param);
+	if (IS_ERR(eq->core)) {
+		err = PTR_ERR(eq->core);
+		goto err_wq;
+	}
+
+	return 0;
+err_wq:
+	destroy_workqueue(eq->wq);
+err_mempool:
+	mempool_destroy(eq->pool);
+	return err;
+}
+
+static int
+mlx5_ib_destroy_pf_eq(struct mlx5_ib_dev *dev, struct mlx5_ib_pf_eq *eq)
+{
+	int err;
+
+	err = mlx5_eq_destroy_generic(dev->mdev, eq->core);
+	cancel_work_sync(&eq->work);
+	destroy_workqueue(eq->wq);
+	mempool_destroy(eq->pool);
+
+	return err;
+}
+
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent)
 {
 	if (!(ent->dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
@@ -1217,7 +1458,7 @@ void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent)
 
 int mlx5_ib_odp_init_one(struct mlx5_ib_dev *dev)
 {
-	int ret;
+	int ret = 0;
 
 	if (dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT) {
 		ret = mlx5_cmd_null_mkey(dev->mdev, &dev->null_mkey);
@@ -1227,7 +1468,20 @@ int mlx5_ib_odp_init_one(struct mlx5_ib_dev *dev)
 		}
 	}
 
-	return 0;
+	if (!MLX5_CAP_GEN(dev->mdev, pg))
+		return ret;
+
+	ret = mlx5_ib_create_pf_eq(dev, &dev->odp_pf_eq);
+
+	return ret;
+}
+
+void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *dev)
+{
+	if (!MLX5_CAP_GEN(dev->mdev, pg))
+		return;
+
+	mlx5_ib_destroy_pf_eq(dev, &dev->odp_pf_eq);
 }
 
 int mlx5_ib_odp_init(void)
@@ -1237,4 +1491,3 @@ int mlx5_ib_odp_init(void)
 
 	return 0;
 }
-
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/dev.c b/drivers/net/ethernet/mellanox/mlx5/core/dev.c
index 37ba7c78859d..7eedbea38a78 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/dev.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/dev.c
@@ -139,17 +139,6 @@ void mlx5_add_device(struct mlx5_interface *intf, struct mlx5_priv *priv)
 
 		spin_lock_irq(&priv->ctx_lock);
 		list_add_tail(&dev_ctx->list, &priv->ctx_list);
-
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-		if (dev_ctx->intf->pfault) {
-			if (priv->pfault) {
-				mlx5_core_err(dev, "multiple page fault handlers not supported");
-			} else {
-				priv->pfault_ctx = dev_ctx->context;
-				priv->pfault = dev_ctx->intf->pfault;
-			}
-		}
-#endif
 		spin_unlock_irq(&priv->ctx_lock);
 	}
 
@@ -179,15 +168,6 @@ void mlx5_remove_device(struct mlx5_interface *intf, struct mlx5_priv *priv)
 	if (!dev_ctx)
 		return;
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	spin_lock_irq(&priv->ctx_lock);
-	if (priv->pfault == dev_ctx->intf->pfault)
-		priv->pfault = NULL;
-	spin_unlock_irq(&priv->ctx_lock);
-
-	synchronize_srcu(&priv->pfault_srcu);
-#endif
-
 	spin_lock_irq(&priv->ctx_lock);
 	list_del(&dev_ctx->list);
 	spin_unlock_irq(&priv->ctx_lock);
@@ -447,20 +427,6 @@ void mlx5_core_event(struct mlx5_core_dev *dev, enum mlx5_dev_event event,
 	spin_unlock_irqrestore(&priv->ctx_lock, flags);
 }
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-void mlx5_core_page_fault(struct mlx5_core_dev *dev,
-			  struct mlx5_pagefault *pfault)
-{
-	struct mlx5_priv *priv = &dev->priv;
-	int srcu_idx;
-
-	srcu_idx = srcu_read_lock(&priv->pfault_srcu);
-	if (priv->pfault)
-		priv->pfault(dev, priv->pfault_ctx, pfault);
-	srcu_read_unlock(&priv->pfault_srcu, srcu_idx);
-}
-#endif
-
 void mlx5_dev_list_lock(void)
 {
 	mutex_lock(&mlx5_intf_mutex);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/main.c b/drivers/net/ethernet/mellanox/mlx5/core/main.c
index be9913773fa9..b4449c0a03f5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -1428,14 +1428,6 @@ static int init_one(struct pci_dev *pdev,
 	INIT_LIST_HEAD(&priv->waiting_events_list);
 	priv->is_accum_events = false;
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	err = init_srcu_struct(&priv->pfault_srcu);
-	if (err) {
-		dev_err(&pdev->dev, "init_srcu_struct failed with error code %d\n",
-			err);
-		goto clean_dev;
-	}
-#endif
 	mutex_init(&priv->bfregs.reg_head.lock);
 	mutex_init(&priv->bfregs.wc_head.lock);
 	INIT_LIST_HEAD(&priv->bfregs.reg_head.list);
@@ -1444,7 +1436,7 @@ static int init_one(struct pci_dev *pdev,
 	err = mlx5_pci_init(dev, priv);
 	if (err) {
 		dev_err(&pdev->dev, "mlx5_pci_init failed with error code %d\n", err);
-		goto clean_srcu;
+		goto clean_dev;
 	}
 
 	err = mlx5_health_init(dev);
@@ -1477,11 +1469,7 @@ clean_health:
 	mlx5_health_cleanup(dev);
 close_pci:
 	mlx5_pci_close(dev, priv);
-clean_srcu:
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	cleanup_srcu_struct(&priv->pfault_srcu);
 clean_dev:
-#endif
 	devlink_free(devlink);
 
 	return err;
@@ -1505,9 +1493,6 @@ static void remove_one(struct pci_dev *pdev)
 	mlx5_pagealloc_cleanup(dev);
 	mlx5_health_cleanup(dev);
 	mlx5_pci_close(dev, priv);
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	cleanup_srcu_struct(&priv->pfault_srcu);
-#endif
 	devlink_free(devlink);
 }
 
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
* Unmerged path include/linux/mlx5/driver.h
* Unmerged path include/linux/mlx5/eq.h
