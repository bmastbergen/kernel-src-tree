kvm/x86: Hyper-V SynIC tracepoints

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Andrey Smetanin <asmetanin@virtuozzo.com>
commit 18659a9cb1885d00dd428f8857f7f628e54a45ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/18659a9c.failed

Trace the following Hyper SynIC events:
* set msr
* set sint irq
* ack sint
* sint irq eoi

	Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
CC: Gleb Natapov <gleb@kernel.org>
CC: Paolo Bonzini <pbonzini@redhat.com>
CC: Roman Kagan <rkagan@virtuozzo.com>
CC: Denis V. Lunev <den@openvz.org>
CC: qemu-devel@nongnu.org
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 18659a9cb1885d00dd428f8857f7f628e54a45ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/trace.h
diff --cc arch/x86/kvm/hyperv.c
index 457dbf00fcf1,2d83d4598507..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -30,23 -33,628 +30,377 @@@
  
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ static inline u64 synic_read_sint(struct kvm_vcpu_hv_synic *synic, int sint)
+ {
+ 	return atomic64_read(&synic->sint[sint]);
+ }
+ 
+ static inline int synic_get_sint_vector(u64 sint_value)
+ {
+ 	if (sint_value & HV_SYNIC_SINT_MASKED)
+ 		return -1;
+ 	return sint_value & HV_SYNIC_SINT_VECTOR_MASK;
+ }
+ 
+ static bool synic_has_vector_connected(struct kvm_vcpu_hv_synic *synic,
+ 				      int vector)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool synic_has_vector_auto_eoi(struct kvm_vcpu_hv_synic *synic,
+ 				     int vector)
+ {
+ 	int i;
+ 	u64 sint_value;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		sint_value = synic_read_sint(synic, i);
+ 		if (synic_get_sint_vector(sint_value) == vector &&
+ 		    sint_value & HV_SYNIC_SINT_AUTO_EOI)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static int synic_set_sint(struct kvm_vcpu_hv_synic *synic, int sint,
+ 			  u64 data, bool host)
+ {
+ 	int vector;
+ 
+ 	vector = data & HV_SYNIC_SINT_VECTOR_MASK;
+ 	if (vector < 16 && !host)
+ 		return 1;
+ 	/*
+ 	 * Guest may configure multiple SINTs to use the same vector, so
+ 	 * we maintain a bitmap of vectors handled by synic, and a
+ 	 * bitmap of vectors with auto-eoi behavior.  The bitmaps are
+ 	 * updated here, and atomically queried on fast paths.
+ 	 */
+ 
+ 	atomic64_set(&synic->sint[sint], data);
+ 
+ 	if (synic_has_vector_connected(synic, vector))
+ 		__set_bit(vector, synic->vec_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->vec_bitmap);
+ 
+ 	if (synic_has_vector_auto_eoi(synic, vector))
+ 		__set_bit(vector, synic->auto_eoi_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->auto_eoi_bitmap);
+ 
+ 	/* Load SynIC vectors into EOI exit bitmap */
+ 	kvm_make_request(KVM_REQ_SCAN_IOAPIC, synic_to_vcpu(synic));
+ 	return 0;
+ }
+ 
+ static struct kvm_vcpu_hv_synic *synic_get(struct kvm *kvm, u32 vcpu_id)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	if (vcpu_id >= atomic_read(&kvm->online_vcpus))
+ 		return NULL;
+ 	vcpu = kvm_get_vcpu(kvm, vcpu_id);
+ 	if (!vcpu)
+ 		return NULL;
+ 	synic = vcpu_to_synic(vcpu);
+ 	return (synic->active) ? synic : NULL;
+ }
+ 
+ static void synic_clear_sint_msg_pending(struct kvm_vcpu_hv_synic *synic,
+ 					u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *msg;
+ 	struct hv_message_page *msg_page;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page)) {
+ 		vcpu_err(vcpu, "Hyper-V SynIC can't get msg page, gpa 0x%llx\n",
+ 			 gpa);
+ 		return;
+ 	}
+ 	msg_page = kmap_atomic(page);
+ 
+ 	msg = &msg_page->sint_message[sint];
+ 	msg->header.message_flags.msg_pending = 0;
+ 
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ }
+ 
+ static void kvm_hv_notify_acked_sint(struct kvm_vcpu *vcpu, u32 sint)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	int gsi, idx, stimers_pending;
+ 
+ 	trace_kvm_hv_notify_acked_sint(vcpu->vcpu_id, sint);
+ 
+ 	if (synic->msg_page & HV_SYNIC_SIMP_ENABLE)
+ 		synic_clear_sint_msg_pending(synic, sint);
+ 
+ 	/* Try to deliver pending Hyper-V SynIC timers messages */
+ 	stimers_pending = 0;
+ 	for (idx = 0; idx < ARRAY_SIZE(hv_vcpu->stimer); idx++) {
+ 		stimer = &hv_vcpu->stimer[idx];
+ 		if (stimer->msg_pending &&
+ 		    (stimer->config & HV_STIMER_ENABLE) &&
+ 		    HV_STIMER_SINT(stimer->config) == sint) {
+ 			set_bit(stimer->index,
+ 				hv_vcpu->stimer_pending_bitmap);
+ 			stimers_pending++;
+ 		}
+ 	}
+ 	if (stimers_pending)
+ 		kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 
+ 	idx = srcu_read_lock(&kvm->irq_srcu);
+ 	gsi = atomic_read(&synic->sint_to_gsi[sint]);
+ 	if (gsi != -1)
+ 		kvm_notify_acked_gsi(kvm, gsi);
+ 	srcu_read_unlock(&kvm->irq_srcu, idx);
+ }
+ 
+ static void synic_exit(struct kvm_vcpu_hv_synic *synic, u32 msr)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_vcpu_hv *hv_vcpu = &vcpu->arch.hyperv;
+ 
+ 	hv_vcpu->exit.type = KVM_EXIT_HYPERV_SYNIC;
+ 	hv_vcpu->exit.u.synic.msr = msr;
+ 	hv_vcpu->exit.u.synic.control = synic->control;
+ 	hv_vcpu->exit.u.synic.evt_page = synic->evt_page;
+ 	hv_vcpu->exit.u.synic.msg_page = synic->msg_page;
+ 
+ 	kvm_make_request(KVM_REQ_HV_EXIT, vcpu);
+ }
+ 
+ static int synic_set_msr(struct kvm_vcpu_hv_synic *synic,
+ 			 u32 msr, u64 data, bool host)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	trace_kvm_hv_synic_set_msr(vcpu->vcpu_id, msr, data, host);
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		synic->control = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		if (!host) {
+ 			ret = 1;
+ 			break;
+ 		}
+ 		synic->version = data;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		if (data & HV_SYNIC_SIEFP_ENABLE)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->evt_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		if (data & HV_SYNIC_SIMP_ENABLE)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->msg_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_EOM: {
+ 		int i;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ 		break;
+ 	}
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		ret = synic_set_sint(synic, msr - HV_X64_MSR_SINT0, data, host);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_get_msr(struct kvm_vcpu_hv_synic *synic, u32 msr, u64 *pdata)
+ {
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		*pdata = synic->control;
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		*pdata = synic->version;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		*pdata = synic->evt_page;
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		*pdata = synic->msg_page;
+ 		break;
+ 	case HV_X64_MSR_EOM:
+ 		*pdata = 0;
+ 		break;
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		*pdata = atomic64_read(&synic->sint[msr - HV_X64_MSR_SINT0]);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_lapic_irq irq;
+ 	int ret, vector;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint))
+ 		return -EINVAL;
+ 
+ 	vector = synic_get_sint_vector(synic_read_sint(synic, sint));
+ 	if (vector < 0)
+ 		return -ENOENT;
+ 
+ 	memset(&irq, 0, sizeof(irq));
+ 	irq.dest_id = kvm_apic_id(vcpu->arch.apic);
+ 	irq.dest_mode = APIC_DEST_PHYSICAL;
+ 	irq.delivery_mode = APIC_DM_FIXED;
+ 	irq.vector = vector;
+ 	irq.level = 1;
+ 
+ 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, NULL, &irq, NULL);
+ 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
+ 	return ret;
+ }
+ 
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vcpu_id, u32 sint)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	return synic_set_irq(synic, sint);
+ }
+ 
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector)
+ {
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	int i;
+ 
+ 	trace_kvm_hv_synic_send_eoi(vcpu->vcpu_id, vector);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ }
+ 
+ static int kvm_hv_set_sint_gsi(struct kvm *kvm, u32 vcpu_id, u32 sint, int gsi)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint_to_gsi))
+ 		return -EINVAL;
+ 
+ 	atomic_set(&synic->sint_to_gsi[sint], gsi);
+ 	return 0;
+ }
+ 
+ void kvm_hv_irq_routing_update(struct kvm *kvm)
+ {
+ 	struct kvm_irq_routing_table *irq_rt;
+ 	struct kvm_kernel_irq_routing_entry *e;
+ 	u32 gsi;
+ 
+ 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+ 					lockdep_is_held(&kvm->irq_lock));
+ 
+ 	for (gsi = 0; gsi < irq_rt->nr_rt_entries; gsi++) {
+ 		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+ 			if (e->type == KVM_IRQ_ROUTING_HV_SINT)
+ 				kvm_hv_set_sint_gsi(kvm, e->hv_sint.vcpu,
+ 						    e->hv_sint.sint, gsi);
+ 		}
+ 	}
+ }
+ 
+ static void synic_init(struct kvm_vcpu_hv_synic *synic)
+ {
+ 	int i;
+ 
+ 	memset(synic, 0, sizeof(*synic));
+ 	synic->version = HV_SYNIC_VERSION_1;
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		atomic64_set(&synic->sint[i], HV_SYNIC_SINT_MASKED);
+ 		atomic_set(&synic->sint_to_gsi[i], -1);
+ 	}
+ }
+ 
++>>>>>>> 18659a9cb188 (kvm/x86: Hyper-V SynIC tracepoints)
  static u64 get_time_ref_counter(struct kvm *kvm)
  {
 -	return div_u64(get_kernel_ns() + kvm->arch.kvmclock_offset, 100);
 -}
 -
 -static void stimer_mark_pending(struct kvm_vcpu_hv_stimer *stimer,
 -				bool vcpu_kick)
 -{
 -	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
 -
 -	set_bit(stimer->index,
 -		vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
 -	kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
 -	if (vcpu_kick)
 -		kvm_vcpu_kick(vcpu);
 -}
 -
 -static void stimer_cleanup(struct kvm_vcpu_hv_stimer *stimer)
 -{
 -	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
 -
 -	hrtimer_cancel(&stimer->timer);
 -	clear_bit(stimer->index,
 -		  vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
 -	stimer->msg_pending = false;
 -	stimer->exp_time = 0;
 -}
 -
 -static enum hrtimer_restart stimer_timer_callback(struct hrtimer *timer)
 -{
 -	struct kvm_vcpu_hv_stimer *stimer;
 -
 -	stimer = container_of(timer, struct kvm_vcpu_hv_stimer, timer);
 -	stimer_mark_pending(stimer, true);
 -
 -	return HRTIMER_NORESTART;
 -}
 -
 -/*
 - * stimer_start() assumptions:
 - * a) stimer->count is not equal to 0
 - * b) stimer->config has HV_STIMER_ENABLE flag
 - */
 -static int stimer_start(struct kvm_vcpu_hv_stimer *stimer)
 -{
 -	u64 time_now;
 -	ktime_t ktime_now;
 -
 -	time_now = get_time_ref_counter(stimer_to_vcpu(stimer)->kvm);
 -	ktime_now = ktime_get();
 -
 -	if (stimer->config & HV_STIMER_PERIODIC) {
 -		if (stimer->exp_time) {
 -			if (time_now >= stimer->exp_time) {
 -				u64 remainder;
 -
 -				div64_u64_rem(time_now - stimer->exp_time,
 -					      stimer->count, &remainder);
 -				stimer->exp_time =
 -					time_now + (stimer->count - remainder);
 -			}
 -		} else
 -			stimer->exp_time = time_now + stimer->count;
 -
 -		hrtimer_start(&stimer->timer,
 -			      ktime_add_ns(ktime_now,
 -					   100 * (stimer->exp_time - time_now)),
 -			      HRTIMER_MODE_ABS);
 -		return 0;
 -	}
 -	stimer->exp_time = stimer->count;
 -	if (time_now >= stimer->count) {
 -		/*
 -		 * Expire timer according to Hypervisor Top-Level Functional
 -		 * specification v4(15.3.1):
 -		 * "If a one shot is enabled and the specified count is in
 -		 * the past, it will expire immediately."
 -		 */
 -		stimer_mark_pending(stimer, false);
 -		return 0;
 -	}
 -
 -	hrtimer_start(&stimer->timer,
 -		      ktime_add_ns(ktime_now, 100 * (stimer->count - time_now)),
 -		      HRTIMER_MODE_ABS);
 -	return 0;
 -}
 -
 -static int stimer_set_config(struct kvm_vcpu_hv_stimer *stimer, u64 config,
 -			     bool host)
 -{
 -	stimer_cleanup(stimer);
 -	if ((stimer->config & HV_STIMER_ENABLE) && HV_STIMER_SINT(config) == 0)
 -		config &= ~HV_STIMER_ENABLE;
 -	stimer->config = config;
 -	stimer_mark_pending(stimer, false);
 -	return 0;
 -}
 -
 -static int stimer_set_count(struct kvm_vcpu_hv_stimer *stimer, u64 count,
 -			    bool host)
 -{
 -	stimer_cleanup(stimer);
 -	stimer->count = count;
 -	if (stimer->count == 0)
 -		stimer->config &= ~HV_STIMER_ENABLE;
 -	else if (stimer->config & HV_STIMER_AUTOENABLE)
 -		stimer->config |= HV_STIMER_ENABLE;
 -	stimer_mark_pending(stimer, false);
 -	return 0;
 -}
 -
 -static int stimer_get_config(struct kvm_vcpu_hv_stimer *stimer, u64 *pconfig)
 -{
 -	*pconfig = stimer->config;
 -	return 0;
 -}
 -
 -static int stimer_get_count(struct kvm_vcpu_hv_stimer *stimer, u64 *pcount)
 -{
 -	*pcount = stimer->count;
 -	return 0;
 -}
 -
 -static int synic_deliver_msg(struct kvm_vcpu_hv_synic *synic, u32 sint,
 -			     struct hv_message *src_msg)
 -{
 -	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
 -	struct page *page;
 -	gpa_t gpa;
 -	struct hv_message *dst_msg;
 -	int r;
 -	struct hv_message_page *msg_page;
 -
 -	if (!(synic->msg_page & HV_SYNIC_SIMP_ENABLE))
 -		return -ENOENT;
 -
 -	gpa = synic->msg_page & PAGE_MASK;
 -	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
 -	if (is_error_page(page))
 -		return -EFAULT;
 -
 -	msg_page = kmap_atomic(page);
 -	dst_msg = &msg_page->sint_message[sint];
 -	if (sync_cmpxchg(&dst_msg->header.message_type, HVMSG_NONE,
 -			 src_msg->header.message_type) != HVMSG_NONE) {
 -		dst_msg->header.message_flags.msg_pending = 1;
 -		r = -EAGAIN;
 -	} else {
 -		memcpy(&dst_msg->u.payload, &src_msg->u.payload,
 -		       src_msg->header.payload_size);
 -		dst_msg->header.message_type = src_msg->header.message_type;
 -		dst_msg->header.payload_size = src_msg->header.payload_size;
 -		r = synic_set_irq(synic, sint);
 -		if (r >= 1)
 -			r = 0;
 -		else if (r == 0)
 -			r = -EFAULT;
 -	}
 -	kunmap_atomic(msg_page);
 -	kvm_release_page_dirty(page);
 -	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
 -	return r;
 -}
 -
 -static int stimer_send_msg(struct kvm_vcpu_hv_stimer *stimer)
 -{
 -	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
 -	struct hv_message *msg = &stimer->msg;
 -	struct hv_timer_message_payload *payload =
 -			(struct hv_timer_message_payload *)&msg->u.payload;
 -
 -	payload->expiration_time = stimer->exp_time;
 -	payload->delivery_time = get_time_ref_counter(vcpu->kvm);
 -	return synic_deliver_msg(vcpu_to_synic(vcpu),
 -				 HV_STIMER_SINT(stimer->config), msg);
 -}
 -
 -static void stimer_expiration(struct kvm_vcpu_hv_stimer *stimer)
 -{
 -	stimer->msg_pending = true;
 -	if (!stimer_send_msg(stimer)) {
 -		stimer->msg_pending = false;
 -		if (!(stimer->config & HV_STIMER_PERIODIC))
 -			stimer->config &= ~HV_STIMER_ENABLE;
 -	}
 -}
 -
 -void kvm_hv_process_stimers(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
 -	struct kvm_vcpu_hv_stimer *stimer;
 -	u64 time_now, exp_time;
 -	int i;
 -
 -	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
 -		if (test_and_clear_bit(i, hv_vcpu->stimer_pending_bitmap)) {
 -			stimer = &hv_vcpu->stimer[i];
 -			if (stimer->config & HV_STIMER_ENABLE) {
 -				exp_time = stimer->exp_time;
 -
 -				if (exp_time) {
 -					time_now =
 -						get_time_ref_counter(vcpu->kvm);
 -					if (time_now >= exp_time)
 -						stimer_expiration(stimer);
 -				}
 -
 -				if ((stimer->config & HV_STIMER_ENABLE) &&
 -				    stimer->count)
 -					stimer_start(stimer);
 -				else
 -					stimer_cleanup(stimer);
 -			}
 -		}
 -}
 -
 -void kvm_hv_vcpu_uninit(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
 -	int i;
 -
 -	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
 -		stimer_cleanup(&hv_vcpu->stimer[i]);
 -}
 -
 -static void stimer_prepare_msg(struct kvm_vcpu_hv_stimer *stimer)
 -{
 -	struct hv_message *msg = &stimer->msg;
 -	struct hv_timer_message_payload *payload =
 -			(struct hv_timer_message_payload *)&msg->u.payload;
 -
 -	memset(&msg->header, 0, sizeof(msg->header));
 -	msg->header.message_type = HVMSG_TIMER_EXPIRED;
 -	msg->header.payload_size = sizeof(*payload);
 -
 -	payload->timer_index = stimer->index;
 -	payload->expiration_time = 0;
 -	payload->delivery_time = 0;
 -}
 -
 -static void stimer_init(struct kvm_vcpu_hv_stimer *stimer, int timer_index)
 -{
 -	memset(stimer, 0, sizeof(*stimer));
 -	stimer->index = timer_index;
 -	hrtimer_init(&stimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 -	stimer->timer.function = stimer_timer_callback;
 -	stimer_prepare_msg(stimer);
 -}
 -
 -void kvm_hv_vcpu_init(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
 -	int i;
 -
 -	synic_init(&hv_vcpu->synic);
 -
 -	bitmap_zero(hv_vcpu->stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
 -	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
 -		stimer_init(&hv_vcpu->stimer[i], i);
 -}
 +	struct kvm_hv *hv = &kvm->arch.hyperv;
 +	struct kvm_vcpu *vcpu;
 +	u64 tsc;
  
 -int kvm_hv_activate_synic(struct kvm_vcpu *vcpu)
 -{
  	/*
 -	 * Hyper-V SynIC auto EOI SINT's are
 -	 * not compatible with APICV, so deactivate APICV
 +	 * The guest has not set up the TSC page or the clock isn't
 +	 * stable, fall back to get_kvmclock_ns.
  	 */
 -	kvm_vcpu_deactivate_apicv(vcpu);
 -	vcpu_to_synic(vcpu)->active = true;
 -	return 0;
 +	if (!hv->tsc_ref.tsc_sequence)
 +		return div_u64(get_kvmclock_ns(kvm), 100);
 +
 +	vcpu = kvm_get_vcpu(kvm, 0);
 +	tsc = kvm_read_l1_tsc(vcpu, rdtsc());
 +	return mul_u64_u64_shr(tsc, hv->tsc_ref.tsc_scale, 64)
 +		+ hv->tsc_ref.tsc_offset;
  }
  
  static bool kvm_hv_msr_partition_wide(u32 msr)
diff --cc arch/x86/kvm/trace.h
index 1d779c897539,4be350003bce..000000000000
--- a/arch/x86/kvm/trace.h
+++ b/arch/x86/kvm/trace.h
@@@ -1030,77 -1026,98 +1030,172 @@@ TRACE_EVENT(kvm_pi_irte_update
  );
  
  /*
++<<<<<<< HEAD
 + * Tracepoint for AMD AVIC
 + */
 +TRACE_EVENT(kvm_avic_incomplete_ipi,
 +	    TP_PROTO(u32 vcpu, u32 icrh, u32 icrl, u32 id, u32 index),
 +	    TP_ARGS(vcpu, icrh, icrl, id, index),
 +
 +	TP_STRUCT__entry(
 +		__field(u32, vcpu)
 +		__field(u32, icrh)
 +		__field(u32, icrl)
 +		__field(u32, id)
 +		__field(u32, index)
 +	),
 +
 +	TP_fast_assign(
 +		__entry->vcpu = vcpu;
 +		__entry->icrh = icrh;
 +		__entry->icrl = icrl;
 +		__entry->id = id;
 +		__entry->index = index;
 +	),
 +
 +	TP_printk("vcpu=%u, icrh:icrl=%#010x:%08x, id=%u, index=%u\n",
 +		  __entry->vcpu, __entry->icrh, __entry->icrl,
 +		  __entry->id, __entry->index)
 +);
 +
 +TRACE_EVENT(kvm_avic_unaccelerated_access,
 +	    TP_PROTO(u32 vcpu, u32 offset, bool ft, bool rw, u32 vec),
 +	    TP_ARGS(vcpu, offset, ft, rw, vec),
 +
 +	TP_STRUCT__entry(
 +		__field(u32, vcpu)
 +		__field(u32, offset)
 +		__field(bool, ft)
 +		__field(bool, rw)
 +		__field(u32, vec)
 +	),
 +
 +	TP_fast_assign(
 +		__entry->vcpu = vcpu;
 +		__entry->offset = offset;
 +		__entry->ft = ft;
 +		__entry->rw = rw;
 +		__entry->vec = vec;
 +	),
 +
 +	TP_printk("vcpu=%u, offset=%#x(%s), %s, %s, vec=%#x\n",
 +		  __entry->vcpu,
 +		  __entry->offset,
 +		  __print_symbolic(__entry->offset, kvm_trace_symbol_apic),
 +		  __entry->ft ? "trap" : "fault",
 +		  __entry->rw ? "write" : "read",
 +		  __entry->vec)
 +);
 +
 +TRACE_EVENT(kvm_hv_timer_state,
 +		TP_PROTO(unsigned int vcpu_id, unsigned int hv_timer_in_use),
 +		TP_ARGS(vcpu_id, hv_timer_in_use),
 +		TP_STRUCT__entry(
 +			__field(unsigned int, vcpu_id)
 +			__field(unsigned int, hv_timer_in_use)
 +			),
 +		TP_fast_assign(
 +			__entry->vcpu_id = vcpu_id;
 +			__entry->hv_timer_in_use = hv_timer_in_use;
 +			),
 +		TP_printk("vcpu_id %x hv_timer %x\n",
 +			__entry->vcpu_id,
 +			__entry->hv_timer_in_use)
 +);
++=======
+  * Tracepoint for kvm_hv_notify_acked_sint.
+  */
+ TRACE_EVENT(kvm_hv_notify_acked_sint,
+ 	TP_PROTO(int vcpu_id, u32 sint),
+ 	TP_ARGS(vcpu_id, sint),
+ 
+ 	TP_STRUCT__entry(
+ 		__field(int, vcpu_id)
+ 		__field(u32, sint)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->vcpu_id = vcpu_id;
+ 		__entry->sint = sint;
+ 	),
+ 
+ 	TP_printk("vcpu_id %d sint %u", __entry->vcpu_id, __entry->sint)
+ );
+ 
+ /*
+  * Tracepoint for synic_set_irq.
+  */
+ TRACE_EVENT(kvm_hv_synic_set_irq,
+ 	TP_PROTO(int vcpu_id, u32 sint, int vector, int ret),
+ 	TP_ARGS(vcpu_id, sint, vector, ret),
+ 
+ 	TP_STRUCT__entry(
+ 		__field(int, vcpu_id)
+ 		__field(u32, sint)
+ 		__field(int, vector)
+ 		__field(int, ret)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->vcpu_id = vcpu_id;
+ 		__entry->sint = sint;
+ 		__entry->vector = vector;
+ 		__entry->ret = ret;
+ 	),
+ 
+ 	TP_printk("vcpu_id %d sint %u vector %d ret %d",
+ 		  __entry->vcpu_id, __entry->sint, __entry->vector,
+ 		  __entry->ret)
+ );
+ 
+ /*
+  * Tracepoint for kvm_hv_synic_send_eoi.
+  */
+ TRACE_EVENT(kvm_hv_synic_send_eoi,
+ 	TP_PROTO(int vcpu_id, int vector),
+ 	TP_ARGS(vcpu_id, vector),
+ 
+ 	TP_STRUCT__entry(
+ 		__field(int, vcpu_id)
+ 		__field(u32, sint)
+ 		__field(int, vector)
+ 		__field(int, ret)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->vcpu_id = vcpu_id;
+ 		__entry->vector	= vector;
+ 	),
+ 
+ 	TP_printk("vcpu_id %d vector %d", __entry->vcpu_id, __entry->vector)
+ );
+ 
+ /*
+  * Tracepoint for synic_set_msr.
+  */
+ TRACE_EVENT(kvm_hv_synic_set_msr,
+ 	TP_PROTO(int vcpu_id, u32 msr, u64 data, bool host),
+ 	TP_ARGS(vcpu_id, msr, data, host),
+ 
+ 	TP_STRUCT__entry(
+ 		__field(int, vcpu_id)
+ 		__field(u32, msr)
+ 		__field(u64, data)
+ 		__field(bool, host)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->vcpu_id = vcpu_id;
+ 		__entry->msr = msr;
+ 		__entry->data = data;
+ 		__entry->host = host
+ 	),
+ 
+ 	TP_printk("vcpu_id %d msr 0x%x data 0x%llx host %d",
+ 		  __entry->vcpu_id, __entry->msr, __entry->data, __entry->host)
+ );
+ 
++>>>>>>> 18659a9cb188 (kvm/x86: Hyper-V SynIC tracepoints)
  #endif /* _TRACE_KVM_H */
  
  #undef TRACE_INCLUDE_PATH
* Unmerged path arch/x86/kvm/hyperv.c
* Unmerged path arch/x86/kvm/trace.h
