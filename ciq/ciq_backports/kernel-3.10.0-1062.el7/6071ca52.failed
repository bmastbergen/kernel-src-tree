mm: page_counter: let page_counter_try_charge() return bool

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] page_counter: let page_counter_try_charge() return bool (Davide Caratti) [1658641]
Rebuild_FUZZ: 96.49%
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 6071ca5201066f4b2a61cfb693dd186d6bc6e9f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/6071ca52.failed

page_counter_try_charge() currently returns 0 on success and -ENOMEM on
failure, which is surprising behavior given the function name.

Make it follow the expected pattern of try_stuff() functions that return a
boolean true to indicate success, or false for failure.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Vladimir Davydov <vdavydov@virtuozzo.com
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6071ca5201066f4b2a61cfb693dd186d6bc6e9f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/page_counter.h
#	mm/memcontrol.c
diff --cc include/linux/page_counter.h
index 7cce3be99ff3,7e62920a3a94..000000000000
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@@ -34,14 -34,15 +34,21 @@@ static inline unsigned long page_counte
  	return atomic_long_read(&counter->count);
  }
  
 -void page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
 +int page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
  void page_counter_charge(struct page_counter *counter, unsigned long nr_pages);
++<<<<<<< HEAD
 +int page_counter_try_charge(struct page_counter *counter,
 +			    unsigned long nr_pages,
 +			    struct page_counter **fail);
 +int page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
++=======
+ bool page_counter_try_charge(struct page_counter *counter,
+ 			     unsigned long nr_pages,
+ 			     struct page_counter **fail);
+ void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
++>>>>>>> 6071ca520106 (mm: page_counter: let page_counter_try_charge() return bool)
  int page_counter_limit(struct page_counter *counter, unsigned long limit);
 -int page_counter_memparse(const char *buf, const char *max,
 -			  unsigned long *nr_pages);
 +int page_counter_memparse(const char *buf, unsigned long *nr_pages);
  
  static inline void page_counter_reset_watermark(struct page_counter *counter)
  {
diff --cc mm/memcontrol.c
index c2be36e1df50,d47de73d7c36..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -2587,54 -1973,98 +2587,65 @@@ static int memcg_cpu_hotplug_callback(s
  	return NOTIFY_OK;
  }
  
 -/*
 - * Scheduled by try_charge() to be executed from the userland return path
 - * and reclaims memory over the high limit.
 - */
 -void mem_cgroup_handle_over_high(void)
 -{
 -	unsigned int nr_pages = current->memcg_nr_pages_over_high;
 -	struct mem_cgroup *memcg, *pos;
 -
 -	if (likely(!nr_pages))
 -		return;
 -
 -	pos = memcg = get_mem_cgroup_from_mm(current->mm);
 -
 -	do {
 -		if (page_counter_read(&pos->memory) <= pos->high)
 -			continue;
 -		mem_cgroup_events(pos, MEMCG_HIGH, 1);
 -		try_to_free_mem_cgroup_pages(pos, nr_pages, GFP_KERNEL, true);
 -	} while ((pos = parent_mem_cgroup(pos)));
  
 -	css_put(&memcg->css);
 -	current->memcg_nr_pages_over_high = 0;
 -}
 +/* See __mem_cgroup_try_charge() for details */
 +enum {
 +	CHARGE_OK,		/* success */
 +	CHARGE_RETRY,		/* need to retry but retry is not bad */
 +	CHARGE_NOMEM,		/* we can't do more. return -ENOMEM */
 +	CHARGE_WOULDBLOCK,	/* GFP_WAIT wasn't set and no enough res. */
 +};
  
 -static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,
 -		      unsigned int nr_pages)
 +static int mem_cgroup_do_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,
 +				unsigned int nr_pages, unsigned int min_pages,
 +				bool invoke_oom)
  {
 -	unsigned int batch = max(CHARGE_BATCH, nr_pages);
 -	int nr_retries = MEM_CGROUP_RECLAIM_RETRIES;
  	struct mem_cgroup *mem_over_limit;
  	struct page_counter *counter;
 -	unsigned long nr_reclaimed;
 -	bool may_swap = true;
 -	bool drained = false;
 +	unsigned long flags = 0;
 +	int ret;
  
 -	if (mem_cgroup_is_root(memcg))
 -		return 0;
 -retry:
 -	if (consume_stock(memcg, nr_pages))
 -		return 0;
 +	ret = page_counter_try_charge(&memcg->memory, nr_pages, &counter);
 +
++<<<<<<< HEAD
 +	if (likely(!ret)) {
 +		if (!do_swap_account)
 +			return CHARGE_OK;
 +		ret = page_counter_try_charge(&memcg->memsw, nr_pages, &counter);
 +		if (likely(!ret))
 +			return CHARGE_OK;
  
 +		page_counter_uncharge(&memcg->memory, nr_pages);
++=======
+ 	if (!do_swap_account ||
+ 	    page_counter_try_charge(&memcg->memsw, batch, &counter)) {
+ 		if (page_counter_try_charge(&memcg->memory, batch, &counter))
+ 			goto done_restock;
+ 		if (do_swap_account)
+ 			page_counter_uncharge(&memcg->memsw, batch);
+ 		mem_over_limit = mem_cgroup_from_counter(counter, memory);
+ 	} else {
++>>>>>>> 6071ca520106 (mm: page_counter: let page_counter_try_charge() return bool)
  		mem_over_limit = mem_cgroup_from_counter(counter, memsw);
 -		may_swap = false;
 -	}
 -
 -	if (batch > nr_pages) {
 -		batch = nr_pages;
 -		goto retry;
 -	}
 -
 +		flags |= MEM_CGROUP_RECLAIM_NOSWAP;
 +	} else
 +		mem_over_limit = mem_cgroup_from_counter(counter, memory);
  	/*
 -	 * Unlike in global OOM situations, memcg is not in a physical
 -	 * memory shortage.  Allow dying and OOM-killed tasks to
 -	 * bypass the last charges so that they can exit quickly and
 -	 * free their memory.
 +	 * Never reclaim on behalf of optional batching, retry with a
 +	 * single page instead.
  	 */
 -	if (unlikely(test_thread_flag(TIF_MEMDIE) ||
 -		     fatal_signal_pending(current) ||
 -		     current->flags & PF_EXITING))
 -		goto force;
 -
 -	if (unlikely(task_in_memcg_oom(current)))
 -		goto nomem;
 +	if (nr_pages > min_pages)
 +		return CHARGE_RETRY;
  
  	if (!(gfp_mask & __GFP_WAIT))
 -		goto nomem;
 +		return CHARGE_WOULDBLOCK;
  
 -	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);
 -
 -	nr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,
 -						    gfp_mask, may_swap);
 +	if (gfp_mask & __GFP_NORETRY)
 +		return CHARGE_NOMEM;
  
 +	ret = mem_cgroup_reclaim(mem_over_limit, gfp_mask, flags);
  	if (mem_cgroup_margin(mem_over_limit) >= nr_pages)
 -		goto retry;
 -
 -	if (!drained) {
 -		drain_all_stock(mem_over_limit);
 -		drained = true;
 -		goto retry;
 -	}
 -
 -	if (gfp_mask & __GFP_NORETRY)
 -		goto nomem;
 +		return CHARGE_RETRY;
  	/*
  	 * Even though the limit is exceeded at this point, reclaim
  	 * may have been able to free some pages.  Retry the charge
@@@ -3445,1193 -2500,23 +3456,1227 @@@ out
  
  static DEFINE_MUTEX(memcg_limit_mutex);
  
 -static int mem_cgroup_resize_limit(struct mem_cgroup *memcg,
 -				   unsigned long limit)
 +int __kmem_cache_destroy_memcg_children(struct kmem_cache *s)
  {
 -	unsigned long curusage;
 -	unsigned long oldusage;
 -	bool enlarge = false;
 -	int retry_count;
 -	int ret;
 +	struct kmem_cache *c;
 +	int i, failed = 0;
  
  	/*
 -	 * For keeping hierarchical_reclaim simple, how long we should retry
 -	 * is depends on callers. We set our retry-count to be function
 -	 * of # of children which we should visit in this loop.
 +	 * If the cache is being destroyed, we trust that there is no one else
 +	 * requesting objects from it. Even if there are, the sanity checks in
 +	 * kmem_cache_destroy should caught this ill-case.
 +	 *
 +	 * Still, we don't want anyone else freeing memcg_caches under our
 +	 * noses, which can happen if a new memcg comes to life. As usual,
 +	 * we'll take the memcg_limit_mutex to protect ourselves against this.
  	 */
 -	retry_count = MEM_CGROUP_RECLAIM_RETRIES *
 -		      mem_cgroup_count_children(memcg);
 +	mutex_lock(&memcg_limit_mutex);
 +	for (i = 0; i < memcg_limited_groups_array_size; i++) {
 +		c = s->memcg_params->memcg_caches[i];
 +		if (!c)
 +			continue;
 +
 +		/*
 +		 * We will now manually delete the caches, so to avoid races
 +		 * we need to cancel all pending destruction workers and
 +		 * proceed with destruction ourselves.
 +		 *
 +		 * kmem_cache_destroy() will call kmem_cache_shrink internally,
 +		 * and that could spawn the workers again: it is likely that
 +		 * the cache still have active pages until this very moment.
 +		 * This would lead us back to mem_cgroup_destroy_cache.
 +		 *
 +		 * But that will not execute at all if the "dead" flag is not
 +		 * set, so flip it down to guarantee we are in control.
 +		 */
 +		c->memcg_params->dead = false;
 +		cancel_work_sync(&c->memcg_params->destroy);
 +		kmem_cache_destroy(c);
 +
 +		if (cache_from_memcg(s, i))
 +			failed++;
 +	}
 +	mutex_unlock(&memcg_limit_mutex);
 +	return failed;
 +}
 +
 +struct create_work {
 +	struct mem_cgroup *memcg;
 +	struct kmem_cache *cachep;
 +	struct work_struct work;
 +};
 +
 +static void mem_cgroup_destroy_all_caches(struct mem_cgroup *memcg)
 +{
 +	struct kmem_cache *cachep;
 +	struct memcg_cache_params *params;
 +
 +	if (!memcg_kmem_is_active(memcg))
 +		return;
 +
 +	mutex_lock(&memcg->slab_caches_mutex);
 +	list_for_each_entry(params, &memcg->memcg_slab_caches, list) {
 +		cachep = memcg_params_to_cache(params);
 +		cachep->memcg_params->dead = true;
 +		schedule_work(&cachep->memcg_params->destroy);
 +	}
 +	mutex_unlock(&memcg->slab_caches_mutex);
 +}
 +
 +static void memcg_create_cache_work_func(struct work_struct *w)
 +{
 +	struct create_work *cw;
 +
 +	cw = container_of(w, struct create_work, work);
 +	memcg_create_kmem_cache(cw->memcg, cw->cachep);
 +	/* Drop the reference gotten when we enqueued. */
 +	css_put(&cw->memcg->css);
 +	kfree(cw);
 +}
 +
 +/*
 + * Enqueue the creation of a per-memcg kmem_cache.
 + */
 +static void __memcg_create_cache_enqueue(struct mem_cgroup *memcg,
 +					 struct kmem_cache *cachep)
 +{
 +	struct create_work *cw;
 +
 +	cw = kmalloc(sizeof(struct create_work), GFP_NOWAIT);
 +	if (cw == NULL) {
 +		css_put(&memcg->css);
 +		return;
 +	}
 +
 +	cw->memcg = memcg;
 +	cw->cachep = cachep;
 +
 +	INIT_WORK(&cw->work, memcg_create_cache_work_func);
 +	schedule_work(&cw->work);
 +}
 +
 +static void memcg_create_cache_enqueue(struct mem_cgroup *memcg,
 +				       struct kmem_cache *cachep)
 +{
 +	/*
 +	 * We need to stop accounting when we kmalloc, because if the
 +	 * corresponding kmalloc cache is not yet created, the first allocation
 +	 * in __memcg_create_cache_enqueue will recurse.
 +	 *
 +	 * However, it is better to enclose the whole function. Depending on
 +	 * the debugging options enabled, INIT_WORK(), for instance, can
 +	 * trigger an allocation. This too, will make us recurse. Because at
 +	 * this point we can't allow ourselves back into memcg_kmem_get_cache,
 +	 * the safest choice is to do it like this, wrapping the whole function.
 +	 */
 +	memcg_stop_kmem_account();
 +	__memcg_create_cache_enqueue(memcg, cachep);
 +	memcg_resume_kmem_account();
 +}
 +/*
 + * Return the kmem_cache we're supposed to use for a slab allocation.
 + * We try to use the current memcg's version of the cache.
 + *
 + * If the cache does not exist yet, if we are the first user of it,
 + * we either create it immediately, if possible, or create it asynchronously
 + * in a workqueue.
 + * In the latter case, we will let the current allocation go through with
 + * the original cache.
 + *
 + * Can't be called in interrupt context or from kernel threads.
 + * This function needs to be called with rcu_read_lock() held.
 + */
 +struct kmem_cache *__memcg_kmem_get_cache(struct kmem_cache *cachep,
 +					  gfp_t gfp)
 +{
 +	struct mem_cgroup *memcg;
 +	int idx;
 +
 +	VM_BUG_ON(!cachep->memcg_params);
 +	VM_BUG_ON(!cachep->memcg_params->is_root_cache);
 +
 +	if (!current->mm || current->memcg_kmem_skip_account)
 +		return cachep;
 +
 +	rcu_read_lock();
 +	memcg = mem_cgroup_from_task(rcu_dereference(current->mm->owner));
 +
 +	if (!memcg_can_account_kmem(memcg))
 +		goto out;
 +
 +	idx = memcg_cache_id(memcg);
 +
 +	/*
 +	 * barrier to mare sure we're always seeing the up to date value.  The
 +	 * code updating memcg_caches will issue a write barrier to match this.
 +	 */
 +	read_barrier_depends();
 +	if (likely(cachep->memcg_params->memcg_caches[idx])) {
 +		cachep = cachep->memcg_params->memcg_caches[idx];
 +		goto out;
 +	}
 +
 +	/* The corresponding put will be done in the workqueue. */
 +	if (!css_tryget(&memcg->css))
 +		goto out;
 +	rcu_read_unlock();
 +
 +	/*
 +	 * If we are in a safe context (can wait, and not in interrupt
 +	 * context), we could be be predictable and return right away.
 +	 * This would guarantee that the allocation being performed
 +	 * already belongs in the new cache.
 +	 *
 +	 * However, there are some clashes that can arrive from locking.
 +	 * For instance, because we acquire the slab_mutex while doing
 +	 * kmem_cache_dup, this means no further allocation could happen
 +	 * with the slab_mutex held.
 +	 *
 +	 * Also, because cache creation issue get_online_cpus(), this
 +	 * creates a lock chain: memcg_slab_mutex -> cpu_hotplug_mutex,
 +	 * that ends up reversed during cpu hotplug. (cpuset allocates
 +	 * a bunch of GFP_KERNEL memory during cpuup). Due to all that,
 +	 * better to defer everything.
 +	 */
 +	memcg_create_cache_enqueue(memcg, cachep);
 +	return cachep;
 +out:
 +	rcu_read_unlock();
 +	return cachep;
 +}
 +EXPORT_SYMBOL(__memcg_kmem_get_cache);
 +
++<<<<<<< HEAD
 +/*
 + * We need to verify if the allocation against current->mm->owner's memcg is
 + * possible for the given order. But the page is not allocated yet, so we'll
 + * need a further commit step to do the final arrangements.
 + *
 + * It is possible for the task to switch cgroups in this mean time, so at
 + * commit time, we can't rely on task conversion any longer.  We'll then use
 + * the handle argument to return to the caller which cgroup we should commit
 + * against. We could also return the memcg directly and avoid the pointer
 + * passing, but a boolean return value gives better semantics considering
 + * the compiled-out case as well.
 + *
 + * Returning true means the allocation is possible.
 + */
 +bool
 +__memcg_kmem_newpage_charge(gfp_t gfp, struct mem_cgroup **_memcg, int order)
++=======
++void __memcg_kmem_put_cache(struct kmem_cache *cachep)
++{
++	if (!is_root_cache(cachep))
++		css_put(&cachep->memcg_params.memcg->css);
++}
++
++int __memcg_kmem_charge_memcg(struct page *page, gfp_t gfp, int order,
++			      struct mem_cgroup *memcg)
++{
++	unsigned int nr_pages = 1 << order;
++	struct page_counter *counter;
++	int ret;
++
++	if (!memcg_kmem_is_active(memcg))
++		return 0;
++
++	if (!page_counter_try_charge(&memcg->kmem, nr_pages, &counter))
++		return -ENOMEM;
++
++	ret = try_charge(memcg, gfp, nr_pages);
++	if (ret) {
++		page_counter_uncharge(&memcg->kmem, nr_pages);
++		return ret;
++	}
++
++	page->mem_cgroup = memcg;
++
++	return 0;
++}
++
++int __memcg_kmem_charge(struct page *page, gfp_t gfp, int order)
++>>>>>>> 6071ca520106 (mm: page_counter: let page_counter_try_charge() return bool)
 +{
 +	struct mem_cgroup *memcg;
 +	int ret;
 +
 +	*_memcg = NULL;
 +	memcg = try_get_mem_cgroup_from_mm(current->mm);
 +
 +	/*
 +	 * very rare case described in mem_cgroup_from_task. Unfortunately there
 +	 * isn't much we can do without complicating this too much, and it would
 +	 * be gfp-dependent anyway. Just let it go
 +	 */
 +	if (unlikely(!memcg))
 +		return true;
 +
 +	if (!memcg_can_account_kmem(memcg)) {
 +		css_put(&memcg->css);
 +		return true;
 +	}
 +
 +	ret = memcg_charge_kmem(memcg, gfp, 1 << order);
 +	if (!ret)
 +		*_memcg = memcg;
 +
 +	css_put(&memcg->css);
 +	return (ret == 0);
 +}
 +
 +void __memcg_kmem_commit_charge(struct page *page, struct mem_cgroup *memcg,
 +			      int order)
 +{
 +	struct page_cgroup *pc;
 +
 +	VM_BUG_ON(mem_cgroup_is_root(memcg));
 +
 +	/* The page allocation failed. Revert */
 +	if (!page) {
 +		memcg_uncharge_kmem(memcg, 1 << order);
 +		return;
 +	}
 +
 +	pc = lookup_page_cgroup(page);
 +	lock_page_cgroup(pc);
 +	pc->mem_cgroup = memcg;
 +	SetPageCgroupUsed(pc);
 +	unlock_page_cgroup(pc);
 +}
 +
 +void __memcg_kmem_uncharge_pages(struct page *page, int order)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	struct page_cgroup *pc;
 +
 +
 +	pc = lookup_page_cgroup(page);
 +	/*
 +	 * Fast unlocked return. Theoretically might have changed, have to
 +	 * check again after locking.
 +	 */
 +	if (!PageCgroupUsed(pc))
 +		return;
 +
 +	lock_page_cgroup(pc);
 +	if (PageCgroupUsed(pc)) {
 +		memcg = pc->mem_cgroup;
 +		ClearPageCgroupUsed(pc);
 +	}
 +	unlock_page_cgroup(pc);
 +
 +	/*
 +	 * We trust that only if there is a memcg associated with the page, it
 +	 * is a valid allocation
 +	 */
 +	if (!memcg)
 +		return;
 +
 +	VM_BUG_ON_PAGE(mem_cgroup_is_root(memcg), page);
 +	memcg_uncharge_kmem(memcg, 1 << order);
 +}
 +#else
 +static inline void mem_cgroup_destroy_all_caches(struct mem_cgroup *memcg)
 +{
 +}
 +#endif /* CONFIG_MEMCG_KMEM */
 +
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +
 +#define PCGF_NOCOPY_AT_SPLIT (1 << PCG_LOCK | 1 << PCG_MIGRATION)
 +/*
 + * Because tail pages are not marked as "used", set it. We're under
 + * zone->lru_lock, 'splitting on pmd' and compound_lock.
 + * charge/uncharge will be never happen and move_account() is done under
 + * compound_lock(), so we don't have to take care of races.
 + */
 +void mem_cgroup_split_huge_fixup(struct page *head)
 +{
 +	struct page_cgroup *head_pc = lookup_page_cgroup(head);
 +	struct page_cgroup *pc;
 +	struct mem_cgroup *memcg;
 +	int i;
 +
 +	if (mem_cgroup_disabled())
 +		return;
 +
 +	memcg = head_pc->mem_cgroup;
 +	for (i = 1; i < HPAGE_PMD_NR; i++) {
 +		pc = head_pc + i;
 +		pc->mem_cgroup = memcg;
 +		smp_wmb();/* see __commit_charge() */
 +		pc->flags = head_pc->flags & ~PCGF_NOCOPY_AT_SPLIT;
 +	}
 +	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS_HUGE],
 +		       HPAGE_PMD_NR);
 +}
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 +
 +/**
 + * mem_cgroup_move_account - move account of the page
 + * @page: the page
 + * @nr_pages: number of regular pages (>1 for huge pages)
 + * @pc:	page_cgroup of the page.
 + * @from: mem_cgroup which the page is moved from.
 + * @to:	mem_cgroup which the page is moved to. @from != @to.
 + *
 + * The caller must confirm following.
 + * - page is not on LRU (isolate_page() is useful.)
 + * - compound_lock is held when nr_pages > 1
 + *
 + * This function doesn't do "charge" to new cgroup and doesn't do "uncharge"
 + * from old cgroup.
 + */
 +static int mem_cgroup_move_account(struct page *page,
 +				   unsigned int nr_pages,
 +				   struct page_cgroup *pc,
 +				   struct mem_cgroup *from,
 +				   struct mem_cgroup *to)
 +{
 +	unsigned long flags;
 +	int ret;
 +	bool anon = PageAnon(page);
 +
 +	VM_BUG_ON(from == to);
 +	VM_BUG_ON_PAGE(PageLRU(page), page);
 +	/*
 +	 * The page is isolated from LRU. So, collapse function
 +	 * will not handle this page. But page splitting can happen.
 +	 * Do this check under compound_page_lock(). The caller should
 +	 * hold it.
 +	 */
 +	ret = -EBUSY;
 +	if (nr_pages > 1 && !PageTransHuge(page))
 +		goto out;
 +
 +	lock_page_cgroup(pc);
 +
 +	ret = -EINVAL;
 +	if (!PageCgroupUsed(pc) || pc->mem_cgroup != from)
 +		goto unlock;
 +
 +	move_lock_mem_cgroup(from, &flags);
 +
 +	if (!anon && page_mapped(page)) {
 +		/* Update mapped_file data for mem_cgroup */
 +		preempt_disable();
 +		__this_cpu_dec(from->stat->count[MEM_CGROUP_STAT_FILE_MAPPED]);
 +		__this_cpu_inc(to->stat->count[MEM_CGROUP_STAT_FILE_MAPPED]);
 +		preempt_enable();
 +	}
 +	mem_cgroup_charge_statistics(from, page, anon, -nr_pages);
 +
 +	/* caller should have done css_get */
 +	pc->mem_cgroup = to;
 +	mem_cgroup_charge_statistics(to, page, anon, nr_pages);
 +	move_unlock_mem_cgroup(from, &flags);
 +	ret = 0;
 +unlock:
 +	unlock_page_cgroup(pc);
 +	/*
 +	 * check events
 +	 */
 +	memcg_check_events(to, page);
 +	memcg_check_events(from, page);
 +out:
 +	return ret;
 +}
 +
 +/**
 + * mem_cgroup_move_parent - moves page to the parent group
 + * @page: the page to move
 + * @pc: page_cgroup of the page
 + * @child: page's cgroup
 + *
 + * move charges to its parent or the root cgroup if the group has no
 + * parent (aka use_hierarchy==0).
 + * Although this might fail (get_page_unless_zero, isolate_lru_page or
 + * mem_cgroup_move_account fails) the failure is always temporary and
 + * it signals a race with a page removal/uncharge or migration. In the
 + * first case the page is on the way out and it will vanish from the LRU
 + * on the next attempt and the call should be retried later.
 + * Isolation from the LRU fails only if page has been isolated from
 + * the LRU since we looked at it and that usually means either global
 + * reclaim or migration going on. The page will either get back to the
 + * LRU or vanish.
 + * Finaly mem_cgroup_move_account fails only if the page got uncharged
 + * (!PageCgroupUsed) or moved to a different group. The page will
 + * disappear in the next attempt.
 + */
 +static int mem_cgroup_move_parent(struct page *page,
 +				  struct page_cgroup *pc,
 +				  struct mem_cgroup *child)
 +{
 +	struct mem_cgroup *parent;
 +	unsigned int nr_pages;
 +	unsigned long uninitialized_var(flags);
 +	int ret;
 +
 +	VM_BUG_ON(mem_cgroup_is_root(child));
 +
 +	ret = -EBUSY;
 +	if (!get_page_unless_zero(page))
 +		goto out;
 +	if (isolate_lru_page(page))
 +		goto put;
 +
 +	nr_pages = hpage_nr_pages(page);
 +
 +	parent = parent_mem_cgroup(child);
 +	/*
 +	 * If no parent, move charges to root cgroup.
 +	 */
 +	if (!parent)
 +		parent = root_mem_cgroup;
 +
 +	if (nr_pages > 1) {
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +		flags = compound_lock_irqsave(page);
 +	}
 +
 +	ret = mem_cgroup_move_account(page, nr_pages,
 +				pc, child, parent);
 +	if (!ret) {
 +		/* Take charge off the local counters */
 +		page_counter_cancel(&child->memory, nr_pages);
 +		if (do_swap_account)
 +			page_counter_cancel(&child->memsw, nr_pages);
 +	}
 +
 +	if (nr_pages > 1)
 +		compound_unlock_irqrestore(page, flags);
 +	putback_lru_page(page);
 +put:
 +	put_page(page);
 +out:
 +	return ret;
 +}
 +
 +/*
 + * Charge the memory controller for page usage.
 + * Return
 + * 0 if the charge was successful
 + * < 0 if the cgroup is over its limit
 + */
 +static int mem_cgroup_charge_common(struct page *page, struct mm_struct *mm,
 +				gfp_t gfp_mask, enum charge_type ctype)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	unsigned int nr_pages = 1;
 +	bool oom = true;
 +	int ret;
 +
 +	if (PageTransHuge(page)) {
 +		nr_pages <<= compound_order(page);
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +		/*
 +		 * Never OOM-kill a process for a huge page.  The
 +		 * fault handler will fall back to regular pages.
 +		 */
 +		oom = false;
 +	}
 +
 +	ret = __mem_cgroup_try_charge(mm, gfp_mask, nr_pages, &memcg, oom);
 +	if (ret == -ENOMEM)
 +		return ret;
 +	__mem_cgroup_commit_charge(memcg, page, nr_pages, ctype, false);
 +	return 0;
 +}
 +
 +int mem_cgroup_newpage_charge(struct page *page,
 +			      struct mm_struct *mm, gfp_t gfp_mask)
 +{
 +	if (mem_cgroup_disabled())
 +		return 0;
 +	VM_BUG_ON_PAGE(page_mapped(page), page);
 +	VM_BUG_ON_PAGE(page->mapping && !PageAnon(page), page);
 +	VM_BUG_ON(!mm);
 +	return mem_cgroup_charge_common(page, mm, gfp_mask,
 +					MEM_CGROUP_CHARGE_TYPE_ANON);
 +}
 +
 +/*
 + * While swap-in, try_charge -> commit or cancel, the page is locked.
 + * And when try_charge() successfully returns, one refcnt to memcg without
 + * struct page_cgroup is acquired. This refcnt will be consumed by
 + * "commit()" or removed by "cancel()"
 + */
 +static int __mem_cgroup_try_charge_swapin(struct mm_struct *mm,
 +					  struct page *page,
 +					  gfp_t mask,
 +					  struct mem_cgroup **memcgp)
 +{
 +	struct mem_cgroup *memcg;
 +	struct page_cgroup *pc;
 +	int ret;
 +
 +	pc = lookup_page_cgroup(page);
 +	/*
 +	 * Every swap fault against a single page tries to charge the
 +	 * page, bail as early as possible.  shmem_unuse() encounters
 +	 * already charged pages, too.  The USED bit is protected by
 +	 * the page lock, which serializes swap cache removal, which
 +	 * in turn serializes uncharging.
 +	 */
 +	if (PageCgroupUsed(pc))
 +		return 0;
 +	if (!do_swap_account)
 +		goto charge_cur_mm;
 +	memcg = try_get_mem_cgroup_from_page(page);
 +	if (!memcg)
 +		goto charge_cur_mm;
 +	*memcgp = memcg;
 +	ret = __mem_cgroup_try_charge(NULL, mask, 1, memcgp, true);
 +	css_put(&memcg->css);
 +	if (ret == -EINTR)
 +		ret = 0;
 +	return ret;
 +charge_cur_mm:
 +	ret = __mem_cgroup_try_charge(mm, mask, 1, memcgp, true);
 +	if (ret == -EINTR)
 +		ret = 0;
 +	return ret;
 +}
 +
 +int mem_cgroup_try_charge_swapin(struct mm_struct *mm, struct page *page,
 +				 gfp_t gfp_mask, struct mem_cgroup **memcgp)
 +{
 +	*memcgp = NULL;
 +	if (mem_cgroup_disabled())
 +		return 0;
 +	/*
 +	 * A racing thread's fault, or swapoff, may have already
 +	 * updated the pte, and even removed page from swap cache: in
 +	 * those cases unuse_pte()'s pte_same() test will fail; but
 +	 * there's also a KSM case which does need to charge the page.
 +	 */
 +	if (!PageSwapCache(page)) {
 +		int ret;
 +
 +		ret = __mem_cgroup_try_charge(mm, gfp_mask, 1, memcgp, true);
 +		if (ret == -EINTR)
 +			ret = 0;
 +		return ret;
 +	}
 +	return __mem_cgroup_try_charge_swapin(mm, page, gfp_mask, memcgp);
 +}
 +
 +void mem_cgroup_cancel_charge_swapin(struct mem_cgroup *memcg)
 +{
 +	if (mem_cgroup_disabled())
 +		return;
 +	if (!memcg)
 +		return;
 +	__mem_cgroup_cancel_charge(memcg, 1);
 +}
 +
 +static void
 +__mem_cgroup_commit_charge_swapin(struct page *page, struct mem_cgroup *memcg,
 +					enum charge_type ctype)
 +{
 +	if (mem_cgroup_disabled())
 +		return;
 +	if (!memcg)
 +		return;
 +
 +	__mem_cgroup_commit_charge(memcg, page, 1, ctype, true);
 +	/*
 +	 * Now swap is on-memory. This means this page may be
 +	 * counted both as mem and swap....double count.
 +	 * Fix it by uncharging from memsw. Basically, this SwapCache is stable
 +	 * under lock_page(). But in do_swap_page()::memory.c, reuse_swap_page()
 +	 * may call delete_from_swap_cache() before reach here.
 +	 */
 +	if (do_swap_account && PageSwapCache(page)) {
 +		swp_entry_t ent = {.val = page_private(page)};
 +		mem_cgroup_uncharge_swap(ent);
 +	}
 +}
 +
 +void mem_cgroup_commit_charge_swapin(struct page *page,
 +				     struct mem_cgroup *memcg)
 +{
 +	__mem_cgroup_commit_charge_swapin(page, memcg,
 +					  MEM_CGROUP_CHARGE_TYPE_ANON);
 +}
 +
 +int mem_cgroup_cache_charge(struct page *page, struct mm_struct *mm,
 +				gfp_t gfp_mask)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	enum charge_type type = MEM_CGROUP_CHARGE_TYPE_CACHE;
 +	int ret;
 +
 +	if (mem_cgroup_disabled())
 +		return 0;
 +	if (PageCompound(page))
 +		return 0;
 +
 +	if (!PageSwapCache(page))
 +		ret = mem_cgroup_charge_common(page, mm, gfp_mask, type);
 +	else { /* page is swapcache/shmem */
 +		ret = __mem_cgroup_try_charge_swapin(mm, page,
 +						     gfp_mask, &memcg);
 +		if (!ret)
 +			__mem_cgroup_commit_charge_swapin(page, memcg, type);
 +	}
 +	return ret;
 +}
 +
 +static void mem_cgroup_do_uncharge(struct mem_cgroup *memcg,
 +				   unsigned int nr_pages,
 +				   const enum charge_type ctype)
 +{
 +	struct memcg_batch_info *batch = NULL;
 +	bool uncharge_memsw = true;
 +
 +	/* If swapout, usage of swap doesn't decrease */
 +	if (!do_swap_account || ctype == MEM_CGROUP_CHARGE_TYPE_SWAPOUT)
 +		uncharge_memsw = false;
 +
 +	batch = &current->memcg_batch;
 +	/*
 +	 * In usual, we do css_get() when we remember memcg pointer.
 +	 * But in this case, we keep res->usage until end of a series of
 +	 * uncharges. Then, it's ok to ignore memcg's refcnt.
 +	 */
 +	if (!batch->memcg)
 +		batch->memcg = memcg;
 +	/*
 +	 * do_batch > 0 when unmapping pages or inode invalidate/truncate.
 +	 * In those cases, all pages freed continuously can be expected to be in
 +	 * the same cgroup and we have chance to coalesce uncharges.
 +	 * But we do uncharge one by one if this is killed by OOM(TIF_MEMDIE)
 +	 * because we want to do uncharge as soon as possible.
 +	 */
 +
 +	if (!batch->do_batch || test_thread_flag(TIF_MEMDIE))
 +		goto direct_uncharge;
 +
 +	if (nr_pages > 1)
 +		goto direct_uncharge;
 +
 +	/*
 +	 * In typical case, batch->memcg == mem. This means we can
 +	 * merge a series of uncharges to an uncharge of page_counter.
 +	 * If not, we uncharge page_counter ony by one.
 +	 */
 +	if (batch->memcg != memcg)
 +		goto direct_uncharge;
 +	/* remember freed charge and uncharge it later */
 +	batch->nr_pages++;
 +	if (uncharge_memsw)
 +		batch->memsw_nr_pages++;
 +	return;
 +direct_uncharge:
 +	page_counter_uncharge(&memcg->memory, nr_pages);
 +	if (uncharge_memsw)
 +		page_counter_uncharge(&memcg->memsw, nr_pages);
 +	if (unlikely(batch->memcg != memcg))
 +		memcg_oom_recover(memcg);
 +}
 +
 +/*
 + * uncharge if !page_mapped(page)
 + */
 +static struct mem_cgroup *
 +__mem_cgroup_uncharge_common(struct page *page, enum charge_type ctype,
 +			     bool end_migration)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	unsigned int nr_pages = 1;
 +	struct page_cgroup *pc;
 +	bool anon;
 +
 +	if (mem_cgroup_disabled())
 +		return NULL;
 +
 +	if (PageTransHuge(page)) {
 +		nr_pages <<= compound_order(page);
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +	}
 +	/*
 +	 * Check if our page_cgroup is valid
 +	 */
 +	pc = lookup_page_cgroup(page);
 +	if (unlikely(!PageCgroupUsed(pc)))
 +		return NULL;
 +
 +	lock_page_cgroup(pc);
 +
 +	memcg = pc->mem_cgroup;
 +
 +	if (!PageCgroupUsed(pc))
 +		goto unlock_out;
 +
 +	anon = PageAnon(page);
 +
 +	switch (ctype) {
 +	case MEM_CGROUP_CHARGE_TYPE_ANON:
 +		/*
 +		 * Generally PageAnon tells if it's the anon statistics to be
 +		 * updated; but sometimes e.g. mem_cgroup_uncharge_page() is
 +		 * used before page reached the stage of being marked PageAnon.
 +		 */
 +		anon = true;
 +		/* fallthrough */
 +	case MEM_CGROUP_CHARGE_TYPE_DROP:
 +		/* See mem_cgroup_prepare_migration() */
 +		if (page_mapped(page))
 +			goto unlock_out;
 +		/*
 +		 * Pages under migration may not be uncharged.  But
 +		 * end_migration() /must/ be the one uncharging the
 +		 * unused post-migration page and so it has to call
 +		 * here with the migration bit still set.  See the
 +		 * page_counter handling below.
 +		 */
 +		if (!end_migration && PageCgroupMigration(pc))
 +			goto unlock_out;
 +		break;
 +	case MEM_CGROUP_CHARGE_TYPE_SWAPOUT:
 +		if (!PageAnon(page)) {	/* Shared memory */
 +			if (page->mapping && !page_is_file_cache(page))
 +				goto unlock_out;
 +		} else if (page_mapped(page)) /* Anon */
 +				goto unlock_out;
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	mem_cgroup_charge_statistics(memcg, page, anon, -nr_pages);
 +
 +	ClearPageCgroupUsed(pc);
 +	/*
 +	 * pc->mem_cgroup is not cleared here. It will be accessed when it's
 +	 * freed from LRU. This is safe because uncharged page is expected not
 +	 * to be reused (freed soon). Exception is SwapCache, it's handled by
 +	 * special functions.
 +	 */
 +
 +	unlock_page_cgroup(pc);
 +	/*
 +	 * even after unlock, we have memcg->memory.usage here and this memcg
 +	 * will never be freed.
 +	 */
 +	memcg_check_events(memcg, page);
 +	if (do_swap_account && ctype == MEM_CGROUP_CHARGE_TYPE_SWAPOUT) {
 +		mem_cgroup_swap_statistics(memcg, true);
 +		mem_cgroup_get(memcg);
 +	}
 +	/*
 +	 * Migration does not charge the page_counter for the
 +	 * replacement page, so leave it alone when phasing out the
 +	 * page that is unused after the migration.
 +	 */
 +	if (!end_migration && !mem_cgroup_is_root(memcg))
 +		mem_cgroup_do_uncharge(memcg, nr_pages, ctype);
 +
 +	return memcg;
 +
 +unlock_out:
 +	unlock_page_cgroup(pc);
 +	return NULL;
 +}
 +
 +void mem_cgroup_uncharge_page(struct page *page)
 +{
 +	/* early check. */
 +	if (page_mapped(page))
 +		return;
 +	VM_BUG_ON_PAGE(page->mapping && !PageAnon(page), page);
 +	/*
 +	 * If the page is in swap cache, uncharge should be deferred
 +	 * to the swap path, which also properly accounts swap usage
 +	 * and handles memcg lifetime.
 +	 *
 +	 * Note that this check is not stable and reclaim may add the
 +	 * page to swap cache at any time after this.  However, if the
 +	 * page is not in swap cache by the time page->mapcount hits
 +	 * 0, there won't be any page table references to the swap
 +	 * slot, and reclaim will free it and not actually write the
 +	 * page to disk.
 +	 */
 +	if (PageSwapCache(page))
 +		return;
 +	__mem_cgroup_uncharge_common(page, MEM_CGROUP_CHARGE_TYPE_ANON, false);
 +}
 +
 +void mem_cgroup_uncharge_cache_page(struct page *page)
 +{
 +	VM_BUG_ON_PAGE(page_mapped(page), page);
 +	VM_BUG_ON_PAGE(page->mapping, page);
 +	__mem_cgroup_uncharge_common(page, MEM_CGROUP_CHARGE_TYPE_CACHE, false);
 +}
 +
 +/*
 + * Batch_start/batch_end is called in unmap_page_range/invlidate/trucate.
 + * In that cases, pages are freed continuously and we can expect pages
 + * are in the same memcg. All these calls itself limits the number of
 + * pages freed at once, then uncharge_start/end() is called properly.
 + * This may be called prural(2) times in a context,
 + */
 +
 +void mem_cgroup_uncharge_start(void)
 +{
 +	current->memcg_batch.do_batch++;
 +	/* We can do nest. */
 +	if (current->memcg_batch.do_batch == 1) {
 +		current->memcg_batch.memcg = NULL;
 +		current->memcg_batch.nr_pages = 0;
 +		current->memcg_batch.memsw_nr_pages = 0;
 +	}
 +}
 +
 +void mem_cgroup_uncharge_end(void)
 +{
 +	struct memcg_batch_info *batch = &current->memcg_batch;
 +
 +	if (!batch->do_batch)
 +		return;
 +
 +	batch->do_batch--;
 +	if (batch->do_batch) /* If stacked, do nothing. */
 +		return;
 +
 +	if (!batch->memcg)
 +		return;
 +	/*
 +	 * This "batch->memcg" is valid without any css_get/put etc...
 +	 * bacause we hide charges behind us.
 +	 */
 +	if (batch->nr_pages)
 +		page_counter_uncharge(&batch->memcg->memory, batch->nr_pages);
 +	if (batch->memsw_nr_pages)
 +		page_counter_uncharge(&batch->memcg->memsw, batch->memsw_nr_pages);
 +	memcg_oom_recover(batch->memcg);
 +	/* forget this pointer (for sanity check) */
 +	batch->memcg = NULL;
 +}
 +
 +#ifdef CONFIG_SWAP
 +/*
 + * called after __delete_from_swap_cache() and drop "page" account.
 + * memcg information is recorded to swap_cgroup of "ent"
 + */
 +void
 +mem_cgroup_uncharge_swapcache(struct page *page, swp_entry_t ent, bool swapout)
 +{
 +	struct mem_cgroup *memcg;
 +	int ctype = MEM_CGROUP_CHARGE_TYPE_SWAPOUT;
 +
 +	if (!swapout) /* this was a swap cache but the swap is unused ! */
 +		ctype = MEM_CGROUP_CHARGE_TYPE_DROP;
 +
 +	memcg = __mem_cgroup_uncharge_common(page, ctype, false);
 +
 +	/*
 +	 * record memcg information,  if swapout && memcg != NULL,
 +	 * mem_cgroup_get() was called in uncharge().
 +	 */
 +	if (do_swap_account && swapout && memcg)
 +		swap_cgroup_record(ent, mem_cgroup_id(memcg));
 +}
 +#endif
 +
 +#ifdef CONFIG_MEMCG_SWAP
 +/*
 + * called from swap_entry_free(). remove record in swap_cgroup and
 + * uncharge "memsw" account.
 + */
 +void mem_cgroup_uncharge_swap(swp_entry_t ent)
 +{
 +	struct mem_cgroup *memcg;
 +	unsigned short id;
 +
 +	if (!do_swap_account)
 +		return;
 +
 +	id = swap_cgroup_record(ent, 0);
 +	rcu_read_lock();
 +	memcg = mem_cgroup_lookup(id);
 +	if (memcg) {
 +		/*
 +		 * We uncharge this because swap is freed.
 +		 * This memcg can be obsolete one. We avoid calling css_tryget
 +		 */
 +		if (!mem_cgroup_is_root(memcg))
 +			page_counter_uncharge(&memcg->memsw, 1);
 +		mem_cgroup_swap_statistics(memcg, false);
 +		mem_cgroup_put(memcg);
 +	}
 +	rcu_read_unlock();
 +}
 +
 +/**
 + * mem_cgroup_move_swap_account - move swap charge and swap_cgroup's record.
 + * @entry: swap entry to be moved
 + * @from:  mem_cgroup which the entry is moved from
 + * @to:  mem_cgroup which the entry is moved to
 + *
 + * It succeeds only when the swap_cgroup's record for this entry is the same
 + * as the mem_cgroup's id of @from.
 + *
 + * Returns 0 on success, -EINVAL on failure.
 + *
 + * The caller must have charged to @to, IOW, called page_counter_charge() about
 + * both res and memsw, and called css_get().
 + */
 +static int mem_cgroup_move_swap_account(swp_entry_t entry,
 +				struct mem_cgroup *from, struct mem_cgroup *to)
 +{
 +	unsigned short old_id, new_id;
 +
 +	old_id = mem_cgroup_id(from);
 +	new_id = mem_cgroup_id(to);
 +
 +	if (swap_cgroup_cmpxchg(entry, old_id, new_id) == old_id) {
 +		mem_cgroup_swap_statistics(from, false);
 +		mem_cgroup_swap_statistics(to, true);
 +		/*
 +		 * This function is only called from task migration context now.
 +		 * It postpones page_counter and refcount handling till the end
 +		 * of task migration(mem_cgroup_clear_mc()) for performance
 +		 * improvement. But we cannot postpone mem_cgroup_get(to)
 +		 * because if the process that has been moved to @to does
 +		 * swap-in, the refcount of @to might be decreased to 0.
 +		 */
 +		mem_cgroup_get(to);
 +		return 0;
 +	}
 +	return -EINVAL;
 +}
 +#else
 +static inline int mem_cgroup_move_swap_account(swp_entry_t entry,
 +				struct mem_cgroup *from, struct mem_cgroup *to)
 +{
 +	return -EINVAL;
 +}
 +#endif
 +
 +/*
 + * Before starting migration, account PAGE_SIZE to mem_cgroup that the old
 + * page belongs to.
 + */
 +void mem_cgroup_prepare_migration(struct page *page, struct page *newpage,
 +				  struct mem_cgroup **memcgp)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	unsigned int nr_pages = 1;
 +	struct page_cgroup *pc;
 +	enum charge_type ctype;
 +
 +	*memcgp = NULL;
 +
 +	if (mem_cgroup_disabled())
 +		return;
 +
 +	if (PageTransHuge(page))
 +		nr_pages <<= compound_order(page);
 +
 +	pc = lookup_page_cgroup(page);
 +	lock_page_cgroup(pc);
 +	if (PageCgroupUsed(pc)) {
 +		memcg = pc->mem_cgroup;
 +		css_get(&memcg->css);
 +		/*
 +		 * At migrating an anonymous page, its mapcount goes down
 +		 * to 0 and uncharge() will be called. But, even if it's fully
 +		 * unmapped, migration may fail and this page has to be
 +		 * charged again. We set MIGRATION flag here and delay uncharge
 +		 * until end_migration() is called
 +		 *
 +		 * Corner Case Thinking
 +		 * A)
 +		 * When the old page was mapped as Anon and it's unmap-and-freed
 +		 * while migration was ongoing.
 +		 * If unmap finds the old page, uncharge() of it will be delayed
 +		 * until end_migration(). If unmap finds a new page, it's
 +		 * uncharged when it make mapcount to be 1->0. If unmap code
 +		 * finds swap_migration_entry, the new page will not be mapped
 +		 * and end_migration() will find it(mapcount==0).
 +		 *
 +		 * B)
 +		 * When the old page was mapped but migraion fails, the kernel
 +		 * remaps it. A charge for it is kept by MIGRATION flag even
 +		 * if mapcount goes down to 0. We can do remap successfully
 +		 * without charging it again.
 +		 *
 +		 * C)
 +		 * The "old" page is under lock_page() until the end of
 +		 * migration, so, the old page itself will not be swapped-out.
 +		 * If the new page is swapped out before end_migraton, our
 +		 * hook to usual swap-out path will catch the event.
 +		 */
 +		if (PageAnon(page))
 +			SetPageCgroupMigration(pc);
 +	}
 +	unlock_page_cgroup(pc);
 +	/*
 +	 * If the page is not charged at this point,
 +	 * we return here.
 +	 */
 +	if (!memcg)
 +		return;
 +
 +	*memcgp = memcg;
 +	/*
 +	 * We charge new page before it's used/mapped. So, even if unlock_page()
 +	 * is called before end_migration, we can catch all events on this new
 +	 * page. In the case new page is migrated but not remapped, new page's
 +	 * mapcount will be finally 0 and we call uncharge in end_migration().
 +	 */
 +	if (PageAnon(page))
 +		ctype = MEM_CGROUP_CHARGE_TYPE_ANON;
 +	else
 +		ctype = MEM_CGROUP_CHARGE_TYPE_CACHE;
 +	/*
 +	 * The page is committed to the memcg, but it's not actually
 +	 * charged to the page_counter since we plan on replacing the
 +	 * old one and only one page is going to be left afterwards.
 +	 */
 +	__mem_cgroup_commit_charge(memcg, newpage, nr_pages, ctype, false);
 +}
 +
 +/* remove redundant charge if migration failed*/
 +void mem_cgroup_end_migration(struct mem_cgroup *memcg,
 +	struct page *oldpage, struct page *newpage, bool migration_ok)
 +{
 +	struct page *used, *unused;
 +	struct page_cgroup *pc;
 +	bool anon;
 +
 +	if (!memcg)
 +		return;
 +
 +	if (!migration_ok) {
 +		used = oldpage;
 +		unused = newpage;
 +	} else {
 +		used = newpage;
 +		unused = oldpage;
 +	}
 +	anon = PageAnon(used);
 +	__mem_cgroup_uncharge_common(unused,
 +				     anon ? MEM_CGROUP_CHARGE_TYPE_ANON
 +				     : MEM_CGROUP_CHARGE_TYPE_CACHE,
 +				     true);
 +	css_put(&memcg->css);
 +	/*
 +	 * We disallowed uncharge of pages under migration because mapcount
 +	 * of the page goes down to zero, temporarly.
 +	 * Clear the flag and check the page should be charged.
 +	 */
 +	pc = lookup_page_cgroup(oldpage);
 +	lock_page_cgroup(pc);
 +	ClearPageCgroupMigration(pc);
 +	unlock_page_cgroup(pc);
 +
 +	/*
 +	 * If a page is a file cache, radix-tree replacement is very atomic
 +	 * and we can skip this check. When it was an Anon page, its mapcount
 +	 * goes down to 0. But because we added MIGRATION flage, it's not
 +	 * uncharged yet. There are several case but page->mapcount check
 +	 * and USED bit check in mem_cgroup_uncharge_page() will do enough
 +	 * check. (see prepare_charge() also)
 +	 */
 +	if (anon)
 +		mem_cgroup_uncharge_page(used);
 +}
 +
 +/*
 + * At replace page cache, newpage is not under any memcg but it's on
 + * LRU. So, this function doesn't touch page_counter but handles LRU
 + * in correct way. Both pages are locked so we cannot race with uncharge.
 + */
 +void mem_cgroup_replace_page_cache(struct page *oldpage,
 +				  struct page *newpage)
 +{
 +	struct mem_cgroup *memcg = NULL;
 +	struct page_cgroup *pc;
 +	enum charge_type type = MEM_CGROUP_CHARGE_TYPE_CACHE;
 +
 +	if (mem_cgroup_disabled())
 +		return;
 +
 +	pc = lookup_page_cgroup(oldpage);
 +	/* fix accounting on old pages */
 +	lock_page_cgroup(pc);
 +	if (PageCgroupUsed(pc)) {
 +		memcg = pc->mem_cgroup;
 +		mem_cgroup_charge_statistics(memcg, oldpage, false, -1);
 +		ClearPageCgroupUsed(pc);
 +	}
 +	unlock_page_cgroup(pc);
 +
 +	/*
 +	 * When called from shmem_replace_page(), in some cases the
 +	 * oldpage has already been charged, and in some cases not.
 +	 */
 +	if (!memcg)
 +		return;
 +	/*
 +	 * Even if newpage->mapping was NULL before starting replacement,
 +	 * the newpage may be on LRU(or pagevec for LRU) already. We lock
 +	 * LRU while we overwrite pc->mem_cgroup.
 +	 */
 +	__mem_cgroup_commit_charge(memcg, newpage, 1, type, true);
 +}
 +
 +#ifdef CONFIG_DEBUG_VM
 +static struct page_cgroup *lookup_page_cgroup_used(struct page *page)
 +{
 +	struct page_cgroup *pc;
 +
 +	pc = lookup_page_cgroup(page);
 +	/*
 +	 * Can be NULL while feeding pages into the page allocator for
 +	 * the first time, i.e. during boot or memory hotplug;
 +	 * or when mem_cgroup_disabled().
 +	 */
 +	if (likely(pc) && PageCgroupUsed(pc))
 +		return pc;
 +	return NULL;
 +}
 +
 +bool mem_cgroup_bad_page_check(struct page *page)
 +{
 +	if (mem_cgroup_disabled())
 +		return false;
 +
 +	return lookup_page_cgroup_used(page) != NULL;
 +}
 +
 +void mem_cgroup_print_bad_page(struct page *page)
 +{
 +	struct page_cgroup *pc;
 +
 +	pc = lookup_page_cgroup_used(page);
 +	if (pc) {
 +		pr_alert("pc:%p pc->flags:%lx pc->mem_cgroup:%p\n",
 +			 pc, pc->flags, pc->mem_cgroup);
 +	}
 +}
 +#endif
 +
 +static int mem_cgroup_resize_limit(struct mem_cgroup *memcg,
 +				   unsigned long limit)
 +{
 +	unsigned long curusage;
 +	unsigned long oldusage;
 +	unsigned long memswlimit;
 +	bool enlarge = false;
 +	int retry_count;
 +	int ret;
 +
 +	/*
 +	 * For keeping hierarchical_reclaim simple, how long we should retry
 +	 * is depends on callers. We set our retry-count to be function
 +	 * of # of children which we should visit in this loop.
 +	 */
 +	retry_count = MEM_CGROUP_RECLAIM_RETRIES *
 +		      mem_cgroup_count_children(memcg);
  
  	oldusage = page_counter_read(&memcg->memory);
  
* Unmerged path include/linux/page_counter.h
diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c
index 42e59aa80d47..6536d14605b1 100644
--- a/mm/hugetlb_cgroup.c
+++ b/mm/hugetlb_cgroup.c
@@ -198,7 +198,8 @@ again:
 	}
 	rcu_read_unlock();
 
-	ret = page_counter_try_charge(&h_cg->hugepage[idx], nr_pages, &counter);
+	if (!page_counter_try_charge(&h_cg->hugepage[idx], nr_pages, &counter))
+		ret = -ENOMEM;
 	css_put(&h_cg->css);
 done:
 	*ptr = h_cg;
* Unmerged path mm/memcontrol.c
diff --git a/mm/page_counter.c b/mm/page_counter.c
index f0cbc0825426..727d76ce0f5b 100644
--- a/mm/page_counter.c
+++ b/mm/page_counter.c
@@ -61,12 +61,12 @@ void page_counter_charge(struct page_counter *counter, unsigned long nr_pages)
  * @nr_pages: number of pages to charge
  * @fail: points first counter to hit its limit, if any
  *
- * Returns 0 on success, or -ENOMEM and @fail if the counter or one of
- * its ancestors has hit its configured limit.
+ * Returns %true on success, or %false and @fail if the counter or one
+ * of its ancestors has hit its configured limit.
  */
-int page_counter_try_charge(struct page_counter *counter,
-			    unsigned long nr_pages,
-			    struct page_counter **fail)
+bool page_counter_try_charge(struct page_counter *counter,
+			     unsigned long nr_pages,
+			     struct page_counter **fail)
 {
 	struct page_counter *c;
 
@@ -104,13 +104,13 @@ int page_counter_try_charge(struct page_counter *counter,
 		if (new > c->watermark)
 			c->watermark = new;
 	}
-	return 0;
+	return true;
 
 failed:
 	for (c = counter; c != *fail; c = c->parent)
 		page_counter_cancel(c, nr_pages);
 
-	return -ENOMEM;
+	return false;
 }
 
 /**
