net/mlx5e: Convert large order kzalloc allocations to kvzalloc

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Convert large order kzalloc allocations to kvzalloc (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 96.67%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit ca11b798998a62c2bf87ea0477b5c60af25ba46d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ca11b798.failed

Replace calls to kzalloc_node with kvzalloc_node, as it fallsback
to lower-order pages if the higher-order trials fail.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit ca11b798998a62c2bf87ea0477b5c60af25ba46d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a0bfa9c3ce47,42ef8c818544..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -289,66 -350,16 +289,71 @@@ static inline void mlx5e_build_umr_wqe(
  static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
  				     struct mlx5e_channel *c)
  {
 -	int wq_sz = mlx5_wq_ll_get_size(&rq->mpwqe.wq);
 +	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
 +	int mtt_sz = mlx5e_get_wqe_mtt_sz();
 +	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
 +	int i;
  
++<<<<<<< HEAD
 +	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
 +				      GFP_KERNEL, cpu_to_node(c->cpu));
++=======
+ 	rq->mpwqe.info = kvzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				       GFP_KERNEL, cpu_to_node(c->cpu));
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  	if (!rq->mpwqe.info)
 -		return -ENOMEM;
 +		goto err_out;
  
 -	mlx5e_build_umr_wqe(rq, &c->icosq, &rq->mpwqe.umr_wqe);
 +	/* We allocate more than mtt_sz as we will align the pointer */
 +	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
 +					cpu_to_node(c->cpu));
 +	if (unlikely(!rq->mpwqe.mtt_no_align))
 +		goto err_free_wqe_info;
 +
 +	for (i = 0; i < wq_sz; i++) {
 +		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
 +
 +		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
 +					MLX5_UMR_ALIGN);
 +		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
 +						  PCI_DMA_TODEVICE);
 +		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
 +			goto err_unmap_mtts;
 +
 +		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
 +	}
  
  	return 0;
 +
 +err_unmap_mtts:
 +	while (--i >= 0) {
 +		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
 +
 +		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
 +				 PCI_DMA_TODEVICE);
 +	}
 +	kfree(rq->mpwqe.mtt_no_align);
 +err_free_wqe_info:
 +	kfree(rq->mpwqe.info);
 +
 +err_out:
 +	return -ENOMEM;
 +}
 +
 +static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
 +{
 +	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
 +	int mtt_sz = mlx5e_get_wqe_mtt_sz();
 +	int i;
 +
 +	for (i = 0; i < wq_sz; i++) {
 +		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
 +
 +		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
 +				 PCI_DMA_TODEVICE);
 +	}
 +	kfree(rq->mpwqe.mtt_no_align);
 +	kfree(rq->mpwqe.info);
  }
  
  static int mlx5e_create_umr_mkey(struct mlx5_core_dev *mdev,
@@@ -554,8 -667,16 +559,21 @@@ static int mlx5e_alloc_rq(struct mlx5e_
  
  	return 0;
  
++<<<<<<< HEAD
 +err_destroy_umr_mkey:
 +	mlx5_core_destroy_mkey(mdev, &rq->umr_mkey);
++=======
+ err_free:
+ 	switch (rq->wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		kvfree(rq->mpwqe.info);
+ 		mlx5_core_destroy_mkey(mdev, &rq->umr_mkey);
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		kvfree(rq->wqe.frags);
+ 		mlx5e_free_di_list(rq);
+ 	}
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  
  err_rq_wq_destroy:
  	if (rq->xdp_prog)
@@@ -572,13 -696,18 +590,17 @@@ static void mlx5e_free_rq(struct mlx5e_
  	if (rq->xdp_prog)
  		bpf_prog_put(rq->xdp_prog);
  
 -	xdp_rxq_info_unreg(&rq->xdp_rxq);
 -	if (rq->page_pool)
 -		page_pool_destroy(rq->page_pool);
 -
  	switch (rq->wq_type) {
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
++<<<<<<< HEAD
 +		mlx5e_rq_free_mpwqe_info(rq);
++=======
+ 		kvfree(rq->mpwqe.info);
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  		mlx5_core_destroy_mkey(rq->mdev, &rq->umr_mkey);
  		break;
 -	default: /* MLX5_WQ_TYPE_CYCLIC */
 -		kvfree(rq->wqe.frags);
 -		mlx5e_free_di_list(rq);
 +	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 +		kfree(rq->wqe.frag_info);
  	}
  
  	for (i = rq->page_cache.head; i != rq->page_cache.tail;
@@@ -840,8 -972,8 +862,13 @@@ static int mlx5e_alloc_xdpsq_db(struct 
  {
  	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  
++<<<<<<< HEAD
 +	sq->db.di = kzalloc_node(sizeof(*sq->db.di) * wq_sz,
 +				     GFP_KERNEL, numa);
++=======
+ 	sq->db.di = kvzalloc_node(sizeof(*sq->db.di) * wq_sz,
+ 				  GFP_KERNEL, numa);
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  	if (!sq->db.di) {
  		mlx5e_free_xdpsq_db(sq);
  		return -ENOMEM;
@@@ -898,8 -1031,8 +925,13 @@@ static int mlx5e_alloc_icosq_db(struct 
  {
  	u8 wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  
++<<<<<<< HEAD
 +	sq->db.ico_wqe = kzalloc_node(sizeof(*sq->db.ico_wqe) * wq_sz,
 +				      GFP_KERNEL, numa);
++=======
+ 	sq->db.ico_wqe = kvzalloc_node(sizeof(*sq->db.ico_wqe) * wq_sz,
+ 				       GFP_KERNEL, numa);
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  	if (!sq->db.ico_wqe)
  		return -ENOMEM;
  
@@@ -955,10 -1086,10 +987,17 @@@ static int mlx5e_alloc_txqsq_db(struct 
  	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
  
++<<<<<<< HEAD
 +	sq->db.dma_fifo = kzalloc_node(df_sz * sizeof(*sq->db.dma_fifo),
 +					   GFP_KERNEL, numa);
 +	sq->db.wqe_info = kzalloc_node(wq_sz * sizeof(*sq->db.wqe_info),
 +					   GFP_KERNEL, numa);
++=======
+ 	sq->db.dma_fifo = kvzalloc_node(df_sz * sizeof(*sq->db.dma_fifo),
+ 					GFP_KERNEL, numa);
+ 	sq->db.wqe_info = kvzalloc_node(wq_sz * sizeof(*sq->db.wqe_info),
+ 					GFP_KERNEL, numa);
++>>>>>>> ca11b798998a (net/mlx5e: Convert large order kzalloc allocations to kvzalloc)
  	if (!sq->db.dma_fifo || !sq->db.wqe_info) {
  		mlx5e_free_txqsq_db(sq);
  		return -ENOMEM;
@@@ -1764,9 -2018,79 +1803,9 @@@ static void mlx5e_close_channel(struct 
  	mlx5e_close_cq(&c->icosq.cq);
  	netif_napi_del(&c->napi);
  
- 	kfree(c);
+ 	kvfree(c);
  }
  
 -#define DEFAULT_FRAG_SIZE (2048)
 -
 -static void mlx5e_build_rq_frags_info(struct mlx5_core_dev *mdev,
 -				      struct mlx5e_params *params,
 -				      struct mlx5e_rq_frags_info *info)
 -{
 -	u32 byte_count = MLX5E_SW2HW_MTU(params, params->sw_mtu);
 -	int frag_size_max = DEFAULT_FRAG_SIZE;
 -	u32 buf_size = 0;
 -	int i;
 -
 -#ifdef CONFIG_MLX5_EN_IPSEC
 -	if (MLX5_IPSEC_DEV(mdev))
 -		byte_count += MLX5E_METADATA_ETHER_LEN;
 -#endif
 -
 -	if (mlx5e_rx_is_linear_skb(mdev, params)) {
 -		int frag_stride;
 -
 -		frag_stride = mlx5e_rx_get_linear_frag_sz(params);
 -		frag_stride = roundup_pow_of_two(frag_stride);
 -
 -		info->arr[0].frag_size = byte_count;
 -		info->arr[0].frag_stride = frag_stride;
 -		info->num_frags = 1;
 -		info->wqe_bulk = PAGE_SIZE / frag_stride;
 -		goto out;
 -	}
 -
 -	if (byte_count > PAGE_SIZE +
 -	    (MLX5E_MAX_RX_FRAGS - 1) * frag_size_max)
 -		frag_size_max = PAGE_SIZE;
 -
 -	i = 0;
 -	while (buf_size < byte_count) {
 -		int frag_size = byte_count - buf_size;
 -
 -		if (i < MLX5E_MAX_RX_FRAGS - 1)
 -			frag_size = min(frag_size, frag_size_max);
 -
 -		info->arr[i].frag_size = frag_size;
 -		info->arr[i].frag_stride = roundup_pow_of_two(frag_size);
 -
 -		buf_size += frag_size;
 -		i++;
 -	}
 -	info->num_frags = i;
 -	/* number of different wqes sharing a page */
 -	info->wqe_bulk = 1 + (info->num_frags % 2);
 -
 -out:
 -	info->wqe_bulk = max_t(u8, info->wqe_bulk, 8);
 -	info->log_num_frags = order_base_2(info->num_frags);
 -}
 -
 -static inline u8 mlx5e_get_rqwq_log_stride(u8 wq_type, int ndsegs)
 -{
 -	int sz = sizeof(struct mlx5_wqe_data_seg) * ndsegs;
 -
 -	switch (wq_type) {
 -	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
 -		sz += sizeof(struct mlx5e_rx_wqe_ll);
 -		break;
 -	default: /* MLX5_WQ_TYPE_CYCLIC */
 -		sz += sizeof(struct mlx5e_rx_wqe_cyc);
 -	}
 -
 -	return order_base_2(sz);
 -}
 -
  static void mlx5e_build_rq_param(struct mlx5e_priv *priv,
  				 struct mlx5e_params *params,
  				 struct mlx5e_rq_param *param)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
