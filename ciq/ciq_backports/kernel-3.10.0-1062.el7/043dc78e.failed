net/mlx5e: TX, Use actual WQE size for SQ edge fill

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: TX, Use actual WQE size for SQ edge fill (Alaa Hleihel) [1641354 1642498]
Rebuild_FUZZ: 95.92%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 043dc78ecf07f3fc5b87270518d7f322aea2f748
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/043dc78e.failed

We fill SQ edge with NOPs to avoid WQEs wrap.
Here, instead of doing that in advance for the maximum possible
WQE size, we do it on-demand using the actual WQE size.
We re-order some parts in mlx5e_sq_xmit to finish the calculation
of WQE size (ds_cnt) before doing any writes to the WQE buffer.

When SQ work queue is fragmented (introduced in an downstream patch),
dealing with WQE wraps becomes more frequent. This change would drastically
reduce the overhead in this case.

Performance tests:
ConnectX-5 100Gbps, CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
Packet rate of 64B packets, single transmit ring, size 8K.

Before: 14.9 Mpps
After:  15.8 Mpps

Improvement of 6%.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 043dc78ecf07f3fc5b87270518d7f322aea2f748)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 13394c230883,3c0f0a0343fd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -429,8 -439,6 +429,11 @@@ struct mlx5e_icosq 
  	struct mlx5_wq_cyc         wq;
  	void __iomem              *uar_map;
  	u32                        sqn;
++<<<<<<< HEAD
 +	u16                        edge;
 +	__be32                     mkey_be;
++=======
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  	unsigned long              state;
  
  	/* control path */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a11f70af9727,a8b1e43384ca..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -928,9 -959,7 +928,13 @@@ static int mlx5e_alloc_icosq(struct mlx
  	if (err)
  		goto err_sq_wq_destroy;
  
++<<<<<<< HEAD
 +	sq->edge = (sq->wq.sz_m1 + 1) - MLX5E_ICOSQ_MAX_WQEBBS;
 +
 +        return 0;
++=======
+ 	return 0;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  
  err_sq_wq_destroy:
  	mlx5_wq_destroy(&sq->wq_ctrl);
@@@ -1002,7 -1034,8 +1006,12 @@@ static int mlx5e_alloc_txqsq(struct mlx
  	if (err)
  		goto err_sq_wq_destroy;
  
++<<<<<<< HEAD
 +	sq->edge = (sq->wq.sz_m1 + 1) - MLX5_SEND_WQE_MAX_WQEBBS;
++=======
+ 	INIT_WORK(&sq->dim.work, mlx5e_tx_dim_work);
+ 	sq->dim.mode = params->tx_cq_moderation.cq_period_mode;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  
  	return 0;
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 29331bf7aa1d,f4d2c8886492..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -440,18 -378,82 +440,59 @@@ static void mlx5e_post_rx_mpwqe(struct 
  	mlx5_wq_ll_update_db_record(wq);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 +{
 +	int err;
 +
 +	err = mlx5e_alloc_rx_umr_mpwqe(rq, ix);
 +	if (unlikely(err)) {
 +		rq->stats->buff_alloc_err++;
 +		return err;
++=======
+ static inline u16 mlx5e_icosq_wrap_cnt(struct mlx5e_icosq *sq)
+ {
+ 	return sq->pc >> MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ }
+ 
+ static inline void mlx5e_fill_icosq_edge(struct mlx5e_icosq *sq,
+ 					 struct mlx5_wq_cyc *wq,
+ 					 u16 pi)
+ {
+ 	struct mlx5e_sq_wqe_info *edge_wi, *wi = &sq->db.ico_wqe[pi];
+ 	u8 nnops = mlx5_wq_cyc_get_size(wq) - pi;
+ 
+ 	edge_wi = wi + nnops;
+ 
+ 	/* fill sq edge with nops to avoid wqe wrapping two pages */
+ 	for (; wi < edge_wi; wi++) {
+ 		wi->opcode = MLX5_OPCODE_NOP;
+ 		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
+ 	}
+ }
+ 
+ static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
+ {
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
+ 	struct mlx5e_icosq *sq = &rq->channel->icosq;
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_umr_wqe *umr_wqe;
+ 	u16 xlt_offset = ix << (MLX5E_LOG_ALIGNED_MPWQE_PPW - 1);
+ 	u16 pi;
+ 	int err;
+ 	int i;
+ 
+ 	pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+ 
+ 	if (unlikely(pi + MLX5E_UMR_WQEBBS > mlx5_wq_cyc_get_size(wq))) {
+ 		mlx5e_fill_icosq_edge(sq, wq, pi);
+ 		pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  	}
 -
 -	umr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 -	if (unlikely(mlx5e_icosq_wrap_cnt(sq) < 2))
 -		memcpy(umr_wqe, &rq->mpwqe.umr_wqe,
 -		       offsetof(struct mlx5e_umr_wqe, inline_mtts));
 -
 -	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++) {
 -		err = mlx5e_page_alloc_mapped(rq, dma_info);
 -		if (unlikely(err))
 -			goto err_unmap;
 -		umr_wqe->inline_mtts[i].ptag = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
 -	}
 -
 -	bitmap_zero(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 -	wi->consumed_strides = 0;
 -
  	rq->mpwqe.umr_in_progress = true;
 -
 -	umr_wqe->ctrl.opmod_idx_opcode =
 -		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 -			    MLX5_OPCODE_UMR);
 -	umr_wqe->uctrl.xlt_offset = cpu_to_be16(xlt_offset);
 -
 -	sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_UMR;
 -	sq->pc += MLX5E_UMR_WQEBBS;
 -	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &umr_wqe->ctrl);
 -
 +	mlx5e_post_umr_wqe(rq, ix);
  	return 0;
 -
 -err_unmap:
 -	while (--i >= 0) {
 -		dma_info--;
 -		mlx5e_page_release(rq, dma_info, true);
 -	}
 -	rq->stats.buff_alloc_err++;
 -
 -	return err;
  }
  
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 31cc86df395a,fc68e72b0b2b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -221,25 -230,20 +221,21 @@@ mlx5e_txwqe_build_eseg_csum(struct mlx5
  }
  
  static inline u16
- mlx5e_txwqe_build_eseg_gso(struct mlx5e_txqsq *sq, struct sk_buff *skb,
- 			   struct mlx5_wqe_eth_seg *eseg, unsigned int *num_bytes)
+ mlx5e_tx_get_gso_ihs(struct mlx5e_txqsq *sq, struct sk_buff *skb)
  {
 +	struct mlx5e_sq_stats *stats = sq->stats;
  	u16 ihs;
  
- 	eseg->mss    = cpu_to_be16(skb_shinfo(skb)->gso_size);
- 
  	if (skb->encapsulation) {
  		ihs = skb_inner_transport_offset(skb) + inner_tcp_hdrlen(skb);
 -		sq->stats.tso_inner_packets++;
 -		sq->stats.tso_inner_bytes += skb->len - ihs;
 +		stats->tso_inner_packets++;
 +		stats->tso_inner_bytes += skb->len - ihs;
  	} else {
  		ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
 -		sq->stats.tso_packets++;
 -		sq->stats.tso_bytes += skb->len - ihs;
 +		stats->tso_packets++;
 +		stats->tso_bytes += skb->len - ihs;
  	}
  
- 	*num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
  	return ihs;
  }
  
@@@ -286,11 -290,33 +282,29 @@@ mlx5e_txwqe_build_dsegs(struct mlx5e_tx
  	}
  
  	return num_dma;
 -
 -dma_unmap_wqe_err:
 -	mlx5e_dma_unmap_wqe_err(sq, num_dma);
 -	return -ENOMEM;
  }
  
+ static inline void mlx5e_fill_sq_edge(struct mlx5e_txqsq *sq,
+ 				      struct mlx5_wq_cyc *wq,
+ 				      u16 pi)
+ {
+ 	struct mlx5e_tx_wqe_info *edge_wi, *wi = &sq->db.wqe_info[pi];
+ 	u8 nnops = mlx5_wq_cyc_get_size(wq) - pi;
+ 
+ 	edge_wi = wi + nnops;
+ 
+ 	/* fill sq edge with nops to avoid wqe wrap around */
+ 	for (; wi < edge_wi; wi++) {
+ 		wi->skb        = NULL;
+ 		wi->num_wqebbs = 1;
+ 		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
+ 	}
+ 	sq->stats.nop += nnops;
+ }
+ 
  static inline void
  mlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,
- 		     u8 opcode, u16 ds_cnt, u32 num_bytes, u8 num_dma,
+ 		     u8 opcode, u16 ds_cnt, u8 num_wqebbs, u32 num_bytes, u8 num_dma,
  		     struct mlx5e_tx_wqe_info *wi, struct mlx5_wqe_ctrl_seg *cseg)
  {
  	struct mlx5_wq_cyc *wq = &sq->wq;
@@@ -317,53 -342,79 +330,111 @@@
  
  	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
  		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
++<<<<<<< HEAD
 +
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.wqe_info[pi].skb = NULL;
 +		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
 +		sq->stats->nop++;
 +	}
 +}
 +
 +static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 +				 struct mlx5e_tx_wqe *wqe, u16 pi)
- {
- 	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
++=======
+ }
  
- 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
- 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ #define INL_HDR_START_SZ (sizeof(((struct mlx5_wqe_eth_seg *)NULL)->inline_hdr.start))
+ 
+ netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 			  struct mlx5e_tx_wqe *wqe, u16 pi)
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5_wqe_eth_seg  *eseg;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 	struct mlx5e_tx_wqe_info *wi;
  
 +	struct mlx5e_sq_stats *stats = sq->stats;
  	unsigned char *skb_data = skb->data;
  	unsigned int skb_len = skb->len;
- 	u8  opcode = MLX5_OPCODE_SEND;
- 	unsigned int num_bytes;
+ 	u16 ds_cnt, ds_cnt_inl = 0;
+ 	u8 num_wqebbs, opcode;
+ 	u16 headlen, ihs;
+ 	u32 num_bytes;
  	int num_dma;
- 	u16 headlen;
- 	u16 ds_cnt;
- 	u16 ihs;
- 
- 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 	__be16 mss;
  
+ 	/* Calc ihs and ds cnt, no writes to wqe yet */
+ 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
  	if (skb_is_gso(skb)) {
++<<<<<<< HEAD
 +		opcode = MLX5_OPCODE_LSO;
 +		ihs = mlx5e_txwqe_build_eseg_gso(sq, skb, eseg, &num_bytes);
 +		stats->packets += skb_shinfo(skb)->gso_segs;
++=======
+ 		opcode    = MLX5_OPCODE_LSO;
+ 		mss       = cpu_to_be16(skb_shinfo(skb)->gso_size);
+ 		ihs       = mlx5e_tx_get_gso_ihs(sq, skb);
+ 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 		sq->stats.packets += skb_shinfo(skb)->gso_segs;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  	} else {
- 		ihs = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
+ 		opcode    = MLX5_OPCODE_SEND;
+ 		mss       = 0;
+ 		ihs       = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
  		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
 -		sq->stats.packets++;
 +		stats->packets++;
  	}
++<<<<<<< HEAD
 +	stats->bytes     += num_bytes;
 +	stats->xmit_more += skb->xmit_more;
++=======
+ 
+ 	sq->stats.bytes     += num_bytes;
+ 	sq->stats.xmit_more += skb->xmit_more;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
+ 
+ 	headlen = skb_len - ihs - skb->data_len;
+ 	ds_cnt += !!headlen;
+ 	ds_cnt += skb_shinfo(skb)->nr_frags;
+ 
+ 	if (ihs) {
+ 		ihs += !!skb_vlan_tag_present(skb) * VLAN_HLEN;
+ 
+ 		ds_cnt_inl = DIV_ROUND_UP(ihs - INL_HDR_START_SZ, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	if (unlikely(pi + num_wqebbs > mlx5_wq_cyc_get_size(wq))) {
+ 		mlx5e_fill_sq_edge(sq, wq, pi);
+ 		mlx5e_sq_fetch_wqe(sq, &wqe, &pi);
+ 	}
+ 
+ 	/* fill wqe */
+ 	wi   = &sq->db.wqe_info[pi];
+ 	cseg = &wqe->ctrl;
+ 	eseg = &wqe->eth;
+ 	dseg =  wqe->data;
+ 
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 
+ 	eseg->mss = mss;
  
- 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
  	if (ihs) {
  		if (skb_vlan_tag_present(skb)) {
++<<<<<<< HEAD
 +			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, ihs, &skb_data, &skb_len);
 +			ihs += VLAN_HLEN;
 +			stats->added_vlan_packets++;
++=======
+ 			mlx5e_insert_vlan(eseg->inline_hdr.start, skb,
+ 					  ihs - VLAN_HLEN, &skb_data, &skb_len);
+ 			sq->stats.added_vlan_packets++;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  		} else {
  			memcpy(eseg->inline_hdr.start, skb_data, ihs);
  			mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
@@@ -375,17 -426,15 +446,15 @@@
  		if (skb->vlan_proto == cpu_to_be16(ETH_P_8021AD))
  			eseg->insert.type |= cpu_to_be16(MLX5_ETH_WQE_SVLAN);
  		eseg->insert.vlan_tci = cpu_to_be16(skb_vlan_tag_get(skb));
 -		sq->stats.added_vlan_packets++;
 +		stats->added_vlan_packets++;
  	}
  
- 	headlen = skb_len - skb->data_len;
- 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen,
- 					  (struct mlx5_wqe_data_seg *)cseg + ds_cnt);
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen, dseg);
  	if (unlikely(num_dma < 0))
 -		goto err_drop;
 +		goto dma_unmap_wqe_err;
  
- 	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt + num_dma,
- 			     num_bytes, num_dma, wi, cseg);
+ 	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt, num_wqebbs, num_bytes,
+ 			     num_dma, wi, cseg);
  
  	return NETDEV_TX_OK;
  
@@@ -592,60 -631,85 +649,110 @@@ mlx5i_txwqe_build_datagram(struct mlx5_
  netdev_tx_t mlx5i_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
  			  struct mlx5_av *av, u32 dqpn, u32 dqkey)
  {
++<<<<<<< HEAD
 +	struct mlx5_wq_cyc       *wq   = &sq->wq;
 +	u16                       pi   = sq->pc & wq->sz_m1;
 +	struct mlx5i_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 +	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
++=======
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5i_tx_wqe *wqe;
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  
- 	struct mlx5_wqe_ctrl_seg     *cseg = &wqe->ctrl;
- 	struct mlx5_wqe_datagram_seg *datagram = &wqe->datagram;
- 	struct mlx5_wqe_eth_seg      *eseg = &wqe->eth;
+ 	struct mlx5_wqe_datagram_seg *datagram;
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5_wqe_eth_seg  *eseg;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 	struct mlx5e_tx_wqe_info *wi;
  
 +	struct mlx5e_sq_stats *stats = sq->stats;
  	unsigned char *skb_data = skb->data;
  	unsigned int skb_len = skb->len;
- 	u8  opcode = MLX5_OPCODE_SEND;
- 	unsigned int num_bytes;
+ 	u16 ds_cnt, ds_cnt_inl = 0;
+ 	u8 num_wqebbs, opcode;
+ 	u16 headlen, ihs, pi;
+ 	u32 num_bytes;
  	int num_dma;
- 	u16 headlen;
- 	u16 ds_cnt;
- 	u16 ihs;
+ 	__be16 mss;
  
- 	memset(wqe, 0, sizeof(*wqe));
+ 	mlx5i_sq_fetch_wqe(sq, &wqe, &pi);
+ 
+ 	/* Calc ihs and ds cnt, no writes to wqe yet */
+ 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
+ 	if (skb_is_gso(skb)) {
+ 		opcode    = MLX5_OPCODE_LSO;
+ 		mss       = cpu_to_be16(skb_shinfo(skb)->gso_size);
+ 		ihs       = mlx5e_tx_get_gso_ihs(sq, skb);
+ 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 		sq->stats.packets += skb_shinfo(skb)->gso_segs;
+ 	} else {
+ 		opcode    = MLX5_OPCODE_SEND;
+ 		mss       = 0;
+ 		ihs       = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
+ 		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
+ 		sq->stats.packets++;
+ 	}
+ 
+ 	sq->stats.bytes     += num_bytes;
+ 	sq->stats.xmit_more += skb->xmit_more;
+ 
+ 	headlen = skb_len - ihs - skb->data_len;
+ 	ds_cnt += !!headlen;
+ 	ds_cnt += skb_shinfo(skb)->nr_frags;
+ 
+ 	if (ihs) {
+ 		ds_cnt_inl = DIV_ROUND_UP(ihs - INL_HDR_START_SZ, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	if (unlikely(pi + num_wqebbs > mlx5_wq_cyc_get_size(wq))) {
+ 		mlx5e_fill_sq_edge(sq, wq, pi);
+ 		mlx5i_sq_fetch_wqe(sq, &wqe, &pi);
+ 	}
+ 
+ 	/* fill wqe */
+ 	wi       = &sq->db.wqe_info[pi];
+ 	cseg     = &wqe->ctrl;
+ 	datagram = &wqe->datagram;
+ 	eseg     = &wqe->eth;
+ 	dseg     =  wqe->data;
  
  	mlx5i_txwqe_build_datagram(av, dqpn, dqkey, datagram);
  
  	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
  
++<<<<<<< HEAD
 +	if (skb_is_gso(skb)) {
 +		opcode = MLX5_OPCODE_LSO;
 +		ihs = mlx5e_txwqe_build_eseg_gso(sq, skb, eseg, &num_bytes);
 +		stats->packets += skb_shinfo(skb)->gso_segs;
 +	} else {
 +		ihs = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
 +		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
 +		stats->packets++;
 +	}
 +
 +	stats->bytes     += num_bytes;
 +	stats->xmit_more += skb->xmit_more;
 +
 +	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
++=======
+ 	eseg->mss = mss;
+ 
++>>>>>>> 043dc78ecf07 (net/mlx5e: TX, Use actual WQE size for SQ edge fill)
  	if (ihs) {
  		memcpy(eseg->inline_hdr.start, skb_data, ihs);
- 		mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
  		eseg->inline_hdr.sz = cpu_to_be16(ihs);
- 		ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr.start), MLX5_SEND_WQE_DS);
+ 		dseg += ds_cnt_inl;
  	}
  
- 	headlen = skb_len - skb->data_len;
- 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen,
- 					  (struct mlx5_wqe_data_seg *)cseg + ds_cnt);
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen, dseg);
  	if (unlikely(num_dma < 0))
 -		goto err_drop;
 +		goto dma_unmap_wqe_err;
  
- 	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt + num_dma,
- 			     num_bytes, num_dma, wi, cseg);
+ 	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt, num_wqebbs, num_bytes,
+ 			     num_dma, wi, cseg);
  
  	return NETDEV_TX_OK;
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.h b/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.h
index 155c71afccb4..66365dd9fcef 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.h
@@ -93,6 +93,29 @@ const struct mlx5e_profile *mlx5i_pkey_get_profile(void);
 /* Extract mlx5e_priv from IPoIB netdev */
 #define mlx5i_epriv(netdev) ((void *)(((struct mlx5i_priv *)netdev_priv(netdev))->mlx5e_priv))
 
+struct mlx5_wqe_eth_pad {
+	u8 rsvd0[16];
+};
+
+struct mlx5i_tx_wqe {
+	struct mlx5_wqe_ctrl_seg     ctrl;
+	struct mlx5_wqe_datagram_seg datagram;
+	struct mlx5_wqe_eth_pad      pad;
+	struct mlx5_wqe_eth_seg      eth;
+	struct mlx5_wqe_data_seg     data[0];
+};
+
+static inline void mlx5i_sq_fetch_wqe(struct mlx5e_txqsq *sq,
+				      struct mlx5i_tx_wqe **wqe,
+				      u16 *pi)
+{
+	struct mlx5_wq_cyc *wq = &sq->wq;
+
+	*pi  = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+	*wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
+	memset(*wqe, 0, sizeof(**wqe));
+}
+
 netdev_tx_t mlx5i_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 			  struct mlx5_av *av, u32 dqpn, u32 dqkey);
 void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
