KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 44883f01fe6ae436a8604c47d8435276fef369b0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/44883f01.failed

Some of the MSRs returned by GET_MSR_INDEX_LIST currently cannot be sent back
to KVM_GET_MSR and/or KVM_SET_MSR; either they can never be sent back, or you
they are only accepted under special conditions.  This makes the API a pain to
use.

To avoid this pain, this patch makes it so that the result of the get-list
ioctl can always be used for host-initiated get and set.  Since we don't have
a separate way to check for read-only MSRs, this means some Hyper-V MSRs are
ignored when written.  Arguably they should not even be in the result of
GET_MSR_INDEX_LIST, but I am leaving there in case userspace is using the
outcome of GET_MSR_INDEX_LIST to derive the support for the corresponding
Hyper-V feature.

	Cc: stable@vger.kernel.org
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 44883f01fe6ae436a8604c47d8435276fef369b0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/hyperv.h
diff --cc arch/x86/kvm/hyperv.c
index a44fd11c98a7,01d209ab5481..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -30,6 -36,393 +30,396 @@@
  
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ static inline u64 synic_read_sint(struct kvm_vcpu_hv_synic *synic, int sint)
+ {
+ 	return atomic64_read(&synic->sint[sint]);
+ }
+ 
+ static inline int synic_get_sint_vector(u64 sint_value)
+ {
+ 	if (sint_value & HV_SYNIC_SINT_MASKED)
+ 		return -1;
+ 	return sint_value & HV_SYNIC_SINT_VECTOR_MASK;
+ }
+ 
+ static bool synic_has_vector_connected(struct kvm_vcpu_hv_synic *synic,
+ 				      int vector)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool synic_has_vector_auto_eoi(struct kvm_vcpu_hv_synic *synic,
+ 				     int vector)
+ {
+ 	int i;
+ 	u64 sint_value;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		sint_value = synic_read_sint(synic, i);
+ 		if (synic_get_sint_vector(sint_value) == vector &&
+ 		    sint_value & HV_SYNIC_SINT_AUTO_EOI)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
+ 				int vector)
+ {
+ 	if (vector < HV_SYNIC_FIRST_VALID_VECTOR)
+ 		return;
+ 
+ 	if (synic_has_vector_connected(synic, vector))
+ 		__set_bit(vector, synic->vec_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->vec_bitmap);
+ 
+ 	if (synic_has_vector_auto_eoi(synic, vector))
+ 		__set_bit(vector, synic->auto_eoi_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->auto_eoi_bitmap);
+ }
+ 
+ static int synic_set_sint(struct kvm_vcpu_hv_synic *synic, int sint,
+ 			  u64 data, bool host)
+ {
+ 	int vector, old_vector;
+ 	bool masked;
+ 
+ 	vector = data & HV_SYNIC_SINT_VECTOR_MASK;
+ 	masked = data & HV_SYNIC_SINT_MASKED;
+ 
+ 	/*
+ 	 * Valid vectors are 16-255, however, nested Hyper-V attempts to write
+ 	 * default '0x10000' value on boot and this should not #GP. We need to
+ 	 * allow zero-initing the register from host as well.
+ 	 */
+ 	if (vector < HV_SYNIC_FIRST_VALID_VECTOR && !host && !masked)
+ 		return 1;
+ 	/*
+ 	 * Guest may configure multiple SINTs to use the same vector, so
+ 	 * we maintain a bitmap of vectors handled by synic, and a
+ 	 * bitmap of vectors with auto-eoi behavior.  The bitmaps are
+ 	 * updated here, and atomically queried on fast paths.
+ 	 */
+ 	old_vector = synic_read_sint(synic, sint) & HV_SYNIC_SINT_VECTOR_MASK;
+ 
+ 	atomic64_set(&synic->sint[sint], data);
+ 
+ 	synic_update_vector(synic, old_vector);
+ 
+ 	synic_update_vector(synic, vector);
+ 
+ 	/* Load SynIC vectors into EOI exit bitmap */
+ 	kvm_make_request(KVM_REQ_SCAN_IOAPIC, synic_to_vcpu(synic));
+ 	return 0;
+ }
+ 
+ static struct kvm_vcpu *get_vcpu_by_vpidx(struct kvm *kvm, u32 vpidx)
+ {
+ 	struct kvm_vcpu *vcpu = NULL;
+ 	int i;
+ 
+ 	if (vpidx < KVM_MAX_VCPUS)
+ 		vcpu = kvm_get_vcpu(kvm, vpidx);
+ 	if (vcpu && vcpu_to_hv_vcpu(vcpu)->vp_index == vpidx)
+ 		return vcpu;
+ 	kvm_for_each_vcpu(i, vcpu, kvm)
+ 		if (vcpu_to_hv_vcpu(vcpu)->vp_index == vpidx)
+ 			return vcpu;
+ 	return NULL;
+ }
+ 
+ static struct kvm_vcpu_hv_synic *synic_get(struct kvm *kvm, u32 vpidx)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	vcpu = get_vcpu_by_vpidx(kvm, vpidx);
+ 	if (!vcpu)
+ 		return NULL;
+ 	synic = vcpu_to_synic(vcpu);
+ 	return (synic->active) ? synic : NULL;
+ }
+ 
+ static void synic_clear_sint_msg_pending(struct kvm_vcpu_hv_synic *synic,
+ 					u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *msg;
+ 	struct hv_message_page *msg_page;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page)) {
+ 		vcpu_err(vcpu, "Hyper-V SynIC can't get msg page, gpa 0x%llx\n",
+ 			 gpa);
+ 		return;
+ 	}
+ 	msg_page = kmap_atomic(page);
+ 
+ 	msg = &msg_page->sint_message[sint];
+ 	msg->header.message_flags.msg_pending = 0;
+ 
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ }
+ 
+ static void kvm_hv_notify_acked_sint(struct kvm_vcpu *vcpu, u32 sint)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	int gsi, idx, stimers_pending;
+ 
+ 	trace_kvm_hv_notify_acked_sint(vcpu->vcpu_id, sint);
+ 
+ 	if (synic->msg_page & HV_SYNIC_SIMP_ENABLE)
+ 		synic_clear_sint_msg_pending(synic, sint);
+ 
+ 	/* Try to deliver pending Hyper-V SynIC timers messages */
+ 	stimers_pending = 0;
+ 	for (idx = 0; idx < ARRAY_SIZE(hv_vcpu->stimer); idx++) {
+ 		stimer = &hv_vcpu->stimer[idx];
+ 		if (stimer->msg_pending &&
+ 		    (stimer->config & HV_STIMER_ENABLE) &&
+ 		    HV_STIMER_SINT(stimer->config) == sint) {
+ 			set_bit(stimer->index,
+ 				hv_vcpu->stimer_pending_bitmap);
+ 			stimers_pending++;
+ 		}
+ 	}
+ 	if (stimers_pending)
+ 		kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 
+ 	idx = srcu_read_lock(&kvm->irq_srcu);
+ 	gsi = atomic_read(&synic->sint_to_gsi[sint]);
+ 	if (gsi != -1)
+ 		kvm_notify_acked_gsi(kvm, gsi);
+ 	srcu_read_unlock(&kvm->irq_srcu, idx);
+ }
+ 
+ static void synic_exit(struct kvm_vcpu_hv_synic *synic, u32 msr)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_vcpu_hv *hv_vcpu = &vcpu->arch.hyperv;
+ 
+ 	hv_vcpu->exit.type = KVM_EXIT_HYPERV_SYNIC;
+ 	hv_vcpu->exit.u.synic.msr = msr;
+ 	hv_vcpu->exit.u.synic.control = synic->control;
+ 	hv_vcpu->exit.u.synic.evt_page = synic->evt_page;
+ 	hv_vcpu->exit.u.synic.msg_page = synic->msg_page;
+ 
+ 	kvm_make_request(KVM_REQ_HV_EXIT, vcpu);
+ }
+ 
+ static int synic_set_msr(struct kvm_vcpu_hv_synic *synic,
+ 			 u32 msr, u64 data, bool host)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	int ret;
+ 
+ 	if (!synic->active && !host)
+ 		return 1;
+ 
+ 	trace_kvm_hv_synic_set_msr(vcpu->vcpu_id, msr, data, host);
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		synic->control = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		if (!host) {
+ 			ret = 1;
+ 			break;
+ 		}
+ 		synic->version = data;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		if ((data & HV_SYNIC_SIEFP_ENABLE) && !host &&
+ 		    !synic->dont_zero_synic_pages)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->evt_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		if ((data & HV_SYNIC_SIMP_ENABLE) && !host &&
+ 		    !synic->dont_zero_synic_pages)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->msg_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_EOM: {
+ 		int i;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ 		break;
+ 	}
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		ret = synic_set_sint(synic, msr - HV_X64_MSR_SINT0, data, host);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_get_msr(struct kvm_vcpu_hv_synic *synic, u32 msr, u64 *pdata,
+ 			 bool host)
+ {
+ 	int ret;
+ 
+ 	if (!synic->active && !host)
+ 		return 1;
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		*pdata = synic->control;
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		*pdata = synic->version;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		*pdata = synic->evt_page;
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		*pdata = synic->msg_page;
+ 		break;
+ 	case HV_X64_MSR_EOM:
+ 		*pdata = 0;
+ 		break;
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		*pdata = atomic64_read(&synic->sint[msr - HV_X64_MSR_SINT0]);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_lapic_irq irq;
+ 	int ret, vector;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint))
+ 		return -EINVAL;
+ 
+ 	vector = synic_get_sint_vector(synic_read_sint(synic, sint));
+ 	if (vector < 0)
+ 		return -ENOENT;
+ 
+ 	memset(&irq, 0, sizeof(irq));
+ 	irq.shorthand = APIC_DEST_SELF;
+ 	irq.dest_mode = APIC_DEST_PHYSICAL;
+ 	irq.delivery_mode = APIC_DM_FIXED;
+ 	irq.vector = vector;
+ 	irq.level = 1;
+ 
+ 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
+ 	return ret;
+ }
+ 
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vpidx, u32 sint)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vpidx);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	return synic_set_irq(synic, sint);
+ }
+ 
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector)
+ {
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	int i;
+ 
+ 	trace_kvm_hv_synic_send_eoi(vcpu->vcpu_id, vector);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ }
+ 
+ static int kvm_hv_set_sint_gsi(struct kvm *kvm, u32 vpidx, u32 sint, int gsi)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vpidx);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint_to_gsi))
+ 		return -EINVAL;
+ 
+ 	atomic_set(&synic->sint_to_gsi[sint], gsi);
+ 	return 0;
+ }
+ 
+ void kvm_hv_irq_routing_update(struct kvm *kvm)
+ {
+ 	struct kvm_irq_routing_table *irq_rt;
+ 	struct kvm_kernel_irq_routing_entry *e;
+ 	u32 gsi;
+ 
+ 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+ 					lockdep_is_held(&kvm->irq_lock));
+ 
+ 	for (gsi = 0; gsi < irq_rt->nr_rt_entries; gsi++) {
+ 		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+ 			if (e->type == KVM_IRQ_ROUTING_HV_SINT)
+ 				kvm_hv_set_sint_gsi(kvm, e->hv_sint.vcpu,
+ 						    e->hv_sint.sint, gsi);
+ 		}
+ 	}
+ }
+ 
+ static void synic_init(struct kvm_vcpu_hv_synic *synic)
+ {
+ 	int i;
+ 
+ 	memset(synic, 0, sizeof(*synic));
+ 	synic->version = HV_SYNIC_VERSION_1;
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		atomic64_set(&synic->sint[i], HV_SYNIC_SINT_MASKED);
+ 		atomic_set(&synic->sint_to_gsi[i], -1);
+ 	}
+ }
+ 
++>>>>>>> 44883f01fe6a (KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd)
  static u64 get_time_ref_counter(struct kvm *kvm)
  {
  	struct kvm_hv *hv = &kvm->arch.hyperv;
@@@ -306,6 -1006,20 +696,23 @@@ static int kvm_hv_set_msr_pw(struct kvm
  			kvm_make_request(KVM_REQ_HV_RESET, vcpu);
  		}
  		break;
++<<<<<<< HEAD
++=======
+ 	case HV_X64_MSR_REENLIGHTENMENT_CONTROL:
+ 		hv->hv_reenlightenment_control = data;
+ 		break;
+ 	case HV_X64_MSR_TSC_EMULATION_CONTROL:
+ 		hv->hv_tsc_emulation_control = data;
+ 		break;
+ 	case HV_X64_MSR_TSC_EMULATION_STATUS:
+ 		hv->hv_tsc_emulation_status = data;
+ 		break;
+ 	case HV_X64_MSR_TIME_REF_COUNT:
+ 		/* read-only, but still ignore it if host-initiated */
+ 		if (!host)
+ 			return 1;
+ 		break;
++>>>>>>> 44883f01fe6a (KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd)
  	default:
  		vcpu_unimpl(vcpu, "Hyper-V uhandled wrmsr: 0x%x data 0x%llx\n",
  			    msr, data);
@@@ -362,6 -1082,37 +769,40 @@@ static int kvm_hv_set_msr(struct kvm_vc
  			return 1;
  		hv->runtime_offset = data - current_task_runtime_100ns();
  		break;
++<<<<<<< HEAD
++=======
+ 	case HV_X64_MSR_SCONTROL:
+ 	case HV_X64_MSR_SVERSION:
+ 	case HV_X64_MSR_SIEFP:
+ 	case HV_X64_MSR_SIMP:
+ 	case HV_X64_MSR_EOM:
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		return synic_set_msr(vcpu_to_synic(vcpu), msr, data, host);
+ 	case HV_X64_MSR_STIMER0_CONFIG:
+ 	case HV_X64_MSR_STIMER1_CONFIG:
+ 	case HV_X64_MSR_STIMER2_CONFIG:
+ 	case HV_X64_MSR_STIMER3_CONFIG: {
+ 		int timer_index = (msr - HV_X64_MSR_STIMER0_CONFIG)/2;
+ 
+ 		return stimer_set_config(vcpu_to_stimer(vcpu, timer_index),
+ 					 data, host);
+ 	}
+ 	case HV_X64_MSR_STIMER0_COUNT:
+ 	case HV_X64_MSR_STIMER1_COUNT:
+ 	case HV_X64_MSR_STIMER2_COUNT:
+ 	case HV_X64_MSR_STIMER3_COUNT: {
+ 		int timer_index = (msr - HV_X64_MSR_STIMER0_COUNT)/2;
+ 
+ 		return stimer_set_count(vcpu_to_stimer(vcpu, timer_index),
+ 					data, host);
+ 	}
+ 	case HV_X64_MSR_TSC_FREQUENCY:
+ 	case HV_X64_MSR_APIC_FREQUENCY:
+ 		/* read-only, but still ignore it if host-initiated */
+ 		if (!host)
+ 			return 1;
+ 		break;
++>>>>>>> 44883f01fe6a (KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd)
  	default:
  		vcpu_unimpl(vcpu, "Hyper-V uhandled wrmsr: 0x%x data 0x%llx\n",
  			    msr, data);
@@@ -438,6 -1190,31 +880,34 @@@ static int kvm_hv_get_msr(struct kvm_vc
  	case HV_X64_MSR_VP_RUNTIME:
  		data = current_task_runtime_100ns() + hv->runtime_offset;
  		break;
++<<<<<<< HEAD
++=======
+ 	case HV_X64_MSR_SCONTROL:
+ 	case HV_X64_MSR_SVERSION:
+ 	case HV_X64_MSR_SIEFP:
+ 	case HV_X64_MSR_SIMP:
+ 	case HV_X64_MSR_EOM:
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		return synic_get_msr(vcpu_to_synic(vcpu), msr, pdata, host);
+ 	case HV_X64_MSR_STIMER0_CONFIG:
+ 	case HV_X64_MSR_STIMER1_CONFIG:
+ 	case HV_X64_MSR_STIMER2_CONFIG:
+ 	case HV_X64_MSR_STIMER3_CONFIG: {
+ 		int timer_index = (msr - HV_X64_MSR_STIMER0_CONFIG)/2;
+ 
+ 		return stimer_get_config(vcpu_to_stimer(vcpu, timer_index),
+ 					 pdata);
+ 	}
+ 	case HV_X64_MSR_STIMER0_COUNT:
+ 	case HV_X64_MSR_STIMER1_COUNT:
+ 	case HV_X64_MSR_STIMER2_COUNT:
+ 	case HV_X64_MSR_STIMER3_COUNT: {
+ 		int timer_index = (msr - HV_X64_MSR_STIMER0_COUNT)/2;
+ 
+ 		return stimer_get_count(vcpu_to_stimer(vcpu, timer_index),
+ 					pdata);
+ 	}
++>>>>>>> 44883f01fe6a (KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd)
  	case HV_X64_MSR_TSC_FREQUENCY:
  		data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
  		break;
@@@ -475,9 -1252,124 +945,9 @@@ int kvm_hv_get_msr_common(struct kvm_vc
  		mutex_unlock(&vcpu->kvm->arch.hyperv.hv_lock);
  		return r;
  	} else
- 		return kvm_hv_get_msr(vcpu, msr, pdata);
+ 		return kvm_hv_get_msr(vcpu, msr, pdata, host);
  }
  
 -static __always_inline int get_sparse_bank_no(u64 valid_bank_mask, int bank_no)
 -{
 -	int i = 0, j;
 -
 -	if (!(valid_bank_mask & BIT_ULL(bank_no)))
 -		return -1;
 -
 -	for (j = 0; j < bank_no; j++)
 -		if (valid_bank_mask & BIT_ULL(j))
 -			i++;
 -
 -	return i;
 -}
 -
 -static u64 kvm_hv_flush_tlb(struct kvm_vcpu *current_vcpu, u64 ingpa,
 -			    u16 rep_cnt, bool ex)
 -{
 -	struct kvm *kvm = current_vcpu->kvm;
 -	struct kvm_vcpu_hv *hv_current = &current_vcpu->arch.hyperv;
 -	struct hv_tlb_flush_ex flush_ex;
 -	struct hv_tlb_flush flush;
 -	struct kvm_vcpu *vcpu;
 -	unsigned long vcpu_bitmap[BITS_TO_LONGS(KVM_MAX_VCPUS)] = {0};
 -	unsigned long valid_bank_mask = 0;
 -	u64 sparse_banks[64];
 -	int sparse_banks_len, i;
 -	bool all_cpus;
 -
 -	if (!ex) {
 -		if (unlikely(kvm_read_guest(kvm, ingpa, &flush, sizeof(flush))))
 -			return HV_STATUS_INVALID_HYPERCALL_INPUT;
 -
 -		trace_kvm_hv_flush_tlb(flush.processor_mask,
 -				       flush.address_space, flush.flags);
 -
 -		sparse_banks[0] = flush.processor_mask;
 -		all_cpus = flush.flags & HV_FLUSH_ALL_PROCESSORS;
 -	} else {
 -		if (unlikely(kvm_read_guest(kvm, ingpa, &flush_ex,
 -					    sizeof(flush_ex))))
 -			return HV_STATUS_INVALID_HYPERCALL_INPUT;
 -
 -		trace_kvm_hv_flush_tlb_ex(flush_ex.hv_vp_set.valid_bank_mask,
 -					  flush_ex.hv_vp_set.format,
 -					  flush_ex.address_space,
 -					  flush_ex.flags);
 -
 -		valid_bank_mask = flush_ex.hv_vp_set.valid_bank_mask;
 -		all_cpus = flush_ex.hv_vp_set.format !=
 -			HV_GENERIC_SET_SPARSE_4K;
 -
 -		sparse_banks_len = bitmap_weight(&valid_bank_mask, 64) *
 -			sizeof(sparse_banks[0]);
 -
 -		if (!sparse_banks_len && !all_cpus)
 -			goto ret_success;
 -
 -		if (!all_cpus &&
 -		    kvm_read_guest(kvm,
 -				   ingpa + offsetof(struct hv_tlb_flush_ex,
 -						    hv_vp_set.bank_contents),
 -				   sparse_banks,
 -				   sparse_banks_len))
 -			return HV_STATUS_INVALID_HYPERCALL_INPUT;
 -	}
 -
 -	cpumask_clear(&hv_current->tlb_lush);
 -
 -	kvm_for_each_vcpu(i, vcpu, kvm) {
 -		struct kvm_vcpu_hv *hv = &vcpu->arch.hyperv;
 -		int bank = hv->vp_index / 64, sbank = 0;
 -
 -		if (!all_cpus) {
 -			/* Banks >64 can't be represented */
 -			if (bank >= 64)
 -				continue;
 -
 -			/* Non-ex hypercalls can only address first 64 vCPUs */
 -			if (!ex && bank)
 -				continue;
 -
 -			if (ex) {
 -				/*
 -				 * Check is the bank of this vCPU is in sparse
 -				 * set and get the sparse bank number.
 -				 */
 -				sbank = get_sparse_bank_no(valid_bank_mask,
 -							   bank);
 -
 -				if (sbank < 0)
 -					continue;
 -			}
 -
 -			if (!(sparse_banks[sbank] & BIT_ULL(hv->vp_index % 64)))
 -				continue;
 -		}
 -
 -		/*
 -		 * vcpu->arch.cr3 may not be up-to-date for running vCPUs so we
 -		 * can't analyze it here, flush TLB regardless of the specified
 -		 * address space.
 -		 */
 -		__set_bit(i, vcpu_bitmap);
 -	}
 -
 -	kvm_make_vcpus_request_mask(kvm,
 -				    KVM_REQ_TLB_FLUSH | KVM_REQUEST_NO_WAKEUP,
 -				    vcpu_bitmap, &hv_current->tlb_lush);
 -
 -ret_success:
 -	/* We always do full TLB flush, set rep_done = rep_cnt. */
 -	return (u64)HV_STATUS_SUCCESS |
 -		((u64)rep_cnt << HV_HYPERCALL_REP_COMP_OFFSET);
 -}
 -
  bool kvm_hv_hypercall_enabled(struct kvm *kvm)
  {
  	return READ_ONCE(kvm->arch.hyperv.hv_hypercall) & HV_X64_MSR_HYPERCALL_ENABLE;
diff --cc arch/x86/kvm/hyperv.h
index 638d4fbc7358,d6aa969e20f1..000000000000
--- a/arch/x86/kvm/hyperv.h
+++ b/arch/x86/kvm/hyperv.h
@@@ -24,8 -24,32 +24,13 @@@
  #ifndef __ARCH_X86_KVM_HYPERV_H__
  #define __ARCH_X86_KVM_HYPERV_H__
  
 -static inline struct kvm_vcpu_hv *vcpu_to_hv_vcpu(struct kvm_vcpu *vcpu)
 -{
 -	return &vcpu->arch.hyperv;
 -}
 -
 -static inline struct kvm_vcpu *hv_vcpu_to_vcpu(struct kvm_vcpu_hv *hv_vcpu)
 -{
 -	struct kvm_vcpu_arch *arch;
 -
 -	arch = container_of(hv_vcpu, struct kvm_vcpu_arch, hyperv);
 -	return container_of(arch, struct kvm_vcpu, arch);
 -}
 -
 -static inline struct kvm_vcpu_hv_synic *vcpu_to_synic(struct kvm_vcpu *vcpu)
 -{
 -	return &vcpu->arch.hyperv.synic;
 -}
 -
 -static inline struct kvm_vcpu *synic_to_vcpu(struct kvm_vcpu_hv_synic *synic)
 -{
 -	return hv_vcpu_to_vcpu(container_of(synic, struct kvm_vcpu_hv, synic));
 -}
 -
  int kvm_hv_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data, bool host);
++<<<<<<< HEAD
 +int kvm_hv_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
++=======
+ int kvm_hv_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host);
+ 
++>>>>>>> 44883f01fe6a (KVM: x86: ensure all MSRs can always be KVM_GET/SET_MSR'd)
  bool kvm_hv_hypercall_enabled(struct kvm *kvm);
  int kvm_hv_hypercall(struct kvm_vcpu *vcpu);
  
* Unmerged path arch/x86/kvm/hyperv.c
* Unmerged path arch/x86/kvm/hyperv.h
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 4fe46876c6e7..f640dd3496d8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1968,10 +1968,11 @@ static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		vcpu->arch.mcg_status = data;
 		break;
 	case MSR_IA32_MCG_CTL:
-		if (!(mcg_cap & MCG_CTL_P))
+		if (!(mcg_cap & MCG_CTL_P) &&
+		    (data || !msr_info->host_initiated))
 			return 1;
 		if (data != 0 && data != ~(u64)0)
-			return -1;
+			return 1;
 		vcpu->arch.mcg_ctl = data;
 		break;
 	default:
@@ -2315,7 +2316,7 @@ int kvm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 }
 EXPORT_SYMBOL_GPL(kvm_get_msr);
 
-static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
+static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)
 {
 	u64 data;
 	u64 mcg_cap = vcpu->arch.mcg_cap;
@@ -2330,7 +2331,7 @@ static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
 		data = vcpu->arch.mcg_cap;
 		break;
 	case MSR_IA32_MCG_CTL:
-		if (!(mcg_cap & MCG_CTL_P))
+		if (!(mcg_cap & MCG_CTL_P) && !host)
 			return 1;
 		data = vcpu->arch.mcg_ctl;
 		break;
@@ -2456,7 +2457,8 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_IA32_MCG_CTL:
 	case MSR_IA32_MCG_STATUS:
 	case MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:
-		return get_msr_mce(vcpu, msr_info->index, &msr_info->data);
+		return get_msr_mce(vcpu, msr_info->index, &msr_info->data,
+				   msr_info->host_initiated);
 	case MSR_K7_CLK_CTL:
 		/*
 		 * Provide expected ramp-up count for K7. All other
@@ -2473,7 +2475,8 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:
 	case HV_X64_MSR_CRASH_CTL:
 		return kvm_hv_get_msr_common(vcpu,
-					     msr_info->index, &msr_info->data);
+					     msr_info->index, &msr_info->data,
+					     msr_info->host_initiated);
 		break;
 	case MSR_IA32_BBL_CR_CTL3:
 		/* This legacy MSR exists but isn't fully documented in current
