netfilter: nf_conntrack: resolve clash for matching conntracks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Martynas Pumputis <martynas@weave.works>
commit ed07d9a021df6da53456663a76999189badc432a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ed07d9a0.failed

This patch enables the clash resolution for NAT (disabled in
"590b52e10d41") if clashing conntracks match (i.e. both tuples are equal)
and a protocol allows it.

The clash might happen for a connections-less protocol (e.g. UDP) when
two threads in parallel writes to the same socket and consequent calls
to "get_unique_tuple" return the same tuples (incl. reply tuples).

In this case it is safe to perform the resolution, as the losing CT
describes the same mangling as the winning CT, so no modifications to
the packet are needed, and the result of rules traversal for the loser's
packet stays valid.

	Signed-off-by: Martynas Pumputis <martynas@weave.works>
	Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
(cherry picked from commit ed07d9a021df6da53456663a76999189badc432a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/netfilter/nf_conntrack_core.c
diff --cc net/netfilter/nf_conntrack_core.c
index 5a73e052c209,4ced7c7102b6..000000000000
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@@ -471,25 -628,33 +471,42 @@@ nf_ct_key_equal(struct nf_conntrack_tup
  	 */
  	return nf_ct_tuple_equal(tuple, &h->tuple) &&
  	       nf_ct_zone_equal(ct, zone, NF_CT_DIRECTION(h)) &&
 -	       nf_ct_is_confirmed(ct) &&
 -	       net_eq(net, nf_ct_net(ct));
 +	       nf_ct_is_confirmed(ct);
  }
  
++<<<<<<< HEAD
 +/* must be called with rcu read lock held */
 +void nf_conntrack_get_ht(struct net *net, struct hlist_nulls_head **hash, unsigned int *hsize)
++=======
+ static inline bool
+ nf_ct_match(const struct nf_conn *ct1, const struct nf_conn *ct2)
+ {
+ 	return nf_ct_tuple_equal(&ct1->tuplehash[IP_CT_DIR_ORIGINAL].tuple,
+ 				 &ct2->tuplehash[IP_CT_DIR_ORIGINAL].tuple) &&
+ 	       nf_ct_tuple_equal(&ct1->tuplehash[IP_CT_DIR_REPLY].tuple,
+ 				 &ct2->tuplehash[IP_CT_DIR_REPLY].tuple) &&
+ 	       nf_ct_zone_equal(ct1, nf_ct_zone(ct2), IP_CT_DIR_ORIGINAL) &&
+ 	       nf_ct_zone_equal(ct1, nf_ct_zone(ct2), IP_CT_DIR_REPLY) &&
+ 	       net_eq(nf_ct_net(ct1), nf_ct_net(ct2));
+ }
+ 
+ /* caller must hold rcu readlock and none of the nf_conntrack_locks */
+ static void nf_ct_gc_expired(struct nf_conn *ct)
++>>>>>>> ed07d9a021df (netfilter: nf_conntrack: resolve clash for matching conntracks)
  {
 -	if (!atomic_inc_not_zero(&ct->ct_general.use))
 -		return;
 +	struct hlist_nulls_head *hptr;
 +	unsigned int sequence, hsz;
  
 -	if (nf_ct_should_gc(ct))
 -		nf_ct_kill(ct);
 +	do {
 +		sequence = read_seqcount_begin(&nf_conntrack_generation);
 +		hsz = net->ct.htable_size;
 +		hptr = net->ct.hash;
 +	} while (read_seqcount_retry(&nf_conntrack_generation, sequence));
  
 -	nf_ct_put(ct);
 +	*hash = hptr;
 +	*hsize = hsz;
  }
 +EXPORT_SYMBOL_GPL(nf_conntrack_get_ht);
  
  /*
   * Warning :
@@@ -640,6 -798,65 +657,68 @@@ out
  }
  EXPORT_SYMBOL_GPL(nf_conntrack_hash_check_insert);
  
++<<<<<<< HEAD
++=======
+ static inline void nf_ct_acct_update(struct nf_conn *ct,
+ 				     enum ip_conntrack_info ctinfo,
+ 				     unsigned int len)
+ {
+ 	struct nf_conn_acct *acct;
+ 
+ 	acct = nf_conn_acct_find(ct);
+ 	if (acct) {
+ 		struct nf_conn_counter *counter = acct->counter;
+ 
+ 		atomic64_inc(&counter[CTINFO2DIR(ctinfo)].packets);
+ 		atomic64_add(len, &counter[CTINFO2DIR(ctinfo)].bytes);
+ 	}
+ }
+ 
+ static void nf_ct_acct_merge(struct nf_conn *ct, enum ip_conntrack_info ctinfo,
+ 			     const struct nf_conn *loser_ct)
+ {
+ 	struct nf_conn_acct *acct;
+ 
+ 	acct = nf_conn_acct_find(loser_ct);
+ 	if (acct) {
+ 		struct nf_conn_counter *counter = acct->counter;
+ 		unsigned int bytes;
+ 
+ 		/* u32 should be fine since we must have seen one packet. */
+ 		bytes = atomic64_read(&counter[CTINFO2DIR(ctinfo)].bytes);
+ 		nf_ct_acct_update(ct, ctinfo, bytes);
+ 	}
+ }
+ 
+ /* Resolve race on insertion if this protocol allows this. */
+ static int nf_ct_resolve_clash(struct net *net, struct sk_buff *skb,
+ 			       enum ip_conntrack_info ctinfo,
+ 			       struct nf_conntrack_tuple_hash *h)
+ {
+ 	/* This is the conntrack entry already in hashes that won race. */
+ 	struct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);
+ 	const struct nf_conntrack_l4proto *l4proto;
+ 	enum ip_conntrack_info oldinfo;
+ 	struct nf_conn *loser_ct = nf_ct_get(skb, &oldinfo);
+ 
+ 	l4proto = __nf_ct_l4proto_find(nf_ct_l3num(ct), nf_ct_protonum(ct));
+ 	if (l4proto->allow_clash &&
+ 	    !nf_ct_is_dying(ct) &&
+ 	    atomic_inc_not_zero(&ct->ct_general.use)) {
+ 		if (((ct->status & IPS_NAT_DONE_MASK) == 0) ||
+ 		    nf_ct_match(ct, loser_ct)) {
+ 			nf_ct_acct_merge(ct, ctinfo, loser_ct);
+ 			nf_conntrack_put(&loser_ct->ct_general);
+ 			nf_ct_set(skb, ct, oldinfo);
+ 			return NF_ACCEPT;
+ 		}
+ 		nf_ct_put(ct);
+ 	}
+ 	NF_CT_STAT_INC(net, drop);
+ 	return NF_DROP;
+ }
+ 
++>>>>>>> ed07d9a021df (netfilter: nf_conntrack: resolve clash for matching conntracks)
  /* Confirm a connection given skb; places it in hash table */
  int
  __nf_conntrack_confirm(struct sk_buff *skb)
* Unmerged path net/netfilter/nf_conntrack_core.c
