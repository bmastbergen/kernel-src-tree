i40e: disallow changing the number of descriptors when AF_XDP is on

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Björn Töpel <bjorn.topel@intel.com>
commit 3ab52af58fa481324bb7c839a2187c54c4af912b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/3ab52af5.failed

When an AF_XDP UMEM is attached to any of the Rx rings, we disallow a
user to change the number of descriptors via e.g. "ethtool -G IFNAME".

Otherwise, the size of the stash/reuse queue can grow unbounded, which
would result in OOM or leaking userspace buffers.

	Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 3ab52af58fa481324bb7c839a2187c54c4af912b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
#	drivers/net/ethernet/intel/i40e/i40e_xsk.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
index b5afd479a9c5,09809dffe399..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
@@@ -87,4 -87,8 +87,11 @@@ static inline void i40e_arm_wb(struct i
  	}
  }
  
++<<<<<<< HEAD
++=======
+ void i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring);
+ void i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring);
+ bool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi);
+ 
++>>>>>>> 3ab52af58fa4 (i40e: disallow changing the number of descriptors when AF_XDP is on)
  #endif /* I40E_TXRX_COMMON_ */
diff --cc drivers/net/ethernet/intel/i40e/i40e_xsk.c
index bf502f2307c2,add1e457886d..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_xsk.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
@@@ -659,3 -728,240 +659,243 @@@ int i40e_clean_rx_irq_zc(struct i40e_ri
  	return failure ? budget : (int)total_rx_packets;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * i40e_xmit_zc - Performs zero-copy Tx AF_XDP
+  * @xdp_ring: XDP Tx ring
+  * @budget: NAPI budget
+  *
+  * Returns true if the work is finished.
+  **/
+ static bool i40e_xmit_zc(struct i40e_ring *xdp_ring, unsigned int budget)
+ {
+ 	struct i40e_tx_desc *tx_desc = NULL;
+ 	struct i40e_tx_buffer *tx_bi;
+ 	bool work_done = true;
+ 	dma_addr_t dma;
+ 	u32 len;
+ 
+ 	while (budget-- > 0) {
+ 		if (!unlikely(I40E_DESC_UNUSED(xdp_ring))) {
+ 			xdp_ring->tx_stats.tx_busy++;
+ 			work_done = false;
+ 			break;
+ 		}
+ 
+ 		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &dma, &len))
+ 			break;
+ 
+ 		dma_sync_single_for_device(xdp_ring->dev, dma, len,
+ 					   DMA_BIDIRECTIONAL);
+ 
+ 		tx_bi = &xdp_ring->tx_bi[xdp_ring->next_to_use];
+ 		tx_bi->bytecount = len;
+ 
+ 		tx_desc = I40E_TX_DESC(xdp_ring, xdp_ring->next_to_use);
+ 		tx_desc->buffer_addr = cpu_to_le64(dma);
+ 		tx_desc->cmd_type_offset_bsz =
+ 			build_ctob(I40E_TX_DESC_CMD_ICRC
+ 				   | I40E_TX_DESC_CMD_EOP,
+ 				   0, len, 0);
+ 
+ 		xdp_ring->next_to_use++;
+ 		if (xdp_ring->next_to_use == xdp_ring->count)
+ 			xdp_ring->next_to_use = 0;
+ 	}
+ 
+ 	if (tx_desc) {
+ 		/* Request an interrupt for the last frame and bump tail ptr. */
+ 		tx_desc->cmd_type_offset_bsz |= (I40E_TX_DESC_CMD_RS <<
+ 						 I40E_TXD_QW1_CMD_SHIFT);
+ 		i40e_xdp_ring_update_tail(xdp_ring);
+ 
+ 		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
+ 	}
+ 
+ 	return !!budget && work_done;
+ }
+ 
+ /**
+  * i40e_clean_xdp_tx_buffer - Frees and unmaps an XDP Tx entry
+  * @tx_ring: XDP Tx ring
+  * @tx_bi: Tx buffer info to clean
+  **/
+ static void i40e_clean_xdp_tx_buffer(struct i40e_ring *tx_ring,
+ 				     struct i40e_tx_buffer *tx_bi)
+ {
+ 	xdp_return_frame(tx_bi->xdpf);
+ 	dma_unmap_single(tx_ring->dev,
+ 			 dma_unmap_addr(tx_bi, dma),
+ 			 dma_unmap_len(tx_bi, len), DMA_TO_DEVICE);
+ 	dma_unmap_len_set(tx_bi, len, 0);
+ }
+ 
+ /**
+  * i40e_clean_xdp_tx_irq - Completes AF_XDP entries, and cleans XDP entries
+  * @tx_ring: XDP Tx ring
+  * @tx_bi: Tx buffer info to clean
+  *
+  * Returns true if cleanup/tranmission is done.
+  **/
+ bool i40e_clean_xdp_tx_irq(struct i40e_vsi *vsi,
+ 			   struct i40e_ring *tx_ring, int napi_budget)
+ {
+ 	unsigned int ntc, total_bytes = 0, budget = vsi->work_limit;
+ 	u32 i, completed_frames, frames_ready, xsk_frames = 0;
+ 	struct xdp_umem *umem = tx_ring->xsk_umem;
+ 	u32 head_idx = i40e_get_head(tx_ring);
+ 	bool work_done = true, xmit_done;
+ 	struct i40e_tx_buffer *tx_bi;
+ 
+ 	if (head_idx < tx_ring->next_to_clean)
+ 		head_idx += tx_ring->count;
+ 	frames_ready = head_idx - tx_ring->next_to_clean;
+ 
+ 	if (frames_ready == 0) {
+ 		goto out_xmit;
+ 	} else if (frames_ready > budget) {
+ 		completed_frames = budget;
+ 		work_done = false;
+ 	} else {
+ 		completed_frames = frames_ready;
+ 	}
+ 
+ 	ntc = tx_ring->next_to_clean;
+ 
+ 	for (i = 0; i < completed_frames; i++) {
+ 		tx_bi = &tx_ring->tx_bi[ntc];
+ 
+ 		if (tx_bi->xdpf)
+ 			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
+ 		else
+ 			xsk_frames++;
+ 
+ 		tx_bi->xdpf = NULL;
+ 		total_bytes += tx_bi->bytecount;
+ 
+ 		if (++ntc >= tx_ring->count)
+ 			ntc = 0;
+ 	}
+ 
+ 	tx_ring->next_to_clean += completed_frames;
+ 	if (unlikely(tx_ring->next_to_clean >= tx_ring->count))
+ 		tx_ring->next_to_clean -= tx_ring->count;
+ 
+ 	if (xsk_frames)
+ 		xsk_umem_complete_tx(umem, xsk_frames);
+ 
+ 	i40e_arm_wb(tx_ring, vsi, budget);
+ 	i40e_update_tx_stats(tx_ring, completed_frames, total_bytes);
+ 
+ out_xmit:
+ 	xmit_done = i40e_xmit_zc(tx_ring, budget);
+ 
+ 	return work_done && xmit_done;
+ }
+ 
+ /**
+  * i40e_xsk_async_xmit - Implements the ndo_xsk_async_xmit
+  * @dev: the netdevice
+  * @queue_id: queue id to wake up
+  *
+  * Returns <0 for errors, 0 otherwise.
+  **/
+ int i40e_xsk_async_xmit(struct net_device *dev, u32 queue_id)
+ {
+ 	struct i40e_netdev_priv *np = netdev_priv(dev);
+ 	struct i40e_vsi *vsi = np->vsi;
+ 	struct i40e_ring *ring;
+ 
+ 	if (test_bit(__I40E_VSI_DOWN, vsi->state))
+ 		return -ENETDOWN;
+ 
+ 	if (!i40e_enabled_xdp_vsi(vsi))
+ 		return -ENXIO;
+ 
+ 	if (queue_id >= vsi->num_queue_pairs)
+ 		return -ENXIO;
+ 
+ 	if (!vsi->xdp_rings[queue_id]->xsk_umem)
+ 		return -ENXIO;
+ 
+ 	ring = vsi->xdp_rings[queue_id];
+ 
+ 	/* The idea here is that if NAPI is running, mark a miss, so
+ 	 * it will run again. If not, trigger an interrupt and
+ 	 * schedule the NAPI from interrupt context. If NAPI would be
+ 	 * scheduled here, the interrupt affinity would not be
+ 	 * honored.
+ 	 */
+ 	if (!napi_if_scheduled_mark_missed(&ring->q_vector->napi))
+ 		i40e_force_wb(vsi, ring->q_vector);
+ 
+ 	return 0;
+ }
+ 
+ void i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring)
+ {
+ 	u16 i;
+ 
+ 	for (i = 0; i < rx_ring->count; i++) {
+ 		struct i40e_rx_buffer *rx_bi = &rx_ring->rx_bi[i];
+ 
+ 		if (!rx_bi->addr)
+ 			continue;
+ 
+ 		xsk_umem_fq_reuse(rx_ring->xsk_umem, rx_bi->handle);
+ 		rx_bi->addr = NULL;
+ 	}
+ }
+ 
+ /**
+  * i40e_xsk_clean_xdp_ring - Clean the XDP Tx ring on shutdown
+  * @xdp_ring: XDP Tx ring
+  **/
+ void i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring)
+ {
+ 	u16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;
+ 	struct xdp_umem *umem = tx_ring->xsk_umem;
+ 	struct i40e_tx_buffer *tx_bi;
+ 	u32 xsk_frames = 0;
+ 
+ 	while (ntc != ntu) {
+ 		tx_bi = &tx_ring->tx_bi[ntc];
+ 
+ 		if (tx_bi->xdpf)
+ 			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
+ 		else
+ 			xsk_frames++;
+ 
+ 		tx_bi->xdpf = NULL;
+ 
+ 		ntc++;
+ 		if (ntc >= tx_ring->count)
+ 			ntc = 0;
+ 	}
+ 
+ 	if (xsk_frames)
+ 		xsk_umem_complete_tx(umem, xsk_frames);
+ }
+ 
+ /**
+  * i40e_xsk_any_rx_ring_enabled - Checks if Rx rings have AF_XDP UMEM attached
+  * @vsi: vsi
+  *
+  * Returns true if any of the Rx rings has an AF_XDP UMEM attached
+  **/
+ bool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi)
+ {
+ 	int i;
+ 
+ 	if (!vsi->xsk_umems)
+ 		return false;
+ 
+ 	for (i = 0; i < vsi->num_queue_pairs; i++) {
+ 		if (vsi->xsk_umems[i])
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
++>>>>>>> 3ab52af58fa4 (i40e: disallow changing the number of descriptors when AF_XDP is on)
diff --git a/drivers/net/ethernet/intel/i40e/i40e_ethtool.c b/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
index d7d3974beca2..6ff9c1255522 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
@@ -5,6 +5,7 @@
 
 #include "i40e.h"
 #include "i40e_diag.h"
+#include "i40e_txrx_common.h"
 
 #include "i40e_ethtool_stats.h"
 
@@ -1493,6 +1494,13 @@ static int i40e_set_ringparam(struct net_device *netdev,
 	    (new_rx_count == vsi->rx_rings[0]->count))
 		return 0;
 
+	/* If there is a AF_XDP UMEM attached to any of Rx rings,
+	 * disallow changing the number of descriptors -- regardless
+	 * if the netdev is running or not.
+	 */
+	if (i40e_xsk_any_rx_ring_enabled(vsi))
+		return -EBUSY;
+
 	while (test_and_set_bit(__I40E_CONFIG_BUSY, pf->state)) {
 		timeout--;
 		if (!timeout)
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_xsk.c
