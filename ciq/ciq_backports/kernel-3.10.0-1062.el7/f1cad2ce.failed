iavf: remove references to old names

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jesse Brandeburg <jesse.brandeburg@intel.com>
commit f1cad2ce06f28c439dc674893b0d9a8a720acdb8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f1cad2ce.failed

Remove the register name references to I40E_VF* and change to
IAVF_VF. Update the descriptor names and defines to the IAVF
name.

	Signed-off-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit f1cad2ce06f28c439dc674893b0d9a8a720acdb8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40evf/i40evf_main.c
#	drivers/net/ethernet/intel/iavf/i40e_register.h
#	drivers/net/ethernet/intel/iavf/i40e_type.h
#	drivers/net/ethernet/intel/iavf/iavf.h
diff --cc drivers/net/ethernet/intel/i40evf/i40evf_main.c
index 950c8aeb0fc9,9d6bffca8ccc..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40evf_main.c
+++ b/drivers/net/ethernet/intel/i40evf/i40evf_main.c
@@@ -267,11 -265,11 +264,11 @@@ void i40evf_irq_enable(struct i40evf_ad
  {
  	struct i40e_hw *hw = &adapter->hw;
  
 -	iavf_misc_irq_enable(adapter);
 -	iavf_irq_enable_queues(adapter, ~0);
 +	i40evf_misc_irq_enable(adapter);
 +	i40evf_irq_enable_queues(adapter, ~0);
  
  	if (flush)
- 		rd32(hw, I40E_VFGEN_RSTAT);
+ 		iavf_flush(hw);
  }
  
  /**
@@@ -636,10 -635,10 +633,10 @@@ static void i40evf_configure_rx(struct 
  #endif
  
  	for (i = 0; i < adapter->num_active_queues; i++) {
- 		adapter->rx_rings[i].tail = hw->hw_addr + I40E_QRX_TAIL1(i);
+ 		adapter->rx_rings[i].tail = hw->hw_addr + IAVF_QRX_TAIL1(i);
  		adapter->rx_rings[i].rx_buf_len = rx_buf_len;
  
 -		if (adapter->flags & IAVF_FLAG_LEGACY_RX)
 +		if (adapter->flags & I40EVF_FLAG_LEGACY_RX)
  			clear_ring_build_skb_enabled(&adapter->rx_rings[i]);
  		else
  			set_ring_build_skb_enabled(&adapter->rx_rings[i]);
@@@ -1361,14 -1360,13 +1358,18 @@@ static int i40evf_init_rss(struct i40ev
  		else
  			adapter->hena = I40E_DEFAULT_RSS_HENA;
  
- 		wr32(hw, I40E_VFQF_HENA(0), (u32)adapter->hena);
- 		wr32(hw, I40E_VFQF_HENA(1), (u32)(adapter->hena >> 32));
+ 		wr32(hw, IAVF_VFQF_HENA(0), (u32)adapter->hena);
+ 		wr32(hw, IAVF_VFQF_HENA(1), (u32)(adapter->hena >> 32));
  	}
  
++<<<<<<< HEAD:drivers/net/ethernet/intel/i40evf/i40evf_main.c
 +	i40evf_fill_rss_lut(adapter);
 +
++=======
+ 	iavf_fill_rss_lut(adapter);
++>>>>>>> f1cad2ce06f2 (iavf: remove references to old names):drivers/net/ethernet/intel/iavf/iavf_main.c
  	netdev_rss_key_fill((void *)adapter->rss_key, adapter->rss_key_size);
 -	ret = iavf_config_rss(adapter);
 +	ret = i40evf_config_rss(adapter);
  
  	return ret;
  }
@@@ -1580,12 -1580,12 +1581,18 @@@ static void i40evf_watchdog_task(struc
  	struct i40e_hw *hw = &adapter->hw;
  	u32 reg_val;
  
 -	if (test_and_set_bit(__IAVF_IN_CRITICAL_TASK, &adapter->crit_section))
 +	if (test_and_set_bit(__I40EVF_IN_CRITICAL_TASK, &adapter->crit_section))
  		goto restart_watchdog;
  
++<<<<<<< HEAD:drivers/net/ethernet/intel/i40evf/i40evf_main.c
 +	if (adapter->flags & I40EVF_FLAG_PF_COMMS_FAILED) {
 +		reg_val = rd32(hw, I40E_VFGEN_RSTAT) &
 +			  I40E_VFGEN_RSTAT_VFR_STATE_MASK;
++=======
+ 	if (adapter->flags & IAVF_FLAG_PF_COMMS_FAILED) {
+ 		reg_val = rd32(hw, IAVF_VFGEN_RSTAT) &
+ 			  IAVF_VFGEN_RSTAT_VFR_STATE_MASK;
++>>>>>>> f1cad2ce06f2 (iavf: remove references to old names):drivers/net/ethernet/intel/iavf/iavf_main.c
  		if ((reg_val == VIRTCHNL_VFR_VFACTIVE) ||
  		    (reg_val == VIRTCHNL_VFR_COMPLETED)) {
  			/* A chance for redemption! */
@@@ -1612,10 -1612,10 +1619,17 @@@
  		goto watchdog_done;
  
  	/* check for reset */
++<<<<<<< HEAD:drivers/net/ethernet/intel/i40evf/i40evf_main.c
 +	reg_val = rd32(hw, I40E_VF_ARQLEN1) & I40E_VF_ARQLEN1_ARQENABLE_MASK;
 +	if (!(adapter->flags & I40EVF_FLAG_RESET_PENDING) && !reg_val) {
 +		adapter->state = __I40EVF_RESETTING;
 +		adapter->flags |= I40EVF_FLAG_RESET_PENDING;
++=======
+ 	reg_val = rd32(hw, IAVF_VF_ARQLEN1) & IAVF_VF_ARQLEN1_ARQENABLE_MASK;
+ 	if (!(adapter->flags & IAVF_FLAG_RESET_PENDING) && !reg_val) {
+ 		adapter->state = __IAVF_RESETTING;
+ 		adapter->flags |= IAVF_FLAG_RESET_PENDING;
++>>>>>>> f1cad2ce06f2 (iavf: remove references to old names):drivers/net/ethernet/intel/iavf/iavf_main.c
  		dev_err(&adapter->pdev->dev, "Hardware reset detected\n");
  		schedule_work(&adapter->reset_task);
  		adapter->aq_required = 0;
@@@ -1879,16 -1879,16 +1893,22 @@@ static void i40evf_reset_task(struct wo
  		/* Restart the AQ here. If we have been reset but didn't
  		 * detect it, or if the PF had to reinit, our AQ will be hosed.
  		 */
 -		iavf_shutdown_adminq(hw);
 -		iavf_init_adminq(hw);
 -		iavf_request_reset(adapter);
 +		i40evf_shutdown_adminq(hw);
 +		i40evf_init_adminq(hw);
 +		i40evf_request_reset(adapter);
  	}
 -	adapter->flags |= IAVF_FLAG_RESET_PENDING;
 +	adapter->flags |= I40EVF_FLAG_RESET_PENDING;
  
  	/* poll until we see the reset actually happen */
++<<<<<<< HEAD:drivers/net/ethernet/intel/i40evf/i40evf_main.c
 +	for (i = 0; i < I40EVF_RESET_WAIT_COUNT; i++) {
 +		reg_val = rd32(hw, I40E_VF_ARQLEN1) &
 +			  I40E_VF_ARQLEN1_ARQENABLE_MASK;
++=======
+ 	for (i = 0; i < IAVF_RESET_WAIT_COUNT; i++) {
+ 		reg_val = rd32(hw, IAVF_VF_ARQLEN1) &
+ 			  IAVF_VF_ARQLEN1_ARQENABLE_MASK;
++>>>>>>> f1cad2ce06f2 (iavf: remove references to old names):drivers/net/ethernet/intel/iavf/iavf_main.c
  		if (!reg_val)
  			break;
  		usleep_range(5000, 10000);
@@@ -1899,12 -1899,12 +1919,12 @@@
  	}
  
  	/* wait until the reset is complete and the PF is responding to us */
 -	for (i = 0; i < IAVF_RESET_WAIT_COUNT; i++) {
 +	for (i = 0; i < I40EVF_RESET_WAIT_COUNT; i++) {
  		/* sleep first to make sure a minimum wait time is met */
 -		msleep(IAVF_RESET_WAIT_MS);
 +		msleep(I40EVF_RESET_WAIT_MS);
  
- 		reg_val = rd32(hw, I40E_VFGEN_RSTAT) &
- 			  I40E_VFGEN_RSTAT_VFR_STATE_MASK;
+ 		reg_val = rd32(hw, IAVF_VFGEN_RSTAT) &
+ 			  IAVF_VFGEN_RSTAT_VFR_STATE_MASK;
  		if (reg_val == VIRTCHNL_VFR_VFACTIVE)
  			break;
  	}
* Unmerged path drivers/net/ethernet/intel/iavf/i40e_register.h
* Unmerged path drivers/net/ethernet/intel/iavf/i40e_type.h
* Unmerged path drivers/net/ethernet/intel/iavf/iavf.h
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_adminq.c b/drivers/net/ethernet/intel/i40evf/i40e_adminq.c
index 21a0dbf6ccf6..9a143ae13d8f 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_adminq.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_adminq.c
@@ -27,16 +27,16 @@ static void i40e_adminq_init_regs(struct i40e_hw *hw)
 {
 	/* set head and tail registers in our local struct */
 	if (i40e_is_vf(hw)) {
-		hw->aq.asq.tail = I40E_VF_ATQT1;
-		hw->aq.asq.head = I40E_VF_ATQH1;
-		hw->aq.asq.len  = I40E_VF_ATQLEN1;
-		hw->aq.asq.bal  = I40E_VF_ATQBAL1;
-		hw->aq.asq.bah  = I40E_VF_ATQBAH1;
-		hw->aq.arq.tail = I40E_VF_ARQT1;
-		hw->aq.arq.head = I40E_VF_ARQH1;
-		hw->aq.arq.len  = I40E_VF_ARQLEN1;
-		hw->aq.arq.bal  = I40E_VF_ARQBAL1;
-		hw->aq.arq.bah  = I40E_VF_ARQBAH1;
+		hw->aq.asq.tail = IAVF_VF_ATQT1;
+		hw->aq.asq.head = IAVF_VF_ATQH1;
+		hw->aq.asq.len  = IAVF_VF_ATQLEN1;
+		hw->aq.asq.bal  = IAVF_VF_ATQBAL1;
+		hw->aq.asq.bah  = IAVF_VF_ATQBAH1;
+		hw->aq.arq.tail = IAVF_VF_ARQT1;
+		hw->aq.arq.head = IAVF_VF_ARQH1;
+		hw->aq.arq.len  = IAVF_VF_ARQLEN1;
+		hw->aq.arq.bal  = IAVF_VF_ARQBAL1;
+		hw->aq.arq.bah  = IAVF_VF_ARQBAH1;
 	}
 }
 
@@ -274,7 +274,7 @@ static i40e_status i40e_config_asq_regs(struct i40e_hw *hw)
 
 	/* set starting point */
 	wr32(hw, hw->aq.asq.len, (hw->aq.num_asq_entries |
-				  I40E_VF_ATQLEN1_ATQENABLE_MASK));
+				  IAVF_VF_ATQLEN1_ATQENABLE_MASK));
 	wr32(hw, hw->aq.asq.bal, lower_32_bits(hw->aq.asq.desc_buf.pa));
 	wr32(hw, hw->aq.asq.bah, upper_32_bits(hw->aq.asq.desc_buf.pa));
 
@@ -303,7 +303,7 @@ static i40e_status i40e_config_arq_regs(struct i40e_hw *hw)
 
 	/* set starting point */
 	wr32(hw, hw->aq.arq.len, (hw->aq.num_arq_entries |
-				  I40E_VF_ARQLEN1_ARQENABLE_MASK));
+				  IAVF_VF_ARQLEN1_ARQENABLE_MASK));
 	wr32(hw, hw->aq.arq.bal, lower_32_bits(hw->aq.arq.desc_buf.pa));
 	wr32(hw, hw->aq.arq.bah, upper_32_bits(hw->aq.arq.desc_buf.pa));
 
@@ -816,7 +816,7 @@ i40e_status i40evf_asq_send_command(struct i40e_hw *hw,
 	/* update the error if time out occurred */
 	if ((!cmd_completed) &&
 	    (!details->async && !details->postpone)) {
-		if (rd32(hw, hw->aq.asq.len) & I40E_VF_ATQLEN1_ATQCRIT_MASK) {
+		if (rd32(hw, hw->aq.asq.len) & IAVF_VF_ATQLEN1_ATQCRIT_MASK) {
 			i40e_debug(hw, I40E_DEBUG_AQ_MESSAGE,
 				   "AQTX: AQ Critical error.\n");
 			status = I40E_ERR_ADMIN_QUEUE_CRITICAL_ERROR;
@@ -885,7 +885,7 @@ i40e_status i40evf_clean_arq_element(struct i40e_hw *hw,
 	}
 
 	/* set next_to_use to head */
-	ntu = rd32(hw, hw->aq.arq.head) & I40E_VF_ARQH1_ARQH_MASK;
+	ntu = rd32(hw, hw->aq.arq.head) & IAVF_VF_ARQH1_ARQH_MASK;
 	if (ntu == ntc) {
 		/* nothing to do - shouldn't need to update ring's values */
 		ret_code = I40E_ERR_ADMIN_QUEUE_NO_WORK;
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_common.c b/drivers/net/ethernet/intel/i40evf/i40e_common.c
index eea280ba411e..19e9c249fe10 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_common.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_common.c
@@ -336,7 +336,7 @@ bool i40evf_check_asq_alive(struct i40e_hw *hw)
 {
 	if (hw->aq.asq.len)
 		return !!(rd32(hw, hw->aq.asq.len) &
-			  I40E_VF_ATQLEN1_ATQENABLE_MASK);
+			  IAVF_VF_ATQLEN1_ATQENABLE_MASK);
 	else
 		return false;
 }
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_osdep.h b/drivers/net/ethernet/intel/i40evf/i40e_osdep.h
index d394f0fe8516..516ee726bf7a 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_osdep.h
+++ b/drivers/net/ethernet/intel/i40evf/i40e_osdep.h
@@ -24,7 +24,7 @@
 
 #define wr64(a, reg, value)	writeq((value), ((a)->hw_addr + (reg)))
 #define rd64(a, reg)		readq((a)->hw_addr + (reg))
-#define i40e_flush(a)		readl((a)->hw_addr + I40E_VFGEN_RSTAT)
+#define iavf_flush(a)		readl((a)->hw_addr + IAVF_VFGEN_RSTAT)
 
 /* memory allocation tracking */
 struct i40e_dma_mem {
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index 800d621606e8..766332ae11b1 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@ -11,14 +11,14 @@
 static inline __le64 build_ctob(u32 td_cmd, u32 td_offset, unsigned int size,
 				u32 td_tag)
 {
-	return cpu_to_le64(I40E_TX_DESC_DTYPE_DATA |
+	return cpu_to_le64(IAVF_TX_DESC_DTYPE_DATA |
 			   ((u64)td_cmd  << I40E_TXD_QW1_CMD_SHIFT) |
 			   ((u64)td_offset << I40E_TXD_QW1_OFFSET_SHIFT) |
 			   ((u64)size  << I40E_TXD_QW1_TX_BUF_SZ_SHIFT) |
 			   ((u64)td_tag  << I40E_TXD_QW1_L2TAG1_SHIFT));
 }
 
-#define I40E_TXD_CMD (I40E_TX_DESC_CMD_EOP | I40E_TX_DESC_CMD_RS)
+#define I40E_TXD_CMD (IAVF_TX_DESC_CMD_EOP | IAVF_TX_DESC_CMD_RS)
 
 /**
  * i40e_unmap_and_free_tx_resource - Release a Tx buffer
@@ -198,7 +198,7 @@ static bool i40e_clean_tx_irq(struct i40e_vsi *vsi,
 	unsigned int budget = vsi->work_limit;
 
 	tx_buf = &tx_ring->tx_bi[i];
-	tx_desc = I40E_TX_DESC(tx_ring, i);
+	tx_desc = IAVF_TX_DESC(tx_ring, i);
 	i -= tx_ring->count;
 
 	do {
@@ -214,7 +214,7 @@ static bool i40e_clean_tx_irq(struct i40e_vsi *vsi,
 		i40e_trace(clean_tx_irq, tx_ring, tx_desc, tx_buf);
 		/* if the descriptor isn't done, no work yet to do */
 		if (!(eop_desc->cmd_type_offset_bsz &
-		      cpu_to_le64(I40E_TX_DESC_DTYPE_DESC_DONE)))
+		      cpu_to_le64(IAVF_TX_DESC_DTYPE_DESC_DONE)))
 			break;
 
 		/* clear next_to_watch to prevent false hangs */
@@ -248,7 +248,7 @@ static bool i40e_clean_tx_irq(struct i40e_vsi *vsi,
 			if (unlikely(!i)) {
 				i -= tx_ring->count;
 				tx_buf = tx_ring->tx_bi;
-				tx_desc = I40E_TX_DESC(tx_ring, 0);
+				tx_desc = IAVF_TX_DESC(tx_ring, 0);
 			}
 
 			/* unmap any remaining paged data */
@@ -268,7 +268,7 @@ static bool i40e_clean_tx_irq(struct i40e_vsi *vsi,
 		if (unlikely(!i)) {
 			i -= tx_ring->count;
 			tx_buf = tx_ring->tx_bi;
-			tx_desc = I40E_TX_DESC(tx_ring, 0);
+			tx_desc = IAVF_TX_DESC(tx_ring, 0);
 		}
 
 		prefetch(tx_desc);
@@ -342,11 +342,11 @@ static void i40e_enable_wb_on_itr(struct i40e_vsi *vsi,
 	if (q_vector->arm_wb_state)
 		return;
 
-	val = I40E_VFINT_DYN_CTLN1_WB_ON_ITR_MASK |
-	      I40E_VFINT_DYN_CTLN1_ITR_INDX_MASK; /* set noitr */
+	val = IAVF_VFINT_DYN_CTLN1_WB_ON_ITR_MASK |
+	      IAVF_VFINT_DYN_CTLN1_ITR_INDX_MASK; /* set noitr */
 
 	wr32(&vsi->back->hw,
-	     I40E_VFINT_DYN_CTLN1(q_vector->reg_idx), val);
+	     IAVF_VFINT_DYN_CTLN1(q_vector->reg_idx), val);
 	q_vector->arm_wb_state = true;
 }
 
@@ -358,14 +358,14 @@ static void i40e_enable_wb_on_itr(struct i40e_vsi *vsi,
  **/
 void i40evf_force_wb(struct i40e_vsi *vsi, struct i40e_q_vector *q_vector)
 {
-	u32 val = I40E_VFINT_DYN_CTLN1_INTENA_MASK |
-		  I40E_VFINT_DYN_CTLN1_ITR_INDX_MASK | /* set noitr */
-		  I40E_VFINT_DYN_CTLN1_SWINT_TRIG_MASK |
-		  I40E_VFINT_DYN_CTLN1_SW_ITR_INDX_ENA_MASK
+	u32 val = IAVF_VFINT_DYN_CTLN1_INTENA_MASK |
+		  IAVF_VFINT_DYN_CTLN1_ITR_INDX_MASK | /* set noitr */
+		  IAVF_VFINT_DYN_CTLN1_SWINT_TRIG_MASK |
+		  IAVF_VFINT_DYN_CTLN1_SW_ITR_INDX_ENA_MASK
 		  /* allow 00 to be written to the index */;
 
 	wr32(&vsi->back->hw,
-	     I40E_VFINT_DYN_CTLN1(q_vector->reg_idx),
+	     IAVF_VFINT_DYN_CTLN1(q_vector->reg_idx),
 	     val);
 }
 
@@ -893,7 +893,7 @@ bool i40evf_alloc_rx_buffers(struct i40e_ring *rx_ring, u16 cleaned_count)
 	if (!rx_ring->netdev || !cleaned_count)
 		return false;
 
-	rx_desc = I40E_RX_DESC(rx_ring, ntu);
+	rx_desc = IAVF_RX_DESC(rx_ring, ntu);
 	bi = &rx_ring->rx_bi[ntu];
 
 	do {
@@ -915,7 +915,7 @@ bool i40evf_alloc_rx_buffers(struct i40e_ring *rx_ring, u16 cleaned_count)
 		bi++;
 		ntu++;
 		if (unlikely(ntu == rx_ring->count)) {
-			rx_desc = I40E_RX_DESC(rx_ring, 0);
+			rx_desc = IAVF_RX_DESC(rx_ring, 0);
 			bi = rx_ring->rx_bi;
 			ntu = 0;
 		}
@@ -974,7 +974,7 @@ static inline void i40e_rx_checksum(struct i40e_vsi *vsi,
 		return;
 
 	/* did the hardware decode the packet and checksum? */
-	if (!(rx_status & BIT(I40E_RX_DESC_STATUS_L3L4P_SHIFT)))
+	if (!(rx_status & BIT(IAVF_RX_DESC_STATUS_L3L4P_SHIFT)))
 		return;
 
 	/* both known and outer_ip must be set for the below code to work */
@@ -987,25 +987,25 @@ static inline void i40e_rx_checksum(struct i40e_vsi *vsi,
 	       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV6);
 
 	if (ipv4 &&
-	    (rx_error & (BIT(I40E_RX_DESC_ERROR_IPE_SHIFT) |
-			 BIT(I40E_RX_DESC_ERROR_EIPE_SHIFT))))
+	    (rx_error & (BIT(IAVF_RX_DESC_ERROR_IPE_SHIFT) |
+			 BIT(IAVF_RX_DESC_ERROR_EIPE_SHIFT))))
 		goto checksum_fail;
 
 	/* likely incorrect csum if alternate IP extension headers found */
 	if (ipv6 &&
-	    rx_status & BIT(I40E_RX_DESC_STATUS_IPV6EXADD_SHIFT))
+	    rx_status & BIT(IAVF_RX_DESC_STATUS_IPV6EXADD_SHIFT))
 		/* don't increment checksum err here, non-fatal err */
 		return;
 
 	/* there was some L4 error, count error and punt packet to the stack */
-	if (rx_error & BIT(I40E_RX_DESC_ERROR_L4E_SHIFT))
+	if (rx_error & BIT(IAVF_RX_DESC_ERROR_L4E_SHIFT))
 		goto checksum_fail;
 
 	/* handle packets that were not able to be checksummed due
 	 * to arrival speed, in this case the stack can compute
 	 * the csum.
 	 */
-	if (rx_error & BIT(I40E_RX_DESC_ERROR_PPRS_SHIFT))
+	if (rx_error & BIT(IAVF_RX_DESC_ERROR_PPRS_SHIFT))
 		return;
 
 	/* Only report checksum unnecessary for TCP, UDP, or SCTP */
@@ -1062,8 +1062,8 @@ static inline void i40e_rx_hash(struct i40e_ring *ring,
 {
 	u32 hash;
 	const __le64 rss_mask =
-		cpu_to_le64((u64)I40E_RX_DESC_FLTSTAT_RSS_HASH <<
-			    I40E_RX_DESC_STATUS_FLTSTAT_SHIFT);
+		cpu_to_le64((u64)IAVF_RX_DESC_FLTSTAT_RSS_HASH <<
+			    IAVF_RX_DESC_STATUS_FLTSTAT_SHIFT);
 
 	if (ring->netdev->features & NETIF_F_RXHASH)
 		return;
@@ -1447,10 +1447,10 @@ static bool i40e_is_non_eop(struct i40e_ring *rx_ring,
 	ntc = (ntc < rx_ring->count) ? ntc : 0;
 	rx_ring->next_to_clean = ntc;
 
-	prefetch(I40E_RX_DESC(rx_ring, ntc));
+	prefetch(IAVF_RX_DESC(rx_ring, ntc));
 
 	/* if we are the last buffer then there is nothing else to do */
-#define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)
+#define I40E_RXD_EOF BIT(IAVF_RX_DESC_STATUS_EOF_SHIFT)
 	if (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))
 		return false;
 
@@ -1493,7 +1493,7 @@ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
 			cleaned_count = 0;
 		}
 
-		rx_desc = I40E_RX_DESC(rx_ring, rx_ring->next_to_clean);
+		rx_desc = IAVF_RX_DESC(rx_ring, rx_ring->next_to_clean);
 
 		/* status_error_len will always be zero for unused descriptors
 		 * because it's cleared in cleanup, and overlaps with hdr_addr
@@ -1539,7 +1539,7 @@ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
 
 		/* ERR_MASK will only have valid bits if EOP set, and
 		 * what we are doing here is actually checking
-		 * I40E_RX_DESC_ERROR_RXE_SHIFT, since it is the zeroth bit in
+		 * IAVF_RX_DESC_ERROR_RXE_SHIFT, since it is the zeroth bit in
 		 * the error field
 		 */
 		if (unlikely(i40e_test_staterr(rx_desc, BIT(I40E_RXD_QW1_ERROR_SHIFT)))) {
@@ -1564,7 +1564,7 @@ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
 		i40evf_process_skb_fields(rx_ring, rx_desc, skb, rx_ptype);
 
 
-		vlan_tag = (qword & BIT(I40E_RX_DESC_STATUS_L2TAG1P_SHIFT)) ?
+		vlan_tag = (qword & BIT(IAVF_RX_DESC_STATUS_L2TAG1P_SHIFT)) ?
 			   le16_to_cpu(rx_desc->wb.qword0.lo_dword.l2tag1) : 0;
 
 		i40e_trace(clean_rx_irq_rx, rx_ring, rx_desc, skb);
@@ -1609,15 +1609,15 @@ static inline u32 i40e_buildreg_itr(const int type, u16 itr)
 	 */
 	itr &= I40E_ITR_MASK;
 
-	val = I40E_VFINT_DYN_CTLN1_INTENA_MASK |
-	      (type << I40E_VFINT_DYN_CTLN1_ITR_INDX_SHIFT) |
-	      (itr << (I40E_VFINT_DYN_CTLN1_INTERVAL_SHIFT - 1));
+	val = IAVF_VFINT_DYN_CTLN1_INTENA_MASK |
+	      (type << IAVF_VFINT_DYN_CTLN1_ITR_INDX_SHIFT) |
+	      (itr << (IAVF_VFINT_DYN_CTLN1_INTERVAL_SHIFT - 1));
 
 	return val;
 }
 
 /* a small macro to shorten up some long lines */
-#define INTREG I40E_VFINT_DYN_CTLN1
+#define INTREG IAVF_VFINT_DYN_CTLN1
 
 /* The act of updating the ITR will cause it to immediately trigger. In order
  * to prevent this from throwing off adaptive update statistics we defer the
@@ -1978,7 +1978,7 @@ static int i40e_tx_enable_csum(struct sk_buff *skb, u32 *tx_flags,
 	l4.hdr = skb_transport_header(skb);
 
 	/* compute outer L2 header size */
-	offset = ((ip.hdr - skb->data) / 2) << I40E_TX_DESC_LENGTH_MACLEN_SHIFT;
+	offset = ((ip.hdr - skb->data) / 2) << IAVF_TX_DESC_LENGTH_MACLEN_SHIFT;
 
 	if (skb->encapsulation) {
 		u32 tunnel = 0;
@@ -2061,10 +2061,10 @@ static int i40e_tx_enable_csum(struct sk_buff *skb, u32 *tx_flags,
 		 * need the hardware to recompute it is in the case of TSO.
 		 */
 		cmd |= (*tx_flags & I40E_TX_FLAGS_TSO) ?
-		       I40E_TX_DESC_CMD_IIPT_IPV4_CSUM :
-		       I40E_TX_DESC_CMD_IIPT_IPV4;
+		       IAVF_TX_DESC_CMD_IIPT_IPV4_CSUM :
+		       IAVF_TX_DESC_CMD_IIPT_IPV4;
 	} else if (*tx_flags & I40E_TX_FLAGS_IPV6) {
-		cmd |= I40E_TX_DESC_CMD_IIPT_IPV6;
+		cmd |= IAVF_TX_DESC_CMD_IIPT_IPV6;
 
 		exthdr = ip.hdr + sizeof(*ip.v6);
 		l4_proto = ip.v6->nexthdr;
@@ -2074,26 +2074,26 @@ static int i40e_tx_enable_csum(struct sk_buff *skb, u32 *tx_flags,
 	}
 
 	/* compute inner L3 header size */
-	offset |= ((l4.hdr - ip.hdr) / 4) << I40E_TX_DESC_LENGTH_IPLEN_SHIFT;
+	offset |= ((l4.hdr - ip.hdr) / 4) << IAVF_TX_DESC_LENGTH_IPLEN_SHIFT;
 
 	/* Enable L4 checksum offloads */
 	switch (l4_proto) {
 	case IPPROTO_TCP:
 		/* enable checksum offloads */
-		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_TCP;
-		offset |= l4.tcp->doff << I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+		cmd |= IAVF_TX_DESC_CMD_L4T_EOFT_TCP;
+		offset |= l4.tcp->doff << IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
 		break;
 	case IPPROTO_SCTP:
 		/* enable SCTP checksum offload */
-		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_SCTP;
+		cmd |= IAVF_TX_DESC_CMD_L4T_EOFT_SCTP;
 		offset |= (sizeof(struct sctphdr) >> 2) <<
-			  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+			  IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
 		break;
 	case IPPROTO_UDP:
 		/* enable UDP checksum offload */
-		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_UDP;
+		cmd |= IAVF_TX_DESC_CMD_L4T_EOFT_UDP;
 		offset |= (sizeof(struct udphdr) >> 2) <<
-			  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+			  IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
 		break;
 	default:
 		if (*tx_flags & I40E_TX_FLAGS_TSO)
@@ -2122,12 +2122,12 @@ static void i40e_create_tx_ctx(struct i40e_ring *tx_ring,
 	struct i40e_tx_context_desc *context_desc;
 	int i = tx_ring->next_to_use;
 
-	if ((cd_type_cmd_tso_mss == I40E_TX_DESC_DTYPE_CONTEXT) &&
+	if ((cd_type_cmd_tso_mss == IAVF_TX_DESC_DTYPE_CONTEXT) &&
 	    !cd_tunneling && !cd_l2tag2)
 		return;
 
 	/* grab the next descriptor */
-	context_desc = I40E_TX_CTXTDESC(tx_ring, i);
+	context_desc = IAVF_TX_CTXTDESC(tx_ring, i);
 
 	i++;
 	tx_ring->next_to_use = (i < tx_ring->count) ? i : 0;
@@ -2270,7 +2270,7 @@ static inline void i40evf_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 	dma_addr_t dma;
 
 	if (tx_flags & I40E_TX_FLAGS_HW_VLAN) {
-		td_cmd |= I40E_TX_DESC_CMD_IL2TAG1;
+		td_cmd |= IAVF_TX_DESC_CMD_IL2TAG1;
 		td_tag = (tx_flags & I40E_TX_FLAGS_VLAN_MASK) >>
 			 I40E_TX_FLAGS_VLAN_SHIFT;
 	}
@@ -2279,7 +2279,7 @@ static inline void i40evf_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 
 	dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);
 
-	tx_desc = I40E_TX_DESC(tx_ring, i);
+	tx_desc = IAVF_TX_DESC(tx_ring, i);
 	tx_bi = first;
 
 	for (frag = &skb_shinfo(skb)->frags[0];; frag++) {
@@ -2305,7 +2305,7 @@ static inline void i40evf_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 			i++;
 
 			if (i == tx_ring->count) {
-				tx_desc = I40E_TX_DESC(tx_ring, 0);
+				tx_desc = IAVF_TX_DESC(tx_ring, 0);
 				i = 0;
 			}
 
@@ -2326,7 +2326,7 @@ static inline void i40evf_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 		i++;
 
 		if (i == tx_ring->count) {
-			tx_desc = I40E_TX_DESC(tx_ring, 0);
+			tx_desc = IAVF_TX_DESC(tx_ring, 0);
 			i = 0;
 		}
 
@@ -2404,7 +2404,7 @@ dma_error:
 static netdev_tx_t i40e_xmit_frame_ring(struct sk_buff *skb,
 					struct i40e_ring *tx_ring)
 {
-	u64 cd_type_cmd_tso_mss = I40E_TX_DESC_DTYPE_CONTEXT;
+	u64 cd_type_cmd_tso_mss = IAVF_TX_DESC_DTYPE_CONTEXT;
 	u32 cd_tunneling = 0, cd_l2tag2 = 0;
 	struct i40e_tx_buffer *first;
 	u32 td_offset = 0;
@@ -2475,7 +2475,7 @@ static netdev_tx_t i40e_xmit_frame_ring(struct sk_buff *skb,
 	skb_tx_timestamp(skb);
 
 	/* always enable CRC insertion offload */
-	td_cmd |= I40E_TX_DESC_CMD_ICRC;
+	td_cmd |= IAVF_TX_DESC_CMD_ICRC;
 
 	i40e_create_tx_ctx(tx_ring, cd_type_cmd_tso_mss,
 			   cd_tunneling, cd_l2tag2);
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
index d7a9fa0d91eb..7f707cf4097e 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
@@ -188,7 +188,7 @@ static inline bool i40e_test_staterr(union i40e_rx_desc *rx_desc,
 		(i)++;				\
 		if ((i) == (r)->count)		\
 			i = 0;			\
-		(n) = I40E_RX_DESC((r), (i));	\
+		(n) = IAVF_RX_DESC((r), (i));	\
 	} while (0)
 
 #define I40E_RX_NEXT_DESC_PREFETCH(r, i, n)		\
* Unmerged path drivers/net/ethernet/intel/i40evf/i40evf_main.c
* Unmerged path drivers/net/ethernet/intel/iavf/i40e_register.h
* Unmerged path drivers/net/ethernet/intel/iavf/i40e_type.h
* Unmerged path drivers/net/ethernet/intel/iavf/iavf.h
