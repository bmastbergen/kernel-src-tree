iommu/iova: Simplify cached node logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [iommu] iova: Simplify cached node logic (Jerry Snitselaar) [1615865]
Rebuild_FUZZ: 91.43%
commit-author Robin Murphy <robin.murphy@arm.com>
commit 973f5fbedb0721ab964386a5fe5120998e71580c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/973f5fbe.failed

The logic of __get_cached_rbnode() is a little obtuse, but then
__get_prev_node_of_cached_rbnode_or_last_node_and_update_limit_pfn()
wouldn't exactly roll off the tongue...

Now that we have the invariant that there is always a valid node to
start searching downwards from, everything gets a bit easier to follow
if we simplify that function to do what it says on the tin and return
the cached node (or anchor node as appropriate) directly. In turn, we
can then deduplicate the rb_prev() and limit_pfn logic into the main
loop itself, further reduce the amount of code under the lock, and
generally make the inner workings a bit less subtle.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 973f5fbedb0721ab964386a5fe5120998e71580c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
diff --cc drivers/iommu/iova.c
index 1a0166896ba6,7b7363518733..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -47,12 -51,16 +47,17 @@@ init_iova_domain(struct iova_domain *io
  
  	spin_lock_init(&iovad->iova_rbtree_lock);
  	iovad->rbroot = RB_ROOT;
++<<<<<<< HEAD
 +	iovad->cached32_node = NULL;
++=======
+ 	iovad->cached_node = &iovad->anchor.node;
+ 	iovad->cached32_node = &iovad->anchor.node;
++>>>>>>> 973f5fbedb07 (iommu/iova: Simplify cached node logic)
  	iovad->granule = granule;
  	iovad->start_pfn = start_pfn;
 -	iovad->dma_32bit_pfn = 1UL << (32 - iova_shift(iovad));
 +	iovad->dma_32bit_pfn = pfn_32bit + 1;
  	iovad->flush_cb = NULL;
  	iovad->fq = NULL;
 -	iovad->anchor.pfn_lo = iovad->anchor.pfn_hi = IOVA_ANCHOR;
 -	rb_link_node(&iovad->anchor.node, NULL, &iovad->rbroot.rb_node);
 -	rb_insert_color(&iovad->anchor.node, &iovad->rbroot);
  	init_iova_rcaches(iovad);
  }
  EXPORT_SYMBOL_GPL(init_iova_domain);
@@@ -107,18 -115,12 +112,25 @@@ int init_iova_flush_queue(struct iova_d
  EXPORT_SYMBOL_GPL(init_iova_flush_queue);
  
  static struct rb_node *
- __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)
+ __get_cached_rbnode(struct iova_domain *iovad, unsigned long limit_pfn)
  {
++<<<<<<< HEAD
 +	if ((*limit_pfn > iovad->dma_32bit_pfn) ||
 +		(iovad->cached32_node == NULL))
 +		return rb_last(&iovad->rbroot);
 +	else {
 +		struct rb_node *prev_node = rb_prev(iovad->cached32_node);
 +		struct iova *curr_iova =
 +			container_of(iovad->cached32_node, struct iova, node);
 +		*limit_pfn = curr_iova->pfn_lo;
 +		return prev_node;
 +	}
++=======
+ 	if (limit_pfn <= iovad->dma_32bit_pfn)
+ 		return iovad->cached32_node;
+ 
+ 	return iovad->cached_node;
++>>>>>>> 973f5fbedb07 (iommu/iova: Simplify cached node logic)
  }
  
  static void
@@@ -134,73 -136,73 +146,100 @@@ static voi
  __cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)
  {
  	struct iova *cached_iova;
 +	struct rb_node *curr;
  
++<<<<<<< HEAD
 +	if (!iovad->cached32_node)
 +		return;
 +	curr = iovad->cached32_node;
 +	cached_iova = container_of(curr, struct iova, node);
 +
 +	if (free->pfn_lo >= cached_iova->pfn_lo) {
 +		struct rb_node *node = rb_next(&free->node);
 +		struct iova *iova = container_of(node, struct iova, node);
 +
 +		/* only cache if it's below 32bit pfn */
 +		if (node && iova->pfn_lo < iovad->dma_32bit_pfn)
 +			iovad->cached32_node = node;
 +		else
 +			iovad->cached32_node = NULL;
 +	}
++=======
+ 	cached_iova = rb_entry(iovad->cached32_node, struct iova, node);
+ 	if (free->pfn_hi < iovad->dma_32bit_pfn &&
+ 	    free->pfn_lo >= cached_iova->pfn_lo)
+ 		iovad->cached32_node = rb_next(&free->node);
+ 
+ 	cached_iova = rb_entry(iovad->cached_node, struct iova, node);
+ 	if (free->pfn_lo >= cached_iova->pfn_lo)
+ 		iovad->cached_node = rb_next(&free->node);
++>>>>>>> 973f5fbedb07 (iommu/iova: Simplify cached node logic)
  }
  
 -/* Insert the iova into domain rbtree by holding writer lock */
 -static void
 -iova_insert_rbtree(struct rb_root *root, struct iova *iova,
 -		   struct rb_node *start)
 +/*
 + * Computes the padding size required, to make the start address
 + * naturally aligned on the power-of-two order of its size
 + */
 +static unsigned int
 +iova_get_pad_size(unsigned int size, unsigned int limit_pfn)
  {
 -	struct rb_node **new, *parent = NULL;
 -
 -	new = (start) ? &start : &(root->rb_node);
 -	/* Figure out where to put new node */
 -	while (*new) {
 -		struct iova *this = rb_entry(*new, struct iova, node);
 -
 -		parent = *new;
 -
 -		if (iova->pfn_lo < this->pfn_lo)
 -			new = &((*new)->rb_left);
 -		else if (iova->pfn_lo > this->pfn_lo)
 -			new = &((*new)->rb_right);
 -		else {
 -			WARN_ON(1); /* this should not happen */
 -			return;
 -		}
 -	}
 -	/* Add new node and rebalance tree. */
 -	rb_link_node(&iova->node, parent, new);
 -	rb_insert_color(&iova->node, root);
 +	return (limit_pfn - size) & (__roundup_pow_of_two(size) - 1);
  }
  
  static int __alloc_and_insert_iova_range(struct iova_domain *iovad,
  		unsigned long size, unsigned long limit_pfn,
  			struct iova *new, bool size_aligned)
  {
- 	struct rb_node *prev, *curr = NULL;
+ 	struct rb_node *curr, *prev;
+ 	struct iova *curr_iova;
  	unsigned long flags;
 -	unsigned long new_pfn;
 -	unsigned long align_mask = ~0UL;
 -
 -	if (size_aligned)
 -		align_mask <<= fls_long(size - 1);
 +	unsigned long saved_pfn;
 +	unsigned int pad_size = 0;
  
  	/* Walk the tree backwards */
  	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
++<<<<<<< HEAD
 +	saved_pfn = limit_pfn;
 +	curr = __get_cached_rbnode(iovad, &limit_pfn);
 +	prev = curr;
 +	while (curr) {
 +		struct iova *curr_iova = container_of(curr, struct iova, node);
 +
 +		if (limit_pfn <= curr_iova->pfn_lo) {
 +			goto move_left;
 +		} else if (limit_pfn > curr_iova->pfn_hi) {
 +			if (size_aligned)
 +				pad_size = iova_get_pad_size(size, limit_pfn);
 +			if ((curr_iova->pfn_hi + size + pad_size) < limit_pfn)
 +				break;	/* found a free slot */
 +		}
 +		limit_pfn = curr_iova->pfn_lo;
 +move_left:
++=======
+ 	curr = __get_cached_rbnode(iovad, limit_pfn);
+ 	curr_iova = rb_entry(curr, struct iova, node);
+ 	do {
+ 		limit_pfn = min(limit_pfn, curr_iova->pfn_lo);
+ 		new_pfn = (limit_pfn - size) & align_mask;
++>>>>>>> 973f5fbedb07 (iommu/iova: Simplify cached node logic)
  		prev = curr;
  		curr = rb_prev(curr);
- 	}
+ 		curr_iova = rb_entry(curr, struct iova, node);
+ 	} while (curr && new_pfn <= curr_iova->pfn_hi);
  
++<<<<<<< HEAD
 +	if (!curr) {
 +		if (size_aligned)
 +			pad_size = iova_get_pad_size(size, limit_pfn);
 +		if ((iovad->start_pfn + size + pad_size) > limit_pfn) {
 +			spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
 +			return -ENOMEM;
 +		}
++=======
+ 	if (limit_pfn < size || new_pfn < iovad->start_pfn) {
+ 		spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+ 		return -ENOMEM;
++>>>>>>> 973f5fbedb07 (iommu/iova: Simplify cached node logic)
  	}
  
  	/* pfn_lo will point to size aligned address if size_aligned is set */
* Unmerged path drivers/iommu/iova.c
