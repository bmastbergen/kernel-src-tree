seqlock: Introduce raw_read_seqcount_latch()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 7fc26327b75685f37f58d64bdb061460f834f80d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/7fc26327.failed

Because with latches there is a strict data dependency on the seq load
we can avoid the rmb in favour of a read_barrier_depends.

	Suggested-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
(cherry picked from commit 7fc26327b75685f37f58d64bdb061460f834f80d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/seqlock.h
diff --cc include/linux/seqlock.h
index 48f2f69e3867,890c7ef709d5..000000000000
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@@ -34,6 -34,8 +34,11 @@@
  
  #include <linux/spinlock.h>
  #include <linux/preempt.h>
++<<<<<<< HEAD
++=======
+ #include <linux/lockdep.h>
+ #include <linux/compiler.h>
++>>>>>>> 7fc26327b756 (seqlock: Introduce raw_read_seqcount_latch())
  #include <asm/processor.h>
  
  /*
@@@ -171,9 -221,100 +176,106 @@@ static inline int read_seqcount_retry(c
  }
  
  
++<<<<<<< HEAD
 +/*
 + * raw_write_seqcount_latch - redirect readers to even/odd copy
 + * @s: pointer to seqcount_t
++=======
+ 
+ static inline void raw_write_seqcount_begin(seqcount_t *s)
+ {
+ 	s->sequence++;
+ 	smp_wmb();
+ }
+ 
+ static inline void raw_write_seqcount_end(seqcount_t *s)
+ {
+ 	smp_wmb();
+ 	s->sequence++;
+ }
+ 
+ static inline int raw_read_seqcount_latch(seqcount_t *s)
+ {
+ 	return lockless_dereference(s->sequence);
+ }
+ 
+ /**
+  * raw_write_seqcount_latch - redirect readers to even/odd copy
+  * @s: pointer to seqcount_t
+  *
+  * The latch technique is a multiversion concurrency control method that allows
+  * queries during non-atomic modifications. If you can guarantee queries never
+  * interrupt the modification -- e.g. the concurrency is strictly between CPUs
+  * -- you most likely do not need this.
+  *
+  * Where the traditional RCU/lockless data structures rely on atomic
+  * modifications to ensure queries observe either the old or the new state the
+  * latch allows the same for non-atomic updates. The trade-off is doubling the
+  * cost of storage; we have to maintain two copies of the entire data
+  * structure.
+  *
+  * Very simply put: we first modify one copy and then the other. This ensures
+  * there is always one copy in a stable state, ready to give us an answer.
+  *
+  * The basic form is a data structure like:
+  *
+  * struct latch_struct {
+  *	seqcount_t		seq;
+  *	struct data_struct	data[2];
+  * };
+  *
+  * Where a modification, which is assumed to be externally serialized, does the
+  * following:
+  *
+  * void latch_modify(struct latch_struct *latch, ...)
+  * {
+  *	smp_wmb();	<- Ensure that the last data[1] update is visible
+  *	latch->seq++;
+  *	smp_wmb();	<- Ensure that the seqcount update is visible
+  *
+  *	modify(latch->data[0], ...);
+  *
+  *	smp_wmb();	<- Ensure that the data[0] update is visible
+  *	latch->seq++;
+  *	smp_wmb();	<- Ensure that the seqcount update is visible
+  *
+  *	modify(latch->data[1], ...);
+  * }
+  *
+  * The query will have a form like:
+  *
+  * struct entry *latch_query(struct latch_struct *latch, ...)
+  * {
+  *	struct entry *entry;
+  *	unsigned seq, idx;
+  *
+  *	do {
+  *		seq = lockless_dereference(latch->seq);
+  *
+  *		idx = seq & 0x01;
+  *		entry = data_query(latch->data[idx], ...);
+  *
+  *		smp_rmb();
+  *	} while (seq != latch->seq);
+  *
+  *	return entry;
+  * }
+  *
+  * So during the modification, queries are first redirected to data[1]. Then we
+  * modify data[0]. When that is complete, we redirect queries back to data[0]
+  * and we can modify data[1].
+  *
+  * NOTE: The non-requirement for atomic modifications does _NOT_ include
+  *       the publishing of new entries in the case where data is a dynamic
+  *       data structure.
+  *
+  *       An iteration might start in data[0] and get suspended long enough
+  *       to miss an entire modification sequence, once it resumes it might
+  *       observe the new entry.
+  *
+  * NOTE: When data is a dynamic data structure; one should use regular RCU
+  *       patterns to manage the lifetimes of the objects within.
++>>>>>>> 7fc26327b756 (seqlock: Introduce raw_read_seqcount_latch())
   */
  static inline void raw_write_seqcount_latch(seqcount_t *s)
  {
* Unmerged path include/linux/seqlock.h
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index d7063566cc22..40e670f0a4c7 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -317,7 +317,7 @@ static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
 	u64 now;
 
 	do {
-		seq = raw_read_seqcount(&tkf->seq);
+		seq = raw_read_seqcount_latch(&tkf->seq);
 		tkr = tkf->base + (seq & 0x01);
 		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
 	} while (read_seqcount_retry(&tkf->seq, seq));
