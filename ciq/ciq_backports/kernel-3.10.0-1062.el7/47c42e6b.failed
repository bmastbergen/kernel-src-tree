KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size'

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 47c42e6b4192a2ac8b6c9858ebcf400a9eff7a10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/47c42e6b.failed

The cr4_pae flag is a bit of a misnomer, its purpose is really to track
whether the guest PTE that is being shadowed is a 4-byte entry or an
8-byte entry.  Prior to supporting nested EPT, the size of the gpte was
reflected purely by CR4.PAE.  KVM fudged things a bit for direct sptes,
but it was mostly harmless since the size of the gpte never mattered.
Now that a spte may be tracking an indirect EPT entry, relying on
CR4.PAE is wrong and ill-named.

For direct shadow pages, force the gpte_size to '1' as they are always
8-byte entries; EPT entries can only be 8-bytes and KVM always uses
8-byte entries for NPT and its identity map (when running with EPT but
not unrestricted guest).

Likewise, nested EPT entries are always 8-bytes.  Nested EPT presents a
unique scenario as the size of the entries are not dictated by CR4.PAE,
but neither is the shadow page a direct map.  To handle this scenario,
set cr0_wp=1 and smap_andnot_wp=1, an otherwise impossible combination,
to denote a nested EPT shadow page.  Use the information to avoid
incorrectly zapping an unsync'd indirect page in __kvm_sync_page().

Providing a consistent and accurate gpte_size fixes a bug reported by
Vitaly where fast_cr3_switch() always fails when switching from L2 to
L1 as kvm_mmu_get_page() would force role.cr4_pae=0 for direct pages,
whereas kvm_calc_mmu_role_common() would set it according to CR4.PAE.

Fixes: 7dcd575520082 ("x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed")
	Reported-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Tested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 47c42e6b4192a2ac8b6c9858ebcf400a9eff7a10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/mmutrace.h
diff --cc arch/x86/kvm/mmu.c
index d16e6f650c80,776a58b00682..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -161,7 -180,24 +161,28 @@@ struct kvm_shadow_walk_iterator 
  	unsigned index;
  };
  
++<<<<<<< HEAD
 +#define for_each_shadow_entry(_vcpu, _addr, _walker)    \
++=======
+ static const union kvm_mmu_page_role mmu_base_role_mask = {
+ 	.cr0_wp = 1,
+ 	.gpte_is_8_bytes = 1,
+ 	.nxe = 1,
+ 	.smep_andnot_wp = 1,
+ 	.smap_andnot_wp = 1,
+ 	.smm = 1,
+ 	.guest_mode = 1,
+ 	.ad_disabled = 1,
+ };
+ 
+ #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
+ 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
+ 					 (_root), (_addr));                \
+ 	     shadow_walk_okay(&(_walker));			           \
+ 	     shadow_walk_next(&(_walker)))
+ 
+ #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
  	     shadow_walk_okay(&(_walker));			\
  	     shadow_walk_next(&(_walker)))
@@@ -2076,39 -2205,46 +2097,54 @@@ static int kvm_mmu_prepare_zap_page(str
  static void kvm_mmu_commit_zap_page(struct kvm *kvm,
  				    struct list_head *invalid_list);
  
++<<<<<<< HEAD
 +/*
 + * NOTE: we should pay more attention on the zapped-obsolete page
 + * (is_obsolete_sp(sp) && sp->role.invalid) when you do hash list walk
 + * since it has been deleted from active_mmu_pages but still can be found
 + * at hast list.
 + *
 + * for_each_gfn_indirect_valid_sp has skipped that kind of page and
 + * kvm_mmu_get_page(), the only user of for_each_gfn_sp(), has skipped
 + * all the obsolete pages.
 + */
 +#define for_each_gfn_sp(_kvm, _sp, _gfn)				\
++=======
+ 
+ #define for_each_valid_sp(_kvm, _sp, _gfn)				\
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  	hlist_for_each_entry(_sp,					\
  	  &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)], hash_link) \
 -		if ((_sp)->role.invalid) {    \
 -		} else
 +		if ((_sp)->gfn != (_gfn)) {} else
  
  #define for_each_gfn_indirect_valid_sp(_kvm, _sp, _gfn)			\
 -	for_each_valid_sp(_kvm, _sp, _gfn)				\
 -		if ((_sp)->gfn != (_gfn) || (_sp)->role.direct) {} else
 +	for_each_gfn_sp(_kvm, _sp, _gfn)				\
 +		if ((_sp)->role.direct || (_sp)->role.invalid) {} else
  
+ static inline bool is_ept_sp(struct kvm_mmu_page *sp)
+ {
+ 	return sp->role.cr0_wp && sp->role.smap_andnot_wp;
+ }
+ 
  /* @sp->gfn should be write-protected at the call site */
  static bool __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
  			    struct list_head *invalid_list)
  {
 -	if ((!is_ept_sp(sp) && sp->role.gpte_is_8_bytes != !!is_pae(vcpu)) ||
 -	    vcpu->arch.mmu->sync_page(vcpu, sp) == 0) {
++<<<<<<< HEAD
 +	if (sp->role.cr4_pae != !!is_pae(vcpu)) {
  		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
  		return false;
  	}
  
 -	return true;
 -}
 -
 -static bool kvm_mmu_remote_flush_or_zap(struct kvm *kvm,
 -					struct list_head *invalid_list,
 -					bool remote_flush)
 -{
 -	if (!remote_flush && !list_empty(invalid_list))
 +	if (vcpu->arch.mmu.sync_page(vcpu, sp) == 0) {
++=======
++	if ((!is_ept_sp(sp) && sp->role.gpte_is_8_bytes != !!is_pae(vcpu)) ||
++	    vcpu->arch.mmu->sync_page(vcpu, sp) == 0) {
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
 +		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
  		return false;
 +	}
  
 -	if (!list_empty(invalid_list))
 -		kvm_mmu_commit_zap_page(kvm, invalid_list);
 -	else
 -		kvm_flush_remote_tlbs(kvm);
  	return true;
  }
  
@@@ -2298,10 -2429,10 +2334,10 @@@ static struct kvm_mmu_page *kvm_mmu_get
  	role.level = level;
  	role.direct = direct;
  	if (role.direct)
- 		role.cr4_pae = 0;
+ 		role.gpte_is_8_bytes = true;
  	role.access = access;
 -	if (!vcpu->arch.mmu->direct_map
 -	    && vcpu->arch.mmu->root_level <= PT32_ROOT_LEVEL) {
 +	if (!vcpu->arch.mmu.direct_map
 +	    && vcpu->arch.mmu.root_level <= PT32_ROOT_LEVEL) {
  		quadrant = gaddr >> (PAGE_SHIFT + (PT64_PT_BITS * level));
  		quadrant &= (1 << ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1;
  		role.quadrant = quadrant;
@@@ -4420,14 -4776,66 +4456,66 @@@ static void paging32E_init_context(stru
  	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
  }
  
++<<<<<<< HEAD
++=======
+ static union kvm_mmu_extended_role kvm_calc_mmu_role_ext(struct kvm_vcpu *vcpu)
+ {
+ 	union kvm_mmu_extended_role ext = {0};
+ 
+ 	ext.cr0_pg = !!is_paging(vcpu);
+ 	ext.cr4_smep = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
+ 	ext.cr4_smap = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
+ 	ext.cr4_pse = !!is_pse(vcpu);
+ 	ext.cr4_pke = !!kvm_read_cr4_bits(vcpu, X86_CR4_PKE);
+ 	ext.cr4_la57 = !!kvm_read_cr4_bits(vcpu, X86_CR4_LA57);
+ 	ext.maxphyaddr = cpuid_maxphyaddr(vcpu);
+ 
+ 	ext.valid = 1;
+ 
+ 	return ext;
+ }
+ 
+ static union kvm_mmu_role kvm_calc_mmu_role_common(struct kvm_vcpu *vcpu,
+ 						   bool base_only)
+ {
+ 	union kvm_mmu_role role = {0};
+ 
+ 	role.base.access = ACC_ALL;
+ 	role.base.nxe = !!is_nx(vcpu);
+ 	role.base.cr0_wp = is_write_protection(vcpu);
+ 	role.base.smm = is_smm(vcpu);
+ 	role.base.guest_mode = is_guest_mode(vcpu);
+ 
+ 	if (base_only)
+ 		return role;
+ 
+ 	role.ext = kvm_calc_mmu_role_ext(vcpu);
+ 
+ 	return role;
+ }
+ 
+ static union kvm_mmu_role
+ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
+ {
+ 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ 
+ 	role.base.ad_disabled = (shadow_accessed_mask == 0);
+ 	role.base.level = kvm_x86_ops->get_tdp_level(vcpu);
+ 	role.base.direct = true;
+ 	role.base.gpte_is_8_bytes = true;
+ 
+ 	return role;
+ }
+ 
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *context = vcpu->arch.mmu;
 -	union kvm_mmu_role new_role =
 -		kvm_calc_tdp_mmu_root_page_role(vcpu, false);
 -
 -	new_role.base.word &= mmu_base_role_mask.word;
 -	if (new_role.as_u64 == context->mmu_role.as_u64)
 -		return;
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
  
 -	context->mmu_role.as_u64 = new_role.as_u64;
 +	context->base_role.word = 0;
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
 +	context->base_role.ad_disabled = (shadow_accessed_mask == 0);
  	context->page_fault = tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
@@@ -4467,13 -4875,37 +4555,38 @@@
  	reset_tdp_shadow_zero_bits_mask(vcpu, context);
  }
  
++<<<<<<< HEAD
++=======
+ static union kvm_mmu_role
+ kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
+ {
+ 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ 
+ 	role.base.smep_andnot_wp = role.ext.cr4_smep &&
+ 		!is_write_protection(vcpu);
+ 	role.base.smap_andnot_wp = role.ext.cr4_smap &&
+ 		!is_write_protection(vcpu);
+ 	role.base.direct = !is_paging(vcpu);
+ 	role.base.gpte_is_8_bytes = !!is_pae(vcpu);
+ 
+ 	if (!is_long_mode(vcpu))
+ 		role.base.level = PT32E_ROOT_LEVEL;
+ 	else if (is_la57_mode(vcpu))
+ 		role.base.level = PT64_ROOT_5LEVEL;
+ 	else
+ 		role.base.level = PT64_ROOT_4LEVEL;
+ 
+ 	return role;
+ }
+ 
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *context = vcpu->arch.mmu;
 -	union kvm_mmu_role new_role =
 -		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
 +	bool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
 +	bool smap = kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
  
 -	new_role.base.word &= mmu_base_role_mask.word;
 -	if (new_role.as_u64 == context->mmu_role.as_u64)
 -		return;
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
  
  	if (!is_paging(vcpu))
  		nonpaging_init_context(vcpu, context);
@@@ -4497,14 -4921,50 +4610,46 @@@
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
  
++<<<<<<< HEAD
++=======
+ static union kvm_mmu_role
+ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
+ 				   bool execonly)
+ {
+ 	union kvm_mmu_role role = {0};
+ 
+ 	/* SMM flag is inherited from root_mmu */
+ 	role.base.smm = vcpu->arch.root_mmu.mmu_role.base.smm;
+ 
+ 	role.base.level = PT64_ROOT_4LEVEL;
+ 	role.base.gpte_is_8_bytes = true;
+ 	role.base.direct = false;
+ 	role.base.ad_disabled = !accessed_dirty;
+ 	role.base.guest_mode = true;
+ 	role.base.access = ACC_ALL;
+ 
+ 	/*
+ 	 * WP=1 and NOT_WP=1 is an impossible combination, use WP and the
+ 	 * SMAP variation to denote shadow EPT entries.
+ 	 */
+ 	role.base.cr0_wp = true;
+ 	role.base.smap_andnot_wp = true;
+ 
+ 	role.ext = kvm_calc_mmu_role_ext(vcpu);
+ 	role.ext.execonly = execonly;
+ 
+ 	return role;
+ }
+ 
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 -			     bool accessed_dirty, gpa_t new_eptp)
 +			     bool accessed_dirty)
  {
 -	struct kvm_mmu *context = vcpu->arch.mmu;
 -	union kvm_mmu_role new_role =
 -		kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty,
 -						   execonly);
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
  
 -	__kvm_mmu_new_cr3(vcpu, new_eptp, new_role.base, false);
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
  
 -	new_role.base.word &= mmu_base_role_mask.word;
 -	if (new_role.as_u64 == context->mmu_role.as_u64)
 -		return;
 -
 -	context->shadow_root_level = PT64_ROOT_4LEVEL;
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
  
  	context->nx = true;
  	context->ept_ad = accessed_dirty;
diff --cc arch/x86/kvm/mmutrace.h
index cfa5286730cc,dd30dccd2ad5..000000000000
--- a/arch/x86/kvm/mmutrace.h
+++ b/arch/x86/kvm/mmutrace.h
@@@ -30,11 -29,10 +30,15 @@@
  								        \
  	role.word = __entry->role;					\
  									\
++<<<<<<< HEAD
 +	trace_seq_printf(p, "sp gen %lx gfn %llx l%u%s q%u%s %s%s"	\
++=======
+ 	trace_seq_printf(p, "sp gfn %llx l%u %u-byte q%u%s %s%s"	\
++>>>>>>> 47c42e6b4192 (KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size')
  			 " %snxe %sad root %u %s%c",			\
 +			 __entry->mmu_valid_gen,			\
  			 __entry->gfn, role.level,			\
- 			 role.cr4_pae ? " pae" : "",			\
+ 			 role.gpte_is_8_bytes ? 8 : 4,			\
  			 role.quadrant,					\
  			 role.direct ? " direct" : "",			\
  			 access_str[role.access],			\
diff --git a/Documentation/virtual/kvm/mmu.txt b/Documentation/virtual/kvm/mmu.txt
index f50d45b1e967..1497bec6b3c0 100644
--- a/Documentation/virtual/kvm/mmu.txt
+++ b/Documentation/virtual/kvm/mmu.txt
@@ -142,7 +142,7 @@ Shadow pages contain the following information:
     If clear, this page corresponds to a guest page table denoted by the gfn
     field.
   role.quadrant:
-    When role.cr4_pae=0, the guest uses 32-bit gptes while the host uses 64-bit
+    When role.gpte_is_8_bytes=0, the guest uses 32-bit gptes while the host uses 64-bit
     sptes.  That means a guest page table contains more ptes than the host,
     so multiple shadow pages are needed to shadow one guest page.
     For first-level shadow pages, role.quadrant can be 0 or 1 and denotes the
@@ -158,9 +158,9 @@ Shadow pages contain the following information:
     The page is invalid and should not be used.  It is a root page that is
     currently pinned (by a cpu hardware register pointing to it); once it is
     unpinned it will be destroyed.
-  role.cr4_pae:
-    Contains the value of cr4.pae for which the page is valid (e.g. whether
-    32-bit or 64-bit gptes are in use).
+  role.gpte_is_8_bytes:
+    Reflects the size of the guest PTE for which the page is valid, i.e. '1'
+    if 64-bit gptes are in use, '0' if 32-bit gptes are in use.
   role.nxe:
     Contains the value of efer.nxe for which the page is valid.
   role.cr0_wp:
@@ -173,6 +173,9 @@ Shadow pages contain the following information:
     Contains the value of cr4.smap && !cr0.wp for which the page is valid
     (pages for which this is true are different from other pages; see the
     treatment of cr0.wp=0 below).
+  role.ept_sp:
+    This is a virtual flag to denote a shadowed nested EPT page.  ept_sp
+    is true if "cr0_wp && smap_andnot_wp", an otherwise invalid combination.
   role.smm:
     Is 1 if the page is valid in system management mode.  This field
     determines which of the kvm_memslots array was used to build this
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 86bec63d5f8a..e29a3861001c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -216,14 +216,14 @@ struct kvm_mmu_memory_cache {
  * kvm_memory_slot.arch.gfn_track which is 16 bits, so the role bits used
  * by indirect shadow page can not be more than 15 bits.
  *
- * Currently, we used 14 bits that are @level, @cr4_pae, @quadrant, @access,
+ * Currently, we used 14 bits that are @level, @gpte_is_8_bytes, @quadrant, @access,
  * @nxe, @cr0_wp, @smep_andnot_wp and @smap_andnot_wp.
  */
 union kvm_mmu_page_role {
 	unsigned word;
 	struct {
 		unsigned level:4;
-		unsigned cr4_pae:1;
+		unsigned gpte_is_8_bytes:1;
 		unsigned quadrant:2;
 		unsigned direct:1;
 		unsigned access:3;
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/mmutrace.h
