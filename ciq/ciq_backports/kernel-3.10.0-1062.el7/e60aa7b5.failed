iommu/iova: Extend rbtree node caching

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [iommu] iova: Extend rbtree node caching (Jerry Snitselaar) [1615865]
Rebuild_FUZZ: 91.43%
commit-author Robin Murphy <robin.murphy@arm.com>
commit e60aa7b53845a261dd419652f12ab9f89e668843
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/e60aa7b5.failed

The cached node mechanism provides a significant performance benefit for
allocations using a 32-bit DMA mask, but in the case of non-PCI devices
or where the 32-bit space is full, the loss of this benefit can be
significant - on large systems there can be many thousands of entries in
the tree, such that walking all the way down to find free space every
time becomes increasingly awful.

Maintain a similar cached node for the whole IOVA space as a superset of
the 32-bit space so that performance can remain much more consistent.

Inspired by work by Zhen Lei <thunder.leizhen@huawei.com>.

	Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
	Tested-by: Zhen Lei <thunder.leizhen@huawei.com>
	Tested-by: Nate Watterson <nwatters@codeaurora.org>
	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit e60aa7b53845a261dd419652f12ab9f89e668843)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
diff --cc drivers/iommu/iova.c
index 1a0166896ba6,c6f5a22f8d20..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -109,16 -111,20 +110,30 @@@ EXPORT_SYMBOL_GPL(init_iova_flush_queue
  static struct rb_node *
  __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)
  {
- 	if ((*limit_pfn > iovad->dma_32bit_pfn) ||
- 		(iovad->cached32_node == NULL))
+ 	struct rb_node *cached_node = NULL;
+ 	struct iova *curr_iova;
+ 
+ 	if (*limit_pfn <= iovad->dma_32bit_pfn)
+ 		cached_node = iovad->cached32_node;
+ 	if (!cached_node)
+ 		cached_node = iovad->cached_node;
+ 	if (!cached_node)
  		return rb_last(&iovad->rbroot);
++<<<<<<< HEAD
 +	else {
 +		struct rb_node *prev_node = rb_prev(iovad->cached32_node);
 +		struct iova *curr_iova =
 +			container_of(iovad->cached32_node, struct iova, node);
 +		*limit_pfn = curr_iova->pfn_lo;
 +		return prev_node;
 +	}
++=======
+ 
+ 	curr_iova = rb_entry(cached_node, struct iova, node);
+ 	*limit_pfn = min(*limit_pfn, curr_iova->pfn_lo);
+ 
+ 	return rb_prev(cached_node);
++>>>>>>> e60aa7b53845 (iommu/iova: Extend rbtree node caching)
  }
  
  static void
@@@ -134,33 -140,43 +149,43 @@@ static voi
  __cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)
  {
  	struct iova *cached_iova;
- 	struct rb_node *curr;
  
++<<<<<<< HEAD
 +	if (!iovad->cached32_node)
 +		return;
 +	curr = iovad->cached32_node;
 +	cached_iova = container_of(curr, struct iova, node);
 +
 +	if (free->pfn_lo >= cached_iova->pfn_lo) {
 +		struct rb_node *node = rb_next(&free->node);
 +		struct iova *iova = container_of(node, struct iova, node);
 +
 +		/* only cache if it's below 32bit pfn */
 +		if (node && iova->pfn_lo < iovad->dma_32bit_pfn)
 +			iovad->cached32_node = node;
 +		else
 +			iovad->cached32_node = NULL;
 +	}
++=======
+ 	cached_iova = rb_entry(iovad->cached32_node, struct iova, node);
+ 	if (free->pfn_hi < iovad->dma_32bit_pfn &&
+ 	    iovad->cached32_node && free->pfn_lo >= cached_iova->pfn_lo)
+ 		iovad->cached32_node = rb_next(&free->node);
+ 
+ 	cached_iova = rb_entry(iovad->cached_node, struct iova, node);
+ 	if (iovad->cached_node && free->pfn_lo >= cached_iova->pfn_lo)
+ 		iovad->cached_node = rb_next(&free->node);
++>>>>>>> e60aa7b53845 (iommu/iova: Extend rbtree node caching)
  }
  
 -/* Insert the iova into domain rbtree by holding writer lock */
 -static void
 -iova_insert_rbtree(struct rb_root *root, struct iova *iova,
 -		   struct rb_node *start)
 +/*
 + * Computes the padding size required, to make the start address
 + * naturally aligned on the power-of-two order of its size
 + */
 +static unsigned int
 +iova_get_pad_size(unsigned int size, unsigned int limit_pfn)
  {
 -	struct rb_node **new, *parent = NULL;
 -
 -	new = (start) ? &start : &(root->rb_node);
 -	/* Figure out where to put new node */
 -	while (*new) {
 -		struct iova *this = rb_entry(*new, struct iova, node);
 -
 -		parent = *new;
 -
 -		if (iova->pfn_lo < this->pfn_lo)
 -			new = &((*new)->rb_left);
 -		else if (iova->pfn_lo > this->pfn_lo)
 -			new = &((*new)->rb_right);
 -		else {
 -			WARN_ON(1); /* this should not happen */
 -			return;
 -		}
 -	}
 -	/* Add new node and rebalance tree. */
 -	rb_link_node(&iova->node, parent, new);
 -	rb_insert_color(&iova->node, root);
 +	return (limit_pfn - size) & (__roundup_pow_of_two(size) - 1);
  }
  
  static int __alloc_and_insert_iova_range(struct iova_domain *iovad,
@@@ -169,8 -185,11 +194,16 @@@
  {
  	struct rb_node *prev, *curr = NULL;
  	unsigned long flags;
++<<<<<<< HEAD
 +	unsigned long saved_pfn;
 +	unsigned int pad_size = 0;
++=======
+ 	unsigned long new_pfn;
+ 	unsigned long align_mask = ~0UL;
+ 
+ 	if (size_aligned)
+ 		align_mask <<= fls_long(size - 1);
++>>>>>>> e60aa7b53845 (iommu/iova: Extend rbtree node caching)
  
  	/* Walk the tree backwards */
  	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
@@@ -204,40 -217,12 +236,46 @@@ move_left
  	}
  
  	/* pfn_lo will point to size aligned address if size_aligned is set */
 -	new->pfn_lo = new_pfn;
 +	new->pfn_lo = limit_pfn - (size + pad_size);
  	new->pfn_hi = new->pfn_lo + size - 1;
  
++<<<<<<< HEAD
 +	/* Insert the new_iova into domain rbtree by holding writer lock */
 +	/* Add new node and rebalance tree. */
 +	{
 +		struct rb_node **entry, *parent = NULL;
 +
 +		/* If we have 'prev', it's a valid place to start the
 +		   insertion. Otherwise, start from the root. */
 +		if (prev)
 +			entry = &prev;
 +		else
 +			entry = &iovad->rbroot.rb_node;
 +
 +		/* Figure out where to put new node */
 +		while (*entry) {
 +			struct iova *this = container_of(*entry,
 +							struct iova, node);
 +			parent = *entry;
 +
 +			if (new->pfn_lo < this->pfn_lo)
 +				entry = &((*entry)->rb_left);
 +			else if (new->pfn_lo > this->pfn_lo)
 +				entry = &((*entry)->rb_right);
 +			else
 +				BUG(); /* this should not happen */
 +		}
 +
 +		/* Add new node and rebalance tree. */
 +		rb_link_node(&new->node, parent, entry);
 +		rb_insert_color(&new->node, &iovad->rbroot);
 +	}
 +	__cached_rbnode_insert_update(iovad, saved_pfn, new);
++=======
+ 	/* If we have 'prev', it's a valid place to start the insertion. */
+ 	iova_insert_rbtree(&iovad->rbroot, new, prev);
+ 	__cached_rbnode_insert_update(iovad, new);
++>>>>>>> e60aa7b53845 (iommu/iova: Extend rbtree node caching)
  
  	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
  
* Unmerged path drivers/iommu/iova.c
diff --git a/include/linux/iova.h b/include/linux/iova.h
index 09ce7dfe55b2..f84ab0b505bc 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -70,7 +70,8 @@ struct iova_fq {
 struct iova_domain {
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
 	struct rb_root	rbroot;		/* iova domain rbtree root */
-	struct rb_node	*cached32_node; /* Save last alloced node */
+	struct rb_node	*cached_node;	/* Save last alloced node */
+	struct rb_node	*cached32_node; /* Save last 32-bit alloced node */
 	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
