RDMA/umem: Get rid of struct ib_umem.odp_data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit 597ecc5a095406a668e53ab330495ddb65327f77
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/597ecc5a.failed

This no longer has any use, we can use container_of to get to the
umem_odp, and a simple flag to indicate if this is an odp MR. Remove the
few remaining references to it.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 597ecc5a095406a668e53ab330495ddb65327f77)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
#	drivers/infiniband/hw/mlx5/odp.c
#	include/rdma/ib_umem.h
diff --cc drivers/infiniband/core/umem.c
index 097e8522c0fc,fec5d489e311..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -108,9 -108,16 +108,22 @@@ struct ib_umem *ib_umem_get(struct ib_u
  	if (!can_do_mlock())
  		return ERR_PTR(-EPERM);
  
++<<<<<<< HEAD
 +	umem = kzalloc(sizeof *umem, GFP_KERNEL);
 +	if (!umem)
 +		return ERR_PTR(-ENOMEM);
++=======
+ 	if (access & IB_ACCESS_ON_DEMAND) {
+ 		umem = kzalloc(sizeof(struct ib_umem_odp), GFP_KERNEL);
+ 		if (!umem)
+ 			return ERR_PTR(-ENOMEM);
+ 		umem->is_odp = 1;
+ 	} else {
+ 		umem = kzalloc(sizeof(*umem), GFP_KERNEL);
+ 		if (!umem)
+ 			return ERR_PTR(-ENOMEM);
+ 	}
++>>>>>>> 597ecc5a0954 (RDMA/umem: Get rid of struct ib_umem.odp_data)
  
  	umem->context    = context;
  	umem->length     = size;
@@@ -226,7 -240,16 +239,20 @@@ out
  }
  EXPORT_SYMBOL(ib_umem_get);
  
++<<<<<<< HEAD
 +static void ib_umem_account(struct work_struct *work)
++=======
+ static void __ib_umem_release_tail(struct ib_umem *umem)
+ {
+ 	mmdrop(umem->owning_mm);
+ 	if (umem->is_odp)
+ 		kfree(to_ib_umem_odp(umem));
+ 	else
+ 		kfree(umem);
+ }
+ 
+ static void ib_umem_release_defer(struct work_struct *work)
++>>>>>>> 597ecc5a0954 (RDMA/umem: Get rid of struct ib_umem.odp_data)
  {
  	struct ib_umem *umem = container_of(work, struct ib_umem, work);
  
@@@ -244,12 -267,10 +270,12 @@@
  void ib_umem_release(struct ib_umem *umem)
  {
  	struct ib_ucontext *context = umem->context;
 +	struct mm_struct *mm;
 +	struct task_struct *task;
 +	unsigned long diff;
  
- 	if (umem->odp_data) {
+ 	if (umem->is_odp) {
  		ib_umem_odp_release(to_ib_umem_odp(umem));
 -		__ib_umem_release_tail(umem);
  		return;
  	}
  
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 45dd3865a782,d4780bded74a..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -392,11 -393,11 +393,16 @@@ next_mr
  			return ERR_CAST(odp);
  		}
  
 -		mtt = implicit_mr_alloc(mr->ibmr.pd, &odp->umem, 0,
 +		mtt = implicit_mr_alloc(mr->ibmr.pd, odp->umem, 0,
  					mr->access_flags);
  		if (IS_ERR(mtt)) {
++<<<<<<< HEAD
 +			mutex_unlock(&mr->umem->odp_data->umem_mutex);
 +			ib_umem_release(odp->umem);
++=======
+ 			mutex_unlock(&odp_mr->umem_mutex);
+ 			ib_umem_release(&odp->umem);
++>>>>>>> 597ecc5a0954 (RDMA/umem: Get rid of struct ib_umem.odp_data)
  			return ERR_CAST(mtt);
  		}
  
diff --cc include/rdma/ib_umem.h
index e58b679102d1,5d3755ec5afa..000000000000
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@@ -45,12 -45,11 +45,16 @@@ struct ib_umem 
  	size_t			length;
  	unsigned long		address;
  	int			page_shift;
- 	int                     writable;
- 	int                     hugetlb;
+ 	u32 writable : 1;
+ 	u32 hugetlb : 1;
+ 	u32 is_odp : 1;
  	struct work_struct	work;
++<<<<<<< HEAD
 +	struct mm_struct       *mm;
 +	unsigned long		diff;
 +	struct ib_umem_odp     *odp_data;
++=======
++>>>>>>> 597ecc5a0954 (RDMA/umem: Get rid of struct ib_umem.odp_data)
  	struct sg_table sg_head;
  	int             nmap;
  	int             npages;
* Unmerged path drivers/infiniband/core/umem.c
diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c
index 505862826f1c..beee83c20a3a 100644
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@ -274,6 +274,7 @@ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
 	umem->address    = addr;
 	umem->page_shift = PAGE_SHIFT;
 	umem->writable   = 1;
+	umem->is_odp = 1;
 
 	odp_data = kzalloc(sizeof(*odp_data), GFP_KERNEL);
 	if (!odp_data) {
@@ -307,8 +308,6 @@ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
 			 &context->no_private_counters);
 	up_write(&context->umem_rwsem);
 
-	umem->odp_data = odp_data;
-
 	return odp_data;
 
 out_page_list:
diff --git a/drivers/infiniband/hw/mlx5/mem.c b/drivers/infiniband/hw/mlx5/mem.c
index f3dbd75a0a96..549234988bb4 100644
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -57,7 +57,7 @@ void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
 	int entry;
 	unsigned long page_shift = umem->page_shift;
 
-	if (umem->odp_data) {
+	if (umem->is_odp) {
 		*ncont = ib_umem_page_count(umem);
 		*count = *ncont << (page_shift - PAGE_SHIFT);
 		*shift = page_shift;
@@ -152,14 +152,13 @@ void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 	struct scatterlist *sg;
 	int entry;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	const bool odp = umem->odp_data != NULL;
-
-	if (odp) {
+	if (umem->is_odp) {
 		WARN_ON(shift != 0);
 		WARN_ON(access_flags != (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE));
 
 		for (i = 0; i < num_pages; ++i) {
-			dma_addr_t pa = umem->odp_data->dma_list[offset + i];
+			dma_addr_t pa =
+				to_ib_umem_odp(umem)->dma_list[offset + i];
 
 			pas[i] = cpu_to_be64(umem_dma_to_mtt(pa));
 		}
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 126fe4816f0b..5f2d93561f54 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -98,7 +98,7 @@ static bool use_umr_mtt_update(struct mlx5_ib_mr *mr, u64 start, u64 length)
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 static void update_odp_mr(struct mlx5_ib_mr *mr)
 {
-	if (mr->umem->odp_data) {
+	if (mr->umem->is_odp) {
 		/*
 		 * This barrier prevents the compiler from moving the
 		 * setting of umem->odp_data->private to point to our
@@ -107,7 +107,7 @@ static void update_odp_mr(struct mlx5_ib_mr *mr)
 		 * handle invalidations.
 		 */
 		smp_wmb();
-		mr->umem->odp_data->private = mr;
+		to_ib_umem_odp(mr->umem)->private = mr;
 		/*
 		 * Make sure we will see the new
 		 * umem->odp_data->private value in the invalidation
@@ -1544,15 +1544,16 @@ static int dereg_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
 	struct ib_umem *umem = mr->umem;
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	if (umem && umem->odp_data) {
+	if (umem && umem->is_odp) {
+		struct ib_umem_odp *umem_odp = to_ib_umem_odp(umem);
+
 		/* Prevent new page faults from succeeding */
 		mr->live = 0;
 		/* Wait for all running page-fault handlers to finish. */
 		synchronize_srcu(&dev->mr_srcu);
 		/* Destroy all page mappings */
-		if (umem->odp_data->page_list)
-			mlx5_ib_invalidate_range(to_ib_umem_odp(umem),
-						 ib_umem_start(umem),
+		if (umem_odp->page_list)
+			mlx5_ib_invalidate_range(umem_odp, ib_umem_start(umem),
 						 ib_umem_end(umem));
 		else
 			mlx5_ib_free_implicit_mr(mr);
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
* Unmerged path include/rdma/ib_umem.h
