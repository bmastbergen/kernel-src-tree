locking/arch: Rename set_mb() to smp_store_mb()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit b92b8b35a2e38bde319fd1d68ec84628c1f1b0fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/b92b8b35.failed

Since set_mb() is really about an smp_mb() -- not a IO/DMA barrier
like mb() rename it to match the recent smp_load_acquire() and
smp_store_release().

	Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b92b8b35a2e38bde319fd1d68ec84628c1f1b0fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/barrier.h
#	arch/arm64/include/asm/barrier.h
#	arch/ia64/include/asm/barrier.h
#	arch/metag/include/asm/barrier.h
#	arch/mips/include/asm/barrier.h
#	arch/powerpc/include/asm/barrier.h
#	arch/s390/include/asm/barrier.h
#	arch/sparc/include/asm/barrier_64.h
#	arch/x86/include/asm/barrier.h
#	arch/x86/um/asm/barrier.h
#	include/asm-generic/barrier.h
#	include/linux/sched.h
#	kernel/locking/qspinlock_paravirt.h
diff --cc arch/arm/include/asm/barrier.h
index 9486fe4b7399,6c2327e1c732..000000000000
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@@ -66,7 -66,25 +66,14 @@@
  #define read_barrier_depends()		do { } while(0)
  #define smp_read_barrier_depends()	do { } while(0)
  
++<<<<<<< HEAD
 +#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
++=======
+ #define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
+ 
+ #define smp_mb__before_atomic()	smp_mb()
+ #define smp_mb__after_atomic()	smp_mb()
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #endif /* !__ASSEMBLY__ */
  #endif /* __ASM_BARRIER_H */
diff --cc arch/arm64/include/asm/barrier.h
index 0fe52fcc096f,0fa47c4275cb..000000000000
--- a/arch/arm64/include/asm/barrier.h
+++ b/arch/arm64/include/asm/barrier.h
@@@ -49,9 -114,12 +49,13 @@@
  #define read_barrier_depends()		do { } while(0)
  #define smp_read_barrier_depends()	do { } while(0)
  
++<<<<<<< HEAD
 +#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
++=======
+ #define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  #define nop()		asm volatile("nop");
  
 -#define smp_mb__before_atomic()	smp_mb()
 -#define smp_mb__after_atomic()	smp_mb()
 -
  #endif	/* __ASSEMBLY__ */
  
  #endif	/* __ASM_BARRIER_H */
diff --cc arch/ia64/include/asm/barrier.h
index 6aa1cdb884e1,843ba435e43b..000000000000
--- a/arch/ia64/include/asm/barrier.h
+++ b/arch/ia64/include/asm/barrier.h
@@@ -54,12 -54,30 +54,39 @@@
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
  
++<<<<<<< HEAD
 +/*
 + * XXX check on this ---I suspect what Linus really wants here is
 + * acquire vs release semantics but we can't discuss this stuff with
 + * Linus just yet.  Grrr...
 + */
 +#define set_mb(var, value)	do { (var) = (value); mb(); } while (0)
++=======
+ #define smp_mb__before_atomic()	barrier()
+ #define smp_mb__after_atomic()	barrier()
+ 
+ /*
+  * IA64 GCC turns volatile stores into st.rel and volatile loads into ld.acq no
+  * need for asm trickery!
+  */
+ 
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	ACCESS_ONCE(*p) = (v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  /*
   * The group barrier in front of the rsm & ssm are necessary to ensure
diff --cc arch/metag/include/asm/barrier.h
index 21ee80547b5a,5a696e507930..000000000000
--- a/arch/metag/include/asm/barrier.h
+++ b/arch/metag/include/asm/barrier.h
@@@ -81,6 -84,24 +81,28 @@@ static inline void fence(void
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
  
++<<<<<<< HEAD
 +#define set_mb(var, value) do { var = value; smp_mb(); } while (0)
++=======
+ #define smp_store_mb(var, value) do { WRITE_ONCE(var, value); smp_mb(); } while (0)
+ 
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	ACCESS_ONCE(*p) = (v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_mb__before_atomic()	barrier()
+ #define smp_mb__after_atomic()	barrier()
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #endif /* _ASM_METAG_BARRIER_H */
diff --cc arch/mips/include/asm/barrier.h
index 5659ce269858,7ecba84656d4..000000000000
--- a/arch/mips/include/asm/barrier.h
+++ b/arch/mips/include/asm/barrier.h
@@@ -112,8 -112,8 +112,13 @@@
  #define __WEAK_LLSC_MB		"		\n"
  #endif
  
++<<<<<<< HEAD
 +#define set_mb(var, value) \
 +	do { var = value; smp_mb(); } while (0)
++=======
+ #define smp_store_mb(var, value) \
+ 	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #define smp_llsc_mb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
  
diff --cc arch/powerpc/include/asm/barrier.h
index df6b7335588b,39505d660a70..000000000000
--- a/arch/powerpc/include/asm/barrier.h
+++ b/arch/powerpc/include/asm/barrier.h
@@@ -34,10 -34,7 +34,14 @@@
  #define rmb()  __asm__ __volatile__ ("sync" : : : "memory")
  #define wmb()  __asm__ __volatile__ ("sync" : : : "memory")
  
++<<<<<<< HEAD
 +#define gmb()  __asm__ __volatile__ ("ori 31,31,0": : : "memory")
 +#define barrier_nospec() gmb()
 +
 +#define set_mb(var, value)	do { var = value; mb(); } while (0)
++=======
+ #define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #ifdef __SUBARCH_HAS_LWSYNC
  #    define SMPWMB      LWSYNC
diff --cc arch/s390/include/asm/barrier.h
index 09f1de1aeca7,e6f8615a11eb..000000000000
--- a/arch/s390/include/asm/barrier.h
+++ b/arch/s390/include/asm/barrier.h
@@@ -47,7 -36,7 +47,11 @@@ static inline void gmb(void
  #define smp_mb__before_atomic()		smp_mb()
  #define smp_mb__after_atomic()		smp_mb()
  
++<<<<<<< HEAD
 +#define set_mb(var, value)		do { var = value; mb(); } while (0)
++=======
+ #define smp_store_mb(var, value)		do { WRITE_ONCE(var, value); mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #define smp_store_release(p, v)						\
  do {									\
diff --cc arch/sparc/include/asm/barrier_64.h
index 2e7b41eb2e56,809941e33e12..000000000000
--- a/arch/sparc/include/asm/barrier_64.h
+++ b/arch/sparc/include/asm/barrier_64.h
@@@ -40,8 -40,8 +40,13 @@@ do {	__asm__ __volatile__("ba,pt	%%xcc
  #define dma_rmb()	rmb()
  #define dma_wmb()	wmb()
  
++<<<<<<< HEAD
 +#define set_mb(__var, __value) \
 +	do { __var = __value; membar_safe("#StoreLoad"); } while(0)
++=======
+ #define smp_store_mb(__var, __value) \
+ 	do { WRITE_ONCE(__var, __value); membar_safe("#StoreLoad"); } while(0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #ifdef CONFIG_SMP
  #define smp_mb()	mb()
diff --cc arch/x86/include/asm/barrier.h
index 3550d30e3020,e51a8f803f55..000000000000
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@@ -67,7 -40,7 +67,11 @@@ static inline unsigned long array_index
  #define smp_mb()	barrier()
  #define smp_rmb()	barrier()
  #define smp_wmb()	barrier()
++<<<<<<< HEAD
 +#define set_mb(var, value) do { var = value; barrier(); } while (0)
++=======
+ #define smp_store_mb(var, value) do { WRITE_ONCE(var, value); barrier(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  #endif /* SMP */
  
  #define read_barrier_depends()		do { } while (0)
diff --cc arch/x86/um/asm/barrier.h
index 672878e3adb3,b9531d343134..000000000000
--- a/arch/x86/um/asm/barrier.h
+++ b/arch/x86/um/asm/barrier.h
@@@ -48,9 -39,8 +48,14 @@@
  #define smp_mb()	barrier()
  #define smp_rmb()	barrier()
  #define smp_wmb()	barrier()
++<<<<<<< HEAD
 +#define set_mb(var, value) do { var = value; barrier(); } while (0)
 +
 +#endif /* CONFIG_SMP */
++=======
+ 
+ #define smp_store_mb(var, value) do { WRITE_ONCE(var, value); barrier(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
diff --cc include/asm-generic/barrier.h
index 85b306c8b318,e6a83d712ef6..000000000000
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@@ -94,10 -66,8 +94,15 @@@
  #define smp_read_barrier_depends()	do { } while (0)
  #endif
  
++<<<<<<< HEAD
 +#endif
 +
 +#ifndef set_mb
 +#define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
++=======
+ #ifndef smp_store_mb
+ #define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  #endif
  
  #ifndef smp_mb__before_atomic
diff --cc include/linux/sched.h
index 83cd9508a135,18f197223ebd..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -241,6 -242,43 +241,46 @@@ extern char ___assert_task_state[1 - 2*
  				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
  				 (task->flags & PF_FROZEN) == 0)
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
+ 
+ #define __set_task_state(tsk, state_value)			\
+ 	do {							\
+ 		(tsk)->task_state_change = _THIS_IP_;		\
+ 		(tsk)->state = (state_value);			\
+ 	} while (0)
+ #define set_task_state(tsk, state_value)			\
+ 	do {							\
+ 		(tsk)->task_state_change = _THIS_IP_;		\
+ 		smp_store_mb((tsk)->state, (state_value));		\
+ 	} while (0)
+ 
+ /*
+  * set_current_state() includes a barrier so that the write of current->state
+  * is correctly serialised wrt the caller's subsequent test of whether to
+  * actually sleep:
+  *
+  *	set_current_state(TASK_UNINTERRUPTIBLE);
+  *	if (do_i_need_to_sleep())
+  *		schedule();
+  *
+  * If the caller does not need such serialisation then use __set_current_state()
+  */
+ #define __set_current_state(state_value)			\
+ 	do {							\
+ 		current->task_state_change = _THIS_IP_;		\
+ 		current->state = (state_value);			\
+ 	} while (0)
+ #define set_current_state(state_value)				\
+ 	do {							\
+ 		current->task_state_change = _THIS_IP_;		\
+ 		smp_store_mb(current->state, (state_value));		\
+ 	} while (0)
+ 
+ #else
+ 
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  #define __set_task_state(tsk, state_value)		\
  	do { (tsk)->state = (state_value); } while (0)
  #define set_task_state(tsk, state_value)		\
@@@ -257,10 -295,12 +297,15 @@@
   *
   * If the caller does not need such serialisation then use __set_current_state()
   */
 -#define __set_current_state(state_value)		\
 +#define __set_current_state(state_value)			\
  	do { current->state = (state_value); } while (0)
++<<<<<<< HEAD
 +#define set_current_state(state_value)		\
 +	set_mb(current->state, (state_value))
++=======
+ #define set_current_state(state_value)			\
+ 	smp_store_mb(current->state, (state_value))
 -
 -#endif
++>>>>>>> b92b8b35a2e3 (locking/arch: Rename set_mb() to smp_store_mb())
  
  /* Task command name length */
  #define TASK_COMM_LEN 16
* Unmerged path kernel/locking/qspinlock_paravirt.h
diff --git a/Documentation/memory-barriers.txt b/Documentation/memory-barriers.txt
index 8324e221276e..5e44ab2d6bb1 100644
--- a/Documentation/memory-barriers.txt
+++ b/Documentation/memory-barriers.txt
@@ -1089,7 +1089,7 @@ CPU from reordering them.
 
 There are some more advanced barrier functions:
 
- (*) set_mb(var, value)
+ (*) smp_store_mb(var, value)
 
      This assigns the value to the variable and then inserts a full memory
      barrier after it, depending on the function.  It isn't guaranteed to
@@ -1348,7 +1348,7 @@ after it has altered the task state:
 	CPU 1
 	===============================
 	set_current_state();
-	  set_mb();
+	  smp_store_mb();
 	    STORE current->state
 	    <general barrier>
 	LOAD event_indicated
@@ -1389,7 +1389,7 @@ between the STORE to indicate the event and the STORE to set TASK_RUNNING:
 	CPU 1				CPU 2
 	===============================	===============================
 	set_current_state();		STORE event_indicated
-	  set_mb();			wake_up();
+	  smp_store_mb();		wake_up();
 	    STORE current->state	  <write barrier>
 	    <general barrier>		  STORE current->state
 	LOAD event_indicated
* Unmerged path arch/arm/include/asm/barrier.h
* Unmerged path arch/arm64/include/asm/barrier.h
* Unmerged path arch/ia64/include/asm/barrier.h
* Unmerged path arch/metag/include/asm/barrier.h
* Unmerged path arch/mips/include/asm/barrier.h
* Unmerged path arch/powerpc/include/asm/barrier.h
* Unmerged path arch/s390/include/asm/barrier.h
diff --git a/arch/sh/include/asm/barrier.h b/arch/sh/include/asm/barrier.h
index 43715308b068..bf91037db4e0 100644
--- a/arch/sh/include/asm/barrier.h
+++ b/arch/sh/include/asm/barrier.h
@@ -32,7 +32,7 @@
 #define ctrl_barrier()	__asm__ __volatile__ ("nop;nop;nop;nop;nop;nop;nop;nop")
 #endif
 
-#define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
+#define smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)
 
 #include <asm-generic/barrier.h>
 
* Unmerged path arch/sparc/include/asm/barrier_64.h
* Unmerged path arch/x86/include/asm/barrier.h
* Unmerged path arch/x86/um/asm/barrier.h
diff --git a/fs/select.c b/fs/select.c
index 0d775cffd01a..a09a4377aba2 100644
--- a/fs/select.c
+++ b/fs/select.c
@@ -190,7 +190,7 @@ static int __pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)
 	 * doesn't imply write barrier and the users expect write
 	 * barrier semantics on wakeup functions.  The following
 	 * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
-	 * and is paired with set_mb() in poll_schedule_timeout.
+	 * and is paired with smp_store_mb() in poll_schedule_timeout.
 	 */
 	smp_wmb();
 	pwq->triggered = 1;
@@ -246,7 +246,7 @@ int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
 	/*
 	 * Prepare for the next iteration.
 	 *
-	 * The following set_mb() serves two purposes.  First, it's
+	 * The following smp_store_mb() serves two purposes.  First, it's
 	 * the counterpart rmb of the wmb in pollwake() such that data
 	 * written before wake up is always visible after wake up.
 	 * Second, the full barrier guarantees that triggered clearing
@@ -254,7 +254,7 @@ int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
 	 * this problem doesn't exist for the first iteration as
 	 * add_wait_queue() has full barrier semantics.
 	 */
-	set_mb(pwq->triggered, 0);
+	smp_store_mb(pwq->triggered, 0);
 
 	return rc;
 }
* Unmerged path include/asm-generic/barrier.h
* Unmerged path include/linux/sched.h
diff --git a/kernel/futex.c b/kernel/futex.c
index 80b39fa689a3..03d0a786ca14 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -2081,7 +2081,7 @@ static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
 {
 	/*
 	 * The task state is guaranteed to be set before another task can
-	 * wake it. set_current_state() is implemented using set_mb() and
+	 * wake it. set_current_state() is implemented using smp_store_mb() and
 	 * queue_me() calls spin_unlock() upon completion, both serializing
 	 * access to the hash list and forcing another memory barrier.
 	 */
* Unmerged path kernel/locking/qspinlock_paravirt.h
diff --git a/kernel/wait.c b/kernel/wait.c
index 7034637c982d..15c1d0892c71 100644
--- a/kernel/wait.c
+++ b/kernel/wait.c
@@ -229,7 +229,7 @@ long wait_woken(wait_queue_t *wait, unsigned mode, long timeout)
 	 * condition being true _OR_ WQ_FLAG_WOKEN such that we will not miss
 	 * an event.
 	 */
-	set_mb(wait->flags, wait->flags & ~WQ_FLAG_WOKEN); /* B */
+	smp_store_mb(wait->flags, wait->flags & ~WQ_FLAG_WOKEN); /* B */
 
 	return timeout;
 }
@@ -242,7 +242,7 @@ int woken_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)
 	 * doesn't imply write barrier and the users expects write
 	 * barrier semantics on wakeup functions.  The following
 	 * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
-	 * and is paired with set_mb() in wait_woken().
+	 * and is paired with smp_store_mb() in wait_woken().
 	 */
 	smp_wmb(); /* C */
 	wait->flags |= WQ_FLAG_WOKEN;
