x86/kvm/mmu: make space for source data caching in struct kvm_mmu

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm/mmu: make space for source data caching in struct kvm_mmu (Vitaly Kuznetsov) [1565739 1497611]
Rebuild_FUZZ: 96.83%
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 36d9594dfbf22a59adb986d85e0543886ab898f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/36d9594d.failed

In preparation to MMU reconfiguration avoidance we need a space to
cache source data. As this partially intersects with kvm_mmu_page_role,
create 64bit sized union kvm_mmu_role holding both base and extended data.
No functional change.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 36d9594dfbf22a59adb986d85e0543886ab898f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/mmu.c
index e7052d2c946d,3301973527aa..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2292,9 -2372,10 +2292,13 @@@ static struct kvm_mmu_page *kvm_mmu_get
  	struct kvm_mmu_page *sp;
  	bool need_sync = false;
  	bool flush = false;
 -	int collisions = 0;
  	LIST_HEAD(invalid_list);
  
++<<<<<<< HEAD
 +	role = vcpu->arch.mmu.base_role;
++=======
+ 	role = vcpu->arch.mmu->mmu_role.base;
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  	role.level = level;
  	role.direct = direct;
  	if (role.direct)
@@@ -4420,14 -4724,27 +4425,19 @@@ static void paging32E_init_context(stru
  	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
  }
  
 -static union kvm_mmu_page_role
 -kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu)
 -{
 -	union kvm_mmu_page_role role = {0};
 -
 -	role.guest_mode = is_guest_mode(vcpu);
 -	role.smm = is_smm(vcpu);
 -	role.ad_disabled = (shadow_accessed_mask == 0);
 -	role.level = kvm_x86_ops->get_tdp_level(vcpu);
 -	role.direct = true;
 -	role.access = ACC_ALL;
 -
 -	return role;
 -}
 -
  static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_mmu *context = vcpu->arch.mmu;
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
  
++<<<<<<< HEAD
 +	context->base_role.word = 0;
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
 +	context->base_role.ad_disabled = (shadow_accessed_mask == 0);
++=======
+ 	context->mmu_role.base.word = mmu_base_role_mask.word &
+ 				  kvm_calc_tdp_mmu_root_page_role(vcpu).word;
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  	context->page_fault = tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
@@@ -4484,15 -4824,8 +4494,20 @@@ void kvm_init_shadow_mmu(struct kvm_vcp
  	else
  		paging32_init_context(vcpu, context);
  
++<<<<<<< HEAD
 +	context->base_role.nxe = is_nx(vcpu);
 +	context->base_role.cr4_pae = !!is_pae(vcpu);
 +	context->base_role.cr0_wp  = is_write_protection(vcpu);
 +	context->base_role.smep_andnot_wp
 +		= smep && !is_write_protection(vcpu);
 +	context->base_role.smap_andnot_wp
 +		= smap && !is_write_protection(vcpu);
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
++=======
+ 	context->mmu_role.base.word = mmu_base_role_mask.word &
+ 				  kvm_calc_shadow_mmu_root_page_role(vcpu).word;
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  	reset_shadow_zero_bits_mask(vcpu, context);
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
@@@ -4513,11 -4864,11 +4528,17 @@@ void kvm_init_shadow_ept_mmu(struct kvm
  	context->sync_page = ept_sync_page;
  	context->invlpg = ept_invlpg;
  	context->update_pte = ept_update_pte;
 -	context->root_level = PT64_ROOT_4LEVEL;
 +	context->root_level = context->shadow_root_level;
 +	context->root_hpa = INVALID_PAGE;
  	context->direct_map = false;
++<<<<<<< HEAD
 +	context->base_role.ad_disabled = !accessed_dirty;
 +	context->base_role.guest_mode = 1;
++=======
+ 	context->mmu_role.base.word =
+ 		root_page_role.word & mmu_base_role_mask.word;
+ 
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  	update_permission_bitmask(vcpu, context, true);
  	update_pkru_bitmask(vcpu, context, true);
  	update_last_nonleaf_level(vcpu, context);
@@@ -4819,8 -5186,8 +4842,13 @@@ static void kvm_mmu_pte_write(struct kv
  			entry = *spte;
  			mmu_page_zap_pte(vcpu->kvm, sp, spte);
  			if (gentry &&
++<<<<<<< HEAD
 +			      !((sp->role.word ^ vcpu->arch.mmu.base_role.word)
 +			      & mask.word) && rmap_can_add(vcpu))
++=======
+ 			      !((sp->role.word ^ base_role)
+ 			      & mmu_base_role_mask.word) && rmap_can_add(vcpu))
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  				mmu_pte_write_new_pte(vcpu, sp, spte, &gentry);
  			if (need_remote_flush(entry, *spte))
  				remote_flush = true;
@@@ -5420,7 -5881,19 +5448,23 @@@ static void mmu_destroy_caches(void
  
  int kvm_mmu_module_init(void)
  {
++<<<<<<< HEAD
 +	kvm_mmu_clear_all_pte_masks();
++=======
+ 	int ret = -ENOMEM;
+ 
+ 	/*
+ 	 * MMU roles use union aliasing which is, generally speaking, an
+ 	 * undefined behavior. However, we supposedly know how compilers behave
+ 	 * and the current status quo is unlikely to change. Guardians below are
+ 	 * supposed to let us know if the assumption becomes false.
+ 	 */
+ 	BUILD_BUG_ON(sizeof(union kvm_mmu_page_role) != sizeof(u32));
+ 	BUILD_BUG_ON(sizeof(union kvm_mmu_extended_role) != sizeof(u32));
+ 	BUILD_BUG_ON(sizeof(union kvm_mmu_role) != sizeof(u64));
+ 
+ 	kvm_mmu_reset_all_pte_masks();
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  
  	pte_list_desc_cache = kmem_cache_create("pte_list_desc",
  					    sizeof(struct pte_list_desc),
diff --cc arch/x86/kvm/vmx.c
index 5501a1957de2,6f44d3a63434..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -7931,7 -9190,137 +7931,141 @@@ static int handle_pml_full(struct kvm_v
  
  static int handle_preemption_timer(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	kvm_lapic_expired_hv_timer(vcpu);
++=======
+ 	if (!to_vmx(vcpu)->req_immediate_exit)
+ 		kvm_lapic_expired_hv_timer(vcpu);
+ 	return 1;
+ }
+ 
+ static bool valid_ept_address(struct kvm_vcpu *vcpu, u64 address)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	int maxphyaddr = cpuid_maxphyaddr(vcpu);
+ 
+ 	/* Check for memory type validity */
+ 	switch (address & VMX_EPTP_MT_MASK) {
+ 	case VMX_EPTP_MT_UC:
+ 		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT))
+ 			return false;
+ 		break;
+ 	case VMX_EPTP_MT_WB:
+ 		if (!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT))
+ 			return false;
+ 		break;
+ 	default:
+ 		return false;
+ 	}
+ 
+ 	/* only 4 levels page-walk length are valid */
+ 	if ((address & VMX_EPTP_PWL_MASK) != VMX_EPTP_PWL_4)
+ 		return false;
+ 
+ 	/* Reserved bits should not be set */
+ 	if (address >> maxphyaddr || ((address >> 7) & 0x1f))
+ 		return false;
+ 
+ 	/* AD, if set, should be supported */
+ 	if (address & VMX_EPTP_AD_ENABLE_BIT) {
+ 		if (!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT))
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
+ 				     struct vmcs12 *vmcs12)
+ {
+ 	u32 index = vcpu->arch.regs[VCPU_REGS_RCX];
+ 	u64 address;
+ 	bool accessed_dirty;
+ 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
+ 
+ 	if (!nested_cpu_has_eptp_switching(vmcs12) ||
+ 	    !nested_cpu_has_ept(vmcs12))
+ 		return 1;
+ 
+ 	if (index >= VMFUNC_EPTP_ENTRIES)
+ 		return 1;
+ 
+ 
+ 	if (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,
+ 				     &address, index * 8, 8))
+ 		return 1;
+ 
+ 	accessed_dirty = !!(address & VMX_EPTP_AD_ENABLE_BIT);
+ 
+ 	/*
+ 	 * If the (L2) guest does a vmfunc to the currently
+ 	 * active ept pointer, we don't have to do anything else
+ 	 */
+ 	if (vmcs12->ept_pointer != address) {
+ 		if (!valid_ept_address(vcpu, address))
+ 			return 1;
+ 
+ 		kvm_mmu_unload(vcpu);
+ 		mmu->ept_ad = accessed_dirty;
+ 		mmu->mmu_role.base.ad_disabled = !accessed_dirty;
+ 		vmcs12->ept_pointer = address;
+ 		/*
+ 		 * TODO: Check what's the correct approach in case
+ 		 * mmu reload fails. Currently, we just let the next
+ 		 * reload potentially fail
+ 		 */
+ 		kvm_mmu_reload(vcpu);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int handle_vmfunc(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	struct vmcs12 *vmcs12;
+ 	u32 function = vcpu->arch.regs[VCPU_REGS_RAX];
+ 
+ 	/*
+ 	 * VMFUNC is only supported for nested guests, but we always enable the
+ 	 * secondary control for simplicity; for non-nested mode, fake that we
+ 	 * didn't by injecting #UD.
+ 	 */
+ 	if (!is_guest_mode(vcpu)) {
+ 		kvm_queue_exception(vcpu, UD_VECTOR);
+ 		return 1;
+ 	}
+ 
+ 	vmcs12 = get_vmcs12(vcpu);
+ 	if ((vmcs12->vm_function_control & (1 << function)) == 0)
+ 		goto fail;
+ 
+ 	switch (function) {
+ 	case 0:
+ 		if (nested_vmx_eptp_switching(vcpu, vmcs12))
+ 			goto fail;
+ 		break;
+ 	default:
+ 		goto fail;
+ 	}
+ 	return kvm_skip_emulated_instruction(vcpu);
+ 
+ fail:
+ 	nested_vmx_vmexit(vcpu, vmx->exit_reason,
+ 			  vmcs_read32(VM_EXIT_INTR_INFO),
+ 			  vmcs_readl(EXIT_QUALIFICATION));
+ 	return 1;
+ }
+ 
+ static int handle_encls(struct kvm_vcpu *vcpu)
+ {
+ 	/*
+ 	 * SGX virtualization is not yet supported.  There is no software
+ 	 * enable bit for SGX, so we have to trap ENCLS and inject a #UD
+ 	 * to prevent the guest from executing ENCLS.
+ 	 */
+ 	kvm_queue_exception(vcpu, UD_VECTOR);
++>>>>>>> 36d9594dfbf2 (x86/kvm/mmu: make space for source data caching in struct kvm_mmu)
  	return 1;
  }
  
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 86bec63d5f8a..da084e24b98a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -220,7 +220,7 @@ struct kvm_mmu_memory_cache {
  * @nxe, @cr0_wp, @smep_andnot_wp and @smap_andnot_wp.
  */
 union kvm_mmu_page_role {
-	unsigned word;
+	u32 word;
 	struct {
 		unsigned level:4;
 		unsigned cr4_pae:1;
@@ -246,6 +246,18 @@ union kvm_mmu_page_role {
 	};
 };
 
+union kvm_mmu_extended_role {
+	u32 word;
+};
+
+union kvm_mmu_role {
+	u64 as_u64;
+	struct {
+		union kvm_mmu_page_role base;
+		union kvm_mmu_extended_role ext;
+	};
+};
+
 struct kvm_rmap_head {
 	unsigned long val;
 };
@@ -321,7 +333,7 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
-	union kvm_mmu_page_role base_role;
+	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;
 	u8 ept_ad;
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/vmx.c
