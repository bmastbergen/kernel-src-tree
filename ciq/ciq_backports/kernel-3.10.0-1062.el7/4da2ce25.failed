mm: distinguish CMA and MOVABLE isolation in has_unmovable_pages()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] distinguish CMA and MOVABLE isolation in has_unmovable_pages() (Baoquan He) [1601867]
Rebuild_FUZZ: 96.88%
commit-author Michal Hocko <mhocko@suse.com>
commit 4da2ce250f986060750fcc5b29112914e31803ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/4da2ce25.failed

Joonsoo has noticed that "mm: drop migrate type checks from
has_unmovable_pages" would break CMA allocator because it relies on
has_unmovable_pages returning false even for CMA pageblocks which in
fact don't have to be movable:

 alloc_contig_range
   start_isolate_page_range
     set_migratetype_isolate
       has_unmovable_pages

This is a result of the code sharing between CMA and memory hotplug
while each one has a different idea of what has_unmovable_pages should
return.  This is unfortunate but fixing it properly would require a lot
of code duplication.

Fix the issue by introducing the requested migrate type argument and
special case MIGRATE_CMA case where CMA page blocks are handled
properly.  This will work for memory hotplug because it requires
MIGRATE_MOVABLE.

Link: http://lkml.kernel.org/r/20171019122118.y6cndierwl2vnguj@dhcp22.suse.cz
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Reported-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
	Tested-by: Ran Wang <ran.wang_1@nxp.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Igor Mammedov <imammedo@redhat.com>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
	Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
	Cc: Xishi Qiu <qiuxishi@huawei.com>
	Cc: Yasuaki Ishimatsu <yasu.isimatu@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4da2ce250f986060750fcc5b29112914e31803ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_isolation.c
diff --cc mm/page_isolation.c
index c1b1a182a88b,165ed8117bd1..000000000000
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@@ -7,9 -8,14 +7,16 @@@
  #include <linux/pageblock-flags.h>
  #include <linux/memory.h>
  #include <linux/hugetlb.h>
 -#include <linux/page_owner.h>
 -#include <linux/migrate.h>
  #include "internal.h"
  
++<<<<<<< HEAD
 +static int set_migratetype_isolate(struct page *page,
++=======
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/page_isolation.h>
+ 
+ static int set_migratetype_isolate(struct page *page, int migratetype,
++>>>>>>> 4da2ce250f98 (mm: distinguish CMA and MOVABLE isolation in has_unmovable_pages())
  				bool skip_hwpoisoned_pages)
  {
  	struct zone *zone;
@@@ -58,12 -64,14 +65,12 @@@
  out:
  	if (!ret) {
  		unsigned long nr_pages;
- 		int migratetype = get_pageblock_migratetype(page);
+ 		int mt = get_pageblock_migratetype(page);
  
  		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
 -		zone->nr_isolate_pageblock++;
 -		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE,
 -									NULL);
 +		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
  
- 		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
+ 		__mod_zone_freepage_state(zone, -nr_pages, mt);
  	}
  
  	spin_unlock_irqrestore(&zone->lock, flags);
diff --git a/include/linux/page-isolation.h b/include/linux/page-isolation.h
index d25739cc9418..afdcc90c8e2e 100644
--- a/include/linux/page-isolation.h
+++ b/include/linux/page-isolation.h
@@ -22,7 +22,7 @@ static inline bool is_migrate_isolate(int migratetype)
 #endif
 
 bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
-			 bool skip_hwpoisoned_pages);
+			 int migratetype, bool skip_hwpoisoned_pages);
 void set_pageblock_migratetype(struct page *page, int migratetype);
 int move_freepages_block(struct zone *zone, struct page *page,
 				int migratetype);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index cca37dc51e66..25c7fe6ae463 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -7108,6 +7108,7 @@ void set_pageblock_flags_group(struct page *page, unsigned long flags,
  * expect this function should be exact.
  */
 bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
+			 int migratetype,
 			 bool skip_hwpoisoned_pages)
 {
 	unsigned long pfn, iter, found;
@@ -7119,6 +7120,15 @@ bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
 	if (zone_idx(zone) == ZONE_MOVABLE)
 		return false;
 
+	/*
+	 * CMA allocations (alloc_contig_range) really need to mark isolate
+	 * CMA pageblocks even when they are not movable in fact so consider
+	 * them movable here.
+	 */
+	if (is_migrate_cma(migratetype) &&
+			is_migrate_cma(get_pageblock_migratetype(page)))
+		return false;
+
 	pfn = page_to_pfn(page);
 	for (found = 0, iter = 0; iter < pageblock_nr_pages; iter++) {
 		unsigned long check = pfn + iter;
@@ -7198,7 +7208,7 @@ bool is_pageblock_removable_nolock(struct page *page)
 	if (!zone_spans_pfn(zone, pfn))
 		return false;
 
-	return !has_unmovable_pages(zone, page, 0, true);
+	return !has_unmovable_pages(zone, page, 0, MIGRATE_MOVABLE, true);
 }
 
 #if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)
* Unmerged path mm/page_isolation.c
