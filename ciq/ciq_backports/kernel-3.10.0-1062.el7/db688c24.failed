vhost_net: use packet weight for rx handler, too

jira LE-1907
cve CVE-2019-3900
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Paolo Abeni <pabeni@redhat.com>
commit db688c24eada63b1efe6d0d7d835e5c3bdd71fd3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/db688c24.failed

Similar to commit a2ac99905f1e ("vhost-net: set packet weight of
tx polling to 2 * vq size"), we need a packet-based limit for
handler_rx, too - elsewhere, under rx flood with small packets,
tx can be delayed for a very long time, even without busypolling.

The pkt limit applied to handle_rx must be the same applied by
handle_tx, or we will get unfair scheduling between rx and tx.
Tying such limit to the queue length makes it less effective for
large queue length values and can introduce large process
scheduler latencies, so a constant valued is used - likewise
the existing bytes limit.

The selected limit has been validated with PVP[1] performance
test with different queue sizes:

queue size		256	512	1024

baseline		366	354	362
weight 128		715	723	670
weight 256		740	745	733
weight 512		600	460	583
weight 1024		423	427	418

A packet weight of 256 gives peek performances in under all the
tested scenarios.

No measurable regression in unidirectional performance tests has
been detected.

[1] https://developers.redhat.com/blog/2017/06/05/measuring-and-comparing-open-vswitch-performance/

	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Acked-by: Jason Wang <jasowang@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit db688c24eada63b1efe6d0d7d835e5c3bdd71fd3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/net.c
diff --cc drivers/vhost/net.c
index 90b124ca16a4,c4b49fca4871..000000000000
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@@ -716,8 -769,11 +718,14 @@@ static void handle_rx(struct vhost_net 
  	size_t vhost_hlen, sock_hlen;
  	size_t vhost_len, sock_len;
  	struct socket *sock;
++<<<<<<< HEAD
++=======
+ 	struct iov_iter fixup;
+ 	__virtio16 num_buffers;
+ 	int recv_pkts = 0;
++>>>>>>> db688c24eada (vhost_net: use packet weight for rx handler, too)
  
 -	mutex_lock_nested(&vq->mutex, 0);
 +	mutex_lock(&vq->mutex);
  	sock = vq->private_data;
  	if (!sock)
  		goto out;
@@@ -807,13 -875,17 +815,14 @@@
  		if (unlikely(vq_log))
  			vhost_log_write(vq, vq_log, log, vhost_len);
  		total_len += vhost_len;
- 		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
+ 		if (unlikely(total_len >= VHOST_NET_WEIGHT) ||
+ 		    unlikely(++recv_pkts >= VHOST_NET_PKT_WEIGHT)) {
  			vhost_poll_queue(&vq->poll);
 -			goto out;
 +			break;
  		}
  	}
 -	vhost_net_enable_vq(net, vq);
  out:
 -	if (nheads)
 -		vhost_add_used_and_signal_n(&net->dev, vq, vq->heads,
 -					    nheads);
 +	vhost_rx_signal_used(nvq);
  	mutex_unlock(&vq->mutex);
  }
  
* Unmerged path drivers/vhost/net.c
