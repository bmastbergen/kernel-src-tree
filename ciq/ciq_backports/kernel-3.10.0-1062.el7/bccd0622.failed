IB/uverbs: Add UVERBS_ATTR_FLAGS_IN to the specs language

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit bccd06223f21654eb268e153426a77deb117c1e8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/bccd0622.failed

This clearly indicates that the input is a bitwise combination of values
in an enum, and identifies which enum contains the definition of the bits.

Special accessors are provided that handle the mandatory validation of the
allowed bits and enforce the correct type for bitwise flags.

If we had introduced this at the start then the kabi would have uniformly
used u64 data to pass flags, however today there is a mixture of u64 and
u32 flags. All places are converted to accept both sizes and the accessor
fixes it. This allows all existing flags to grow to u64 in future without
any hassle.

Finally all flags are, by definition, optional. If flags are not passed
the accessor does not fail, but provides a value of zero.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit bccd06223f21654eb268e153426a77deb117c1e8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs_std_types_counters.c
#	drivers/infiniband/core/uverbs_std_types_cq.c
#	drivers/infiniband/core/uverbs_std_types_mr.c
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/main.c
#	include/rdma/uverbs_ioctl.h
diff --cc drivers/infiniband/core/uverbs_std_types_cq.c
index b0dbae9dd0d7,3179203a2dd7..000000000000
--- a/drivers/infiniband/core/uverbs_std_types_cq.c
+++ b/drivers/infiniband/core/uverbs_std_types_cq.c
@@@ -82,18 -84,18 +82,20 @@@ static int UVERBS_HANDLER(UVERBS_METHOD
  	if (ret)
  		return ret;
  
- 	/* Optional param, if it doesn't exist, we get -ENOENT and skip it */
- 	if (IS_UVERBS_COPY_ERR(uverbs_copy_from(&attr.flags, attrs,
- 						UVERBS_ATTR_CREATE_CQ_FLAGS)))
- 		return -EFAULT;
+ 	ret = uverbs_get_flags32(&attr.flags, attrs,
+ 				 UVERBS_ATTR_CREATE_CQ_FLAGS,
+ 				 IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION |
+ 					 IB_UVERBS_CQ_FLAGS_IGNORE_OVERRUN);
+ 	if (ret)
+ 		return ret;
  
 -	ev_file_uobj = uverbs_attr_get_uobject(attrs, UVERBS_ATTR_CREATE_CQ_COMP_CHANNEL);
 -	if (!IS_ERR(ev_file_uobj)) {
 +	ev_file_attr = uverbs_attr_get(attrs, UVERBS_ATTR_CREATE_CQ_COMP_CHANNEL);
 +	if (!IS_ERR(ev_file_attr)) {
 +		ev_file_uobj = ev_file_attr->obj_attr.uobject;
 +
  		ev_file = container_of(ev_file_uobj,
  				       struct ib_uverbs_completion_event_file,
 -				       uobj);
 +				       uobj_file.uobj);
  		uverbs_uobject_get(ev_file_uobj);
  	}
  
@@@ -146,25 -147,31 +148,50 @@@ err_event_file
  	return ret;
  };
  
++<<<<<<< HEAD
 +static DECLARE_UVERBS_NAMED_METHOD(UVERBS_METHOD_CQ_CREATE,
 +	&UVERBS_ATTR_IDR(UVERBS_ATTR_CREATE_CQ_HANDLE, UVERBS_OBJECT_CQ,
 +			 UVERBS_ACCESS_NEW,
 +			 UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_CQE,
++=======
+ DECLARE_UVERBS_NAMED_METHOD(
+ 	UVERBS_METHOD_CQ_CREATE,
+ 	UVERBS_ATTR_IDR(UVERBS_ATTR_CREATE_CQ_HANDLE,
+ 			UVERBS_OBJECT_CQ,
+ 			UVERBS_ACCESS_NEW,
+ 			UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_CQE,
+ 			   UVERBS_ATTR_TYPE(u32),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_USER_HANDLE,
+ 			   UVERBS_ATTR_TYPE(u64),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_FD(UVERBS_ATTR_CREATE_CQ_COMP_CHANNEL,
+ 		       UVERBS_OBJECT_COMP_CHANNEL,
+ 		       UVERBS_ACCESS_READ,
+ 		       UA_OPTIONAL),
+ 	UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_COMP_VECTOR,
+ 			   UVERBS_ATTR_TYPE(u32),
+ 			   UA_MANDATORY),
+ 	UVERBS_ATTR_FLAGS_IN(UVERBS_ATTR_CREATE_CQ_FLAGS,
+ 			     enum ib_uverbs_ex_create_cq_flags),
+ 	UVERBS_ATTR_PTR_OUT(UVERBS_ATTR_CREATE_CQ_RESP_CQE,
++>>>>>>> bccd06223f21 (IB/uverbs: Add UVERBS_ATTR_FLAGS_IN to the specs language)
  			    UVERBS_ATTR_TYPE(u32),
 -			    UA_MANDATORY),
 -	UVERBS_ATTR_UHW());
 +			    UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_USER_HANDLE,
 +			    UVERBS_ATTR_TYPE(u64),
 +			    UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&UVERBS_ATTR_FD(UVERBS_ATTR_CREATE_CQ_COMP_CHANNEL,
 +			UVERBS_OBJECT_COMP_CHANNEL,
 +			UVERBS_ACCESS_READ),
 +	&UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_COMP_VECTOR, UVERBS_ATTR_TYPE(u32),
 +			    UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&UVERBS_ATTR_PTR_IN(UVERBS_ATTR_CREATE_CQ_FLAGS, UVERBS_ATTR_TYPE(u32)),
 +	&UVERBS_ATTR_PTR_OUT(UVERBS_ATTR_CREATE_CQ_RESP_CQE, UVERBS_ATTR_TYPE(u32),
 +			     UA_FLAGS(UVERBS_ATTR_SPEC_F_MANDATORY)),
 +	&uverbs_uhw_compat_in, &uverbs_uhw_compat_out);
  
  static int UVERBS_HANDLER(UVERBS_METHOD_CQ_DESTROY)(struct ib_device *ib_dev,
  						    struct ib_uverbs_file *file,
diff --cc drivers/infiniband/hw/mlx5/main.c
index 1d3555c545d0,06d6309b719a..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3325,6 -3652,352 +3325,355 @@@ unlock
  	return ERR_PTR(err);
  }
  
++<<<<<<< HEAD
++=======
+ static struct mlx5_ib_flow_prio *_get_flow_table(struct mlx5_ib_dev *dev,
+ 						 int priority, bool mcast)
+ {
+ 	int max_table_size;
+ 	struct mlx5_flow_namespace *ns = NULL;
+ 	struct mlx5_ib_flow_prio *prio;
+ 
+ 	max_table_size = BIT(MLX5_CAP_FLOWTABLE_NIC_RX(dev->mdev,
+ 			     log_max_ft_size));
+ 	if (max_table_size < MLX5_FS_MAX_ENTRIES)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (mcast)
+ 		priority = MLX5_IB_FLOW_MCAST_PRIO;
+ 	else
+ 		priority = ib_prio_to_core_prio(priority, false);
+ 
+ 	ns = mlx5_get_flow_namespace(dev->mdev, MLX5_FLOW_NAMESPACE_BYPASS);
+ 	if (!ns)
+ 		return ERR_PTR(-ENOTSUPP);
+ 
+ 	prio = &dev->flow_db->prios[priority];
+ 
+ 	if (prio->flow_table)
+ 		return prio;
+ 
+ 	return _get_prio(ns, prio, priority, MLX5_FS_MAX_ENTRIES,
+ 			 MLX5_FS_MAX_TYPES);
+ }
+ 
+ static struct mlx5_ib_flow_handler *
+ _create_raw_flow_rule(struct mlx5_ib_dev *dev,
+ 		      struct mlx5_ib_flow_prio *ft_prio,
+ 		      struct mlx5_flow_destination *dst,
+ 		      struct mlx5_ib_flow_matcher  *fs_matcher,
+ 		      void *cmd_in, int inlen)
+ {
+ 	struct mlx5_ib_flow_handler *handler;
+ 	struct mlx5_flow_act flow_act = {.flow_tag = MLX5_FS_DEFAULT_FLOW_TAG};
+ 	struct mlx5_flow_spec *spec;
+ 	struct mlx5_flow_table *ft = ft_prio->flow_table;
+ 	int err = 0;
+ 
+ 	spec = kvzalloc(sizeof(*spec), GFP_KERNEL);
+ 	handler = kzalloc(sizeof(*handler), GFP_KERNEL);
+ 	if (!handler || !spec) {
+ 		err = -ENOMEM;
+ 		goto free;
+ 	}
+ 
+ 	INIT_LIST_HEAD(&handler->list);
+ 
+ 	memcpy(spec->match_value, cmd_in, inlen);
+ 	memcpy(spec->match_criteria, fs_matcher->matcher_mask.match_params,
+ 	       fs_matcher->mask_len);
+ 	spec->match_criteria_enable = fs_matcher->match_criteria_enable;
+ 
+ 	flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
+ 	handler->rule = mlx5_add_flow_rules(ft, spec,
+ 					    &flow_act, dst, 1);
+ 
+ 	if (IS_ERR(handler->rule)) {
+ 		err = PTR_ERR(handler->rule);
+ 		goto free;
+ 	}
+ 
+ 	ft_prio->refcount++;
+ 	handler->prio = ft_prio;
+ 	handler->dev = dev;
+ 	ft_prio->flow_table = ft;
+ 
+ free:
+ 	if (err)
+ 		kfree(handler);
+ 	kvfree(spec);
+ 	return err ? ERR_PTR(err) : handler;
+ }
+ 
+ static bool raw_fs_is_multicast(struct mlx5_ib_flow_matcher *fs_matcher,
+ 				void *match_v)
+ {
+ 	void *match_c;
+ 	void *match_v_set_lyr_2_4, *match_c_set_lyr_2_4;
+ 	void *dmac, *dmac_mask;
+ 	void *ipv4, *ipv4_mask;
+ 
+ 	if (!(fs_matcher->match_criteria_enable &
+ 	      (1 << MATCH_CRITERIA_ENABLE_OUTER_BIT)))
+ 		return false;
+ 
+ 	match_c = fs_matcher->matcher_mask.match_params;
+ 	match_v_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_v,
+ 					   outer_headers);
+ 	match_c_set_lyr_2_4 = MLX5_ADDR_OF(fte_match_param, match_c,
+ 					   outer_headers);
+ 
+ 	dmac = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dmac_47_16);
+ 	dmac_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dmac_47_16);
+ 
+ 	if (is_multicast_ether_addr(dmac) &&
+ 	    is_multicast_ether_addr(dmac_mask))
+ 		return true;
+ 
+ 	ipv4 = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_v_set_lyr_2_4,
+ 			    dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	ipv4_mask = MLX5_ADDR_OF(fte_match_set_lyr_2_4, match_c_set_lyr_2_4,
+ 				 dst_ipv4_dst_ipv6.ipv4_layout.ipv4);
+ 
+ 	if (ipv4_is_multicast(*(__be32 *)(ipv4)) &&
+ 	    ipv4_is_multicast(*(__be32 *)(ipv4_mask)))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ struct mlx5_ib_flow_handler *
+ mlx5_ib_raw_fs_rule_add(struct mlx5_ib_dev *dev,
+ 			struct mlx5_ib_flow_matcher *fs_matcher,
+ 			void *cmd_in, int inlen, int dest_id,
+ 			int dest_type)
+ {
+ 	struct mlx5_flow_destination *dst;
+ 	struct mlx5_ib_flow_prio *ft_prio;
+ 	int priority = fs_matcher->priority;
+ 	struct mlx5_ib_flow_handler *handler;
+ 	bool mcast;
+ 	int err;
+ 
+ 	if (fs_matcher->flow_type != MLX5_IB_FLOW_TYPE_NORMAL)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	if (fs_matcher->priority > MLX5_IB_FLOW_LAST_PRIO)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	dst = kzalloc(sizeof(*dst), GFP_KERNEL);
+ 	if (!dst)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mcast = raw_fs_is_multicast(fs_matcher, cmd_in);
+ 	mutex_lock(&dev->flow_db->lock);
+ 
+ 	ft_prio = _get_flow_table(dev, priority, mcast);
+ 	if (IS_ERR(ft_prio)) {
+ 		err = PTR_ERR(ft_prio);
+ 		goto unlock;
+ 	}
+ 
+ 	if (dest_type == MLX5_FLOW_DESTINATION_TYPE_TIR) {
+ 		dst->type = dest_type;
+ 		dst->tir_num = dest_id;
+ 	} else {
+ 		dst->type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE_NUM;
+ 		dst->ft_num = dest_id;
+ 	}
+ 
+ 	handler = _create_raw_flow_rule(dev, ft_prio, dst, fs_matcher, cmd_in,
+ 					inlen);
+ 
+ 	if (IS_ERR(handler)) {
+ 		err = PTR_ERR(handler);
+ 		goto destroy_ft;
+ 	}
+ 
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	atomic_inc(&fs_matcher->usecnt);
+ 	handler->flow_matcher = fs_matcher;
+ 
+ 	kfree(dst);
+ 
+ 	return handler;
+ 
+ destroy_ft:
+ 	put_flow_table(dev, ft_prio, false);
+ unlock:
+ 	mutex_unlock(&dev->flow_db->lock);
+ 	kfree(dst);
+ 
+ 	return ERR_PTR(err);
+ }
+ 
+ static u32 mlx5_ib_flow_action_flags_to_accel_xfrm_flags(u32 mlx5_flags)
+ {
+ 	u32 flags = 0;
+ 
+ 	if (mlx5_flags & MLX5_IB_UAPI_FLOW_ACTION_FLAGS_REQUIRE_METADATA)
+ 		flags |= MLX5_ACCEL_XFRM_FLAG_REQUIRE_METADATA;
+ 
+ 	return flags;
+ }
+ 
+ #define MLX5_FLOW_ACTION_ESP_CREATE_LAST_SUPPORTED	MLX5_IB_UAPI_FLOW_ACTION_FLAGS_REQUIRE_METADATA
+ static struct ib_flow_action *
+ mlx5_ib_create_flow_action_esp(struct ib_device *device,
+ 			       const struct ib_flow_action_attrs_esp *attr,
+ 			       struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_dev *mdev = to_mdev(device);
+ 	struct ib_uverbs_flow_action_esp_keymat_aes_gcm *aes_gcm;
+ 	struct mlx5_accel_esp_xfrm_attrs accel_attrs = {};
+ 	struct mlx5_ib_flow_action *action;
+ 	u64 action_flags;
+ 	u64 flags;
+ 	int err = 0;
+ 
+ 	err = uverbs_get_flags64(
+ 		&action_flags, attrs, MLX5_IB_ATTR_CREATE_FLOW_ACTION_FLAGS,
+ 		((MLX5_FLOW_ACTION_ESP_CREATE_LAST_SUPPORTED << 1) - 1));
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	flags = mlx5_ib_flow_action_flags_to_accel_xfrm_flags(action_flags);
+ 
+ 	/* We current only support a subset of the standard features. Only a
+ 	 * keymat of type AES_GCM, with icv_len == 16, iv_algo == SEQ and esn
+ 	 * (with overlap). Full offload mode isn't supported.
+ 	 */
+ 	if (!attr->keymat || attr->replay || attr->encap ||
+ 	    attr->spi || attr->seq || attr->tfc_pad ||
+ 	    attr->hard_limit_pkts ||
+ 	    (attr->flags & ~(IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			     IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ENCRYPT)))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	if (attr->keymat->protocol !=
+ 	    IB_UVERBS_FLOW_ACTION_ESP_KEYMAT_AES_GCM)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	aes_gcm = &attr->keymat->keymat.aes_gcm;
+ 
+ 	if (aes_gcm->icv_len != 16 ||
+ 	    aes_gcm->iv_algo != IB_UVERBS_FLOW_ACTION_IV_ALGO_SEQ)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	action = kmalloc(sizeof(*action), GFP_KERNEL);
+ 	if (!action)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	action->esp_aes_gcm.ib_flags = attr->flags;
+ 	memcpy(&accel_attrs.keymat.aes_gcm.aes_key, &aes_gcm->aes_key,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.aes_key));
+ 	accel_attrs.keymat.aes_gcm.key_len = aes_gcm->key_len * 8;
+ 	memcpy(&accel_attrs.keymat.aes_gcm.salt, &aes_gcm->salt,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.salt));
+ 	memcpy(&accel_attrs.keymat.aes_gcm.seq_iv, &aes_gcm->iv,
+ 	       sizeof(accel_attrs.keymat.aes_gcm.seq_iv));
+ 	accel_attrs.keymat.aes_gcm.icv_len = aes_gcm->icv_len * 8;
+ 	accel_attrs.keymat.aes_gcm.iv_algo = MLX5_ACCEL_ESP_AES_GCM_IV_ALGO_SEQ;
+ 	accel_attrs.keymat_type = MLX5_ACCEL_ESP_KEYMAT_AES_GCM;
+ 
+ 	accel_attrs.esn = attr->esn;
+ 	if (attr->flags & IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_TRIGGERED;
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ENCRYPT)
+ 		accel_attrs.action |= MLX5_ACCEL_ESP_ACTION_ENCRYPT;
+ 
+ 	action->esp_aes_gcm.ctx =
+ 		mlx5_accel_esp_create_xfrm(mdev->mdev, &accel_attrs, flags);
+ 	if (IS_ERR(action->esp_aes_gcm.ctx)) {
+ 		err = PTR_ERR(action->esp_aes_gcm.ctx);
+ 		goto err_parse;
+ 	}
+ 
+ 	action->esp_aes_gcm.ib_flags = attr->flags;
+ 
+ 	return &action->ib_action;
+ 
+ err_parse:
+ 	kfree(action);
+ 	return ERR_PTR(err);
+ }
+ 
+ static int
+ mlx5_ib_modify_flow_action_esp(struct ib_flow_action *action,
+ 			       const struct ib_flow_action_attrs_esp *attr,
+ 			       struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_flow_action *maction = to_mflow_act(action);
+ 	struct mlx5_accel_esp_xfrm_attrs accel_attrs;
+ 	int err = 0;
+ 
+ 	if (attr->keymat || attr->replay || attr->encap ||
+ 	    attr->spi || attr->seq || attr->tfc_pad ||
+ 	    attr->hard_limit_pkts ||
+ 	    (attr->flags & ~(IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			     IB_FLOW_ACTION_ESP_FLAGS_MOD_ESP_ATTRS |
+ 			     IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)))
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Only the ESN value or the MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP can
+ 	 * be modified.
+ 	 */
+ 	if (!(maction->esp_aes_gcm.ib_flags &
+ 	      IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED) &&
+ 	    attr->flags & (IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED |
+ 			   IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW))
+ 		return -EINVAL;
+ 
+ 	memcpy(&accel_attrs, &maction->esp_aes_gcm.ctx->attrs,
+ 	       sizeof(accel_attrs));
+ 
+ 	accel_attrs.esn = attr->esn;
+ 	if (attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW)
+ 		accel_attrs.flags |= MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 	else
+ 		accel_attrs.flags &= ~MLX5_ACCEL_ESP_FLAGS_ESN_STATE_OVERLAP;
+ 
+ 	err = mlx5_accel_esp_modify_xfrm(maction->esp_aes_gcm.ctx,
+ 					 &accel_attrs);
+ 	if (err)
+ 		return err;
+ 
+ 	maction->esp_aes_gcm.ib_flags &=
+ 		~IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW;
+ 	maction->esp_aes_gcm.ib_flags |=
+ 		attr->flags & IB_UVERBS_FLOW_ACTION_ESP_FLAGS_ESN_NEW_WINDOW;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5_ib_destroy_flow_action(struct ib_flow_action *action)
+ {
+ 	struct mlx5_ib_flow_action *maction = to_mflow_act(action);
+ 
+ 	switch (action->type) {
+ 	case IB_FLOW_ACTION_ESP:
+ 		/*
+ 		 * We only support aes_gcm by now, so we implicitly know this is
+ 		 * the underline crypto.
+ 		 */
+ 		mlx5_accel_esp_destroy_xfrm(maction->esp_aes_gcm.ctx);
+ 		break;
+ 	default:
+ 		WARN_ON(true);
+ 		break;
+ 	}
+ 
+ 	kfree(maction);
+ 	return 0;
+ }
+ 
++>>>>>>> bccd06223f21 (IB/uverbs: Add UVERBS_ATTR_FLAGS_IN to the specs language)
  static int mlx5_ib_mcg_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
  	struct mlx5_ib_dev *dev = to_mdev(ibqp->device);
@@@ -4776,7 -5515,128 +5125,132 @@@ static void mlx5_ib_cleanup_multiport_m
  	mlx5_nic_vport_disable_roce(dev->mdev);
  }
  
++<<<<<<< HEAD
 +static void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++=======
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_dm,
+ 	UVERBS_OBJECT_DM,
+ 	UVERBS_METHOD_DM_ALLOC,
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,
+ 			    UVERBS_ATTR_TYPE(u64),
+ 			    UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_PAGE_INDEX,
+ 			    UVERBS_ATTR_TYPE(u16),
+ 			    UA_MANDATORY));
+ 
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_flow_action,
+ 	UVERBS_OBJECT_FLOW_ACTION,
+ 	UVERBS_METHOD_FLOW_ACTION_ESP_CREATE,
+ 	UVERBS_ATTR_FLAGS_IN(MLX5_IB_ATTR_CREATE_FLOW_ACTION_FLAGS,
+ 			     enum mlx5_ib_uapi_flow_action_flags));
+ 
+ #define NUM_TREES	5
+ static int populate_specs_root(struct mlx5_ib_dev *dev)
+ {
+ 	const struct uverbs_object_tree_def *default_root[NUM_TREES + 1] = {
+ 		uverbs_default_get_objects()};
+ 	size_t num_trees = 1;
+ 
+ 	if (mlx5_accel_ipsec_device_caps(dev->mdev) & MLX5_ACCEL_IPSEC_CAP_DEVICE &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = &mlx5_ib_flow_action;
+ 
+ 	if (MLX5_CAP_DEV_MEM(dev->mdev, memic) &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = &mlx5_ib_dm;
+ 
+ 	if (MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &
+ 			    MLX5_GENERAL_OBJ_TYPES_CAP_UCTX &&
+ 	    !WARN_ON(num_trees >= ARRAY_SIZE(default_root)))
+ 		default_root[num_trees++] = mlx5_ib_get_devx_tree();
+ 
+ 	num_trees += mlx5_ib_get_flow_trees(default_root + num_trees);
+ 
+ 	dev->ib_dev.driver_specs_root =
+ 		uverbs_alloc_spec_tree(num_trees, default_root);
+ 
+ 	return PTR_ERR_OR_ZERO(dev->ib_dev.driver_specs_root);
+ }
+ 
+ static void depopulate_specs_root(struct mlx5_ib_dev *dev)
+ {
+ 	uverbs_free_spec_tree(dev->ib_dev.driver_specs_root);
+ }
+ 
+ static int mlx5_ib_read_counters(struct ib_counters *counters,
+ 				 struct ib_counters_read_attr *read_attr,
+ 				 struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 	struct mlx5_read_counters_attr mread_attr = {};
+ 	struct mlx5_ib_flow_counters_desc *desc;
+ 	int ret, i;
+ 
+ 	mutex_lock(&mcounters->mcntrs_mutex);
+ 	if (mcounters->cntrs_max_index > read_attr->ncounters) {
+ 		ret = -EINVAL;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.out = kcalloc(mcounters->counters_num, sizeof(u64),
+ 				 GFP_KERNEL);
+ 	if (!mread_attr.out) {
+ 		ret = -ENOMEM;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.hw_cntrs_hndl = mcounters->hw_cntrs_hndl;
+ 	mread_attr.flags = read_attr->flags;
+ 	ret = mcounters->read_counters(counters->device, &mread_attr);
+ 	if (ret)
+ 		goto err_read;
+ 
+ 	/* do the pass over the counters data array to assign according to the
+ 	 * descriptions and indexing pairs
+ 	 */
+ 	desc = mcounters->counters_data;
+ 	for (i = 0; i < mcounters->ncounters; i++)
+ 		read_attr->counters_buff[desc[i].index] += mread_attr.out[desc[i].description];
+ 
+ err_read:
+ 	kfree(mread_attr.out);
+ err_bound:
+ 	mutex_unlock(&mcounters->mcntrs_mutex);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_destroy_counters(struct ib_counters *counters)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 
+ 	counters_clear_description(counters);
+ 	if (mcounters->hw_cntrs_hndl)
+ 		mlx5_fc_destroy(to_mdev(counters->device)->mdev,
+ 				mcounters->hw_cntrs_hndl);
+ 
+ 	kfree(mcounters);
+ 
+ 	return 0;
+ }
+ 
+ static struct ib_counters *mlx5_ib_create_counters(struct ib_device *device,
+ 						   struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters;
+ 
+ 	mcounters = kzalloc(sizeof(*mcounters), GFP_KERNEL);
+ 	if (!mcounters)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mutex_init(&mcounters->mcntrs_mutex);
+ 
+ 	return &mcounters->ibcntrs;
+ }
+ 
+ void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++>>>>>>> bccd06223f21 (IB/uverbs: Add UVERBS_ATTR_FLAGS_IN to the specs language)
  {
  	mlx5_ib_cleanup_multiport_master(dev);
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
diff --cc include/rdma/uverbs_ioctl.h
index 2cc45699a237,5e6d0569d97c..000000000000
--- a/include/rdma/uverbs_ioctl.h
+++ b/include/rdma/uverbs_ioctl.h
@@@ -192,132 -187,127 +192,151 @@@ struct uverbs_object_tree_def 
  	const struct uverbs_object_def * const (*objects)[];
  };
  
 -/*
 - * =======================================
 - *	Attribute Specifications
 - * =======================================
 - */
 -
 -#define UVERBS_ATTR_SIZE(_min_len, _len)			\
 -	.u.ptr.min_len = _min_len, .u.ptr.len = _len
 -
 -#define UVERBS_ATTR_NO_DATA() UVERBS_ATTR_SIZE(0, 0)
 +#define UA_FLAGS(_flags)  .flags = _flags
 +#define __UVERBS_ATTR0(_id, _type, _fld, _attr, ...)              \
 +	((const struct uverbs_attr_def)				  \
 +	 {.id = _id, .attr = {{._fld = {.type = _type, _attr, .flags = 0, } }, } })
 +#define __UVERBS_ATTR1(_id, _type, _fld, _attr, _extra1, ...)      \
 +	((const struct uverbs_attr_def)				  \
 +	 {.id = _id, .attr = {{._fld = {.type = _type, _attr, _extra1 } },} })
 +#define __UVERBS_ATTR2(_id, _type, _fld, _attr, _extra1, _extra2)    \
 +	((const struct uverbs_attr_def)				  \
 +	 {.id = _id, .attr = {{._fld = {.type = _type, _attr, _extra1, _extra2 } },} })
 +#define __UVERBS_ATTR(_id, _type, _fld, _attr, _extra1, _extra2, _n, ...)	\
 +	__UVERBS_ATTR##_n(_id, _type, _fld, _attr, _extra1, _extra2)
  
 -/*
 - * Specifies a uapi structure that cannot be extended. The user must always
 - * supply the whole structure and nothing more. The structure must be declared
 - * in a header under include/uapi/rdma.
 - */
  #define UVERBS_ATTR_TYPE(_type)					\
 -	.u.ptr.min_len = sizeof(_type), .u.ptr.len = sizeof(_type)
 -/*
 - * Specifies a uapi structure where the user must provide at least up to
 - * member 'last'.  Anything after last and up until the end of the structure
 - * can be non-zero, anything longer than the end of the structure must be
 - * zero. The structure must be declared in a header under include/uapi/rdma.
 - */
 -#define UVERBS_ATTR_STRUCT(_type, _last)                                       \
 -	.zero_trailing = 1,                                                    \
 -	UVERBS_ATTR_SIZE(((uintptr_t)(&((_type *)0)->_last + 1)),              \
 -			 sizeof(_type))
 -/*
 - * Specifies at least min_len bytes must be passed in, but the amount can be
 - * larger, up to the protocol maximum size. No check for zeroing is done.
 - */
 -#define UVERBS_ATTR_MIN_SIZE(_min_len) UVERBS_ATTR_SIZE(_min_len, USHRT_MAX)
 -
 -/* Must be used in the '...' of any UVERBS_ATTR */
 -#define UA_ALLOC_AND_COPY .alloc_and_copy = 1
 -#define UA_MANDATORY .mandatory = 1
 -#define UA_OPTIONAL .mandatory = 0
 -
 -#define UVERBS_ATTR_IDR(_attr_id, _idr_type, _access, ...)                     \
 -	(&(const struct uverbs_attr_def){                                      \
 -		.id = _attr_id,                                                \
 -		.attr = { .type = UVERBS_ATTR_TYPE_IDR,                        \
 -			  .u.obj.obj_type = _idr_type,                         \
 -			  .u.obj.access = _access,                             \
 -			  __VA_ARGS__ } })
 -
 -#define UVERBS_ATTR_FD(_attr_id, _fd_type, _access, ...)                       \
 -	(&(const struct uverbs_attr_def){                                      \
 -		.id = (_attr_id) +                                             \
 -		      BUILD_BUG_ON_ZERO((_access) != UVERBS_ACCESS_NEW &&      \
 -					(_access) != UVERBS_ACCESS_READ),      \
 -		.attr = { .type = UVERBS_ATTR_TYPE_FD,                         \
 -			  .u.obj.obj_type = _fd_type,                          \
 -			  .u.obj.access = _access,                             \
 -			  __VA_ARGS__ } })
 -
 -#define UVERBS_ATTR_PTR_IN(_attr_id, _type, ...)                               \
 -	(&(const struct uverbs_attr_def){                                      \
 -		.id = _attr_id,                                                \
 -		.attr = { .type = UVERBS_ATTR_TYPE_PTR_IN,                     \
 -			  _type,                                               \
 -			  __VA_ARGS__ } })
 -
 -#define UVERBS_ATTR_PTR_OUT(_attr_id, _type, ...)                              \
 -	(&(const struct uverbs_attr_def){                                      \
 -		.id = _attr_id,                                                \
 -		.attr = { .type = UVERBS_ATTR_TYPE_PTR_OUT,                    \
 -			  _type,                                               \
 -			  __VA_ARGS__ } })
 -
 -/* _enum_arry should be a 'static const union uverbs_attr_spec[]' */
 -#define UVERBS_ATTR_ENUM_IN(_attr_id, _enum_arr, ...)                          \
 -	(&(const struct uverbs_attr_def){                                      \
 -		.id = _attr_id,                                                \
 -		.attr = { .type = UVERBS_ATTR_TYPE_ENUM_IN,                    \
 -			  .u2.enum_def.ids = _enum_arr,                        \
 -			  .u.enum_def.num_elems = ARRAY_SIZE(_enum_arr),       \
 -			  __VA_ARGS__ },                                       \
 -	})
 +	.min_len = sizeof(_type), .len = sizeof(_type)
 +#define UVERBS_ATTR_STRUCT(_type, _last)			\
 +	.min_len = ((uintptr_t)(&((_type *)0)->_last + 1)), .len = sizeof(_type)
 +#define UVERBS_ATTR_SIZE(_min_len, _len)			\
 +	.min_len = _min_len, .len = _len
 +#define UVERBS_ATTR_MIN_SIZE(_min_len)				\
 +	UVERBS_ATTR_SIZE(_min_len, USHRT_MAX)
  
  /*
++<<<<<<< HEAD
 + * In new compiler, UVERBS_ATTR could be simplified by declaring it as
 + * [_id] = {.type = _type, .len = _len, ##__VA_ARGS__}
 + * But since we support older compilers too, we need the more complex code.
++=======
+  * An input value that is a bitwise combination of values of _enum_type.
+  * This permits the flag value to be passed as either a u32 or u64, it must
+  * be retrieved via uverbs_get_flag().
+  */
+ #define UVERBS_ATTR_FLAGS_IN(_attr_id, _enum_type, ...)                        \
+ 	UVERBS_ATTR_PTR_IN(                                                    \
+ 		_attr_id,                                                      \
+ 		UVERBS_ATTR_SIZE(sizeof(u32) + BUILD_BUG_ON_ZERO(              \
+ 						       !sizeof(_enum_type *)), \
+ 				 sizeof(u64)),                                 \
+ 		__VA_ARGS__)
+ 
+ /*
+  * This spec is used in order to pass information to the hardware driver in a
+  * legacy way. Every verb that could get driver specific data should get this
+  * spec.
++>>>>>>> bccd06223f21 (IB/uverbs: Add UVERBS_ATTR_FLAGS_IN to the specs language)
   */
 -#define UVERBS_ATTR_UHW()                                                      \
 -	UVERBS_ATTR_PTR_IN(UVERBS_ATTR_UHW_IN,                                 \
 -			   UVERBS_ATTR_MIN_SIZE(0),			       \
 -			   UA_OPTIONAL),				       \
 -	UVERBS_ATTR_PTR_OUT(UVERBS_ATTR_UHW_OUT,                               \
 -			    UVERBS_ATTR_MIN_SIZE(0),			       \
 -			    UA_OPTIONAL)
 +#define UVERBS_ATTR(_id, _type, _fld, _attr, ...)			\
 +	__UVERBS_ATTR(_id, _type, _fld, _attr, ##__VA_ARGS__, 2, 1, 0)
 +#define UVERBS_ATTR_PTR_IN_SZ(_id, _len, ...)				\
 +	UVERBS_ATTR(_id, UVERBS_ATTR_TYPE_PTR_IN, ptr, _len, ##__VA_ARGS__)
 +/* If sizeof(_type) <= sizeof(u64), this will be inlined rather than a pointer */
 +#define UVERBS_ATTR_PTR_IN(_id, _type, ...)				\
 +	UVERBS_ATTR_PTR_IN_SZ(_id, _type, ##__VA_ARGS__)
 +#define UVERBS_ATTR_PTR_OUT_SZ(_id, _len, ...)				\
 +	UVERBS_ATTR(_id, UVERBS_ATTR_TYPE_PTR_OUT, ptr, _len, ##__VA_ARGS__)
 +#define UVERBS_ATTR_PTR_OUT(_id, _type, ...)				\
 +	UVERBS_ATTR_PTR_OUT_SZ(_id, _type, ##__VA_ARGS__)
 +#define UVERBS_ATTR_ENUM_IN(_id, _enum_arr, ...)			\
 +	UVERBS_ATTR(_id, UVERBS_ATTR_TYPE_ENUM_IN, enum_def,		\
 +		    .ids = (_enum_arr),					\
 +		    .num_elems = ARRAY_SIZE(_enum_arr), ##__VA_ARGS__)
  
  /*
 - * =======================================
 - *	Declaration helpers
 - * =======================================
 + * In new compiler, UVERBS_ATTR_IDR (and FD) could be simplified by declaring
 + * it as
 + * {.id = _id,								\
 + *  .attr {.type = __obj_class,						\
 + *         .obj = {.obj_type = _idr_type,				\
 + *                       .access = _access                              \
 + *                }, ##__VA_ARGS__ } }
 + * But since we support older compilers too, we need the more complex code.
   */
 -
 -#define DECLARE_UVERBS_OBJECT_TREE(_name, ...)                                 \
 -	static const struct uverbs_object_def *const _name##_ptr[] = {         \
 -		__VA_ARGS__,                                                   \
 -	};                                                                     \
 -	static const struct uverbs_object_tree_def _name = {                   \
 -		.num_objects = ARRAY_SIZE(_name##_ptr),                        \
 -		.objects = &_name##_ptr,                                       \
 +#define ___UVERBS_ATTR_OBJ0(_id, _obj_class, _obj_type, _access, ...)\
 +	((const struct uverbs_attr_def)					\
 +	{.id = _id,							\
 +	 .attr = { {.obj = {.type = _obj_class, .obj_type = _obj_type,	\
 +			    .access = _access, .flags = 0 } }, } })
 +#define ___UVERBS_ATTR_OBJ1(_id, _obj_class, _obj_type, _access, _flags)\
 +	((const struct uverbs_attr_def)					\
 +	{.id = _id,							\
 +	.attr = { {.obj = {.type = _obj_class, .obj_type = _obj_type,	\
 +			   .access = _access, _flags} }, } })
 +#define ___UVERBS_ATTR_OBJ(_id, _obj_class, _obj_type, _access, _flags, \
 +			   _n, ...)					\
 +	___UVERBS_ATTR_OBJ##_n(_id, _obj_class, _obj_type, _access, _flags)
 +#define __UVERBS_ATTR_OBJ(_id, _obj_class, _obj_type, _access, ...)	\
 +	___UVERBS_ATTR_OBJ(_id, _obj_class, _obj_type, _access,		\
 +			   ##__VA_ARGS__, 1, 0)
 +#define UVERBS_ATTR_IDR(_id, _idr_type, _access, ...)			 \
 +	__UVERBS_ATTR_OBJ(_id, UVERBS_ATTR_TYPE_IDR, _idr_type, _access,\
 +			  ##__VA_ARGS__)
 +#define UVERBS_ATTR_FD(_id, _fd_type, _access, ...)			\
 +	__UVERBS_ATTR_OBJ(_id, UVERBS_ATTR_TYPE_FD, _fd_type,		\
 +			  (_access) + BUILD_BUG_ON_ZERO(		\
 +				(_access) != UVERBS_ACCESS_NEW &&	\
 +				(_access) != UVERBS_ACCESS_READ),	\
 +			  ##__VA_ARGS__)
 +#define DECLARE_UVERBS_ATTR_SPEC(_name, ...)				\
 +	const struct uverbs_attr_def _name = __VA_ARGS__
 +
 +#define DECLARE_UVERBS_ENUM(_name, ...)					\
 +	const struct uverbs_enum_spec _name = {				\
 +		.len = ARRAY_SIZE(((struct uverbs_attr_spec[]){__VA_ARGS__})),\
 +		.ids = {__VA_ARGS__},					\
  	}
 +#define _UVERBS_METHOD_ATTRS_SZ(...)					\
 +	(sizeof((const struct uverbs_attr_def * const []){__VA_ARGS__}) /\
 +	 sizeof(const struct uverbs_attr_def *))
 +#define _UVERBS_METHOD(_id, _handler, _flags, ...)			\
 +	((const struct uverbs_method_def) {				\
 +	 .id = _id,							\
 +	 .flags = _flags,						\
 +	 .handler = _handler,						\
 +	 .num_attrs = _UVERBS_METHOD_ATTRS_SZ(__VA_ARGS__),		\
 +	 .attrs = &(const struct uverbs_attr_def * const []){__VA_ARGS__} })
 +#define DECLARE_UVERBS_METHOD(_name, _id, _handler, ...)		\
 +	const struct uverbs_method_def _name =				\
 +		_UVERBS_METHOD(_id, _handler, 0, ##__VA_ARGS__)
 +#define DECLARE_UVERBS_CTX_METHOD(_name, _id, _handler, _flags, ...)	\
 +	const struct uverbs_method_def _name =				\
 +		_UVERBS_METHOD(_id, _handler,				\
 +			       UVERBS_ACTION_FLAG_CREATE_ROOT,		\
 +			       ##__VA_ARGS__)
 +#define _UVERBS_OBJECT_METHODS_SZ(...)					\
 +	(sizeof((const struct uverbs_method_def * const []){__VA_ARGS__}) / \
 +	 sizeof(const struct uverbs_method_def *))
 +#define _UVERBS_OBJECT(_id, _type_attrs, ...)				\
 +	((const struct uverbs_object_def) {				\
 +	 .id = _id,							\
 +	 .type_attrs = _type_attrs,					\
 +	 .num_methods = _UVERBS_OBJECT_METHODS_SZ(__VA_ARGS__),		\
 +	 .methods = &(const struct uverbs_method_def * const []){__VA_ARGS__} })
 +#define DECLARE_UVERBS_OBJECT(_name, _id, _type_attrs, ...)		\
 +	const struct uverbs_object_def _name =				\
 +		_UVERBS_OBJECT(_id, _type_attrs, ##__VA_ARGS__)
 +#define _UVERBS_TREE_OBJECTS_SZ(...)					\
 +	(sizeof((const struct uverbs_object_def * const []){__VA_ARGS__}) / \
 +	 sizeof(const struct uverbs_object_def *))
 +#define _UVERBS_OBJECT_TREE(...)					\
 +	((const struct uverbs_object_tree_def) {			\
 +	 .num_objects = _UVERBS_TREE_OBJECTS_SZ(__VA_ARGS__),		\
 +	 .objects = &(const struct uverbs_object_def * const []){__VA_ARGS__} })
 +#define DECLARE_UVERBS_OBJECT_TREE(_name, ...)				\
 +	const struct uverbs_object_tree_def _name =			\
 +		_UVERBS_OBJECT_TREE(__VA_ARGS__)
  
  /* =================================================
   *              Parsing infrastructure
* Unmerged path drivers/infiniband/core/uverbs_std_types_counters.c
* Unmerged path drivers/infiniband/core/uverbs_std_types_mr.c
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
diff --git a/drivers/infiniband/core/uverbs_ioctl.c b/drivers/infiniband/core/uverbs_ioctl.c
index ec7b453e622b..c70b6d7ddc44 100644
--- a/drivers/infiniband/core/uverbs_ioctl.c
+++ b/drivers/infiniband/core/uverbs_ioctl.c
@@ -465,3 +465,54 @@ out:
 
 	return err;
 }
+
+int uverbs_get_flags64(u64 *to, const struct uverbs_attr_bundle *attrs_bundle,
+		       size_t idx, u64 allowed_bits)
+{
+	const struct uverbs_attr *attr;
+	u64 flags;
+
+	attr = uverbs_attr_get(attrs_bundle, idx);
+	/* Missing attribute means 0 flags */
+	if (IS_ERR(attr)) {
+		*to = 0;
+		return 0;
+	}
+
+	/*
+	 * New userspace code should use 8 bytes to pass flags, but we
+	 * transparently support old userspaces that were using 4 bytes as
+	 * well.
+	 */
+	if (attr->ptr_attr.len == 8)
+		flags = attr->ptr_attr.data;
+	else if (attr->ptr_attr.len == 4)
+		memcpy(&flags, &attr->ptr_attr.data, 4);
+	else
+		return -EINVAL;
+
+	if (flags & ~allowed_bits)
+		return -EINVAL;
+
+	*to = flags;
+	return 0;
+}
+EXPORT_SYMBOL(uverbs_get_flags64);
+
+int uverbs_get_flags32(u32 *to, const struct uverbs_attr_bundle *attrs_bundle,
+		       size_t idx, u64 allowed_bits)
+{
+	u64 flags;
+	int ret;
+
+	ret = uverbs_get_flags64(&flags, attrs_bundle, idx, allowed_bits);
+	if (ret)
+		return ret;
+
+	if (flags > U32_MAX)
+		return -EINVAL;
+	*to = flags;
+
+	return 0;
+}
+EXPORT_SYMBOL(uverbs_get_flags32);
* Unmerged path drivers/infiniband/core/uverbs_std_types_counters.c
* Unmerged path drivers/infiniband/core/uverbs_std_types_cq.c
* Unmerged path drivers/infiniband/core/uverbs_std_types_mr.c
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path include/rdma/uverbs_ioctl.h
