X86/KVM: Properly update 'tsc_offset' to represent the running guest

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm: Properly update 'tsc_offset' to represent the running guest (Vitaly Kuznetsov) [1565739 1497611]
Rebuild_FUZZ: 96.97%
commit-author KarimAllah Ahmed <karahmed@amazon.de>
commit e79f245ddec17bbd89d73cd0169dba4be46c9b55
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/e79f245d.failed

Update 'tsc_offset' on vmentry/vmexit of L2 guests to ensure that it always
captures the TSC_OFFSET of the running guest whether it is the L1 or L2
guest.

	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: kvm@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Reviewed-by: Jim Mattson <jmattson@google.com>
	Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
[AMD changes, fix update_ia32_tsc_adjust_msr. - Paolo]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e79f245ddec17bbd89d73cd0169dba4be46c9b55)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 5501a1957de2,7207e6cc07c1..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -10511,17 -11186,7 +10522,21 @@@ static int prepare_vmcs02(struct kvm_vc
  		vmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);
  	}
  
++<<<<<<< HEAD
 +	set_cr4_guest_host_mask(vmx);
 +
 +	if (from_vmentry &&
 +	    vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS)
 +		vmcs_write64(GUEST_BNDCFGS, vmcs12->guest_bndcfgs);
 +
 +	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)
 +		vmcs_write64(TSC_OFFSET,
 +			vcpu->arch.tsc_offset + vmcs12->tsc_offset);
 +	else
 +		vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
++=======
+ 	vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
++>>>>>>> e79f245ddec1 (X86/KVM: Properly update 'tsc_offset' to represent the running guest)
  
  	if (kvm_has_tsc_control)
  		decache_tsc_multiplier(vmx);
@@@ -11340,42 -12037,32 +11359,59 @@@ static void nested_vmx_vmexit(struct kv
  	/* trying to cancel vmlaunch/vmresume is a bug */
  	WARN_ON_ONCE(vmx->nested.nested_run_pending);
  
 -	/*
 -	 * The only expected VM-instruction error is "VM entry with
 -	 * invalid control field(s)." Anything else indicates a
 -	 * problem with L0.
 -	 */
 -	WARN_ON_ONCE(vmx->fail && (vmcs_read32(VM_INSTRUCTION_ERROR) !=
 -				   VMXERR_ENTRY_INVALID_CONTROL_FIELD));
 -
  	leave_guest_mode(vcpu);
 -
 +	if (exit_reason == -1)
 +		sync_vmcs12(vcpu, vmcs12);
 +	else
 +		prepare_vmcs12(vcpu, vmcs12, exit_reason, exit_intr_info,
 +			       exit_qualification);
 +
++<<<<<<< HEAD
 +	if (nested_vmx_store_msr(vcpu, vmcs12->vm_exit_msr_store_addr,
 +				 vmcs12->vm_exit_msr_store_count))
 +		nested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);
++=======
+ 	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)
+ 		vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+ 
+ 	if (likely(!vmx->fail)) {
+ 		if (exit_reason == -1)
+ 			sync_vmcs12(vcpu, vmcs12);
+ 		else
+ 			prepare_vmcs12(vcpu, vmcs12, exit_reason, exit_intr_info,
+ 				       exit_qualification);
+ 
+ 		if (nested_vmx_store_msr(vcpu, vmcs12->vm_exit_msr_store_addr,
+ 					 vmcs12->vm_exit_msr_store_count))
+ 			nested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);
+ 	}
++>>>>>>> e79f245ddec1 (X86/KVM: Properly update 'tsc_offset' to represent the running guest)
  
  	vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 +
 +	/*
 +	 * TODO: SDM says that with acknowledge interrupt on exit, bit 31 of
 +	 * the VM-exit interrupt information (valid interrupt) is always set to
 +	 * 1 on EXIT_REASON_EXTERNAL_INTERRUPT, so we shouldn't need
 +	 * kvm_cpu_has_interrupt().  See the commit message for details.
 +	 */
 +	if (nested_exit_intr_ack_set(vcpu) &&
 +	    exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&
 +	    kvm_cpu_has_interrupt(vcpu)) {
 +		int irq = kvm_cpu_get_interrupt(vcpu);
 +		WARN_ON(irq < 0);
 +		vmcs12->vm_exit_intr_info = irq |
 +			INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;
 +	}
 +
 +	if (exit_reason != -1)
 +		trace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,
 +					       vmcs12->exit_qualification,
 +					       vmcs12->idt_vectoring_info_field,
 +					       vmcs12->vm_exit_intr_info,
 +					       vmcs12->vm_exit_intr_error_code,
 +					       KVM_ISA_VMX);
 +
  	vm_entry_controls_reset_shadow(vmx);
  	vm_exit_controls_reset_shadow(vmx);
  	vmx_segment_cache_clear(vmx);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4091026435c1..3e8f34eb8766 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -924,6 +924,7 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
+	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index 2abd6b8af3db..3051965016d8 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -1278,12 +1278,23 @@ static void init_sys_seg(struct vmcb_seg *seg, uint32_t type)
 	seg->base = 0;
 }
 
+static u64 svm_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_svm *svm = to_svm(vcpu);
+
+	if (is_guest_mode(vcpu))
+		return svm->nested.hsave->control.tsc_offset;
+
+	return vcpu->arch.tsc_offset;
+}
+
 static void svm_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	u64 g_tsc_offset = 0;
 
 	if (is_guest_mode(vcpu)) {
+		/* Write L1's TSC offset.  */
 		g_tsc_offset = svm->vmcb->control.tsc_offset -
 			       svm->nested.hsave->control.tsc_offset;
 		svm->nested.hsave->control.tsc_offset = offset;
@@ -3004,6 +3015,7 @@ static int nested_svm_vmexit(struct vcpu_svm *svm)
 	/* Restore the original control entries */
 	copy_vmcb_control_area(vmcb, hsave);
 
+	svm->vcpu.arch.tsc_offset = svm->vmcb->control.tsc_offset;
 	kvm_clear_exception_queue(&svm->vcpu);
 	kvm_clear_interrupt_queue(&svm->vcpu);
 
@@ -3169,10 +3181,12 @@ static void enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb_gpa,
 	/* We don't want to see VMMCALLs from a nested guest */
 	clr_intercept(svm, INTERCEPT_VMMCALL);
 
+	svm->vcpu.arch.tsc_offset += nested_vmcb->control.tsc_offset;
+	svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+
 	svm->vmcb->control.virt_ext = nested_vmcb->control.virt_ext;
 	svm->vmcb->control.int_vector = nested_vmcb->control.int_vector;
 	svm->vmcb->control.int_state = nested_vmcb->control.int_state;
-	svm->vmcb->control.tsc_offset += nested_vmcb->control.tsc_offset;
 	svm->vmcb->control.event_inj = nested_vmcb->control.event_inj;
 	svm->vmcb->control.event_inj_err = nested_vmcb->control.event_inj_err;
 
@@ -5810,6 +5824,7 @@ static struct kvm_x86_ops svm_x86_ops = {
 
 	.has_wbinvd_exit = svm_has_wbinvd_exit,
 
+	.read_l1_tsc_offset = svm_read_l1_tsc_offset,
 	.write_tsc_offset = svm_write_tsc_offset,
 
 	.set_tdp_cr3 = set_tdp_cr3,
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index f751a44d4548..71f73590d1b7 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1361,7 +1361,7 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 
 static void update_ia32_tsc_adjust_msr(struct kvm_vcpu *vcpu, s64 offset)
 {
-	u64 curr_offset = vcpu->arch.tsc_offset;
+	u64 curr_offset = kvm_x86_ops->read_l1_tsc_offset(vcpu);
 	vcpu->arch.ia32_tsc_adjust_msr += offset - curr_offset;
 }
 
@@ -1403,7 +1403,9 @@ static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
-	return vcpu->arch.tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	u64 tsc_offset = kvm_x86_ops->read_l1_tsc_offset(vcpu);
+
+	return tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
