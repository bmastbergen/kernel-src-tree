s390/bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [s390] bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64 (Yauheni Kaliuta) [1691710]
Rebuild_FUZZ: 96.77%
commit-author Heiko Carstens <heiko.carstens@de.ibm.com>
commit b1cb7e2b6c3e8758e7406422b66c54c066737977
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/b1cb7e2b.failed

Since z9 109 we have the flogr instruction which can be used to implement
optimized versions of __ffs, ffs, __fls, fls and fls64.
So implement and use them, instead of the generic variants.
This reduces the size of the kernel image (defconfig, -march=z9-109)
by 19,648 bytes.

	Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit b1cb7e2b6c3e8758e7406422b66c54c066737977)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/bitops.h
diff --cc arch/s390/include/asm/bitops.h
index 6038349c8410,c6dbd6115cc5..000000000000
--- a/arch/s390/include/asm/bitops.h
+++ b/arch/s390/include/asm/bitops.h
@@@ -274,183 -300,64 +274,237 @@@ static inline int test_bit(unsigned lon
  }
  
  /*
 - * ATTENTION:
 - * find_first_bit_left() and find_next_bit_left() use MSB0 encoding.
 + * Optimized find bit helper functions.
 + */
 +
++<<<<<<< HEAD
 +/**
 + * __ffz_word_loop - find byte offset of first long != -1UL
 + * @addr: pointer to array of unsigned long
 + * @size: size of the array in bits
 + */
 +static inline unsigned long __ffz_word_loop(const unsigned long *addr,
 +					    unsigned long size)
 +{
 +	typedef struct { long _[__BITOPS_WORDS(size)]; } addrtype;
 +	unsigned long bytes = 0;
 +
 +	asm volatile(
 +#ifndef CONFIG_64BIT
 +		"	ahi	%1,-1\n"
 +		"	sra	%1,5\n"
 +		"	jz	1f\n"
 +		"0:	c	%2,0(%0,%3)\n"
 +		"	jne	1f\n"
 +		"	la	%0,4(%0)\n"
 +		"	brct	%1,0b\n"
 +		"1:\n"
 +#else
 +		"	aghi	%1,-1\n"
 +		"	srag	%1,%1,6\n"
 +		"	jz	1f\n"
 +		"0:	cg	%2,0(%0,%3)\n"
 +		"	jne	1f\n"
 +		"	la	%0,8(%0)\n"
 +		"	brct	%1,0b\n"
 +		"1:\n"
 +#endif
 +		: "+&a" (bytes), "+&d" (size)
 +		: "d" (-1UL), "a" (addr), "m" (*(addrtype *) addr)
 +		: "cc" );
 +	return bytes;
 +}
 +
 +/**
 + * __ffs_word_loop - find byte offset of first long != 0UL
 + * @addr: pointer to array of unsigned long
 + * @size: size of the array in bits
 + */
 +static inline unsigned long __ffs_word_loop(const unsigned long *addr,
 +					    unsigned long size)
 +{
 +	typedef struct { long _[__BITOPS_WORDS(size)]; } addrtype;
 +	unsigned long bytes = 0;
 +
 +	asm volatile(
 +#ifndef CONFIG_64BIT
 +		"	ahi	%1,-1\n"
 +		"	sra	%1,5\n"
 +		"	jz	1f\n"
 +		"0:	c	%2,0(%0,%3)\n"
 +		"	jne	1f\n"
 +		"	la	%0,4(%0)\n"
 +		"	brct	%1,0b\n"
 +		"1:\n"
 +#else
 +		"	aghi	%1,-1\n"
 +		"	srag	%1,%1,6\n"
 +		"	jz	1f\n"
 +		"0:	cg	%2,0(%0,%3)\n"
 +		"	jne	1f\n"
 +		"	la	%0,8(%0)\n"
 +		"	brct	%1,0b\n"
 +		"1:\n"
 +#endif
 +		: "+&a" (bytes), "+&a" (size)
 +		: "d" (0UL), "a" (addr), "m" (*(addrtype *) addr)
 +		: "cc" );
 +	return bytes;
 +}
 +
 +/**
 + * __ffz_word - add number of the first unset bit
 + * @nr: base value the bit number is added to
 + * @word: the word that is searched for unset bits
 + */
 +static inline unsigned long __ffz_word(unsigned long nr, unsigned long word)
 +{
 +#ifdef CONFIG_64BIT
 +	if ((word & 0xffffffff) == 0xffffffff) {
 +		word >>= 32;
 +		nr += 32;
 +	}
 +#endif
 +	if ((word & 0xffff) == 0xffff) {
 +		word >>= 16;
 +		nr += 16;
 +	}
 +	if ((word & 0xff) == 0xff) {
 +		word >>= 8;
 +		nr += 8;
 +	}
 +	return nr + _zb_findmap[(unsigned char) word];
 +}
 +
 +/**
 + * __ffs_word - add number of the first set bit
 + * @nr: base value the bit number is added to
 + * @word: the word that is searched for set bits
 + */
 +static inline unsigned long __ffs_word(unsigned long nr, unsigned long word)
 +{
 +#ifdef CONFIG_64BIT
 +	if ((word & 0xffffffff) == 0) {
 +		word >>= 32;
 +		nr += 32;
 +	}
 +#endif
 +	if ((word & 0xffff) == 0) {
 +		word >>= 16;
 +		nr += 16;
 +	}
 +	if ((word & 0xff) == 0) {
 +		word >>= 8;
 +		nr += 8;
 +	}
 +	return nr + _sb_findmap[(unsigned char) word];
 +}
 +
 +
 +/**
 + * __load_ulong_be - load big endian unsigned long
 + * @p: pointer to array of unsigned long
 + * @offset: byte offset of source value in the array
 + */
 +static inline unsigned long __load_ulong_be(const unsigned long *p,
 +					    unsigned long offset)
 +{
 +	p = (unsigned long *)((unsigned long) p + offset);
 +	return *p;
 +}
 +
 +/**
 + * __load_ulong_le - load little endian unsigned long
 + * @p: pointer to array of unsigned long
 + * @offset: byte offset of source value in the array
 + */
 +static inline unsigned long __load_ulong_le(const unsigned long *p,
 +					    unsigned long offset)
 +{
 +	unsigned long word;
 +
 +	p = (unsigned long *)((unsigned long) p + offset);
 +#ifndef CONFIG_64BIT
 +	asm volatile(
 +		"	ic	%0,%O1(%R1)\n"
 +		"	icm	%0,2,%O1+1(%R1)\n"
 +		"	icm	%0,4,%O1+2(%R1)\n"
 +		"	icm	%0,8,%O1+3(%R1)"
 +		: "=&d" (word) : "Q" (*p) : "cc");
 +#else
 +	asm volatile(
 +		"	lrvg	%0,%1"
 +		: "=d" (word) : "m" (*p) );
 +#endif
 +	return word;
 +}
 +
 +/*
 + * The various find bit functions.
   */
 -unsigned long find_first_bit_left(const unsigned long *addr, unsigned long size);
 -unsigned long find_next_bit_left(const unsigned long *addr, unsigned long size,
 -				 unsigned long offset);
  
 +/*
 + * ffz - find first zero in word.
 + * @word: The word to search
 + *
 + * Undefined if no zero exists, so code should check against ~0UL first.
 + */
 +static inline unsigned long ffz(unsigned long word)
 +{
 +	return __ffz_word(0, word);
++=======
+ #ifdef CONFIG_HAVE_MARCH_Z9_109_FEATURES
+ 
+ /**
+  * __flogr - find leftmost one
+  * @word - The word to search
+  *
+  * Returns the bit number of the most significant bit set,
+  * where the most significant bit has bit number 0.
+  * If no bit is set this function returns 64.
+  */
+ static inline unsigned char __flogr(unsigned long word)
+ {
+ 	if (__builtin_constant_p(word)) {
+ 		unsigned long bit = 0;
+ 
+ 		if (!word)
+ 			return 64;
+ 		if (!(word & 0xffffffff00000000UL)) {
+ 			word <<= 32;
+ 			bit += 32;
+ 		}
+ 		if (!(word & 0xffff000000000000UL)) {
+ 			word <<= 16;
+ 			bit += 16;
+ 		}
+ 		if (!(word & 0xff00000000000000UL)) {
+ 			word <<= 8;
+ 			bit += 8;
+ 		}
+ 		if (!(word & 0xf000000000000000UL)) {
+ 			word <<= 4;
+ 			bit += 4;
+ 		}
+ 		if (!(word & 0xc000000000000000UL)) {
+ 			word <<= 2;
+ 			bit += 2;
+ 		}
+ 		if (!(word & 0x8000000000000000UL)) {
+ 			word <<= 1;
+ 			bit += 1;
+ 		}
+ 		return bit;
+ 	} else {
+ 		register unsigned long bit asm("4") = word;
+ 		register unsigned long out asm("5");
+ 
+ 		asm volatile(
+ 			"       flogr   %[bit],%[bit]\n"
+ 			: [bit] "+d" (bit), [out] "=d" (out) : : "cc");
+ 		return bit;
+ 	}
++>>>>>>> b1cb7e2b6c3e (s390/bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64)
  }
  
  /**
@@@ -459,337 -366,83 +513,412 @@@
   *
   * Undefined if no bit exists, so code should check against 0 first.
   */
++<<<<<<< HEAD
 +static inline unsigned long __ffs (unsigned long word)
 +{
 +	return __ffs_word(0, word);
++=======
+ static inline unsigned long __ffs(unsigned long word)
+ {
+ 	return __flogr(-word & word) ^ (BITS_PER_LONG - 1);
++>>>>>>> b1cb7e2b6c3e (s390/bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64)
  }
  
  /**
   * ffs - find first bit set
++<<<<<<< HEAD
 + * @x: the word to search
 + *
 + * This is defined the same way as
 + * the libc and compiler builtin ffs routines, therefore
 + * differs in spirit from the above ffz (man ffs).
 + */
 +static inline int ffs(int x)
 +{
 +	if (!x)
 +		return 0;
 +	return __ffs_word(1, x);
 +}
 +
 +/**
 + * find_first_zero_bit - find the first zero bit in a memory region
 + * @addr: The address to start the search at
 + * @size: The maximum size to search
 + *
 + * Returns the bit-number of the first zero bit, not the number of the byte
 + * containing a bit.
 + */
 +static inline unsigned long find_first_zero_bit(const unsigned long *addr,
 +						unsigned long size)
 +{
 +	unsigned long bytes, bits;
 +
 +        if (!size)
 +                return 0;
 +	bytes = __ffz_word_loop(addr, size);
 +	bits = __ffz_word(bytes*8, __load_ulong_be(addr, bytes));
 +	return (bits < size) ? bits : size;
 +}
 +#define find_first_zero_bit find_first_zero_bit
 +
 +/**
 + * find_first_bit - find the first set bit in a memory region
 + * @addr: The address to start the search at
 + * @size: The maximum size to search
 + *
 + * Returns the bit-number of the first set bit, not the number of the byte
 + * containing a bit.
 + */
 +static inline unsigned long find_first_bit(const unsigned long * addr,
 +					   unsigned long size)
 +{
 +	unsigned long bytes, bits;
 +
 +        if (!size)
 +                return 0;
 +	bytes = __ffs_word_loop(addr, size);
 +	bits = __ffs_word(bytes*8, __load_ulong_be(addr, bytes));
 +	return (bits < size) ? bits : size;
 +}
 +#define find_first_bit find_first_bit
 +
 +/*
 + * Big endian variant whichs starts bit counting from left using
 + * the flogr (find leftmost one) instruction.
 + */
 +static inline unsigned long __flo_word(unsigned long nr, unsigned long val)
 +{
 +	register unsigned long bit asm("2") = val;
 +	register unsigned long out asm("3");
 +
 +	asm volatile (
 +		"	.insn	rre,0xb9830000,%[bit],%[bit]\n"
 +		: [bit] "+d" (bit), [out] "=d" (out) : : "cc");
 +	return nr + bit;
 +}
 +
 +/*
 + * 64 bit special left bitops format:
 + * order in memory:
 + *    00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f
 + *    10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f
 + *    20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f
 + *    30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f
 + * after that follows the next long with bit numbers
 + *    40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f
 + *    50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f
 + *    60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f
 + *    70 71 72 73 74 75 76 77 78 79 7a 7b 7c 7d 7e 7f
 + * The reason for this bit ordering is the fact that
 + * the hardware sets bits in a bitmap starting at bit 0
 + * and we don't want to scan the bitmap from the 'wrong
 + * end'.
 + */
 +static inline unsigned long find_first_bit_left(const unsigned long *addr,
 +						unsigned long size)
 +{
 +	unsigned long bytes, bits;
 +
 +	if (!size)
 +		return 0;
 +	bytes = __ffs_word_loop(addr, size);
 +	bits = __flo_word(bytes * 8, __load_ulong_be(addr, bytes));
 +	return (bits < size) ? bits : size;
 +}
 +
 +static inline int find_next_bit_left(const unsigned long *addr,
 +				     unsigned long size,
 +				     unsigned long offset)
 +{
 +	const unsigned long *p;
 +	unsigned long bit, set;
 +
 +	if (offset >= size)
 +		return size;
 +	bit = offset & (BITS_PER_LONG - 1);
 +	offset -= bit;
 +	size -= offset;
 +	p = addr + offset / BITS_PER_LONG;
 +	if (bit) {
 +		set = __flo_word(0, *p & (~0UL >> bit));
 +		if (set >= size)
 +			return size + offset;
 +		if (set < BITS_PER_LONG)
 +			return set + offset;
 +		offset += BITS_PER_LONG;
 +		size -= BITS_PER_LONG;
 +		p++;
 +	}
 +	return offset + find_first_bit_left(p, size);
 +}
 +
 +#define for_each_set_bit_left(bit, addr, size)				\
 +	for ((bit) = find_first_bit_left((addr), (size));		\
 +	     (bit) < (size);						\
 +	     (bit) = find_next_bit_left((addr), (size), (bit) + 1))
 +
 +/* same as for_each_set_bit() but use bit as value to start with */
 +#define for_each_set_bit_left_cont(bit, addr, size)			\
 +	for ((bit) = find_next_bit_left((addr), (size), (bit));		\
 +	     (bit) < (size);						\
 +	     (bit) = find_next_bit_left((addr), (size), (bit) + 1))
 +
 +/**
 + * find_next_zero_bit - find the first zero bit in a memory region
 + * @addr: The address to base the search on
 + * @offset: The bitnumber to start searching at
 + * @size: The maximum size to search
 + */
 +static inline int find_next_zero_bit (const unsigned long * addr,
 +				      unsigned long size,
 +				      unsigned long offset)
 +{
 +        const unsigned long *p;
 +	unsigned long bit, set;
 +
 +	if (offset >= size)
 +		return size;
 +	bit = offset & (BITS_PER_LONG - 1);
 +	offset -= bit;
 +	size -= offset;
 +	p = addr + offset / BITS_PER_LONG;
 +	if (bit) {
 +		/*
 +		 * __ffz_word returns BITS_PER_LONG
 +		 * if no zero bit is present in the word.
 +		 */
 +		set = __ffz_word(bit, *p >> bit);
 +		if (set >= size)
 +			return size + offset;
 +		if (set < BITS_PER_LONG)
 +			return set + offset;
 +		offset += BITS_PER_LONG;
 +		size -= BITS_PER_LONG;
 +		p++;
 +	}
 +	return offset + find_first_zero_bit(p, size);
 +}
 +#define find_next_zero_bit find_next_zero_bit
 +
 +/**
 + * find_next_bit - find the first set bit in a memory region
 + * @addr: The address to base the search on
 + * @offset: The bitnumber to start searching at
 + * @size: The maximum size to search
 + */
 +static inline int find_next_bit (const unsigned long * addr,
 +				 unsigned long size,
 +				 unsigned long offset)
 +{
 +        const unsigned long *p;
 +	unsigned long bit, set;
 +
 +	if (offset >= size)
 +		return size;
 +	bit = offset & (BITS_PER_LONG - 1);
 +	offset -= bit;
 +	size -= offset;
 +	p = addr + offset / BITS_PER_LONG;
 +	if (bit) {
 +		/*
 +		 * __ffs_word returns BITS_PER_LONG
 +		 * if no one bit is present in the word.
 +		 */
 +		set = __ffs_word(0, *p & (~0UL << bit));
 +		if (set >= size)
 +			return size + offset;
 +		if (set < BITS_PER_LONG)
 +			return set + offset;
 +		offset += BITS_PER_LONG;
 +		size -= BITS_PER_LONG;
 +		p++;
 +	}
 +	return offset + find_first_bit(p, size);
 +}
 +#define find_next_bit find_next_bit
 +
 +/*
 + * Every architecture must define this function. It's the fastest
 + * way of searching a 140-bit bitmap where the first 100 bits are
 + * unlikely to be set. It's guaranteed that at least one of the 140
 + * bits is cleared.
 + */
 +static inline int sched_find_first_bit(unsigned long *b)
 +{
 +	return find_first_bit(b, 140);
 +}
 +
++=======
+  * @word: the word to search
+  *
+  * This is defined the same way as the libc and
+  * compiler builtin ffs routines (man ffs).
+  */
+ static inline int ffs(int word)
+ {
+ 	unsigned long mask = 2 * BITS_PER_LONG - 1;
+ 	unsigned int val = (unsigned int)word;
+ 
+ 	return (1 + (__flogr(-val & val) ^ (BITS_PER_LONG - 1))) & mask;
+ }
+ 
+ /**
+  * __fls - find last (most-significant) set bit in a long word
+  * @word: the word to search
+  *
+  * Undefined if no set bit exists, so code should check against 0 first.
+  */
+ static inline unsigned long __fls(unsigned long word)
+ {
+ 	return __flogr(word) ^ (BITS_PER_LONG - 1);
+ }
+ 
+ /**
+  * fls64 - find last set bit in a 64-bit word
+  * @word: the word to search
+  *
+  * This is defined in a similar way as the libc and compiler builtin
+  * ffsll, but returns the position of the most significant set bit.
+  *
+  * fls64(value) returns 0 if value is 0 or the position of the last
+  * set bit if value is nonzero. The last (most significant) bit is
+  * at position 64.
+  */
+ static inline int fls64(unsigned long word)
+ {
+ 	unsigned long mask = 2 * BITS_PER_LONG - 1;
+ 
+ 	return (1 + (__flogr(word) ^ (BITS_PER_LONG - 1))) & mask;
+ }
+ 
+ /**
+  * fls - find last (most-significant) bit set
+  * @word: the word to search
+  *
+  * This is defined the same way as ffs.
+  * Note fls(0) = 0, fls(1) = 1, fls(0x80000000) = 32.
+  */
+ static inline int fls(int word)
+ {
+ 	return fls64((unsigned int)word);
+ }
+ 
+ #else /* CONFIG_HAVE_MARCH_Z9_109_FEATURES */
+ 
+ #include <asm-generic/bitops/__ffs.h>
+ #include <asm-generic/bitops/ffs.h>
+ #include <asm-generic/bitops/__fls.h>
++>>>>>>> b1cb7e2b6c3e (s390/bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64)
  #include <asm-generic/bitops/fls.h>
 +#include <asm-generic/bitops/__fls.h>
  #include <asm-generic/bitops/fls64.h>
  
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_HAVE_MARCH_Z9_109_FEATURES */
+ 
+ #include <asm-generic/bitops/ffz.h>
+ #include <asm-generic/bitops/find.h>
++>>>>>>> b1cb7e2b6c3e (s390/bitops: use flogr instruction to implement __ffs, ffs, __fls, fls and fls64)
  #include <asm-generic/bitops/hweight.h>
  #include <asm-generic/bitops/lock.h>
 -#include <asm-generic/bitops/sched.h>
 +
 +/*
 + * ATTENTION: intel byte ordering convention for ext2 and minix !!
 + * bit 0 is the LSB of addr; bit 31 is the MSB of addr;
 + * bit 32 is the LSB of (addr+4).
 + * That combined with the little endian byte order of Intel gives the
 + * following bit order in memory:
 + *    07 06 05 04 03 02 01 00 15 14 13 12 11 10 09 08 \
 + *    23 22 21 20 19 18 17 16 31 30 29 28 27 26 25 24
 + */
 +
 +static inline int find_first_zero_bit_le(void *vaddr, unsigned int size)
 +{
 +	unsigned long bytes, bits;
 +
 +        if (!size)
 +                return 0;
 +	bytes = __ffz_word_loop(vaddr, size);
 +	bits = __ffz_word(bytes*8, __load_ulong_le(vaddr, bytes));
 +	return (bits < size) ? bits : size;
 +}
 +#define find_first_zero_bit_le find_first_zero_bit_le
 +
 +static inline int find_next_zero_bit_le(void *vaddr, unsigned long size,
 +					  unsigned long offset)
 +{
 +        unsigned long *addr = vaddr, *p;
 +	unsigned long bit, set;
 +
 +        if (offset >= size)
 +                return size;
 +	bit = offset & (BITS_PER_LONG - 1);
 +	offset -= bit;
 +	size -= offset;
 +	p = addr + offset / BITS_PER_LONG;
 +        if (bit) {
 +		/*
 +		 * s390 version of ffz returns BITS_PER_LONG
 +		 * if no zero bit is present in the word.
 +		 */
 +		set = __ffz_word(bit, __load_ulong_le(p, 0) >> bit);
 +		if (set >= size)
 +			return size + offset;
 +		if (set < BITS_PER_LONG)
 +			return set + offset;
 +		offset += BITS_PER_LONG;
 +		size -= BITS_PER_LONG;
 +		p++;
 +        }
 +	return offset + find_first_zero_bit_le(p, size);
 +}
 +#define find_next_zero_bit_le find_next_zero_bit_le
 +
 +static inline unsigned long find_first_bit_le(void *vaddr, unsigned long size)
 +{
 +	unsigned long bytes, bits;
 +
 +	if (!size)
 +		return 0;
 +	bytes = __ffs_word_loop(vaddr, size);
 +	bits = __ffs_word(bytes*8, __load_ulong_le(vaddr, bytes));
 +	return (bits < size) ? bits : size;
 +}
 +#define find_first_bit_le find_first_bit_le
 +
 +static inline int find_next_bit_le(void *vaddr, unsigned long size,
 +				     unsigned long offset)
 +{
 +	unsigned long *addr = vaddr, *p;
 +	unsigned long bit, set;
 +
 +	if (offset >= size)
 +		return size;
 +	bit = offset & (BITS_PER_LONG - 1);
 +	offset -= bit;
 +	size -= offset;
 +	p = addr + offset / BITS_PER_LONG;
 +	if (bit) {
 +		/*
 +		 * s390 version of ffz returns BITS_PER_LONG
 +		 * if no zero bit is present in the word.
 +		 */
 +		set = __ffs_word(0, __load_ulong_le(p, 0) & (~0UL << bit));
 +		if (set >= size)
 +			return size + offset;
 +		if (set < BITS_PER_LONG)
 +			return set + offset;
 +		offset += BITS_PER_LONG;
 +		size -= BITS_PER_LONG;
 +		p++;
 +	}
 +	return offset + find_first_bit_le(p, size);
 +}
 +#define find_next_bit_le find_next_bit_le
 +
  #include <asm-generic/bitops/le.h>
 +
  #include <asm-generic/bitops/ext2-atomic-setbit.h>
  
  #endif /* _S390_BITOPS_H */
* Unmerged path arch/s390/include/asm/bitops.h
