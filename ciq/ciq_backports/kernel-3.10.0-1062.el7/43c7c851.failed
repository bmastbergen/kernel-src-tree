RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit 43c7c851b9bce9e6091f2c882871a3b388aa38c3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/43c7c851.failed

Any messages related to a device should be printed with the dev_*
formatters. This provides greater consistency for the user.

The core does not set pr_fmt so this has no significant change.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Reviewed-by: Steve Wise <swise@opengridcomputing.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
(cherry picked from commit 43c7c851b9bce9e6091f2c882871a3b388aa38c3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/cache.c
#	drivers/infiniband/core/device.c
diff --cc drivers/infiniband/core/cache.c
index 36d3478f5cc1,ebc64418d809..000000000000
--- a/drivers/infiniband/core/cache.c
+++ b/drivers/infiniband/core/cache.c
@@@ -163,94 -183,240 +163,296 @@@ int ib_cache_gid_parse_type_str(const c
  }
  EXPORT_SYMBOL(ib_cache_gid_parse_type_str);
  
++<<<<<<< HEAD
 +/* This function expects that rwlock will be write locked in all
 + * scenarios and that lock will be locked in sleep-able (RoCE)
 + * scenarios.
++=======
+ static struct ib_gid_table *rdma_gid_table(struct ib_device *device, u8 port)
+ {
+ 	return device->cache.ports[port - rdma_start_port(device)].gid;
+ }
+ 
+ static bool is_gid_entry_free(const struct ib_gid_table_entry *entry)
+ {
+ 	return !entry;
+ }
+ 
+ static bool is_gid_entry_valid(const struct ib_gid_table_entry *entry)
+ {
+ 	return entry && entry->state == GID_TABLE_ENTRY_VALID;
+ }
+ 
+ static void schedule_free_gid(struct kref *kref)
+ {
+ 	struct ib_gid_table_entry *entry =
+ 			container_of(kref, struct ib_gid_table_entry, kref);
+ 
+ 	queue_work(ib_wq, &entry->del_work);
+ }
+ 
+ static void free_gid_entry_locked(struct ib_gid_table_entry *entry)
+ {
+ 	struct ib_device *device = entry->attr.device;
+ 	u8 port_num = entry->attr.port_num;
+ 	struct ib_gid_table *table = rdma_gid_table(device, port_num);
+ 
+ 	dev_dbg(&device->dev, "%s port=%d index=%d gid %pI6\n", __func__,
+ 		port_num, entry->attr.index, entry->attr.gid.raw);
+ 
+ 	if (rdma_cap_roce_gid_table(device, port_num) &&
+ 	    entry->state != GID_TABLE_ENTRY_INVALID)
+ 		device->del_gid(&entry->attr, &entry->context);
+ 
+ 	write_lock_irq(&table->rwlock);
+ 
+ 	/*
+ 	 * The only way to avoid overwriting NULL in table is
+ 	 * by comparing if it is same entry in table or not!
+ 	 * If new entry in table is added by the time we free here,
+ 	 * don't overwrite the table entry.
+ 	 */
+ 	if (entry == table->data_vec[entry->attr.index])
+ 		table->data_vec[entry->attr.index] = NULL;
+ 	/* Now this index is ready to be allocated */
+ 	write_unlock_irq(&table->rwlock);
+ 
+ 	if (entry->attr.ndev)
+ 		dev_put(entry->attr.ndev);
+ 	kfree(entry);
+ }
+ 
+ static void free_gid_entry(struct kref *kref)
+ {
+ 	struct ib_gid_table_entry *entry =
+ 			container_of(kref, struct ib_gid_table_entry, kref);
+ 
+ 	free_gid_entry_locked(entry);
+ }
+ 
+ /**
+  * free_gid_work - Release reference to the GID entry
+  * @work: Work structure to refer to GID entry which needs to be
+  * deleted.
+  *
+  * free_gid_work() frees the entry from the HCA's hardware table
+  * if provider supports it. It releases reference to netdevice.
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
   */
 -static void free_gid_work(struct work_struct *work)
 +static int write_gid(struct ib_device *ib_dev, u8 port,
 +		     struct ib_gid_table *table, int ix,
 +		     const union ib_gid *gid,
 +		     const struct ib_gid_attr *attr,
 +		     enum gid_table_write_action action,
 +		     bool  default_gid)
 +	__releases(&table->rwlock) __acquires(&table->rwlock)
  {
++<<<<<<< HEAD
++=======
+ 	struct ib_gid_table_entry *entry =
+ 		container_of(work, struct ib_gid_table_entry, del_work);
+ 	struct ib_device *device = entry->attr.device;
+ 	u8 port_num = entry->attr.port_num;
+ 	struct ib_gid_table *table = rdma_gid_table(device, port_num);
+ 
+ 	mutex_lock(&table->lock);
+ 	free_gid_entry_locked(entry);
+ 	mutex_unlock(&table->lock);
+ }
+ 
+ static struct ib_gid_table_entry *
+ alloc_gid_entry(const struct ib_gid_attr *attr)
+ {
+ 	struct ib_gid_table_entry *entry;
+ 
+ 	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+ 	if (!entry)
+ 		return NULL;
+ 	kref_init(&entry->kref);
+ 	memcpy(&entry->attr, attr, sizeof(*attr));
+ 	if (entry->attr.ndev)
+ 		dev_hold(entry->attr.ndev);
+ 	INIT_WORK(&entry->del_work, free_gid_work);
+ 	entry->state = GID_TABLE_ENTRY_INVALID;
+ 	return entry;
+ }
+ 
+ static void store_gid_entry(struct ib_gid_table *table,
+ 			    struct ib_gid_table_entry *entry)
+ {
+ 	entry->state = GID_TABLE_ENTRY_VALID;
+ 
+ 	dev_dbg(&entry->attr.device->dev, "%s port=%d index=%d gid %pI6\n",
+ 		__func__, entry->attr.port_num, entry->attr.index,
+ 		entry->attr.gid.raw);
+ 
+ 	lockdep_assert_held(&table->lock);
+ 	write_lock_irq(&table->rwlock);
+ 	table->data_vec[entry->attr.index] = entry;
+ 	write_unlock_irq(&table->rwlock);
+ }
+ 
+ static void get_gid_entry(struct ib_gid_table_entry *entry)
+ {
+ 	kref_get(&entry->kref);
+ }
+ 
+ static void put_gid_entry(struct ib_gid_table_entry *entry)
+ {
+ 	kref_put(&entry->kref, schedule_free_gid);
+ }
+ 
+ static void put_gid_entry_locked(struct ib_gid_table_entry *entry)
+ {
+ 	kref_put(&entry->kref, free_gid_entry);
+ }
+ 
+ static int add_roce_gid(struct ib_gid_table_entry *entry)
+ {
+ 	const struct ib_gid_attr *attr = &entry->attr;
+ 	int ret;
+ 
+ 	if (!attr->ndev) {
+ 		dev_err(&attr->device->dev, "%s NULL netdev port=%d index=%d\n",
+ 			__func__, attr->port_num, attr->index);
+ 		return -EINVAL;
+ 	}
+ 	if (rdma_cap_roce_gid_table(attr->device, attr->port_num)) {
+ 		ret = attr->device->add_gid(attr, &entry->context);
+ 		if (ret) {
+ 			dev_err(&attr->device->dev,
+ 				"%s GID add failed port=%d index=%d\n",
+ 				__func__, attr->port_num, attr->index);
+ 			return ret;
+ 		}
+ 	}
+ 	return 0;
+ }
+ 
+ /**
+  * add_modify_gid - Add or modify GID table entry
+  *
+  * @table:	GID table in which GID to be added or modified
+  * @attr:	Attributes of the GID
+  *
+  * Returns 0 on success or appropriate error code. It accepts zero
+  * GID addition for non RoCE ports for HCA's who report them as valid
+  * GID. However such zero GIDs are not added to the cache.
+  */
+ static int add_modify_gid(struct ib_gid_table *table,
+ 			  const struct ib_gid_attr *attr)
+ {
+ 	struct ib_gid_table_entry *entry;
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
  	int ret = 0;
 +	struct net_device *old_net_dev;
 +	enum ib_gid_type old_gid_type;
  
 -	/*
 -	 * Invalidate any old entry in the table to make it safe to write to
 -	 * this index.
 +	/* in rdma_cap_roce_gid_table, this funciton should be protected by a
 +	 * sleep-able lock.
  	 */
 -	if (is_gid_entry_valid(table->data_vec[attr->index]))
 -		put_gid_entry(table->data_vec[attr->index]);
  
 -	/*
 -	 * Some HCA's report multiple GID entries with only one valid GID, and
 -	 * leave other unused entries as the zero GID. Convert zero GIDs to
 -	 * empty table entries instead of storing them.
 -	 */
 -	if (rdma_is_zero_gid(&attr->gid))
 -		return 0;
 +	if (rdma_cap_roce_gid_table(ib_dev, port)) {
 +		table->data_vec[ix].props |= GID_TABLE_ENTRY_INVALID;
 +		write_unlock_irq(&table->rwlock);
 +		/* GID_TABLE_WRITE_ACTION_MODIFY currently isn't supported by
 +		 * RoCE providers and thus only updates the cache.
 +		 */
 +		if (action == GID_TABLE_WRITE_ACTION_ADD)
 +			ret = ib_dev->add_gid(ib_dev, port, ix, gid, attr,
 +					      &table->data_vec[ix].context);
 +		else if (action == GID_TABLE_WRITE_ACTION_DEL)
 +			ret = ib_dev->del_gid(ib_dev, port, ix,
 +					      &table->data_vec[ix].context);
 +		write_lock_irq(&table->rwlock);
 +	}
  
 -	entry = alloc_gid_entry(attr);
 -	if (!entry)
 -		return -ENOMEM;
 +	old_net_dev = table->data_vec[ix].attr.ndev;
 +	old_gid_type = table->data_vec[ix].attr.gid_type;
 +	if (old_net_dev && old_net_dev != attr->ndev)
 +		dev_put(old_net_dev);
 +	/* if modify_gid failed, just delete the old gid */
 +	if (ret || action == GID_TABLE_WRITE_ACTION_DEL) {
 +		gid = &zgid;
 +		attr = &zattr;
 +		table->data_vec[ix].context = NULL;
 +	}
  
 -	if (rdma_protocol_roce(attr->device, attr->port_num)) {
 -		ret = add_roce_gid(entry);
 -		if (ret)
 -			goto done;
 +	memcpy(&table->data_vec[ix].gid, gid, sizeof(*gid));
 +	memcpy(&table->data_vec[ix].attr, attr, sizeof(*attr));
 +	if (default_gid) {
 +		table->data_vec[ix].props |= GID_TABLE_ENTRY_DEFAULT;
 +		if (action == GID_TABLE_WRITE_ACTION_DEL)
 +			table->data_vec[ix].attr.gid_type = old_gid_type;
  	}
 +	if (table->data_vec[ix].attr.ndev &&
 +	    table->data_vec[ix].attr.ndev != old_net_dev)
 +		dev_hold(table->data_vec[ix].attr.ndev);
  
 -	store_gid_entry(table, entry);
 -	return 0;
 +	table->data_vec[ix].props &= ~GID_TABLE_ENTRY_INVALID;
  
 -done:
 -	put_gid_entry(entry);
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int add_gid(struct ib_device *ib_dev, u8 port,
 +		   struct ib_gid_table *table, int ix,
 +		   const union ib_gid *gid,
 +		   const struct ib_gid_attr *attr,
 +		   bool  default_gid) {
 +	return write_gid(ib_dev, port, table, ix, gid, attr,
 +			 GID_TABLE_WRITE_ACTION_ADD, default_gid);
++=======
+ /**
+  * del_gid - Delete GID table entry
+  *
+  * @ib_dev:	IB device whose GID entry to be deleted
+  * @port:	Port number of the IB device
+  * @table:	GID table of the IB device for a port
+  * @ix:		GID entry index to delete
+  *
+  */
+ static void del_gid(struct ib_device *ib_dev, u8 port,
+ 		    struct ib_gid_table *table, int ix)
+ {
+ 	struct ib_gid_table_entry *entry;
+ 
+ 	lockdep_assert_held(&table->lock);
+ 
+ 	dev_dbg(&ib_dev->dev, "%s port=%d index=%d gid %pI6\n", __func__, port,
+ 		ix, table->data_vec[ix]->attr.gid.raw);
+ 
+ 	write_lock_irq(&table->rwlock);
+ 	entry = table->data_vec[ix];
+ 	entry->state = GID_TABLE_ENTRY_PENDING_DEL;
+ 	/*
+ 	 * For non RoCE protocol, GID entry slot is ready to use.
+ 	 */
+ 	if (!rdma_protocol_roce(ib_dev, port))
+ 		table->data_vec[ix] = NULL;
+ 	write_unlock_irq(&table->rwlock);
+ 
+ 	put_gid_entry_locked(entry);
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
 +}
 +
 +static int modify_gid(struct ib_device *ib_dev, u8 port,
 +		      struct ib_gid_table *table, int ix,
 +		      const union ib_gid *gid,
 +		      const struct ib_gid_attr *attr,
 +		      bool  default_gid) {
 +	return write_gid(ib_dev, port, table, ix, gid, attr,
 +			 GID_TABLE_WRITE_ACTION_MODIFY, default_gid);
 +}
 +
 +static int del_gid(struct ib_device *ib_dev, u8 port,
 +		   struct ib_gid_table *table, int ix,
 +		   bool  default_gid) {
 +	return write_gid(ib_dev, port, table, ix, &zgid, &zattr,
 +			 GID_TABLE_WRITE_ACTION_DEL, default_gid);
  }
  
 -/* rwlock should be read locked, or lock should be held */
 +/* rwlock should be read locked */
  static int find_gid(struct ib_gid_table *table, const union ib_gid *gid,
  		    const struct ib_gid_attr *val, bool default_gid,
  		    unsigned long mask, int *pempty)
@@@ -596,12 -766,30 +798,30 @@@ err_free_table
  	return NULL;
  }
  
 -static void release_gid_table(struct ib_device *device, u8 port,
 -			      struct ib_gid_table *table)
 +static void release_gid_table(struct ib_gid_table *table)
  {
++<<<<<<< HEAD
 +	if (table) {
 +		kfree(table->data_vec);
 +		kfree(table);
++=======
+ 	bool leak = false;
+ 	int i;
+ 
+ 	if (!table)
+ 		return;
+ 
+ 	for (i = 0; i < table->sz; i++) {
+ 		if (is_gid_entry_free(table->data_vec[i]))
+ 			continue;
+ 		if (kref_read(&table->data_vec[i]->kref) > 1) {
+ 			dev_err(&device->dev,
+ 				"GID entry ref leak for index %d ref=%d\n", i,
+ 				kref_read(&table->data_vec[i]->kref));
+ 			leak = true;
+ 		}
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
  	}
 -	if (leak)
 -		return;
 -
 -	kfree(table->data_vec);
 -	kfree(table);
  }
  
  static void cleanup_gid_table_port(struct ib_device *ib_dev, u8 port,
@@@ -1092,6 -1168,151 +1312,154 @@@ int ib_get_cached_port_state(struct ib_
  }
  EXPORT_SYMBOL(ib_get_cached_port_state);
  
++<<<<<<< HEAD
++=======
+ /**
+  * rdma_get_gid_attr - Returns GID attributes for a port of a device
+  * at a requested gid_index, if a valid GID entry exists.
+  * @device:		The device to query.
+  * @port_num:		The port number on the device where the GID value
+  *			is to be queried.
+  * @index:		Index of the GID table entry whose attributes are to
+  *                      be queried.
+  *
+  * rdma_get_gid_attr() acquires reference count of gid attributes from the
+  * cached GID table. Caller must invoke rdma_put_gid_attr() to release
+  * reference to gid attribute regardless of link layer.
+  *
+  * Returns pointer to valid gid attribute or ERR_PTR for the appropriate error
+  * code.
+  */
+ const struct ib_gid_attr *
+ rdma_get_gid_attr(struct ib_device *device, u8 port_num, int index)
+ {
+ 	const struct ib_gid_attr *attr = ERR_PTR(-EINVAL);
+ 	struct ib_gid_table *table;
+ 	unsigned long flags;
+ 
+ 	if (!rdma_is_port_valid(device, port_num))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	table = rdma_gid_table(device, port_num);
+ 	if (index < 0 || index >= table->sz)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	read_lock_irqsave(&table->rwlock, flags);
+ 	if (!is_gid_entry_valid(table->data_vec[index]))
+ 		goto done;
+ 
+ 	get_gid_entry(table->data_vec[index]);
+ 	attr = &table->data_vec[index]->attr;
+ done:
+ 	read_unlock_irqrestore(&table->rwlock, flags);
+ 	return attr;
+ }
+ EXPORT_SYMBOL(rdma_get_gid_attr);
+ 
+ /**
+  * rdma_put_gid_attr - Release reference to the GID attribute
+  * @attr:		Pointer to the GID attribute whose reference
+  *			needs to be released.
+  *
+  * rdma_put_gid_attr() must be used to release reference whose
+  * reference is acquired using rdma_get_gid_attr() or any APIs
+  * which returns a pointer to the ib_gid_attr regardless of link layer
+  * of IB or RoCE.
+  *
+  */
+ void rdma_put_gid_attr(const struct ib_gid_attr *attr)
+ {
+ 	struct ib_gid_table_entry *entry =
+ 		container_of(attr, struct ib_gid_table_entry, attr);
+ 
+ 	put_gid_entry(entry);
+ }
+ EXPORT_SYMBOL(rdma_put_gid_attr);
+ 
+ /**
+  * rdma_hold_gid_attr - Get reference to existing GID attribute
+  *
+  * @attr:		Pointer to the GID attribute whose reference
+  *			needs to be taken.
+  *
+  * Increase the reference count to a GID attribute to keep it from being
+  * freed. Callers are required to already be holding a reference to attribute.
+  *
+  */
+ void rdma_hold_gid_attr(const struct ib_gid_attr *attr)
+ {
+ 	struct ib_gid_table_entry *entry =
+ 		container_of(attr, struct ib_gid_table_entry, attr);
+ 
+ 	get_gid_entry(entry);
+ }
+ EXPORT_SYMBOL(rdma_hold_gid_attr);
+ 
+ /**
+  * rdma_read_gid_attr_ndev_rcu - Read GID attribute netdevice
+  * which must be in UP state.
+  *
+  * @attr:Pointer to the GID attribute
+  *
+  * Returns pointer to netdevice if the netdevice was attached to GID and
+  * netdevice is in UP state. Caller must hold RCU lock as this API
+  * reads the netdev flags which can change while netdevice migrates to
+  * different net namespace. Returns ERR_PTR with error code otherwise.
+  *
+  */
+ struct net_device *rdma_read_gid_attr_ndev_rcu(const struct ib_gid_attr *attr)
+ {
+ 	struct ib_gid_table_entry *entry =
+ 			container_of(attr, struct ib_gid_table_entry, attr);
+ 	struct ib_device *device = entry->attr.device;
+ 	struct net_device *ndev = ERR_PTR(-ENODEV);
+ 	u8 port_num = entry->attr.port_num;
+ 	struct ib_gid_table *table;
+ 	unsigned long flags;
+ 	bool valid;
+ 
+ 	table = rdma_gid_table(device, port_num);
+ 
+ 	read_lock_irqsave(&table->rwlock, flags);
+ 	valid = is_gid_entry_valid(table->data_vec[attr->index]);
+ 	if (valid && attr->ndev && (READ_ONCE(attr->ndev->flags) & IFF_UP))
+ 		ndev = attr->ndev;
+ 	read_unlock_irqrestore(&table->rwlock, flags);
+ 	return ndev;
+ }
+ 
+ static int config_non_roce_gid_cache(struct ib_device *device,
+ 				     u8 port, int gid_tbl_len)
+ {
+ 	struct ib_gid_attr gid_attr = {};
+ 	struct ib_gid_table *table;
+ 	int ret = 0;
+ 	int i;
+ 
+ 	gid_attr.device = device;
+ 	gid_attr.port_num = port;
+ 	table = rdma_gid_table(device, port);
+ 
+ 	mutex_lock(&table->lock);
+ 	for (i = 0; i < gid_tbl_len; ++i) {
+ 		if (!device->query_gid)
+ 			continue;
+ 		ret = device->query_gid(device, port, i, &gid_attr.gid);
+ 		if (ret) {
+ 			dev_warn(&device->dev,
+ 				 "query_gid failed (%d) for index %d\n", ret,
+ 				 i);
+ 			goto err;
+ 		}
+ 		gid_attr.index = i;
+ 		add_modify_gid(table, &gid_attr);
+ 	}
+ err:
+ 	mutex_unlock(&table->lock);
+ 	return ret;
+ }
+ 
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
  static void ib_cache_update(struct ib_device *device,
  			    u8                port,
  			    bool	      enforce_security)
diff --cc drivers/infiniband/core/device.c
index 910f35ee49f7,7c3ff43092fd..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -526,18 -532,28 +528,42 @@@ int ib_register_device(struct ib_devic
  		goto port_cleanup;
  	}
  
++<<<<<<< HEAD
 +	memset(&device->attrs, 0, sizeof(device->attrs));
 +	ret = device->query_device(device, &device->attrs, &uhw);
 +	if (ret) {
 +		pr_warn("Couldn't query the device attributes\n");
 +		goto cache_cleanup;
++=======
+ 	device->index = __dev_new_index();
+ 
+ 	ret = ib_device_register_rdmacg(device);
+ 	if (ret) {
+ 		dev_warn(&device->dev,
+ 			 "Couldn't register device with rdma cgroup\n");
+ 		goto cache_cleanup;
+ 	}
+ 
+ 	memset(&device->attrs, 0, sizeof(device->attrs));
+ 	ret = device->query_device(device, &device->attrs, &uhw);
+ 	if (ret) {
+ 		dev_warn(&device->dev,
+ 			 "Couldn't query the device attributes\n");
+ 		goto cg_cleanup;
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
  	}
  
  	ret = ib_device_register_sysfs(device, port_callback);
  	if (ret) {
++<<<<<<< HEAD
 +		pr_warn("Couldn't register device %s with driver model\n",
 +			device->name);
 +		goto cache_cleanup;
++=======
+ 		dev_warn(&device->dev,
+ 			 "Couldn't register device with driver model\n");
+ 		goto cg_cleanup;
++>>>>>>> 43c7c851b9bc (RDMA/core: Use dev_err/dbg/etc instead of pr_* + ibdev->name)
  	}
  
  	device->reg_state = IB_DEV_REGISTERED;
* Unmerged path drivers/infiniband/core/cache.c
diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index 744a59d24274..16a0bff26f2b 100644
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -2335,8 +2335,8 @@ static void cma_listen_on_dev(struct rdma_id_private *id_priv,
 
 	ret = rdma_listen(id, id_priv->backlog);
 	if (ret)
-		pr_warn("RDMA CMA: cma_listen_on_dev, error %d, listening on device %s\n",
-			ret, cma_dev->device->name);
+		dev_warn(&cma_dev->device->dev,
+			 "RDMA CMA: cma_listen_on_dev, error %d\n", ret);
 }
 
 static void cma_listen_on_all(struct rdma_id_private *id_priv)
@@ -4063,9 +4063,10 @@ static int cma_join_ib_multicast(struct rdma_id_private *id_priv,
 	    (!ib_sa_sendonly_fullmem_support(&sa_client,
 					     id_priv->id.device,
 					     id_priv->id.port_num))) {
-		pr_warn("RDMA CM: %s port %u Unable to multicast join\n"
-			"RDMA CM: SM doesn't support Send Only Full Member option\n",
-			id_priv->id.device->name, id_priv->id.port_num);
+		dev_warn(
+			&id_priv->id.device->dev,
+			"RDMA CM: port %u Unable to multicast join: SM doesn't support Send Only Full Member option\n",
+			id_priv->id.port_num);
 		return -EOPNOTSUPP;
 	}
 
* Unmerged path drivers/infiniband/core/device.c
diff --git a/drivers/infiniband/core/fmr_pool.c b/drivers/infiniband/core/fmr_pool.c
index 593fc31a7a26..815166389487 100644
--- a/drivers/infiniband/core/fmr_pool.c
+++ b/drivers/infiniband/core/fmr_pool.c
@@ -222,7 +222,7 @@ struct ib_fmr_pool *ib_create_fmr_pool(struct ib_pd             *pd,
 	device = pd->device;
 	if (!device->alloc_fmr    || !device->dealloc_fmr  ||
 	    !device->map_phys_fmr || !device->unmap_fmr) {
-		pr_info(PFX "Device %s does not support FMRs\n", device->name);
+		dev_info(&device->dev, "Device does not support FMRs\n");
 		return ERR_PTR(-ENOSYS);
 	}
 
diff --git a/drivers/infiniband/core/restrack.c b/drivers/infiniband/core/restrack.c
index 340f64074ccd..f708d469892d 100644
--- a/drivers/infiniband/core/restrack.c
+++ b/drivers/infiniband/core/restrack.c
@@ -40,8 +40,7 @@ void rdma_restrack_clean(struct rdma_restrack_root *res)
 
 	dev = container_of(res, struct ib_device, res);
 	pr_err("restrack: %s", CUT_HERE);
-	pr_err("restrack: BUG: RESTRACK detected leak of resources on %s\n",
-	       dev->name);
+	dev_err(&dev->dev, "BUG: RESTRACK detected leak of resources\n");
 	hash_for_each(res->hash, bkt, e, node) {
 		if (rdma_is_kernel_res(e)) {
 			owner = e->kern_name;
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index a3a988e6b429..1075ba71efcf 100644
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -1629,14 +1629,16 @@ static int _ib_modify_qp(struct ib_qp *qp, struct ib_qp_attr *attr,
 
 	if (rdma_ib_or_roce(qp->device, port)) {
 		if (attr_mask & IB_QP_RQ_PSN && attr->rq_psn & ~0xffffff) {
-			pr_warn("%s: %s rq_psn overflow, masking to 24 bits\n",
-				__func__, qp->device->name);
+			dev_warn(&qp->device->dev,
+				 "%s rq_psn overflow, masking to 24 bits\n",
+				 __func__);
 			attr->rq_psn &= 0xffffff;
 		}
 
 		if (attr_mask & IB_QP_SQ_PSN && attr->sq_psn & ~0xffffff) {
-			pr_warn("%s: %s sq_psn overflow, masking to 24 bits\n",
-				__func__, qp->device->name);
+			dev_warn(&qp->device->dev,
+				 " %s sq_psn overflow, masking to 24 bits\n",
+				 __func__);
 			attr->sq_psn &= 0xffffff;
 		}
 	}
