x86/process: Optimize TIF checks in __switch_to_xtra()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] process: Optimize TIF checks in __switch_to_xtra() (Waiman Long) [1671826]
Rebuild_FUZZ: 96.15%
commit-author Kyle Huey <me@kylehuey.com>
commit af8b3cd3934ec60f4c2a420d19a9d416554f140b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/af8b3cd3.failed

Help the compiler to avoid reevaluating the thread flags for each checked
bit by reordering the bit checks and providing an explicit xor for
evaluation.

With default defconfigs for each arch,

x86_64: arch/x86/kernel/process.o
text       data     bss     dec     hex
3056       8577      16   11649    2d81	Before
3024	   8577      16	  11617	   2d61	After

i386: arch/x86/kernel/process.o
text       data     bss     dec     hex
2957	   8673	      8	  11638	   2d76	Before
2925	   8673       8	  11606	   2d56	After

Originally-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Kyle Huey <khuey@kylehuey.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andy Lutomirski <luto@kernel.org>
Link: http://lkml.kernel.org/r/20170214081104.9244-2-khuey@kylehuey.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit af8b3cd3934ec60f4c2a420d19a9d416554f140b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/process.c
diff --cc arch/x86/kernel/process.c
index 5ad9710f0312,ea9ea2582dab..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -197,197 -182,61 +197,221 @@@ int set_tsc_mode(unsigned int val
  	return 0;
  }
  
++<<<<<<< HEAD
 +#ifdef CONFIG_SMP
 +
 +struct ssb_state {
 +	struct ssb_state	*shared_state;
 +	raw_spinlock_t		lock;
 +	unsigned int		disable_state;
 +	unsigned long		local_state;
 +};
 +
 +#define LSTATE_SSB	0
 +
 +static DEFINE_PER_CPU(struct ssb_state, ssb_state);
 +
 +void speculative_store_bypass_ht_init(void)
 +{
 +	struct ssb_state *st = this_cpu_ptr(&ssb_state);
 +	unsigned int this_cpu = smp_processor_id();
 +	unsigned int cpu;
 +
 +	st->local_state = 0;
 +
 +	/*
 +	 * Shared state setup happens once on the first bringup
 +	 * of the CPU. It's not destroyed on CPU hotunplug.
 +	 */
 +	if (st->shared_state)
 +		return;
 +
 +	raw_spin_lock_init(&st->lock);
 +
 +	/*
 +	 * Go over HT siblings and check whether one of them has set up the
 +	 * shared state pointer already.
 +	 */
 +	for_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {
 +		if (cpu == this_cpu)
 +			continue;
 +
 +		if (!per_cpu(ssb_state, cpu).shared_state)
 +			continue;
 +
 +		/* Link it to the state of the sibling: */
 +		st->shared_state = per_cpu(ssb_state, cpu).shared_state;
 +		return;
 +	}
 +
 +	/*
 +	 * First HT sibling to come up on the core.  Link shared state of
 +	 * the first HT sibling to itself. The siblings on the same core
 +	 * which come up later will see the shared state pointer and link
 +	 * themself to the state of this CPU.
 +	 */
 +	st->shared_state = st;
 +}
 +
 +/*
 + * Logic is: First HT sibling enables SSBD for both siblings in the core
 + * and last sibling to disable it, disables it for the whole core. This how
 + * MSR_SPEC_CTRL works in "hardware":
 + *
 + *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
 + */
 +static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
 +{
 +	struct ssb_state *st = this_cpu_ptr(&ssb_state);
 +	u64 msr = x86_amd_ls_cfg_base;
 +
 +	if (!static_cpu_has(X86_FEATURE_ZEN)) {
 +		msr |= ssbd_tif_to_amd_ls_cfg(tifn);
 +		wrmsrl(MSR_AMD64_LS_CFG, msr);
 +		return;
 +	}
 +
 +	if (tifn & _TIF_SSBD) {
 +		/*
 +		 * Since this can race with prctl(), block reentry on the
 +		 * same CPU.
 +		 */
 +		if (__test_and_set_bit(LSTATE_SSB, &st->local_state))
 +			return;
 +
 +		msr |= x86_amd_ls_cfg_ssbd_mask;
 +
 +		raw_spin_lock(&st->shared_state->lock);
 +		/* First sibling enables SSBD: */
 +		if (!st->shared_state->disable_state)
 +			wrmsrl(MSR_AMD64_LS_CFG, msr);
 +		st->shared_state->disable_state++;
 +		raw_spin_unlock(&st->shared_state->lock);
 +	} else {
 +		if (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))
 +			return;
 +
 +		raw_spin_lock(&st->shared_state->lock);
 +		st->shared_state->disable_state--;
 +		if (!st->shared_state->disable_state)
 +			wrmsrl(MSR_AMD64_LS_CFG, msr);
 +		raw_spin_unlock(&st->shared_state->lock);
 +	}
 +}
 +#else
 +static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
 +{
 +	u64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
 +
 +	wrmsrl(MSR_AMD64_LS_CFG, msr);
 +}
 +#endif
 +
 +static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
 +{
 +	/*
 +	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,
 +	 * so ssbd_tif_to_spec_ctrl() just works.
 +	 */
 +	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
 +}
 +
 +static __always_inline void intel_set_ssb_state(unsigned long tifn)
 +{
 +	spec_ctrl_set_ssbd(ssbd_tif_to_spec_ctrl(tifn));
 +	wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl_pcp.entry64));
 +}
 +
 +static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
 +{
 +	if (!static_key_false(&ssbd_userset_key))
 +		return;	/* Don't do anything if not user settable */
 +
 +	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
 +		amd_set_ssb_virt_state(tifn);
 +	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
 +		amd_set_core_ssb_state(tifn);
 +	else
 +		intel_set_ssb_state(tifn);
 +}
 +
 +void speculative_store_bypass_update(unsigned long tif)
 +{
 +	preempt_disable();
 +	__speculative_store_bypass_update(tif);
 +	preempt_enable();
 +}
 +EXPORT_SYMBOL_GPL(speculative_store_bypass_update);
 +
 +void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 +		      struct tss_struct *tss)
++=======
+ static inline void switch_to_bitmap(struct tss_struct *tss,
+ 				    struct thread_struct *prev,
+ 				    struct thread_struct *next,
+ 				    unsigned long tifp, unsigned long tifn)
++>>>>>>> af8b3cd3934e (x86/process: Optimize TIF checks in __switch_to_xtra())
  {
- 	struct thread_struct *prev, *next;
- 
- 	prev = &prev_p->thread;
- 	next = &next_p->thread;
- 
- 	if (test_tsk_thread_flag(prev_p, TIF_BLOCKSTEP) ^
- 	    test_tsk_thread_flag(next_p, TIF_BLOCKSTEP)) {
- 		unsigned long debugctl = get_debugctlmsr();
- 
- 		debugctl &= ~DEBUGCTLMSR_BTF;
- 		if (test_tsk_thread_flag(next_p, TIF_BLOCKSTEP))
- 			debugctl |= DEBUGCTLMSR_BTF;
- 
- 		update_debugctlmsr(debugctl);
- 	}
- 
- 	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
- 	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
- 		/* prev and next are different */
- 		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
- 			hard_disable_TSC();
- 		else
- 			hard_enable_TSC();
- 	}
- 
- 	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
+ 	if (tifn & _TIF_IO_BITMAP) {
  		/*
  		 * Copy the relevant range of the IO bitmap.
  		 * Normally this is 128 bytes or less:
  		 */
  		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
  		       max(prev->io_bitmap_max, next->io_bitmap_max));
++<<<<<<< HEAD
 +	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
++=======
+ 		/*
+ 		 * Make sure that the TSS limit is correct for the CPU
+ 		 * to notice the IO bitmap.
+ 		 */
+ 		refresh_tss_limit();
+ 	} else if (tifp & _TIF_IO_BITMAP) {
++>>>>>>> af8b3cd3934e (x86/process: Optimize TIF checks in __switch_to_xtra())
  		/*
  		 * Clear any possible leftover bits:
  		 */
  		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
  	}
+ }
+ 
+ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
+ 		      struct tss_struct *tss)
+ {
+ 	struct thread_struct *prev, *next;
+ 	unsigned long tifp, tifn;
+ 
+ 	prev = &prev_p->thread;
+ 	next = &next_p->thread;
+ 
+ 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
+ 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
+ 	switch_to_bitmap(tss, prev, next, tifp, tifn);
+ 
  	propagate_user_return_notify(prev_p, next_p);
  
++<<<<<<< HEAD
 +	if (test_tsk_thread_flag(prev_p, TIF_SSBD) ^
 +	    test_tsk_thread_flag(next_p, TIF_SSBD))
 +		__speculative_store_bypass_update(task_thread_info(next_p)->flags);
++=======
+ 	if ((tifp ^ tifn) & _TIF_BLOCKSTEP) {
+ 		unsigned long debugctl = get_debugctlmsr();
+ 
+ 		debugctl &= ~DEBUGCTLMSR_BTF;
+ 		if (tifn & _TIF_BLOCKSTEP)
+ 			debugctl |= DEBUGCTLMSR_BTF;
+ 		update_debugctlmsr(debugctl);
+ 	}
+ 
+ 	if ((tifp ^ tifn) & _TIF_NOTSC) {
+ 		if (tifn & _TIF_NOTSC)
+ 			hard_disable_TSC();
+ 		else
+ 			hard_enable_TSC();
+ 	}
++>>>>>>> af8b3cd3934e (x86/process: Optimize TIF checks in __switch_to_xtra())
  }
  
  /*
* Unmerged path arch/x86/kernel/process.c
