xdp: add MEM_TYPE_ZERO_COPY

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Björn Töpel <bjorn.topel@intel.com>
commit 02b55e5657c3a569fc681ba851e464cfa6b90d4f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/02b55e56.failed

Here, a new type of allocator support is added to the XDP return
API. A zero-copy allocated xdp_buff cannot be converted to an
xdp_frame. Instead is the buff has to be copied. This is not supported
at all in this commit.

Also, an opaque "handle" is added to xdp_buff. This can be used as a
context for the zero-copy allocator implementation.

	Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 02b55e5657c3a569fc681ba851e464cfa6b90d4f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xdp.h
#	net/core/xdp.c
diff --cc include/net/xdp.h
index 6ac69520ed7c,2deea7166a34..000000000000
--- a/include/net/xdp.h
+++ b/include/net/xdp.h
@@@ -36,13 -36,26 +36,27 @@@
  enum xdp_mem_type {
  	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
  	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
++<<<<<<< HEAD
++=======
+ 	MEM_TYPE_PAGE_POOL,
+ 	MEM_TYPE_ZERO_COPY,
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
  	MEM_TYPE_MAX,
  };
  
 -/* XDP flags for ndo_xdp_xmit */
 -#define XDP_XMIT_FLUSH		(1U << 0)	/* doorbell signal consumer */
 -#define XDP_XMIT_FLAGS_MASK	XDP_XMIT_FLUSH
 -
  struct xdp_mem_info {
  	u32 type; /* enum xdp_mem_type, but known size type */
 -	u32 id;
  };
  
++<<<<<<< HEAD
++=======
+ struct page_pool;
+ 
+ struct zero_copy_allocator {
+ 	void (*free)(struct zero_copy_allocator *zca, unsigned long handle);
+ };
+ 
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
  struct xdp_rxq_info {
  	struct net_device *dev;
  	u32 queue_index;
@@@ -73,14 -88,38 +88,44 @@@ struct xdp_frame 
  static inline
  struct xdp_frame *convert_to_xdp_frame(struct xdp_buff *xdp)
  {
++<<<<<<< HEAD
 +	return NULL;
++=======
+ 	struct xdp_frame *xdp_frame;
+ 	int metasize;
+ 	int headroom;
+ 
+ 	/* TODO: implement clone, copy, use "native" MEM_TYPE */
+ 	if (xdp->rxq->mem.type == MEM_TYPE_ZERO_COPY)
+ 		return NULL;
+ 
+ 	/* Assure headroom is available for storing info */
+ 	headroom = xdp->data - xdp->data_hard_start;
+ 	metasize = xdp->data - xdp->data_meta;
+ 	metasize = metasize > 0 ? metasize : 0;
+ 	if (unlikely((headroom - metasize) < sizeof(*xdp_frame)))
+ 		return NULL;
+ 
+ 	/* Store info in top of packet */
+ 	xdp_frame = xdp->data_hard_start;
+ 
+ 	xdp_frame->data = xdp->data;
+ 	xdp_frame->len  = xdp->data_end - xdp->data;
+ 	xdp_frame->headroom = headroom - sizeof(*xdp_frame);
+ 	xdp_frame->metasize = metasize;
+ 
+ 	/* rxq only valid until napi_schedule ends, convert to xdp_mem_info */
+ 	xdp_frame->mem = xdp->rxq->mem;
+ 
+ 	return xdp_frame;
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
  }
  
 -void xdp_return_frame(struct xdp_frame *xdpf);
 -void xdp_return_frame_rx_napi(struct xdp_frame *xdpf);
 -void xdp_return_buff(struct xdp_buff *xdp);
 +static inline
 +void xdp_return_frame(struct xdp_frame *xdpf)
 +{
 +	return;
 +}
  
  int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
  		     struct net_device *dev, u32 queue_index);
diff --cc net/core/xdp.c
index e553510efc2e,9d1f22072d5d..000000000000
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@@ -13,9 -17,121 +13,111 @@@
  #define REG_STATE_UNREGISTERED	0x2
  #define REG_STATE_UNUSED	0x3
  
++<<<<<<< HEAD
++=======
+ static DEFINE_IDA(mem_id_pool);
+ static DEFINE_MUTEX(mem_id_lock);
+ #define MEM_ID_MAX 0xFFFE
+ #define MEM_ID_MIN 1
+ static int mem_id_next = MEM_ID_MIN;
+ 
+ static bool mem_id_init; /* false */
+ static struct rhashtable *mem_id_ht;
+ 
+ struct xdp_mem_allocator {
+ 	struct xdp_mem_info mem;
+ 	union {
+ 		void *allocator;
+ 		struct page_pool *page_pool;
+ 		struct zero_copy_allocator *zc_alloc;
+ 	};
+ 	struct rhash_head node;
+ 	struct rcu_head rcu;
+ };
+ 
+ static u32 xdp_mem_id_hashfn(const void *data, u32 len, u32 seed)
+ {
+ 	const u32 *k = data;
+ 	const u32 key = *k;
+ 
+ 	BUILD_BUG_ON(FIELD_SIZEOF(struct xdp_mem_allocator, mem.id)
+ 		     != sizeof(u32));
+ 
+ 	/* Use cyclic increasing ID as direct hash key, see rht_bucket_index */
+ 	return key << RHT_HASH_RESERVED_SPACE;
+ }
+ 
+ static int xdp_mem_id_cmp(struct rhashtable_compare_arg *arg,
+ 			  const void *ptr)
+ {
+ 	const struct xdp_mem_allocator *xa = ptr;
+ 	u32 mem_id = *(u32 *)arg->key;
+ 
+ 	return xa->mem.id != mem_id;
+ }
+ 
+ static const struct rhashtable_params mem_id_rht_params = {
+ 	.nelem_hint = 64,
+ 	.head_offset = offsetof(struct xdp_mem_allocator, node),
+ 	.key_offset  = offsetof(struct xdp_mem_allocator, mem.id),
+ 	.key_len = FIELD_SIZEOF(struct xdp_mem_allocator, mem.id),
+ 	.max_size = MEM_ID_MAX,
+ 	.min_size = 8,
+ 	.automatic_shrinking = true,
+ 	.hashfn    = xdp_mem_id_hashfn,
+ 	.obj_cmpfn = xdp_mem_id_cmp,
+ };
+ 
+ static void __xdp_mem_allocator_rcu_free(struct rcu_head *rcu)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 
+ 	xa = container_of(rcu, struct xdp_mem_allocator, rcu);
+ 
+ 	/* Allow this ID to be reused */
+ 	ida_simple_remove(&mem_id_pool, xa->mem.id);
+ 
+ 	/* Notice, driver is expected to free the *allocator,
+ 	 * e.g. page_pool, and MUST also use RCU free.
+ 	 */
+ 
+ 	/* Poison memory */
+ 	xa->mem.id = 0xFFFF;
+ 	xa->mem.type = 0xF0F0;
+ 	xa->allocator = (void *)0xDEAD9001;
+ 
+ 	kfree(xa);
+ }
+ 
+ static void __xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 	int id = xdp_rxq->mem.id;
+ 	int err;
+ 
+ 	if (id == 0)
+ 		return;
+ 
+ 	mutex_lock(&mem_id_lock);
+ 
+ 	xa = rhashtable_lookup(mem_id_ht, &id, mem_id_rht_params);
+ 	if (!xa) {
+ 		mutex_unlock(&mem_id_lock);
+ 		return;
+ 	}
+ 
+ 	err = rhashtable_remove_fast(mem_id_ht, &xa->node, mem_id_rht_params);
+ 	WARN_ON(err);
+ 
+ 	call_rcu(&xa->rcu, __xdp_mem_allocator_rcu_free);
+ 
+ 	mutex_unlock(&mem_id_lock);
+ }
+ 
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
  void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq)
  {
 -	/* Simplify driver cleanup code paths, allow unreg "unused" */
 -	if (xdp_rxq->reg_state == REG_STATE_UNUSED)
 -		return;
 -
 -	WARN(!(xdp_rxq->reg_state == REG_STATE_REGISTERED), "Driver BUG");
 -
 -	__xdp_rxq_info_unreg_mem_model(xdp_rxq);
 -
 -	xdp_rxq->reg_state = REG_STATE_UNREGISTERED;
 -	xdp_rxq->dev = NULL;
 -
 -	/* Reset mem info to defaults */
 -	xdp_rxq->mem.id = 0;
 -	xdp_rxq->mem.type = 0;
 +	return;
  }
  EXPORT_SYMBOL_GPL(xdp_rxq_info_unreg);
  
@@@ -42,6 -246,127 +144,129 @@@ EXPORT_SYMBOL_GPL(xdp_rxq_info_is_reg)
  int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
  			       enum xdp_mem_type type, void *allocator)
  {
++<<<<<<< HEAD
++=======
+ 	struct xdp_mem_allocator *xdp_alloc;
+ 	gfp_t gfp = GFP_KERNEL;
+ 	int id, errno, ret;
+ 	void *ptr;
+ 
+ 	if (xdp_rxq->reg_state != REG_STATE_REGISTERED) {
+ 		WARN(1, "Missing register, driver bug");
+ 		return -EFAULT;
+ 	}
+ 
+ 	if (!__is_supported_mem_type(type))
+ 		return -EOPNOTSUPP;
+ 
+ 	xdp_rxq->mem.type = type;
+ 
+ 	if (!allocator) {
+ 		if (type == MEM_TYPE_PAGE_POOL || type == MEM_TYPE_ZERO_COPY)
+ 			return -EINVAL; /* Setup time check page_pool req */
+ 		return 0;
+ 	}
+ 
+ 	/* Delay init of rhashtable to save memory if feature isn't used */
+ 	if (!mem_id_init) {
+ 		mutex_lock(&mem_id_lock);
+ 		ret = __mem_id_init_hash_table();
+ 		mutex_unlock(&mem_id_lock);
+ 		if (ret < 0) {
+ 			WARN_ON(1);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	xdp_alloc = kzalloc(sizeof(*xdp_alloc), gfp);
+ 	if (!xdp_alloc)
+ 		return -ENOMEM;
+ 
+ 	mutex_lock(&mem_id_lock);
+ 	id = __mem_id_cyclic_get(gfp);
+ 	if (id < 0) {
+ 		errno = id;
+ 		goto err;
+ 	}
+ 	xdp_rxq->mem.id = id;
+ 	xdp_alloc->mem  = xdp_rxq->mem;
+ 	xdp_alloc->allocator = allocator;
+ 
+ 	/* Insert allocator into ID lookup table */
+ 	ptr = rhashtable_insert_slow(mem_id_ht, &id, &xdp_alloc->node);
+ 	if (IS_ERR(ptr)) {
+ 		errno = PTR_ERR(ptr);
+ 		goto err;
+ 	}
+ 
+ 	mutex_unlock(&mem_id_lock);
+ 
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
  	return 0;
 -err:
 -	mutex_unlock(&mem_id_lock);
 -	kfree(xdp_alloc);
 -	return errno;
  }
  EXPORT_SYMBOL_GPL(xdp_rxq_info_reg_mem_model);
++<<<<<<< HEAD
++=======
+ 
+ /* XDP RX runs under NAPI protection, and in different delivery error
+  * scenarios (e.g. queue full), it is possible to return the xdp_frame
+  * while still leveraging this protection.  The @napi_direct boolian
+  * is used for those calls sites.  Thus, allowing for faster recycling
+  * of xdp_frames/pages in those cases.
+  */
+ static void __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,
+ 			 unsigned long handle)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 	struct page *page;
+ 
+ 	switch (mem->type) {
+ 	case MEM_TYPE_PAGE_POOL:
+ 		rcu_read_lock();
+ 		/* mem->id is valid, checked in xdp_rxq_info_reg_mem_model() */
+ 		xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);
+ 		page = virt_to_head_page(data);
+ 		if (xa)
+ 			page_pool_put_page(xa->page_pool, page, napi_direct);
+ 		else
+ 			put_page(page);
+ 		rcu_read_unlock();
+ 		break;
+ 	case MEM_TYPE_PAGE_SHARED:
+ 		page_frag_free(data);
+ 		break;
+ 	case MEM_TYPE_PAGE_ORDER0:
+ 		page = virt_to_page(data); /* Assumes order0 page*/
+ 		put_page(page);
+ 		break;
+ 	case MEM_TYPE_ZERO_COPY:
+ 		/* NB! Only valid from an xdp_buff! */
+ 		rcu_read_lock();
+ 		/* mem->id is valid, checked in xdp_rxq_info_reg_mem_model() */
+ 		xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);
+ 		xa->zc_alloc->free(xa->zc_alloc, handle);
+ 		rcu_read_unlock();
+ 	default:
+ 		/* Not possible, checked in xdp_rxq_info_reg_mem_model() */
+ 		break;
+ 	}
+ }
+ 
+ void xdp_return_frame(struct xdp_frame *xdpf)
+ {
+ 	__xdp_return(xdpf->data, &xdpf->mem, false, 0);
+ }
+ EXPORT_SYMBOL_GPL(xdp_return_frame);
+ 
+ void xdp_return_frame_rx_napi(struct xdp_frame *xdpf)
+ {
+ 	__xdp_return(xdpf->data, &xdpf->mem, true, 0);
+ }
+ EXPORT_SYMBOL_GPL(xdp_return_frame_rx_napi);
+ 
+ void xdp_return_buff(struct xdp_buff *xdp)
+ {
+ 	__xdp_return(xdp->data, &xdp->rxq->mem, true, xdp->handle);
+ }
+ EXPORT_SYMBOL_GPL(xdp_return_buff);
++>>>>>>> 02b55e5657c3 (xdp: add MEM_TYPE_ZERO_COPY)
* Unmerged path include/net/xdp.h
* Unmerged path net/core/xdp.c
