net/mlx5e: Replace call to MPWQE free with dealloc in interface down flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Replace call to MPWQE free with dealloc in interface down flow (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 97.18%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit afab995e06ee1fb76de195a4fba9d03267f8dbe3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/afab995e.failed

No need to expose the MPWQE free function to control path.
The dealloc function already exposed, use it.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit afab995e06ee1fb76de195a4fba9d03267f8dbe3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index d500f9aa1bd7,dc9aa070eafa..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -848,12 -892,18 +848,27 @@@ bool mlx5e_post_rx_wqes(struct mlx5e_r
  bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
  void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
++<<<<<<< HEAD
 +void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 +
 +u8 mlx5e_mpwqe_get_log_stride_size(struct mlx5_core_dev *mdev,
 +				   struct mlx5e_params *params);
 +u8 mlx5e_mpwqe_get_log_num_strides(struct mlx5_core_dev *mdev,
 +				   struct mlx5e_params *params);
++=======
+ struct sk_buff *
+ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
+ 				u16 cqe_bcnt, u32 head_offset, u32 page_idx);
+ struct sk_buff *
+ mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
+ 				   u16 cqe_bcnt, u32 head_offset, u32 page_idx);
+ struct sk_buff *
+ mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+ 			  struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
+ struct sk_buff *
+ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+ 			     struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
++>>>>>>> afab995e06ee (net/mlx5e: Replace call to MPWQE free with dealloc in interface down flow)
  
  void mlx5e_update_stats(struct mlx5e_priv *priv);
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a0bfa9c3ce47,c214b23e14ba..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -748,29 -872,33 +748,54 @@@ static void mlx5e_free_rx_descs(struct 
  	__be16 wqe_ix_be;
  	u16 wqe_ix;
  
 -	if (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ) {
 -		struct mlx5_wq_ll *wq = &rq->mpwqe.wq;
 -
 +	/* UMR WQE (if in progress) is always at wq->head */
 +	if (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ &&
 +	    rq->mpwqe.umr_in_progress)
 +		mlx5e_free_rx_mpwqe(rq, &rq->mpwqe.info[wq->head]);
 +
++<<<<<<< HEAD
 +	while (!mlx5_wq_ll_is_empty(wq)) {
 +		wqe_ix_be = *wq->tail_next;
 +		wqe_ix    = be16_to_cpu(wqe_ix_be);
 +		wqe       = mlx5_wq_ll_get_wqe(&rq->wq, wqe_ix);
 +		rq->dealloc_wqe(rq, wqe_ix);
 +		mlx5_wq_ll_pop(&rq->wq, wqe_ix_be,
 +			       &wqe->next.next_wqe_index);
++=======
+ 		/* UMR WQE (if in progress) is always at wq->head */
+ 		if (rq->mpwqe.umr_in_progress)
+ 			rq->dealloc_wqe(rq, wq->head);
+ 
+ 		while (!mlx5_wq_ll_is_empty(wq)) {
+ 			struct mlx5e_rx_wqe_ll *wqe;
+ 
+ 			wqe_ix_be = *wq->tail_next;
+ 			wqe_ix    = be16_to_cpu(wqe_ix_be);
+ 			wqe       = mlx5_wq_ll_get_wqe(wq, wqe_ix);
+ 			rq->dealloc_wqe(rq, wqe_ix);
+ 			mlx5_wq_ll_pop(wq, wqe_ix_be,
+ 				       &wqe->next.next_wqe_index);
+ 		}
+ 	} else {
+ 		struct mlx5_wq_cyc *wq = &rq->wqe.wq;
+ 
+ 		while (!mlx5_wq_cyc_is_empty(wq)) {
+ 			wqe_ix = mlx5_wq_cyc_get_tail(wq);
+ 			rq->dealloc_wqe(rq, wqe_ix);
+ 			mlx5_wq_cyc_pop(wq);
+ 		}
++>>>>>>> afab995e06ee (net/mlx5e: Replace call to MPWQE free with dealloc in interface down flow)
  	}
  
 +	if (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST && rq->wqe.page_reuse) {
 +		/* Clean outstanding pages on handled WQEs that decided to do page-reuse,
 +		 * but yet to be re-posted.
 +		 */
 +		int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
 +
 +		for (wqe_ix = 0; wqe_ix < wq_sz; wqe_ix++)
 +			rq->dealloc_wqe(rq, wqe_ix);
 +	}
  }
  
  static int mlx5e_open_rq(struct mlx5e_channel *c,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 3631c4f3022b,e6b3d178c45f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -355,38 -396,63 +355,39 @@@ mlx5e_copy_skb_header_mpwqe(struct devi
  	}
  }
  
 -static void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi)
 -{
 -	const bool no_xdp_xmit =
 -		bitmap_empty(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 -	struct mlx5e_dma_info *dma_info = wi->umr.dma_info;
 -	int i;
 -
 -	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++)
 -		if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
 -			mlx5e_page_release(rq, &dma_info[i], true);
 -}
 -
 -static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
++<<<<<<< HEAD
 +static inline void mlx5e_post_umr_wqe(struct mlx5e_rq *rq, u16 ix)
  {
 -	struct mlx5_wq_ll *wq = &rq->mpwqe.wq;
 -	struct mlx5e_rx_wqe_ll *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
 -
 -	rq->mpwqe.umr_in_progress = false;
 -
 -	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 -
 -	/* ensure wqes are visible to device before updating doorbell record */
 -	dma_wmb();
 -
 -	mlx5_wq_ll_update_db_record(wq);
 -}
 -
 -static inline u16 mlx5e_icosq_wrap_cnt(struct mlx5e_icosq *sq)
 -{
 -	return sq->pc >> MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 -}
 -
 -static inline void mlx5e_fill_icosq_frag_edge(struct mlx5e_icosq *sq,
 -					      struct mlx5_wq_cyc *wq,
 -					      u16 pi, u16 frag_pi)
 -{
 -	struct mlx5e_sq_wqe_info *edge_wi, *wi = &sq->db.ico_wqe[pi];
 -	u8 nnops = mlx5_wq_cyc_get_frag_size(wq) - frag_pi;
 -
 -	edge_wi = wi + nnops;
 +	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
 +	struct mlx5e_icosq *sq = &rq->channel->icosq;
 +	struct mlx5_wq_cyc *wq = &sq->wq;
 +	struct mlx5e_umr_wqe *wqe;
 +	u8 num_wqebbs = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_BB);
 +	u16 pi;
  
 -	/* fill sq frag edge with nops to avoid wqe wrapping two pages */
 -	for (; wi < edge_wi; wi++) {
 -		wi->opcode = MLX5_OPCODE_NOP;
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_NOP;
  		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
  	}
 +
 +	wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 +	memcpy(wqe, &wi->umr.wqe, sizeof(*wqe));
 +	wqe->ctrl.opmod_idx_opcode =
 +		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 +			    MLX5_OPCODE_UMR);
 +
 +	sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_UMR;
 +	sq->pc += num_wqebbs;
 +	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &wqe->ctrl);
  }
  
 -static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 +static int mlx5e_alloc_rx_umr_mpwqe(struct mlx5e_rq *rq,
 +				    u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
 +	int pg_strides = mlx5e_mpwqe_strides_per_page(rq);
  	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
 -	struct mlx5e_icosq *sq = &rq->channel->icosq;
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5e_umr_wqe *umr_wqe;
 -	u16 xlt_offset = ix << (MLX5E_LOG_ALIGNED_MPWQE_PPW - 1);
 -	u16 pi, frag_pi;
  	int err;
  	int i;
  
@@@ -413,47 -502,6 +414,50 @@@ err_unmap
  	return err;
  }
  
 +void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi)
++=======
++static void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi)
++>>>>>>> afab995e06ee (net/mlx5e: Replace call to MPWQE free with dealloc in interface down flow)
 +{
 +	int pg_strides = mlx5e_mpwqe_strides_per_page(rq);
 +	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
 +	int i;
 +
 +	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++) {
 +		page_ref_sub(dma_info->page, pg_strides - wi->skbs_frags[i]);
 +		mlx5e_page_release(rq, dma_info, true);
 +	}
 +}
 +
 +static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
 +{
 +	struct mlx5_wq_ll *wq = &rq->wq;
 +	struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
 +
 +	rq->mpwqe.umr_in_progress = false;
 +
 +	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 +
 +	/* ensure wqes are visible to device before updating doorbell record */
 +	dma_wmb();
 +
 +	mlx5_wq_ll_update_db_record(wq);
 +}
 +
 +static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 +{
 +	int err;
 +
 +	err = mlx5e_alloc_rx_umr_mpwqe(rq, ix);
 +	if (unlikely(err)) {
 +		rq->stats->buff_alloc_err++;
 +		return err;
 +	}
 +	rq->mpwqe.umr_in_progress = true;
 +	mlx5e_post_umr_wqe(rq, ix);
 +	return 0;
 +}
 +
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
