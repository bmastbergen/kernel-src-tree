svcrdma: Avoid releasing a page in svc_xprt_release()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit a53d5cb0646a12586ae45c892c7a411d47ee1a1d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/a53d5cb0.failed

svc_xprt_release() invokes svc_free_res_pages(), which releases
pages between rq_respages and rq_next_page.

Historically, the RPC/RDMA transport has set these two pointers to
be different by one, which means:

- one page gets released when svc_recv returns 0. This normally
happens whenever one or more RDMA Reads need to be dispatched to
complete construction of an RPC Call.

- one page gets released after every call to svc_send.

In both cases, this released page is immediately refilled by
svc_alloc_arg. There does not seem to be a reason for releasing this
page.

To avoid this unnecessary memory allocator traffic, set rq_next_page
more carefully.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit a53d5cb0646a12586ae45c892c7a411d47ee1a1d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
diff --cc net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 2eed6e104513,ddb7def48b12..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@@ -108,61 -108,264 +108,72 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
 -static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc);
 -
 -static inline struct svc_rdma_recv_ctxt *
 -svc_rdma_next_recv_ctxt(struct list_head *list)
 -{
 -	return list_first_entry_or_null(list, struct svc_rdma_recv_ctxt,
 -					rc_list);
 -}
 -
 -static struct svc_rdma_recv_ctxt *
 -svc_rdma_recv_ctxt_alloc(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 -	dma_addr_t addr;
 -	void *buffer;
 -
 -	ctxt = kmalloc(sizeof(*ctxt), GFP_KERNEL);
 -	if (!ctxt)
 -		goto fail0;
 -	buffer = kmalloc(rdma->sc_max_req_size, GFP_KERNEL);
 -	if (!buffer)
 -		goto fail1;
 -	addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
 -				 rdma->sc_max_req_size, DMA_FROM_DEVICE);
 -	if (ib_dma_mapping_error(rdma->sc_pd->device, addr))
 -		goto fail2;
 -
 -	ctxt->rc_recv_wr.next = NULL;
 -	ctxt->rc_recv_wr.wr_cqe = &ctxt->rc_cqe;
 -	ctxt->rc_recv_wr.sg_list = &ctxt->rc_recv_sge;
 -	ctxt->rc_recv_wr.num_sge = 1;
 -	ctxt->rc_cqe.done = svc_rdma_wc_receive;
 -	ctxt->rc_recv_sge.addr = addr;
 -	ctxt->rc_recv_sge.length = rdma->sc_max_req_size;
 -	ctxt->rc_recv_sge.lkey = rdma->sc_pd->local_dma_lkey;
 -	ctxt->rc_recv_buf = buffer;
 -	ctxt->rc_temp = false;
 -	return ctxt;
 -
 -fail2:
 -	kfree(buffer);
 -fail1:
 -	kfree(ctxt);
 -fail0:
 -	return NULL;
 -}
 -
 -static void svc_rdma_recv_ctxt_destroy(struct svcxprt_rdma *rdma,
 -				       struct svc_rdma_recv_ctxt *ctxt)
 -{
 -	ib_dma_unmap_single(rdma->sc_pd->device, ctxt->rc_recv_sge.addr,
 -			    ctxt->rc_recv_sge.length, DMA_FROM_DEVICE);
 -	kfree(ctxt->rc_recv_buf);
 -	kfree(ctxt);
 -}
 -
 -/**
 - * svc_rdma_recv_ctxts_destroy - Release all recv_ctxt's for an xprt
 - * @rdma: svcxprt_rdma being torn down
 - *
 - */
 -void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 -
 -	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts))) {
 -		list_del(&ctxt->rc_list);
 -		svc_rdma_recv_ctxt_destroy(rdma, ctxt);
 -	}
 -}
 -
 -static struct svc_rdma_recv_ctxt *
 -svc_rdma_recv_ctxt_get(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 -
 -	spin_lock(&rdma->sc_recv_lock);
 -	ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts);
 -	if (!ctxt)
 -		goto out_empty;
 -	list_del(&ctxt->rc_list);
 -	spin_unlock(&rdma->sc_recv_lock);
 -
 -out:
 -	ctxt->rc_page_count = 0;
 -	return ctxt;
 -
 -out_empty:
 -	spin_unlock(&rdma->sc_recv_lock);
 -
 -	ctxt = svc_rdma_recv_ctxt_alloc(rdma);
 -	if (!ctxt)
 -		return NULL;
 -	goto out;
 -}
 -
 -/**
 - * svc_rdma_recv_ctxt_put - Return recv_ctxt to free list
 - * @rdma: controlling svcxprt_rdma
 - * @ctxt: object to return to the free list
 - *
 - */
 -void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
 -			    struct svc_rdma_recv_ctxt *ctxt)
 -{
 -	unsigned int i;
 -
 -	for (i = 0; i < ctxt->rc_page_count; i++)
 -		put_page(ctxt->rc_pages[i]);
 -
 -	if (!ctxt->rc_temp) {
 -		spin_lock(&rdma->sc_recv_lock);
 -		list_add(&ctxt->rc_list, &rdma->sc_recv_ctxts);
 -		spin_unlock(&rdma->sc_recv_lock);
 -	} else
 -		svc_rdma_recv_ctxt_destroy(rdma, ctxt);
 -}
 -
 -static int __svc_rdma_post_recv(struct svcxprt_rdma *rdma,
 -				struct svc_rdma_recv_ctxt *ctxt)
 -{
 -	struct ib_recv_wr *bad_recv_wr;
 -	int ret;
 -
 -	svc_xprt_get(&rdma->sc_xprt);
 -	ret = ib_post_recv(rdma->sc_qp, &ctxt->rc_recv_wr, &bad_recv_wr);
 -	trace_svcrdma_post_recv(&ctxt->rc_recv_wr, ret);
 -	if (ret)
 -		goto err_post;
 -	return 0;
 -
 -err_post:
 -	svc_rdma_recv_ctxt_put(rdma, ctxt);
 -	svc_xprt_put(&rdma->sc_xprt);
 -	return ret;
 -}
 -
 -static int svc_rdma_post_recv(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 -
 -	ctxt = svc_rdma_recv_ctxt_get(rdma);
 -	if (!ctxt)
 -		return -ENOMEM;
 -	return __svc_rdma_post_recv(rdma, ctxt);
 -}
 -
 -/**
 - * svc_rdma_post_recvs - Post initial set of Recv WRs
 - * @rdma: fresh svcxprt_rdma
 - *
 - * Returns true if successful, otherwise false.
 +/*
 + * Replace the pages in the rq_argpages array with the pages from the SGE in
 + * the RDMA_RECV completion. The SGL should contain full pages up until the
 + * last one.
   */
 -bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma)
 +static void svc_rdma_build_arg_xdr(struct svc_rqst *rqstp,
 +				   struct svc_rdma_op_ctxt *ctxt)
  {
 -	struct svc_rdma_recv_ctxt *ctxt;
 -	unsigned int i;
 -	int ret;
 +	struct page *page;
 +	int sge_no;
 +	u32 len;
  
 -	for (i = 0; i < rdma->sc_max_requests; i++) {
 -		ctxt = svc_rdma_recv_ctxt_get(rdma);
 -		if (!ctxt)
 -			return false;
 -		ctxt->rc_temp = true;
 -		ret = __svc_rdma_post_recv(rdma, ctxt);
 -		if (ret) {
 -			pr_err("svcrdma: failure posting recv buffers: %d\n",
 -			       ret);
 -			return false;
 -		}
++<<<<<<< HEAD
 +	/* The reply path assumes the Call's transport header resides
 +	 * in rqstp->rq_pages[0].
 +	 */
 +	page = ctxt->pages[0];
 +	put_page(rqstp->rq_pages[0]);
 +	rqstp->rq_pages[0] = page;
 +
 +	/* Set up the XDR head */
 +	rqstp->rq_arg.head[0].iov_base = page_address(page);
 +	rqstp->rq_arg.head[0].iov_len =
 +		min_t(size_t, ctxt->byte_len, ctxt->sge[0].length);
 +	rqstp->rq_arg.len = ctxt->byte_len;
 +	rqstp->rq_arg.buflen = ctxt->byte_len;
 +
 +	/* Compute bytes past head in the SGL */
 +	len = ctxt->byte_len - rqstp->rq_arg.head[0].iov_len;
 +
 +	/* If data remains, store it in the pagelist */
 +	rqstp->rq_arg.page_len = len;
 +	rqstp->rq_arg.page_base = 0;
 +
 +	sge_no = 1;
 +	while (len && sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no];
 +		put_page(rqstp->rq_pages[sge_no]);
 +		rqstp->rq_pages[sge_no] = page;
 +		len -= min_t(u32, len, ctxt->sge[sge_no].length);
 +		sge_no++;
  	}
 -	return true;
 -}
 -
 -/**
 - * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC
 - * @cq: Completion Queue context
 - * @wc: Work Completion object
 - *
 - * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
 - * the Receive completion handler could be running.
 - */
 -static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)
 -{
 -	struct svcxprt_rdma *rdma = cq->cq_context;
 -	struct ib_cqe *cqe = wc->wr_cqe;
 -	struct svc_rdma_recv_ctxt *ctxt;
 -
 -	trace_svcrdma_wc_receive(wc);
 -
 -	/* WARNING: Only wc->wr_cqe and wc->status are reliable */
 -	ctxt = container_of(cqe, struct svc_rdma_recv_ctxt, rc_cqe);
 -
 -	if (wc->status != IB_WC_SUCCESS)
 -		goto flushed;
 -
 -	if (svc_rdma_post_recv(rdma))
 -		goto post_err;
 -
 -	/* All wc fields are now known to be valid */
 -	ctxt->rc_byte_len = wc->byte_len;
 -	ib_dma_sync_single_for_cpu(rdma->sc_pd->device,
 -				   ctxt->rc_recv_sge.addr,
 -				   wc->byte_len, DMA_FROM_DEVICE);
 -
 -	spin_lock(&rdma->sc_rq_dto_lock);
 -	list_add_tail(&ctxt->rc_list, &rdma->sc_rq_dto_q);
 -	spin_unlock(&rdma->sc_rq_dto_lock);
 -	set_bit(XPT_DATA, &rdma->sc_xprt.xpt_flags);
 -	if (!test_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags))
 -		svc_xprt_enqueue(&rdma->sc_xprt);
 -	goto out;
 -
 -flushed:
 -	if (wc->status != IB_WC_WR_FLUSH_ERR)
 -		pr_err("svcrdma: Recv: %s (%u/0x%x)\n",
 -		       ib_wc_status_msg(wc->status),
 -		       wc->status, wc->vendor_err);
 -post_err:
 -	svc_rdma_recv_ctxt_put(rdma, ctxt);
 -	set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
 -	svc_xprt_enqueue(&rdma->sc_xprt);
 -out:
 -	svc_xprt_put(&rdma->sc_xprt);
 -}
 -
 -/**
 - * svc_rdma_flush_recv_queues - Drain pending Receive work
 - * @rdma: svcxprt_rdma being shut down
 - *
 - */
 -void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 +	rqstp->rq_respages = &rqstp->rq_pages[sge_no];
 +	rqstp->rq_next_page = rqstp->rq_respages + 1;
  
 -	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_read_complete_q))) {
 -		list_del(&ctxt->rc_list);
 -		svc_rdma_recv_ctxt_put(rdma, ctxt);
 +	/* If not all pages were used from the SGL, free the remaining ones */
 +	len = sge_no;
 +	while (sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no++];
 +		put_page(page);
  	}
 -	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_rq_dto_q))) {
 -		list_del(&ctxt->rc_list);
 -		svc_rdma_recv_ctxt_put(rdma, ctxt);
 -	}
 -}
 -
 -static void svc_rdma_build_arg_xdr(struct svc_rqst *rqstp,
 -				   struct svc_rdma_recv_ctxt *ctxt)
 -{
 -	struct xdr_buf *arg = &rqstp->rq_arg;
 +	ctxt->count = len;
  
 +	/* Set up tail */
 +	rqstp->rq_arg.tail[0].iov_base = NULL;
 +	rqstp->rq_arg.tail[0].iov_len = 0;
++=======
+ 	arg->head[0].iov_base = ctxt->rc_recv_buf;
+ 	arg->head[0].iov_len = ctxt->rc_byte_len;
+ 	arg->tail[0].iov_base = NULL;
+ 	arg->tail[0].iov_len = 0;
+ 	arg->page_len = 0;
+ 	arg->page_base = 0;
+ 	arg->buflen = ctxt->rc_byte_len;
+ 	arg->len = ctxt->rc_byte_len;
++>>>>>>> a53d5cb0646a (svcrdma: Avoid releasing a page in svc_xprt_release())
  }
  
  /* This accommodates the largest possible Write chunk,
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
diff --git a/net/sunrpc/xprtrdma/svc_rdma_sendto.c b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
index 00a497c70989..ed3c2449880a 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_sendto.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
@@ -461,7 +461,9 @@ static void svc_rdma_save_io_pages(struct svc_rqst *rqstp,
 		ctxt->pages[i + 1] = rqstp->rq_respages[i];
 		rqstp->rq_respages[i] = NULL;
 	}
-	rqstp->rq_next_page = rqstp->rq_respages + 1;
+
+	/* Prevent svc_xprt_release from releasing pages in rq_pages */
+	rqstp->rq_next_page = rqstp->rq_respages;
 }
 
 /**
