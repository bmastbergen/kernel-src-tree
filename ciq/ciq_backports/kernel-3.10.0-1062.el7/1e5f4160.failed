svcrdma: Simplify svc_rdma_recv_ctxt_put

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 1e5f4160745690a0476929d128a336cae95c1df9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/1e5f4160.failed

Currently svc_rdma_recv_ctxt_put's callers have to know whether they
want to free the ctxt's pages or not. This means the human
developers have to know when and why to set that free_pages
argument.

Instead, the ctxt should carry that information with it so that
svc_rdma_recv_ctxt_put does the right thing no matter who is
calling.

We want to keep track of the number of pages in the Receive buffer
separately from the number of pages pulled over by RDMA Read. This
is so that the correct number of pages can be freed properly and
that number is well-documented.

So now, rc_hdr_count is the number of pages consumed by head[0]
(ie., the page index where the Read chunk should start); and
rc_page_count is always the number of pages that need to be released
when the ctxt is put.

The @free_pages argument is no longer needed.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit 1e5f4160745690a0476929d128a336cae95c1df9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sunrpc/svc_rdma.h
#	net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
#	net/sunrpc/xprtrdma/svc_rdma_rw.c
diff --cc include/linux/sunrpc/svc_rdma.h
index 88da0c9bd7b1,f0bd0b6d8931..000000000000
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@@ -155,6 -170,11 +155,14 @@@ extern int svc_rdma_handle_bc_reply(str
  				    struct xdr_buf *rcvbuf);
  
  /* svc_rdma_recvfrom.c */
++<<<<<<< HEAD
++=======
+ extern void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma);
+ extern bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma);
+ extern void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
+ 				   struct svc_rdma_recv_ctxt *ctxt);
+ extern void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  extern int svc_rdma_recvfrom(struct svc_rqst *);
  
  /* svc_rdma_rw.c */
diff --cc net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 330d542fd96e,ecfe7c90a268..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@@ -108,6 -108,237 +108,240 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
++<<<<<<< HEAD
++=======
+ static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc);
+ 
+ static inline struct svc_rdma_recv_ctxt *
+ svc_rdma_next_recv_ctxt(struct list_head *list)
+ {
+ 	return list_first_entry_or_null(list, struct svc_rdma_recv_ctxt,
+ 					rc_list);
+ }
+ 
+ /**
+  * svc_rdma_recv_ctxts_destroy - Release all recv_ctxt's for an xprt
+  * @rdma: svcxprt_rdma being torn down
+  *
+  */
+ void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts))) {
+ 		list_del(&ctxt->rc_list);
+ 		kfree(ctxt);
+ 	}
+ }
+ 
+ static struct svc_rdma_recv_ctxt *
+ svc_rdma_recv_ctxt_get(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	spin_lock(&rdma->sc_recv_lock);
+ 	ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts);
+ 	if (!ctxt)
+ 		goto out_empty;
+ 	list_del(&ctxt->rc_list);
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ out:
+ 	ctxt->rc_recv_wr.num_sge = 0;
+ 	ctxt->rc_page_count = 0;
+ 	return ctxt;
+ 
+ out_empty:
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ 	ctxt = kmalloc(sizeof(*ctxt), GFP_KERNEL);
+ 	if (!ctxt)
+ 		return NULL;
+ 	goto out;
+ }
+ 
+ static void svc_rdma_recv_ctxt_unmap(struct svcxprt_rdma *rdma,
+ 				     struct svc_rdma_recv_ctxt *ctxt)
+ {
+ 	struct ib_device *device = rdma->sc_cm_id->device;
+ 	int i;
+ 
+ 	for (i = 0; i < ctxt->rc_recv_wr.num_sge; i++)
+ 		ib_dma_unmap_page(device,
+ 				  ctxt->rc_sges[i].addr,
+ 				  ctxt->rc_sges[i].length,
+ 				  DMA_FROM_DEVICE);
+ }
+ 
+ /**
+  * svc_rdma_recv_ctxt_put - Return recv_ctxt to free list
+  * @rdma: controlling svcxprt_rdma
+  * @ctxt: object to return to the free list
+  *
+  */
+ void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
+ 			    struct svc_rdma_recv_ctxt *ctxt)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ctxt->rc_page_count; i++)
+ 		put_page(ctxt->rc_pages[i]);
+ 	spin_lock(&rdma->sc_recv_lock);
+ 	list_add(&ctxt->rc_list, &rdma->sc_recv_ctxts);
+ 	spin_unlock(&rdma->sc_recv_lock);
+ }
+ 
+ static int svc_rdma_post_recv(struct svcxprt_rdma *rdma)
+ {
+ 	struct ib_device *device = rdma->sc_cm_id->device;
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 	struct ib_recv_wr *bad_recv_wr;
+ 	int sge_no, buflen, ret;
+ 	struct page *page;
+ 	dma_addr_t pa;
+ 
+ 	ctxt = svc_rdma_recv_ctxt_get(rdma);
+ 	if (!ctxt)
+ 		return -ENOMEM;
+ 
+ 	buflen = 0;
+ 	ctxt->rc_cqe.done = svc_rdma_wc_receive;
+ 	for (sge_no = 0; buflen < rdma->sc_max_req_size; sge_no++) {
+ 		if (sge_no >= rdma->sc_max_sge) {
+ 			pr_err("svcrdma: Too many sges (%d)\n", sge_no);
+ 			goto err_put_ctxt;
+ 		}
+ 
+ 		page = alloc_page(GFP_KERNEL);
+ 		if (!page)
+ 			goto err_put_ctxt;
+ 		ctxt->rc_pages[sge_no] = page;
+ 		ctxt->rc_page_count++;
+ 
+ 		pa = ib_dma_map_page(device, ctxt->rc_pages[sge_no],
+ 				     0, PAGE_SIZE, DMA_FROM_DEVICE);
+ 		if (ib_dma_mapping_error(device, pa))
+ 			goto err_put_ctxt;
+ 		ctxt->rc_sges[sge_no].addr = pa;
+ 		ctxt->rc_sges[sge_no].length = PAGE_SIZE;
+ 		ctxt->rc_sges[sge_no].lkey = rdma->sc_pd->local_dma_lkey;
+ 		ctxt->rc_recv_wr.num_sge++;
+ 
+ 		buflen += PAGE_SIZE;
+ 	}
+ 	ctxt->rc_recv_wr.next = NULL;
+ 	ctxt->rc_recv_wr.sg_list = &ctxt->rc_sges[0];
+ 	ctxt->rc_recv_wr.wr_cqe = &ctxt->rc_cqe;
+ 
+ 	svc_xprt_get(&rdma->sc_xprt);
+ 	ret = ib_post_recv(rdma->sc_qp, &ctxt->rc_recv_wr, &bad_recv_wr);
+ 	trace_svcrdma_post_recv(&ctxt->rc_recv_wr, ret);
+ 	if (ret)
+ 		goto err_post;
+ 	return 0;
+ 
+ err_put_ctxt:
+ 	svc_rdma_recv_ctxt_unmap(rdma, ctxt);
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	return -ENOMEM;
+ err_post:
+ 	svc_rdma_recv_ctxt_unmap(rdma, ctxt);
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	svc_xprt_put(&rdma->sc_xprt);
+ 	return ret;
+ }
+ 
+ /**
+  * svc_rdma_post_recvs - Post initial set of Recv WRs
+  * @rdma: fresh svcxprt_rdma
+  *
+  * Returns true if successful, otherwise false.
+  */
+ bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma)
+ {
+ 	unsigned int i;
+ 	int ret;
+ 
+ 	for (i = 0; i < rdma->sc_max_requests; i++) {
+ 		ret = svc_rdma_post_recv(rdma);
+ 		if (ret) {
+ 			pr_err("svcrdma: failure posting recv buffers: %d\n",
+ 			       ret);
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ /**
+  * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC
+  * @cq: Completion Queue context
+  * @wc: Work Completion object
+  *
+  * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
+  * the Receive completion handler could be running.
+  */
+ static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct svcxprt_rdma *rdma = cq->cq_context;
+ 	struct ib_cqe *cqe = wc->wr_cqe;
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	trace_svcrdma_wc_receive(wc);
+ 
+ 	/* WARNING: Only wc->wr_cqe and wc->status are reliable */
+ 	ctxt = container_of(cqe, struct svc_rdma_recv_ctxt, rc_cqe);
+ 	svc_rdma_recv_ctxt_unmap(rdma, ctxt);
+ 
+ 	if (wc->status != IB_WC_SUCCESS)
+ 		goto flushed;
+ 
+ 	if (svc_rdma_post_recv(rdma))
+ 		goto post_err;
+ 
+ 	/* All wc fields are now known to be valid */
+ 	ctxt->rc_byte_len = wc->byte_len;
+ 	spin_lock(&rdma->sc_rq_dto_lock);
+ 	list_add_tail(&ctxt->rc_list, &rdma->sc_rq_dto_q);
+ 	spin_unlock(&rdma->sc_rq_dto_lock);
+ 	set_bit(XPT_DATA, &rdma->sc_xprt.xpt_flags);
+ 	if (!test_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags))
+ 		svc_xprt_enqueue(&rdma->sc_xprt);
+ 	goto out;
+ 
+ flushed:
+ 	if (wc->status != IB_WC_WR_FLUSH_ERR)
+ 		pr_err("svcrdma: Recv: %s (%u/0x%x)\n",
+ 		       ib_wc_status_msg(wc->status),
+ 		       wc->status, wc->vendor_err);
+ post_err:
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 	svc_xprt_enqueue(&rdma->sc_xprt);
+ out:
+ 	svc_xprt_put(&rdma->sc_xprt);
+ }
+ 
+ /**
+  * svc_rdma_flush_recv_queues - Drain pending Receive work
+  * @rdma: svcxprt_rdma being shut down
+  *
+  */
+ void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_read_complete_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_rq_dto_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ }
+ 
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  /*
   * Replace the pages in the rq_argpages array with the pages from the SGE in
   * the RDMA_RECV completion. The SGL should contain full pages up until the
@@@ -142,23 -373,26 +376,35 @@@ static void svc_rdma_build_arg_xdr(stru
  	rqstp->rq_arg.page_base = 0;
  
  	sge_no = 1;
 -	while (len && sge_no < ctxt->rc_recv_wr.num_sge) {
 -		page = ctxt->rc_pages[sge_no];
 +	while (len && sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no];
  		put_page(rqstp->rq_pages[sge_no]);
  		rqstp->rq_pages[sge_no] = page;
 -		len -= min_t(u32, len, ctxt->rc_sges[sge_no].length);
 +		len -= min_t(u32, len, ctxt->sge[sge_no].length);
  		sge_no++;
  	}
+ 	ctxt->rc_hdr_count = sge_no;
  	rqstp->rq_respages = &rqstp->rq_pages[sge_no];
  	rqstp->rq_next_page = rqstp->rq_respages + 1;
  
  	/* If not all pages were used from the SGL, free the remaining ones */
++<<<<<<< HEAD
 +	len = sge_no;
 +	while (sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no++];
 +		put_page(page);
 +	}
 +	ctxt->count = len;
++=======
+ 	while (sge_no < ctxt->rc_recv_wr.num_sge) {
+ 		page = ctxt->rc_pages[sge_no++];
+ 		put_page(page);
+ 	}
+ 
+ 	/* @ctxt's pages have all been released or moved to @rqstp->rq_pages.
+ 	 */
+ 	ctxt->rc_page_count = 0;
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  
  	/* Set up tail */
  	rqstp->rq_arg.tail[0].iov_base = NULL;
@@@ -368,15 -602,18 +614,23 @@@ static void rdma_read_complete(struct s
  {
  	int page_no;
  
++<<<<<<< HEAD
 +	/* Copy RPC pages */
 +	for (page_no = 0; page_no < head->count; page_no++) {
++=======
+ 	/* Move Read chunk pages to rqstp so that they will be released
+ 	 * when svc_process is done with them.
+ 	 */
+ 	for (page_no = 0; page_no < head->rc_page_count; page_no++) {
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  		put_page(rqstp->rq_pages[page_no]);
 -		rqstp->rq_pages[page_no] = head->rc_pages[page_no];
 +		rqstp->rq_pages[page_no] = head->pages[page_no];
  	}
+ 	head->rc_page_count = 0;
  
  	/* Point rq_arg.pages past header */
 -	rqstp->rq_arg.pages = &rqstp->rq_pages[head->rc_hdr_count];
 -	rqstp->rq_arg.page_len = head->rc_arg.page_len;
 +	rqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];
 +	rqstp->rq_arg.page_len = head->arg.page_len;
  
  	/* rq_respages starts after the last arg page */
  	rqstp->rq_respages = &rqstp->rq_pages[page_no];
@@@ -545,7 -780,7 +799,11 @@@ int svc_rdma_recvfrom(struct svc_rqst *
  	if (svc_rdma_is_backchannel_reply(xprt, p)) {
  		ret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt, p,
  					       &rqstp->rq_arg);
++<<<<<<< HEAD
 +		svc_rdma_put_context(ctxt, 0);
++=======
+ 		svc_rdma_recv_ctxt_put(rdma_xprt, ctxt);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  		return ret;
  	}
  
@@@ -554,7 -789,7 +812,11 @@@
  		goto out_readchunk;
  
  complete:
++<<<<<<< HEAD
 +	svc_rdma_put_context(ctxt, 0);
++=======
+ 	svc_rdma_recv_ctxt_put(rdma_xprt, ctxt);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  	rqstp->rq_prot = IPPROTO_MAX;
  	svc_xprt_copy_addrs(rqstp, xprt);
  	return rqstp->rq_arg.len;
@@@ -567,16 -802,16 +829,28 @@@ out_readchunk
  
  out_err:
  	svc_rdma_send_error(rdma_xprt, p, ret);
++<<<<<<< HEAD
 +	svc_rdma_put_context(ctxt, 0);
++=======
+ 	svc_rdma_recv_ctxt_put(rdma_xprt, ctxt);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  	return 0;
  
  out_postfail:
  	if (ret == -EINVAL)
  		svc_rdma_send_error(rdma_xprt, p, ret);
++<<<<<<< HEAD
 +	svc_rdma_put_context(ctxt, 1);
 +	return ret;
 +
 +out_drop:
 +	svc_rdma_put_context(ctxt, 1);
++=======
+ 	svc_rdma_recv_ctxt_put(rdma_xprt, ctxt);
+ 	return ret;
+ 
+ out_drop:
+ 	svc_rdma_recv_ctxt_put(rdma_xprt, ctxt);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  	return 0;
  }
diff --cc net/sunrpc/xprtrdma/svc_rdma_rw.c
index 506b9ec4883d,8242aa318ac1..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_rw.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_rw.c
@@@ -281,10 -282,10 +281,14 @@@ static void svc_rdma_wc_read_done(struc
  			pr_err("svcrdma: read ctx: %s (%u/0x%x)\n",
  			       ib_wc_status_msg(wc->status),
  			       wc->status, wc->vendor_err);
++<<<<<<< HEAD
 +		svc_rdma_put_context(info->ri_readctxt, 1);
++=======
+ 		svc_rdma_recv_ctxt_put(rdma, info->ri_readctxt);
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  	} else {
  		spin_lock(&rdma->sc_rq_dto_lock);
 -		list_add_tail(&info->ri_readctxt->rc_list,
 +		list_add_tail(&info->ri_readctxt->list,
  			      &rdma->sc_read_complete_q);
  		spin_unlock(&rdma->sc_rq_dto_lock);
  
@@@ -830,17 -831,17 +834,28 @@@ int svc_rdma_recv_read_chunk(struct svc
  	int ret;
  
  	/* The request (with page list) is constructed in
 -	 * head->rc_arg. Pages involved with RDMA Read I/O are
 +	 * head->arg. Pages involved with RDMA Read I/O are
  	 * transferred there.
  	 */
++<<<<<<< HEAD
 +	head->hdr_count = head->count;
 +	head->arg.head[0] = rqstp->rq_arg.head[0];
 +	head->arg.tail[0] = rqstp->rq_arg.tail[0];
 +	head->arg.pages = head->pages;
 +	head->arg.page_base = 0;
 +	head->arg.page_len = 0;
 +	head->arg.len = rqstp->rq_arg.len;
 +	head->arg.buflen = rqstp->rq_arg.buflen;
++=======
+ 	head->rc_page_count = head->rc_hdr_count;
+ 	head->rc_arg.head[0] = rqstp->rq_arg.head[0];
+ 	head->rc_arg.tail[0] = rqstp->rq_arg.tail[0];
+ 	head->rc_arg.pages = head->rc_pages;
+ 	head->rc_arg.page_base = 0;
+ 	head->rc_arg.page_len = 0;
+ 	head->rc_arg.len = rqstp->rq_arg.len;
+ 	head->rc_arg.buflen = rqstp->rq_arg.buflen;
++>>>>>>> 1e5f41607456 (svcrdma: Simplify svc_rdma_recv_ctxt_put)
  
  	info = svc_rdma_read_info_alloc(rdma);
  	if (!info)
* Unmerged path include/linux/sunrpc/svc_rdma.h
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_rw.c
