mm: add new mmget() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Vegard Nossum <vegard.nossum@oracle.com>
commit 3fce371bfac2be0396ffc1e763600e6c6b1bb52a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/3fce371b.failed

Apart from adding the helper function itself, the rest of the kernel is
converted mechanically using:

  git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_users);/mmget\(\1\);/'
  git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_users);/mmget\(\&\1\);/'

This is needed for a later patch that hooks into the helper, but might
be a worthwhile cleanup on its own.

(Michal Hocko provided most of the kerneldoc comment.)

Link: http://lkml.kernel.org/r/20161218123229.22952-2-vegard.nossum@oracle.com
	Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: David Rientjes <rientjes@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3fce371bfac2be0396ffc1e763600e6c6b1bb52a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/kernel/smp.c
#	arch/blackfin/mach-common/smp.c
#	arch/metag/kernel/smp.c
#	arch/sh/kernel/smp.c
#	arch/xtensa/kernel/smp.c
#	include/linux/sched.h
diff --cc arch/arc/kernel/smp.c
index 5c7fd603d216,b8e8d3944481..000000000000
--- a/arch/arc/kernel/smp.c
+++ b/arch/arc/kernel/smp.c
@@@ -125,9 -139,17 +125,14 @@@ void __cpuinit start_kernel_secondary(v
  	/* MMU, Caches, Vector Table, Interrupts etc */
  	setup_processor();
  
++<<<<<<< HEAD
 +	atomic_inc(&mm->mm_users);
 +	atomic_inc(&mm->mm_count);
++=======
+ 	mmget(mm);
+ 	mmgrab(mm);
++>>>>>>> 3fce371bfac2 (mm: add new mmget() helper)
  	current->active_mm = mm;
 -	cpumask_set_cpu(cpu, mm_cpumask(mm));
 -
 -	/* Some SMP H/w setup - for each cpu */
 -	if (plat_smp_ops.init_per_cpu)
 -		plat_smp_ops.init_per_cpu(cpu);
 -
 -	if (machine_desc->init_per_cpu)
 -		machine_desc->init_per_cpu(cpu);
  
  	notify_cpu_starting(cpu);
  	set_cpu_online(cpu, true);
diff --cc arch/blackfin/mach-common/smp.c
index 1bc2ce6f3c94,a2e6db2ce811..000000000000
--- a/arch/blackfin/mach-common/smp.c
+++ b/arch/blackfin/mach-common/smp.c
@@@ -310,8 -307,8 +310,13 @@@ void __cpuinit secondary_start_kernel(v
  	local_irq_disable();
  
  	/* Attach the new idle task to the global mm. */
++<<<<<<< HEAD
 +	atomic_inc(&mm->mm_users);
 +	atomic_inc(&mm->mm_count);
++=======
+ 	mmget(mm);
+ 	mmgrab(mm);
++>>>>>>> 3fce371bfac2 (mm: add new mmget() helper)
  	current->active_mm = mm;
  
  	preempt_disable();
diff --cc arch/metag/kernel/smp.c
index f443ec9a7cbe,c622293254e4..000000000000
--- a/arch/metag/kernel/smp.c
+++ b/arch/metag/kernel/smp.c
@@@ -357,8 -344,8 +357,13 @@@ asmlinkage void secondary_start_kernel(
  	 * All kernel threads share the same mm context; grab a
  	 * reference and switch to it.
  	 */
++<<<<<<< HEAD
 +	atomic_inc(&mm->mm_users);
 +	atomic_inc(&mm->mm_count);
++=======
+ 	mmget(mm);
+ 	mmgrab(mm);
++>>>>>>> 3fce371bfac2 (mm: add new mmget() helper)
  	current->active_mm = mm;
  	cpumask_set_cpu(cpu, mm_cpumask(mm));
  	enter_lazy_tlb(mm, current);
diff --cc arch/sh/kernel/smp.c
index 45696451f0ea,edc4769b047e..000000000000
--- a/arch/sh/kernel/smp.c
+++ b/arch/sh/kernel/smp.c
@@@ -180,11 -178,13 +180,16 @@@ asmlinkage void __cpuinit start_seconda
  	struct mm_struct *mm = &init_mm;
  
  	enable_mmu();
++<<<<<<< HEAD
 +	atomic_inc(&mm->mm_count);
 +	atomic_inc(&mm->mm_users);
++=======
+ 	mmgrab(mm);
+ 	mmget(mm);
++>>>>>>> 3fce371bfac2 (mm: add new mmget() helper)
  	current->active_mm = mm;
 -#ifdef CONFIG_MMU
  	enter_lazy_tlb(mm, current);
  	local_flush_tlb_all();
 -#endif
  
  	per_cpu_trap_init();
  
diff --cc include/linux/sched.h
index 83cd9508a135,4a28deb5f210..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -2612,6 -2934,41 +2612,44 @@@ static inline void mmdrop(struct mm_str
  		__mmdrop(mm);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void mmdrop_async_fn(struct work_struct *work)
+ {
+ 	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
+ 	__mmdrop(mm);
+ }
+ 
+ static inline void mmdrop_async(struct mm_struct *mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+ 		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+ 		schedule_work(&mm->async_put_work);
+ 	}
+ }
+ 
+ /**
+  * mmget() - Pin the address space associated with a &struct mm_struct.
+  * @mm: The address space to pin.
+  *
+  * Make sure that the address space of the given &struct mm_struct doesn't
+  * go away. This does not protect against parts of the address space being
+  * modified or freed, however.
+  *
+  * Never use this function to pin this address space for an
+  * unbounded/indefinite amount of time.
+  *
+  * Use mmput() to release the reference acquired by mmget().
+  *
+  * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+  * of &mm_struct.mm_count vs &mm_struct.mm_users.
+  */
+ static inline void mmget(struct mm_struct *mm)
+ {
+ 	atomic_inc(&mm->mm_users);
+ }
+ 
++>>>>>>> 3fce371bfac2 (mm: add new mmget() helper)
  static inline bool mmget_not_zero(struct mm_struct *mm)
  {
  	return atomic_inc_not_zero(&mm->mm_users);
* Unmerged path arch/xtensa/kernel/smp.c
* Unmerged path arch/arc/kernel/smp.c
* Unmerged path arch/blackfin/mach-common/smp.c
diff --git a/arch/frv/mm/mmu-context.c b/arch/frv/mm/mmu-context.c
index 81757d55a5b5..3473bde77f56 100644
--- a/arch/frv/mm/mmu-context.c
+++ b/arch/frv/mm/mmu-context.c
@@ -188,7 +188,7 @@ int cxn_pin_by_pid(pid_t pid)
 		task_lock(tsk);
 		if (tsk->mm) {
 			mm = tsk->mm;
-			atomic_inc(&mm->mm_users);
+			mmget(mm);
 			ret = 0;
 		}
 		task_unlock(tsk);
* Unmerged path arch/metag/kernel/smp.c
* Unmerged path arch/sh/kernel/smp.c
* Unmerged path arch/xtensa/kernel/smp.c
* Unmerged path include/linux/sched.h
diff --git a/kernel/fork.c b/kernel/fork.c
index 9bff3b28c357..17eadddd3c19 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -799,7 +799,7 @@ struct mm_struct *get_task_mm(struct task_struct *task)
 		if (task->flags & PF_KTHREAD)
 			mm = NULL;
 		else
-			atomic_inc(&mm->mm_users);
+			mmget(mm);
 	}
 	task_unlock(task);
 	return mm;
@@ -1005,7 +1005,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 		return 0;
 
 	if (clone_flags & CLONE_VM) {
-		atomic_inc(&oldmm->mm_users);
+		mmget(oldmm);
 		mm = oldmm;
 		goto good_mm;
 	}
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 8cf0784a0684..71e6dfb145c9 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1673,7 +1673,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 	 * that.
 	 */
 	start_mm = &init_mm;
-	atomic_inc(&init_mm.mm_users);
+	mmget(&init_mm);
 
 	/*
 	 * Keep on scanning until all entries have gone.  Usually,
@@ -1722,7 +1722,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 		if (atomic_read(&start_mm->mm_users) == 1) {
 			mmput(start_mm);
 			start_mm = &init_mm;
-			atomic_inc(&init_mm.mm_users);
+			mmget(&init_mm);
 		}
 
 		/*
@@ -1759,8 +1759,8 @@ int try_to_unuse(unsigned int type, bool frontswap,
 			struct mm_struct *prev_mm = start_mm;
 			struct mm_struct *mm;
 
-			atomic_inc(&new_start_mm->mm_users);
-			atomic_inc(&prev_mm->mm_users);
+			mmget(new_start_mm);
+			mmget(prev_mm);
 			spin_lock(&mmlist_lock);
 			while (swap_count(*swap_map) && !retval &&
 					(p = p->next) != &start_mm->mmlist) {
@@ -1783,7 +1783,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 
 				if (set_start_mm && *swap_map < swcount) {
 					mmput(new_start_mm);
-					atomic_inc(&mm->mm_users);
+					mmget(mm);
 					new_start_mm = mm;
 					set_start_mm = 0;
 				}
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index b825c80850c2..89e849fcfecc 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -181,7 +181,7 @@ int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
 	work->addr = hva;
 	work->arch = *arch;
 	work->mm = current->mm;
-	atomic_inc(&work->mm->mm_users);
+	mmget(work->mm);
 	kvm_get_kvm(work->vcpu->kvm);
 
 	/* this can't really happen otherwise gfn_to_pfn_async
