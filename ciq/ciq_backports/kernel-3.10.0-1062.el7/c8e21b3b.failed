net/mlx5: EQ, Create all EQs in one place

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5: EQ, Create all EQs in one place (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 94.87%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit c8e21b3b576b78fe1b07522aea046af2634a24e8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/c8e21b3b.failed

Instead of creating the EQ table in three steps at driver load,
 - allocate irq vectors
 - allocate async EQs
 - allocate completion EQs
Gather all of the procedures into one function in eq.c and call it from
driver load.

This will help us reduce the EQ and EQ table private structures
visibility to eq.c in downstream refactoring.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit c8e21b3b576b78fe1b07522aea046af2634a24e8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index 9a3af24e5f23,44ccd4206104..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -815,7 -820,9 +815,13 @@@ void mlx5_eq_cleanup(struct mlx5_core_d
  	mlx5_eq_debugfs_cleanup(dev);
  }
  
++<<<<<<< HEAD
 +int mlx5_start_eqs(struct mlx5_core_dev *dev)
++=======
+ /* Async EQs */
+ 
+ static int create_async_eqs(struct mlx5_core_dev *dev)
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  {
  	struct mlx5_eq_table *table = &dev->priv.eq_table;
  	u64 async_event_mask = MLX5_ASYNC_EVENT_MASK;
@@@ -938,12 -945,282 +944,293 @@@ static void destroy_async_eqs(struct ml
  			      err);
  }
  
++<<<<<<< HEAD
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen)
 +{
 +	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {0};
 +
 +	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
 +	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
++=======
+ /* Completion EQs */
+ 
+ static int set_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 
+ 	if (!zalloc_cpumask_var(&priv->irq_info[vecidx].mask, GFP_KERNEL)) {
+ 		mlx5_core_warn(mdev, "zalloc_cpumask_var failed");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	cpumask_set_cpu(cpumask_local_spread(i, priv->numa_node),
+ 			priv->irq_info[vecidx].mask);
+ 
+ 	if (IS_ENABLED(CONFIG_SMP) &&
+ 	    irq_set_affinity_hint(irq, priv->irq_info[vecidx].mask))
+ 		mlx5_core_warn(mdev, "irq_set_affinity_hint failed, irq 0x%.4x", irq);
+ 
+ 	return 0;
+ }
+ 
+ static void clear_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 
+ 	irq_set_affinity_hint(irq, NULL);
+ 	free_cpumask_var(priv->irq_info[vecidx].mask);
+ }
+ 
+ static int set_comp_irq_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table.num_comp_vectors; i++) {
+ 		err = set_comp_irq_affinity_hint(mdev, i);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return 0;
+ 
+ err_out:
+ 	for (i--; i >= 0; i--)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ 
+ 	return err;
+ }
+ 
+ static void clear_comp_irqs_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table.num_comp_vectors; i++)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ }
+ 
+ static void destroy_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq *eq, *n;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (dev->rmap) {
+ 		free_irq_cpu_rmap(dev->rmap);
+ 		dev->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		list_del(&eq->list);
+ 		if (mlx5_destroy_unmap_eq(dev, eq))
+ 			mlx5_core_warn(dev, "failed to destroy EQ 0x%x\n",
+ 				       eq->eqn);
+ 		kfree(eq);
+ 	}
+ }
+ 
+ static int create_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	struct mlx5_eq *eq;
+ 	int ncomp_vec;
+ 	int nent;
+ 	int err;
+ 	int i;
+ 
+ 	INIT_LIST_HEAD(&table->comp_eqs_list);
+ 	ncomp_vec = table->num_comp_vectors;
+ 	nent = MLX5_COMP_EQ_SIZE;
+ #ifdef CONFIG_RFS_ACCEL
+ 	dev->rmap = alloc_irq_cpu_rmap(ncomp_vec);
+ 	if (!dev->rmap)
+ 		return -ENOMEM;
+ #endif
+ 	for (i = 0; i < ncomp_vec; i++) {
+ 		int vecidx = i + MLX5_EQ_VEC_COMP_BASE;
+ 
+ 		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
+ 		if (!eq) {
+ 			err = -ENOMEM;
+ 			goto clean;
+ 		}
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 		irq_cpu_rmap_add(dev->rmap, pci_irq_vector(dev->pdev, vecidx));
+ #endif
+ 		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
+ 		err = mlx5_create_map_eq(dev, eq, vecidx, nent, 0,
+ 					 name, MLX5_EQ_TYPE_COMP);
+ 		if (err) {
+ 			kfree(eq);
+ 			goto clean;
+ 		}
+ 		mlx5_core_dbg(dev, "allocated completion EQN %d\n", eq->eqn);
+ 		/* add tail, to keep the list ordered, for mlx5_vector2eqn to work */
+ 		list_add_tail(&eq->list, &table->comp_eqs_list);
+ 	}
+ 
+ 	err = set_comp_irq_affinity_hints(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to alloc affinity hint cpumask\n");
+ 		goto clean;
+ 	}
+ 
+ 	return 0;
+ 
+ clean:
+ 	destroy_comp_eqs(dev);
+ 	return err;
+ }
+ 
+ int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
+ 		    unsigned int *irqn)
+ {
+ 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq *eq, *n;
+ 	int err = -ENOENT;
+ 	int i = 0;
+ 
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		if (i++ == vector) {
+ 			*eqn = eq->eqn;
+ 			*irqn = eq->irqn;
+ 			err = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_vector2eqn);
+ 
+ struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn)
+ {
+ 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq *eq;
+ 
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list) {
+ 		if (eq->eqn == eqn)
+ 			return eq;
+ 	}
+ 
+ 	return ERR_PTR(-ENOENT);
+ }
+ 
+ /* This function should only be called after mlx5_cmd_force_teardown_hca */
+ void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = &dev->priv.eq_table;
+ 	struct mlx5_eq *eq;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (dev->rmap) {
+ 		free_irq_cpu_rmap(dev->rmap);
+ 		dev->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list)
+ 		free_irq(eq->irqn, eq);
+ 
+ 	free_irq(table->pages_eq.irqn, &table->pages_eq);
+ 	free_irq(table->async_eq.irqn, &table->async_eq);
+ 	free_irq(table->cmd_eq.irqn, &table->cmd_eq);
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	if (MLX5_CAP_GEN(dev, pg))
+ 		free_irq(table->pfault_eq.irqn, &table->pfault_eq);
+ #endif
+ 	pci_free_irq_vectors(dev->pdev);
+ }
+ 
+ static int alloc_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 	struct mlx5_eq_table *table = &priv->eq_table;
+ 	int num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?
+ 		      MLX5_CAP_GEN(dev, max_num_eqs) :
+ 		      1 << MLX5_CAP_GEN(dev, log_max_eq);
+ 	int nvec;
+ 	int err;
+ 
+ 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
+ 	       MLX5_EQ_VEC_COMP_BASE;
+ 	nvec = min_t(int, nvec, num_eqs);
+ 	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
+ 		return -ENOMEM;
+ 
+ 	priv->irq_info = kcalloc(nvec, sizeof(*priv->irq_info), GFP_KERNEL);
+ 	if (!priv->irq_info)
+ 		return -ENOMEM;
+ 
+ 	nvec = pci_alloc_irq_vectors(dev->pdev, MLX5_EQ_VEC_COMP_BASE + 1,
+ 				     nvec, PCI_IRQ_MSIX);
+ 	if (nvec < 0) {
+ 		err = nvec;
+ 		goto err_free_irq_info;
+ 	}
+ 
+ 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+ 
+ 	return 0;
+ 
+ err_free_irq_info:
+ 	kfree(priv->irq_info);
+ 	return err;
+ }
+ 
+ static void free_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 
+ 	pci_free_irq_vectors(dev->pdev);
+ 	kfree(priv->irq_info);
+ }
+ 
+ int mlx5_eq_table_create(struct mlx5_core_dev *dev)
+ {
+ 	int err;
+ 
+ 	err = alloc_irq_vectors(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "alloc irq vectors failed\n");
+ 		return err;
+ 	}
+ 
+ 	err = create_async_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create async EQs\n");
+ 		goto err_async_eqs;
+ 	}
+ 
+ 	err = create_comp_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create completion EQs\n");
+ 		goto err_comp_eqs;
+ 	}
+ 
+ 	return 0;
+ err_comp_eqs:
+ 	destroy_async_eqs(dev);
+ err_async_eqs:
+ 	free_irq_vectors(dev);
+ 	return err;
+ }
+ 
+ void mlx5_eq_table_destroy(struct mlx5_core_dev *dev)
+ {
+ 	destroy_comp_eqs(dev);
+ 	destroy_async_eqs(dev);
+ 	free_irq_vectors(dev);
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/main.c
index be9913773fa9,21cc9bbc2563..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@@ -317,49 -319,6 +317,52 @@@ static void release_bar(struct pci_dev 
  	pci_release_regions(pdev);
  }
  
++<<<<<<< HEAD
 +static int mlx5_alloc_irq_vectors(struct mlx5_core_dev *dev)
 +{
 +	struct mlx5_priv *priv = &dev->priv;
 +	struct mlx5_eq_table *table = &priv->eq_table;
 +	int num_eqs = 1 << MLX5_CAP_GEN(dev, log_max_eq);
 +	int nvec;
 +	int err;
 +
 +	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
 +	       MLX5_EQ_VEC_COMP_BASE;
 +	nvec = min_t(int, nvec, num_eqs);
 +	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
 +		return -ENOMEM;
 +
 +	priv->irq_info = kcalloc(nvec, sizeof(*priv->irq_info), GFP_KERNEL);
 +	if (!priv->irq_info)
 +		return -ENOMEM;
 +
 +	nvec = pci_alloc_irq_vectors(dev->pdev,
 +			MLX5_EQ_VEC_COMP_BASE + 1, nvec,
 +			PCI_IRQ_MSIX);
 +	if (nvec < 0) {
 +		err = nvec;
 +		goto err_free_irq_info;
 +	}
 +
 +	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
 +
 +	return 0;
 +
 +err_free_irq_info:
 +	kfree(priv->irq_info);
 +	return err;
 +}
 +
 +static void mlx5_free_irq_vectors(struct mlx5_core_dev *dev)
 +{
 +	struct mlx5_priv *priv = &dev->priv;
 +
 +	pci_free_irq_vectors(dev->pdev);
 +	kfree(priv->irq_info);
 +}
 +
++=======
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  struct mlx5_reg_host_endianness {
  	u8	he;
  	u8      rsvd[15];
@@@ -1200,18 -964,6 +1197,21 @@@ static int mlx5_load_one(struct mlx5_co
  		goto err_fw_tracer;
  	}
  
++<<<<<<< HEAD
 +	err = alloc_comp_eqs(dev);
 +	if (err) {
 +		dev_err(&pdev->dev, "Failed to alloc completion EQs\n");
 +		goto err_comp_eqs;
 +	}
 +
 +	err = mlx5_irq_set_affinity_hints(dev);
 +	if (err) {
 +		dev_err(&pdev->dev, "Failed to alloc affinity hint cpumask\n");
 +		goto err_affinity_hints;
 +	}
 +
++=======
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  	err = mlx5_fpga_device_start(dev);
  	if (err) {
  		dev_err(&pdev->dev, "fpga device start failed %d\n", err);
@@@ -1280,12 -1032,6 +1280,15 @@@ err_ipsec_start
  	mlx5_fpga_device_stop(dev);
  
  err_fpga_start:
++<<<<<<< HEAD
 +	mlx5_irq_clear_affinity_hints(dev);
 +
 +err_affinity_hints:
 +	free_comp_eqs(dev);
 +
 +err_comp_eqs:
++=======
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  	mlx5_fw_tracer_cleanup(dev->tracer);
  
  err_fw_tracer:
@@@ -1354,15 -1097,12 +1354,17 @@@ static int mlx5_unload_one(struct mlx5_
  	mlx5_accel_ipsec_cleanup(dev);
  	mlx5_accel_tls_cleanup(dev);
  	mlx5_fpga_device_stop(dev);
++<<<<<<< HEAD
 +	mlx5_irq_clear_affinity_hints(dev);
 +	free_comp_eqs(dev);
++=======
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  	mlx5_fw_tracer_cleanup(dev->tracer);
- 	mlx5_stop_eqs(dev);
+ 	mlx5_eq_table_destroy(dev);
  	mlx5_put_uars_page(dev, priv->uar);
- 	mlx5_free_irq_vectors(dev);
  	if (cleanup)
  		mlx5_cleanup_once(dev);
 -	mlx5_stop_health_poll(dev, cleanup);
 +	mlx5_stop_health_poll(dev);
  	err = mlx5_cmd_teardown_hca(dev);
  	if (err) {
  		dev_err(&dev->pdev->dev, "tear_down_hca failed, skip cleanup\n");
diff --cc drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 2ac07968015d,3fa6d26875fe..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@@ -129,10 -132,10 +129,17 @@@ int mlx5_create_map_eq(struct mlx5_core
  int mlx5_destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
  int mlx5_eq_add_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
  int mlx5_eq_del_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq);
++<<<<<<< HEAD
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen);
 +int mlx5_start_eqs(struct mlx5_core_dev *dev);
 +void mlx5_stop_eqs(struct mlx5_core_dev *dev);
++=======
+ int mlx5_eq_table_create(struct mlx5_core_dev *dev);
+ void mlx5_eq_table_destroy(struct mlx5_core_dev *dev);
+ /* This function should only be called after mlx5_cmd_force_teardown_hca */
+ void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev);
++>>>>>>> c8e21b3b576b (net/mlx5: EQ, Create all EQs in one place)
  struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
  u32 mlx5_eq_poll_irq_disabled(struct mlx5_eq *eq);
  void mlx5_cq_tasklet_cb(unsigned long data);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
index 7ecadb501743..8386caf6d9c9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
@@ -358,6 +358,16 @@ out:
 	return param;
 }
 
+static int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
+			      u32 *out, int outlen)
+{
+	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {};
+
+	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
+	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
+	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
+}
+
 static u64 eq_read_field(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 			 int index)
 {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
