svcrdma: Fix incorrect return value/type in svc_rdma_post_recvs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit af7fd74ec2434ec999160af32d10be17fbf225ed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/af7fd74e.failed

This crept in during the development process and wasn't caught
before I posted the "final" version.

	Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
Fixes: 0b2613c5883f ('svcrdma: Allocate recv_ctxt's on CPU ... ')
	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit af7fd74ec2434ec999160af32d10be17fbf225ed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
diff --cc net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 2eed6e104513,841fca143804..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@@ -108,61 -108,267 +108,248 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
 -static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc);
 -
 -static inline struct svc_rdma_recv_ctxt *
 -svc_rdma_next_recv_ctxt(struct list_head *list)
 -{
 -	return list_first_entry_or_null(list, struct svc_rdma_recv_ctxt,
 -					rc_list);
 -}
 -
 -static struct svc_rdma_recv_ctxt *
 -svc_rdma_recv_ctxt_alloc(struct svcxprt_rdma *rdma)
 -{
 -	struct svc_rdma_recv_ctxt *ctxt;
 -	dma_addr_t addr;
 -	void *buffer;
 -
 -	ctxt = kmalloc(sizeof(*ctxt), GFP_KERNEL);
 -	if (!ctxt)
 -		goto fail0;
 -	buffer = kmalloc(rdma->sc_max_req_size, GFP_KERNEL);
 -	if (!buffer)
 -		goto fail1;
 -	addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
 -				 rdma->sc_max_req_size, DMA_FROM_DEVICE);
 -	if (ib_dma_mapping_error(rdma->sc_pd->device, addr))
 -		goto fail2;
 -
 -	ctxt->rc_recv_wr.next = NULL;
 -	ctxt->rc_recv_wr.wr_cqe = &ctxt->rc_cqe;
 -	ctxt->rc_recv_wr.sg_list = &ctxt->rc_recv_sge;
 -	ctxt->rc_recv_wr.num_sge = 1;
 -	ctxt->rc_cqe.done = svc_rdma_wc_receive;
 -	ctxt->rc_recv_sge.addr = addr;
 -	ctxt->rc_recv_sge.length = rdma->sc_max_req_size;
 -	ctxt->rc_recv_sge.lkey = rdma->sc_pd->local_dma_lkey;
 -	ctxt->rc_recv_buf = buffer;
 -	ctxt->rc_temp = false;
 -	return ctxt;
 -
 -fail2:
 -	kfree(buffer);
 -fail1:
 -	kfree(ctxt);
 -fail0:
 -	return NULL;
 -}
 -
 -static void svc_rdma_recv_ctxt_destroy(struct svcxprt_rdma *rdma,
 -				       struct svc_rdma_recv_ctxt *ctxt)
 -{
 -	ib_dma_unmap_single(rdma->sc_pd->device, ctxt->rc_recv_sge.addr,
 -			    ctxt->rc_recv_sge.length, DMA_FROM_DEVICE);
 -	kfree(ctxt->rc_recv_buf);
 -	kfree(ctxt);
 -}
 -
 -/**
 - * svc_rdma_recv_ctxts_destroy - Release all recv_ctxt's for an xprt
 - * @rdma: svcxprt_rdma being torn down
 - *
 +/*
 + * Replace the pages in the rq_argpages array with the pages from the SGE in
 + * the RDMA_RECV completion. The SGL should contain full pages up until the
 + * last one.
   */
++<<<<<<< HEAD
++=======
+ void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_destroy(rdma, ctxt);
+ 	}
+ }
+ 
+ static struct svc_rdma_recv_ctxt *
+ svc_rdma_recv_ctxt_get(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	spin_lock(&rdma->sc_recv_lock);
+ 	ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_recv_ctxts);
+ 	if (!ctxt)
+ 		goto out_empty;
+ 	list_del(&ctxt->rc_list);
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ out:
+ 	ctxt->rc_page_count = 0;
+ 	return ctxt;
+ 
+ out_empty:
+ 	spin_unlock(&rdma->sc_recv_lock);
+ 
+ 	ctxt = svc_rdma_recv_ctxt_alloc(rdma);
+ 	if (!ctxt)
+ 		return NULL;
+ 	goto out;
+ }
+ 
+ /**
+  * svc_rdma_recv_ctxt_put - Return recv_ctxt to free list
+  * @rdma: controlling svcxprt_rdma
+  * @ctxt: object to return to the free list
+  *
+  */
+ void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
+ 			    struct svc_rdma_recv_ctxt *ctxt)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ctxt->rc_page_count; i++)
+ 		put_page(ctxt->rc_pages[i]);
+ 
+ 	if (!ctxt->rc_temp) {
+ 		spin_lock(&rdma->sc_recv_lock);
+ 		list_add(&ctxt->rc_list, &rdma->sc_recv_ctxts);
+ 		spin_unlock(&rdma->sc_recv_lock);
+ 	} else
+ 		svc_rdma_recv_ctxt_destroy(rdma, ctxt);
+ }
+ 
+ static int __svc_rdma_post_recv(struct svcxprt_rdma *rdma,
+ 				struct svc_rdma_recv_ctxt *ctxt)
+ {
+ 	struct ib_recv_wr *bad_recv_wr;
+ 	int ret;
+ 
+ 	svc_xprt_get(&rdma->sc_xprt);
+ 	ret = ib_post_recv(rdma->sc_qp, &ctxt->rc_recv_wr, &bad_recv_wr);
+ 	trace_svcrdma_post_recv(&ctxt->rc_recv_wr, ret);
+ 	if (ret)
+ 		goto err_post;
+ 	return 0;
+ 
+ err_post:
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	svc_xprt_put(&rdma->sc_xprt);
+ 	return ret;
+ }
+ 
+ static int svc_rdma_post_recv(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	ctxt = svc_rdma_recv_ctxt_get(rdma);
+ 	if (!ctxt)
+ 		return -ENOMEM;
+ 	return __svc_rdma_post_recv(rdma, ctxt);
+ }
+ 
+ /**
+  * svc_rdma_post_recvs - Post initial set of Recv WRs
+  * @rdma: fresh svcxprt_rdma
+  *
+  * Returns true if successful, otherwise false.
+  */
+ bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 	unsigned int i;
+ 	int ret;
+ 
+ 	for (i = 0; i < rdma->sc_max_requests; i++) {
+ 		ctxt = svc_rdma_recv_ctxt_get(rdma);
+ 		if (!ctxt)
+ 			return false;
+ 		ctxt->rc_temp = true;
+ 		ret = __svc_rdma_post_recv(rdma, ctxt);
+ 		if (ret) {
+ 			pr_err("svcrdma: failure posting recv buffers: %d\n",
+ 			       ret);
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ /**
+  * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC
+  * @cq: Completion Queue context
+  * @wc: Work Completion object
+  *
+  * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
+  * the Receive completion handler could be running.
+  */
+ static void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct svcxprt_rdma *rdma = cq->cq_context;
+ 	struct ib_cqe *cqe = wc->wr_cqe;
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	trace_svcrdma_wc_receive(wc);
+ 
+ 	/* WARNING: Only wc->wr_cqe and wc->status are reliable */
+ 	ctxt = container_of(cqe, struct svc_rdma_recv_ctxt, rc_cqe);
+ 
+ 	if (wc->status != IB_WC_SUCCESS)
+ 		goto flushed;
+ 
+ 	if (svc_rdma_post_recv(rdma))
+ 		goto post_err;
+ 
+ 	/* All wc fields are now known to be valid */
+ 	ctxt->rc_byte_len = wc->byte_len;
+ 	ib_dma_sync_single_for_cpu(rdma->sc_pd->device,
+ 				   ctxt->rc_recv_sge.addr,
+ 				   wc->byte_len, DMA_FROM_DEVICE);
+ 
+ 	spin_lock(&rdma->sc_rq_dto_lock);
+ 	list_add_tail(&ctxt->rc_list, &rdma->sc_rq_dto_q);
+ 	spin_unlock(&rdma->sc_rq_dto_lock);
+ 	set_bit(XPT_DATA, &rdma->sc_xprt.xpt_flags);
+ 	if (!test_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags))
+ 		svc_xprt_enqueue(&rdma->sc_xprt);
+ 	goto out;
+ 
+ flushed:
+ 	if (wc->status != IB_WC_WR_FLUSH_ERR)
+ 		pr_err("svcrdma: Recv: %s (%u/0x%x)\n",
+ 		       ib_wc_status_msg(wc->status),
+ 		       wc->status, wc->vendor_err);
+ post_err:
+ 	svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 	svc_xprt_enqueue(&rdma->sc_xprt);
+ out:
+ 	svc_xprt_put(&rdma->sc_xprt);
+ }
+ 
+ /**
+  * svc_rdma_flush_recv_queues - Drain pending Receive work
+  * @rdma: svcxprt_rdma being shut down
+  *
+  */
+ void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_recv_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_read_complete_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ 	while ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_rq_dto_q))) {
+ 		list_del(&ctxt->rc_list);
+ 		svc_rdma_recv_ctxt_put(rdma, ctxt);
+ 	}
+ }
+ 
++>>>>>>> af7fd74ec243 (svcrdma: Fix incorrect return value/type in svc_rdma_post_recvs)
  static void svc_rdma_build_arg_xdr(struct svc_rqst *rqstp,
 -				   struct svc_rdma_recv_ctxt *ctxt)
 +				   struct svc_rdma_op_ctxt *ctxt)
  {
 -	struct xdr_buf *arg = &rqstp->rq_arg;
 -
 -	arg->head[0].iov_base = ctxt->rc_recv_buf;
 -	arg->head[0].iov_len = ctxt->rc_byte_len;
 -	arg->tail[0].iov_base = NULL;
 -	arg->tail[0].iov_len = 0;
 -	arg->page_len = 0;
 -	arg->page_base = 0;
 -	arg->buflen = ctxt->rc_byte_len;
 -	arg->len = ctxt->rc_byte_len;
 -
 -	rqstp->rq_respages = &rqstp->rq_pages[0];
 +	struct page *page;
 +	int sge_no;
 +	u32 len;
 +
 +	/* The reply path assumes the Call's transport header resides
 +	 * in rqstp->rq_pages[0].
 +	 */
 +	page = ctxt->pages[0];
 +	put_page(rqstp->rq_pages[0]);
 +	rqstp->rq_pages[0] = page;
 +
 +	/* Set up the XDR head */
 +	rqstp->rq_arg.head[0].iov_base = page_address(page);
 +	rqstp->rq_arg.head[0].iov_len =
 +		min_t(size_t, ctxt->byte_len, ctxt->sge[0].length);
 +	rqstp->rq_arg.len = ctxt->byte_len;
 +	rqstp->rq_arg.buflen = ctxt->byte_len;
 +
 +	/* Compute bytes past head in the SGL */
 +	len = ctxt->byte_len - rqstp->rq_arg.head[0].iov_len;
 +
 +	/* If data remains, store it in the pagelist */
 +	rqstp->rq_arg.page_len = len;
 +	rqstp->rq_arg.page_base = 0;
 +
 +	sge_no = 1;
 +	while (len && sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no];
 +		put_page(rqstp->rq_pages[sge_no]);
 +		rqstp->rq_pages[sge_no] = page;
 +		len -= min_t(u32, len, ctxt->sge[sge_no].length);
 +		sge_no++;
 +	}
 +	rqstp->rq_respages = &rqstp->rq_pages[sge_no];
  	rqstp->rq_next_page = rqstp->rq_respages + 1;
 +
 +	/* If not all pages were used from the SGL, free the remaining ones */
 +	len = sge_no;
 +	while (sge_no < ctxt->count) {
 +		page = ctxt->pages[sge_no++];
 +		put_page(page);
 +	}
 +	ctxt->count = len;
 +
 +	/* Set up tail */
 +	rqstp->rq_arg.tail[0].iov_base = NULL;
 +	rqstp->rq_arg.tail[0].iov_len = 0;
  }
  
  /* This accommodates the largest possible Write chunk,
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
