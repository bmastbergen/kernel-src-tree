mm: vmscan: fix do_try_to_free_pages() livelock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [mm] vmscan: fix do_try_to_free_pages() livelock (Rafael Aquini) [1563411]
Rebuild_FUZZ: 95.56%
commit-author Lisa Du <cldu@marvell.com>
commit 6e543d5780e36ff5ee56c44d7e2e30db3457a7ed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/6e543d57.failed

This patch is based on KOSAKI's work and I add a little more description,
please refer https://lkml.org/lkml/2012/6/14/74.

Currently, I found system can enter a state that there are lots of free
pages in a zone but only order-0 and order-1 pages which means the zone is
heavily fragmented, then high order allocation could make direct reclaim
path's long stall(ex, 60 seconds) especially in no swap and no compaciton
enviroment.  This problem happened on v3.4, but it seems issue still lives
in current tree, the reason is do_try_to_free_pages enter live lock:

kswapd will go to sleep if the zones have been fully scanned and are still
not balanced.  As kswapd thinks there's little point trying all over again
to avoid infinite loop.  Instead it changes order from high-order to
0-order because kswapd think order-0 is the most important.  Look at
73ce02e9 in detail.  If watermarks are ok, kswapd will go back to sleep
and may leave zone->all_unreclaimable =3D 0.  It assume high-order users
can still perform direct reclaim if they wish.

Direct reclaim continue to reclaim for a high order which is not a
COSTLY_ORDER without oom-killer until kswapd turn on
zone->all_unreclaimble= .  This is because to avoid too early oom-kill.
So it means direct_reclaim depends on kswapd to break this loop.

In worst case, direct-reclaim may continue to page reclaim forever when
kswapd sleeps forever until someone like watchdog detect and finally kill
the process.  As described in:
http://thread.gmane.org/gmane.linux.kernel.mm/103737

We can't turn on zone->all_unreclaimable from direct reclaim path because
direct reclaim path don't take any lock and this way is racy.  Thus this
patch removes zone->all_unreclaimable field completely and recalculates
zone reclaimable state every time.

Note: we can't take the idea that direct-reclaim see zone->pages_scanned
directly and kswapd continue to use zone->all_unreclaimable.  Because, it
is racy.  commit 929bea7c71 (vmscan: all_unreclaimable() use
zone->all_unreclaimable as a name) describes the detail.

[akpm@linux-foundation.org: uninline zone_reclaimable_pages() and zone_reclaimable()]
	Cc: Aaditya Kumar <aaditya.kumar.30@gmail.com>
	Cc: Ying Han <yinghan@google.com>
	Cc: Nick Piggin <npiggin@gmail.com>
	Acked-by: Rik van Riel <riel@redhat.com>
	Cc: Mel Gorman <mel@csn.ul.ie>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Bob Liu <lliubbo@gmail.com>
	Cc: Neil Zhang <zhangwm@marvell.com>
	Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Signed-off-by: Lisa Du <cldu@marvell.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6e543d5780e36ff5ee56c44d7e2e30db3457a7ed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/vmstat.h
#	mm/vmscan.c
diff --cc include/linux/vmstat.h
index 99c3389fb3ec,e4b948080d20..000000000000
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@@ -142,6 -142,8 +142,11 @@@ static inline unsigned long zone_page_s
  	return x;
  }
  
++<<<<<<< HEAD
++=======
+ extern unsigned long global_reclaimable_pages(void);
+ 
++>>>>>>> 6e543d5780e3 (mm: vmscan: fix do_try_to_free_pages() livelock)
  #ifdef CONFIG_NUMA
  /*
   * Determine the per node value of a stat item. This function
diff --cc mm/vmscan.c
index a0e4019d5137,fe715daeb8bc..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -172,14 -144,9 +172,18 @@@ static bool global_reclaim(struct scan_
  {
  	return true;
  }
 +
 +static bool sane_reclaim(struct scan_control *sc)
 +{
 +	return true;
 +}
  #endif
  
++<<<<<<< HEAD
 +static unsigned long zone_reclaimable_pages(struct zone *zone)
++=======
+ unsigned long zone_reclaimable_pages(struct zone *zone)
++>>>>>>> 6e543d5780e3 (mm: vmscan: fix do_try_to_free_pages() livelock)
  {
  	int nr;
  
@@@ -193,7 -160,7 +197,11 @@@
  	return nr;
  }
  
++<<<<<<< HEAD
 +static bool zone_reclaimable(struct zone *zone)
++=======
+ bool zone_reclaimable(struct zone *zone)
++>>>>>>> 6e543d5780e3 (mm: vmscan: fix do_try_to_free_pages() livelock)
  {
  	return zone->pages_scanned < zone_reclaimable_pages(zone) * 6;
  }
@@@ -3418,6 -3254,27 +3422,30 @@@ void wakeup_kswapd(struct zone *zone, i
  	wake_up_interruptible(&pgdat->kswapd_wait);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * The reclaimable count would be mostly accurate.
+  * The less reclaimable pages may be
+  * - mlocked pages, which will be moved to unevictable list when encountered
+  * - mapped pages, which may require several travels to be reclaimed
+  * - dirty pages, which is not "instantly" reclaimable
+  */
+ unsigned long global_reclaimable_pages(void)
+ {
+ 	int nr;
+ 
+ 	nr = global_page_state(NR_ACTIVE_FILE) +
+ 	     global_page_state(NR_INACTIVE_FILE);
+ 
+ 	if (get_nr_swap_pages() > 0)
+ 		nr += global_page_state(NR_ACTIVE_ANON) +
+ 		      global_page_state(NR_INACTIVE_ANON);
+ 
+ 	return nr;
+ }
+ 
++>>>>>>> 6e543d5780e3 (mm: vmscan: fix do_try_to_free_pages() livelock)
  #ifdef CONFIG_HIBERNATION
  /*
   * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of
diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h
index 1397ccf81e91..cf55945c83fb 100644
--- a/include/linux/mm_inline.h
+++ b/include/linux/mm_inline.h
@@ -2,6 +2,7 @@
 #define LINUX_MM_INLINE_H
 
 #include <linux/huge_mm.h>
+#include <linux/swap.h>
 
 /**
  * page_is_file_cache - should the page be on a file LRU or anon LRU?
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ec9595db6870..a9ae57fa9498 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -375,7 +375,6 @@ struct zone {
 	 * free areas of different sizes
 	 */
 	spinlock_t		lock;
-	int                     all_unreclaimable; /* All pages pinned */
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
 	/* Set to true when the PG_migrate_skip bits should be cleared */
 	bool			compact_blockskip_flush;
* Unmerged path include/linux/vmstat.h
diff --git a/mm/internal.h b/mm/internal.h
index 388ef5f262ce..0ae276d46a54 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -86,6 +86,8 @@ extern unsigned long highest_memmap_pfn;
  */
 extern int isolate_lru_page(struct page *page);
 extern void putback_lru_page(struct page *page);
+extern unsigned long zone_reclaimable_pages(struct zone *zone);
+extern bool zone_reclaimable(struct zone *zone);
 
 /*
  * in mm/rmap.c:
diff --git a/mm/migrate.c b/mm/migrate.c
index dc35415df847..54b8ee96c6c2 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1586,7 +1586,7 @@ static bool migrate_balanced_pgdat(struct pglist_data *pgdat,
 		if (!populated_zone(zone))
 			continue;
 
-		if (zone->all_unreclaimable)
+		if (!zone_reclaimable(zone))
 			continue;
 
 		/* Avoid waking kswapd by allocating pages_to_migrate pages. */
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index ef7b3753abd8..913f1a794034 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -36,8 +36,11 @@
 #include <linux/pagevec.h>
 #include <linux/timer.h>
 #include <linux/sched/rt.h>
+#include <linux/mm_inline.h>
 #include <trace/events/writeback.h>
 
+#include "internal.h"
+
 /*
  * Sleep at most 200ms at a time in balance_dirty_pages().
  */
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 1c4d218f3bcd..fb502c1889d4 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -57,6 +57,7 @@
 #include <linux/ftrace_event.h>
 #include <linux/memcontrol.h>
 #include <linux/prefetch.h>
+#include <linux/mm_inline.h>
 #include <linux/migrate.h>
 #include <linux/page_ext.h>
 #include <linux/hugetlb.h>
@@ -827,7 +828,6 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 	int to_free = count;
 
 	spin_lock(&zone->lock);
-	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
 	while (to_free) {
@@ -876,7 +876,6 @@ static void free_one_page(struct zone *zone, struct page *page, int order,
 				int migratetype)
 {
 	spin_lock(&zone->lock);
-	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
 	__free_one_page(page, zone, order, migratetype);
@@ -4074,7 +4073,7 @@ void show_free_areas(unsigned int filter)
 			K(zone_page_state(zone, NR_FREE_CMA_PAGES)),
 			K(zone_page_state(zone, NR_WRITEBACK_TEMP)),
 			zone->pages_scanned,
-			(zone->all_unreclaimable ? "yes" : "no")
+			(!zone_reclaimable(zone) ? "yes" : "no")
 			);
 		printk("lowmem_reserve[]:");
 		for (i = 0; i < MAX_NR_ZONES; i++)
* Unmerged path mm/vmscan.c
diff --git a/mm/vmstat.c b/mm/vmstat.c
index bf0311ec1984..2d0dbc6b5ab5 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -21,6 +21,9 @@
 #include <linux/math64.h>
 #include <linux/writeback.h>
 #include <linux/compaction.h>
+#include <linux/mm_inline.h>
+
+#include "internal.h"
 
 #ifdef CONFIG_VM_EVENT_COUNTERS
 DEFINE_PER_CPU(struct vm_event_state, vm_event_states) = {{0}};
@@ -1117,7 +1120,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   "\n  all_unreclaimable: %u"
 		   "\n  start_pfn:         %lu"
 		   "\n  inactive_ratio:    %u",
-		   zone->all_unreclaimable,
+		   !zone_reclaimable(zone),
 		   zone->zone_start_pfn,
 		   zone->inactive_ratio);
 	seq_putc(m, '\n');
