sched/smt: Make sched_smt_present track topology

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Peter Zijlstra (Intel) <peterz@infradead.org>
commit c5511d03ec090980732e929c318a7a6374b5550e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/c5511d03.failed

Currently the 'sched_smt_present' static key is enabled when at CPU bringup
SMT topology is observed, but it is never disabled. However there is demand
to also disable the key when the topology changes such that there is no SMT
present anymore.

Implement this by making the key count the number of cores that have SMT
enabled.

In particular, the SMT topology bits are set before interrrupts are enabled
and similarly, are cleared after interrupts are disabled for the last time
and the CPU dies.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: David Woodhouse <dwmw@amazon.co.uk>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Casey Schaufler <casey.schaufler@intel.com>
	Cc: Asit Mallick <asit.k.mallick@intel.com>
	Cc: Arjan van de Ven <arjan@linux.intel.com>
	Cc: Jon Masters <jcm@redhat.com>
	Cc: Waiman Long <longman9394@gmail.com>
	Cc: Greg KH <gregkh@linuxfoundation.org>
	Cc: Dave Stewart <david.c.stewart@intel.com>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: stable@vger.kernel.org
Link: https://lkml.kernel.org/r/20181125185004.246110444@linutronix.de


(cherry picked from commit c5511d03ec090980732e929c318a7a6374b5550e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 6a1d0422eb1f,6fedf3a98581..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -8299,53 -5713,139 +8299,128 @@@ static int cpuset_cpu_active(struct not
  		 * restore the original sched domains by considering the
  		 * cpuset configurations.
  		 */
 -		cpuset_force_rebuild();
 +
 +	case CPU_ONLINE:
 +		cpuset_update_active_cpus(true);
 +		break;
 +	default:
 +		return NOTIFY_DONE;
  	}
 -	cpuset_update_active_cpus();
 +	return NOTIFY_OK;
  }
  
 -static int cpuset_cpu_inactive(unsigned int cpu)
 +static int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,
 +			       void *hcpu)
  {
++<<<<<<< HEAD
 +	unsigned long flags;
 +	long cpu = (long)hcpu;
 +	struct dl_bw *dl_b;
 +	bool overflow;
 +	int cpus;
++=======
+ 	if (!cpuhp_tasks_frozen) {
+ 		if (dl_cpu_busy(cpu))
+ 			return -EBUSY;
+ 		cpuset_update_active_cpus();
+ 	} else {
+ 		num_cpus_frozen++;
+ 		partition_sched_domains(1, NULL, NULL);
+ 	}
+ 	return 0;
+ }
+ 
+ int sched_cpu_activate(unsigned int cpu)
+ {
+ 	struct rq *rq = cpu_rq(cpu);
+ 	struct rq_flags rf;
+ 
+ #ifdef CONFIG_SCHED_SMT
+ 	/*
+ 	 * When going up, increment the number of cores with SMT present.
+ 	 */
+ 	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
+ 		static_branch_inc_cpuslocked(&sched_smt_present);
+ #endif
+ 	set_cpu_active(cpu, true);
+ 
+ 	if (sched_smp_initialized) {
+ 		sched_domains_numa_masks_set(cpu);
+ 		cpuset_cpu_active();
+ 	}
+ 
+ 	/*
+ 	 * Put the rq online, if not already. This happens:
+ 	 *
+ 	 * 1) In the early boot process, because we build the real domains
+ 	 *    after all CPUs have been brought up.
+ 	 *
+ 	 * 2) At runtime, if cpuset_cpu_active() fails to rebuild the
+ 	 *    domains.
+ 	 */
+ 	rq_lock_irqsave(rq, &rf);
+ 	if (rq->rd) {
+ 		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
+ 		set_rq_online(rq);
+ 	}
+ 	rq_unlock_irqrestore(rq, &rf);
+ 
+ 	update_max_interval();
+ 
+ 	return 0;
+ }
+ 
+ int sched_cpu_deactivate(unsigned int cpu)
+ {
+ 	int ret;
+ 
+ 	set_cpu_active(cpu, false);
+ 	/*
+ 	 * We've cleared cpu_active_mask, wait for all preempt-disabled and RCU
+ 	 * users of this state to go away such that all new such users will
+ 	 * observe it.
+ 	 *
+ 	 * Do sync before park smpboot threads to take care the rcu boost case.
+ 	 */
+ 	synchronize_rcu_mult(call_rcu, call_rcu_sched);
++>>>>>>> c5511d03ec09 (sched/smt: Make sched_smt_present track topology)
+ 
+ #ifdef CONFIG_SCHED_SMT
+ 	/*
+ 	 * When going down, decrement the number of cores with SMT present.
+ 	 */
+ 	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
+ 		static_branch_dec_cpuslocked(&sched_smt_present);
+ #endif
  
  	if (!sched_smp_initialized)
 -		return 0;
 -
 -	ret = cpuset_cpu_inactive(cpu);
 -	if (ret) {
 -		set_cpu_active(cpu, true);
 -		return ret;
 -	}
 -	sched_domains_numa_masks_clear(cpu);
 -	return 0;
 -}
 -
 -static void sched_rq_cpu_starting(unsigned int cpu)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 +		return NOTIFY_DONE;
  
 -	rq->calc_load_update = calc_load_update;
 -	update_max_interval();
 -}
 -
 -int sched_cpu_starting(unsigned int cpu)
 -{
 -	sched_rq_cpu_starting(cpu);
 -	sched_tick_start(cpu);
 -	return 0;
 -}
 +	switch (action) {
 +	case CPU_DOWN_PREPARE:
 +		rcu_read_lock_sched();
 +		dl_b = dl_bw_of(cpu);
  
 -#ifdef CONFIG_HOTPLUG_CPU
 -int sched_cpu_dying(unsigned int cpu)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 -	struct rq_flags rf;
 +		raw_spin_lock_irqsave(&dl_b->lock, flags);
 +		cpus = dl_bw_cpus(cpu);
 +		overflow = __dl_overflow(dl_b, cpus, 0, 0);
 +		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
  
 -	/* Handle pending wakeups and then migrate everything off */
 -	sched_ttwu_pending();
 -	sched_tick_stop(cpu);
 +		rcu_read_unlock_sched();
  
 -	rq_lock_irqsave(rq, &rf);
 -	if (rq->rd) {
 -		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
 -		set_rq_offline(rq);
 +		if (overflow)
 +			return notifier_from_errno(-EBUSY);
 +		cpuset_update_active_cpus(false);
 +		break;
 +	case CPU_DOWN_PREPARE_FROZEN:
 +		num_cpus_frozen++;
 +		partition_sched_domains(1, NULL, NULL);
 +		break;
 +	default:
 +		return NOTIFY_DONE;
  	}
 -	migrate_tasks(rq, &rf);
 -	BUG_ON(rq->nr_running != 1);
 -	rq_unlock_irqrestore(rq, &rf);
 -
 -	calc_load_migrate(rq);
 -	update_max_interval();
 -	nohz_balance_exit_idle(rq);
 -	hrtick_clear(rq);
 -	return 0;
 +	return NOTIFY_OK;
  }
 -#endif
  
  void __init sched_init_smp(void)
  {
* Unmerged path kernel/sched/core.c
