stop_machine: Disable preemption after queueing stopper threads

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Isaac J. Manjarres <isaacm@codeaurora.org>
commit 2610e88946632afb78aa58e61f11368ac4c0af7b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/2610e889.failed

This commit:

  9fb8d5dc4b64 ("stop_machine, Disable preemption when waking two stopper threads")

does not fully address the race condition that can occur
as follows:

On one CPU, call it CPU 3, thread 1 invokes
cpu_stop_queue_two_works(2, 3,...), and the execution is such
that thread 1 queues the works for migration/2 and migration/3,
and is preempted after releasing the locks for migration/2 and
migration/3, but before waking the threads.

Then, On CPU 2, a kworker, call it thread 2, is running,
and it invokes cpu_stop_queue_two_works(1, 2,...), such that
thread 2 queues the works for migration/1 and migration/2.
Meanwhile, on CPU 3, thread 1 resumes execution, and wakes
migration/2 and migration/3. This means that when CPU 2
releases the locks for migration/1 and migration/2, but before
it wakes those threads, it can be preempted by migration/2.

If thread 2 is preempted by migration/2, then migration/2 will
execute the first work item successfully, since migration/3
was woken up by CPU 3, but when it goes to execute the second
work item, it disables preemption, calls multi_cpu_stop(),
and thus, CPU 2 will wait forever for migration/1, which should
have been woken up by thread 2. However migration/1 cannot be
woken up by thread 2, since it is a kworker, so it is affine to
CPU 2, but CPU 2 is running migration/2 with preemption
disabled, so thread 2 will never run.

Disable preemption after queueing works for stopper threads
to ensure that the operation of queueing the works and waking
the stopper threads is atomic.

Co-Developed-by: Prasad Sodagudi <psodagud@codeaurora.org>
Co-Developed-by: Pavankumar Kondeti <pkondeti@codeaurora.org>
	Signed-off-by: Isaac J. Manjarres <isaacm@codeaurora.org>
	Signed-off-by: Prasad Sodagudi <psodagud@codeaurora.org>
	Signed-off-by: Pavankumar Kondeti <pkondeti@codeaurora.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: bigeasy@linutronix.de
	Cc: gregkh@linuxfoundation.org
	Cc: matt@codeblueprint.co.uk
Fixes: 9fb8d5dc4b64 ("stop_machine, Disable preemption when waking two stopper threads")
Link: http://lkml.kernel.org/r/1531856129-9871-1-git-send-email-isaacm@codeaurora.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 2610e88946632afb78aa58e61f11368ac4c0af7b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/stop_machine.c
diff --cc kernel/stop_machine.c
index bfd90d7f3d08,e190d1ef3a23..000000000000
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@@ -242,14 -243,46 +242,42 @@@ static int cpu_stop_queue_two_works(in
  	err = -ENOENT;
  	if (!stopper1->enabled || !stopper2->enabled)
  		goto unlock;
 -	/*
 -	 * Ensure that if we race with __stop_cpus() the stoppers won't get
 -	 * queued up in reverse order leading to system deadlock.
 -	 *
 -	 * We can't miss stop_cpus_in_progress if queue_stop_cpus_work() has
 -	 * queued a work on cpu1 but not on cpu2, we hold both locks.
 -	 *
 -	 * It can be falsely true but it is safe to spin until it is cleared,
 -	 * queue_stop_cpus_work() does everything under preempt_disable().
 -	 */
 -	err = -EDEADLK;
 -	if (unlikely(stop_cpus_in_progress))
 -			goto unlock;
  
  	err = 0;
++<<<<<<< HEAD
 +	__cpu_stop_queue_work(stopper1, work1);
 +	__cpu_stop_queue_work(stopper2, work2);
 +unlock:
 +	spin_unlock(&stopper2->lock);
 +	spin_unlock_irq(&stopper1->lock);
 +	lg_double_unlock(&stop_cpus_lock, cpu1, cpu2);
++=======
+ 	__cpu_stop_queue_work(stopper1, work1, &wakeq);
+ 	__cpu_stop_queue_work(stopper2, work2, &wakeq);
+ 	/*
+ 	 * The waking up of stopper threads has to happen
+ 	 * in the same scheduling context as the queueing.
+ 	 * Otherwise, there is a possibility of one of the
+ 	 * above stoppers being woken up by another CPU,
+ 	 * and preempting us. This will cause us to n ot
+ 	 * wake up the other stopper forever.
+ 	 */
+ 	preempt_disable();
+ unlock:
+ 	raw_spin_unlock(&stopper2->lock);
+ 	raw_spin_unlock_irq(&stopper1->lock);
+ 
+ 	if (unlikely(err == -EDEADLK)) {
+ 		while (stop_cpus_in_progress)
+ 			cpu_relax();
+ 		goto retry;
+ 	}
+ 
+ 	if (!err) {
+ 		wake_up_q(&wakeq);
+ 		preempt_enable();
+ 	}
++>>>>>>> 2610e8894663 (stop_machine: Disable preemption after queueing stopper threads)
  
  	return err;
  }
* Unmerged path kernel/stop_machine.c
