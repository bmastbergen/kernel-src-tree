blk-mq: introduce blk_mq_complete_request_sync()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit 1b8f21b74c3c9c82fce5a751d7aefb7cc0b8d33d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/1b8f21b7.failed

In NVMe's error handler, follows the typical steps of tearing down
hardware for recovering controller:

1) stop blk_mq hw queues
2) stop the real hw queues
3) cancel in-flight requests via
	blk_mq_tagset_busy_iter(tags, cancel_request, ...)
cancel_request():
	mark the request as abort
	blk_mq_complete_request(req);
4) destroy real hw queues

However, there may be race between #3 and #4, because blk_mq_complete_request()
may run q->mq_ops->complete(rq) remotelly and asynchronously, and
->complete(rq) may be run after #4.

This patch introduces blk_mq_complete_request_sync() for fixing the
above race.

	Cc: Sagi Grimberg <sagi@grimberg.me>
	Cc: Bart Van Assche <bvanassche@acm.org>
	Cc: James Smart <james.smart@broadcom.com>
	Cc: linux-nvme@lists.infradead.org
	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 1b8f21b74c3c9c82fce5a751d7aefb7cc0b8d33d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blk-mq.h
diff --cc include/linux/blk-mq.h
index 5f4c366b1ff0,db29928de467..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -326,16 -295,16 +326,23 @@@ struct blk_mq_hw_ctx *blk_mq_alloc_sing
  
  int blk_mq_request_started(struct request *rq);
  void blk_mq_start_request(struct request *rq);
 -void blk_mq_end_request(struct request *rq, blk_status_t error);
 -void __blk_mq_end_request(struct request *rq, blk_status_t error);
 +void blk_mq_end_request(struct request *rq, int error);
 +void __blk_mq_end_request(struct request *rq, int error);
  
  void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
 +void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 +				bool kick_requeue_list);
  void blk_mq_kick_requeue_list(struct request_queue *q);
  void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
++<<<<<<< HEAD
 +void blk_mq_complete_request(struct request *rq, int error);
 +
++=======
+ bool blk_mq_complete_request(struct request *rq);
+ void blk_mq_complete_request_sync(struct request *rq);
+ bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
+ 			   struct bio *bio);
++>>>>>>> 1b8f21b74c3c (blk-mq: introduce blk_mq_complete_request_sync())
  bool blk_mq_queue_stopped(struct request_queue *q);
  void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
  void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 3dd6f1d72cee..7fc08a72fc9a 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -585,6 +585,13 @@ void blk_mq_complete_request(struct request *rq, int error)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+void blk_mq_complete_request_sync(struct request *rq)
+{
+	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	rq->q->mq_ops->complete(rq);
+}
+EXPORT_SYMBOL_GPL(blk_mq_complete_request_sync);
+
 int blk_mq_request_started(struct request *rq)
 {
 	return test_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
* Unmerged path include/linux/blk-mq.h
