x86/speculation: Add PR_SPEC_DISABLE_NOEXEC

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Waiman Long <longman@redhat.com>
commit 71368af9027f18fe5d1c6f372cfdff7e4bde8b48
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/71368af9.failed

With the default SPEC_STORE_BYPASS_SECCOMP/SPEC_STORE_BYPASS_PRCTL mode,
the TIF_SSBD bit will be inherited when a new task is fork'ed or cloned.
It will also remain when a new program is execve'ed.

Only certain class of applications (like Java) that can run on behalf of
multiple users on a single thread will require disabling speculative store
bypass for security purposes. Those applications will call prctl(2) at
startup time to disable SSB. They won't rely on the fact the SSB might have
been disabled. Other applications that don't need SSBD will just move on
without checking if SSBD has been turned on or not.

The fact that the TIF_SSBD is inherited across execve(2) boundary will
cause performance of applications that don't need SSBD but their
predecessors have SSBD on to be unwittingly impacted especially if they
write to memory a lot.

To remedy this problem, a new PR_SPEC_DISABLE_NOEXEC argument for the
PR_SET_SPECULATION_CTRL option of prctl(2) is added to allow applications
to specify that the SSBD feature bit on the task structure should be
cleared whenever a new program is being execve'ed.

	Suggested-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: linux-doc@vger.kernel.org
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: David Woodhouse <dwmw@amazon.co.uk>
	Cc: Jiri Kosina <jikos@kernel.org>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: KarimAllah Ahmed <karahmed@amazon.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Link: https://lkml.kernel.org/r/1547676096-3281-1-git-send-email-longman@redhat.com

(cherry picked from commit 71368af9027f18fe5d1c6f372cfdff7e4bde8b48)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/userspace-api/spec_ctrl.txt
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/process.c
#	include/linux/sched.h
#	tools/include/uapi/linux/prctl.h
diff --cc Documentation/userspace-api/spec_ctrl.txt
index 209884ad3c4e,1129c7550a48..000000000000
--- a/Documentation/userspace-api/spec_ctrl.txt
+++ b/Documentation/userspace-api/spec_ctrl.txt
@@@ -90,3 -94,13 +92,16 @@@ Speculation misfeature control
     * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_STORE_BYPASS, PR_SPEC_ENABLE, 0, 0);
     * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_STORE_BYPASS, PR_SPEC_DISABLE, 0, 0);
     * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_STORE_BYPASS, PR_SPEC_FORCE_DISABLE, 0, 0);
++<<<<<<< HEAD:Documentation/userspace-api/spec_ctrl.txt
++=======
+    * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_STORE_BYPASS, PR_SPEC_DISABLE_NOEXEC, 0, 0);
+ 
+ - PR_SPEC_INDIR_BRANCH: Indirect Branch Speculation in User Processes
+                         (Mitigate Spectre V2 style attacks against user processes)
+ 
+   Invocations:
+    * prctl(PR_GET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, 0, 0, 0);
+    * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_ENABLE, 0, 0);
+    * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_DISABLE, 0, 0);
+    * prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_FORCE_DISABLE, 0, 0);
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC):Documentation/userspace-api/spec_ctrl.rst
diff --cc arch/x86/kernel/cpu/bugs.c
index 12a489be7869,2faeaf46347a..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -422,16 -798,26 +422,38 @@@ static int ssb_prctl_set(struct task_st
  		if (task_spec_ssb_force_disable(task))
  			return -EPERM;
  		task_clear_spec_ssb_disable(task);
++<<<<<<< HEAD
 +		update = test_and_clear_tsk_thread_flag(task, TIF_SSBD);
 +		break;
 +	case PR_SPEC_DISABLE:
 +		task_set_spec_ssb_disable(task);
 +		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
++=======
+ 		task_clear_spec_ssb_noexec(task);
+ 		task_update_spec_tif(task);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_clear_spec_ssb_noexec(task);
+ 		task_update_spec_tif(task);
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC)
  		break;
  	case PR_SPEC_FORCE_DISABLE:
  		task_set_spec_ssb_disable(task);
  		task_set_spec_ssb_force_disable(task);
++<<<<<<< HEAD
 +		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
++=======
+ 		task_clear_spec_ssb_noexec(task);
+ 		task_update_spec_tif(task);
+ 		break;
+ 	case PR_SPEC_DISABLE_NOEXEC:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_noexec(task);
+ 		task_update_spec_tif(task);
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC)
  		break;
  	default:
  		return -ERANGE;
diff --cc arch/x86/kernel/process.c
index 5ad9710f0312,58ac7be52c7a..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -197,6 -190,111 +197,114 @@@ int set_tsc_mode(unsigned int val
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ DEFINE_PER_CPU(u64, msr_misc_features_shadow);
+ 
+ static void set_cpuid_faulting(bool on)
+ {
+ 	u64 msrval;
+ 
+ 	msrval = this_cpu_read(msr_misc_features_shadow);
+ 	msrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;
+ 	msrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);
+ 	this_cpu_write(msr_misc_features_shadow, msrval);
+ 	wrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);
+ }
+ 
+ static void disable_cpuid(void)
+ {
+ 	preempt_disable();
+ 	if (!test_and_set_thread_flag(TIF_NOCPUID)) {
+ 		/*
+ 		 * Must flip the CPU state synchronously with
+ 		 * TIF_NOCPUID in the current running context.
+ 		 */
+ 		set_cpuid_faulting(true);
+ 	}
+ 	preempt_enable();
+ }
+ 
+ static void enable_cpuid(void)
+ {
+ 	preempt_disable();
+ 	if (test_and_clear_thread_flag(TIF_NOCPUID)) {
+ 		/*
+ 		 * Must flip the CPU state synchronously with
+ 		 * TIF_NOCPUID in the current running context.
+ 		 */
+ 		set_cpuid_faulting(false);
+ 	}
+ 	preempt_enable();
+ }
+ 
+ static int get_cpuid_mode(void)
+ {
+ 	return !test_thread_flag(TIF_NOCPUID);
+ }
+ 
+ static int set_cpuid_mode(struct task_struct *task, unsigned long cpuid_enabled)
+ {
+ 	if (!static_cpu_has(X86_FEATURE_CPUID_FAULT))
+ 		return -ENODEV;
+ 
+ 	if (cpuid_enabled)
+ 		enable_cpuid();
+ 	else
+ 		disable_cpuid();
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Called immediately after a successful exec.
+  */
+ void arch_setup_new_exec(void)
+ {
+ 	/* If cpuid was previously disabled for this task, re-enable it. */
+ 	if (test_thread_flag(TIF_NOCPUID))
+ 		enable_cpuid();
+ 
+ 	/*
+ 	 * Don't inherit TIF_SSBD across exec boundary when
+ 	 * PR_SPEC_DISABLE_NOEXEC is used.
+ 	 */
+ 	if (test_thread_flag(TIF_SSBD) &&
+ 	    task_spec_ssb_noexec(current)) {
+ 		clear_thread_flag(TIF_SSBD);
+ 		task_clear_spec_ssb_disable(current);
+ 		task_clear_spec_ssb_noexec(current);
+ 		speculation_ctrl_update(task_thread_info(current)->flags);
+ 	}
+ }
+ 
+ static inline void switch_to_bitmap(struct thread_struct *prev,
+ 				    struct thread_struct *next,
+ 				    unsigned long tifp, unsigned long tifn)
+ {
+ 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+ 
+ 	if (tifn & _TIF_IO_BITMAP) {
+ 		/*
+ 		 * Copy the relevant range of the IO bitmap.
+ 		 * Normally this is 128 bytes or less:
+ 		 */
+ 		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
+ 		       max(prev->io_bitmap_max, next->io_bitmap_max));
+ 		/*
+ 		 * Make sure that the TSS limit is correct for the CPU
+ 		 * to notice the IO bitmap.
+ 		 */
+ 		refresh_tss_limit();
+ 	} else if (tifp & _TIF_IO_BITMAP) {
+ 		/*
+ 		 * Clear any possible leftover bits:
+ 		 */
+ 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+ 	}
+ }
+ 
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC)
  #ifdef CONFIG_SMP
  
  struct ssb_state {
diff --cc include/linux/sched.h
index 83cd9508a135,fc836dc71bba..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -2116,37 -1424,54 +2116,48 @@@ extern void thread_group_cputime_adjust
   * child is not running and in turn not changing child->flags
   * at the same time the parent does it.
   */
 -#define clear_stopped_child_used_math(child)	do { (child)->flags &= ~PF_USED_MATH; } while (0)
 -#define set_stopped_child_used_math(child)	do { (child)->flags |= PF_USED_MATH; } while (0)
 -#define clear_used_math()			clear_stopped_child_used_math(current)
 -#define set_used_math()				set_stopped_child_used_math(current)
 -
 +#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)
 +#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)
 +#define clear_used_math() clear_stopped_child_used_math(current)
 +#define set_used_math() set_stopped_child_used_math(current)
  #define conditional_stopped_child_used_math(condition, child) \
  	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)
 -
 -#define conditional_used_math(condition)	conditional_stopped_child_used_math(condition, current)
 -
 +#define conditional_used_math(condition) \
 +	conditional_stopped_child_used_math(condition, current)
  #define copy_to_stopped_child_used_math(child) \
  	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)
 -
  /* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */
 -#define tsk_used_math(p)			((p)->flags & PF_USED_MATH)
 -#define used_math()				tsk_used_math(current)
 -
 -static inline bool is_percpu_thread(void)
 -{
 -#ifdef CONFIG_SMP
 -	return (current->flags & PF_NO_SETAFFINITY) &&
 -		(current->nr_cpus_allowed  == 1);
 -#else
 -	return true;
 -#endif
 -}
 +#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 +#define used_math() tsk_used_math(current)
  
  /* Per-process atomic flags. */
++<<<<<<< HEAD
 +#define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 +#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
 +#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
++=======
+ #define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
+ #define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */
+ #define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
+ #define PFA_SPEC_SSB_DISABLE		3	/* Speculative Store Bypass disabled */
+ #define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/
+ #define PFA_SPEC_IB_DISABLE		5	/* Indirect branch speculation restricted */
+ #define PFA_SPEC_IB_FORCE_DISABLE	6	/* Indirect branch speculation permanently restricted */
+ #define PFA_SPEC_SSB_NOEXEC		7	/* Speculative Store Bypass clear on execve() */
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC)
  
 -#define TASK_PFA_TEST(name, func)					\
 -	static inline bool task_##func(struct task_struct *p)		\
 -	{ return test_bit(PFA_##name, &p->atomic_flags); }
 +#define PFA_SPEC_SSB_DISABLE	   3	/* Speculative Store Bypass disabled */
 +#define PFA_SPEC_SSB_FORCE_DISABLE 4	/* Speculative Store Bypass force disabled*/
  
 -#define TASK_PFA_SET(name, func)					\
 -	static inline void task_set_##func(struct task_struct *p)	\
 -	{ set_bit(PFA_##name, &p->atomic_flags); }
 -
 -#define TASK_PFA_CLEAR(name, func)					\
 -	static inline void task_clear_##func(struct task_struct *p)	\
 -	{ clear_bit(PFA_##name, &p->atomic_flags); }
 +#define TASK_PFA_TEST(name, func)                                      \
 +       static inline bool task_##func(struct task_struct *p)           \
 +       { return test_bit(PFA_##name, &p->atomic_flags); }
 +#define TASK_PFA_SET(name, func)                                       \
 +       static inline void task_set_##func(struct task_struct *p)       \
 +       { set_bit(PFA_##name, &p->atomic_flags); }
 +#define TASK_PFA_CLEAR(name, func)                                     \
 +       static inline void task_clear_##func(struct task_struct *p)     \
 +       { clear_bit(PFA_##name, &p->atomic_flags); }
  
  TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)
  TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)
@@@ -2163,722 -1488,164 +2174,726 @@@ TASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ss
  TASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)
  TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)
  
 -TASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)
 -TASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)
 -TASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)
++TASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)
++TASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)
++TASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)
++
 +TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 +TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 +
 +/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */
 +static inline gfp_t memalloc_noio_flags(gfp_t flags)
 +{
 +	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
 +		flags &= ~__GFP_IO;
 +	return flags;
 +}
 +
 +static inline unsigned int memalloc_noio_save(void)
 +{
 +	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
 +	current->flags |= PF_MEMALLOC_NOIO;
 +	return flags;
 +}
 +
 +static inline void memalloc_noio_restore(unsigned int flags)
 +{
 +	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
 +}
 +
 +/*
 + * task->jobctl flags
 + */
 +#define JOBCTL_STOP_SIGMASK	0xffff	/* signr of the last group stop */
 +
 +#define JOBCTL_STOP_DEQUEUED_BIT 16	/* stop signal dequeued */
 +#define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
 +#define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
 +#define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
 +#define JOBCTL_TRAP_NOTIFY_BIT	20	/* trap for NOTIFY */
 +#define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 +#define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
 +
 +#define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
 +#define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
 +#define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
 +#define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
 +#define JOBCTL_TRAP_NOTIFY	(1 << JOBCTL_TRAP_NOTIFY_BIT)
 +#define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
 +#define JOBCTL_LISTENING	(1 << JOBCTL_LISTENING_BIT)
 +
 +#define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 +#define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 +
 +extern bool task_set_jobctl_pending(struct task_struct *task,
 +				    unsigned int mask);
 +extern void task_clear_jobctl_trapping(struct task_struct *task);
 +extern void task_clear_jobctl_pending(struct task_struct *task,
 +				      unsigned int mask);
 +
 +#ifdef CONFIG_PREEMPT_RCU
 +
 +#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
 +#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
 +
 +static inline void rcu_copy_process(struct task_struct *p)
 +{
 +	p->rcu_read_lock_nesting = 0;
 +	p->rcu_read_unlock_special = 0;
 +#ifdef CONFIG_TREE_PREEMPT_RCU
 +	p->rcu_blocked_node = NULL;
 +#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 +#ifdef CONFIG_RCU_BOOST
 +	p->rcu_boost_mutex = NULL;
 +#endif /* #ifdef CONFIG_RCU_BOOST */
 +	INIT_LIST_HEAD(&p->rcu_node_entry);
 +}
 +
 +#else
 +
 +static inline void rcu_copy_process(struct task_struct *p)
 +{
 +}
 +
 +#endif
 +
 +static inline void tsk_restore_flags(struct task_struct *task,
 +				unsigned long orig_flags, unsigned long flags)
 +{
 +	task->flags &= ~flags;
 +	task->flags |= orig_flags & flags;
 +}
 +
 +extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,
 +				     const struct cpumask *trial);
 +extern int task_can_attach(struct task_struct *p,
 +			   const struct cpumask *cs_cpus_allowed);
 +#ifdef CONFIG_SMP
 +extern void do_set_cpus_allowed(struct task_struct *p,
 +			       const struct cpumask *new_mask);
 +
 +extern int set_cpus_allowed_ptr(struct task_struct *p,
 +				const struct cpumask *new_mask);
 +#else
 +static inline void do_set_cpus_allowed(struct task_struct *p,
 +				      const struct cpumask *new_mask)
 +{
 +}
 +static inline int set_cpus_allowed_ptr(struct task_struct *p,
 +				       const struct cpumask *new_mask)
 +{
 +	if (!cpumask_test_cpu(0, new_mask))
 +		return -EINVAL;
 +	return 0;
 +}
 +#endif
 +
 +#ifdef CONFIG_NO_HZ_COMMON
 +void calc_load_enter_idle(void);
 +void calc_load_exit_idle(void);
 +#else
 +static inline void calc_load_enter_idle(void) { }
 +static inline void calc_load_exit_idle(void) { }
 +#endif /* CONFIG_NO_HZ_COMMON */
 +
 +#ifndef CONFIG_CPUMASK_OFFSTACK
 +static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 +{
 +	return set_cpus_allowed_ptr(p, &new_mask);
 +}
 +#endif
 +
 +/*
 + * Do not use outside of architecture code which knows its limitations.
 + *
 + * sched_clock() has no promise of monotonicity or bounded drift between
 + * CPUs, use (which you should not) requires disabling IRQs.
 + *
 + * Please use one of the three interfaces below.
 + */
 +extern unsigned long long notrace sched_clock(void);
 +/*
 + * See the comment in kernel/sched/clock.c
 + */
 +extern u64 cpu_clock(int cpu);
 +extern u64 local_clock(void);
 +extern u64 running_clock(void);
 +extern u64 sched_clock_cpu(int cpu);
 +
 +
 +extern void sched_clock_init(void);
 +
 +#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
 +static inline void sched_clock_tick(void)
 +{
 +}
 +
 +static inline void sched_clock_idle_sleep_event(void)
 +{
 +}
 +
 +static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 +{
 +}
 +#else
 +/*
 + * Architectures can set this to 1 if they have specified
 + * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
 + * but then during bootup it turns out that sched_clock()
 + * is reliable after all:
 + */
 +extern int sched_clock_stable(void);
 +extern void set_sched_clock_stable(void);
 +extern void clear_sched_clock_stable(void);
 +
 +extern void sched_clock_tick(void);
 +extern void sched_clock_idle_sleep_event(void);
 +extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 +#endif
 +
 +#ifdef CONFIG_IRQ_TIME_ACCOUNTING
 +/*
 + * An i/f to runtime opt-in for irq time accounting based off of sched_clock.
 + * The reason for this explicit opt-in is not to have perf penalty with
 + * slow sched_clocks.
 + */
 +extern void enable_sched_clock_irqtime(void);
 +extern void disable_sched_clock_irqtime(void);
 +#else
 +static inline void enable_sched_clock_irqtime(void) {}
 +static inline void disable_sched_clock_irqtime(void) {}
 +#endif
 +
 +extern unsigned long long
 +task_sched_runtime(struct task_struct *task);
 +
 +/* sched_exec is called by processes performing an exec */
 +#ifdef CONFIG_SMP
 +extern void sched_exec(void);
 +#else
 +#define sched_exec()   {}
 +#endif
 +
 +extern void sched_clock_idle_sleep_event(void);
 +extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +extern void idle_task_exit(void);
 +#else
 +static inline void idle_task_exit(void) {}
 +#endif
 +
 +#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)
 +extern void wake_up_nohz_cpu(int cpu);
 +#else
 +static inline void wake_up_nohz_cpu(int cpu) { }
 +#endif
 +
 +#ifdef CONFIG_NO_HZ_FULL
 +extern bool sched_can_stop_tick(void);
 +extern u64 scheduler_tick_max_deferment(void);
 +#else
 +static inline bool sched_can_stop_tick(void) { return false; }
 +#endif
 +
 +#ifdef CONFIG_SCHED_AUTOGROUP
 +extern void sched_autogroup_create_attach(struct task_struct *p);
 +extern void sched_autogroup_detach(struct task_struct *p);
 +extern void sched_autogroup_fork(struct signal_struct *sig);
 +extern void sched_autogroup_exit(struct signal_struct *sig);
 +extern void sched_autogroup_exit_task(struct task_struct *p);
 +#ifdef CONFIG_PROC_FS
 +extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
 +extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);
 +#endif
 +#else
 +static inline void sched_autogroup_create_attach(struct task_struct *p) { }
 +static inline void sched_autogroup_detach(struct task_struct *p) { }
 +static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 +static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 +static inline void sched_autogroup_exit_task(struct task_struct *p) { }
 +#endif
 +
 +extern int yield_to(struct task_struct *p, bool preempt);
 +extern void set_user_nice(struct task_struct *p, long nice);
 +extern int task_prio(const struct task_struct *p);
 +extern int task_nice(const struct task_struct *p);
 +extern int can_nice(const struct task_struct *p, const int nice);
 +extern int task_curr(const struct task_struct *p);
 +extern int idle_cpu(int cpu);
 +extern int sched_setscheduler(struct task_struct *, int,
 +			      const struct sched_param *);
 +extern int sched_setscheduler_nocheck(struct task_struct *, int,
 +				      const struct sched_param *);
 +extern int sched_setattr(struct task_struct *,
 +			 const struct sched_attr *);
 +extern struct task_struct *idle_task(int cpu);
 +/**
 + * is_idle_task - is the specified task an idle task?
 + * @p: the task in question.
 + *
 + * Return: 1 if @p is an idle task. 0 otherwise.
 + */
 +static inline bool is_idle_task(const struct task_struct *p)
 +{
 +	return p->pid == 0;
 +}
 +extern struct task_struct *curr_task(int cpu);
 +extern void set_curr_task(int cpu, struct task_struct *p);
 +
 +void yield(void);
 +
 +/*
 + * The default (Linux) execution domain.
 + */
 +extern struct exec_domain	default_exec_domain;
 +
 +union thread_union {
 +	struct thread_info thread_info;
 +	unsigned long stack[THREAD_SIZE/sizeof(long)];
 +};
 +
 +#ifndef __HAVE_ARCH_KSTACK_END
 +static inline int kstack_end(void *addr)
 +{
 +	/* Reliable end of stack detection:
 +	 * Some APM bios versions misalign the stack
 +	 */
 +	return !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));
 +}
 +#endif
 +
 +extern union thread_union init_thread_union;
 +extern struct task_struct init_task;
 +
 +extern struct   mm_struct init_mm;
 +
 +extern struct pid_namespace init_pid_ns;
 +
 +/*
 + * find a task by one of its numerical ids
 + *
 + * find_task_by_pid_ns():
 + *      finds a task by its pid in the specified namespace
 + * find_task_by_vpid():
 + *      finds a task by its virtual pid
 + *
 + * see also find_vpid() etc in include/linux/pid.h
 + */
 +
 +extern struct task_struct *find_task_by_vpid(pid_t nr);
 +extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 +		struct pid_namespace *ns);
 +
 +extern void __set_special_pids(struct pid *pid);
 +
 +/* per-UID process charging. */
 +extern struct user_struct * alloc_uid(kuid_t);
 +static inline struct user_struct *get_uid(struct user_struct *u)
 +{
 +	atomic_inc(&u->__count);
 +	return u;
 +}
 +extern void free_uid(struct user_struct *);
 +
 +#include <asm/current.h>
 +
 +extern void xtime_update(unsigned long ticks);
 +
 +extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 +extern int wake_up_process(struct task_struct *tsk);
 +extern void wake_up_new_task(struct task_struct *tsk);
 +#ifdef CONFIG_SMP
 + extern void kick_process(struct task_struct *tsk);
 +#else
 + static inline void kick_process(struct task_struct *tsk) { }
 +#endif
 +extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
 +extern void sched_dead(struct task_struct *p);
 +
 +extern void proc_caches_init(void);
 +extern void flush_signals(struct task_struct *);
 +extern void __flush_signals(struct task_struct *);
 +extern void ignore_signals(struct task_struct *);
 +extern void flush_signal_handlers(struct task_struct *, int force_default);
 +extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
 +
 +static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
 +{
 +	unsigned long flags;
 +	int ret;
 +
 +	spin_lock_irqsave(&tsk->sighand->siglock, flags);
 +	ret = dequeue_signal(tsk, mask, info);
 +	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 +
 +	return ret;
 +}
 +
 +extern void block_all_signals(int (*notifier)(void *priv), void *priv,
 +			      sigset_t *mask);
 +extern void unblock_all_signals(void);
 +extern void release_task(struct task_struct * p);
 +extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 +extern int force_sigsegv(int, struct task_struct *);
 +extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 +extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 +extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
 +extern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,
 +				const struct cred *, u32);
 +extern int kill_pgrp(struct pid *pid, int sig, int priv);
 +extern int kill_pid(struct pid *pid, int sig, int priv);
 +extern int kill_proc_info(int, struct siginfo *, pid_t);
 +extern __must_check bool do_notify_parent(struct task_struct *, int);
 +extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 +extern void force_sig(int, struct task_struct *);
 +extern int send_sig(int, struct task_struct *, int);
 +extern int zap_other_threads(struct task_struct *p);
 +extern struct sigqueue *sigqueue_alloc(void);
 +extern void sigqueue_free(struct sigqueue *);
 +extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 +extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 +
 +static inline void restore_saved_sigmask(void)
 +{
 +	if (test_and_clear_restore_sigmask())
 +		__set_current_blocked(&current->saved_sigmask);
 +}
 +
 +static inline sigset_t *sigmask_to_save(void)
 +{
 +	sigset_t *res = &current->blocked;
 +	if (unlikely(test_restore_sigmask()))
 +		res = &current->saved_sigmask;
 +	return res;
 +}
 +
 +static inline int kill_cad_pid(int sig, int priv)
 +{
 +	return kill_pid(cad_pid, sig, priv);
 +}
 +
 +/* These can be the second arg to send_sig_info/send_group_sig_info.  */
 +#define SEND_SIG_NOINFO ((struct siginfo *) 0)
 +#define SEND_SIG_PRIV	((struct siginfo *) 1)
 +#define SEND_SIG_FORCED	((struct siginfo *) 2)
 +
 +/*
 + * True if we are on the alternate signal stack.
 + */
 +static inline int on_sig_stack(unsigned long sp)
 +{
 +#ifdef CONFIG_STACK_GROWSUP
 +	return sp >= current->sas_ss_sp &&
 +		sp - current->sas_ss_sp < current->sas_ss_size;
 +#else
 +	return sp > current->sas_ss_sp &&
 +		sp - current->sas_ss_sp <= current->sas_ss_size;
 +#endif
 +}
 +
 +static inline int sas_ss_flags(unsigned long sp)
 +{
 +	return (current->sas_ss_size == 0 ? SS_DISABLE
 +		: on_sig_stack(sp) ? SS_ONSTACK : 0);
 +}
 +
 +static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
 +{
 +	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))
 +#ifdef CONFIG_STACK_GROWSUP
 +		return current->sas_ss_sp;
 +#else
 +		return current->sas_ss_sp + current->sas_ss_size;
 +#endif
 +	return sp;
 +}
 +
 +/*
 + * Routines for handling mm_structs
 + */
 +extern struct mm_struct * mm_alloc(void);
 +
 +/* mmdrop drops the mm and the page tables */
 +extern void __mmdrop(struct mm_struct *);
 +static inline void mmdrop(struct mm_struct *mm)
 +{
 +	/*
 +	 * The implicit full barrier implied by atomic_dec_and_test() is
 +	 * required by the membarrier system call before returning to
 +	 * user-space, after storing to rq->curr.
 +	 */
 +	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 +		__mmdrop(mm);
 +}
 +
 +static inline bool mmget_not_zero(struct mm_struct *mm)
 +{
 +	return atomic_inc_not_zero(&mm->mm_users);
 +}
 +
 +/* mmput gets rid of the mappings and all user-space */
 +extern void mmput(struct mm_struct *);
 +/* Grab a reference to a task's mm, if it is not already going away */
 +extern struct mm_struct *get_task_mm(struct task_struct *task);
 +/*
 + * Grab a reference to a task's mm, if it is not already going away
 + * and ptrace_may_access with the mode parameter passed to it
 + * succeeds.
 + */
 +extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
 +/* Remove the current tasks stale references to the old mm_struct */
 +extern void mm_release(struct task_struct *, struct mm_struct *);
 +/* Allocate a new mm structure and copy contents from tsk->mm */
 +extern struct mm_struct *dup_mm(struct task_struct *tsk);
 +
 +extern int copy_thread(unsigned long, unsigned long, unsigned long,
 +			struct task_struct *);
 +extern void flush_thread(void);
 +extern void exit_thread(void);
 +
 +extern void exit_files(struct task_struct *);
 +extern void __cleanup_sighand(struct sighand_struct *);
 +
 +extern void exit_itimers(struct signal_struct *);
 +extern void flush_itimer_signals(void);
 +
 +extern void do_group_exit(int);
 +
 +extern int allow_signal(int);
 +extern int disallow_signal(int);
 +
 +extern int do_execve(struct filename *,
 +		     const char __user * const __user *,
 +		     const char __user * const __user *);
 +extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 +struct task_struct *fork_idle(int);
 +extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 +
 +extern void __set_task_comm(struct task_struct *tsk, char *from, bool exec);
 +static inline void set_task_comm(struct task_struct *tsk, char *from)
 +{
 +	__set_task_comm(tsk, from, false);
 +}
 +extern char *get_task_comm(char *to, struct task_struct *tsk);
 +
 +#ifdef CONFIG_SMP
 +void scheduler_ipi(void);
 +extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 +#else
 +static inline void scheduler_ipi(void) { }
 +static inline unsigned long wait_task_inactive(struct task_struct *p,
 +					       long match_state)
 +{
 +	return 1;
 +}
 +#endif
 +
 +#define next_task(p) \
 +	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
 +
 +#define for_each_process(p) \
 +	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
 +
 +extern bool current_is_single_threaded(void);
 +
 +/*
 + * Careful: do_each_thread/while_each_thread is a double loop so
 + *          'break' will not work as expected - use goto instead.
 + */
 +#define do_each_thread(g, t) \
 +	for (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do
 +
 +#define while_each_thread(g, t) \
 +	while ((t = next_thread(t)) != g)
  
 -TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 -TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 +#define __for_each_thread(signal, t)	\
 +	list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)
  
 -TASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)
 -TASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)
 -TASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)
 +#define for_each_thread(p, t)		\
 +	__for_each_thread((p)->signal, t)
  
 -TASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)
 -TASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)
 +/* Careful: this is a double loop, 'break' won't work as expected. */
 +#define for_each_process_thread(p, t)	\
 +	for_each_process(p) for_each_thread(p, t)
  
 -static inline void
 -current_restore_flags(unsigned long orig_flags, unsigned long flags)
 +static inline int get_nr_threads(struct task_struct *tsk)
  {
 -	current->flags &= ~flags;
 -	current->flags |= orig_flags & flags;
 +	return tsk->signal->nr_threads;
  }
  
 -extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
 -extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);
 -#ifdef CONFIG_SMP
 -extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);
 -extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);
 -#else
 -static inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 +static inline bool thread_group_leader(struct task_struct *p)
  {
 +	return p->exit_signal >= 0;
  }
 -static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 +
 +/* Do to the insanities of de_thread it is possible for a process
 + * to have the pid of the thread group leader without actually being
 + * the thread group leader.  For iteration through the pids in proc
 + * all we care about is that we have a task with the appropriate
 + * pid, we don't actually care if we have the right task.
 + */
 +static inline int has_group_leader_pid(struct task_struct *p)
  {
 -	if (!cpumask_test_cpu(0, new_mask))
 -		return -EINVAL;
 -	return 0;
 +	return p->pid == p->tgid;
  }
 -#endif
  
 -#ifndef cpu_relax_yield
 -#define cpu_relax_yield() cpu_relax()
 -#endif
 +static inline
 +int same_thread_group(struct task_struct *p1, struct task_struct *p2)
 +{
 +	return p1->tgid == p2->tgid;
 +}
  
 -extern int yield_to(struct task_struct *p, bool preempt);
 -extern void set_user_nice(struct task_struct *p, long nice);
 -extern int task_prio(const struct task_struct *p);
 +static inline struct task_struct *next_thread(const struct task_struct *p)
 +{
 +	return list_entry_rcu(p->thread_group.next,
 +			      struct task_struct, thread_group);
 +}
  
 -/**
 - * task_nice - return the nice value of a given task.
 - * @p: the task in question.
 +/*
 + * simple wrapper function that always returns NULL
 + * for RHEL-7 to enable upstream backports that have this check in it.
 + * Returning NULL means the task's stack is direct-mapped and not vmalloc'd,
 + * which is the case for RHEL-7.
   *
 - * Return: The nice value [ -20 ... 0 ... 19 ].
   */
 -static inline int task_nice(const struct task_struct *p)
 +static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
  {
 -	return PRIO_TO_NICE((p)->static_prio);
 +	return NULL;
  }
  
 -extern int can_nice(const struct task_struct *p, const int nice);
 -extern int task_curr(const struct task_struct *p);
 -extern int idle_cpu(int cpu);
 -extern int available_idle_cpu(int cpu);
 -extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
 -extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
 -extern int sched_setattr(struct task_struct *, const struct sched_attr *);
 -extern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);
 -extern struct task_struct *idle_task(int cpu);
 +static inline int thread_group_empty(struct task_struct *p)
 +{
 +	return list_empty(&p->thread_group);
 +}
  
 -/**
 - * is_idle_task - is the specified task an idle task?
 - * @p: the task in question.
 +#define delay_group_leader(p) \
 +		(thread_group_leader(p) && !thread_group_empty(p))
 +
 +/*
 + * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
 + * subscriptions and synchronises with wait4().  Also used in procfs.  Also
 + * pins the final release of task.io_context.  Also protects ->cpuset and
 + * ->cgroup.subsys[]. And ->vfork_done.
   *
 - * Return: 1 if @p is an idle task. 0 otherwise.
 + * Nests both inside and outside of qread_lock(&tasklist_lock).
 + * It must not be nested with qwrite_lock_irq(&tasklist_lock),
 + * neither inside nor outside.
   */
 -static inline bool is_idle_task(const struct task_struct *p)
 +static inline void task_lock(struct task_struct *p)
  {
 -	return !!(p->flags & PF_IDLE);
 +	spin_lock(&p->alloc_lock);
  }
  
 -extern struct task_struct *curr_task(int cpu);
 -extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 +static inline void task_unlock(struct task_struct *p)
 +{
 +	spin_unlock(&p->alloc_lock);
 +}
  
 -void yield(void);
 +extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 +							unsigned long *flags);
  
 -union thread_union {
 -#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 -	struct task_struct task;
 -#endif
 -#ifndef CONFIG_THREAD_INFO_IN_TASK
 -	struct thread_info thread_info;
 -#endif
 -	unsigned long stack[THREAD_SIZE/sizeof(long)];
 -};
 +static inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
 +						       unsigned long *flags)
 +{
 +	struct sighand_struct *ret;
  
 -#ifndef CONFIG_THREAD_INFO_IN_TASK
 -extern struct thread_info init_thread_info;
 -#endif
 +	ret = __lock_task_sighand(tsk, flags);
 +	(void)__cond_lock(&tsk->sighand->siglock, ret);
 +	return ret;
 +}
  
 -extern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];
 +static inline void unlock_task_sighand(struct task_struct *tsk,
 +						unsigned long *flags)
 +{
 +	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 +}
  
 -#ifdef CONFIG_THREAD_INFO_IN_TASK
 -static inline struct thread_info *task_thread_info(struct task_struct *task)
 +#ifdef CONFIG_CGROUPS
 +static inline void threadgroup_change_begin(struct task_struct *tsk)
  {
 -	return &task->thread_info;
 +	down_read(&tsk->signal->group_rwsem);
 +}
 +static inline void threadgroup_change_end(struct task_struct *tsk)
 +{
 +	up_read(&tsk->signal->group_rwsem);
  }
 -#elif !defined(__HAVE_THREAD_FUNCTIONS)
 -# define task_thread_info(task)	((struct thread_info *)(task)->stack)
 -#endif
  
 -/*
 - * find a task by one of its numerical ids
 +/**
 + * threadgroup_lock - lock threadgroup
 + * @tsk: member task of the threadgroup to lock
   *
 - * find_task_by_pid_ns():
 - *      finds a task by its pid in the specified namespace
 - * find_task_by_vpid():
 - *      finds a task by its virtual pid
 + * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter
 + * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or
 + * change ->group_leader/pid.  This is useful for cases where the threadgroup
 + * needs to stay stable across blockable operations.
   *
 - * see also find_vpid() etc in include/linux/pid.h
 + * fork and exit paths explicitly call threadgroup_change_{begin|end}() for
 + * synchronization.  While held, no new task will be added to threadgroup
 + * and no existing live task will have its PF_EXITING set.
 + *
 + * de_thread() does threadgroup_change_{begin|end}() when a non-leader
 + * sub-thread becomes a new leader.
   */
 +static inline void threadgroup_lock(struct task_struct *tsk)
 +{
 +	down_write(&tsk->signal->group_rwsem);
 +}
  
 -extern struct task_struct *find_task_by_vpid(pid_t nr);
 -extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
 -
 -/*
 - * find a task by its virtual pid and get the task struct
 +/**
 + * threadgroup_unlock - unlock threadgroup
 + * @tsk: member task of the threadgroup to unlock
 + *
 + * Reverse threadgroup_lock().
   */
 -extern struct task_struct *find_get_task_by_vpid(pid_t nr);
 -
 -extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 -extern int wake_up_process(struct task_struct *tsk);
 -extern void wake_up_new_task(struct task_struct *tsk);
 -
 -#ifdef CONFIG_SMP
 -extern void kick_process(struct task_struct *tsk);
 +static inline void threadgroup_unlock(struct task_struct *tsk)
 +{
 +	up_write(&tsk->signal->group_rwsem);
 +}
  #else
 -static inline void kick_process(struct task_struct *tsk) { }
 +static inline void threadgroup_change_begin(struct task_struct *tsk) {}
 +static inline void threadgroup_change_end(struct task_struct *tsk) {}
 +static inline void threadgroup_lock(struct task_struct *tsk) {}
 +static inline void threadgroup_unlock(struct task_struct *tsk) {}
  #endif
  
 -extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
 +#ifndef __HAVE_THREAD_FUNCTIONS
 +
 +#define task_thread_info(task)	((struct thread_info *)(task)->stack)
 +#define task_stack_page(task)	((task)->stack)
  
 -static inline void set_task_comm(struct task_struct *tsk, const char *from)
 +static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
  {
 -	__set_task_comm(tsk, from, false);
 +	*task_thread_info(p) = *task_thread_info(org);
 +	task_thread_info(p)->task = p;
  }
  
 -extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);
 -#define get_task_comm(buf, tsk) ({			\
 -	BUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);	\
 -	__get_task_comm(buf, sizeof(buf), tsk);		\
 -})
 +static inline unsigned long *end_of_stack(struct task_struct *p)
 +{
 +	return (unsigned long *)(task_thread_info(p) + 1);
 +}
  
 -#ifdef CONFIG_SMP
 -void scheduler_ipi(void);
 -extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 -#else
 -static inline void scheduler_ipi(void) { }
 -static inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 +#endif
 +
 +static inline int object_is_on_stack(void *obj)
  {
 -	return 1;
 +	void *stack = task_stack_page(current);
 +
 +	return (obj >= stack) && (obj < (stack + THREAD_SIZE));
 +}
 +
 +extern void thread_info_cache_init(void);
 +
 +#ifdef CONFIG_DEBUG_STACK_USAGE
 +static inline unsigned long stack_not_used(struct task_struct *p)
 +{
 +	unsigned long *n = end_of_stack(p);
 +
 +	do { 	/* Skip over canary */
 +		n++;
 +	} while (!*n);
 +
 +	return (unsigned long)n - (unsigned long)end_of_stack(p);
  }
  #endif
  
diff --cc tools/include/uapi/linux/prctl.h
index 2c762bde8b17,094bb03b9cc2..000000000000
--- a/tools/include/uapi/linux/prctl.h
+++ b/tools/include/uapi/linux/prctl.h
@@@ -201,4 -207,26 +201,29 @@@ struct prctl_mm_map 
  # define PR_SVE_VL_LEN_MASK		0xffff
  # define PR_SVE_VL_INHERIT		(1 << 17) /* inherit across exec */
  
++<<<<<<< HEAD
++=======
+ /* Per task speculation control */
+ #define PR_GET_SPECULATION_CTRL		52
+ #define PR_SET_SPECULATION_CTRL		53
+ /* Speculation control variants */
+ # define PR_SPEC_STORE_BYPASS		0
+ # define PR_SPEC_INDIRECT_BRANCH	1
+ /* Return and control values for PR_SET/GET_SPECULATION_CTRL */
+ # define PR_SPEC_NOT_AFFECTED		0
+ # define PR_SPEC_PRCTL			(1UL << 0)
+ # define PR_SPEC_ENABLE			(1UL << 1)
+ # define PR_SPEC_DISABLE		(1UL << 2)
+ # define PR_SPEC_FORCE_DISABLE		(1UL << 3)
+ # define PR_SPEC_DISABLE_NOEXEC		(1UL << 4)
+ 
+ /* Reset arm64 pointer authentication keys */
+ #define PR_PAC_RESET_KEYS		54
+ # define PR_PAC_APIAKEY			(1UL << 0)
+ # define PR_PAC_APIBKEY			(1UL << 1)
+ # define PR_PAC_APDAKEY			(1UL << 2)
+ # define PR_PAC_APDBKEY			(1UL << 3)
+ # define PR_PAC_APGAKEY			(1UL << 4)
+ 
++>>>>>>> 71368af9027f (x86/speculation: Add PR_SPEC_DISABLE_NOEXEC)
  #endif /* _LINUX_PRCTL_H */
* Unmerged path Documentation/userspace-api/spec_ctrl.txt
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/process.c
* Unmerged path include/linux/sched.h
diff --git a/include/uapi/linux/prctl.h b/include/uapi/linux/prctl.h
index a817b5c2de19..02376de8e771 100644
--- a/include/uapi/linux/prctl.h
+++ b/include/uapi/linux/prctl.h
@@ -203,5 +203,6 @@ struct prctl_mm_map {
 # define PR_SPEC_ENABLE			(1UL << 1)
 # define PR_SPEC_DISABLE		(1UL << 2)
 # define PR_SPEC_FORCE_DISABLE		(1UL << 3)
+# define PR_SPEC_DISABLE_NOEXEC		(1UL << 4)
 
 #endif /* _LINUX_PRCTL_H */
* Unmerged path tools/include/uapi/linux/prctl.h
