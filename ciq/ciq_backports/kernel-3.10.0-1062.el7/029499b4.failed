KVM: x86: MMU: Make mmu_set_spte() return emulate value

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
commit 029499b477389f7d6486c8c759a8498bcfecf322
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/029499b4.failed

mmu_set_spte()'s code is based on the assumption that the emulate
parameter has a valid pointer value if set_spte() returns true and
write_fault is not zero.  In other cases, emulate may be NULL, so a
NULL-check is needed.

Stop passing emulate pointer and make mmu_set_spte() return the emulate
value instead to clean up this complex interface.  Prefetch functions
can just throw away the return value.

	Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 029499b477389f7d6486c8c759a8498bcfecf322)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index dc9dadcf984a,74c120cd63fd..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2797,10 -2564,9 +2797,16 @@@ done
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 +			 unsigned pte_access, int write_fault, int *emulate,
 +			 int level, gfn_t gfn, kvm_pfn_t pfn, bool speculative,
 +			 bool host_writable)
++=======
+ static bool mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep, unsigned pte_access,
+ 			 int write_fault, int level, gfn_t gfn, pfn_t pfn,
+ 			 bool speculative, bool host_writable)
++>>>>>>> 029499b47738 (KVM: x86: MMU: Make mmu_set_spte() return emulate value)
  {
  	int was_rmapped = 0;
  	int rmap_count;
@@@ -2857,10 -2624,12 +2864,12 @@@
  	}
  
  	kvm_release_pfn_clean(pfn);
+ 
+ 	return emulate;
  }
  
 -static pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
 -				     bool no_dirty_log)
 +static kvm_pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
 +					 bool no_dirty_log)
  {
  	struct kvm_memory_slot *slot;
  
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index 402565943e9c..f28273432d98 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -512,8 +512,8 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 * we call mmu_set_spte() with host_writable = true because
 	 * pte_prefetch_gfn_to_pfn always gets a writable pfn.
 	 */
-	mmu_set_spte(vcpu, spte, pte_access, 0, NULL, PT_PAGE_TABLE_LEVEL,
-		     gfn, pfn, true, true);
+	mmu_set_spte(vcpu, spte, pte_access, 0, PT_PAGE_TABLE_LEVEL, gfn, pfn,
+		     true, true);
 
 	return true;
 }
@@ -593,7 +593,7 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,
 	struct kvm_mmu_page *sp = NULL;
 	struct kvm_shadow_walk_iterator it;
 	unsigned direct_access, access = gw->pt_access;
-	int top_level, emulate = 0;
+	int top_level, emulate;
 
 	direct_access = gw->pte_access;
 
@@ -659,8 +659,8 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,
 	}
 
 	clear_sp_write_flooding_count(it.sptep);
-	mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault, &emulate,
-		     it.level, gw->gfn, pfn, prefault, map_writable);
+	emulate = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
+			       it.level, gw->gfn, pfn, prefault, map_writable);
 	FNAME(pte_prefetch)(vcpu, gw, it.sptep);
 
 	return emulate;
