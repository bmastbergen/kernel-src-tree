locking/lockdep: Provide a type check for lock_is_held

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit f8319483f57f1ca22370f4150bb990aca7728a67
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f8319483.failed

Christoph requested lockdep_assert_held() variants that distinguish
between held-for-read or held-for-write.

Provide:

  int lock_is_held_type(struct lockdep_map *lock, int read)

which takes the same argument as lock_acquire(.read) and matches it to
the held_lock instance.

Use of this function should be gated by the debug_locks variable. When
that is 0 the return value of the lock_is_held_type() function is
undefined. This is done to allow both negative and positive tests for
holding locks.

By default we provide (positive) lockdep_assert_held{,_exclusive,_read}()
macros.

Requested-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Tested-by: Jens Axboe <axboe@fb.com>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit f8319483f57f1ca22370f4150bb990aca7728a67)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/lockdep.c
diff --cc kernel/lockdep.c
index 45fa9acd8f03,cff580a6edf9..000000000000
--- a/kernel/lockdep.c
+++ b/kernel/lockdep.c
@@@ -3182,9 -3327,9 +3182,9 @@@ static int __lock_acquire(struct lockde
  		chain_key = 0;
  		chain_head = 1;
  	}
 -	chain_key = iterate_chain_key(chain_key, class_idx);
 +	chain_key = iterate_chain_key(chain_key, id);
  
- 	if (nest_lock && !__lock_is_held(nest_lock))
+ 	if (nest_lock && !__lock_is_held(nest_lock, -1))
  		return print_lock_nested_lock_not_held(curr, hlock, ip);
  
  	if (!validate_chain(curr, lock, hlock, chain_head, chain_key))
@@@ -3444,76 -3575,75 +3444,80 @@@ found_it
  	return 1;
  }
  
 -static int __lock_is_held(struct lockdep_map *lock, int read)
++<<<<<<< HEAD:kernel/lockdep.c
 +/*
 + * Remove the lock to the list of currently held locks - this gets
 + * called on mutex_unlock()/spin_unlock*() (or on a failed
 + * mutex_lock_interruptible()). This is done for unlocks that nest
 + * perfectly. (i.e. the current top of the lock-stack is unlocked)
 + */
 +static int lock_release_nested(struct task_struct *curr,
 +			       struct lockdep_map *lock, unsigned long ip)
  {
 -	struct task_struct *curr = current;
 -	int i;
 -
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 -
 -		if (match_held_lock(hlock, lock)) {
 -			if (read == -1 || hlock->read == read)
 -				return 1;
 -
 -			return 0;
 -		}
 -	}
 +	struct held_lock *hlock;
 +	unsigned int depth;
  
 -	return 0;
 -}
 +	/*
 +	 * Pop off the top of the lock stack:
 +	 */
 +	depth = curr->lockdep_depth - 1;
 +	hlock = curr->held_locks + depth;
  
 -static struct pin_cookie __lock_pin_lock(struct lockdep_map *lock)
 -{
 -	struct pin_cookie cookie = NIL_COOKIE;
 -	struct task_struct *curr = current;
 -	int i;
 +	/*
 +	 * Is the unlock non-nested:
 +	 */
 +	if (hlock->instance != lock || hlock->references)
 +		return lock_release_non_nested(curr, lock, ip);
 +	curr->lockdep_depth--;
  
 -	if (unlikely(!debug_locks))
 -		return cookie;
 +	/*
 +	 * No more locks, but somehow we've got hash left over, who left it?
 +	 */
 +	if (DEBUG_LOCKS_WARN_ON(!depth && (hlock->prev_chain_key != 0)))
 +		return 0;
  
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 +	curr->curr_chain_key = hlock->prev_chain_key;
  
 -		if (match_held_lock(hlock, lock)) {
 -			/*
 -			 * Grab 16bits of randomness; this is sufficient to not
 -			 * be guessable and still allows some pin nesting in
 -			 * our u32 pin_count.
 -			 */
 -			cookie.val = 1 + (prandom_u32() >> 16);
 -			hlock->pin_count += cookie.val;
 -			return cookie;
 -		}
 -	}
 +	lock_release_holdtime(hlock);
  
 -	WARN(1, "pinning an unheld lock\n");
 -	return cookie;
 +#ifdef CONFIG_DEBUG_LOCKDEP
 +	hlock->prev_chain_key = 0;
 +	hlock->class_idx = 0;
 +	hlock->acquire_ip = 0;
 +	hlock->irq_context = 0;
 +#endif
 +	return 1;
  }
  
 -static void __lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 +/*
 + * Remove the lock to the list of currently held locks - this gets
 + * called on mutex_unlock()/spin_unlock*() (or on a failed
 + * mutex_lock_interruptible()). This is done for unlocks that nest
 + * perfectly. (i.e. the current top of the lock-stack is unlocked)
 + */
 +static void
 +__lock_release(struct lockdep_map *lock, int nested, unsigned long ip)
  {
  	struct task_struct *curr = current;
 -	int i;
  
 -	if (unlikely(!debug_locks))
 +	if (!check_unlock(curr, lock, ip))
  		return;
  
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 -
 -		if (match_held_lock(hlock, lock)) {
 -			hlock->pin_count += cookie.val;
 +	if (nested) {
 +		if (!lock_release_nested(curr, lock, ip))
 +			return;
 +	} else {
 +		if (!lock_release_non_nested(curr, lock, ip))
  			return;
 -		}
  	}
  
 -	WARN(1, "pinning an unheld lock\n");
 +	check_chain_key(curr);
  }
  
 -static void __lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 +static int __lock_is_held(struct lockdep_map *lock)
++=======
++static int __lock_is_held(struct lockdep_map *lock, int read)
++>>>>>>> f8319483f57f (locking/lockdep: Provide a type check for lock_is_held):kernel/locking/lockdep.c
  {
  	struct task_struct *curr = current;
  	int i;
@@@ -3521,11 -3651,23 +3525,15 @@@
  	for (i = 0; i < curr->lockdep_depth; i++) {
  		struct held_lock *hlock = curr->held_locks + i;
  
- 		if (match_held_lock(hlock, lock))
- 			return 1;
+ 		if (match_held_lock(hlock, lock)) {
 -			if (WARN(!hlock->pin_count, "unpinning an unpinned lock\n"))
 -				return;
 -
 -			hlock->pin_count -= cookie.val;
 -
 -			if (WARN((int)hlock->pin_count < 0, "pin count corrupted\n"))
 -				hlock->pin_count = 0;
++			if (read == -1 || hlock->read == read)
++				return 1;
+ 
 -			return;
++			return 0;
+ 		}
  	}
  
 -	WARN(1, "unpinning an unheld lock\n");
 +	return 0;
  }
  
  /*
@@@ -3648,8 -3791,62 +3656,8 @@@ int lock_is_held_type(struct lockdep_ma
  
  	return ret;
  }
- EXPORT_SYMBOL_GPL(lock_is_held);
+ EXPORT_SYMBOL_GPL(lock_is_held_type);
  
 -struct pin_cookie lock_pin_lock(struct lockdep_map *lock)
 -{
 -	struct pin_cookie cookie = NIL_COOKIE;
 -	unsigned long flags;
 -
 -	if (unlikely(current->lockdep_recursion))
 -		return cookie;
 -
 -	raw_local_irq_save(flags);
 -	check_flags(flags);
 -
 -	current->lockdep_recursion = 1;
 -	cookie = __lock_pin_lock(lock);
 -	current->lockdep_recursion = 0;
 -	raw_local_irq_restore(flags);
 -
 -	return cookie;
 -}
 -EXPORT_SYMBOL_GPL(lock_pin_lock);
 -
 -void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 -{
 -	unsigned long flags;
 -
 -	if (unlikely(current->lockdep_recursion))
 -		return;
 -
 -	raw_local_irq_save(flags);
 -	check_flags(flags);
 -
 -	current->lockdep_recursion = 1;
 -	__lock_repin_lock(lock, cookie);
 -	current->lockdep_recursion = 0;
 -	raw_local_irq_restore(flags);
 -}
 -EXPORT_SYMBOL_GPL(lock_repin_lock);
 -
 -void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 -{
 -	unsigned long flags;
 -
 -	if (unlikely(current->lockdep_recursion))
 -		return;
 -
 -	raw_local_irq_save(flags);
 -	check_flags(flags);
 -
 -	current->lockdep_recursion = 1;
 -	__lock_unpin_lock(lock, cookie);
 -	current->lockdep_recursion = 0;
 -	raw_local_irq_restore(flags);
 -}
 -EXPORT_SYMBOL_GPL(lock_unpin_lock);
 -
  void lockdep_set_current_reclaim_state(gfp_t gfp_mask)
  {
  	current->lockdep_reclaim_gfp = gfp_mask;
diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 67c46470a4a2..71c46aaa054c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -337,9 +337,18 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
 
-#define lockdep_is_held(lock)	lock_is_held(&(lock)->dep_map)
+/*
+ * Same "read" as for lock_acquire(), except -1 means any.
+ */
+extern int lock_is_held_type(struct lockdep_map *lock, int read);
+
+static inline int lock_is_held(struct lockdep_map *lock)
+{
+	return lock_is_held_type(lock, -1);
+}
 
-extern int lock_is_held(struct lockdep_map *lock);
+#define lockdep_is_held(lock)		lock_is_held(&(lock)->dep_map)
+#define lockdep_is_held_type(lock, r)	lock_is_held_type(&(lock)->dep_map, (r))
 
 extern void lock_set_class(struct lockdep_map *lock, const char *name,
 			   struct lock_class_key *key, unsigned int subclass,
@@ -363,6 +372,14 @@ extern void lockdep_trace_alloc(gfp_t mask);
 		WARN_ON(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
 
+#define lockdep_assert_held_exclusive(l)	do {			\
+		WARN_ON(debug_locks && !lockdep_is_held_type(l, 0));	\
+	} while (0)
+
+#define lockdep_assert_held_read(l)	do {				\
+		WARN_ON(debug_locks && !lockdep_is_held_type(l, 1));	\
+	} while (0)
+
 #define lockdep_assert_held_once(l)	do {				\
 		WARN_ON_ONCE(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
@@ -416,7 +433,11 @@ struct lock_class_key { };
 
 #define lockdep_depth(tsk)	(0)
 
+#define lockdep_is_held_type(l, r)		(1)
+
 #define lockdep_assert_held(l)			do { (void)(l); } while (0)
+#define lockdep_assert_held_exclusive(l)	do { (void)(l); } while (0)
+#define lockdep_assert_held_read(l)		do { (void)(l); } while (0)
 #define lockdep_assert_held_once(l)		do { (void)(l); } while (0)
 
 #define lockdep_recursing(tsk)			(0)
* Unmerged path kernel/lockdep.c
