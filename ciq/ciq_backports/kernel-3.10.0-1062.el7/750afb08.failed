cross-tree: phase out dma_zalloc_coherent()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Luis Chamberlain <mcgrof@kernel.org>
commit 750afb08ca71310fcf0c4e2cb1565c63b8235b60
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/750afb08.failed

We already need to zero out memory for dma_alloc_coherent(), as such
using dma_zalloc_coherent() is superflous. Phase it out.

This change was generated with the following Coccinelle SmPL patch:

@ replace_dma_zalloc_coherent @
expression dev, size, data, handle, flags;
@@

-dma_zalloc_coherent(dev, size, handle, flags)
+dma_alloc_coherent(dev, size, handle, flags)

	Suggested-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Luis Chamberlain <mcgrof@kernel.org>
[hch: re-ran the script on the latest tree]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 750afb08ca71310fcf0c4e2cb1565c63b8235b60)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/mips/lantiq/xway/dma.c
#	arch/powerpc/sysdev/fsl_rmu.c
#	drivers/atm/he.c
#	drivers/atm/idt77252.c
#	drivers/block/skd_main.c
#	drivers/crypto/cavium/cpt/cptpf_main.c
#	drivers/crypto/cavium/cpt/cptvf_main.c
#	drivers/crypto/cavium/nitrox/nitrox_lib.c
#	drivers/crypto/hisilicon/sec/sec_algs.c
#	drivers/crypto/hisilicon/sec/sec_drv.c
#	drivers/crypto/ixp4xx_crypto.c
#	drivers/crypto/mediatek/mtk-platform.c
#	drivers/dma/imx-sdma.c
#	drivers/dma/mediatek/mtk-hsdma.c
#	drivers/dma/mxs-dma.c
#	drivers/dma/xgene-dma.c
#	drivers/dma/xilinx/xilinx_dma.c
#	drivers/dma/xilinx/zynqmp_dma.c
#	drivers/gpu/drm/drm_pci.c
#	drivers/infiniband/hw/cxgb3/cxio_hal.c
#	drivers/infiniband/hw/cxgb4/qp.c
#	drivers/infiniband/hw/hns/hns_roce_alloc.c
#	drivers/infiniband/hw/hns/hns_roce_hw_v2.c
#	drivers/input/touchscreen/raspberrypi-ts.c
#	drivers/iommu/mtk_iommu_v1.c
#	drivers/media/pci/intel/ipu3/ipu3-cio2.c
#	drivers/media/platform/mtk-vcodec/mtk_vcodec_util.c
#	drivers/mmc/host/sdhci.c
#	drivers/net/ethernet/aeroflex/greth.c
#	drivers/net/ethernet/alacritech/slicoss.c
#	drivers/net/ethernet/apm/xgene-v2/main.c
#	drivers/net/ethernet/atheros/atl1c/atl1c_main.c
#	drivers/net/ethernet/broadcom/bcm63xx_enet.c
#	drivers/net/ethernet/broadcom/bcmsysport.c
#	drivers/net/ethernet/broadcom/bgmac.c
#	drivers/net/ethernet/broadcom/bnx2.c
#	drivers/net/ethernet/cavium/thunder/nicvf_queues.c
#	drivers/net/ethernet/faraday/ftgmac100.c
#	drivers/net/ethernet/faraday/ftmac100.c
#	drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
#	drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
#	drivers/net/ethernet/hisilicon/hns3/hns3pf/hclge_cmd.c
#	drivers/net/ethernet/hisilicon/hns3/hns3vf/hclgevf_cmd.c
#	drivers/net/ethernet/huawei/hinic/hinic_hw_api_cmd.c
#	drivers/net/ethernet/huawei/hinic/hinic_hw_eqs.c
#	drivers/net/ethernet/huawei/hinic/hinic_hw_io.c
#	drivers/net/ethernet/huawei/hinic/hinic_hw_qp.c
#	drivers/net/ethernet/huawei/hinic/hinic_hw_wq.c
#	drivers/net/ethernet/ibm/emac/mal.c
#	drivers/net/ethernet/intel/e1000/e1000_ethtool.c
#	drivers/net/ethernet/intel/ixgb/ixgb_main.c
#	drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
#	drivers/net/ethernet/marvell/pxa168_eth.c
#	drivers/net/ethernet/mediatek/mtk_eth_soc.c
#	drivers/net/ethernet/myricom/myri10ge/myri10ge.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
#	drivers/net/ethernet/ni/nixge.c
#	drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
#	drivers/net/ethernet/pasemi/pasemi_mac.c
#	drivers/net/ethernet/qualcomm/emac/emac-mac.c
#	drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
#	drivers/net/ethernet/sgi/meth.c
#	drivers/net/ethernet/socionext/netsec.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
#	drivers/net/ethernet/tundra/tsi108_eth.c
#	drivers/net/ethernet/xilinx/ll_temac_main.c
#	drivers/net/ethernet/xilinx/xilinx_axienet_main.c
#	drivers/net/fddi/defxx.c
#	drivers/net/fddi/skfp/skfddi.c
#	drivers/net/wan/fsl_ucc_hdlc.c
#	drivers/net/wireless/ath/wcn36xx/dxe.c
#	drivers/net/wireless/ath/wil6210/txrx_edma.c
#	drivers/net/wireless/b43/dma.c
#	drivers/net/wireless/b43legacy/dma.c
#	drivers/net/wireless/broadcom/brcm80211/brcmfmac/pcie.c
#	drivers/net/wireless/intel/iwlwifi/pcie/rx.c
#	drivers/ntb/hw/mscc/ntb_hw_switchtec.c
#	drivers/nvme/host/pci.c
#	drivers/pci/controller/pcie-iproc-msi.c
#	drivers/pci/switch/switchtec.c
#	drivers/rapidio/devices/tsi721_dma.c
#	drivers/s390/net/ism_drv.c
#	drivers/scsi/3w-sas.c
#	drivers/scsi/a100u2w.c
#	drivers/scsi/arcmsr/arcmsr_hba.c
#	drivers/scsi/be2iscsi/be_main.c
#	drivers/scsi/be2iscsi/be_mgmt.c
#	drivers/scsi/bfa/bfad_bsg.c
#	drivers/scsi/bnx2fc/bnx2fc_hwi.c
#	drivers/scsi/bnx2fc/bnx2fc_tgt.c
#	drivers/scsi/csiostor/csio_wr.c
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/megaraid/megaraid_mbox.c
#	drivers/scsi/mesh.c
#	drivers/scsi/mvumi.c
#	drivers/scsi/pm8001/pm8001_sas.c
#	drivers/scsi/qedi/qedi_main.c
#	drivers/scsi/qla2xxx/qla_init.c
#	drivers/soc/fsl/qbman/dpaa_sys.c
#	drivers/spi/spi-pic32-sqi.c
#	drivers/staging/mt7621-eth/mtk_eth_soc.c
#	drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c
#	drivers/staging/vt6655/device_main.c
#	drivers/usb/gadget/udc/bdc/bdc_core.c
#	drivers/video/da8xx-fb.c
#	include/linux/pci-dma-compat.h
#	sound/aoa/soundbus/i2sbus/core.c
#	sound/sparc/dbri.c
diff --cc arch/mips/lantiq/xway/dma.c
index 08f7ebd9c774,5e6a1a45cbd2..000000000000
--- a/arch/mips/lantiq/xway/dma.c
+++ b/arch/mips/lantiq/xway/dma.c
@@@ -128,12 -129,11 +128,18 @@@ ltq_dma_alloc(struct ltq_dma_channel *c
  	unsigned long flags;
  
  	ch->desc = 0;
++<<<<<<< HEAD
 +	ch->desc_base = dma_alloc_coherent(NULL,
 +				LTQ_DESC_NUM * LTQ_DESC_SIZE,
 +				&ch->phys, GFP_ATOMIC);
 +	memset(ch->desc_base, 0, LTQ_DESC_NUM * LTQ_DESC_SIZE);
++=======
+ 	ch->desc_base = dma_alloc_coherent(ch->dev,
+ 					   LTQ_DESC_NUM * LTQ_DESC_SIZE,
+ 					   &ch->phys, GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
 -	spin_lock_irqsave(&ltq_dma_lock, flags);
 +	local_irq_save(flags);
  	ltq_dma_w32(ch->nr, LTQ_DMA_CS);
  	ltq_dma_w32(ch->phys, LTQ_DMA_CDBA);
  	ltq_dma_w32(LTQ_DESC_NUM, LTQ_DMA_CDLEN);
diff --cc arch/powerpc/sysdev/fsl_rmu.c
index 00e224a1048c,ebed46f80254..000000000000
--- a/arch/powerpc/sysdev/fsl_rmu.c
+++ b/arch/powerpc/sysdev/fsl_rmu.c
@@@ -745,8 -757,9 +745,14 @@@ fsl_open_outb_mbox(struct rio_mport *mp
  
  	/* Initialize outbound message descriptor ring */
  	rmu->msg_tx_ring.virt = dma_alloc_coherent(priv->dev,
++<<<<<<< HEAD
 +				rmu->msg_tx_ring.size * RIO_MSG_DESC_SIZE,
 +				&rmu->msg_tx_ring.phys, GFP_KERNEL);
++=======
+ 						   rmu->msg_tx_ring.size * RIO_MSG_DESC_SIZE,
+ 						   &rmu->msg_tx_ring.phys,
+ 						   GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!rmu->msg_tx_ring.virt) {
  		rc = -ENOMEM;
  		goto out_dma;
diff --cc drivers/atm/he.c
index 507362a76a73,2e9d1cfe3aeb..000000000000
--- a/drivers/atm/he.c
+++ b/drivers/atm/he.c
@@@ -534,8 -533,10 +534,15 @@@ static void he_init_tx_lbfp(struct he_d
  
  static int he_init_tpdrq(struct he_dev *he_dev)
  {
++<<<<<<< HEAD
 +	he_dev->tpdrq_base = pci_alloc_consistent(he_dev->pci_dev,
 +		CONFIG_TPDRQ_SIZE * sizeof(struct he_tpdrq), &he_dev->tpdrq_phys);
++=======
+ 	he_dev->tpdrq_base = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 						CONFIG_TPDRQ_SIZE * sizeof(struct he_tpdrq),
+ 						&he_dev->tpdrq_phys,
+ 						GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->tpdrq_base == NULL) {
  		hprintk("failed to alloc tpdrq\n");
  		return -ENOMEM;
@@@ -805,8 -806,9 +812,14 @@@ static int he_init_group(struct he_dev 
  		goto out_free_rbpl_virt;
  	}
  
++<<<<<<< HEAD
 +	he_dev->rbpl_base = pci_alloc_consistent(he_dev->pci_dev,
 +		CONFIG_RBPL_SIZE * sizeof(struct he_rbp), &he_dev->rbpl_phys);
++=======
+ 	he_dev->rbpl_base = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 					       CONFIG_RBPL_SIZE * sizeof(struct he_rbp),
+ 					       &he_dev->rbpl_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->rbpl_base == NULL) {
  		hprintk("failed to alloc rbpl_base\n");
  		goto out_destroy_rbpl_pool;
@@@ -844,8 -845,9 +857,14 @@@
  
  	/* rx buffer ready queue */
  
++<<<<<<< HEAD
 +	he_dev->rbrq_base = pci_alloc_consistent(he_dev->pci_dev,
 +		CONFIG_RBRQ_SIZE * sizeof(struct he_rbrq), &he_dev->rbrq_phys);
++=======
+ 	he_dev->rbrq_base = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 					       CONFIG_RBRQ_SIZE * sizeof(struct he_rbrq),
+ 					       &he_dev->rbrq_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->rbrq_base == NULL) {
  		hprintk("failed to allocate rbrq\n");
  		goto out_free_rbpl;
@@@ -868,8 -869,9 +887,14 @@@
  
  	/* tx buffer ready queue */
  
++<<<<<<< HEAD
 +	he_dev->tbrq_base = pci_alloc_consistent(he_dev->pci_dev,
 +		CONFIG_TBRQ_SIZE * sizeof(struct he_tbrq), &he_dev->tbrq_phys);
++=======
+ 	he_dev->tbrq_base = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 					       CONFIG_TBRQ_SIZE * sizeof(struct he_tbrq),
+ 					       &he_dev->tbrq_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->tbrq_base == NULL) {
  		hprintk("failed to allocate tbrq\n");
  		goto out_free_rbpq_base;
@@@ -913,8 -914,9 +938,14 @@@ static int he_init_irq(struct he_dev *h
  	/* 2.9.3.5  tail offset for each interrupt queue is located after the
  		    end of the interrupt queue */
  
++<<<<<<< HEAD
 +	he_dev->irq_base = pci_alloc_consistent(he_dev->pci_dev,
 +			(CONFIG_IRQ_SIZE+1) * sizeof(struct he_irq), &he_dev->irq_phys);
++=======
+ 	he_dev->irq_base = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 					      (CONFIG_IRQ_SIZE + 1) * sizeof(struct he_irq),
+ 					      &he_dev->irq_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->irq_base == NULL) {
  		hprintk("failed to allocate irq\n");
  		return -ENOMEM;
@@@ -1468,8 -1463,9 +1499,14 @@@ static int he_start(struct atm_dev *dev
  
  	/* host status page */
  
++<<<<<<< HEAD
 +	he_dev->hsp = pci_alloc_consistent(he_dev->pci_dev,
 +				sizeof(struct he_hsp), &he_dev->hsp_phys);
++=======
+ 	he_dev->hsp = dma_alloc_coherent(&he_dev->pci_dev->dev,
+ 					 sizeof(struct he_hsp),
+ 					 &he_dev->hsp_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (he_dev->hsp == NULL) {
  		hprintk("failed to allocate host status page\n");
  		return -ENOMEM;
diff --cc drivers/atm/idt77252.c
index 1bdf104e90bb,43a14579e80e..000000000000
--- a/drivers/atm/idt77252.c
+++ b/drivers/atm/idt77252.c
@@@ -641,8 -641,8 +641,13 @@@ alloc_scq(struct idt77252_dev *card, in
  	scq = kzalloc(sizeof(struct scq_info), GFP_KERNEL);
  	if (!scq)
  		return NULL;
++<<<<<<< HEAD
 +	scq->base = pci_alloc_consistent(card->pcidev, SCQ_SIZE,
 +					 &scq->paddr);
++=======
+ 	scq->base = dma_alloc_coherent(&card->pcidev->dev, SCQ_SIZE,
+ 				       &scq->paddr, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (scq->base == NULL) {
  		kfree(scq);
  		return NULL;
@@@ -972,8 -971,8 +977,13 @@@ init_rsq(struct idt77252_dev *card
  {
  	struct rsq_entry *rsqe;
  
++<<<<<<< HEAD
 +	card->rsq.base = pci_alloc_consistent(card->pcidev, RSQSIZE,
 +					      &card->rsq.paddr);
++=======
+ 	card->rsq.base = dma_alloc_coherent(&card->pcidev->dev, RSQSIZE,
+ 					    &card->rsq.paddr, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (card->rsq.base == NULL) {
  		printk("%s: can't allocate RSQ.\n", card->name);
  		return -1;
@@@ -3400,8 -3390,10 +3410,15 @@@ static int init_card(struct atm_dev *de
  	writel(0, SAR_REG_GP);
  
  	/* Initialize RAW Cell Handle Register  */
++<<<<<<< HEAD
 +	card->raw_cell_hnd = pci_alloc_consistent(card->pcidev, 2 * sizeof(u32),
 +						  &card->raw_cell_paddr);
++=======
+ 	card->raw_cell_hnd = dma_alloc_coherent(&card->pcidev->dev,
+ 						2 * sizeof(u32),
+ 						&card->raw_cell_paddr,
+ 						GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!card->raw_cell_hnd) {
  		printk("%s: memory allocation failure.\n", card->name);
  		deinit_card(card);
diff --cc drivers/crypto/ixp4xx_crypto.c
index 21180d6cad6e,1b0d156bb9be..000000000000
--- a/drivers/crypto/ixp4xx_crypto.c
+++ b/drivers/crypto/ixp4xx_crypto.c
@@@ -263,13 -258,13 +263,18 @@@ static inline const struct ix_hash_alg
  
  static int setup_crypt_desc(void)
  {
 -	struct device *dev = &pdev->dev;
  	BUILD_BUG_ON(sizeof(struct crypt_ctl) != 64);
  	crypt_virt = dma_alloc_coherent(dev,
++<<<<<<< HEAD
 +			NPE_QLEN * sizeof(struct crypt_ctl),
 +			&crypt_phys, GFP_ATOMIC);
++=======
+ 					NPE_QLEN * sizeof(struct crypt_ctl),
+ 					&crypt_phys, GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!crypt_virt)
  		return -ENOMEM;
 +	memset(crypt_virt, 0, NPE_QLEN * sizeof(struct crypt_ctl));
  	return 0;
  }
  
diff --cc drivers/dma/imx-sdma.c
index 092867bf795c,86708fb9bda1..000000000000
--- a/drivers/dma/imx-sdma.c
+++ b/drivers/dma/imx-sdma.c
@@@ -793,14 -1178,13 +793,20 @@@ static int sdma_set_channel_priority(st
  	return 0;
  }
  
 -static int sdma_request_channel0(struct sdma_engine *sdma)
 +static int sdma_request_channel(struct sdma_channel *sdmac)
  {
 +	struct sdma_engine *sdma = sdmac->sdma;
 +	int channel = sdmac->channel;
  	int ret = -EBUSY;
  
++<<<<<<< HEAD
 +	sdmac->bd = dma_alloc_coherent(NULL, PAGE_SIZE, &sdmac->bd_phys, GFP_KERNEL);
 +	if (!sdmac->bd) {
++=======
+ 	sdma->bd0 = dma_alloc_coherent(NULL, PAGE_SIZE, &sdma->bd0_phys,
+ 				       GFP_NOWAIT);
+ 	if (!sdma->bd0) {
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		ret = -ENOMEM;
  		goto out;
  	}
@@@ -820,24 -1199,35 +826,38 @@@ out
  	return ret;
  }
  
 -
 -static int sdma_alloc_bd(struct sdma_desc *desc)
 +static struct sdma_channel *to_sdma_chan(struct dma_chan *chan)
  {
++<<<<<<< HEAD
 +	return container_of(chan, struct sdma_channel, chan);
++=======
+ 	u32 bd_size = desc->num_bd * sizeof(struct sdma_buffer_descriptor);
+ 	int ret = 0;
+ 
+ 	desc->bd = dma_alloc_coherent(NULL, bd_size, &desc->bd_phys,
+ 				      GFP_NOWAIT);
+ 	if (!desc->bd) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ out:
+ 	return ret;
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  }
  
 -static void sdma_free_bd(struct sdma_desc *desc)
 +static dma_cookie_t sdma_tx_submit(struct dma_async_tx_descriptor *tx)
  {
 -	u32 bd_size = desc->num_bd * sizeof(struct sdma_buffer_descriptor);
 +	unsigned long flags;
 +	struct sdma_channel *sdmac = to_sdma_chan(tx->chan);
 +	dma_cookie_t cookie;
  
 -	dma_free_coherent(NULL, bd_size, desc->bd, desc->bd_phys);
 -}
 +	spin_lock_irqsave(&sdmac->lock, flags);
  
 -static void sdma_desc_free(struct virt_dma_desc *vd)
 -{
 -	struct sdma_desc *desc = container_of(vd, struct sdma_desc, vd);
 +	cookie = dma_cookie_assign(tx);
  
 -	sdma_free_bd(desc);
 -	kfree(desc);
 +	spin_unlock_irqrestore(&sdmac->lock, flags);
 +
 +	return cookie;
  }
  
  static int sdma_alloc_chan_resources(struct dma_chan *chan)
diff --cc drivers/dma/mxs-dma.c
index b48a79c28845,22cc7f68ef6e..000000000000
--- a/drivers/dma/mxs-dma.c
+++ b/drivers/dma/mxs-dma.c
@@@ -353,12 -416,9 +353,18 @@@ static int mxs_dma_alloc_chan_resources
  	struct mxs_dma_engine *mxs_dma = mxs_chan->mxs_dma;
  	int ret;
  
++<<<<<<< HEAD
 +	if (data)
 +		mxs_chan->chan_irq = data->chan_irq;
 +
 +	mxs_chan->ccw = dma_alloc_coherent(mxs_dma->dma_device.dev,
 +				CCW_BLOCK_SIZE, &mxs_chan->ccw_phys,
 +				GFP_KERNEL);
++=======
+ 	mxs_chan->ccw = dma_alloc_coherent(mxs_dma->dma_device.dev,
+ 					   CCW_BLOCK_SIZE,
+ 					   &mxs_chan->ccw_phys, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!mxs_chan->ccw) {
  		ret = -ENOMEM;
  		goto err_alloc;
diff --cc drivers/gpu/drm/drm_pci.c
index 4db9c515b74f,693748ad8b88..000000000000
--- a/drivers/gpu/drm/drm_pci.c
+++ b/drivers/gpu/drm/drm_pci.c
@@@ -61,7 -61,9 +61,13 @@@ drm_dma_handle_t *drm_pci_alloc(struct 
  		return NULL;
  
  	dmah->size = size;
++<<<<<<< HEAD
 +	dmah->vaddr = dma_alloc_coherent(&dev->pdev->dev, size, &dmah->busaddr, GFP_KERNEL | __GFP_COMP);
++=======
+ 	dmah->vaddr = dma_alloc_coherent(&dev->pdev->dev, size,
+ 					 &dmah->busaddr,
+ 					 GFP_KERNEL | __GFP_COMP);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
  	if (dmah->vaddr == NULL) {
  		kfree(dmah);
diff --cc drivers/infiniband/hw/cxgb3/cxio_hal.c
index b4e840bf7d03,8ac72ac7cbac..000000000000
--- a/drivers/infiniband/hw/cxgb3/cxio_hal.c
+++ b/drivers/infiniband/hw/cxgb3/cxio_hal.c
@@@ -293,8 -292,8 +293,13 @@@ int cxio_create_qp(struct cxio_rdev *rd
  		goto err3;
  
  	wq->queue = dma_alloc_coherent(&(rdev_p->rnic_info.pdev->dev),
++<<<<<<< HEAD
 +					     depth * sizeof(union t3_wr),
 +					     &(wq->dma_addr), GFP_KERNEL);
++=======
+ 				       depth * sizeof(union t3_wr),
+ 				       &(wq->dma_addr), GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!wq->queue)
  		goto err4;
  
diff --cc drivers/infiniband/hw/cxgb4/qp.c
index 4106eed1b8fb,504cf525508f..000000000000
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@@ -2104,3 -2490,356 +2104,359 @@@ int c4iw_ib_query_qp(struct ib_qp *ibqp
  	init_attr->sq_sig_type = qhp->sq_sig_all ? IB_SIGNAL_ALL_WR : 0;
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void free_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,
+ 			   struct c4iw_wr_wait *wr_waitp)
+ {
+ 	struct c4iw_rdev *rdev = &srq->rhp->rdev;
+ 	struct sk_buff *skb = srq->destroy_skb;
+ 	struct t4_srq *wq = &srq->wq;
+ 	struct fw_ri_res_wr *res_wr;
+ 	struct fw_ri_res *res;
+ 	int wr_len;
+ 
+ 	wr_len = sizeof(*res_wr) + sizeof(*res);
+ 	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+ 
+ 	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
+ 	memset(res_wr, 0, wr_len);
+ 	res_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |
+ 			FW_RI_RES_WR_NRES_V(1) |
+ 			FW_WR_COMPL_F);
+ 	res_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));
+ 	res_wr->cookie = (uintptr_t)wr_waitp;
+ 	res = res_wr->res;
+ 	res->u.srq.restype = FW_RI_RES_TYPE_SRQ;
+ 	res->u.srq.op = FW_RI_RES_OP_RESET;
+ 	res->u.srq.srqid = cpu_to_be32(srq->idx);
+ 	res->u.srq.eqid = cpu_to_be32(wq->qid);
+ 
+ 	c4iw_init_wr_wait(wr_waitp);
+ 	c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);
+ 
+ 	dma_free_coherent(&rdev->lldi.pdev->dev,
+ 			  wq->memsize, wq->queue,
+ 			dma_unmap_addr(wq, mapping));
+ 	c4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);
+ 	kfree(wq->sw_rq);
+ 	c4iw_put_qpid(rdev, wq->qid, uctx);
+ }
+ 
+ static int alloc_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,
+ 			   struct c4iw_wr_wait *wr_waitp)
+ {
+ 	struct c4iw_rdev *rdev = &srq->rhp->rdev;
+ 	int user = (uctx != &rdev->uctx);
+ 	struct t4_srq *wq = &srq->wq;
+ 	struct fw_ri_res_wr *res_wr;
+ 	struct fw_ri_res *res;
+ 	struct sk_buff *skb;
+ 	int wr_len;
+ 	int eqsize;
+ 	int ret = -ENOMEM;
+ 
+ 	wq->qid = c4iw_get_qpid(rdev, uctx);
+ 	if (!wq->qid)
+ 		goto err;
+ 
+ 	if (!user) {
+ 		wq->sw_rq = kcalloc(wq->size, sizeof(*wq->sw_rq),
+ 				    GFP_KERNEL);
+ 		if (!wq->sw_rq)
+ 			goto err_put_qpid;
+ 		wq->pending_wrs = kcalloc(srq->wq.size,
+ 					  sizeof(*srq->wq.pending_wrs),
+ 					  GFP_KERNEL);
+ 		if (!wq->pending_wrs)
+ 			goto err_free_sw_rq;
+ 	}
+ 
+ 	wq->rqt_size = wq->size;
+ 	wq->rqt_hwaddr = c4iw_rqtpool_alloc(rdev, wq->rqt_size);
+ 	if (!wq->rqt_hwaddr)
+ 		goto err_free_pending_wrs;
+ 	wq->rqt_abs_idx = (wq->rqt_hwaddr - rdev->lldi.vr->rq.start) >>
+ 		T4_RQT_ENTRY_SHIFT;
+ 
+ 	wq->queue = dma_alloc_coherent(&rdev->lldi.pdev->dev, wq->memsize,
+ 				       &wq->dma_addr, GFP_KERNEL);
+ 	if (!wq->queue)
+ 		goto err_free_rqtpool;
+ 
+ 	dma_unmap_addr_set(wq, mapping, wq->dma_addr);
+ 
+ 	wq->bar2_va = c4iw_bar2_addrs(rdev, wq->qid, CXGB4_BAR2_QTYPE_EGRESS,
+ 				      &wq->bar2_qid,
+ 			user ? &wq->bar2_pa : NULL);
+ 
+ 	/*
+ 	 * User mode must have bar2 access.
+ 	 */
+ 
+ 	if (user && !wq->bar2_va) {
+ 		pr_warn(MOD "%s: srqid %u not in BAR2 range.\n",
+ 			pci_name(rdev->lldi.pdev), wq->qid);
+ 		ret = -EINVAL;
+ 		goto err_free_queue;
+ 	}
+ 
+ 	/* build fw_ri_res_wr */
+ 	wr_len = sizeof(*res_wr) + sizeof(*res);
+ 
+ 	skb = alloc_skb(wr_len, GFP_KERNEL | __GFP_NOFAIL);
+ 	if (!skb)
+ 		goto err_free_queue;
+ 	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+ 
+ 	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
+ 	memset(res_wr, 0, wr_len);
+ 	res_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |
+ 			FW_RI_RES_WR_NRES_V(1) |
+ 			FW_WR_COMPL_F);
+ 	res_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));
+ 	res_wr->cookie = (uintptr_t)wr_waitp;
+ 	res = res_wr->res;
+ 	res->u.srq.restype = FW_RI_RES_TYPE_SRQ;
+ 	res->u.srq.op = FW_RI_RES_OP_WRITE;
+ 
+ 	/*
+ 	 * eqsize is the number of 64B entries plus the status page size.
+ 	 */
+ 	eqsize = wq->size * T4_RQ_NUM_SLOTS +
+ 		rdev->hw_queue.t4_eq_status_entries;
+ 	res->u.srq.eqid = cpu_to_be32(wq->qid);
+ 	res->u.srq.fetchszm_to_iqid =
+ 						/* no host cidx updates */
+ 		cpu_to_be32(FW_RI_RES_WR_HOSTFCMODE_V(0) |
+ 		FW_RI_RES_WR_CPRIO_V(0) |       /* don't keep in chip cache */
+ 		FW_RI_RES_WR_PCIECHN_V(0) |     /* set by uP at ri_init time */
+ 		FW_RI_RES_WR_FETCHRO_V(0));     /* relaxed_ordering */
+ 	res->u.srq.dcaen_to_eqsize =
+ 		cpu_to_be32(FW_RI_RES_WR_DCAEN_V(0) |
+ 		FW_RI_RES_WR_DCACPU_V(0) |
+ 		FW_RI_RES_WR_FBMIN_V(2) |
+ 		FW_RI_RES_WR_FBMAX_V(3) |
+ 		FW_RI_RES_WR_CIDXFTHRESHO_V(0) |
+ 		FW_RI_RES_WR_CIDXFTHRESH_V(0) |
+ 		FW_RI_RES_WR_EQSIZE_V(eqsize));
+ 	res->u.srq.eqaddr = cpu_to_be64(wq->dma_addr);
+ 	res->u.srq.srqid = cpu_to_be32(srq->idx);
+ 	res->u.srq.pdid = cpu_to_be32(srq->pdid);
+ 	res->u.srq.hwsrqsize = cpu_to_be32(wq->rqt_size);
+ 	res->u.srq.hwsrqaddr = cpu_to_be32(wq->rqt_hwaddr -
+ 			rdev->lldi.vr->rq.start);
+ 
+ 	c4iw_init_wr_wait(wr_waitp);
+ 
+ 	ret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, wq->qid, __func__);
+ 	if (ret)
+ 		goto err_free_queue;
+ 
+ 	pr_debug("%s srq %u eqid %u pdid %u queue va %p pa 0x%llx\n"
+ 			" bar2_addr %p rqt addr 0x%x size %d\n",
+ 			__func__, srq->idx, wq->qid, srq->pdid, wq->queue,
+ 			(u64)virt_to_phys(wq->queue), wq->bar2_va,
+ 			wq->rqt_hwaddr, wq->rqt_size);
+ 
+ 	return 0;
+ err_free_queue:
+ 	dma_free_coherent(&rdev->lldi.pdev->dev,
+ 			  wq->memsize, wq->queue,
+ 			dma_unmap_addr(wq, mapping));
+ err_free_rqtpool:
+ 	c4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);
+ err_free_pending_wrs:
+ 	if (!user)
+ 		kfree(wq->pending_wrs);
+ err_free_sw_rq:
+ 	if (!user)
+ 		kfree(wq->sw_rq);
+ err_put_qpid:
+ 	c4iw_put_qpid(rdev, wq->qid, uctx);
+ err:
+ 	return ret;
+ }
+ 
+ void c4iw_copy_wr_to_srq(struct t4_srq *srq, union t4_recv_wr *wqe, u8 len16)
+ {
+ 	u64 *src, *dst;
+ 
+ 	src = (u64 *)wqe;
+ 	dst = (u64 *)((u8 *)srq->queue + srq->wq_pidx * T4_EQ_ENTRY_SIZE);
+ 	while (len16) {
+ 		*dst++ = *src++;
+ 		if (dst >= (u64 *)&srq->queue[srq->size])
+ 			dst = (u64 *)srq->queue;
+ 		*dst++ = *src++;
+ 		if (dst >= (u64 *)&srq->queue[srq->size])
+ 			dst = (u64 *)srq->queue;
+ 		len16--;
+ 	}
+ }
+ 
+ struct ib_srq *c4iw_create_srq(struct ib_pd *pd, struct ib_srq_init_attr *attrs,
+ 			       struct ib_udata *udata)
+ {
+ 	struct c4iw_dev *rhp;
+ 	struct c4iw_srq *srq;
+ 	struct c4iw_pd *php;
+ 	struct c4iw_create_srq_resp uresp;
+ 	struct c4iw_ucontext *ucontext;
+ 	struct c4iw_mm_entry *srq_key_mm, *srq_db_key_mm;
+ 	int rqsize;
+ 	int ret;
+ 	int wr_len;
+ 
+ 	pr_debug("%s ib_pd %p\n", __func__, pd);
+ 
+ 	php = to_c4iw_pd(pd);
+ 	rhp = php->rhp;
+ 
+ 	if (!rhp->rdev.lldi.vr->srq.size)
+ 		return ERR_PTR(-EINVAL);
+ 	if (attrs->attr.max_wr > rhp->rdev.hw_queue.t4_max_rq_size)
+ 		return ERR_PTR(-E2BIG);
+ 	if (attrs->attr.max_sge > T4_MAX_RECV_SGE)
+ 		return ERR_PTR(-E2BIG);
+ 
+ 	/*
+ 	 * SRQ RQT and RQ must be a power of 2 and at least 16 deep.
+ 	 */
+ 	rqsize = attrs->attr.max_wr + 1;
+ 	rqsize = roundup_pow_of_two(max_t(u16, rqsize, 16));
+ 
+ 	ucontext = udata ? to_c4iw_ucontext(pd->uobject->context) : NULL;
+ 
+ 	srq = kzalloc(sizeof(*srq), GFP_KERNEL);
+ 	if (!srq)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	srq->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);
+ 	if (!srq->wr_waitp) {
+ 		ret = -ENOMEM;
+ 		goto err_free_srq;
+ 	}
+ 
+ 	srq->idx = c4iw_alloc_srq_idx(&rhp->rdev);
+ 	if (srq->idx < 0) {
+ 		ret = -ENOMEM;
+ 		goto err_free_wr_wait;
+ 	}
+ 
+ 	wr_len = sizeof(struct fw_ri_res_wr) + sizeof(struct fw_ri_res);
+ 	srq->destroy_skb = alloc_skb(wr_len, GFP_KERNEL);
+ 	if (!srq->destroy_skb) {
+ 		ret = -ENOMEM;
+ 		goto err_free_srq_idx;
+ 	}
+ 
+ 	srq->rhp = rhp;
+ 	srq->pdid = php->pdid;
+ 
+ 	srq->wq.size = rqsize;
+ 	srq->wq.memsize =
+ 		(rqsize + rhp->rdev.hw_queue.t4_eq_status_entries) *
+ 		sizeof(*srq->wq.queue);
+ 	if (ucontext)
+ 		srq->wq.memsize = roundup(srq->wq.memsize, PAGE_SIZE);
+ 
+ 	ret = alloc_srq_queue(srq, ucontext ? &ucontext->uctx :
+ 			&rhp->rdev.uctx, srq->wr_waitp);
+ 	if (ret)
+ 		goto err_free_skb;
+ 	attrs->attr.max_wr = rqsize - 1;
+ 
+ 	if (CHELSIO_CHIP_VERSION(rhp->rdev.lldi.adapter_type) > CHELSIO_T6)
+ 		srq->flags = T4_SRQ_LIMIT_SUPPORT;
+ 
+ 	ret = insert_handle(rhp, &rhp->qpidr, srq, srq->wq.qid);
+ 	if (ret)
+ 		goto err_free_queue;
+ 
+ 	if (udata) {
+ 		srq_key_mm = kmalloc(sizeof(*srq_key_mm), GFP_KERNEL);
+ 		if (!srq_key_mm) {
+ 			ret = -ENOMEM;
+ 			goto err_remove_handle;
+ 		}
+ 		srq_db_key_mm = kmalloc(sizeof(*srq_db_key_mm), GFP_KERNEL);
+ 		if (!srq_db_key_mm) {
+ 			ret = -ENOMEM;
+ 			goto err_free_srq_key_mm;
+ 		}
+ 		memset(&uresp, 0, sizeof(uresp));
+ 		uresp.flags = srq->flags;
+ 		uresp.qid_mask = rhp->rdev.qpmask;
+ 		uresp.srqid = srq->wq.qid;
+ 		uresp.srq_size = srq->wq.size;
+ 		uresp.srq_memsize = srq->wq.memsize;
+ 		uresp.rqt_abs_idx = srq->wq.rqt_abs_idx;
+ 		spin_lock(&ucontext->mmap_lock);
+ 		uresp.srq_key = ucontext->key;
+ 		ucontext->key += PAGE_SIZE;
+ 		uresp.srq_db_gts_key = ucontext->key;
+ 		ucontext->key += PAGE_SIZE;
+ 		spin_unlock(&ucontext->mmap_lock);
+ 		ret = ib_copy_to_udata(udata, &uresp, sizeof(uresp));
+ 		if (ret)
+ 			goto err_free_srq_db_key_mm;
+ 		srq_key_mm->key = uresp.srq_key;
+ 		srq_key_mm->addr = virt_to_phys(srq->wq.queue);
+ 		srq_key_mm->len = PAGE_ALIGN(srq->wq.memsize);
+ 		insert_mmap(ucontext, srq_key_mm);
+ 		srq_db_key_mm->key = uresp.srq_db_gts_key;
+ 		srq_db_key_mm->addr = (u64)(unsigned long)srq->wq.bar2_pa;
+ 		srq_db_key_mm->len = PAGE_SIZE;
+ 		insert_mmap(ucontext, srq_db_key_mm);
+ 	}
+ 
+ 	pr_debug("%s srq qid %u idx %u size %u memsize %lu num_entries %u\n",
+ 		 __func__, srq->wq.qid, srq->idx, srq->wq.size,
+ 			(unsigned long)srq->wq.memsize, attrs->attr.max_wr);
+ 
+ 	spin_lock_init(&srq->lock);
+ 	return &srq->ibsrq;
+ err_free_srq_db_key_mm:
+ 	kfree(srq_db_key_mm);
+ err_free_srq_key_mm:
+ 	kfree(srq_key_mm);
+ err_remove_handle:
+ 	remove_handle(rhp, &rhp->qpidr, srq->wq.qid);
+ err_free_queue:
+ 	free_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,
+ 		       srq->wr_waitp);
+ err_free_skb:
+ 	kfree_skb(srq->destroy_skb);
+ err_free_srq_idx:
+ 	c4iw_free_srq_idx(&rhp->rdev, srq->idx);
+ err_free_wr_wait:
+ 	c4iw_put_wr_wait(srq->wr_waitp);
+ err_free_srq:
+ 	kfree(srq);
+ 	return ERR_PTR(ret);
+ }
+ 
+ int c4iw_destroy_srq(struct ib_srq *ibsrq)
+ {
+ 	struct c4iw_dev *rhp;
+ 	struct c4iw_srq *srq;
+ 	struct c4iw_ucontext *ucontext;
+ 
+ 	srq = to_c4iw_srq(ibsrq);
+ 	rhp = srq->rhp;
+ 
+ 	pr_debug("%s id %d\n", __func__, srq->wq.qid);
+ 
+ 	remove_handle(rhp, &rhp->qpidr, srq->wq.qid);
+ 	ucontext = ibsrq->uobject ?
+ 		to_c4iw_ucontext(ibsrq->uobject->context) : NULL;
+ 	free_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,
+ 		       srq->wr_waitp);
+ 	c4iw_free_srq_idx(&rhp->rdev, srq->idx);
+ 	c4iw_put_wr_wait(srq->wr_waitp);
+ 	kfree(srq);
+ 	return 0;
+ }
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
diff --cc drivers/mmc/host/sdhci.c
index 6a2d4a908043,eba9bcc92ad3..000000000000
--- a/drivers/mmc/host/sdhci.c
+++ b/drivers/mmc/host/sdhci.c
@@@ -3347,8 -3759,13 +3347,18 @@@ int sdhci_setup_host(struct sdhci_host 
  		}
  
  		host->align_buffer_sz = SDHCI_MAX_SEGS * SDHCI_ADMA2_ALIGN;
++<<<<<<< HEAD
 +		buf = dma_alloc_coherent(mmc_dev(mmc), host->align_buffer_sz +
 +					 host->adma_table_sz, &dma, GFP_KERNEL);
++=======
+ 		/*
+ 		 * Use zalloc to zero the reserved high 32-bits of 128-bit
+ 		 * descriptors so that they never need to be written.
+ 		 */
+ 		buf = dma_alloc_coherent(mmc_dev(mmc),
+ 					 host->align_buffer_sz + host->adma_table_sz,
+ 					 &dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!buf) {
  			pr_warn("%s: Unable to allocate ADMA buffers - falling back to standard DMA\n",
  				mmc_hostname(mmc));
diff --cc drivers/net/ethernet/aeroflex/greth.c
index 48dd6ff189ba,47e5984f16fb..000000000000
--- a/drivers/net/ethernet/aeroflex/greth.c
+++ b/drivers/net/ethernet/aeroflex/greth.c
@@@ -1455,7 -1435,7 +1455,11 @@@ static int greth_of_probe(struct platfo
  	/* Allocate TX descriptor ring in coherent memory */
  	greth->tx_bd_base = dma_alloc_coherent(greth->dev, 1024,
  					       &greth->tx_bd_base_phys,
++<<<<<<< HEAD
 +					       GFP_KERNEL | __GFP_ZERO);
++=======
+ 					       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!greth->tx_bd_base) {
  		err = -ENOMEM;
  		goto error3;
@@@ -1464,7 -1444,7 +1468,11 @@@
  	/* Allocate RX descriptor ring in coherent memory */
  	greth->rx_bd_base = dma_alloc_coherent(greth->dev, 1024,
  					       &greth->rx_bd_base_phys,
++<<<<<<< HEAD
 +					       GFP_KERNEL | __GFP_ZERO);
++=======
+ 					       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!greth->rx_bd_base) {
  		err = -ENOMEM;
  		goto error4;
diff --cc drivers/net/ethernet/atheros/atl1c/atl1c_main.c
index d65ccfd57624,3a3b35b5df67..000000000000
--- a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+++ b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
@@@ -1016,13 -1019,12 +1016,18 @@@ static int atl1c_setup_ring_resources(s
  		sizeof(struct atl1c_recv_ret_status) * rx_desc_count +
  		8 * 4;
  
++<<<<<<< HEAD
 +	ring_header->desc = pci_alloc_consistent(pdev, ring_header->size,
 +				&ring_header->dma);
++=======
+ 	ring_header->desc = dma_alloc_coherent(&pdev->dev, ring_header->size,
+ 					       &ring_header->dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (unlikely(!ring_header->desc)) {
 -		dev_err(&pdev->dev, "could not get memory for DMA buffer\n");
 +		dev_err(&pdev->dev, "pci_alloc_consistend failed\n");
  		goto err_nomem;
  	}
 +	memset(ring_header->desc, 0, ring_header->size);
  	/* init TPD ring */
  
  	tpd_ring[0].dma = roundup(ring_header->dma, 8);
diff --cc drivers/net/ethernet/broadcom/bcm63xx_enet.c
index 1f7233bbb0e3,09cd188826b1..000000000000
--- a/drivers/net/ethernet/broadcom/bcm63xx_enet.c
+++ b/drivers/net/ethernet/broadcom/bcm63xx_enet.c
@@@ -862,8 -936,7 +862,12 @@@ static int bcm_enet_open(struct net_dev
  
  	/* allocate rx dma ring */
  	size = priv->rx_ring_size * sizeof(struct bcm_enet_desc);
++<<<<<<< HEAD
 +	p = dma_alloc_coherent(kdev, size, &priv->rx_desc_dma,
 +			       GFP_KERNEL | __GFP_ZERO);
++=======
+ 	p = dma_alloc_coherent(kdev, size, &priv->rx_desc_dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!p) {
  		ret = -ENOMEM;
  		goto out_freeirq_tx;
@@@ -874,8 -947,7 +878,12 @@@
  
  	/* allocate tx dma ring */
  	size = priv->tx_ring_size * sizeof(struct bcm_enet_desc);
++<<<<<<< HEAD
 +	p = dma_alloc_coherent(kdev, size, &priv->tx_desc_dma,
 +			       GFP_KERNEL | __GFP_ZERO);
++=======
+ 	p = dma_alloc_coherent(kdev, size, &priv->tx_desc_dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!p) {
  		ret = -ENOMEM;
  		goto out_free_rx_ring;
@@@ -1862,8 -1936,844 +1870,847 @@@ struct platform_driver bcm63xx_enet_dri
  };
  
  /*
 - * switch mii access callbacks
 + * reserve & remap memory space shared between all macs
   */
++<<<<<<< HEAD
++=======
+ static int bcmenet_sw_mdio_read(struct bcm_enet_priv *priv,
+ 				int ext, int phy_id, int location)
+ {
+ 	u32 reg;
+ 	int ret;
+ 
+ 	spin_lock_bh(&priv->enetsw_mdio_lock);
+ 	enetsw_writel(priv, 0, ENETSW_MDIOC_REG);
+ 
+ 	reg = ENETSW_MDIOC_RD_MASK |
+ 		(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |
+ 		(location << ENETSW_MDIOC_REG_SHIFT);
+ 
+ 	if (ext)
+ 		reg |= ENETSW_MDIOC_EXT_MASK;
+ 
+ 	enetsw_writel(priv, reg, ENETSW_MDIOC_REG);
+ 	udelay(50);
+ 	ret = enetsw_readw(priv, ENETSW_MDIOD_REG);
+ 	spin_unlock_bh(&priv->enetsw_mdio_lock);
+ 	return ret;
+ }
+ 
+ static void bcmenet_sw_mdio_write(struct bcm_enet_priv *priv,
+ 				 int ext, int phy_id, int location,
+ 				 uint16_t data)
+ {
+ 	u32 reg;
+ 
+ 	spin_lock_bh(&priv->enetsw_mdio_lock);
+ 	enetsw_writel(priv, 0, ENETSW_MDIOC_REG);
+ 
+ 	reg = ENETSW_MDIOC_WR_MASK |
+ 		(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |
+ 		(location << ENETSW_MDIOC_REG_SHIFT);
+ 
+ 	if (ext)
+ 		reg |= ENETSW_MDIOC_EXT_MASK;
+ 
+ 	reg |= data;
+ 
+ 	enetsw_writel(priv, reg, ENETSW_MDIOC_REG);
+ 	udelay(50);
+ 	spin_unlock_bh(&priv->enetsw_mdio_lock);
+ }
+ 
+ static inline int bcm_enet_port_is_rgmii(int portid)
+ {
+ 	return portid >= ENETSW_RGMII_PORT0;
+ }
+ 
+ /*
+  * enet sw PHY polling
+  */
+ static void swphy_poll_timer(struct timer_list *t)
+ {
+ 	struct bcm_enet_priv *priv = from_timer(priv, t, swphy_poll);
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		struct bcm63xx_enetsw_port *port;
+ 		int val, j, up, advertise, lpa, speed, duplex, media;
+ 		int external_phy = bcm_enet_port_is_rgmii(i);
+ 		u8 override;
+ 
+ 		port = &priv->used_ports[i];
+ 		if (!port->used)
+ 			continue;
+ 
+ 		if (port->bypass_link)
+ 			continue;
+ 
+ 		/* dummy read to clear */
+ 		for (j = 0; j < 2; j++)
+ 			val = bcmenet_sw_mdio_read(priv, external_phy,
+ 						   port->phy_id, MII_BMSR);
+ 
+ 		if (val == 0xffff)
+ 			continue;
+ 
+ 		up = (val & BMSR_LSTATUS) ? 1 : 0;
+ 		if (!(up ^ priv->sw_port_link[i]))
+ 			continue;
+ 
+ 		priv->sw_port_link[i] = up;
+ 
+ 		/* link changed */
+ 		if (!up) {
+ 			dev_info(&priv->pdev->dev, "link DOWN on %s\n",
+ 				 port->name);
+ 			enetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,
+ 				      ENETSW_PORTOV_REG(i));
+ 			enetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |
+ 				      ENETSW_PTCTRL_TXDIS_MASK,
+ 				      ENETSW_PTCTRL_REG(i));
+ 			continue;
+ 		}
+ 
+ 		advertise = bcmenet_sw_mdio_read(priv, external_phy,
+ 						 port->phy_id, MII_ADVERTISE);
+ 
+ 		lpa = bcmenet_sw_mdio_read(priv, external_phy, port->phy_id,
+ 					   MII_LPA);
+ 
+ 		/* figure out media and duplex from advertise and LPA values */
+ 		media = mii_nway_result(lpa & advertise);
+ 		duplex = (media & ADVERTISE_FULL) ? 1 : 0;
+ 
+ 		if (media & (ADVERTISE_100FULL | ADVERTISE_100HALF))
+ 			speed = 100;
+ 		else
+ 			speed = 10;
+ 
+ 		if (val & BMSR_ESTATEN) {
+ 			advertise = bcmenet_sw_mdio_read(priv, external_phy,
+ 						port->phy_id, MII_CTRL1000);
+ 
+ 			lpa = bcmenet_sw_mdio_read(priv, external_phy,
+ 						port->phy_id, MII_STAT1000);
+ 
+ 			if (advertise & (ADVERTISE_1000FULL | ADVERTISE_1000HALF)
+ 					&& lpa & (LPA_1000FULL | LPA_1000HALF)) {
+ 				speed = 1000;
+ 				duplex = (lpa & LPA_1000FULL);
+ 			}
+ 		}
+ 
+ 		dev_info(&priv->pdev->dev,
+ 			 "link UP on %s, %dMbps, %s-duplex\n",
+ 			 port->name, speed, duplex ? "full" : "half");
+ 
+ 		override = ENETSW_PORTOV_ENABLE_MASK |
+ 			ENETSW_PORTOV_LINKUP_MASK;
+ 
+ 		if (speed == 1000)
+ 			override |= ENETSW_IMPOV_1000_MASK;
+ 		else if (speed == 100)
+ 			override |= ENETSW_IMPOV_100_MASK;
+ 		if (duplex)
+ 			override |= ENETSW_IMPOV_FDX_MASK;
+ 
+ 		enetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));
+ 	}
+ 
+ 	priv->swphy_poll.expires = jiffies + HZ;
+ 	add_timer(&priv->swphy_poll);
+ }
+ 
+ /*
+  * open callback, allocate dma rings & buffers and start rx operation
+  */
+ static int bcm_enetsw_open(struct net_device *dev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct device *kdev;
+ 	int i, ret;
+ 	unsigned int size;
+ 	void *p;
+ 	u32 val;
+ 
+ 	priv = netdev_priv(dev);
+ 	kdev = &priv->pdev->dev;
+ 
+ 	/* mask all interrupts and request them */
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	ret = request_irq(priv->irq_rx, bcm_enet_isr_dma,
+ 			  0, dev->name, dev);
+ 	if (ret)
+ 		goto out_freeirq;
+ 
+ 	if (priv->irq_tx != -1) {
+ 		ret = request_irq(priv->irq_tx, bcm_enet_isr_dma,
+ 				  0, dev->name, dev);
+ 		if (ret)
+ 			goto out_freeirq_rx;
+ 	}
+ 
+ 	/* allocate rx dma ring */
+ 	size = priv->rx_ring_size * sizeof(struct bcm_enet_desc);
+ 	p = dma_alloc_coherent(kdev, size, &priv->rx_desc_dma, GFP_KERNEL);
+ 	if (!p) {
+ 		dev_err(kdev, "cannot allocate rx ring %u\n", size);
+ 		ret = -ENOMEM;
+ 		goto out_freeirq_tx;
+ 	}
+ 
+ 	priv->rx_desc_alloc_size = size;
+ 	priv->rx_desc_cpu = p;
+ 
+ 	/* allocate tx dma ring */
+ 	size = priv->tx_ring_size * sizeof(struct bcm_enet_desc);
+ 	p = dma_alloc_coherent(kdev, size, &priv->tx_desc_dma, GFP_KERNEL);
+ 	if (!p) {
+ 		dev_err(kdev, "cannot allocate tx ring\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_rx_ring;
+ 	}
+ 
+ 	priv->tx_desc_alloc_size = size;
+ 	priv->tx_desc_cpu = p;
+ 
+ 	priv->tx_skb = kcalloc(priv->tx_ring_size, sizeof(struct sk_buff *),
+ 			       GFP_KERNEL);
+ 	if (!priv->tx_skb) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_tx_ring;
+ 	}
+ 
+ 	priv->tx_desc_count = priv->tx_ring_size;
+ 	priv->tx_dirty_desc = 0;
+ 	priv->tx_curr_desc = 0;
+ 	spin_lock_init(&priv->tx_lock);
+ 
+ 	/* init & fill rx ring with skbs */
+ 	priv->rx_skb = kcalloc(priv->rx_ring_size, sizeof(struct sk_buff *),
+ 			       GFP_KERNEL);
+ 	if (!priv->rx_skb) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_tx_skb;
+ 	}
+ 
+ 	priv->rx_desc_count = 0;
+ 	priv->rx_dirty_desc = 0;
+ 	priv->rx_curr_desc = 0;
+ 
+ 	/* disable all ports */
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		enetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,
+ 			      ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |
+ 			      ENETSW_PTCTRL_TXDIS_MASK,
+ 			      ENETSW_PTCTRL_REG(i));
+ 
+ 		priv->sw_port_link[i] = 0;
+ 	}
+ 
+ 	/* reset mib */
+ 	val = enetsw_readb(priv, ENETSW_GMCR_REG);
+ 	val |= ENETSW_GMCR_RST_MIB_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_GMCR_REG);
+ 	mdelay(1);
+ 	val &= ~ENETSW_GMCR_RST_MIB_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_GMCR_REG);
+ 	mdelay(1);
+ 
+ 	/* force CPU port state */
+ 	val = enetsw_readb(priv, ENETSW_IMPOV_REG);
+ 	val |= ENETSW_IMPOV_FORCE_MASK | ENETSW_IMPOV_LINKUP_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_IMPOV_REG);
+ 
+ 	/* enable switch forward engine */
+ 	val = enetsw_readb(priv, ENETSW_SWMODE_REG);
+ 	val |= ENETSW_SWMODE_FWD_EN_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_SWMODE_REG);
+ 
+ 	/* enable jumbo on all ports */
+ 	enetsw_writel(priv, 0x1ff, ENETSW_JMBCTL_PORT_REG);
+ 	enetsw_writew(priv, 9728, ENETSW_JMBCTL_MAXSIZE_REG);
+ 
+ 	/* initialize flow control buffer allocation */
+ 	enet_dma_writel(priv, ENETDMA_BUFALLOC_FORCE_MASK | 0,
+ 			ENETDMA_BUFALLOC_REG(priv->rx_chan));
+ 
+ 	if (bcm_enet_refill_rx(dev)) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	/* write rx & tx ring addresses */
+ 	enet_dmas_writel(priv, priv->rx_desc_dma,
+ 			 ENETDMAS_RSTART_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, priv->tx_desc_dma,
+ 			 ENETDMAS_RSTART_REG, priv->tx_chan);
+ 
+ 	/* clear remaining state ram for rx & tx channel */
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->tx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->tx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->tx_chan);
+ 
+ 	/* set dma maximum burst len */
+ 	enet_dmac_writel(priv, priv->dma_maxburst,
+ 			 ENETDMAC_MAXBURST, priv->rx_chan);
+ 	enet_dmac_writel(priv, priv->dma_maxburst,
+ 			 ENETDMAC_MAXBURST, priv->tx_chan);
+ 
+ 	/* set flow control low/high threshold to 1/3 / 2/3 */
+ 	val = priv->rx_ring_size / 3;
+ 	enet_dma_writel(priv, val, ENETDMA_FLOWCL_REG(priv->rx_chan));
+ 	val = (priv->rx_ring_size * 2) / 3;
+ 	enet_dma_writel(priv, val, ENETDMA_FLOWCH_REG(priv->rx_chan));
+ 
+ 	/* all set, enable mac and interrupts, start dma engine and
+ 	 * kick rx dma channel
+ 	 */
+ 	wmb();
+ 	enet_dma_writel(priv, ENETDMA_CFG_EN_MASK, ENETDMA_CFG_REG);
+ 	enet_dmac_writel(priv, ENETDMAC_CHANCFG_EN_MASK,
+ 			 ENETDMAC_CHANCFG, priv->rx_chan);
+ 
+ 	/* watch "packet transferred" interrupt in rx and tx */
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IR, priv->rx_chan);
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IR, priv->tx_chan);
+ 
+ 	/* make sure we enable napi before rx interrupt  */
+ 	napi_enable(&priv->napi);
+ 
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	netif_carrier_on(dev);
+ 	netif_start_queue(dev);
+ 
+ 	/* apply override config for bypass_link ports here. */
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		struct bcm63xx_enetsw_port *port;
+ 		u8 override;
+ 		port = &priv->used_ports[i];
+ 		if (!port->used)
+ 			continue;
+ 
+ 		if (!port->bypass_link)
+ 			continue;
+ 
+ 		override = ENETSW_PORTOV_ENABLE_MASK |
+ 			ENETSW_PORTOV_LINKUP_MASK;
+ 
+ 		switch (port->force_speed) {
+ 		case 1000:
+ 			override |= ENETSW_IMPOV_1000_MASK;
+ 			break;
+ 		case 100:
+ 			override |= ENETSW_IMPOV_100_MASK;
+ 			break;
+ 		case 10:
+ 			break;
+ 		default:
+ 			pr_warn("invalid forced speed on port %s: assume 10\n",
+ 			       port->name);
+ 			break;
+ 		}
+ 
+ 		if (port->force_duplex_full)
+ 			override |= ENETSW_IMPOV_FDX_MASK;
+ 
+ 
+ 		enetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));
+ 	}
+ 
+ 	/* start phy polling timer */
+ 	timer_setup(&priv->swphy_poll, swphy_poll_timer, 0);
+ 	mod_timer(&priv->swphy_poll, jiffies);
+ 	return 0;
+ 
+ out:
+ 	for (i = 0; i < priv->rx_ring_size; i++) {
+ 		struct bcm_enet_desc *desc;
+ 
+ 		if (!priv->rx_skb[i])
+ 			continue;
+ 
+ 		desc = &priv->rx_desc_cpu[i];
+ 		dma_unmap_single(kdev, desc->address, priv->rx_skb_size,
+ 				 DMA_FROM_DEVICE);
+ 		kfree_skb(priv->rx_skb[i]);
+ 	}
+ 	kfree(priv->rx_skb);
+ 
+ out_free_tx_skb:
+ 	kfree(priv->tx_skb);
+ 
+ out_free_tx_ring:
+ 	dma_free_coherent(kdev, priv->tx_desc_alloc_size,
+ 			  priv->tx_desc_cpu, priv->tx_desc_dma);
+ 
+ out_free_rx_ring:
+ 	dma_free_coherent(kdev, priv->rx_desc_alloc_size,
+ 			  priv->rx_desc_cpu, priv->rx_desc_dma);
+ 
+ out_freeirq_tx:
+ 	if (priv->irq_tx != -1)
+ 		free_irq(priv->irq_tx, dev);
+ 
+ out_freeirq_rx:
+ 	free_irq(priv->irq_rx, dev);
+ 
+ out_freeirq:
+ 	return ret;
+ }
+ 
+ /* stop callback */
+ static int bcm_enetsw_stop(struct net_device *dev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct device *kdev;
+ 	int i;
+ 
+ 	priv = netdev_priv(dev);
+ 	kdev = &priv->pdev->dev;
+ 
+ 	del_timer_sync(&priv->swphy_poll);
+ 	netif_stop_queue(dev);
+ 	napi_disable(&priv->napi);
+ 	del_timer_sync(&priv->rx_timeout);
+ 
+ 	/* mask all interrupts */
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	/* disable dma & mac */
+ 	bcm_enet_disable_dma(priv, priv->tx_chan);
+ 	bcm_enet_disable_dma(priv, priv->rx_chan);
+ 
+ 	/* force reclaim of all tx buffers */
+ 	bcm_enet_tx_reclaim(dev, 1);
+ 
+ 	/* free the rx skb ring */
+ 	for (i = 0; i < priv->rx_ring_size; i++) {
+ 		struct bcm_enet_desc *desc;
+ 
+ 		if (!priv->rx_skb[i])
+ 			continue;
+ 
+ 		desc = &priv->rx_desc_cpu[i];
+ 		dma_unmap_single(kdev, desc->address, priv->rx_skb_size,
+ 				 DMA_FROM_DEVICE);
+ 		kfree_skb(priv->rx_skb[i]);
+ 	}
+ 
+ 	/* free remaining allocated memory */
+ 	kfree(priv->rx_skb);
+ 	kfree(priv->tx_skb);
+ 	dma_free_coherent(kdev, priv->rx_desc_alloc_size,
+ 			  priv->rx_desc_cpu, priv->rx_desc_dma);
+ 	dma_free_coherent(kdev, priv->tx_desc_alloc_size,
+ 			  priv->tx_desc_cpu, priv->tx_desc_dma);
+ 	if (priv->irq_tx != -1)
+ 		free_irq(priv->irq_tx, dev);
+ 	free_irq(priv->irq_rx, dev);
+ 
+ 	return 0;
+ }
+ 
+ /* try to sort out phy external status by walking the used_port field
+  * in the bcm_enet_priv structure. in case the phy address is not
+  * assigned to any physical port on the switch, assume it is external
+  * (and yell at the user).
+  */
+ static int bcm_enetsw_phy_is_external(struct bcm_enet_priv *priv, int phy_id)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->num_ports; ++i) {
+ 		if (!priv->used_ports[i].used)
+ 			continue;
+ 		if (priv->used_ports[i].phy_id == phy_id)
+ 			return bcm_enet_port_is_rgmii(i);
+ 	}
+ 
+ 	printk_once(KERN_WARNING  "bcm63xx_enet: could not find a used port with phy_id %i, assuming phy is external\n",
+ 		    phy_id);
+ 	return 1;
+ }
+ 
+ /* can't use bcmenet_sw_mdio_read directly as we need to sort out
+  * external/internal status of the given phy_id first.
+  */
+ static int bcm_enetsw_mii_mdio_read(struct net_device *dev, int phy_id,
+ 				    int location)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 	return bcmenet_sw_mdio_read(priv,
+ 				    bcm_enetsw_phy_is_external(priv, phy_id),
+ 				    phy_id, location);
+ }
+ 
+ /* can't use bcmenet_sw_mdio_write directly as we need to sort out
+  * external/internal status of the given phy_id first.
+  */
+ static void bcm_enetsw_mii_mdio_write(struct net_device *dev, int phy_id,
+ 				      int location,
+ 				      int val)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 	bcmenet_sw_mdio_write(priv, bcm_enetsw_phy_is_external(priv, phy_id),
+ 			      phy_id, location, val);
+ }
+ 
+ static int bcm_enetsw_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+ {
+ 	struct mii_if_info mii;
+ 
+ 	mii.dev = dev;
+ 	mii.mdio_read = bcm_enetsw_mii_mdio_read;
+ 	mii.mdio_write = bcm_enetsw_mii_mdio_write;
+ 	mii.phy_id = 0;
+ 	mii.phy_id_mask = 0x3f;
+ 	mii.reg_num_mask = 0x1f;
+ 	return generic_mii_ioctl(&mii, if_mii(rq), cmd, NULL);
+ 
+ }
+ 
+ static const struct net_device_ops bcm_enetsw_ops = {
+ 	.ndo_open		= bcm_enetsw_open,
+ 	.ndo_stop		= bcm_enetsw_stop,
+ 	.ndo_start_xmit		= bcm_enet_start_xmit,
+ 	.ndo_change_mtu		= bcm_enet_change_mtu,
+ 	.ndo_do_ioctl		= bcm_enetsw_ioctl,
+ };
+ 
+ 
+ static const struct bcm_enet_stats bcm_enetsw_gstrings_stats[] = {
+ 	{ "rx_packets", DEV_STAT(rx_packets), -1 },
+ 	{ "tx_packets",	DEV_STAT(tx_packets), -1 },
+ 	{ "rx_bytes", DEV_STAT(rx_bytes), -1 },
+ 	{ "tx_bytes", DEV_STAT(tx_bytes), -1 },
+ 	{ "rx_errors", DEV_STAT(rx_errors), -1 },
+ 	{ "tx_errors", DEV_STAT(tx_errors), -1 },
+ 	{ "rx_dropped",	DEV_STAT(rx_dropped), -1 },
+ 	{ "tx_dropped",	DEV_STAT(tx_dropped), -1 },
+ 
+ 	{ "tx_good_octets", GEN_STAT(mib.tx_gd_octets), ETHSW_MIB_RX_GD_OCT },
+ 	{ "tx_unicast", GEN_STAT(mib.tx_unicast), ETHSW_MIB_RX_BRDCAST },
+ 	{ "tx_broadcast", GEN_STAT(mib.tx_brdcast), ETHSW_MIB_RX_BRDCAST },
+ 	{ "tx_multicast", GEN_STAT(mib.tx_mult), ETHSW_MIB_RX_MULT },
+ 	{ "tx_64_octets", GEN_STAT(mib.tx_64), ETHSW_MIB_RX_64 },
+ 	{ "tx_65_127_oct", GEN_STAT(mib.tx_65_127), ETHSW_MIB_RX_65_127 },
+ 	{ "tx_128_255_oct", GEN_STAT(mib.tx_128_255), ETHSW_MIB_RX_128_255 },
+ 	{ "tx_256_511_oct", GEN_STAT(mib.tx_256_511), ETHSW_MIB_RX_256_511 },
+ 	{ "tx_512_1023_oct", GEN_STAT(mib.tx_512_1023), ETHSW_MIB_RX_512_1023},
+ 	{ "tx_1024_1522_oct", GEN_STAT(mib.tx_1024_max),
+ 	  ETHSW_MIB_RX_1024_1522 },
+ 	{ "tx_1523_2047_oct", GEN_STAT(mib.tx_1523_2047),
+ 	  ETHSW_MIB_RX_1523_2047 },
+ 	{ "tx_2048_4095_oct", GEN_STAT(mib.tx_2048_4095),
+ 	  ETHSW_MIB_RX_2048_4095 },
+ 	{ "tx_4096_8191_oct", GEN_STAT(mib.tx_4096_8191),
+ 	  ETHSW_MIB_RX_4096_8191 },
+ 	{ "tx_8192_9728_oct", GEN_STAT(mib.tx_8192_9728),
+ 	  ETHSW_MIB_RX_8192_9728 },
+ 	{ "tx_oversize", GEN_STAT(mib.tx_ovr), ETHSW_MIB_RX_OVR },
+ 	{ "tx_oversize_drop", GEN_STAT(mib.tx_ovr), ETHSW_MIB_RX_OVR_DISC },
+ 	{ "tx_dropped",	GEN_STAT(mib.tx_drop), ETHSW_MIB_RX_DROP },
+ 	{ "tx_undersize", GEN_STAT(mib.tx_underrun), ETHSW_MIB_RX_UND },
+ 	{ "tx_pause", GEN_STAT(mib.tx_pause), ETHSW_MIB_RX_PAUSE },
+ 
+ 	{ "rx_good_octets", GEN_STAT(mib.rx_gd_octets), ETHSW_MIB_TX_ALL_OCT },
+ 	{ "rx_broadcast", GEN_STAT(mib.rx_brdcast), ETHSW_MIB_TX_BRDCAST },
+ 	{ "rx_multicast", GEN_STAT(mib.rx_mult), ETHSW_MIB_TX_MULT },
+ 	{ "rx_unicast", GEN_STAT(mib.rx_unicast), ETHSW_MIB_TX_MULT },
+ 	{ "rx_pause", GEN_STAT(mib.rx_pause), ETHSW_MIB_TX_PAUSE },
+ 	{ "rx_dropped", GEN_STAT(mib.rx_drop), ETHSW_MIB_TX_DROP_PKTS },
+ 
+ };
+ 
+ #define BCM_ENETSW_STATS_LEN	\
+ 	(sizeof(bcm_enetsw_gstrings_stats) / sizeof(struct bcm_enet_stats))
+ 
+ static void bcm_enetsw_get_strings(struct net_device *netdev,
+ 				   u32 stringset, u8 *data)
+ {
+ 	int i;
+ 
+ 	switch (stringset) {
+ 	case ETH_SS_STATS:
+ 		for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 			memcpy(data + i * ETH_GSTRING_LEN,
+ 			       bcm_enetsw_gstrings_stats[i].stat_string,
+ 			       ETH_GSTRING_LEN);
+ 		}
+ 		break;
+ 	}
+ }
+ 
+ static int bcm_enetsw_get_sset_count(struct net_device *netdev,
+ 				     int string_set)
+ {
+ 	switch (string_set) {
+ 	case ETH_SS_STATS:
+ 		return BCM_ENETSW_STATS_LEN;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ static void bcm_enetsw_get_drvinfo(struct net_device *netdev,
+ 				   struct ethtool_drvinfo *drvinfo)
+ {
+ 	strncpy(drvinfo->driver, bcm_enet_driver_name, 32);
+ 	strncpy(drvinfo->version, bcm_enet_driver_version, 32);
+ 	strncpy(drvinfo->fw_version, "N/A", 32);
+ 	strncpy(drvinfo->bus_info, "bcm63xx", 32);
+ }
+ 
+ static void bcm_enetsw_get_ethtool_stats(struct net_device *netdev,
+ 					 struct ethtool_stats *stats,
+ 					 u64 *data)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	int i;
+ 
+ 	priv = netdev_priv(netdev);
+ 
+ 	for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 		const struct bcm_enet_stats *s;
+ 		u32 lo, hi;
+ 		char *p;
+ 		int reg;
+ 
+ 		s = &bcm_enetsw_gstrings_stats[i];
+ 
+ 		reg = s->mib_reg;
+ 		if (reg == -1)
+ 			continue;
+ 
+ 		lo = enetsw_readl(priv, ENETSW_MIB_REG(reg));
+ 		p = (char *)priv + s->stat_offset;
+ 
+ 		if (s->sizeof_stat == sizeof(u64)) {
+ 			hi = enetsw_readl(priv, ENETSW_MIB_REG(reg + 1));
+ 			*(u64 *)p = ((u64)hi << 32 | lo);
+ 		} else {
+ 			*(u32 *)p = lo;
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 		const struct bcm_enet_stats *s;
+ 		char *p;
+ 
+ 		s = &bcm_enetsw_gstrings_stats[i];
+ 
+ 		if (s->mib_reg == -1)
+ 			p = (char *)&netdev->stats + s->stat_offset;
+ 		else
+ 			p = (char *)priv + s->stat_offset;
+ 
+ 		data[i] = (s->sizeof_stat == sizeof(u64)) ?
+ 			*(u64 *)p : *(u32 *)p;
+ 	}
+ }
+ 
+ static void bcm_enetsw_get_ringparam(struct net_device *dev,
+ 				     struct ethtool_ringparam *ering)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 
+ 	/* rx/tx ring is actually only limited by memory */
+ 	ering->rx_max_pending = 8192;
+ 	ering->tx_max_pending = 8192;
+ 	ering->rx_mini_max_pending = 0;
+ 	ering->rx_jumbo_max_pending = 0;
+ 	ering->rx_pending = priv->rx_ring_size;
+ 	ering->tx_pending = priv->tx_ring_size;
+ }
+ 
+ static int bcm_enetsw_set_ringparam(struct net_device *dev,
+ 				    struct ethtool_ringparam *ering)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	int was_running;
+ 
+ 	priv = netdev_priv(dev);
+ 
+ 	was_running = 0;
+ 	if (netif_running(dev)) {
+ 		bcm_enetsw_stop(dev);
+ 		was_running = 1;
+ 	}
+ 
+ 	priv->rx_ring_size = ering->rx_pending;
+ 	priv->tx_ring_size = ering->tx_pending;
+ 
+ 	if (was_running) {
+ 		int err;
+ 
+ 		err = bcm_enetsw_open(dev);
+ 		if (err)
+ 			dev_close(dev);
+ 	}
+ 	return 0;
+ }
+ 
+ static const struct ethtool_ops bcm_enetsw_ethtool_ops = {
+ 	.get_strings		= bcm_enetsw_get_strings,
+ 	.get_sset_count		= bcm_enetsw_get_sset_count,
+ 	.get_ethtool_stats      = bcm_enetsw_get_ethtool_stats,
+ 	.get_drvinfo		= bcm_enetsw_get_drvinfo,
+ 	.get_ringparam		= bcm_enetsw_get_ringparam,
+ 	.set_ringparam		= bcm_enetsw_set_ringparam,
+ };
+ 
+ /* allocate netdevice, request register memory and register device. */
+ static int bcm_enetsw_probe(struct platform_device *pdev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct net_device *dev;
+ 	struct bcm63xx_enetsw_platform_data *pd;
+ 	struct resource *res_mem;
+ 	int ret, irq_rx, irq_tx;
+ 
+ 	if (!bcm_enet_shared_base[0])
+ 		return -EPROBE_DEFER;
+ 
+ 	res_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+ 	irq_rx = platform_get_irq(pdev, 0);
+ 	irq_tx = platform_get_irq(pdev, 1);
+ 	if (!res_mem || irq_rx < 0)
+ 		return -ENODEV;
+ 
+ 	ret = 0;
+ 	dev = alloc_etherdev(sizeof(*priv));
+ 	if (!dev)
+ 		return -ENOMEM;
+ 	priv = netdev_priv(dev);
+ 	memset(priv, 0, sizeof(*priv));
+ 
+ 	/* initialize default and fetch platform data */
+ 	priv->enet_is_sw = true;
+ 	priv->irq_rx = irq_rx;
+ 	priv->irq_tx = irq_tx;
+ 	priv->rx_ring_size = BCMENET_DEF_RX_DESC;
+ 	priv->tx_ring_size = BCMENET_DEF_TX_DESC;
+ 	priv->dma_maxburst = BCMENETSW_DMA_MAXBURST;
+ 
+ 	pd = dev_get_platdata(&pdev->dev);
+ 	if (pd) {
+ 		memcpy(dev->dev_addr, pd->mac_addr, ETH_ALEN);
+ 		memcpy(priv->used_ports, pd->used_ports,
+ 		       sizeof(pd->used_ports));
+ 		priv->num_ports = pd->num_ports;
+ 		priv->dma_has_sram = pd->dma_has_sram;
+ 		priv->dma_chan_en_mask = pd->dma_chan_en_mask;
+ 		priv->dma_chan_int_mask = pd->dma_chan_int_mask;
+ 		priv->dma_chan_width = pd->dma_chan_width;
+ 	}
+ 
+ 	ret = bcm_enet_change_mtu(dev, dev->mtu);
+ 	if (ret)
+ 		goto out;
+ 
+ 	priv->base = devm_ioremap_resource(&pdev->dev, res_mem);
+ 	if (IS_ERR(priv->base)) {
+ 		ret = PTR_ERR(priv->base);
+ 		goto out;
+ 	}
+ 
+ 	priv->mac_clk = devm_clk_get(&pdev->dev, "enetsw");
+ 	if (IS_ERR(priv->mac_clk)) {
+ 		ret = PTR_ERR(priv->mac_clk);
+ 		goto out;
+ 	}
+ 	ret = clk_prepare_enable(priv->mac_clk);
+ 	if (ret)
+ 		goto out;
+ 
+ 	priv->rx_chan = 0;
+ 	priv->tx_chan = 1;
+ 	spin_lock_init(&priv->rx_lock);
+ 
+ 	/* init rx timeout (used for oom) */
+ 	timer_setup(&priv->rx_timeout, bcm_enet_refill_rx_timer, 0);
+ 
+ 	/* register netdevice */
+ 	dev->netdev_ops = &bcm_enetsw_ops;
+ 	netif_napi_add(dev, &priv->napi, bcm_enet_poll, 16);
+ 	dev->ethtool_ops = &bcm_enetsw_ethtool_ops;
+ 	SET_NETDEV_DEV(dev, &pdev->dev);
+ 
+ 	spin_lock_init(&priv->enetsw_mdio_lock);
+ 
+ 	ret = register_netdev(dev);
+ 	if (ret)
+ 		goto out_disable_clk;
+ 
+ 	netif_carrier_off(dev);
+ 	platform_set_drvdata(pdev, dev);
+ 	priv->pdev = pdev;
+ 	priv->net_dev = dev;
+ 
+ 	return 0;
+ 
+ out_disable_clk:
+ 	clk_disable_unprepare(priv->mac_clk);
+ out:
+ 	free_netdev(dev);
+ 	return ret;
+ }
+ 
+ 
+ /* exit func, stops hardware and unregisters netdevice */
+ static int bcm_enetsw_remove(struct platform_device *pdev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct net_device *dev;
+ 
+ 	/* stop netdevice */
+ 	dev = platform_get_drvdata(pdev);
+ 	priv = netdev_priv(dev);
+ 	unregister_netdev(dev);
+ 
+ 	clk_disable_unprepare(priv->mac_clk);
+ 
+ 	free_netdev(dev);
+ 	return 0;
+ }
+ 
+ struct platform_driver bcm63xx_enetsw_driver = {
+ 	.probe	= bcm_enetsw_probe,
+ 	.remove	= bcm_enetsw_remove,
+ 	.driver	= {
+ 		.name	= "bcm63xx_enetsw",
+ 		.owner  = THIS_MODULE,
+ 	},
+ };
+ 
+ /* reserve & remap memory space shared between all macs */
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  static int bcm_enet_shared_probe(struct platform_device *pdev)
  {
  	struct resource *res;
diff --cc drivers/net/ethernet/broadcom/bgmac.c
index 566081066bd6,2d3a44c40221..000000000000
--- a/drivers/net/ethernet/broadcom/bgmac.c
+++ b/drivers/net/ethernet/broadcom/bgmac.c
@@@ -416,60 -630,50 +416,74 @@@ static int bgmac_dma_alloc(struct bgma
  
  	for (i = 0; i < BGMAC_MAX_TX_RINGS; i++) {
  		ring = &bgmac->tx_ring[i];
 +		ring->num_slots = BGMAC_TX_RING_SLOTS;
  		ring->mmio_base = ring_base[i];
 +		if (bgmac_dma_unaligned(bgmac, ring, BGMAC_DMA_RING_TX))
 +			bgmac_warn(bgmac, "TX on ring 0x%X supports unaligned addressing but this feature is not implemented\n",
 +				   ring->mmio_base);
  
  		/* Alloc ring of descriptors */
++<<<<<<< HEAD
 +		size = ring->num_slots * sizeof(struct bgmac_dma_desc);
 +		ring->cpu_base = dma_zalloc_coherent(dma_dev, size,
 +						     &ring->dma_base,
 +						     GFP_KERNEL);
++=======
+ 		size = BGMAC_TX_RING_SLOTS * sizeof(struct bgmac_dma_desc);
+ 		ring->cpu_base = dma_alloc_coherent(dma_dev, size,
+ 						    &ring->dma_base,
+ 						    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!ring->cpu_base) {
 -			dev_err(bgmac->dev, "Allocation of TX ring 0x%X failed\n",
 -				ring->mmio_base);
 +			bgmac_err(bgmac, "Allocation of TX ring 0x%X failed\n",
 +				  ring->mmio_base);
  			goto err_dma_free;
  		}
 -
 -		ring->unaligned = bgmac_dma_unaligned(bgmac, ring,
 -						      BGMAC_DMA_RING_TX);
 -		if (ring->unaligned)
 -			ring->index_base = lower_32_bits(ring->dma_base);
 -		else
 -			ring->index_base = 0;
 +		if (ring->dma_base & 0xC0000000)
 +			bgmac_warn(bgmac, "DMA address using 0xC0000000 bit(s), it may need translation trick\n");
  
  		/* No need to alloc TX slots yet */
  	}
  
  	for (i = 0; i < BGMAC_MAX_RX_RINGS; i++) {
 +		int j;
 +
  		ring = &bgmac->rx_ring[i];
 +		ring->num_slots = BGMAC_RX_RING_SLOTS;
  		ring->mmio_base = ring_base[i];
 +		if (bgmac_dma_unaligned(bgmac, ring, BGMAC_DMA_RING_RX))
 +			bgmac_warn(bgmac, "RX on ring 0x%X supports unaligned addressing but this feature is not implemented\n",
 +				   ring->mmio_base);
  
  		/* Alloc ring of descriptors */
++<<<<<<< HEAD
 +		size = ring->num_slots * sizeof(struct bgmac_dma_desc);
 +		ring->cpu_base = dma_zalloc_coherent(dma_dev, size,
 +						     &ring->dma_base,
 +						     GFP_KERNEL);
++=======
+ 		size = BGMAC_RX_RING_SLOTS * sizeof(struct bgmac_dma_desc);
+ 		ring->cpu_base = dma_alloc_coherent(dma_dev, size,
+ 						    &ring->dma_base,
+ 						    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!ring->cpu_base) {
 -			dev_err(bgmac->dev, "Allocation of RX ring 0x%X failed\n",
 -				ring->mmio_base);
 +			bgmac_err(bgmac, "Allocation of RX ring 0x%X failed\n",
 +				  ring->mmio_base);
 +			err = -ENOMEM;
  			goto err_dma_free;
  		}
 +		if (ring->dma_base & 0xC0000000)
 +			bgmac_warn(bgmac, "DMA address using 0xC0000000 bit(s), it may need translation trick\n");
  
 -		ring->unaligned = bgmac_dma_unaligned(bgmac, ring,
 -						      BGMAC_DMA_RING_RX);
 -		if (ring->unaligned)
 -			ring->index_base = lower_32_bits(ring->dma_base);
 -		else
 -			ring->index_base = 0;
 +		/* Alloc RX slots */
 +		for (j = 0; j < ring->num_slots; j++) {
 +			err = bgmac_dma_rx_skb_for_slot(bgmac, &ring->slots[j]);
 +			if (err) {
 +				bgmac_err(bgmac, "Can't allocate skb for slot in RX ring\n");
 +				goto err_dma_free;
 +			}
 +		}
  	}
  
  	return 0;
diff --cc drivers/net/ethernet/broadcom/bnx2.c
index 6b838496bd1a,d63371d70bce..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2.c
+++ b/drivers/net/ethernet/broadcom/bnx2.c
@@@ -843,9 -844,9 +843,15 @@@ bnx2_alloc_stats_blk(struct net_device 
  						 BNX2_SBLK_MSIX_ALIGN_SIZE);
  	bp->status_stats_size = status_blk_size +
  				sizeof(struct statistics_block);
++<<<<<<< HEAD
 +	status_blk = dma_zalloc_coherent(&bp->pdev->dev, bp->status_stats_size,
 +					 &bp->status_blk_mapping, GFP_KERNEL);
 +	if (status_blk == NULL)
++=======
+ 	status_blk = dma_alloc_coherent(&bp->pdev->dev, bp->status_stats_size,
+ 					&bp->status_blk_mapping, GFP_KERNEL);
+ 	if (!status_blk)
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		return -ENOMEM;
  
  	bp->status_blk = status_blk;
diff --cc drivers/net/ethernet/faraday/ftgmac100.c
index 21b85fb7d05f,3e5e97186fc4..000000000000
--- a/drivers/net/ethernet/faraday/ftgmac100.c
+++ b/drivers/net/ethernet/faraday/ftgmac100.c
@@@ -765,16 -892,105 +765,113 @@@ static void ftgmac100_free_buffers(stru
  
  		if (!skb)
  			continue;
 -		ftgmac100_free_tx_packet(priv, i, skb, txdes,
 -					 le32_to_cpu(txdes->txdes0));
 +
 +		dma_unmap_single(priv->dev, map, skb_headlen(skb), DMA_TO_DEVICE);
 +		dev_kfree_skb(skb);
  	}
 +
 +	dma_free_coherent(priv->dev, sizeof(struct ftgmac100_descs),
 +			  priv->descs, priv->descs_dma_addr);
  }
  
++<<<<<<< HEAD
 +static int ftgmac100_alloc_buffers(struct ftgmac100 *priv)
++=======
+ static void ftgmac100_free_rings(struct ftgmac100 *priv)
+ {
+ 	/* Free skb arrays */
+ 	kfree(priv->rx_skbs);
+ 	kfree(priv->tx_skbs);
+ 
+ 	/* Free descriptors */
+ 	if (priv->rxdes)
+ 		dma_free_coherent(priv->dev, MAX_RX_QUEUE_ENTRIES *
+ 				  sizeof(struct ftgmac100_rxdes),
+ 				  priv->rxdes, priv->rxdes_dma);
+ 	priv->rxdes = NULL;
+ 
+ 	if (priv->txdes)
+ 		dma_free_coherent(priv->dev, MAX_TX_QUEUE_ENTRIES *
+ 				  sizeof(struct ftgmac100_txdes),
+ 				  priv->txdes, priv->txdes_dma);
+ 	priv->txdes = NULL;
+ 
+ 	/* Free scratch packet buffer */
+ 	if (priv->rx_scratch)
+ 		dma_free_coherent(priv->dev, RX_BUF_SIZE,
+ 				  priv->rx_scratch, priv->rx_scratch_dma);
+ }
+ 
+ static int ftgmac100_alloc_rings(struct ftgmac100 *priv)
+ {
+ 	/* Allocate skb arrays */
+ 	priv->rx_skbs = kcalloc(MAX_RX_QUEUE_ENTRIES, sizeof(void *),
+ 				GFP_KERNEL);
+ 	if (!priv->rx_skbs)
+ 		return -ENOMEM;
+ 	priv->tx_skbs = kcalloc(MAX_TX_QUEUE_ENTRIES, sizeof(void *),
+ 				GFP_KERNEL);
+ 	if (!priv->tx_skbs)
+ 		return -ENOMEM;
+ 
+ 	/* Allocate descriptors */
+ 	priv->rxdes = dma_alloc_coherent(priv->dev,
+ 					 MAX_RX_QUEUE_ENTRIES * sizeof(struct ftgmac100_rxdes),
+ 					 &priv->rxdes_dma, GFP_KERNEL);
+ 	if (!priv->rxdes)
+ 		return -ENOMEM;
+ 	priv->txdes = dma_alloc_coherent(priv->dev,
+ 					 MAX_TX_QUEUE_ENTRIES * sizeof(struct ftgmac100_txdes),
+ 					 &priv->txdes_dma, GFP_KERNEL);
+ 	if (!priv->txdes)
+ 		return -ENOMEM;
+ 
+ 	/* Allocate scratch packet buffer */
+ 	priv->rx_scratch = dma_alloc_coherent(priv->dev,
+ 					      RX_BUF_SIZE,
+ 					      &priv->rx_scratch_dma,
+ 					      GFP_KERNEL);
+ 	if (!priv->rx_scratch)
+ 		return -ENOMEM;
+ 
+ 	return 0;
+ }
+ 
+ static void ftgmac100_init_rings(struct ftgmac100 *priv)
+ {
+ 	struct ftgmac100_rxdes *rxdes = NULL;
+ 	struct ftgmac100_txdes *txdes = NULL;
+ 	int i;
+ 
+ 	/* Update entries counts */
+ 	priv->rx_q_entries = priv->new_rx_q_entries;
+ 	priv->tx_q_entries = priv->new_tx_q_entries;
+ 
+ 	if (WARN_ON(priv->rx_q_entries < MIN_RX_QUEUE_ENTRIES))
+ 		return;
+ 
+ 	/* Initialize RX ring */
+ 	for (i = 0; i < priv->rx_q_entries; i++) {
+ 		rxdes = &priv->rxdes[i];
+ 		rxdes->rxdes0 = 0;
+ 		rxdes->rxdes3 = cpu_to_le32(priv->rx_scratch_dma);
+ 	}
+ 	/* Mark the end of the ring */
+ 	rxdes->rxdes0 |= cpu_to_le32(priv->rxdes0_edorr_mask);
+ 
+ 	if (WARN_ON(priv->tx_q_entries < MIN_RX_QUEUE_ENTRIES))
+ 		return;
+ 
+ 	/* Initialize TX ring */
+ 	for (i = 0; i < priv->tx_q_entries; i++) {
+ 		txdes = &priv->txdes[i];
+ 		txdes->txdes0 = 0;
+ 	}
+ 	txdes->txdes0 |= cpu_to_le32(priv->txdes0_edotr_mask);
+ }
+ 
+ static int ftgmac100_alloc_rx_buffers(struct ftgmac100 *priv)
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  {
  	int i;
  
diff --cc drivers/net/ethernet/faraday/ftmac100.c
index a6eda8d83138,2a0e820526dc..000000000000
--- a/drivers/net/ethernet/faraday/ftmac100.c
+++ b/drivers/net/ethernet/faraday/ftmac100.c
@@@ -734,8 -736,7 +734,12 @@@ static int ftmac100_alloc_buffers(struc
  
  	priv->descs = dma_alloc_coherent(priv->dev,
  					 sizeof(struct ftmac100_descs),
++<<<<<<< HEAD
 +					 &priv->descs_dma_addr,
 +					 GFP_KERNEL | __GFP_ZERO);
++=======
+ 					 &priv->descs_dma_addr, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!priv->descs)
  		return -ENOMEM;
  
diff --cc drivers/net/ethernet/ibm/emac/mal.c
index 610ed223d1db,787d5aca5278..000000000000
--- a/drivers/net/ethernet/ibm/emac/mal.c
+++ b/drivers/net/ethernet/ibm/emac/mal.c
@@@ -638,7 -637,7 +638,11 @@@ static int mal_probe(struct platform_de
  		(NUM_TX_BUFF * mal->num_tx_chans +
  		 NUM_RX_BUFF * mal->num_rx_chans);
  	mal->bd_virt = dma_alloc_coherent(&ofdev->dev, bd_size, &mal->bd_dma,
++<<<<<<< HEAD
 +					  GFP_KERNEL | __GFP_ZERO);
++=======
+ 					  GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (mal->bd_virt == NULL) {
  		err = -ENOMEM;
  		goto fail_unmap;
diff --cc drivers/net/ethernet/intel/e1000/e1000_ethtool.c
index cd4d72eaf8a7,a41008523c98..000000000000
--- a/drivers/net/ethernet/intel/e1000/e1000_ethtool.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_ethtool.c
@@@ -1013,7 -994,7 +1013,11 @@@ static int e1000_setup_desc_rings(struc
  	txdr->size = txdr->count * sizeof(struct e1000_tx_desc);
  	txdr->size = ALIGN(txdr->size, 4096);
  	txdr->desc = dma_alloc_coherent(&pdev->dev, txdr->size, &txdr->dma,
++<<<<<<< HEAD
 +					GFP_KERNEL | __GFP_ZERO);
++=======
+ 					GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!txdr->desc) {
  		ret_val = 2;
  		goto err_nomem;
@@@ -1071,7 -1052,7 +1075,11 @@@
  
  	rxdr->size = rxdr->count * sizeof(struct e1000_rx_desc);
  	rxdr->desc = dma_alloc_coherent(&pdev->dev, rxdr->size, &rxdr->dma,
++<<<<<<< HEAD
 +					GFP_KERNEL | __GFP_ZERO);
++=======
+ 					GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!rxdr->desc) {
  		ret_val = 6;
  		goto err_nomem;
diff --cc drivers/net/ethernet/intel/ixgb/ixgb_main.c
index dd42930f2b56,e5ac2d3fd816..000000000000
--- a/drivers/net/ethernet/intel/ixgb/ixgb_main.c
+++ b/drivers/net/ethernet/intel/ixgb/ixgb_main.c
@@@ -719,7 -681,7 +719,11 @@@ ixgb_setup_tx_resources(struct ixgb_ada
  	txdr->size = ALIGN(txdr->size, 4096);
  
  	txdr->desc = dma_alloc_coherent(&pdev->dev, txdr->size, &txdr->dma,
++<<<<<<< HEAD
 +					GFP_KERNEL | __GFP_ZERO);
++=======
+ 					GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!txdr->desc) {
  		vfree(txdr->buffer_info);
  		return -ENOMEM;
diff --cc drivers/net/ethernet/marvell/pxa168_eth.c
index 420c0eb4bc75,f8a6d6e3cb7a..000000000000
--- a/drivers/net/ethernet/marvell/pxa168_eth.c
+++ b/drivers/net/ethernet/marvell/pxa168_eth.c
@@@ -582,12 -556,11 +582,20 @@@ static int init_hash_table(struct pxa16
  	 * function.Driver can dynamically switch to them if the 1/2kB hash
  	 * table is full.
  	 */
++<<<<<<< HEAD
 +	if (pep->htpr == NULL) {
 +		pep->htpr = dma_alloc_coherent(pep->dev->dev.parent,
 +					       HASH_ADDR_TABLE_SIZE,
 +					       &pep->htpr_dma,
 +					       GFP_KERNEL | __GFP_ZERO);
 +		if (pep->htpr == NULL)
++=======
+ 	if (!pep->htpr) {
+ 		pep->htpr = dma_alloc_coherent(pep->dev->dev.parent,
+ 					       HASH_ADDR_TABLE_SIZE,
+ 					       &pep->htpr_dma, GFP_KERNEL);
+ 		if (!pep->htpr)
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  			return -ENOMEM;
  	} else {
  		memset(pep->htpr, 0, HASH_ADDR_TABLE_SIZE);
@@@ -1026,7 -1046,7 +1034,11 @@@ static int rxq_init(struct net_device *
  	pep->rx_desc_area_size = size;
  	pep->p_rx_desc_area = dma_alloc_coherent(pep->dev->dev.parent, size,
  						 &pep->rx_desc_dma,
++<<<<<<< HEAD
 +						 GFP_KERNEL | __GFP_ZERO);
++=======
+ 						 GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!pep->p_rx_desc_area)
  		goto out;
  
@@@ -1087,7 -1105,7 +1099,11 @@@ static int txq_init(struct net_device *
  	pep->tx_desc_area_size = size;
  	pep->p_tx_desc_area = dma_alloc_coherent(pep->dev->dev.parent, size,
  						 &pep->tx_desc_dma,
++<<<<<<< HEAD
 +						 GFP_KERNEL | __GFP_ZERO);
++=======
+ 						 GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!pep->p_tx_desc_area)
  		goto out;
  	/* Initialize the next_desc_ptr links in the Tx descriptors ring */
diff --cc drivers/net/ethernet/myricom/myri10ge/myri10ge.c
index eb1a6c95ce8d,19ce0e605096..000000000000
--- a/drivers/net/ethernet/myricom/myri10ge/myri10ge.c
+++ b/drivers/net/ethernet/myricom/myri10ge/myri10ge.c
@@@ -3812,7 -3606,7 +3812,11 @@@ static int myri10ge_alloc_slices(struc
  		bytes = mgp->max_intr_slots * sizeof(*ss->rx_done.entry);
  		ss->rx_done.entry = dma_alloc_coherent(&pdev->dev, bytes,
  						       &ss->rx_done.bus,
++<<<<<<< HEAD
 +						       GFP_KERNEL | __GFP_ZERO);
++=======
+ 						       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (ss->rx_done.entry == NULL)
  			goto abort;
  		bytes = sizeof(*ss->fw_stats);
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 1b191a368eb3,7d2d4241498f..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -2169,14 -2169,18 +2169,24 @@@ nfp_net_tx_ring_alloc(struct nfp_net_d
  
  	tx_ring->cnt = dp->txd_cnt;
  
++<<<<<<< HEAD
 +	tx_ring->size = sizeof(*tx_ring->txds) * tx_ring->cnt;
 +	tx_ring->txds = dma_zalloc_coherent(dp->dev, tx_ring->size,
 +					    &tx_ring->dma, GFP_KERNEL);
 +	if (!tx_ring->txds)
++=======
+ 	tx_ring->size = array_size(tx_ring->cnt, sizeof(*tx_ring->txds));
+ 	tx_ring->txds = dma_alloc_coherent(dp->dev, tx_ring->size,
+ 					   &tx_ring->dma,
+ 					   GFP_KERNEL | __GFP_NOWARN);
+ 	if (!tx_ring->txds) {
+ 		netdev_warn(dp->netdev, "failed to allocate TX descriptor ring memory, requested descriptor count: %d, consider lowering descriptor count\n",
+ 			    tx_ring->cnt);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		goto err_alloc;
 -	}
  
 -	tx_ring->txbufs = kvcalloc(tx_ring->cnt, sizeof(*tx_ring->txbufs),
 -				   GFP_KERNEL);
 +	sz = sizeof(*tx_ring->txbufs) * tx_ring->cnt;
 +	tx_ring->txbufs = kzalloc(sz, GFP_KERNEL);
  	if (!tx_ring->txbufs)
  		goto err_alloc;
  
@@@ -2311,17 -2317,28 +2321,27 @@@ static void nfp_net_rx_ring_free(struc
  static int
  nfp_net_rx_ring_alloc(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring)
  {
 -	int err;
 -
 -	if (dp->netdev) {
 -		err = xdp_rxq_info_reg(&rx_ring->xdp_rxq, dp->netdev,
 -				       rx_ring->idx);
 -		if (err < 0)
 -			return err;
 -	}
 +	int sz;
  
  	rx_ring->cnt = dp->rxd_cnt;
++<<<<<<< HEAD
 +	rx_ring->size = sizeof(*rx_ring->rxds) * rx_ring->cnt;
 +	rx_ring->rxds = dma_zalloc_coherent(dp->dev, rx_ring->size,
 +					    &rx_ring->dma, GFP_KERNEL);
 +	if (!rx_ring->rxds)
++=======
+ 	rx_ring->size = array_size(rx_ring->cnt, sizeof(*rx_ring->rxds));
+ 	rx_ring->rxds = dma_alloc_coherent(dp->dev, rx_ring->size,
+ 					   &rx_ring->dma,
+ 					   GFP_KERNEL | __GFP_NOWARN);
+ 	if (!rx_ring->rxds) {
+ 		netdev_warn(dp->netdev, "failed to allocate RX descriptor ring memory, requested descriptor count: %d, consider lowering descriptor count\n",
+ 			    rx_ring->cnt);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		goto err_alloc;
 -	}
  
 -	rx_ring->rxbufs = kvcalloc(rx_ring->cnt, sizeof(*rx_ring->rxbufs),
 -				   GFP_KERNEL);
 +	sz = sizeof(*rx_ring->rxbufs) * rx_ring->cnt;
 +	rx_ring->rxbufs = kzalloc(sz, GFP_KERNEL);
  	if (!rx_ring->rxbufs)
  		goto err_alloc;
  
diff --cc drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
index 5a5293af83d7,552d930e3940..000000000000
--- a/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
+++ b/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
@@@ -1468,9 -1439,9 +1468,15 @@@ pch_gbe_alloc_rx_buffers_pool(struct pc
  	bufsz = adapter->rx_buffer_len;
  
  	size = rx_ring->count * bufsz + PCH_GBE_RESERVE_MEMORY;
++<<<<<<< HEAD
 +	rx_ring->rx_buff_pool = dma_alloc_coherent(&pdev->dev, size,
 +						   &rx_ring->rx_buff_pool_logic,
 +						   GFP_KERNEL | __GFP_ZERO);
++=======
+ 	rx_ring->rx_buff_pool =
+ 		dma_alloc_coherent(&pdev->dev, size,
+ 				   &rx_ring->rx_buff_pool_logic, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!rx_ring->rx_buff_pool)
  		return -ENOMEM;
  
@@@ -1772,8 -1756,7 +1778,12 @@@ int pch_gbe_setup_tx_resources(struct p
  	tx_ring->size = tx_ring->count * (int)sizeof(struct pch_gbe_tx_desc);
  
  	tx_ring->desc = dma_alloc_coherent(&pdev->dev, tx_ring->size,
++<<<<<<< HEAD
 +					   &tx_ring->dma,
 +					   GFP_KERNEL | __GFP_ZERO);
++=======
+ 					   &tx_ring->dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!tx_ring->desc) {
  		vfree(tx_ring->buffer_info);
  		return -ENOMEM;
@@@ -1817,8 -1799,7 +1827,12 @@@ int pch_gbe_setup_rx_resources(struct p
  
  	rx_ring->size = rx_ring->count * (int)sizeof(struct pch_gbe_rx_desc);
  	rx_ring->desc =	dma_alloc_coherent(&pdev->dev, rx_ring->size,
++<<<<<<< HEAD
 +					   &rx_ring->dma,
 +					   GFP_KERNEL | __GFP_ZERO);
++=======
+ 						  &rx_ring->dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!rx_ring->desc) {
  		vfree(rx_ring->buffer_info);
  		return -ENOMEM;
diff --cc drivers/net/ethernet/pasemi/pasemi_mac.c
index c8a6ad2e9b98,d21041554507..000000000000
--- a/drivers/net/ethernet/pasemi/pasemi_mac.c
+++ b/drivers/net/ethernet/pasemi/pasemi_mac.c
@@@ -441,8 -403,7 +441,12 @@@ static int pasemi_mac_setup_rx_resource
  
  	ring->buffers = dma_alloc_coherent(&mac->dma_pdev->dev,
  					   RX_RING_SIZE * sizeof(u64),
++<<<<<<< HEAD
 +					   &ring->buf_dma,
 +					   GFP_KERNEL | __GFP_ZERO);
++=======
+ 					   &ring->buf_dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!ring->buffers)
  		goto out_ring_desc;
  
diff --cc drivers/net/ethernet/sgi/meth.c
index 264d1b9b4050,0e1b7e960b98..000000000000
--- a/drivers/net/ethernet/sgi/meth.c
+++ b/drivers/net/ethernet/sgi/meth.c
@@@ -213,8 -212,7 +213,12 @@@ static int meth_init_tx_ring(struct met
  {
  	/* Init TX ring */
  	priv->tx_ring = dma_alloc_coherent(NULL, TX_RING_BUFFER_SIZE,
++<<<<<<< HEAD
 +	                                   &priv->tx_ring_dma,
 +					   GFP_ATOMIC | __GFP_ZERO);
++=======
+ 					   &priv->tx_ring_dma, GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!priv->tx_ring)
  		return -ENOMEM;
  
diff --cc drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 2b0083b9138f,0c4ab3444cc3..000000000000
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@@ -1130,52 -1449,328 +1130,251 @@@ static void dma_free_tx_skbufs(struct s
  {
  	int i;
  
 -	for (i = 0; i < DMA_TX_SIZE; i++)
 -		stmmac_free_tx_buffer(priv, queue, i);
 -}
 +	for (i = 0; i < priv->dma_tx_size; i++) {
 +		if (priv->tx_skbuff[i] != NULL) {
 +			struct dma_desc *p;
 +			if (priv->extend_desc)
 +				p = &((priv->dma_etx + i)->basic);
 +			else
 +				p = priv->dma_tx + i;
  
++<<<<<<< HEAD
 +			if (priv->tx_skbuff_dma[i])
 +				dma_unmap_single(priv->device,
 +						 priv->tx_skbuff_dma[i],
 +						 priv->hw->desc->get_tx_len(p),
 +						 DMA_TO_DEVICE);
 +			dev_kfree_skb_any(priv->tx_skbuff[i]);
 +			priv->tx_skbuff[i] = NULL;
 +			priv->tx_skbuff_dma[i] = 0;
++=======
+ /**
+  * free_dma_rx_desc_resources - free RX dma desc resources
+  * @priv: private structure
+  */
+ static void free_dma_rx_desc_resources(struct stmmac_priv *priv)
+ {
+ 	u32 rx_count = priv->plat->rx_queues_to_use;
+ 	u32 queue;
+ 
+ 	/* Free RX queue resources */
+ 	for (queue = 0; queue < rx_count; queue++) {
+ 		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+ 
+ 		/* Release the DMA RX socket buffers */
+ 		dma_free_rx_skbufs(priv, queue);
+ 
+ 		/* Free DMA regions of consistent memory previously allocated */
+ 		if (!priv->extend_desc)
+ 			dma_free_coherent(priv->device,
+ 					  DMA_RX_SIZE * sizeof(struct dma_desc),
+ 					  rx_q->dma_rx, rx_q->dma_rx_phy);
+ 		else
+ 			dma_free_coherent(priv->device, DMA_RX_SIZE *
+ 					  sizeof(struct dma_extended_desc),
+ 					  rx_q->dma_erx, rx_q->dma_rx_phy);
+ 
+ 		kfree(rx_q->rx_skbuff_dma);
+ 		kfree(rx_q->rx_skbuff);
+ 	}
+ }
+ 
+ /**
+  * free_dma_tx_desc_resources - free TX dma desc resources
+  * @priv: private structure
+  */
+ static void free_dma_tx_desc_resources(struct stmmac_priv *priv)
+ {
+ 	u32 tx_count = priv->plat->tx_queues_to_use;
+ 	u32 queue;
+ 
+ 	/* Free TX queue resources */
+ 	for (queue = 0; queue < tx_count; queue++) {
+ 		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+ 
+ 		/* Release the DMA TX socket buffers */
+ 		dma_free_tx_skbufs(priv, queue);
+ 
+ 		/* Free DMA regions of consistent memory previously allocated */
+ 		if (!priv->extend_desc)
+ 			dma_free_coherent(priv->device,
+ 					  DMA_TX_SIZE * sizeof(struct dma_desc),
+ 					  tx_q->dma_tx, tx_q->dma_tx_phy);
+ 		else
+ 			dma_free_coherent(priv->device, DMA_TX_SIZE *
+ 					  sizeof(struct dma_extended_desc),
+ 					  tx_q->dma_etx, tx_q->dma_tx_phy);
+ 
+ 		kfree(tx_q->tx_skbuff_dma);
+ 		kfree(tx_q->tx_skbuff);
+ 	}
+ }
+ 
+ /**
+  * alloc_dma_rx_desc_resources - alloc RX resources.
+  * @priv: private structure
+  * Description: according to which descriptor can be used (extend or basic)
+  * this function allocates the resources for TX and RX paths. In case of
+  * reception, for example, it pre-allocated the RX socket buffer in order to
+  * allow zero-copy mechanism.
+  */
+ static int alloc_dma_rx_desc_resources(struct stmmac_priv *priv)
+ {
+ 	u32 rx_count = priv->plat->rx_queues_to_use;
+ 	int ret = -ENOMEM;
+ 	u32 queue;
+ 
+ 	/* RX queues buffers and DMA */
+ 	for (queue = 0; queue < rx_count; queue++) {
+ 		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+ 
+ 		rx_q->queue_index = queue;
+ 		rx_q->priv_data = priv;
+ 
+ 		rx_q->rx_skbuff_dma = kmalloc_array(DMA_RX_SIZE,
+ 						    sizeof(dma_addr_t),
+ 						    GFP_KERNEL);
+ 		if (!rx_q->rx_skbuff_dma)
+ 			goto err_dma;
+ 
+ 		rx_q->rx_skbuff = kmalloc_array(DMA_RX_SIZE,
+ 						sizeof(struct sk_buff *),
+ 						GFP_KERNEL);
+ 		if (!rx_q->rx_skbuff)
+ 			goto err_dma;
+ 
+ 		if (priv->extend_desc) {
+ 			rx_q->dma_erx = dma_alloc_coherent(priv->device,
+ 							   DMA_RX_SIZE * sizeof(struct dma_extended_desc),
+ 							   &rx_q->dma_rx_phy,
+ 							   GFP_KERNEL);
+ 			if (!rx_q->dma_erx)
+ 				goto err_dma;
+ 
+ 		} else {
+ 			rx_q->dma_rx = dma_alloc_coherent(priv->device,
+ 							  DMA_RX_SIZE * sizeof(struct dma_desc),
+ 							  &rx_q->dma_rx_phy,
+ 							  GFP_KERNEL);
+ 			if (!rx_q->dma_rx)
+ 				goto err_dma;
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		}
  	}
 -
 -	return 0;
 -
 -err_dma:
 -	free_dma_rx_desc_resources(priv);
 -
 -	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * alloc_dma_tx_desc_resources - alloc TX resources.
+  * @priv: private structure
+  * Description: according to which descriptor can be used (extend or basic)
+  * this function allocates the resources for TX and RX paths. In case of
+  * reception, for example, it pre-allocated the RX socket buffer in order to
+  * allow zero-copy mechanism.
+  */
+ static int alloc_dma_tx_desc_resources(struct stmmac_priv *priv)
+ {
+ 	u32 tx_count = priv->plat->tx_queues_to_use;
+ 	int ret = -ENOMEM;
+ 	u32 queue;
+ 
+ 	/* TX queues buffers and DMA */
+ 	for (queue = 0; queue < tx_count; queue++) {
+ 		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+ 
+ 		tx_q->queue_index = queue;
+ 		tx_q->priv_data = priv;
+ 
+ 		tx_q->tx_skbuff_dma = kmalloc_array(DMA_TX_SIZE,
+ 						    sizeof(*tx_q->tx_skbuff_dma),
+ 						    GFP_KERNEL);
+ 		if (!tx_q->tx_skbuff_dma)
+ 			goto err_dma;
+ 
+ 		tx_q->tx_skbuff = kmalloc_array(DMA_TX_SIZE,
+ 						sizeof(struct sk_buff *),
+ 						GFP_KERNEL);
+ 		if (!tx_q->tx_skbuff)
+ 			goto err_dma;
+ 
+ 		if (priv->extend_desc) {
+ 			tx_q->dma_etx = dma_alloc_coherent(priv->device,
+ 							   DMA_TX_SIZE * sizeof(struct dma_extended_desc),
+ 							   &tx_q->dma_tx_phy,
+ 							   GFP_KERNEL);
+ 			if (!tx_q->dma_etx)
+ 				goto err_dma;
+ 		} else {
+ 			tx_q->dma_tx = dma_alloc_coherent(priv->device,
+ 							  DMA_TX_SIZE * sizeof(struct dma_desc),
+ 							  &tx_q->dma_tx_phy,
+ 							  GFP_KERNEL);
+ 			if (!tx_q->dma_tx)
+ 				goto err_dma;
+ 		}
+ 	}
+ 
+ 	return 0;
+ 
+ err_dma:
+ 	free_dma_tx_desc_resources(priv);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * alloc_dma_desc_resources - alloc TX/RX resources.
+  * @priv: private structure
+  * Description: according to which descriptor can be used (extend or basic)
+  * this function allocates the resources for TX and RX paths. In case of
+  * reception, for example, it pre-allocated the RX socket buffer in order to
+  * allow zero-copy mechanism.
+  */
+ static int alloc_dma_desc_resources(struct stmmac_priv *priv)
+ {
+ 	/* RX Allocation */
+ 	int ret = alloc_dma_rx_desc_resources(priv);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = alloc_dma_tx_desc_resources(priv);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * free_dma_desc_resources - free dma desc resources
+  * @priv: private structure
+  */
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  static void free_dma_desc_resources(struct stmmac_priv *priv)
  {
 -	/* Release the DMA RX socket buffers */
 -	free_dma_rx_desc_resources(priv);
 -
 -	/* Release the DMA TX socket buffers */
 -	free_dma_tx_desc_resources(priv);
 -}
 -
 -/**
 - *  stmmac_mac_enable_rx_queues - Enable MAC rx queues
 - *  @priv: driver private structure
 - *  Description: It is used for enabling the rx queues in the MAC
 - */
 -static void stmmac_mac_enable_rx_queues(struct stmmac_priv *priv)
 -{
 -	u32 rx_queues_count = priv->plat->rx_queues_to_use;
 -	int queue;
 -	u8 mode;
 -
 -	for (queue = 0; queue < rx_queues_count; queue++) {
 -		mode = priv->plat->rx_queues_cfg[queue].mode_to_use;
 -		stmmac_rx_queue_enable(priv, priv->hw, mode, queue);
 +	/* Release the DMA TX/RX socket buffers */
 +	dma_free_rx_skbufs(priv);
 +	dma_free_tx_skbufs(priv);
 +
 +	/* Free DMA regions of consistent memory previously allocated */
 +	if (!priv->extend_desc) {
 +		dma_free_coherent(priv->device,
 +				  priv->dma_tx_size * sizeof(struct dma_desc),
 +				  priv->dma_tx, priv->dma_tx_phy);
 +		dma_free_coherent(priv->device,
 +				  priv->dma_rx_size * sizeof(struct dma_desc),
 +				  priv->dma_rx, priv->dma_rx_phy);
 +	} else {
 +		dma_free_coherent(priv->device, priv->dma_tx_size *
 +				  sizeof(struct dma_extended_desc),
 +				  priv->dma_etx, priv->dma_tx_phy);
 +		dma_free_coherent(priv->device, priv->dma_rx_size *
 +				  sizeof(struct dma_extended_desc),
 +				  priv->dma_erx, priv->dma_rx_phy);
  	}
 -}
 -
 -/**
 - * stmmac_start_rx_dma - start RX DMA channel
 - * @priv: driver private structure
 - * @chan: RX channel index
 - * Description:
 - * This starts a RX DMA channel
 - */
 -static void stmmac_start_rx_dma(struct stmmac_priv *priv, u32 chan)
 -{
 -	netdev_dbg(priv->dev, "DMA RX processes started in channel %d\n", chan);
 -	stmmac_start_rx(priv, priv->ioaddr, chan);
 -}
 -
 -/**
 - * stmmac_start_tx_dma - start TX DMA channel
 - * @priv: driver private structure
 - * @chan: TX channel index
 - * Description:
 - * This starts a TX DMA channel
 - */
 -static void stmmac_start_tx_dma(struct stmmac_priv *priv, u32 chan)
 -{
 -	netdev_dbg(priv->dev, "DMA TX processes started in channel %d\n", chan);
 -	stmmac_start_tx(priv, priv->ioaddr, chan);
 -}
 -
 -/**
 - * stmmac_stop_rx_dma - stop RX DMA channel
 - * @priv: driver private structure
 - * @chan: RX channel index
 - * Description:
 - * This stops a RX DMA channel
 - */
 -static void stmmac_stop_rx_dma(struct stmmac_priv *priv, u32 chan)
 -{
 -	netdev_dbg(priv->dev, "DMA RX processes stopped in channel %d\n", chan);
 -	stmmac_stop_rx(priv, priv->ioaddr, chan);
 -}
 -
 -/**
 - * stmmac_stop_tx_dma - stop TX DMA channel
 - * @priv: driver private structure
 - * @chan: TX channel index
 - * Description:
 - * This stops a TX DMA channel
 - */
 -static void stmmac_stop_tx_dma(struct stmmac_priv *priv, u32 chan)
 -{
 -	netdev_dbg(priv->dev, "DMA TX processes stopped in channel %d\n", chan);
 -	stmmac_stop_tx(priv, priv->ioaddr, chan);
 -}
 -
 -/**
 - * stmmac_start_all_dma - start all RX and TX DMA channels
 - * @priv: driver private structure
 - * Description:
 - * This starts all the RX and TX DMA channels
 - */
 -static void stmmac_start_all_dma(struct stmmac_priv *priv)
 -{
 -	u32 rx_channels_count = priv->plat->rx_queues_to_use;
 -	u32 tx_channels_count = priv->plat->tx_queues_to_use;
 -	u32 chan = 0;
 -
 -	for (chan = 0; chan < rx_channels_count; chan++)
 -		stmmac_start_rx_dma(priv, chan);
 -
 -	for (chan = 0; chan < tx_channels_count; chan++)
 -		stmmac_start_tx_dma(priv, chan);
 -}
 -
 -/**
 - * stmmac_stop_all_dma - stop all RX and TX DMA channels
 - * @priv: driver private structure
 - * Description:
 - * This stops the RX and TX DMA channels
 - */
 -static void stmmac_stop_all_dma(struct stmmac_priv *priv)
 -{
 -	u32 rx_channels_count = priv->plat->rx_queues_to_use;
 -	u32 tx_channels_count = priv->plat->tx_queues_to_use;
 -	u32 chan = 0;
 -
 -	for (chan = 0; chan < rx_channels_count; chan++)
 -		stmmac_stop_rx_dma(priv, chan);
 -
 -	for (chan = 0; chan < tx_channels_count; chan++)
 -		stmmac_stop_tx_dma(priv, chan);
 +	kfree(priv->rx_skbuff_dma);
 +	kfree(priv->rx_skbuff);
 +	kfree(priv->tx_skbuff_dma);
 +	kfree(priv->tx_skbuff);
  }
  
  /**
diff --cc drivers/net/ethernet/tundra/tsi108_eth.c
index 3c69a0460832,37925a1d58de..000000000000
--- a/drivers/net/ethernet/tundra/tsi108_eth.c
+++ b/drivers/net/ethernet/tundra/tsi108_eth.c
@@@ -1308,15 -1311,16 +1308,25 @@@ static int tsi108_open(struct net_devic
  		       data->id, dev->irq, dev->name);
  	}
  
++<<<<<<< HEAD
 +	data->rxring = dma_alloc_coherent(NULL, rxring_size, &data->rxdma,
 +					  GFP_KERNEL | __GFP_ZERO);
 +	if (!data->rxring)
 +		return -ENOMEM;
 +
 +	data->txring = dma_alloc_coherent(NULL, txring_size, &data->txdma,
 +					  GFP_KERNEL | __GFP_ZERO);
++=======
+ 	data->rxring = dma_alloc_coherent(&data->pdev->dev, rxring_size,
+ 					  &data->rxdma, GFP_KERNEL);
+ 	if (!data->rxring)
+ 		return -ENOMEM;
+ 
+ 	data->txring = dma_alloc_coherent(&data->pdev->dev, txring_size,
+ 					  &data->txdma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!data->txring) {
 -		dma_free_coherent(&data->pdev->dev, rxring_size, data->rxring,
 -				    data->rxdma);
 +		pci_free_consistent(0, rxring_size, data->rxring, data->rxdma);
  		return -ENOMEM;
  	}
  
diff --cc drivers/net/ethernet/xilinx/ll_temac_main.c
index 20bd66a6a202,15bb058db392..000000000000
--- a/drivers/net/ethernet/xilinx/ll_temac_main.c
+++ b/drivers/net/ethernet/xilinx/ll_temac_main.c
@@@ -245,13 -245,13 +245,21 @@@ static int temac_dma_bd_init(struct net
  	/* returns a virtual address and a physical address. */
  	lp->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
  					 sizeof(*lp->tx_bd_v) * TX_BD_NUM,
++<<<<<<< HEAD
 +					 &lp->tx_bd_p, GFP_KERNEL | __GFP_ZERO);
++=======
+ 					 &lp->tx_bd_p, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!lp->tx_bd_v)
  		goto out;
  
  	lp->rx_bd_v = dma_alloc_coherent(ndev->dev.parent,
  					 sizeof(*lp->rx_bd_v) * RX_BD_NUM,
++<<<<<<< HEAD
 +					 &lp->rx_bd_p, GFP_KERNEL | __GFP_ZERO);
++=======
+ 					 &lp->rx_bd_p, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!lp->rx_bd_v)
  		goto out;
  
diff --cc drivers/net/ethernet/xilinx/xilinx_axienet_main.c
index 5957277788fe,0789d8af7d72..000000000000
--- a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
@@@ -198,20 -198,16 +198,31 @@@ static int axienet_dma_bd_init(struct n
  	lp->tx_bd_tail = 0;
  	lp->rx_bd_ci = 0;
  
++<<<<<<< HEAD
 +	/*
 +	 * Allocate the Tx and Rx buffer descriptors.
 +	 */
 +	lp->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
 +					 sizeof(*lp->tx_bd_v) * TX_BD_NUM,
 +					 &lp->tx_bd_p,
 +					 GFP_KERNEL | __GFP_ZERO);
++=======
+ 	/* Allocate the Tx and Rx buffer descriptors. */
+ 	lp->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
+ 					 sizeof(*lp->tx_bd_v) * TX_BD_NUM,
+ 					 &lp->tx_bd_p, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!lp->tx_bd_v)
  		goto out;
  
  	lp->rx_bd_v = dma_alloc_coherent(ndev->dev.parent,
  					 sizeof(*lp->rx_bd_v) * RX_BD_NUM,
++<<<<<<< HEAD
 +					 &lp->rx_bd_p,
 +					 GFP_KERNEL | __GFP_ZERO);
++=======
+ 					 &lp->rx_bd_p, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!lp->rx_bd_v)
  		goto out;
  
diff --cc drivers/net/fddi/defxx.c
index 745ea2b14539,38ac8ef41f5f..000000000000
--- a/drivers/net/fddi/defxx.c
+++ b/drivers/net/fddi/defxx.c
@@@ -1070,7 -1141,7 +1070,11 @@@ static int dfx_driver_init(struct net_d
  					(PI_ALIGN_K_DESC_BLK - 1);
  	bp->kmalloced = top_v = dma_alloc_coherent(bp->bus_dev, alloc_size,
  						   &bp->kmalloced_dma,
++<<<<<<< HEAD
 +						   GFP_ATOMIC | __GFP_ZERO);
++=======
+ 						   GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (top_v == NULL)
  		return DFX_K_FAILURE;
  
diff --cc drivers/net/fddi/skfp/skfddi.c
index 1f99975e1467,5d661f60b101..000000000000
--- a/drivers/net/fddi/skfp/skfddi.c
+++ b/drivers/net/fddi/skfp/skfddi.c
@@@ -409,9 -409,10 +409,16 @@@ static  int skfp_driver_init(struct net
  	if (bp->SharedMemSize > 0) {
  		bp->SharedMemSize += 16;	// for descriptor alignment
  
++<<<<<<< HEAD
 +		bp->SharedMemAddr = pci_alloc_consistent(&bp->pdev,
 +							 bp->SharedMemSize,
 +							 &bp->SharedMemDMA);
++=======
+ 		bp->SharedMemAddr = dma_alloc_coherent(&bp->pdev.dev,
+ 						       bp->SharedMemSize,
+ 						       &bp->SharedMemDMA,
+ 						       GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!bp->SharedMemAddr) {
  			printk("could not allocate mem for ");
  			printk("hardware module: %ld byte\n",
diff --cc drivers/net/wireless/b43/dma.c
index f7c70b3a6ea9,b34e51933257..000000000000
--- a/drivers/net/wireless/b43/dma.c
+++ b/drivers/net/wireless/b43/dma.c
@@@ -433,7 -433,7 +433,11 @@@ static int alloc_ringmemory(struct b43_
  
  	ring->descbase = dma_alloc_coherent(ring->dev->dev->dma_dev,
  					    ring_mem_size, &(ring->dmabase),
++<<<<<<< HEAD:drivers/net/wireless/b43/dma.c
 +					    GFP_KERNEL | __GFP_ZERO);
++=======
+ 					    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent()):drivers/net/wireless/broadcom/b43/dma.c
  	if (!ring->descbase)
  		return -ENOMEM;
  
diff --cc drivers/net/wireless/b43legacy/dma.c
index faeafe219c57,2ce1537d983c..000000000000
--- a/drivers/net/wireless/b43legacy/dma.c
+++ b/drivers/net/wireless/b43legacy/dma.c
@@@ -333,8 -333,7 +333,12 @@@ static int alloc_ringmemory(struct b43l
  	/* GFP flags must match the flags in free_ringmemory()! */
  	ring->descbase = dma_alloc_coherent(ring->dev->dev->dma_dev,
  					    B43legacy_DMA_RINGMEMSIZE,
++<<<<<<< HEAD:drivers/net/wireless/b43legacy/dma.c
 +					    &(ring->dmabase),
 +					    GFP_KERNEL | __GFP_ZERO);
++=======
+ 					    &(ring->dmabase), GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent()):drivers/net/wireless/broadcom/b43legacy/dma.c
  	if (!ring->descbase)
  		return -ENOMEM;
  
diff --cc drivers/net/wireless/broadcom/brcm80211/brcmfmac/pcie.c
index e6e9b00b79d7,0f69b3fa296e..000000000000
--- a/drivers/net/wireless/broadcom/brcm80211/brcmfmac/pcie.c
+++ b/drivers/net/wireless/broadcom/brcm80211/brcmfmac/pcie.c
@@@ -1251,9 -1280,11 +1251,17 @@@ static int brcmf_pcie_init_scratchbuffe
  	u64 address;
  	u32 addr;
  
++<<<<<<< HEAD
 +	devinfo->shared.scratch = dma_alloc_coherent(&devinfo->pdev->dev,
 +		BRCMF_DMA_D2H_SCRATCH_BUF_LEN,
 +		&devinfo->shared.scratch_dmahandle, GFP_KERNEL);
++=======
+ 	devinfo->shared.scratch =
+ 		dma_alloc_coherent(&devinfo->pdev->dev,
+ 				   BRCMF_DMA_D2H_SCRATCH_BUF_LEN,
+ 				   &devinfo->shared.scratch_dmahandle,
+ 				   GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!devinfo->shared.scratch)
  		goto fail;
  
@@@ -1268,9 -1297,11 +1276,17 @@@
  	       BRCMF_SHARED_DMA_SCRATCH_LEN_OFFSET;
  	brcmf_pcie_write_tcm32(devinfo, addr, BRCMF_DMA_D2H_SCRATCH_BUF_LEN);
  
++<<<<<<< HEAD
 +	devinfo->shared.ringupd = dma_alloc_coherent(&devinfo->pdev->dev,
 +		BRCMF_DMA_D2H_RINGUPD_BUF_LEN,
 +		&devinfo->shared.ringupd_dmahandle, GFP_KERNEL);
++=======
+ 	devinfo->shared.ringupd =
+ 		dma_alloc_coherent(&devinfo->pdev->dev,
+ 				   BRCMF_DMA_D2H_RINGUPD_BUF_LEN,
+ 				   &devinfo->shared.ringupd_dmahandle,
+ 				   GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!devinfo->shared.ringupd)
  		goto fail;
  
diff --cc drivers/net/wireless/intel/iwlwifi/pcie/rx.c
index a06b6612b658,9e850c25877b..000000000000
--- a/drivers/net/wireless/intel/iwlwifi/pcie/rx.c
+++ b/drivers/net/wireless/intel/iwlwifi/pcie/rx.c
@@@ -607,7 -631,141 +607,145 @@@ void iwl_pcie_rx_allocator_work(struct 
  	iwl_pcie_rx_allocator(trans_pcie->trans);
  }
  
++<<<<<<< HEAD
 +static int iwl_pcie_rx_alloc(struct iwl_trans *trans)
++=======
+ static int iwl_pcie_free_bd_size(struct iwl_trans *trans, bool use_rx_td)
+ {
+ 	struct iwl_rx_transfer_desc *rx_td;
+ 
+ 	if (use_rx_td)
+ 		return sizeof(*rx_td);
+ 	else
+ 		return trans->cfg->mq_rx_supported ? sizeof(__le64) :
+ 			sizeof(__le32);
+ }
+ 
+ static void iwl_pcie_free_rxq_dma(struct iwl_trans *trans,
+ 				  struct iwl_rxq *rxq)
+ {
+ 	struct device *dev = trans->dev;
+ 	bool use_rx_td = (trans->cfg->device_family >=
+ 			  IWL_DEVICE_FAMILY_22560);
+ 	int free_size = iwl_pcie_free_bd_size(trans, use_rx_td);
+ 
+ 	if (rxq->bd)
+ 		dma_free_coherent(trans->dev,
+ 				  free_size * rxq->queue_size,
+ 				  rxq->bd, rxq->bd_dma);
+ 	rxq->bd_dma = 0;
+ 	rxq->bd = NULL;
+ 
+ 	if (rxq->rb_stts)
+ 		dma_free_coherent(trans->dev,
+ 				  use_rx_td ? sizeof(__le16) :
+ 				  sizeof(struct iwl_rb_status),
+ 				  rxq->rb_stts, rxq->rb_stts_dma);
+ 	rxq->rb_stts_dma = 0;
+ 	rxq->rb_stts = NULL;
+ 
+ 	if (rxq->used_bd)
+ 		dma_free_coherent(trans->dev,
+ 				  (use_rx_td ? sizeof(*rxq->cd) :
+ 				   sizeof(__le32)) * rxq->queue_size,
+ 				  rxq->used_bd, rxq->used_bd_dma);
+ 	rxq->used_bd_dma = 0;
+ 	rxq->used_bd = NULL;
+ 
+ 	if (trans->cfg->device_family < IWL_DEVICE_FAMILY_22560)
+ 		return;
+ 
+ 	if (rxq->tr_tail)
+ 		dma_free_coherent(dev, sizeof(__le16),
+ 				  rxq->tr_tail, rxq->tr_tail_dma);
+ 	rxq->tr_tail_dma = 0;
+ 	rxq->tr_tail = NULL;
+ 
+ 	if (rxq->cr_tail)
+ 		dma_free_coherent(dev, sizeof(__le16),
+ 				  rxq->cr_tail, rxq->cr_tail_dma);
+ 	rxq->cr_tail_dma = 0;
+ 	rxq->cr_tail = NULL;
+ }
+ 
+ static int iwl_pcie_alloc_rxq_dma(struct iwl_trans *trans,
+ 				  struct iwl_rxq *rxq)
+ {
+ 	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);
+ 	struct device *dev = trans->dev;
+ 	int i;
+ 	int free_size;
+ 	bool use_rx_td = (trans->cfg->device_family >=
+ 			  IWL_DEVICE_FAMILY_22560);
+ 
+ 	spin_lock_init(&rxq->lock);
+ 	if (trans->cfg->mq_rx_supported)
+ 		rxq->queue_size = MQ_RX_TABLE_SIZE;
+ 	else
+ 		rxq->queue_size = RX_QUEUE_SIZE;
+ 
+ 	free_size = iwl_pcie_free_bd_size(trans, use_rx_td);
+ 
+ 	/*
+ 	 * Allocate the circular buffer of Read Buffer Descriptors
+ 	 * (RBDs)
+ 	 */
+ 	rxq->bd = dma_alloc_coherent(dev, free_size * rxq->queue_size,
+ 				     &rxq->bd_dma, GFP_KERNEL);
+ 	if (!rxq->bd)
+ 		goto err;
+ 
+ 	if (trans->cfg->mq_rx_supported) {
+ 		rxq->used_bd = dma_alloc_coherent(dev,
+ 						  (use_rx_td ? sizeof(*rxq->cd) : sizeof(__le32)) * rxq->queue_size,
+ 						  &rxq->used_bd_dma,
+ 						  GFP_KERNEL);
+ 		if (!rxq->used_bd)
+ 			goto err;
+ 	}
+ 
+ 	/* Allocate the driver's pointer to receive buffer status */
+ 	rxq->rb_stts = dma_alloc_coherent(dev,
+ 					  use_rx_td ? sizeof(__le16) : sizeof(struct iwl_rb_status),
+ 					  &rxq->rb_stts_dma, GFP_KERNEL);
+ 	if (!rxq->rb_stts)
+ 		goto err;
+ 
+ 	if (!use_rx_td)
+ 		return 0;
+ 
+ 	/* Allocate the driver's pointer to TR tail */
+ 	rxq->tr_tail = dma_alloc_coherent(dev, sizeof(__le16),
+ 					  &rxq->tr_tail_dma, GFP_KERNEL);
+ 	if (!rxq->tr_tail)
+ 		goto err;
+ 
+ 	/* Allocate the driver's pointer to CR tail */
+ 	rxq->cr_tail = dma_alloc_coherent(dev, sizeof(__le16),
+ 					  &rxq->cr_tail_dma, GFP_KERNEL);
+ 	if (!rxq->cr_tail)
+ 		goto err;
+ 	/*
+ 	 * W/A 22560 device step Z0 must be non zero bug
+ 	 * TODO: remove this when stop supporting Z0
+ 	 */
+ 	*rxq->cr_tail = cpu_to_le16(500);
+ 
+ 	return 0;
+ 
+ err:
+ 	for (i = 0; i < trans->num_rx_queues; i++) {
+ 		struct iwl_rxq *rxq = &trans_pcie->rxq[i];
+ 
+ 		iwl_pcie_free_rxq_dma(trans, rxq);
+ 	}
+ 	kfree(trans_pcie->rxq);
+ 
+ 	return -ENOMEM;
+ }
+ 
+ int iwl_pcie_rx_alloc(struct iwl_trans *trans)
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  {
  	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);
  	struct iwl_rb_allocator *rba = &trans_pcie->rba;
diff --cc drivers/nvme/host/pci.c
index 35f7a153bd5e,e8d0942c9c92..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -1144,11 -1485,10 +1144,16 @@@ static int nvme_alloc_queue(struct nvme
  	if (dev->ctrl.queue_count > qid)
  		return 0;
  
++<<<<<<< HEAD
 +	nvmeq->cqes = dma_zalloc_coherent(dev->dev, CQ_SIZE(depth),
 +					&nvmeq->cq_dma_addr, GFP_KERNEL);
++=======
+ 	nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(depth),
+ 					 &nvmeq->cq_dma_addr, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!nvmeq->cqes)
  		goto free_nvmeq;
 +	memset((void *)nvmeq->cqes, 0, CQ_SIZE(depth));
  
  	if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
  		goto free_cqdma;
diff --cc drivers/rapidio/devices/tsi721_dma.c
index 502663f5f7c6,7f5d4436f594..000000000000
--- a/drivers/rapidio/devices/tsi721_dma.c
+++ b/drivers/rapidio/devices/tsi721_dma.c
@@@ -67,30 -80,37 +67,40 @@@ static int tsi721_bdma_ch_init(struct t
  	dma_addr_t	bd_phys;
  	dma_addr_t	sts_phys;
  	int		sts_size;
 -#ifdef CONFIG_PCI_MSI
 -	struct tsi721_device *priv = to_tsi721(bdma_chan->dchan.device);
 -#endif
 +	int		bd_num = bdma_chan->bd_num;
  
 -	tsi_debug(DMA, &bdma_chan->dchan.dev->device, "DMAC%d", bdma_chan->id);
 +	dev_dbg(dev, "Init Block DMA Engine, CH%d\n", bdma_chan->id);
  
++<<<<<<< HEAD
 +	/* Allocate space for DMA descriptors */
 +	bd_ptr = dma_zalloc_coherent(dev,
 +				bd_num * sizeof(struct tsi721_dma_desc),
 +				&bd_phys, GFP_KERNEL);
++=======
+ 	/*
+ 	 * Allocate space for DMA descriptors
+ 	 * (add an extra element for link descriptor)
+ 	 */
+ 	bd_ptr = dma_alloc_coherent(dev,
+ 				    (bd_num + 1) * sizeof(struct tsi721_dma_desc),
+ 				    &bd_phys, GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!bd_ptr)
  		return -ENOMEM;
  
  	bdma_chan->bd_phys = bd_phys;
  	bdma_chan->bd_base = bd_ptr;
  
 -	tsi_debug(DMA, &bdma_chan->dchan.dev->device,
 -		  "DMAC%d descriptors @ %p (phys = %pad)",
 -		  bdma_chan->id, bd_ptr, &bd_phys);
 +	dev_dbg(dev, "DMA descriptors @ %p (phys = %llx)\n",
 +		bd_ptr, (unsigned long long)bd_phys);
  
  	/* Allocate space for descriptor status FIFO */
 -	sts_size = ((bd_num + 1) >= TSI721_DMA_MINSTSSZ) ?
 -					(bd_num + 1) : TSI721_DMA_MINSTSSZ;
 +	sts_size = (bd_num >= TSI721_DMA_MINSTSSZ) ?
 +					bd_num : TSI721_DMA_MINSTSSZ;
  	sts_size = roundup_pow_of_two(sts_size);
- 	sts_ptr = dma_zalloc_coherent(dev,
+ 	sts_ptr = dma_alloc_coherent(dev,
  				     sts_size * sizeof(struct tsi721_dma_sts),
 -				     &sts_phys, GFP_ATOMIC);
 +				     &sts_phys, GFP_KERNEL);
  	if (!sts_ptr) {
  		/* Free space allocated for DMA descriptors */
  		dma_free_coherent(dev,
diff --cc drivers/scsi/3w-sas.c
index 61702ac00d42,cd096104bcec..000000000000
--- a/drivers/scsi/3w-sas.c
+++ b/drivers/scsi/3w-sas.c
@@@ -663,7 -646,9 +663,13 @@@ static int twl_allocate_memory(TW_Devic
  	unsigned long *cpu_addr;
  	int retval = 1;
  
++<<<<<<< HEAD
 +	cpu_addr = pci_alloc_consistent(tw_dev->tw_pci_dev, size*TW_Q_LENGTH, &dma_handle);
++=======
+ 	cpu_addr = dma_alloc_coherent(&tw_dev->tw_pci_dev->dev,
+ 				      size * TW_Q_LENGTH, &dma_handle,
+ 				      GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!cpu_addr) {
  		TW_PRINTK(tw_dev->host, TW_DRIVER, 0x5, "Memory allocation failed");
  		goto out;
diff --cc drivers/scsi/a100u2w.c
index 0163457c12bb,66c514310f3c..000000000000
--- a/drivers/scsi/a100u2w.c
+++ b/drivers/scsi/a100u2w.c
@@@ -1125,8 -1123,8 +1125,13 @@@ static int inia100_probe_one(struct pci
  
  	/* Get total memory needed for SCB */
  	sz = ORC_MAXQUEUE * sizeof(struct orc_scb);
++<<<<<<< HEAD
 +	host->scb_virt = pci_alloc_consistent(pdev, sz,
 +			&host->scb_phys);
++=======
+ 	host->scb_virt = dma_alloc_coherent(&pdev->dev, sz, &host->scb_phys,
+ 					    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!host->scb_virt) {
  		printk("inia100: SCB memory allocation error\n");
  		goto out_host_put;
@@@ -1135,8 -1132,8 +1140,13 @@@
  
  	/* Get total memory needed for ESCB */
  	sz = ORC_MAXQUEUE * sizeof(struct orc_extended_scb);
++<<<<<<< HEAD
 +	host->escb_virt = pci_alloc_consistent(pdev, sz,
 +			&host->escb_phys);
++=======
+ 	host->escb_virt = dma_alloc_coherent(&pdev->dev, sz, &host->escb_phys,
+ 					     GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!host->escb_virt) {
  		printk("inia100: ESCB memory allocation error\n");
  		goto out_free_scb_array;
diff --cc drivers/scsi/arcmsr/arcmsr_hba.c
index 278c9fa62067,57c6fa388bf6..000000000000
--- a/drivers/scsi/arcmsr/arcmsr_hba.c
+++ b/drivers/scsi/arcmsr/arcmsr_hba.c
@@@ -433,6 -569,121 +433,117 @@@ static void arcmsr_flush_adapter_cache(
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static bool arcmsr_alloc_io_queue(struct AdapterControlBlock *acb)
+ {
+ 	bool rtn = true;
+ 	void *dma_coherent;
+ 	dma_addr_t dma_coherent_handle;
+ 	struct pci_dev *pdev = acb->pdev;
+ 
+ 	switch (acb->adapter_type) {
+ 	case ACB_ADAPTER_TYPE_B: {
+ 		struct MessageUnit_B *reg;
+ 		acb->roundup_ccbsize = roundup(sizeof(struct MessageUnit_B), 32);
+ 		dma_coherent = dma_alloc_coherent(&pdev->dev,
+ 						  acb->roundup_ccbsize,
+ 						  &dma_coherent_handle,
+ 						  GFP_KERNEL);
+ 		if (!dma_coherent) {
+ 			pr_notice("arcmsr%d: DMA allocation failed\n", acb->host->host_no);
+ 			return false;
+ 		}
+ 		acb->dma_coherent_handle2 = dma_coherent_handle;
+ 		acb->dma_coherent2 = dma_coherent;
+ 		reg = (struct MessageUnit_B *)dma_coherent;
+ 		acb->pmuB = reg;
+ 		if (acb->pdev->device == PCI_DEVICE_ID_ARECA_1203) {
+ 			reg->drv2iop_doorbell = MEM_BASE0(ARCMSR_DRV2IOP_DOORBELL_1203);
+ 			reg->drv2iop_doorbell_mask = MEM_BASE0(ARCMSR_DRV2IOP_DOORBELL_MASK_1203);
+ 			reg->iop2drv_doorbell = MEM_BASE0(ARCMSR_IOP2DRV_DOORBELL_1203);
+ 			reg->iop2drv_doorbell_mask = MEM_BASE0(ARCMSR_IOP2DRV_DOORBELL_MASK_1203);
+ 		} else {
+ 			reg->drv2iop_doorbell = MEM_BASE0(ARCMSR_DRV2IOP_DOORBELL);
+ 			reg->drv2iop_doorbell_mask = MEM_BASE0(ARCMSR_DRV2IOP_DOORBELL_MASK);
+ 			reg->iop2drv_doorbell = MEM_BASE0(ARCMSR_IOP2DRV_DOORBELL);
+ 			reg->iop2drv_doorbell_mask = MEM_BASE0(ARCMSR_IOP2DRV_DOORBELL_MASK);
+ 		}
+ 		reg->message_wbuffer = MEM_BASE1(ARCMSR_MESSAGE_WBUFFER);
+ 		reg->message_rbuffer = MEM_BASE1(ARCMSR_MESSAGE_RBUFFER);
+ 		reg->message_rwbuffer = MEM_BASE1(ARCMSR_MESSAGE_RWBUFFER);
+ 		}
+ 		break;
+ 	case ACB_ADAPTER_TYPE_D: {
+ 		struct MessageUnit_D *reg;
+ 
+ 		acb->roundup_ccbsize = roundup(sizeof(struct MessageUnit_D), 32);
+ 		dma_coherent = dma_alloc_coherent(&pdev->dev,
+ 						  acb->roundup_ccbsize,
+ 						  &dma_coherent_handle,
+ 						  GFP_KERNEL);
+ 		if (!dma_coherent) {
+ 			pr_notice("arcmsr%d: DMA allocation failed\n", acb->host->host_no);
+ 			return false;
+ 		}
+ 		acb->dma_coherent_handle2 = dma_coherent_handle;
+ 		acb->dma_coherent2 = dma_coherent;
+ 		reg = (struct MessageUnit_D *)dma_coherent;
+ 		acb->pmuD = reg;
+ 		reg->chip_id = MEM_BASE0(ARCMSR_ARC1214_CHIP_ID);
+ 		reg->cpu_mem_config = MEM_BASE0(ARCMSR_ARC1214_CPU_MEMORY_CONFIGURATION);
+ 		reg->i2o_host_interrupt_mask = MEM_BASE0(ARCMSR_ARC1214_I2_HOST_INTERRUPT_MASK);
+ 		reg->sample_at_reset = MEM_BASE0(ARCMSR_ARC1214_SAMPLE_RESET);
+ 		reg->reset_request = MEM_BASE0(ARCMSR_ARC1214_RESET_REQUEST);
+ 		reg->host_int_status = MEM_BASE0(ARCMSR_ARC1214_MAIN_INTERRUPT_STATUS);
+ 		reg->pcief0_int_enable = MEM_BASE0(ARCMSR_ARC1214_PCIE_F0_INTERRUPT_ENABLE);
+ 		reg->inbound_msgaddr0 = MEM_BASE0(ARCMSR_ARC1214_INBOUND_MESSAGE0);
+ 		reg->inbound_msgaddr1 = MEM_BASE0(ARCMSR_ARC1214_INBOUND_MESSAGE1);
+ 		reg->outbound_msgaddr0 = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_MESSAGE0);
+ 		reg->outbound_msgaddr1 = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_MESSAGE1);
+ 		reg->inbound_doorbell = MEM_BASE0(ARCMSR_ARC1214_INBOUND_DOORBELL);
+ 		reg->outbound_doorbell = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_DOORBELL);
+ 		reg->outbound_doorbell_enable = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_DOORBELL_ENABLE);
+ 		reg->inboundlist_base_low = MEM_BASE0(ARCMSR_ARC1214_INBOUND_LIST_BASE_LOW);
+ 		reg->inboundlist_base_high = MEM_BASE0(ARCMSR_ARC1214_INBOUND_LIST_BASE_HIGH);
+ 		reg->inboundlist_write_pointer = MEM_BASE0(ARCMSR_ARC1214_INBOUND_LIST_WRITE_POINTER);
+ 		reg->outboundlist_base_low = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_LIST_BASE_LOW);
+ 		reg->outboundlist_base_high = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_LIST_BASE_HIGH);
+ 		reg->outboundlist_copy_pointer = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_LIST_COPY_POINTER);
+ 		reg->outboundlist_read_pointer = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_LIST_READ_POINTER);
+ 		reg->outboundlist_interrupt_cause = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_INTERRUPT_CAUSE);
+ 		reg->outboundlist_interrupt_enable = MEM_BASE0(ARCMSR_ARC1214_OUTBOUND_INTERRUPT_ENABLE);
+ 		reg->message_wbuffer = MEM_BASE0(ARCMSR_ARC1214_MESSAGE_WBUFFER);
+ 		reg->message_rbuffer = MEM_BASE0(ARCMSR_ARC1214_MESSAGE_RBUFFER);
+ 		reg->msgcode_rwbuffer = MEM_BASE0(ARCMSR_ARC1214_MESSAGE_RWBUFFER);
+ 		}
+ 		break;
+ 	case ACB_ADAPTER_TYPE_E: {
+ 		uint32_t completeQ_size;
+ 		completeQ_size = sizeof(struct deliver_completeQ) * ARCMSR_MAX_HBE_DONEQUEUE + 128;
+ 		acb->roundup_ccbsize = roundup(completeQ_size, 32);
+ 		dma_coherent = dma_alloc_coherent(&pdev->dev,
+ 						  acb->roundup_ccbsize,
+ 						  &dma_coherent_handle,
+ 						  GFP_KERNEL);
+ 		if (!dma_coherent){
+ 			pr_notice("arcmsr%d: DMA allocation failed\n", acb->host->host_no);
+ 			return false;
+ 		}
+ 		acb->dma_coherent_handle2 = dma_coherent_handle;
+ 		acb->dma_coherent2 = dma_coherent;
+ 		acb->pCompletionQ = dma_coherent;
+ 		acb->completionQ_entry = acb->roundup_ccbsize / sizeof(struct deliver_completeQ);
+ 		acb->doneq_index = 0;
+ 		}
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	return rtn;
+ }
+ 
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  static int arcmsr_alloc_ccb_pool(struct AdapterControlBlock *acb)
  {
  	struct pci_dev *pdev = acb->pdev;
diff --cc drivers/scsi/be2iscsi/be_main.c
index 8451c95ff857,74e260027c7d..000000000000
--- a/drivers/scsi/be2iscsi/be_main.c
+++ b/drivers/scsi/be2iscsi/be_main.c
@@@ -3328,10 -3321,10 +3328,15 @@@ static int be_queue_alloc(struct beiscs
  	q->len = len;
  	q->entry_size = entry_size;
  	mem->size = len * entry_size;
++<<<<<<< HEAD
 +	mem->va = pci_alloc_consistent(phba->pcidev, mem->size, &mem->dma);
++=======
+ 	mem->va = dma_alloc_coherent(&phba->pcidev->dev, mem->size, &mem->dma,
+ 				     GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!mem->va)
  		return -ENOMEM;
 +	memset(mem->va, 0, mem->size);
  	return 0;
  }
  
diff --cc drivers/scsi/be2iscsi/be_mgmt.c
index 19956757020b,d4febaadfaa3..000000000000
--- a/drivers/scsi/be2iscsi/be_mgmt.c
+++ b/drivers/scsi/be2iscsi/be_mgmt.c
@@@ -282,7 -293,8 +282,12 @@@ static int beiscsi_prep_nemb_cmd(struc
  				 struct be_dma_mem *cmd,
  				 u8 subsystem, u8 opcode, u32 size)
  {
++<<<<<<< HEAD
 +	cmd->va = pci_alloc_consistent(phba->ctrl.pdev, size, &cmd->dma);
++=======
+ 	cmd->va = dma_alloc_coherent(&phba->ctrl.pdev->dev, size, &cmd->dma,
+ 				     GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!cmd->va) {
  		beiscsi_log(phba, KERN_ERR, BEISCSI_LOG_CONFIG,
  			    "BG_%d : Failed to allocate memory for if info\n");
@@@ -1497,9 -1510,9 +1502,15 @@@ int beiscsi_mgmt_invalidate_icds(struc
  		return -EINVAL;
  
  	nonemb_cmd.size = sizeof(union be_invldt_cmds_params);
++<<<<<<< HEAD
 +	nonemb_cmd.va = pci_zalloc_consistent(phba->ctrl.pdev,
 +					      nonemb_cmd.size,
 +					      &nonemb_cmd.dma);
++=======
+ 	nonemb_cmd.va = dma_alloc_coherent(&phba->ctrl.pdev->dev,
+ 					   nonemb_cmd.size, &nonemb_cmd.dma,
+ 					   GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!nonemb_cmd.va) {
  		beiscsi_log(phba, KERN_ERR, BEISCSI_LOG_EH,
  			    "BM_%d : invldt_cmds_params alloc failed\n");
diff --cc drivers/scsi/bfa/bfad_bsg.c
index daad88226ea4,d8e6d7480f35..000000000000
--- a/drivers/scsi/bfa/bfad_bsg.c
+++ b/drivers/scsi/bfa/bfad_bsg.c
@@@ -3269,8 -3264,9 +3269,14 @@@ bfad_fcxp_map_sg(struct bfad_s *bfad, v
  	/* Allocate dma coherent memory */
  	buf_info = buf_base;
  	buf_info->size = payload_len;
++<<<<<<< HEAD
 +	buf_info->virt = dma_alloc_coherent(&bfad->pcidev->dev, buf_info->size,
 +					&buf_info->phys, GFP_KERNEL);
++=======
+ 	buf_info->virt = dma_alloc_coherent(&bfad->pcidev->dev,
+ 					    buf_info->size, &buf_info->phys,
+ 					    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!buf_info->virt)
  		goto out_free_mem;
  
diff --cc drivers/scsi/bnx2fc/bnx2fc_hwi.c
index 26de61d65a4d,039328d9ef13..000000000000
--- a/drivers/scsi/bnx2fc/bnx2fc_hwi.c
+++ b/drivers/scsi/bnx2fc/bnx2fc_hwi.c
@@@ -2033,21 -2031,17 +2033,32 @@@ static int bnx2fc_allocate_hash_table(s
  	}
  
  	for (i = 0; i < segment_count; ++i) {
++<<<<<<< HEAD
 +		hba->hash_tbl_segments[i] =
 +			dma_alloc_coherent(&hba->pcidev->dev,
 +					   BNX2FC_HASH_TBL_CHUNK_SIZE,
 +					   &dma_segment_array[i],
 +					   GFP_KERNEL);
++=======
+ 		hba->hash_tbl_segments[i] = dma_alloc_coherent(&hba->pcidev->dev,
+ 							       BNX2FC_HASH_TBL_CHUNK_SIZE,
+ 							       &dma_segment_array[i],
+ 							       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!hba->hash_tbl_segments[i]) {
  			printk(KERN_ERR PFX "hash segment alloc failed\n");
  			goto cleanup_dma;
  		}
 +		memset(hba->hash_tbl_segments[i], 0,
 +		       BNX2FC_HASH_TBL_CHUNK_SIZE);
  	}
  
++<<<<<<< HEAD
 +	hba->hash_tbl_pbl = dma_alloc_coherent(&hba->pcidev->dev,
 +					       PAGE_SIZE,
++=======
+ 	hba->hash_tbl_pbl = dma_alloc_coherent(&hba->pcidev->dev, PAGE_SIZE,
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  					       &hba->hash_tbl_pbl_dma,
  					       GFP_KERNEL);
  	if (!hba->hash_tbl_pbl) {
@@@ -2148,8 -2139,7 +2159,12 @@@ int bnx2fc_setup_fw_resc(struct bnx2fc_
  		return -ENOMEM;
  	}
  
++<<<<<<< HEAD
 +	hba->stats_buffer = dma_alloc_coherent(&hba->pcidev->dev,
 +					       PAGE_SIZE,
++=======
+ 	hba->stats_buffer = dma_alloc_coherent(&hba->pcidev->dev, PAGE_SIZE,
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  					       &hba->stats_buf_dma,
  					       GFP_KERNEL);
  	if (!hba->stats_buffer) {
diff --cc drivers/scsi/bnx2fc/bnx2fc_tgt.c
index ac221deceec3,d735e87e416a..000000000000
--- a/drivers/scsi/bnx2fc/bnx2fc_tgt.c
+++ b/drivers/scsi/bnx2fc/bnx2fc_tgt.c
@@@ -702,7 -699,7 +702,11 @@@ static int bnx2fc_alloc_session_resc(st
  			   CNIC_PAGE_MASK;
  
  	tgt->rq = dma_alloc_coherent(&hba->pcidev->dev, tgt->rq_mem_size,
++<<<<<<< HEAD
 +					&tgt->rq_dma, GFP_KERNEL);
++=======
+ 				     &tgt->rq_dma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!tgt->rq) {
  		printk(KERN_ERR PFX "unable to allocate RQ memory %d\n",
  			tgt->rq_mem_size);
@@@ -740,8 -735,9 +744,14 @@@
  	tgt->xferq_mem_size = (tgt->xferq_mem_size + (CNIC_PAGE_SIZE - 1)) &
  			       CNIC_PAGE_MASK;
  
++<<<<<<< HEAD
 +	tgt->xferq = dma_alloc_coherent(&hba->pcidev->dev, tgt->xferq_mem_size,
 +					&tgt->xferq_dma, GFP_KERNEL);
++=======
+ 	tgt->xferq = dma_alloc_coherent(&hba->pcidev->dev,
+ 					tgt->xferq_mem_size, &tgt->xferq_dma,
+ 					GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!tgt->xferq) {
  		printk(KERN_ERR PFX "unable to allocate XFERQ %d\n",
  			tgt->xferq_mem_size);
@@@ -754,8 -749,9 +764,14 @@@
  	tgt->confq_mem_size = (tgt->confq_mem_size + (CNIC_PAGE_SIZE - 1)) &
  			       CNIC_PAGE_MASK;
  
++<<<<<<< HEAD
 +	tgt->confq = dma_alloc_coherent(&hba->pcidev->dev, tgt->confq_mem_size,
 +					&tgt->confq_dma, GFP_KERNEL);
++=======
+ 	tgt->confq = dma_alloc_coherent(&hba->pcidev->dev,
+ 					tgt->confq_mem_size, &tgt->confq_dma,
+ 					GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!tgt->confq) {
  		printk(KERN_ERR PFX "unable to allocate CONFQ %d\n",
  			tgt->confq_mem_size);
diff --cc drivers/scsi/csiostor/csio_wr.c
index c44a1af23fd2,66bbd21819ae..000000000000
--- a/drivers/scsi/csiostor/csio_wr.c
+++ b/drivers/scsi/csiostor/csio_wr.c
@@@ -233,7 -233,8 +233,12 @@@ csio_wr_alloc_q(struct csio_hw *hw, uin
  
  	q = wrm->q_arr[free_idx];
  
++<<<<<<< HEAD
 +	q->vstart = pci_alloc_consistent(hw->pdev, qsz, &q->pstart);
++=======
+ 	q->vstart = dma_alloc_coherent(&hw->pdev->dev, qsz, &q->pstart,
+ 				       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!q->vstart) {
  		csio_err(hw,
  			 "Failed to allocate DMA memory for "
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index 709fa50c3365,12fd74761ae0..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -6172,6 -6201,290 +6172,293 @@@ lpfc_set_features(struct lpfc_hba *phba
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_ras_stop_fwlog: Disable FW logging by the adapter
+  * @phba: Pointer to HBA context object.
+  *
+  * Disable FW logging into host memory on the adapter. To
+  * be done before reading logs from the host memory.
+  **/
+ void
+ lpfc_ras_stop_fwlog(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;
+ 
+ 	ras_fwlog->ras_active = false;
+ 
+ 	/* Disable FW logging to host memory */
+ 	writel(LPFC_CTL_PDEV_CTL_DDL_RAS,
+ 	       phba->sli4_hba.conf_regs_memmap_p + LPFC_CTL_PDEV_CTL_OFFSET);
+ }
+ 
+ /**
+  * lpfc_sli4_ras_dma_free - Free memory allocated for FW logging.
+  * @phba: Pointer to HBA context object.
+  *
+  * This function is called to free memory allocated for RAS FW logging
+  * support in the driver.
+  **/
+ void
+ lpfc_sli4_ras_dma_free(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;
+ 	struct lpfc_dmabuf *dmabuf, *next;
+ 
+ 	if (!list_empty(&ras_fwlog->fwlog_buff_list)) {
+ 		list_for_each_entry_safe(dmabuf, next,
+ 				    &ras_fwlog->fwlog_buff_list,
+ 				    list) {
+ 			list_del(&dmabuf->list);
+ 			dma_free_coherent(&phba->pcidev->dev,
+ 					  LPFC_RAS_MAX_ENTRY_SIZE,
+ 					  dmabuf->virt, dmabuf->phys);
+ 			kfree(dmabuf);
+ 		}
+ 	}
+ 
+ 	if (ras_fwlog->lwpd.virt) {
+ 		dma_free_coherent(&phba->pcidev->dev,
+ 				  sizeof(uint32_t) * 2,
+ 				  ras_fwlog->lwpd.virt,
+ 				  ras_fwlog->lwpd.phys);
+ 		ras_fwlog->lwpd.virt = NULL;
+ 	}
+ 
+ 	ras_fwlog->ras_active = false;
+ }
+ 
+ /**
+  * lpfc_sli4_ras_dma_alloc: Allocate memory for FW support
+  * @phba: Pointer to HBA context object.
+  * @fwlog_buff_count: Count of buffers to be created.
+  *
+  * This routine DMA memory for Log Write Position Data[LPWD] and buffer
+  * to update FW log is posted to the adapter.
+  * Buffer count is calculated based on module param ras_fwlog_buffsize
+  * Size of each buffer posted to FW is 64K.
+  **/
+ 
+ static int
+ lpfc_sli4_ras_dma_alloc(struct lpfc_hba *phba,
+ 			uint32_t fwlog_buff_count)
+ {
+ 	struct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	int rc = 0, i = 0;
+ 
+ 	/* Initialize List */
+ 	INIT_LIST_HEAD(&ras_fwlog->fwlog_buff_list);
+ 
+ 	/* Allocate memory for the LWPD */
+ 	ras_fwlog->lwpd.virt = dma_alloc_coherent(&phba->pcidev->dev,
+ 					    sizeof(uint32_t) * 2,
+ 					    &ras_fwlog->lwpd.phys,
+ 					    GFP_KERNEL);
+ 	if (!ras_fwlog->lwpd.virt) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"6185 LWPD Memory Alloc Failed\n");
+ 
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ras_fwlog->fw_buffcount = fwlog_buff_count;
+ 	for (i = 0; i < ras_fwlog->fw_buffcount; i++) {
+ 		dmabuf = kzalloc(sizeof(struct lpfc_dmabuf),
+ 				 GFP_KERNEL);
+ 		if (!dmabuf) {
+ 			rc = -ENOMEM;
+ 			lpfc_printf_log(phba, KERN_WARNING, LOG_INIT,
+ 					"6186 Memory Alloc failed FW logging");
+ 			goto free_mem;
+ 		}
+ 
+ 		dmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev,
+ 						  LPFC_RAS_MAX_ENTRY_SIZE,
+ 						  &dmabuf->phys, GFP_KERNEL);
+ 		if (!dmabuf->virt) {
+ 			kfree(dmabuf);
+ 			rc = -ENOMEM;
+ 			lpfc_printf_log(phba, KERN_WARNING, LOG_INIT,
+ 					"6187 DMA Alloc Failed FW logging");
+ 			goto free_mem;
+ 		}
+ 		dmabuf->buffer_tag = i;
+ 		list_add_tail(&dmabuf->list, &ras_fwlog->fwlog_buff_list);
+ 	}
+ 
+ free_mem:
+ 	if (rc)
+ 		lpfc_sli4_ras_dma_free(phba);
+ 
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_ras_mbox_cmpl: Completion handler for RAS MBX command
+  * @phba: pointer to lpfc hba data structure.
+  * @pmboxq: pointer to the driver internal queue element for mailbox command.
+  *
+  * Completion handler for driver's RAS MBX command to the device.
+  **/
+ static void
+ lpfc_sli4_ras_mbox_cmpl(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb)
+ {
+ 	MAILBOX_t *mb;
+ 	union lpfc_sli4_cfg_shdr *shdr;
+ 	uint32_t shdr_status, shdr_add_status;
+ 	struct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;
+ 
+ 	mb = &pmb->u.mb;
+ 
+ 	shdr = (union lpfc_sli4_cfg_shdr *)
+ 		&pmb->u.mqe.un.ras_fwlog.header.cfg_shdr;
+ 	shdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);
+ 	shdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);
+ 
+ 	if (mb->mbxStatus != MBX_SUCCESS || shdr_status) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX,
+ 				"6188 FW LOG mailbox "
+ 				"completed with status x%x add_status x%x,"
+ 				" mbx status x%x\n",
+ 				shdr_status, shdr_add_status, mb->mbxStatus);
+ 
+ 		ras_fwlog->ras_hwsupport = false;
+ 		goto disable_ras;
+ 	}
+ 
+ 	ras_fwlog->ras_active = true;
+ 	mempool_free(pmb, phba->mbox_mem_pool);
+ 
+ 	return;
+ 
+ disable_ras:
+ 	/* Free RAS DMA memory */
+ 	lpfc_sli4_ras_dma_free(phba);
+ 	mempool_free(pmb, phba->mbox_mem_pool);
+ }
+ 
+ /**
+  * lpfc_sli4_ras_fwlog_init: Initialize memory and post RAS MBX command
+  * @phba: pointer to lpfc hba data structure.
+  * @fwlog_level: Logging verbosity level.
+  * @fwlog_enable: Enable/Disable logging.
+  *
+  * Initialize memory and post mailbox command to enable FW logging in host
+  * memory.
+  **/
+ int
+ lpfc_sli4_ras_fwlog_init(struct lpfc_hba *phba,
+ 			 uint32_t fwlog_level,
+ 			 uint32_t fwlog_enable)
+ {
+ 	struct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;
+ 	struct lpfc_mbx_set_ras_fwlog *mbx_fwlog = NULL;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	LPFC_MBOXQ_t *mbox;
+ 	uint32_t len = 0, fwlog_buffsize, fwlog_entry_count;
+ 	int rc = 0;
+ 
+ 	fwlog_buffsize = (LPFC_RAS_MIN_BUFF_POST_SIZE *
+ 			  phba->cfg_ras_fwlog_buffsize);
+ 	fwlog_entry_count = (fwlog_buffsize/LPFC_RAS_MAX_ENTRY_SIZE);
+ 
+ 	/*
+ 	 * If re-enabling FW logging support use earlier allocated
+ 	 * DMA buffers while posting MBX command.
+ 	 **/
+ 	if (!ras_fwlog->lwpd.virt) {
+ 		rc = lpfc_sli4_ras_dma_alloc(phba, fwlog_entry_count);
+ 		if (rc) {
+ 			lpfc_printf_log(phba, KERN_WARNING, LOG_INIT,
+ 					"6189 FW Log Memory Allocation Failed");
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	/* Setup Mailbox command */
+ 	mbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);
+ 	if (!mbox) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"6190 RAS MBX Alloc Failed");
+ 		rc = -ENOMEM;
+ 		goto mem_free;
+ 	}
+ 
+ 	ras_fwlog->fw_loglevel = fwlog_level;
+ 	len = (sizeof(struct lpfc_mbx_set_ras_fwlog) -
+ 		sizeof(struct lpfc_sli4_cfg_mhdr));
+ 
+ 	lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_LOWLEVEL,
+ 			 LPFC_MBOX_OPCODE_SET_DIAG_LOG_OPTION,
+ 			 len, LPFC_SLI4_MBX_EMBED);
+ 
+ 	mbx_fwlog = (struct lpfc_mbx_set_ras_fwlog *)&mbox->u.mqe.un.ras_fwlog;
+ 	bf_set(lpfc_fwlog_enable, &mbx_fwlog->u.request,
+ 	       fwlog_enable);
+ 	bf_set(lpfc_fwlog_loglvl, &mbx_fwlog->u.request,
+ 	       ras_fwlog->fw_loglevel);
+ 	bf_set(lpfc_fwlog_buffcnt, &mbx_fwlog->u.request,
+ 	       ras_fwlog->fw_buffcount);
+ 	bf_set(lpfc_fwlog_buffsz, &mbx_fwlog->u.request,
+ 	       LPFC_RAS_MAX_ENTRY_SIZE/SLI4_PAGE_SIZE);
+ 
+ 	/* Update DMA buffer address */
+ 	list_for_each_entry(dmabuf, &ras_fwlog->fwlog_buff_list, list) {
+ 		memset(dmabuf->virt, 0, LPFC_RAS_MAX_ENTRY_SIZE);
+ 
+ 		mbx_fwlog->u.request.buff_fwlog[dmabuf->buffer_tag].addr_lo =
+ 			putPaddrLow(dmabuf->phys);
+ 
+ 		mbx_fwlog->u.request.buff_fwlog[dmabuf->buffer_tag].addr_hi =
+ 			putPaddrHigh(dmabuf->phys);
+ 	}
+ 
+ 	/* Update LPWD address */
+ 	mbx_fwlog->u.request.lwpd.addr_lo = putPaddrLow(ras_fwlog->lwpd.phys);
+ 	mbx_fwlog->u.request.lwpd.addr_hi = putPaddrHigh(ras_fwlog->lwpd.phys);
+ 
+ 	mbox->vport = phba->pport;
+ 	mbox->mbox_cmpl = lpfc_sli4_ras_mbox_cmpl;
+ 
+ 	rc = lpfc_sli_issue_mbox(phba, mbox, MBX_NOWAIT);
+ 
+ 	if (rc == MBX_NOT_FINISHED) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"6191 FW-Log Mailbox failed. "
+ 				"status %d mbxStatus : x%x", rc,
+ 				bf_get(lpfc_mqe_status, &mbox->u.mqe));
+ 		mempool_free(mbox, phba->mbox_mem_pool);
+ 		rc = -EIO;
+ 		goto mem_free;
+ 	} else
+ 		rc = 0;
+ mem_free:
+ 	if (rc)
+ 		lpfc_sli4_ras_dma_free(phba);
+ 
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_ras_setup - Check if RAS supported on the adapter
+  * @phba: Pointer to HBA context object.
+  *
+  * Check if RAS is supported on the adapter and initialize it.
+  **/
+ void
+ lpfc_sli4_ras_setup(struct lpfc_hba *phba)
+ {
+ 	/* Check RAS FW Log needs to be enabled or not */
+ 	if (lpfc_check_fwlog_support(phba))
+ 		return;
+ 
+ 	lpfc_sli4_ras_fwlog_init(phba, phba->cfg_ras_fwlog_level,
+ 				 LPFC_RAS_ENABLE_LOGGING);
+ }
+ 
+ /**
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
   * lpfc_sli4_alloc_resource_identifiers - Allocate all SLI4 resource extents.
   * @phba: Pointer to HBA context object.
   *
diff --cc drivers/scsi/megaraid/megaraid_mbox.c
index 3316d8031e82,f112458023ff..000000000000
--- a/drivers/scsi/megaraid/megaraid_mbox.c
+++ b/drivers/scsi/megaraid/megaraid_mbox.c
@@@ -1004,8 -967,10 +1004,15 @@@ megaraid_alloc_cmd_packets(adapter_t *a
  	 * Allocate the common 16-byte aligned memory for the handshake
  	 * mailbox.
  	 */
++<<<<<<< HEAD
 +	raid_dev->una_mbox64 = pci_alloc_consistent(adapter->pdev,
 +			sizeof(mbox64_t), &raid_dev->una_mbox64_dma);
++=======
+ 	raid_dev->una_mbox64 = dma_alloc_coherent(&adapter->pdev->dev,
+ 						  sizeof(mbox64_t),
+ 						  &raid_dev->una_mbox64_dma,
+ 						  GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
  	if (!raid_dev->una_mbox64) {
  		con_log(CL_ANN, (KERN_WARNING
@@@ -1032,8 -996,8 +1039,13 @@@
  			align;
  
  	// Allocate memory for commands issued internally
++<<<<<<< HEAD
 +	adapter->ibuf = pci_alloc_consistent(pdev, MBOX_IBUF_SIZE,
 +				&adapter->ibuf_dma_h);
++=======
+ 	adapter->ibuf = dma_alloc_coherent(&pdev->dev, MBOX_IBUF_SIZE,
+ 					   &adapter->ibuf_dma_h, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!adapter->ibuf) {
  
  		con_log(CL_ANN, (KERN_WARNING
@@@ -2978,9 -2898,8 +2990,14 @@@ megaraid_mbox_product_info(adapter_t *a
  	 * Issue an ENQUIRY3 command to find out certain adapter parameters,
  	 * e.g., max channels, max commands etc.
  	 */
++<<<<<<< HEAD
 +	pinfo = pci_alloc_consistent(adapter->pdev, sizeof(mraid_pinfo_t),
 +			&pinfo_dma_h);
 +
++=======
+ 	pinfo = dma_alloc_coherent(&adapter->pdev->dev, sizeof(mraid_pinfo_t),
+ 				   &pinfo_dma_h, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (pinfo == NULL) {
  		con_log(CL_ANN, (KERN_WARNING
  			"megaraid: out of memory, %s %d\n", __func__,
diff --cc drivers/scsi/mesh.c
index 275503e62554,c9dc7740e9e7..000000000000
--- a/drivers/scsi/mesh.c
+++ b/drivers/scsi/mesh.c
@@@ -1915,9 -1915,9 +1915,15 @@@ static int mesh_probe(struct macio_dev 
  	/* We use the PCI APIs for now until the generic one gets fixed
  	 * enough or until we get some macio-specific versions
  	 */
++<<<<<<< HEAD
 +	dma_cmd_space = pci_alloc_consistent(macio_get_pci_dev(mdev),
 +					     ms->dma_cmd_size,
 +					     &dma_cmd_bus);
++=======
+ 	dma_cmd_space = dma_alloc_coherent(&macio_get_pci_dev(mdev)->dev,
+ 					   ms->dma_cmd_size, &dma_cmd_bus,
+ 					   GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (dma_cmd_space == NULL) {
  		printk(KERN_ERR "mesh: can't allocate DMA table\n");
  		goto out_unmap;
diff --cc drivers/scsi/mvumi.c
index bd9c13b61984,36f64205ecfa..000000000000
--- a/drivers/scsi/mvumi.c
+++ b/drivers/scsi/mvumi.c
@@@ -142,8 -143,9 +142,14 @@@ static struct mvumi_res *mvumi_alloc_me
  
  	case RESOURCE_UNCACHED_MEMORY:
  		size = round_up(size, 8);
++<<<<<<< HEAD
 +		res->virt_addr = pci_alloc_consistent(mhba->pdev, size,
 +							&res->bus_addr);
++=======
+ 		res->virt_addr = dma_alloc_coherent(&mhba->pdev->dev, size,
+ 						    &res->bus_addr,
+ 						    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  		if (!res->virt_addr) {
  			dev_err(&mhba->pdev->dev,
  					"unable to allocate consistent mem,"
@@@ -258,7 -247,8 +264,12 @@@ static int mvumi_internal_cmd_sgl(struc
  	if (size == 0)
  		return 0;
  
++<<<<<<< HEAD
 +	virt_addr = pci_alloc_consistent(mhba->pdev, size, &phy_addr);
++=======
+ 	virt_addr = dma_alloc_coherent(&mhba->pdev->dev, size, &phy_addr,
+ 				       GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!virt_addr)
  		return -1;
  
diff --cc drivers/scsi/pm8001/pm8001_sas.c
index d6ff17b78a6f,4c5a3d23e010..000000000000
--- a/drivers/scsi/pm8001/pm8001_sas.c
+++ b/drivers/scsi/pm8001/pm8001_sas.c
@@@ -116,8 -116,8 +116,13 @@@ int pm8001_mem_alloc(struct pci_dev *pd
  	u64 align_offset = 0;
  	if (align)
  		align_offset = (dma_addr_t)align - 1;
++<<<<<<< HEAD
 +	mem_virt_alloc =
 +		pci_alloc_consistent(pdev, mem_size + align, &mem_dma_handle);
++=======
+ 	mem_virt_alloc = dma_alloc_coherent(&pdev->dev, mem_size + align,
+ 					    &mem_dma_handle, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!mem_virt_alloc) {
  		pm8001_printk("memory allocation error\n");
  		return -1;
diff --cc drivers/scsi/qedi/qedi_main.c
index 1a50b83d3179,e74a62448ba4..000000000000
--- a/drivers/scsi/qedi/qedi_main.c
+++ b/drivers/scsi/qedi/qedi_main.c
@@@ -1473,7 -1510,7 +1472,11 @@@ static int qedi_alloc_bdq(struct qedi_c
  
  	/* Allocate list of PBL pages */
  	qedi->bdq_pbl_list = dma_alloc_coherent(&qedi->pdev->dev,
++<<<<<<< HEAD
 +						PAGE_SIZE,
++=======
+ 						QEDI_PAGE_SIZE,
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  						&qedi->bdq_pbl_list_dma,
  						GFP_KERNEL);
  	if (!qedi->bdq_pbl_list) {
@@@ -1571,11 -1608,10 +1574,18 @@@ static int qedi_alloc_global_queues(str
  		    (qedi->global_queues[i]->cq_pbl_size +
  		    (QEDI_PAGE_SIZE - 1));
  
++<<<<<<< HEAD
 +		qedi->global_queues[i]->cq =
 +		    dma_alloc_coherent(&qedi->pdev->dev,
 +				       qedi->global_queues[i]->cq_mem_size,
 +				       &qedi->global_queues[i]->cq_dma,
 +				       GFP_KERNEL);
++=======
+ 		qedi->global_queues[i]->cq = dma_alloc_coherent(&qedi->pdev->dev,
+ 								qedi->global_queues[i]->cq_mem_size,
+ 								&qedi->global_queues[i]->cq_dma,
+ 								GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
  		if (!qedi->global_queues[i]->cq) {
  			QEDI_WARN(&qedi->dbg_ctx,
@@@ -1583,14 -1619,10 +1593,21 @@@
  			status = -ENOMEM;
  			goto mem_alloc_failure;
  		}
++<<<<<<< HEAD
 +		memset(qedi->global_queues[i]->cq, 0,
 +		       qedi->global_queues[i]->cq_mem_size);
 +
 +		qedi->global_queues[i]->cq_pbl =
 +		    dma_alloc_coherent(&qedi->pdev->dev,
 +				       qedi->global_queues[i]->cq_pbl_size,
 +				       &qedi->global_queues[i]->cq_pbl_dma,
 +				       GFP_KERNEL);
++=======
+ 		qedi->global_queues[i]->cq_pbl = dma_alloc_coherent(&qedi->pdev->dev,
+ 								    qedi->global_queues[i]->cq_pbl_size,
+ 								    &qedi->global_queues[i]->cq_pbl_dma,
+ 								    GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
  		if (!qedi->global_queues[i]->cq_pbl) {
  			QEDI_WARN(&qedi->dbg_ctx,
@@@ -1668,8 -1698,6 +1685,11 @@@ int qedi_alloc_sq(struct qedi_ctx *qedi
  		rval = -ENOMEM;
  		goto out;
  	}
++<<<<<<< HEAD
 +	memset(ep->sq, 0, ep->sq_mem_size);
 +
++=======
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	ep->sq_pbl = dma_alloc_coherent(&qedi->pdev->dev, ep->sq_pbl_size,
  					&ep->sq_pbl_dma, GFP_KERNEL);
  	if (!ep->sq_pbl) {
diff --cc drivers/scsi/qla2xxx/qla_init.c
index 1c4bb984c354,aeeb0144bd55..000000000000
--- a/drivers/scsi/qla2xxx/qla_init.c
+++ b/drivers/scsi/qla2xxx/qla_init.c
@@@ -2866,7 -3099,7 +2866,11 @@@ qla2x00_alloc_offload_mem(scsi_qla_host
  			    FCE_SIZE, ha->fce, ha->fce_dma);
  
  		/* Allocate memory for Fibre Channel Event Buffer. */
++<<<<<<< HEAD
 +		tc = dma_zalloc_coherent(&ha->pdev->dev, FCE_SIZE, &tc_dma,
++=======
+ 		tc = dma_alloc_coherent(&ha->pdev->dev, FCE_SIZE, &tc_dma,
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  					GFP_KERNEL);
  		if (!tc) {
  			ql_log(ql_log_warn, vha, 0x00be,
diff --cc drivers/staging/vt6655/device_main.c
index 08b250f01dae,c9097e7367d8..000000000000
--- a/drivers/staging/vt6655/device_main.c
+++ b/drivers/staging/vt6655/device_main.c
@@@ -404,448 -171,1486 +404,1705 @@@ static void device_get_options(PSDevic
  }
  
  static void
 -device_set_options(struct vnt_private *priv)
 -{
 -	priv->byShortRetryLimit = priv->opts.short_retry;
 -	priv->byLongRetryLimit = priv->opts.long_retry;
 -	priv->byBBType = priv->opts.bbp_type;
 -	priv->byPacketType = priv->byBBType;
 -	priv->byAutoFBCtrl = AUTO_FB_0;
 -	priv->bUpdateBBVGA = true;
 -	priv->byPreambleType = 0;
 +device_set_options(PSDevice pDevice) {
 +	unsigned char abyBroadcastAddr[ETH_ALEN] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};
 +	unsigned char abySNAP_RFC1042[ETH_ALEN] = {0xAA, 0xAA, 0x03, 0x00, 0x00, 0x00};
 +	unsigned char abySNAP_Bridgetunnel[ETH_ALEN] = {0xAA, 0xAA, 0x03, 0x00, 0x00, 0xF8};
 +
 +	memcpy(pDevice->abyBroadcastAddr, abyBroadcastAddr, ETH_ALEN);
 +	memcpy(pDevice->abySNAP_RFC1042, abySNAP_RFC1042, ETH_ALEN);
 +	memcpy(pDevice->abySNAP_Bridgetunnel, abySNAP_Bridgetunnel, ETH_ALEN);
 +
 +	pDevice->uChannel = pDevice->sOpts.channel_num;
 +	pDevice->wRTSThreshold = pDevice->sOpts.rts_thresh;
 +	pDevice->wFragmentationThreshold = pDevice->sOpts.frag_thresh;
 +	pDevice->byShortRetryLimit = pDevice->sOpts.short_retry;
 +	pDevice->byLongRetryLimit = pDevice->sOpts.long_retry;
 +	pDevice->wMaxTransmitMSDULifetime = DEFAULT_MSDU_LIFETIME;
 +	pDevice->byShortPreamble = (pDevice->sOpts.flags & DEVICE_FLAGS_PREAMBLE_TYPE) ? 1 : 0;
 +	pDevice->byOpMode = (pDevice->sOpts.flags & DEVICE_FLAGS_OP_MODE) ? 1 : 0;
 +	pDevice->ePSMode = (pDevice->sOpts.flags & DEVICE_FLAGS_PS_MODE) ? 1 : 0;
 +	pDevice->b11hEnable = (pDevice->sOpts.flags & DEVICE_FLAGS_80211h_MODE) ? 1 : 0;
 +	pDevice->bDiversityRegCtlON = (pDevice->sOpts.flags & DEVICE_FLAGS_DiversityANT) ? 1 : 0;
 +	pDevice->uConnectionRate = pDevice->sOpts.data_rate;
 +	if (pDevice->uConnectionRate < RATE_AUTO) pDevice->bFixRate = true;
 +	pDevice->byBBType = pDevice->sOpts.bbp_type;
 +	pDevice->byPacketType = pDevice->byBBType;
 +
 +//PLICE_DEBUG->
 +	pDevice->byAutoFBCtrl = AUTO_FB_0;
 +	//pDevice->byAutoFBCtrl = AUTO_FB_1;
 +//PLICE_DEBUG<-
 +	pDevice->bUpdateBBVGA = true;
 +	pDevice->byFOETuning = 0;
 +	pDevice->wCTSDuration = 0;
 +	pDevice->byPreambleType = 0;
 +
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " uChannel= %d\n", (int)pDevice->uChannel);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byOpMode= %d\n", (int)pDevice->byOpMode);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " ePSMode= %d\n", (int)pDevice->ePSMode);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " wRTSThreshold= %d\n", (int)pDevice->wRTSThreshold);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byShortRetryLimit= %d\n", (int)pDevice->byShortRetryLimit);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byLongRetryLimit= %d\n", (int)pDevice->byLongRetryLimit);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byPreambleType= %d\n", (int)pDevice->byPreambleType);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byShortPreamble= %d\n", (int)pDevice->byShortPreamble);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " uConnectionRate= %d\n", (int)pDevice->uConnectionRate);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " byBBType= %d\n", (int)pDevice->byBBType);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " pDevice->b11hEnable= %d\n", (int)pDevice->b11hEnable);
 +	DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO " pDevice->bDiversityRegCtlON= %d\n", (int)pDevice->bDiversityRegCtlON);
 +}
  
 -	pr_debug(" byShortRetryLimit= %d\n", (int)priv->byShortRetryLimit);
 -	pr_debug(" byLongRetryLimit= %d\n", (int)priv->byLongRetryLimit);
 -	pr_debug(" byPreambleType= %d\n", (int)priv->byPreambleType);
 -	pr_debug(" byShortPreamble= %d\n", (int)priv->byShortPreamble);
 -	pr_debug(" byBBType= %d\n", (int)priv->byBBType);
 +static void s_vCompleteCurrentMeasure(PSDevice pDevice, unsigned char byResult)
 +{
 +	unsigned int ii;
 +	unsigned long dwDuration = 0;
 +	unsigned char byRPI0 = 0;
 +
 +	for (ii = 1; ii < 8; ii++) {
 +		pDevice->dwRPIs[ii] *= 255;
 +		dwDuration |= *((unsigned short *)(pDevice->pCurrMeasureEID->sReq.abyDuration));
 +		dwDuration <<= 10;
 +		pDevice->dwRPIs[ii] /= dwDuration;
 +		pDevice->abyRPIs[ii] = (unsigned char)pDevice->dwRPIs[ii];
 +		byRPI0 += pDevice->abyRPIs[ii];
 +	}
 +	pDevice->abyRPIs[0] = (0xFF - byRPI0);
 +
 +	if (pDevice->uNumOfMeasureEIDs == 0) {
 +		VNTWIFIbMeasureReport(pDevice->pMgmt,
 +				      true,
 +				      pDevice->pCurrMeasureEID,
 +				      byResult,
 +				      pDevice->byBasicMap,
 +				      pDevice->byCCAFraction,
 +				      pDevice->abyRPIs
 +			);
 +	} else {
 +		VNTWIFIbMeasureReport(pDevice->pMgmt,
 +				      false,
 +				      pDevice->pCurrMeasureEID,
 +				      byResult,
 +				      pDevice->byBasicMap,
 +				      pDevice->byCCAFraction,
 +				      pDevice->abyRPIs
 +			);
 +		CARDbStartMeasure(pDevice, pDevice->pCurrMeasureEID++, pDevice->uNumOfMeasureEIDs);
 +	}
  }
  
 -/*
 - * Initialisation of MAC & BBP registers
 - */
 +//
 +// Initialisation of MAC & BBP registers
 +//
  
 -static void device_init_registers(struct vnt_private *priv)
 +static void device_init_registers(PSDevice pDevice, DEVICE_INIT_TYPE InitType)
  {
 -	unsigned long flags;
  	unsigned int ii;
  	unsigned char byValue;
 +	unsigned char byValue1;
  	unsigned char byCCKPwrdBm = 0;
  	unsigned char byOFDMPwrdBm = 0;
 -
 -	MACbShutdown(priv);
 -	BBvSoftwareReset(priv);
 -
 -	/* Do MACbSoftwareReset in MACvInitialize */
 -	MACbSoftwareReset(priv);
 -
 -	priv->bAES = false;
 -
 -	/* Only used in 11g type, sync with ERP IE */
 -	priv->bProtectMode = false;
 -
 -	priv->bNonERPPresent = false;
 -	priv->bBarkerPreambleMd = false;
 -	priv->wCurrentRate = RATE_1M;
 -	priv->byTopOFDMBasicRate = RATE_24M;
 -	priv->byTopCCKBasicRate = RATE_1M;
 -
 -	/* init MAC */
 -	MACvInitialize(priv);
 -
 -	/* Get Local ID */
 -	VNSvInPortB(priv->PortOffset + MAC_REG_LOCALID, &priv->byLocalID);
 -
 -	spin_lock_irqsave(&priv->lock, flags);
 -
 -	SROMvReadAllContents(priv->PortOffset, priv->abyEEPROM);
 -
 -	spin_unlock_irqrestore(&priv->lock, flags);
 -
 -	/* Get Channel range */
 -	priv->byMinChannel = 1;
 -	priv->byMaxChannel = CB_MAX_CHANNEL;
 -
 -	/* Get Antena */
 -	byValue = SROMbyReadEmbedded(priv->PortOffset, EEP_OFS_ANTENNA);
 -	if (byValue & EEP_ANTINV)
 -		priv->bTxRxAntInv = true;
 -	else
 -		priv->bTxRxAntInv = false;
 -
 -	byValue &= (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN);
 -	/* if not set default is All */
 -	if (byValue == 0)
 -		byValue = (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN);
 -
 -	if (byValue == (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN)) {
 -		priv->byAntennaCount = 2;
 -		priv->byTxAntennaMode = ANT_B;
 -		priv->dwTxAntennaSel = 1;
 -		priv->dwRxAntennaSel = 1;
 -
 -		if (priv->bTxRxAntInv)
 -			priv->byRxAntennaMode = ANT_A;
 +	int zonetype = 0;
 +	PSMgmtObject    pMgmt = &(pDevice->sMgmtObj);
 +	MACbShutdown(pDevice->PortOffset);
 +	BBvSoftwareReset(pDevice->PortOffset);
 +
 +	if ((InitType == DEVICE_INIT_COLD) ||
 +	    (InitType == DEVICE_INIT_DXPL)) {
 +		// Do MACbSoftwareReset in MACvInitialize
 +		MACbSoftwareReset(pDevice->PortOffset);
 +		// force CCK
 +		pDevice->bCCK = true;
 +		pDevice->bAES = false;
 +		pDevice->bProtectMode = false;      //Only used in 11g type, sync with ERP IE
 +		pDevice->bNonERPPresent = false;
 +		pDevice->bBarkerPreambleMd = false;
 +		pDevice->wCurrentRate = RATE_1M;
 +		pDevice->byTopOFDMBasicRate = RATE_24M;
 +		pDevice->byTopCCKBasicRate = RATE_1M;
 +
 +		pDevice->byRevId = 0;                   //Target to IF pin while programming to RF chip.
 +
 +		// init MAC
 +		MACvInitialize(pDevice->PortOffset);
 +
 +		// Get Local ID
 +		VNSvInPortB(pDevice->PortOffset + MAC_REG_LOCALID, &(pDevice->byLocalID));
 +
 +		spin_lock_irq(&pDevice->lock);
 +		SROMvReadAllContents(pDevice->PortOffset, pDevice->abyEEPROM);
 +
 +		spin_unlock_irq(&pDevice->lock);
 +
 +		// Get Channel range
 +
 +		pDevice->byMinChannel = 1;
 +		pDevice->byMaxChannel = CB_MAX_CHANNEL;
 +
 +		// Get Antena
 +		byValue = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_ANTENNA);
 +		if (byValue & EEP_ANTINV)
 +			pDevice->bTxRxAntInv = true;
  		else
 -			priv->byRxAntennaMode = ANT_B;
 -	} else  {
 -		priv->byAntennaCount = 1;
 -		priv->dwTxAntennaSel = 0;
 -		priv->dwRxAntennaSel = 0;
 -
 -		if (byValue & EEP_ANTENNA_AUX) {
 -			priv->byTxAntennaMode = ANT_A;
 -
 -			if (priv->bTxRxAntInv)
 -				priv->byRxAntennaMode = ANT_B;
 +			pDevice->bTxRxAntInv = false;
 +
 +		byValue &= (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN);
 +		if (byValue == 0) // if not set default is All
 +			byValue = (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN);
 +
 +		pDevice->ulDiversityNValue = 100*260;//100*SROMbyReadEmbedded(pDevice->PortOffset, 0x51);
 +		pDevice->ulDiversityMValue = 100*16;//SROMbyReadEmbedded(pDevice->PortOffset, 0x52);
 +		pDevice->byTMax = 1;//SROMbyReadEmbedded(pDevice->PortOffset, 0x53);
 +		pDevice->byTMax2 = 4;//SROMbyReadEmbedded(pDevice->PortOffset, 0x54);
 +		pDevice->ulSQ3TH = 0;//(unsigned long) SROMbyReadEmbedded(pDevice->PortOffset, 0x55);
 +		pDevice->byTMax3 = 64;//SROMbyReadEmbedded(pDevice->PortOffset, 0x56);
 +
 +		if (byValue == (EEP_ANTENNA_AUX | EEP_ANTENNA_MAIN)) {
 +			pDevice->byAntennaCount = 2;
 +			pDevice->byTxAntennaMode = ANT_B;
 +			pDevice->dwTxAntennaSel = 1;
 +			pDevice->dwRxAntennaSel = 1;
 +			if (pDevice->bTxRxAntInv == true)
 +				pDevice->byRxAntennaMode = ANT_A;
  			else
 -				priv->byRxAntennaMode = ANT_A;
 -		} else {
 -			priv->byTxAntennaMode = ANT_B;
 -
 -			if (priv->bTxRxAntInv)
 -				priv->byRxAntennaMode = ANT_A;
 +				pDevice->byRxAntennaMode = ANT_B;
 +			// chester for antenna
 +			byValue1 = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_ANTENNA);
 +			if ((byValue1 & 0x08) == 0)
 +				pDevice->bDiversityEnable = false;//SROMbyReadEmbedded(pDevice->PortOffset, 0x50);
  			else
++<<<<<<< HEAD
 +				pDevice->bDiversityEnable = true;
 +		} else  {
 +			pDevice->bDiversityEnable = false;
 +			pDevice->byAntennaCount = 1;
 +			pDevice->dwTxAntennaSel = 0;
 +			pDevice->dwRxAntennaSel = 0;
 +			if (byValue & EEP_ANTENNA_AUX) {
 +				pDevice->byTxAntennaMode = ANT_A;
 +				if (pDevice->bTxRxAntInv == true)
 +					pDevice->byRxAntennaMode = ANT_B;
 +				else
 +					pDevice->byRxAntennaMode = ANT_A;
++=======
+ 				priv->byRxAntennaMode = ANT_B;
+ 		}
+ 	}
+ 
+ 	/* Set initial antenna mode */
+ 	BBvSetTxAntennaMode(priv, priv->byTxAntennaMode);
+ 	BBvSetRxAntennaMode(priv, priv->byRxAntennaMode);
+ 
+ 	/* zonetype initial */
+ 	priv->byOriginalZonetype = priv->abyEEPROM[EEP_OFS_ZONETYPE];
+ 
+ 	if (!priv->bZoneRegExist)
+ 		priv->byZoneType = priv->abyEEPROM[EEP_OFS_ZONETYPE];
+ 
+ 	pr_debug("priv->byZoneType = %x\n", priv->byZoneType);
+ 
+ 	/* Init RF module */
+ 	RFbInit(priv);
+ 
+ 	/* Get Desire Power Value */
+ 	priv->byCurPwr = 0xFF;
+ 	priv->byCCKPwr = SROMbyReadEmbedded(priv->PortOffset, EEP_OFS_PWR_CCK);
+ 	priv->byOFDMPwrG = SROMbyReadEmbedded(priv->PortOffset, EEP_OFS_PWR_OFDMG);
+ 
+ 	/* Load power Table */
+ 	for (ii = 0; ii < CB_MAX_CHANNEL_24G; ii++) {
+ 		priv->abyCCKPwrTbl[ii + 1] =
+ 			SROMbyReadEmbedded(priv->PortOffset,
+ 					   (unsigned char)(ii + EEP_OFS_CCK_PWR_TBL));
+ 		if (priv->abyCCKPwrTbl[ii + 1] == 0)
+ 			priv->abyCCKPwrTbl[ii + 1] = priv->byCCKPwr;
+ 
+ 		priv->abyOFDMPwrTbl[ii + 1] =
+ 			SROMbyReadEmbedded(priv->PortOffset,
+ 					   (unsigned char)(ii + EEP_OFS_OFDM_PWR_TBL));
+ 		if (priv->abyOFDMPwrTbl[ii + 1] == 0)
+ 			priv->abyOFDMPwrTbl[ii + 1] = priv->byOFDMPwrG;
+ 
+ 		priv->abyCCKDefaultPwr[ii + 1] = byCCKPwrdBm;
+ 		priv->abyOFDMDefaultPwr[ii + 1] = byOFDMPwrdBm;
+ 	}
+ 
+ 	/* recover 12,13 ,14channel for EUROPE by 11 channel */
+ 	for (ii = 11; ii < 14; ii++) {
+ 		priv->abyCCKPwrTbl[ii] = priv->abyCCKPwrTbl[10];
+ 		priv->abyOFDMPwrTbl[ii] = priv->abyOFDMPwrTbl[10];
+ 	}
+ 
+ 	/* Load OFDM A Power Table */
+ 	for (ii = 0; ii < CB_MAX_CHANNEL_5G; ii++) {
+ 		priv->abyOFDMPwrTbl[ii + CB_MAX_CHANNEL_24G + 1] =
+ 			SROMbyReadEmbedded(priv->PortOffset,
+ 					   (unsigned char)(ii + EEP_OFS_OFDMA_PWR_TBL));
+ 
+ 		priv->abyOFDMDefaultPwr[ii + CB_MAX_CHANNEL_24G + 1] =
+ 			SROMbyReadEmbedded(priv->PortOffset,
+ 					   (unsigned char)(ii + EEP_OFS_OFDMA_PWR_dBm));
+ 	}
+ 
+ 	if (priv->byLocalID > REV_ID_VT3253_B1) {
+ 		MACvSelectPage1(priv->PortOffset);
+ 
+ 		VNSvOutPortB(priv->PortOffset + MAC_REG_MSRCTL + 1,
+ 			     (MSRCTL1_TXPWR | MSRCTL1_CSAPAREN));
+ 
+ 		MACvSelectPage0(priv->PortOffset);
+ 	}
+ 
+ 	/* use relative tx timeout and 802.11i D4 */
+ 	MACvWordRegBitsOn(priv->PortOffset,
+ 			  MAC_REG_CFG, (CFG_TKIPOPT | CFG_NOTXTIMEOUT));
+ 
+ 	/* set performance parameter by registry */
+ 	MACvSetShortRetryLimit(priv, priv->byShortRetryLimit);
+ 	MACvSetLongRetryLimit(priv, priv->byLongRetryLimit);
+ 
+ 	/* reset TSF counter */
+ 	VNSvOutPortB(priv->PortOffset + MAC_REG_TFTCTL, TFTCTL_TSFCNTRST);
+ 	/* enable TSF counter */
+ 	VNSvOutPortB(priv->PortOffset + MAC_REG_TFTCTL, TFTCTL_TSFCNTREN);
+ 
+ 	/* initialize BBP registers */
+ 	BBbVT3253Init(priv);
+ 
+ 	if (priv->bUpdateBBVGA) {
+ 		priv->byBBVGACurrent = priv->abyBBVGA[0];
+ 		priv->byBBVGANew = priv->byBBVGACurrent;
+ 		BBvSetVGAGainOffset(priv, priv->abyBBVGA[0]);
+ 	}
+ 
+ 	BBvSetRxAntennaMode(priv, priv->byRxAntennaMode);
+ 	BBvSetTxAntennaMode(priv, priv->byTxAntennaMode);
+ 
+ 	/* Set BB and packet type at the same time. */
+ 	/* Set Short Slot Time, xIFS, and RSPINF. */
+ 	priv->wCurrentRate = RATE_54M;
+ 
+ 	priv->bRadioOff = false;
+ 
+ 	priv->byRadioCtl = SROMbyReadEmbedded(priv->PortOffset,
+ 						 EEP_OFS_RADIOCTL);
+ 	priv->bHWRadioOff = false;
+ 
+ 	if (priv->byRadioCtl & EEP_RADIOCTL_ENABLE) {
+ 		/* Get GPIO */
+ 		MACvGPIOIn(priv->PortOffset, &priv->byGPIO);
+ 
+ 		if (((priv->byGPIO & GPIO0_DATA) &&
+ 		     !(priv->byRadioCtl & EEP_RADIOCTL_INV)) ||
+ 		     (!(priv->byGPIO & GPIO0_DATA) &&
+ 		     (priv->byRadioCtl & EEP_RADIOCTL_INV)))
+ 			priv->bHWRadioOff = true;
+ 	}
+ 
+ 	if (priv->bHWRadioOff || priv->bRadioControlOff)
+ 		CARDbRadioPowerOff(priv);
+ 
+ 	/* get Permanent network address */
+ 	SROMvReadEtherAddress(priv->PortOffset, priv->abyCurrentNetAddr);
+ 	pr_debug("Network address = %pM\n", priv->abyCurrentNetAddr);
+ 
+ 	/* reset Tx pointer */
+ 	CARDvSafeResetRx(priv);
+ 	/* reset Rx pointer */
+ 	CARDvSafeResetTx(priv);
+ 
+ 	if (priv->byLocalID <= REV_ID_VT3253_A1)
+ 		MACvRegBitsOn(priv->PortOffset, MAC_REG_RCR, RCR_WPAERR);
+ 
+ 	/* Turn On Rx DMA */
+ 	MACvReceive0(priv->PortOffset);
+ 	MACvReceive1(priv->PortOffset);
+ 
+ 	/* start the adapter */
+ 	MACvStart(priv->PortOffset);
+ }
+ 
+ static void device_print_info(struct vnt_private *priv)
+ {
+ 	dev_info(&priv->pcid->dev, "MAC=%pM IO=0x%lx Mem=0x%lx IRQ=%d\n",
+ 		 priv->abyCurrentNetAddr, (unsigned long)priv->ioaddr,
+ 		 (unsigned long)priv->PortOffset, priv->pcid->irq);
+ }
+ 
+ static void device_free_info(struct vnt_private *priv)
+ {
+ 	if (!priv)
+ 		return;
+ 
+ 	if (priv->mac_hw)
+ 		ieee80211_unregister_hw(priv->hw);
+ 
+ 	if (priv->PortOffset)
+ 		iounmap(priv->PortOffset);
+ 
+ 	if (priv->pcid)
+ 		pci_release_regions(priv->pcid);
+ 
+ 	if (priv->hw)
+ 		ieee80211_free_hw(priv->hw);
+ }
+ 
+ static bool device_init_rings(struct vnt_private *priv)
+ {
+ 	void *vir_pool;
+ 
+ 	/*allocate all RD/TD rings a single pool*/
+ 	vir_pool = dma_alloc_coherent(&priv->pcid->dev,
+ 				      priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc) + priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc) + priv->opts.tx_descs[0] * sizeof(struct vnt_tx_desc) + priv->opts.tx_descs[1] * sizeof(struct vnt_tx_desc),
+ 				      &priv->pool_dma, GFP_ATOMIC);
+ 	if (!vir_pool) {
+ 		dev_err(&priv->pcid->dev, "allocate desc dma memory failed\n");
+ 		return false;
+ 	}
+ 
+ 	priv->aRD0Ring = vir_pool;
+ 	priv->aRD1Ring = vir_pool +
+ 		priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc);
+ 
+ 	priv->rd0_pool_dma = priv->pool_dma;
+ 	priv->rd1_pool_dma = priv->rd0_pool_dma +
+ 		priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc);
+ 
+ 	priv->tx0_bufs = dma_alloc_coherent(&priv->pcid->dev,
+ 					    priv->opts.tx_descs[0] * PKT_BUF_SZ + priv->opts.tx_descs[1] * PKT_BUF_SZ + CB_BEACON_BUF_SIZE + CB_MAX_BUF_SIZE,
+ 					    &priv->tx_bufs_dma0, GFP_ATOMIC);
+ 	if (!priv->tx0_bufs) {
+ 		dev_err(&priv->pcid->dev, "allocate buf dma memory failed\n");
+ 
+ 		dma_free_coherent(&priv->pcid->dev,
+ 				  priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc) +
+ 				  priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc) +
+ 				  priv->opts.tx_descs[0] * sizeof(struct vnt_tx_desc) +
+ 				  priv->opts.tx_descs[1] * sizeof(struct vnt_tx_desc),
+ 				  vir_pool, priv->pool_dma);
+ 		return false;
+ 	}
+ 
+ 	priv->td0_pool_dma = priv->rd1_pool_dma +
+ 		priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc);
+ 
+ 	priv->td1_pool_dma = priv->td0_pool_dma +
+ 		priv->opts.tx_descs[0] * sizeof(struct vnt_tx_desc);
+ 
+ 	/* vir_pool: pvoid type */
+ 	priv->apTD0Rings = vir_pool
+ 		+ priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc)
+ 		+ priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc);
+ 
+ 	priv->apTD1Rings = vir_pool
+ 		+ priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc)
+ 		+ priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc)
+ 		+ priv->opts.tx_descs[0] * sizeof(struct vnt_tx_desc);
+ 
+ 	priv->tx1_bufs = priv->tx0_bufs +
+ 		priv->opts.tx_descs[0] * PKT_BUF_SZ;
+ 
+ 	priv->tx_beacon_bufs = priv->tx1_bufs +
+ 		priv->opts.tx_descs[1] * PKT_BUF_SZ;
+ 
+ 	priv->pbyTmpBuff = priv->tx_beacon_bufs +
+ 		CB_BEACON_BUF_SIZE;
+ 
+ 	priv->tx_bufs_dma1 = priv->tx_bufs_dma0 +
+ 		priv->opts.tx_descs[0] * PKT_BUF_SZ;
+ 
+ 	priv->tx_beacon_dma = priv->tx_bufs_dma1 +
+ 		priv->opts.tx_descs[1] * PKT_BUF_SZ;
+ 
+ 	return true;
+ }
+ 
+ static void device_free_rings(struct vnt_private *priv)
+ {
+ 	dma_free_coherent(&priv->pcid->dev,
+ 			  priv->opts.rx_descs0 * sizeof(struct vnt_rx_desc) +
+ 			  priv->opts.rx_descs1 * sizeof(struct vnt_rx_desc) +
+ 			  priv->opts.tx_descs[0] * sizeof(struct vnt_tx_desc) +
+ 			  priv->opts.tx_descs[1] * sizeof(struct vnt_tx_desc),
+ 			  priv->aRD0Ring, priv->pool_dma);
+ 
+ 	if (priv->tx0_bufs)
+ 		dma_free_coherent(&priv->pcid->dev,
+ 				  priv->opts.tx_descs[0] * PKT_BUF_SZ +
+ 				  priv->opts.tx_descs[1] * PKT_BUF_SZ +
+ 				  CB_BEACON_BUF_SIZE +
+ 				  CB_MAX_BUF_SIZE,
+ 				  priv->tx0_bufs, priv->tx_bufs_dma0);
+ }
+ 
+ static int device_init_rd0_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 	dma_addr_t      curr = priv->rd0_pool_dma;
+ 	struct vnt_rx_desc *desc;
+ 	int ret;
+ 
+ 	/* Init the RD0 ring entries */
+ 	for (i = 0; i < priv->opts.rx_descs0;
+ 	     i ++, curr += sizeof(struct vnt_rx_desc)) {
+ 		desc = &priv->aRD0Ring[i];
+ 		desc->rd_info = kzalloc(sizeof(*desc->rd_info), GFP_KERNEL);
+ 		if (!desc->rd_info) {
+ 			ret = -ENOMEM;
+ 			goto err_free_desc;
+ 		}
+ 
+ 		if (!device_alloc_rx_buf(priv, desc)) {
+ 			dev_err(&priv->pcid->dev, "can not alloc rx bufs\n");
+ 			ret = -ENOMEM;
+ 			goto err_free_rd;
+ 		}
+ 
+ 		desc->next = &priv->aRD0Ring[(i + 1) % priv->opts.rx_descs0];
+ 		desc->next_desc = cpu_to_le32(curr + sizeof(struct vnt_rx_desc));
+ 	}
+ 
+ 	if (i > 0)
+ 		priv->aRD0Ring[i-1].next_desc = cpu_to_le32(priv->rd0_pool_dma);
+ 	priv->pCurrRD[0] = &priv->aRD0Ring[0];
+ 
+ 	return 0;
+ 
+ err_free_rd:
+ 	kfree(desc->rd_info);
+ 
+ err_free_desc:
+ 	while (--i) {
+ 		desc = &priv->aRD0Ring[i];
+ 		device_free_rx_buf(priv, desc);
+ 		kfree(desc->rd_info);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int device_init_rd1_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 	dma_addr_t      curr = priv->rd1_pool_dma;
+ 	struct vnt_rx_desc *desc;
+ 	int ret;
+ 
+ 	/* Init the RD1 ring entries */
+ 	for (i = 0; i < priv->opts.rx_descs1;
+ 	     i ++, curr += sizeof(struct vnt_rx_desc)) {
+ 		desc = &priv->aRD1Ring[i];
+ 		desc->rd_info = kzalloc(sizeof(*desc->rd_info), GFP_KERNEL);
+ 		if (!desc->rd_info) {
+ 			ret = -ENOMEM;
+ 			goto err_free_desc;
+ 		}
+ 
+ 		if (!device_alloc_rx_buf(priv, desc)) {
+ 			dev_err(&priv->pcid->dev, "can not alloc rx bufs\n");
+ 			ret = -ENOMEM;
+ 			goto err_free_rd;
+ 		}
+ 
+ 		desc->next = &priv->aRD1Ring[(i+1) % priv->opts.rx_descs1];
+ 		desc->next_desc = cpu_to_le32(curr + sizeof(struct vnt_rx_desc));
+ 	}
+ 
+ 	if (i > 0)
+ 		priv->aRD1Ring[i-1].next_desc = cpu_to_le32(priv->rd1_pool_dma);
+ 	priv->pCurrRD[1] = &priv->aRD1Ring[0];
+ 
+ 	return 0;
+ 
+ err_free_rd:
+ 	kfree(desc->rd_info);
+ 
+ err_free_desc:
+ 	while (--i) {
+ 		desc = &priv->aRD1Ring[i];
+ 		device_free_rx_buf(priv, desc);
+ 		kfree(desc->rd_info);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void device_free_rd0_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->opts.rx_descs0; i++) {
+ 		struct vnt_rx_desc *desc = &priv->aRD0Ring[i];
+ 
+ 		device_free_rx_buf(priv, desc);
+ 		kfree(desc->rd_info);
+ 	}
+ }
+ 
+ static void device_free_rd1_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->opts.rx_descs1; i++) {
+ 		struct vnt_rx_desc *desc = &priv->aRD1Ring[i];
+ 
+ 		device_free_rx_buf(priv, desc);
+ 		kfree(desc->rd_info);
+ 	}
+ }
+ 
+ static int device_init_td0_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 	dma_addr_t  curr;
+ 	struct vnt_tx_desc *desc;
+ 	int ret;
+ 
+ 	curr = priv->td0_pool_dma;
+ 	for (i = 0; i < priv->opts.tx_descs[0];
+ 	     i++, curr += sizeof(struct vnt_tx_desc)) {
+ 		desc = &priv->apTD0Rings[i];
+ 		desc->td_info = kzalloc(sizeof(*desc->td_info), GFP_KERNEL);
+ 		if (!desc->td_info) {
+ 			ret = -ENOMEM;
+ 			goto err_free_desc;
+ 		}
+ 
+ 		desc->td_info->buf = priv->tx0_bufs + i * PKT_BUF_SZ;
+ 		desc->td_info->buf_dma = priv->tx_bufs_dma0 + i * PKT_BUF_SZ;
+ 
+ 		desc->next = &(priv->apTD0Rings[(i+1) % priv->opts.tx_descs[0]]);
+ 		desc->next_desc = cpu_to_le32(curr + sizeof(struct vnt_tx_desc));
+ 	}
+ 
+ 	if (i > 0)
+ 		priv->apTD0Rings[i-1].next_desc = cpu_to_le32(priv->td0_pool_dma);
+ 	priv->apTailTD[0] = priv->apCurrTD[0] = &priv->apTD0Rings[0];
+ 
+ 	return 0;
+ 
+ err_free_desc:
+ 	while (--i) {
+ 		desc = &priv->apTD0Rings[i];
+ 		kfree(desc->td_info);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int device_init_td1_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 	dma_addr_t  curr;
+ 	struct vnt_tx_desc *desc;
+ 	int ret;
+ 
+ 	/* Init the TD ring entries */
+ 	curr = priv->td1_pool_dma;
+ 	for (i = 0; i < priv->opts.tx_descs[1];
+ 	     i++, curr += sizeof(struct vnt_tx_desc)) {
+ 		desc = &priv->apTD1Rings[i];
+ 		desc->td_info = kzalloc(sizeof(*desc->td_info), GFP_KERNEL);
+ 		if (!desc->td_info) {
+ 			ret = -ENOMEM;
+ 			goto err_free_desc;
+ 		}
+ 
+ 		desc->td_info->buf = priv->tx1_bufs + i * PKT_BUF_SZ;
+ 		desc->td_info->buf_dma = priv->tx_bufs_dma1 + i * PKT_BUF_SZ;
+ 
+ 		desc->next = &(priv->apTD1Rings[(i + 1) % priv->opts.tx_descs[1]]);
+ 		desc->next_desc = cpu_to_le32(curr + sizeof(struct vnt_tx_desc));
+ 	}
+ 
+ 	if (i > 0)
+ 		priv->apTD1Rings[i-1].next_desc = cpu_to_le32(priv->td1_pool_dma);
+ 	priv->apTailTD[1] = priv->apCurrTD[1] = &priv->apTD1Rings[0];
+ 
+ 	return 0;
+ 
+ err_free_desc:
+ 	while (--i) {
+ 		desc = &priv->apTD1Rings[i];
+ 		kfree(desc->td_info);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void device_free_td0_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->opts.tx_descs[0]; i++) {
+ 		struct vnt_tx_desc *desc = &priv->apTD0Rings[i];
+ 		struct vnt_td_info *td_info = desc->td_info;
+ 
+ 		dev_kfree_skb(td_info->skb);
+ 		kfree(desc->td_info);
+ 	}
+ }
+ 
+ static void device_free_td1_ring(struct vnt_private *priv)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->opts.tx_descs[1]; i++) {
+ 		struct vnt_tx_desc *desc = &priv->apTD1Rings[i];
+ 		struct vnt_td_info *td_info = desc->td_info;
+ 
+ 		dev_kfree_skb(td_info->skb);
+ 		kfree(desc->td_info);
+ 	}
+ }
+ 
+ /*-----------------------------------------------------------------*/
+ 
+ static int device_rx_srv(struct vnt_private *priv, unsigned int idx)
+ {
+ 	struct vnt_rx_desc *rd;
+ 	int works = 0;
+ 
+ 	for (rd = priv->pCurrRD[idx];
+ 	     rd->rd0.owner == OWNED_BY_HOST;
+ 	     rd = rd->next) {
+ 		if (works++ > 15)
+ 			break;
+ 
+ 		if (!rd->rd_info->skb)
+ 			break;
+ 
+ 		if (vnt_receive_frame(priv, rd)) {
+ 			if (!device_alloc_rx_buf(priv, rd)) {
+ 				dev_err(&priv->pcid->dev,
+ 					"can not allocate rx buf\n");
+ 				break;
+ 			}
+ 		}
+ 		rd->rd0.owner = OWNED_BY_NIC;
+ 	}
+ 
+ 	priv->pCurrRD[idx] = rd;
+ 
+ 	return works;
+ }
+ 
+ static bool device_alloc_rx_buf(struct vnt_private *priv,
+ 				struct vnt_rx_desc *rd)
+ {
+ 	struct vnt_rd_info *rd_info = rd->rd_info;
+ 
+ 	rd_info->skb = dev_alloc_skb((int)priv->rx_buf_sz);
+ 	if (!rd_info->skb)
+ 		return false;
+ 
+ 	rd_info->skb_dma =
+ 		dma_map_single(&priv->pcid->dev,
+ 			       skb_put(rd_info->skb, skb_tailroom(rd_info->skb)),
+ 			       priv->rx_buf_sz, DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(&priv->pcid->dev, rd_info->skb_dma)) {
+ 		dev_kfree_skb(rd_info->skb);
+ 		rd_info->skb = NULL;
+ 		return false;
+ 	}
+ 
+ 	*((unsigned int *)&rd->rd0) = 0; /* FIX cast */
+ 
+ 	rd->rd0.res_count = cpu_to_le16(priv->rx_buf_sz);
+ 	rd->rd0.owner = OWNED_BY_NIC;
+ 	rd->rd1.req_count = cpu_to_le16(priv->rx_buf_sz);
+ 	rd->buff_addr = cpu_to_le32(rd_info->skb_dma);
+ 
+ 	return true;
+ }
+ 
+ static void device_free_rx_buf(struct vnt_private *priv,
+ 				struct vnt_rx_desc *rd)
+ {
+ 	struct vnt_rd_info *rd_info = rd->rd_info;
+ 
+ 	dma_unmap_single(&priv->pcid->dev, rd_info->skb_dma,
+ 			priv->rx_buf_sz, DMA_FROM_DEVICE);
+ 	dev_kfree_skb(rd_info->skb);
+ }
+ 
+ static const u8 fallback_rate0[5][5] = {
+ 	{RATE_18M, RATE_18M, RATE_12M, RATE_12M, RATE_12M},
+ 	{RATE_24M, RATE_24M, RATE_18M, RATE_12M, RATE_12M},
+ 	{RATE_36M, RATE_36M, RATE_24M, RATE_18M, RATE_18M},
+ 	{RATE_48M, RATE_48M, RATE_36M, RATE_24M, RATE_24M},
+ 	{RATE_54M, RATE_54M, RATE_48M, RATE_36M, RATE_36M}
+ };
+ 
+ static const u8 fallback_rate1[5][5] = {
+ 	{RATE_18M, RATE_18M, RATE_12M, RATE_6M, RATE_6M},
+ 	{RATE_24M, RATE_24M, RATE_18M, RATE_6M, RATE_6M},
+ 	{RATE_36M, RATE_36M, RATE_24M, RATE_12M, RATE_12M},
+ 	{RATE_48M, RATE_48M, RATE_24M, RATE_12M, RATE_12M},
+ 	{RATE_54M, RATE_54M, RATE_36M, RATE_18M, RATE_18M}
+ };
+ 
+ static int vnt_int_report_rate(struct vnt_private *priv,
+ 			       struct vnt_td_info *context, u8 tsr0, u8 tsr1)
+ {
+ 	struct vnt_tx_fifo_head *fifo_head;
+ 	struct ieee80211_tx_info *info;
+ 	struct ieee80211_rate *rate;
+ 	u16 fb_option;
+ 	u8 tx_retry = (tsr0 & TSR0_NCR);
+ 	s8 idx;
+ 
+ 	if (!context)
+ 		return -ENOMEM;
+ 
+ 	if (!context->skb)
+ 		return -EINVAL;
+ 
+ 	fifo_head = (struct vnt_tx_fifo_head *)context->buf;
+ 	fb_option = (le16_to_cpu(fifo_head->fifo_ctl) &
+ 			(FIFOCTL_AUTO_FB_0 | FIFOCTL_AUTO_FB_1));
+ 
+ 	info = IEEE80211_SKB_CB(context->skb);
+ 	idx = info->control.rates[0].idx;
+ 
+ 	if (fb_option && !(tsr1 & TSR1_TERR)) {
+ 		u8 tx_rate;
+ 		u8 retry = tx_retry;
+ 
+ 		rate = ieee80211_get_tx_rate(priv->hw, info);
+ 		tx_rate = rate->hw_value - RATE_18M;
+ 
+ 		if (retry > 4)
+ 			retry = 4;
+ 
+ 		if (fb_option & FIFOCTL_AUTO_FB_0)
+ 			tx_rate = fallback_rate0[tx_rate][retry];
+ 		else if (fb_option & FIFOCTL_AUTO_FB_1)
+ 			tx_rate = fallback_rate1[tx_rate][retry];
+ 
+ 		if (info->band == NL80211_BAND_5GHZ)
+ 			idx = tx_rate - RATE_6M;
+ 		else
+ 			idx = tx_rate;
+ 	}
+ 
+ 	ieee80211_tx_info_clear_status(info);
+ 
+ 	info->status.rates[0].count = tx_retry;
+ 
+ 	if (!(tsr1 & TSR1_TERR)) {
+ 		info->status.rates[0].idx = idx;
+ 
+ 		if (info->flags & IEEE80211_TX_CTL_NO_ACK)
+ 			info->flags |= IEEE80211_TX_STAT_NOACK_TRANSMITTED;
+ 		else
+ 			info->flags |= IEEE80211_TX_STAT_ACK;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int device_tx_srv(struct vnt_private *priv, unsigned int idx)
+ {
+ 	struct vnt_tx_desc *desc;
+ 	int                      works = 0;
+ 	unsigned char byTsr0;
+ 	unsigned char byTsr1;
+ 
+ 	for (desc = priv->apTailTD[idx]; priv->iTDUsed[idx] > 0; desc = desc->next) {
+ 		if (desc->td0.owner == OWNED_BY_NIC)
+ 			break;
+ 		if (works++ > 15)
+ 			break;
+ 
+ 		byTsr0 = desc->td0.tsr0;
+ 		byTsr1 = desc->td0.tsr1;
+ 
+ 		/* Only the status of first TD in the chain is correct */
+ 		if (desc->td1.tcr & TCR_STP) {
+ 			if ((desc->td_info->flags & TD_FLAGS_NETIF_SKB) != 0) {
+ 				if (!(byTsr1 & TSR1_TERR)) {
+ 					if (byTsr0 != 0) {
+ 						pr_debug(" Tx[%d] OK but has error. tsr1[%02X] tsr0[%02X]\n",
+ 							 (int)idx, byTsr1,
+ 							 byTsr0);
+ 					}
+ 				} else {
+ 					pr_debug(" Tx[%d] dropped & tsr1[%02X] tsr0[%02X]\n",
+ 						 (int)idx, byTsr1, byTsr0);
+ 				}
+ 			}
+ 
+ 			if (byTsr1 & TSR1_TERR) {
+ 				if ((desc->td_info->flags & TD_FLAGS_PRIV_SKB) != 0) {
+ 					pr_debug(" Tx[%d] fail has error. tsr1[%02X] tsr0[%02X]\n",
+ 						 (int)idx, byTsr1, byTsr0);
+ 				}
+ 			}
+ 
+ 			vnt_int_report_rate(priv, desc->td_info, byTsr0, byTsr1);
+ 
+ 			device_free_tx_buf(priv, desc);
+ 			priv->iTDUsed[idx]--;
+ 		}
+ 	}
+ 
+ 	priv->apTailTD[idx] = desc;
+ 
+ 	return works;
+ }
+ 
+ static void device_error(struct vnt_private *priv, unsigned short status)
+ {
+ 	if (status & ISR_FETALERR) {
+ 		dev_err(&priv->pcid->dev, "Hardware fatal error\n");
+ 
+ 		MACbShutdown(priv);
+ 		return;
+ 	}
+ }
+ 
+ static void device_free_tx_buf(struct vnt_private *priv,
+ 			       struct vnt_tx_desc *desc)
+ {
+ 	struct vnt_td_info *td_info = desc->td_info;
+ 	struct sk_buff *skb = td_info->skb;
+ 
+ 	if (skb)
+ 		ieee80211_tx_status_irqsafe(priv->hw, skb);
+ 
+ 	td_info->skb = NULL;
+ 	td_info->flags = 0;
+ }
+ 
+ static void vnt_check_bb_vga(struct vnt_private *priv)
+ {
+ 	long dbm;
+ 	int i;
+ 
+ 	if (!priv->bUpdateBBVGA)
+ 		return;
+ 
+ 	if (priv->hw->conf.flags & IEEE80211_CONF_OFFCHANNEL)
+ 		return;
+ 
+ 	if (!(priv->vif->bss_conf.assoc && priv->uCurrRSSI))
+ 		return;
+ 
+ 	RFvRSSITodBm(priv, (u8)priv->uCurrRSSI, &dbm);
+ 
+ 	for (i = 0; i < BB_VGA_LEVEL; i++) {
+ 		if (dbm < priv->ldBmThreshold[i]) {
+ 			priv->byBBVGANew = priv->abyBBVGA[i];
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (priv->byBBVGANew == priv->byBBVGACurrent) {
+ 		priv->uBBVGADiffCount = 1;
+ 		return;
+ 	}
+ 
+ 	priv->uBBVGADiffCount++;
+ 
+ 	if (priv->uBBVGADiffCount == 1) {
+ 		/* first VGA diff gain */
+ 		BBvSetVGAGainOffset(priv, priv->byBBVGANew);
+ 
+ 		dev_dbg(&priv->pcid->dev,
+ 			"First RSSI[%d] NewGain[%d] OldGain[%d] Count[%d]\n",
+ 			(int)dbm, priv->byBBVGANew,
+ 			priv->byBBVGACurrent,
+ 			(int)priv->uBBVGADiffCount);
+ 	}
+ 
+ 	if (priv->uBBVGADiffCount >= BB_VGA_CHANGE_THRESHOLD) {
+ 		dev_dbg(&priv->pcid->dev,
+ 			"RSSI[%d] NewGain[%d] OldGain[%d] Count[%d]\n",
+ 			(int)dbm, priv->byBBVGANew,
+ 			priv->byBBVGACurrent,
+ 			(int)priv->uBBVGADiffCount);
+ 
+ 		BBvSetVGAGainOffset(priv, priv->byBBVGANew);
+ 	}
+ }
+ 
+ static void vnt_interrupt_process(struct vnt_private *priv)
+ {
+ 	struct ieee80211_low_level_stats *low_stats = &priv->low_stats;
+ 	int             max_count = 0;
+ 	u32 mib_counter;
+ 	u32 isr;
+ 	unsigned long flags;
+ 
+ 	MACvReadISR(priv->PortOffset, &isr);
+ 
+ 	if (isr == 0)
+ 		return;
+ 
+ 	if (isr == 0xffffffff) {
+ 		pr_debug("isr = 0xffff\n");
+ 		return;
+ 	}
+ 
+ 	MACvIntDisable(priv->PortOffset);
+ 
+ 	spin_lock_irqsave(&priv->lock, flags);
+ 
+ 	/* Read low level stats */
+ 	MACvReadMIBCounter(priv->PortOffset, &mib_counter);
+ 
+ 	low_stats->dot11RTSSuccessCount += mib_counter & 0xff;
+ 	low_stats->dot11RTSFailureCount += (mib_counter >> 8) & 0xff;
+ 	low_stats->dot11ACKFailureCount += (mib_counter >> 16) & 0xff;
+ 	low_stats->dot11FCSErrorCount += (mib_counter >> 24) & 0xff;
+ 
+ 	/*
+ 	 * TBD....
+ 	 * Must do this after doing rx/tx, cause ISR bit is slow
+ 	 * than RD/TD write back
+ 	 * update ISR counter
+ 	 */
+ 	while (isr && priv->vif) {
+ 		MACvWriteISR(priv->PortOffset, isr);
+ 
+ 		if (isr & ISR_FETALERR) {
+ 			pr_debug(" ISR_FETALERR\n");
+ 			VNSvOutPortB(priv->PortOffset + MAC_REG_SOFTPWRCTL, 0);
+ 			VNSvOutPortW(priv->PortOffset +
+ 				     MAC_REG_SOFTPWRCTL, SOFTPWRCTL_SWPECTI);
+ 			device_error(priv, isr);
+ 		}
+ 
+ 		if (isr & ISR_TBTT) {
+ 			if (priv->op_mode != NL80211_IFTYPE_ADHOC)
+ 				vnt_check_bb_vga(priv);
+ 
+ 			priv->bBeaconSent = false;
+ 			if (priv->bEnablePSMode)
+ 				PSbIsNextTBTTWakeUp((void *)priv);
+ 
+ 			if ((priv->op_mode == NL80211_IFTYPE_AP ||
+ 			    priv->op_mode == NL80211_IFTYPE_ADHOC) &&
+ 			    priv->vif->bss_conf.enable_beacon) {
+ 				MACvOneShotTimer1MicroSec(priv,
+ 							  (priv->vif->bss_conf.beacon_int - MAKE_BEACON_RESERVED) << 10);
+ 			}
+ 
+ 			/* TODO: adhoc PS mode */
+ 		}
+ 
+ 		if (isr & ISR_BNTX) {
+ 			if (priv->op_mode == NL80211_IFTYPE_ADHOC) {
+ 				priv->bIsBeaconBufReadySet = false;
+ 				priv->cbBeaconBufReadySetCnt = 0;
+ 			}
+ 
+ 			priv->bBeaconSent = true;
+ 		}
+ 
+ 		if (isr & ISR_RXDMA0)
+ 			max_count += device_rx_srv(priv, TYPE_RXDMA0);
+ 
+ 		if (isr & ISR_RXDMA1)
+ 			max_count += device_rx_srv(priv, TYPE_RXDMA1);
+ 
+ 		if (isr & ISR_TXDMA0)
+ 			max_count += device_tx_srv(priv, TYPE_TXDMA0);
+ 
+ 		if (isr & ISR_AC0DMA)
+ 			max_count += device_tx_srv(priv, TYPE_AC0DMA);
+ 
+ 		if (isr & ISR_SOFTTIMER1) {
+ 			if (priv->vif->bss_conf.enable_beacon)
+ 				vnt_beacon_make(priv, priv->vif);
+ 		}
+ 
+ 		/* If both buffers available wake the queue */
+ 		if (AVAIL_TD(priv, TYPE_TXDMA0) &&
+ 		    AVAIL_TD(priv, TYPE_AC0DMA) &&
+ 		    ieee80211_queue_stopped(priv->hw, 0))
+ 			ieee80211_wake_queues(priv->hw);
+ 
+ 		MACvReadISR(priv->PortOffset, &isr);
+ 
+ 		MACvReceive0(priv->PortOffset);
+ 		MACvReceive1(priv->PortOffset);
+ 
+ 		if (max_count > priv->opts.int_works)
+ 			break;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&priv->lock, flags);
+ 
+ 	MACvIntEnable(priv->PortOffset, IMR_MASK_VALUE);
+ }
+ 
+ static void vnt_interrupt_work(struct work_struct *work)
+ {
+ 	struct vnt_private *priv =
+ 		container_of(work, struct vnt_private, interrupt_work);
+ 
+ 	if (priv->vif)
+ 		vnt_interrupt_process(priv);
+ }
+ 
+ static irqreturn_t vnt_interrupt(int irq,  void *arg)
+ {
+ 	struct vnt_private *priv = arg;
+ 
+ 	if (priv->vif)
+ 		schedule_work(&priv->interrupt_work);
+ 
+ 	return IRQ_HANDLED;
+ }
+ 
+ static int vnt_tx_packet(struct vnt_private *priv, struct sk_buff *skb)
+ {
+ 	struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;
+ 	struct vnt_tx_desc *head_td;
+ 	u32 dma_idx;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&priv->lock, flags);
+ 
+ 	if (ieee80211_is_data(hdr->frame_control))
+ 		dma_idx = TYPE_AC0DMA;
+ 	else
+ 		dma_idx = TYPE_TXDMA0;
+ 
+ 	if (AVAIL_TD(priv, dma_idx) < 1) {
+ 		spin_unlock_irqrestore(&priv->lock, flags);
+ 		ieee80211_stop_queues(priv->hw);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	head_td = priv->apCurrTD[dma_idx];
+ 
+ 	head_td->td1.tcr = 0;
+ 
+ 	head_td->td_info->skb = skb;
+ 
+ 	if (dma_idx == TYPE_AC0DMA)
+ 		head_td->td_info->flags = TD_FLAGS_NETIF_SKB;
+ 
+ 	priv->apCurrTD[dma_idx] = head_td->next;
+ 
+ 	spin_unlock_irqrestore(&priv->lock, flags);
+ 
+ 	vnt_generate_fifo_header(priv, dma_idx, head_td, skb);
+ 
+ 	spin_lock_irqsave(&priv->lock, flags);
+ 
+ 	priv->bPWBitOn = false;
+ 
+ 	/* Set TSR1 & ReqCount in TxDescHead */
+ 	head_td->td1.tcr |= (TCR_STP | TCR_EDP | EDMSDU);
+ 	head_td->td1.req_count = cpu_to_le16(head_td->td_info->req_count);
+ 
+ 	head_td->buff_addr = cpu_to_le32(head_td->td_info->buf_dma);
+ 
+ 	/* Poll Transmit the adapter */
+ 	wmb();
+ 	head_td->td0.owner = OWNED_BY_NIC;
+ 	wmb(); /* second memory barrier */
+ 
+ 	if (head_td->td_info->flags & TD_FLAGS_NETIF_SKB)
+ 		MACvTransmitAC0(priv->PortOffset);
+ 	else
+ 		MACvTransmit0(priv->PortOffset);
+ 
+ 	priv->iTDUsed[dma_idx]++;
+ 
+ 	spin_unlock_irqrestore(&priv->lock, flags);
+ 
+ 	return 0;
+ }
+ 
+ static void vnt_tx_80211(struct ieee80211_hw *hw,
+ 			 struct ieee80211_tx_control *control,
+ 			 struct sk_buff *skb)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 
+ 	if (vnt_tx_packet(priv, skb))
+ 		ieee80211_free_txskb(hw, skb);
+ }
+ 
+ static int vnt_start(struct ieee80211_hw *hw)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 	int ret;
+ 
+ 	priv->rx_buf_sz = PKT_BUF_SZ;
+ 	if (!device_init_rings(priv))
+ 		return -ENOMEM;
+ 
+ 	ret = request_irq(priv->pcid->irq, vnt_interrupt,
+ 			  IRQF_SHARED, "vt6655", priv);
+ 	if (ret) {
+ 		dev_dbg(&priv->pcid->dev, "failed to start irq\n");
+ 		goto err_free_rings;
+ 	}
+ 
+ 	dev_dbg(&priv->pcid->dev, "call device init rd0 ring\n");
+ 	ret = device_init_rd0_ring(priv);
+ 	if (ret)
+ 		goto err_free_irq;
+ 	ret = device_init_rd1_ring(priv);
+ 	if (ret)
+ 		goto err_free_rd0_ring;
+ 	ret = device_init_td0_ring(priv);
+ 	if (ret)
+ 		goto err_free_rd1_ring;
+ 	ret = device_init_td1_ring(priv);
+ 	if (ret)
+ 		goto err_free_td0_ring;
+ 
+ 	device_init_registers(priv);
+ 
+ 	dev_dbg(&priv->pcid->dev, "call MACvIntEnable\n");
+ 	MACvIntEnable(priv->PortOffset, IMR_MASK_VALUE);
+ 
+ 	ieee80211_wake_queues(hw);
+ 
+ 	return 0;
+ 
+ err_free_td0_ring:
+ 	device_free_td0_ring(priv);
+ err_free_rd1_ring:
+ 	device_free_rd1_ring(priv);
+ err_free_rd0_ring:
+ 	device_free_rd0_ring(priv);
+ err_free_irq:
+ 	free_irq(priv->pcid->irq, priv);
+ err_free_rings:
+ 	device_free_rings(priv);
+ 	return ret;
+ }
+ 
+ static void vnt_stop(struct ieee80211_hw *hw)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 
+ 	ieee80211_stop_queues(hw);
+ 
+ 	cancel_work_sync(&priv->interrupt_work);
+ 
+ 	MACbShutdown(priv);
+ 	MACbSoftwareReset(priv);
+ 	CARDbRadioPowerOff(priv);
+ 
+ 	device_free_td0_ring(priv);
+ 	device_free_td1_ring(priv);
+ 	device_free_rd0_ring(priv);
+ 	device_free_rd1_ring(priv);
+ 	device_free_rings(priv);
+ 
+ 	free_irq(priv->pcid->irq, priv);
+ }
+ 
+ static int vnt_add_interface(struct ieee80211_hw *hw, struct ieee80211_vif *vif)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 
+ 	priv->vif = vif;
+ 
+ 	switch (vif->type) {
+ 	case NL80211_IFTYPE_STATION:
+ 		break;
+ 	case NL80211_IFTYPE_ADHOC:
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_RCR, RCR_UNICAST);
+ 
+ 		MACvRegBitsOn(priv->PortOffset, MAC_REG_HOSTCR, HOSTCR_ADHOC);
+ 
+ 		break;
+ 	case NL80211_IFTYPE_AP:
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_RCR, RCR_UNICAST);
+ 
+ 		MACvRegBitsOn(priv->PortOffset, MAC_REG_HOSTCR, HOSTCR_AP);
+ 
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	priv->op_mode = vif->type;
+ 
+ 	return 0;
+ }
+ 
+ static void vnt_remove_interface(struct ieee80211_hw *hw,
+ 				 struct ieee80211_vif *vif)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 
+ 	switch (vif->type) {
+ 	case NL80211_IFTYPE_STATION:
+ 		break;
+ 	case NL80211_IFTYPE_ADHOC:
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_TCR, TCR_AUTOBCNTX);
+ 		MACvRegBitsOff(priv->PortOffset,
+ 			       MAC_REG_TFTCTL, TFTCTL_TSFCNTREN);
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_HOSTCR, HOSTCR_ADHOC);
+ 		break;
+ 	case NL80211_IFTYPE_AP:
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_TCR, TCR_AUTOBCNTX);
+ 		MACvRegBitsOff(priv->PortOffset,
+ 			       MAC_REG_TFTCTL, TFTCTL_TSFCNTREN);
+ 		MACvRegBitsOff(priv->PortOffset, MAC_REG_HOSTCR, HOSTCR_AP);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	priv->op_mode = NL80211_IFTYPE_UNSPECIFIED;
+ }
+ 
+ static int vnt_config(struct ieee80211_hw *hw, u32 changed)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 	struct ieee80211_conf *conf = &hw->conf;
+ 	u8 bb_type;
+ 
+ 	if (changed & IEEE80211_CONF_CHANGE_PS) {
+ 		if (conf->flags & IEEE80211_CONF_PS)
+ 			PSvEnablePowerSaving(priv, conf->listen_interval);
+ 		else
+ 			PSvDisablePowerSaving(priv);
+ 	}
+ 
+ 	if ((changed & IEEE80211_CONF_CHANGE_CHANNEL) ||
+ 	    (conf->flags & IEEE80211_CONF_OFFCHANNEL)) {
+ 		set_channel(priv, conf->chandef.chan);
+ 
+ 		if (conf->chandef.chan->band == NL80211_BAND_5GHZ)
+ 			bb_type = BB_TYPE_11A;
+ 		else
+ 			bb_type = BB_TYPE_11G;
+ 
+ 		if (priv->byBBType != bb_type) {
+ 			priv->byBBType = bb_type;
+ 
+ 			CARDbSetPhyParameter(priv, priv->byBBType);
+ 		}
+ 	}
+ 
+ 	if (changed & IEEE80211_CONF_CHANGE_POWER) {
+ 		if (priv->byBBType == BB_TYPE_11B)
+ 			priv->wCurrentRate = RATE_1M;
+ 		else
+ 			priv->wCurrentRate = RATE_54M;
+ 
+ 		RFbSetPower(priv, priv->wCurrentRate,
+ 			    conf->chandef.chan->hw_value);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void vnt_bss_info_changed(struct ieee80211_hw *hw,
+ 				 struct ieee80211_vif *vif,
+ 				 struct ieee80211_bss_conf *conf, u32 changed)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 
+ 	priv->current_aid = conf->aid;
+ 
+ 	if (changed & BSS_CHANGED_BSSID && conf->bssid) {
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&priv->lock, flags);
+ 
+ 		MACvWriteBSSIDAddress(priv->PortOffset, (u8 *)conf->bssid);
+ 
+ 		spin_unlock_irqrestore(&priv->lock, flags);
+ 	}
+ 
+ 	if (changed & BSS_CHANGED_BASIC_RATES) {
+ 		priv->basic_rates = conf->basic_rates;
+ 
+ 		CARDvUpdateBasicTopRate(priv);
+ 
+ 		dev_dbg(&priv->pcid->dev,
+ 			"basic rates %x\n", conf->basic_rates);
+ 	}
+ 
+ 	if (changed & BSS_CHANGED_ERP_PREAMBLE) {
+ 		if (conf->use_short_preamble) {
+ 			MACvEnableBarkerPreambleMd(priv->PortOffset);
+ 			priv->byPreambleType = true;
+ 		} else {
+ 			MACvDisableBarkerPreambleMd(priv->PortOffset);
+ 			priv->byPreambleType = false;
+ 		}
+ 	}
+ 
+ 	if (changed & BSS_CHANGED_ERP_CTS_PROT) {
+ 		if (conf->use_cts_prot)
+ 			MACvEnableProtectMD(priv->PortOffset);
+ 		else
+ 			MACvDisableProtectMD(priv->PortOffset);
+ 	}
+ 
+ 	if (changed & BSS_CHANGED_ERP_SLOT) {
+ 		if (conf->use_short_slot)
+ 			priv->bShortSlotTime = true;
+ 		else
+ 			priv->bShortSlotTime = false;
+ 
+ 		CARDbSetPhyParameter(priv, priv->byBBType);
+ 		BBvSetVGAGainOffset(priv, priv->abyBBVGA[0]);
+ 	}
+ 
+ 	if (changed & BSS_CHANGED_TXPOWER)
+ 		RFbSetPower(priv, priv->wCurrentRate,
+ 			    conf->chandef.chan->hw_value);
+ 
+ 	if (changed & BSS_CHANGED_BEACON_ENABLED) {
+ 		dev_dbg(&priv->pcid->dev,
+ 			"Beacon enable %d\n", conf->enable_beacon);
+ 
+ 		if (conf->enable_beacon) {
+ 			vnt_beacon_enable(priv, vif, conf);
+ 
+ 			MACvRegBitsOn(priv->PortOffset, MAC_REG_TCR,
+ 				      TCR_AUTOBCNTX);
+ 		} else {
+ 			MACvRegBitsOff(priv->PortOffset, MAC_REG_TCR,
+ 				       TCR_AUTOBCNTX);
+ 		}
+ 	}
+ 
+ 	if (changed & (BSS_CHANGED_ASSOC | BSS_CHANGED_BEACON_INFO) &&
+ 	    priv->op_mode != NL80211_IFTYPE_AP) {
+ 		if (conf->assoc && conf->beacon_rate) {
+ 			CARDbUpdateTSF(priv, conf->beacon_rate->hw_value,
+ 				       conf->sync_tsf);
+ 
+ 			CARDbSetBeaconPeriod(priv, conf->beacon_int);
+ 
+ 			CARDvSetFirstNextTBTT(priv, conf->beacon_int);
+ 		} else {
+ 			VNSvOutPortB(priv->PortOffset + MAC_REG_TFTCTL,
+ 				     TFTCTL_TSFCNTRST);
+ 			VNSvOutPortB(priv->PortOffset + MAC_REG_TFTCTL,
+ 				     TFTCTL_TSFCNTREN);
+ 		}
+ 	}
+ }
+ 
+ static u64 vnt_prepare_multicast(struct ieee80211_hw *hw,
+ 				 struct netdev_hw_addr_list *mc_list)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 	struct netdev_hw_addr *ha;
+ 	u64 mc_filter = 0;
+ 	u32 bit_nr = 0;
+ 
+ 	netdev_hw_addr_list_for_each(ha, mc_list) {
+ 		bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;
+ 
+ 		mc_filter |= 1ULL << (bit_nr & 0x3f);
+ 	}
+ 
+ 	priv->mc_list_count = mc_list->count;
+ 
+ 	return mc_filter;
+ }
+ 
+ static void vnt_configure(struct ieee80211_hw *hw,
+ 			  unsigned int changed_flags,
+ 			  unsigned int *total_flags, u64 multicast)
+ {
+ 	struct vnt_private *priv = hw->priv;
+ 	u8 rx_mode = 0;
+ 
+ 	*total_flags &= FIF_ALLMULTI | FIF_OTHER_BSS | FIF_BCN_PRBRESP_PROMISC;
+ 
+ 	VNSvInPortB(priv->PortOffset + MAC_REG_RCR, &rx_mode);
+ 
+ 	dev_dbg(&priv->pcid->dev, "rx mode in = %x\n", rx_mode);
+ 
+ 	if (changed_flags & FIF_ALLMULTI) {
+ 		if (*total_flags & FIF_ALLMULTI) {
+ 			unsigned long flags;
+ 
+ 			spin_lock_irqsave(&priv->lock, flags);
+ 
+ 			if (priv->mc_list_count > 2) {
+ 				MACvSelectPage1(priv->PortOffset);
+ 
+ 				VNSvOutPortD(priv->PortOffset +
+ 					     MAC_REG_MAR0, 0xffffffff);
+ 				VNSvOutPortD(priv->PortOffset +
+ 					    MAC_REG_MAR0 + 4, 0xffffffff);
+ 
+ 				MACvSelectPage0(priv->PortOffset);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  			} else {
 -				MACvSelectPage1(priv->PortOffset);
 -
 -				VNSvOutPortD(priv->PortOffset +
 -					     MAC_REG_MAR0, (u32)multicast);
 -				VNSvOutPortD(priv->PortOffset +
 -					     MAC_REG_MAR0 + 4,
 -					     (u32)(multicast >> 32));
 +				pDevice->byTxAntennaMode = ANT_B;
 +				if (pDevice->bTxRxAntInv == true)
 +					pDevice->byRxAntennaMode = ANT_A;
 +				else
 +					pDevice->byRxAntennaMode = ANT_B;
 +			}
 +		}
 +		DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "bDiversityEnable=[%d],NValue=[%d],MValue=[%d],TMax=[%d],TMax2=[%d]\n",
 +			pDevice->bDiversityEnable, (int)pDevice->ulDiversityNValue, (int)pDevice->ulDiversityMValue, pDevice->byTMax, pDevice->byTMax2);
 +
 +//#ifdef ZoneType_DefaultSetting
 +//2008-8-4 <add> by chester
 +//zonetype initial
 +		pDevice->byOriginalZonetype = pDevice->abyEEPROM[EEP_OFS_ZONETYPE];
 +		zonetype = Config_FileOperation(pDevice, false, NULL);
 +		if (zonetype >= 0) {         //read zonetype file ok!
 +			if ((zonetype == 0) &&
 +			    (pDevice->abyEEPROM[EEP_OFS_ZONETYPE] != 0x00)) {          //for USA
 +				pDevice->abyEEPROM[EEP_OFS_ZONETYPE] = 0;
 +				pDevice->abyEEPROM[EEP_OFS_MAXCHANNEL] = 0x0B;
 +				DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "Init Zone Type :USA\n");
 +			} else if ((zonetype == 1) &&
 +				 (pDevice->abyEEPROM[EEP_OFS_ZONETYPE] != 0x01)) {   //for Japan
 +				pDevice->abyEEPROM[EEP_OFS_ZONETYPE] = 0x01;
 +				pDevice->abyEEPROM[EEP_OFS_MAXCHANNEL] = 0x0D;
 +			} else if ((zonetype == 2) &&
 +				 (pDevice->abyEEPROM[EEP_OFS_ZONETYPE] != 0x02)) {   //for Europe
 +				pDevice->abyEEPROM[EEP_OFS_ZONETYPE] = 0x02;
 +				pDevice->abyEEPROM[EEP_OFS_MAXCHANNEL] = 0x0D;
 +				DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "Init Zone Type :Europe\n");
 +			}
  
 -				MACvSelectPage0(priv->PortOffset);
 +			else {
 +				if (zonetype != pDevice->abyEEPROM[EEP_OFS_ZONETYPE])
 +					printk("zonetype in file[%02x] mismatch with in EEPROM[%02x]\n", zonetype, pDevice->abyEEPROM[EEP_OFS_ZONETYPE]);
 +				else
 +					printk("Read Zonetype file success,use default zonetype setting[%02x]\n", zonetype);
  			}
 +		} else
 +			printk("Read Zonetype file fail,use default zonetype setting[%02x]\n", SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_ZONETYPE));
  
 -			spin_unlock_irqrestore(&priv->lock, flags);
 +		// Get RFType
 +		pDevice->byRFType = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_RFTYPE);
  
 -			rx_mode |= RCR_MULTICAST | RCR_BROADCAST;
 -		} else {
 -			rx_mode &= ~(RCR_MULTICAST | RCR_BROADCAST);
 +		if ((pDevice->byRFType & RF_EMU) != 0) {
 +			// force change RevID for VT3253 emu
 +			pDevice->byRevId = 0x80;
  		}
 -	}
  
 -	if (changed_flags & (FIF_OTHER_BSS | FIF_BCN_PRBRESP_PROMISC)) {
 -		rx_mode |= RCR_MULTICAST | RCR_BROADCAST;
 +		pDevice->byRFType &= RF_MASK;
 +		DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "pDevice->byRFType = %x\n", pDevice->byRFType);
  
 -		if (*total_flags & (FIF_OTHER_BSS | FIF_BCN_PRBRESP_PROMISC))
 -			rx_mode &= ~RCR_BSSID;
 -		else
 -			rx_mode |= RCR_BSSID;
 -	}
 +		if (pDevice->bZoneRegExist == false) {
 +			pDevice->byZoneType = pDevice->abyEEPROM[EEP_OFS_ZONETYPE];
 +		}
 +		DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "pDevice->byZoneType = %x\n", pDevice->byZoneType);
  
 -	VNSvOutPortB(priv->PortOffset + MAC_REG_RCR, rx_mode);
 +		//Init RF module
 +		RFbInit(pDevice);
  
 -	dev_dbg(&priv->pcid->dev, "rx mode out= %x\n", rx_mode);
 -}
 +		//Get Desire Power Value
 +		pDevice->byCurPwr = 0xFF;
 +		pDevice->byCCKPwr = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_PWR_CCK);
 +		pDevice->byOFDMPwrG = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_PWR_OFDMG);
 +		//byCCKPwrdBm = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_CCK_PWR_dBm);
  
 -static int vnt_set_key(struct ieee80211_hw *hw, enum set_key_cmd cmd,
 -		       struct ieee80211_vif *vif, struct ieee80211_sta *sta,
 -		       struct ieee80211_key_conf *key)
 -{
 -	struct vnt_private *priv = hw->priv;
 +		//byOFDMPwrdBm = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_OFDM_PWR_dBm);
  
 -	switch (cmd) {
 -	case SET_KEY:
 -		if (vnt_set_keys(hw, sta, vif, key))
 -			return -EOPNOTSUPP;
 -		break;
 -	case DISABLE_KEY:
 -		if (test_bit(key->hw_key_idx, &priv->key_entry_inuse))
 -			clear_bit(key->hw_key_idx, &priv->key_entry_inuse);
 -	default:
 -		break;
 -	}
 +		// Load power Table
  
 -	return 0;
 -}
 +		for (ii = 0; ii < CB_MAX_CHANNEL_24G; ii++) {
 +			pDevice->abyCCKPwrTbl[ii + 1] = SROMbyReadEmbedded(pDevice->PortOffset, (unsigned char)(ii + EEP_OFS_CCK_PWR_TBL));
 +			if (pDevice->abyCCKPwrTbl[ii + 1] == 0) {
 +				pDevice->abyCCKPwrTbl[ii+1] = pDevice->byCCKPwr;
 +			}
 +			pDevice->abyOFDMPwrTbl[ii + 1] = SROMbyReadEmbedded(pDevice->PortOffset, (unsigned char)(ii + EEP_OFS_OFDM_PWR_TBL));
 +			if (pDevice->abyOFDMPwrTbl[ii + 1] == 0) {
 +				pDevice->abyOFDMPwrTbl[ii + 1] = pDevice->byOFDMPwrG;
 +			}
 +			pDevice->abyCCKDefaultPwr[ii + 1] = byCCKPwrdBm;
 +			pDevice->abyOFDMDefaultPwr[ii + 1] = byOFDMPwrdBm;
 +		}
 +		//2008-8-4 <add> by chester
 +		//recover 12,13 ,14channel for EUROPE by 11 channel
 +		if (((pDevice->abyEEPROM[EEP_OFS_ZONETYPE] == ZoneType_Japan) ||
 +		     (pDevice->abyEEPROM[EEP_OFS_ZONETYPE] == ZoneType_Europe)) &&
 +		    (pDevice->byOriginalZonetype == ZoneType_USA)) {
 +			for (ii = 11; ii < 14; ii++) {
 +				pDevice->abyCCKPwrTbl[ii] = pDevice->abyCCKPwrTbl[10];
 +				pDevice->abyOFDMPwrTbl[ii] = pDevice->abyOFDMPwrTbl[10];
  
 -static int vnt_get_stats(struct ieee80211_hw *hw,
 -			 struct ieee80211_low_level_stats *stats)
 -{
 -	struct vnt_private *priv = hw->priv;
 +			}
 +		}
  
 -	memcpy(stats, &priv->low_stats, sizeof(*stats));
 +		// Load OFDM A Power Table
 +		for (ii = 0; ii < CB_MAX_CHANNEL_5G; ii++) { //RobertYu:20041224, bug using CB_MAX_CHANNEL
 +			pDevice->abyOFDMPwrTbl[ii + CB_MAX_CHANNEL_24G + 1] = SROMbyReadEmbedded(pDevice->PortOffset, (unsigned char)(ii + EEP_OFS_OFDMA_PWR_TBL));
 +			pDevice->abyOFDMDefaultPwr[ii + CB_MAX_CHANNEL_24G + 1] = SROMbyReadEmbedded(pDevice->PortOffset, (unsigned char)(ii + EEP_OFS_OFDMA_PWR_dBm));
 +		}
 +		init_channel_table((void *)pDevice);
  
 -	return 0;
 -}
 +		if (pDevice->byLocalID > REV_ID_VT3253_B1) {
 +			MACvSelectPage1(pDevice->PortOffset);
 +			VNSvOutPortB(pDevice->PortOffset + MAC_REG_MSRCTL + 1, (MSRCTL1_TXPWR | MSRCTL1_CSAPAREN));
 +			MACvSelectPage0(pDevice->PortOffset);
 +		}
  
 -static u64 vnt_get_tsf(struct ieee80211_hw *hw, struct ieee80211_vif *vif)
 -{
 -	struct vnt_private *priv = hw->priv;
 -	u64 tsf;
 +		// use relative tx timeout and 802.11i D4
 +		MACvWordRegBitsOn(pDevice->PortOffset, MAC_REG_CFG, (CFG_TKIPOPT | CFG_NOTXTIMEOUT));
  
 -	CARDbGetCurrentTSF(priv, &tsf);
 +		// set performance parameter by registry
 +		MACvSetShortRetryLimit(pDevice->PortOffset, pDevice->byShortRetryLimit);
 +		MACvSetLongRetryLimit(pDevice->PortOffset, pDevice->byLongRetryLimit);
  
 -	return tsf;
 -}
 +		// reset TSF counter
 +		VNSvOutPortB(pDevice->PortOffset + MAC_REG_TFTCTL, TFTCTL_TSFCNTRST);
 +		// enable TSF counter
 +		VNSvOutPortB(pDevice->PortOffset + MAC_REG_TFTCTL, TFTCTL_TSFCNTREN);
  
 -static void vnt_set_tsf(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
 -			u64 tsf)
 -{
 -	struct vnt_private *priv = hw->priv;
 +		// initialize BBP registers
 +		BBbVT3253Init(pDevice);
  
 -	CARDvUpdateNextTBTT(priv, tsf, vif->bss_conf.beacon_int);
 -}
 +		if (pDevice->bUpdateBBVGA) {
 +			pDevice->byBBVGACurrent = pDevice->abyBBVGA[0];
 +			pDevice->byBBVGANew = pDevice->byBBVGACurrent;
 +			BBvSetVGAGainOffset(pDevice, pDevice->abyBBVGA[0]);
 +		}
 +		BBvSetRxAntennaMode(pDevice->PortOffset, pDevice->byRxAntennaMode);
 +		BBvSetTxAntennaMode(pDevice->PortOffset, pDevice->byTxAntennaMode);
  
 -static void vnt_reset_tsf(struct ieee80211_hw *hw, struct ieee80211_vif *vif)
 -{
 -	struct vnt_private *priv = hw->priv;
 +		pDevice->byCurrentCh = 0;
  
 -	/* reset TSF counter */
 -	VNSvOutPortB(priv->PortOffset + MAC_REG_TFTCTL, TFTCTL_TSFCNTRST);
 +		//pDevice->NetworkType = Ndis802_11Automode;
 +		// Set BB and packet type at the same time.
 +		// Set Short Slot Time, xIFS, and RSPINF.
 +		if (pDevice->uConnectionRate == RATE_AUTO) {
 +			pDevice->wCurrentRate = RATE_54M;
 +		} else {
 +			pDevice->wCurrentRate = (unsigned short)pDevice->uConnectionRate;
 +		}
 +
 +		// default G Mode
 +		VNTWIFIbConfigPhyMode(pDevice->pMgmt, PHY_TYPE_11G);
 +		VNTWIFIbConfigPhyMode(pDevice->pMgmt, PHY_TYPE_AUTO);
 +
 +		pDevice->bRadioOff = false;
 +
 +		pDevice->byRadioCtl = SROMbyReadEmbedded(pDevice->PortOffset, EEP_OFS_RADIOCTL);
 +		pDevice->bHWRadioOff = false;
 +
 +		if (pDevice->byRadioCtl & EEP_RADIOCTL_ENABLE) {
 +			// Get GPIO
 +			MACvGPIOIn(pDevice->PortOffset, &pDevice->byGPIO);
 +//2008-4-14 <add> by chester for led issue
 +#ifdef FOR_LED_ON_NOTEBOOK
 +			if (pDevice->byGPIO & GPIO0_DATA) { pDevice->bHWRadioOff = true; }
 +			if (!(pDevice->byGPIO & GPIO0_DATA)) { pDevice->bHWRadioOff = false; }
 +
 +		}
 +		if ((pDevice->bRadioControlOff == true)) {
 +			CARDbRadioPowerOff(pDevice);
 +		} else  CARDbRadioPowerOn(pDevice);
 +#else
 +		if (((pDevice->byGPIO & GPIO0_DATA) && !(pDevice->byRadioCtl & EEP_RADIOCTL_INV)) ||
 +		    (!(pDevice->byGPIO & GPIO0_DATA) && (pDevice->byRadioCtl & EEP_RADIOCTL_INV))) {
 +			pDevice->bHWRadioOff = true;
 +		}
 +	}
 +	if ((pDevice->bHWRadioOff == true) || (pDevice->bRadioControlOff == true)) {
 +		CARDbRadioPowerOff(pDevice);
 +	}
 +
 +#endif
 +}
 +pMgmt->eScanType = WMAC_SCAN_PASSIVE;
 +// get Permanent network address
 +SROMvReadEtherAddress(pDevice->PortOffset, pDevice->abyCurrentNetAddr);
 +DBG_PRT(MSG_LEVEL_DEBUG, KERN_INFO "Network address = %pM\n",
 +	pDevice->abyCurrentNetAddr);
 +
 +// reset Tx pointer
 +CARDvSafeResetRx(pDevice);
 +// reset Rx pointer
 +CARDvSafeResetTx(pDevice);
 +
 +if (pDevice->byLocalID <= REV_ID_VT3253_A1) {
 +	MACvRegBitsOn(pDevice->PortOffset, MAC_REG_RCR, RCR_WPAERR);
  }
  
 -static const struct ieee80211_ops vnt_mac_ops = {
 -	.tx			= vnt_tx_80211,
 -	.start			= vnt_start,
 -	.stop			= vnt_stop,
 -	.add_interface		= vnt_add_interface,
 -	.remove_interface	= vnt_remove_interface,
 -	.config			= vnt_config,
 -	.bss_info_changed	= vnt_bss_info_changed,
 -	.prepare_multicast	= vnt_prepare_multicast,
 -	.configure_filter	= vnt_configure,
 -	.set_key		= vnt_set_key,
 -	.get_stats		= vnt_get_stats,
 -	.get_tsf		= vnt_get_tsf,
 -	.set_tsf		= vnt_set_tsf,
 -	.reset_tsf		= vnt_reset_tsf,
 -};
 +pDevice->eEncryptionStatus = Ndis802_11EncryptionDisabled;
  
 -static int vnt_init(struct vnt_private *priv)
 -{
 -	SET_IEEE80211_PERM_ADDR(priv->hw, priv->abyCurrentNetAddr);
 +// Turn On Rx DMA
 +MACvReceive0(pDevice->PortOffset);
 +MACvReceive1(pDevice->PortOffset);
  
 -	vnt_init_bands(priv);
 +// start the adapter
 +MACvStart(pDevice->PortOffset);
  
 -	if (ieee80211_register_hw(priv->hw))
 -		return -ENODEV;
 +netif_stop_queue(pDevice->dev);
 +}
  
 -	priv->mac_hw = true;
 +static void device_init_diversity_timer(PSDevice pDevice) {
 +	init_timer(&pDevice->TimerSQ3Tmax1);
 +	pDevice->TimerSQ3Tmax1.data = (unsigned long) pDevice;
 +	pDevice->TimerSQ3Tmax1.function = (TimerFunction)TimerSQ3CallBack;
 +	pDevice->TimerSQ3Tmax1.expires = RUN_AT(HZ);
  
 -	CARDbRadioPowerOff(priv);
 +	init_timer(&pDevice->TimerSQ3Tmax2);
 +	pDevice->TimerSQ3Tmax2.data = (unsigned long) pDevice;
 +	pDevice->TimerSQ3Tmax2.function = (TimerFunction)TimerSQ3CallBack;
 +	pDevice->TimerSQ3Tmax2.expires = RUN_AT(HZ);
  
 -	return 0;
 +	init_timer(&pDevice->TimerSQ3Tmax3);
 +	pDevice->TimerSQ3Tmax3.data = (unsigned long) pDevice;
 +	pDevice->TimerSQ3Tmax3.function = (TimerFunction)TimerState1CallBack;
 +	pDevice->TimerSQ3Tmax3.expires = RUN_AT(HZ);
 +
 +	return;
 +}
 +
 +static bool device_release_WPADEV(PSDevice pDevice)
 +{
 +	viawget_wpa_header *wpahdr;
 +	int ii = 0;
 +	// wait_queue_head_t	Set_wait;
 +	//send device close to wpa_supplicnat layer
 +	if (pDevice->bWPADEVUp == true) {
 +		wpahdr = (viawget_wpa_header *)pDevice->skb->data;
 +		wpahdr->type = VIAWGET_DEVICECLOSE_MSG;
 +		wpahdr->resp_ie_len = 0;
 +		wpahdr->req_ie_len = 0;
 +		skb_put(pDevice->skb, sizeof(viawget_wpa_header));
 +		pDevice->skb->dev = pDevice->wpadev;
 +		skb_reset_mac_header(pDevice->skb);
 +		pDevice->skb->pkt_type = PACKET_HOST;
 +		pDevice->skb->protocol = htons(ETH_P_802_2);
 +		memset(pDevice->skb->cb, 0, sizeof(pDevice->skb->cb));
 +		netif_rx(pDevice->skb);
 +		pDevice->skb = dev_alloc_skb((int)pDevice->rx_buf_sz);
 +
 +		//wait release WPADEV
 +		//    init_waitqueue_head(&Set_wait);
 +		//    wait_event_timeout(Set_wait, ((pDevice->wpadev==NULL)&&(pDevice->skb == NULL)),5*HZ);    //1s wait
 +		while ((pDevice->bWPADEVUp == true)) {
 +			set_current_state(TASK_UNINTERRUPTIBLE);
 +			schedule_timeout(HZ / 20);          //wait 50ms
 +			ii++;
 +			if (ii > 20)
 +				break;
 +		}
 +	}
 +	return true;
  }
  
 +static const struct net_device_ops device_netdev_ops = {
 +	.ndo_open               = device_open,
 +	.ndo_stop               = device_close,
 +	.ndo_do_ioctl           = device_ioctl,
 +	.ndo_get_stats          = device_get_stats,
 +	.ndo_start_xmit         = device_xmit,
 +	.ndo_set_rx_mode	= device_set_multi,
 +};
 +
  static int
  vt6655_probe(struct pci_dev *pcid, const struct pci_device_id *ent)
  {
diff --cc drivers/video/da8xx-fb.c
index 0810939936f4,43f2a4816860..000000000000
--- a/drivers/video/da8xx-fb.c
+++ b/drivers/video/da8xx-fb.c
@@@ -1344,11 -1446,9 +1344,17 @@@ static int fb_probe(struct platform_dev
  		da8xx_fb_fix.line_length - 1;
  
  	/* allocate palette buffer */
++<<<<<<< HEAD:drivers/video/da8xx-fb.c
 +	par->v_palette_base = dma_alloc_coherent(NULL,
 +					       PALETTE_SIZE,
 +					       (resource_size_t *)
 +					       &par->p_palette_base,
 +					       GFP_KERNEL | GFP_DMA);
++=======
+ 	par->v_palette_base = dma_alloc_coherent(NULL, PALETTE_SIZE,
+ 						 &par->p_palette_base,
+ 						 GFP_KERNEL | GFP_DMA);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent()):drivers/video/fbdev/da8xx-fb.c
  	if (!par->v_palette_base) {
  		dev_err(&device->dev,
  			"GLCD: kmalloc for palette buffer failed\n");
diff --cc include/linux/pci-dma-compat.h
index 1944b552e6ff,249d4d7fbf18..000000000000
--- a/include/linux/pci-dma-compat.h
+++ b/include/linux/pci-dma-compat.h
@@@ -29,8 -24,7 +29,12 @@@ static inline void 
  pci_zalloc_consistent(struct pci_dev *hwdev, size_t size,
  		      dma_addr_t *dma_handle)
  {
++<<<<<<< HEAD
 +	return dma_zalloc_coherent(hwdev == NULL ? NULL : &hwdev->dev,
 +				   size, dma_handle, GFP_ATOMIC);
++=======
+ 	return dma_alloc_coherent(&hwdev->dev, size, dma_handle, GFP_ATOMIC);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  }
  
  static inline void
diff --cc sound/aoa/soundbus/i2sbus/core.c
index cdbfc21211c4,40ebde2e1ab1..000000000000
--- a/sound/aoa/soundbus/i2sbus/core.c
+++ b/sound/aoa/soundbus/i2sbus/core.c
@@@ -45,15 -47,11 +45,22 @@@ static int alloc_dbdma_descriptor_ring(
  	/* We use the PCI APIs for now until the generic one gets fixed
  	 * enough or until we get some macio-specific versions
  	 */
++<<<<<<< HEAD
 +	r->space = dma_alloc_coherent(
 +			&macio_get_pci_dev(i2sdev->macio)->dev,
 +			r->size,
 +			&r->bus_addr,
 +			GFP_KERNEL);
++=======
+ 	r->space = dma_alloc_coherent(&macio_get_pci_dev(i2sdev->macio)->dev,
+ 				      r->size, &r->bus_addr, GFP_KERNEL);
+ 	if (!r->space)
+ 		return -ENOMEM;
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  
 +	if (!r->space) return -ENOMEM;
 +
 +	memset(r->space, 0, r->size);
  	r->cmds = (void*)DBDMA_ALIGN(r->space);
  	r->bus_cmd_start = r->bus_addr +
  			   (dma_addr_t)((char*)r->cmds - (char*)r->space);
diff --cc sound/sparc/dbri.c
index be1b1aa96b7e,9e71d7cda999..000000000000
--- a/sound/sparc/dbri.c
+++ b/sound/sparc/dbri.c
@@@ -2534,14 -2541,12 +2534,19 @@@ static int snd_dbri_create(struct snd_c
  	dbri->op = op;
  	dbri->irq = irq;
  
++<<<<<<< HEAD
 +	dbri->dma = dma_alloc_coherent(&op->dev,
 +				       sizeof(struct dbri_dma),
 +				       &dbri->dma_dvma, GFP_ATOMIC);
++=======
+ 	dbri->dma = dma_alloc_coherent(&op->dev, sizeof(struct dbri_dma),
+ 				       &dbri->dma_dvma, GFP_KERNEL);
++>>>>>>> 750afb08ca71 (cross-tree: phase out dma_zalloc_coherent())
  	if (!dbri->dma)
  		return -ENOMEM;
 +	memset((void *)dbri->dma, 0, sizeof(struct dbri_dma));
  
 -	dprintk(D_GEN, "DMA Cmd Block 0x%p (%pad)\n",
 +	dprintk(D_GEN, "DMA Cmd Block 0x%p (0x%08x)\n",
  		dbri->dma, dbri->dma_dvma);
  
  	/* Map the registers into memory. */
* Unmerged path drivers/block/skd_main.c
* Unmerged path drivers/crypto/cavium/cpt/cptpf_main.c
* Unmerged path drivers/crypto/cavium/cpt/cptvf_main.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_lib.c
* Unmerged path drivers/crypto/hisilicon/sec/sec_algs.c
* Unmerged path drivers/crypto/hisilicon/sec/sec_drv.c
* Unmerged path drivers/crypto/mediatek/mtk-platform.c
* Unmerged path drivers/dma/mediatek/mtk-hsdma.c
* Unmerged path drivers/dma/xgene-dma.c
* Unmerged path drivers/dma/xilinx/xilinx_dma.c
* Unmerged path drivers/dma/xilinx/zynqmp_dma.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_alloc.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v2.c
* Unmerged path drivers/input/touchscreen/raspberrypi-ts.c
* Unmerged path drivers/iommu/mtk_iommu_v1.c
* Unmerged path drivers/media/pci/intel/ipu3/ipu3-cio2.c
* Unmerged path drivers/media/platform/mtk-vcodec/mtk_vcodec_util.c
* Unmerged path drivers/net/ethernet/alacritech/slicoss.c
* Unmerged path drivers/net/ethernet/apm/xgene-v2/main.c
* Unmerged path drivers/net/ethernet/broadcom/bcmsysport.c
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_queues.c
* Unmerged path drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3pf/hclge_cmd.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3vf/hclgevf_cmd.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_api_cmd.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_eqs.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_io.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_qp.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_wq.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
* Unmerged path drivers/net/ethernet/mediatek/mtk_eth_soc.c
* Unmerged path drivers/net/ethernet/ni/nixge.c
* Unmerged path drivers/net/ethernet/qualcomm/emac/emac-mac.c
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
* Unmerged path drivers/net/ethernet/socionext/netsec.c
* Unmerged path drivers/net/wan/fsl_ucc_hdlc.c
* Unmerged path drivers/net/wireless/ath/wcn36xx/dxe.c
* Unmerged path drivers/net/wireless/ath/wil6210/txrx_edma.c
* Unmerged path drivers/ntb/hw/mscc/ntb_hw_switchtec.c
* Unmerged path drivers/pci/controller/pcie-iproc-msi.c
* Unmerged path drivers/pci/switch/switchtec.c
* Unmerged path drivers/s390/net/ism_drv.c
* Unmerged path drivers/soc/fsl/qbman/dpaa_sys.c
* Unmerged path drivers/spi/spi-pic32-sqi.c
* Unmerged path drivers/staging/mt7621-eth/mtk_eth_soc.c
* Unmerged path drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c
* Unmerged path drivers/usb/gadget/udc/bdc/bdc_core.c
* Unmerged path arch/mips/lantiq/xway/dma.c
* Unmerged path arch/powerpc/sysdev/fsl_rmu.c
* Unmerged path drivers/atm/he.c
* Unmerged path drivers/atm/idt77252.c
* Unmerged path drivers/block/skd_main.c
* Unmerged path drivers/crypto/cavium/cpt/cptpf_main.c
* Unmerged path drivers/crypto/cavium/cpt/cptvf_main.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_lib.c
diff --git a/drivers/crypto/ccp/ccp-dev-v5.c b/drivers/crypto/ccp/ccp-dev-v5.c
index 612898b4aaad..fe5404f7acb2 100644
--- a/drivers/crypto/ccp/ccp-dev-v5.c
+++ b/drivers/crypto/ccp/ccp-dev-v5.c
@@ -692,9 +692,9 @@ static int ccp5_init(struct ccp_device *ccp)
 		/* Page alignment satisfies our needs for N <= 128 */
 		BUILD_BUG_ON(COMMANDS_PER_QUEUE > 128);
 		cmd_q->qsize = Q_SIZE(Q_DESC_SIZE);
-		cmd_q->qbase = dma_zalloc_coherent(dev, cmd_q->qsize,
-						   &cmd_q->qbase_dma,
-						   GFP_KERNEL);
+		cmd_q->qbase = dma_alloc_coherent(dev, cmd_q->qsize,
+						  &cmd_q->qbase_dma,
+						  GFP_KERNEL);
 		if (!cmd_q->qbase) {
 			dev_err(dev, "unable to allocate command queue\n");
 			ret = -ENOMEM;
* Unmerged path drivers/crypto/hisilicon/sec/sec_algs.c
* Unmerged path drivers/crypto/hisilicon/sec/sec_drv.c
* Unmerged path drivers/crypto/ixp4xx_crypto.c
* Unmerged path drivers/crypto/mediatek/mtk-platform.c
diff --git a/drivers/crypto/qat/qat_common/adf_admin.c b/drivers/crypto/qat/qat_common/adf_admin.c
index 3744b22f0c46..d28cba34773e 100644
--- a/drivers/crypto/qat/qat_common/adf_admin.c
+++ b/drivers/crypto/qat/qat_common/adf_admin.c
@@ -244,18 +244,18 @@ int adf_init_admin_comms(struct adf_accel_dev *accel_dev)
 			     dev_to_node(&GET_DEV(accel_dev)));
 	if (!admin)
 		return -ENOMEM;
-	admin->virt_addr = dma_zalloc_coherent(&GET_DEV(accel_dev), PAGE_SIZE,
-					       &admin->phy_addr, GFP_KERNEL);
+	admin->virt_addr = dma_alloc_coherent(&GET_DEV(accel_dev), PAGE_SIZE,
+					      &admin->phy_addr, GFP_KERNEL);
 	if (!admin->virt_addr) {
 		dev_err(&GET_DEV(accel_dev), "Failed to allocate dma buff\n");
 		kfree(admin);
 		return -ENOMEM;
 	}
 
-	admin->virt_tbl_addr = dma_zalloc_coherent(&GET_DEV(accel_dev),
-						   PAGE_SIZE,
-						   &admin->const_tbl_addr,
-						   GFP_KERNEL);
+	admin->virt_tbl_addr = dma_alloc_coherent(&GET_DEV(accel_dev),
+						  PAGE_SIZE,
+						  &admin->const_tbl_addr,
+						  GFP_KERNEL);
 	if (!admin->virt_tbl_addr) {
 		dev_err(&GET_DEV(accel_dev), "Failed to allocate const_tbl\n");
 		dma_free_coherent(&GET_DEV(accel_dev), PAGE_SIZE,
diff --git a/drivers/crypto/qat/qat_common/qat_algs.c b/drivers/crypto/qat/qat_common/qat_algs.c
index c6d52ba29a2e..8167df309373 100644
--- a/drivers/crypto/qat/qat_common/qat_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_algs.c
@@ -608,16 +608,16 @@ static int qat_alg_aead_setkey(struct crypto_aead *tfm, const uint8_t *key,
 
 		dev = &GET_DEV(inst->accel_dev);
 		ctx->inst = inst;
-		ctx->enc_cd = dma_zalloc_coherent(dev, sizeof(*ctx->enc_cd),
-						  &ctx->enc_cd_paddr,
-						  GFP_ATOMIC);
+		ctx->enc_cd = dma_alloc_coherent(dev, sizeof(*ctx->enc_cd),
+						 &ctx->enc_cd_paddr,
+						 GFP_ATOMIC);
 		if (!ctx->enc_cd) {
 			spin_unlock(&ctx->lock);
 			return -ENOMEM;
 		}
-		ctx->dec_cd = dma_zalloc_coherent(dev, sizeof(*ctx->dec_cd),
-						  &ctx->dec_cd_paddr,
-						  GFP_ATOMIC);
+		ctx->dec_cd = dma_alloc_coherent(dev, sizeof(*ctx->dec_cd),
+						 &ctx->dec_cd_paddr,
+						 GFP_ATOMIC);
 		if (!ctx->dec_cd) {
 			spin_unlock(&ctx->lock);
 			goto out_free_enc;
@@ -1022,16 +1022,16 @@ static int qat_alg_ablkcipher_setkey(struct crypto_ablkcipher *tfm,
 
 		dev = &GET_DEV(inst->accel_dev);
 		ctx->inst = inst;
-		ctx->enc_cd = dma_zalloc_coherent(dev, sizeof(*ctx->enc_cd),
-						  &ctx->enc_cd_paddr,
-						  GFP_ATOMIC);
+		ctx->enc_cd = dma_alloc_coherent(dev, sizeof(*ctx->enc_cd),
+						 &ctx->enc_cd_paddr,
+						 GFP_ATOMIC);
 		if (!ctx->enc_cd) {
 			spin_unlock(&ctx->lock);
 			return -ENOMEM;
 		}
-		ctx->dec_cd = dma_zalloc_coherent(dev, sizeof(*ctx->dec_cd),
-						  &ctx->dec_cd_paddr,
-						  GFP_ATOMIC);
+		ctx->dec_cd = dma_alloc_coherent(dev, sizeof(*ctx->dec_cd),
+						 &ctx->dec_cd_paddr,
+						 GFP_ATOMIC);
 		if (!ctx->dec_cd) {
 			spin_unlock(&ctx->lock);
 			goto out_free_enc;
diff --git a/drivers/crypto/qat/qat_common/qat_asym_algs.c b/drivers/crypto/qat/qat_common/qat_asym_algs.c
index 2002b9b5a771..d15ccdbe9756 100644
--- a/drivers/crypto/qat/qat_common/qat_asym_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_asym_algs.c
@@ -332,10 +332,10 @@ static int qat_dh_compute_value(struct kpp_request *req)
 		} else {
 			int shift = ctx->p_size - req->src_len;
 
-			qat_req->src_align = dma_zalloc_coherent(dev,
-								 ctx->p_size,
-								 &qat_req->in.dh.in.b,
-								 GFP_KERNEL);
+			qat_req->src_align = dma_alloc_coherent(dev,
+								ctx->p_size,
+								&qat_req->in.dh.in.b,
+								GFP_KERNEL);
 			if (unlikely(!qat_req->src_align))
 				return ret;
 
@@ -360,9 +360,9 @@ static int qat_dh_compute_value(struct kpp_request *req)
 			goto unmap_src;
 
 	} else {
-		qat_req->dst_align = dma_zalloc_coherent(dev, ctx->p_size,
-							 &qat_req->out.dh.r,
-							 GFP_KERNEL);
+		qat_req->dst_align = dma_alloc_coherent(dev, ctx->p_size,
+							&qat_req->out.dh.r,
+							GFP_KERNEL);
 		if (unlikely(!qat_req->dst_align))
 			goto unmap_src;
 	}
@@ -447,7 +447,7 @@ static int qat_dh_set_params(struct qat_dh_ctx *ctx, struct dh *params)
 		return -EINVAL;
 
 	ctx->p_size = params->p_size;
-	ctx->p = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_p, GFP_KERNEL);
+	ctx->p = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_p, GFP_KERNEL);
 	if (!ctx->p)
 		return -ENOMEM;
 	memcpy(ctx->p, params->p, ctx->p_size);
@@ -458,7 +458,7 @@ static int qat_dh_set_params(struct qat_dh_ctx *ctx, struct dh *params)
 		return 0;
 	}
 
-	ctx->g = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_g, GFP_KERNEL);
+	ctx->g = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_g, GFP_KERNEL);
 	if (!ctx->g)
 		return -ENOMEM;
 	memcpy(ctx->g + (ctx->p_size - params->g_size), params->g,
@@ -503,8 +503,8 @@ static int qat_dh_set_secret(struct crypto_kpp *tfm, const void *buf,
 	if (ret < 0)
 		goto err_clear_ctx;
 
-	ctx->xa = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_xa,
-				      GFP_KERNEL);
+	ctx->xa = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_xa,
+				     GFP_KERNEL);
 	if (!ctx->xa) {
 		ret = -ENOMEM;
 		goto err_clear_ctx;
@@ -737,9 +737,9 @@ static int qat_rsa_enc(struct akcipher_request *req)
 	} else {
 		int shift = ctx->key_sz - req->src_len;
 
-		qat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,
-							 &qat_req->in.rsa.enc.m,
-							 GFP_KERNEL);
+		qat_req->src_align = dma_alloc_coherent(dev, ctx->key_sz,
+							&qat_req->in.rsa.enc.m,
+							GFP_KERNEL);
 		if (unlikely(!qat_req->src_align))
 			return ret;
 
@@ -756,9 +756,9 @@ static int qat_rsa_enc(struct akcipher_request *req)
 			goto unmap_src;
 
 	} else {
-		qat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,
-							 &qat_req->out.rsa.enc.c,
-							 GFP_KERNEL);
+		qat_req->dst_align = dma_alloc_coherent(dev, ctx->key_sz,
+							&qat_req->out.rsa.enc.c,
+							GFP_KERNEL);
 		if (unlikely(!qat_req->dst_align))
 			goto unmap_src;
 
@@ -881,9 +881,9 @@ static int qat_rsa_dec(struct akcipher_request *req)
 	} else {
 		int shift = ctx->key_sz - req->src_len;
 
-		qat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,
-							 &qat_req->in.rsa.dec.c,
-							 GFP_KERNEL);
+		qat_req->src_align = dma_alloc_coherent(dev, ctx->key_sz,
+							&qat_req->in.rsa.dec.c,
+							GFP_KERNEL);
 		if (unlikely(!qat_req->src_align))
 			return ret;
 
@@ -900,9 +900,9 @@ static int qat_rsa_dec(struct akcipher_request *req)
 			goto unmap_src;
 
 	} else {
-		qat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,
-							 &qat_req->out.rsa.dec.m,
-							 GFP_KERNEL);
+		qat_req->dst_align = dma_alloc_coherent(dev, ctx->key_sz,
+							&qat_req->out.rsa.dec.m,
+							GFP_KERNEL);
 		if (unlikely(!qat_req->dst_align))
 			goto unmap_src;
 
@@ -988,7 +988,7 @@ int qat_rsa_set_n(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)
 		goto err;
 
 	ret = -ENOMEM;
-	ctx->n = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_n, GFP_KERNEL);
+	ctx->n = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_n, GFP_KERNEL);
 	if (!ctx->n)
 		goto err;
 
@@ -1016,7 +1016,7 @@ int qat_rsa_set_e(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)
 		return -EINVAL;
 	}
 
-	ctx->e = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_e, GFP_KERNEL);
+	ctx->e = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_e, GFP_KERNEL);
 	if (!ctx->e)
 		return -ENOMEM;
 
@@ -1041,7 +1041,7 @@ int qat_rsa_set_d(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)
 		goto err;
 
 	ret = -ENOMEM;
-	ctx->d = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_d, GFP_KERNEL);
+	ctx->d = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_d, GFP_KERNEL);
 	if (!ctx->d)
 		goto err;
 
@@ -1074,7 +1074,7 @@ static void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)
 	qat_rsa_drop_leading_zeros(&ptr, &len);
 	if (!len)
 		goto err;
-	ctx->p = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_p, GFP_KERNEL);
+	ctx->p = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_p, GFP_KERNEL);
 	if (!ctx->p)
 		goto err;
 	memcpy(ctx->p + (half_key_sz - len), ptr, len);
@@ -1085,7 +1085,7 @@ static void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)
 	qat_rsa_drop_leading_zeros(&ptr, &len);
 	if (!len)
 		goto free_p;
-	ctx->q = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_q, GFP_KERNEL);
+	ctx->q = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_q, GFP_KERNEL);
 	if (!ctx->q)
 		goto free_p;
 	memcpy(ctx->q + (half_key_sz - len), ptr, len);
@@ -1096,8 +1096,8 @@ static void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)
 	qat_rsa_drop_leading_zeros(&ptr, &len);
 	if (!len)
 		goto free_q;
-	ctx->dp = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_dp,
-				      GFP_KERNEL);
+	ctx->dp = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_dp,
+				     GFP_KERNEL);
 	if (!ctx->dp)
 		goto free_q;
 	memcpy(ctx->dp + (half_key_sz - len), ptr, len);
@@ -1108,8 +1108,8 @@ static void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)
 	qat_rsa_drop_leading_zeros(&ptr, &len);
 	if (!len)
 		goto free_dp;
-	ctx->dq = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_dq,
-				      GFP_KERNEL);
+	ctx->dq = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_dq,
+				     GFP_KERNEL);
 	if (!ctx->dq)
 		goto free_dp;
 	memcpy(ctx->dq + (half_key_sz - len), ptr, len);
@@ -1120,8 +1120,8 @@ static void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)
 	qat_rsa_drop_leading_zeros(&ptr, &len);
 	if (!len)
 		goto free_dq;
-	ctx->qinv = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_qinv,
-					GFP_KERNEL);
+	ctx->qinv = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_qinv,
+				       GFP_KERNEL);
 	if (!ctx->qinv)
 		goto free_dq;
 	memcpy(ctx->qinv + (half_key_sz - len), ptr, len);
* Unmerged path drivers/dma/imx-sdma.c
* Unmerged path drivers/dma/mediatek/mtk-hsdma.c
* Unmerged path drivers/dma/mxs-dma.c
* Unmerged path drivers/dma/xgene-dma.c
* Unmerged path drivers/dma/xilinx/xilinx_dma.c
* Unmerged path drivers/dma/xilinx/zynqmp_dma.c
* Unmerged path drivers/gpu/drm/drm_pci.c
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_rcfw.c b/drivers/infiniband/hw/bnxt_re/qplib_rcfw.c
index be4e33e9f962..cdbd00d7ef49 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_rcfw.c
+++ b/drivers/infiniband/hw/bnxt_re/qplib_rcfw.c
@@ -756,8 +756,8 @@ struct bnxt_qplib_rcfw_sbuf *bnxt_qplib_rcfw_alloc_sbuf(
 		return NULL;
 
 	sbuf->size = size;
-	sbuf->sb = dma_zalloc_coherent(&rcfw->pdev->dev, sbuf->size,
-				       &sbuf->dma_addr, GFP_ATOMIC);
+	sbuf->sb = dma_alloc_coherent(&rcfw->pdev->dev, sbuf->size,
+				      &sbuf->dma_addr, GFP_ATOMIC);
 	if (!sbuf->sb)
 		goto bail;
 
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_res.c b/drivers/infiniband/hw/bnxt_re/qplib_res.c
index 59eeac55626f..57d4951679cb 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_res.c
+++ b/drivers/infiniband/hw/bnxt_re/qplib_res.c
@@ -105,10 +105,10 @@ static int __alloc_pbl(struct pci_dev *pdev, struct bnxt_qplib_pbl *pbl,
 
 	if (!sghead) {
 		for (i = 0; i < pages; i++) {
-			pbl->pg_arr[i] = dma_zalloc_coherent(&pdev->dev,
-							     pbl->pg_size,
-							     &pbl->pg_map_arr[i],
-							     GFP_KERNEL);
+			pbl->pg_arr[i] = dma_alloc_coherent(&pdev->dev,
+							    pbl->pg_size,
+							    &pbl->pg_map_arr[i],
+							    GFP_KERNEL);
 			if (!pbl->pg_arr[i])
 				goto fail;
 			pbl->pg_count++;
* Unmerged path drivers/infiniband/hw/cxgb3/cxio_hal.c
* Unmerged path drivers/infiniband/hw/cxgb4/qp.c
diff --git a/drivers/infiniband/hw/hfi1/init.c b/drivers/infiniband/hw/hfi1/init.c
index d0f1408a5fdc..5fe442cfdce5 100644
--- a/drivers/infiniband/hw/hfi1/init.c
+++ b/drivers/infiniband/hw/hfi1/init.c
@@ -883,10 +883,10 @@ int hfi1_init(struct hfi1_devdata *dd, int reinit)
 		goto done;
 
 	/* allocate dummy tail memory for all receive contexts */
-	dd->rcvhdrtail_dummy_kvaddr = dma_zalloc_coherent(
-		&dd->pcidev->dev, sizeof(u64),
-		&dd->rcvhdrtail_dummy_dma,
-		GFP_KERNEL);
+	dd->rcvhdrtail_dummy_kvaddr = dma_alloc_coherent(&dd->pcidev->dev,
+							 sizeof(u64),
+							 &dd->rcvhdrtail_dummy_dma,
+							 GFP_KERNEL);
 
 	if (!dd->rcvhdrtail_dummy_kvaddr) {
 		dd_dev_err(dd, "cannot allocate dummy tail memory\n");
@@ -1831,9 +1831,9 @@ int hfi1_create_rcvhdrq(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)
 			gfp_flags = GFP_KERNEL;
 		else
 			gfp_flags = GFP_USER;
-		rcd->rcvhdrq = dma_zalloc_coherent(
-			&dd->pcidev->dev, amt, &rcd->rcvhdrq_dma,
-			gfp_flags | __GFP_COMP);
+		rcd->rcvhdrq = dma_alloc_coherent(&dd->pcidev->dev, amt,
+						  &rcd->rcvhdrq_dma,
+						  gfp_flags | __GFP_COMP);
 
 		if (!rcd->rcvhdrq) {
 			dd_dev_err(dd,
@@ -1844,9 +1844,10 @@ int hfi1_create_rcvhdrq(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)
 
 		if (HFI1_CAP_KGET_MASK(rcd->flags, DMA_RTAIL) ||
 		    HFI1_CAP_UGET_MASK(rcd->flags, DMA_RTAIL)) {
-			rcd->rcvhdrtail_kvaddr = dma_zalloc_coherent(
-				&dd->pcidev->dev, PAGE_SIZE,
-				&rcd->rcvhdrqtailaddr_dma, gfp_flags);
+			rcd->rcvhdrtail_kvaddr = dma_alloc_coherent(&dd->pcidev->dev,
+								    PAGE_SIZE,
+								    &rcd->rcvhdrqtailaddr_dma,
+								    gfp_flags);
 			if (!rcd->rcvhdrtail_kvaddr)
 				goto bail_free;
 		}
@@ -1942,10 +1943,10 @@ int hfi1_setup_eagerbufs(struct hfi1_ctxtdata *rcd)
 	while (alloced_bytes < rcd->egrbufs.size &&
 	       rcd->egrbufs.alloced < rcd->egrbufs.count) {
 		rcd->egrbufs.buffers[idx].addr =
-			dma_zalloc_coherent(&dd->pcidev->dev,
-					    rcd->egrbufs.rcvtid_size,
-					    &rcd->egrbufs.buffers[idx].dma,
-					    gfp_flags);
+			dma_alloc_coherent(&dd->pcidev->dev,
+					   rcd->egrbufs.rcvtid_size,
+					   &rcd->egrbufs.buffers[idx].dma,
+					   gfp_flags);
 		if (rcd->egrbufs.buffers[idx].addr) {
 			rcd->egrbufs.buffers[idx].len =
 				rcd->egrbufs.rcvtid_size;
diff --git a/drivers/infiniband/hw/hfi1/pio.c b/drivers/infiniband/hw/hfi1/pio.c
index 8395189b93a6..fd2f3470d08d 100644
--- a/drivers/infiniband/hw/hfi1/pio.c
+++ b/drivers/infiniband/hw/hfi1/pio.c
@@ -2098,11 +2098,10 @@ int init_credit_return(struct hfi1_devdata *dd)
 		int bytes = TXE_NUM_CONTEXTS * sizeof(struct credit_return);
 
 		set_dev_node(&dd->pcidev->dev, i);
-		dd->cr_base[i].va = dma_zalloc_coherent(
-					&dd->pcidev->dev,
-					bytes,
-					&dd->cr_base[i].dma,
-					GFP_KERNEL);
+		dd->cr_base[i].va = dma_alloc_coherent(&dd->pcidev->dev,
+						       bytes,
+						       &dd->cr_base[i].dma,
+						       GFP_KERNEL);
 		if (!dd->cr_base[i].va) {
 			set_dev_node(&dd->pcidev->dev, dd->node);
 			dd_dev_err(dd,
diff --git a/drivers/infiniband/hw/hfi1/sdma.c b/drivers/infiniband/hw/hfi1/sdma.c
index 7bbb70964ebc..beebafb93491 100644
--- a/drivers/infiniband/hw/hfi1/sdma.c
+++ b/drivers/infiniband/hw/hfi1/sdma.c
@@ -1458,12 +1458,9 @@ int sdma_init(struct hfi1_devdata *dd, u8 port)
 		timer_setup(&sde->err_progress_check_timer,
 			    sdma_err_progress_check, 0);
 
-		sde->descq = dma_zalloc_coherent(
-			&dd->pcidev->dev,
-			descq_cnt * sizeof(u64[2]),
-			&sde->descq_phys,
-			GFP_KERNEL
-		);
+		sde->descq = dma_alloc_coherent(&dd->pcidev->dev,
+						descq_cnt * sizeof(u64[2]),
+						&sde->descq_phys, GFP_KERNEL);
 		if (!sde->descq)
 			goto bail;
 		sde->tx_ring =
@@ -1475,24 +1472,18 @@ int sdma_init(struct hfi1_devdata *dd, u8 port)
 
 	dd->sdma_heads_size = L1_CACHE_BYTES * num_engines;
 	/* Allocate memory for DMA of head registers to memory */
-	dd->sdma_heads_dma = dma_zalloc_coherent(
-		&dd->pcidev->dev,
-		dd->sdma_heads_size,
-		&dd->sdma_heads_phys,
-		GFP_KERNEL
-	);
+	dd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,
+						dd->sdma_heads_size,
+						&dd->sdma_heads_phys,
+						GFP_KERNEL);
 	if (!dd->sdma_heads_dma) {
 		dd_dev_err(dd, "failed to allocate SendDMA head memory\n");
 		goto bail;
 	}
 
 	/* Allocate memory for pad */
-	dd->sdma_pad_dma = dma_zalloc_coherent(
-		&dd->pcidev->dev,
-		sizeof(u32),
-		&dd->sdma_pad_phys,
-		GFP_KERNEL
-	);
+	dd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, sizeof(u32),
+					      &dd->sdma_pad_phys, GFP_KERNEL);
 	if (!dd->sdma_pad_dma) {
 		dd_dev_err(dd, "failed to allocate SendDMA pad memory\n");
 		goto bail;
* Unmerged path drivers/infiniband/hw/hns/hns_roce_alloc.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v2.c
diff --git a/drivers/infiniband/hw/i40iw/i40iw_utils.c b/drivers/infiniband/hw/i40iw/i40iw_utils.c
index a9ea966877f2..59e978141ad4 100644
--- a/drivers/infiniband/hw/i40iw/i40iw_utils.c
+++ b/drivers/infiniband/hw/i40iw/i40iw_utils.c
@@ -745,8 +745,8 @@ enum i40iw_status_code i40iw_allocate_dma_mem(struct i40iw_hw *hw,
 	if (!mem)
 		return I40IW_ERR_PARAM;
 	mem->size = ALIGN(size, alignment);
-	mem->va = dma_zalloc_coherent(&pcidev->dev, mem->size,
-				      (dma_addr_t *)&mem->pa, GFP_KERNEL);
+	mem->va = dma_alloc_coherent(&pcidev->dev, mem->size,
+				     (dma_addr_t *)&mem->pa, GFP_KERNEL);
 	if (!mem->va)
 		return I40IW_ERR_NO_MEMORY;
 	return 0;
diff --git a/drivers/infiniband/hw/mthca/mthca_memfree.c b/drivers/infiniband/hw/mthca/mthca_memfree.c
index 2fe503e86c1d..3875d8c9da7b 100644
--- a/drivers/infiniband/hw/mthca/mthca_memfree.c
+++ b/drivers/infiniband/hw/mthca/mthca_memfree.c
@@ -623,8 +623,9 @@ int mthca_alloc_db(struct mthca_dev *dev, enum mthca_db_type type,
 	page = dev->db_tab->page + end;
 
 alloc:
-	page->db_rec = dma_zalloc_coherent(&dev->pdev->dev, MTHCA_ICM_PAGE_SIZE,
-					   &page->mapping, GFP_KERNEL);
+	page->db_rec = dma_alloc_coherent(&dev->pdev->dev,
+					  MTHCA_ICM_PAGE_SIZE, &page->mapping,
+					  GFP_KERNEL);
 	if (!page->db_rec) {
 		ret = -ENOMEM;
 		goto out;
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_hw.c b/drivers/infiniband/hw/ocrdma/ocrdma_hw.c
index 552d2e7b3aff..9d7c1613ec01 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_hw.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_hw.c
@@ -380,8 +380,8 @@ static int ocrdma_alloc_q(struct ocrdma_dev *dev,
 	q->len = len;
 	q->entry_size = entry_size;
 	q->size = len * entry_size;
-	q->va = dma_zalloc_coherent(&dev->nic_info.pdev->dev, q->size,
-				    &q->dma, GFP_KERNEL);
+	q->va = dma_alloc_coherent(&dev->nic_info.pdev->dev, q->size, &q->dma,
+				   GFP_KERNEL);
 	if (!q->va)
 		return -ENOMEM;
 	return 0;
@@ -1819,7 +1819,7 @@ int ocrdma_mbx_create_cq(struct ocrdma_dev *dev, struct ocrdma_cq *cq,
 		return -ENOMEM;
 	ocrdma_init_mch(&cmd->cmd.req, OCRDMA_CMD_CREATE_CQ,
 			OCRDMA_SUBSYS_COMMON, sizeof(*cmd));
-	cq->va = dma_zalloc_coherent(&pdev->dev, cq->len, &cq->pa, GFP_KERNEL);
+	cq->va = dma_alloc_coherent(&pdev->dev, cq->len, &cq->pa, GFP_KERNEL);
 	if (!cq->va) {
 		status = -ENOMEM;
 		goto mem_err;
@@ -2209,7 +2209,7 @@ static int ocrdma_set_create_qp_sq_cmd(struct ocrdma_create_qp_req *cmd,
 	qp->sq.max_cnt = max_wqe_allocated;
 	len = (hw_pages * hw_page_size);
 
-	qp->sq.va = dma_zalloc_coherent(&pdev->dev, len, &pa, GFP_KERNEL);
+	qp->sq.va = dma_alloc_coherent(&pdev->dev, len, &pa, GFP_KERNEL);
 	if (!qp->sq.va)
 		return -EINVAL;
 	qp->sq.len = len;
@@ -2259,7 +2259,7 @@ static int ocrdma_set_create_qp_rq_cmd(struct ocrdma_create_qp_req *cmd,
 	qp->rq.max_cnt = max_rqe_allocated;
 	len = (hw_pages * hw_page_size);
 
-	qp->rq.va = dma_zalloc_coherent(&pdev->dev, len, &pa, GFP_KERNEL);
+	qp->rq.va = dma_alloc_coherent(&pdev->dev, len, &pa, GFP_KERNEL);
 	if (!qp->rq.va)
 		return -ENOMEM;
 	qp->rq.pa = pa;
@@ -2315,8 +2315,8 @@ static int ocrdma_set_create_qp_ird_cmd(struct ocrdma_create_qp_req *cmd,
 	if (dev->attr.ird == 0)
 		return 0;
 
-	qp->ird_q_va = dma_zalloc_coherent(&pdev->dev, ird_q_len, &pa,
-					   GFP_KERNEL);
+	qp->ird_q_va = dma_alloc_coherent(&pdev->dev, ird_q_len, &pa,
+					  GFP_KERNEL);
 	if (!qp->ird_q_va)
 		return -ENOMEM;
 	ocrdma_build_q_pages(&cmd->ird_addr[0], dev->attr.num_ird_pages,
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_stats.c b/drivers/infiniband/hw/ocrdma/ocrdma_stats.c
index 24d20a4aa262..7ecd93f5dad6 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_stats.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_stats.c
@@ -73,8 +73,8 @@ bool ocrdma_alloc_stats_resources(struct ocrdma_dev *dev)
 	mem->size = max_t(u32, sizeof(struct ocrdma_rdma_stats_req),
 			sizeof(struct ocrdma_rdma_stats_resp));
 
-	mem->va = dma_zalloc_coherent(&dev->nic_info.pdev->dev, mem->size,
-				      &mem->pa, GFP_KERNEL);
+	mem->va = dma_alloc_coherent(&dev->nic_info.pdev->dev, mem->size,
+				     &mem->pa, GFP_KERNEL);
 	if (!mem->va) {
 		pr_err("%s: stats mbox allocation failed\n", __func__);
 		return false;
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
index ae3e9b166250..8f6c7409be49 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
@@ -522,8 +522,8 @@ struct ib_ucontext *ocrdma_alloc_ucontext(struct ib_device *ibdev,
 	INIT_LIST_HEAD(&ctx->mm_head);
 	mutex_init(&ctx->mm_list_lock);
 
-	ctx->ah_tbl.va = dma_zalloc_coherent(&pdev->dev, map_len,
-					     &ctx->ah_tbl.pa, GFP_KERNEL);
+	ctx->ah_tbl.va = dma_alloc_coherent(&pdev->dev, map_len,
+					    &ctx->ah_tbl.pa, GFP_KERNEL);
 	if (!ctx->ah_tbl.va) {
 		kfree(ctx);
 		return ERR_PTR(-ENOMEM);
@@ -856,7 +856,7 @@ static int ocrdma_build_pbl_tbl(struct ocrdma_dev *dev, struct ocrdma_hw_mr *mr)
 		return -ENOMEM;
 
 	for (i = 0; i < mr->num_pbls; i++) {
-		va = dma_zalloc_coherent(&pdev->dev, dma_len, &pa, GFP_KERNEL);
+		va = dma_alloc_coherent(&pdev->dev, dma_len, &pa, GFP_KERNEL);
 		if (!va) {
 			ocrdma_free_mr_pbl_tbl(dev, mr);
 			status = -ENOMEM;
diff --git a/drivers/infiniband/hw/qedr/verbs.c b/drivers/infiniband/hw/qedr/verbs.c
index 318a25dc7043..60b26cf456f3 100644
--- a/drivers/infiniband/hw/qedr/verbs.c
+++ b/drivers/infiniband/hw/qedr/verbs.c
@@ -559,8 +559,8 @@ static struct qedr_pbl *qedr_alloc_pbl_tbl(struct qedr_dev *dev,
 		return ERR_PTR(-ENOMEM);
 
 	for (i = 0; i < pbl_info->num_pbls; i++) {
-		va = dma_zalloc_coherent(&pdev->dev, pbl_info->pbl_size,
-					 &pa, flags);
+		va = dma_alloc_coherent(&pdev->dev, pbl_info->pbl_size, &pa,
+					flags);
 		if (!va)
 			goto err;
 
diff --git a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
index f493046158a6..d9a604f92cd7 100644
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c
@@ -907,8 +907,8 @@ static int pvrdma_pci_probe(struct pci_dev *pdev,
 	dev_info(&pdev->dev, "device version %d, driver version %d\n",
 		 dev->dsr_version, PVRDMA_VERSION);
 
-	dev->dsr = dma_zalloc_coherent(&pdev->dev, sizeof(*dev->dsr),
-				       &dev->dsrbase, GFP_KERNEL);
+	dev->dsr = dma_alloc_coherent(&pdev->dev, sizeof(*dev->dsr),
+				      &dev->dsrbase, GFP_KERNEL);
 	if (!dev->dsr) {
 		dev_err(&pdev->dev, "failed to allocate shared region\n");
 		ret = -ENOMEM;
* Unmerged path drivers/input/touchscreen/raspberrypi-ts.c
* Unmerged path drivers/iommu/mtk_iommu_v1.c
* Unmerged path drivers/media/pci/intel/ipu3/ipu3-cio2.c
* Unmerged path drivers/media/platform/mtk-vcodec/mtk_vcodec_util.c
diff --git a/drivers/misc/genwqe/card_utils.c b/drivers/misc/genwqe/card_utils.c
index 8937d073ffde..e3619456758a 100644
--- a/drivers/misc/genwqe/card_utils.c
+++ b/drivers/misc/genwqe/card_utils.c
@@ -220,8 +220,8 @@ void *__genwqe_alloc_consistent(struct genwqe_dev *cd, size_t size,
 	if (get_order(size) >= MAX_ORDER)
 		return NULL;
 
-	return dma_zalloc_coherent(&cd->pci_dev->dev, size, dma_handle,
-				   GFP_KERNEL);
+	return dma_alloc_coherent(&cd->pci_dev->dev, size, dma_handle,
+				  GFP_KERNEL);
 }
 
 void __genwqe_free_consistent(struct genwqe_dev *cd, size_t size,
* Unmerged path drivers/mmc/host/sdhci.c
* Unmerged path drivers/net/ethernet/aeroflex/greth.c
* Unmerged path drivers/net/ethernet/alacritech/slicoss.c
diff --git a/drivers/net/ethernet/amazon/ena/ena_com.c b/drivers/net/ethernet/amazon/ena/ena_com.c
index 16c6ea4ea135..e484a8ff0f4e 100644
--- a/drivers/net/ethernet/amazon/ena/ena_com.c
+++ b/drivers/net/ethernet/amazon/ena/ena_com.c
@@ -111,8 +111,8 @@ static int ena_com_admin_init_sq(struct ena_com_admin_queue *queue)
 	struct ena_com_admin_sq *sq = &queue->sq;
 	u16 size = ADMIN_SQ_SIZE(queue->q_depth);
 
-	sq->entries = dma_zalloc_coherent(queue->q_dmadev, size, &sq->dma_addr,
-					  GFP_KERNEL);
+	sq->entries = dma_alloc_coherent(queue->q_dmadev, size, &sq->dma_addr,
+					 GFP_KERNEL);
 
 	if (!sq->entries) {
 		pr_err("memory allocation failed");
@@ -133,8 +133,8 @@ static int ena_com_admin_init_cq(struct ena_com_admin_queue *queue)
 	struct ena_com_admin_cq *cq = &queue->cq;
 	u16 size = ADMIN_CQ_SIZE(queue->q_depth);
 
-	cq->entries = dma_zalloc_coherent(queue->q_dmadev, size, &cq->dma_addr,
-					  GFP_KERNEL);
+	cq->entries = dma_alloc_coherent(queue->q_dmadev, size, &cq->dma_addr,
+					 GFP_KERNEL);
 
 	if (!cq->entries) {
 		pr_err("memory allocation failed");
@@ -156,8 +156,8 @@ static int ena_com_admin_init_aenq(struct ena_com_dev *dev,
 
 	dev->aenq.q_depth = ENA_ASYNC_QUEUE_DEPTH;
 	size = ADMIN_AENQ_SIZE(ENA_ASYNC_QUEUE_DEPTH);
-	aenq->entries = dma_zalloc_coherent(dev->dmadev, size, &aenq->dma_addr,
-					    GFP_KERNEL);
+	aenq->entries = dma_alloc_coherent(dev->dmadev, size, &aenq->dma_addr,
+					   GFP_KERNEL);
 
 	if (!aenq->entries) {
 		pr_err("memory allocation failed");
@@ -344,15 +344,15 @@ static int ena_com_init_io_sq(struct ena_com_dev *ena_dev,
 		dev_node = dev_to_node(ena_dev->dmadev);
 		set_dev_node(ena_dev->dmadev, ctx->numa_node);
 		io_sq->desc_addr.virt_addr =
-			dma_zalloc_coherent(ena_dev->dmadev, size,
-					    &io_sq->desc_addr.phys_addr,
-					    GFP_KERNEL);
+			dma_alloc_coherent(ena_dev->dmadev, size,
+					   &io_sq->desc_addr.phys_addr,
+					   GFP_KERNEL);
 		set_dev_node(ena_dev->dmadev, dev_node);
 		if (!io_sq->desc_addr.virt_addr) {
 			io_sq->desc_addr.virt_addr =
-				dma_zalloc_coherent(ena_dev->dmadev, size,
-						    &io_sq->desc_addr.phys_addr,
-						    GFP_KERNEL);
+				dma_alloc_coherent(ena_dev->dmadev, size,
+						   &io_sq->desc_addr.phys_addr,
+						   GFP_KERNEL);
 		}
 
 		if (!io_sq->desc_addr.virt_addr) {
@@ -425,14 +425,14 @@ static int ena_com_init_io_cq(struct ena_com_dev *ena_dev,
 	prev_node = dev_to_node(ena_dev->dmadev);
 	set_dev_node(ena_dev->dmadev, ctx->numa_node);
 	io_cq->cdesc_addr.virt_addr =
-		dma_zalloc_coherent(ena_dev->dmadev, size,
-				    &io_cq->cdesc_addr.phys_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, size,
+				   &io_cq->cdesc_addr.phys_addr, GFP_KERNEL);
 	set_dev_node(ena_dev->dmadev, prev_node);
 	if (!io_cq->cdesc_addr.virt_addr) {
 		io_cq->cdesc_addr.virt_addr =
-			dma_zalloc_coherent(ena_dev->dmadev, size,
-					    &io_cq->cdesc_addr.phys_addr,
-					    GFP_KERNEL);
+			dma_alloc_coherent(ena_dev->dmadev, size,
+					   &io_cq->cdesc_addr.phys_addr,
+					   GFP_KERNEL);
 	}
 
 	if (!io_cq->cdesc_addr.virt_addr) {
@@ -1025,8 +1025,8 @@ static int ena_com_hash_key_allocate(struct ena_com_dev *ena_dev)
 	struct ena_rss *rss = &ena_dev->rss;
 
 	rss->hash_key =
-		dma_zalloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),
-				    &rss->hash_key_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),
+				   &rss->hash_key_dma_addr, GFP_KERNEL);
 
 	if (unlikely(!rss->hash_key))
 		return -ENOMEM;
@@ -1049,8 +1049,8 @@ static int ena_com_hash_ctrl_init(struct ena_com_dev *ena_dev)
 	struct ena_rss *rss = &ena_dev->rss;
 
 	rss->hash_ctrl =
-		dma_zalloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),
-				    &rss->hash_ctrl_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),
+				   &rss->hash_ctrl_dma_addr, GFP_KERNEL);
 
 	if (unlikely(!rss->hash_ctrl))
 		return -ENOMEM;
@@ -1093,8 +1093,8 @@ static int ena_com_indirect_table_allocate(struct ena_com_dev *ena_dev,
 		sizeof(struct ena_admin_rss_ind_table_entry);
 
 	rss->rss_ind_tbl =
-		dma_zalloc_coherent(ena_dev->dmadev, tbl_size,
-				    &rss->rss_ind_tbl_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, tbl_size,
+				   &rss->rss_ind_tbl_dma_addr, GFP_KERNEL);
 	if (unlikely(!rss->rss_ind_tbl))
 		goto mem_err1;
 
@@ -1648,9 +1648,9 @@ int ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev)
 
 	spin_lock_init(&mmio_read->lock);
 	mmio_read->read_resp =
-		dma_zalloc_coherent(ena_dev->dmadev,
-				    sizeof(*mmio_read->read_resp),
-				    &mmio_read->read_resp_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev,
+				   sizeof(*mmio_read->read_resp),
+				   &mmio_read->read_resp_dma_addr, GFP_KERNEL);
 	if (unlikely(!mmio_read->read_resp))
 		return -ENOMEM;
 
@@ -2616,8 +2616,8 @@ int ena_com_allocate_host_info(struct ena_com_dev *ena_dev)
 	struct ena_host_attribute *host_attr = &ena_dev->host_attr;
 
 	host_attr->host_info =
-		dma_zalloc_coherent(ena_dev->dmadev, SZ_4K,
-				    &host_attr->host_info_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, SZ_4K,
+				   &host_attr->host_info_dma_addr, GFP_KERNEL);
 	if (unlikely(!host_attr->host_info))
 		return -ENOMEM;
 
@@ -2634,8 +2634,9 @@ int ena_com_allocate_debug_area(struct ena_com_dev *ena_dev,
 	struct ena_host_attribute *host_attr = &ena_dev->host_attr;
 
 	host_attr->debug_area_virt_addr =
-		dma_zalloc_coherent(ena_dev->dmadev, debug_area_size,
-				    &host_attr->debug_area_dma_addr, GFP_KERNEL);
+		dma_alloc_coherent(ena_dev->dmadev, debug_area_size,
+				   &host_attr->debug_area_dma_addr,
+				   GFP_KERNEL);
 	if (unlikely(!host_attr->debug_area_virt_addr)) {
 		host_attr->debug_area_size = 0;
 		return -ENOMEM;
* Unmerged path drivers/net/ethernet/apm/xgene-v2/main.c
diff --git a/drivers/net/ethernet/atheros/alx/main.c b/drivers/net/ethernet/atheros/alx/main.c
index a1f6adf6682e..5103305712a7 100644
--- a/drivers/net/ethernet/atheros/alx/main.c
+++ b/drivers/net/ethernet/atheros/alx/main.c
@@ -664,10 +664,9 @@ static int alx_alloc_rings(struct alx_priv *alx)
 			    alx->num_txq +
 			    sizeof(struct alx_rrd) * alx->rx_ringsz +
 			    sizeof(struct alx_rfd) * alx->rx_ringsz;
-	alx->descmem.virt = dma_zalloc_coherent(&alx->hw.pdev->dev,
-						alx->descmem.size,
-						&alx->descmem.dma,
-						GFP_KERNEL);
+	alx->descmem.virt = dma_alloc_coherent(&alx->hw.pdev->dev,
+					       alx->descmem.size,
+					       &alx->descmem.dma, GFP_KERNEL);
 	if (!alx->descmem.virt)
 		return -ENOMEM;
 
* Unmerged path drivers/net/ethernet/atheros/atl1c/atl1c_main.c
* Unmerged path drivers/net/ethernet/broadcom/bcm63xx_enet.c
* Unmerged path drivers/net/ethernet/broadcom/bcmsysport.c
* Unmerged path drivers/net/ethernet/broadcom/bgmac.c
* Unmerged path drivers/net/ethernet/broadcom/bnx2.c
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt.c b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
index 178f4c0452c9..b5b404365454 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
@@ -3264,19 +3264,19 @@ static int bnxt_alloc_stats(struct bnxt *bp)
 			return 0;
 
 		bp->hw_rx_port_stats_ext =
-			dma_zalloc_coherent(&pdev->dev,
-					    sizeof(struct rx_port_stats_ext),
-					    &bp->hw_rx_port_stats_ext_map,
-					    GFP_KERNEL);
+			dma_alloc_coherent(&pdev->dev,
+					   sizeof(struct rx_port_stats_ext),
+					   &bp->hw_rx_port_stats_ext_map,
+					   GFP_KERNEL);
 		if (!bp->hw_rx_port_stats_ext)
 			return 0;
 
 		if (bp->hwrm_spec_code >= 0x10902) {
 			bp->hw_tx_port_stats_ext =
-				dma_zalloc_coherent(&pdev->dev,
-					    sizeof(struct tx_port_stats_ext),
-					    &bp->hw_tx_port_stats_ext_map,
-					    GFP_KERNEL);
+				dma_alloc_coherent(&pdev->dev,
+						   sizeof(struct tx_port_stats_ext),
+						   &bp->hw_tx_port_stats_ext_map,
+						   GFP_KERNEL);
 		}
 		bp->flags |= BNXT_FLAG_PORT_STATS_EXT;
 	}
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt_dcb.c b/drivers/net/ethernet/broadcom/bnxt/bnxt_dcb.c
index 15c7041e937b..70775158c8c4 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt_dcb.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt_dcb.c
@@ -316,8 +316,8 @@ static int bnxt_hwrm_set_dcbx_app(struct bnxt *bp, struct dcb_app *app,
 
 	n = IEEE_8021QAZ_MAX_TCS;
 	data_len = sizeof(*data) + sizeof(*fw_app) * n;
-	data = dma_zalloc_coherent(&bp->pdev->dev, data_len, &mapping,
-				   GFP_KERNEL);
+	data = dma_alloc_coherent(&bp->pdev->dev, data_len, &mapping,
+				  GFP_KERNEL);
 	if (!data)
 		return -ENOMEM;
 
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt_devlink.c b/drivers/net/ethernet/broadcom/bnxt/bnxt_devlink.c
index 140dbd62106d..7f56032e44ac 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt_devlink.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt_devlink.c
@@ -85,8 +85,8 @@ static int bnxt_hwrm_nvm_req(struct bnxt *bp, u32 param_id, void *msg,
 		return -EFAULT;
 	}
 
-	data_addr = dma_zalloc_coherent(&bp->pdev->dev, bytesize,
-					&data_dma_addr, GFP_KERNEL);
+	data_addr = dma_alloc_coherent(&bp->pdev->dev, bytesize,
+				       &data_dma_addr, GFP_KERNEL);
 	if (!data_addr)
 		return -ENOMEM;
 
diff --git a/drivers/net/ethernet/broadcom/tg3.c b/drivers/net/ethernet/broadcom/tg3.c
index 34c729a025a9..d33a8fd41812 100644
--- a/drivers/net/ethernet/broadcom/tg3.c
+++ b/drivers/net/ethernet/broadcom/tg3.c
@@ -8696,10 +8696,10 @@ static int tg3_mem_rx_acquire(struct tg3 *tp)
 		if (!i && tg3_flag(tp, ENABLE_RSS))
 			continue;
 
-		tnapi->rx_rcb = dma_zalloc_coherent(&tp->pdev->dev,
-						    TG3_RX_RCB_RING_BYTES(tp),
-						    &tnapi->rx_rcb_mapping,
-						    GFP_KERNEL);
+		tnapi->rx_rcb = dma_alloc_coherent(&tp->pdev->dev,
+						   TG3_RX_RCB_RING_BYTES(tp),
+						   &tnapi->rx_rcb_mapping,
+						   GFP_KERNEL);
 		if (!tnapi->rx_rcb)
 			goto err_out;
 	}
@@ -8751,9 +8751,9 @@ static int tg3_alloc_consistent(struct tg3 *tp)
 {
 	int i;
 
-	tp->hw_stats = dma_zalloc_coherent(&tp->pdev->dev,
-					   sizeof(struct tg3_hw_stats),
-					   &tp->stats_mapping, GFP_KERNEL);
+	tp->hw_stats = dma_alloc_coherent(&tp->pdev->dev,
+					  sizeof(struct tg3_hw_stats),
+					  &tp->stats_mapping, GFP_KERNEL);
 	if (!tp->hw_stats)
 		goto err_out;
 
@@ -8761,10 +8761,10 @@ static int tg3_alloc_consistent(struct tg3 *tp)
 		struct tg3_napi *tnapi = &tp->napi[i];
 		struct tg3_hw_status *sblk;
 
-		tnapi->hw_status = dma_zalloc_coherent(&tp->pdev->dev,
-						       TG3_HW_STATUS_SIZE,
-						       &tnapi->status_mapping,
-						       GFP_KERNEL);
+		tnapi->hw_status = dma_alloc_coherent(&tp->pdev->dev,
+						      TG3_HW_STATUS_SIZE,
+						      &tnapi->status_mapping,
+						      GFP_KERNEL);
 		if (!tnapi->hw_status)
 			goto err_out;
 
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_queues.c
diff --git a/drivers/net/ethernet/chelsio/cxgb3/sge.c b/drivers/net/ethernet/chelsio/cxgb3/sge.c
index 36e0a1dcac88..376479799cc8 100644
--- a/drivers/net/ethernet/chelsio/cxgb3/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/sge.c
@@ -620,7 +620,7 @@ static void *alloc_ring(struct pci_dev *pdev, size_t nelem, size_t elem_size,
 {
 	size_t len = nelem * elem_size;
 	void *s = NULL;
-	void *p = dma_zalloc_coherent(&pdev->dev, len, phys, GFP_KERNEL);
+	void *p = dma_alloc_coherent(&pdev->dev, len, phys, GFP_KERNEL);
 
 	if (!p)
 		return NULL;
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index afbe558f3c09..730a9c20bab8 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -691,7 +691,7 @@ static void *alloc_ring(struct device *dev, size_t nelem, size_t elem_size,
 {
 	size_t len = nelem * elem_size + stat_size;
 	void *s = NULL;
-	void *p = dma_zalloc_coherent(dev, len, phys, GFP_KERNEL);
+	void *p = dma_alloc_coherent(dev, len, phys, GFP_KERNEL);
 
 	if (!p)
 		return NULL;
diff --git a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
index bb114a41f95b..ea9a6ba14eaa 100644
--- a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
@@ -755,7 +755,7 @@ static void *alloc_ring(struct device *dev, size_t nelem, size_t hwsize,
 	 * Allocate the hardware ring and PCI DMA bus address space for said.
 	 */
 	size_t hwlen = nelem * hwsize + stat_size;
-	void *hwring = dma_zalloc_coherent(dev, hwlen, busaddrp, GFP_KERNEL);
+	void *hwring = dma_alloc_coherent(dev, hwlen, busaddrp, GFP_KERNEL);
 
 	if (!hwring)
 		return NULL;
diff --git a/drivers/net/ethernet/emulex/benet/be_cmds.c b/drivers/net/ethernet/emulex/benet/be_cmds.c
index 1e9d882c04ef..59a7f0b99069 100644
--- a/drivers/net/ethernet/emulex/benet/be_cmds.c
+++ b/drivers/net/ethernet/emulex/benet/be_cmds.c
@@ -1808,9 +1808,9 @@ int be_cmd_get_fat_dump(struct be_adapter *adapter, u32 buf_len, void *buf)
 	total_size = buf_len;
 
 	get_fat_cmd.size = sizeof(struct be_cmd_req_get_fat) + 60*1024;
-	get_fat_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					     get_fat_cmd.size,
-					     &get_fat_cmd.dma, GFP_ATOMIC);
+	get_fat_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					    get_fat_cmd.size,
+					    &get_fat_cmd.dma, GFP_ATOMIC);
 	if (!get_fat_cmd.va)
 		return -ENOMEM;
 
@@ -2302,8 +2302,8 @@ int be_cmd_read_port_transceiver_data(struct be_adapter *adapter,
 		return -EINVAL;
 
 	cmd.size = sizeof(struct be_cmd_resp_port_type);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va) {
 		dev_err(&adapter->pdev->dev, "Memory allocation failed\n");
 		return -ENOMEM;
@@ -3066,8 +3066,8 @@ int lancer_fw_download(struct be_adapter *adapter,
 
 	flash_cmd.size = sizeof(struct lancer_cmd_req_write_object)
 				+ LANCER_FW_DOWNLOAD_CHUNK;
-	flash_cmd.va = dma_zalloc_coherent(dev, flash_cmd.size,
-					   &flash_cmd.dma, GFP_KERNEL);
+	flash_cmd.va = dma_alloc_coherent(dev, flash_cmd.size, &flash_cmd.dma,
+					  GFP_KERNEL);
 	if (!flash_cmd.va)
 		return -ENOMEM;
 
@@ -3184,8 +3184,8 @@ int be_fw_download(struct be_adapter *adapter, const struct firmware *fw)
 	}
 
 	flash_cmd.size = sizeof(struct be_cmd_write_flashrom);
-	flash_cmd.va = dma_zalloc_coherent(dev, flash_cmd.size, &flash_cmd.dma,
-					   GFP_KERNEL);
+	flash_cmd.va = dma_alloc_coherent(dev, flash_cmd.size, &flash_cmd.dma,
+					  GFP_KERNEL);
 	if (!flash_cmd.va)
 		return -ENOMEM;
 
@@ -3435,8 +3435,8 @@ int be_cmd_get_phy_info(struct be_adapter *adapter)
 		goto err;
 	}
 	cmd.size = sizeof(struct be_cmd_req_get_phy_info);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va) {
 		dev_err(&adapter->pdev->dev, "Memory alloc failure\n");
 		status = -ENOMEM;
@@ -3522,9 +3522,9 @@ int be_cmd_get_cntl_attributes(struct be_adapter *adapter)
 
 	memset(&attribs_cmd, 0, sizeof(struct be_dma_mem));
 	attribs_cmd.size = sizeof(struct be_cmd_resp_cntl_attribs);
-	attribs_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					     attribs_cmd.size,
-					     &attribs_cmd.dma, GFP_ATOMIC);
+	attribs_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					    attribs_cmd.size,
+					    &attribs_cmd.dma, GFP_ATOMIC);
 	if (!attribs_cmd.va) {
 		dev_err(&adapter->pdev->dev, "Memory allocation failure\n");
 		status = -ENOMEM;
@@ -3699,10 +3699,10 @@ int be_cmd_get_mac_from_list(struct be_adapter *adapter, u8 *mac,
 
 	memset(&get_mac_list_cmd, 0, sizeof(struct be_dma_mem));
 	get_mac_list_cmd.size = sizeof(struct be_cmd_resp_get_mac_list);
-	get_mac_list_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-						  get_mac_list_cmd.size,
-						  &get_mac_list_cmd.dma,
-						  GFP_ATOMIC);
+	get_mac_list_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+						 get_mac_list_cmd.size,
+						 &get_mac_list_cmd.dma,
+						 GFP_ATOMIC);
 
 	if (!get_mac_list_cmd.va) {
 		dev_err(&adapter->pdev->dev,
@@ -3829,8 +3829,8 @@ int be_cmd_set_mac_list(struct be_adapter *adapter, u8 *mac_array,
 
 	memset(&cmd, 0, sizeof(struct be_dma_mem));
 	cmd.size = sizeof(struct be_cmd_req_set_mac_list);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_KERNEL);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_KERNEL);
 	if (!cmd.va)
 		return -ENOMEM;
 
@@ -4035,8 +4035,8 @@ int be_cmd_get_acpi_wol_cap(struct be_adapter *adapter)
 
 	memset(&cmd, 0, sizeof(struct be_dma_mem));
 	cmd.size = sizeof(struct be_cmd_resp_acpi_wol_magic_config_v1);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va) {
 		dev_err(&adapter->pdev->dev, "Memory allocation failure\n");
 		status = -ENOMEM;
@@ -4089,9 +4089,9 @@ int be_cmd_set_fw_log_level(struct be_adapter *adapter, u32 level)
 
 	memset(&extfat_cmd, 0, sizeof(struct be_dma_mem));
 	extfat_cmd.size = sizeof(struct be_cmd_resp_get_ext_fat_caps);
-	extfat_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					    extfat_cmd.size, &extfat_cmd.dma,
-					    GFP_ATOMIC);
+	extfat_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					   extfat_cmd.size, &extfat_cmd.dma,
+					   GFP_ATOMIC);
 	if (!extfat_cmd.va)
 		return -ENOMEM;
 
@@ -4127,9 +4127,9 @@ int be_cmd_get_fw_log_level(struct be_adapter *adapter)
 
 	memset(&extfat_cmd, 0, sizeof(struct be_dma_mem));
 	extfat_cmd.size = sizeof(struct be_cmd_resp_get_ext_fat_caps);
-	extfat_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					    extfat_cmd.size, &extfat_cmd.dma,
-					    GFP_ATOMIC);
+	extfat_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					   extfat_cmd.size, &extfat_cmd.dma,
+					   GFP_ATOMIC);
 
 	if (!extfat_cmd.va) {
 		dev_err(&adapter->pdev->dev, "%s: Memory allocation failure\n",
@@ -4354,8 +4354,8 @@ int be_cmd_get_func_config(struct be_adapter *adapter, struct be_resources *res)
 
 	memset(&cmd, 0, sizeof(struct be_dma_mem));
 	cmd.size = sizeof(struct be_cmd_resp_get_func_config);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va) {
 		dev_err(&adapter->pdev->dev, "Memory alloc failure\n");
 		status = -ENOMEM;
@@ -4452,8 +4452,8 @@ int be_cmd_get_profile_config(struct be_adapter *adapter,
 
 	memset(&cmd, 0, sizeof(struct be_dma_mem));
 	cmd.size = sizeof(struct be_cmd_resp_get_profile_config);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va)
 		return -ENOMEM;
 
@@ -4539,8 +4539,8 @@ static int be_cmd_set_profile_config(struct be_adapter *adapter, void *desc,
 
 	memset(&cmd, 0, sizeof(struct be_dma_mem));
 	cmd.size = sizeof(struct be_cmd_req_set_profile_config);
-	cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
-				     GFP_ATOMIC);
+	cmd.va = dma_alloc_coherent(&adapter->pdev->dev, cmd.size, &cmd.dma,
+				    GFP_ATOMIC);
 	if (!cmd.va)
 		return -ENOMEM;
 
diff --git a/drivers/net/ethernet/emulex/benet/be_ethtool.c b/drivers/net/ethernet/emulex/benet/be_ethtool.c
index 7f7e206f95f8..8b35327e1bc8 100644
--- a/drivers/net/ethernet/emulex/benet/be_ethtool.c
+++ b/drivers/net/ethernet/emulex/benet/be_ethtool.c
@@ -274,8 +274,8 @@ static int lancer_cmd_read_file(struct be_adapter *adapter, u8 *file_name,
 	int status = 0;
 
 	read_cmd.size = LANCER_READ_FILE_CHUNK;
-	read_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev, read_cmd.size,
-					  &read_cmd.dma, GFP_ATOMIC);
+	read_cmd.va = dma_alloc_coherent(&adapter->pdev->dev, read_cmd.size,
+					 &read_cmd.dma, GFP_ATOMIC);
 
 	if (!read_cmd.va) {
 		dev_err(&adapter->pdev->dev,
@@ -814,7 +814,7 @@ static int be_set_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)
 	}
 
 	cmd.size = sizeof(struct be_cmd_req_acpi_wol_magic_config);
-	cmd.va = dma_zalloc_coherent(dev, cmd.size, &cmd.dma, GFP_KERNEL);
+	cmd.va = dma_alloc_coherent(dev, cmd.size, &cmd.dma, GFP_KERNEL);
 	if (!cmd.va)
 		return -ENOMEM;
 
@@ -850,9 +850,9 @@ static int be_test_ddr_dma(struct be_adapter *adapter)
 	};
 
 	ddrdma_cmd.size = sizeof(struct be_cmd_req_ddrdma_test);
-	ddrdma_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					    ddrdma_cmd.size, &ddrdma_cmd.dma,
-					    GFP_KERNEL);
+	ddrdma_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					   ddrdma_cmd.size, &ddrdma_cmd.dma,
+					   GFP_KERNEL);
 	if (!ddrdma_cmd.va)
 		return -ENOMEM;
 
@@ -1013,9 +1013,9 @@ static int be_read_eeprom(struct net_device *netdev,
 
 	memset(&eeprom_cmd, 0, sizeof(struct be_dma_mem));
 	eeprom_cmd.size = sizeof(struct be_cmd_req_seeprom_read);
-	eeprom_cmd.va = dma_zalloc_coherent(&adapter->pdev->dev,
-					    eeprom_cmd.size, &eeprom_cmd.dma,
-					    GFP_KERNEL);
+	eeprom_cmd.va = dma_alloc_coherent(&adapter->pdev->dev,
+					   eeprom_cmd.size, &eeprom_cmd.dma,
+					   GFP_KERNEL);
 
 	if (!eeprom_cmd.va)
 		return -ENOMEM;
diff --git a/drivers/net/ethernet/emulex/benet/be_main.c b/drivers/net/ethernet/emulex/benet/be_main.c
index 6a4181e1f95b..430bc207b442 100644
--- a/drivers/net/ethernet/emulex/benet/be_main.c
+++ b/drivers/net/ethernet/emulex/benet/be_main.c
@@ -159,8 +159,8 @@ static int be_queue_alloc(struct be_adapter *adapter, struct be_queue_info *q,
 	q->len = len;
 	q->entry_size = entry_size;
 	mem->size = len * entry_size;
-	mem->va = dma_zalloc_coherent(&adapter->pdev->dev, mem->size, &mem->dma,
-				      GFP_KERNEL);
+	mem->va = dma_alloc_coherent(&adapter->pdev->dev, mem->size,
+				     &mem->dma, GFP_KERNEL);
 	if (!mem->va)
 		return -ENOMEM;
 	return 0;
@@ -5677,9 +5677,9 @@ static int be_drv_init(struct be_adapter *adapter)
 	int status = 0;
 
 	mbox_mem_alloc->size = sizeof(struct be_mcc_mailbox) + 16;
-	mbox_mem_alloc->va = dma_zalloc_coherent(dev, mbox_mem_alloc->size,
-						 &mbox_mem_alloc->dma,
-						 GFP_KERNEL);
+	mbox_mem_alloc->va = dma_alloc_coherent(dev, mbox_mem_alloc->size,
+						&mbox_mem_alloc->dma,
+						GFP_KERNEL);
 	if (!mbox_mem_alloc->va)
 		return -ENOMEM;
 
@@ -5688,8 +5688,8 @@ static int be_drv_init(struct be_adapter *adapter)
 	mbox_mem_align->dma = PTR_ALIGN(mbox_mem_alloc->dma, 16);
 
 	rx_filter->size = sizeof(struct be_cmd_req_rx_filter);
-	rx_filter->va = dma_zalloc_coherent(dev, rx_filter->size,
-					    &rx_filter->dma, GFP_KERNEL);
+	rx_filter->va = dma_alloc_coherent(dev, rx_filter->size,
+					   &rx_filter->dma, GFP_KERNEL);
 	if (!rx_filter->va) {
 		status = -ENOMEM;
 		goto free_mbox;
@@ -5703,8 +5703,8 @@ static int be_drv_init(struct be_adapter *adapter)
 		stats_cmd->size = sizeof(struct be_cmd_req_get_stats_v1);
 	else
 		stats_cmd->size = sizeof(struct be_cmd_req_get_stats_v2);
-	stats_cmd->va = dma_zalloc_coherent(dev, stats_cmd->size,
-					    &stats_cmd->dma, GFP_KERNEL);
+	stats_cmd->va = dma_alloc_coherent(dev, stats_cmd->size,
+					   &stats_cmd->dma, GFP_KERNEL);
 	if (!stats_cmd->va) {
 		status = -ENOMEM;
 		goto free_rx_filter;
* Unmerged path drivers/net/ethernet/faraday/ftgmac100.c
* Unmerged path drivers/net/ethernet/faraday/ftmac100.c
* Unmerged path drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3pf/hclge_cmd.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3vf/hclgevf_cmd.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_api_cmd.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_eqs.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_io.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_qp.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_hw_wq.c
* Unmerged path drivers/net/ethernet/ibm/emac/mal.c
* Unmerged path drivers/net/ethernet/intel/e1000/e1000_ethtool.c
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index e2400a9cba88..510554d4f052 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -2326,8 +2326,8 @@ static int e1000_alloc_ring_dma(struct e1000_adapter *adapter,
 {
 	struct pci_dev *pdev = adapter->pdev;
 
-	ring->desc = dma_zalloc_coherent(&pdev->dev, ring->size, &ring->dma,
-					 GFP_KERNEL);
+	ring->desc = dma_alloc_coherent(&pdev->dev, ring->size, &ring->dma,
+					GFP_KERNEL);
 	if (!ring->desc)
 		return -ENOMEM;
 
diff --git a/drivers/net/ethernet/intel/i40e/i40e_main.c b/drivers/net/ethernet/intel/i40e/i40e_main.c
index 1287392c94d6..6e95e8a43c7f 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@ -109,8 +109,8 @@ int i40e_allocate_dma_mem_d(struct i40e_hw *hw, struct i40e_dma_mem *mem,
 	struct i40e_pf *pf = (struct i40e_pf *)hw->back;
 
 	mem->size = ALIGN(size, alignment);
-	mem->va = dma_zalloc_coherent(&pf->pdev->dev, mem->size,
-				      &mem->pa, GFP_KERNEL);
+	mem->va = dma_alloc_coherent(&pf->pdev->dev, mem->size, &mem->pa,
+				     GFP_KERNEL);
 	if (!mem->va)
 		return -ENOMEM;
 
* Unmerged path drivers/net/ethernet/intel/ixgb/ixgb_main.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
* Unmerged path drivers/net/ethernet/marvell/pxa168_eth.c
* Unmerged path drivers/net/ethernet/mediatek/mtk_eth_soc.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/alloc.c b/drivers/net/ethernet/mellanox/mlx4/alloc.c
index 94f4dc4a77e9..7e85464e2577 100644
--- a/drivers/net/ethernet/mellanox/mlx4/alloc.c
+++ b/drivers/net/ethernet/mellanox/mlx4/alloc.c
@@ -584,8 +584,8 @@ static int mlx4_buf_direct_alloc(struct mlx4_dev *dev, int size,
 	buf->npages       = 1;
 	buf->page_shift   = get_order(size) + PAGE_SHIFT;
 	buf->direct.buf   =
-		dma_zalloc_coherent(&dev->persist->pdev->dev,
-				    size, &t, GFP_KERNEL);
+		dma_alloc_coherent(&dev->persist->pdev->dev, size, &t,
+				   GFP_KERNEL);
 	if (!buf->direct.buf)
 		return -ENOMEM;
 
@@ -624,8 +624,8 @@ int mlx4_buf_alloc(struct mlx4_dev *dev, int size, int max_direct,
 
 		for (i = 0; i < buf->nbufs; ++i) {
 			buf->page_list[i].buf =
-				dma_zalloc_coherent(&dev->persist->pdev->dev,
-						    PAGE_SIZE, &t, GFP_KERNEL);
+				dma_alloc_coherent(&dev->persist->pdev->dev,
+						   PAGE_SIZE, &t, GFP_KERNEL);
 			if (!buf->page_list[i].buf)
 				goto err_free;
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/alloc.c b/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
index 323ffe8bf7e4..7270fdad9fdd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
@@ -63,8 +63,8 @@ static void *mlx5_dma_zalloc_coherent_node(struct mlx5_core_dev *dev,
 	mutex_lock(&priv->alloc_mutex);
 	original_node = dev_to_node(&dev->pdev->dev);
 	set_dev_node(&dev->pdev->dev, node);
-	cpu_handle = dma_zalloc_coherent(&dev->pdev->dev, size,
-					 dma_handle, GFP_KERNEL);
+	cpu_handle = dma_alloc_coherent(&dev->pdev->dev, size, dma_handle,
+					GFP_KERNEL);
 	set_dev_node(&dev->pdev->dev, original_node);
 	mutex_unlock(&priv->alloc_mutex);
 	return cpu_handle;
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
index 2b68a3b4a702..90d45ff03ad1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -1718,8 +1718,8 @@ static int alloc_cmd_page(struct mlx5_core_dev *dev, struct mlx5_cmd *cmd)
 {
 	struct device *ddev = &dev->pdev->dev;
 
-	cmd->cmd_alloc_buf = dma_zalloc_coherent(ddev, MLX5_ADAPTER_PAGE_SIZE,
-						 &cmd->alloc_dma, GFP_KERNEL);
+	cmd->cmd_alloc_buf = dma_alloc_coherent(ddev, MLX5_ADAPTER_PAGE_SIZE,
+						&cmd->alloc_dma, GFP_KERNEL);
 	if (!cmd->cmd_alloc_buf)
 		return -ENOMEM;
 
@@ -1733,9 +1733,9 @@ static int alloc_cmd_page(struct mlx5_core_dev *dev, struct mlx5_cmd *cmd)
 
 	dma_free_coherent(ddev, MLX5_ADAPTER_PAGE_SIZE, cmd->cmd_alloc_buf,
 			  cmd->alloc_dma);
-	cmd->cmd_alloc_buf = dma_zalloc_coherent(ddev,
-						 2 * MLX5_ADAPTER_PAGE_SIZE - 1,
-						 &cmd->alloc_dma, GFP_KERNEL);
+	cmd->cmd_alloc_buf = dma_alloc_coherent(ddev,
+						2 * MLX5_ADAPTER_PAGE_SIZE - 1,
+						&cmd->alloc_dma, GFP_KERNEL);
 	if (!cmd->cmd_alloc_buf)
 		return -ENOMEM;
 
* Unmerged path drivers/net/ethernet/myricom/myri10ge/myri10ge.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
* Unmerged path drivers/net/ethernet/ni/nixge.c
* Unmerged path drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
* Unmerged path drivers/net/ethernet/pasemi/pasemi_mac.c
diff --git a/drivers/net/ethernet/qlogic/qed/qed_cxt.c b/drivers/net/ethernet/qlogic/qed/qed_cxt.c
index dc1c1b616084..c2ad405b2f50 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_cxt.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_cxt.c
@@ -936,9 +936,9 @@ static int qed_cxt_src_t2_alloc(struct qed_hwfn *p_hwfn)
 		u32 size = min_t(u32, total_size, psz);
 		void **p_virt = &p_mngr->t2[i].p_virt;
 
-		*p_virt = dma_zalloc_coherent(&p_hwfn->cdev->pdev->dev,
-					      size, &p_mngr->t2[i].p_phys,
-					      GFP_KERNEL);
+		*p_virt = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev, size,
+					     &p_mngr->t2[i].p_phys,
+					     GFP_KERNEL);
 		if (!p_mngr->t2[i].p_virt) {
 			rc = -ENOMEM;
 			goto t2_fail;
@@ -1054,8 +1054,8 @@ static int qed_ilt_blk_alloc(struct qed_hwfn *p_hwfn,
 		u32 size;
 
 		size = min_t(u32, sz_left, p_blk->real_size_in_page);
-		p_virt = dma_zalloc_coherent(&p_hwfn->cdev->pdev->dev, size,
-					     &p_phys, GFP_KERNEL);
+		p_virt = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev, size,
+					    &p_phys, GFP_KERNEL);
 		if (!p_virt)
 			return -ENOMEM;
 
@@ -2306,9 +2306,9 @@ qed_cxt_dynamic_ilt_alloc(struct qed_hwfn *p_hwfn,
 		goto out0;
 	}
 
-	p_virt = dma_zalloc_coherent(&p_hwfn->cdev->pdev->dev,
-				     p_blk->real_size_in_page, &p_phys,
-				     GFP_KERNEL);
+	p_virt = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,
+				    p_blk->real_size_in_page, &p_phys,
+				    GFP_KERNEL);
 	if (!p_virt) {
 		rc = -ENOMEM;
 		goto out1;
diff --git a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_ctx.c b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_ctx.c
index d344e9d43832..af38d3d73291 100644
--- a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_ctx.c
+++ b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_ctx.c
@@ -434,14 +434,14 @@ int qlcnic_82xx_fw_cmd_create_tx_ctx(struct qlcnic_adapter *adapter,
 	*(tx_ring->hw_consumer) = 0;
 
 	rq_size = SIZEOF_HOSTRQ_TX(struct qlcnic_hostrq_tx_ctx);
-	rq_addr = dma_zalloc_coherent(&adapter->pdev->dev, rq_size,
-				      &rq_phys_addr, GFP_KERNEL);
+	rq_addr = dma_alloc_coherent(&adapter->pdev->dev, rq_size,
+				     &rq_phys_addr, GFP_KERNEL);
 	if (!rq_addr)
 		return -ENOMEM;
 
 	rsp_size = SIZEOF_CARDRSP_TX(struct qlcnic_cardrsp_tx_ctx);
-	rsp_addr = dma_zalloc_coherent(&adapter->pdev->dev, rsp_size,
-				       &rsp_phys_addr, GFP_KERNEL);
+	rsp_addr = dma_alloc_coherent(&adapter->pdev->dev, rsp_size,
+				      &rsp_phys_addr, GFP_KERNEL);
 	if (!rsp_addr) {
 		err = -ENOMEM;
 		goto out_free_rq;
@@ -855,8 +855,8 @@ int qlcnic_82xx_get_nic_info(struct qlcnic_adapter *adapter,
 	struct qlcnic_cmd_args cmd;
 	size_t  nic_size = sizeof(struct qlcnic_info_le);
 
-	nic_info_addr = dma_zalloc_coherent(&adapter->pdev->dev, nic_size,
-					    &nic_dma_t, GFP_KERNEL);
+	nic_info_addr = dma_alloc_coherent(&adapter->pdev->dev, nic_size,
+					   &nic_dma_t, GFP_KERNEL);
 	if (!nic_info_addr)
 		return -ENOMEM;
 
@@ -909,8 +909,8 @@ int qlcnic_82xx_set_nic_info(struct qlcnic_adapter *adapter,
 	if (adapter->ahw->op_mode != QLCNIC_MGMT_FUNC)
 		return err;
 
-	nic_info_addr = dma_zalloc_coherent(&adapter->pdev->dev, nic_size,
-					    &nic_dma_t, GFP_KERNEL);
+	nic_info_addr = dma_alloc_coherent(&adapter->pdev->dev, nic_size,
+					   &nic_dma_t, GFP_KERNEL);
 	if (!nic_info_addr)
 		return -ENOMEM;
 
@@ -964,8 +964,8 @@ int qlcnic_82xx_get_pci_info(struct qlcnic_adapter *adapter,
 	void *pci_info_addr;
 	int err = 0, i;
 
-	pci_info_addr = dma_zalloc_coherent(&adapter->pdev->dev, pci_size,
-					    &pci_info_dma_t, GFP_KERNEL);
+	pci_info_addr = dma_alloc_coherent(&adapter->pdev->dev, pci_size,
+					   &pci_info_dma_t, GFP_KERNEL);
 	if (!pci_info_addr)
 		return -ENOMEM;
 
@@ -1078,8 +1078,8 @@ int qlcnic_get_port_stats(struct qlcnic_adapter *adapter, const u8 func,
 		return -EIO;
 	}
 
-	stats_addr = dma_zalloc_coherent(&adapter->pdev->dev, stats_size,
-					 &stats_dma_t, GFP_KERNEL);
+	stats_addr = dma_alloc_coherent(&adapter->pdev->dev, stats_size,
+					&stats_dma_t, GFP_KERNEL);
 	if (!stats_addr)
 		return -ENOMEM;
 
@@ -1134,8 +1134,8 @@ int qlcnic_get_mac_stats(struct qlcnic_adapter *adapter,
 	if (mac_stats == NULL)
 		return -ENOMEM;
 
-	stats_addr = dma_zalloc_coherent(&adapter->pdev->dev, stats_size,
-					 &stats_dma_t, GFP_KERNEL);
+	stats_addr = dma_alloc_coherent(&adapter->pdev->dev, stats_size,
+					&stats_dma_t, GFP_KERNEL);
 	if (!stats_addr)
 		return -ENOMEM;
 
* Unmerged path drivers/net/ethernet/qualcomm/emac/emac-mac.c
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
diff --git a/drivers/net/ethernet/sfc/falcon/nic.c b/drivers/net/ethernet/sfc/falcon/nic.c
index a8ecb33390da..9c07b5175581 100644
--- a/drivers/net/ethernet/sfc/falcon/nic.c
+++ b/drivers/net/ethernet/sfc/falcon/nic.c
@@ -33,8 +33,8 @@
 int ef4_nic_alloc_buffer(struct ef4_nic *efx, struct ef4_buffer *buffer,
 			 unsigned int len, gfp_t gfp_flags)
 {
-	buffer->addr = dma_zalloc_coherent(&efx->pci_dev->dev, len,
-					   &buffer->dma_addr, gfp_flags);
+	buffer->addr = dma_alloc_coherent(&efx->pci_dev->dev, len,
+					  &buffer->dma_addr, gfp_flags);
 	if (!buffer->addr)
 		return -ENOMEM;
 	buffer->len = len;
diff --git a/drivers/net/ethernet/sfc/nic.c b/drivers/net/ethernet/sfc/nic.c
index aa1945a858d5..c2d45a40eb48 100644
--- a/drivers/net/ethernet/sfc/nic.c
+++ b/drivers/net/ethernet/sfc/nic.c
@@ -34,8 +34,8 @@
 int efx_nic_alloc_buffer(struct efx_nic *efx, struct efx_buffer *buffer,
 			 unsigned int len, gfp_t gfp_flags)
 {
-	buffer->addr = dma_zalloc_coherent(&efx->pci_dev->dev, len,
-					   &buffer->dma_addr, gfp_flags);
+	buffer->addr = dma_alloc_coherent(&efx->pci_dev->dev, len,
+					  &buffer->dma_addr, gfp_flags);
 	if (!buffer->addr)
 		return -ENOMEM;
 	buffer->len = len;
* Unmerged path drivers/net/ethernet/sgi/meth.c
* Unmerged path drivers/net/ethernet/socionext/netsec.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
* Unmerged path drivers/net/ethernet/tundra/tsi108_eth.c
* Unmerged path drivers/net/ethernet/xilinx/ll_temac_main.c
* Unmerged path drivers/net/ethernet/xilinx/xilinx_axienet_main.c
* Unmerged path drivers/net/fddi/defxx.c
* Unmerged path drivers/net/fddi/skfp/skfddi.c
diff --git a/drivers/net/vmxnet3/vmxnet3_drv.c b/drivers/net/vmxnet3/vmxnet3_drv.c
index 38baf3345fd9..8a4b65781988 100644
--- a/drivers/net/vmxnet3/vmxnet3_drv.c
+++ b/drivers/net/vmxnet3/vmxnet3_drv.c
@@ -535,8 +535,8 @@ vmxnet3_tq_create(struct vmxnet3_tx_queue *tq,
 	}
 
 	sz = tq->tx_ring.size * sizeof(tq->buf_info[0]);
-	tq->buf_info = dma_zalloc_coherent(&adapter->pdev->dev, sz,
-					   &tq->buf_info_pa, GFP_KERNEL);
+	tq->buf_info = dma_alloc_coherent(&adapter->pdev->dev, sz,
+					  &tq->buf_info_pa, GFP_KERNEL);
 	if (!tq->buf_info)
 		goto err;
 
@@ -1813,8 +1813,8 @@ vmxnet3_rq_create(struct vmxnet3_rx_queue *rq, struct vmxnet3_adapter *adapter)
 
 	sz = sizeof(struct vmxnet3_rx_buf_info) * (rq->rx_ring[0].size +
 						   rq->rx_ring[1].size);
-	bi = dma_zalloc_coherent(&adapter->pdev->dev, sz, &rq->buf_info_pa,
-				 GFP_KERNEL);
+	bi = dma_alloc_coherent(&adapter->pdev->dev, sz, &rq->buf_info_pa,
+				GFP_KERNEL);
 	if (!bi)
 		goto err;
 
* Unmerged path drivers/net/wan/fsl_ucc_hdlc.c
diff --git a/drivers/net/wireless/ath/ath10k/ce.c b/drivers/net/wireless/ath/ath10k/ce.c
index a8afd690290f..56de0f138bec 100644
--- a/drivers/net/wireless/ath/ath10k/ce.c
+++ b/drivers/net/wireless/ath/ath10k/ce.c
@@ -1028,10 +1028,9 @@ ath10k_ce_alloc_dest_ring(struct ath10k *ar, unsigned int ce_id,
 	 * coherent DMA are unsupported
 	 */
 	dest_ring->base_addr_owner_space_unaligned =
-		dma_zalloc_coherent(ar->dev,
-				    (nentries * sizeof(struct ce_desc) +
-				     CE_DESC_RING_ALIGN),
-				    &base_addr, GFP_KERNEL);
+		dma_alloc_coherent(ar->dev,
+				   (nentries * sizeof(struct ce_desc) + CE_DESC_RING_ALIGN),
+				   &base_addr, GFP_KERNEL);
 	if (!dest_ring->base_addr_owner_space_unaligned) {
 		kfree(dest_ring);
 		return ERR_PTR(-ENOMEM);
diff --git a/drivers/net/wireless/ath/ath10k/mac.c b/drivers/net/wireless/ath/ath10k/mac.c
index 5683f1a5330e..aefa1f051a30 100644
--- a/drivers/net/wireless/ath/ath10k/mac.c
+++ b/drivers/net/wireless/ath/ath10k/mac.c
@@ -5039,10 +5039,10 @@ static int ath10k_add_interface(struct ieee80211_hw *hw,
 	if (vif->type == NL80211_IFTYPE_ADHOC ||
 	    vif->type == NL80211_IFTYPE_MESH_POINT ||
 	    vif->type == NL80211_IFTYPE_AP) {
-		arvif->beacon_buf = dma_zalloc_coherent(ar->dev,
-							IEEE80211_MAX_FRAME_LEN,
-							&arvif->beacon_paddr,
-							GFP_ATOMIC);
+		arvif->beacon_buf = dma_alloc_coherent(ar->dev,
+						       IEEE80211_MAX_FRAME_LEN,
+						       &arvif->beacon_paddr,
+						       GFP_ATOMIC);
 		if (!arvif->beacon_buf) {
 			ret = -ENOMEM;
 			ath10k_warn(ar, "failed to allocate beacon buffer: %d\n",
diff --git a/drivers/net/wireless/ath/ath10k/pci.c b/drivers/net/wireless/ath/ath10k/pci.c
index e27cac272688..4f4d9f04ef0e 100644
--- a/drivers/net/wireless/ath/ath10k/pci.c
+++ b/drivers/net/wireless/ath/ath10k/pci.c
@@ -906,8 +906,7 @@ static int ath10k_pci_diag_read_mem(struct ath10k *ar, u32 address, void *data,
 	 */
 	alloc_nbytes = min_t(unsigned int, nbytes, DIAG_TRANSFER_LIMIT);
 
-	data_buf = (unsigned char *)dma_zalloc_coherent(ar->dev,
-						       alloc_nbytes,
+	data_buf = (unsigned char *)dma_alloc_coherent(ar->dev, alloc_nbytes,
 						       &ce_data_base,
 						       GFP_ATOMIC);
 
* Unmerged path drivers/net/wireless/ath/wcn36xx/dxe.c
* Unmerged path drivers/net/wireless/ath/wil6210/txrx_edma.c
* Unmerged path drivers/net/wireless/b43/dma.c
* Unmerged path drivers/net/wireless/b43legacy/dma.c
* Unmerged path drivers/net/wireless/broadcom/brcm80211/brcmfmac/pcie.c
* Unmerged path drivers/net/wireless/intel/iwlwifi/pcie/rx.c
diff --git a/drivers/net/wireless/ralink/rt2x00/rt2x00mmio.c b/drivers/net/wireless/ralink/rt2x00/rt2x00mmio.c
index 528cb0401df1..4956a54151cb 100644
--- a/drivers/net/wireless/ralink/rt2x00/rt2x00mmio.c
+++ b/drivers/net/wireless/ralink/rt2x00/rt2x00mmio.c
@@ -119,9 +119,9 @@ static int rt2x00mmio_alloc_queue_dma(struct rt2x00_dev *rt2x00dev,
 	/*
 	 * Allocate DMA memory for descriptor and buffer.
 	 */
-	addr = dma_zalloc_coherent(rt2x00dev->dev,
-				   queue->limit * queue->desc_size, &dma,
-				   GFP_KERNEL);
+	addr = dma_alloc_coherent(rt2x00dev->dev,
+				  queue->limit * queue->desc_size, &dma,
+				  GFP_KERNEL);
 	if (!addr)
 		return -ENOMEM;
 
* Unmerged path drivers/ntb/hw/mscc/ntb_hw_switchtec.c
* Unmerged path drivers/nvme/host/pci.c
* Unmerged path drivers/pci/controller/pcie-iproc-msi.c
* Unmerged path drivers/pci/switch/switchtec.c
diff --git a/drivers/rapidio/devices/tsi721.c b/drivers/rapidio/devices/tsi721.c
index 69872237f498..44af83584e38 100644
--- a/drivers/rapidio/devices/tsi721.c
+++ b/drivers/rapidio/devices/tsi721.c
@@ -1008,9 +1008,9 @@ static int tsi721_doorbell_init(struct tsi721_device *priv)
 	INIT_WORK(&priv->idb_work, tsi721_db_dpc);
 
 	/* Allocate buffer for inbound doorbells queue */
-	priv->idb_base = dma_zalloc_coherent(&priv->pdev->dev,
-				IDB_QSIZE * TSI721_IDB_ENTRY_SIZE,
-				&priv->idb_dma, GFP_KERNEL);
+	priv->idb_base = dma_alloc_coherent(&priv->pdev->dev,
+					    IDB_QSIZE * TSI721_IDB_ENTRY_SIZE,
+					    &priv->idb_dma, GFP_KERNEL);
 	if (!priv->idb_base)
 		return -ENOMEM;
 
@@ -1073,9 +1073,9 @@ static int tsi721_bdma_maint_init(struct tsi721_device *priv)
 	regs = priv->regs + TSI721_DMAC_BASE(TSI721_DMACH_MAINT);
 
 	/* Allocate space for DMA descriptors */
-	bd_ptr = dma_zalloc_coherent(&priv->pdev->dev,
-					bd_num * sizeof(struct tsi721_dma_desc),
-					&bd_phys, GFP_KERNEL);
+	bd_ptr = dma_alloc_coherent(&priv->pdev->dev,
+				    bd_num * sizeof(struct tsi721_dma_desc),
+				    &bd_phys, GFP_KERNEL);
 	if (!bd_ptr)
 		return -ENOMEM;
 
@@ -1090,7 +1090,7 @@ static int tsi721_bdma_maint_init(struct tsi721_device *priv)
 	sts_size = (bd_num >= TSI721_DMA_MINSTSSZ) ?
 					bd_num : TSI721_DMA_MINSTSSZ;
 	sts_size = roundup_pow_of_two(sts_size);
-	sts_ptr = dma_zalloc_coherent(&priv->pdev->dev,
+	sts_ptr = dma_alloc_coherent(&priv->pdev->dev,
 				     sts_size * sizeof(struct tsi721_dma_sts),
 				     &sts_phys, GFP_KERNEL);
 	if (!sts_ptr) {
@@ -1544,10 +1544,10 @@ static int tsi721_open_outb_mbox(struct rio_mport *mport, void *dev_id,
 
 	/* Outbound message descriptor status FIFO allocation */
 	priv->omsg_ring[mbox].sts_size = roundup_pow_of_two(entries + 1);
-	priv->omsg_ring[mbox].sts_base = dma_zalloc_coherent(&priv->pdev->dev,
-			priv->omsg_ring[mbox].sts_size *
-						sizeof(struct tsi721_dma_sts),
-			&priv->omsg_ring[mbox].sts_phys, GFP_KERNEL);
+	priv->omsg_ring[mbox].sts_base = dma_alloc_coherent(&priv->pdev->dev,
+							    priv->omsg_ring[mbox].sts_size * sizeof(struct tsi721_dma_sts),
+							    &priv->omsg_ring[mbox].sts_phys,
+							    GFP_KERNEL);
 	if (priv->omsg_ring[mbox].sts_base == NULL) {
 		dev_dbg(&priv->pdev->dev,
 			"Unable to allocate OB MSG descriptor status FIFO "
* Unmerged path drivers/rapidio/devices/tsi721_dma.c
* Unmerged path drivers/s390/net/ism_drv.c
* Unmerged path drivers/scsi/3w-sas.c
* Unmerged path drivers/scsi/a100u2w.c
* Unmerged path drivers/scsi/arcmsr/arcmsr_hba.c
* Unmerged path drivers/scsi/be2iscsi/be_main.c
* Unmerged path drivers/scsi/be2iscsi/be_mgmt.c
* Unmerged path drivers/scsi/bfa/bfad_bsg.c
* Unmerged path drivers/scsi/bnx2fc/bnx2fc_hwi.c
* Unmerged path drivers/scsi/bnx2fc/bnx2fc_tgt.c
diff --git a/drivers/scsi/bnx2i/bnx2i_hwi.c b/drivers/scsi/bnx2i/bnx2i_hwi.c
index 8737e202227b..546038dbf635 100644
--- a/drivers/scsi/bnx2i/bnx2i_hwi.c
+++ b/drivers/scsi/bnx2i/bnx2i_hwi.c
@@ -1073,8 +1073,8 @@ int bnx2i_alloc_qp_resc(struct bnx2i_hba *hba, struct bnx2i_endpoint *ep)
 
 	/* Allocate memory area for actual SQ element */
 	ep->qp.sq_virt =
-		dma_zalloc_coherent(&hba->pcidev->dev, ep->qp.sq_mem_size,
-					&ep->qp.sq_phys, GFP_KERNEL);
+		dma_alloc_coherent(&hba->pcidev->dev, ep->qp.sq_mem_size,
+				   &ep->qp.sq_phys, GFP_KERNEL);
 	if (!ep->qp.sq_virt) {
 		printk(KERN_ALERT "bnx2i: unable to alloc SQ BD memory %d\n",
 				  ep->qp.sq_mem_size);
@@ -1109,8 +1109,8 @@ int bnx2i_alloc_qp_resc(struct bnx2i_hba *hba, struct bnx2i_endpoint *ep)
 
 	/* Allocate memory area for actual CQ element */
 	ep->qp.cq_virt =
-		dma_zalloc_coherent(&hba->pcidev->dev, ep->qp.cq_mem_size,
-					&ep->qp.cq_phys, GFP_KERNEL);
+		dma_alloc_coherent(&hba->pcidev->dev, ep->qp.cq_mem_size,
+				   &ep->qp.cq_phys, GFP_KERNEL);
 	if (!ep->qp.cq_virt) {
 		printk(KERN_ALERT "bnx2i: unable to alloc CQ BD memory %d\n",
 				  ep->qp.cq_mem_size);
* Unmerged path drivers/scsi/csiostor/csio_wr.c
diff --git a/drivers/scsi/lpfc/lpfc_bsg.c b/drivers/scsi/lpfc/lpfc_bsg.c
index 664fa2d0dac5..ce0f6539684d 100644
--- a/drivers/scsi/lpfc/lpfc_bsg.c
+++ b/drivers/scsi/lpfc/lpfc_bsg.c
@@ -2696,8 +2696,8 @@ lpfc_bsg_dma_page_alloc(struct lpfc_hba *phba)
 	INIT_LIST_HEAD(&dmabuf->list);
 
 	/* now, allocate dma buffer */
-	dmabuf->virt = dma_zalloc_coherent(&pcidev->dev, BSG_MBOX_SIZE,
-					   &(dmabuf->phys), GFP_KERNEL);
+	dmabuf->virt = dma_alloc_coherent(&pcidev->dev, BSG_MBOX_SIZE,
+					  &(dmabuf->phys), GFP_KERNEL);
 
 	if (!dmabuf->virt) {
 		kfree(dmabuf);
diff --git a/drivers/scsi/lpfc/lpfc_init.c b/drivers/scsi/lpfc/lpfc_init.c
index 627f7716d6d1..1f801af8e6a9 100644
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@ -6871,9 +6871,9 @@ lpfc_sli4_create_rpi_hdr(struct lpfc_hba *phba)
 	if (!dmabuf)
 		return NULL;
 
-	dmabuf->virt = dma_zalloc_coherent(&phba->pcidev->dev,
-					   LPFC_HDR_TEMPLATE_SIZE,
-					   &dmabuf->phys, GFP_KERNEL);
+	dmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev,
+					  LPFC_HDR_TEMPLATE_SIZE,
+					  &dmabuf->phys, GFP_KERNEL);
 	if (!dmabuf->virt) {
 		rpi_hdr = NULL;
 		goto err_free_dmabuf;
@@ -7293,8 +7293,8 @@ lpfc_sli_pci_mem_setup(struct lpfc_hba *phba)
 	}
 
 	/* Allocate memory for SLI-2 structures */
-	phba->slim2p.virt = dma_zalloc_coherent(&pdev->dev, SLI2_SLIM_SIZE,
-						&phba->slim2p.phys, GFP_KERNEL);
+	phba->slim2p.virt = dma_alloc_coherent(&pdev->dev, SLI2_SLIM_SIZE,
+					       &phba->slim2p.phys, GFP_KERNEL);
 	if (!phba->slim2p.virt)
 		goto out_iounmap;
 
@@ -7712,8 +7712,8 @@ lpfc_create_bootstrap_mbox(struct lpfc_hba *phba)
 	 * plus an alignment restriction of 16 bytes.
 	 */
 	bmbx_size = sizeof(struct lpfc_bmbx_create) + (LPFC_ALIGN_16_BYTE - 1);
-	dmabuf->virt = dma_zalloc_coherent(&phba->pcidev->dev, bmbx_size,
-					   &dmabuf->phys, GFP_KERNEL);
+	dmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev, bmbx_size,
+					  &dmabuf->phys, GFP_KERNEL);
 	if (!dmabuf->virt) {
 		kfree(dmabuf);
 		return -ENOMEM;
diff --git a/drivers/scsi/lpfc/lpfc_mbox.c b/drivers/scsi/lpfc/lpfc_mbox.c
index f6a5083a621e..4d3b94317515 100644
--- a/drivers/scsi/lpfc/lpfc_mbox.c
+++ b/drivers/scsi/lpfc/lpfc_mbox.c
@@ -1827,9 +1827,9 @@ lpfc_sli4_config(struct lpfc_hba *phba, struct lpfcMboxq *mbox,
 		 * page, this is used as a priori size of SLI4_PAGE_SIZE for
 		 * the later DMA memory free.
 		 */
-		viraddr = dma_zalloc_coherent(&phba->pcidev->dev,
-					      SLI4_PAGE_SIZE, &phyaddr,
-					      GFP_KERNEL);
+		viraddr = dma_alloc_coherent(&phba->pcidev->dev,
+					     SLI4_PAGE_SIZE, &phyaddr,
+					     GFP_KERNEL);
 		/* In case of malloc fails, proceed with whatever we have */
 		if (!viraddr)
 			break;
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/megaraid/megaraid_mbox.c
diff --git a/drivers/scsi/megaraid/megaraid_sas_base.c b/drivers/scsi/megaraid/megaraid_sas_base.c
index 4a0c6dd70101..543b47c6118e 100644
--- a/drivers/scsi/megaraid/megaraid_sas_base.c
+++ b/drivers/scsi/megaraid/megaraid_sas_base.c
@@ -2279,9 +2279,9 @@ static int megasas_get_ld_vf_affiliation_111(struct megasas_instance *instance,
 			       sizeof(struct MR_LD_VF_AFFILIATION_111));
 	else {
 		new_affiliation_111 =
-			dma_zalloc_coherent(&instance->pdev->dev,
-					      sizeof(struct MR_LD_VF_AFFILIATION_111),
-					      &new_affiliation_111_h, GFP_KERNEL);
+			dma_alloc_coherent(&instance->pdev->dev,
+					   sizeof(struct MR_LD_VF_AFFILIATION_111),
+					   &new_affiliation_111_h, GFP_KERNEL);
 		if (!new_affiliation_111) {
 			dev_printk(KERN_DEBUG, &instance->pdev->dev, "SR-IOV: Couldn't allocate "
 			       "memory for new affiliation for scsi%d\n",
@@ -2386,10 +2386,9 @@ static int megasas_get_ld_vf_affiliation_12(struct megasas_instance *instance,
 		       sizeof(struct MR_LD_VF_AFFILIATION));
 	else {
 		new_affiliation =
-			dma_zalloc_coherent(&instance->pdev->dev,
-					      (MAX_LOGICAL_DRIVES + 1) *
-					      sizeof(struct MR_LD_VF_AFFILIATION),
-					      &new_affiliation_h, GFP_KERNEL);
+			dma_alloc_coherent(&instance->pdev->dev,
+					   (MAX_LOGICAL_DRIVES + 1) * sizeof(struct MR_LD_VF_AFFILIATION),
+					   &new_affiliation_h, GFP_KERNEL);
 		if (!new_affiliation) {
 			dev_printk(KERN_DEBUG, &instance->pdev->dev, "SR-IOV: Couldn't allocate "
 			       "memory for new affiliation for scsi%d\n",
@@ -2552,9 +2551,10 @@ int megasas_sriov_start_heartbeat(struct megasas_instance *instance,
 
 	if (initial) {
 		instance->hb_host_mem =
-			dma_zalloc_coherent(&instance->pdev->dev,
-					      sizeof(struct MR_CTRL_HB_HOST_MEM),
-					      &instance->hb_host_mem_h, GFP_KERNEL);
+			dma_alloc_coherent(&instance->pdev->dev,
+					   sizeof(struct MR_CTRL_HB_HOST_MEM),
+					   &instance->hb_host_mem_h,
+					   GFP_KERNEL);
 		if (!instance->hb_host_mem) {
 			dev_printk(KERN_DEBUG, &instance->pdev->dev, "SR-IOV: Couldn't allocate"
 			       " memory for heartbeat host memory for scsi%d\n",
@@ -5838,9 +5838,9 @@ megasas_get_seq_num(struct megasas_instance *instance,
 	}
 
 	dcmd = &cmd->frame->dcmd;
-	el_info = dma_zalloc_coherent(&instance->pdev->dev,
-			sizeof(struct megasas_evt_log_info), &el_info_h,
-			GFP_KERNEL);
+	el_info = dma_alloc_coherent(&instance->pdev->dev,
+				     sizeof(struct megasas_evt_log_info),
+				     &el_info_h, GFP_KERNEL);
 	if (!el_info) {
 		megasas_return_cmd(instance, cmd);
 		return -ENOMEM;
diff --git a/drivers/scsi/megaraid/megaraid_sas_fusion.c b/drivers/scsi/megaraid/megaraid_sas_fusion.c
index 80e5cffa9aff..1b8f53c25583 100644
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.c
@@ -692,8 +692,9 @@ megasas_alloc_rdpq_fusion(struct megasas_instance *instance)
 	array_size = sizeof(struct MPI2_IOC_INIT_RDPQ_ARRAY_ENTRY) *
 		     MAX_MSIX_QUEUES_FUSION;
 
-	fusion->rdpq_virt = dma_zalloc_coherent(&instance->pdev->dev,
-			array_size, &fusion->rdpq_phys, GFP_KERNEL);
+	fusion->rdpq_virt = dma_alloc_coherent(&instance->pdev->dev,
+					       array_size, &fusion->rdpq_phys,
+					       GFP_KERNEL);
 	if (!fusion->rdpq_virt) {
 		dev_err(&instance->pdev->dev,
 			"Failed from %s %d\n",  __func__, __LINE__);
* Unmerged path drivers/scsi/mesh.c
* Unmerged path drivers/scsi/mvumi.c
* Unmerged path drivers/scsi/pm8001/pm8001_sas.c
diff --git a/drivers/scsi/qedf/qedf_main.c b/drivers/scsi/qedf/qedf_main.c
index 8c2a89f0dd55..cb71806117da 100644
--- a/drivers/scsi/qedf/qedf_main.c
+++ b/drivers/scsi/qedf/qedf_main.c
@@ -1055,16 +1055,17 @@ static int qedf_alloc_sq(struct qedf_ctx *qedf, struct qedf_rport *fcport)
 	    sizeof(void *);
 	fcport->sq_pbl_size = fcport->sq_pbl_size + QEDF_PAGE_SIZE;
 
-	fcport->sq = dma_zalloc_coherent(&qedf->pdev->dev,
-	    fcport->sq_mem_size, &fcport->sq_dma, GFP_KERNEL);
+	fcport->sq = dma_alloc_coherent(&qedf->pdev->dev, fcport->sq_mem_size,
+					&fcport->sq_dma, GFP_KERNEL);
 	if (!fcport->sq) {
 		QEDF_WARN(&(qedf->dbg_ctx), "Could not allocate send queue.\n");
 		rval = 1;
 		goto out;
 	}
 
-	fcport->sq_pbl = dma_zalloc_coherent(&qedf->pdev->dev,
-	    fcport->sq_pbl_size, &fcport->sq_pbl_dma, GFP_KERNEL);
+	fcport->sq_pbl = dma_alloc_coherent(&qedf->pdev->dev,
+					    fcport->sq_pbl_size,
+					    &fcport->sq_pbl_dma, GFP_KERNEL);
 	if (!fcport->sq_pbl) {
 		QEDF_WARN(&(qedf->dbg_ctx), "Could not allocate send queue PBL.\n");
 		rval = 1;
@@ -2685,8 +2686,10 @@ static int qedf_alloc_bdq(struct qedf_ctx *qedf)
 	}
 
 	/* Allocate list of PBL pages */
-	qedf->bdq_pbl_list = dma_zalloc_coherent(&qedf->pdev->dev,
-	    QEDF_PAGE_SIZE, &qedf->bdq_pbl_list_dma, GFP_KERNEL);
+	qedf->bdq_pbl_list = dma_alloc_coherent(&qedf->pdev->dev,
+						QEDF_PAGE_SIZE,
+						&qedf->bdq_pbl_list_dma,
+						GFP_KERNEL);
 	if (!qedf->bdq_pbl_list) {
 		QEDF_ERR(&(qedf->dbg_ctx), "Could not allocate list of PBL pages.\n");
 		return -ENOMEM;
@@ -2775,9 +2778,10 @@ static int qedf_alloc_global_queues(struct qedf_ctx *qedf)
 		    ALIGN(qedf->global_queues[i]->cq_pbl_size, QEDF_PAGE_SIZE);
 
 		qedf->global_queues[i]->cq =
-		    dma_zalloc_coherent(&qedf->pdev->dev,
-			qedf->global_queues[i]->cq_mem_size,
-			&qedf->global_queues[i]->cq_dma, GFP_KERNEL);
+		    dma_alloc_coherent(&qedf->pdev->dev,
+				       qedf->global_queues[i]->cq_mem_size,
+				       &qedf->global_queues[i]->cq_dma,
+				       GFP_KERNEL);
 
 		if (!qedf->global_queues[i]->cq) {
 			QEDF_WARN(&(qedf->dbg_ctx), "Could not allocate cq.\n");
@@ -2786,9 +2790,10 @@ static int qedf_alloc_global_queues(struct qedf_ctx *qedf)
 		}
 
 		qedf->global_queues[i]->cq_pbl =
-		    dma_zalloc_coherent(&qedf->pdev->dev,
-			qedf->global_queues[i]->cq_pbl_size,
-			&qedf->global_queues[i]->cq_pbl_dma, GFP_KERNEL);
+		    dma_alloc_coherent(&qedf->pdev->dev,
+				       qedf->global_queues[i]->cq_pbl_size,
+				       &qedf->global_queues[i]->cq_pbl_dma,
+				       GFP_KERNEL);
 
 		if (!qedf->global_queues[i]->cq_pbl) {
 			QEDF_WARN(&(qedf->dbg_ctx), "Could not allocate cq PBL.\n");
* Unmerged path drivers/scsi/qedi/qedi_main.c
diff --git a/drivers/scsi/qla2xxx/qla_attr.c b/drivers/scsi/qla2xxx/qla_attr.c
index 6fff0cceac31..31e1a987f0c2 100644
--- a/drivers/scsi/qla2xxx/qla_attr.c
+++ b/drivers/scsi/qla2xxx/qla_attr.c
@@ -1847,8 +1847,8 @@ qla2x00_get_fc_host_stats(struct Scsi_Host *shost)
 	if (qla2x00_reset_active(vha))
 		goto done;
 
-	stats = dma_zalloc_coherent(&ha->pdev->dev, sizeof(*stats),
-				    &stats_dma, GFP_KERNEL);
+	stats = dma_alloc_coherent(&ha->pdev->dev, sizeof(*stats), &stats_dma,
+				   GFP_KERNEL);
 	if (!stats) {
 		ql_log(ql_log_warn, vha, 0x707d,
 		    "Failed to allocate memory for stats.\n");
diff --git a/drivers/scsi/qla2xxx/qla_bsg.c b/drivers/scsi/qla2xxx/qla_bsg.c
index 45566a2a3f01..187e595e18de 100644
--- a/drivers/scsi/qla2xxx/qla_bsg.c
+++ b/drivers/scsi/qla2xxx/qla_bsg.c
@@ -2257,8 +2257,8 @@ qla2x00_get_priv_stats(struct fc_bsg_job *bsg_job)
 	if (!IS_FWI2_CAPABLE(ha))
 		return -EPERM;
 
-	stats = dma_zalloc_coherent(&ha->pdev->dev, sizeof(*stats),
-				    &stats_dma, GFP_KERNEL);
+	stats = dma_alloc_coherent(&ha->pdev->dev, sizeof(*stats), &stats_dma,
+				   GFP_KERNEL);
 	if (!stats) {
 		ql_log(ql_log_warn, vha, 0x70e2,
 		    "Failed to allocate memory for stats.\n");
diff --git a/drivers/scsi/qla2xxx/qla_gs.c b/drivers/scsi/qla2xxx/qla_gs.c
index c2b98e72c28c..1ce9e57254c7 100644
--- a/drivers/scsi/qla2xxx/qla_gs.c
+++ b/drivers/scsi/qla2xxx/qla_gs.c
@@ -4339,9 +4339,10 @@ int qla24xx_async_gpnft(scsi_qla_host_t *vha, u8 fc4_type, srb_t *sp)
 			return rval;
 		}
 
-		sp->u.iocb_cmd.u.ctarg.req = dma_zalloc_coherent(
-			&vha->hw->pdev->dev, sizeof(struct ct_sns_pkt),
-			&sp->u.iocb_cmd.u.ctarg.req_dma, GFP_KERNEL);
+		sp->u.iocb_cmd.u.ctarg.req = dma_alloc_coherent(&vha->hw->pdev->dev,
+								sizeof(struct ct_sns_pkt),
+								&sp->u.iocb_cmd.u.ctarg.req_dma,
+								GFP_KERNEL);
 		sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
 		if (!sp->u.iocb_cmd.u.ctarg.req) {
 			ql_log(ql_log_warn, vha, 0xffff,
@@ -4357,9 +4358,10 @@ int qla24xx_async_gpnft(scsi_qla_host_t *vha, u8 fc4_type, srb_t *sp)
 			((vha->hw->max_fibre_devices - 1) *
 			    sizeof(struct ct_sns_gpn_ft_data));
 
-		sp->u.iocb_cmd.u.ctarg.rsp = dma_zalloc_coherent(
-			&vha->hw->pdev->dev, rspsz,
-			&sp->u.iocb_cmd.u.ctarg.rsp_dma, GFP_KERNEL);
+		sp->u.iocb_cmd.u.ctarg.rsp = dma_alloc_coherent(&vha->hw->pdev->dev,
+								rspsz,
+								&sp->u.iocb_cmd.u.ctarg.rsp_dma,
+								GFP_KERNEL);
 		sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
 		if (!sp->u.iocb_cmd.u.ctarg.rsp) {
 			ql_log(ql_log_warn, vha, 0xffff,
* Unmerged path drivers/scsi/qla2xxx/qla_init.c
diff --git a/drivers/scsi/smartpqi/smartpqi_init.c b/drivers/scsi/smartpqi/smartpqi_init.c
index 90b0cb79e371..18f7159e0228 100644
--- a/drivers/scsi/smartpqi/smartpqi_init.c
+++ b/drivers/scsi/smartpqi/smartpqi_init.c
@@ -3609,9 +3609,9 @@ static int pqi_alloc_operational_queues(struct pqi_ctrl_info *ctrl_info)
 	alloc_length += PQI_EXTRA_SGL_MEMORY;
 
 	ctrl_info->queue_memory_base =
-		dma_zalloc_coherent(&ctrl_info->pci_dev->dev,
-			alloc_length,
-			&ctrl_info->queue_memory_base_dma_handle, GFP_KERNEL);
+		dma_alloc_coherent(&ctrl_info->pci_dev->dev, alloc_length,
+				   &ctrl_info->queue_memory_base_dma_handle,
+				   GFP_KERNEL);
 
 	if (!ctrl_info->queue_memory_base)
 		return -ENOMEM;
@@ -3748,10 +3748,9 @@ static int pqi_alloc_admin_queues(struct pqi_ctrl_info *ctrl_info)
 		PQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT;
 
 	ctrl_info->admin_queue_memory_base =
-		dma_zalloc_coherent(&ctrl_info->pci_dev->dev,
-			alloc_length,
-			&ctrl_info->admin_queue_memory_base_dma_handle,
-			GFP_KERNEL);
+		dma_alloc_coherent(&ctrl_info->pci_dev->dev, alloc_length,
+				   &ctrl_info->admin_queue_memory_base_dma_handle,
+				   GFP_KERNEL);
 
 	if (!ctrl_info->admin_queue_memory_base)
 		return -ENOMEM;
@@ -4635,9 +4634,10 @@ static void pqi_free_all_io_requests(struct pqi_ctrl_info *ctrl_info)
 
 static inline int pqi_alloc_error_buffer(struct pqi_ctrl_info *ctrl_info)
 {
-	ctrl_info->error_buffer = dma_zalloc_coherent(&ctrl_info->pci_dev->dev,
-		ctrl_info->error_buffer_length,
-		&ctrl_info->error_buffer_dma_handle, GFP_KERNEL);
+	ctrl_info->error_buffer = dma_alloc_coherent(&ctrl_info->pci_dev->dev,
+						     ctrl_info->error_buffer_length,
+						     &ctrl_info->error_buffer_dma_handle,
+						     GFP_KERNEL);
 
 	if (!ctrl_info->error_buffer)
 		return -ENOMEM;
@@ -7515,8 +7515,8 @@ static int pqi_ofa_alloc_mem(struct pqi_ctrl_info *ctrl_info,
 		dma_addr_t dma_handle;
 
 		ctrl_info->pqi_ofa_chunk_virt_addr[i] =
-			dma_zalloc_coherent(dev, chunk_size, &dma_handle,
-						GFP_KERNEL);
+			dma_alloc_coherent(dev, chunk_size, &dma_handle,
+					   GFP_KERNEL);
 
 		if (!ctrl_info->pqi_ofa_chunk_virt_addr[i])
 			break;
@@ -7573,10 +7573,10 @@ static void pqi_ofa_setup_host_buffer(struct pqi_ctrl_info *ctrl_info,
 	struct device *dev;
 
 	dev = &ctrl_info->pci_dev->dev;
-	pqi_ofa_memory = dma_zalloc_coherent(dev,
-				PQI_OFA_MEMORY_DESCRIPTOR_LENGTH,
-				&ctrl_info->pqi_ofa_mem_dma_handle,
-				GFP_KERNEL);
+	pqi_ofa_memory = dma_alloc_coherent(dev,
+					    PQI_OFA_MEMORY_DESCRIPTOR_LENGTH,
+					    &ctrl_info->pqi_ofa_mem_dma_handle,
+					    GFP_KERNEL);
 
 	if (!pqi_ofa_memory)
 		return;
* Unmerged path drivers/soc/fsl/qbman/dpaa_sys.c
* Unmerged path drivers/spi/spi-pic32-sqi.c
* Unmerged path drivers/staging/mt7621-eth/mtk_eth_soc.c
* Unmerged path drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c
* Unmerged path drivers/staging/vt6655/device_main.c
* Unmerged path drivers/usb/gadget/udc/bdc/bdc_core.c
diff --git a/drivers/usb/host/uhci-hcd.c b/drivers/usb/host/uhci-hcd.c
index f9c3947577fc..a1c2c8dae856 100644
--- a/drivers/usb/host/uhci-hcd.c
+++ b/drivers/usb/host/uhci-hcd.c
@@ -600,9 +600,9 @@ static int uhci_start(struct usb_hcd *hcd)
 	uhci->dentry = dentry;
 #endif
 
-	uhci->frame = dma_zalloc_coherent(uhci_dev(uhci),
-			UHCI_NUMFRAMES * sizeof(*uhci->frame),
-			&uhci->frame_dma_handle, GFP_KERNEL);
+	uhci->frame = dma_alloc_coherent(uhci_dev(uhci),
+					 UHCI_NUMFRAMES * sizeof(*uhci->frame),
+					 &uhci->frame_dma_handle, GFP_KERNEL);
 	if (!uhci->frame) {
 		dev_err(uhci_dev(uhci),
 			"unable to allocate consistent memory for frame list\n");
diff --git a/drivers/usb/host/xhci-mem.c b/drivers/usb/host/xhci-mem.c
index 332420d10be9..ebde6936bb2c 100644
--- a/drivers/usb/host/xhci-mem.c
+++ b/drivers/usb/host/xhci-mem.c
@@ -1657,8 +1657,8 @@ static int scratchpad_alloc(struct xhci_hcd *xhci, gfp_t flags)
 	xhci->dcbaa->dev_context_ptrs[0] = cpu_to_le64(xhci->scratchpad->sp_dma);
 	for (i = 0; i < num_sp; i++) {
 		dma_addr_t dma;
-		void *buf = dma_zalloc_coherent(dev, xhci->page_size, &dma,
-				flags);
+		void *buf = dma_alloc_coherent(dev, xhci->page_size, &dma,
+					       flags);
 		if (!buf)
 			goto fail_sp4;
 
@@ -1782,8 +1782,8 @@ int xhci_alloc_erst(struct xhci_hcd *xhci,
 	struct xhci_erst_entry *entry;
 
 	size = sizeof(struct xhci_erst_entry) * evt_ring->num_segs;
-	erst->entries = dma_zalloc_coherent(xhci_to_hcd(xhci)->self.sysdev,
-					    size, &erst->erst_dma_addr, flags);
+	erst->entries = dma_alloc_coherent(xhci_to_hcd(xhci)->self.sysdev,
+					   size, &erst->erst_dma_addr, flags);
 	if (!erst->entries)
 		return -ENOMEM;
 
* Unmerged path drivers/video/da8xx-fb.c
* Unmerged path include/linux/pci-dma-compat.h
* Unmerged path sound/aoa/soundbus/i2sbus/core.c
* Unmerged path sound/sparc/dbri.c
