bpf: add a bpf_override_function helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Josef Bacik <jbacik@fb.com>
commit dd0bb688eaa241b5655d396d45366cba9225aed9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/dd0bb688.failed

Error injection is sloppy and very ad-hoc.  BPF could fill this niche
perfectly with it's kprobe functionality.  We could make sure errors are
only triggered in specific call chains that we care about with very
specific situations.  Accomplish this with the bpf_override_funciton
helper.  This will modify the probe'd callers return value to the
specified value and set the PC to an override function that simply
returns, bypassing the originally probed function.  This gives us a nice
clean way to implement systematic error injection for all of our code
paths.

	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Josef Bacik <jbacik@fb.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit dd0bb688eaa241b5655d396d45366cba9225aed9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	arch/x86/Kconfig
#	include/linux/filter.h
#	include/uapi/linux/bpf.h
#	kernel/events/core.c
#	kernel/trace/trace_kprobe.c
#	kernel/trace/trace_probe.h
diff --cc arch/Kconfig
index 25cf18bf72ac,6e8520f09bc1..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -194,8 -196,12 +194,15 @@@ config HAVE_OPTPROBE
  config HAVE_KPROBES_ON_FTRACE
  	bool
  
++<<<<<<< HEAD
 +config HAVE_NMI_WATCHDOG
++=======
+ config HAVE_KPROBE_OVERRIDE
+ 	bool
+ 
+ config HAVE_NMI
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  	bool
 -
  #
  # An arch should select this if it provides all these things:
  #
diff --cc arch/x86/Kconfig
index eb5527a34c97,51458c1a0b4a..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -15,137 -22,173 +15,160 @@@ config X86_3
  config X86_64
  	def_bool y
  	depends on 64BIT
 -	# Options that are inherently 64-bit kernel only:
 -	select ARCH_HAS_GIGANTIC_PAGE if (MEMORY_ISOLATION && COMPACTION) || CMA
 -	select ARCH_SUPPORTS_INT128
 -	select ARCH_USE_CMPXCHG_LOCKREF
 -	select HAVE_ARCH_SOFT_DIRTY
 -	select MODULES_USE_ELF_RELA
  	select X86_DEV_DMA_OPS
 +	select ARCH_USE_CMPXCHG_LOCKREF
 +	select HAVE_LIVEPATCH
  
 -#
 -# Arch settings
 -#
 -# ( Note that options that are marked 'if X86_64' could in principle be
 -#   ported to 32-bit as well. )
 -#
 +### Arch settings
  config X86
  	def_bool y
 -	#
 -	# Note: keep this list sorted alphabetically
 -	#
 -	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
 -	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
 -	select ANON_INODES
 -	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_DISCARD_MEMBLOCK
 -	select ARCH_HAS_ACPI_TABLE_UPGRADE	if ACPI
 -	select ARCH_HAS_DEBUG_VIRTUAL
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_HAS_ELF_RANDOMIZE
 -	select ARCH_HAS_FAST_MULTIPLIER
 -	select ARCH_HAS_FORTIFY_SOURCE
 -	select ARCH_HAS_GCOV_PROFILE_ALL
 -	select ARCH_HAS_KCOV			if X86_64
 +	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
 +	select ARCH_HAS_MEMBARRIER_SYNC_CORE
 +	select ARCH_HAS_SYNC_CORE_BEFORE_USERMODE
  	select ARCH_HAS_PMEM_API		if X86_64
 -	# Causing hangs/crashes, see the commit that added this change for details.
 -	select ARCH_HAS_REFCOUNT		if BROKEN
  	select ARCH_HAS_UACCESS_FLUSHCACHE	if X86_64
 -	select ARCH_HAS_SET_MEMORY
 -	select ARCH_HAS_SG_CHAIN
 -	select ARCH_HAS_STRICT_KERNEL_RWX
 -	select ARCH_HAS_STRICT_MODULE_RWX
 -	select ARCH_HAS_UBSAN_SANITIZE_ALL
 -	select ARCH_HAS_ZONE_DEVICE		if X86_64
 -	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 -	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
 -	select ARCH_MIGHT_HAVE_PC_PARPORT
 -	select ARCH_MIGHT_HAVE_PC_SERIO
 -	select ARCH_SUPPORTS_ATOMIC_RMW
 -	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 -	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
 -	select ARCH_USE_BUILTIN_BSWAP
 -	select ARCH_USE_QUEUED_RWLOCKS
 -	select ARCH_USE_QUEUED_SPINLOCKS
 -	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 -	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
 -	select ARCH_WANTS_THP_SWAP		if X86_64
 -	select BUILDTIME_EXTABLE_SORT
 -	select CLKEVT_I8253
 -	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 -	select CLOCKSOURCE_WATCHDOG
 -	select DCACHE_WORD_ACCESS
 -	select EDAC_ATOMIC_SCRUB
 -	select EDAC_SUPPORT
 -	select GENERIC_CLOCKEVENTS
 -	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
 -	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 -	select GENERIC_CMOS_UPDATE
 -	select GENERIC_CPU_AUTOPROBE
 -	select GENERIC_EARLY_IOREMAP
 -	select GENERIC_FIND_FIRST_BIT
 -	select GENERIC_IOMAP
 -	select GENERIC_IRQ_EFFECTIVE_AFF_MASK	if SMP
 -	select GENERIC_IRQ_MIGRATION		if SMP
 -	select GENERIC_IRQ_PROBE
 -	select GENERIC_IRQ_SHOW
 -	select GENERIC_PENDING_IRQ		if SMP
 -	select GENERIC_SMP_IDLE_THREAD
 -	select GENERIC_STRNCPY_FROM_USER
 -	select GENERIC_STRNLEN_USER
 -	select GENERIC_TIME_VSYSCALL
 -	select HARDLOCKUP_CHECK_TIMESTAMP	if X86_64
 -	select HAVE_ACPI_APEI			if ACPI
 -	select HAVE_ACPI_APEI_NMI		if ACPI
 -	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
 -	select HAVE_ARCH_AUDITSYSCALL
 -	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
 -	select HAVE_ARCH_JUMP_LABEL
 -	select HAVE_ARCH_KASAN			if X86_64 && SPARSEMEM_VMEMMAP
 -	select HAVE_ARCH_KGDB
 -	select HAVE_ARCH_KMEMCHECK
 -	select HAVE_ARCH_MMAP_RND_BITS		if MMU
 -	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 -	select HAVE_ARCH_COMPAT_MMAP_BASES	if MMU && COMPAT
 -	select HAVE_ARCH_SECCOMP_FILTER
 -	select HAVE_ARCH_TRACEHOOK
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 -	select HAVE_ARCH_VMAP_STACK		if X86_64
 -	select HAVE_ARCH_WITHIN_STACK_FRAMES
 -	select HAVE_CC_STACKPROTECTOR
 -	select HAVE_CMPXCHG_DOUBLE
 -	select HAVE_CMPXCHG_LOCAL
 -	select HAVE_CONTEXT_TRACKING		if X86_64
 -	select HAVE_COPY_THREAD_TLS
 -	select HAVE_C_RECORDMCOUNT
 -	select HAVE_DEBUG_KMEMLEAK
 -	select HAVE_DEBUG_STACKOVERFLOW
 -	select HAVE_DMA_API_DEBUG
 -	select HAVE_DMA_CONTIGUOUS
 -	select HAVE_DYNAMIC_FTRACE
 -	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 -	select HAVE_EBPF_JIT			if X86_64
 -	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 -	select HAVE_EXIT_THREAD
 -	select HAVE_FENTRY			if X86_64 || DYNAMIC_FTRACE
 -	select HAVE_FTRACE_MCOUNT_RECORD
 -	select HAVE_FUNCTION_GRAPH_TRACER
 -	select HAVE_FUNCTION_TRACER
 -	select HAVE_GCC_PLUGINS
 -	select HAVE_HW_BREAKPOINT
 +	select ARCH_HAS_UACCESS_MCSAFE		if X86_64 && X86_MCE
 +	select ARCH_HAS_MMIO_FLUSH
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
  	select HAVE_IDE
++<<<<<<< HEAD
++=======
+ 	select HAVE_IOREMAP_PROT
+ 	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
+ 	select HAVE_IRQ_TIME_ACCOUNTING
+ 	select HAVE_KERNEL_BZIP2
+ 	select HAVE_KERNEL_GZIP
+ 	select HAVE_KERNEL_LZ4
+ 	select HAVE_KERNEL_LZMA
+ 	select HAVE_KERNEL_LZO
+ 	select HAVE_KERNEL_XZ
+ 	select HAVE_KPROBES
+ 	select HAVE_KPROBES_ON_FTRACE
+ 	select HAVE_KPROBE_OVERRIDE
+ 	select HAVE_KRETPROBES
+ 	select HAVE_KVM
+ 	select HAVE_LIVEPATCH			if X86_64
+ 	select HAVE_MEMBLOCK
+ 	select HAVE_MEMBLOCK_NODE_MAP
+ 	select HAVE_MIXED_BREAKPOINTS_REGS
+ 	select HAVE_MOD_ARCH_SPECIFIC
+ 	select HAVE_NMI
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  	select HAVE_OPROFILE
 -	select HAVE_OPTPROBES
  	select HAVE_PCSPKR_PLATFORM
  	select HAVE_PERF_EVENTS
 +	select HAVE_IOREMAP_PROT
 +	select HAVE_KPROBES
 +	select HAVE_MEMBLOCK
 +	select HAVE_MEMBLOCK_NODE_MAP
 +	select ARCH_DISCARD_MEMBLOCK
 +	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
 +	select HAVE_OPTPROBES
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
 +	select HAVE_ARCH_MMAP_RND_BITS		if MMU
 +	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 +	select HAVE_C_RECORDMCOUNT
 +	select HAVE_DYNAMIC_FTRACE
 +	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 +	select HAVE_FUNCTION_TRACER
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
 +	select HAVE_KERNEL_GZIP
 +	select HAVE_KERNEL_BZIP2
 +	select HAVE_KERNEL_LZMA
 +	select HAVE_KERNEL_XZ
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
 +	select HAVE_MIXED_BREAKPOINTS_REGS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
 -	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
 +	select HAVE_DEBUG_KMEMLEAK
 +	select ANON_INODES
 +	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
 +	select HAVE_CMPXCHG_LOCAL
 +	select HAVE_CMPXCHG_DOUBLE
 +	select HAVE_ARCH_KMEMCHECK
  	select HAVE_RCU_TABLE_FREE
 -	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_RELIABLE_STACKTRACE		if X86_64 && FRAME_POINTER_UNWINDER && STACK_VALIDATION
 -	select HAVE_STACK_VALIDATION		if X86_64
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select HAVE_UNSTABLE_SCHED_CLOCK
  	select HAVE_USER_RETURN_NOTIFIER
 -	select IRQ_FORCED_THREADING
 -	select PCI_LOCKLESS_CONFIG
 -	select PERF_EVENTS
 -	select RTC_LIB
 -	select RTC_MC146818_LIB
 +	select ARCH_HAS_ELF_RANDOMIZE
 +	select HAVE_ARCH_JUMP_LABEL
 +	select HAVE_TEXT_POKE_SMP
 +	select HAVE_GENERIC_HARDIRQS
 +	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select SPARSE_IRQ
 -	select SRCU
 -	select SYSCTL_EXCEPTION_TRACE
 -	select THREAD_INFO_IN_TASK
 -	select USER_STACKTRACE_SUPPORT
 +	select GENERIC_FIND_FIRST_BIT
 +	select GENERIC_IRQ_PROBE
 +	select GENERIC_PENDING_IRQ if SMP
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 +	select IRQ_FORCED_THREADING
 +	select USE_GENERIC_SMP_HELPERS if SMP
 +	select HAVE_EBPF_JIT if X86_64
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 +	select HAVE_ARCH_HUGE_VMAP if X86_64 || (X86_32 && X86_PAE)
 +	select HAVE_ARCH_WITHIN_STACK_FRAMES
 +	select CLKEVT_I8253
 +	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 +	select GENERIC_IOMAP
 +	select DCACHE_WORD_ACCESS
 +	select GENERIC_SMP_IDLE_THREAD
 +	select ARCH_WANT_IPC_PARSE_VERSION if X86_32
 +	select HAVE_ARCH_SECCOMP_FILTER
 +	select BUILDTIME_EXTABLE_SORT
 +	select GENERIC_CMOS_UPDATE
 +	select HAVE_ARCH_SOFT_DIRTY
 +	select GENERIC_CLOCKEVENTS
 +	select ARCH_CLOCKSOURCE_DATA if X86_64
 +	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 +	select GENERIC_CLOCKEVENTS_BROADCAST if X86_64 || (X86_32 && X86_LOCAL_APIC)
 +	select GENERIC_TIME_VSYSCALL if X86_64
 +	select HARDLOCKUP_CHECK_TIMESTAMP if X86_64
 +	select KTIME_SCALAR if X86_32
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
 +	select HAVE_CONTEXT_TRACKING if X86_64
 +	select HAVE_IRQ_TIME_ACCOUNTING
  	select VIRT_TO_BUS
 -	select X86_FEATURE_NAMES		if PROC_FS
 +	select MODULES_USE_ELF_REL if X86_32
 +	select MODULES_USE_ELF_RELA if X86_64
 +	select CLONE_BACKWARDS if X86_32
 +	select ARCH_USE_BUILTIN_BSWAP
 +	select ARCH_USE_QUEUED_SPINLOCKS
 +	select ARCH_USE_QUEUED_RWLOCKS
 +	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 +	select OLD_SIGACTION if X86_32
 +	select COMPAT_OLD_SIGACTION if IA32_EMULATION
 +	select RTC_LIB
 +	select HAVE_CC_STACKPROTECTOR
 +	select HAVE_STACK_VALIDATION if X86_64
 +	select HAVE_RELIABLE_STACKTRACE		if X86_64 && FRAME_POINTER && STACK_VALIDATION
 +	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select GENERIC_CPU_VULNERABILITIES
 +	select HOTPLUG_SMT			if SMP
  
  config INSTRUCTION_DECODER
  	def_bool y
diff --cc include/linux/filter.h
index cddbb31734e8,eaec066f99e8..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -421,11 -460,9 +421,15 @@@ struct bpf_prog 
  				gpl_compatible:1, /* Is filter GPL compatible? */
  				cb_access:1,	/* Is control block accessed? */
  				dst_needed:1,	/* Do we need dst entry? */
++<<<<<<< HEAD
 +				blinded:1,	/* Was blinded */
 +				is_func:1;	/* program is a bpf function */
++=======
+ 				kprobe_override:1; /* Do we override a kprobe? */
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  	kmemcheck_bitfield_end(meta);
  	enum bpf_prog_type	type;		/* Type of BPF program */
 +	enum bpf_attach_type	expected_attach_type; /* For some prog types */
  	u32			len;		/* Number of filter blocks */
  	u32			jited_len;	/* Size of jited insns in bytes */
  	u8			tag[BPF_TAG_SIZE];
diff --cc include/uapi/linux/bpf.h
index 000f5ce2d064,adb66f78b674..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -603,8 -741,7 +607,12 @@@ struct xdp_md 
  	FN(perf_event_read_value),	\
  	FN(perf_prog_read_value),	\
  	FN(getsockopt),			\
++<<<<<<< HEAD
 +	FN(override_return),		\
 +	FN(sock_ops_cb_flags_set),
++=======
+ 	FN(override_return),
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc kernel/events/core.c
index 693c1f0ae90b,ac240d31b5bf..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -8258,7 -8171,14 +8258,18 @@@ static int perf_event_set_bpf_prog(stru
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	if (is_tracepoint) {
++=======
+ 	/* Kprobe override only works for kprobes, not uprobes. */
+ 	if (prog->kprobe_override &&
+ 	    !(event->tp_event->flags & TRACE_EVENT_FL_KPROBE)) {
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_tracepoint || is_syscall_tp) {
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  		int off = trace_event_get_offsets(event->tp_event);
  
  		if (prog->aux->max_ctx_offset > off) {
diff --cc kernel/trace/trace_kprobe.c
index f043398d9608,8e3c9ec1faf7..000000000000
--- a/kernel/trace/trace_kprobe.c
+++ b/kernel/trace/trace_kprobe.c
@@@ -44,8 -42,9 +44,9 @@@ struct event_file_link 
  	(offsetof(struct trace_kprobe, tp.args) +	\
  	(sizeof(struct probe_arg) * (n)))
  
+ DEFINE_PER_CPU(int, bpf_kprobe_override);
  
 -static nokprobe_inline bool trace_kprobe_is_return(struct trace_kprobe *tk)
 +static __kprobes bool trace_kprobe_is_return(struct trace_kprobe *tk)
  {
  	return tk->rp.handler != NULL;
  }
@@@ -78,6 -77,23 +79,26 @@@ static __kprobes bool trace_kprobe_is_o
  	return !!strchr(trace_kprobe_symbol(tk), ':');
  }
  
++<<<<<<< HEAD
++=======
+ static nokprobe_inline unsigned long trace_kprobe_nhit(struct trace_kprobe *tk)
+ {
+ 	unsigned long nhit = 0;
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu)
+ 		nhit += *per_cpu_ptr(tk->nhit, cpu);
+ 
+ 	return nhit;
+ }
+ 
+ int trace_kprobe_ftrace(struct trace_event_call *call)
+ {
+ 	struct trace_kprobe *tk = (struct trace_kprobe *)call->data;
+ 	return kprobe_ftrace(&tk->rp.kp);
+ }
+ 
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  static int register_kprobe_event(struct trace_kprobe *tk);
  static int unregister_kprobe_event(struct trace_kprobe *tk);
  
@@@ -1135,41 -1177,59 +1156,75 @@@ static int kretprobe_event_define_field
  #ifdef CONFIG_PERF_EVENTS
  
  /* Kprobe profile handler */
++<<<<<<< HEAD
 +static __kprobes void
++=======
+ static int
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  kprobe_perf_func(struct trace_kprobe *tk, struct pt_regs *regs)
  {
 -	struct trace_event_call *call = &tk->tp.call;
 +	struct ftrace_event_call *call = &tk->tp.call;
  	struct kprobe_trace_entry_head *entry;
  	struct hlist_head *head;
  	int size, __size, dsize;
  	int rctx;
  
- 	if (bpf_prog_array_valid(call) && !trace_call_bpf(call, regs))
- 		return;
+ 	if (bpf_prog_array_valid(call)) {
+ 		int ret;
+ 
+ 		ret = trace_call_bpf(call, regs);
+ 
+ 		/*
+ 		 * We need to check and see if we modified the pc of the
+ 		 * pt_regs, and if so clear the kprobe and return 1 so that we
+ 		 * don't do the instruction skipping.  Also reset our state so
+ 		 * we are clean the next pass through.
+ 		 */
+ 		if (__this_cpu_read(bpf_kprobe_override)) {
+ 			__this_cpu_write(bpf_kprobe_override, 0);
+ 			reset_current_kprobe();
+ 			return 1;
+ 		}
+ 		if (!ret)
+ 			return 0;
+ 	}
+ 
++<<<<<<< HEAD
++=======
+ 	head = this_cpu_ptr(call->perf_events);
+ 	if (hlist_empty(head))
+ 		return 0;
  
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  	dsize = __get_data_size(&tk->tp, regs);
  	__size = sizeof(*entry) + tk->tp.size + dsize;
  	size = ALIGN(__size + sizeof(u32), sizeof(u64));
  	size -= sizeof(u32);
 +	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,
 +		     "profile buffer not large enough"))
 +		return;
  
 -	entry = perf_trace_buf_alloc(size, NULL, &rctx);
 +	entry = perf_trace_buf_prepare(size, call->event.type, NULL, &rctx);
  	if (!entry)
- 		return;
+ 		return 0;
  
  	entry->ip = (unsigned long)tk->rp.kp.addr;
  	memset(&entry[1], 0, dsize);
  	store_trace_args(sizeof(*entry), &tk->tp, regs, (u8 *)&entry[1], dsize);
++<<<<<<< HEAD
 +
 +	head = this_cpu_ptr(call->perf_events);
 +	perf_trace_buf_submit(entry, size, rctx,
 +					entry->ip, 1, regs, head, NULL);
++=======
+ 	perf_trace_buf_submit(entry, size, rctx, call->event.type, 1, regs,
+ 			      head, NULL, NULL);
+ 	return 0;
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  }
 -NOKPROBE_SYMBOL(kprobe_perf_func);
  
  /* Kretprobe profile handler */
 -static void
 +static __kprobes void
  kretprobe_perf_func(struct trace_kprobe *tk, struct kretprobe_instance *ri,
  		    struct pt_regs *regs)
  {
@@@ -1238,24 -1297,25 +1293,25 @@@ int kprobe_register(struct ftrace_event
  	return 0;
  }
  
 -static int kprobe_dispatcher(struct kprobe *kp, struct pt_regs *regs)
 +static __kprobes
 +int kprobe_dispatcher(struct kprobe *kp, struct pt_regs *regs)
  {
  	struct trace_kprobe *tk = container_of(kp, struct trace_kprobe, rp.kp);
+ 	int ret = 0;
  
 -	raw_cpu_inc(*tk->nhit);
 +	tk->nhit++;
  
  	if (tk->tp.flags & TP_FLAG_TRACE)
  		kprobe_trace_func(tk, regs);
  #ifdef CONFIG_PERF_EVENTS
  	if (tk->tp.flags & TP_FLAG_PROFILE)
- 		kprobe_perf_func(tk, regs);
+ 		ret = kprobe_perf_func(tk, regs);
  #endif
- 	return 0;	/* We don't tweek kernel, so just return 0 */
+ 	return ret;
  }
 -NOKPROBE_SYMBOL(kprobe_dispatcher);
  
 -static int
 -kretprobe_dispatcher(struct kretprobe_instance *ri, struct pt_regs *regs)
 +static __kprobes
 +int kretprobe_dispatcher(struct kretprobe_instance *ri, struct pt_regs *regs)
  {
  	struct trace_kprobe *tk = container_of(ri->rp, struct trace_kprobe, rp);
  
diff --cc kernel/trace/trace_probe.h
index cb7776aed9a8,adbb3f7d1fb5..000000000000
--- a/kernel/trace/trace_probe.h
+++ b/kernel/trace/trace_probe.h
@@@ -242,7 -253,16 +242,8 @@@ struct symbol_cache
  unsigned long update_symbol_cache(struct symbol_cache *sc);
  void free_symbol_cache(struct symbol_cache *sc);
  struct symbol_cache *alloc_symbol_cache(const char *sym, long offset);
+ int trace_kprobe_ftrace(struct trace_event_call *call);
  #else
 -/* uprobes do not support symbol fetch methods */
 -#define fetch_symbol_u8			NULL
 -#define fetch_symbol_u16		NULL
 -#define fetch_symbol_u32		NULL
 -#define fetch_symbol_u64		NULL
 -#define fetch_symbol_string		NULL
 -#define fetch_symbol_string_size	NULL
 -
  struct symbol_cache {
  };
  static inline unsigned long __used update_symbol_cache(struct symbol_cache *sc)
@@@ -259,7 -279,12 +260,16 @@@ alloc_symbol_cache(const char *sym, lon
  {
  	return NULL;
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_KPROBE_EVENT */
++=======
+ 
+ static inline int trace_kprobe_ftrace(struct trace_event_call *call)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_KPROBE_EVENTS */
++>>>>>>> dd0bb688eaa2 (bpf: add a bpf_override_function helper)
  
  struct probe_arg {
  	struct fetch_param	fetch;
* Unmerged path arch/Kconfig
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/include/asm/kprobes.h b/arch/x86/include/asm/kprobes.h
index 9454c167629f..386f0e0b957c 100644
--- a/arch/x86/include/asm/kprobes.h
+++ b/arch/x86/include/asm/kprobes.h
@@ -64,6 +64,10 @@ extern const int kretprobe_blacklist_size;
 void arch_remove_kprobe(struct kprobe *p);
 asmlinkage void kretprobe_trampoline(void);
 
+#ifdef CONFIG_KPROBES_ON_FTRACE
+extern void arch_ftrace_kprobe_override_function(struct pt_regs *regs);
+#endif
+
 /* Architecture specific copy of original instruction*/
 struct arch_specific_insn {
 	/* copy of the original instruction */
diff --git a/arch/x86/include/asm/ptrace.h b/arch/x86/include/asm/ptrace.h
index 85476dc8f56c..c043a5c1342f 100644
--- a/arch/x86/include/asm/ptrace.h
+++ b/arch/x86/include/asm/ptrace.h
@@ -83,6 +83,11 @@ static inline unsigned long regs_return_value(struct pt_regs *regs)
 	return regs->ax;
 }
 
+static inline void regs_set_return_value(struct pt_regs *regs, unsigned long rc)
+{
+	regs->ax = rc;
+}
+
 /*
  * user_mode_vm(regs) determines whether a register set came from user mode.
  * This is true if V8086 mode was enabled OR if the register set was from
diff --git a/arch/x86/kernel/kprobes/ftrace.c b/arch/x86/kernel/kprobes/ftrace.c
index 24c736d2f33d..4dad55ac178b 100644
--- a/arch/x86/kernel/kprobes/ftrace.c
+++ b/arch/x86/kernel/kprobes/ftrace.c
@@ -94,3 +94,17 @@ int __kprobes arch_prepare_kprobe_ftrace(struct kprobe *p)
 	p->ainsn.boostable = -1;
 	return 0;
 }
+
+asmlinkage void override_func(void);
+asm(
+	".type override_func, @function\n"
+	"override_func:\n"
+	"	ret\n"
+	".size override_func, .-override_func\n"
+);
+
+void arch_ftrace_kprobe_override_function(struct pt_regs *regs)
+{
+	regs->ip = (unsigned long)&override_func;
+}
+NOKPROBE_SYMBOL(arch_ftrace_kprobe_override_function);
* Unmerged path include/linux/filter.h
diff --git a/include/linux/ftrace_event.h b/include/linux/ftrace_event.h
index 5faf497c76ce..35281cc3f5f2 100644
--- a/include/linux/ftrace_event.h
+++ b/include/linux/ftrace_event.h
@@ -453,6 +453,7 @@ do {									\
 struct perf_event;
 
 DECLARE_PER_CPU(struct pt_regs, perf_trace_regs);
+DECLARE_PER_CPU(int, bpf_kprobe_override);
 
 extern int  perf_trace_init(struct perf_event *event);
 extern void perf_trace_destroy(struct perf_event *event);
* Unmerged path include/uapi/linux/bpf.h
diff --git a/kernel/bpf/core.c b/kernel/bpf/core.c
index 5f61bbcde13a..1fba7af5ea5b 100644
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@ -1255,6 +1255,9 @@ static unsigned int __bpf_prog_ret0_warn(const void *ctx,
 bool bpf_prog_array_compatible(struct bpf_array *array,
 			       const struct bpf_prog *fp)
 {
+	if (fp->kprobe_override)
+		return false;
+
 	if (!array->owner_prog_type) {
 		/* There's no owner yet where we could check for
 		 * compatibility.
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index 5cb5e92d5f64..a3cf34cc72af 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -5492,6 +5492,8 @@ static int fixup_bpf_calls(struct bpf_verifier_env *env)
 			prog->dst_needed = 1;
 		if (insn->imm == BPF_FUNC_get_prandom_u32)
 			bpf_user_rnd_init_once();
+		if (insn->imm == BPF_FUNC_override_return)
+			prog->kprobe_override = 1;
 		if (insn->imm == BPF_FUNC_tail_call) {
 			/* If we tail call into other programs, we
 			 * cannot make any assumptions since they
* Unmerged path kernel/events/core.c
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index fbf6fe2cbd91..25bafa78365a 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -514,6 +514,17 @@ config FUNCTION_PROFILER
 
 	  If in doubt, say N.
 
+config BPF_KPROBE_OVERRIDE
+	bool "Enable BPF programs to override a kprobed function"
+	depends on BPF_EVENTS
+	depends on KPROBES_ON_FTRACE
+	depends on HAVE_KPROBE_OVERRIDE
+	depends on DYNAMIC_FTRACE_WITH_REGS
+	default n
+	help
+	 Allows BPF to override the execution of a probed function and
+	 set a different return value.  This is used for error injection.
+
 config FTRACE_MCOUNT_RECORD
 	def_bool y
 	depends on DYNAMIC_FTRACE
diff --git a/kernel/trace/bpf_trace.c b/kernel/trace/bpf_trace.c
index 98b7c00724d3..31ad73913d43 100644
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@ -13,6 +13,10 @@
 #include <linux/filter.h>
 #include <linux/uaccess.h>
 #include <linux/ctype.h>
+#include <linux/kprobes.h>
+#include <asm/kprobes.h>
+
+#include "trace_probe.h"
 #include "trace.h"
 
 u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
@@ -76,6 +80,29 @@ unsigned int trace_call_bpf(struct ftrace_event_call *call, void *ctx)
 }
 EXPORT_SYMBOL_GPL(trace_call_bpf);
 
+#ifdef CONFIG_BPF_KPROBE_OVERRIDE
+BPF_CALL_2(bpf_override_return, struct pt_regs *, regs, unsigned long, rc)
+{
+	__this_cpu_write(bpf_kprobe_override, 1);
+	regs_set_return_value(regs, rc);
+	arch_ftrace_kprobe_override_function(regs);
+	return 0;
+}
+#else
+BPF_CALL_2(bpf_override_return, struct pt_regs *, regs, unsigned long, rc)
+{
+	return -EINVAL;
+}
+#endif
+
+static const struct bpf_func_proto bpf_override_return_proto = {
+	.func		= bpf_override_return,
+	.gpl_only	= true,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_PTR_TO_CTX,
+	.arg2_type	= ARG_ANYTHING,
+};
+
 BPF_CALL_3(bpf_probe_read, void *, dst, u32, size, const void *, unsafe_ptr)
 {
 	int ret;
@@ -496,6 +523,10 @@ kprobe_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
 		return &bpf_get_stackid_proto;
 	case BPF_FUNC_perf_event_read_value:
 		return &bpf_perf_event_read_value_proto;
+	case BPF_FUNC_override_return:
+		pr_warn_ratelimited("%s[%d] is installing a program with bpf_override_return helper that may cause unexpected behavior!",
+				    current->comm, task_pid_nr(current));
+		return &bpf_override_return_proto;
 	default:
 		return tracing_func_proto(func_id, prog);
 	}
@@ -730,6 +761,10 @@ int perf_event_attach_bpf_prog(struct perf_event *event,
 	struct bpf_prog_array *new_array;
 	int ret = -EEXIST;
 
+	/* Kprobe override only works for ftrace based kprobes. */
+	if (prog->kprobe_override && !trace_kprobe_ftrace(event->tp_event))
+		return -EINVAL;
+
 	mutex_lock(&bpf_event_mutex);
 
 	if (event->prog)
* Unmerged path kernel/trace/trace_kprobe.c
* Unmerged path kernel/trace/trace_probe.h
