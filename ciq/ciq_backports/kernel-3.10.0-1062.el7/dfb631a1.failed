RDMA/mlx5: Do not generate the uabi specs unconditionally

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit dfb631a187b9b04c066df60e28adf05334112ca6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/dfb631a1.failed

For DM there is no reason not to add the spec for the START_OFFSET, if DM
is not supported then ib_dev.alloc_dm is already set to NULL which ensures
we do not call the method.

For IPSEC, the core code should be setting ib_dev.create_flow_action_esp
to NULL to disable it, not relying on wonky manipulation of the specs.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit dfb631a187b9b04c066df60e28adf05334112ca6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index e944c3c2b419,f12e045981fc..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -4821,7 -5532,116 +4821,120 @@@ static void mlx5_ib_cleanup_multiport_m
  	mlx5_nic_vport_disable_roce(dev->mdev);
  }
  
++<<<<<<< HEAD
 +static void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++=======
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_dm,
+ 	UVERBS_OBJECT_DM,
+ 	UVERBS_METHOD_DM_ALLOC,
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,
+ 			    UVERBS_ATTR_TYPE(u64),
+ 			    UA_MANDATORY),
+ 	UVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_PAGE_INDEX,
+ 			    UVERBS_ATTR_TYPE(u16),
+ 			    UA_MANDATORY));
+ 
+ ADD_UVERBS_ATTRIBUTES_SIMPLE(
+ 	mlx5_ib_flow_action,
+ 	UVERBS_OBJECT_FLOW_ACTION,
+ 	UVERBS_METHOD_FLOW_ACTION_ESP_CREATE,
+ 	UVERBS_ATTR_FLAGS_IN(MLX5_IB_ATTR_CREATE_FLOW_ACTION_FLAGS,
+ 			     enum mlx5_ib_uapi_flow_action_flags));
+ 
+ static int populate_specs_root(struct mlx5_ib_dev *dev)
+ {
+ 	const struct uverbs_object_tree_def **trees = dev->driver_trees;
+ 	size_t num_trees = 0;
+ 
+ 	trees[num_trees++] = &mlx5_ib_flow_action;
+ 	trees[num_trees++] = &mlx5_ib_dm;
+ 
+ 	if (MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &
+ 	    MLX5_GENERAL_OBJ_TYPES_CAP_UCTX)
+ 		trees[num_trees++] = mlx5_ib_get_devx_tree();
+ 
+ 	num_trees += mlx5_ib_get_flow_trees(trees + num_trees);
+ 
+ 	WARN_ON(num_trees >= ARRAY_SIZE(dev->driver_trees));
+ 	trees[num_trees] = NULL;
+ 	dev->ib_dev.driver_specs = trees;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5_ib_read_counters(struct ib_counters *counters,
+ 				 struct ib_counters_read_attr *read_attr,
+ 				 struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 	struct mlx5_read_counters_attr mread_attr = {};
+ 	struct mlx5_ib_flow_counters_desc *desc;
+ 	int ret, i;
+ 
+ 	mutex_lock(&mcounters->mcntrs_mutex);
+ 	if (mcounters->cntrs_max_index > read_attr->ncounters) {
+ 		ret = -EINVAL;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.out = kcalloc(mcounters->counters_num, sizeof(u64),
+ 				 GFP_KERNEL);
+ 	if (!mread_attr.out) {
+ 		ret = -ENOMEM;
+ 		goto err_bound;
+ 	}
+ 
+ 	mread_attr.hw_cntrs_hndl = mcounters->hw_cntrs_hndl;
+ 	mread_attr.flags = read_attr->flags;
+ 	ret = mcounters->read_counters(counters->device, &mread_attr);
+ 	if (ret)
+ 		goto err_read;
+ 
+ 	/* do the pass over the counters data array to assign according to the
+ 	 * descriptions and indexing pairs
+ 	 */
+ 	desc = mcounters->counters_data;
+ 	for (i = 0; i < mcounters->ncounters; i++)
+ 		read_attr->counters_buff[desc[i].index] += mread_attr.out[desc[i].description];
+ 
+ err_read:
+ 	kfree(mread_attr.out);
+ err_bound:
+ 	mutex_unlock(&mcounters->mcntrs_mutex);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_destroy_counters(struct ib_counters *counters)
+ {
+ 	struct mlx5_ib_mcounters *mcounters = to_mcounters(counters);
+ 
+ 	counters_clear_description(counters);
+ 	if (mcounters->hw_cntrs_hndl)
+ 		mlx5_fc_destroy(to_mdev(counters->device)->mdev,
+ 				mcounters->hw_cntrs_hndl);
+ 
+ 	kfree(mcounters);
+ 
+ 	return 0;
+ }
+ 
+ static struct ib_counters *mlx5_ib_create_counters(struct ib_device *device,
+ 						   struct uverbs_attr_bundle *attrs)
+ {
+ 	struct mlx5_ib_mcounters *mcounters;
+ 
+ 	mcounters = kzalloc(sizeof(*mcounters), GFP_KERNEL);
+ 	if (!mcounters)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mutex_init(&mcounters->mcntrs_mutex);
+ 
+ 	return &mcounters->ibcntrs;
+ }
+ 
+ void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
++>>>>>>> dfb631a187b9 (RDMA/mlx5: Do not generate the uabi specs unconditionally)
  {
  	mlx5_ib_cleanup_multiport_master(dev);
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@@ -5023,6 -5874,18 +5136,21 @@@ static int mlx5_ib_stage_caps_init(stru
  	dev->ib_dev.uverbs_ex_cmd_mask |=
  			(1ull << IB_USER_VERBS_EX_CMD_CREATE_FLOW) |
  			(1ull << IB_USER_VERBS_EX_CMD_DESTROY_FLOW);
++<<<<<<< HEAD
++=======
+ 	if (mlx5_accel_ipsec_device_caps(dev->mdev) &
+ 	    MLX5_ACCEL_IPSEC_CAP_DEVICE) {
+ 		dev->ib_dev.create_flow_action_esp =
+ 			mlx5_ib_create_flow_action_esp;
+ 		dev->ib_dev.modify_flow_action_esp =
+ 			mlx5_ib_modify_flow_action_esp;
+ 	}
+ 	dev->ib_dev.destroy_flow_action = mlx5_ib_destroy_flow_action;
+ 	dev->ib_dev.driver_id = RDMA_DRIVER_MLX5;
+ 	dev->ib_dev.create_counters = mlx5_ib_create_counters;
+ 	dev->ib_dev.destroy_counters = mlx5_ib_destroy_counters;
+ 	dev->ib_dev.read_counters = mlx5_ib_read_counters;
++>>>>>>> dfb631a187b9 (RDMA/mlx5: Do not generate the uabi specs unconditionally)
  
  	err = init_node_data(dev);
  	if (err)
* Unmerged path drivers/infiniband/hw/mlx5/main.c
