xfs: move pre/post-bmap tracing into xfs_iext_update_extent

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Christoph Hellwig <hch@lst.de>
commit ca5d8e5b7b9030005e38e7c43e08c0cd4eb2a78f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ca5d8e5b.failed

xfs_iext_update_extent already has basically all the information needed
to centralize the bmap pre/post tracing.  We just need to pass inode +
bmap state instead of the inode fork pointer to get all trace annotations.

In addition to covering all the existing trace points this gives us
tracing coverage for the extent shifting operations for free.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit ca5d8e5b7b9030005e38e7c43e08c0cd4eb2a78f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/libxfs/xfs_bmap.c
#	fs/xfs/libxfs/xfs_inode_fork.c
#	fs/xfs/libxfs/xfs_inode_fork.h
diff --cc fs/xfs/libxfs/xfs_bmap.c
index b9959d9eb265,c2d6f2b4112a..000000000000
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@@ -1660,14 -1669,11 +1660,19 @@@ xfs_bmap_add_extent_delay_real
  		 * The left and right neighbors are both contiguous with new.
  		 */
  		bma->idx--;
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(xfs_iext_get_ext(ifp, bma->idx),
 +			LEFT.br_blockcount + PREV.br_blockcount +
 +			RIGHT.br_blockcount);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 		LEFT.br_blockcount += PREV.br_blockcount + RIGHT.br_blockcount;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &LEFT);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  		xfs_iext_remove(bma->ip, bma->idx + 1, 2, state);
 -		(*nextents)--;
 +		bma->ip->i_d.di_nextents--;
  		if (bma->cur == NULL)
  			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
  		else {
@@@ -1703,10 -1703,9 +1708,16 @@@
  		 */
  		bma->idx--;
  
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(xfs_iext_get_ext(ifp, bma->idx),
 +			LEFT.br_blockcount + PREV.br_blockcount);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 		old = LEFT;
+ 		LEFT.br_blockcount += PREV.br_blockcount;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &LEFT);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  		xfs_iext_remove(bma->ip, bma->idx + 1, 1, state);
  		if (bma->cur == NULL)
@@@ -1733,11 -1727,9 +1744,17 @@@
  		 * Filling in all of a previously delayed allocation extent.
  		 * The right neighbor is contiguous, the left is not.
  		 */
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_startblock(ep, new->br_startblock);
 +		xfs_bmbt_set_blockcount(ep,
 +			PREV.br_blockcount + RIGHT.br_blockcount);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 		PREV.br_startblock = new->br_startblock;
+ 		PREV.br_blockcount += RIGHT.br_blockcount;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  		xfs_iext_remove(bma->ip, bma->idx + 1, 1, state);
  		if (bma->cur == NULL)
@@@ -1765,12 -1752,11 +1782,18 @@@
  		 * Neither the left nor right neighbors are contiguous with
  		 * the new one.
  		 */
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_startblock(ep, new->br_startblock);
 +		xfs_bmbt_set_state(ep, new->br_state);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 		PREV.br_startblock = new->br_startblock;
+ 		PREV.br_state = new->br_state;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
 -		(*nextents)++;
 +		bma->ip->i_d.di_nextents++;
  		if (bma->cur == NULL)
  			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
  		else {
@@@ -1794,16 -1777,19 +1817,32 @@@
  		 * Filling in the first part of a previous delayed allocation.
  		 * The left neighbor is contiguous.
  		 */
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx - 1, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(xfs_iext_get_ext(ifp, bma->idx - 1),
 +			LEFT.br_blockcount + new->br_blockcount);
 +		xfs_bmbt_set_startoff(ep,
 +			PREV.br_startoff + new->br_blockcount);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx - 1, state, _THIS_IP_);
 +
 +		temp = PREV.br_blockcount - new->br_blockcount;
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(ep, temp);
++=======
+ 		old = LEFT;
+ 		temp = PREV.br_blockcount - new->br_blockcount;
+ 		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(bma->ip, temp),
+ 				startblockval(PREV.br_startblock));
+ 
+ 		LEFT.br_blockcount += new->br_blockcount;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx - 1, &LEFT);
+ 
+ 		PREV.br_blockcount = temp = PREV.br_blockcount - new->br_blockcount;
+ 		PREV.br_startoff += new->br_blockcount;
+ 		PREV.br_startblock = nullstartblock(da_new);
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
+ 
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		if (bma->cur == NULL)
  			rval = XFS_ILOG_DEXT;
  		else {
@@@ -1869,9 -1838,13 +1908,17 @@@
  		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(bma->ip, temp),
  			startblockval(PREV.br_startblock) -
  			(bma->cur ? bma->cur->bc_private.b.allocated : 0));
++<<<<<<< HEAD
 +		ep = xfs_iext_get_ext(ifp, bma->idx + 1);
 +		xfs_bmbt_set_startblock(ep, nullstartblock(da_new));
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx + 1, state, _THIS_IP_);
++=======
+ 
+ 		PREV.br_startoff = new_endoff;
+ 		PREV.br_blockcount = temp;
+ 		PREV.br_startblock = nullstartblock(da_new);
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx + 1, &PREV);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		break;
  
  	case BMAP_RIGHT_FILLING | BMAP_RIGHT_CONTIG:
@@@ -1879,14 -1852,12 +1926,23 @@@
  		 * Filling in the last part of a previous delayed allocation.
  		 * The right neighbor is contiguous with the new allocation.
  		 */
++<<<<<<< HEAD
 +		temp = PREV.br_blockcount - new->br_blockcount;
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx + 1, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(ep, temp);
 +		xfs_bmbt_set_allf(xfs_iext_get_ext(ifp, bma->idx + 1),
 +			new->br_startoff, new->br_startblock,
 +			new->br_blockcount + RIGHT.br_blockcount,
 +			RIGHT.br_state);
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx + 1, state, _THIS_IP_);
++=======
+ 		old = RIGHT;
+ 		RIGHT.br_startoff = new->br_startoff;
+ 		RIGHT.br_startblock = new->br_startblock;
+ 		RIGHT.br_blockcount += new->br_blockcount;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx + 1, &RIGHT);
+ 
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		if (bma->cur == NULL)
  			rval = XFS_ILOG_DEXT;
  		else {
@@@ -1906,11 -1871,13 +1962,18 @@@
  				goto done;
  		}
  
 -		temp = PREV.br_blockcount - new->br_blockcount;
  		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(bma->ip, temp),
  			startblockval(PREV.br_startblock));
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, state, _THIS_IP_);
 +		xfs_bmbt_set_startblock(ep, nullstartblock(da_new));
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 
+ 		PREV.br_blockcount = temp;
+ 		PREV.br_startblock = nullstartblock(da_new);
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  		bma->idx++;
  		break;
@@@ -1953,9 -1914,12 +2016,16 @@@
  		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(bma->ip, temp),
  			startblockval(PREV.br_startblock) -
  			(bma->cur ? bma->cur->bc_private.b.allocated : 0));
++<<<<<<< HEAD
 +		ep = xfs_iext_get_ext(ifp, bma->idx);
 +		xfs_bmbt_set_startblock(ep, nullstartblock(da_new));
 +		trace_xfs_bmap_post_update(bma->ip, bma->idx, state, _THIS_IP_);
++=======
+ 
+ 		PREV.br_startblock = nullstartblock(da_new);
+ 		PREV.br_blockcount = temp;
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  		bma->idx++;
  		break;
@@@ -1981,19 -1945,31 +2051,43 @@@
  		 *  PREV @ idx          LEFT              RIGHT
  		 *                      inserted at idx + 1
  		 */
++<<<<<<< HEAD
 +		temp = new->br_startoff - PREV.br_startoff;
 +		temp2 = PREV.br_startoff + PREV.br_blockcount - new_endoff;
 +		trace_xfs_bmap_pre_update(bma->ip, bma->idx, 0, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(ep, temp);	/* truncate PREV */
 +		LEFT = *new;
 +		RIGHT.br_state = PREV.br_state;
 +		RIGHT.br_startblock = nullstartblock(
 +				(int)xfs_bmap_worst_indlen(bma->ip, temp2));
 +		RIGHT.br_startoff = new_endoff;
 +		RIGHT.br_blockcount = temp2;
++=======
+ 		old = PREV;
+ 
+ 		/* LEFT is the new middle */
+ 		LEFT = *new;
+ 
+ 		/* RIGHT is the new right */
+ 		RIGHT.br_state = PREV.br_state;
+ 		RIGHT.br_startoff = new_endoff;
+ 		RIGHT.br_blockcount =
+ 			PREV.br_startoff + PREV.br_blockcount - new_endoff;
+ 		RIGHT.br_startblock =
+ 			nullstartblock(xfs_bmap_worst_indlen(bma->ip,
+ 					RIGHT.br_blockcount));
+ 
+ 		/* truncate PREV */
+ 		PREV.br_blockcount = new->br_startoff - PREV.br_startoff;
+ 		PREV.br_startblock =
+ 			nullstartblock(xfs_bmap_worst_indlen(bma->ip,
+ 					PREV.br_blockcount));
+ 		xfs_iext_update_extent(bma->ip, state, bma->idx, &PREV);
+ 
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		/* insert LEFT (r[0]) and RIGHT (r[1]) at the same time */
  		xfs_iext_insert(bma->ip, bma->idx + 1, 2, &LEFT, state);
 -		(*nextents)++;
 -
 +		bma->ip->i_d.di_nextents++;
  		if (bma->cur == NULL)
  			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
  		else {
@@@ -4653,12 -4657,213 +4715,216 @@@ xfs_bmap_split_indlen
  	return stolen;
  }
  
++<<<<<<< HEAD
++=======
+ int
+ xfs_bmap_del_extent_delay(
+ 	struct xfs_inode	*ip,
+ 	int			whichfork,
+ 	xfs_extnum_t		*idx,
+ 	struct xfs_bmbt_irec	*got,
+ 	struct xfs_bmbt_irec	*del)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_ifork	*ifp = XFS_IFORK_PTR(ip, whichfork);
+ 	struct xfs_bmbt_irec	new;
+ 	int64_t			da_old, da_new, da_diff = 0;
+ 	xfs_fileoff_t		del_endoff, got_endoff;
+ 	xfs_filblks_t		got_indlen, new_indlen, stolen;
+ 	int			state = xfs_bmap_fork_to_state(whichfork);
+ 	int			error = 0;
+ 	bool			isrt;
+ 
+ 	XFS_STATS_INC(mp, xs_del_exlist);
+ 
+ 	isrt = (whichfork == XFS_DATA_FORK) && XFS_IS_REALTIME_INODE(ip);
+ 	del_endoff = del->br_startoff + del->br_blockcount;
+ 	got_endoff = got->br_startoff + got->br_blockcount;
+ 	da_old = startblockval(got->br_startblock);
+ 	da_new = 0;
+ 
+ 	ASSERT(*idx >= 0);
+ 	ASSERT(*idx <= xfs_iext_count(ifp));
+ 	ASSERT(del->br_blockcount > 0);
+ 	ASSERT(got->br_startoff <= del->br_startoff);
+ 	ASSERT(got_endoff >= del_endoff);
+ 
+ 	if (isrt) {
+ 		uint64_t rtexts = XFS_FSB_TO_B(mp, del->br_blockcount);
+ 
+ 		do_div(rtexts, mp->m_sb.sb_rextsize);
+ 		xfs_mod_frextents(mp, rtexts);
+ 	}
+ 
+ 	/*
+ 	 * Update the inode delalloc counter now and wait to update the
+ 	 * sb counters as we might have to borrow some blocks for the
+ 	 * indirect block accounting.
+ 	 */
+ 	error = xfs_trans_reserve_quota_nblks(NULL, ip,
+ 			-((long)del->br_blockcount), 0,
+ 			isrt ? XFS_QMOPT_RES_RTBLKS : XFS_QMOPT_RES_REGBLKS);
+ 	if (error)
+ 		return error;
+ 	ip->i_delayed_blks -= del->br_blockcount;
+ 
+ 	if (got->br_startoff == del->br_startoff)
+ 		state |= BMAP_LEFT_FILLING;
+ 	if (got_endoff == del_endoff)
+ 		state |= BMAP_RIGHT_FILLING;
+ 
+ 	switch (state & (BMAP_LEFT_FILLING | BMAP_RIGHT_FILLING)) {
+ 	case BMAP_LEFT_FILLING | BMAP_RIGHT_FILLING:
+ 		/*
+ 		 * Matches the whole extent.  Delete the entry.
+ 		 */
+ 		xfs_iext_remove(ip, *idx, 1, state);
+ 		--*idx;
+ 		break;
+ 	case BMAP_LEFT_FILLING:
+ 		/*
+ 		 * Deleting the first part of the extent.
+ 		 */
+ 		got->br_startoff = del_endoff;
+ 		got->br_blockcount -= del->br_blockcount;
+ 		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip,
+ 				got->br_blockcount), da_old);
+ 		got->br_startblock = nullstartblock((int)da_new);
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 		break;
+ 	case BMAP_RIGHT_FILLING:
+ 		/*
+ 		 * Deleting the last part of the extent.
+ 		 */
+ 		got->br_blockcount = got->br_blockcount - del->br_blockcount;
+ 		da_new = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip,
+ 				got->br_blockcount), da_old);
+ 		got->br_startblock = nullstartblock((int)da_new);
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 		break;
+ 	case 0:
+ 		/*
+ 		 * Deleting the middle of the extent.
+ 		 *
+ 		 * Distribute the original indlen reservation across the two new
+ 		 * extents.  Steal blocks from the deleted extent if necessary.
+ 		 * Stealing blocks simply fudges the fdblocks accounting below.
+ 		 * Warn if either of the new indlen reservations is zero as this
+ 		 * can lead to delalloc problems.
+ 		 */
+ 		got->br_blockcount = del->br_startoff - got->br_startoff;
+ 		got_indlen = xfs_bmap_worst_indlen(ip, got->br_blockcount);
+ 
+ 		new.br_blockcount = got_endoff - del_endoff;
+ 		new_indlen = xfs_bmap_worst_indlen(ip, new.br_blockcount);
+ 
+ 		WARN_ON_ONCE(!got_indlen || !new_indlen);
+ 		stolen = xfs_bmap_split_indlen(da_old, &got_indlen, &new_indlen,
+ 						       del->br_blockcount);
+ 
+ 		got->br_startblock = nullstartblock((int)got_indlen);
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 
+ 		new.br_startoff = del_endoff;
+ 		new.br_state = got->br_state;
+ 		new.br_startblock = nullstartblock((int)new_indlen);
+ 
+ 		++*idx;
+ 		xfs_iext_insert(ip, *idx, 1, &new, state);
+ 
+ 		da_new = got_indlen + new_indlen - stolen;
+ 		del->br_blockcount -= stolen;
+ 		break;
+ 	}
+ 
+ 	ASSERT(da_old >= da_new);
+ 	da_diff = da_old - da_new;
+ 	if (!isrt)
+ 		da_diff += del->br_blockcount;
+ 	if (da_diff)
+ 		xfs_mod_fdblocks(mp, da_diff, false);
+ 	return error;
+ }
+ 
+ void
+ xfs_bmap_del_extent_cow(
+ 	struct xfs_inode	*ip,
+ 	xfs_extnum_t		*idx,
+ 	struct xfs_bmbt_irec	*got,
+ 	struct xfs_bmbt_irec	*del)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_ifork	*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);
+ 	struct xfs_bmbt_irec	new;
+ 	xfs_fileoff_t		del_endoff, got_endoff;
+ 	int			state = BMAP_COWFORK;
+ 
+ 	XFS_STATS_INC(mp, xs_del_exlist);
+ 
+ 	del_endoff = del->br_startoff + del->br_blockcount;
+ 	got_endoff = got->br_startoff + got->br_blockcount;
+ 
+ 	ASSERT(*idx >= 0);
+ 	ASSERT(*idx <= xfs_iext_count(ifp));
+ 	ASSERT(del->br_blockcount > 0);
+ 	ASSERT(got->br_startoff <= del->br_startoff);
+ 	ASSERT(got_endoff >= del_endoff);
+ 	ASSERT(!isnullstartblock(got->br_startblock));
+ 
+ 	if (got->br_startoff == del->br_startoff)
+ 		state |= BMAP_LEFT_FILLING;
+ 	if (got_endoff == del_endoff)
+ 		state |= BMAP_RIGHT_FILLING;
+ 
+ 	switch (state & (BMAP_LEFT_FILLING | BMAP_RIGHT_FILLING)) {
+ 	case BMAP_LEFT_FILLING | BMAP_RIGHT_FILLING:
+ 		/*
+ 		 * Matches the whole extent.  Delete the entry.
+ 		 */
+ 		xfs_iext_remove(ip, *idx, 1, state);
+ 		--*idx;
+ 		break;
+ 	case BMAP_LEFT_FILLING:
+ 		/*
+ 		 * Deleting the first part of the extent.
+ 		 */
+ 		got->br_startoff = del_endoff;
+ 		got->br_blockcount -= del->br_blockcount;
+ 		got->br_startblock = del->br_startblock + del->br_blockcount;
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 		break;
+ 	case BMAP_RIGHT_FILLING:
+ 		/*
+ 		 * Deleting the last part of the extent.
+ 		 */
+ 		got->br_blockcount -= del->br_blockcount;
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 		break;
+ 	case 0:
+ 		/*
+ 		 * Deleting the middle of the extent.
+ 		 */
+ 		got->br_blockcount = del->br_startoff - got->br_startoff;
+ 		xfs_iext_update_extent(ip, state, *idx, got);
+ 
+ 		new.br_startoff = del_endoff;
+ 		new.br_blockcount = got_endoff - del_endoff;
+ 		new.br_state = got->br_state;
+ 		new.br_startblock = del->br_startblock + del->br_blockcount;
+ 
+ 		++*idx;
+ 		xfs_iext_insert(ip, *idx, 1, &new, state);
+ 		break;
+ 	}
+ }
+ 
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  /*
   * Called by xfs_bmapi to update file extent records and the btree
 - * after removing space.
 + * after removing space (or undoing a delayed allocation).
   */
  STATIC int				/* error */
 -xfs_bmap_del_extent_real(
 +xfs_bmap_del_extent(
  	xfs_inode_t		*ip,	/* incore inode pointer */
  	xfs_trans_t		*tp,	/* current transaction pointer */
  	xfs_extnum_t		*idx,	/* extent number to update/delete */
@@@ -4773,11 -4962,8 +5039,15 @@@
  		/*
  		 * Matches the whole extent.  Delete the entry.
  		 */
++<<<<<<< HEAD
 +		xfs_iext_remove(ip, *idx, 1,
 +				whichfork == XFS_ATTR_FORK ? BMAP_ATTRFORK : 0);
++=======
+ 		xfs_iext_remove(ip, *idx, 1, state);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		--*idx;
 +		if (delay)
 +			break;
  
  		XFS_IFORK_NEXT_SET(ip, whichfork,
  			XFS_IFORK_NEXTENTS(ip, whichfork) - 1);
@@@ -4795,20 -4980,10 +5065,27 @@@
  		/*
  		 * Deleting the first part of the extent.
  		 */
++<<<<<<< HEAD
 +		trace_xfs_bmap_pre_update(ip, *idx, state, _THIS_IP_);
 +		xfs_bmbt_set_startoff(ep, del_endoff);
 +		temp = got.br_blockcount - del->br_blockcount;
 +		xfs_bmbt_set_blockcount(ep, temp);
 +		if (delay) {
 +			temp = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip, temp),
 +				da_old);
 +			xfs_bmbt_set_startblock(ep, nullstartblock((int)temp));
 +			trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 +			da_new = temp;
 +			break;
 +		}
 +		xfs_bmbt_set_startblock(ep, del_endblock);
 +		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
++=======
+ 		got.br_startoff = del_endoff;
+ 		got.br_startblock = del_endblock;
+ 		got.br_blockcount -= del->br_blockcount;
+ 		xfs_iext_update_extent(ip, state, *idx, &got);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		if (!cur) {
  			flags |= xfs_ilog_fext(whichfork);
  			break;
@@@ -4823,18 -4996,8 +5100,23 @@@
  		/*
  		 * Deleting the last part of the extent.
  		 */
++<<<<<<< HEAD
 +		temp = got.br_blockcount - del->br_blockcount;
 +		trace_xfs_bmap_pre_update(ip, *idx, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(ep, temp);
 +		if (delay) {
 +			temp = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip, temp),
 +				da_old);
 +			xfs_bmbt_set_startblock(ep, nullstartblock((int)temp));
 +			trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 +			da_new = temp;
 +			break;
 +		}
 +		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
++=======
+ 		got.br_blockcount -= del->br_blockcount;
+ 		xfs_iext_update_extent(ip, state, *idx, &got);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		if (!cur) {
  			flags |= xfs_ilog_fext(whichfork);
  			break;
@@@ -4850,97 -5010,63 +5132,129 @@@
  		/*
  		 * Deleting the middle of the extent.
  		 */
++<<<<<<< HEAD
 +		temp = del->br_startoff - got.br_startoff;
 +		trace_xfs_bmap_pre_update(ip, *idx, state, _THIS_IP_);
 +		xfs_bmbt_set_blockcount(ep, temp);
++=======
+ 		old = got;
+ 
+ 		got.br_blockcount = del->br_startoff - got.br_startoff;
+ 		xfs_iext_update_extent(ip, state, *idx, &got);
+ 
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		new.br_startoff = del_endoff;
 -		new.br_blockcount = got_endoff - del_endoff;
 +		temp2 = got_endoff - del_endoff;
 +		new.br_blockcount = temp2;
  		new.br_state = got.br_state;
 -		new.br_startblock = del_endblock;
 -
 -		flags |= XFS_ILOG_CORE;
 -		if (cur) {
 -			error = xfs_bmbt_update(cur, &got);
 -			if (error)
 -				goto done;
 -			error = xfs_btree_increment(cur, 0, &i);
 -			if (error)
 -				goto done;
 -			cur->bc_rec.b = new;
 -			error = xfs_btree_insert(cur, &i);
 -			if (error && error != -ENOSPC)
 -				goto done;
 -			/*
 -			 * If get no-space back from btree insert, it tried a
 -			 * split, and we have a zero block reservation.  Fix up
 -			 * our state and return the error.
 -			 */
 -			if (error == -ENOSPC) {
 +		if (!delay) {
 +			new.br_startblock = del_endblock;
 +			flags |= XFS_ILOG_CORE;
 +			if (cur) {
 +				if ((error = xfs_bmbt_update(cur,
 +						got.br_startoff,
 +						got.br_startblock, temp,
 +						got.br_state)))
 +					goto done;
 +				if ((error = xfs_btree_increment(cur, 0, &i)))
 +					goto done;
 +				cur->bc_rec.b = new;
 +				error = xfs_btree_insert(cur, &i);
 +				if (error && error != -ENOSPC)
 +					goto done;
  				/*
 -				 * Reset the cursor, don't trust it after any
 -				 * insert operation.
 +				 * If get no-space back from btree insert,
 +				 * it tried a split, and we have a zero
 +				 * block reservation.
 +				 * Fix up our state and return the error.
  				 */
 -				error = xfs_bmbt_lookup_eq(cur, &got, &i);
 -				if (error)
 +				if (error == -ENOSPC) {
 +					/*
 +					 * Reset the cursor, don't trust
 +					 * it after any insert operation.
 +					 */
 +					if ((error = xfs_bmbt_lookup_eq(cur,
 +							got.br_startoff,
 +							got.br_startblock,
 +							temp, &i)))
 +						goto done;
 +					XFS_WANT_CORRUPTED_GOTO(mp,
 +								i == 1, done);
 +					/*
 +					 * Update the btree record back
 +					 * to the original value.
 +					 */
 +					if ((error = xfs_bmbt_update(cur,
 +							got.br_startoff,
 +							got.br_startblock,
 +							got.br_blockcount,
 +							got.br_state)))
 +						goto done;
 +					/*
 +					 * Reset the extent record back
 +					 * to the original value.
 +					 */
 +					xfs_bmbt_set_blockcount(ep,
 +						got.br_blockcount);
 +					flags = 0;
 +					error = -ENOSPC;
  					goto done;
 +				}
  				XFS_WANT_CORRUPTED_GOTO(mp, i == 1, done);
++<<<<<<< HEAD
 +			} else
 +				flags |= xfs_ilog_fext(whichfork);
 +			XFS_IFORK_NEXT_SET(ip, whichfork,
 +				XFS_IFORK_NEXTENTS(ip, whichfork) + 1);
 +		} else {
 +			xfs_filblks_t	stolen;
 +			ASSERT(whichfork == XFS_DATA_FORK);
 +
 +			/*
 +			 * Distribute the original indlen reservation across the
 +			 * two new extents. Steal blocks from the deleted extent
 +			 * if necessary. Stealing blocks simply fudges the
 +			 * fdblocks accounting in xfs_bunmapi().
 +			 */
 +			temp = xfs_bmap_worst_indlen(ip, got.br_blockcount);
 +			temp2 = xfs_bmap_worst_indlen(ip, new.br_blockcount);
 +			stolen = xfs_bmap_split_indlen(da_old, &temp, &temp2,
 +						       del->br_blockcount);
 +			da_new = temp + temp2 - stolen;
 +			del->br_blockcount -= stolen;
 +
 +			/*
 +			 * Set the reservation for each extent. Warn if either
 +			 * is zero as this can lead to delalloc problems.
 +			 */
 +			WARN_ON_ONCE(!temp || !temp2);
 +			xfs_bmbt_set_startblock(ep, nullstartblock((int)temp));
 +			new.br_startblock = nullstartblock((int)temp2);
 +		}
 +		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
++=======
+ 				/*
+ 				 * Update the btree record back
+ 				 * to the original value.
+ 				 */
+ 				error = xfs_bmbt_update(cur, &old);
+ 				if (error)
+ 					goto done;
+ 				/*
+ 				 * Reset the extent record back
+ 				 * to the original value.
+ 				 */
+ 				xfs_iext_update_extent(ip, state, *idx, &old);
+ 				flags = 0;
+ 				error = -ENOSPC;
+ 				goto done;
+ 			}
+ 			XFS_WANT_CORRUPTED_GOTO(mp, i == 1, done);
+ 		} else
+ 			flags |= xfs_ilog_fext(whichfork);
+ 		XFS_IFORK_NEXT_SET(ip, whichfork,
+ 			XFS_IFORK_NEXTENTS(ip, whichfork) + 1);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  		xfs_iext_insert(ip, *idx + 1, 1, &new, state);
  		++*idx;
  		break;
@@@ -5427,13 -5535,13 +5741,17 @@@ xfs_bmse_merge
  	int				whichfork,
  	xfs_fileoff_t			shift,		/* shift fsb */
  	int				current_ext,	/* idx of gotp */
 -	struct xfs_bmbt_irec		*got,		/* extent to shift */
 -	struct xfs_bmbt_irec		*left,		/* preceding extent */
 +	struct xfs_bmbt_rec_host	*gotp,		/* extent to shift */
 +	struct xfs_bmbt_rec_host	*leftp,		/* preceding extent */
  	struct xfs_btree_cur		*cur,
 -	int				*logflags,	/* output */
 -	struct xfs_defer_ops		*dfops)
 +	int				*logflags)	/* output */
  {
++<<<<<<< HEAD
 +	struct xfs_bmbt_irec		got;
 +	struct xfs_bmbt_irec		left;
++=======
+ 	struct xfs_bmbt_irec		new;
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  	xfs_filblks_t			blockcount;
  	int				error, i;
  	struct xfs_mount		*mp = ip->i_mount;
@@@ -5485,10 -5584,22 +5803,25 @@@
  		return error;
  	XFS_WANT_CORRUPTED_RETURN(mp, i == 1);
  
 -	error = xfs_bmbt_update(cur, &new);
 -	if (error)
 -		return error;
 +	left.br_blockcount = blockcount;
  
++<<<<<<< HEAD
 +	return xfs_bmbt_update(cur, left.br_startoff, left.br_startblock,
 +			       left.br_blockcount, left.br_state);
++=======
+ done:
+ 	xfs_iext_update_extent(ip, xfs_bmap_fork_to_state(whichfork),
+ 			current_ext - 1, &new);
+ 	xfs_iext_remove(ip, current_ext, 1, 0);
+ 
+ 	/* update reverse mapping. rmap functions merge the rmaps for us */
+ 	error = xfs_rmap_unmap_extent(mp, dfops, ip, whichfork, got);
+ 	if (error)
+ 		return error;
+ 	memcpy(&new, got, sizeof(new));
+ 	new.br_startoff = left->br_startoff + left->br_blockcount;
+ 	return xfs_rmap_map_extent(mp, dfops, ip, whichfork, &new);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  }
  
  /*
@@@ -5547,23 -5689,38 +5880,36 @@@ xfs_bmse_shift_one
  	 * Increment the extent index for the next iteration, update the start
  	 * offset of the in-core extent and update the btree if applicable.
  	 */
 -update_current_ext:
 +	(*current_ext)++;
 +	xfs_bmbt_set_startoff(gotp, startoff);
  	*logflags |= XFS_ILOG_CORE;
 -
 -	new = *got;
 -	new.br_startoff = startoff;
 -
 -	if (cur) {
 -		error = xfs_bmbt_lookup_eq(cur, got, &i);
 -		if (error)
 -			return error;
 -		XFS_WANT_CORRUPTED_RETURN(mp, i == 1);
 -
 -		error = xfs_bmbt_update(cur, &new);
 -		if (error)
 -			return error;
 -	} else {
 +	if (!cur) {
  		*logflags |= XFS_ILOG_DEXT;
 +		return 0;
  	}
  
++<<<<<<< HEAD
 +	error = xfs_bmbt_lookup_eq(cur, got.br_startoff, got.br_startblock,
 +				   got.br_blockcount, &i);
++=======
+ 	xfs_iext_update_extent(ip, xfs_bmap_fork_to_state(whichfork),
+ 			*current_ext, &new);
+ 
+ 	if (direction == SHIFT_LEFT)
+ 		(*current_ext)++;
+ 	else
+ 		(*current_ext)--;
+ 
+ 	/* update reverse mapping */
+ 	error = xfs_rmap_unmap_extent(mp, dfops, ip, whichfork, got);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  	if (error)
  		return error;
 -	return xfs_rmap_map_extent(mp, dfops, ip, whichfork, &new);
 +	XFS_WANT_CORRUPTED_RETURN(mp, i == 1);
 +
 +	got.br_startoff = startoff;
 +	return xfs_bmbt_update(cur, got.br_startoff, got.br_startblock,
 +				got.br_blockcount, got.br_state);
  }
  
  /*
@@@ -5682,3 -5884,300 +6028,303 @@@ del_cursor
  
  	return error;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Splits an extent into two extents at split_fsb block such that it is
+  * the first block of the current_ext. @current_ext is a target extent
+  * to be split. @split_fsb is a block where the extents is split.
+  * If split_fsb lies in a hole or the first block of extents, just return 0.
+  */
+ STATIC int
+ xfs_bmap_split_extent_at(
+ 	struct xfs_trans	*tp,
+ 	struct xfs_inode	*ip,
+ 	xfs_fileoff_t		split_fsb,
+ 	xfs_fsblock_t		*firstfsb,
+ 	struct xfs_defer_ops	*dfops)
+ {
+ 	int				whichfork = XFS_DATA_FORK;
+ 	struct xfs_btree_cur		*cur = NULL;
+ 	struct xfs_bmbt_irec		got;
+ 	struct xfs_bmbt_irec		new; /* split extent */
+ 	struct xfs_mount		*mp = ip->i_mount;
+ 	struct xfs_ifork		*ifp;
+ 	xfs_fsblock_t			gotblkcnt; /* new block count for got */
+ 	xfs_extnum_t			current_ext;
+ 	int				error = 0;
+ 	int				logflags = 0;
+ 	int				i = 0;
+ 
+ 	if (unlikely(XFS_TEST_ERROR(
+ 	    (XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_EXTENTS &&
+ 	     XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE),
+ 	     mp, XFS_ERRTAG_BMAPIFORMAT))) {
+ 		XFS_ERROR_REPORT("xfs_bmap_split_extent_at",
+ 				 XFS_ERRLEVEL_LOW, mp);
+ 		return -EFSCORRUPTED;
+ 	}
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	ifp = XFS_IFORK_PTR(ip, whichfork);
+ 	if (!(ifp->if_flags & XFS_IFEXTENTS)) {
+ 		/* Read in all the extents */
+ 		error = xfs_iread_extents(tp, ip, whichfork);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	/*
+ 	 * If there are not extents, or split_fsb lies in a hole we are done.
+ 	 */
+ 	if (!xfs_iext_lookup_extent(ip, ifp, split_fsb, &current_ext, &got) ||
+ 	    got.br_startoff >= split_fsb)
+ 		return 0;
+ 
+ 	gotblkcnt = split_fsb - got.br_startoff;
+ 	new.br_startoff = split_fsb;
+ 	new.br_startblock = got.br_startblock + gotblkcnt;
+ 	new.br_blockcount = got.br_blockcount - gotblkcnt;
+ 	new.br_state = got.br_state;
+ 
+ 	if (ifp->if_flags & XFS_IFBROOT) {
+ 		cur = xfs_bmbt_init_cursor(mp, tp, ip, whichfork);
+ 		cur->bc_private.b.firstblock = *firstfsb;
+ 		cur->bc_private.b.dfops = dfops;
+ 		cur->bc_private.b.flags = 0;
+ 		error = xfs_bmbt_lookup_eq(cur, &got, &i);
+ 		if (error)
+ 			goto del_cursor;
+ 		XFS_WANT_CORRUPTED_GOTO(mp, i == 1, del_cursor);
+ 	}
+ 
+ 	got.br_blockcount = gotblkcnt;
+ 	xfs_iext_update_extent(ip, xfs_bmap_fork_to_state(whichfork),
+ 			current_ext, &got);
+ 
+ 	logflags = XFS_ILOG_CORE;
+ 	if (cur) {
+ 		error = xfs_bmbt_update(cur, &got);
+ 		if (error)
+ 			goto del_cursor;
+ 	} else
+ 		logflags |= XFS_ILOG_DEXT;
+ 
+ 	/* Add new extent */
+ 	current_ext++;
+ 	xfs_iext_insert(ip, current_ext, 1, &new, 0);
+ 	XFS_IFORK_NEXT_SET(ip, whichfork,
+ 			   XFS_IFORK_NEXTENTS(ip, whichfork) + 1);
+ 
+ 	if (cur) {
+ 		error = xfs_bmbt_lookup_eq(cur, &new, &i);
+ 		if (error)
+ 			goto del_cursor;
+ 		XFS_WANT_CORRUPTED_GOTO(mp, i == 0, del_cursor);
+ 		error = xfs_btree_insert(cur, &i);
+ 		if (error)
+ 			goto del_cursor;
+ 		XFS_WANT_CORRUPTED_GOTO(mp, i == 1, del_cursor);
+ 	}
+ 
+ 	/*
+ 	 * Convert to a btree if necessary.
+ 	 */
+ 	if (xfs_bmap_needs_btree(ip, whichfork)) {
+ 		int tmp_logflags; /* partial log flag return val */
+ 
+ 		ASSERT(cur == NULL);
+ 		error = xfs_bmap_extents_to_btree(tp, ip, firstfsb, dfops,
+ 				&cur, 0, &tmp_logflags, whichfork);
+ 		logflags |= tmp_logflags;
+ 	}
+ 
+ del_cursor:
+ 	if (cur) {
+ 		cur->bc_private.b.allocated = 0;
+ 		xfs_btree_del_cursor(cur,
+ 				error ? XFS_BTREE_ERROR : XFS_BTREE_NOERROR);
+ 	}
+ 
+ 	if (logflags)
+ 		xfs_trans_log_inode(tp, ip, logflags);
+ 	return error;
+ }
+ 
+ int
+ xfs_bmap_split_extent(
+ 	struct xfs_inode        *ip,
+ 	xfs_fileoff_t           split_fsb)
+ {
+ 	struct xfs_mount        *mp = ip->i_mount;
+ 	struct xfs_trans        *tp;
+ 	struct xfs_defer_ops    dfops;
+ 	xfs_fsblock_t           firstfsb;
+ 	int                     error;
+ 
+ 	error = xfs_trans_alloc(mp, &M_RES(mp)->tr_write,
+ 			XFS_DIOSTRAT_SPACE_RES(mp, 0), 0, 0, &tp);
+ 	if (error)
+ 		return error;
+ 
+ 	xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+ 
+ 	xfs_defer_init(&dfops, &firstfsb);
+ 
+ 	error = xfs_bmap_split_extent_at(tp, ip, split_fsb,
+ 			&firstfsb, &dfops);
+ 	if (error)
+ 		goto out;
+ 
+ 	error = xfs_defer_finish(&tp, &dfops);
+ 	if (error)
+ 		goto out;
+ 
+ 	return xfs_trans_commit(tp);
+ 
+ out:
+ 	xfs_defer_cancel(&dfops);
+ 	xfs_trans_cancel(tp);
+ 	return error;
+ }
+ 
+ /* Deferred mapping is only for real extents in the data fork. */
+ static bool
+ xfs_bmap_is_update_needed(
+ 	struct xfs_bmbt_irec	*bmap)
+ {
+ 	return  bmap->br_startblock != HOLESTARTBLOCK &&
+ 		bmap->br_startblock != DELAYSTARTBLOCK;
+ }
+ 
+ /* Record a bmap intent. */
+ static int
+ __xfs_bmap_add(
+ 	struct xfs_mount		*mp,
+ 	struct xfs_defer_ops		*dfops,
+ 	enum xfs_bmap_intent_type	type,
+ 	struct xfs_inode		*ip,
+ 	int				whichfork,
+ 	struct xfs_bmbt_irec		*bmap)
+ {
+ 	int				error;
+ 	struct xfs_bmap_intent		*bi;
+ 
+ 	trace_xfs_bmap_defer(mp,
+ 			XFS_FSB_TO_AGNO(mp, bmap->br_startblock),
+ 			type,
+ 			XFS_FSB_TO_AGBNO(mp, bmap->br_startblock),
+ 			ip->i_ino, whichfork,
+ 			bmap->br_startoff,
+ 			bmap->br_blockcount,
+ 			bmap->br_state);
+ 
+ 	bi = kmem_alloc(sizeof(struct xfs_bmap_intent), KM_SLEEP | KM_NOFS);
+ 	INIT_LIST_HEAD(&bi->bi_list);
+ 	bi->bi_type = type;
+ 	bi->bi_owner = ip;
+ 	bi->bi_whichfork = whichfork;
+ 	bi->bi_bmap = *bmap;
+ 
+ 	error = xfs_defer_ijoin(dfops, bi->bi_owner);
+ 	if (error) {
+ 		kmem_free(bi);
+ 		return error;
+ 	}
+ 
+ 	xfs_defer_add(dfops, XFS_DEFER_OPS_TYPE_BMAP, &bi->bi_list);
+ 	return 0;
+ }
+ 
+ /* Map an extent into a file. */
+ int
+ xfs_bmap_map_extent(
+ 	struct xfs_mount	*mp,
+ 	struct xfs_defer_ops	*dfops,
+ 	struct xfs_inode	*ip,
+ 	struct xfs_bmbt_irec	*PREV)
+ {
+ 	if (!xfs_bmap_is_update_needed(PREV))
+ 		return 0;
+ 
+ 	return __xfs_bmap_add(mp, dfops, XFS_BMAP_MAP, ip,
+ 			XFS_DATA_FORK, PREV);
+ }
+ 
+ /* Unmap an extent out of a file. */
+ int
+ xfs_bmap_unmap_extent(
+ 	struct xfs_mount	*mp,
+ 	struct xfs_defer_ops	*dfops,
+ 	struct xfs_inode	*ip,
+ 	struct xfs_bmbt_irec	*PREV)
+ {
+ 	if (!xfs_bmap_is_update_needed(PREV))
+ 		return 0;
+ 
+ 	return __xfs_bmap_add(mp, dfops, XFS_BMAP_UNMAP, ip,
+ 			XFS_DATA_FORK, PREV);
+ }
+ 
+ /*
+  * Process one of the deferred bmap operations.  We pass back the
+  * btree cursor to maintain our lock on the bmapbt between calls.
+  */
+ int
+ xfs_bmap_finish_one(
+ 	struct xfs_trans		*tp,
+ 	struct xfs_defer_ops		*dfops,
+ 	struct xfs_inode		*ip,
+ 	enum xfs_bmap_intent_type	type,
+ 	int				whichfork,
+ 	xfs_fileoff_t			startoff,
+ 	xfs_fsblock_t			startblock,
+ 	xfs_filblks_t			*blockcount,
+ 	xfs_exntst_t			state)
+ {
+ 	xfs_fsblock_t			firstfsb;
+ 	int				error = 0;
+ 
+ 	/*
+ 	 * firstfsb is tied to the transaction lifetime and is used to
+ 	 * ensure correct AG locking order and schedule work item
+ 	 * continuations.  XFS_BUI_MAX_FAST_EXTENTS (== 1) restricts us
+ 	 * to only making one bmap call per transaction, so it should
+ 	 * be safe to have it as a local variable here.
+ 	 */
+ 	firstfsb = NULLFSBLOCK;
+ 
+ 	trace_xfs_bmap_deferred(tp->t_mountp,
+ 			XFS_FSB_TO_AGNO(tp->t_mountp, startblock), type,
+ 			XFS_FSB_TO_AGBNO(tp->t_mountp, startblock),
+ 			ip->i_ino, whichfork, startoff, *blockcount, state);
+ 
+ 	if (WARN_ON_ONCE(whichfork != XFS_DATA_FORK))
+ 		return -EFSCORRUPTED;
+ 
+ 	if (XFS_TEST_ERROR(false, tp->t_mountp,
+ 			XFS_ERRTAG_BMAP_FINISH_ONE))
+ 		return -EIO;
+ 
+ 	switch (type) {
+ 	case XFS_BMAP_MAP:
+ 		error = xfs_bmapi_remap(tp, ip, startoff, *blockcount,
+ 				startblock, dfops);
+ 		*blockcount = 0;
+ 		break;
+ 	case XFS_BMAP_UNMAP:
+ 		error = __xfs_bunmapi(tp, ip, startoff, blockcount,
+ 				XFS_BMAPI_REMAP, 1, &firstfsb, dfops);
+ 		break;
+ 	default:
+ 		ASSERT(0);
+ 		error = -EFSCORRUPTED;
+ 	}
+ 
+ 	return error;
+ }
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
diff --cc fs/xfs/libxfs/xfs_inode_fork.c
index e4a2be82a920,7f40f53e6c43..000000000000
--- a/fs/xfs/libxfs/xfs_inode_fork.c
+++ b/fs/xfs/libxfs/xfs_inode_fork.c
@@@ -1876,3 -2020,20 +1876,23 @@@ xfs_iext_get_extent
  	xfs_bmbt_get_all(xfs_iext_get_ext(ifp, idx), gotp);
  	return true;
  }
++<<<<<<< HEAD
++=======
+ 
+ void
+ xfs_iext_update_extent(
+ 	struct xfs_inode	*ip,
+ 	int			state,
+ 	xfs_extnum_t		idx,
+ 	struct xfs_bmbt_irec	*gotp)
+ {
+ 	struct xfs_ifork	*ifp = xfs_iext_state_to_fork(ip, state);
+ 
+ 	ASSERT(idx >= 0);
+ 	ASSERT(idx < xfs_iext_count(ifp));
+ 
+ 	trace_xfs_bmap_pre_update(ip, idx, state, _RET_IP_);
+ 	xfs_bmbt_set_all(xfs_iext_get_ext(ifp, idx), gotp);
+ 	trace_xfs_bmap_post_update(ip, idx, state, _RET_IP_);
+ }
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
diff --cc fs/xfs/libxfs/xfs_inode_fork.h
index e53b530cc0d4,6750f0462d21..000000000000
--- a/fs/xfs/libxfs/xfs_inode_fork.h
+++ b/fs/xfs/libxfs/xfs_inode_fork.h
@@@ -173,6 -187,8 +173,11 @@@ bool		xfs_iext_lookup_extent(struct xfs
  			xfs_extnum_t *idxp, struct xfs_bmbt_irec *gotp);
  bool		xfs_iext_get_extent(struct xfs_ifork *ifp, xfs_extnum_t idx,
  			struct xfs_bmbt_irec *gotp);
++<<<<<<< HEAD
++=======
+ void		xfs_iext_update_extent(struct xfs_inode *ip, int state,
+ 			xfs_extnum_t idx, struct xfs_bmbt_irec *gotp);
++>>>>>>> ca5d8e5b7b90 (xfs: move pre/post-bmap tracing into xfs_iext_update_extent)
  
  extern struct kmem_zone	*xfs_ifork_zone;
  
* Unmerged path fs/xfs/libxfs/xfs_bmap.c
* Unmerged path fs/xfs/libxfs/xfs_inode_fork.c
* Unmerged path fs/xfs/libxfs/xfs_inode_fork.h
