crypto: chelsio - Handle PCI shutdown event

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [crypto] chelsio - Handle PCI shutdown event (Arjun Vynipadath) [1642432]
Rebuild_FUZZ: 89.74%
commit-author Harsh Jain <harsh@chelsio.com>
commit fef4912b66d6246d958d97382d20d0dd23bcf0bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/fef4912b.failed

chcr receives "CXGB4_STATE_DETACH" event on PCI Shutdown.
Wait for processing of inflight request and Mark the device unavailable.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit fef4912b66d6246d958d97382d20d0dd23bcf0bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_core.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index 8a76dcf1a215,df526414f03f..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -1279,14 -1304,21 +1307,26 @@@ error
  static int chcr_aes_encrypt(struct ablkcipher_request *req)
  {
  	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+ 	struct chcr_dev *dev = c_ctx(tfm)->dev;
  	struct sk_buff *skb = NULL;
 -	int err, isfull = 0;
 +	int err;
  	struct uld_ctx *u_ctx = ULD_CTX(c_ctx(tfm));
  
+ 	err = chcr_inc_wrcount(dev);
+ 	if (err)
+ 		return -ENXIO;
  	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
  					    c_ctx(tfm)->tx_qidx))) {
++<<<<<<< HEAD
 +		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 +			return -EBUSY;
++=======
+ 		isfull = 1;
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+ 			err = -ENOSPC;
+ 			goto error;
+ 		}
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  	}
  
  	err = process_cipher(req, u_ctx->lldi.rxq_ids[c_ctx(tfm)->rx_qidx],
@@@ -1296,24 -1328,33 +1336,36 @@@
  	skb->dev = u_ctx->lldi.ports[0];
  	set_wr_txq(skb, CPL_PRIORITY_DATA, c_ctx(tfm)->tx_qidx);
  	chcr_send_wr(skb);
++<<<<<<< HEAD
 +	return -EINPROGRESS;
++=======
+ 	return isfull ? -EBUSY : -EINPROGRESS;
+ error:
+ 	chcr_dec_wrcount(dev);
+ 	return err;
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  }
  
  static int chcr_aes_decrypt(struct ablkcipher_request *req)
  {
  	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
  	struct uld_ctx *u_ctx = ULD_CTX(c_ctx(tfm));
+ 	struct chcr_dev *dev = c_ctx(tfm)->dev;
  	struct sk_buff *skb = NULL;
 -	int err, isfull = 0;
 +	int err;
  
+ 	err = chcr_inc_wrcount(dev);
+ 	if (err)
+ 		return -ENXIO;
+ 
  	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
  					    c_ctx(tfm)->tx_qidx))) {
 -		isfull = 1;
  		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 -			return -ENOSPC;
 +			return -EBUSY;
  	}
  
 -	err = process_cipher(req, u_ctx->lldi.rxq_ids[c_ctx(tfm)->rx_qidx],
 -			     &skb, CHCR_DECRYPT_OP);
 +	 err = process_cipher(req, u_ctx->lldi.rxq_ids[c_ctx(tfm)->rx_qidx],
 +			      &skb, CHCR_DECRYPT_OP);
  	if (err || !skb)
  		return err;
  	skb->dev = u_ctx->lldi.ports[0];
@@@ -1564,11 -1613,6 +1618,14 @@@ static int chcr_ahash_update(struct aha
  
  	bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));
  	u_ctx = ULD_CTX(h_ctx(rtfm));
++<<<<<<< HEAD
 +	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
 +					    h_ctx(rtfm)->tx_qidx))) {
 +		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 +			return -EBUSY;
 +	}
++=======
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  
  	if (nbytes + req_ctx->reqlen >= bs) {
  		remainder = (nbytes + req_ctx->reqlen) % bs;
@@@ -1619,9 -1680,11 +1693,11 @@@
  	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
  	chcr_send_wr(skb);
  
 -	return isfull ? -EBUSY : -EINPROGRESS;
 +	return -EINPROGRESS;
  unmap:
  	chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
+ err:
+ 	chcr_dec_wrcount(dev);
  	return error;
  }
  
@@@ -1703,8 -1781,11 +1794,16 @@@ static int chcr_ahash_finup(struct ahas
  
  	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
  					    h_ctx(rtfm)->tx_qidx))) {
++<<<<<<< HEAD
 +		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 +			return -EBUSY;
++=======
+ 		isfull = 1;
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+ 			error = -ENOSPC;
+ 			goto err;
+ 		}
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  	}
  	chcr_init_hctx_per_wr(req_ctx);
  	error = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);
@@@ -1763,9 -1846,11 +1864,11 @@@
  	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
  	chcr_send_wr(skb);
  
 -	return isfull ? -EBUSY : -EINPROGRESS;
 +	return -EINPROGRESS;
  unmap:
  	chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
+ err:
+ 	chcr_dec_wrcount(dev);
  	return error;
  }
  
@@@ -1785,8 -1874,11 +1892,16 @@@ static int chcr_ahash_digest(struct aha
  	u_ctx = ULD_CTX(h_ctx(rtfm));
  	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
  					    h_ctx(rtfm)->tx_qidx))) {
++<<<<<<< HEAD
 +		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 +			return -EBUSY;
++=======
+ 		isfull = 1;
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+ 			error = -ENOSPC;
+ 			goto err;
+ 		}
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  	}
  
  	chcr_init_hctx_per_wr(req_ctx);
@@@ -1842,9 -1936,11 +1959,11 @@@
  	skb->dev = u_ctx->lldi.ports[0];
  	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
  	chcr_send_wr(skb);
 -	return isfull ? -EBUSY : -EINPROGRESS;
 +	return -EINPROGRESS;
  unmap:
  	chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
+ err:
+ 	chcr_dec_wrcount(dev);
  	return error;
  }
  
@@@ -3558,10 -3643,14 +3679,17 @@@ static int chcr_aead_op(struct aead_req
  			create_wr_t create_wr_fn)
  {
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
  	struct uld_ctx *u_ctx;
  	struct sk_buff *skb;
- 
- 	if (!a_ctx(tfm)->dev) {
++<<<<<<< HEAD
++=======
+ 	int isfull = 0;
+ 	struct chcr_dev *cdev;
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
+ 
+ 	cdev = a_ctx(tfm)->dev;
+ 	if (!cdev) {
  		pr_err("chcr : %s : No crypto device.\n", __func__);
  		return -ENXIO;
  	}
@@@ -3568,8 -3665,11 +3704,16 @@@
  	u_ctx = ULD_CTX(a_ctx(tfm));
  	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
  				   a_ctx(tfm)->tx_qidx)) {
++<<<<<<< HEAD
 +		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
 +			return -EBUSY;
++=======
+ 		isfull = 1;
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+ 			chcr_dec_wrcount(cdev);
+ 			return -ENOSPC;
+ 		}
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  	}
  
  	/* Form a WR from req */
diff --cc drivers/crypto/chelsio/chcr_core.h
index 1e83b5dec34f,1159dee964ed..000000000000
--- a/drivers/crypto/chelsio/chcr_core.h
+++ b/drivers/crypto/chelsio/chcr_core.h
@@@ -73,6 -70,68 +70,71 @@@ struct _key_ctx 
  	unsigned char key[0];
  };
  
++<<<<<<< HEAD
++=======
+ #define KEYCTX_TX_WR_IV_S  55
+ #define KEYCTX_TX_WR_IV_M  0x1ffULL
+ #define KEYCTX_TX_WR_IV_V(x) ((x) << KEYCTX_TX_WR_IV_S)
+ #define KEYCTX_TX_WR_IV_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_IV_S) & KEYCTX_TX_WR_IV_M)
+ 
+ #define KEYCTX_TX_WR_AAD_S 47
+ #define KEYCTX_TX_WR_AAD_M 0xffULL
+ #define KEYCTX_TX_WR_AAD_V(x) ((x) << KEYCTX_TX_WR_AAD_S)
+ #define KEYCTX_TX_WR_AAD_G(x) (((x) >> KEYCTX_TX_WR_AAD_S) & \
+ 				KEYCTX_TX_WR_AAD_M)
+ 
+ #define KEYCTX_TX_WR_AADST_S 39
+ #define KEYCTX_TX_WR_AADST_M 0xffULL
+ #define KEYCTX_TX_WR_AADST_V(x) ((x) << KEYCTX_TX_WR_AADST_S)
+ #define KEYCTX_TX_WR_AADST_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_AADST_S) & KEYCTX_TX_WR_AADST_M)
+ 
+ #define KEYCTX_TX_WR_CIPHER_S 30
+ #define KEYCTX_TX_WR_CIPHER_M 0x1ffULL
+ #define KEYCTX_TX_WR_CIPHER_V(x) ((x) << KEYCTX_TX_WR_CIPHER_S)
+ #define KEYCTX_TX_WR_CIPHER_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_CIPHER_S) & KEYCTX_TX_WR_CIPHER_M)
+ 
+ #define KEYCTX_TX_WR_CIPHERST_S 23
+ #define KEYCTX_TX_WR_CIPHERST_M 0x7f
+ #define KEYCTX_TX_WR_CIPHERST_V(x) ((x) << KEYCTX_TX_WR_CIPHERST_S)
+ #define KEYCTX_TX_WR_CIPHERST_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_CIPHERST_S) & KEYCTX_TX_WR_CIPHERST_M)
+ 
+ #define KEYCTX_TX_WR_AUTH_S 14
+ #define KEYCTX_TX_WR_AUTH_M 0x1ff
+ #define KEYCTX_TX_WR_AUTH_V(x) ((x) << KEYCTX_TX_WR_AUTH_S)
+ #define KEYCTX_TX_WR_AUTH_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_AUTH_S) & KEYCTX_TX_WR_AUTH_M)
+ 
+ #define KEYCTX_TX_WR_AUTHST_S 7
+ #define KEYCTX_TX_WR_AUTHST_M 0x7f
+ #define KEYCTX_TX_WR_AUTHST_V(x) ((x) << KEYCTX_TX_WR_AUTHST_S)
+ #define KEYCTX_TX_WR_AUTHST_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_AUTHST_S) & KEYCTX_TX_WR_AUTHST_M)
+ 
+ #define KEYCTX_TX_WR_AUTHIN_S 0
+ #define KEYCTX_TX_WR_AUTHIN_M 0x7f
+ #define KEYCTX_TX_WR_AUTHIN_V(x) ((x) << KEYCTX_TX_WR_AUTHIN_S)
+ #define KEYCTX_TX_WR_AUTHIN_G(x) \
+ 	(((x) >> KEYCTX_TX_WR_AUTHIN_S) & KEYCTX_TX_WR_AUTHIN_M)
+ 
+ #define WQ_RETRY	5
+ struct chcr_driver_data {
+ 	struct list_head act_dev;
+ 	struct list_head inact_dev;
+ 	atomic_t dev_count;
+ 	struct mutex drv_mutex;
+ 	struct uld_ctx *last_dev;
+ };
+ 
+ enum chcr_state {
+ 	CHCR_INIT = 0,
+ 	CHCR_ATTACH,
+ 	CHCR_DETACH,
+ };
++>>>>>>> fef4912b66d6 (crypto: chelsio - Handle PCI shutdown event)
  struct chcr_wr {
  	struct fw_crypto_lookaside_wr wreq;
  	struct ulp_txpkt ulptx;
@@@ -83,17 -142,25 +145,21 @@@
  
  struct chcr_dev {
  	spinlock_t lock_chcr_dev;
- 	struct uld_ctx *u_ctx;
+ 	enum chcr_state state;
+ 	atomic_t inflight;
+ 	int wqretry;
+ 	struct delayed_work detach_work;
+ 	struct completion detach_comp;
  	unsigned char tx_channel_id;
 +	unsigned char rx_channel_id;
  };
  
  struct uld_ctx {
  	struct list_head entry;
  	struct cxgb4_lld_info lldi;
- 	struct chcr_dev *dev;
+ 	struct chcr_dev dev;
  };
  
 -struct sge_opaque_hdr {
 -	void *dev;
 -	dma_addr_t addr[MAX_SKB_FRAGS + 1];
 -};
 -
  struct chcr_ipsec_req {
  	struct ulp_txpkt ulptx;
  	struct ulptx_idata sc_imm;
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
diff --git a/drivers/crypto/chelsio/chcr_core.c b/drivers/crypto/chelsio/chcr_core.c
index 2c472e3c6aeb..f71a97939418 100644
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@ -26,10 +26,7 @@
 #include "chcr_core.h"
 #include "cxgb4_uld.h"
 
-static LIST_HEAD(uld_ctx_list);
-static DEFINE_MUTEX(dev_mutex);
-static atomic_t dev_count;
-static struct uld_ctx *ctx_rr;
+static struct chcr_driver_data drv_data;
 
 typedef int (*chcr_handler_func)(struct chcr_dev *dev, unsigned char *input);
 static int cpl_fw6_pld_handler(struct chcr_dev *dev, unsigned char *input);
@@ -53,6 +50,29 @@ static struct cxgb4_uld_info chcr_uld_info = {
 #endif /* CONFIG_CHELSIO_IPSEC_INLINE */
 };
 
+static void detach_work_fn(struct work_struct *work)
+{
+	struct chcr_dev *dev;
+
+	dev = container_of(work, struct chcr_dev, detach_work.work);
+
+	if (atomic_read(&dev->inflight)) {
+		dev->wqretry--;
+		if (dev->wqretry) {
+			pr_debug("Request Inflight Count %d\n",
+				atomic_read(&dev->inflight));
+
+			schedule_delayed_work(&dev->detach_work, WQ_DETACH_TM);
+		} else {
+			WARN(1, "CHCR:%d request Still Pending\n",
+				atomic_read(&dev->inflight));
+			complete(&dev->detach_comp);
+		}
+	} else {
+		complete(&dev->detach_comp);
+	}
+}
+
 struct uld_ctx *assign_chcr_device(void)
 {
 	struct uld_ctx *u_ctx = NULL;
@@ -63,56 +83,70 @@ struct uld_ctx *assign_chcr_device(void)
 	 * Although One session must use the same device to
 	 * maintain request-response ordering.
 	 */
-	mutex_lock(&dev_mutex);
-	if (!list_empty(&uld_ctx_list)) {
-		u_ctx = ctx_rr;
-		if (list_is_last(&ctx_rr->entry, &uld_ctx_list))
-			ctx_rr = list_first_entry(&uld_ctx_list,
-						  struct uld_ctx,
-						  entry);
+	mutex_lock(&drv_data.drv_mutex);
+	if (!list_empty(&drv_data.act_dev)) {
+		u_ctx = drv_data.last_dev;
+		if (list_is_last(&drv_data.last_dev->entry, &drv_data.act_dev))
+			drv_data.last_dev = list_first_entry(&drv_data.act_dev,
+						  struct uld_ctx, entry);
 		else
-			ctx_rr = list_next_entry(ctx_rr, entry);
+			drv_data.last_dev =
+				list_next_entry(drv_data.last_dev, entry);
 	}
-	mutex_unlock(&dev_mutex);
+	mutex_unlock(&drv_data.drv_mutex);
 	return u_ctx;
 }
 
-static int chcr_dev_add(struct uld_ctx *u_ctx)
+static void chcr_dev_add(struct uld_ctx *u_ctx)
 {
 	struct chcr_dev *dev;
 
-	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
-	if (!dev)
-		return -ENXIO;
+	dev = &u_ctx->dev;
+	dev->state = CHCR_ATTACH;
+	atomic_set(&dev->inflight, 0);
+	mutex_lock(&drv_data.drv_mutex);
+	list_move(&u_ctx->entry, &drv_data.act_dev);
+	if (!drv_data.last_dev)
+		drv_data.last_dev = u_ctx;
+	mutex_unlock(&drv_data.drv_mutex);
+}
 
+static void chcr_dev_init(struct uld_ctx *u_ctx)
+{
+	struct chcr_dev *dev;
+
+	dev = &u_ctx->dev;
 	spin_lock_init(&dev->lock_chcr_dev);
-	u_ctx->dev = dev;
-	dev->u_ctx = u_ctx;
-	atomic_inc(&dev_count);
-	mutex_lock(&dev_mutex);
-	list_add_tail(&u_ctx->entry, &uld_ctx_list);
-	if (!ctx_rr)
-		ctx_rr = u_ctx;
-	mutex_unlock(&dev_mutex);
-	return 0;
+	INIT_DELAYED_WORK(&dev->detach_work, detach_work_fn);
+	init_completion(&dev->detach_comp);
+	dev->state = CHCR_INIT;
+	dev->wqretry = WQ_RETRY;
+	atomic_inc(&drv_data.dev_count);
+	atomic_set(&dev->inflight, 0);
+	mutex_lock(&drv_data.drv_mutex);
+	list_add_tail(&u_ctx->entry, &drv_data.inact_dev);
+	if (!drv_data.last_dev)
+		drv_data.last_dev = u_ctx;
+	mutex_unlock(&drv_data.drv_mutex);
 }
 
-static int chcr_dev_remove(struct uld_ctx *u_ctx)
+static int chcr_dev_move(struct uld_ctx *u_ctx)
 {
-	if (ctx_rr == u_ctx) {
-		if (list_is_last(&ctx_rr->entry, &uld_ctx_list))
-			ctx_rr = list_first_entry(&uld_ctx_list,
-						  struct uld_ctx,
-						  entry);
+	 mutex_lock(&drv_data.drv_mutex);
+	if (drv_data.last_dev == u_ctx) {
+		if (list_is_last(&drv_data.last_dev->entry, &drv_data.act_dev))
+			drv_data.last_dev = list_first_entry(&drv_data.act_dev,
+						  struct uld_ctx, entry);
 		else
-			ctx_rr = list_next_entry(ctx_rr, entry);
+			drv_data.last_dev =
+				list_next_entry(drv_data.last_dev, entry);
 	}
-	list_del(&u_ctx->entry);
-	if (list_empty(&uld_ctx_list))
-		ctx_rr = NULL;
-	kfree(u_ctx->dev);
-	u_ctx->dev = NULL;
-	atomic_dec(&dev_count);
+	list_move(&u_ctx->entry, &drv_data.inact_dev);
+	if (list_empty(&drv_data.act_dev))
+		drv_data.last_dev = NULL;
+	atomic_dec(&drv_data.dev_count);
+	mutex_unlock(&drv_data.drv_mutex);
+
 	return 0;
 }
 
@@ -167,6 +201,7 @@ static void *chcr_uld_add(const struct cxgb4_lld_info *lld)
 		goto out;
 	}
 	u_ctx->lldi = *lld;
+	chcr_dev_init(u_ctx);
 #ifdef CONFIG_CHELSIO_IPSEC_INLINE
 	if (lld->crypto & ULP_CRYPTO_IPSEC_INLINE)
 		chcr_add_xfrmops(lld);
@@ -179,7 +214,7 @@ int chcr_uld_rx_handler(void *handle, const __be64 *rsp,
 			const struct pkt_gl *pgl)
 {
 	struct uld_ctx *u_ctx = (struct uld_ctx *)handle;
-	struct chcr_dev *dev = u_ctx->dev;
+	struct chcr_dev *dev = &u_ctx->dev;
 	const struct cpl_fw6_pld *rpl = (struct cpl_fw6_pld *)rsp;
 
 	if (rpl->opcode != CPL_FW6_PLD) {
@@ -201,6 +236,28 @@ int chcr_uld_tx_handler(struct sk_buff *skb, struct net_device *dev)
 }
 #endif /* CONFIG_CHELSIO_IPSEC_INLINE */
 
+static void chcr_detach_device(struct uld_ctx *u_ctx)
+{
+	struct chcr_dev *dev = &u_ctx->dev;
+
+	spin_lock_bh(&dev->lock_chcr_dev);
+	if (dev->state == CHCR_DETACH) {
+		spin_unlock_bh(&dev->lock_chcr_dev);
+		pr_debug("Detached Event received for already detach device\n");
+		return;
+	}
+	dev->state = CHCR_DETACH;
+	spin_unlock_bh(&dev->lock_chcr_dev);
+
+	if (atomic_read(&dev->inflight) != 0) {
+		schedule_delayed_work(&dev->detach_work, WQ_DETACH_TM);
+		wait_for_completion(&dev->detach_comp);
+	}
+
+	// Move u_ctx to inactive_dev list
+	chcr_dev_move(u_ctx);
+}
+
 static int chcr_uld_state_change(void *handle, enum cxgb4_state state)
 {
 	struct uld_ctx *u_ctx = handle;
@@ -208,23 +265,16 @@ static int chcr_uld_state_change(void *handle, enum cxgb4_state state)
 
 	switch (state) {
 	case CXGB4_STATE_UP:
-		if (!u_ctx->dev) {
-			ret = chcr_dev_add(u_ctx);
-			if (ret != 0)
-				return ret;
+		if (u_ctx->dev.state != CHCR_INIT) {
+			// ALready Initialised.
+			return 0;
 		}
-		if (atomic_read(&dev_count) == 1)
-			ret = start_crypto();
+		chcr_dev_add(u_ctx);
+		ret = start_crypto();
 		break;
 
 	case CXGB4_STATE_DETACH:
-		if (u_ctx->dev) {
-			mutex_lock(&dev_mutex);
-			chcr_dev_remove(u_ctx);
-			mutex_unlock(&dev_mutex);
-		}
-		if (!atomic_read(&dev_count))
-			stop_crypto();
+		chcr_detach_device(u_ctx);
 		break;
 
 	case CXGB4_STATE_START_RECOVERY:
@@ -237,7 +287,13 @@ static int chcr_uld_state_change(void *handle, enum cxgb4_state state)
 
 static int __init chcr_crypto_init(void)
 {
+	INIT_LIST_HEAD(&drv_data.act_dev);
+	INIT_LIST_HEAD(&drv_data.inact_dev);
+	atomic_set(&drv_data.dev_count, 0);
+	mutex_init(&drv_data.drv_mutex);
+	drv_data.last_dev = NULL;
 	cxgb4_register_uld(CXGB4_ULD_CRYPTO, &chcr_uld_info);
+
 	return 0;
 }
 
@@ -245,18 +301,20 @@ static void __exit chcr_crypto_exit(void)
 {
 	struct uld_ctx *u_ctx, *tmp;
 
-	if (atomic_read(&dev_count))
-		stop_crypto();
+	stop_crypto();
 
+	cxgb4_unregister_uld(CXGB4_ULD_CRYPTO);
 	/* Remove all devices from list */
-	mutex_lock(&dev_mutex);
-	list_for_each_entry_safe(u_ctx, tmp, &uld_ctx_list, entry) {
-		if (u_ctx->dev)
-			chcr_dev_remove(u_ctx);
+	mutex_lock(&drv_data.drv_mutex);
+	list_for_each_entry_safe(u_ctx, tmp, &drv_data.act_dev, entry) {
+		list_del(&u_ctx->entry);
 		kfree(u_ctx);
 	}
-	mutex_unlock(&dev_mutex);
-	cxgb4_unregister_uld(CXGB4_ULD_CRYPTO);
+	list_for_each_entry_safe(u_ctx, tmp, &drv_data.inact_dev, entry) {
+		list_del(&u_ctx->entry);
+		kfree(u_ctx);
+	}
+	mutex_unlock(&drv_data.drv_mutex);
 }
 
 module_init(chcr_crypto_init);
* Unmerged path drivers/crypto/chelsio/chcr_core.h
