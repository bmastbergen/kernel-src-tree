net/mlx5e: Cleanup attach encap function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Cleanup attach encap function (Alaa Hleihel) [1695493]
Rebuild_FUZZ: 94.74%
commit-author Roi Dayan <roid@mellanox.com>
commit 733d4f367c34c971ce2d4f466d71ae5a189cbe5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/733d4f36.failed

Remove the tunnel info argument which we can get from the other args.
Also reorder the args to have input args first and output args later.

This patch doesn't change functionality.

	Signed-off-by: Roi Dayan <roid@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 733d4f367c34c971ce2d4f466d71ae5a189cbe5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 5862f33b4ba7,7a363e44ec45..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -806,41 -847,138 +806,59 @@@ static void mlx5e_tc_del_nic_flow(struc
  }
  
  static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 -			       struct mlx5e_tc_flow *flow, int out_index);
 +			       struct mlx5e_tc_flow *flow);
  
  static int mlx5e_attach_encap(struct mlx5e_priv *priv,
++<<<<<<< HEAD
 +			      struct ip_tunnel_info *tun_info,
 +			      struct net_device *mirred_dev,
 +			      struct net_device **encap_dev,
 +			      struct mlx5e_tc_flow *flow);
++=======
+ 			      struct mlx5e_tc_flow *flow,
+ 			      struct net_device *mirred_dev,
+ 			      int out_index,
+ 			      struct netlink_ext_ack *extack,
+ 			      struct net_device **encap_dev);
++>>>>>>> 733d4f367c34 (net/mlx5e: Cleanup attach encap function)
  
  static struct mlx5_flow_handle *
 -mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
 -			   struct mlx5e_tc_flow *flow,
 -			   struct mlx5_flow_spec *spec,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	rule = mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
 -	if (IS_ERR(rule))
 -		return rule;
 -
 -	if (attr->split_count) {
 -		flow->rule[1] = mlx5_eswitch_add_fwd_rule(esw, spec, attr);
 -		if (IS_ERR(flow->rule[1])) {
 -			mlx5_eswitch_del_offloaded_rule(esw, rule, attr);
 -			return flow->rule[1];
 -		}
 -	}
 -
 -	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
 -			     struct mlx5e_tc_flow *flow,
 -			   struct mlx5_esw_flow_attr *attr)
 -{
 -	flow->flags &= ~MLX5E_TC_FLOW_OFFLOADED;
 -
 -	if (attr->split_count)
 -		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
 -
 -	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 -}
 -
 -static struct mlx5_flow_handle *
 -mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct mlx5_flow_spec *spec,
 -			      struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	struct mlx5_flow_handle *rule;
 -
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 -	slow_attr->split_count = 0;
 -	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN;
 -
 -	rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, slow_attr);
 -	if (!IS_ERR(rule))
 -		flow->flags |= MLX5E_TC_FLOW_SLOW;
 -
 -	return rule;
 -}
 -
 -static void
 -mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
 -				  struct mlx5e_tc_flow *flow,
 -				  struct mlx5_esw_flow_attr *slow_attr)
 -{
 -	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 -	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 -	slow_attr->split_count = 0;
 -	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN;
 -	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
 -	flow->flags &= ~MLX5E_TC_FLOW_SLOW;
 -}
 -
 -static int
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	u32 max_chain = mlx5_eswitch_get_chain_range(esw);
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr = attr->parse_attr;
 -	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
  	struct net_device *out_dev, *encap_dev = NULL;
 -	struct mlx5_fc *counter = NULL;
 +	struct mlx5_flow_handle *rule = NULL;
  	struct mlx5e_rep_priv *rpriv;
  	struct mlx5e_priv *out_priv;
 -	int err = 0, encap_err = 0;
 -	int out_index;
 -
 -	if (!mlx5_eswitch_prios_supported(esw) && attr->prio != 1) {
 -		NL_SET_ERR_MSG(extack, "E-switch priorities unsupported, upgrade FW");
 -		return -EOPNOTSUPP;
 -	}
 -
 -	if (attr->chain > max_chain) {
 -		NL_SET_ERR_MSG(extack, "Requested chain is out of supported range");
 -		err = -EOPNOTSUPP;
 -		goto err_max_prio_chain;
 -	}
 -
 -	if (attr->prio > max_prio) {
 -		NL_SET_ERR_MSG(extack, "Requested priority is out of supported range");
 -		err = -EOPNOTSUPP;
 -		goto err_max_prio_chain;
 -	}
 -
 -	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {
 -		int mirred_ifindex;
 -
 -		if (!(attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP))
 -			continue;
 +	int err;
  
 -		mirred_ifindex = parse_attr->mirred_ifindex[out_index];
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP) {
  		out_dev = __dev_get_by_index(dev_net(priv->netdev),
++<<<<<<< HEAD
 +					     attr->parse_attr->mirred_ifindex);
 +		err = mlx5e_attach_encap(priv, &parse_attr->tun_info,
 +					 out_dev, &encap_dev, flow);
 +		if (err) {
 +			rule = ERR_PTR(err);
 +			if (err != -EAGAIN)
 +				goto err_attach_encap;
 +		}
++=======
+ 					     mirred_ifindex);
+ 		err = mlx5e_attach_encap(priv, flow, out_dev, out_index,
+ 					 extack, &encap_dev);
+ 		if (err && err != -EAGAIN)
+ 			goto err_attach_encap;
+ 		if (err == -EAGAIN)
+ 			encap_err = err;
++>>>>>>> 733d4f367c34 (net/mlx5e: Cleanup attach encap function)
  		out_priv = netdev_priv(encap_dev);
  		rpriv = out_priv->ppriv;
 -		attr->dests[out_index].rep = rpriv->rep;
 -		attr->dests[out_index].mdev = out_priv->mdev;
 +		attr->out_rep = rpriv->rep;
 +		attr->out_mdev = out_priv->mdev;
  	}
  
  	err = mlx5_eswitch_add_vlan_action(esw, attr);
@@@ -2119,424 -2321,46 +2137,445 @@@ static inline int hash_encap_info(struc
  	return jhash(key, sizeof(*key), 0);
  }
  
 -
 -static bool is_merged_eswitch_dev(struct mlx5e_priv *priv,
 -				  struct net_device *peer_netdev)
 +static int mlx5e_route_lookup_ipv4(struct mlx5e_priv *priv,
 +				   struct net_device *mirred_dev,
 +				   struct net_device **out_dev,
 +				   struct flowi4 *fl4,
 +				   struct neighbour **out_n,
 +				   u8 *out_ttl)
  {
 -	struct mlx5e_priv *peer_priv;
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5e_rep_priv *uplink_rpriv;
 +	struct rtable *rt;
 +	struct neighbour *n = NULL;
  
 -	peer_priv = netdev_priv(peer_netdev);
 +#if IS_ENABLED(CONFIG_INET)
 +	int ret;
  
 -	return (MLX5_CAP_ESW(priv->mdev, merged_eswitch) &&
 -		(priv->netdev->netdev_ops == peer_netdev->netdev_ops) &&
 -		same_hw_devs(priv, peer_priv) &&
 -		MLX5_VPORT_MANAGER(peer_priv->mdev) &&
 -		(peer_priv->mdev->priv.eswitch->mode == SRIOV_OFFLOADS));
 -}
 +	rt = ip_route_output_key(dev_net(mirred_dev), fl4);
 +	ret = PTR_ERR_OR_ZERO(rt);
 +	if (ret)
 +		return ret;
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 +	/* if the egress device isn't on the same HW e-switch, we use the uplink */
 +	if (!switchdev_port_same_parent_id(priv->netdev, rt->dst.dev))
 +		*out_dev = uplink_rpriv->netdev;
 +	else
 +		*out_dev = rt->dst.dev;
  
 +	if (!(*out_ttl))
 +		*out_ttl = ip4_dst_hoplimit(&rt->dst);
 +	n = dst_neigh_lookup(&rt->dst, &fl4->daddr);
 +	ip_rt_put(rt);
 +	if (!n)
 +		return -ENOMEM;
  
 +	*out_n = n;
  
 -static int mlx5e_attach_encap(struct mlx5e_priv *priv,
 -			      struct mlx5e_tc_flow *flow,
 -			      struct net_device *mirred_dev,
 -			      int out_index,
 -			      struct netlink_ext_ack *extack,
 -			      struct net_device **encap_dev)
 +	return 0;
 +}
 +
 +static int mlx5e_route_lookup_ipv6(struct mlx5e_priv *priv,
 +				   struct net_device *mirred_dev,
 +				   struct net_device **out_dev,
 +				   struct flowi6 *fl6,
 +				   struct neighbour **out_n,
 +				   u8 *out_ttl)
  {
 +	struct neighbour *n = NULL;
 +	struct dst_entry *dst;
 +
 +#if IS_ENABLED(CONFIG_INET) && IS_ENABLED(CONFIG_IPV6)
 +	struct mlx5e_rep_priv *uplink_rpriv;
  	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 -	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 -	struct mlx5e_tc_flow_parse_attr *parse_attr;
 -	struct ip_tunnel_info *tun_info;
 -	struct ip_tunnel_key *key;
 +	int ret;
 +
 +	ret = ipv6_stub->ipv6_dst_lookup(dev_net(mirred_dev), NULL, &dst,
 +					 fl6);
 +	if (ret < 0)
 +		return ret;
 +
 +	if (!(*out_ttl))
 +		*out_ttl = ip6_dst_hoplimit(dst);
 +
 +	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 +	/* if the egress device isn't on the same HW e-switch, we use the uplink */
 +	if (!switchdev_port_same_parent_id(priv->netdev, dst->dev))
 +		*out_dev = uplink_rpriv->netdev;
 +	else
 +		*out_dev = dst->dev;
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +
 +	n = dst_neigh_lookup(dst, &fl6->daddr);
 +	dst_release(dst);
 +	if (!n)
 +		return -ENOMEM;
 +
 +	*out_n = n;
 +	return 0;
 +}
 +
 +static void gen_vxlan_header_ipv4(struct net_device *out_dev,
 +				  char buf[], int encap_size,
 +				  unsigned char h_dest[ETH_ALEN],
 +				  u8 tos, u8 ttl,
 +				  __be32 daddr,
 +				  __be32 saddr,
 +				  __be16 udp_dst_port,
 +				  __be32 vx_vni)
 +{
 +	struct ethhdr *eth = (struct ethhdr *)buf;
 +	struct iphdr  *ip = (struct iphdr *)((char *)eth + sizeof(struct ethhdr));
 +	struct udphdr *udp = (struct udphdr *)((char *)ip + sizeof(struct iphdr));
 +	struct vxlanhdr *vxh = (struct vxlanhdr *)((char *)udp + sizeof(struct udphdr));
 +
 +	memset(buf, 0, encap_size);
 +
 +	ether_addr_copy(eth->h_dest, h_dest);
 +	ether_addr_copy(eth->h_source, out_dev->dev_addr);
 +	eth->h_proto = htons(ETH_P_IP);
 +
 +	ip->daddr = daddr;
 +	ip->saddr = saddr;
 +
 +	ip->tos = tos;
 +	ip->ttl = ttl;
 +	ip->protocol = IPPROTO_UDP;
 +	ip->version = 0x4;
 +	ip->ihl = 0x5;
 +
 +	udp->dest = udp_dst_port;
 +	vxh->vx_flags = VXLAN_HF_VNI;
 +	vxh->vx_vni = vxlan_vni_field(vx_vni);
 +}
 +
 +static void gen_vxlan_header_ipv6(struct net_device *out_dev,
 +				  char buf[], int encap_size,
 +				  unsigned char h_dest[ETH_ALEN],
 +				  u8 tos, u8 ttl,
 +				  struct in6_addr *daddr,
 +				  struct in6_addr *saddr,
 +				  __be16 udp_dst_port,
 +				  __be32 vx_vni)
 +{
 +	struct ethhdr *eth = (struct ethhdr *)buf;
 +	struct ipv6hdr *ip6h = (struct ipv6hdr *)((char *)eth + sizeof(struct ethhdr));
 +	struct udphdr *udp = (struct udphdr *)((char *)ip6h + sizeof(struct ipv6hdr));
 +	struct vxlanhdr *vxh = (struct vxlanhdr *)((char *)udp + sizeof(struct udphdr));
 +
 +	memset(buf, 0, encap_size);
 +
 +	ether_addr_copy(eth->h_dest, h_dest);
 +	ether_addr_copy(eth->h_source, out_dev->dev_addr);
 +	eth->h_proto = htons(ETH_P_IPV6);
 +
 +	ip6_flow_hdr(ip6h, tos, 0);
 +	/* the HW fills up ipv6 payload len */
 +	ip6h->nexthdr     = IPPROTO_UDP;
 +	ip6h->hop_limit   = ttl;
 +	ip6h->daddr	  = *daddr;
 +	ip6h->saddr	  = *saddr;
 +
 +	udp->dest = udp_dst_port;
 +	vxh->vx_flags = VXLAN_HF_VNI;
 +	vxh->vx_vni = vxlan_vni_field(vx_vni);
 +}
 +
 +static int mlx5e_create_encap_header_ipv4(struct mlx5e_priv *priv,
 +					  struct net_device *mirred_dev,
 +					  struct mlx5e_encap_entry *e)
 +{
 +	int max_encap_size = MLX5_CAP_ESW(priv->mdev, max_encap_header_size);
 +	int ipv4_encap_size = ETH_HLEN + sizeof(struct iphdr) + VXLAN_HLEN;
 +	struct ip_tunnel_key *tun_key = &e->tun_info.key;
 +	struct net_device *out_dev;
 +	struct neighbour *n = NULL;
 +	struct flowi4 fl4 = {};
 +	u8 nud_state, tos, ttl;
 +	char *encap_header;
 +	int err;
 +
 +	if (max_encap_size < ipv4_encap_size) {
 +		mlx5_core_warn(priv->mdev, "encap size %d too big, max supported is %d\n",
 +			       ipv4_encap_size, max_encap_size);
 +		return -EOPNOTSUPP;
 +	}
 +
 +	encap_header = kzalloc(ipv4_encap_size, GFP_KERNEL);
 +	if (!encap_header)
 +		return -ENOMEM;
 +
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		fl4.flowi4_proto = IPPROTO_UDP;
 +		fl4.fl4_dport = tun_key->tp_dst;
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
 +		goto free_encap;
 +	}
 +
 +	tos = tun_key->tos;
 +	ttl = tun_key->ttl;
 +
 +	fl4.flowi4_tos = tun_key->tos;
 +	fl4.daddr = tun_key->u.ipv4.dst;
 +	fl4.saddr = tun_key->u.ipv4.src;
 +
 +	err = mlx5e_route_lookup_ipv4(priv, mirred_dev, &out_dev,
 +				      &fl4, &n, &ttl);
 +	if (err)
 +		goto free_encap;
 +
 +	/* used by mlx5e_detach_encap to lookup a neigh hash table
 +	 * entry in the neigh hash table when a user deletes a rule
 +	 */
 +	e->m_neigh.dev = n->dev;
 +	e->m_neigh.family = n->ops->family;
 +	memcpy(&e->m_neigh.dst_ip, n->primary_key, n->tbl->key_len);
 +	e->out_dev = out_dev;
 +
 +	/* It's importent to add the neigh to the hash table before checking
 +	 * the neigh validity state. So if we'll get a notification, in case the
 +	 * neigh changes it's validity state, we would find the relevant neigh
 +	 * in the hash.
 +	 */
 +	err = mlx5e_rep_encap_entry_attach(netdev_priv(out_dev), e);
 +	if (err)
 +		goto free_encap;
 +
 +	read_lock_bh(&n->lock);
 +	nud_state = n->nud_state;
 +	ether_addr_copy(e->h_dest, n->ha);
 +	read_unlock_bh(&n->lock);
 +
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		gen_vxlan_header_ipv4(out_dev, encap_header,
 +				      ipv4_encap_size, e->h_dest, tos, ttl,
 +				      fl4.daddr,
 +				      fl4.saddr, tun_key->tp_dst,
 +				      tunnel_id_to_key32(tun_key->tun_id));
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
 +		goto destroy_neigh_entry;
 +	}
 +	e->encap_size = ipv4_encap_size;
 +	e->encap_header = encap_header;
 +
 +	if (!(nud_state & NUD_VALID)) {
 +		neigh_event_send(n, NULL);
 +		err = -EAGAIN;
 +		goto out;
 +	}
 +
 +	err = mlx5_encap_alloc(priv->mdev, e->tunnel_type,
 +			       ipv4_encap_size, encap_header, &e->encap_id);
 +	if (err)
 +		goto destroy_neigh_entry;
 +
 +	e->flags |= MLX5_ENCAP_ENTRY_VALID;
 +	mlx5e_rep_queue_neigh_stats_work(netdev_priv(out_dev));
 +	neigh_release(n);
 +	return err;
 +
 +destroy_neigh_entry:
 +	mlx5e_rep_encap_entry_detach(netdev_priv(e->out_dev), e);
 +free_encap:
 +	kfree(encap_header);
 +out:
 +	if (n)
 +		neigh_release(n);
 +	return err;
 +}
 +
 +static int mlx5e_create_encap_header_ipv6(struct mlx5e_priv *priv,
 +					  struct net_device *mirred_dev,
 +					  struct mlx5e_encap_entry *e)
 +{
 +	int max_encap_size = MLX5_CAP_ESW(priv->mdev, max_encap_header_size);
 +	int ipv6_encap_size = ETH_HLEN + sizeof(struct ipv6hdr) + VXLAN_HLEN;
 +	struct ip_tunnel_key *tun_key = &e->tun_info.key;
 +	struct net_device *out_dev;
 +	struct neighbour *n = NULL;
 +	struct flowi6 fl6 = {};
 +	u8 nud_state, tos, ttl;
 +	char *encap_header;
 +	int err;
 +
 +	if (max_encap_size < ipv6_encap_size) {
 +		mlx5_core_warn(priv->mdev, "encap size %d too big, max supported is %d\n",
 +			       ipv6_encap_size, max_encap_size);
 +		return -EOPNOTSUPP;
 +	}
 +
 +	encap_header = kzalloc(ipv6_encap_size, GFP_KERNEL);
 +	if (!encap_header)
 +		return -ENOMEM;
 +
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		fl6.flowi6_proto = IPPROTO_UDP;
 +		fl6.fl6_dport = tun_key->tp_dst;
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
 +		goto free_encap;
 +	}
 +
 +	tos = tun_key->tos;
 +	ttl = tun_key->ttl;
 +
 +	fl6.flowlabel = ip6_make_flowinfo(RT_TOS(tun_key->tos), tun_key->label);
 +	fl6.daddr = tun_key->u.ipv6.dst;
 +	fl6.saddr = tun_key->u.ipv6.src;
 +
 +	err = mlx5e_route_lookup_ipv6(priv, mirred_dev, &out_dev,
 +				      &fl6, &n, &ttl);
 +	if (err)
 +		goto free_encap;
 +
 +	/* used by mlx5e_detach_encap to lookup a neigh hash table
 +	 * entry in the neigh hash table when a user deletes a rule
 +	 */
 +	e->m_neigh.dev = n->dev;
 +	e->m_neigh.family = n->ops->family;
 +	memcpy(&e->m_neigh.dst_ip, n->primary_key, n->tbl->key_len);
 +	e->out_dev = out_dev;
 +
 +	/* It's importent to add the neigh to the hash table before checking
 +	 * the neigh validity state. So if we'll get a notification, in case the
 +	 * neigh changes it's validity state, we would find the relevant neigh
 +	 * in the hash.
 +	 */
 +	err = mlx5e_rep_encap_entry_attach(netdev_priv(out_dev), e);
 +	if (err)
 +		goto free_encap;
 +
 +	read_lock_bh(&n->lock);
 +	nud_state = n->nud_state;
 +	ether_addr_copy(e->h_dest, n->ha);
 +	read_unlock_bh(&n->lock);
 +
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		gen_vxlan_header_ipv6(out_dev, encap_header,
 +				      ipv6_encap_size, e->h_dest, tos, ttl,
 +				      &fl6.daddr,
 +				      &fl6.saddr, tun_key->tp_dst,
 +				      tunnel_id_to_key32(tun_key->tun_id));
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
 +		goto destroy_neigh_entry;
 +	}
 +
 +	e->encap_size = ipv6_encap_size;
 +	e->encap_header = encap_header;
 +
 +	if (!(nud_state & NUD_VALID)) {
 +		neigh_event_send(n, NULL);
 +		err = -EAGAIN;
 +		goto out;
 +	}
 +
 +	err = mlx5_encap_alloc(priv->mdev, e->tunnel_type,
 +			       ipv6_encap_size, encap_header, &e->encap_id);
 +	if (err)
 +		goto destroy_neigh_entry;
 +
 +	e->flags |= MLX5_ENCAP_ENTRY_VALID;
 +	mlx5e_rep_queue_neigh_stats_work(netdev_priv(out_dev));
 +	neigh_release(n);
 +	return err;
 +
 +destroy_neigh_entry:
 +	mlx5e_rep_encap_entry_detach(netdev_priv(e->out_dev), e);
 +free_encap:
 +	kfree(encap_header);
 +out:
 +	if (n)
 +		neigh_release(n);
 +	return err;
 +}
 +
 +bool mlx5e_tc_tun_device_to_offload(struct mlx5e_priv *priv,
 +				    struct net_device *netdev)
 +{
 +	if (netif_is_vxlan(netdev) &&
 +	    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap))
 +		return true;
 +
 +	return false;
 +}
 +
 +static int mlx5e_attach_encap(struct mlx5e_priv *priv,
++<<<<<<< HEAD
 +			      struct ip_tunnel_info *tun_info,
 +			      struct net_device *mirred_dev,
 +			      struct net_device **encap_dev,
 +			      struct mlx5e_tc_flow *flow)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5e_rep_priv *uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw,
 +									   REP_ETH);
 +	struct net_device *up_dev = uplink_rpriv->netdev;
 +	unsigned short family = ip_tunnel_info_af(tun_info);
 +	struct mlx5e_priv *up_priv = netdev_priv(up_dev);
++=======
++			      struct mlx5e_tc_flow *flow,
++			      struct net_device *mirred_dev,
++			      int out_index,
++			      struct netlink_ext_ack *extack,
++			      struct net_device **encap_dev)
++{
++	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
++>>>>>>> 733d4f367c34 (net/mlx5e: Cleanup attach encap function)
 +	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
- 	struct ip_tunnel_key *key = &tun_info->key;
++	struct mlx5e_tc_flow_parse_attr *parse_attr;
++	struct ip_tunnel_info *tun_info;
++	struct ip_tunnel_key *key;
  	struct mlx5e_encap_entry *e;
++<<<<<<< HEAD
 +	int tunnel_type, err = 0;
++=======
+ 	unsigned short family;
++>>>>>>> 733d4f367c34 (net/mlx5e: Cleanup attach encap function)
  	uintptr_t hash_key;
  	bool found = false;
 -	int err = 0;
 +
 +	/* udp dst port must be set */
 +	if (!memchr_inv(&key->tp_dst, 0, sizeof(key->tp_dst)))
 +		goto vxlan_encap_offload_err;
 +
 +	/* setting udp src port isn't supported */
 +	if (memchr_inv(&key->tp_src, 0, sizeof(key->tp_src))) {
 +vxlan_encap_offload_err:
 +		netdev_warn(priv->netdev,
 +			    "must set udp dst port and not set udp src port\n");
 +		return -EOPNOTSUPP;
 +	}
 +
 +	if (mlx5e_vxlan_lookup_port(up_priv, be16_to_cpu(key->tp_dst)) &&
 +	    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap)) {
 +		tunnel_type = MLX5_HEADER_TYPE_VXLAN;
 +	} else {
 +		netdev_warn(priv->netdev,
 +			    "%d isn't an offloaded vxlan udp dport\n", be16_to_cpu(key->tp_dst));
 +		return -EOPNOTSUPP;
 +	}
  
+ 	parse_attr = attr->parse_attr;
+ 	tun_info = &parse_attr->tun_info[out_index];
+ 	family = ip_tunnel_info_af(tun_info);
+ 	key = &tun_info->key;
+ 
  	hash_key = hash_encap_info(key);
  
  	hash_for_each_possible_rcu(esw->offloads.encap_tbl, e,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
