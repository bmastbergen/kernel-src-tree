uio_hv_generic: defer opening vmbus until first use

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Stephen Hemminger <stephen@networkplumber.org>
commit cdfa835c6e5e87d145f9f632b58843de97509f2b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/cdfa835c.failed

This fixes two design flaws in hv_uio_generic.

Since hv_uio_probe is called from vmbus_probe with lock held
it potentially can cause sleep in an atomic section because
vmbus_open will wait for response from host.

The hv_uio_generic driver could not handle applications
exiting and restarting because the vmbus channel was
persistent.  Change the semantics so that the buffers are
allocated on probe, but not attached to host until
device is opened.

	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
(cherry picked from commit cdfa835c6e5e87d145f9f632b58843de97509f2b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/uio/uio_hv_generic.c
diff --cc drivers/uio/uio_hv_generic.c
index d608865b1e1e,c2493d011225..000000000000
--- a/drivers/uio/uio_hv_generic.c
+++ b/drivers/uio/uio_hv_generic.c
@@@ -54,6 -55,15 +54,18 @@@ enum hv_uio_map 
  struct hv_uio_private_data {
  	struct uio_info info;
  	struct hv_device *device;
++<<<<<<< HEAD
++=======
+ 	atomic_t refcnt;
+ 
+ 	void	*recv_buf;
+ 	u32	recv_gpadl;
+ 	char	recv_name[32];	/* "recv_4294967295" */
+ 
+ 	void	*send_buf;
+ 	u32	send_gpadl;
+ 	char	send_name[32];
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  };
  
  /*
@@@ -91,6 -102,134 +103,137 @@@ static void hv_uio_channel_cb(void *con
  	uio_event_notify(&pdata->info);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Callback from vmbus_event when channel is rescinded.
+  */
+ static void hv_uio_rescind(struct vmbus_channel *channel)
+ {
+ 	struct hv_device *hv_dev = channel->primary_channel->device_obj;
+ 	struct hv_uio_private_data *pdata = hv_get_drvdata(hv_dev);
+ 
+ 	/*
+ 	 * Turn off the interrupt file handle
+ 	 * Next read for event will return -EIO
+ 	 */
+ 	pdata->info.irq = 0;
+ 
+ 	/* Wake up reader */
+ 	uio_event_notify(&pdata->info);
+ }
+ 
+ /* Sysfs API to allow mmap of the ring buffers
+  * The ring buffer is allocated as contiguous memory by vmbus_open
+  */
+ static int hv_uio_ring_mmap(struct file *filp, struct kobject *kobj,
+ 			    struct bin_attribute *attr,
+ 			    struct vm_area_struct *vma)
+ {
+ 	struct vmbus_channel *channel
+ 		= container_of(kobj, struct vmbus_channel, kobj);
+ 	void *ring_buffer = page_address(channel->ringbuffer_page);
+ 
+ 	if (channel->state != CHANNEL_OPENED_STATE)
+ 		return -ENODEV;
+ 
+ 	return vm_iomap_memory(vma, virt_to_phys(ring_buffer),
+ 			       channel->ringbuffer_pagecount << PAGE_SHIFT);
+ }
+ 
+ static const struct bin_attribute ring_buffer_bin_attr = {
+ 	.attr = {
+ 		.name = "ring",
+ 		.mode = 0600,
+ 	},
+ 	.size = 2 * HV_RING_SIZE * PAGE_SIZE,
+ 	.mmap = hv_uio_ring_mmap,
+ };
+ 
+ /* Callback from VMBUS subsystem when new channel created. */
+ static void
+ hv_uio_new_channel(struct vmbus_channel *new_sc)
+ {
+ 	struct hv_device *hv_dev = new_sc->primary_channel->device_obj;
+ 	struct device *device = &hv_dev->device;
+ 	const size_t ring_bytes = HV_RING_SIZE * PAGE_SIZE;
+ 	int ret;
+ 
+ 	/* Create host communication ring */
+ 	ret = vmbus_open(new_sc, ring_bytes, ring_bytes, NULL, 0,
+ 			 hv_uio_channel_cb, new_sc);
+ 	if (ret) {
+ 		dev_err(device, "vmbus_open subchannel failed: %d\n", ret);
+ 		return;
+ 	}
+ 
+ 	/* Disable interrupts on sub channel */
+ 	new_sc->inbound.ring_buffer->interrupt_mask = 1;
+ 	set_channel_read_mode(new_sc, HV_CALL_ISR);
+ 
+ 	ret = sysfs_create_bin_file(&new_sc->kobj, &ring_buffer_bin_attr);
+ 	if (ret) {
+ 		dev_err(device, "sysfs create ring bin file failed; %d\n", ret);
+ 		vmbus_close(new_sc);
+ 	}
+ }
+ 
+ /* free the reserved buffers for send and receive */
+ static void
+ hv_uio_cleanup(struct hv_device *dev, struct hv_uio_private_data *pdata)
+ {
+ 	if (pdata->send_gpadl) {
+ 		vmbus_teardown_gpadl(dev->channel, pdata->send_gpadl);
+ 		pdata->send_gpadl = 0;
+ 		vfree(pdata->send_buf);
+ 	}
+ 
+ 	if (pdata->recv_gpadl) {
+ 		vmbus_teardown_gpadl(dev->channel, pdata->recv_gpadl);
+ 		pdata->recv_gpadl = 0;
+ 		vfree(pdata->recv_buf);
+ 	}
+ }
+ 
+ /* VMBus primary channel is opened on first use */
+ static int
+ hv_uio_open(struct uio_info *info, struct inode *inode)
+ {
+ 	struct hv_uio_private_data *pdata
+ 		= container_of(info, struct hv_uio_private_data, info);
+ 	struct hv_device *dev = pdata->device;
+ 	int ret;
+ 
+ 	if (atomic_inc_return(&pdata->refcnt) != 1)
+ 		return 0;
+ 
+ 	ret = vmbus_connect_ring(dev->channel,
+ 				 hv_uio_channel_cb, dev->channel);
+ 
+ 	if (ret == 0)
+ 		dev->channel->inbound.ring_buffer->interrupt_mask = 1;
+ 	else
+ 		atomic_dec(&pdata->refcnt);
+ 
+ 	return ret;
+ }
+ 
+ /* VMBus primary channel is closed on last close */
+ static int
+ hv_uio_release(struct uio_info *info, struct inode *inode)
+ {
+ 	struct hv_uio_private_data *pdata
+ 		= container_of(info, struct hv_uio_private_data, info);
+ 	struct hv_device *dev = pdata->device;
+ 	int ret = 0;
+ 
+ 	if (atomic_dec_and_test(&pdata->refcnt))
+ 		ret = vmbus_disconnect_ring(dev->channel);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  static int
  hv_uio_probe(struct hv_device *dev,
  	     const struct hv_vmbus_device_id *dev_id)
@@@ -102,9 -249,8 +253,14 @@@
  	if (!pdata)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	ret = vmbus_open(dev->channel, HV_RING_SIZE * PAGE_SIZE,
 +			 HV_RING_SIZE * PAGE_SIZE, NULL, 0,
 +			 hv_uio_channel_cb, pdata);
++=======
+ 	ret = vmbus_alloc_ring(channel, HV_RING_SIZE * PAGE_SIZE,
+ 			       HV_RING_SIZE * PAGE_SIZE);
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  	if (ret)
  		goto fail;
  
@@@ -126,11 -267,12 +277,19 @@@
  
  	/* mem resources */
  	pdata->info.mem[TXRX_RING_MAP].name = "txrx_rings";
+ 	ring_buffer = page_address(channel->ringbuffer_page);
  	pdata->info.mem[TXRX_RING_MAP].addr
++<<<<<<< HEAD
 +		= (uintptr_t)dev->channel->ringbuffer_pages;
 +	pdata->info.mem[TXRX_RING_MAP].size
 +		= dev->channel->ringbuffer_pagecount << PAGE_SHIFT;
 +	pdata->info.mem[TXRX_RING_MAP].memtype = UIO_MEM_LOGICAL;
++=======
+ 		= (uintptr_t)virt_to_phys(ring_buffer);
+ 	pdata->info.mem[TXRX_RING_MAP].size
+ 		= channel->ringbuffer_pagecount << PAGE_SHIFT;
+ 	pdata->info.mem[TXRX_RING_MAP].memtype = UIO_MEM_IOVA;
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  
  	pdata->info.mem[INT_PAGE_MAP].name = "int_page";
  	pdata->info.mem[INT_PAGE_MAP].addr
@@@ -144,6 -286,45 +303,48 @@@
  	pdata->info.mem[MON_PAGE_MAP].size = PAGE_SIZE;
  	pdata->info.mem[MON_PAGE_MAP].memtype = UIO_MEM_LOGICAL;
  
++<<<<<<< HEAD
++=======
+ 	pdata->recv_buf = vzalloc(RECV_BUFFER_SIZE);
+ 	if (pdata->recv_buf == NULL) {
+ 		ret = -ENOMEM;
+ 		goto fail_close;
+ 	}
+ 
+ 	ret = vmbus_establish_gpadl(channel, pdata->recv_buf,
+ 				    RECV_BUFFER_SIZE, &pdata->recv_gpadl);
+ 	if (ret)
+ 		goto fail_close;
+ 
+ 	/* put Global Physical Address Label in name */
+ 	snprintf(pdata->recv_name, sizeof(pdata->recv_name),
+ 		 "recv:%u", pdata->recv_gpadl);
+ 	pdata->info.mem[RECV_BUF_MAP].name = pdata->recv_name;
+ 	pdata->info.mem[RECV_BUF_MAP].addr
+ 		= (uintptr_t)pdata->recv_buf;
+ 	pdata->info.mem[RECV_BUF_MAP].size = RECV_BUFFER_SIZE;
+ 	pdata->info.mem[RECV_BUF_MAP].memtype = UIO_MEM_VIRTUAL;
+ 
+ 	pdata->send_buf = vzalloc(SEND_BUFFER_SIZE);
+ 	if (pdata->send_buf == NULL) {
+ 		ret = -ENOMEM;
+ 		goto fail_close;
+ 	}
+ 
+ 	ret = vmbus_establish_gpadl(channel, pdata->send_buf,
+ 				    SEND_BUFFER_SIZE, &pdata->send_gpadl);
+ 	if (ret)
+ 		goto fail_close;
+ 
+ 	snprintf(pdata->send_name, sizeof(pdata->send_name),
+ 		 "send:%u", pdata->send_gpadl);
+ 	pdata->info.mem[SEND_BUF_MAP].name = pdata->send_name;
+ 	pdata->info.mem[SEND_BUF_MAP].addr
+ 		= (uintptr_t)pdata->send_buf;
+ 	pdata->info.mem[SEND_BUF_MAP].size = SEND_BUFFER_SIZE;
+ 	pdata->info.mem[SEND_BUF_MAP].memtype = UIO_MEM_VIRTUAL;
+ 
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  	pdata->info.priv = pdata;
  	pdata->device = dev;
  
@@@ -153,12 -334,20 +354,27 @@@
  		goto fail_close;
  	}
  
++<<<<<<< HEAD
++=======
+ 	vmbus_set_chn_rescind_callback(channel, hv_uio_rescind);
+ 	vmbus_set_sc_create_callback(channel, hv_uio_new_channel);
+ 
+ 	ret = sysfs_create_bin_file(&channel->kobj, &ring_buffer_bin_attr);
+ 	if (ret)
+ 		dev_notice(&dev->device,
+ 			   "sysfs create ring bin file failed; %d\n", ret);
+ 
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  	hv_set_drvdata(dev, pdata);
  
  	return 0;
  
  fail_close:
++<<<<<<< HEAD
 +	vmbus_close(dev->channel);
++=======
+ 	hv_uio_cleanup(dev, pdata);
++>>>>>>> cdfa835c6e5e (uio_hv_generic: defer opening vmbus until first use)
  fail:
  	kfree(pdata);
  
@@@ -174,8 -363,10 +390,9 @@@ hv_uio_remove(struct hv_device *dev
  		return 0;
  
  	uio_unregister_device(&pdata->info);
 -	hv_uio_cleanup(dev, pdata);
  	hv_set_drvdata(dev, NULL);
- 	vmbus_close(dev->channel);
+ 
+ 	vmbus_free_ring(dev->channel);
  	kfree(pdata);
  	return 0;
  }
* Unmerged path drivers/uio/uio_hv_generic.c
