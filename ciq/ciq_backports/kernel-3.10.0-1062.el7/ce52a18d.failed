locking/lockdep: Add a faster path in __lock_release()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Waiman Long <longman@redhat.com>
commit ce52a18db45842f5b992851a552bd7f6acb2241b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ce52a18d.failed

When __lock_release() is called, the most likely unlock scenario is
on the innermost lock in the chain.  In this case, we can skip some of
the checks and provide a faster path to completion.

	Signed-off-by: Waiman Long <longman@redhat.com>
	Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Will Deacon <will.deacon@arm.com>
Link: http://lkml.kernel.org/r/1538511560-10090-4-git-send-email-longman@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit ce52a18db45842f5b992851a552bd7f6acb2241b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/lockdep.c
diff --cc kernel/lockdep.c
index afa5a3e0029b,511d30f88bce..000000000000
--- a/kernel/lockdep.c
+++ b/kernel/lockdep.c
@@@ -3425,95 -3626,98 +3425,118 @@@ found_it
  	curr->lockdep_depth = i;
  	curr->curr_chain_key = hlock->prev_chain_key;
  
++<<<<<<< HEAD:kernel/lockdep.c
 +	for (i++; i < depth; i++) {
 +		hlock = curr->held_locks + i;
 +		if (!__lock_acquire(hlock->instance,
 +			hlock_class(hlock)->subclass, hlock->trylock,
 +				hlock->read, hlock->check, hlock->hardirqs_off,
 +				hlock->nest_lock, hlock->acquire_ip,
 +				hlock->references))
 +			return 0;
 +	}
++=======
+ 	/*
+ 	 * The most likely case is when the unlock is on the innermost
+ 	 * lock. In this case, we are done!
+ 	 */
+ 	if (i == depth-1)
+ 		return 1;
+ 
+ 	if (reacquire_held_locks(curr, depth, i + 1))
+ 		return 0;
++>>>>>>> ce52a18db458 (locking/lockdep: Add a faster path in __lock_release()):kernel/locking/lockdep.c
  
  	/*
  	 * We had N bottles of beer on the wall, we drank one, but now
  	 * there's not N-1 bottles of beer left on the wall...
  	 */
++<<<<<<< HEAD:kernel/lockdep.c
 +	if (DEBUG_LOCKS_WARN_ON(curr->lockdep_depth != depth - 1))
 +		return 0;
 +	return 1;
++=======
+ 	DEBUG_LOCKS_WARN_ON(curr->lockdep_depth != depth-1);
+ 
+ 	/*
+ 	 * Since reacquire_held_locks() would have called check_chain_key()
+ 	 * indirectly via __lock_acquire(), we don't need to do it again
+ 	 * on return.
+ 	 */
+ 	return 0;
++>>>>>>> ce52a18db458 (locking/lockdep: Add a faster path in __lock_release()):kernel/locking/lockdep.c
  }
  
 -static int __lock_is_held(const struct lockdep_map *lock, int read)
 +/*
 + * Remove the lock to the list of currently held locks - this gets
 + * called on mutex_unlock()/spin_unlock*() (or on a failed
 + * mutex_lock_interruptible()). This is done for unlocks that nest
 + * perfectly. (i.e. the current top of the lock-stack is unlocked)
 + */
 +static int lock_release_nested(struct task_struct *curr,
 +			       struct lockdep_map *lock, unsigned long ip)
  {
 -	struct task_struct *curr = current;
 -	int i;
 -
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 -
 -		if (match_held_lock(hlock, lock)) {
 -			if (read == -1 || hlock->read == read)
 -				return 1;
 -
 -			return 0;
 -		}
 -	}
 +	struct held_lock *hlock;
 +	unsigned int depth;
  
 -	return 0;
 -}
 +	/*
 +	 * Pop off the top of the lock stack:
 +	 */
 +	depth = curr->lockdep_depth - 1;
 +	hlock = curr->held_locks + depth;
  
 -static struct pin_cookie __lock_pin_lock(struct lockdep_map *lock)
 -{
 -	struct pin_cookie cookie = NIL_COOKIE;
 -	struct task_struct *curr = current;
 -	int i;
 +	/*
 +	 * Is the unlock non-nested:
 +	 */
 +	if (hlock->instance != lock || hlock->references)
 +		return lock_release_non_nested(curr, lock, ip);
 +	curr->lockdep_depth--;
  
 -	if (unlikely(!debug_locks))
 -		return cookie;
 +	/*
 +	 * No more locks, but somehow we've got hash left over, who left it?
 +	 */
 +	if (DEBUG_LOCKS_WARN_ON(!depth && (hlock->prev_chain_key != 0)))
 +		return 0;
  
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 +	curr->curr_chain_key = hlock->prev_chain_key;
  
 -		if (match_held_lock(hlock, lock)) {
 -			/*
 -			 * Grab 16bits of randomness; this is sufficient to not
 -			 * be guessable and still allows some pin nesting in
 -			 * our u32 pin_count.
 -			 */
 -			cookie.val = 1 + (prandom_u32() >> 16);
 -			hlock->pin_count += cookie.val;
 -			return cookie;
 -		}
 -	}
 +	lock_release_holdtime(hlock);
  
 -	WARN(1, "pinning an unheld lock\n");
 -	return cookie;
 +#ifdef CONFIG_DEBUG_LOCKDEP
 +	hlock->prev_chain_key = 0;
 +	hlock->class_idx = 0;
 +	hlock->acquire_ip = 0;
 +	hlock->irq_context = 0;
 +#endif
 +	return 1;
  }
  
 -static void __lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 +/*
 + * Remove the lock to the list of currently held locks - this gets
 + * called on mutex_unlock()/spin_unlock*() (or on a failed
 + * mutex_lock_interruptible()). This is done for unlocks that nest
 + * perfectly. (i.e. the current top of the lock-stack is unlocked)
 + */
 +static void
 +__lock_release(struct lockdep_map *lock, int nested, unsigned long ip)
  {
  	struct task_struct *curr = current;
 -	int i;
  
 -	if (unlikely(!debug_locks))
 +	if (!check_unlock(curr, lock, ip))
  		return;
  
 -	for (i = 0; i < curr->lockdep_depth; i++) {
 -		struct held_lock *hlock = curr->held_locks + i;
 -
 -		if (match_held_lock(hlock, lock)) {
 -			hlock->pin_count += cookie.val;
 +	if (nested) {
 +		if (!lock_release_nested(curr, lock, ip))
 +			return;
 +	} else {
 +		if (!lock_release_non_nested(curr, lock, ip))
  			return;
 -		}
  	}
  
 -	WARN(1, "pinning an unheld lock\n");
 +	check_chain_key(curr);
  }
  
 -static void __lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)
 +static int __lock_is_held(struct lockdep_map *lock)
  {
  	struct task_struct *curr = current;
  	int i;
* Unmerged path kernel/lockdep.c
