kvm: vmx: Flush TLB when the APIC-access address changes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jim Mattson <jmattson@google.com>
commit fb6c8198431311027c3434d4e94ab8bc040f7aea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/fb6c8198.failed

Quoting from the Intel SDM, volume 3, section 28.3.3.4: Guidelines for
Use of the INVEPT Instruction:

If EPT was in use on a logical processor at one time with EPTP X, it
is recommended that software use the INVEPT instruction with the
"single-context" INVEPT type and with EPTP X in the INVEPT descriptor
before a VM entry on the same logical processor that enables EPT with
EPTP X and either (a) the "virtualize APIC accesses" VM-execution
control was changed from 0 to 1; or (b) the value of the APIC-access
address was changed.

In the nested case, the burden falls on L1, unless L0 enables EPT in
vmcs02 when L1 doesn't enable EPT in vmcs12.

	Signed-off-by: Jim Mattson <jmattson@google.com>
	Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
(cherry picked from commit fb6c8198431311027c3434d4e94ab8bc040f7aea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 25b1e0648f1e,e2f608283a5a..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -10507,38 -10262,17 +10516,47 @@@ static int prepare_vmcs02(struct kvm_vc
  
  	}
  
++<<<<<<< HEAD
 +	if (enable_pml) {
 +		/*
 +		 * Conceptually we want to copy the PML address and index from
 +		 * vmcs01 here, and then back to vmcs01 on nested vmexit. But,
 +		 * since we always flush the log on each vmexit, this happens
 +		 * to be equivalent to simply resetting the fields in vmcs02.
 +		 */
 +		ASSERT(vmx->pml_pg);
 +		vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));
 +		vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
++=======
+ 	if (nested_cpu_has_ept(vmcs12)) {
+ 		kvm_mmu_unload(vcpu);
+ 		nested_ept_init_mmu_context(vcpu);
+ 	} else if (nested_cpu_has2(vmcs12,
+ 				   SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {
+ 		vmx_flush_tlb_ept_only(vcpu);
++>>>>>>> fb6c81984313 (kvm: vmx: Flush TLB when the APIC-access address changes)
 +	}
 +
 +	if (nested_cpu_has_ept(vmcs12)) {
 +		if (nested_ept_init_mmu_context(vcpu)) {
 +			*entry_failure_code = ENTRY_FAIL_DEFAULT;
 +			return 1;
 +		}
  	}
  
 +	if (from_vmentry &&
 +	    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER))
 +		vcpu->arch.efer = vmcs12->guest_ia32_efer;
 +	else if (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE)
 +		vcpu->arch.efer |= (EFER_LMA | EFER_LME);
 +	else
 +		vcpu->arch.efer &= ~(EFER_LMA | EFER_LME);
 +	/* Note: modifies VM_ENTRY/EXIT_CONTROLS and GUEST/HOST_IA32_EFER */
 +	vmx_set_efer(vcpu, vcpu->arch.efer);
 +
  	/*
 -	 * This sets GUEST_CR0 to vmcs12->guest_cr0, possibly modifying those
 -	 * bits which we consider mandatory enabled.
 +	 * This sets GUEST_CR0 to vmcs12->guest_cr0, with possibly a modified
 +	 * TS bit (for lazy fpu) and bits which we consider mandatory enabled.
  	 * The CR0_READ_SHADOW is what L2 should have expected to read given
  	 * the specifications by L1; It's not enough to take
  	 * vmcs12->cr0_read_shadow because on our cr0_guest_host_mask we we
* Unmerged path arch/x86/kvm/vmx.c
