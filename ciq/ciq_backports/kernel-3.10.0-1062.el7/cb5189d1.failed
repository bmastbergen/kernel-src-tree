net/mlx5e: Do not recycle RX pages in interface down flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Do not recycle RX pages in interface down flow (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 96.36%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit cb5189d173097af805ff74c88191aba25fc60a55
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/cb5189d1.failed

Keep all page-pool recycle calls within NAPI context.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit cb5189d173097af805ff74c88191aba25fc60a55)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 3631c4f3022b,97db5eeca9f3..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -248,82 -248,137 +248,109 @@@ static void mlx5e_page_dma_unmap(struc
  void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
  			bool recycle)
  {
 -	if (likely(recycle)) {
 -		if (mlx5e_rx_cache_put(rq, dma_info))
 -			return;
 +	if (likely(recycle) && mlx5e_rx_cache_put(rq, dma_info))
 +		return;
  
 -		mlx5e_page_dma_unmap(rq, dma_info);
 -		page_pool_recycle_direct(rq->page_pool, dma_info->page);
 -	} else {
 -		mlx5e_page_dma_unmap(rq, dma_info);
 -		put_page(dma_info->page);
 -	}
 +	mlx5e_page_dma_unmap(rq, dma_info);
 +	put_page(dma_info->page);
  }
  
 -static inline int mlx5e_get_rx_frag(struct mlx5e_rq *rq,
 -				    struct mlx5e_wqe_frag_info *frag)
 +static inline bool mlx5e_page_reuse(struct mlx5e_rq *rq,
 +				    struct mlx5e_wqe_frag_info *wi)
  {
 -	int err = 0;
 -
 -	if (!frag->offset)
 -		/* On first frag (offset == 0), replenish page (dma_info actually).
 -		 * Other frags that point to the same dma_info (with a different
 -		 * offset) should just use the new one without replenishing again
 -		 * by themselves.
 -		 */
 -		err = mlx5e_page_alloc_mapped(rq, frag->di);
 -
 -	return err;
 +	return rq->wqe.page_reuse && wi->di.page &&
 +		(wi->offset + rq->wqe.frag_sz <= RQ_PAGE_SIZE(rq)) &&
 +		!mlx5e_page_is_reserved(wi->di.page);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix)
 +{
 +	struct mlx5e_wqe_frag_info *wi = &rq->wqe.frag_info[ix];
++=======
+ static inline void mlx5e_put_rx_frag(struct mlx5e_rq *rq,
+ 				     struct mlx5e_wqe_frag_info *frag,
+ 				     bool recycle)
+ {
+ 	if (frag->last_in_page)
+ 		mlx5e_page_release(rq, frag->di, recycle);
+ }
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  
 -static inline struct mlx5e_wqe_frag_info *get_frag(struct mlx5e_rq *rq, u16 ix)
 -{
 -	return &rq->wqe.frags[ix << rq->wqe.info.log_num_frags];
 -}
 -
 -static int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe_cyc *wqe,
 -			      u16 ix)
 -{
 -	struct mlx5e_wqe_frag_info *frag = get_frag(rq, ix);
 -	int err;
 -	int i;
 -
 -	for (i = 0; i < rq->wqe.info.num_frags; i++, frag++) {
 -		err = mlx5e_get_rx_frag(rq, frag);
 -		if (unlikely(err))
 -			goto free_frags;
 -
 -		wqe->data[i].addr = cpu_to_be64(frag->di->addr +
 -						frag->offset + rq->buff.headroom);
 +	/* check if page exists, hence can be reused */
 +	if (!wi->di.page) {
 +		if (unlikely(mlx5e_page_alloc_mapped(rq, &wi->di)))
 +			return -ENOMEM;
 +		wi->offset = 0;
  	}
  
 +	wqe->data.addr = cpu_to_be64(wi->di.addr + wi->offset + rq->buff.headroom);
  	return 0;
++<<<<<<< HEAD
++=======
+ 
+ free_frags:
+ 	while (--i >= 0)
+ 		mlx5e_put_rx_frag(rq, --frag, true);
+ 
+ 	return err;
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
  static inline void mlx5e_free_rx_wqe(struct mlx5e_rq *rq,
- 				     struct mlx5e_wqe_frag_info *wi)
+ 				     struct mlx5e_wqe_frag_info *wi,
+ 				     bool recycle)
  {
++<<<<<<< HEAD
 +	mlx5e_page_release(rq, &wi->di, true);
 +	wi->di.page = NULL;
++=======
+ 	int i;
+ 
+ 	for (i = 0; i < rq->wqe.info.num_frags; i++, wi++)
+ 		mlx5e_put_rx_frag(rq, wi, recycle);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
 -void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix)
 +static inline void mlx5e_free_rx_wqe_reuse(struct mlx5e_rq *rq,
 +					   struct mlx5e_wqe_frag_info *wi)
  {
 -	struct mlx5e_wqe_frag_info *wi = get_frag(rq, ix);
 +	if (mlx5e_page_reuse(rq, wi)) {
 +		rq->stats->page_reuse++;
 +		return;
 +	}
  
- 	mlx5e_free_rx_wqe(rq, wi);
+ 	mlx5e_free_rx_wqe(rq, wi, false);
  }
  
 -static int mlx5e_alloc_rx_wqes(struct mlx5e_rq *rq, u16 ix, u8 wqe_bulk)
 +void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix)
  {
 -	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 -	int err;
 -	int i;
 -
 -	for (i = 0; i < wqe_bulk; i++) {
 -		struct mlx5e_rx_wqe_cyc *wqe = mlx5_wq_cyc_get_wqe(wq, ix + i);
 -
 -		err = mlx5e_alloc_rx_wqe(rq, wqe, ix + i);
 -		if (unlikely(err))
 -			goto free_wqes;
 -	}
 -
 -	return 0;
 -
 -free_wqes:
 -	while (--i >= 0)
 -		mlx5e_dealloc_rx_wqe(rq, ix + i);
 +	struct mlx5e_wqe_frag_info *wi = &rq->wqe.frag_info[ix];
  
 -	return err;
 +	if (wi->di.page)
 +		mlx5e_free_rx_wqe(rq, wi);
  }
  
 -static inline void
 -mlx5e_add_skb_frag(struct mlx5e_rq *rq, struct sk_buff *skb,
 -		   struct mlx5e_dma_info *di, u32 frag_offset, u32 len,
 -		   unsigned int truesize)
 +static inline int mlx5e_mpwqe_strides_per_page(struct mlx5e_rq *rq)
  {
 -	dma_sync_single_for_cpu(rq->pdev,
 -				di->addr + frag_offset,
 -				len, DMA_FROM_DEVICE);
 -	page_ref_inc(di->page);
 -	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
 -			di->page, frag_offset, len, truesize);
 +	return rq->mpwqe.num_strides >> MLX5_MPWRQ_WQE_PAGE_ORDER;
  }
  
 -static inline void
 -mlx5e_copy_skb_header(struct device *pdev, struct sk_buff *skb,
 -		      struct mlx5e_dma_info *dma_info,
 -		      int offset_from, int offset_to, u32 headlen)
 +static inline void mlx5e_add_skb_frag_mpwqe(struct mlx5e_rq *rq,
 +					    struct sk_buff *skb,
 +					    struct mlx5e_mpw_info *wi,
 +					    u32 page_idx, u32 frag_offset,
 +					    u32 len)
  {
 -	const void *from = page_address(dma_info->page) + offset_from;
 -	/* Aligning len to sizeof(long) optimizes memcpy performance */
 -	unsigned int len = ALIGN(headlen, sizeof(long));
 +	unsigned int truesize = ALIGN(len, BIT(rq->mpwqe.log_stride_sz));
  
 -	dma_sync_single_for_cpu(pdev, dma_info->addr + offset_from, len,
 -				DMA_FROM_DEVICE);
 -	skb_copy_to_linear_data_offset(skb, offset_to, from, len);
 +	dma_sync_single_for_cpu(rq->pdev,
 +				wi->umr.dma_info[page_idx].addr + frag_offset,
 +				len, DMA_FROM_DEVICE);
 +	wi->skbs_frags[page_idx]++;
 +	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
 +			wi->umr.dma_info[page_idx].page, frag_offset,
 +			len, truesize);
  }
  
  static inline void
@@@ -355,41 -398,80 +382,47 @@@ mlx5e_copy_skb_header_mpwqe(struct devi
  	}
  }
  
++<<<<<<< HEAD
 +static inline void mlx5e_post_umr_wqe(struct mlx5e_rq *rq, u16 ix)
++=======
+ static void
+ mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi, bool recycle)
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  {
 -	const bool no_xdp_xmit =
 -		bitmap_empty(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 -	struct mlx5e_dma_info *dma_info = wi->umr.dma_info;
 -	int i;
 -
 -	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++)
 -		if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
 -			mlx5e_page_release(rq, &dma_info[i], recycle);
 -}
 -
 -static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
 -{
 -	struct mlx5_wq_ll *wq = &rq->mpwqe.wq;
 -	struct mlx5e_rx_wqe_ll *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
 -
 -	rq->mpwqe.umr_in_progress = false;
 -
 -	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 -
 -	/* ensure wqes are visible to device before updating doorbell record */
 -	dma_wmb();
 -
 -	mlx5_wq_ll_update_db_record(wq);
 -}
 -
 -static inline u16 mlx5e_icosq_wrap_cnt(struct mlx5e_icosq *sq)
 -{
 -	return sq->pc >> MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 -}
 -
 -static inline void mlx5e_fill_icosq_frag_edge(struct mlx5e_icosq *sq,
 -					      struct mlx5_wq_cyc *wq,
 -					      u16 pi, u16 frag_pi)
 -{
 -	struct mlx5e_sq_wqe_info *edge_wi, *wi = &sq->db.ico_wqe[pi];
 -	u8 nnops = mlx5_wq_cyc_get_frag_size(wq) - frag_pi;
 -
 -	edge_wi = wi + nnops;
 +	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
 +	struct mlx5e_icosq *sq = &rq->channel->icosq;
 +	struct mlx5_wq_cyc *wq = &sq->wq;
 +	struct mlx5e_umr_wqe *wqe;
 +	u8 num_wqebbs = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_BB);
 +	u16 pi;
  
 -	/* fill sq frag edge with nops to avoid wqe wrapping two pages */
 -	for (; wi < edge_wi; wi++) {
 -		wi->opcode = MLX5_OPCODE_NOP;
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_NOP;
  		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
  	}
 +
 +	wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 +	memcpy(wqe, &wi->umr.wqe, sizeof(*wqe));
 +	wqe->ctrl.opmod_idx_opcode =
 +		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 +			    MLX5_OPCODE_UMR);
 +
 +	sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_UMR;
 +	sq->pc += num_wqebbs;
 +	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &wqe->ctrl);
  }
  
 -static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 +static int mlx5e_alloc_rx_umr_mpwqe(struct mlx5e_rq *rq,
 +				    u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
 +	int pg_strides = mlx5e_mpwqe_strides_per_page(rq);
  	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
 -	struct mlx5e_icosq *sq = &rq->channel->icosq;
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5e_umr_wqe *umr_wqe;
 -	u16 xlt_offset = ix << (MLX5E_LOG_ALIGNED_MPWQE_PPW - 1);
 -	u16 pi, frag_pi;
  	int err;
  	int i;
  
 -	pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
 -	frag_pi = mlx5_wq_cyc_ctr2fragix(wq, sq->pc);
 -
 -	if (unlikely(frag_pi + MLX5E_UMR_WQEBBS > mlx5_wq_cyc_get_frag_size(wq))) {
 -		mlx5e_fill_icosq_frag_edge(sq, wq, pi, frag_pi);
 -		pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
 -	}
 -
 -	umr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 -	if (unlikely(mlx5e_icosq_wrap_cnt(sq) < 2))
 -		memcpy(umr_wqe, &rq->mpwqe.umr_wqe,
 -		       offsetof(struct mlx5e_umr_wqe, inline_mtts));
 -
++<<<<<<< HEAD
  	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++) {
  		err = mlx5e_page_alloc_mapped(rq, dma_info);
  		if (unlikely(err))
@@@ -413,47 -505,6 +446,52 @@@ err_unmap
  	return err;
  }
  
 +void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi)
 +{
 +	int pg_strides = mlx5e_mpwqe_strides_per_page(rq);
 +	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
 +	int i;
 +
 +	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++) {
 +		page_ref_sub(dma_info->page, pg_strides - wi->skbs_frags[i]);
 +		mlx5e_page_release(rq, dma_info, true);
 +	}
++=======
++	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++)
++		if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
++			mlx5e_page_release(rq, &dma_info[i], recycle);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
 +}
 +
 +static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
 +{
 +	struct mlx5_wq_ll *wq = &rq->wq;
 +	struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
 +
 +	rq->mpwqe.umr_in_progress = false;
 +
 +	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 +
 +	/* ensure wqes are visible to device before updating doorbell record */
 +	dma_wmb();
 +
 +	mlx5_wq_ll_update_db_record(wq);
 +}
 +
 +static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 +{
 +	int err;
 +
 +	err = mlx5e_alloc_rx_umr_mpwqe(rq, ix);
 +	if (unlikely(err)) {
 +		rq->stats->buff_alloc_err++;
 +		return err;
 +	}
 +	rq->mpwqe.umr_in_progress = true;
 +	mlx5e_post_umr_wqe(rq, ix);
 +	return 0;
 +}
 +
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
@@@ -955,10 -1115,10 +993,17 @@@ void mlx5e_handle_rx_cqe(struct mlx5e_r
  	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
  	napi_gro_receive(rq->cq.napi, skb);
  
++<<<<<<< HEAD
 +	mlx5e_free_rx_wqe_reuse(rq, wi);
 +wq_ll_pop:
 +	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
 +		       &wqe->next.next_wqe_index);
++=======
+ free_wqe:
+ 	mlx5e_free_rx_wqe(rq, wi, true);
+ wq_cyc_pop:
+ 	mlx5_wq_cyc_pop(wq);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
  #ifdef CONFIG_MLX5_ESWITCH
@@@ -1000,10 -1157,10 +1045,17 @@@ void mlx5e_handle_rx_cqe_rep(struct mlx
  
  	napi_gro_receive(rq->cq.napi, skb);
  
++<<<<<<< HEAD
 +	mlx5e_free_rx_wqe_reuse(rq, wi);
 +wq_ll_pop:
 +	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
 +		       &wqe->next.next_wqe_index);
++=======
+ free_wqe:
+ 	mlx5e_free_rx_wqe(rq, wi, true);
+ wq_cyc_pop:
+ 	mlx5_wq_cyc_pop(wq);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  #endif
  
@@@ -1088,8 -1293,10 +1140,15 @@@ mpwrq_cqe_out
  	if (likely(wi->consumed_strides < rq->mpwqe.num_strides))
  		return;
  
++<<<<<<< HEAD
 +	mlx5e_free_rx_mpwqe(rq, wi);
 +	mlx5_wq_ll_pop(&rq->wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++=======
+ 	wq  = &rq->mpwqe.wq;
+ 	wqe = mlx5_wq_ll_get_wqe(wq, wqe_id);
+ 	mlx5e_free_rx_mpwqe(rq, wi, true);
+ 	mlx5_wq_ll_pop(wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
@@@ -1325,9 -1524,8 +1384,14 @@@ void mlx5i_handle_rx_cqe(struct mlx5e_r
  	napi_gro_receive(rq->cq.napi, skb);
  
  wq_free_wqe:
++<<<<<<< HEAD
 +	mlx5e_free_rx_wqe_reuse(rq, wi);
 +	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
 +		       &wqe->next.next_wqe_index);
++=======
+ 	mlx5e_free_rx_wqe(rq, wi, true);
+ 	mlx5_wq_cyc_pop(wq);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
  #endif /* CONFIG_MLX5_CORE_IPOIB */
@@@ -1336,38 -1534,34 +1400,54 @@@
  
  void mlx5e_ipsec_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
  {
 -	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
  	struct mlx5e_wqe_frag_info *wi;
 +	struct mlx5e_rx_wqe *wqe;
 +	__be16 wqe_counter_be;
  	struct sk_buff *skb;
 +	u16 wqe_counter;
  	u32 cqe_bcnt;
 -	u16 ci;
  
 -	ci       = mlx5_wq_cyc_ctr2ix(wq, be16_to_cpu(cqe->wqe_counter));
 -	wi       = get_frag(rq, ci);
 -	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
 +	wqe_counter_be = cqe->wqe_counter;
 +	wqe_counter    = be16_to_cpu(wqe_counter_be);
 +	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
 +	wi             = &rq->wqe.frag_info[wqe_counter];
 +	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
  
 -	skb = rq->wqe.skb_from_cqe(rq, cqe, wi, cqe_bcnt);
 +	skb = skb_from_cqe(rq, cqe, wi, cqe_bcnt);
  	if (unlikely(!skb)) {
  		/* a DROP, save the page-reuse checks */
++<<<<<<< HEAD
 +		mlx5e_free_rx_wqe(rq, wi);
 +		goto wq_ll_pop;
++=======
+ 		mlx5e_free_rx_wqe(rq, wi, true);
+ 		goto wq_cyc_pop;
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  	}
 -	skb = mlx5e_ipsec_handle_rx_skb(rq->netdev, skb, &cqe_bcnt);
 +	skb = mlx5e_ipsec_handle_rx_skb(rq->netdev, skb);
  	if (unlikely(!skb)) {
++<<<<<<< HEAD
 +		mlx5e_free_rx_wqe(rq, wi);
 +		goto wq_ll_pop;
++=======
+ 		mlx5e_free_rx_wqe(rq, wi, true);
+ 		goto wq_cyc_pop;
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  	}
  
  	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
  	napi_gro_receive(rq->cq.napi, skb);
  
++<<<<<<< HEAD
 +	mlx5e_free_rx_wqe_reuse(rq, wi);
 +wq_ll_pop:
 +	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
 +		       &wqe->next.next_wqe_index);
++=======
+ 	mlx5e_free_rx_wqe(rq, wi, true);
+ wq_cyc_pop:
+ 	mlx5_wq_cyc_pop(wq);
++>>>>>>> cb5189d17309 (net/mlx5e: Do not recycle RX pages in interface down flow)
  }
  
  #endif /* CONFIG_MLX5_EN_IPSEC */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
