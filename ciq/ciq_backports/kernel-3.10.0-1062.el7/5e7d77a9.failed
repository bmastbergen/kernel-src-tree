net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: TX, Obsolete maintaining local copies of skb->len/data (Alaa Hleihel) [1641354 1642498]
Rebuild_FUZZ: 96.83%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 5e7d77a9c5738457cdaa6bc230799c8f80733481
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5e7d77a9.failed

Instead of maintaining a local copy of skb->len/data and updating
it upon every copy to the WQE inline part, just calculate it once
when needed, using the ihs.

This obsoletes the function mlx5e_tx_skb_pull_inline.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 5e7d77a9c5738457cdaa6bc230799c8f80733481)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 31cc86df395a,f29deb44bf3b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -317,59 -331,84 +305,120 @@@ mlx5e_txwqe_complete(struct mlx5e_txqs
  
  	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
  		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 -}
  
 -#define INL_HDR_START_SZ (sizeof(((struct mlx5_wqe_eth_seg *)NULL)->inline_hdr.start))
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.wqe_info[pi].skb = NULL;
 +		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
 +		sq->stats->nop++;
 +	}
 +}
  
 -netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 -			  struct mlx5e_tx_wqe *wqe, u16 pi)
 +static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 +				 struct mlx5e_tx_wqe *wqe, u16 pi)
  {
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5_wqe_ctrl_seg *cseg;
 -	struct mlx5_wqe_eth_seg  *eseg;
 -	struct mlx5_wqe_data_seg *dseg;
 -	struct mlx5e_tx_wqe_info *wi;
 +	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
 +
 +	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 +	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
  
  	struct mlx5e_sq_stats *stats = sq->stats;
++<<<<<<< HEAD
 +	unsigned char *skb_data = skb->data;
 +	unsigned int skb_len = skb->len;
 +	u8  opcode = MLX5_OPCODE_SEND;
 +	unsigned int num_bytes;
 +	int num_dma;
 +	u16 headlen;
 +	u16 ds_cnt;
 +	u16 ihs;
++=======
+ 	u16 ds_cnt, ds_cnt_inl = 0;
+ 	u16 headlen, ihs, frag_pi;
+ 	u8 num_wqebbs, opcode;
+ 	u32 num_bytes;
+ 	int num_dma;
+ 	__be16 mss;
+ 
+ 	/* Calc ihs and ds cnt, no writes to wqe yet */
+ 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
+ 	if (skb_is_gso(skb)) {
+ 		opcode    = MLX5_OPCODE_LSO;
+ 		mss       = cpu_to_be16(skb_shinfo(skb)->gso_size);
+ 		ihs       = mlx5e_tx_get_gso_ihs(sq, skb);
+ 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 		stats->packets += skb_shinfo(skb)->gso_segs;
+ 	} else {
+ 		opcode    = MLX5_OPCODE_SEND;
+ 		mss       = 0;
+ 		ihs       = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
+ 		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
+ 		stats->packets++;
+ 	}
+ 
+ 	stats->bytes     += num_bytes;
+ 	stats->xmit_more += skb->xmit_more;
+ 
+ 	headlen = skb->len - ihs - skb->data_len;
+ 	ds_cnt += !!headlen;
+ 	ds_cnt += skb_shinfo(skb)->nr_frags;
+ 
+ 	if (ihs) {
+ 		ihs += !!skb_vlan_tag_present(skb) * VLAN_HLEN;
+ 
+ 		ds_cnt_inl = DIV_ROUND_UP(ihs - INL_HDR_START_SZ, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	frag_pi = mlx5_wq_cyc_ctr2fragix(wq, sq->pc);
+ 	if (unlikely(frag_pi + num_wqebbs > mlx5_wq_cyc_get_frag_size(wq))) {
+ 		mlx5e_fill_sq_frag_edge(sq, wq, pi, frag_pi);
+ 		mlx5e_sq_fetch_wqe(sq, &wqe, &pi);
+ 	}
+ 
+ 	/* fill wqe */
+ 	wi   = &sq->db.wqe_info[pi];
+ 	cseg = &wqe->ctrl;
+ 	eseg = &wqe->eth;
+ 	dseg =  wqe->data;
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  
  	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
  
 -	eseg->mss = mss;
 +	if (skb_is_gso(skb)) {
 +		opcode = MLX5_OPCODE_LSO;
 +		ihs = mlx5e_txwqe_build_eseg_gso(sq, skb, eseg, &num_bytes);
 +		stats->packets += skb_shinfo(skb)->gso_segs;
 +	} else {
 +		ihs = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
 +		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
 +		stats->packets++;
 +	}
 +	stats->bytes     += num_bytes;
 +	stats->xmit_more += skb->xmit_more;
  
 +	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
  	if (ihs) {
+ 		eseg->inline_hdr.sz = cpu_to_be16(ihs);
  		if (skb_vlan_tag_present(skb)) {
++<<<<<<< HEAD
 +			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, ihs, &skb_data, &skb_len);
 +			ihs += VLAN_HLEN;
++=======
+ 			ihs -= VLAN_HLEN;
+ 			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, ihs);
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  			stats->added_vlan_packets++;
  		} else {
- 			memcpy(eseg->inline_hdr.start, skb_data, ihs);
- 			mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
+ 			memcpy(eseg->inline_hdr.start, skb->data, ihs);
  		}
++<<<<<<< HEAD
 +		eseg->inline_hdr.sz = cpu_to_be16(ihs);
 +		ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr.start), MLX5_SEND_WQE_DS);
++=======
+ 		dseg += ds_cnt_inl;
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  	} else if (skb_vlan_tag_present(skb)) {
  		eseg->insert.type = cpu_to_be16(MLX5_ETH_WQE_INSERT_VLAN);
  		if (skb->vlan_proto == cpu_to_be16(ETH_P_8021AD))
@@@ -378,14 -417,12 +427,18 @@@
  		stats->added_vlan_packets++;
  	}
  
++<<<<<<< HEAD
 +	headlen = skb_len - skb->data_len;
 +	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen,
 +					  (struct mlx5_wqe_data_seg *)cseg + ds_cnt);
++=======
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + ihs, headlen, dseg);
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  	if (unlikely(num_dma < 0))
 -		goto err_drop;
 +		goto dma_unmap_wqe_err;
  
 -	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt, num_wqebbs, num_bytes,
 -			     num_dma, wi, cseg);
 +	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt + num_dma,
 +			     num_bytes, num_dma, wi, cseg);
  
  	return NETDEV_TX_OK;
  
@@@ -592,37 -619,35 +645,44 @@@ mlx5i_txwqe_build_datagram(struct mlx5_
  netdev_tx_t mlx5i_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
  			  struct mlx5_av *av, u32 dqpn, u32 dqkey)
  {
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5i_tx_wqe *wqe;
 +	struct mlx5_wq_cyc       *wq   = &sq->wq;
 +	u16                       pi   = sq->pc & wq->sz_m1;
 +	struct mlx5i_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 +	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
  
 -	struct mlx5_wqe_datagram_seg *datagram;
 -	struct mlx5_wqe_ctrl_seg *cseg;
 -	struct mlx5_wqe_eth_seg  *eseg;
 -	struct mlx5_wqe_data_seg *dseg;
 -	struct mlx5e_tx_wqe_info *wi;
 +	struct mlx5_wqe_ctrl_seg     *cseg = &wqe->ctrl;
 +	struct mlx5_wqe_datagram_seg *datagram = &wqe->datagram;
 +	struct mlx5_wqe_eth_seg      *eseg = &wqe->eth;
  
  	struct mlx5e_sq_stats *stats = sq->stats;
++<<<<<<< HEAD
 +	unsigned char *skb_data = skb->data;
 +	unsigned int skb_len = skb->len;
 +	u8  opcode = MLX5_OPCODE_SEND;
 +	unsigned int num_bytes;
++=======
+ 	u16 headlen, ihs, pi, frag_pi;
+ 	u16 ds_cnt, ds_cnt_inl = 0;
+ 	u8 num_wqebbs, opcode;
+ 	u32 num_bytes;
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  	int num_dma;
 -	__be16 mss;
 +	u16 headlen;
 +	u16 ds_cnt;
 +	u16 ihs;
 +
 +	memset(wqe, 0, sizeof(*wqe));
 +
 +	mlx5i_txwqe_build_datagram(av, dqpn, dqkey, datagram);
 +
 +	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
  
 -	/* Calc ihs and ds cnt, no writes to wqe yet */
 -	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
  	if (skb_is_gso(skb)) {
 -		opcode    = MLX5_OPCODE_LSO;
 -		mss       = cpu_to_be16(skb_shinfo(skb)->gso_size);
 -		ihs       = mlx5e_tx_get_gso_ihs(sq, skb);
 -		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
 +		opcode = MLX5_OPCODE_LSO;
 +		ihs = mlx5e_txwqe_build_eseg_gso(sq, skb, eseg, &num_bytes);
  		stats->packets += skb_shinfo(skb)->gso_segs;
  	} else {
 -		opcode    = MLX5_OPCODE_SEND;
 -		mss       = 0;
 -		ihs       = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
 +		ihs = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
  		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
  		stats->packets++;
  	}
@@@ -630,22 -655,49 +690,59 @@@
  	stats->bytes     += num_bytes;
  	stats->xmit_more += skb->xmit_more;
  
++<<<<<<< HEAD
 +	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
++=======
+ 	headlen = skb->len - ihs - skb->data_len;
+ 	ds_cnt += !!headlen;
+ 	ds_cnt += skb_shinfo(skb)->nr_frags;
+ 
+ 	if (ihs) {
+ 		ds_cnt_inl = DIV_ROUND_UP(ihs - INL_HDR_START_SZ, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	frag_pi = mlx5_wq_cyc_ctr2fragix(wq, sq->pc);
+ 	if (unlikely(frag_pi + num_wqebbs > mlx5_wq_cyc_get_frag_size(wq))) {
+ 		pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+ 		mlx5e_fill_sq_frag_edge(sq, wq, pi, frag_pi);
+ 	}
+ 
+ 	mlx5i_sq_fetch_wqe(sq, &wqe, &pi);
+ 
+ 	/* fill wqe */
+ 	wi       = &sq->db.wqe_info[pi];
+ 	cseg     = &wqe->ctrl;
+ 	datagram = &wqe->datagram;
+ 	eseg     = &wqe->eth;
+ 	dseg     =  wqe->data;
+ 
+ 	mlx5i_txwqe_build_datagram(av, dqpn, dqkey, datagram);
+ 
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 
+ 	eseg->mss = mss;
+ 
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  	if (ihs) {
- 		memcpy(eseg->inline_hdr.start, skb_data, ihs);
- 		mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
+ 		memcpy(eseg->inline_hdr.start, skb->data, ihs);
  		eseg->inline_hdr.sz = cpu_to_be16(ihs);
 -		dseg += ds_cnt_inl;
 +		ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr.start), MLX5_SEND_WQE_DS);
  	}
  
++<<<<<<< HEAD
 +	headlen = skb_len - skb->data_len;
 +	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen,
 +					  (struct mlx5_wqe_data_seg *)cseg + ds_cnt);
++=======
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + ihs, headlen, dseg);
++>>>>>>> 5e7d77a9c573 (net/mlx5e: TX, Obsolete maintaining local copies of skb->len/data)
  	if (unlikely(num_dma < 0))
 -		goto err_drop;
 +		goto dma_unmap_wqe_err;
  
 -	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt, num_wqebbs, num_bytes,
 -			     num_dma, wi, cseg);
 +	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt + num_dma,
 +			     num_bytes, num_dma, wi, cseg);
  
  	return NETDEV_TX_OK;
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
