net/mlx5e: RX, Recycle buffer of UMR WQEs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: RX, Recycle buffer of UMR WQEs (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 94.87%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit ab966d7e4ff988a48b3ad72e7abf903aa840afd1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/ab966d7e.failed

Upon a new UMR post, check if the WQE buffer contains
a previous UMR WQE. If so, modify the dynamic fields
instead of a whole WQE overwrite. This saves a memcpy.

In current setting, after 2 WQ cycles (12 UMR posts),
this will always be the case.

No degradation sensed.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit ab966d7e4ff988a48b3ad72e7abf903aa840afd1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 95481ee556bb,176645762e49..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -434,18 -365,65 +434,51 @@@ static void mlx5e_post_rx_mpwqe(struct 
  	mlx5_wq_ll_update_db_record(wq);
  }
  
+ static inline u16 mlx5e_icosq_wrap_cnt(struct mlx5e_icosq *sq)
+ {
+ 	return sq->pc >> MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ }
+ 
  static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
++<<<<<<< HEAD
++=======
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
+ 	struct mlx5e_icosq *sq = &rq->channel->icosq;
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_umr_wqe *umr_wqe;
+ 	u16 xlt_offset = ix << (MLX5E_LOG_ALIGNED_MPWQE_PPW - 1);
++>>>>>>> ab966d7e4ff9 (net/mlx5e: RX, Recycle buffer of UMR WQEs)
  	int err;
 -	u16 pi;
 -	int i;
  
 -	/* fill sq edge with nops to avoid wqe wrap around */
 -	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 -		sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_NOP;
 -		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
 +	err = mlx5e_alloc_rx_umr_mpwqe(rq, ix);
 +	if (unlikely(err)) {
 +		rq->stats->buff_alloc_err++;
 +		return err;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	umr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	if (unlikely(mlx5e_icosq_wrap_cnt(sq) < 2))
+ 		memcpy(umr_wqe, &rq->mpwqe.umr_wqe,
+ 		       offsetof(struct mlx5e_umr_wqe, inline_mtts));
+ 
+ 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++) {
+ 		err = mlx5e_page_alloc_mapped(rq, dma_info);
+ 		if (unlikely(err))
+ 			goto err_unmap;
+ 		umr_wqe->inline_mtts[i].ptag = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
+ 	}
+ 
+ 	bitmap_zero(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
+ 	wi->consumed_strides = 0;
+ 
++>>>>>>> ab966d7e4ff9 (net/mlx5e: RX, Recycle buffer of UMR WQEs)
  	rq->mpwqe.umr_in_progress = true;
 -
 -	umr_wqe->ctrl.opmod_idx_opcode =
 -		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 -			    MLX5_OPCODE_UMR);
 -	umr_wqe->uctrl.xlt_offset = cpu_to_be16(xlt_offset);
 -
 -	sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_UMR;
 -	sq->pc += MLX5E_UMR_WQEBBS;
 -	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &umr_wqe->ctrl);
 -
 +	mlx5e_post_umr_wqe(rq, ix);
  	return 0;
 -
 -err_unmap:
 -	while (--i >= 0) {
 -		dma_info--;
 -		mlx5e_page_release(rq, dma_info, true);
 -	}
 -	rq->stats.buff_alloc_err++;
 -
 -	return err;
  }
  
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
