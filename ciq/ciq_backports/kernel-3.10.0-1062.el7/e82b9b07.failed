vhost: introduce vhost_exceeds_weight()

jira LE-1907
cve CVE-2019-3900
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [vhost] introduce vhost_exceeds_weight() (Jason Wang) [1702943] {CVE-2019-3900}
Rebuild_FUZZ: 90.14%
commit-author Jason Wang <jasowang@redhat.com>
commit e82b9b0727ff6d665fff2d326162b460dded554d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/e82b9b07.failed

We used to have vhost_exceeds_weight() for vhost-net to:

- prevent vhost kthread from hogging the cpu
- balance the time spent between TX and RX

This function could be useful for vsock and scsi as well. So move it
to vhost.c. Device must specify a weight which counts the number of
requests, or it can also specific a byte_weight which counts the
number of bytes that has been processed.

	Signed-off-by: Jason Wang <jasowang@redhat.com>
	Reviewed-by: Stefan Hajnoczi <stefanha@redhat.com>
	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
(cherry picked from commit e82b9b0727ff6d665fff2d326162b460dded554d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/net.c
#	drivers/vhost/scsi.c
#	drivers/vhost/vhost.c
#	drivers/vhost/vhost.h
#	drivers/vhost/vsock.c
diff --cc drivers/vhost/net.c
index 90b124ca16a4,061a06dc12a3..000000000000
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@@ -377,13 -588,174 +377,180 @@@ static bool vhost_exceeds_maxpend(struc
  	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
  	struct vhost_virtqueue *vq = &nvq->vq;
  
 -	return (nvq->upend_idx + UIO_MAXIOV - nvq->done_idx) % UIO_MAXIOV >
 -	       min_t(unsigned int, VHOST_MAX_PEND, vq->num >> 2);
 +	return (nvq->upend_idx + vq->num - VHOST_MAX_PEND) % UIO_MAXIOV
 +		== nvq->done_idx;
  }
  
++<<<<<<< HEAD
 +/* Expects to be always run from workqueue - which acts as
 + * read-size critical section for our kind of RCU. */
 +static void handle_tx(struct vhost_net *net)
++=======
+ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
+ 			    size_t hdr_size, int out)
+ {
+ 	/* Skip header. TODO: support TSO. */
+ 	size_t len = iov_length(vq->iov, out);
+ 
+ 	iov_iter_init(iter, WRITE, vq->iov, out, len);
+ 	iov_iter_advance(iter, hdr_size);
+ 
+ 	return iov_iter_count(iter);
+ }
+ 
+ static int get_tx_bufs(struct vhost_net *net,
+ 		       struct vhost_net_virtqueue *nvq,
+ 		       struct msghdr *msg,
+ 		       unsigned int *out, unsigned int *in,
+ 		       size_t *len, bool *busyloop_intr)
+ {
+ 	struct vhost_virtqueue *vq = &nvq->vq;
+ 	int ret;
+ 
+ 	ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
+ 
+ 	if (ret < 0 || ret == vq->num)
+ 		return ret;
+ 
+ 	if (*in) {
+ 		vq_err(vq, "Unexpected descriptor format for TX: out %d, int %d\n",
+ 			*out, *in);
+ 		return -EFAULT;
+ 	}
+ 
+ 	/* Sanity check */
+ 	*len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);
+ 	if (*len == 0) {
+ 		vq_err(vq, "Unexpected header len for TX: %zd expected %zd\n",
+ 			*len, nvq->vhost_hlen);
+ 		return -EFAULT;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static bool tx_can_batch(struct vhost_virtqueue *vq, size_t total_len)
+ {
+ 	return total_len < VHOST_NET_WEIGHT &&
+ 	       !vhost_vq_avail_empty(vq->dev, vq);
+ }
+ 
+ #define SKB_FRAG_PAGE_ORDER     get_order(32768)
+ 
+ static bool vhost_net_page_frag_refill(struct vhost_net *net, unsigned int sz,
+ 				       struct page_frag *pfrag, gfp_t gfp)
+ {
+ 	if (pfrag->page) {
+ 		if (pfrag->offset + sz <= pfrag->size)
+ 			return true;
+ 		__page_frag_cache_drain(pfrag->page, net->refcnt_bias);
+ 	}
+ 
+ 	pfrag->offset = 0;
+ 	net->refcnt_bias = 0;
+ 	if (SKB_FRAG_PAGE_ORDER) {
+ 		/* Avoid direct reclaim but allow kswapd to wake */
+ 		pfrag->page = alloc_pages((gfp & ~__GFP_DIRECT_RECLAIM) |
+ 					  __GFP_COMP | __GFP_NOWARN |
+ 					  __GFP_NORETRY,
+ 					  SKB_FRAG_PAGE_ORDER);
+ 		if (likely(pfrag->page)) {
+ 			pfrag->size = PAGE_SIZE << SKB_FRAG_PAGE_ORDER;
+ 			goto done;
+ 		}
+ 	}
+ 	pfrag->page = alloc_page(gfp);
+ 	if (likely(pfrag->page)) {
+ 		pfrag->size = PAGE_SIZE;
+ 		goto done;
+ 	}
+ 	return false;
+ 
+ done:
+ 	net->refcnt_bias = USHRT_MAX;
+ 	page_ref_add(pfrag->page, USHRT_MAX - 1);
+ 	return true;
+ }
+ 
+ #define VHOST_NET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)
+ 
+ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
+ 			       struct iov_iter *from)
+ {
+ 	struct vhost_virtqueue *vq = &nvq->vq;
+ 	struct vhost_net *net = container_of(vq->dev, struct vhost_net,
+ 					     dev);
+ 	struct socket *sock = vq->private_data;
+ 	struct page_frag *alloc_frag = &net->page_frag;
+ 	struct virtio_net_hdr *gso;
+ 	struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+ 	struct tun_xdp_hdr *hdr;
+ 	size_t len = iov_iter_count(from);
+ 	int headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;
+ 	int buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 	int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+ 	int sock_hlen = nvq->sock_hlen;
+ 	void *buf;
+ 	int copied;
+ 
+ 	if (unlikely(len < nvq->sock_hlen))
+ 		return -EFAULT;
+ 
+ 	if (SKB_DATA_ALIGN(len + pad) +
+ 	    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) > PAGE_SIZE)
+ 		return -ENOSPC;
+ 
+ 	buflen += SKB_DATA_ALIGN(len + pad);
+ 	alloc_frag->offset = ALIGN((u64)alloc_frag->offset, SMP_CACHE_BYTES);
+ 	if (unlikely(!vhost_net_page_frag_refill(net, buflen,
+ 						 alloc_frag, GFP_KERNEL)))
+ 		return -ENOMEM;
+ 
+ 	buf = (char *)page_address(alloc_frag->page) + alloc_frag->offset;
+ 	copied = copy_page_from_iter(alloc_frag->page,
+ 				     alloc_frag->offset +
+ 				     offsetof(struct tun_xdp_hdr, gso),
+ 				     sock_hlen, from);
+ 	if (copied != sock_hlen)
+ 		return -EFAULT;
+ 
+ 	hdr = buf;
+ 	gso = &hdr->gso;
+ 
+ 	if ((gso->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&
+ 	    vhost16_to_cpu(vq, gso->csum_start) +
+ 	    vhost16_to_cpu(vq, gso->csum_offset) + 2 >
+ 	    vhost16_to_cpu(vq, gso->hdr_len)) {
+ 		gso->hdr_len = cpu_to_vhost16(vq,
+ 			       vhost16_to_cpu(vq, gso->csum_start) +
+ 			       vhost16_to_cpu(vq, gso->csum_offset) + 2);
+ 
+ 		if (vhost16_to_cpu(vq, gso->hdr_len) > len)
+ 			return -EINVAL;
+ 	}
+ 
+ 	len -= sock_hlen;
+ 	copied = copy_page_from_iter(alloc_frag->page,
+ 				     alloc_frag->offset + pad,
+ 				     len, from);
+ 	if (copied != len)
+ 		return -EFAULT;
+ 
+ 	xdp->data_hard_start = buf;
+ 	xdp->data = buf + pad;
+ 	xdp->data_end = xdp->data + len;
+ 	hdr->buflen = buflen;
+ 
+ 	--net->refcnt_bias;
+ 	alloc_frag->offset += buflen;
+ 
+ 	++nvq->batched_xdp;
+ 
+ 	return 0;
+ }
+ 
+ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  {
  	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
  	struct vhost_virtqueue *vq = &nvq->vq;
@@@ -399,39 -770,111 +566,134 @@@
  	};
  	size_t len, total_len = 0;
  	int err;
++<<<<<<< HEAD
 +	size_t hdr_size;
 +	struct socket *sock;
++=======
+ 	int sent_pkts = 0;
+ 	bool sock_can_batch = (sock->sk->sk_sndbuf == INT_MAX);
+ 
+ 	for (;;) {
+ 		bool busyloop_intr = false;
+ 
+ 		if (nvq->done_idx == VHOST_NET_BATCH)
+ 			vhost_tx_batch(net, nvq, sock, &msg);
+ 
+ 		head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ 				   &busyloop_intr);
+ 		/* On error, stop handling until the next kick. */
+ 		if (unlikely(head < 0))
+ 			break;
+ 		/* Nothing new?  Wait for eventfd to tell us they refilled. */
+ 		if (head == vq->num) {
+ 			if (unlikely(busyloop_intr)) {
+ 				vhost_poll_queue(&vq->poll);
+ 			} else if (unlikely(vhost_enable_notify(&net->dev,
+ 								vq))) {
+ 				vhost_disable_notify(&net->dev, vq);
+ 				continue;
+ 			}
+ 			break;
+ 		}
+ 
+ 		total_len += len;
+ 
+ 		/* For simplicity, TX batching is only enabled if
+ 		 * sndbuf is unlimited.
+ 		 */
+ 		if (sock_can_batch) {
+ 			err = vhost_net_build_xdp(nvq, &msg.msg_iter);
+ 			if (!err) {
+ 				goto done;
+ 			} else if (unlikely(err != -ENOSPC)) {
+ 				vhost_tx_batch(net, nvq, sock, &msg);
+ 				vhost_discard_vq_desc(vq, 1);
+ 				vhost_net_enable_vq(net, vq);
+ 				break;
+ 			}
+ 
+ 			/* We can't build XDP buff, go for single
+ 			 * packet path but let's flush batched
+ 			 * packets.
+ 			 */
+ 			vhost_tx_batch(net, nvq, sock, &msg);
+ 			msg.msg_control = NULL;
+ 		} else {
+ 			if (tx_can_batch(vq, total_len))
+ 				msg.msg_flags |= MSG_MORE;
+ 			else
+ 				msg.msg_flags &= ~MSG_MORE;
+ 		}
+ 
+ 		/* TODO: Check specific error and bomb out unless ENOBUFS? */
+ 		err = sock->ops->sendmsg(sock, &msg, len);
+ 		if (unlikely(err < 0)) {
+ 			vhost_discard_vq_desc(vq, 1);
+ 			vhost_net_enable_vq(net, vq);
+ 			break;
+ 		}
+ 		if (err != len)
+ 			pr_debug("Truncated TX packet: len %d != %zd\n",
+ 				 err, len);
+ done:
+ 		vq->heads[nvq->done_idx].id = cpu_to_vhost32(vq, head);
+ 		vq->heads[nvq->done_idx].len = 0;
+ 		++nvq->done_idx;
+ 		if (vhost_exceeds_weight(vq, ++sent_pkts, total_len))
+ 			break;
+ 	}
+ 
+ 	vhost_tx_batch(net, nvq, sock, &msg);
+ }
+ 
+ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
+ {
+ 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
+ 	struct vhost_virtqueue *vq = &nvq->vq;
+ 	unsigned out, in;
+ 	int head;
+ 	struct msghdr msg = {
+ 		.msg_name = NULL,
+ 		.msg_namelen = 0,
+ 		.msg_control = NULL,
+ 		.msg_controllen = 0,
+ 		.msg_flags = MSG_DONTWAIT,
+ 	};
+ 	struct tun_msg_ctl ctl;
+ 	size_t len, total_len = 0;
+ 	int err;
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  	struct vhost_net_ubuf_ref *uninitialized_var(ubufs);
 -	bool zcopy_used;
 +	bool zcopy, zcopy_used;
  	int sent_pkts = 0;
  
 -	for (;;) {
 -		bool busyloop_intr;
 +	mutex_lock(&vq->mutex);
 +	sock = vq->private_data;
 +	if (!sock)
 +		goto out;
 +
 +	if (!vq_iotlb_prefetch(vq))
 +		goto out;
  
 +	vhost_disable_notify(&net->dev, vq);
 +
 +	hdr_size = nvq->vhost_hlen;
 +	zcopy = nvq->ubufs;
 +
 +	for (;;) {
  		/* Release DMAs done buffers first */
 -		vhost_zerocopy_signal_used(net, vq);
 +		if (zcopy)
 +			vhost_zerocopy_signal_used(net, vq);
  
 -		busyloop_intr = false;
 -		head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
 -				   &busyloop_intr);
 +		/* If more outstanding DMAs, queue the work.
 +		 * Handle upend_idx wrap around
 +		 */
 +		if (unlikely(vhost_exceeds_maxpend(net)))
 +			break;
 +
 +		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov,
 +						ARRAY_SIZE(vq->iov),
 +						&out, &in);
  		/* On error, stop handling until the next kick. */
  		if (unlikely(head < 0))
  			break;
@@@ -520,12 -943,36 +782,16 @@@
  		else
  			vhost_zerocopy_signal_used(net, vq);
  		vhost_net_tx_packet(net);
++<<<<<<< HEAD
 +		if (unlikely(total_len >= VHOST_NET_WEIGHT) ||
 +		    unlikely(++sent_pkts >= VHOST_NET_PKT_WEIGHT(vq))) {
 +			vhost_poll_queue(&vq->poll);
++=======
+ 		if (unlikely(vhost_exceeds_weight(vq, ++sent_pkts,
+ 						  total_len)))
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  			break;
- 		}
  	}
 -}
 -
 -/* Expects to be always run from workqueue - which acts as
 - * read-size critical section for our kind of RCU. */
 -static void handle_tx(struct vhost_net *net)
 -{
 -	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 -	struct vhost_virtqueue *vq = &nvq->vq;
 -	struct socket *sock;
 -
 -	mutex_lock_nested(&vq->mutex, VHOST_NET_VQ_TX);
 -	sock = vq->private_data;
 -	if (!sock)
 -		goto out;
 -
 -	if (!vq_iotlb_prefetch(vq))
 -		goto out;
 -
 -	vhost_disable_notify(&net->dev, vq);
 -	vhost_net_disable_vq(net, vq);
 -
 -	if (vhost_sock_zcopy(sock))
 -		handle_tx_zerocopy(net, sock);
 -	else
 -		handle_tx_copy(net, sock);
 -
  out:
  	mutex_unlock(&vq->mutex);
  }
@@@ -792,28 -1215,30 +1058,33 @@@ static void handle_rx(struct vhost_net 
  		}
  		/* TODO: Should check and handle checksum. */
  
 -		num_buffers = cpu_to_vhost16(vq, headcount);
 +		hdr.num_buffers = cpu_to_vhost16(vq, headcount);
  		if (likely(mergeable) &&
 -		    copy_to_iter(&num_buffers, sizeof num_buffers,
 -				 &fixup) != sizeof num_buffers) {
 +		    memcpy_toiovecend(nvq->hdr, (void *)&hdr.num_buffers,
 +				      offsetof(typeof(hdr), num_buffers),
 +				      sizeof hdr.num_buffers)) {
  			vq_err(vq, "Failed num_buffers write");
  			vhost_discard_vq_desc(vq, headcount);
 -			goto out;
 +			break;
  		}
  		nvq->done_idx += headcount;
 -		if (nvq->done_idx > VHOST_NET_BATCH)
 -			vhost_net_signal_used(nvq);
 +		if (nvq->done_idx > VHOST_RX_BATCH)
 +			vhost_rx_signal_used(nvq);
  		if (unlikely(vq_log))
 -			vhost_log_write(vq, vq_log, log, vhost_len,
 -					vq->iov, in);
 +			vhost_log_write(vq, vq_log, log, vhost_len);
  		total_len += vhost_len;
++<<<<<<< HEAD
 +		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
 +			vhost_poll_queue(&vq->poll);
 +			break;
 +		}
++=======
+ 		if (unlikely(vhost_exceeds_weight(vq, ++recv_pkts, total_len)))
+ 			goto out;
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  	}
 -	if (unlikely(busyloop_intr))
 -		vhost_poll_queue(&vq->poll);
 -	else
 -		vhost_net_enable_vq(net, vq);
  out:
 -	vhost_net_signal_used(nvq);
 +	vhost_rx_signal_used(nvq);
  	mutex_unlock(&vq->mutex);
  }
  
@@@ -883,20 -1320,22 +1154,26 @@@ static int vhost_net_open(struct inode 
  		n->vqs[i].ubuf_info = NULL;
  		n->vqs[i].upend_idx = 0;
  		n->vqs[i].done_idx = 0;
 -		n->vqs[i].batched_xdp = 0;
  		n->vqs[i].vhost_hlen = 0;
  		n->vqs[i].sock_hlen = 0;
 -		n->vqs[i].rx_ring = NULL;
 -		vhost_net_buf_init(&n->vqs[i].rxq);
  	}
++<<<<<<< HEAD
 +	r = vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);
 +	if (r < 0) {
 +		kfree(n);
 +		kfree(vqs);
 +		return r;
 +	}
++=======
+ 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ 		       UIO_MAXIOV + VHOST_NET_BATCH,
+ 		       VHOST_NET_WEIGHT, VHOST_NET_PKT_WEIGHT);
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  
 -	vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
 -	vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
 +	vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);
 +	vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);
  
  	f->private_data = n;
 -	n->page_frag.page = NULL;
 -	n->refcnt_bias = 0;
  
  	return 0;
  }
diff --cc drivers/vhost/scsi.c
index 2b14ae4c9782,27c9dac9f518..000000000000
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@@ -49,10 -49,20 +49,16 @@@
  
  #include "vhost.h"
  
 -#define VHOST_SCSI_VERSION  "v0.1"
 -#define VHOST_SCSI_NAMELEN 256
 -#define VHOST_SCSI_MAX_CDB_SIZE 32
 -#define VHOST_SCSI_DEFAULT_TAGS 256
 -#define VHOST_SCSI_PREALLOC_SGLS 2048
 -#define VHOST_SCSI_PREALLOC_UPAGES 2048
 -#define VHOST_SCSI_PREALLOC_PROT_SGLS 2048
 +#define TCM_VHOST_VERSION  "v0.1"
 +#define TCM_VHOST_NAMELEN 256
 +#define TCM_VHOST_MAX_CDB_SIZE 32
  
+ /* Max number of requests before requeueing the job.
+  * Using this limit prevents one virtqueue from starving others with
+  * request.
+  */
+ #define VHOST_SCSI_WEIGHT 256
+ 
  struct vhost_scsi_inflight {
  	/* Wait for the flush operation to finish */
  	struct completion comp;
@@@ -1212,46 -1598,47 +1218,51 @@@ static int vhost_scsi_set_features(stru
  
  static int vhost_scsi_open(struct inode *inode, struct file *f)
  {
 -	struct vhost_scsi *vs;
 +	struct vhost_scsi *s;
  	struct vhost_virtqueue **vqs;
 -	int r = -ENOMEM, i;
 +	int r, i;
  
 -	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);
 -	if (!vs) {
 -		vs = vzalloc(sizeof(*vs));
 -		if (!vs)
 -			goto err_vs;
 -	}
 +	s = kzalloc(sizeof(*s), GFP_KERNEL);
 +	if (!s)
 +		return -ENOMEM;
  
 -	vqs = kmalloc_array(VHOST_SCSI_MAX_VQ, sizeof(*vqs), GFP_KERNEL);
 -	if (!vqs)
 -		goto err_vqs;
 +	vqs = kmalloc(VHOST_SCSI_MAX_VQ * sizeof(*vqs), GFP_KERNEL);
 +	if (!vqs) {
 +		kfree(s);
 +		return -ENOMEM;
 +	}
  
 -	vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
 -	vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
 +	vhost_work_init(&s->vs_completion_work, vhost_scsi_complete_cmd_work);
 +	vhost_work_init(&s->vs_event_work, tcm_vhost_evt_work);
  
 -	vs->vs_events_nr = 0;
 -	vs->vs_events_missed = false;
 +	s->vs_events_nr = 0;
 +	s->vs_events_missed = false;
  
 -	vqs[VHOST_SCSI_VQ_CTL] = &vs->vqs[VHOST_SCSI_VQ_CTL].vq;
 -	vqs[VHOST_SCSI_VQ_EVT] = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
 -	vs->vqs[VHOST_SCSI_VQ_CTL].vq.handle_kick = vhost_scsi_ctl_handle_kick;
 -	vs->vqs[VHOST_SCSI_VQ_EVT].vq.handle_kick = vhost_scsi_evt_handle_kick;
 +	vqs[VHOST_SCSI_VQ_CTL] = &s->vqs[VHOST_SCSI_VQ_CTL].vq;
 +	vqs[VHOST_SCSI_VQ_EVT] = &s->vqs[VHOST_SCSI_VQ_EVT].vq;
 +	s->vqs[VHOST_SCSI_VQ_CTL].vq.handle_kick = vhost_scsi_ctl_handle_kick;
 +	s->vqs[VHOST_SCSI_VQ_EVT].vq.handle_kick = vhost_scsi_evt_handle_kick;
  	for (i = VHOST_SCSI_VQ_IO; i < VHOST_SCSI_MAX_VQ; i++) {
 -		vqs[i] = &vs->vqs[i].vq;
 -		vs->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;
 +		vqs[i] = &s->vqs[i].vq;
 +		s->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;
  	}
++<<<<<<< HEAD
 +	r = vhost_dev_init(&s->dev, vqs, VHOST_SCSI_MAX_VQ);
++=======
+ 	vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV,
+ 		       VHOST_SCSI_WEIGHT, 0);
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  
 -	vhost_scsi_init_inflight(vs, NULL);
 +	tcm_vhost_init_inflight(s, NULL);
  
 -	f->private_data = vs;
 -	return 0;
 +	if (r < 0) {
 +		kfree(vqs);
 +		kfree(s);
 +		return r;
 +	}
  
 -err_vqs:
 -	kvfree(vs);
 -err_vs:
 -	return r;
 +	f->private_data = s;
 +	return 0;
  }
  
  static int vhost_scsi_release(struct inode *inode, struct file *f)
diff --cc drivers/vhost/vhost.c
index 56c68f4f484c,3f3eac4bcc58..000000000000
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@@ -393,9 -413,26 +393,30 @@@ static void vhost_dev_free_iovecs(struc
  		vhost_vq_free_iovecs(dev->vqs[i]);
  }
  
++<<<<<<< HEAD
 +long vhost_dev_init(struct vhost_dev *dev,
 +		    struct vhost_virtqueue **vqs, int nvqs)
++=======
+ bool vhost_exceeds_weight(struct vhost_virtqueue *vq,
+ 			  int pkts, int total_len)
+ {
+ 	struct vhost_dev *dev = vq->dev;
+ 
+ 	if ((dev->byte_weight && total_len >= dev->byte_weight) ||
+ 	    pkts >= dev->weight) {
+ 		vhost_poll_queue(&vq->poll);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ EXPORT_SYMBOL_GPL(vhost_exceeds_weight);
+ 
+ void vhost_dev_init(struct vhost_dev *dev,
+ 		    struct vhost_virtqueue **vqs, int nvqs,
+ 		    int iov_limit, int weight, int byte_weight)
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  {
 -	struct vhost_virtqueue *vq;
  	int i;
  
  	dev->vqs = vqs;
@@@ -407,6 -443,9 +428,12 @@@
  	dev->iotlb = NULL;
  	dev->mm = NULL;
  	dev->worker = NULL;
++<<<<<<< HEAD
++=======
+ 	dev->iov_limit = iov_limit;
+ 	dev->weight = weight;
+ 	dev->byte_weight = byte_weight;
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  	init_llist_head(&dev->work_list);
  	init_waitqueue_head(&dev->wait);
  	INIT_LIST_HEAD(&dev->read_list);
diff --cc drivers/vhost/vhost.h
index 886d7c3bccf5,27a78a9b8cc7..000000000000
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@@ -172,9 -170,14 +172,20 @@@ struct vhost_dev 
  	struct list_head read_list;
  	struct list_head pending_list;
  	wait_queue_head_t wait;
++<<<<<<< HEAD
 +};
 +
 +long vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);
++=======
+ 	int iov_limit;
+ 	int weight;
+ 	int byte_weight;
+ };
+ 
+ bool vhost_exceeds_weight(struct vhost_virtqueue *vq, int pkts, int total_len);
+ void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs,
+ 		    int nvqs, int iov_limit, int weight, int byte_weight);
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  long vhost_dev_set_owner(struct vhost_dev *dev);
  bool vhost_dev_has_owner(struct vhost_dev *dev);
  long vhost_dev_check_owner(struct vhost_dev *);
diff --cc drivers/vhost/vsock.c
index 9906913b630c,47c6d4d43c70..000000000000
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@@ -20,10 -21,17 +20,18 @@@
  #include "vhost.h"
  
  #define VHOST_VSOCK_DEFAULT_HOST_CID	2
+ /* Max number of bytes transferred before requeueing the job.
+  * Using this limit prevents one virtqueue from starving others. */
+ #define VHOST_VSOCK_WEIGHT 0x80000
+ /* Max number of packets transferred before requeueing the job.
+  * Using this limit prevents one virtqueue from starving others with
+  * small pkts.
+  */
+ #define VHOST_VSOCK_PKT_WEIGHT 256
  
  enum {
 -	VHOST_VSOCK_FEATURES = VHOST_FEATURES,
 +	VHOST_VSOCK_FEATURES = VHOST_FEATURES |
 +			       (1ULL << VIRTIO_F_VERSION_1),
  };
  
  /* Used to track all the vhost_vsock instances on the system. */
@@@ -500,7 -539,9 +508,13 @@@ static int vhost_vsock_dev_open(struct 
  	vsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;
  	vsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;
  
++<<<<<<< HEAD
 +	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));
++=======
+ 	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ 		       UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT,
+ 		       VHOST_VSOCK_WEIGHT);
++>>>>>>> e82b9b0727ff (vhost: introduce vhost_exceeds_weight())
  
  	file->private_data = vsock;
  	spin_lock_init(&vsock->send_pkt_list_lock);
* Unmerged path drivers/vhost/net.c
* Unmerged path drivers/vhost/scsi.c
* Unmerged path drivers/vhost/vhost.c
* Unmerged path drivers/vhost/vhost.h
* Unmerged path drivers/vhost/vsock.c
