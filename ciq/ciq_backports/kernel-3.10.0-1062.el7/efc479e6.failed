kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Roman Kagan <rkagan@virtuozzo.com>
commit efc479e6900c22bad9a2b649d13405ed9cde2d53
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/efc479e6.failed

There is a flaw in the Hyper-V SynIC implementation in KVM: when message
page or event flags page is enabled by setting the corresponding msr,
KVM zeroes it out.  This is problematic because on migration the
corresponding MSRs are loaded on the destination, so the content of
those pages is lost.

This went unnoticed so far because the only user of those pages was
in-KVM hyperv synic timers, which could continue working despite that
zeroing.

Newer QEMU uses those pages for Hyper-V VMBus implementation, and
zeroing them breaks the migration.

Besides, in newer QEMU the content of those pages is fully managed by
QEMU, so zeroing them is undesirable even when writing the MSRs from the
guest side.

To support this new scheme, introduce a new capability,
KVM_CAP_HYPERV_SYNIC2, which, when enabled, makes sure that the synic
pages aren't zeroed out in KVM.

	Signed-off-by: Roman Kagan <rkagan@virtuozzo.com>
	Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
(cherry picked from commit efc479e6900c22bad9a2b649d13405ed9cde2d53)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/virtual/kvm/api.txt
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/hyperv.h
#	arch/x86/kvm/x86.c
#	include/uapi/linux/kvm.h
diff --cc Documentation/virtual/kvm/api.txt
index 9488596d832f,78ac577c9378..000000000000
--- a/Documentation/virtual/kvm/api.txt
+++ b/Documentation/virtual/kvm/api.txt
@@@ -3387,3 -4174,167 +3387,170 @@@ available, means that that the kernel h
  H_RANDOM hypercall backed by a hardware random-number generator.
  If present, the kernel H_RANDOM handler can be enabled for guest use
  with the KVM_CAP_PPC_ENABLE_HCALL capability.
++<<<<<<< HEAD
++=======
+ 
+ 8.2 KVM_CAP_HYPERV_SYNIC
+ 
+ Architectures: x86
+ This capability, if KVM_CHECK_EXTENSION indicates that it is
+ available, means that that the kernel has an implementation of the
+ Hyper-V Synthetic interrupt controller(SynIC). Hyper-V SynIC is
+ used to support Windows Hyper-V based guest paravirt drivers(VMBus).
+ 
+ In order to use SynIC, it has to be activated by setting this
+ capability via KVM_ENABLE_CAP ioctl on the vcpu fd. Note that this
+ will disable the use of APIC hardware virtualization even if supported
+ by the CPU, as it's incompatible with SynIC auto-EOI behavior.
+ 
+ 8.3 KVM_CAP_PPC_RADIX_MMU
+ 
+ Architectures: ppc
+ 
+ This capability, if KVM_CHECK_EXTENSION indicates that it is
+ available, means that that the kernel can support guests using the
+ radix MMU defined in Power ISA V3.00 (as implemented in the POWER9
+ processor).
+ 
+ 8.4 KVM_CAP_PPC_HASH_MMU_V3
+ 
+ Architectures: ppc
+ 
+ This capability, if KVM_CHECK_EXTENSION indicates that it is
+ available, means that that the kernel can support guests using the
+ hashed page table MMU defined in Power ISA V3.00 (as implemented in
+ the POWER9 processor), including in-memory segment tables.
+ 
+ 8.5 KVM_CAP_MIPS_VZ
+ 
+ Architectures: mips
+ 
+ This capability, if KVM_CHECK_EXTENSION on the main kvm handle indicates that
+ it is available, means that full hardware assisted virtualization capabilities
+ of the hardware are available for use through KVM. An appropriate
+ KVM_VM_MIPS_* type must be passed to KVM_CREATE_VM to create a VM which
+ utilises it.
+ 
+ If KVM_CHECK_EXTENSION on a kvm VM handle indicates that this capability is
+ available, it means that the VM is using full hardware assisted virtualization
+ capabilities of the hardware. This is useful to check after creating a VM with
+ KVM_VM_MIPS_DEFAULT.
+ 
+ The value returned by KVM_CHECK_EXTENSION should be compared against known
+ values (see below). All other values are reserved. This is to allow for the
+ possibility of other hardware assisted virtualization implementations which
+ may be incompatible with the MIPS VZ ASE.
+ 
+  0: The trap & emulate implementation is in use to run guest code in user
+     mode. Guest virtual memory segments are rearranged to fit the guest in the
+     user mode address space.
+ 
+  1: The MIPS VZ ASE is in use, providing full hardware assisted
+     virtualization, including standard guest virtual memory segments.
+ 
+ 8.6 KVM_CAP_MIPS_TE
+ 
+ Architectures: mips
+ 
+ This capability, if KVM_CHECK_EXTENSION on the main kvm handle indicates that
+ it is available, means that the trap & emulate implementation is available to
+ run guest code in user mode, even if KVM_CAP_MIPS_VZ indicates that hardware
+ assisted virtualisation is also available. KVM_VM_MIPS_TE (0) must be passed
+ to KVM_CREATE_VM to create a VM which utilises it.
+ 
+ If KVM_CHECK_EXTENSION on a kvm VM handle indicates that this capability is
+ available, it means that the VM is using trap & emulate.
+ 
+ 8.7 KVM_CAP_MIPS_64BIT
+ 
+ Architectures: mips
+ 
+ This capability indicates the supported architecture type of the guest, i.e. the
+ supported register and address width.
+ 
+ The values returned when this capability is checked by KVM_CHECK_EXTENSION on a
+ kvm VM handle correspond roughly to the CP0_Config.AT register field, and should
+ be checked specifically against known values (see below). All other values are
+ reserved.
+ 
+  0: MIPS32 or microMIPS32.
+     Both registers and addresses are 32-bits wide.
+     It will only be possible to run 32-bit guest code.
+ 
+  1: MIPS64 or microMIPS64 with access only to 32-bit compatibility segments.
+     Registers are 64-bits wide, but addresses are 32-bits wide.
+     64-bit guest code may run but cannot access MIPS64 memory segments.
+     It will also be possible to run 32-bit guest code.
+ 
+  2: MIPS64 or microMIPS64 with access to all address segments.
+     Both registers and addresses are 64-bits wide.
+     It will be possible to run 64-bit or 32-bit guest code.
+ 
+ 8.8 KVM_CAP_X86_GUEST_MWAIT
+ 
+ Architectures: x86
+ 
+ This capability indicates that guest using memory monotoring instructions
+ (MWAIT/MWAITX) to stop the virtual CPU will not cause a VM exit.  As such time
+ spent while virtual CPU is halted in this way will then be accounted for as
+ guest running time on the host (as opposed to e.g. HLT).
+ 
+ 8.9 KVM_CAP_ARM_USER_IRQ
+ 
+ Architectures: arm, arm64
+ This capability, if KVM_CHECK_EXTENSION indicates that it is available, means
+ that if userspace creates a VM without an in-kernel interrupt controller, it
+ will be notified of changes to the output level of in-kernel emulated devices,
+ which can generate virtual interrupts, presented to the VM.
+ For such VMs, on every return to userspace, the kernel
+ updates the vcpu's run->s.regs.device_irq_level field to represent the actual
+ output level of the device.
+ 
+ Whenever kvm detects a change in the device output level, kvm guarantees at
+ least one return to userspace before running the VM.  This exit could either
+ be a KVM_EXIT_INTR or any other exit event, like KVM_EXIT_MMIO. This way,
+ userspace can always sample the device output level and re-compute the state of
+ the userspace interrupt controller.  Userspace should always check the state
+ of run->s.regs.device_irq_level on every kvm exit.
+ The value in run->s.regs.device_irq_level can represent both level and edge
+ triggered interrupt signals, depending on the device.  Edge triggered interrupt
+ signals will exit to userspace with the bit in run->s.regs.device_irq_level
+ set exactly once per edge signal.
+ 
+ The field run->s.regs.device_irq_level is available independent of
+ run->kvm_valid_regs or run->kvm_dirty_regs bits.
+ 
+ If KVM_CAP_ARM_USER_IRQ is supported, the KVM_CHECK_EXTENSION ioctl returns a
+ number larger than 0 indicating the version of this capability is implemented
+ and thereby which bits in in run->s.regs.device_irq_level can signal values.
+ 
+ Currently the following bits are defined for the device_irq_level bitmap:
+ 
+   KVM_CAP_ARM_USER_IRQ >= 1:
+ 
+     KVM_ARM_DEV_EL1_VTIMER -  EL1 virtual timer
+     KVM_ARM_DEV_EL1_PTIMER -  EL1 physical timer
+     KVM_ARM_DEV_PMU        -  ARM PMU overflow interrupt signal
+ 
+ Future versions of kvm may implement additional events. These will get
+ indicated by returning a higher number from KVM_CHECK_EXTENSION and will be
+ listed above.
+ 
+ 8.10 KVM_CAP_PPC_SMT_POSSIBLE
+ 
+ Architectures: ppc
+ 
+ Querying this capability returns a bitmap indicating the possible
+ virtual SMT modes that can be set using KVM_CAP_PPC_SMT.  If bit N
+ (counting from the right) is set, then a virtual SMT mode of 2^N is
+ available.
+ 
+ 8.11 KVM_CAP_HYPERV_SYNIC2
+ 
+ Architectures: x86
+ 
+ This capability enables a newer version of Hyper-V Synthetic interrupt
+ controller (SynIC).  The only difference with KVM_CAP_HYPERV_SYNIC is that KVM
+ doesn't clear SynIC message and event flags pages when they are enabled by
+ writing to the respective MSRs.
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
diff --cc arch/x86/include/asm/kvm_host.h
index d78aa4d6276c,9d8de5dd7546..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -407,6 -440,31 +407,34 @@@ struct kvm_mtrr 
  	struct list_head head;
  };
  
++<<<<<<< HEAD
++=======
+ /* Hyper-V SynIC timer */
+ struct kvm_vcpu_hv_stimer {
+ 	struct hrtimer timer;
+ 	int index;
+ 	u64 config;
+ 	u64 count;
+ 	u64 exp_time;
+ 	struct hv_message msg;
+ 	bool msg_pending;
+ };
+ 
+ /* Hyper-V synthetic interrupt controller (SynIC)*/
+ struct kvm_vcpu_hv_synic {
+ 	u64 version;
+ 	u64 control;
+ 	u64 msg_page;
+ 	u64 evt_page;
+ 	atomic64_t sint[HV_SYNIC_SINT_COUNT];
+ 	atomic_t sint_to_gsi[HV_SYNIC_SINT_COUNT];
+ 	DECLARE_BITMAP(auto_eoi_bitmap, 256);
+ 	DECLARE_BITMAP(vec_bitmap, 256);
+ 	bool active;
+ 	bool dont_zero_synic_pages;
+ };
+ 
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  /* Hyper-V per vcpu emulation context */
  struct kvm_vcpu_hv {
  	u64 hv_vapic;
diff --cc arch/x86/kvm/hyperv.c
index a44fd11c98a7,a8084406707e..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -30,6 -35,359 +30,362 @@@
  
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ static inline u64 synic_read_sint(struct kvm_vcpu_hv_synic *synic, int sint)
+ {
+ 	return atomic64_read(&synic->sint[sint]);
+ }
+ 
+ static inline int synic_get_sint_vector(u64 sint_value)
+ {
+ 	if (sint_value & HV_SYNIC_SINT_MASKED)
+ 		return -1;
+ 	return sint_value & HV_SYNIC_SINT_VECTOR_MASK;
+ }
+ 
+ static bool synic_has_vector_connected(struct kvm_vcpu_hv_synic *synic,
+ 				      int vector)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool synic_has_vector_auto_eoi(struct kvm_vcpu_hv_synic *synic,
+ 				     int vector)
+ {
+ 	int i;
+ 	u64 sint_value;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		sint_value = synic_read_sint(synic, i);
+ 		if (synic_get_sint_vector(sint_value) == vector &&
+ 		    sint_value & HV_SYNIC_SINT_AUTO_EOI)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static int synic_set_sint(struct kvm_vcpu_hv_synic *synic, int sint,
+ 			  u64 data, bool host)
+ {
+ 	int vector;
+ 
+ 	vector = data & HV_SYNIC_SINT_VECTOR_MASK;
+ 	if (vector < 16 && !host)
+ 		return 1;
+ 	/*
+ 	 * Guest may configure multiple SINTs to use the same vector, so
+ 	 * we maintain a bitmap of vectors handled by synic, and a
+ 	 * bitmap of vectors with auto-eoi behavior.  The bitmaps are
+ 	 * updated here, and atomically queried on fast paths.
+ 	 */
+ 
+ 	atomic64_set(&synic->sint[sint], data);
+ 
+ 	if (synic_has_vector_connected(synic, vector))
+ 		__set_bit(vector, synic->vec_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->vec_bitmap);
+ 
+ 	if (synic_has_vector_auto_eoi(synic, vector))
+ 		__set_bit(vector, synic->auto_eoi_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->auto_eoi_bitmap);
+ 
+ 	/* Load SynIC vectors into EOI exit bitmap */
+ 	kvm_make_request(KVM_REQ_SCAN_IOAPIC, synic_to_vcpu(synic));
+ 	return 0;
+ }
+ 
+ static struct kvm_vcpu_hv_synic *synic_get(struct kvm *kvm, u32 vcpu_id)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	if (vcpu_id >= atomic_read(&kvm->online_vcpus))
+ 		return NULL;
+ 	vcpu = kvm_get_vcpu(kvm, vcpu_id);
+ 	if (!vcpu)
+ 		return NULL;
+ 	synic = vcpu_to_synic(vcpu);
+ 	return (synic->active) ? synic : NULL;
+ }
+ 
+ static void synic_clear_sint_msg_pending(struct kvm_vcpu_hv_synic *synic,
+ 					u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *msg;
+ 	struct hv_message_page *msg_page;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page)) {
+ 		vcpu_err(vcpu, "Hyper-V SynIC can't get msg page, gpa 0x%llx\n",
+ 			 gpa);
+ 		return;
+ 	}
+ 	msg_page = kmap_atomic(page);
+ 
+ 	msg = &msg_page->sint_message[sint];
+ 	msg->header.message_flags.msg_pending = 0;
+ 
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ }
+ 
+ static void kvm_hv_notify_acked_sint(struct kvm_vcpu *vcpu, u32 sint)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	int gsi, idx, stimers_pending;
+ 
+ 	trace_kvm_hv_notify_acked_sint(vcpu->vcpu_id, sint);
+ 
+ 	if (synic->msg_page & HV_SYNIC_SIMP_ENABLE)
+ 		synic_clear_sint_msg_pending(synic, sint);
+ 
+ 	/* Try to deliver pending Hyper-V SynIC timers messages */
+ 	stimers_pending = 0;
+ 	for (idx = 0; idx < ARRAY_SIZE(hv_vcpu->stimer); idx++) {
+ 		stimer = &hv_vcpu->stimer[idx];
+ 		if (stimer->msg_pending &&
+ 		    (stimer->config & HV_STIMER_ENABLE) &&
+ 		    HV_STIMER_SINT(stimer->config) == sint) {
+ 			set_bit(stimer->index,
+ 				hv_vcpu->stimer_pending_bitmap);
+ 			stimers_pending++;
+ 		}
+ 	}
+ 	if (stimers_pending)
+ 		kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 
+ 	idx = srcu_read_lock(&kvm->irq_srcu);
+ 	gsi = atomic_read(&synic->sint_to_gsi[sint]);
+ 	if (gsi != -1)
+ 		kvm_notify_acked_gsi(kvm, gsi);
+ 	srcu_read_unlock(&kvm->irq_srcu, idx);
+ }
+ 
+ static void synic_exit(struct kvm_vcpu_hv_synic *synic, u32 msr)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_vcpu_hv *hv_vcpu = &vcpu->arch.hyperv;
+ 
+ 	hv_vcpu->exit.type = KVM_EXIT_HYPERV_SYNIC;
+ 	hv_vcpu->exit.u.synic.msr = msr;
+ 	hv_vcpu->exit.u.synic.control = synic->control;
+ 	hv_vcpu->exit.u.synic.evt_page = synic->evt_page;
+ 	hv_vcpu->exit.u.synic.msg_page = synic->msg_page;
+ 
+ 	kvm_make_request(KVM_REQ_HV_EXIT, vcpu);
+ }
+ 
+ static int synic_set_msr(struct kvm_vcpu_hv_synic *synic,
+ 			 u32 msr, u64 data, bool host)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	trace_kvm_hv_synic_set_msr(vcpu->vcpu_id, msr, data, host);
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		synic->control = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		if (!host) {
+ 			ret = 1;
+ 			break;
+ 		}
+ 		synic->version = data;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		if ((data & HV_SYNIC_SIEFP_ENABLE) && !host &&
+ 		    !synic->dont_zero_synic_pages)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->evt_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		if ((data & HV_SYNIC_SIMP_ENABLE) && !host &&
+ 		    !synic->dont_zero_synic_pages)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->msg_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_EOM: {
+ 		int i;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ 		break;
+ 	}
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		ret = synic_set_sint(synic, msr - HV_X64_MSR_SINT0, data, host);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_get_msr(struct kvm_vcpu_hv_synic *synic, u32 msr, u64 *pdata)
+ {
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		*pdata = synic->control;
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		*pdata = synic->version;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		*pdata = synic->evt_page;
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		*pdata = synic->msg_page;
+ 		break;
+ 	case HV_X64_MSR_EOM:
+ 		*pdata = 0;
+ 		break;
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		*pdata = atomic64_read(&synic->sint[msr - HV_X64_MSR_SINT0]);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_lapic_irq irq;
+ 	int ret, vector;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint))
+ 		return -EINVAL;
+ 
+ 	vector = synic_get_sint_vector(synic_read_sint(synic, sint));
+ 	if (vector < 0)
+ 		return -ENOENT;
+ 
+ 	memset(&irq, 0, sizeof(irq));
+ 	irq.shorthand = APIC_DEST_SELF;
+ 	irq.dest_mode = APIC_DEST_PHYSICAL;
+ 	irq.delivery_mode = APIC_DM_FIXED;
+ 	irq.vector = vector;
+ 	irq.level = 1;
+ 
+ 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
+ 	return ret;
+ }
+ 
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vcpu_id, u32 sint)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	return synic_set_irq(synic, sint);
+ }
+ 
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector)
+ {
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	int i;
+ 
+ 	trace_kvm_hv_synic_send_eoi(vcpu->vcpu_id, vector);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ }
+ 
+ static int kvm_hv_set_sint_gsi(struct kvm *kvm, u32 vcpu_id, u32 sint, int gsi)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint_to_gsi))
+ 		return -EINVAL;
+ 
+ 	atomic_set(&synic->sint_to_gsi[sint], gsi);
+ 	return 0;
+ }
+ 
+ void kvm_hv_irq_routing_update(struct kvm *kvm)
+ {
+ 	struct kvm_irq_routing_table *irq_rt;
+ 	struct kvm_kernel_irq_routing_entry *e;
+ 	u32 gsi;
+ 
+ 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+ 					lockdep_is_held(&kvm->irq_lock));
+ 
+ 	for (gsi = 0; gsi < irq_rt->nr_rt_entries; gsi++) {
+ 		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+ 			if (e->type == KVM_IRQ_ROUTING_HV_SINT)
+ 				kvm_hv_set_sint_gsi(kvm, e->hv_sint.vcpu,
+ 						    e->hv_sint.sint, gsi);
+ 		}
+ 	}
+ }
+ 
+ static void synic_init(struct kvm_vcpu_hv_synic *synic)
+ {
+ 	int i;
+ 
+ 	memset(synic, 0, sizeof(*synic));
+ 	synic->version = HV_SYNIC_VERSION_1;
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		atomic64_set(&synic->sint[i], HV_SYNIC_SINT_MASKED);
+ 		atomic_set(&synic->sint_to_gsi[i], -1);
+ 	}
+ }
+ 
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  static u64 get_time_ref_counter(struct kvm *kvm)
  {
  	struct kvm_hv *hv = &kvm->arch.hyperv;
@@@ -49,6 -407,302 +405,305 @@@
  		+ hv->tsc_ref.tsc_offset;
  }
  
++<<<<<<< HEAD
++=======
+ static void stimer_mark_pending(struct kvm_vcpu_hv_stimer *stimer,
+ 				bool vcpu_kick)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 
+ 	set_bit(stimer->index,
+ 		vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
+ 	kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 	if (vcpu_kick)
+ 		kvm_vcpu_kick(vcpu);
+ }
+ 
+ static void stimer_cleanup(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 
+ 	trace_kvm_hv_stimer_cleanup(stimer_to_vcpu(stimer)->vcpu_id,
+ 				    stimer->index);
+ 
+ 	hrtimer_cancel(&stimer->timer);
+ 	clear_bit(stimer->index,
+ 		  vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
+ 	stimer->msg_pending = false;
+ 	stimer->exp_time = 0;
+ }
+ 
+ static enum hrtimer_restart stimer_timer_callback(struct hrtimer *timer)
+ {
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 
+ 	stimer = container_of(timer, struct kvm_vcpu_hv_stimer, timer);
+ 	trace_kvm_hv_stimer_callback(stimer_to_vcpu(stimer)->vcpu_id,
+ 				     stimer->index);
+ 	stimer_mark_pending(stimer, true);
+ 
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ /*
+  * stimer_start() assumptions:
+  * a) stimer->count is not equal to 0
+  * b) stimer->config has HV_STIMER_ENABLE flag
+  */
+ static int stimer_start(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	u64 time_now;
+ 	ktime_t ktime_now;
+ 
+ 	time_now = get_time_ref_counter(stimer_to_vcpu(stimer)->kvm);
+ 	ktime_now = ktime_get();
+ 
+ 	if (stimer->config & HV_STIMER_PERIODIC) {
+ 		if (stimer->exp_time) {
+ 			if (time_now >= stimer->exp_time) {
+ 				u64 remainder;
+ 
+ 				div64_u64_rem(time_now - stimer->exp_time,
+ 					      stimer->count, &remainder);
+ 				stimer->exp_time =
+ 					time_now + (stimer->count - remainder);
+ 			}
+ 		} else
+ 			stimer->exp_time = time_now + stimer->count;
+ 
+ 		trace_kvm_hv_stimer_start_periodic(
+ 					stimer_to_vcpu(stimer)->vcpu_id,
+ 					stimer->index,
+ 					time_now, stimer->exp_time);
+ 
+ 		hrtimer_start(&stimer->timer,
+ 			      ktime_add_ns(ktime_now,
+ 					   100 * (stimer->exp_time - time_now)),
+ 			      HRTIMER_MODE_ABS);
+ 		return 0;
+ 	}
+ 	stimer->exp_time = stimer->count;
+ 	if (time_now >= stimer->count) {
+ 		/*
+ 		 * Expire timer according to Hypervisor Top-Level Functional
+ 		 * specification v4(15.3.1):
+ 		 * "If a one shot is enabled and the specified count is in
+ 		 * the past, it will expire immediately."
+ 		 */
+ 		stimer_mark_pending(stimer, false);
+ 		return 0;
+ 	}
+ 
+ 	trace_kvm_hv_stimer_start_one_shot(stimer_to_vcpu(stimer)->vcpu_id,
+ 					   stimer->index,
+ 					   time_now, stimer->count);
+ 
+ 	hrtimer_start(&stimer->timer,
+ 		      ktime_add_ns(ktime_now, 100 * (stimer->count - time_now)),
+ 		      HRTIMER_MODE_ABS);
+ 	return 0;
+ }
+ 
+ static int stimer_set_config(struct kvm_vcpu_hv_stimer *stimer, u64 config,
+ 			     bool host)
+ {
+ 	trace_kvm_hv_stimer_set_config(stimer_to_vcpu(stimer)->vcpu_id,
+ 				       stimer->index, config, host);
+ 
+ 	stimer_cleanup(stimer);
+ 	if ((stimer->config & HV_STIMER_ENABLE) && HV_STIMER_SINT(config) == 0)
+ 		config &= ~HV_STIMER_ENABLE;
+ 	stimer->config = config;
+ 	stimer_mark_pending(stimer, false);
+ 	return 0;
+ }
+ 
+ static int stimer_set_count(struct kvm_vcpu_hv_stimer *stimer, u64 count,
+ 			    bool host)
+ {
+ 	trace_kvm_hv_stimer_set_count(stimer_to_vcpu(stimer)->vcpu_id,
+ 				      stimer->index, count, host);
+ 
+ 	stimer_cleanup(stimer);
+ 	stimer->count = count;
+ 	if (stimer->count == 0)
+ 		stimer->config &= ~HV_STIMER_ENABLE;
+ 	else if (stimer->config & HV_STIMER_AUTOENABLE)
+ 		stimer->config |= HV_STIMER_ENABLE;
+ 	stimer_mark_pending(stimer, false);
+ 	return 0;
+ }
+ 
+ static int stimer_get_config(struct kvm_vcpu_hv_stimer *stimer, u64 *pconfig)
+ {
+ 	*pconfig = stimer->config;
+ 	return 0;
+ }
+ 
+ static int stimer_get_count(struct kvm_vcpu_hv_stimer *stimer, u64 *pcount)
+ {
+ 	*pcount = stimer->count;
+ 	return 0;
+ }
+ 
+ static int synic_deliver_msg(struct kvm_vcpu_hv_synic *synic, u32 sint,
+ 			     struct hv_message *src_msg)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *dst_msg;
+ 	int r;
+ 	struct hv_message_page *msg_page;
+ 
+ 	if (!(synic->msg_page & HV_SYNIC_SIMP_ENABLE))
+ 		return -ENOENT;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page))
+ 		return -EFAULT;
+ 
+ 	msg_page = kmap_atomic(page);
+ 	dst_msg = &msg_page->sint_message[sint];
+ 	if (sync_cmpxchg(&dst_msg->header.message_type, HVMSG_NONE,
+ 			 src_msg->header.message_type) != HVMSG_NONE) {
+ 		dst_msg->header.message_flags.msg_pending = 1;
+ 		r = -EAGAIN;
+ 	} else {
+ 		memcpy(&dst_msg->u.payload, &src_msg->u.payload,
+ 		       src_msg->header.payload_size);
+ 		dst_msg->header.message_type = src_msg->header.message_type;
+ 		dst_msg->header.payload_size = src_msg->header.payload_size;
+ 		r = synic_set_irq(synic, sint);
+ 		if (r >= 1)
+ 			r = 0;
+ 		else if (r == 0)
+ 			r = -EFAULT;
+ 	}
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ 	return r;
+ }
+ 
+ static int stimer_send_msg(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 	struct hv_message *msg = &stimer->msg;
+ 	struct hv_timer_message_payload *payload =
+ 			(struct hv_timer_message_payload *)&msg->u.payload;
+ 
+ 	payload->expiration_time = stimer->exp_time;
+ 	payload->delivery_time = get_time_ref_counter(vcpu->kvm);
+ 	return synic_deliver_msg(vcpu_to_synic(vcpu),
+ 				 HV_STIMER_SINT(stimer->config), msg);
+ }
+ 
+ static void stimer_expiration(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	int r;
+ 
+ 	stimer->msg_pending = true;
+ 	r = stimer_send_msg(stimer);
+ 	trace_kvm_hv_stimer_expiration(stimer_to_vcpu(stimer)->vcpu_id,
+ 				       stimer->index, r);
+ 	if (!r) {
+ 		stimer->msg_pending = false;
+ 		if (!(stimer->config & HV_STIMER_PERIODIC))
+ 			stimer->config &= ~HV_STIMER_ENABLE;
+ 	}
+ }
+ 
+ void kvm_hv_process_stimers(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	u64 time_now, exp_time;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		if (test_and_clear_bit(i, hv_vcpu->stimer_pending_bitmap)) {
+ 			stimer = &hv_vcpu->stimer[i];
+ 			if (stimer->config & HV_STIMER_ENABLE) {
+ 				exp_time = stimer->exp_time;
+ 
+ 				if (exp_time) {
+ 					time_now =
+ 						get_time_ref_counter(vcpu->kvm);
+ 					if (time_now >= exp_time)
+ 						stimer_expiration(stimer);
+ 				}
+ 
+ 				if ((stimer->config & HV_STIMER_ENABLE) &&
+ 				    stimer->count)
+ 					stimer_start(stimer);
+ 				else
+ 					stimer_cleanup(stimer);
+ 			}
+ 		}
+ }
+ 
+ void kvm_hv_vcpu_uninit(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		stimer_cleanup(&hv_vcpu->stimer[i]);
+ }
+ 
+ static void stimer_prepare_msg(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct hv_message *msg = &stimer->msg;
+ 	struct hv_timer_message_payload *payload =
+ 			(struct hv_timer_message_payload *)&msg->u.payload;
+ 
+ 	memset(&msg->header, 0, sizeof(msg->header));
+ 	msg->header.message_type = HVMSG_TIMER_EXPIRED;
+ 	msg->header.payload_size = sizeof(*payload);
+ 
+ 	payload->timer_index = stimer->index;
+ 	payload->expiration_time = 0;
+ 	payload->delivery_time = 0;
+ }
+ 
+ static void stimer_init(struct kvm_vcpu_hv_stimer *stimer, int timer_index)
+ {
+ 	memset(stimer, 0, sizeof(*stimer));
+ 	stimer->index = timer_index;
+ 	hrtimer_init(&stimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+ 	stimer->timer.function = stimer_timer_callback;
+ 	stimer_prepare_msg(stimer);
+ }
+ 
+ void kvm_hv_vcpu_init(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	int i;
+ 
+ 	synic_init(&hv_vcpu->synic);
+ 
+ 	bitmap_zero(hv_vcpu->stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		stimer_init(&hv_vcpu->stimer[i], i);
+ }
+ 
+ int kvm_hv_activate_synic(struct kvm_vcpu *vcpu, bool dont_zero_synic_pages)
+ {
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 
+ 	/*
+ 	 * Hyper-V SynIC auto EOI SINT's are
+ 	 * not compatible with APICV, so deactivate APICV
+ 	 */
+ 	kvm_vcpu_deactivate_apicv(vcpu);
+ 	synic->active = true;
+ 	synic->dont_zero_synic_pages = dont_zero_synic_pages;
+ 	return 0;
+ }
+ 
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  static bool kvm_hv_msr_partition_wide(u32 msr)
  {
  	bool r = false;
diff --cc arch/x86/kvm/hyperv.h
index 638d4fbc7358,12f65fe1011d..000000000000
--- a/arch/x86/kvm/hyperv.h
+++ b/arch/x86/kvm/hyperv.h
@@@ -29,6 -52,38 +29,40 @@@ int kvm_hv_get_msr_common(struct kvm_vc
  bool kvm_hv_hypercall_enabled(struct kvm *kvm);
  int kvm_hv_hypercall(struct kvm_vcpu *vcpu);
  
++<<<<<<< HEAD
++=======
+ void kvm_hv_irq_routing_update(struct kvm *kvm);
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vcpu_id, u32 sint);
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector);
+ int kvm_hv_activate_synic(struct kvm_vcpu *vcpu, bool dont_zero_synic_pages);
+ 
+ void kvm_hv_vcpu_init(struct kvm_vcpu *vcpu);
+ void kvm_hv_vcpu_uninit(struct kvm_vcpu *vcpu);
+ 
+ static inline struct kvm_vcpu_hv_stimer *vcpu_to_stimer(struct kvm_vcpu *vcpu,
+ 							int timer_index)
+ {
+ 	return &vcpu_to_hv_vcpu(vcpu)->stimer[timer_index];
+ }
+ 
+ static inline struct kvm_vcpu *stimer_to_vcpu(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu;
+ 
+ 	hv_vcpu = container_of(stimer - stimer->index, struct kvm_vcpu_hv,
+ 			       stimer[0]);
+ 	return hv_vcpu_to_vcpu(hv_vcpu);
+ }
+ 
+ static inline bool kvm_hv_has_stimer_pending(struct kvm_vcpu *vcpu)
+ {
+ 	return !bitmap_empty(vcpu->arch.hyperv.stimer_pending_bitmap,
+ 			     HV_SYNIC_STIMER_COUNT);
+ }
+ 
+ void kvm_hv_process_stimers(struct kvm_vcpu *vcpu);
+ 
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  void kvm_hv_setup_tsc_page(struct kvm *kvm,
  			   struct pvclock_vcpu_time_info *hv_clock);
  
diff --cc arch/x86/kvm/x86.c
index b00aaf0d573a,4f41c5222ecd..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -2606,6 -2658,8 +2606,11 @@@ int kvm_vm_ioctl_check_extension(struc
  	case KVM_CAP_HYPERV:
  	case KVM_CAP_HYPERV_VAPIC:
  	case KVM_CAP_HYPERV_SPIN:
++<<<<<<< HEAD
++=======
+ 	case KVM_CAP_HYPERV_SYNIC:
+ 	case KVM_CAP_HYPERV_SYNIC2:
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  	case KVM_CAP_PCI_SEGMENT:
  	case KVM_CAP_DEBUGREGS:
  	case KVM_CAP_X86_ROBUST_SINGLESTEP:
@@@ -3311,6 -3376,26 +3316,29 @@@ static int kvm_set_guest_paused(struct 
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
+ 				     struct kvm_enable_cap *cap)
+ {
+ 	if (cap->flags)
+ 		return -EINVAL;
+ 
+ 	switch (cap->cap) {
+ 	case KVM_CAP_HYPERV_SYNIC2:
+ 		if (cap->args[0])
+ 			return -EINVAL;
+ 	case KVM_CAP_HYPERV_SYNIC:
+ 		if (!irqchip_in_kernel(vcpu->kvm))
+ 			return -EINVAL;
+ 		return kvm_hv_activate_synic(vcpu, cap->cap ==
+ 					     KVM_CAP_HYPERV_SYNIC2);
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  long kvm_arch_vcpu_ioctl(struct file *filp,
  			 unsigned int ioctl, unsigned long arg)
  {
diff --cc include/uapi/linux/kvm.h
index 6b525ccf5361,38b2cfbc8112..000000000000
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@@ -709,10 -902,32 +709,28 @@@ struct kvm_ppc_resize_hpt 
  #define KVM_CAP_GUEST_DEBUG_HW_WPS 120
  #define KVM_CAP_SPLIT_IRQCHIP 121
  #define KVM_CAP_IOEVENTFD_ANY_LENGTH 122
 -#define KVM_CAP_HYPERV_SYNIC 123
 -#define KVM_CAP_S390_RI 124
 -#define KVM_CAP_SPAPR_TCE_64 125
 -#define KVM_CAP_ARM_PMU_V3 126
 -#define KVM_CAP_VCPU_ATTRIBUTES 127
  #define KVM_CAP_MAX_VCPU_ID 128
  #define KVM_CAP_X2APIC_API 129
 -#define KVM_CAP_S390_USER_INSTR0 130
 -#define KVM_CAP_MSI_DEVID 131
 -#define KVM_CAP_PPC_HTM 132
  #define KVM_CAP_SPAPR_RESIZE_HPT 133
++<<<<<<< HEAD
 +#define KVM_CAP_PPC_GET_CPU_CHAR 151
++=======
+ #define KVM_CAP_PPC_MMU_RADIX 134
+ #define KVM_CAP_PPC_MMU_HASH_V3 135
+ #define KVM_CAP_IMMEDIATE_EXIT 136
+ #define KVM_CAP_MIPS_VZ 137
+ #define KVM_CAP_MIPS_TE 138
+ #define KVM_CAP_MIPS_64BIT 139
+ #define KVM_CAP_S390_GS 140
+ #define KVM_CAP_S390_AIS 141
+ #define KVM_CAP_SPAPR_TCE_VFIO 142
+ #define KVM_CAP_X86_GUEST_MWAIT 143
+ #define KVM_CAP_ARM_USER_IRQ 144
+ #define KVM_CAP_S390_CMMA_MIGRATION 145
+ #define KVM_CAP_PPC_FWNMI 146
+ #define KVM_CAP_PPC_SMT_POSSIBLE 147
+ #define KVM_CAP_HYPERV_SYNIC2 148
++>>>>>>> efc479e6900c (kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2)
  
  #ifdef KVM_CAP_IRQ_ROUTING
  
* Unmerged path Documentation/virtual/kvm/api.txt
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/hyperv.c
* Unmerged path arch/x86/kvm/hyperv.h
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path include/uapi/linux/kvm.h
