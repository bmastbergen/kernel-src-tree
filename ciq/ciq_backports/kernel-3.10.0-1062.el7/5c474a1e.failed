s390/mm: introduce ptep_flush_lazy helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [s390] mm: introduce ptep_flush_lazy helper (Aaron Tomlin) [1677343]
Rebuild_FUZZ: 93.51%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 5c474a1e2265c5156e6c63f87a7e99053039b8b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5c474a1e.failed

Isolate the logic of IDTE vs. IPTE flushing of ptes in two functions,
ptep_flush_lazy and __tlb_flush_mm_lazy.

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 5c474a1e2265c5156e6c63f87a7e99053039b8b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/pgtable.h
diff --cc arch/s390/include/asm/pgtable.h
index b22f965817c2,125e37909998..000000000000
--- a/arch/s390/include/asm/pgtable.h
+++ b/arch/s390/include/asm/pgtable.h
@@@ -492,88 -411,9 +492,82 @@@ static inline void csp(unsigned int *pt
   */
  #define SEGMENT_NONE	__pgprot(_SEGMENT_ENTRY_INVALID | \
  				 _SEGMENT_ENTRY_PROTECT)
 -#define SEGMENT_READ	__pgprot(_SEGMENT_ENTRY_PROTECT)
 -#define SEGMENT_WRITE	__pgprot(0)
 +#define SEGMENT_RO	__pgprot(_SEGMENT_ENTRY_PROTECT | \
 +				 _SEGMENT_ENTRY_READ | \
 +				 _SEGMENT_ENTRY_NOEXEC)
 +#define SEGMENT_RX	__pgprot(_SEGMENT_ENTRY_PROTECT | \
 +				 _SEGMENT_ENTRY_READ)
 +#define SEGMENT_RW	__pgprot(_SEGMENT_ENTRY_READ | \
 +				 _SEGMENT_ENTRY_WRITE | \
 +				 _SEGMENT_ENTRY_NOEXEC)
 +#define SEGMENT_RWX	__pgprot(_SEGMENT_ENTRY_READ | \
 +				 _SEGMENT_ENTRY_WRITE)
 +#define SEGMENT_KERNEL	__pgprot(_SEGMENT_ENTRY |	\
 +				 _SEGMENT_ENTRY_LARGE |	\
 +				 _SEGMENT_ENTRY_READ |	\
 +				 _SEGMENT_ENTRY_WRITE | \
 +				 _SEGMENT_ENTRY_YOUNG | \
 +				 _SEGMENT_ENTRY_DIRTY | \
 +				 _SEGMENT_ENTRY_NOEXEC)
 +#define SEGMENT_KERNEL_RO __pgprot(_SEGMENT_ENTRY |	\
 +				 _SEGMENT_ENTRY_LARGE |	\
 +				 _SEGMENT_ENTRY_READ |	\
 +				 _SEGMENT_ENTRY_YOUNG |	\
 +				 _SEGMENT_ENTRY_PROTECT | \
 +				 _SEGMENT_ENTRY_NOEXEC)
 +
 +static inline void cspg(unsigned long *ptr, unsigned long old, unsigned long new)
 +{
 +	register unsigned long reg2 asm("2") = old;
 +	register unsigned long reg3 asm("3") = new;
 +	unsigned long address = (unsigned long)ptr | 1;
 +
 +	asm volatile(
 +		"	.insn	rre,0xb98a0000,%0,%3"
 +		: "+d" (reg2), "+m" (*ptr)
 +		: "d" (reg3), "d" (address)
 +		: "cc");
 +}
 +
 +#define CRDTE_DTT_PAGE		0x00UL
 +#define CRDTE_DTT_SEGMENT	0x10UL
 +#define CRDTE_DTT_REGION3	0x14UL
 +#define CRDTE_DTT_REGION2	0x18UL
 +#define CRDTE_DTT_REGION1	0x1cUL
 +
 +static inline void crdte(unsigned long old, unsigned long new,
 +			 unsigned long table, unsigned long dtt,
 +			 unsigned long address, unsigned long asce)
 +{
 +	register unsigned long reg2 asm("2") = old;
 +	register unsigned long reg3 asm("3") = new;
 +	register unsigned long reg4 asm("4") = table | dtt;
 +	register unsigned long reg5 asm("5") = address;
 +
 +	asm volatile(".insn rrf,0xb98f0000,%0,%2,%4,0"
 +		     : "+d" (reg2)
 +		     : "d" (reg3), "d" (reg4), "d" (reg5), "a" (asce)
 +		     : "memory", "cc");
 +}
 +
 +/*
 + * Region3 entry (large page) protection definitions.
 + */
 +
 +#define REGION3_KERNEL	__pgprot(_REGION_ENTRY_TYPE_R3 | \
 +				 _REGION3_ENTRY_LARGE |	 \
 +				 _REGION3_ENTRY_READ |	 \
 +				 _REGION3_ENTRY_WRITE |	 \
 +				 _REGION3_ENTRY_YOUNG |	 \
 +				 _REGION3_ENTRY_DIRTY | \
 +				 _REGION_ENTRY_NOEXEC)
 +#define REGION3_KERNEL_RO __pgprot(_REGION_ENTRY_TYPE_R3 | \
 +				   _REGION3_ENTRY_LARGE |  \
 +				   _REGION3_ENTRY_READ |   \
 +				   _REGION3_ENTRY_YOUNG |  \
 +				   _REGION_ENTRY_PROTECT | \
 +				   _REGION_ENTRY_NOEXEC)
  
- static inline int mm_exclusive(struct mm_struct *mm)
- {
- 	return likely(mm == current->active_mm &&
- 		      atomic_read(&mm->context.attach_count) <= 1);
- }
- 
  static inline int mm_has_pgste(struct mm_struct *mm)
  {
  #ifdef CONFIG_PGSTE
@@@ -1171,54 -1031,15 +1165,66 @@@ static inline void __ptep_ipte(unsigne
  	}
  }
  
++<<<<<<< HEAD
 +static inline void __ptep_ipte_range(unsigned long address, int nr, pte_t *ptep)
 +{
 +	unsigned long pto = (unsigned long) ptep;
 +
 +#ifndef CONFIG_64BIT
 +	/* pto in ESA mode must point to the start of the segment table */
 +	pto &= 0x7ffffc00;
 +#endif
 +	/* Invalidate a range of ptes + global TLB flush of the ptes */
 +	do {
 +		asm volatile(
 +			"	.insn rrf,0xb2210000,%2,%0,%1,0"
 +			: "+a" (address), "+a" (nr) : "a" (pto) : "memory");
 +	} while (nr != 255);
 +}
 +
 +#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
 +static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
 +					    unsigned long addr, pte_t *ptep)
 +{
 +	pgste_t pgste;
 +	pte_t pte;
 +	int young;
 +
 +	if (mm_has_pgste(vma->vm_mm)) {
 +		pgste = pgste_get_lock(ptep);
 +		pgste = pgste_ipte_notify(vma->vm_mm, addr, ptep, pgste);
 +	}
 +
 +	pte = *ptep;
 +	__ptep_ipte(addr, ptep);
 +	young = pte_young(pte);
 +	pte = pte_mkold(pte);
 +
 +	if (mm_has_pgste(vma->vm_mm)) {
 +		pgste_set_pte(ptep, pte);
 +		pgste_set_unlock(ptep, pgste);
 +	} else
 +		*ptep = pte;
 +
 +	return young;
 +}
 +
 +#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH
 +static inline int ptep_clear_flush_young(struct vm_area_struct *vma,
 +					 unsigned long address, pte_t *ptep)
 +{
 +	return ptep_test_and_clear_young(vma, address, ptep);
++=======
+ static inline void ptep_flush_lazy(struct mm_struct *mm,
+ 				   unsigned long address, pte_t *ptep)
+ {
+ 	int active = (mm == current->active_mm) ? 1 : 0;
+ 
+ 	if (atomic_read(&mm->context.attach_count) > active)
+ 		__ptep_ipte(address, ptep);
+ 	else
+ 		mm->context.flush_mm = 1;
++>>>>>>> 5c474a1e2265 (s390/mm: introduce ptep_flush_lazy helper)
  }
  
  /*
@@@ -1347,10 -1161,10 +1349,10 @@@ static inline pte_t ptep_get_and_clear_
  
  	pte = *ptep;
  	if (!full)
- 		__ptep_ipte(address, ptep);
+ 		ptep_flush_lazy(mm, address, ptep);
  	pte_val(*ptep) = _PAGE_INVALID;
  
 -	if (!full && mm_has_pgste(mm)) {
 +	if (mm_has_pgste(mm)) {
  		pgste = pgste_update_all(&pte, pgste);
  		pgste_set_unlock(ptep, pgste);
  	}
diff --git a/arch/s390/include/asm/mmu_context.h b/arch/s390/include/asm/mmu_context.h
index 57a6373c2555..9874627895a2 100644
--- a/arch/s390/include/asm/mmu_context.h
+++ b/arch/s390/include/asm/mmu_context.h
@@ -75,8 +75,7 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	WARN_ON(atomic_read(&prev->context.attach_count) < 0);
 	atomic_inc(&next->context.attach_count);
 	/* Check for TLBs not flushed yet */
-	if (next->context.flush_mm)
-		__tlb_flush_mm(next);
+	__tlb_flush_mm_lazy(next);
 }
 
 #define enter_lazy_tlb(mm,tsk)	do { } while (0)
* Unmerged path arch/s390/include/asm/pgtable.h
diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h
index 6d6d92b4ea11..2cb846c4b37f 100644
--- a/arch/s390/include/asm/tlb.h
+++ b/arch/s390/include/asm/tlb.h
@@ -63,13 +63,14 @@ static inline void tlb_gather_mmu(struct mmu_gather *tlb,
 
 static inline void tlb_flush_mmu(struct mmu_gather *tlb)
 {
+	__tlb_flush_mm_lazy(tlb->mm);
 	tlb_table_flush(tlb);
 }
 
 static inline void tlb_finish_mmu(struct mmu_gather *tlb,
 				  unsigned long start, unsigned long end)
 {
-	tlb_table_flush(tlb);
+	tlb_flush_mmu(tlb);
 }
 
 /*
diff --git a/arch/s390/include/asm/tlbflush.h b/arch/s390/include/asm/tlbflush.h
index d41a81016f6d..61a4ae1ca0c5 100644
--- a/arch/s390/include/asm/tlbflush.h
+++ b/arch/s390/include/asm/tlbflush.h
@@ -71,7 +71,7 @@ static inline void __tlb_flush_mm(struct mm_struct * mm)
 		__tlb_flush_full(mm);
 }
 
-static inline void __tlb_flush_mm_cond(struct mm_struct * mm)
+static inline void __tlb_flush_mm_lazy(struct mm_struct * mm)
 {
 	if (mm->context.flush_mm) {
 		__tlb_flush_mm(mm);
@@ -103,13 +103,13 @@ static inline void __tlb_flush_mm_cond(struct mm_struct * mm)
 
 static inline void flush_tlb_mm(struct mm_struct *mm)
 {
-	__tlb_flush_mm_cond(mm);
+	__tlb_flush_mm_lazy(mm);
 }
 
 static inline void flush_tlb_range(struct vm_area_struct *vma,
 				   unsigned long start, unsigned long end)
 {
-	__tlb_flush_mm_cond(vma->vm_mm);
+	__tlb_flush_mm_lazy(vma->vm_mm);
 }
 
 static inline void flush_tlb_kernel_range(unsigned long start,
diff --git a/arch/s390/mm/pgtable.c b/arch/s390/mm/pgtable.c
index 51db8b210e5f..669331a268cd 100644
--- a/arch/s390/mm/pgtable.c
+++ b/arch/s390/mm/pgtable.c
@@ -1014,7 +1014,6 @@ void tlb_table_flush(struct mmu_gather *tlb)
 	struct mmu_table_batch **batch = &tlb->batch;
 
 	if (*batch) {
-		__tlb_flush_mm(tlb->mm);
 		call_rcu_sched(&(*batch)->rcu, tlb_remove_table_rcu);
 		*batch = NULL;
 	}
@@ -1024,11 +1023,12 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 {
 	struct mmu_table_batch **batch = &tlb->batch;
 
+	tlb->mm->context.flush_mm = 1;
 	if (*batch == NULL) {
 		*batch = (struct mmu_table_batch *)
 			__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
 		if (*batch == NULL) {
-			__tlb_flush_mm(tlb->mm);
+			__tlb_flush_mm_lazy(tlb->mm);
 			tlb_remove_table_one(table);
 			return;
 		}
@@ -1036,7 +1036,7 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 	}
 	(*batch)->tables[(*batch)->nr++] = table;
 	if ((*batch)->nr == MAX_TABLE_BATCH)
-		tlb_table_flush(tlb);
+		tlb_flush_mmu(tlb);
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
