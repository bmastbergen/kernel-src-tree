tcp/dccp: fix another race at listener dismantle

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 7716682cc58e305e22207d5bb315f26af6b1e243
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/7716682c.failed

Ilya reported following lockdep splat:

kernel: =========================
kernel: [ BUG: held lock freed! ]
kernel: 4.5.0-rc1-ceph-00026-g5e0a311 #1 Not tainted
kernel: -------------------------
kernel: swapper/5/0 is freeing memory
ffff880035c9d200-ffff880035c9dbff, with a lock still held there!
kernel: (&(&queue->rskq_lock)->rlock){+.-...}, at:
[<ffffffff816f6a88>] inet_csk_reqsk_queue_add+0x28/0xa0
kernel: 4 locks held by swapper/5/0:
kernel: #0:  (rcu_read_lock){......}, at: [<ffffffff8169ef6b>]
netif_receive_skb_internal+0x4b/0x1f0
kernel: #1:  (rcu_read_lock){......}, at: [<ffffffff816e977f>]
ip_local_deliver_finish+0x3f/0x380
kernel: #2:  (slock-AF_INET){+.-...}, at: [<ffffffff81685ffb>]
sk_clone_lock+0x19b/0x440
kernel: #3:  (&(&queue->rskq_lock)->rlock){+.-...}, at:
[<ffffffff816f6a88>] inet_csk_reqsk_queue_add+0x28/0xa0

To properly fix this issue, inet_csk_reqsk_queue_add() needs
to return to its callers if the child as been queued
into accept queue.

We also need to make sure listener is still there before
calling sk->sk_data_ready(), by holding a reference on it,
since the reference carried by the child can disappear as
soon as the child is put on accept queue.

	Reported-by: Ilya Dryomov <idryomov@gmail.com>
Fixes: ebb516af60e1 ("tcp/dccp: fix race at listener dismantle phase")
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7716682cc58e305e22207d5bb315f26af6b1e243)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/inet_connection_sock.h
#	net/dccp/ipv4.c
#	net/dccp/ipv6.c
#	net/ipv4/inet_connection_sock.c
#	net/ipv4/tcp_ipv4.c
#	net/ipv6/tcp_ipv6.c
diff --cc include/net/inet_connection_sock.h
index 06d0d0fec202,49dcad4fe99e..000000000000
--- a/include/net/inet_connection_sock.h
+++ b/include/net/inet_connection_sock.h
@@@ -264,33 -264,24 +264,39 @@@ int inet_csk_bind_conflict(const struc
  			   const struct inet_bind_bucket *tb, bool relax);
  int inet_csk_get_port(struct sock *sk, unsigned short snum);
  
 -struct dst_entry *inet_csk_route_req(const struct sock *sk, struct flowi4 *fl4,
 +struct dst_entry *inet_csk_route_req(struct sock *sk, struct flowi4 *fl4,
  				     const struct request_sock *req);
 -struct dst_entry *inet_csk_route_child_sock(const struct sock *sk,
 -					    struct sock *newsk,
 +struct dst_entry *inet_csk_route_child_sock(struct sock *sk, struct sock *newsk,
  					    const struct request_sock *req);
  
++<<<<<<< HEAD
 +static inline void inet_csk_reqsk_queue_add(struct sock *sk,
 +					    struct request_sock *req,
 +					    struct sock *child)
 +{
 +	reqsk_queue_add(&inet_csk(sk)->icsk_accept_queue, req, sk, child);
 +}
 +
++=======
+ struct sock *inet_csk_reqsk_queue_add(struct sock *sk,
+ 				      struct request_sock *req,
+ 				      struct sock *child);
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  void inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,
  				   unsigned long timeout);
 -struct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,
 -					 struct request_sock *req,
 -					 bool own_req);
  
 -static inline void inet_csk_reqsk_queue_added(struct sock *sk)
 +static inline void inet_csk_reqsk_queue_removed(struct sock *sk,
 +						struct request_sock *req)
 +{
 +	if (reqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req) == 0)
 +		inet_csk_delete_keepalive_timer(sk);
 +}
 +
 +static inline void inet_csk_reqsk_queue_added(struct sock *sk,
 +					      const unsigned long timeout)
  {
 -	reqsk_queue_added(&inet_csk(sk)->icsk_accept_queue);
 +	if (reqsk_queue_added(&inet_csk(sk)->icsk_accept_queue) == 0)
 +		inet_csk_reset_keepalive_timer(sk, timeout);
  }
  
  static inline int inet_csk_reqsk_queue_len(const struct sock *sk)
diff --cc net/dccp/ipv4.c
index 167a59e2d662,902d606324a0..000000000000
--- a/net/dccp/ipv4.c
+++ b/net/dccp/ipv4.c
@@@ -873,6 -822,31 +873,34 @@@ static int dccp_v4_rcv(struct sk_buff *
  		goto no_dccp_socket;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (sk->sk_state == DCCP_NEW_SYN_RECV) {
+ 		struct request_sock *req = inet_reqsk(sk);
+ 		struct sock *nsk;
+ 
+ 		sk = req->rsk_listener;
+ 		if (unlikely(sk->sk_state != DCCP_LISTEN)) {
+ 			inet_csk_reqsk_queue_drop_and_put(sk, req);
+ 			goto lookup;
+ 		}
+ 		sock_hold(sk);
+ 		nsk = dccp_check_req(sk, skb, req);
+ 		if (!nsk) {
+ 			reqsk_put(req);
+ 			goto discard_and_relse;
+ 		}
+ 		if (nsk == sk) {
+ 			reqsk_put(req);
+ 		} else if (dccp_child_process(sk, nsk, skb)) {
+ 			dccp_v4_ctl_send_reset(sk, skb);
+ 			goto discard_and_relse;
+ 		} else {
+ 			sock_put(sk);
+ 			return 0;
+ 		}
+ 	}
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  	/*
  	 * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage
  	 *	o if MinCsCov = 0, only packets with CsCov = 0 are accepted
diff --cc net/dccp/ipv6.c
index 7d38ad8ed2e9,b8608b71a66d..000000000000
--- a/net/dccp/ipv6.c
+++ b/net/dccp/ipv6.c
@@@ -796,6 -689,31 +796,34 @@@ static int dccp_v6_rcv(struct sk_buff *
  		goto no_dccp_socket;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (sk->sk_state == DCCP_NEW_SYN_RECV) {
+ 		struct request_sock *req = inet_reqsk(sk);
+ 		struct sock *nsk;
+ 
+ 		sk = req->rsk_listener;
+ 		if (unlikely(sk->sk_state != DCCP_LISTEN)) {
+ 			inet_csk_reqsk_queue_drop_and_put(sk, req);
+ 			goto lookup;
+ 		}
+ 		sock_hold(sk);
+ 		nsk = dccp_check_req(sk, skb, req);
+ 		if (!nsk) {
+ 			reqsk_put(req);
+ 			goto discard_and_relse;
+ 		}
+ 		if (nsk == sk) {
+ 			reqsk_put(req);
+ 		} else if (dccp_child_process(sk, nsk, skb)) {
+ 			dccp_v6_ctl_send_reset(sk, skb);
+ 			goto discard_and_relse;
+ 		} else {
+ 			sock_put(sk);
+ 			return 0;
+ 		}
+ 	}
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  	/*
  	 * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage
  	 *	o if MinCsCov = 0, only packets with CsCov = 0 are accepted
diff --cc net/ipv4/inet_connection_sock.c
index 3f406790278d,64148914803a..000000000000
--- a/net/ipv4/inet_connection_sock.c
+++ b/net/ipv4/inet_connection_sock.c
@@@ -789,6 -764,72 +789,75 @@@ int inet_csk_listen_start(struct sock *
  }
  EXPORT_SYMBOL_GPL(inet_csk_listen_start);
  
++<<<<<<< HEAD
++=======
+ static void inet_child_forget(struct sock *sk, struct request_sock *req,
+ 			      struct sock *child)
+ {
+ 	sk->sk_prot->disconnect(child, O_NONBLOCK);
+ 
+ 	sock_orphan(child);
+ 
+ 	percpu_counter_inc(sk->sk_prot->orphan_count);
+ 
+ 	if (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->tfo_listener) {
+ 		BUG_ON(tcp_sk(child)->fastopen_rsk != req);
+ 		BUG_ON(sk != req->rsk_listener);
+ 
+ 		/* Paranoid, to prevent race condition if
+ 		 * an inbound pkt destined for child is
+ 		 * blocked by sock lock in tcp_v4_rcv().
+ 		 * Also to satisfy an assertion in
+ 		 * tcp_v4_destroy_sock().
+ 		 */
+ 		tcp_sk(child)->fastopen_rsk = NULL;
+ 	}
+ 	inet_csk_destroy_sock(child);
+ 	reqsk_put(req);
+ }
+ 
+ struct sock *inet_csk_reqsk_queue_add(struct sock *sk,
+ 				      struct request_sock *req,
+ 				      struct sock *child)
+ {
+ 	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
+ 
+ 	spin_lock(&queue->rskq_lock);
+ 	if (unlikely(sk->sk_state != TCP_LISTEN)) {
+ 		inet_child_forget(sk, req, child);
+ 		child = NULL;
+ 	} else {
+ 		req->sk = child;
+ 		req->dl_next = NULL;
+ 		if (queue->rskq_accept_head == NULL)
+ 			queue->rskq_accept_head = req;
+ 		else
+ 			queue->rskq_accept_tail->dl_next = req;
+ 		queue->rskq_accept_tail = req;
+ 		sk_acceptq_added(sk);
+ 	}
+ 	spin_unlock(&queue->rskq_lock);
+ 	return child;
+ }
+ EXPORT_SYMBOL(inet_csk_reqsk_queue_add);
+ 
+ struct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,
+ 					 struct request_sock *req, bool own_req)
+ {
+ 	if (own_req) {
+ 		inet_csk_reqsk_queue_drop(sk, req);
+ 		reqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);
+ 		if (inet_csk_reqsk_queue_add(sk, req, child))
+ 			return child;
+ 	}
+ 	/* Too bad, another child took ownership of the request, undo. */
+ 	bh_unlock_sock(child);
+ 	sock_put(child);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(inet_csk_complete_hashdance);
+ 
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  /*
   *	This routine closes sockets which have been at least partially
   *	opened, but not yet accepted.
diff --cc net/ipv4/tcp_ipv4.c
index b564e298eb60,487ac67059e2..000000000000
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@@ -1696,6 -1595,35 +1696,38 @@@ process
  	if (sk->sk_state == TCP_TIME_WAIT)
  		goto do_time_wait;
  
++<<<<<<< HEAD
++=======
+ 	if (sk->sk_state == TCP_NEW_SYN_RECV) {
+ 		struct request_sock *req = inet_reqsk(sk);
+ 		struct sock *nsk;
+ 
+ 		sk = req->rsk_listener;
+ 		if (unlikely(tcp_v4_inbound_md5_hash(sk, skb))) {
+ 			reqsk_put(req);
+ 			goto discard_it;
+ 		}
+ 		if (unlikely(sk->sk_state != TCP_LISTEN)) {
+ 			inet_csk_reqsk_queue_drop_and_put(sk, req);
+ 			goto lookup;
+ 		}
+ 		sock_hold(sk);
+ 		nsk = tcp_check_req(sk, skb, req, false);
+ 		if (!nsk) {
+ 			reqsk_put(req);
+ 			goto discard_and_relse;
+ 		}
+ 		if (nsk == sk) {
+ 			reqsk_put(req);
+ 		} else if (tcp_child_process(sk, nsk, skb)) {
+ 			tcp_v4_send_reset(nsk, skb);
+ 			goto discard_and_relse;
+ 		} else {
+ 			sock_put(sk);
+ 			return 0;
+ 		}
+ 	}
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  	if (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {
  		NET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);
  		goto discard_and_relse;
diff --cc net/ipv6/tcp_ipv6.c
index 644e6e01bdc9,5c8c84273028..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -1443,6 -1385,37 +1443,40 @@@ process
  	if (sk->sk_state == TCP_TIME_WAIT)
  		goto do_time_wait;
  
++<<<<<<< HEAD
++=======
+ 	if (sk->sk_state == TCP_NEW_SYN_RECV) {
+ 		struct request_sock *req = inet_reqsk(sk);
+ 		struct sock *nsk;
+ 
+ 		sk = req->rsk_listener;
+ 		tcp_v6_fill_cb(skb, hdr, th);
+ 		if (tcp_v6_inbound_md5_hash(sk, skb)) {
+ 			reqsk_put(req);
+ 			goto discard_it;
+ 		}
+ 		if (unlikely(sk->sk_state != TCP_LISTEN)) {
+ 			inet_csk_reqsk_queue_drop_and_put(sk, req);
+ 			goto lookup;
+ 		}
+ 		sock_hold(sk);
+ 		nsk = tcp_check_req(sk, skb, req, false);
+ 		if (!nsk) {
+ 			reqsk_put(req);
+ 			goto discard_and_relse;
+ 		}
+ 		if (nsk == sk) {
+ 			reqsk_put(req);
+ 			tcp_v6_restore_cb(skb);
+ 		} else if (tcp_child_process(sk, nsk, skb)) {
+ 			tcp_v6_send_reset(nsk, skb);
+ 			goto discard_and_relse;
+ 		} else {
+ 			sock_put(sk);
+ 			return 0;
+ 		}
+ 	}
++>>>>>>> 7716682cc58e (tcp/dccp: fix another race at listener dismantle)
  	if (hdr->hop_limit < inet6_sk(sk)->min_hopcount) {
  		NET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);
  		goto discard_and_relse;
* Unmerged path include/net/inet_connection_sock.h
* Unmerged path net/dccp/ipv4.c
* Unmerged path net/dccp/ipv6.c
* Unmerged path net/ipv4/inet_connection_sock.c
* Unmerged path net/ipv4/tcp_ipv4.c
* Unmerged path net/ipv6/tcp_ipv6.c
