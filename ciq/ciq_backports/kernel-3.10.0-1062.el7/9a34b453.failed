clk: Add support for runtime PM

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [thunderbolt] Add support for runtime PM (Jarod Wilson) [1588927]
Rebuild_FUZZ: 91.23%
commit-author Marek Szyprowski <m.szyprowski@samsung.com>
commit 9a34b45397e5a389e25a0c5d39983300d040e5e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/9a34b453.failed

Registers for some clocks might be located in the SOC area, which are under the
power domain. To enable access to those registers respective domain has to be
turned on. Additionally, registers for such clocks will usually loose its
contents when power domain is turned off, so additional saving and restoring of
them might be needed in the clock controller driver.

This patch adds basic infrastructure in the clocks core to allow implementing
driver for such clocks under power domains. Clock provider can supply a
struct device pointer, which is the used by clock core for tracking and managing
clock's controller runtime pm state. Each clk_prepare() operation
will first call pm_runtime_get_sync() on the supplied device, while
clk_unprepare() will do pm_runtime_put_sync() at the end.

Additional calls to pm_runtime_get/put functions are required to ensure that any
register access (like calculating/changing clock rates and unpreparing/disabling
unused clocks on boot) will be done with clock controller in runtime resumend
state.

When one wants to register clock controller, which make use of this feature, he
has to:
1. Provide a struct device to the core when registering the provider.
2. Ensure to enable runtime PM for that device before registering clocks.
3. Make sure that the runtime PM status of the controller device reflects
   the HW state.

	Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Reviewed-by: Ulf Hansson <ulf.hansson@linaro.org>
	Acked-by: Krzysztof Kozlowski <krzk@kernel.org>
	Signed-off-by: Michael Turquette <mturquette@baylibre.com>
Link: lkml.kernel.org/r/1503302703-13801-2-git-send-email-m.szyprowski@samsung.com
(cherry picked from commit 9a34b45397e5a389e25a0c5d39983300d040e5e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/clk/clk.c
diff --cc drivers/clk/clk.c
index 83ca8a76ebb8,5bb6308fbf0d..000000000000
--- a/drivers/clk/clk.c
+++ b/drivers/clk/clk.c
@@@ -19,7 -21,11 +19,8 @@@
  #include <linux/of.h>
  #include <linux/device.h>
  #include <linux/init.h>
+ #include <linux/pm_runtime.h>
  #include <linux/sched.h>
 -#include <linux/clkdev.h>
 -
 -#include "clk.h"
  
  static DEFINE_SPINLOCK(enable_lock);
  static DEFINE_MUTEX(prepare_lock);
@@@ -34,6 -40,75 +35,78 @@@ static HLIST_HEAD(clk_root_list)
  static HLIST_HEAD(clk_orphan_list);
  static LIST_HEAD(clk_notifier_list);
  
++<<<<<<< HEAD
++=======
+ /***    private data structures    ***/
+ 
+ struct clk_core {
+ 	const char		*name;
+ 	const struct clk_ops	*ops;
+ 	struct clk_hw		*hw;
+ 	struct module		*owner;
+ 	struct device		*dev;
+ 	struct clk_core		*parent;
+ 	const char		**parent_names;
+ 	struct clk_core		**parents;
+ 	u8			num_parents;
+ 	u8			new_parent_index;
+ 	unsigned long		rate;
+ 	unsigned long		req_rate;
+ 	unsigned long		new_rate;
+ 	struct clk_core		*new_parent;
+ 	struct clk_core		*new_child;
+ 	unsigned long		flags;
+ 	bool			orphan;
+ 	unsigned int		enable_count;
+ 	unsigned int		prepare_count;
+ 	unsigned long		min_rate;
+ 	unsigned long		max_rate;
+ 	unsigned long		accuracy;
+ 	int			phase;
+ 	struct hlist_head	children;
+ 	struct hlist_node	child_node;
+ 	struct hlist_head	clks;
+ 	unsigned int		notifier_count;
+ #ifdef CONFIG_DEBUG_FS
+ 	struct dentry		*dentry;
+ 	struct hlist_node	debug_node;
+ #endif
+ 	struct kref		ref;
+ };
+ 
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/clk.h>
+ 
+ struct clk {
+ 	struct clk_core	*core;
+ 	const char *dev_id;
+ 	const char *con_id;
+ 	unsigned long min_rate;
+ 	unsigned long max_rate;
+ 	struct hlist_node clks_node;
+ };
+ 
+ /***           runtime pm          ***/
+ static int clk_pm_runtime_get(struct clk_core *core)
+ {
+ 	int ret = 0;
+ 
+ 	if (!core->dev)
+ 		return 0;
+ 
+ 	ret = pm_runtime_get_sync(core->dev);
+ 	return ret < 0 ? ret : 0;
+ }
+ 
+ static void clk_pm_runtime_put(struct clk_core *core)
+ {
+ 	if (!core->dev)
+ 		return;
+ 
+ 	pm_runtime_put_sync(core->dev);
+ }
+ 
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  /***           locking             ***/
  static void clk_prepare_lock(void)
  {
@@@ -90,211 -170,182 +163,212 @@@ static void clk_enable_unlock(unsigned 
  	spin_unlock_irqrestore(&enable_lock, flags);
  }
  
 -static bool clk_core_is_prepared(struct clk_core *core)
 +/***        debugfs support        ***/
 +
 +#ifdef CONFIG_COMMON_CLK_DEBUG
 +#include <linux/debugfs.h>
 +
 +static struct dentry *rootdir;
 +static struct dentry *orphandir;
 +static int inited = 0;
 +
 +static void clk_summary_show_one(struct seq_file *s, struct clk *c, int level)
  {
 -	bool ret = false;
++<<<<<<< HEAD
 +	if (!c)
 +		return;
  
 -	/*
 -	 * .is_prepared is optional for clocks that can prepare
 -	 * fall back to software usage counter if it is missing
 -	 */
 -	if (!core->ops->is_prepared)
 -		return core->prepare_count;
 +	seq_printf(s, "%*s%-*s %-11d %-12d %-10lu",
 +		   level * 3 + 1, "",
 +		   30 - level * 3, c->name,
 +		   c->enable_count, c->prepare_count, c->rate);
 +	seq_printf(s, "\n");
 +}
  
 -	if (!clk_pm_runtime_get(core)) {
 -		ret = core->ops->is_prepared(core->hw);
 -		clk_pm_runtime_put(core);
 -	}
 +static void clk_summary_show_subtree(struct seq_file *s, struct clk *c,
 +				     int level)
 +{
 +	struct clk *child;
  
 -	return ret;
 +	if (!c)
 +		return;
 +
 +	clk_summary_show_one(s, c, level);
 +
 +	hlist_for_each_entry(child, &c->children, child_node)
 +		clk_summary_show_subtree(s, child, level + 1);
  }
  
 -static bool clk_core_is_enabled(struct clk_core *core)
 +static int clk_summary_show(struct seq_file *s, void *data)
  {
 -	bool ret = false;
 +	struct clk *c;
  
 -	/*
 -	 * .is_enabled is only mandatory for clocks that gate
 -	 * fall back to software usage counter if .is_enabled is missing
 -	 */
 -	if (!core->ops->is_enabled)
 -		return core->enable_count;
 +	seq_printf(s, "   clock                        enable_cnt  prepare_cnt  rate\n");
 +	seq_printf(s, "---------------------------------------------------------------------\n");
  
 -	/*
 -	 * Check if clock controller's device is runtime active before
 -	 * calling .is_enabled callback. If not, assume that clock is
 -	 * disabled, because we might be called from atomic context, from
 -	 * which pm_runtime_get() is not allowed.
 -	 * This function is called mainly from clk_disable_unused_subtree,
 -	 * which ensures proper runtime pm activation of controller before
 -	 * taking enable spinlock, but the below check is needed if one tries
 -	 * to call it from other places.
 -	 */
 -	if (core->dev) {
 -		pm_runtime_get_noresume(core->dev);
 -		if (!pm_runtime_active(core->dev)) {
 -			ret = false;
 -			goto done;
 -		}
 -	}
 +	clk_prepare_lock();
  
 -	ret = core->ops->is_enabled(core->hw);
 -done:
 -	clk_pm_runtime_put(core);
 +	hlist_for_each_entry(c, &clk_root_list, child_node)
 +		clk_summary_show_subtree(s, c, 0);
  
 -	return ret;
 -}
 +	hlist_for_each_entry(c, &clk_orphan_list, child_node)
 +		clk_summary_show_subtree(s, c, 0);
  
 -/***    helper functions   ***/
 +	clk_prepare_unlock();
  
 -const char *__clk_get_name(const struct clk *clk)
 -{
 -	return !clk ? NULL : clk->core->name;
 +	return 0;
  }
 -EXPORT_SYMBOL_GPL(__clk_get_name);
  
 -const char *clk_hw_get_name(const struct clk_hw *hw)
 -{
 -	return hw->core->name;
 -}
 -EXPORT_SYMBOL_GPL(clk_hw_get_name);
  
 -struct clk_hw *__clk_get_hw(struct clk *clk)
 +static int clk_summary_open(struct inode *inode, struct file *file)
  {
 -	return !clk ? NULL : clk->core->hw;
 +	return single_open(file, clk_summary_show, inode->i_private);
  }
 -EXPORT_SYMBOL_GPL(__clk_get_hw);
  
 -unsigned int clk_hw_get_num_parents(const struct clk_hw *hw)
 -{
 -	return hw->core->num_parents;
 -}
 -EXPORT_SYMBOL_GPL(clk_hw_get_num_parents);
 +static const struct file_operations clk_summary_fops = {
 +	.open		= clk_summary_open,
 +	.read		= seq_read,
 +	.llseek		= seq_lseek,
 +	.release	= single_release,
 +};
  
 -struct clk_hw *clk_hw_get_parent(const struct clk_hw *hw)
 +static void clk_dump_one(struct seq_file *s, struct clk *c, int level)
  {
 -	return hw->core->parent ? hw->core->parent->hw : NULL;
 +	if (!c)
 +		return;
 +
 +	seq_printf(s, "\"%s\": { ", c->name);
 +	seq_printf(s, "\"enable_count\": %d,", c->enable_count);
 +	seq_printf(s, "\"prepare_count\": %d,", c->prepare_count);
 +	seq_printf(s, "\"rate\": %lu", c->rate);
  }
 -EXPORT_SYMBOL_GPL(clk_hw_get_parent);
  
 -static struct clk_core *__clk_lookup_subtree(const char *name,
 -					     struct clk_core *core)
 +static void clk_dump_subtree(struct seq_file *s, struct clk *c, int level)
  {
 -	struct clk_core *child;
 -	struct clk_core *ret;
 +	struct clk *child;
  
 -	if (!strcmp(core->name, name))
 -		return core;
 +	if (!c)
 +		return;
  
 -	hlist_for_each_entry(child, &core->children, child_node) {
 -		ret = __clk_lookup_subtree(name, child);
 -		if (ret)
 -			return ret;
 +	clk_dump_one(s, c, level);
 +
 +	hlist_for_each_entry(child, &c->children, child_node) {
 +		seq_printf(s, ",");
 +		clk_dump_subtree(s, child, level + 1);
  	}
  
 -	return NULL;
 +	seq_printf(s, "}");
  }
  
 -static struct clk_core *clk_core_lookup(const char *name)
 +static int clk_dump(struct seq_file *s, void *data)
  {
 -	struct clk_core *root_clk;
 -	struct clk_core *ret;
 +	struct clk *c;
 +	bool first_node = true;
  
 -	if (!name)
 -		return NULL;
 +	seq_printf(s, "{");
  
 -	/* search the 'proper' clk tree first */
 -	hlist_for_each_entry(root_clk, &clk_root_list, child_node) {
 -		ret = __clk_lookup_subtree(name, root_clk);
 -		if (ret)
 -			return ret;
 +	clk_prepare_lock();
 +
 +	hlist_for_each_entry(c, &clk_root_list, child_node) {
 +		if (!first_node)
 +			seq_printf(s, ",");
 +		first_node = false;
 +		clk_dump_subtree(s, c, 0);
  	}
  
 -	/* if not found, then search the orphan tree */
 -	hlist_for_each_entry(root_clk, &clk_orphan_list, child_node) {
 -		ret = __clk_lookup_subtree(name, root_clk);
 -		if (ret)
 -			return ret;
 +	hlist_for_each_entry(c, &clk_orphan_list, child_node) {
 +		seq_printf(s, ",");
 +		clk_dump_subtree(s, c, 0);
  	}
  
 -	return NULL;
 -}
 +	clk_prepare_unlock();
  
 -static struct clk_core *clk_core_get_parent_by_index(struct clk_core *core,
 -							 u8 index)
 -{
 -	if (!core || index >= core->num_parents)
 -		return NULL;
 +	seq_printf(s, "}");
 +	return 0;
 +}
  
 -	if (!core->parents[index])
 -		core->parents[index] =
 -				clk_core_lookup(core->parent_names[index]);
  
 -	return core->parents[index];
 +static int clk_dump_open(struct inode *inode, struct file *file)
 +{
 +	return single_open(file, clk_dump, inode->i_private);
  }
  
 -struct clk_hw *
 -clk_hw_get_parent_by_index(const struct clk_hw *hw, unsigned int index)
 +static const struct file_operations clk_dump_fops = {
 +	.open		= clk_dump_open,
 +	.read		= seq_read,
 +	.llseek		= seq_lseek,
 +	.release	= single_release,
 +};
 +
 +/* caller must hold prepare_lock */
 +static int clk_debug_create_one(struct clk *clk, struct dentry *pdentry)
  {
 -	struct clk_core *parent;
 +	struct dentry *d;
 +	int ret = -ENOMEM;
 +
 +	if (!clk || !pdentry) {
 +		ret = -EINVAL;
 +		goto out;
 +	}
  
 -	parent = clk_core_get_parent_by_index(hw->core, index);
 +	d = debugfs_create_dir(clk->name, pdentry);
 +	if (!d)
 +		goto out;
  
 -	return !parent ? NULL : parent->hw;
 -}
 -EXPORT_SYMBOL_GPL(clk_hw_get_parent_by_index);
 +	clk->dentry = d;
  
 -unsigned int __clk_get_enable_count(struct clk *clk)
 -{
 -	return !clk ? 0 : clk->core->enable_count;
 +	d = debugfs_create_u32("clk_rate", S_IRUGO, clk->dentry,
 +			(u32 *)&clk->rate);
 +	if (!d)
 +		goto err_out;
 +
 +	d = debugfs_create_x32("clk_flags", S_IRUGO, clk->dentry,
 +			(u32 *)&clk->flags);
 +	if (!d)
 +		goto err_out;
 +
 +	d = debugfs_create_u32("clk_prepare_count", S_IRUGO, clk->dentry,
 +			(u32 *)&clk->prepare_count);
 +	if (!d)
 +		goto err_out;
 +
 +	d = debugfs_create_u32("clk_enable_count", S_IRUGO, clk->dentry,
 +			(u32 *)&clk->enable_count);
 +	if (!d)
 +		goto err_out;
 +
 +	d = debugfs_create_u32("clk_notifier_count", S_IRUGO, clk->dentry,
 +			(u32 *)&clk->notifier_count);
 +	if (!d)
 +		goto err_out;
 +
 +	ret = 0;
 +	goto out;
 +
 +err_out:
 +	debugfs_remove(clk->dentry);
 +out:
 +	return ret;
  }
  
 -static unsigned long clk_core_get_rate_nolock(struct clk_core *core)
 +/* caller must hold prepare_lock */
 +static int clk_debug_create_subtree(struct clk *clk, struct dentry *pdentry)
  {
 -	unsigned long ret;
 +	struct clk *child;
 +	int ret = -EINVAL;;
  
 -	if (!core) {
 -		ret = 0;
 +	if (!clk || !pdentry)
  		goto out;
 -	}
  
 -	ret = core->rate;
 +	ret = clk_debug_create_one(clk, pdentry);
  
 -	if (!core->num_parents)
 +	if (ret)
  		goto out;
  
 -	if (!core->parent)
 -		ret = 0;
 +	hlist_for_each_entry(child, &clk->children, child_node)
 +		clk_debug_create_subtree(child, clk->dentry);
  
 +	ret = 0;
  out:
  	return ret;
  }
@@@ -426,279 -466,93 +500,340 @@@ static int __init clk_debug_init(void
  
  	return 0;
  }
 -
 -struct clk *__clk_lookup(const char *name)
 +late_initcall(clk_debug_init);
 +#else
 +static inline int clk_debug_register(struct clk *clk) { return 0; }
 +static inline void clk_debug_reparent(struct clk *clk, struct clk *new_parent)
  {
 -	struct clk_core *core = clk_core_lookup(name);
 -
 -	return !core ? NULL : core->hw->clk;
  }
 +#endif
  
 -static void clk_core_get_boundaries(struct clk_core *core,
 -				    unsigned long *min_rate,
 -				    unsigned long *max_rate)
 +/* caller must hold prepare_lock */
 +static void clk_unprepare_unused_subtree(struct clk *clk)
  {
 -	struct clk *clk_user;
 +	struct clk *child;
  
 -	*min_rate = core->min_rate;
 -	*max_rate = core->max_rate;
 +	if (!clk)
 +		return;
  
 -	hlist_for_each_entry(clk_user, &core->clks, clks_node)
 -		*min_rate = max(*min_rate, clk_user->min_rate);
 +	hlist_for_each_entry(child, &clk->children, child_node)
 +		clk_unprepare_unused_subtree(child);
  
 -	hlist_for_each_entry(clk_user, &core->clks, clks_node)
 -		*max_rate = min(*max_rate, clk_user->max_rate);
 -}
 +	if (clk->prepare_count)
 +		return;
 +
 +	if (clk->flags & CLK_IGNORE_UNUSED)
 +		return;
 +
 +	if (__clk_is_prepared(clk)) {
 +		if (clk->ops->unprepare_unused)
 +			clk->ops->unprepare_unused(clk->hw);
 +		else if (clk->ops->unprepare)
 +			clk->ops->unprepare(clk->hw);
 +	}
 +}
 +EXPORT_SYMBOL_GPL(__clk_get_flags);
  
 -void clk_hw_set_rate_range(struct clk_hw *hw, unsigned long min_rate,
 -			   unsigned long max_rate)
 +/* caller must hold prepare_lock */
 +static void clk_disable_unused_subtree(struct clk *clk)
  {
 -	hw->core->min_rate = min_rate;
 -	hw->core->max_rate = max_rate;
 +	struct clk *child;
 +	unsigned long flags;
 +
 +	if (!clk)
 +		goto out;
 +
 +	hlist_for_each_entry(child, &clk->children, child_node)
 +		clk_disable_unused_subtree(child);
 +
 +	flags = clk_enable_lock();
 +
 +	if (clk->enable_count)
 +		goto unlock_out;
 +
 +	if (clk->flags & CLK_IGNORE_UNUSED)
 +		goto unlock_out;
 +
 +	/*
 +	 * some gate clocks have special needs during the disable-unused
 +	 * sequence.  call .disable_unused if available, otherwise fall
 +	 * back to .disable
 +	 */
 +	if (__clk_is_enabled(clk)) {
 +		if (clk->ops->disable_unused)
 +			clk->ops->disable_unused(clk->hw);
 +		else if (clk->ops->disable)
 +			clk->ops->disable(clk->hw);
 +	}
 +
 +unlock_out:
 +	clk_enable_unlock(flags);
 +
 +out:
 +	return;
  }
 -EXPORT_SYMBOL_GPL(clk_hw_set_rate_range);
  
 -/*
 - * Helper for finding best parent to provide a given frequency. This can be used
 - * directly as a determine_rate callback (e.g. for a mux), or from a more
 - * complex clock that may combine a mux with other operations.
 - */
 -int __clk_mux_determine_rate(struct clk_hw *hw,
 -			     struct clk_rate_request *req)
 +static bool clk_ignore_unused;
 +static int __init clk_ignore_unused_setup(char *__unused)
  {
 -	return clk_mux_determine_rate_flags(hw, req, 0);
 +	clk_ignore_unused = true;
 +	return 1;
  }
 -EXPORT_SYMBOL_GPL(__clk_mux_determine_rate);
 +__setup("clk_ignore_unused", clk_ignore_unused_setup);
  
 -int __clk_mux_determine_rate_closest(struct clk_hw *hw,
 -				     struct clk_rate_request *req)
 +static int clk_disable_unused(void)
  {
 -	return clk_mux_determine_rate_flags(hw, req, CLK_MUX_ROUND_CLOSEST);
 +	struct clk *clk;
 +
 +	if (clk_ignore_unused) {
 +		pr_warn("clk: Not disabling unused clocks\n");
 +		return 0;
 +	}
 +
 +	clk_prepare_lock();
 +
 +	hlist_for_each_entry(clk, &clk_root_list, child_node)
 +		clk_disable_unused_subtree(clk);
 +
 +	hlist_for_each_entry(clk, &clk_orphan_list, child_node)
 +		clk_disable_unused_subtree(clk);
 +
 +	hlist_for_each_entry(clk, &clk_root_list, child_node)
 +		clk_unprepare_unused_subtree(clk);
 +
 +	hlist_for_each_entry(clk, &clk_orphan_list, child_node)
 +		clk_unprepare_unused_subtree(clk);
 +
 +	clk_prepare_unlock();
 +
 +	return 0;
  }
 -EXPORT_SYMBOL_GPL(__clk_mux_determine_rate_closest);
 +late_initcall(clk_disable_unused);
  
 -/***        clk api        ***/
 +/***    helper functions   ***/
  
 -static void clk_core_unprepare(struct clk_core *core)
 +const char *__clk_get_name(struct clk *clk)
  {
 -	lockdep_assert_held(&prepare_lock);
 +	return !clk ? NULL : clk->name;
 +}
 +EXPORT_SYMBOL_GPL(__clk_get_name);
  
 -	if (!core)
 -		return;
 +struct clk_hw *__clk_get_hw(struct clk *clk)
 +{
 +	return !clk ? NULL : clk->hw;
 +}
 +EXPORT_SYMBOL_GPL(__clk_get_hw);
 +
 +u8 __clk_get_num_parents(struct clk *clk)
 +{
 +	return !clk ? 0 : clk->num_parents;
 +}
 +
 +struct clk *__clk_get_parent(struct clk *clk)
 +{
 +	return !clk ? NULL : clk->parent;
 +}
 +
 +unsigned int __clk_get_enable_count(struct clk *clk)
 +{
 +	return !clk ? 0 : clk->enable_count;
 +}
 +
 +unsigned int __clk_get_prepare_count(struct clk *clk)
 +{
 +	return !clk ? 0 : clk->prepare_count;
 +}
 +
 +unsigned long __clk_get_rate(struct clk *clk)
 +{
 +	unsigned long ret;
 +
 +	if (!clk) {
 +		ret = 0;
 +		goto out;
 +	}
 +
 +	ret = clk->rate;
 +
 +	if (clk->flags & CLK_IS_ROOT)
 +		goto out;
 +
 +	if (!clk->parent)
 +		ret = 0;
 +
 +out:
 +	return ret;
 +}
 +
 +unsigned long __clk_get_flags(struct clk *clk)
 +{
 +	return !clk ? 0 : clk->flags;
 +}
 +
 +bool __clk_is_prepared(struct clk *clk)
 +{
 +	int ret;
 +
 +	if (!clk)
 +		return false;
++=======
++	bool ret = false;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
 +
 +	/*
 +	 * .is_prepared is optional for clocks that can prepare
 +	 * fall back to software usage counter if it is missing
 +	 */
 +	if (!clk->ops->is_prepared) {
 +		ret = clk->prepare_count ? 1 : 0;
 +		goto out;
 +	}
 +
++<<<<<<< HEAD
 +	ret = clk->ops->is_prepared(clk->hw);
 +out:
 +	return !!ret;
++=======
++	if (!clk_pm_runtime_get(core)) {
++		ret = core->ops->is_prepared(core->hw);
++		clk_pm_runtime_put(core);
++	}
++
++	return ret;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
 +}
 +
 +bool __clk_is_enabled(struct clk *clk)
 +{
++<<<<<<< HEAD
 +	int ret;
 +
 +	if (!clk)
 +		return false;
++=======
++	bool ret = false;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
 +
 +	/*
 +	 * .is_enabled is only mandatory for clocks that gate
 +	 * fall back to software usage counter if .is_enabled is missing
 +	 */
 +	if (!clk->ops->is_enabled) {
 +		ret = clk->enable_count ? 1 : 0;
 +		goto out;
 +	}
 +
++<<<<<<< HEAD
 +	ret = clk->ops->is_enabled(clk->hw);
 +out:
 +	return !!ret;
++=======
++	/*
++	 * Check if clock controller's device is runtime active before
++	 * calling .is_enabled callback. If not, assume that clock is
++	 * disabled, because we might be called from atomic context, from
++	 * which pm_runtime_get() is not allowed.
++	 * This function is called mainly from clk_disable_unused_subtree,
++	 * which ensures proper runtime pm activation of controller before
++	 * taking enable spinlock, but the below check is needed if one tries
++	 * to call it from other places.
++	 */
++	if (core->dev) {
++		pm_runtime_get_noresume(core->dev);
++		if (!pm_runtime_active(core->dev)) {
++			ret = false;
++			goto done;
++		}
++	}
++
++	ret = core->ops->is_enabled(core->hw);
++done:
++	clk_pm_runtime_put(core);
++
++	return ret;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
 +}
 +
 +static struct clk *__clk_lookup_subtree(const char *name, struct clk *clk)
 +{
 +	struct clk *child;
 +	struct clk *ret;
 +
 +	if (!strcmp(clk->name, name))
 +		return clk;
 +
 +	hlist_for_each_entry(child, &clk->children, child_node) {
 +		ret = __clk_lookup_subtree(name, child);
 +		if (ret)
 +			return ret;
 +	}
 +
 +	return NULL;
 +}
 +
 +struct clk *__clk_lookup(const char *name)
 +{
 +	struct clk *root_clk;
 +	struct clk *ret;
 +
 +	if (!name)
 +		return NULL;
 +
 +	/* search the 'proper' clk tree first */
 +	hlist_for_each_entry(root_clk, &clk_root_list, child_node) {
 +		ret = __clk_lookup_subtree(name, root_clk);
 +		if (ret)
 +			return ret;
 +	}
 +
 +	/* if not found, then search the orphan tree */
 +	hlist_for_each_entry(root_clk, &clk_orphan_list, child_node) {
 +		ret = __clk_lookup_subtree(name, root_clk);
 +		if (ret)
 +			return ret;
 +	}
 +
 +	return NULL;
 +}
 +
 +/***        clk api        ***/
  
 -	if (WARN_ON(core->prepare_count == 0))
 +void __clk_unprepare(struct clk *clk)
 +{
 +	if (!clk)
  		return;
  
 -	if (WARN_ON(core->prepare_count == 1 && core->flags & CLK_IS_CRITICAL))
 +	if (WARN_ON(clk->prepare_count == 0))
  		return;
  
 -	if (--core->prepare_count > 0)
 +	if (--clk->prepare_count > 0)
  		return;
  
 -	WARN_ON(core->enable_count > 0);
 +	WARN_ON(clk->enable_count > 0);
 +
 +	if (clk->ops->unprepare)
 +		clk->ops->unprepare(clk->hw);
  
++<<<<<<< HEAD
 +	__clk_unprepare(clk->parent);
++=======
+ 	trace_clk_unprepare(core);
+ 
+ 	if (core->ops->unprepare)
+ 		core->ops->unprepare(core->hw);
+ 
+ 	clk_pm_runtime_put(core);
+ 
+ 	trace_clk_unprepare_complete(core);
+ 	clk_core_unprepare(core->parent);
+ }
+ 
+ static void clk_core_unprepare_lock(struct clk_core *core)
+ {
+ 	clk_prepare_lock();
+ 	clk_core_unprepare(core);
+ 	clk_prepare_unlock();
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  }
  
  /**
@@@ -724,28 -579,52 +859,54 @@@ int __clk_prepare(struct clk *clk
  {
  	int ret = 0;
  
 -	lockdep_assert_held(&prepare_lock);
 -
 -	if (!core)
 +	if (!clk)
  		return 0;
  
++<<<<<<< HEAD
 +	if (clk->prepare_count == 0) {
 +		ret = __clk_prepare(clk->parent);
 +		if (ret)
 +			return ret;
 +
 +		if (clk->ops->prepare) {
 +			ret = clk->ops->prepare(clk->hw);
 +			if (ret) {
 +				__clk_unprepare(clk->parent);
 +				return ret;
 +			}
 +		}
++=======
+ 	if (core->prepare_count == 0) {
+ 		ret = clk_pm_runtime_get(core);
+ 		if (ret)
+ 			return ret;
+ 
+ 		ret = clk_core_prepare(core->parent);
+ 		if (ret)
+ 			goto runtime_put;
+ 
+ 		trace_clk_prepare(core);
+ 
+ 		if (core->ops->prepare)
+ 			ret = core->ops->prepare(core->hw);
+ 
+ 		trace_clk_prepare_complete(core);
+ 
+ 		if (ret)
+ 			goto unprepare;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  	}
  
 -	core->prepare_count++;
 +	clk->prepare_count++;
  
  	return 0;
+ unprepare:
+ 	clk_core_unprepare(core->parent);
+ runtime_put:
+ 	clk_pm_runtime_put(core);
+ 	return ret;
  }
  
 -static int clk_core_prepare_lock(struct clk_core *core)
 -{
 -	int ret;
 -
 -	clk_prepare_lock();
 -	ret = clk_core_prepare(core);
 -	clk_prepare_unlock();
 -
 -	return ret;
 -}
 -
  /**
   * clk_prepare - prepare a clock source
   * @clk: the clk being prepared
@@@ -865,33 -785,197 +1026,193 @@@ int clk_enable(struct clk *clk
  
  	return ret;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(clk_enable);
++=======
+ 
+ static void clk_core_disable_unprepare(struct clk_core *core)
+ {
+ 	clk_core_disable_lock(core);
+ 	clk_core_unprepare_lock(core);
+ }
+ 
+ static void clk_unprepare_unused_subtree(struct clk_core *core)
+ {
+ 	struct clk_core *child;
+ 
+ 	lockdep_assert_held(&prepare_lock);
+ 
+ 	hlist_for_each_entry(child, &core->children, child_node)
+ 		clk_unprepare_unused_subtree(child);
+ 
+ 	if (core->prepare_count)
+ 		return;
+ 
+ 	if (core->flags & CLK_IGNORE_UNUSED)
+ 		return;
+ 
+ 	if (clk_pm_runtime_get(core))
+ 		return;
+ 
+ 	if (clk_core_is_prepared(core)) {
+ 		trace_clk_unprepare(core);
+ 		if (core->ops->unprepare_unused)
+ 			core->ops->unprepare_unused(core->hw);
+ 		else if (core->ops->unprepare)
+ 			core->ops->unprepare(core->hw);
+ 		trace_clk_unprepare_complete(core);
+ 	}
+ 
+ 	clk_pm_runtime_put(core);
+ }
+ 
+ static void clk_disable_unused_subtree(struct clk_core *core)
+ {
+ 	struct clk_core *child;
+ 	unsigned long flags;
+ 
+ 	lockdep_assert_held(&prepare_lock);
+ 
+ 	hlist_for_each_entry(child, &core->children, child_node)
+ 		clk_disable_unused_subtree(child);
+ 
+ 	if (core->flags & CLK_OPS_PARENT_ENABLE)
+ 		clk_core_prepare_enable(core->parent);
+ 
+ 	if (clk_pm_runtime_get(core))
+ 		goto unprepare_out;
+ 
+ 	flags = clk_enable_lock();
+ 
+ 	if (core->enable_count)
+ 		goto unlock_out;
+ 
+ 	if (core->flags & CLK_IGNORE_UNUSED)
+ 		goto unlock_out;
+ 
+ 	/*
+ 	 * some gate clocks have special needs during the disable-unused
+ 	 * sequence.  call .disable_unused if available, otherwise fall
+ 	 * back to .disable
+ 	 */
+ 	if (clk_core_is_enabled(core)) {
+ 		trace_clk_disable(core);
+ 		if (core->ops->disable_unused)
+ 			core->ops->disable_unused(core->hw);
+ 		else if (core->ops->disable)
+ 			core->ops->disable(core->hw);
+ 		trace_clk_disable_complete(core);
+ 	}
+ 
+ unlock_out:
+ 	clk_enable_unlock(flags);
+ 	clk_pm_runtime_put(core);
+ unprepare_out:
+ 	if (core->flags & CLK_OPS_PARENT_ENABLE)
+ 		clk_core_disable_unprepare(core->parent);
+ }
+ 
+ static bool clk_ignore_unused;
+ static int __init clk_ignore_unused_setup(char *__unused)
+ {
+ 	clk_ignore_unused = true;
+ 	return 1;
+ }
+ __setup("clk_ignore_unused", clk_ignore_unused_setup);
+ 
+ static int clk_disable_unused(void)
+ {
+ 	struct clk_core *core;
+ 
+ 	if (clk_ignore_unused) {
+ 		pr_warn("clk: Not disabling unused clocks\n");
+ 		return 0;
+ 	}
+ 
+ 	clk_prepare_lock();
+ 
+ 	hlist_for_each_entry(core, &clk_root_list, child_node)
+ 		clk_disable_unused_subtree(core);
+ 
+ 	hlist_for_each_entry(core, &clk_orphan_list, child_node)
+ 		clk_disable_unused_subtree(core);
+ 
+ 	hlist_for_each_entry(core, &clk_root_list, child_node)
+ 		clk_unprepare_unused_subtree(core);
+ 
+ 	hlist_for_each_entry(core, &clk_orphan_list, child_node)
+ 		clk_unprepare_unused_subtree(core);
+ 
+ 	clk_prepare_unlock();
+ 
+ 	return 0;
+ }
+ late_initcall_sync(clk_disable_unused);
+ 
+ static int clk_core_round_rate_nolock(struct clk_core *core,
+ 				      struct clk_rate_request *req)
+ {
+ 	struct clk_core *parent;
+ 	long rate;
+ 
+ 	lockdep_assert_held(&prepare_lock);
+ 
+ 	if (!core)
+ 		return 0;
+ 
+ 	parent = core->parent;
+ 	if (parent) {
+ 		req->best_parent_hw = parent->hw;
+ 		req->best_parent_rate = parent->rate;
+ 	} else {
+ 		req->best_parent_hw = NULL;
+ 		req->best_parent_rate = 0;
+ 	}
+ 
+ 	if (core->ops->determine_rate) {
+ 		return core->ops->determine_rate(core->hw, req);
+ 	} else if (core->ops->round_rate) {
+ 		rate = core->ops->round_rate(core->hw, req->rate,
+ 					     &req->best_parent_rate);
+ 		if (rate < 0)
+ 			return rate;
+ 
+ 		req->rate = rate;
+ 	} else if (core->flags & CLK_SET_RATE_PARENT) {
+ 		return clk_core_round_rate_nolock(parent, req);
+ 	} else {
+ 		req->rate = core->rate;
+ 	}
+ 
+ 	return 0;
+ }
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  
  /**
 - * __clk_determine_rate - get the closest rate actually supported by a clock
 - * @hw: determine the rate of this clock
 - * @req: target rate request
 + * __clk_round_rate - round the given rate for a clk
 + * @clk: round the rate of this clock
   *
 - * Useful for clk_ops such as .set_rate and .determine_rate.
 + * Caller must hold prepare_lock.  Useful for clk_ops such as .set_rate
   */
 -int __clk_determine_rate(struct clk_hw *hw, struct clk_rate_request *req)
 +unsigned long __clk_round_rate(struct clk *clk, unsigned long rate)
  {
 -	if (!hw) {
 -		req->rate = 0;
 -		return 0;
 -	}
 -
 -	return clk_core_round_rate_nolock(hw->core, req);
 -}
 -EXPORT_SYMBOL_GPL(__clk_determine_rate);
 +	unsigned long parent_rate = 0;
  
 -unsigned long clk_hw_round_rate(struct clk_hw *hw, unsigned long rate)
 -{
 -	int ret;
 -	struct clk_rate_request req;
 +	if (!clk)
 +		return 0;
  
 -	clk_core_get_boundaries(hw->core, &req.min_rate, &req.max_rate);
 -	req.rate = rate;
 +	if (!clk->ops->round_rate) {
 +		if (clk->flags & CLK_SET_RATE_PARENT)
 +			return __clk_round_rate(clk->parent, rate);
 +		else
 +			return clk->rate;
 +	}
  
 -	ret = clk_core_round_rate_nolock(hw->core, &req);
 -	if (ret)
 -		return 0;
 +	if (clk->parent)
 +		parent_rate = clk->parent->rate;
  
 -	return req.rate;
 +	return clk->ops->round_rate(clk->hw, rate, &parent_rate);
  }
 -EXPORT_SYMBOL_GPL(clk_hw_round_rate);
  
  /**
   * clk_round_rate - round the given rate for a clk
@@@ -951,8 -1047,81 +1272,84 @@@ static int __clk_notify(struct clk *clk
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * __clk_recalc_accuracies
+  * @core: first clk in the subtree
+  *
+  * Walks the subtree of clks starting with clk and recalculates accuracies as
+  * it goes.  Note that if a clk does not implement the .recalc_accuracy
+  * callback then it is assumed that the clock will take on the accuracy of its
+  * parent.
+  */
+ static void __clk_recalc_accuracies(struct clk_core *core)
+ {
+ 	unsigned long parent_accuracy = 0;
+ 	struct clk_core *child;
+ 
+ 	lockdep_assert_held(&prepare_lock);
+ 
+ 	if (core->parent)
+ 		parent_accuracy = core->parent->accuracy;
+ 
+ 	if (core->ops->recalc_accuracy)
+ 		core->accuracy = core->ops->recalc_accuracy(core->hw,
+ 							  parent_accuracy);
+ 	else
+ 		core->accuracy = parent_accuracy;
+ 
+ 	hlist_for_each_entry(child, &core->children, child_node)
+ 		__clk_recalc_accuracies(child);
+ }
+ 
+ static long clk_core_get_accuracy(struct clk_core *core)
+ {
+ 	unsigned long accuracy;
+ 
+ 	clk_prepare_lock();
+ 	if (core && (core->flags & CLK_GET_ACCURACY_NOCACHE))
+ 		__clk_recalc_accuracies(core);
+ 
+ 	accuracy = __clk_get_accuracy(core);
+ 	clk_prepare_unlock();
+ 
+ 	return accuracy;
+ }
+ 
+ /**
+  * clk_get_accuracy - return the accuracy of clk
+  * @clk: the clk whose accuracy is being returned
+  *
+  * Simply returns the cached accuracy of the clk, unless
+  * CLK_GET_ACCURACY_NOCACHE flag is set, which means a recalc_rate will be
+  * issued.
+  * If clk is NULL then returns 0.
+  */
+ long clk_get_accuracy(struct clk *clk)
+ {
+ 	if (!clk)
+ 		return 0;
+ 
+ 	return clk_core_get_accuracy(clk->core);
+ }
+ EXPORT_SYMBOL_GPL(clk_get_accuracy);
+ 
+ static unsigned long clk_recalc(struct clk_core *core,
+ 				unsigned long parent_rate)
+ {
+ 	unsigned long rate = parent_rate;
+ 
+ 	if (core->ops->recalc_rate && !clk_pm_runtime_get(core)) {
+ 		rate = core->ops->recalc_rate(core->hw, parent_rate);
+ 		clk_pm_runtime_put(core);
+ 	}
+ 	return rate;
+ }
+ 
+ /**
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
   * __clk_recalc_rates
 - * @core: first clk in the subtree
 + * @clk: first clk in the subtree
   * @msg: notification type (see include/linux/clk.h)
   *
   * Walks the subtree of clks starting with clk and recalculates rates as it
@@@ -1161,30 -1544,142 +1558,84 @@@ static struct clk *clk_propagate_rate_c
   * walk down a subtree and set the new rates notifying the rate
   * change on the way
   */
 -static void clk_change_rate(struct clk_core *core)
 +static void clk_change_rate(struct clk *clk)
  {
 -	struct clk_core *child;
 -	struct hlist_node *tmp;
 +	struct clk *child;
  	unsigned long old_rate;
  	unsigned long best_parent_rate = 0;
 -	bool skip_set_rate = false;
 -	struct clk_core *old_parent;
 -	struct clk_core *parent = NULL;
  
 -	old_rate = core->rate;
 +	old_rate = clk->rate;
  
 -	if (core->new_parent) {
 -		parent = core->new_parent;
 -		best_parent_rate = core->new_parent->rate;
 -	} else if (core->parent) {
 -		parent = core->parent;
 -		best_parent_rate = core->parent->rate;
 -	}
 -
 -	if (core->flags & CLK_SET_RATE_UNGATE) {
 -		unsigned long flags;
 -
 -		clk_core_prepare(core);
 -		flags = clk_enable_lock();
 -		clk_core_enable(core);
 -		clk_enable_unlock(flags);
 -	}
 -
 -	if (core->new_parent && core->new_parent != core->parent) {
 -		old_parent = __clk_set_parent_before(core, core->new_parent);
 -		trace_clk_set_parent(core, core->new_parent);
 -
 -		if (core->ops->set_rate_and_parent) {
 -			skip_set_rate = true;
 -			core->ops->set_rate_and_parent(core->hw, core->new_rate,
 -					best_parent_rate,
 -					core->new_parent_index);
 -		} else if (core->ops->set_parent) {
 -			core->ops->set_parent(core->hw, core->new_parent_index);
 -		}
 -
 -		trace_clk_set_parent_complete(core, core->new_parent);
 -		__clk_set_parent_after(core, core->new_parent, old_parent);
 -	}
 -
 -	if (core->flags & CLK_OPS_PARENT_ENABLE)
 -		clk_core_prepare_enable(parent);
 -
 -	trace_clk_set_rate(core, core->new_rate);
 -
 -	if (!skip_set_rate && core->ops->set_rate)
 -		core->ops->set_rate(core->hw, core->new_rate, best_parent_rate);
 -
 -	trace_clk_set_rate_complete(core, core->new_rate);
 +	if (clk->parent)
 +		best_parent_rate = clk->parent->rate;
  
 -	core->rate = clk_recalc(core, best_parent_rate);
 +	if (clk->ops->set_rate)
 +		clk->ops->set_rate(clk->hw, clk->new_rate, best_parent_rate);
  
 -	if (core->flags & CLK_SET_RATE_UNGATE) {
 -		unsigned long flags;
 -
 -		flags = clk_enable_lock();
 -		clk_core_disable(core);
 -		clk_enable_unlock(flags);
 -		clk_core_unprepare(core);
 -	}
 -
 -	if (core->flags & CLK_OPS_PARENT_ENABLE)
 -		clk_core_disable_unprepare(parent);
 -
 -	if (core->notifier_count && old_rate != core->rate)
 -		__clk_notify(core, POST_RATE_CHANGE, old_rate, core->rate);
 +	if (clk->ops->recalc_rate)
 +		clk->rate = clk->ops->recalc_rate(clk->hw, best_parent_rate);
 +	else
 +		clk->rate = best_parent_rate;
  
 -	if (core->flags & CLK_RECALC_NEW_RATES)
 -		(void)clk_calc_new_rates(core, core->new_rate);
 +	if (clk->notifier_count && old_rate != clk->rate)
 +		__clk_notify(clk, POST_RATE_CHANGE, old_rate, clk->rate);
  
 -	/*
 -	 * Use safe iteration, as change_rate can actually swap parents
 -	 * for certain clock types.
 -	 */
 -	hlist_for_each_entry_safe(child, tmp, &core->children, child_node) {
 -		/* Skip children who will be reparented to another clock */
 -		if (child->new_parent && child->new_parent != core)
 -			continue;
 +	hlist_for_each_entry(child, &clk->children, child_node)
  		clk_change_rate(child);
++<<<<<<< HEAD
++=======
+ 	}
+ 
+ 	/* handle the new child who might not be in core->children yet */
+ 	if (core->new_child)
+ 		clk_change_rate(core->new_child);
+ }
+ 
+ static int clk_core_set_rate_nolock(struct clk_core *core,
+ 				    unsigned long req_rate)
+ {
+ 	struct clk_core *top, *fail_clk;
+ 	unsigned long rate = req_rate;
+ 	int ret = 0;
+ 
+ 	if (!core)
+ 		return 0;
+ 
+ 	/* bail early if nothing to do */
+ 	if (rate == clk_core_get_rate_nolock(core))
+ 		return 0;
+ 
+ 	if ((core->flags & CLK_SET_RATE_GATE) && core->prepare_count)
+ 		return -EBUSY;
+ 
+ 	/* calculate new rates and get the topmost changed clock */
+ 	top = clk_calc_new_rates(core, rate);
+ 	if (!top)
+ 		return -EINVAL;
+ 
+ 	ret = clk_pm_runtime_get(core);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* notify that we are about to change rates */
+ 	fail_clk = clk_propagate_rate_change(top, PRE_RATE_CHANGE);
+ 	if (fail_clk) {
+ 		pr_debug("%s: failed to set %s rate\n", __func__,
+ 				fail_clk->name);
+ 		clk_propagate_rate_change(top, ABORT_RATE_CHANGE);
+ 		ret = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	/* change the rates */
+ 	clk_change_rate(top);
+ 
+ 	core->req_rate = req_rate;
+ err:
+ 	clk_pm_runtime_put(core);
+ 
+ 	return ret;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  }
  
  /**
@@@ -1296,35 -1892,51 +1747,50 @@@ static struct clk *__clk_init_parent(st
  		goto out;
  	}
  
 -	/* check that we are allowed to re-parent if the clock is in use */
 -	if ((core->flags & CLK_SET_PARENT_GATE) && core->prepare_count) {
 -		ret = -EBUSY;
 +	if (!clk->ops->get_parent) {
 +		WARN(!clk->ops->get_parent,
 +			"%s: multi-parent clocks must implement .get_parent\n",
 +			__func__);
  		goto out;
 -	}
 +	};
  
 -	/* try finding the new parent index */
 -	if (parent) {
 -		p_index = clk_fetch_parent_index(core, parent);
 -		if (p_index < 0) {
 -			pr_debug("%s: clk %s can not be parent of clk %s\n",
 -					__func__, parent->name, core->name);
 -			ret = p_index;
 -			goto out;
 -		}
 -		p_rate = parent->rate;
 -	}
 +	/*
 +	 * Do our best to cache parent clocks in clk->parents.  This prevents
 +	 * unnecessary and expensive calls to __clk_lookup.  We don't set
 +	 * clk->parent here; that is done by the calling function
 +	 */
 +
++<<<<<<< HEAD
 +	index = clk->ops->get_parent(clk->hw);
  
 +	if (!clk->parents)
 +		clk->parents =
 +			kzalloc((sizeof(struct clk*) * clk->num_parents),
 +					GFP_KERNEL);
++=======
+ 	ret = clk_pm_runtime_get(core);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* propagate PRE_RATE_CHANGE notifications */
+ 	ret = __clk_speculate_rates(core, p_rate);
+ 
+ 	/* abort if a driver objects */
+ 	if (ret & NOTIFY_STOP_MASK)
+ 		goto runtime_put;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  
 -	/* do the re-parent */
 -	ret = __clk_set_parent(core, parent, p_index);
 -
 -	/* propagate rate an accuracy recalculation accordingly */
 -	if (ret) {
 -		__clk_recalc_rates(core, ABORT_RATE_CHANGE);
 -	} else {
 -		__clk_recalc_rates(core, POST_RATE_CHANGE);
 -		__clk_recalc_accuracies(core);
 -	}
 +	if (!clk->parents)
 +		ret = __clk_lookup(clk->parent_names[index]);
 +	else if (!clk->parents[index])
 +		ret = clk->parents[index] =
 +			__clk_lookup(clk->parent_names[index]);
 +	else
 +		ret = clk->parents[index];
  
+ runtime_put:
+ 	clk_pm_runtime_put(core);
  out:
 -	clk_prepare_unlock();
 -
  	return ret;
  }
  
@@@ -1370,177 -2198,266 +1836,186 @@@ static u8 clk_fetch_parent_index(struc
  		}
  	}
  
 -	clk_prepare_unlock();
 -
 -	seq_puts(s, "}\n");
 -	return 0;
 -}
 -
 -
 -static int clk_dump_open(struct inode *inode, struct file *file)
 -{
 -	return single_open(file, clk_dump, inode->i_private);
 -}
 -
 -static const struct file_operations clk_dump_fops = {
 -	.open		= clk_dump_open,
 -	.read		= seq_read,
 -	.llseek		= seq_lseek,
 -	.release	= single_release,
 -};
 -
 -static int possible_parents_dump(struct seq_file *s, void *data)
 -{
 -	struct clk_core *core = s->private;
 -	int i;
 -
 -	for (i = 0; i < core->num_parents - 1; i++)
 -		seq_printf(s, "%s ", core->parent_names[i]);
 -
 -	seq_printf(s, "%s\n", core->parent_names[i]);
 -
 -	return 0;
 +	return i;
  }
  
 -static int possible_parents_open(struct inode *inode, struct file *file)
 +static int __clk_set_parent(struct clk *clk, struct clk *parent, u8 p_index)
  {
 -	return single_open(file, possible_parents_dump, inode->i_private);
 -}
 -
 -static const struct file_operations possible_parents_fops = {
 -	.open		= possible_parents_open,
 -	.read		= seq_read,
 -	.llseek		= seq_lseek,
 -	.release	= single_release,
 -};
 -
 -static int clk_debug_create_one(struct clk_core *core, struct dentry *pdentry)
 -{
 -	struct dentry *d;
 -	int ret = -ENOMEM;
 -
 -	if (!core || !pdentry) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	d = debugfs_create_dir(core->name, pdentry);
 -	if (!d)
 -		goto out;
 -
 -	core->dentry = d;
 -
 -	d = debugfs_create_u32("clk_rate", S_IRUGO, core->dentry,
 -			(u32 *)&core->rate);
 -	if (!d)
 -		goto err_out;
 -
 -	d = debugfs_create_u32("clk_accuracy", S_IRUGO, core->dentry,
 -			(u32 *)&core->accuracy);
 -	if (!d)
 -		goto err_out;
 -
 -	d = debugfs_create_u32("clk_phase", S_IRUGO, core->dentry,
 -			(u32 *)&core->phase);
 -	if (!d)
 -		goto err_out;
 -
 -	d = debugfs_create_x32("clk_flags", S_IRUGO, core->dentry,
 -			(u32 *)&core->flags);
 -	if (!d)
 -		goto err_out;
 -
 -	d = debugfs_create_u32("clk_prepare_count", S_IRUGO, core->dentry,
 -			(u32 *)&core->prepare_count);
 -	if (!d)
 -		goto err_out;
 +	unsigned long flags;
 +	int ret = 0;
 +	struct clk *old_parent = clk->parent;
 +	bool migrated_enable = false;
  
 -	d = debugfs_create_u32("clk_enable_count", S_IRUGO, core->dentry,
 -			(u32 *)&core->enable_count);
 -	if (!d)
 -		goto err_out;
 +	/* migrate prepare */
 +	if (clk->prepare_count)
 +		__clk_prepare(parent);
  
 -	d = debugfs_create_u32("clk_notifier_count", S_IRUGO, core->dentry,
 -			(u32 *)&core->notifier_count);
 -	if (!d)
 -		goto err_out;
 +	flags = clk_enable_lock();
  
 -	if (core->num_parents > 1) {
 -		d = debugfs_create_file("clk_possible_parents", S_IRUGO,
 -				core->dentry, core, &possible_parents_fops);
 -		if (!d)
 -			goto err_out;
 +	/* migrate enable */
 +	if (clk->enable_count) {
 +		__clk_enable(parent);
 +		migrated_enable = true;
  	}
  
 -	if (core->ops->debug_init) {
 -		ret = core->ops->debug_init(core->hw, core->dentry);
 -		if (ret)
 -			goto err_out;
 -	}
 +	/* update the clk tree topology */
 +	clk_reparent(clk, parent);
  
 -	ret = 0;
 -	goto out;
 +	clk_enable_unlock(flags);
  
 -err_out:
 -	debugfs_remove_recursive(core->dentry);
 -	core->dentry = NULL;
 -out:
 -	return ret;
 -}
 +	/* change clock input source */
 +	if (parent && clk->ops->set_parent)
 +		ret = clk->ops->set_parent(clk->hw, p_index);
  
 -/**
 - * clk_debug_register - add a clk node to the debugfs clk directory
 - * @core: the clk being added to the debugfs clk directory
 - *
 - * Dynamically adds a clk to the debugfs clk directory if debugfs has been
 - * initialized.  Otherwise it bails out early since the debugfs clk directory
 - * will be created lazily by clk_debug_init as part of a late_initcall.
 - */
 -static int clk_debug_register(struct clk_core *core)
 -{
 -	int ret = 0;
 +	if (ret) {
 +		/*
 +		 * The error handling is tricky due to that we need to release
 +		 * the spinlock while issuing the .set_parent callback. This
 +		 * means the new parent might have been enabled/disabled in
 +		 * between, which must be considered when doing rollback.
 +		 */
 +		flags = clk_enable_lock();
  
 -	mutex_lock(&clk_debug_lock);
 -	hlist_add_head(&core->debug_node, &clk_debug_list);
 +		clk_reparent(clk, old_parent);
  
 -	if (!inited)
 -		goto unlock;
 +		if (migrated_enable && clk->enable_count) {
 +			__clk_disable(parent);
 +		} else if (migrated_enable && (clk->enable_count == 0)) {
 +			__clk_disable(old_parent);
 +		} else if (!migrated_enable && clk->enable_count) {
 +			__clk_disable(parent);
 +			__clk_enable(old_parent);
 +		}
  
 -	ret = clk_debug_create_one(core, rootdir);
 -unlock:
 -	mutex_unlock(&clk_debug_lock);
 +		clk_enable_unlock(flags);
  
 -	return ret;
 -}
 +		if (clk->prepare_count)
 +			__clk_unprepare(parent);
  
 - /**
 - * clk_debug_unregister - remove a clk node from the debugfs clk directory
 - * @core: the clk being removed from the debugfs clk directory
 - *
 - * Dynamically removes a clk and all its child nodes from the
 - * debugfs clk directory if clk->dentry points to debugfs created by
 - * clk_debug_register in __clk_core_init.
 - */
 -static void clk_debug_unregister(struct clk_core *core)
 -{
 -	mutex_lock(&clk_debug_lock);
 -	hlist_del_init(&core->debug_node);
 -	debugfs_remove_recursive(core->dentry);
 -	core->dentry = NULL;
 -	mutex_unlock(&clk_debug_lock);
 -}
 +		return ret;
 +	}
  
 -struct dentry *clk_debugfs_add_file(struct clk_hw *hw, char *name, umode_t mode,
 -				void *data, const struct file_operations *fops)
 -{
 -	struct dentry *d = NULL;
 +	/* clean up enable for old parent if migration was done */
 +	if (migrated_enable) {
 +		flags = clk_enable_lock();
 +		__clk_disable(old_parent);
 +		clk_enable_unlock(flags);
 +	}
  
 -	if (hw->core->dentry)
 -		d = debugfs_create_file(name, mode, hw->core->dentry, data,
 -					fops);
 +	/* clean up prepare for old parent if migration was done */
 +	if (clk->prepare_count)
 +		__clk_unprepare(old_parent);
  
 -	return d;
 +	/* update debugfs with new clk tree topology */
 +	clk_debug_reparent(clk, parent);
 +	return 0;
  }
 -EXPORT_SYMBOL_GPL(clk_debugfs_add_file);
  
  /**
 - * clk_debug_init - lazily populate the debugfs clk directory
 + * clk_set_parent - switch the parent of a mux clk
 + * @clk: the mux clk whose input we are switching
 + * @parent: the new input to clk
   *
 - * clks are often initialized very early during boot before memory can be
 - * dynamically allocated and well before debugfs is setup. This function
 - * populates the debugfs clk directory once at boot-time when we know that
 - * debugfs is setup. It should only be called once at boot-time, all other clks
 - * added dynamically will be done so with clk_debug_register.
 + * Re-parent clk to use parent as it's new input source.  If clk has the
 + * CLK_SET_PARENT_GATE flag set then clk must be gated for this
 + * operation to succeed.  After successfully changing clk's parent
 + * clk_set_parent will update the clk topology, sysfs topology and
 + * propagate rate recalculation via __clk_recalc_rates.  Returns 0 on
 + * success, -EERROR otherwise.
   */
 -static int __init clk_debug_init(void)
 +int clk_set_parent(struct clk *clk, struct clk *parent)
  {
 -	struct clk_core *core;
 -	struct dentry *d;
 +	int ret = 0;
 +	u8 p_index = 0;
 +	unsigned long p_rate = 0;
  
 -	rootdir = debugfs_create_dir("clk", NULL);
 +	if (!clk || !clk->ops)
 +		return -EINVAL;
  
 -	if (!rootdir)
 -		return -ENOMEM;
 +	/* verify ops for for multi-parent clks */
 +	if ((clk->num_parents > 1) && (!clk->ops->set_parent))
 +		return -ENOSYS;
  
 -	d = debugfs_create_file("clk_summary", S_IRUGO, rootdir, &all_lists,
 -				&clk_summary_fops);
 -	if (!d)
 -		return -ENOMEM;
 +	/* prevent racing with updates to the clock topology */
 +	clk_prepare_lock();
  
 -	d = debugfs_create_file("clk_dump", S_IRUGO, rootdir, &all_lists,
 -				&clk_dump_fops);
 -	if (!d)
 -		return -ENOMEM;
 +	if (clk->parent == parent)
 +		goto out;
  
 -	d = debugfs_create_file("clk_orphan_summary", S_IRUGO, rootdir,
 -				&orphan_list, &clk_summary_fops);
 -	if (!d)
 -		return -ENOMEM;
 +	/* check that we are allowed to re-parent if the clock is in use */
 +	if ((clk->flags & CLK_SET_PARENT_GATE) && clk->prepare_count) {
 +		ret = -EBUSY;
 +		goto out;
 +	}
  
 -	d = debugfs_create_file("clk_orphan_dump", S_IRUGO, rootdir,
 -				&orphan_list, &clk_dump_fops);
 -	if (!d)
 -		return -ENOMEM;
 +	/* try finding the new parent index */
 +	if (parent) {
 +		p_index = clk_fetch_parent_index(clk, parent);
 +		p_rate = parent->rate;
 +		if (p_index == clk->num_parents) {
 +			pr_debug("%s: clk %s can not be parent of clk %s\n",
 +					__func__, parent->name, clk->name);
 +			ret = -EINVAL;
 +			goto out;
 +		}
 +	}
  
 -	mutex_lock(&clk_debug_lock);
 -	hlist_for_each_entry(core, &clk_debug_list, debug_node)
 -		clk_debug_create_one(core, rootdir);
 +	/* propagate PRE_RATE_CHANGE notifications */
 +	if (clk->notifier_count)
 +		ret = __clk_speculate_rates(clk, p_rate);
  
 -	inited = 1;
 -	mutex_unlock(&clk_debug_lock);
 +	/* abort if a driver objects */
 +	if (ret & NOTIFY_STOP_MASK)
 +		goto out;
  
 -	return 0;
 -}
 -late_initcall(clk_debug_init);
 -#else
 -static inline int clk_debug_register(struct clk_core *core) { return 0; }
 -static inline void clk_debug_reparent(struct clk_core *core,
 -				      struct clk_core *new_parent)
 -{
 -}
 -static inline void clk_debug_unregister(struct clk_core *core)
 -{
 +	/* do the re-parent */
 +	ret = __clk_set_parent(clk, parent, p_index);
 +
 +	/* propagate rate recalculation accordingly */
 +	if (ret)
 +		__clk_recalc_rates(clk, ABORT_RATE_CHANGE);
 +	else
 +		__clk_recalc_rates(clk, POST_RATE_CHANGE);
 +
 +out:
 +	clk_prepare_unlock();
 +
 +	return ret;
  }
 -#endif
 +EXPORT_SYMBOL_GPL(clk_set_parent);
  
  /**
 - * __clk_core_init - initialize the data structures in a struct clk_core
 - * @core:	clk_core being initialized
 + * __clk_init - initialize the data structures in a struct clk
 + * @dev:	device initializing this clk, placeholder for now
 + * @clk:	clk being initialized
   *
 - * Initializes the lists in struct clk_core, queries the hardware for the
 + * Initializes the lists in struct clk, queries the hardware for the
   * parent and rate and sets them both.
   */
 -static int __clk_core_init(struct clk_core *core)
 +int __clk_init(struct device *dev, struct clk *clk)
  {
++<<<<<<< HEAD
 +	int i, ret = 0;
 +	struct clk *orphan;
++=======
+ 	int i, ret;
+ 	struct clk_core *orphan;
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
  	struct hlist_node *tmp2;
 -	unsigned long rate;
  
 -	if (!core)
 +	if (!clk)
  		return -EINVAL;
  
  	clk_prepare_lock();
  
+ 	ret = clk_pm_runtime_get(core);
+ 	if (ret)
+ 		goto unlock;
+ 
  	/* check to see if a clock with this name is already registered */
 -	if (clk_core_lookup(core->name)) {
 +	if (__clk_lookup(clk->name)) {
  		pr_debug("%s: clk %s already initialized\n",
 -				__func__, core->name);
 +				__func__, clk->name);
  		ret = -EEXIST;
  		goto out;
  	}
@@@ -1651,16 -2589,30 +2126,18 @@@
  	 * The .init callback is not used by any of the basic clock types, but
  	 * exists for weird hardware that must perform initialization magic.
  	 * Please consider other ways of solving initialization problems before
 -	 * using this callback, as its use is discouraged.
 +	 * using this callback, as it's use is discouraged.
  	 */
 -	if (core->ops->init)
 -		core->ops->init(core->hw);
 -
 -	if (core->flags & CLK_IS_CRITICAL) {
 -		unsigned long flags;
 +	if (clk->ops->init)
 +		clk->ops->init(clk->hw);
  
 -		clk_core_prepare(core);
 -
 -		flags = clk_enable_lock();
 -		clk_core_enable(core);
 -		clk_enable_unlock(flags);
 -	}
 +	clk_debug_register(clk);
  
 -	kref_init(&core->ref);
  out:
+ 	clk_pm_runtime_put(core);
+ unlock:
  	clk_prepare_unlock();
  
 -	if (!ret)
 -		clk_debug_register(core);
 -
  	return ret;
  }
  
@@@ -1759,42 -2753,127 +2236,99 @@@ fail_name
   * @dev: device that is registering this clock
   * @hw: link to hardware-specific clock data
   *
 - * clk_hw_register is the primary interface for populating the clock tree with
 - * new clock nodes. It returns an integer equal to zero indicating success or
 - * less than zero indicating failure. Drivers must test for an error code after
 - * calling clk_hw_register().
 - */
 -int clk_hw_register(struct device *dev, struct clk_hw *hw)
 -{
 -	return PTR_ERR_OR_ZERO(clk_register(dev, hw));
 -}
 -EXPORT_SYMBOL_GPL(clk_hw_register);
 -
 -/* Free memory allocated for a clock. */
 -static void __clk_release(struct kref *ref)
 -{
 -	struct clk_core *core = container_of(ref, struct clk_core, ref);
 -	int i = core->num_parents;
 -
 -	lockdep_assert_held(&prepare_lock);
 -
 -	kfree(core->parents);
 -	while (--i >= 0)
 -		kfree_const(core->parent_names[i]);
 -
 -	kfree(core->parent_names);
 -	kfree_const(core->name);
 -	kfree(core);
 -}
 -
 -/*
 - * Empty clk_ops for unregistered clocks. These are used temporarily
 - * after clk_unregister() was called on a clock and until last clock
 - * consumer calls clk_put() and the struct clk object is freed.
 + * clk_register is the primary interface for populating the clock tree with new
 + * clock nodes.  It returns a pointer to the newly allocated struct clk which
 + * cannot be dereferenced by driver code but may be used in conjuction with the
 + * rest of the clock API.  In the event of an error clk_register will return an
 + * error code; drivers must test for an error code after calling clk_register.
   */
 -static int clk_nodrv_prepare_enable(struct clk_hw *hw)
 -{
 -	return -ENXIO;
 -}
 -
 -static void clk_nodrv_disable_unprepare(struct clk_hw *hw)
 -{
 -	WARN_ON_ONCE(1);
 -}
 -
 -static int clk_nodrv_set_rate(struct clk_hw *hw, unsigned long rate,
 -					unsigned long parent_rate)
 -{
 -	return -ENXIO;
 -}
 -
 -static int clk_nodrv_set_parent(struct clk_hw *hw, u8 index)
 +struct clk *clk_register(struct device *dev, struct clk_hw *hw)
  {
 -	return -ENXIO;
 -}
 +	int ret;
 +	struct clk *clk;
  
 -static const struct clk_ops clk_nodrv_ops = {
 -	.enable		= clk_nodrv_prepare_enable,
 -	.disable	= clk_nodrv_disable_unprepare,
 -	.prepare	= clk_nodrv_prepare_enable,
 -	.unprepare	= clk_nodrv_disable_unprepare,
 -	.set_rate	= clk_nodrv_set_rate,
 -	.set_parent	= clk_nodrv_set_parent,
 -};
 +	clk = kzalloc(sizeof(*clk), GFP_KERNEL);
 +	if (!clk) {
 +		pr_err("%s: could not allocate clk\n", __func__);
 +		ret = -ENOMEM;
 +		goto fail_out;
 +	}
  
 -/**
 - * clk_unregister - unregister a currently registered clock
 - * @clk: clock to unregister
 - */
 -void clk_unregister(struct clk *clk)
 -{
 -	unsigned long flags;
++<<<<<<< HEAD
 +	ret = _clk_register(dev, hw, clk);
++=======
++	core->name = kstrdup_const(hw->init->name, GFP_KERNEL);
++	if (!core->name) {
++		ret = -ENOMEM;
++		goto fail_name;
++	}
++	core->ops = hw->init->ops;
++	if (dev && pm_runtime_enabled(dev))
++		core->dev = dev;
++	if (dev && dev->driver)
++		core->owner = dev->driver->owner;
++	core->hw = hw;
++	core->flags = hw->init->flags;
++	core->num_parents = hw->init->num_parents;
++	core->min_rate = 0;
++	core->max_rate = ULONG_MAX;
++	hw->core = core;
+ 
 -	if (!clk || WARN_ON_ONCE(IS_ERR(clk)))
 -		return;
++	/* allocate local copy in case parent_names is __initdata */
++	core->parent_names = kcalloc(core->num_parents, sizeof(char *),
++					GFP_KERNEL);
+ 
 -	clk_debug_unregister(clk->core);
++	if (!core->parent_names) {
++		ret = -ENOMEM;
++		goto fail_parent_names;
++	}
+ 
 -	clk_prepare_lock();
+ 
 -	if (clk->core->ops == &clk_nodrv_ops) {
 -		pr_err("%s: unregistered clock: %s\n", __func__,
 -		       clk->core->name);
 -		goto unlock;
++	/* copy each string name in case parent_names is __initdata */
++	for (i = 0; i < core->num_parents; i++) {
++		core->parent_names[i] = kstrdup_const(hw->init->parent_names[i],
++						GFP_KERNEL);
++		if (!core->parent_names[i]) {
++			ret = -ENOMEM;
++			goto fail_parent_names_copy;
++		}
+ 	}
 -	/*
 -	 * Assign empty clock ops for consumers that might still hold
 -	 * a reference to this clock.
 -	 */
 -	flags = clk_enable_lock();
 -	clk->core->ops = &clk_nodrv_ops;
 -	clk_enable_unlock(flags);
+ 
 -	if (!hlist_empty(&clk->core->children)) {
 -		struct clk_core *child;
 -		struct hlist_node *t;
++	/* avoid unnecessary string look-ups of clk_core's possible parents. */
++	core->parents = kcalloc(core->num_parents, sizeof(*core->parents),
++				GFP_KERNEL);
++	if (!core->parents) {
++		ret = -ENOMEM;
++		goto fail_parents;
++	};
++
++	INIT_HLIST_HEAD(&core->clks);
+ 
 -		/* Reparent all children to the orphan list. */
 -		hlist_for_each_entry_safe(child, t, &clk->core->children,
 -					  child_node)
 -			clk_core_set_parent(child, NULL);
++	hw->clk = __clk_create_clk(hw, NULL, NULL);
++	if (IS_ERR(hw->clk)) {
++		ret = PTR_ERR(hw->clk);
++		goto fail_parents;
+ 	}
+ 
 -	hlist_del_init(&clk->core->child_node);
++	ret = __clk_core_init(core);
++>>>>>>> 9a34b45397e5 (clk: Add support for runtime PM)
 +	if (!ret)
 +		return clk;
  
 -	if (clk->core->prepare_count)
 -		pr_warn("%s: unregistering prepared clock: %s\n",
 -					__func__, clk->core->name);
 -	kref_put(&clk->core->ref, __clk_release);
 -unlock:
 -	clk_prepare_unlock();
 +	kfree(clk);
 +fail_out:
 +	return ERR_PTR(ret);
  }
 -EXPORT_SYMBOL_GPL(clk_unregister);
 +EXPORT_SYMBOL_GPL(clk_register);
  
  /**
 - * clk_hw_unregister - unregister a currently registered clk_hw
 - * @hw: hardware-specific clock data to unregister
 + * clk_unregister - unregister a currently registered clock
 + * @clk: clock to unregister
 + *
 + * Currently unimplemented.
   */
 -void clk_hw_unregister(struct clk_hw *hw)
 -{
 -	clk_unregister(hw->clk);
 -}
 -EXPORT_SYMBOL_GPL(clk_hw_unregister);
 +void clk_unregister(struct clk *clk) {}
 +EXPORT_SYMBOL_GPL(clk_unregister);
  
  static void devm_clk_release(struct device *dev, void *res)
  {
* Unmerged path drivers/clk/clk.c
