perf stat: Move create_perf_stat_counter() to stat.c

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jiri Olsa <jolsa@kernel.org>
commit d09cefd2ef9945b4b767bb67f473a0eb2066374f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d09cefd2.failed

Move create_perf_stat_counter() to the 'stat' class, so that we can use
it globally.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Andi Kleen <andi@firstfloor.org>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/20180830063252.23729-9-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit d09cefd2ef9945b4b767bb67f473a0eb2066374f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/builtin-stat.c
diff --cc tools/perf/builtin-stat.c
index 5193f0aa2406,142cff8eb12b..000000000000
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@@ -234,70 -234,6 +234,73 @@@ static void perf_stat__reset_stats(void
  		perf_stat__reset_shadow_per_stat(&stat_config.stats[i]);
  }
  
++<<<<<<< HEAD
 +static int create_perf_stat_counter(struct perf_evsel *evsel,
 +				    struct perf_stat_config *config)
 +{
 +	struct perf_event_attr *attr = &evsel->attr;
 +	struct perf_evsel *leader = evsel->leader;
 +
 +	if (config->scale) {
 +		attr->read_format = PERF_FORMAT_TOTAL_TIME_ENABLED |
 +				    PERF_FORMAT_TOTAL_TIME_RUNNING;
 +	}
 +
 +	/*
 +	 * The event is part of non trivial group, let's enable
 +	 * the group read (for leader) and ID retrieval for all
 +	 * members.
 +	 */
 +	if (leader->nr_members > 1)
 +		attr->read_format |= PERF_FORMAT_ID|PERF_FORMAT_GROUP;
 +
 +	attr->inherit = !config->no_inherit;
 +
 +	/*
 +	 * Some events get initialized with sample_(period/type) set,
 +	 * like tracepoints. Clear it up for counting.
 +	 */
 +	attr->sample_period = 0;
 +
 +	if (config->identifier)
 +		attr->sample_type = PERF_SAMPLE_IDENTIFIER;
 +
 +	/*
 +	 * Disabling all counters initially, they will be enabled
 +	 * either manually by us or by kernel via enable_on_exec
 +	 * set later.
 +	 */
 +	if (perf_evsel__is_group_leader(evsel)) {
 +		attr->disabled = 1;
 +
 +		/*
 +		 * In case of initial_delay we enable tracee
 +		 * events manually.
 +		 */
 +		if (target__none(&target) && !config->initial_delay)
 +			attr->enable_on_exec = 1;
 +	}
 +
 +	if (target__has_cpu(&target) && !target__has_per_thread(&target))
 +		return perf_evsel__open_per_cpu(evsel, perf_evsel__cpus(evsel));
 +
 +	return perf_evsel__open_per_thread(evsel, evsel->threads);
 +}
 +
 +/*
 + * Does the counter have nsecs as a unit?
 + */
 +static inline int nsec_counter(struct perf_evsel *evsel)
 +{
 +	if (perf_evsel__match(evsel, SOFTWARE, SW_CPU_CLOCK) ||
 +	    perf_evsel__match(evsel, SOFTWARE, SW_TASK_CLOCK))
 +		return 1;
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> d09cefd2ef99 (perf stat: Move create_perf_stat_counter() to stat.c)
  static int process_synthesized_event(struct perf_tool *tool __maybe_unused,
  				     union perf_event *event,
  				     struct perf_sample *sample __maybe_unused,
* Unmerged path tools/perf/builtin-stat.c
diff --git a/tools/perf/util/stat.c b/tools/perf/util/stat.c
index 79602e19daba..3d5d25810d56 100644
--- a/tools/perf/util/stat.c
+++ b/tools/perf/util/stat.c
@@ -427,3 +427,56 @@ size_t perf_event__fprintf_stat_config(union perf_event *event, FILE *fp)
 
 	return ret;
 }
+
+int create_perf_stat_counter(struct perf_evsel *evsel,
+			     struct perf_stat_config *config,
+			     struct target *target)
+{
+	struct perf_event_attr *attr = &evsel->attr;
+	struct perf_evsel *leader = evsel->leader;
+
+	if (config->scale) {
+		attr->read_format = PERF_FORMAT_TOTAL_TIME_ENABLED |
+				    PERF_FORMAT_TOTAL_TIME_RUNNING;
+	}
+
+	/*
+	 * The event is part of non trivial group, let's enable
+	 * the group read (for leader) and ID retrieval for all
+	 * members.
+	 */
+	if (leader->nr_members > 1)
+		attr->read_format |= PERF_FORMAT_ID|PERF_FORMAT_GROUP;
+
+	attr->inherit = !config->no_inherit;
+
+	/*
+	 * Some events get initialized with sample_(period/type) set,
+	 * like tracepoints. Clear it up for counting.
+	 */
+	attr->sample_period = 0;
+
+	if (config->identifier)
+		attr->sample_type = PERF_SAMPLE_IDENTIFIER;
+
+	/*
+	 * Disabling all counters initially, they will be enabled
+	 * either manually by us or by kernel via enable_on_exec
+	 * set later.
+	 */
+	if (perf_evsel__is_group_leader(evsel)) {
+		attr->disabled = 1;
+
+		/*
+		 * In case of initial_delay we enable tracee
+		 * events manually.
+		 */
+		if (target__none(target) && !config->initial_delay)
+			attr->enable_on_exec = 1;
+	}
+
+	if (target__has_cpu(target) && !target__has_per_thread(target))
+		return perf_evsel__open_per_cpu(evsel, perf_evsel__cpus(evsel));
+
+	return perf_evsel__open_per_thread(evsel, evsel->threads);
+}
diff --git a/tools/perf/util/stat.h b/tools/perf/util/stat.h
index 4b88aef959fb..326eed357841 100644
--- a/tools/perf/util/stat.h
+++ b/tools/perf/util/stat.h
@@ -163,4 +163,8 @@ int perf_event__process_stat_event(struct perf_tool *tool,
 size_t perf_event__fprintf_stat(union perf_event *event, FILE *fp);
 size_t perf_event__fprintf_stat_round(union perf_event *event, FILE *fp);
 size_t perf_event__fprintf_stat_config(union perf_event *event, FILE *fp);
+
+int create_perf_stat_counter(struct perf_evsel *evsel,
+			     struct perf_stat_config *config,
+			     struct target *target);
 #endif
