IB/hfi1: Remove race conditions in user_sdma send path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Michael J. Ruhl <michael.j.ruhl@intel.com>
commit 28a9a9e83ceae2cee25b9af9ad20d53aaa9ab951
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/28a9a9e8.failed

Packet queue state is over used to determine SDMA descriptor
availablitity and packet queue request state.

cpu 0  ret = user_sdma_send_pkts(req, pcount);
cpu 0  if (atomic_read(&pq->n_reqs))
cpu 1  IRQ user_sdma_txreq_cb calls pq_update() (state to _INACTIVE)
cpu 0        xchg(&pq->state, SDMA_PKT_Q_ACTIVE);

At this point pq->n_reqs == 0 and pq->state is incorrectly
SDMA_PKT_Q_ACTIVE.  The close path will hang waiting for the state
to return to _INACTIVE.

This can also change the state from _DEFERRED to _ACTIVE.  However,
this is a mostly benign race.

Remove the racy code path.

Use n_reqs to determine if a packet queue is active or not.

	Reviewed-by: Mitko Haralanov <mitko.haralanov@intel.com>
	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 28a9a9e83ceae2cee25b9af9ad20d53aaa9ab951)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/user_sdma.c
diff --cc drivers/infiniband/hw/hfi1/user_sdma.c
index 3d0dc7ab45d1,fd6e9f56f53e..000000000000
--- a/drivers/infiniband/hw/hfi1/user_sdma.c
+++ b/drivers/infiniband/hw/hfi1/user_sdma.c
@@@ -276,7 -275,7 +275,11 @@@ int hfi1_user_sdma_free_queues(struct h
  		/* Wait until all requests have been freed. */
  		wait_event_interruptible(
  			pq->wait,
++<<<<<<< HEAD
 +			(ACCESS_ONCE(pq->state) == SDMA_PKT_Q_INACTIVE));
++=======
+ 			!atomic_read(&pq->n_reqs));
++>>>>>>> 28a9a9e83cea (IB/hfi1: Remove race conditions in user_sdma send path)
  		kfree(pq->reqs);
  		kfree(pq->req_in_use);
  		kmem_cache_destroy(pq->txreq_cache);
@@@ -561,24 -566,12 +571,19 @@@ int hfi1_user_sdma_process_request(stru
  		req->ahg_idx = sdma_ahg_alloc(req->sde);
  
  	set_comp_state(pq, cq, info.comp_idx, QUEUED, 0);
++<<<<<<< HEAD
 +	atomic_inc(&pq->n_reqs);
 +	req_queued = 1;
++=======
+ 	pq->state = SDMA_PKT_Q_ACTIVE;
++>>>>>>> 28a9a9e83cea (IB/hfi1: Remove race conditions in user_sdma send path)
  	/* Send the first N packets in the request to buy us some time */
  	ret = user_sdma_send_pkts(req, pcount);
 -	if (unlikely(ret < 0 && ret != -EBUSY))
 +	if (unlikely(ret < 0 && ret != -EBUSY)) {
 +		req->status = ret;
  		goto free_req;
 +	}
  
- 	/*
- 	 * It is possible that the SDMA engine would have processed all the
- 	 * submitted packets by the time we get here. Therefore, only set
- 	 * packet queue state to ACTIVE if there are still uncompleted
- 	 * requests.
- 	 */
- 	if (atomic_read(&pq->n_reqs))
- 		xchg(&pq->state, SDMA_PKT_Q_ACTIVE);
- 
  	/*
  	 * This is a somewhat blocking send implementation.
  	 * The driver will block the caller until all packets of the
* Unmerged path drivers/infiniband/hw/hfi1/user_sdma.c
diff --git a/drivers/infiniband/hw/hfi1/user_sdma.h b/drivers/infiniband/hw/hfi1/user_sdma.h
index d2bc77f75253..40d69e8d1255 100644
--- a/drivers/infiniband/hw/hfi1/user_sdma.h
+++ b/drivers/infiniband/hw/hfi1/user_sdma.h
@@ -105,9 +105,10 @@ static inline int ahg_header_set(u32 *arr, int idx, size_t array_size,
 #define TXREQ_FLAGS_REQ_ACK   BIT(0)      /* Set the ACK bit in the header */
 #define TXREQ_FLAGS_REQ_DISABLE_SH BIT(1) /* Disable header suppression */
 
-#define SDMA_PKT_Q_INACTIVE BIT(0)
-#define SDMA_PKT_Q_ACTIVE   BIT(1)
-#define SDMA_PKT_Q_DEFERRED BIT(2)
+enum pkt_q_sdma_state {
+	SDMA_PKT_Q_ACTIVE,
+	SDMA_PKT_Q_DEFERRED,
+};
 
 /*
  * Maximum retry attempts to submit a TX request
@@ -133,7 +134,7 @@ struct hfi1_user_sdma_pkt_q {
 	struct user_sdma_request *reqs;
 	unsigned long *req_in_use;
 	struct iowait busy;
-	unsigned state;
+	enum pkt_q_sdma_state state;
 	wait_queue_head_t wait;
 	unsigned long unpinned;
 	struct mmu_rb_handler *handler;
