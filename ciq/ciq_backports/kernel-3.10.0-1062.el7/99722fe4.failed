svcrdma: Persistently allocate and DMA-map Send buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 99722fe4d5a634707ced8d8f42b883b87a86b3c5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/99722fe4.failed

While sending each RPC Reply, svc_rdma_sendto allocates and DMA-
maps a separate buffer where the RPC/RDMA transport header is
constructed. The buffer is unmapped and released in the Send
completion handler. This is significant per-RPC overhead,
especially for small RPCs.

Instead, allocate and DMA-map a buffer, and cache it in each
svc_rdma_send_ctxt. This buffer and its mapping can be re-used
for each RPC, saving the cost of memory allocation and DMA
mapping.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: J. Bruce Fields <bfields@redhat.com>
(cherry picked from commit 99722fe4d5a634707ced8d8f42b883b87a86b3c5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sunrpc/svc_rdma.h
#	net/sunrpc/xprtrdma/svc_rdma_backchannel.c
#	net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
#	net/sunrpc/xprtrdma/svc_rdma_sendto.c
diff --cc include/linux/sunrpc/svc_rdma.h
index e1e3c42bedad,96b14a72d359..000000000000
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@@ -142,12 -144,30 +142,39 @@@ struct svcxprt_rdma 
  
  #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
  
++<<<<<<< HEAD
 +/* Track DMA maps for this transport and context */
 +static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 +					   struct svc_rdma_op_ctxt *ctxt)
 +{
 +	ctxt->mapped_sges++;
 +}
++=======
+ struct svc_rdma_recv_ctxt {
+ 	struct list_head	rc_list;
+ 	struct ib_recv_wr	rc_recv_wr;
+ 	struct ib_cqe		rc_cqe;
+ 	struct ib_sge		rc_recv_sge;
+ 	void			*rc_recv_buf;
+ 	struct xdr_buf		rc_arg;
+ 	bool			rc_temp;
+ 	u32			rc_byte_len;
+ 	unsigned int		rc_page_count;
+ 	unsigned int		rc_hdr_count;
+ 	struct page		*rc_pages[RPCSVC_MAXPAGES];
+ };
+ 
+ struct svc_rdma_send_ctxt {
+ 	struct list_head	sc_list;
+ 	struct ib_send_wr	sc_send_wr;
+ 	struct ib_cqe		sc_cqe;
+ 	void			*sc_xprt_buf;
+ 	int			sc_page_count;
+ 	int			sc_cur_sge_no;
+ 	struct page		*sc_pages[RPCSVC_MAXPAGES];
+ 	struct ib_sge		sc_sges[];
+ };
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  
  /* svc_rdma_backchannel.c */
  extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
@@@ -169,12 -194,18 +196,27 @@@ extern int svc_rdma_send_reply_chunk(st
  				     struct xdr_buf *xdr);
  
  /* svc_rdma_sendto.c */
++<<<<<<< HEAD
 +extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
 +				  struct svc_rdma_op_ctxt *ctxt,
 +				  __be32 *rdma_resp, unsigned int len);
 +extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
 +				 struct svc_rdma_op_ctxt *ctxt,
 +				 u32 inv_rkey);
++=======
+ extern void svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma);
+ extern struct svc_rdma_send_ctxt *
+ 		svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma);
+ extern void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
+ 				   struct svc_rdma_send_ctxt *ctxt);
+ extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
+ extern void svc_rdma_sync_reply_hdr(struct svcxprt_rdma *rdma,
+ 				    struct svc_rdma_send_ctxt *ctxt,
+ 				    unsigned int len);
+ extern int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
+ 				  struct svc_rdma_send_ctxt *ctxt,
+ 				  struct xdr_buf *xdr, __be32 *wr_lst);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  extern int svc_rdma_sendto(struct svc_rqst *);
  
  /* svc_rdma_transport.c */
diff --cc net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 855b0740ad11,343e7add672c..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@@ -114,39 -115,21 +114,51 @@@ out_notfound
   * the adapter has a small maximum SQ depth.
   */
  static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
- 			      struct rpc_rqst *rqst)
+ 			      struct rpc_rqst *rqst,
+ 			      struct svc_rdma_send_ctxt *ctxt)
  {
++<<<<<<< HEAD
 +	struct svc_rdma_op_ctxt *ctxt;
 +	int ret;
 +
 +	ctxt = svc_rdma_get_context(rdma);
 +
 +	/* rpcrdma_bc_send_request builds the transport header and
 +	 * the backchannel RPC message in the same buffer. Thus only
 +	 * one SGE is needed to send both.
 +	 */
 +	ret = svc_rdma_map_reply_hdr(rdma, ctxt, rqst->rq_buffer,
 +				     rqst->rq_snd_buf.len);
++=======
+ 	int ret;
+ 
+ 	ret = svc_rdma_map_reply_msg(rdma, ctxt, &rqst->rq_snd_buf, NULL);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  	if (ret < 0)
- 		goto out_err;
+ 		return -EIO;
  
  	/* Bump page refcnt so Send completion doesn't release
  	 * the rq_buffer before all retransmits are complete.
  	 */
  	get_page(virt_to_page(rqst->rq_buffer));
++<<<<<<< HEAD
 +	ret = svc_rdma_post_send_wr(rdma, ctxt, 0);
 +	if (ret)
 +		goto out_unmap;
 +
 +out_err:
 +	dprintk("svcrdma: %s returns %d\n", __func__, ret);
 +	return ret;
 +
 +out_unmap:
 +	svc_rdma_unmap_dma(ctxt);
 +	svc_rdma_put_context(ctxt, 1);
 +	ret = -EIO;
 +	goto out_err;
++=======
+ 	ctxt->sc_send_wr.opcode = IB_WR_SEND;
+ 	return svc_rdma_send(rdma, &ctxt->sc_send_wr);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  }
  
  /* Server-side transport endpoint wants a whole page for its send
diff --cc net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
index 2eed6e104513,09ce09b3ac6e..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
@@@ -392,18 -601,16 +392,21 @@@ static void rdma_read_complete(struct s
  static void svc_rdma_send_error(struct svcxprt_rdma *xprt,
  				__be32 *rdma_argp, int status)
  {
++<<<<<<< HEAD
 +	struct svc_rdma_op_ctxt *ctxt;
 +	__be32 *p, *err_msgp;
++=======
+ 	struct svc_rdma_send_ctxt *ctxt;
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  	unsigned int length;
- 	struct page *page;
+ 	__be32 *p;
  	int ret;
  
- 	page = alloc_page(GFP_KERNEL);
- 	if (!page)
+ 	ctxt = svc_rdma_send_ctxt_get(xprt);
+ 	if (!ctxt)
  		return;
- 	err_msgp = page_address(page);
  
- 	p = err_msgp;
+ 	p = ctxt->sc_xprt_buf;
  	*p++ = *rdma_argp;
  	*p++ = *(rdma_argp + 1);
  	*p++ = xprt->sc_fc_credits;
@@@ -419,22 -626,13 +422,27 @@@
  		*p++ = err_chunk;
  		trace_svcrdma_err_chunk(*rdma_argp);
  	}
++<<<<<<< HEAD
 +	length = (unsigned long)p - (unsigned long)err_msgp;
 +
 +	/* Map transport header; no RPC message payload */
 +	ctxt = svc_rdma_get_context(xprt);
 +	ret = svc_rdma_map_reply_hdr(xprt, ctxt, err_msgp, length);
 +	if (ret) {
 +		dprintk("svcrdma: Error %d mapping send for protocol error\n",
 +			ret);
 +		return;
 +	}
++=======
+ 	length = (unsigned long)p - (unsigned long)ctxt->sc_xprt_buf;
+ 	svc_rdma_sync_reply_hdr(xprt, ctxt, length);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  
 -	ctxt->sc_send_wr.opcode = IB_WR_SEND;
 -	ret = svc_rdma_send(xprt, &ctxt->sc_send_wr);
 -	if (ret)
 -		svc_rdma_send_ctxt_put(xprt, ctxt);
 +	ret = svc_rdma_post_send_wr(xprt, ctxt, 0);
 +	if (ret) {
 +		svc_rdma_unmap_dma(ctxt);
 +		svc_rdma_put_context(ctxt, 1);
 +	}
  }
  
  /* By convention, backchannel calls arrive via rdma_msg type
diff --cc net/sunrpc/xprtrdma/svc_rdma_sendto.c
index 00a497c70989,4a3efaea277c..000000000000
--- a/net/sunrpc/xprtrdma/svc_rdma_sendto.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_sendto.c
@@@ -114,6 -114,215 +114,218 @@@
  
  #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
  
++<<<<<<< HEAD
++=======
+ static void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc);
+ 
+ static inline struct svc_rdma_send_ctxt *
+ svc_rdma_next_send_ctxt(struct list_head *list)
+ {
+ 	return list_first_entry_or_null(list, struct svc_rdma_send_ctxt,
+ 					sc_list);
+ }
+ 
+ static struct svc_rdma_send_ctxt *
+ svc_rdma_send_ctxt_alloc(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 	dma_addr_t addr;
+ 	void *buffer;
+ 	size_t size;
+ 	int i;
+ 
+ 	size = sizeof(*ctxt);
+ 	size += rdma->sc_max_send_sges * sizeof(struct ib_sge);
+ 	ctxt = kmalloc(size, GFP_KERNEL);
+ 	if (!ctxt)
+ 		goto fail0;
+ 	buffer = kmalloc(rdma->sc_max_req_size, GFP_KERNEL);
+ 	if (!buffer)
+ 		goto fail1;
+ 	addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
+ 				 rdma->sc_max_req_size, DMA_TO_DEVICE);
+ 	if (ib_dma_mapping_error(rdma->sc_pd->device, addr))
+ 		goto fail2;
+ 
+ 	ctxt->sc_send_wr.next = NULL;
+ 	ctxt->sc_send_wr.wr_cqe = &ctxt->sc_cqe;
+ 	ctxt->sc_send_wr.sg_list = ctxt->sc_sges;
+ 	ctxt->sc_send_wr.send_flags = IB_SEND_SIGNALED;
+ 	ctxt->sc_cqe.done = svc_rdma_wc_send;
+ 	ctxt->sc_xprt_buf = buffer;
+ 	ctxt->sc_sges[0].addr = addr;
+ 
+ 	for (i = 0; i < rdma->sc_max_send_sges; i++)
+ 		ctxt->sc_sges[i].lkey = rdma->sc_pd->local_dma_lkey;
+ 	return ctxt;
+ 
+ fail2:
+ 	kfree(buffer);
+ fail1:
+ 	kfree(ctxt);
+ fail0:
+ 	return NULL;
+ }
+ 
+ /**
+  * svc_rdma_send_ctxts_destroy - Release all send_ctxt's for an xprt
+  * @rdma: svcxprt_rdma being torn down
+  *
+  */
+ void svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	while ((ctxt = svc_rdma_next_send_ctxt(&rdma->sc_send_ctxts))) {
+ 		list_del(&ctxt->sc_list);
+ 		ib_dma_unmap_single(rdma->sc_pd->device,
+ 				    ctxt->sc_sges[0].addr,
+ 				    rdma->sc_max_req_size,
+ 				    DMA_TO_DEVICE);
+ 		kfree(ctxt->sc_xprt_buf);
+ 		kfree(ctxt);
+ 	}
+ }
+ 
+ /**
+  * svc_rdma_send_ctxt_get - Get a free send_ctxt
+  * @rdma: controlling svcxprt_rdma
+  *
+  * Returns a ready-to-use send_ctxt, or NULL if none are
+  * available and a fresh one cannot be allocated.
+  */
+ struct svc_rdma_send_ctxt *svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma)
+ {
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	spin_lock(&rdma->sc_send_lock);
+ 	ctxt = svc_rdma_next_send_ctxt(&rdma->sc_send_ctxts);
+ 	if (!ctxt)
+ 		goto out_empty;
+ 	list_del(&ctxt->sc_list);
+ 	spin_unlock(&rdma->sc_send_lock);
+ 
+ out:
+ 	ctxt->sc_send_wr.num_sge = 0;
+ 	ctxt->sc_cur_sge_no = 0;
+ 	ctxt->sc_page_count = 0;
+ 	return ctxt;
+ 
+ out_empty:
+ 	spin_unlock(&rdma->sc_send_lock);
+ 	ctxt = svc_rdma_send_ctxt_alloc(rdma);
+ 	if (!ctxt)
+ 		return NULL;
+ 	goto out;
+ }
+ 
+ /**
+  * svc_rdma_send_ctxt_put - Return send_ctxt to free list
+  * @rdma: controlling svcxprt_rdma
+  * @ctxt: object to return to the free list
+  *
+  * Pages left in sc_pages are DMA unmapped and released.
+  */
+ void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
+ 			    struct svc_rdma_send_ctxt *ctxt)
+ {
+ 	struct ib_device *device = rdma->sc_cm_id->device;
+ 	unsigned int i;
+ 
+ 	/* The first SGE contains the transport header, which
+ 	 * remains mapped until @ctxt is destroyed.
+ 	 */
+ 	for (i = 1; i < ctxt->sc_send_wr.num_sge; i++)
+ 		ib_dma_unmap_page(device,
+ 				  ctxt->sc_sges[i].addr,
+ 				  ctxt->sc_sges[i].length,
+ 				  DMA_TO_DEVICE);
+ 
+ 	for (i = 0; i < ctxt->sc_page_count; ++i)
+ 		put_page(ctxt->sc_pages[i]);
+ 
+ 	spin_lock(&rdma->sc_send_lock);
+ 	list_add(&ctxt->sc_list, &rdma->sc_send_ctxts);
+ 	spin_unlock(&rdma->sc_send_lock);
+ }
+ 
+ /**
+  * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC
+  * @cq: Completion Queue context
+  * @wc: Work Completion object
+  *
+  * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that
+  * the Send completion handler could be running.
+  */
+ static void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct svcxprt_rdma *rdma = cq->cq_context;
+ 	struct ib_cqe *cqe = wc->wr_cqe;
+ 	struct svc_rdma_send_ctxt *ctxt;
+ 
+ 	trace_svcrdma_wc_send(wc);
+ 
+ 	atomic_inc(&rdma->sc_sq_avail);
+ 	wake_up(&rdma->sc_send_wait);
+ 
+ 	ctxt = container_of(cqe, struct svc_rdma_send_ctxt, sc_cqe);
+ 	svc_rdma_send_ctxt_put(rdma, ctxt);
+ 
+ 	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+ 		set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 		svc_xprt_enqueue(&rdma->sc_xprt);
+ 		if (wc->status != IB_WC_WR_FLUSH_ERR)
+ 			pr_err("svcrdma: Send: %s (%u/0x%x)\n",
+ 			       ib_wc_status_msg(wc->status),
+ 			       wc->status, wc->vendor_err);
+ 	}
+ 
+ 	svc_xprt_put(&rdma->sc_xprt);
+ }
+ 
+ /**
+  * svc_rdma_send - Post a single Send WR
+  * @rdma: transport on which to post the WR
+  * @wr: prepared Send WR to post
+  *
+  * Returns zero the Send WR was posted successfully. Otherwise, a
+  * negative errno is returned.
+  */
+ int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr)
+ {
+ 	struct ib_send_wr *bad_wr;
+ 	int ret;
+ 
+ 	might_sleep();
+ 
+ 	/* If the SQ is full, wait until an SQ entry is available */
+ 	while (1) {
+ 		if ((atomic_dec_return(&rdma->sc_sq_avail) < 0)) {
+ 			atomic_inc(&rdma_stat_sq_starve);
+ 			trace_svcrdma_sq_full(rdma);
+ 			atomic_inc(&rdma->sc_sq_avail);
+ 			wait_event(rdma->sc_send_wait,
+ 				   atomic_read(&rdma->sc_sq_avail) > 1);
+ 			if (test_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags))
+ 				return -ENOTCONN;
+ 			trace_svcrdma_sq_retry(rdma);
+ 			continue;
+ 		}
+ 
+ 		svc_xprt_get(&rdma->sc_xprt);
+ 		ret = ib_post_send(rdma->sc_qp, wr, &bad_wr);
+ 		trace_svcrdma_post_send(wr, ret);
+ 		if (ret) {
+ 			set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+ 			svc_xprt_put(&rdma->sc_xprt);
+ 			wake_up(&rdma->sc_send_wait);
+ 		}
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  static u32 xdr_padsize(u32 len)
  {
  	return (len & 3) ? (4 - (len & 3)) : 0;
@@@ -357,38 -534,53 +569,58 @@@ out_maperr
  	return -EIO;
  }
  
 -/* ib_dma_map_page() is used here because svc_rdma_dma_unmap()
 - * handles DMA-unmap and it uses ib_dma_unmap_page() exclusively.
 - */
 -static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,
 -				struct svc_rdma_send_ctxt *ctxt,
 -				unsigned char *base,
 -				unsigned int len)
 -{
 -	return svc_rdma_dma_map_page(rdma, ctxt, virt_to_page(base),
 -				     offset_in_page(base), len);
 -}
 -
  /**
-  * svc_rdma_map_reply_hdr - DMA map the transport header buffer
+  * svc_rdma_sync_reply_hdr - DMA sync the transport header buffer
   * @rdma: controlling transport
-  * @ctxt: op_ctxt for the Send WR
-  * @rdma_resp: buffer containing transport header
+  * @ctxt: send_ctxt for the Send WR
   * @len: length of transport header
   *
-  * Returns:
-  *	%0 if the header is DMA mapped,
-  *	%-EIO if DMA mapping failed.
   */
++<<<<<<< HEAD
 +int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
 +			   struct svc_rdma_op_ctxt *ctxt,
 +			   __be32 *rdma_resp,
 +			   unsigned int len)
 +{
 +	ctxt->direction = DMA_TO_DEVICE;
 +	ctxt->pages[0] = virt_to_page(rdma_resp);
 +	ctxt->count = 1;
 +	return svc_rdma_dma_map_page(rdma, ctxt, 0, ctxt->pages[0], 0, len);
++=======
+ void svc_rdma_sync_reply_hdr(struct svcxprt_rdma *rdma,
+ 			     struct svc_rdma_send_ctxt *ctxt,
+ 			     unsigned int len)
+ {
+ 	ctxt->sc_sges[0].length = len;
+ 	ctxt->sc_send_wr.num_sge++;
+ 	ib_dma_sync_single_for_device(rdma->sc_pd->device,
+ 				      ctxt->sc_sges[0].addr, len,
+ 				      DMA_TO_DEVICE);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  }
  
- /* Load the xdr_buf into the ctxt's sge array, and DMA map each
+ /* svc_rdma_map_reply_msg - Map the buffer holding RPC message
+  * @rdma: controlling transport
+  * @ctxt: send_ctxt for the Send WR
+  * @xdr: prepared xdr_buf containing RPC message
+  * @wr_lst: pointer to Call header's Write list, or NULL
+  *
+  * Load the xdr_buf into the ctxt's sge array, and DMA map each
   * element as it is added.
   *
   * Returns zero on success, or a negative errno on failure.
   */
++<<<<<<< HEAD
 +static int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
 +				  struct svc_rdma_op_ctxt *ctxt,
 +				  struct xdr_buf *xdr, __be32 *wr_lst)
++=======
+ int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
+ 			   struct svc_rdma_send_ctxt *ctxt,
+ 			   struct xdr_buf *xdr, __be32 *wr_lst)
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  {
 -	unsigned int len, remaining;
 -	unsigned long page_off;
 +	unsigned int len, sge_no, remaining, page_off;
  	struct page **ppages;
  	unsigned char *base;
  	u32 xdr_pad;
@@@ -456,9 -652,9 +688,13 @@@ static void svc_rdma_save_io_pages(stru
  {
  	int i, pages = rqstp->rq_next_page - rqstp->rq_respages;
  
 -	ctxt->sc_page_count += pages;
 +	ctxt->count += pages;
  	for (i = 0; i < pages; i++) {
++<<<<<<< HEAD
 +		ctxt->pages[i + 1] = rqstp->rq_respages[i];
++=======
+ 		ctxt->sc_pages[i] = rqstp->rq_respages[i];
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  		rqstp->rq_respages[i] = NULL;
  	}
  	rqstp->rq_next_page = rqstp->rq_respages + 1;
@@@ -524,17 -684,8 +761,22 @@@ static int svc_rdma_send_reply_msg(stru
  				   struct svc_rqst *rqstp,
  				   __be32 *wr_lst, __be32 *rp_ch)
  {
++<<<<<<< HEAD
 +	struct svc_rdma_op_ctxt *ctxt;
 +	u32 inv_rkey;
  	int ret;
  
 +	ctxt = svc_rdma_get_context(rdma);
 +
 +	ret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp,
 +				     svc_rdma_reply_hdr_len(rdma_resp));
 +	if (ret < 0)
 +		goto err;
 +
++=======
++	int ret;
++
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  	if (!rp_ch) {
  		ret = svc_rdma_map_reply_msg(rdma, ctxt,
  					     &rqstp->rq_res, wr_lst);
@@@ -544,19 -695,16 +786,32 @@@
  
  	svc_rdma_save_io_pages(rqstp, ctxt);
  
++<<<<<<< HEAD
 +	inv_rkey = 0;
 +	if (rdma->sc_snd_w_inv)
 +		inv_rkey = svc_rdma_get_inv_rkey(rdma_argp, wr_lst, rp_ch);
 +	ret = svc_rdma_post_send_wr(rdma, ctxt, inv_rkey);
 +	if (ret)
 +		goto err;
 +
 +	return 0;
 +
 +err:
 +	svc_rdma_unmap_dma(ctxt);
 +	svc_rdma_put_context(ctxt, 1);
 +	return ret;
++=======
+ 	ctxt->sc_send_wr.opcode = IB_WR_SEND;
+ 	if (rdma->sc_snd_w_inv) {
+ 		ctxt->sc_send_wr.ex.invalidate_rkey =
+ 			svc_rdma_get_inv_rkey(rdma_argp, wr_lst, rp_ch);
+ 		if (ctxt->sc_send_wr.ex.invalidate_rkey)
+ 			ctxt->sc_send_wr.opcode = IB_WR_SEND_WITH_INV;
+ 	}
+ 	dprintk("svcrdma: posting Send WR with %u sge(s)\n",
+ 		ctxt->sc_send_wr.num_sge);
+ 	return svc_rdma_send(rdma, &ctxt->sc_send_wr);
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  }
  
  /* Given the client-provided Write and Reply chunks, the server was not
@@@ -567,38 -715,29 +822,56 @@@
   * Remote Invalidation is skipped for simplicity.
   */
  static int svc_rdma_send_error_msg(struct svcxprt_rdma *rdma,
- 				   __be32 *rdma_resp, struct svc_rqst *rqstp)
+ 				   struct svc_rdma_send_ctxt *ctxt,
+ 				   struct svc_rqst *rqstp)
  {
++<<<<<<< HEAD
 +	struct svc_rdma_op_ctxt *ctxt;
 +	__be32 *p;
 +	int ret;
 +
 +	ctxt = svc_rdma_get_context(rdma);
 +
 +	/* Replace the original transport header with an
 +	 * RDMA_ERROR response. XID etc are preserved.
 +	 */
 +	trace_svcrdma_err_chunk(*rdma_resp);
 +	p = rdma_resp + 3;
++=======
+ 	__be32 *p;
+ 	int ret;
+ 
+ 	p = ctxt->sc_xprt_buf;
+ 	trace_svcrdma_err_chunk(*p);
+ 	p += 3;
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  	*p++ = rdma_error;
  	*p   = err_chunk;
- 
- 	ret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp, 20);
- 	if (ret < 0)
- 		goto err;
+ 	svc_rdma_sync_reply_hdr(rdma, ctxt, RPCRDMA_HDRLEN_ERR);
  
  	svc_rdma_save_io_pages(rqstp, ctxt);
  
++<<<<<<< HEAD
 +	ret = svc_rdma_post_send_wr(rdma, ctxt, 0);
 +	if (ret)
 +		goto err;
 +
 +	return 0;
 +
 +err:
 +	svc_rdma_unmap_dma(ctxt);
 +	svc_rdma_put_context(ctxt, 1);
 +	return ret;
++=======
+ 	ctxt->sc_send_wr.opcode = IB_WR_SEND;
+ 	ret = svc_rdma_send(rdma, &ctxt->sc_send_wr);
+ 	if (ret) {
+ 		svc_rdma_send_ctxt_put(rdma, ctxt);
+ 		return ret;
+ 	}
+ 
+ 	return 0;
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  }
  
  void svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)
@@@ -622,15 -761,13 +895,15 @@@ int svc_rdma_sendto(struct svc_rqst *rq
  	struct svc_xprt *xprt = rqstp->rq_xprt;
  	struct svcxprt_rdma *rdma =
  		container_of(xprt, struct svcxprt_rdma, sc_xprt);
 -	struct svc_rdma_recv_ctxt *rctxt = rqstp->rq_xprt_ctxt;
  	__be32 *p, *rdma_argp, *rdma_resp, *wr_lst, *rp_ch;
  	struct xdr_buf *xdr = &rqstp->rq_res;
- 	struct page *res_page;
+ 	struct svc_rdma_send_ctxt *sctxt;
  	int ret;
  
 -	rdma_argp = rctxt->rc_recv_buf;
 +	/* Find the call's chunk lists to decide how to send the reply.
 +	 * Receive places the Call's xprt header at the start of page 0.
 +	 */
 +	rdma_argp = page_address(rqstp->rq_pages[0]);
  	svc_rdma_get_write_arrays(rdma_argp, &wr_lst, &rp_ch);
  
  	/* Create the RDMA response header. xprt->xpt_mutex,
@@@ -670,23 -807,30 +943,40 @@@
  		svc_rdma_xdr_encode_reply_chunk(rdma_resp, rp_ch, ret);
  	}
  
- 	ret = svc_rdma_send_reply_msg(rdma, rdma_argp, rdma_resp, rqstp,
+ 	svc_rdma_sync_reply_hdr(rdma, sctxt, svc_rdma_reply_hdr_len(rdma_resp));
+ 	ret = svc_rdma_send_reply_msg(rdma, sctxt, rdma_argp, rqstp,
  				      wr_lst, rp_ch);
  	if (ret < 0)
++<<<<<<< HEAD
 +		goto err0;
 +	return 0;
++=======
+ 		goto err1;
+ 	ret = 0;
+ 
+ out:
+ 	rqstp->rq_xprt_ctxt = NULL;
+ 	svc_rdma_recv_ctxt_put(rdma, rctxt);
+ 	return ret;
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  
   err2:
  	if (ret != -E2BIG && ret != -EINVAL)
  		goto err1;
  
- 	ret = svc_rdma_send_error_msg(rdma, rdma_resp, rqstp);
+ 	ret = svc_rdma_send_error_msg(rdma, sctxt, rqstp);
  	if (ret < 0)
++<<<<<<< HEAD
 +		goto err0;
 +	return 0;
++=======
+ 		goto err1;
+ 	ret = 0;
+ 	goto out;
++>>>>>>> 99722fe4d5a6 (svcrdma: Persistently allocate and DMA-map Send buffers)
  
   err1:
- 	put_page(res_page);
+ 	svc_rdma_send_ctxt_put(rdma, sctxt);
   err0:
  	trace_svcrdma_send_failed(rqstp, ret);
  	set_bit(XPT_CLOSE, &xprt->xpt_flags);
* Unmerged path include/linux/sunrpc/svc_rdma.h
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_backchannel.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_recvfrom.c
* Unmerged path net/sunrpc/xprtrdma/svc_rdma_sendto.c
