pagemap: rework hugetlb and thp report

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
commit 356515e7b64c2629f686109d426baaf868cdf7e8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/356515e7.failed

This patch moves pmd dissection out of reporting loop: huge pages are
reported as bunch of normal pages with contiguous PFNs.

Add missing "FILE" bit in hugetlb vmas.

	Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Reviewed-by: Mark Williamson <mwilliamson@undo-software.com>
	Tested-by:  Mark Williamson <mwilliamson@undo-software.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 356515e7b64c2629f686109d426baaf868cdf7e8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
diff --cc fs/proc/task_mmu.c
index bf81d9f4a02e,98ba9ea96b19..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -1114,60 -1034,44 +1114,88 @@@ static void pte_to_pagemap_entry(pagema
  
  	if (page && !PageAnon(page))
  		flags |= PM_FILE;
 -	if (vma->vm_flags & VM_SOFTDIRTY)
 -		flags |= PM_SOFT_DIRTY;
 +	if ((vma->vm_flags & VM_SOFTDIRTY))
 +		flags2 |= __PM_SOFT_DIRTY;
  
 -	return make_pme(frame, flags);
 +	*pme = make_pme(PM_PFRAME(frame) | PM_STATUS2(pm->v2, flags2) | flags);
  }
  
++<<<<<<< HEAD
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,
 +		pmd_t pmd, int offset, int pmd_flags2)
 +{
 +	/*
 +	 * Currently pmd for thp is always present because thp can not be
 +	 * swapped-out, migrated, or HWPOISONed (split in such cases instead.)
 +	 * This if-check is just to prepare for future implementation.
 +	 */
 +	if (pmd_present(pmd))
 +		*pme = make_pme(PM_PFRAME(pmd_pfn(pmd) + offset)
 +				| PM_STATUS2(pm->v2, pmd_flags2) | PM_PRESENT);
 +	else
 +		*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, pmd_flags2));
 +}
 +#else
 +static inline void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,
 +		pmd_t pmd, int offset, int pmd_flags2)
 +{
 +}
 +#endif
 +
 +static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
++=======
+ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
  			     struct mm_walk *walk)
  {
 -	struct vm_area_struct *vma = walk->vma;
 +	struct vm_area_struct *vma;
  	struct pagemapread *pm = walk->private;
  	spinlock_t *ptl;
 -	pte_t *pte, *orig_pte;
 +	pte_t *pte;
  	int err = 0;
  
++<<<<<<< HEAD
 +	/* find the first VMA at or above 'addr' */
 +	vma = find_vma(walk->mm, addr);
 +	if (vma && pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 +		int pmd_flags2;
 +
 +		if ((vma->vm_flags & VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))
 +			pmd_flags2 = __PM_SOFT_DIRTY;
 +		else
 +			pmd_flags2 = 0;
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	if (pmd_trans_huge_lock(pmdp, vma, &ptl) == 1) {
+ 		u64 flags = 0, frame = 0;
+ 		pmd_t pmd = *pmdp;
+ 
+ 		if ((vma->vm_flags & VM_SOFTDIRTY) || pmd_soft_dirty(pmd))
+ 			flags |= PM_SOFT_DIRTY;
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
+ 
+ 		/*
+ 		 * Currently pmd for thp is always present because thp
+ 		 * can not be swapped-out, migrated, or HWPOISONed
+ 		 * (split in such cases instead.)
+ 		 * This if-check is just to prepare for future implementation.
+ 		 */
+ 		if (pmd_present(pmd)) {
+ 			flags |= PM_PRESENT;
+ 			frame = pmd_pfn(pmd) +
+ 				((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 		}
  
  		for (; addr != end; addr += PAGE_SIZE) {
- 			unsigned long offset;
- 			pagemap_entry_t pme;
+ 			pagemap_entry_t pme = make_pme(frame, flags);
  
++<<<<<<< HEAD
 +			offset = (addr & ~PAGEMAP_WALK_MASK) >>
 +					PAGE_SHIFT;
 +			thp_pmd_to_pagemap_entry(&pme, pm, *pmd, offset, pmd_flags2);
++=======
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
  			err = add_to_pagemap(addr, &pme, pm);
  			if (err)
  				break;
@@@ -1176,54 -1082,24 +1206,65 @@@
  		return err;
  	}
  
- 	if (pmd_trans_unstable(pmd))
+ 	if (pmd_trans_unstable(pmdp))
  		return 0;
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
++<<<<<<< HEAD
 +	while (1) {
 +		/* End of address space hole, which we mark as non-present. */
 +		unsigned long hole_end;
++=======
+ 	/*
+ 	 * We can assume that @vma always points to a valid one and @end never
+ 	 * goes beyond vma->vm_end.
+ 	 */
+ 	orig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);
+ 	for (; addr < end; pte++, addr += PAGE_SIZE) {
+ 		pagemap_entry_t pme;
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
  
 -		pme = pte_to_pagemap_entry(pm, vma, addr, *pte);
 -		err = add_to_pagemap(addr, &pme, pm);
 -		if (err)
 +		if (vma)
 +			hole_end = min(end, vma->vm_start);
 +		else
 +			hole_end = end;
 +
 +		for (; addr < hole_end; addr += PAGE_SIZE) {
 +			pagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm->v2));
 +
 +			err = add_to_pagemap(addr, &pme, pm);
 +			if (err)
 +				return err;
 +		}
 +
 +		if (!vma || vma->vm_start >= end)
  			break;
 +		/*
 +		 * We can't possibly be in a hugetlb VMA. In general,
 +		 * for a mm_walk with a pmd_entry and a hugetlb_entry,
 +		 * the pmd_entry can only be called on addresses in a
 +		 * hugetlb if the walk starts in a non-hugetlb VMA and
 +		 * spans a hugepage VMA. Since pagemap_read walks are
 +		 * PMD-sized and PMD-aligned, this will never be true.
 +		 */
 +		BUG_ON(is_vm_hugetlb_page(vma));
 +
 +		/* Addresses in the VMA. */
 +		for (; addr < min(end, vma->vm_end); addr += PAGE_SIZE) {
 +			pagemap_entry_t pme;
 +			pte = pte_offset_map(pmd, addr);
 +			pte_to_pagemap_entry(&pme, pm, vma, addr, *pte);
 +			pte_unmap(pte);
 +			err = add_to_pagemap(addr, &pme, pm);
 +			if (err)
 +				return err;
 +		}
 +
 +		if (addr == end)
 +			break;
 +
 +		vma = find_vma(walk->mm, addr);
  	}
 -	pte_unmap_unlock(orig_pte, ptl);
  
  	cond_resched();
  
@@@ -1231,40 -1107,35 +1272,67 @@@
  }
  
  #ifdef CONFIG_HUGETLB_PAGE
++<<<<<<< HEAD
 +static void huge_pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,
 +					pte_t pte, int offset, int flags2)
 +{
 +	if (pte_present(pte))
 +		*pme = make_pme(PM_PFRAME(pte_pfn(pte) + offset)	|
 +				PM_STATUS2(pm->v2, flags2)		|
 +				PM_PRESENT);
 +	else
 +		*pme = make_pme(PM_NOT_PRESENT(pm->v2)			|
 +				PM_STATUS2(pm->v2, flags2));
 +}
 +
++=======
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
  /* This function walks within one hugetlb entry in the single call */
- static int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,
+ static int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,
  				 unsigned long addr, unsigned long end,
  				 struct mm_walk *walk)
  {
  	struct pagemapread *pm = walk->private;
++<<<<<<< HEAD
 +	struct vm_area_struct *vma;
 +	int err = 0;
 +	int flags2;
 +	pagemap_entry_t pme;
++=======
+ 	struct vm_area_struct *vma = walk->vma;
+ 	u64 flags = 0, frame = 0;
+ 	int err = 0;
+ 	pte_t pte;
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
 +
 +	vma = find_vma(walk->mm, addr);
 +	WARN_ON_ONCE(!vma);
  
 -	if (vma->vm_flags & VM_SOFTDIRTY)
 -		flags |= PM_SOFT_DIRTY;
 +	if (vma && (vma->vm_flags & VM_SOFTDIRTY))
 +		flags2 = __PM_SOFT_DIRTY;
 +	else
 +		flags2 = 0;
  
+ 	pte = huge_ptep_get(ptep);
+ 	if (pte_present(pte)) {
+ 		struct page *page = pte_page(pte);
+ 
+ 		if (!PageAnon(page))
+ 			flags |= PM_FILE;
+ 
+ 		flags |= PM_PRESENT;
+ 		frame = pte_pfn(pte) +
+ 			((addr & ~hmask) >> PAGE_SHIFT);
+ 	}
+ 
  	for (; addr != end; addr += PAGE_SIZE) {
++<<<<<<< HEAD
 +		int offset = (addr & ~hmask) >> PAGE_SHIFT;
 +		huge_pte_to_pagemap_entry(&pme, pm, *pte, offset, flags2);
++=======
+ 		pagemap_entry_t pme = make_pme(frame, flags);
+ 
++>>>>>>> 356515e7b64c (pagemap: rework hugetlb and thp report)
  		err = add_to_pagemap(addr, &pme, pm);
  		if (err)
  			return err;
* Unmerged path fs/proc/task_mmu.c
