RDMA/umem: Use umem->owning_mm inside ODP

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jason Gunthorpe <jgg@mellanox.com>
commit f27a0d50a4bc2861b472c2e3740d63a29d1ac460
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/f27a0d50.failed

Since ODP had a single struct mmu_notifier located in the ucontext it
could only handle a single MM at a time, and this prevented it from using
the new owning_mm system.

With the prior rework it is now simple to let ODP track multiple MMs per
ucontext, finish the job so that the per_mm is allocated on a mm by mm
basis, and freed when the last umem is dropped from the ucontext.

As a side effect the new saner locking removes the lockdep splat about
nesting the umem_rwsem between mmu_notifier_unregister and
ib_umem_odp_release.

It also makes ODP work with multiple processes, across, fork, etc.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit f27a0d50a4bc2861b472c2e3740d63a29d1ac460)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_odp.c
#	drivers/infiniband/core/uverbs_cmd.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/odp.c
#	include/rdma/ib_umem_odp.h
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/umem_odp.c
index 505862826f1c,0577f9ff600f..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -257,30 -278,151 +257,170 @@@ static const struct mmu_notifier_ops ib
  	.invalidate_range_end       = ib_umem_notifier_invalidate_range_end,
  };
  
- struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
+ static void add_umem_to_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_insert(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 
+ 	if (likely(!atomic_read(&per_mm->notifier_count)))
+ 		umem_odp->mn_counters_active = true;
+ 	else
+ 		list_add(&umem_odp->no_private_counters,
+ 			 &per_mm->no_private_counters);
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static void remove_umem_from_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 
+ 	down_write(&per_mm->umem_rwsem);
+ 	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
+ 		rbt_ib_umem_remove(&umem_odp->interval_tree,
+ 				   &per_mm->umem_tree);
+ 	if (!umem_odp->mn_counters_active) {
+ 		list_del(&umem_odp->no_private_counters);
+ 		complete_all(&umem_odp->notifier_completion);
+ 	}
+ 
+ 	up_write(&per_mm->umem_rwsem);
+ }
+ 
+ static struct ib_ucontext_per_mm *alloc_per_mm(struct ib_ucontext *ctx,
+ 					       struct mm_struct *mm)
+ {
++<<<<<<< HEAD
++=======
+ 	struct ib_ucontext_per_mm *per_mm;
+ 	int ret;
+ 
+ 	per_mm = kzalloc(sizeof(*per_mm), GFP_KERNEL);
+ 	if (!per_mm)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	per_mm->context = ctx;
+ 	per_mm->mm = mm;
+ 	per_mm->umem_tree = RB_ROOT_CACHED;
+ 	init_rwsem(&per_mm->umem_rwsem);
+ 	INIT_LIST_HEAD(&per_mm->no_private_counters);
+ 
+ 	rcu_read_lock();
+ 	per_mm->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
+ 	rcu_read_unlock();
+ 
+ 	WARN_ON(mm != current->mm);
+ 
+ 	per_mm->mn.ops = &ib_umem_notifiers;
+ 	ret = mmu_notifier_register(&per_mm->mn, per_mm->mm);
+ 	if (ret) {
+ 		dev_err(&ctx->device->dev,
+ 			"Failed to register mmu_notifier %d\n", ret);
+ 		goto out_pid;
+ 	}
+ 
+ 	list_add(&per_mm->ucontext_list, &ctx->per_mm_list);
+ 	return per_mm;
+ 
+ out_pid:
+ 	put_pid(per_mm->tgid);
+ 	kfree(per_mm);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int get_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	struct ib_ucontext_per_mm *per_mm;
+ 
+ 	/*
+ 	 * Generally speaking we expect only one or two per_mm in this list,
+ 	 * so no reason to optimize this search today.
+ 	 */
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	list_for_each_entry(per_mm, &ctx->per_mm_list, ucontext_list) {
+ 		if (per_mm->mm == umem_odp->umem.owning_mm)
+ 			goto found;
+ 	}
+ 
+ 	per_mm = alloc_per_mm(ctx, umem_odp->umem.owning_mm);
+ 	if (IS_ERR(per_mm)) {
+ 		mutex_unlock(&ctx->per_mm_list_lock);
+ 		return PTR_ERR(per_mm);
+ 	}
+ 
+ found:
+ 	umem_odp->per_mm = per_mm;
+ 	per_mm->odp_mrs_count++;
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	return 0;
+ }
+ 
+ void put_per_mm(struct ib_umem_odp *umem_odp)
+ {
+ 	struct ib_ucontext_per_mm *per_mm = umem_odp->per_mm;
+ 	struct ib_ucontext *ctx = umem_odp->umem.context;
+ 	bool need_free;
+ 
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	umem_odp->per_mm = NULL;
+ 	per_mm->odp_mrs_count--;
+ 	need_free = per_mm->odp_mrs_count == 0;
+ 	if (need_free)
+ 		list_del(&per_mm->ucontext_list);
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 
+ 	if (!need_free)
+ 		return;
+ 
+ 	mmu_notifier_unregister(&per_mm->mn, per_mm->mm);
+ 	put_pid(per_mm->tgid);
+ 	kfree(per_mm);
+ }
+ 
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext_per_mm *per_mm,
  				      unsigned long addr, size_t size)
  {
+ 	struct ib_ucontext *ctx = per_mm->context;
+ 	struct ib_umem_odp *odp_data;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	struct ib_umem *umem;
 +	struct ib_umem_odp *odp_data;
  	int pages = size >> PAGE_SHIFT;
  	int ret;
  
 -	odp_data = kzalloc(sizeof(*odp_data), GFP_KERNEL);
 -	if (!odp_data)
 +	umem = kzalloc(sizeof(*umem), GFP_KERNEL);
 +	if (!umem)
  		return ERR_PTR(-ENOMEM);
++<<<<<<< HEAD
 +
 +	umem->context    = context;
++=======
+ 	umem = &odp_data->umem;
+ 	umem->context    = ctx;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	umem->length     = size;
  	umem->address    = addr;
  	umem->page_shift = PAGE_SHIFT;
  	umem->writable   = 1;
++<<<<<<< HEAD
 +
 +	odp_data = kzalloc(sizeof(*odp_data), GFP_KERNEL);
 +	if (!odp_data) {
 +		ret = -ENOMEM;
 +		goto out_umem;
 +	}
 +	odp_data->umem = umem;
++=======
+ 	umem->is_odp = 1;
+ 	odp_data->per_mm = per_mm;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  
  	mutex_init(&odp_data->umem_mutex);
  	init_completion(&odp_data->notifier_completion);
@@@ -297,17 -441,14 +437,28 @@@
  		goto out_page_list;
  	}
  
++<<<<<<< HEAD
 +	down_write(&context->umem_rwsem);
 +	context->odp_mrs_count++;
 +	rbt_ib_umem_insert(&odp_data->interval_tree, &context->umem_tree);
 +	if (likely(!atomic_read(&context->notifier_count)))
 +		odp_data->mn_counters_active = true;
 +	else
 +		list_add(&odp_data->no_private_counters,
 +			 &context->no_private_counters);
 +	up_write(&context->umem_rwsem);
 +
 +	umem->odp_data = odp_data;
++=======
+ 	/*
+ 	 * Caller must ensure that the umem_odp that the per_mm came from
+ 	 * cannot be freed during the call to ib_alloc_odp_umem.
+ 	 */
+ 	mutex_lock(&ctx->per_mm_list_lock);
+ 	per_mm->odp_mrs_count++;
+ 	mutex_unlock(&ctx->per_mm_list_lock);
+ 	add_umem_to_per_mm(odp_data);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  
  	return odp_data;
  
@@@ -321,15 -460,15 +472,19 @@@ out_umem
  }
  EXPORT_SYMBOL(ib_alloc_odp_umem);
  
 -int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access)
 +int ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem,
 +		    int access)
  {
++<<<<<<< HEAD
++=======
+ 	struct ib_umem *umem = &umem_odp->umem;
+ 	/*
+ 	 * NOTE: This must called in a process context where umem->owning_mm
+ 	 * == current->mm
+ 	 */
+ 	struct mm_struct *mm = umem->owning_mm;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	int ret_val;
- 	struct pid *our_pid;
- 	struct mm_struct *mm = get_task_mm(current);
- 
- 	if (!mm)
- 		return -EINVAL;
  
  	if (access & IB_ACCESS_HUGETLB) {
  		struct vm_area_struct *vma;
@@@ -349,104 -488,43 +504,134 @@@
  		umem->hugetlb = 0;
  	}
  
++<<<<<<< HEAD
 +	/* Prevent creating ODP MRs in child processes */
 +	rcu_read_lock();
 +	our_pid = get_task_pid(current->group_leader, PIDTYPE_PID);
 +	rcu_read_unlock();
 +	put_pid(our_pid);
 +	if (context->tgid != our_pid) {
 +		ret_val = -EINVAL;
 +		goto out_mm;
 +	}
 +
 +	umem->odp_data = kzalloc(sizeof(*umem->odp_data), GFP_KERNEL);
 +	if (!umem->odp_data) {
 +		ret_val = -ENOMEM;
 +		goto out_mm;
 +	}
 +	umem->odp_data->umem = umem;
++=======
+ 	mutex_init(&umem_odp->umem_mutex);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
 +
 +	mutex_init(&umem->odp_data->umem_mutex);
  
 -	init_completion(&umem_odp->notifier_completion);
 +	init_completion(&umem->odp_data->notifier_completion);
  
  	if (ib_umem_num_pages(umem)) {
++<<<<<<< HEAD
 +		umem->odp_data->page_list = vzalloc(ib_umem_num_pages(umem) *
 +					    sizeof(*umem->odp_data->page_list));
 +		if (!umem->odp_data->page_list) {
 +			ret_val = -ENOMEM;
 +			goto out_odp_data;
 +		}
++=======
+ 		umem_odp->page_list =
+ 			vzalloc(array_size(sizeof(*umem_odp->page_list),
+ 					   ib_umem_num_pages(umem)));
+ 		if (!umem_odp->page_list)
+ 			return -ENOMEM;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  
 -		umem_odp->dma_list =
 -			vzalloc(array_size(sizeof(*umem_odp->dma_list),
 -					   ib_umem_num_pages(umem)));
 -		if (!umem_odp->dma_list) {
 +		umem->odp_data->dma_list = vzalloc(ib_umem_num_pages(umem) *
 +					  sizeof(*umem->odp_data->dma_list));
 +		if (!umem->odp_data->dma_list) {
  			ret_val = -ENOMEM;
  			goto out_page_list;
  		}
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * When using MMU notifiers, we will get a
 +	 * notification before the "current" task (and MM) is
 +	 * destroyed. We use the umem_rwsem semaphore to synchronize.
 +	 */
 +	down_write(&context->umem_rwsem);
 +	context->odp_mrs_count++;
 +	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
 +		rbt_ib_umem_insert(&umem->odp_data->interval_tree,
 +				   &context->umem_tree);
 +	if (likely(!atomic_read(&context->notifier_count)) ||
 +	    context->odp_mrs_count == 1)
 +		umem->odp_data->mn_counters_active = true;
 +	else
 +		list_add(&umem->odp_data->no_private_counters,
 +			 &context->no_private_counters);
 +	downgrade_write(&context->umem_rwsem);
 +
 +	if (context->odp_mrs_count == 1) {
 +		/*
 +		 * Note that at this point, no MMU notifier is running
 +		 * for this context!
 +		 */
 +		atomic_set(&context->notifier_count, 0);
 +		INIT_HLIST_NODE(&context->mn.hlist);
 +		context->mn.ops = &ib_umem_notifiers;
 +		ret_val = mmu_notifier_register(&context->mn, mm);
 +		if (ret_val) {
 +			pr_err("Failed to register mmu_notifier %d\n", ret_val);
 +			ret_val = -EBUSY;
 +			goto out_mutex;
 +		}
 +	}
 +
 +	up_read(&context->umem_rwsem);
 +
 +	/*
 +	 * Note that doing an mmput can cause a notifier for the relevant mm.
 +	 * If the notifier is called while we hold the umem_rwsem, this will
 +	 * cause a deadlock. Therefore, we release the reference only after we
 +	 * released the semaphore.
 +	 */
 +	mmput(mm);
 +	return 0;
 +
 +out_mutex:
 +	up_read(&context->umem_rwsem);
 +	vfree(umem->odp_data->dma_list);
 +out_page_list:
 +	vfree(umem->odp_data->page_list);
 +out_odp_data:
 +	kfree(umem->odp_data);
 +out_mm:
 +	mmput(mm);
++=======
+ 	ret_val = get_per_mm(umem_odp);
+ 	if (ret_val)
+ 		goto out_dma_list;
+ 	add_umem_to_per_mm(umem_odp);
+ 
+ 	return 0;
+ 
+ out_dma_list:
+ 	vfree(umem_odp->dma_list);
+ out_page_list:
+ 	vfree(umem_odp->page_list);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	return ret_val;
  }
  
  void ib_umem_odp_release(struct ib_umem_odp *umem_odp)
  {
++<<<<<<< HEAD
 +	struct ib_umem *umem = umem_odp->umem;
 +	struct ib_ucontext *context = umem->context;
++=======
+ 	struct ib_umem *umem = &umem_odp->umem;
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  
  	/*
  	 * Ensure that no more pages are mapped in the umem.
@@@ -457,58 -535,10 +642,63 @@@
  	ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem),
  				    ib_umem_end(umem));
  
++<<<<<<< HEAD
 +	down_write(&context->umem_rwsem);
 +	if (likely(ib_umem_start(umem) != ib_umem_end(umem)))
 +		rbt_ib_umem_remove(&umem_odp->interval_tree,
 +				   &context->umem_tree);
 +	context->odp_mrs_count--;
 +	if (!umem_odp->mn_counters_active) {
 +		list_del(&umem_odp->no_private_counters);
 +		complete_all(&umem_odp->notifier_completion);
 +	}
 +
 +	/*
 +	 * Downgrade the lock to a read lock. This ensures that the notifiers
 +	 * (who lock the mutex for reading) will be able to finish, and we
 +	 * will be able to enventually obtain the mmu notifiers SRCU. Note
 +	 * that since we are doing it atomically, no other user could register
 +	 * and unregister while we do the check.
 +	 */
 +	downgrade_write(&context->umem_rwsem);
 +	if (!context->odp_mrs_count) {
 +		struct task_struct *owning_process = NULL;
 +		struct mm_struct *owning_mm        = NULL;
 +
 +		owning_process = get_pid_task(context->tgid,
 +					      PIDTYPE_PID);
 +		if (owning_process == NULL)
 +			/*
 +			 * The process is already dead, notifier were removed
 +			 * already.
 +			 */
 +			goto out;
 +
 +		owning_mm = get_task_mm(owning_process);
 +		if (owning_mm == NULL)
 +			/*
 +			 * The process' mm is already dead, notifier were
 +			 * removed already.
 +			 */
 +			goto out_put_task;
 +		mmu_notifier_unregister(&context->mn, owning_mm);
 +
 +		mmput(owning_mm);
 +
 +out_put_task:
 +		put_task_struct(owning_process);
 +	}
 +out:
 +	up_read(&context->umem_rwsem);
 +
++=======
+ 	remove_umem_from_per_mm(umem_odp);
+ 	put_per_mm(umem_odp);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	vfree(umem_odp->dma_list);
  	vfree(umem_odp->page_list);
 +	kfree(umem_odp);
 +	kfree(umem);
  }
  
  /*
@@@ -622,9 -652,9 +812,9 @@@ int ib_umem_odp_map_dma_pages(struct ib
  			      u64 bcnt, u64 access_mask,
  			      unsigned long current_seq)
  {
 -	struct ib_umem *umem = &umem_odp->umem;
 +	struct ib_umem *umem = umem_odp->umem;
  	struct task_struct *owning_process  = NULL;
- 	struct mm_struct   *owning_mm       = NULL;
+ 	struct mm_struct *owning_mm = umem_odp->umem.owning_mm;
  	struct page       **local_page_list = NULL;
  	u64 page_mask, off;
  	int j, k, ret = 0, start_idx, npages = 0, page_shift;
diff --cc drivers/infiniband/core/uverbs_cmd.c
index 507521d53e3f,d77b0b9793c7..000000000000
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@@ -109,14 -120,12 +109,19 @@@ ssize_t ib_uverbs_get_context(struct ib
  	rcu_read_lock();
  	ucontext->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
  	rcu_read_unlock();
 -	ucontext->closing = false;
 -	ucontext->cleanup_retryable = false;
 +	ucontext->closing = 0;
  
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
++<<<<<<< HEAD
 +	ucontext->umem_tree = RB_ROOT;
 +	init_rwsem(&ucontext->umem_rwsem);
 +	ucontext->odp_mrs_count = 0;
 +	INIT_LIST_HEAD(&ucontext->no_private_counters);
 +
++=======
+ 	mutex_init(&ucontext->per_mm_list_lock);
+ 	INIT_LIST_HEAD(&ucontext->per_mm_list);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	if (!(ib_dev->attrs.device_cap_flags & IB_DEVICE_ON_DEMAND_PAGING))
  		ucontext->invalidate_range = NULL;
  
diff --cc drivers/infiniband/hw/mlx5/main.c
index 7e6f22b16875,1348a08261a9..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1787,11 -1861,19 +1787,24 @@@ static int mlx5_ib_dealloc_ucontext(str
  	struct mlx5_ib_dev *dev = to_mdev(ibcontext->device);
  	struct mlx5_bfreg_info *bfregi;
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	/* All umem's must be destroyed before destroying the ucontext. */
+ 	mutex_lock(&ibcontext->per_mm_list_lock);
+ 	WARN_ON(!list_empty(&ibcontext->per_mm_list));
+ 	mutex_unlock(&ibcontext->per_mm_list_lock);
+ #endif
+ 
+ 	if (context->devx_uid)
+ 		mlx5_ib_devx_destroy(dev, context);
+ 
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  	bfregi = &context->bfregi;
 -	mlx5_ib_dealloc_transport_domain(dev, context->tdn);
 +	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
 +		mlx5_ib_dealloc_transport_domain(dev, context->tdn);
  
 +	free_page(context->upd_xlt_page);
  	deallocate_uars(dev, context);
  	kfree(bfregi->sys_pages);
  	kfree(bfregi->count);
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 45dd3865a782,b04eb6775326..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -386,9 -393,10 +386,14 @@@ next_mr
  		if (nentries)
  			nentries++;
  	} else {
++<<<<<<< HEAD
 +		odp = ib_alloc_odp_umem(ctx, addr, MLX5_IMR_MTT_SIZE);
++=======
+ 		odp = ib_alloc_odp_umem(odp_mr->per_mm, addr,
+ 					MLX5_IMR_MTT_SIZE);
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  		if (IS_ERR(odp)) {
 -			mutex_unlock(&odp_mr->umem_mutex);
 +			mutex_unlock(&mr->umem->odp_data->umem_mutex);
  			return ERR_CAST(odp);
  		}
  
diff --cc include/rdma/ib_umem_odp.h
index cbe16c505673,259eb08dfc9e..000000000000
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@@ -89,9 -91,26 +89,32 @@@ static inline struct ib_umem_odp *to_ib
  
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
  
++<<<<<<< HEAD
 +int ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem,
 +		    int access);
 +struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext *context,
++=======
+ struct ib_ucontext_per_mm {
+ 	struct ib_ucontext *context;
+ 	struct mm_struct *mm;
+ 	struct pid *tgid;
+ 
+ 	struct rb_root_cached umem_tree;
+ 	/* Protects umem_tree */
+ 	struct rw_semaphore umem_rwsem;
+ 	atomic_t notifier_count;
+ 
+ 	struct mmu_notifier mn;
+ 	/* A list of umems that don't have private mmu notifier counters yet. */
+ 	struct list_head no_private_counters;
+ 	unsigned int odp_mrs_count;
+ 
+ 	struct list_head ucontext_list;
+ };
+ 
+ int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access);
+ struct ib_umem_odp *ib_alloc_odp_umem(struct ib_ucontext_per_mm *per_mm,
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  				      unsigned long addr, size_t size);
  void ib_umem_odp_release(struct ib_umem_odp *umem_odp);
  
diff --cc include/rdma/ib_verbs.h
index b950ea239071,6437e6af758d..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -1460,10 -1478,16 +1460,19 @@@ enum rdma_remove_reason 
  	RDMA_REMOVE_CLOSE,
  	/* Driver is being hot-unplugged. This call should delete the actual object itself */
  	RDMA_REMOVE_DRIVER_REMOVE,
 -	/* uobj is being cleaned-up before being committed */
 -	RDMA_REMOVE_ABORT,
 +	/* Context is being cleaned-up, but commit was just completed */
 +	RDMA_REMOVE_DURING_CLEANUP,
  };
  
++<<<<<<< HEAD
++=======
+ struct ib_rdmacg_object {
+ #ifdef CONFIG_CGROUP_RDMA
+ 	struct rdma_cgroup	*cg;		/* owner rdma cgroup */
+ #endif
+ };
+ 
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  struct ib_ucontext {
  	struct ib_device       *device;
  	struct ib_uverbs_file  *ufile;
@@@ -1478,21 -1502,13 +1487,27 @@@
  
  	struct pid             *tgid;
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	struct rb_root      umem_tree;
 +	/*
 +	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
 +	 * mmu notifiers registration.
 +	 */
 +	struct rw_semaphore	umem_rwsem;
  	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
  				 unsigned long start, unsigned long end);
++<<<<<<< HEAD
++=======
+ 	struct mutex per_mm_list_lock;
+ 	struct list_head per_mm_list;
+ #endif
++>>>>>>> f27a0d50a4bc (RDMA/umem: Use umem->owning_mm inside ODP)
  
 -	struct ib_rdmacg_object	cg_obj;
 +	struct mmu_notifier	mn;
 +	atomic_t		notifier_count;
 +	/* A list of umems that don't have private mmu notifier counters yet. */
 +	struct list_head	no_private_counters;
 +	int                     odp_mrs_count;
 +#endif
  };
  
  struct ib_uobject {
* Unmerged path drivers/infiniband/core/umem_odp.c
* Unmerged path drivers/infiniband/core/uverbs_cmd.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
* Unmerged path include/rdma/ib_umem_odp.h
* Unmerged path include/rdma/ib_verbs.h
