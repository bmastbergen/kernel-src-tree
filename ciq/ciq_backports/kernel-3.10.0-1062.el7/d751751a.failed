iommu/iova: Consolidate code for adding new node to iovad domain rbtree

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [iommu] iova: Consolidate code for adding new node to iovad domain rbtree (Jerry Snitselaar) [1615865]
Rebuild_FUZZ: 95.59%
commit-author Marek Szyprowski <m.szyprowski@samsung.com>
commit d751751a9f7f2f8e406d5a09565d337f009835d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d751751a.failed

This patch consolidates almost the same code used in iova_insert_rbtree()
and __alloc_and_insert_iova_range() functions. While touching this code,
replace BUG() with WARN_ON(1) to avoid taking down the whole system in
case of corrupted iova tree or incorrect calls.

	Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit d751751a9f7f2f8e406d5a09565d337f009835d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
diff --cc drivers/iommu/iova.c
index da12302bcc16,e80a4105ac2a..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -204,39 -182,11 +232,44 @@@ move_left
  	}
  
  	/* pfn_lo will point to size aligned address if size_aligned is set */
 -	new->pfn_lo = limit_pfn - (size + pad_size) + 1;
 +	new->pfn_lo = limit_pfn - (size + pad_size);
  	new->pfn_hi = new->pfn_lo + size - 1;
  
++<<<<<<< HEAD
 +	/* Insert the new_iova into domain rbtree by holding writer lock */
 +	/* Add new node and rebalance tree. */
 +	{
 +		struct rb_node **entry, *parent = NULL;
 +
 +		/* If we have 'prev', it's a valid place to start the
 +		   insertion. Otherwise, start from the root. */
 +		if (prev)
 +			entry = &prev;
 +		else
 +			entry = &iovad->rbroot.rb_node;
 +
 +		/* Figure out where to put new node */
 +		while (*entry) {
 +			struct iova *this = container_of(*entry,
 +							struct iova, node);
 +			parent = *entry;
 +
 +			if (new->pfn_lo < this->pfn_lo)
 +				entry = &((*entry)->rb_left);
 +			else if (new->pfn_lo > this->pfn_lo)
 +				entry = &((*entry)->rb_right);
 +			else
 +				BUG(); /* this should not happen */
 +		}
 +
 +		/* Add new node and rebalance tree. */
 +		rb_link_node(&new->node, parent, entry);
 +		rb_insert_color(&new->node, &iovad->rbroot);
 +	}
++=======
+ 	/* If we have 'prev', it's a valid place to start the insertion. */
+ 	iova_insert_rbtree(&iovad->rbroot, new, prev);
++>>>>>>> d751751a9f7f (iommu/iova: Consolidate code for adding new node to iovad domain rbtree)
  	__cached_rbnode_insert_update(iovad, saved_pfn, new);
  
  	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
@@@ -245,28 -195,6 +278,31 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void
 +iova_insert_rbtree(struct rb_root *root, struct iova *iova)
 +{
 +	struct rb_node **new = &(root->rb_node), *parent = NULL;
 +	/* Figure out where to put new node */
 +	while (*new) {
 +		struct iova *this = container_of(*new, struct iova, node);
 +
 +		parent = *new;
 +
 +		if (iova->pfn_lo < this->pfn_lo)
 +			new = &((*new)->rb_left);
 +		else if (iova->pfn_lo > this->pfn_lo)
 +			new = &((*new)->rb_right);
 +		else
 +			BUG(); /* this should not happen */
 +	}
 +	/* Add new node and rebalance tree. */
 +	rb_link_node(&iova->node, parent, new);
 +	rb_insert_color(&iova->node, root);
 +}
 +
++=======
++>>>>>>> d751751a9f7f (iommu/iova: Consolidate code for adding new node to iovad domain rbtree)
  static struct kmem_cache *iova_cache;
  static unsigned int iova_cache_users;
  static DEFINE_MUTEX(iova_cache_mutex);
* Unmerged path drivers/iommu/iova.c
