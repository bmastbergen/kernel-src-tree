locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 76695af20c015206cffb84b15912be6797d0cca2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/76695af2.failed

Replace ACCESS_ONCE() macro in smp_store_release() and smp_load_acquire()
with WRITE_ONCE() and READ_ONCE() on x86, arm, arm64, ia64, metag, mips,
powerpc, s390, sparc and asm-generic since ACCESS_ONCE() does not work
reliably on non-scalar types.

WRITE_ONCE() and READ_ONCE() were introduced in the following commits:

  230fa253df63 ("kernel: Provide READ_ONCE and ASSIGN_ONCE")
  43239cbe79fc ("kernel: Change ASSIGN_ONCE(val, x) to WRITE_ONCE(x, val)")

	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Davidlohr Bueso <dbueso@suse.de>
	Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
	Acked-by: Ralf Baechle <ralf@linux-mips.org>
	Cc: Alexander Duyck <alexander.h.duyck@redhat.com>
	Cc: Andre Przywara <andre.przywara@arm.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Fenghua Yu <fenghua.yu@intel.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: James Hogan <james.hogan@imgtec.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Russell King <linux@arm.linux.org.uk>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: linux-arch@vger.kernel.org
Link: http://lkml.kernel.org/r/1438528264-714-1-git-send-email-andreyknvl@google.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 76695af20c015206cffb84b15912be6797d0cca2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/barrier.h
#	arch/arm64/include/asm/barrier.h
#	arch/ia64/include/asm/barrier.h
#	arch/metag/include/asm/barrier.h
#	arch/mips/include/asm/barrier.h
#	arch/sparc/include/asm/barrier_64.h
diff --cc arch/arm/include/asm/barrier.h
index 9486fe4b7399,70393574e0fa..000000000000
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@@ -63,6 -63,21 +63,24 @@@
  #define smp_wmb()	dmb(ishst)
  #endif
  
++<<<<<<< HEAD
++=======
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	___p1;								\
+ })
+ 
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  #define read_barrier_depends()		do { } while(0)
  #define smp_read_barrier_depends()	do { } while(0)
  
diff --cc arch/arm64/include/asm/barrier.h
index 0fe52fcc096f,ef93b20bc964..000000000000
--- a/arch/arm64/include/asm/barrier.h
+++ b/arch/arm64/include/asm/barrier.h
@@@ -38,14 -39,78 +38,33 @@@
  #define smp_mb()	barrier()
  #define smp_rmb()	barrier()
  #define smp_wmb()	barrier()
++<<<<<<< HEAD
++=======
+ 
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	___p1;								\
+ })
+ 
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  #else
 -
 -#define smp_mb()	dmb(ish)
 -#define smp_rmb()	dmb(ishld)
 -#define smp_wmb()	dmb(ishst)
 -
 -#define smp_store_release(p, v)						\
 -do {									\
 -	compiletime_assert_atomic_type(*p);				\
 -	switch (sizeof(*p)) {						\
 -	case 1:								\
 -		asm volatile ("stlrb %w1, %0"				\
 -				: "=Q" (*p) : "r" (v) : "memory");	\
 -		break;							\
 -	case 2:								\
 -		asm volatile ("stlrh %w1, %0"				\
 -				: "=Q" (*p) : "r" (v) : "memory");	\
 -		break;							\
 -	case 4:								\
 -		asm volatile ("stlr %w1, %0"				\
 -				: "=Q" (*p) : "r" (v) : "memory");	\
 -		break;							\
 -	case 8:								\
 -		asm volatile ("stlr %1, %0"				\
 -				: "=Q" (*p) : "r" (v) : "memory");	\
 -		break;							\
 -	}								\
 -} while (0)
 -
 -#define smp_load_acquire(p)						\
 -({									\
 -	typeof(*p) ___p1;						\
 -	compiletime_assert_atomic_type(*p);				\
 -	switch (sizeof(*p)) {						\
 -	case 1:								\
 -		asm volatile ("ldarb %w0, %1"				\
 -			: "=r" (___p1) : "Q" (*p) : "memory");		\
 -		break;							\
 -	case 2:								\
 -		asm volatile ("ldarh %w0, %1"				\
 -			: "=r" (___p1) : "Q" (*p) : "memory");		\
 -		break;							\
 -	case 4:								\
 -		asm volatile ("ldar %w0, %1"				\
 -			: "=r" (___p1) : "Q" (*p) : "memory");		\
 -		break;							\
 -	case 8:								\
 -		asm volatile ("ldar %0, %1"				\
 -			: "=r" (___p1) : "Q" (*p) : "memory");		\
 -		break;							\
 -	}								\
 -	___p1;								\
 -})
 -
 +#define smp_mb()	asm volatile("dmb ish" : : : "memory")
 +#define smp_rmb()	asm volatile("dmb ishld" : : : "memory")
 +#define smp_wmb()	asm volatile("dmb ishst" : : : "memory")
  #endif
  
 +#define gmb()		do { } while (0)
 +
  #define read_barrier_depends()		do { } while(0)
  #define smp_read_barrier_depends()	do { } while(0)
  
diff --cc arch/ia64/include/asm/barrier.h
index 6aa1cdb884e1,df896a1c41d3..000000000000
--- a/arch/ia64/include/asm/barrier.h
+++ b/arch/ia64/include/asm/barrier.h
@@@ -54,12 -54,30 +54,32 @@@
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
  
 -#define smp_mb__before_atomic()	barrier()
 -#define smp_mb__after_atomic()	barrier()
 -
  /*
 - * IA64 GCC turns volatile stores into st.rel and volatile loads into ld.acq no
 - * need for asm trickery!
 + * XXX check on this ---I suspect what Linus really wants here is
 + * acquire vs release semantics but we can't discuss this stuff with
 + * Linus just yet.  Grrr...
   */
++<<<<<<< HEAD
 +#define set_mb(var, value)	do { (var) = (value); mb(); } while (0)
++=======
+ 
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); mb(); } while (0)
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  
  /*
   * The group barrier in front of the rsm & ssm are necessary to ensure
diff --cc arch/metag/include/asm/barrier.h
index 21ee80547b5a,172b7e5efc53..000000000000
--- a/arch/metag/include/asm/barrier.h
+++ b/arch/metag/include/asm/barrier.h
@@@ -81,6 -84,24 +81,28 @@@ static inline void fence(void
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
  
++<<<<<<< HEAD
 +#define set_mb(var, value) do { var = value; smp_mb(); } while (0)
++=======
+ #define smp_store_mb(var, value) do { WRITE_ONCE(var, value); smp_mb(); } while (0)
+ 
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_mb__before_atomic()	barrier()
+ #define smp_mb__after_atomic()	barrier()
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  
  #endif /* _ASM_METAG_BARRIER_H */
diff --cc arch/mips/include/asm/barrier.h
index 5659ce269858,752e0b86c171..000000000000
--- a/arch/mips/include/asm/barrier.h
+++ b/arch/mips/include/asm/barrier.h
@@@ -129,4 -129,22 +129,25 @@@
  #define nudge_writes() mb()
  #endif
  
++<<<<<<< HEAD
++=======
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	smp_mb();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_mb__before_atomic()	smp_mb__before_llsc()
+ #define smp_mb__after_atomic()	smp_llsc_mb()
+ 
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  #endif /* __ASM_BARRIER_H */
diff --cc arch/sparc/include/asm/barrier_64.h
index 2e7b41eb2e56,14a928601657..000000000000
--- a/arch/sparc/include/asm/barrier_64.h
+++ b/arch/sparc/include/asm/barrier_64.h
@@@ -56,4 -56,22 +56,25 @@@ do {	__asm__ __volatile__("ba,pt	%%xcc
  #define read_barrier_depends()		do { } while (0)
  #define smp_read_barrier_depends()	do { } while (0)
  
++<<<<<<< HEAD
++=======
+ #define smp_store_release(p, v)						\
+ do {									\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	WRITE_ONCE(*p, v);						\
+ } while (0)
+ 
+ #define smp_load_acquire(p)						\
+ ({									\
+ 	typeof(*p) ___p1 = READ_ONCE(*p);				\
+ 	compiletime_assert_atomic_type(*p);				\
+ 	barrier();							\
+ 	___p1;								\
+ })
+ 
+ #define smp_mb__before_atomic()	barrier()
+ #define smp_mb__after_atomic()	barrier()
+ 
++>>>>>>> 76695af20c01 (locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire())
  #endif /* !(__SPARC64_BARRIER_H) */
* Unmerged path arch/arm/include/asm/barrier.h
* Unmerged path arch/arm64/include/asm/barrier.h
* Unmerged path arch/ia64/include/asm/barrier.h
* Unmerged path arch/metag/include/asm/barrier.h
* Unmerged path arch/mips/include/asm/barrier.h
diff --git a/arch/powerpc/include/asm/barrier.h b/arch/powerpc/include/asm/barrier.h
index df6b7335588b..939f491fbcfe 100644
--- a/arch/powerpc/include/asm/barrier.h
+++ b/arch/powerpc/include/asm/barrier.h
@@ -79,12 +79,12 @@
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_lwsync();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	smp_lwsync();							\
 	___p1;								\
diff --git a/arch/s390/include/asm/barrier.h b/arch/s390/include/asm/barrier.h
index 09f1de1aeca7..47e07f25d5b3 100644
--- a/arch/s390/include/asm/barrier.h
+++ b/arch/s390/include/asm/barrier.h
@@ -53,12 +53,12 @@ static inline void gmb(void)
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	barrier();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	barrier();							\
 	___p1;								\
* Unmerged path arch/sparc/include/asm/barrier_64.h
diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h
index 3550d30e3020..c7b2256118ad 100644
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@ -84,12 +84,12 @@ static inline unsigned long array_index_mask_nospec(unsigned long index,
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
 	___p1;								\
@@ -101,12 +101,12 @@ do {									\
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	barrier();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	barrier();							\
 	___p1;								\
diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 85b306c8b318..b3aa3c4aba87 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -113,14 +113,14 @@
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 #endif
 
 #ifndef smp_load_acquire
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
 	___p1;								\
