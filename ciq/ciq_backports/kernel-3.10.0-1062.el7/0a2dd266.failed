mm: make tlb_flush_pending global

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Minchan Kim <minchan@kernel.org>
commit 0a2dd266dd6b7a31503b5bbe63af05961a6b446d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/0a2dd266.failed

Currently, tlb_flush_pending is used only for CONFIG_[NUMA_BALANCING|
COMPACTION] but upcoming patches to solve subtle TLB flush batching
problem will use it regardless of compaction/NUMA so this patch doesn't
remove the dependency.

[akpm@linux-foundation.org: remove more ifdefs from world's ugliest printk statement]
Link: http://lkml.kernel.org/r/20170802000818.4760-6-namit@vmware.com
	Signed-off-by: Minchan Kim <minchan@kernel.org>
	Signed-off-by: Nadav Amit <namit@vmware.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jeff Dike <jdike@addtoit.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Nadav Amit <nadav.amit@gmail.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0a2dd266dd6b7a31503b5bbe63af05961a6b446d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm_types.h
#	mm/debug.c
diff --cc include/linux/mm_types.h
index ff35333d8d6c,892a7b0196fd..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -503,8 -492,7 +502,12 @@@ struct mm_struct 
  	 * can move process memory needs to flush the TLB when moving a
  	 * PROT_NONE or PROT_NUMA mapped page.
  	 */
++<<<<<<< HEAD
 +	bool tlb_flush_pending;
 +#endif
++=======
+ 	atomic_t tlb_flush_pending;
++>>>>>>> 0a2dd266dd6b (mm: make tlb_flush_pending global)
  #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
  	/* See flush_tlb_batched_pending() */
  	bool tlb_flush_batched;
@@@ -568,7 -520,12 +571,16 @@@ static inline cpumask_t *mm_cpumask(str
  	return mm->cpu_vm_mask_var;
  }
  
++<<<<<<< HEAD
 +#if defined(CONFIG_NUMA_BALANCING) || defined(CONFIG_COMPACTION)
++=======
+ struct mmu_gather;
+ extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
+ 				unsigned long start, unsigned long end);
+ extern void tlb_finish_mmu(struct mmu_gather *tlb,
+ 				unsigned long start, unsigned long end);
+ 
++>>>>>>> 0a2dd266dd6b (mm: make tlb_flush_pending global)
  /*
   * Memory barriers to keep this state in sync are graciously provided by
   * the page table locks, outside of which no page table modifications happen.
@@@ -590,23 -553,60 +602,26 @@@ static inline void set_tlb_flush_pendin
  	 */
  	smp_mb__before_spinlock();
  }
 -
  /* Clearing is done after a TLB flush, which also provides a barrier. */
 -static inline void dec_tlb_flush_pending(struct mm_struct *mm)
 +static inline void clear_tlb_flush_pending(struct mm_struct *mm)
  {
 -	/*
 -	 * Guarantee that the tlb_flush_pending does not not leak into the
 -	 * critical section, since we must order the PTE change and changes to
 -	 * the pending TLB flush indication. We could have relied on TLB flush
 -	 * as a memory barrier, but this behavior is not clearly documented.
 -	 */
 -	smp_mb__before_atomic();
 -	atomic_dec(&mm->tlb_flush_pending);
 +	barrier();
 +	mm->tlb_flush_pending = false;
  }
 -
 -struct vm_fault;
 -
 -struct vm_special_mapping {
 -	const char *name;	/* The name, e.g. "[vdso]". */
 -
 -	/*
 -	 * If .fault is not provided, this points to a
 -	 * NULL-terminated array of pages that back the special mapping.
 -	 *
 -	 * This must not be NULL unless .fault is provided.
 -	 */
 -	struct page **pages;
 -
 -	/*
 -	 * If non-NULL, then this is called to resolve page faults
 -	 * on the special mapping.  If used, .pages is not checked.
 -	 */
 -	int (*fault)(const struct vm_special_mapping *sm,
 -		     struct vm_area_struct *vma,
 -		     struct vm_fault *vmf);
 -
 -	int (*mremap)(const struct vm_special_mapping *sm,
 -		     struct vm_area_struct *new_vma);
 -};
 -
 -enum tlb_flush_reason {
 -	TLB_FLUSH_ON_TASK_SWITCH,
 -	TLB_REMOTE_SHOOTDOWN,
 -	TLB_LOCAL_SHOOTDOWN,
 -	TLB_LOCAL_MM_SHOOTDOWN,
 -	TLB_REMOTE_SEND_IPI,
 -	NR_TLB_FLUSH_REASONS,
 -};
 -
 - /*
 -  * A swap entry has to fit into a "unsigned long", as the entry is hidden
 -  * in the "index" field of the swapper address space.
 -  */
 -typedef struct {
 -	unsigned long val;
 -} swp_entry_t;
++<<<<<<< HEAD
 +#else
 +static inline bool tlb_flush_pending(struct mm_struct *mm)
 +{
 +	return false;
 +}
 +static inline void set_tlb_flush_pending(struct mm_struct *mm)
 +{
 +}
 +static inline void clear_tlb_flush_pending(struct mm_struct *mm)
 +{
 +}
 +#endif
++=======
++>>>>>>> 0a2dd266dd6b (mm: make tlb_flush_pending global)
  
  #endif /* _LINUX_MM_TYPES_H */
* Unmerged path mm/debug.c
* Unmerged path include/linux/mm_types.h
* Unmerged path mm/debug.c
