x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] kvm/mmu: check if tdp/shadow MMU reconfiguration is needed (Vitaly Kuznetsov) [1565739 1497611]
Rebuild_FUZZ: 96.67%
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 7dcd575520082f186231777f4ac9f59ce14d6961
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/7dcd5755.failed

MMU reconfiguration in init_kvm_tdp_mmu()/kvm_init_shadow_mmu() can be
avoided if the source data used to configure it didn't change; enhance
MMU extended role with the required fields and consolidate common code in
kvm_calc_mmu_role_common().

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 7dcd575520082f186231777f4ac9f59ce14d6961)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/include/asm/kvm_host.h
index 86bec63d5f8a,4b09d4aa9bf4..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -246,6 -282,34 +246,37 @@@ union kvm_mmu_page_role 
  	};
  };
  
++<<<<<<< HEAD
++=======
+ union kvm_mmu_extended_role {
+ /*
+  * This structure complements kvm_mmu_page_role caching everything needed for
+  * MMU configuration. If nothing in both these structures changed, MMU
+  * re-configuration can be skipped. @valid bit is set on first usage so we don't
+  * treat all-zero structure as valid data.
+  */
+ 	u32 word;
+ 	struct {
+ 		unsigned int valid:1;
+ 		unsigned int execonly:1;
+ 		unsigned int cr0_pg:1;
+ 		unsigned int cr4_pse:1;
+ 		unsigned int cr4_pke:1;
+ 		unsigned int cr4_smap:1;
+ 		unsigned int cr4_smep:1;
+ 		unsigned int cr4_la57:1;
+ 	};
+ };
+ 
+ union kvm_mmu_role {
+ 	u64 as_u64;
+ 	struct {
+ 		union kvm_mmu_page_role base;
+ 		union kvm_mmu_extended_role ext;
+ 	};
+ };
+ 
++>>>>>>> 7dcd57552008 (x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed)
  struct kvm_rmap_head {
  	unsigned long val;
  };
diff --cc arch/x86/kvm/mmu.c
index e7052d2c946d,9ed046fbab5e..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -4420,14 -4724,65 +4420,76 @@@ static void paging32E_init_context(stru
  	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
  }
  
++<<<<<<< HEAD
 +static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
 +
 +	context->base_role.word = 0;
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
 +	context->base_role.ad_disabled = (shadow_accessed_mask == 0);
++=======
+ static union kvm_mmu_extended_role kvm_calc_mmu_role_ext(struct kvm_vcpu *vcpu)
+ {
+ 	union kvm_mmu_extended_role ext = {0};
+ 
+ 	ext.cr0_pg = !!is_paging(vcpu);
+ 	ext.cr4_smep = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
+ 	ext.cr4_smap = !!kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
+ 	ext.cr4_pse = !!is_pse(vcpu);
+ 	ext.cr4_pke = !!kvm_read_cr4_bits(vcpu, X86_CR4_PKE);
+ 	ext.cr4_la57 = !!kvm_read_cr4_bits(vcpu, X86_CR4_LA57);
+ 
+ 	ext.valid = 1;
+ 
+ 	return ext;
+ }
+ 
+ static union kvm_mmu_role kvm_calc_mmu_role_common(struct kvm_vcpu *vcpu,
+ 						   bool base_only)
+ {
+ 	union kvm_mmu_role role = {0};
+ 
+ 	role.base.access = ACC_ALL;
+ 	role.base.nxe = !!is_nx(vcpu);
+ 	role.base.cr4_pae = !!is_pae(vcpu);
+ 	role.base.cr0_wp = is_write_protection(vcpu);
+ 	role.base.smm = is_smm(vcpu);
+ 	role.base.guest_mode = is_guest_mode(vcpu);
+ 
+ 	if (base_only)
+ 		return role;
+ 
+ 	role.ext = kvm_calc_mmu_role_ext(vcpu);
+ 
+ 	return role;
+ }
+ 
+ static union kvm_mmu_role
+ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
+ {
+ 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ 
+ 	role.base.ad_disabled = (shadow_accessed_mask == 0);
+ 	role.base.level = kvm_x86_ops->get_tdp_level(vcpu);
+ 	role.base.direct = true;
+ 
+ 	return role;
+ }
+ 
+ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_mmu *context = vcpu->arch.mmu;
+ 	union kvm_mmu_role new_role =
+ 		kvm_calc_tdp_mmu_root_page_role(vcpu, false);
+ 
+ 	new_role.base.word &= mmu_base_role_mask.word;
+ 	if (new_role.as_u64 == context->mmu_role.as_u64)
+ 		return;
+ 
+ 	context->mmu_role.as_u64 = new_role.as_u64;
++>>>>>>> 7dcd57552008 (x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed)
  	context->page_fault = tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
@@@ -4467,13 -4822,36 +4529,46 @@@
  	reset_tdp_shadow_zero_bits_mask(vcpu, context);
  }
  
++<<<<<<< HEAD
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 +{
 +	bool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
 +	bool smap = kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);
 +	struct kvm_mmu *context = &vcpu->arch.mmu;
 +
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
++=======
+ static union kvm_mmu_role
+ kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
+ {
+ 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ 
+ 	role.base.smep_andnot_wp = role.ext.cr4_smep &&
+ 		!is_write_protection(vcpu);
+ 	role.base.smap_andnot_wp = role.ext.cr4_smap &&
+ 		!is_write_protection(vcpu);
+ 	role.base.direct = !is_paging(vcpu);
+ 
+ 	if (!is_long_mode(vcpu))
+ 		role.base.level = PT32E_ROOT_LEVEL;
+ 	else if (is_la57_mode(vcpu))
+ 		role.base.level = PT64_ROOT_5LEVEL;
+ 	else
+ 		role.base.level = PT64_ROOT_4LEVEL;
+ 
+ 	return role;
+ }
+ 
+ void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_mmu *context = vcpu->arch.mmu;
+ 	union kvm_mmu_role new_role =
+ 		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
+ 
+ 	new_role.base.word &= mmu_base_role_mask.word;
+ 	if (new_role.as_u64 == context->mmu_role.as_u64)
+ 		return;
++>>>>>>> 7dcd57552008 (x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed)
  
  	if (!is_paging(vcpu))
  		nonpaging_init_context(vcpu, context);
@@@ -4484,15 -4862,7 +4579,19 @@@
  	else
  		paging32_init_context(vcpu, context);
  
++<<<<<<< HEAD
 +	context->base_role.nxe = is_nx(vcpu);
 +	context->base_role.cr4_pae = !!is_pae(vcpu);
 +	context->base_role.cr0_wp  = is_write_protection(vcpu);
 +	context->base_role.smep_andnot_wp
 +		= smep && !is_write_protection(vcpu);
 +	context->base_role.smap_andnot_wp
 +		= smap && !is_write_protection(vcpu);
 +	context->base_role.guest_mode = is_guest_mode(vcpu);
 +	context->base_role.smm = is_smm(vcpu);
++=======
+ 	context->mmu_role.as_u64 = new_role.as_u64;
++>>>>>>> 7dcd57552008 (x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed)
  	reset_shadow_zero_bits_mask(vcpu, context);
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
@@@ -4586,6 -4995,20 +4685,23 @@@ static void init_kvm_mmu(struct kvm_vcp
  	else
  		init_kvm_softmmu(vcpu);
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(kvm_init_mmu);
+ 
+ static union kvm_mmu_page_role
+ kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
+ {
+ 	union kvm_mmu_role role;
+ 
+ 	if (tdp_enabled)
+ 		role = kvm_calc_tdp_mmu_root_page_role(vcpu, true);
+ 	else
+ 		role = kvm_calc_shadow_mmu_root_page_role(vcpu, true);
+ 
+ 	return role.base;
+ }
++>>>>>>> 7dcd57552008 (x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed)
  
  void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
  {
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/mmu.c
