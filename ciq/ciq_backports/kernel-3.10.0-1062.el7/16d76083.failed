net/mlx5: EQ, Different EQ types

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5: EQ, Different EQ types (Alaa Hleihel) [1642498]
Rebuild_FUZZ: 93.33%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 16d760839ceef510cf95cbfadc069c4473c7a277
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/16d76083.failed

In mlx5 we have three types of usages for EQs,
1. Asynchronous EQs, used internally by mlx5 core for
 a. FW command completions
 b. FW page requests
 c. one EQ for all other Asynchronous events

2. Completion EQs, used for CQ completion (we create one per core)

3. *Special type of EQ (page fault) used for RDMA on demand paging
(ODP).

*The 3rd type shouldn't be special at least in mlx5 core, it is yet
another async events EQ with specific use case, it will be removed in
the next two patches, and will completely move its logic to mlx5_ib,
as it is rdma specific.

In this patch we remove use case (eq type) specific fields from
struct mlx5_eq into a new eq type specific structures.

struct mlx5_eq_async;
truct mlx5_eq_comp;
struct mlx5_eq_pagefault;

Separate between their type specific flows.

In the future we will allow users to create there own generic EQs.
for now we will allow only one for ODP in next patches.

We will introduce event listeners registration API for those who
want to receive mlx5 async events.
After that mlx5 eq handling will be clean from feature/user specific
handling.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit 16d760839ceef510cf95cbfadc069c4473c7a277)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
#	drivers/net/ethernet/mellanox/mlx5/core/main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index e502527fbe4d,252c9f0569b1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -61,6 -66,29 +61,32 @@@ enum 
  	MLX5_EQ_DOORBEL_OFFSET	= 0x40,
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_irq_info {
+ 	cpumask_var_t mask;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ };
+ 
+ struct mlx5_eq_table {
+ 	struct list_head        comp_eqs_list;
+ 	struct mlx5_eq          pages_eq;
+ 	struct mlx5_eq          async_eq;
+ 	struct mlx5_eq	        cmd_eq;
+ 
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	struct mlx5_eq_pagefault pfault_eq;
+ #endif
+ 	struct mutex            lock; /* sync async eqs creations */
+ 	u8			num_async_eqs;
+ 	int			num_comp_vectors;
+ 	struct mlx5_irq_info	*irq_info;
+ #ifdef CONFIG_RFS_ACCEL
+ 	struct cpu_rmap         *rmap;
+ #endif
+ };
+ 
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  #define MLX5_ASYNC_EVENT_MASK ((1ull << MLX5_EVENT_TYPE_PATH_MIG)	    | \
  			       (1ull << MLX5_EVENT_TYPE_COMM_EST)	    | \
  			       (1ull << MLX5_EVENT_TYPE_SQ_DRAINED)	    | \
@@@ -629,10 -709,11 +707,16 @@@ static void init_eq_buf(struct mlx5_eq 
  	}
  }
  
++<<<<<<< HEAD
 +int mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
 +		       int nent, u64 mask, const char *name,
 +		       enum mlx5_eq_type type)
++=======
+ static int
+ mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
+ 		   int nent, u64 mask, const char *name, irq_handler_t handler)
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  {
 -	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
  	struct mlx5_cq_table *cq_table = &eq->cq_table;
  	u32 out[MLX5_ST_SZ_DW(create_eq_out)] = {0};
  	struct mlx5_priv *priv = &dev->priv;
@@@ -802,23 -850,79 +853,78 @@@ int mlx5_eq_del_cq(struct mlx5_eq *eq, 
  	return 0;
  }
  
 -int mlx5_eq_table_init(struct mlx5_core_dev *dev)
 +int mlx5_eq_init(struct mlx5_core_dev *dev)
  {
 -	struct mlx5_eq_table *eq_table;
  	int err;
  
 -	eq_table = kvzalloc(sizeof(*eq_table), GFP_KERNEL);
 -	if (!eq_table)
 -		return -ENOMEM;
 -
 -	dev->priv.eq_table = eq_table;
 -
  	err = mlx5_eq_debugfs_init(dev);
++<<<<<<< HEAD
++=======
+ 	if (err)
+ 		goto kvfree_eq_table;
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  
+ 	mutex_init(&eq_table->lock);
+ 
+ 	return 0;
+ 
+ kvfree_eq_table:
+ 	kvfree(eq_table);
+ 	dev->priv.eq_table = NULL;
  	return err;
  }
  
 -void mlx5_eq_table_cleanup(struct mlx5_core_dev *dev)
 +void mlx5_eq_cleanup(struct mlx5_core_dev *dev)
  {
  	mlx5_eq_debugfs_cleanup(dev);
 -	kvfree(dev->priv.eq_table);
  }
  
++<<<<<<< HEAD
 +int mlx5_start_eqs(struct mlx5_core_dev *dev)
++=======
+ /* Async EQs */
+ 
+ int mlx5_create_async_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
+ 			 int nent, u64 mask, const char *name, irq_handler_t handler)
+ {
+ 	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
+ 	u8 vecdix;
+ 	int err;
+ 
+ 	mutex_lock(&eq_table->lock);
+ 	if (eq_table->num_async_eqs >= MLX5_EQ_MAX_ASYNC_EQS) {
+ 		err = -ENOSPC;
+ 		goto unlock;
+ 	}
+ 
+ 	vecdix = eq_table->num_async_eqs + 1;
+ 
+ 	err = mlx5_create_map_eq(dev, eq, vecdix, nent, mask, name, handler);
+ 	if (!err)
+ 		eq_table->num_async_eqs++;
+ 
+ unlock:
+ 	mutex_unlock(&eq_table->lock);
+ 	return err;
+ }
+ 
+ int mlx5_destroy_async_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
+ {
+ 	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
+ 	int err;
+ 
+ 	mutex_lock(&eq_table->lock);
+ 	err = mlx5_destroy_unmap_eq(dev, eq);
+ 	if (!err)
+ 		eq_table->num_async_eqs--;
+ 	mutex_unlock(&eq_table->lock);
+ 	return err;
+ }
+ 
+ static int create_async_eqs(struct mlx5_core_dev *dev)
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  {
 -	struct mlx5_eq_table *table = dev->priv.eq_table;
 +	struct mlx5_eq_table *table = &dev->priv.eq_table;
  	u64 async_event_mask = MLX5_ASYNC_EVENT_MASK;
  	int err;
  
@@@ -939,12 -1034,329 +1036,338 @@@ void mlx5_stop_eqs(struct mlx5_core_de
  			      err);
  }
  
 -struct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev)
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen)
  {
++<<<<<<< HEAD
 +	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {0};
 +
 +	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
 +	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
++=======
+ 	return &dev->priv.eq_table->async_eq;
+ }
+ 
+ void mlx5_eq_synchronize_async_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->async_eq.irqn);
+ }
+ 
+ void mlx5_eq_synchronize_cmd_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->cmd_eq.irqn);
+ }
+ 
+ /* Completion EQs */
+ 
+ static int set_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	if (!zalloc_cpumask_var(&irq_info->mask, GFP_KERNEL)) {
+ 		mlx5_core_warn(mdev, "zalloc_cpumask_var failed");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	cpumask_set_cpu(cpumask_local_spread(i, priv->numa_node),
+ 			irq_info->mask);
+ 
+ 	if (IS_ENABLED(CONFIG_SMP) &&
+ 	    irq_set_affinity_hint(irq, irq_info->mask))
+ 		mlx5_core_warn(mdev, "irq_set_affinity_hint failed, irq 0x%.4x", irq);
+ 
+ 	return 0;
+ }
+ 
+ static void clear_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	irq_set_affinity_hint(irq, NULL);
+ 	free_cpumask_var(irq_info->mask);
+ }
+ 
+ static int set_comp_irq_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++) {
+ 		err = set_comp_irq_affinity_hint(mdev, i);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return 0;
+ 
+ err_out:
+ 	for (i--; i >= 0; i--)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ 
+ 	return err;
+ }
+ 
+ static void clear_comp_irqs_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ }
+ 
+ static void destroy_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		list_del(&eq->list);
+ 		if (mlx5_destroy_unmap_eq(dev, &eq->core))
+ 			mlx5_core_warn(dev, "failed to destroy comp EQ 0x%x\n",
+ 				       eq->core.eqn);
+ 		tasklet_disable(&eq->tasklet_ctx.task);
+ 		kfree(eq);
+ 	}
+ }
+ 
+ static int create_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	struct mlx5_eq_comp *eq;
+ 	int ncomp_vec;
+ 	int nent;
+ 	int err;
+ 	int i;
+ 
+ 	INIT_LIST_HEAD(&table->comp_eqs_list);
+ 	ncomp_vec = table->num_comp_vectors;
+ 	nent = MLX5_COMP_EQ_SIZE;
+ #ifdef CONFIG_RFS_ACCEL
+ 	table->rmap = alloc_irq_cpu_rmap(ncomp_vec);
+ 	if (!table->rmap)
+ 		return -ENOMEM;
+ #endif
+ 	for (i = 0; i < ncomp_vec; i++) {
+ 		int vecidx = i + MLX5_EQ_VEC_COMP_BASE;
+ 
+ 		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
+ 		if (!eq) {
+ 			err = -ENOMEM;
+ 			goto clean;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.list);
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.process_list);
+ 		spin_lock_init(&eq->tasklet_ctx.lock);
+ 		tasklet_init(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb,
+ 			     (unsigned long)&eq->tasklet_ctx);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 		irq_cpu_rmap_add(table->rmap, pci_irq_vector(dev->pdev, vecidx));
+ #endif
+ 		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
+ 		err = mlx5_create_map_eq(dev, &eq->core, vecidx, nent, 0,
+ 					 name, mlx5_eq_comp_int);
+ 		if (err) {
+ 			kfree(eq);
+ 			goto clean;
+ 		}
+ 		mlx5_core_dbg(dev, "allocated completion EQN %d\n", eq->core.eqn);
+ 		/* add tail, to keep the list ordered, for mlx5_vector2eqn to work */
+ 		list_add_tail(&eq->list, &table->comp_eqs_list);
+ 	}
+ 
+ 	err = set_comp_irq_affinity_hints(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to alloc affinity hint cpumask\n");
+ 		goto clean;
+ 	}
+ 
+ 	return 0;
+ 
+ clean:
+ 	destroy_comp_eqs(dev);
+ 	return err;
+ }
+ 
+ int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
+ 		    unsigned int *irqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 	int err = -ENOENT;
+ 	int i = 0;
+ 
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		if (i++ == vector) {
+ 			*eqn = eq->core.eqn;
+ 			*irqn = eq->core.irqn;
+ 			err = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_vector2eqn);
+ 
+ unsigned int mlx5_comp_vectors_count(struct mlx5_core_dev *dev)
+ {
+ 	return dev->priv.eq_table->num_comp_vectors;
+ }
+ EXPORT_SYMBOL(mlx5_comp_vectors_count);
+ 
+ struct cpumask *
+ mlx5_comp_irq_get_affinity_mask(struct mlx5_core_dev *dev, int vector)
+ {
+ 	/* TODO: consider irq_get_affinity_mask(irq) */
+ 	return dev->priv.eq_table->irq_info[vector + MLX5_EQ_VEC_COMP_BASE].mask;
+ }
+ EXPORT_SYMBOL(mlx5_comp_irq_get_affinity_mask);
+ 
+ struct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev)
+ {
+ #ifdef CONFIG_RFS_ACCEL
+ 	return dev->priv.eq_table->rmap;
+ #else
+ 	return NULL;
+ #endif
+ }
+ 
+ struct mlx5_eq_comp *mlx5_eqn2comp_eq(struct mlx5_core_dev *dev, int eqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq;
+ 
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list) {
+ 		if (eq->core.eqn == eqn)
+ 			return eq;
+ 	}
+ 
+ 	return ERR_PTR(-ENOENT);
+ }
+ 
+ /* This function should only be called after mlx5_cmd_force_teardown_hca */
+ void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list)
+ 		free_irq(eq->core.irqn, eq);
+ 
+ 	free_irq(table->pages_eq.irqn, &table->pages_eq);
+ 	free_irq(table->async_eq.irqn, &table->async_eq);
+ 	free_irq(table->cmd_eq.irqn, &table->cmd_eq);
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	if (MLX5_CAP_GEN(dev, pg))
+ 		free_irq(table->pfault_eq.core.irqn, &table->pfault_eq.core);
+ #endif
+ 	pci_free_irq_vectors(dev->pdev);
+ }
+ 
+ static int alloc_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 	struct mlx5_eq_table *table = priv->eq_table;
+ 	int num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?
+ 		      MLX5_CAP_GEN(dev, max_num_eqs) :
+ 		      1 << MLX5_CAP_GEN(dev, log_max_eq);
+ 	int nvec;
+ 	int err;
+ 
+ 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
+ 	       MLX5_EQ_VEC_COMP_BASE;
+ 	nvec = min_t(int, nvec, num_eqs);
+ 	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
+ 		return -ENOMEM;
+ 
+ 	table->irq_info = kcalloc(nvec, sizeof(*table->irq_info), GFP_KERNEL);
+ 	if (!table->irq_info)
+ 		return -ENOMEM;
+ 
+ 	nvec = pci_alloc_irq_vectors(dev->pdev, MLX5_EQ_VEC_COMP_BASE + 1,
+ 				     nvec, PCI_IRQ_MSIX);
+ 	if (nvec < 0) {
+ 		err = nvec;
+ 		goto err_free_irq_info;
+ 	}
+ 
+ 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+ 
+ 	return 0;
+ 
+ err_free_irq_info:
+ 	kfree(table->irq_info);
+ 	return err;
+ }
+ 
+ static void free_irq_vectors(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_priv *priv = &dev->priv;
+ 
+ 	pci_free_irq_vectors(dev->pdev);
+ 	kfree(priv->eq_table->irq_info);
+ }
+ 
+ int mlx5_eq_table_create(struct mlx5_core_dev *dev)
+ {
+ 	int err;
+ 
+ 	err = alloc_irq_vectors(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "alloc irq vectors failed\n");
+ 		return err;
+ 	}
+ 
+ 	err = create_async_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create async EQs\n");
+ 		goto err_async_eqs;
+ 	}
+ 
+ 	err = create_comp_eqs(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to create completion EQs\n");
+ 		goto err_comp_eqs;
+ 	}
+ 
+ 	return 0;
+ err_comp_eqs:
+ 	destroy_async_eqs(dev);
+ err_async_eqs:
+ 	free_irq_vectors(dev);
+ 	return err;
+ }
+ 
+ void mlx5_eq_table_destroy(struct mlx5_core_dev *dev)
+ {
+ 	destroy_comp_eqs(dev);
+ 	destroy_async_eqs(dev);
+ 	free_irq_vectors(dev);
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/main.c
index be9913773fa9,3de83fe65f2b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@@ -53,6 -53,7 +53,10 @@@
  #endif
  #include <net/devlink.h>
  #include "mlx5_core.h"
++<<<<<<< HEAD
++=======
+ #include "lib/eq.h"
++>>>>>>> 16d760839cee (net/mlx5: EQ, Different EQ types)
  #include "fs_core.h"
  #include "lib/mpfs.h"
  #include "eswitch.h"
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/cq.c b/drivers/net/ethernet/mellanox/mlx5/core/cq.c
index 4b85abb5c9f7..3a34381b9a9f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/cq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cq.c
@@ -92,10 +92,10 @@ int mlx5_core_create_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq,
 	u32 dout[MLX5_ST_SZ_DW(destroy_cq_out)];
 	u32 out[MLX5_ST_SZ_DW(create_cq_out)];
 	u32 din[MLX5_ST_SZ_DW(destroy_cq_in)];
-	struct mlx5_eq *eq;
+	struct mlx5_eq_comp *eq;
 	int err;
 
-	eq = mlx5_eqn2eq(dev, eqn);
+	eq = mlx5_eqn2comp_eq(dev, eqn);
 	if (IS_ERR(eq))
 		return PTR_ERR(eq);
 
@@ -119,7 +119,7 @@ int mlx5_core_create_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq,
 	INIT_LIST_HEAD(&cq->tasklet_ctx.list);
 
 	/* Add to comp EQ CQ tree to recv comp events */
-	err = mlx5_eq_add_cq(eq, cq);
+	err = mlx5_eq_add_cq(&eq->core, cq);
 	if (err)
 		goto err_cmd;
 
@@ -139,7 +139,7 @@ int mlx5_core_create_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq,
 	return 0;
 
 err_cq_add:
-	mlx5_eq_del_cq(eq, cq);
+	mlx5_eq_del_cq(&eq->core, cq);
 err_cmd:
 	memset(din, 0, sizeof(din));
 	memset(dout, 0, sizeof(dout));
@@ -161,7 +161,7 @@ int mlx5_core_destroy_cq(struct mlx5_core_dev *dev, struct mlx5_core_cq *cq)
 	if (err)
 		return err;
 
-	err = mlx5_eq_del_cq(cq->eq, cq);
+	err = mlx5_eq_del_cq(&cq->eq->core, cq);
 	if (err)
 		return err;
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 48dde39aa588..a0ce6f5a6cac 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -245,7 +245,7 @@ static void mlx5e_enable_async_events(struct mlx5e_priv *priv)
 static void mlx5e_disable_async_events(struct mlx5e_priv *priv)
 {
 	clear_bit(MLX5E_STATE_ASYNC_EVENTS_ENABLED, &priv->state);
-	synchronize_irq(pci_irq_vector(priv->mdev->pdev, MLX5_EQ_VEC_ASYNC));
+	mlx5_eq_synchronize_async_irq(priv->mdev);
 }
 
 static inline int mlx5e_get_wqe_mtt_sz(void)
@@ -3706,17 +3706,17 @@ static netdev_features_t mlx5e_features_check(struct sk_buff *skb,
 static bool mlx5e_tx_timeout_eq_recover(struct net_device *dev,
 					struct mlx5e_txqsq *sq)
 {
-	struct mlx5_eq *eq = sq->cq.mcq.eq;
+	struct mlx5_eq_comp *eq = sq->cq.mcq.eq;
 	u32 eqe_count;
 
 	netdev_err(dev, "EQ 0x%x: Cons = 0x%x, irqn = 0x%x\n",
-		   eq->eqn, eq->cons_index, eq->irqn);
+		   eq->core.eqn, eq->core.cons_index, eq->core.irqn);
 
 	eqe_count = mlx5_eq_poll_irq_disabled(eq);
 	if (!eqe_count)
 		return false;
 
-	netdev_err(dev, "Recover %d eqes on EQ 0x%x\n", eqe_count, eq->eqn);
+	netdev_err(dev, "Recover %d eqes on EQ 0x%x\n", eqe_count, eq->core.eqn);
 	sq->channel->stats->eq_rearm++;
 	return true;
 }
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
index 8798f4084ee6..ec38744befc5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
@@ -1567,7 +1567,7 @@ static void esw_disable_vport(struct mlx5_eswitch *esw, int vport_num)
 	/* Mark this vport as disabled to discard new events */
 	vport->enabled = false;
 
-	synchronize_irq(pci_irq_vector(esw->dev->pdev, MLX5_EQ_VEC_ASYNC));
+	mlx5_eq_synchronize_async_irq(esw->dev);
 	/* Wait for current already scheduled events to complete */
 	flush_workqueue(esw->work_queue);
 	/* Disable events from this vport */
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/health.c b/drivers/net/ethernet/mellanox/mlx5/core/health.c
index b565dc1876d3..efb034277d22 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/health.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/health.c
@@ -81,7 +81,7 @@ static void trigger_cmd_completions(struct mlx5_core_dev *dev)
 	u64 vector;
 
 	/* wait for pending handlers to complete */
-	synchronize_irq(pci_irq_vector(dev->pdev, MLX5_EQ_VEC_CMD));
+	mlx5_eq_synchronize_cmd_irq(dev);
 	spin_lock_irqsave(&dev->cmd.alloc_lock, flags);
 	vector = ~dev->cmd.bitmask & ((1ul << (1 << dev->cmd.log_sz)) - 1);
 	if (!vector)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/main.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 2ac07968015d..fb5264ca4085 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -137,10 +137,6 @@ struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
 u32 mlx5_eq_poll_irq_disabled(struct mlx5_eq *eq);
 void mlx5_cq_tasklet_cb(unsigned long data);
 void mlx5_cmd_comp_handler(struct mlx5_core_dev *dev, u64 vec, bool forced);
-int mlx5_debug_eq_add(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
-void mlx5_debug_eq_remove(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
-int mlx5_eq_debugfs_init(struct mlx5_core_dev *dev);
-void mlx5_eq_debugfs_cleanup(struct mlx5_core_dev *dev);
 int mlx5_cq_debugfs_init(struct mlx5_core_dev *dev);
 void mlx5_cq_debugfs_cleanup(struct mlx5_core_dev *dev);
 
diff --git a/include/linux/mlx5/cq.h b/include/linux/mlx5/cq.h
index 31a750570c38..28b757a64029 100644
--- a/include/linux/mlx5/cq.h
+++ b/include/linux/mlx5/cq.h
@@ -60,7 +60,7 @@ struct mlx5_core_cq {
 	} tasklet_ctx;
 	int			reset_notify_added;
 	struct list_head	reset_notify;
-	struct mlx5_eq		*eq;
+	struct mlx5_eq_comp	*eq;
 	u16 uid;
 };
 
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index af4fa7465c37..56d903810855 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -215,14 +215,6 @@ enum mlx5_port_status {
 	MLX5_PORT_DOWN      = 2,
 };
 
-enum mlx5_eq_type {
-	MLX5_EQ_TYPE_COMP,
-	MLX5_EQ_TYPE_ASYNC,
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	MLX5_EQ_TYPE_PF,
-#endif
-};
-
 struct mlx5_bfreg_info {
 	u32		       *sys_pages;
 	int			num_low_latency_bfregs;
@@ -754,7 +746,7 @@ struct mlx5_pagefault {
 		} rdma;
 	};
 
-	struct mlx5_eq	       *eq;
+	struct mlx5_eq_pagefault *eq;
 	struct work_struct	work;
 };
 
