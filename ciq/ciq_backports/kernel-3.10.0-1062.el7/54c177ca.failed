net/mlx5e: Branch according to classified tunnel type

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Branch according to classified tunnel type (Alaa Hleihel) [1642362 1642498]
Rebuild_FUZZ: 96.08%
commit-author Oz Shlomo <ozsh@mellanox.com>
commit 54c177ca9c6efe5df516eefb886761b89a82eaf0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/54c177ca.failed

Currently the tunnel offloading encap/decap methods assumes that VXLAN
is the sole tunneling protocol. Lay the infrastructure for supporting
multiple tunneling protocols by branching according to the tunnel
net device kind.

Encap filters tunnel type is determined according to the egress/mirred
net device. Decap filters classify the tunnel type according to the
filter's ingress net device kind.

Distinguish between the tunnel type as defined by the SW model and
the FW reformat type that specifies the HW operation being made.

	Signed-off-by: Oz Shlomo <ozsh@mellanox.com>
	Reviewed-by: Eli Britstein <elibr@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 54c177ca9c6efe5df516eefb886761b89a82eaf0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index fbb4f1b36627,7d7f490d8ff1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -72,8 -74,18 +72,20 @@@ enum 
  	MLX5E_TC_FLOW_OFFLOADED	= BIT(MLX5E_TC_FLOW_BASE + 2),
  	MLX5E_TC_FLOW_HAIRPIN	= BIT(MLX5E_TC_FLOW_BASE + 3),
  	MLX5E_TC_FLOW_HAIRPIN_RSS = BIT(MLX5E_TC_FLOW_BASE + 4),
 -	MLX5E_TC_FLOW_SLOW	  = BIT(MLX5E_TC_FLOW_BASE + 5),
  };
  
++<<<<<<< HEAD
++=======
+ #define MLX5E_TC_MAX_SPLITS 1
+ 
+ enum {
+ 	MLX5E_TC_TUNNEL_TYPE_UNKNOWN,
+ 	MLX5E_TC_TUNNEL_TYPE_VXLAN
+ };
+ 
+ static int mlx5e_get_tunnel_type(struct net_device *tunnel_dev);
+ 
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  struct mlx5e_tc_flow {
  	struct rhash_head	node;
  	struct mlx5e_priv	*priv;
@@@ -666,10 -684,19 +678,22 @@@ static void mlx5e_hairpin_flow_del(stru
  	}
  }
  
++<<<<<<< HEAD
 +static struct mlx5_flow_handle *
++=======
+ static const char *mlx5e_netdev_kind(struct net_device *dev)
+ {
+ 	if (dev->rtnl_link_ops)
+ 		return dev->rtnl_link_ops->kind;
+ 	else
+ 		return "";
+ }
+ 
+ static int
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  mlx5e_tc_add_nic_flow(struct mlx5e_priv *priv,
  		      struct mlx5e_tc_flow_parse_attr *parse_attr,
 -		      struct mlx5e_tc_flow *flow,
 -		      struct netlink_ext_ack *extack)
 +		      struct mlx5e_tc_flow *flow)
  {
  	struct mlx5_nic_flow_attr *attr = flow->nic_attr;
  	struct mlx5_core_dev *dev = priv->mdev;
@@@ -909,9 -1052,11 +933,17 @@@ void mlx5e_tc_encap_flows_add(struct ml
  	struct mlx5e_tc_flow *flow;
  	int err;
  
++<<<<<<< HEAD
 +	err = mlx5_encap_alloc(priv->mdev, e->tunnel_type,
 +			       e->encap_size, e->encap_header,
 +			       &e->encap_id);
++=======
+ 	err = mlx5_packet_reformat_alloc(priv->mdev,
+ 					 e->reformat_type,
+ 					 e->encap_size, e->encap_header,
+ 					 MLX5_FLOW_NAMESPACE_FDB,
+ 					 &e->encap_id);
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	if (err) {
  		mlx5_core_warn(priv->mdev, "Failed to offload cached encapsulation header, %d\n",
  			       err);
@@@ -1068,8 -1281,10 +1100,9 @@@ static void parse_vxlan_attr(struct mlx
  
  static int parse_tunnel_attr(struct mlx5e_priv *priv,
  			     struct mlx5_flow_spec *spec,
- 			     struct tc_cls_flower_offload *f)
+ 			     struct tc_cls_flower_offload *f,
+ 			     struct net_device *filter_dev)
  {
 -	struct netlink_ext_ack *extack = f->common.extack;
  	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
  				       outer_headers);
  	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
@@@ -1079,47 -1294,28 +1112,69 @@@
  		skb_flow_dissector_target(f->dissector,
  					  FLOW_DISSECTOR_KEY_ENC_CONTROL,
  					  f->key);
+ 	int tunnel_type;
+ 	int err = 0;
  
++<<<<<<< HEAD
 +	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_ENC_PORTS)) {
 +		struct flow_dissector_key_ports *key =
 +			skb_flow_dissector_target(f->dissector,
 +						  FLOW_DISSECTOR_KEY_ENC_PORTS,
 +						  f->key);
 +		struct flow_dissector_key_ports *mask =
 +			skb_flow_dissector_target(f->dissector,
 +						  FLOW_DISSECTOR_KEY_ENC_PORTS,
 +						  f->mask);
 +		struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +		struct mlx5e_rep_priv *uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 +		struct net_device *up_dev = uplink_rpriv->netdev;
 +		struct mlx5e_priv *up_priv = netdev_priv(up_dev);
 +
 +		/* Full udp dst port must be given */
 +		if (memchr_inv(&mask->dst, 0xff, sizeof(mask->dst)))
 +			goto vxlan_match_offload_err;
 +
 +		if (mlx5e_vxlan_lookup_port(up_priv, be16_to_cpu(key->dst)) &&
 +		    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap))
 +			parse_vxlan_attr(spec, f);
 +		else {
 +			netdev_warn(priv->netdev,
 +				    "%d isn't an offloaded vxlan udp dport\n", be16_to_cpu(key->dst));
 +			return -EOPNOTSUPP;
 +		}
 +
 +		MLX5_SET(fte_match_set_lyr_2_4, headers_c,
 +			 udp_dport, ntohs(mask->dst));
 +		MLX5_SET(fte_match_set_lyr_2_4, headers_v,
 +			 udp_dport, ntohs(key->dst));
 +
 +		MLX5_SET(fte_match_set_lyr_2_4, headers_c,
 +			 udp_sport, ntohs(mask->src));
 +		MLX5_SET(fte_match_set_lyr_2_4, headers_v,
 +			 udp_sport, ntohs(key->src));
 +	} else { /* udp dst port must be given */
 +vxlan_match_offload_err:
++=======
+ 	tunnel_type = mlx5e_get_tunnel_type(filter_dev);
+ 	if (tunnel_type == MLX5E_TC_TUNNEL_TYPE_VXLAN) {
+ 		err = parse_tunnel_vxlan_attr(priv, spec, f,
+ 					      headers_c, headers_v);
+ 	} else {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "decapsulation offload is not supported");
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  		netdev_warn(priv->netdev,
- 			    "IP tunnel decap offload supported only for vxlan, must set UDP dport\n");
+ 			    "decapsulation offload is not supported for %s net device (%d)\n",
+ 			    mlx5e_netdev_kind(filter_dev), tunnel_type);
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (err) {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "failed to parse tunnel attributes");
+ 		netdev_warn(priv->netdev,
+ 			    "failed to parse %s tunnel attributes (%d)\n",
+ 			    mlx5e_netdev_kind(filter_dev), tunnel_type);
  		return -EOPNOTSUPP;
  	}
  
@@@ -1214,8 -1420,10 +1269,9 @@@
  static int __parse_cls_flower(struct mlx5e_priv *priv,
  			      struct mlx5_flow_spec *spec,
  			      struct tc_cls_flower_offload *f,
+ 			      struct net_device *filter_dev,
  			      u8 *match_level)
  {
 -	struct netlink_ext_ack *extack = f->common.extack;
  	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
  				       outer_headers);
  	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
@@@ -1600,8 -1814,10 +1656,9 @@@
  static int parse_cls_flower(struct mlx5e_priv *priv,
  			    struct mlx5e_tc_flow *flow,
  			    struct mlx5_flow_spec *spec,
- 			    struct tc_cls_flower_offload *f)
+ 			    struct tc_cls_flower_offload *f,
+ 			    struct net_device *filter_dev)
  {
 -	struct netlink_ext_ack *extack = f->common.extack;
  	struct mlx5_core_dev *dev = priv->mdev;
  	struct mlx5_eswitch *esw = dev->priv.eswitch;
  	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@@ -2195,70 -2455,36 +2252,89 @@@ static int mlx5e_route_lookup_ipv6(stru
  	return 0;
  }
  
 -static int mlx5e_gen_vxlan_header(char buf[], struct ip_tunnel_key *tun_key)
 +static void gen_vxlan_header_ipv4(struct net_device *out_dev,
 +				  char buf[], int encap_size,
 +				  unsigned char h_dest[ETH_ALEN],
 +				  u8 tos, u8 ttl,
 +				  __be32 daddr,
 +				  __be32 saddr,
 +				  __be16 udp_dst_port,
 +				  __be32 vx_vni)
  {
 -	__be32 tun_id = tunnel_id_to_key32(tun_key->tun_id);
 -	struct udphdr *udp = (struct udphdr *)(buf);
 -	struct vxlanhdr *vxh = (struct vxlanhdr *)
 -			       ((char *)udp + sizeof(struct udphdr));
 +	struct ethhdr *eth = (struct ethhdr *)buf;
 +	struct iphdr  *ip = (struct iphdr *)((char *)eth + sizeof(struct ethhdr));
 +	struct udphdr *udp = (struct udphdr *)((char *)ip + sizeof(struct iphdr));
 +	struct vxlanhdr *vxh = (struct vxlanhdr *)((char *)udp + sizeof(struct udphdr));
  
 -	udp->dest = tun_key->tp_dst;
 -	vxh->vx_flags = VXLAN_HF_VNI;
 -	vxh->vx_vni = vxlan_vni_field(tun_id);
 +	memset(buf, 0, encap_size);
  
 -	return 0;
 +	ether_addr_copy(eth->h_dest, h_dest);
 +	ether_addr_copy(eth->h_source, out_dev->dev_addr);
 +	eth->h_proto = htons(ETH_P_IP);
 +
 +	ip->daddr = daddr;
 +	ip->saddr = saddr;
 +
 +	ip->tos = tos;
 +	ip->ttl = ttl;
 +	ip->protocol = IPPROTO_UDP;
 +	ip->version = 0x4;
 +	ip->ihl = 0x5;
 +
 +	udp->dest = udp_dst_port;
 +	vxh->vx_flags = VXLAN_HF_VNI;
 +	vxh->vx_vni = vxlan_vni_field(vx_vni);
  }
  
++<<<<<<< HEAD
 +static void gen_vxlan_header_ipv6(struct net_device *out_dev,
 +				  char buf[], int encap_size,
 +				  unsigned char h_dest[ETH_ALEN],
 +				  u8 tos, u8 ttl,
 +				  struct in6_addr *daddr,
 +				  struct in6_addr *saddr,
 +				  __be16 udp_dst_port,
 +				  __be32 vx_vni)
 +{
 +	struct ethhdr *eth = (struct ethhdr *)buf;
 +	struct ipv6hdr *ip6h = (struct ipv6hdr *)((char *)eth + sizeof(struct ethhdr));
 +	struct udphdr *udp = (struct udphdr *)((char *)ip6h + sizeof(struct ipv6hdr));
 +	struct vxlanhdr *vxh = (struct vxlanhdr *)((char *)udp + sizeof(struct udphdr));
 +
 +	memset(buf, 0, encap_size);
 +
 +	ether_addr_copy(eth->h_dest, h_dest);
 +	ether_addr_copy(eth->h_source, out_dev->dev_addr);
 +	eth->h_proto = htons(ETH_P_IPV6);
 +
 +	ip6_flow_hdr(ip6h, tos, 0);
 +	/* the HW fills up ipv6 payload len */
 +	ip6h->nexthdr     = IPPROTO_UDP;
 +	ip6h->hop_limit   = ttl;
 +	ip6h->daddr	  = *daddr;
 +	ip6h->saddr	  = *saddr;
 +
 +	udp->dest = udp_dst_port;
 +	vxh->vx_flags = VXLAN_HF_VNI;
 +	vxh->vx_vni = vxlan_vni_field(vx_vni);
++=======
+ static int mlx5e_gen_ip_tunnel_header(char buf[], __u8 *ip_proto,
+ 				      struct mlx5e_encap_entry *e)
+ {
+ 	int err = 0;
+ 	struct ip_tunnel_key *key = &e->tun_info.key;
+ 
+ 	if (e->tunnel_type == MLX5E_TC_TUNNEL_TYPE_VXLAN) {
+ 		*ip_proto = IPPROTO_UDP;
+ 		err = mlx5e_gen_vxlan_header(buf, key);
+ 	} else {
+ 		pr_warn("mlx5: Cannot generate tunnel header for tunnel type (%d)\n"
+ 			, e->tunnel_type);
+ 		err = -EOPNOTSUPP;
+ 	}
+ 
+ 	return err;
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  }
  
  static int mlx5e_create_encap_header_ipv4(struct mlx5e_priv *priv,
@@@ -2266,7 -2492,9 +2342,13 @@@
  					  struct mlx5e_encap_entry *e)
  {
  	int max_encap_size = MLX5_CAP_ESW(priv->mdev, max_encap_header_size);
++<<<<<<< HEAD
 +	int ipv4_encap_size = ETH_HLEN + sizeof(struct iphdr) + VXLAN_HLEN;
++=======
+ 	int ipv4_encap_size = ETH_HLEN +
+ 			      sizeof(struct iphdr) +
+ 			      e->tunnel_hlen;
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	struct ip_tunnel_key *tun_key = &e->tun_info.key;
  	struct net_device *out_dev;
  	struct neighbour *n = NULL;
@@@ -2329,18 -2548,27 +2411,40 @@@
  	ether_addr_copy(e->h_dest, n->ha);
  	read_unlock_bh(&n->lock);
  
++<<<<<<< HEAD
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		gen_vxlan_header_ipv4(out_dev, encap_header,
 +				      ipv4_encap_size, e->h_dest, tos, ttl,
 +				      fl4.daddr,
 +				      fl4.saddr, tun_key->tp_dst,
 +				      tunnel_id_to_key32(tun_key->tun_id));
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
++=======
+ 	/* add ethernet header */
+ 	eth = (struct ethhdr *)encap_header;
+ 	ether_addr_copy(eth->h_dest, e->h_dest);
+ 	ether_addr_copy(eth->h_source, out_dev->dev_addr);
+ 	eth->h_proto = htons(ETH_P_IP);
+ 
+ 	/* add ip header */
+ 	ip = (struct iphdr *)((char *)eth + sizeof(struct ethhdr));
+ 	ip->tos = tun_key->tos;
+ 	ip->version = 0x4;
+ 	ip->ihl = 0x5;
+ 	ip->ttl = ttl;
+ 	ip->daddr = fl4.daddr;
+ 	ip->saddr = fl4.saddr;
+ 
+ 	/* add tunneling protocol header */
+ 	err = mlx5e_gen_ip_tunnel_header((char *)ip + sizeof(struct iphdr),
+ 					  &ip->protocol, e);
+ 	if (err)
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  		goto destroy_neigh_entry;
 -
 +	}
  	e->encap_size = ipv4_encap_size;
  	e->encap_header = encap_header;
  
@@@ -2350,8 -2578,11 +2454,16 @@@
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_encap_alloc(priv->mdev, e->tunnel_type,
 +			       ipv4_encap_size, encap_header, &e->encap_id);
++=======
+ 	err = mlx5_packet_reformat_alloc(priv->mdev,
+ 					 e->reformat_type,
+ 					 ipv4_encap_size, encap_header,
+ 					 MLX5_FLOW_NAMESPACE_FDB,
+ 					 &e->encap_id);
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	if (err)
  		goto destroy_neigh_entry;
  
@@@ -2375,7 -2606,9 +2487,13 @@@ static int mlx5e_create_encap_header_ip
  					  struct mlx5e_encap_entry *e)
  {
  	int max_encap_size = MLX5_CAP_ESW(priv->mdev, max_encap_header_size);
++<<<<<<< HEAD
 +	int ipv6_encap_size = ETH_HLEN + sizeof(struct ipv6hdr) + VXLAN_HLEN;
++=======
+ 	int ipv6_encap_size = ETH_HLEN +
+ 			      sizeof(struct ipv6hdr) +
+ 			      e->tunnel_hlen;
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	struct ip_tunnel_key *tun_key = &e->tun_info.key;
  	struct net_device *out_dev;
  	struct neighbour *n = NULL;
@@@ -2438,18 -2662,25 +2556,39 @@@
  	ether_addr_copy(e->h_dest, n->ha);
  	read_unlock_bh(&n->lock);
  
++<<<<<<< HEAD
 +	switch (e->tunnel_type) {
 +	case MLX5_HEADER_TYPE_VXLAN:
 +		gen_vxlan_header_ipv6(out_dev, encap_header,
 +				      ipv6_encap_size, e->h_dest, tos, ttl,
 +				      &fl6.daddr,
 +				      &fl6.saddr, tun_key->tp_dst,
 +				      tunnel_id_to_key32(tun_key->tun_id));
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
++=======
+ 	/* add ethernet header */
+ 	eth = (struct ethhdr *)encap_header;
+ 	ether_addr_copy(eth->h_dest, e->h_dest);
+ 	ether_addr_copy(eth->h_source, out_dev->dev_addr);
+ 	eth->h_proto = htons(ETH_P_IPV6);
+ 
+ 	/* add ip header */
+ 	ip6h = (struct ipv6hdr *)((char *)eth + sizeof(struct ethhdr));
+ 	ip6_flow_hdr(ip6h, tun_key->tos, 0);
+ 	/* the HW fills up ipv6 payload len */
+ 	ip6h->hop_limit   = ttl;
+ 	ip6h->daddr	  = fl6.daddr;
+ 	ip6h->saddr	  = fl6.saddr;
+ 
+ 	/* add tunneling protocol header */
+ 	err = mlx5e_gen_ip_tunnel_header((char *)ip6h + sizeof(struct ipv6hdr),
+ 					 &ip6h->nexthdr, e);
+ 	if (err)
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  		goto destroy_neigh_entry;
 +	}
  
  	e->encap_size = ipv6_encap_size;
  	e->encap_header = encap_header;
@@@ -2460,8 -2691,11 +2599,16 @@@
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_encap_alloc(priv->mdev, e->tunnel_type,
 +			       ipv6_encap_size, encap_header, &e->encap_id);
++=======
+ 	err = mlx5_packet_reformat_alloc(priv->mdev,
+ 					 e->reformat_type,
+ 					 ipv6_encap_size, encap_header,
+ 					 MLX5_FLOW_NAMESPACE_FDB,
+ 					 &e->encap_id);
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	if (err)
  		goto destroy_neigh_entry;
  
@@@ -2505,30 -2774,9 +2690,33 @@@ static int mlx5e_attach_encap(struct ml
  	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
  	struct ip_tunnel_key *key = &tun_info->key;
  	struct mlx5e_encap_entry *e;
- 	int tunnel_type, err = 0;
  	uintptr_t hash_key;
  	bool found = false;
++<<<<<<< HEAD
 +
 +	/* udp dst port must be set */
 +	if (!memchr_inv(&key->tp_dst, 0, sizeof(key->tp_dst)))
 +		goto vxlan_encap_offload_err;
 +
 +	/* setting udp src port isn't supported */
 +	if (memchr_inv(&key->tp_src, 0, sizeof(key->tp_src))) {
 +vxlan_encap_offload_err:
 +		netdev_warn(priv->netdev,
 +			    "must set udp dst port and not set udp src port\n");
 +		return -EOPNOTSUPP;
 +	}
 +
 +	if (mlx5e_vxlan_lookup_port(up_priv, be16_to_cpu(key->tp_dst)) &&
 +	    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap)) {
 +		tunnel_type = MLX5_HEADER_TYPE_VXLAN;
 +	} else {
 +		netdev_warn(priv->netdev,
 +			    "%d isn't an offloaded vxlan udp dport\n", be16_to_cpu(key->tp_dst));
 +		return -EOPNOTSUPP;
 +	}
++=======
+ 	int err = 0;
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  
  	hash_key = hash_encap_info(key);
  
@@@ -2764,46 -3094,175 +2955,202 @@@ int mlx5e_configure_flower(struct mlx5e
  	flow->flags = flow_flags;
  	flow->priv = priv;
  
++<<<<<<< HEAD
 +	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 +	if (err < 0)
 +		goto err_free;
 +
 +	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 +		err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
 +	} else {
 +		err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
 +		if (err < 0)
 +			goto err_free;
 +		flow->rule = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
++=======
+ 	*__flow = flow;
+ 	*__parse_attr = parse_attr;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct net_device *filter_dev,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_ESWITCH;
+ 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 	parse_attr->filter_dev = filter_dev;
+ 	flow->esw_attr->parse_attr = parse_attr;
+ 	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
+ 			       f, filter_dev);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->esw_attr->chain = f->common.chain_index;
+ 	flow->esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+ 	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	if (!(flow->esw_attr->action &
+ 	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT))
+ 		kvfree(parse_attr);
+ 
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_add_nic_flow(struct mlx5e_priv *priv,
+ 		   struct tc_cls_flower_offload *f,
+ 		   u16 flow_flags,
+ 		   struct net_device *filter_dev,
+ 		   struct mlx5e_tc_flow **__flow)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5e_tc_flow_parse_attr *parse_attr;
+ 	struct mlx5e_tc_flow *flow;
+ 	int attr_size, err;
+ 
+ 	/* multi-chain not supported for NIC rules */
+ 	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
+ 		return -EOPNOTSUPP;
+ 
+ 	flow_flags |= MLX5E_TC_FLOW_NIC;
+ 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
+ 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
+ 			       &parse_attr, &flow);
+ 	if (err)
+ 		goto out;
+ 
+ 	parse_attr->filter_dev = filter_dev;
+ 	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
+ 			       f, filter_dev);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
+ 	kvfree(parse_attr);
+ 	*__flow = flow;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	kfree(flow);
+ 	kvfree(parse_attr);
+ out:
+ 	return err;
+ }
+ 
+ static int
+ mlx5e_tc_add_flow(struct mlx5e_priv *priv,
+ 		  struct tc_cls_flower_offload *f,
+ 		  int flags,
+ 		  struct net_device *filter_dev,
+ 		  struct mlx5e_tc_flow **flow)
+ {
+ 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+ 	u16 flow_flags;
+ 	int err;
+ 
+ 	get_flags(flags, &flow_flags);
+ 
+ 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (esw && esw->mode == SRIOV_OFFLOADS)
+ 		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
+ 					 filter_dev, flow);
+ 	else
+ 		err = mlx5e_add_nic_flow(priv, f, flow_flags,
+ 					 filter_dev, flow);
+ 
+ 	return err;
+ }
+ 
+ int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
+ 			   struct tc_cls_flower_offload *f, int flags)
+ {
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct rhashtable *tc_ht = get_tc_ht(priv);
+ 	struct mlx5e_tc_flow *flow;
+ 	int err = 0;
+ 
+ 	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
+ 	if (flow) {
+ 		NL_SET_ERR_MSG_MOD(extack,
+ 				   "flow cookie already exists, ignoring");
+ 		netdev_warn_once(priv->netdev,
+ 				 "flow cookie %lx already exists, ignoring\n",
+ 				 f->cookie);
+ 		goto out;
++>>>>>>> 54c177ca9c6e (net/mlx5e: Branch according to classified tunnel type)
  	}
  
 -	err = mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
 -	if (err)
 -		goto out;
 +	if (IS_ERR(flow->rule)) {
 +		err = PTR_ERR(flow->rule);
 +		if (err != -EAGAIN)
 +			goto err_free;
 +	}
 +
 +	if (err != -EAGAIN)
 +		flow->flags |= MLX5E_TC_FLOW_OFFLOADED;
 +
 +	if (!(flow->flags & MLX5E_TC_FLOW_ESWITCH) ||
 +	    !(flow->esw_attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP))
 +		kvfree(parse_attr);
  
  	err = rhashtable_insert_fast(tc_ht, &flow->node, tc_ht_params);
 -	if (err)
 -		goto err_free;
 +	if (err) {
 +		mlx5e_tc_del_flow(priv, flow);
 +		kfree(flow);
 +	}
  
 -	return 0;
 +	return err;
  
  err_free:
 -	mlx5e_tc_del_flow(priv, flow);
 +	kvfree(parse_attr);
  	kfree(flow);
 -out:
  	return err;
  }
  
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
index c078c6703dc7..2c8798332c32 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
@@ -149,6 +149,8 @@ struct mlx5e_encap_entry {
 
 	struct net_device *out_dev;
 	int tunnel_type;
+	int tunnel_hlen;
+	int reformat_type;
 	u8 flags;
 	char *encap_header;
 	int encap_size;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
