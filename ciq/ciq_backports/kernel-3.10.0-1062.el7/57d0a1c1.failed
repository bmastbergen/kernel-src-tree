xdp: allow page_pool as an allocator type in xdp_return_frame

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit 57d0a1c1ac9e6a836bbab4698ba2a2e03f64bf1b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/57d0a1c1.failed

New allocator type MEM_TYPE_PAGE_POOL for page_pool usage.

The registered allocator page_pool pointer is not available directly
from xdp_rxq_info, but it could be (if needed).  For now, the driver
should keep separate track of the page_pool pointer, which it should
use for RX-ring page allocation.

As suggested by Saeed, to maintain a symmetric API it is the drivers
responsibility to allocate/create and free/destroy the page_pool.
Thus, after the driver have called xdp_rxq_info_unreg(), it is drivers
responsibility to free the page_pool, but with a RCU free call.  This
is done easily via the page_pool helper page_pool_destroy() (which
avoids touching any driver code during the RCU callback, which could
happen after the driver have been unloaded).

V8: address issues found by kbuild test robot
 - Address sparse should be static warnings
 - Allow xdp.o to be compiled without page_pool.o

V9: Remove inline from .c file, compiler knows best

	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 57d0a1c1ac9e6a836bbab4698ba2a2e03f64bf1b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/xdp.c
diff --cc net/core/xdp.c
index e553510efc2e,33e382afbd95..000000000000
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@@ -5,6 -5,10 +5,13 @@@
   */
  #include <linux/types.h>
  #include <linux/mm.h>
++<<<<<<< HEAD
++=======
+ #include <linux/slab.h>
+ #include <linux/idr.h>
+ #include <linux/rhashtable.h>
+ #include <net/page_pool.h>
++>>>>>>> 57d0a1c1ac9e (xdp: allow page_pool as an allocator type in xdp_return_frame)
  
  #include <net/xdp.h>
  
@@@ -13,9 -17,120 +20,110 @@@
  #define REG_STATE_UNREGISTERED	0x2
  #define REG_STATE_UNUSED	0x3
  
++<<<<<<< HEAD
++=======
+ static DEFINE_IDA(mem_id_pool);
+ static DEFINE_MUTEX(mem_id_lock);
+ #define MEM_ID_MAX 0xFFFE
+ #define MEM_ID_MIN 1
+ static int mem_id_next = MEM_ID_MIN;
+ 
+ static bool mem_id_init; /* false */
+ static struct rhashtable *mem_id_ht;
+ 
+ struct xdp_mem_allocator {
+ 	struct xdp_mem_info mem;
+ 	union {
+ 		void *allocator;
+ 		struct page_pool *page_pool;
+ 	};
+ 	struct rhash_head node;
+ 	struct rcu_head rcu;
+ };
+ 
+ static u32 xdp_mem_id_hashfn(const void *data, u32 len, u32 seed)
+ {
+ 	const u32 *k = data;
+ 	const u32 key = *k;
+ 
+ 	BUILD_BUG_ON(FIELD_SIZEOF(struct xdp_mem_allocator, mem.id)
+ 		     != sizeof(u32));
+ 
+ 	/* Use cyclic increasing ID as direct hash key, see rht_bucket_index */
+ 	return key << RHT_HASH_RESERVED_SPACE;
+ }
+ 
+ static int xdp_mem_id_cmp(struct rhashtable_compare_arg *arg,
+ 			  const void *ptr)
+ {
+ 	const struct xdp_mem_allocator *xa = ptr;
+ 	u32 mem_id = *(u32 *)arg->key;
+ 
+ 	return xa->mem.id != mem_id;
+ }
+ 
+ static const struct rhashtable_params mem_id_rht_params = {
+ 	.nelem_hint = 64,
+ 	.head_offset = offsetof(struct xdp_mem_allocator, node),
+ 	.key_offset  = offsetof(struct xdp_mem_allocator, mem.id),
+ 	.key_len = FIELD_SIZEOF(struct xdp_mem_allocator, mem.id),
+ 	.max_size = MEM_ID_MAX,
+ 	.min_size = 8,
+ 	.automatic_shrinking = true,
+ 	.hashfn    = xdp_mem_id_hashfn,
+ 	.obj_cmpfn = xdp_mem_id_cmp,
+ };
+ 
+ static void __xdp_mem_allocator_rcu_free(struct rcu_head *rcu)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 
+ 	xa = container_of(rcu, struct xdp_mem_allocator, rcu);
+ 
+ 	/* Allow this ID to be reused */
+ 	ida_simple_remove(&mem_id_pool, xa->mem.id);
+ 
+ 	/* Notice, driver is expected to free the *allocator,
+ 	 * e.g. page_pool, and MUST also use RCU free.
+ 	 */
+ 
+ 	/* Poison memory */
+ 	xa->mem.id = 0xFFFF;
+ 	xa->mem.type = 0xF0F0;
+ 	xa->allocator = (void *)0xDEAD9001;
+ 
+ 	kfree(xa);
+ }
+ 
+ static void __xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 	int id = xdp_rxq->mem.id;
+ 	int err;
+ 
+ 	if (id == 0)
+ 		return;
+ 
+ 	mutex_lock(&mem_id_lock);
+ 
+ 	xa = rhashtable_lookup(mem_id_ht, &id, mem_id_rht_params);
+ 	if (!xa) {
+ 		mutex_unlock(&mem_id_lock);
+ 		return;
+ 	}
+ 
+ 	err = rhashtable_remove_fast(mem_id_ht, &xa->node, mem_id_rht_params);
+ 	WARN_ON(err);
+ 
+ 	call_rcu(&xa->rcu, __xdp_mem_allocator_rcu_free);
+ 
+ 	mutex_unlock(&mem_id_lock);
+ }
+ 
++>>>>>>> 57d0a1c1ac9e (xdp: allow page_pool as an allocator type in xdp_return_frame)
  void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq)
  {
 -	/* Simplify driver cleanup code paths, allow unreg "unused" */
 -	if (xdp_rxq->reg_state == REG_STATE_UNUSED)
 -		return;
 -
 -	WARN(!(xdp_rxq->reg_state == REG_STATE_REGISTERED), "Driver BUG");
 -
 -	__xdp_rxq_info_unreg_mem_model(xdp_rxq);
 -
 -	xdp_rxq->reg_state = REG_STATE_UNREGISTERED;
 -	xdp_rxq->dev = NULL;
 -
 -	/* Reset mem info to defaults */
 -	xdp_rxq->mem.id = 0;
 -	xdp_rxq->mem.type = 0;
 +	return;
  }
  EXPORT_SYMBOL_GPL(xdp_rxq_info_unreg);
  
@@@ -39,9 -180,161 +147,166 @@@ bool xdp_rxq_info_is_reg(struct xdp_rxq
  }
  EXPORT_SYMBOL_GPL(xdp_rxq_info_is_reg);
  
++<<<<<<< HEAD
++int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
++			       enum xdp_mem_type type, void *allocator)
++{
++=======
+ static int __mem_id_init_hash_table(void)
+ {
+ 	struct rhashtable *rht;
+ 	int ret;
+ 
+ 	if (unlikely(mem_id_init))
+ 		return 0;
+ 
+ 	rht = kzalloc(sizeof(*rht), GFP_KERNEL);
+ 	if (!rht)
+ 		return -ENOMEM;
+ 
+ 	ret = rhashtable_init(rht, &mem_id_rht_params);
+ 	if (ret < 0) {
+ 		kfree(rht);
+ 		return ret;
+ 	}
+ 	mem_id_ht = rht;
+ 	smp_mb(); /* mutex lock should provide enough pairing */
+ 	mem_id_init = true;
+ 
+ 	return 0;
+ }
+ 
+ /* Allocate a cyclic ID that maps to allocator pointer.
+  * See: https://www.kernel.org/doc/html/latest/core-api/idr.html
+  *
+  * Caller must lock mem_id_lock.
+  */
+ static int __mem_id_cyclic_get(gfp_t gfp)
+ {
+ 	int retries = 1;
+ 	int id;
+ 
+ again:
+ 	id = ida_simple_get(&mem_id_pool, mem_id_next, MEM_ID_MAX, gfp);
+ 	if (id < 0) {
+ 		if (id == -ENOSPC) {
+ 			/* Cyclic allocator, reset next id */
+ 			if (retries--) {
+ 				mem_id_next = MEM_ID_MIN;
+ 				goto again;
+ 			}
+ 		}
+ 		return id; /* errno */
+ 	}
+ 	mem_id_next = id + 1;
+ 
+ 	return id;
+ }
+ 
+ static bool __is_supported_mem_type(enum xdp_mem_type type)
+ {
+ 	if (type == MEM_TYPE_PAGE_POOL)
+ 		return is_page_pool_compiled_in();
+ 
+ 	if (type >= MEM_TYPE_MAX)
+ 		return false;
+ 
+ 	return true;
+ }
+ 
  int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
  			       enum xdp_mem_type type, void *allocator)
  {
+ 	struct xdp_mem_allocator *xdp_alloc;
+ 	gfp_t gfp = GFP_KERNEL;
+ 	int id, errno, ret;
+ 	void *ptr;
+ 
+ 	if (xdp_rxq->reg_state != REG_STATE_REGISTERED) {
+ 		WARN(1, "Missing register, driver bug");
+ 		return -EFAULT;
+ 	}
+ 
+ 	if (!__is_supported_mem_type(type))
+ 		return -EOPNOTSUPP;
+ 
+ 	xdp_rxq->mem.type = type;
+ 
+ 	if (!allocator) {
+ 		if (type == MEM_TYPE_PAGE_POOL)
+ 			return -EINVAL; /* Setup time check page_pool req */
+ 		return 0;
+ 	}
+ 
+ 	/* Delay init of rhashtable to save memory if feature isn't used */
+ 	if (!mem_id_init) {
+ 		mutex_lock(&mem_id_lock);
+ 		ret = __mem_id_init_hash_table();
+ 		mutex_unlock(&mem_id_lock);
+ 		if (ret < 0) {
+ 			WARN_ON(1);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	xdp_alloc = kzalloc(sizeof(*xdp_alloc), gfp);
+ 	if (!xdp_alloc)
+ 		return -ENOMEM;
+ 
+ 	mutex_lock(&mem_id_lock);
+ 	id = __mem_id_cyclic_get(gfp);
+ 	if (id < 0) {
+ 		errno = id;
+ 		goto err;
+ 	}
+ 	xdp_rxq->mem.id = id;
+ 	xdp_alloc->mem  = xdp_rxq->mem;
+ 	xdp_alloc->allocator = allocator;
+ 
+ 	/* Insert allocator into ID lookup table */
+ 	ptr = rhashtable_insert_slow(mem_id_ht, &id, &xdp_alloc->node);
+ 	if (IS_ERR(ptr)) {
+ 		errno = PTR_ERR(ptr);
+ 		goto err;
+ 	}
+ 
+ 	mutex_unlock(&mem_id_lock);
+ 
++>>>>>>> 57d0a1c1ac9e (xdp: allow page_pool as an allocator type in xdp_return_frame)
  	return 0;
 -err:
 -	mutex_unlock(&mem_id_lock);
 -	kfree(xdp_alloc);
 -	return errno;
  }
  EXPORT_SYMBOL_GPL(xdp_rxq_info_reg_mem_model);
++<<<<<<< HEAD
++=======
+ 
+ void xdp_return_frame(void *data, struct xdp_mem_info *mem)
+ {
+ 	struct xdp_mem_allocator *xa;
+ 	struct page *page;
+ 
+ 	switch (mem->type) {
+ 	case MEM_TYPE_PAGE_POOL:
+ 		rcu_read_lock();
+ 		/* mem->id is valid, checked in xdp_rxq_info_reg_mem_model() */
+ 		xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);
+ 		page = virt_to_head_page(data);
+ 		if (xa)
+ 			page_pool_put_page(xa->page_pool, page);
+ 		else
+ 			put_page(page);
+ 		rcu_read_unlock();
+ 		break;
+ 	case MEM_TYPE_PAGE_SHARED:
+ 		page_frag_free(data);
+ 		break;
+ 	case MEM_TYPE_PAGE_ORDER0:
+ 		page = virt_to_page(data); /* Assumes order0 page*/
+ 		put_page(page);
+ 		break;
+ 	default:
+ 		/* Not possible, checked in xdp_rxq_info_reg_mem_model() */
+ 		break;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(xdp_return_frame);
++>>>>>>> 57d0a1c1ac9e (xdp: allow page_pool as an allocator type in xdp_return_frame)
diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 1fe77db59518..c79087153148 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -117,7 +117,12 @@ void __page_pool_put_page(struct page_pool *pool,
 
 static inline void page_pool_put_page(struct page_pool *pool, struct page *page)
 {
+	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
+	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
+	 */
+#ifdef CONFIG_PAGE_POOL
 	__page_pool_put_page(pool, page, false);
+#endif
 }
 /* Very limited use-cases allow recycle direct */
 static inline void page_pool_recycle_direct(struct page_pool *pool,
@@ -126,4 +131,13 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
+static inline bool is_page_pool_compiled_in(void)
+{
+#ifdef CONFIG_PAGE_POOL
+	return true;
+#else
+	return false;
+#endif
+}
+
 #endif /* _NET_PAGE_POOL_H */
diff --git a/include/net/xdp.h b/include/net/xdp.h
index 6ac69520ed7c..b5482373ef53 100644
--- a/include/net/xdp.h
+++ b/include/net/xdp.h
@@ -36,6 +36,7 @@
 enum xdp_mem_type {
 	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
 	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
+	MEM_TYPE_PAGE_POOL,
 	MEM_TYPE_MAX,
 };
 
@@ -43,6 +44,8 @@ struct xdp_mem_info {
 	u32 type; /* enum xdp_mem_type, but known size type */
 };
 
+struct page_pool;
+
 struct xdp_rxq_info {
 	struct net_device *dev;
 	u32 queue_index;
* Unmerged path net/core/xdp.c
