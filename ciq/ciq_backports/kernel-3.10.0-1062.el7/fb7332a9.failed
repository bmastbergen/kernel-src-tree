mmu_gather: move minimal range calculations into generic code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Will Deacon <will.deacon@arm.com>
commit fb7332a9fedfd62b1ba6530c86f39f0fa38afd49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/fb7332a9.failed

On architectures with hardware broadcasting of TLB invalidation messages
, it makes sense to reduce the range of the mmu_gather structure when
unmapping page ranges based on the dirty address information passed to
tlb_remove_tlb_entry.

arm64 already does this by directly manipulating the start/end fields
of the gather structure, but this confuses the generic code which
does not expect these fields to change and can end up calculating
invalid, negative ranges when forcing a flush in zap_pte_range.

This patch moves the minimal range calculation out of the arm64 code
and into the generic implementation, simplifying zap_pte_range in the
process (which no longer needs to care about start/end, since they will
point to the appropriate ranges already). With the range being tracked
by core code, the need_flush flag is dropped in favour of checking that
the end of the range has actually been set.

	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
	Cc: Michal Simek <monstr@monstr.eu>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit fb7332a9fedfd62b1ba6530c86f39f0fa38afd49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/tlb.h
#	mm/memory.c
diff --cc arch/arm64/include/asm/tlb.h
index 5546653e5cc8,c028fe37456f..000000000000
--- a/arch/arm64/include/asm/tlb.h
+++ b/arch/arm64/include/asm/tlb.h
@@@ -22,172 -22,50 +22,205 @@@
  #include <linux/pagemap.h>
  #include <linux/swap.h>
  
 -#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +#include <asm/pgalloc.h>
 +#include <asm/tlbflush.h>
  
 -#define tlb_remove_entry(tlb, entry)	tlb_remove_table(tlb, entry)
 -static inline void __tlb_remove_table(void *_table)
 -{
 -	free_page_and_swap_cache((struct page *)_table);
 -}
 -#else
 -#define tlb_remove_entry(tlb, entry)	tlb_remove_page(tlb, entry)
 -#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 +#define MMU_GATHER_BUNDLE	8
  
++<<<<<<< HEAD
 +/*
 + * TLB handling.  This allows us to remove pages from the page
 + * tables, and efficiently handle the TLB issues.
 + */
 +struct mmu_gather {
 +	struct mm_struct	*mm;
 +	unsigned int		fullmm;
 +	struct vm_area_struct	*vma;
 +	unsigned long		start, end;
 +	unsigned long		range_start;
 +	unsigned long		range_end;
 +	unsigned int		nr;
 +	unsigned int		max;
 +	struct page		**pages;
 +	struct page		*local[MMU_GATHER_BUNDLE];
 +};
 +
 +/*
 + * This is unnecessarily complex.  There's three ways the TLB shootdown
 + * code is used:
 + *  1. Unmapping a range of vmas.  See zap_page_range(), unmap_region().
 + *     tlb->fullmm = 0, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.
 + *  2. Unmapping all vmas.  See exit_mmap().
 + *     tlb->fullmm = 1, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.  Additionally, page tables will be freed.
 + *  3. Unmapping argument pages.  See shift_arg_pages().
 + *     tlb->fullmm = 0, but tlb_start_vma/tlb_end_vma will not be called.
 + *     tlb->vma will be NULL.
 + */
++=======
+ #include <asm-generic/tlb.h>
+ 
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  static inline void tlb_flush(struct mmu_gather *tlb)
  {
 -	if (tlb->fullmm) {
 +	if (tlb->fullmm || !tlb->vma)
  		flush_tlb_mm(tlb->mm);
++<<<<<<< HEAD
 +	else if (tlb->range_end > 0) {
 +		flush_tlb_range(tlb->vma, tlb->range_start, tlb->range_end);
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void tlb_add_flush(struct mmu_gather *tlb, unsigned long addr)
 +{
 +	if (!tlb->fullmm) {
 +		if (addr < tlb->range_start)
 +			tlb->range_start = addr;
 +		if (addr + PAGE_SIZE > tlb->range_end)
 +			tlb->range_end = addr + PAGE_SIZE;
 +	}
 +}
 +
 +static inline void __tlb_alloc_page(struct mmu_gather *tlb)
 +{
 +	unsigned long addr = __get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
 +
 +	if (addr) {
 +		tlb->pages = (void *)addr;
 +		tlb->max = PAGE_SIZE / sizeof(struct page *);
 +	}
 +}
 +
 +static inline void tlb_flush_mmu(struct mmu_gather *tlb)
 +{
 +	tlb_flush(tlb);
 +	free_pages_and_swap_cache(tlb->pages, tlb->nr);
 +	tlb->nr = 0;
 +	if (tlb->pages == tlb->local)
 +		__tlb_alloc_page(tlb);
 +}
 +
 +static inline void
 +tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
 +{
 +	tlb->mm = mm;
 +	tlb->fullmm = !(start | (end+1));
 +	tlb->start = start;
 +	tlb->end = end;
 +	tlb->vma = NULL;
 +	tlb->max = ARRAY_SIZE(tlb->local);
 +	tlb->pages = tlb->local;
 +	tlb->nr = 0;
 +	__tlb_alloc_page(tlb);
 +}
 +
 +static inline void
 +tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 +{
 +	tlb_flush_mmu(tlb);
 +
 +	/* keep the page table cache within bounds */
 +	check_pgt_cache();
 +
 +	if (tlb->pages != tlb->local)
 +		free_pages((unsigned long)tlb->pages, 0);
 +}
 +
 +/*
 + * Memorize the range for the TLB flush.
 + */
 +static inline void
 +tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep, unsigned long addr)
 +{
 +	tlb_add_flush(tlb, addr);
 +}
 +
 +/*
 + * In the case of tlb vma handling, we can optimise these away in the
 + * case where we're doing a full MM flush.  When we're doing a munmap,
 + * the vmas are adjusted to only cover the region to be torn down.
 + */
 +static inline void
 +tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm) {
 +		tlb->vma = vma;
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void
 +tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm)
 +		tlb_flush(tlb);
 +}
 +
 +static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	tlb->pages[tlb->nr++] = page;
 +	VM_BUG_ON(tlb->nr > tlb->max);
 +	return tlb->max - tlb->nr;
 +}
 +
 +static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	if (!__tlb_remove_page(tlb, page))
 +		tlb_flush_mmu(tlb);
 +}
 +
++=======
+ 	} else {
+ 		struct vm_area_struct vma = { .vm_mm = tlb->mm, };
+ 		flush_tlb_range(&vma, tlb->start, tlb->end);
+ 	}
+ }
+ 
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
 -				  unsigned long addr)
 +	unsigned long addr)
  {
  	pgtable_page_dtor(pte);
++<<<<<<< HEAD
 +	tlb_add_flush(tlb, addr);
 +	tlb_remove_page(tlb, pte);
++=======
+ 	tlb_remove_entry(tlb, pte);
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  }
  
 -#if CONFIG_ARM64_PGTABLE_LEVELS > 2
 +#ifndef CONFIG_ARM64_64K_PAGES
  static inline void __pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmdp,
  				  unsigned long addr)
  {
++<<<<<<< HEAD
 +	tlb_add_flush(tlb, addr);
 +	tlb_remove_page(tlb, virt_to_page(pmdp));
 +}
 +#endif
 +
 +#define pte_free_tlb(tlb, ptep, addr)	__pte_free_tlb(tlb, ptep, addr)
 +#define pmd_free_tlb(tlb, pmdp, addr)	__pmd_free_tlb(tlb, pmdp, addr)
 +#define pud_free_tlb(tlb, pudp, addr)	pud_free((tlb)->mm, pudp)
 +
 +#define tlb_migrate_finish(mm)		do { } while (0)
 +
++=======
+ 	tlb_remove_entry(tlb, virt_to_page(pmdp));
+ }
+ #endif
+ 
+ #if CONFIG_ARM64_PGTABLE_LEVELS > 3
+ static inline void __pud_free_tlb(struct mmu_gather *tlb, pud_t *pudp,
+ 				  unsigned long addr)
+ {
+ 	tlb_remove_entry(tlb, virt_to_page(pudp));
+ }
+ #endif
+ 
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  #endif
diff --cc mm/memory.c
index f33fb19b1965,c71edae9ba44..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -245,20 -229,25 +242,37 @@@ void tlb_gather_mmu(struct mmu_gather *
  #ifdef CONFIG_HAVE_RCU_TABLE_FREE
  	tlb->batch = NULL;
  #endif
+ 
+ 	__tlb_reset_range(tlb);
  }
  
 -static void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
 +void tlb_flush_mmu(struct mmu_gather *tlb)
  {
++<<<<<<< HEAD
 +	struct mmu_gather_batch *batch;
 +
 +	if (!tlb->need_flush)
 +		return;
 +	tlb->need_flush = 0;
++=======
+ 	if (!tlb->end)
+ 		return;
+ 
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  	tlb_flush(tlb);
 +	mmu_notifier_invalidate_range(tlb->mm, tlb->start, tlb->end);
  #ifdef CONFIG_HAVE_RCU_TABLE_FREE
  	tlb_table_flush(tlb);
  #endif
++<<<<<<< HEAD
++=======
+ 	__tlb_reset_range(tlb);
+ }
+ 
+ static void tlb_flush_mmu_free(struct mmu_gather *tlb)
+ {
+ 	struct mmu_gather_batch *batch;
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  
  	for (batch = &tlb->local; batch; batch = batch->next) {
  		free_pages_and_swap_cache(batch->pages, batch->nr);
@@@ -267,6 -256,12 +281,15 @@@
  	tlb->active = &tlb->local;
  }
  
++<<<<<<< HEAD
++=======
+ void tlb_flush_mmu(struct mmu_gather *tlb)
+ {
+ 	tlb_flush_mmu_tlbonly(tlb);
+ 	tlb_flush_mmu_free(tlb);
+ }
+ 
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  /* tlb_finish_mmu
   *	Called at the end of the shootdown operation to free up any resources
   *	that were required.
@@@ -1248,30 -1181,21 +1269,47 @@@ again
  
  	add_mm_rss_vec(mm, rss);
  	arch_leave_lazy_mmu_mode();
 +	pte_unmap_unlock(start_pte, ptl);
 +
++<<<<<<< HEAD
 +	/*
 +	 * mmu_gather ran out of room to batch pages, we break out of
 +	 * the PTE lock to avoid doing the potential expensive TLB invalidate
 +	 * and page-free while holding it.
 +	 */
 +	if (force_flush) {
 +		unsigned long old_end;
 +
 +		force_flush = 0;
 +
 +		/*
 +		 * Flush the TLB just for the previous segment,
 +		 * then update the range to be the remaining
 +		 * TLB range.
 +		 */
 +		old_end = tlb->end;
 +		tlb->end = addr;
  
 +		tlb_flush_mmu(tlb);
 +
 +		tlb->start = addr;
 +		tlb->end = old_end;
++=======
+ 	/* Do the actual TLB flush before dropping ptl */
+ 	if (force_flush)
+ 		tlb_flush_mmu_tlbonly(tlb);
+ 	pte_unmap_unlock(start_pte, ptl);
+ 
+ 	/*
+ 	 * If we forced a TLB flush (either due to running out of
+ 	 * batch buffers or because we needed to flush dirty TLB
+ 	 * entries before releasing the ptl), free the batched
+ 	 * memory too. Restart if we didn't do everything.
+ 	 */
+ 	if (force_flush) {
+ 		force_flush = 0;
+ 		tlb_flush_mmu_free(tlb);
++>>>>>>> fb7332a9fedf (mmu_gather: move minimal range calculations into generic code)
  
  		if (addr != end)
  			goto again;
* Unmerged path arch/arm64/include/asm/tlb.h
diff --git a/arch/microblaze/include/asm/tlb.h b/arch/microblaze/include/asm/tlb.h
index 8aa97817cc8c..99b6ded54849 100644
--- a/arch/microblaze/include/asm/tlb.h
+++ b/arch/microblaze/include/asm/tlb.h
@@ -14,7 +14,6 @@
 #define tlb_flush(tlb)	flush_tlb_mm((tlb)->mm)
 
 #include <linux/pagemap.h>
-#include <asm-generic/tlb.h>
 
 #ifdef CONFIG_MMU
 #define tlb_start_vma(tlb, vma)		do { } while (0)
@@ -22,4 +21,6 @@
 #define __tlb_remove_tlb_entry(tlb, pte, address) do { } while (0)
 #endif
 
+#include <asm-generic/tlb.h>
+
 #endif /* _ASM_MICROBLAZE_TLB_H */
diff --git a/arch/powerpc/include/asm/pgalloc.h b/arch/powerpc/include/asm/pgalloc.h
index e9a9f60e596d..fc3ee06eab87 100644
--- a/arch/powerpc/include/asm/pgalloc.h
+++ b/arch/powerpc/include/asm/pgalloc.h
@@ -3,7 +3,6 @@
 #ifdef __KERNEL__
 
 #include <linux/mm.h>
-#include <asm-generic/tlb.h>
 
 #ifdef CONFIG_PPC_BOOK3E
 extern void tlb_flush_pgtable(struct mmu_gather *tlb, unsigned long address);
@@ -14,6 +13,8 @@ static inline void tlb_flush_pgtable(struct mmu_gather *tlb,
 }
 #endif /* !CONFIG_PPC_BOOK3E */
 
+extern void tlb_remove_table(struct mmu_gather *tlb, void *table);
+
 #ifdef CONFIG_PPC64
 #include <asm/pgalloc-64.h>
 #else
diff --git a/arch/powerpc/include/asm/tlb.h b/arch/powerpc/include/asm/tlb.h
index e2b428b0f7ba..20733fa518ae 100644
--- a/arch/powerpc/include/asm/tlb.h
+++ b/arch/powerpc/include/asm/tlb.h
@@ -27,6 +27,7 @@
 
 #define tlb_start_vma(tlb, vma)	do { } while (0)
 #define tlb_end_vma(tlb, vma)	do { } while (0)
+#define __tlb_remove_tlb_entry	__tlb_remove_tlb_entry
 
 extern void tlb_flush(struct mmu_gather *tlb);
 
diff --git a/arch/powerpc/mm/hugetlbpage.c b/arch/powerpc/mm/hugetlbpage.c
index 1869df64301d..5adaa6605afe 100644
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@ -519,8 +519,6 @@ static void free_hugepd_range(struct mmu_gather *tlb, hugepd_t *hpdp, int pdshif
 	for (i = 0; i < num_hugepd; i++, hpdp++)
 		hpdp->pd = 0;
 
-	tlb->need_flush = 1;
-
 #ifdef CONFIG_PPC_FSL_BOOK3E
 	hugepd_free(tlb, hugepte);
 #else
diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h
index c3b25d4ce7d7..06837322d4e4 100644
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@ -96,10 +96,9 @@ struct mmu_gather {
 #endif
 	unsigned long		start;
 	unsigned long		end;
-	unsigned int		need_flush : 1,	/* Did free PTEs */
 	/* we are in the middle of an operation to clear
 	 * a full mm and can make some optimizations */
-				fullmm : 1,
+	unsigned int		fullmm : 1,
 	/* we have performed an operation which
 	 * requires a complete flush of the tlb */
 				need_flush_all : 1;
@@ -128,16 +127,54 @@ static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 		tlb_flush_mmu(tlb);
 }
 
+static inline void __tlb_adjust_range(struct mmu_gather *tlb,
+				      unsigned long address)
+{
+	tlb->start = min(tlb->start, address);
+	tlb->end = max(tlb->end, address + PAGE_SIZE);
+}
+
+static inline void __tlb_reset_range(struct mmu_gather *tlb)
+{
+	tlb->start = TASK_SIZE;
+	tlb->end = 0;
+}
+
+/*
+ * In the case of tlb vma handling, we can optimise these away in the
+ * case where we're doing a full MM flush.  When we're doing a munmap,
+ * the vmas are adjusted to only cover the region to be torn down.
+ */
+#ifndef tlb_start_vma
+#define tlb_start_vma(tlb, vma) do { } while (0)
+#endif
+
+#define __tlb_end_vma(tlb, vma)					\
+	do {							\
+		if (!tlb->fullmm && tlb->end) {			\
+			tlb_flush(tlb);				\
+			__tlb_reset_range(tlb);			\
+		}						\
+	} while (0)
+
+#ifndef tlb_end_vma
+#define tlb_end_vma	__tlb_end_vma
+#endif
+
+#ifndef __tlb_remove_tlb_entry
+#define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)
+#endif
+
 /**
  * tlb_remove_tlb_entry - remember a pte unmapping for later tlb invalidation.
  *
- * Record the fact that pte's were really umapped in ->need_flush, so we can
- * later optimise away the tlb invalidate.   This helps when userspace is
- * unmapping already-unmapped pages, which happens quite a lot.
+ * Record the fact that pte's were really unmapped by updating the range,
+ * so we can later optimise away the tlb invalidate.   This helps when
+ * userspace is unmapping already-unmapped pages, which happens quite a lot.
  */
 #define tlb_remove_tlb_entry(tlb, ptep, address)		\
 	do {							\
-		tlb->need_flush = 1;				\
+		__tlb_adjust_range(tlb, address);		\
 		__tlb_remove_tlb_entry(tlb, ptep, address);	\
 	} while (0)
 
@@ -151,7 +188,7 @@ static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 
 #define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)		\
 	do {							\
-		tlb->need_flush = 1;				\
+		__tlb_adjust_range(tlb, address);		\
 		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);	\
 	} while (0)
 
@@ -171,21 +208,21 @@ static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 
 #define pte_free_tlb(tlb, ptep, address)			\
 	do {							\
-		tlb->need_flush = 1;				\
+		__tlb_adjust_range(tlb, address);		\
 		__pte_free_tlb(tlb, ptep, address);		\
 	} while (0)
 
 #ifndef __ARCH_HAS_4LEVEL_HACK
 #define pud_free_tlb(tlb, pudp, address)			\
 	do {							\
-		tlb->need_flush = 1;				\
+		__tlb_adjust_range(tlb, address);		\
 		__pud_free_tlb(tlb, pudp, address);		\
 	} while (0)
 #endif
 
 #define pmd_free_tlb(tlb, pmdp, address)			\
 	do {							\
-		tlb->need_flush = 1;				\
+		__tlb_adjust_range(tlb, address);		\
 		__pmd_free_tlb(tlb, pmdp, address);		\
 	} while (0)
 
* Unmerged path mm/memory.c
