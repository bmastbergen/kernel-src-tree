kvm: x86: Use fast CR3 switch for nested VMX

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Junaid Shahid <junaids@google.com>
commit 50c28f21d045dde8c52548f8482d456b3f0956f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/50c28f21.failed

Use the fast CR3 switch mechanism to locklessly change the MMU root
page when switching between L1 and L2. The switch from L2 to L1 should
always go through the fast path, while the switch from L1 to L2 should
go through the fast path if L1's CR3/EPTP for L2 hasn't changed
since the last time.

	Signed-off-by: Junaid Shahid <junaids@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 50c28f21d045dde8c52548f8482d456b3f0956f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index db86a346eaea,9b73cfcef917..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3914,10 -4036,62 +3914,65 @@@ static void nonpaging_init_context(stru
  	context->nx = false;
  }
  
 -static bool fast_cr3_switch(struct kvm_vcpu *vcpu, gpa_t new_cr3,
 -			    union kvm_mmu_page_role new_role)
 +void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	mmu_free_roots(vcpu);
++=======
+ 	struct kvm_mmu *mmu = &vcpu->arch.mmu;
+ 
+ 	/*
+ 	 * For now, limit the fast switch to 64-bit hosts+VMs in order to avoid
+ 	 * having to deal with PDPTEs. We may add support for 32-bit hosts/VMs
+ 	 * later if necessary.
+ 	 */
+ 	if (mmu->shadow_root_level >= PT64_ROOT_4LEVEL &&
+ 	    mmu->root_level >= PT64_ROOT_4LEVEL) {
+ 		gpa_t prev_cr3 = mmu->prev_root.cr3;
+ 
+ 		if (mmu_check_root(vcpu, new_cr3 >> PAGE_SHIFT))
+ 			return false;
+ 
+ 		swap(mmu->root_hpa, mmu->prev_root.hpa);
+ 		mmu->prev_root.cr3 = mmu->get_cr3(vcpu);
+ 
+ 		if (new_cr3 == prev_cr3 &&
+ 		    VALID_PAGE(mmu->root_hpa) &&
+ 		    page_header(mmu->root_hpa) != NULL &&
+ 		    new_role.word == page_header(mmu->root_hpa)->role.word) {
+ 			/*
+ 			 * It is possible that the cached previous root page is
+ 			 * obsolete because of a change in the MMU
+ 			 * generation number. However, that is accompanied by
+ 			 * KVM_REQ_MMU_RELOAD, which will free the root that we
+ 			 * have set here and allocate a new one.
+ 			 */
+ 
+ 			kvm_make_request(KVM_REQ_LOAD_CR3, vcpu);
+ 			kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
+ 			__clear_sp_write_flooding_count(
+ 				page_header(mmu->root_hpa));
+ 
+ 			return true;
+ 		}
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void __kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3,
+ 			      union kvm_mmu_page_role new_role)
+ {
+ 	if (!fast_cr3_switch(vcpu, new_cr3, new_role))
+ 		kvm_mmu_free_roots(vcpu, false);
+ }
+ 
+ void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3)
+ {
+ 	__kvm_mmu_new_cr3(vcpu, new_cr3, kvm_mmu_calc_root_page_role(vcpu));
++>>>>>>> 50c28f21d045 (kvm: x86: Use fast CR3 switch for nested VMX)
  }
+ EXPORT_SYMBOL_GPL(kvm_mmu_new_cr3);
  
  static unsigned long get_cr3(struct kvm_vcpu *vcpu)
  {
@@@ -4497,14 -4711,29 +4552,19 @@@ void kvm_init_shadow_mmu(struct kvm_vcp
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
  
 -static union kvm_mmu_page_role
 -kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty)
 -{
 -	union kvm_mmu_page_role role = vcpu->arch.mmu.base_role;
 -
 -	role.level = PT64_ROOT_4LEVEL;
 -	role.direct = false;
 -	role.ad_disabled = !accessed_dirty;
 -	role.guest_mode = true;
 -	role.access = ACC_ALL;
 -
 -	return role;
 -}
 -
  void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
- 			     bool accessed_dirty)
+ 			     bool accessed_dirty, gpa_t new_eptp)
  {
  	struct kvm_mmu *context = &vcpu->arch.mmu;
 -	union kvm_mmu_page_role root_page_role =
 -		kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty);
  
++<<<<<<< HEAD
 +	MMU_WARN_ON(VALID_PAGE(context->root_hpa));
 +
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
++=======
+ 	__kvm_mmu_new_cr3(vcpu, new_eptp, root_page_role);
+ 	context->shadow_root_level = PT64_ROOT_4LEVEL;
++>>>>>>> 50c28f21d045 (kvm: x86: Use fast CR3 switch for nested VMX)
  
  	context->nx = true;
  	context->ept_ad = accessed_dirty;
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 3d1af6f75377..4d2e76908fa6 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -58,7 +58,7 @@ reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
 
 void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu);
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
-			     bool accessed_dirty);
+			     bool accessed_dirty, gpa_t new_eptp);
 bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu);
 
 static inline unsigned int kvm_mmu_available_pages(struct kvm *kvm)
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 5501a1957de2..95b12d6b5bbb 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -9710,11 +9710,11 @@ static int nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 	if (!valid_ept_address(vcpu, nested_ept_get_cr3(vcpu)))
 		return 1;
 
-	kvm_mmu_unload(vcpu);
 	kvm_init_shadow_ept_mmu(vcpu,
 			to_vmx(vcpu)->nested.nested_vmx_ept_caps &
 			VMX_EPT_EXECUTE_ONLY_BIT,
-			nested_ept_ad_enabled(vcpu));
+			nested_ept_ad_enabled(vcpu),
+			nested_ept_get_cr3(vcpu));
 	vcpu->arch.mmu.set_cr3           = vmx_set_cr3;
 	vcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;
 	vcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;
@@ -10251,12 +10251,16 @@ static int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3, bool ne
 				return 1;
 			}
 		}
-
-		vcpu->arch.cr3 = cr3;
-		__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
 	}
 
-	kvm_mmu_reset_context(vcpu);
+	if (!nested_ept)
+		kvm_mmu_new_cr3(vcpu, cr3);
+
+	vcpu->arch.cr3 = cr3;
+	__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
+
+	kvm_init_mmu(vcpu, false);
+
 	return 0;
 }
 
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 365126b76acd..f670f682c0ec 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2068,16 +2068,22 @@ static void shrink_halt_poll_ns(struct kvm_vcpu *vcpu)
 
 static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
 {
+	int ret = -EINTR;
+	int idx = srcu_read_lock(&vcpu->kvm->srcu);
+
 	if (kvm_arch_vcpu_runnable(vcpu)) {
 		kvm_make_request(KVM_REQ_UNHALT, vcpu);
-		return -EINTR;
+		goto out;
 	}
 	if (kvm_cpu_has_pending_timer(vcpu))
-		return -EINTR;
+		goto out;
 	if (signal_pending(current))
-		return -EINTR;
+		goto out;
 
-	return 0;
+	ret = 0;
+out:
+	srcu_read_unlock(&vcpu->kvm->srcu, idx);
+	return ret;
 }
 
 /*
