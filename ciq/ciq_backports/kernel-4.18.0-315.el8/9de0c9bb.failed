lockdep: Support deadlock detection for recursive read locks in check_noncircular()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Boqun Feng <boqun.feng@gmail.com>
commit 9de0c9bbcedf752e762c67f105bff342e30f9105
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/9de0c9bb.failed

Currently, lockdep only has limit support for deadlock detection for
recursive read locks.

This patch support deadlock detection for recursive read locks. The
basic idea is:

We are about to add dependency B -> A in to the dependency graph, we use
check_noncircular() to find whether we have a strong dependency path
A -> .. -> B so that we have a strong dependency circle (a closed strong
dependency path):

	 A -> .. -> B -> A

, which doesn't have two adjacent dependencies as -(*R)-> L -(S*)->.

Since A -> .. -> B is already a strong dependency path, so if either
B -> A is -(E*)-> or A -> .. -> B is -(*N)->, the circle A -> .. -> B ->
A is strong, otherwise not. So we introduce a new match function
hlock_conflict() to replace the class_equal() for the deadlock check in
check_noncircular().

	Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200807074238.1632519-10-boqun.feng@gmail.com
(cherry picked from commit 9de0c9bbcedf752e762c67f105bff342e30f9105)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index 3abcaf4f62ca,9160f1ddc435..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -1850,18 -1974,18 +1877,23 @@@ unsigned long lockdep_count_backward_de
  
  /*
   * Check that the dependency graph starting at <src> can lead to
 - * <target> or not.
 + * <target> or not. Print an error and return 0 if it does.
   */
++<<<<<<< HEAD
 +static noinline int
 +check_path(struct lock_class *target, struct lock_list *src_entry,
++=======
+ static noinline enum bfs_result
+ check_path(struct held_lock *target, struct lock_list *src_entry,
+ 	   bool (*match)(struct lock_list *entry, void *data),
++>>>>>>> 9de0c9bbcedf (lockdep: Support deadlock detection for recursive read locks in check_noncircular())
  	   struct lock_list **target_entry)
  {
 -	enum bfs_result ret;
 +	int ret;
  
- 	ret = __bfs_forwards(src_entry, (void *)target, class_equal,
- 			     target_entry);
+ 	ret = __bfs_forwards(src_entry, target, match, target_entry);
  
 -	if (unlikely(bfs_error(ret)))
 +	if (unlikely(ret < 0))
  		print_bfs_bug(ret);
  
  	return ret;
@@@ -1887,9 -2010,9 +1919,9 @@@ check_noncircular(struct held_lock *src
  
  	debug_atomic_inc(nr_cyclic_checks);
  
- 	ret = check_path(hlock_class(target), &src_entry, &target_entry);
+ 	ret = check_path(target, &src_entry, hlock_conflict, &target_entry);
  
 -	if (unlikely(ret == BFS_RMATCH)) {
 +	if (unlikely(!ret)) {
  		if (!*trace) {
  			/*
  			 * If save_trace fails here, the printing might
@@@ -1925,13 -2048,10 +1957,13 @@@ check_redundant(struct held_lock *src, 
  
  	debug_atomic_inc(nr_redundant_checks);
  
- 	ret = check_path(hlock_class(target), &src_entry, &target_entry);
+ 	ret = check_path(target, &src_entry, class_equal, &target_entry);
  
 -	if (ret == BFS_RMATCH)
 +	if (!ret) {
  		debug_atomic_inc(nr_redundant);
 +		ret = 2;
 +	} else if (ret < 0)
 +		ret = 0;
  
  	return ret;
  }
* Unmerged path kernel/locking/lockdep.c
