scsi: smartpqi: Add support for RAID5 and RAID6 writes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Don Brace <don.brace@microchip.com>
commit 6702d2c40f31b200d90614d1b0a841f14ba22ee0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/6702d2c4.failed

Add in new IU definition and implement support for RAID5 and RAID6 writes.

Link: https://lore.kernel.org/r/161549372734.25025.963261942897080281.stgit@brunhilda
	Reviewed-by: Scott Benesh <scott.benesh@microchip.com>
	Reviewed-by: Mike McGowen <mike.mcgowen@microchip.com>
	Reviewed-by: Scott Teel <scott.teel@microchip.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@microchip.com>
	Signed-off-by: Don Brace <don.brace@microchip.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 6702d2c40f31b200d90614d1b0a841f14ba22ee0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/smartpqi/smartpqi_init.c
diff --cc drivers/scsi/smartpqi/smartpqi_init.c
index 15d4619f5e0b,17b697022473..000000000000
--- a/drivers/scsi/smartpqi/smartpqi_init.c
+++ b/drivers/scsi/smartpqi/smartpqi_init.c
@@@ -2268,292 -2241,406 +2272,660 @@@ static inline void pqi_set_encryption_i
   * Attempt to perform RAID bypass mapping for a logical volume I/O.
   */
  
++<<<<<<< HEAD
 +#define PQI_RAID_BYPASS_INELIGIBLE	1
 +
++=======
+ static bool pqi_aio_raid_level_supported(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	bool is_supported = true;
+ 
+ 	switch (rmd->raid_level) {
+ 	case SA_RAID_0:
+ 		break;
+ 	case SA_RAID_1:
+ 		if (rmd->is_write)
+ 			is_supported = false;
+ 		break;
+ 	case SA_RAID_5:
+ 		if (rmd->is_write && !ctrl_info->enable_r5_writes)
+ 			is_supported = false;
+ 		break;
+ 	case SA_RAID_6:
+ 		if (rmd->is_write && !ctrl_info->enable_r6_writes)
+ 			is_supported = false;
+ 		break;
+ 	case SA_RAID_ADM:
+ 		if (rmd->is_write)
+ 			is_supported = false;
+ 		break;
+ 	default:
+ 		is_supported = false;
+ 	}
+ 
+ 	return is_supported;
+ }
+ 
+ #define PQI_RAID_BYPASS_INELIGIBLE	1
+ 
+ static int pqi_get_aio_lba_and_block_count(struct scsi_cmnd *scmd,
+ 			struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	/* Check for valid opcode, get LBA and block count. */
+ 	switch (scmd->cmnd[0]) {
+ 	case WRITE_6:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_6:
+ 		rmd->first_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |
+ 			(scmd->cmnd[2] << 8) | scmd->cmnd[3]);
+ 		rmd->block_cnt = (u32)scmd->cmnd[4];
+ 		if (rmd->block_cnt == 0)
+ 			rmd->block_cnt = 256;
+ 		break;
+ 	case WRITE_10:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_10:
+ 		rmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
+ 		rmd->block_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);
+ 		break;
+ 	case WRITE_12:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_12:
+ 		rmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
+ 		rmd->block_cnt = get_unaligned_be32(&scmd->cmnd[6]);
+ 		break;
+ 	case WRITE_16:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_16:
+ 		rmd->first_block = get_unaligned_be64(&scmd->cmnd[2]);
+ 		rmd->block_cnt = get_unaligned_be32(&scmd->cmnd[10]);
+ 		break;
+ 	default:
+ 		/* Process via normal I/O path. */
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 	}
+ 
+ 	put_unaligned_le32(scsi_bufflen(scmd), &rmd->data_length);
+ 
+ 	return 0;
+ }
+ 
+ static int pci_get_aio_common_raid_map_values(struct pqi_ctrl_info *ctrl_info,
+ 					struct pqi_scsi_dev_raid_map_data *rmd,
+ 					struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 
+ 	rmd->last_block = rmd->first_block + rmd->block_cnt - 1;
+ 
+ 	/* Check for invalid block or wraparound. */
+ 	if (rmd->last_block >=
+ 		get_unaligned_le64(&raid_map->volume_blk_cnt) ||
+ 		rmd->last_block < rmd->first_block)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	rmd->data_disks_per_row =
+ 			get_unaligned_le16(&raid_map->data_disks_per_row);
+ 	rmd->strip_size = get_unaligned_le16(&raid_map->strip_size);
+ 	rmd->layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
+ 
+ 	/* Calculate stripe information for the request. */
+ 	rmd->blocks_per_row = rmd->data_disks_per_row * rmd->strip_size;
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->first_row = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->last_row = tmpdiv;
+ 	rmd->first_row_offset = (u32)(rmd->first_block - (rmd->first_row * rmd->blocks_per_row));
+ 	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row * rmd->blocks_per_row));
+ 	tmpdiv = rmd->first_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->first_column = tmpdiv;
+ 	tmpdiv = rmd->last_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->last_column = tmpdiv;
+ #else
+ 	rmd->first_row = rmd->first_block / rmd->blocks_per_row;
+ 	rmd->last_row = rmd->last_block / rmd->blocks_per_row;
+ 	rmd->first_row_offset = (u32)(rmd->first_block -
+ 				(rmd->first_row * rmd->blocks_per_row));
+ 	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row *
+ 				rmd->blocks_per_row));
+ 	rmd->first_column = rmd->first_row_offset / rmd->strip_size;
+ 	rmd->last_column = rmd->last_row_offset / rmd->strip_size;
+ #endif
+ 
+ 	/* If this isn't a single row/column then give to the controller. */
+ 	if (rmd->first_row != rmd->last_row ||
+ 			rmd->first_column != rmd->last_column)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Proceeding with driver mapping. */
+ 	rmd->total_disks_per_row = rmd->data_disks_per_row +
+ 		get_unaligned_le16(&raid_map->metadata_disks_per_row);
+ 	rmd->map_row = ((u32)(rmd->first_row >>
+ 		raid_map->parity_rotation_shift)) %
+ 		get_unaligned_le16(&raid_map->row_cnt);
+ 	rmd->map_index = (rmd->map_row * rmd->total_disks_per_row) +
+ 			rmd->first_column;
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_calc_aio_raid_adm(struct pqi_scsi_dev_raid_map_data *rmd,
+ 				struct pqi_scsi_dev *device)
+ {
+ 	/* RAID ADM */
+ 	/*
+ 	 * Handles N-way mirrors  (R1-ADM) and R10 with # of drives
+ 	 * divisible by 3.
+ 	 */
+ 	rmd->offload_to_mirror = device->offload_to_mirror;
+ 
+ 	if (rmd->offload_to_mirror == 0)  {
+ 		/* use physical disk in the first mirrored group. */
+ 		rmd->map_index %= rmd->data_disks_per_row;
+ 	} else {
+ 		do {
+ 			/*
+ 			 * Determine mirror group that map_index
+ 			 * indicates.
+ 			 */
+ 			rmd->current_group =
+ 				rmd->map_index / rmd->data_disks_per_row;
+ 
+ 			if (rmd->offload_to_mirror !=
+ 					rmd->current_group) {
+ 				if (rmd->current_group <
+ 					rmd->layout_map_count - 1) {
+ 					/*
+ 					 * Select raid index from
+ 					 * next group.
+ 					 */
+ 					rmd->map_index += rmd->data_disks_per_row;
+ 					rmd->current_group++;
+ 				} else {
+ 					/*
+ 					 * Select raid index from first
+ 					 * group.
+ 					 */
+ 					rmd->map_index %= rmd->data_disks_per_row;
+ 					rmd->current_group = 0;
+ 				}
+ 			}
+ 		} while (rmd->offload_to_mirror != rmd->current_group);
+ 	}
+ 
+ 	/* Set mirror group to use next time. */
+ 	rmd->offload_to_mirror =
+ 		(rmd->offload_to_mirror >= rmd->layout_map_count - 1) ?
+ 			0 : rmd->offload_to_mirror + 1;
+ 	device->offload_to_mirror = rmd->offload_to_mirror;
+ 	/*
+ 	 * Avoid direct use of device->offload_to_mirror within this
+ 	 * function since multiple threads might simultaneously
+ 	 * increment it beyond the range of device->layout_map_count -1.
+ 	 */
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_calc_aio_r5_or_r6(struct pqi_scsi_dev_raid_map_data *rmd,
+ 				struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 	/* RAID 50/60 */
+ 	/* Verify first and last block are in same RAID group */
+ 	rmd->stripesize = rmd->blocks_per_row * rmd->layout_map_count;
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->first_group = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->last_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->last_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->last_group = tmpdiv;
+ #else
+ 	rmd->first_group = (rmd->first_block % rmd->stripesize) / rmd->blocks_per_row;
+ 	rmd->last_group = (rmd->last_block % rmd->stripesize) / rmd->blocks_per_row;
+ #endif
+ 	if (rmd->first_group != rmd->last_group)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single row of RAID 5/6 */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->first_row = tmpdiv;
+ 	rmd->r5or6_first_row = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->r5or6_last_row = tmpdiv;
+ #else
+ 	rmd->first_row = rmd->r5or6_first_row =
+ 		rmd->first_block / rmd->stripesize;
+ 	rmd->r5or6_last_row = rmd->last_block / rmd->stripesize;
+ #endif
+ 	if (rmd->r5or6_first_row != rmd->r5or6_last_row)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single column */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_row_offset;
+ 	rmd->first_row_offset = (u32)do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->r5or6_first_row_offset = rmd->first_row_offset;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->blocks_per_row);
+ 	tmpdiv = rmd->r5or6_first_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->first_column = rmd->r5or6_first_column = tmpdiv;
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->r5or6_last_column = tmpdiv;
+ #else
+ 	rmd->first_row_offset = rmd->r5or6_first_row_offset =
+ 		(u32)((rmd->first_block %
+ 				rmd->stripesize) %
+ 				rmd->blocks_per_row);
+ 
+ 	rmd->r5or6_last_row_offset =
+ 		(u32)((rmd->last_block % rmd->stripesize) %
+ 		rmd->blocks_per_row);
+ 
+ 	rmd->first_column =
+ 			rmd->r5or6_first_row_offset / rmd->strip_size;
+ 	rmd->r5or6_first_column = rmd->first_column;
+ 	rmd->r5or6_last_column = rmd->r5or6_last_row_offset / rmd->strip_size;
+ #endif
+ 	if (rmd->r5or6_first_column != rmd->r5or6_last_column)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Request is eligible */
+ 	rmd->map_row =
+ 		((u32)(rmd->first_row >> raid_map->parity_rotation_shift)) %
+ 		get_unaligned_le16(&raid_map->row_cnt);
+ 
+ 	rmd->map_index = (rmd->first_group *
+ 		(get_unaligned_le16(&raid_map->row_cnt) *
+ 		rmd->total_disks_per_row)) +
+ 		(rmd->map_row * rmd->total_disks_per_row) + rmd->first_column;
+ 
+ 	if (rmd->is_write) {
+ 		u32 index;
+ 
+ 		/*
+ 		 * p_parity_it_nexus and q_parity_it_nexus are pointers to the
+ 		 * parity entries inside the device's raid_map.
+ 		 *
+ 		 * A device's RAID map is bounded by: number of RAID disks squared.
+ 		 *
+ 		 * The devices RAID map size is checked during device
+ 		 * initialization.
+ 		 */
+ 		index = DIV_ROUND_UP(rmd->map_index + 1, rmd->total_disks_per_row);
+ 		index *= rmd->total_disks_per_row;
+ 		index -= get_unaligned_le16(&raid_map->metadata_disks_per_row);
+ 
+ 		rmd->p_parity_it_nexus = raid_map->disk_data[index].aio_handle;
+ 		if (rmd->raid_level == SA_RAID_6) {
+ 			rmd->q_parity_it_nexus = raid_map->disk_data[index + 1].aio_handle;
+ 			rmd->xor_mult = raid_map->disk_data[rmd->map_index].xor_mult[1];
+ 		}
+ 		if (rmd->blocks_per_row == 0)
+ 			return PQI_RAID_BYPASS_INELIGIBLE;
+ #if BITS_PER_LONG == 32
+ 		tmpdiv = rmd->first_block;
+ 		do_div(tmpdiv, rmd->blocks_per_row);
+ 		rmd->row = tmpdiv;
+ #else
+ 		rmd->row = rmd->first_block / rmd->blocks_per_row;
+ #endif
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void pqi_set_aio_cdb(struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	/* Build the new CDB for the physical disk I/O. */
+ 	if (rmd->disk_block > 0xffffffff) {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_16 : READ_16;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be64(rmd->disk_block, &rmd->cdb[2]);
+ 		put_unaligned_be32(rmd->disk_block_cnt, &rmd->cdb[10]);
+ 		rmd->cdb[14] = 0;
+ 		rmd->cdb[15] = 0;
+ 		rmd->cdb_length = 16;
+ 	} else {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_10 : READ_10;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be32((u32)rmd->disk_block, &rmd->cdb[2]);
+ 		rmd->cdb[6] = 0;
+ 		put_unaligned_be16((u16)rmd->disk_block_cnt, &rmd->cdb[7]);
+ 		rmd->cdb[9] = 0;
+ 		rmd->cdb_length = 10;
+ 	}
+ }
+ 
++>>>>>>> 6702d2c40f31 (scsi: smartpqi: Add support for RAID5 and RAID6 writes)
  static int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,
  	struct pqi_scsi_dev *device, struct scsi_cmnd *scmd,
  	struct pqi_queue_group *queue_group)
  {
  	struct raid_map *raid_map;
 -	int rc;
 +	bool is_write = false;
 +	u32 map_index;
 +	u64 first_block;
 +	u64 last_block;
 +	u32 block_cnt;
 +	u32 blocks_per_row;
 +	u64 first_row;
 +	u64 last_row;
 +	u32 first_row_offset;
 +	u32 last_row_offset;
 +	u32 first_column;
 +	u32 last_column;
 +	u64 r0_first_row;
 +	u64 r0_last_row;
 +	u32 r5or6_blocks_per_row;
 +	u64 r5or6_first_row;
 +	u64 r5or6_last_row;
 +	u32 r5or6_first_row_offset;
 +	u32 r5or6_last_row_offset;
 +	u32 r5or6_first_column;
 +	u32 r5or6_last_column;
 +	u16 data_disks_per_row;
 +	u32 total_disks_per_row;
 +	u16 layout_map_count;
 +	u32 stripesize;
 +	u16 strip_size;
 +	u32 first_group;
 +	u32 last_group;
 +	u32 current_group;
 +	u32 map_row;
 +	u32 aio_handle;
 +	u64 disk_block;
 +	u32 disk_block_cnt;
 +	u8 cdb[16];
 +	u8 cdb_length;
 +	int offload_to_mirror;
  	struct pqi_encryption_info *encryption_info_ptr;
  	struct pqi_encryption_info encryption_info;
 -	struct pqi_scsi_dev_raid_map_data rmd = {0};
 +#if BITS_PER_LONG == 32
 +	u64 tmpdiv;
 +#endif
  
 -	rc = pqi_get_aio_lba_and_block_count(scmd, &rmd);
 -	if (rc)
 +	/* Check for valid opcode, get LBA and block count. */
 +	switch (scmd->cmnd[0]) {
 +	case WRITE_6:
 +		is_write = true;
 +		/* fall through */
 +	case READ_6:
 +		first_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |
 +			(scmd->cmnd[2] << 8) | scmd->cmnd[3]);
 +		block_cnt = (u32)scmd->cmnd[4];
 +		if (block_cnt == 0)
 +			block_cnt = 256;
 +		break;
 +	case WRITE_10:
 +		is_write = true;
 +		/* fall through */
 +	case READ_10:
 +		first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
 +		block_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);
 +		break;
 +	case WRITE_12:
 +		is_write = true;
 +		/* fall through */
 +	case READ_12:
 +		first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
 +		block_cnt = get_unaligned_be32(&scmd->cmnd[6]);
 +		break;
 +	case WRITE_16:
 +		is_write = true;
 +		/* fall through */
 +	case READ_16:
 +		first_block = get_unaligned_be64(&scmd->cmnd[2]);
 +		block_cnt = get_unaligned_be32(&scmd->cmnd[10]);
 +		break;
 +	default:
 +		/* Process via normal I/O path. */
  		return PQI_RAID_BYPASS_INELIGIBLE;
 +	}
  
 +	/* Check for write to non-RAID-0. */
 +	if (is_write && device->raid_level != SA_RAID_0)
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
++<<<<<<< HEAD
 +	if (unlikely(block_cnt == 0))
++=======
+ 	rmd.raid_level = device->raid_level;
+ 
+ 	if (!pqi_aio_raid_level_supported(ctrl_info, &rmd))
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	if (unlikely(rmd.block_cnt == 0))
++>>>>>>> 6702d2c40f31 (scsi: smartpqi: Add support for RAID5 and RAID6 writes)
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 +	last_block = first_block + block_cnt - 1;
  	raid_map = device->raid_map;
  
 -	rc = pci_get_aio_common_raid_map_values(ctrl_info, &rmd, raid_map);
 -	if (rc)
 +	/* Check for invalid block or wraparound. */
 +	if (last_block >= get_unaligned_le64(&raid_map->volume_blk_cnt) ||
 +		last_block < first_block)
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +	data_disks_per_row = get_unaligned_le16(&raid_map->data_disks_per_row);
 +	strip_size = get_unaligned_le16(&raid_map->strip_size);
 +	layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
 +
 +	/* Calculate stripe information for the request. */
 +	blocks_per_row = data_disks_per_row * strip_size;
 +#if BITS_PER_LONG == 32
 +	tmpdiv = first_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	first_row = tmpdiv;
 +	tmpdiv = last_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	last_row = tmpdiv;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	tmpdiv = first_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	first_column = tmpdiv;
 +	tmpdiv = last_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	last_column = tmpdiv;
 +#else
 +	first_row = first_block / blocks_per_row;
 +	last_row = last_block / blocks_per_row;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	first_column = first_row_offset / strip_size;
 +	last_column = last_row_offset / strip_size;
 +#endif
 +
 +	/* If this isn't a single row/column then give to the controller. */
 +	if (first_row != last_row || first_column != last_column)
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 +	/* Proceeding with driver mapping. */
 +	total_disks_per_row = data_disks_per_row +
 +		get_unaligned_le16(&raid_map->metadata_disks_per_row);
 +	map_row = ((u32)(first_row >> raid_map->parity_rotation_shift)) %
 +		get_unaligned_le16(&raid_map->row_cnt);
 +	map_index = (map_row * total_disks_per_row) + first_column;
 +
  	/* RAID 1 */
  	if (device->raid_level == SA_RAID_1) {
  		if (device->offload_to_mirror)
 -			rmd.map_index += rmd.data_disks_per_row;
 +			map_index += data_disks_per_row;
  		device->offload_to_mirror = !device->offload_to_mirror;
  	} else if (device->raid_level == SA_RAID_ADM) {
 -		rc = pqi_calc_aio_raid_adm(&rmd, device);
 +		/* RAID ADM */
 +		/*
 +		 * Handles N-way mirrors  (R1-ADM) and R10 with # of drives
 +		 * divisible by 3.
 +		 */
 +		offload_to_mirror = device->offload_to_mirror;
 +		if (offload_to_mirror == 0)  {
 +			/* use physical disk in the first mirrored group. */
 +			map_index %= data_disks_per_row;
 +		} else {
 +			do {
 +				/*
 +				 * Determine mirror group that map_index
 +				 * indicates.
 +				 */
 +				current_group = map_index / data_disks_per_row;
 +
 +				if (offload_to_mirror != current_group) {
 +					if (current_group <
 +						layout_map_count - 1) {
 +						/*
 +						 * Select raid index from
 +						 * next group.
 +						 */
 +						map_index += data_disks_per_row;
 +						current_group++;
 +					} else {
 +						/*
 +						 * Select raid index from first
 +						 * group.
 +						 */
 +						map_index %= data_disks_per_row;
 +						current_group = 0;
 +					}
 +				}
 +			} while (offload_to_mirror != current_group);
 +		}
 +
 +		/* Set mirror group to use next time. */
 +		offload_to_mirror =
 +			(offload_to_mirror >= layout_map_count - 1) ?
 +				0 : offload_to_mirror + 1;
 +		device->offload_to_mirror = offload_to_mirror;
 +		/*
 +		 * Avoid direct use of device->offload_to_mirror within this
 +		 * function since multiple threads might simultaneously
 +		 * increment it beyond the range of device->layout_map_count -1.
 +		 */
  	} else if ((device->raid_level == SA_RAID_5 ||
++<<<<<<< HEAD
 +		device->raid_level == SA_RAID_6) && layout_map_count > 1) {
 +		/* RAID 50/60 */
 +		/* Verify first and last block are in same RAID group */
 +		r5or6_blocks_per_row = strip_size * data_disks_per_row;
 +		stripesize = r5or6_blocks_per_row * layout_map_count;
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		first_group = tmpdiv;
 +		tmpdiv = last_block;
 +		last_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = last_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		last_group = tmpdiv;
 +#else
 +		first_group = (first_block % stripesize) / r5or6_blocks_per_row;
 +		last_group = (last_block % stripesize) / r5or6_blocks_per_row;
 +#endif
 +		if (first_group != last_group)
++=======
+ 		device->raid_level == SA_RAID_6) &&
+ 		(rmd.layout_map_count > 1 || rmd.is_write)) {
+ 		rc = pqi_calc_aio_r5_or_r6(&rmd, raid_map);
+ 		if (rc)
++>>>>>>> 6702d2c40f31 (scsi: smartpqi: Add support for RAID5 and RAID6 writes)
  			return PQI_RAID_BYPASS_INELIGIBLE;
 -	}
  
 -	if (unlikely(rmd.map_index >= RAID_MAP_MAX_ENTRIES))
 -		return PQI_RAID_BYPASS_INELIGIBLE;
 +		/* Verify request is in a single row of RAID 5/6 */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		do_div(tmpdiv, stripesize);
 +		first_row = r5or6_first_row = r0_first_row = tmpdiv;
 +		tmpdiv = last_block;
 +		do_div(tmpdiv, stripesize);
 +		r5or6_last_row = r0_last_row = tmpdiv;
 +#else
 +		first_row = r5or6_first_row = r0_first_row =
 +			first_block / stripesize;
 +		r5or6_last_row = r0_last_row = last_block / stripesize;
 +#endif
 +		if (r5or6_first_row != r5or6_last_row)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
  
 -	rmd.aio_handle = raid_map->disk_data[rmd.map_index].aio_handle;
 -	rmd.disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 -		rmd.first_row * rmd.strip_size +
 -		(rmd.first_row_offset - rmd.first_column * rmd.strip_size);
 -	rmd.disk_block_cnt = rmd.block_cnt;
 +		/* Verify request is in a single column */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_row_offset;
 +		first_row_offset = (u32)do_div(tmpdiv, r5or6_blocks_per_row);
 +		r5or6_first_row_offset = first_row_offset;
 +		tmpdiv = last_block;
 +		r5or6_last_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = r5or6_last_row_offset;
 +		r5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);
 +		tmpdiv = r5or6_first_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		first_column = r5or6_first_column = tmpdiv;
 +		tmpdiv = r5or6_last_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		r5or6_last_column = tmpdiv;
 +#else
 +		first_row_offset = r5or6_first_row_offset =
 +			(u32)((first_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		r5or6_last_row_offset =
 +			(u32)((last_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		first_column = r5or6_first_row_offset / strip_size;
 +		r5or6_first_column = first_column;
 +		r5or6_last_column = r5or6_last_row_offset / strip_size;
 +#endif
 +		if (r5or6_first_column != r5or6_last_column)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +		/* Request is eligible */
 +		map_row =
 +			((u32)(first_row >> raid_map->parity_rotation_shift)) %
 +			get_unaligned_le16(&raid_map->row_cnt);
 +
 +		map_index = (first_group *
 +			(get_unaligned_le16(&raid_map->row_cnt) *
 +			total_disks_per_row)) +
 +			(map_row * total_disks_per_row) + first_column;
 +	}
 +
 +	aio_handle = raid_map->disk_data[map_index].aio_handle;
 +	disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 +		first_row * strip_size +
 +		(first_row_offset - first_column * strip_size);
 +	disk_block_cnt = block_cnt;
  
  	/* Handle differing logical/physical block sizes. */
  	if (raid_map->phys_blk_shift) {
@@@ -2592,8 -2662,27 +2964,32 @@@
  		encryption_info_ptr = NULL;
  	}
  
++<<<<<<< HEAD
 +	return pqi_aio_submit_io(ctrl_info, scmd, aio_handle,
 +		cdb, cdb_length, queue_group, encryption_info_ptr, true);
++=======
+ 	if (rmd.is_write) {
+ 		switch (device->raid_level) {
+ 		case SA_RAID_0:
+ 			return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 				rmd.cdb, rmd.cdb_length, queue_group,
+ 				encryption_info_ptr, true);
+ 		case SA_RAID_5:
+ 		case SA_RAID_6:
+ 			return pqi_aio_submit_r56_write_io(ctrl_info, scmd, queue_group,
+ 					encryption_info_ptr, device, &rmd);
+ 		default:
+ 			return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 				rmd.cdb, rmd.cdb_length, queue_group,
+ 				encryption_info_ptr, true);
+ 		}
+ 	} else {
+ 		return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 			rmd.cdb, rmd.cdb_length, queue_group,
+ 			encryption_info_ptr, true);
+ 	}
+ 
++>>>>>>> 6702d2c40f31 (scsi: smartpqi: Add support for RAID5 and RAID6 writes)
  }
  
  #define PQI_STATUS_IDLE		0x0
diff --git a/drivers/scsi/smartpqi/smartpqi.h b/drivers/scsi/smartpqi/smartpqi.h
index 7d3f956e949f..a2ee42c3962d 100644
--- a/drivers/scsi/smartpqi/smartpqi.h
+++ b/drivers/scsi/smartpqi/smartpqi.h
@@ -257,6 +257,7 @@ struct pqi_device_capability {
 };
 
 #define PQI_MAX_EMBEDDED_SG_DESCRIPTORS		4
+#define PQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS	3
 
 struct pqi_raid_path_request {
 	struct pqi_iu_header header;
@@ -312,6 +313,37 @@ struct pqi_aio_path_request {
 		sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
 };
 
+struct pqi_aio_r56_path_request {
+	struct pqi_iu_header header;
+	__le16	request_id;
+	__le16	volume_id;		/* ID of the RAID volume */
+	__le32	data_it_nexus;		/* IT nexus for the data drive */
+	__le32	p_parity_it_nexus;	/* IT nexus for the P parity drive */
+	__le32	q_parity_it_nexus;	/* IT nexus for the Q parity drive */
+	__le32	data_length;		/* total bytes to read/write */
+	u8	data_direction : 2;
+	u8	partial : 1;
+	u8	mem_type : 1;		/* 0 = PCIe, 1 = DDR */
+	u8	fence : 1;
+	u8	encryption_enable : 1;
+	u8	reserved : 2;
+	u8	task_attribute : 3;
+	u8	command_priority : 4;
+	u8	reserved1 : 1;
+	__le16	data_encryption_key_index;
+	u8	cdb[16];
+	__le16	error_index;
+	u8	num_sg_descriptors;
+	u8	cdb_length;
+	u8	xor_multiplier;
+	u8	reserved2[3];
+	__le32	encrypt_tweak_lower;
+	__le32	encrypt_tweak_upper;
+	__le64	row;			/* row = logical LBA/blocks per row */
+	u8	reserved3[8];
+	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS];
+};
+
 struct pqi_io_response {
 	struct pqi_iu_header header;
 	__le16	request_id;
@@ -484,6 +516,8 @@ struct pqi_raid_error_info {
 #define PQI_REQUEST_IU_TASK_MANAGEMENT			0x13
 #define PQI_REQUEST_IU_RAID_PATH_IO			0x14
 #define PQI_REQUEST_IU_AIO_PATH_IO			0x15
+#define PQI_REQUEST_IU_AIO_PATH_RAID5_IO		0x18
+#define PQI_REQUEST_IU_AIO_PATH_RAID6_IO		0x19
 #define PQI_REQUEST_IU_GENERAL_ADMIN			0x60
 #define PQI_REQUEST_IU_REPORT_VENDOR_EVENT_CONFIG	0x72
 #define PQI_REQUEST_IU_SET_VENDOR_EVENT_CONFIG		0x73
@@ -1127,6 +1161,7 @@ struct pqi_ctrl_info {
 	u16		max_inbound_iu_length_per_firmware;
 	u16		max_inbound_iu_length;
 	unsigned int	max_sg_per_iu;
+	unsigned int	max_sg_per_r56_iu;
 	void		*admin_queue_memory_base;
 	u32		admin_queue_memory_length;
 	dma_addr_t	admin_queue_memory_base_dma_handle;
@@ -1158,6 +1193,8 @@ struct pqi_ctrl_info {
 	u8		soft_reset_handshake_supported : 1;
 	u8		raid_iu_timeout_supported: 1;
 	u8		tmf_iu_timeout_supported: 1;
+	u8		enable_r5_writes : 1;
+	u8		enable_r6_writes : 1;
 
 	struct list_head scsi_device_list;
 	spinlock_t	scsi_device_list_lock;
* Unmerged path drivers/scsi/smartpqi/smartpqi_init.c
