bpf: Introduce bpf_per_cpu_ptr()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Hao Luo <haoluo@google.com>
commit eaa6bcb71ef6ed3dc18fc525ee7e293b06b4882b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/eaa6bcb7.failed

Add bpf_per_cpu_ptr() to help bpf programs access percpu vars.
bpf_per_cpu_ptr() has the same semantic as per_cpu_ptr() in the kernel
except that it may return NULL. This happens when the cpu parameter is
out of range. So the caller must check the returned value.

	Signed-off-by: Hao Luo <haoluo@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
Link: https://lore.kernel.org/bpf/20200929235049.2533242-5-haoluo@google.com
(cherry picked from commit eaa6bcb71ef6ed3dc18fc525ee7e293b06b4882b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/helpers.c
#	kernel/bpf/verifier.c
#	kernel/trace/bpf_trace.c
#	tools/include/uapi/linux/bpf.h
diff --cc include/linux/bpf.h
index e36b4db03c9a,9dde15b2479d..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -278,6 -292,9 +278,12 @@@ enum bpf_arg_type 
  	ARG_PTR_TO_ALLOC_MEM,	/* pointer to dynamically allocated memory */
  	ARG_PTR_TO_ALLOC_MEM_OR_NULL,	/* pointer to dynamically allocated memory or NULL */
  	ARG_CONST_ALLOC_SIZE_OR_ZERO,	/* number of allocated bytes requested */
++<<<<<<< HEAD
++=======
+ 	ARG_PTR_TO_BTF_ID_SOCK_COMMON,	/* pointer to in-kernel sock_common or bpf-mirrored bpf_sock */
+ 	ARG_PTR_TO_PERCPU_BTF_ID,	/* pointer to in-kernel percpu type */
+ 	__BPF_ARG_TYPE_MAX,
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  };
  
  /* type of values returned from helper functions */
@@@ -1814,6 -1829,9 +1822,12 @@@ extern const struct bpf_func_proto bpf_
  extern const struct bpf_func_proto bpf_skc_to_tcp_timewait_sock_proto;
  extern const struct bpf_func_proto bpf_skc_to_tcp_request_sock_proto;
  extern const struct bpf_func_proto bpf_skc_to_udp6_sock_proto;
++<<<<<<< HEAD
++=======
+ extern const struct bpf_func_proto bpf_copy_from_user_proto;
+ extern const struct bpf_func_proto bpf_snprintf_btf_proto;
+ extern const struct bpf_func_proto bpf_per_cpu_ptr_proto;
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  
  const struct bpf_func_proto *bpf_tracing_func_proto(
  	enum bpf_func_id func_id, const struct bpf_prog *prog);
diff --cc include/uapi/linux/bpf.h
index 818231d02d19,f3c1b637ab39..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3444,6 -3448,261 +3444,264 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		fills in e.g. MAC addresses based on the L3 information from
+  * 		the packet. This helper is supported for IPv4 and IPv6 protocols.
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
+  *
+  * void *bpf_per_cpu_ptr(const void *percpu_ptr, u32 cpu)
+  *     Description
+  *             Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *             pointer to the percpu kernel variable on *cpu*. A ksym is an
+  *             extern variable decorated with '__ksym'. For ksym, there is a
+  *             global var (either static or global) defined of the same name
+  *             in the kernel. The ksym is percpu if the global var is percpu.
+  *             The returned pointer points to the global percpu var on *cpu*.
+  *
+  *             bpf_per_cpu_ptr() has the same semantic as per_cpu_ptr() in the
+  *             kernel, except that bpf_per_cpu_ptr() may return NULL. This
+  *             happens if *cpu* is larger than nr_cpu_ids. The caller of
+  *             bpf_per_cpu_ptr() must check the returned value.
+  *     Return
+  *             A pointer pointing to the kernel percpu variable on *cpu*, or
+  *             NULL, if *cpu* is invalid.
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3588,6 -3847,18 +3846,21 @@@
  	FN(skc_to_tcp_request_sock),	\
  	FN(skc_to_udp6_sock),		\
  	FN(get_task_stack),		\
++<<<<<<< HEAD
++=======
+ 	FN(load_hdr_opt),		\
+ 	FN(store_hdr_opt),		\
+ 	FN(reserve_hdr_opt),		\
+ 	FN(inode_storage_get),		\
+ 	FN(inode_storage_delete),	\
+ 	FN(d_path),			\
+ 	FN(copy_from_user),		\
+ 	FN(snprintf_btf),		\
+ 	FN(seq_printf_btf),		\
+ 	FN(skb_cgroup_classid),		\
+ 	FN(redirect_neigh),		\
+ 	FN(bpf_per_cpu_ptr),            \
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  	/* */
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
diff --cc kernel/bpf/helpers.c
index 558c98f4de99,14fe3f64fd82..000000000000
--- a/kernel/bpf/helpers.c
+++ b/kernel/bpf/helpers.c
@@@ -609,6 -601,44 +609,47 @@@ const struct bpf_func_proto bpf_event_o
  	.arg5_type      = ARG_CONST_SIZE_OR_ZERO,
  };
  
++<<<<<<< HEAD
++=======
+ BPF_CALL_3(bpf_copy_from_user, void *, dst, u32, size,
+ 	   const void __user *, user_ptr)
+ {
+ 	int ret = copy_from_user(dst, user_ptr, size);
+ 
+ 	if (unlikely(ret)) {
+ 		memset(dst, 0, size);
+ 		ret = -EFAULT;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ const struct bpf_func_proto bpf_copy_from_user_proto = {
+ 	.func		= bpf_copy_from_user,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_UNINIT_MEM,
+ 	.arg2_type	= ARG_CONST_SIZE_OR_ZERO,
+ 	.arg3_type	= ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_2(bpf_per_cpu_ptr, const void *, ptr, u32, cpu)
+ {
+ 	if (cpu >= nr_cpu_ids)
+ 		return (unsigned long)NULL;
+ 
+ 	return (unsigned long)per_cpu_ptr((const void __percpu *)ptr, cpu);
+ }
+ 
+ const struct bpf_func_proto bpf_per_cpu_ptr_proto = {
+ 	.func		= bpf_per_cpu_ptr,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_PTR_TO_MEM_OR_BTF_ID_OR_NULL,
+ 	.arg1_type	= ARG_PTR_TO_PERCPU_BTF_ID,
+ 	.arg2_type	= ARG_ANYTHING,
+ };
+ 
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  const struct bpf_func_proto bpf_get_current_task_proto __weak;
  const struct bpf_func_proto bpf_probe_read_user_proto __weak;
  const struct bpf_func_proto bpf_probe_read_user_str_proto __weak;
@@@ -669,8 -699,14 +710,10 @@@ bpf_base_func_proto(enum bpf_func_id fu
  		if (!perfmon_capable())
  			return NULL;
  		return bpf_get_trace_printk_proto();
 -	case BPF_FUNC_snprintf_btf:
 -		if (!perfmon_capable())
 -			return NULL;
 -		return &bpf_snprintf_btf_proto;
  	case BPF_FUNC_jiffies64:
  		return &bpf_jiffies64_proto;
+ 	case BPF_FUNC_bpf_per_cpu_ptr:
+ 		return &bpf_per_cpu_ptr_proto;
  	default:
  		break;
  	}
diff --cc kernel/bpf/verifier.c
index 3fd224c98ac3,216b8ece23ce..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -246,6 -239,7 +246,10 @@@ struct bpf_call_arg_meta 
  	int ref_obj_id;
  	int func_id;
  	u32 btf_id;
++<<<<<<< HEAD
++=======
+ 	u32 ret_btf_id;
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  };
  
  struct btf *btf_vmlinux;
@@@ -3937,6 -3961,155 +3945,158 @@@ static int resolve_map_arg_type(struct 
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ struct bpf_reg_types {
+ 	const enum bpf_reg_type types[10];
+ 	u32 *btf_id;
+ };
+ 
+ static const struct bpf_reg_types map_key_value_types = {
+ 	.types = {
+ 		PTR_TO_STACK,
+ 		PTR_TO_PACKET,
+ 		PTR_TO_PACKET_META,
+ 		PTR_TO_MAP_VALUE,
+ 	},
+ };
+ 
+ static const struct bpf_reg_types sock_types = {
+ 	.types = {
+ 		PTR_TO_SOCK_COMMON,
+ 		PTR_TO_SOCKET,
+ 		PTR_TO_TCP_SOCK,
+ 		PTR_TO_XDP_SOCK,
+ 	},
+ };
+ 
+ static const struct bpf_reg_types btf_id_sock_common_types = {
+ 	.types = {
+ 		PTR_TO_SOCK_COMMON,
+ 		PTR_TO_SOCKET,
+ 		PTR_TO_TCP_SOCK,
+ 		PTR_TO_XDP_SOCK,
+ 		PTR_TO_BTF_ID,
+ 	},
+ 	.btf_id = &btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON],
+ };
+ 
+ static const struct bpf_reg_types mem_types = {
+ 	.types = {
+ 		PTR_TO_STACK,
+ 		PTR_TO_PACKET,
+ 		PTR_TO_PACKET_META,
+ 		PTR_TO_MAP_VALUE,
+ 		PTR_TO_MEM,
+ 		PTR_TO_RDONLY_BUF,
+ 		PTR_TO_RDWR_BUF,
+ 	},
+ };
+ 
+ static const struct bpf_reg_types int_ptr_types = {
+ 	.types = {
+ 		PTR_TO_STACK,
+ 		PTR_TO_PACKET,
+ 		PTR_TO_PACKET_META,
+ 		PTR_TO_MAP_VALUE,
+ 	},
+ };
+ 
+ static const struct bpf_reg_types fullsock_types = { .types = { PTR_TO_SOCKET } };
+ static const struct bpf_reg_types scalar_types = { .types = { SCALAR_VALUE } };
+ static const struct bpf_reg_types context_types = { .types = { PTR_TO_CTX } };
+ static const struct bpf_reg_types alloc_mem_types = { .types = { PTR_TO_MEM } };
+ static const struct bpf_reg_types const_map_ptr_types = { .types = { CONST_PTR_TO_MAP } };
+ static const struct bpf_reg_types btf_ptr_types = { .types = { PTR_TO_BTF_ID } };
+ static const struct bpf_reg_types spin_lock_types = { .types = { PTR_TO_MAP_VALUE } };
+ static const struct bpf_reg_types percpu_btf_ptr_types = { .types = { PTR_TO_PERCPU_BTF_ID } };
+ 
+ static const struct bpf_reg_types *compatible_reg_types[__BPF_ARG_TYPE_MAX] = {
+ 	[ARG_PTR_TO_MAP_KEY]		= &map_key_value_types,
+ 	[ARG_PTR_TO_MAP_VALUE]		= &map_key_value_types,
+ 	[ARG_PTR_TO_UNINIT_MAP_VALUE]	= &map_key_value_types,
+ 	[ARG_PTR_TO_MAP_VALUE_OR_NULL]	= &map_key_value_types,
+ 	[ARG_CONST_SIZE]		= &scalar_types,
+ 	[ARG_CONST_SIZE_OR_ZERO]	= &scalar_types,
+ 	[ARG_CONST_ALLOC_SIZE_OR_ZERO]	= &scalar_types,
+ 	[ARG_CONST_MAP_PTR]		= &const_map_ptr_types,
+ 	[ARG_PTR_TO_CTX]		= &context_types,
+ 	[ARG_PTR_TO_CTX_OR_NULL]	= &context_types,
+ 	[ARG_PTR_TO_SOCK_COMMON]	= &sock_types,
+ 	[ARG_PTR_TO_BTF_ID_SOCK_COMMON]	= &btf_id_sock_common_types,
+ 	[ARG_PTR_TO_SOCKET]		= &fullsock_types,
+ 	[ARG_PTR_TO_SOCKET_OR_NULL]	= &fullsock_types,
+ 	[ARG_PTR_TO_BTF_ID]		= &btf_ptr_types,
+ 	[ARG_PTR_TO_SPIN_LOCK]		= &spin_lock_types,
+ 	[ARG_PTR_TO_MEM]		= &mem_types,
+ 	[ARG_PTR_TO_MEM_OR_NULL]	= &mem_types,
+ 	[ARG_PTR_TO_UNINIT_MEM]		= &mem_types,
+ 	[ARG_PTR_TO_ALLOC_MEM]		= &alloc_mem_types,
+ 	[ARG_PTR_TO_ALLOC_MEM_OR_NULL]	= &alloc_mem_types,
+ 	[ARG_PTR_TO_INT]		= &int_ptr_types,
+ 	[ARG_PTR_TO_LONG]		= &int_ptr_types,
+ 	[ARG_PTR_TO_PERCPU_BTF_ID]	= &percpu_btf_ptr_types,
+ };
+ 
+ static int check_reg_type(struct bpf_verifier_env *env, u32 regno,
+ 			  enum bpf_arg_type arg_type,
+ 			  const u32 *arg_btf_id)
+ {
+ 	struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];
+ 	enum bpf_reg_type expected, type = reg->type;
+ 	const struct bpf_reg_types *compatible;
+ 	int i, j;
+ 
+ 	compatible = compatible_reg_types[arg_type];
+ 	if (!compatible) {
+ 		verbose(env, "verifier internal error: unsupported arg type %d\n", arg_type);
+ 		return -EFAULT;
+ 	}
+ 
+ 	for (i = 0; i < ARRAY_SIZE(compatible->types); i++) {
+ 		expected = compatible->types[i];
+ 		if (expected == NOT_INIT)
+ 			break;
+ 
+ 		if (type == expected)
+ 			goto found;
+ 	}
+ 
+ 	verbose(env, "R%d type=%s expected=", regno, reg_type_str[type]);
+ 	for (j = 0; j + 1 < i; j++)
+ 		verbose(env, "%s, ", reg_type_str[compatible->types[j]]);
+ 	verbose(env, "%s\n", reg_type_str[compatible->types[j]]);
+ 	return -EACCES;
+ 
+ found:
+ 	if (type == PTR_TO_BTF_ID) {
+ 		if (!arg_btf_id) {
+ 			if (!compatible->btf_id) {
+ 				verbose(env, "verifier internal error: missing arg compatible BTF ID\n");
+ 				return -EFAULT;
+ 			}
+ 			arg_btf_id = compatible->btf_id;
+ 		}
+ 
+ 		if (!btf_struct_ids_match(&env->log, reg->off, reg->btf_id,
+ 					  *arg_btf_id)) {
+ 			verbose(env, "R%d is of type %s but %s is expected\n",
+ 				regno, kernel_type_name(reg->btf_id),
+ 				kernel_type_name(*arg_btf_id));
+ 			return -EACCES;
+ 		}
+ 
+ 		if (!tnum_is_const(reg->var_off) || reg->var_off.value) {
+ 			verbose(env, "R%d is a pointer to in-kernel struct with non-zero offset\n",
+ 				regno);
+ 			return -EACCES;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  static int check_func_arg(struct bpf_verifier_env *env, u32 arg,
  			  struct bpf_call_arg_meta *meta,
  			  const struct bpf_func_proto *fn)
@@@ -5029,7 -5125,33 +5195,31 @@@ static int check_helper_call(struct bpf
  	} else if (fn->ret_type == RET_PTR_TO_ALLOC_MEM_OR_NULL) {
  		mark_reg_known_zero(env, regs, BPF_REG_0);
  		regs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;
 -		regs[BPF_REG_0].id = ++env->id_gen;
  		regs[BPF_REG_0].mem_size = meta.mem_size;
+ 	} else if (fn->ret_type == RET_PTR_TO_MEM_OR_BTF_ID_OR_NULL) {
+ 		const struct btf_type *t;
+ 
+ 		mark_reg_known_zero(env, regs, BPF_REG_0);
+ 		t = btf_type_skip_modifiers(btf_vmlinux, meta.ret_btf_id, NULL);
+ 		if (!btf_type_is_struct(t)) {
+ 			u32 tsize;
+ 			const struct btf_type *ret;
+ 			const char *tname;
+ 
+ 			/* resolve the type size of ksym. */
+ 			ret = btf_resolve_size(btf_vmlinux, t, &tsize);
+ 			if (IS_ERR(ret)) {
+ 				tname = btf_name_by_offset(btf_vmlinux, t->name_off);
+ 				verbose(env, "unable to resolve the size of type '%s': %ld\n",
+ 					tname, PTR_ERR(ret));
+ 				return -EINVAL;
+ 			}
+ 			regs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;
+ 			regs[BPF_REG_0].mem_size = tsize;
+ 		} else {
+ 			regs[BPF_REG_0].type = PTR_TO_BTF_ID_OR_NULL;
+ 			regs[BPF_REG_0].btf_id = meta.ret_btf_id;
+ 		}
  	} else if (fn->ret_type == RET_PTR_TO_BTF_ID_OR_NULL) {
  		int ret_btf_id;
  
diff --cc kernel/trace/bpf_trace.c
index fa9081cef38b,364a322e2898..000000000000
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@@ -1170,6 -1323,12 +1170,15 @@@ bpf_tracing_func_proto(enum bpf_func_i
  		return &bpf_jiffies64_proto;
  	case BPF_FUNC_get_task_stack:
  		return &bpf_get_task_stack_proto;
++<<<<<<< HEAD
++=======
+ 	case BPF_FUNC_copy_from_user:
+ 		return prog->aux->sleepable ? &bpf_copy_from_user_proto : NULL;
+ 	case BPF_FUNC_snprintf_btf:
+ 		return &bpf_snprintf_btf_proto;
+ 	case BPF_FUNC_bpf_per_cpu_ptr:
+ 		return &bpf_per_cpu_ptr_proto;
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  	default:
  		return NULL;
  	}
diff --cc tools/include/uapi/linux/bpf.h
index 4c62783528f1,f3c1b637ab39..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3424,6 -3448,261 +3424,264 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		fills in e.g. MAC addresses based on the L3 information from
+  * 		the packet. This helper is supported for IPv4 and IPv6 protocols.
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
+  *
+  * void *bpf_per_cpu_ptr(const void *percpu_ptr, u32 cpu)
+  *     Description
+  *             Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *             pointer to the percpu kernel variable on *cpu*. A ksym is an
+  *             extern variable decorated with '__ksym'. For ksym, there is a
+  *             global var (either static or global) defined of the same name
+  *             in the kernel. The ksym is percpu if the global var is percpu.
+  *             The returned pointer points to the global percpu var on *cpu*.
+  *
+  *             bpf_per_cpu_ptr() has the same semantic as per_cpu_ptr() in the
+  *             kernel, except that bpf_per_cpu_ptr() may return NULL. This
+  *             happens if *cpu* is larger than nr_cpu_ids. The caller of
+  *             bpf_per_cpu_ptr() must check the returned value.
+  *     Return
+  *             A pointer pointing to the kernel percpu variable on *cpu*, or
+  *             NULL, if *cpu* is invalid.
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3568,6 -3847,18 +3826,21 @@@
  	FN(skc_to_tcp_request_sock),	\
  	FN(skc_to_udp6_sock),		\
  	FN(get_task_stack),		\
++<<<<<<< HEAD
++=======
+ 	FN(load_hdr_opt),		\
+ 	FN(store_hdr_opt),		\
+ 	FN(reserve_hdr_opt),		\
+ 	FN(inode_storage_get),		\
+ 	FN(inode_storage_delete),	\
+ 	FN(d_path),			\
+ 	FN(copy_from_user),		\
+ 	FN(snprintf_btf),		\
+ 	FN(seq_printf_btf),		\
+ 	FN(skb_cgroup_classid),		\
+ 	FN(redirect_neigh),		\
+ 	FN(bpf_per_cpu_ptr),            \
++>>>>>>> eaa6bcb71ef6 (bpf: Introduce bpf_per_cpu_ptr())
  	/* */
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
* Unmerged path include/linux/bpf.h
diff --git a/include/linux/btf.h b/include/linux/btf.h
index 1913d3c439c2..6053c73b95ee 100644
--- a/include/linux/btf.h
+++ b/include/linux/btf.h
@@ -107,6 +107,11 @@ btf_resolve_size(const struct btf *btf, const struct btf_type *type,
 	     i < btf_type_vlen(struct_type);			\
 	     i++, member++)
 
+#define for_each_vsi(i, datasec_type, member)			\
+	for (i = 0, member = btf_type_var_secinfo(datasec_type);	\
+	     i < btf_type_vlen(datasec_type);			\
+	     i++, member++)
+
 static inline bool btf_type_is_ptr(const struct btf_type *t)
 {
 	return BTF_INFO_KIND(t->info) == BTF_KIND_PTR;
@@ -191,6 +196,12 @@ static inline const struct btf_member *btf_type_member(const struct btf_type *t)
 	return (const struct btf_member *)(t + 1);
 }
 
+static inline const struct btf_var_secinfo *btf_type_var_secinfo(
+		const struct btf_type *t)
+{
+	return (const struct btf_var_secinfo *)(t + 1);
+}
+
 #ifdef CONFIG_BPF_SYSCALL
 const struct btf_type *btf_type_by_id(const struct btf *btf, u32 type_id);
 const char *btf_name_by_offset(const struct btf *btf, u32 offset);
* Unmerged path include/uapi/linux/bpf.h
diff --git a/kernel/bpf/btf.c b/kernel/bpf/btf.c
index 85c8de2dcc53..53282e1879ff 100644
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@ -188,11 +188,6 @@
 	     i < btf_type_vlen(struct_type);				\
 	     i++, member++)
 
-#define for_each_vsi(i, struct_type, member)			\
-	for (i = 0, member = btf_type_var_secinfo(struct_type);	\
-	     i < btf_type_vlen(struct_type);			\
-	     i++, member++)
-
 #define for_each_vsi_from(i, from, struct_type, member)				\
 	for (i = from, member = btf_type_var_secinfo(struct_type) + from;	\
 	     i < btf_type_vlen(struct_type);					\
@@ -598,11 +593,6 @@ static const struct btf_var *btf_type_var(const struct btf_type *t)
 	return (const struct btf_var *)(t + 1);
 }
 
-static const struct btf_var_secinfo *btf_type_var_secinfo(const struct btf_type *t)
-{
-	return (const struct btf_var_secinfo *)(t + 1);
-}
-
 static const struct btf_kind_operations *btf_type_ops(const struct btf_type *t)
 {
 	return kind_ops[BTF_INFO_KIND(t->info)];
* Unmerged path kernel/bpf/helpers.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path tools/include/uapi/linux/bpf.h
