bpf: Add redirect_neigh helper as redirect drop-in

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit b4ab31414970a7a03a5d55d75083f2c101a30592
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/b4ab3141.failed

Add a redirect_neigh() helper as redirect() drop-in replacement
for the xmit side. Main idea for the helper is to be very similar
in semantics to the latter just that the skb gets injected into
the neighboring subsystem in order to let the stack do the work
it knows best anyway to populate the L2 addresses of the packet
and then hand over to dev_queue_xmit() as redirect() does.

This solves two bigger items: i) skbs don't need to go up to the
stack on the host facing veth ingress side for traffic egressing
the container to achieve the same for populating L2 which also
has the huge advantage that ii) the skb->sk won't get orphaned in
ip_rcv_core() when entering the IP routing layer on the host stack.

Given that skb->sk neither gets orphaned when crossing the netns
as per 9c4c325252c5 ("skbuff: preserve sock reference when scrubbing
the skb.") the helper can then push the skbs directly to the phys
device where FQ scheduler can do its work and TCP stack gets proper
backpressure given we hold on to skb->sk as long as skb is still
residing in queues.

With the helper used in BPF data path to then push the skb to the
phys device, I observed a stable/consistent TCP_STREAM improvement
on veth devices for traffic going container -> host -> host ->
container from ~10Gbps to ~15Gbps for a single stream in my test
environment.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Reviewed-by: David Ahern <dsahern@gmail.com>
	Acked-by: Martin KaFai Lau <kafai@fb.com>
	Cc: David Ahern <dsahern@kernel.org>
Link: https://lore.kernel.org/bpf/f207de81629e1724899b73b8112e0013be782d35.1601477936.git.daniel@iogearbox.net
(cherry picked from commit b4ab31414970a7a03a5d55d75083f2c101a30592)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index 0a8b1917a074,1f17c6752deb..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3444,6 -3427,244 +3444,247 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		fills in e.g. MAC addresses based on the L3 information from
+  * 		the packet. This helper is supported for IPv4 and IPv6 protocols.
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
++>>>>>>> b4ab31414970 (bpf: Add redirect_neigh helper as redirect drop-in)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3588,6 -3809,17 +3829,20 @@@
  	FN(skc_to_tcp_request_sock),	\
  	FN(skc_to_udp6_sock),		\
  	FN(get_task_stack),		\
++<<<<<<< HEAD
++=======
+ 	FN(load_hdr_opt),		\
+ 	FN(store_hdr_opt),		\
+ 	FN(reserve_hdr_opt),		\
+ 	FN(inode_storage_get),		\
+ 	FN(inode_storage_delete),	\
+ 	FN(d_path),			\
+ 	FN(copy_from_user),		\
+ 	FN(snprintf_btf),		\
+ 	FN(seq_printf_btf),		\
+ 	FN(skb_cgroup_classid),		\
+ 	FN(redirect_neigh),		\
++>>>>>>> b4ab31414970 (bpf: Add redirect_neigh helper as redirect drop-in)
  	/* */
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
diff --cc tools/include/uapi/linux/bpf.h
index 0dc0309df9c7,1f17c6752deb..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3424,6 -3427,244 +3424,247 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		fills in e.g. MAC addresses based on the L3 information from
+  * 		the packet. This helper is supported for IPv4 and IPv6 protocols.
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
++>>>>>>> b4ab31414970 (bpf: Add redirect_neigh helper as redirect drop-in)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3568,6 -3809,17 +3809,20 @@@
  	FN(skc_to_tcp_request_sock),	\
  	FN(skc_to_udp6_sock),		\
  	FN(get_task_stack),		\
++<<<<<<< HEAD
++=======
+ 	FN(load_hdr_opt),		\
+ 	FN(store_hdr_opt),		\
+ 	FN(reserve_hdr_opt),		\
+ 	FN(inode_storage_get),		\
+ 	FN(inode_storage_delete),	\
+ 	FN(d_path),			\
+ 	FN(copy_from_user),		\
+ 	FN(snprintf_btf),		\
+ 	FN(seq_printf_btf),		\
+ 	FN(skb_cgroup_classid),		\
+ 	FN(redirect_neigh),		\
++>>>>>>> b4ab31414970 (bpf: Add redirect_neigh helper as redirect drop-in)
  	/* */
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 4126434d7a0d..3ff5766e493f 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -2570,6 +2570,11 @@ static inline int skb_mac_header_was_set(const struct sk_buff *skb)
 	return skb->mac_header != (typeof(skb->mac_header))~0U;
 }
 
+static inline void skb_unset_mac_header(struct sk_buff *skb)
+{
+	skb->mac_header = (typeof(skb->mac_header))~0U;
+}
+
 static inline void skb_reset_mac_header(struct sk_buff *skb)
 {
 	skb->mac_header = skb->data - skb->head;
* Unmerged path include/uapi/linux/bpf.h
diff --git a/net/core/filter.c b/net/core/filter.c
index a22f97d754fb..e01a00f16094 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -2173,13 +2173,233 @@ static int __bpf_redirect(struct sk_buff *skb, struct net_device *dev,
 		return __bpf_redirect_no_mac(skb, dev, flags);
 }
 
+#if IS_ENABLED(CONFIG_IPV6)
+static int bpf_out_neigh_v6(struct net *net, struct sk_buff *skb)
+{
+	struct dst_entry *dst = skb_dst(skb);
+	struct net_device *dev = dst->dev;
+	u32 hh_len = LL_RESERVED_SPACE(dev);
+	const struct in6_addr *nexthop;
+	struct neighbour *neigh;
+
+	if (dev_xmit_recursion()) {
+		net_crit_ratelimited("bpf: recursion limit reached on datapath, buggy bpf program?\n");
+		goto out_drop;
+	}
+
+	skb->dev = dev;
+	skb->tstamp = 0;
+
+	if (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {
+		struct sk_buff *skb2;
+
+		skb2 = skb_realloc_headroom(skb, hh_len);
+		if (unlikely(!skb2)) {
+			kfree_skb(skb);
+			return -ENOMEM;
+		}
+		if (skb->sk)
+			skb_set_owner_w(skb2, skb->sk);
+		consume_skb(skb);
+		skb = skb2;
+	}
+
+	rcu_read_lock_bh();
+	nexthop = rt6_nexthop(container_of(dst, struct rt6_info, dst),
+			      &ipv6_hdr(skb)->daddr);
+	neigh = ip_neigh_gw6(dev, nexthop);
+	if (likely(!IS_ERR(neigh))) {
+		int ret;
+
+		sock_confirm_neigh(skb, neigh);
+		dev_xmit_recursion_inc();
+		ret = neigh_output(neigh, skb, false);
+		dev_xmit_recursion_dec();
+		rcu_read_unlock_bh();
+		return ret;
+	}
+	rcu_read_unlock_bh();
+	IP6_INC_STATS(dev_net(dst->dev),
+		      ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);
+out_drop:
+	kfree_skb(skb);
+	return -ENETDOWN;
+}
+
+static int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev)
+{
+	const struct ipv6hdr *ip6h = ipv6_hdr(skb);
+	struct net *net = dev_net(dev);
+	int err, ret = NET_XMIT_DROP;
+	struct dst_entry *dst;
+	struct flowi6 fl6 = {
+		.flowi6_flags	= FLOWI_FLAG_ANYSRC,
+		.flowi6_mark	= skb->mark,
+		.flowlabel	= ip6_flowinfo(ip6h),
+		.flowi6_oif	= dev->ifindex,
+		.flowi6_proto	= ip6h->nexthdr,
+		.daddr		= ip6h->daddr,
+		.saddr		= ip6h->saddr,
+	};
+
+	dst = ipv6_stub->ipv6_dst_lookup_flow(net, NULL, &fl6, NULL);
+	if (IS_ERR(dst))
+		goto out_drop;
+
+	skb_dst_set(skb, dst);
+
+	err = bpf_out_neigh_v6(net, skb);
+	if (unlikely(net_xmit_eval(err)))
+		dev->stats.tx_errors++;
+	else
+		ret = NET_XMIT_SUCCESS;
+	goto out_xmit;
+out_drop:
+	dev->stats.tx_errors++;
+	kfree_skb(skb);
+out_xmit:
+	return ret;
+}
+#else
+static int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev)
+{
+	kfree_skb(skb);
+	return NET_XMIT_DROP;
+}
+#endif /* CONFIG_IPV6 */
+
+#if IS_ENABLED(CONFIG_INET)
+static int bpf_out_neigh_v4(struct net *net, struct sk_buff *skb)
+{
+	struct dst_entry *dst = skb_dst(skb);
+	struct rtable *rt = container_of(dst, struct rtable, dst);
+	struct net_device *dev = dst->dev;
+	u32 hh_len = LL_RESERVED_SPACE(dev);
+	struct neighbour *neigh;
+	bool is_v6gw = false;
+
+	if (dev_xmit_recursion()) {
+		net_crit_ratelimited("bpf: recursion limit reached on datapath, buggy bpf program?\n");
+		goto out_drop;
+	}
+
+	skb->dev = dev;
+	skb->tstamp = 0;
+
+	if (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {
+		struct sk_buff *skb2;
+
+		skb2 = skb_realloc_headroom(skb, hh_len);
+		if (unlikely(!skb2)) {
+			kfree_skb(skb);
+			return -ENOMEM;
+		}
+		if (skb->sk)
+			skb_set_owner_w(skb2, skb->sk);
+		consume_skb(skb);
+		skb = skb2;
+	}
+
+	rcu_read_lock_bh();
+	neigh = ip_neigh_for_gw(rt, skb, &is_v6gw);
+	if (likely(!IS_ERR(neigh))) {
+		int ret;
+
+		sock_confirm_neigh(skb, neigh);
+		dev_xmit_recursion_inc();
+		ret = neigh_output(neigh, skb, is_v6gw);
+		dev_xmit_recursion_dec();
+		rcu_read_unlock_bh();
+		return ret;
+	}
+	rcu_read_unlock_bh();
+out_drop:
+	kfree_skb(skb);
+	return -ENETDOWN;
+}
+
+static int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev)
+{
+	const struct iphdr *ip4h = ip_hdr(skb);
+	struct net *net = dev_net(dev);
+	int err, ret = NET_XMIT_DROP;
+	struct rtable *rt;
+	struct flowi4 fl4 = {
+		.flowi4_flags	= FLOWI_FLAG_ANYSRC,
+		.flowi4_mark	= skb->mark,
+		.flowi4_tos	= RT_TOS(ip4h->tos),
+		.flowi4_oif	= dev->ifindex,
+		.flowi4_proto	= ip4h->protocol,
+		.daddr		= ip4h->daddr,
+		.saddr		= ip4h->saddr,
+	};
+
+	rt = ip_route_output_flow(net, &fl4, NULL);
+	if (IS_ERR(rt))
+		goto out_drop;
+	if (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {
+		ip_rt_put(rt);
+		goto out_drop;
+	}
+
+	skb_dst_set(skb, &rt->dst);
+
+	err = bpf_out_neigh_v4(net, skb);
+	if (unlikely(net_xmit_eval(err)))
+		dev->stats.tx_errors++;
+	else
+		ret = NET_XMIT_SUCCESS;
+	goto out_xmit;
+out_drop:
+	dev->stats.tx_errors++;
+	kfree_skb(skb);
+out_xmit:
+	return ret;
+}
+#else
+static int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev)
+{
+	kfree_skb(skb);
+	return NET_XMIT_DROP;
+}
+#endif /* CONFIG_INET */
+
+static int __bpf_redirect_neigh(struct sk_buff *skb, struct net_device *dev)
+{
+	struct ethhdr *ethh = eth_hdr(skb);
+
+	if (unlikely(skb->mac_header >= skb->network_header))
+		goto out;
+	bpf_push_mac_rcsum(skb);
+	if (is_multicast_ether_addr(ethh->h_dest))
+		goto out;
+
+	skb_pull(skb, sizeof(*ethh));
+	skb_unset_mac_header(skb);
+	skb_reset_network_header(skb);
+
+	if (skb->protocol == htons(ETH_P_IP))
+		return __bpf_redirect_neigh_v4(skb, dev);
+	else if (skb->protocol == htons(ETH_P_IPV6))
+		return __bpf_redirect_neigh_v6(skb, dev);
+out:
+	kfree_skb(skb);
+	return -ENOTSUPP;
+}
+
+/* Internal, non-exposed redirect flags. */
+enum {
+	BPF_F_NEIGH = (1ULL << 1),
+#define BPF_F_REDIRECT_INTERNAL	(BPF_F_NEIGH)
+};
+
 BPF_CALL_3(bpf_clone_redirect, struct sk_buff *, skb, u32, ifindex, u64, flags)
 {
 	struct net_device *dev;
 	struct sk_buff *clone;
 	int ret;
 
-	if (unlikely(flags & ~(BPF_F_INGRESS)))
+	if (unlikely(flags & (~(BPF_F_INGRESS) | BPF_F_REDIRECT_INTERNAL)))
 		return -EINVAL;
 
 	dev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);
@@ -2216,23 +2436,11 @@ static const struct bpf_func_proto bpf_clone_redirect_proto = {
 DEFINE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);
 EXPORT_PER_CPU_SYMBOL_GPL(bpf_redirect_info);
 
-BPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)
-{
-	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
-
-	if (unlikely(flags & ~(BPF_F_INGRESS)))
-		return TC_ACT_SHOT;
-
-	ri->flags = flags;
-	ri->tgt_index = ifindex;
-
-	return TC_ACT_REDIRECT;
-}
-
 int skb_do_redirect(struct sk_buff *skb)
 {
 	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
 	struct net_device *dev;
+	u32 flags = ri->flags;
 
 	dev = dev_get_by_index_rcu(dev_net(skb->dev), ri->tgt_index);
 	ri->tgt_index = 0;
@@ -2241,7 +2449,22 @@ int skb_do_redirect(struct sk_buff *skb)
 		return -EINVAL;
 	}
 
-	return __bpf_redirect(skb, dev, ri->flags);
+	return flags & BPF_F_NEIGH ?
+	       __bpf_redirect_neigh(skb, dev) :
+	       __bpf_redirect(skb, dev, flags);
+}
+
+BPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)
+{
+	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+
+	if (unlikely(flags & (~(BPF_F_INGRESS) | BPF_F_REDIRECT_INTERNAL)))
+		return TC_ACT_SHOT;
+
+	ri->flags = flags;
+	ri->tgt_index = ifindex;
+
+	return TC_ACT_REDIRECT;
 }
 
 static const struct bpf_func_proto bpf_redirect_proto = {
@@ -2252,6 +2475,27 @@ static const struct bpf_func_proto bpf_redirect_proto = {
 	.arg2_type      = ARG_ANYTHING,
 };
 
+BPF_CALL_2(bpf_redirect_neigh, u32, ifindex, u64, flags)
+{
+	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+
+	if (unlikely(flags))
+		return TC_ACT_SHOT;
+
+	ri->flags = BPF_F_NEIGH;
+	ri->tgt_index = ifindex;
+
+	return TC_ACT_REDIRECT;
+}
+
+static const struct bpf_func_proto bpf_redirect_neigh_proto = {
+	.func		= bpf_redirect_neigh,
+	.gpl_only	= false,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_ANYTHING,
+	.arg2_type	= ARG_ANYTHING,
+};
+
 BPF_CALL_2(bpf_msg_apply_bytes, struct sk_msg *, msg, u32, bytes)
 {
 	msg->apply_bytes = bytes;
@@ -6469,6 +6713,8 @@ tc_cls_act_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
 		return bpf_get_skb_set_tunnel_proto(func_id);
 	case BPF_FUNC_redirect:
 		return &bpf_redirect_proto;
+	case BPF_FUNC_redirect_neigh:
+		return &bpf_redirect_neigh_proto;
 	case BPF_FUNC_get_route_realm:
 		return &bpf_get_route_realm_proto;
 	case BPF_FUNC_get_hash_recalc:
* Unmerged path tools/include/uapi/linux/bpf.h
