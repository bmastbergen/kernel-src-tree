xsk: Rearrange internal structs for better performance

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit 8ef4e27eb3f03edfbfbe5657b8061f2a47757037
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/8ef4e27e.failed

Rearrange the xdp_sock, xdp_umem and xsk_buff_pool structures so
that they get smaller and align better to the cache lines. In the
previous commits of this patch set, these structs have been
reordered with the focus on functionality and simplicity, not
performance. This patch improves throughput performance by around
3%.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/1598603189-32145-10-git-send-email-magnus.karlsson@intel.com
(cherry picked from commit 8ef4e27eb3f03edfbfbe5657b8061f2a47757037)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xdp_sock.h
#	include/net/xsk_buff_pool.h
diff --cc include/net/xdp_sock.h
index c9d87cc40c11,1a9559c0cbdd..000000000000
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@@ -24,19 -22,15 +24,31 @@@ struct xdp_umem 
  	u64 size;
  	u32 headroom;
  	u32 chunk_size;
++<<<<<<< HEAD
 +	struct user_struct *user;
 +	refcount_t users;
 +	struct work_struct work;
 +	struct page **pgs;
 +	u32 npgs;
 +	u16 queue_id;
 +	u8 need_wakeup;
 +	u8 flags;
 +	int id;
 +	struct net_device *dev;
 +	bool zc;
 +	spinlock_t xsk_tx_list_lock;
 +	struct list_head xsk_tx_list;
++=======
+ 	u32 chunks;
+ 	u32 npgs;
+ 	struct user_struct *user;
+ 	refcount_t users;
+ 	u8 flags;
+ 	bool zc;
+ 	struct page **pgs;
+ 	int id;
+ 	struct list_head xsk_dma_list;
++>>>>>>> 8ef4e27eb3f0 (xsk: Rearrange internal structs for better performance)
  };
  
  struct xsk_map {
@@@ -59,10 -54,9 +71,9 @@@ struct xdp_sock 
  		XSK_BOUND,
  		XSK_UNBOUND,
  	} state;
- 	/* Protects multiple processes in the control path */
- 	struct mutex mutex;
+ 
  	struct xsk_queue *tx ____cacheline_aligned_in_smp;
 -	struct list_head tx_list;
 +	struct list_head list;
  	/* Mutual exclusion of NAPI TX thread and sendmsg error paths
  	 * in the SKB destructor callback.
  	 */
@@@ -77,6 -71,10 +88,13 @@@
  	struct list_head map_list;
  	/* Protects map_list */
  	spinlock_t map_list_lock;
++<<<<<<< HEAD
++=======
+ 	/* Protects multiple processes in the control path */
+ 	struct mutex mutex;
+ 	struct xsk_queue *fq_tmp; /* Only as tmp storage before bind */
+ 	struct xsk_queue *cq_tmp; /* Only as tmp storage before bind */
++>>>>>>> 8ef4e27eb3f0 (xsk: Rearrange internal structs for better performance)
  };
  
  #ifdef CONFIG_XDP_SOCKETS
diff --cc include/net/xsk_buff_pool.h
index 6842990e2712,38d03a64c9ea..000000000000
--- a/include/net/xsk_buff_pool.h
+++ b/include/net/xsk_buff_pool.h
@@@ -26,9 -28,36 +26,31 @@@ struct xdp_buff_xsk 
  	struct list_head free_list_node;
  };
  
 -struct xsk_dma_map {
 -	dma_addr_t *dma_pages;
 -	struct device *dev;
 -	struct net_device *netdev;
 -	refcount_t users;
 -	struct list_head list; /* Protected by the RTNL_LOCK */
 -	u32 dma_pages_cnt;
 -	bool dma_need_sync;
 -};
 -
  struct xsk_buff_pool {
++<<<<<<< HEAD
 +	struct xsk_queue *fq;
 +	struct list_head free_list;
++=======
+ 	/* Members only used in the control path first. */
+ 	struct device *dev;
+ 	struct net_device *netdev;
+ 	struct list_head xsk_tx_list;
+ 	/* Protects modifications to the xsk_tx_list */
+ 	spinlock_t xsk_tx_list_lock;
+ 	refcount_t users;
+ 	struct xdp_umem *umem;
+ 	struct work_struct work;
+ 	struct list_head free_list;
+ 	u32 heads_cnt;
+ 	u16 queue_id;
+ 
+ 	/* Data path members as close to free_heads at the end as possible. */
+ 	struct xsk_queue *fq ____cacheline_aligned_in_smp;
+ 	struct xsk_queue *cq;
+ 	/* For performance reasons, each buff pool has its own array of dma_pages
+ 	 * even when they are identical.
+ 	 */
++>>>>>>> 8ef4e27eb3f0 (xsk: Rearrange internal structs for better performance)
  	dma_addr_t *dma_pages;
  	struct xdp_buff_xsk *heads;
  	u64 chunk_mask;
@@@ -40,10 -68,11 +61,18 @@@
  	u32 headroom;
  	u32 chunk_size;
  	u32 frame_len;
++<<<<<<< HEAD
 +	bool dma_need_sync;
 +	bool unaligned;
 +	void *addrs;
 +	struct device *dev;
++=======
+ 	u8 cached_need_wakeup;
+ 	bool uses_need_wakeup;
+ 	bool dma_need_sync;
+ 	bool unaligned;
+ 	void *addrs;
++>>>>>>> 8ef4e27eb3f0 (xsk: Rearrange internal structs for better performance)
  	struct xdp_buff_xsk *free_heads[];
  };
  
* Unmerged path include/net/xdp_sock.h
* Unmerged path include/net/xsk_buff_pool.h
