locking: Fix typos in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit e2db7592be8e83df47519116621411e1056b21c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/e2db7592.failed

Fix ~16 single-word typos in locking code comments.

	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Paul E. McKenney <paulmck@kernel.org>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit e2db7592be8e83df47519116621411e1056b21c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index 92742ec5bc7e,0e97287891db..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -1584,58 -1686,106 +1584,81 @@@ static int __bfs(struct lock_list *sour
  
  	lockdep_assert_locked();
  
 -	__cq_init(cq);
 -	__cq_enqueue(cq, source_entry);
 -
 -	while ((lock = __bfs_next(lock, offset)) || (lock = __cq_dequeue(cq))) {
 -		if (!lock->class)
 -			return BFS_EINVALIDNODE;
 +	if (match(source_entry, data)) {
 +		*target_entry = source_entry;
 +		ret = 0;
 +		goto exit;
 +	}
  
 -		/*
 -		 * Step 1: check whether we already finish on this one.
 -		 *
 -		 * If we have visited all the dependencies from this @lock to
 -		 * others (iow, if we have visited all lock_list entries in
 -		 * @lock->class->locks_{after,before}) we skip, otherwise go
 -		 * and visit all the dependencies in the list and mark this
 -		 * list accessed.
 -		 */
 -		if (lock_accessed(lock))
 -			continue;
 -		else
 -			mark_lock_accessed(lock);
 +	head = get_dep_list(source_entry, offset);
 +	if (list_empty(head))
 +		goto exit;
  
 -		/*
 -		 * Step 2: check whether prev dependency and this form a strong
 -		 *         dependency path.
 -		 */
 -		if (lock->parent) { /* Parent exists, check prev dependency */
 -			u8 dep = lock->dep;
 -			bool prev_only_xr = lock->parent->only_xr;
 -
 -			/*
 -			 * Mask out all -(S*)-> if we only have *R in previous
 -			 * step, because -(*R)-> -(S*)-> don't make up a strong
 -			 * dependency.
 -			 */
 -			if (prev_only_xr)
 -				dep &= ~(DEP_SR_MASK | DEP_SN_MASK);
 +	__cq_init(cq);
 +	__cq_enqueue(cq, source_entry);
  
 -			/* If nothing left, we skip */
 -			if (!dep)
 -				continue;
 +	while ((lock = __cq_dequeue(cq))) {
  
 -			/* If there are only -(*R)-> left, set that for the next step */
 -			lock->only_xr = !(dep & (DEP_SN_MASK | DEP_EN_MASK));
 +		if (!lock->class) {
 +			ret = -2;
 +			goto exit;
  		}
  
++<<<<<<< HEAD
++=======
+ 		/*
+ 		 * Step 3: we haven't visited this and there is a strong
+ 		 *         dependency path to this, so check with @match.
+ 		 *         If @skip is provide and returns true, we skip this
+ 		 *         lock (and any path this lock is in).
+ 		 */
+ 		if (skip && skip(lock, data))
+ 			continue;
+ 
+ 		if (match(lock, data)) {
+ 			*target_entry = lock;
+ 			return BFS_RMATCH;
+ 		}
+ 
+ 		/*
+ 		 * Step 4: if not match, expand the path by adding the
+ 		 *         forward or backwards dependencies in the search
+ 		 *
+ 		 */
+ 		first = true;
++>>>>>>> e2db7592be8e (locking: Fix typos in comments)
  		head = get_dep_list(lock, offset);
 -		list_for_each_entry_rcu(entry, head, entry) {
 -			visit_lock_entry(entry, lock);
  
 -			/*
 -			 * Note we only enqueue the first of the list into the
 -			 * queue, because we can always find a sibling
 -			 * dependency from one (see __bfs_next()), as a result
 -			 * the space of queue is saved.
 -			 */
 -			if (!first)
 -				continue;
 -
 -			first = false;
 -
 -			if (__cq_enqueue(cq, entry))
 -				return BFS_EQUEUEFULL;
 +		list_for_each_entry_rcu(entry, head, entry) {
 +			if (!lock_accessed(entry)) {
 +				unsigned int cq_depth;
 +				mark_lock_accessed(entry, lock);
 +				if (match(entry, data)) {
 +					*target_entry = entry;
 +					ret = 0;
 +					goto exit;
 +				}
  
 -			cq_depth = __cq_get_elem_count(cq);
 -			if (max_bfs_queue_depth < cq_depth)
 -				max_bfs_queue_depth = cq_depth;
 +				if (__cq_enqueue(cq, entry)) {
 +					ret = -1;
 +					goto exit;
 +				}
 +				cq_depth = __cq_get_elem_count(cq);
 +				if (max_bfs_queue_depth < cq_depth)
 +					max_bfs_queue_depth = cq_depth;
 +			}
  		}
  	}
 -
 -	return BFS_RNOMATCH;
 +exit:
 +	return ret;
  }
  
 -static inline enum bfs_result
 -__bfs_forwards(struct lock_list *src_entry,
 -	       void *data,
 -	       bool (*match)(struct lock_list *entry, void *data),
 -	       bool (*skip)(struct lock_list *entry, void *data),
 -	       struct lock_list **target_entry)
 +static inline int __bfs_forwards(struct lock_list *src_entry,
 +			void *data,
 +			int (*match)(struct lock_list *entry, void *data),
 +			struct lock_list **target_entry)
  {
 -	return __bfs(src_entry, data, match, skip, target_entry,
 +	return __bfs(src_entry, data, match, target_entry,
  		     offsetof(struct lock_class, locks_after));
  
  }
@@@ -1753,9 -1905,66 +1776,39 @@@ print_circular_bug_header(struct lock_l
  	print_circular_bug_entry(entry, depth);
  }
  
++<<<<<<< HEAD
 +static inline int class_equal(struct lock_list *entry, void *data)
++=======
+ /*
+  * We are about to add A -> B into the dependency graph, and in __bfs() a
+  * strong dependency path A -> .. -> B is found: hlock_class equals
+  * entry->class.
+  *
+  * If A -> .. -> B can replace A -> B in any __bfs() search (means the former
+  * is _stronger_ than or equal to the latter), we consider A -> B as redundant.
+  * For example if A -> .. -> B is -(EN)-> (i.e. A -(E*)-> .. -(*N)-> B), and A
+  * -> B is -(ER)-> or -(EN)->, then we don't need to add A -> B into the
+  * dependency graph, as any strong path ..-> A -> B ->.. we can get with
+  * having dependency A -> B, we could already get a equivalent path ..-> A ->
+  * .. -> B -> .. with A -> .. -> B. Therefore A -> B is redundant.
+  *
+  * We need to make sure both the start and the end of A -> .. -> B is not
+  * weaker than A -> B. For the start part, please see the comment in
+  * check_redundant(). For the end part, we need:
+  *
+  * Either
+  *
+  *     a) A -> B is -(*R)-> (everything is not weaker than that)
+  *
+  * or
+  *
+  *     b) A -> .. -> B is -(*N)-> (nothing is stronger than this)
+  *
+  */
+ static inline bool hlock_equal(struct lock_list *entry, void *data)
++>>>>>>> e2db7592be8e (locking: Fix typos in comments)
  {
 -	struct held_lock *hlock = (struct held_lock *)data;
 -
 -	return hlock_class(hlock) == entry->class && /* Found A -> .. -> B */
 -	       (hlock->read == 2 ||  /* A -> B is -(*R)-> */
 -		!entry->only_xr); /* A -> .. -> B is -(*N)-> */
 -}
 -
 -/*
 - * We are about to add B -> A into the dependency graph, and in __bfs() a
 - * strong dependency path A -> .. -> B is found: hlock_class equals
 - * entry->class.
 - *
 - * We will have a deadlock case (conflict) if A -> .. -> B -> A is a strong
 - * dependency cycle, that means:
 - *
 - * Either
 - *
 - *     a) B -> A is -(E*)->
 - *
 - * or
 - *
 - *     b) A -> .. -> B is -(*N)-> (i.e. A -> .. -(*N)-> B)
 - *
 - * as then we don't have -(*R)-> -(S*)-> in the cycle.
 - */
 -static inline bool hlock_conflict(struct lock_list *entry, void *data)
 -{
 -	struct held_lock *hlock = (struct held_lock *)data;
 -
 -	return hlock_class(hlock) == entry->class && /* Found A -> .. -> B */
 -	       (hlock->read == 0 || /* B -> A is -(E*)-> */
 -		!entry->only_xr); /* A -> .. -> B is -(*N)-> */
 +	return entry->class == data;
  }
  
  static noinline void print_circular_bug(struct lock_list *this,
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index 099c78fcf62d..f82471f6788b 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -22,7 +22,7 @@
  * assembler to insert a extra (16-bit) IT instruction, depending on the
  * presence or absence of neighbouring conditional instructions.
  *
- * To avoid this unpredictableness, an approprite IT is inserted explicitly:
+ * To avoid this unpredictability, an appropriate IT is inserted explicitly:
  * the assembler won't change IT instructions which are explicitly present
  * in the input.
  */
diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c251a3aab4f1..b5cb79fa919a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -154,7 +154,7 @@ extern void lockdep_set_selftest_task(struct task_struct *task);
 extern void lockdep_init_task(struct task_struct *task);
 
 /*
- * Split the recrursion counter in two to readily detect 'off' vs recursion.
+ * Split the recursion counter in two to readily detect 'off' vs recursion.
  */
 #define LOCKDEP_RECURSION_BITS	16
 #define LOCKDEP_OFF		(1U << LOCKDEP_RECURSION_BITS)
diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index da5c425c35e7..3a18ff1c4ade 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -102,7 +102,7 @@ do {								\
 
 /*
  * This is the same regardless of which rwsem implementation that is being used.
- * It is just a heuristic meant to be called by somebody alreadying holding the
+ * It is just a heuristic meant to be called by somebody already holding the
  * rwsem to see if somebody from an incompatible type is wanting access to the
  * lock.
  */
* Unmerged path kernel/locking/lockdep.c
diff --git a/kernel/locking/lockdep_proc.c b/kernel/locking/lockdep_proc.c
index 1b7f187aa020..e1c053a147f1 100644
--- a/kernel/locking/lockdep_proc.c
+++ b/kernel/locking/lockdep_proc.c
@@ -348,7 +348,7 @@ static int lockdep_stats_show(struct seq_file *m, void *v)
 			debug_locks);
 
 	/*
-	 * Zappped classes and lockdep data buffers reuse statistics.
+	 * Zapped classes and lockdep data buffers reuse statistics.
 	 */
 	seq_puts(m, "\n");
 	seq_printf(m, " zapped classes:                %11lu\n",
diff --git a/kernel/locking/mcs_spinlock.h b/kernel/locking/mcs_spinlock.h
index 5e10153b4d3c..85251d8771d9 100644
--- a/kernel/locking/mcs_spinlock.h
+++ b/kernel/locking/mcs_spinlock.h
@@ -7,7 +7,7 @@
  * The MCS lock (proposed by Mellor-Crummey and Scott) is a simple spin-lock
  * with the desirable properties of being fair, and with each cpu trying
  * to acquire the lock spinning on a local variable.
- * It avoids expensive cache bouncings that common test-and-set spin-lock
+ * It avoids expensive cache bounces that common test-and-set spin-lock
  * implementations incur.
  */
 #ifndef __LINUX_MCS_SPINLOCK_H
diff --git a/kernel/locking/mutex.c b/kernel/locking/mutex.c
index 30414376a31d..f1d88196fbbe 100644
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -101,7 +101,7 @@ static inline unsigned long __owner_flags(unsigned long owner)
 }
 
 /*
- * Trylock variant that retuns the owning task on failure.
+ * Trylock variant that returns the owning task on failure.
  */
 static inline struct task_struct *__mutex_trylock_or_owner(struct mutex *lock)
 {
@@ -216,7 +216,7 @@ __mutex_add_waiter(struct mutex *lock, struct mutex_waiter *waiter,
 
 /*
  * Give up ownership to a specific task, when @task = NULL, this is equivalent
- * to a regular unlock. Sets PICKUP on a handoff, clears HANDOF, preserves
+ * to a regular unlock. Sets PICKUP on a handoff, clears HANDOFF, preserves
  * WAITERS. Provides RELEASE semantics like a regular unlock, the
  * __mutex_trylock() provides a matching ACQUIRE semantics for the handoff.
  */
diff --git a/kernel/locking/osq_lock.c b/kernel/locking/osq_lock.c
index 1f7734949ac8..7df2a8d6f799 100644
--- a/kernel/locking/osq_lock.c
+++ b/kernel/locking/osq_lock.c
@@ -135,7 +135,7 @@ bool osq_lock(struct optimistic_spin_queue *lock)
 	 */
 
 	/*
-	 * Wait to acquire the lock or cancelation. Note that need_resched()
+	 * Wait to acquire the lock or cancellation. Note that need_resched()
 	 * will come with an IPI, which will wake smp_cond_load_relaxed() if it
 	 * is implemented with a monitor-wait. vcpu_is_preempted() relies on
 	 * polling, be careful.
@@ -160,7 +160,7 @@ bool osq_lock(struct optimistic_spin_queue *lock)
 
 		/*
 		 * We can only fail the cmpxchg() racing against an unlock(),
-		 * in which case we should observe @node->locked becomming
+		 * in which case we should observe @node->locked becoming
 		 * true.
 		 */
 		if (smp_load_acquire(&node->locked))
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index b9716eb0e70d..a09ba750ff04 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -725,7 +725,7 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	} else if (prerequeue_top_waiter == waiter) {
 		/*
 		 * The waiter was the top waiter on the lock, but is
-		 * no longer the top prority waiter. Replace waiter in
+		 * no longer the top priority waiter. Replace waiter in
 		 * the owner tasks pi waiters tree with the new top
 		 * (highest priority) waiter and adjust the priority
 		 * of the owner.
@@ -1213,7 +1213,7 @@ static void rt_mutex_handle_deadlock(int res, int detect_deadlock,
 		return;
 
 	/*
-	 * Yell lowdly and stop the task right here.
+	 * Yell loudly and stop the task right here.
 	 */
 	rt_mutex_print_deadlock(w);
 	while (1) {
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index 21b2c89becdb..ac86cf00dcfc 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -827,7 +827,7 @@ static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
 		 *    we try to get it. The new owner may be a spinnable
 		 *    writer.
 		 *
-		 * To take advantage of two scenarios listed agove, the RT
+		 * To take advantage of two scenarios listed above, the RT
 		 * task is made to retry one more time to see if it can
 		 * acquire the lock or continue spinning on the new owning
 		 * writer. Of course, if the time lag is long enough or the
diff --git a/kernel/locking/spinlock.c b/kernel/locking/spinlock.c
index 0ff08380f531..c8d7ad9fb9b2 100644
--- a/kernel/locking/spinlock.c
+++ b/kernel/locking/spinlock.c
@@ -58,10 +58,10 @@ EXPORT_PER_CPU_SYMBOL(__mmiowb_state);
 /*
  * We build the __lock_function inlines here. They are too large for
  * inlining all over the place, but here is only one user per function
- * which embedds them into the calling _lock_function below.
+ * which embeds them into the calling _lock_function below.
  *
  * This could be a long-held lock. We both prepare to spin for a long
- * time (making _this_ CPU preemptable if possible), and we also signal
+ * time (making _this_ CPU preemptible if possible), and we also signal
  * towards that other CPU that it should break the lock ASAP.
  */
 #define BUILD_LOCK_OPS(op, locktype)					\
