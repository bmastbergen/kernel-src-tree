xsk: Move addrs from buffer pool to umem

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit 7f7ffa4e9c38f01d380ed9df6adb238fd5e6eea5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/7f7ffa4e.failed

Replicate the addrs pointer in the buffer pool to the umem. This mapping
will be the same for all buffer pools sharing the same umem. In the
buffer pool we leave the addrs pointer for performance reasons.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/1598603189-32145-8-git-send-email-magnus.karlsson@intel.com
(cherry picked from commit 7f7ffa4e9c38f01d380ed9df6adb238fd5e6eea5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xdp_sock.h
#	net/xdp/xdp_umem.c
#	net/xdp/xsk_buff_pool.c
diff --cc include/net/xdp_sock.h
index c9d87cc40c11,126d24364b5a..000000000000
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@@ -18,9 -18,7 +18,13 @@@ struct xsk_queue
  struct xdp_buff;
  
  struct xdp_umem {
++<<<<<<< HEAD
 +	struct xsk_queue *fq;
 +	struct xsk_queue *cq;
 +	struct xsk_buff_pool *pool;
++=======
+ 	void *addrs;
++>>>>>>> 7f7ffa4e9c38 (xsk: Move addrs from buffer pool to umem)
  	u64 size;
  	u32 headroom;
  	u32 chunk_size;
diff --cc net/xdp/xdp_umem.c
index fb8d9af5bc04,77604c30aa0f..000000000000
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@@ -195,25 -39,27 +195,44 @@@ static void xdp_umem_unaccount_pages(st
  	}
  }
  
+ static void xdp_umem_addr_unmap(struct xdp_umem *umem)
+ {
+ 	vunmap(umem->addrs);
+ 	umem->addrs = NULL;
+ }
+ 
+ static int xdp_umem_addr_map(struct xdp_umem *umem, struct page **pages,
+ 			     u32 nr_pages)
+ {
+ 	umem->addrs = vmap(pages, nr_pages, VM_MAP, PAGE_KERNEL);
+ 	if (!umem->addrs)
+ 		return -ENOMEM;
+ 	return 0;
+ }
+ 
  static void xdp_umem_release(struct xdp_umem *umem)
  {
 -	umem->zc = false;
 +	rtnl_lock();
 +	xdp_umem_clear_dev(umem);
 +	rtnl_unlock();
 +
  	ida_simple_remove(&umem_ida, umem->id);
  
++<<<<<<< HEAD
 +	if (umem->fq) {
 +		xskq_destroy(umem->fq);
 +		umem->fq = NULL;
 +	}
 +
 +	if (umem->cq) {
 +		xskq_destroy(umem->cq);
 +		umem->cq = NULL;
 +	}
 +
 +	xp_destroy(umem->pool);
++=======
+ 	xdp_umem_addr_unmap(umem);
++>>>>>>> 7f7ffa4e9c38 (xsk: Move addrs from buffer pool to umem)
  	xdp_umem_unpin_pages(umem);
  
  	xdp_umem_unaccount_pages(umem);
@@@ -372,15 -208,13 +391,25 @@@ static int xdp_umem_reg(struct xdp_ume
  	if (err)
  		goto out_account;
  
++<<<<<<< HEAD
 +	umem->pool = xp_create(umem->pgs, umem->npgs, chunks, chunk_size,
 +			       headroom, size, unaligned_chunks);
 +	if (!umem->pool) {
 +		err = -ENOMEM;
 +		goto out_pin;
 +	}
 +	return 0;
 +
 +out_pin:
++=======
+ 	err = xdp_umem_addr_map(umem, umem->pgs, umem->npgs);
+ 	if (err)
+ 		goto out_unpin;
+ 
+ 	return 0;
+ 
+ out_unpin:
++>>>>>>> 7f7ffa4e9c38 (xsk: Move addrs from buffer pool to umem)
  	xdp_umem_unpin_pages(umem);
  out_account:
  	xdp_umem_unaccount_pages(umem);
diff --cc net/xdp/xsk_buff_pool.c
index a2044c245215,c56387439f67..000000000000
--- a/net/xdp/xsk_buff_pool.c
+++ b/net/xdp/xsk_buff_pool.c
@@@ -2,23 -2,39 +2,9 @@@
  
  #include <net/xsk_buff_pool.h>
  #include <net/xdp_sock.h>
 -#include <net/xdp_sock_drv.h>
 -#include <linux/dma-direct.h>
 -#include <linux/dma-noncoherent.h>
 -#include <linux/swiotlb.h>
  
  #include "xsk_queue.h"
 -#include "xdp_umem.h"
 -#include "xsk.h"
 -
 -void xp_add_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)
 -{
 -	unsigned long flags;
 -
 -	if (!xs->tx)
 -		return;
 -
 -	spin_lock_irqsave(&pool->xsk_tx_list_lock, flags);
 -	list_add_rcu(&xs->tx_list, &pool->xsk_tx_list);
 -	spin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);
 -}
 -
 -void xp_del_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)
 -{
 -	unsigned long flags;
 -
 -	if (!xs->tx)
 -		return;
 -
 -	spin_lock_irqsave(&pool->xsk_tx_list_lock, flags);
 -	list_del_rcu(&xs->tx_list);
 -	spin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);
 -}
  
- static void xp_addr_unmap(struct xsk_buff_pool *pool)
- {
- 	vunmap(pool->addrs);
- }
- 
- static int xp_addr_map(struct xsk_buff_pool *pool,
- 		       struct page **pages, u32 nr_pages)
- {
- 	pool->addrs = vmap(pages, nr_pages, VM_MAP, PAGE_KERNEL);
- 	if (!pool->addrs)
- 		return -ENOMEM;
- 	return 0;
- }
- 
  void xp_destroy(struct xsk_buff_pool *pool)
  {
  	if (!pool)
@@@ -35,10 -49,10 +20,9 @@@ struct xsk_buff_pool *xp_create(struct 
  {
  	struct xsk_buff_pool *pool;
  	struct xdp_buff_xsk *xskb;
- 	int err;
  	u32 i;
  
 -	pool = kvzalloc(struct_size(pool, free_heads, umem->chunks),
 -			GFP_KERNEL);
 +	pool = kvzalloc(struct_size(pool, free_heads, chunks), GFP_KERNEL);
  	if (!pool)
  		goto out;
  
@@@ -46,15 -60,26 +30,29 @@@
  	if (!pool->heads)
  		goto out;
  
++<<<<<<< HEAD
 +	pool->chunk_mask = ~((u64)chunk_size - 1);
 +	pool->addrs_cnt = size;
 +	pool->heads_cnt = chunks;
 +	pool->free_heads_cnt = chunks;
 +	pool->headroom = headroom;
 +	pool->chunk_size = chunk_size;
 +	pool->unaligned = unaligned;
 +	pool->frame_len = chunk_size - headroom - XDP_PACKET_HEADROOM;
++=======
+ 	pool->chunk_mask = ~((u64)umem->chunk_size - 1);
+ 	pool->addrs_cnt = umem->size;
+ 	pool->heads_cnt = umem->chunks;
+ 	pool->free_heads_cnt = umem->chunks;
+ 	pool->headroom = umem->headroom;
+ 	pool->chunk_size = umem->chunk_size;
+ 	pool->unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;
+ 	pool->frame_len = umem->chunk_size - umem->headroom -
+ 		XDP_PACKET_HEADROOM;
+ 	pool->umem = umem;
+ 	pool->addrs = umem->addrs;
++>>>>>>> 7f7ffa4e9c38 (xsk: Move addrs from buffer pool to umem)
  	INIT_LIST_HEAD(&pool->free_list);
 -	INIT_LIST_HEAD(&pool->xsk_tx_list);
 -	spin_lock_init(&pool->xsk_tx_list_lock);
 -	refcount_set(&pool->users, 1);
 -
 -	pool->fq = xs->fq_tmp;
 -	pool->cq = xs->cq_tmp;
 -	xs->fq_tmp = NULL;
 -	xs->cq_tmp = NULL;
  
  	for (i = 0; i < pool->free_heads_cnt; i++) {
  		xskb = &pool->heads[i];
@@@ -63,9 -88,7 +61,13 @@@
  		pool->free_heads[i] = xskb;
  	}
  
++<<<<<<< HEAD
 +	err = xp_addr_map(pool, pages, nr_pages);
 +	if (!err)
 +		return pool;
++=======
+ 	return pool;
++>>>>>>> 7f7ffa4e9c38 (xsk: Move addrs from buffer pool to umem)
  
  out:
  	xp_destroy(pool);
* Unmerged path include/net/xdp_sock.h
* Unmerged path net/xdp/xdp_umem.c
* Unmerged path net/xdp/xsk_buff_pool.c
