xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit c4655761d3cf62bf5f86650e79349c1bfa5c6285
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/c4655761.failed

Rename the AF_XDP zero-copy driver interface functions to better
reflect what they do after the replacement of umems with buffer
pools in the previous commit. Mostly it is about replacing the
umem name from the function names with xsk_buff and also have
them take the a buffer pool pointer instead of a umem. The
various ring functions have also been renamed in the process so
that they have the same naming convention as the internal
functions in xsk_queue.h. This so that it will be clearer what
they do and also for consistency.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/1598603189-32145-3-git-send-email-magnus.karlsson@intel.com
(cherry picked from commit c4655761d3cf62bf5f86650e79349c1bfa5c6285)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_main.c
#	drivers/net/ethernet/intel/i40e/i40e_xsk.c
#	drivers/net/ethernet/intel/ice/ice_base.c
#	drivers/net/ethernet/intel/ice/ice_xsk.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
#	drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.h
#	drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	include/net/xdp_sock_drv.h
#	net/ethtool/channels.c
#	net/ethtool/ioctl.c
#	net/xdp/xdp_umem.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_main.c
index 8048edf980ca,05c6d3ea11e6..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@@ -3144,7 -3138,7 +3144,11 @@@ static struct xdp_umem *i40e_xsk_umem(s
  	if (!xdp_on || !test_bit(qid, ring->vsi->af_xdp_zc_qps))
  		return NULL;
  
++<<<<<<< HEAD
 +	return xdp_get_umem_from_qid(ring->vsi->netdev, qid);
++=======
+ 	return xsk_get_pool_from_qid(ring->vsi->netdev, qid);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
  
  /**
@@@ -3291,7 -3285,8 +3295,12 @@@ static int i40e_configure_rx_ring(struc
  		ret = i40e_alloc_rx_bi_zc(ring);
  		if (ret)
  			return ret;
++<<<<<<< HEAD
 +		ring->rx_buf_len = xsk_umem_get_rx_frame_size(ring->xsk_umem);
++=======
+ 		ring->rx_buf_len =
+ 		  xsk_pool_get_rx_frame_size(ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		/* For AF_XDP ZC, we disallow packets to span on
  		 * multiple buffers, thus letting us skip that
  		 * handling in the fast-path.
@@@ -3374,8 -3369,8 +3383,13 @@@
  	ring->tail = hw->hw_addr + I40E_QRX_TAIL(pf_q);
  	writel(0, ring->tail);
  
++<<<<<<< HEAD
 +	if (ring->xsk_umem) {
 +		xsk_buff_set_rxq_info(ring->xsk_umem, &ring->xdp_rxq);
++=======
+ 	if (ring->xsk_pool) {
+ 		xsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		ok = i40e_alloc_rx_buffers_zc(ring, I40E_DESC_UNUSED(ring));
  	} else {
  		ok = !i40e_alloc_rx_buffers(ring, I40E_DESC_UNUSED(ring));
diff --cc drivers/net/ethernet/intel/i40e/i40e_xsk.c
index fdb4bf6e6a23,95b9a7e280fa..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_xsk.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
@@@ -53,7 -55,7 +53,11 @@@ static int i40e_xsk_umem_enable(struct 
  	    qid >= netdev->real_num_tx_queues)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	err = xsk_buff_dma_map(umem, &vsi->back->pdev->dev, I40E_RX_DMA_ATTR);
++=======
+ 	err = xsk_pool_dma_map(pool, &vsi->back->pdev->dev, I40E_RX_DMA_ATTR);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	if (err)
  		return err;
  
@@@ -93,8 -96,8 +97,13 @@@ static int i40e_xsk_umem_disable(struc
  	bool if_running;
  	int err;
  
++<<<<<<< HEAD
 +	umem = xdp_get_umem_from_qid(netdev, qid);
 +	if (!umem)
++=======
+ 	pool = xsk_get_pool_from_qid(netdev, qid);
+ 	if (!pool)
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		return -EINVAL;
  
  	if_running = netif_running(vsi->netdev) && i40e_enabled_xdp_vsi(vsi);
@@@ -106,7 -109,7 +115,11 @@@
  	}
  
  	clear_bit(qid, vsi->af_xdp_zc_qps);
++<<<<<<< HEAD
 +	xsk_buff_dma_unmap(umem, I40E_RX_DMA_ATTR);
++=======
+ 	xsk_pool_dma_unmap(pool, I40E_RX_DMA_ATTR);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	if (if_running) {
  		err = i40e_queue_pair_enable(vsi, qid);
@@@ -191,7 -195,7 +204,11 @@@ bool i40e_alloc_rx_buffers_zc(struct i4
  	rx_desc = I40E_RX_DESC(rx_ring, ntu);
  	bi = i40e_rx_bi(rx_ring, ntu);
  	do {
++<<<<<<< HEAD
 +		xdp = xsk_buff_alloc(rx_ring->xsk_umem);
++=======
+ 		xdp = xsk_buff_alloc(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		if (!xdp) {
  			ok = false;
  			goto no_buffers;
@@@ -368,11 -362,11 +385,19 @@@ int i40e_clean_rx_irq_zc(struct i40e_ri
  	i40e_finalize_xdp_rx(rx_ring, xdp_xmit);
  	i40e_update_rx_stats(rx_ring, total_rx_bytes, total_rx_packets);
  
++<<<<<<< HEAD
 +	if (xsk_umem_uses_need_wakeup(rx_ring->xsk_umem)) {
 +		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
 +			xsk_set_rx_need_wakeup(rx_ring->xsk_umem);
 +		else
 +			xsk_clear_rx_need_wakeup(rx_ring->xsk_umem);
++=======
+ 	if (xsk_uses_need_wakeup(rx_ring->xsk_pool)) {
+ 		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
+ 			xsk_set_rx_need_wakeup(rx_ring->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		return (int)total_rx_packets;
  	}
@@@ -394,13 -389,16 +419,21 @@@ static bool i40e_xmit_zc(struct i40e_ri
  	dma_addr_t dma;
  
  	while (budget-- > 0) {
++<<<<<<< HEAD
 +		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &desc))
 +			break;
 +
 +		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_umem, desc.addr);
 +		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_umem, dma,
++=======
+ 		if (!xsk_tx_peek_desc(xdp_ring->xsk_pool, &desc))
+ 			break;
+ 
+ 		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc.addr);
+ 		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma,
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  						 desc.len);
  
 -		tx_bi = &xdp_ring->tx_bi[xdp_ring->next_to_use];
 -		tx_bi->bytecount = desc.len;
 -
  		tx_desc = I40E_TX_DESC(xdp_ring, xdp_ring->next_to_use);
  		tx_desc->buffer_addr = cpu_to_le64(dma);
  		tx_desc->cmd_type_offset_bsz =
@@@ -422,7 -420,7 +455,11 @@@
  						 I40E_TXD_QW1_CMD_SHIFT);
  		i40e_xdp_ring_update_tail(xdp_ring);
  
++<<<<<<< HEAD
 +		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
++=======
+ 		xsk_tx_release(xdp_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		i40e_update_tx_stats(xdp_ring, sent_frames, total_bytes);
  	}
  
@@@ -494,13 -492,13 +531,22 @@@ skip
  		tx_ring->next_to_clean -= tx_ring->count;
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(umem, xsk_frames);
++=======
+ 		xsk_tx_completed(bp, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	i40e_arm_wb(tx_ring, vsi, completed_frames);
  
  out_xmit:
++<<<<<<< HEAD
 +	if (xsk_umem_uses_need_wakeup(tx_ring->xsk_umem))
 +		xsk_set_tx_need_wakeup(tx_ring->xsk_umem);
++=======
+ 	if (xsk_uses_need_wakeup(tx_ring->xsk_pool))
+ 		xsk_set_tx_need_wakeup(tx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	return i40e_xmit_zc(tx_ring, I40E_DESC_UNUSED(tx_ring));
  }
@@@ -591,7 -589,7 +637,11 @@@ void i40e_xsk_clean_tx_ring(struct i40e
  	}
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(umem, xsk_frames);
++=======
+ 		xsk_tx_completed(bp, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
  
  /**
@@@ -606,7 -605,7 +656,11 @@@ bool i40e_xsk_any_rx_ring_enabled(struc
  	int i;
  
  	for (i = 0; i < vsi->num_queue_pairs; i++) {
++<<<<<<< HEAD
 +		if (xdp_get_umem_from_qid(netdev, i))
++=======
+ 		if (xsk_get_pool_from_qid(netdev, i))
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			return true;
  	}
  
diff --cc drivers/net/ethernet/intel/ice/ice_base.c
index 87008476d8fe,fe4320e2d1f2..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_base.c
+++ b/drivers/net/ethernet/intel/ice/ice_base.c
@@@ -313,7 -313,7 +313,11 @@@ int ice_setup_rx_ctx(struct ice_ring *r
  			xdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);
  
  			ring->rx_buf_len =
++<<<<<<< HEAD
 +				xsk_umem_get_rx_frame_size(ring->xsk_umem);
++=======
+ 				xsk_pool_get_rx_frame_size(ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			/* For AF_XDP ZC, we disallow packets to span on
  			 * multiple buffers, thus letting us skip that
  			 * handling in the fast-path.
@@@ -324,7 -324,7 +328,11 @@@
  							 NULL);
  			if (err)
  				return err;
++<<<<<<< HEAD
 +			xsk_buff_set_rxq_info(ring->xsk_umem, &ring->xdp_rxq);
++=======
+ 			xsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  			dev_info(dev, "Registered XDP mem model MEM_TYPE_XSK_BUFF_POOL on Rx ring %d\n",
  				 ring->q_index);
@@@ -417,9 -417,9 +425,15 @@@
  	ring->tail = hw->hw_addr + QRX_TAIL(pf_q);
  	writel(0, ring->tail);
  
++<<<<<<< HEAD
 +	if (ring->xsk_umem) {
 +		if (!xsk_buff_can_alloc(ring->xsk_umem, num_bufs)) {
 +			dev_warn(dev, "UMEM does not provide enough addresses to fill %d buffers on Rx ring %d\n",
++=======
+ 	if (ring->xsk_pool) {
+ 		if (!xsk_buff_can_alloc(ring->xsk_pool, num_bufs)) {
+ 			dev_warn(dev, "XSK buffer pool does not provide enough addresses to fill %d buffers on Rx ring %d\n",
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  				 num_bufs, ring->q_index);
  			dev_warn(dev, "Change Rx ring/fill queue size to avoid performance issues\n");
  
diff --cc drivers/net/ethernet/intel/ice/ice_xsk.c
index de6950876f56,dffef377c5cb..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@@ -305,14 -305,14 +305,19 @@@ static void ice_xsk_remove_umem(struct 
   *
   * Returns 0 on success, negative on failure
   */
 -static int ice_xsk_pool_disable(struct ice_vsi *vsi, u16 qid)
 +static int ice_xsk_umem_disable(struct ice_vsi *vsi, u16 qid)
  {
 -	if (!vsi->xsk_pools || qid >= vsi->num_xsk_pools ||
 -	    !vsi->xsk_pools[qid])
 +	if (!vsi->xsk_umems || qid >= vsi->num_xsk_umems ||
 +	    !vsi->xsk_umems[qid])
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	xsk_buff_dma_unmap(vsi->xsk_umems[qid], ICE_RX_DMA_ATTR);
 +	ice_xsk_remove_umem(vsi, qid);
++=======
+ 	xsk_pool_dma_unmap(vsi->xsk_pools[qid], ICE_RX_DMA_ATTR);
+ 	ice_xsk_remove_pool(vsi, qid);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	return 0;
  }
@@@ -342,13 -342,13 +347,17 @@@ ice_xsk_umem_enable(struct ice_vsi *vsi
  	if (err)
  		return err;
  
 -	if (vsi->xsk_pools && vsi->xsk_pools[qid])
 +	if (vsi->xsk_umems && vsi->xsk_umems[qid])
  		return -EBUSY;
  
 -	vsi->xsk_pools[qid] = pool;
 -	vsi->num_xsk_pools_used++;
 +	vsi->xsk_umems[qid] = umem;
 +	vsi->num_xsk_umems_used++;
  
++<<<<<<< HEAD
 +	err = xsk_buff_dma_map(vsi->xsk_umems[qid], ice_pf_to_dev(vsi->back),
++=======
+ 	err = xsk_pool_dma_map(vsi->xsk_pools[qid], ice_pf_to_dev(vsi->back),
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			       ICE_RX_DMA_ATTR);
  	if (err)
  		return err;
@@@ -425,7 -425,7 +434,11 @@@ bool ice_alloc_rx_bufs_zc(struct ice_ri
  	rx_buf = &rx_ring->rx_buf[ntu];
  
  	do {
++<<<<<<< HEAD
 +		rx_buf->xdp = xsk_buff_alloc(rx_ring->xsk_umem);
++=======
+ 		rx_buf->xdp = xsk_buff_alloc(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		if (!rx_buf->xdp) {
  			ret = true;
  			break;
@@@ -648,11 -645,11 +661,19 @@@ int ice_clean_rx_irq_zc(struct ice_rin
  	ice_finalize_xdp_rx(rx_ring, xdp_xmit);
  	ice_update_rx_ring_stats(rx_ring, total_rx_packets, total_rx_bytes);
  
++<<<<<<< HEAD
 +	if (xsk_umem_uses_need_wakeup(rx_ring->xsk_umem)) {
 +		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
 +			xsk_set_rx_need_wakeup(rx_ring->xsk_umem);
 +		else
 +			xsk_clear_rx_need_wakeup(rx_ring->xsk_umem);
++=======
+ 	if (xsk_uses_need_wakeup(rx_ring->xsk_pool)) {
+ 		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
+ 			xsk_set_rx_need_wakeup(rx_ring->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		return (int)total_rx_packets;
  	}
@@@ -685,11 -682,11 +706,19 @@@ static bool ice_xmit_zc(struct ice_rin
  
  		tx_buf = &xdp_ring->tx_buf[xdp_ring->next_to_use];
  
++<<<<<<< HEAD
 +		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &desc))
 +			break;
 +
 +		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_umem, desc.addr);
 +		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_umem, dma,
++=======
+ 		if (!xsk_tx_peek_desc(xdp_ring->xsk_pool, &desc))
+ 			break;
+ 
+ 		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc.addr);
+ 		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma,
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  						 desc.len);
  
  		tx_buf->bytecount = desc.len;
@@@ -706,7 -703,7 +735,11 @@@
  
  	if (tx_desc) {
  		ice_xdp_ring_update_tail(xdp_ring);
++<<<<<<< HEAD
 +		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
++=======
+ 		xsk_tx_release(xdp_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	}
  
  	return budget > 0 && work_done;
@@@ -780,10 -777,10 +813,17 @@@ bool ice_clean_tx_irq_zc(struct ice_rin
  	xdp_ring->next_to_clean = ntc;
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(xdp_ring->xsk_umem, xsk_frames);
 +
 +	if (xsk_umem_uses_need_wakeup(xdp_ring->xsk_umem))
 +		xsk_set_tx_need_wakeup(xdp_ring->xsk_umem);
++=======
+ 		xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
+ 
+ 	if (xsk_uses_need_wakeup(xdp_ring->xsk_pool))
+ 		xsk_set_tx_need_wakeup(xdp_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	ice_update_tx_ring_stats(xdp_ring, total_packets, total_bytes);
  	xmit_done = ice_xmit_zc(xdp_ring, ICE_DFLT_IRQ_WORK);
@@@ -899,5 -896,5 +939,9 @@@ void ice_xsk_clean_xdp_ring(struct ice_
  	}
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(xdp_ring->xsk_umem, xsk_frames);
++=======
+ 		xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index ee7da5cadab7,c4d41f8aad4a..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -3705,8 -3713,8 +3705,13 @@@ static void ixgbe_configure_srrctl(stru
  	srrctl = IXGBE_RX_HDR_SIZE << IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT;
  
  	/* configure the packet buffer length */
++<<<<<<< HEAD
 +	if (rx_ring->xsk_umem) {
 +		u32 xsk_buf_len = xsk_umem_get_rx_frame_size(rx_ring->xsk_umem);
++=======
+ 	if (rx_ring->xsk_pool) {
+ 		u32 xsk_buf_len = xsk_pool_get_rx_frame_size(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		/* If the MAC support setting RXDCTL.RLPML, the
  		 * SRRCTL[n].BSIZEPKT is set to PAGE_SIZE and
@@@ -4056,7 -4064,7 +4061,11 @@@ void ixgbe_configure_rx_ring(struct ixg
  		WARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
  						   MEM_TYPE_XSK_BUFF_POOL,
  						   NULL));
++<<<<<<< HEAD
 +		xsk_buff_set_rxq_info(ring->xsk_umem, &ring->xdp_rxq);
++=======
+ 		xsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	} else {
  		WARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
  						   MEM_TYPE_PAGE_SHARED, NULL));
@@@ -4111,8 -4119,8 +4120,13 @@@
  #endif
  	}
  
++<<<<<<< HEAD
 +	if (ring->xsk_umem && hw->mac.type != ixgbe_mac_82599EB) {
 +		u32 xsk_buf_len = xsk_umem_get_rx_frame_size(ring->xsk_umem);
++=======
+ 	if (ring->xsk_pool && hw->mac.type != ixgbe_mac_82599EB) {
+ 		u32 xsk_buf_len = xsk_pool_get_rx_frame_size(ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		rxdctl &= ~(IXGBE_RXDCTL_RLPMLMASK |
  			    IXGBE_RXDCTL_RLPML_EN);
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
index ec7121f352e2,6af34da45b8d..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
@@@ -17,11 -17,11 +17,15 @@@ struct xdp_umem *ixgbe_xsk_umem(struct 
  	if (!xdp_on || !test_bit(qid, adapter->af_xdp_zc_qps))
  		return NULL;
  
++<<<<<<< HEAD
 +	return xdp_get_umem_from_qid(adapter->netdev, qid);
++=======
+ 	return xsk_get_pool_from_qid(adapter->netdev, qid);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
  
 -static int ixgbe_xsk_pool_enable(struct ixgbe_adapter *adapter,
 -				 struct xsk_buff_pool *pool,
 +static int ixgbe_xsk_umem_enable(struct ixgbe_adapter *adapter,
 +				 struct xdp_umem *umem,
  				 u16 qid)
  {
  	struct net_device *netdev = adapter->netdev;
@@@ -35,7 -35,7 +39,11 @@@
  	    qid >= netdev->real_num_tx_queues)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	err = xsk_buff_dma_map(umem, &adapter->pdev->dev, IXGBE_RX_DMA_ATTR);
++=======
+ 	err = xsk_pool_dma_map(pool, &adapter->pdev->dev, IXGBE_RX_DMA_ATTR);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	if (err)
  		return err;
  
@@@ -59,13 -59,13 +67,18 @@@
  	return 0;
  }
  
 -static int ixgbe_xsk_pool_disable(struct ixgbe_adapter *adapter, u16 qid)
 +static int ixgbe_xsk_umem_disable(struct ixgbe_adapter *adapter, u16 qid)
  {
 -	struct xsk_buff_pool *pool;
 +	struct xdp_umem *umem;
  	bool if_running;
  
++<<<<<<< HEAD
 +	umem = xdp_get_umem_from_qid(adapter->netdev, qid);
 +	if (!umem)
++=======
+ 	pool = xsk_get_pool_from_qid(adapter->netdev, qid);
+ 	if (!pool)
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		return -EINVAL;
  
  	if_running = netif_running(adapter->netdev) &&
@@@ -75,7 -75,7 +88,11 @@@
  		ixgbe_txrx_ring_disable(adapter, qid);
  
  	clear_bit(qid, adapter->af_xdp_zc_qps);
++<<<<<<< HEAD
 +	xsk_buff_dma_unmap(umem, IXGBE_RX_DMA_ATTR);
++=======
+ 	xsk_pool_dma_unmap(pool, IXGBE_RX_DMA_ATTR);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	if (if_running)
  		ixgbe_txrx_ring_enable(adapter, qid);
@@@ -149,7 -150,7 +166,11 @@@ bool ixgbe_alloc_rx_buffers_zc(struct i
  	i -= rx_ring->count;
  
  	do {
++<<<<<<< HEAD
 +		bi->xdp = xsk_buff_alloc(rx_ring->xsk_umem);
++=======
+ 		bi->xdp = xsk_buff_alloc(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		if (!bi->xdp) {
  			ok = false;
  			break;
@@@ -344,11 -345,11 +365,19 @@@ int ixgbe_clean_rx_irq_zc(struct ixgbe_
  	q_vector->rx.total_packets += total_rx_packets;
  	q_vector->rx.total_bytes += total_rx_bytes;
  
++<<<<<<< HEAD
 +	if (xsk_umem_uses_need_wakeup(rx_ring->xsk_umem)) {
 +		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
 +			xsk_set_rx_need_wakeup(rx_ring->xsk_umem);
 +		else
 +			xsk_clear_rx_need_wakeup(rx_ring->xsk_umem);
++=======
+ 	if (xsk_uses_need_wakeup(rx_ring->xsk_pool)) {
+ 		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
+ 			xsk_set_rx_need_wakeup(rx_ring->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(rx_ring->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		return (int)total_rx_packets;
  	}
@@@ -387,12 -389,11 +416,20 @@@ static bool ixgbe_xmit_zc(struct ixgbe_
  			break;
  		}
  
++<<<<<<< HEAD
 +		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &desc))
 +			break;
 +
 +		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_umem, desc.addr);
 +		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_umem, dma,
 +						 desc.len);
++=======
+ 		if (!xsk_tx_peek_desc(pool, &desc))
+ 			break;
+ 
+ 		dma = xsk_buff_raw_get_dma(pool, desc.addr);
+ 		xsk_buff_raw_dma_sync_for_device(pool, dma, desc.len);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		tx_bi = &xdp_ring->tx_buffer_info[xdp_ring->next_to_use];
  		tx_bi->bytecount = desc.len;
@@@ -418,7 -419,7 +455,11 @@@
  
  	if (tx_desc) {
  		ixgbe_xdp_ring_update_tail(xdp_ring);
++<<<<<<< HEAD
 +		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
++=======
+ 		xsk_tx_release(pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	}
  
  	return !!budget && work_done;
@@@ -484,10 -485,10 +525,17 @@@ bool ixgbe_clean_xdp_tx_irq(struct ixgb
  	q_vector->tx.total_packets += total_packets;
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(umem, xsk_frames);
 +
 +	if (xsk_umem_uses_need_wakeup(tx_ring->xsk_umem))
 +		xsk_set_tx_need_wakeup(tx_ring->xsk_umem);
++=======
+ 		xsk_tx_completed(pool, xsk_frames);
+ 
+ 	if (xsk_uses_need_wakeup(pool))
+ 		xsk_set_tx_need_wakeup(pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	return ixgbe_xmit_zc(tx_ring, q_vector->tx.work_limit);
  }
@@@ -546,5 -547,5 +594,9 @@@ void ixgbe_xsk_clean_tx_ring(struct ixg
  	}
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(umem, xsk_frames);
++=======
+ 		xsk_tx_completed(pool, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
index 5b62d5904c1f,d521449186cc..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
@@@ -448,7 -445,7 +448,11 @@@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_c
  	} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(sq->umem, xsk_frames);
++=======
+ 		xsk_tx_completed(sq->xsk_pool, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	sq->stats->cqes += i;
  
@@@ -478,7 -475,7 +482,11 @@@ void mlx5e_free_xdpsq_descs(struct mlx5
  	}
  
  	if (xsk_frames)
++<<<<<<< HEAD
 +		xsk_umem_complete_tx(sq->umem, xsk_frames);
++=======
+ 		xsk_tx_completed(sq->xsk_pool, xsk_frames);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
  
  int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
index d147b2f13b54,7f88ccf67fdd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
@@@ -19,10 -19,10 +19,14 @@@ struct sk_buff *mlx5e_xsk_skb_from_cqe_
  					      struct mlx5e_wqe_frag_info *wi,
  					      u32 cqe_bcnt);
  
 -static inline int mlx5e_xsk_page_alloc_pool(struct mlx5e_rq *rq,
 +static inline int mlx5e_xsk_page_alloc_umem(struct mlx5e_rq *rq,
  					    struct mlx5e_dma_info *dma_info)
  {
++<<<<<<< HEAD
 +	dma_info->xsk = xsk_buff_alloc(rq->umem);
++=======
+ 	dma_info->xsk = xsk_buff_alloc(rq->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	if (!dma_info->xsk)
  		return -ENOMEM;
  
@@@ -38,13 -38,13 +42,23 @@@
  
  static inline bool mlx5e_xsk_update_rx_wakeup(struct mlx5e_rq *rq, bool alloc_err)
  {
++<<<<<<< HEAD
 +	if (!xsk_umem_uses_need_wakeup(rq->umem))
 +		return alloc_err;
 +
 +	if (unlikely(alloc_err))
 +		xsk_set_rx_need_wakeup(rq->umem);
 +	else
 +		xsk_clear_rx_need_wakeup(rq->umem);
++=======
+ 	if (!xsk_uses_need_wakeup(rq->xsk_pool))
+ 		return alloc_err;
+ 
+ 	if (unlikely(alloc_err))
+ 		xsk_set_rx_need_wakeup(rq->xsk_pool);
+ 	else
+ 		xsk_clear_rx_need_wakeup(rq->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	return false;
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.c
index f03bfdbe9a65,aa91cbdfe969..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.c
@@@ -87,7 -87,7 +87,11 @@@ bool mlx5e_xsk_tx(struct mlx5e_xdpsq *s
  			break;
  		}
  
++<<<<<<< HEAD
 +		if (!xsk_umem_consume_tx(umem, &desc)) {
++=======
+ 		if (!xsk_tx_peek_desc(pool, &desc)) {
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			/* TX will get stuck until something wakes it up by
  			 * triggering NAPI. Currently it's expected that the
  			 * application calls sendto() if there are consumed, but
@@@ -96,11 -96,11 +100,19 @@@
  			break;
  		}
  
++<<<<<<< HEAD
 +		xdptxd.dma_addr = xsk_buff_raw_get_dma(umem, desc.addr);
 +		xdptxd.data = xsk_buff_raw_get_data(umem, desc.addr);
 +		xdptxd.len = desc.len;
 +
 +		xsk_buff_raw_dma_sync_for_device(umem, xdptxd.dma_addr, xdptxd.len);
++=======
+ 		xdptxd.dma_addr = xsk_buff_raw_get_dma(pool, desc.addr);
+ 		xdptxd.data = xsk_buff_raw_get_data(pool, desc.addr);
+ 		xdptxd.len = desc.len;
+ 
+ 		xsk_buff_raw_dma_sync_for_device(pool, xdptxd.dma_addr, xdptxd.len);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  		ret = INDIRECT_CALL_2(sq->xmit_xdp_frame, mlx5e_xmit_xdp_frame_mpwqe,
  				      mlx5e_xmit_xdp_frame, sq, &xdptxd, &xdpi, check_result);
@@@ -119,7 -119,7 +131,11 @@@
  			mlx5e_xdp_mpwqe_complete(sq);
  		mlx5e_xmit_xdp_doorbell(sq);
  
++<<<<<<< HEAD
 +		xsk_umem_consume_tx_done(umem);
++=======
+ 		xsk_tx_release(pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	}
  
  	return !(budget && work_done);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.h
index 39fa0a705856,a05085035f23..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.h
@@@ -15,13 -15,13 +15,23 @@@ bool mlx5e_xsk_tx(struct mlx5e_xdpsq *s
  
  static inline void mlx5e_xsk_update_tx_wakeup(struct mlx5e_xdpsq *sq)
  {
++<<<<<<< HEAD
 +	if (!xsk_umem_uses_need_wakeup(sq->umem))
 +		return;
 +
 +	if (sq->pc != sq->cc)
 +		xsk_clear_tx_need_wakeup(sq->umem);
 +	else
 +		xsk_set_tx_need_wakeup(sq->umem);
++=======
+ 	if (!xsk_uses_need_wakeup(sq->xsk_pool))
+ 		return;
+ 
+ 	if (sq->pc != sq->cc)
+ 		xsk_clear_tx_need_wakeup(sq->xsk_pool);
+ 	else
+ 		xsk_set_tx_need_wakeup(sq->xsk_pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  }
  
  #endif /* __MLX5_EN_XSK_TX_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
index 331ca2b0f8a4,3503e7711178..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
@@@ -11,21 -11,21 +11,29 @@@ static int mlx5e_xsk_map_umem(struct ml
  {
  	struct device *dev = priv->mdev->device;
  
++<<<<<<< HEAD:drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
 +	return xsk_buff_dma_map(umem, dev, 0);
++=======
+ 	return xsk_pool_dma_map(pool, dev, 0);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces):drivers/net/ethernet/mellanox/mlx5/core/en/xsk/pool.c
  }
  
 -static void mlx5e_xsk_unmap_pool(struct mlx5e_priv *priv,
 -				 struct xsk_buff_pool *pool)
 +static void mlx5e_xsk_unmap_umem(struct mlx5e_priv *priv,
 +				 struct xdp_umem *umem)
  {
++<<<<<<< HEAD:drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
 +	return xsk_buff_dma_unmap(umem, 0);
++=======
+ 	return xsk_pool_dma_unmap(pool, 0);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces):drivers/net/ethernet/mellanox/mlx5/core/en/xsk/pool.c
  }
  
 -static int mlx5e_xsk_get_pools(struct mlx5e_xsk *xsk)
 +static int mlx5e_xsk_get_umems(struct mlx5e_xsk *xsk)
  {
 -	if (!xsk->pools) {
 -		xsk->pools = kcalloc(MLX5E_MAX_NUM_CHANNELS,
 -				     sizeof(*xsk->pools), GFP_KERNEL);
 -		if (unlikely(!xsk->pools))
 +	if (!xsk->umems) {
 +		xsk->umems = kcalloc(MLX5E_MAX_NUM_CHANNELS,
 +				     sizeof(*xsk->umems), GFP_KERNEL);
 +		if (unlikely(!xsk->umems))
  			return -ENOMEM;
  	}
  
@@@ -55,23 -55,23 +63,33 @@@ static int mlx5e_xsk_add_umem(struct ml
  	return 0;
  }
  
 -static void mlx5e_xsk_remove_pool(struct mlx5e_xsk *xsk, u16 ix)
 +static void mlx5e_xsk_remove_umem(struct mlx5e_xsk *xsk, u16 ix)
  {
 -	xsk->pools[ix] = NULL;
 +	xsk->umems[ix] = NULL;
  
 -	mlx5e_xsk_put_pools(xsk);
 +	mlx5e_xsk_put_umems(xsk);
  }
  
 -static bool mlx5e_xsk_is_pool_sane(struct xsk_buff_pool *pool)
 +static bool mlx5e_xsk_is_umem_sane(struct xdp_umem *umem)
  {
++<<<<<<< HEAD:drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
 +	return xsk_umem_get_headroom(umem) <= 0xffff &&
 +		xsk_umem_get_chunk_size(umem) <= 0xffff;
++=======
+ 	return xsk_pool_get_headroom(pool) <= 0xffff &&
+ 		xsk_pool_get_chunk_size(pool) <= 0xffff;
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces):drivers/net/ethernet/mellanox/mlx5/core/en/xsk/pool.c
  }
  
 -void mlx5e_build_xsk_param(struct xsk_buff_pool *pool, struct mlx5e_xsk_param *xsk)
 +void mlx5e_build_xsk_param(struct xdp_umem *umem, struct mlx5e_xsk_param *xsk)
  {
++<<<<<<< HEAD:drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
 +	xsk->headroom = xsk_umem_get_headroom(umem);
 +	xsk->chunk_size = xsk_umem_get_chunk_size(umem);
++=======
+ 	xsk->headroom = xsk_pool_get_headroom(pool);
+ 	xsk->chunk_size = xsk_pool_get_chunk_size(pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces):drivers/net/ethernet/mellanox/mlx5/core/en/xsk/pool.c
  }
  
  static int mlx5e_xsk_enable_locked(struct mlx5e_priv *priv,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 34a481662f35,26834625556d..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -512,7 -477,7 +512,11 @@@ static int mlx5e_alloc_rq(struct mlx5e_
  	if (xsk) {
  		err = xdp_rxq_info_reg_mem_model(&rq->xdp_rxq,
  						 MEM_TYPE_XSK_BUFF_POOL, NULL);
++<<<<<<< HEAD
 +		xsk_buff_set_rxq_info(rq->umem, &rq->xdp_rxq);
++=======
+ 		xsk_pool_set_rxq_info(rq->xsk_pool, &rq->xdp_rxq);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	} else {
  		/* Create a page_pool and register it with rxq */
  		pp_params.order     = 0;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 41cd85afded4,57034c58b971..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -404,7 -407,7 +404,11 @@@ static int mlx5e_alloc_rx_wqes(struct m
  		 * allocating one-by-one, failing and moving frames to the
  		 * Reuse Ring.
  		 */
++<<<<<<< HEAD
 +		if (unlikely(!xsk_buff_can_alloc(rq->umem, pages_desired)))
++=======
+ 		if (unlikely(!xsk_buff_can_alloc(rq->xsk_pool, pages_desired)))
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			return -ENOMEM;
  	}
  
@@@ -502,8 -505,8 +506,13 @@@ static int mlx5e_alloc_rx_mpwqe(struct 
  	/* Check in advance that we have enough frames, instead of allocating
  	 * one-by-one, failing and moving frames to the Reuse Ring.
  	 */
++<<<<<<< HEAD
 +	if (rq->umem &&
 +	    unlikely(!xsk_buff_can_alloc(rq->umem, MLX5_MPWRQ_PAGES_PER_WQE))) {
++=======
+ 	if (rq->xsk_pool &&
+ 	    unlikely(!xsk_buff_can_alloc(rq->xsk_pool, MLX5_MPWRQ_PAGES_PER_WQE))) {
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  		err = -ENOMEM;
  		goto err;
  	}
diff --cc include/net/xdp_sock_drv.h
index ccf848f7efa4,a7c7d2eff860..000000000000
--- a/include/net/xdp_sock_drv.h
+++ b/include/net/xdp_sock_drv.h
@@@ -11,47 -11,50 +11,62 @@@
  
  #ifdef CONFIG_XDP_SOCKETS
  
++<<<<<<< HEAD
 +void xsk_umem_complete_tx(struct xdp_umem *umem, u32 nb_entries);
 +bool xsk_umem_consume_tx(struct xdp_umem *umem, struct xdp_desc *desc);
 +void xsk_umem_consume_tx_done(struct xdp_umem *umem);
 +struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev, u16 queue_id);
 +void xsk_set_rx_need_wakeup(struct xdp_umem *umem);
 +void xsk_set_tx_need_wakeup(struct xdp_umem *umem);
 +void xsk_clear_rx_need_wakeup(struct xdp_umem *umem);
 +void xsk_clear_tx_need_wakeup(struct xdp_umem *umem);
 +bool xsk_umem_uses_need_wakeup(struct xdp_umem *umem);
++=======
+ void xsk_tx_completed(struct xsk_buff_pool *pool, u32 nb_entries);
+ bool xsk_tx_peek_desc(struct xsk_buff_pool *pool, struct xdp_desc *desc);
+ void xsk_tx_release(struct xsk_buff_pool *pool);
+ struct xsk_buff_pool *xsk_get_pool_from_qid(struct net_device *dev,
+ 					    u16 queue_id);
+ void xsk_set_rx_need_wakeup(struct xsk_buff_pool *pool);
+ void xsk_set_tx_need_wakeup(struct xsk_buff_pool *pool);
+ void xsk_clear_rx_need_wakeup(struct xsk_buff_pool *pool);
+ void xsk_clear_tx_need_wakeup(struct xsk_buff_pool *pool);
+ bool xsk_uses_need_wakeup(struct xsk_buff_pool *pool);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
- static inline u32 xsk_umem_get_headroom(struct xdp_umem *umem)
+ static inline u32 xsk_pool_get_headroom(struct xsk_buff_pool *pool)
  {
- 	return XDP_PACKET_HEADROOM + umem->headroom;
+ 	return XDP_PACKET_HEADROOM + pool->headroom;
  }
  
- static inline u32 xsk_umem_get_chunk_size(struct xdp_umem *umem)
+ static inline u32 xsk_pool_get_chunk_size(struct xsk_buff_pool *pool)
  {
- 	return umem->chunk_size;
+ 	return pool->chunk_size;
  }
  
- static inline u32 xsk_umem_get_rx_frame_size(struct xdp_umem *umem)
+ static inline u32 xsk_pool_get_rx_frame_size(struct xsk_buff_pool *pool)
  {
- 	return xsk_umem_get_chunk_size(umem) - xsk_umem_get_headroom(umem);
+ 	return xsk_pool_get_chunk_size(pool) - xsk_pool_get_headroom(pool);
  }
  
- static inline void xsk_buff_set_rxq_info(struct xdp_umem *umem,
+ static inline void xsk_pool_set_rxq_info(struct xsk_buff_pool *pool,
  					 struct xdp_rxq_info *rxq)
  {
- 	xp_set_rxq_info(umem->pool, rxq);
+ 	xp_set_rxq_info(pool, rxq);
  }
  
- static inline void xsk_buff_dma_unmap(struct xdp_umem *umem,
+ static inline void xsk_pool_dma_unmap(struct xsk_buff_pool *pool,
  				      unsigned long attrs)
  {
- 	xp_dma_unmap(umem->pool, attrs);
+ 	xp_dma_unmap(pool, attrs);
  }
  
- static inline int xsk_buff_dma_map(struct xdp_umem *umem, struct device *dev,
- 				   unsigned long attrs)
+ static inline int xsk_pool_dma_map(struct xsk_buff_pool *pool,
+ 				   struct device *dev, unsigned long attrs)
  {
- 	return xp_dma_map(umem->pool, dev, attrs, umem->pgs, umem->npgs);
+ 	struct xdp_umem *umem = pool->umem;
+ 
+ 	return xp_dma_map(pool, dev, attrs, umem->pgs, umem->npgs);
  }
  
  static inline dma_addr_t xsk_buff_xdp_get_dma(struct xdp_buff *xdp)
@@@ -125,8 -129,8 +141,13 @@@ static inline void xsk_tx_release(struc
  {
  }
  
++<<<<<<< HEAD
 +static inline struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 +						     u16 queue_id)
++=======
+ static inline struct xsk_buff_pool *
+ xsk_get_pool_from_qid(struct net_device *dev, u16 queue_id)
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  {
  	return NULL;
  }
diff --cc net/ethtool/channels.c
index 0579c51ad1d6,9ecda09ecb11..000000000000
--- a/net/ethtool/channels.c
+++ b/net/ethtool/channels.c
@@@ -203,10 -223,9 +203,14 @@@ int ethnl_set_channels(struct sk_buff *
  	from_channel = channels.combined_count +
  		       min(channels.rx_count, channels.tx_count);
  	for (i = from_channel; i < old_total; i++)
++<<<<<<< HEAD
 +		if (xdp_get_umem_from_qid(dev, i)) {
 +			ret = -EINVAL;
++=======
+ 		if (xsk_get_pool_from_qid(dev, i)) {
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			GENL_SET_ERR_MSG(info, "requested channel counts are too low for existing zerocopy AF_XDP sockets");
 -			return -EINVAL;
 +			goto out_ops;
  		}
  
  	ret = dev->ethtool_ops->set_channels(dev, &channels);
diff --cc net/ethtool/ioctl.c
index f3a25ed2b40b,925b5731bf0c..000000000000
--- a/net/ethtool/ioctl.c
+++ b/net/ethtool/ioctl.c
@@@ -1798,7 -1706,7 +1798,11 @@@ static noinline_for_stack int ethtool_s
  		min(channels.rx_count, channels.tx_count);
  	to_channel = curr.combined_count + max(curr.rx_count, curr.tx_count);
  	for (i = from_channel; i < to_channel; i++)
++<<<<<<< HEAD
 +		if (xdp_get_umem_from_qid(dev, i))
++=======
+ 		if (xsk_get_pool_from_qid(dev, i))
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			return -EINVAL;
  
  	ret = dev->ethtool_ops->set_channels(dev, &channels);
diff --cc net/xdp/xdp_umem.c
index fb8d9af5bc04,adde4d576b38..000000000000
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@@ -51,7 -51,8 +51,12 @@@ void xdp_del_sk_umem(struct xdp_umem *u
   * not know if the device has more tx queues than rx, or the opposite.
   * This might also change during run time.
   */
++<<<<<<< HEAD
 +static int xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
++=======
+ static int xsk_reg_pool_at_qid(struct net_device *dev,
+ 			       struct xsk_buff_pool *pool,
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  			       u16 queue_id)
  {
  	if (queue_id >= max_t(unsigned int,
@@@ -67,24 -68,24 +72,35 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
 +struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 +				       u16 queue_id)
++=======
+ struct xsk_buff_pool *xsk_get_pool_from_qid(struct net_device *dev,
+ 					    u16 queue_id)
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  {
  	if (queue_id < dev->real_num_rx_queues)
 -		return dev->_rx[queue_id].pool;
 +		return dev->_rx[queue_id].umem;
  	if (queue_id < dev->real_num_tx_queues)
 -		return dev->_tx[queue_id].pool;
 +		return dev->_tx[queue_id].umem;
  
  	return NULL;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(xdp_get_umem_from_qid);
 +
 +static void xdp_clear_umem_at_qid(struct net_device *dev, u16 queue_id)
++=======
+ EXPORT_SYMBOL(xsk_get_pool_from_qid);
+ 
+ static void xsk_clear_pool_at_qid(struct net_device *dev, u16 queue_id)
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  {
  	if (queue_id < dev->real_num_rx_queues)
 -		dev->_rx[queue_id].pool = NULL;
 +		dev->_rx[queue_id].umem = NULL;
  	if (queue_id < dev->real_num_tx_queues)
 -		dev->_tx[queue_id].pool = NULL;
 +		dev->_tx[queue_id].umem = NULL;
  }
  
  int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
@@@ -102,10 -103,10 +118,17 @@@
  	if (force_zc && force_copy)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	if (xdp_get_umem_from_qid(dev, queue_id))
 +		return -EBUSY;
 +
 +	err = xdp_reg_umem_at_qid(dev, umem, queue_id);
++=======
+ 	if (xsk_get_pool_from_qid(dev, queue_id))
+ 		return -EBUSY;
+ 
+ 	err = xsk_reg_pool_at_qid(dev, umem->pool, queue_id);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	if (err)
  		return err;
  
@@@ -147,7 -148,7 +170,11 @@@ err_unreg_umem
  	if (!force_zc)
  		err = 0; /* fallback to copy mode */
  	if (err)
++<<<<<<< HEAD
 +		xdp_clear_umem_at_qid(dev, queue_id);
++=======
+ 		xsk_clear_pool_at_qid(dev, queue_id);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  	return err;
  }
  
@@@ -172,7 -173,7 +199,11 @@@ void xdp_umem_clear_dev(struct xdp_ume
  			WARN(1, "failed to disable umem!\n");
  	}
  
++<<<<<<< HEAD
 +	xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
++=======
+ 	xsk_clear_pool_at_qid(umem->dev, umem->queue_id);
++>>>>>>> c4655761d3cf (xsk: i40e: ice: ixgbe: mlx5: Rename xsk zero-copy driver interfaces)
  
  	dev_put(umem->dev);
  	umem->dev = NULL;
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_main.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_xsk.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_base.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_xsk.c
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xsk/tx.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --git a/include/net/xdp_sock.h b/include/net/xdp_sock.h
index c9d87cc40c11..ccf6cb54f9a6 100644
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@ -52,6 +52,7 @@ struct xdp_sock {
 	struct net_device *dev;
 	struct xdp_umem *umem;
 	struct list_head flush_node;
+	struct xsk_buff_pool *pool;
 	u16 queue_id;
 	bool zc;
 	enum {
* Unmerged path include/net/xdp_sock_drv.h
* Unmerged path net/ethtool/channels.c
* Unmerged path net/ethtool/ioctl.c
* Unmerged path net/xdp/xdp_umem.c
diff --git a/net/xdp/xsk.c b/net/xdp/xsk.c
index 10c97cce9e3d..6568d8b2c7ec 100644
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@ -41,8 +41,10 @@ bool xsk_is_setup_for_bpf_map(struct xdp_sock *xs)
 		READ_ONCE(xs->umem->fq);
 }
 
-void xsk_set_rx_need_wakeup(struct xdp_umem *umem)
+void xsk_set_rx_need_wakeup(struct xsk_buff_pool *pool)
 {
+	struct xdp_umem *umem = pool->umem;
+
 	if (umem->need_wakeup & XDP_WAKEUP_RX)
 		return;
 
@@ -51,8 +53,9 @@ void xsk_set_rx_need_wakeup(struct xdp_umem *umem)
 }
 EXPORT_SYMBOL(xsk_set_rx_need_wakeup);
 
-void xsk_set_tx_need_wakeup(struct xdp_umem *umem)
+void xsk_set_tx_need_wakeup(struct xsk_buff_pool *pool)
 {
+	struct xdp_umem *umem = pool->umem;
 	struct xdp_sock *xs;
 
 	if (umem->need_wakeup & XDP_WAKEUP_TX)
@@ -68,8 +71,10 @@ void xsk_set_tx_need_wakeup(struct xdp_umem *umem)
 }
 EXPORT_SYMBOL(xsk_set_tx_need_wakeup);
 
-void xsk_clear_rx_need_wakeup(struct xdp_umem *umem)
+void xsk_clear_rx_need_wakeup(struct xsk_buff_pool *pool)
 {
+	struct xdp_umem *umem = pool->umem;
+
 	if (!(umem->need_wakeup & XDP_WAKEUP_RX))
 		return;
 
@@ -78,8 +83,9 @@ void xsk_clear_rx_need_wakeup(struct xdp_umem *umem)
 }
 EXPORT_SYMBOL(xsk_clear_rx_need_wakeup);
 
-void xsk_clear_tx_need_wakeup(struct xdp_umem *umem)
+void xsk_clear_tx_need_wakeup(struct xsk_buff_pool *pool)
 {
+	struct xdp_umem *umem = pool->umem;
 	struct xdp_sock *xs;
 
 	if (!(umem->need_wakeup & XDP_WAKEUP_TX))
@@ -95,11 +101,11 @@ void xsk_clear_tx_need_wakeup(struct xdp_umem *umem)
 }
 EXPORT_SYMBOL(xsk_clear_tx_need_wakeup);
 
-bool xsk_umem_uses_need_wakeup(struct xdp_umem *umem)
+bool xsk_uses_need_wakeup(struct xsk_buff_pool *pool)
 {
-	return umem->flags & XDP_UMEM_USES_NEED_WAKEUP;
+	return pool->umem->flags & XDP_UMEM_USES_NEED_WAKEUP;
 }
-EXPORT_SYMBOL(xsk_umem_uses_need_wakeup);
+EXPORT_SYMBOL(xsk_uses_need_wakeup);
 
 void xp_release(struct xdp_buff_xsk *xskb)
 {
@@ -157,12 +163,12 @@ static int __xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len,
 	struct xdp_buff *xsk_xdp;
 	int err;
 
-	if (len > xsk_umem_get_rx_frame_size(xs->umem)) {
+	if (len > xsk_pool_get_rx_frame_size(xs->pool)) {
 		xs->rx_dropped++;
 		return -ENOSPC;
 	}
 
-	xsk_xdp = xsk_buff_alloc(xs->umem);
+	xsk_xdp = xsk_buff_alloc(xs->pool);
 	if (!xsk_xdp) {
 		xs->rx_dropped++;
 		return -ENOSPC;
@@ -251,27 +257,28 @@ void __xsk_map_flush(void)
 	}
 }
 
-void xsk_umem_complete_tx(struct xdp_umem *umem, u32 nb_entries)
+void xsk_tx_completed(struct xsk_buff_pool *pool, u32 nb_entries)
 {
-	xskq_prod_submit_n(umem->cq, nb_entries);
+	xskq_prod_submit_n(pool->umem->cq, nb_entries);
 }
-EXPORT_SYMBOL(xsk_umem_complete_tx);
+EXPORT_SYMBOL(xsk_tx_completed);
 
-void xsk_umem_consume_tx_done(struct xdp_umem *umem)
+void xsk_tx_release(struct xsk_buff_pool *pool)
 {
 	struct xdp_sock *xs;
 
 	rcu_read_lock();
-	list_for_each_entry_rcu(xs, &umem->xsk_tx_list, list) {
+	list_for_each_entry_rcu(xs, &pool->umem->xsk_tx_list, list) {
 		__xskq_cons_release(xs->tx);
 		xs->sk.sk_write_space(&xs->sk);
 	}
 	rcu_read_unlock();
 }
-EXPORT_SYMBOL(xsk_umem_consume_tx_done);
+EXPORT_SYMBOL(xsk_tx_release);
 
-bool xsk_umem_consume_tx(struct xdp_umem *umem, struct xdp_desc *desc)
+bool xsk_tx_peek_desc(struct xsk_buff_pool *pool, struct xdp_desc *desc)
 {
+	struct xdp_umem *umem = pool->umem;
 	struct xdp_sock *xs;
 
 	rcu_read_lock();
@@ -298,7 +305,7 @@ bool xsk_umem_consume_tx(struct xdp_umem *umem, struct xdp_desc *desc)
 	rcu_read_unlock();
 	return false;
 }
-EXPORT_SYMBOL(xsk_umem_consume_tx);
+EXPORT_SYMBOL(xsk_tx_peek_desc);
 
 static int xsk_wakeup(struct xdp_sock *xs, u8 flags)
 {
@@ -361,7 +368,7 @@ static int xsk_generic_xmit(struct sock *sk)
 
 		skb_put(skb, len);
 		addr = desc.addr;
-		buffer = xsk_buff_raw_get_data(xs->umem, addr);
+		buffer = xsk_buff_raw_get_data(xs->pool, addr);
 		err = skb_store_bits(skb, 0, buffer, len);
 		/* This is the backpressure mechanism for the Tx path.
 		 * Reserve space in the completion queue and only proceed
@@ -773,6 +780,8 @@ static int xsk_setsockopt(struct socket *sock, int level, int optname,
 			return PTR_ERR(umem);
 		}
 
+		xs->pool = umem->pool;
+
 		/* Make sure umem is ready before it can be seen by others */
 		smp_wmb();
 		WRITE_ONCE(xs->umem, umem);
