bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Toke Høiland-Jørgensen <toke@redhat.com>
commit ba452c9e996d8a4c347b32805f91abb70de5de7e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/ba452c9e.failed

Based on the discussion in [0], update the bpf_redirect_neigh() helper to
accept an optional parameter specifying the nexthop information. This makes
it possible to combine bpf_fib_lookup() and bpf_redirect_neigh() without
incurring a duplicate FIB lookup - since the FIB lookup helper will return
the nexthop information even if no neighbour is present, this can simply
be passed on to bpf_redirect_neigh() if bpf_fib_lookup() returns
BPF_FIB_LKUP_RET_NO_NEIGH. Thus fix & extend it before helper API is frozen.

  [0] https://lore.kernel.org/bpf/393e17fc-d187-3a8d-2f0d-a627c7c63fca@iogearbox.net/

	Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Reviewed-by: David Ahern <dsahern@kernel.org>
Link: https://lore.kernel.org/bpf/160322915615.32199.1187570224032024535.stgit@toke.dk
(cherry picked from commit ba452c9e996d8a4c347b32805f91abb70de5de7e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	net/core/filter.c
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index 0a8b1917a074,e6ceac3f7d62..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3444,6 -3451,297 +3444,300 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, struct bpf_redir_neigh *params, int plen, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		populates L2 addresses as well, meaning, internally, the helper
+  * 		relies on the neighbor lookup for the L2 address of the nexthop.
+  *
+  * 		The helper will perform a FIB lookup based on the skb's
+  * 		networking header to get the address of the next hop, unless
+  * 		this is supplied by the caller in the *params* argument. The
+  * 		*plen* argument indicates the len of *params* and should be set
+  * 		to 0 if *params* is NULL.
+  *
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types, and enabled
+  * 		for IPv4 and IPv6 protocols.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
+  *
+  * void *bpf_per_cpu_ptr(const void *percpu_ptr, u32 cpu)
+  *     Description
+  *             Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *             pointer to the percpu kernel variable on *cpu*. A ksym is an
+  *             extern variable decorated with '__ksym'. For ksym, there is a
+  *             global var (either static or global) defined of the same name
+  *             in the kernel. The ksym is percpu if the global var is percpu.
+  *             The returned pointer points to the global percpu var on *cpu*.
+  *
+  *             bpf_per_cpu_ptr() has the same semantic as per_cpu_ptr() in the
+  *             kernel, except that bpf_per_cpu_ptr() may return NULL. This
+  *             happens if *cpu* is larger than nr_cpu_ids. The caller of
+  *             bpf_per_cpu_ptr() must check the returned value.
+  *     Return
+  *             A pointer pointing to the kernel percpu variable on *cpu*, or
+  *             NULL, if *cpu* is invalid.
+  *
+  * void *bpf_this_cpu_ptr(const void *percpu_ptr)
+  *	Description
+  *		Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *		pointer to the percpu kernel variable on this cpu. See the
+  *		description of 'ksym' in **bpf_per_cpu_ptr**\ ().
+  *
+  *		bpf_this_cpu_ptr() has the same semantic as this_cpu_ptr() in
+  *		the kernel. Different from **bpf_per_cpu_ptr**\ (), it would
+  *		never return NULL.
+  *	Return
+  *		A pointer pointing to the kernel percpu variable on this cpu.
+  *
+  * long bpf_redirect_peer(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*.
+  * 		This helper is somewhat similar to **bpf_redirect**\ (), except
+  * 		that the redirection happens to the *ifindex*' peer device and
+  * 		the netns switch takes place from ingress to ingress without
+  * 		going through the CPU's backlog queue.
+  *
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types at the ingress
+  * 		hook and for veth device types. The peer device must reside in a
+  * 		different network namespace.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
++>>>>>>> ba452c9e996d (bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --cc net/core/filter.c
index a22f97d754fb,6d0fa65a4a46..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -2173,6 -2164,259 +2173,262 @@@ static int __bpf_redirect(struct sk_buf
  		return __bpf_redirect_no_mac(skb, dev, flags);
  }
  
++<<<<<<< HEAD
++=======
+ #if IS_ENABLED(CONFIG_IPV6)
+ static int bpf_out_neigh_v6(struct net *net, struct sk_buff *skb,
+ 			    struct net_device *dev, struct bpf_nh_params *nh)
+ {
+ 	u32 hh_len = LL_RESERVED_SPACE(dev);
+ 	const struct in6_addr *nexthop;
+ 	struct dst_entry *dst = NULL;
+ 	struct neighbour *neigh;
+ 
+ 	if (dev_xmit_recursion()) {
+ 		net_crit_ratelimited("bpf: recursion limit reached on datapath, buggy bpf program?\n");
+ 		goto out_drop;
+ 	}
+ 
+ 	skb->dev = dev;
+ 	skb->tstamp = 0;
+ 
+ 	if (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {
+ 		struct sk_buff *skb2;
+ 
+ 		skb2 = skb_realloc_headroom(skb, hh_len);
+ 		if (unlikely(!skb2)) {
+ 			kfree_skb(skb);
+ 			return -ENOMEM;
+ 		}
+ 		if (skb->sk)
+ 			skb_set_owner_w(skb2, skb->sk);
+ 		consume_skb(skb);
+ 		skb = skb2;
+ 	}
+ 
+ 	rcu_read_lock_bh();
+ 	if (!nh) {
+ 		dst = skb_dst(skb);
+ 		nexthop = rt6_nexthop(container_of(dst, struct rt6_info, dst),
+ 				      &ipv6_hdr(skb)->daddr);
+ 	} else {
+ 		nexthop = &nh->ipv6_nh;
+ 	}
+ 	neigh = ip_neigh_gw6(dev, nexthop);
+ 	if (likely(!IS_ERR(neigh))) {
+ 		int ret;
+ 
+ 		sock_confirm_neigh(skb, neigh);
+ 		dev_xmit_recursion_inc();
+ 		ret = neigh_output(neigh, skb, false);
+ 		dev_xmit_recursion_dec();
+ 		rcu_read_unlock_bh();
+ 		return ret;
+ 	}
+ 	rcu_read_unlock_bh();
+ 	if (dst)
+ 		IP6_INC_STATS(dev_net(dst->dev),
+ 			      ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);
+ out_drop:
+ 	kfree_skb(skb);
+ 	return -ENETDOWN;
+ }
+ 
+ static int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev,
+ 				   struct bpf_nh_params *nh)
+ {
+ 	const struct ipv6hdr *ip6h = ipv6_hdr(skb);
+ 	struct net *net = dev_net(dev);
+ 	int err, ret = NET_XMIT_DROP;
+ 
+ 	if (!nh) {
+ 		struct dst_entry *dst;
+ 		struct flowi6 fl6 = {
+ 			.flowi6_flags = FLOWI_FLAG_ANYSRC,
+ 			.flowi6_mark  = skb->mark,
+ 			.flowlabel    = ip6_flowinfo(ip6h),
+ 			.flowi6_oif   = dev->ifindex,
+ 			.flowi6_proto = ip6h->nexthdr,
+ 			.daddr	      = ip6h->daddr,
+ 			.saddr	      = ip6h->saddr,
+ 		};
+ 
+ 		dst = ipv6_stub->ipv6_dst_lookup_flow(net, NULL, &fl6, NULL);
+ 		if (IS_ERR(dst))
+ 			goto out_drop;
+ 
+ 		skb_dst_set(skb, dst);
+ 	} else if (nh->nh_family != AF_INET6) {
+ 		goto out_drop;
+ 	}
+ 
+ 	err = bpf_out_neigh_v6(net, skb, dev, nh);
+ 	if (unlikely(net_xmit_eval(err)))
+ 		dev->stats.tx_errors++;
+ 	else
+ 		ret = NET_XMIT_SUCCESS;
+ 	goto out_xmit;
+ out_drop:
+ 	dev->stats.tx_errors++;
+ 	kfree_skb(skb);
+ out_xmit:
+ 	return ret;
+ }
+ #else
+ static int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev,
+ 				   struct bpf_nh_params *nh)
+ {
+ 	kfree_skb(skb);
+ 	return NET_XMIT_DROP;
+ }
+ #endif /* CONFIG_IPV6 */
+ 
+ #if IS_ENABLED(CONFIG_INET)
+ static int bpf_out_neigh_v4(struct net *net, struct sk_buff *skb,
+ 			    struct net_device *dev, struct bpf_nh_params *nh)
+ {
+ 	u32 hh_len = LL_RESERVED_SPACE(dev);
+ 	struct neighbour *neigh;
+ 	bool is_v6gw = false;
+ 
+ 	if (dev_xmit_recursion()) {
+ 		net_crit_ratelimited("bpf: recursion limit reached on datapath, buggy bpf program?\n");
+ 		goto out_drop;
+ 	}
+ 
+ 	skb->dev = dev;
+ 	skb->tstamp = 0;
+ 
+ 	if (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {
+ 		struct sk_buff *skb2;
+ 
+ 		skb2 = skb_realloc_headroom(skb, hh_len);
+ 		if (unlikely(!skb2)) {
+ 			kfree_skb(skb);
+ 			return -ENOMEM;
+ 		}
+ 		if (skb->sk)
+ 			skb_set_owner_w(skb2, skb->sk);
+ 		consume_skb(skb);
+ 		skb = skb2;
+ 	}
+ 
+ 	rcu_read_lock_bh();
+ 	if (!nh) {
+ 		struct dst_entry *dst = skb_dst(skb);
+ 		struct rtable *rt = container_of(dst, struct rtable, dst);
+ 
+ 		neigh = ip_neigh_for_gw(rt, skb, &is_v6gw);
+ 	} else if (nh->nh_family == AF_INET6) {
+ 		neigh = ip_neigh_gw6(dev, &nh->ipv6_nh);
+ 		is_v6gw = true;
+ 	} else if (nh->nh_family == AF_INET) {
+ 		neigh = ip_neigh_gw4(dev, nh->ipv4_nh);
+ 	} else {
+ 		rcu_read_unlock_bh();
+ 		goto out_drop;
+ 	}
+ 
+ 	if (likely(!IS_ERR(neigh))) {
+ 		int ret;
+ 
+ 		sock_confirm_neigh(skb, neigh);
+ 		dev_xmit_recursion_inc();
+ 		ret = neigh_output(neigh, skb, is_v6gw);
+ 		dev_xmit_recursion_dec();
+ 		rcu_read_unlock_bh();
+ 		return ret;
+ 	}
+ 	rcu_read_unlock_bh();
+ out_drop:
+ 	kfree_skb(skb);
+ 	return -ENETDOWN;
+ }
+ 
+ static int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev,
+ 				   struct bpf_nh_params *nh)
+ {
+ 	const struct iphdr *ip4h = ip_hdr(skb);
+ 	struct net *net = dev_net(dev);
+ 	int err, ret = NET_XMIT_DROP;
+ 
+ 	if (!nh) {
+ 		struct flowi4 fl4 = {
+ 			.flowi4_flags = FLOWI_FLAG_ANYSRC,
+ 			.flowi4_mark  = skb->mark,
+ 			.flowi4_tos   = RT_TOS(ip4h->tos),
+ 			.flowi4_oif   = dev->ifindex,
+ 			.flowi4_proto = ip4h->protocol,
+ 			.daddr	      = ip4h->daddr,
+ 			.saddr	      = ip4h->saddr,
+ 		};
+ 		struct rtable *rt;
+ 
+ 		rt = ip_route_output_flow(net, &fl4, NULL);
+ 		if (IS_ERR(rt))
+ 			goto out_drop;
+ 		if (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {
+ 			ip_rt_put(rt);
+ 			goto out_drop;
+ 		}
+ 
+ 		skb_dst_set(skb, &rt->dst);
+ 	}
+ 
+ 	err = bpf_out_neigh_v4(net, skb, dev, nh);
+ 	if (unlikely(net_xmit_eval(err)))
+ 		dev->stats.tx_errors++;
+ 	else
+ 		ret = NET_XMIT_SUCCESS;
+ 	goto out_xmit;
+ out_drop:
+ 	dev->stats.tx_errors++;
+ 	kfree_skb(skb);
+ out_xmit:
+ 	return ret;
+ }
+ #else
+ static int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev,
+ 				   struct bpf_nh_params *nh)
+ {
+ 	kfree_skb(skb);
+ 	return NET_XMIT_DROP;
+ }
+ #endif /* CONFIG_INET */
+ 
+ static int __bpf_redirect_neigh(struct sk_buff *skb, struct net_device *dev,
+ 				struct bpf_nh_params *nh)
+ {
+ 	struct ethhdr *ethh = eth_hdr(skb);
+ 
+ 	if (unlikely(skb->mac_header >= skb->network_header))
+ 		goto out;
+ 	bpf_push_mac_rcsum(skb);
+ 	if (is_multicast_ether_addr(ethh->h_dest))
+ 		goto out;
+ 
+ 	skb_pull(skb, sizeof(*ethh));
+ 	skb_unset_mac_header(skb);
+ 	skb_reset_network_header(skb);
+ 
+ 	if (skb->protocol == htons(ETH_P_IP))
+ 		return __bpf_redirect_neigh_v4(skb, dev, nh);
+ 	else if (skb->protocol == htons(ETH_P_IPV6))
+ 		return __bpf_redirect_neigh_v6(skb, dev, nh);
+ out:
+ 	kfree_skb(skb);
+ 	return -ENOTSUPP;
+ }
+ 
+ /* Internal, non-exposed redirect flags. */
+ enum {
+ 	BPF_F_NEIGH	= (1ULL << 1),
+ 	BPF_F_PEER	= (1ULL << 2),
+ 	BPF_F_NEXTHOP	= (1ULL << 3),
+ #define BPF_F_REDIRECT_INTERNAL	(BPF_F_NEIGH | BPF_F_PEER | BPF_F_NEXTHOP)
+ };
+ 
++>>>>>>> ba452c9e996d (bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop)
  BPF_CALL_3(bpf_clone_redirect, struct sk_buff *, skb, u32, ifindex, u64, flags)
  {
  	struct net_device *dev;
@@@ -2216,6 -2460,41 +2472,44 @@@ static const struct bpf_func_proto bpf_
  DEFINE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);
  EXPORT_PER_CPU_SYMBOL_GPL(bpf_redirect_info);
  
++<<<<<<< HEAD
++=======
+ int skb_do_redirect(struct sk_buff *skb)
+ {
+ 	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+ 	struct net *net = dev_net(skb->dev);
+ 	struct net_device *dev;
+ 	u32 flags = ri->flags;
+ 
+ 	dev = dev_get_by_index_rcu(net, ri->tgt_index);
+ 	ri->tgt_index = 0;
+ 	ri->flags = 0;
+ 	if (unlikely(!dev))
+ 		goto out_drop;
+ 	if (flags & BPF_F_PEER) {
+ 		const struct net_device_ops *ops = dev->netdev_ops;
+ 
+ 		if (unlikely(!ops->ndo_get_peer_dev ||
+ 			     !skb_at_tc_ingress(skb)))
+ 			goto out_drop;
+ 		dev = ops->ndo_get_peer_dev(dev);
+ 		if (unlikely(!dev ||
+ 			     !is_skb_forwardable(dev, skb) ||
+ 			     net_eq(net, dev_net(dev))))
+ 			goto out_drop;
+ 		skb->dev = dev;
+ 		return -EAGAIN;
+ 	}
+ 	return flags & BPF_F_NEIGH ?
+ 	       __bpf_redirect_neigh(skb, dev, flags & BPF_F_NEXTHOP ?
+ 				    &ri->nh : NULL) :
+ 	       __bpf_redirect(skb, dev, flags);
+ out_drop:
+ 	kfree_skb(skb);
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> ba452c9e996d (bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop)
  BPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)
  {
  	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
@@@ -2252,6 -2516,55 +2546,58 @@@ static const struct bpf_func_proto bpf_
  	.arg2_type      = ARG_ANYTHING,
  };
  
++<<<<<<< HEAD
++=======
+ BPF_CALL_2(bpf_redirect_peer, u32, ifindex, u64, flags)
+ {
+ 	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+ 
+ 	if (unlikely(flags))
+ 		return TC_ACT_SHOT;
+ 
+ 	ri->flags = BPF_F_PEER;
+ 	ri->tgt_index = ifindex;
+ 
+ 	return TC_ACT_REDIRECT;
+ }
+ 
+ static const struct bpf_func_proto bpf_redirect_peer_proto = {
+ 	.func           = bpf_redirect_peer,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_ANYTHING,
+ 	.arg2_type      = ARG_ANYTHING,
+ };
+ 
+ BPF_CALL_4(bpf_redirect_neigh, u32, ifindex, struct bpf_redir_neigh *, params,
+ 	   int, plen, u64, flags)
+ {
+ 	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+ 
+ 	if (unlikely((plen && plen < sizeof(*params)) || flags))
+ 		return TC_ACT_SHOT;
+ 
+ 	ri->flags = BPF_F_NEIGH | (plen ? BPF_F_NEXTHOP : 0);
+ 	ri->tgt_index = ifindex;
+ 
+ 	BUILD_BUG_ON(sizeof(struct bpf_redir_neigh) != sizeof(struct bpf_nh_params));
+ 	if (plen)
+ 		memcpy(&ri->nh, params, sizeof(ri->nh));
+ 
+ 	return TC_ACT_REDIRECT;
+ }
+ 
+ static const struct bpf_func_proto bpf_redirect_neigh_proto = {
+ 	.func		= bpf_redirect_neigh,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_ANYTHING,
+ 	.arg2_type      = ARG_PTR_TO_MEM_OR_NULL,
+ 	.arg3_type      = ARG_CONST_SIZE_OR_ZERO,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
++>>>>>>> ba452c9e996d (bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop)
  BPF_CALL_2(bpf_msg_apply_bytes, struct sk_msg *, msg, u32, bytes)
  {
  	msg->apply_bytes = bytes;
diff --cc tools/include/uapi/linux/bpf.h
index 0dc0309df9c7,e6ceac3f7d62..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3424,6 -3451,297 +3424,300 @@@ union bpf_attr 
   *		A non-negative value equal to or less than *size* on success,
   *		or a negative error in case of failure.
   *
++<<<<<<< HEAD
++=======
+  * long bpf_load_hdr_opt(struct bpf_sock_ops *skops, void *searchby_res, u32 len, u64 flags)
+  *	Description
+  *		Load header option.  Support reading a particular TCP header
+  *		option for bpf program (**BPF_PROG_TYPE_SOCK_OPS**).
+  *
+  *		If *flags* is 0, it will search the option from the
+  *		*skops*\ **->skb_data**.  The comment in **struct bpf_sock_ops**
+  *		has details on what skb_data contains under different
+  *		*skops*\ **->op**.
+  *
+  *		The first byte of the *searchby_res* specifies the
+  *		kind that it wants to search.
+  *
+  *		If the searching kind is an experimental kind
+  *		(i.e. 253 or 254 according to RFC6994).  It also
+  *		needs to specify the "magic" which is either
+  *		2 bytes or 4 bytes.  It then also needs to
+  *		specify the size of the magic by using
+  *		the 2nd byte which is "kind-length" of a TCP
+  *		header option and the "kind-length" also
+  *		includes the first 2 bytes "kind" and "kind-length"
+  *		itself as a normal TCP header option also does.
+  *
+  *		For example, to search experimental kind 254 with
+  *		2 byte magic 0xeB9F, the searchby_res should be
+  *		[ 254, 4, 0xeB, 0x9F, 0, 0, .... 0 ].
+  *
+  *		To search for the standard window scale option (3),
+  *		the *searchby_res* should be [ 3, 0, 0, .... 0 ].
+  *		Note, kind-length must be 0 for regular option.
+  *
+  *		Searching for No-Op (0) and End-of-Option-List (1) are
+  *		not supported.
+  *
+  *		*len* must be at least 2 bytes which is the minimal size
+  *		of a header option.
+  *
+  *		Supported flags:
+  *
+  *		* **BPF_LOAD_HDR_OPT_TCP_SYN** to search from the
+  *		  saved_syn packet or the just-received syn packet.
+  *
+  *	Return
+  *		> 0 when found, the header option is copied to *searchby_res*.
+  *		The return value is the total length copied. On failure, a
+  *		negative error code is returned:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOMSG** if the option is not found.
+  *
+  *		**-ENOENT** if no syn packet is available when
+  *		**BPF_LOAD_HDR_OPT_TCP_SYN** is used.
+  *
+  *		**-ENOSPC** if there is not enough space.  Only *len* number of
+  *		bytes are copied.
+  *
+  *		**-EFAULT** on failure to parse the header options in the
+  *		packet.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_store_hdr_opt(struct bpf_sock_ops *skops, const void *from, u32 len, u64 flags)
+  *	Description
+  *		Store header option.  The data will be copied
+  *		from buffer *from* with length *len* to the TCP header.
+  *
+  *		The buffer *from* should have the whole option that
+  *		includes the kind, kind-length, and the actual
+  *		option data.  The *len* must be at least kind-length
+  *		long.  The kind-length does not have to be 4 byte
+  *		aligned.  The kernel will take care of the padding
+  *		and setting the 4 bytes aligned value to th->doff.
+  *
+  *		This helper will check for duplicated option
+  *		by searching the same option in the outgoing skb.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** If param is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *		Nothing has been written
+  *
+  *		**-EEXIST** if the option already exists.
+  *
+  *		**-EFAULT** on failrue to parse the existing header options.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * long bpf_reserve_hdr_opt(struct bpf_sock_ops *skops, u32 len, u64 flags)
+  *	Description
+  *		Reserve *len* bytes for the bpf header option.  The
+  *		space will be used by **bpf_store_hdr_opt**\ () later in
+  *		**BPF_SOCK_OPS_WRITE_HDR_OPT_CB**.
+  *
+  *		If **bpf_reserve_hdr_opt**\ () is called multiple times,
+  *		the total number of bytes will be reserved.
+  *
+  *		This helper can only be called during
+  *		**BPF_SOCK_OPS_HDR_OPT_LEN_CB**.
+  *
+  *	Return
+  *		0 on success, or negative error in case of failure:
+  *
+  *		**-EINVAL** if a parameter is invalid.
+  *
+  *		**-ENOSPC** if there is not enough space in the header.
+  *
+  *		**-EPERM** if the helper cannot be used under the current
+  *		*skops*\ **->op**.
+  *
+  * void *bpf_inode_storage_get(struct bpf_map *map, void *inode, void *value, u64 flags)
+  *	Description
+  *		Get a bpf_local_storage from an *inode*.
+  *
+  *		Logically, it could be thought of as getting the value from
+  *		a *map* with *inode* as the **key**.  From this
+  *		perspective,  the usage is not much different from
+  *		**bpf_map_lookup_elem**\ (*map*, **&**\ *inode*) except this
+  *		helper enforces the key must be an inode and the map must also
+  *		be a **BPF_MAP_TYPE_INODE_STORAGE**.
+  *
+  *		Underneath, the value is stored locally at *inode* instead of
+  *		the *map*.  The *map* is used as the bpf-local-storage
+  *		"type". The bpf-local-storage "type" (i.e. the *map*) is
+  *		searched against all bpf_local_storage residing at *inode*.
+  *
+  *		An optional *flags* (**BPF_LOCAL_STORAGE_GET_F_CREATE**) can be
+  *		used such that a new bpf_local_storage will be
+  *		created if one does not exist.  *value* can be used
+  *		together with **BPF_LOCAL_STORAGE_GET_F_CREATE** to specify
+  *		the initial value of a bpf_local_storage.  If *value* is
+  *		**NULL**, the new bpf_local_storage will be zero initialized.
+  *	Return
+  *		A bpf_local_storage pointer is returned on success.
+  *
+  *		**NULL** if not found or there was an error in adding
+  *		a new bpf_local_storage.
+  *
+  * int bpf_inode_storage_delete(struct bpf_map *map, void *inode)
+  *	Description
+  *		Delete a bpf_local_storage from an *inode*.
+  *	Return
+  *		0 on success.
+  *
+  *		**-ENOENT** if the bpf_local_storage cannot be found.
+  *
+  * long bpf_d_path(struct path *path, char *buf, u32 sz)
+  *	Description
+  *		Return full path for given **struct path** object, which
+  *		needs to be the kernel BTF *path* object. The path is
+  *		returned in the provided buffer *buf* of size *sz* and
+  *		is zero terminated.
+  *
+  *	Return
+  *		On success, the strictly positive length of the string,
+  *		including the trailing NUL character. On error, a negative
+  *		value.
+  *
+  * long bpf_copy_from_user(void *dst, u32 size, const void *user_ptr)
+  * 	Description
+  * 		Read *size* bytes from user space address *user_ptr* and store
+  * 		the data in *dst*. This is a wrapper of **copy_from_user**\ ().
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * long bpf_snprintf_btf(char *str, u32 str_size, struct btf_ptr *ptr, u32 btf_ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to store a string representation of *ptr*->ptr in *str*,
+  *		using *ptr*->type_id.  This value should specify the type
+  *		that *ptr*->ptr points to. LLVM __builtin_btf_type_id(type, 1)
+  *		can be used to look up vmlinux BTF type ids. Traversing the
+  *		data structure using BTF, the type information and values are
+  *		stored in the first *str_size* - 1 bytes of *str*.  Safe copy of
+  *		the pointer data is carried out to avoid kernel crashes during
+  *		operation.  Smaller types can use string space on the stack;
+  *		larger programs can use map data to store the string
+  *		representation.
+  *
+  *		The string can be subsequently shared with userspace via
+  *		bpf_perf_event_output() or ring buffer interfaces.
+  *		bpf_trace_printk() is to be avoided as it places too small
+  *		a limit on string size to be useful.
+  *
+  *		*flags* is a combination of
+  *
+  *		**BTF_F_COMPACT**
+  *			no formatting around type information
+  *		**BTF_F_NONAME**
+  *			no struct/union member names/types
+  *		**BTF_F_PTR_RAW**
+  *			show raw (unobfuscated) pointer values;
+  *			equivalent to printk specifier %px.
+  *		**BTF_F_ZERO**
+  *			show zero-valued struct/union members; they
+  *			are not displayed by default
+  *
+  *	Return
+  *		The number of bytes that were written (or would have been
+  *		written if output had to be truncated due to string size),
+  *		or a negative error in cases of failure.
+  *
+  * long bpf_seq_printf_btf(struct seq_file *m, struct btf_ptr *ptr, u32 ptr_size, u64 flags)
+  *	Description
+  *		Use BTF to write to seq_write a string representation of
+  *		*ptr*->ptr, using *ptr*->type_id as per bpf_snprintf_btf().
+  *		*flags* are identical to those used for bpf_snprintf_btf.
+  *	Return
+  *		0 on success or a negative error in case of failure.
+  *
+  * u64 bpf_skb_cgroup_classid(struct sk_buff *skb)
+  * 	Description
+  * 		See **bpf_get_cgroup_classid**\ () for the main description.
+  * 		This helper differs from **bpf_get_cgroup_classid**\ () in that
+  * 		the cgroup v1 net_cls class is retrieved only from the *skb*'s
+  * 		associated socket instead of the current process.
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_redirect_neigh(u32 ifindex, struct bpf_redir_neigh *params, int plen, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*
+  * 		and fill in L2 addresses from neighboring subsystem. This helper
+  * 		is somewhat similar to **bpf_redirect**\ (), except that it
+  * 		populates L2 addresses as well, meaning, internally, the helper
+  * 		relies on the neighbor lookup for the L2 address of the nexthop.
+  *
+  * 		The helper will perform a FIB lookup based on the skb's
+  * 		networking header to get the address of the next hop, unless
+  * 		this is supplied by the caller in the *params* argument. The
+  * 		*plen* argument indicates the len of *params* and should be set
+  * 		to 0 if *params* is NULL.
+  *
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types, and enabled
+  * 		for IPv4 and IPv6 protocols.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
+  *
+  * void *bpf_per_cpu_ptr(const void *percpu_ptr, u32 cpu)
+  *     Description
+  *             Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *             pointer to the percpu kernel variable on *cpu*. A ksym is an
+  *             extern variable decorated with '__ksym'. For ksym, there is a
+  *             global var (either static or global) defined of the same name
+  *             in the kernel. The ksym is percpu if the global var is percpu.
+  *             The returned pointer points to the global percpu var on *cpu*.
+  *
+  *             bpf_per_cpu_ptr() has the same semantic as per_cpu_ptr() in the
+  *             kernel, except that bpf_per_cpu_ptr() may return NULL. This
+  *             happens if *cpu* is larger than nr_cpu_ids. The caller of
+  *             bpf_per_cpu_ptr() must check the returned value.
+  *     Return
+  *             A pointer pointing to the kernel percpu variable on *cpu*, or
+  *             NULL, if *cpu* is invalid.
+  *
+  * void *bpf_this_cpu_ptr(const void *percpu_ptr)
+  *	Description
+  *		Take a pointer to a percpu ksym, *percpu_ptr*, and return a
+  *		pointer to the percpu kernel variable on this cpu. See the
+  *		description of 'ksym' in **bpf_per_cpu_ptr**\ ().
+  *
+  *		bpf_this_cpu_ptr() has the same semantic as this_cpu_ptr() in
+  *		the kernel. Different from **bpf_per_cpu_ptr**\ (), it would
+  *		never return NULL.
+  *	Return
+  *		A pointer pointing to the kernel percpu variable on this cpu.
+  *
+  * long bpf_redirect_peer(u32 ifindex, u64 flags)
+  * 	Description
+  * 		Redirect the packet to another net device of index *ifindex*.
+  * 		This helper is somewhat similar to **bpf_redirect**\ (), except
+  * 		that the redirection happens to the *ifindex*' peer device and
+  * 		the netns switch takes place from ingress to ingress without
+  * 		going through the CPU's backlog queue.
+  *
+  * 		The *flags* argument is reserved and must be 0. The helper is
+  * 		currently only supported for tc BPF program types at the ingress
+  * 		hook and for veth device types. The peer device must reside in a
+  * 		different network namespace.
+  * 	Return
+  * 		The helper returns **TC_ACT_REDIRECT** on success or
+  * 		**TC_ACT_SHOT** on error.
++>>>>>>> ba452c9e996d (bpf: Fix bpf_redirect_neigh helper api to support supplying nexthop)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --git a/include/linux/filter.h b/include/linux/filter.h
index 32b2debf4aab..831865331eb0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -616,12 +616,21 @@ struct bpf_skb_data_end {
 	void *data_end;
 };
 
+struct bpf_nh_params {
+	u32 nh_family;
+	union {
+		u32 ipv4_nh;
+		struct in6_addr ipv6_nh;
+	};
+};
+
 struct bpf_redirect_info {
 	u32 flags;
 	u32 tgt_index;
 	void *tgt_value;
 	struct bpf_map *map;
 	u32 kern_flags;
+	struct bpf_nh_params nh;
 };
 
 DECLARE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path net/core/filter.c
diff --git a/scripts/bpf_helpers_doc.py b/scripts/bpf_helpers_doc.py
index f43d6268fa78..1159ef6b19e0 100755
--- a/scripts/bpf_helpers_doc.py
+++ b/scripts/bpf_helpers_doc.py
@@ -452,6 +452,7 @@ class PrinterHelpers(Printer):
             'struct bpf_perf_event_data',
             'struct bpf_perf_event_value',
             'struct bpf_pidns_info',
+            'struct bpf_redir_neigh',
             'struct bpf_sk_lookup',
             'struct bpf_sock',
             'struct bpf_sock_addr',
* Unmerged path tools/include/uapi/linux/bpf.h
