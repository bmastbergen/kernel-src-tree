lockdep: Optimize the memory usage of circular queue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Boqun Feng <boqun.feng@gmail.com>
commit 6d1823ccc480866e571ab1206665d693aeb600cf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/6d1823cc.failed

Qian Cai reported a BFS_EQUEUEFULL warning [1] after read recursive
deadlock detection merged into tip tree recently. Unlike the previous
lockep graph searching, which iterate every lock class (every node in
the graph) exactly once, the graph searching for read recurisve deadlock
detection needs to iterate every lock dependency (every edge in the
graph) once, as a result, the maximum memory cost of the circular queue
changes from O(V), where V is the number of lock classes (nodes or
vertices) in the graph, to O(E), where E is the number of lock
dependencies (edges), because every lock class or dependency gets
enqueued once in the BFS. Therefore we hit the BFS_EQUEUEFULL case.

However, actually we don't need to enqueue all dependencies for the BFS,
because every time we enqueue a dependency, we almostly enqueue all
other dependencies in the same dependency list ("almostly" is because
we currently check before enqueue, so if a dependency doesn't pass the
check stage we won't enqueue it, however, we can always do in reverse
ordering), based on this, we can only enqueue the first dependency from
a dependency list and every time we want to fetch a new dependency to
work, we can either:

  1)	fetch the dependency next to the current dependency in the
	dependency list
or

  2)	if the dependency in 1) doesn't exist, fetch the dependency from
	the queue.

With this approach, the "max bfs queue depth" for a x86_64_defconfig +
lockdep and selftest config kernel can get descreased from:

        max bfs queue depth:                   201

to (after apply this patch)

        max bfs queue depth:                   61

While I'm at it, clean up the code logic a little (e.g. directly return
other than set a "ret" value and goto the "exit" label).

[1]: https://lore.kernel.org/lkml/17343f6f7f2438fc376125384133c5ba70c2a681.camel@redhat.com/

	Reported-by: Qian Cai <cai@redhat.com>
	Reported-by: syzbot+62ebe501c1ce9a91f68c@syzkaller.appspotmail.com
	Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200917080210.108095-1-boqun.feng@gmail.com
(cherry picked from commit 6d1823ccc480866e571ab1206665d693aeb600cf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index 916b17c27bc1,9560a4e55277..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -1563,73 -1566,189 +1563,243 @@@ static inline u8 calc_depb(struct held_
  }
  
  /*
 - * Initialize a lock_list entry @lock belonging to @class as the root for a BFS
 - * search.
 + * Forward- or backward-dependency search, used for both circular dependency
 + * checking and hardirq-unsafe/softirq-unsafe checking.
   */
++<<<<<<< HEAD
 +static int __bfs(struct lock_list *source_entry,
 +		 void *data,
 +		 int (*match)(struct lock_list *entry, void *data),
 +		 struct lock_list **target_entry,
 +		 int offset)
++=======
+ static inline void __bfs_init_root(struct lock_list *lock,
+ 				   struct lock_class *class)
+ {
+ 	lock->class = class;
+ 	lock->parent = NULL;
+ 	lock->only_xr = 0;
+ }
+ 
+ /*
+  * Initialize a lock_list entry @lock based on a lock acquisition @hlock as the
+  * root for a BFS search.
+  *
+  * ->only_xr of the initial lock node is set to @hlock->read == 2, to make sure
+  * that <prev> -> @hlock and @hlock -> <whatever __bfs() found> is not -(*R)->
+  * and -(S*)->.
+  */
+ static inline void bfs_init_root(struct lock_list *lock,
+ 				 struct held_lock *hlock)
+ {
+ 	__bfs_init_root(lock, hlock_class(hlock));
+ 	lock->only_xr = (hlock->read == 2);
+ }
+ 
+ /*
+  * Similar to bfs_init_root() but initialize the root for backwards BFS.
+  *
+  * ->only_xr of the initial lock node is set to @hlock->read != 0, to make sure
+  * that <next> -> @hlock and @hlock -> <whatever backwards BFS found> is not
+  * -(*S)-> and -(R*)-> (reverse order of -(*R)-> and -(S*)->).
+  */
+ static inline void bfs_init_rootb(struct lock_list *lock,
+ 				  struct held_lock *hlock)
+ {
+ 	__bfs_init_root(lock, hlock_class(hlock));
+ 	lock->only_xr = (hlock->read != 0);
+ }
+ 
+ static inline struct lock_list *__bfs_next(struct lock_list *lock, int offset)
+ {
+ 	if (!lock || !lock->parent)
+ 		return NULL;
+ 
+ 	return list_next_or_null_rcu(get_dep_list(lock->parent, offset),
+ 				     &lock->entry, struct lock_list, entry);
+ }
+ 
+ /*
+  * Breadth-First Search to find a strong path in the dependency graph.
+  *
+  * @source_entry: the source of the path we are searching for.
+  * @data: data used for the second parameter of @match function
+  * @match: match function for the search
+  * @target_entry: pointer to the target of a matched path
+  * @offset: the offset to struct lock_class to determine whether it is
+  *          locks_after or locks_before
+  *
+  * We may have multiple edges (considering different kinds of dependencies,
+  * e.g. ER and SN) between two nodes in the dependency graph. But
+  * only the strong dependency path in the graph is relevant to deadlocks. A
+  * strong dependency path is a dependency path that doesn't have two adjacent
+  * dependencies as -(*R)-> -(S*)->, please see:
+  *
+  *         Documentation/locking/lockdep-design.rst
+  *
+  * for more explanation of the definition of strong dependency paths
+  *
+  * In __bfs(), we only traverse in the strong dependency path:
+  *
+  *     In lock_list::only_xr, we record whether the previous dependency only
+  *     has -(*R)-> in the search, and if it does (prev only has -(*R)->), we
+  *     filter out any -(S*)-> in the current dependency and after that, the
+  *     ->only_xr is set according to whether we only have -(*R)-> left.
+  */
+ static enum bfs_result __bfs(struct lock_list *source_entry,
+ 			     void *data,
+ 			     bool (*match)(struct lock_list *entry, void *data),
+ 			     struct lock_list **target_entry,
+ 			     int offset)
++>>>>>>> 6d1823ccc480 (lockdep: Optimize the memory usage of circular queue)
  {
- 	struct lock_list *entry;
- 	struct lock_list *lock;
- 	struct list_head *head;
  	struct circular_queue *cq = &lock_cq;
++<<<<<<< HEAD
 +	int ret = 1;
 +
 +	lockdep_assert_locked();
 +
 +	if (match(source_entry, data)) {
 +		*target_entry = source_entry;
 +		ret = 0;
 +		goto exit;
 +	}
 +
 +	head = get_dep_list(source_entry, offset);
 +	if (list_empty(head))
 +		goto exit;
 +
 +	__cq_init(cq);
 +	__cq_enqueue(cq, source_entry);
 +
 +	while ((lock = __cq_dequeue(cq))) {
 +
 +		if (!lock->class) {
 +			ret = -2;
 +			goto exit;
 +		}
 +
 +		head = get_dep_list(lock, offset);
 +
 +		list_for_each_entry_rcu(entry, head, entry) {
 +			if (!lock_accessed(entry)) {
 +				unsigned int cq_depth;
 +				mark_lock_accessed(entry, lock);
 +				if (match(entry, data)) {
 +					*target_entry = entry;
 +					ret = 0;
 +					goto exit;
 +				}
 +
 +				if (__cq_enqueue(cq, entry)) {
 +					ret = -1;
 +					goto exit;
 +				}
 +				cq_depth = __cq_get_elem_count(cq);
 +				if (max_bfs_queue_depth < cq_depth)
 +					max_bfs_queue_depth = cq_depth;
 +			}
++=======
+ 	struct lock_list *lock = NULL;
+ 	struct lock_list *entry;
+ 	struct list_head *head;
+ 	unsigned int cq_depth;
+ 	bool first;
+ 
+ 	lockdep_assert_locked();
+ 
+ 	__cq_init(cq);
+ 	__cq_enqueue(cq, source_entry);
+ 
+ 	while ((lock = __bfs_next(lock, offset)) || (lock = __cq_dequeue(cq))) {
+ 		if (!lock->class)
+ 			return BFS_EINVALIDNODE;
+ 
+ 		/*
+ 		 * Step 1: check whether we already finish on this one.
+ 		 *
+ 		 * If we have visited all the dependencies from this @lock to
+ 		 * others (iow, if we have visited all lock_list entries in
+ 		 * @lock->class->locks_{after,before}) we skip, otherwise go
+ 		 * and visit all the dependencies in the list and mark this
+ 		 * list accessed.
+ 		 */
+ 		if (lock_accessed(lock))
+ 			continue;
+ 		else
+ 			mark_lock_accessed(lock);
+ 
+ 		/*
+ 		 * Step 2: check whether prev dependency and this form a strong
+ 		 *         dependency path.
+ 		 */
+ 		if (lock->parent) { /* Parent exists, check prev dependency */
+ 			u8 dep = lock->dep;
+ 			bool prev_only_xr = lock->parent->only_xr;
+ 
+ 			/*
+ 			 * Mask out all -(S*)-> if we only have *R in previous
+ 			 * step, because -(*R)-> -(S*)-> don't make up a strong
+ 			 * dependency.
+ 			 */
+ 			if (prev_only_xr)
+ 				dep &= ~(DEP_SR_MASK | DEP_SN_MASK);
+ 
+ 			/* If nothing left, we skip */
+ 			if (!dep)
+ 				continue;
+ 
+ 			/* If there are only -(*R)-> left, set that for the next step */
+ 			lock->only_xr = !(dep & (DEP_SN_MASK | DEP_EN_MASK));
+ 		}
+ 
+ 		/*
+ 		 * Step 3: we haven't visited this and there is a strong
+ 		 *         dependency path to this, so check with @match.
+ 		 */
+ 		if (match(lock, data)) {
+ 			*target_entry = lock;
+ 			return BFS_RMATCH;
+ 		}
+ 
+ 		/*
+ 		 * Step 4: if not match, expand the path by adding the
+ 		 *         forward or backwards dependencis in the search
+ 		 *
+ 		 */
+ 		first = true;
+ 		head = get_dep_list(lock, offset);
+ 		list_for_each_entry_rcu(entry, head, entry) {
+ 			visit_lock_entry(entry, lock);
+ 
+ 			/*
+ 			 * Note we only enqueue the first of the list into the
+ 			 * queue, because we can always find a sibling
+ 			 * dependency from one (see __bfs_next()), as a result
+ 			 * the space of queue is saved.
+ 			 */
+ 			if (!first)
+ 				continue;
+ 
+ 			first = false;
+ 
+ 			if (__cq_enqueue(cq, entry))
+ 				return BFS_EQUEUEFULL;
+ 
+ 			cq_depth = __cq_get_elem_count(cq);
+ 			if (max_bfs_queue_depth < cq_depth)
+ 				max_bfs_queue_depth = cq_depth;
++>>>>>>> 6d1823ccc480 (lockdep: Optimize the memory usage of circular queue)
  		}
  	}
- exit:
- 	return ret;
+ 
+ 	return BFS_RNOMATCH;
  }
  
 -static inline enum bfs_result
 -__bfs_forwards(struct lock_list *src_entry,
 -	       void *data,
 -	       bool (*match)(struct lock_list *entry, void *data),
 -	       struct lock_list **target_entry)
 +static inline int __bfs_forwards(struct lock_list *src_entry,
 +			void *data,
 +			int (*match)(struct lock_list *entry, void *data),
 +			struct lock_list **target_entry)
  {
  	return __bfs(src_entry, data, match, target_entry,
  		     offsetof(struct lock_class, locks_after));
* Unmerged path kernel/locking/lockdep.c
