xsk: Move xsk_tx_list and its lock to buffer pool

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit a5aa8e529e3667eb377ec132d4b4926dee065a45
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/a5aa8e52.failed

Move the xsk_tx_list and the xsk_tx_list_lock from the umem to
the buffer pool. This so that we in a later commit can share the
umem between multiple HW queues. There is one xsk_tx_list per
device and queue id, so it should be located in the buffer pool.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/1598603189-32145-7-git-send-email-magnus.karlsson@intel.com
(cherry picked from commit a5aa8e529e3667eb377ec132d4b4926dee065a45)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xsk_buff_pool.h
#	net/xdp/xdp_umem.c
#	net/xdp/xsk.c
#	net/xdp/xsk_buff_pool.c
diff --cc include/net/xsk_buff_pool.h
index 6842990e2712,83f100c6d440..000000000000
--- a/include/net/xsk_buff_pool.h
+++ b/include/net/xsk_buff_pool.h
@@@ -40,20 -43,35 +40,37 @@@ struct xsk_buff_pool 
  	u32 headroom;
  	u32 chunk_size;
  	u32 frame_len;
 -	u16 queue_id;
 -	u8 cached_need_wakeup;
 -	bool uses_need_wakeup;
  	bool dma_need_sync;
  	bool unaligned;
 -	struct xdp_umem *umem;
  	void *addrs;
  	struct device *dev;
++<<<<<<< HEAD
++=======
+ 	struct net_device *netdev;
+ 	struct list_head xsk_tx_list;
+ 	/* Protects modifications to the xsk_tx_list */
+ 	spinlock_t xsk_tx_list_lock;
+ 	refcount_t users;
+ 	struct work_struct work;
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  	struct xdp_buff_xsk *free_heads[];
  };
  
  /* AF_XDP core. */
 -struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,
 -						struct xdp_umem *umem);
 -int xp_assign_dev(struct xsk_buff_pool *pool, struct net_device *dev,
 -		  u16 queue_id, u16 flags);
 +struct xsk_buff_pool *xp_create(struct page **pages, u32 nr_pages, u32 chunks,
 +				u32 chunk_size, u32 headroom, u64 size,
 +				bool unaligned);
 +void xp_set_fq(struct xsk_buff_pool *pool, struct xsk_queue *fq);
  void xp_destroy(struct xsk_buff_pool *pool);
  void xp_release(struct xdp_buff_xsk *xskb);
++<<<<<<< HEAD
++=======
+ void xp_get_pool(struct xsk_buff_pool *pool);
+ void xp_put_pool(struct xsk_buff_pool *pool);
+ void xp_clear_dev(struct xsk_buff_pool *pool);
+ void xp_add_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs);
+ void xp_del_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs);
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  
  /* AF_XDP, and XDP core. */
  void xp_free(struct xdp_buff_xsk *xskb);
diff --cc net/xdp/xdp_umem.c
index fb8d9af5bc04,77515925f3c5..000000000000
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@@ -23,162 -23,6 +23,165 @@@
  
  static DEFINE_IDA(umem_ida);
  
++<<<<<<< HEAD
 +void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 +{
 +	unsigned long flags;
 +
 +	if (!xs->tx)
 +		return;
 +
 +	spin_lock_irqsave(&umem->xsk_tx_list_lock, flags);
 +	list_add_rcu(&xs->list, &umem->xsk_tx_list);
 +	spin_unlock_irqrestore(&umem->xsk_tx_list_lock, flags);
 +}
 +
 +void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 +{
 +	unsigned long flags;
 +
 +	if (!xs->tx)
 +		return;
 +
 +	spin_lock_irqsave(&umem->xsk_tx_list_lock, flags);
 +	list_del_rcu(&xs->list);
 +	spin_unlock_irqrestore(&umem->xsk_tx_list_lock, flags);
 +}
 +
 +/* The umem is stored both in the _rx struct and the _tx struct as we do
 + * not know if the device has more tx queues than rx, or the opposite.
 + * This might also change during run time.
 + */
 +static int xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
 +			       u16 queue_id)
 +{
 +	if (queue_id >= max_t(unsigned int,
 +			      dev->real_num_rx_queues,
 +			      dev->real_num_tx_queues))
 +		return -EINVAL;
 +
 +	if (queue_id < dev->real_num_rx_queues)
 +		dev->_rx[queue_id].umem = umem;
 +	if (queue_id < dev->real_num_tx_queues)
 +		dev->_tx[queue_id].umem = umem;
 +
 +	return 0;
 +}
 +
 +struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 +				       u16 queue_id)
 +{
 +	if (queue_id < dev->real_num_rx_queues)
 +		return dev->_rx[queue_id].umem;
 +	if (queue_id < dev->real_num_tx_queues)
 +		return dev->_tx[queue_id].umem;
 +
 +	return NULL;
 +}
 +EXPORT_SYMBOL(xdp_get_umem_from_qid);
 +
 +static void xdp_clear_umem_at_qid(struct net_device *dev, u16 queue_id)
 +{
 +	if (queue_id < dev->real_num_rx_queues)
 +		dev->_rx[queue_id].umem = NULL;
 +	if (queue_id < dev->real_num_tx_queues)
 +		dev->_tx[queue_id].umem = NULL;
 +}
 +
 +int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 +			u16 queue_id, u16 flags)
 +{
 +	bool force_zc, force_copy;
 +	struct netdev_bpf bpf;
 +	int err = 0;
 +
 +	ASSERT_RTNL();
 +
 +	force_zc = flags & XDP_ZEROCOPY;
 +	force_copy = flags & XDP_COPY;
 +
 +	if (force_zc && force_copy)
 +		return -EINVAL;
 +
 +	if (xdp_get_umem_from_qid(dev, queue_id))
 +		return -EBUSY;
 +
 +	err = xdp_reg_umem_at_qid(dev, umem, queue_id);
 +	if (err)
 +		return err;
 +
 +	umem->dev = dev;
 +	umem->queue_id = queue_id;
 +
 +	if (flags & XDP_USE_NEED_WAKEUP) {
 +		umem->flags |= XDP_UMEM_USES_NEED_WAKEUP;
 +		/* Tx needs to be explicitly woken up the first time.
 +		 * Also for supporting drivers that do not implement this
 +		 * feature. They will always have to call sendto().
 +		 */
 +		xsk_set_tx_need_wakeup(umem);
 +	}
 +
 +	dev_hold(dev);
 +
 +	if (force_copy)
 +		/* For copy-mode, we are done. */
 +		return 0;
 +
 +	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_wakeup) {
 +		err = -EOPNOTSUPP;
 +		goto err_unreg_umem;
 +	}
 +
 +	bpf.command = XDP_SETUP_XSK_UMEM;
 +	bpf.xsk.umem = umem;
 +	bpf.xsk.queue_id = queue_id;
 +
 +	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
 +	if (err)
 +		goto err_unreg_umem;
 +
 +	umem->zc = true;
 +	return 0;
 +
 +err_unreg_umem:
 +	if (!force_zc)
 +		err = 0; /* fallback to copy mode */
 +	if (err)
 +		xdp_clear_umem_at_qid(dev, queue_id);
 +	return err;
 +}
 +
 +void xdp_umem_clear_dev(struct xdp_umem *umem)
 +{
 +	struct netdev_bpf bpf;
 +	int err;
 +
 +	ASSERT_RTNL();
 +
 +	if (!umem->dev)
 +		return;
 +
 +	if (umem->zc) {
 +		bpf.command = XDP_SETUP_XSK_UMEM;
 +		bpf.xsk.umem = NULL;
 +		bpf.xsk.queue_id = umem->queue_id;
 +
 +		err = umem->dev->netdev_ops->ndo_bpf(umem->dev, &bpf);
 +
 +		if (err)
 +			WARN(1, "failed to disable umem!\n");
 +	}
 +
 +	xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
 +
 +	dev_put(umem->dev);
 +	umem->dev = NULL;
 +	umem->zc = false;
 +}
 +
++=======
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  static void xdp_umem_unpin_pages(struct xdp_umem *umem)
  {
  	unpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
diff --cc net/xdp/xsk.c
index 10c97cce9e3d,067e85424d36..000000000000
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@@ -262,7 -300,7 +262,11 @@@ void xsk_umem_consume_tx_done(struct xd
  	struct xdp_sock *xs;
  
  	rcu_read_lock();
++<<<<<<< HEAD
 +	list_for_each_entry_rcu(xs, &umem->xsk_tx_list, list) {
++=======
+ 	list_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  		__xskq_cons_release(xs->tx);
  		xs->sk.sk_write_space(&xs->sk);
  	}
@@@ -275,8 -313,8 +279,13 @@@ bool xsk_umem_consume_tx(struct xdp_ume
  	struct xdp_sock *xs;
  
  	rcu_read_lock();
++<<<<<<< HEAD
 +	list_for_each_entry_rcu(xs, &umem->xsk_tx_list, list) {
 +		if (!xskq_cons_peek_desc(xs->tx, desc, umem)) {
++=======
+ 	list_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {
+ 		if (!xskq_cons_peek_desc(xs->tx, desc, pool)) {
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  			xs->tx->queue_empty_descs++;
  			continue;
  		}
diff --cc net/xdp/xsk_buff_pool.c
index a2044c245215,dbd913ef4928..000000000000
--- a/net/xdp/xsk_buff_pool.c
+++ b/net/xdp/xsk_buff_pool.c
@@@ -2,9 -2,39 +2,33 @@@
  
  #include <net/xsk_buff_pool.h>
  #include <net/xdp_sock.h>
 -#include <net/xdp_sock_drv.h>
 -#include <linux/dma-direct.h>
 -#include <linux/dma-noncoherent.h>
 -#include <linux/swiotlb.h>
  
  #include "xsk_queue.h"
 -#include "xdp_umem.h"
 -#include "xsk.h"
  
+ void xp_add_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)
+ {
+ 	unsigned long flags;
+ 
+ 	if (!xs->tx)
+ 		return;
+ 
+ 	spin_lock_irqsave(&pool->xsk_tx_list_lock, flags);
+ 	list_add_rcu(&xs->tx_list, &pool->xsk_tx_list);
+ 	spin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);
+ }
+ 
+ void xp_del_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)
+ {
+ 	unsigned long flags;
+ 
+ 	if (!xs->tx)
+ 		return;
+ 
+ 	spin_lock_irqsave(&pool->xsk_tx_list_lock, flags);
+ 	list_del_rcu(&xs->tx_list);
+ 	spin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);
+ }
+ 
  static void xp_addr_unmap(struct xsk_buff_pool *pool)
  {
  	vunmap(pool->addrs);
@@@ -46,15 -76,25 +70,26 @@@ struct xsk_buff_pool *xp_create(struct 
  	if (!pool->heads)
  		goto out;
  
 -	pool->chunk_mask = ~((u64)umem->chunk_size - 1);
 -	pool->addrs_cnt = umem->size;
 -	pool->heads_cnt = umem->chunks;
 -	pool->free_heads_cnt = umem->chunks;
 -	pool->headroom = umem->headroom;
 -	pool->chunk_size = umem->chunk_size;
 -	pool->unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;
 -	pool->frame_len = umem->chunk_size - umem->headroom -
 -		XDP_PACKET_HEADROOM;
 -	pool->umem = umem;
 +	pool->chunk_mask = ~((u64)chunk_size - 1);
 +	pool->addrs_cnt = size;
 +	pool->heads_cnt = chunks;
 +	pool->free_heads_cnt = chunks;
 +	pool->headroom = headroom;
 +	pool->chunk_size = chunk_size;
 +	pool->unaligned = unaligned;
 +	pool->frame_len = chunk_size - headroom - XDP_PACKET_HEADROOM;
  	INIT_LIST_HEAD(&pool->free_list);
++<<<<<<< HEAD
++=======
+ 	INIT_LIST_HEAD(&pool->xsk_tx_list);
+ 	spin_lock_init(&pool->xsk_tx_list_lock);
+ 	refcount_set(&pool->users, 1);
+ 
+ 	pool->fq = xs->fq_tmp;
+ 	pool->cq = xs->cq_tmp;
+ 	xs->fq_tmp = NULL;
+ 	xs->cq_tmp = NULL;
++>>>>>>> a5aa8e529e36 (xsk: Move xsk_tx_list and its lock to buffer pool)
  
  	for (i = 0; i < pool->free_heads_cnt; i++) {
  		xskb = &pool->heads[i];
diff --git a/include/net/xdp_sock.h b/include/net/xdp_sock.h
index c9d87cc40c11..4d8d41e52e1e 100644
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@ -35,8 +35,6 @@ struct xdp_umem {
 	int id;
 	struct net_device *dev;
 	bool zc;
-	spinlock_t xsk_tx_list_lock;
-	struct list_head xsk_tx_list;
 };
 
 struct xsk_map {
@@ -62,7 +60,7 @@ struct xdp_sock {
 	/* Protects multiple processes in the control path */
 	struct mutex mutex;
 	struct xsk_queue *tx ____cacheline_aligned_in_smp;
-	struct list_head list;
+	struct list_head tx_list;
 	/* Mutual exclusion of NAPI TX thread and sendmsg error paths
 	 * in the SKB destructor callback.
 	 */
* Unmerged path include/net/xsk_buff_pool.h
* Unmerged path net/xdp/xdp_umem.c
diff --git a/net/xdp/xdp_umem.h b/net/xdp/xdp_umem.h
index 32067fe98f65..5a19a7e1ca51 100644
--- a/net/xdp/xdp_umem.h
+++ b/net/xdp/xdp_umem.h
@@ -14,8 +14,6 @@ void xdp_umem_clear_dev(struct xdp_umem *umem);
 bool xdp_umem_validate_queues(struct xdp_umem *umem);
 void xdp_get_umem(struct xdp_umem *umem);
 void xdp_put_umem(struct xdp_umem *umem);
-void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs);
-void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs);
 struct xdp_umem *xdp_umem_create(struct xdp_umem_reg *mr);
 
 #endif /* XDP_UMEM_H_ */
* Unmerged path net/xdp/xsk.c
* Unmerged path net/xdp/xsk_buff_pool.c
