locking/lockdep: Clean up check_redundant() a bit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 175b1a60e8805617d74aefe17ce0d3a32eceb55c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/175b1a60.failed

In preparation for adding an TRACE_IRQFLAGS dependent skip function to
check_redundant(), move it below the TRACE_IRQFLAGS #ifdef.

While there, provide a stub function to reduce #ifdef usage.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
(cherry picked from commit 175b1a60e8805617d74aefe17ce0d3a32eceb55c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/lockdep.c
diff --cc kernel/locking/lockdep.c
index 53f2c3da5c8b,f2ae8a65f667..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -1923,45 -2130,57 +1923,48 @@@ check_noncircular(struct held_lock *src
  	return ret;
  }
  
 -#ifdef CONFIG_TRACE_IRQFLAGS
 -
++<<<<<<< HEAD
 +#ifdef CONFIG_LOCKDEP_SMALL
  /*
 - * Forwards and backwards subgraph searching, for the purposes of
 - * proving that two subgraphs can be connected by a new dependency
 - * without creating any illegal irq-safe -> irq-unsafe lock dependency.
 - *
 - * A irq safe->unsafe deadlock happens with the following conditions:
 - *
 - * 1) We have a strong dependency path A -> ... -> B
 - *
 - * 2) and we have ENABLED_IRQ usage of B and USED_IN_IRQ usage of A, therefore
 - *    irq can create a new dependency B -> A (consider the case that a holder
 - *    of B gets interrupted by an irq whose handler will try to acquire A).
 - *
 - * 3) the dependency circle A -> ... -> B -> A we get from 1) and 2) is a
 - *    strong circle:
 - *
 - *      For the usage bits of B:
 - *        a) if A -> B is -(*N)->, then B -> A could be any type, so any
 - *           ENABLED_IRQ usage suffices.
 - *        b) if A -> B is -(*R)->, then B -> A must be -(E*)->, so only
 - *           ENABLED_IRQ_*_READ usage suffices.
 + * Check that the dependency graph starting at <src> can lead to
 + * <target> or not. If it can, <src> -> <target> dependency is already
 + * in the graph.
   *
 - *      For the usage bits of A:
 - *        c) if A -> B is -(E*)->, then B -> A could be any type, so any
 - *           USED_IN_IRQ usage suffices.
 - *        d) if A -> B is -(S*)->, then B -> A must be -(*N)->, so only
 - *           USED_IN_IRQ_*_READ usage suffices.
 + * Print an error and return 2 if it does or 1 if it does not.
   */
 +static noinline int
 +check_redundant(struct held_lock *src, struct held_lock *target)
 +{
 +	int ret;
 +	struct lock_list *uninitialized_var(target_entry);
 +	struct lock_list src_entry = {
 +		.class = hlock_class(src),
 +		.parent = NULL,
 +	};
  
 -/*
 - * There is a strong dependency path in the dependency graph: A -> B, and now
 - * we need to decide which usage bit of A should be accumulated to detect
 - * safe->unsafe bugs.
 - *
 - * Note that usage_accumulate() is used in backwards search, so ->only_xr
 - * stands for whether A -> B only has -(S*)-> (in this case ->only_xr is true).
 - *
 - * As above, if only_xr is false, which means A -> B has -(E*)-> dependency
 - * path, any usage of A should be considered. Otherwise, we should only
 - * consider _READ usage.
 - */
 -static inline bool usage_accumulate(struct lock_list *entry, void *mask)
 +	debug_atomic_inc(nr_redundant_checks);
 +
 +	ret = check_path(hlock_class(target), &src_entry, &target_entry);
 +
 +	if (!ret) {
 +		debug_atomic_inc(nr_redundant);
 +		ret = 2;
 +	} else if (ret < 0)
 +		ret = 0;
 +
 +	return ret;
 +}
 +#endif
 +
++=======
++>>>>>>> 175b1a60e880 (locking/lockdep: Clean up check_redundant() a bit)
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +
 +static inline int usage_accumulate(struct lock_list *entry, void *mask)
  {
 -	if (!entry->only_xr)
 -		*(unsigned long *)mask |= entry->class->usage_mask;
 -	else /* Mask out _READ usage bits */
 -		*(unsigned long *)mask |= (entry->class->usage_mask & LOCKF_IRQ);
 +	*(unsigned long *)mask |= entry->class->usage_mask;
  
 -	return false;
 +	return 0;
  }
  
  /*
@@@ -2644,9 -2939,10 +2695,16 @@@ check_prev_add(struct task_struct *curr
  	 * Is the <prev> -> <next> link redundant?
  	 */
  	ret = check_redundant(prev, next);
++<<<<<<< HEAD
 +	if (ret != 1)
 +		return ret;
 +#endif
++=======
+ 	if (bfs_error(ret))
+ 		return 0;
+ 	else if (ret == BFS_RMATCH)
+ 		return 2;
++>>>>>>> 175b1a60e880 (locking/lockdep: Clean up check_redundant() a bit)
  
  	if (!*trace) {
  		*trace = save_trace();
* Unmerged path kernel/locking/lockdep.c
