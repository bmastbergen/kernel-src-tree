xsk: Enable sharing of dma mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit 921b68692abb4fd02237b6875b2056bc59435116
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/921b6869.failed

Enable the sharing of dma mappings by moving them out from the buffer
pool. Instead we put each dma mapped umem region in a list in the umem
structure. If dma has already been mapped for this umem and device, it
is not mapped again and the existing dma mappings are reused.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/1598603189-32145-9-git-send-email-magnus.karlsson@intel.com
(cherry picked from commit 921b68692abb4fd02237b6875b2056bc59435116)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xdp_sock.h
#	net/xdp/xsk_buff_pool.c
diff --cc include/net/xdp_sock.h
index c9d87cc40c11,282aeba0d20f..000000000000
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@@ -24,19 -22,15 +24,23 @@@ struct xdp_umem 
  	u64 size;
  	u32 headroom;
  	u32 chunk_size;
 -	u32 chunks;
  	struct user_struct *user;
  	refcount_t users;
 +	struct work_struct work;
  	struct page **pgs;
  	u32 npgs;
 +	u16 queue_id;
 +	u8 need_wakeup;
  	u8 flags;
  	int id;
 +	struct net_device *dev;
  	bool zc;
++<<<<<<< HEAD
 +	spinlock_t xsk_tx_list_lock;
 +	struct list_head xsk_tx_list;
++=======
+ 	struct list_head xsk_dma_list;
++>>>>>>> 921b68692abb (xsk: Enable sharing of dma mappings)
  };
  
  struct xsk_map {
diff --cc net/xdp/xsk_buff_pool.c
index a2044c245215,547eb41f7837..000000000000
--- a/net/xdp/xsk_buff_pool.c
+++ b/net/xdp/xsk_buff_pool.c
@@@ -86,18 -104,197 +86,201 @@@ void xp_set_rxq_info(struct xsk_buff_po
  }
  EXPORT_SYMBOL(xp_set_rxq_info);
  
++<<<<<<< HEAD
 +void xp_dma_unmap(struct xsk_buff_pool *pool, unsigned long attrs)
++=======
+ static void xp_disable_drv_zc(struct xsk_buff_pool *pool)
  {
- 	dma_addr_t *dma;
- 	u32 i;
+ 	struct netdev_bpf bpf;
+ 	int err;
  
- 	if (pool->dma_pages_cnt == 0)
+ 	ASSERT_RTNL();
+ 
+ 	if (pool->umem->zc) {
+ 		bpf.command = XDP_SETUP_XSK_POOL;
+ 		bpf.xsk.pool = NULL;
+ 		bpf.xsk.queue_id = pool->queue_id;
+ 
+ 		err = pool->netdev->netdev_ops->ndo_bpf(pool->netdev, &bpf);
+ 
+ 		if (err)
+ 			WARN(1, "Failed to disable zero-copy!\n");
+ 	}
+ }
+ 
+ int xp_assign_dev(struct xsk_buff_pool *pool, struct net_device *netdev,
+ 		  u16 queue_id, u16 flags)
+ {
+ 	bool force_zc, force_copy;
+ 	struct netdev_bpf bpf;
+ 	int err = 0;
+ 
+ 	ASSERT_RTNL();
+ 
+ 	force_zc = flags & XDP_ZEROCOPY;
+ 	force_copy = flags & XDP_COPY;
+ 
+ 	if (force_zc && force_copy)
+ 		return -EINVAL;
+ 
+ 	if (xsk_get_pool_from_qid(netdev, queue_id))
+ 		return -EBUSY;
+ 
+ 	pool->netdev = netdev;
+ 	pool->queue_id = queue_id;
+ 	err = xsk_reg_pool_at_qid(netdev, pool, queue_id);
+ 	if (err)
+ 		return err;
+ 
+ 	if (flags & XDP_USE_NEED_WAKEUP) {
+ 		pool->uses_need_wakeup = true;
+ 		/* Tx needs to be explicitly woken up the first time.
+ 		 * Also for supporting drivers that do not implement this
+ 		 * feature. They will always have to call sendto().
+ 		 */
+ 		pool->cached_need_wakeup = XDP_WAKEUP_TX;
+ 	}
+ 
+ 	dev_hold(netdev);
+ 
+ 	if (force_copy)
+ 		/* For copy-mode, we are done. */
+ 		return 0;
+ 
+ 	if (!netdev->netdev_ops->ndo_bpf ||
+ 	    !netdev->netdev_ops->ndo_xsk_wakeup) {
+ 		err = -EOPNOTSUPP;
+ 		goto err_unreg_pool;
+ 	}
+ 
+ 	bpf.command = XDP_SETUP_XSK_POOL;
+ 	bpf.xsk.pool = pool;
+ 	bpf.xsk.queue_id = queue_id;
+ 
+ 	err = netdev->netdev_ops->ndo_bpf(netdev, &bpf);
+ 	if (err)
+ 		goto err_unreg_pool;
+ 
+ 	if (!pool->dma_pages) {
+ 		WARN(1, "Driver did not DMA map zero-copy buffers");
+ 		goto err_unreg_xsk;
+ 	}
+ 	pool->umem->zc = true;
+ 	return 0;
+ 
+ err_unreg_xsk:
+ 	xp_disable_drv_zc(pool);
+ err_unreg_pool:
+ 	if (!force_zc)
+ 		err = 0; /* fallback to copy mode */
+ 	if (err)
+ 		xsk_clear_pool_at_qid(netdev, queue_id);
+ 	return err;
+ }
+ 
+ void xp_clear_dev(struct xsk_buff_pool *pool)
+ {
+ 	if (!pool->netdev)
+ 		return;
+ 
+ 	xp_disable_drv_zc(pool);
+ 	xsk_clear_pool_at_qid(pool->netdev, pool->queue_id);
+ 	dev_put(pool->netdev);
+ 	pool->netdev = NULL;
+ }
+ 
+ static void xp_release_deferred(struct work_struct *work)
+ {
+ 	struct xsk_buff_pool *pool = container_of(work, struct xsk_buff_pool,
+ 						  work);
+ 
+ 	rtnl_lock();
+ 	xp_clear_dev(pool);
+ 	rtnl_unlock();
+ 
+ 	if (pool->fq) {
+ 		xskq_destroy(pool->fq);
+ 		pool->fq = NULL;
+ 	}
+ 
+ 	if (pool->cq) {
+ 		xskq_destroy(pool->cq);
+ 		pool->cq = NULL;
+ 	}
+ 
+ 	xdp_put_umem(pool->umem);
+ 	xp_destroy(pool);
+ }
+ 
+ void xp_get_pool(struct xsk_buff_pool *pool)
+ {
+ 	refcount_inc(&pool->users);
+ }
+ 
+ void xp_put_pool(struct xsk_buff_pool *pool)
+ {
+ 	if (!pool)
  		return;
  
- 	for (i = 0; i < pool->dma_pages_cnt; i++) {
- 		dma = &pool->dma_pages[i];
+ 	if (refcount_dec_and_test(&pool->users)) {
+ 		INIT_WORK(&pool->work, xp_release_deferred);
+ 		schedule_work(&pool->work);
+ 	}
+ }
+ 
+ static struct xsk_dma_map *xp_find_dma_map(struct xsk_buff_pool *pool)
+ {
+ 	struct xsk_dma_map *dma_map;
+ 
+ 	list_for_each_entry(dma_map, &pool->umem->xsk_dma_list, list) {
+ 		if (dma_map->netdev == pool->netdev)
+ 			return dma_map;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static struct xsk_dma_map *xp_create_dma_map(struct device *dev, struct net_device *netdev,
+ 					     u32 nr_pages, struct xdp_umem *umem)
+ {
+ 	struct xsk_dma_map *dma_map;
+ 
+ 	dma_map = kzalloc(sizeof(*dma_map), GFP_KERNEL);
+ 	if (!dma_map)
+ 		return NULL;
+ 
+ 	dma_map->dma_pages = kvcalloc(nr_pages, sizeof(*dma_map->dma_pages), GFP_KERNEL);
+ 	if (!dma_map) {
+ 		kfree(dma_map);
+ 		return NULL;
+ 	}
+ 
+ 	dma_map->netdev = netdev;
+ 	dma_map->dev = dev;
+ 	dma_map->dma_need_sync = false;
+ 	dma_map->dma_pages_cnt = nr_pages;
+ 	refcount_set(&dma_map->users, 0);
+ 	list_add(&dma_map->list, &umem->xsk_dma_list);
+ 	return dma_map;
+ }
+ 
+ static void xp_destroy_dma_map(struct xsk_dma_map *dma_map)
+ {
+ 	list_del(&dma_map->list);
+ 	kvfree(dma_map->dma_pages);
+ 	kfree(dma_map);
+ }
+ 
+ static void __xp_dma_unmap(struct xsk_dma_map *dma_map, unsigned long attrs)
++>>>>>>> 921b68692abb (xsk: Enable sharing of dma mappings)
+ {
+ 	dma_addr_t *dma;
+ 	u32 i;
+ 
+ 	for (i = 0; i < dma_map->dma_pages_cnt; i++) {
+ 		dma = &dma_map->dma_pages[i];
  		if (*dma) {
- 			dma_unmap_page_attrs(pool->dev, *dma, PAGE_SIZE,
+ 			dma_unmap_page_attrs(dma_map->dev, *dma, PAGE_SIZE,
  					     DMA_BIDIRECTIONAL, attrs);
  			*dma = 0;
  		}
* Unmerged path include/net/xdp_sock.h
diff --git a/include/net/xsk_buff_pool.h b/include/net/xsk_buff_pool.h
index 6842990e2712..47d7ccf3435b 100644
--- a/include/net/xsk_buff_pool.h
+++ b/include/net/xsk_buff_pool.h
@@ -26,9 +26,22 @@ struct xdp_buff_xsk {
 	struct list_head free_list_node;
 };
 
+struct xsk_dma_map {
+	dma_addr_t *dma_pages;
+	struct device *dev;
+	struct net_device *netdev;
+	refcount_t users;
+	struct list_head list; /* Protected by the RTNL_LOCK */
+	u32 dma_pages_cnt;
+	bool dma_need_sync;
+};
+
 struct xsk_buff_pool {
 	struct xsk_queue *fq;
 	struct list_head free_list;
+	/* For performance reasons, each buff pool has its own array of dma_pages
+	 * even when they are identical.
+	 */
 	dma_addr_t *dma_pages;
 	struct xdp_buff_xsk *heads;
 	u64 chunk_mask;
diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index fb8d9af5bc04..7b15dc7f558d 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -362,6 +362,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	INIT_LIST_HEAD(&umem->xsk_tx_list);
 	spin_lock_init(&umem->xsk_tx_list_lock);
 
+	INIT_LIST_HEAD(&umem->xsk_dma_list);
 	refcount_set(&umem->users, 1);
 
 	err = xdp_umem_account_pages(umem);
* Unmerged path net/xdp/xsk_buff_pool.c
