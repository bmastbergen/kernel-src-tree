scsi: smartpqi: Align code with oob driver

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Kevin Barnett <kevin.barnett@microchip.com>
commit 583891c9e509256a2b2902607c2e7a7c36beb0d3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/583891c9.failed

Reduce differences between out-of-box driver and kernel.org driver. No
functional changes.

Link: https://lore.kernel.org/r/161549375094.25025.9268879575316758510.stgit@brunhilda
	Reviewed-by: Scott Benesh <scott.benesh@microchip.com>
	Reviewed-by: Mike McGowen <mike.mcgowen@microchip.com>
	Reviewed-by: Scott Teel <scott.teel@microchip.com>
	Signed-off-by: Kevin Barnett <kevin.barnett@microchip.com>
	Signed-off-by: Don Brace <don.brace@microchip.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 583891c9e509256a2b2902607c2e7a7c36beb0d3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/smartpqi/smartpqi.h
#	drivers/scsi/smartpqi/smartpqi_init.c
diff --cc drivers/scsi/smartpqi/smartpqi.h
index 2a82a166f773,6639432f3dab..000000000000
--- a/drivers/scsi/smartpqi/smartpqi.h
+++ b/drivers/scsi/smartpqi/smartpqi.h
@@@ -308,10 -308,75 +307,78 @@@ struct pqi_aio_path_request 
  	u8	cdb_length;
  	u8	lun_number[8];
  	u8	reserved4[4];
- 	struct pqi_sg_descriptor
- 		sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
+ 	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
  };
  
++<<<<<<< HEAD
++=======
+ #define PQI_RAID1_NVME_XFER_LIMIT	(32 * 1024)	/* 32 KiB */
+ 
+ struct pqi_aio_r1_path_request {
+ 	struct pqi_iu_header header;
+ 	__le16	request_id;
+ 	__le16	volume_id;	/* ID of the RAID volume */
+ 	__le32	it_nexus_1;	/* IT nexus of the 1st drive in the RAID volume */
+ 	__le32	it_nexus_2;	/* IT nexus of the 2nd drive in the RAID volume */
+ 	__le32	it_nexus_3;	/* IT nexus of the 3rd drive in the RAID volume */
+ 	__le32	data_length;	/* total bytes to read/write */
+ 	u8	data_direction : 2;
+ 	u8	partial : 1;
+ 	u8	memory_type : 1;
+ 	u8	fence : 1;
+ 	u8	encryption_enable : 1;
+ 	u8	reserved : 2;
+ 	u8	task_attribute : 3;
+ 	u8	command_priority : 4;
+ 	u8	reserved2 : 1;
+ 	__le16	data_encryption_key_index;
+ 	u8	cdb[16];
+ 	__le16	error_index;
+ 	u8	num_sg_descriptors;
+ 	u8	cdb_length;
+ 	u8	num_drives;	/* number of drives in the RAID volume (2 or 3) */
+ 	u8	reserved3[3];
+ 	__le32	encrypt_tweak_lower;
+ 	__le32	encrypt_tweak_upper;
+ 	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
+ };
+ 
+ #define PQI_DEFAULT_MAX_WRITE_RAID_5_6			(8 * 1024U)
+ #define PQI_DEFAULT_MAX_TRANSFER_ENCRYPTED_SAS_SATA	(~0U)
+ #define PQI_DEFAULT_MAX_TRANSFER_ENCRYPTED_NVME		(32 * 1024U)
+ 
+ struct pqi_aio_r56_path_request {
+ 	struct pqi_iu_header header;
+ 	__le16	request_id;
+ 	__le16	volume_id;		/* ID of the RAID volume */
+ 	__le32	data_it_nexus;		/* IT nexus for the data drive */
+ 	__le32	p_parity_it_nexus;	/* IT nexus for the P parity drive */
+ 	__le32	q_parity_it_nexus;	/* IT nexus for the Q parity drive */
+ 	__le32	data_length;		/* total bytes to read/write */
+ 	u8	data_direction : 2;
+ 	u8	partial : 1;
+ 	u8	mem_type : 1;		/* 0 = PCIe, 1 = DDR */
+ 	u8	fence : 1;
+ 	u8	encryption_enable : 1;
+ 	u8	reserved : 2;
+ 	u8	task_attribute : 3;
+ 	u8	command_priority : 4;
+ 	u8	reserved1 : 1;
+ 	__le16	data_encryption_key_index;
+ 	u8	cdb[16];
+ 	__le16	error_index;
+ 	u8	num_sg_descriptors;
+ 	u8	cdb_length;
+ 	u8	xor_multiplier;
+ 	u8	reserved2[3];
+ 	__le32	encrypt_tweak_lower;
+ 	__le32	encrypt_tweak_upper;
+ 	__le64	row;			/* row = logical LBA/blocks per row */
+ 	u8	reserved3[8];
+ 	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS];
+ };
+ 
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  struct pqi_io_response {
  	struct pqi_iu_header header;
  	__le16	request_id;
@@@ -761,13 -829,28 +828,33 @@@ struct pqi_config_table_firmware_featur
  	u8	features_supported[];
  /*	u8	features_requested_by_host[]; */
  /*	u8	features_enabled[]; */
 -/* The 2 fields below are only valid if the MAX_KNOWN_FEATURE bit is set. */
 -/*	__le16	firmware_max_known_feature; */
 -/*	__le16	host_max_known_feature; */
  };
  
++<<<<<<< HEAD
 +#define PQI_FIRMWARE_FEATURE_OFA			0
 +#define PQI_FIRMWARE_FEATURE_SMP			1
 +#define PQI_FIRMWARE_FEATURE_SOFT_RESET_HANDSHAKE	11
 +#define PQI_FIRMWARE_FEATURE_RAID_IU_TIMEOUT		13
 +#define PQI_FIRMWARE_FEATURE_TMF_IU_TIMEOUT		14
++=======
+ #define PQI_FIRMWARE_FEATURE_OFA				0
+ #define PQI_FIRMWARE_FEATURE_SMP				1
+ #define PQI_FIRMWARE_FEATURE_MAX_KNOWN_FEATURE			2
+ #define PQI_FIRMWARE_FEATURE_RAID_0_READ_BYPASS			3
+ #define PQI_FIRMWARE_FEATURE_RAID_1_READ_BYPASS			4
+ #define PQI_FIRMWARE_FEATURE_RAID_5_READ_BYPASS			5
+ #define PQI_FIRMWARE_FEATURE_RAID_6_READ_BYPASS			6
+ #define PQI_FIRMWARE_FEATURE_RAID_0_WRITE_BYPASS		7
+ #define PQI_FIRMWARE_FEATURE_RAID_1_WRITE_BYPASS		8
+ #define PQI_FIRMWARE_FEATURE_RAID_5_WRITE_BYPASS		9
+ #define PQI_FIRMWARE_FEATURE_RAID_6_WRITE_BYPASS		10
+ #define PQI_FIRMWARE_FEATURE_SOFT_RESET_HANDSHAKE		11
+ #define PQI_FIRMWARE_FEATURE_UNIQUE_SATA_WWN			12
+ #define PQI_FIRMWARE_FEATURE_RAID_IU_TIMEOUT			13
+ #define PQI_FIRMWARE_FEATURE_TMF_IU_TIMEOUT			14
+ #define PQI_FIRMWARE_FEATURE_RAID_BYPASS_ON_ENCRYPTED_NVME	15
+ #define PQI_FIRMWARE_FEATURE_MAXIMUM				15
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  
  struct pqi_config_table_debug {
  	struct pqi_config_table_section_header header;
@@@ -908,6 -991,56 +995,59 @@@ struct raid_map 
  
  #pragma pack()
  
++<<<<<<< HEAD
++=======
+ struct pqi_scsi_dev_raid_map_data {
+ 	bool	is_write;
+ 	u8	raid_level;
+ 	u32	map_index;
+ 	u64	first_block;
+ 	u64	last_block;
+ 	u32	data_length;
+ 	u32	block_cnt;
+ 	u32	blocks_per_row;
+ 	u64	first_row;
+ 	u64	last_row;
+ 	u32	first_row_offset;
+ 	u32	last_row_offset;
+ 	u32	first_column;
+ 	u32	last_column;
+ 	u64	r5or6_first_row;
+ 	u64	r5or6_last_row;
+ 	u32	r5or6_first_row_offset;
+ 	u32	r5or6_last_row_offset;
+ 	u32	r5or6_first_column;
+ 	u32	r5or6_last_column;
+ 	u16	data_disks_per_row;
+ 	u32	total_disks_per_row;
+ 	u16	layout_map_count;
+ 	u32	stripesize;
+ 	u16	strip_size;
+ 	u32	first_group;
+ 	u32	last_group;
+ 	u32	map_row;
+ 	u32	aio_handle;
+ 	u64	disk_block;
+ 	u32	disk_block_cnt;
+ 	u8	cdb[16];
+ 	u8	cdb_length;
+ 
+ 	/* RAID 1 specific */
+ #define NUM_RAID1_MAP_ENTRIES	3
+ 	u32	num_it_nexus_entries;
+ 	u32	it_nexus[NUM_RAID1_MAP_ENTRIES];
+ 
+ 	/* RAID 5 / RAID 6 specific */
+ 	u32	p_parity_it_nexus;	/* aio_handle */
+ 	u32	q_parity_it_nexus;	/* aio_handle */
+ 	u8	xor_mult;
+ 	u64	row;
+ 	u64	stripe_lba;
+ 	u32	p_index;
+ 	u32	q_index;
+ };
+ 
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  #define RAID_CTLR_LUNID		"\0\0\0\0\0\0\0\0"
  
  struct pqi_scsi_dev {
@@@ -1156,8 -1290,19 +1296,24 @@@ struct pqi_ctrl_info 
  	u8		pqi_mode_enabled : 1;
  	u8		pqi_reset_quiesce_supported : 1;
  	u8		soft_reset_handshake_supported : 1;
++<<<<<<< HEAD
 +	u8		raid_iu_timeout_supported: 1;
 +	u8		tmf_iu_timeout_supported: 1;
++=======
+ 	u8		raid_iu_timeout_supported : 1;
+ 	u8		tmf_iu_timeout_supported : 1;
+ 	u8		enable_r1_writes : 1;
+ 	u8		enable_r5_writes : 1;
+ 	u8		enable_r6_writes : 1;
+ 	u8		lv_drive_type_mix_valid : 1;
+ 
+ 	u8		ciss_report_log_flags;
+ 	u32		max_transfer_encrypted_sas_sata;
+ 	u32		max_transfer_encrypted_nvme;
+ 	u32		max_write_raid_5_6;
+ 	u32		max_write_raid_1_10_2drive;
+ 	u32		max_write_raid_1_10_3drive;
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  
  	struct list_head scsi_device_list;
  	spinlock_t	scsi_device_list_lock;
diff --cc drivers/scsi/smartpqi/smartpqi_init.c
index 590c50864b81,143bb7b64095..000000000000
--- a/drivers/scsi/smartpqi/smartpqi_init.c
+++ b/drivers/scsi/smartpqi/smartpqi_init.c
@@@ -687,6 -690,103 +681,106 @@@ static int pqi_identify_physical_device
  	return rc;
  }
  
++<<<<<<< HEAD
++=======
+ static inline u32 pqi_aio_limit_to_bytes(__le16 *limit)
+ {
+ 	u32 bytes;
+ 
+ 	bytes = get_unaligned_le16(limit);
+ 	if (bytes == 0)
+ 		bytes = ~0;
+ 	else
+ 		bytes *= 1024;
+ 
+ 	return bytes;
+ }
+ 
+ #pragma pack(1)
+ 
+ struct bmic_sense_feature_buffer {
+ 	struct bmic_sense_feature_buffer_header header;
+ 	struct bmic_sense_feature_io_page_aio_subpage aio_subpage;
+ };
+ 
+ #pragma pack()
+ 
+ #define MINIMUM_AIO_SUBPAGE_BUFFER_LENGTH	\
+ 	offsetofend(struct bmic_sense_feature_buffer, \
+ 		aio_subpage.max_write_raid_1_10_3drive)
+ 
+ #define MINIMUM_AIO_SUBPAGE_LENGTH	\
+ 	(offsetofend(struct bmic_sense_feature_io_page_aio_subpage, \
+ 		max_write_raid_1_10_3drive) - \
+ 		sizeof_field(struct bmic_sense_feature_io_page_aio_subpage, header))
+ 
+ static int pqi_get_advanced_raid_bypass_config(struct pqi_ctrl_info *ctrl_info)
+ {
+ 	int rc;
+ 	enum dma_data_direction dir;
+ 	struct pqi_raid_path_request request;
+ 	struct bmic_sense_feature_buffer *buffer;
+ 
+ 	buffer = kmalloc(sizeof(*buffer), GFP_KERNEL);
+ 	if (!buffer)
+ 		return -ENOMEM;
+ 
+ 	rc = pqi_build_raid_path_request(ctrl_info, &request, BMIC_SENSE_FEATURE, RAID_CTLR_LUNID,
+ 		buffer, sizeof(*buffer), 0, &dir);
+ 	if (rc)
+ 		goto error;
+ 
+ 	request.cdb[2] = BMIC_SENSE_FEATURE_IO_PAGE;
+ 	request.cdb[3] = BMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE;
+ 
+ 	rc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL, NO_TIMEOUT);
+ 
+ 	pqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1, dir);
+ 
+ 	if (rc)
+ 		goto error;
+ 
+ 	if (buffer->header.page_code != BMIC_SENSE_FEATURE_IO_PAGE ||
+ 		buffer->header.subpage_code !=
+ 			BMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE ||
+ 		get_unaligned_le16(&buffer->header.buffer_length) <
+ 			MINIMUM_AIO_SUBPAGE_BUFFER_LENGTH ||
+ 		buffer->aio_subpage.header.page_code !=
+ 			BMIC_SENSE_FEATURE_IO_PAGE ||
+ 		buffer->aio_subpage.header.subpage_code !=
+ 			BMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE ||
+ 		get_unaligned_le16(&buffer->aio_subpage.header.page_length) <
+ 			MINIMUM_AIO_SUBPAGE_LENGTH) {
+ 		goto error;
+ 	}
+ 
+ 	ctrl_info->max_transfer_encrypted_sas_sata =
+ 		pqi_aio_limit_to_bytes(
+ 			&buffer->aio_subpage.max_transfer_encrypted_sas_sata);
+ 
+ 	ctrl_info->max_transfer_encrypted_nvme =
+ 		pqi_aio_limit_to_bytes(
+ 			&buffer->aio_subpage.max_transfer_encrypted_nvme);
+ 
+ 	ctrl_info->max_write_raid_5_6 =
+ 		pqi_aio_limit_to_bytes(
+ 			&buffer->aio_subpage.max_write_raid_5_6);
+ 
+ 	ctrl_info->max_write_raid_1_10_2drive =
+ 		pqi_aio_limit_to_bytes(
+ 			&buffer->aio_subpage.max_write_raid_1_10_2drive);
+ 
+ 	ctrl_info->max_write_raid_1_10_3drive =
+ 		pqi_aio_limit_to_bytes(
+ 			&buffer->aio_subpage.max_write_raid_1_10_3drive);
+ 
+ error:
+ 	kfree(buffer);
+ 
+ 	return rc;
+ }
+ 
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  static int pqi_flush_cache(struct pqi_ctrl_info *ctrl_info,
  	enum bmic_flush_cache_shutdown_event shutdown_event)
  {
@@@ -2268,55 -2355,47 +2339,60 @@@ static inline void pqi_set_encryption_i
   * Attempt to perform RAID bypass mapping for a logical volume I/O.
   */
  
 -static bool pqi_aio_raid_level_supported(struct pqi_ctrl_info *ctrl_info,
 -	struct pqi_scsi_dev_raid_map_data *rmd)
 -{
 -	bool is_supported = true;
 -
 -	switch (rmd->raid_level) {
 -	case SA_RAID_0:
 -		break;
 -	case SA_RAID_1:
 -		if (rmd->is_write && (!ctrl_info->enable_r1_writes ||
 -			rmd->data_length > ctrl_info->max_write_raid_1_10_2drive))
 -			is_supported = false;
 -		break;
 -	case SA_RAID_TRIPLE:
 -		if (rmd->is_write && (!ctrl_info->enable_r1_writes ||
 -			rmd->data_length > ctrl_info->max_write_raid_1_10_3drive))
 -			is_supported = false;
 -		break;
 -	case SA_RAID_5:
 -		if (rmd->is_write && (!ctrl_info->enable_r5_writes ||
 -			rmd->data_length > ctrl_info->max_write_raid_5_6))
 -			is_supported = false;
 -		break;
 -	case SA_RAID_6:
 -		if (rmd->is_write && (!ctrl_info->enable_r6_writes ||
 -			rmd->data_length > ctrl_info->max_write_raid_5_6))
 -			is_supported = false;
 -		break;
 -	default:
 -		is_supported = false;
 -		break;
 -	}
 -
 -	return is_supported;
 -}
 -
  #define PQI_RAID_BYPASS_INELIGIBLE	1
  
++<<<<<<< HEAD
 +static int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,
 +	struct pqi_scsi_dev *device, struct scsi_cmnd *scmd,
 +	struct pqi_queue_group *queue_group)
++=======
+ static int pqi_get_aio_lba_and_block_count(struct scsi_cmnd *scmd,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  {
 +	struct raid_map *raid_map;
 +	bool is_write = false;
 +	u32 map_index;
 +	u64 first_block;
 +	u64 last_block;
 +	u32 block_cnt;
 +	u32 blocks_per_row;
 +	u64 first_row;
 +	u64 last_row;
 +	u32 first_row_offset;
 +	u32 last_row_offset;
 +	u32 first_column;
 +	u32 last_column;
 +	u64 r0_first_row;
 +	u64 r0_last_row;
 +	u32 r5or6_blocks_per_row;
 +	u64 r5or6_first_row;
 +	u64 r5or6_last_row;
 +	u32 r5or6_first_row_offset;
 +	u32 r5or6_last_row_offset;
 +	u32 r5or6_first_column;
 +	u32 r5or6_last_column;
 +	u16 data_disks_per_row;
 +	u32 total_disks_per_row;
 +	u16 layout_map_count;
 +	u32 stripesize;
 +	u16 strip_size;
 +	u32 first_group;
 +	u32 last_group;
 +	u32 current_group;
 +	u32 map_row;
 +	u32 aio_handle;
 +	u64 disk_block;
 +	u32 disk_block_cnt;
 +	u8 cdb[16];
 +	u8 cdb_length;
 +	int offload_to_mirror;
 +	struct pqi_encryption_info *encryption_info_ptr;
 +	struct pqi_encryption_info encryption_info;
 +#if BITS_PER_LONG == 32
 +	u64 tmpdiv;
 +#endif
 +
  	/* Check for valid opcode, get LBA and block count. */
  	switch (scmd->cmnd[0]) {
  	case WRITE_6:
@@@ -2355,245 -2434,336 +2431,515 @@@
  		return PQI_RAID_BYPASS_INELIGIBLE;
  	}
  
++<<<<<<< HEAD
 +	/* Check for write to non-RAID-0. */
 +	if (is_write && device->raid_level != SA_RAID_0)
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +	if (unlikely(block_cnt == 0))
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +	last_block = first_block + block_cnt - 1;
 +	raid_map = device->raid_map;
 +
 +	/* Check for invalid block or wraparound. */
 +	if (last_block >= get_unaligned_le64(&raid_map->volume_blk_cnt) ||
 +		last_block < first_block)
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +	data_disks_per_row = get_unaligned_le16(&raid_map->data_disks_per_row);
 +	strip_size = get_unaligned_le16(&raid_map->strip_size);
 +	layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
++=======
+ 	put_unaligned_le32(scsi_bufflen(scmd), &rmd->data_length);
+ 
+ 	return 0;
+ }
+ 
+ static int pci_get_aio_common_raid_map_values(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_scsi_dev_raid_map_data *rmd, struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 
+ 	rmd->last_block = rmd->first_block + rmd->block_cnt - 1;
+ 
+ 	/* Check for invalid block or wraparound. */
+ 	if (rmd->last_block >=
+ 		get_unaligned_le64(&raid_map->volume_blk_cnt) ||
+ 		rmd->last_block < rmd->first_block)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	rmd->data_disks_per_row =
+ 		get_unaligned_le16(&raid_map->data_disks_per_row);
+ 	rmd->strip_size = get_unaligned_le16(&raid_map->strip_size);
+ 	rmd->layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  
  	/* Calculate stripe information for the request. */
 -	rmd->blocks_per_row = rmd->data_disks_per_row * rmd->strip_size;
 +	blocks_per_row = data_disks_per_row * strip_size;
  #if BITS_PER_LONG == 32
 -	tmpdiv = rmd->first_block;
 -	do_div(tmpdiv, rmd->blocks_per_row);
 -	rmd->first_row = tmpdiv;
 -	tmpdiv = rmd->last_block;
 -	do_div(tmpdiv, rmd->blocks_per_row);
 -	rmd->last_row = tmpdiv;
 -	rmd->first_row_offset = (u32)(rmd->first_block - (rmd->first_row * rmd->blocks_per_row));
 -	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row * rmd->blocks_per_row));
 -	tmpdiv = rmd->first_row_offset;
 -	do_div(tmpdiv, rmd->strip_size);
 -	rmd->first_column = tmpdiv;
 -	tmpdiv = rmd->last_row_offset;
 -	do_div(tmpdiv, rmd->strip_size);
 -	rmd->last_column = tmpdiv;
 +	tmpdiv = first_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	first_row = tmpdiv;
 +	tmpdiv = last_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	last_row = tmpdiv;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	tmpdiv = first_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	first_column = tmpdiv;
 +	tmpdiv = last_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	last_column = tmpdiv;
  #else
++<<<<<<< HEAD
 +	first_row = first_block / blocks_per_row;
 +	last_row = last_block / blocks_per_row;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	first_column = first_row_offset / strip_size;
 +	last_column = last_row_offset / strip_size;
 +#endif
 +
 +	/* If this isn't a single row/column then give to the controller. */
 +	if (first_row != last_row || first_column != last_column)
++=======
+ 	rmd->first_row = rmd->first_block / rmd->blocks_per_row;
+ 	rmd->last_row = rmd->last_block / rmd->blocks_per_row;
+ 	rmd->first_row_offset = (u32)(rmd->first_block -
+ 		(rmd->first_row * rmd->blocks_per_row));
+ 	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row *
+ 		rmd->blocks_per_row));
+ 	rmd->first_column = rmd->first_row_offset / rmd->strip_size;
+ 	rmd->last_column = rmd->last_row_offset / rmd->strip_size;
+ #endif
+ 
+ 	/* If this isn't a single row/column then give to the controller. */
+ 	if (rmd->first_row != rmd->last_row ||
+ 		rmd->first_column != rmd->last_column)
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
  	/* Proceeding with driver mapping. */
 -	rmd->total_disks_per_row = rmd->data_disks_per_row +
 +	total_disks_per_row = data_disks_per_row +
  		get_unaligned_le16(&raid_map->metadata_disks_per_row);
 -	rmd->map_row = ((u32)(rmd->first_row >>
 -		raid_map->parity_rotation_shift)) %
 +	map_row = ((u32)(first_row >> raid_map->parity_rotation_shift)) %
  		get_unaligned_le16(&raid_map->row_cnt);
++<<<<<<< HEAD
 +	map_index = (map_row * total_disks_per_row) + first_column;
++=======
+ 	rmd->map_index = (rmd->map_row * rmd->total_disks_per_row) +
+ 		rmd->first_column;
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_calc_aio_r5_or_r6(struct pqi_scsi_dev_raid_map_data *rmd,
+ 	struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 	/* RAID 50/60 */
+ 	/* Verify first and last block are in same RAID group. */
+ 	rmd->stripesize = rmd->blocks_per_row * rmd->layout_map_count;
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->first_group = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->last_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->last_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->last_group = tmpdiv;
+ #else
+ 	rmd->first_group = (rmd->first_block % rmd->stripesize) / rmd->blocks_per_row;
+ 	rmd->last_group = (rmd->last_block % rmd->stripesize) / rmd->blocks_per_row;
+ #endif
+ 	if (rmd->first_group != rmd->last_group)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single row of RAID 5/6. */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->first_row = tmpdiv;
+ 	rmd->r5or6_first_row = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->r5or6_last_row = tmpdiv;
+ #else
+ 	rmd->first_row = rmd->r5or6_first_row =
+ 		rmd->first_block / rmd->stripesize;
+ 	rmd->r5or6_last_row = rmd->last_block / rmd->stripesize;
+ #endif
+ 	if (rmd->r5or6_first_row != rmd->r5or6_last_row)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single column. */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_row_offset;
+ 	rmd->first_row_offset = (u32)do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->r5or6_first_row_offset = rmd->first_row_offset;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->blocks_per_row);
+ 	tmpdiv = rmd->r5or6_first_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->first_column = rmd->r5or6_first_column = tmpdiv;
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->r5or6_last_column = tmpdiv;
+ #else
+ 	rmd->first_row_offset = rmd->r5or6_first_row_offset =
+ 		(u32)((rmd->first_block % rmd->stripesize) %
+ 		rmd->blocks_per_row);
+ 
+ 	rmd->r5or6_last_row_offset =
+ 		(u32)((rmd->last_block % rmd->stripesize) %
+ 		rmd->blocks_per_row);
+ 
+ 	rmd->first_column =
+ 		rmd->r5or6_first_row_offset / rmd->strip_size;
+ 	rmd->r5or6_first_column = rmd->first_column;
+ 	rmd->r5or6_last_column = rmd->r5or6_last_row_offset / rmd->strip_size;
+ #endif
+ 	if (rmd->r5or6_first_column != rmd->r5or6_last_column)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Request is eligible. */
+ 	rmd->map_row =
+ 		((u32)(rmd->first_row >> raid_map->parity_rotation_shift)) %
+ 		get_unaligned_le16(&raid_map->row_cnt);
+ 
+ 	rmd->map_index = (rmd->first_group *
+ 		(get_unaligned_le16(&raid_map->row_cnt) *
+ 		rmd->total_disks_per_row)) +
+ 		(rmd->map_row * rmd->total_disks_per_row) + rmd->first_column;
+ 
+ 	if (rmd->is_write) {
+ 		u32 index;
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  
 +	/* RAID 1 */
 +	if (device->raid_level == SA_RAID_1) {
 +		if (device->offload_to_mirror)
 +			map_index += data_disks_per_row;
 +		device->offload_to_mirror = !device->offload_to_mirror;
 +	} else if (device->raid_level == SA_RAID_ADM) {
 +		/* RAID ADM */
  		/*
 -		 * p_parity_it_nexus and q_parity_it_nexus are pointers to the
 -		 * parity entries inside the device's raid_map.
 -		 *
 -		 * A device's RAID map is bounded by: number of RAID disks squared.
 -		 *
 -		 * The devices RAID map size is checked during device
 -		 * initialization.
 +		 * Handles N-way mirrors  (R1-ADM) and R10 with # of drives
 +		 * divisible by 3.
  		 */
++<<<<<<< HEAD
 +		offload_to_mirror = device->offload_to_mirror;
 +		if (offload_to_mirror == 0)  {
 +			/* use physical disk in the first mirrored group. */
 +			map_index %= data_disks_per_row;
++=======
+ 		index = DIV_ROUND_UP(rmd->map_index + 1, rmd->total_disks_per_row);
+ 		index *= rmd->total_disks_per_row;
+ 		index -= get_unaligned_le16(&raid_map->metadata_disks_per_row);
+ 
+ 		rmd->p_parity_it_nexus = raid_map->disk_data[index].aio_handle;
+ 		if (rmd->raid_level == SA_RAID_6) {
+ 			rmd->q_parity_it_nexus = raid_map->disk_data[index + 1].aio_handle;
+ 			rmd->xor_mult = raid_map->disk_data[rmd->map_index].xor_mult[1];
+ 		}
+ 		if (rmd->blocks_per_row == 0)
+ 			return PQI_RAID_BYPASS_INELIGIBLE;
+ #if BITS_PER_LONG == 32
+ 		tmpdiv = rmd->first_block;
+ 		do_div(tmpdiv, rmd->blocks_per_row);
+ 		rmd->row = tmpdiv;
+ #else
+ 		rmd->row = rmd->first_block / rmd->blocks_per_row;
+ #endif
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void pqi_set_aio_cdb(struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	/* Build the new CDB for the physical disk I/O. */
+ 	if (rmd->disk_block > 0xffffffff) {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_16 : READ_16;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be64(rmd->disk_block, &rmd->cdb[2]);
+ 		put_unaligned_be32(rmd->disk_block_cnt, &rmd->cdb[10]);
+ 		rmd->cdb[14] = 0;
+ 		rmd->cdb[15] = 0;
+ 		rmd->cdb_length = 16;
+ 	} else {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_10 : READ_10;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be32((u32)rmd->disk_block, &rmd->cdb[2]);
+ 		rmd->cdb[6] = 0;
+ 		put_unaligned_be16((u16)rmd->disk_block_cnt, &rmd->cdb[7]);
+ 		rmd->cdb[9] = 0;
+ 		rmd->cdb_length = 10;
+ 	}
+ }
+ 
+ static void pqi_calc_aio_r1_nexus(struct raid_map *raid_map,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	u32 index;
+ 	u32 group;
+ 
+ 	group = rmd->map_index / rmd->data_disks_per_row;
+ 
+ 	index = rmd->map_index - (group * rmd->data_disks_per_row);
+ 	rmd->it_nexus[0] = raid_map->disk_data[index].aio_handle;
+ 	index += rmd->data_disks_per_row;
+ 	rmd->it_nexus[1] = raid_map->disk_data[index].aio_handle;
+ 	if (rmd->layout_map_count > 2) {
+ 		index += rmd->data_disks_per_row;
+ 		rmd->it_nexus[2] = raid_map->disk_data[index].aio_handle;
+ 	}
+ 
+ 	rmd->num_it_nexus_entries = rmd->layout_map_count;
+ }
+ 
+ static int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_scsi_dev *device, struct scsi_cmnd *scmd,
+ 	struct pqi_queue_group *queue_group)
+ {
+ 	int rc;
+ 	struct raid_map *raid_map;
+ 	u32 group;
+ 	u32 next_bypass_group;
+ 	struct pqi_encryption_info *encryption_info_ptr;
+ 	struct pqi_encryption_info encryption_info;
+ 	struct pqi_scsi_dev_raid_map_data rmd = { 0 };
+ 
+ 	rc = pqi_get_aio_lba_and_block_count(scmd, &rmd);
+ 	if (rc)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	rmd.raid_level = device->raid_level;
+ 
+ 	if (!pqi_aio_raid_level_supported(ctrl_info, &rmd))
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	if (unlikely(rmd.block_cnt == 0))
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	raid_map = device->raid_map;
+ 
+ 	rc = pci_get_aio_common_raid_map_values(ctrl_info, &rmd, raid_map);
+ 	if (rc)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	if (device->raid_level == SA_RAID_1 ||
+ 		device->raid_level == SA_RAID_TRIPLE) {
+ 		if (rmd.is_write) {
+ 			pqi_calc_aio_r1_nexus(raid_map, &rmd);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  		} else {
 -			group = device->next_bypass_group;
 -			next_bypass_group = group + 1;
 -			if (next_bypass_group >= rmd.layout_map_count)
 -				next_bypass_group = 0;
 -			device->next_bypass_group = next_bypass_group;
 -			rmd.map_index += group * rmd.data_disks_per_row;
 +			do {
 +				/*
 +				 * Determine mirror group that map_index
 +				 * indicates.
 +				 */
 +				current_group = map_index / data_disks_per_row;
 +
 +				if (offload_to_mirror != current_group) {
 +					if (current_group <
 +						layout_map_count - 1) {
 +						/*
 +						 * Select raid index from
 +						 * next group.
 +						 */
 +						map_index += data_disks_per_row;
 +						current_group++;
 +					} else {
 +						/*
 +						 * Select raid index from first
 +						 * group.
 +						 */
 +						map_index %= data_disks_per_row;
 +						current_group = 0;
 +					}
 +				}
 +			} while (offload_to_mirror != current_group);
  		}
 +
 +		/* Set mirror group to use next time. */
 +		offload_to_mirror =
 +			(offload_to_mirror >= layout_map_count - 1) ?
 +				0 : offload_to_mirror + 1;
 +		device->offload_to_mirror = offload_to_mirror;
 +		/*
 +		 * Avoid direct use of device->offload_to_mirror within this
 +		 * function since multiple threads might simultaneously
 +		 * increment it beyond the range of device->layout_map_count -1.
 +		 */
  	} else if ((device->raid_level == SA_RAID_5 ||
 -		device->raid_level == SA_RAID_6) &&
 -		(rmd.layout_map_count > 1 || rmd.is_write)) {
 -		rc = pqi_calc_aio_r5_or_r6(&rmd, raid_map);
 -		if (rc)
 +		device->raid_level == SA_RAID_6) && layout_map_count > 1) {
 +		/* RAID 50/60 */
 +		/* Verify first and last block are in same RAID group */
 +		r5or6_blocks_per_row = strip_size * data_disks_per_row;
 +		stripesize = r5or6_blocks_per_row * layout_map_count;
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		first_group = tmpdiv;
 +		tmpdiv = last_block;
 +		last_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = last_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		last_group = tmpdiv;
 +#else
 +		first_group = (first_block % stripesize) / r5or6_blocks_per_row;
 +		last_group = (last_block % stripesize) / r5or6_blocks_per_row;
 +#endif
 +		if (first_group != last_group)
  			return PQI_RAID_BYPASS_INELIGIBLE;
 -	}
  
 -	if (unlikely(rmd.map_index >= RAID_MAP_MAX_ENTRIES))
 -		return PQI_RAID_BYPASS_INELIGIBLE;
 +		/* Verify request is in a single row of RAID 5/6 */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		do_div(tmpdiv, stripesize);
 +		first_row = r5or6_first_row = r0_first_row = tmpdiv;
 +		tmpdiv = last_block;
 +		do_div(tmpdiv, stripesize);
 +		r5or6_last_row = r0_last_row = tmpdiv;
 +#else
 +		first_row = r5or6_first_row = r0_first_row =
 +			first_block / stripesize;
 +		r5or6_last_row = r0_last_row = last_block / stripesize;
 +#endif
 +		if (r5or6_first_row != r5or6_last_row)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +		/* Verify request is in a single column */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_row_offset;
 +		first_row_offset = (u32)do_div(tmpdiv, r5or6_blocks_per_row);
 +		r5or6_first_row_offset = first_row_offset;
 +		tmpdiv = last_block;
 +		r5or6_last_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = r5or6_last_row_offset;
 +		r5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);
 +		tmpdiv = r5or6_first_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		first_column = r5or6_first_column = tmpdiv;
 +		tmpdiv = r5or6_last_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		r5or6_last_column = tmpdiv;
 +#else
 +		first_row_offset = r5or6_first_row_offset =
 +			(u32)((first_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		r5or6_last_row_offset =
 +			(u32)((last_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		first_column = r5or6_first_row_offset / strip_size;
 +		r5or6_first_column = first_column;
 +		r5or6_last_column = r5or6_last_row_offset / strip_size;
 +#endif
 +		if (r5or6_first_column != r5or6_last_column)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +		/* Request is eligible */
 +		map_row =
 +			((u32)(first_row >> raid_map->parity_rotation_shift)) %
 +			get_unaligned_le16(&raid_map->row_cnt);
  
 -	rmd.aio_handle = raid_map->disk_data[rmd.map_index].aio_handle;
 -	rmd.disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 -		rmd.first_row * rmd.strip_size +
 -		(rmd.first_row_offset - rmd.first_column * rmd.strip_size);
 -	rmd.disk_block_cnt = rmd.block_cnt;
 +		map_index = (first_group *
 +			(get_unaligned_le16(&raid_map->row_cnt) *
 +			total_disks_per_row)) +
 +			(map_row * total_disks_per_row) + first_column;
 +	}
 +
 +	aio_handle = raid_map->disk_data[map_index].aio_handle;
 +	disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 +		first_row * strip_size +
 +		(first_row_offset - first_column * strip_size);
 +	disk_block_cnt = block_cnt;
  
  	/* Handle differing logical/physical block sizes. */
  	if (raid_map->phys_blk_shift) {
 -		rmd.disk_block <<= raid_map->phys_blk_shift;
 -		rmd.disk_block_cnt <<= raid_map->phys_blk_shift;
 +		disk_block <<= raid_map->phys_blk_shift;
 +		disk_block_cnt <<= raid_map->phys_blk_shift;
  	}
  
 -	if (unlikely(rmd.disk_block_cnt > 0xffff))
 +	if (unlikely(disk_block_cnt > 0xffff))
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 -	pqi_set_aio_cdb(&rmd);
 -
 +	/* Build the new CDB for the physical disk I/O. */
 +	if (disk_block > 0xffffffff) {
 +		cdb[0] = is_write ? WRITE_16 : READ_16;
 +		cdb[1] = 0;
 +		put_unaligned_be64(disk_block, &cdb[2]);
 +		put_unaligned_be32(disk_block_cnt, &cdb[10]);
 +		cdb[14] = 0;
 +		cdb[15] = 0;
 +		cdb_length = 16;
 +	} else {
 +		cdb[0] = is_write ? WRITE_10 : READ_10;
 +		cdb[1] = 0;
 +		put_unaligned_be32((u32)disk_block, &cdb[2]);
 +		cdb[6] = 0;
 +		put_unaligned_be16((u16)disk_block_cnt, &cdb[7]);
 +		cdb[9] = 0;
 +		cdb_length = 10;
 +	}
 +
++<<<<<<< HEAD
 +	if (get_unaligned_le16(&raid_map->flags) &
 +		RAID_MAP_ENCRYPTION_ENABLED) {
 +		pqi_set_encryption_info(&encryption_info, raid_map,
 +			first_block);
++=======
+ 	if (get_unaligned_le16(&raid_map->flags) & RAID_MAP_ENCRYPTION_ENABLED) {
+ 		if (rmd.data_length > device->max_transfer_encrypted)
+ 			return PQI_RAID_BYPASS_INELIGIBLE;
+ 		pqi_set_encryption_info(&encryption_info, raid_map, rmd.first_block);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  		encryption_info_ptr = &encryption_info;
  	} else {
  		encryption_info_ptr = NULL;
  	}
  
++<<<<<<< HEAD
 +	return pqi_aio_submit_io(ctrl_info, scmd, aio_handle,
 +		cdb, cdb_length, queue_group, encryption_info_ptr, true);
++=======
+ 	if (rmd.is_write) {
+ 		switch (device->raid_level) {
+ 		case SA_RAID_1:
+ 		case SA_RAID_TRIPLE:
+ 			return pqi_aio_submit_r1_write_io(ctrl_info, scmd, queue_group,
+ 				encryption_info_ptr, device, &rmd);
+ 		case SA_RAID_5:
+ 		case SA_RAID_6:
+ 			return pqi_aio_submit_r56_write_io(ctrl_info, scmd, queue_group,
+ 				encryption_info_ptr, device, &rmd);
+ 		}
+ 	}
+ 
+ 	return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 		rmd.cdb, rmd.cdb_length, queue_group,
+ 		encryption_info_ptr, true);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  }
  
  #define PQI_STATUS_IDLE		0x0
@@@ -4651,9 -4812,10 +4990,16 @@@ static void pqi_free_all_io_requests(st
  
  static inline int pqi_alloc_error_buffer(struct pqi_ctrl_info *ctrl_info)
  {
++<<<<<<< HEAD
 +	ctrl_info->error_buffer = dma_zalloc_coherent(&ctrl_info->pci_dev->dev,
 +		ctrl_info->error_buffer_length,
 +		&ctrl_info->error_buffer_dma_handle, GFP_KERNEL);
++=======
+ 	ctrl_info->error_buffer = dma_alloc_coherent(&ctrl_info->pci_dev->dev,
+ 				     ctrl_info->error_buffer_length,
+ 				     &ctrl_info->error_buffer_dma_handle,
+ 				     GFP_KERNEL);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  	if (!ctrl_info->error_buffer)
  		return -ENOMEM;
  
@@@ -4813,10 -4972,16 +5156,10 @@@ static void pqi_calculate_queue_resourc
  		PQI_OPERATIONAL_IQ_ELEMENT_LENGTH) /
  		sizeof(struct pqi_sg_descriptor)) +
  		PQI_MAX_EMBEDDED_SG_DESCRIPTORS;
 -
 -	ctrl_info->max_sg_per_r56_iu =
 -		((ctrl_info->max_inbound_iu_length -
 -		PQI_OPERATIONAL_IQ_ELEMENT_LENGTH) /
 -		sizeof(struct pqi_sg_descriptor)) +
 -		PQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS;
  }
  
- static inline void pqi_set_sg_descriptor(
- 	struct pqi_sg_descriptor *sg_descriptor, struct scatterlist *sg)
+ static inline void pqi_set_sg_descriptor(struct pqi_sg_descriptor *sg_descriptor,
+ 	struct scatterlist *sg)
  {
  	u64 address = (u64)sg_dma_address(sg);
  	unsigned int length = sg_dma_len(sg);
@@@ -5304,6 -5530,129 +5634,132 @@@ static int pqi_aio_submit_io(struct pqi
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static  int pqi_aio_submit_r1_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	int rc;
+ 	struct pqi_io_request *io_request;
+ 	struct pqi_aio_r1_path_request *r1_request;
+ 
+ 	io_request = pqi_alloc_io_request(ctrl_info);
+ 	io_request->io_complete_callback = pqi_aio_io_complete;
+ 	io_request->scmd = scmd;
+ 	io_request->raid_bypass = true;
+ 
+ 	r1_request = io_request->iu;
+ 	memset(r1_request, 0, offsetof(struct pqi_aio_r1_path_request, sg_descriptors));
+ 
+ 	r1_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID1_IO;
+ 	put_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r1_request->volume_id);
+ 	r1_request->num_drives = rmd->num_it_nexus_entries;
+ 	put_unaligned_le32(rmd->it_nexus[0], &r1_request->it_nexus_1);
+ 	put_unaligned_le32(rmd->it_nexus[1], &r1_request->it_nexus_2);
+ 	if (rmd->num_it_nexus_entries == 3)
+ 		put_unaligned_le32(rmd->it_nexus[2], &r1_request->it_nexus_3);
+ 
+ 	put_unaligned_le32(scsi_bufflen(scmd), &r1_request->data_length);
+ 	r1_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;
+ 	put_unaligned_le16(io_request->index, &r1_request->request_id);
+ 	r1_request->error_index = r1_request->request_id;
+ 	if (rmd->cdb_length > sizeof(r1_request->cdb))
+ 		rmd->cdb_length = sizeof(r1_request->cdb);
+ 	r1_request->cdb_length = rmd->cdb_length;
+ 	memcpy(r1_request->cdb, rmd->cdb, rmd->cdb_length);
+ 
+ 	/* The direction is always write. */
+ 	r1_request->data_direction = SOP_READ_FLAG;
+ 
+ 	if (encryption_info) {
+ 		r1_request->encryption_enable = true;
+ 		put_unaligned_le16(encryption_info->data_encryption_key_index,
+ 				&r1_request->data_encryption_key_index);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_lower,
+ 				&r1_request->encrypt_tweak_lower);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_upper,
+ 				&r1_request->encrypt_tweak_upper);
+ 	}
+ 
+ 	rc = pqi_build_aio_r1_sg_list(ctrl_info, r1_request, scmd, io_request);
+ 	if (rc) {
+ 		pqi_free_io_request(io_request);
+ 		return SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 
+ 	pqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_aio_submit_r56_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	int rc;
+ 	struct pqi_io_request *io_request;
+ 	struct pqi_aio_r56_path_request *r56_request;
+ 
+ 	io_request = pqi_alloc_io_request(ctrl_info);
+ 	io_request->io_complete_callback = pqi_aio_io_complete;
+ 	io_request->scmd = scmd;
+ 	io_request->raid_bypass = true;
+ 
+ 	r56_request = io_request->iu;
+ 	memset(r56_request, 0, offsetof(struct pqi_aio_r56_path_request, sg_descriptors));
+ 
+ 	if (device->raid_level == SA_RAID_5 || device->raid_level == SA_RAID_51)
+ 		r56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID5_IO;
+ 	else
+ 		r56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID6_IO;
+ 
+ 	put_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r56_request->volume_id);
+ 	put_unaligned_le32(rmd->aio_handle, &r56_request->data_it_nexus);
+ 	put_unaligned_le32(rmd->p_parity_it_nexus, &r56_request->p_parity_it_nexus);
+ 	if (rmd->raid_level == SA_RAID_6) {
+ 		put_unaligned_le32(rmd->q_parity_it_nexus, &r56_request->q_parity_it_nexus);
+ 		r56_request->xor_multiplier = rmd->xor_mult;
+ 	}
+ 	put_unaligned_le32(scsi_bufflen(scmd), &r56_request->data_length);
+ 	r56_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;
+ 	put_unaligned_le64(rmd->row, &r56_request->row);
+ 
+ 	put_unaligned_le16(io_request->index, &r56_request->request_id);
+ 	r56_request->error_index = r56_request->request_id;
+ 
+ 	if (rmd->cdb_length > sizeof(r56_request->cdb))
+ 		rmd->cdb_length = sizeof(r56_request->cdb);
+ 	r56_request->cdb_length = rmd->cdb_length;
+ 	memcpy(r56_request->cdb, rmd->cdb, rmd->cdb_length);
+ 
+ 	/* The direction is always write. */
+ 	r56_request->data_direction = SOP_READ_FLAG;
+ 
+ 	if (encryption_info) {
+ 		r56_request->encryption_enable = true;
+ 		put_unaligned_le16(encryption_info->data_encryption_key_index,
+ 				&r56_request->data_encryption_key_index);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_lower,
+ 				&r56_request->encrypt_tweak_lower);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_upper,
+ 				&r56_request->encrypt_tweak_upper);
+ 	}
+ 
+ 	rc = pqi_build_aio_r56_sg_list(ctrl_info, r56_request, scmd, io_request);
+ 	if (rc) {
+ 		pqi_free_io_request(io_request);
+ 		return SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 
+ 	pqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  static inline u16 pqi_get_hw_queue(struct pqi_ctrl_info *ctrl_info,
  	struct scsi_cmnd *scmd)
  {
@@@ -6251,8 -6681,12 +6708,17 @@@ static DEVICE_ATTR(model, 0444, pqi_mod
  static DEVICE_ATTR(serial_number, 0444, pqi_serial_number_show, NULL);
  static DEVICE_ATTR(vendor, 0444, pqi_vendor_show, NULL);
  static DEVICE_ATTR(rescan, 0200, NULL, pqi_host_rescan_store);
++<<<<<<< HEAD
 +static DEVICE_ATTR(lockup_action, 0644,
 +	pqi_lockup_action_show, pqi_lockup_action_store);
++=======
+ static DEVICE_ATTR(lockup_action, 0644, pqi_lockup_action_show,
+ 	pqi_lockup_action_store);
+ static DEVICE_ATTR(enable_r5_writes, 0644,
+ 	pqi_host_enable_r5_writes_show, pqi_host_enable_r5_writes_store);
+ static DEVICE_ATTR(enable_r6_writes, 0644,
+ 	pqi_host_enable_r6_writes_show, pqi_host_enable_r6_writes_store);
++>>>>>>> 583891c9e509 (scsi: smartpqi: Align code with oob driver)
  
  static struct device_attribute *pqi_shost_attrs[] = {
  	&dev_attr_driver_version,
@@@ -7053,13 -7558,12 +7511,11 @@@ static int pqi_process_config_table(str
  	 * Copy the config table contents from I/O memory space into the
  	 * temporary buffer.
  	 */
- 	table_iomem_addr = ctrl_info->iomem_base +
- 		ctrl_info->config_table_offset;
+ 	table_iomem_addr = ctrl_info->iomem_base + ctrl_info->config_table_offset;
  	memcpy_fromio(config_table, table_iomem_addr, table_length);
  
 -	firmware_feature_section_present = false;
  	section_info.ctrl_info = ctrl_info;
- 	section_offset =
- 		get_unaligned_le32(&config_table->first_section_offset);
+ 	section_offset = get_unaligned_le32(&config_table->first_section_offset);
  
  	while (section_offset) {
  		section = (void *)config_table + section_offset;
@@@ -7094,10 -7597,17 +7548,9 @@@
  			break;
  		}
  
- 		section_offset =
- 			get_unaligned_le16(&section->next_section_offset);
+ 		section_offset = get_unaligned_le16(&section->next_section_offset);
  	}
  
 -	/*
 -	 * We process the firmware feature section after all other sections
 -	 * have been processed so that the feature bit callbacks can take
 -	 * into account the settings configured by other sections.
 -	 */
 -	if (firmware_feature_section_present)
 -		pqi_process_firmware_features_section(&feature_section_info);
 -
  	kfree(config_table);
  
  	return 0;
* Unmerged path drivers/scsi/smartpqi/smartpqi.h
* Unmerged path drivers/scsi/smartpqi/smartpqi_init.c
diff --git a/drivers/scsi/smartpqi/smartpqi_sas_transport.c b/drivers/scsi/smartpqi/smartpqi_sas_transport.c
index c9b00b3368d7..77923c6ec2c6 100644
--- a/drivers/scsi/smartpqi/smartpqi_sas_transport.c
+++ b/drivers/scsi/smartpqi/smartpqi_sas_transport.c
@@ -107,8 +107,7 @@ static int pqi_sas_port_add_rphy(struct pqi_sas_port *pqi_sas_port,
 
 static struct sas_rphy *pqi_sas_rphy_alloc(struct pqi_sas_port *pqi_sas_port)
 {
-	if (pqi_sas_port->device &&
-		pqi_sas_port->device->is_expander_smp_device)
+	if (pqi_sas_port->device && pqi_sas_port->device->is_expander_smp_device)
 		return sas_expander_alloc(pqi_sas_port->port,
 				SAS_FANOUT_EXPANDER_DEVICE);
 
@@ -161,7 +160,7 @@ static void pqi_free_sas_port(struct pqi_sas_port *pqi_sas_port)
 
 	list_for_each_entry_safe(pqi_sas_phy, next,
 		&pqi_sas_port->phy_list_head, phy_list_entry)
-		pqi_free_sas_phy(pqi_sas_phy);
+			pqi_free_sas_phy(pqi_sas_phy);
 
 	sas_port_delete(pqi_sas_port->port);
 	list_del(&pqi_sas_port->port_list_entry);
@@ -191,7 +190,7 @@ static void pqi_free_sas_node(struct pqi_sas_node *pqi_sas_node)
 
 	list_for_each_entry_safe(pqi_sas_port, next,
 		&pqi_sas_node->port_list_head, port_list_entry)
-		pqi_free_sas_port(pqi_sas_port);
+			pqi_free_sas_port(pqi_sas_port);
 
 	kfree(pqi_sas_node);
 }
@@ -498,7 +497,7 @@ static unsigned int pqi_build_sas_smp_handler_reply(
 
 	job->reply_len = le16_to_cpu(error_info->sense_data_length);
 	memcpy(job->reply, error_info->data,
-			le16_to_cpu(error_info->sense_data_length));
+		le16_to_cpu(error_info->sense_data_length));
 
 	return job->reply_payload.payload_len -
 		get_unaligned_le32(&error_info->data_in_transferred);
@@ -547,6 +546,7 @@ void pqi_sas_smp_handler(struct bsg_job *job, struct Scsi_Host *shost,
 		goto out;
 
 	reslen = pqi_build_sas_smp_handler_reply(smp_buf, job, &error_info);
+
 out:
 	bsg_job_done(job, rc, reslen);
 }
diff --git a/drivers/scsi/smartpqi/smartpqi_sis.c b/drivers/scsi/smartpqi/smartpqi_sis.c
index f0199bd87dd1..c954620628e0 100644
--- a/drivers/scsi/smartpqi/smartpqi_sis.c
+++ b/drivers/scsi/smartpqi/smartpqi_sis.c
@@ -71,7 +71,7 @@ struct sis_base_struct {
 						/* error response data */
 	__le32	error_buffer_element_length;	/* length of each PQI error */
 						/* response buffer element */
-						/*   in bytes */
+						/* in bytes */
 	__le32	error_buffer_num_elements;	/* total number of PQI error */
 						/* response buffers available */
 };
@@ -146,7 +146,7 @@ bool sis_is_firmware_running(struct pqi_ctrl_info *ctrl_info)
 bool sis_is_kernel_up(struct pqi_ctrl_info *ctrl_info)
 {
 	return readl(&ctrl_info->registers->sis_firmware_status) &
-				SIS_CTRL_KERNEL_UP;
+		SIS_CTRL_KERNEL_UP;
 }
 
 u32 sis_get_product_id(struct pqi_ctrl_info *ctrl_info)
