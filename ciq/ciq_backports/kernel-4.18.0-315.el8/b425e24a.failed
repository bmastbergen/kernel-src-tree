xsk: Clear pool even for inactive queues

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Maxim Mikityanskiy <maximmi@mellanox.com>
commit b425e24a934e21a502d25089c6c7443d799c5594
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/b425e24a.failed

The number of queues can change by other means, rather than ethtool. For
example, attaching an mqprio qdisc with num_tc > 1 leads to creating
multiple sets of TX queues, which may be then destroyed when mqprio is
deleted. If an AF_XDP socket is created while mqprio is active,
dev->_tx[queue_id].pool will be filled, but then real_num_tx_queues may
decrease with deletion of mqprio, which will mean that the pool won't be
NULLed, and a further increase of the number of TX queues may expose a
dangling pointer.

To avoid any potential misbehavior, this commit clears pool for RX and
TX queues, regardless of real_num_*_queues, still taking into
consideration num_*_queues to avoid overflows.

Fixes: 1c1efc2af158 ("xsk: Create and free buffer pool independently from umem")
Fixes: a41b4f3c58dd ("xsk: simplify xdp_clear_umem_at_qid implementation")
	Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Björn Töpel <bjorn.topel@intel.com>
Link: https://lore.kernel.org/bpf/20210118160333.333439-1-maximmi@mellanox.com
(cherry picked from commit b425e24a934e21a502d25089c6c7443d799c5594)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/xdp/xsk.c
diff --cc net/xdp/xsk.c
index 57e57607ff9e,4a83117507f5..000000000000
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@@ -95,11 -88,51 +95,56 @@@ void xsk_clear_tx_need_wakeup(struct xd
  }
  EXPORT_SYMBOL(xsk_clear_tx_need_wakeup);
  
 -bool xsk_uses_need_wakeup(struct xsk_buff_pool *pool)
 +bool xsk_umem_uses_need_wakeup(struct xdp_umem *umem)
  {
++<<<<<<< HEAD
 +	return umem->flags & XDP_UMEM_USES_NEED_WAKEUP;
++=======
+ 	return pool->uses_need_wakeup;
+ }
+ EXPORT_SYMBOL(xsk_uses_need_wakeup);
+ 
+ struct xsk_buff_pool *xsk_get_pool_from_qid(struct net_device *dev,
+ 					    u16 queue_id)
+ {
+ 	if (queue_id < dev->real_num_rx_queues)
+ 		return dev->_rx[queue_id].pool;
+ 	if (queue_id < dev->real_num_tx_queues)
+ 		return dev->_tx[queue_id].pool;
+ 
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(xsk_get_pool_from_qid);
+ 
+ void xsk_clear_pool_at_qid(struct net_device *dev, u16 queue_id)
+ {
+ 	if (queue_id < dev->num_rx_queues)
+ 		dev->_rx[queue_id].pool = NULL;
+ 	if (queue_id < dev->num_tx_queues)
+ 		dev->_tx[queue_id].pool = NULL;
+ }
+ 
+ /* The buffer pool is stored both in the _rx struct and the _tx struct as we do
+  * not know if the device has more tx queues than rx, or the opposite.
+  * This might also change during run time.
+  */
+ int xsk_reg_pool_at_qid(struct net_device *dev, struct xsk_buff_pool *pool,
+ 			u16 queue_id)
+ {
+ 	if (queue_id >= max_t(unsigned int,
+ 			      dev->real_num_rx_queues,
+ 			      dev->real_num_tx_queues))
+ 		return -EINVAL;
+ 
+ 	if (queue_id < dev->real_num_rx_queues)
+ 		dev->_rx[queue_id].pool = pool;
+ 	if (queue_id < dev->real_num_tx_queues)
+ 		dev->_tx[queue_id].pool = pool;
+ 
+ 	return 0;
++>>>>>>> b425e24a934e (xsk: Clear pool even for inactive queues)
  }
 +EXPORT_SYMBOL(xsk_umem_uses_need_wakeup);
  
  void xp_release(struct xdp_buff_xsk *xskb)
  {
* Unmerged path net/xdp/xsk.c
