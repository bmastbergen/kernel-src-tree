net: ena: introduce ndo_xdp_xmit() function for XDP_REDIRECT

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Shay Agroskin <shayagr@amazon.com>
commit f1a25589130366a96a2a0d165e9f4d9289336e9f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/f1a25589.failed

This patch implements the ndo_xdp_xmit() net_device function which is
called when a packet is redirected to this driver using an
XDP_REDIRECT directive.

The function receives an array of xdp frames that it needs to xmit.
The TX queues that are used to xmit these frames are the XDP
queues used by the XDP_TX flow. Therefore a lock is added to synchronize
both flows (XDP_TX and XDP_REDIRECT).

	Signed-off-by: Shay Agroskin <shayagr@amazon.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit f1a25589130366a96a2a0d165e9f4d9289336e9f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amazon/ena/ena_netdev.c
diff --cc drivers/net/ethernet/amazon/ena/ena_netdev.c
index a34982bbddb7,06596fa1f9fe..000000000000
--- a/drivers/net/ethernet/amazon/ena/ena_netdev.c
+++ b/drivers/net/ethernet/amazon/ena/ena_netdev.c
@@@ -276,21 -281,18 +276,25 @@@ error_report_dma_error
  	return -EINVAL;
  }
  
++<<<<<<< HEAD
 +static int ena_xdp_xmit_buff(struct net_device *dev,
 +			     struct xdp_buff *xdp,
 +			     int qid,
 +			     struct ena_rx_buffer *rx_info)
++=======
+ static int ena_xdp_xmit_frame(struct ena_ring *xdp_ring,
+ 			      struct net_device *dev,
+ 			      struct xdp_frame *xdpf,
+ 			      int flags)
++>>>>>>> f1a255891303 (net: ena: introduce ndo_xdp_xmit() function for XDP_REDIRECT)
  {
- 	struct ena_adapter *adapter = netdev_priv(dev);
  	struct ena_com_tx_ctx ena_tx_ctx = {};
  	struct ena_tx_buffer *tx_info;
- 	struct ena_ring *xdp_ring;
  	u16 next_to_use, req_id;
- 	int rc;
  	void *push_hdr;
  	u32 push_len;
+ 	int rc;
  
- 	xdp_ring = &adapter->tx_ring[qid];
  	next_to_use = xdp_ring->next_to_use;
  	req_id = xdp_ring->free_ids[next_to_use];
  	tx_info = &xdp_ring->tx_buffer_info[req_id];
@@@ -319,28 -319,76 +323,89 @@@
  	/* trigger the dma engine. ena_com_write_sq_doorbell()
  	 * has a mb
  	 */
++<<<<<<< HEAD
 +	ena_com_write_sq_doorbell(xdp_ring->ena_com_io_sq);
 +	u64_stats_update_begin(&xdp_ring->syncp);
 +	xdp_ring->tx_stats.doorbells++;
 +	u64_stats_update_end(&xdp_ring->syncp);
++=======
+ 	if (flags & XDP_XMIT_FLUSH) {
+ 		ena_com_write_sq_doorbell(xdp_ring->ena_com_io_sq);
+ 		ena_increase_stat(&xdp_ring->tx_stats.doorbells, 1,
+ 				  &xdp_ring->syncp);
+ 	}
++>>>>>>> f1a255891303 (net: ena: introduce ndo_xdp_xmit() function for XDP_REDIRECT)
  
- 	return NETDEV_TX_OK;
+ 	return rc;
  
  error_unmap_dma:
  	ena_unmap_tx_buff(xdp_ring, tx_info);
  	tx_info->xdpf = NULL;
  error_drop_packet:
++<<<<<<< HEAD
 +	__free_page(tx_info->xdp_rx_page);
 +	return NETDEV_TX_OK;
++=======
+ 	xdp_return_frame(xdpf);
+ 	return rc;
+ }
+ 
+ static int ena_xdp_xmit(struct net_device *dev, int n,
+ 			struct xdp_frame **frames, u32 flags)
+ {
+ 	struct ena_adapter *adapter = netdev_priv(dev);
+ 	int qid, i, err, drops = 0;
+ 	struct ena_ring *xdp_ring;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	if (!test_bit(ENA_FLAG_DEV_UP, &adapter->flags))
+ 		return -ENETDOWN;
+ 
+ 	/* We assume that all rings have the same XDP program */
+ 	if (!READ_ONCE(adapter->rx_ring->xdp_bpf_prog))
+ 		return -ENXIO;
+ 
+ 	qid = smp_processor_id() % adapter->xdp_num_queues;
+ 	qid += adapter->xdp_first_ring;
+ 	xdp_ring = &adapter->tx_ring[qid];
+ 
+ 	/* Other CPU ids might try to send thorugh this queue */
+ 	spin_lock(&xdp_ring->xdp_tx_lock);
+ 
+ 	for (i = 0; i < n; i++) {
+ 		err = ena_xdp_xmit_frame(xdp_ring, dev, frames[i], 0);
+ 		/* The descriptor is freed by ena_xdp_xmit_frame in case
+ 		 * of an error.
+ 		 */
+ 		if (err)
+ 			drops++;
+ 	}
+ 
+ 	/* Ring doorbell to make device aware of the packets */
+ 	if (flags & XDP_XMIT_FLUSH) {
+ 		ena_com_write_sq_doorbell(xdp_ring->ena_com_io_sq);
+ 		ena_increase_stat(&xdp_ring->tx_stats.doorbells, 1,
+ 				  &xdp_ring->syncp);
+ 	}
+ 
+ 	spin_unlock(&xdp_ring->xdp_tx_lock);
+ 
+ 	/* Return number of packets sent */
+ 	return n - drops;
++>>>>>>> f1a255891303 (net: ena: introduce ndo_xdp_xmit() function for XDP_REDIRECT)
  }
  
 -static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 +static int ena_xdp_execute(struct ena_ring *rx_ring,
 +			   struct xdp_buff *xdp,
 +			   struct ena_rx_buffer *rx_info)
  {
  	struct bpf_prog *xdp_prog;
+ 	struct ena_ring *xdp_ring;
  	u32 verdict = XDP_PASS;
 -	struct xdp_frame *xdpf;
  	u64 *xdp_stat;
+ 	int qid;
  
  	rcu_read_lock();
  	xdp_prog = READ_ONCE(rx_ring->xdp_bpf_prog);
@@@ -350,21 -398,44 +415,34 @@@
  
  	verdict = bpf_prog_run_xdp(xdp_prog, xdp);
  
 -	switch (verdict) {
 -	case XDP_TX:
 -		xdpf = xdp_convert_buff_to_frame(xdp);
 -		if (unlikely(!xdpf)) {
 -			trace_xdp_exception(rx_ring->netdev, xdp_prog, verdict);
 -			xdp_stat = &rx_ring->rx_stats.xdp_aborted;
 -			break;
 -		}
 +	if (verdict == XDP_TX) {
 +		ena_xdp_xmit_buff(rx_ring->netdev,
 +				  xdp,
 +				  rx_ring->qid + rx_ring->adapter->num_io_queues,
 +				  rx_info);
  
++<<<<<<< HEAD
++=======
+ 		/* Find xmit queue */
+ 		qid = rx_ring->qid + rx_ring->adapter->num_io_queues;
+ 		xdp_ring = &rx_ring->adapter->tx_ring[qid];
+ 
+ 		/* The XDP queues are shared between XDP_TX and XDP_REDIRECT */
+ 		spin_lock(&xdp_ring->xdp_tx_lock);
+ 
+ 		ena_xdp_xmit_frame(xdp_ring, rx_ring->netdev, xdpf, XDP_XMIT_FLUSH);
+ 
+ 		spin_unlock(&xdp_ring->xdp_tx_lock);
++>>>>>>> f1a255891303 (net: ena: introduce ndo_xdp_xmit() function for XDP_REDIRECT)
  		xdp_stat = &rx_ring->rx_stats.xdp_tx;
 -		break;
 -	case XDP_REDIRECT:
 -		if (likely(!xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog))) {
 -			xdp_stat = &rx_ring->rx_stats.xdp_redirect;
 -			break;
 -		}
 -		fallthrough;
 -	case XDP_ABORTED:
 +	} else if (unlikely(verdict == XDP_ABORTED)) {
  		trace_xdp_exception(rx_ring->netdev, xdp_prog, verdict);
  		xdp_stat = &rx_ring->rx_stats.xdp_aborted;
 -		break;
 -	case XDP_DROP:
 +	} else if (unlikely(verdict == XDP_DROP)) {
  		xdp_stat = &rx_ring->rx_stats.xdp_drop;
 -		break;
 -	case XDP_PASS:
 +	} else if (unlikely(verdict == XDP_PASS)) {
  		xdp_stat = &rx_ring->rx_stats.xdp_pass;
 -		break;
 -	default:
 +	} else {
  		bpf_warn_invalid_xdp_action(verdict);
  		xdp_stat = &rx_ring->rx_stats.xdp_invalid;
  	}
* Unmerged path drivers/net/ethernet/amazon/ena/ena_netdev.c
diff --git a/drivers/net/ethernet/amazon/ena/ena_netdev.h b/drivers/net/ethernet/amazon/ena/ena_netdev.h
index f43877889011..125aee473057 100644
--- a/drivers/net/ethernet/amazon/ena/ena_netdev.h
+++ b/drivers/net/ethernet/amazon/ena/ena_netdev.h
@@ -269,6 +269,7 @@ struct ena_ring {
 	struct ena_com_io_sq *ena_com_io_sq;
 	struct bpf_prog *xdp_bpf_prog;
 	struct xdp_rxq_info xdp_rxq;
+	spinlock_t xdp_tx_lock;	/* synchronize XDP TX/Redirect traffic */
 
 	u16 next_to_use;
 	u16 next_to_clean;
