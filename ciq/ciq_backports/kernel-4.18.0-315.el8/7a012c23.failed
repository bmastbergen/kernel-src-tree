scsi: smartpqi: Add support for RAID1 writes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-315.el8
commit-author Don Brace <don.brace@microchip.com>
commit 7a012c23c7a7d9cdc7b6db0e8837f8a413dbe436
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-315.el8/7a012c23.failed

Add RAID1 write IU and implement RAID1 write support. Change brand names
ADM/ADG to TRIPLE/RAID-6.

Link: https://lore.kernel.org/r/161549373324.25025.2441592111049564780.stgit@brunhilda
	Reviewed-by: Scott Benesh <scott.benesh@microchip.com>
	Reviewed-by: Scott Teel <scott.teel@microchip.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@microchip.com>
	Signed-off-by: Don Brace <don.brace@microchip.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 7a012c23c7a7d9cdc7b6db0e8837f8a413dbe436)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/smartpqi/smartpqi.h
#	drivers/scsi/smartpqi/smartpqi_init.c
diff --cc drivers/scsi/smartpqi/smartpqi.h
index 7d3f956e949f,bed80c4c4598..000000000000
--- a/drivers/scsi/smartpqi/smartpqi.h
+++ b/drivers/scsi/smartpqi/smartpqi.h
@@@ -312,6 -313,67 +312,70 @@@ struct pqi_aio_path_request 
  		sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
  };
  
++<<<<<<< HEAD
++=======
+ #define PQI_RAID1_NVME_XFER_LIMIT	(32 * 1024)	/* 32 KiB */
+ struct pqi_aio_r1_path_request {
+ 	struct pqi_iu_header header;
+ 	__le16	request_id;
+ 	__le16	volume_id;	/* ID of the RAID volume */
+ 	__le32	it_nexus_1;	/* IT nexus of the 1st drive in the RAID volume */
+ 	__le32	it_nexus_2;	/* IT nexus of the 2nd drive in the RAID volume */
+ 	__le32	it_nexus_3;	/* IT nexus of the 3rd drive in the RAID volume */
+ 	__le32	data_length;	/* total bytes to read/write */
+ 	u8	data_direction : 2;
+ 	u8	partial : 1;
+ 	u8	memory_type : 1;
+ 	u8	fence : 1;
+ 	u8	encryption_enable : 1;
+ 	u8	reserved : 2;
+ 	u8	task_attribute : 3;
+ 	u8	command_priority : 4;
+ 	u8	reserved2 : 1;
+ 	__le16	data_encryption_key_index;
+ 	u8	cdb[16];
+ 	__le16	error_index;
+ 	u8	num_sg_descriptors;
+ 	u8	cdb_length;
+ 	u8	num_drives;	/* number of drives in the RAID volume (2 or 3) */
+ 	u8	reserved3[3];
+ 	__le32	encrypt_tweak_lower;
+ 	__le32	encrypt_tweak_upper;
+ 	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_SG_DESCRIPTORS];
+ };
+ 
+ struct pqi_aio_r56_path_request {
+ 	struct pqi_iu_header header;
+ 	__le16	request_id;
+ 	__le16	volume_id;		/* ID of the RAID volume */
+ 	__le32	data_it_nexus;		/* IT nexus for the data drive */
+ 	__le32	p_parity_it_nexus;	/* IT nexus for the P parity drive */
+ 	__le32	q_parity_it_nexus;	/* IT nexus for the Q parity drive */
+ 	__le32	data_length;		/* total bytes to read/write */
+ 	u8	data_direction : 2;
+ 	u8	partial : 1;
+ 	u8	mem_type : 1;		/* 0 = PCIe, 1 = DDR */
+ 	u8	fence : 1;
+ 	u8	encryption_enable : 1;
+ 	u8	reserved : 2;
+ 	u8	task_attribute : 3;
+ 	u8	command_priority : 4;
+ 	u8	reserved1 : 1;
+ 	__le16	data_encryption_key_index;
+ 	u8	cdb[16];
+ 	__le16	error_index;
+ 	u8	num_sg_descriptors;
+ 	u8	cdb_length;
+ 	u8	xor_multiplier;
+ 	u8	reserved2[3];
+ 	__le32	encrypt_tweak_lower;
+ 	__le32	encrypt_tweak_upper;
+ 	__le64	row;			/* row = logical LBA/blocks per row */
+ 	u8	reserved3[8];
+ 	struct pqi_sg_descriptor sg_descriptors[PQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS];
+ };
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  struct pqi_io_response {
  	struct pqi_iu_header header;
  	__le16	request_id;
@@@ -484,6 -546,9 +548,12 @@@ struct pqi_raid_error_info 
  #define PQI_REQUEST_IU_TASK_MANAGEMENT			0x13
  #define PQI_REQUEST_IU_RAID_PATH_IO			0x14
  #define PQI_REQUEST_IU_AIO_PATH_IO			0x15
++<<<<<<< HEAD
++=======
+ #define PQI_REQUEST_IU_AIO_PATH_RAID5_IO		0x18
+ #define PQI_REQUEST_IU_AIO_PATH_RAID6_IO		0x19
+ #define PQI_REQUEST_IU_AIO_PATH_RAID1_IO		0x1A
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  #define PQI_REQUEST_IU_GENERAL_ADMIN			0x60
  #define PQI_REQUEST_IU_REPORT_VENDOR_EVENT_CONFIG	0x72
  #define PQI_REQUEST_IU_SET_VENDOR_EVENT_CONFIG		0x73
@@@ -908,6 -973,56 +978,59 @@@ struct raid_map 
  
  #pragma pack()
  
++<<<<<<< HEAD
++=======
+ struct pqi_scsi_dev_raid_map_data {
+ 	bool	is_write;
+ 	u8	raid_level;
+ 	u32	map_index;
+ 	u64	first_block;
+ 	u64	last_block;
+ 	u32	data_length;
+ 	u32	block_cnt;
+ 	u32	blocks_per_row;
+ 	u64	first_row;
+ 	u64	last_row;
+ 	u32	first_row_offset;
+ 	u32	last_row_offset;
+ 	u32	first_column;
+ 	u32	last_column;
+ 	u64	r5or6_first_row;
+ 	u64	r5or6_last_row;
+ 	u32	r5or6_first_row_offset;
+ 	u32	r5or6_last_row_offset;
+ 	u32	r5or6_first_column;
+ 	u32	r5or6_last_column;
+ 	u16	data_disks_per_row;
+ 	u32	total_disks_per_row;
+ 	u16	layout_map_count;
+ 	u32	stripesize;
+ 	u16	strip_size;
+ 	u32	first_group;
+ 	u32	last_group;
+ 	u32	map_row;
+ 	u32	aio_handle;
+ 	u64	disk_block;
+ 	u32	disk_block_cnt;
+ 	u8	cdb[16];
+ 	u8	cdb_length;
+ 
+ 	/* RAID1 specific */
+ #define NUM_RAID1_MAP_ENTRIES	3
+ 	u32	num_it_nexus_entries;
+ 	u32	it_nexus[NUM_RAID1_MAP_ENTRIES];
+ 
+ 	/* RAID5 RAID6 specific */
+ 	u32	p_parity_it_nexus;	/* aio_handle */
+ 	u32	q_parity_it_nexus;	/* aio_handle */
+ 	u8	xor_mult;
+ 	u64	row;
+ 	u64	stripe_lba;
+ 	u32	p_index;
+ 	u32	q_index;
+ };
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  #define RAID_CTLR_LUNID		"\0\0\0\0\0\0\0\0"
  
  struct pqi_scsi_dev {
@@@ -1158,6 -1273,9 +1280,12 @@@ struct pqi_ctrl_info 
  	u8		soft_reset_handshake_supported : 1;
  	u8		raid_iu_timeout_supported: 1;
  	u8		tmf_iu_timeout_supported: 1;
++<<<<<<< HEAD
++=======
+ 	u8		enable_r1_writes : 1;
+ 	u8		enable_r5_writes : 1;
+ 	u8		enable_r6_writes : 1;
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  
  	struct list_head scsi_device_list;
  	spinlock_t	scsi_device_list_lock;
diff --cc drivers/scsi/smartpqi/smartpqi_init.c
index 15d4619f5e0b,a36861184fc0..000000000000
--- a/drivers/scsi/smartpqi/smartpqi_init.c
+++ b/drivers/scsi/smartpqi/smartpqi_init.c
@@@ -67,6 -67,14 +67,17 @@@ static int pqi_aio_submit_io(struct pqi
  	struct scsi_cmnd *scmd, u32 aio_handle, u8 *cdb,
  	unsigned int cdb_length, struct pqi_queue_group *queue_group,
  	struct pqi_encryption_info *encryption_info, bool raid_bypass);
++<<<<<<< HEAD
++=======
+ static  int pqi_aio_submit_r1_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd);
+ static int pqi_aio_submit_r56_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd);
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  static void pqi_ofa_ctrl_quiesce(struct pqi_ctrl_info *ctrl_info);
  static void pqi_ofa_ctrl_unquiesce(struct pqi_ctrl_info *ctrl_info);
  static int pqi_ofa_ctrl_restart(struct pqi_ctrl_info *ctrl_info);
@@@ -2268,292 -2245,372 +2279,625 @@@ static inline void pqi_set_encryption_i
   * Attempt to perform RAID bypass mapping for a logical volume I/O.
   */
  
++<<<<<<< HEAD
++#define PQI_RAID_BYPASS_INELIGIBLE	1
++
++=======
+ static bool pqi_aio_raid_level_supported(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	bool is_supported = true;
+ 
+ 	switch (rmd->raid_level) {
+ 	case SA_RAID_0:
+ 		break;
+ 	case SA_RAID_1:
+ 	case SA_RAID_TRIPLE:
+ 		if (rmd->is_write && !ctrl_info->enable_r1_writes)
+ 			is_supported = false;
+ 		break;
+ 	case SA_RAID_5:
+ 		if (rmd->is_write && !ctrl_info->enable_r5_writes)
+ 			is_supported = false;
+ 		break;
+ 	case SA_RAID_6:
+ 		if (rmd->is_write && !ctrl_info->enable_r6_writes)
+ 			is_supported = false;
+ 		break;
+ 	default:
+ 		is_supported = false;
+ 	}
+ 
+ 	return is_supported;
+ }
+ 
  #define PQI_RAID_BYPASS_INELIGIBLE	1
  
+ static int pqi_get_aio_lba_and_block_count(struct scsi_cmnd *scmd,
+ 			struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	/* Check for valid opcode, get LBA and block count. */
+ 	switch (scmd->cmnd[0]) {
+ 	case WRITE_6:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_6:
+ 		rmd->first_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |
+ 			(scmd->cmnd[2] << 8) | scmd->cmnd[3]);
+ 		rmd->block_cnt = (u32)scmd->cmnd[4];
+ 		if (rmd->block_cnt == 0)
+ 			rmd->block_cnt = 256;
+ 		break;
+ 	case WRITE_10:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_10:
+ 		rmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
+ 		rmd->block_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);
+ 		break;
+ 	case WRITE_12:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_12:
+ 		rmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
+ 		rmd->block_cnt = get_unaligned_be32(&scmd->cmnd[6]);
+ 		break;
+ 	case WRITE_16:
+ 		rmd->is_write = true;
+ 		fallthrough;
+ 	case READ_16:
+ 		rmd->first_block = get_unaligned_be64(&scmd->cmnd[2]);
+ 		rmd->block_cnt = get_unaligned_be32(&scmd->cmnd[10]);
+ 		break;
+ 	default:
+ 		/* Process via normal I/O path. */
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 	}
+ 
+ 	put_unaligned_le32(scsi_bufflen(scmd), &rmd->data_length);
+ 
+ 	return 0;
+ }
+ 
+ static int pci_get_aio_common_raid_map_values(struct pqi_ctrl_info *ctrl_info,
+ 					struct pqi_scsi_dev_raid_map_data *rmd,
+ 					struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 
+ 	rmd->last_block = rmd->first_block + rmd->block_cnt - 1;
+ 
+ 	/* Check for invalid block or wraparound. */
+ 	if (rmd->last_block >=
+ 		get_unaligned_le64(&raid_map->volume_blk_cnt) ||
+ 		rmd->last_block < rmd->first_block)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	rmd->data_disks_per_row =
+ 			get_unaligned_le16(&raid_map->data_disks_per_row);
+ 	rmd->strip_size = get_unaligned_le16(&raid_map->strip_size);
+ 	rmd->layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
+ 
+ 	/* Calculate stripe information for the request. */
+ 	rmd->blocks_per_row = rmd->data_disks_per_row * rmd->strip_size;
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->first_row = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->last_row = tmpdiv;
+ 	rmd->first_row_offset = (u32)(rmd->first_block - (rmd->first_row * rmd->blocks_per_row));
+ 	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row * rmd->blocks_per_row));
+ 	tmpdiv = rmd->first_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->first_column = tmpdiv;
+ 	tmpdiv = rmd->last_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->last_column = tmpdiv;
+ #else
+ 	rmd->first_row = rmd->first_block / rmd->blocks_per_row;
+ 	rmd->last_row = rmd->last_block / rmd->blocks_per_row;
+ 	rmd->first_row_offset = (u32)(rmd->first_block -
+ 				(rmd->first_row * rmd->blocks_per_row));
+ 	rmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row *
+ 				rmd->blocks_per_row));
+ 	rmd->first_column = rmd->first_row_offset / rmd->strip_size;
+ 	rmd->last_column = rmd->last_row_offset / rmd->strip_size;
+ #endif
+ 
+ 	/* If this isn't a single row/column then give to the controller. */
+ 	if (rmd->first_row != rmd->last_row ||
+ 			rmd->first_column != rmd->last_column)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Proceeding with driver mapping. */
+ 	rmd->total_disks_per_row = rmd->data_disks_per_row +
+ 		get_unaligned_le16(&raid_map->metadata_disks_per_row);
+ 	rmd->map_row = ((u32)(rmd->first_row >>
+ 		raid_map->parity_rotation_shift)) %
+ 		get_unaligned_le16(&raid_map->row_cnt);
+ 	rmd->map_index = (rmd->map_row * rmd->total_disks_per_row) +
+ 			rmd->first_column;
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_calc_aio_r5_or_r6(struct pqi_scsi_dev_raid_map_data *rmd,
+ 				struct raid_map *raid_map)
+ {
+ #if BITS_PER_LONG == 32
+ 	u64 tmpdiv;
+ #endif
+ 	/* RAID 50/60 */
+ 	/* Verify first and last block are in same RAID group */
+ 	rmd->stripesize = rmd->blocks_per_row * rmd->layout_map_count;
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->first_group = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->last_group = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->last_group;
+ 	do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->last_group = tmpdiv;
+ #else
+ 	rmd->first_group = (rmd->first_block % rmd->stripesize) / rmd->blocks_per_row;
+ 	rmd->last_group = (rmd->last_block % rmd->stripesize) / rmd->blocks_per_row;
+ #endif
+ 	if (rmd->first_group != rmd->last_group)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single row of RAID 5/6 */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->first_row = tmpdiv;
+ 	rmd->r5or6_first_row = tmpdiv;
+ 	tmpdiv = rmd->last_block;
+ 	do_div(tmpdiv, rmd->stripesize);
+ 	rmd->r5or6_last_row = tmpdiv;
+ #else
+ 	rmd->first_row = rmd->r5or6_first_row =
+ 		rmd->first_block / rmd->stripesize;
+ 	rmd->r5or6_last_row = rmd->last_block / rmd->stripesize;
+ #endif
+ 	if (rmd->r5or6_first_row != rmd->r5or6_last_row)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Verify request is in a single column */
+ #if BITS_PER_LONG == 32
+ 	tmpdiv = rmd->first_block;
+ 	rmd->first_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->first_row_offset;
+ 	rmd->first_row_offset = (u32)do_div(tmpdiv, rmd->blocks_per_row);
+ 	rmd->r5or6_first_row_offset = rmd->first_row_offset;
+ 	tmpdiv = rmd->last_block;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->stripesize);
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	rmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->blocks_per_row);
+ 	tmpdiv = rmd->r5or6_first_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->first_column = rmd->r5or6_first_column = tmpdiv;
+ 	tmpdiv = rmd->r5or6_last_row_offset;
+ 	do_div(tmpdiv, rmd->strip_size);
+ 	rmd->r5or6_last_column = tmpdiv;
+ #else
+ 	rmd->first_row_offset = rmd->r5or6_first_row_offset =
+ 		(u32)((rmd->first_block %
+ 				rmd->stripesize) %
+ 				rmd->blocks_per_row);
+ 
+ 	rmd->r5or6_last_row_offset =
+ 		(u32)((rmd->last_block % rmd->stripesize) %
+ 		rmd->blocks_per_row);
+ 
+ 	rmd->first_column =
+ 			rmd->r5or6_first_row_offset / rmd->strip_size;
+ 	rmd->r5or6_first_column = rmd->first_column;
+ 	rmd->r5or6_last_column = rmd->r5or6_last_row_offset / rmd->strip_size;
+ #endif
+ 	if (rmd->r5or6_first_column != rmd->r5or6_last_column)
+ 		return PQI_RAID_BYPASS_INELIGIBLE;
+ 
+ 	/* Request is eligible */
+ 	rmd->map_row =
+ 		((u32)(rmd->first_row >> raid_map->parity_rotation_shift)) %
+ 		get_unaligned_le16(&raid_map->row_cnt);
+ 
+ 	rmd->map_index = (rmd->first_group *
+ 		(get_unaligned_le16(&raid_map->row_cnt) *
+ 		rmd->total_disks_per_row)) +
+ 		(rmd->map_row * rmd->total_disks_per_row) + rmd->first_column;
+ 
+ 	if (rmd->is_write) {
+ 		u32 index;
+ 
+ 		/*
+ 		 * p_parity_it_nexus and q_parity_it_nexus are pointers to the
+ 		 * parity entries inside the device's raid_map.
+ 		 *
+ 		 * A device's RAID map is bounded by: number of RAID disks squared.
+ 		 *
+ 		 * The devices RAID map size is checked during device
+ 		 * initialization.
+ 		 */
+ 		index = DIV_ROUND_UP(rmd->map_index + 1, rmd->total_disks_per_row);
+ 		index *= rmd->total_disks_per_row;
+ 		index -= get_unaligned_le16(&raid_map->metadata_disks_per_row);
+ 
+ 		rmd->p_parity_it_nexus = raid_map->disk_data[index].aio_handle;
+ 		if (rmd->raid_level == SA_RAID_6) {
+ 			rmd->q_parity_it_nexus = raid_map->disk_data[index + 1].aio_handle;
+ 			rmd->xor_mult = raid_map->disk_data[rmd->map_index].xor_mult[1];
+ 		}
+ 		if (rmd->blocks_per_row == 0)
+ 			return PQI_RAID_BYPASS_INELIGIBLE;
+ #if BITS_PER_LONG == 32
+ 		tmpdiv = rmd->first_block;
+ 		do_div(tmpdiv, rmd->blocks_per_row);
+ 		rmd->row = tmpdiv;
+ #else
+ 		rmd->row = rmd->first_block / rmd->blocks_per_row;
+ #endif
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void pqi_set_aio_cdb(struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	/* Build the new CDB for the physical disk I/O. */
+ 	if (rmd->disk_block > 0xffffffff) {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_16 : READ_16;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be64(rmd->disk_block, &rmd->cdb[2]);
+ 		put_unaligned_be32(rmd->disk_block_cnt, &rmd->cdb[10]);
+ 		rmd->cdb[14] = 0;
+ 		rmd->cdb[15] = 0;
+ 		rmd->cdb_length = 16;
+ 	} else {
+ 		rmd->cdb[0] = rmd->is_write ? WRITE_10 : READ_10;
+ 		rmd->cdb[1] = 0;
+ 		put_unaligned_be32((u32)rmd->disk_block, &rmd->cdb[2]);
+ 		rmd->cdb[6] = 0;
+ 		put_unaligned_be16((u16)rmd->disk_block_cnt, &rmd->cdb[7]);
+ 		rmd->cdb[9] = 0;
+ 		rmd->cdb_length = 10;
+ 	}
+ }
+ 
+ static void pqi_calc_aio_r1_nexus(struct raid_map *raid_map,
+ 				struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	u32 index;
+ 	u32 group;
+ 
+ 	group = rmd->map_index / rmd->data_disks_per_row;
+ 
+ 	index = rmd->map_index - (group * rmd->data_disks_per_row);
+ 	rmd->it_nexus[0] = raid_map->disk_data[index].aio_handle;
+ 	index += rmd->data_disks_per_row;
+ 	rmd->it_nexus[1] = raid_map->disk_data[index].aio_handle;
+ 	if (rmd->layout_map_count > 2) {
+ 		index += rmd->data_disks_per_row;
+ 		rmd->it_nexus[2] = raid_map->disk_data[index].aio_handle;
+ 	}
+ 
+ 	rmd->num_it_nexus_entries = rmd->layout_map_count;
+ }
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  static int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,
  	struct pqi_scsi_dev *device, struct scsi_cmnd *scmd,
  	struct pqi_queue_group *queue_group)
  {
++<<<<<<< HEAD
 +	struct raid_map *raid_map;
 +	bool is_write = false;
 +	u32 map_index;
 +	u64 first_block;
 +	u64 last_block;
 +	u32 block_cnt;
 +	u32 blocks_per_row;
 +	u64 first_row;
 +	u64 last_row;
 +	u32 first_row_offset;
 +	u32 last_row_offset;
 +	u32 first_column;
 +	u32 last_column;
 +	u64 r0_first_row;
 +	u64 r0_last_row;
 +	u32 r5or6_blocks_per_row;
 +	u64 r5or6_first_row;
 +	u64 r5or6_last_row;
 +	u32 r5or6_first_row_offset;
 +	u32 r5or6_last_row_offset;
 +	u32 r5or6_first_column;
 +	u32 r5or6_last_column;
 +	u16 data_disks_per_row;
 +	u32 total_disks_per_row;
 +	u16 layout_map_count;
 +	u32 stripesize;
 +	u16 strip_size;
 +	u32 first_group;
 +	u32 last_group;
 +	u32 current_group;
 +	u32 map_row;
 +	u32 aio_handle;
 +	u64 disk_block;
 +	u32 disk_block_cnt;
 +	u8 cdb[16];
 +	u8 cdb_length;
 +	int offload_to_mirror;
++=======
+ 	int rc;
+ 	struct raid_map *raid_map;
+ 	u32 group;
+ 	u32 next_bypass_group;
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  	struct pqi_encryption_info *encryption_info_ptr;
  	struct pqi_encryption_info encryption_info;
 -	struct pqi_scsi_dev_raid_map_data rmd = {0};
 +#if BITS_PER_LONG == 32
 +	u64 tmpdiv;
 +#endif
  
 -	rc = pqi_get_aio_lba_and_block_count(scmd, &rmd);
 -	if (rc)
 +	/* Check for valid opcode, get LBA and block count. */
 +	switch (scmd->cmnd[0]) {
 +	case WRITE_6:
 +		is_write = true;
 +		/* fall through */
 +	case READ_6:
 +		first_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |
 +			(scmd->cmnd[2] << 8) | scmd->cmnd[3]);
 +		block_cnt = (u32)scmd->cmnd[4];
 +		if (block_cnt == 0)
 +			block_cnt = 256;
 +		break;
 +	case WRITE_10:
 +		is_write = true;
 +		/* fall through */
 +	case READ_10:
 +		first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
 +		block_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);
 +		break;
 +	case WRITE_12:
 +		is_write = true;
 +		/* fall through */
 +	case READ_12:
 +		first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);
 +		block_cnt = get_unaligned_be32(&scmd->cmnd[6]);
 +		break;
 +	case WRITE_16:
 +		is_write = true;
 +		/* fall through */
 +	case READ_16:
 +		first_block = get_unaligned_be64(&scmd->cmnd[2]);
 +		block_cnt = get_unaligned_be32(&scmd->cmnd[10]);
 +		break;
 +	default:
 +		/* Process via normal I/O path. */
  		return PQI_RAID_BYPASS_INELIGIBLE;
 +	}
  
 -	rmd.raid_level = device->raid_level;
 -
 -	if (!pqi_aio_raid_level_supported(ctrl_info, &rmd))
 +	/* Check for write to non-RAID-0. */
 +	if (is_write && device->raid_level != SA_RAID_0)
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 -	if (unlikely(rmd.block_cnt == 0))
 +	if (unlikely(block_cnt == 0))
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 +	last_block = first_block + block_cnt - 1;
  	raid_map = device->raid_map;
  
 -	rc = pci_get_aio_common_raid_map_values(ctrl_info, &rmd, raid_map);
 -	if (rc)
 +	/* Check for invalid block or wraparound. */
 +	if (last_block >= get_unaligned_le64(&raid_map->volume_blk_cnt) ||
 +		last_block < first_block)
 +		return PQI_RAID_BYPASS_INELIGIBLE;
 +
++<<<<<<< HEAD
 +	data_disks_per_row = get_unaligned_le16(&raid_map->data_disks_per_row);
 +	strip_size = get_unaligned_le16(&raid_map->strip_size);
 +	layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);
 +
 +	/* Calculate stripe information for the request. */
 +	blocks_per_row = data_disks_per_row * strip_size;
 +#if BITS_PER_LONG == 32
 +	tmpdiv = first_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	first_row = tmpdiv;
 +	tmpdiv = last_block;
 +	do_div(tmpdiv, blocks_per_row);
 +	last_row = tmpdiv;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	tmpdiv = first_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	first_column = tmpdiv;
 +	tmpdiv = last_row_offset;
 +	do_div(tmpdiv, strip_size);
 +	last_column = tmpdiv;
 +#else
 +	first_row = first_block / blocks_per_row;
 +	last_row = last_block / blocks_per_row;
 +	first_row_offset = (u32)(first_block - (first_row * blocks_per_row));
 +	last_row_offset = (u32)(last_block - (last_row * blocks_per_row));
 +	first_column = first_row_offset / strip_size;
 +	last_column = last_row_offset / strip_size;
 +#endif
 +
 +	/* If this isn't a single row/column then give to the controller. */
 +	if (first_row != last_row || first_column != last_column)
  		return PQI_RAID_BYPASS_INELIGIBLE;
  
 +	/* Proceeding with driver mapping. */
 +	total_disks_per_row = data_disks_per_row +
 +		get_unaligned_le16(&raid_map->metadata_disks_per_row);
 +	map_row = ((u32)(first_row >> raid_map->parity_rotation_shift)) %
 +		get_unaligned_le16(&raid_map->row_cnt);
 +	map_index = (map_row * total_disks_per_row) + first_column;
 +
 +	/* RAID 1 */
 +	if (device->raid_level == SA_RAID_1) {
 +		if (device->offload_to_mirror)
 +			map_index += data_disks_per_row;
 +		device->offload_to_mirror = !device->offload_to_mirror;
 +	} else if (device->raid_level == SA_RAID_ADM) {
 +		/* RAID ADM */
 +		/*
 +		 * Handles N-way mirrors  (R1-ADM) and R10 with # of drives
 +		 * divisible by 3.
 +		 */
 +		offload_to_mirror = device->offload_to_mirror;
 +		if (offload_to_mirror == 0)  {
 +			/* use physical disk in the first mirrored group. */
 +			map_index %= data_disks_per_row;
 +		} else {
 +			do {
 +				/*
 +				 * Determine mirror group that map_index
 +				 * indicates.
 +				 */
 +				current_group = map_index / data_disks_per_row;
 +
 +				if (offload_to_mirror != current_group) {
 +					if (current_group <
 +						layout_map_count - 1) {
 +						/*
 +						 * Select raid index from
 +						 * next group.
 +						 */
 +						map_index += data_disks_per_row;
 +						current_group++;
 +					} else {
 +						/*
 +						 * Select raid index from first
 +						 * group.
 +						 */
 +						map_index %= data_disks_per_row;
 +						current_group = 0;
 +					}
 +				}
 +			} while (offload_to_mirror != current_group);
 +		}
 +
 +		/* Set mirror group to use next time. */
 +		offload_to_mirror =
 +			(offload_to_mirror >= layout_map_count - 1) ?
 +				0 : offload_to_mirror + 1;
 +		device->offload_to_mirror = offload_to_mirror;
 +		/*
 +		 * Avoid direct use of device->offload_to_mirror within this
 +		 * function since multiple threads might simultaneously
 +		 * increment it beyond the range of device->layout_map_count -1.
 +		 */
++=======
+ 	if (device->raid_level == SA_RAID_1 ||
+ 		device->raid_level == SA_RAID_TRIPLE) {
+ 		if (rmd.is_write) {
+ 			pqi_calc_aio_r1_nexus(raid_map, &rmd);
+ 		} else {
+ 			group = device->next_bypass_group;
+ 			next_bypass_group = group + 1;
+ 			if (next_bypass_group >= rmd.layout_map_count)
+ 				next_bypass_group = 0;
+ 			device->next_bypass_group = next_bypass_group;
+ 			rmd.map_index += group * rmd.data_disks_per_row;
+ 		}
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  	} else if ((device->raid_level == SA_RAID_5 ||
 -		device->raid_level == SA_RAID_6) &&
 -		(rmd.layout_map_count > 1 || rmd.is_write)) {
 -		rc = pqi_calc_aio_r5_or_r6(&rmd, raid_map);
 -		if (rc)
 +		device->raid_level == SA_RAID_6) && layout_map_count > 1) {
 +		/* RAID 50/60 */
 +		/* Verify first and last block are in same RAID group */
 +		r5or6_blocks_per_row = strip_size * data_disks_per_row;
 +		stripesize = r5or6_blocks_per_row * layout_map_count;
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		first_group = tmpdiv;
 +		tmpdiv = last_block;
 +		last_group = do_div(tmpdiv, stripesize);
 +		tmpdiv = last_group;
 +		do_div(tmpdiv, r5or6_blocks_per_row);
 +		last_group = tmpdiv;
 +#else
 +		first_group = (first_block % stripesize) / r5or6_blocks_per_row;
 +		last_group = (last_block % stripesize) / r5or6_blocks_per_row;
 +#endif
 +		if (first_group != last_group)
  			return PQI_RAID_BYPASS_INELIGIBLE;
 -	}
  
 -	if (unlikely(rmd.map_index >= RAID_MAP_MAX_ENTRIES))
 -		return PQI_RAID_BYPASS_INELIGIBLE;
 +		/* Verify request is in a single row of RAID 5/6 */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		do_div(tmpdiv, stripesize);
 +		first_row = r5or6_first_row = r0_first_row = tmpdiv;
 +		tmpdiv = last_block;
 +		do_div(tmpdiv, stripesize);
 +		r5or6_last_row = r0_last_row = tmpdiv;
 +#else
 +		first_row = r5or6_first_row = r0_first_row =
 +			first_block / stripesize;
 +		r5or6_last_row = r0_last_row = last_block / stripesize;
 +#endif
 +		if (r5or6_first_row != r5or6_last_row)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
 +
 +		/* Verify request is in a single column */
 +#if BITS_PER_LONG == 32
 +		tmpdiv = first_block;
 +		first_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = first_row_offset;
 +		first_row_offset = (u32)do_div(tmpdiv, r5or6_blocks_per_row);
 +		r5or6_first_row_offset = first_row_offset;
 +		tmpdiv = last_block;
 +		r5or6_last_row_offset = do_div(tmpdiv, stripesize);
 +		tmpdiv = r5or6_last_row_offset;
 +		r5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);
 +		tmpdiv = r5or6_first_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		first_column = r5or6_first_column = tmpdiv;
 +		tmpdiv = r5or6_last_row_offset;
 +		do_div(tmpdiv, strip_size);
 +		r5or6_last_column = tmpdiv;
 +#else
 +		first_row_offset = r5or6_first_row_offset =
 +			(u32)((first_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		r5or6_last_row_offset =
 +			(u32)((last_block % stripesize) %
 +			r5or6_blocks_per_row);
 +
 +		first_column = r5or6_first_row_offset / strip_size;
 +		r5or6_first_column = first_column;
 +		r5or6_last_column = r5or6_last_row_offset / strip_size;
 +#endif
 +		if (r5or6_first_column != r5or6_last_column)
 +			return PQI_RAID_BYPASS_INELIGIBLE;
  
 -	rmd.aio_handle = raid_map->disk_data[rmd.map_index].aio_handle;
 -	rmd.disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 -		rmd.first_row * rmd.strip_size +
 -		(rmd.first_row_offset - rmd.first_column * rmd.strip_size);
 -	rmd.disk_block_cnt = rmd.block_cnt;
 +		/* Request is eligible */
 +		map_row =
 +			((u32)(first_row >> raid_map->parity_rotation_shift)) %
 +			get_unaligned_le16(&raid_map->row_cnt);
 +
 +		map_index = (first_group *
 +			(get_unaligned_le16(&raid_map->row_cnt) *
 +			total_disks_per_row)) +
 +			(map_row * total_disks_per_row) + first_column;
 +	}
 +
 +	aio_handle = raid_map->disk_data[map_index].aio_handle;
 +	disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +
 +		first_row * strip_size +
 +		(first_row_offset - first_column * strip_size);
 +	disk_block_cnt = block_cnt;
  
  	/* Handle differing logical/physical block sizes. */
  	if (raid_map->phys_blk_shift) {
@@@ -2592,8 -2632,31 +2936,36 @@@
  		encryption_info_ptr = NULL;
  	}
  
++<<<<<<< HEAD
 +	return pqi_aio_submit_io(ctrl_info, scmd, aio_handle,
 +		cdb, cdb_length, queue_group, encryption_info_ptr, true);
++=======
+ 	if (rmd.is_write) {
+ 		switch (device->raid_level) {
+ 		case SA_RAID_0:
+ 			return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 				rmd.cdb, rmd.cdb_length, queue_group,
+ 				encryption_info_ptr, true);
+ 		case SA_RAID_1:
+ 		case SA_RAID_TRIPLE:
+ 			return pqi_aio_submit_r1_write_io(ctrl_info, scmd, queue_group,
+ 				encryption_info_ptr, device, &rmd);
+ 		case SA_RAID_5:
+ 		case SA_RAID_6:
+ 			return pqi_aio_submit_r56_write_io(ctrl_info, scmd, queue_group,
+ 					encryption_info_ptr, device, &rmd);
+ 		default:
+ 			return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 				rmd.cdb, rmd.cdb_length, queue_group,
+ 				encryption_info_ptr, true);
+ 		}
+ 	} else {
+ 		return pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,
+ 			rmd.cdb, rmd.cdb_length, queue_group,
+ 			encryption_info_ptr, true);
+ 	}
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  }
  
  #define PQI_STATUS_IDLE		0x0
@@@ -4900,6 -4969,80 +5272,83 @@@ out
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int pqi_build_aio_r1_sg_list(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_aio_r1_path_request *request, struct scsi_cmnd *scmd,
+ 	struct pqi_io_request *io_request)
+ {
+ 	u16 iu_length;
+ 	int sg_count;
+ 	bool chained;
+ 	unsigned int num_sg_in_iu;
+ 	struct scatterlist *sg;
+ 	struct pqi_sg_descriptor *sg_descriptor;
+ 
+ 	sg_count = scsi_dma_map(scmd);
+ 	if (sg_count < 0)
+ 		return sg_count;
+ 
+ 	iu_length = offsetof(struct pqi_aio_r1_path_request, sg_descriptors) -
+ 		PQI_REQUEST_HEADER_LENGTH;
+ 	num_sg_in_iu = 0;
+ 
+ 	if (sg_count == 0)
+ 		goto out;
+ 
+ 	sg = scsi_sglist(scmd);
+ 	sg_descriptor = request->sg_descriptors;
+ 
+ 	num_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,
+ 		ctrl_info->max_sg_per_iu, &chained);
+ 
+ 	request->partial = chained;
+ 	iu_length += num_sg_in_iu * sizeof(*sg_descriptor);
+ 
+ out:
+ 	put_unaligned_le16(iu_length, &request->header.iu_length);
+ 	request->num_sg_descriptors = num_sg_in_iu;
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_build_aio_r56_sg_list(struct pqi_ctrl_info *ctrl_info,
+ 	struct pqi_aio_r56_path_request *request, struct scsi_cmnd *scmd,
+ 	struct pqi_io_request *io_request)
+ {
+ 	u16 iu_length;
+ 	int sg_count;
+ 	bool chained;
+ 	unsigned int num_sg_in_iu;
+ 	struct scatterlist *sg;
+ 	struct pqi_sg_descriptor *sg_descriptor;
+ 
+ 	sg_count = scsi_dma_map(scmd);
+ 	if (sg_count < 0)
+ 		return sg_count;
+ 
+ 	iu_length = offsetof(struct pqi_aio_r56_path_request, sg_descriptors) -
+ 		PQI_REQUEST_HEADER_LENGTH;
+ 	num_sg_in_iu = 0;
+ 
+ 	if (sg_count != 0) {
+ 		sg = scsi_sglist(scmd);
+ 		sg_descriptor = request->sg_descriptors;
+ 
+ 		num_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,
+ 			ctrl_info->max_sg_per_r56_iu, &chained);
+ 
+ 		request->partial = chained;
+ 		iu_length += num_sg_in_iu * sizeof(*sg_descriptor);
+ 	}
+ 
+ 	put_unaligned_le16(iu_length, &request->header.iu_length);
+ 	request->num_sg_descriptors = num_sg_in_iu;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  static int pqi_build_aio_sg_list(struct pqi_ctrl_info *ctrl_info,
  	struct pqi_aio_path_request *request, struct scsi_cmnd *scmd,
  	struct pqi_io_request *io_request)
@@@ -5304,6 -5447,131 +5753,134 @@@ static int pqi_aio_submit_io(struct pqi
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static  int pqi_aio_submit_r1_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ 
+ {
+ 	int rc;
+ 	struct pqi_io_request *io_request;
+ 	struct pqi_aio_r1_path_request *r1_request;
+ 
+ 	io_request = pqi_alloc_io_request(ctrl_info);
+ 	io_request->io_complete_callback = pqi_aio_io_complete;
+ 	io_request->scmd = scmd;
+ 	io_request->raid_bypass = true;
+ 
+ 	r1_request = io_request->iu;
+ 	memset(r1_request, 0, offsetof(struct pqi_aio_r1_path_request, sg_descriptors));
+ 
+ 	r1_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID1_IO;
+ 
+ 	put_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r1_request->volume_id);
+ 	r1_request->num_drives = rmd->num_it_nexus_entries;
+ 	put_unaligned_le32(rmd->it_nexus[0], &r1_request->it_nexus_1);
+ 	put_unaligned_le32(rmd->it_nexus[1], &r1_request->it_nexus_2);
+ 	if (rmd->num_it_nexus_entries == 3)
+ 		put_unaligned_le32(rmd->it_nexus[2], &r1_request->it_nexus_3);
+ 
+ 	put_unaligned_le32(scsi_bufflen(scmd), &r1_request->data_length);
+ 	r1_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;
+ 	put_unaligned_le16(io_request->index, &r1_request->request_id);
+ 	r1_request->error_index = r1_request->request_id;
+ 	if (rmd->cdb_length > sizeof(r1_request->cdb))
+ 		rmd->cdb_length = sizeof(r1_request->cdb);
+ 	r1_request->cdb_length = rmd->cdb_length;
+ 	memcpy(r1_request->cdb, rmd->cdb, rmd->cdb_length);
+ 
+ 	/* The direction is always write. */
+ 	r1_request->data_direction = SOP_READ_FLAG;
+ 
+ 	if (encryption_info) {
+ 		r1_request->encryption_enable = true;
+ 		put_unaligned_le16(encryption_info->data_encryption_key_index,
+ 				&r1_request->data_encryption_key_index);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_lower,
+ 				&r1_request->encrypt_tweak_lower);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_upper,
+ 				&r1_request->encrypt_tweak_upper);
+ 	}
+ 
+ 	rc = pqi_build_aio_r1_sg_list(ctrl_info, r1_request, scmd, io_request);
+ 	if (rc) {
+ 		pqi_free_io_request(io_request);
+ 		return SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 
+ 	pqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);
+ 
+ 	return 0;
+ }
+ 
+ static int pqi_aio_submit_r56_write_io(struct pqi_ctrl_info *ctrl_info,
+ 	struct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,
+ 	struct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,
+ 	struct pqi_scsi_dev_raid_map_data *rmd)
+ {
+ 	int rc;
+ 	struct pqi_io_request *io_request;
+ 	struct pqi_aio_r56_path_request *r56_request;
+ 
+ 	io_request = pqi_alloc_io_request(ctrl_info);
+ 	io_request->io_complete_callback = pqi_aio_io_complete;
+ 	io_request->scmd = scmd;
+ 	io_request->raid_bypass = true;
+ 
+ 	r56_request = io_request->iu;
+ 	memset(r56_request, 0, offsetof(struct pqi_aio_r56_path_request, sg_descriptors));
+ 
+ 	if (device->raid_level == SA_RAID_5 || device->raid_level == SA_RAID_51)
+ 		r56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID5_IO;
+ 	else
+ 		r56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID6_IO;
+ 
+ 	put_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r56_request->volume_id);
+ 	put_unaligned_le32(rmd->aio_handle, &r56_request->data_it_nexus);
+ 	put_unaligned_le32(rmd->p_parity_it_nexus, &r56_request->p_parity_it_nexus);
+ 	if (rmd->raid_level == SA_RAID_6) {
+ 		put_unaligned_le32(rmd->q_parity_it_nexus, &r56_request->q_parity_it_nexus);
+ 		r56_request->xor_multiplier = rmd->xor_mult;
+ 	}
+ 	put_unaligned_le32(scsi_bufflen(scmd), &r56_request->data_length);
+ 	r56_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;
+ 	put_unaligned_le64(rmd->row, &r56_request->row);
+ 
+ 	put_unaligned_le16(io_request->index, &r56_request->request_id);
+ 	r56_request->error_index = r56_request->request_id;
+ 
+ 	if (rmd->cdb_length > sizeof(r56_request->cdb))
+ 		rmd->cdb_length = sizeof(r56_request->cdb);
+ 	r56_request->cdb_length = rmd->cdb_length;
+ 	memcpy(r56_request->cdb, rmd->cdb, rmd->cdb_length);
+ 
+ 	/* The direction is always write. */
+ 	r56_request->data_direction = SOP_READ_FLAG;
+ 
+ 	if (encryption_info) {
+ 		r56_request->encryption_enable = true;
+ 		put_unaligned_le16(encryption_info->data_encryption_key_index,
+ 				&r56_request->data_encryption_key_index);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_lower,
+ 				&r56_request->encrypt_tweak_lower);
+ 		put_unaligned_le32(encryption_info->encrypt_tweak_upper,
+ 				&r56_request->encrypt_tweak_upper);
+ 	}
+ 
+ 	rc = pqi_build_aio_r56_sg_list(ctrl_info, r56_request, scmd, io_request);
+ 	if (rc) {
+ 		pqi_free_io_request(io_request);
+ 		return SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 
+ 	pqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 7a012c23c7a7 (scsi: smartpqi: Add support for RAID1 writes)
  static inline u16 pqi_get_hw_queue(struct pqi_ctrl_info *ctrl_info,
  	struct scsi_cmnd *scmd)
  {
* Unmerged path drivers/scsi/smartpqi/smartpqi.h
* Unmerged path drivers/scsi/smartpqi/smartpqi_init.c
