x86/fpu/signal: Handle #PF in the direct restore path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit fcb3635f5018e53024c6be3c3213737f469f74ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/fcb3635f.failed

If *RSTOR raises an exception, then the slow path is taken. That's wrong
because if the reason was not #PF then going through the slow path is waste
of time because that will end up with the same conclusion that the data is
invalid.

Now that the wrapper around *RSTOR return an negative error code, which is
the negated trap number, it's possible to differentiate.

If the *RSTOR raised #PF then handle it directly in the fast path and if it
was some other exception, e.g. #GP, then give up and do not try the fast
path.

This removes the legacy frame FRSTOR code from the slow path because FRSTOR
is not a ia32_fxstate frame and is therefore handled in the fast path.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20210623121457.696022863@linutronix.de
(cherry picked from commit fcb3635f5018e53024c6be3c3213737f469f74ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/signal.c
diff --cc arch/x86/kernel/fpu/signal.c
index 7e120ce9728a,4c252d0c8e6a..000000000000
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@@ -249,34 -250,85 +249,97 @@@ sanitize_restored_user_xstate(union fpr
  	}
  }
  
 -static int __restore_fpregs_from_user(void __user *buf, u64 xrestore,
 -				      bool fx_only)
 +/*
 + * Restore the extended state if present. Otherwise, restore the FP/SSE state.
 + */
 +static int copy_user_to_fpregs_zeroing(void __user *buf, u64 xbv, int fx_only)
  {
 -	if (use_xsave()) {
 -		u64 init_bv = xfeatures_mask_uabi() & ~xrestore;
 -		int ret;
 +	u64 init_bv;
 +	int r;
  
 -		if (likely(!fx_only))
 -			ret = xrstor_from_user_sigframe(buf, xrestore);
 -		else
 -			ret = fxrstor_from_user_sigframe(buf);
 -
 -		if (!ret && unlikely(init_bv))
 -			os_xrstor(&init_fpstate.xsave, init_bv);
 -		return ret;
 +	if (use_xsave()) {
 +		if (fx_only) {
 +			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
 +
 +			r = copy_user_to_fxregs(buf);
 +			if (!r)
 +				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 +			return r;
 +		} else {
 +			init_bv = xfeatures_mask_user() & ~xbv;
 +
 +			r = xrstor_from_user_sigframe(buf, xbv);
 +			if (!r && unlikely(init_bv))
 +				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 +			return r;
 +		}
  	} else if (use_fxsr()) {
 -		return fxrstor_from_user_sigframe(buf);
 -	} else {
 +		return copy_user_to_fxregs(buf);
 +	} else
  		return frstor_from_user_sigframe(buf);
++<<<<<<< HEAD
++=======
+ 	}
+ }
+ 
+ /*
+  * Attempt to restore the FPU registers directly from user memory.
+  * Pagefaults are handled and any errors returned are fatal.
+  */
+ static int restore_fpregs_from_user(void __user *buf, u64 xrestore,
+ 				    bool fx_only, unsigned int size)
+ {
+ 	struct fpu *fpu = &current->thread.fpu;
+ 	int ret;
+ 
+ retry:
+ 	fpregs_lock();
+ 	pagefault_disable();
+ 	ret = __restore_fpregs_from_user(buf, xrestore, fx_only);
+ 	pagefault_enable();
+ 
+ 	if (unlikely(ret)) {
+ 		/*
+ 		 * The above did an FPU restore operation, restricted to
+ 		 * the user portion of the registers, and failed, but the
+ 		 * microcode might have modified the FPU registers
+ 		 * nevertheless.
+ 		 *
+ 		 * If the FPU registers do not belong to current, then
+ 		 * invalidate the FPU register state otherwise the task
+ 		 * might preempt current and return to user space with
+ 		 * corrupted FPU registers.
+ 		 */
+ 		if (test_thread_flag(TIF_NEED_FPU_LOAD))
+ 			__cpu_invalidate_fpregs_state();
+ 		fpregs_unlock();
+ 
+ 		/* Try to handle #PF, but anything else is fatal. */
+ 		if (ret != -EFAULT)
+ 			return -EINVAL;
+ 
+ 		ret = fault_in_pages_readable(buf, size);
+ 		if (!ret)
+ 			goto retry;
+ 		return ret;
+ 	}
+ 
+ 	/*
+ 	 * Restore supervisor states: previous context switch etc has done
+ 	 * XSAVES and saved the supervisor states in the kernel buffer from
+ 	 * which they can be restored now.
+ 	 *
+ 	 * It would be optimal to handle this with a single XRSTORS, but
+ 	 * this does not work because the rest of the FPU registers have
+ 	 * been restored from a user buffer directly.
+ 	 */
+ 	if (test_thread_flag(TIF_NEED_FPU_LOAD) && xfeatures_mask_supervisor())
+ 		os_xrstor(&fpu->state.xsave, xfeatures_mask_supervisor());
+ 
+ 	fpregs_mark_activate();
+ 	fpregs_unlock();
+ 	return 0;
++>>>>>>> fcb3635f5018 (x86/fpu/signal: Handle #PF in the direct restore path)
  }
  
  static int __fpu_restore_sig(void __user *buf, void __user *buf_fx,
@@@ -303,71 -354,26 +365,85 @@@
  		user_xfeatures = fx_sw_user.xfeatures;
  	}
  
 -	if (likely(!ia32_fxstate)) {
 +	if (!ia32_fxstate) {
  		/*
  		 * Attempt to restore the FPU registers directly from user
 -		 * memory. For that to succeed, the user access cannot cause page
 -		 * faults. If it does, fall back to the slow path below, going
 -		 * through the kernel buffer with the enabled pagefault handler.
 +		 * memory. For that to succeed, the user access cannot cause
 +		 * page faults. If it does, fall back to the slow path below,
 +		 * going through the kernel buffer with the enabled pagefault
 +		 * handler.
  		 */
++<<<<<<< HEAD
 +		fpregs_lock();
 +		pagefault_disable();
 +		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
 +		pagefault_enable();
 +		if (!ret) {
 +
 +			/*
 +			 * Restore supervisor states: previous context switch
 +			 * etc has done XSAVES and saved the supervisor states
 +			 * in the kernel buffer from which they can be restored
 +			 * now.
 +			 *
 +			 * We cannot do a single XRSTORS here - which would
 +			 * be nice - because the rest of the FPU registers are
 +			 * being restored from a user buffer directly. The
 +			 * single XRSTORS happens below, when the user buffer
 +			 * has been copied to the kernel one.
 +			 */
 +			if (test_thread_flag(TIF_NEED_FPU_LOAD) &&
 +			    xfeatures_mask_supervisor())
 +				copy_kernel_to_xregs(&fpu->state.xsave,
 +						     xfeatures_mask_supervisor());
 +			fpregs_mark_activate();
 +			fpregs_unlock();
 +			return 0;
 +		}
 +
 +		/*
 +		 * The above did an FPU restore operation, restricted to
 +		 * the user portion of the registers, and failed, but the
 +		 * microcode might have modified the FPU registers
 +		 * nevertheless.
 +		 *
 +		 * If the FPU registers do not belong to current, then
 +		 * invalidate the FPU register state otherwise the task might
 +		 * preempt current and return to user space with corrupted
 +		 * FPU registers.
 +		 *
 +		 * In case current owns the FPU registers then no further
 +		 * action is required. The fixup below will handle it
 +		 * correctly.
 +		 */
 +		if (test_thread_flag(TIF_NEED_FPU_LOAD))
 +			__cpu_invalidate_fpregs_state();
 +
 +		fpregs_unlock();
 +	} else {
 +		/*
 +		 * For 32-bit frames with fxstate, copy the fxstate so it can
 +		 * be reconstructed later.
 +		 */
 +		ret = __copy_from_user(&env, buf, sizeof(env));
 +		if (ret)
 +			return ret;
 +		envp = &env;
++=======
+ 		return restore_fpregs_from_user(buf_fx, user_xfeatures, fx_only,
+ 						state_size);
++>>>>>>> fcb3635f5018 (x86/fpu/signal: Handle #PF in the direct restore path)
  	}
  
+ 	/*
+ 	 * Copy the legacy state because the FP portion of the FX frame has
+ 	 * to be ignored for histerical raisins. The legacy state is folded
+ 	 * in once the larger state has been copied.
+ 	 */
+ 	ret = __copy_from_user(&env, buf, sizeof(env));
+ 	if (ret)
+ 		return ret;
+ 
  	/*
  	 * By setting TIF_NEED_FPU_LOAD it is ensured that our xstate is
  	 * not modified on context switch and that the xstate is considered
@@@ -382,11 -388,10 +458,10 @@@
  		 * supervisor state is preserved. Save the full state for
  		 * simplicity. There is no point in optimizing this by only
  		 * saving the supervisor states and then shuffle them to
- 		 * the right place in memory. This is the slow path and the
- 		 * above XRSTOR failed or ia32_fxstate is true. Shrug.
+ 		 * the right place in memory. It's ia32 mode. Shrug.
  		 */
  		if (xfeatures_mask_supervisor())
 -			os_xsave(&fpu->state.xsave);
 +			copy_xregs_to_kernel(&fpu->state.xsave);
  		set_thread_flag(TIF_NEED_FPU_LOAD);
  	}
  	__fpu_invalidate_fpregs_state(fpu);
@@@ -409,10 -414,10 +484,10 @@@
  		 * Restore previously saved supervisor xstates along with
  		 * copied-in user xstates.
  		 */
 -		ret = os_xrstor_safe(&fpu->state.xsave,
 -				     user_xfeatures | xfeatures_mask_supervisor());
 +		ret = copy_kernel_to_xregs_err(&fpu->state.xsave,
 +					       user_xfeatures | xfeatures_mask_supervisor());
  
- 	} else if (use_fxsr()) {
+ 	} else {
  		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
  		if (ret)
  			return -EFAULT;
@@@ -423,21 -428,17 +498,26 @@@
  		if (use_xsave()) {
  			u64 init_bv;
  
 -			init_bv = xfeatures_mask_uabi() & ~XFEATURE_MASK_FPSSE;
 -			os_xrstor(&init_fpstate.xsave, init_bv);
 +			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
 +			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
  		}
  
++<<<<<<< HEAD
 +		ret = copy_kernel_to_fxregs_err(&fpu->state.fxsave);
 +	} else {
 +		ret = __copy_from_user(&fpu->state.fsave, buf_fx, state_size);
 +		if (ret)
 +			return ret;
 +
 +		fpregs_lock();
 +		ret = frstor_safe(&fpu->state.fsave);
++=======
+ 		ret = fxrstor_safe(&fpu->state.fxsave);
++>>>>>>> fcb3635f5018 (x86/fpu/signal: Handle #PF in the direct restore path)
  	}
+ 
  	if (!ret)
  		fpregs_mark_activate();
 -	else
 -		fpregs_deactivate(fpu);
  	fpregs_unlock();
  	return ret;
  }
* Unmerged path arch/x86/kernel/fpu/signal.c
