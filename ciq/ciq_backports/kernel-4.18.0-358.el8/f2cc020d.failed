tracing: Fix various typos in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit f2cc020d7876de7583feb52ec939a32419cf9468
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/f2cc020d.failed

Fix ~59 single-word typos in the tracing code comments, and fix
the grammar in a handful of places.

Link: https://lore.kernel.org/r/20210322224546.GA1981273@gmail.com
Link: https://lkml.kernel.org/r/20210323174935.GA4176821@gmail.com

	Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
(cherry picked from commit f2cc020d7876de7583feb52ec939a32419cf9468)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/nds32/kernel/ftrace.c
#	include/linux/trace_events.h
#	include/trace/events/sched.h
#	kernel/trace/fgraph.c
#	kernel/trace/ring_buffer.c
#	kernel/trace/synth_event_gen_test.c
#	kernel/trace/trace.c
#	kernel/trace/trace_events_synth.c
#	kernel/trace/trace_hwlat.c
#	kernel/trace/trace_probe.c
#	kernel/trace/trace_probe_tmpl.h
diff --cc include/linux/trace_events.h
index 9b39700490cd,36e27c1f42e0..000000000000
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@@ -147,9 -152,68 +147,74 @@@ enum print_line_t 
  
  enum print_line_t trace_handle_return(struct trace_seq *s);
  
++<<<<<<< HEAD
 +void tracing_generic_entry_update(struct trace_entry *entry,
 +				  unsigned long flags,
 +				  int pc);
++=======
+ static inline void tracing_generic_entry_update(struct trace_entry *entry,
+ 						unsigned short type,
+ 						unsigned int trace_ctx)
+ {
+ 	entry->preempt_count		= trace_ctx & 0xff;
+ 	entry->pid			= current->pid;
+ 	entry->type			= type;
+ 	entry->flags =			trace_ctx >> 16;
+ }
+ 
+ unsigned int tracing_gen_ctx_irq_test(unsigned int irqs_status);
+ 
+ enum trace_flag_type {
+ 	TRACE_FLAG_IRQS_OFF		= 0x01,
+ 	TRACE_FLAG_IRQS_NOSUPPORT	= 0x02,
+ 	TRACE_FLAG_NEED_RESCHED		= 0x04,
+ 	TRACE_FLAG_HARDIRQ		= 0x08,
+ 	TRACE_FLAG_SOFTIRQ		= 0x10,
+ 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
+ 	TRACE_FLAG_NMI			= 0x40,
+ };
+ 
+ #ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
+ static inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)
+ {
+ 	unsigned int irq_status = irqs_disabled_flags(irqflags) ?
+ 		TRACE_FLAG_IRQS_OFF : 0;
+ 	return tracing_gen_ctx_irq_test(irq_status);
+ }
+ static inline unsigned int tracing_gen_ctx(void)
+ {
+ 	unsigned long irqflags;
+ 
+ 	local_save_flags(irqflags);
+ 	return tracing_gen_ctx_flags(irqflags);
+ }
+ #else
+ 
+ static inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)
+ {
+ 	return tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);
+ }
+ static inline unsigned int tracing_gen_ctx(void)
+ {
+ 	return tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);
+ }
+ #endif
+ 
+ static inline unsigned int tracing_gen_ctx_dec(void)
+ {
+ 	unsigned int trace_ctx;
+ 
+ 	trace_ctx = tracing_gen_ctx();
+ 	/*
+ 	 * Subtract one from the preemption counter if preemption is enabled,
+ 	 * see trace_event_buffer_reserve()for details.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_PREEMPTION))
+ 		trace_ctx--;
+ 	return trace_ctx;
+ }
+ 
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  struct trace_event_file;
  
  struct ring_buffer_event *
diff --cc include/trace/events/sched.h
index 835464eb58d1,1eca2305ca42..000000000000
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@@ -90,8 -174,8 +90,13 @@@ DEFINE_EVENT(sched_wakeup_template, sch
  	     TP_ARGS(p));
  
  /*
++<<<<<<< HEAD
 + * Tracepoint called when the task is actually woken; p->state == TASK_RUNNNG.
 + * It it not always called from the waking context.
++=======
+  * Tracepoint called when the task is actually woken; p->state == TASK_RUNNING.
+  * It is not always called from the waking context.
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
   */
  DEFINE_EVENT(sched_wakeup_template, sched_wakeup,
  	     TP_PROTO(struct task_struct *p),
diff --cc kernel/trace/ring_buffer.c
index faa6a3aeb77a,f4216df58e31..000000000000
--- a/kernel/trace/ring_buffer.c
+++ b/kernel/trace/ring_buffer.c
@@@ -2643,6 -3137,30 +2643,33 @@@ rb_wakeups(struct trace_buffer *buffer
   * The least significant bit can be cleared this way, and it
   * just so happens that it is the same bit corresponding to
   * the current context.
++<<<<<<< HEAD
++=======
+  *
+  * Now the TRANSITION bit breaks the above slightly. The TRANSITION bit
+  * is set when a recursion is detected at the current context, and if
+  * the TRANSITION bit is already set, it will fail the recursion.
+  * This is needed because there's a lag between the changing of
+  * interrupt context and updating the preempt count. In this case,
+  * a false positive will be found. To handle this, one extra recursion
+  * is allowed, and this is done by the TRANSITION bit. If the TRANSITION
+  * bit is already set, then it is considered a recursion and the function
+  * ends. Otherwise, the TRANSITION bit is set, and that bit is returned.
+  *
+  * On the trace_recursive_unlock(), the TRANSITION bit will be the first
+  * to be cleared. Even if it wasn't the context that set it. That is,
+  * if an interrupt comes in while NORMAL bit is set and the ring buffer
+  * is called before preempt_count() is updated, since the check will
+  * be on the NORMAL bit, the TRANSITION bit will then be set. If an
+  * NMI then comes in, it will set the NMI bit, but when the NMI code
+  * does the trace_recursive_unlock() it will clear the TRANSITION bit
+  * and leave the NMI bit set. But this is fine, because the interrupt
+  * code that set the TRANSITION bit will then clear the NMI bit when it
+  * calls trace_recursive_unlock(). If another NMI comes in, it will
+  * set the TRANSITION bit and continue.
+  *
+  * Note: The TRANSITION bit only handles a single transition between context.
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
   */
  
  static __always_inline int
diff --cc kernel/trace/trace.c
index 3efee662ba03,c8e54b674d3e..000000000000
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@@ -491,16 -540,22 +491,24 @@@ trace_find_filtered_pid(struct trace_pi
   * Returns false if @task should be traced.
   */
  bool
 -trace_ignore_this_task(struct trace_pid_list *filtered_pids,
 -		       struct trace_pid_list *filtered_no_pids,
 -		       struct task_struct *task)
 +trace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
  {
  	/*
++<<<<<<< HEAD
 +	 * Return false, because if filtered_pids does not exist,
 +	 * all pids are good to trace.
++=======
+ 	 * If filtered_no_pids is not empty, and the task's pid is listed
+ 	 * in filtered_no_pids, then return true.
+ 	 * Otherwise, if filtered_pids is empty, that means we can
+ 	 * trace all tasks. If it has content, then only trace pids
+ 	 * within filtered_pids.
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  	 */
 +	if (!filtered_pids)
 +		return false;
  
 -	return (filtered_pids &&
 -		!trace_find_filtered_pid(filtered_pids, task->pid)) ||
 -		(filtered_no_pids &&
 -		 trace_find_filtered_pid(filtered_no_pids, task->pid));
 +	return !trace_find_filtered_pid(filtered_pids, task->pid);
  }
  
  /**
@@@ -3029,6 -3356,26 +3037,29 @@@ int trace_array_vprintk(struct trace_ar
  	return __trace_array_vprintk(tr->array_buffer.buffer, ip, fmt, args);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * trace_array_printk - Print a message to a specific instance
+  * @tr: The instance trace_array descriptor
+  * @ip: The instruction pointer that this is called from.
+  * @fmt: The format to print (printf format)
+  *
+  * If a subsystem sets up its own instance, they have the right to
+  * printk strings into their tracing instance buffer using this
+  * function. Note, this function will not write into the top level
+  * buffer (use trace_printk() for that), as writing into the top level
+  * buffer should only have events that can be individually disabled.
+  * trace_printk() is only used for debugging a kernel, and should not
+  * be ever incorporated in normal use.
+  *
+  * trace_array_printk() can be used, as it will not add noise to the
+  * top level tracing buffer.
+  *
+  * Note, trace_array_init_printk() must be called on @tr before this
+  * can be used.
+  */
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  __printf(3, 0)
  int trace_array_printk(struct trace_array *tr,
  		       unsigned long ip, const char *fmt, ...)
@@@ -6657,6 -7488,251 +6688,254 @@@ static const struct file_operations sna
  
  #endif /* CONFIG_TRACER_SNAPSHOT */
  
++<<<<<<< HEAD
++=======
+ #define TRACING_LOG_ERRS_MAX	8
+ #define TRACING_LOG_LOC_MAX	128
+ 
+ #define CMD_PREFIX "  Command: "
+ 
+ struct err_info {
+ 	const char	**errs;	/* ptr to loc-specific array of err strings */
+ 	u8		type;	/* index into errs -> specific err string */
+ 	u8		pos;	/* MAX_FILTER_STR_VAL = 256 */
+ 	u64		ts;
+ };
+ 
+ struct tracing_log_err {
+ 	struct list_head	list;
+ 	struct err_info		info;
+ 	char			loc[TRACING_LOG_LOC_MAX]; /* err location */
+ 	char			cmd[MAX_FILTER_STR_VAL]; /* what caused err */
+ };
+ 
+ static DEFINE_MUTEX(tracing_err_log_lock);
+ 
+ static struct tracing_log_err *get_tracing_log_err(struct trace_array *tr)
+ {
+ 	struct tracing_log_err *err;
+ 
+ 	if (tr->n_err_log_entries < TRACING_LOG_ERRS_MAX) {
+ 		err = kzalloc(sizeof(*err), GFP_KERNEL);
+ 		if (!err)
+ 			err = ERR_PTR(-ENOMEM);
+ 		tr->n_err_log_entries++;
+ 
+ 		return err;
+ 	}
+ 
+ 	err = list_first_entry(&tr->err_log, struct tracing_log_err, list);
+ 	list_del(&err->list);
+ 
+ 	return err;
+ }
+ 
+ /**
+  * err_pos - find the position of a string within a command for error careting
+  * @cmd: The tracing command that caused the error
+  * @str: The string to position the caret at within @cmd
+  *
+  * Finds the position of the first occurrence of @str within @cmd.  The
+  * return value can be passed to tracing_log_err() for caret placement
+  * within @cmd.
+  *
+  * Returns the index within @cmd of the first occurrence of @str or 0
+  * if @str was not found.
+  */
+ unsigned int err_pos(char *cmd, const char *str)
+ {
+ 	char *found;
+ 
+ 	if (WARN_ON(!strlen(cmd)))
+ 		return 0;
+ 
+ 	found = strstr(cmd, str);
+ 	if (found)
+ 		return found - cmd;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * tracing_log_err - write an error to the tracing error log
+  * @tr: The associated trace array for the error (NULL for top level array)
+  * @loc: A string describing where the error occurred
+  * @cmd: The tracing command that caused the error
+  * @errs: The array of loc-specific static error strings
+  * @type: The index into errs[], which produces the specific static err string
+  * @pos: The position the caret should be placed in the cmd
+  *
+  * Writes an error into tracing/error_log of the form:
+  *
+  * <loc>: error: <text>
+  *   Command: <cmd>
+  *              ^
+  *
+  * tracing/error_log is a small log file containing the last
+  * TRACING_LOG_ERRS_MAX errors (8).  Memory for errors isn't allocated
+  * unless there has been a tracing error, and the error log can be
+  * cleared and have its memory freed by writing the empty string in
+  * truncation mode to it i.e. echo > tracing/error_log.
+  *
+  * NOTE: the @errs array along with the @type param are used to
+  * produce a static error string - this string is not copied and saved
+  * when the error is logged - only a pointer to it is saved.  See
+  * existing callers for examples of how static strings are typically
+  * defined for use with tracing_log_err().
+  */
+ void tracing_log_err(struct trace_array *tr,
+ 		     const char *loc, const char *cmd,
+ 		     const char **errs, u8 type, u8 pos)
+ {
+ 	struct tracing_log_err *err;
+ 
+ 	if (!tr)
+ 		tr = &global_trace;
+ 
+ 	mutex_lock(&tracing_err_log_lock);
+ 	err = get_tracing_log_err(tr);
+ 	if (PTR_ERR(err) == -ENOMEM) {
+ 		mutex_unlock(&tracing_err_log_lock);
+ 		return;
+ 	}
+ 
+ 	snprintf(err->loc, TRACING_LOG_LOC_MAX, "%s: error: ", loc);
+ 	snprintf(err->cmd, MAX_FILTER_STR_VAL,"\n" CMD_PREFIX "%s\n", cmd);
+ 
+ 	err->info.errs = errs;
+ 	err->info.type = type;
+ 	err->info.pos = pos;
+ 	err->info.ts = local_clock();
+ 
+ 	list_add_tail(&err->list, &tr->err_log);
+ 	mutex_unlock(&tracing_err_log_lock);
+ }
+ 
+ static void clear_tracing_err_log(struct trace_array *tr)
+ {
+ 	struct tracing_log_err *err, *next;
+ 
+ 	mutex_lock(&tracing_err_log_lock);
+ 	list_for_each_entry_safe(err, next, &tr->err_log, list) {
+ 		list_del(&err->list);
+ 		kfree(err);
+ 	}
+ 
+ 	tr->n_err_log_entries = 0;
+ 	mutex_unlock(&tracing_err_log_lock);
+ }
+ 
+ static void *tracing_err_log_seq_start(struct seq_file *m, loff_t *pos)
+ {
+ 	struct trace_array *tr = m->private;
+ 
+ 	mutex_lock(&tracing_err_log_lock);
+ 
+ 	return seq_list_start(&tr->err_log, *pos);
+ }
+ 
+ static void *tracing_err_log_seq_next(struct seq_file *m, void *v, loff_t *pos)
+ {
+ 	struct trace_array *tr = m->private;
+ 
+ 	return seq_list_next(v, &tr->err_log, pos);
+ }
+ 
+ static void tracing_err_log_seq_stop(struct seq_file *m, void *v)
+ {
+ 	mutex_unlock(&tracing_err_log_lock);
+ }
+ 
+ static void tracing_err_log_show_pos(struct seq_file *m, u8 pos)
+ {
+ 	u8 i;
+ 
+ 	for (i = 0; i < sizeof(CMD_PREFIX) - 1; i++)
+ 		seq_putc(m, ' ');
+ 	for (i = 0; i < pos; i++)
+ 		seq_putc(m, ' ');
+ 	seq_puts(m, "^\n");
+ }
+ 
+ static int tracing_err_log_seq_show(struct seq_file *m, void *v)
+ {
+ 	struct tracing_log_err *err = v;
+ 
+ 	if (err) {
+ 		const char *err_text = err->info.errs[err->info.type];
+ 		u64 sec = err->info.ts;
+ 		u32 nsec;
+ 
+ 		nsec = do_div(sec, NSEC_PER_SEC);
+ 		seq_printf(m, "[%5llu.%06u] %s%s", sec, nsec / 1000,
+ 			   err->loc, err_text);
+ 		seq_printf(m, "%s", err->cmd);
+ 		tracing_err_log_show_pos(m, err->info.pos);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static const struct seq_operations tracing_err_log_seq_ops = {
+ 	.start  = tracing_err_log_seq_start,
+ 	.next   = tracing_err_log_seq_next,
+ 	.stop   = tracing_err_log_seq_stop,
+ 	.show   = tracing_err_log_seq_show
+ };
+ 
+ static int tracing_err_log_open(struct inode *inode, struct file *file)
+ {
+ 	struct trace_array *tr = inode->i_private;
+ 	int ret = 0;
+ 
+ 	ret = tracing_check_open_get_tr(tr);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* If this file was opened for write, then erase contents */
+ 	if ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC))
+ 		clear_tracing_err_log(tr);
+ 
+ 	if (file->f_mode & FMODE_READ) {
+ 		ret = seq_open(file, &tracing_err_log_seq_ops);
+ 		if (!ret) {
+ 			struct seq_file *m = file->private_data;
+ 			m->private = tr;
+ 		} else {
+ 			trace_array_put(tr);
+ 		}
+ 	}
+ 	return ret;
+ }
+ 
+ static ssize_t tracing_err_log_write(struct file *file,
+ 				     const char __user *buffer,
+ 				     size_t count, loff_t *ppos)
+ {
+ 	return count;
+ }
+ 
+ static int tracing_err_log_release(struct inode *inode, struct file *file)
+ {
+ 	struct trace_array *tr = inode->i_private;
+ 
+ 	trace_array_put(tr);
+ 
+ 	if (file->f_mode & FMODE_READ)
+ 		seq_release(inode, file);
+ 
+ 	return 0;
+ }
+ 
+ static const struct file_operations tracing_err_log_fops = {
+ 	.open           = tracing_err_log_open,
+ 	.write		= tracing_err_log_write,
+ 	.read           = seq_read,
+ 	.llseek         = seq_lseek,
+ 	.release        = tracing_err_log_release,
+ };
+ 
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  static int tracing_buffers_open(struct inode *inode, struct file *filp)
  {
  	struct trace_array *tr = inode->i_private;
diff --cc kernel/trace/trace_hwlat.c
index 941d00c9d04e,632ef88131a9..000000000000
--- a/kernel/trace/trace_hwlat.c
+++ b/kernel/trace/trace_hwlat.c
@@@ -85,6 -83,7 +85,10 @@@ struct hwlat_sample 
  	u64			nmi_total_ts;	/* Total time spent in NMIs */
  	struct timespec64	timestamp;	/* wall time */
  	int			nmi_count;	/* # NMIs during this sample */
++<<<<<<< HEAD
++=======
+ 	int			count;		/* # of iterations over thresh */
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  };
  
  /* keep the global state somewhere. */
diff --cc kernel/trace/trace_probe.c
index daf54bda4dc8,15413ad7cef2..000000000000
--- a/kernel/trace/trace_probe.c
+++ b/kernel/trace/trace_probe.c
@@@ -288,35 -140,68 +288,72 @@@ fail
  	return NULL;
  }
  
 -static struct trace_probe_log trace_probe_log;
 -
 -void trace_probe_log_init(const char *subsystem, int argc, const char **argv)
 +/* Special function : only accept unsigned long */
 +static void fetch_kernel_stack_address(struct pt_regs *regs, void *dummy, void *dest)
  {
 -	trace_probe_log.subsystem = subsystem;
 -	trace_probe_log.argc = argc;
 -	trace_probe_log.argv = argv;
 -	trace_probe_log.index = 0;
 +	*(unsigned long *)dest = kernel_stack_pointer(regs);
  }
 +NOKPROBE_SYMBOL(fetch_kernel_stack_address);
  
 -void trace_probe_log_clear(void)
 +static void fetch_user_stack_address(struct pt_regs *regs, void *dummy, void *dest)
  {
 -	memset(&trace_probe_log, 0, sizeof(trace_probe_log));
 +	*(unsigned long *)dest = user_stack_pointer(regs);
  }
 +NOKPROBE_SYMBOL(fetch_user_stack_address);
  
 -void trace_probe_log_set_index(int index)
 +static fetch_func_t get_fetch_size_function(const struct fetch_type *type,
 +					    fetch_func_t orig_fn,
 +					    const struct fetch_type *ftbl)
  {
 -	trace_probe_log.index = index;
 -}
 +	int i;
  
 -void __trace_probe_log_err(int offset, int err_type)
 -{
 -	char *command, *p;
 -	int i, len = 0, pos = 0;
 +	if (type != &ftbl[FETCH_TYPE_STRING])
 +		return NULL;	/* Only string type needs size function */
  
 -	if (!trace_probe_log.argv)
 -		return;
 +	for (i = 0; i < FETCH_MTD_END; i++)
 +		if (type->fetch[i] == orig_fn)
 +			return ftbl[FETCH_TYPE_STRSIZE].fetch[i];
  
++<<<<<<< HEAD
 +	WARN_ON(1);	/* This should not happen */
 +
 +	return NULL;
++=======
+ 	/* Recalculate the length and allocate buffer */
+ 	for (i = 0; i < trace_probe_log.argc; i++) {
+ 		if (i == trace_probe_log.index)
+ 			pos = len;
+ 		len += strlen(trace_probe_log.argv[i]) + 1;
+ 	}
+ 	command = kzalloc(len, GFP_KERNEL);
+ 	if (!command)
+ 		return;
+ 
+ 	if (trace_probe_log.index >= trace_probe_log.argc) {
+ 		/**
+ 		 * Set the error position is next to the last arg + space.
+ 		 * Note that len includes the terminal null and the cursor
+ 		 * appears at pos + 1.
+ 		 */
+ 		pos = len;
+ 		offset = 0;
+ 	}
+ 
+ 	/* And make a command string from argv array */
+ 	p = command;
+ 	for (i = 0; i < trace_probe_log.argc; i++) {
+ 		len = strlen(trace_probe_log.argv[i]);
+ 		strcpy(p, trace_probe_log.argv[i]);
+ 		p[len] = ' ';
+ 		p += len + 1;
+ 	}
+ 	*(p - 1) = '\0';
+ 
+ 	tracing_log_err(NULL, trace_probe_log.subsystem, command,
+ 			trace_probe_err_text, err_type, pos + offset);
+ 
+ 	kfree(command);
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  }
  
  /* Split symbol and offset. */
@@@ -531,36 -540,70 +568,41 @@@ static int __parse_bitfield_probe_arg(c
  }
  
  /* String length checking wrapper */
 -static int traceprobe_parse_probe_arg_body(char *arg, ssize_t *size,
 -		struct probe_arg *parg, unsigned int flags, int offset)
 +int traceprobe_parse_probe_arg(char *arg, ssize_t *size,
 +		struct probe_arg *parg, bool is_return, bool is_kprobe,
 +		const struct fetch_type *ftbl)
  {
 -	struct fetch_insn *code, *scode, *tmp = NULL;
 -	char *t, *t2, *t3;
 -	int ret, len;
 +	const char *t;
 +	int ret;
  
 -	len = strlen(arg);
 -	if (len > MAX_ARGSTR_LEN) {
 -		trace_probe_log_err(offset, ARG_TOO_LONG);
 -		return -EINVAL;
 -	} else if (len == 0) {
 -		trace_probe_log_err(offset, NO_ARG_BODY);
 -		return -EINVAL;
 +	if (strlen(arg) > MAX_ARGSTR_LEN) {
 +		pr_info("Argument is too long.: %s\n",  arg);
 +		return -ENOSPC;
  	}
 -
  	parg->comm = kstrdup(arg, GFP_KERNEL);
 -	if (!parg->comm)
 +	if (!parg->comm) {
 +		pr_info("Failed to allocate memory for command '%s'.\n", arg);
  		return -ENOMEM;
 -
 -	t = strchr(arg, ':');
 +	}
 +	t = strchr(parg->comm, ':');
  	if (t) {
 -		*t = '\0';
 -		t2 = strchr(++t, '[');
 -		if (t2) {
 -			*t2++ = '\0';
 -			t3 = strchr(t2, ']');
 -			if (!t3) {
 -				offset += t2 + strlen(t2) - arg;
 -				trace_probe_log_err(offset,
 -						    ARRAY_NO_CLOSE);
 -				return -EINVAL;
 -			} else if (t3[1] != '\0') {
 -				trace_probe_log_err(offset + t3 + 1 - arg,
 -						    BAD_ARRAY_SUFFIX);
 -				return -EINVAL;
 -			}
 -			*t3 = '\0';
 -			if (kstrtouint(t2, 0, &parg->count) || !parg->count) {
 -				trace_probe_log_err(offset + t2 - arg,
 -						    BAD_ARRAY_NUM);
 -				return -EINVAL;
 -			}
 -			if (parg->count > MAX_ARRAY_LEN) {
 -				trace_probe_log_err(offset + t2 - arg,
 -						    ARRAY_TOO_BIG);
 -				return -EINVAL;
 -			}
 -		}
 +		arg[t - parg->comm] = '\0';
 +		t++;
  	}
 -
  	/*
++<<<<<<< HEAD
 +	 * The default type of $comm should be "string", and it can't be
 +	 * dereferenced.
++=======
+ 	 * Since $comm and immediate string can not be dereferenced,
+ 	 * we can find those by strcmp.
++>>>>>>> f2cc020d7876 (tracing: Fix various typos in comments)
  	 */
 -	if (strcmp(arg, "$comm") == 0 || strncmp(arg, "\\\"", 2) == 0) {
 -		/* The type of $comm must be "string", and not an array. */
 -		if (parg->count || (t && strcmp(t, "string")))
 -			return -EINVAL;
 -		parg->type = find_fetch_type("string");
 -	} else
 -		parg->type = find_fetch_type(t);
 +	if (!t && strcmp(arg, "$comm") == 0)
 +		t = "string";
 +	parg->type = find_fetch_type(t, ftbl);
  	if (!parg->type) {
 -		trace_probe_log_err(offset + (t ? (t - arg) : 0), BAD_TYPE);
 +		pr_info("Unsupported type: %s\n", t);
  		return -EINVAL;
  	}
  	parg->offset = *size;
* Unmerged path arch/nds32/kernel/ftrace.c
* Unmerged path kernel/trace/fgraph.c
* Unmerged path kernel/trace/synth_event_gen_test.c
* Unmerged path kernel/trace/trace_events_synth.c
* Unmerged path kernel/trace/trace_probe_tmpl.h
diff --git a/arch/microblaze/include/asm/ftrace.h b/arch/microblaze/include/asm/ftrace.h
index 5db7f4489f05..6a92bed37794 100644
--- a/arch/microblaze/include/asm/ftrace.h
+++ b/arch/microblaze/include/asm/ftrace.h
@@ -13,7 +13,7 @@ extern void ftrace_call_graph(void);
 #endif
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-/* reloction of mcount call site is the same as the address */
+/* relocation of mcount call site is the same as the address */
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
 	return addr;
* Unmerged path arch/nds32/kernel/ftrace.c
diff --git a/arch/powerpc/include/asm/ftrace.h b/arch/powerpc/include/asm/ftrace.h
index 3dfb80b86561..6b3360aaf712 100644
--- a/arch/powerpc/include/asm/ftrace.h
+++ b/arch/powerpc/include/asm/ftrace.h
@@ -10,7 +10,7 @@
 
 #ifdef __ASSEMBLY__
 
-/* Based off of objdump optput from glibc */
+/* Based off of objdump output from glibc */
 
 #define MCOUNT_SAVE_FRAME			\
 	stwu	r1,-48(r1);			\
@@ -50,7 +50,7 @@ extern void _mcount(void);
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
-       /* reloction of mcount call site is the same as the address */
+       /* relocation of mcount call site is the same as the address */
        return addr;
 }
 
diff --git a/arch/sh/kernel/ftrace.c b/arch/sh/kernel/ftrace.c
index 96dd9f7da250..fab3dd03d710 100644
--- a/arch/sh/kernel/ftrace.c
+++ b/arch/sh/kernel/ftrace.c
@@ -67,7 +67,7 @@ static unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr)
  * Modifying code must take extra care. On an SMP machine, if
  * the code being modified is also being executed on another CPU
  * that CPU will have undefined results and possibly take a GPF.
- * We use kstop_machine to stop other CPUS from exectuing code.
+ * We use kstop_machine to stop other CPUS from executing code.
  * But this does not stop NMIs from happening. We still need
  * to protect against that. We separate out the modification of
  * the code to take care of this.
diff --git a/arch/sparc/include/asm/ftrace.h b/arch/sparc/include/asm/ftrace.h
index d3aa1a524431..e284394cb3aa 100644
--- a/arch/sparc/include/asm/ftrace.h
+++ b/arch/sparc/include/asm/ftrace.h
@@ -17,7 +17,7 @@ void _mcount(void);
 #endif
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-/* reloction of mcount call site is the same as the address */
+/* relocation of mcount call site is the same as the address */
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
 	return addr;
diff --git a/fs/tracefs/inode.c b/fs/tracefs/inode.c
index bea8ad876bf9..51f6df2b18da 100644
--- a/fs/tracefs/inode.c
+++ b/fs/tracefs/inode.c
@@ -472,7 +472,7 @@ struct dentry *tracefs_create_dir(const char *name, struct dentry *parent)
  *
  * The instances directory is special as it allows for mkdir and rmdir to
  * to be done by userspace. When a mkdir or rmdir is performed, the inode
- * locks are released and the methhods passed in (@mkdir and @rmdir) are
+ * locks are released and the methods passed in (@mkdir and @rmdir) are
  * called without locks and with the name of the directory being created
  * within the instances directory.
  *
diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 86acd245f9aa..246ad40cac44 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -32,7 +32,7 @@
 /*
  * If the arch's mcount caller does not support all of ftrace's
  * features, then it must call an indirect function that
- * does. Or at least does enough to prevent any unwelcomed side effects.
+ * does. Or at least does enough to prevent any unwelcome side effects.
  */
 #if !ARCH_SUPPORTS_FTRACE_OPS
 # define FTRACE_FORCE_LIST_FUNC 1
@@ -343,7 +343,7 @@ DECLARE_PER_CPU(int, disable_stack_tracer);
  */
 static inline void stack_tracer_disable(void)
 {
-	/* Preemption or interupts must be disabled */
+	/* Preemption or interrupts must be disabled */
 	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT))
 		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
 	this_cpu_inc(disable_stack_tracer);
* Unmerged path include/linux/trace_events.h
diff --git a/include/linux/tracepoint.h b/include/linux/tracepoint.h
index 29330bf0d656..825cb5fa1120 100644
--- a/include/linux/tracepoint.h
+++ b/include/linux/tracepoint.h
@@ -431,7 +431,7 @@ extern void syscall_unregfunc(void);
  *	*
  *	* The declared 'local variable' is called '__entry'
  *	*
- *	* __field(pid_t, prev_prid) is equivalent to a standard declariton:
+ *	* __field(pid_t, prev_prid) is equivalent to a standard declaration:
  *	*
  *	*	pid_t	prev_pid;
  *	*
diff --git a/include/trace/events/io_uring.h b/include/trace/events/io_uring.h
index be97b7fa0ac9..e90f782c711a 100644
--- a/include/trace/events/io_uring.h
+++ b/include/trace/events/io_uring.h
@@ -49,7 +49,7 @@ TRACE_EVENT(io_uring_create,
 );
 
 /**
- * io_uring_register - called after a buffer/file/eventfd was succesfully
+ * io_uring_register - called after a buffer/file/eventfd was successfully
  * 					   registered for a ring
  *
  * @ctx:			pointer to a ring context structure
diff --git a/include/trace/events/rcu.h b/include/trace/events/rcu.h
index 12f25393eaeb..c1c528116026 100644
--- a/include/trace/events/rcu.h
+++ b/include/trace/events/rcu.h
@@ -48,7 +48,7 @@ TRACE_EVENT(rcu_utilization,
  * RCU flavor, the grace-period number, and a string identifying the
  * grace-period-related event as follows:
  *
- *	"AccReadyCB": CPU acclerates new callbacks to RCU_NEXT_READY_TAIL.
+ *	"AccReadyCB": CPU accelerates new callbacks to RCU_NEXT_READY_TAIL.
  *	"AccWaitCB": CPU accelerates new callbacks to RCU_WAIT_TAIL.
  *	"newreq": Request a new grace period.
  *	"start": Start a grace period.
* Unmerged path include/trace/events/sched.h
diff --git a/include/trace/events/timer.h b/include/trace/events/timer.h
index 295517f109d7..ef19c5eb2ed0 100644
--- a/include/trace/events/timer.h
+++ b/include/trace/events/timer.h
@@ -119,7 +119,7 @@ TRACE_EVENT(timer_expire_entry,
  * When used in combination with the timer_expire_entry tracepoint we can
  * determine the runtime of the timer callback function.
  *
- * NOTE: Do NOT derefernce timer in TP_fast_assign. The pointer might
+ * NOTE: Do NOT dereference timer in TP_fast_assign. The pointer might
  * be invalid. We solely track the pointer.
  */
 DEFINE_EVENT(timer_class, timer_expire_exit,
diff --git a/kernel/trace/bpf_trace.c b/kernel/trace/bpf_trace.c
index 235070f4b98b..4aa84b34435f 100644
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@ -648,7 +648,7 @@ BPF_CALL_5(bpf_seq_printf, struct seq_file *, m, char *, fmt, u32, fmt_size,
 		/* fmt[i] != 0 && fmt[last] == 0, so we can access fmt[i + 1] */
 		i++;
 
-		/* skip optional "[0 +-][num]" width formating field */
+		/* skip optional "[0 +-][num]" width formatting field */
 		while (fmt[i] == '0' || fmt[i] == '+'  || fmt[i] == '-' ||
 		       fmt[i] == ' ')
 			i++;
@@ -736,7 +736,8 @@ BPF_CALL_5(bpf_seq_printf, struct seq_file *, m, char *, fmt, u32, fmt_size,
 		fmt_cnt++;
 	}
 
-	/* Maximumly we can have MAX_SEQ_PRINTF_VARARGS parameter, just give
+	/*
+	 * The maximum we can have is MAX_SEQ_PRINTF_VARARGS parameters, so just give
 	 * all of them to seq_printf().
 	 */
 	seq_printf(m, fmt, params[0], params[1], params[2], params[3],
* Unmerged path kernel/trace/fgraph.c
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 4d18d741e0f0..350f5f1ed5f5 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -1095,7 +1095,7 @@ static struct ftrace_ops global_ops = {
 };
 
 /*
- * Used by the stack undwinder to know about dynamic ftrace trampolines.
+ * Used by the stack unwinder to know about dynamic ftrace trampolines.
  */
 struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
 {
@@ -2966,7 +2966,7 @@ static int ftrace_shutdown(struct ftrace_ops *ops, int command)
 		 * When the kernel is preeptive, tasks can be preempted
 		 * while on a ftrace trampoline. Just scheduling a task on
 		 * a CPU is not good enough to flush them. Calling
-		 * synchornize_rcu_tasks() will wait for those tasks to
+		 * synchronize_rcu_tasks() will wait for those tasks to
 		 * execute and either schedule voluntarily or enter user space.
 		 */
 		if (IS_ENABLED(CONFIG_PREEMPTION))
@@ -5314,7 +5314,7 @@ EXPORT_SYMBOL_GPL(modify_ftrace_direct);
  * @reset - non zero to reset all filters before applying this filter.
  *
  * Filters denote which functions should be enabled when tracing is enabled
- * If @ip is NULL, it failes to update filter.
+ * If @ip is NULL, it fails to update filter.
  */
 int ftrace_set_filter_ip(struct ftrace_ops *ops, unsigned long ip,
 			 int remove, int reset)
@@ -6228,7 +6228,7 @@ clear_mod_from_hash(struct ftrace_page *pg, struct ftrace_hash *hash)
 	}
 }
 
-/* Clear any records from hashs */
+/* Clear any records from hashes */
 static void clear_mod_from_hashes(struct ftrace_page *pg)
 {
 	struct trace_array *tr;
* Unmerged path kernel/trace/ring_buffer.c
* Unmerged path kernel/trace/synth_event_gen_test.c
* Unmerged path kernel/trace/trace.c
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d08237c45b3a..231ccbfbf6b6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1374,7 +1374,7 @@ __event_trigger_test_discard(struct trace_event_file *file,
 
 /**
  * event_trigger_unlock_commit - handle triggers and finish event commit
- * @file: The file pointer assoctiated to the event
+ * @file: The file pointer associated with the event
  * @buffer: The ring buffer that the event is being written to
  * @event: The event meta data in the ring buffer
  * @entry: The event itself
@@ -1402,7 +1402,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
 
 /**
  * event_trigger_unlock_commit_regs - handle triggers and finish event commit
- * @file: The file pointer assoctiated to the event
+ * @file: The file pointer associated with the event
  * @buffer: The ring buffer that the event is being written to
  * @event: The event meta data in the ring buffer
  * @entry: The event itself
diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 3e09a589685b..26b6404d80c0 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -15,7 +15,7 @@ static char __percpu *perf_trace_buf[PERF_NR_CONTEXTS];
 
 /*
  * Force it to be aligned to unsigned long to avoid misaligned accesses
- * suprises
+ * surprises
  */
 typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
 	perf_trace_t;
diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b27fc91c90e4..a9f5aace4d8b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2227,7 +2227,7 @@ void trace_event_eval_update(struct trace_eval_map **map, int len)
 		}
 
 		/*
-		 * Since calls are grouped by systems, the likelyhood that the
+		 * Since calls are grouped by systems, the likelihood that the
 		 * next call in the iteration belongs to the same system as the
 		 * previous call is high. As an optimization, we skip seaching
 		 * for a map[] that matches the call's system if the last call
@@ -2284,7 +2284,7 @@ __trace_add_new_event(struct trace_event_call *call, struct trace_array *tr)
 }
 
 /*
- * Just create a decriptor for early init. A descriptor is required
+ * Just create a descriptor for early init. A descriptor is required
  * for enabling events at boot. We want to enable events before
  * the filesystem is initialized.
  */
diff --git a/kernel/trace/trace_events_filter.c b/kernel/trace/trace_events_filter.c
index 4649bff3e8a4..739f616cea05 100644
--- a/kernel/trace/trace_events_filter.c
+++ b/kernel/trace/trace_events_filter.c
@@ -308,7 +308,7 @@ enum {
  * and "FALSE" the program entry after that, we are now done with the first
  * pass.
  *
- * Making the above "a || b && c" have a progam of:
+ * Making the above "a || b && c" have a program of:
  *  prog[0] = { "a", 1, 2 }
  *  prog[1] = { "b", 0, 2 }
  *  prog[2] = { "c", 0, 3 }
@@ -402,7 +402,7 @@ enum {
  * F: return FALSE
  *
  * As "r = a; if (!r) goto n5;" is obviously the same as
- * "if (!a) goto n5;" without doing anything we can interperate the
+ * "if (!a) goto n5;" without doing anything we can interpret the
  * program as:
  * n1: if (!a) goto n5;
  * n2: if (!b) goto n5;
* Unmerged path kernel/trace/trace_events_synth.c
diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c
index 92d7e3b54bf4..376228138644 100644
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@ -1011,7 +1011,7 @@ print_graph_prologue(struct trace_iterator *iter, struct trace_seq *s,
  *  - we are inside irq code
  *  - we just entered irq code
  *
- * retunns 0 if
+ * returns 0 if
  *  - funcgraph-interrupts option is set
  *  - we are not inside irq code
  */
* Unmerged path kernel/trace/trace_hwlat.c
diff --git a/kernel/trace/trace_kprobe.c b/kernel/trace/trace_kprobe.c
index caa8f0e5fcb6..423a7d38bfd7 100644
--- a/kernel/trace/trace_kprobe.c
+++ b/kernel/trace/trace_kprobe.c
@@ -1392,7 +1392,7 @@ kretprobe_dispatcher(struct kretprobe_instance *ri, struct pt_regs *regs)
 	if (tk->tp.flags & TP_FLAG_PROFILE)
 		kretprobe_perf_func(tk, ri, regs);
 #endif
-	return 0;	/* We don't tweek kernel, so just return 0 */
+	return 0;	/* We don't tweak kernel, so just return 0 */
 }
 NOKPROBE_SYMBOL(kretprobe_dispatcher);
 
* Unmerged path kernel/trace/trace_probe.c
diff --git a/kernel/trace/trace_probe.h b/kernel/trace/trace_probe.h
index 75daff22ccea..60f20fec93ae 100644
--- a/kernel/trace/trace_probe.h
+++ b/kernel/trace/trace_probe.h
@@ -116,7 +116,7 @@ struct fetch_type {
 	size_t			size;		/* Byte size of type */
 	int			is_signed;	/* Signed flag */
 	print_type_func_t	print;		/* Print functions */
-	const char		*fmt;		/* Fromat string */
+	const char		*fmt;		/* Format string */
 	const char		*fmttype;	/* Name in format file */
 	/* Fetch functions */
 	fetch_func_t		fetch[FETCH_MTD_END];
* Unmerged path kernel/trace/trace_probe_tmpl.h
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index 795c9dfb68d1..3c569dd47969 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -867,7 +867,7 @@ trace_selftest_startup_preemptoff(struct tracer *trace, struct trace_array *tr)
 	int ret;
 
 	/*
-	 * Now that the big kernel lock is no longer preemptable,
+	 * Now that the big kernel lock is no longer preemptible,
 	 * and this is called with the BKL held, it will always
 	 * fail. If preemption is already disabled, simply
 	 * pass the test. When the BKL is removed, or becomes
@@ -929,7 +929,7 @@ trace_selftest_startup_preemptirqsoff(struct tracer *trace, struct trace_array *
 	int ret;
 
 	/*
-	 * Now that the big kernel lock is no longer preemptable,
+	 * Now that the big kernel lock is no longer preemptible,
 	 * and this is called with the BKL held, it will always
 	 * fail. If preemption is already disabled, simply
 	 * pass the test. When the BKL is removed, or becomes
diff --git a/kernel/trace/trace_seq.c b/kernel/trace/trace_seq.c
index 2e42cd703a1b..d6fa8fea3dc9 100644
--- a/kernel/trace/trace_seq.c
+++ b/kernel/trace/trace_seq.c
@@ -15,7 +15,7 @@
  * The buffer size is currently PAGE_SIZE, although it may become dynamic
  * in the future.
  *
- * A write to the buffer will either succed or fail. That is, unlike
+ * A write to the buffer will either succeed or fail. That is, unlike
  * sprintf() there will not be a partial write (well it may write into
  * the buffer but it wont update the pointers). This allows users to
  * try to write something into the trace_seq buffer and if it fails
@@ -75,7 +75,7 @@ int trace_print_seq(struct seq_file *m, struct trace_seq *s)
  * @fmt: printf format string
  *
  * The tracer may use either sequence operations or its own
- * copy to user routines. To simplify formating of a trace
+ * copy to user routines. To simplify formatting of a trace
  * trace_seq_printf() is used to store strings into a special
  * buffer (@s). Then the output may be either used by
  * the sequencer or pulled into another buffer.
@@ -135,7 +135,7 @@ EXPORT_SYMBOL_GPL(trace_seq_bitmask);
  * @fmt: printf format string
  *
  * The tracer may use either sequence operations or its own
- * copy to user routines. To simplify formating of a trace
+ * copy to user routines. To simplify formatting of a trace
  * trace_seq_printf is used to store strings into a special
  * buffer (@s). Then the output may be either used by
  * the sequencer or pulled into another buffer.
@@ -228,7 +228,7 @@ EXPORT_SYMBOL_GPL(trace_seq_puts);
  * @c: simple character to record
  *
  * The tracer may use either the sequence operations or its own
- * copy to user routines. This function records a simple charater
+ * copy to user routines. This function records a simple character
  * into a special buffer (@s) for later retrieval by a sequencer
  * or other mechanism.
  */
@@ -350,7 +350,7 @@ int trace_seq_path(struct trace_seq *s, const struct path *path)
 EXPORT_SYMBOL_GPL(trace_seq_path);
 
 /**
- * trace_seq_to_user - copy the squence buffer to user space
+ * trace_seq_to_user - copy the sequence buffer to user space
  * @s: trace sequence descriptor
  * @ubuf: The userspace memory location to copy to
  * @cnt: The amount to copy
@@ -365,7 +365,7 @@ EXPORT_SYMBOL_GPL(trace_seq_path);
  *
  * On failure it returns -EBUSY if all of the content in the
  * sequence has been already read, which includes nothing in the
- * sequenc (@s->len == @s->readpos).
+ * sequence (@s->len == @s->readpos).
  *
  * Returns -EFAULT if the copy to userspace fails.
  */
