mm: mmu_notifier: fix and extend kerneldoc

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Krzysztof Kozlowski <krzk@kernel.org>
commit d49653f35adff8c778e7c5fbd4dbdf929594eca8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/d49653f3.failed

Fix W=1 compile warnings (invalid kerneldoc):

    mm/mmu_notifier.c:187: warning: Function parameter or member 'interval_sub' not described in 'mmu_interval_read_bgin'
    mm/mmu_notifier.c:708: warning: Function parameter or member 'subscription' not described in 'mmu_notifier_registr'
    mm/mmu_notifier.c:708: warning: Excess function parameter 'mn' description in 'mmu_notifier_register'
    mm/mmu_notifier.c:880: warning: Function parameter or member 'subscription' not described in 'mmu_notifier_put'
    mm/mmu_notifier.c:880: warning: Excess function parameter 'mn' description in 'mmu_notifier_put'
    mm/mmu_notifier.c:982: warning: Function parameter or member 'ops' not described in 'mmu_interval_notifier_insert'

	Signed-off-by: Krzysztof Kozlowski <krzk@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
Link: http://lkml.kernel.org/r/20200728171109.28687-4-krzk@kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d49653f35adff8c778e7c5fbd4dbdf929594eca8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mmu_notifier.c
diff --cc mm/mmu_notifier.c
index 4361d699fa34,4fc918163dd3..000000000000
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@@ -166,12 -166,12 +166,16 @@@ static void mn_itree_inv_end(struct mmu
  /**
   * mmu_interval_read_begin - Begin a read side critical section against a VA
   *                           range
++<<<<<<< HEAD
 + * mni: The range to use
++=======
+  * @interval_sub: The interval subscription
++>>>>>>> d49653f35adf (mm: mmu_notifier: fix and extend kerneldoc)
   *
   * mmu_iterval_read_begin()/mmu_iterval_read_retry() implement a
 - * collision-retry scheme similar to seqcount for the VA range under
 - * subscription. If the mm invokes invalidation during the critical section
 - * then mmu_interval_read_retry() will return true.
 + * collision-retry scheme similar to seqcount for the VA range under mni. If
 + * the mm invokes invalidation during the critical section then
 + * mmu_interval_read_retry() will return true.
   *
   * This is useful to obtain shadow PTEs where teardown or setup of the SPTEs
   * require a blocking context.  The critical region formed by this can sleep,
@@@ -855,183 -854,9 +859,183 @@@ static void mmu_notifier_free_rcu(struc
  	mmdrop(mm);
  }
  
 +static int __mmu_interval_notifier_insert(
 +	struct mmu_interval_notifier *mni, struct mm_struct *mm,
 +	struct mmu_notifier_mm *mmn_mm, unsigned long start,
 +	unsigned long length, const struct mmu_interval_notifier_ops *ops)
 +{
 +	mni->mm = mm;
 +	mni->ops = ops;
 +	RB_CLEAR_NODE(&mni->interval_tree.rb);
 +	mni->interval_tree.start = start;
 +	/*
 +	 * Note that the representation of the intervals in the interval tree
 +	 * considers the ending point as contained in the interval.
 +	 */
 +	if (length == 0 ||
 +	    check_add_overflow(start, length - 1, &mni->interval_tree.last))
 +		return -EOVERFLOW;
 +
 +	/* Must call with a mmget() held */
 +	if (WARN_ON(atomic_read(&mm->mm_users) <= 0))
 +		return -EINVAL;
 +
 +	/* pairs with mmdrop in mmu_interval_notifier_remove() */
 +	mmgrab(mm);
 +
 +	/*
 +	 * If some invalidate_range_start/end region is going on in parallel
 +	 * we don't know what VA ranges are affected, so we must assume this
 +	 * new range is included.
 +	 *
 +	 * If the itree is invalidating then we are not allowed to change
 +	 * it. Retrying until invalidation is done is tricky due to the
 +	 * possibility for live lock, instead defer the add to
 +	 * mn_itree_inv_end() so this algorithm is deterministic.
 +	 *
 +	 * In all cases the value for the mni->invalidate_seq should be
 +	 * odd, see mmu_interval_read_begin()
 +	 */
 +	spin_lock(&mmn_mm->lock);
 +	if (mmn_mm->active_invalidate_ranges) {
 +		if (mn_itree_is_invalidating(mmn_mm))
 +			hlist_add_head(&mni->deferred_item,
 +				       &mmn_mm->deferred_list);
 +		else {
 +			mmn_mm->invalidate_seq |= 1;
 +			interval_tree_insert(&mni->interval_tree,
 +					     &mmn_mm->itree);
 +		}
 +		mni->invalidate_seq = mmn_mm->invalidate_seq;
 +	} else {
 +		WARN_ON(mn_itree_is_invalidating(mmn_mm));
 +		/*
 +		 * The starting seq for a mni not under invalidation should be
 +		 * odd, not equal to the current invalidate_seq and
 +		 * invalidate_seq should not 'wrap' to the new seq any time
 +		 * soon.
 +		 */
 +		mni->invalidate_seq = mmn_mm->invalidate_seq - 1;
 +		interval_tree_insert(&mni->interval_tree, &mmn_mm->itree);
 +	}
 +	spin_unlock(&mmn_mm->lock);
 +	return 0;
 +}
 +
 +/**
 + * mmu_interval_notifier_insert - Insert an interval notifier
 + * @mni: Interval notifier to register
 + * @start: Starting virtual address to monitor
 + * @length: Length of the range to monitor
 + * @mm : mm_struct to attach to
 + *
 + * This function subscribes the interval notifier for notifications from the
 + * mm.  Upon return the ops related to mmu_interval_notifier will be called
 + * whenever an event that intersects with the given range occurs.
 + *
 + * Upon return the range_notifier may not be present in the interval tree yet.
 + * The caller must use the normal interval notifier read flow via
 + * mmu_interval_read_begin() to establish SPTEs for this range.
 + */
 +int mmu_interval_notifier_insert(struct mmu_interval_notifier *mni,
 +				 struct mm_struct *mm, unsigned long start,
 +				 unsigned long length,
 +				 const struct mmu_interval_notifier_ops *ops)
 +{
 +	struct mmu_notifier_mm *mmn_mm;
 +	int ret;
 +
 +	might_lock(&mm->mmap_lock);
 +
 +	mmn_mm = smp_load_acquire(&mm->mmu_notifier_mm);
 +	if (!mmn_mm || !mmn_mm->has_itree) {
 +		ret = mmu_notifier_register(NULL, mm);
 +		if (ret)
 +			return ret;
 +		mmn_mm = mm->mmu_notifier_mm;
 +	}
 +	return __mmu_interval_notifier_insert(mni, mm, mmn_mm, start, length,
 +					      ops);
 +}
 +EXPORT_SYMBOL_GPL(mmu_interval_notifier_insert);
 +
 +int mmu_interval_notifier_insert_locked(
 +	struct mmu_interval_notifier *mni, struct mm_struct *mm,
 +	unsigned long start, unsigned long length,
 +	const struct mmu_interval_notifier_ops *ops)
 +{
 +	struct mmu_notifier_mm *mmn_mm;
 +	int ret;
 +
 +	mmap_assert_write_locked(mm);
 +
 +	mmn_mm = mm->mmu_notifier_mm;
 +	if (!mmn_mm || !mmn_mm->has_itree) {
 +		ret = __mmu_notifier_register(NULL, mm);
 +		if (ret)
 +			return ret;
 +		mmn_mm = mm->mmu_notifier_mm;
 +	}
 +	return __mmu_interval_notifier_insert(mni, mm, mmn_mm, start, length,
 +					      ops);
 +}
 +EXPORT_SYMBOL_GPL(mmu_interval_notifier_insert_locked);
 +
 +/**
 + * mmu_interval_notifier_remove - Remove a interval notifier
 + * @mni: Interval notifier to unregister
 + *
 + * This function must be paired with mmu_interval_notifier_insert(). It cannot
 + * be called from any ops callback.
 + *
 + * Once this returns ops callbacks are no longer running on other CPUs and
 + * will not be called in future.
 + */
 +void mmu_interval_notifier_remove(struct mmu_interval_notifier *mni)
 +{
 +	struct mm_struct *mm = mni->mm;
 +	struct mmu_notifier_mm *mmn_mm = mm->mmu_notifier_mm;
 +	unsigned long seq = 0;
 +
 +	might_sleep();
 +
 +	spin_lock(&mmn_mm->lock);
 +	if (mn_itree_is_invalidating(mmn_mm)) {
 +		/*
 +		 * remove is being called after insert put this on the
 +		 * deferred list, but before the deferred list was processed.
 +		 */
 +		if (RB_EMPTY_NODE(&mni->interval_tree.rb)) {
 +			hlist_del(&mni->deferred_item);
 +		} else {
 +			hlist_add_head(&mni->deferred_item,
 +				       &mmn_mm->deferred_list);
 +			seq = mmn_mm->invalidate_seq;
 +		}
 +	} else {
 +		WARN_ON(RB_EMPTY_NODE(&mni->interval_tree.rb));
 +		interval_tree_remove(&mni->interval_tree, &mmn_mm->itree);
 +	}
 +	spin_unlock(&mmn_mm->lock);
 +
 +	/*
 +	 * The possible sleep on progress in the invalidation requires the
 +	 * caller not hold any locks held by invalidation callbacks.
 +	 */
 +	lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);
 +	lock_map_release(&__mmu_notifier_invalidate_range_start_map);
 +	if (seq)
 +		wait_event(mmn_mm->wq,
 +			   READ_ONCE(mmn_mm->invalidate_seq) != seq);
 +
 +	/* pairs with mmgrab in mmu_interval_notifier_insert() */
 +	mmdrop(mm);
 +}
 +EXPORT_SYMBOL_GPL(mmu_interval_notifier_remove);
 +
  /**
   * mmu_notifier_put - Release the reference on the notifier
-  * @mn: The notifier to act on
+  * @subscription: The notifier to act on
   *
   * This function must be paired with each mmu_notifier_get(), it releases the
   * reference obtained by the get. If this is the last reference then process
@@@ -1069,9 -890,190 +1073,193 @@@ void mmu_notifier_put(struct mmu_notifi
  	return;
  
  out_unlock:
 -	spin_unlock(&mm->notifier_subscriptions->lock);
 +	spin_unlock(&mm->mmu_notifier_mm->lock);
  }
  EXPORT_SYMBOL_GPL(mmu_notifier_put);
++<<<<<<< HEAD
++=======
+ 
+ static int __mmu_interval_notifier_insert(
+ 	struct mmu_interval_notifier *interval_sub, struct mm_struct *mm,
+ 	struct mmu_notifier_subscriptions *subscriptions, unsigned long start,
+ 	unsigned long length, const struct mmu_interval_notifier_ops *ops)
+ {
+ 	interval_sub->mm = mm;
+ 	interval_sub->ops = ops;
+ 	RB_CLEAR_NODE(&interval_sub->interval_tree.rb);
+ 	interval_sub->interval_tree.start = start;
+ 	/*
+ 	 * Note that the representation of the intervals in the interval tree
+ 	 * considers the ending point as contained in the interval.
+ 	 */
+ 	if (length == 0 ||
+ 	    check_add_overflow(start, length - 1,
+ 			       &interval_sub->interval_tree.last))
+ 		return -EOVERFLOW;
+ 
+ 	/* Must call with a mmget() held */
+ 	if (WARN_ON(atomic_read(&mm->mm_count) <= 0))
+ 		return -EINVAL;
+ 
+ 	/* pairs with mmdrop in mmu_interval_notifier_remove() */
+ 	mmgrab(mm);
+ 
+ 	/*
+ 	 * If some invalidate_range_start/end region is going on in parallel
+ 	 * we don't know what VA ranges are affected, so we must assume this
+ 	 * new range is included.
+ 	 *
+ 	 * If the itree is invalidating then we are not allowed to change
+ 	 * it. Retrying until invalidation is done is tricky due to the
+ 	 * possibility for live lock, instead defer the add to
+ 	 * mn_itree_inv_end() so this algorithm is deterministic.
+ 	 *
+ 	 * In all cases the value for the interval_sub->invalidate_seq should be
+ 	 * odd, see mmu_interval_read_begin()
+ 	 */
+ 	spin_lock(&subscriptions->lock);
+ 	if (subscriptions->active_invalidate_ranges) {
+ 		if (mn_itree_is_invalidating(subscriptions))
+ 			hlist_add_head(&interval_sub->deferred_item,
+ 				       &subscriptions->deferred_list);
+ 		else {
+ 			subscriptions->invalidate_seq |= 1;
+ 			interval_tree_insert(&interval_sub->interval_tree,
+ 					     &subscriptions->itree);
+ 		}
+ 		interval_sub->invalidate_seq = subscriptions->invalidate_seq;
+ 	} else {
+ 		WARN_ON(mn_itree_is_invalidating(subscriptions));
+ 		/*
+ 		 * The starting seq for a subscription not under invalidation
+ 		 * should be odd, not equal to the current invalidate_seq and
+ 		 * invalidate_seq should not 'wrap' to the new seq any time
+ 		 * soon.
+ 		 */
+ 		interval_sub->invalidate_seq =
+ 			subscriptions->invalidate_seq - 1;
+ 		interval_tree_insert(&interval_sub->interval_tree,
+ 				     &subscriptions->itree);
+ 	}
+ 	spin_unlock(&subscriptions->lock);
+ 	return 0;
+ }
+ 
+ /**
+  * mmu_interval_notifier_insert - Insert an interval notifier
+  * @interval_sub: Interval subscription to register
+  * @start: Starting virtual address to monitor
+  * @length: Length of the range to monitor
+  * @mm: mm_struct to attach to
+  * @ops: Interval notifier operations to be called on matching events
+  *
+  * This function subscribes the interval notifier for notifications from the
+  * mm.  Upon return the ops related to mmu_interval_notifier will be called
+  * whenever an event that intersects with the given range occurs.
+  *
+  * Upon return the range_notifier may not be present in the interval tree yet.
+  * The caller must use the normal interval notifier read flow via
+  * mmu_interval_read_begin() to establish SPTEs for this range.
+  */
+ int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,
+ 				 struct mm_struct *mm, unsigned long start,
+ 				 unsigned long length,
+ 				 const struct mmu_interval_notifier_ops *ops)
+ {
+ 	struct mmu_notifier_subscriptions *subscriptions;
+ 	int ret;
+ 
+ 	might_lock(&mm->mmap_lock);
+ 
+ 	subscriptions = smp_load_acquire(&mm->notifier_subscriptions);
+ 	if (!subscriptions || !subscriptions->has_itree) {
+ 		ret = mmu_notifier_register(NULL, mm);
+ 		if (ret)
+ 			return ret;
+ 		subscriptions = mm->notifier_subscriptions;
+ 	}
+ 	return __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,
+ 					      start, length, ops);
+ }
+ EXPORT_SYMBOL_GPL(mmu_interval_notifier_insert);
+ 
+ int mmu_interval_notifier_insert_locked(
+ 	struct mmu_interval_notifier *interval_sub, struct mm_struct *mm,
+ 	unsigned long start, unsigned long length,
+ 	const struct mmu_interval_notifier_ops *ops)
+ {
+ 	struct mmu_notifier_subscriptions *subscriptions =
+ 		mm->notifier_subscriptions;
+ 	int ret;
+ 
+ 	mmap_assert_write_locked(mm);
+ 
+ 	if (!subscriptions || !subscriptions->has_itree) {
+ 		ret = __mmu_notifier_register(NULL, mm);
+ 		if (ret)
+ 			return ret;
+ 		subscriptions = mm->notifier_subscriptions;
+ 	}
+ 	return __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,
+ 					      start, length, ops);
+ }
+ EXPORT_SYMBOL_GPL(mmu_interval_notifier_insert_locked);
+ 
+ /**
+  * mmu_interval_notifier_remove - Remove a interval notifier
+  * @interval_sub: Interval subscription to unregister
+  *
+  * This function must be paired with mmu_interval_notifier_insert(). It cannot
+  * be called from any ops callback.
+  *
+  * Once this returns ops callbacks are no longer running on other CPUs and
+  * will not be called in future.
+  */
+ void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub)
+ {
+ 	struct mm_struct *mm = interval_sub->mm;
+ 	struct mmu_notifier_subscriptions *subscriptions =
+ 		mm->notifier_subscriptions;
+ 	unsigned long seq = 0;
+ 
+ 	might_sleep();
+ 
+ 	spin_lock(&subscriptions->lock);
+ 	if (mn_itree_is_invalidating(subscriptions)) {
+ 		/*
+ 		 * remove is being called after insert put this on the
+ 		 * deferred list, but before the deferred list was processed.
+ 		 */
+ 		if (RB_EMPTY_NODE(&interval_sub->interval_tree.rb)) {
+ 			hlist_del(&interval_sub->deferred_item);
+ 		} else {
+ 			hlist_add_head(&interval_sub->deferred_item,
+ 				       &subscriptions->deferred_list);
+ 			seq = subscriptions->invalidate_seq;
+ 		}
+ 	} else {
+ 		WARN_ON(RB_EMPTY_NODE(&interval_sub->interval_tree.rb));
+ 		interval_tree_remove(&interval_sub->interval_tree,
+ 				     &subscriptions->itree);
+ 	}
+ 	spin_unlock(&subscriptions->lock);
+ 
+ 	/*
+ 	 * The possible sleep on progress in the invalidation requires the
+ 	 * caller not hold any locks held by invalidation callbacks.
+ 	 */
+ 	lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);
+ 	lock_map_release(&__mmu_notifier_invalidate_range_start_map);
+ 	if (seq)
+ 		wait_event(subscriptions->wq,
+ 			   READ_ONCE(subscriptions->invalidate_seq) != seq);
+ 
+ 	/* pairs with mmgrab in mmu_interval_notifier_insert() */
+ 	mmdrop(mm);
+ }
+ EXPORT_SYMBOL_GPL(mmu_interval_notifier_remove);
+ 
++>>>>>>> d49653f35adf (mm: mmu_notifier: fix and extend kerneldoc)
  /**
   * mmu_notifier_synchronize - Ensure all mmu_notifiers are freed
   *
* Unmerged path mm/mmu_notifier.c
