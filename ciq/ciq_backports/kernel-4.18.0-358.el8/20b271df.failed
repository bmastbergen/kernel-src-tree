arch/kmap: define kmap_atomic_prot() for all arch's

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ira Weiny <ira.weiny@intel.com>
commit 20b271dfe9d932b02b067a1f7ba9805c5b8d79bd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/20b271df.failed

To support kmap_atomic_prot(), all architectures need to support
protections passed to their kmap_atomic_high() function.  Pass protections
into kmap_atomic_high() and change the name to kmap_atomic_high_prot() to
match.

Then define kmap_atomic_prot() as a core function which calls
kmap_atomic_high_prot() when needed.

Finally, redefine kmap_atomic() as a wrapper of kmap_atomic_prot() with
the default kmap_prot exported by the architectures.

	Signed-off-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Christian KÃ¶nig <christian.koenig@amd.com>
	Cc: Chris Zankel <chris@zankel.net>
	Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Helge Deller <deller@gmx.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20200507150004.1423069-11-ira.weiny@intel.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 20b271dfe9d932b02b067a1f7ba9805c5b8d79bd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/mm/highmem.c
#	arch/arm/mm/highmem.c
#	arch/csky/mm/highmem.c
#	arch/microblaze/include/asm/highmem.h
#	arch/mips/mm/highmem.c
#	arch/nds32/mm/highmem.c
#	arch/powerpc/include/asm/highmem.h
#	arch/sparc/mm/highmem.c
#	arch/x86/include/asm/highmem.h
#	arch/xtensa/mm/highmem.c
#	include/linux/highmem.h
diff --cc arch/arc/mm/highmem.c
index 48e700151810,479b0d72d3cf..000000000000
--- a/arch/arc/mm/highmem.c
+++ b/arch/arc/mm/highmem.c
@@@ -53,17 -49,7 +53,21 @@@
  extern pte_t * pkmap_page_table;
  static pte_t * fixmap_page_table;
  
++<<<<<<< HEAD
 +void *kmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +
 +	return kmap_high(page);
 +}
 +EXPORT_SYMBOL(kmap);
 +
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	int idx, cpu_idx;
  	unsigned long vaddr;
@@@ -82,9 -63,9 +86,13 @@@
  
  	return (void *)vaddr;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(kmap_atomic);
++=======
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  
 -void kunmap_atomic_high(void *kv)
 +void __kunmap_atomic(void *kv)
  {
  	unsigned long kvaddr = (unsigned long)kv;
  
diff --cc arch/arm/mm/highmem.c
index d02f8187b1cc,e013f6b81328..000000000000
--- a/arch/arm/mm/highmem.c
+++ b/arch/arm/mm/highmem.c
@@@ -34,25 -31,7 +34,29 @@@ static inline pte_t get_fixmap_pte(unsi
  	return *ptep;
  }
  
++<<<<<<< HEAD
 +void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +EXPORT_SYMBOL(kmap);
 +
 +void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +EXPORT_SYMBOL(kunmap);
 +
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	unsigned int idx;
  	unsigned long vaddr;
@@@ -97,9 -71,9 +101,13 @@@
  
  	return (void *)vaddr;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(kmap_atomic);
++=======
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  
 -void kunmap_atomic_high(void *kvaddr)
 +void __kunmap_atomic(void *kvaddr)
  {
  	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
  	int idx, type;
diff --cc arch/microblaze/include/asm/highmem.h
index 332c78e15198,d7c55cfd27bd..000000000000
--- a/arch/microblaze/include/asm/highmem.h
+++ b/arch/microblaze/include/asm/highmem.h
@@@ -51,32 -51,6 +51,35 @@@ extern pte_t *pkmap_page_table
  #define PKMAP_NR(virt)  ((virt - PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +extern void *kmap_atomic_prot(struct page *page, pgprot_t prot);
 +extern void __kunmap_atomic(void *kvaddr);
 +
 +static inline void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +
 +static inline void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +
 +static inline void *kmap_atomic(struct page *page)
 +{
 +	return kmap_atomic_prot(page, kmap_prot);
 +}
 +
++=======
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  #define flush_cache_kmaps()	{ flush_icache(); flush_dcache(); }
  
  #endif /* __KERNEL__ */
diff --cc arch/mips/mm/highmem.c
index d08e6d7d533b,8e8726992720..000000000000
--- a/arch/mips/mm/highmem.c
+++ b/arch/mips/mm/highmem.c
@@@ -12,39 -12,13 +12,43 @@@ static pte_t *kmap_pte
  
  unsigned long highstart_pfn, highend_pfn;
  
 -void kmap_flush_tlb(unsigned long addr)
 +void *kmap(struct page *page)
  {
 -	flush_tlb_one(addr);
 +	void *addr;
 +
++<<<<<<< HEAD
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	addr = kmap_high(page);
 +	flush_tlb_one((unsigned long)addr);
 +
 +	return addr;
  }
 -EXPORT_SYMBOL(kmap_flush_tlb);
 +EXPORT_SYMBOL(kmap);
  
 +void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +EXPORT_SYMBOL(kunmap);
 +
 +/*
 + * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
 + * no global lock is needed and because the kmap code must perform a global TLB
 + * invalidation when the kmap pool wraps.
 + *
 + * However when holding an atomic kmap is is not legal to sleep, so atomic
 + * kmaps are appropriate for short, tight code paths only.
 + */
 +
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	unsigned long vaddr;
  	int idx, type;
@@@ -60,14 -29,14 +64,22 @@@
  #ifdef CONFIG_DEBUG_HIGHMEM
  	BUG_ON(!pte_none(*(kmap_pte - idx)));
  #endif
++<<<<<<< HEAD
 +	set_pte(kmap_pte-idx, mk_pte(page, PAGE_KERNEL));
++=======
+ 	set_pte(kmap_pte-idx, mk_pte(page, prot));
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  	local_flush_tlb_one((unsigned long)vaddr);
  
  	return (void*) vaddr;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(kmap_atomic);
++=======
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  
 -void kunmap_atomic_high(void *kvaddr)
 +void __kunmap_atomic(void *kvaddr)
  {
  	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
  	int type __maybe_unused;
diff --cc arch/nds32/mm/highmem.c
index 022779af6148,4284cd59e21a..000000000000
--- a/arch/nds32/mm/highmem.c
+++ b/arch/nds32/mm/highmem.c
@@@ -10,29 -10,7 +10,33 @@@
  #include <asm/fixmap.h>
  #include <asm/tlbflush.h>
  
++<<<<<<< HEAD
 +void *kmap(struct page *page)
 +{
 +	unsigned long vaddr;
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	vaddr = (unsigned long)kmap_high(page);
 +	return (void *)vaddr;
 +}
 +
 +EXPORT_SYMBOL(kmap);
 +
 +void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +
 +EXPORT_SYMBOL(kunmap);
 +
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	unsigned int idx;
  	unsigned long vaddr, pte;
@@@ -48,7 -21,7 +52,11 @@@
  
  	idx = type + KM_TYPE_NR * smp_processor_id();
  	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
++<<<<<<< HEAD
 +	pte = (page_to_pfn(page) << PAGE_SHIFT) | (PAGE_KERNEL);
++=======
+ 	pte = (page_to_pfn(page) << PAGE_SHIFT) | prot;
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  	ptep = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
  	set_pte(ptep, pte);
  
@@@ -58,10 -31,9 +66,14 @@@
  	__nds32__isb();
  	return (void *)vaddr;
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
 +
 +EXPORT_SYMBOL(kmap_atomic);
  
 -void kunmap_atomic_high(void *kvaddr)
 +void __kunmap_atomic(void *kvaddr)
  {
  	if (kvaddr >= (void *)FIXADDR_START) {
  		unsigned long vaddr = (unsigned long)kvaddr;
diff --cc arch/powerpc/include/asm/highmem.h
index cec820f961da,8d8ee3fcd800..000000000000
--- a/arch/powerpc/include/asm/highmem.h
+++ b/arch/powerpc/include/asm/highmem.h
@@@ -59,33 -59,6 +59,36 @@@ extern pte_t *pkmap_page_table
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +extern void *kmap_atomic_prot(struct page *page, pgprot_t prot);
 +extern void __kunmap_atomic(void *kvaddr);
 +
 +static inline void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +
 +static inline void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +
 +static inline void *kmap_atomic(struct page *page)
 +{
 +	return kmap_atomic_prot(page, kmap_prot);
 +}
 +
 +
++=======
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  #define flush_cache_kmaps()	flush_cache_all()
  
  #endif /* __KERNEL__ */
diff --cc arch/sparc/mm/highmem.c
index 86bc2a58d26c,9309bcab4ae6..000000000000
--- a/arch/sparc/mm/highmem.c
+++ b/arch/sparc/mm/highmem.c
@@@ -49,7 -54,7 +49,11 @@@ void __init kmap_init(void
          kmap_prot = __pgprot(SRMMU_ET_PTE | SRMMU_PRIV | SRMMU_CACHE);
  }
  
++<<<<<<< HEAD
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	unsigned long vaddr;
  	long idx, type;
@@@ -83,9 -83,9 +87,13 @@@
  
  	return (void*) vaddr;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(kmap_atomic);
++=======
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  
 -void kunmap_atomic_high(void *kvaddr)
 +void __kunmap_atomic(void *kvaddr)
  {
  	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
  	int type;
diff --cc arch/x86/include/asm/highmem.h
index a8059930056d,0f420b24e0fc..000000000000
--- a/arch/x86/include/asm/highmem.h
+++ b/arch/x86/include/asm/highmem.h
@@@ -58,15 -58,6 +58,18 @@@ extern unsigned long highstart_pfn, hig
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +
 +void *kmap(struct page *page);
 +void kunmap(struct page *page);
 +
 +void *kmap_atomic_prot(struct page *page, pgprot_t prot);
 +void *kmap_atomic(struct page *page);
 +void __kunmap_atomic(void *kvaddr);
++=======
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  void *kmap_atomic_pfn(unsigned long pfn);
  void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
  
diff --cc arch/xtensa/mm/highmem.c
index 184ceadccc1a,99b5ad137ab5..000000000000
--- a/arch/xtensa/mm/highmem.c
+++ b/arch/xtensa/mm/highmem.c
@@@ -37,7 -37,7 +37,11 @@@ static inline enum fixed_addresses kmap
  		color;
  }
  
++<<<<<<< HEAD
 +void *kmap_atomic(struct page *page)
++=======
+ void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  {
  	enum fixed_addresses idx;
  	unsigned long vaddr;
@@@ -53,13 -48,13 +57,21 @@@
  #ifdef CONFIG_DEBUG_HIGHMEM
  	BUG_ON(!pte_none(*(kmap_pte + idx)));
  #endif
++<<<<<<< HEAD
 +	set_pte(kmap_pte + idx, mk_pte(page, PAGE_KERNEL_EXEC));
 +
 +	return (void *)vaddr;
 +}
 +EXPORT_SYMBOL(kmap_atomic);
++=======
+ 	set_pte(kmap_pte + idx, mk_pte(page, prot));
+ 
+ 	return (void *)vaddr;
+ }
+ EXPORT_SYMBOL(kmap_atomic_high_prot);
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  
 -void kunmap_atomic_high(void *kvaddr)
 +void __kunmap_atomic(void *kvaddr)
  {
  	if (kvaddr >= (void *)FIXADDR_START &&
  	    kvaddr < (void *)FIXADDR_TOP) {
diff --cc include/linux/highmem.h
index ea5cdbd8c2c3,9c559c670299..000000000000
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@@ -32,8 -32,61 +32,66 @@@ static inline void invalidate_kernel_vm
  #include <asm/kmap_types.h>
  
  #ifdef CONFIG_HIGHMEM
++<<<<<<< HEAD
 +#include <asm/highmem.h>
 +
++=======
+ extern void *kmap_atomic_high_prot(struct page *page, pgprot_t prot);
+ extern void kunmap_atomic_high(void *kvaddr);
+ #include <asm/highmem.h>
+ 
+ #ifndef ARCH_HAS_KMAP_FLUSH_TLB
+ static inline void kmap_flush_tlb(unsigned long addr) { }
+ #endif
+ 
+ void *kmap_high(struct page *page);
+ static inline void *kmap(struct page *page)
+ {
+ 	void *addr;
+ 
+ 	might_sleep();
+ 	if (!PageHighMem(page))
+ 		addr = page_address(page);
+ 	else
+ 		addr = kmap_high(page);
+ 	kmap_flush_tlb((unsigned long)addr);
+ 	return addr;
+ }
+ 
+ void kunmap_high(struct page *page);
+ 
+ static inline void kunmap(struct page *page)
+ {
+ 	might_sleep();
+ 	if (!PageHighMem(page))
+ 		return;
+ 	kunmap_high(page);
+ }
+ 
+ /*
+  * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
+  * no global lock is needed and because the kmap code must perform a global TLB
+  * invalidation when the kmap pool wraps.
+  *
+  * However when holding an atomic kmap is is not legal to sleep, so atomic
+  * kmaps are appropriate for short, tight code paths only.
+  *
+  * The use of kmap_atomic/kunmap_atomic is discouraged - kmap/kunmap
+  * gives a more generic (and caching) interface. But kmap_atomic can
+  * be used in IRQ contexts, so in some (very limited) cases we need
+  * it.
+  */
+ static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ {
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 	return kmap_atomic_high_prot(page, prot);
+ }
+ #define kmap_atomic(page)	kmap_atomic_prot(page, kmap_prot)
+ 
++>>>>>>> 20b271dfe9d9 (arch/kmap: define kmap_atomic_prot() for all arch's)
  /* declarations for linux/mm/highmem.c */
  unsigned int nr_free_highpages(void);
  extern atomic_long_t _totalhigh_pages;
* Unmerged path arch/csky/mm/highmem.c
* Unmerged path arch/arc/mm/highmem.c
* Unmerged path arch/arm/mm/highmem.c
* Unmerged path arch/csky/mm/highmem.c
* Unmerged path arch/microblaze/include/asm/highmem.h
* Unmerged path arch/mips/mm/highmem.c
* Unmerged path arch/nds32/mm/highmem.c
* Unmerged path arch/powerpc/include/asm/highmem.h
* Unmerged path arch/sparc/mm/highmem.c
* Unmerged path arch/x86/include/asm/highmem.h
* Unmerged path arch/xtensa/mm/highmem.c
* Unmerged path include/linux/highmem.h
