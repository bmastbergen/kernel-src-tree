bpf: Stop caching subprog index in the bpf_pseudo_func insn

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Martin KaFai Lau <kafai@fb.com>
commit 3990ed4c426652fcd469f8c9dc08156294b36c28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/3990ed4c.failed

This patch is to fix an out-of-bound access issue when jit-ing the
bpf_pseudo_func insn (i.e. ld_imm64 with src_reg == BPF_PSEUDO_FUNC)

In jit_subprog(), it currently reuses the subprog index cached in
insn[1].imm.  This subprog index is an index into a few array related
to subprogs.  For example, in jit_subprog(), it is an index to the newly
allocated 'struct bpf_prog **func' array.

The subprog index was cached in insn[1].imm after add_subprog().  However,
this could become outdated (and too big in this case) if some subprogs
are completely removed during dead code elimination (in
adjust_subprog_starts_after_remove).  The cached index in insn[1].imm
is not updated accordingly and causing out-of-bound issue in the later
jit_subprog().

Unlike bpf_pseudo_'func' insn, the current bpf_pseudo_'call' insn
is handling the DCE properly by calling find_subprog(insn->imm) to
figure out the index instead of caching the subprog index.
The existing bpf_adj_branches() will adjust the insn->imm
whenever insn is added or removed.

Instead of having two ways handling subprog index,
this patch is to make bpf_pseudo_func works more like
bpf_pseudo_call.

First change is to stop caching the subprog index result
in insn[1].imm after add_subprog().  The verification
process will use find_subprog(insn->imm) to figure
out the subprog index.

Second change is in bpf_adj_branches() and have it to
adjust the insn->imm for the bpf_pseudo_func insn also
whenever insn is added or removed.

Third change is in jit_subprog().  Like the bpf_pseudo_call handling,
bpf_pseudo_func temporarily stores the find_subprog() result
in insn->off.  It is fine because the prog's insn has been finalized
at this point.  insn->off will be reset back to 0 later to avoid
confusing the userspace prog dump tool.

Fixes: 69c087ba6225 ("bpf: Add bpf_for_each_map_elem() helper")
	Signed-off-by: Martin KaFai Lau <kafai@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20211106014014.651018-1-kafai@fb.com
(cherry picked from commit 3990ed4c426652fcd469f8c9dc08156294b36c28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/verifier.c
diff --cc kernel/bpf/verifier.c
index 44c3b39c5d13,890b3ec375a3..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -242,6 -234,12 +242,15 @@@ static bool bpf_pseudo_call(const struc
  	       insn->src_reg == BPF_PSEUDO_CALL;
  }
  
++<<<<<<< HEAD
++=======
+ static bool bpf_pseudo_kfunc_call(const struct bpf_insn *insn)
+ {
+ 	return insn->code == (BPF_JMP | BPF_CALL) &&
+ 	       insn->src_reg == BPF_PSEUDO_KFUNC_CALL;
+ }
+ 
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  struct bpf_call_arg_meta {
  	struct bpf_map *map_ptr;
  	bool raw_mode;
@@@ -1563,19 -1941,24 +1572,28 @@@ static int check_subprogs(struct bpf_ve
  
  	/* Add entry function. */
  	ret = add_subprog(env, 0);
 -	if (ret)
 +	if (ret < 0)
  		return ret;
  
 -	for (i = 0; i < insn_cnt; i++, insn++) {
 -		if (!bpf_pseudo_func(insn) && !bpf_pseudo_call(insn) &&
 -		    !bpf_pseudo_kfunc_call(insn))
 +	/* determine subprog starts. The end is one before the next starts */
 +	for (i = 0; i < insn_cnt; i++) {
 +		if (!bpf_pseudo_call(insn + i))
  			continue;
 -
  		if (!env->bpf_capable) {
 -			verbose(env, "loading/calling other bpf or kernel functions are allowed for CAP_BPF and CAP_SYS_ADMIN\n");
 +			verbose(env,
 +				"function calls to other bpf functions are allowed for CAP_BPF and CAP_SYS_ADMIN\n");
  			return -EPERM;
  		}
++<<<<<<< HEAD
 +		ret = add_subprog(env, i + insn[i].imm + 1);
++=======
+ 
+ 		if (bpf_pseudo_func(insn) || bpf_pseudo_call(insn))
+ 			ret = add_subprog(env, i + insn->imm + 1);
+ 		else
+ 			ret = add_kfunc_call(env, insn->imm, insn->off);
+ 
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  		if (ret < 0)
  			return ret;
  	}
@@@ -8454,6 -9373,25 +8472,28 @@@ static int check_ld_imm(struct bpf_veri
  		return 0;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (insn->src_reg == BPF_PSEUDO_FUNC) {
+ 		struct bpf_prog_aux *aux = env->prog->aux;
+ 		u32 subprogno = find_subprog(env,
+ 					     env->insn_idx + insn->imm + 1);
+ 
+ 		if (!aux->func_info) {
+ 			verbose(env, "missing btf func_info\n");
+ 			return -EINVAL;
+ 		}
+ 		if (aux->func_info_aux[subprogno].linkage != BTF_FUNC_STATIC) {
+ 			verbose(env, "callback function not static\n");
+ 			return -EINVAL;
+ 		}
+ 
+ 		dst_reg->type = PTR_TO_FUNC;
+ 		dst_reg->subprogno = subprogno;
+ 		return 0;
+ 	}
+ 
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  	map = env->used_maps[aux->map_index];
  	mark_reg_known_zero(env, regs, insn->dst_reg);
  	dst_reg->map_ptr = map;
@@@ -11505,8 -12546,9 +11545,14 @@@ static int jit_subprogs(struct bpf_veri
  		return 0;
  
  	for (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {
++<<<<<<< HEAD
 +		if (!bpf_pseudo_call(insn))
 +			continue;
++=======
+ 		if (!bpf_pseudo_func(insn) && !bpf_pseudo_call(insn))
+ 			continue;
+ 
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  		/* Upon error here we cannot fall back to interpreter but
  		 * need a hard reject of the program. Thus -EFAULT is
  		 * propagated in any case.
@@@ -11608,6 -12658,12 +11660,15 @@@
  	for (i = 0; i < env->subprog_cnt; i++) {
  		insn = func[i]->insnsi;
  		for (j = 0; j < func[i]->len; j++, insn++) {
++<<<<<<< HEAD
++=======
+ 			if (bpf_pseudo_func(insn)) {
+ 				subprog = insn->off;
+ 				insn[0].imm = (u32)(long)func[subprog]->bpf_func;
+ 				insn[1].imm = ((u64)(long)func[subprog]->bpf_func) >> 32;
+ 				continue;
+ 			}
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  			if (!bpf_pseudo_call(insn))
  				continue;
  			subprog = insn->off;
@@@ -11653,6 -12708,12 +11714,15 @@@
  	 * later look the same as if they were interpreted only.
  	 */
  	for (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {
++<<<<<<< HEAD
++=======
+ 		if (bpf_pseudo_func(insn)) {
+ 			insn[0].imm = env->insn_aux_data[i].call_imm;
+ 			insn[1].imm = insn->off;
+ 			insn->off = 0;
+ 			continue;
+ 		}
++>>>>>>> 3990ed4c4266 (bpf: Stop caching subprog index in the bpf_pseudo_func insn)
  		if (!bpf_pseudo_call(insn))
  			continue;
  		insn->off = env->insn_aux_data[i].call_imm;
diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index 95b950412549..d0f17a2e9407 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -447,6 +447,12 @@ bpf_ctx_record_field_size(struct bpf_insn_access_aux *aux, u32 size)
 	aux->ctx_field_size = size;
 }
 
+static inline bool bpf_pseudo_func(const struct bpf_insn *insn)
+{
+	return insn->code == (BPF_LD | BPF_IMM | BPF_DW) &&
+	       insn->src_reg == BPF_PSEUDO_FUNC;
+}
+
 struct bpf_prog_ops {
 	int (*test_run)(struct bpf_prog *prog, const union bpf_attr *kattr,
 			union bpf_attr __user *uattr);
diff --git a/kernel/bpf/core.c b/kernel/bpf/core.c
index b1c9ad42b7c4..cfc864bb76bd 100644
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@ -382,6 +382,13 @@ static int bpf_adj_branches(struct bpf_prog *prog, u32 pos, s32 end_old,
 			i = end_new;
 			insn = prog->insnsi + end_old;
 		}
+		if (bpf_pseudo_func(insn)) {
+			ret = bpf_adj_delta_to_imm(insn, pos, end_old,
+						   end_new, i, probe_pass);
+			if (ret)
+				return ret;
+			continue;
+		}
 		code = insn->code;
 		if ((BPF_CLASS(code) != BPF_JMP &&
 		     BPF_CLASS(code) != BPF_JMP32) ||
* Unmerged path kernel/bpf/verifier.c
