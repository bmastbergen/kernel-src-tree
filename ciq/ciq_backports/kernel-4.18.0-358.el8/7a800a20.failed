block: use bi_max_vecs to find the bvec pool

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 7a800a20ae6329e803c5c646b20811a6ae9ca136
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/7a800a20.failed

Instead of encoding of the bvec pool using magic bio flags, just use
a helper to find the pool based on the max_vecs value.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7a800a20ae6329e803c5c646b20811a6ae9ca136)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/bio.c
diff --cc block/bio.c
index 3992af35a0a5,a0eabe2f8b07..000000000000
--- a/block/bio.c
+++ b/block/bio.c
@@@ -41,19 -29,31 +41,37 @@@ struct biovec_slab 
  	int nr_vecs;
  	char *name;
  	struct kmem_cache *slab;
 -} bvec_slabs[] __read_mostly = {
 -	{ .nr_vecs = 16, .name = "biovec-16" },
 -	{ .nr_vecs = 64, .name = "biovec-64" },
 -	{ .nr_vecs = 128, .name = "biovec-128" },
 -	{ .nr_vecs = BIO_MAX_PAGES, .name = "biovec-max" },
  };
  
+ static struct biovec_slab *biovec_slab(unsigned short nr_vecs)
+ {
+ 	switch (nr_vecs) {
+ 	/* smaller bios use inline vecs */
+ 	case 5 ... 16:
+ 		return &bvec_slabs[0];
+ 	case 17 ... 64:
+ 		return &bvec_slabs[1];
+ 	case 65 ... 128:
+ 		return &bvec_slabs[2];
+ 	case 129 ... BIO_MAX_PAGES:
+ 		return &bvec_slabs[3];
+ 	default:
+ 		BUG();
+ 		return NULL;
+ 	}
+ }
+ 
 +/*
 + * if you change this list, also change bvec_alloc or things will
 + * break badly! cannot be bigger than what you can fit into an
 + * unsigned short
 + */
 +#define BV(x, n) { .nr_vecs = x, .name = "biovec-"#n }
 +static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
 +	BV(1, 1), BV(4, 4), BV(16, 16), BV(64, 64), BV(128, 128), BV(BIO_MAX_PAGES, max),
 +};
 +#undef BV
 +
  /*
   * fs_bio_set is the bio_set containing bio and iovec memory pools used by
   * IO code that does not need private memory pools.
@@@ -181,34 -169,19 +187,45 @@@ static inline gfp_t bvec_alloc_gfp(gfp_
  		__GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;
  }
  
- struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,
- 			   mempool_t *pool)
+ struct bio_vec *bvec_alloc(mempool_t *pool, unsigned short *nr_vecs,
+ 		gfp_t gfp_mask)
  {
++<<<<<<< HEAD
 +	/*
 +	 * see comment near bvec_array define!
 +	 */
 +	switch (nr) {
 +	case 1:
 +		*idx = 0;
 +		break;
 +	case 2 ... 4:
 +		*idx = 1;
 +		break;
 +	case 5 ... 16:
 +		*idx = 2;
 +		break;
 +	case 17 ... 64:
 +		*idx = 3;
 +		break;
 +	case 65 ... 128:
 +		*idx = 4;
 +		break;
 +	case 129 ... BIO_MAX_PAGES:
 +		*idx = 5;
 +		break;
 +	default:
++=======
+ 	struct biovec_slab *bvs = biovec_slab(*nr_vecs);
+ 
+ 	if (WARN_ON_ONCE(!bvs))
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  		return NULL;
- 	}
+ 
+ 	/*
+ 	 * Upgrade the nr_vecs request to take full advantage of the allocation.
+ 	 * We also rely on this in the bvec_free path.
+ 	 */
+ 	*nr_vecs = bvs->nr_vecs;
  
  	/*
  	 * Try a slab allocation first for all smaller allocations.  If that
@@@ -494,27 -439,24 +503,40 @@@ struct bio *bio_alloc_bioset(gfp_t gfp_
  	if (unlikely(!p))
  		return NULL;
  
++<<<<<<< HEAD
 +	bio = p + front_pad;
 +	bio_init(bio, NULL, 0);
 +
 +	if (nr_iovecs > inline_vecs) {
 +		unsigned long idx = 0;
++=======
+ 	bio = p + bs->front_pad;
+ 	if (nr_iovecs > BIO_INLINE_VECS) {
+ 		struct bio_vec *bvl = NULL;
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  
- 		bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+ 		bvl = bvec_alloc(&bs->bvec_pool, &nr_iovecs, gfp_mask);
  		if (!bvl && gfp_mask != saved_gfp) {
  			punt_bios_to_rescuer(bs);
  			gfp_mask = saved_gfp;
++<<<<<<< HEAD
 +			bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
++=======
+ 			bvl = bvec_alloc(&bs->bvec_pool, &nr_iovecs, gfp_mask);
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  		}
- 
  		if (unlikely(!bvl))
  			goto err_free;
  
++<<<<<<< HEAD
 +		bio->bi_flags |= idx << BVEC_POOL_OFFSET;
 +		bio->bi_max_vecs = bvec_nr_vecs(idx);
++=======
+ 		bio_init(bio, bvl, nr_iovecs);
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  	} else if (nr_iovecs) {
 -		bio_init(bio, bio->bi_inline_vecs, BIO_INLINE_VECS);
 -	} else {
 -		bio_init(bio, NULL, 0);
 +		bvl = bio->bi_inline_vecs;
 +		bio->bi_max_vecs = inline_vecs;
  	}
  
  	bio->bi_pool = bs;
@@@ -680,14 -628,13 +702,14 @@@ EXPORT_SYMBOL(bio_phys_segments)
   */
  void __bio_clone_fast(struct bio *bio, struct bio *bio_src)
  {
- 	BUG_ON(bio->bi_pool && BVEC_POOL_IDX(bio));
+ 	WARN_ON_ONCE(bio->bi_pool && bio->bi_max_vecs);
  
  	/*
 -	 * most users will be overriding ->bi_bdev with a new target,
 +	 * most users will be overriding ->bi_disk with a new target,
  	 * so we don't set nor calculate new physical/hw segment counts here
  	 */
 -	bio->bi_bdev = bio_src->bi_bdev;
 +	bio->bi_disk = bio_src->bi_disk;
 +	bio->bi_partno = bio_src->bi_partno;
  	bio_set_flag(bio, BIO_CLONED);
  	if (bio_flagged(bio_src, BIO_THROTTLED))
  		bio_set_flag(bio, BIO_THROTTLED);
@@@ -949,26 -900,35 +971,42 @@@ int bio_add_page(struct bio *bio, struc
  }
  EXPORT_SYMBOL(bio_add_page);
  
 -void bio_release_pages(struct bio *bio, bool mark_dirty)
 +static int __bio_iov_bvec_add_pages(struct bio *bio, struct iov_iter *iter)
  {
 -	struct bvec_iter_all iter_all;
 -	struct bio_vec *bvec;
 -
 -	if (bio_flagged(bio, BIO_NO_PAGE_REF))
 -		return;
 -
 -	bio_for_each_segment_all(bvec, bio, iter_all) {
 -		if (mark_dirty && !PageCompound(bvec->bv_page))
 -			set_page_dirty_lock(bvec->bv_page);
 -		put_page(bvec->bv_page);
 +	const struct bio_vec *bv = iter->bvec;
 +	unsigned int len;
 +	size_t size;
 +
 +	if (WARN_ON_ONCE(iter->iov_offset > bv->bv_len))
 +		return -EINVAL;
 +
 +	len = min_t(size_t, bv->bv_len - iter->iov_offset, iter->count);
 +	size = bio_add_page(bio, bv->bv_page, len,
 +				bv->bv_offset + iter->iov_offset);
 +	if (size == len) {
 +		if (!bio_flagged(bio, BIO_NO_PAGE_REF))
 +			get_page(bv->bv_page);
 +		iov_iter_advance(iter, size);
 +		return 0;
  	}
 -}
 -EXPORT_SYMBOL_GPL(bio_release_pages);
  
++<<<<<<< HEAD
 +	return -EINVAL;
++=======
+ static int bio_iov_bvec_set(struct bio *bio, struct iov_iter *iter)
+ {
+ 	WARN_ON_ONCE(bio->bi_max_vecs);
+ 
+ 	bio->bi_vcnt = iter->nr_segs;
+ 	bio->bi_io_vec = (struct bio_vec *)iter->bvec;
+ 	bio->bi_iter.bi_bvec_done = iter->iov_offset;
+ 	bio->bi_iter.bi_size = iter->count;
+ 	bio_set_flag(bio, BIO_NO_PAGE_REF);
+ 	bio_set_flag(bio, BIO_CLONED);
+ 
+ 	iov_iter_advance(iter, iter->count);
+ 	return 0;
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  }
  
  #define PAGE_PTRS_PER_BVEC     (sizeof(struct bio_vec) / sizeof(struct page *))
@@@ -2264,25 -1589,15 +2302,31 @@@ static void __init biovec_init_slabs(vo
  {
  	int i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < BVEC_POOL_NR; i++) {
 +		int size;
++=======
+ 	bio_integrity_init();
+ 
+ 	for (i = 0; i < ARRAY_SIZE(bvec_slabs); i++) {
++>>>>>>> 7a800a20ae63 (block: use bi_max_vecs to find the bvec pool)
  		struct biovec_slab *bvs = bvec_slabs + i;
  
 -		bvs->slab = kmem_cache_create(bvs->name,
 -				bvs->nr_vecs * sizeof(struct bio_vec), 0,
 -				SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);
 +		if (bvs->nr_vecs <= BIO_INLINE_VECS) {
 +			bvs->slab = NULL;
 +			continue;
 +		}
 +
 +		size = bvs->nr_vecs * sizeof(struct bio_vec);
 +		bvs->slab = kmem_cache_create(bvs->name, size, 0,
 +                                SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
  	}
 +}
 +
 +static int __init init_bio(void)
 +{
 +	bio_integrity_init();
 +	biovec_init_slabs();
  
  	if (bioset_init(&fs_bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS))
  		panic("bio: can't allocate bios\n");
diff --git a/block/bio-integrity.c b/block/bio-integrity.c
index aec83099635a..4b531dcf3db7 100644
--- a/block/bio-integrity.c
+++ b/block/bio-integrity.c
@@ -42,7 +42,7 @@ static void __bio_integrity_free(struct bio_set *bs,
 	if (bs && mempool_initialized(&bs->bio_integrity_pool)) {
 		if (bip->bip_vec)
 			bvec_free(&bs->bvec_integrity_pool, bip->bip_vec,
-				  bip->bip_slab);
+				  bip->bip_max_vcnt);
 		mempool_free(bip, &bs->bio_integrity_pool);
 	} else {
 		kfree(bip);
@@ -82,14 +82,11 @@ struct bio_integrity_payload *bio_integrity_alloc(struct bio *bio,
 	memset(bip, 0, sizeof(*bip));
 
 	if (nr_vecs > inline_vecs) {
-		unsigned long idx = 0;
-
-		bip->bip_vec = bvec_alloc(gfp_mask, nr_vecs, &idx,
-					  &bs->bvec_integrity_pool);
+		bip->bip_max_vcnt = nr_vecs;
+		bip->bip_vec = bvec_alloc(&bs->bvec_integrity_pool,
+					  &bip->bip_max_vcnt, gfp_mask);
 		if (!bip->bip_vec)
 			goto err;
-		bip->bip_max_vcnt = bvec_nr_vecs(idx);
-		bip->bip_slab = idx;
 	} else {
 		bip->bip_vec = bip->bip_inline_vecs;
 		bip->bip_max_vcnt = inline_vecs;
* Unmerged path block/bio.c
diff --git a/block/blk.h b/block/blk.h
index ea3cdaf19317..6252a8e6ee06 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -60,9 +60,9 @@ void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
 void blk_freeze_queue(struct request_queue *q);
 
 #define BIO_INLINE_VECS 4
-struct bio_vec *bvec_alloc(gfp_t, int, unsigned long *, mempool_t *);
-void bvec_free(mempool_t *, struct bio_vec *, unsigned int);
-unsigned int bvec_nr_vecs(unsigned short idx);
+struct bio_vec *bvec_alloc(mempool_t *pool, unsigned short *nr_vecs,
+		gfp_t gfp_mask);
+void bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned short nr_vecs);
 
 static inline bool biovec_phys_mergeable(struct request_queue *q,
 		struct bio_vec *vec1, struct bio_vec *vec2)
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 7179bf0d2290..df054b300281 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -296,7 +296,6 @@ struct bio_integrity_payload {
 
 	struct bvec_iter	bip_iter;
 
-	unsigned short		bip_slab;	/* slab the bip came from */
 	unsigned short		bip_vcnt;	/* # of integrity bio_vecs */
 	unsigned short		bip_max_vcnt;	/* integrity bio_vec slots */
 	unsigned short		bip_flags;	/* control flags */
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 486878f9b7db..0269e6fb5d7a 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -182,7 +182,7 @@ struct bio {
 						 * top bits REQ_OP. Use
 						 * accessors.
 						 */
-	unsigned short		bi_flags;	/* status, etc and bvec pool number */
+	unsigned short		bi_flags;	/* BIO_* below */
 	unsigned short		bi_ioprio;
 	unsigned short		bi_write_hint;
 	blk_status_t		bi_status;
@@ -270,33 +270,6 @@ struct bio {
 #define BIO_TRACKED 12		/* set if bio goes through the rq_qos path */
 #define	BIO_CGROUP_ACCT	BIO_QUEUE_ENTERED /* has been accounted to a cgroup */
 
-/* See BVEC_POOL_OFFSET below before adding new flags */
-
-/*
- * We support 6 different bvec pools, the last one is magic in that it
- * is backed by a mempool.
- */
-#define BVEC_POOL_NR		6
-#define BVEC_POOL_MAX		(BVEC_POOL_NR - 1)
-
-/*
- * Top 3 bits of bio flags indicate the pool the bvecs came from.  We add
- * 1 to the actual index so that 0 indicates that there are no bvecs to be
- * freed.
- */
-#define BVEC_POOL_BITS		(3)
-#define BVEC_POOL_OFFSET	(16 - BVEC_POOL_BITS)
-#define BVEC_POOL_IDX(bio)	((bio)->bi_flags >> BVEC_POOL_OFFSET)
-#if (1<< BVEC_POOL_BITS) < (BVEC_POOL_NR+1)
-# error "BVEC_POOL_BITS is too small"
-#endif
-
-/*
- * Flags starting here get preserved by bio_reset() - this includes
- * only BVEC_POOL_IDX()
- */
-#define BIO_RESET_BITS	BVEC_POOL_OFFSET
-
 typedef __u32 __bitwise blk_mq_req_flags_t;
 
 /*
