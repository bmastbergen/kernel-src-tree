highmem: Make DEBUG_HIGHMEM functional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 389755c250814185938f5b04334a4f0184c30647
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/389755c2.failed

For some obscure reason when CONFIG_DEBUG_HIGHMEM is enabled the stack
depth is increased from 20 to 41. But the only thing DEBUG_HIGHMEM does is
to enable a few BUG_ON()'s in the mapping code.

That's a leftover from the historical mapping code which had fixed entries
for various purposes. DEBUG_HIGHMEM inserted guard mappings between the map
types. But that got all ditched when kmap_atomic() switched to a stack
based map management. Though the WITH_KM_FENCE magic survived without being
functional. All the thing does today is to increase the stack depth.

Add a working implementation to the generic kmap_local* implementation.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Linus Torvalds <torvalds@linuxfoundation.org>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
Link: https://lore.kernel.org/r/20201103095857.268258322@linutronix.de

(cherry picked from commit 389755c250814185938f5b04334a4f0184c30647)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/highmem.c
diff --cc mm/highmem.c
index 51171d0f44c2,67d2d5983cb0..000000000000
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@@ -365,8 -367,156 +365,160 @@@ void kunmap_high(struct page *page
  	if (need_wakeup)
  		wake_up(pkmap_map_wait);
  }
 +
  EXPORT_SYMBOL(kunmap_high);
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_HIGHMEM */
+ 
+ #ifdef CONFIG_KMAP_LOCAL
+ 
+ #include <asm/kmap_size.h>
+ 
+ static DEFINE_PER_CPU(int, __kmap_local_idx);
+ 
+ /*
+  * With DEBUG_HIGHMEM the stack depth is doubled and every second
+  * slot is unused which acts as a guard page
+  */
+ #ifdef CONFIG_DEBUG_HIGHMEM
+ # define KM_INCR	2
+ #else
+ # define KM_INCR	1
+ #endif
+ 
+ static inline int kmap_local_idx_push(void)
+ {
+ 	int idx = __this_cpu_add_return(__kmap_local_idx, KM_INCR) - 1;
+ 
+ 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+ 	BUG_ON(idx >= KM_MAX_IDX);
+ 	return idx;
+ }
+ 
+ static inline int kmap_local_idx(void)
+ {
+ 	return __this_cpu_read(__kmap_local_idx) - 1;
+ }
+ 
+ static inline void kmap_local_idx_pop(void)
+ {
+ 	int idx = __this_cpu_sub_return(__kmap_local_idx, KM_INCR);
+ 
+ 	BUG_ON(idx < 0);
+ }
+ 
+ #ifndef arch_kmap_local_post_map
+ # define arch_kmap_local_post_map(vaddr, pteval)	do { } while (0)
+ #endif
+ #ifndef arch_kmap_local_pre_unmap
+ # define arch_kmap_local_pre_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_post_unmap
+ # define arch_kmap_local_post_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_map_idx
+ #define arch_kmap_local_map_idx(idx, pfn)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_unmap_idx
+ #define arch_kmap_local_unmap_idx(idx, vaddr)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_high_get
+ static inline void *arch_kmap_local_high_get(struct page *page)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ /* Unmap a local mapping which was obtained by kmap_high_get() */
+ static inline void kmap_high_unmap_local(unsigned long vaddr)
+ {
+ #ifdef ARCH_NEEDS_KMAP_HIGH_GET
+ 	if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP))
+ 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
+ #endif
+ }
+ 
+ static inline int kmap_local_calc_idx(int idx)
+ {
+ 	return idx + KM_MAX_IDX * smp_processor_id();
+ }
+ 
+ static pte_t *__kmap_pte;
+ 
+ static pte_t *kmap_get_pte(void)
+ {
+ 	if (!__kmap_pte)
+ 		__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
+ 	return __kmap_pte;
+ }
+ 
+ void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
+ {
+ 	pte_t pteval, *kmap_pte = kmap_get_pte();
+ 	unsigned long vaddr;
+ 	int idx;
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 	BUG_ON(!pte_none(*(kmap_pte - idx)));
+ 	pteval = pfn_pte(pfn, prot);
+ 	set_pte_at(&init_mm, vaddr, kmap_pte - idx, pteval);
+ 	arch_kmap_local_post_map(vaddr, pteval);
+ 	preempt_enable();
+ 
+ 	return (void *)vaddr;
+ }
+ EXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);
+ 
+ void *__kmap_local_page_prot(struct page *page, pgprot_t prot)
+ {
+ 	void *kmap;
+ 
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	/* Try kmap_high_get() if architecture has it enabled */
+ 	kmap = arch_kmap_local_high_get(page);
+ 	if (kmap)
+ 		return kmap;
+ 
+ 	return __kmap_local_pfn_prot(page_to_pfn(page), prot);
+ }
+ EXPORT_SYMBOL(__kmap_local_page_prot);
+ 
+ void kunmap_local_indexed(void *vaddr)
+ {
+ 	unsigned long addr = (unsigned long) vaddr & PAGE_MASK;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int idx;
+ 
+ 	if (addr < __fix_to_virt(FIX_KMAP_END) ||
+ 	    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {
+ 		WARN_ON_ONCE(addr < PAGE_OFFSET);
+ 
+ 		/* Handle mappings which were obtained by kmap_high_get() */
+ 		kmap_high_unmap_local(addr);
+ 		return;
+ 	}
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);
+ 	WARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
+ 
+ 	arch_kmap_local_pre_unmap(addr);
+ 	pte_clear(&init_mm, addr, kmap_pte - idx);
+ 	arch_kmap_local_post_unmap(addr);
+ 	kmap_local_idx_pop();
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL(kunmap_local_indexed);
++>>>>>>> 389755c25081 (highmem: Make DEBUG_HIGHMEM functional)
  #endif
  
  #if defined(HASHED_PAGE_VIRTUAL)
* Unmerged path mm/highmem.c
