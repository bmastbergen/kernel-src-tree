mm/highmem: Provide kmap_local*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit f3ba3c710ac5a30cd058615a9eb62d2ad95bb782
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/f3ba3c71.failed

Now that the kmap atomic index is stored in task struct provide a
preemptible variant. On context switch the maps of an outgoing task are
removed and the map of the incoming task are restored. That's obviously
slow, but highmem is slow anyway.

The kmap_local.*() functions can be invoked from both preemptible and
atomic context. kmap local sections disable migration to keep the resulting
virtual mapping address correct, but disable neither pagefaults nor
preemption.

A wholesale conversion of kmap_atomic to be fully preemptible is not
possible because some of the usage sites might rely on the preemption
disable for serialization or on the implicit pagefault disable. Needs to be
done on a case by case basis.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/20201118204007.468533059@linutronix.de

(cherry picked from commit f3ba3c710ac5a30cd058615a9eb62d2ad95bb782)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/highmem-internal.h
#	include/linux/highmem.h
#	mm/highmem.c
diff --cc include/linux/highmem.h
index b1641d4ce279,f597830f26b4..000000000000
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@@ -11,6 -11,119 +11,122 @@@
  
  #include <asm/cacheflush.h>
  
++<<<<<<< HEAD
++=======
+ #include "highmem-internal.h"
+ 
+ /**
+  * kmap - Map a page for long term usage
+  * @page:	Pointer to the page to be mapped
+  *
+  * Returns: The virtual address of the mapping
+  *
+  * Can only be invoked from preemptible task context because on 32bit
+  * systems with CONFIG_HIGHMEM enabled this function might sleep.
+  *
+  * For systems with CONFIG_HIGHMEM=n and for pages in the low memory area
+  * this returns the virtual address of the direct kernel mapping.
+  *
+  * The returned virtual address is globally visible and valid up to the
+  * point where it is unmapped via kunmap(). The pointer can be handed to
+  * other contexts.
+  *
+  * For highmem pages on 32bit systems this can be slow as the mapping space
+  * is limited and protected by a global lock. In case that there is no
+  * mapping slot available the function blocks until a slot is released via
+  * kunmap().
+  */
+ static inline void *kmap(struct page *page);
+ 
+ /**
+  * kunmap - Unmap the virtual address mapped by kmap()
+  * @addr:	Virtual address to be unmapped
+  *
+  * Counterpart to kmap(). A NOOP for CONFIG_HIGHMEM=n and for mappings of
+  * pages in the low memory area.
+  */
+ static inline void kunmap(struct page *page);
+ 
+ /**
+  * kmap_to_page - Get the page for a kmap'ed address
+  * @addr:	The address to look up
+  *
+  * Returns: The page which is mapped to @addr.
+  */
+ static inline struct page *kmap_to_page(void *addr);
+ 
+ /**
+  * kmap_flush_unused - Flush all unused kmap mappings in order to
+  *		       remove stray mappings
+  */
+ static inline void kmap_flush_unused(void);
+ 
+ /**
+  * kmap_local_page - Map a page for temporary usage
+  * @page:	Pointer to the page to be mapped
+  *
+  * Returns: The virtual address of the mapping
+  *
+  * Can be invoked from any context.
+  *
+  * Requires careful handling when nesting multiple mappings because the map
+  * management is stack based. The unmap has to be in the reverse order of
+  * the map operation:
+  *
+  * addr1 = kmap_local_page(page1);
+  * addr2 = kmap_local_page(page2);
+  * ...
+  * kunmap_local(addr2);
+  * kunmap_local(addr1);
+  *
+  * Unmapping addr1 before addr2 is invalid and causes malfunction.
+  *
+  * Contrary to kmap() mappings the mapping is only valid in the context of
+  * the caller and cannot be handed to other contexts.
+  *
+  * On CONFIG_HIGHMEM=n kernels and for low memory pages this returns the
+  * virtual address of the direct mapping. Only real highmem pages are
+  * temporarily mapped.
+  *
+  * While it is significantly faster than kmap() for the higmem case it
+  * comes with restrictions about the pointer validity. Only use when really
+  * necessary.
+  *
+  * On HIGHMEM enabled systems mapping a highmem page has the side effect of
+  * disabling migration in order to keep the virtual address stable across
+  * preemption. No caller of kmap_local_page() can rely on this side effect.
+  */
+ static inline void *kmap_local_page(struct page *page);
+ 
+ /**
+  * kmap_atomic - Atomically map a page for temporary usage - Deprecated!
+  * @page:	Pointer to the page to be mapped
+  *
+  * Returns: The virtual address of the mapping
+  *
+  * Effectively a wrapper around kmap_local_page() which disables pagefaults
+  * and preemption.
+  *
+  * Do not use in new code. Use kmap_local_page() instead.
+  */
+ static inline void *kmap_atomic(struct page *page);
+ 
+ /**
+  * kunmap_atomic - Unmap the virtual address mapped by kmap_atomic()
+  * @addr:	Virtual address to be unmapped
+  *
+  * Counterpart to kmap_atomic().
+  *
+  * Effectively a wrapper around kunmap_local() which additionally undoes
+  * the side effects of kmap_atomic(), i.e. reenabling pagefaults and
+  * preemption.
+  */
+ 
+ /* Highmem related interfaces for management code */
+ static inline unsigned int nr_free_highpages(void);
+ static inline unsigned long totalhigh_pages(void);
+ 
++>>>>>>> f3ba3c710ac5 (mm/highmem: Provide kmap_local*)
  #ifndef ARCH_HAS_FLUSH_ANON_PAGE
  static inline void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)
  {
diff --cc mm/highmem.c
index 51171d0f44c2,83f9660f168f..000000000000
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@@ -365,8 -358,259 +365,263 @@@ void kunmap_high(struct page *page
  	if (need_wakeup)
  		wake_up(pkmap_map_wait);
  }
 +
  EXPORT_SYMBOL(kunmap_high);
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_HIGHMEM */
+ 
+ #ifdef CONFIG_KMAP_LOCAL
+ 
+ #include <asm/kmap_size.h>
+ 
+ /*
+  * With DEBUG_KMAP_LOCAL the stack depth is doubled and every second
+  * slot is unused which acts as a guard page
+  */
+ #ifdef CONFIG_DEBUG_KMAP_LOCAL
+ # define KM_INCR	2
+ #else
+ # define KM_INCR	1
+ #endif
+ 
+ static inline int kmap_local_idx_push(void)
+ {
+ 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+ 	current->kmap_ctrl.idx += KM_INCR;
+ 	BUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);
+ 	return current->kmap_ctrl.idx - 1;
+ }
+ 
+ static inline int kmap_local_idx(void)
+ {
+ 	return current->kmap_ctrl.idx - 1;
+ }
+ 
+ static inline void kmap_local_idx_pop(void)
+ {
+ 	current->kmap_ctrl.idx -= KM_INCR;
+ 	BUG_ON(current->kmap_ctrl.idx < 0);
+ }
+ 
+ #ifndef arch_kmap_local_post_map
+ # define arch_kmap_local_post_map(vaddr, pteval)	do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_pre_unmap
+ # define arch_kmap_local_pre_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_post_unmap
+ # define arch_kmap_local_post_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_map_idx
+ #define arch_kmap_local_map_idx(idx, pfn)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_unmap_idx
+ #define arch_kmap_local_unmap_idx(idx, vaddr)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_high_get
+ static inline void *arch_kmap_local_high_get(struct page *page)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ /* Unmap a local mapping which was obtained by kmap_high_get() */
+ static inline bool kmap_high_unmap_local(unsigned long vaddr)
+ {
+ #ifdef ARCH_NEEDS_KMAP_HIGH_GET
+ 	if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
+ 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
+ 		return true;
+ 	}
+ #endif
+ 	return false;
+ }
+ 
+ static inline int kmap_local_calc_idx(int idx)
+ {
+ 	return idx + KM_MAX_IDX * smp_processor_id();
+ }
+ 
+ static pte_t *__kmap_pte;
+ 
+ static pte_t *kmap_get_pte(void)
+ {
+ 	if (!__kmap_pte)
+ 		__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
+ 	return __kmap_pte;
+ }
+ 
+ void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
+ {
+ 	pte_t pteval, *kmap_pte = kmap_get_pte();
+ 	unsigned long vaddr;
+ 	int idx;
+ 
+ 	/*
+ 	 * Disable migration so resulting virtual address is stable
+ 	 * accross preemption.
+ 	 */
+ 	migrate_disable();
+ 	preempt_disable();
+ 	idx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 	BUG_ON(!pte_none(*(kmap_pte - idx)));
+ 	pteval = pfn_pte(pfn, prot);
+ 	set_pte_at(&init_mm, vaddr, kmap_pte - idx, pteval);
+ 	arch_kmap_local_post_map(vaddr, pteval);
+ 	current->kmap_ctrl.pteval[kmap_local_idx()] = pteval;
+ 	preempt_enable();
+ 
+ 	return (void *)vaddr;
+ }
+ EXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);
+ 
+ void *__kmap_local_page_prot(struct page *page, pgprot_t prot)
+ {
+ 	void *kmap;
+ 
+ 	/*
+ 	 * To broaden the usage of the actual kmap_local() machinery always map
+ 	 * pages when debugging is enabled and the architecture has no problems
+ 	 * with alias mappings.
+ 	 */
+ 	if (!IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) && !PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	/* Try kmap_high_get() if architecture has it enabled */
+ 	kmap = arch_kmap_local_high_get(page);
+ 	if (kmap)
+ 		return kmap;
+ 
+ 	return __kmap_local_pfn_prot(page_to_pfn(page), prot);
+ }
+ EXPORT_SYMBOL(__kmap_local_page_prot);
+ 
+ void kunmap_local_indexed(void *vaddr)
+ {
+ 	unsigned long addr = (unsigned long) vaddr & PAGE_MASK;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int idx;
+ 
+ 	if (addr < __fix_to_virt(FIX_KMAP_END) ||
+ 	    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {
+ 		if (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {
+ 			/* This _should_ never happen! See above. */
+ 			WARN_ON_ONCE(1);
+ 			return;
+ 		}
+ 		/*
+ 		 * Handle mappings which were obtained by kmap_high_get()
+ 		 * first as the virtual address of such mappings is below
+ 		 * PAGE_OFFSET. Warn for all other addresses which are in
+ 		 * the user space part of the virtual address space.
+ 		 */
+ 		if (!kmap_high_unmap_local(addr))
+ 			WARN_ON_ONCE(addr < PAGE_OFFSET);
+ 		return;
+ 	}
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);
+ 	WARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
+ 
+ 	arch_kmap_local_pre_unmap(addr);
+ 	pte_clear(&init_mm, addr, kmap_pte - idx);
+ 	arch_kmap_local_post_unmap(addr);
+ 	current->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);
+ 	kmap_local_idx_pop();
+ 	preempt_enable();
+ 	migrate_enable();
+ }
+ EXPORT_SYMBOL(kunmap_local_indexed);
+ 
+ /*
+  * Invoked before switch_to(). This is safe even when during or after
+  * clearing the maps an interrupt which needs a kmap_local happens because
+  * the task::kmap_ctrl.idx is not modified by the unmapping code so a
+  * nested kmap_local will use the next unused index and restore the index
+  * on unmap. The already cleared kmaps of the outgoing task are irrelevant
+  * because the interrupt context does not know about them. The same applies
+  * when scheduling back in for an interrupt which happens before the
+  * restore is complete.
+  */
+ void __kmap_local_sched_out(void)
+ {
+ 	struct task_struct *tsk = current;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int i;
+ 
+ 	/* Clear kmaps */
+ 	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+ 		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+ 		unsigned long addr;
+ 		int idx;
+ 
+ 		/* With debug all even slots are unmapped and act as guard */
+ 		if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !(i & 0x01)) {
+ 			WARN_ON_ONCE(!pte_none(pteval));
+ 			continue;
+ 		}
+ 		if (WARN_ON_ONCE(pte_none(pteval)))
+ 			continue;
+ 
+ 		/*
+ 		 * This is a horrible hack for XTENSA to calculate the
+ 		 * coloured PTE index. Uses the PFN encoded into the pteval
+ 		 * and the map index calculation because the actual mapped
+ 		 * virtual address is not stored in task::kmap_ctrl.
+ 		 * For any sane architecture this is optimized out.
+ 		 */
+ 		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+ 
+ 		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 		arch_kmap_local_pre_unmap(addr);
+ 		pte_clear(&init_mm, addr, kmap_pte - idx);
+ 		arch_kmap_local_post_unmap(addr);
+ 	}
+ }
+ 
+ void __kmap_local_sched_in(void)
+ {
+ 	struct task_struct *tsk = current;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int i;
+ 
+ 	/* Restore kmaps */
+ 	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+ 		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+ 		unsigned long addr;
+ 		int idx;
+ 
+ 		/* With debug all even slots are unmapped and act as guard */
+ 		if (IS_ENABLED(CONFIG_DEBUG_HIGHMEM) && !(i & 0x01)) {
+ 			WARN_ON_ONCE(!pte_none(pteval));
+ 			continue;
+ 		}
+ 		if (WARN_ON_ONCE(pte_none(pteval)))
+ 			continue;
+ 
+ 		/* See comment in __kmap_local_sched_out() */
+ 		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+ 		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 		set_pte_at(&init_mm, addr, kmap_pte - idx, pteval);
+ 		arch_kmap_local_post_map(addr, pteval);
+ 	}
+ }
+ 
+ void kmap_local_fork(struct task_struct *tsk)
+ {
+ 	if (WARN_ON_ONCE(tsk->kmap_ctrl.idx))
+ 		memset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));
+ }
+ 
++>>>>>>> f3ba3c710ac5 (mm/highmem: Provide kmap_local*)
  #endif
  
  #if defined(HASHED_PAGE_VIRTUAL)
* Unmerged path include/linux/highmem-internal.h
* Unmerged path include/linux/highmem-internal.h
* Unmerged path include/linux/highmem.h
* Unmerged path mm/highmem.c
