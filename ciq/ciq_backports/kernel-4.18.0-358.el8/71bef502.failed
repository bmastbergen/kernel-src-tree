scsi: qla2xxx: edif: Increment command and completion counts

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Quinn Tran <qutran@marvell.com>
commit 71bef5020cd13e1aaa878d10481aafc1ecd4a8f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/71bef502.failed

Increment the command and the completion counts.

Link: https://lore.kernel.org/r/20210624052606.21613-11-njavali@marvell.com
	Signed-off-by: Quinn Tran <qutran@marvell.com>
	Signed-off-by: Nilesh Javali <njavali@marvell.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 71bef5020cd13e1aaa878d10481aafc1ecd4a8f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_edif.c
#	drivers/scsi/qla2xxx/qla_isr.c
diff --cc drivers/scsi/qla2xxx/qla_edif.c
index 8c567362bb60,ccbe0e1bfcbc..000000000000
--- a/drivers/scsi/qla2xxx/qla_edif.c
+++ b/drivers/scsi/qla2xxx/qla_edif.c
@@@ -742,4 -1847,1563 +742,1566 @@@ qla_edb_stop(scsi_qla_host_t *vha
  		    "%s doorbell not enabled\n", __func__);
  		return;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/* grab lock so list doesn't move */
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	vha->e_dbell.db_flags &= ~EDB_ACTIVE; /* mark it not active */
+ 	/* hopefully this is a null list at this point */
+ 	list_for_each_entry_safe(node, q, &vha->e_dbell.head, list) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910f,
+ 		    "%s freeing edb_node type=%x\n",
+ 		    __func__, node->ntype);
+ 		qla_edb_node_free(vha, node);
+ 		list_del(&node->list);
+ 
+ 		kfree(node);
+ 	}
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* wake up doorbell waiters - they'll be dismissed with error code */
+ 	complete_all(&vha->e_dbell.dbell);
+ }
+ 
+ static struct edb_node *
+ qla_edb_node_alloc(scsi_qla_host_t *vha, uint32_t ntype)
+ {
+ 	struct edb_node	*node;
+ 
+ 	node = kzalloc(sizeof(*node), GFP_ATOMIC);
+ 	if (!node) {
+ 		/* couldn't get space */
+ 		ql_dbg(ql_dbg_edif, vha, 0x9100,
+ 		    "edb node unable to be allocated\n");
+ 		return NULL;
+ 	}
+ 
+ 	node->ntype = ntype;
+ 	INIT_LIST_HEAD(&node->list);
+ 	return node;
+ }
+ 
+ /* adds a already alllocated enode to the linked list */
+ static bool
+ qla_edb_node_add(scsi_qla_host_t *vha, struct edb_node *ptr)
+ {
+ 	unsigned long		flags;
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled\n", __func__);
+ 		return false;
+ 	}
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 	list_add_tail(&ptr->list, &vha->e_dbell.head);
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* ring doorbell for waiters */
+ 	complete(&vha->e_dbell.dbell);
+ 
+ 	return true;
+ }
+ 
+ /* adds event to doorbell list */
+ void
+ qla_edb_eventcreate(scsi_qla_host_t *vha, uint32_t dbtype,
+ 	uint32_t data, uint32_t data2, fc_port_t	*sfcport)
+ {
+ 	struct edb_node	*edbnode;
+ 	fc_port_t *fcport = sfcport;
+ 	port_id_t id;
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		return;
+ 	}
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		if (fcport)
+ 			fcport->edif.auth_state = dbtype;
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled (type=%d\n", __func__, dbtype);
+ 		return;
+ 	}
+ 
+ 	edbnode = qla_edb_node_alloc(vha, dbtype);
+ 	if (!edbnode) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to alloc db node\n", __func__);
+ 		return;
+ 	}
+ 
+ 	if (!fcport) {
+ 		id.b.domain = (data >> 16) & 0xff;
+ 		id.b.area = (data >> 8) & 0xff;
+ 		id.b.al_pa = data & 0xff;
+ 		ql_dbg(ql_dbg_edif, vha, 0x09222,
+ 		    "%s: Arrived s_id: %06x\n", __func__,
+ 		    id.b24);
+ 		fcport = qla2x00_find_fcport_by_pid(vha, &id);
+ 		if (!fcport) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			    "%s can't find fcport for sid= 0x%x - ignoring\n",
+ 			__func__, id.b24);
+ 			kfree(edbnode);
+ 			return;
+ 		}
+ 	}
+ 
+ 	/* populate the edb node */
+ 	switch (dbtype) {
+ 	case VND_CMD_AUTH_STATE_NEEDED:
+ 	case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 		edbnode->u.plogi_did.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 		edbnode->u.els_sid.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 		edbnode->u.sa_aen.port_id = fcport->d_id;
+ 		edbnode->u.sa_aen.status =  data;
+ 		edbnode->u.sa_aen.key_type =  data2;
+ 		break;
+ 	default:
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			"%s unknown type: %x\n", __func__, dbtype);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		edbnode = NULL;
+ 		break;
+ 	}
+ 
+ 	if (edbnode && (!qla_edb_node_add(vha, edbnode))) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to add dbnode\n", __func__);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		return;
+ 	}
+ 	if (edbnode && fcport)
+ 		fcport->edif.auth_state = dbtype;
+ 	ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 	    "%s Doorbell produced : type=%d %p\n", __func__, dbtype, edbnode);
+ }
+ 
+ static struct edb_node *
+ qla_edb_getnext(scsi_qla_host_t *vha)
+ {
+ 	unsigned long	flags;
+ 	struct edb_node	*edbnode = NULL;
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* db nodes are fifo - no qualifications done */
+ 	if (!list_empty(&vha->e_dbell.head)) {
+ 		edbnode = list_first_entry(&vha->e_dbell.head,
+ 		    struct edb_node, list);
+ 		list_del(&edbnode->list);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	return edbnode;
+ }
+ 
+ /*
+  * app uses separate thread to read this. It'll wait until the doorbell
+  * is rung by the driver or the max wait time has expired
+  */
+ ssize_t
+ edif_doorbell_show(struct device *dev, struct device_attribute *attr,
+ 		char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	struct edb_node	*dbnode = NULL;
+ 	struct edif_app_dbell *ap = (struct edif_app_dbell *)buf;
+ 	uint32_t dat_siz, buf_size, sz;
+ 
+ 	/* TODO: app currently hardcoded to 256. Will transition to bsg */
+ 	sz = 256;
+ 
+ 	/* stop new threads from waiting if we're not init'd */
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif db not enabled\n", __func__);
+ 		return 0;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif not enabled\n", __func__);
+ 		return -1;
+ 	}
+ 
+ 	buf_size = 0;
+ 	while ((sz - buf_size) >= sizeof(struct edb_node)) {
+ 		/* remove the next item from the doorbell list */
+ 		dat_siz = 0;
+ 		dbnode = qla_edb_getnext(vha);
+ 		if (dbnode) {
+ 			ap->event_code = dbnode->ntype;
+ 			switch (dbnode->ntype) {
+ 			case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 			case VND_CMD_AUTH_STATE_NEEDED:
+ 				ap->port_id = dbnode->u.plogi_did;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 				ap->port_id = dbnode->u.els_sid;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 				ap->port_id = dbnode->u.sa_aen.port_id;
+ 				memcpy(ap->event_data, &dbnode->u,
+ 						sizeof(struct edif_sa_update_aen));
+ 				dat_siz += sizeof(struct edif_sa_update_aen);
+ 				break;
+ 			default:
+ 				/* unknown node type, rtn unknown ntype */
+ 				ap->event_code = VND_CMD_AUTH_STATE_UNDEF;
+ 				memcpy(ap->event_data, &dbnode->ntype, 4);
+ 				dat_siz += 4;
+ 				break;
+ 			}
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 				"%s Doorbell consumed : type=%d %p\n",
+ 				__func__, dbnode->ntype, dbnode);
+ 			/* we're done with the db node, so free it up */
+ 			qla_edb_node_free(vha, dbnode);
+ 			kfree(dbnode);
+ 		} else {
+ 			break;
+ 		}
+ 
+ 		ap->event_data_size = dat_siz;
+ 		/* 8bytes = ap->event_code + ap->event_data_size */
+ 		buf_size += dat_siz + 8;
+ 		ap = (struct edif_app_dbell *)(buf + buf_size);
+ 	}
+ 	return buf_size;
+ }
+ 
+ static void qla_noop_sp_done(srb_t *sp, int res)
+ {
+ 	sp->free(sp);
+ }
+ 
+ /*
+  * Called from work queue
+  * build and send the sa_update iocb to delete an rx sa_index
+  */
+ int
+ qla24xx_issue_sa_replace_iocb(scsi_qla_host_t *vha, struct qla_work_evt *e)
+ {
+ 	srb_t *sp;
+ 	fc_port_t	*fcport = NULL;
+ 	struct srb_iocb *iocb_cmd = NULL;
+ 	int rval = QLA_SUCCESS;
+ 	struct	edif_sa_ctl *sa_ctl = e->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = e->u.sa_update.nport_handle;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 	    "%s: starting,  sa_ctl: %p\n", __func__, sa_ctl);
+ 
+ 	if (!sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "sa_ctl allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport = sa_ctl->fcport;
+ 
+ 	/* Alloc SRB structure */
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		 "SRB allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport->flags |= FCF_ASYNC_SENT;
+ 	iocb_cmd = &sp->u.iocb_cmd;
+ 	iocb_cmd->u.sa_update.sa_ctl = sa_ctl;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3073,
+ 	    "Enter: SA REPL portid=%06x, sa_ctl %p, index %x, nport_handle: 0x%x\n",
+ 	    fcport->d_id.b24, sa_ctl, sa_ctl->index, nport_handle);
+ 	/*
+ 	 * if this is a sadb cleanup delete, mark it so the isr can
+ 	 * take the correct action
+ 	 */
+ 	if (sa_ctl->flags & EDIF_SA_CTL_FLG_CLEANUP_DEL) {
+ 		/* mark this srb as a cleanup delete */
+ 		sp->flags |= SRB_EDIF_CLEANUP_DELETE;
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "%s: sp 0x%p flagged as cleanup delete\n", __func__, sp);
+ 	}
+ 
+ 	sp->type = SRB_SA_REPLACE;
+ 	sp->name = "SA_REPLACE";
+ 	sp->fcport = fcport;
+ 	sp->free = qla2x00_rel_sp;
+ 	sp->done = qla_noop_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	if (rval != QLA_SUCCESS)
+ 		rval = QLA_FUNCTION_FAILED;
+ 
+ 	return rval;
+ }
+ 
+ void qla24xx_sa_update_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	int	itr = 0;
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct	qla_sa_update_frame	*sa_frame =
+ 		&sp->u.iocb_cmd.u.sa_update.sa_frame;
+ 	u8 flags = 0;
+ 
+ 	switch (sa_frame->flags & (SAU_FLG_INV | SAU_FLG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_INVALIDATE;
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX;
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX | SA_FLAG_INVALIDATE;
+ 		break;
+ 	}
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	sa_update_iocb->flags = flags;
+ 	sa_update_iocb->salt = cpu_to_le32(sa_frame->salt);
+ 	sa_update_iocb->spi = cpu_to_le32(sa_frame->spi);
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_frame->fast_sa_index);
+ 
+ 	sa_update_iocb->sa_control |= SA_CNTL_ENC_FCSP;
+ 	if (sp->fcport->edif.aes_gmac)
+ 		sa_update_iocb->sa_control |= SA_CNTL_AES_GMAC;
+ 
+ 	if (sa_frame->flags & SAU_FLG_KEY256) {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY256;
+ 		for (itr = 0; itr < 32; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	} else {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY128;
+ 		for (itr = 0; itr < 16; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU Port ID = %02x%02x%02x, flags=%xh, index=%u, ctl=%xh, SPI 0x%x flags 0x%x hdl=%x gmac %d\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], sa_update_iocb->flags, sa_update_iocb->sa_index,
+ 	    sa_update_iocb->sa_control, sa_update_iocb->spi, sa_frame->flags, sp->handle,
+ 	    sp->fcport->edif.aes_gmac);
+ 
+ 	if (sa_frame->flags & SAU_FLG_TX)
+ 		sp->fcport->edif.tx_sa_pending = 1;
+ 	else
+ 		sp->fcport->edif.rx_sa_pending = 1;
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void
+ qla24xx_sa_replace_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct srb_iocb *srb_iocb = &sp->u.iocb_cmd;
+ 	struct	edif_sa_ctl		*sa_ctl = srb_iocb->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = sp->fcport->loop_id;
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(nport_handle);
+ 
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	/* Invalidate the index. salt, spi, control & key are ignore */
+ 	sa_update_iocb->flags = SA_FLAG_INVALIDATE;
+ 	sa_update_iocb->salt = 0;
+ 	sa_update_iocb->spi = 0;
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_ctl->index);
+ 	sa_update_iocb->sa_control = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU DELETE RX Port ID = %02x:%02x:%02x, lid %d flags=%xh, index=%u, hdl=%x\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], nport_handle, sa_update_iocb->flags,
+ 	    sa_update_iocb->sa_index, sp->handle);
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void qla24xx_auth_els(scsi_qla_host_t *vha, void **pkt, struct rsp_que **rsp)
+ {
+ 	struct purex_entry_24xx *p = *pkt;
+ 	struct enode		*ptr;
+ 	int		sid;
+ 	u16 totlen;
+ 	struct purexevent	*purex;
+ 	struct scsi_qla_host *host = NULL;
+ 	int rc;
+ 	struct fc_port *fcport;
+ 	struct qla_els_pt_arg a;
+ 	be_id_t beid;
+ 
+ 	memset(&a, 0, sizeof(a));
+ 
+ 	a.els_opcode = ELS_AUTH_ELS;
+ 	a.nport_handle = p->nport_handle;
+ 	a.rx_xchg_address = p->rx_xchg_addr;
+ 	a.did.b.domain = p->s_id[2];
+ 	a.did.b.area   = p->s_id[1];
+ 	a.did.b.al_pa  = p->s_id[0];
+ 	a.tx_byte_count = a.tx_len = sizeof(struct fc_els_ls_rjt);
+ 	a.tx_addr = vha->hw->elsrej.cdma;
+ 	a.vp_idx = vha->vp_idx;
+ 	a.control_flags = EPD_ELS_RJT;
+ 
+ 	sid = p->s_id[0] | (p->s_id[1] << 8) | (p->s_id[2] << 16);
+ 
+ 	totlen = (le16_to_cpu(p->frame_size) & 0x0fff) - PURX_ELS_HEADER_SIZE;
+ 	if (le16_to_cpu(p->status_flags) & 0x8000) {
+ 		totlen = le16_to_cpu(p->trunc_frame_size);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (totlen > MAX_PAYLOAD) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x0910d,
+ 		    "%s WARNING: verbose ELS frame received (totlen=%x)\n",
+ 		    __func__, totlen);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif support not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x910e, "%s edif not enabled\n",
+ 		    __func__);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	ptr = qla_enode_alloc(vha, N_PUREX);
+ 	if (!ptr) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09109,
+ 		    "WARNING: enode allloc failed for sid=%x\n",
+ 		    sid);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	purex = &ptr->u.purexinfo;
+ 	purex->pur_info.pur_sid = a.did;
+ 	purex->pur_info.pur_pend = 0;
+ 	purex->pur_info.pur_bytes_rcvd = totlen;
+ 	purex->pur_info.pur_rx_xchg_address = le32_to_cpu(p->rx_xchg_addr);
+ 	purex->pur_info.pur_nphdl = le16_to_cpu(p->nport_handle);
+ 	purex->pur_info.pur_did.b.domain =  p->d_id[2];
+ 	purex->pur_info.pur_did.b.area =  p->d_id[1];
+ 	purex->pur_info.pur_did.b.al_pa =  p->d_id[0];
+ 	purex->pur_info.vp_idx = p->vp_idx;
+ 
+ 	rc = __qla_copy_purex_to_buffer(vha, pkt, rsp, purex->msgp,
+ 		purex->msgp_len);
+ 	if (rc) {
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 	beid.al_pa = purex->pur_info.pur_did.b.al_pa;
+ 	beid.area   = purex->pur_info.pur_did.b.area;
+ 	beid.domain = purex->pur_info.pur_did.b.domain;
+ 	host = qla_find_host_by_d_id(vha, beid);
+ 	if (!host) {
+ 		ql_log(ql_log_fatal, vha, 0x508b,
+ 		    "%s Drop ELS due to unable to find host %06x\n",
+ 		    __func__, purex->pur_info.pur_did.b24);
+ 
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 
+ 	fcport = qla2x00_find_fcport_by_pid(host, &purex->pur_info.pur_sid);
+ 
+ 	if (host->e_dbell.db_flags != EDB_ACTIVE ||
+ 	    (fcport && fcport->loop_id == FC_NO_LOOP_ID)) {
+ 		ql_dbg(ql_dbg_edif, host, 0x0910c, "%s e_dbell.db_flags =%x %06x\n",
+ 		    __func__, host->e_dbell.db_flags,
+ 		    fcport ? fcport->d_id.b24 : 0);
+ 
+ 		qla_els_reject_iocb(host, (*rsp)->qpair, &a);
+ 		qla_enode_free(host, ptr);
+ 		return;
+ 	}
+ 
+ 	/* add the local enode to the list */
+ 	qla_enode_add(host, ptr);
+ 
+ 	ql_dbg(ql_dbg_edif, host, 0x0910c,
+ 	    "%s COMPLETE purex->pur_info.pur_bytes_rcvd =%xh s:%06x -> d:%06x xchg=%xh\n",
+ 	    __func__, purex->pur_info.pur_bytes_rcvd, purex->pur_info.pur_sid.b24,
+ 	    purex->pur_info.pur_did.b24, p->rx_xchg_addr);
+ 
+ 	qla_edb_eventcreate(host, VND_CMD_AUTH_STATE_ELS_RCVD, sid, 0, NULL);
+ }
+ 
+ static uint16_t  qla_edif_get_sa_index_from_freepool(fc_port_t *fcport, int dir)
+ {
+ 	struct scsi_qla_host *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	void *sa_id_map;
+ 	unsigned long flags = 0;
+ 	u16 sa_index;
+ 
+ 	ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_id_map = ha->edif_tx_sa_id_map;
+ 	else
+ 		sa_id_map = ha->edif_rx_sa_id_map;
+ 
+ 	spin_lock_irqsave(&ha->sadb_fp_lock, flags);
+ 	sa_index = find_first_zero_bit(sa_id_map, EDIF_NUM_SA_INDEX);
+ 	if (sa_index >=  EDIF_NUM_SA_INDEX) {
+ 		spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 	set_bit(sa_index, sa_id_map);
+ 	spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 
+ 	if (dir)
+ 		sa_index += EDIF_TX_SA_INDEX_BASE;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: index retrieved from free pool %d\n", __func__, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* find an sadb entry for an nport_handle */
+ static struct edif_sa_index_entry *
+ qla_edif_sadb_find_sa_index_entry(uint16_t nport_handle,
+ 		struct list_head *sa_list)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct edif_sa_index_entry *tentry;
+ 	struct list_head *indx_list = sa_list;
+ 
+ 	list_for_each_entry_safe(entry, tentry, indx_list, next) {
+ 		if (entry->handle == nport_handle)
+ 			return entry;
+ 	}
+ 	return NULL;
+ }
+ 
+ /* remove an sa_index from the nport_handle and return it to the free pool */
+ static int qla_edif_sadb_delete_sa_index(fc_port_t *fcport, uint16_t nport_handle,
+ 		uint16_t sa_index)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	int dir = (sa_index < EDIF_TX_SA_INDEX_BASE) ? 0 : 1;
+ 	int slot = 0;
+ 	int free_slot_count = 0;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: no entry found for nport_handle 0x%x\n",
+ 		    __func__, nport_handle);
+ 		return -1;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	/*
+ 	 * each tx/rx direction has up to 2 sa indexes/slots. 1 slot for in flight traffic
+ 	 * the other is use at re-key time.
+ 	 */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == sa_index) {
+ 			entry->sa_pair[slot].sa_index = INVALID_EDIF_SA_INDEX;
+ 			entry->sa_pair[slot].spi = 0;
+ 			free_slot_count++;
+ 			qla_edif_add_sa_index_to_freepool(fcport, dir, sa_index);
+ 		} else if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot_count++;
+ 		}
+ 	}
+ 
+ 	if (free_slot_count == 2) {
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: sa_index %d removed, free_slot_count: %d\n",
+ 	    __func__, sa_index, free_slot_count);
+ 
+ 	return 0;
+ }
+ 
+ void
+ qla28xx_sa_update_iocb_entry(scsi_qla_host_t *v, struct req_que *req,
+ 	struct sa_update_28xx *pkt)
+ {
+ 	const char *func = "SA_UPDATE_RESPONSE_IOCB";
+ 	srb_t *sp;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	int old_sa_deleted = 1;
+ 	uint16_t nport_handle;
+ 	struct scsi_qla_host *vha;
+ 
+ 	sp = qla2x00_get_sp_from_handle(v, func, req, pkt);
+ 
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, v, 0x3063,
+ 			"%s: no sp found for pkt\n", __func__);
+ 		return;
+ 	}
+ 	/* use sp->vha due to npiv */
+ 	vha = sp->vha;
+ 
+ 	switch (pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * dig the nport handle out of the iocb, fcport->loop_id can not be trusted
+ 	 * to be correct during cleanup sa_update iocbs.
+ 	 */
+ 	nport_handle = sp->fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: %8phN comp status=%x old_sa_info=%x new_sa_info=%x lid %d, index=0x%x pkt_flags %xh hdl=%x\n",
+ 	    __func__, sp->fcport->port_name, pkt->u.comp_sts, pkt->old_sa_info, pkt->new_sa_info,
+ 	    nport_handle, pkt->sa_index, pkt->flags, sp->handle);
+ 
+ 	/* if rx delete, remove the timer */
+ 	if ((pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) ==  SA_FLAG_INVALIDATE) {
+ 		struct edif_list_entry *edif_entry;
+ 
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 
+ 		edif_entry = qla_edif_list_find_sa_index(sp->fcport, nport_handle);
+ 		if (edif_entry) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: removing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 			qla_edif_list_delete_sa_index(sp->fcport, edif_entry);
+ 			del_timer(&edif_entry->timer);
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: releasing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 
+ 			kfree(edif_entry);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * if this is a delete for either tx or rx, make sure it succeeded.
+ 	 * The new_sa_info field should be 0xffff on success
+ 	 */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE)
+ 		old_sa_deleted = (le16_to_cpu(pkt->new_sa_info) == 0xffff) ? 1 : 0;
+ 
+ 	/* Process update and delete the same way */
+ 
+ 	/* If this is an sadb cleanup delete, bypass sending events to IPSEC */
+ 	if (sp->flags & SRB_EDIF_CLEANUP_DELETE) {
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: nph 0x%x, sa_index %d removed from fw\n",
+ 		    __func__, sp->fcport->loop_id, pkt->sa_index);
+ 
+ 	} else if ((pkt->entry_status == 0) && (pkt->u.comp_sts == 0) &&
+ 	    old_sa_deleted) {
+ 		/*
+ 		 * Note: Wa are only keeping track of latest SA,
+ 		 * so we know when we can start enableing encryption per I/O.
+ 		 * If all SA's get deleted, let FW reject the IOCB.
+ 
+ 		 * TODO: edif: don't set enabled here I think
+ 		 * TODO: edif: prli complete is where it should be set
+ 		 */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			"SA(%x)updated for s_id %02x%02x%02x\n",
+ 			pkt->new_sa_info,
+ 			pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 		sp->fcport->edif.enable = 1;
+ 		if (pkt->flags & SA_FLAG_TX) {
+ 			sp->fcport->edif.tx_sa_set = 1;
+ 			sp->fcport->edif.tx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		} else {
+ 			sp->fcport->edif.rx_sa_set = 1;
+ 			sp->fcport->edif.rx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 		}
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: %8phN SA update FAILED: sa_index: %d, new_sa_info %d, %02x%02x%02x\n",
+ 		    __func__, sp->fcport->port_name, pkt->sa_index, pkt->new_sa_info,
+ 		    pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 
+ 		if (pkt->flags & SA_FLAG_TX)
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		else
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 	}
+ 
+ 	/* for delete, release sa_ctl, sa_index */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE) {
+ 		/* release the sa_ctl */
+ 		sa_ctl = qla_edif_find_sa_ctl_by_index(sp->fcport,
+ 		    le16_to_cpu(pkt->sa_index), (pkt->flags & SA_FLAG_TX));
+ 		if (sa_ctl &&
+ 		    qla_edif_find_sa_ctl_by_index(sp->fcport, sa_ctl->index,
+ 			(pkt->flags & SA_FLAG_TX)) != NULL) {
+ 			ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			    "%s: freeing sa_ctl for index %d\n",
+ 			    __func__, sa_ctl->index);
+ 			qla_edif_free_sa_ctl(sp->fcport, sa_ctl, sa_ctl->index);
+ 		} else {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: sa_ctl NOT freed, sa_ctl: %p\n",
+ 			    __func__, sa_ctl);
+ 		}
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, le16_to_cpu(pkt->sa_index), nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 	/*
+ 	 * check for a failed sa_update and remove
+ 	 * the sadb entry.
+ 	 */
+ 	} else if (pkt->u.comp_sts) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, pkt->sa_index, nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 	}
+ 
+ 	sp->done(sp, 0);
+ }
+ 
+ /**
+  * qla28xx_start_scsi_edif() - Send a SCSI type 6 command to the ISP
+  * @sp: command to send to the ISP
+  *
+  * Return: non-zero if a failure occurred, else zero.
+  */
+ int
+ qla28xx_start_scsi_edif(srb_t *sp)
+ {
+ 	int             nseg;
+ 	unsigned long   flags;
+ 	struct scsi_cmnd *cmd;
+ 	uint32_t        *clr_ptr;
+ 	uint32_t        index, i;
+ 	uint32_t        handle;
+ 	uint16_t        cnt;
+ 	int16_t        req_cnt;
+ 	uint16_t        tot_dsds;
+ 	__be32 *fcp_dl;
+ 	uint8_t additional_cdb_len;
+ 	struct ct6_dsd *ctx;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	struct cmd_type_6 *cmd_pkt;
+ 	struct dsd64	*cur_dsd;
+ 	uint8_t		avail_dsds = 0;
+ 	struct scatterlist *sg;
+ 	struct req_que *req = sp->qpair->req;
+ 	spinlock_t *lock = sp->qpair->qp_lock_ptr;
+ 
+ 	/* Setup device pointers. */
+ 	cmd = GET_CMD_SP(sp);
+ 
+ 	/* So we know we haven't pci_map'ed anything yet */
+ 	tot_dsds = 0;
+ 
+ 	/* Send marker if required */
+ 	if (vha->marker_needed != 0) {
+ 		if (qla2x00_marker(vha, sp->qpair, 0, 0, MK_SYNC_ALL) !=
+ 			QLA_SUCCESS) {
+ 			ql_log(ql_log_warn, vha, 0x300c,
+ 			    "qla2x00_marker failed for cmd=%p.\n", cmd);
+ 			return QLA_FUNCTION_FAILED;
+ 		}
+ 		vha->marker_needed = 0;
+ 	}
+ 
+ 	/* Acquire ring specific lock */
+ 	spin_lock_irqsave(lock, flags);
+ 
+ 	/* Check for room in outstanding command list. */
+ 	handle = req->current_outstanding_cmd;
+ 	for (index = 1; index < req->num_outstanding_cmds; index++) {
+ 		handle++;
+ 		if (handle == req->num_outstanding_cmds)
+ 			handle = 1;
+ 		if (!req->outstanding_cmds[handle])
+ 			break;
+ 	}
+ 	if (index == req->num_outstanding_cmds)
+ 		goto queuing_error;
+ 
+ 	/* Map the sg table so we have an accurate count of sg entries needed */
+ 	if (scsi_sg_count(cmd)) {
+ 		nseg = dma_map_sg(&ha->pdev->dev, scsi_sglist(cmd),
+ 		    scsi_sg_count(cmd), cmd->sc_data_direction);
+ 		if (unlikely(!nseg))
+ 			goto queuing_error;
+ 	} else {
+ 		nseg = 0;
+ 	}
+ 
+ 	tot_dsds = nseg;
+ 	req_cnt = qla24xx_calc_iocbs(vha, tot_dsds);
+ 	if (req->cnt < (req_cnt + 2)) {
+ 		cnt = IS_SHADOW_REG_CAPABLE(ha) ? *req->out_ptr :
+ 		    rd_reg_dword(req->req_q_out);
+ 		if (req->ring_index < cnt)
+ 			req->cnt = cnt - req->ring_index;
+ 		else
+ 			req->cnt = req->length -
+ 			    (req->ring_index - cnt);
+ 		if (req->cnt < (req_cnt + 2))
+ 			goto queuing_error;
+ 	}
+ 
+ 	ctx = sp->u.scmd.ct6_ctx =
+ 	    mempool_alloc(ha->ctx_mempool, GFP_ATOMIC);
+ 	if (!ctx) {
+ 		ql_log(ql_log_fatal, vha, 0x3010,
+ 		    "Failed to allocate ctx for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	memset(ctx, 0, sizeof(struct ct6_dsd));
+ 	ctx->fcp_cmnd = dma_pool_zalloc(ha->fcp_cmnd_dma_pool,
+ 	    GFP_ATOMIC, &ctx->fcp_cmnd_dma);
+ 	if (!ctx->fcp_cmnd) {
+ 		ql_log(ql_log_fatal, vha, 0x3011,
+ 		    "Failed to allocate fcp_cmnd for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	/* Initialize the DSD list and dma handle */
+ 	INIT_LIST_HEAD(&ctx->dsd_list);
+ 	ctx->dsd_use_cnt = 0;
+ 
+ 	if (cmd->cmd_len > 16) {
+ 		additional_cdb_len = cmd->cmd_len - 16;
+ 		if ((cmd->cmd_len % 4) != 0) {
+ 			/*
+ 			 * SCSI command bigger than 16 bytes must be
+ 			 * multiple of 4
+ 			 */
+ 			ql_log(ql_log_warn, vha, 0x3012,
+ 			    "scsi cmd len %d not multiple of 4 for cmd=%p.\n",
+ 			    cmd->cmd_len, cmd);
+ 			goto queuing_error_fcp_cmnd;
+ 		}
+ 		ctx->fcp_cmnd_len = 12 + cmd->cmd_len + 4;
+ 	} else {
+ 		additional_cdb_len = 0;
+ 		ctx->fcp_cmnd_len = 12 + 16 + 4;
+ 	}
+ 
+ 	cmd_pkt = (struct cmd_type_6 *)req->ring_ptr;
+ 	cmd_pkt->handle = make_handle(req->id, handle);
+ 
+ 	/*
+ 	 * Zero out remaining portion of packet.
+ 	 * tagged queuing modifier -- default is TSK_SIMPLE (0).
+ 	 */
+ 	clr_ptr = (uint32_t *)cmd_pkt + 2;
+ 	memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
+ 	cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
+ 
+ 	/* No data transfer */
+ 	if (!scsi_bufflen(cmd) || cmd->sc_data_direction == DMA_NONE) {
+ 		cmd_pkt->byte_count = cpu_to_le32(0);
+ 		goto no_dsds;
+ 	}
+ 
+ 	/* Set transfer direction */
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_WRITE_DATA);
+ 		vha->qla_stats.output_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.output_requests++;
+ 		sp->fcport->edif.tx_bytes += scsi_bufflen(cmd);
+ 	} else if (cmd->sc_data_direction == DMA_FROM_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_READ_DATA);
+ 		vha->qla_stats.input_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.input_requests++;
+ 		sp->fcport->edif.rx_bytes += scsi_bufflen(cmd);
+ 	}
+ 
+ 	cmd_pkt->control_flags |= cpu_to_le16(CF_EN_EDIF);
+ 	cmd_pkt->control_flags &= ~(cpu_to_le16(CF_NEW_SA));
+ 
+ 	/* One DSD is available in the Command Type 6 IOCB */
+ 	avail_dsds = 1;
+ 	cur_dsd = &cmd_pkt->fcp_dsd;
+ 
+ 	/* Load data segments */
+ 	scsi_for_each_sg(cmd, sg, tot_dsds, i) {
+ 		dma_addr_t      sle_dma;
+ 		cont_a64_entry_t *cont_pkt;
+ 
+ 		/* Allocate additional continuation packets? */
+ 		if (avail_dsds == 0) {
+ 			/*
+ 			 * Five DSDs are available in the Continuation
+ 			 * Type 1 IOCB.
+ 			 */
+ 			cont_pkt = qla2x00_prep_cont_type1_iocb(vha, req);
+ 			cur_dsd = cont_pkt->dsd;
+ 			avail_dsds = 5;
+ 		}
+ 
+ 		sle_dma = sg_dma_address(sg);
+ 		put_unaligned_le64(sle_dma, &cur_dsd->address);
+ 		cur_dsd->length = cpu_to_le32(sg_dma_len(sg));
+ 		cur_dsd++;
+ 		avail_dsds--;
+ 	}
+ 
+ no_dsds:
+ 	/* Set NPORT-ID and LUN number*/
+ 	cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
+ 	cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
+ 	cmd_pkt->vp_index = sp->vha->vp_idx;
+ 
+ 	cmd_pkt->entry_type = COMMAND_TYPE_6;
+ 
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 
+ 	int_to_scsilun(cmd->device->lun, &cmd_pkt->lun);
+ 	host_to_fcp_swap((uint8_t *)&cmd_pkt->lun, sizeof(cmd_pkt->lun));
+ 
+ 	/* build FCP_CMND IU */
+ 	int_to_scsilun(cmd->device->lun, &ctx->fcp_cmnd->lun);
+ 	ctx->fcp_cmnd->additional_cdb_len = additional_cdb_len;
+ 
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 1;
+ 	else if (cmd->sc_data_direction == DMA_FROM_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 2;
+ 
+ 	/* Populate the FCP_PRIO. */
+ 	if (ha->flags.fcp_prio_enabled)
+ 		ctx->fcp_cmnd->task_attribute |=
+ 		    sp->fcport->fcp_prio << 3;
+ 
+ 	memcpy(ctx->fcp_cmnd->cdb, cmd->cmnd, cmd->cmd_len);
+ 
+ 	fcp_dl = (__be32 *)(ctx->fcp_cmnd->cdb + 16 +
+ 	    additional_cdb_len);
+ 	*fcp_dl = htonl((uint32_t)scsi_bufflen(cmd));
+ 
+ 	cmd_pkt->fcp_cmnd_dseg_len = cpu_to_le16(ctx->fcp_cmnd_len);
+ 	put_unaligned_le64(ctx->fcp_cmnd_dma, &cmd_pkt->fcp_cmnd_dseg_address);
+ 
+ 	sp->flags |= SRB_FCP_CMND_DMA_VALID;
+ 	cmd_pkt->byte_count = cpu_to_le32((uint32_t)scsi_bufflen(cmd));
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 	cmd_pkt->entry_status = 0;
+ 
+ 	/* Build command packet. */
+ 	req->current_outstanding_cmd = handle;
+ 	req->outstanding_cmds[handle] = sp;
+ 	sp->handle = handle;
+ 	cmd->host_scribble = (unsigned char *)(unsigned long)handle;
+ 	req->cnt -= req_cnt;
+ 
+ 	/* Adjust ring index. */
+ 	wmb();
+ 	req->ring_index++;
+ 	if (req->ring_index == req->length) {
+ 		req->ring_index = 0;
+ 		req->ring_ptr = req->ring;
+ 	} else {
+ 		req->ring_ptr++;
+ 	}
+ 
+ 	sp->qpair->cmd_cnt++;
+ 	/* Set chip new ring index. */
+ 	wrt_reg_dword(req->req_q_in, req->ring_index);
+ 
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_SUCCESS;
+ 
+ queuing_error_fcp_cmnd:
+ 	dma_pool_free(ha->fcp_cmnd_dma_pool, ctx->fcp_cmnd, ctx->fcp_cmnd_dma);
+ queuing_error:
+ 	if (tot_dsds)
+ 		scsi_dma_unmap(cmd);
+ 
+ 	if (sp->u.scmd.ct6_ctx) {
+ 		mempool_free(sp->u.scmd.ct6_ctx, ha->ctx_mempool);
+ 		sp->u.scmd.ct6_ctx = NULL;
+ 	}
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_FUNCTION_FAILED;
+ }
+ 
+ /**********************************************
+  * edif update/delete sa_index list functions *
+  **********************************************/
+ 
+ /* clear the edif_indx_list for this port */
+ void qla_edif_list_del(fc_port_t *fcport)
+ {
+ 	struct edif_list_entry *indx_lst;
+ 	struct edif_list_entry *tindx_lst;
+ 	struct list_head *indx_list = &fcport->edif.edif_indx_list;
+ 	unsigned long flags = 0;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	list_for_each_entry_safe(indx_lst, tindx_lst, indx_list, next) {
+ 		list_del(&indx_lst->next);
+ 		kfree(indx_lst);
+ 	}
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ }
+ 
+ /******************
+  * SADB functions *
+  ******************/
+ 
+ /* allocate/retrieve an sa_index for a given spi */
+ static uint16_t qla_edif_sadb_get_sa_index(fc_port_t *fcport,
+ 		struct qla_sa_update_frame *sa_frame)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	uint16_t sa_index;
+ 	int dir = sa_frame->flags & SAU_FLG_TX;
+ 	int slot = 0;
+ 	int free_slot = -1;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry  fc_port: %p, nport_handle: 0x%x\n",
+ 	    __func__, fcport, nport_handle);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		if ((sa_frame->flags & (SAU_FLG_TX | SAU_FLG_INV)) == SAU_FLG_INV) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: rx delete request with no entry\n", __func__);
+ 			return RX_DELETE_NO_EDIF_SA_INDEX;
+ 		}
+ 
+ 		/* if there is no entry for this nport, add one */
+ 		entry = kzalloc((sizeof(struct edif_sa_index_entry)), GFP_ATOMIC);
+ 		if (!entry)
+ 			return INVALID_EDIF_SA_INDEX;
+ 
+ 		sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 		if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 			kfree(entry);
+ 			return INVALID_EDIF_SA_INDEX;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&entry->next);
+ 		entry->handle = nport_handle;
+ 		entry->fcport = fcport;
+ 		entry->sa_pair[0].spi = sa_frame->spi;
+ 		entry->sa_pair[0].sa_index = sa_index;
+ 		entry->sa_pair[1].spi = 0;
+ 		entry->sa_pair[1].sa_index = INVALID_EDIF_SA_INDEX;
+ 		spin_lock_irqsave(&ha->sadb_lock, flags);
+ 		list_add_tail(&entry->next, sa_list);
+ 		spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Created new sadb entry for nport_handle 0x%x, spi 0x%x, returning sa_index %d\n",
+ 		    __func__, nport_handle, sa_frame->spi, sa_index);
+ 
+ 		return sa_index;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 
+ 	/* see if we already have an entry for this spi */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot = slot;
+ 		} else {
+ 			if (entry->sa_pair[slot].spi == sa_frame->spi) {
+ 				spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 				ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 				    "%s: sadb slot %d entry for lid 0x%x, spi 0x%x found, sa_index %d\n",
+ 				    __func__, slot, entry->handle, sa_frame->spi,
+ 				    entry->sa_pair[slot].sa_index);
+ 				return entry->sa_pair[slot].sa_index;
+ 			}
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	/* both slots are used */
+ 	if (free_slot == -1) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: WARNING: No free slots in sadb for nport_handle 0x%x, spi: 0x%x\n",
+ 		    __func__, entry->handle, sa_frame->spi);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Slot 0  spi: 0x%x  sa_index: %d,  Slot 1  spi: 0x%x  sa_index: %d\n",
+ 		    __func__, entry->sa_pair[0].spi, entry->sa_pair[0].sa_index,
+ 		    entry->sa_pair[1].spi, entry->sa_pair[1].sa_index);
+ 
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	/* there is at least one free slot, use it */
+ 	sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 	if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 		ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 		    "%s: empty freepool!!\n", __func__);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	entry->sa_pair[free_slot].spi = sa_frame->spi;
+ 	entry->sa_pair[free_slot].sa_index = sa_index;
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 	ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 	    "%s: sadb slot %d entry for nport_handle 0x%x, spi 0x%x added, returning sa_index %d\n",
+ 	    __func__, free_slot, entry->handle, sa_frame->spi, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* release any sadb entries -- only done at teardown */
+ void qla_edif_sadb_release(struct qla_hw_data *ha)
+ {
+ 	struct list_head *pos;
+ 	struct list_head *tmp;
+ 	struct edif_sa_index_entry *entry;
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_rx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_tx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ }
+ 
+ /**************************
+  * sadb freepool functions
+  **************************/
+ 
+ /* build the rx and tx sa_index free pools -- only done at fcport init */
+ int qla_edif_sadb_build_free_pool(struct qla_hw_data *ha)
+ {
+ 	ha->edif_tx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 
+ 	if (!ha->edif_tx_sa_id_map) {
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb tx.\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ha->edif_rx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 	if (!ha->edif_rx_sa_id_map) {
+ 		kfree(ha->edif_tx_sa_id_map);
+ 		ha->edif_tx_sa_id_map = NULL;
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb rx.\n");
+ 		return -ENOMEM;
+ 	}
+ 	return 0;
+ }
+ 
+ /* release the free pool - only done during fcport teardown */
+ void qla_edif_sadb_release_free_pool(struct qla_hw_data *ha)
+ {
+ 	kfree(ha->edif_tx_sa_id_map);
+ 	ha->edif_tx_sa_id_map = NULL;
+ 	kfree(ha->edif_rx_sa_id_map);
+ 	ha->edif_rx_sa_id_map = NULL;
+ }
+ 
+ static void __chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		fc_port_t *fcport, uint32_t handle, uint16_t sa_index)
+ {
+ 	struct edif_list_entry *edif_entry;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	uint16_t delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 	uint16_t cached_nport_handle;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	edif_entry = qla_edif_list_find_sa_index(fcport, nport_handle);
+ 	if (!edif_entry) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;		/* no pending delete for this handle */
+ 	}
+ 
+ 	/*
+ 	 * check for no pending delete for this index or iocb does not
+ 	 * match rx sa_index
+ 	 */
+ 	if (edif_entry->delete_sa_index == INVALID_EDIF_SA_INDEX ||
+ 	    edif_entry->update_sa_index != sa_index) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * wait until we have seen at least EDIF_DELAY_COUNT transfers before
+ 	 * queueing RX delete
+ 	 */
+ 	if (edif_entry->count++ < EDIF_RX_DELETE_FILTER_COUNT) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 	    "%s: invalidating delete_sa_index,  update_sa_index: 0x%x sa_index: 0x%x, delete_sa_index: 0x%x\n",
+ 	    __func__, edif_entry->update_sa_index, sa_index, edif_entry->delete_sa_index);
+ 
+ 	delete_sa_index = edif_entry->delete_sa_index;
+ 	edif_entry->delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	cached_nport_handle = edif_entry->handle;
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 
+ 	/* sanity check on the nport handle */
+ 	if (nport_handle != cached_nport_handle) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE nport_handle mismatch: lid: 0x%x, edif_entry nph: 0x%x\n",
+ 		    __func__, nport_handle, cached_nport_handle);
+ 	}
+ 
+ 	/* find the sa_ctl for the delete and schedule the delete */
+ 	sa_ctl = qla_edif_find_sa_ctl_by_index(fcport, delete_sa_index, 0);
+ 	if (sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl: %p, index recvd %d\n",
+ 		    __func__, sa_ctl, sa_index);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "delete index %d, update index: %d, nport handle: 0x%x, handle: 0x%x\n",
+ 		    delete_sa_index,
+ 		    edif_entry->update_sa_index, nport_handle, handle);
+ 
+ 		sa_ctl->flags = EDIF_SA_CTL_FLG_DEL;
+ 		set_bit(EDIF_SA_CTL_REPL, &sa_ctl->state);
+ 		qla_post_sa_replace_work(fcport->vha, fcport,
+ 		    nport_handle, sa_ctl);
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl not found for delete_sa_index: %d\n",
+ 		    __func__, delete_sa_index);
+ 	}
+ }
+ 
+ void qla_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		srb_t *sp, struct sts_entry_24xx *sts24)
+ {
+ 	fc_port_t *fcport = sp->fcport;
+ 	/* sa_index used by this iocb */
+ 	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
+ 	uint32_t handle;
+ 
+ 	handle = (uint32_t)LSW(sts24->handle);
+ 
+ 	/* find out if this status iosb is for a scsi read */
+ 	if (cmd->sc_data_direction != DMA_FROM_DEVICE)
+ 		return;
+ 
+ 	return __chk_edif_rx_sa_delete_pending(vha, fcport, handle,
+ 	   le16_to_cpu(sts24->edif_sa_index));
+ }
+ 
+ void qlt_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha, fc_port_t *fcport,
+ 		struct ctio7_from_24xx *pkt)
+ {
+ 	__chk_edif_rx_sa_delete_pending(vha, fcport,
+ 	    pkt->handle, le16_to_cpu(pkt->edif_sa_index));
+ }
+ 
+ static void qla_parse_auth_els_ctl(struct srb *sp)
+ {
+ 	struct qla_els_pt_arg *a = &sp->u.bsg_cmd.u.els_arg;
+ 	struct bsg_job *bsg_job = sp->u.bsg_cmd.bsg_job;
+ 	struct fc_bsg_request *request = bsg_job->request;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	a->tx_len = a->tx_byte_count = sp->remap.req.len;
+ 	a->tx_addr = sp->remap.req.dma;
+ 	a->rx_len = a->rx_byte_count = sp->remap.rsp.len;
+ 	a->rx_addr = sp->remap.rsp.dma;
+ 
+ 	if (p->e.sub_cmd == SEND_ELS_REPLY) {
+ 		a->control_flags = p->e.extra_control_flags << 13;
+ 		a->rx_xchg_address = cpu_to_le32(p->e.extra_rx_xchg_address);
+ 		if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_ACC)
+ 			a->els_opcode = ELS_LS_ACC;
+ 		else if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_RJT)
+ 			a->els_opcode = ELS_LS_RJT;
+ 	}
+ 	a->did = sp->fcport->d_id;
+ 	a->els_opcode =  request->rqst_data.h_els.command_code;
+ 	a->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	a->vp_idx = sp->vha->vp_idx;
+ }
+ 
+ int qla_edif_process_els(scsi_qla_host_t *vha, struct bsg_job *bsg_job)
+ {
+ 	struct fc_bsg_request *bsg_request = bsg_job->request;
+ 	struct fc_bsg_reply *bsg_reply = bsg_job->reply;
+ 	fc_port_t *fcport = NULL;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	srb_t *sp;
+ 	int rval =  (DID_ERROR << 16);
+ 	port_id_t d_id;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	d_id.b.al_pa = bsg_request->rqst_data.h_els.port_id[2];
+ 	d_id.b.area = bsg_request->rqst_data.h_els.port_id[1];
+ 	d_id.b.domain = bsg_request->rqst_data.h_els.port_id[0];
+ 
+ 	/* find matching d_id in fcport list */
+ 	fcport = qla2x00_find_fcport_by_pid(vha, &d_id);
+ 	if (!fcport) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x911a,
+ 		    "%s fcport not find online portid=%06x.\n",
+ 		    __func__, d_id.b24);
+ 		SET_DID_STATUS(bsg_reply->result, DID_ERROR);
+ 		return -EIO;
+ 	}
+ 
+ 	if (qla_bsg_check(vha, bsg_job, fcport))
+ 		return 0;
+ 
+ 	if (fcport->loop_id == FC_NO_LOOP_ID) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910d,
+ 		    "%s ELS code %x, no loop id.\n", __func__,
+ 		    bsg_request->rqst_data.r_els.els_code);
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		return -ENXIO;
+ 	}
+ 
+ 	if (!vha->flags.online) {
+ 		ql_log(ql_log_warn, vha, 0x7005, "Host not online.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EIO;
+ 		goto done;
+ 	}
+ 
+ 	/* pass through is supported only for ISP 4Gb or higher */
+ 	if (!IS_FWI2_CAPABLE(ha)) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7001,
+ 		    "ELS passthru not supported for ISP23xx based adapters.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EPERM;
+ 		goto done;
+ 	}
+ 
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7004,
+ 		    "Failed get sp pid=%06x\n", fcport->d_id.b24);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done;
+ 	}
+ 
+ 	sp->remap.req.len = bsg_job->request_payload.payload_len;
+ 	sp->remap.req.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.req.dma);
+ 	if (!sp->remap.req.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7005,
+ 		    "Failed allocate request dma len=%x\n",
+ 		    bsg_job->request_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->remap.rsp.len = bsg_job->reply_payload.payload_len;
+ 	sp->remap.rsp.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.rsp.dma);
+ 	if (!sp->remap.rsp.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7006,
+ 		    "Failed allocate response dma len=%x\n",
+ 		    bsg_job->reply_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_remap_req;
+ 	}
+ 	sg_copy_to_buffer(bsg_job->request_payload.sg_list,
+ 	    bsg_job->request_payload.sg_cnt, sp->remap.req.buf,
+ 	    sp->remap.req.len);
+ 	sp->remap.remapped = true;
+ 
+ 	sp->type = SRB_ELS_CMD_HST_NOLOGIN;
+ 	sp->name = "SPCN_BSG_HST_NOLOGIN";
+ 	sp->u.bsg_cmd.bsg_job = bsg_job;
+ 	qla_parse_auth_els_ctl(sp);
+ 
+ 	sp->free = qla2x00_bsg_sp_free;
+ 	sp->done = qla2x00_bsg_job_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x700a,
+ 	    "%s %s %8phN xchg %x ctlflag %x hdl %x reqlen %xh bsg ptr %p\n",
+ 	    __func__, sc_to_str(p->e.sub_cmd), fcport->port_name,
+ 	    p->e.extra_rx_xchg_address, p->e.extra_control_flags,
+ 	    sp->handle, sp->remap.req.len, bsg_job);
+ 
+ 	if (rval != QLA_SUCCESS) {
+ 		ql_log(ql_log_warn, vha, 0x700e,
+ 		    "qla2x00_start_sp failed = %d\n", rval);
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		rval = -EIO;
+ 		goto done_free_remap_rsp;
+ 	}
+ 	return rval;
+ 
+ done_free_remap_rsp:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.rsp.buf,
+ 	    sp->remap.rsp.dma);
+ done_free_remap_req:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.req.buf,
+ 	    sp->remap.req.dma);
+ done_free_sp:
+ 	qla2x00_rel_sp(sp);
+ 
+ done:
+ 	return rval;
+ }
+ 
+ void qla_edif_sess_down(struct scsi_qla_host *vha, struct fc_port *sess)
+ {
+ 	if (sess->edif.app_sess_online && vha->e_dbell.db_flags & EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xf09c,
+ 			"%s: sess %8phN send port_offline event\n",
+ 			__func__, sess->port_name);
+ 		sess->edif.app_sess_online = 0;
+ 		qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SESSION_SHUTDOWN,
+ 		    sess->d_id.b24, 0, sess);
+ 		qla2x00_post_aen_work(vha, FCH_EVT_PORT_OFFLINE, sess->d_id.b24);
+ 	}
++>>>>>>> 71bef5020cd1 (scsi: qla2xxx: edif: Increment command and completion counts)
  }
diff --cc drivers/scsi/qla2xxx/qla_isr.c
index 6e542510f034,e8928fd83049..000000000000
--- a/drivers/scsi/qla2xxx/qla_isr.c
+++ b/drivers/scsi/qla2xxx/qla_isr.c
@@@ -2988,6 -3193,9 +2988,12 @@@ qla2x00_status_entry(scsi_qla_host_t *v
  	}
  
  	/* Fast path completion. */
++<<<<<<< HEAD
++=======
+ 	qla_chk_edif_rx_sa_delete_pending(vha, sp, sts24);
+ 	sp->qpair->cmd_completion_cnt++;
+ 
++>>>>>>> 71bef5020cd1 (scsi: qla2xxx: edif: Increment command and completion counts)
  	if (comp_status == CS_COMPLETE && scsi_status == 0) {
  		qla2x00_process_completed_request(vha, req, handle);
  
* Unmerged path drivers/scsi/qla2xxx/qla_edif.c
* Unmerged path drivers/scsi/qla2xxx/qla_isr.c
