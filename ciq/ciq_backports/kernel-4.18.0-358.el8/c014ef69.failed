locking/rtmutex: Add wake_state to rt_mutex_waiter

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit c014ef69b3acdb8c9e7fc412e96944f4d5c36fa0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/c014ef69.failed

Regular sleeping locks like mutexes, rtmutexes and rw_semaphores are always
entering and leaving a blocking section with task state == TASK_RUNNING.

On a non-RT kernel spinlocks and rwlocks never affect the task state, but
on RT kernels these locks are converted to rtmutex based 'sleeping' locks.

So in case of contention the task goes to block, which requires to carefully
preserve the task state, and restore it after acquiring the lock taking
regular wakeups for the task into account, which happened while the task was
blocked. This state preserving is achieved by having a separate task state
for blocking on a RT spin/rwlock and a saved_state field in task_struct
along with careful handling of these wakeup scenarios in try_to_wake_up().

To avoid conditionals in the rtmutex code, store the wake state which has
to be used for waking a lock waiter in rt_mutex_waiter which allows to
handle the regular and RT spin/rwlocks by handing it to wake_up_state().

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20210815211303.079800739@linutronix.de
(cherry picked from commit c014ef69b3acdb8c9e7fc412e96944f4d5c36fa0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/rtmutex_common.h
diff --cc kernel/locking/rtmutex_common.h
index 0f314a21d6ca,fcc55de46b28..000000000000
--- a/kernel/locking/rtmutex_common.h
+++ b/kernel/locking/rtmutex_common.h
@@@ -32,7 -33,8 +33,12 @@@ struct rt_mutex_waiter 
  	struct rb_node		tree_entry;
  	struct rb_node		pi_tree_entry;
  	struct task_struct	*task;
++<<<<<<< HEAD
 +	struct rt_mutex		*lock;
++=======
+ 	struct rt_mutex_base	*lock;
+ 	unsigned int		wake_state;
++>>>>>>> c014ef69b3ac (locking/rtmutex: Add wake_state to rt_mutex_waiter)
  	int			prio;
  	u64			deadline;
  };
@@@ -161,9 -164,15 +168,15 @@@ static inline void rt_mutex_init_waiter
  	waiter->task = NULL;
  }
  
+ static inline void rtlock_init_rtmutex_waiter(struct rt_mutex_waiter *waiter)
+ {
+ 	rt_mutex_init_waiter(waiter);
+ 	waiter->wake_state = TASK_RTLOCK_WAIT;
+ }
+ 
  #else /* CONFIG_RT_MUTEXES */
  /* Used in rcu/tree_plugin.h */
 -static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)
 +static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)
  {
  	return NULL;
  }
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index dc54c0c2b3d8..526a34db7ac0 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -691,7 +691,7 @@ static int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,
 		 * to get the lock.
 		 */
 		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
-			wake_up_process(rt_mutex_top_waiter(lock)->task);
+			wake_up_state(waiter->task, waiter->wake_state);
 		raw_spin_unlock_irq(&lock->wait_lock);
 		return 0;
 	}
* Unmerged path kernel/locking/rtmutex_common.h
