io-mapping: Cleanup atomic iomap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 351191ad55c8a1eccaf23e4187c62056229c0779
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/351191ad.failed

Switch the atomic iomap implementation over to kmap_local and stick the
preempt/pagefault mechanics into the generic code similar to the
kmap_atomic variants.

Rename the x86 map function in preparation for a non-atomic variant.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Linus Torvalds <torvalds@linuxfoundation.org>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
Link: https://lore.kernel.org/r/20201103095858.625310005@linutronix.de

(cherry picked from commit 351191ad55c8a1eccaf23e4187c62056229c0779)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/iomap.h
#	arch/x86/mm/iomap_32.c
#	include/linux/io-mapping.h
diff --cc arch/x86/include/asm/iomap.h
index 363e33eb6ec1,e2de092fc38c..000000000000
--- a/arch/x86/include/asm/iomap.h
+++ b/arch/x86/include/asm/iomap.h
@@@ -22,20 -9,14 +22,24 @@@
  #include <linux/fs.h>
  #include <linux/mm.h>
  #include <linux/uaccess.h>
 -#include <linux/highmem.h>
  #include <asm/cacheflush.h>
 +#include <asm/pgtable.h>
  #include <asm/tlbflush.h>
  
++<<<<<<< HEAD
 +void __iomem *
 +iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
 +
 +void
 +iounmap_atomic(void __iomem *kvaddr);
++=======
+ void __iomem *__iomap_local_pfn_prot(unsigned long pfn, pgprot_t prot);
++>>>>>>> 351191ad55c8 (io-mapping: Cleanup atomic iomap)
  
 -int iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
 +int
 +iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
  
 -void iomap_free(resource_size_t base, unsigned long size);
 +void
 +iomap_free(resource_size_t base, unsigned long size);
  
  #endif /* _ASM_X86_IOMAP_H */
diff --cc arch/x86/mm/iomap_32.c
index 1dc9a1bb31c3,9aaa756ddf21..000000000000
--- a/arch/x86/mm/iomap_32.c
+++ b/arch/x86/mm/iomap_32.c
@@@ -57,28 -44,7 +57,32 @@@ void iomap_free(resource_size_t base, u
  }
  EXPORT_SYMBOL_GPL(iomap_free);
  
++<<<<<<< HEAD
 +void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 +{
 +	unsigned long vaddr;
 +	int idx, type;
 +
 +	preempt_disable();
 +	pagefault_disable();
 +
 +	type = kmap_atomic_idx_push();
 +	idx = type + KM_TYPE_NR * smp_processor_id();
 +	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 +	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
 +	arch_flush_lazy_mmu_mode();
 +
 +	return (void *)vaddr;
 +}
 +
 +/*
 + * Map 'pfn' using protections 'prot'
 + */
 +void __iomem *
 +iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
++=======
+ void __iomem *__iomap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
++>>>>>>> 351191ad55c8 (io-mapping: Cleanup atomic iomap)
  {
  	/*
  	 * For non-PAT systems, translate non-WB request to UC- just in
@@@ -94,36 -60,6 +98,42 @@@
  	/* Filter out unsupported __PAGE_KERNEL* bits: */
  	pgprot_val(prot) &= __default_kernel_pte_mask;
  
++<<<<<<< HEAD
 +	return (void __force __iomem *) kmap_atomic_prot_pfn(pfn, prot);
 +}
 +EXPORT_SYMBOL_GPL(iomap_atomic_prot_pfn);
 +
 +void
 +iounmap_atomic(void __iomem *kvaddr)
 +{
 +	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 +
 +	if (vaddr >= __fix_to_virt(FIX_KMAP_END) &&
 +	    vaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {
 +		int idx, type;
 +
 +		type = kmap_atomic_idx();
 +		idx = type + KM_TYPE_NR * smp_processor_id();
 +
 +#ifdef CONFIG_DEBUG_HIGHMEM
 +		WARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
 +#endif
 +		/*
 +		 * Force other mappings to Oops if they'll try to access this
 +		 * pte without first remap it.  Keeping stale mappings around
 +		 * is a bad idea also, in case the page changes cacheability
 +		 * attributes or becomes a protected page in a hypervisor.
 +		 */
 +		kpte_clear_flush(kmap_pte-idx, vaddr);
 +		kmap_atomic_idx_pop();
 +	}
 +
 +	pagefault_enable();
 +	preempt_enable();
 +}
 +EXPORT_SYMBOL_GPL(iounmap_atomic);
++=======
+ 	return (void __force __iomem *)__kmap_local_pfn_prot(pfn, prot);
+ }
+ EXPORT_SYMBOL_GPL(__iomap_local_pfn_prot);
++>>>>>>> 351191ad55c8 (io-mapping: Cleanup atomic iomap)
diff --cc include/linux/io-mapping.h
index 58df02bd93c9,60e7c83e4904..000000000000
--- a/include/linux/io-mapping.h
+++ b/include/linux/io-mapping.h
@@@ -80,8 -69,9 +80,14 @@@ io_mapping_map_atomic_wc(struct io_mapp
  
  	BUG_ON(offset >= mapping->size);
  	phys_addr = mapping->base + offset;
++<<<<<<< HEAD
 +	pfn = (unsigned long) (phys_addr >> PAGE_SHIFT);
 +	return iomap_atomic_prot_pfn(pfn, mapping->prot);
++=======
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	return __iomap_local_pfn_prot(PHYS_PFN(phys_addr), mapping->prot);
++>>>>>>> 351191ad55c8 (io-mapping: Cleanup atomic iomap)
  }
  
  static inline void
* Unmerged path arch/x86/include/asm/iomap.h
* Unmerged path arch/x86/mm/iomap_32.c
* Unmerged path include/linux/io-mapping.h
