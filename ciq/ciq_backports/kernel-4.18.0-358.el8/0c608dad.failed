x86/process: Add AVX-512 usage elapsed time to /proc/pid/arch_status

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Aubrey Li <aubrey.li@linux.intel.com>
commit 0c608dad2a771c0a11b6d12148d1a8b975e015d4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/0c608dad.failed

AVX-512 components usage can result in turbo frequency drop. So it's useful
to expose AVX-512 usage elapsed time as a heuristic hint for user space job
schedulers to cluster the AVX-512 using tasks together.

Examples:
$ while [ 1 ]; do cat /proc/tid/arch_status | grep AVX512; sleep 1; done
AVX512_elapsed_ms:      4
AVX512_elapsed_ms:      8
AVX512_elapsed_ms:      4

This means that 4 milliseconds have elapsed since the tsks AVX512 usage was
detected when the task was scheduled out.

$ cat /proc/tid/arch_status | grep AVX512
AVX512_elapsed_ms:      -1

'-1' indicates that no AVX512 usage was recorded for this task.

The time exposed is not necessarily accurate when the arch_status file is
read as the AVX512 usage is only evaluated when a task is scheduled
out. Accurate usage information can be obtained with performance counters.

[ tglx: Massaged changelog ]

	Signed-off-by: Aubrey Li <aubrey.li@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: akpm@linux-foundation.org
	Cc: peterz@infradead.org
	Cc: hpa@zytor.com
	Cc: ak@linux.intel.com
	Cc: tim.c.chen@linux.intel.com
	Cc: dave.hansen@intel.com
	Cc: arjan@linux.intel.com
	Cc: adobriyan@gmail.com
	Cc: aubrey.li@intel.com
	Cc: linux-api@vger.kernel.org
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Arjan van de Ven <arjan@linux.intel.com>
	Cc: Alexey Dobriyan <adobriyan@gmail.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Linux API <linux-api@vger.kernel.org>
Link: https://lkml.kernel.org/r/20190606012236.9391-2-aubrey.li@linux.intel.com

(cherry picked from commit 0c608dad2a771c0a11b6d12148d1a8b975e015d4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/Kconfig
index 59d902bb0cb0,8a49b4b03f6b..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -225,8 -217,7 +225,12 @@@ config X8
  	select USER_STACKTRACE_SUPPORT
  	select VIRT_TO_BUS
  	select X86_FEATURE_NAMES		if PROC_FS
++<<<<<<< HEAD
 +	select HAVE_ARCH_KCSAN			if X86_64
 +	imply IMA_SECURE_AND_OR_TRUSTED_BOOT    if EFI
++=======
+ 	select PROC_PID_ARCH_STATUS		if PROC_FS
++>>>>>>> 0c608dad2a77 (x86/process: Add AVX-512 usage elapsed time to /proc/pid/arch_status)
  
  config INSTRUCTION_DECODER
  	def_bool y
diff --cc arch/x86/kernel/fpu/xstate.c
index 06b993b18d08,591ddde3b3e8..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1342,186 -1243,47 +1344,233 @@@ int copy_user_to_xstate(struct xregs_st
  	return 0;
  }
  
++<<<<<<< HEAD
 +/*
 + * Save only supervisor states to the kernel buffer.  This blows away all
 + * old states, and is intended to be used only in __fpu__restore_sig(), where
 + * user states are restored from the user buffer.
 + */
 +void copy_supervisor_to_kernel(struct xregs_state *xstate)
 +{
 +	struct xstate_header *header;
 +	u64 max_bit, min_bit;
 +	u32 lmask, hmask;
 +	int err, i;
 +
 +	if (WARN_ON(!boot_cpu_has(X86_FEATURE_XSAVES)))
 +		return;
 +
 +	if (!xfeatures_mask_supervisor())
 +		return;
 +
 +	max_bit = __fls(xfeatures_mask_supervisor());
 +	min_bit = __ffs(xfeatures_mask_supervisor());
 +
 +	lmask = xfeatures_mask_supervisor();
 +	hmask = xfeatures_mask_supervisor() >> 32;
 +	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
 +
 +	/* We should never fault when copying to a kernel buffer: */
 +	if (WARN_ON_FPU(err))
 +		return;
 +
 +	/*
 +	 * At this point, the buffer has only supervisor states and must be
 +	 * converted back to normal kernel format.
 +	 */
 +	header = &xstate->header;
 +	header->xcomp_bv |= xfeatures_mask_all;
 +
 +	/*
 +	 * This only moves states up in the buffer.  Start with
 +	 * the last state and move backwards so that states are
 +	 * not overwritten until after they are moved.  Note:
 +	 * memmove() allows overlapping src/dst buffers.
 +	 */
 +	for (i = max_bit; i >= min_bit; i--) {
 +		u8 *xbuf = (u8 *)xstate;
 +
 +		if (!((header->xfeatures >> i) & 1))
 +			continue;
 +
 +		/* Move xfeature 'i' into its normal location */
 +		memmove(xbuf + xstate_comp_offsets[i],
 +			xbuf + xstate_supervisor_only_offsets[i],
 +			xstate_sizes[i]);
 +	}
 +}
 +
 +/**
 + * copy_dynamic_supervisor_to_kernel() - Save dynamic supervisor states to
 + *                                       an xsave area
 + * @xstate: A pointer to an xsave area
 + * @mask: Represent the dynamic supervisor features saved into the xsave area
 + *
 + * Only the dynamic supervisor states sets in the mask are saved into the xsave
 + * area (See the comment in XFEATURE_MASK_DYNAMIC for the details of dynamic
 + * supervisor feature). Besides the dynamic supervisor states, the legacy
 + * region and XSAVE header are also saved into the xsave area. The supervisor
 + * features in the XFEATURE_MASK_SUPERVISOR_SUPPORTED and
 + * XFEATURE_MASK_SUPERVISOR_UNSUPPORTED are not saved.
 + *
 + * The xsave area must be 64-bytes aligned.
 + */
 +void copy_dynamic_supervisor_to_kernel(struct xregs_state *xstate, u64 mask)
 +{
 +	u64 dynamic_mask = xfeatures_mask_dynamic() & mask;
 +	u32 lmask, hmask;
 +	int err;
 +
 +	if (WARN_ON_FPU(!boot_cpu_has(X86_FEATURE_XSAVES)))
 +		return;
 +
 +	if (WARN_ON_FPU(!dynamic_mask))
 +		return;
 +
 +	lmask = dynamic_mask;
 +	hmask = dynamic_mask >> 32;
 +
 +	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
 +
 +	/* Should never fault when copying to a kernel buffer */
 +	WARN_ON_FPU(err);
 +}
 +
 +/**
 + * copy_kernel_to_dynamic_supervisor() - Restore dynamic supervisor states from
 + *                                       an xsave area
 + * @xstate: A pointer to an xsave area
 + * @mask: Represent the dynamic supervisor features restored from the xsave area
 + *
 + * Only the dynamic supervisor states sets in the mask are restored from the
 + * xsave area (See the comment in XFEATURE_MASK_DYNAMIC for the details of
 + * dynamic supervisor feature). Besides the dynamic supervisor states, the
 + * legacy region and XSAVE header are also restored from the xsave area. The
 + * supervisor features in the XFEATURE_MASK_SUPERVISOR_SUPPORTED and
 + * XFEATURE_MASK_SUPERVISOR_UNSUPPORTED are not restored.
 + *
 + * The xsave area must be 64-bytes aligned.
 + */
 +void copy_kernel_to_dynamic_supervisor(struct xregs_state *xstate, u64 mask)
 +{
 +	u64 dynamic_mask = xfeatures_mask_dynamic() & mask;
 +	u32 lmask, hmask;
 +	int err;
 +
 +	if (WARN_ON_FPU(!boot_cpu_has(X86_FEATURE_XSAVES)))
 +		return;
 +
 +	if (WARN_ON_FPU(!dynamic_mask))
 +		return;
 +
 +	lmask = dynamic_mask;
 +	hmask = dynamic_mask >> 32;
 +
 +	XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
 +
 +	/* Should never fault when copying from a kernel buffer */
 +	WARN_ON_FPU(err);
 +}
 +
 +#ifdef CONFIG_IOMMU_SUPPORT
 +void update_pasid(void)
 +{
 +	u64 pasid_state;
 +	u32 pasid;
 +
 +	if (!cpu_feature_enabled(X86_FEATURE_ENQCMD))
 +		return;
 +
 +	if (!current->mm)
 +		return;
 +
 +	pasid = READ_ONCE(current->mm->pasid);
 +	/* Set the valid bit in the PASID MSR/state only for valid pasid. */
 +	pasid_state = pasid == PASID_DISABLED ?
 +		      pasid : pasid | MSR_IA32_PASID_VALID;
 +
 +	/*
 +	 * No need to hold fregs_lock() since the task's fpstate won't
 +	 * be changed by others (e.g. ptrace) while the task is being
 +	 * switched to or is in IPI.
 +	 */
 +	if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
 +		/* The MSR is active and can be directly updated. */
 +		wrmsrl(MSR_IA32_PASID, pasid_state);
 +	} else {
 +		struct fpu *fpu = &current->thread.fpu;
 +		struct ia32_pasid_state *ppasid_state;
 +		struct xregs_state *xsave;
 +
 +		/*
 +		 * The CPU's xstate registers are not currently active. Just
 +		 * update the PASID state in the memory buffer here. The
 +		 * PASID MSR will be loaded when returning to user mode.
 +		 */
 +		xsave = &fpu->state.xsave;
 +		xsave->header.xfeatures |= XFEATURE_MASK_PASID;
 +		ppasid_state = get_xsave_addr(xsave, XFEATURE_PASID);
 +		/*
 +		 * Since XFEATURE_MASK_PASID is set in xfeatures, ppasid_state
 +		 * won't be NULL and no need to check its value.
 +		 *
 +		 * Only update the task's PASID state when it's different
 +		 * from the mm's pasid.
 +		 */
 +		if (ppasid_state->pasid != pasid_state) {
 +			/*
 +			 * Invalid fpregs so that state restoring will pick up
 +			 * the PASID state.
 +			 */
 +			__fpu_invalidate_fpregs_state(fpu);
 +			ppasid_state->pasid = pasid_state;
 +		}
 +	}
 +}
 +#endif /* CONFIG_IOMMU_SUPPORT */
++=======
+ #ifdef CONFIG_PROC_PID_ARCH_STATUS
+ /*
+  * Report the amount of time elapsed in millisecond since last AVX512
+  * use in the task.
+  */
+ static void avx512_status(struct seq_file *m, struct task_struct *task)
+ {
+ 	unsigned long timestamp = READ_ONCE(task->thread.fpu.avx512_timestamp);
+ 	long delta;
+ 
+ 	if (!timestamp) {
+ 		/*
+ 		 * Report -1 if no AVX512 usage
+ 		 */
+ 		delta = -1;
+ 	} else {
+ 		delta = (long)(jiffies - timestamp);
+ 		/*
+ 		 * Cap to LONG_MAX if time difference > LONG_MAX
+ 		 */
+ 		if (delta < 0)
+ 			delta = LONG_MAX;
+ 		delta = jiffies_to_msecs(delta);
+ 	}
+ 
+ 	seq_put_decimal_ll(m, "AVX512_elapsed_ms:\t", delta);
+ 	seq_putc(m, '\n');
+ }
+ 
+ /*
+  * Report architecture specific information
+  */
+ int proc_pid_arch_status(struct seq_file *m, struct pid_namespace *ns,
+ 			struct pid *pid, struct task_struct *task)
+ {
+ 	/*
+ 	 * Report AVX512 state if the processor and build option supported.
+ 	 */
+ 	if (cpu_feature_enabled(X86_FEATURE_AVX512F))
+ 		avx512_status(m, task);
+ 
+ 	return 0;
+ }
+ #endif /* CONFIG_PROC_PID_ARCH_STATUS */
++>>>>>>> 0c608dad2a77 (x86/process: Add AVX-512 usage elapsed time to /proc/pid/arch_status)
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/kernel/fpu/xstate.c
