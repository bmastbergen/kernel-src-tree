x86: Fix various typos in comments, take #2

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit 163b099146b85d1b05bd2eaa045acbeee25c29e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/163b0991.failed

Fix another ~42 single-word typos in arch/x86/ code comments,
missed a few in the first pass, in particular in .S files.

	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Bjorn Helgaas <bhelgaas@google.com>
	Cc: linux-kernel@vger.kernel.org
(cherry picked from commit 163b099146b85d1b05bd2eaa045acbeee25c29e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/set_memory.h
#	arch/x86/kernel/apm_32.c
#	arch/x86/kernel/smp.c
#	arch/x86/kernel/umip.c
#	arch/x86/mm/fault.c
#	arch/x86/xen/mmu_pv.c
diff --cc arch/x86/include/asm/set_memory.h
index 54bf92016155,43fa081a1adb..000000000000
--- a/arch/x86/include/asm/set_memory.h
+++ b/arch/x86/include/asm/set_memory.h
@@@ -8,8 -8,8 +8,13 @@@
  /*
   * The set_memory_* API can be used to change various attributes of a virtual
   * address range. The attributes include:
++<<<<<<< HEAD
 + * Cachability   : UnCached, WriteCombining, WriteThrough, WriteBack
 + * Executability : eXeutable, NoteXecutable
++=======
+  * Cacheability  : UnCached, WriteCombining, WriteThrough, WriteBack
+  * Executability : eXecutable, NoteXecutable
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
   * Read/Write    : ReadOnly, ReadWrite
   * Presence      : NotPresent
   * Encryption    : Encrypted, Decrypted
diff --cc arch/x86/kernel/apm_32.c
index ec00d1ff5098,241dda687eb9..000000000000
--- a/arch/x86/kernel/apm_32.c
+++ b/arch/x86/kernel/apm_32.c
@@@ -1034,7 -1025,7 +1034,11 @@@ static int apm_enable_power_management(
   *	status which gives the rough battery status, and current power
   *	source. The bat value returned give an estimate as a percentage
   *	of life and a status value for the battery. The estimated life
++<<<<<<< HEAD
 + *	if reported is a lifetime in secodnds/minutes at current powwer
++=======
+  *	if reported is a lifetime in seconds/minutes at current power
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
   *	consumption.
   */
  
diff --cc arch/x86/kernel/smp.c
index 04adc8d60aed,06db901fabe8..000000000000
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@@ -211,35 -174,38 +211,41 @@@ static void native_stop_other_cpus(int 
  		/* sync above data before sending IRQ */
  		wmb();
  
 -		apic_send_IPI_allbutself(REBOOT_VECTOR);
 +		apic->send_IPI_allbutself(REBOOT_VECTOR);
  
  		/*
 -		 * Don't wait longer than a second for IPI completion. The
 -		 * wait request is not checked here because that would
 -		 * prevent an NMI shutdown attempt in case that not all
 -		 * CPUs reach shutdown state.
 +		 * Don't wait longer than a second if the caller
 +		 * didn't ask us to wait.
  		 */
  		timeout = USEC_PER_SEC;
 -		while (num_online_cpus() > 1 && timeout--)
 +		while (num_online_cpus() > 1 && (wait || timeout--))
  			udelay(1);
  	}
 -
 +	
  	/* if the REBOOT_VECTOR didn't work, try with the NMI */
 -	if (num_online_cpus() > 1) {
 -		/*
 -		 * If NMI IPI is enabled, try to register the stop handler
 -		 * and send the IPI. In any case try to wait for the other
 -		 * CPUs to stop.
 -		 */
 -		if (!smp_no_nmi_ipi && !register_stop_handler()) {
 -			/* Sync above data before sending IRQ */
 -			wmb();
 +	if ((num_online_cpus() > 1) && (!smp_no_nmi_ipi))  {
 +		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
 +					 NMI_FLAG_FIRST, "smp_stop"))
 +			/* Note: we ignore failures here */
 +			/* Hope the REBOOT_IRQ is good enough */
 +			goto finish;
 +
 +		/* sync above data before sending IRQ */
 +		wmb();
 +
 +		pr_emerg("Shutting down cpus with NMI\n");
  
 -			pr_emerg("Shutting down cpus with NMI\n");
 +		apic->send_IPI_allbutself(NMI_VECTOR);
  
 -			apic_send_IPI_allbutself(NMI_VECTOR);
 -		}
  		/*
++<<<<<<< HEAD
 +		 * Don't wait longer than a 10 ms if the caller
 +		 * didn't ask us to wait.
++=======
+ 		 * Don't wait longer than 10 ms if the caller didn't
+ 		 * request it. If wait is true, the machine hangs here if
+ 		 * one or more CPUs do not reach shutdown state.
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
  		 */
  		timeout = USEC_PER_MSEC * 10;
  		while (num_online_cpus() > 1 && (wait || timeout--))
diff --cc arch/x86/kernel/umip.c
index 820c01c90ba8,fac1daae7994..000000000000
--- a/arch/x86/kernel/umip.c
+++ b/arch/x86/kernel/umip.c
@@@ -242,7 -272,7 +242,11 @@@ static int emulate_umip_insn(struct ins
  		 * by whether the operand is a register or a memory location.
  		 * If operand is a register, return as many bytes as the operand
  		 * size. If operand is memory, return only the two least
++<<<<<<< HEAD
 +		 * siginificant bytes of CR0.
++=======
+ 		 * significant bytes.
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
  		 */
  		if (X86_MODRM_MOD(insn->modrm.value) == 3)
  			*data_size = insn->opnd_bytes;
diff --cc arch/x86/mm/fault.c
index ddd5a5d2db4e,1c548ad00752..000000000000
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@@ -1501,27 -1459,47 +1501,38 @@@ trace_page_fault_entries(unsigned long 
  		trace_page_fault_kernel(address, regs, error_code);
  }
  
 -static __always_inline void
 -handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 -			      unsigned long address)
 -{
 -	trace_page_fault_entries(regs, error_code, address);
 -
 -	if (unlikely(kmmio_fault(regs, address)))
 -		return;
 -
 -	/* Was the fault on kernel-controlled part of the address space? */
 -	if (unlikely(fault_in_kernel_space(address))) {
 -		do_kern_addr_fault(regs, error_code, address);
 -	} else {
 -		do_user_addr_fault(regs, error_code, address);
 -		/*
 -		 * User address page fault handling might have reenabled
 -		 * interrupts. Fixing up all potential exit points of
 -		 * do_user_addr_fault() and its leaf functions is just not
 -		 * doable w/o creating an unholy mess or turning the code
 -		 * upside down.
 -		 */
 -		local_irq_disable();
 -	}
 -}
 -
 -DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 +/*
 + * We must have this function blacklisted from kprobes, tagged with notrace
 + * and call read_cr2() before calling anything else. To avoid calling any
 + * kind of tracing machinery before we've observed the CR2 value.
 + *
 + * exception_{enter,exit}() contains all sorts of tracepoints.
 + */
 +dotraplinkage void notrace
 +do_page_fault(struct pt_regs *regs, unsigned long error_code)
  {
 -	unsigned long address = read_cr2();
 -	irqentry_state_t state;
 +	unsigned long address = read_cr2(); /* Get the faulting address */
 +	enum ctx_state prev_state;
  
 -	prefetchw(&current->mm->mmap_lock);
 +	prev_state = exception_enter();
  
  	/*
++<<<<<<< HEAD
 +	 * KVM has two types of events that are, logically, interrupts, but
 +	 * are unfortunately delivered using the #PF vector.  These events are
 +	 * "you just accessed valid memory, but the host doesn't have it right
 +	 * now, so I'll put you to sleep if you continue" and "that memory
 +	 * you tried to access earlier is available now."
++=======
+ 	 * KVM uses #PF vector to deliver 'page not present' events to guests
+ 	 * (asynchronous page fault mechanism). The event happens when a
+ 	 * userspace task is trying to access some valid (from guest's point of
+ 	 * view) memory which is not currently mapped by the host (e.g. the
+ 	 * memory is swapped out). Note, the corresponding "page ready" event
+ 	 * which is injected when the memory becomes available, is delivered via
+ 	 * an interrupt mechanism and not a #PF exception
+ 	 * (see arch/x86/kernel/kvm.c: sysvec_kvm_asyncpf_interrupt()).
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
  	 *
  	 * We are relying on the interrupted context being sane (valid RSP,
  	 * relevant locks not held, etc.), which is fine as long as the
diff --cc arch/x86/xen/mmu_pv.c
index 411a67325cbf,1e28c880f642..000000000000
--- a/arch/x86/xen/mmu_pv.c
+++ b/arch/x86/xen/mmu_pv.c
@@@ -2668,7 -2341,137 +2668,141 @@@ void xen_destroy_contiguous_region(phys
  
  	spin_unlock_irqrestore(&xen_reservation_lock, flags);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
++=======
+ 
+ static noinline void xen_flush_tlb_all(void)
+ {
+ 	struct mmuext_op *op;
+ 	struct multicall_space mcs;
+ 
+ 	preempt_disable();
+ 
+ 	mcs = xen_mc_entry(sizeof(*op));
+ 
+ 	op = mcs.args;
+ 	op->cmd = MMUEXT_TLB_FLUSH_ALL;
+ 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+ 
+ 	xen_mc_issue(PARAVIRT_LAZY_MMU);
+ 
+ 	preempt_enable();
+ }
+ 
+ #define REMAP_BATCH_SIZE 16
+ 
+ struct remap_data {
+ 	xen_pfn_t *pfn;
+ 	bool contiguous;
+ 	bool no_translate;
+ 	pgprot_t prot;
+ 	struct mmu_update *mmu_update;
+ };
+ 
+ static int remap_area_pfn_pte_fn(pte_t *ptep, unsigned long addr, void *data)
+ {
+ 	struct remap_data *rmd = data;
+ 	pte_t pte = pte_mkspecial(mfn_pte(*rmd->pfn, rmd->prot));
+ 
+ 	/*
+ 	 * If we have a contiguous range, just update the pfn itself,
+ 	 * else update pointer to be "next pfn".
+ 	 */
+ 	if (rmd->contiguous)
+ 		(*rmd->pfn)++;
+ 	else
+ 		rmd->pfn++;
+ 
+ 	rmd->mmu_update->ptr = virt_to_machine(ptep).maddr;
+ 	rmd->mmu_update->ptr |= rmd->no_translate ?
+ 		MMU_PT_UPDATE_NO_TRANSLATE :
+ 		MMU_NORMAL_PT_UPDATE;
+ 	rmd->mmu_update->val = pte_val_ma(pte);
+ 	rmd->mmu_update++;
+ 
+ 	return 0;
+ }
+ 
+ int xen_remap_pfn(struct vm_area_struct *vma, unsigned long addr,
+ 		  xen_pfn_t *pfn, int nr, int *err_ptr, pgprot_t prot,
+ 		  unsigned int domid, bool no_translate, struct page **pages)
+ {
+ 	int err = 0;
+ 	struct remap_data rmd;
+ 	struct mmu_update mmu_update[REMAP_BATCH_SIZE];
+ 	unsigned long range;
+ 	int mapped = 0;
+ 
+ 	BUG_ON(!((vma->vm_flags & (VM_PFNMAP | VM_IO)) == (VM_PFNMAP | VM_IO)));
+ 
+ 	rmd.pfn = pfn;
+ 	rmd.prot = prot;
+ 	/*
+ 	 * We use the err_ptr to indicate if there we are doing a contiguous
+ 	 * mapping or a discontiguous mapping.
+ 	 */
+ 	rmd.contiguous = !err_ptr;
+ 	rmd.no_translate = no_translate;
+ 
+ 	while (nr) {
+ 		int index = 0;
+ 		int done = 0;
+ 		int batch = min(REMAP_BATCH_SIZE, nr);
+ 		int batch_left = batch;
+ 
+ 		range = (unsigned long)batch << PAGE_SHIFT;
+ 
+ 		rmd.mmu_update = mmu_update;
+ 		err = apply_to_page_range(vma->vm_mm, addr, range,
+ 					  remap_area_pfn_pte_fn, &rmd);
+ 		if (err)
+ 			goto out;
+ 
+ 		/*
+ 		 * We record the error for each page that gives an error, but
+ 		 * continue mapping until the whole set is done
+ 		 */
+ 		do {
+ 			int i;
+ 
+ 			err = HYPERVISOR_mmu_update(&mmu_update[index],
+ 						    batch_left, &done, domid);
+ 
+ 			/*
+ 			 * @err_ptr may be the same buffer as @gfn, so
+ 			 * only clear it after each chunk of @gfn is
+ 			 * used.
+ 			 */
+ 			if (err_ptr) {
+ 				for (i = index; i < index + done; i++)
+ 					err_ptr[i] = 0;
+ 			}
+ 			if (err < 0) {
+ 				if (!err_ptr)
+ 					goto out;
+ 				err_ptr[i] = err;
+ 				done++; /* Skip failed frame. */
+ 			} else
+ 				mapped += done;
+ 			batch_left -= done;
+ 			index += done;
+ 		} while (batch_left);
+ 
+ 		nr -= batch;
+ 		addr += range;
+ 		if (err_ptr)
+ 			err_ptr += batch;
+ 		cond_resched();
+ 	}
+ out:
+ 
+ 	xen_flush_tlb_all();
+ 
+ 	return err < 0 ? err : mapped;
+ }
+ EXPORT_SYMBOL_GPL(xen_remap_pfn);
++>>>>>>> 163b099146b8 (x86: Fix various typos in comments, take #2)
  
  #ifdef CONFIG_KEXEC_CORE
  phys_addr_t paddr_vmcoreinfo_note(void)
diff --git a/arch/x86/boot/compressed/efi_thunk_64.S b/arch/x86/boot/compressed/efi_thunk_64.S
index 5af41b130925..ce0308f61172 100644
--- a/arch/x86/boot/compressed/efi_thunk_64.S
+++ b/arch/x86/boot/compressed/efi_thunk_64.S
@@ -5,7 +5,7 @@
  * Early support for invoking 32-bit EFI services from a 64-bit kernel.
  *
  * Because this thunking occurs before ExitBootServices() we have to
- * restore the firmware's 32-bit GDT before we make EFI serivce calls,
+ * restore the firmware's 32-bit GDT before we make EFI service calls,
  * since the firmware's 32-bit IDT is still currently installed and it
  * needs to be able to service interrupts.
  *
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 80b99129936a..97ed0899e274 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -198,7 +198,7 @@ SYM_FUNC_START(startup_32)
 	/*
 	 * Setup for the jump to 64bit mode
 	 *
-	 * When the jump is performend we will be in long mode but
+	 * When the jump is performed we will be in long mode but
 	 * in 32bit compatibility mode with EFER.LME = 1, CS.L = 0, CS.D = 1
 	 * (and in turn EFER.LMA = 1).	To jump into 64bit mode we use
 	 * the new gdt/idt that has __KERNEL_CS with CS.L = 1.
diff --git a/arch/x86/crypto/crc32-pclmul_glue.c b/arch/x86/crypto/crc32-pclmul_glue.c
index c8d9cdacbf10..2bd5a4b672f9 100644
--- a/arch/x86/crypto/crc32-pclmul_glue.c
+++ b/arch/x86/crypto/crc32-pclmul_glue.c
@@ -24,7 +24,7 @@
 /*
  * Copyright 2012 Xyratex Technology Limited
  *
- * Wrappers for kernel crypto shash api to pclmulqdq crc32 imlementation.
+ * Wrappers for kernel crypto shash api to pclmulqdq crc32 implementation.
  */
 #include <linux/init.h>
 #include <linux/module.h>
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
index c830aef77070..eafbf60ebf88 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
@@ -103,7 +103,7 @@
 
 /*
  * Combined G1 & G2 function. Reordered with help of rotates to have moves
- * at begining.
+ * at beginning.
  */
 #define g1g2_3(ab, cd, Tx0, Tx1, Tx2, Tx3, Ty0, Ty1, Ty2, Ty3, x, y) \
 	/* G1,1 && G2,1 */ \
diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S
index ea11e444c691..6a3125c496a0 100644
--- a/arch/x86/entry/entry_32.S
+++ b/arch/x86/entry/entry_32.S
@@ -234,7 +234,7 @@
 	 *
 	 * Lets build a 5 entry IRET frame after that, such that struct pt_regs
 	 * is complete and in particular regs->sp is correct. This gives us
-	 * the original 6 enties as gap:
+	 * the original 6 entries as gap:
 	 *
 	 * 14*4(%esp) - <previous context>
 	 * 13*4(%esp) - gap / flags
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 0d0bf32551bd..8eace0e4bc3d 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -612,7 +612,7 @@ SYM_CODE_START(\asmsym)
 	/*
 	 * No need to switch back to the IST stack. The current stack is either
 	 * identical to the stack in the IRET frame or the VC fall-back stack,
-	 * so it is definitly mapped even with PTI enabled.
+	 * so it is definitely mapped even with PTI enabled.
 	 */
 	jmp	paranoid_exit
 
diff --git a/arch/x86/entry/vdso/vdso2c.c b/arch/x86/entry/vdso/vdso2c.c
index 7380908045c7..827984fe656f 100644
--- a/arch/x86/entry/vdso/vdso2c.c
+++ b/arch/x86/entry/vdso/vdso2c.c
@@ -216,7 +216,7 @@ int main(int argc, char **argv)
 
 	/*
 	 * Figure out the struct name.  If we're writing to a .so file,
-	 * generate raw output insted.
+	 * generate raw output instead.
 	 */
 	name = strdup(argv[3]);
 	namelen = strlen(name);
diff --git a/arch/x86/entry/vdso/vdso32/system_call.S b/arch/x86/entry/vdso/vdso32/system_call.S
index de1fff7188aa..b15adf7f5ef8 100644
--- a/arch/x86/entry/vdso/vdso32/system_call.S
+++ b/arch/x86/entry/vdso/vdso32/system_call.S
@@ -29,7 +29,7 @@ __kernel_vsyscall:
 	 * anyone with an AMD CPU, for example).  Nonetheless, we try to keep
 	 * it working approximately as well as it ever worked.
 	 *
-	 * This link may eludicate some of the history:
+	 * This link may elucidate some of the history:
 	 *   https://android-review.googlesource.com/#/q/Iac3295376d61ef83e713ac9b528f3b50aa780cd7
 	 * personally, I find it hard to understand what's going on there.
 	 *
diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c
index 89461dec393d..e9f9f6ef7b16 100644
--- a/arch/x86/entry/vdso/vma.c
+++ b/arch/x86/entry/vdso/vma.c
@@ -376,7 +376,7 @@ int map_vdso_once(const struct vdso_image *image, unsigned long addr)
 	mmap_write_lock(mm);
 	/*
 	 * Check if we have already mapped vdso blob - fail to prevent
-	 * abusing from userspace install_speciall_mapping, which may
+	 * abusing from userspace install_special_mapping, which may
 	 * not do accounting and rlimit right.
 	 * We could search vma near context.vdso, but it's a slowpath,
 	 * so let's explicitly check all VMAs to be completely sure.
diff --git a/arch/x86/entry/vdso/vsgx.S b/arch/x86/entry/vdso/vsgx.S
index 86a0e94f68df..99dafac992e2 100644
--- a/arch/x86/entry/vdso/vsgx.S
+++ b/arch/x86/entry/vdso/vsgx.S
@@ -137,7 +137,7 @@ SYM_FUNC_START(__vdso_sgx_enter_enclave)
 
 	/*
 	 * If the return from callback is zero or negative, return immediately,
-	 * else re-execute ENCLU with the postive return value interpreted as
+	 * else re-execute ENCLU with the positive return value interpreted as
 	 * the requested ENCLU function.
 	 */
 	cmp	$0, %eax
diff --git a/arch/x86/events/intel/bts.c b/arch/x86/events/intel/bts.c
index a36798d58d7f..fe7ebfe8788f 100644
--- a/arch/x86/events/intel/bts.c
+++ b/arch/x86/events/intel/bts.c
@@ -602,7 +602,7 @@ static __init int bts_init(void)
 		 * we cannot use the user mapping since it will not be available
 		 * if we're not running the owning process.
 		 *
-		 * With PTI we can't use the kernal map either, because its not
+		 * With PTI we can't use the kernel map either, because its not
 		 * there when we run userspace.
 		 *
 		 * For now, disable this driver when using PTI.
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index 9e180f679c3f..b35edcfaba8d 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -2809,7 +2809,7 @@ static int handle_pmi_common(struct pt_regs *regs, u64 status)
 	 * processing loop coming after that the function, otherwise
 	 * phony regular samples may be generated in the sampling buffer
 	 * not marked with the EXACT tag. Another possibility is to have
-	 * one PEBS event and at least one non-PEBS event whic hoverflows
+	 * one PEBS event and at least one non-PEBS event which overflows
 	 * while PEBS has armed. In this case, bit 62 of GLOBAL_STATUS will
 	 * not be set, yet the overflow status bit for the PEBS counter will
 	 * be on Skylake.
diff --git a/arch/x86/events/intel/p4.c b/arch/x86/events/intel/p4.c
index a4cc66005ce8..48af66217b0b 100644
--- a/arch/x86/events/intel/p4.c
+++ b/arch/x86/events/intel/p4.c
@@ -1313,7 +1313,7 @@ static __initconst const struct x86_pmu p4_pmu = {
 	.get_event_constraints	= x86_get_event_constraints,
 	/*
 	 * IF HT disabled we may need to use all
-	 * ARCH_P4_MAX_CCCR counters simulaneously
+	 * ARCH_P4_MAX_CCCR counters simultaneously
 	 * though leave it restricted at moment assuming
 	 * HT is on
 	 */
diff --git a/arch/x86/include/asm/agp.h b/arch/x86/include/asm/agp.h
index 8e25bf4f323a..b6deb41cb856 100644
--- a/arch/x86/include/asm/agp.h
+++ b/arch/x86/include/asm/agp.h
@@ -9,7 +9,7 @@
  * Functions to keep the agpgart mappings coherent with the MMU. The
  * GART gives the CPU a physical alias of pages in memory. The alias
  * region is mapped uncacheable. Make sure there are no conflicting
- * mappings with different cachability attributes for the same
+ * mappings with different cacheability attributes for the same
  * page. This avoids data corruption on some CPUs.
  */
 
diff --git a/arch/x86/include/asm/intel_pt.h b/arch/x86/include/asm/intel_pt.h
index 423b788f495e..ebe8d2ea44fe 100644
--- a/arch/x86/include/asm/intel_pt.h
+++ b/arch/x86/include/asm/intel_pt.h
@@ -3,7 +3,7 @@
 #define _ASM_X86_INTEL_PT_H
 
 #define PT_CPUID_LEAVES		2
-#define PT_CPUID_REGS_NUM	4 /* number of regsters (eax, ebx, ecx, edx) */
+#define PT_CPUID_REGS_NUM	4 /* number of registers (eax, ebx, ecx, edx) */
 
 enum pt_capabilities {
 	PT_CAP_max_subleaf = 0,
* Unmerged path arch/x86/include/asm/set_memory.h
diff --git a/arch/x86/kernel/amd_nb.c b/arch/x86/kernel/amd_nb.c
index e99064ba6512..ceff3d0038ed 100644
--- a/arch/x86/kernel/amd_nb.c
+++ b/arch/x86/kernel/amd_nb.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Shared support code for AMD K8 northbridges and derivates.
+ * Shared support code for AMD K8 northbridges and derivatives.
  * Copyright 2006 Andi Kleen, SUSE Labs.
  */
 
* Unmerged path arch/x86/kernel/apm_32.c
diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index afa387000727..ea978518cdcc 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -337,7 +337,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 *  The operating system must reload CR3 to cause the TLB to be flushed"
 	 *
 	 * As a result, boot_cpu_has(X86_FEATURE_PGE) in arch/x86/include/asm/tlbflush.h
-	 * should be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE
+	 * should be false so that __flush_tlb_all() causes CR3 instead of CR4.PGE
 	 * to be modified.
 	 */
 	if (c->x86 == 5 && c->x86_model == 9) {
diff --git a/arch/x86/kernel/cpu/mce/severity.c b/arch/x86/kernel/cpu/mce/severity.c
index f1c83cc79c2f..a8e455936560 100644
--- a/arch/x86/kernel/cpu/mce/severity.c
+++ b/arch/x86/kernel/cpu/mce/severity.c
@@ -129,7 +129,7 @@ static struct severity {
 		MASK(MCI_STATUS_OVER|MCI_UC_SAR, MCI_STATUS_UC|MCI_STATUS_AR)
 		),
 	MCESEV(
-		KEEP, "Non signalled machine check",
+		KEEP, "Non signaled machine check",
 		SER, BITCLR(MCI_STATUS_S)
 		),
 
diff --git a/arch/x86/kernel/cpu/mtrr/mtrr.c b/arch/x86/kernel/cpu/mtrr/mtrr.c
index 19afb39941c3..865ed0a6d0d9 100644
--- a/arch/x86/kernel/cpu/mtrr/mtrr.c
+++ b/arch/x86/kernel/cpu/mtrr/mtrr.c
@@ -803,7 +803,7 @@ void mtrr_ap_init(void)
 	 *
 	 * This routine is called in two cases:
 	 *
-	 *   1. very earily time of software resume, when there absolutely
+	 *   1. very early time of software resume, when there absolutely
 	 *      isn't mtrr entry changes;
 	 *
 	 *   2. cpu hotadd time. We let mtrr_add/del_page hold cpuhotplug
diff --git a/arch/x86/kernel/cpu/resctrl/monitor.c b/arch/x86/kernel/cpu/resctrl/monitor.c
index 90dd00e3a954..9d5741f01bad 100644
--- a/arch/x86/kernel/cpu/resctrl/monitor.c
+++ b/arch/x86/kernel/cpu/resctrl/monitor.c
@@ -342,7 +342,7 @@ void mon_event_count(void *info)
  * timer. Having 1s interval makes the calculation of bandwidth simpler.
  *
  * Although MBA's goal is to restrict the bandwidth to a maximum, there may
- * be a need to increase the bandwidth to avoid uncecessarily restricting
+ * be a need to increase the bandwidth to avoid unnecessarily restricting
  * the L2 <-> L3 traffic.
  *
  * Since MBA controls the L2 external bandwidth where as MBM measures the
@@ -425,7 +425,7 @@ static void update_mba_bw(struct rdtgroup *rgrp, struct rdt_domain *dom_mbm)
 
 	/*
 	 * Delta values are updated dynamically package wise for each
-	 * rdtgrp everytime the throttle MSR changes value.
+	 * rdtgrp every time the throttle MSR changes value.
 	 *
 	 * This is because (1)the increase in bandwidth is not perfectly
 	 * linear and only "approximately" linear even when the hardware
diff --git a/arch/x86/kernel/cpu/resctrl/rdtgroup.c b/arch/x86/kernel/cpu/resctrl/rdtgroup.c
index a71a5a039c24..488128dd9592 100644
--- a/arch/x86/kernel/cpu/resctrl/rdtgroup.c
+++ b/arch/x86/kernel/cpu/resctrl/rdtgroup.c
@@ -2590,7 +2590,7 @@ static int mkdir_mondata_subdir_alldom(struct kernfs_node *parent_kn,
 /*
  * This creates a directory mon_data which contains the monitored data.
  *
- * mon_data has one directory for each domain whic are named
+ * mon_data has one directory for each domain which are named
  * in the format mon_<domain_name>_<domain_id>. For ex: A mon_data
  * with L3 domain looks as below:
  * ./mon_data:
diff --git a/arch/x86/kernel/relocate_kernel_32.S b/arch/x86/kernel/relocate_kernel_32.S
index 77630d57e7bf..8176c84c747f 100644
--- a/arch/x86/kernel/relocate_kernel_32.S
+++ b/arch/x86/kernel/relocate_kernel_32.S
@@ -109,7 +109,7 @@ identity_mapped:
 	 *  - Write protect disabled
 	 *  - No task switch
 	 *  - Don't do FP software emulation.
-	 *  - Proctected mode enabled
+	 *  - Protected mode enabled
 	 */
 	movl	%cr0, %eax
 	andl	$~(X86_CR0_PG | X86_CR0_AM | X86_CR0_WP | X86_CR0_TS | X86_CR0_EM), %eax
diff --git a/arch/x86/kernel/relocate_kernel_64.S b/arch/x86/kernel/relocate_kernel_64.S
index 11eda21eb697..7dc14e254308 100644
--- a/arch/x86/kernel/relocate_kernel_64.S
+++ b/arch/x86/kernel/relocate_kernel_64.S
@@ -119,7 +119,7 @@ identity_mapped:
 	 *  - Write protect disabled
 	 *  - No task switch
 	 *  - Don't do FP software emulation.
-	 *  - Proctected mode enabled
+	 *  - Protected mode enabled
 	 */
 	movq	%cr0, %rax
 	andq	$~(X86_CR0_AM | X86_CR0_WP | X86_CR0_TS | X86_CR0_EM), %rax
* Unmerged path arch/x86/kernel/smp.c
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index ec534f978867..87163d3b0416 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -473,7 +473,7 @@ void check_tsc_sync_target(void)
 	/*
 	 * Add the result to the previous adjustment value.
 	 *
-	 * The adjustement value is slightly off by the overhead of the
+	 * The adjustment value is slightly off by the overhead of the
 	 * sync mechanism (observed values are ~200 TSC cycles), but this
 	 * really depends on CPU, node distance and frequency. So
 	 * compensating for this is hard to get right. Experiments show
* Unmerged path arch/x86/kernel/umip.c
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index fc0024ea9481..67cafe893887 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -727,7 +727,7 @@ static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 	struct amd_svm_iommu_ir *ir;
 
 	/**
-	 * In some cases, the existing irte is updaed and re-set,
+	 * In some cases, the existing irte is updated and re-set,
 	 * so we need to check here if it's already been * added
 	 * to the ir_list.
 	 */
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index feb6c0506109..cbd9a349c44f 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -3556,7 +3556,7 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 * snapshot restore (migration).
 	 *
 	 * In this flow, it is assumed that vmcs12 cache was
-	 * trasferred as part of captured nVMX state and should
+	 * transferred as part of captured nVMX state and should
 	 * therefore not be read from guest memory (which may not
 	 * exist on destination host yet).
 	 */
diff --git a/arch/x86/math-emu/reg_ld_str.c b/arch/x86/math-emu/reg_ld_str.c
index f3779743d15e..7e20a84f2a68 100644
--- a/arch/x86/math-emu/reg_ld_str.c
+++ b/arch/x86/math-emu/reg_ld_str.c
@@ -964,7 +964,7 @@ int FPU_store_bcd(FPU_REG *st0_ptr, u_char st0_tag, u_char __user *d)
 /* The return value (in eax) is zero if the result is exact,
    if bits are changed due to rounding, truncation, etc, then
    a non-zero value is returned */
-/* Overflow is signalled by a non-zero return value (in eax).
+/* Overflow is signaled by a non-zero return value (in eax).
    In the case of overflow, the returned significand always has the
    largest possible value */
 int FPU_round_to_int(FPU_REG *r, u_char tag)
diff --git a/arch/x86/math-emu/reg_round.S b/arch/x86/math-emu/reg_round.S
index 04563421ee7d..fef9971cf7cf 100644
--- a/arch/x86/math-emu/reg_round.S
+++ b/arch/x86/math-emu/reg_round.S
@@ -575,7 +575,7 @@ Normalise_result:
 #ifdef PECULIAR_486
 	/*
 	 * This implements a special feature of 80486 behaviour.
-	 * Underflow will be signalled even if the number is
+	 * Underflow will be signaled even if the number is
 	 * not a denormal after rounding.
 	 * This difference occurs only for masked underflow, and not
 	 * in the unmasked case.
* Unmerged path arch/x86/mm/fault.c
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index d56c62c50b99..03b16e0c9c69 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -682,7 +682,7 @@ void __init init_mem_mapping(void)
 
 #ifdef CONFIG_X86_64
 	if (max_pfn > max_low_pfn) {
-		/* can we preseve max_low_pfn ?*/
+		/* can we preserve max_low_pfn ?*/
 		max_low_pfn = max_pfn;
 	}
 #else
diff --git a/arch/x86/mm/pkeys.c b/arch/x86/mm/pkeys.c
index 1dcfc91c8f0c..a32b8834f7bf 100644
--- a/arch/x86/mm/pkeys.c
+++ b/arch/x86/mm/pkeys.c
@@ -136,7 +136,7 @@ u32 init_pkru_value = PKRU_AD_KEY( 1) | PKRU_AD_KEY( 2) | PKRU_AD_KEY( 3) |
 /*
  * Called from the FPU code when creating a fresh set of FPU
  * registers.  This is called from a very specific context where
- * we know the FPU regstiers are safe for use and we can use PKRU
+ * we know the FPU registers are safe for use and we can use PKRU
  * directly.
  */
 void copy_init_pkru_to_fpregs(void)
diff --git a/arch/x86/platform/efi/quirks.c b/arch/x86/platform/efi/quirks.c
index c41778b88a86..974ca62ff590 100644
--- a/arch/x86/platform/efi/quirks.c
+++ b/arch/x86/platform/efi/quirks.c
@@ -439,7 +439,7 @@ void __init efi_free_boot_services(void)
 		 * 1.4.4 with SGX enabled booting Linux via Fedora 24's
 		 * grub2-efi on a hard disk.  (And no, I don't know why
 		 * this happened, but Linux should still try to boot rather
-		 * panicing early.)
+		 * panicking early.)
 		 */
 		rm_size = real_mode_size_needed();
 		if (rm_size && (start + rm_size) < (1<<20) && size >= rm_size) {
diff --git a/arch/x86/platform/olpc/olpc-xo15-sci.c b/arch/x86/platform/olpc/olpc-xo15-sci.c
index c0533fbc39e3..8a0836068f19 100644
--- a/arch/x86/platform/olpc/olpc-xo15-sci.c
+++ b/arch/x86/platform/olpc/olpc-xo15-sci.c
@@ -31,7 +31,7 @@ static bool				lid_wake_on_close;
  * wake-on-close. This is implemented as standard by the XO-1.5 DSDT.
  *
  * We provide here a sysfs attribute that will additionally enable
- * wake-on-close behavior. This is useful (e.g.) when we oportunistically
+ * wake-on-close behavior. This is useful (e.g.) when we opportunistically
  * suspend with the display running; if the lid is then closed, we want to
  * wake up to turn the display off.
  *
diff --git a/arch/x86/platform/olpc/olpc_dt.c b/arch/x86/platform/olpc/olpc_dt.c
index 9d67c1434607..9c768ca60492 100644
--- a/arch/x86/platform/olpc/olpc_dt.c
+++ b/arch/x86/platform/olpc/olpc_dt.c
@@ -136,7 +136,7 @@ void * __init prom_early_alloc(unsigned long size)
 		const size_t chunk_size = max(PAGE_SIZE, size);
 
 		/*
-		 * To mimimize the number of allocations, grab at least
+		 * To minimize the number of allocations, grab at least
 		 * PAGE_SIZE of memory (that's an arbitrary choice that's
 		 * fast enough on the platforms we care about while minimizing
 		 * wasted bootmem) and hand off chunks of it to callers.
diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index a7d966964c6f..13d1c7bf55aa 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -309,7 +309,7 @@ int hibernate_resume_nonboot_cpu_disable(void)
 
 /*
  * When bsp_check() is called in hibernate and suspend, cpu hotplug
- * is disabled already. So it's unnessary to handle race condition between
+ * is disabled already. So it's unnecessary to handle race condition between
  * cpumask query and cpu hotplug.
  */
 static int bsp_check(void)
diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c
index 8cc42761ff25..48b24b9fc535 100644
--- a/arch/x86/realmode/init.c
+++ b/arch/x86/realmode/init.c
@@ -105,7 +105,7 @@ static void __init setup_real_mode(void)
 		*ptr += phys_base;
 	}
 
-	/* Must be perfomed *after* relocation. */
+	/* Must be performed *after* relocation. */
 	trampoline_header = (struct trampoline_header *)
 		__va(real_mode_header->trampoline_header);
 
* Unmerged path arch/x86/xen/mmu_pv.c
