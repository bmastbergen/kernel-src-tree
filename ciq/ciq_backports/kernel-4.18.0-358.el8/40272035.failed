powerpc/bpf: Reallocate BPF registers to volatile registers when possible on PPC32

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Christophe Leroy <christophe.leroy@csgroup.eu>
commit 40272035e1d0edcd515ad45be297c4cce044536d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/40272035.failed

When the BPF routine doesn't call any function, the non volatile
registers can be reallocated to volatile registers in order to
avoid having to save them/restore on the stack.

Before this patch, the test #359 ADD default X is:

   0:	7c 64 1b 78 	mr      r4,r3
   4:	38 60 00 00 	li      r3,0
   8:	94 21 ff b0 	stwu    r1,-80(r1)
   c:	60 00 00 00 	nop
  10:	92 e1 00 2c 	stw     r23,44(r1)
  14:	93 01 00 30 	stw     r24,48(r1)
  18:	93 21 00 34 	stw     r25,52(r1)
  1c:	93 41 00 38 	stw     r26,56(r1)
  20:	39 80 00 00 	li      r12,0
  24:	39 60 00 00 	li      r11,0
  28:	3b 40 00 00 	li      r26,0
  2c:	3b 20 00 00 	li      r25,0
  30:	7c 98 23 78 	mr      r24,r4
  34:	7c 77 1b 78 	mr      r23,r3
  38:	39 80 00 42 	li      r12,66
  3c:	39 60 00 00 	li      r11,0
  40:	7d 8c d2 14 	add     r12,r12,r26
  44:	39 60 00 00 	li      r11,0
  48:	7d 83 63 78 	mr      r3,r12
  4c:	82 e1 00 2c 	lwz     r23,44(r1)
  50:	83 01 00 30 	lwz     r24,48(r1)
  54:	83 21 00 34 	lwz     r25,52(r1)
  58:	83 41 00 38 	lwz     r26,56(r1)
  5c:	38 21 00 50 	addi    r1,r1,80
  60:	4e 80 00 20 	blr

After this patch, the same test has become:

   0:	7c 64 1b 78 	mr      r4,r3
   4:	38 60 00 00 	li      r3,0
   8:	94 21 ff b0 	stwu    r1,-80(r1)
   c:	60 00 00 00 	nop
  10:	39 80 00 00 	li      r12,0
  14:	39 60 00 00 	li      r11,0
  18:	39 00 00 00 	li      r8,0
  1c:	38 e0 00 00 	li      r7,0
  20:	7c 86 23 78 	mr      r6,r4
  24:	7c 65 1b 78 	mr      r5,r3
  28:	39 80 00 42 	li      r12,66
  2c:	39 60 00 00 	li      r11,0
  30:	7d 8c 42 14 	add     r12,r12,r8
  34:	39 60 00 00 	li      r11,0
  38:	7d 83 63 78 	mr      r3,r12
  3c:	38 21 00 50 	addi    r1,r1,80
  40:	4e 80 00 20 	blr

	Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/b94562d7d2bb21aec89de0c40bb3cd91054b65a2.1616430991.git.christophe.leroy@csgroup.eu
(cherry picked from commit 40272035e1d0edcd515ad45be297c4cce044536d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/net/bpf_jit.h
#	arch/powerpc/net/bpf_jit_comp.c
#	arch/powerpc/net/bpf_jit_comp32.c
#	arch/powerpc/net/bpf_jit_comp64.c
diff --cc arch/powerpc/net/bpf_jit.h
index e560db2fd804,776abef4d2a0..000000000000
--- a/arch/powerpc/net/bpf_jit.h
+++ b/arch/powerpc/net/bpf_jit.h
@@@ -147,6 -157,18 +157,21 @@@ static inline void bpf_set_seen_registe
  	ctx->seen |= 1 << (31 - i);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void bpf_clear_seen_register(struct codegen_context *ctx, int i)
+ {
+ 	ctx->seen &= ~(1 << (31 - i));
+ }
+ 
+ void bpf_jit_emit_func_call_rel(u32 *image, struct codegen_context *ctx, u64 func);
+ int bpf_jit_build_body(struct bpf_prog *fp, u32 *image, struct codegen_context *ctx,
+ 		       u32 *addrs, bool extra_pass);
+ void bpf_jit_build_prologue(u32 *image, struct codegen_context *ctx);
+ void bpf_jit_build_epilogue(u32 *image, struct codegen_context *ctx);
+ void bpf_jit_realloc_regs(struct codegen_context *ctx);
+ 
++>>>>>>> 40272035e1d0 (powerpc/bpf: Reallocate BPF registers to volatile registers when possible on PPC32)
  #endif
  
  #endif
diff --cc arch/powerpc/net/bpf_jit_comp.c
index d82b3f4a3716,798ac4350a82..000000000000
--- a/arch/powerpc/net/bpf_jit_comp.c
+++ b/arch/powerpc/net/bpf_jit_comp.c
@@@ -15,620 -12,163 +15,641 @@@
  #include <linux/netdevice.h>
  #include <linux/filter.h>
  #include <linux/if_vlan.h>
 -#include <asm/kprobes.h>
 -#include <linux/bpf.h>
  
 -#include "bpf_jit.h"
 +#include "bpf_jit32.h"
  
 -static void bpf_jit_fill_ill_insns(void *area, unsigned int size)
 +static inline void bpf_flush_icache(void *start, void *end)
  {
 -	memset32(area, BREAKPOINT_INSTRUCTION, size / 4);
 +	smp_wmb();
 +	flush_icache_range((unsigned long)start, (unsigned long)end);
  }
  
 -/* Fix the branch target addresses for subprog calls */
 -static int bpf_jit_fixup_subprog_calls(struct bpf_prog *fp, u32 *image,
 -				       struct codegen_context *ctx, u32 *addrs)
 +static void bpf_jit_build_prologue(struct bpf_prog *fp, u32 *image,
 +				   struct codegen_context *ctx)
  {
 -	const struct bpf_insn *insn = fp->insnsi;
 -	bool func_addr_fixed;
 -	u64 func_addr;
 -	u32 tmp_idx;
 -	int i, ret;
 +	int i;
 +	const struct sock_filter *filter = fp->insns;
 +
 +	if (ctx->seen & (SEEN_MEM | SEEN_DATAREF)) {
 +		/* Make stackframe */
 +		if (ctx->seen & SEEN_DATAREF) {
 +			/* If we call any helpers (for loads), save LR */
 +			EMIT(PPC_INST_MFLR | __PPC_RT(R0));
 +			PPC_BPF_STL(0, 1, PPC_LR_STKOFF);
 +
 +			/* Back up non-volatile regs. */
 +			PPC_BPF_STL(r_D, 1, -(REG_SZ*(32-r_D)));
 +			PPC_BPF_STL(r_HL, 1, -(REG_SZ*(32-r_HL)));
 +		}
 +		if (ctx->seen & SEEN_MEM) {
 +			/*
 +			 * Conditionally save regs r15-r31 as some will be used
 +			 * for M[] data.
 +			 */
 +			for (i = r_M; i < (r_M+16); i++) {
 +				if (ctx->seen & (1 << (i-r_M)))
 +					PPC_BPF_STL(i, 1, -(REG_SZ*(32-i)));
 +			}
 +		}
 +		PPC_BPF_STLU(1, 1, -BPF_PPC_STACKFRAME);
 +	}
  
 -	for (i = 0; i < fp->len; i++) {
 +	if (ctx->seen & SEEN_DATAREF) {
  		/*
 -		 * During the extra pass, only the branch target addresses for
 -		 * the subprog calls need to be fixed. All other instructions
 -		 * can left untouched.
 -		 *
 -		 * The JITed image length does not change because we already
 -		 * ensure that the JITed instruction sequence for these calls
 -		 * are of fixed length by padding them with NOPs.
 +		 * If this filter needs to access skb data,
 +		 * prepare r_D and r_HL:
 +		 *  r_HL = skb->len - skb->data_len
 +		 *  r_D	 = skb->data
  		 */
 -		if (insn[i].code == (BPF_JMP | BPF_CALL) &&
 -		    insn[i].src_reg == BPF_PSEUDO_CALL) {
 -			ret = bpf_jit_get_func_addr(fp, &insn[i], true,
 -						    &func_addr,
 -						    &func_addr_fixed);
 -			if (ret < 0)
 -				return ret;
 +		PPC_LWZ_OFFS(r_scratch1, r_skb, offsetof(struct sk_buff,
 +							 data_len));
 +		PPC_LWZ_OFFS(r_HL, r_skb, offsetof(struct sk_buff, len));
 +		EMIT(PPC_RAW_SUB(r_HL, r_HL, r_scratch1));
 +		PPC_LL_OFFS(r_D, r_skb, offsetof(struct sk_buff, data));
 +	}
  
 -			/*
 -			 * Save ctx->idx as this would currently point to the
 -			 * end of the JITed image and set it to the offset of
 -			 * the instruction sequence corresponding to the
 -			 * subprog call temporarily.
 -			 */
 -			tmp_idx = ctx->idx;
 -			ctx->idx = addrs[i] / 4;
 -			bpf_jit_emit_func_call_rel(image, ctx, func_addr);
 +	if (ctx->seen & SEEN_XREG) {
 +		/*
 +		 * TODO: Could also detect whether first instr. sets X and
 +		 * avoid this (as below, with A).
 +		 */
 +		EMIT(PPC_RAW_LI(r_X, 0));
 +	}
  
 -			/*
 -			 * Restore ctx->idx here. This is safe as the length
 -			 * of the JITed sequence remains unchanged.
 -			 */
 -			ctx->idx = tmp_idx;
 +	/* make sure we dont leak kernel information to user */
 +	if (bpf_needs_clear_a(&filter[0]))
 +		EMIT(PPC_RAW_LI(r_A, 0));
 +}
 +
 +static void bpf_jit_build_epilogue(u32 *image, struct codegen_context *ctx)
 +{
 +	int i;
 +
 +	if (ctx->seen & (SEEN_MEM | SEEN_DATAREF)) {
 +		EMIT(PPC_RAW_ADDI(1, 1, BPF_PPC_STACKFRAME));
 +		if (ctx->seen & SEEN_DATAREF) {
 +			PPC_BPF_LL(0, 1, PPC_LR_STKOFF);
 +			EMIT(PPC_RAW_MTLR(0));
 +			PPC_BPF_LL(r_D, 1, -(REG_SZ*(32-r_D)));
 +			PPC_BPF_LL(r_HL, 1, -(REG_SZ*(32-r_HL)));
 +		}
 +		if (ctx->seen & SEEN_MEM) {
 +			/* Restore any saved non-vol registers */
 +			for (i = r_M; i < (r_M+16); i++) {
 +				if (ctx->seen & (1 << (i-r_M)))
 +					PPC_BPF_LL(i, 1, -(REG_SZ*(32-i)));
 +			}
  		}
  	}
 +	/* The RETs have left a return value in R3. */
  
 -	return 0;
 +	EMIT(PPC_RAW_BLR());
  }
  
 -struct powerpc64_jit_data {
 -	struct bpf_binary_header *header;
 -	u32 *addrs;
 -	u8 *image;
 -	u32 proglen;
 -	struct codegen_context ctx;
 -};
 +#define CHOOSE_LOAD_FUNC(K, func) \
 +	((int)K < 0 ? ((int)K >= SKF_LL_OFF ? func##_negative_offset : func) : func##_positive_offset)
  
 -bool bpf_jit_needs_zext(void)
 +/* Assemble the body code between the prologue & epilogue. */
 +static int bpf_jit_build_body(struct bpf_prog *fp, u32 *image,
 +			      struct codegen_context *ctx,
 +			      unsigned int *addrs)
  {
 -	return true;
 +	const struct sock_filter *filter = fp->insns;
 +	int flen = fp->len;
 +	u8 *func;
 +	unsigned int true_cond;
 +	int i;
 +
 +	/* Start of epilogue code */
 +	unsigned int exit_addr = addrs[flen];
 +
 +	for (i = 0; i < flen; i++) {
 +		unsigned int K = filter[i].k;
 +		u16 code = bpf_anc_helper(&filter[i]);
 +
 +		/*
 +		 * addrs[] maps a BPF bytecode address into a real offset from
 +		 * the start of the body code.
 +		 */
 +		addrs[i] = ctx->idx * 4;
 +
 +		switch (code) {
 +			/*** ALU ops ***/
 +		case BPF_ALU | BPF_ADD | BPF_X: /* A += X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_ADD(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_ADD | BPF_K: /* A += K; */
 +			if (!K)
 +				break;
 +			EMIT(PPC_RAW_ADDI(r_A, r_A, IMM_L(K)));
 +			if (K >= 32768)
 +				EMIT(PPC_RAW_ADDIS(r_A, r_A, IMM_HA(K)));
 +			break;
 +		case BPF_ALU | BPF_SUB | BPF_X: /* A -= X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_SUB(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_SUB | BPF_K: /* A -= K */
 +			if (!K)
 +				break;
 +			EMIT(PPC_RAW_ADDI(r_A, r_A, IMM_L(-K)));
 +			if (K >= 32768)
 +				EMIT(PPC_RAW_ADDIS(r_A, r_A, IMM_HA(-K)));
 +			break;
 +		case BPF_ALU | BPF_MUL | BPF_X: /* A *= X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_MULW(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_MUL | BPF_K: /* A *= K */
 +			if (K < 32768)
 +				EMIT(PPC_RAW_MULI(r_A, r_A, K));
 +			else {
 +				PPC_LI32(r_scratch1, K);
 +				EMIT(PPC_RAW_MULW(r_A, r_A, r_scratch1));
 +			}
 +			break;
 +		case BPF_ALU | BPF_MOD | BPF_X: /* A %= X; */
 +		case BPF_ALU | BPF_DIV | BPF_X: /* A /= X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_CMPWI(r_X, 0));
 +			if (ctx->pc_ret0 != -1) {
 +				PPC_BCC(COND_EQ, addrs[ctx->pc_ret0]);
 +			} else {
 +				PPC_BCC_SHORT(COND_NE, (ctx->idx*4)+12);
 +				EMIT(PPC_RAW_LI(r_ret, 0));
 +				PPC_JMP(exit_addr);
 +			}
 +			if (code == (BPF_ALU | BPF_MOD | BPF_X)) {
 +				EMIT(PPC_RAW_DIVWU(r_scratch1, r_A, r_X));
 +				EMIT(PPC_RAW_MULW(r_scratch1, r_X, r_scratch1));
 +				EMIT(PPC_RAW_SUB(r_A, r_A, r_scratch1));
 +			} else {
 +				EMIT(PPC_RAW_DIVWU(r_A, r_A, r_X));
 +			}
 +			break;
 +		case BPF_ALU | BPF_MOD | BPF_K: /* A %= K; */
 +			PPC_LI32(r_scratch2, K);
 +			EMIT(PPC_RAW_DIVWU(r_scratch1, r_A, r_scratch2));
 +			EMIT(PPC_RAW_MULW(r_scratch1, r_scratch2, r_scratch1));
 +			EMIT(PPC_RAW_SUB(r_A, r_A, r_scratch1));
 +			break;
 +		case BPF_ALU | BPF_DIV | BPF_K: /* A /= K */
 +			if (K == 1)
 +				break;
 +			PPC_LI32(r_scratch1, K);
 +			EMIT(PPC_RAW_DIVWU(r_A, r_A, r_scratch1));
 +			break;
 +		case BPF_ALU | BPF_AND | BPF_X:
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_AND(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_AND | BPF_K:
 +			if (!IMM_H(K))
 +				EMIT(PPC_RAW_ANDI(r_A, r_A, K));
 +			else {
 +				PPC_LI32(r_scratch1, K);
 +				EMIT(PPC_RAW_AND(r_A, r_A, r_scratch1));
 +			}
 +			break;
 +		case BPF_ALU | BPF_OR | BPF_X:
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_OR(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_OR | BPF_K:
 +			if (IMM_L(K))
 +				EMIT(PPC_RAW_ORI(r_A, r_A, IMM_L(K)));
 +			if (K >= 65536)
 +				EMIT(PPC_RAW_ORIS(r_A, r_A, IMM_H(K)));
 +			break;
 +		case BPF_ANC | SKF_AD_ALU_XOR_X:
 +		case BPF_ALU | BPF_XOR | BPF_X: /* A ^= X */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_XOR(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_XOR | BPF_K: /* A ^= K */
 +			if (IMM_L(K))
 +				EMIT(PPC_RAW_XORI(r_A, r_A, IMM_L(K)));
 +			if (K >= 65536)
 +				EMIT(PPC_RAW_XORIS(r_A, r_A, IMM_H(K)));
 +			break;
 +		case BPF_ALU | BPF_LSH | BPF_X: /* A <<= X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_SLW(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_LSH | BPF_K:
 +			if (K == 0)
 +				break;
 +			else
 +				EMIT(PPC_RAW_SLWI(r_A, r_A, K));
 +			break;
 +		case BPF_ALU | BPF_RSH | BPF_X: /* A >>= X; */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_SRW(r_A, r_A, r_X));
 +			break;
 +		case BPF_ALU | BPF_RSH | BPF_K: /* A >>= K; */
 +			if (K == 0)
 +				break;
 +			else
 +				EMIT(PPC_RAW_SRWI(r_A, r_A, K));
 +			break;
 +		case BPF_ALU | BPF_NEG:
 +			EMIT(PPC_RAW_NEG(r_A, r_A));
 +			break;
 +		case BPF_RET | BPF_K:
 +			PPC_LI32(r_ret, K);
 +			if (!K) {
 +				if (ctx->pc_ret0 == -1)
 +					ctx->pc_ret0 = i;
 +			}
 +			/*
 +			 * If this isn't the very last instruction, branch to
 +			 * the epilogue if we've stuff to clean up.  Otherwise,
 +			 * if there's nothing to tidy, just return.  If we /are/
 +			 * the last instruction, we're about to fall through to
 +			 * the epilogue to return.
 +			 */
 +			if (i != flen - 1) {
 +				/*
 +				 * Note: 'seen' is properly valid only on pass
 +				 * #2.	Both parts of this conditional are the
 +				 * same instruction size though, meaning the
 +				 * first pass will still correctly determine the
 +				 * code size/addresses.
 +				 */
 +				if (ctx->seen)
 +					PPC_JMP(exit_addr);
 +				else
 +					EMIT(PPC_RAW_BLR());
 +			}
 +			break;
 +		case BPF_RET | BPF_A:
 +			EMIT(PPC_RAW_MR(r_ret, r_A));
 +			if (i != flen - 1) {
 +				if (ctx->seen)
 +					PPC_JMP(exit_addr);
 +				else
 +					EMIT(PPC_RAW_BLR());
 +			}
 +			break;
 +		case BPF_MISC | BPF_TAX: /* X = A */
 +			EMIT(PPC_RAW_MR(r_X, r_A));
 +			break;
 +		case BPF_MISC | BPF_TXA: /* A = X */
 +			ctx->seen |= SEEN_XREG;
 +			EMIT(PPC_RAW_MR(r_A, r_X));
 +			break;
 +
 +			/*** Constant loads/M[] access ***/
 +		case BPF_LD | BPF_IMM: /* A = K */
 +			PPC_LI32(r_A, K);
 +			break;
 +		case BPF_LDX | BPF_IMM: /* X = K */
 +			PPC_LI32(r_X, K);
 +			break;
 +		case BPF_LD | BPF_MEM: /* A = mem[K] */
 +			EMIT(PPC_RAW_MR(r_A, r_M + (K & 0xf)));
 +			ctx->seen |= SEEN_MEM | (1<<(K & 0xf));
 +			break;
 +		case BPF_LDX | BPF_MEM: /* X = mem[K] */
 +			EMIT(PPC_RAW_MR(r_X, r_M + (K & 0xf)));
 +			ctx->seen |= SEEN_MEM | (1<<(K & 0xf));
 +			break;
 +		case BPF_ST: /* mem[K] = A */
 +			EMIT(PPC_RAW_MR(r_M + (K & 0xf), r_A));
 +			ctx->seen |= SEEN_MEM | (1<<(K & 0xf));
 +			break;
 +		case BPF_STX: /* mem[K] = X */
 +			EMIT(PPC_RAW_MR(r_M + (K & 0xf), r_X));
 +			ctx->seen |= SEEN_XREG | SEEN_MEM | (1<<(K & 0xf));
 +			break;
 +		case BPF_LD | BPF_W | BPF_LEN: /*	A = skb->len; */
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
 +			PPC_LWZ_OFFS(r_A, r_skb, offsetof(struct sk_buff, len));
 +			break;
 +		case BPF_LDX | BPF_W | BPF_ABS: /* A = *((u32 *)(seccomp_data + K)); */
 +			PPC_LWZ_OFFS(r_A, r_skb, K);
 +			break;
 +		case BPF_LDX | BPF_W | BPF_LEN: /* X = skb->len; */
 +			PPC_LWZ_OFFS(r_X, r_skb, offsetof(struct sk_buff, len));
 +			break;
 +
 +			/*** Ancillary info loads ***/
 +		case BPF_ANC | SKF_AD_PROTOCOL: /* A = ntohs(skb->protocol); */
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,
 +						  protocol) != 2);
 +			PPC_NTOHS_OFFS(r_A, r_skb, offsetof(struct sk_buff,
 +							    protocol));
 +			break;
 +		case BPF_ANC | SKF_AD_IFINDEX:
 +		case BPF_ANC | SKF_AD_HATYPE:
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct net_device,
 +						ifindex) != 4);
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct net_device,
 +						type) != 2);
 +			PPC_LL_OFFS(r_scratch1, r_skb, offsetof(struct sk_buff,
 +								dev));
 +			EMIT(PPC_RAW_CMPDI(r_scratch1, 0));
 +			if (ctx->pc_ret0 != -1) {
 +				PPC_BCC(COND_EQ, addrs[ctx->pc_ret0]);
 +			} else {
 +				/* Exit, returning 0; first pass hits here. */
 +				PPC_BCC_SHORT(COND_NE, ctx->idx * 4 + 12);
 +				EMIT(PPC_RAW_LI(r_ret, 0));
 +				PPC_JMP(exit_addr);
 +			}
 +			if (code == (BPF_ANC | SKF_AD_IFINDEX)) {
 +				PPC_LWZ_OFFS(r_A, r_scratch1,
 +				     offsetof(struct net_device, ifindex));
 +			} else {
 +				PPC_LHZ_OFFS(r_A, r_scratch1,
 +				     offsetof(struct net_device, type));
 +			}
 +
 +			break;
 +		case BPF_ANC | SKF_AD_MARK:
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +			PPC_LWZ_OFFS(r_A, r_skb, offsetof(struct sk_buff,
 +							  mark));
 +			break;
 +		case BPF_ANC | SKF_AD_RXHASH:
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +			PPC_LWZ_OFFS(r_A, r_skb, offsetof(struct sk_buff,
 +							  hash));
 +			break;
 +		case BPF_ANC | SKF_AD_VLAN_TAG:
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +
 +			PPC_LHZ_OFFS(r_A, r_skb, offsetof(struct sk_buff,
 +							  vlan_tci));
 +#ifdef VLAN_TAG_PRESENT
 +			PPC_ANDI(r_A, r_A, ~VLAN_TAG_PRESENT);
 +#endif
 +			break;
 +		case BPF_ANC | SKF_AD_VLAN_TAG_PRESENT:
 +			PPC_LBZ_OFFS(r_A, r_skb, PKT_VLAN_PRESENT_OFFSET());
 +			if (PKT_VLAN_PRESENT_BIT)
 +				EMIT(PPC_RAW_SRWI(r_A, r_A, PKT_VLAN_PRESENT_BIT));
 +			if (PKT_VLAN_PRESENT_BIT < 7)
 +				EMIT(PPC_RAW_ANDI(r_A, r_A, 1));
 +			break;
 +		case BPF_ANC | SKF_AD_QUEUE:
 +			BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,
 +						  queue_mapping) != 2);
 +			PPC_LHZ_OFFS(r_A, r_skb, offsetof(struct sk_buff,
 +							  queue_mapping));
 +			break;
 +		case BPF_ANC | SKF_AD_PKTTYPE:
 +			PPC_LBZ_OFFS(r_A, r_skb, PKT_TYPE_OFFSET());
 +			EMIT(PPC_RAW_ANDI(r_A, r_A, PKT_TYPE_MAX));
 +			EMIT(PPC_RAW_SRWI(r_A, r_A, 5));
 +			break;
 +		case BPF_ANC | SKF_AD_CPU:
 +			PPC_BPF_LOAD_CPU(r_A);
 +			break;
 +			/*** Absolute loads from packet header/data ***/
 +		case BPF_LD | BPF_W | BPF_ABS:
 +			func = CHOOSE_LOAD_FUNC(K, sk_load_word);
 +			goto common_load;
 +		case BPF_LD | BPF_H | BPF_ABS:
 +			func = CHOOSE_LOAD_FUNC(K, sk_load_half);
 +			goto common_load;
 +		case BPF_LD | BPF_B | BPF_ABS:
 +			func = CHOOSE_LOAD_FUNC(K, sk_load_byte);
 +		common_load:
 +			/* Load from [K]. */
 +			ctx->seen |= SEEN_DATAREF;
 +			PPC_FUNC_ADDR(r_scratch1, func);
 +			EMIT(PPC_RAW_MTLR(r_scratch1));
 +			PPC_LI32(r_addr, K);
 +			EMIT(PPC_RAW_BLRL());
 +			/*
 +			 * Helper returns 'lt' condition on error, and an
 +			 * appropriate return value in r3
 +			 */
 +			PPC_BCC(COND_LT, exit_addr);
 +			break;
 +
 +			/*** Indirect loads from packet header/data ***/
 +		case BPF_LD | BPF_W | BPF_IND:
 +			func = sk_load_word;
 +			goto common_load_ind;
 +		case BPF_LD | BPF_H | BPF_IND:
 +			func = sk_load_half;
 +			goto common_load_ind;
 +		case BPF_LD | BPF_B | BPF_IND:
 +			func = sk_load_byte;
 +		common_load_ind:
 +			/*
 +			 * Load from [X + K].  Negative offsets are tested for
 +			 * in the helper functions.
 +			 */
 +			ctx->seen |= SEEN_DATAREF | SEEN_XREG;
 +			PPC_FUNC_ADDR(r_scratch1, func);
 +			EMIT(PPC_RAW_MTLR(r_scratch1));
 +			EMIT(PPC_RAW_ADDI(r_addr, r_X, IMM_L(K)));
 +			if (K >= 32768)
 +				EMIT(PPC_RAW_ADDIS(r_addr, r_addr, IMM_HA(K)));
 +			EMIT(PPC_RAW_BLRL());
 +			/* If error, cr0.LT set */
 +			PPC_BCC(COND_LT, exit_addr);
 +			break;
 +
 +		case BPF_LDX | BPF_B | BPF_MSH:
 +			func = CHOOSE_LOAD_FUNC(K, sk_load_byte_msh);
 +			goto common_load;
 +			break;
 +
 +			/*** Jump and branches ***/
 +		case BPF_JMP | BPF_JA:
 +			if (K != 0)
 +				PPC_JMP(addrs[i + 1 + K]);
 +			break;
 +
 +		case BPF_JMP | BPF_JGT | BPF_K:
 +		case BPF_JMP | BPF_JGT | BPF_X:
 +			true_cond = COND_GT;
 +			goto cond_branch;
 +		case BPF_JMP | BPF_JGE | BPF_K:
 +		case BPF_JMP | BPF_JGE | BPF_X:
 +			true_cond = COND_GE;
 +			goto cond_branch;
 +		case BPF_JMP | BPF_JEQ | BPF_K:
 +		case BPF_JMP | BPF_JEQ | BPF_X:
 +			true_cond = COND_EQ;
 +			goto cond_branch;
 +		case BPF_JMP | BPF_JSET | BPF_K:
 +		case BPF_JMP | BPF_JSET | BPF_X:
 +			true_cond = COND_NE;
 +		cond_branch:
 +			/* same targets, can avoid doing the test :) */
 +			if (filter[i].jt == filter[i].jf) {
 +				if (filter[i].jt > 0)
 +					PPC_JMP(addrs[i + 1 + filter[i].jt]);
 +				break;
 +			}
 +
 +			switch (code) {
 +			case BPF_JMP | BPF_JGT | BPF_X:
 +			case BPF_JMP | BPF_JGE | BPF_X:
 +			case BPF_JMP | BPF_JEQ | BPF_X:
 +				ctx->seen |= SEEN_XREG;
 +				EMIT(PPC_RAW_CMPLW(r_A, r_X));
 +				break;
 +			case BPF_JMP | BPF_JSET | BPF_X:
 +				ctx->seen |= SEEN_XREG;
 +				EMIT(PPC_RAW_AND_DOT(r_scratch1, r_A, r_X));
 +				break;
 +			case BPF_JMP | BPF_JEQ | BPF_K:
 +			case BPF_JMP | BPF_JGT | BPF_K:
 +			case BPF_JMP | BPF_JGE | BPF_K:
 +				if (K < 32768)
 +					EMIT(PPC_RAW_CMPLWI(r_A, K));
 +				else {
 +					PPC_LI32(r_scratch1, K);
 +					EMIT(PPC_RAW_CMPLW(r_A, r_scratch1));
 +				}
 +				break;
 +			case BPF_JMP | BPF_JSET | BPF_K:
 +				if (K < 32768)
 +					/* PPC_ANDI is /only/ dot-form */
 +					EMIT(PPC_RAW_ANDI(r_scratch1, r_A, K));
 +				else {
 +					PPC_LI32(r_scratch1, K);
 +					EMIT(PPC_RAW_AND_DOT(r_scratch1, r_A,
 +						    r_scratch1));
 +				}
 +				break;
 +			}
 +			/* Sometimes branches are constructed "backward", with
 +			 * the false path being the branch and true path being
 +			 * a fallthrough to the next instruction.
 +			 */
 +			if (filter[i].jt == 0)
 +				/* Swap the sense of the branch */
 +				PPC_BCC(true_cond ^ COND_CMP_TRUE,
 +					addrs[i + 1 + filter[i].jf]);
 +			else {
 +				PPC_BCC(true_cond, addrs[i + 1 + filter[i].jt]);
 +				if (filter[i].jf != 0)
 +					PPC_JMP(addrs[i + 1 + filter[i].jf]);
 +			}
 +			break;
 +		default:
 +			/* The filter contains something cruel & unusual.
 +			 * We don't handle it, but also there shouldn't be
 +			 * anything missing from our list.
 +			 */
 +			if (printk_ratelimit())
 +				pr_err("BPF filter opcode %04x (@%d) unsupported\n",
 +				       filter[i].code, i);
 +			return -ENOTSUPP;
 +		}
 +
 +	}
 +	/* Set end-of-body-code address for exit. */
 +	addrs[i] = ctx->idx * 4;
 +
 +	return 0;
  }
  
 -struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *fp)
 +void bpf_jit_compile(struct bpf_prog *fp)
  {
 -	u32 proglen;
 -	u32 alloclen;
 -	u8 *image = NULL;
 +	unsigned int proglen;
 +	unsigned int alloclen;
 +	u32 *image = NULL;
  	u32 *code_base;
 -	u32 *addrs;
 -	struct powerpc64_jit_data *jit_data;
 +	unsigned int *addrs;
  	struct codegen_context cgctx;
  	int pass;
 -	int flen;
 -	struct bpf_binary_header *bpf_hdr;
 -	struct bpf_prog *org_fp = fp;
 -	struct bpf_prog *tmp_fp;
 -	bool bpf_blinded = false;
 -	bool extra_pass = false;
 -
 -	if (!fp->jit_requested)
 -		return org_fp;
 -
 -	tmp_fp = bpf_jit_blind_constants(org_fp);
 -	if (IS_ERR(tmp_fp))
 -		return org_fp;
 -
 -	if (tmp_fp != org_fp) {
 -		bpf_blinded = true;
 -		fp = tmp_fp;
 -	}
 -
 -	jit_data = fp->aux->jit_data;
 -	if (!jit_data) {
 -		jit_data = kzalloc(sizeof(*jit_data), GFP_KERNEL);
 -		if (!jit_data) {
 -			fp = org_fp;
 -			goto out;
 -		}
 -		fp->aux->jit_data = jit_data;
 -	}
 +	int flen = fp->len;
  
 -	flen = fp->len;
 -	addrs = jit_data->addrs;
 -	if (addrs) {
 -		cgctx = jit_data->ctx;
 -		image = jit_data->image;
 -		bpf_hdr = jit_data->header;
 -		proglen = jit_data->proglen;
 -		alloclen = proglen + FUNCTION_DESCR_SIZE;
 -		extra_pass = true;
 -		goto skip_init_ctx;
 -	}
 +	if (!bpf_jit_enable)
 +		return;
  
  	addrs = kcalloc(flen + 1, sizeof(*addrs), GFP_KERNEL);
++<<<<<<< HEAD
 +	if (addrs == NULL)
 +		return;
++=======
+ 	if (addrs == NULL) {
+ 		fp = org_fp;
+ 		goto out_addrs;
+ 	}
+ 
+ 	memset(&cgctx, 0, sizeof(struct codegen_context));
+ 	memcpy(cgctx.b2p, b2p, sizeof(cgctx.b2p));
+ 
+ 	/* Make sure that the stack is quadword aligned. */
+ 	cgctx.stack_size = round_up(fp->aux->stack_depth, 16);
+ 
+ 	/* Scouting faux-generate pass 0 */
+ 	if (bpf_jit_build_body(fp, 0, &cgctx, addrs, false)) {
+ 		/* We hit something illegal or unsupported. */
+ 		fp = org_fp;
+ 		goto out_addrs;
+ 	}
++>>>>>>> 40272035e1d0 (powerpc/bpf: Reallocate BPF registers to volatile registers when possible on PPC32)
  
  	/*
 -	 * If we have seen a tail call, we need a second pass.
 -	 * This is because bpf_jit_emit_common_epilogue() is called
 -	 * from bpf_jit_emit_tail_call() with a not yet stable ctx->seen.
 +	 * There are multiple assembly passes as the generated code will change
 +	 * size as it settles down, figuring out the max branch offsets/exit
 +	 * paths required.
 +	 *
 +	 * The range of standard conditional branches is +/- 32Kbytes.	Since
 +	 * BPF_MAXINSNS = 4096, we can only jump from (worst case) start to
 +	 * finish with 8 bytes/instruction.  Not feasible, so long jumps are
 +	 * used, distinct from short branches.
 +	 *
 +	 * Current:
 +	 *
 +	 * For now, both branch types assemble to 2 words (short branches padded
 +	 * with a NOP); this is less efficient, but assembly will always complete
 +	 * after exactly 3 passes:
 +	 *
 +	 * First pass: No code buffer; Program is "faux-generated" -- no code
 +	 * emitted but maximum size of output determined (and addrs[] filled
 +	 * in).	 Also, we note whether we use M[], whether we use skb data, etc.
 +	 * All generation choices assumed to be 'worst-case', e.g. branches all
 +	 * far (2 instructions), return path code reduction not available, etc.
 +	 *
 +	 * Second pass: Code buffer allocated with size determined previously.
 +	 * Prologue generated to support features we have seen used.  Exit paths
 +	 * determined and addrs[] is filled in again, as code may be slightly
 +	 * smaller as a result.
 +	 *
 +	 * Third pass: Code generated 'for real', and branch destinations
 +	 * determined from now-accurate addrs[] map.
 +	 *
 +	 * Ideal:
 +	 *
 +	 * If we optimise this, near branches will be shorter.	On the
 +	 * first assembly pass, we should err on the side of caution and
 +	 * generate the biggest code.  On subsequent passes, branches will be
 +	 * generated short or long and code size will reduce.  With smaller
 +	 * code, more branches may fall into the short category, and code will
 +	 * reduce more.
 +	 *
 +	 * Finally, if we see one pass generate code the same size as the
 +	 * previous pass we have converged and should now generate code for
 +	 * real.  Allocating at the end will also save the memory that would
 +	 * otherwise be wasted by the (small) current code shrinkage.
 +	 * Preferably, we should do a small number of passes (e.g. 5) and if we
 +	 * haven't converged by then, get impatient and force code to generate
 +	 * as-is, even if the odd branch would be left long.  The chances of a
 +	 * long jump are tiny with all but the most enormous of BPF filter
 +	 * inputs, so we should usually converge on the third pass.
  	 */
 -	if (cgctx.seen & SEEN_TAILCALL) {
 -		cgctx.idx = 0;
 -		if (bpf_jit_build_body(fp, 0, &cgctx, addrs, false)) {
 -			fp = org_fp;
 -			goto out_addrs;
 -		}
 -	}
 +
 +	cgctx.idx = 0;
 +	cgctx.seen = 0;
 +	cgctx.pc_ret0 = -1;
 +	/* Scouting faux-generate pass 0 */
 +	if (bpf_jit_build_body(fp, 0, &cgctx, addrs))
 +		/* We hit something illegal or unsupported. */
 +		goto out;
  
+ 	bpf_jit_realloc_regs(&cgctx);
  	/*
  	 * Pretend to build prologue, given the features we've seen.  This will
  	 * update ctgtx.idx as it pretends to output instructions, then we can
diff --cc arch/powerpc/net/bpf_jit_comp64.c
index 1009d58e53fa,57a8c1153851..000000000000
--- a/arch/powerpc/net/bpf_jit_comp64.c
+++ b/arch/powerpc/net/bpf_jit_comp64.c
@@@ -72,7 -64,11 +72,15 @@@ static int bpf_jit_stack_offsetof(struc
  	BUG();
  }
  
++<<<<<<< HEAD
 +static void bpf_jit_build_prologue(u32 *image, struct codegen_context *ctx)
++=======
+ void bpf_jit_realloc_regs(struct codegen_context *ctx)
+ {
+ }
+ 
+ void bpf_jit_build_prologue(u32 *image, struct codegen_context *ctx)
++>>>>>>> 40272035e1d0 (powerpc/bpf: Reallocate BPF registers to volatile registers when possible on PPC32)
  {
  	int i;
  
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit.h
diff --git a/arch/powerpc/net/bpf_jit64.h b/arch/powerpc/net/bpf_jit64.h
index 732e55680cb4..68083ab51cf2 100644
--- a/arch/powerpc/net/bpf_jit64.h
+++ b/arch/powerpc/net/bpf_jit64.h
@@ -43,7 +43,7 @@
 #define TMP_REG_2	(MAX_BPF_JIT_REG + 1)
 
 /* BPF to ppc register mappings */
-static const int b2p[] = {
+const int b2p[MAX_BPF_JIT_REG + 2] = {
 	/* function return value */
 	[BPF_REG_0] = 8,
 	/* function arguments */
* Unmerged path arch/powerpc/net/bpf_jit_comp.c
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
