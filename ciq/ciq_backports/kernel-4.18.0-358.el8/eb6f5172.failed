x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit eb6f51723f03c9a1c098ed196a31a03e626b9fb6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/eb6f5172.failed

When xsave with init state optimization is used then a component's state
in the task's xsave buffer can be stale when the corresponding feature bit
is not set.

fpregs_get() and xfpregs_get() invoke fpstate_sanitize_xstate() to update
the task's xsave buffer before retrieving the FX or FP state. That's just
duplicated code as copy_xstate_to_kernel() already handles this correctly.

Add a copy mode argument to the function which allows to restrict the state
copy to the FP and SSE features.

Also rename the function to copy_xstate_to_uabi_buf() so the name reflects
what it is doing.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20210623121452.805327286@linutronix.de
(cherry picked from commit eb6f51723f03c9a1c098ed196a31a03e626b9fb6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/fpu/xstate.h
#	arch/x86/kernel/fpu/regset.c
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/include/asm/fpu/xstate.h
index ea7518773595,732ae793c2ab..000000000000
--- a/arch/x86/include/asm/fpu/xstate.h
+++ b/arch/x86/include/asm/fpu/xstate.h
@@@ -101,11 -101,8 +101,14 @@@ extern void __init update_regset_xstate
  					     u64 xstate_mask);
  
  void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);
 +const void *get_xsave_field_ptr(int xfeature_nr);
  int using_compacted_format(void);
  int xfeature_size(int xfeature_nr);
++<<<<<<< HEAD
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
 +int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
++=======
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
  int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
  void copy_supervisor_to_kernel(struct xregs_state *xsave);
diff --cc arch/x86/kernel/fpu/regset.c
index 7bcf779c46cb,783f84dfcd46..000000000000
--- a/arch/x86/kernel/fpu/regset.c
+++ b/arch/x86/kernel/fpu/regset.c
@@@ -86,40 -84,17 +86,45 @@@ int xfpregs_set(struct task_struct *tar
  }
  
  int xstateregs_get(struct task_struct *target, const struct user_regset *regset,
 -		struct membuf to)
 +		unsigned int pos, unsigned int count,
 +		void *kbuf, void __user *ubuf)
  {
  	struct fpu *fpu = &target->thread.fpu;
 +	struct xregs_state *xsave;
 +	int ret;
  
 -	if (!cpu_feature_enabled(X86_FEATURE_XSAVE))
 +	if (!boot_cpu_has(X86_FEATURE_XSAVE))
  		return -ENODEV;
  
 +	xsave = &fpu->state.xsave;
 +
  	fpu__prepare_read(fpu);
  
++<<<<<<< HEAD
 +	if (using_compacted_format()) {
 +		if (kbuf)
 +			ret = copy_xstate_to_kernel(kbuf, xsave, pos, count);
 +		else
 +			ret = copy_xstate_to_user(ubuf, xsave, pos, count);
 +	} else {
 +		fpstate_sanitize_xstate(fpu);
 +		/*
 +		 * Copy the 48 bytes defined by the software into the xsave
 +		 * area in the thread struct, so that we can copy the whole
 +		 * area to user using one user_regset_copyout().
 +		 */
 +		memcpy(&xsave->i387.sw_reserved, xstate_fx_sw_bytes, sizeof(xstate_fx_sw_bytes));
 +
 +		/*
 +		 * Copy the xstate memory layout.
 +		 */
 +		ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);
 +	}
 +	return ret;
++=======
+ 	copy_xstate_to_uabi_buf(to, &fpu->state.xsave, XSTATE_COPY_XSAVE);
+ 	return 0;
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  }
  
  int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
diff --cc arch/x86/kernel/fpu/xstate.c
index b679c5d6ee1e,8d023a26ae22..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1089,189 -1062,116 +1089,264 @@@ static inline bool xfeatures_mxcsr_quir
  	return true;
  }
  
 -static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
 -			 void *init_xstate, unsigned int size)
 +static void fill_gap(unsigned to, void **kbuf, unsigned *pos, unsigned *count)
  {
 -	membuf_write(to, from_xstate ? xstate : init_xstate, size);
 +	if (*pos < to) {
 +		unsigned size = to - *pos;
 +
 +		if (size > *count)
 +			size = *count;
 +		memcpy(*kbuf, (void *)&init_fpstate.xsave + *pos, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
  }
  
 +static void copy_part(unsigned offset, unsigned size, void *from,
 +			void **kbuf, unsigned *pos, unsigned *count)
 +{
 +	fill_gap(offset, kbuf, pos, count);
 +	if (size > *count)
 +		size = *count;
 +	if (size) {
 +		memcpy(*kbuf, from, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
 +}
 +
++<<<<<<< HEAD
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a kernel-space ptrace buffer.
 + *
 + * It supports partial copy but pos always starts from zero. This is called
 + * from xstateregs_get() and there we check the CPU has XSAVES.
 + */
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
++=======
+ /**
+  * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
+  * @to:		membuf descriptor
+  * @xsave:	The kernel xstate buffer to copy from
+  * @copy_mode:	The requested copy mode
+  *
+  * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
+  * format, i.e. from the kernel internal hardware dependent storage format
+  * to the requested @mode. UABI XSTATE is always uncompacted!
+  *
+  * It supports partial copy but @to.pos always starts from zero.
+  */
+ void copy_xstate_to_uabi_buf(struct membuf to, struct xregs_state *xsave,
+ 			     enum xstate_copy_mode copy_mode)
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  {
 -	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 -	struct xregs_state *xinit = &init_fpstate.xsave;
  	struct xstate_header header;
 -	unsigned int zerofrom;
 +	const unsigned off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 +	unsigned count = size_total;
  	int i;
  
++<<<<<<< HEAD
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
 +
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
++=======
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  	header.xfeatures = xsave->header.xfeatures;
- 	header.xfeatures &= xfeatures_mask_user();
  
+ 	/* Mask out the feature bits depending on copy mode */
+ 	switch (copy_mode) {
+ 	case XSTATE_COPY_FP:
+ 		header.xfeatures &= XFEATURE_MASK_FP;
+ 		break;
+ 
+ 	case XSTATE_COPY_FX:
+ 		header.xfeatures &= XFEATURE_MASK_FP | XFEATURE_MASK_SSE;
+ 		break;
+ 
+ 	case XSTATE_COPY_XSAVE:
+ 		header.xfeatures &= xfeatures_mask_user();
+ 		break;
+ 	}
+ 
++<<<<<<< HEAD
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(0, off_mxcsr,
 +			  &xsave->i387, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM))
 +		copy_part(off_mxcsr, MXCSR_AND_FLAGS_SIZE,
 +			  &xsave->i387.mxcsr, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(offsetof(struct fxregs_state, st_space), 128,
 +			  &xsave->i387.st_space, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_SSE)
 +		copy_part(xstate_offsets[XFEATURE_SSE], 256,
 +			  &xsave->i387.xmm_space, &kbuf, &offset_start, &count);
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	copy_part(offsetof(struct fxregs_state, sw_reserved), 48,
 +		  xstate_fx_sw_bytes, &kbuf, &offset_start, &count);
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	copy_part(offsetof(struct xregs_state, header), sizeof(header),
 +		  &header, &kbuf, &offset_start, &count);
++=======
+ 	/* Copy FP state up to MXCSR */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_FP, &to, &xsave->i387,
+ 		     &xinit->i387, off_mxcsr);
+ 
+ 	/* Copy MXCSR when SSE or YMM are set in the feature mask */
+ 	copy_feature(header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM),
+ 		     &to, &xsave->i387.mxcsr, &xinit->i387.mxcsr,
+ 		     MXCSR_AND_FLAGS_SIZE);
+ 
+ 	/* Copy the remaining FP state */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_FP,
+ 		     &to, &xsave->i387.st_space, &xinit->i387.st_space,
+ 		     sizeof(xsave->i387.st_space));
+ 
+ 	/* Copy the SSE state - shared with YMM, but independently managed */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_SSE,
+ 		     &to, &xsave->i387.xmm_space, &xinit->i387.xmm_space,
+ 		     sizeof(xsave->i387.xmm_space));
+ 
+ 	if (copy_mode != XSTATE_COPY_XSAVE)
+ 		goto out;
+ 
+ 	/* Zero the padding area */
+ 	membuf_zero(&to, sizeof(xsave->i387.padding));
+ 
+ 	/* Copy xsave->i387.sw_reserved */
+ 	membuf_write(&to, xstate_fx_sw_bytes, sizeof(xsave->i387.sw_reserved));
+ 
+ 	/* Copy the user space relevant state of @xsave->header */
+ 	membuf_write(&to, &header, sizeof(header));
+ 
+ 	zerofrom = offsetof(struct xregs_state, extended_state_area);
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  
  	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
  		/*
 -		 * The ptrace buffer is in non-compacted XSAVE format.
 -		 * In non-compacted format disabled features still occupy
 -		 * state space, but there is no state to copy from in the
 -		 * compacted init_fpstate. The gap tracking will zero this
 -		 * later.
 +		 * Copy only in-use xstates:
  		 */
 -		if (!(xfeatures_mask_user() & BIT_ULL(i)))
 -			continue;
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
  
 -		/*
 -		 * If there was a feature or alignment gap, zero the space
 -		 * in the destination buffer.
 -		 */
 -		if (zerofrom < xstate_offsets[i])
 -			membuf_zero(&to, xstate_offsets[i] - zerofrom);
 +			copy_part(xstate_offsets[i], xstate_sizes[i],
 +				  src, &kbuf, &offset_start, &count);
 +		}
  
 -		copy_feature(header.xfeatures & BIT_ULL(i), &to,
 -			     __raw_xsave_addr(xsave, i),
 -			     __raw_xsave_addr(xinit, i),
 -			     xstate_sizes[i]);
 +	}
 +	fill_gap(size_total, &kbuf, &offset_start, &count);
  
 +	return 0;
 +}
 +
 +static inline int
 +__copy_xstate_to_user(void __user *ubuf, const void *data, unsigned int offset, unsigned int size, unsigned int size_total)
 +{
 +	if (!size)
 +		return 0;
 +
 +	if (offset < size_total) {
 +		unsigned int copy = min(size, size_total - offset);
 +
 +		if (__copy_to_user(ubuf + offset, data, copy))
 +			return -EFAULT;
 +	}
 +	return 0;
 +}
 +
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a user-space buffer. It supports partial copy but pos always starts from
 + * zero. This is called from xstateregs_get() and there we check the CPU
 + * has XSAVES.
 + */
 +int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
 +{
 +	unsigned int offset, size;
 +	int ret, i;
 +	struct xstate_header header;
 +
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
 +
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
 +	header.xfeatures = xsave->header.xfeatures;
 +	header.xfeatures &= xfeatures_mask_user();
 +
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	offset = offsetof(struct xregs_state, header);
 +	size = sizeof(header);
 +
 +	ret = __copy_xstate_to_user(ubuf, &header, offset, size, size_total);
 +	if (ret)
 +		return ret;
 +
 +	for (i = 0; i < XFEATURE_MAX; i++) {
  		/*
 -		 * Keep track of the last copied state in the non-compacted
 -		 * target buffer for gap zeroing.
 +		 * Copy only in-use xstates:
  		 */
 -		zerofrom = xstate_offsets[i] + xstate_sizes[i];
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
 +
 +			offset = xstate_offsets[i];
 +			size = xstate_sizes[i];
 +
 +			/* The next component has to fit fully into the output buffer: */
 +			if (offset + size > size_total)
 +				break;
 +
 +			ret = __copy_xstate_to_user(ubuf, src, offset, size, size_total);
 +			if (ret)
 +				return ret;
 +		}
 +
 +	}
 +
++<<<<<<< HEAD
 +	if (xfeatures_mxcsr_quirk(header.xfeatures)) {
 +		offset = offsetof(struct fxregs_state, mxcsr);
 +		size = MXCSR_AND_FLAGS_SIZE;
 +		__copy_xstate_to_user(ubuf, &xsave->i387.mxcsr, offset, size, size_total);
  	}
  
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	offset = offsetof(struct fxregs_state, sw_reserved);
 +	size = sizeof(xstate_fx_sw_bytes);
 +
 +	ret = __copy_xstate_to_user(ubuf, xstate_fx_sw_bytes, offset, size, size_total);
 +	if (ret)
 +		return ret;
 +
 +	return 0;
++=======
+ out:
+ 	if (to.left)
+ 		membuf_zero(&to, to.left);
++>>>>>>> eb6f51723f03 (x86/fpu: Make copy_xstate_to_kernel() usable for [x]fpregs_get())
  }
  
  static inline bool mxcsr_valid(struct xstate_header *hdr, const u32 *mxcsr)
* Unmerged path arch/x86/include/asm/fpu/xstate.h
* Unmerged path arch/x86/kernel/fpu/regset.c
* Unmerged path arch/x86/kernel/fpu/xstate.c
