scsi: qla2xxx: edif: Add encryption to I/O path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Quinn Tran <qutran@marvell.com>
commit 44d018577f179383ea2c409f3a392e9dbd1a155e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/44d01857.failed

Some FC adapters from Marvell offer the ability to encrypt data in flight
(EDIF). This feature requires an application to act as an authenticator.

After the completion of PLOGI, both sides have authenticated and PRLI
completed, encrypted I/Os are allowed to proceed.

 - Use new firmware API to encrypt traffic on the wire

 - Add driver parameter to enable|disable EDIF feature

   # modprobe qla2xxx ql2xsecenable=1

Link: https://lore.kernel.org/r/20210624052606.21613-10-njavali@marvell.com
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Reviewed-by: Himanshu Madhani <himanshu.madhani@oracle.com>
Co-developed-by: Larry Wisneski <Larry.Wisneski@marvell.com>
	Signed-off-by: Larry Wisneski <Larry.Wisneski@marvell.com>
Co-developed-by: Duane Grigsby <duane.grigsby@marvell.com>
	Signed-off-by: Duane Grigsby <duane.grigsby@marvell.com>
Co-developed-by: Rick Hicksted Jr <rhicksted@marvell.com>
	Signed-off-by: Rick Hicksted Jr <rhicksted@marvell.com>
	Signed-off-by: Quinn Tran <qutran@marvell.com>
	Signed-off-by: Nilesh Javali <njavali@marvell.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 44d018577f179383ea2c409f3a392e9dbd1a155e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_edif.c
#	drivers/scsi/qla2xxx/qla_gbl.h
#	drivers/scsi/qla2xxx/qla_init.c
#	drivers/scsi/qla2xxx/qla_target.c
diff --cc drivers/scsi/qla2xxx/qla_edif.c
index 8c567362bb60,8e730cc882e6..000000000000
--- a/drivers/scsi/qla2xxx/qla_edif.c
+++ b/drivers/scsi/qla2xxx/qla_edif.c
@@@ -140,35 -547,30 +140,35 @@@ qla_edif_app_start(scsi_qla_host_t *vha
  	}
  
  	list_for_each_entry_safe(fcport, tf, &vha->vp_fcports, list) {
- 		if ((fcport->flags & FCF_FCSP_DEVICE)) {
- 			ql_dbg(ql_dbg_edif, vha, 0xf084,
- 			    "%s: sess %p %8phC lid %#04x s_id %06x logout %d\n",
- 			    __func__, fcport, fcport->port_name,
- 			    fcport->loop_id, fcport->d_id.b24,
- 			    fcport->logout_on_delete);
+ 		ql_dbg(ql_dbg_edif, vha, 0xf084,
+ 		    "%s: sess %p %8phC lid %#04x s_id %06x logout %d\n",
+ 		    __func__, fcport, fcport->port_name,
+ 		    fcport->loop_id, fcport->d_id.b24,
+ 		    fcport->logout_on_delete);
  
++<<<<<<< HEAD
 +			if (atomic_read(&vha->loop_state) == LOOP_DOWN)
 +				break;
++=======
+ 		ql_dbg(ql_dbg_edif, vha, 0xf084,
+ 		    "keep %d els_logo %d disc state %d auth state %d stop state %d\n",
+ 		    fcport->keep_nport_handle,
+ 		    fcport->send_els_logo, fcport->disc_state,
+ 		    fcport->edif.auth_state, fcport->edif.app_stop);
+ 
+ 		if (atomic_read(&vha->loop_state) == LOOP_DOWN)
+ 			break;
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  
- 			if (!fcport->edif.secured_login)
- 				continue;
+ 		fcport->edif.app_started = 1;
+ 		fcport->edif.app_stop = 0;
  
- 			fcport->edif.app_started = 1;
- 			if (fcport->edif.app_stop ||
- 			    (fcport->disc_state != DSC_LOGIN_COMPLETE &&
- 			     fcport->disc_state != DSC_LOGIN_PEND &&
- 			     fcport->disc_state != DSC_DELETED)) {
- 				/* no activity */
- 				fcport->edif.app_stop = 0;
- 
- 				ql_dbg(ql_dbg_edif, vha, 0x911e,
- 				    "%s wwpn %8phC calling qla_edif_reset_auth_wait\n",
- 				    __func__, fcport->port_name);
- 				fcport->edif.app_sess_online = 1;
- 				qla_edif_reset_auth_wait(fcport, DSC_LOGIN_PEND, 0);
- 			}
- 			qla_edif_sa_ctl_init(vha, fcport);
- 		}
+ 		ql_dbg(ql_dbg_edif, vha, 0x911e,
+ 		    "%s wwpn %8phC calling qla_edif_reset_auth_wait\n",
+ 		    __func__, fcport->port_name);
+ 		fcport->edif.app_sess_online = 1;
+ 		qla_edif_reset_auth_wait(fcport, DSC_LOGIN_PEND, 0);
+ 		qla_edif_sa_ctl_init(vha, fcport);
  	}
  
  	if (vha->pur_cinfo.enode_flags != ENODE_ACTIVE) {
@@@ -742,4 -1847,1562 +745,1565 @@@ qla_edb_stop(scsi_qla_host_t *vha
  		    "%s doorbell not enabled\n", __func__);
  		return;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/* grab lock so list doesn't move */
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	vha->e_dbell.db_flags &= ~EDB_ACTIVE; /* mark it not active */
+ 	/* hopefully this is a null list at this point */
+ 	list_for_each_entry_safe(node, q, &vha->e_dbell.head, list) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910f,
+ 		    "%s freeing edb_node type=%x\n",
+ 		    __func__, node->ntype);
+ 		qla_edb_node_free(vha, node);
+ 		list_del(&node->list);
+ 
+ 		kfree(node);
+ 	}
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* wake up doorbell waiters - they'll be dismissed with error code */
+ 	complete_all(&vha->e_dbell.dbell);
+ }
+ 
+ static struct edb_node *
+ qla_edb_node_alloc(scsi_qla_host_t *vha, uint32_t ntype)
+ {
+ 	struct edb_node	*node;
+ 
+ 	node = kzalloc(sizeof(*node), GFP_ATOMIC);
+ 	if (!node) {
+ 		/* couldn't get space */
+ 		ql_dbg(ql_dbg_edif, vha, 0x9100,
+ 		    "edb node unable to be allocated\n");
+ 		return NULL;
+ 	}
+ 
+ 	node->ntype = ntype;
+ 	INIT_LIST_HEAD(&node->list);
+ 	return node;
+ }
+ 
+ /* adds a already alllocated enode to the linked list */
+ static bool
+ qla_edb_node_add(scsi_qla_host_t *vha, struct edb_node *ptr)
+ {
+ 	unsigned long		flags;
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled\n", __func__);
+ 		return false;
+ 	}
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 	list_add_tail(&ptr->list, &vha->e_dbell.head);
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* ring doorbell for waiters */
+ 	complete(&vha->e_dbell.dbell);
+ 
+ 	return true;
+ }
+ 
+ /* adds event to doorbell list */
+ void
+ qla_edb_eventcreate(scsi_qla_host_t *vha, uint32_t dbtype,
+ 	uint32_t data, uint32_t data2, fc_port_t	*sfcport)
+ {
+ 	struct edb_node	*edbnode;
+ 	fc_port_t *fcport = sfcport;
+ 	port_id_t id;
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		return;
+ 	}
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		if (fcport)
+ 			fcport->edif.auth_state = dbtype;
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled (type=%d\n", __func__, dbtype);
+ 		return;
+ 	}
+ 
+ 	edbnode = qla_edb_node_alloc(vha, dbtype);
+ 	if (!edbnode) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to alloc db node\n", __func__);
+ 		return;
+ 	}
+ 
+ 	if (!fcport) {
+ 		id.b.domain = (data >> 16) & 0xff;
+ 		id.b.area = (data >> 8) & 0xff;
+ 		id.b.al_pa = data & 0xff;
+ 		ql_dbg(ql_dbg_edif, vha, 0x09222,
+ 		    "%s: Arrived s_id: %06x\n", __func__,
+ 		    id.b24);
+ 		fcport = qla2x00_find_fcport_by_pid(vha, &id);
+ 		if (!fcport) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			    "%s can't find fcport for sid= 0x%x - ignoring\n",
+ 			__func__, id.b24);
+ 			kfree(edbnode);
+ 			return;
+ 		}
+ 	}
+ 
+ 	/* populate the edb node */
+ 	switch (dbtype) {
+ 	case VND_CMD_AUTH_STATE_NEEDED:
+ 	case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 		edbnode->u.plogi_did.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 		edbnode->u.els_sid.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 		edbnode->u.sa_aen.port_id = fcport->d_id;
+ 		edbnode->u.sa_aen.status =  data;
+ 		edbnode->u.sa_aen.key_type =  data2;
+ 		break;
+ 	default:
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			"%s unknown type: %x\n", __func__, dbtype);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		edbnode = NULL;
+ 		break;
+ 	}
+ 
+ 	if (edbnode && (!qla_edb_node_add(vha, edbnode))) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to add dbnode\n", __func__);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		return;
+ 	}
+ 	if (edbnode && fcport)
+ 		fcport->edif.auth_state = dbtype;
+ 	ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 	    "%s Doorbell produced : type=%d %p\n", __func__, dbtype, edbnode);
+ }
+ 
+ static struct edb_node *
+ qla_edb_getnext(scsi_qla_host_t *vha)
+ {
+ 	unsigned long	flags;
+ 	struct edb_node	*edbnode = NULL;
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* db nodes are fifo - no qualifications done */
+ 	if (!list_empty(&vha->e_dbell.head)) {
+ 		edbnode = list_first_entry(&vha->e_dbell.head,
+ 		    struct edb_node, list);
+ 		list_del(&edbnode->list);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	return edbnode;
+ }
+ 
+ /*
+  * app uses separate thread to read this. It'll wait until the doorbell
+  * is rung by the driver or the max wait time has expired
+  */
+ ssize_t
+ edif_doorbell_show(struct device *dev, struct device_attribute *attr,
+ 		char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	struct edb_node	*dbnode = NULL;
+ 	struct edif_app_dbell *ap = (struct edif_app_dbell *)buf;
+ 	uint32_t dat_siz, buf_size, sz;
+ 
+ 	/* TODO: app currently hardcoded to 256. Will transition to bsg */
+ 	sz = 256;
+ 
+ 	/* stop new threads from waiting if we're not init'd */
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif db not enabled\n", __func__);
+ 		return 0;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif not enabled\n", __func__);
+ 		return -1;
+ 	}
+ 
+ 	buf_size = 0;
+ 	while ((sz - buf_size) >= sizeof(struct edb_node)) {
+ 		/* remove the next item from the doorbell list */
+ 		dat_siz = 0;
+ 		dbnode = qla_edb_getnext(vha);
+ 		if (dbnode) {
+ 			ap->event_code = dbnode->ntype;
+ 			switch (dbnode->ntype) {
+ 			case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 			case VND_CMD_AUTH_STATE_NEEDED:
+ 				ap->port_id = dbnode->u.plogi_did;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 				ap->port_id = dbnode->u.els_sid;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 				ap->port_id = dbnode->u.sa_aen.port_id;
+ 				memcpy(ap->event_data, &dbnode->u,
+ 						sizeof(struct edif_sa_update_aen));
+ 				dat_siz += sizeof(struct edif_sa_update_aen);
+ 				break;
+ 			default:
+ 				/* unknown node type, rtn unknown ntype */
+ 				ap->event_code = VND_CMD_AUTH_STATE_UNDEF;
+ 				memcpy(ap->event_data, &dbnode->ntype, 4);
+ 				dat_siz += 4;
+ 				break;
+ 			}
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 				"%s Doorbell consumed : type=%d %p\n",
+ 				__func__, dbnode->ntype, dbnode);
+ 			/* we're done with the db node, so free it up */
+ 			qla_edb_node_free(vha, dbnode);
+ 			kfree(dbnode);
+ 		} else {
+ 			break;
+ 		}
+ 
+ 		ap->event_data_size = dat_siz;
+ 		/* 8bytes = ap->event_code + ap->event_data_size */
+ 		buf_size += dat_siz + 8;
+ 		ap = (struct edif_app_dbell *)(buf + buf_size);
+ 	}
+ 	return buf_size;
+ }
+ 
+ static void qla_noop_sp_done(srb_t *sp, int res)
+ {
+ 	sp->free(sp);
+ }
+ 
+ /*
+  * Called from work queue
+  * build and send the sa_update iocb to delete an rx sa_index
+  */
+ int
+ qla24xx_issue_sa_replace_iocb(scsi_qla_host_t *vha, struct qla_work_evt *e)
+ {
+ 	srb_t *sp;
+ 	fc_port_t	*fcport = NULL;
+ 	struct srb_iocb *iocb_cmd = NULL;
+ 	int rval = QLA_SUCCESS;
+ 	struct	edif_sa_ctl *sa_ctl = e->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = e->u.sa_update.nport_handle;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 	    "%s: starting,  sa_ctl: %p\n", __func__, sa_ctl);
+ 
+ 	if (!sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "sa_ctl allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport = sa_ctl->fcport;
+ 
+ 	/* Alloc SRB structure */
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		 "SRB allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport->flags |= FCF_ASYNC_SENT;
+ 	iocb_cmd = &sp->u.iocb_cmd;
+ 	iocb_cmd->u.sa_update.sa_ctl = sa_ctl;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3073,
+ 	    "Enter: SA REPL portid=%06x, sa_ctl %p, index %x, nport_handle: 0x%x\n",
+ 	    fcport->d_id.b24, sa_ctl, sa_ctl->index, nport_handle);
+ 	/*
+ 	 * if this is a sadb cleanup delete, mark it so the isr can
+ 	 * take the correct action
+ 	 */
+ 	if (sa_ctl->flags & EDIF_SA_CTL_FLG_CLEANUP_DEL) {
+ 		/* mark this srb as a cleanup delete */
+ 		sp->flags |= SRB_EDIF_CLEANUP_DELETE;
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "%s: sp 0x%p flagged as cleanup delete\n", __func__, sp);
+ 	}
+ 
+ 	sp->type = SRB_SA_REPLACE;
+ 	sp->name = "SA_REPLACE";
+ 	sp->fcport = fcport;
+ 	sp->free = qla2x00_rel_sp;
+ 	sp->done = qla_noop_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	if (rval != QLA_SUCCESS)
+ 		rval = QLA_FUNCTION_FAILED;
+ 
+ 	return rval;
+ }
+ 
+ void qla24xx_sa_update_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	int	itr = 0;
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct	qla_sa_update_frame	*sa_frame =
+ 		&sp->u.iocb_cmd.u.sa_update.sa_frame;
+ 	u8 flags = 0;
+ 
+ 	switch (sa_frame->flags & (SAU_FLG_INV | SAU_FLG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_INVALIDATE;
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX;
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX | SA_FLAG_INVALIDATE;
+ 		break;
+ 	}
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	sa_update_iocb->flags = flags;
+ 	sa_update_iocb->salt = cpu_to_le32(sa_frame->salt);
+ 	sa_update_iocb->spi = cpu_to_le32(sa_frame->spi);
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_frame->fast_sa_index);
+ 
+ 	sa_update_iocb->sa_control |= SA_CNTL_ENC_FCSP;
+ 	if (sp->fcport->edif.aes_gmac)
+ 		sa_update_iocb->sa_control |= SA_CNTL_AES_GMAC;
+ 
+ 	if (sa_frame->flags & SAU_FLG_KEY256) {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY256;
+ 		for (itr = 0; itr < 32; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	} else {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY128;
+ 		for (itr = 0; itr < 16; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU Port ID = %02x%02x%02x, flags=%xh, index=%u, ctl=%xh, SPI 0x%x flags 0x%x hdl=%x gmac %d\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], sa_update_iocb->flags, sa_update_iocb->sa_index,
+ 	    sa_update_iocb->sa_control, sa_update_iocb->spi, sa_frame->flags, sp->handle,
+ 	    sp->fcport->edif.aes_gmac);
+ 
+ 	if (sa_frame->flags & SAU_FLG_TX)
+ 		sp->fcport->edif.tx_sa_pending = 1;
+ 	else
+ 		sp->fcport->edif.rx_sa_pending = 1;
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void
+ qla24xx_sa_replace_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct srb_iocb *srb_iocb = &sp->u.iocb_cmd;
+ 	struct	edif_sa_ctl		*sa_ctl = srb_iocb->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = sp->fcport->loop_id;
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(nport_handle);
+ 
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	/* Invalidate the index. salt, spi, control & key are ignore */
+ 	sa_update_iocb->flags = SA_FLAG_INVALIDATE;
+ 	sa_update_iocb->salt = 0;
+ 	sa_update_iocb->spi = 0;
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_ctl->index);
+ 	sa_update_iocb->sa_control = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU DELETE RX Port ID = %02x:%02x:%02x, lid %d flags=%xh, index=%u, hdl=%x\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], nport_handle, sa_update_iocb->flags,
+ 	    sa_update_iocb->sa_index, sp->handle);
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void qla24xx_auth_els(scsi_qla_host_t *vha, void **pkt, struct rsp_que **rsp)
+ {
+ 	struct purex_entry_24xx *p = *pkt;
+ 	struct enode		*ptr;
+ 	int		sid;
+ 	u16 totlen;
+ 	struct purexevent	*purex;
+ 	struct scsi_qla_host *host = NULL;
+ 	int rc;
+ 	struct fc_port *fcport;
+ 	struct qla_els_pt_arg a;
+ 	be_id_t beid;
+ 
+ 	memset(&a, 0, sizeof(a));
+ 
+ 	a.els_opcode = ELS_AUTH_ELS;
+ 	a.nport_handle = p->nport_handle;
+ 	a.rx_xchg_address = p->rx_xchg_addr;
+ 	a.did.b.domain = p->s_id[2];
+ 	a.did.b.area   = p->s_id[1];
+ 	a.did.b.al_pa  = p->s_id[0];
+ 	a.tx_byte_count = a.tx_len = sizeof(struct fc_els_ls_rjt);
+ 	a.tx_addr = vha->hw->elsrej.cdma;
+ 	a.vp_idx = vha->vp_idx;
+ 	a.control_flags = EPD_ELS_RJT;
+ 
+ 	sid = p->s_id[0] | (p->s_id[1] << 8) | (p->s_id[2] << 16);
+ 
+ 	totlen = (le16_to_cpu(p->frame_size) & 0x0fff) - PURX_ELS_HEADER_SIZE;
+ 	if (le16_to_cpu(p->status_flags) & 0x8000) {
+ 		totlen = le16_to_cpu(p->trunc_frame_size);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (totlen > MAX_PAYLOAD) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x0910d,
+ 		    "%s WARNING: verbose ELS frame received (totlen=%x)\n",
+ 		    __func__, totlen);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif support not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x910e, "%s edif not enabled\n",
+ 		    __func__);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	ptr = qla_enode_alloc(vha, N_PUREX);
+ 	if (!ptr) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09109,
+ 		    "WARNING: enode allloc failed for sid=%x\n",
+ 		    sid);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	purex = &ptr->u.purexinfo;
+ 	purex->pur_info.pur_sid = a.did;
+ 	purex->pur_info.pur_pend = 0;
+ 	purex->pur_info.pur_bytes_rcvd = totlen;
+ 	purex->pur_info.pur_rx_xchg_address = le32_to_cpu(p->rx_xchg_addr);
+ 	purex->pur_info.pur_nphdl = le16_to_cpu(p->nport_handle);
+ 	purex->pur_info.pur_did.b.domain =  p->d_id[2];
+ 	purex->pur_info.pur_did.b.area =  p->d_id[1];
+ 	purex->pur_info.pur_did.b.al_pa =  p->d_id[0];
+ 	purex->pur_info.vp_idx = p->vp_idx;
+ 
+ 	rc = __qla_copy_purex_to_buffer(vha, pkt, rsp, purex->msgp,
+ 		purex->msgp_len);
+ 	if (rc) {
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 	beid.al_pa = purex->pur_info.pur_did.b.al_pa;
+ 	beid.area   = purex->pur_info.pur_did.b.area;
+ 	beid.domain = purex->pur_info.pur_did.b.domain;
+ 	host = qla_find_host_by_d_id(vha, beid);
+ 	if (!host) {
+ 		ql_log(ql_log_fatal, vha, 0x508b,
+ 		    "%s Drop ELS due to unable to find host %06x\n",
+ 		    __func__, purex->pur_info.pur_did.b24);
+ 
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 
+ 	fcport = qla2x00_find_fcport_by_pid(host, &purex->pur_info.pur_sid);
+ 
+ 	if (host->e_dbell.db_flags != EDB_ACTIVE ||
+ 	    (fcport && fcport->loop_id == FC_NO_LOOP_ID)) {
+ 		ql_dbg(ql_dbg_edif, host, 0x0910c, "%s e_dbell.db_flags =%x %06x\n",
+ 		    __func__, host->e_dbell.db_flags,
+ 		    fcport ? fcport->d_id.b24 : 0);
+ 
+ 		qla_els_reject_iocb(host, (*rsp)->qpair, &a);
+ 		qla_enode_free(host, ptr);
+ 		return;
+ 	}
+ 
+ 	/* add the local enode to the list */
+ 	qla_enode_add(host, ptr);
+ 
+ 	ql_dbg(ql_dbg_edif, host, 0x0910c,
+ 	    "%s COMPLETE purex->pur_info.pur_bytes_rcvd =%xh s:%06x -> d:%06x xchg=%xh\n",
+ 	    __func__, purex->pur_info.pur_bytes_rcvd, purex->pur_info.pur_sid.b24,
+ 	    purex->pur_info.pur_did.b24, p->rx_xchg_addr);
+ 
+ 	qla_edb_eventcreate(host, VND_CMD_AUTH_STATE_ELS_RCVD, sid, 0, NULL);
+ }
+ 
+ static uint16_t  qla_edif_get_sa_index_from_freepool(fc_port_t *fcport, int dir)
+ {
+ 	struct scsi_qla_host *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	void *sa_id_map;
+ 	unsigned long flags = 0;
+ 	u16 sa_index;
+ 
+ 	ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_id_map = ha->edif_tx_sa_id_map;
+ 	else
+ 		sa_id_map = ha->edif_rx_sa_id_map;
+ 
+ 	spin_lock_irqsave(&ha->sadb_fp_lock, flags);
+ 	sa_index = find_first_zero_bit(sa_id_map, EDIF_NUM_SA_INDEX);
+ 	if (sa_index >=  EDIF_NUM_SA_INDEX) {
+ 		spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 	set_bit(sa_index, sa_id_map);
+ 	spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 
+ 	if (dir)
+ 		sa_index += EDIF_TX_SA_INDEX_BASE;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: index retrieved from free pool %d\n", __func__, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* find an sadb entry for an nport_handle */
+ static struct edif_sa_index_entry *
+ qla_edif_sadb_find_sa_index_entry(uint16_t nport_handle,
+ 		struct list_head *sa_list)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct edif_sa_index_entry *tentry;
+ 	struct list_head *indx_list = sa_list;
+ 
+ 	list_for_each_entry_safe(entry, tentry, indx_list, next) {
+ 		if (entry->handle == nport_handle)
+ 			return entry;
+ 	}
+ 	return NULL;
+ }
+ 
+ /* remove an sa_index from the nport_handle and return it to the free pool */
+ static int qla_edif_sadb_delete_sa_index(fc_port_t *fcport, uint16_t nport_handle,
+ 		uint16_t sa_index)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	int dir = (sa_index < EDIF_TX_SA_INDEX_BASE) ? 0 : 1;
+ 	int slot = 0;
+ 	int free_slot_count = 0;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: no entry found for nport_handle 0x%x\n",
+ 		    __func__, nport_handle);
+ 		return -1;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	/*
+ 	 * each tx/rx direction has up to 2 sa indexes/slots. 1 slot for in flight traffic
+ 	 * the other is use at re-key time.
+ 	 */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == sa_index) {
+ 			entry->sa_pair[slot].sa_index = INVALID_EDIF_SA_INDEX;
+ 			entry->sa_pair[slot].spi = 0;
+ 			free_slot_count++;
+ 			qla_edif_add_sa_index_to_freepool(fcport, dir, sa_index);
+ 		} else if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot_count++;
+ 		}
+ 	}
+ 
+ 	if (free_slot_count == 2) {
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: sa_index %d removed, free_slot_count: %d\n",
+ 	    __func__, sa_index, free_slot_count);
+ 
+ 	return 0;
+ }
+ 
+ void
+ qla28xx_sa_update_iocb_entry(scsi_qla_host_t *v, struct req_que *req,
+ 	struct sa_update_28xx *pkt)
+ {
+ 	const char *func = "SA_UPDATE_RESPONSE_IOCB";
+ 	srb_t *sp;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	int old_sa_deleted = 1;
+ 	uint16_t nport_handle;
+ 	struct scsi_qla_host *vha;
+ 
+ 	sp = qla2x00_get_sp_from_handle(v, func, req, pkt);
+ 
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, v, 0x3063,
+ 			"%s: no sp found for pkt\n", __func__);
+ 		return;
+ 	}
+ 	/* use sp->vha due to npiv */
+ 	vha = sp->vha;
+ 
+ 	switch (pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * dig the nport handle out of the iocb, fcport->loop_id can not be trusted
+ 	 * to be correct during cleanup sa_update iocbs.
+ 	 */
+ 	nport_handle = sp->fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: %8phN comp status=%x old_sa_info=%x new_sa_info=%x lid %d, index=0x%x pkt_flags %xh hdl=%x\n",
+ 	    __func__, sp->fcport->port_name, pkt->u.comp_sts, pkt->old_sa_info, pkt->new_sa_info,
+ 	    nport_handle, pkt->sa_index, pkt->flags, sp->handle);
+ 
+ 	/* if rx delete, remove the timer */
+ 	if ((pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) ==  SA_FLAG_INVALIDATE) {
+ 		struct edif_list_entry *edif_entry;
+ 
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 
+ 		edif_entry = qla_edif_list_find_sa_index(sp->fcport, nport_handle);
+ 		if (edif_entry) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: removing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 			qla_edif_list_delete_sa_index(sp->fcport, edif_entry);
+ 			del_timer(&edif_entry->timer);
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: releasing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 
+ 			kfree(edif_entry);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * if this is a delete for either tx or rx, make sure it succeeded.
+ 	 * The new_sa_info field should be 0xffff on success
+ 	 */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE)
+ 		old_sa_deleted = (le16_to_cpu(pkt->new_sa_info) == 0xffff) ? 1 : 0;
+ 
+ 	/* Process update and delete the same way */
+ 
+ 	/* If this is an sadb cleanup delete, bypass sending events to IPSEC */
+ 	if (sp->flags & SRB_EDIF_CLEANUP_DELETE) {
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: nph 0x%x, sa_index %d removed from fw\n",
+ 		    __func__, sp->fcport->loop_id, pkt->sa_index);
+ 
+ 	} else if ((pkt->entry_status == 0) && (pkt->u.comp_sts == 0) &&
+ 	    old_sa_deleted) {
+ 		/*
+ 		 * Note: Wa are only keeping track of latest SA,
+ 		 * so we know when we can start enableing encryption per I/O.
+ 		 * If all SA's get deleted, let FW reject the IOCB.
+ 
+ 		 * TODO: edif: don't set enabled here I think
+ 		 * TODO: edif: prli complete is where it should be set
+ 		 */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			"SA(%x)updated for s_id %02x%02x%02x\n",
+ 			pkt->new_sa_info,
+ 			pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 		sp->fcport->edif.enable = 1;
+ 		if (pkt->flags & SA_FLAG_TX) {
+ 			sp->fcport->edif.tx_sa_set = 1;
+ 			sp->fcport->edif.tx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		} else {
+ 			sp->fcport->edif.rx_sa_set = 1;
+ 			sp->fcport->edif.rx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 		}
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: %8phN SA update FAILED: sa_index: %d, new_sa_info %d, %02x%02x%02x\n",
+ 		    __func__, sp->fcport->port_name, pkt->sa_index, pkt->new_sa_info,
+ 		    pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 
+ 		if (pkt->flags & SA_FLAG_TX)
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		else
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 	}
+ 
+ 	/* for delete, release sa_ctl, sa_index */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE) {
+ 		/* release the sa_ctl */
+ 		sa_ctl = qla_edif_find_sa_ctl_by_index(sp->fcport,
+ 		    le16_to_cpu(pkt->sa_index), (pkt->flags & SA_FLAG_TX));
+ 		if (sa_ctl &&
+ 		    qla_edif_find_sa_ctl_by_index(sp->fcport, sa_ctl->index,
+ 			(pkt->flags & SA_FLAG_TX)) != NULL) {
+ 			ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			    "%s: freeing sa_ctl for index %d\n",
+ 			    __func__, sa_ctl->index);
+ 			qla_edif_free_sa_ctl(sp->fcport, sa_ctl, sa_ctl->index);
+ 		} else {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: sa_ctl NOT freed, sa_ctl: %p\n",
+ 			    __func__, sa_ctl);
+ 		}
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, le16_to_cpu(pkt->sa_index), nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 	/*
+ 	 * check for a failed sa_update and remove
+ 	 * the sadb entry.
+ 	 */
+ 	} else if (pkt->u.comp_sts) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, pkt->sa_index, nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 	}
+ 
+ 	sp->done(sp, 0);
+ }
+ 
+ /**
+  * qla28xx_start_scsi_edif() - Send a SCSI type 6 command to the ISP
+  * @sp: command to send to the ISP
+  *
+  * Return: non-zero if a failure occurred, else zero.
+  */
+ int
+ qla28xx_start_scsi_edif(srb_t *sp)
+ {
+ 	int             nseg;
+ 	unsigned long   flags;
+ 	struct scsi_cmnd *cmd;
+ 	uint32_t        *clr_ptr;
+ 	uint32_t        index, i;
+ 	uint32_t        handle;
+ 	uint16_t        cnt;
+ 	int16_t        req_cnt;
+ 	uint16_t        tot_dsds;
+ 	__be32 *fcp_dl;
+ 	uint8_t additional_cdb_len;
+ 	struct ct6_dsd *ctx;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	struct cmd_type_6 *cmd_pkt;
+ 	struct dsd64	*cur_dsd;
+ 	uint8_t		avail_dsds = 0;
+ 	struct scatterlist *sg;
+ 	struct req_que *req = sp->qpair->req;
+ 	spinlock_t *lock = sp->qpair->qp_lock_ptr;
+ 
+ 	/* Setup device pointers. */
+ 	cmd = GET_CMD_SP(sp);
+ 
+ 	/* So we know we haven't pci_map'ed anything yet */
+ 	tot_dsds = 0;
+ 
+ 	/* Send marker if required */
+ 	if (vha->marker_needed != 0) {
+ 		if (qla2x00_marker(vha, sp->qpair, 0, 0, MK_SYNC_ALL) !=
+ 			QLA_SUCCESS) {
+ 			ql_log(ql_log_warn, vha, 0x300c,
+ 			    "qla2x00_marker failed for cmd=%p.\n", cmd);
+ 			return QLA_FUNCTION_FAILED;
+ 		}
+ 		vha->marker_needed = 0;
+ 	}
+ 
+ 	/* Acquire ring specific lock */
+ 	spin_lock_irqsave(lock, flags);
+ 
+ 	/* Check for room in outstanding command list. */
+ 	handle = req->current_outstanding_cmd;
+ 	for (index = 1; index < req->num_outstanding_cmds; index++) {
+ 		handle++;
+ 		if (handle == req->num_outstanding_cmds)
+ 			handle = 1;
+ 		if (!req->outstanding_cmds[handle])
+ 			break;
+ 	}
+ 	if (index == req->num_outstanding_cmds)
+ 		goto queuing_error;
+ 
+ 	/* Map the sg table so we have an accurate count of sg entries needed */
+ 	if (scsi_sg_count(cmd)) {
+ 		nseg = dma_map_sg(&ha->pdev->dev, scsi_sglist(cmd),
+ 		    scsi_sg_count(cmd), cmd->sc_data_direction);
+ 		if (unlikely(!nseg))
+ 			goto queuing_error;
+ 	} else {
+ 		nseg = 0;
+ 	}
+ 
+ 	tot_dsds = nseg;
+ 	req_cnt = qla24xx_calc_iocbs(vha, tot_dsds);
+ 	if (req->cnt < (req_cnt + 2)) {
+ 		cnt = IS_SHADOW_REG_CAPABLE(ha) ? *req->out_ptr :
+ 		    rd_reg_dword(req->req_q_out);
+ 		if (req->ring_index < cnt)
+ 			req->cnt = cnt - req->ring_index;
+ 		else
+ 			req->cnt = req->length -
+ 			    (req->ring_index - cnt);
+ 		if (req->cnt < (req_cnt + 2))
+ 			goto queuing_error;
+ 	}
+ 
+ 	ctx = sp->u.scmd.ct6_ctx =
+ 	    mempool_alloc(ha->ctx_mempool, GFP_ATOMIC);
+ 	if (!ctx) {
+ 		ql_log(ql_log_fatal, vha, 0x3010,
+ 		    "Failed to allocate ctx for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	memset(ctx, 0, sizeof(struct ct6_dsd));
+ 	ctx->fcp_cmnd = dma_pool_zalloc(ha->fcp_cmnd_dma_pool,
+ 	    GFP_ATOMIC, &ctx->fcp_cmnd_dma);
+ 	if (!ctx->fcp_cmnd) {
+ 		ql_log(ql_log_fatal, vha, 0x3011,
+ 		    "Failed to allocate fcp_cmnd for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	/* Initialize the DSD list and dma handle */
+ 	INIT_LIST_HEAD(&ctx->dsd_list);
+ 	ctx->dsd_use_cnt = 0;
+ 
+ 	if (cmd->cmd_len > 16) {
+ 		additional_cdb_len = cmd->cmd_len - 16;
+ 		if ((cmd->cmd_len % 4) != 0) {
+ 			/*
+ 			 * SCSI command bigger than 16 bytes must be
+ 			 * multiple of 4
+ 			 */
+ 			ql_log(ql_log_warn, vha, 0x3012,
+ 			    "scsi cmd len %d not multiple of 4 for cmd=%p.\n",
+ 			    cmd->cmd_len, cmd);
+ 			goto queuing_error_fcp_cmnd;
+ 		}
+ 		ctx->fcp_cmnd_len = 12 + cmd->cmd_len + 4;
+ 	} else {
+ 		additional_cdb_len = 0;
+ 		ctx->fcp_cmnd_len = 12 + 16 + 4;
+ 	}
+ 
+ 	cmd_pkt = (struct cmd_type_6 *)req->ring_ptr;
+ 	cmd_pkt->handle = make_handle(req->id, handle);
+ 
+ 	/*
+ 	 * Zero out remaining portion of packet.
+ 	 * tagged queuing modifier -- default is TSK_SIMPLE (0).
+ 	 */
+ 	clr_ptr = (uint32_t *)cmd_pkt + 2;
+ 	memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
+ 	cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
+ 
+ 	/* No data transfer */
+ 	if (!scsi_bufflen(cmd) || cmd->sc_data_direction == DMA_NONE) {
+ 		cmd_pkt->byte_count = cpu_to_le32(0);
+ 		goto no_dsds;
+ 	}
+ 
+ 	/* Set transfer direction */
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_WRITE_DATA);
+ 		vha->qla_stats.output_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.output_requests++;
+ 		sp->fcport->edif.tx_bytes += scsi_bufflen(cmd);
+ 	} else if (cmd->sc_data_direction == DMA_FROM_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_READ_DATA);
+ 		vha->qla_stats.input_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.input_requests++;
+ 		sp->fcport->edif.rx_bytes += scsi_bufflen(cmd);
+ 	}
+ 
+ 	cmd_pkt->control_flags |= cpu_to_le16(CF_EN_EDIF);
+ 	cmd_pkt->control_flags &= ~(cpu_to_le16(CF_NEW_SA));
+ 
+ 	/* One DSD is available in the Command Type 6 IOCB */
+ 	avail_dsds = 1;
+ 	cur_dsd = &cmd_pkt->fcp_dsd;
+ 
+ 	/* Load data segments */
+ 	scsi_for_each_sg(cmd, sg, tot_dsds, i) {
+ 		dma_addr_t      sle_dma;
+ 		cont_a64_entry_t *cont_pkt;
+ 
+ 		/* Allocate additional continuation packets? */
+ 		if (avail_dsds == 0) {
+ 			/*
+ 			 * Five DSDs are available in the Continuation
+ 			 * Type 1 IOCB.
+ 			 */
+ 			cont_pkt = qla2x00_prep_cont_type1_iocb(vha, req);
+ 			cur_dsd = cont_pkt->dsd;
+ 			avail_dsds = 5;
+ 		}
+ 
+ 		sle_dma = sg_dma_address(sg);
+ 		put_unaligned_le64(sle_dma, &cur_dsd->address);
+ 		cur_dsd->length = cpu_to_le32(sg_dma_len(sg));
+ 		cur_dsd++;
+ 		avail_dsds--;
+ 	}
+ 
+ no_dsds:
+ 	/* Set NPORT-ID and LUN number*/
+ 	cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
+ 	cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
+ 	cmd_pkt->vp_index = sp->vha->vp_idx;
+ 
+ 	cmd_pkt->entry_type = COMMAND_TYPE_6;
+ 
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 
+ 	int_to_scsilun(cmd->device->lun, &cmd_pkt->lun);
+ 	host_to_fcp_swap((uint8_t *)&cmd_pkt->lun, sizeof(cmd_pkt->lun));
+ 
+ 	/* build FCP_CMND IU */
+ 	int_to_scsilun(cmd->device->lun, &ctx->fcp_cmnd->lun);
+ 	ctx->fcp_cmnd->additional_cdb_len = additional_cdb_len;
+ 
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 1;
+ 	else if (cmd->sc_data_direction == DMA_FROM_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 2;
+ 
+ 	/* Populate the FCP_PRIO. */
+ 	if (ha->flags.fcp_prio_enabled)
+ 		ctx->fcp_cmnd->task_attribute |=
+ 		    sp->fcport->fcp_prio << 3;
+ 
+ 	memcpy(ctx->fcp_cmnd->cdb, cmd->cmnd, cmd->cmd_len);
+ 
+ 	fcp_dl = (__be32 *)(ctx->fcp_cmnd->cdb + 16 +
+ 	    additional_cdb_len);
+ 	*fcp_dl = htonl((uint32_t)scsi_bufflen(cmd));
+ 
+ 	cmd_pkt->fcp_cmnd_dseg_len = cpu_to_le16(ctx->fcp_cmnd_len);
+ 	put_unaligned_le64(ctx->fcp_cmnd_dma, &cmd_pkt->fcp_cmnd_dseg_address);
+ 
+ 	sp->flags |= SRB_FCP_CMND_DMA_VALID;
+ 	cmd_pkt->byte_count = cpu_to_le32((uint32_t)scsi_bufflen(cmd));
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 	cmd_pkt->entry_status = 0;
+ 
+ 	/* Build command packet. */
+ 	req->current_outstanding_cmd = handle;
+ 	req->outstanding_cmds[handle] = sp;
+ 	sp->handle = handle;
+ 	cmd->host_scribble = (unsigned char *)(unsigned long)handle;
+ 	req->cnt -= req_cnt;
+ 
+ 	/* Adjust ring index. */
+ 	wmb();
+ 	req->ring_index++;
+ 	if (req->ring_index == req->length) {
+ 		req->ring_index = 0;
+ 		req->ring_ptr = req->ring;
+ 	} else {
+ 		req->ring_ptr++;
+ 	}
+ 
+ 	/* Set chip new ring index. */
+ 	wrt_reg_dword(req->req_q_in, req->ring_index);
+ 
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_SUCCESS;
+ 
+ queuing_error_fcp_cmnd:
+ 	dma_pool_free(ha->fcp_cmnd_dma_pool, ctx->fcp_cmnd, ctx->fcp_cmnd_dma);
+ queuing_error:
+ 	if (tot_dsds)
+ 		scsi_dma_unmap(cmd);
+ 
+ 	if (sp->u.scmd.ct6_ctx) {
+ 		mempool_free(sp->u.scmd.ct6_ctx, ha->ctx_mempool);
+ 		sp->u.scmd.ct6_ctx = NULL;
+ 	}
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_FUNCTION_FAILED;
+ }
+ 
+ /**********************************************
+  * edif update/delete sa_index list functions *
+  **********************************************/
+ 
+ /* clear the edif_indx_list for this port */
+ void qla_edif_list_del(fc_port_t *fcport)
+ {
+ 	struct edif_list_entry *indx_lst;
+ 	struct edif_list_entry *tindx_lst;
+ 	struct list_head *indx_list = &fcport->edif.edif_indx_list;
+ 	unsigned long flags = 0;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	list_for_each_entry_safe(indx_lst, tindx_lst, indx_list, next) {
+ 		list_del(&indx_lst->next);
+ 		kfree(indx_lst);
+ 	}
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ }
+ 
+ /******************
+  * SADB functions *
+  ******************/
+ 
+ /* allocate/retrieve an sa_index for a given spi */
+ static uint16_t qla_edif_sadb_get_sa_index(fc_port_t *fcport,
+ 		struct qla_sa_update_frame *sa_frame)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	uint16_t sa_index;
+ 	int dir = sa_frame->flags & SAU_FLG_TX;
+ 	int slot = 0;
+ 	int free_slot = -1;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry  fc_port: %p, nport_handle: 0x%x\n",
+ 	    __func__, fcport, nport_handle);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		if ((sa_frame->flags & (SAU_FLG_TX | SAU_FLG_INV)) == SAU_FLG_INV) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: rx delete request with no entry\n", __func__);
+ 			return RX_DELETE_NO_EDIF_SA_INDEX;
+ 		}
+ 
+ 		/* if there is no entry for this nport, add one */
+ 		entry = kzalloc((sizeof(struct edif_sa_index_entry)), GFP_ATOMIC);
+ 		if (!entry)
+ 			return INVALID_EDIF_SA_INDEX;
+ 
+ 		sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 		if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 			kfree(entry);
+ 			return INVALID_EDIF_SA_INDEX;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&entry->next);
+ 		entry->handle = nport_handle;
+ 		entry->fcport = fcport;
+ 		entry->sa_pair[0].spi = sa_frame->spi;
+ 		entry->sa_pair[0].sa_index = sa_index;
+ 		entry->sa_pair[1].spi = 0;
+ 		entry->sa_pair[1].sa_index = INVALID_EDIF_SA_INDEX;
+ 		spin_lock_irqsave(&ha->sadb_lock, flags);
+ 		list_add_tail(&entry->next, sa_list);
+ 		spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Created new sadb entry for nport_handle 0x%x, spi 0x%x, returning sa_index %d\n",
+ 		    __func__, nport_handle, sa_frame->spi, sa_index);
+ 
+ 		return sa_index;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 
+ 	/* see if we already have an entry for this spi */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot = slot;
+ 		} else {
+ 			if (entry->sa_pair[slot].spi == sa_frame->spi) {
+ 				spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 				ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 				    "%s: sadb slot %d entry for lid 0x%x, spi 0x%x found, sa_index %d\n",
+ 				    __func__, slot, entry->handle, sa_frame->spi,
+ 				    entry->sa_pair[slot].sa_index);
+ 				return entry->sa_pair[slot].sa_index;
+ 			}
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	/* both slots are used */
+ 	if (free_slot == -1) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: WARNING: No free slots in sadb for nport_handle 0x%x, spi: 0x%x\n",
+ 		    __func__, entry->handle, sa_frame->spi);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Slot 0  spi: 0x%x  sa_index: %d,  Slot 1  spi: 0x%x  sa_index: %d\n",
+ 		    __func__, entry->sa_pair[0].spi, entry->sa_pair[0].sa_index,
+ 		    entry->sa_pair[1].spi, entry->sa_pair[1].sa_index);
+ 
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	/* there is at least one free slot, use it */
+ 	sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 	if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 		ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 		    "%s: empty freepool!!\n", __func__);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	entry->sa_pair[free_slot].spi = sa_frame->spi;
+ 	entry->sa_pair[free_slot].sa_index = sa_index;
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 	ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 	    "%s: sadb slot %d entry for nport_handle 0x%x, spi 0x%x added, returning sa_index %d\n",
+ 	    __func__, free_slot, entry->handle, sa_frame->spi, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* release any sadb entries -- only done at teardown */
+ void qla_edif_sadb_release(struct qla_hw_data *ha)
+ {
+ 	struct list_head *pos;
+ 	struct list_head *tmp;
+ 	struct edif_sa_index_entry *entry;
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_rx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_tx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ }
+ 
+ /**************************
+  * sadb freepool functions
+  **************************/
+ 
+ /* build the rx and tx sa_index free pools -- only done at fcport init */
+ int qla_edif_sadb_build_free_pool(struct qla_hw_data *ha)
+ {
+ 	ha->edif_tx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 
+ 	if (!ha->edif_tx_sa_id_map) {
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb tx.\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ha->edif_rx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 	if (!ha->edif_rx_sa_id_map) {
+ 		kfree(ha->edif_tx_sa_id_map);
+ 		ha->edif_tx_sa_id_map = NULL;
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb rx.\n");
+ 		return -ENOMEM;
+ 	}
+ 	return 0;
+ }
+ 
+ /* release the free pool - only done during fcport teardown */
+ void qla_edif_sadb_release_free_pool(struct qla_hw_data *ha)
+ {
+ 	kfree(ha->edif_tx_sa_id_map);
+ 	ha->edif_tx_sa_id_map = NULL;
+ 	kfree(ha->edif_rx_sa_id_map);
+ 	ha->edif_rx_sa_id_map = NULL;
+ }
+ 
+ static void __chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		fc_port_t *fcport, uint32_t handle, uint16_t sa_index)
+ {
+ 	struct edif_list_entry *edif_entry;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	uint16_t delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 	uint16_t cached_nport_handle;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	edif_entry = qla_edif_list_find_sa_index(fcport, nport_handle);
+ 	if (!edif_entry) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;		/* no pending delete for this handle */
+ 	}
+ 
+ 	/*
+ 	 * check for no pending delete for this index or iocb does not
+ 	 * match rx sa_index
+ 	 */
+ 	if (edif_entry->delete_sa_index == INVALID_EDIF_SA_INDEX ||
+ 	    edif_entry->update_sa_index != sa_index) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * wait until we have seen at least EDIF_DELAY_COUNT transfers before
+ 	 * queueing RX delete
+ 	 */
+ 	if (edif_entry->count++ < EDIF_RX_DELETE_FILTER_COUNT) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 	    "%s: invalidating delete_sa_index,  update_sa_index: 0x%x sa_index: 0x%x, delete_sa_index: 0x%x\n",
+ 	    __func__, edif_entry->update_sa_index, sa_index, edif_entry->delete_sa_index);
+ 
+ 	delete_sa_index = edif_entry->delete_sa_index;
+ 	edif_entry->delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	cached_nport_handle = edif_entry->handle;
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 
+ 	/* sanity check on the nport handle */
+ 	if (nport_handle != cached_nport_handle) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE nport_handle mismatch: lid: 0x%x, edif_entry nph: 0x%x\n",
+ 		    __func__, nport_handle, cached_nport_handle);
+ 	}
+ 
+ 	/* find the sa_ctl for the delete and schedule the delete */
+ 	sa_ctl = qla_edif_find_sa_ctl_by_index(fcport, delete_sa_index, 0);
+ 	if (sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl: %p, index recvd %d\n",
+ 		    __func__, sa_ctl, sa_index);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "delete index %d, update index: %d, nport handle: 0x%x, handle: 0x%x\n",
+ 		    delete_sa_index,
+ 		    edif_entry->update_sa_index, nport_handle, handle);
+ 
+ 		sa_ctl->flags = EDIF_SA_CTL_FLG_DEL;
+ 		set_bit(EDIF_SA_CTL_REPL, &sa_ctl->state);
+ 		qla_post_sa_replace_work(fcport->vha, fcport,
+ 		    nport_handle, sa_ctl);
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl not found for delete_sa_index: %d\n",
+ 		    __func__, delete_sa_index);
+ 	}
+ }
+ 
+ void qla_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		srb_t *sp, struct sts_entry_24xx *sts24)
+ {
+ 	fc_port_t *fcport = sp->fcport;
+ 	/* sa_index used by this iocb */
+ 	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
+ 	uint32_t handle;
+ 
+ 	handle = (uint32_t)LSW(sts24->handle);
+ 
+ 	/* find out if this status iosb is for a scsi read */
+ 	if (cmd->sc_data_direction != DMA_FROM_DEVICE)
+ 		return;
+ 
+ 	return __chk_edif_rx_sa_delete_pending(vha, fcport, handle,
+ 	   le16_to_cpu(sts24->edif_sa_index));
+ }
+ 
+ void qlt_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha, fc_port_t *fcport,
+ 		struct ctio7_from_24xx *pkt)
+ {
+ 	__chk_edif_rx_sa_delete_pending(vha, fcport,
+ 	    pkt->handle, le16_to_cpu(pkt->edif_sa_index));
+ }
+ 
+ static void qla_parse_auth_els_ctl(struct srb *sp)
+ {
+ 	struct qla_els_pt_arg *a = &sp->u.bsg_cmd.u.els_arg;
+ 	struct bsg_job *bsg_job = sp->u.bsg_cmd.bsg_job;
+ 	struct fc_bsg_request *request = bsg_job->request;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	a->tx_len = a->tx_byte_count = sp->remap.req.len;
+ 	a->tx_addr = sp->remap.req.dma;
+ 	a->rx_len = a->rx_byte_count = sp->remap.rsp.len;
+ 	a->rx_addr = sp->remap.rsp.dma;
+ 
+ 	if (p->e.sub_cmd == SEND_ELS_REPLY) {
+ 		a->control_flags = p->e.extra_control_flags << 13;
+ 		a->rx_xchg_address = cpu_to_le32(p->e.extra_rx_xchg_address);
+ 		if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_ACC)
+ 			a->els_opcode = ELS_LS_ACC;
+ 		else if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_RJT)
+ 			a->els_opcode = ELS_LS_RJT;
+ 	}
+ 	a->did = sp->fcport->d_id;
+ 	a->els_opcode =  request->rqst_data.h_els.command_code;
+ 	a->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	a->vp_idx = sp->vha->vp_idx;
+ }
+ 
+ int qla_edif_process_els(scsi_qla_host_t *vha, struct bsg_job *bsg_job)
+ {
+ 	struct fc_bsg_request *bsg_request = bsg_job->request;
+ 	struct fc_bsg_reply *bsg_reply = bsg_job->reply;
+ 	fc_port_t *fcport = NULL;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	srb_t *sp;
+ 	int rval =  (DID_ERROR << 16);
+ 	port_id_t d_id;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	d_id.b.al_pa = bsg_request->rqst_data.h_els.port_id[2];
+ 	d_id.b.area = bsg_request->rqst_data.h_els.port_id[1];
+ 	d_id.b.domain = bsg_request->rqst_data.h_els.port_id[0];
+ 
+ 	/* find matching d_id in fcport list */
+ 	fcport = qla2x00_find_fcport_by_pid(vha, &d_id);
+ 	if (!fcport) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x911a,
+ 		    "%s fcport not find online portid=%06x.\n",
+ 		    __func__, d_id.b24);
+ 		SET_DID_STATUS(bsg_reply->result, DID_ERROR);
+ 		return -EIO;
+ 	}
+ 
+ 	if (qla_bsg_check(vha, bsg_job, fcport))
+ 		return 0;
+ 
+ 	if (fcport->loop_id == FC_NO_LOOP_ID) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910d,
+ 		    "%s ELS code %x, no loop id.\n", __func__,
+ 		    bsg_request->rqst_data.r_els.els_code);
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		return -ENXIO;
+ 	}
+ 
+ 	if (!vha->flags.online) {
+ 		ql_log(ql_log_warn, vha, 0x7005, "Host not online.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EIO;
+ 		goto done;
+ 	}
+ 
+ 	/* pass through is supported only for ISP 4Gb or higher */
+ 	if (!IS_FWI2_CAPABLE(ha)) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7001,
+ 		    "ELS passthru not supported for ISP23xx based adapters.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EPERM;
+ 		goto done;
+ 	}
+ 
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7004,
+ 		    "Failed get sp pid=%06x\n", fcport->d_id.b24);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done;
+ 	}
+ 
+ 	sp->remap.req.len = bsg_job->request_payload.payload_len;
+ 	sp->remap.req.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.req.dma);
+ 	if (!sp->remap.req.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7005,
+ 		    "Failed allocate request dma len=%x\n",
+ 		    bsg_job->request_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->remap.rsp.len = bsg_job->reply_payload.payload_len;
+ 	sp->remap.rsp.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.rsp.dma);
+ 	if (!sp->remap.rsp.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7006,
+ 		    "Failed allocate response dma len=%x\n",
+ 		    bsg_job->reply_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_remap_req;
+ 	}
+ 	sg_copy_to_buffer(bsg_job->request_payload.sg_list,
+ 	    bsg_job->request_payload.sg_cnt, sp->remap.req.buf,
+ 	    sp->remap.req.len);
+ 	sp->remap.remapped = true;
+ 
+ 	sp->type = SRB_ELS_CMD_HST_NOLOGIN;
+ 	sp->name = "SPCN_BSG_HST_NOLOGIN";
+ 	sp->u.bsg_cmd.bsg_job = bsg_job;
+ 	qla_parse_auth_els_ctl(sp);
+ 
+ 	sp->free = qla2x00_bsg_sp_free;
+ 	sp->done = qla2x00_bsg_job_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x700a,
+ 	    "%s %s %8phN xchg %x ctlflag %x hdl %x reqlen %xh bsg ptr %p\n",
+ 	    __func__, sc_to_str(p->e.sub_cmd), fcport->port_name,
+ 	    p->e.extra_rx_xchg_address, p->e.extra_control_flags,
+ 	    sp->handle, sp->remap.req.len, bsg_job);
+ 
+ 	if (rval != QLA_SUCCESS) {
+ 		ql_log(ql_log_warn, vha, 0x700e,
+ 		    "qla2x00_start_sp failed = %d\n", rval);
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		rval = -EIO;
+ 		goto done_free_remap_rsp;
+ 	}
+ 	return rval;
+ 
+ done_free_remap_rsp:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.rsp.buf,
+ 	    sp->remap.rsp.dma);
+ done_free_remap_req:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.req.buf,
+ 	    sp->remap.req.dma);
+ done_free_sp:
+ 	qla2x00_rel_sp(sp);
+ 
+ done:
+ 	return rval;
+ }
+ 
+ void qla_edif_sess_down(struct scsi_qla_host *vha, struct fc_port *sess)
+ {
+ 	if (sess->edif.app_sess_online && vha->e_dbell.db_flags & EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xf09c,
+ 			"%s: sess %8phN send port_offline event\n",
+ 			__func__, sess->port_name);
+ 		sess->edif.app_sess_online = 0;
+ 		qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SESSION_SHUTDOWN,
+ 		    sess->d_id.b24, 0, sess);
+ 		qla2x00_post_aen_work(vha, FCH_EVT_PORT_OFFLINE, sess->d_id.b24);
+ 	}
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  }
diff --cc drivers/scsi/qla2xxx/qla_gbl.h
index 50ae84979282,2b8bdb146a8f..000000000000
--- a/drivers/scsi/qla2xxx/qla_gbl.h
+++ b/drivers/scsi/qla2xxx/qla_gbl.h
@@@ -282,7 -296,10 +283,14 @@@ extern int  qla2x00_vp_abort_isp(scsi_q
  /*
   * Global Function Prototypes in qla_iocb.c source file.
   */
- 
++<<<<<<< HEAD
++
++=======
+ void qla_els_pt_iocb(struct scsi_qla_host *vha,
+ 	struct els_entry_24xx *pkt, struct qla_els_pt_arg *a);
+ cont_a64_entry_t *qla2x00_prep_cont_type1_iocb(scsi_qla_host_t *vha,
+ 		struct req_que *que);
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  extern uint16_t qla2x00_calc_iocbs_32(uint16_t);
  extern uint16_t qla2x00_calc_iocbs_64(uint16_t);
  extern void qla2x00_build_scsi_iocbs_32(srb_t *, cmd_entry_t *, uint16_t);
@@@ -954,6 -987,15 +962,17 @@@ void qla_edb_stop(scsi_qla_host_t *vha)
  int32_t qla_edif_app_mgmt(struct bsg_job *bsg_job);
  void qla_enode_init(scsi_qla_host_t *vha);
  void qla_enode_stop(scsi_qla_host_t *vha);
++<<<<<<< HEAD
++=======
+ void qla_edif_flush_sa_ctl_lists(fc_port_t *fcport);
+ void qla_edb_init(scsi_qla_host_t *vha);
+ int qla28xx_start_scsi_edif(srb_t *sp);
+ void qla24xx_sa_update_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb);
+ void qla24xx_sa_replace_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb);
+ void qla24xx_auth_els(scsi_qla_host_t *vha, void **pkt, struct rsp_que **rsp);
+ void qla28xx_sa_update_iocb_entry(scsi_qla_host_t *vha, struct req_que *req,
+ 		struct sa_update_28xx *pkt);
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  void qla_handle_els_plogi_done(scsi_qla_host_t *vha, struct event_arg *ea);
  
  #define QLA2XX_HW_ERROR			BIT_0
diff --cc drivers/scsi/qla2xxx/qla_init.c
index 2a2edf816619,ad0d3f536a31..000000000000
--- a/drivers/scsi/qla2xxx/qla_init.c
+++ b/drivers/scsi/qla2xxx/qla_init.c
@@@ -343,19 -342,28 +343,38 @@@ qla2x00_async_login(struct scsi_qla_hos
  	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
  
  	sp->done = qla2x00_async_login_sp_done;
 -	if (N2N_TOPO(fcport->vha->hw) && fcport_is_bigger(fcport)) {
 +	if (N2N_TOPO(fcport->vha->hw) && fcport_is_bigger(fcport))
  		lio->u.logio.flags |= SRB_LOGIN_PRLI_ONLY;
++<<<<<<< HEAD
 +	else
 +		lio->u.logio.flags |= SRB_LOGIN_COND_PLOGI;
++=======
+ 	} else {
+ 		if (vha->hw->flags.edif_enabled &&
+ 		    vha->e_dbell.db_flags & EDB_ACTIVE) {
+ 			lio->u.logio.flags |=
+ 				(SRB_LOGIN_FCSP | SRB_LOGIN_SKIP_PRLI);
+ 			ql_dbg(ql_dbg_disc, vha, 0x2072,
+ 			    "Async-login: w/ FCSP %8phC hdl=%x, loopid=%x portid=%06x\n",
+ 			    fcport->port_name, sp->handle, fcport->loop_id, fcport->d_id.b24);
+ 		} else {
+ 			lio->u.logio.flags |= SRB_LOGIN_COND_PLOGI;
+ 		}
+ 	}
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  
  	if (NVME_TARGET(vha->hw, fcport))
  		lio->u.logio.flags |= SRB_LOGIN_SKIP_PRLI;
  
++<<<<<<< HEAD
 +	ql_dbg(ql_dbg_disc, vha, 0x2072,
 +	       "Async-login - %8phC hdl=%x, loopid=%x portid=%02x%02x%02x retries=%d.\n",
++=======
+ 	ql_log(ql_log_warn, vha, 0x2072,
+ 	       "Async-login - %8phC hdl=%x, loopid=%x portid=%06x retries=%d.\n",
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  	       fcport->port_name, sp->handle, fcport->loop_id,
- 	       fcport->d_id.b.domain, fcport->d_id.b.area, fcport->d_id.b.al_pa,
- 	       fcport->login_retry);
+ 	       fcport->d_id.b24, fcport->login_retry);
  
  	rval = qla2x00_start_sp(sp);
  	if (rval != QLA_SUCCESS) {
diff --cc drivers/scsi/qla2xxx/qla_target.c
index cc3934c87715,c3a589659658..000000000000
--- a/drivers/scsi/qla2xxx/qla_target.c
+++ b/drivers/scsi/qla2xxx/qla_target.c
@@@ -4744,6 -4797,17 +4773,20 @@@ static int qlt_handle_login(struct scsi
  		goto out;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (vha->hw->flags.edif_enabled &&
+ 	    !(vha->e_dbell.db_flags & EDB_ACTIVE) &&
+ 	    iocb->u.isp24.status_subcode == ELS_PLOGI &&
+ 	    !(le16_to_cpu(iocb->u.isp24.flags) & NOTIFY24XX_FLAGS_FCSP)) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			"%s %d Term INOT due to app not available lid=%d, NportID %06X ",
+ 			__func__, __LINE__, loop_id, port_id.b24);
+ 		qlt_send_term_imm_notif(vha, iocb, 1);
+ 		goto out;
+ 	}
+ 
++>>>>>>> 44d018577f17 (scsi: qla2xxx: edif: Add encryption to I/O path)
  	pla = qlt_plogi_ack_find_add(vha, &port_id, iocb);
  	if (!pla) {
  		ql_dbg(ql_dbg_disc + ql_dbg_verbose, vha, 0xffff,
* Unmerged path drivers/scsi/qla2xxx/qla_edif.c
diff --git a/drivers/scsi/qla2xxx/qla_fw.h b/drivers/scsi/qla2xxx/qla_fw.h
index 44db5e1fc35e..8d2b2702be67 100644
--- a/drivers/scsi/qla2xxx/qla_fw.h
+++ b/drivers/scsi/qla2xxx/qla_fw.h
@@ -490,6 +490,9 @@ struct cmd_type_6 {
 	struct scsi_lun lun;		/* FCP LUN (BE). */
 
 	__le16	control_flags;		/* Control flags. */
+#define CF_NEW_SA			BIT_12
+#define CF_EN_EDIF			BIT_9
+#define CF_ADDITIONAL_PARAM_BLK		BIT_8
 #define CF_DIF_SEG_DESCR_ENABLE		BIT_3
 #define CF_DATA_SEG_DESCR_ENABLE	BIT_2
 #define CF_READ_DATA			BIT_1
* Unmerged path drivers/scsi/qla2xxx/qla_gbl.h
diff --git a/drivers/scsi/qla2xxx/qla_gs.c b/drivers/scsi/qla2xxx/qla_gs.c
index 35ef268ad9b4..e4cb9f185255 100644
--- a/drivers/scsi/qla2xxx/qla_gs.c
+++ b/drivers/scsi/qla2xxx/qla_gs.c
@@ -633,7 +633,7 @@ static int qla_async_rftid(scsi_qla_host_t *vha, port_id_t *d_id)
 	ct_req->req.rft_id.port_id = port_id_to_be_id(vha->d_id);
 	ct_req->req.rft_id.fc4_types[2] = 0x01;		/* FCP-3 */
 
-	if (vha->flags.nvme_enabled)
+	if (vha->flags.nvme_enabled && qla_ini_mode_enabled(vha))
 		ct_req->req.rft_id.fc4_types[6] = 1;    /* NVMe type 28h */
 
 	sp->u.iocb_cmd.u.ctarg.req_size = RFT_ID_REQ_SIZE;
* Unmerged path drivers/scsi/qla2xxx/qla_init.c
diff --git a/drivers/scsi/qla2xxx/qla_iocb.c b/drivers/scsi/qla2xxx/qla_iocb.c
index 7c55022f5a26..1df382bec86a 100644
--- a/drivers/scsi/qla2xxx/qla_iocb.c
+++ b/drivers/scsi/qla2xxx/qla_iocb.c
@@ -119,7 +119,7 @@ qla2x00_prep_cont_type0_iocb(struct scsi_qla_host *vha)
  *
  * Returns a pointer to the continuation type 1 IOCB packet.
  */
-static inline cont_a64_entry_t *
+cont_a64_entry_t *
 qla2x00_prep_cont_type1_iocb(scsi_qla_host_t *vha, struct req_que *req)
 {
 	cont_a64_entry_t *cont_pkt;
@@ -1962,6 +1962,9 @@ qla2xxx_start_scsi_mq(srb_t *sp)
 	struct qla_hw_data *ha = vha->hw;
 	struct qla_qpair *qpair = sp->qpair;
 
+	if (sp->fcport->edif.enable)
+		return qla28xx_start_scsi_edif(sp);
+
 	/* Acquire qpair specific lock */
 	spin_lock_irqsave(&qpair->qp_lock, flags);
 
diff --git a/drivers/scsi/qla2xxx/qla_mbx.c b/drivers/scsi/qla2xxx/qla_mbx.c
index 611f9530ce87..511bd1f7594d 100644
--- a/drivers/scsi/qla2xxx/qla_mbx.c
+++ b/drivers/scsi/qla2xxx/qla_mbx.c
@@ -740,7 +740,7 @@ qla2x00_execute_fw(scsi_qla_host_t *vha, uint32_t risc_addr)
 			mcp->mb[11] |= EXE_FW_FORCE_SEMAPHORE;
 
 		mcp->out_mb |= MBX_4 | MBX_3 | MBX_2 | MBX_1 | MBX_11;
-		mcp->in_mb |= MBX_3 | MBX_2 | MBX_1;
+		mcp->in_mb |= MBX_5 | MBX_3 | MBX_2 | MBX_1;
 	} else {
 		mcp->mb[1] = LSW(risc_addr);
 		mcp->out_mb |= MBX_1;
@@ -796,6 +796,12 @@ qla2x00_execute_fw(scsi_qla_host_t *vha, uint32_t risc_addr)
 		}
 	}
 
+	if (IS_QLA28XX(ha) && (mcp->mb[5] & BIT_10) && ql2xsecenable) {
+		ha->flags.edif_enabled = 1;
+		ql_log(ql_log_info, vha, 0xffff,
+		    "%s: edif is enabled\n", __func__);
+	}
+
 done:
 	ql_dbg(ql_dbg_mbx + ql_dbg_verbose, vha, 0x1028,
 	    "Done %s.\n", __func__);
@@ -4947,7 +4953,7 @@ qla24xx_get_port_login_templ(scsi_qla_host_t *vha, dma_addr_t buf_dma,
 	return rval;
 }
 
-#define PUREX_CMD_COUNT	2
+#define PUREX_CMD_COUNT	4
 int
 qla25xx_set_els_cmds_supported(scsi_qla_host_t *vha)
 {
@@ -4955,6 +4961,7 @@ qla25xx_set_els_cmds_supported(scsi_qla_host_t *vha)
 	mbx_cmd_t mc;
 	mbx_cmd_t *mcp = &mc;
 	uint8_t *els_cmd_map;
+	uint8_t active_cnt = 0;
 	dma_addr_t els_cmd_map_dma;
 	uint8_t cmd_opcode[PUREX_CMD_COUNT];
 	uint8_t i, index, purex_bit;
@@ -4976,10 +4983,20 @@ qla25xx_set_els_cmds_supported(scsi_qla_host_t *vha)
 	}
 
 	/* List of Purex ELS */
-	cmd_opcode[0] = ELS_FPIN;
-	cmd_opcode[1] = ELS_RDP;
+	if (ql2xrdpenable) {
+		cmd_opcode[active_cnt] = ELS_RDP;
+		active_cnt++;
+	}
+	if (ha->flags.scm_supported_f) {
+		cmd_opcode[active_cnt] = ELS_FPIN;
+		active_cnt++;
+	}
+	if (ha->flags.edif_enabled) {
+		cmd_opcode[active_cnt] = ELS_AUTH_ELS;
+		active_cnt++;
+	}
 
-	for (i = 0; i < PUREX_CMD_COUNT; i++) {
+	for (i = 0; i < active_cnt; i++) {
 		index = cmd_opcode[i] / 8;
 		purex_bit = cmd_opcode[i] % 8;
 		els_cmd_map[index] |= 1 << purex_bit;
diff --git a/drivers/scsi/qla2xxx/qla_nvme.c b/drivers/scsi/qla2xxx/qla_nvme.c
index d57a41901566..77c7b67a7f9c 100644
--- a/drivers/scsi/qla2xxx/qla_nvme.c
+++ b/drivers/scsi/qla2xxx/qla_nvme.c
@@ -464,6 +464,10 @@ static inline int qla2x00_start_nvme_mq(srb_t *sp)
 	} else if (fd->io_dir == 0) {
 		cmd_pkt->control_flags = 0;
 	}
+
+	if (sp->fcport->edif.enable && fd->io_dir != 0)
+		cmd_pkt->control_flags |= cpu_to_le16(CF_EN_EDIF);
+
 	/* Set BIT_13 of control flags for Async event */
 	if (vha->flags.nvme2_enabled &&
 	    cmd->sqe.common.opcode == nvme_admin_async_event) {
diff --git a/drivers/scsi/qla2xxx/qla_os.c b/drivers/scsi/qla2xxx/qla_os.c
index fbb549c49362..cad244b85fc6 100644
--- a/drivers/scsi/qla2xxx/qla_os.c
+++ b/drivers/scsi/qla2xxx/qla_os.c
@@ -54,6 +54,11 @@ static struct kmem_cache *ctx_cachep;
  */
 uint ql_errlev = 0x8001;
 
+int ql2xsecenable;
+module_param(ql2xsecenable, int, S_IRUGO);
+MODULE_PARM_DESC(ql2xsecenable,
+	"Enable/disable security. 0(Default) - Security disabled. 1 - Security enabled.");
+
 static int ql2xenableclass2;
 module_param(ql2xenableclass2, int, S_IRUGO|S_IRUSR);
 MODULE_PARM_DESC(ql2xenableclass2,
@@ -3884,7 +3889,7 @@ qla2x00_mem_alloc(struct qla_hw_data *ha, uint16_t req_len, uint16_t rsp_len,
 	if (!ha->srb_mempool)
 		goto fail_free_gid_list;
 
-	if (IS_P3P_TYPE(ha)) {
+	if (IS_P3P_TYPE(ha) || IS_QLA27XX(ha) || (ql2xsecenable && IS_QLA28XX(ha))) {
 		/* Allocate cache for CT6 Ctx. */
 		if (!ctx_cachep) {
 			ctx_cachep = kmem_cache_create("qla2xxx_ctx",
@@ -3918,7 +3923,7 @@ qla2x00_mem_alloc(struct qla_hw_data *ha, uint16_t req_len, uint16_t rsp_len,
 	    "init_cb=%p gid_list=%p, srb_mempool=%p s_dma_pool=%p.\n",
 	    ha->init_cb, ha->gid_list, ha->srb_mempool, ha->s_dma_pool);
 
-	if (IS_P3P_TYPE(ha) || ql2xenabledif) {
+	if (IS_P3P_TYPE(ha) || ql2xenabledif || (IS_QLA28XX(ha) && ql2xsecenable)) {
 		ha->dl_dma_pool = dma_pool_create(name, &ha->pdev->dev,
 			DSD_LIST_DMA_POOL_SIZE, 8, 0);
 		if (!ha->dl_dma_pool) {
* Unmerged path drivers/scsi/qla2xxx/qla_target.c
diff --git a/drivers/scsi/qla2xxx/qla_target.h b/drivers/scsi/qla2xxx/qla_target.h
index 6b5ee6c3a43f..7e1af4ed6d6a 100644
--- a/drivers/scsi/qla2xxx/qla_target.h
+++ b/drivers/scsi/qla2xxx/qla_target.h
@@ -247,6 +247,10 @@ struct ctio_to_2xxx {
 #define CTIO_PORT_LOGGED_OUT		0x29
 #define CTIO_PORT_CONF_CHANGED		0x2A
 #define CTIO_SRR_RECEIVED		0x45
+#define CTIO_FAST_AUTH_ERR		0x63
+#define CTIO_FAST_INCOMP_PAD_LEN	0x65
+#define CTIO_FAST_INVALID_REQ		0x66
+#define CTIO_FAST_SPI_ERR		0x67
 #endif
 
 #ifndef CTIO_RET_TYPE
@@ -417,7 +421,16 @@ struct ctio7_to_24xx {
 		struct {
 			__le16	reserved1;
 			__le16 flags;
-			__le32	residual;
+			union {
+				__le32	residual;
+				struct {
+					uint8_t rsvd1;
+					uint8_t edif_flags;
+#define EF_EN_EDIF	BIT_0
+#define EF_NEW_SA	BIT_1
+					uint16_t rsvd2;
+				};
+			};
 			__le16 ox_id;
 			__le16	scsi_status;
 			__le32	relative_offset;
@@ -882,6 +895,7 @@ struct qla_tgt_cmd {
 	unsigned int term_exchg:1;
 	unsigned int cmd_sent_to_fw:1;
 	unsigned int cmd_in_wq:1;
+	unsigned int edif:1;
 
 	/*
 	 * This variable may be set from outside the LIO and I/O completion
