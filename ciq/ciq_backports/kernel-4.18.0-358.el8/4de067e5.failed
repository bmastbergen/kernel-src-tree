scsi: qla2xxx: edif: Add N2N support for EDIF

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Quinn Tran <qutran@marvell.com>
commit 4de067e5df12c4db4d3d930ba58354d23674f67c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/4de067e5.failed

For EDIF + N2N to work, firmware 9.8 or later is required. The driver will
pause after PLOGI to allow app to authenticate. Once authentication
completes, app will tell driver to do PRLI.

Link: https://lore.kernel.org/r/20210817051315.2477-6-njavali@marvell.com
	Signed-off-by: Quinn Tran <qutran@marvell.com>
	Signed-off-by: Nilesh Javali <njavali@marvell.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 4de067e5df12c4db4d3d930ba58354d23674f67c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_def.h
#	drivers/scsi/qla2xxx/qla_edif.c
#	drivers/scsi/qla2xxx/qla_edif.h
#	drivers/scsi/qla2xxx/qla_gbl.h
#	drivers/scsi/qla2xxx/qla_init.c
#	drivers/scsi/qla2xxx/qla_mbx.c
#	drivers/scsi/qla2xxx/qla_os.c
#	drivers/scsi/qla2xxx/qla_target.c
diff --cc drivers/scsi/qla2xxx/qla_def.h
index 8b39720b2812,031107b6024f..000000000000
--- a/drivers/scsi/qla2xxx/qla_def.h
+++ b/drivers/scsi/qla2xxx/qla_def.h
@@@ -2578,7 -2633,14 +2578,8 @@@ typedef struct fc_port 
  		uint64_t	rx_bytes;
  		uint8_t		non_secured_login;
  		uint8_t		auth_state;
+ 		uint16_t	authok:1;
  		uint16_t	rekey_cnt;
 -		struct list_head edif_indx_list;
 -		spinlock_t  indx_list_lock;
 -
 -		struct list_head tx_sa_list;
 -		struct list_head rx_sa_list;
 -		spinlock_t	sa_list_lock;
  	} edif;
  } fc_port_t;
  
@@@ -3946,7 -4022,9 +3947,8 @@@ struct qla_hw_data 
  		uint32_t	scm_supported_f:1;
  				/* Enabled in Driver */
  		uint32_t	scm_enabled:1;
 -		uint32_t	edif_hw:1;
  		uint32_t	edif_enabled:1;
+ 		uint32_t	n2n_fw_acc_sec:1;
  		uint32_t	plogi_template_valid:1;
  		uint32_t	port_isolated:1;
  	} flags;
@@@ -4629,8 -4708,25 +4631,26 @@@
  
  	struct qla_hw_data_stat stat;
  	pci_error_state_t pci_error_state;
++<<<<<<< HEAD
++=======
+ 	u64 prev_cmd_cnt;
+ 	struct dma_pool *purex_dma_pool;
+ 	struct btree_head32 host_map;
+ 
+ #define EDIF_NUM_SA_INDEX	512
+ #define EDIF_TX_SA_INDEX_BASE	EDIF_NUM_SA_INDEX
+ 	void *edif_rx_sa_id_map;
+ 	void *edif_tx_sa_id_map;
+ 	spinlock_t sadb_fp_lock;
+ 
+ 	struct list_head sadb_tx_index_list;
+ 	struct list_head sadb_rx_index_list;
+ 	spinlock_t sadb_lock;	/* protects list */
+ 	struct els_reject elsrej;
+ 	u8 edif_post_stop_cnt_down;
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  };
  
 -#define RX_ELS_SIZE (roundup(sizeof(struct enode) + ELS_MAX_PAYLOAD, SMP_CACHE_BYTES))
 -
  struct active_regions {
  	uint8_t global;
  	struct {
diff --cc drivers/scsi/qla2xxx/qla_edif.c
index 0f46f8100846,bb5cda85b60f..000000000000
--- a/drivers/scsi/qla2xxx/qla_edif.c
+++ b/drivers/scsi/qla2xxx/qla_edif.c
@@@ -139,17 -546,28 +139,42 @@@ qla_edif_app_start(scsi_qla_host_t *vha
  		     __func__);
  	}
  
++<<<<<<< HEAD
 +	list_for_each_entry_safe(fcport, tf, &vha->vp_fcports, list) {
 +		if ((fcport->flags & FCF_FCSP_DEVICE)) {
 +			ql_dbg(ql_dbg_edif, vha, 0xf084,
 +			    "%s: sess %p %8phC lid %#04x s_id %06x logout %d\n",
 +			    __func__, fcport, fcport->port_name,
 +			    fcport->loop_id, fcport->d_id.b24,
 +			    fcport->logout_on_delete);
 +
 +			if (atomic_read(&vha->loop_state) == LOOP_DOWN)
 +				break;
 +
++=======
+ 	if (N2N_TOPO(vha->hw)) {
+ 		if (vha->hw->flags.n2n_fw_acc_sec)
+ 			set_bit(N2N_LINK_RESET, &vha->dpc_flags);
+ 		else
+ 			set_bit(ISP_ABORT_NEEDED, &vha->dpc_flags);
+ 		qla2xxx_wake_dpc(vha);
+ 	} else {
+ 		list_for_each_entry_safe(fcport, tf, &vha->vp_fcports, list) {
+ 			ql_dbg(ql_dbg_edif, vha, 0xf084,
+ 			       "%s: sess %p %8phC lid %#04x s_id %06x logout %d\n",
+ 			       __func__, fcport, fcport->port_name,
+ 			       fcport->loop_id, fcport->d_id.b24,
+ 			       fcport->logout_on_delete);
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0xf084,
+ 			       "keep %d els_logo %d disc state %d auth state %d stop state %d\n",
+ 			       fcport->keep_nport_handle,
+ 			       fcport->send_els_logo, fcport->disc_state,
+ 			       fcport->edif.auth_state, fcport->edif.app_stop);
+ 
+ 			if (atomic_read(&vha->loop_state) == LOOP_DOWN)
+ 				break;
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  			if (!fcport->edif.secured_login)
  				continue;
  
@@@ -162,8 -580,8 +187,13 @@@
  				fcport->edif.app_stop = 0;
  
  				ql_dbg(ql_dbg_edif, vha, 0x911e,
++<<<<<<< HEAD
 +				    "%s wwpn %8phC calling qla_edif_reset_auth_wait\n",
 +				    __func__, fcport->port_name);
++=======
+ 				       "%s wwpn %8phC calling qla_edif_reset_auth_wait\n",
+ 				       __func__, fcport->port_name);
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  				fcport->edif.app_sess_online = 1;
  				qla_edif_reset_auth_wait(fcport, DSC_LOGIN_PEND, 0);
  			}
@@@ -742,4 -1865,1598 +774,1601 @@@ qla_edb_stop(scsi_qla_host_t *vha
  		    "%s doorbell not enabled\n", __func__);
  		return;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/* grab lock so list doesn't move */
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	vha->e_dbell.db_flags &= ~EDB_ACTIVE; /* mark it not active */
+ 	/* hopefully this is a null list at this point */
+ 	list_for_each_entry_safe(node, q, &vha->e_dbell.head, list) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910f,
+ 		    "%s freeing edb_node type=%x\n",
+ 		    __func__, node->ntype);
+ 		qla_edb_node_free(vha, node);
+ 		list_del(&node->list);
+ 
+ 		kfree(node);
+ 	}
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* wake up doorbell waiters - they'll be dismissed with error code */
+ 	complete_all(&vha->e_dbell.dbell);
+ }
+ 
+ static struct edb_node *
+ qla_edb_node_alloc(scsi_qla_host_t *vha, uint32_t ntype)
+ {
+ 	struct edb_node	*node;
+ 
+ 	node = kzalloc(sizeof(*node), GFP_ATOMIC);
+ 	if (!node) {
+ 		/* couldn't get space */
+ 		ql_dbg(ql_dbg_edif, vha, 0x9100,
+ 		    "edb node unable to be allocated\n");
+ 		return NULL;
+ 	}
+ 
+ 	node->ntype = ntype;
+ 	INIT_LIST_HEAD(&node->list);
+ 	return node;
+ }
+ 
+ /* adds a already allocated enode to the linked list */
+ static bool
+ qla_edb_node_add(scsi_qla_host_t *vha, struct edb_node *ptr)
+ {
+ 	unsigned long		flags;
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled\n", __func__);
+ 		return false;
+ 	}
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 	list_add_tail(&ptr->list, &vha->e_dbell.head);
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* ring doorbell for waiters */
+ 	complete(&vha->e_dbell.dbell);
+ 
+ 	return true;
+ }
+ 
+ /* adds event to doorbell list */
+ void
+ qla_edb_eventcreate(scsi_qla_host_t *vha, uint32_t dbtype,
+ 	uint32_t data, uint32_t data2, fc_port_t	*sfcport)
+ {
+ 	struct edb_node	*edbnode;
+ 	fc_port_t *fcport = sfcport;
+ 	port_id_t id;
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		return;
+ 	}
+ 
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		if (fcport)
+ 			fcport->edif.auth_state = dbtype;
+ 		/* doorbell list not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s doorbell not enabled (type=%d\n", __func__, dbtype);
+ 		return;
+ 	}
+ 
+ 	edbnode = qla_edb_node_alloc(vha, dbtype);
+ 	if (!edbnode) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to alloc db node\n", __func__);
+ 		return;
+ 	}
+ 
+ 	if (!fcport) {
+ 		id.b.domain = (data >> 16) & 0xff;
+ 		id.b.area = (data >> 8) & 0xff;
+ 		id.b.al_pa = data & 0xff;
+ 		ql_dbg(ql_dbg_edif, vha, 0x09222,
+ 		    "%s: Arrived s_id: %06x\n", __func__,
+ 		    id.b24);
+ 		fcport = qla2x00_find_fcport_by_pid(vha, &id);
+ 		if (!fcport) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			    "%s can't find fcport for sid= 0x%x - ignoring\n",
+ 			__func__, id.b24);
+ 			kfree(edbnode);
+ 			return;
+ 		}
+ 	}
+ 
+ 	/* populate the edb node */
+ 	switch (dbtype) {
+ 	case VND_CMD_AUTH_STATE_NEEDED:
+ 	case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 		edbnode->u.plogi_did.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 		edbnode->u.els_sid.b24 = fcport->d_id.b24;
+ 		break;
+ 	case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 		edbnode->u.sa_aen.port_id = fcport->d_id;
+ 		edbnode->u.sa_aen.status =  data;
+ 		edbnode->u.sa_aen.key_type =  data2;
+ 		break;
+ 	default:
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 			"%s unknown type: %x\n", __func__, dbtype);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		edbnode = NULL;
+ 		break;
+ 	}
+ 
+ 	if (edbnode && (!qla_edb_node_add(vha, edbnode))) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 		    "%s unable to add dbnode\n", __func__);
+ 		qla_edb_node_free(vha, edbnode);
+ 		kfree(edbnode);
+ 		return;
+ 	}
+ 	if (edbnode && fcport)
+ 		fcport->edif.auth_state = dbtype;
+ 	ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 	    "%s Doorbell produced : type=%d %p\n", __func__, dbtype, edbnode);
+ }
+ 
+ static struct edb_node *
+ qla_edb_getnext(scsi_qla_host_t *vha)
+ {
+ 	unsigned long	flags;
+ 	struct edb_node	*edbnode = NULL;
+ 
+ 	spin_lock_irqsave(&vha->e_dbell.db_lock, flags);
+ 
+ 	/* db nodes are fifo - no qualifications done */
+ 	if (!list_empty(&vha->e_dbell.head)) {
+ 		edbnode = list_first_entry(&vha->e_dbell.head,
+ 		    struct edb_node, list);
+ 		list_del(&edbnode->list);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&vha->e_dbell.db_lock, flags);
+ 
+ 	return edbnode;
+ }
+ 
+ void
+ qla_edif_timer(scsi_qla_host_t *vha)
+ {
+ 	struct qla_hw_data *ha = vha->hw;
+ 
+ 	if (!vha->vp_idx && N2N_TOPO(ha) && ha->flags.n2n_fw_acc_sec) {
+ 		if (vha->e_dbell.db_flags != EDB_ACTIVE &&
+ 		    ha->edif_post_stop_cnt_down) {
+ 			ha->edif_post_stop_cnt_down--;
+ 
+ 			/*
+ 			 * turn off auto 'Plogi Acc + secure=1' feature
+ 			 * Set Add FW option[3]
+ 			 * BIT_15, if.
+ 			 */
+ 			if (ha->edif_post_stop_cnt_down == 0) {
+ 				ql_dbg(ql_dbg_async, vha, 0x911d,
+ 				       "%s chip reset to turn off PLOGI ACC + secure\n",
+ 				       __func__);
+ 				set_bit(ISP_ABORT_NEEDED, &vha->dpc_flags);
+ 			}
+ 		} else {
+ 			ha->edif_post_stop_cnt_down = 60;
+ 		}
+ 	}
+ }
+ 
+ /*
+  * app uses separate thread to read this. It'll wait until the doorbell
+  * is rung by the driver or the max wait time has expired
+  */
+ ssize_t
+ edif_doorbell_show(struct device *dev, struct device_attribute *attr,
+ 		char *buf)
+ {
+ 	scsi_qla_host_t *vha = shost_priv(class_to_shost(dev));
+ 	struct edb_node	*dbnode = NULL;
+ 	struct edif_app_dbell *ap = (struct edif_app_dbell *)buf;
+ 	uint32_t dat_siz, buf_size, sz;
+ 
+ 	/* TODO: app currently hardcoded to 256. Will transition to bsg */
+ 	sz = 256;
+ 
+ 	/* stop new threads from waiting if we're not init'd */
+ 	if (vha->e_dbell.db_flags != EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif db not enabled\n", __func__);
+ 		return 0;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif not enabled */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x09122,
+ 		    "%s error - edif not enabled\n", __func__);
+ 		return -1;
+ 	}
+ 
+ 	buf_size = 0;
+ 	while ((sz - buf_size) >= sizeof(struct edb_node)) {
+ 		/* remove the next item from the doorbell list */
+ 		dat_siz = 0;
+ 		dbnode = qla_edb_getnext(vha);
+ 		if (dbnode) {
+ 			ap->event_code = dbnode->ntype;
+ 			switch (dbnode->ntype) {
+ 			case VND_CMD_AUTH_STATE_SESSION_SHUTDOWN:
+ 			case VND_CMD_AUTH_STATE_NEEDED:
+ 				ap->port_id = dbnode->u.plogi_did;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_ELS_RCVD:
+ 				ap->port_id = dbnode->u.els_sid;
+ 				dat_siz += sizeof(ap->port_id);
+ 				break;
+ 			case VND_CMD_AUTH_STATE_SAUPDATE_COMPL:
+ 				ap->port_id = dbnode->u.sa_aen.port_id;
+ 				memcpy(ap->event_data, &dbnode->u,
+ 						sizeof(struct edif_sa_update_aen));
+ 				dat_siz += sizeof(struct edif_sa_update_aen);
+ 				break;
+ 			default:
+ 				/* unknown node type, rtn unknown ntype */
+ 				ap->event_code = VND_CMD_AUTH_STATE_UNDEF;
+ 				memcpy(ap->event_data, &dbnode->ntype, 4);
+ 				dat_siz += 4;
+ 				break;
+ 			}
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x09102,
+ 				"%s Doorbell consumed : type=%d %p\n",
+ 				__func__, dbnode->ntype, dbnode);
+ 			/* we're done with the db node, so free it up */
+ 			qla_edb_node_free(vha, dbnode);
+ 			kfree(dbnode);
+ 		} else {
+ 			break;
+ 		}
+ 
+ 		ap->event_data_size = dat_siz;
+ 		/* 8bytes = ap->event_code + ap->event_data_size */
+ 		buf_size += dat_siz + 8;
+ 		ap = (struct edif_app_dbell *)(buf + buf_size);
+ 	}
+ 	return buf_size;
+ }
+ 
+ static void qla_noop_sp_done(srb_t *sp, int res)
+ {
+ 	sp->free(sp);
+ }
+ 
+ /*
+  * Called from work queue
+  * build and send the sa_update iocb to delete an rx sa_index
+  */
+ int
+ qla24xx_issue_sa_replace_iocb(scsi_qla_host_t *vha, struct qla_work_evt *e)
+ {
+ 	srb_t *sp;
+ 	fc_port_t	*fcport = NULL;
+ 	struct srb_iocb *iocb_cmd = NULL;
+ 	int rval = QLA_SUCCESS;
+ 	struct	edif_sa_ctl *sa_ctl = e->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = e->u.sa_update.nport_handle;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 	    "%s: starting,  sa_ctl: %p\n", __func__, sa_ctl);
+ 
+ 	if (!sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "sa_ctl allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport = sa_ctl->fcport;
+ 
+ 	/* Alloc SRB structure */
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		 "SRB allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	fcport->flags |= FCF_ASYNC_SENT;
+ 	iocb_cmd = &sp->u.iocb_cmd;
+ 	iocb_cmd->u.sa_update.sa_ctl = sa_ctl;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3073,
+ 	    "Enter: SA REPL portid=%06x, sa_ctl %p, index %x, nport_handle: 0x%x\n",
+ 	    fcport->d_id.b24, sa_ctl, sa_ctl->index, nport_handle);
+ 	/*
+ 	 * if this is a sadb cleanup delete, mark it so the isr can
+ 	 * take the correct action
+ 	 */
+ 	if (sa_ctl->flags & EDIF_SA_CTL_FLG_CLEANUP_DEL) {
+ 		/* mark this srb as a cleanup delete */
+ 		sp->flags |= SRB_EDIF_CLEANUP_DELETE;
+ 		ql_dbg(ql_dbg_edif, vha, 0x70e6,
+ 		    "%s: sp 0x%p flagged as cleanup delete\n", __func__, sp);
+ 	}
+ 
+ 	sp->type = SRB_SA_REPLACE;
+ 	sp->name = "SA_REPLACE";
+ 	sp->fcport = fcport;
+ 	sp->free = qla2x00_rel_sp;
+ 	sp->done = qla_noop_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	if (rval != QLA_SUCCESS)
+ 		rval = QLA_FUNCTION_FAILED;
+ 
+ 	return rval;
+ }
+ 
+ void qla24xx_sa_update_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	int	itr = 0;
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct	qla_sa_update_frame	*sa_frame =
+ 		&sp->u.iocb_cmd.u.sa_update.sa_frame;
+ 	u8 flags = 0;
+ 
+ 	switch (sa_frame->flags & (SAU_FLG_INV | SAU_FLG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_INVALIDATE;
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX;
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x911d,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, sa_frame->fast_sa_index);
+ 		flags |= SA_FLAG_TX | SA_FLAG_INVALIDATE;
+ 		break;
+ 	}
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	sa_update_iocb->flags = flags;
+ 	sa_update_iocb->salt = cpu_to_le32(sa_frame->salt);
+ 	sa_update_iocb->spi = cpu_to_le32(sa_frame->spi);
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_frame->fast_sa_index);
+ 
+ 	sa_update_iocb->sa_control |= SA_CNTL_ENC_FCSP;
+ 	if (sp->fcport->edif.aes_gmac)
+ 		sa_update_iocb->sa_control |= SA_CNTL_AES_GMAC;
+ 
+ 	if (sa_frame->flags & SAU_FLG_KEY256) {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY256;
+ 		for (itr = 0; itr < 32; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	} else {
+ 		sa_update_iocb->sa_control |= SA_CNTL_KEY128;
+ 		for (itr = 0; itr < 16; itr++)
+ 			sa_update_iocb->sa_key[itr] = sa_frame->sa_key[itr];
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU Port ID = %02x%02x%02x, flags=%xh, index=%u, ctl=%xh, SPI 0x%x flags 0x%x hdl=%x gmac %d\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], sa_update_iocb->flags, sa_update_iocb->sa_index,
+ 	    sa_update_iocb->sa_control, sa_update_iocb->spi, sa_frame->flags, sp->handle,
+ 	    sp->fcport->edif.aes_gmac);
+ 
+ 	if (sa_frame->flags & SAU_FLG_TX)
+ 		sp->fcport->edif.tx_sa_pending = 1;
+ 	else
+ 		sp->fcport->edif.rx_sa_pending = 1;
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void
+ qla24xx_sa_replace_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb)
+ {
+ 	struct	scsi_qla_host		*vha = sp->vha;
+ 	struct srb_iocb *srb_iocb = &sp->u.iocb_cmd;
+ 	struct	edif_sa_ctl		*sa_ctl = srb_iocb->u.sa_update.sa_ctl;
+ 	uint16_t nport_handle = sp->fcport->loop_id;
+ 
+ 	sa_update_iocb->entry_type = SA_UPDATE_IOCB_TYPE;
+ 	sa_update_iocb->entry_count = 1;
+ 	sa_update_iocb->sys_define = 0;
+ 	sa_update_iocb->entry_status = 0;
+ 	sa_update_iocb->handle = sp->handle;
+ 
+ 	sa_update_iocb->u.nport_handle = cpu_to_le16(nport_handle);
+ 
+ 	sa_update_iocb->vp_index = sp->fcport->vha->vp_idx;
+ 	sa_update_iocb->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	sa_update_iocb->port_id[1] = sp->fcport->d_id.b.area;
+ 	sa_update_iocb->port_id[2] = sp->fcport->d_id.b.domain;
+ 
+ 	/* Invalidate the index. salt, spi, control & key are ignore */
+ 	sa_update_iocb->flags = SA_FLAG_INVALIDATE;
+ 	sa_update_iocb->salt = 0;
+ 	sa_update_iocb->spi = 0;
+ 	sa_update_iocb->sa_index = cpu_to_le16(sa_ctl->index);
+ 	sa_update_iocb->sa_control = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x921d,
+ 	    "%s SAU DELETE RX Port ID = %02x:%02x:%02x, lid %d flags=%xh, index=%u, hdl=%x\n",
+ 	    __func__, sa_update_iocb->port_id[2], sa_update_iocb->port_id[1],
+ 	    sa_update_iocb->port_id[0], nport_handle, sa_update_iocb->flags,
+ 	    sa_update_iocb->sa_index, sp->handle);
+ 
+ 	sp->fcport->vha->qla_stats.control_requests++;
+ }
+ 
+ void qla24xx_auth_els(scsi_qla_host_t *vha, void **pkt, struct rsp_que **rsp)
+ {
+ 	struct purex_entry_24xx *p = *pkt;
+ 	struct enode		*ptr;
+ 	int		sid;
+ 	u16 totlen;
+ 	struct purexevent	*purex;
+ 	struct scsi_qla_host *host = NULL;
+ 	int rc;
+ 	struct fc_port *fcport;
+ 	struct qla_els_pt_arg a;
+ 	be_id_t beid;
+ 
+ 	memset(&a, 0, sizeof(a));
+ 
+ 	a.els_opcode = ELS_AUTH_ELS;
+ 	a.nport_handle = p->nport_handle;
+ 	a.rx_xchg_address = p->rx_xchg_addr;
+ 	a.did.b.domain = p->s_id[2];
+ 	a.did.b.area   = p->s_id[1];
+ 	a.did.b.al_pa  = p->s_id[0];
+ 	a.tx_byte_count = a.tx_len = sizeof(struct fc_els_ls_rjt);
+ 	a.tx_addr = vha->hw->elsrej.cdma;
+ 	a.vp_idx = vha->vp_idx;
+ 	a.control_flags = EPD_ELS_RJT;
+ 
+ 	sid = p->s_id[0] | (p->s_id[1] << 8) | (p->s_id[2] << 16);
+ 
+ 	totlen = (le16_to_cpu(p->frame_size) & 0x0fff) - PURX_ELS_HEADER_SIZE;
+ 	if (le16_to_cpu(p->status_flags) & 0x8000) {
+ 		totlen = le16_to_cpu(p->trunc_frame_size);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (totlen > MAX_PAYLOAD) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x0910d,
+ 		    "%s WARNING: verbose ELS frame received (totlen=%x)\n",
+ 		    __func__, totlen);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	if (!vha->hw->flags.edif_enabled) {
+ 		/* edif support not enabled */
+ 		ql_dbg(ql_dbg_edif, vha, 0x910e, "%s edif not enabled\n",
+ 		    __func__);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	ptr = qla_enode_alloc(vha, N_PUREX);
+ 	if (!ptr) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x09109,
+ 		    "WARNING: enode alloc failed for sid=%x\n",
+ 		    sid);
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		__qla_consume_iocb(vha, pkt, rsp);
+ 		return;
+ 	}
+ 
+ 	purex = &ptr->u.purexinfo;
+ 	purex->pur_info.pur_sid = a.did;
+ 	purex->pur_info.pur_pend = 0;
+ 	purex->pur_info.pur_bytes_rcvd = totlen;
+ 	purex->pur_info.pur_rx_xchg_address = le32_to_cpu(p->rx_xchg_addr);
+ 	purex->pur_info.pur_nphdl = le16_to_cpu(p->nport_handle);
+ 	purex->pur_info.pur_did.b.domain =  p->d_id[2];
+ 	purex->pur_info.pur_did.b.area =  p->d_id[1];
+ 	purex->pur_info.pur_did.b.al_pa =  p->d_id[0];
+ 	purex->pur_info.vp_idx = p->vp_idx;
+ 
+ 	rc = __qla_copy_purex_to_buffer(vha, pkt, rsp, purex->msgp,
+ 		purex->msgp_len);
+ 	if (rc) {
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 	beid.al_pa = purex->pur_info.pur_did.b.al_pa;
+ 	beid.area   = purex->pur_info.pur_did.b.area;
+ 	beid.domain = purex->pur_info.pur_did.b.domain;
+ 	host = qla_find_host_by_d_id(vha, beid);
+ 	if (!host) {
+ 		ql_log(ql_log_fatal, vha, 0x508b,
+ 		    "%s Drop ELS due to unable to find host %06x\n",
+ 		    __func__, purex->pur_info.pur_did.b24);
+ 
+ 		qla_els_reject_iocb(vha, (*rsp)->qpair, &a);
+ 		qla_enode_free(vha, ptr);
+ 		return;
+ 	}
+ 
+ 	fcport = qla2x00_find_fcport_by_pid(host, &purex->pur_info.pur_sid);
+ 
+ 	if (host->e_dbell.db_flags != EDB_ACTIVE ||
+ 	    (fcport && EDIF_SESSION_DOWN(fcport))) {
+ 		ql_dbg(ql_dbg_edif, host, 0x0910c, "%s e_dbell.db_flags =%x %06x\n",
+ 		    __func__, host->e_dbell.db_flags,
+ 		    fcport ? fcport->d_id.b24 : 0);
+ 
+ 		qla_els_reject_iocb(host, (*rsp)->qpair, &a);
+ 		qla_enode_free(host, ptr);
+ 		return;
+ 	}
+ 
+ 	/* add the local enode to the list */
+ 	qla_enode_add(host, ptr);
+ 
+ 	ql_dbg(ql_dbg_edif, host, 0x0910c,
+ 	    "%s COMPLETE purex->pur_info.pur_bytes_rcvd =%xh s:%06x -> d:%06x xchg=%xh\n",
+ 	    __func__, purex->pur_info.pur_bytes_rcvd, purex->pur_info.pur_sid.b24,
+ 	    purex->pur_info.pur_did.b24, p->rx_xchg_addr);
+ 
+ 	qla_edb_eventcreate(host, VND_CMD_AUTH_STATE_ELS_RCVD, sid, 0, NULL);
+ }
+ 
+ static uint16_t  qla_edif_get_sa_index_from_freepool(fc_port_t *fcport, int dir)
+ {
+ 	struct scsi_qla_host *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	void *sa_id_map;
+ 	unsigned long flags = 0;
+ 	u16 sa_index;
+ 
+ 	ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_id_map = ha->edif_tx_sa_id_map;
+ 	else
+ 		sa_id_map = ha->edif_rx_sa_id_map;
+ 
+ 	spin_lock_irqsave(&ha->sadb_fp_lock, flags);
+ 	sa_index = find_first_zero_bit(sa_id_map, EDIF_NUM_SA_INDEX);
+ 	if (sa_index >=  EDIF_NUM_SA_INDEX) {
+ 		spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 	set_bit(sa_index, sa_id_map);
+ 	spin_unlock_irqrestore(&ha->sadb_fp_lock, flags);
+ 
+ 	if (dir)
+ 		sa_index += EDIF_TX_SA_INDEX_BASE;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: index retrieved from free pool %d\n", __func__, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* find an sadb entry for an nport_handle */
+ static struct edif_sa_index_entry *
+ qla_edif_sadb_find_sa_index_entry(uint16_t nport_handle,
+ 		struct list_head *sa_list)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct edif_sa_index_entry *tentry;
+ 	struct list_head *indx_list = sa_list;
+ 
+ 	list_for_each_entry_safe(entry, tentry, indx_list, next) {
+ 		if (entry->handle == nport_handle)
+ 			return entry;
+ 	}
+ 	return NULL;
+ }
+ 
+ /* remove an sa_index from the nport_handle and return it to the free pool */
+ static int qla_edif_sadb_delete_sa_index(fc_port_t *fcport, uint16_t nport_handle,
+ 		uint16_t sa_index)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	int dir = (sa_index < EDIF_TX_SA_INDEX_BASE) ? 0 : 1;
+ 	int slot = 0;
+ 	int free_slot_count = 0;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry\n", __func__);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: no entry found for nport_handle 0x%x\n",
+ 		    __func__, nport_handle);
+ 		return -1;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	/*
+ 	 * each tx/rx direction has up to 2 sa indexes/slots. 1 slot for in flight traffic
+ 	 * the other is use at re-key time.
+ 	 */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == sa_index) {
+ 			entry->sa_pair[slot].sa_index = INVALID_EDIF_SA_INDEX;
+ 			entry->sa_pair[slot].spi = 0;
+ 			free_slot_count++;
+ 			qla_edif_add_sa_index_to_freepool(fcport, dir, sa_index);
+ 		} else if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot_count++;
+ 		}
+ 	}
+ 
+ 	if (free_slot_count == 2) {
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: sa_index %d removed, free_slot_count: %d\n",
+ 	    __func__, sa_index, free_slot_count);
+ 
+ 	return 0;
+ }
+ 
+ void
+ qla28xx_sa_update_iocb_entry(scsi_qla_host_t *v, struct req_que *req,
+ 	struct sa_update_28xx *pkt)
+ {
+ 	const char *func = "SA_UPDATE_RESPONSE_IOCB";
+ 	srb_t *sp;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	int old_sa_deleted = 1;
+ 	uint16_t nport_handle;
+ 	struct scsi_qla_host *vha;
+ 
+ 	sp = qla2x00_get_sp_from_handle(v, func, req, pkt);
+ 
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_edif, v, 0x3063,
+ 			"%s: no sp found for pkt\n", __func__);
+ 		return;
+ 	}
+ 	/* use sp->vha due to npiv */
+ 	vha = sp->vha;
+ 
+ 	switch (pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) {
+ 	case 0:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 1:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE RX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 2:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA UPDATE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	case 3:
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: EDIF SA DELETE TX IOCB  vha: 0x%p  index: %d\n",
+ 		    __func__, vha, pkt->sa_index);
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * dig the nport handle out of the iocb, fcport->loop_id can not be trusted
+ 	 * to be correct during cleanup sa_update iocbs.
+ 	 */
+ 	nport_handle = sp->fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: %8phN comp status=%x old_sa_info=%x new_sa_info=%x lid %d, index=0x%x pkt_flags %xh hdl=%x\n",
+ 	    __func__, sp->fcport->port_name, pkt->u.comp_sts, pkt->old_sa_info, pkt->new_sa_info,
+ 	    nport_handle, pkt->sa_index, pkt->flags, sp->handle);
+ 
+ 	/* if rx delete, remove the timer */
+ 	if ((pkt->flags & (SA_FLAG_INVALIDATE | SA_FLAG_TX)) ==  SA_FLAG_INVALIDATE) {
+ 		struct edif_list_entry *edif_entry;
+ 
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 
+ 		edif_entry = qla_edif_list_find_sa_index(sp->fcport, nport_handle);
+ 		if (edif_entry) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: removing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 			qla_edif_list_delete_sa_index(sp->fcport, edif_entry);
+ 			del_timer(&edif_entry->timer);
+ 
+ 			ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 			    "%s: releasing edif_entry %p, new sa_index: 0x%x\n",
+ 			    __func__, edif_entry, pkt->sa_index);
+ 
+ 			kfree(edif_entry);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * if this is a delete for either tx or rx, make sure it succeeded.
+ 	 * The new_sa_info field should be 0xffff on success
+ 	 */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE)
+ 		old_sa_deleted = (le16_to_cpu(pkt->new_sa_info) == 0xffff) ? 1 : 0;
+ 
+ 	/* Process update and delete the same way */
+ 
+ 	/* If this is an sadb cleanup delete, bypass sending events to IPSEC */
+ 	if (sp->flags & SRB_EDIF_CLEANUP_DELETE) {
+ 		sp->fcport->flags &= ~(FCF_ASYNC_SENT | FCF_ASYNC_ACTIVE);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: nph 0x%x, sa_index %d removed from fw\n",
+ 		    __func__, sp->fcport->loop_id, pkt->sa_index);
+ 
+ 	} else if ((pkt->entry_status == 0) && (pkt->u.comp_sts == 0) &&
+ 	    old_sa_deleted) {
+ 		/*
+ 		 * Note: Wa are only keeping track of latest SA,
+ 		 * so we know when we can start enableing encryption per I/O.
+ 		 * If all SA's get deleted, let FW reject the IOCB.
+ 
+ 		 * TODO: edif: don't set enabled here I think
+ 		 * TODO: edif: prli complete is where it should be set
+ 		 */
+ 		ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			"SA(%x)updated for s_id %02x%02x%02x\n",
+ 			pkt->new_sa_info,
+ 			pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 		sp->fcport->edif.enable = 1;
+ 		if (pkt->flags & SA_FLAG_TX) {
+ 			sp->fcport->edif.tx_sa_set = 1;
+ 			sp->fcport->edif.tx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		} else {
+ 			sp->fcport->edif.rx_sa_set = 1;
+ 			sp->fcport->edif.rx_sa_pending = 0;
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				QL_VND_SA_STAT_SUCCESS,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 		}
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: %8phN SA update FAILED: sa_index: %d, new_sa_info %d, %02x%02x%02x\n",
+ 		    __func__, sp->fcport->port_name, pkt->sa_index, pkt->new_sa_info,
+ 		    pkt->port_id[2], pkt->port_id[1], pkt->port_id[0]);
+ 
+ 		if (pkt->flags & SA_FLAG_TX)
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_TX_SA_KEY, sp->fcport);
+ 		else
+ 			qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SAUPDATE_COMPL,
+ 				(le16_to_cpu(pkt->u.comp_sts) << 16) | QL_VND_SA_STAT_FAILED,
+ 				QL_VND_RX_SA_KEY, sp->fcport);
+ 	}
+ 
+ 	/* for delete, release sa_ctl, sa_index */
+ 	if (pkt->flags & SA_FLAG_INVALIDATE) {
+ 		/* release the sa_ctl */
+ 		sa_ctl = qla_edif_find_sa_ctl_by_index(sp->fcport,
+ 		    le16_to_cpu(pkt->sa_index), (pkt->flags & SA_FLAG_TX));
+ 		if (sa_ctl &&
+ 		    qla_edif_find_sa_ctl_by_index(sp->fcport, sa_ctl->index,
+ 			(pkt->flags & SA_FLAG_TX)) != NULL) {
+ 			ql_dbg(ql_dbg_edif + ql_dbg_verbose, vha, 0x3063,
+ 			    "%s: freeing sa_ctl for index %d\n",
+ 			    __func__, sa_ctl->index);
+ 			qla_edif_free_sa_ctl(sp->fcport, sa_ctl, sa_ctl->index);
+ 		} else {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: sa_ctl NOT freed, sa_ctl: %p\n",
+ 			    __func__, sa_ctl);
+ 		}
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, le16_to_cpu(pkt->sa_index), nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 	/*
+ 	 * check for a failed sa_update and remove
+ 	 * the sadb entry.
+ 	 */
+ 	} else if (pkt->u.comp_sts) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: freeing sa_index %d, nph: 0x%x\n",
+ 		    __func__, pkt->sa_index, nport_handle);
+ 		qla_edif_sadb_delete_sa_index(sp->fcport, nport_handle,
+ 		    le16_to_cpu(pkt->sa_index));
+ 		switch (le16_to_cpu(pkt->u.comp_sts)) {
+ 		case CS_PORT_EDIF_UNAVAIL:
+ 		case CS_PORT_EDIF_LOGOUT:
+ 			qlt_schedule_sess_for_deletion(sp->fcport);
+ 			break;
+ 		default:
+ 			break;
+ 		}
+ 	}
+ 
+ 	sp->done(sp, 0);
+ }
+ 
+ /**
+  * qla28xx_start_scsi_edif() - Send a SCSI type 6 command to the ISP
+  * @sp: command to send to the ISP
+  *
+  * Return: non-zero if a failure occurred, else zero.
+  */
+ int
+ qla28xx_start_scsi_edif(srb_t *sp)
+ {
+ 	int             nseg;
+ 	unsigned long   flags;
+ 	struct scsi_cmnd *cmd;
+ 	uint32_t        *clr_ptr;
+ 	uint32_t        index, i;
+ 	uint32_t        handle;
+ 	uint16_t        cnt;
+ 	int16_t        req_cnt;
+ 	uint16_t        tot_dsds;
+ 	__be32 *fcp_dl;
+ 	uint8_t additional_cdb_len;
+ 	struct ct6_dsd *ctx;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	struct cmd_type_6 *cmd_pkt;
+ 	struct dsd64	*cur_dsd;
+ 	uint8_t		avail_dsds = 0;
+ 	struct scatterlist *sg;
+ 	struct req_que *req = sp->qpair->req;
+ 	spinlock_t *lock = sp->qpair->qp_lock_ptr;
+ 
+ 	/* Setup device pointers. */
+ 	cmd = GET_CMD_SP(sp);
+ 
+ 	/* So we know we haven't pci_map'ed anything yet */
+ 	tot_dsds = 0;
+ 
+ 	/* Send marker if required */
+ 	if (vha->marker_needed != 0) {
+ 		if (qla2x00_marker(vha, sp->qpair, 0, 0, MK_SYNC_ALL) !=
+ 			QLA_SUCCESS) {
+ 			ql_log(ql_log_warn, vha, 0x300c,
+ 			    "qla2x00_marker failed for cmd=%p.\n", cmd);
+ 			return QLA_FUNCTION_FAILED;
+ 		}
+ 		vha->marker_needed = 0;
+ 	}
+ 
+ 	/* Acquire ring specific lock */
+ 	spin_lock_irqsave(lock, flags);
+ 
+ 	/* Check for room in outstanding command list. */
+ 	handle = req->current_outstanding_cmd;
+ 	for (index = 1; index < req->num_outstanding_cmds; index++) {
+ 		handle++;
+ 		if (handle == req->num_outstanding_cmds)
+ 			handle = 1;
+ 		if (!req->outstanding_cmds[handle])
+ 			break;
+ 	}
+ 	if (index == req->num_outstanding_cmds)
+ 		goto queuing_error;
+ 
+ 	/* Map the sg table so we have an accurate count of sg entries needed */
+ 	if (scsi_sg_count(cmd)) {
+ 		nseg = dma_map_sg(&ha->pdev->dev, scsi_sglist(cmd),
+ 		    scsi_sg_count(cmd), cmd->sc_data_direction);
+ 		if (unlikely(!nseg))
+ 			goto queuing_error;
+ 	} else {
+ 		nseg = 0;
+ 	}
+ 
+ 	tot_dsds = nseg;
+ 	req_cnt = qla24xx_calc_iocbs(vha, tot_dsds);
+ 	if (req->cnt < (req_cnt + 2)) {
+ 		cnt = IS_SHADOW_REG_CAPABLE(ha) ? *req->out_ptr :
+ 		    rd_reg_dword(req->req_q_out);
+ 		if (req->ring_index < cnt)
+ 			req->cnt = cnt - req->ring_index;
+ 		else
+ 			req->cnt = req->length -
+ 			    (req->ring_index - cnt);
+ 		if (req->cnt < (req_cnt + 2))
+ 			goto queuing_error;
+ 	}
+ 
+ 	ctx = sp->u.scmd.ct6_ctx =
+ 	    mempool_alloc(ha->ctx_mempool, GFP_ATOMIC);
+ 	if (!ctx) {
+ 		ql_log(ql_log_fatal, vha, 0x3010,
+ 		    "Failed to allocate ctx for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	memset(ctx, 0, sizeof(struct ct6_dsd));
+ 	ctx->fcp_cmnd = dma_pool_zalloc(ha->fcp_cmnd_dma_pool,
+ 	    GFP_ATOMIC, &ctx->fcp_cmnd_dma);
+ 	if (!ctx->fcp_cmnd) {
+ 		ql_log(ql_log_fatal, vha, 0x3011,
+ 		    "Failed to allocate fcp_cmnd for cmd=%p.\n", cmd);
+ 		goto queuing_error;
+ 	}
+ 
+ 	/* Initialize the DSD list and dma handle */
+ 	INIT_LIST_HEAD(&ctx->dsd_list);
+ 	ctx->dsd_use_cnt = 0;
+ 
+ 	if (cmd->cmd_len > 16) {
+ 		additional_cdb_len = cmd->cmd_len - 16;
+ 		if ((cmd->cmd_len % 4) != 0) {
+ 			/*
+ 			 * SCSI command bigger than 16 bytes must be
+ 			 * multiple of 4
+ 			 */
+ 			ql_log(ql_log_warn, vha, 0x3012,
+ 			    "scsi cmd len %d not multiple of 4 for cmd=%p.\n",
+ 			    cmd->cmd_len, cmd);
+ 			goto queuing_error_fcp_cmnd;
+ 		}
+ 		ctx->fcp_cmnd_len = 12 + cmd->cmd_len + 4;
+ 	} else {
+ 		additional_cdb_len = 0;
+ 		ctx->fcp_cmnd_len = 12 + 16 + 4;
+ 	}
+ 
+ 	cmd_pkt = (struct cmd_type_6 *)req->ring_ptr;
+ 	cmd_pkt->handle = make_handle(req->id, handle);
+ 
+ 	/*
+ 	 * Zero out remaining portion of packet.
+ 	 * tagged queuing modifier -- default is TSK_SIMPLE (0).
+ 	 */
+ 	clr_ptr = (uint32_t *)cmd_pkt + 2;
+ 	memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
+ 	cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
+ 
+ 	/* No data transfer */
+ 	if (!scsi_bufflen(cmd) || cmd->sc_data_direction == DMA_NONE) {
+ 		cmd_pkt->byte_count = cpu_to_le32(0);
+ 		goto no_dsds;
+ 	}
+ 
+ 	/* Set transfer direction */
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_WRITE_DATA);
+ 		vha->qla_stats.output_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.output_requests++;
+ 		sp->fcport->edif.tx_bytes += scsi_bufflen(cmd);
+ 	} else if (cmd->sc_data_direction == DMA_FROM_DEVICE) {
+ 		cmd_pkt->control_flags = cpu_to_le16(CF_READ_DATA);
+ 		vha->qla_stats.input_bytes += scsi_bufflen(cmd);
+ 		vha->qla_stats.input_requests++;
+ 		sp->fcport->edif.rx_bytes += scsi_bufflen(cmd);
+ 	}
+ 
+ 	cmd_pkt->control_flags |= cpu_to_le16(CF_EN_EDIF);
+ 	cmd_pkt->control_flags &= ~(cpu_to_le16(CF_NEW_SA));
+ 
+ 	/* One DSD is available in the Command Type 6 IOCB */
+ 	avail_dsds = 1;
+ 	cur_dsd = &cmd_pkt->fcp_dsd;
+ 
+ 	/* Load data segments */
+ 	scsi_for_each_sg(cmd, sg, tot_dsds, i) {
+ 		dma_addr_t      sle_dma;
+ 		cont_a64_entry_t *cont_pkt;
+ 
+ 		/* Allocate additional continuation packets? */
+ 		if (avail_dsds == 0) {
+ 			/*
+ 			 * Five DSDs are available in the Continuation
+ 			 * Type 1 IOCB.
+ 			 */
+ 			cont_pkt = qla2x00_prep_cont_type1_iocb(vha, req);
+ 			cur_dsd = cont_pkt->dsd;
+ 			avail_dsds = 5;
+ 		}
+ 
+ 		sle_dma = sg_dma_address(sg);
+ 		put_unaligned_le64(sle_dma, &cur_dsd->address);
+ 		cur_dsd->length = cpu_to_le32(sg_dma_len(sg));
+ 		cur_dsd++;
+ 		avail_dsds--;
+ 	}
+ 
+ no_dsds:
+ 	/* Set NPORT-ID and LUN number*/
+ 	cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
+ 	cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
+ 	cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
+ 	cmd_pkt->vp_index = sp->vha->vp_idx;
+ 
+ 	cmd_pkt->entry_type = COMMAND_TYPE_6;
+ 
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 
+ 	int_to_scsilun(cmd->device->lun, &cmd_pkt->lun);
+ 	host_to_fcp_swap((uint8_t *)&cmd_pkt->lun, sizeof(cmd_pkt->lun));
+ 
+ 	/* build FCP_CMND IU */
+ 	int_to_scsilun(cmd->device->lun, &ctx->fcp_cmnd->lun);
+ 	ctx->fcp_cmnd->additional_cdb_len = additional_cdb_len;
+ 
+ 	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 1;
+ 	else if (cmd->sc_data_direction == DMA_FROM_DEVICE)
+ 		ctx->fcp_cmnd->additional_cdb_len |= 2;
+ 
+ 	/* Populate the FCP_PRIO. */
+ 	if (ha->flags.fcp_prio_enabled)
+ 		ctx->fcp_cmnd->task_attribute |=
+ 		    sp->fcport->fcp_prio << 3;
+ 
+ 	memcpy(ctx->fcp_cmnd->cdb, cmd->cmnd, cmd->cmd_len);
+ 
+ 	fcp_dl = (__be32 *)(ctx->fcp_cmnd->cdb + 16 +
+ 	    additional_cdb_len);
+ 	*fcp_dl = htonl((uint32_t)scsi_bufflen(cmd));
+ 
+ 	cmd_pkt->fcp_cmnd_dseg_len = cpu_to_le16(ctx->fcp_cmnd_len);
+ 	put_unaligned_le64(ctx->fcp_cmnd_dma, &cmd_pkt->fcp_cmnd_dseg_address);
+ 
+ 	sp->flags |= SRB_FCP_CMND_DMA_VALID;
+ 	cmd_pkt->byte_count = cpu_to_le32((uint32_t)scsi_bufflen(cmd));
+ 	/* Set total data segment count. */
+ 	cmd_pkt->entry_count = (uint8_t)req_cnt;
+ 	cmd_pkt->entry_status = 0;
+ 
+ 	/* Build command packet. */
+ 	req->current_outstanding_cmd = handle;
+ 	req->outstanding_cmds[handle] = sp;
+ 	sp->handle = handle;
+ 	cmd->host_scribble = (unsigned char *)(unsigned long)handle;
+ 	req->cnt -= req_cnt;
+ 
+ 	/* Adjust ring index. */
+ 	wmb();
+ 	req->ring_index++;
+ 	if (req->ring_index == req->length) {
+ 		req->ring_index = 0;
+ 		req->ring_ptr = req->ring;
+ 	} else {
+ 		req->ring_ptr++;
+ 	}
+ 
+ 	sp->qpair->cmd_cnt++;
+ 	/* Set chip new ring index. */
+ 	wrt_reg_dword(req->req_q_in, req->ring_index);
+ 
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_SUCCESS;
+ 
+ queuing_error_fcp_cmnd:
+ 	dma_pool_free(ha->fcp_cmnd_dma_pool, ctx->fcp_cmnd, ctx->fcp_cmnd_dma);
+ queuing_error:
+ 	if (tot_dsds)
+ 		scsi_dma_unmap(cmd);
+ 
+ 	if (sp->u.scmd.ct6_ctx) {
+ 		mempool_free(sp->u.scmd.ct6_ctx, ha->ctx_mempool);
+ 		sp->u.scmd.ct6_ctx = NULL;
+ 	}
+ 	spin_unlock_irqrestore(lock, flags);
+ 
+ 	return QLA_FUNCTION_FAILED;
+ }
+ 
+ /**********************************************
+  * edif update/delete sa_index list functions *
+  **********************************************/
+ 
+ /* clear the edif_indx_list for this port */
+ void qla_edif_list_del(fc_port_t *fcport)
+ {
+ 	struct edif_list_entry *indx_lst;
+ 	struct edif_list_entry *tindx_lst;
+ 	struct list_head *indx_list = &fcport->edif.edif_indx_list;
+ 	unsigned long flags = 0;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	list_for_each_entry_safe(indx_lst, tindx_lst, indx_list, next) {
+ 		list_del(&indx_lst->next);
+ 		kfree(indx_lst);
+ 	}
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ }
+ 
+ /******************
+  * SADB functions *
+  ******************/
+ 
+ /* allocate/retrieve an sa_index for a given spi */
+ static uint16_t qla_edif_sadb_get_sa_index(fc_port_t *fcport,
+ 		struct qla_sa_update_frame *sa_frame)
+ {
+ 	struct edif_sa_index_entry *entry;
+ 	struct list_head *sa_list;
+ 	uint16_t sa_index;
+ 	int dir = sa_frame->flags & SAU_FLG_TX;
+ 	int slot = 0;
+ 	int free_slot = -1;
+ 	scsi_qla_host_t *vha = fcport->vha;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 	    "%s: entry  fc_port: %p, nport_handle: 0x%x\n",
+ 	    __func__, fcport, nport_handle);
+ 
+ 	if (dir)
+ 		sa_list = &ha->sadb_tx_index_list;
+ 	else
+ 		sa_list = &ha->sadb_rx_index_list;
+ 
+ 	entry = qla_edif_sadb_find_sa_index_entry(nport_handle, sa_list);
+ 	if (!entry) {
+ 		if ((sa_frame->flags & (SAU_FLG_TX | SAU_FLG_INV)) == SAU_FLG_INV) {
+ 			ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 			    "%s: rx delete request with no entry\n", __func__);
+ 			return RX_DELETE_NO_EDIF_SA_INDEX;
+ 		}
+ 
+ 		/* if there is no entry for this nport, add one */
+ 		entry = kzalloc((sizeof(struct edif_sa_index_entry)), GFP_ATOMIC);
+ 		if (!entry)
+ 			return INVALID_EDIF_SA_INDEX;
+ 
+ 		sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 		if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 			kfree(entry);
+ 			return INVALID_EDIF_SA_INDEX;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&entry->next);
+ 		entry->handle = nport_handle;
+ 		entry->fcport = fcport;
+ 		entry->sa_pair[0].spi = sa_frame->spi;
+ 		entry->sa_pair[0].sa_index = sa_index;
+ 		entry->sa_pair[1].spi = 0;
+ 		entry->sa_pair[1].sa_index = INVALID_EDIF_SA_INDEX;
+ 		spin_lock_irqsave(&ha->sadb_lock, flags);
+ 		list_add_tail(&entry->next, sa_list);
+ 		spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Created new sadb entry for nport_handle 0x%x, spi 0x%x, returning sa_index %d\n",
+ 		    __func__, nport_handle, sa_frame->spi, sa_index);
+ 
+ 		return sa_index;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 
+ 	/* see if we already have an entry for this spi */
+ 	for (slot = 0; slot < 2; slot++) {
+ 		if (entry->sa_pair[slot].sa_index == INVALID_EDIF_SA_INDEX) {
+ 			free_slot = slot;
+ 		} else {
+ 			if (entry->sa_pair[slot].spi == sa_frame->spi) {
+ 				spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 				ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 				    "%s: sadb slot %d entry for lid 0x%x, spi 0x%x found, sa_index %d\n",
+ 				    __func__, slot, entry->handle, sa_frame->spi,
+ 				    entry->sa_pair[slot].sa_index);
+ 				return entry->sa_pair[slot].sa_index;
+ 			}
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 
+ 	/* both slots are used */
+ 	if (free_slot == -1) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: WARNING: No free slots in sadb for nport_handle 0x%x, spi: 0x%x\n",
+ 		    __func__, entry->handle, sa_frame->spi);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: Slot 0  spi: 0x%x  sa_index: %d,  Slot 1  spi: 0x%x  sa_index: %d\n",
+ 		    __func__, entry->sa_pair[0].spi, entry->sa_pair[0].sa_index,
+ 		    entry->sa_pair[1].spi, entry->sa_pair[1].sa_index);
+ 
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	/* there is at least one free slot, use it */
+ 	sa_index = qla_edif_get_sa_index_from_freepool(fcport, dir);
+ 	if (sa_index == INVALID_EDIF_SA_INDEX) {
+ 		ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 		    "%s: empty freepool!!\n", __func__);
+ 		return INVALID_EDIF_SA_INDEX;
+ 	}
+ 
+ 	spin_lock_irqsave(&ha->sadb_lock, flags);
+ 	entry->sa_pair[free_slot].spi = sa_frame->spi;
+ 	entry->sa_pair[free_slot].sa_index = sa_index;
+ 	spin_unlock_irqrestore(&ha->sadb_lock, flags);
+ 	ql_dbg(ql_dbg_edif, fcport->vha, 0x3063,
+ 	    "%s: sadb slot %d entry for nport_handle 0x%x, spi 0x%x added, returning sa_index %d\n",
+ 	    __func__, free_slot, entry->handle, sa_frame->spi, sa_index);
+ 
+ 	return sa_index;
+ }
+ 
+ /* release any sadb entries -- only done at teardown */
+ void qla_edif_sadb_release(struct qla_hw_data *ha)
+ {
+ 	struct list_head *pos;
+ 	struct list_head *tmp;
+ 	struct edif_sa_index_entry *entry;
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_rx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ 
+ 	list_for_each_safe(pos, tmp, &ha->sadb_tx_index_list) {
+ 		entry = list_entry(pos, struct edif_sa_index_entry, next);
+ 		list_del(&entry->next);
+ 		kfree(entry);
+ 	}
+ }
+ 
+ /**************************
+  * sadb freepool functions
+  **************************/
+ 
+ /* build the rx and tx sa_index free pools -- only done at fcport init */
+ int qla_edif_sadb_build_free_pool(struct qla_hw_data *ha)
+ {
+ 	ha->edif_tx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 
+ 	if (!ha->edif_tx_sa_id_map) {
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb tx.\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ha->edif_rx_sa_id_map =
+ 	    kcalloc(BITS_TO_LONGS(EDIF_NUM_SA_INDEX), sizeof(long), GFP_KERNEL);
+ 	if (!ha->edif_rx_sa_id_map) {
+ 		kfree(ha->edif_tx_sa_id_map);
+ 		ha->edif_tx_sa_id_map = NULL;
+ 		ql_log_pci(ql_log_fatal, ha->pdev, 0x0009,
+ 		    "Unable to allocate memory for sadb rx.\n");
+ 		return -ENOMEM;
+ 	}
+ 	return 0;
+ }
+ 
+ /* release the free pool - only done during fcport teardown */
+ void qla_edif_sadb_release_free_pool(struct qla_hw_data *ha)
+ {
+ 	kfree(ha->edif_tx_sa_id_map);
+ 	ha->edif_tx_sa_id_map = NULL;
+ 	kfree(ha->edif_rx_sa_id_map);
+ 	ha->edif_rx_sa_id_map = NULL;
+ }
+ 
+ static void __chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		fc_port_t *fcport, uint32_t handle, uint16_t sa_index)
+ {
+ 	struct edif_list_entry *edif_entry;
+ 	struct edif_sa_ctl *sa_ctl;
+ 	uint16_t delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	unsigned long flags = 0;
+ 	uint16_t nport_handle = fcport->loop_id;
+ 	uint16_t cached_nport_handle;
+ 
+ 	spin_lock_irqsave(&fcport->edif.indx_list_lock, flags);
+ 	edif_entry = qla_edif_list_find_sa_index(fcport, nport_handle);
+ 	if (!edif_entry) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;		/* no pending delete for this handle */
+ 	}
+ 
+ 	/*
+ 	 * check for no pending delete for this index or iocb does not
+ 	 * match rx sa_index
+ 	 */
+ 	if (edif_entry->delete_sa_index == INVALID_EDIF_SA_INDEX ||
+ 	    edif_entry->update_sa_index != sa_index) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * wait until we have seen at least EDIF_DELAY_COUNT transfers before
+ 	 * queueing RX delete
+ 	 */
+ 	if (edif_entry->count++ < EDIF_RX_DELETE_FILTER_COUNT) {
+ 		spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 		return;
+ 	}
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x5033,
+ 	    "%s: invalidating delete_sa_index,  update_sa_index: 0x%x sa_index: 0x%x, delete_sa_index: 0x%x\n",
+ 	    __func__, edif_entry->update_sa_index, sa_index, edif_entry->delete_sa_index);
+ 
+ 	delete_sa_index = edif_entry->delete_sa_index;
+ 	edif_entry->delete_sa_index = INVALID_EDIF_SA_INDEX;
+ 	cached_nport_handle = edif_entry->handle;
+ 	spin_unlock_irqrestore(&fcport->edif.indx_list_lock, flags);
+ 
+ 	/* sanity check on the nport handle */
+ 	if (nport_handle != cached_nport_handle) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE nport_handle mismatch: lid: 0x%x, edif_entry nph: 0x%x\n",
+ 		    __func__, nport_handle, cached_nport_handle);
+ 	}
+ 
+ 	/* find the sa_ctl for the delete and schedule the delete */
+ 	sa_ctl = qla_edif_find_sa_ctl_by_index(fcport, delete_sa_index, 0);
+ 	if (sa_ctl) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl: %p, index recvd %d\n",
+ 		    __func__, sa_ctl, sa_index);
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "delete index %d, update index: %d, nport handle: 0x%x, handle: 0x%x\n",
+ 		    delete_sa_index,
+ 		    edif_entry->update_sa_index, nport_handle, handle);
+ 
+ 		sa_ctl->flags = EDIF_SA_CTL_FLG_DEL;
+ 		set_bit(EDIF_SA_CTL_REPL, &sa_ctl->state);
+ 		qla_post_sa_replace_work(fcport->vha, fcport,
+ 		    nport_handle, sa_ctl);
+ 	} else {
+ 		ql_dbg(ql_dbg_edif, vha, 0x3063,
+ 		    "%s: POST SA DELETE sa_ctl not found for delete_sa_index: %d\n",
+ 		    __func__, delete_sa_index);
+ 	}
+ }
+ 
+ void qla_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha,
+ 		srb_t *sp, struct sts_entry_24xx *sts24)
+ {
+ 	fc_port_t *fcport = sp->fcport;
+ 	/* sa_index used by this iocb */
+ 	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
+ 	uint32_t handle;
+ 
+ 	handle = (uint32_t)LSW(sts24->handle);
+ 
+ 	/* find out if this status iosb is for a scsi read */
+ 	if (cmd->sc_data_direction != DMA_FROM_DEVICE)
+ 		return;
+ 
+ 	return __chk_edif_rx_sa_delete_pending(vha, fcport, handle,
+ 	   le16_to_cpu(sts24->edif_sa_index));
+ }
+ 
+ void qlt_chk_edif_rx_sa_delete_pending(scsi_qla_host_t *vha, fc_port_t *fcport,
+ 		struct ctio7_from_24xx *pkt)
+ {
+ 	__chk_edif_rx_sa_delete_pending(vha, fcport,
+ 	    pkt->handle, le16_to_cpu(pkt->edif_sa_index));
+ }
+ 
+ static void qla_parse_auth_els_ctl(struct srb *sp)
+ {
+ 	struct qla_els_pt_arg *a = &sp->u.bsg_cmd.u.els_arg;
+ 	struct bsg_job *bsg_job = sp->u.bsg_cmd.bsg_job;
+ 	struct fc_bsg_request *request = bsg_job->request;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	a->tx_len = a->tx_byte_count = sp->remap.req.len;
+ 	a->tx_addr = sp->remap.req.dma;
+ 	a->rx_len = a->rx_byte_count = sp->remap.rsp.len;
+ 	a->rx_addr = sp->remap.rsp.dma;
+ 
+ 	if (p->e.sub_cmd == SEND_ELS_REPLY) {
+ 		a->control_flags = p->e.extra_control_flags << 13;
+ 		a->rx_xchg_address = cpu_to_le32(p->e.extra_rx_xchg_address);
+ 		if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_ACC)
+ 			a->els_opcode = ELS_LS_ACC;
+ 		else if (p->e.extra_control_flags == BSG_CTL_FLAG_LS_RJT)
+ 			a->els_opcode = ELS_LS_RJT;
+ 	}
+ 	a->did = sp->fcport->d_id;
+ 	a->els_opcode =  request->rqst_data.h_els.command_code;
+ 	a->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	a->vp_idx = sp->vha->vp_idx;
+ }
+ 
+ int qla_edif_process_els(scsi_qla_host_t *vha, struct bsg_job *bsg_job)
+ {
+ 	struct fc_bsg_request *bsg_request = bsg_job->request;
+ 	struct fc_bsg_reply *bsg_reply = bsg_job->reply;
+ 	fc_port_t *fcport = NULL;
+ 	struct qla_hw_data *ha = vha->hw;
+ 	srb_t *sp;
+ 	int rval =  (DID_ERROR << 16);
+ 	port_id_t d_id;
+ 	struct qla_bsg_auth_els_request *p =
+ 	    (struct qla_bsg_auth_els_request *)bsg_job->request;
+ 
+ 	d_id.b.al_pa = bsg_request->rqst_data.h_els.port_id[2];
+ 	d_id.b.area = bsg_request->rqst_data.h_els.port_id[1];
+ 	d_id.b.domain = bsg_request->rqst_data.h_els.port_id[0];
+ 
+ 	/* find matching d_id in fcport list */
+ 	fcport = qla2x00_find_fcport_by_pid(vha, &d_id);
+ 	if (!fcport) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x911a,
+ 		    "%s fcport not find online portid=%06x.\n",
+ 		    __func__, d_id.b24);
+ 		SET_DID_STATUS(bsg_reply->result, DID_ERROR);
+ 		return -EIO;
+ 	}
+ 
+ 	if (qla_bsg_check(vha, bsg_job, fcport))
+ 		return 0;
+ 
+ 	if (fcport->loop_id == FC_NO_LOOP_ID) {
+ 		ql_dbg(ql_dbg_edif, vha, 0x910d,
+ 		    "%s ELS code %x, no loop id.\n", __func__,
+ 		    bsg_request->rqst_data.r_els.els_code);
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		return -ENXIO;
+ 	}
+ 
+ 	if (!vha->flags.online) {
+ 		ql_log(ql_log_warn, vha, 0x7005, "Host not online.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EIO;
+ 		goto done;
+ 	}
+ 
+ 	/* pass through is supported only for ISP 4Gb or higher */
+ 	if (!IS_FWI2_CAPABLE(ha)) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7001,
+ 		    "ELS passthru not supported for ISP23xx based adapters.\n");
+ 		SET_DID_STATUS(bsg_reply->result, DID_BAD_TARGET);
+ 		rval = -EPERM;
+ 		goto done;
+ 	}
+ 
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7004,
+ 		    "Failed get sp pid=%06x\n", fcport->d_id.b24);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done;
+ 	}
+ 
+ 	sp->remap.req.len = bsg_job->request_payload.payload_len;
+ 	sp->remap.req.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.req.dma);
+ 	if (!sp->remap.req.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7005,
+ 		    "Failed allocate request dma len=%x\n",
+ 		    bsg_job->request_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->remap.rsp.len = bsg_job->reply_payload.payload_len;
+ 	sp->remap.rsp.buf = dma_pool_alloc(ha->purex_dma_pool,
+ 	    GFP_KERNEL, &sp->remap.rsp.dma);
+ 	if (!sp->remap.rsp.buf) {
+ 		ql_dbg(ql_dbg_user, vha, 0x7006,
+ 		    "Failed allocate response dma len=%x\n",
+ 		    bsg_job->reply_payload.payload_len);
+ 		rval = -ENOMEM;
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		goto done_free_remap_req;
+ 	}
+ 	sg_copy_to_buffer(bsg_job->request_payload.sg_list,
+ 	    bsg_job->request_payload.sg_cnt, sp->remap.req.buf,
+ 	    sp->remap.req.len);
+ 	sp->remap.remapped = true;
+ 
+ 	sp->type = SRB_ELS_CMD_HST_NOLOGIN;
+ 	sp->name = "SPCN_BSG_HST_NOLOGIN";
+ 	sp->u.bsg_cmd.bsg_job = bsg_job;
+ 	qla_parse_auth_els_ctl(sp);
+ 
+ 	sp->free = qla2x00_bsg_sp_free;
+ 	sp->done = qla2x00_bsg_job_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 
+ 	ql_dbg(ql_dbg_edif, vha, 0x700a,
+ 	    "%s %s %8phN xchg %x ctlflag %x hdl %x reqlen %xh bsg ptr %p\n",
+ 	    __func__, sc_to_str(p->e.sub_cmd), fcport->port_name,
+ 	    p->e.extra_rx_xchg_address, p->e.extra_control_flags,
+ 	    sp->handle, sp->remap.req.len, bsg_job);
+ 
+ 	if (rval != QLA_SUCCESS) {
+ 		ql_log(ql_log_warn, vha, 0x700e,
+ 		    "qla2x00_start_sp failed = %d\n", rval);
+ 		SET_DID_STATUS(bsg_reply->result, DID_IMM_RETRY);
+ 		rval = -EIO;
+ 		goto done_free_remap_rsp;
+ 	}
+ 	return rval;
+ 
+ done_free_remap_rsp:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.rsp.buf,
+ 	    sp->remap.rsp.dma);
+ done_free_remap_req:
+ 	dma_pool_free(ha->purex_dma_pool, sp->remap.req.buf,
+ 	    sp->remap.req.dma);
+ done_free_sp:
+ 	qla2x00_rel_sp(sp);
+ 
+ done:
+ 	return rval;
+ }
+ 
+ void qla_edif_sess_down(struct scsi_qla_host *vha, struct fc_port *sess)
+ {
+ 	if (sess->edif.app_sess_online && vha->e_dbell.db_flags & EDB_ACTIVE) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xf09c,
+ 			"%s: sess %8phN send port_offline event\n",
+ 			__func__, sess->port_name);
+ 		sess->edif.app_sess_online = 0;
+ 		qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_SESSION_SHUTDOWN,
+ 		    sess->d_id.b24, 0, sess);
+ 		qla2x00_post_aen_work(vha, FCH_EVT_PORT_OFFLINE, sess->d_id.b24);
+ 	}
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  }
diff --cc drivers/scsi/qla2xxx/qla_edif.h
index d7d1433295c7,9e8f28d0caa1..000000000000
--- a/drivers/scsi/qla2xxx/qla_edif.h
+++ b/drivers/scsi/qla2xxx/qla_edif.h
@@@ -30,4 -51,86 +30,89 @@@ struct edif_dbell 
  	struct	completion	dbell;
  };
  
++<<<<<<< HEAD
++=======
+ #define SA_UPDATE_IOCB_TYPE            0x71    /* Security Association Update IOCB entry */
+ struct sa_update_28xx {
+ 	uint8_t entry_type;             /* Entry type. */
+ 	uint8_t entry_count;            /* Entry count. */
+ 	uint8_t sys_define;             /* System Defined. */
+ 	uint8_t entry_status;           /* Entry Status. */
+ 
+ 	uint32_t handle;                /* IOCB System handle. */
+ 
+ 	union {
+ 		__le16 nport_handle;  /* in: N_PORT handle. */
+ 		__le16 comp_sts;              /* out: completion status */
+ #define CS_PORT_EDIF_UNAVAIL	0x28
+ #define CS_PORT_EDIF_LOGOUT	0x29
+ #define CS_PORT_EDIF_SUPP_NOT_RDY 0x64
+ #define CS_PORT_EDIF_INV_REQ      0x66
+ 	} u;
+ 	uint8_t vp_index;
+ 	uint8_t reserved_1;
+ 	uint8_t port_id[3];
+ 	uint8_t flags;
+ #define SA_FLAG_INVALIDATE BIT_0
+ #define SA_FLAG_TX	   BIT_1 // 1=tx, 0=rx
+ 
+ 	uint8_t sa_key[32];     /* 256 bit key */
+ 	__le32 salt;
+ 	__le32 spi;
+ 	uint8_t sa_control;
+ #define SA_CNTL_ENC_FCSP        (1 << 3)
+ #define SA_CNTL_ENC_OPD         (2 << 3)
+ #define SA_CNTL_ENC_MSK         (3 << 3)  // mask bits 4,3
+ #define SA_CNTL_AES_GMAC	(1 << 2)
+ #define SA_CNTL_KEY256          (2 << 0)
+ #define SA_CNTL_KEY128          0
+ 
+ 	uint8_t reserved_2;
+ 	__le16 sa_index;   // reserve: bit 11-15
+ 	__le16 old_sa_info;
+ 	__le16 new_sa_info;
+ };
+ 
+ #define        NUM_ENTRIES     256
+ #define        MAX_PAYLOAD     1024
+ #define        PUR_GET         1
+ 
+ struct dinfo {
+ 	int		nodecnt;
+ 	int		lstate;
+ };
+ 
+ struct pur_ninfo {
+ 	unsigned int	pur_pend:1;
+ 	port_id_t       pur_sid;
+ 	port_id_t	pur_did;
+ 	uint8_t		vp_idx;
+ 	short           pur_bytes_rcvd;
+ 	unsigned short  pur_nphdl;
+ 	unsigned int    pur_rx_xchg_address;
+ };
+ 
+ struct purexevent {
+ 	struct  pur_ninfo	pur_info;
+ 	unsigned char		*msgp;
+ 	u32			msgp_len;
+ };
+ 
+ #define	N_UNDEF		0
+ #define	N_PUREX		1
+ struct enode {
+ 	struct list_head	list;
+ 	struct dinfo		dinfo;
+ 	uint32_t		ntype;
+ 	union {
+ 		struct purexevent	purexinfo;
+ 	} u;
+ };
+ 
+ #define EDIF_SESSION_DOWN(_s) \
+ 	(qla_ini_mode_enabled(_s->vha) && (_s->disc_state == DSC_DELETE_PEND || \
+ 	 _s->disc_state == DSC_DELETED || \
+ 	 !_s->edif.app_sess_online))
+ 
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  #endif	/* __QLA_EDIF_H */
diff --cc drivers/scsi/qla2xxx/qla_gbl.h
index 50ae84979282,1c3f055d41b8..000000000000
--- a/drivers/scsi/qla2xxx/qla_gbl.h
+++ b/drivers/scsi/qla2xxx/qla_gbl.h
@@@ -954,6 -987,16 +954,18 @@@ void qla_edb_stop(scsi_qla_host_t *vha)
  int32_t qla_edif_app_mgmt(struct bsg_job *bsg_job);
  void qla_enode_init(scsi_qla_host_t *vha);
  void qla_enode_stop(scsi_qla_host_t *vha);
++<<<<<<< HEAD
++=======
+ void qla_edif_flush_sa_ctl_lists(fc_port_t *fcport);
+ void qla_edb_init(scsi_qla_host_t *vha);
+ void qla_edif_timer(scsi_qla_host_t *vha);
+ int qla28xx_start_scsi_edif(srb_t *sp);
+ void qla24xx_sa_update_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb);
+ void qla24xx_sa_replace_iocb(srb_t *sp, struct sa_update_28xx *sa_update_iocb);
+ void qla24xx_auth_els(scsi_qla_host_t *vha, void **pkt, struct rsp_que **rsp);
+ void qla28xx_sa_update_iocb_entry(scsi_qla_host_t *vha, struct req_que *req,
+ 		struct sa_update_28xx *pkt);
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  void qla_handle_els_plogi_done(scsi_qla_host_t *vha, struct event_arg *ea);
  
  #define QLA2XX_HW_ERROR			BIT_0
diff --cc drivers/scsi/qla2xxx/qla_init.c
index a3b6e4dac55b,4c5acfde0788..000000000000
--- a/drivers/scsi/qla2xxx/qla_init.c
+++ b/drivers/scsi/qla2xxx/qla_init.c
@@@ -1418,6 -1426,60 +1411,63 @@@ void __qla24xx_handle_gpdb_event(scsi_q
  	spin_unlock_irqrestore(&vha->hw->tgt.sess_lock, flags);
  }
  
++<<<<<<< HEAD
++=======
+ static int	qla_chk_secure_login(scsi_qla_host_t	*vha, fc_port_t *fcport,
+ 	struct port_database_24xx *pd)
+ {
+ 	int rc = 0;
+ 
+ 	if (pd->secure_login) {
+ 		ql_dbg(ql_dbg_disc, vha, 0x104d,
+ 		    "Secure Login established on %8phC\n",
+ 		    fcport->port_name);
+ 		fcport->edif.secured_login = 1;
+ 		fcport->edif.non_secured_login = 0;
+ 		fcport->flags |= FCF_FCSP_DEVICE;
+ 	} else {
+ 		ql_dbg(ql_dbg_disc, vha, 0x104d,
+ 		    "non-Secure Login %8phC",
+ 		    fcport->port_name);
+ 		fcport->edif.secured_login = 0;
+ 		fcport->edif.non_secured_login = 1;
+ 	}
+ 	if (vha->hw->flags.edif_enabled) {
+ 		if (fcport->edif.secured_login) {
+ 			qla2x00_set_fcport_disc_state(fcport, DSC_LOGIN_AUTH_PEND);
+ 			/* Start edif prli timer & ring doorbell for app */
+ 			fcport->edif.rx_sa_set = 0;
+ 			fcport->edif.tx_sa_set = 0;
+ 			fcport->edif.rx_sa_pending = 0;
+ 			fcport->edif.tx_sa_pending = 0;
+ 
+ 			qla2x00_post_aen_work(vha, FCH_EVT_PORT_ONLINE,
+ 			    fcport->d_id.b24);
+ 
+ 			if (vha->e_dbell.db_flags ==  EDB_ACTIVE) {
+ 				ql_dbg(ql_dbg_disc, vha, 0x20ef,
+ 				    "%s %d %8phC EDIF: post DB_AUTH: AUTH needed\n",
+ 				    __func__, __LINE__, fcport->port_name);
+ 				fcport->edif.app_started = 1;
+ 				fcport->edif.app_sess_online = 1;
+ 
+ 				qla_edb_eventcreate(vha, VND_CMD_AUTH_STATE_NEEDED,
+ 				    fcport->d_id.b24, 0, fcport);
+ 			}
+ 
+ 			rc = 1;
+ 		} else if (qla_ini_mode_enabled(vha) || qla_dual_mode_enabled(vha)) {
+ 			ql_dbg(ql_dbg_disc, vha, 0x2117,
+ 			    "%s %d %8phC post prli\n",
+ 			    __func__, __LINE__, fcport->port_name);
+ 			qla24xx_post_prli_work(vha, fcport);
+ 			rc = 1;
+ 		}
+ 	}
+ 	return rc;
+ }
+ 
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  static
  void qla24xx_handle_gpdb_event(scsi_qla_host_t *vha, struct event_arg *ea)
  {
@@@ -1460,8 -1527,14 +1515,18 @@@
  	case PDS_PRLI_COMPLETE:
  		__qla24xx_parse_gpdb(vha, fcport, pd);
  		break;
++<<<<<<< HEAD
++=======
+ 	case PDS_PLOGI_COMPLETE:
+ 		if (qla_chk_secure_login(vha, fcport, pd)) {
+ 			ql_dbg(ql_dbg_disc, vha, 0x20d5, "%s %d %8phC, ls %x\n",
+ 			       __func__, __LINE__, fcport->port_name, ls);
+ 			return;
+ 		}
+ 		fallthrough;
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  	case PDS_PLOGI_PENDING:
 +	case PDS_PLOGI_COMPLETE:
  	case PDS_PRLI_PENDING:
  	case PDS_PRLI2_PENDING:
  		/* Set discovery state back to GNL to Relogin attempt */
@@@ -4094,9 -4190,23 +4169,22 @@@ qla24xx_update_fw_options(scsi_qla_host
  			ha->fw_options[2] |= BIT_8;
  		else
  			ha->fw_options[2] &= ~BIT_8;
+ 
+ 		/*
+ 		 * N2N: set Secure=1 for PLOGI ACC and
+ 		 * fw shal not send PRLI after PLOGI Acc
+ 		 */
+ 		if (ha->flags.edif_enabled &&
+ 		    vha->e_dbell.db_flags & EDB_ACTIVE) {
+ 			ha->fw_options[3] |= BIT_15;
+ 			ha->flags.n2n_fw_acc_sec = 1;
+ 		} else {
+ 			ha->fw_options[3] &= ~BIT_15;
+ 			ha->flags.n2n_fw_acc_sec = 0;
+ 		}
  	}
  
 -	if (ql2xrdpenable || ha->flags.scm_supported_f ||
 -	    ha->flags.edif_enabled)
 +	if (ql2xrdpenable || ha->flags.scm_supported_f)
  		ha->fw_options[1] |= ADD_FO1_ENABLE_PUREX_IOCB;
  
  	/* Enable Async 8130/8131 events -- transceiver insertion/removal */
@@@ -5226,6 -5344,16 +5321,19 @@@ qla2x00_configure_loop(scsi_qla_host_t 
  			    "LOOP READY.\n");
  			ha->flags.fw_init_done = 1;
  
++<<<<<<< HEAD
++=======
+ 			if (ha->flags.edif_enabled &&
+ 			    !(vha->e_dbell.db_flags & EDB_ACTIVE) &&
+ 			    N2N_TOPO(vha->hw)) {
+ 				/*
+ 				 * use port online to wake up app to get ready
+ 				 * for authentication
+ 				 */
+ 				qla2x00_post_aen_work(vha, FCH_EVT_PORT_ONLINE, 0);
+ 			}
+ 
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  			/*
  			 * Process any ATIO queue entries that came in
  			 * while we weren't online.
diff --cc drivers/scsi/qla2xxx/qla_mbx.c
index 611f9530ce87,2964f5280bed..000000000000
--- a/drivers/scsi/qla2xxx/qla_mbx.c
+++ b/drivers/scsi/qla2xxx/qla_mbx.c
@@@ -1131,6 -1137,13 +1131,16 @@@ qla2x00_get_fw_version(scsi_qla_host_t 
  			       ha->fw_attributes_ext[0]);
  			vha->flags.nvme2_enabled = 1;
  		}
++<<<<<<< HEAD
++=======
+ 
+ 		if (IS_QLA28XX(ha) && ha->flags.edif_hw && ql2xsecenable &&
+ 		    (ha->fw_attributes_ext[0] & FW_ATTR_EXT0_EDIF)) {
+ 			ha->flags.edif_enabled = 1;
+ 			ql_log(ql_log_info, vha, 0xffff,
+ 			       "%s: edif is enabled\n", __func__);
+ 		}
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  	}
  
  	if (IS_QLA27XX(ha) || IS_QLA28XX(ha)) {
diff --cc drivers/scsi/qla2xxx/qla_os.c
index 6ae02d789e96,bc8abe226fa6..000000000000
--- a/drivers/scsi/qla2xxx/qla_os.c
+++ b/drivers/scsi/qla2xxx/qla_os.c
@@@ -3819,6 -3963,7 +3819,10 @@@ void qla2x00_mark_device_lost(scsi_qla_
  		qla2x00_set_fcport_state(fcport, FCS_DEVICE_LOST);
  		qla2x00_schedule_rport_del(vha, fcport);
  	}
++<<<<<<< HEAD
++=======
+ 
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  	/*
  	 * We may need to retry the login, so don't change the state of the
  	 * port but do the retries.
diff --cc drivers/scsi/qla2xxx/qla_target.c
index cc3934c87715,2f4da88995ea..000000000000
--- a/drivers/scsi/qla2xxx/qla_target.c
+++ b/drivers/scsi/qla2xxx/qla_target.c
@@@ -635,6 -635,11 +635,14 @@@ int qla24xx_async_notify_ack(scsi_qla_h
  	case SRB_NACK_PLOGI:
  		fcport->fw_login_state = DSC_LS_PLOGI_PEND;
  		c = "PLOGI";
++<<<<<<< HEAD
++=======
+ 		if (vha->hw->flags.edif_enabled &&
+ 		    (le16_to_cpu(ntfy->u.isp24.flags) & NOTIFY24XX_FLAGS_FCSP)) {
+ 			fcport->flags |= FCF_FCSP_DEVICE;
+ 			fcport->edif.secured_login = 1;
+ 		}
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  		break;
  	case SRB_NACK_PRLI:
  		fcport->fw_login_state = DSC_LS_PRLI_PEND;
@@@ -976,6 -992,21 +990,24 @@@ void qlt_free_session_done(struct work_
  		sess->send_els_logo);
  
  	if (!IS_SW_RESV_ADDR(sess->d_id)) {
++<<<<<<< HEAD
++=======
+ 		if (ha->flags.edif_enabled &&
+ 		    (!own || own->iocb.u.isp24.status_subcode == ELS_PLOGI)) {
+ 			sess->edif.authok = 0;
+ 			if (!ha->flags.host_shutting_down) {
+ 				ql_dbg(ql_dbg_edif, vha, 0x911e,
+ 					"%s wwpn %8phC calling qla2x00_release_all_sadb\n",
+ 					__func__, sess->port_name);
+ 				qla2x00_release_all_sadb(vha, sess);
+ 			} else {
+ 				ql_dbg(ql_dbg_edif, vha, 0x911e,
+ 					"%s bypassing release_all_sadb\n",
+ 					__func__);
+ 			}
+ 			qla_edif_sess_down(vha, sess);
+ 		}
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  		qla2x00_mark_device_lost(vha, sess, 0);
  
  		if (sess->send_els_logo) {
@@@ -4744,6 -4806,34 +4776,37 @@@ static int qlt_handle_login(struct scsi
  		goto out;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (vha->hw->flags.edif_enabled &&
+ 	    !(vha->e_dbell.db_flags & EDB_ACTIVE) &&
+ 	    iocb->u.isp24.status_subcode == ELS_PLOGI &&
+ 	    !(le16_to_cpu(iocb->u.isp24.flags) & NOTIFY24XX_FLAGS_FCSP)) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			"%s %d Term INOT due to app not available lid=%d, NportID %06X ",
+ 			__func__, __LINE__, loop_id, port_id.b24);
+ 		qlt_send_term_imm_notif(vha, iocb, 1);
+ 		goto out;
+ 	}
+ 
+ 	if (vha->hw->flags.edif_enabled) {
+ 		if (!(vha->e_dbell.db_flags & EDB_ACTIVE)) {
+ 			ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			       "%s %d Term INOT due to app not started lid=%d, NportID %06X ",
+ 			       __func__, __LINE__, loop_id, port_id.b24);
+ 			qlt_send_term_imm_notif(vha, iocb, 1);
+ 			goto out;
+ 		} else if (iocb->u.isp24.status_subcode == ELS_PLOGI &&
+ 			   !(le16_to_cpu(iocb->u.isp24.flags) & NOTIFY24XX_FLAGS_FCSP)) {
+ 			ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			       "%s %d Term INOT due to unsecure lid=%d, NportID %06X ",
+ 			       __func__, __LINE__, loop_id, port_id.b24);
+ 			qlt_send_term_imm_notif(vha, iocb, 1);
+ 			goto out;
+ 		}
+ 	}
+ 
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  	pla = qlt_plogi_ack_find_add(vha, &port_id, iocb);
  	if (!pla) {
  		ql_dbg(ql_dbg_disc + ql_dbg_verbose, vha, 0xffff,
@@@ -4809,6 -4899,20 +4872,23 @@@
  	qlt_plogi_ack_link(vha, pla, sess, QLT_PLOGI_LINK_SAME_WWN);
  	sess->d_id = port_id;
  	sess->login_gen++;
++<<<<<<< HEAD
++=======
+ 	sess->loop_id = loop_id;
+ 
+ 	if (iocb->u.isp24.status_subcode == ELS_PLOGI) {
+ 		/* remote port has assigned Port ID */
+ 		if (N2N_TOPO(vha->hw) && fcport_is_bigger(sess))
+ 			vha->d_id = sess->d_id;
+ 
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "%s %8phC - send port online\n",
+ 		    __func__, sess->port_name);
+ 
+ 		qla2x00_post_aen_work(vha, FCH_EVT_PORT_ONLINE,
+ 		    sess->d_id.b24);
+ 	}
++>>>>>>> 4de067e5df12 (scsi: qla2xxx: edif: Add N2N support for EDIF)
  
  	if (iocb->u.isp24.status_subcode == ELS_PRLI) {
  		sess->fw_login_state = DSC_LS_PRLI_PEND;
* Unmerged path drivers/scsi/qla2xxx/qla_def.h
* Unmerged path drivers/scsi/qla2xxx/qla_edif.c
* Unmerged path drivers/scsi/qla2xxx/qla_edif.h
diff --git a/drivers/scsi/qla2xxx/qla_fw.h b/drivers/scsi/qla2xxx/qla_fw.h
index 44db5e1fc35e..1db738dc4da0 100644
--- a/drivers/scsi/qla2xxx/qla_fw.h
+++ b/drivers/scsi/qla2xxx/qla_fw.h
@@ -806,6 +806,7 @@ struct els_entry_24xx {
 #define EPD_RX_XCHG		(3 << 13)
 #define ECF_CLR_PASSTHRU_PEND	BIT_12
 #define ECF_INCL_FRAME_HDR	BIT_11
+#define ECF_SEC_LOGIN		BIT_3
 
 	union {
 		struct {
* Unmerged path drivers/scsi/qla2xxx/qla_gbl.h
* Unmerged path drivers/scsi/qla2xxx/qla_init.c
diff --git a/drivers/scsi/qla2xxx/qla_inline.h b/drivers/scsi/qla2xxx/qla_inline.h
index d8fbc6422b8b..707e5bec48ee 100644
--- a/drivers/scsi/qla2xxx/qla_inline.h
+++ b/drivers/scsi/qla2xxx/qla_inline.h
@@ -479,3 +479,19 @@ bool qla_pci_disconnected(struct scsi_qla_host *vha,
 	}
 	return ret;
 }
+
+static inline bool
+fcport_is_smaller(fc_port_t *fcport)
+{
+	if (wwn_to_u64(fcport->port_name) <
+		wwn_to_u64(fcport->vha->port_name))
+		return true;
+	else
+		return false;
+}
+
+static inline bool
+fcport_is_bigger(fc_port_t *fcport)
+{
+	return !fcport_is_smaller(fcport);
+}
diff --git a/drivers/scsi/qla2xxx/qla_iocb.c b/drivers/scsi/qla2xxx/qla_iocb.c
index 7c55022f5a26..3a11a0411faf 100644
--- a/drivers/scsi/qla2xxx/qla_iocb.c
+++ b/drivers/scsi/qla2xxx/qla_iocb.c
@@ -2786,7 +2786,10 @@ qla24xx_els_logo_iocb(srb_t *sp, struct els_entry_24xx *els_iocb)
 	els_iocb->s_id[0] = vha->d_id.b.domain;
 
 	if (elsio->u.els_logo.els_cmd == ELS_DCMD_PLOGI) {
-		els_iocb->control_flags = 0;
+		if (vha->hw->flags.edif_enabled)
+			els_iocb->control_flags = cpu_to_le16(ECF_SEC_LOGIN);
+		else
+			els_iocb->control_flags = 0;
 		els_iocb->tx_byte_count = els_iocb->tx_len =
 			cpu_to_le32(sizeof(struct els_plogi_payload));
 		put_unaligned_le64(elsio->u.els_plogi.els_plogi_pyld_dma,
@@ -3027,7 +3030,7 @@ qla24xx_els_dcmd2_iocb(scsi_qla_host_t *vha, int els_opcode,
 	qla2x00_set_fcport_disc_state(fcport, DSC_LOGIN_PEND);
 	elsio = &sp->u.iocb_cmd;
 	ql_dbg(ql_dbg_io, vha, 0x3073,
-	    "Enter: PLOGI portid=%06x\n", fcport->d_id.b24);
+	       "%s Enter: PLOGI portid=%06x\n", __func__, fcport->d_id.b24);
 
 	sp->type = SRB_ELS_DCMD;
 	sp->name = "ELS_DCMD";
@@ -3070,6 +3073,13 @@ qla24xx_els_dcmd2_iocb(scsi_qla_host_t *vha, int els_opcode,
 	elsio->u.els_plogi.els_cmd = els_opcode;
 	elsio->u.els_plogi.els_plogi_pyld->opcode = els_opcode;
 
+	if (els_opcode == ELS_DCMD_PLOGI && vha->hw->flags.edif_enabled &&
+	    vha->e_dbell.db_flags & EDB_ACTIVE) {
+		struct fc_els_flogi *p = ptr;
+
+		p->fl_csp.sp_features |= cpu_to_be16(FC_SP_FT_SEC);
+	}
+
 	ql_dbg(ql_dbg_disc + ql_dbg_buffer, vha, 0x3073, "PLOGI buffer:\n");
 	ql_dump_buffer(ql_dbg_disc + ql_dbg_buffer, vha, 0x0109,
 	    (uint8_t *)elsio->u.els_plogi.els_plogi_pyld,
* Unmerged path drivers/scsi/qla2xxx/qla_mbx.c
* Unmerged path drivers/scsi/qla2xxx/qla_os.c
* Unmerged path drivers/scsi/qla2xxx/qla_target.c
