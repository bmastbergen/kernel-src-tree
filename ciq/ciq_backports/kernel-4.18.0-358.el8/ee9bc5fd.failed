{x86,powerpc,microblaze}/kmap: move preempt disable

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ira Weiny <ira.weiny@intel.com>
commit ee9bc5fdf5b6d24875fc55d43d5a0728bc2add21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/ee9bc5fd.failed

During this kmap() conversion series we must maintain bisect-ability.  To
do this, kmap_atomic_prot() in x86, powerpc, and microblaze need to remain
functional.

Create a temporary inline version of kmap_atomic_prot within these
architectures so we can rework their kmap_atomic() calls and then lift
kmap_atomic_prot() to the core.

	Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
	Signed-off-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Christian KÃ¶nig <christian.koenig@amd.com>
	Cc: Chris Zankel <chris@zankel.net>
	Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Helge Deller <deller@gmx.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20200507150004.1423069-6-ira.weiny@intel.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ee9bc5fdf5b6d24875fc55d43d5a0728bc2add21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/microblaze/include/asm/highmem.h
#	arch/powerpc/include/asm/highmem.h
#	arch/x86/include/asm/highmem.h
diff --cc arch/microblaze/include/asm/highmem.h
index 332c78e15198,c38d920a1171..000000000000
--- a/arch/microblaze/include/asm/highmem.h
+++ b/arch/microblaze/include/asm/highmem.h
@@@ -51,27 -51,18 +51,40 @@@ extern pte_t *pkmap_page_table
  #define PKMAP_NR(virt)  ((virt - PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +extern void *kmap_atomic_prot(struct page *page, pgprot_t prot);
++=======
+ extern void *kmap_atomic_high_prot(struct page *page, pgprot_t prot);
+ static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ {
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	return kmap_atomic_high_prot(page, prot);
+ }
++>>>>>>> ee9bc5fdf5b6 ({x86,powerpc,microblaze}/kmap: move preempt disable)
  extern void __kunmap_atomic(void *kvaddr);
  
 +static inline void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +
 +static inline void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +
  static inline void *kmap_atomic(struct page *page)
  {
  	return kmap_atomic_prot(page, kmap_prot);
diff --cc arch/powerpc/include/asm/highmem.h
index cec820f961da,d049806a8354..000000000000
--- a/arch/powerpc/include/asm/highmem.h
+++ b/arch/powerpc/include/asm/highmem.h
@@@ -59,27 -59,18 +59,40 @@@ extern pte_t *pkmap_page_table
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +extern void *kmap_atomic_prot(struct page *page, pgprot_t prot);
++=======
+ extern void *kmap_atomic_high_prot(struct page *page, pgprot_t prot);
+ static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ {
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	return kmap_atomic_high_prot(page, prot);
+ }
++>>>>>>> ee9bc5fdf5b6 ({x86,powerpc,microblaze}/kmap: move preempt disable)
  extern void __kunmap_atomic(void *kvaddr);
  
 +static inline void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +
 +static inline void kunmap(struct page *page)
 +{
 +	BUG_ON(in_interrupt());
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +
  static inline void *kmap_atomic(struct page *page)
  {
  	return kmap_atomic_prot(page, kmap_prot);
diff --cc arch/x86/include/asm/highmem.h
index a8059930056d,61f47fef40e5..000000000000
--- a/arch/x86/include/asm/highmem.h
+++ b/arch/x86/include/asm/highmem.h
@@@ -58,13 -58,16 +58,26 @@@ extern unsigned long highstart_pfn, hig
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +
 +void *kmap(struct page *page);
 +void kunmap(struct page *page);
 +
 +void *kmap_atomic_prot(struct page *page, pgprot_t prot);
++=======
+ extern void *kmap_atomic_high_prot(struct page *page, pgprot_t prot);
+ static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ {
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	return kmap_atomic_high_prot(page, prot);
+ }
++>>>>>>> ee9bc5fdf5b6 ({x86,powerpc,microblaze}/kmap: move preempt disable)
  void *kmap_atomic(struct page *page);
  void __kunmap_atomic(void *kvaddr);
  void *kmap_atomic_pfn(unsigned long pfn);
* Unmerged path arch/microblaze/include/asm/highmem.h
diff --git a/arch/microblaze/mm/highmem.c b/arch/microblaze/mm/highmem.c
index d7569f77fa15..0e3efaa8a004 100644
--- a/arch/microblaze/mm/highmem.c
+++ b/arch/microblaze/mm/highmem.c
@@ -32,18 +32,12 @@
  */
 #include <asm/tlbflush.h>
 
-void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
 {
 
 	unsigned long vaddr;
 	int idx, type;
 
-	preempt_disable();
-	pagefault_disable();
-	if (!PageHighMem(page))
-		return page_address(page);
-
-
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
@@ -55,7 +49,7 @@ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 
 	return (void *) vaddr;
 }
-EXPORT_SYMBOL(kmap_atomic_prot);
+EXPORT_SYMBOL(kmap_atomic_high_prot);
 
 void __kunmap_atomic(void *kvaddr)
 {
* Unmerged path arch/powerpc/include/asm/highmem.h
diff --git a/arch/powerpc/mm/highmem.c b/arch/powerpc/mm/highmem.c
index 668e87d03f9e..7b8b3b37a8af 100644
--- a/arch/powerpc/mm/highmem.c
+++ b/arch/powerpc/mm/highmem.c
@@ -30,16 +30,11 @@
  * be used in IRQ contexts, so in some (very limited) cases we need
  * it.
  */
-void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
 {
 	unsigned long vaddr;
 	int idx, type;
 
-	preempt_disable();
-	pagefault_disable();
-	if (!PageHighMem(page))
-		return page_address(page);
-
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
@@ -51,7 +46,7 @@ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 
 	return (void*) vaddr;
 }
-EXPORT_SYMBOL(kmap_atomic_prot);
+EXPORT_SYMBOL(kmap_atomic_high_prot);
 
 void __kunmap_atomic(void *kvaddr)
 {
* Unmerged path arch/x86/include/asm/highmem.h
diff --git a/arch/x86/mm/highmem_32.c b/arch/x86/mm/highmem_32.c
index 0d4bdcb84da5..85cef93dccad 100644
--- a/arch/x86/mm/highmem_32.c
+++ b/arch/x86/mm/highmem_32.c
@@ -30,17 +30,11 @@ EXPORT_SYMBOL(kunmap);
  * However when holding an atomic kmap it is not legal to sleep, so atomic
  * kmaps are appropriate for short, tight code paths only.
  */
-void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
 {
 	unsigned long vaddr;
 	int idx, type;
 
-	preempt_disable();
-	pagefault_disable();
-
-	if (!PageHighMem(page))
-		return page_address(page);
-
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
@@ -50,7 +44,7 @@ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 
 	return (void *)vaddr;
 }
-EXPORT_SYMBOL(kmap_atomic_prot);
+EXPORT_SYMBOL(kmap_atomic_high_prot);
 
 void *kmap_atomic(struct page *page)
 {
