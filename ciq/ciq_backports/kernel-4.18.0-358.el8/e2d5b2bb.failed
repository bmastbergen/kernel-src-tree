bpf: Fix nested bpf_bprintf_prepare with more per-cpu buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Florent Revest <revest@chromium.org>
commit e2d5b2bb769fa5f500760caba76436ba3a10a895
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/e2d5b2bb.failed

The bpf_seq_printf, bpf_trace_printk and bpf_snprintf helpers share one
per-cpu buffer that they use to store temporary data (arguments to
bprintf). They "get" that buffer with try_get_fmt_tmp_buf and "put" it
by the end of their scope with bpf_bprintf_cleanup.

If one of these helpers gets called within the scope of one of these
helpers, for example: a first bpf program gets called, uses
bpf_trace_printk which calls raw_spin_lock_irqsave which is traced by
another bpf program that calls bpf_snprintf, then the second "get"
fails. Essentially, these helpers are not re-entrant. They would return
-EBUSY and print a warning message once.

This patch triples the number of bprintf buffers to allow three levels
of nesting. This is very similar to what was done for tracepoints in
"9594dc3c7e7 bpf: fix nested bpf tracepoints with per-cpu data"

Fixes: d9c9e4db186a ("bpf: Factorize bpf_trace_printk and bpf_seq_printf")
	Reported-by: syzbot+63122d0bc347f18c1884@syzkaller.appspotmail.com
	Signed-off-by: Florent Revest <revest@chromium.org>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20210511081054.2125874-1-revest@chromium.org
(cherry picked from commit e2d5b2bb769fa5f500760caba76436ba3a10a895)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/helpers.c
diff --cc kernel/bpf/helpers.c
index cc97656102f8,ef658a9ea5c9..000000000000
--- a/kernel/bpf/helpers.c
+++ b/kernel/bpf/helpers.c
@@@ -670,6 -669,323 +670,326 @@@ const struct bpf_func_proto bpf_this_cp
  	.arg1_type	= ARG_PTR_TO_PERCPU_BTF_ID,
  };
  
++<<<<<<< HEAD
++=======
+ static int bpf_trace_copy_string(char *buf, void *unsafe_ptr, char fmt_ptype,
+ 		size_t bufsz)
+ {
+ 	void __user *user_ptr = (__force void __user *)unsafe_ptr;
+ 
+ 	buf[0] = 0;
+ 
+ 	switch (fmt_ptype) {
+ 	case 's':
+ #ifdef CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE
+ 		if ((unsigned long)unsafe_ptr < TASK_SIZE)
+ 			return strncpy_from_user_nofault(buf, user_ptr, bufsz);
+ 		fallthrough;
+ #endif
+ 	case 'k':
+ 		return strncpy_from_kernel_nofault(buf, unsafe_ptr, bufsz);
+ 	case 'u':
+ 		return strncpy_from_user_nofault(buf, user_ptr, bufsz);
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ /* Per-cpu temp buffers which can be used by printf-like helpers for %s or %p
+  */
+ #define MAX_PRINTF_BUF_LEN	512
+ 
+ /* Support executing three nested bprintf helper calls on a given CPU */
+ struct bpf_bprintf_buffers {
+ 	char tmp_bufs[3][MAX_PRINTF_BUF_LEN];
+ };
+ static DEFINE_PER_CPU(struct bpf_bprintf_buffers, bpf_bprintf_bufs);
+ static DEFINE_PER_CPU(int, bpf_bprintf_nest_level);
+ 
+ static int try_get_fmt_tmp_buf(char **tmp_buf)
+ {
+ 	struct bpf_bprintf_buffers *bufs;
+ 	int nest_level;
+ 
+ 	preempt_disable();
+ 	nest_level = this_cpu_inc_return(bpf_bprintf_nest_level);
+ 	if (WARN_ON_ONCE(nest_level > ARRAY_SIZE(bufs->tmp_bufs))) {
+ 		this_cpu_dec(bpf_bprintf_nest_level);
+ 		preempt_enable();
+ 		return -EBUSY;
+ 	}
+ 	bufs = this_cpu_ptr(&bpf_bprintf_bufs);
+ 	*tmp_buf = bufs->tmp_bufs[nest_level - 1];
+ 
+ 	return 0;
+ }
+ 
+ void bpf_bprintf_cleanup(void)
+ {
+ 	if (this_cpu_read(bpf_bprintf_nest_level)) {
+ 		this_cpu_dec(bpf_bprintf_nest_level);
+ 		preempt_enable();
+ 	}
+ }
+ 
+ /*
+  * bpf_bprintf_prepare - Generic pass on format strings for bprintf-like helpers
+  *
+  * Returns a negative value if fmt is an invalid format string or 0 otherwise.
+  *
+  * This can be used in two ways:
+  * - Format string verification only: when bin_args is NULL
+  * - Arguments preparation: in addition to the above verification, it writes in
+  *   bin_args a binary representation of arguments usable by bstr_printf where
+  *   pointers from BPF have been sanitized.
+  *
+  * In argument preparation mode, if 0 is returned, safe temporary buffers are
+  * allocated and bpf_bprintf_cleanup should be called to free them after use.
+  */
+ int bpf_bprintf_prepare(char *fmt, u32 fmt_size, const u64 *raw_args,
+ 			u32 **bin_args, u32 num_args)
+ {
+ 	char *unsafe_ptr = NULL, *tmp_buf = NULL, *tmp_buf_end, *fmt_end;
+ 	size_t sizeof_cur_arg, sizeof_cur_ip;
+ 	int err, i, num_spec = 0;
+ 	u64 cur_arg;
+ 	char fmt_ptype, cur_ip[16], ip_spec[] = "%pXX";
+ 
+ 	fmt_end = strnchr(fmt, fmt_size, 0);
+ 	if (!fmt_end)
+ 		return -EINVAL;
+ 	fmt_size = fmt_end - fmt;
+ 
+ 	if (bin_args) {
+ 		if (num_args && try_get_fmt_tmp_buf(&tmp_buf))
+ 			return -EBUSY;
+ 
+ 		tmp_buf_end = tmp_buf + MAX_PRINTF_BUF_LEN;
+ 		*bin_args = (u32 *)tmp_buf;
+ 	}
+ 
+ 	for (i = 0; i < fmt_size; i++) {
+ 		if ((!isprint(fmt[i]) && !isspace(fmt[i])) || !isascii(fmt[i])) {
+ 			err = -EINVAL;
+ 			goto out;
+ 		}
+ 
+ 		if (fmt[i] != '%')
+ 			continue;
+ 
+ 		if (fmt[i + 1] == '%') {
+ 			i++;
+ 			continue;
+ 		}
+ 
+ 		if (num_spec >= num_args) {
+ 			err = -EINVAL;
+ 			goto out;
+ 		}
+ 
+ 		/* The string is zero-terminated so if fmt[i] != 0, we can
+ 		 * always access fmt[i + 1], in the worst case it will be a 0
+ 		 */
+ 		i++;
+ 
+ 		/* skip optional "[0 +-][num]" width formatting field */
+ 		while (fmt[i] == '0' || fmt[i] == '+'  || fmt[i] == '-' ||
+ 		       fmt[i] == ' ')
+ 			i++;
+ 		if (fmt[i] >= '1' && fmt[i] <= '9') {
+ 			i++;
+ 			while (fmt[i] >= '0' && fmt[i] <= '9')
+ 				i++;
+ 		}
+ 
+ 		if (fmt[i] == 'p') {
+ 			sizeof_cur_arg = sizeof(long);
+ 
+ 			if ((fmt[i + 1] == 'k' || fmt[i + 1] == 'u') &&
+ 			    fmt[i + 2] == 's') {
+ 				fmt_ptype = fmt[i + 1];
+ 				i += 2;
+ 				goto fmt_str;
+ 			}
+ 
+ 			if (fmt[i + 1] == 0 || isspace(fmt[i + 1]) ||
+ 			    ispunct(fmt[i + 1]) || fmt[i + 1] == 'K' ||
+ 			    fmt[i + 1] == 'x' || fmt[i + 1] == 's' ||
+ 			    fmt[i + 1] == 'S') {
+ 				/* just kernel pointers */
+ 				if (tmp_buf)
+ 					cur_arg = raw_args[num_spec];
+ 				i++;
+ 				goto nocopy_fmt;
+ 			}
+ 
+ 			if (fmt[i + 1] == 'B') {
+ 				if (tmp_buf)  {
+ 					err = snprintf(tmp_buf,
+ 						       (tmp_buf_end - tmp_buf),
+ 						       "%pB",
+ 						       (void *)(long)raw_args[num_spec]);
+ 					tmp_buf += (err + 1);
+ 				}
+ 
+ 				i++;
+ 				num_spec++;
+ 				continue;
+ 			}
+ 
+ 			/* only support "%pI4", "%pi4", "%pI6" and "%pi6". */
+ 			if ((fmt[i + 1] != 'i' && fmt[i + 1] != 'I') ||
+ 			    (fmt[i + 2] != '4' && fmt[i + 2] != '6')) {
+ 				err = -EINVAL;
+ 				goto out;
+ 			}
+ 
+ 			i += 2;
+ 			if (!tmp_buf)
+ 				goto nocopy_fmt;
+ 
+ 			sizeof_cur_ip = (fmt[i] == '4') ? 4 : 16;
+ 			if (tmp_buf_end - tmp_buf < sizeof_cur_ip) {
+ 				err = -ENOSPC;
+ 				goto out;
+ 			}
+ 
+ 			unsafe_ptr = (char *)(long)raw_args[num_spec];
+ 			err = copy_from_kernel_nofault(cur_ip, unsafe_ptr,
+ 						       sizeof_cur_ip);
+ 			if (err < 0)
+ 				memset(cur_ip, 0, sizeof_cur_ip);
+ 
+ 			/* hack: bstr_printf expects IP addresses to be
+ 			 * pre-formatted as strings, ironically, the easiest way
+ 			 * to do that is to call snprintf.
+ 			 */
+ 			ip_spec[2] = fmt[i - 1];
+ 			ip_spec[3] = fmt[i];
+ 			err = snprintf(tmp_buf, tmp_buf_end - tmp_buf,
+ 				       ip_spec, &cur_ip);
+ 
+ 			tmp_buf += err + 1;
+ 			num_spec++;
+ 
+ 			continue;
+ 		} else if (fmt[i] == 's') {
+ 			fmt_ptype = fmt[i];
+ fmt_str:
+ 			if (fmt[i + 1] != 0 &&
+ 			    !isspace(fmt[i + 1]) &&
+ 			    !ispunct(fmt[i + 1])) {
+ 				err = -EINVAL;
+ 				goto out;
+ 			}
+ 
+ 			if (!tmp_buf)
+ 				goto nocopy_fmt;
+ 
+ 			if (tmp_buf_end == tmp_buf) {
+ 				err = -ENOSPC;
+ 				goto out;
+ 			}
+ 
+ 			unsafe_ptr = (char *)(long)raw_args[num_spec];
+ 			err = bpf_trace_copy_string(tmp_buf, unsafe_ptr,
+ 						    fmt_ptype,
+ 						    tmp_buf_end - tmp_buf);
+ 			if (err < 0) {
+ 				tmp_buf[0] = '\0';
+ 				err = 1;
+ 			}
+ 
+ 			tmp_buf += err;
+ 			num_spec++;
+ 
+ 			continue;
+ 		}
+ 
+ 		sizeof_cur_arg = sizeof(int);
+ 
+ 		if (fmt[i] == 'l') {
+ 			sizeof_cur_arg = sizeof(long);
+ 			i++;
+ 		}
+ 		if (fmt[i] == 'l') {
+ 			sizeof_cur_arg = sizeof(long long);
+ 			i++;
+ 		}
+ 
+ 		if (fmt[i] != 'i' && fmt[i] != 'd' && fmt[i] != 'u' &&
+ 		    fmt[i] != 'x' && fmt[i] != 'X') {
+ 			err = -EINVAL;
+ 			goto out;
+ 		}
+ 
+ 		if (tmp_buf)
+ 			cur_arg = raw_args[num_spec];
+ nocopy_fmt:
+ 		if (tmp_buf) {
+ 			tmp_buf = PTR_ALIGN(tmp_buf, sizeof(u32));
+ 			if (tmp_buf_end - tmp_buf < sizeof_cur_arg) {
+ 				err = -ENOSPC;
+ 				goto out;
+ 			}
+ 
+ 			if (sizeof_cur_arg == 8) {
+ 				*(u32 *)tmp_buf = *(u32 *)&cur_arg;
+ 				*(u32 *)(tmp_buf + 4) = *((u32 *)&cur_arg + 1);
+ 			} else {
+ 				*(u32 *)tmp_buf = (u32)(long)cur_arg;
+ 			}
+ 			tmp_buf += sizeof_cur_arg;
+ 		}
+ 		num_spec++;
+ 	}
+ 
+ 	err = 0;
+ out:
+ 	if (err)
+ 		bpf_bprintf_cleanup();
+ 	return err;
+ }
+ 
+ #define MAX_SNPRINTF_VARARGS		12
+ 
+ BPF_CALL_5(bpf_snprintf, char *, str, u32, str_size, char *, fmt,
+ 	   const void *, data, u32, data_len)
+ {
+ 	int err, num_args;
+ 	u32 *bin_args;
+ 
+ 	if (data_len % 8 || data_len > MAX_SNPRINTF_VARARGS * 8 ||
+ 	    (data_len && !data))
+ 		return -EINVAL;
+ 	num_args = data_len / 8;
+ 
+ 	/* ARG_PTR_TO_CONST_STR guarantees that fmt is zero-terminated so we
+ 	 * can safely give an unbounded size.
+ 	 */
+ 	err = bpf_bprintf_prepare(fmt, UINT_MAX, data, &bin_args, num_args);
+ 	if (err < 0)
+ 		return err;
+ 
+ 	err = bstr_printf(str, str_size, fmt, bin_args);
+ 
+ 	bpf_bprintf_cleanup();
+ 
+ 	return err + 1;
+ }
+ 
+ const struct bpf_func_proto bpf_snprintf_proto = {
+ 	.func		= bpf_snprintf,
+ 	.gpl_only	= true,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_MEM_OR_NULL,
+ 	.arg2_type	= ARG_CONST_SIZE_OR_ZERO,
+ 	.arg3_type	= ARG_PTR_TO_CONST_STR,
+ 	.arg4_type	= ARG_PTR_TO_MEM_OR_NULL,
+ 	.arg5_type	= ARG_CONST_SIZE_OR_ZERO,
+ };
+ 
++>>>>>>> e2d5b2bb769f (bpf: Fix nested bpf_bprintf_prepare with more per-cpu buffers)
  const struct bpf_func_proto bpf_get_current_task_proto __weak;
  const struct bpf_func_proto bpf_probe_read_user_proto __weak;
  const struct bpf_func_proto bpf_probe_read_user_str_proto __weak;
* Unmerged path kernel/bpf/helpers.c
