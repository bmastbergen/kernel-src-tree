KVM: selftests: Add PTE helper for x86-64 in preparation for hugepages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Sean Christopherson <seanjc@google.com>
commit f681d6861b0c7b28af1a339171602a6e82b1cbda
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/f681d686.failed

Add a helper to retrieve a PTE pointer given a PFN, address, and level
in preparation for adding hugepage support.

No functional change intended.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210622200529.3650424-17-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f681d6861b0c7b28af1a339171602a6e82b1cbda)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/kvm/lib/x86_64/processor.c
diff --cc tools/testing/selftests/kvm/lib/x86_64/processor.c
index 4e80df28f873,6796b65e181c..000000000000
--- a/tools/testing/selftests/kvm/lib/x86_64/processor.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/processor.c
@@@ -221,11 -216,21 +221,26 @@@ void virt_pgd_alloc(struct kvm_vm *vm, 
  	}
  }
  
++<<<<<<< HEAD
 +void virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
 +	uint32_t pgd_memslot)
++=======
+ static void *virt_get_pte(struct kvm_vm *vm, uint64_t pt_pfn, uint64_t vaddr,
+ 			  int level)
+ {
+ 	uint64_t *page_table = addr_gpa2hva(vm, pt_pfn << vm->page_shift);
+ 	int index = vaddr >> (vm->page_shift + level * 9) & 0x1ffu;
+ 
+ 	return &page_table[index];
+ }
+ 
+ void virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)
++>>>>>>> f681d6861b0c (KVM: selftests: Add PTE helper for x86-64 in preparation for hugepages)
  {
- 	uint16_t index[4];
  	struct pageMapL4Entry *pml4e;
+ 	struct pageDirectoryPointerEntry *pdpe;
+ 	struct pageDirectoryEntry *pde;
+ 	struct pageTableEntry *pte;
  
  	TEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, "Attempt to use "
  		"unknown or unsupported guest mode, mode: 0x%x", vm->mode);
@@@ -247,49 -252,35 +262,75 @@@
  		"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
  		paddr, vm->max_gfn, vm->page_size);
  
- 	index[0] = (vaddr >> 12) & 0x1ffu;
- 	index[1] = (vaddr >> 21) & 0x1ffu;
- 	index[2] = (vaddr >> 30) & 0x1ffu;
- 	index[3] = (vaddr >> 39) & 0x1ffu;
- 
  	/* Allocate page directory pointer table if not present. */
++<<<<<<< HEAD
 +	pml4e = addr_gpa2hva(vm, vm->pgd);
 +	if (!pml4e[index[3]].present) {
 +		pml4e[index[3]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pml4e[index[3]].writable = true;
 +		pml4e[index[3]].present = true;
 +	}
 +
 +	/* Allocate page directory table if not present. */
 +	struct pageDirectoryPointerEntry *pdpe;
 +	pdpe = addr_gpa2hva(vm, pml4e[index[3]].address * vm->page_size);
 +	if (!pdpe[index[2]].present) {
 +		pdpe[index[2]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pdpe[index[2]].writable = true;
 +		pdpe[index[2]].present = true;
 +	}
 +
 +	/* Allocate page table if not present. */
 +	struct pageDirectoryEntry *pde;
 +	pde = addr_gpa2hva(vm, pdpe[index[2]].address * vm->page_size);
 +	if (!pde[index[1]].present) {
 +		pde[index[1]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pde[index[1]].writable = true;
 +		pde[index[1]].present = true;
 +	}
 +
 +	/* Fill in page table entry. */
 +	struct pageTableEntry *pte;
 +	pte = addr_gpa2hva(vm, pde[index[1]].address * vm->page_size);
 +	pte[index[0]].address = paddr >> vm->page_shift;
 +	pte[index[0]].writable = true;
 +	pte[index[0]].present = 1;
++=======
+ 	pml4e = virt_get_pte(vm, vm->pgd >> vm->page_shift, vaddr, 3);
+ 	if (!pml4e->present) {
+ 		pml4e->pfn = vm_alloc_page_table(vm) >> vm->page_shift;
+ 		pml4e->writable = true;
+ 		pml4e->present = true;
+ 	}
+ 
+ 	/* Allocate page directory table if not present. */
+ 	pdpe = virt_get_pte(vm, pml4e->pfn, vaddr, 2);
+ 	if (!pdpe->present) {
+ 		pdpe->pfn = vm_alloc_page_table(vm) >> vm->page_shift;
+ 		pdpe->writable = true;
+ 		pdpe->present = true;
+ 	}
+ 
+ 	/* Allocate page table if not present. */
+ 	pde = virt_get_pte(vm, pdpe->pfn, vaddr, 1);
+ 	if (!pde->present) {
+ 		pde->pfn = vm_alloc_page_table(vm) >> vm->page_shift;
+ 		pde->writable = true;
+ 		pde->present = true;
+ 	}
+ 
+ 	/* Fill in page table entry. */
+ 	pte = virt_get_pte(vm, pde->pfn, vaddr, 0);
+ 	pte->pfn = paddr >> vm->page_shift;
+ 	pte->writable = true;
+ 	pte->present = 1;
++>>>>>>> f681d6861b0c (KVM: selftests: Add PTE helper for x86-64 in preparation for hugepages)
  }
  
  void virt_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)
* Unmerged path tools/testing/selftests/kvm/lib/x86_64/processor.c
