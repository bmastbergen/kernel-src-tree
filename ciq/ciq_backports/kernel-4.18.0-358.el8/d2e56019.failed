KVM: nSVM: Move TLB flushing logic (or lack thereof) to dedicated helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Sean Christopherson <seanjc@google.com>
commit d2e5601907bd294411920a84c0231473557d16b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/d2e56019.failed

Introduce nested_svm_transition_tlb_flush() and use it force an MMU sync
and TLB flush on nSVM VM-Enter and VM-Exit instead of sneaking the logic
into the __kvm_mmu_new_pgd() call sites.  Add a partial todo list to
document issues that need to be addressed before the unconditional sync
and flush can be modified to look more like nVMX's logic.

In addition to making nSVM's forced flushing more overt (guess who keeps
losing track of it), the new helper brings further convergence between
nSVM and nVMX, and also sets the stage for dropping the "skip" params
from __kvm_mmu_new_pgd().

	Cc: Maxim Levitsky <mlevitsk@redhat.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210609234235.1244004-7-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d2e5601907bd294411920a84c0231473557d16b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
diff --cc arch/x86/kvm/svm/nested.c
index 776469a3419a,20e672236a75..000000000000
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@@ -486,10 -493,8 +501,15 @@@ static void nested_vmcb02_prepare_save(
  
  static void nested_vmcb02_prepare_control(struct vcpu_svm *svm)
  {
++<<<<<<< HEAD
 +	const u32 int_ctl_vmcb01_bits =
 +		V_INTR_MASKING_MASK | V_GIF_MASK | V_GIF_ENABLE_MASK;
 +
 +	const u32 int_ctl_vmcb12_bits = V_TPR_MASK | V_IRQ_INJECTION_BITS_MASK;
++=======
+ 	const u32 mask = V_INTR_MASKING_MASK | V_GIF_ENABLE_MASK | V_GIF_MASK;
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
++>>>>>>> d2e5601907bd (KVM: nSVM: Move TLB flushing logic (or lack thereof) to dedicated helper)
  
  	/*
  	 * Filled at exit: exit_code, exit_code_hi, exit_info_1, exit_info_2,
@@@ -514,14 -519,14 +534,14 @@@
  
  	/* nested_cr3.  */
  	if (nested_npt_enabled(svm))
- 		nested_svm_init_mmu_context(&svm->vcpu);
+ 		nested_svm_init_mmu_context(vcpu);
  
- 	svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset =
- 		svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+ 	svm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset =
+ 		vcpu->arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
  
  	svm->vmcb->control.int_ctl             =
 -		(svm->nested.ctl.int_ctl & ~mask) |
 -		(svm->vmcb01.ptr->control.int_ctl & mask);
 +		(svm->nested.ctl.int_ctl & int_ctl_vmcb12_bits) |
 +		(svm->vmcb01.ptr->control.int_ctl & int_ctl_vmcb01_bits);
  
  	svm->vmcb->control.virt_ext            = svm->nested.ctl.virt_ext;
  	svm->vmcb->control.int_vector          = svm->nested.ctl.int_vector;
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 3b76d5727c6b..3c13d27113c7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4709,7 +4709,7 @@ void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, u32 cr0, u32 cr4, u32 efer,
 	struct kvm_mmu *context = &vcpu->arch.guest_mmu;
 	union kvm_mmu_role new_role = kvm_calc_shadow_npt_root_page_role(vcpu);
 
-	__kvm_mmu_new_pgd(vcpu, nested_cr3, new_role.base, false, false);
+	__kvm_mmu_new_pgd(vcpu, nested_cr3, new_role.base, true, true);
 
 	if (new_role.as_u64 != context->mmu_role.as_u64) {
 		shadow_mmu_init_context(vcpu, context, cr0, cr4, efer, new_role);
* Unmerged path arch/x86/kvm/svm/nested.c
