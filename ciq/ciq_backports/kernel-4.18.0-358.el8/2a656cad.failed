mm/highmem: Take kmap_high_get() properly into account

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 2a656cad337e0e1ca582f58847d7b0c7eeba4dc8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/2a656cad.failed

kunmap_local() warns when the virtual address to unmap is below
PAGE_OFFSET. This is correct except for the case that the mapping was
obtained via kmap_high_get() because the PKMAP addresses are right below
PAGE_OFFSET.

Cure it by skipping the WARN_ON() when the unmap was handled by
kunmap_high().

Fixes: 298fa1ad5571 ("highmem: Provide generic variant of kmap_atomic*")
	Reported-by: vtolkm@googlemail.com
	Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Tested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
Link: https://lore.kernel.org/r/87y2j6n8mj.fsf@nanos.tec.linutronix.de

(cherry picked from commit 2a656cad337e0e1ca582f58847d7b0c7eeba4dc8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/highmem.c
diff --cc mm/highmem.c
index 51171d0f44c2,78c481a30c9a..000000000000
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@@ -365,8 -358,164 +365,168 @@@ void kunmap_high(struct page *page
  	if (need_wakeup)
  		wake_up(pkmap_map_wait);
  }
 +
  EXPORT_SYMBOL(kunmap_high);
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_HIGHMEM */
+ 
+ #ifdef CONFIG_KMAP_LOCAL
+ 
+ #include <asm/kmap_size.h>
+ 
+ static DEFINE_PER_CPU(int, __kmap_local_idx);
+ 
+ /*
+  * With DEBUG_HIGHMEM the stack depth is doubled and every second
+  * slot is unused which acts as a guard page
+  */
+ #ifdef CONFIG_DEBUG_HIGHMEM
+ # define KM_INCR	2
+ #else
+ # define KM_INCR	1
+ #endif
+ 
+ static inline int kmap_local_idx_push(void)
+ {
+ 	int idx = __this_cpu_add_return(__kmap_local_idx, KM_INCR) - 1;
+ 
+ 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+ 	BUG_ON(idx >= KM_MAX_IDX);
+ 	return idx;
+ }
+ 
+ static inline int kmap_local_idx(void)
+ {
+ 	return __this_cpu_read(__kmap_local_idx) - 1;
+ }
+ 
+ static inline void kmap_local_idx_pop(void)
+ {
+ 	int idx = __this_cpu_sub_return(__kmap_local_idx, KM_INCR);
+ 
+ 	BUG_ON(idx < 0);
+ }
+ 
+ #ifndef arch_kmap_local_post_map
+ # define arch_kmap_local_post_map(vaddr, pteval)	do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_pre_unmap
+ # define arch_kmap_local_pre_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_post_unmap
+ # define arch_kmap_local_post_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_map_idx
+ #define arch_kmap_local_map_idx(idx, pfn)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_unmap_idx
+ #define arch_kmap_local_unmap_idx(idx, vaddr)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_high_get
+ static inline void *arch_kmap_local_high_get(struct page *page)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ /* Unmap a local mapping which was obtained by kmap_high_get() */
+ static inline bool kmap_high_unmap_local(unsigned long vaddr)
+ {
+ #ifdef ARCH_NEEDS_KMAP_HIGH_GET
+ 	if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
+ 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
+ 		return true;
+ 	}
+ #endif
+ 	return false;
+ }
+ 
+ static inline int kmap_local_calc_idx(int idx)
+ {
+ 	return idx + KM_MAX_IDX * smp_processor_id();
+ }
+ 
+ static pte_t *__kmap_pte;
+ 
+ static pte_t *kmap_get_pte(void)
+ {
+ 	if (!__kmap_pte)
+ 		__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
+ 	return __kmap_pte;
+ }
+ 
+ void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
+ {
+ 	pte_t pteval, *kmap_pte = kmap_get_pte();
+ 	unsigned long vaddr;
+ 	int idx;
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 	BUG_ON(!pte_none(*(kmap_pte - idx)));
+ 	pteval = pfn_pte(pfn, prot);
+ 	set_pte_at(&init_mm, vaddr, kmap_pte - idx, pteval);
+ 	arch_kmap_local_post_map(vaddr, pteval);
+ 	preempt_enable();
+ 
+ 	return (void *)vaddr;
+ }
+ EXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);
+ 
+ void *__kmap_local_page_prot(struct page *page, pgprot_t prot)
+ {
+ 	void *kmap;
+ 
+ 	if (!PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	/* Try kmap_high_get() if architecture has it enabled */
+ 	kmap = arch_kmap_local_high_get(page);
+ 	if (kmap)
+ 		return kmap;
+ 
+ 	return __kmap_local_pfn_prot(page_to_pfn(page), prot);
+ }
+ EXPORT_SYMBOL(__kmap_local_page_prot);
+ 
+ void kunmap_local_indexed(void *vaddr)
+ {
+ 	unsigned long addr = (unsigned long) vaddr & PAGE_MASK;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int idx;
+ 
+ 	if (addr < __fix_to_virt(FIX_KMAP_END) ||
+ 	    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {
+ 		/*
+ 		 * Handle mappings which were obtained by kmap_high_get()
+ 		 * first as the virtual address of such mappings is below
+ 		 * PAGE_OFFSET. Warn for all other addresses which are in
+ 		 * the user space part of the virtual address space.
+ 		 */
+ 		if (!kmap_high_unmap_local(addr))
+ 			WARN_ON_ONCE(addr < PAGE_OFFSET);
+ 		return;
+ 	}
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);
+ 	WARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
+ 
+ 	arch_kmap_local_pre_unmap(addr);
+ 	pte_clear(&init_mm, addr, kmap_pte - idx);
+ 	arch_kmap_local_post_unmap(addr);
+ 	kmap_local_idx_pop();
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL(kunmap_local_indexed);
++>>>>>>> 2a656cad337e (mm/highmem: Take kmap_high_get() properly into account)
  #endif
  
  #if defined(HASHED_PAGE_VIRTUAL)
* Unmerged path mm/highmem.c
