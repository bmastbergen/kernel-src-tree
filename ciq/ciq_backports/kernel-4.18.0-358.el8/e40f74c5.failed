cpumask: Introduce DYING mask

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit e40f74c535b8a0ecf3ef0388b51a34cdadb34fb5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/e40f74c5.failed

Introduce a cpumask that indicates (for each CPU) what direction the
CPU hotplug is currently going. Notably, it tracks rollbacks. Eg. when
an up fails and we do a roll-back down, it will accurately reflect the
direction.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
Link: https://lkml.kernel.org/r/20210310150109.151441252@infradead.org
(cherry picked from commit e40f74c535b8a0ecf3ef0388b51a34cdadb34fb5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/cpumask.h
diff --cc include/linux/cpumask.h
index 3adc2fc32e67,e6b948a6000d..000000000000
--- a/include/linux/cpumask.h
+++ b/include/linux/cpumask.h
@@@ -875,6 -873,82 +885,85 @@@ static inline const struct cpumask *get
  	return to_cpumask(p);
  }
  
++<<<<<<< HEAD
++=======
+ #if NR_CPUS > 1
+ /**
+  * num_online_cpus() - Read the number of online CPUs
+  *
+  * Despite the fact that __num_online_cpus is of type atomic_t, this
+  * interface gives only a momentary snapshot and is not protected against
+  * concurrent CPU hotplug operations unless invoked from a cpuhp_lock held
+  * region.
+  */
+ static inline unsigned int num_online_cpus(void)
+ {
+ 	return atomic_read(&__num_online_cpus);
+ }
+ #define num_possible_cpus()	cpumask_weight(cpu_possible_mask)
+ #define num_present_cpus()	cpumask_weight(cpu_present_mask)
+ #define num_active_cpus()	cpumask_weight(cpu_active_mask)
+ 
+ static inline bool cpu_online(unsigned int cpu)
+ {
+ 	return cpumask_test_cpu(cpu, cpu_online_mask);
+ }
+ 
+ static inline bool cpu_possible(unsigned int cpu)
+ {
+ 	return cpumask_test_cpu(cpu, cpu_possible_mask);
+ }
+ 
+ static inline bool cpu_present(unsigned int cpu)
+ {
+ 	return cpumask_test_cpu(cpu, cpu_present_mask);
+ }
+ 
+ static inline bool cpu_active(unsigned int cpu)
+ {
+ 	return cpumask_test_cpu(cpu, cpu_active_mask);
+ }
+ 
+ static inline bool cpu_dying(unsigned int cpu)
+ {
+ 	return cpumask_test_cpu(cpu, cpu_dying_mask);
+ }
+ 
+ #else
+ 
+ #define num_online_cpus()	1U
+ #define num_possible_cpus()	1U
+ #define num_present_cpus()	1U
+ #define num_active_cpus()	1U
+ 
+ static inline bool cpu_online(unsigned int cpu)
+ {
+ 	return cpu == 0;
+ }
+ 
+ static inline bool cpu_possible(unsigned int cpu)
+ {
+ 	return cpu == 0;
+ }
+ 
+ static inline bool cpu_present(unsigned int cpu)
+ {
+ 	return cpu == 0;
+ }
+ 
+ static inline bool cpu_active(unsigned int cpu)
+ {
+ 	return cpu == 0;
+ }
+ 
+ static inline bool cpu_dying(unsigned int cpu)
+ {
+ 	return false;
+ }
+ 
+ #endif /* NR_CPUS > 1 */
+ 
++>>>>>>> e40f74c535b8 (cpumask: Introduce DYING mask)
  #define cpu_is_offline(cpu)	unlikely(!cpu_online(cpu))
  
  #if NR_CPUS <= BITS_PER_LONG
* Unmerged path include/linux/cpumask.h
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 4f2a7706340a..df6f04312615 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -152,6 +152,9 @@ static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,
 	int (*cb)(unsigned int cpu);
 	int ret, cnt;
 
+	if (cpu_dying(cpu) != !bringup)
+		set_cpu_dying(cpu, !bringup);
+
 	if (st->fail == state) {
 		st->fail = CPUHP_INVALID;
 
@@ -2322,6 +2325,9 @@ EXPORT_SYMBOL(__cpu_present_mask);
 struct cpumask __cpu_active_mask __read_mostly;
 EXPORT_SYMBOL(__cpu_active_mask);
 
+struct cpumask __cpu_dying_mask __read_mostly;
+EXPORT_SYMBOL(__cpu_dying_mask);
+
 atomic_t __num_online_cpus __read_mostly;
 EXPORT_SYMBOL(__num_online_cpus);
 
