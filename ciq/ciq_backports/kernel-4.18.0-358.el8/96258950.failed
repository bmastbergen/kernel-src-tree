x86/fpu: Fix copy_xstate_to_kernel() gap handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 9625895011d130033d1bc7aac0d77a9bf68ff8a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/96258950.failed

The gap handling in copy_xstate_to_kernel() is wrong when XSAVES is in
use.

Using init_fpstate for copying the init state of features which are
not set in the xstate header is only correct for the legacy area, but
not for the extended features area because when XSAVES is in use then
init_fpstate is in compacted form which means the xstate offsets which
are used to copy from init_fpstate are not valid.

Fortunately, this is not a real problem today because all extended
features in use have an all-zeros init state, but it is wrong
nevertheless and with a potentially dynamically sized init_fpstate this
would result in an access outside of the init_fpstate.

Fix this by keeping track of the last copied state in the target buffer and
explicitly zero it when there is a feature or alignment gap.

Use the compacted offset when accessing the extended feature space in
init_fpstate.

As this is not a functional issue on older kernels this is intentionally
not tagged for stable.

Fixes: b8be15d58806 ("x86/fpu/xstate: Re-enable XSAVES")
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20210623121451.294282032@linutronix.de
(cherry picked from commit 9625895011d130033d1bc7aac0d77a9bf68ff8a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/kernel/fpu/xstate.c
index e32ec3bf8b8a,d822ce76f0a1..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1081,32 -1084,10 +1081,39 @@@ static inline bool xfeatures_mxcsr_quir
  	return true;
  }
  
++<<<<<<< HEAD
 +static void fill_gap(unsigned to, void **kbuf, unsigned *pos, unsigned *count)
 +{
 +	if (*pos < to) {
 +		unsigned size = to - *pos;
 +
 +		if (size > *count)
 +			size = *count;
 +		memcpy(*kbuf, (void *)&init_fpstate.xsave + *pos, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
 +}
 +
 +static void copy_part(unsigned offset, unsigned size, void *from,
 +			void **kbuf, unsigned *pos, unsigned *count)
 +{
 +	fill_gap(offset, kbuf, pos, count);
 +	if (size > *count)
 +		size = *count;
 +	if (size) {
 +		memcpy(*kbuf, from, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
++=======
+ static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
+ 			 void *init_xstate, unsigned int size)
+ {
+ 	membuf_write(to, from_xstate ? xstate : init_xstate, size);
++>>>>>>> 9625895011d1 (x86/fpu: Fix copy_xstate_to_kernel() gap handling)
  }
  
  /*
@@@ -1116,19 -1097,14 +1123,25 @@@
   * It supports partial copy but pos always starts from zero. This is called
   * from xstateregs_get() and there we check the CPU has XSAVES.
   */
 -void copy_xstate_to_kernel(struct membuf to, struct xregs_state *xsave)
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
  {
+ 	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
+ 	struct xregs_state *xinit = &init_fpstate.xsave;
  	struct xstate_header header;
++<<<<<<< HEAD
 +	const unsigned off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 +	unsigned count = size_total;
++=======
+ 	unsigned int zerofrom;
++>>>>>>> 9625895011d1 (x86/fpu: Fix copy_xstate_to_kernel() gap handling)
  	int i;
  
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
 +
  	/*
  	 * The destination is a ptrace buffer; we put in only user xstates:
  	 */
@@@ -1136,134 -1112,68 +1149,195 @@@
  	header.xfeatures = xsave->header.xfeatures;
  	header.xfeatures &= xfeatures_mask_user();
  
++<<<<<<< HEAD
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(0, off_mxcsr,
 +			  &xsave->i387, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM))
 +		copy_part(off_mxcsr, MXCSR_AND_FLAGS_SIZE,
 +			  &xsave->i387.mxcsr, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(offsetof(struct fxregs_state, st_space), 128,
 +			  &xsave->i387.st_space, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_SSE)
 +		copy_part(xstate_offsets[XFEATURE_SSE], 256,
 +			  &xsave->i387.xmm_space, &kbuf, &offset_start, &count);
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	copy_part(offsetof(struct fxregs_state, sw_reserved), 48,
 +		  xstate_fx_sw_bytes, &kbuf, &offset_start, &count);
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	copy_part(offsetof(struct xregs_state, header), sizeof(header),
 +		  &header, &kbuf, &offset_start, &count);
++=======
+ 	/* Copy FP state up to MXCSR */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_FP, &to, &xsave->i387,
+ 		     &xinit->i387, off_mxcsr);
+ 
+ 	/* Copy MXCSR when SSE or YMM are set in the feature mask */
+ 	copy_feature(header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM),
+ 		     &to, &xsave->i387.mxcsr, &xinit->i387.mxcsr,
+ 		     MXCSR_AND_FLAGS_SIZE);
+ 
+ 	/* Copy the remaining FP state */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_FP,
+ 		     &to, &xsave->i387.st_space, &xinit->i387.st_space,
+ 		     sizeof(xsave->i387.st_space));
+ 
+ 	/* Copy the SSE state - shared with YMM, but independently managed */
+ 	copy_feature(header.xfeatures & XFEATURE_MASK_SSE,
+ 		     &to, &xsave->i387.xmm_space, &xinit->i387.xmm_space,
+ 		     sizeof(xsave->i387.xmm_space));
+ 
+ 	/* Zero the padding area */
+ 	membuf_zero(&to, sizeof(xsave->i387.padding));
+ 
+ 	/* Copy xsave->i387.sw_reserved */
+ 	membuf_write(&to, xstate_fx_sw_bytes, sizeof(xsave->i387.sw_reserved));
+ 
+ 	/* Copy the user space relevant state of @xsave->header */
+ 	membuf_write(&to, &header, sizeof(header));
+ 
+ 	zerofrom = offsetof(struct xregs_state, extended_state_area);
++>>>>>>> 9625895011d1 (x86/fpu: Fix copy_xstate_to_kernel() gap handling)
  
  	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
  		/*
- 		 * Copy only in-use xstates:
+ 		 * The ptrace buffer is in non-compacted XSAVE format.
+ 		 * In non-compacted format disabled features still occupy
+ 		 * state space, but there is no state to copy from in the
+ 		 * compacted init_fpstate. The gap tracking will zero this
+ 		 * later.
  		 */
- 		if ((header.xfeatures >> i) & 1) {
- 			void *src = __raw_xsave_addr(xsave, i);
+ 		if (!(xfeatures_mask_user() & BIT_ULL(i)))
+ 			continue;
  
++<<<<<<< HEAD
 +			copy_part(xstate_offsets[i], xstate_sizes[i],
 +				  src, &kbuf, &offset_start, &count);
 +		}
++=======
+ 		/*
+ 		 * If there was a feature or alignment gap, zero the space
+ 		 * in the destination buffer.
+ 		 */
+ 		if (zerofrom < xstate_offsets[i])
+ 			membuf_zero(&to, xstate_offsets[i] - zerofrom);
++>>>>>>> 9625895011d1 (x86/fpu: Fix copy_xstate_to_kernel() gap handling)
  
+ 		copy_feature(header.xfeatures & BIT_ULL(i), &to,
+ 			     __raw_xsave_addr(xsave, i),
+ 			     __raw_xsave_addr(xinit, i),
+ 			     xstate_sizes[i]);
+ 
+ 		/*
+ 		 * Keep track of the last copied state in the non-compacted
+ 		 * target buffer for gap zeroing.
+ 		 */
+ 		zerofrom = xstate_offsets[i] + xstate_sizes[i];
  	}
++<<<<<<< HEAD
 +	fill_gap(size_total, &kbuf, &offset_start, &count);
 +
 +	return 0;
 +}
 +
 +static inline int
 +__copy_xstate_to_user(void __user *ubuf, const void *data, unsigned int offset, unsigned int size, unsigned int size_total)
 +{
 +	if (!size)
 +		return 0;
 +
 +	if (offset < size_total) {
 +		unsigned int copy = min(size, size_total - offset);
 +
 +		if (__copy_to_user(ubuf + offset, data, copy))
 +			return -EFAULT;
 +	}
 +	return 0;
 +}
 +
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a user-space buffer. It supports partial copy but pos always starts from
 + * zero. This is called from xstateregs_get() and there we check the CPU
 + * has XSAVES.
 + */
 +int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
 +{
 +	unsigned int offset, size;
 +	int ret, i;
 +	struct xstate_header header;
 +
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
 +
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
 +	header.xfeatures = xsave->header.xfeatures;
 +	header.xfeatures &= xfeatures_mask_user();
 +
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	offset = offsetof(struct xregs_state, header);
 +	size = sizeof(header);
 +
 +	ret = __copy_xstate_to_user(ubuf, &header, offset, size, size_total);
 +	if (ret)
 +		return ret;
 +
 +	for (i = 0; i < XFEATURE_MAX; i++) {
 +		/*
 +		 * Copy only in-use xstates:
 +		 */
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
 +
 +			offset = xstate_offsets[i];
 +			size = xstate_sizes[i];
 +
 +			/* The next component has to fit fully into the output buffer: */
 +			if (offset + size > size_total)
 +				break;
 +
 +			ret = __copy_xstate_to_user(ubuf, src, offset, size, size_total);
 +			if (ret)
 +				return ret;
 +		}
 +
 +	}
 +
 +	if (xfeatures_mxcsr_quirk(header.xfeatures)) {
 +		offset = offsetof(struct fxregs_state, mxcsr);
 +		size = MXCSR_AND_FLAGS_SIZE;
 +		__copy_xstate_to_user(ubuf, &xsave->i387.mxcsr, offset, size, size_total);
 +	}
 +
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	offset = offsetof(struct fxregs_state, sw_reserved);
 +	size = sizeof(xstate_fx_sw_bytes);
 +
 +	ret = __copy_xstate_to_user(ubuf, xstate_fx_sw_bytes, offset, size, size_total);
 +	if (ret)
 +		return ret;
 +
 +	return 0;
++=======
+ 
+ 	if (to.left)
+ 		membuf_zero(&to, to.left);
++>>>>>>> 9625895011d1 (x86/fpu: Fix copy_xstate_to_kernel() gap handling)
  }
  
  /*
* Unmerged path arch/x86/kernel/fpu/xstate.c
