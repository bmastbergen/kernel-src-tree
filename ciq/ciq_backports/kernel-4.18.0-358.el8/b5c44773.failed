sched: Use cpu_dying() to fix balance_push vs hotplug-rollback

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit b5c4477366fb5e6a2f0f38742c33acd666c07698
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/b5c44773.failed

Use the new cpu_dying() state to simplify and fix the balance_push()
vs CPU hotplug rollback state.

Specifically, we currently rely on notifiers sched_cpu_dying() /
sched_cpu_activate() to terminate balance_push, however if the
cpu_down() fails when we're past sched_cpu_deactivate(), it should
terminate balance_push at that point and not wait until we hit
sched_cpu_activate().

Similarly, when cpu_up() fails and we're going back down, balance_push
should be active, where it currently is not.

So instead, make sure balance_push is enabled below SCHED_AP_ACTIVE
(when !cpu_active()), and gate it's utility with cpu_dying().

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
Link: https://lkml.kernel.org/r/YHgAYef83VQhKdC2@hirez.programming.kicks-ass.net
(cherry picked from commit b5c4477366fb5e6a2f0f38742c33acd666c07698)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 7d10e0df3c83,7d031da20df3..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1047,10 -1797,25 +1047,27 @@@ static inline bool is_cpu_allowed(struc
  	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
  		return false;
  
 -	/* migrate_disabled() must be allowed to finish. */
 -	if (is_migration_disabled(p))
 +	if (is_per_cpu_kthread(p))
  		return cpu_online(cpu);
  
++<<<<<<< HEAD
 +	return cpu_active(cpu);
++=======
+ 	/* Non kernel threads are not allowed during either online or offline. */
+ 	if (!(p->flags & PF_KTHREAD))
+ 		return cpu_active(cpu);
+ 
+ 	/* KTHREAD_IS_PER_CPU is always allowed. */
+ 	if (kthread_is_per_cpu(p))
+ 		return cpu_online(cpu);
+ 
+ 	/* Regular kernel threads don't get to stay during offline. */
+ 	if (cpu_dying(cpu))
+ 		return false;
+ 
+ 	/* But are allowed during online. */
+ 	return cpu_online(cpu);
++>>>>>>> b5c4477366fb (sched: Use cpu_dying() to fix balance_push vs hotplug-rollback)
  }
  
  /*
@@@ -6069,119 -7609,148 +6086,214 @@@ void idle_task_exit(void
  	/* finish_cpu(), as ran on the BP, will clean up the active_mm state */
  }
  
 -static int __balance_push_cpu_stop(void *arg)
 +/*
 + * Since this CPU is going 'away' for a while, fold any nr_active delta
 + * we might have. Assumes we're called after migrate_tasks() so that the
 + * nr_active count is stable. We need to take the teardown thread which
 + * is calling this into account, so we hand in adjust = 1 to the load
 + * calculation.
 + *
 + * Also see the comment "Global load-average calculations".
 + */
 +static void calc_load_migrate(struct rq *rq)
  {
 -	struct task_struct *p = arg;
 -	struct rq *rq = this_rq();
 -	struct rq_flags rf;
 -	int cpu;
 +	long delta = calc_load_fold_active(rq, 1);
 +	if (delta)
 +		atomic_long_add(delta, &calc_load_tasks);
 +}
  
 -	raw_spin_lock_irq(&p->pi_lock);
 -	rq_lock(rq, &rf);
 +static struct task_struct *__pick_migrate_task(struct rq *rq)
 +{
 +	const struct sched_class *class;
 +	struct task_struct *next;
 +
 +	for_each_class(class) {
 +		next = class->pick_next_task(rq);
 +		if (next) {
 +			next->sched_class->put_prev_task(rq, next);
 +			return next;
 +		}
 +	}
 +
 +	/* The idle class should always have a runnable task */
 +	BUG();
 +}
 +
 +/*
 + * Migrate all tasks from the rq, sleeping tasks will be migrated by
 + * try_to_wake_up()->select_task_rq().
 + *
 + * Called with rq->lock held even though we'er in stop_machine() and
 + * there's no concurrency possible, we hold the required locks anyway
 + * because of lock validation efforts.
 + */
 +static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf)
 +{
 +	struct rq *rq = dead_rq;
 +	struct task_struct *next, *stop = rq->stop;
 +	struct rq_flags orf = *rf;
 +	int dest_cpu;
  
 +	/*
 +	 * Fudge the rq selection such that the below task selection loop
 +	 * doesn't get stuck on the currently eligible stop task.
 +	 *
 +	 * We're currently inside stop_machine() and the rq is either stuck
 +	 * in the stop_machine_cpu_stop() loop, or we're executing this code,
 +	 * either way we should never end up calling schedule() until we're
 +	 * done here.
 +	 */
 +	rq->stop = NULL;
 +
 +	/*
 +	 * put_prev_task() and pick_next_task() sched
 +	 * class method both need to have an up-to-date
 +	 * value of rq->clock[_task]
 +	 */
  	update_rq_clock(rq);
  
 -	if (task_rq(p) == rq && task_on_rq_queued(p)) {
 -		cpu = select_fallback_rq(rq->cpu, p);
 -		rq = __migrate_task(rq, &rf, p, cpu);
 -	}
 +	for (;;) {
 +		/*
 +		 * There's this thread running, bail when that's the only
 +		 * remaining thread:
 +		 */
 +		if (rq->nr_running == 1)
 +			break;
  
++<<<<<<< HEAD
 +		next = __pick_migrate_task(rq);
++=======
+ 	rq_unlock(rq, &rf);
+ 	raw_spin_unlock_irq(&p->pi_lock);
+ 
+ 	put_task_struct(p);
+ 
+ 	return 0;
+ }
+ 
+ static DEFINE_PER_CPU(struct cpu_stop_work, push_work);
+ 
+ /*
+  * Ensure we only run per-cpu kthreads once the CPU goes !active.
+  *
+  * This is enabled below SCHED_AP_ACTIVE; when !cpu_active(), but only
+  * effective when the hotplug motion is down.
+  */
+ static void balance_push(struct rq *rq)
+ {
+ 	struct task_struct *push_task = rq->curr;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 	SCHED_WARN_ON(rq->cpu != smp_processor_id());
+ 
+ 	/*
+ 	 * Ensure the thing is persistent until balance_push_set(.on = false);
+ 	 */
+ 	rq->balance_callback = &balance_push_callback;
+ 
+ 	/*
+ 	 * Only active while going offline.
+ 	 */
+ 	if (!cpu_dying(rq->cpu))
+ 		return;
+ 
+ 	/*
+ 	 * Both the cpu-hotplug and stop task are in this case and are
+ 	 * required to complete the hotplug process.
+ 	 *
+ 	 * XXX: the idle task does not match kthread_is_per_cpu() due to
+ 	 * histerical raisins.
+ 	 */
+ 	if (rq->idle == push_task ||
+ 	    ((push_task->flags & PF_KTHREAD) && kthread_is_per_cpu(push_task)) ||
+ 	    is_migration_disabled(push_task)) {
++>>>>>>> b5c4477366fb (sched: Use cpu_dying() to fix balance_push vs hotplug-rollback)
  
  		/*
 -		 * If this is the idle task on the outgoing CPU try to wake
 -		 * up the hotplug control thread which might wait for the
 -		 * last task to vanish. The rcuwait_active() check is
 -		 * accurate here because the waiter is pinned on this CPU
 -		 * and can't obviously be running in parallel.
 +		 * Rules for changing task_struct::cpus_mask are holding
 +		 * both pi_lock and rq->lock, such that holding either
 +		 * stabilizes the mask.
  		 *
 -		 * On RT kernels this also has to check whether there are
 -		 * pinned and scheduled out tasks on the runqueue. They
 -		 * need to leave the migrate disabled section first.
 +		 * Drop rq->lock is not quite as disastrous as it usually is
 +		 * because !cpu_active at this point, which means load-balance
 +		 * will not interfere. Also, stop-machine.
 +		 */
 +		rq_unlock(rq, rf);
 +		raw_spin_lock(&next->pi_lock);
 +		rq_relock(rq, rf);
 +
 +		/*
 +		 * Since we're inside stop-machine, _nothing_ should have
 +		 * changed the task, WARN if weird stuff happened, because in
 +		 * that case the above rq->lock drop is a fail too.
  		 */
 -		if (!rq->nr_running && !rq_has_pinned_tasks(rq) &&
 -		    rcuwait_active(&rq->hotplug_wait)) {
 -			raw_spin_unlock(&rq->lock);
 -			rcuwait_wake_up(&rq->hotplug_wait);
 -			raw_spin_lock(&rq->lock);
 +		if (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) {
 +			raw_spin_unlock(&next->pi_lock);
 +			continue;
  		}
 -		return;
 +
 +		/* Find suitable destination for @next, with force if needed. */
 +		dest_cpu = select_fallback_rq(dead_rq->cpu, next);
 +		rq = __migrate_task(rq, rf, next, dest_cpu);
 +		if (rq != dead_rq) {
 +			rq_unlock(rq, rf);
 +			rq = dead_rq;
 +			*rf = orf;
 +			rq_relock(rq, rf);
 +		}
 +		raw_spin_unlock(&next->pi_lock);
  	}
  
 -	get_task_struct(push_task);
 -	/*
 -	 * Temporarily drop rq->lock such that we can wake-up the stop task.
 -	 * Both preemption and IRQs are still disabled.
 -	 */
 -	raw_spin_unlock(&rq->lock);
 -	stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
 -			    this_cpu_ptr(&push_work));
 -	/*
 -	 * At this point need_resched() is true and we'll take the loop in
 -	 * schedule(). The next pick is obviously going to be the stop task
 -	 * which kthread_is_per_cpu() and will push this task away.
 -	 */
 -	raw_spin_lock(&rq->lock);
 +	rq->stop = stop;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void balance_push_set(int cpu, bool on)
+ {
+ 	struct rq *rq = cpu_rq(cpu);
+ 	struct rq_flags rf;
+ 
+ 	rq_lock_irqsave(rq, &rf);
+ 	if (on) {
+ 		WARN_ON_ONCE(rq->balance_callback);
+ 		rq->balance_callback = &balance_push_callback;
+ 	} else if (rq->balance_callback == &balance_push_callback) {
+ 		rq->balance_callback = NULL;
+ 	}
+ 	rq_unlock_irqrestore(rq, &rf);
+ }
+ 
+ /*
+  * Invoked from a CPUs hotplug control thread after the CPU has been marked
+  * inactive. All tasks which are not per CPU kernel threads are either
+  * pushed off this CPU now via balance_push() or placed on a different CPU
+  * during wakeup. Wait until the CPU is quiescent.
+  */
+ static void balance_hotplug_wait(void)
+ {
+ 	struct rq *rq = this_rq();
+ 
+ 	rcuwait_wait_event(&rq->hotplug_wait,
+ 			   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),
+ 			   TASK_UNINTERRUPTIBLE);
+ }
+ 
+ #else
+ 
+ static inline void balance_push(struct rq *rq)
+ {
+ }
+ 
+ static inline void balance_push_set(int cpu, bool on)
+ {
+ }
+ 
+ static inline void balance_hotplug_wait(void)
+ {
+ }
+ 
++>>>>>>> b5c4477366fb (sched: Use cpu_dying() to fix balance_push vs hotplug-rollback)
  #endif /* CONFIG_HOTPLUG_CPU */
  
  void set_rq_online(struct rq *rq)
@@@ -6267,6 -7836,12 +6379,15 @@@ int sched_cpu_activate(unsigned int cpu
  	struct rq *rq = cpu_rq(cpu);
  	struct rq_flags rf;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Clear the balance_push callback and prepare to schedule
+ 	 * regular tasks.
+ 	 */
+ 	balance_push_set(cpu, false);
+ 
++>>>>>>> b5c4477366fb (sched: Use cpu_dying() to fix balance_push vs hotplug-rollback)
  #ifdef CONFIG_SCHED_SMT
  	/*
  	 * When going up, increment the number of cores with SMT present.
* Unmerged path kernel/sched/core.c
