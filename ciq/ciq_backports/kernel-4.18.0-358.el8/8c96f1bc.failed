mm/kmemleak: turn kmemleak_lock and object->lock to raw_spinlock_t

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author He Zhe <zhe.he@windriver.com>
commit 8c96f1bc6fc49c724c4cdd22d3e99260263b7384
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/8c96f1bc.failed

kmemleak_lock as a rwlock on RT can possibly be acquired in atomic
context which does work.

Since the kmemleak operation is performed in atomic context make it a
raw_spinlock_t so it can also be acquired on RT.  This is used for
debugging and is not enabled by default in a production like environment
(where performance/latency matters) so it makes sense to make it a
raw_spinlock_t instead trying to get rid of the atomic context.  Turn
also the kmemleak_object->lock into raw_spinlock_t which is acquired
(nested) while the kmemleak_lock is held.

The time spent in "echo scan > kmemleak" slightly improved on 64core box
with this patch applied after boot.

[bigeasy@linutronix.de: redo the description, update comments. Merge the individual bits:  He Zhe did the kmemleak_lock, Liu Haitao the ->lock and Yongxin Liu forwarded Liu's patch.]
Link: http://lkml.kernel.org/r/20191219170834.4tah3prf2gdothz4@linutronix.de
Link: https://lkml.kernel.org/r/20181218150744.GB20197@arrakis.emea.arm.com
Link: https://lkml.kernel.org/r/1542877459-144382-1-git-send-email-zhe.he@windriver.com
Link: https://lkml.kernel.org/r/20190927082230.34152-1-yongxin.liu@windriver.com
	Signed-off-by: He Zhe <zhe.he@windriver.com>
	Signed-off-by: Liu Haitao <haitao.liu@windriver.com>
	Signed-off-by: Yongxin Liu <yongxin.liu@windriver.com>
	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8c96f1bc6fc49c724c4cdd22d3e99260263b7384)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/kmemleak.c
diff --cc mm/kmemleak.c
index d3d254bb7371,3a4259eeb5a0..000000000000
--- a/mm/kmemleak.c
+++ b/mm/kmemleak.c
@@@ -479,19 -419,23 +479,25 @@@ static struct kmemleak_object *mem_pool
  	struct kmemleak_object *object;
  
  	/* try the slab allocator first */
 -	if (object_cache) {
 -		object = kmem_cache_alloc(object_cache, gfp_kmemleak_mask(gfp));
 -		if (object)
 -			return object;
 -	}
 +	object = kmem_cache_alloc(object_cache, gfp_kmemleak_mask(gfp));
 +	if (object)
 +		return object;
  
  	/* slab allocation failed, try the memory pool */
- 	write_lock_irqsave(&kmemleak_lock, flags);
+ 	raw_spin_lock_irqsave(&kmemleak_lock, flags);
  	object = list_first_entry_or_null(&mem_pool_free_list,
  					  typeof(*object), object_list);
  	if (object)
  		list_del(&object->object_list);
  	else if (mem_pool_free_count)
  		object = &mem_pool[--mem_pool_free_count];
++<<<<<<< HEAD
 +	write_unlock_irqrestore(&kmemleak_lock, flags);
++=======
+ 	else
+ 		pr_warn_once("Memory pool empty, consider increasing CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE\n");
+ 	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
++>>>>>>> 8c96f1bc6fc4 (mm/kmemleak: turn kmemleak_lock and object->lock to raw_spinlock_t)
  
  	return object;
  }
@@@ -845,9 -795,10 +851,9 @@@ static void add_scan_area(unsigned lon
  		return;
  	}
  
 -	if (scan_area_cache)
 -		area = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));
 +	area = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));
  
- 	spin_lock_irqsave(&object->lock, flags);
+ 	raw_spin_lock_irqsave(&object->lock, flags);
  	if (!area) {
  		pr_warn_once("Cannot allocate a scan area, scanning the full object\n");
  		/* mark the object for full scan to avoid false positives */
* Unmerged path mm/kmemleak.c
