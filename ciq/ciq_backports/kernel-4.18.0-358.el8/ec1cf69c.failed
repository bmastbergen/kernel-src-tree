KVM: X86: Add per-vm stat for max rmap list size

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Peter Xu <peterx@redhat.com>
commit ec1cf69c376970f42761e641cf5074b84f8db243
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/ec1cf69c.failed

Add a new statistic max_mmu_rmap_size, which stores the maximum size of rmap
for the vm.

	Signed-off-by: Peter Xu <peterx@redhat.com>
Message-Id: <20210625153214.43106-2-peterx@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ec1cf69c376970f42761e641cf5074b84f8db243)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index 9eecaf6a8fa7,ab200f2cb1f0..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -214,51 -224,71 +214,119 @@@ EXPORT_SYMBOL_GPL(host_xss)
  u64 __read_mostly supported_xss;
  EXPORT_SYMBOL_GPL(supported_xss);
  
++<<<<<<< HEAD
 +struct kvm_stats_debugfs_item debugfs_entries[] = {
 +	VCPU_STAT("pf_fixed", pf_fixed),
 +	VCPU_STAT("pf_guest", pf_guest),
 +	VCPU_STAT("tlb_flush", tlb_flush),
 +	VCPU_STAT("invlpg", invlpg),
 +	VCPU_STAT("exits", exits),
 +	VCPU_STAT("io_exits", io_exits),
 +	VCPU_STAT("mmio_exits", mmio_exits),
 +	VCPU_STAT("signal_exits", signal_exits),
 +	VCPU_STAT("irq_window", irq_window_exits),
 +	VCPU_STAT("nmi_window", nmi_window_exits),
 +	VCPU_STAT("halt_exits", halt_exits),
 +	VCPU_STAT("halt_successful_poll", halt_successful_poll),
 +	VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
 +	VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
 +	VCPU_STAT("halt_wakeup", halt_wakeup),
 +	VCPU_STAT("hypercalls", hypercalls),
 +	VCPU_STAT("request_irq", request_irq_exits),
 +	VCPU_STAT("irq_exits", irq_exits),
 +	VCPU_STAT("host_state_reload", host_state_reload),
 +	VCPU_STAT("fpu_reload", fpu_reload),
 +	VCPU_STAT("insn_emulation", insn_emulation),
 +	VCPU_STAT("insn_emulation_fail", insn_emulation_fail),
 +	VCPU_STAT("irq_injections", irq_injections),
 +	VCPU_STAT("nmi_injections", nmi_injections),
 +	VCPU_STAT("req_event", req_event),
 +	VCPU_STAT("l1d_flush", l1d_flush),
 +	VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
 +	VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
 +	VCPU_STAT("nested_run", nested_run),
 +	VCPU_STAT("directed_yield_attempted", directed_yield_attempted),
 +	VCPU_STAT("directed_yield_successful", directed_yield_successful),
 +	VCPU_STAT("guest_mode", guest_mode),
 +	VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
 +	VM_STAT("mmu_pte_write", mmu_pte_write),
 +	VM_STAT("mmu_pde_zapped", mmu_pde_zapped),
 +	VM_STAT("mmu_flooded", mmu_flooded),
 +	VM_STAT("mmu_recycled", mmu_recycled),
 +	VM_STAT("mmu_cache_miss", mmu_cache_miss),
 +	VM_STAT("mmu_unsync", mmu_unsync),
 +	VM_STAT("remote_tlb_flush", remote_tlb_flush),
 +	VM_STAT("largepages", lpages, .mode = 0444),
 +	VM_STAT("nx_largepages_splitted", nx_lpage_splits, .mode = 0444),
 +	VM_STAT("max_mmu_page_hash_collisions", max_mmu_page_hash_collisions),
 +	{ NULL }
++=======
+ const struct _kvm_stats_desc kvm_vm_stats_desc[] = {
+ 	KVM_GENERIC_VM_STATS(),
+ 	STATS_DESC_COUNTER(VM, mmu_shadow_zapped),
+ 	STATS_DESC_COUNTER(VM, mmu_pte_write),
+ 	STATS_DESC_COUNTER(VM, mmu_pde_zapped),
+ 	STATS_DESC_COUNTER(VM, mmu_flooded),
+ 	STATS_DESC_COUNTER(VM, mmu_recycled),
+ 	STATS_DESC_COUNTER(VM, mmu_cache_miss),
+ 	STATS_DESC_ICOUNTER(VM, mmu_unsync),
+ 	STATS_DESC_ICOUNTER(VM, lpages),
+ 	STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+ 	STATS_DESC_PCOUNTER(VM, max_mmu_rmap_size),
+ 	STATS_DESC_PCOUNTER(VM, max_mmu_page_hash_collisions)
+ };
+ static_assert(ARRAY_SIZE(kvm_vm_stats_desc) ==
+ 		sizeof(struct kvm_vm_stat) / sizeof(u64));
+ 
+ const struct kvm_stats_header kvm_vm_stats_header = {
+ 	.name_size = KVM_STATS_NAME_SIZE,
+ 	.num_desc = ARRAY_SIZE(kvm_vm_stats_desc),
+ 	.id_offset = sizeof(struct kvm_stats_header),
+ 	.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,
+ 	.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +
+ 		       sizeof(kvm_vm_stats_desc),
+ };
+ 
+ const struct _kvm_stats_desc kvm_vcpu_stats_desc[] = {
+ 	KVM_GENERIC_VCPU_STATS(),
+ 	STATS_DESC_COUNTER(VCPU, pf_fixed),
+ 	STATS_DESC_COUNTER(VCPU, pf_guest),
+ 	STATS_DESC_COUNTER(VCPU, tlb_flush),
+ 	STATS_DESC_COUNTER(VCPU, invlpg),
+ 	STATS_DESC_COUNTER(VCPU, exits),
+ 	STATS_DESC_COUNTER(VCPU, io_exits),
+ 	STATS_DESC_COUNTER(VCPU, mmio_exits),
+ 	STATS_DESC_COUNTER(VCPU, signal_exits),
+ 	STATS_DESC_COUNTER(VCPU, irq_window_exits),
+ 	STATS_DESC_COUNTER(VCPU, nmi_window_exits),
+ 	STATS_DESC_COUNTER(VCPU, l1d_flush),
+ 	STATS_DESC_COUNTER(VCPU, halt_exits),
+ 	STATS_DESC_COUNTER(VCPU, request_irq_exits),
+ 	STATS_DESC_COUNTER(VCPU, irq_exits),
+ 	STATS_DESC_COUNTER(VCPU, host_state_reload),
+ 	STATS_DESC_COUNTER(VCPU, fpu_reload),
+ 	STATS_DESC_COUNTER(VCPU, insn_emulation),
+ 	STATS_DESC_COUNTER(VCPU, insn_emulation_fail),
+ 	STATS_DESC_COUNTER(VCPU, hypercalls),
+ 	STATS_DESC_COUNTER(VCPU, irq_injections),
+ 	STATS_DESC_COUNTER(VCPU, nmi_injections),
+ 	STATS_DESC_COUNTER(VCPU, req_event),
+ 	STATS_DESC_COUNTER(VCPU, nested_run),
+ 	STATS_DESC_COUNTER(VCPU, directed_yield_attempted),
+ 	STATS_DESC_COUNTER(VCPU, directed_yield_successful),
+ 	STATS_DESC_ICOUNTER(VCPU, guest_mode)
+ };
+ static_assert(ARRAY_SIZE(kvm_vcpu_stats_desc) ==
+ 		sizeof(struct kvm_vcpu_stat) / sizeof(u64));
+ 
+ const struct kvm_stats_header kvm_vcpu_stats_header = {
+ 	.name_size = KVM_STATS_NAME_SIZE,
+ 	.num_desc = ARRAY_SIZE(kvm_vcpu_stats_desc),
+ 	.id_offset = sizeof(struct kvm_stats_header),
+ 	.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,
+ 	.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +
+ 		       sizeof(kvm_vcpu_stats_desc),
++>>>>>>> ec1cf69c3769 (KVM: X86: Add per-vm stat for max rmap list size)
  };
  
  u64 __read_mostly host_xcr0;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d0906079b8f5..8ac293f8d82e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1190,6 +1190,7 @@ struct kvm_vm_stat {
 	u64 lpages;
 	u64 nx_lpage_splits;
 	u64 max_mmu_page_hash_collisions;
+	u64 max_mmu_rmap_size;
 };
 
 struct kvm_vcpu_stat {
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 799107c112fc..4a9f27f13a50 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -2717,6 +2717,8 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 	if (is_shadow_present_pte(*sptep)) {
 		if (!was_rmapped) {
 			rmap_count = rmap_add(vcpu, sptep, gfn);
+			if (rmap_count > vcpu->kvm->stat.max_mmu_rmap_size)
+				vcpu->kvm->stat.max_mmu_rmap_size = rmap_count;
 			if (rmap_count > RMAP_RECYCLE_THRESHOLD)
 				rmap_recycle(vcpu, sptep, gfn);
 		}
* Unmerged path arch/x86/kvm/x86.c
