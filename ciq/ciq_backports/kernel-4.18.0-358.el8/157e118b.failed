x86/mm/highmem: Use generic kmap atomic implementation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 157e118b55113d1e6c7f8ddfcec0a1dbf3a69511
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/157e118b.failed

Convert X86 to the generic kmap atomic implementation and make the
iomap_atomic() naming convention consistent while at it.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/r/20201103095857.375127260@linutronix.de

(cherry picked from commit 157e118b55113d1e6c7f8ddfcec0a1dbf3a69511)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/include/asm/fixmap.h
#	arch/x86/include/asm/highmem.h
#	arch/x86/mm/highmem_32.c
#	arch/x86/mm/init_32.c
#	include/linux/highmem.h
#	include/linux/io-mapping.h
#	mm/highmem.c
diff --cc arch/x86/Kconfig
index bd9df7cf6689,33c273cb3023..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -14,12 -14,11 +14,16 @@@ config X86_3
  	select ARCH_WANT_IPC_PARSE_VERSION
  	select CLKSRC_I8253
  	select CLONE_BACKWARDS
++<<<<<<< HEAD
 +	select HAVE_AOUT
 +	select HAVE_GENERIC_DMA_COHERENT
++=======
+ 	select GENERIC_VDSO_32
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  	select HAVE_DEBUG_STACKOVERFLOW
+ 	select KMAP_LOCAL
  	select MODULES_USE_ELF_REL
  	select OLD_SIGACTION
- 	select GENERIC_VDSO_32
  
  config X86_64
  	def_bool y
diff --cc arch/x86/include/asm/fixmap.h
index 3ea818f86acf,8eba66a33e39..000000000000
--- a/arch/x86/include/asm/fixmap.h
+++ b/arch/x86/include/asm/fixmap.h
@@@ -25,15 -25,13 +25,15 @@@
  #define FIXMAP_PMD_TOP	507
  
  #ifndef __ASSEMBLY__
 +#include <linux/rh_kabi.h>
  #include <linux/kernel.h>
 +#include RH_KABI_FAKE_INCLUDE(<asm/acpi.h>)
  #include <asm/apicdef.h>
  #include <asm/page.h>
 -#include <asm/pgtable_types.h>
 +#include RH_KABI_HIDE_INCLUDE(<asm/pgtable_types.h>)
  #ifdef CONFIG_X86_32
  #include <linux/threads.h>
- #include <asm/kmap_types.h>
+ #include <asm/kmap_size.h>
  #else
  #include <uapi/asm/vsyscall.h>
  #endif
@@@ -156,8 -151,6 +156,11 @@@ extern void reserve_top_address(unsigne
  
  extern int fixmaps_set;
  
++<<<<<<< HEAD
 +extern pte_t *kmap_pte;
 +#define kmap_prot PAGE_KERNEL
++=======
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  extern pte_t *pkmap_page_table;
  
  void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
diff --cc arch/x86/include/asm/highmem.h
index a8059930056d,032e020853aa..000000000000
--- a/arch/x86/include/asm/highmem.h
+++ b/arch/x86/include/asm/highmem.h
@@@ -58,20 -57,17 +57,32 @@@ extern unsigned long highstart_pfn, hig
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))
  
++<<<<<<< HEAD
 +extern void *kmap_high(struct page *page);
 +extern void kunmap_high(struct page *page);
 +
 +void *kmap(struct page *page);
 +void kunmap(struct page *page);
 +
 +void *kmap_atomic_prot(struct page *page, pgprot_t prot);
 +void *kmap_atomic(struct page *page);
 +void __kunmap_atomic(void *kvaddr);
 +void *kmap_atomic_pfn(unsigned long pfn);
 +void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
 +
++=======
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  #define flush_cache_kmaps()	do { } while (0)
  
+ #define	arch_kmap_local_post_map(vaddr, pteval)		\
+ 	arch_flush_lazy_mmu_mode()
+ 
+ #define	arch_kmap_local_post_unmap(vaddr)		\
+ 	do {						\
+ 		flush_tlb_one_kernel((vaddr));		\
+ 		arch_flush_lazy_mmu_mode();		\
+ 	} while (0)
+ 
  extern void add_highpages_with_active_regions(int nid, unsigned long start_pfn,
  					unsigned long end_pfn);
  
diff --cc arch/x86/mm/highmem_32.c
index 0d4bdcb84da5,2c54b76d8f84..000000000000
--- a/arch/x86/mm/highmem_32.c
+++ b/arch/x86/mm/highmem_32.c
@@@ -3,107 -4,6 +3,110 @@@
  #include <linux/swap.h> /* for totalram_pages */
  #include <linux/memblock.h>
  
++<<<<<<< HEAD
 +void *kmap(struct page *page)
 +{
 +	might_sleep();
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +	return kmap_high(page);
 +}
 +EXPORT_SYMBOL(kmap);
 +
 +void kunmap(struct page *page)
 +{
 +	if (in_interrupt())
 +		BUG();
 +	if (!PageHighMem(page))
 +		return;
 +	kunmap_high(page);
 +}
 +EXPORT_SYMBOL(kunmap);
 +
 +/*
 + * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
 + * no global lock is needed and because the kmap code must perform a global TLB
 + * invalidation when the kmap pool wraps.
 + *
 + * However when holding an atomic kmap it is not legal to sleep, so atomic
 + * kmaps are appropriate for short, tight code paths only.
 + */
 +void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 +{
 +	unsigned long vaddr;
 +	int idx, type;
 +
 +	preempt_disable();
 +	pagefault_disable();
 +
 +	if (!PageHighMem(page))
 +		return page_address(page);
 +
 +	type = kmap_atomic_idx_push();
 +	idx = type + KM_TYPE_NR*smp_processor_id();
 +	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 +	BUG_ON(!pte_none(*(kmap_pte-idx)));
 +	set_pte(kmap_pte-idx, mk_pte(page, prot));
 +	arch_flush_lazy_mmu_mode();
 +
 +	return (void *)vaddr;
 +}
 +EXPORT_SYMBOL(kmap_atomic_prot);
 +
 +void *kmap_atomic(struct page *page)
 +{
 +	return kmap_atomic_prot(page, kmap_prot);
 +}
 +EXPORT_SYMBOL(kmap_atomic);
 +
 +/*
 + * This is the same as kmap_atomic() but can map memory that doesn't
 + * have a struct page associated with it.
 + */
 +void *kmap_atomic_pfn(unsigned long pfn)
 +{
 +	return kmap_atomic_prot_pfn(pfn, kmap_prot);
 +}
 +EXPORT_SYMBOL_GPL(kmap_atomic_pfn);
 +
 +void __kunmap_atomic(void *kvaddr)
 +{
 +	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 +
 +	if (vaddr >= __fix_to_virt(FIX_KMAP_END) &&
 +	    vaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {
 +		int idx, type;
 +
 +		type = kmap_atomic_idx();
 +		idx = type + KM_TYPE_NR * smp_processor_id();
 +
 +#ifdef CONFIG_DEBUG_HIGHMEM
 +		WARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
 +#endif
 +		/*
 +		 * Force other mappings to Oops if they'll try to access this
 +		 * pte without first remap it.  Keeping stale mappings around
 +		 * is a bad idea also, in case the page changes cacheability
 +		 * attributes or becomes a protected page in a hypervisor.
 +		 */
 +		kpte_clear_flush(kmap_pte-idx, vaddr);
 +		kmap_atomic_idx_pop();
 +		arch_flush_lazy_mmu_mode();
 +	}
 +#ifdef CONFIG_DEBUG_HIGHMEM
 +	else {
 +		BUG_ON(vaddr < PAGE_OFFSET);
 +		BUG_ON(vaddr >= (unsigned long)high_memory);
 +	}
 +#endif
 +
 +	pagefault_enable();
 +	preempt_enable();
 +}
 +EXPORT_SYMBOL(__kunmap_atomic);
 +
++=======
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  void __init set_highmem_pages_init(void)
  {
  	struct zone *zone;
diff --cc arch/x86/mm/init_32.c
index 6cae1166d8d6,da31c2635ee4..000000000000
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@@ -393,28 -394,6 +393,31 @@@ repeat
  	return last_map_addr;
  }
  
++<<<<<<< HEAD
 +pte_t *kmap_pte;
 +
 +static inline pte_t *kmap_get_fixmap_pte(unsigned long vaddr)
 +{
 +	pgd_t *pgd = pgd_offset_k(vaddr);
 +	p4d_t *p4d = p4d_offset(pgd, vaddr);
 +	pud_t *pud = pud_offset(p4d, vaddr);
 +	pmd_t *pmd = pmd_offset(pud, vaddr);
 +	return pte_offset_kernel(pmd, vaddr);
 +}
 +
 +static void __init kmap_init(void)
 +{
 +	unsigned long kmap_vstart;
 +
 +	/*
 +	 * Cache the first kmap pte:
 +	 */
 +	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
 +	kmap_pte = kmap_get_fixmap_pte(kmap_vstart);
 +}
 +
++=======
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  #ifdef CONFIG_HIGHMEM
  static void __init permanent_kmaps_init(pgd_t *pgd_base)
  {
diff --cc include/linux/highmem.h
index b1641d4ce279,1222a31be842..000000000000
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@@ -99,7 -216,8 +99,12 @@@ static inline void __kunmap_atomic(voi
  
  #endif /* CONFIG_HIGHMEM */
  
++<<<<<<< HEAD
 +#if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
++=======
+ #if !defined(CONFIG_KMAP_LOCAL)
+ #if defined(CONFIG_HIGHMEM)
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  
  DECLARE_PER_CPU(int, __kmap_atomic_idx);
  
diff --cc include/linux/io-mapping.h
index 58df02bd93c9,3b0940be72e9..000000000000
--- a/include/linux/io-mapping.h
+++ b/include/linux/io-mapping.h
@@@ -80,8 -69,7 +80,12 @@@ io_mapping_map_atomic_wc(struct io_mapp
  
  	BUG_ON(offset >= mapping->size);
  	phys_addr = mapping->base + offset;
++<<<<<<< HEAD
 +	pfn = (unsigned long) (phys_addr >> PAGE_SHIFT);
 +	return iomap_atomic_prot_pfn(pfn, mapping->prot);
++=======
+ 	return iomap_atomic_pfn_prot(PHYS_PFN(phys_addr), mapping->prot);
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  }
  
  static inline void
diff --cc mm/highmem.c
index 51171d0f44c2,77677c6844f7..000000000000
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@@ -29,11 -29,13 +29,16 @@@
  #include <linux/highmem.h>
  #include <linux/kgdb.h>
  #include <asm/tlbflush.h>
 -#include <linux/vmalloc.h>
  
++<<<<<<< HEAD
 +
 +#if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
++=======
+ #ifndef CONFIG_KMAP_LOCAL
+ #ifdef CONFIG_HIGHMEM
++>>>>>>> 157e118b5511 (x86/mm/highmem: Use generic kmap atomic implementation)
  DEFINE_PER_CPU(int, __kmap_atomic_idx);
  #endif
 -#endif
  
  /*
   * Virtual_count is not a pure "count".
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/include/asm/fixmap.h
* Unmerged path arch/x86/include/asm/highmem.h
diff --git a/arch/x86/include/asm/iomap.h b/arch/x86/include/asm/iomap.h
index 363e33eb6ec1..aa3be11e8dc2 100644
--- a/arch/x86/include/asm/iomap.h
+++ b/arch/x86/include/asm/iomap.h
@@ -22,20 +22,22 @@
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/uaccess.h>
+#include <linux/highmem.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 
-void __iomem *
-iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot);
+void __iomem *iomap_atomic_pfn_prot(unsigned long pfn, pgprot_t prot);
 
-void
-iounmap_atomic(void __iomem *kvaddr);
+static inline void iounmap_atomic(void __iomem *vaddr)
+{
+	kunmap_local_indexed((void __force *)vaddr);
+	pagefault_enable();
+	preempt_enable();
+}
 
-int
-iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
+int iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot);
 
-void
-iomap_free(resource_size_t base, unsigned long size);
+void iomap_free(resource_size_t base, unsigned long size);
 
 #endif /* _ASM_X86_IOMAP_H */
diff --git a/arch/x86/include/asm/kmap_types.h b/arch/x86/include/asm/kmap_types.h
deleted file mode 100644
index 04ab8266e347..000000000000
--- a/arch/x86/include/asm/kmap_types.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_X86_KMAP_TYPES_H
-#define _ASM_X86_KMAP_TYPES_H
-
-#if defined(CONFIG_X86_32) && defined(CONFIG_DEBUG_HIGHMEM)
-#define  __WITH_KM_FENCE
-#endif
-
-#include <asm-generic/kmap_types.h>
-
-#undef __WITH_KM_FENCE
-
-#endif /* _ASM_X86_KMAP_TYPES_H */
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 3e12d3196046..f6865098cca5 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -43,7 +43,6 @@
 #ifndef __ASSEMBLY__
 
 #include <asm/desc_defs.h>
-#include <asm/kmap_types.h>
 #include <asm/pgtable_types.h>
 #include <asm/nospec-branch.h>
 
* Unmerged path arch/x86/mm/highmem_32.c
* Unmerged path arch/x86/mm/init_32.c
diff --git a/arch/x86/mm/iomap_32.c b/arch/x86/mm/iomap_32.c
index 1dc9a1bb31c3..b5f8a41c701e 100644
--- a/arch/x86/mm/iomap_32.c
+++ b/arch/x86/mm/iomap_32.c
@@ -57,28 +57,7 @@ void iomap_free(resource_size_t base, unsigned long size)
 }
 EXPORT_SYMBOL_GPL(iomap_free);
 
-void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
-{
-	unsigned long vaddr;
-	int idx, type;
-
-	preempt_disable();
-	pagefault_disable();
-
-	type = kmap_atomic_idx_push();
-	idx = type + KM_TYPE_NR * smp_processor_id();
-	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
-	arch_flush_lazy_mmu_mode();
-
-	return (void *)vaddr;
-}
-
-/*
- * Map 'pfn' using protections 'prot'
- */
-void __iomem *
-iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
+void __iomem *iomap_atomic_pfn_prot(unsigned long pfn, pgprot_t prot)
 {
 	/*
 	 * For non-PAT systems, translate non-WB request to UC- just in
@@ -94,36 +73,8 @@ iomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 	/* Filter out unsupported __PAGE_KERNEL* bits: */
 	pgprot_val(prot) &= __default_kernel_pte_mask;
 
-	return (void __force __iomem *) kmap_atomic_prot_pfn(pfn, prot);
-}
-EXPORT_SYMBOL_GPL(iomap_atomic_prot_pfn);
-
-void
-iounmap_atomic(void __iomem *kvaddr)
-{
-	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
-
-	if (vaddr >= __fix_to_virt(FIX_KMAP_END) &&
-	    vaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {
-		int idx, type;
-
-		type = kmap_atomic_idx();
-		idx = type + KM_TYPE_NR * smp_processor_id();
-
-#ifdef CONFIG_DEBUG_HIGHMEM
-		WARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-#endif
-		/*
-		 * Force other mappings to Oops if they'll try to access this
-		 * pte without first remap it.  Keeping stale mappings around
-		 * is a bad idea also, in case the page changes cacheability
-		 * attributes or becomes a protected page in a hypervisor.
-		 */
-		kpte_clear_flush(kmap_pte-idx, vaddr);
-		kmap_atomic_idx_pop();
-	}
-
-	pagefault_enable();
-	preempt_enable();
+	preempt_disable();
+	pagefault_disable();
+	return (void __force __iomem *)__kmap_local_pfn_prot(pfn, prot);
 }
-EXPORT_SYMBOL_GPL(iounmap_atomic);
+EXPORT_SYMBOL_GPL(iomap_atomic_pfn_prot);
* Unmerged path include/linux/highmem.h
* Unmerged path include/linux/io-mapping.h
* Unmerged path mm/highmem.c
