sched/core: Rework the __schedule() preempt argument

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit b4bfa3fcfe3b827ddb8b16edd45896caac5a1194
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/b4bfa3fc.failed

PREEMPT_RT needs to hand a special state into __schedule() when a task
blocks on a 'sleeping' spin/rwlock. This is required to handle
rcu_note_context_switch() correctly without having special casing in the
RCU code. From an RCU point of view the blocking on the sleeping spinlock
is equivalent to preemption, because the task might be in a read side
critical section.

schedule_debug() also has a check which would trigger with the !preempt
case, but that could be handled differently.

To avoid adding another argument and extra checks which cannot be optimized
out by the compiler, the following solution has been chosen:

 - Replace the boolean 'preempt' argument with an unsigned integer
   'sched_mode' argument and define constants to hand in:
   (0 == no preemption, 1 = preemption).

 - Add two masks to apply on that mode: one for the debug/rcu invocations,
   and one for the actual scheduling decision.

   For a non RT kernel these masks are UINT_MAX, i.e. all bits are set,
   which allows the compiler to optimize the AND operation out, because it is
   not masking out anything. IOW, it's not different from the boolean.

   RT enabled kernels will define these masks separately.

No functional change.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20210815211302.315473019@linutronix.de
(cherry picked from commit b4bfa3fcfe3b827ddb8b16edd45896caac5a1194)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 9945a9321de5,ebc24e136222..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -3670,56 -5321,516 +3670,68 @@@ static void put_prev_task_balance(struc
  		if (class->balance(rq, prev, rf))
  			break;
  	}
 -#endif
 -
 -	put_prev_task(rq, prev);
 -}
 -
 -/*
 - * Pick up the highest-prio task:
 - */
 -static inline struct task_struct *
 -__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 -{
 -	const struct sched_class *class;
 -	struct task_struct *p;
 -
 -	/*
 -	 * Optimization: we know that if all tasks are in the fair class we can
 -	 * call that function directly, but only if the @prev task wasn't of a
 -	 * higher scheduling class, because otherwise those lose the
 -	 * opportunity to pull in more work from other CPUs.
 -	 */
 -	if (likely(prev->sched_class <= &fair_sched_class &&
 -		   rq->nr_running == rq->cfs.h_nr_running)) {
 -
 -		p = pick_next_task_fair(rq, prev, rf);
 -		if (unlikely(p == RETRY_TASK))
 -			goto restart;
 -
 -		/* Assume the next prioritized class is idle_sched_class */
 -		if (!p) {
 -			put_prev_task(rq, prev);
 -			p = pick_next_task_idle(rq);
 -		}
 -
 -		return p;
 -	}
 -
 -restart:
 -	put_prev_task_balance(rq, prev, rf);
 -
 -	for_each_class(class) {
 -		p = class->pick_next_task(rq);
 -		if (p)
 -			return p;
 -	}
 -
 -	/* The idle class should always have a runnable task: */
 -	BUG();
 -}
 -
 -#ifdef CONFIG_SCHED_CORE
 -static inline bool is_task_rq_idle(struct task_struct *t)
 -{
 -	return (task_rq(t)->idle == t);
 -}
 -
 -static inline bool cookie_equals(struct task_struct *a, unsigned long cookie)
 -{
 -	return is_task_rq_idle(a) || (a->core_cookie == cookie);
 -}
 -
 -static inline bool cookie_match(struct task_struct *a, struct task_struct *b)
 -{
 -	if (is_task_rq_idle(a) || is_task_rq_idle(b))
 -		return true;
 -
 -	return a->core_cookie == b->core_cookie;
 -}
 -
 -// XXX fairness/fwd progress conditions
 -/*
 - * Returns
 - * - NULL if there is no runnable task for this class.
 - * - the highest priority task for this runqueue if it matches
 - *   rq->core->core_cookie or its priority is greater than max.
 - * - Else returns idle_task.
 - */
 -static struct task_struct *
 -pick_task(struct rq *rq, const struct sched_class *class, struct task_struct *max, bool in_fi)
 -{
 -	struct task_struct *class_pick, *cookie_pick;
 -	unsigned long cookie = rq->core->core_cookie;
 -
 -	class_pick = class->pick_task(rq);
 -	if (!class_pick)
 -		return NULL;
 -
 -	if (!cookie) {
 -		/*
 -		 * If class_pick is tagged, return it only if it has
 -		 * higher priority than max.
 -		 */
 -		if (max && class_pick->core_cookie &&
 -		    prio_less(class_pick, max, in_fi))
 -			return idle_sched_class.pick_task(rq);
 -
 -		return class_pick;
 -	}
 -
 -	/*
 -	 * If class_pick is idle or matches cookie, return early.
 -	 */
 -	if (cookie_equals(class_pick, cookie))
 -		return class_pick;
 -
 -	cookie_pick = sched_core_find(rq, cookie);
 -
 -	/*
 -	 * If class > max && class > cookie, it is the highest priority task on
 -	 * the core (so far) and it must be selected, otherwise we must go with
 -	 * the cookie pick in order to satisfy the constraint.
 -	 */
 -	if (prio_less(cookie_pick, class_pick, in_fi) &&
 -	    (!max || prio_less(max, class_pick, in_fi)))
 -		return class_pick;
 -
 -	return cookie_pick;
 -}
 -
 -extern void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi);
 -
 -static struct task_struct *
 -pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 -{
 -	struct task_struct *next, *max = NULL;
 -	const struct sched_class *class;
 -	const struct cpumask *smt_mask;
 -	bool fi_before = false;
 -	int i, j, cpu, occ = 0;
 -	bool need_sync;
 -
 -	if (!sched_core_enabled(rq))
 -		return __pick_next_task(rq, prev, rf);
 -
 -	cpu = cpu_of(rq);
 -
 -	/* Stopper task is switching into idle, no need core-wide selection. */
 -	if (cpu_is_offline(cpu)) {
 -		/*
 -		 * Reset core_pick so that we don't enter the fastpath when
 -		 * coming online. core_pick would already be migrated to
 -		 * another cpu during offline.
 -		 */
 -		rq->core_pick = NULL;
 -		return __pick_next_task(rq, prev, rf);
 -	}
 -
 -	/*
 -	 * If there were no {en,de}queues since we picked (IOW, the task
 -	 * pointers are all still valid), and we haven't scheduled the last
 -	 * pick yet, do so now.
 -	 *
 -	 * rq->core_pick can be NULL if no selection was made for a CPU because
 -	 * it was either offline or went offline during a sibling's core-wide
 -	 * selection. In this case, do a core-wide selection.
 -	 */
 -	if (rq->core->core_pick_seq == rq->core->core_task_seq &&
 -	    rq->core->core_pick_seq != rq->core_sched_seq &&
 -	    rq->core_pick) {
 -		WRITE_ONCE(rq->core_sched_seq, rq->core->core_pick_seq);
 -
 -		next = rq->core_pick;
 -		if (next != prev) {
 -			put_prev_task(rq, prev);
 -			set_next_task(rq, next);
 -		}
 -
 -		rq->core_pick = NULL;
 -		return next;
 -	}
 -
 -	put_prev_task_balance(rq, prev, rf);
 -
 -	smt_mask = cpu_smt_mask(cpu);
 -	need_sync = !!rq->core->core_cookie;
 -
 -	/* reset state */
 -	rq->core->core_cookie = 0UL;
 -	if (rq->core->core_forceidle) {
 -		need_sync = true;
 -		fi_before = true;
 -		rq->core->core_forceidle = false;
 -	}
 -
 -	/*
 -	 * core->core_task_seq, core->core_pick_seq, rq->core_sched_seq
 -	 *
 -	 * @task_seq guards the task state ({en,de}queues)
 -	 * @pick_seq is the @task_seq we did a selection on
 -	 * @sched_seq is the @pick_seq we scheduled
 -	 *
 -	 * However, preemptions can cause multiple picks on the same task set.
 -	 * 'Fix' this by also increasing @task_seq for every pick.
 -	 */
 -	rq->core->core_task_seq++;
 -
 -	/*
 -	 * Optimize for common case where this CPU has no cookies
 -	 * and there are no cookied tasks running on siblings.
 -	 */
 -	if (!need_sync) {
 -		for_each_class(class) {
 -			next = class->pick_task(rq);
 -			if (next)
 -				break;
 -		}
 -
 -		if (!next->core_cookie) {
 -			rq->core_pick = NULL;
 -			/*
 -			 * For robustness, update the min_vruntime_fi for
 -			 * unconstrained picks as well.
 -			 */
 -			WARN_ON_ONCE(fi_before);
 -			task_vruntime_update(rq, next, false);
 -			goto done;
 -		}
 -	}
 -
 -	for_each_cpu(i, smt_mask) {
 -		struct rq *rq_i = cpu_rq(i);
 -
 -		rq_i->core_pick = NULL;
 -
 -		if (i != cpu)
 -			update_rq_clock(rq_i);
 -	}
 -
 -	/*
 -	 * Try and select tasks for each sibling in descending sched_class
 -	 * order.
 -	 */
 -	for_each_class(class) {
 -again:
 -		for_each_cpu_wrap(i, smt_mask, cpu) {
 -			struct rq *rq_i = cpu_rq(i);
 -			struct task_struct *p;
 -
 -			if (rq_i->core_pick)
 -				continue;
 -
 -			/*
 -			 * If this sibling doesn't yet have a suitable task to
 -			 * run; ask for the most eligible task, given the
 -			 * highest priority task already selected for this
 -			 * core.
 -			 */
 -			p = pick_task(rq_i, class, max, fi_before);
 -			if (!p)
 -				continue;
 -
 -			if (!is_task_rq_idle(p))
 -				occ++;
 -
 -			rq_i->core_pick = p;
 -			if (rq_i->idle == p && rq_i->nr_running) {
 -				rq->core->core_forceidle = true;
 -				if (!fi_before)
 -					rq->core->core_forceidle_seq++;
 -			}
 -
 -			/*
 -			 * If this new candidate is of higher priority than the
 -			 * previous; and they're incompatible; we need to wipe
 -			 * the slate and start over. pick_task makes sure that
 -			 * p's priority is more than max if it doesn't match
 -			 * max's cookie.
 -			 *
 -			 * NOTE: this is a linear max-filter and is thus bounded
 -			 * in execution time.
 -			 */
 -			if (!max || !cookie_match(max, p)) {
 -				struct task_struct *old_max = max;
 -
 -				rq->core->core_cookie = p->core_cookie;
 -				max = p;
 -
 -				if (old_max) {
 -					rq->core->core_forceidle = false;
 -					for_each_cpu(j, smt_mask) {
 -						if (j == i)
 -							continue;
 -
 -						cpu_rq(j)->core_pick = NULL;
 -					}
 -					occ = 1;
 -					goto again;
 -				}
 -			}
 -		}
 -	}
 -
 -	rq->core->core_pick_seq = rq->core->core_task_seq;
 -	next = rq->core_pick;
 -	rq->core_sched_seq = rq->core->core_pick_seq;
 -
 -	/* Something should have been selected for current CPU */
 -	WARN_ON_ONCE(!next);
 -
 -	/*
 -	 * Reschedule siblings
 -	 *
 -	 * NOTE: L1TF -- at this point we're no longer running the old task and
 -	 * sending an IPI (below) ensures the sibling will no longer be running
 -	 * their task. This ensures there is no inter-sibling overlap between
 -	 * non-matching user state.
 -	 */
 -	for_each_cpu(i, smt_mask) {
 -		struct rq *rq_i = cpu_rq(i);
 -
 -		/*
 -		 * An online sibling might have gone offline before a task
 -		 * could be picked for it, or it might be offline but later
 -		 * happen to come online, but its too late and nothing was
 -		 * picked for it.  That's Ok - it will pick tasks for itself,
 -		 * so ignore it.
 -		 */
 -		if (!rq_i->core_pick)
 -			continue;
 -
 -		/*
 -		 * Update for new !FI->FI transitions, or if continuing to be in !FI:
 -		 * fi_before     fi      update?
 -		 *  0            0       1
 -		 *  0            1       1
 -		 *  1            0       1
 -		 *  1            1       0
 -		 */
 -		if (!(fi_before && rq->core->core_forceidle))
 -			task_vruntime_update(rq_i, rq_i->core_pick, rq->core->core_forceidle);
 -
 -		rq_i->core_pick->core_occupation = occ;
 -
 -		if (i == cpu) {
 -			rq_i->core_pick = NULL;
 -			continue;
 -		}
 -
 -		/* Did we break L1TF mitigation requirements? */
 -		WARN_ON_ONCE(!cookie_match(next, rq_i->core_pick));
 -
 -		if (rq_i->curr == rq_i->core_pick) {
 -			rq_i->core_pick = NULL;
 -			continue;
 -		}
 -
 -		resched_curr(rq_i);
 -	}
 -
 -done:
 -	set_next_task(rq, next);
 -	return next;
 -}
 -
 -static bool try_steal_cookie(int this, int that)
 -{
 -	struct rq *dst = cpu_rq(this), *src = cpu_rq(that);
 -	struct task_struct *p;
 -	unsigned long cookie;
 -	bool success = false;
 -
 -	local_irq_disable();
 -	double_rq_lock(dst, src);
 -
 -	cookie = dst->core->core_cookie;
 -	if (!cookie)
 -		goto unlock;
 -
 -	if (dst->curr != dst->idle)
 -		goto unlock;
 -
 -	p = sched_core_find(src, cookie);
 -	if (p == src->idle)
 -		goto unlock;
 -
 -	do {
 -		if (p == src->core_pick || p == src->curr)
 -			goto next;
 -
 -		if (!cpumask_test_cpu(this, &p->cpus_mask))
 -			goto next;
 -
 -		if (p->core_occupation > dst->idle->core_occupation)
 -			goto next;
 -
 -		p->on_rq = TASK_ON_RQ_MIGRATING;
 -		deactivate_task(src, p, 0);
 -		set_task_cpu(p, this);
 -		activate_task(dst, p, 0);
 -		p->on_rq = TASK_ON_RQ_QUEUED;
 -
 -		resched_curr(dst);
 -
 -		success = true;
 -		break;
 -
 -next:
 -		p = sched_core_next(p, cookie);
 -	} while (p);
 -
 -unlock:
 -	double_rq_unlock(dst, src);
 -	local_irq_enable();
 -
 -	return success;
 -}
 -
 -static bool steal_cookie_task(int cpu, struct sched_domain *sd)
 -{
 -	int i;
 -
 -	for_each_cpu_wrap(i, sched_domain_span(sd), cpu) {
 -		if (i == cpu)
 -			continue;
 -
 -		if (need_resched())
 -			break;
 -
 -		if (try_steal_cookie(cpu, i))
 -			return true;
 -	}
 -
 -	return false;
 -}
 -
 -static void sched_core_balance(struct rq *rq)
 -{
 -	struct sched_domain *sd;
 -	int cpu = cpu_of(rq);
 -
 -	preempt_disable();
 -	rcu_read_lock();
 -	raw_spin_rq_unlock_irq(rq);
 -	for_each_domain(cpu, sd) {
 -		if (need_resched())
 -			break;
 -
 -		if (steal_cookie_task(cpu, sd))
 -			break;
 -	}
 -	raw_spin_rq_lock_irq(rq);
 -	rcu_read_unlock();
 -	preempt_enable();
 -}
 -
 -static DEFINE_PER_CPU(struct callback_head, core_balance_head);
 -
 -void queue_core_balance(struct rq *rq)
 -{
 -	if (!sched_core_enabled(rq))
 -		return;
 -
 -	if (!rq->core->core_cookie)
 -		return;
 -
 -	if (!rq->nr_running) /* not forced idle */
 -		return;
 +#endif
  
 -	queue_balance_callback(rq, &per_cpu(core_balance_head, rq->cpu), sched_core_balance);
 +	put_prev_task(rq, prev);
  }
  
 -static inline void sched_core_cpu_starting(unsigned int cpu)
 +/*
 + * Pick up the highest-prio task:
 + */
 +static inline struct task_struct *
 +pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  {
 -	const struct cpumask *smt_mask = cpu_smt_mask(cpu);
 -	struct rq *rq, *core_rq = NULL;
 -	int i;
 +	const struct sched_class *class;
 +	struct task_struct *p;
 +
 +	/*
 +	 * Optimization: we know that if all tasks are in the fair class we can
 +	 * call that function directly, but only if the @prev task wasn't of a
 +	 * higher scheduling class, because otherwise those lose the
 +	 * opportunity to pull in more work from other CPUs.
 +	 */
 +	if (likely((prev->sched_class == &idle_sched_class ||
 +		    prev->sched_class == &fair_sched_class) &&
 +		   rq->nr_running == rq->cfs.h_nr_running)) {
  
 -	core_rq = cpu_rq(cpu)->core;
 +		p = pick_next_task_fair(rq, prev, rf);
 +		if (unlikely(p == RETRY_TASK))
 +			goto restart;
  
 -	if (!core_rq) {
 -		for_each_cpu(i, smt_mask) {
 -			rq = cpu_rq(i);
 -			if (rq->core && rq->core == rq)
 -				core_rq = rq;
 +		/* Assume the next prioritized class is idle_sched_class */
 +		if (!p) {
 +			put_prev_task(rq, prev);
 +			p = pick_next_task_idle(rq);
  		}
  
 -		if (!core_rq)
 -			core_rq = cpu_rq(cpu);
 +		return p;
 +	}
  
 -		for_each_cpu(i, smt_mask) {
 -			rq = cpu_rq(i);
 +restart:
 +	put_prev_task_balance(rq, prev, rf);
  
 -			WARN_ON_ONCE(rq->core && rq->core != core_rq);
 -			rq->core = core_rq;
 -		}
 +	for_each_class(class) {
 +		p = class->pick_next_task(rq);
 +		if (p)
 +			return p;
  	}
 -}
 -#else /* !CONFIG_SCHED_CORE */
  
 -static inline void sched_core_cpu_starting(unsigned int cpu) {}
 -
 -static struct task_struct *
 -pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 -{
 -	return __pick_next_task(rq, prev, rf);
 +	/* The idle class should always have a runnable task: */
 +	BUG();
  }
  
 -#endif /* CONFIG_SCHED_CORE */
 -
+ /*
+  * Constants for the sched_mode argument of __schedule().
+  *
+  * The mode argument allows RT enabled kernels to differentiate a
+  * preemption from blocking on an 'sleeping' spin/rwlock. Note that
+  * SM_MASK_PREEMPT for !RT has all bits set, which allows the compiler to
+  * optimize the AND operation out and just check for zero.
+  */
+ #define SM_NONE			0x0
+ #define SM_PREEMPT		0x1
+ #define SM_MASK_PREEMPT		(~0U)
+ 
  /*
   * __schedule() is the main scheduler function.
   *
@@@ -3772,7 -5883,7 +3784,11 @@@ static void __sched notrace __schedule(
  	rq = cpu_rq(cpu);
  	prev = rq->curr;
  
++<<<<<<< HEAD
 +	schedule_debug(prev);
++=======
+ 	schedule_debug(prev, !!sched_mode);
++>>>>>>> b4bfa3fcfe3b (sched/core: Rework the __schedule() preempt argument)
  
  	if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
  		hrtick_clear(rq);
@@@ -3811,10 -5922,10 +3827,15 @@@
  	 *  - we form a control dependency vs deactivate_task() below.
  	 *  - ptrace_{,un}freeze_traced() can change ->state underneath us.
  	 */
++<<<<<<< HEAD
 +	prev_state = prev->state;
 +	if (!preempt && prev_state) {
++=======
+ 	prev_state = READ_ONCE(prev->__state);
+ 	if (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {
++>>>>>>> b4bfa3fcfe3b (sched/core: Rework the __schedule() preempt argument)
  		if (signal_pending_state(prev_state, prev)) {
 -			WRITE_ONCE(prev->__state, TASK_RUNNING);
 +			prev->state = TASK_RUNNING;
  		} else {
  			prev->sched_contributes_to_load =
  				(prev_state & TASK_UNINTERRUPTIBLE) &&
@@@ -3872,7 -5986,10 +3893,14 @@@
  		 */
  		++*switch_count;
  
++<<<<<<< HEAD
 +		trace_sched_switch(preempt, prev, next);
++=======
+ 		migrate_disable_switch(rq, prev);
+ 		psi_sched_switch(prev, next, !task_on_rq_queued(prev));
+ 
+ 		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next);
++>>>>>>> b4bfa3fcfe3b (sched/core: Rework the __schedule() preempt argument)
  
  		/* Also unlocks the rq: */
  		rq = context_switch(rq, prev, next, &rf);
@@@ -3979,9 -6097,9 +4007,9 @@@ void __sched schedule_idle(void
  	 * current task can be in any other state. Note, idle is always in the
  	 * TASK_RUNNING state.
  	 */
 -	WARN_ON_ONCE(current->__state);
 +	WARN_ON_ONCE(current->state);
  	do {
- 		__schedule(false);
+ 		__schedule(SM_NONE);
  	} while (need_resched());
  }
  
* Unmerged path kernel/sched/core.c
