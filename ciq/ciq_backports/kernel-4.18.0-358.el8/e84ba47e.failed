x86/fpu: Hook up PKRU into ptrace()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit e84ba47e313dbc097bf859bb6e4f9219883d5f78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/e84ba47e.failed

One nice thing about having PKRU be XSAVE-managed is that it gets naturally
exposed into the XSAVE-using ABIs.  Now that XSAVE will not be used to
manage PKRU, these ABIs need to be manually enabled to deal with PKRU.

ptrace() uses copy_uabi_xstate_to_kernel() to collect the tracee's
XSTATE. As PKRU is not in the task's XSTATE buffer, use task->thread.pkru
for filling in up the ptrace buffer.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
Link: https://lkml.kernel.org/r/20210623121456.508770763@linutronix.de
(cherry picked from commit e84ba47e313dbc097bf859bb6e4f9219883d5f78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/fpu/xstate.h
#	arch/x86/kernel/fpu/regset.c
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/include/asm/fpu/xstate.h
index 1a0fecae6ef8,4ff4a0093a48..000000000000
--- a/arch/x86/include/asm/fpu/xstate.h
+++ b/arch/x86/include/asm/fpu/xstate.h
@@@ -102,14 -125,21 +102,32 @@@ extern void __init update_regset_xstate
  					     u64 xstate_mask);
  
  void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);
 +const void *get_xsave_field_ptr(int xfeature_nr);
 +int using_compacted_format(void);
  int xfeature_size(int xfeature_nr);
++<<<<<<< HEAD
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
 +int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
 +int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
 +int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
 +void copy_dynamic_supervisor_to_kernel(struct xregs_state *xstate, u64 mask);
 +void copy_kernel_to_dynamic_supervisor(struct xregs_state *xstate, u64 mask);
++=======
+ int copy_uabi_from_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
+ int copy_sigframe_from_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
+ 
+ void xsaves(struct xregs_state *xsave, u64 mask);
+ void xrstors(struct xregs_state *xsave, u64 mask);
+ 
+ enum xstate_copy_mode {
+ 	XSTATE_COPY_FP,
+ 	XSTATE_COPY_FX,
+ 	XSTATE_COPY_XSAVE,
+ };
+ 
+ struct membuf;
+ void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 			     enum xstate_copy_mode mode);
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  
  #endif
diff --cc arch/x86/kernel/fpu/regset.c
index 7bcf779c46cb,66ed317ebc0d..000000000000
--- a/arch/x86/kernel/fpu/regset.c
+++ b/arch/x86/kernel/fpu/regset.c
@@@ -34,14 -68,18 +34,24 @@@ int xfpregs_get(struct task_struct *tar
  {
  	struct fpu *fpu = &target->thread.fpu;
  
 -	if (!cpu_feature_enabled(X86_FEATURE_FXSR))
 +	if (!boot_cpu_has(X86_FEATURE_FXSR))
  		return -ENODEV;
  
 -	sync_fpstate(fpu);
 +	fpu__prepare_read(fpu);
 +	fpstate_sanitize_xstate(fpu);
  
++<<<<<<< HEAD
 +	return user_regset_copyout(&pos, &count, &kbuf, &ubuf,
 +				   &fpu->state.fxsave, 0, -1);
++=======
+ 	if (!use_xsave()) {
+ 		return membuf_write(&to, &fpu->state.fxsave,
+ 				    sizeof(fpu->state.fxsave));
+ 	}
+ 
+ 	copy_xstate_to_uabi_buf(to, target, XSTATE_COPY_FX);
+ 	return 0;
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  }
  
  int xfpregs_set(struct task_struct *target, const struct user_regset *regset,
@@@ -86,40 -124,15 +96,50 @@@
  }
  
  int xstateregs_get(struct task_struct *target, const struct user_regset *regset,
 -		struct membuf to)
 +		unsigned int pos, unsigned int count,
 +		void *kbuf, void __user *ubuf)
  {
++<<<<<<< HEAD
 +	struct fpu *fpu = &target->thread.fpu;
 +	struct xregs_state *xsave;
 +	int ret;
 +
 +	if (!boot_cpu_has(X86_FEATURE_XSAVE))
 +		return -ENODEV;
 +
 +	xsave = &fpu->state.xsave;
 +
 +	fpu__prepare_read(fpu);
 +
 +	if (using_compacted_format()) {
 +		if (kbuf)
 +			ret = copy_xstate_to_kernel(kbuf, xsave, pos, count);
 +		else
 +			ret = copy_xstate_to_user(ubuf, xsave, pos, count);
 +	} else {
 +		fpstate_sanitize_xstate(fpu);
 +		/*
 +		 * Copy the 48 bytes defined by the software into the xsave
 +		 * area in the thread struct, so that we can copy the whole
 +		 * area to user using one user_regset_copyout().
 +		 */
 +		memcpy(&xsave->i387.sw_reserved, xstate_fx_sw_bytes, sizeof(xstate_fx_sw_bytes));
 +
 +		/*
 +		 * Copy the xstate memory layout.
 +		 */
 +		ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);
 +	}
 +	return ret;
++=======
+ 	if (!cpu_feature_enabled(X86_FEATURE_XSAVE))
+ 		return -ENODEV;
+ 
+ 	sync_fpstate(&target->thread.fpu);
+ 
+ 	copy_xstate_to_uabi_buf(to, target, XSTATE_COPY_XSAVE);
+ 	return 0;
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  }
  
  int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
@@@ -300,27 -318,30 +320,39 @@@ int fpregs_get(struct task_struct *targ
  {
  	struct fpu *fpu = &target->thread.fpu;
  	struct user_i387_ia32_struct env;
 -	struct fxregs_state fxsave, *fx;
  
 -	sync_fpstate(fpu);
 +	fpu__prepare_read(fpu);
  
 -	if (!cpu_feature_enabled(X86_FEATURE_FPU))
 -		return fpregs_soft_get(target, regset, to);
 +	if (!boot_cpu_has(X86_FEATURE_FPU))
 +		return fpregs_soft_get(target, regset, pos, count, kbuf, ubuf);
 +
 +	if (!boot_cpu_has(X86_FEATURE_FXSR))
 +		return user_regset_copyout(&pos, &count, &kbuf, &ubuf,
 +					   &fpu->state.fsave, 0,
 +					   -1);
  
 -	if (!cpu_feature_enabled(X86_FEATURE_FXSR)) {
 -		return membuf_write(&to, &fpu->state.fsave,
 -				    sizeof(struct fregs_state));
 +	fpstate_sanitize_xstate(fpu);
 +
 +	if (kbuf && pos == 0 && count == sizeof(env)) {
 +		convert_from_fxsr(kbuf, target);
 +		return 0;
  	}
  
 -	if (use_xsave()) {
 -		struct membuf mb = { .p = &fxsave, .left = sizeof(fxsave) };
 +	convert_from_fxsr(&env, target);
  
++<<<<<<< HEAD
 +	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, &env, 0, -1);
++=======
+ 		/* Handle init state optimized xstate correctly */
+ 		copy_xstate_to_uabi_buf(mb, target, XSTATE_COPY_FP);
+ 		fx = &fxsave;
+ 	} else {
+ 		fx = &fpu->state.fxsave;
+ 	}
+ 
+ 	__convert_from_fxsr(&env, target, fx);
+ 	return membuf_write(&to, &env, sizeof(env));
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  }
  
  int fpregs_set(struct task_struct *target, const struct user_regset *regset,
diff --cc arch/x86/kernel/fpu/xstate.c
index 4641d3145e59,9fd124a001b0..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -994,128 -953,136 +994,172 @@@ int arch_set_user_pkey_access(struct ta
  }
  #endif /* ! CONFIG_ARCH_HAS_PKEYS */
  
 -static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
 -			 void *init_xstate, unsigned int size)
 +/*
 + * Weird legacy quirk: SSE and YMM states store information in the
 + * MXCSR and MXCSR_FLAGS fields of the FP area. That means if the FP
 + * area is marked as unused in the xfeatures header, we need to copy
 + * MXCSR and MXCSR_FLAGS if either SSE or YMM are in use.
 + */
 +static inline bool xfeatures_mxcsr_quirk(u64 xfeatures)
 +{
 +	if (!(xfeatures & (XFEATURE_MASK_SSE|XFEATURE_MASK_YMM)))
 +		return false;
 +
 +	if (xfeatures & XFEATURE_MASK_FP)
 +		return false;
 +
 +	return true;
 +}
 +
++<<<<<<< HEAD
 +static void fill_gap(unsigned to, void **kbuf, unsigned *pos, unsigned *count)
 +{
 +	if (*pos < to) {
 +		unsigned size = to - *pos;
 +
 +		if (size > *count)
 +			size = *count;
 +		memcpy(*kbuf, (void *)&init_fpstate.xsave + *pos, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
 +}
 +
 +static void copy_part(unsigned offset, unsigned size, void *from,
 +			void **kbuf, unsigned *pos, unsigned *count)
  {
 -	membuf_write(to, from_xstate ? xstate : init_xstate, size);
 +	fill_gap(offset, kbuf, pos, count);
 +	if (size > *count)
 +		size = *count;
 +	if (size) {
 +		memcpy(*kbuf, from, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
  }
  
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a kernel-space ptrace buffer.
 + *
 + * It supports partial copy but pos always starts from zero. This is called
 + * from xstateregs_get() and there we check the CPU has XSAVES.
 + */
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
 +{
++=======
+ /**
+  * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
+  * @to:		membuf descriptor
+  * @tsk:	The task from which to copy the saved xstate
+  * @copy_mode:	The requested copy mode
+  *
+  * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
+  * format, i.e. from the kernel internal hardware dependent storage format
+  * to the requested @mode. UABI XSTATE is always uncompacted!
+  *
+  * It supports partial copy but @to.pos always starts from zero.
+  */
+ void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
+ 			     enum xstate_copy_mode copy_mode)
+ {
+ 	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
+ 	struct xregs_state *xsave = &tsk->thread.fpu.state.xsave;
+ 	struct xregs_state *xinit = &init_fpstate.xsave;
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  	struct xstate_header header;
 -	unsigned int zerofrom;
 +	const unsigned off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 +	unsigned count = size_total;
  	int i;
  
 -	header.xfeatures = xsave->header.xfeatures;
 -
 -	/* Mask out the feature bits depending on copy mode */
 -	switch (copy_mode) {
 -	case XSTATE_COPY_FP:
 -		header.xfeatures &= XFEATURE_MASK_FP;
 -		break;
 -
 -	case XSTATE_COPY_FX:
 -		header.xfeatures &= XFEATURE_MASK_FP | XFEATURE_MASK_SSE;
 -		break;
 -
 -	case XSTATE_COPY_XSAVE:
 -		header.xfeatures &= xfeatures_mask_uabi();
 -		break;
 -	}
 -
 -	/* Copy FP state up to MXCSR */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_FP, &to, &xsave->i387,
 -		     &xinit->i387, off_mxcsr);
 -
 -	/* Copy MXCSR when SSE or YMM are set in the feature mask */
 -	copy_feature(header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM),
 -		     &to, &xsave->i387.mxcsr, &xinit->i387.mxcsr,
 -		     MXCSR_AND_FLAGS_SIZE);
 -
 -	/* Copy the remaining FP state */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_FP,
 -		     &to, &xsave->i387.st_space, &xinit->i387.st_space,
 -		     sizeof(xsave->i387.st_space));
 -
 -	/* Copy the SSE state - shared with YMM, but independently managed */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_SSE,
 -		     &to, &xsave->i387.xmm_space, &xinit->i387.xmm_space,
 -		     sizeof(xsave->i387.xmm_space));
 -
 -	if (copy_mode != XSTATE_COPY_XSAVE)
 -		goto out;
 -
 -	/* Zero the padding area */
 -	membuf_zero(&to, sizeof(xsave->i387.padding));
 -
 -	/* Copy xsave->i387.sw_reserved */
 -	membuf_write(&to, xstate_fx_sw_bytes, sizeof(xsave->i387.sw_reserved));
 -
 -	/* Copy the user space relevant state of @xsave->header */
 -	membuf_write(&to, &header, sizeof(header));
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
  
 -	zerofrom = offsetof(struct xregs_state, extended_state_area);
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
 +	header.xfeatures = xsave->header.xfeatures;
 +	header.xfeatures &= xfeatures_mask_user();
 +
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(0, off_mxcsr,
 +			  &xsave->i387, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM))
 +		copy_part(off_mxcsr, MXCSR_AND_FLAGS_SIZE,
 +			  &xsave->i387.mxcsr, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(offsetof(struct fxregs_state, st_space), 128,
 +			  &xsave->i387.st_space, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_SSE)
 +		copy_part(xstate_offsets[XFEATURE_SSE], 256,
 +			  &xsave->i387.xmm_space, &kbuf, &offset_start, &count);
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	copy_part(offsetof(struct fxregs_state, sw_reserved), 48,
 +		  xstate_fx_sw_bytes, &kbuf, &offset_start, &count);
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	copy_part(offsetof(struct xregs_state, header), sizeof(header),
 +		  &header, &kbuf, &offset_start, &count);
  
  	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
  		/*
 -		 * The ptrace buffer is in non-compacted XSAVE format.
 -		 * In non-compacted format disabled features still occupy
 -		 * state space, but there is no state to copy from in the
 -		 * compacted init_fpstate. The gap tracking will zero this
 -		 * later.
 +		 * Copy only in-use xstates:
  		 */
 -		if (!(xfeatures_mask_uabi() & BIT_ULL(i)))
 -			continue;
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
  
 -		/*
 -		 * If there was a feature or alignment gap, zero the space
 -		 * in the destination buffer.
 -		 */
 -		if (zerofrom < xstate_offsets[i])
 -			membuf_zero(&to, xstate_offsets[i] - zerofrom);
 +			copy_part(xstate_offsets[i], xstate_sizes[i],
 +				  src, &kbuf, &offset_start, &count);
 +		}
  
++<<<<<<< HEAD
++=======
+ 		if (i == XFEATURE_PKRU) {
+ 			struct pkru_state pkru = {0};
+ 			/*
+ 			 * PKRU is not necessarily up to date in the
+ 			 * thread's XSAVE buffer.  Fill this part from the
+ 			 * per-thread storage.
+ 			 */
+ 			pkru.pkru = tsk->thread.pkru;
+ 			membuf_write(&to, &pkru, sizeof(pkru));
+ 		} else {
+ 			copy_feature(header.xfeatures & BIT_ULL(i), &to,
+ 				     __raw_xsave_addr(xsave, i),
+ 				     __raw_xsave_addr(xinit, i),
+ 				     xstate_sizes[i]);
+ 		}
+ 		/*
+ 		 * Keep track of the last copied state in the non-compacted
+ 		 * target buffer for gap zeroing.
+ 		 */
+ 		zerofrom = xstate_offsets[i] + xstate_sizes[i];
++>>>>>>> e84ba47e313d (x86/fpu: Hook up PKRU into ptrace())
  	}
 +	fill_gap(size_total, &kbuf, &offset_start, &count);
  
 -out:
 -	if (to.left)
 -		membuf_zero(&to, to.left);
 +	return 0;
  }
  
 -static int copy_from_buffer(void *dst, unsigned int offset, unsigned int size,
 -			    const void *kbuf, const void __user *ubuf)
 +static inline int
 +__copy_xstate_to_user(void __user *ubuf, const void *data, unsigned int offset, unsigned int size, unsigned int size_total)
  {
 -	if (kbuf) {
 -		memcpy(dst, kbuf + offset, size);
 -	} else {
 -		if (copy_from_user(dst, ubuf + offset, size))
 +	if (!size)
 +		return 0;
 +
 +	if (offset < size_total) {
 +		unsigned int copy = min(size, size_total - offset);
 +
 +		if (__copy_to_user(ubuf + offset, data, copy))
  			return -EFAULT;
  	}
  	return 0;
* Unmerged path arch/x86/include/asm/fpu/xstate.h
* Unmerged path arch/x86/kernel/fpu/regset.c
* Unmerged path arch/x86/kernel/fpu/xstate.c
