KVM: x86: avoid loading PDPTRs after migration when possible

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Maxim Levitsky <mlevitsk@redhat.com>
commit 158a48ecf776d0ebc916befcb0dc0862f136a31f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/158a48ec.failed

if new KVM_*_SREGS2 ioctls are used, the PDPTRs are
a part of the migration state and are correctly
restored by those ioctls.

	Signed-off-by: Maxim Levitsky <mlevitsk@redhat.com>
Message-Id: <20210607090203.133058-9-mlevitsk@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 158a48ecf776d0ebc916befcb0dc0862f136a31f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index e95615b31a99,1727178b8961..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -813,6 -820,7 +813,10 @@@ int load_pdptrs(struct kvm_vcpu *vcpu, 
  
  	memcpy(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs));
  	kvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);
++<<<<<<< HEAD
++=======
+ 	vcpu->arch.pdptrs_from_userspace = false;
++>>>>>>> 158a48ecf776 (KVM: x86: avoid loading PDPTRs after migration when possible)
  
  out:
  
@@@ -10135,13 -10236,42 +10139,38 @@@ skip_protected_regs
  	if (pending_vec < max_bits) {
  		kvm_queue_interrupt(vcpu, pending_vec, false);
  		pr_debug("Set back pending irq %d\n", pending_vec);
 -		kvm_make_request(KVM_REQ_EVENT, vcpu);
  	}
 -	return 0;
 -}
  
 -static int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)
 -{
 -	int mmu_reset_needed = 0;
 -	bool valid_pdptrs = sregs2->flags & KVM_SREGS2_FLAGS_PDPTRS_VALID;
 -	bool pae = (sregs2->cr0 & X86_CR0_PG) && (sregs2->cr4 & X86_CR4_PAE) &&
 -		!(sregs2->efer & EFER_LMA);
 -	int i, ret;
 +	kvm_make_request(KVM_REQ_EVENT, vcpu);
  
++<<<<<<< HEAD
 +	ret = 0;
 +out:
 +	return ret;
++=======
+ 	if (sregs2->flags & ~KVM_SREGS2_FLAGS_PDPTRS_VALID)
+ 		return -EINVAL;
+ 
+ 	if (valid_pdptrs && (!pae || vcpu->arch.guest_state_protected))
+ 		return -EINVAL;
+ 
+ 	ret = __set_sregs_common(vcpu, (struct kvm_sregs *)sregs2,
+ 				 &mmu_reset_needed, !valid_pdptrs);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (valid_pdptrs) {
+ 		for (i = 0; i < 4 ; i++)
+ 			kvm_pdptr_write(vcpu, i, sregs2->pdptrs[i]);
+ 
+ 		kvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);
+ 		mmu_reset_needed = 1;
+ 		vcpu->arch.pdptrs_from_userspace = true;
+ 	}
+ 	if (mmu_reset_needed)
+ 		kvm_mmu_reset_context(vcpu);
+ 	return 0;
++>>>>>>> 158a48ecf776 (KVM: x86: avoid loading PDPTRs after migration when possible)
  }
  
  int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 51153c61e071..9fec1b235838 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -858,6 +858,12 @@ struct kvm_vcpu_arch {
 	/* Protected Guests */
 	bool guest_state_protected;
 
+	/*
+	 * Set when PDPTS were loaded directly by the userspace without
+	 * reading the guest memory
+	 */
+	bool pdptrs_from_userspace;
+
 #if IS_ENABLED(CONFIG_HYPERV)
 	hpa_t hv_root_tdp;
 #endif
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 776469a3419a..6c4c6f509c2f 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -1387,7 +1387,8 @@ static bool svm_get_nested_state_pages(struct kvm_vcpu *vcpu)
 	if (WARN_ON(!is_guest_mode(vcpu)))
 		return true;
 
-	if (!nested_npt_enabled(svm) && is_pae_paging(vcpu))
+	if (!vcpu->arch.pdptrs_from_userspace &&
+	    !nested_npt_enabled(svm) && is_pae_paging(vcpu))
 		/*
 		 * Reload the guest's PDPTRs since after a migration
 		 * the guest CR3 might be restored prior to setting the nested
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 678193d5b717..80a05acf4bf3 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -3139,7 +3139,8 @@ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 	struct page *page;
 	u64 hpa;
 
-	if (!nested_cpu_has_ept(vmcs12) && is_pae_paging(vcpu)) {
+	if (!vcpu->arch.pdptrs_from_userspace &&
+	    !nested_cpu_has_ept(vmcs12) && is_pae_paging(vcpu)) {
 		/*
 		 * Reload the guest's PDPTRs since after a migration
 		 * the guest CR3 might be restored prior to setting the nested
* Unmerged path arch/x86/kvm/x86.c
