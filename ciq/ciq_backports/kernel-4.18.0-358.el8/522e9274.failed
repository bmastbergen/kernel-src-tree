x86/fpu: Deduplicate copy_uabi_from_user/kernel_to_xstate()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 522e92743b35351bda1b6a9136560f833a9c2490
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/522e9274.failed

copy_uabi_from_user_to_xstate() and copy_uabi_from_kernel_to_xstate() are
almost identical except for the copy function.

Unify them.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Acked-by: Andy Lutomirski <luto@kernel.org>
Link: https://lkml.kernel.org/r/20210623121454.414215896@linutronix.de
(cherry picked from commit 522e92743b35351bda1b6a9136560f833a9c2490)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/kernel/fpu/xstate.c
index 4641d3145e59,0eb42a1b11e1..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -994,235 -953,142 +994,245 @@@ int arch_set_user_pkey_access(struct ta
  }
  #endif /* ! CONFIG_ARCH_HAS_PKEYS */
  
++<<<<<<< HEAD
 +/*
 + * Weird legacy quirk: SSE and YMM states store information in the
 + * MXCSR and MXCSR_FLAGS fields of the FP area. That means if the FP
 + * area is marked as unused in the xfeatures header, we need to copy
 + * MXCSR and MXCSR_FLAGS if either SSE or YMM are in use.
 + */
 +static inline bool xfeatures_mxcsr_quirk(u64 xfeatures)
 +{
 +	if (!(xfeatures & (XFEATURE_MASK_SSE|XFEATURE_MASK_YMM)))
 +		return false;
 +
 +	if (xfeatures & XFEATURE_MASK_FP)
 +		return false;
 +
 +	return true;
 +}
 +
 +static void fill_gap(unsigned to, void **kbuf, unsigned *pos, unsigned *count)
++=======
+ static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
+ 			 void *init_xstate, unsigned int size)
++>>>>>>> 522e92743b35 (x86/fpu: Deduplicate copy_uabi_from_user/kernel_to_xstate())
 +{
 +	if (*pos < to) {
 +		unsigned size = to - *pos;
 +
 +		if (size > *count)
 +			size = *count;
 +		memcpy(*kbuf, (void *)&init_fpstate.xsave + *pos, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
 +}
 +
 +static void copy_part(unsigned offset, unsigned size, void *from,
 +			void **kbuf, unsigned *pos, unsigned *count)
  {
 -	membuf_write(to, from_xstate ? xstate : init_xstate, size);
 +	fill_gap(offset, kbuf, pos, count);
 +	if (size > *count)
 +		size = *count;
 +	if (size) {
 +		memcpy(*kbuf, from, size);
 +		*kbuf += size;
 +		*pos += size;
 +		*count -= size;
 +	}
  }
  
 -/**
 - * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer
 - * @to:		membuf descriptor
 - * @xsave:	The kernel xstate buffer to copy from
 - * @copy_mode:	The requested copy mode
 - *
 - * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming
 - * format, i.e. from the kernel internal hardware dependent storage format
 - * to the requested @mode. UABI XSTATE is always uncompacted!
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a kernel-space ptrace buffer.
   *
 - * It supports partial copy but @to.pos always starts from zero.
 + * It supports partial copy but pos always starts from zero. This is called
 + * from xstateregs_get() and there we check the CPU has XSAVES.
   */
 -void copy_xstate_to_uabi_buf(struct membuf to, struct xregs_state *xsave,
 -			     enum xstate_copy_mode copy_mode)
 +int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
  {
 -	const unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 -	struct xregs_state *xinit = &init_fpstate.xsave;
  	struct xstate_header header;
 -	unsigned int zerofrom;
 +	const unsigned off_mxcsr = offsetof(struct fxregs_state, mxcsr);
 +	unsigned count = size_total;
  	int i;
  
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
 +
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
  	header.xfeatures = xsave->header.xfeatures;
 +	header.xfeatures &= xfeatures_mask_user();
 +
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(0, off_mxcsr,
 +			  &xsave->i387, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM))
 +		copy_part(off_mxcsr, MXCSR_AND_FLAGS_SIZE,
 +			  &xsave->i387.mxcsr, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_FP)
 +		copy_part(offsetof(struct fxregs_state, st_space), 128,
 +			  &xsave->i387.st_space, &kbuf, &offset_start, &count);
 +	if (header.xfeatures & XFEATURE_MASK_SSE)
 +		copy_part(xstate_offsets[XFEATURE_SSE], 256,
 +			  &xsave->i387.xmm_space, &kbuf, &offset_start, &count);
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	copy_part(offsetof(struct fxregs_state, sw_reserved), 48,
 +		  xstate_fx_sw_bytes, &kbuf, &offset_start, &count);
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	copy_part(offsetof(struct xregs_state, header), sizeof(header),
 +		  &header, &kbuf, &offset_start, &count);
  
 -	/* Mask out the feature bits depending on copy mode */
 -	switch (copy_mode) {
 -	case XSTATE_COPY_FP:
 -		header.xfeatures &= XFEATURE_MASK_FP;
 -		break;
 +	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
 +		/*
 +		 * Copy only in-use xstates:
 +		 */
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
  
 -	case XSTATE_COPY_FX:
 -		header.xfeatures &= XFEATURE_MASK_FP | XFEATURE_MASK_SSE;
 -		break;
 +			copy_part(xstate_offsets[i], xstate_sizes[i],
 +				  src, &kbuf, &offset_start, &count);
 +		}
  
 -	case XSTATE_COPY_XSAVE:
 -		header.xfeatures &= xfeatures_mask_user();
 -		break;
  	}
 +	fill_gap(size_total, &kbuf, &offset_start, &count);
  
 -	/* Copy FP state up to MXCSR */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_FP, &to, &xsave->i387,
 -		     &xinit->i387, off_mxcsr);
 +	return 0;
 +}
  
 -	/* Copy MXCSR when SSE or YMM are set in the feature mask */
 -	copy_feature(header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM),
 -		     &to, &xsave->i387.mxcsr, &xinit->i387.mxcsr,
 -		     MXCSR_AND_FLAGS_SIZE);
 +static inline int
 +__copy_xstate_to_user(void __user *ubuf, const void *data, unsigned int offset, unsigned int size, unsigned int size_total)
 +{
 +	if (!size)
 +		return 0;
  
 -	/* Copy the remaining FP state */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_FP,
 -		     &to, &xsave->i387.st_space, &xinit->i387.st_space,
 -		     sizeof(xsave->i387.st_space));
 +	if (offset < size_total) {
 +		unsigned int copy = min(size, size_total - offset);
  
 -	/* Copy the SSE state - shared with YMM, but independently managed */
 -	copy_feature(header.xfeatures & XFEATURE_MASK_SSE,
 -		     &to, &xsave->i387.xmm_space, &xinit->i387.xmm_space,
 -		     sizeof(xsave->i387.xmm_space));
 +		if (__copy_to_user(ubuf + offset, data, copy))
 +			return -EFAULT;
 +	}
 +	return 0;
 +}
  
 -	if (copy_mode != XSTATE_COPY_XSAVE)
 -		goto out;
 +/*
 + * Convert from kernel XSAVES compacted format to standard format and copy
 + * to a user-space buffer. It supports partial copy but pos always starts from
 + * zero. This is called from xstateregs_get() and there we check the CPU
 + * has XSAVES.
 + */
 +int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset_start, unsigned int size_total)
 +{
 +	unsigned int offset, size;
 +	int ret, i;
 +	struct xstate_header header;
  
 -	/* Zero the padding area */
 -	membuf_zero(&to, sizeof(xsave->i387.padding));
 +	/*
 +	 * Currently copy_regset_to_user() starts from pos 0:
 +	 */
 +	if (unlikely(offset_start != 0))
 +		return -EFAULT;
  
 -	/* Copy xsave->i387.sw_reserved */
 -	membuf_write(&to, xstate_fx_sw_bytes, sizeof(xsave->i387.sw_reserved));
 +	/*
 +	 * The destination is a ptrace buffer; we put in only user xstates:
 +	 */
 +	memset(&header, 0, sizeof(header));
 +	header.xfeatures = xsave->header.xfeatures;
 +	header.xfeatures &= xfeatures_mask_user();
  
 -	/* Copy the user space relevant state of @xsave->header */
 -	membuf_write(&to, &header, sizeof(header));
 +	/*
 +	 * Copy xregs_state->header:
 +	 */
 +	offset = offsetof(struct xregs_state, header);
 +	size = sizeof(header);
  
 -	zerofrom = offsetof(struct xregs_state, extended_state_area);
 +	ret = __copy_xstate_to_user(ubuf, &header, offset, size, size_total);
 +	if (ret)
 +		return ret;
  
 -	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
 +	for (i = 0; i < XFEATURE_MAX; i++) {
  		/*
 -		 * The ptrace buffer is in non-compacted XSAVE format.
 -		 * In non-compacted format disabled features still occupy
 -		 * state space, but there is no state to copy from in the
 -		 * compacted init_fpstate. The gap tracking will zero this
 -		 * later.
 +		 * Copy only in-use xstates:
  		 */
 -		if (!(xfeatures_mask_user() & BIT_ULL(i)))
 -			continue;
 +		if ((header.xfeatures >> i) & 1) {
 +			void *src = __raw_xsave_addr(xsave, i);
  
 -		/*
 -		 * If there was a feature or alignment gap, zero the space
 -		 * in the destination buffer.
 -		 */
 -		if (zerofrom < xstate_offsets[i])
 -			membuf_zero(&to, xstate_offsets[i] - zerofrom);
 +			offset = xstate_offsets[i];
 +			size = xstate_sizes[i];
  
 -		copy_feature(header.xfeatures & BIT_ULL(i), &to,
 -			     __raw_xsave_addr(xsave, i),
 -			     __raw_xsave_addr(xinit, i),
 -			     xstate_sizes[i]);
 +			/* The next component has to fit fully into the output buffer: */
 +			if (offset + size > size_total)
 +				break;
 +
 +			ret = __copy_xstate_to_user(ubuf, src, offset, size, size_total);
 +			if (ret)
 +				return ret;
 +		}
  
 -		/*
 -		 * Keep track of the last copied state in the non-compacted
 -		 * target buffer for gap zeroing.
 -		 */
 -		zerofrom = xstate_offsets[i] + xstate_sizes[i];
  	}
  
 -out:
 -	if (to.left)
 -		membuf_zero(&to, to.left);
 +	if (xfeatures_mxcsr_quirk(header.xfeatures)) {
 +		offset = offsetof(struct fxregs_state, mxcsr);
 +		size = MXCSR_AND_FLAGS_SIZE;
 +		__copy_xstate_to_user(ubuf, &xsave->i387.mxcsr, offset, size, size_total);
 +	}
 +
 +	/*
 +	 * Fill xsave->i387.sw_reserved value for ptrace frame:
 +	 */
 +	offset = offsetof(struct fxregs_state, sw_reserved);
 +	size = sizeof(xstate_fx_sw_bytes);
 +
 +	ret = __copy_xstate_to_user(ubuf, xstate_fx_sw_bytes, offset, size, size_total);
 +	if (ret)
 +		return ret;
 +
 +	return 0;
  }
  
- static inline bool mxcsr_valid(struct xstate_header *hdr, const u32 *mxcsr)
+ static int copy_from_buffer(void *dst, unsigned int offset, unsigned int size,
+ 			    const void *kbuf, const void __user *ubuf)
  {
- 	u64 mask = XFEATURE_MASK_FP | XFEATURE_MASK_SSE | XFEATURE_MASK_YMM;
- 
- 	/* Only check if it is in use */
- 	if (hdr->xfeatures & mask) {
- 		/* Reserved bits in MXCSR must be zero. */
- 		if (*mxcsr & ~mxcsr_feature_mask)
- 			return false;
+ 	if (kbuf) {
+ 		memcpy(dst, kbuf + offset, size);
+ 	} else {
+ 		if (copy_from_user(dst, ubuf + offset, size))
+ 			return -EFAULT;
  	}
- 	return true;
+ 	return 0;
  }
  
++<<<<<<< HEAD
 +/*
 + * Convert from a ptrace standard-format kernel buffer to kernel XSAVE[S] format
 + * and copy to the target thread. This is called from xstateregs_set().
 + */
 +int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf)
++=======
+ 
+ static int copy_uabi_to_xstate(struct xregs_state *xsave, const void *kbuf,
+ 			       const void __user *ubuf)
++>>>>>>> 522e92743b35 (x86/fpu: Deduplicate copy_uabi_from_user/kernel_to_xstate())
  {
  	unsigned int offset, size;
- 	int i;
  	struct xstate_header hdr;
+ 	u64 mask;
+ 	int i;
  
  	offset = offsetof(struct xregs_state, header);
- 	size = sizeof(hdr);
- 
- 	memcpy(&hdr, kbuf + offset, size);
+ 	if (copy_from_buffer(&hdr, offset, sizeof(hdr), kbuf, ubuf))
+ 		return -EFAULT;
  
  	if (validate_user_xstate_header(&hdr))
  		return -EINVAL;
@@@ -1268,54 -1156,10 +1300,9 @@@ int copy_uabi_from_kernel_to_xstate(str
   * XSAVE[S] format and copy to the target thread. This is called from the
   * sigreturn() and rt_sigreturn() system calls.
   */
 -int copy_sigframe_from_user_to_xstate(struct xregs_state *xsave,
 -				      const void __user *ubuf)
 +int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
  {
- 	unsigned int offset, size;
- 	int i;
- 	struct xstate_header hdr;
- 
- 	offset = offsetof(struct xregs_state, header);
- 	size = sizeof(hdr);
- 
- 	if (copy_from_user(&hdr, ubuf + offset, size))
- 		return -EFAULT;
- 
- 	if (validate_user_xstate_header(&hdr))
- 		return -EINVAL;
- 
- 	for (i = 0; i < XFEATURE_MAX; i++) {
- 		u64 mask = ((u64)1 << i);
- 
- 		if (hdr.xfeatures & mask) {
- 			void *dst = __raw_xsave_addr(xsave, i);
- 
- 			offset = xstate_offsets[i];
- 			size = xstate_sizes[i];
- 
- 			if (copy_from_user(dst, ubuf + offset, size))
- 				return -EFAULT;
- 		}
- 	}
- 
- 	if (xfeatures_mxcsr_quirk(hdr.xfeatures)) {
- 		offset = offsetof(struct fxregs_state, mxcsr);
- 		size = MXCSR_AND_FLAGS_SIZE;
- 		if (copy_from_user(&xsave->i387.mxcsr, ubuf + offset, size))
- 			return -EFAULT;
- 	}
- 
- 	/*
- 	 * The state that came in from userspace was user-state only.
- 	 * Mask all the user states out of 'xfeatures':
- 	 */
- 	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;
- 
- 	/*
- 	 * Add back in the features that came in from userspace:
- 	 */
- 	xsave->header.xfeatures |= hdr.xfeatures;
- 
- 	return 0;
+ 	return copy_uabi_to_xstate(xsave, NULL, ubuf);
  }
  
  /**
* Unmerged path arch/x86/kernel/fpu/xstate.c
