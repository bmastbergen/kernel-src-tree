RDMA/rxe: Disallow MR dereg and invalidate when bound

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Bob Pearson <rpearsonhpe@gmail.com>
commit 570d2b99d00d9e023328c0a0b8000ab485113384
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/570d2b99.failed

Check that an MR has no bound MWs before allowing a dereg or invalidate
operation.

Link: https://lore.kernel.org/r/20210608042552.33275-11-rpearsonhpe@gmail.com
	Signed-off-by: Bob Pearson <rpearsonhpe@gmail.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 570d2b99d00d9e023328c0a0b8000ab485113384)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/sw/rxe/rxe_loc.h
#	drivers/infiniband/sw/rxe/rxe_mr.c
diff --cc drivers/infiniband/sw/rxe/rxe_loc.h
index e6f574973298,1ddb20855dee..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_loc.h
+++ b/drivers/infiniband/sw/rxe/rxe_loc.h
@@@ -71,42 -71,25 +71,48 @@@ struct rxe_mmap_info *rxe_create_mmap_i
  int rxe_mmap(struct ib_ucontext *context, struct vm_area_struct *vma);
  
  /* rxe_mr.c */
 +enum copy_direction {
 +	to_mr_obj,
 +	from_mr_obj,
 +};
 +
  u8 rxe_get_next_key(u32 last_key);
  void rxe_mr_init_dma(struct rxe_pd *pd, int access, struct rxe_mr *mr);
 +
  int rxe_mr_init_user(struct rxe_pd *pd, u64 start, u64 length, u64 iova,
 -		     int access, struct rxe_mr *mr);
 +		     int access, struct ib_udata *udata, struct rxe_mr *mr);
 +
  int rxe_mr_init_fast(struct rxe_pd *pd, int max_pages, struct rxe_mr *mr);
 +
  int rxe_mr_copy(struct rxe_mr *mr, u64 iova, void *addr, int length,
 -		enum rxe_mr_copy_dir dir, u32 *crcp);
 +		enum copy_direction dir, u32 *crcp);
 +
  int copy_data(struct rxe_pd *pd, int access,
  	      struct rxe_dma_info *dma, void *addr, int length,
 -	      enum rxe_mr_copy_dir dir, u32 *crcp);
 +	      enum copy_direction dir, u32 *crcp);
 +
  void *iova_to_vaddr(struct rxe_mr *mr, u64 iova, int length);
 +
 +enum lookup_type {
 +	lookup_local,
 +	lookup_remote,
 +};
 +
  struct rxe_mr *lookup_mr(struct rxe_pd *pd, int access, u32 key,
 -			 enum rxe_mr_lookup_type type);
 +			 enum lookup_type type);
 +
  int mr_check_range(struct rxe_mr *mr, u64 iova, size_t length);
++<<<<<<< HEAD
 +
++=======
+ int advance_dma_data(struct rxe_dma_info *dma, unsigned int length);
+ int rxe_invalidate_mr(struct rxe_qp *qp, u32 rkey);
+ int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
++>>>>>>> 570d2b99d00d (RDMA/rxe: Disallow MR dereg and invalidate when bound)
  void rxe_mr_cleanup(struct rxe_pool_entry *arg);
  
 +int advance_dma_data(struct rxe_dma_info *dma, unsigned int length);
 +
  /* rxe_mw.c */
  int rxe_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata);
  int rxe_dealloc_mw(struct ib_mw *ibmw);
diff --cc drivers/infiniband/sw/rxe/rxe_mr.c
index bbc664b45cb6,7f169329a8bf..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_mr.c
+++ b/drivers/infiniband/sw/rxe/rxe_mr.c
@@@ -540,3 -525,72 +540,75 @@@ struct rxe_mr *lookup_mr(struct rxe_pd 
  
  	return mr;
  }
++<<<<<<< HEAD
++=======
+ 
+ int rxe_invalidate_mr(struct rxe_qp *qp, u32 rkey)
+ {
+ 	struct rxe_dev *rxe = to_rdev(qp->ibqp.device);
+ 	struct rxe_mr *mr;
+ 	int ret;
+ 
+ 	mr = rxe_pool_get_index(&rxe->mr_pool, rkey >> 8);
+ 	if (!mr) {
+ 		pr_err("%s: No MR for rkey %#x\n", __func__, rkey);
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	if (rkey != mr->ibmr.rkey) {
+ 		pr_err("%s: rkey (%#x) doesn't match mr->ibmr.rkey (%#x)\n",
+ 			__func__, rkey, mr->ibmr.rkey);
+ 		ret = -EINVAL;
+ 		goto err_drop_ref;
+ 	}
+ 
+ 	if (atomic_read(&mr->num_mw) > 0) {
+ 		pr_warn("%s: Attempt to invalidate an MR while bound to MWs\n",
+ 			__func__);
+ 		ret = -EINVAL;
+ 		goto err_drop_ref;
+ 	}
+ 
+ 	mr->state = RXE_MR_STATE_FREE;
+ 	ret = 0;
+ 
+ err_drop_ref:
+ 	rxe_drop_ref(mr);
+ err:
+ 	return ret;
+ }
+ 
+ int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
+ {
+ 	struct rxe_mr *mr = to_rmr(ibmr);
+ 
+ 	if (atomic_read(&mr->num_mw) > 0) {
+ 		pr_warn("%s: Attempt to deregister an MR while bound to MWs\n",
+ 			__func__);
+ 		return -EINVAL;
+ 	}
+ 
+ 	mr->state = RXE_MR_STATE_ZOMBIE;
+ 	rxe_drop_ref(mr_pd(mr));
+ 	rxe_drop_index(mr);
+ 	rxe_drop_ref(mr);
+ 
+ 	return 0;
+ }
+ 
+ void rxe_mr_cleanup(struct rxe_pool_entry *arg)
+ {
+ 	struct rxe_mr *mr = container_of(arg, typeof(*mr), pelem);
+ 	int i;
+ 
+ 	ib_umem_release(mr->umem);
+ 
+ 	if (mr->map) {
+ 		for (i = 0; i < mr->num_map; i++)
+ 			kfree(mr->map[i]);
+ 
+ 		kfree(mr->map);
+ 	}
+ }
++>>>>>>> 570d2b99d00d (RDMA/rxe: Disallow MR dereg and invalidate when bound)
* Unmerged path drivers/infiniband/sw/rxe/rxe_loc.h
* Unmerged path drivers/infiniband/sw/rxe/rxe_mr.c
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 38b4d0e125b5..f41df3e65366 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -959,17 +959,6 @@ static struct ib_mr *rxe_reg_user_mr(struct ib_pd *ibpd,
 	return ERR_PTR(err);
 }
 
-static int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
-{
-	struct rxe_mr *mr = to_rmr(ibmr);
-
-	mr->state = RXE_MR_STATE_ZOMBIE;
-	rxe_drop_ref(mr_pd(mr));
-	rxe_drop_index(mr);
-	rxe_drop_ref(mr);
-	return 0;
-}
-
 static struct ib_mr *rxe_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,
 				  u32 max_num_sg)
 {
