nvme: fix handling of large MDTS values

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Bart Van Assche <bvanassche@acm.org>
commit 8609c63fce58e94d82f6b6bf29c7806062e2e867
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/8609c63f.failed

Instead of triggering an integer overflow and undefined behavior if MDTS is
large, set max_hw_sectors to UINT_MAX.

	Signed-off-by: Bart Van Assche <bvanassche@acm.org>
	Reviewed-by: Keith Busch <kbusch@kernel.org>
[hch: rebased to account for the new nvme_mps_to_sectors helper]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8609c63fce58e94d82f6b6bf29c7806062e2e867)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/core.c
diff --cc drivers/nvme/host/core.c
index beaf8705f770,314705da2c10..000000000000
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -3127,28 -3047,74 +3127,94 @@@ out
  	return 0;
  }
  
++<<<<<<< HEAD
 +/*
 + * Initialize the cached copies of the Identify data and various controller
 + * register in our nvme_ctrl structure.  This should be called as soon as
 + * the admin queue is fully up and running.
 + */
 +int nvme_init_ctrl_finish(struct nvme_ctrl *ctrl)
++=======
+ static inline u32 nvme_mps_to_sectors(struct nvme_ctrl *ctrl, u32 units)
+ {
+ 	u32 page_shift = NVME_CAP_MPSMIN(ctrl->cap) + 12, val;
+ 
+ 	if (check_shl_overflow(1U, units + page_shift - 9, &val))
+ 		return UINT_MAX;
+ 	return val;
+ }
+ 
+ static int nvme_init_non_mdts_limits(struct nvme_ctrl *ctrl)
+ {
+ 	struct nvme_command c = { };
+ 	struct nvme_id_ctrl_nvm *id;
+ 	int ret;
+ 
+ 	if (ctrl->oncs & NVME_CTRL_ONCS_DSM) {
+ 		ctrl->max_discard_sectors = UINT_MAX;
+ 		ctrl->max_discard_segments = NVME_DSM_MAX_RANGES;
+ 	} else {
+ 		ctrl->max_discard_sectors = 0;
+ 		ctrl->max_discard_segments = 0;
+ 	}
+ 
+ 	/*
+ 	 * Even though NVMe spec explicitly states that MDTS is not applicable
+ 	 * to the write-zeroes, we are cautious and limit the size to the
+ 	 * controllers max_hw_sectors value, which is based on the MDTS field
+ 	 * and possibly other limiting factors.
+ 	 */
+ 	if ((ctrl->oncs & NVME_CTRL_ONCS_WRITE_ZEROES) &&
+ 	    !(ctrl->quirks & NVME_QUIRK_DISABLE_WRITE_ZEROES))
+ 		ctrl->max_zeroes_sectors = ctrl->max_hw_sectors;
+ 	else
+ 		ctrl->max_zeroes_sectors = 0;
+ 
+ 	if (nvme_ctrl_limited_cns(ctrl))
+ 		return 0;
+ 
+ 	id = kzalloc(sizeof(*id), GFP_KERNEL);
+ 	if (!id)
+ 		return 0;
+ 
+ 	c.identify.opcode = nvme_admin_identify;
+ 	c.identify.cns = NVME_ID_CNS_CS_CTRL;
+ 	c.identify.csi = NVME_CSI_NVM;
+ 
+ 	ret = nvme_submit_sync_cmd(ctrl->admin_q, &c, id, sizeof(*id));
+ 	if (ret)
+ 		goto free_data;
+ 
+ 	if (id->dmrl)
+ 		ctrl->max_discard_segments = id->dmrl;
+ 	if (id->dmrsl)
+ 		ctrl->max_discard_sectors = le32_to_cpu(id->dmrsl);
+ 	if (id->wzsl)
+ 		ctrl->max_zeroes_sectors = nvme_mps_to_sectors(ctrl, id->wzsl);
+ 
+ free_data:
+ 	kfree(id);
+ 	return ret;
+ }
+ 
+ static int nvme_init_identify(struct nvme_ctrl *ctrl)
++>>>>>>> 8609c63fce58 (nvme: fix handling of large MDTS values)
  {
  	struct nvme_id_ctrl *id;
 +	int ret, page_shift;
  	u32 max_hw_sectors;
  	bool prev_apst_enabled;
 -	int ret;
 +
 +	ret = ctrl->ops->reg_read32(ctrl, NVME_REG_VS, &ctrl->vs);
 +	if (ret) {
 +		dev_err(ctrl->device, "Reading VS failed (%d)\n", ret);
 +		return ret;
 +	}
 +	page_shift = NVME_CAP_MPSMIN(ctrl->cap) + 12;
 +	ctrl->sqsize = min_t(u16, NVME_CAP_MQES(ctrl->cap), ctrl->sqsize);
 +
 +	if (ctrl->vs >= NVME_VS(1, 1, 0))
 +		ctrl->subsystem = NVME_CAP_NSSRC(ctrl->cap);
  
  	ret = nvme_identify_ctrl(ctrl, &id);
  	if (ret) {
* Unmerged path drivers/nvme/host/core.c
