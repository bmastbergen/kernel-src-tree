sched/fair: Correctly insert cfs_rq's to list on unthrottle

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Odin Ugedal <odin@uged.al>
commit a7b359fc6a37faaf472125867c8dc5a068c90982
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/a7b359fc.failed

Fix an issue where fairness is decreased since cfs_rq's can end up not
being decayed properly. For two sibling control groups with the same
priority, this can often lead to a load ratio of 99/1 (!!).

This happens because when a cfs_rq is throttled, all the descendant
cfs_rq's will be removed from the leaf list. When they initial cfs_rq
is unthrottled, it will currently only re add descendant cfs_rq's if
they have one or more entities enqueued. This is not a perfect
heuristic.

Instead, we insert all cfs_rq's that contain one or more enqueued
entities, or it its load is not completely decayed.

Can often lead to situations like this for equally weighted control
groups:

  $ ps u -C stress
  USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
  root       10009 88.8  0.0   3676   100 pts/1    R+   11:04   0:13 stress --cpu 1
  root       10023  3.0  0.0   3676   104 pts/1    R+   11:04   0:00 stress --cpu 1

Fixes: 31bc6aeaab1d ("sched/fair: Optimize update_blocked_averages()")
[vingo: !SMP build fix]
	Signed-off-by: Odin Ugedal <odin@uged.al>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lore.kernel.org/r/20210612112815.61678-1-odin@uged.al
(cherry picked from commit a7b359fc6a37faaf472125867c8dc5a068c90982)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 199e9addfd3d,bfaa6e1f6067..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -7872,23 -8019,6 +7895,26 @@@ static bool __update_blocked_others(str
  
  #ifdef CONFIG_FAIR_GROUP_SCHED
  
++<<<<<<< HEAD
 +static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)
 +{
 +        if (cfs_rq->load.weight)
 +                return false;
 +
 +        if (cfs_rq->avg.load_sum)
 +                return false;
 +
 +        if (cfs_rq->avg.util_sum)
 +                return false;
 +
 +	if (cfs_rq->avg.runnable_sum)
 +		return false;
 +
 +        return true;
 +}
 +
++=======
++>>>>>>> a7b359fc6a37 (sched/fair: Correctly insert cfs_rq's to list on unthrottle)
  static bool __update_blocked_fair(struct rq *rq, bool *done)
  {
  	struct cfs_rq *cfs_rq, *pos;
* Unmerged path kernel/sched/fair.c
