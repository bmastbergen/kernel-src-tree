net: compound page support in skb_seq_read

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Willem de Bruijn <willemb@google.com>
commit 97550f6fa59254435d864b92603de3ca4b5a99f8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/97550f6f.failed

skb_seq_read iterates over an skb, returning pointer and length of
the next data range with each call.

It relies on kmap_atomic to access highmem pages when needed.

An skb frag may be backed by a compound page, but kmap_atomic maps
only a single page. There are not enough kmap slots to always map all
pages concurrently.

Instead, if kmap_atomic is needed, iterate over each page.

As this increases the number of calls, avoid this unless needed.
The necessary condition is captured in skb_frag_must_loop.

I tried to make the change as obvious as possible. It should be easy
to verify that nothing changes if skb_frag_must_loop returns false.

Tested:
  On an x86 platform with
    CONFIG_HIGHMEM=y
    CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP=y
    CONFIG_NETFILTER_XT_MATCH_STRING=y

  Run
    ip link set dev lo mtu 1500
    iptables -A OUTPUT -m string --string 'badstring' -algo bm -j ACCEPT
    dd if=/dev/urandom of=in bs=1M count=20
    nc -l -p 8000 > /dev/null &
    nc -w 1 -q 0 localhost 8000 < in

	Signed-off-by: Willem de Bruijn <willemb@google.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit 97550f6fa59254435d864b92603de3ca4b5a99f8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
diff --cc net/core/skbuff.c
index 7552c46dea85,0da035c1e53f..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -3359,14 -3497,27 +3360,31 @@@ next_skb
  		st->stepped_offset += skb_headlen(st->cur_skb);
  
  	while (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {
+ 		unsigned int pg_idx, pg_off, pg_sz;
+ 
  		frag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];
- 		block_limit = skb_frag_size(frag) + st->stepped_offset;
  
+ 		pg_idx = 0;
+ 		pg_off = skb_frag_off(frag);
+ 		pg_sz = skb_frag_size(frag);
+ 
+ 		if (skb_frag_must_loop(skb_frag_page(frag))) {
+ 			pg_idx = (pg_off + st->frag_off) >> PAGE_SHIFT;
+ 			pg_off = offset_in_page(pg_off + st->frag_off);
+ 			pg_sz = min_t(unsigned int, pg_sz - st->frag_off,
+ 						    PAGE_SIZE - pg_off);
+ 		}
+ 
+ 		block_limit = pg_sz + st->stepped_offset;
  		if (abs_offset < block_limit) {
  			if (!st->frag_data)
- 				st->frag_data = kmap_atomic(skb_frag_page(frag));
+ 				st->frag_data = kmap_atomic(skb_frag_page(frag) + pg_idx);
  
++<<<<<<< HEAD
 +			*data = (u8 *) st->frag_data + frag->page_offset +
++=======
+ 			*data = (u8 *)st->frag_data + pg_off +
++>>>>>>> 97550f6fa592 (net: compound page support in skb_seq_read)
  				(abs_offset - st->stepped_offset);
  
  			return block_limit - abs_offset;
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index a7d918d3a600..a6b008c006c4 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -1227,6 +1227,7 @@ struct skb_seq_state {
 	struct sk_buff	*root_skb;
 	struct sk_buff	*cur_skb;
 	__u8		*frag_data;
+	__u32		frag_off;
 };
 
 void skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,
* Unmerged path net/core/skbuff.c
