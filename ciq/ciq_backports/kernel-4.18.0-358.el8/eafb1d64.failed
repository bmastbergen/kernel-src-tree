mm, slub: don't call flush_all() from slab_debug_trace_open()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit eafb1d64030abf5f885026c2074d120c13e0ca9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/eafb1d64.failed

slab_debug_trace_open() can only be called on caches with SLAB_STORE_USER flag
and as with all slub debugging flags, such caches avoid cpu or percpu partial
slabs altogether, so there's nothing to flush.

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: Christoph Lameter <cl@linux.com>
(cherry picked from commit eafb1d64030abf5f885026c2074d120c13e0ca9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index e4751a240249,f6063ec97a55..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -5755,6 -5731,170 +5755,173 @@@ static int __init slab_sysfs_init(void
  __initcall(slab_sysfs_init);
  #endif /* CONFIG_SYSFS */
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_SLUB_DEBUG) && defined(CONFIG_DEBUG_FS)
+ static int slab_debugfs_show(struct seq_file *seq, void *v)
+ {
+ 
+ 	struct location *l;
+ 	unsigned int idx = *(unsigned int *)v;
+ 	struct loc_track *t = seq->private;
+ 
+ 	if (idx < t->count) {
+ 		l = &t->loc[idx];
+ 
+ 		seq_printf(seq, "%7ld ", l->count);
+ 
+ 		if (l->addr)
+ 			seq_printf(seq, "%pS", (void *)l->addr);
+ 		else
+ 			seq_puts(seq, "<not-available>");
+ 
+ 		if (l->sum_time != l->min_time) {
+ 			seq_printf(seq, " age=%ld/%llu/%ld",
+ 				l->min_time, div_u64(l->sum_time, l->count),
+ 				l->max_time);
+ 		} else
+ 			seq_printf(seq, " age=%ld", l->min_time);
+ 
+ 		if (l->min_pid != l->max_pid)
+ 			seq_printf(seq, " pid=%ld-%ld", l->min_pid, l->max_pid);
+ 		else
+ 			seq_printf(seq, " pid=%ld",
+ 				l->min_pid);
+ 
+ 		if (num_online_cpus() > 1 && !cpumask_empty(to_cpumask(l->cpus)))
+ 			seq_printf(seq, " cpus=%*pbl",
+ 				 cpumask_pr_args(to_cpumask(l->cpus)));
+ 
+ 		if (nr_online_nodes > 1 && !nodes_empty(l->nodes))
+ 			seq_printf(seq, " nodes=%*pbl",
+ 				 nodemask_pr_args(&l->nodes));
+ 
+ 		seq_puts(seq, "\n");
+ 	}
+ 
+ 	if (!idx && !t->count)
+ 		seq_puts(seq, "No data\n");
+ 
+ 	return 0;
+ }
+ 
+ static void slab_debugfs_stop(struct seq_file *seq, void *v)
+ {
+ }
+ 
+ static void *slab_debugfs_next(struct seq_file *seq, void *v, loff_t *ppos)
+ {
+ 	struct loc_track *t = seq->private;
+ 
+ 	v = ppos;
+ 	++*ppos;
+ 	if (*ppos <= t->count)
+ 		return v;
+ 
+ 	return NULL;
+ }
+ 
+ static void *slab_debugfs_start(struct seq_file *seq, loff_t *ppos)
+ {
+ 	return ppos;
+ }
+ 
+ static const struct seq_operations slab_debugfs_sops = {
+ 	.start  = slab_debugfs_start,
+ 	.next   = slab_debugfs_next,
+ 	.stop   = slab_debugfs_stop,
+ 	.show   = slab_debugfs_show,
+ };
+ 
+ static int slab_debug_trace_open(struct inode *inode, struct file *filep)
+ {
+ 
+ 	struct kmem_cache_node *n;
+ 	enum track_item alloc;
+ 	int node;
+ 	struct loc_track *t = __seq_open_private(filep, &slab_debugfs_sops,
+ 						sizeof(struct loc_track));
+ 	struct kmem_cache *s = file_inode(filep)->i_private;
+ 
+ 	if (strcmp(filep->f_path.dentry->d_name.name, "alloc_traces") == 0)
+ 		alloc = TRACK_ALLOC;
+ 	else
+ 		alloc = TRACK_FREE;
+ 
+ 	if (!alloc_loc_track(t, PAGE_SIZE / sizeof(struct location), GFP_KERNEL))
+ 		return -ENOMEM;
+ 
+ 	for_each_kmem_cache_node(s, node, n) {
+ 		unsigned long flags;
+ 		struct page *page;
+ 
+ 		if (!atomic_long_read(&n->nr_slabs))
+ 			continue;
+ 
+ 		spin_lock_irqsave(&n->list_lock, flags);
+ 		list_for_each_entry(page, &n->partial, slab_list)
+ 			process_slab(t, s, page, alloc);
+ 		list_for_each_entry(page, &n->full, slab_list)
+ 			process_slab(t, s, page, alloc);
+ 		spin_unlock_irqrestore(&n->list_lock, flags);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int slab_debug_trace_release(struct inode *inode, struct file *file)
+ {
+ 	struct seq_file *seq = file->private_data;
+ 	struct loc_track *t = seq->private;
+ 
+ 	free_loc_track(t);
+ 	return seq_release_private(inode, file);
+ }
+ 
+ static const struct file_operations slab_debugfs_fops = {
+ 	.open    = slab_debug_trace_open,
+ 	.read    = seq_read,
+ 	.llseek  = seq_lseek,
+ 	.release = slab_debug_trace_release,
+ };
+ 
+ static void debugfs_slab_add(struct kmem_cache *s)
+ {
+ 	struct dentry *slab_cache_dir;
+ 
+ 	if (unlikely(!slab_debugfs_root))
+ 		return;
+ 
+ 	slab_cache_dir = debugfs_create_dir(s->name, slab_debugfs_root);
+ 
+ 	debugfs_create_file("alloc_traces", 0400,
+ 		slab_cache_dir, s, &slab_debugfs_fops);
+ 
+ 	debugfs_create_file("free_traces", 0400,
+ 		slab_cache_dir, s, &slab_debugfs_fops);
+ }
+ 
+ void debugfs_slab_release(struct kmem_cache *s)
+ {
+ 	debugfs_remove_recursive(debugfs_lookup(s->name, slab_debugfs_root));
+ }
+ 
+ static int __init slab_debugfs_init(void)
+ {
+ 	struct kmem_cache *s;
+ 
+ 	slab_debugfs_root = debugfs_create_dir("slab", NULL);
+ 
+ 	list_for_each_entry(s, &slab_caches, list)
+ 		if (s->flags & SLAB_STORE_USER)
+ 			debugfs_slab_add(s);
+ 
+ 	return 0;
+ 
+ }
+ __initcall(slab_debugfs_init);
+ #endif
++>>>>>>> eafb1d64030a (mm, slub: don't call flush_all() from slab_debug_trace_open())
  /*
   * The /proc/slabinfo ABI
   */
* Unmerged path mm/slub.c
