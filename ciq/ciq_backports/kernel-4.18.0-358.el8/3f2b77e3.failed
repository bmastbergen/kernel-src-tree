mm, slub: validate slab from partial list or page allocator before making it cpu slab

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 3f2b77e35a4fc3c83132a1a1a2fc7a2c803a2514
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/3f2b77e3.failed

When we obtain a new slab page from node partial list or page allocator, we
assign it to kmem_cache_cpu, perform some checks, and if they fail, we undo
the assignment.

In order to allow doing the checks without irq disabled, restructure the code
so that the checks are done first, and kmem_cache_cpu.page assignment only
after they pass.

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
(cherry picked from commit 3f2b77e35a4fc3c83132a1a1a2fc7a2c803a2514)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 18ea2f53cb90,a5e974defcb7..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -2690,23 -2791,31 +2690,25 @@@ new_slab
  		goto redo;
  	}
  
 -	local_irq_save(flags);
 -	if (unlikely(c->page)) {
 -		local_irq_restore(flags);
 -		goto reread_page;
 -	}
 -
 -new_objects:
 -
 -	lockdep_assert_irqs_disabled();
 -
  	freelist = get_partial(s, gfpflags, node, &page);
- 	if (freelist) {
- 		c->page = page;
+ 	if (freelist)
  		goto check_new_page;
- 	}
  
 -	local_irq_restore(flags);
 -	put_cpu_ptr(s->cpu_slab);
  	page = new_slab(s, gfpflags, node);
 -	c = get_cpu_ptr(s->cpu_slab);
  
  	if (unlikely(!page)) {
  		slab_out_of_memory(s, gfpflags, node);
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	c = raw_cpu_ptr(s->cpu_slab);
 +	if (c->page)
 +		flush_slab(s, c);
 +
++=======
+ 	local_irq_save(flags);
++>>>>>>> 3f2b77e35a4f (mm, slub: validate slab from partial list or page allocator before making it cpu slab)
  	/*
  	 * No other reference to the page yet so we can
  	 * muck around with it freely without cmpxchg
@@@ -2720,10 -2828,11 +2721,14 @@@
  check_new_page:
  
  	if (kmem_cache_debug(s)) {
 -		if (!alloc_debug_processing(s, page, freelist, addr)) {
 +		if (!alloc_debug_processing(s, page, freelist, addr))
  			/* Slab failed checks. Next slab needed */
++<<<<<<< HEAD
++=======
+ 			local_irq_restore(flags);
++>>>>>>> 3f2b77e35a4f (mm, slub: validate slab from partial list or page allocator before making it cpu slab)
  			goto new_slab;
 -		} else {
 +		else
  			/*
  			 * For debug case, we don't load freelist so that all
  			 * allocations go through alloc_debug_processing()
@@@ -2742,7 -2856,12 +2751,11 @@@
  
  return_single:
  
+ 	if (unlikely(c->page))
+ 		flush_slab(s, c);
+ 	c->page = page;
+ 
  	deactivate_slab(s, page, get_freepointer(s, freelist), c);
 -	local_irq_restore(flags);
  	return freelist;
  }
  
* Unmerged path mm/slub.c
