rcu/nocb: Re-offload support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Frederic Weisbecker <frederic@kernel.org>
commit 254e11efde66ca0a0ce0c99a62c377314b5984ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/254e11ef.failed

To re-offload the callback processing off of a CPU, it is necessary to
clear SEGCBLIST_SOFTIRQ_ONLY, set SEGCBLIST_OFFLOADED, and then notify
both the CB and GP kthreads so that they both set their own bit flag and
start processing the callbacks remotely.  The re-offloading worker is
then notified that it can stop the RCU_SOFTIRQ handler (or rcuc kthread,
as the case may be) from processing the callbacks locally.

Ordering must be carefully enforced so that the callbacks that used to be
processed locally without locking will have the same ordering properties
when they are invoked by the nocb CB and GP kthreads.

This commit makes this change.

	Cc: Josh Triplett <josh@joshtriplett.org>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Cc: Lai Jiangshan <jiangshanlai@gmail.com>
	Cc: Joel Fernandes <joel@joelfernandes.org>
	Cc: Neeraj Upadhyay <neeraju@codeaurora.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Inspired-by: Paul E. McKenney <paulmck@kernel.org>
	Tested-by: Boqun Feng <boqun.feng@gmail.com>
	Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
[ paulmck: Export rcu_nocb_cpu_offload(). ]
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 254e11efde66ca0a0ce0c99a62c377314b5984ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rcupdate.h
#	kernel/rcu/tree_plugin.h
diff --cc include/linux/rcupdate.h
index 95a12eb5f6b0,e0ee52e2756d..000000000000
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@@ -118,8 -104,12 +118,17 @@@ static inline void rcu_user_exit(void) 
  
  #ifdef CONFIG_RCU_NOCB_CPU
  void rcu_init_nohz(void);
++<<<<<<< HEAD
 +#else /* #ifdef CONFIG_RCU_NOCB_CPU */
 +static inline void rcu_init_nohz(void) { }
++=======
+ int rcu_nocb_cpu_offload(int cpu);
+ int rcu_nocb_cpu_deoffload(int cpu);
+ #else /* #ifdef CONFIG_RCU_NOCB_CPU */
+ static inline void rcu_init_nohz(void) { }
+ static inline int rcu_nocb_cpu_offload(int cpu) { return -EINVAL; }
+ static inline int rcu_nocb_cpu_deoffload(int cpu) { return 0; }
++>>>>>>> 254e11efde66 (rcu/nocb: Re-offload support)
  #endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
  
  /**
diff --cc kernel/rcu/tree_plugin.h
index 10e9e427ecfe,03ae1ce4790f..000000000000
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@@ -1928,6 -1928,52 +1928,55 @@@ static void do_nocb_bypass_wakeup_timer
  	__call_rcu_nocb_wake(rdp, true, flags);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Check if we ignore this rdp.
+  *
+  * We check that without holding the nocb lock but
+  * we make sure not to miss a freshly offloaded rdp
+  * with the current ordering:
+  *
+  *  rdp_offload_toggle()        nocb_gp_enabled_cb()
+  * -------------------------   ----------------------------
+  *    WRITE flags                 LOCK nocb_gp_lock
+  *    LOCK nocb_gp_lock           READ/WRITE nocb_gp_sleep
+  *    READ/WRITE nocb_gp_sleep    UNLOCK nocb_gp_lock
+  *    UNLOCK nocb_gp_lock         READ flags
+  */
+ static inline bool nocb_gp_enabled_cb(struct rcu_data *rdp)
+ {
+ 	u8 flags = SEGCBLIST_OFFLOADED | SEGCBLIST_KTHREAD_GP;
+ 
+ 	return rcu_segcblist_test_flags(&rdp->cblist, flags);
+ }
+ 
+ static inline bool nocb_gp_update_state(struct rcu_data *rdp, bool *needwake_state)
+ {
+ 	struct rcu_segcblist *cblist = &rdp->cblist;
+ 
+ 	if (rcu_segcblist_test_flags(cblist, SEGCBLIST_OFFLOADED)) {
+ 		if (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP)) {
+ 			rcu_segcblist_set_flags(cblist, SEGCBLIST_KTHREAD_GP);
+ 			if (rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB))
+ 				*needwake_state = true;
+ 		}
+ 		return true;
+ 	} else {
+ 		/*
+ 		 * De-offloading. Clear our flag and notify the de-offload worker.
+ 		 * We will ignore this rdp until it ever gets re-offloaded.
+ 		 */
+ 		WARN_ON_ONCE(!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP));
+ 		rcu_segcblist_clear_flags(cblist, SEGCBLIST_KTHREAD_GP);
+ 		if (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB))
+ 			*needwake_state = true;
+ 		return false;
+ 	}
+ }
+ 
+ 
++>>>>>>> 254e11efde66 (rcu/nocb: Re-offload support)
  /*
   * No-CBs GP kthreads come here to wait for additional callbacks to show up
   * or for grace periods to end.
@@@ -2106,15 -2178,32 +2159,39 @@@ static void nocb_cb_wait(struct rcu_dat
  		needwake_gp = rcu_advance_cbs(rdp->mynode, rdp);
  		raw_spin_unlock_rcu_node(rnp); /* irqs remain disabled. */
  	}
++<<<<<<< HEAD
 +	if (rcu_segcblist_ready_cbs(&rdp->cblist)) {
 +		rcu_nocb_unlock_irqrestore(rdp, flags);
 +		if (needwake_gp)
 +			rcu_gp_kthread_wake();
 +		return;
++=======
+ 
+ 	WRITE_ONCE(rdp->nocb_cb_sleep, true);
+ 
+ 	if (rcu_segcblist_test_flags(cblist, SEGCBLIST_OFFLOADED)) {
+ 		if (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB)) {
+ 			rcu_segcblist_set_flags(cblist, SEGCBLIST_KTHREAD_CB);
+ 			if (rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP))
+ 				needwake_state = true;
+ 		}
+ 		if (rcu_segcblist_ready_cbs(cblist))
+ 			WRITE_ONCE(rdp->nocb_cb_sleep, false);
+ 	} else {
+ 		/*
+ 		 * De-offloading. Clear our flag and notify the de-offload worker.
+ 		 * We won't touch the callbacks and keep sleeping until we ever
+ 		 * get re-offloaded.
+ 		 */
+ 		WARN_ON_ONCE(!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB));
+ 		rcu_segcblist_clear_flags(cblist, SEGCBLIST_KTHREAD_CB);
+ 		if (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP))
+ 			needwake_state = true;
++>>>>>>> 254e11efde66 (rcu/nocb: Re-offload support)
  	}
  
 -	if (rdp->nocb_cb_sleep)
 -		trace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS("CBSleep"));
 -
 +	trace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS("CBSleep"));
 +	WRITE_ONCE(rdp->nocb_cb_sleep, true);
  	rcu_nocb_unlock_irqrestore(rdp, flags);
  	if (needwake_gp)
  		rcu_gp_kthread_wake();
@@@ -2187,6 -2282,173 +2264,176 @@@ static void do_nocb_deferred_wakeup(str
  		do_nocb_deferred_wakeup_common(rdp);
  }
  
++<<<<<<< HEAD
++=======
+ static int rdp_offload_toggle(struct rcu_data *rdp,
+ 			       bool offload, unsigned long flags)
+ 	__releases(rdp->nocb_lock)
+ {
+ 	struct rcu_segcblist *cblist = &rdp->cblist;
+ 	struct rcu_data *rdp_gp = rdp->nocb_gp_rdp;
+ 	bool wake_gp = false;
+ 
+ 	rcu_segcblist_offload(cblist, offload);
+ 
+ 	if (rdp->nocb_cb_sleep)
+ 		rdp->nocb_cb_sleep = false;
+ 	rcu_nocb_unlock_irqrestore(rdp, flags);
+ 
+ 	/*
+ 	 * Ignore former value of nocb_cb_sleep and force wake up as it could
+ 	 * have been spuriously set to false already.
+ 	 */
+ 	swake_up_one(&rdp->nocb_cb_wq);
+ 
+ 	raw_spin_lock_irqsave(&rdp_gp->nocb_gp_lock, flags);
+ 	if (rdp_gp->nocb_gp_sleep) {
+ 		rdp_gp->nocb_gp_sleep = false;
+ 		wake_gp = true;
+ 	}
+ 	raw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);
+ 
+ 	if (wake_gp)
+ 		wake_up_process(rdp_gp->nocb_gp_kthread);
+ 
+ 	return 0;
+ }
+ 
+ static int __rcu_nocb_rdp_deoffload(struct rcu_data *rdp)
+ {
+ 	struct rcu_segcblist *cblist = &rdp->cblist;
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	printk("De-offloading %d\n", rdp->cpu);
+ 
+ 	rcu_nocb_lock_irqsave(rdp, flags);
+ 	/*
+ 	 * If there are still pending work offloaded, the offline
+ 	 * CPU won't help much handling them.
+ 	 */
+ 	if (cpu_is_offline(rdp->cpu) && !rcu_segcblist_empty(&rdp->cblist)) {
+ 		rcu_nocb_unlock_irqrestore(rdp, flags);
+ 		return -EBUSY;
+ 	}
+ 
+ 	ret = rdp_offload_toggle(rdp, false, flags);
+ 	swait_event_exclusive(rdp->nocb_state_wq,
+ 			      !rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB |
+ 							SEGCBLIST_KTHREAD_GP));
+ 	return ret;
+ }
+ 
+ static long rcu_nocb_rdp_deoffload(void *arg)
+ {
+ 	struct rcu_data *rdp = arg;
+ 
+ 	WARN_ON_ONCE(rdp->cpu != raw_smp_processor_id());
+ 	return __rcu_nocb_rdp_deoffload(rdp);
+ }
+ 
+ int rcu_nocb_cpu_deoffload(int cpu)
+ {
+ 	struct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);
+ 	int ret = 0;
+ 
+ 	if (rdp == rdp->nocb_gp_rdp) {
+ 		pr_info("Can't deoffload an rdp GP leader (yet)\n");
+ 		return -EINVAL;
+ 	}
+ 	mutex_lock(&rcu_state.barrier_mutex);
+ 	cpus_read_lock();
+ 	if (rcu_segcblist_is_offloaded(&rdp->cblist)) {
+ 		if (cpu_online(cpu)) {
+ 			ret = work_on_cpu(cpu, rcu_nocb_rdp_deoffload, rdp);
+ 		} else {
+ 			ret = __rcu_nocb_rdp_deoffload(rdp);
+ 		}
+ 		if (!ret)
+ 			cpumask_clear_cpu(cpu, rcu_nocb_mask);
+ 	}
+ 	cpus_read_unlock();
+ 	mutex_unlock(&rcu_state.barrier_mutex);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(rcu_nocb_cpu_deoffload);
+ 
+ static int __rcu_nocb_rdp_offload(struct rcu_data *rdp)
+ {
+ 	struct rcu_segcblist *cblist = &rdp->cblist;
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	/*
+ 	 * For now we only support re-offload, ie: the rdp must have been
+ 	 * offloaded on boot first.
+ 	 */
+ 	if (!rdp->nocb_gp_rdp)
+ 		return -EINVAL;
+ 
+ 	printk("Offloading %d\n", rdp->cpu);
+ 	/*
+ 	 * Can't use rcu_nocb_lock_irqsave() while we are in
+ 	 * SEGCBLIST_SOFTIRQ_ONLY mode.
+ 	 */
+ 	raw_spin_lock_irqsave(&rdp->nocb_lock, flags);
+ 	/*
+ 	 * We didn't take the nocb lock while working on the
+ 	 * rdp->cblist in SEGCBLIST_SOFTIRQ_ONLY mode.
+ 	 * Every modifications that have been done previously on
+ 	 * rdp->cblist must be visible remotely by the nocb kthreads
+ 	 * upon wake up after reading the cblist flags.
+ 	 *
+ 	 * The layout against nocb_lock enforces that ordering:
+ 	 *
+ 	 *  __rcu_nocb_rdp_offload()   nocb_cb_wait()/nocb_gp_wait()
+ 	 * -------------------------   ----------------------------
+ 	 *      WRITE callbacks           rcu_nocb_lock()
+ 	 *      rcu_nocb_lock()           READ flags
+ 	 *      WRITE flags               READ callbacks
+ 	 *      rcu_nocb_unlock()         rcu_nocb_unlock()
+ 	 */
+ 	ret = rdp_offload_toggle(rdp, true, flags);
+ 	swait_event_exclusive(rdp->nocb_state_wq,
+ 			      rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB) &&
+ 			      rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP));
+ 
+ 	return ret;
+ }
+ 
+ static long rcu_nocb_rdp_offload(void *arg)
+ {
+ 	struct rcu_data *rdp = arg;
+ 
+ 	WARN_ON_ONCE(rdp->cpu != raw_smp_processor_id());
+ 	return __rcu_nocb_rdp_offload(rdp);
+ }
+ 
+ int rcu_nocb_cpu_offload(int cpu)
+ {
+ 	struct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);
+ 	int ret = 0;
+ 
+ 	mutex_lock(&rcu_state.barrier_mutex);
+ 	cpus_read_lock();
+ 	if (!rcu_segcblist_is_offloaded(&rdp->cblist)) {
+ 		if (cpu_online(cpu)) {
+ 			ret = work_on_cpu(cpu, rcu_nocb_rdp_offload, rdp);
+ 		} else {
+ 			ret = __rcu_nocb_rdp_offload(rdp);
+ 		}
+ 		if (!ret)
+ 			cpumask_set_cpu(cpu, rcu_nocb_mask);
+ 	}
+ 	cpus_read_unlock();
+ 	mutex_unlock(&rcu_state.barrier_mutex);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(rcu_nocb_cpu_offload);
+ 
++>>>>>>> 254e11efde66 (rcu/nocb: Re-offload support)
  void __init rcu_init_nohz(void)
  {
  	int cpu;
* Unmerged path include/linux/rcupdate.h
* Unmerged path kernel/rcu/tree_plugin.h
