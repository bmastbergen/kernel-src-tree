KVM: selftests: Add hugepage support for x86-64

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Sean Christopherson <seanjc@google.com>
commit ad5f16e422258d51414e7d8aaf856000eec9dfce
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/ad5f16e4.failed

Add x86-64 hugepage support in the form of a x86-only variant of
virt_pg_map() that takes an explicit page size.  To keep things simple,
follow the existing logic for 4k pages and disallow creating a hugepage
if the upper-level entry is present, even if the desired pfn matches.

Opportunistically fix a double "beyond beyond" reported by checkpatch.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210622200529.3650424-19-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ad5f16e422258d51414e7d8aaf856000eec9dfce)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/kvm/lib/x86_64/processor.c
diff --cc tools/testing/selftests/kvm/lib/x86_64/processor.c
index 4e80df28f873,5e0e3a131dad..000000000000
--- a/tools/testing/selftests/kvm/lib/x86_64/processor.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/processor.c
@@@ -221,75 -186,100 +221,151 @@@ void virt_pgd_alloc(struct kvm_vm *vm, 
  	}
  }
  
 -static void *virt_get_pte(struct kvm_vm *vm, uint64_t pt_pfn, uint64_t vaddr,
 -			  int level)
 +void virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
 +	uint32_t pgd_memslot)
  {
++<<<<<<< HEAD
 +	uint16_t index[4];
 +	struct pageMapL4Entry *pml4e;
++=======
+ 	uint64_t *page_table = addr_gpa2hva(vm, pt_pfn << vm->page_shift);
+ 	int index = vaddr >> (vm->page_shift + level * 9) & 0x1ffu;
  
- 	TEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, "Attempt to use "
- 		"unknown or unsupported guest mode, mode: 0x%x", vm->mode);
+ 	return &page_table[index];
+ }
+ 
+ static struct pageUpperEntry *virt_create_upper_pte(struct kvm_vm *vm,
+ 						    uint64_t pt_pfn,
+ 						    uint64_t vaddr,
+ 						    uint64_t paddr,
+ 						    int level,
+ 						    enum x86_page_size page_size)
+ {
+ 	struct pageUpperEntry *pte = virt_get_pte(vm, pt_pfn, vaddr, level);
+ 
+ 	if (!pte->present) {
+ 		pte->writable = true;
+ 		pte->present = true;
+ 		pte->page_size = (level == page_size);
+ 		if (pte->page_size)
+ 			pte->pfn = paddr >> vm->page_shift;
+ 		else
+ 			pte->pfn = vm_alloc_page_table(vm) >> vm->page_shift;
+ 	} else {
+ 		/*
+ 		 * Entry already present.  Assert that the caller doesn't want
+ 		 * a hugepage at this level, and that there isn't a hugepage at
+ 		 * this level.
+ 		 */
+ 		TEST_ASSERT(level != page_size,
+ 			    "Cannot create hugepage at level: %u, vaddr: 0x%lx\n",
+ 			    page_size, vaddr);
+ 		TEST_ASSERT(!pte->page_size,
+ 			    "Cannot create page table at level: %u, vaddr: 0x%lx\n",
+ 			    level, vaddr);
+ 	}
+ 	return pte;
+ }
+ 
+ void __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
+ 		   enum x86_page_size page_size)
+ {
+ 	const uint64_t pg_size = 1ull << ((page_size * 9) + 12);
+ 	struct pageUpperEntry *pml4e, *pdpe, *pde;
+ 	struct pageTableEntry *pte;
++>>>>>>> ad5f16e42225 (KVM: selftests: Add hugepage support for x86-64)
  
- 	TEST_ASSERT((vaddr % vm->page_size) == 0,
- 		"Virtual address not on page boundary,\n"
- 		"  vaddr: 0x%lx vm->page_size: 0x%x",
- 		vaddr, vm->page_size);
- 	TEST_ASSERT(sparsebit_is_set(vm->vpages_valid,
- 		(vaddr >> vm->page_shift)),
- 		"Invalid virtual address, vaddr: 0x%lx",
- 		vaddr);
- 	TEST_ASSERT((paddr % vm->page_size) == 0,
- 		"Physical address not on page boundary,\n"
- 		"  paddr: 0x%lx vm->page_size: 0x%x",
- 		paddr, vm->page_size);
+ 	TEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K,
+ 		    "Unknown or unsupported guest mode, mode: 0x%x", vm->mode);
+ 
+ 	TEST_ASSERT((vaddr % pg_size) == 0,
+ 		    "Virtual address not aligned,\n"
+ 		    "vaddr: 0x%lx page size: 0x%lx", vaddr, pg_size);
+ 	TEST_ASSERT(sparsebit_is_set(vm->vpages_valid, (vaddr >> vm->page_shift)),
+ 		    "Invalid virtual address, vaddr: 0x%lx", vaddr);
+ 	TEST_ASSERT((paddr % pg_size) == 0,
+ 		    "Physical address not aligned,\n"
+ 		    "  paddr: 0x%lx page size: 0x%lx", paddr, pg_size);
  	TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
- 		"Physical address beyond beyond maximum supported,\n"
- 		"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
- 		paddr, vm->max_gfn, vm->page_size);
+ 		    "Physical address beyond maximum supported,\n"
+ 		    "  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
+ 		    paddr, vm->max_gfn, vm->page_size);
  
++<<<<<<< HEAD
 +	index[0] = (vaddr >> 12) & 0x1ffu;
 +	index[1] = (vaddr >> 21) & 0x1ffu;
 +	index[2] = (vaddr >> 30) & 0x1ffu;
 +	index[3] = (vaddr >> 39) & 0x1ffu;
 +
 +	/* Allocate page directory pointer table if not present. */
 +	pml4e = addr_gpa2hva(vm, vm->pgd);
 +	if (!pml4e[index[3]].present) {
 +		pml4e[index[3]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pml4e[index[3]].writable = true;
 +		pml4e[index[3]].present = true;
 +	}
 +
 +	/* Allocate page directory table if not present. */
 +	struct pageDirectoryPointerEntry *pdpe;
 +	pdpe = addr_gpa2hva(vm, pml4e[index[3]].address * vm->page_size);
 +	if (!pdpe[index[2]].present) {
 +		pdpe[index[2]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pdpe[index[2]].writable = true;
 +		pdpe[index[2]].present = true;
 +	}
 +
 +	/* Allocate page table if not present. */
 +	struct pageDirectoryEntry *pde;
 +	pde = addr_gpa2hva(vm, pdpe[index[2]].address * vm->page_size);
 +	if (!pde[index[1]].present) {
 +		pde[index[1]].address = vm_phy_page_alloc(vm,
 +			KVM_GUEST_PAGE_TABLE_MIN_PADDR, pgd_memslot)
 +			>> vm->page_shift;
 +		pde[index[1]].writable = true;
 +		pde[index[1]].present = true;
 +	}
 +
 +	/* Fill in page table entry. */
 +	struct pageTableEntry *pte;
 +	pte = addr_gpa2hva(vm, pde[index[1]].address * vm->page_size);
 +	pte[index[0]].address = paddr >> vm->page_shift;
 +	pte[index[0]].writable = true;
 +	pte[index[0]].present = 1;
++=======
+ 	/*
+ 	 * Allocate upper level page tables, if not already present.  Return
+ 	 * early if a hugepage was created.
+ 	 */
+ 	pml4e = virt_create_upper_pte(vm, vm->pgd >> vm->page_shift,
+ 				      vaddr, paddr, 3, page_size);
+ 	if (pml4e->page_size)
+ 		return;
+ 
+ 	pdpe = virt_create_upper_pte(vm, pml4e->pfn, vaddr, paddr, 2, page_size);
+ 	if (pdpe->page_size)
+ 		return;
+ 
+ 	pde = virt_create_upper_pte(vm, pdpe->pfn, vaddr, paddr, 1, page_size);
+ 	if (pde->page_size)
+ 		return;
+ 
+ 	/* Fill in page table entry. */
+ 	pte = virt_get_pte(vm, pde->pfn, vaddr, 0);
+ 	TEST_ASSERT(!pte->present,
+ 		    "PTE already present for 4k page at vaddr: 0x%lx\n", vaddr);
+ 	pte->pfn = paddr >> vm->page_shift;
+ 	pte->writable = true;
+ 	pte->present = 1;
++>>>>>>> ad5f16e42225 (KVM: selftests: Add hugepage support for x86-64)
+ }
+ 
+ void virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)
+ {
+ 	__virt_pg_map(vm, vaddr, paddr, X86_PAGE_SIZE_4K);
  }
  
  void virt_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index 09cbccd1b785..45ff8c4d50f3 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -408,6 +408,14 @@ struct kvm_cpuid2 *kvm_get_supported_hv_cpuid(void);
 void vcpu_set_hv_cpuid(struct kvm_vm *vm, uint32_t vcpuid);
 struct kvm_cpuid2 *vcpu_get_supported_hv_cpuid(struct kvm_vm *vm, uint32_t vcpuid);
 
+enum x86_page_size {
+	X86_PAGE_SIZE_4K = 0,
+	X86_PAGE_SIZE_2M,
+	X86_PAGE_SIZE_1G,
+};
+void __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
+		   enum x86_page_size page_size);
+
 /*
  * Basic CPU control in CR0
  */
* Unmerged path tools/testing/selftests/kvm/lib/x86_64/processor.c
