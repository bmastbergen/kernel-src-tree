mm/highmem: fix CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ira Weiny <ira.weiny@intel.com>
commit 487cfade12fae0eb707bdce71c4d585128238a7d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/487cfade.failed

The kernel test robot found that __kmap_local_sched_out() was not
correctly skipping the guard pages when DEBUG_KMAP_LOCAL_FORCE_MAP was
set.[1] This was due to DEBUG_HIGHMEM check being used.

Change the configuration check to be correct.

[1] https://lore.kernel.org/lkml/20210304083825.GB17830@xsang-OptiPlex-9020/

Link: https://lkml.kernel.org/r/20210318230657.1497881-1-ira.weiny@intel.com
Fixes: 0e91a0c6984c ("mm/highmem: Provide CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP")
	Signed-off-by: Ira Weiny <ira.weiny@intel.com>
	Reported-by: kernel test robot <oliver.sang@intel.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Oliver Sang <oliver.sang@intel.com>
	Cc: Chaitanya Kulkarni <Chaitanya.Kulkarni@wdc.com>
	Cc: David Sterba <dsterba@suse.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 487cfade12fae0eb707bdce71c4d585128238a7d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/highmem.c
diff --cc mm/highmem.c
index 51171d0f44c2,6ef8f5e05e7e..000000000000
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@@ -365,8 -358,323 +365,327 @@@ void kunmap_high(struct page *page
  	if (need_wakeup)
  		wake_up(pkmap_map_wait);
  }
 +
  EXPORT_SYMBOL(kunmap_high);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ void zero_user_segments(struct page *page, unsigned start1, unsigned end1,
+ 		unsigned start2, unsigned end2)
+ {
+ 	unsigned int i;
+ 
+ 	BUG_ON(end1 > page_size(page) || end2 > page_size(page));
+ 
+ 	if (start1 >= end1)
+ 		start1 = end1 = 0;
+ 	if (start2 >= end2)
+ 		start2 = end2 = 0;
+ 
+ 	for (i = 0; i < compound_nr(page); i++) {
+ 		void *kaddr = NULL;
+ 
+ 		if (start1 >= PAGE_SIZE) {
+ 			start1 -= PAGE_SIZE;
+ 			end1 -= PAGE_SIZE;
+ 		} else {
+ 			unsigned this_end = min_t(unsigned, end1, PAGE_SIZE);
+ 
+ 			if (end1 > start1) {
+ 				kaddr = kmap_atomic(page + i);
+ 				memset(kaddr + start1, 0, this_end - start1);
+ 			}
+ 			end1 -= this_end;
+ 			start1 = 0;
+ 		}
+ 
+ 		if (start2 >= PAGE_SIZE) {
+ 			start2 -= PAGE_SIZE;
+ 			end2 -= PAGE_SIZE;
+ 		} else {
+ 			unsigned this_end = min_t(unsigned, end2, PAGE_SIZE);
+ 
+ 			if (end2 > start2) {
+ 				if (!kaddr)
+ 					kaddr = kmap_atomic(page + i);
+ 				memset(kaddr + start2, 0, this_end - start2);
+ 			}
+ 			end2 -= this_end;
+ 			start2 = 0;
+ 		}
+ 
+ 		if (kaddr) {
+ 			kunmap_atomic(kaddr);
+ 			flush_dcache_page(page + i);
+ 		}
+ 
+ 		if (!end1 && !end2)
+ 			break;
+ 	}
+ 
+ 	BUG_ON((start1 | start2 | end1 | end2) != 0);
+ }
+ EXPORT_SYMBOL(zero_user_segments);
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ #endif /* CONFIG_HIGHMEM */
+ 
+ #ifdef CONFIG_KMAP_LOCAL
+ 
+ #include <asm/kmap_size.h>
+ 
+ /*
+  * With DEBUG_KMAP_LOCAL the stack depth is doubled and every second
+  * slot is unused which acts as a guard page
+  */
+ #ifdef CONFIG_DEBUG_KMAP_LOCAL
+ # define KM_INCR	2
+ #else
+ # define KM_INCR	1
+ #endif
+ 
+ static inline int kmap_local_idx_push(void)
+ {
+ 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+ 	current->kmap_ctrl.idx += KM_INCR;
+ 	BUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);
+ 	return current->kmap_ctrl.idx - 1;
+ }
+ 
+ static inline int kmap_local_idx(void)
+ {
+ 	return current->kmap_ctrl.idx - 1;
+ }
+ 
+ static inline void kmap_local_idx_pop(void)
+ {
+ 	current->kmap_ctrl.idx -= KM_INCR;
+ 	BUG_ON(current->kmap_ctrl.idx < 0);
+ }
+ 
+ #ifndef arch_kmap_local_post_map
+ # define arch_kmap_local_post_map(vaddr, pteval)	do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_pre_unmap
+ # define arch_kmap_local_pre_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_post_unmap
+ # define arch_kmap_local_post_unmap(vaddr)		do { } while (0)
+ #endif
+ 
+ #ifndef arch_kmap_local_map_idx
+ #define arch_kmap_local_map_idx(idx, pfn)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_unmap_idx
+ #define arch_kmap_local_unmap_idx(idx, vaddr)	kmap_local_calc_idx(idx)
+ #endif
+ 
+ #ifndef arch_kmap_local_high_get
+ static inline void *arch_kmap_local_high_get(struct page *page)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ #ifndef arch_kmap_local_set_pte
+ #define arch_kmap_local_set_pte(mm, vaddr, ptep, ptev)	\
+ 	set_pte_at(mm, vaddr, ptep, ptev)
+ #endif
+ 
+ /* Unmap a local mapping which was obtained by kmap_high_get() */
+ static inline bool kmap_high_unmap_local(unsigned long vaddr)
+ {
+ #ifdef ARCH_NEEDS_KMAP_HIGH_GET
+ 	if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
+ 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
+ 		return true;
+ 	}
+ #endif
+ 	return false;
+ }
+ 
+ static inline int kmap_local_calc_idx(int idx)
+ {
+ 	return idx + KM_MAX_IDX * smp_processor_id();
+ }
+ 
+ static pte_t *__kmap_pte;
+ 
+ static pte_t *kmap_get_pte(void)
+ {
+ 	if (!__kmap_pte)
+ 		__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
+ 	return __kmap_pte;
+ }
+ 
+ void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
+ {
+ 	pte_t pteval, *kmap_pte = kmap_get_pte();
+ 	unsigned long vaddr;
+ 	int idx;
+ 
+ 	/*
+ 	 * Disable migration so resulting virtual address is stable
+ 	 * accross preemption.
+ 	 */
+ 	migrate_disable();
+ 	preempt_disable();
+ 	idx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 	BUG_ON(!pte_none(*(kmap_pte - idx)));
+ 	pteval = pfn_pte(pfn, prot);
+ 	arch_kmap_local_set_pte(&init_mm, vaddr, kmap_pte - idx, pteval);
+ 	arch_kmap_local_post_map(vaddr, pteval);
+ 	current->kmap_ctrl.pteval[kmap_local_idx()] = pteval;
+ 	preempt_enable();
+ 
+ 	return (void *)vaddr;
+ }
+ EXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);
+ 
+ void *__kmap_local_page_prot(struct page *page, pgprot_t prot)
+ {
+ 	void *kmap;
+ 
+ 	/*
+ 	 * To broaden the usage of the actual kmap_local() machinery always map
+ 	 * pages when debugging is enabled and the architecture has no problems
+ 	 * with alias mappings.
+ 	 */
+ 	if (!IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) && !PageHighMem(page))
+ 		return page_address(page);
+ 
+ 	/* Try kmap_high_get() if architecture has it enabled */
+ 	kmap = arch_kmap_local_high_get(page);
+ 	if (kmap)
+ 		return kmap;
+ 
+ 	return __kmap_local_pfn_prot(page_to_pfn(page), prot);
+ }
+ EXPORT_SYMBOL(__kmap_local_page_prot);
+ 
+ void kunmap_local_indexed(void *vaddr)
+ {
+ 	unsigned long addr = (unsigned long) vaddr & PAGE_MASK;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int idx;
+ 
+ 	if (addr < __fix_to_virt(FIX_KMAP_END) ||
+ 	    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {
+ 		if (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {
+ 			/* This _should_ never happen! See above. */
+ 			WARN_ON_ONCE(1);
+ 			return;
+ 		}
+ 		/*
+ 		 * Handle mappings which were obtained by kmap_high_get()
+ 		 * first as the virtual address of such mappings is below
+ 		 * PAGE_OFFSET. Warn for all other addresses which are in
+ 		 * the user space part of the virtual address space.
+ 		 */
+ 		if (!kmap_high_unmap_local(addr))
+ 			WARN_ON_ONCE(addr < PAGE_OFFSET);
+ 		return;
+ 	}
+ 
+ 	preempt_disable();
+ 	idx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);
+ 	WARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
+ 
+ 	arch_kmap_local_pre_unmap(addr);
+ 	pte_clear(&init_mm, addr, kmap_pte - idx);
+ 	arch_kmap_local_post_unmap(addr);
+ 	current->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);
+ 	kmap_local_idx_pop();
+ 	preempt_enable();
+ 	migrate_enable();
+ }
+ EXPORT_SYMBOL(kunmap_local_indexed);
+ 
+ /*
+  * Invoked before switch_to(). This is safe even when during or after
+  * clearing the maps an interrupt which needs a kmap_local happens because
+  * the task::kmap_ctrl.idx is not modified by the unmapping code so a
+  * nested kmap_local will use the next unused index and restore the index
+  * on unmap. The already cleared kmaps of the outgoing task are irrelevant
+  * because the interrupt context does not know about them. The same applies
+  * when scheduling back in for an interrupt which happens before the
+  * restore is complete.
+  */
+ void __kmap_local_sched_out(void)
+ {
+ 	struct task_struct *tsk = current;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int i;
+ 
+ 	/* Clear kmaps */
+ 	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+ 		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+ 		unsigned long addr;
+ 		int idx;
+ 
+ 		/* With debug all even slots are unmapped and act as guard */
+ 		if (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {
+ 			WARN_ON_ONCE(!pte_none(pteval));
+ 			continue;
+ 		}
+ 		if (WARN_ON_ONCE(pte_none(pteval)))
+ 			continue;
+ 
+ 		/*
+ 		 * This is a horrible hack for XTENSA to calculate the
+ 		 * coloured PTE index. Uses the PFN encoded into the pteval
+ 		 * and the map index calculation because the actual mapped
+ 		 * virtual address is not stored in task::kmap_ctrl.
+ 		 * For any sane architecture this is optimized out.
+ 		 */
+ 		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+ 
+ 		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 		arch_kmap_local_pre_unmap(addr);
+ 		pte_clear(&init_mm, addr, kmap_pte - idx);
+ 		arch_kmap_local_post_unmap(addr);
+ 	}
+ }
+ 
+ void __kmap_local_sched_in(void)
+ {
+ 	struct task_struct *tsk = current;
+ 	pte_t *kmap_pte = kmap_get_pte();
+ 	int i;
+ 
+ 	/* Restore kmaps */
+ 	for (i = 0; i < tsk->kmap_ctrl.idx; i++) {
+ 		pte_t pteval = tsk->kmap_ctrl.pteval[i];
+ 		unsigned long addr;
+ 		int idx;
+ 
+ 		/* With debug all even slots are unmapped and act as guard */
+ 		if (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {
+ 			WARN_ON_ONCE(!pte_none(pteval));
+ 			continue;
+ 		}
+ 		if (WARN_ON_ONCE(pte_none(pteval)))
+ 			continue;
+ 
+ 		/* See comment in __kmap_local_sched_out() */
+ 		idx = arch_kmap_local_map_idx(i, pte_pfn(pteval));
+ 		addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 		set_pte_at(&init_mm, addr, kmap_pte - idx, pteval);
+ 		arch_kmap_local_post_map(addr, pteval);
+ 	}
+ }
+ 
+ void kmap_local_fork(struct task_struct *tsk)
+ {
+ 	if (WARN_ON_ONCE(tsk->kmap_ctrl.idx))
+ 		memset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));
+ }
+ 
++>>>>>>> 487cfade12fa (mm/highmem: fix CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)
  #endif
  
  #if defined(HASHED_PAGE_VIRTUAL)
* Unmerged path mm/highmem.c
