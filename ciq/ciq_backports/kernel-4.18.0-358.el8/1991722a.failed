mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 1991722a70ffb1f0199a5690a31a5c7822007b1f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/1991722a.failed

The 'subscription' is placed on the 'notifier_subscriptions' list.

This eliminates the poor name 'mn' for this variable.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 1991722a70ffb1f0199a5690a31a5c7822007b1f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmu_notifier.h
#	mm/mmu_notifier.c
diff --cc include/linux/mmu_notifier.h
index d924803cea7c,4342fa75dc24..000000000000
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@@ -199,16 -162,17 +199,23 @@@ struct mmu_notifier_ops 
  	 * address space but may still be referenced by sptes until
  	 * the last refcount is dropped.
  	 *
 -	 * If blockable argument is set to false then the callback cannot
 -	 * sleep and has to return with -EAGAIN. 0 should be returned
 -	 * otherwise. Please note that if invalidate_range_start approves
 -	 * a non-blocking behavior then the same applies to
 -	 * invalidate_range_end.
 -	 *
 +	 * If both of these callbacks cannot block, and invalidate_range
 +	 * cannot block, mmu_notifier_ops.flags should have
 +	 * MMU_INVALIDATE_DOES_NOT_BLOCK set.
  	 */
++<<<<<<< HEAD
 +	void (*invalidate_range_start)(struct mmu_notifier *mn,
 +				       struct mm_struct *mm,
 +				       unsigned long start, unsigned long end);
 +	void (*invalidate_range_end)(struct mmu_notifier *mn,
 +				     struct mm_struct *mm,
 +				     unsigned long start, unsigned long end);
++=======
+ 	int (*invalidate_range_start)(struct mmu_notifier *subscription,
+ 				      const struct mmu_notifier_range *range);
+ 	void (*invalidate_range_end)(struct mmu_notifier *subscription,
+ 				     const struct mmu_notifier_range *range);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  
  	/*
  	 * invalidate_range() is either called between
@@@ -227,28 -191,24 +234,45 @@@
  	 * Note that this function might be called with just a sub-range
  	 * of what was passed to invalidate_range_start()/end(), if
  	 * called between those functions.
 +	 *
 +	 * If this callback cannot block, and invalidate_range_{start,end}
 +	 * cannot block, mmu_notifier_ops.flags should have
 +	 * MMU_INVALIDATE_DOES_NOT_BLOCK set.
  	 */
- 	void (*invalidate_range)(struct mmu_notifier *mn, struct mm_struct *mm,
- 				 unsigned long start, unsigned long end);
+ 	void (*invalidate_range)(struct mmu_notifier *subscription,
+ 				 struct mm_struct *mm,
+ 				 unsigned long start,
+ 				 unsigned long end);
  
++<<<<<<< HEAD
 +       /*
 +	* These callbacks are used with the get/put interface to manage the
 +	* lifetime of the mmu_notifier memory. alloc_notifier() returns a new
 +	* notifier for use with the mm.
 +	*
 +	* free_notifier() is only called after the mmu_notifier has been
 +	* fully put, calls to any ops callback are prevented and no ops
 +	* callbacks are currently running. It is called from a SRCU callback
 +	* and cannot sleep.
 +	*/
 +	RH_KABI_USE(1, struct mmu_notifier *(*alloc_notifier)(struct mm_struct *mm))
 +	RH_KABI_USE(2, void (*free_notifier)(struct mmu_notifier *mn))
 +	RH_KABI_RESERVE(3)
 +	RH_KABI_RESERVE(4)
++=======
+ 	/*
+ 	 * These callbacks are used with the get/put interface to manage the
+ 	 * lifetime of the mmu_notifier memory. alloc_notifier() returns a new
+ 	 * notifier for use with the mm.
+ 	 *
+ 	 * free_notifier() is only called after the mmu_notifier has been
+ 	 * fully put, calls to any ops callback are prevented and no ops
+ 	 * callbacks are currently running. It is called from a SRCU callback
+ 	 * and cannot sleep.
+ 	 */
+ 	struct mmu_notifier *(*alloc_notifier)(struct mm_struct *mm);
+ 	void (*free_notifier)(struct mmu_notifier *subscription);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  };
  
  /*
@@@ -317,22 -277,20 +341,22 @@@ mmu_notifier_get(const struct mmu_notif
  {
  	struct mmu_notifier *ret;
  
 -	down_write(&mm->mmap_sem);
 +	mmap_write_lock(mm);
  	ret = mmu_notifier_get_locked(ops, mm);
 -	up_write(&mm->mmap_sem);
 +	mmap_write_unlock(mm);
  	return ret;
  }
- void mmu_notifier_put(struct mmu_notifier *mn);
+ void mmu_notifier_put(struct mmu_notifier *subscription);
  void mmu_notifier_synchronize(void);
  
- extern int mmu_notifier_register(struct mmu_notifier *mn,
+ extern int mmu_notifier_register(struct mmu_notifier *subscription,
  				 struct mm_struct *mm);
- extern int __mmu_notifier_register(struct mmu_notifier *mn,
+ extern int __mmu_notifier_register(struct mmu_notifier *subscription,
  				   struct mm_struct *mm);
- extern void mmu_notifier_unregister(struct mmu_notifier *mn,
+ extern void mmu_notifier_unregister(struct mmu_notifier *subscription,
  				    struct mm_struct *mm);
 +extern void mmu_notifier_unregister_no_release(struct mmu_notifier *mn,
 +					       struct mm_struct *mm);
  
  unsigned long mmu_interval_read_begin(struct mmu_interval_notifier *mni);
  int mmu_interval_notifier_insert(struct mmu_interval_notifier *mni,
diff --cc mm/mmu_notifier.c
index 4361d699fa34,12b35d8b444a..000000000000
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@@ -301,10 -291,10 +301,10 @@@ EXPORT_SYMBOL_GPL(mmu_notifier_call_src
   * can't go away from under us as exit_mmap holds an mm_count pin
   * itself.
   */
 -static void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,
 +static void mn_hlist_release(struct mmu_notifier_mm *mmn_mm,
  			     struct mm_struct *mm)
  {
- 	struct mmu_notifier *mn;
+ 	struct mmu_notifier *subscription;
  	int id;
  
  	/*
@@@ -312,29 -302,29 +312,40 @@@
  	 * ->release returns.
  	 */
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist)
++=======
+ 	hlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist)
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  		/*
  		 * If ->release runs before mmu_notifier_unregister it must be
  		 * handled, as it's the only way for the driver to flush all
  		 * existing sptes and stop the driver from establishing any more
  		 * sptes before all the pages in the mm are freed.
  		 */
- 		if (mn->ops->release)
- 			mn->ops->release(mn, mm);
+ 		if (subscription->ops->release)
+ 			subscription->ops->release(subscription, mm);
  
++<<<<<<< HEAD
 +	spin_lock(&mmn_mm->lock);
 +	while (unlikely(!hlist_empty(&mmn_mm->list))) {
 +		mn = hlist_entry(mmn_mm->list.first, struct mmu_notifier,
 +				 hlist);
++=======
+ 	spin_lock(&subscriptions->lock);
+ 	while (unlikely(!hlist_empty(&subscriptions->list))) {
+ 		subscription = hlist_entry(subscriptions->list.first,
+ 					   struct mmu_notifier, hlist);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  		/*
  		 * We arrived before mmu_notifier_unregister so
  		 * mmu_notifier_unregister will do nothing other than to wait
  		 * for ->release to finish and for mmu_notifier_unregister to
  		 * return.
  		 */
- 		hlist_del_init_rcu(&mn->hlist);
+ 		hlist_del_init_rcu(&subscription->hlist);
  	}
 -	spin_unlock(&subscriptions->lock);
 +	spin_unlock(&mmn_mm->lock);
  	srcu_read_unlock(&srcu, id);
  
  	/*
@@@ -373,9 -364,11 +384,17 @@@ int __mmu_notifier_clear_flush_young(st
  	int young = 0, id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops->clear_flush_young)
 +			young |= mn->ops->clear_flush_young(mn, mm, start, end);
++=======
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops->clear_flush_young)
+ 			young |= subscription->ops->clear_flush_young(
+ 				subscription, mm, start, end);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	}
  	srcu_read_unlock(&srcu, id);
  
@@@ -390,9 -383,11 +409,17 @@@ int __mmu_notifier_clear_young(struct m
  	int young = 0, id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops->clear_young)
 +			young |= mn->ops->clear_young(mn, mm, start, end);
++=======
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops->clear_young)
+ 			young |= subscription->ops->clear_young(subscription,
+ 								mm, start, end);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	}
  	srcu_read_unlock(&srcu, id);
  
@@@ -406,9 -401,11 +433,17 @@@ int __mmu_notifier_test_young(struct mm
  	int young = 0, id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops->test_young) {
 +			young = mn->ops->test_young(mn, mm, address);
++=======
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops->test_young) {
+ 			young = subscription->ops->test_young(subscription, mm,
+ 							      address);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  			if (young)
  				break;
  		}
@@@ -425,9 -422,11 +460,17 @@@ void __mmu_notifier_change_pte(struct m
  	int id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops->change_pte)
 +			mn->ops->change_pte(mn, mm, address, pte);
++=======
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops->change_pte)
+ 			subscription->ops->change_pte(subscription, mm, address,
+ 						      pte);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	}
  	srcu_read_unlock(&srcu, id);
  }
@@@ -438,48 -437,90 +481,81 @@@ static void mn_itree_invalidate(struct 
  	struct mmu_interval_notifier *mni;
  	unsigned long cur_seq;
  
 -	for (mni = mn_itree_inv_start_range(subscriptions, range, &cur_seq);
 -	     mni; mni = mn_itree_inv_next(mni, range)) {
 -		bool ret;
 +	for (mni = mn_itree_inv_start_range(mmn_mm, range, &cur_seq); mni;
 +	     mni = mn_itree_inv_next(mni, range)) {
  
 -		ret = mni->ops->invalidate(mni, range, cur_seq);
 -		if (!ret) {
 -			if (WARN_ON(mmu_notifier_range_blockable(range)))
 -				continue;
 -			goto out_would_block;
 -		}
 +		mni->ops->invalidate(mni, range, cur_seq);
  	}
 -	return 0;
 -
 -out_would_block:
 -	/*
 -	 * On -EAGAIN the non-blocking caller is not allowed to call
 -	 * invalidate_range_end()
 -	 */
 -	mn_itree_inv_end(subscriptions);
 -	return -EAGAIN;
  }
  
 -static int mn_hlist_invalidate_range_start(
 -	struct mmu_notifier_subscriptions *subscriptions,
 -	struct mmu_notifier_range *range)
 +static void mn_hlist_invalidate_range_start(struct mmu_notifier_mm *mmn_mm,
 +					    struct mmu_notifier_range *range)
  {
++<<<<<<< HEAD
 +	struct mmu_notifier *mn;
 +	int id;
 +
 +	id = srcu_read_lock(&srcu);
 +	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist) {
 +		if (mn->ops->invalidate_range_start) {
 +			mn->ops->invalidate_range_start(mn, range->mm, range->start, range->end);
++=======
+ 	struct mmu_notifier *subscription;
+ 	int ret = 0;
+ 	int id;
+ 
+ 	id = srcu_read_lock(&srcu);
+ 	hlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist) {
+ 		const struct mmu_notifier_ops *ops = subscription->ops;
+ 
+ 		if (ops->invalidate_range_start) {
+ 			int _ret;
+ 
+ 			if (!mmu_notifier_range_blockable(range))
+ 				non_block_start();
+ 			_ret = ops->invalidate_range_start(subscription, range);
+ 			if (!mmu_notifier_range_blockable(range))
+ 				non_block_end();
+ 			if (_ret) {
+ 				pr_info("%pS callback failed with %d in %sblockable context.\n",
+ 					ops->invalidate_range_start, _ret,
+ 					!mmu_notifier_range_blockable(range) ?
+ 						"non-" :
+ 						"");
+ 				WARN_ON(mmu_notifier_range_blockable(range) ||
+ 					_ret != -EAGAIN);
+ 				ret = _ret;
+ 			}
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  		}
  	}
  	srcu_read_unlock(&srcu, id);
 -
 -	return ret;
  }
  
 -int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 +void __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
  {
 -	struct mmu_notifier_subscriptions *subscriptions =
 -		range->mm->notifier_subscriptions;
 -	int ret;
 +	struct mmu_notifier_mm *mmn_mm = range->mm->mmu_notifier_mm;
  
 -	if (subscriptions->has_itree) {
 -		ret = mn_itree_invalidate(subscriptions, range);
 -		if (ret)
 -			return ret;
 +	if (mmn_mm->has_itree) {
 +		mn_itree_invalidate(mmn_mm, range);
  	}
 -	if (!hlist_empty(&subscriptions->list))
 -		return mn_hlist_invalidate_range_start(subscriptions, range);
 -	return 0;
 +	if (!hlist_empty(&mmn_mm->list))
 +		mn_hlist_invalidate_range_start(mmn_mm, range);
  }
  
 -static void
 -mn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,
 -			struct mmu_notifier_range *range, bool only_end)
 +static void mn_hlist_invalidate_end(struct mmu_notifier_mm *mmn_mm,
 +				    struct mmu_notifier_range *range,
 +				    bool only_end)
  {
- 	struct mmu_notifier *mn;
+ 	struct mmu_notifier *subscription;
  	int id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist) {
++=======
+ 	hlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist) {
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  		/*
  		 * Call invalidate_range here too to avoid the need for the
  		 * subsystem of having to register an invalidate_range_end
@@@ -493,12 -534,18 +569,27 @@@
  		 * is safe to do when we know that a call to invalidate_range()
  		 * already happen under page table lock.
  		 */
++<<<<<<< HEAD
 +		if (!only_end && mn->ops->invalidate_range)
 +			mn->ops->invalidate_range(mn, range->mm,
 +						  range->start,
 +						  range->end);
 +		if (mn->ops->invalidate_range_end) {
 +			mn->ops->invalidate_range_end(mn, range->mm, range->start, range->end);
++=======
+ 		if (!only_end && subscription->ops->invalidate_range)
+ 			subscription->ops->invalidate_range(subscription,
+ 							    range->mm,
+ 							    range->start,
+ 							    range->end);
+ 		if (subscription->ops->invalidate_range_end) {
+ 			if (!mmu_notifier_range_blockable(range))
+ 				non_block_start();
+ 			subscription->ops->invalidate_range_end(subscription,
+ 								range);
+ 			if (!mmu_notifier_range_blockable(range))
+ 				non_block_end();
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  		}
  	}
  	srcu_read_unlock(&srcu, id);
@@@ -525,43 -573,13 +616,51 @@@ void __mmu_notifier_invalidate_range(st
  	int id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops->invalidate_range)
 +			mn->ops->invalidate_range(mn, mm, start, end);
++=======
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops->invalidate_range)
+ 			subscription->ops->invalidate_range(subscription, mm,
+ 							    start, end);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
 +	}
 +	srcu_read_unlock(&srcu, id);
 +}
 +EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range);
 +
 +/*
 + * Must be called while holding mm->mmap_sem for either read or write.
 + * The result is guaranteed to be valid until mm->mmap_sem is dropped.
 + */
 +bool mm_has_blockable_invalidate_notifiers(struct mm_struct *mm)
 +{
 +	struct mmu_notifier *mn;
 +	int id;
 +	bool ret = false;
 +
 +	WARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));
 +
 +	if (!mm_has_notifiers(mm))
 +		return ret;
 +
 +	id = srcu_read_lock(&srcu);
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (!mn->ops->invalidate_range &&
 +		    !mn->ops->invalidate_range_start &&
 +		    !mn->ops->invalidate_range_end)
 +				continue;
 +
 +		if (!(mn->ops->flags & MMU_INVALIDATE_DOES_NOT_BLOCK)) {
 +			ret = true;
 +			break;
 +		}
  	}
  	srcu_read_unlock(&srcu, id);
 +	return ret;
  }
  
  /*
@@@ -569,42 -587,39 +668,43 @@@
   * write mode. A NULL mn signals the notifier is being registered for itree
   * mode.
   */
- int __mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm)
+ int __mmu_notifier_register(struct mmu_notifier *subscription,
+ 			    struct mm_struct *mm)
  {
 -	struct mmu_notifier_subscriptions *subscriptions = NULL;
 +	struct mmu_notifier_mm *mmu_notifier_mm = NULL;
  	int ret;
  
 -	lockdep_assert_held_write(&mm->mmap_sem);
 +	mmap_assert_write_locked(mm);
  	BUG_ON(atomic_read(&mm->mm_users) <= 0);
  
 -	if (IS_ENABLED(CONFIG_LOCKDEP)) {
 -		fs_reclaim_acquire(GFP_KERNEL);
 -		lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);
 -		lock_map_release(&__mmu_notifier_invalidate_range_start_map);
 -		fs_reclaim_release(GFP_KERNEL);
 +	if (mn) {
 +		mn->_rh = kmalloc(sizeof(*mn->_rh), GFP_KERNEL);
 +		if (!mn->_rh) {
 +			return -ENOMEM;
 +		}
 +		mn->_rh->back_ptr = mn;
 +		RH_KABI_AUX_SET_SIZE(mn, mmu_notifier);
  	}
  
 -	if (!mm->notifier_subscriptions) {
 +	if (!mm->mmu_notifier_mm) {
  		/*
  		 * kmalloc cannot be called under mm_take_all_locks(), but we
 -		 * know that mm->notifier_subscriptions can't change while we
 -		 * hold the write side of the mmap_sem.
 +		 * know that mm->mmu_notifier_mm can't change while we hold
 +		 * the write side of the mmap_sem.
  		 */
 -		subscriptions = kzalloc(
 -			sizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);
 -		if (!subscriptions)
 -			return -ENOMEM;
 +		mmu_notifier_mm =
 +			kzalloc(sizeof(struct mmu_notifier_mm), GFP_KERNEL);
 +		if (!mmu_notifier_mm) {
 +			ret = -ENOMEM;
 +			goto out_free_rh;
 +		}
  
 -		INIT_HLIST_HEAD(&subscriptions->list);
 -		spin_lock_init(&subscriptions->lock);
 -		subscriptions->invalidate_seq = 2;
 -		subscriptions->itree = RB_ROOT_CACHED;
 -		init_waitqueue_head(&subscriptions->wq);
 -		INIT_HLIST_HEAD(&subscriptions->deferred_list);
 +		INIT_HLIST_HEAD(&mmu_notifier_mm->list);
 +		spin_lock_init(&mmu_notifier_mm->lock);
 +		mmu_notifier_mm->invalidate_seq = 2;
 +		mmu_notifier_mm->itree = RB_ROOT_CACHED;
 +		init_waitqueue_head(&mmu_notifier_mm->wq);
 +		INIT_HLIST_HEAD(&mmu_notifier_mm->deferred_list);
  	}
  
  	ret = mm_take_all_locks(mm);
@@@ -619,29 -634,29 +719,42 @@@
  	 * We can't race against any other mmu notifier method either
  	 * thanks to mm_take_all_locks().
  	 *
 -	 * release semantics on the initialization of the
 -	 * mmu_notifier_subscriptions's contents are provided for unlocked
 -	 * readers.  acquire can only be used while holding the mmgrab or
 -	 * mmget, and is safe because once created the
 -	 * mmu_notifier_subscriptions is not freed until the mm is destroyed.
 -	 * As above, users holding the mmap_sem or one of the
 +	 * release semantics on the initialization of the mmu_notifier_mm's
 +	 * contents are provided for unlocked readers.  acquire can only be
 +	 * used while holding the mmgrab or mmget, and is safe because once
 +	 * created the mmu_notififer_mm is not freed until the mm is
 +	 * destroyed.  As above, users holding the mmap_sem or one of the
  	 * mm_take_all_locks() do not need to use acquire semantics.
  	 */
 -	if (subscriptions)
 -		smp_store_release(&mm->notifier_subscriptions, subscriptions);
 +	if (mmu_notifier_mm)
 +		smp_store_release(&mm->mmu_notifier_mm,  mmu_notifier_mm);
 +
++<<<<<<< HEAD
 +	if (mn) {
 +		/* Pairs with the mmdrop in mmu_notifier_unregister_ */
 +		mmgrab(mm);
 +		if (RH_KABI_AUX(mn, mmu_notifier, mm)) {
 +			mn->_rh->mm = mm;
 +			mn->_rh->users = 1;
 +		}
  
 +		spin_lock(&mm->mmu_notifier_mm->lock);
 +		hlist_add_head_rcu(&mn->hlist, &mm->mmu_notifier_mm->list);
 +		spin_unlock(&mm->mmu_notifier_mm->lock);
++=======
+ 	if (subscription) {
+ 		/* Pairs with the mmdrop in mmu_notifier_unregister_* */
+ 		mmgrab(mm);
+ 		subscription->mm = mm;
+ 		subscription->users = 1;
+ 
+ 		spin_lock(&mm->notifier_subscriptions->lock);
+ 		hlist_add_head_rcu(&subscription->hlist,
+ 				   &mm->notifier_subscriptions->list);
+ 		spin_unlock(&mm->notifier_subscriptions->lock);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	} else
 -		mm->notifier_subscriptions->has_itree = true;
 +		mm->mmu_notifier_mm->has_itree = true;
  
  	mm_drop_all_locks(mm);
  	BUG_ON(atomic_read(&mm->mm_users) <= 0);
@@@ -679,9 -692,9 +793,15 @@@ int mmu_notifier_register(struct mmu_no
  {
  	int ret;
  
++<<<<<<< HEAD
 +	mmap_write_lock(mm);
 +	ret = __mmu_notifier_register(mn, mm);
 +	mmap_write_unlock(mm);
++=======
+ 	down_write(&mm->mmap_sem);
+ 	ret = __mmu_notifier_register(subscription, mm);
+ 	up_write(&mm->mmap_sem);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	return ret;
  }
  EXPORT_SYMBOL_GPL(mmu_notifier_register);
@@@ -689,24 -702,22 +809,39 @@@
  static struct mmu_notifier *
  find_get_mmu_notifier(struct mm_struct *mm, const struct mmu_notifier_ops *ops)
  {
- 	struct mmu_notifier *mn;
+ 	struct mmu_notifier *subscription;
  
++<<<<<<< HEAD
 +	spin_lock(&mm->mmu_notifier_mm->lock);
 +	hlist_for_each_entry_rcu (mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (mn->ops != ops)
 +			continue;
 +
 +		if (!RH_KABI_AUX(mn, mmu_notifier, mm))
 +			continue;
 +
 +		if (likely(mn->_rh->users != UINT_MAX))
 +			mn->_rh->users++;
 +		else
 +			mn = ERR_PTR(-EOVERFLOW);
 +		spin_unlock(&mm->mmu_notifier_mm->lock);
 +		return mn;
++=======
+ 	spin_lock(&mm->notifier_subscriptions->lock);
+ 	hlist_for_each_entry_rcu(subscription,
+ 				 &mm->notifier_subscriptions->list, hlist) {
+ 		if (subscription->ops != ops)
+ 			continue;
+ 
+ 		if (likely(subscription->users != UINT_MAX))
+ 			subscription->users++;
+ 		else
+ 			subscription = ERR_PTR(-EOVERFLOW);
+ 		spin_unlock(&mm->notifier_subscriptions->lock);
+ 		return subscription;
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	}
 -	spin_unlock(&mm->notifier_subscriptions->lock);
 +	spin_unlock(&mm->mmu_notifier_mm->lock);
  	return NULL;
  }
  
@@@ -730,28 -741,27 +865,42 @@@
  struct mmu_notifier *mmu_notifier_get_locked(const struct mmu_notifier_ops *ops,
  					     struct mm_struct *mm)
  {
- 	struct mmu_notifier *mn;
+ 	struct mmu_notifier *subscription;
  	int ret;
  
 -	lockdep_assert_held_write(&mm->mmap_sem);
 +	mmap_assert_write_locked(mm);
 +
++<<<<<<< HEAD
 +	if (mm->mmu_notifier_mm) {
 +		mn = find_get_mmu_notifier(mm, ops);
 +		if (mn)
 +			return mn;
 +	}
 +
 +	mn = ops->alloc_notifier(mm);
 +	if (IS_ERR(mn))
 +		return mn;
  
 +	mn->ops = ops;
 +	ret = __mmu_notifier_register(mn, mm);
++=======
+ 	if (mm->notifier_subscriptions) {
+ 		subscription = find_get_mmu_notifier(mm, ops);
+ 		if (subscription)
+ 			return subscription;
+ 	}
+ 
+ 	subscription = ops->alloc_notifier(mm);
+ 	if (IS_ERR(subscription))
+ 		return subscription;
+ 	subscription->ops = ops;
+ 	ret = __mmu_notifier_register(subscription, mm);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	if (ret)
  		goto out_free;
- 	return mn;
+ 	return subscription;
  out_free:
- 	mn->ops->free_notifier(mn);
+ 	subscription->ops->free_notifier(subscription);
  	return ERR_PTR(ret);
  }
  EXPORT_SYMBOL_GPL(mmu_notifier_get_locked);
@@@ -790,17 -801,17 +940,22 @@@ void mmu_notifier_unregister(struct mmu
  		 * exit_mmap will block in mmu_notifier_release to guarantee
  		 * that ->release is called before freeing the pages.
  		 */
- 		if (mn->ops->release)
- 			mn->ops->release(mn, mm);
+ 		if (subscription->ops->release)
+ 			subscription->ops->release(subscription, mm);
  		srcu_read_unlock(&srcu, id);
  
 -		spin_lock(&mm->notifier_subscriptions->lock);
 +		spin_lock(&mm->mmu_notifier_mm->lock);
  		/*
  		 * Can not use list_del_rcu() since __mmu_notifier_release
  		 * can delete it before we hold the lock.
  		 */
++<<<<<<< HEAD
 +		hlist_del_init_rcu(&mn->hlist);
 +		spin_unlock(&mm->mmu_notifier_mm->lock);
++=======
+ 		hlist_del_init_rcu(&subscription->hlist);
+ 		spin_unlock(&mm->notifier_subscriptions->lock);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	}
  
  	/*
@@@ -817,40 -826,13 +972,48 @@@
  }
  EXPORT_SYMBOL_GPL(mmu_notifier_unregister);
  
 +static void mmu_notifier_rh_free_rcu(struct rcu_head *rcu)
 +{
 +	kfree(container_of(rcu, struct mmu_notifier_rh, rcu));
 +}
 +
 +/*
 + * Same as mmu_notifier_unregister but no callback and no srcu synchronization.
 + */
 +void mmu_notifier_unregister_no_release(struct mmu_notifier *mn,
 +					struct mm_struct *mm)
 +{
 +	spin_lock(&mm->mmu_notifier_mm->lock);
 +	/*
 +	 * Can not use list_del_rcu() since __mmu_notifier_release
 +	 * can delete it before we hold the lock.
 +	 */
 +	hlist_del_init_rcu(&mn->hlist);
 +	spin_unlock(&mm->mmu_notifier_mm->lock);
 +
 +	call_srcu(&srcu, &mn->_rh->rcu, mmu_notifier_rh_free_rcu);
 +
 +	BUG_ON(atomic_read(&mm->mm_count) <= 0);
 +	mmdrop(mm);
 +}
 +EXPORT_SYMBOL_GPL(mmu_notifier_unregister_no_release);
 +
  static void mmu_notifier_free_rcu(struct rcu_head *rcu)
  {
++<<<<<<< HEAD
 +	struct mmu_notifier_rh *mn_rh = container_of(rcu, struct mmu_notifier_rh, rcu);
 +	struct mm_struct *mm = mn_rh->mm;
 +	struct mmu_notifier *mn = mn_rh->back_ptr;
 +
 +	kfree(mn_rh);
 +	mn->ops->free_notifier(mn);
++=======
+ 	struct mmu_notifier *subscription =
+ 		container_of(rcu, struct mmu_notifier, rcu);
+ 	struct mm_struct *mm = subscription->mm;
+ 
+ 	subscription->ops->free_notifier(subscription);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
  	/* Pairs with the get in __mmu_notifier_register() */
  	mmdrop(mm);
  }
@@@ -1029,49 -1054,6 +1192,61 @@@ void mmu_interval_notifier_remove(struc
  }
  EXPORT_SYMBOL_GPL(mmu_interval_notifier_remove);
  
 +/**
 + * mmu_notifier_put - Release the reference on the notifier
 + * @mn: The notifier to act on
 + *
 + * This function must be paired with each mmu_notifier_get(), it releases the
 + * reference obtained by the get. If this is the last reference then process
 + * to free the notifier will be run asynchronously.
 + *
 + * Unlike mmu_notifier_unregister() the get/put flow only calls ops->release
 + * when the mm_struct is destroyed. Instead free_notifier is always called to
 + * release any resources held by the user.
 + *
 + * As ops->release is not guaranteed to be called, the user must ensure that
 + * all sptes are dropped, and no new sptes can be established before
 + * mmu_notifier_put() is called.
 + *
 + * This function can be called from the ops->release callback, however the
 + * caller must still ensure it is called pairwise with mmu_notifier_get().
 + *
 + * Modules calling this function must call mmu_notifier_synchronize() in
 + * their __exit functions to ensure the async work is completed.
 + */
- void mmu_notifier_put(struct mmu_notifier *mn)
++void mmu_notifier_put(struct mmu_notifier *subscription)
 +{
++<<<<<<< HEAD
 +	struct mm_struct *mm;
 +
 +	if (!RH_KABI_AUX(mn, mmu_notifier, mm))
 +		return;
 +
 +	mm = mn->_rh->mm;
 +	spin_lock(&mm->mmu_notifier_mm->lock);
 +	if (WARN_ON(!mn->_rh->users) || --mn->_rh->users)
 +		goto out_unlock;
 +	hlist_del_init_rcu(&mn->hlist);
 +	spin_unlock(&mm->mmu_notifier_mm->lock);
 +
 +	call_srcu(&srcu, &mn->_rh->rcu, mmu_notifier_free_rcu);
++=======
++	struct mm_struct *mm = subscription->mm;
++
++	spin_lock(&mm->notifier_subscriptions->lock);
++	if (WARN_ON(!subscription->users) || --subscription->users)
++		goto out_unlock;
++	hlist_del_init_rcu(&subscription->hlist);
++	spin_unlock(&mm->notifier_subscriptions->lock);
++
++	call_srcu(&srcu, &subscription->rcu, mmu_notifier_free_rcu);
++>>>>>>> 1991722a70ff (mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier)
 +	return;
 +
 +out_unlock:
 +	spin_unlock(&mm->mmu_notifier_mm->lock);
 +}
 +EXPORT_SYMBOL_GPL(mmu_notifier_put);
  /**
   * mmu_notifier_synchronize - Ensure all mmu_notifiers are freed
   *
* Unmerged path include/linux/mmu_notifier.h
* Unmerged path mm/mmu_notifier.c
