random32: add noise from network and scheduling activity

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Willy Tarreau <w@1wt.eu>
commit 3744741adab6d9195551ce30e65e726c7a408421
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/3744741a.failed

With the removal of the interrupt perturbations in previous random32
change (random32: make prandom_u32() output unpredictable), the PRNG
has become 100% deterministic again. While SipHash is expected to be
way more robust against brute force than the previous Tausworthe LFSR,
there's still the risk that whoever has even one temporary access to
the PRNG's internal state is able to predict all subsequent draws till
the next reseed (roughly every minute). This may happen through a side
channel attack or any data leak.

This patch restores the spirit of commit f227e3ec3b5c ("random32: update
the net random state on interrupt and activity") in that it will perturb
the internal PRNG's statee using externally collected noise, except that
it will not pick that noise from the random pool's bits nor upon
interrupt, but will rather combine a few elements along the Tx path
that are collectively hard to predict, such as dev, skb and txq
pointers, packet length and jiffies values. These ones are combined
using a single round of SipHash into a single long variable that is
mixed with the net_rand_state upon each invocation.

The operation was inlined because it produces very small and efficient
code, typically 3 xor, 2 add and 2 rol. The performance was measured
to be the same (even very slightly better) than before the switch to
SipHash; on a 6-core 12-thread Core i7-8700k equipped with a 40G NIC
(i40e), the connection rate dropped from 556k/s to 555k/s while the
SYN cookie rate grew from 5.38 Mpps to 5.45 Mpps.

Link: https://lore.kernel.org/netdev/20200808152628.GA27941@SDF.ORG/
	Cc: George Spelvin <lkml@sdf.org>
	Cc: Amit Klein <aksecurity@gmail.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: tytso@mit.edu
	Cc: Florian Westphal <fw@strlen.de>
	Cc: Marc Plumb <lkml.mplumb@gmail.com>
	Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
	Signed-off-by: Willy Tarreau <w@1wt.eu>
(cherry picked from commit 3744741adab6d9195551ce30e65e726c7a408421)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/prandom.h
#	lib/random32.c
#	net/core/dev.c
diff --cc lib/random32.c
index 53acf5adef8f,7f047844e494..000000000000
--- a/lib/random32.c
+++ b/lib/random32.c
@@@ -463,5 -305,272 +463,273 @@@ static void __init prandom_state_selfte
  		pr_warn("prandom: %d/%d self tests failed\n", errors, runs);
  	else
  		pr_info("prandom: %d self tests passed\n", runs);
 -	return 0;
  }
 -core_initcall(prandom_state_selftest);
  #endif
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * The prandom_u32() implementation is now completely separate from the
+  * prandom_state() functions, which are retained (for now) for compatibility.
+  *
+  * Because of (ab)use in the networking code for choosing random TCP/UDP port
+  * numbers, which open DoS possibilities if guessable, we want something
+  * stronger than a standard PRNG.  But the performance requirements of
+  * the network code do not allow robust crypto for this application.
+  *
+  * So this is a homebrew Junior Spaceman implementation, based on the
+  * lowest-latency trustworthy crypto primitive available, SipHash.
+  * (The authors of SipHash have not been consulted about this abuse of
+  * their work.)
+  *
+  * Standard SipHash-2-4 uses 2n+4 rounds to hash n words of input to
+  * one word of output.  This abbreviated version uses 2 rounds per word
+  * of output.
+  */
+ 
+ struct siprand_state {
+ 	unsigned long v0;
+ 	unsigned long v1;
+ 	unsigned long v2;
+ 	unsigned long v3;
+ };
+ 
+ static DEFINE_PER_CPU(struct siprand_state, net_rand_state) __latent_entropy;
+ DEFINE_PER_CPU(unsigned long, net_rand_noise);
+ EXPORT_PER_CPU_SYMBOL(net_rand_noise);
+ 
+ /*
+  * This is the core CPRNG function.  As "pseudorandom", this is not used
+  * for truly valuable things, just intended to be a PITA to guess.
+  * For maximum speed, we do just two SipHash rounds per word.  This is
+  * the same rate as 4 rounds per 64 bits that SipHash normally uses,
+  * so hopefully it's reasonably secure.
+  *
+  * There are two changes from the official SipHash finalization:
+  * - We omit some constants XORed with v2 in the SipHash spec as irrelevant;
+  *   they are there only to make the output rounds distinct from the input
+  *   rounds, and this application has no input rounds.
+  * - Rather than returning v0^v1^v2^v3, return v1+v3.
+  *   If you look at the SipHash round, the last operation on v3 is
+  *   "v3 ^= v0", so "v0 ^ v3" just undoes that, a waste of time.
+  *   Likewise "v1 ^= v2".  (The rotate of v2 makes a difference, but
+  *   it still cancels out half of the bits in v2 for no benefit.)
+  *   Second, since the last combining operation was xor, continue the
+  *   pattern of alternating xor/add for a tiny bit of extra non-linearity.
+  */
+ static inline u32 siprand_u32(struct siprand_state *s)
+ {
+ 	unsigned long v0 = s->v0, v1 = s->v1, v2 = s->v2, v3 = s->v3;
+ 	unsigned long n = raw_cpu_read(net_rand_noise);
+ 
+ 	v3 ^= n;
+ 	PRND_SIPROUND(v0, v1, v2, v3);
+ 	PRND_SIPROUND(v0, v1, v2, v3);
+ 	v0 ^= n;
+ 	s->v0 = v0;  s->v1 = v1;  s->v2 = v2;  s->v3 = v3;
+ 	return v1 + v3;
+ }
+ 
+ 
+ /**
+  *	prandom_u32 - pseudo random number generator
+  *
+  *	A 32 bit pseudo-random number is generated using a fast
+  *	algorithm suitable for simulation. This algorithm is NOT
+  *	considered safe for cryptographic use.
+  */
+ u32 prandom_u32(void)
+ {
+ 	struct siprand_state *state = get_cpu_ptr(&net_rand_state);
+ 	u32 res = siprand_u32(state);
+ 
+ 	trace_prandom_u32(res);
+ 	put_cpu_ptr(&net_rand_state);
+ 	return res;
+ }
+ EXPORT_SYMBOL(prandom_u32);
+ 
+ /**
+  *	prandom_bytes - get the requested number of pseudo-random bytes
+  *	@buf: where to copy the pseudo-random bytes to
+  *	@bytes: the requested number of bytes
+  */
+ void prandom_bytes(void *buf, size_t bytes)
+ {
+ 	struct siprand_state *state = get_cpu_ptr(&net_rand_state);
+ 	u8 *ptr = buf;
+ 
+ 	while (bytes >= sizeof(u32)) {
+ 		put_unaligned(siprand_u32(state), (u32 *)ptr);
+ 		ptr += sizeof(u32);
+ 		bytes -= sizeof(u32);
+ 	}
+ 
+ 	if (bytes > 0) {
+ 		u32 rem = siprand_u32(state);
+ 
+ 		do {
+ 			*ptr++ = (u8)rem;
+ 			rem >>= BITS_PER_BYTE;
+ 		} while (--bytes > 0);
+ 	}
+ 	put_cpu_ptr(&net_rand_state);
+ }
+ EXPORT_SYMBOL(prandom_bytes);
+ 
+ /**
+  *	prandom_seed - add entropy to pseudo random number generator
+  *	@entropy: entropy value
+  *
+  *	Add some additional seed material to the prandom pool.
+  *	The "entropy" is actually our IP address (the only caller is
+  *	the network code), not for unpredictability, but to ensure that
+  *	different machines are initialized differently.
+  */
+ void prandom_seed(u32 entropy)
+ {
+ 	int i;
+ 
+ 	add_device_randomness(&entropy, sizeof(entropy));
+ 
+ 	for_each_possible_cpu(i) {
+ 		struct siprand_state *state = per_cpu_ptr(&net_rand_state, i);
+ 		unsigned long v0 = state->v0, v1 = state->v1;
+ 		unsigned long v2 = state->v2, v3 = state->v3;
+ 
+ 		do {
+ 			v3 ^= entropy;
+ 			PRND_SIPROUND(v0, v1, v2, v3);
+ 			PRND_SIPROUND(v0, v1, v2, v3);
+ 			v0 ^= entropy;
+ 		} while (unlikely(!v0 || !v1 || !v2 || !v3));
+ 
+ 		WRITE_ONCE(state->v0, v0);
+ 		WRITE_ONCE(state->v1, v1);
+ 		WRITE_ONCE(state->v2, v2);
+ 		WRITE_ONCE(state->v3, v3);
+ 	}
+ }
+ EXPORT_SYMBOL(prandom_seed);
+ 
+ /*
+  *	Generate some initially weak seeding values to allow
+  *	the prandom_u32() engine to be started.
+  */
+ static int __init prandom_init_early(void)
+ {
+ 	int i;
+ 	unsigned long v0, v1, v2, v3;
+ 
+ 	if (!arch_get_random_long(&v0))
+ 		v0 = jiffies;
+ 	if (!arch_get_random_long(&v1))
+ 		v1 = random_get_entropy();
+ 	v2 = v0 ^ PRND_K0;
+ 	v3 = v1 ^ PRND_K1;
+ 
+ 	for_each_possible_cpu(i) {
+ 		struct siprand_state *state;
+ 
+ 		v3 ^= i;
+ 		PRND_SIPROUND(v0, v1, v2, v3);
+ 		PRND_SIPROUND(v0, v1, v2, v3);
+ 		v0 ^= i;
+ 
+ 		state = per_cpu_ptr(&net_rand_state, i);
+ 		state->v0 = v0;  state->v1 = v1;
+ 		state->v2 = v2;  state->v3 = v3;
+ 	}
+ 
+ 	return 0;
+ }
+ core_initcall(prandom_init_early);
+ 
+ 
+ /* Stronger reseeding when available, and periodically thereafter. */
+ static void prandom_reseed(struct timer_list *unused);
+ 
+ static DEFINE_TIMER(seed_timer, prandom_reseed);
+ 
+ static void prandom_reseed(struct timer_list *unused)
+ {
+ 	unsigned long expires;
+ 	int i;
+ 
+ 	/*
+ 	 * Reinitialize each CPU's PRNG with 128 bits of key.
+ 	 * No locking on the CPUs, but then somewhat random results are,
+ 	 * well, expected.
+ 	 */
+ 	for_each_possible_cpu(i) {
+ 		struct siprand_state *state;
+ 		unsigned long v0 = get_random_long(), v2 = v0 ^ PRND_K0;
+ 		unsigned long v1 = get_random_long(), v3 = v1 ^ PRND_K1;
+ #if BITS_PER_LONG == 32
+ 		int j;
+ 
+ 		/*
+ 		 * On 32-bit machines, hash in two extra words to
+ 		 * approximate 128-bit key length.  Not that the hash
+ 		 * has that much security, but this prevents a trivial
+ 		 * 64-bit brute force.
+ 		 */
+ 		for (j = 0; j < 2; j++) {
+ 			unsigned long m = get_random_long();
+ 
+ 			v3 ^= m;
+ 			PRND_SIPROUND(v0, v1, v2, v3);
+ 			PRND_SIPROUND(v0, v1, v2, v3);
+ 			v0 ^= m;
+ 		}
+ #endif
+ 		/*
+ 		 * Probably impossible in practice, but there is a
+ 		 * theoretical risk that a race between this reseeding
+ 		 * and the target CPU writing its state back could
+ 		 * create the all-zero SipHash fixed point.
+ 		 *
+ 		 * To ensure that never happens, ensure the state
+ 		 * we write contains no zero words.
+ 		 */
+ 		state = per_cpu_ptr(&net_rand_state, i);
+ 		WRITE_ONCE(state->v0, v0 ? v0 : -1ul);
+ 		WRITE_ONCE(state->v1, v1 ? v1 : -1ul);
+ 		WRITE_ONCE(state->v2, v2 ? v2 : -1ul);
+ 		WRITE_ONCE(state->v3, v3 ? v3 : -1ul);
+ 	}
+ 
+ 	/* reseed every ~60 seconds, in [40 .. 80) interval with slack */
+ 	expires = round_jiffies(jiffies + 40 * HZ + prandom_u32_max(40 * HZ));
+ 	mod_timer(&seed_timer, expires);
+ }
+ 
+ /*
+  * The random ready callback can be called from almost any interrupt.
+  * To avoid worrying about whether it's safe to delay that interrupt
+  * long enough to seed all CPUs, just schedule an immediate timer event.
+  */
+ static void prandom_timer_start(struct random_ready_callback *unused)
+ {
+ 	mod_timer(&seed_timer, jiffies);
+ }
+ 
+ /*
+  * Start periodic full reseeding as soon as strong
+  * random numbers are available.
+  */
+ static int __init prandom_init_late(void)
+ {
+ 	static struct random_ready_callback random_ready = {
+ 		.func = prandom_timer_start
+ 	};
+ 	int ret = add_random_ready_callback(&random_ready);
+ 
+ 	if (ret == -EALREADY) {
+ 		prandom_timer_start(&random_ready);
+ 		ret = 0;
+ 	}
+ 	return ret;
+ }
+ late_initcall(prandom_init_late);
++>>>>>>> 3744741adab6 (random32: add noise from network and scheduling activity)
diff --cc net/core/dev.c
index 93c5e03ab56c,82dc6b48e45f..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -149,6 -144,8 +149,11 @@@
  #include <linux/net_namespace.h>
  #include <linux/indirect_call_wrapper.h>
  #include <net/devlink.h>
++<<<<<<< HEAD
++=======
+ #include <linux/pm_runtime.h>
+ #include <linux/prandom.h>
++>>>>>>> 3744741adab6 (random32: add noise from network and scheduling activity)
  
  #include "net-sysfs.h"
  
* Unmerged path include/linux/prandom.h
* Unmerged path include/linux/prandom.h
diff --git a/kernel/time/timer.c b/kernel/time/timer.c
index 8f5bf3f0c8ce..8cd73b558972 100644
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -1697,6 +1697,8 @@ void update_process_times(int user_tick)
 {
 	struct task_struct *p = current;
 
+	PRANDOM_ADD_NOISE(jiffies, user_tick, p, 0);
+
 	/* Note: this timer irq context must be accounted for as well. */
 	account_process_tick(p, user_tick);
 	run_local_timers();
* Unmerged path lib/random32.c
* Unmerged path net/core/dev.c
