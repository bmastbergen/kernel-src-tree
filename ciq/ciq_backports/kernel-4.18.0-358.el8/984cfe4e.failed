mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 984cfe4e252681d516df056b982e3c47b66fba92
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/984cfe4e.failed

The name mmu_notifier_mm implies that the thing is a mm_struct pointer,
and is difficult to abbreviate. The struct is actually holding the
interval tree and hlist containing the notifiers subscribed to a mm.

Use 'subscriptions' as the variable name for this struct instead of the
really terrible and misleading 'mmn_mm'.

	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 984cfe4e252681d516df056b982e3c47b66fba92)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmu_notifier.h
#	mm/mmu_notifier.c
diff --cc include/linux/mmu_notifier.h
index d924803cea7c,a302925fbc61..000000000000
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@@ -6,17 -5,13 +6,21 @@@
  #include <linux/list.h>
  #include <linux/spinlock.h>
  #include <linux/mm_types.h>
 +#include <linux/mmap_lock.h>
  #include <linux/srcu.h>
  #include <linux/interval_tree.h>
 +#include <linux/rh_kabi.h>
  
++<<<<<<< HEAD
++=======
+ struct mmu_notifier_subscriptions;
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  struct mmu_notifier;
  struct mmu_notifier_range;
 -struct mmu_interval_notifier;
 +struct mmu_notifier_ops;
 +
 +/* mmu_notifier_ops flags */
 +#define MMU_INVALIDATE_DOES_NOT_BLOCK	(0x01)
  
  /**
   * enum mmu_notifier_event - reason for the mmu notifier callback
@@@ -709,12 -692,7 +713,16 @@@ static inline void mmu_notifier_invalid
  {
  }
  
++<<<<<<< HEAD
 +static inline bool mm_has_blockable_invalidate_notifiers(struct mm_struct *mm)
 +{
 +	return false;
 +}
 +
 +static inline void mmu_notifier_mm_init(struct mm_struct *mm)
++=======
+ static inline void mmu_notifier_subscriptions_init(struct mm_struct *mm)
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  {
  }
  
diff --cc mm/mmu_notifier.c
index 4361d699fa34,a409abfb9f26..000000000000
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@@ -274,21 -276,9 +277,21 @@@ static void mn_itree_release(struct mmu
  		WARN_ON(!ret);
  	}
  
- 	mn_itree_inv_end(mmn_mm);
+ 	mn_itree_inv_end(subscriptions);
  }
  
 +/*
 + * This function allows mmu_notifier::release callback to delay a call to
 + * a function that will free appropriate resources. The function must be
 + * quick and must not block.
 + */
 +void mmu_notifier_call_srcu(struct rcu_head *rcu,
 +			    void (*func)(struct rcu_head *rcu))
 +{
 +	call_srcu(&srcu, rcu, func);
 +}
 +EXPORT_SYMBOL_GPL(mmu_notifier_call_srcu);
 +
  /*
   * This function can't run concurrently against mmu_notifier_register
   * because mm->mm_users > 0 during mmu_notifier_register and exit_mmap
@@@ -432,48 -424,86 +437,91 @@@ void __mmu_notifier_change_pte(struct m
  	srcu_read_unlock(&srcu, id);
  }
  
++<<<<<<< HEAD
 +static void mn_itree_invalidate(struct mmu_notifier_mm *mmn_mm,
 +				const struct mmu_notifier_range *range)
++=======
+ static int mn_itree_invalidate(struct mmu_notifier_subscriptions *subscriptions,
+ 			       const struct mmu_notifier_range *range)
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  {
  	struct mmu_interval_notifier *mni;
  	unsigned long cur_seq;
  
++<<<<<<< HEAD
 +	for (mni = mn_itree_inv_start_range(mmn_mm, range, &cur_seq); mni;
 +	     mni = mn_itree_inv_next(mni, range)) {
++=======
+ 	for (mni = mn_itree_inv_start_range(subscriptions, range, &cur_seq);
+ 	     mni; mni = mn_itree_inv_next(mni, range)) {
+ 		bool ret;
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  
 -		ret = mni->ops->invalidate(mni, range, cur_seq);
 -		if (!ret) {
 -			if (WARN_ON(mmu_notifier_range_blockable(range)))
 -				continue;
 -			goto out_would_block;
 -		}
 +		mni->ops->invalidate(mni, range, cur_seq);
  	}
++<<<<<<< HEAD
 +}
 +
 +static void mn_hlist_invalidate_range_start(struct mmu_notifier_mm *mmn_mm,
 +					    struct mmu_notifier_range *range)
++=======
+ 	return 0;
+ 
+ out_would_block:
+ 	/*
+ 	 * On -EAGAIN the non-blocking caller is not allowed to call
+ 	 * invalidate_range_end()
+ 	 */
+ 	mn_itree_inv_end(subscriptions);
+ 	return -EAGAIN;
+ }
+ 
+ static int mn_hlist_invalidate_range_start(
+ 	struct mmu_notifier_subscriptions *subscriptions,
+ 	struct mmu_notifier_range *range)
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  {
  	struct mmu_notifier *mn;
 -	int ret = 0;
  	int id;
  
  	id = srcu_read_lock(&srcu);
- 	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist) {
+ 	hlist_for_each_entry_rcu(mn, &subscriptions->list, hlist) {
  		if (mn->ops->invalidate_range_start) {
 -			int _ret;
 -
 -			if (!mmu_notifier_range_blockable(range))
 -				non_block_start();
 -			_ret = mn->ops->invalidate_range_start(mn, range);
 -			if (!mmu_notifier_range_blockable(range))
 -				non_block_end();
 -			if (_ret) {
 -				pr_info("%pS callback failed with %d in %sblockable context.\n",
 -					mn->ops->invalidate_range_start, _ret,
 -					!mmu_notifier_range_blockable(range) ? "non-" : "");
 -				WARN_ON(mmu_notifier_range_blockable(range) ||
 -					_ret != -EAGAIN);
 -				ret = _ret;
 -			}
 +			mn->ops->invalidate_range_start(mn, range->mm, range->start, range->end);
  		}
  	}
  	srcu_read_unlock(&srcu, id);
 -
 -	return ret;
  }
  
 -int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 +void __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
  {
++<<<<<<< HEAD
 +	struct mmu_notifier_mm *mmn_mm = range->mm->mmu_notifier_mm;
 +
 +	if (mmn_mm->has_itree) {
 +		mn_itree_invalidate(mmn_mm, range);
 +	}
 +	if (!hlist_empty(&mmn_mm->list))
 +		mn_hlist_invalidate_range_start(mmn_mm, range);
++=======
+ 	struct mmu_notifier_subscriptions *subscriptions =
+ 		range->mm->notifier_subscriptions;
+ 	int ret;
+ 
+ 	if (subscriptions->has_itree) {
+ 		ret = mn_itree_invalidate(subscriptions, range);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 	if (!hlist_empty(&subscriptions->list))
+ 		return mn_hlist_invalidate_range_start(subscriptions, range);
+ 	return 0;
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  }
  
- static void mn_hlist_invalidate_end(struct mmu_notifier_mm *mmn_mm,
- 				    struct mmu_notifier_range *range,
- 				    bool only_end)
+ static void
+ mn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,
+ 			struct mmu_notifier_range *range, bool only_end)
  {
  	struct mmu_notifier *mn;
  	int id;
@@@ -571,40 -574,36 +620,47 @@@ bool mm_has_blockable_invalidate_notifi
   */
  int __mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm)
  {
- 	struct mmu_notifier_mm *mmu_notifier_mm = NULL;
+ 	struct mmu_notifier_subscriptions *subscriptions = NULL;
  	int ret;
  
 -	lockdep_assert_held_write(&mm->mmap_sem);
 +	mmap_assert_write_locked(mm);
  	BUG_ON(atomic_read(&mm->mm_users) <= 0);
  
 -	if (IS_ENABLED(CONFIG_LOCKDEP)) {
 -		fs_reclaim_acquire(GFP_KERNEL);
 -		lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);
 -		lock_map_release(&__mmu_notifier_invalidate_range_start_map);
 -		fs_reclaim_release(GFP_KERNEL);
 +	if (mn) {
 +		mn->_rh = kmalloc(sizeof(*mn->_rh), GFP_KERNEL);
 +		if (!mn->_rh) {
 +			return -ENOMEM;
 +		}
 +		mn->_rh->back_ptr = mn;
 +		RH_KABI_AUX_SET_SIZE(mn, mmu_notifier);
  	}
  
- 	if (!mm->mmu_notifier_mm) {
+ 	if (!mm->notifier_subscriptions) {
  		/*
  		 * kmalloc cannot be called under mm_take_all_locks(), but we
- 		 * know that mm->mmu_notifier_mm can't change while we hold
- 		 * the write side of the mmap_sem.
+ 		 * know that mm->notifier_subscriptions can't change while we
+ 		 * hold the write side of the mmap_sem.
  		 */
++<<<<<<< HEAD
 +		mmu_notifier_mm =
 +			kzalloc(sizeof(struct mmu_notifier_mm), GFP_KERNEL);
 +		if (!mmu_notifier_mm) {
 +			ret = -ENOMEM;
 +			goto out_free_rh;
 +		}
- 
- 		INIT_HLIST_HEAD(&mmu_notifier_mm->list);
- 		spin_lock_init(&mmu_notifier_mm->lock);
- 		mmu_notifier_mm->invalidate_seq = 2;
- 		mmu_notifier_mm->itree = RB_ROOT_CACHED;
- 		init_waitqueue_head(&mmu_notifier_mm->wq);
- 		INIT_HLIST_HEAD(&mmu_notifier_mm->deferred_list);
++=======
+ 		subscriptions = kzalloc(
+ 			sizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);
+ 		if (!subscriptions)
+ 			return -ENOMEM;
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
+ 
+ 		INIT_HLIST_HEAD(&subscriptions->list);
+ 		spin_lock_init(&subscriptions->lock);
+ 		subscriptions->invalidate_seq = 2;
+ 		subscriptions->itree = RB_ROOT_CACHED;
+ 		init_waitqueue_head(&subscriptions->wq);
+ 		INIT_HLIST_HEAD(&subscriptions->deferred_list);
  	}
  
  	ret = mm_take_all_locks(mm);
@@@ -619,39 -618,36 +675,50 @@@
  	 * We can't race against any other mmu notifier method either
  	 * thanks to mm_take_all_locks().
  	 *
- 	 * release semantics on the initialization of the mmu_notifier_mm's
- 	 * contents are provided for unlocked readers.  acquire can only be
- 	 * used while holding the mmgrab or mmget, and is safe because once
- 	 * created the mmu_notififer_mm is not freed until the mm is
- 	 * destroyed.  As above, users holding the mmap_sem or one of the
+ 	 * release semantics on the initialization of the
+ 	 * mmu_notifier_subscriptions's contents are provided for unlocked
+ 	 * readers.  acquire can only be used while holding the mmgrab or
+ 	 * mmget, and is safe because once created the
+ 	 * mmu_notifier_subscriptions is not freed until the mm is destroyed.
+ 	 * As above, users holding the mmap_sem or one of the
  	 * mm_take_all_locks() do not need to use acquire semantics.
  	 */
++<<<<<<< HEAD
 +	if (mmu_notifier_mm)
 +		smp_store_release(&mm->mmu_notifier_mm,  mmu_notifier_mm);
++=======
+ 	if (subscriptions)
+ 		smp_store_release(&mm->notifier_subscriptions, subscriptions);
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  
  	if (mn) {
 -		/* Pairs with the mmdrop in mmu_notifier_unregister_* */
 +		/* Pairs with the mmdrop in mmu_notifier_unregister_ */
  		mmgrab(mm);
 -		mn->mm = mm;
 -		mn->users = 1;
 +		if (RH_KABI_AUX(mn, mmu_notifier, mm)) {
 +			mn->_rh->mm = mm;
 +			mn->_rh->users = 1;
 +		}
  
- 		spin_lock(&mm->mmu_notifier_mm->lock);
- 		hlist_add_head_rcu(&mn->hlist, &mm->mmu_notifier_mm->list);
- 		spin_unlock(&mm->mmu_notifier_mm->lock);
+ 		spin_lock(&mm->notifier_subscriptions->lock);
+ 		hlist_add_head_rcu(&mn->hlist,
+ 				   &mm->notifier_subscriptions->list);
+ 		spin_unlock(&mm->notifier_subscriptions->lock);
  	} else
- 		mm->mmu_notifier_mm->has_itree = true;
+ 		mm->notifier_subscriptions->has_itree = true;
  
  	mm_drop_all_locks(mm);
  	BUG_ON(atomic_read(&mm->mm_users) <= 0);
  	return 0;
  
  out_clean:
++<<<<<<< HEAD
 +	kfree(mmu_notifier_mm);
 +out_free_rh:
 +	if (mn)
 +		kfree(mn->_rh);
++=======
+ 	kfree(subscriptions);
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  	return ret;
  }
  EXPORT_SYMBOL_GPL(__mmu_notifier_register);
@@@ -696,17 -693,14 +764,17 @@@ find_get_mmu_notifier(struct mm_struct 
  		if (mn->ops != ops)
  			continue;
  
 -		if (likely(mn->users != UINT_MAX))
 -			mn->users++;
 +		if (!RH_KABI_AUX(mn, mmu_notifier, mm))
 +			continue;
 +
 +		if (likely(mn->_rh->users != UINT_MAX))
 +			mn->_rh->users++;
  		else
  			mn = ERR_PTR(-EOVERFLOW);
- 		spin_unlock(&mm->mmu_notifier_mm->lock);
+ 		spin_unlock(&mm->notifier_subscriptions->lock);
  		return mn;
  	}
- 	spin_unlock(&mm->mmu_notifier_mm->lock);
+ 	spin_unlock(&mm->notifier_subscriptions->lock);
  	return NULL;
  }
  
@@@ -733,9 -727,9 +801,9 @@@ struct mmu_notifier *mmu_notifier_get_l
  	struct mmu_notifier *mn;
  	int ret;
  
 -	lockdep_assert_held_write(&mm->mmap_sem);
 +	mmap_assert_write_locked(mm);
  
- 	if (mm->mmu_notifier_mm) {
+ 	if (mm->notifier_subscriptions) {
  		mn = find_get_mmu_notifier(mm, ops);
  		if (mn)
  			return mn;
@@@ -855,9 -818,49 +923,52 @@@ static void mmu_notifier_free_rcu(struc
  	mmdrop(mm);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * mmu_notifier_put - Release the reference on the notifier
+  * @mn: The notifier to act on
+  *
+  * This function must be paired with each mmu_notifier_get(), it releases the
+  * reference obtained by the get. If this is the last reference then process
+  * to free the notifier will be run asynchronously.
+  *
+  * Unlike mmu_notifier_unregister() the get/put flow only calls ops->release
+  * when the mm_struct is destroyed. Instead free_notifier is always called to
+  * release any resources held by the user.
+  *
+  * As ops->release is not guaranteed to be called, the user must ensure that
+  * all sptes are dropped, and no new sptes can be established before
+  * mmu_notifier_put() is called.
+  *
+  * This function can be called from the ops->release callback, however the
+  * caller must still ensure it is called pairwise with mmu_notifier_get().
+  *
+  * Modules calling this function must call mmu_notifier_synchronize() in
+  * their __exit functions to ensure the async work is completed.
+  */
+ void mmu_notifier_put(struct mmu_notifier *mn)
+ {
+ 	struct mm_struct *mm = mn->mm;
+ 
+ 	spin_lock(&mm->notifier_subscriptions->lock);
+ 	if (WARN_ON(!mn->users) || --mn->users)
+ 		goto out_unlock;
+ 	hlist_del_init_rcu(&mn->hlist);
+ 	spin_unlock(&mm->notifier_subscriptions->lock);
+ 
+ 	call_srcu(&srcu, &mn->rcu, mmu_notifier_free_rcu);
+ 	return;
+ 
+ out_unlock:
+ 	spin_unlock(&mm->notifier_subscriptions->lock);
+ }
+ EXPORT_SYMBOL_GPL(mmu_notifier_put);
+ 
++>>>>>>> 984cfe4e2526 (mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions)
  static int __mmu_interval_notifier_insert(
  	struct mmu_interval_notifier *mni, struct mm_struct *mm,
- 	struct mmu_notifier_mm *mmn_mm, unsigned long start,
+ 	struct mmu_notifier_subscriptions *subscriptions, unsigned long start,
  	unsigned long length, const struct mmu_interval_notifier_ops *ops)
  {
  	mni->mm = mm;
@@@ -938,13 -942,13 +1050,13 @@@ int mmu_interval_notifier_insert(struc
  				 unsigned long length,
  				 const struct mmu_interval_notifier_ops *ops)
  {
- 	struct mmu_notifier_mm *mmn_mm;
+ 	struct mmu_notifier_subscriptions *subscriptions;
  	int ret;
  
 -	might_lock(&mm->mmap_sem);
 +	might_lock(&mm->mmap_lock);
  
- 	mmn_mm = smp_load_acquire(&mm->mmu_notifier_mm);
- 	if (!mmn_mm || !mmn_mm->has_itree) {
+ 	subscriptions = smp_load_acquire(&mm->notifier_subscriptions);
+ 	if (!subscriptions || !subscriptions->has_itree) {
  		ret = mmu_notifier_register(NULL, mm);
  		if (ret)
  			return ret;
@@@ -960,13 -964,13 +1072,13 @@@ int mmu_interval_notifier_insert_locked
  	unsigned long start, unsigned long length,
  	const struct mmu_interval_notifier_ops *ops)
  {
- 	struct mmu_notifier_mm *mmn_mm;
+ 	struct mmu_notifier_subscriptions *subscriptions =
+ 		mm->notifier_subscriptions;
  	int ret;
  
 -	lockdep_assert_held_write(&mm->mmap_sem);
 +	mmap_assert_write_locked(mm);
  
- 	mmn_mm = mm->mmu_notifier_mm;
- 	if (!mmn_mm || !mmn_mm->has_itree) {
+ 	if (!subscriptions || !subscriptions->has_itree) {
  		ret = __mmu_notifier_register(NULL, mm);
  		if (ret)
  			return ret;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index ae5de8851137..f5b05ce8bc08 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -513,7 +513,7 @@ struct mm_struct {
 		/* store ref to file /proc/<pid>/exe symlink points to */
 		struct file __rcu *exe_file;
 #ifdef CONFIG_MMU_NOTIFIER
-		struct mmu_notifier_mm *mmu_notifier_mm;
+		struct mmu_notifier_subscriptions *notifier_subscriptions;
 #endif
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 		pgtable_t pmd_huge_pte; /* protected by page_table_lock */
* Unmerged path include/linux/mmu_notifier.h
diff --git a/kernel/fork.c b/kernel/fork.c
index 83c6e84610ad..7c3de3749f90 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -665,7 +665,7 @@ void __mmdrop(struct mm_struct *mm)
 	WARN_ON_ONCE(mm == current->active_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
-	mmu_notifier_mm_destroy(mm);
+	mmu_notifier_subscriptions_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);
 	free_mm(mm);
@@ -1030,7 +1030,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 	RCU_INIT_POINTER(mm->exe_file, NULL);
-	mmu_notifier_mm_init(mm);
+	mmu_notifier_subscriptions_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
diff --git a/mm/debug.c b/mm/debug.c
index bcc867bf095d..e2ed0165810c 100644
--- a/mm/debug.c
+++ b/mm/debug.c
@@ -187,7 +187,7 @@ void dump_mm(const struct mm_struct *mm)
 #endif
 		"exe_file %px\n"
 #ifdef CONFIG_MMU_NOTIFIER
-		"mmu_notifier_mm %px\n"
+		"notifier_subscriptions %px\n"
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 		"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\n"
@@ -219,7 +219,7 @@ void dump_mm(const struct mm_struct *mm)
 #endif
 		mm->exe_file,
 #ifdef CONFIG_MMU_NOTIFIER
-		mm->mmu_notifier_mm,
+		mm->notifier_subscriptions,
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 		mm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,
* Unmerged path mm/mmu_notifier.c
