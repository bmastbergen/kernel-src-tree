perf/x86: Reset the dirty counter to prevent the leak for an RDPMC task

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Kan Liang <kan.liang@linux.intel.com>
commit 5471eea5d3bf850316f1064a6f57b34c444bce67
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/5471eea5.failed

The counter value of a perf task may leak to another RDPMC task.
For example, a perf stat task as below is running on CPU 0.

    perf stat -e 'branches,cycles' -- taskset -c 0 ./workload

In the meantime, an RDPMC task, which is also running on CPU 0, may read
the GP counters periodically. (The RDPMC task creates a fixed event,
but read four GP counters.)

    $./rdpmc_read_all_counters
    index 0x0 value 0x8001e5970f99
    index 0x1 value 0x8005d750edb6
    index 0x2 value 0x0
    index 0x3 value 0x0

    index 0x0 value 0x8002358e48a5
    index 0x1 value 0x8006bd1e3bc9
    index 0x2 value 0x0
    index 0x3 value 0x0

It is a potential security issue. Once the attacker knows what the other
thread is counting. The PerfMon counter can be used as a side-channel to
attack cryptosystems.

The counter value of the perf stat task leaks to the RDPMC task because
perf never clears the counter when it's stopped.

Three methods were considered to address the issue.

 - Unconditionally reset the counter in x86_pmu_del(). It can bring extra
   overhead even when there is no RDPMC task running.

 - Only reset the un-assigned dirty counters when the RDPMC task is
   scheduled in via sched_task(). It fails for the below case.

	Thread A			Thread B

	clone(CLONE_THREAD) --->
	set_affine(0)
					set_affine(1)
					while (!event-enabled)
						;
	event = perf_event_open()
	mmap(event)
	ioctl(event, IOC_ENABLE); --->
					RDPMC

   Counters are still leaked to the thread B.

 - Only reset the un-assigned dirty counters before updating the CR4.PCE
   bit. The method is implemented here.

The dirty counter is a counter, on which the assigned event has been
deleted, but the counter is not reset. To track the dirty counters,
add a 'dirty' variable in the struct cpu_hw_events.

The security issue can only be found with an RDPMC task. To enable the
RDMPC, the CR4.PCE bit has to be updated. Add a
perf_clear_dirty_counters() right before updating the CR4.PCE bit to
clear the existing dirty counters. Only the current un-assigned dirty
counters are reset, because the RDPMC assigned dirty counters will be
updated soon.

After applying the patch,

        $ ./rdpmc_read_all_counters
        index 0x0 value 0x0
        index 0x1 value 0x0
        index 0x2 value 0x0
        index 0x3 value 0x0

        index 0x0 value 0x0
        index 0x1 value 0x0
        index 0x2 value 0x0
        index 0x3 value 0x0

Performance

The performance of a context switch only be impacted when there are two
or more perf users and one of the users must be an RDPMC user. In other
cases, there is no performance impact.

The worst-case occurs when there are two users: the RDPMC user only
uses one counter; while the other user uses all available counters.
When the RDPMC task is scheduled in, all the counters, other than the
RDPMC assigned one, have to be reset.

Test results for the worst-case, using a modified lat_ctx as measured
on an Ice Lake platform, which has 8 GP and 3 FP counters (ignoring
SLOTS).

    lat_ctx -s 128K -N 1000 processes 2

Without the patch:
  The context switch time is 4.97 us

With the patch:
  The context switch time is 5.16 us

There is ~4% performance drop for the context switching time in the
worst-case.

	Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/1623693582-187370-1-git-send-email-kan.liang@linux.intel.com
(cherry picked from commit 5471eea5d3bf850316f1064a6f57b34c444bce67)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/core.c
#	arch/x86/events/perf_event.h
#	arch/x86/mm/tlb.c
diff --cc arch/x86/events/core.c
index e5de91e5b10c,c0167d52832e..000000000000
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@@ -2394,9 -2474,29 +2396,35 @@@ static int x86_pmu_event_init(struct pe
  	return err;
  }
  
++<<<<<<< HEAD
 +static void refresh_pce(void *ignored)
 +{
 +	load_mm_cr4(this_cpu_read(cpu_tlbstate.loaded_mm));
++=======
+ void perf_clear_dirty_counters(void)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 	int i;
+ 
+ 	 /* Don't need to clear the assigned counter. */
+ 	for (i = 0; i < cpuc->n_events; i++)
+ 		__clear_bit(cpuc->assign[i], cpuc->dirty);
+ 
+ 	if (bitmap_empty(cpuc->dirty, X86_PMC_IDX_MAX))
+ 		return;
+ 
+ 	for_each_set_bit(i, cpuc->dirty, X86_PMC_IDX_MAX) {
+ 		/* Metrics and fake events don't have corresponding HW counters. */
+ 		if (is_metric_idx(i) || (i == INTEL_PMC_IDX_FIXED_VLBR))
+ 			continue;
+ 		else if (i >= INTEL_PMC_IDX_FIXED)
+ 			wrmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + (i - INTEL_PMC_IDX_FIXED), 0);
+ 		else
+ 			wrmsrl(x86_pmu_event_addr(i), 0);
+ 	}
+ 
+ 	bitmap_zero(cpuc->dirty, X86_PMC_IDX_MAX);
++>>>>>>> 5471eea5d3bf (perf/x86: Reset the dirty counter to prevent the leak for an RDPMC task)
  }
  
  static void x86_pmu_event_mapped(struct perf_event *event, struct mm_struct *mm)
diff --cc arch/x86/events/perf_event.h
index 23e9b1e50646,d6003e08b055..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -229,7 -229,7 +229,11 @@@ struct cpu_hw_events 
  	 */
  	struct perf_event	*events[X86_PMC_IDX_MAX]; /* in counter order */
  	unsigned long		active_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
++<<<<<<< HEAD
 +	unsigned long		running[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
++=======
+ 	unsigned long		dirty[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
++>>>>>>> 5471eea5d3bf (perf/x86: Reset the dirty counter to prevent the leak for an RDPMC task)
  	int			enabled;
  
  	int			n_events; /* the # of events in the below arrays */
diff --cc arch/x86/mm/tlb.c
index b2b69dfa6807,cfe6b1e85fa6..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -270,6 -400,31 +271,34 @@@ static void cond_ibpb(struct task_struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PERF_EVENTS
+ static inline void cr4_update_pce_mm(struct mm_struct *mm)
+ {
+ 	if (static_branch_unlikely(&rdpmc_always_available_key) ||
+ 	    (!static_branch_unlikely(&rdpmc_never_available_key) &&
+ 	     atomic_read(&mm->context.perf_rdpmc_allowed))) {
+ 		/*
+ 		 * Clear the existing dirty counters to
+ 		 * prevent the leak for an RDPMC task.
+ 		 */
+ 		perf_clear_dirty_counters();
+ 		cr4_set_bits_irqsoff(X86_CR4_PCE);
+ 	} else
+ 		cr4_clear_bits_irqsoff(X86_CR4_PCE);
+ }
+ 
+ void cr4_update_pce(void *ignored)
+ {
+ 	cr4_update_pce_mm(this_cpu_read(cpu_tlbstate.loaded_mm));
+ }
+ 
+ #else
+ static inline void cr4_update_pce_mm(struct mm_struct *mm) { }
+ #endif
+ 
++>>>>>>> 5471eea5d3bf (perf/x86: Reset the dirty counter to prevent the leak for an RDPMC task)
  void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
  			struct task_struct *tsk)
  {
* Unmerged path arch/x86/events/core.c
* Unmerged path arch/x86/events/perf_event.h
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index d3530eac30bc..4a0e00661c00 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -478,6 +478,7 @@ struct x86_pmu_lbr {
 
 extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
 extern void perf_check_microcode(void);
+extern void perf_clear_dirty_counters(void);
 extern int x86_perf_rdpmc_index(struct perf_event *event);
 #else
 static inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
* Unmerged path arch/x86/mm/tlb.c
