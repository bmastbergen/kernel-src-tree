KVM: x86/mmu: Make walk_shadow_page_lockless_{begin,end} interoperate with the TDP MMU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author David Matlack <dmatlack@google.com>
commit c5c8c7c53004cb70715320018c3b4287071c1cfd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/c5c8c7c5.failed

Acquire the RCU read lock in walk_shadow_page_lockless_begin and release
it in walk_shadow_page_lockless_end when the TDP MMU is enabled.  This
should not introduce any functional changes but is used in the following
commit to make fast_page_fault interoperate with the TDP MMU.

	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20210713220957.3493520-4-dmatlack@google.com>
[Use if...else instead of if(){return;}]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c5c8c7c53004cb70715320018c3b4287071c1cfd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 523dd1d34480,d5b0c8b0e9e9..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3668,12 -3658,9 +3674,18 @@@ static bool get_mmio_spte(struct kvm_vc
  	int root, leaf, level;
  	bool reserved = false;
  
++<<<<<<< HEAD
 +	if (!VALID_PAGE(vcpu->arch.mmu->root_hpa)) {
 +		*sptep = 0ull;
 +		return reserved;
 +	}
 +
 +	if (is_tdp_mmu_root(vcpu->arch.mmu->root_hpa))
++=======
+ 	walk_shadow_page_lockless_begin(vcpu);
+ 
+ 	if (is_tdp_mmu(vcpu->arch.mmu))
++>>>>>>> c5c8c7c53004 (KVM: x86/mmu: Make walk_shadow_page_lockless_{begin,end} interoperate with the TDP MMU)
  		leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
  	else
  		leaf = get_walk(vcpu, addr, sptes, &root);
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 66e1adaafe79..f6780c644efa 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -1395,6 +1395,8 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 /*
  * Return the level of the lowest level SPTE added to sptes.
  * That SPTE may be non-present.
+ *
+ * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
@@ -1406,14 +1408,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	*root_level = vcpu->arch.mmu->shadow_root_level;
 
-	rcu_read_lock();
-
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
 		sptes[leaf] = iter.old_spte;
 	}
 
-	rcu_read_unlock();
-
 	return leaf;
 }
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 65fc3c496cb8..a739a44f4089 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -71,6 +71,16 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
 
+static inline void kvm_tdp_mmu_walk_lockless_begin(void)
+{
+	rcu_read_lock();
+}
+
+static inline void kvm_tdp_mmu_walk_lockless_end(void)
+{
+	rcu_read_unlock();
+}
+
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level);
 
