bpf: Compute program stats for sleepable programs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Alexei Starovoitov <ast@kernel.org>
commit f2dd3b39467411c53703125a111f45b3672c1771
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/f2dd3b39.failed

Since sleepable programs don't migrate from the cpu the excution stats can be
computed for them as well. Reuse the same infrastructure for both sleepable and
non-sleepable programs.

run_cnt     -> the number of times the program was executed.
run_time_ns -> the program execution time in nanoseconds including the
               off-cpu time when the program was sleeping.

	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Andrii Nakryiko <andrii@kernel.org>
	Acked-by: KP Singh <kpsingh@kernel.org>
Link: https://lore.kernel.org/bpf/20210210033634.62081-4-alexei.starovoitov@gmail.com
(cherry picked from commit f2dd3b39467411c53703125a111f45b3672c1771)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/trampoline.c
diff --cc include/linux/bpf.h
index da13e0f46e30,e1840ceaf55f..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -585,10 -563,8 +585,15 @@@ int arch_prepare_bpf_trampoline(struct 
  /* these two functions are called from generated trampoline */
  u64 notrace __bpf_prog_enter(void);
  void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start);
++<<<<<<< HEAD
 +void notrace __bpf_prog_enter_sleepable(void);
 +void notrace __bpf_prog_exit_sleepable(void);
 +void notrace __bpf_tramp_enter(struct bpf_tramp_image *tr);
 +void notrace __bpf_tramp_exit(struct bpf_tramp_image *tr);
++=======
+ u64 notrace __bpf_prog_enter_sleepable(void);
+ void notrace __bpf_prog_exit_sleepable(struct bpf_prog *prog, u64 start);
++>>>>>>> f2dd3b394674 (bpf: Compute program stats for sleepable programs)
  
  struct bpf_ksym {
  	unsigned long		 start;
diff --cc kernel/bpf/trampoline.c
index e4f61015e2e7,48eb021e1421..000000000000
--- a/kernel/bpf/trampoline.c
+++ b/kernel/bpf/trampoline.c
@@@ -514,13 -412,13 +520,18 @@@ static void notrace update_prog_stats(s
  	struct bpf_prog_stats *stats;
  
  	if (static_branch_unlikely(&bpf_stats_enabled_key) &&
- 	    /* static_key could be enabled in __bpf_prog_enter
- 	     * and disabled in __bpf_prog_exit.
+ 	    /* static_key could be enabled in __bpf_prog_enter*
+ 	     * and disabled in __bpf_prog_exit*.
  	     * And vice versa.
- 	     * Hence check that 'start' is not zero.
+ 	     * Hence check that 'start' is valid.
  	     */
++<<<<<<< HEAD
 +	    start) {
 +		stats = this_cpu_ptr(prog->aux->stats);
++=======
+ 	    start > NO_START_TIME) {
+ 		stats = this_cpu_ptr(prog->stats);
++>>>>>>> f2dd3b394674 (bpf: Compute program stats for sleepable programs)
  		u64_stats_update_begin(&stats->syncp);
  		stats->cnt++;
  		stats->nsecs += sched_clock() - start;
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index e25326168f5b..15a2ba002a42 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -1760,15 +1760,12 @@ static int invoke_bpf_prog(const struct btf_func_model *m, u8 **pprog,
 	u8 *prog = *pprog;
 	int cnt = 0;
 
-	if (p->aux->sleepable) {
-		if (emit_call(&prog, __bpf_prog_enter_sleepable, prog))
+	if (emit_call(&prog,
+		      p->aux->sleepable ? __bpf_prog_enter_sleepable :
+		      __bpf_prog_enter, prog))
 			return -EINVAL;
-	} else {
-		if (emit_call(&prog, __bpf_prog_enter, prog))
-			return -EINVAL;
-		/* remember prog start time returned by __bpf_prog_enter */
-		emit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);
-	}
+	/* remember prog start time returned by __bpf_prog_enter */
+	emit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);
 
 	/* arg1: lea rdi, [rbp - stack_size] */
 	EMIT4(0x48, 0x8D, 0x7D, -stack_size);
@@ -1788,18 +1785,14 @@ static int invoke_bpf_prog(const struct btf_func_model *m, u8 **pprog,
 	if (mod_ret)
 		emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
 
-	if (p->aux->sleepable) {
-		if (emit_call(&prog, __bpf_prog_exit_sleepable, prog))
+	/* arg1: mov rdi, progs[i] */
+	emit_mov_imm64(&prog, BPF_REG_1, (long) p >> 32, (u32) (long) p);
+	/* arg2: mov rsi, rbx <- start time in nsec */
+	emit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);
+	if (emit_call(&prog,
+		      p->aux->sleepable ? __bpf_prog_exit_sleepable :
+		      __bpf_prog_exit, prog))
 			return -EINVAL;
-	} else {
-		/* arg1: mov rdi, progs[i] */
-		emit_mov_imm64(&prog, BPF_REG_1, (long) p >> 32,
-			       (u32) (long) p);
-		/* arg2: mov rsi, rbx <- start time in nsec */
-		emit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);
-		if (emit_call(&prog, __bpf_prog_exit, prog))
-			return -EINVAL;
-	}
 
 	*pprog = prog;
 	return 0;
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/trampoline.c
