block: add sysfs entry for virt boundary mask

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Max Gurtovoy <mgurtovoy@nvidia.com>
commit 28af742875d7d2d8ae008701c60cd4b238f3e2b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/28af7428.failed

This entry will expose the bio vector alignment mask for a specific
block device.

	Signed-off-by: Max Gurtovoy <mgurtovoy@nvidia.com>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
Link: https://lore.kernel.org/r/20210405132012.12504-1-mgurtovoy@nvidia.com
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 28af742875d7d2d8ae008701c60cd4b238f3e2b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-sysfs.c
diff --cc block/blk-sysfs.c
index 9d08ebc44db2,e03bedf180ab..000000000000
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@@ -261,31 -264,9 +261,37 @@@ static ssize_t queue_max_hw_sectors_sho
  	return queue_var_show(max_hw_sectors_kb, (page));
  }
  
++<<<<<<< HEAD
 +static ssize_t
 +queue_show_unpriv_sgio(struct request_queue *q, char *page)
 +{
 +	int bit;
 +	bit = test_bit(QUEUE_FLAG_UNPRIV_SGIO, &q->queue_flags);
 +	return queue_var_show(bit, page);
 +}
 +static ssize_t
 +queue_store_unpriv_sgio(struct request_queue *q, const char *page, size_t count)
 +{
 +	unsigned long val;
 +	ssize_t ret;
 +
 +	if (!capable(CAP_SYS_ADMIN))
 +		return -EPERM;
 +
 +	ret = queue_var_store(&val, page, count);
 +	if (ret < 0)
 +		return ret;
 +
 +	if (val)
 +		blk_queue_flag_set(QUEUE_FLAG_UNPRIV_SGIO, q);
 +	else
 +		blk_queue_flag_clear(QUEUE_FLAG_UNPRIV_SGIO, q);
 +	return ret;
++=======
+ static ssize_t queue_virt_boundary_mask_show(struct request_queue *q, char *page)
+ {
+ 	return queue_var_show(q->limits.virt_boundary_mask, (page));
++>>>>>>> 28af742875d7 (block: add sysfs entry for virt boundary mask)
  }
  
  #define QUEUE_SYSFS_BIT_FNS(name, flag, neg)				\
@@@ -570,55 -562,66 +576,68 @@@ static ssize_t queue_dax_show(struct re
  	return queue_var_show(blk_queue_dax(q), page);
  }
  
 -#define QUEUE_RO_ENTRY(_prefix, _name)			\
 -static struct queue_sysfs_entry _prefix##_entry = {	\
 -	.attr	= { .name = _name, .mode = 0444 },	\
 -	.show	= _prefix##_show,			\
 +static struct queue_sysfs_entry queue_requests_entry = {
 +	.attr = {.name = "nr_requests", .mode = 0644 },
 +	.show = queue_requests_show,
 +	.store = queue_requests_store,
  };
  
 -#define QUEUE_RW_ENTRY(_prefix, _name)			\
 -static struct queue_sysfs_entry _prefix##_entry = {	\
 -	.attr	= { .name = _name, .mode = 0644 },	\
 -	.show	= _prefix##_show,			\
 -	.store	= _prefix##_store,			\
 +static struct queue_sysfs_entry queue_ra_entry = {
 +	.attr = {.name = "read_ahead_kb", .mode = 0644 },
 +	.show = queue_ra_show,
 +	.store = queue_ra_store,
  };
  
 -QUEUE_RW_ENTRY(queue_requests, "nr_requests");
 -QUEUE_RW_ENTRY(queue_ra, "read_ahead_kb");
 -QUEUE_RW_ENTRY(queue_max_sectors, "max_sectors_kb");
 -QUEUE_RO_ENTRY(queue_max_hw_sectors, "max_hw_sectors_kb");
 -QUEUE_RO_ENTRY(queue_max_segments, "max_segments");
 -QUEUE_RO_ENTRY(queue_max_integrity_segments, "max_integrity_segments");
 -QUEUE_RO_ENTRY(queue_max_segment_size, "max_segment_size");
 -QUEUE_RW_ENTRY(elv_iosched, "scheduler");
 +static struct queue_sysfs_entry queue_max_sectors_entry = {
 +	.attr = {.name = "max_sectors_kb", .mode = 0644 },
 +	.show = queue_max_sectors_show,
 +	.store = queue_max_sectors_store,
 +};
  
 -QUEUE_RO_ENTRY(queue_logical_block_size, "logical_block_size");
 -QUEUE_RO_ENTRY(queue_physical_block_size, "physical_block_size");
 -QUEUE_RO_ENTRY(queue_chunk_sectors, "chunk_sectors");
 -QUEUE_RO_ENTRY(queue_io_min, "minimum_io_size");
 -QUEUE_RO_ENTRY(queue_io_opt, "optimal_io_size");
 +static struct queue_sysfs_entry queue_max_hw_sectors_entry = {
 +	.attr = {.name = "max_hw_sectors_kb", .mode = 0444 },
 +	.show = queue_max_hw_sectors_show,
 +};
  
 -QUEUE_RO_ENTRY(queue_max_discard_segments, "max_discard_segments");
 -QUEUE_RO_ENTRY(queue_discard_granularity, "discard_granularity");
 -QUEUE_RO_ENTRY(queue_discard_max_hw, "discard_max_hw_bytes");
 -QUEUE_RW_ENTRY(queue_discard_max, "discard_max_bytes");
 -QUEUE_RO_ENTRY(queue_discard_zeroes_data, "discard_zeroes_data");
 +static struct queue_sysfs_entry queue_max_segments_entry = {
 +	.attr = {.name = "max_segments", .mode = 0444 },
 +	.show = queue_max_segments_show,
 +};
  
 -QUEUE_RO_ENTRY(queue_write_same_max, "write_same_max_bytes");
 -QUEUE_RO_ENTRY(queue_write_zeroes_max, "write_zeroes_max_bytes");
 -QUEUE_RO_ENTRY(queue_zone_append_max, "zone_append_max_bytes");
 -QUEUE_RO_ENTRY(queue_zone_write_granularity, "zone_write_granularity");
 +static struct queue_sysfs_entry queue_max_discard_segments_entry = {
 +	.attr = {.name = "max_discard_segments", .mode = 0444 },
 +	.show = queue_max_discard_segments_show,
 +};
  
 -QUEUE_RO_ENTRY(queue_zoned, "zoned");
 -QUEUE_RO_ENTRY(queue_nr_zones, "nr_zones");
 -QUEUE_RO_ENTRY(queue_max_open_zones, "max_open_zones");
 -QUEUE_RO_ENTRY(queue_max_active_zones, "max_active_zones");
 +static struct queue_sysfs_entry queue_max_integrity_segments_entry = {
 +	.attr = {.name = "max_integrity_segments", .mode = 0444 },
 +	.show = queue_max_integrity_segments_show,
 +};
  
++<<<<<<< HEAD
 +static struct queue_sysfs_entry queue_max_segment_size_entry = {
 +	.attr = {.name = "max_segment_size", .mode = 0444 },
 +	.show = queue_max_segment_size_show,
 +};
++=======
+ QUEUE_RW_ENTRY(queue_nomerges, "nomerges");
+ QUEUE_RW_ENTRY(queue_rq_affinity, "rq_affinity");
+ QUEUE_RW_ENTRY(queue_poll, "io_poll");
+ QUEUE_RW_ENTRY(queue_poll_delay, "io_poll_delay");
+ QUEUE_RW_ENTRY(queue_wc, "write_cache");
+ QUEUE_RO_ENTRY(queue_fua, "fua");
+ QUEUE_RO_ENTRY(queue_dax, "dax");
+ QUEUE_RW_ENTRY(queue_io_timeout, "io_timeout");
+ QUEUE_RW_ENTRY(queue_wb_lat, "wbt_lat_usec");
+ QUEUE_RO_ENTRY(queue_virt_boundary_mask, "virt_boundary_mask");
++>>>>>>> 28af742875d7 (block: add sysfs entry for virt boundary mask)
  
 -#ifdef CONFIG_BLK_DEV_THROTTLING_LOW
 -QUEUE_RW_ENTRY(blk_throtl_sample_time, "throttle_sample_time");
 -#endif
 +static struct queue_sysfs_entry queue_iosched_entry = {
 +	.attr = {.name = "scheduler", .mode = 0644 },
 +	.show = elv_iosched_show,
 +	.store = elv_iosched_store,
 +};
  
 -/* legacy alias for logical_block_size: */
  static struct queue_sysfs_entry queue_hw_sector_size_entry = {
  	.attr = {.name = "hw_sector_size", .mode = 0444 },
  	.show = queue_logical_block_size_show,
@@@ -818,8 -674,9 +837,9 @@@ static struct attribute *default_attrs[
  	&queue_poll_delay_entry.attr,
  	&queue_io_timeout_entry.attr,
  #ifdef CONFIG_BLK_DEV_THROTTLING_LOW
 -	&blk_throtl_sample_time_entry.attr,
 +	&throtl_sample_time_entry.attr,
  #endif
+ 	&queue_virt_boundary_mask_entry.attr,
  	NULL,
  };
  
* Unmerged path block/blk-sysfs.c
