RDMA: Use the sg_table directly and remove the opencoded version from umem

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Maor Gottlieb <maorg@nvidia.com>
commit 79fbd3e1241cea83dded06db2b8bcd5893d877d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/79fbd3e1.failed

This allows using the normal sg_table APIs and makes all the code
cleaner. Remove sgt, nents and nmapd from ib_umem.

Link: https://lore.kernel.org/r/20210824142531.3877007-4-maorg@nvidia.com
	Signed-off-by: Maor Gottlieb <maorg@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 79fbd3e1241cea83dded06db2b8bcd5893d877d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
#	drivers/infiniband/core/umem_dmabuf.c
#	drivers/infiniband/hw/irdma/verbs.c
#	include/rdma/ib_umem.h
diff --cc drivers/infiniband/core/umem.c
index 7684c75b36be,86d479772fbc..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -229,19 -220,19 +229,30 @@@ struct ib_umem *ib_umem_get(struct ib_u
  						PAGE_SIZE /
  						sizeof(struct page *)),
  					  gup_flags | FOLL_LONGTERM, page_list);
 -		if (pinned < 0) {
 -			ret = pinned;
 +		if (ret < 0)
  			goto umem_release;
 -		}
  
++<<<<<<< HEAD
 +		cur_base += ret * PAGE_SIZE;
 +		npages -= ret;
 +		sg = __sg_alloc_table_from_pages(&umem->sg_head, page_list, ret,
 +				0, ret << PAGE_SHIFT,
 +				ib_dma_max_seg_size(context->device), sg, npages,
 +				GFP_KERNEL);
 +		umem->sg_nents = umem->sg_head.nents;
 +		if (IS_ERR(sg)) {
 +			unpin_user_pages_dirty_lock(page_list, ret, 0);
 +			ret = PTR_ERR(sg);
++=======
+ 		cur_base += pinned * PAGE_SIZE;
+ 		npages -= pinned;
+ 		ret = sg_alloc_append_table_from_pages(
+ 			&umem->sgt_append, page_list, pinned, 0,
+ 			pinned << PAGE_SHIFT, ib_dma_max_seg_size(device),
+ 			npages, GFP_KERNEL);
+ 		if (ret) {
+ 			unpin_user_pages_dirty_lock(page_list, pinned, 0);
++>>>>>>> 79fbd3e1241c (RDMA: Use the sg_table directly and remove the opencoded version from umem)
  			goto umem_release;
  		}
  	}
@@@ -249,17 -240,10 +260,20 @@@
  	if (access & IB_ACCESS_RELAXED_ORDERING)
  		dma_attr |= DMA_ATTR_WEAK_ORDERING;
  
++<<<<<<< HEAD
 +	umem->nmap =
 +		ib_dma_map_sg_attrs(context->device,
 +				    umem->sg_head.sgl, umem->sg_nents,
 +				    DMA_BIDIRECTIONAL, dma_attr);
 +
 +	if (!umem->nmap) {
 +		ret = -ENOMEM;
++=======
+ 	ret = ib_dma_map_sgtable_attrs(device, &umem->sgt_append.sgt,
+ 				       DMA_BIDIRECTIONAL, dma_attr);
+ 	if (ret)
++>>>>>>> 79fbd3e1241c (RDMA: Use the sg_table directly and remove the opencoded version from umem)
  		goto umem_release;
- 	}
- 
- 	ret = 0;
  	goto out;
  
  umem_release:
diff --cc include/rdma/ib_umem.h
index c6e799d5112d,5ae9dff74dac..000000000000
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@@ -22,12 -24,27 +22,16 @@@ struct ib_umem 
  	unsigned long		address;
  	u32 writable : 1;
  	u32 is_odp : 1;
 -	u32 is_dmabuf : 1;
  	struct work_struct	work;
++<<<<<<< HEAD
 +	struct sg_table sg_head;
 +	int             nmap;
 +	unsigned int    sg_nents;
++=======
+ 	struct sg_append_table sgt_append;
++>>>>>>> 79fbd3e1241c (RDMA: Use the sg_table directly and remove the opencoded version from umem)
  };
  
 -struct ib_umem_dmabuf {
 -	struct ib_umem umem;
 -	struct dma_buf_attachment *attach;
 -	struct sg_table *sgt;
 -	struct scatterlist *first_sg;
 -	struct scatterlist *last_sg;
 -	unsigned long first_sg_offset;
 -	unsigned long last_sg_trim;
 -	void *private;
 -};
 -
 -static inline struct ib_umem_dmabuf *to_ib_umem_dmabuf(struct ib_umem *umem)
 -{
 -	return container_of(umem, struct ib_umem_dmabuf, umem);
 -}
 -
  /* Returns the offset of the umem start relative to the first page. */
  static inline int ib_umem_offset(struct ib_umem *umem)
  {
* Unmerged path drivers/infiniband/core/umem_dmabuf.c
* Unmerged path drivers/infiniband/hw/irdma/verbs.c
* Unmerged path drivers/infiniband/core/umem.c
* Unmerged path drivers/infiniband/core/umem_dmabuf.c
diff --git a/drivers/infiniband/hw/hns/hns_roce_db.c b/drivers/infiniband/hw/hns/hns_roce_db.c
index a668a10c9733..d62a85b85442 100644
--- a/drivers/infiniband/hw/hns/hns_roce_db.c
+++ b/drivers/infiniband/hw/hns/hns_roce_db.c
@@ -42,8 +42,8 @@ int hns_roce_db_map_user(struct hns_roce_ucontext *context,
 
 found:
 	offset = virt - page_addr;
-	db->dma = sg_dma_address(page->umem->sg_head.sgl) + offset;
-	db->virt_addr = sg_virt(page->umem->sg_head.sgl) + offset;
+	db->dma = sg_dma_address(page->umem->sgt_append.sgt.sgl) + offset;
+	db->virt_addr = sg_virt(page->umem->sgt_append.sgt.sgl) + offset;
 	db->u.user_page = page;
 	refcount_inc(&page->refcount);
 
* Unmerged path drivers/infiniband/hw/irdma/verbs.c
diff --git a/drivers/infiniband/hw/mlx4/doorbell.c b/drivers/infiniband/hw/mlx4/doorbell.c
index 714f9df5bf39..c613f98f8f41 100644
--- a/drivers/infiniband/hw/mlx4/doorbell.c
+++ b/drivers/infiniband/hw/mlx4/doorbell.c
@@ -74,7 +74,8 @@ int mlx4_ib_db_map_user(struct ib_udata *udata, unsigned long virt,
 	list_add(&page->list, &context->db_page_list);
 
 found:
-	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
+	db->dma = sg_dma_address(page->umem->sgt_append.sgt.sgl) +
+		  (virt & ~PAGE_MASK);
 	db->u.user_page = page;
 	++page->refcnt;
 
diff --git a/drivers/infiniband/hw/mlx4/mr.c b/drivers/infiniband/hw/mlx4/mr.c
index 0b856eb58bc5..032633e71ede 100644
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -200,7 +200,7 @@ int mlx4_ib_umem_write_mtt(struct mlx4_ib_dev *dev, struct mlx4_mtt *mtt,
 	mtt_shift = mtt->page_shift;
 	mtt_size = 1ULL << mtt_shift;
 
-	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, i) {
+	for_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i) {
 		if (cur_start_addr + len == sg_dma_address(sg)) {
 			/* still the same block */
 			len += sg_dma_len(sg);
@@ -273,7 +273,7 @@ int mlx4_ib_umem_calc_optimal_mtt_size(struct ib_umem *umem, u64 start_va,
 
 	*num_of_mtts = ib_umem_num_dma_blocks(umem, PAGE_SIZE);
 
-	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, i) {
+	for_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i) {
 		/*
 		 * Initialization - save the first chunk start as the
 		 * current_block_start - block means contiguous pages.
diff --git a/drivers/infiniband/hw/mlx5/doorbell.c b/drivers/infiniband/hw/mlx5/doorbell.c
index c66657dc09fd..ca9b78a8fe9b 100644
--- a/drivers/infiniband/hw/mlx5/doorbell.c
+++ b/drivers/infiniband/hw/mlx5/doorbell.c
@@ -79,7 +79,8 @@ int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context,
 	list_add(&page->list, &context->db_page_list);
 
 found:
-	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
+	db->dma = sg_dma_address(page->umem->sgt_append.sgt.sgl) +
+		  (virt & ~PAGE_MASK);
 	db->u.user_page = page;
 	++page->refcnt;
 
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 9ac99d255a3c..982faec0b752 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -1236,7 +1236,8 @@ static int mlx5_ib_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags)
 	orig_sg_length = sg.length;
 
 	cur_mtt = mtt;
-	rdma_for_each_block (mr->umem->sg_head.sgl, &biter, mr->umem->nmap,
+	rdma_for_each_block (mr->umem->sgt_append.sgt.sgl, &biter,
+			     mr->umem->sgt_append.sgt.nents,
 			     BIT(mr->page_shift)) {
 		if (cur_mtt == (void *)mtt + sg.length) {
 			dma_sync_single_for_device(ddev, sg.addr, sg.length,
diff --git a/drivers/infiniband/hw/qedr/verbs.c b/drivers/infiniband/hw/qedr/verbs.c
index 2bb9185e0af3..057e1d5177af 100644
--- a/drivers/infiniband/hw/qedr/verbs.c
+++ b/drivers/infiniband/hw/qedr/verbs.c
@@ -1481,7 +1481,7 @@ static int qedr_init_srq_user_params(struct ib_udata *udata,
 		return PTR_ERR(srq->prod_umem);
 	}
 
-	sg = srq->prod_umem->sg_head.sgl;
+	sg = srq->prod_umem->sgt_append.sgt.sgl;
 	srq->hw_srq.phy_prod_pair_addr = sg_dma_address(sg);
 
 	return 0;
diff --git a/drivers/infiniband/sw/rdmavt/mr.c b/drivers/infiniband/sw/rdmavt/mr.c
index 57ad975dfc73..961854d67bef 100644
--- a/drivers/infiniband/sw/rdmavt/mr.c
+++ b/drivers/infiniband/sw/rdmavt/mr.c
@@ -410,7 +410,7 @@ struct ib_mr *rvt_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 	mr->mr.page_shift = PAGE_SHIFT;
 	m = 0;
 	n = 0;
-	for_each_sg_page (umem->sg_head.sgl, &sg_iter, umem->nmap, 0) {
+	for_each_sgtable_page (&umem->sgt_append.sgt, &sg_iter, 0) {
 		void *vaddr;
 
 		vaddr = page_address(sg_page_iter_page(&sg_iter));
diff --git a/drivers/infiniband/sw/rxe/rxe_mr.c b/drivers/infiniband/sw/rxe/rxe_mr.c
index 93e0debbc309..0e351a6fd8f2 100644
--- a/drivers/infiniband/sw/rxe/rxe_mr.c
+++ b/drivers/infiniband/sw/rxe/rxe_mr.c
@@ -158,7 +158,7 @@ int rxe_mr_init_user(struct rxe_pd *pd, u64 start, u64 length, u64 iova,
 	if (length > 0) {
 		buf = map[0]->buf;
 
-		for_each_sg_page(umem->sg_head.sgl, &sg_iter, umem->nmap, 0) {
+		for_each_sgtable_page (&umem->sgt_append.sgt, &sg_iter, 0) {
 			if (num_buf >= RXE_BUF_PER_MAP) {
 				map++;
 				buf = map[0]->buf;
* Unmerged path include/rdma/ib_umem.h
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bfcbec0959ce..a87ad7735b09 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4045,6 +4045,34 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 				   dma_attrs);
 }
 
+/**
+ * ib_dma_map_sgtable_attrs - Map a scatter/gather table to DMA addresses
+ * @dev: The device for which the DMA addresses are to be created
+ * @sg: The sg_table object describing the buffer
+ * @direction: The direction of the DMA
+ * @attrs: Optional DMA attributes for the map operation
+ */
+static inline int ib_dma_map_sgtable_attrs(struct ib_device *dev,
+					   struct sg_table *sgt,
+					   enum dma_data_direction direction,
+					   unsigned long dma_attrs)
+{
+	if (ib_uses_virt_dma(dev)) {
+		ib_dma_virt_map_sg(dev, sgt->sgl, sgt->orig_nents);
+		return 0;
+	}
+	return dma_map_sgtable(dev->dma_device, sgt, direction, dma_attrs);
+}
+
+static inline void ib_dma_unmap_sgtable_attrs(struct ib_device *dev,
+					      struct sg_table *sgt,
+					      enum dma_data_direction direction,
+					      unsigned long dma_attrs)
+{
+	if (!ib_uses_virt_dma(dev))
+		dma_unmap_sgtable(dev->dma_device, sgt, direction, dma_attrs);
+}
+
 /**
  * ib_dma_map_sg - Map a scatter/gather list to DMA addresses
  * @dev: The device for which the DMA addresses are to be created
