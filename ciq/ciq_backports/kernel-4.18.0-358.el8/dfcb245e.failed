sched: Fix various typos in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-358.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit dfcb245e28481256a10a9133441baf2a93d26642
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-358.el8/dfcb245e.failed

Go over the scheduler source code and fix common typos
in comments - and a typo in an actual variable name.

No change in functionality intended.

	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit dfcb245e28481256a10a9133441baf2a93d26642)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched/mm.h
#	kernel/sched/cputime.c
#	kernel/sched/fair.c
diff --cc include/linux/sched/mm.h
index 96e0862580ce,3bfa6a0cbba4..000000000000
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@@ -179,21 -151,14 +179,32 @@@ static inline bool in_vfork(struct task
   */
  static inline gfp_t current_gfp_context(gfp_t flags)
  {
++<<<<<<< HEAD
 +	if (unlikely(current->flags &
 +		     (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_NOCMA))) {
 +		/*
 +		 * NOIO implies both NOIO and NOFS and it is a weaker context
 +		 * so always make sure it makes precedence
 +		 */
 +		if (current->flags & PF_MEMALLOC_NOIO)
 +			flags &= ~(__GFP_IO | __GFP_FS);
 +		else if (current->flags & PF_MEMALLOC_NOFS)
 +			flags &= ~__GFP_FS;
 +#ifdef CONFIG_CMA
 +		if (current->flags & PF_MEMALLOC_NOCMA)
 +			flags &= ~__GFP_MOVABLE;
 +#endif
 +	}
++=======
+ 	/*
+ 	 * NOIO implies both NOIO and NOFS and it is a weaker context
+ 	 * so always make sure it makes precedence
+ 	 */
+ 	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
+ 		flags &= ~(__GFP_IO | __GFP_FS);
+ 	else if (unlikely(current->flags & PF_MEMALLOC_NOFS))
+ 		flags &= ~__GFP_FS;
++>>>>>>> dfcb245e2848 (sched: Fix various typos in comments)
  	return flags;
  }
  
diff --cc kernel/sched/cputime.c
index 8b9b9afab4c3,ba4a143bdcf3..000000000000
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@@ -524,6 -524,50 +524,53 @@@ void account_idle_ticks(unsigned long t
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Perform (stime * rtime) / total, but avoid multiplication overflow by
+  * losing precision when the numbers are big.
+  */
+ static u64 scale_stime(u64 stime, u64 rtime, u64 total)
+ {
+ 	u64 scaled;
+ 
+ 	for (;;) {
+ 		/* Make sure "rtime" is the bigger of stime/rtime */
+ 		if (stime > rtime)
+ 			swap(rtime, stime);
+ 
+ 		/* Make sure 'total' fits in 32 bits */
+ 		if (total >> 32)
+ 			goto drop_precision;
+ 
+ 		/* Does rtime (and thus stime) fit in 32 bits? */
+ 		if (!(rtime >> 32))
+ 			break;
+ 
+ 		/* Can we just balance rtime/stime rather than dropping bits? */
+ 		if (stime >> 31)
+ 			goto drop_precision;
+ 
+ 		/* We can grow stime and shrink rtime and try to make them both fit */
+ 		stime <<= 1;
+ 		rtime >>= 1;
+ 		continue;
+ 
+ drop_precision:
+ 		/* We drop from rtime, it has more bits than stime */
+ 		rtime >>= 1;
+ 		total >>= 1;
+ 	}
+ 
+ 	/*
+ 	 * Make sure gcc understands that this is a 32x32->64 multiply,
+ 	 * followed by a 64/32->64 divide.
+ 	 */
+ 	scaled = div_u64((u64) (u32) stime * (u64) (u32) rtime, (u32)total);
+ 	return scaled;
+ }
+ 
+ /*
++>>>>>>> dfcb245e2848 (sched: Fix various typos in comments)
   * Adjust tick based cputime random precision against scheduler runtime
   * accounting.
   *
diff --cc kernel/sched/fair.c
index 0e43a2aa5122,fdc8356ea742..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -4293,7 -3977,7 +4293,11 @@@ dequeue_entity(struct cfs_rq *cfs_rq, s
  	 * When dequeuing a sched_entity, we must:
  	 *   - Update loads to have both entity and cfs_rq synced with now.
  	 *   - Subtract its load from the cfs_rq->runnable_avg.
++<<<<<<< HEAD
 +	 *   - Substract its previous weight from cfs_rq->load.weight.
++=======
+ 	 *   - Subtract its previous weight from cfs_rq->load.weight.
++>>>>>>> dfcb245e2848 (sched: Fix various typos in comments)
  	 *   - For group entity, update its weight to reflect the new share
  	 *     of its group cfs_rq.
  	 */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 187fbf3248da..f8f2f0aaab28 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -181,7 +181,7 @@ struct task_group;
  * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).
  *
  * However, with slightly different timing the wakeup TASK_RUNNING store can
- * also collide with the TASK_UNINTERRUPTIBLE store. Loosing that store is not
+ * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not
  * a problem either because that will result in one extra go around the loop
  * and our @cond test will save the day.
  *
@@ -509,7 +509,7 @@ struct sched_dl_entity {
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,
-	 * they are continously updated during task execution. Note that
+	 * they are continuously updated during task execution. Note that
 	 * the remaining runtime could be < 0 in case we are in overrun.
 	 */
 	s64				runtime;	/* Remaining runtime for this instance	*/
diff --git a/include/linux/sched/isolation.h b/include/linux/sched/isolation.h
index dfffbc98a63f..cc9f393e2a70 100644
--- a/include/linux/sched/isolation.h
+++ b/include/linux/sched/isolation.h
@@ -18,7 +18,7 @@ enum hk_flags {
 };
 
 #ifdef CONFIG_CPU_ISOLATION
-DECLARE_STATIC_KEY_FALSE(housekeeping_overriden);
+DECLARE_STATIC_KEY_FALSE(housekeeping_overridden);
 extern int housekeeping_any_cpu(enum hk_flags flags);
 extern const struct cpumask *housekeeping_cpumask(enum hk_flags flags);
 extern bool housekeeping_enabled(enum hk_flags flags);
@@ -51,7 +51,7 @@ static inline void housekeeping_init(void) { }
 static inline bool housekeeping_cpu(int cpu, enum hk_flags flags)
 {
 #ifdef CONFIG_CPU_ISOLATION
-	if (static_branch_unlikely(&housekeeping_overriden))
+	if (static_branch_unlikely(&housekeeping_overridden))
 		return housekeeping_test_cpu(cpu, flags);
 #endif
 	return true;
* Unmerged path include/linux/sched/mm.h
diff --git a/include/linux/sched/stat.h b/include/linux/sched/stat.h
index f30954cc059d..568286411b43 100644
--- a/include/linux/sched/stat.h
+++ b/include/linux/sched/stat.h
@@ -8,7 +8,7 @@
  * Various counters maintained by the scheduler and fork(),
  * exposed via /proc, sys.c or used by drivers via these APIs.
  *
- * ( Note that all these values are aquired without locking,
+ * ( Note that all these values are acquired without locking,
  *   so they can only be relied on in narrow circumstances. )
  */
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 3cc7d5465aa5..ec4b71b10283 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3152,7 +3152,7 @@ unsigned long nr_running(void)
  * preemption, thus the result might have a time-of-check-to-time-of-use
  * race.  The caller is responsible to use it correctly, for example:
  *
- * - from a non-preemptable section (of course)
+ * - from a non-preemptible section (of course)
  *
  * - from a thread that is bound to a single CPU
  *
* Unmerged path kernel/sched/cputime.c
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 7ffc40e0955b..b49c700b8726 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -837,7 +837,7 @@ static void replenish_dl_entity(struct sched_dl_entity *dl_se)
  * refill the runtime and set the deadline a period in the future,
  * because keeping the current (absolute) deadline of the task would
  * result in breaking guarantees promised to other tasks (refer to
- * Documentation/scheduler/sched-deadline.txt for more informations).
+ * Documentation/scheduler/sched-deadline.txt for more information).
  *
  * This function returns true if:
  *
* Unmerged path kernel/sched/fair.c
diff --git a/kernel/sched/isolation.c b/kernel/sched/isolation.c
index e02a43386575..636594aa28ad 100644
--- a/kernel/sched/isolation.c
+++ b/kernel/sched/isolation.c
@@ -8,8 +8,8 @@
  */
 #include "sched.h"
 
-DEFINE_STATIC_KEY_FALSE(housekeeping_overriden);
-EXPORT_SYMBOL_GPL(housekeeping_overriden);
+DEFINE_STATIC_KEY_FALSE(housekeeping_overridden);
+EXPORT_SYMBOL_GPL(housekeeping_overridden);
 static cpumask_var_t housekeeping_mask;
 static unsigned int housekeeping_flags;
 
@@ -21,7 +21,7 @@ EXPORT_SYMBOL_GPL(housekeeping_enabled);
 
 int housekeeping_any_cpu(enum hk_flags flags)
 {
-	if (static_branch_unlikely(&housekeeping_overriden))
+	if (static_branch_unlikely(&housekeeping_overridden))
 		if (housekeeping_flags & flags)
 			return cpumask_any_and(housekeeping_mask, cpu_online_mask);
 	return smp_processor_id();
@@ -30,7 +30,7 @@ EXPORT_SYMBOL_GPL(housekeeping_any_cpu);
 
 const struct cpumask *housekeeping_cpumask(enum hk_flags flags)
 {
-	if (static_branch_unlikely(&housekeeping_overriden))
+	if (static_branch_unlikely(&housekeeping_overridden))
 		if (housekeeping_flags & flags)
 			return housekeeping_mask;
 	return cpu_possible_mask;
@@ -39,7 +39,7 @@ EXPORT_SYMBOL_GPL(housekeeping_cpumask);
 
 void housekeeping_affine(struct task_struct *t, enum hk_flags flags)
 {
-	if (static_branch_unlikely(&housekeeping_overriden))
+	if (static_branch_unlikely(&housekeeping_overridden))
 		if (housekeeping_flags & flags)
 			set_cpus_allowed_ptr(t, housekeeping_mask);
 }
@@ -47,7 +47,7 @@ EXPORT_SYMBOL_GPL(housekeeping_affine);
 
 bool housekeeping_test_cpu(int cpu, enum hk_flags flags)
 {
-	if (static_branch_unlikely(&housekeeping_overriden))
+	if (static_branch_unlikely(&housekeeping_overridden))
 		if (housekeeping_flags & flags)
 			return cpumask_test_cpu(cpu, housekeeping_mask);
 	return true;
@@ -59,7 +59,7 @@ void __init housekeeping_init(void)
 	if (!housekeeping_flags)
 		return;
 
-	static_branch_enable(&housekeeping_overriden);
+	static_branch_enable(&housekeeping_overridden);
 
 	if (housekeeping_flags & HK_FLAG_TICK)
 		sched_tick_offload_init();
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 73dbfecd533e..f59d1a0a41e9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -675,7 +675,7 @@ struct dl_rq {
 	/*
 	 * Deadline values of the currently executing and the
 	 * earliest ready task on this rq. Caching these facilitates
-	 * the decision wether or not a ready but not running task
+	 * the decision whether or not a ready but not running task
 	 * should migrate somewhere else.
 	 */
 	struct {
@@ -1568,7 +1568,7 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 #ifdef CONFIG_SMP
 	/*
 	 * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be
-	 * successfuly executed on another CPU. We must ensure that updates of
+	 * successfully executed on another CPU. We must ensure that updates of
 	 * per-task data have been completed by this moment.
 	 */
 	smp_wmb();
