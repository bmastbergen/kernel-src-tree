locking/mutexes: Modify the way optimistic spinners are queued

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Jason Low <jason.low2@hp.com>
commit 47667fa1502e4d759df87e9cc7fbc0f202483361
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/47667fa1.failed

The mutex->spin_mlock was introduced in order to ensure that only 1 thread
spins for lock acquisition at a time to reduce cache line contention. When
lock->owner is NULL and the lock->count is still not 1, the spinner(s) will
continually release and obtain the lock->spin_mlock. This can generate
quite a bit of overhead/contention, and also might just delay the spinner
from getting the lock.

This patch modifies the way optimistic spinners are queued by queuing before
entering the optimistic spinning loop as oppose to acquiring before every
call to mutex_spin_on_owner(). So in situations where the spinner requires
a few extra spins before obtaining the lock, then there will only be 1 spinner
trying to get the lock and it will avoid the overhead from unnecessarily
unlocking and locking the spin_mlock.

	Signed-off-by: Jason Low <jason.low2@hp.com>
	Cc: tglx@linutronix.de
	Cc: riel@redhat.com
	Cc: akpm@linux-foundation.org
	Cc: davidlohr@hp.com
	Cc: hpa@zytor.com
	Cc: andi@firstfloor.org
	Cc: aswin@hp.com
	Cc: scott.norton@hp.com
	Cc: chegu_vinod@hp.com
	Cc: Waiman.Long@hp.com
	Cc: paulmck@linux.vnet.ibm.com
	Cc: torvalds@linux-foundation.org
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1390936396-3962-3-git-send-email-jason.low2@hp.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 47667fa1502e4d759df87e9cc7fbc0f202483361)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/mutex.c
diff --cc kernel/mutex.c
index 9206c5597f6c,82dad2ccd40b..000000000000
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@@ -454,11 -403,11 +454,15 @@@ __mutex_lock_common(struct mutex *lock
  	if (!mutex_can_spin_on_owner(lock))
  		goto slowpath;
  
+ 	mcs_spin_lock(&lock->mcs_lock, &node);
  	for (;;) {
  		struct task_struct *owner;
++<<<<<<< HEAD:kernel/mutex.c
 +		struct mspin_node  node;
++=======
++>>>>>>> 47667fa1502e (locking/mutexes: Modify the way optimistic spinners are queued):kernel/locking/mutex.c
  
 -		if (use_ww_ctx && ww_ctx->acquired > 0) {
 +		if (!__builtin_constant_p(ww_ctx == NULL) && ww_ctx->acquired > 0) {
  			struct ww_mutex *ww;
  
  			ww = container_of(lock, struct ww_mutex, base);
@@@ -478,12 -427,9 +482,18 @@@
  		 * If there's an owner, wait for it to either
  		 * release the lock or go to sleep.
  		 */
++<<<<<<< HEAD:kernel/mutex.c
 +		mspin_lock(MLOCK(lock), &node);
 +		owner = ACCESS_ONCE(lock->owner);
 +		if (owner && !mutex_spin_on_owner(lock, owner)) {
 +			mspin_unlock(MLOCK(lock), &node);
 +			goto slowpath;
 +		}
++=======
+ 		owner = ACCESS_ONCE(lock->owner);
+ 		if (owner && !mutex_spin_on_owner(lock, owner))
+ 			break;
++>>>>>>> 47667fa1502e (locking/mutexes: Modify the way optimistic spinners are queued):kernel/locking/mutex.c
  
  		if ((atomic_read(&lock->count) == 1) &&
  		    (atomic_cmpxchg(&lock->count, 1, 0) == 1)) {
@@@ -500,7 -446,6 +510,10 @@@
  			preempt_enable();
  			return 0;
  		}
++<<<<<<< HEAD:kernel/mutex.c
 +		mspin_unlock(MLOCK(lock), &node);
++=======
++>>>>>>> 47667fa1502e (locking/mutexes: Modify the way optimistic spinners are queued):kernel/locking/mutex.c
  
  		/*
  		 * When there's no owner, we might have preempted between the
* Unmerged path kernel/mutex.c
