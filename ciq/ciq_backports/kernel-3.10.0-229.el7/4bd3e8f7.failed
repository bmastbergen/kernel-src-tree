mm/swap.c: split put_compound_page()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [mm] swap: split put_compound_page() (Andrea Arcangeli) [1135506]
Rebuild_FUZZ: 92.54%
commit-author Jianyu Zhan <nasa4836@gmail.com>
commit 4bd3e8f7b94785a6f65665fee21ff3dbc2bf4ef8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/4bd3e8f7.failed

Currently, put_compound_page() carefully handles tricky cases to avoid
racing with compound page releasing or splitting, which makes it quite
lenthy (about 200+ lines) and needs deep tab indention, which makes it
quite hard to follow and maintain.

Now based on two helpers introduced in the previous patch ("mm/swap.c:
introduce put_[un]refcounted_compound_page helpers for spliting
put_compound_page"), this patch replaces those two lengthy code paths with
these two helpers, respectively.  Also, it has some comment rephrasing.

After this patch, the put_compound_page() is very compact, thus easy to
read and maintain.

After splitting, the object file is of same size as the original one.
Actually, I've diff'ed put_compound_page()'s orginal disassemble code and
the patched disassemble code, the are 100% the same!

This fact shows that this splitting has no functional change, but it
brings readability.

This patch and the previous one blow the code by 32 lines, mostly due to
comments.

	Signed-off-by: Jianyu Zhan <nasa4836@gmail.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Jiang Liu <liuj97@gmail.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Cc: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4bd3e8f7b94785a6f65665fee21ff3dbc2bf4ef8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index 6dab198384ff,d089c5a0cf98..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -239,135 -244,20 +244,139 @@@ static void put_compound_page(struct pa
  		return;
  	}
  
++<<<<<<< HEAD
 +	/* __split_huge_page_refcount can run under us */
 +	page_head = compound_trans_head(page);
 +
++=======
++>>>>>>> 4bd3e8f7b947 (mm/swap.c: split put_compound_page())
  	/*
- 	 * THP can not break up slab pages so avoid taking
- 	 * compound_lock() and skip the tail page refcounting (in
- 	 * _mapcount) too. Slab performs non-atomic bit ops on
- 	 * page->flags for better performance. In particular
- 	 * slab_unlock() in slub used to be a hot path. It is still
- 	 * hot on arches that do not support
- 	 * this_cpu_cmpxchg_double().
+ 	 * We see the PageCompound set and PageTail set, so @page maybe:
+ 	 *  1. a tail hugetlbfs page, or
+ 	 *  2. a tail THP page, or
+ 	 *  3. a split THP page.
  	 *
- 	 * If "page" is part of a slab or hugetlbfs page it cannot be
- 	 * splitted and the head page cannot change from under us. And
- 	 * if "page" is part of a THP page under splitting, if the
- 	 * head page pointed by the THP tail isn't a THP head anymore,
- 	 * we'll find PageTail clear after smp_rmb() and we'll treat
- 	 * it as a single page.
+ 	 *  Case 3 is possible, as we may race with
+ 	 *  __split_huge_page_refcount tearing down a THP page.
  	 */
++<<<<<<< HEAD
 +	if (!__compound_tail_refcounted(page_head)) {
 +		/*
 +		 * If "page" is a THP tail, we must read the tail page
 +		 * flags after the head page flags. The
 +		 * split_huge_page side enforces write memory barriers
 +		 * between clearing PageTail and before the head page
 +		 * can be freed and reallocated.
 +		 */
 +		smp_rmb();
 +		if (likely(PageTail(page))) {
 +			/*
 +			 * __split_huge_page_refcount cannot race
 +			 * here.
 +			 */
 +			VM_BUG_ON(!PageHead(page_head));
 +			VM_BUG_ON(page_mapcount(page) != 0);
 +			if (put_page_testzero(page_head)) {
 +				/*
 +				 * If this is the tail of a slab
 +				 * compound page, the tail pin must
 +				 * not be the last reference held on
 +				 * the page, because the PG_slab
 +				 * cannot be cleared before all tail
 +				 * pins (which skips the _mapcount
 +				 * tail refcounting) have been
 +				 * released. For hugetlbfs the tail
 +				 * pin may be the last reference on
 +				 * the page instead, because
 +				 * PageHeadHuge will not go away until
 +				 * the compound page enters the buddy
 +				 * allocator.
 +				 */
 +				VM_BUG_ON(PageSlab(page_head));
 +				__put_compound_page(page_head);
 +			}
 +			return;
 +		} else
 +			/*
 +			 * __split_huge_page_refcount run before us,
 +			 * "page" was a THP tail. The split page_head
 +			 * has been freed and reallocated as slab or
 +			 * hugetlbfs page of smaller order (only
 +			 * possible if reallocated as slab on x86).
 +			 */
 +			goto out_put_single;
 +	}
 +
 +	if (likely(page != page_head && get_page_unless_zero(page_head))) {
 +		unsigned long flags;
 +
 +		/*
 +		 * page_head wasn't a dangling pointer but it may not
 +		 * be a head page anymore by the time we obtain the
 +		 * lock. That is ok as long as it can't be freed from
 +		 * under us.
 +		 */
 +		flags = compound_lock_irqsave(page_head);
 +		if (unlikely(!PageTail(page))) {
 +			/* __split_huge_page_refcount run before us */
 +			compound_unlock_irqrestore(page_head, flags);
 +			if (put_page_testzero(page_head)) {
 +				/*
 +				 * The head page may have been freed
 +				 * and reallocated as a compound page
 +				 * of smaller order and then freed
 +				 * again.  All we know is that it
 +				 * cannot have become: a THP page, a
 +				 * compound page of higher order, a
 +				 * tail page.  That is because we
 +				 * still hold the refcount of the
 +				 * split THP tail and page_head was
 +				 * the THP head before the split.
 +				 */
 +				if (PageHead(page_head))
 +					__put_compound_page(page_head);
 +				else
 +					__put_single_page(page_head);
 +			}
 +out_put_single:
 +			if (put_page_testzero(page))
 +				__put_single_page(page);
 +			return;
 +		}
 +		VM_BUG_ON(page_head != page->first_page);
 +		/*
 +		 * We can release the refcount taken by
 +		 * get_page_unless_zero() now that
 +		 * __split_huge_page_refcount() is blocked on the
 +		 * compound_lock.
 +		 */
 +		if (put_page_testzero(page_head))
 +			VM_BUG_ON(1);
 +		/* __split_huge_page_refcount will wait now */
 +		VM_BUG_ON(page_mapcount(page) <= 0);
 +		atomic_dec(&page->_mapcount);
 +		VM_BUG_ON(atomic_read(&page_head->_count) <= 0);
 +		VM_BUG_ON(atomic_read(&page->_count) != 0);
 +		compound_unlock_irqrestore(page_head, flags);
 +
 +		if (put_page_testzero(page_head)) {
 +			if (PageHead(page_head))
 +				__put_compound_page(page_head);
 +			else
 +				__put_single_page(page_head);
 +		}
 +	} else {
 +		/* page_head is a dangling pointer */
 +		VM_BUG_ON(PageTail(page));
 +		goto out_put_single;
 +	}
++=======
+ 	page_head = compound_head(page);
+ 	if (!__compound_tail_refcounted(page_head))
+ 		put_unrefcounted_compound_page(page_head, page);
+ 	else
+ 		put_refcounted_compound_page(page_head, page);
++>>>>>>> 4bd3e8f7b947 (mm/swap.c: split put_compound_page())
  }
  
  void put_page(struct page *page)
* Unmerged path mm/swap.c
