KVM: PPC: Book3S HV: Add transactional memory support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s hv - Add transactional memory support (Don Zickus) [1127366]
Rebuild_FUZZ: 94.34%
commit-author Michael Neuling <mikey@neuling.org>
commit e4e38121507a27d2ccc4b28d9e7fc4818a12c44c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/e4e38121.failed

This adds saving of the transactional memory (TM) checkpointed state
on guest entry and exit.  We only do this if we see that the guest has
an active transaction.

It also adds emulation of the TM state changes when delivering IRQs
into the guest.  According to the architecture, if we are
transactional when an IRQ occurs, the TM state is changed to
suspended, otherwise it's left unchanged.

	Signed-off-by: Michael Neuling <mikey@neuling.org>
	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Acked-by: Scott Wood <scottwood@freescale.com>
(cherry picked from commit e4e38121507a27d2ccc4b28d9e7fc4818a12c44c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_64_mmu_hv.c
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/kvm/book3s_64_mmu_hv.c
index 4e91fbd97699,fb25ebc0af0c..000000000000
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@@ -260,13 -260,16 +260,24 @@@ int kvmppc_mmu_hv_init(void
  	return 0;
  }
  
 +void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)
 +{
 +}
 +
  static void kvmppc_mmu_book3s_64_hv_reset_msr(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	kvmppc_set_msr(vcpu, MSR_SF | MSR_ME);
++=======
+ 	unsigned long msr = vcpu->arch.intr_msr;
+ 
+ 	/* If transactional, change to suspend mode on IRQ delivery */
+ 	if (MSR_TM_TRANSACTIONAL(vcpu->arch.shregs.msr))
+ 		msr |= MSR_TS_S;
+ 	else
+ 		msr |= vcpu->arch.shregs.msr & MSR_TS_MASK;
+ 	kvmppc_set_msr(vcpu, msr);
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  }
  
  /*
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index 0ae4c28ae4b6,61190ddd9f3b..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -642,6 -565,311 +645,314 @@@ toc_tlbie_lock
  	addi	r6,r6,VCPU_SLB_SIZE
  	bdnz	1b
  9:
++<<<<<<< HEAD
++=======
+ 	/* Increment yield count if they have a VPA */
+ 	ld	r3, VCPU_VPA(r4)
+ 	cmpdi	r3, 0
+ 	beq	25f
+ 	lwz	r5, LPPACA_YIELDCOUNT(r3)
+ 	addi	r5, r5, 1
+ 	stw	r5, LPPACA_YIELDCOUNT(r3)
+ 	li	r6, 1
+ 	stb	r6, VCPU_VPA_DIRTY(r4)
+ 25:
+ 
+ BEGIN_FTR_SECTION
+ 	/* Save purr/spurr */
+ 	mfspr	r5,SPRN_PURR
+ 	mfspr	r6,SPRN_SPURR
+ 	std	r5,HSTATE_PURR(r13)
+ 	std	r6,HSTATE_SPURR(r13)
+ 	ld	r7,VCPU_PURR(r4)
+ 	ld	r8,VCPU_SPURR(r4)
+ 	mtspr	SPRN_PURR,r7
+ 	mtspr	SPRN_SPURR,r8
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Set partition DABR */
+ 	/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */
+ 	lwz	r5,VCPU_DABRX(r4)
+ 	ld	r6,VCPU_DABR(r4)
+ 	mtspr	SPRN_DABRX,r5
+ 	mtspr	SPRN_DABR,r6
+  BEGIN_FTR_SECTION_NESTED(89)
+ 	isync
+  END_FTR_SECTION_NESTED(CPU_FTR_ARCH_206, CPU_FTR_ARCH_206, 89)
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 
+ #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+ BEGIN_FTR_SECTION
+ 	b	skip_tm
+ END_FTR_SECTION_IFCLR(CPU_FTR_TM)
+ 
+ 	/* Turn on TM/FP/VSX/VMX so we can restore them. */
+ 	mfmsr	r5
+ 	li	r6, MSR_TM >> 32
+ 	sldi	r6, r6, 32
+ 	or	r5, r5, r6
+ 	ori	r5, r5, MSR_FP
+ 	oris	r5, r5, (MSR_VEC | MSR_VSX)@h
+ 	mtmsrd	r5
+ 
+ 	/*
+ 	 * The user may change these outside of a transaction, so they must
+ 	 * always be context switched.
+ 	 */
+ 	ld	r5, VCPU_TFHAR(r4)
+ 	ld	r6, VCPU_TFIAR(r4)
+ 	ld	r7, VCPU_TEXASR(r4)
+ 	mtspr	SPRN_TFHAR, r5
+ 	mtspr	SPRN_TFIAR, r6
+ 	mtspr	SPRN_TEXASR, r7
+ 
+ 	ld	r5, VCPU_MSR(r4)
+ 	rldicl. r5, r5, 64 - MSR_TS_S_LG, 62
+ 	beq	skip_tm	/* TM not active in guest */
+ 
+ 	/* Make sure the failure summary is set, otherwise we'll program check
+ 	 * when we trechkpt.  It's possible that this might have been not set
+ 	 * on a kvmppc_set_one_reg() call but we shouldn't let this crash the
+ 	 * host.
+ 	 */
+ 	oris	r7, r7, (TEXASR_FS)@h
+ 	mtspr	SPRN_TEXASR, r7
+ 
+ 	/*
+ 	 * We need to load up the checkpointed state for the guest.
+ 	 * We need to do this early as it will blow away any GPRs, VSRs and
+ 	 * some SPRs.
+ 	 */
+ 
+ 	mr	r31, r4
+ 	addi	r3, r31, VCPU_FPRS_TM
+ 	bl	.load_fp_state
+ 	addi	r3, r31, VCPU_VRS_TM
+ 	bl	.load_vr_state
+ 	mr	r4, r31
+ 	lwz	r7, VCPU_VRSAVE_TM(r4)
+ 	mtspr	SPRN_VRSAVE, r7
+ 
+ 	ld	r5, VCPU_LR_TM(r4)
+ 	lwz	r6, VCPU_CR_TM(r4)
+ 	ld	r7, VCPU_CTR_TM(r4)
+ 	ld	r8, VCPU_AMR_TM(r4)
+ 	ld	r9, VCPU_TAR_TM(r4)
+ 	mtlr	r5
+ 	mtcr	r6
+ 	mtctr	r7
+ 	mtspr	SPRN_AMR, r8
+ 	mtspr	SPRN_TAR, r9
+ 
+ 	/*
+ 	 * Load up PPR and DSCR values but don't put them in the actual SPRs
+ 	 * till the last moment to avoid running with userspace PPR and DSCR for
+ 	 * too long.
+ 	 */
+ 	ld	r29, VCPU_DSCR_TM(r4)
+ 	ld	r30, VCPU_PPR_TM(r4)
+ 
+ 	std	r2, PACATMSCRATCH(r13) /* Save TOC */
+ 
+ 	/* Clear the MSR RI since r1, r13 are all going to be foobar. */
+ 	li	r5, 0
+ 	mtmsrd	r5, 1
+ 
+ 	/* Load GPRs r0-r28 */
+ 	reg = 0
+ 	.rept	29
+ 	ld	reg, VCPU_GPRS_TM(reg)(r31)
+ 	reg = reg + 1
+ 	.endr
+ 
+ 	mtspr	SPRN_DSCR, r29
+ 	mtspr	SPRN_PPR, r30
+ 
+ 	/* Load final GPRs */
+ 	ld	29, VCPU_GPRS_TM(29)(r31)
+ 	ld	30, VCPU_GPRS_TM(30)(r31)
+ 	ld	31, VCPU_GPRS_TM(31)(r31)
+ 
+ 	/* TM checkpointed state is now setup.  All GPRs are now volatile. */
+ 	TRECHKPT
+ 
+ 	/* Now let's get back the state we need. */
+ 	HMT_MEDIUM
+ 	GET_PACA(r13)
+ 	ld	r29, HSTATE_DSCR(r13)
+ 	mtspr	SPRN_DSCR, r29
+ 	ld	r4, HSTATE_KVM_VCPU(r13)
+ 	ld	r1, HSTATE_HOST_R1(r13)
+ 	ld	r2, PACATMSCRATCH(r13)
+ 
+ 	/* Set the MSR RI since we have our registers back. */
+ 	li	r5, MSR_RI
+ 	mtmsrd	r5, 1
+ skip_tm:
+ #endif
+ 
+ 	/* Load guest PMU registers */
+ 	/* R4 is live here (vcpu pointer) */
+ 	li	r3, 1
+ 	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
+ 	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
+ 	isync
+ 	lwz	r3, VCPU_PMC(r4)	/* always load up guest PMU registers */
+ 	lwz	r5, VCPU_PMC + 4(r4)	/* to prevent information leak */
+ 	lwz	r6, VCPU_PMC + 8(r4)
+ 	lwz	r7, VCPU_PMC + 12(r4)
+ 	lwz	r8, VCPU_PMC + 16(r4)
+ 	lwz	r9, VCPU_PMC + 20(r4)
+ BEGIN_FTR_SECTION
+ 	lwz	r10, VCPU_PMC + 24(r4)
+ 	lwz	r11, VCPU_PMC + 28(r4)
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	mtspr	SPRN_PMC1, r3
+ 	mtspr	SPRN_PMC2, r5
+ 	mtspr	SPRN_PMC3, r6
+ 	mtspr	SPRN_PMC4, r7
+ 	mtspr	SPRN_PMC5, r8
+ 	mtspr	SPRN_PMC6, r9
+ BEGIN_FTR_SECTION
+ 	mtspr	SPRN_PMC7, r10
+ 	mtspr	SPRN_PMC8, r11
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	ld	r3, VCPU_MMCR(r4)
+ 	ld	r5, VCPU_MMCR + 8(r4)
+ 	ld	r6, VCPU_MMCR + 16(r4)
+ 	ld	r7, VCPU_SIAR(r4)
+ 	ld	r8, VCPU_SDAR(r4)
+ 	mtspr	SPRN_MMCR1, r5
+ 	mtspr	SPRN_MMCRA, r6
+ 	mtspr	SPRN_SIAR, r7
+ 	mtspr	SPRN_SDAR, r8
+ BEGIN_FTR_SECTION
+ 	ld	r5, VCPU_MMCR + 24(r4)
+ 	ld	r6, VCPU_SIER(r4)
+ 	lwz	r7, VCPU_PMC + 24(r4)
+ 	lwz	r8, VCPU_PMC + 28(r4)
+ 	ld	r9, VCPU_MMCR + 32(r4)
+ 	mtspr	SPRN_MMCR2, r5
+ 	mtspr	SPRN_SIER, r6
+ 	mtspr	SPRN_SPMC1, r7
+ 	mtspr	SPRN_SPMC2, r8
+ 	mtspr	SPRN_MMCRS, r9
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
+ 	mtspr	SPRN_MMCR0, r3
+ 	isync
+ 
+ 	/* Load up FP, VMX and VSX registers */
+ 	bl	kvmppc_load_fp
+ 
+ 	ld	r14, VCPU_GPR(R14)(r4)
+ 	ld	r15, VCPU_GPR(R15)(r4)
+ 	ld	r16, VCPU_GPR(R16)(r4)
+ 	ld	r17, VCPU_GPR(R17)(r4)
+ 	ld	r18, VCPU_GPR(R18)(r4)
+ 	ld	r19, VCPU_GPR(R19)(r4)
+ 	ld	r20, VCPU_GPR(R20)(r4)
+ 	ld	r21, VCPU_GPR(R21)(r4)
+ 	ld	r22, VCPU_GPR(R22)(r4)
+ 	ld	r23, VCPU_GPR(R23)(r4)
+ 	ld	r24, VCPU_GPR(R24)(r4)
+ 	ld	r25, VCPU_GPR(R25)(r4)
+ 	ld	r26, VCPU_GPR(R26)(r4)
+ 	ld	r27, VCPU_GPR(R27)(r4)
+ 	ld	r28, VCPU_GPR(R28)(r4)
+ 	ld	r29, VCPU_GPR(R29)(r4)
+ 	ld	r30, VCPU_GPR(R30)(r4)
+ 	ld	r31, VCPU_GPR(R31)(r4)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Switch DSCR to guest value */
+ 	ld	r5, VCPU_DSCR(r4)
+ 	mtspr	SPRN_DSCR, r5
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Skip next section on POWER7 or PPC970 */
+ 	b	8f
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
+ 	mfmsr	r8
+ 	li	r0, 1
+ 	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
+ 	mtmsrd	r8
+ 
+ 	/* Load up POWER8-specific registers */
+ 	ld	r5, VCPU_IAMR(r4)
+ 	lwz	r6, VCPU_PSPB(r4)
+ 	ld	r7, VCPU_FSCR(r4)
+ 	mtspr	SPRN_IAMR, r5
+ 	mtspr	SPRN_PSPB, r6
+ 	mtspr	SPRN_FSCR, r7
+ 	ld	r5, VCPU_DAWR(r4)
+ 	ld	r6, VCPU_DAWRX(r4)
+ 	ld	r7, VCPU_CIABR(r4)
+ 	ld	r8, VCPU_TAR(r4)
+ 	mtspr	SPRN_DAWR, r5
+ 	mtspr	SPRN_DAWRX, r6
+ 	mtspr	SPRN_CIABR, r7
+ 	mtspr	SPRN_TAR, r8
+ 	ld	r5, VCPU_IC(r4)
+ 	ld	r6, VCPU_VTB(r4)
+ 	mtspr	SPRN_IC, r5
+ 	mtspr	SPRN_VTB, r6
+ 	ld	r8, VCPU_EBBHR(r4)
+ 	mtspr	SPRN_EBBHR, r8
+ 	ld	r5, VCPU_EBBRR(r4)
+ 	ld	r6, VCPU_BESCR(r4)
+ 	ld	r7, VCPU_CSIGR(r4)
+ 	ld	r8, VCPU_TACR(r4)
+ 	mtspr	SPRN_EBBRR, r5
+ 	mtspr	SPRN_BESCR, r6
+ 	mtspr	SPRN_CSIGR, r7
+ 	mtspr	SPRN_TACR, r8
+ 	ld	r5, VCPU_TCSCR(r4)
+ 	ld	r6, VCPU_ACOP(r4)
+ 	lwz	r7, VCPU_GUEST_PID(r4)
+ 	ld	r8, VCPU_WORT(r4)
+ 	mtspr	SPRN_TCSCR, r5
+ 	mtspr	SPRN_ACOP, r6
+ 	mtspr	SPRN_PID, r7
+ 	mtspr	SPRN_WORT, r8
+ 8:
+ 
+ 	/*
+ 	 * Set the decrementer to the guest decrementer.
+ 	 */
+ 	ld	r8,VCPU_DEC_EXPIRES(r4)
+ 	mftb	r7
+ 	subf	r3,r7,r8
+ 	mtspr	SPRN_DEC,r3
+ 	stw	r3,VCPU_DEC(r4)
+ 
+ 	ld	r5, VCPU_SPRG0(r4)
+ 	ld	r6, VCPU_SPRG1(r4)
+ 	ld	r7, VCPU_SPRG2(r4)
+ 	ld	r8, VCPU_SPRG3(r4)
+ 	mtspr	SPRN_SPRG0, r5
+ 	mtspr	SPRN_SPRG1, r6
+ 	mtspr	SPRN_SPRG2, r7
+ 	mtspr	SPRN_SPRG3, r8
+ 
+ 	/* Load up DAR and DSISR */
+ 	ld	r5, VCPU_DAR(r4)
+ 	lwz	r6, VCPU_DSISR(r4)
+ 	mtspr	SPRN_DAR, r5
+ 	mtspr	SPRN_DSISR, r6
+ 
+ BEGIN_FTR_SECTION
+ 	/* Restore AMR and UAMOR, set AMOR to all 1s */
+ 	ld	r5,VCPU_AMR(r4)
+ 	ld	r6,VCPU_UAMOR(r4)
+ 	li	r7,-1
+ 	mtspr	SPRN_AMR,r5
+ 	mtspr	SPRN_UAMOR,r6
+ 	mtspr	SPRN_AMOR,r7
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  
  	/* Restore state of CTRL run bit; assume 1 on entry */
  	lwz	r5,VCPU_CTRL(r4)
@@@ -682,22 -912,19 +993,29 @@@ BEGIN_FTR_SECTIO
  	isync
  END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
  	beq	5f
 -	li	r0, BOOK3S_INTERRUPT_EXTERNAL
 -	bne	cr1, 12f
 -	mfspr	r0, SPRN_DEC
 -	cmpwi	r0, 0
 -	li	r0, BOOK3S_INTERRUPT_DECREMENTER
 -	bge	5f
 -
 -12:	mtspr	SPRN_SRR0, r10
 +	li	r0,BOOK3S_INTERRUPT_EXTERNAL
 +12:	mr	r6,r10
  	mr	r10,r0
++<<<<<<< HEAD
 +	mr	r7,r11
 +	li	r11,(MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11,r11,63
 +	b	5f
 +11:	beq	5f
 +	mfspr	r0,SPRN_DEC
 +	cmpwi	r0,0
 +	li	r0,BOOK3S_INTERRUPT_DECREMENTER
 +	blt	12b
 +
 +	/* Move SRR0 and SRR1 into the respective regs */
 +5:	mtspr	SPRN_SRR0, r6
 +	mtspr	SPRN_SRR1, r7
++=======
+ 	mtspr	SPRN_SRR1, r11
+ 	mr	r9, r4
+ 	bl	kvmppc_msr_interrupt
+ 5:
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  
  /*
   * Required state:
@@@ -1214,23 -1596,6 +1532,62 @@@ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201
  1:	addi	r8,r8,16
  	.endr
  
 +	/* Save DEC */
 +	mfspr	r5,SPRN_DEC
 +	mftb	r6
 +	extsw	r5,r5
 +	add	r5,r5,r6
 +	std	r5,VCPU_DEC_EXPIRES(r9)
 +
++<<<<<<< HEAD
++=======
++BEGIN_FTR_SECTION
++	b	8f
++END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
++	/* Save POWER8-specific registers */
++	mfspr	r5, SPRN_IAMR
++	mfspr	r6, SPRN_PSPB
++	mfspr	r7, SPRN_FSCR
++	std	r5, VCPU_IAMR(r9)
++	stw	r6, VCPU_PSPB(r9)
++	std	r7, VCPU_FSCR(r9)
++	mfspr	r5, SPRN_IC
++	mfspr	r6, SPRN_VTB
++	mfspr	r7, SPRN_TAR
++	std	r5, VCPU_IC(r9)
++	std	r6, VCPU_VTB(r9)
++	std	r7, VCPU_TAR(r9)
++	mfspr	r8, SPRN_EBBHR
++	std	r8, VCPU_EBBHR(r9)
++	mfspr	r5, SPRN_EBBRR
++	mfspr	r6, SPRN_BESCR
++	mfspr	r7, SPRN_CSIGR
++	mfspr	r8, SPRN_TACR
++	std	r5, VCPU_EBBRR(r9)
++	std	r6, VCPU_BESCR(r9)
++	std	r7, VCPU_CSIGR(r9)
++	std	r8, VCPU_TACR(r9)
++	mfspr	r5, SPRN_TCSCR
++	mfspr	r6, SPRN_ACOP
++	mfspr	r7, SPRN_PID
++	mfspr	r8, SPRN_WORT
++	std	r5, VCPU_TCSCR(r9)
++	std	r6, VCPU_ACOP(r9)
++	stw	r7, VCPU_GUEST_PID(r9)
++	std	r8, VCPU_WORT(r9)
++8:
++
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
 +	/* Save and reset AMR and UAMOR before turning on the MMU */
 +BEGIN_FTR_SECTION
 +	mfspr	r5,SPRN_AMR
 +	mfspr	r6,SPRN_UAMOR
 +	std	r5,VCPU_AMR(r9)
 +	std	r6,VCPU_UAMOR(r9)
 +	li	r6,0
 +	mtspr	SPRN_AMR,r6
 +END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +
  	/* Unset guest mode */
  	li	r0, KVM_GUEST_MODE_NONE
  	stb	r0, HSTATE_IN_GUEST(r13)
@@@ -1404,8 -1649,7 +1761,12 @@@ kvmppc_hdsi
  	mtspr	SPRN_SRR0, r10
  	mtspr	SPRN_SRR1, r11
  	li	r10, BOOK3S_INTERRUPT_DATA_STORAGE
++<<<<<<< HEAD
 +	li	r11, (MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11, r11, 63
++=======
+ 	bl	kvmppc_msr_interrupt
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  fast_interrupt_c_return:
  6:	ld	r7, VCPU_CTR(r9)
  	lwz	r8, VCPU_XER(r9)
@@@ -1474,8 -1718,7 +1835,12 @@@ kvmppc_hisi
  1:	mtspr	SPRN_SRR0, r10
  	mtspr	SPRN_SRR1, r11
  	li	r10, BOOK3S_INTERRUPT_INST_STORAGE
++<<<<<<< HEAD
 +	li	r11, (MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11, r11, 63
++=======
+ 	bl	kvmppc_msr_interrupt
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  	b	fast_interrupt_c_return
  
  3:	ld	r6, VCPU_KVM(r9)	/* not relocated, use VRMA */
@@@ -1518,8 -1761,7 +1883,12 @@@ sc_1_fast_return
  	mtspr	SPRN_SRR0,r10
  	mtspr	SPRN_SRR1,r11
  	li	r10, BOOK3S_INTERRUPT_SYSCALL
++<<<<<<< HEAD
 +	li	r11, (MSR_ME << 1) | 1  /* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11, r11, 63
++=======
+ 	bl	kvmppc_msr_interrupt
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  	mr	r4,r9
  	b	fast_guest_return
  
@@@ -1814,10 -2089,47 +2183,14 @@@ machine_check_realmode
  	beq	mc_cont
  	/* If not, deliver a machine check.  SRR0/1 are already set */
  	li	r10, BOOK3S_INTERRUPT_MACHINE_CHECK
++<<<<<<< HEAD
 +	li	r11, (MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11, r11, 63
++=======
+ 	bl	kvmppc_msr_interrupt
++>>>>>>> e4e38121507a (KVM: PPC: Book3S HV: Add transactional memory support)
  	b	fast_interrupt_c_return
  
 -/*
 - * Check the reason we woke from nap, and take appropriate action.
 - * Returns:
 - *	0 if nothing needs to be done
 - *	1 if something happened that needs to be handled by the host
 - *	-1 if there was a guest wakeup (IPI)
 - *
 - * Also sets r12 to the interrupt vector for any interrupt that needs
 - * to be handled now by the host (0x500 for external interrupt), or zero.
 - */
 -kvmppc_check_wake_reason:
 -	mfspr	r6, SPRN_SRR1
 -BEGIN_FTR_SECTION
 -	rlwinm	r6, r6, 45-31, 0xf	/* extract wake reason field (P8) */
 -FTR_SECTION_ELSE
 -	rlwinm	r6, r6, 45-31, 0xe	/* P7 wake reason field is 3 bits */
 -ALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)
 -	cmpwi	r6, 8			/* was it an external interrupt? */
 -	li	r12, BOOK3S_INTERRUPT_EXTERNAL
 -	beq	kvmppc_read_intr	/* if so, see what it was */
 -	li	r3, 0
 -	li	r12, 0
 -	cmpwi	r6, 6			/* was it the decrementer? */
 -	beq	0f
 -BEGIN_FTR_SECTION
 -	cmpwi	r6, 5			/* privileged doorbell? */
 -	beq	0f
 -	cmpwi	r6, 3			/* hypervisor doorbell? */
 -	beq	3f
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
 -	li	r3, 1			/* anything else, return 1 */
 -0:	blr
 -
 -	/* hypervisor doorbell */
 -3:	li	r12, BOOK3S_INTERRUPT_H_DOORBELL
 -	li	r3, 1
 -	blr
 -
  /*
   * Determine what sort of external interrupt is pending (if any).
   * Returns:
diff --git a/arch/powerpc/include/asm/reg.h b/arch/powerpc/include/asm/reg.h
index e983e00762dc..0ee7f0b71c5e 100644
--- a/arch/powerpc/include/asm/reg.h
+++ b/arch/powerpc/include/asm/reg.h
@@ -213,6 +213,7 @@
 #define SPRN_ACOP	0x1F	/* Available Coprocessor Register */
 #define SPRN_TFIAR	0x81	/* Transaction Failure Inst Addr   */
 #define SPRN_TEXASR	0x82	/* Transaction EXception & Summary */
+#define   TEXASR_FS	__MASK(63-36)	/* Transaction Failure Summary */
 #define SPRN_TEXASRU	0x83	/* ''	   ''	   ''	 Upper 32  */
 #define SPRN_TFHAR	0x80	/* Transaction Failure Handler Addr */
 #define SPRN_CTRLF	0x088
diff --git a/arch/powerpc/include/asm/tm.h b/arch/powerpc/include/asm/tm.h
index 0c9f8b74dd97..c22d704b6d41 100644
--- a/arch/powerpc/include/asm/tm.h
+++ b/arch/powerpc/include/asm/tm.h
@@ -7,6 +7,8 @@
 
 #include <uapi/asm/tm.h>
 
+#ifndef __ASSEMBLY__
+
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 extern void do_load_up_transact_fpu(struct thread_struct *thread);
 extern void do_load_up_transact_altivec(struct thread_struct *thread);
@@ -21,3 +23,5 @@ extern void tm_recheckpoint(struct thread_struct *thread,
 extern void tm_abort(uint8_t cause);
 extern void tm_save_sprs(struct thread_struct *thread);
 extern void tm_restore_sprs(struct thread_struct *thread);
+
+#endif /* __ASSEMBLY__ */
* Unmerged path arch/powerpc/kvm/book3s_64_mmu_hv.c
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
