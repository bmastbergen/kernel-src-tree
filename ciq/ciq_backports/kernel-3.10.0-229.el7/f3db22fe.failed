NVMe: Fix hot cpu notification dead lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Keith Busch <keith.busch@intel.com>
commit f3db22feb5de6b98b7bae924c2d4b6c8d65bedae
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/f3db22fe.failed

There is a potential dead lock if a cpu event occurs during nvme probe
since it registered with hot cpu notification. This fixes the race by
having the module register with notification outside of probe rather
than have each device register.

The actual work is done in a scheduled work queue instead of in the
notifier since assigning IO queues has the potential to block if the
driver creates additional queues.

	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
(cherry picked from commit f3db22feb5de6b98b7bae924c2d4b6c8d65bedae)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index c9d34d171e13,e0ac1210fe31..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -2003,55 -2200,9 +2015,50 @@@ static int nvme_setup_io_queues(struct 
  	}
  
  	/* Free previously allocated queues that are no longer usable */
 -	nvme_free_queues(dev, nr_io_queues + 1);
 -	nvme_assign_io_queues(dev);
 +	spin_lock(&dev_list_lock);
 +	for (i = dev->queue_count - 1; i > nr_io_queues; i--) {
 +		struct nvme_queue *nvmeq = dev->queues[i];
 +
 +		spin_lock_irq(&nvmeq->q_lock);
 +		nvme_cancel_ios(nvmeq, false);
 +		spin_unlock_irq(&nvmeq->q_lock);
 +
 +		nvme_free_queue(nvmeq);
 +		dev->queue_count--;
 +		dev->queues[i] = NULL;
 +	}
 +	spin_unlock(&dev_list_lock);
 +
 +	cpu = cpumask_first(cpu_online_mask);
 +	for (i = 0; i < nr_io_queues; i++) {
 +		irq_set_affinity_hint(dev->entry[i].vector, get_cpu_mask(cpu));
 +		cpu = cpumask_next(cpu, cpu_online_mask);
 +	}
 +
 +	q_depth = min_t(int, NVME_CAP_MQES(readq(&dev->bar->cap)) + 1,
 +								NVME_Q_DEPTH);
 +	for (i = dev->queue_count - 1; i < nr_io_queues; i++) {
 +		dev->queues[i + 1] = nvme_alloc_queue(dev, i + 1, q_depth, i);
 +		if (!dev->queues[i + 1]) {
 +			result = -ENOMEM;
 +			goto free_queues;
 +		}
 +	}
 +
 +	for (; i < num_possible_cpus(); i++) {
 +		int target = i % rounddown_pow_of_two(dev->queue_count - 1);
 +		dev->queues[i + 1] = dev->queues[target + 1];
 +	}
 +
 +	for (i = 1; i < dev->queue_count; i++) {
 +		result = nvme_create_queue(dev->queues[i], i);
 +		if (result) {
 +			for (--i; i > 0; i--)
 +				nvme_disable_queue(dev, i);
 +			goto free_queues;
 +		}
 +	}
  
- 	dev->nb.notifier_call = &nvme_cpu_notify;
- 	result = register_hotcpu_notifier(&dev->nb);
- 	if (result)
- 		goto free_queues;
- 
  	return 0;
  
   free_queues:
@@@ -2614,9 -2765,14 +2619,15 @@@ static int nvme_probe(struct pci_dev *p
  								GFP_KERNEL);
  	if (!dev->queues)
  		goto free;
 -	dev->io_queue = alloc_percpu(unsigned short);
 -	if (!dev->io_queue)
 -		goto free;
  
  	INIT_LIST_HEAD(&dev->namespaces);
++<<<<<<< HEAD
 +	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
++=======
+ 	dev->reset_workfn = nvme_reset_failed_dev;
+ 	INIT_WORK(&dev->reset_work, nvme_reset_workfn);
+ 	INIT_WORK(&dev->cpu_work, nvme_cpu_workfn);
++>>>>>>> f3db22feb5de (NVMe: Fix hot cpu notification dead lock)
  	dev->pci_dev = pdev;
  	pci_set_drvdata(pdev, dev);
  	result = nvme_set_instance(dev);
* Unmerged path drivers/block/nvme-core.c
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 76a5234815a5..262ba76eae7e 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -85,7 +85,7 @@ struct nvme_dev {
 	struct kref kref;
 	struct miscdevice miscdev;
 	struct work_struct reset_work;
-	struct notifier_block nb;
+	struct work_struct cpu_work;
 	char name[12];
 	char serial[20];
 	char model[40];
