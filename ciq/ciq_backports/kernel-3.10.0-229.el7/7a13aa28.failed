perf hists: Accumulate hist entry stat based on the callchain

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [tools] perf/hists: Accumulate hist entry stat based on the callchain (Jiri Olsa) [1134356]
Rebuild_FUZZ: 98.36%
commit-author Namhyung Kim <namhyung@kernel.org>
commit 7a13aa28aa268359cee006059731f49bcd1f839e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/7a13aa28.failed

Call __hists__add_entry() for each callchain node to get an
accumulated stat for an entry.  Introduce new cumulative_iter ops to
process them properly.

	Signed-off-by: Namhyung Kim <namhyung@kernel.org>
	Tested-by: Arun Sharma <asharma@fb.com>
	Tested-by: Rodrigo Campos <rodrigo@sdfg.com.ar>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/1401335910-16832-6-git-send-email-namhyung@kernel.org
	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
(cherry picked from commit 7a13aa28aa268359cee006059731f49bcd1f839e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/builtin-report.c
#	tools/perf/util/callchain.c
#	tools/perf/util/hist.c
#	tools/perf/util/hist.h
diff --cc tools/perf/builtin-report.c
index 899259cddfcd,e8fa9fea341f..000000000000
--- a/tools/perf/builtin-report.c
+++ b/tools/perf/builtin-report.c
@@@ -260,22 -111,24 +260,42 @@@ static int process_sample_event(struct 
  	if (rep->cpu_list && !test_bit(sample->cpu, rep->cpu_bitmap))
  		return 0;
  
++<<<<<<< HEAD
 +	if (sort__mode == SORT_MODE__BRANCH) {
 +		ret = report__add_branch_hist_entry(tool, &al, sample, evsel, machine);
 +		if (ret < 0)
 +			pr_debug("problem adding lbr entry, skipping event\n");
 +	} else if (rep->mem_mode == 1) {
 +		ret = report__add_mem_hist_entry(tool, &al, sample, evsel, machine, event);
 +		if (ret < 0)
 +			pr_debug("problem adding mem entry, skipping event\n");
 +	} else {
 +		if (al.map != NULL)
 +			al.map->dso->hit = 1;
++=======
+ 	if (sort__mode == SORT_MODE__BRANCH)
+ 		iter.ops = &hist_iter_branch;
+ 	else if (rep->mem_mode)
+ 		iter.ops = &hist_iter_mem;
+ 	else if (symbol_conf.cumulate_callchain)
+ 		iter.ops = &hist_iter_cumulative;
+ 	else
+ 		iter.ops = &hist_iter_normal;
+ 
+ 	if (al.map != NULL)
+ 		al.map->dso->hit = 1;
+ 
+ 	report__inc_stats(rep, NULL);
+ 
+ 	ret = hist_entry_iter__add(&iter, &al, evsel, sample, rep->max_stack);
+ 	if (ret < 0)
+ 		pr_debug("problem adding hist entry, skipping event\n");
++>>>>>>> 7a13aa28aa26 (perf hists: Accumulate hist entry stat based on the callchain)
  
 +		ret = report__add_hist_entry(tool, evsel, &al, sample, machine);
 +		if (ret < 0)
 +			pr_debug("problem incrementing symbol period, skipping event\n");
 +	}
  	return ret;
  }
  
diff --cc tools/perf/util/callchain.c
index b13682b75922,2af69c47b725..000000000000
--- a/tools/perf/util/callchain.c
+++ b/tools/perf/util/callchain.c
@@@ -606,3 -608,25 +606,28 @@@ int callchain_cursor_append(struct call
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ int sample__resolve_callchain(struct perf_sample *sample, struct symbol **parent,
+ 			      struct perf_evsel *evsel, struct addr_location *al,
+ 			      int max_stack)
+ {
+ 	if (sample->callchain == NULL)
+ 		return 0;
+ 
+ 	if (symbol_conf.use_callchain || symbol_conf.cumulate_callchain ||
+ 	    sort__has_parent) {
+ 		return machine__resolve_callchain(al->machine, evsel, al->thread,
+ 						  sample, parent, al, max_stack);
+ 	}
+ 	return 0;
+ }
+ 
+ int hist_entry__append_callchain(struct hist_entry *he, struct perf_sample *sample)
+ {
+ 	if (!symbol_conf.use_callchain)
+ 		return 0;
+ 	return callchain_append(he->callchain, &callchain_cursor, sample->period);
+ }
++>>>>>>> 7a13aa28aa26 (perf hists: Accumulate hist entry stat based on the callchain)
diff --cc tools/perf/util/hist.c
index 9820956c30b9,6079b5acfb6d..000000000000
--- a/tools/perf/util/hist.c
+++ b/tools/perf/util/hist.c
@@@ -438,7 -455,401 +438,405 @@@ struct hist_entry *__hists__add_entry(s
  		.transaction = transaction,
  	};
  
++<<<<<<< HEAD
 +	return add_hist_entry(hists, &entry, al);
++=======
+ 	return add_hist_entry(hists, &entry, al, sample_self);
+ }
+ 
+ static int
+ iter_next_nop_entry(struct hist_entry_iter *iter __maybe_unused,
+ 		    struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_next_nop_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_prepare_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_sample *sample = iter->sample;
+ 	struct mem_info *mi;
+ 
+ 	mi = sample__resolve_mem(sample, al);
+ 	if (mi == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->priv = mi;
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	u64 cost;
+ 	struct mem_info *mi = iter->priv;
+ 	struct hist_entry *he;
+ 
+ 	if (mi == NULL)
+ 		return -EINVAL;
+ 
+ 	cost = iter->sample->weight;
+ 	if (!cost)
+ 		cost = 1;
+ 
+ 	/*
+ 	 * must pass period=weight in order to get the correct
+ 	 * sorting from hists__collapse_resort() which is solely
+ 	 * based on periods. We want sorting be done on nr_events * weight
+ 	 * and this is indirectly achieved by passing period=weight here
+ 	 * and the he_stat__add_period() function.
+ 	 */
+ 	he = __hists__add_entry(&iter->evsel->hists, al, iter->parent, NULL, mi,
+ 				cost, cost, 0, true);
+ 	if (!he)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct hist_entry *he = iter->he;
+ 	struct mem_info *mx;
+ 	int err = -EINVAL;
+ 
+ 	if (he == NULL)
+ 		goto out;
+ 
+ 	if (ui__has_annotation()) {
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 		if (err)
+ 			goto out;
+ 
+ 		mx = he->mem_info;
+ 		err = addr_map_symbol__inc_samples(&mx->daddr, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	err = hist_entry__append_callchain(he, iter->sample);
+ 
+ out:
+ 	/*
+ 	 * We don't need to free iter->priv (mem_info) here since
+ 	 * the mem info was either already freed in add_hist_entry() or
+ 	 * passed to a new hist entry by hist_entry__new().
+ 	 */
+ 	iter->priv = NULL;
+ 
+ 	iter->he = NULL;
+ 	return err;
+ }
+ 
+ static int
+ iter_prepare_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi;
+ 	struct perf_sample *sample = iter->sample;
+ 
+ 	bi = sample__resolve_bstack(sample, al);
+ 	if (!bi)
+ 		return -ENOMEM;
+ 
+ 	iter->curr = 0;
+ 	iter->total = sample->branch_stack->nr;
+ 
+ 	iter->priv = bi;
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_branch_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			     struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_next_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi = iter->priv;
+ 	int i = iter->curr;
+ 
+ 	if (bi == NULL)
+ 		return 0;
+ 
+ 	if (iter->curr >= iter->total)
+ 		return 0;
+ 
+ 	al->map = bi[i].to.map;
+ 	al->sym = bi[i].to.sym;
+ 	al->addr = bi[i].to.addr;
+ 	return 1;
+ }
+ 
+ static int
+ iter_add_next_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi, *bx;
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct hist_entry *he = NULL;
+ 	int i = iter->curr;
+ 	int err = 0;
+ 
+ 	bi = iter->priv;
+ 
+ 	if (iter->hide_unresolved && !(bi[i].from.sym && bi[i].to.sym))
+ 		goto out;
+ 
+ 	/*
+ 	 * The report shows the percentage of total branches captured
+ 	 * and not events sampled. Thus we use a pseudo period of 1.
+ 	 */
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, &bi[i], NULL,
+ 				1, 1, 0, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	if (ui__has_annotation()) {
+ 		bx = he->branch_info;
+ 		err = addr_map_symbol__inc_samples(&bx->from, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 
+ 		err = addr_map_symbol__inc_samples(&bx->to, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ out:
+ 	iter->he = he;
+ 	iter->curr++;
+ 	return err;
+ }
+ 
+ static int
+ iter_finish_branch_entry(struct hist_entry_iter *iter,
+ 			 struct addr_location *al __maybe_unused)
+ {
+ 	zfree(&iter->priv);
+ 	iter->he = NULL;
+ 
+ 	return iter->curr >= iter->total ? 0 : -1;
+ }
+ 
+ static int
+ iter_prepare_normal_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			  struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_normal_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry *he;
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_normal_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	int err;
+ 	struct hist_entry *he = iter->he;
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 
+ 	if (he == NULL)
+ 		return 0;
+ 
+ 	iter->he = NULL;
+ 
+ 	if (ui__has_annotation()) {
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	return hist_entry__append_callchain(he, sample);
+ }
+ 
+ static int
+ iter_prepare_cumulative_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			      struct addr_location *al __maybe_unused)
+ {
+ 	callchain_cursor_commit(&callchain_cursor);
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_cumulative_entry(struct hist_entry_iter *iter,
+ 				 struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry *he;
+ 	int err = 0;
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 
+ 	/*
+ 	 * The iter->he will be over-written after ->add_next_entry()
+ 	 * called so inc stats for the original entry now.
+ 	 */
+ 	if (ui__has_annotation())
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	return err;
+ }
+ 
+ static int
+ iter_next_cumulative_entry(struct hist_entry_iter *iter,
+ 			   struct addr_location *al)
+ {
+ 	struct callchain_cursor_node *node;
+ 
+ 	node = callchain_cursor_current(&callchain_cursor);
+ 	if (node == NULL)
+ 		return 0;
+ 
+ 	al->map = node->map;
+ 	al->sym = node->sym;
+ 	if (node->map)
+ 		al->addr = node->map->map_ip(node->map, node->ip);
+ 	else
+ 		al->addr = node->ip;
+ 
+ 	if (iter->hide_unresolved && al->sym == NULL)
+ 		return 0;
+ 
+ 	callchain_cursor_advance(&callchain_cursor);
+ 	return 1;
+ }
+ 
+ static int
+ iter_add_next_cumulative_entry(struct hist_entry_iter *iter,
+ 			       struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry *he;
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, false);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_cumulative_entry(struct hist_entry_iter *iter,
+ 			     struct addr_location *al __maybe_unused)
+ {
+ 	iter->he = NULL;
+ 	return 0;
+ }
+ 
+ const struct hist_iter_ops hist_iter_mem = {
+ 	.prepare_entry 		= iter_prepare_mem_entry,
+ 	.add_single_entry 	= iter_add_single_mem_entry,
+ 	.next_entry 		= iter_next_nop_entry,
+ 	.add_next_entry 	= iter_add_next_nop_entry,
+ 	.finish_entry 		= iter_finish_mem_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_branch = {
+ 	.prepare_entry 		= iter_prepare_branch_entry,
+ 	.add_single_entry 	= iter_add_single_branch_entry,
+ 	.next_entry 		= iter_next_branch_entry,
+ 	.add_next_entry 	= iter_add_next_branch_entry,
+ 	.finish_entry 		= iter_finish_branch_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_normal = {
+ 	.prepare_entry 		= iter_prepare_normal_entry,
+ 	.add_single_entry 	= iter_add_single_normal_entry,
+ 	.next_entry 		= iter_next_nop_entry,
+ 	.add_next_entry 	= iter_add_next_nop_entry,
+ 	.finish_entry 		= iter_finish_normal_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_cumulative = {
+ 	.prepare_entry 		= iter_prepare_cumulative_entry,
+ 	.add_single_entry 	= iter_add_single_cumulative_entry,
+ 	.next_entry 		= iter_next_cumulative_entry,
+ 	.add_next_entry 	= iter_add_next_cumulative_entry,
+ 	.finish_entry 		= iter_finish_cumulative_entry,
+ };
+ 
+ int hist_entry_iter__add(struct hist_entry_iter *iter, struct addr_location *al,
+ 			 struct perf_evsel *evsel, struct perf_sample *sample,
+ 			 int max_stack_depth)
+ {
+ 	int err, err2;
+ 
+ 	err = sample__resolve_callchain(sample, &iter->parent, evsel, al,
+ 					max_stack_depth);
+ 	if (err)
+ 		return err;
+ 
+ 	iter->evsel = evsel;
+ 	iter->sample = sample;
+ 
+ 	err = iter->ops->prepare_entry(iter, al);
+ 	if (err)
+ 		goto out;
+ 
+ 	err = iter->ops->add_single_entry(iter, al);
+ 	if (err)
+ 		goto out;
+ 
+ 	while (iter->ops->next_entry(iter, al)) {
+ 		err = iter->ops->add_next_entry(iter, al);
+ 		if (err)
+ 			break;
+ 	}
+ 
+ out:
+ 	err2 = iter->ops->finish_entry(iter, al);
+ 	if (!err)
+ 		err = err2;
+ 
+ 	return err;
++>>>>>>> 7a13aa28aa26 (perf hists: Accumulate hist entry stat based on the callchain)
  }
  
  int64_t
diff --cc tools/perf/util/hist.h
index 4b860e6a25f0,78409f95d012..000000000000
--- a/tools/perf/util/hist.h
+++ b/tools/perf/util/hist.h
@@@ -95,7 -96,37 +95,41 @@@ struct hists 
  	u16			col_len[HISTC_NR_COLS];
  };
  
++<<<<<<< HEAD
 +struct hist_entry *__hists__add_entry(struct hists *self,
++=======
+ struct hist_entry_iter;
+ 
+ struct hist_iter_ops {
+ 	int (*prepare_entry)(struct hist_entry_iter *, struct addr_location *);
+ 	int (*add_single_entry)(struct hist_entry_iter *, struct addr_location *);
+ 	int (*next_entry)(struct hist_entry_iter *, struct addr_location *);
+ 	int (*add_next_entry)(struct hist_entry_iter *, struct addr_location *);
+ 	int (*finish_entry)(struct hist_entry_iter *, struct addr_location *);
+ };
+ 
+ struct hist_entry_iter {
+ 	int total;
+ 	int curr;
+ 
+ 	bool hide_unresolved;
+ 
+ 	struct perf_evsel *evsel;
+ 	struct perf_sample *sample;
+ 	struct hist_entry *he;
+ 	struct symbol *parent;
+ 	void *priv;
+ 
+ 	const struct hist_iter_ops *ops;
+ };
+ 
+ extern const struct hist_iter_ops hist_iter_normal;
+ extern const struct hist_iter_ops hist_iter_branch;
+ extern const struct hist_iter_ops hist_iter_mem;
+ extern const struct hist_iter_ops hist_iter_cumulative;
+ 
+ struct hist_entry *__hists__add_entry(struct hists *hists,
++>>>>>>> 7a13aa28aa26 (perf hists: Accumulate hist entry stat based on the callchain)
  				      struct addr_location *al,
  				      struct symbol *parent,
  				      struct branch_info *bi,
* Unmerged path tools/perf/builtin-report.c
* Unmerged path tools/perf/util/callchain.c
* Unmerged path tools/perf/util/hist.c
* Unmerged path tools/perf/util/hist.h
