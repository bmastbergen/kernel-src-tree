KVM: PPC: Use load_fp/vr_state rather than load_up_fpu/altivec

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: Use load_fp/vr_state rather than load_up_fpu/altivec (Don Zickus) [1127366]
Rebuild_FUZZ: 97.56%
commit-author Paul Mackerras <paulus@samba.org>
commit 09548fdaf32ce77a68e7f9a8a3098c1306b04858
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/09548fda.failed

The load_up_fpu and load_up_altivec functions were never intended to
be called from C, and do things like modifying the MSR value in their
callers' stack frames, which are assumed to be interrupt frames.  In
addition, on 32-bit Book S they require the MMU to be off.

This makes KVM use the new load_fp_state() and load_vr_state() functions
instead of load_up_fpu/altivec.  This means we can remove the assembler
glue in book3s_rmhandlers.S, and potentially fixes a bug on Book E,
where load_up_fpu was called directly from C.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit 09548fdaf32ce77a68e7f9a8a3098c1306b04858)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/switch_to.h
#	arch/powerpc/kvm/book3s_pr.c
diff --cc arch/powerpc/include/asm/switch_to.h
index 890358dc679e,971ca336f451..000000000000
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@@ -25,11 -25,8 +25,13 @@@ static inline void save_tar(struct thre
  static inline void save_tar(struct thread_struct *prev) {}
  #endif
  
++<<<<<<< HEAD
 +extern void giveup_fpu(struct task_struct *);
 +extern void load_up_fpu(void);
++=======
++>>>>>>> 09548fdaf32c (KVM: PPC: Use load_fp/vr_state rather than load_up_fpu/altivec)
  extern void enable_kernel_fp(void);
  extern void enable_kernel_altivec(void);
- extern void load_up_altivec(struct task_struct *);
  extern int emulate_altivec(struct pt_regs *);
  extern void __giveup_vsx(struct task_struct *);
  extern void giveup_vsx(struct task_struct *);
diff --cc arch/powerpc/kvm/book3s_pr.c
index d10e1186f935,d63a91f825d3..000000000000
--- a/arch/powerpc/kvm/book3s_pr.c
+++ b/arch/powerpc/kvm/book3s_pr.c
@@@ -716,10 -724,16 +718,23 @@@ static void kvmppc_handle_lost_ext(stru
  	if (!lost_ext)
  		return;
  
++<<<<<<< HEAD
 +	if (lost_ext & MSR_FP)
 +		kvmppc_load_up_fpu();
 +	if (lost_ext & MSR_VEC)
 +		kvmppc_load_up_altivec();
++=======
+ 	if (lost_ext & MSR_FP) {
+ 		enable_kernel_fp();
+ 		load_fp_state(&current->thread.fp_state);
+ 	}
+ #ifdef CONFIG_ALTIVEC
+ 	if (lost_ext & MSR_VEC) {
+ 		enable_kernel_altivec();
+ 		load_vr_state(&current->thread.vr_state);
+ 	}
+ #endif
++>>>>>>> 09548fdaf32c (KVM: PPC: Use load_fp/vr_state rather than load_up_fpu/altivec)
  	current->thread.regs->msr |= lost_ext;
  }
  
diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h
index 99ef8711e906..4ce9d3cbbb54 100644
--- a/arch/powerpc/include/asm/kvm_book3s.h
+++ b/arch/powerpc/include/asm/kvm_book3s.h
@@ -187,9 +187,6 @@ extern void kvmppc_update_lpcr(struct kvm *kvm, unsigned long lpcr,
 
 extern void kvmppc_entry_trampoline(void);
 extern void kvmppc_hv_entry_trampoline(void);
-extern void kvmppc_load_up_fpu(void);
-extern void kvmppc_load_up_altivec(void);
-extern void kvmppc_load_up_vsx(void);
 extern u32 kvmppc_alignment_dsisr(struct kvm_vcpu *vcpu, unsigned int inst);
 extern ulong kvmppc_alignment_dar(struct kvm_vcpu *vcpu, unsigned int inst);
 extern int kvmppc_h_pr(struct kvm_vcpu *vcpu, unsigned long cmd);
* Unmerged path arch/powerpc/include/asm/switch_to.h
diff --git a/arch/powerpc/kvm/book3s_exports.c b/arch/powerpc/kvm/book3s_exports.c
index 852989a9bad3..20d4ea8e656d 100644
--- a/arch/powerpc/kvm/book3s_exports.c
+++ b/arch/powerpc/kvm/book3s_exports.c
@@ -25,9 +25,5 @@ EXPORT_SYMBOL_GPL(kvmppc_hv_entry_trampoline);
 #endif
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 EXPORT_SYMBOL_GPL(kvmppc_entry_trampoline);
-EXPORT_SYMBOL_GPL(kvmppc_load_up_fpu);
-#ifdef CONFIG_ALTIVEC
-EXPORT_SYMBOL_GPL(kvmppc_load_up_altivec);
-#endif
 #endif
 
* Unmerged path arch/powerpc/kvm/book3s_pr.c
diff --git a/arch/powerpc/kvm/book3s_rmhandlers.S b/arch/powerpc/kvm/book3s_rmhandlers.S
index a38c4c9edab8..c78ffbc371a5 100644
--- a/arch/powerpc/kvm/book3s_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_rmhandlers.S
@@ -166,51 +166,4 @@ _GLOBAL(kvmppc_entry_trampoline)
 	mtsrr1	r6
 	RFI
 
-#if defined(CONFIG_PPC_BOOK3S_32)
-#define STACK_LR	INT_FRAME_SIZE+4
-
-/* load_up_xxx have to run with MSR_DR=0 on Book3S_32 */
-#define MSR_EXT_START						\
-	PPC_STL	r20, _NIP(r1);					\
-	mfmsr	r20;						\
-	LOAD_REG_IMMEDIATE(r3, MSR_DR|MSR_EE);			\
-	andc	r3,r20,r3;		/* Disable DR,EE */	\
-	mtmsr	r3;						\
-	sync
-
-#define MSR_EXT_END						\
-	mtmsr	r20;			/* Enable DR,EE */	\
-	sync;							\
-	PPC_LL	r20, _NIP(r1)
-
-#elif defined(CONFIG_PPC_BOOK3S_64)
-#define STACK_LR	_LINK
-#define MSR_EXT_START
-#define MSR_EXT_END
-#endif
-
-/*
- * Activate current's external feature (FPU/Altivec/VSX)
- */
-#define define_load_up(what) 					\
-								\
-_GLOBAL(kvmppc_load_up_ ## what);				\
-	PPC_STLU r1, -INT_FRAME_SIZE(r1);			\
-	mflr	r3;						\
-	PPC_STL	r3, STACK_LR(r1);				\
-	MSR_EXT_START;						\
-								\
-	bl	FUNC(load_up_ ## what);				\
-								\
-	MSR_EXT_END;						\
-	PPC_LL	r3, STACK_LR(r1);				\
-	mtlr	r3;						\
-	addi	r1, r1, INT_FRAME_SIZE;				\
-	blr
-
-define_load_up(fpu)
-#ifdef CONFIG_ALTIVEC
-define_load_up(altivec)
-#endif
-
 #include "book3s_segment.S"
diff --git a/arch/powerpc/kvm/booke.h b/arch/powerpc/kvm/booke.h
index 5fd1ba693579..7ffb69923132 100644
--- a/arch/powerpc/kvm/booke.h
+++ b/arch/powerpc/kvm/booke.h
@@ -112,7 +112,8 @@ static inline void kvmppc_load_guest_fp(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_PPC_FPU
 	if (vcpu->fpu_active && !(current->thread.regs->msr & MSR_FP)) {
-		load_up_fpu();
+		enable_kernel_fp();
+		load_fp_state(&current->thread.fp_state);
 		current->thread.regs->msr |= MSR_FP;
 	}
 #endif
