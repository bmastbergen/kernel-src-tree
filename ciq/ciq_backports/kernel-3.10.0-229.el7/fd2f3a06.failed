nfs: change nfs_page_group_lock argument

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Weston Andros Adamson <dros@primarydata.com>
commit fd2f3a06d30c85a17cf035ebc60c88c2a13a8ece
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/fd2f3a06.failed

Flip the meaning of the second argument from 'wait' to 'nonblock' to
match related functions. Update all five calls to reflect this change.

	Signed-off-by: Weston Andros Adamson <dros@primarydata.com>
	Reviewed-by: Peng Tao <tao.peng@primarydata.com>
	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit fd2f3a06d30c85a17cf035ebc60c88c2a13a8ece)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/nfs/pagelist.c
#	fs/nfs/write.c
diff --cc fs/nfs/pagelist.c
index 3e37cab99ceb,7efa61586bc4..000000000000
--- a/fs/nfs/pagelist.c
+++ b/fs/nfs/pagelist.c
@@@ -133,6 -136,165 +133,168 @@@ nfs_iocounter_wait(struct nfs_io_counte
  	return __nfs_iocounter_wait(c);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * nfs_page_group_lock - lock the head of the page group
+  * @req - request in group that is to be locked
+  * @nonblock - if true don't block waiting for lock
+  *
+  * this lock must be held if modifying the page group list
+  *
+  * returns result from wait_on_bit_lock: 0 on success, < 0 on error
+  */
+ int
+ nfs_page_group_lock(struct nfs_page *req, bool nonblock)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 	int ret;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	do {
+ 		ret = wait_on_bit_lock(&head->wb_flags, PG_HEADLOCK,
+ 			TASK_UNINTERRUPTIBLE);
+ 	} while (!nonblock && ret != 0);
+ 
+ 	WARN_ON_ONCE(ret > 0);
+ 	return ret;
+ }
+ 
+ /*
+  * nfs_page_group_unlock - unlock the head of the page group
+  * @req - request in group that is to be unlocked
+  */
+ void
+ nfs_page_group_unlock(struct nfs_page *req)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	smp_mb__before_atomic();
+ 	clear_bit(PG_HEADLOCK, &head->wb_flags);
+ 	smp_mb__after_atomic();
+ 	wake_up_bit(&head->wb_flags, PG_HEADLOCK);
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit_locked
+  *
+  * must be called with page group lock held
+  */
+ static bool
+ nfs_page_group_sync_on_bit_locked(struct nfs_page *req, unsigned int bit)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 	struct nfs_page *tmp;
+ 
+ 	WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &head->wb_flags));
+ 	WARN_ON_ONCE(test_and_set_bit(bit, &req->wb_flags));
+ 
+ 	tmp = req->wb_this_page;
+ 	while (tmp != req) {
+ 		if (!test_bit(bit, &tmp->wb_flags))
+ 			return false;
+ 		tmp = tmp->wb_this_page;
+ 	}
+ 
+ 	/* true! reset all bits */
+ 	tmp = req;
+ 	do {
+ 		clear_bit(bit, &tmp->wb_flags);
+ 		tmp = tmp->wb_this_page;
+ 	} while (tmp != req);
+ 
+ 	return true;
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit - set bit on current request, but only
+  *   return true if the bit is set for all requests in page group
+  * @req - request in page group
+  * @bit - PG_* bit that is used to sync page group
+  */
+ bool nfs_page_group_sync_on_bit(struct nfs_page *req, unsigned int bit)
+ {
+ 	bool ret;
+ 
+ 	nfs_page_group_lock(req, false);
+ 	ret = nfs_page_group_sync_on_bit_locked(req, bit);
+ 	nfs_page_group_unlock(req);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * nfs_page_group_init - Initialize the page group linkage for @req
+  * @req - a new nfs request
+  * @prev - the previous request in page group, or NULL if @req is the first
+  *         or only request in the group (the head).
+  */
+ static inline void
+ nfs_page_group_init(struct nfs_page *req, struct nfs_page *prev)
+ {
+ 	WARN_ON_ONCE(prev == req);
+ 
+ 	if (!prev) {
+ 		/* a head request */
+ 		req->wb_head = req;
+ 		req->wb_this_page = req;
+ 	} else {
+ 		/* a subrequest */
+ 		WARN_ON_ONCE(prev->wb_this_page != prev->wb_head);
+ 		WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &prev->wb_head->wb_flags));
+ 		req->wb_head = prev->wb_head;
+ 		req->wb_this_page = prev->wb_this_page;
+ 		prev->wb_this_page = req;
+ 
+ 		/* All subrequests take a ref on the head request until
+ 		 * nfs_page_group_destroy is called */
+ 		kref_get(&req->wb_head->wb_kref);
+ 
+ 		/* grab extra ref if head request has extra ref from
+ 		 * the write/commit path to handle handoff between write
+ 		 * and commit lists */
+ 		if (test_bit(PG_INODE_REF, &prev->wb_head->wb_flags)) {
+ 			set_bit(PG_INODE_REF, &req->wb_flags);
+ 			kref_get(&req->wb_kref);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * nfs_page_group_destroy - sync the destruction of page groups
+  * @req - request that no longer needs the page group
+  *
+  * releases the page group reference from each member once all
+  * members have called this function.
+  */
+ static void
+ nfs_page_group_destroy(struct kref *kref)
+ {
+ 	struct nfs_page *req = container_of(kref, struct nfs_page, wb_kref);
+ 	struct nfs_page *tmp, *next;
+ 
+ 	/* subrequests must release the ref on the head request */
+ 	if (req->wb_head != req)
+ 		nfs_release_request(req->wb_head);
+ 
+ 	if (!nfs_page_group_sync_on_bit(req, PG_TEARDOWN))
+ 		return;
+ 
+ 	tmp = req;
+ 	do {
+ 		next = tmp->wb_this_page;
+ 		/* unlink and free */
+ 		tmp->wb_this_page = tmp;
+ 		tmp->wb_head = tmp;
+ 		nfs_free_request(tmp);
+ 		tmp = next;
+ 	} while (tmp != req);
+ }
+ 
++>>>>>>> fd2f3a06d30c (nfs: change nfs_page_group_lock argument)
  /**
   * nfs_create_request - Create an NFS read/write request.
   * @ctx: open context to use
@@@ -441,16 -856,72 +603,80 @@@ static void nfs_pageio_doio(struct nfs_
  static int __nfs_pageio_add_request(struct nfs_pageio_descriptor *desc,
  			   struct nfs_page *req)
  {
++<<<<<<< HEAD
 +	while (!nfs_pageio_do_add_request(desc, req)) {
 +		desc->pg_moreio = 1;
 +		nfs_pageio_doio(desc);
 +		if (desc->pg_error < 0)
 +			return 0;
 +		desc->pg_moreio = 0;
 +		if (desc->pg_recoalesce)
 +			return 0;
 +	}
++=======
+ 	struct nfs_page *subreq;
+ 	unsigned int bytes_left = 0;
+ 	unsigned int offset, pgbase;
+ 	int ret;
+ 
+ 	ret = nfs_page_group_lock(req, true);
+ 	if (ret < 0) {
+ 		desc->pg_error = ret;
+ 		return 0;
+ 	}
+ 
+ 	subreq = req;
+ 	bytes_left = subreq->wb_bytes;
+ 	offset = subreq->wb_offset;
+ 	pgbase = subreq->wb_pgbase;
+ 
+ 	do {
+ 		if (!nfs_pageio_do_add_request(desc, subreq)) {
+ 			/* make sure pg_test call(s) did nothing */
+ 			WARN_ON_ONCE(subreq->wb_bytes != bytes_left);
+ 			WARN_ON_ONCE(subreq->wb_offset != offset);
+ 			WARN_ON_ONCE(subreq->wb_pgbase != pgbase);
+ 
+ 			nfs_page_group_unlock(req);
+ 			desc->pg_moreio = 1;
+ 			nfs_pageio_doio(desc);
+ 			if (desc->pg_error < 0)
+ 				return 0;
+ 			if (desc->pg_recoalesce)
+ 				return 0;
+ 			/* retry add_request for this subreq */
+ 			ret = nfs_page_group_lock(req, true);
+ 			if (ret < 0) {
+ 				desc->pg_error = ret;
+ 				return 0;
+ 			}
+ 			continue;
+ 		}
+ 
+ 		/* check for buggy pg_test call(s) */
+ 		WARN_ON_ONCE(subreq->wb_bytes + subreq->wb_pgbase > PAGE_SIZE);
+ 		WARN_ON_ONCE(subreq->wb_bytes > bytes_left);
+ 		WARN_ON_ONCE(subreq->wb_bytes == 0);
+ 
+ 		bytes_left -= subreq->wb_bytes;
+ 		offset += subreq->wb_bytes;
+ 		pgbase += subreq->wb_bytes;
+ 
+ 		if (bytes_left) {
+ 			subreq = nfs_create_request(req->wb_context,
+ 					req->wb_page,
+ 					subreq, pgbase, bytes_left);
+ 			if (IS_ERR(subreq))
+ 				goto err_ptr;
+ 			nfs_lock_request(subreq);
+ 			subreq->wb_offset  = offset;
+ 			subreq->wb_index = req->wb_index;
+ 		}
+ 	} while (bytes_left > 0);
+ 
+ 	nfs_page_group_unlock(req);
++>>>>>>> fd2f3a06d30c (nfs: change nfs_page_group_lock argument)
  	return 1;
 -err_ptr:
 -	desc->pg_error = PTR_ERR(subreq);
 -	nfs_page_group_unlock(req);
 -	return 0;
  }
  
  static int nfs_do_recoalesce(struct nfs_pageio_descriptor *desc)
diff --cc fs/nfs/write.c
index a89cdd72f163,27715306f24b..000000000000
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@@ -272,7 -241,7 +272,11 @@@ static bool nfs_page_group_covers_page(
  	unsigned int pos = 0;
  	unsigned int len = nfs_page_length(req->wb_page);
  
++<<<<<<< HEAD
 +	nfs_page_group_lock(req);
++=======
+ 	nfs_page_group_lock(req, false);
++>>>>>>> fd2f3a06d30c (nfs: change nfs_page_group_lock argument)
  
  	do {
  		tmp = nfs_page_group_search_locked(req->wb_head, pos);
@@@ -343,36 -315,242 +347,61 @@@ static void nfs_end_page_writeback(stru
  		clear_bdi_congested(&nfss->backing_dev_info, BLK_RW_ASYNC);
  }
  
 -
 -/* nfs_page_group_clear_bits
 - *   @req - an nfs request
 - * clears all page group related bits from @req
 - */
 -static void
 -nfs_page_group_clear_bits(struct nfs_page *req)
 -{
 -	clear_bit(PG_TEARDOWN, &req->wb_flags);
 -	clear_bit(PG_UNLOCKPAGE, &req->wb_flags);
 -	clear_bit(PG_UPTODATE, &req->wb_flags);
 -	clear_bit(PG_WB_END, &req->wb_flags);
 -	clear_bit(PG_REMOVE, &req->wb_flags);
 -}
 -
 -
 -/*
 - * nfs_unroll_locks_and_wait -  unlock all newly locked reqs and wait on @req
 - *
 - * this is a helper function for nfs_lock_and_join_requests
 - *
 - * @inode - inode associated with request page group, must be holding inode lock
 - * @head  - head request of page group, must be holding head lock
 - * @req   - request that couldn't lock and needs to wait on the req bit lock
 - * @nonblock - if true, don't actually wait
 - *
 - * NOTE: this must be called holding page_group bit lock and inode spin lock
 - *       and BOTH will be released before returning.
 - *
 - * returns 0 on success, < 0 on error.
 - */
 -static int
 -nfs_unroll_locks_and_wait(struct inode *inode, struct nfs_page *head,
 -			  struct nfs_page *req, bool nonblock)
 -	__releases(&inode->i_lock)
 -{
 -	struct nfs_page *tmp;
 -	int ret;
 -
 -	/* relinquish all the locks successfully grabbed this run */
 -	for (tmp = head ; tmp != req; tmp = tmp->wb_this_page)
 -		nfs_unlock_request(tmp);
 -
 -	WARN_ON_ONCE(test_bit(PG_TEARDOWN, &req->wb_flags));
 -
 -	/* grab a ref on the request that will be waited on */
 -	kref_get(&req->wb_kref);
 -
 -	nfs_page_group_unlock(head);
 -	spin_unlock(&inode->i_lock);
 -
 -	/* release ref from nfs_page_find_head_request_locked */
 -	nfs_release_request(head);
 -
 -	if (!nonblock)
 -		ret = nfs_wait_on_request(req);
 -	else
 -		ret = -EAGAIN;
 -	nfs_release_request(req);
 -
 -	return ret;
 -}
 -
 -/*
 - * nfs_destroy_unlinked_subrequests - destroy recently unlinked subrequests
 - *
 - * @destroy_list - request list (using wb_this_page) terminated by @old_head
 - * @old_head - the old head of the list
 - *
 - * All subrequests must be locked and removed from all lists, so at this point
 - * they are only "active" in this function, and possibly in nfs_wait_on_request
 - * with a reference held by some other context.
 - */
 -static void
 -nfs_destroy_unlinked_subrequests(struct nfs_page *destroy_list,
 -				 struct nfs_page *old_head)
 -{
 -	while (destroy_list) {
 -		struct nfs_page *subreq = destroy_list;
 -
 -		destroy_list = (subreq->wb_this_page == old_head) ?
 -				   NULL : subreq->wb_this_page;
 -
 -		WARN_ON_ONCE(old_head != subreq->wb_head);
 -
 -		/* make sure old group is not used */
 -		subreq->wb_head = subreq;
 -		subreq->wb_this_page = subreq;
 -
 -		/* subreq is now totally disconnected from page group or any
 -		 * write / commit lists. last chance to wake any waiters */
 -		nfs_unlock_request(subreq);
 -
 -		if (!test_bit(PG_TEARDOWN, &subreq->wb_flags)) {
 -			/* release ref on old head request */
 -			nfs_release_request(old_head);
 -
 -			nfs_page_group_clear_bits(subreq);
 -
 -			/* release the PG_INODE_REF reference */
 -			if (test_and_clear_bit(PG_INODE_REF, &subreq->wb_flags))
 -				nfs_release_request(subreq);
 -			else
 -				WARN_ON_ONCE(1);
 -		} else {
 -			WARN_ON_ONCE(test_bit(PG_CLEAN, &subreq->wb_flags));
 -			/* zombie requests have already released the last
 -			 * reference and were waiting on the rest of the
 -			 * group to complete. Since it's no longer part of a
 -			 * group, simply free the request */
 -			nfs_page_group_clear_bits(subreq);
 -			nfs_free_request(subreq);
 -		}
 -	}
 -}
 -
 -/*
 - * nfs_lock_and_join_requests - join all subreqs to the head req and return
 - *                              a locked reference, cancelling any pending
 - *                              operations for this page.
 - *
 - * @page - the page used to lookup the "page group" of nfs_page structures
 - * @nonblock - if true, don't block waiting for request locks
 - *
 - * This function joins all sub requests to the head request by first
 - * locking all requests in the group, cancelling any pending operations
 - * and finally updating the head request to cover the whole range covered by
 - * the (former) group.  All subrequests are removed from any write or commit
 - * lists, unlinked from the group and destroyed.
 - *
 - * Returns a locked, referenced pointer to the head request - which after
 - * this call is guaranteed to be the only request associated with the page.
 - * Returns NULL if no requests are found for @page, or a ERR_PTR if an
 - * error was encountered.
 - */
 -static struct nfs_page *
 -nfs_lock_and_join_requests(struct page *page, bool nonblock)
 +static struct nfs_page *nfs_find_and_lock_request(struct page *page, bool nonblock)
  {
  	struct inode *inode = page_file_mapping(page)->host;
 -	struct nfs_page *head, *subreq;
 -	struct nfs_page *destroy_list = NULL;
 -	unsigned int total_bytes;
 +	struct nfs_page *req;
  	int ret;
  
 -try_again:
 -	total_bytes = 0;
 -
 -	WARN_ON_ONCE(destroy_list);
 -
  	spin_lock(&inode->i_lock);
++<<<<<<< HEAD
 +	for (;;) {
 +		req = nfs_page_find_head_request_locked(NFS_I(inode), page);
 +		if (req == NULL)
 +			break;
 +		if (nfs_lock_request(req))
 +			break;
 +		/* Note: If we hold the page lock, as is the case in nfs_writepage,
 +		 *	 then the call to nfs_lock_request() will always
 +		 *	 succeed provided that someone hasn't already marked the
 +		 *	 request as dirty (in which case we don't care).
++=======
+ 
+ 	/*
+ 	 * A reference is taken only on the head request which acts as a
+ 	 * reference to the whole page group - the group will not be destroyed
+ 	 * until the head reference is released.
+ 	 */
+ 	head = nfs_page_find_head_request_locked(NFS_I(inode), page);
+ 
+ 	if (!head) {
+ 		spin_unlock(&inode->i_lock);
+ 		return NULL;
+ 	}
+ 
+ 	/* lock each request in the page group */
+ 	ret = nfs_page_group_lock(head, true);
+ 	if (ret < 0)
+ 		return ERR_PTR(ret);
+ 	subreq = head;
+ 	do {
+ 		/*
+ 		 * Subrequests are always contiguous, non overlapping
+ 		 * and in order. If not, it's a programming error.
++>>>>>>> fd2f3a06d30c (nfs: change nfs_page_group_lock argument)
  		 */
 -		WARN_ON_ONCE(subreq->wb_offset !=
 -		     (head->wb_offset + total_bytes));
 -
 -		/* keep track of how many bytes this group covers */
 -		total_bytes += subreq->wb_bytes;
 -
 -		if (!nfs_lock_request(subreq)) {
 -			/* releases page group bit lock and
 -			 * inode spin lock and all references */
 -			ret = nfs_unroll_locks_and_wait(inode, head,
 -				subreq, nonblock);
 -
 -			if (ret == 0)
 -				goto try_again;
 -
 +		spin_unlock(&inode->i_lock);
 +		if (!nonblock)
 +			ret = nfs_wait_on_request(req);
 +		else
 +			ret = -EAGAIN;
 +		nfs_release_request(req);
 +		if (ret != 0)
  			return ERR_PTR(ret);
 -		}
 -
 -		subreq = subreq->wb_this_page;
 -	} while (subreq != head);
 -
 -	/* Now that all requests are locked, make sure they aren't on any list.
 -	 * Commit list removal accounting is done after locks are dropped */
 -	subreq = head;
 -	do {
 -		nfs_clear_request_commit(subreq);
 -		subreq = subreq->wb_this_page;
 -	} while (subreq != head);
 -
 -	/* unlink subrequests from head, destroy them later */
 -	if (head->wb_this_page != head) {
 -		/* destroy list will be terminated by head */
 -		destroy_list = head->wb_this_page;
 -		head->wb_this_page = head;
 -
 -		/* change head request to cover whole range that
 -		 * the former page group covered */
 -		head->wb_bytes = total_bytes;
 +		spin_lock(&inode->i_lock);
  	}
 -
 -	/*
 -	 * prepare head request to be added to new pgio descriptor
 -	 */
 -	nfs_page_group_clear_bits(head);
 -
 -	/*
 -	 * some part of the group was still on the inode list - otherwise
 -	 * the group wouldn't be involved in async write.
 -	 * grab a reference for the head request, iff it needs one.
 -	 */
 -	if (!test_and_set_bit(PG_INODE_REF, &head->wb_flags))
 -		kref_get(&head->wb_kref);
 -
 -	nfs_page_group_unlock(head);
 -
 -	/* drop lock to clean uprequests on destroy list */
  	spin_unlock(&inode->i_lock);
 -
 -	nfs_destroy_unlinked_subrequests(destroy_list, head);
 -
 -	/* still holds ref on head from nfs_page_find_head_request_locked
 -	 * and still has lock on head from lock loop */
 -	return head;
 +	return req;
  }
  
  /*
* Unmerged path fs/nfs/pagelist.c
* Unmerged path fs/nfs/write.c
