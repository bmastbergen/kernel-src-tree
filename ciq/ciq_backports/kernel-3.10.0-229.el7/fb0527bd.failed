locking/mutexes: Introduce cancelable MCS lock for adaptive spinning

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit fb0527bd5ea99bfeb2dd91e3c1433ecf745d6b99
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/fb0527bd.failed

Since we want a task waiting for a mutex_lock() to go to sleep and
reschedule on need_resched() we must be able to abort the
mcs_spin_lock() around the adaptive spin.

Therefore implement a cancelable mcs lock.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: chegu_vinod@hp.com
	Cc: paulmck@linux.vnet.ibm.com
	Cc: Waiman.Long@hp.com
	Cc: torvalds@linux-foundation.org
	Cc: tglx@linutronix.de
	Cc: riel@redhat.com
	Cc: akpm@linux-foundation.org
	Cc: davidlohr@hp.com
	Cc: hpa@zytor.com
	Cc: andi@firstfloor.org
	Cc: aswin@hp.com
	Cc: scott.norton@hp.com
	Cc: Jason Low <jason.low2@hp.com>
Link: http://lkml.kernel.org/n/tip-62hcl5wxydmjzd182zhvk89m@git.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit fb0527bd5ea99bfeb2dd91e3c1433ecf745d6b99)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mutex.h
#	kernel/locking/Makefile
#	kernel/locking/mcs_spinlock.h
#	kernel/mcs_spinlock.c
#	kernel/mutex.c
diff --cc include/linux/mutex.h
index ccd4260834c5,11692dea18aa..000000000000
--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@@ -46,6 -46,7 +46,10 @@@
   * - detects multi-task circular deadlocks and prints out all affected
   *   locks and tasks (and only those tasks)
   */
++<<<<<<< HEAD
++=======
+ struct optimistic_spin_queue;
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning)
  struct mutex {
  	/* 1: unlocked, 0: locked, negative: locked, possible waiters */
  	atomic_t		count;
@@@ -55,7 -56,7 +59,11 @@@
  	struct task_struct	*owner;
  #endif
  #ifdef CONFIG_MUTEX_SPIN_ON_OWNER
++<<<<<<< HEAD
 +	void			*spin_mlock;	/* Spinner MCS lock */
++=======
+ 	struct optimistic_spin_queue	*osq;	/* Spinner MCS lock */
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning)
  #endif
  #ifdef CONFIG_DEBUG_MUTEXES
  	const char 		*name;
diff --cc kernel/mutex.c
index 9206c5597f6c,2670b84067d6..000000000000
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@@ -59,7 -53,7 +59,11 @@@ __mutex_init(struct mutex *lock, const 
  	INIT_LIST_HEAD(&lock->wait_list);
  	mutex_clear_owner(lock);
  #ifdef CONFIG_MUTEX_SPIN_ON_OWNER
++<<<<<<< HEAD:kernel/mutex.c
 +	lock->spin_mlock = NULL;
++=======
+ 	lock->osq = NULL;
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning):kernel/locking/mutex.c
  #endif
  
  	debug_mutex_init(lock, name, key);
@@@ -454,11 -403,13 +458,17 @@@ __mutex_lock_common(struct mutex *lock
  	if (!mutex_can_spin_on_owner(lock))
  		goto slowpath;
  
++<<<<<<< HEAD:kernel/mutex.c
++=======
+ 	if (!osq_lock(&lock->osq))
+ 		goto slowpath;
+ 
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning):kernel/locking/mutex.c
  	for (;;) {
  		struct task_struct *owner;
 +		struct mspin_node  node;
  
 -		if (use_ww_ctx && ww_ctx->acquired > 0) {
 +		if (!__builtin_constant_p(ww_ctx == NULL) && ww_ctx->acquired > 0) {
  			struct ww_mutex *ww;
  
  			ww = container_of(lock, struct ww_mutex, base);
@@@ -496,7 -444,7 +506,11 @@@
  			}
  
  			mutex_set_owner(lock);
++<<<<<<< HEAD:kernel/mutex.c
 +			mspin_unlock(MLOCK(lock), &node);
++=======
+ 			osq_unlock(&lock->osq);
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning):kernel/locking/mutex.c
  			preempt_enable();
  			return 0;
  		}
@@@ -519,6 -466,7 +533,10 @@@
  		 */
  		arch_mutex_cpu_relax();
  	}
++<<<<<<< HEAD:kernel/mutex.c
++=======
+ 	osq_unlock(&lock->osq);
++>>>>>>> fb0527bd5ea9 (locking/mutexes: Introduce cancelable MCS lock for adaptive spinning):kernel/locking/mutex.c
  slowpath:
  #endif
  	spin_lock_mutex(&lock->wait_lock, flags);
* Unmerged path kernel/locking/Makefile
* Unmerged path kernel/locking/mcs_spinlock.h
* Unmerged path kernel/mcs_spinlock.c
* Unmerged path include/linux/mutex.h
* Unmerged path kernel/locking/Makefile
* Unmerged path kernel/locking/mcs_spinlock.h
* Unmerged path kernel/mcs_spinlock.c
* Unmerged path kernel/mutex.c
