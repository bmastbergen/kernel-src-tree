perf tools: Move sys_perf_event_open function from perf.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [tools] perf: Move sys_perf_event_open function from perf.h (Jiri Olsa) [1134356]
Rebuild_FUZZ: 94.44%
commit-author Jiri Olsa <jolsa@kernel.org>
commit 82baa0eb46c15b749723d0c801470fea044657d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/82baa0eb.failed

Into perf-sys.h header, as requested by Peter:
  http://lkml.kernel.org/r/20140502115201.GI30445@twins.programming.kicks-ass.net

Adding HAVE_ATTR_TEST define to turn off/on the attribute
test code in the sys_perf_event_open function.

Requested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
Link: http://lkml.kernel.org/r/1399293219-8732-10-git-send-email-jolsa@kernel.org
	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
(cherry picked from commit 82baa0eb46c15b749723d0c801470fea044657d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/perf-sys.h
#	tools/perf/perf.h
diff --cc tools/perf/perf.h
index 6a5b509ecd88,813571b07246..000000000000
--- a/tools/perf/perf.h
+++ b/tools/perf/perf.h
@@@ -1,162 -1,21 +1,180 @@@
  #ifndef _PERF_PERF_H
  #define _PERF_PERF_H
  
++<<<<<<< HEAD
 +#include <asm/unistd.h>
 +
 +#if defined(__i386__)
 +#define mb()		asm volatile("lock; addl $0,0(%%esp)" ::: "memory")
 +#define wmb()		asm volatile("lock; addl $0,0(%%esp)" ::: "memory")
 +#define rmb()		asm volatile("lock; addl $0,0(%%esp)" ::: "memory")
 +#define cpu_relax()	asm volatile("rep; nop" ::: "memory");
 +#define CPUINFO_PROC	"model name"
 +#ifndef __NR_perf_event_open
 +# define __NR_perf_event_open 336
 +#endif
 +#endif
 +
 +#if defined(__x86_64__)
 +#define mb()		asm volatile("mfence" ::: "memory")
 +#define wmb()		asm volatile("sfence" ::: "memory")
 +#define rmb()		asm volatile("lfence" ::: "memory")
 +#define cpu_relax()	asm volatile("rep; nop" ::: "memory");
 +#define CPUINFO_PROC	"model name"
 +#ifndef __NR_perf_event_open
 +# define __NR_perf_event_open 298
 +#endif
 +#endif
 +
 +#ifdef __powerpc__
 +#include "../../arch/powerpc/include/uapi/asm/unistd.h"
 +#define mb()		asm volatile ("sync" ::: "memory")
 +#define wmb()		asm volatile ("sync" ::: "memory")
 +#define rmb()		asm volatile ("sync" ::: "memory")
 +#define CPUINFO_PROC	"cpu"
 +#endif
 +
 +#ifdef __s390__
 +#define mb()		asm volatile("bcr 15,0" ::: "memory")
 +#define wmb()		asm volatile("bcr 15,0" ::: "memory")
 +#define rmb()		asm volatile("bcr 15,0" ::: "memory")
 +#endif
 +
 +#ifdef __sh__
 +#if defined(__SH4A__) || defined(__SH5__)
 +# define mb()		asm volatile("synco" ::: "memory")
 +# define wmb()		asm volatile("synco" ::: "memory")
 +# define rmb()		asm volatile("synco" ::: "memory")
 +#else
 +# define mb()		asm volatile("" ::: "memory")
 +# define wmb()		asm volatile("" ::: "memory")
 +# define rmb()		asm volatile("" ::: "memory")
 +#endif
 +#define CPUINFO_PROC	"cpu type"
 +#endif
 +
 +#ifdef __hppa__
 +#define mb()		asm volatile("" ::: "memory")
 +#define wmb()		asm volatile("" ::: "memory")
 +#define rmb()		asm volatile("" ::: "memory")
 +#define CPUINFO_PROC	"cpu"
 +#endif
 +
 +#ifdef __sparc__
 +#ifdef __LP64__
 +#define mb()		asm volatile("ba,pt %%xcc, 1f\n"	\
 +				     "membar #StoreLoad\n"	\
 +				     "1:\n":::"memory")
 +#else
 +#define mb()		asm volatile("":::"memory")
 +#endif
 +#define wmb()		asm volatile("":::"memory")
 +#define rmb()		asm volatile("":::"memory")
 +#define CPUINFO_PROC	"cpu"
 +#endif
 +
 +#ifdef __alpha__
 +#define mb()		asm volatile("mb" ::: "memory")
 +#define wmb()		asm volatile("wmb" ::: "memory")
 +#define rmb()		asm volatile("mb" ::: "memory")
 +#define CPUINFO_PROC	"cpu model"
 +#endif
 +
 +#ifdef __ia64__
 +#define mb()		asm volatile ("mf" ::: "memory")
 +#define wmb()		asm volatile ("mf" ::: "memory")
 +#define rmb()		asm volatile ("mf" ::: "memory")
 +#define cpu_relax()	asm volatile ("hint @pause" ::: "memory")
 +#define CPUINFO_PROC	"model name"
 +#endif
 +
 +#ifdef __arm__
 +/*
 + * Use the __kuser_memory_barrier helper in the CPU helper page. See
 + * arch/arm/kernel/entry-armv.S in the kernel source for details.
 + */
 +#define mb()		((void(*)(void))0xffff0fa0)()
 +#define wmb()		((void(*)(void))0xffff0fa0)()
 +#define rmb()		((void(*)(void))0xffff0fa0)()
 +#define CPUINFO_PROC	"Processor"
 +#endif
 +
 +#ifdef __aarch64__
 +#define mb()		asm volatile("dmb ish" ::: "memory")
 +#define wmb()		asm volatile("dmb ishst" ::: "memory")
 +#define rmb()		asm volatile("dmb ishld" ::: "memory")
 +#define cpu_relax()	asm volatile("yield" ::: "memory")
 +#endif
 +
 +#ifdef __mips__
 +#define mb()		asm volatile(					\
 +				".set	mips2\n\t"			\
 +				"sync\n\t"				\
 +				".set	mips0"				\
 +				: /* no output */			\
 +				: /* no input */			\
 +				: "memory")
 +#define wmb()	mb()
 +#define rmb()	mb()
 +#define CPUINFO_PROC	"cpu model"
 +#endif
 +
 +#ifdef __arc__
 +#define mb()		asm volatile("" ::: "memory")
 +#define wmb()		asm volatile("" ::: "memory")
 +#define rmb()		asm volatile("" ::: "memory")
 +#define CPUINFO_PROC	"Processor"
 +#endif
 +
 +#ifdef __metag__
 +#define mb()		asm volatile("" ::: "memory")
 +#define wmb()		asm volatile("" ::: "memory")
 +#define rmb()		asm volatile("" ::: "memory")
 +#define CPUINFO_PROC	"CPU"
 +#endif
 +
 +#ifdef __xtensa__
 +#define mb()		asm volatile("memw" ::: "memory")
 +#define wmb()		asm volatile("memw" ::: "memory")
 +#define rmb()		asm volatile("" ::: "memory")
 +#define CPUINFO_PROC	"core ID"
 +#endif
 +
 +#define barrier() asm volatile ("" ::: "memory")
 +
 +#ifndef cpu_relax
 +#define cpu_relax() barrier()
 +#endif
 +
 +#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
 +
 +
 +#include <time.h>
 +#include <unistd.h>
 +#include <sys/types.h>
 +#include <sys/syscall.h>
 +
 +#include <linux/perf_event.h>
 +#include "util/types.h"
 +#include <stdbool.h>
 +
++=======
+ #include <time.h>
+ #include <stdbool.h>
+ #include <linux/types.h>
+ #include <linux/perf_event.h>
+ 
+ extern bool test_attr__enabled;
+ void test_attr__init(void);
+ void test_attr__open(struct perf_event_attr *attr, pid_t pid, int cpu,
+ 		     int fd, int group_fd, unsigned long flags);
+ 
+ #define HAVE_ATTR_TEST
+ #include "perf-sys.h"
+ 
+ #define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
+ 
++>>>>>>> 82baa0eb46c1 (perf tools: Move sys_perf_event_open function from perf.h)
  #ifndef NSEC_PER_SEC
  # define NSEC_PER_SEC			1000000000ULL
  #endif
@@@ -172,33 -31,6 +190,36 @@@ static inline unsigned long long rdcloc
  	return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
  }
  
++<<<<<<< HEAD
 +/*
 + * Pick up some kernel type conventions:
 + */
 +#define __user
 +#define asmlinkage
 +
 +extern bool test_attr__enabled;
 +void test_attr__init(void);
 +void test_attr__open(struct perf_event_attr *attr, pid_t pid, int cpu,
 +		     int fd, int group_fd, unsigned long flags);
 +
 +static inline int
 +sys_perf_event_open(struct perf_event_attr *attr,
 +		      pid_t pid, int cpu, int group_fd,
 +		      unsigned long flags)
 +{
 +	int fd;
 +
 +	fd = syscall(__NR_perf_event_open, attr, pid, cpu,
 +		     group_fd, flags);
 +
 +	if (unlikely(test_attr__enabled))
 +		test_attr__open(attr, pid, cpu, fd, group_fd, flags);
 +
 +	return fd;
 +}
 +
++=======
++>>>>>>> 82baa0eb46c1 (perf tools: Move sys_perf_event_open function from perf.h)
  #define MAX_NR_CPUS			256
  
  extern const char *input_name;
* Unmerged path tools/perf/perf-sys.h
* Unmerged path tools/perf/perf-sys.h
* Unmerged path tools/perf/perf.h
