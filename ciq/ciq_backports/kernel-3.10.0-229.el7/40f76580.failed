Btrfs: split up __extent_writepage to lower stack usage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Chris Mason <clm@fb.com>
commit 40f765805f082ed679c55bf6ab60212e55fb6fc1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/40f76580.failed

__extent_writepage has two unrelated parts.  First it does the delayed
allocation dance and second it does the mapping and IO for the page
we're actually writing.

This splits it up into those two parts so the stack from one doesn't
impact the stack from the other.

	Signed-off-by: Chris Mason <clm@fb.com>
(cherry picked from commit 40f765805f082ed679c55bf6ab60212e55fb6fc1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/extent_io.c
diff --cc fs/btrfs/extent_io.c
index 4394ec746416,0b5fa91d9a88..000000000000
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@@ -3131,119 -3219,12 +3225,114 @@@ static noinline_for_stack int __extent_
  	struct extent_state *cached_state = NULL;
  	struct extent_map *em;
  	struct block_device *bdev;
- 	int ret;
- 	int nr = 0;
  	size_t pg_offset = 0;
  	size_t blocksize;
- 	loff_t i_size = i_size_read(inode);
- 	unsigned long end_index = i_size >> PAGE_CACHE_SHIFT;
- 	u64 nr_delalloc;
- 	u64 delalloc_end;
- 	int page_started;
- 	int compressed;
- 	int write_flags;
- 	unsigned long nr_written = 0;
- 	bool fill_delalloc = true;
+ 	int ret = 0;
+ 	int nr = 0;
+ 	bool compressed;
  
++<<<<<<< HEAD
 +	if (wbc->sync_mode == WB_SYNC_ALL)
 +		write_flags = WRITE_SYNC;
 +	else
 +		write_flags = WRITE;
 +
 +	trace___extent_writepage(page, inode, wbc);
 +
 +	WARN_ON(!PageLocked(page));
 +
 +	ClearPageError(page);
 +
 +	pg_offset = i_size & (PAGE_CACHE_SIZE - 1);
 +	if (page->index > end_index ||
 +	   (page->index == end_index && !pg_offset)) {
 +		page->mapping->a_ops->invalidatepage(page, 0);
 +		unlock_page(page);
 +		return 0;
 +	}
 +
 +	if (page->index == end_index) {
 +		char *userpage;
 +
 +		userpage = kmap_atomic(page);
 +		memset(userpage + pg_offset, 0,
 +		       PAGE_CACHE_SIZE - pg_offset);
 +		kunmap_atomic(userpage);
 +		flush_dcache_page(page);
 +	}
 +	pg_offset = 0;
 +
 +	set_page_extent_mapped(page);
 +
 +	if (!tree->ops || !tree->ops->fill_delalloc)
 +		fill_delalloc = false;
 +
 +	delalloc_start = start;
 +	delalloc_end = 0;
 +	page_started = 0;
 +	if (!epd->extent_locked && fill_delalloc) {
 +		u64 delalloc_to_write = 0;
 +		/*
 +		 * make sure the wbc mapping index is at least updated
 +		 * to this page.
 +		 */
 +		update_nr_written(page, wbc, 0);
 +
 +		while (delalloc_end < page_end) {
 +			nr_delalloc = find_lock_delalloc_range(inode, tree,
 +						       page,
 +						       &delalloc_start,
 +						       &delalloc_end,
 +						       128 * 1024 * 1024);
 +			if (nr_delalloc == 0) {
 +				delalloc_start = delalloc_end + 1;
 +				continue;
 +			}
 +			ret = tree->ops->fill_delalloc(inode, page,
 +						       delalloc_start,
 +						       delalloc_end,
 +						       &page_started,
 +						       &nr_written);
 +			/* File system has been set read-only */
 +			if (ret) {
 +				SetPageError(page);
 +				goto done;
 +			}
 +			/*
 +			 * delalloc_end is already one less than the total
 +			 * length, so we don't subtract one from
 +			 * PAGE_CACHE_SIZE
 +			 */
 +			delalloc_to_write += (delalloc_end - delalloc_start +
 +					      PAGE_CACHE_SIZE) >>
 +					      PAGE_CACHE_SHIFT;
 +			delalloc_start = delalloc_end + 1;
 +		}
 +		if (wbc->nr_to_write < delalloc_to_write) {
 +			int thresh = 8192;
 +
 +			if (delalloc_to_write < thresh * 2)
 +				thresh = delalloc_to_write;
 +			wbc->nr_to_write = min_t(u64, delalloc_to_write,
 +						 thresh);
 +		}
 +
 +		/* did the fill delalloc function already unlock and start
 +		 * the IO?
 +		 */
 +		if (page_started) {
 +			ret = 0;
 +			/*
 +			 * we've unlocked the page, so we can't update
 +			 * the mapping's writeback index, just update
 +			 * nr_to_write.
 +			 */
 +			wbc->nr_to_write -= nr_written;
 +			goto done_unlocked;
 +		}
 +	}
++=======
++>>>>>>> 40f765805f08 (Btrfs: split up __extent_writepage to lower stack usage)
  	if (tree->ops && tree->ops->writepage_start_hook) {
  		ret = tree->ops->writepage_start_hook(page, start,
  						      page_end);
* Unmerged path fs/btrfs/extent_io.c
diff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c
index df702f3cf8c8..3353319a7963 100644
--- a/fs/btrfs/inode.c
+++ b/fs/btrfs/inode.c
@@ -125,7 +125,7 @@ static int btrfs_init_inode_security(struct btrfs_trans_handle *trans,
  * the btree.  The caller should have done a btrfs_drop_extents so that
  * no overlapping inline items exist in the btree
  */
-static noinline int insert_inline_extent(struct btrfs_trans_handle *trans,
+static int insert_inline_extent(struct btrfs_trans_handle *trans,
 				struct btrfs_path *path, int extent_inserted,
 				struct btrfs_root *root, struct inode *inode,
 				u64 start, size_t size, size_t compressed_size,
