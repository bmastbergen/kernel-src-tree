powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [powerpc] Implement arch_spin_is_locked() using arch_spin_value_unlocked() (Don Zickus) [1127366]
Rebuild_FUZZ: 93.43%
commit-author Michael Ellerman <mpe@ellerman.id.au>
commit 7179ba52889bef7e5e23f72908270e1ab2b7fc6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/7179ba52.failed

At a glance these are just the inverse of each other. The one subtlety
is that arch_spin_value_unlocked() takes the lock by value, rather than
as a pointer, which is important for the lockref code.

On the other hand arch_spin_is_locked() doesn't really care, so
implement it in terms of arch_spin_value_unlocked().

	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
	Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
(cherry picked from commit 7179ba52889bef7e5e23f72908270e1ab2b7fc6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/spinlock.h
diff --cc arch/powerpc/include/asm/spinlock.h
index f6e78d63fb6a,a30ef6999d66..000000000000
--- a/arch/powerpc/include/asm/spinlock.h
+++ b/arch/powerpc/include/asm/spinlock.h
@@@ -28,10 -28,6 +28,13 @@@
  #include <asm/synch.h>
  #include <asm/ppc-opcode.h>
  
++<<<<<<< HEAD
 +#define smp_mb__after_unlock_lock()	smp_mb()  /* Full ordering for lock. */
 +
 +#define arch_spin_is_locked(x)		((x)->slock != 0)
 +
++=======
++>>>>>>> 7179ba52889b (powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked())
  #ifdef CONFIG_PPC64
  /* use 0x800000yy when locked, where yy == CPU number */
  #ifdef __BIG_ENDIAN__
@@@ -56,6 -52,16 +59,19 @@@
  #define SYNC_IO
  #endif
  
++<<<<<<< HEAD
++=======
+ static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)
+ {
+ 	return lock.slock == 0;
+ }
+ 
+ static inline int arch_spin_is_locked(arch_spinlock_t *lock)
+ {
+ 	return !arch_spin_value_unlocked(*lock);
+ }
+ 
++>>>>>>> 7179ba52889b (powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked())
  /*
   * This returns the old value in the lock, so we succeeded
   * in getting the lock if the return value is 0.
* Unmerged path arch/powerpc/include/asm/spinlock.h
