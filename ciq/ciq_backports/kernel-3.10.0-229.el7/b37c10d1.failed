powerpc: Fix ABIv2 issues with stack offsets in assembly code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [powerpc] Fix ABIv2 issues with stack offsets in assembly code (Don Zickus) [1127366]
Rebuild_FUZZ: 92.04%
commit-author Anton Blanchard <anton@samba.org>
commit b37c10d128a2fa3256d4e67c184177270eac4b86
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/b37c10d1.failed

Fix STK_PARAM and use it instead of hardcoding ABIv1 offsets.

	Signed-off-by: Anton Blanchard <anton@samba.org>
(cherry picked from commit b37c10d128a2fa3256d4e67c184177270eac4b86)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/lib/memcpy_power7.S
diff --cc arch/powerpc/lib/memcpy_power7.S
index e4177dbea6bd,87d8eeccd4b7..000000000000
--- a/arch/powerpc/lib/memcpy_power7.S
+++ b/arch/powerpc/lib/memcpy_power7.S
@@@ -226,16 -226,16 +226,16 @@@ _GLOBAL(memcpy_power7
  #ifdef CONFIG_ALTIVEC
  .Lvmx_copy:
  	mflr	r0
- 	std	r4,56(r1)
- 	std	r5,64(r1)
+ 	std	r4,STK_PARAM(R4)(r1)
+ 	std	r5,STK_PARAM(R5)(r1)
  	std	r0,16(r1)
  	stdu	r1,-STACKFRAMESIZE(r1)
 -	bl	enter_vmx_copy
 +	bl	.enter_vmx_copy
  	cmpwi	cr1,r3,0
  	ld	r0,STACKFRAMESIZE+16(r1)
- 	ld	r3,STACKFRAMESIZE+48(r1)
- 	ld	r4,STACKFRAMESIZE+56(r1)
- 	ld	r5,STACKFRAMESIZE+64(r1)
+ 	ld	r3,STACKFRAMESIZE+STK_PARAM(R3)(r1)
+ 	ld	r4,STACKFRAMESIZE+STK_PARAM(R4)(r1)
+ 	ld	r5,STACKFRAMESIZE+STK_PARAM(R5)(r1)
  	mtlr	r0
  
  	/*
@@@ -447,8 -447,8 +447,13 @@@
  	stb	r0,0(r3)
  
  15:	addi	r1,r1,STACKFRAMESIZE
++<<<<<<< HEAD
 +	ld	r3,48(r1)
 +	b	.exit_vmx_copy		/* tail call optimise */
++=======
+ 	ld	r3,STK_PARAM(R3)(r1)
+ 	b	exit_vmx_copy		/* tail call optimise */
++>>>>>>> b37c10d128a2 (powerpc: Fix ABIv2 issues with stack offsets in assembly code)
  
  .Lvmx_unaligned_copy:
  	/* Get the destination 16B aligned */
@@@ -651,6 -651,6 +656,11 @@@
  	stb	r0,0(r3)
  
  15:	addi	r1,r1,STACKFRAMESIZE
++<<<<<<< HEAD
 +	ld	r3,48(r1)
 +	b	.exit_vmx_copy		/* tail call optimise */
++=======
+ 	ld	r3,STK_PARAM(R3)(r1)
+ 	b	exit_vmx_copy		/* tail call optimise */
++>>>>>>> b37c10d128a2 (powerpc: Fix ABIv2 issues with stack offsets in assembly code)
  #endif /* CONFiG_ALTIVEC */
diff --git a/arch/powerpc/include/asm/ppc_asm.h b/arch/powerpc/include/asm/ppc_asm.h
index f627f8c74b2a..1df56a0ed913 100644
--- a/arch/powerpc/include/asm/ppc_asm.h
+++ b/arch/powerpc/include/asm/ppc_asm.h
@@ -204,7 +204,11 @@ END_FW_FTR_SECTION_IFSET(FW_FEATURE_SPLPAR)
 #define __STK_REG(i)   (112 + ((i)-14)*8)
 #define STK_REG(i)     __STK_REG(__REG_##i)
 
+#if defined(_CALL_ELF) && _CALL_ELF == 2
+#define __STK_PARAM(i)	(32 + ((i)-3)*8)
+#else
 #define __STK_PARAM(i)	(48 + ((i)-3)*8)
+#endif
 #define STK_PARAM(i)	__STK_PARAM(__REG_##i)
 
 #define XGLUE(a,b) a##b
diff --git a/arch/powerpc/lib/copypage_power7.S b/arch/powerpc/lib/copypage_power7.S
index 395c594722a2..074dd2230b2f 100644
--- a/arch/powerpc/lib/copypage_power7.S
+++ b/arch/powerpc/lib/copypage_power7.S
@@ -56,15 +56,15 @@ _GLOBAL(copypage_power7)
 
 #ifdef CONFIG_ALTIVEC
 	mflr	r0
-	std	r3,48(r1)
-	std	r4,56(r1)
+	std	r3,STK_PARAM(R3)(r1)
+	std	r4,STK_PARAM(R4)(r1)
 	std	r0,16(r1)
 	stdu	r1,-STACKFRAMESIZE(r1)
 	bl	.enter_vmx_copy
 	cmpwi	r3,0
 	ld	r0,STACKFRAMESIZE+16(r1)
-	ld	r3,STACKFRAMESIZE+48(r1)
-	ld	r4,STACKFRAMESIZE+56(r1)
+	ld	r3,STACKFRAMESIZE+STK_PARAM(R3)(r1)
+	ld	r4,STACKFRAMESIZE+STK_PARAM(R4)(r1)
 	mtlr	r0
 
 	li	r0,(PAGE_SIZE/128)
diff --git a/arch/powerpc/lib/copyuser_power7.S b/arch/powerpc/lib/copyuser_power7.S
index e8e9c36dc784..e6540e169732 100644
--- a/arch/powerpc/lib/copyuser_power7.S
+++ b/arch/powerpc/lib/copyuser_power7.S
@@ -85,9 +85,9 @@
 .Lexit:
 	addi	r1,r1,STACKFRAMESIZE
 .Ldo_err1:
-	ld	r3,48(r1)
-	ld	r4,56(r1)
-	ld	r5,64(r1)
+	ld	r3,STK_PARAM(R3)(r1)
+	ld	r4,STK_PARAM(R4)(r1)
+	ld	r5,STK_PARAM(R5)(r1)
 	b	__copy_tofrom_user_base
 
 
@@ -96,18 +96,18 @@ _GLOBAL(__copy_tofrom_user_power7)
 	cmpldi	r5,16
 	cmpldi	cr1,r5,4096
 
-	std	r3,48(r1)
-	std	r4,56(r1)
-	std	r5,64(r1)
+	std	r3,STK_PARAM(R3)(r1)
+	std	r4,STK_PARAM(R4)(r1)
+	std	r5,STK_PARAM(R5)(r1)
 
 	blt	.Lshort_copy
 	bgt	cr1,.Lvmx_copy
 #else
 	cmpldi	r5,16
 
-	std	r3,48(r1)
-	std	r4,56(r1)
-	std	r5,64(r1)
+	std	r3,STK_PARAM(R3)(r1)
+	std	r4,STK_PARAM(R4)(r1)
+	std	r5,STK_PARAM(R5)(r1)
 
 	blt	.Lshort_copy
 #endif
@@ -298,9 +298,9 @@ err1;	stb	r0,0(r3)
 	bl	.enter_vmx_usercopy
 	cmpwi	cr1,r3,0
 	ld	r0,STACKFRAMESIZE+16(r1)
-	ld	r3,STACKFRAMESIZE+48(r1)
-	ld	r4,STACKFRAMESIZE+56(r1)
-	ld	r5,STACKFRAMESIZE+64(r1)
+	ld	r3,STACKFRAMESIZE+STK_PARAM(R3)(r1)
+	ld	r4,STACKFRAMESIZE+STK_PARAM(R4)(r1)
+	ld	r5,STACKFRAMESIZE+STK_PARAM(R5)(r1)
 	mtlr	r0
 
 	/*
diff --git a/arch/powerpc/lib/memcpy_64.S b/arch/powerpc/lib/memcpy_64.S
index d2bbbc8d7dc0..a0e276c320ec 100644
--- a/arch/powerpc/lib/memcpy_64.S
+++ b/arch/powerpc/lib/memcpy_64.S
@@ -12,7 +12,7 @@
 	.align	7
 _GLOBAL(memcpy)
 BEGIN_FTR_SECTION
-	std	r3,48(r1)	/* save destination pointer for return value */
+	std	r3,STK_PARAM(R3)(r1)	/* save destination pointer for return value */
 FTR_SECTION_ELSE
 	b	memcpy_power7
 ALT_FTR_SECTION_END_IFCLR(CPU_FTR_VMX_COPY)
@@ -71,7 +71,7 @@ END_FTR_SECTION_IFCLR(CPU_FTR_UNALIGNED_LD_STD)
 2:	bf	cr7*4+3,3f
 	lbz	r9,8(r4)
 	stb	r9,0(r3)
-3:	ld	r3,48(r1)	/* return dest pointer */
+3:	ld	r3,STK_PARAM(R3)(r1)	/* return dest pointer */
 	blr
 
 .Lsrc_unaligned:
@@ -154,7 +154,7 @@ END_FTR_SECTION_IFCLR(CPU_FTR_UNALIGNED_LD_STD)
 2:	bf	cr7*4+3,3f
 	rotldi	r9,r9,8
 	stb	r9,0(r3)
-3:	ld	r3,48(r1)	/* return dest pointer */
+3:	ld	r3,STK_PARAM(R3)(r1)	/* return dest pointer */
 	blr
 
 .Ldst_unaligned:
@@ -199,5 +199,5 @@ END_FTR_SECTION_IFCLR(CPU_FTR_UNALIGNED_LD_STD)
 3:	bf	cr7*4+3,4f
 	lbz	r0,0(r4)
 	stb	r0,0(r3)
-4:	ld	r3,48(r1)	/* return dest pointer */
+4:	ld	r3,STK_PARAM(R3)(r1)	/* return dest pointer */
 	blr
* Unmerged path arch/powerpc/lib/memcpy_power7.S
