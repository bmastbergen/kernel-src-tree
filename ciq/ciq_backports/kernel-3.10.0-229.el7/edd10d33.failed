NVMe: Retry failed commands with non-fatal errors

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [block] nvme: Retry failed commands with non-fatal error (David Milburn) [1081734]
Rebuild_FUZZ: 98.97%
commit-author Keith Busch <keith.busch@intel.com>
commit edd10d33283899fb15d99a290dcc9ceb3604ca78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/edd10d33.failed

For commands returned with failed status, queue these for resubmission
and continue retrying them until success or for a limited amount of
time. The final timeout was arbitrarily chosen so requests can't be
retried indefinitely.

Since these are requeued on the nvmeq that submitted the command, the
callbacks have to take an nvmeq instead of an nvme_dev as a parameter
so that we can use the locked queue to append the iod to retry later.

The nvme_iod conviently can be used to track how long we've been trying
to successfully complete an iod request. The nvme_iod also provides the
nvme prp dma mappings, so I had to move a few things around so we can
keep those mappings.

	Signed-off-by: Keith Busch <keith.busch@intel.com>
[fixed checkpatch issue with long line]
	Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
(cherry picked from commit edd10d33283899fb15d99a290dcc9ceb3604ca78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index d27372cd3793,efa9c8f4a7a7..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -477,104 -518,20 +490,110 @@@ int nvme_setup_prps(struct nvme_dev *de
  	return total_len;
  }
  
 +struct nvme_bio_pair {
 +	struct bio b1, b2, *parent;
 +	struct bio_vec *bv1, *bv2;
 +	int err;
 +	atomic_t cnt;
 +};
 +
 +static void nvme_bio_pair_endio(struct bio *bio, int err)
 +{
 +	struct nvme_bio_pair *bp = bio->bi_private;
 +
 +	if (err)
 +		bp->err = err;
 +
 +	if (atomic_dec_and_test(&bp->cnt)) {
 +		bio_endio(bp->parent, bp->err);
 +		kfree(bp->bv1);
 +		kfree(bp->bv2);
 +		kfree(bp);
 +	}
 +}
 +
 +static struct nvme_bio_pair *nvme_bio_split(struct bio *bio, int idx,
 +							int len, int offset)
 +{
 +	struct nvme_bio_pair *bp;
 +
 +	BUG_ON(len > bio->bi_size);
 +	BUG_ON(idx > bio->bi_vcnt);
 +
 +	bp = kmalloc(sizeof(*bp), GFP_ATOMIC);
 +	if (!bp)
 +		return NULL;
 +	bp->err = 0;
 +
 +	bp->b1 = *bio;
 +	bp->b2 = *bio;
 +
 +	bp->b1.bi_size = len;
 +	bp->b2.bi_size -= len;
 +	bp->b1.bi_vcnt = idx;
 +	bp->b2.bi_idx = idx;
 +	bp->b2.bi_sector += len >> 9;
 +
 +	if (offset) {
 +		bp->bv1 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
 +								GFP_ATOMIC);
 +		if (!bp->bv1)
 +			goto split_fail_1;
 +
 +		bp->bv2 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
 +								GFP_ATOMIC);
 +		if (!bp->bv2)
 +			goto split_fail_2;
 +
 +		memcpy(bp->bv1, bio->bi_io_vec,
 +			bio->bi_max_vecs * sizeof(struct bio_vec));
 +		memcpy(bp->bv2, bio->bi_io_vec,
 +			bio->bi_max_vecs * sizeof(struct bio_vec));
 +
 +		bp->b1.bi_io_vec = bp->bv1;
 +		bp->b2.bi_io_vec = bp->bv2;
 +		bp->b2.bi_io_vec[idx].bv_offset += offset;
 +		bp->b2.bi_io_vec[idx].bv_len -= offset;
 +		bp->b1.bi_io_vec[idx].bv_len = offset;
 +		bp->b1.bi_vcnt++;
 +	} else
 +		bp->bv1 = bp->bv2 = NULL;
 +
 +	bp->b1.bi_private = bp;
 +	bp->b2.bi_private = bp;
 +
 +	bp->b1.bi_end_io = nvme_bio_pair_endio;
 +	bp->b2.bi_end_io = nvme_bio_pair_endio;
 +
 +	bp->parent = bio;
 +	atomic_set(&bp->cnt, 2);
 +
 +	return bp;
 +
 + split_fail_2:
 +	kfree(bp->bv1);
 + split_fail_1:
 +	kfree(bp);
 +	return NULL;
 +}
 +
  static int nvme_split_and_submit(struct bio *bio, struct nvme_queue *nvmeq,
 -				 int len)
 +						int idx, int len, int offset)
  {
 -	struct bio *split = bio_split(bio, len >> 9, GFP_ATOMIC, NULL);
 -	if (!split)
 +	struct nvme_bio_pair *bp = nvme_bio_split(bio, idx, len, offset);
 +	if (!bp)
  		return -ENOMEM;
  
- 	if (bio_list_empty(&nvmeq->sq_cong))
 -	bio_chain(split, bio);
 -
+ 	if (!waitqueue_active(&nvmeq->sq_full))
  		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
++<<<<<<< HEAD
 +	bio_list_add(&nvmeq->sq_cong, &bp->b1);
 +	bio_list_add(&nvmeq->sq_cong, &bp->b2);
++=======
+ 	bio_list_add(&nvmeq->sq_cong, split);
+ 	bio_list_add(&nvmeq->sq_cong, bio);
+ 	wake_up(&nvmeq->sq_full);
++>>>>>>> edd10d332838 (NVMe: Retry failed commands with non-fatal errors)
  
  	return 0;
  }
@@@ -627,25 -587,13 +646,13 @@@ static int nvme_map_bio(struct nvme_que
  static int nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
  		struct bio *bio, struct nvme_iod *iod, int cmdid)
  {
- 	struct nvme_dsm_range *range;
+ 	struct nvme_dsm_range *range =
+ 				(struct nvme_dsm_range *)iod_list(iod)[0];
  	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
  
- 	range = dma_pool_alloc(nvmeq->dev->prp_small_pool, GFP_ATOMIC,
- 							&iod->first_dma);
- 	if (!range)
- 		return -ENOMEM;
- 
- 	iod_list(iod)[0] = (__le64 *)range;
- 	iod->npages = 0;
- 
  	range->cattr = cpu_to_le32(0);
 -	range->nlb = cpu_to_le32(bio->bi_iter.bi_size >> ns->lba_shift);
 -	range->slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_iter.bi_sector));
 +	range->nlb = cpu_to_le32(bio->bi_size >> ns->lba_shift);
 +	range->slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
  
  	memset(cmnd, 0, sizeof(*cmnd));
  	cmnd->dsm.opcode = nvme_cmd_dsm;
@@@ -689,44 -637,22 +696,38 @@@ int nvme_submit_flush_data(struct nvme_
  	return nvme_submit_flush(nvmeq, ns, cmdid);
  }
  
- /*
-  * Called with local interrupts disabled and the q_lock held.  May not sleep.
-  */
- static int nvme_submit_bio_queue(struct nvme_queue *nvmeq, struct nvme_ns *ns,
- 								struct bio *bio)
+ static int nvme_submit_iod(struct nvme_queue *nvmeq, struct nvme_iod *iod)
  {
+ 	struct bio *bio = iod->private;
+ 	struct nvme_ns *ns = bio->bi_bdev->bd_disk->private_data;
  	struct nvme_command *cmnd;
- 	struct nvme_iod *iod;
- 	enum dma_data_direction dma_dir;
- 	int cmdid, length, result;
+ 	int cmdid;
  	u16 control;
  	u32 dsmgmt;
- 	int psegs = bio_phys_segments(ns->queue, bio);
  
++<<<<<<< HEAD
 +	if ((bio->bi_rw & REQ_FLUSH) && psegs) {
 +		result = nvme_submit_flush_data(nvmeq, ns);
 +		if (result)
 +			return result;
 +	}
 +
 +	result = -ENOMEM;
 +	iod = nvme_alloc_iod(psegs, bio->bi_size, GFP_ATOMIC);
 +	if (!iod)
 +		goto nomem;
 +	iod->private = bio;
 +
 +	result = -EBUSY;
++=======
++>>>>>>> edd10d332838 (NVMe: Retry failed commands with non-fatal errors)
  	cmdid = alloc_cmdid(nvmeq, iod, bio_completion, NVME_IO_TIMEOUT);
  	if (unlikely(cmdid < 0))
- 		goto free_iod;
+ 		return cmdid;
  
- 	if (bio->bi_rw & REQ_DISCARD) {
- 		result = nvme_submit_discard(nvmeq, ns, bio, iod, cmdid);
- 		if (result)
- 			goto free_cmdid;
- 		return result;
- 	}
- 	if ((bio->bi_rw & REQ_FLUSH) && !psegs)
+ 	if (bio->bi_rw & REQ_DISCARD)
+ 		return nvme_submit_discard(nvmeq, ns, bio, iod, cmdid);
+ 	if ((bio->bi_rw & REQ_FLUSH) && !iod->nents)
  		return nvme_submit_flush(nvmeq, ns, cmdid);
  
  	control = 0;
@@@ -740,27 -666,16 +741,23 @@@
  		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
  
  	cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
- 
  	memset(cmnd, 0, sizeof(*cmnd));
- 	if (bio_data_dir(bio)) {
- 		cmnd->rw.opcode = nvme_cmd_write;
- 		dma_dir = DMA_TO_DEVICE;
- 	} else {
- 		cmnd->rw.opcode = nvme_cmd_read;
- 		dma_dir = DMA_FROM_DEVICE;
- 	}
- 
- 	result = nvme_map_bio(nvmeq, iod, bio, dma_dir, psegs);
- 	if (result <= 0)
- 		goto free_cmdid;
- 	length = result;
  
+ 	cmnd->rw.opcode = bio_data_dir(bio) ? nvme_cmd_write : nvme_cmd_read;
  	cmnd->rw.command_id = cmdid;
  	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
++<<<<<<< HEAD
 +	length = nvme_setup_prps(nvmeq->dev, &cmnd->common, iod, length,
 +								GFP_ATOMIC);
 +	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
 +	cmnd->rw.length = cpu_to_le16((length >> ns->lba_shift) - 1);
++=======
+ 	cmnd->rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+ 	cmnd->rw.prp2 = cpu_to_le64(iod->first_dma);
+ 	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_iter.bi_sector));
+ 	cmnd->rw.length =
+ 		cpu_to_le16((bio->bi_iter.bi_size >> ns->lba_shift) - 1);
++>>>>>>> edd10d332838 (NVMe: Retry failed commands with non-fatal errors)
  	cmnd->rw.control = cpu_to_le16(control);
  	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
  
@@@ -1579,22 -1600,14 +1642,24 @@@ static int nvme_submit_io(struct nvme_n
  		c.rw.metadata = cpu_to_le64(meta_dma_addr);
  	}
  
- 	length = nvme_setup_prps(dev, &c.common, iod, length, GFP_KERNEL);
+ 	length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
+ 	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+ 	c.rw.prp2 = cpu_to_le64(iod->first_dma);
  
 +	nvmeq = get_nvmeq(dev);
 +	/*
 +	 * Since nvme_submit_sync_cmd sleeps, we can't keep preemption
 +	 * disabled.  We may be preempted at any point, and be rescheduled
 +	 * to a different CPU.  That will cause cacheline bouncing, but no
 +	 * additional races since q_lock already protects against other CPUs.
 +	 */
 +	put_nvmeq(nvmeq);
  	if (length != (io.nblocks + 1) << ns->lba_shift)
  		status = -ENOMEM;
 +	else if (!nvmeq || nvmeq->q_suspended)
 +		status = -EBUSY;
  	else
 -		status = nvme_submit_io_cmd(dev, &c, NULL);
 +		status = nvme_submit_sync_cmd(nvmeq, &c, NULL, NVME_IO_TIMEOUT);
  
  	if (meta_len) {
  		if (status == NVME_SC_SUCCESS && !(io.opcode & 1)) {
* Unmerged path drivers/block/nvme-core.c
diff --git a/drivers/block/nvme-scsi.c b/drivers/block/nvme-scsi.c
index b0bd260ce145..2e94037a8677 100644
--- a/drivers/block/nvme-scsi.c
+++ b/drivers/block/nvme-scsi.c
@@ -1562,13 +1562,14 @@ static int nvme_trans_send_fw_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 			res = PTR_ERR(iod);
 			goto out;
 		}
-		length = nvme_setup_prps(dev, &c.common, iod, tot_len,
-								GFP_KERNEL);
+		length = nvme_setup_prps(dev, iod, tot_len, GFP_KERNEL);
 		if (length != tot_len) {
 			res = -ENOMEM;
 			goto out_unmap;
 		}
 
+		c.dlfw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+		c.dlfw.prp2 = cpu_to_le64(iod->first_dma);
 		c.dlfw.numd = cpu_to_le32((tot_len/BYTES_TO_DWORDS) - 1);
 		c.dlfw.offset = cpu_to_le32(offset/BYTES_TO_DWORDS);
 	} else if (opcode == nvme_admin_activate_fw) {
@@ -2093,8 +2094,7 @@ static int nvme_trans_do_nvme_io(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 			res = PTR_ERR(iod);
 			goto out;
 		}
-		retcode = nvme_setup_prps(dev, &c.common, iod, unit_len,
-							GFP_KERNEL);
+		retcode = nvme_setup_prps(dev, iod, unit_len, GFP_KERNEL);
 		if (retcode != unit_len) {
 			nvme_unmap_user_pages(dev,
 				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
@@ -2103,6 +2103,8 @@ static int nvme_trans_do_nvme_io(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 			res = -ENOMEM;
 			goto out;
 		}
+		c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+		c.rw.prp2 = cpu_to_le64(iod->first_dma);
 
 		nvme_offset += unit_num_blocks;
 
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index fe6bef4219c4..1760aedf6a7f 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -132,6 +132,7 @@ struct nvme_iod {
 	int length;		/* Of data, in bytes */
 	unsigned long start_time;
 	dma_addr_t first_dma;
+	struct list_head node;
 	struct scatterlist sg[0];
 };
 
@@ -147,8 +148,7 @@ static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
  */
 void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod);
 
-int nvme_setup_prps(struct nvme_dev *dev, struct nvme_common_command *cmd,
-			struct nvme_iod *iod, int total_len, gfp_t gfp);
+int nvme_setup_prps(struct nvme_dev *, struct nvme_iod *, int , gfp_t);
 struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 				unsigned long addr, unsigned length);
 void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
diff --git a/include/uapi/linux/nvme.h b/include/uapi/linux/nvme.h
index f009c15eb385..a10a3a4e4f97 100644
--- a/include/uapi/linux/nvme.h
+++ b/include/uapi/linux/nvme.h
@@ -434,6 +434,7 @@ enum {
 	NVME_SC_REFTAG_CHECK		= 0x284,
 	NVME_SC_COMPARE_FAILED		= 0x285,
 	NVME_SC_ACCESS_DENIED		= 0x286,
+	NVME_SC_DNR			= 0x4000,
 };
 
 struct nvme_completion {
