kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s - Allow the HV and PR selection per virtual machine (Don Zickus) [1127366]
Rebuild_FUZZ: 92.75%
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
commit cbbc58d4fdfab1a39a6ac1b41fcb17885952157a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/cbbc58d4.failed

This moves the kvmppc_ops callbacks to be a per VM entity. This
enables us to select HV and PR mode when creating a VM. We also
allow both kvm-hv and kvm-pr kernel module to be loaded. To
achieve this we move /dev/kvm ownership to kvm.ko module. Depending on
which KVM mode we select during VM creation we take a reference
count on respective module

	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
[agraf: fix coding style]
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit cbbc58d4fdfab1a39a6ac1b41fcb17885952157a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/kvm_ppc.h
#	arch/powerpc/kvm/44x.c
#	arch/powerpc/kvm/book3s.c
#	arch/powerpc/kvm/book3s.h
#	arch/powerpc/kvm/book3s_hv.c
#	arch/powerpc/kvm/book3s_pr.c
#	arch/powerpc/kvm/book3s_xics.c
#	arch/powerpc/kvm/booke.c
#	arch/powerpc/kvm/e500.c
#	arch/powerpc/kvm/e500mc.c
#	arch/powerpc/kvm/emulate.c
#	arch/powerpc/kvm/powerpc.c
diff --cc arch/powerpc/include/asm/kvm_ppc.h
index 1823f38906c6,3069cf4dcc88..000000000000
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@@ -177,6 -170,68 +177,71 @@@ extern int kvmppc_xics_get_xive(struct 
  extern int kvmppc_xics_int_on(struct kvm *kvm, u32 irq);
  extern int kvmppc_xics_int_off(struct kvm *kvm, u32 irq);
  
++<<<<<<< HEAD
++=======
+ union kvmppc_one_reg {
+ 	u32	wval;
+ 	u64	dval;
+ 	vector128 vval;
+ 	u64	vsxval[2];
+ 	struct {
+ 		u64	addr;
+ 		u64	length;
+ 	}	vpaval;
+ };
+ 
+ struct kvmppc_ops {
+ 	struct module *owner;
+ 	bool is_hv_enabled;
+ 	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+ 	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+ 	int (*get_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+ 			   union kvmppc_one_reg *val);
+ 	int (*set_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+ 			   union kvmppc_one_reg *val);
+ 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
+ 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
+ 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
+ 	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+ 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned int id);
+ 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+ 	int (*check_requests)(struct kvm_vcpu *vcpu);
+ 	int (*get_dirty_log)(struct kvm *kvm, struct kvm_dirty_log *log);
+ 	void (*flush_memslot)(struct kvm *kvm, struct kvm_memory_slot *memslot);
+ 	int (*prepare_memory_region)(struct kvm *kvm,
+ 				     struct kvm_memory_slot *memslot,
+ 				     struct kvm_userspace_memory_region *mem);
+ 	void (*commit_memory_region)(struct kvm *kvm,
+ 				     struct kvm_userspace_memory_region *mem,
+ 				     const struct kvm_memory_slot *old);
+ 	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
+ 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
+ 			   unsigned long end);
+ 	int (*age_hva)(struct kvm *kvm, unsigned long hva);
+ 	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
+ 	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
+ 	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
+ 	void (*free_memslot)(struct kvm_memory_slot *free,
+ 			     struct kvm_memory_slot *dont);
+ 	int (*create_memslot)(struct kvm_memory_slot *slot,
+ 			      unsigned long npages);
+ 	int (*init_vm)(struct kvm *kvm);
+ 	void (*destroy_vm)(struct kvm *kvm);
+ 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);
+ 	int (*emulate_op)(struct kvm_run *run, struct kvm_vcpu *vcpu,
+ 			  unsigned int inst, int *advance);
+ 	int (*emulate_mtspr)(struct kvm_vcpu *vcpu, int sprn, ulong spr_val);
+ 	int (*emulate_mfspr)(struct kvm_vcpu *vcpu, int sprn, ulong *spr_val);
+ 	void (*fast_vcpu_kick)(struct kvm_vcpu *vcpu);
+ 	long (*arch_vm_ioctl)(struct file *filp, unsigned int ioctl,
+ 			      unsigned long arg);
+ 
+ };
+ 
+ extern struct kvmppc_ops *kvmppc_hv_ops;
+ extern struct kvmppc_ops *kvmppc_pr_ops;
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  /*
   * Cuts out inst bits with ordering according to spec.
   * That means the leftmost bit is zero. All given bits are included.
@@@ -281,7 -325,10 +346,14 @@@ static inline void kvmppc_set_host_ipi(
  	paca[cpu].kvm_hstate.host_ipi = host_ipi;
  }
  
++<<<<<<< HEAD
 +extern void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu);
++=======
+ static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->fast_vcpu_kick(vcpu);
+ }
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  
  #else
  static inline void __init kvm_cma_reserve(void)
diff --cc arch/powerpc/kvm/44x.c
index 2f5c6b6d6877,93221e87b911..000000000000
--- a/arch/powerpc/kvm/44x.c
+++ b/arch/powerpc/kvm/44x.c
@@@ -191,9 -211,16 +191,20 @@@ static int __init kvmppc_44x_init(void
  
  	r = kvmppc_booke_init();
  	if (r)
 -		goto err_out;
 +		return r;
  
++<<<<<<< HEAD
 +	return kvm_init(NULL, sizeof(struct kvmppc_vcpu_44x), 0, THIS_MODULE);
++=======
+ 	r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_44x), 0, THIS_MODULE);
+ 	if (r)
+ 		goto err_out;
+ 	kvm_ops_44x.owner = THIS_MODULE;
+ 	kvmppc_pr_ops = &kvm_ops_44x;
+ 
+ err_out:
+ 	return r;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  static void __exit kvmppc_44x_exit(void)
diff --cc arch/powerpc/kvm/book3s.c
index 807103ad2628,ad8f6ed3f136..000000000000
--- a/arch/powerpc/kvm/book3s.c
+++ b/arch/powerpc/kvm/book3s.c
@@@ -69,6 -70,50 +70,53 @@@ void kvmppc_core_load_guest_debugstate(
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long kvmppc_interrupt_offset(struct kvm_vcpu *vcpu)
+ {
+ 	if (!vcpu->kvm->arch.kvm_ops->is_hv_enabled)
+ 		return to_book3s(vcpu)->hior;
+ 	return 0;
+ }
+ 
+ static inline void kvmppc_update_int_pending(struct kvm_vcpu *vcpu,
+ 			unsigned long pending_now, unsigned long old_pending)
+ {
+ 	if (vcpu->kvm->arch.kvm_ops->is_hv_enabled)
+ 		return;
+ 	if (pending_now)
+ 		vcpu->arch.shared->int_pending = 1;
+ 	else if (old_pending)
+ 		vcpu->arch.shared->int_pending = 0;
+ }
+ 
+ static inline bool kvmppc_critical_section(struct kvm_vcpu *vcpu)
+ {
+ 	ulong crit_raw;
+ 	ulong crit_r1;
+ 	bool crit;
+ 
+ 	if (vcpu->kvm->arch.kvm_ops->is_hv_enabled)
+ 		return false;
+ 
+ 	crit_raw = vcpu->arch.shared->critical;
+ 	crit_r1 = kvmppc_get_gpr(vcpu, 1);
+ 
+ 	/* Truncate crit indicators in 32 bit mode */
+ 	if (!(vcpu->arch.shared->msr & MSR_SF)) {
+ 		crit_raw &= 0xffffffff;
+ 		crit_r1 &= 0xffffffff;
+ 	}
+ 
+ 	/* Critical section when crit == r1 */
+ 	crit = (crit_raw == crit_r1);
+ 	/* ... and we're in supervisor mode */
+ 	crit = crit && !(vcpu->arch.shared->msr & MSR_PR);
+ 
+ 	return crit;
+ }
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  void kvmppc_inject_interrupt(struct kvm_vcpu *vcpu, int vec, u64 flags)
  {
  	vcpu->arch.shared->srr0 = kvmppc_get_pc(vcpu);
@@@ -422,6 -475,18 +470,21 @@@ void kvmppc_subarch_vcpu_uninit(struct 
  {
  }
  
++<<<<<<< HEAD
++=======
+ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
+ 				  struct kvm_sregs *sregs)
+ {
+ 	return vcpu->kvm->arch.kvm_ops->get_sregs(vcpu, sregs);
+ }
+ 
+ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
+ 				  struct kvm_sregs *sregs)
+ {
+ 	return vcpu->kvm->arch.kvm_ops->set_sregs(vcpu, sregs);
+ }
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
  {
  	int i;
@@@ -498,8 -563,7 +561,12 @@@ int kvm_vcpu_ioctl_get_one_reg(struct k
  	if (size > sizeof(val))
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	r = kvmppc_get_one_reg(vcpu, reg->id, &val);
 +
++=======
+ 	r = vcpu->kvm->arch.kvm_ops->get_one_reg(vcpu, reg->id, &val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  	if (r == -EINVAL) {
  		r = 0;
  		switch (reg->id) {
@@@ -578,8 -642,7 +645,12 @@@ int kvm_vcpu_ioctl_set_one_reg(struct k
  	if (copy_from_user(&val, (char __user *)(unsigned long)reg->addr, size))
  		return -EFAULT;
  
++<<<<<<< HEAD
 +	r = kvmppc_set_one_reg(vcpu, reg->id, &val);
 +
++=======
+ 	r = vcpu->kvm->arch.kvm_ops->set_one_reg(vcpu, reg->id, &val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  	if (r == -EINVAL) {
  		r = 0;
  		switch (reg->id) {
@@@ -638,6 -701,27 +709,30 @@@
  	return r;
  }
  
++<<<<<<< HEAD
++=======
+ void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_load(vcpu, cpu);
+ }
+ 
+ void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_put(vcpu);
+ }
+ 
+ void kvmppc_set_msr(struct kvm_vcpu *vcpu, u64 msr)
+ {
+ 	vcpu->kvm->arch.kvm_ops->set_msr(vcpu, msr);
+ }
+ EXPORT_SYMBOL_GPL(kvmppc_set_msr);
+ 
+ int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)
+ {
+ 	return vcpu->kvm->arch.kvm_ops->vcpu_run(kvm_run, vcpu);
+ }
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
                                    struct kvm_translation *tr)
  {
@@@ -657,3 -741,141 +752,144 @@@ void kvmppc_decrementer_func(unsigned l
  	kvmppc_core_queue_dec(vcpu);
  	kvm_vcpu_kick(vcpu);
  }
++<<<<<<< HEAD
++=======
+ 
+ struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)
+ {
+ 	return kvm->arch.kvm_ops->vcpu_create(kvm, id);
+ }
+ 
+ void kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);
+ }
+ 
+ int kvmppc_core_check_requests(struct kvm_vcpu *vcpu)
+ {
+ 	return vcpu->kvm->arch.kvm_ops->check_requests(vcpu);
+ }
+ 
+ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
+ {
+ 	return kvm->arch.kvm_ops->get_dirty_log(kvm, log);
+ }
+ 
+ void kvmppc_core_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
+ 			      struct kvm_memory_slot *dont)
+ {
+ 	kvm->arch.kvm_ops->free_memslot(free, dont);
+ }
+ 
+ int kvmppc_core_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
+ 			       unsigned long npages)
+ {
+ 	return kvm->arch.kvm_ops->create_memslot(slot, npages);
+ }
+ 
+ void kvmppc_core_flush_memslot(struct kvm *kvm, struct kvm_memory_slot *memslot)
+ {
+ 	kvm->arch.kvm_ops->flush_memslot(kvm, memslot);
+ }
+ 
+ int kvmppc_core_prepare_memory_region(struct kvm *kvm,
+ 				struct kvm_memory_slot *memslot,
+ 				struct kvm_userspace_memory_region *mem)
+ {
+ 	return kvm->arch.kvm_ops->prepare_memory_region(kvm, memslot, mem);
+ }
+ 
+ void kvmppc_core_commit_memory_region(struct kvm *kvm,
+ 				struct kvm_userspace_memory_region *mem,
+ 				const struct kvm_memory_slot *old)
+ {
+ 	kvm->arch.kvm_ops->commit_memory_region(kvm, mem, old);
+ }
+ 
+ int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)
+ {
+ 	return kvm->arch.kvm_ops->unmap_hva(kvm, hva);
+ }
+ EXPORT_SYMBOL_GPL(kvm_unmap_hva);
+ 
+ int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)
+ {
+ 	return kvm->arch.kvm_ops->unmap_hva_range(kvm, start, end);
+ }
+ 
+ int kvm_age_hva(struct kvm *kvm, unsigned long hva)
+ {
+ 	return kvm->arch.kvm_ops->age_hva(kvm, hva);
+ }
+ 
+ int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
+ {
+ 	return kvm->arch.kvm_ops->test_age_hva(kvm, hva);
+ }
+ 
+ void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)
+ {
+ 	kvm->arch.kvm_ops->set_spte_hva(kvm, hva, pte);
+ }
+ 
+ void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->mmu_destroy(vcpu);
+ }
+ 
+ int kvmppc_core_init_vm(struct kvm *kvm)
+ {
+ 
+ #ifdef CONFIG_PPC64
+ 	INIT_LIST_HEAD(&kvm->arch.spapr_tce_tables);
+ 	INIT_LIST_HEAD(&kvm->arch.rtas_tokens);
+ #endif
+ 
+ 	return kvm->arch.kvm_ops->init_vm(kvm);
+ }
+ 
+ void kvmppc_core_destroy_vm(struct kvm *kvm)
+ {
+ 	kvm->arch.kvm_ops->destroy_vm(kvm);
+ 
+ #ifdef CONFIG_PPC64
+ 	kvmppc_rtas_tokens_free(kvm);
+ 	WARN_ON(!list_empty(&kvm->arch.spapr_tce_tables));
+ #endif
+ }
+ 
+ int kvmppc_core_check_processor_compat(void)
+ {
+ 	/*
+ 	 * We always return 0 for book3s. We check
+ 	 * for compatability while loading the HV
+ 	 * or PR module
+ 	 */
+ 	return 0;
+ }
+ 
+ static int kvmppc_book3s_init(void)
+ {
+ 	int r;
+ 
+ 	r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ 	if (r)
+ 		return r;
+ #ifdef CONFIG_KVM_BOOK3S_32
+ 	r = kvmppc_book3s_init_pr();
+ #endif
+ 	return r;
+ 
+ }
+ 
+ static void kvmppc_book3s_exit(void)
+ {
+ #ifdef CONFIG_KVM_BOOK3S_32
+ 	kvmppc_book3s_exit_pr();
+ #endif
+ 	kvm_exit();
+ }
+ 
+ module_init(kvmppc_book3s_init);
+ module_exit(kvmppc_book3s_exit);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
diff --cc arch/powerpc/kvm/book3s_hv.c
index b327916c88e9,8743048881b7..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -2087,24 -2100,122 +2087,131 @@@ int kvmppc_core_emulate_mfspr(struct kv
  	return EMULATE_FAIL;
  }
  
++<<<<<<< HEAD
 +static int kvmppc_book3s_hv_init(void)
 +{
 +	int r;
 +
 +	r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 +
 +	if (r)
++=======
+ static int kvmppc_core_check_processor_compat_hv(void)
+ {
+ 	if (!cpu_has_feature(CPU_FTR_HVMODE))
+ 		return -EIO;
+ 	return 0;
+ }
+ 
+ static long kvm_arch_vm_ioctl_hv(struct file *filp,
+ 				 unsigned int ioctl, unsigned long arg)
+ {
+ 	struct kvm *kvm __maybe_unused = filp->private_data;
+ 	void __user *argp = (void __user *)arg;
+ 	long r;
+ 
+ 	switch (ioctl) {
+ 
+ 	case KVM_ALLOCATE_RMA: {
+ 		struct kvm_allocate_rma rma;
+ 		struct kvm *kvm = filp->private_data;
+ 
+ 		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
+ 		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
+ 			r = -EFAULT;
+ 		break;
+ 	}
+ 
+ 	case KVM_PPC_ALLOCATE_HTAB: {
+ 		u32 htab_order;
+ 
+ 		r = -EFAULT;
+ 		if (get_user(htab_order, (u32 __user *)argp))
+ 			break;
+ 		r = kvmppc_alloc_reset_hpt(kvm, &htab_order);
+ 		if (r)
+ 			break;
+ 		r = -EFAULT;
+ 		if (put_user(htab_order, (u32 __user *)argp))
+ 			break;
+ 		r = 0;
+ 		break;
+ 	}
+ 
+ 	case KVM_PPC_GET_HTAB_FD: {
+ 		struct kvm_get_htab_fd ghf;
+ 
+ 		r = -EFAULT;
+ 		if (copy_from_user(&ghf, argp, sizeof(ghf)))
+ 			break;
+ 		r = kvm_vm_ioctl_get_htab_fd(kvm, &ghf);
+ 		break;
+ 	}
+ 
+ 	default:
+ 		r = -ENOTTY;
+ 	}
+ 
+ 	return r;
+ }
+ 
+ static struct kvmppc_ops kvm_ops_hv = {
+ 	.is_hv_enabled = true,
+ 	.get_sregs = kvm_arch_vcpu_ioctl_get_sregs_hv,
+ 	.set_sregs = kvm_arch_vcpu_ioctl_set_sregs_hv,
+ 	.get_one_reg = kvmppc_get_one_reg_hv,
+ 	.set_one_reg = kvmppc_set_one_reg_hv,
+ 	.vcpu_load   = kvmppc_core_vcpu_load_hv,
+ 	.vcpu_put    = kvmppc_core_vcpu_put_hv,
+ 	.set_msr     = kvmppc_set_msr_hv,
+ 	.vcpu_run    = kvmppc_vcpu_run_hv,
+ 	.vcpu_create = kvmppc_core_vcpu_create_hv,
+ 	.vcpu_free   = kvmppc_core_vcpu_free_hv,
+ 	.check_requests = kvmppc_core_check_requests_hv,
+ 	.get_dirty_log  = kvm_vm_ioctl_get_dirty_log_hv,
+ 	.flush_memslot  = kvmppc_core_flush_memslot_hv,
+ 	.prepare_memory_region = kvmppc_core_prepare_memory_region_hv,
+ 	.commit_memory_region  = kvmppc_core_commit_memory_region_hv,
+ 	.unmap_hva = kvm_unmap_hva_hv,
+ 	.unmap_hva_range = kvm_unmap_hva_range_hv,
+ 	.age_hva  = kvm_age_hva_hv,
+ 	.test_age_hva = kvm_test_age_hva_hv,
+ 	.set_spte_hva = kvm_set_spte_hva_hv,
+ 	.mmu_destroy  = kvmppc_mmu_destroy_hv,
+ 	.free_memslot = kvmppc_core_free_memslot_hv,
+ 	.create_memslot = kvmppc_core_create_memslot_hv,
+ 	.init_vm =  kvmppc_core_init_vm_hv,
+ 	.destroy_vm = kvmppc_core_destroy_vm_hv,
+ 	.get_smmu_info = kvm_vm_ioctl_get_smmu_info_hv,
+ 	.emulate_op = kvmppc_core_emulate_op_hv,
+ 	.emulate_mtspr = kvmppc_core_emulate_mtspr_hv,
+ 	.emulate_mfspr = kvmppc_core_emulate_mfspr_hv,
+ 	.fast_vcpu_kick = kvmppc_fast_vcpu_kick_hv,
+ 	.arch_vm_ioctl  = kvm_arch_vm_ioctl_hv,
+ };
+ 
+ static int kvmppc_book3s_init_hv(void)
+ {
+ 	int r;
+ 	/*
+ 	 * FIXME!! Do we need to check on all cpus ?
+ 	 */
+ 	r = kvmppc_core_check_processor_compat_hv();
+ 	if (r < 0)
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		return r;
  
- 	r = kvmppc_mmu_hv_init();
+ 	kvm_ops_hv.owner = THIS_MODULE;
+ 	kvmppc_hv_ops = &kvm_ops_hv;
  
+ 	r = kvmppc_mmu_hv_init();
  	return r;
  }
  
 -static void kvmppc_book3s_exit_hv(void)
 +static void kvmppc_book3s_hv_exit(void)
  {
- 	kvm_exit();
+ 	kvmppc_hv_ops = NULL;
  }
  
 -module_init(kvmppc_book3s_init_hv);
 -module_exit(kvmppc_book3s_exit_hv);
 -MODULE_LICENSE("GPL");
 +module_init(kvmppc_book3s_hv_init);
 +module_exit(kvmppc_book3s_hv_exit);
diff --cc arch/powerpc/kvm/book3s_pr.c
index d10e1186f935,fbd985f0cb02..000000000000
--- a/arch/powerpc/kvm/book3s_pr.c
+++ b/arch/powerpc/kvm/book3s_pr.c
@@@ -1493,25 -1513,82 +1493,101 @@@ void kvmppc_core_destroy_vm(struct kvm 
  	}
  }
  
++<<<<<<< HEAD
 +static int kvmppc_book3s_init(void)
 +{
 +	int r;
 +
 +	r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 +
 +	if (r)
++=======
+ static int kvmppc_core_check_processor_compat_pr(void)
+ {
+ 	/* we are always compatible */
+ 	return 0;
+ }
+ 
+ static long kvm_arch_vm_ioctl_pr(struct file *filp,
+ 				 unsigned int ioctl, unsigned long arg)
+ {
+ 	return -ENOTTY;
+ }
+ 
+ static struct kvmppc_ops kvm_ops_pr = {
+ 	.is_hv_enabled = false,
+ 	.get_sregs = kvm_arch_vcpu_ioctl_get_sregs_pr,
+ 	.set_sregs = kvm_arch_vcpu_ioctl_set_sregs_pr,
+ 	.get_one_reg = kvmppc_get_one_reg_pr,
+ 	.set_one_reg = kvmppc_set_one_reg_pr,
+ 	.vcpu_load   = kvmppc_core_vcpu_load_pr,
+ 	.vcpu_put    = kvmppc_core_vcpu_put_pr,
+ 	.set_msr     = kvmppc_set_msr_pr,
+ 	.vcpu_run    = kvmppc_vcpu_run_pr,
+ 	.vcpu_create = kvmppc_core_vcpu_create_pr,
+ 	.vcpu_free   = kvmppc_core_vcpu_free_pr,
+ 	.check_requests = kvmppc_core_check_requests_pr,
+ 	.get_dirty_log = kvm_vm_ioctl_get_dirty_log_pr,
+ 	.flush_memslot = kvmppc_core_flush_memslot_pr,
+ 	.prepare_memory_region = kvmppc_core_prepare_memory_region_pr,
+ 	.commit_memory_region = kvmppc_core_commit_memory_region_pr,
+ 	.unmap_hva = kvm_unmap_hva_pr,
+ 	.unmap_hva_range = kvm_unmap_hva_range_pr,
+ 	.age_hva  = kvm_age_hva_pr,
+ 	.test_age_hva = kvm_test_age_hva_pr,
+ 	.set_spte_hva = kvm_set_spte_hva_pr,
+ 	.mmu_destroy  = kvmppc_mmu_destroy_pr,
+ 	.free_memslot = kvmppc_core_free_memslot_pr,
+ 	.create_memslot = kvmppc_core_create_memslot_pr,
+ 	.init_vm = kvmppc_core_init_vm_pr,
+ 	.destroy_vm = kvmppc_core_destroy_vm_pr,
+ 	.get_smmu_info = kvm_vm_ioctl_get_smmu_info_pr,
+ 	.emulate_op = kvmppc_core_emulate_op_pr,
+ 	.emulate_mtspr = kvmppc_core_emulate_mtspr_pr,
+ 	.emulate_mfspr = kvmppc_core_emulate_mfspr_pr,
+ 	.fast_vcpu_kick = kvm_vcpu_kick,
+ 	.arch_vm_ioctl  = kvm_arch_vm_ioctl_pr,
+ };
+ 
+ 
+ int kvmppc_book3s_init_pr(void)
+ {
+ 	int r;
+ 
+ 	r = kvmppc_core_check_processor_compat_pr();
+ 	if (r < 0)
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		return r;
  
- 	r = kvmppc_mmu_hpte_sysinit();
+ 	kvm_ops_pr.owner = THIS_MODULE;
+ 	kvmppc_pr_ops = &kvm_ops_pr;
  
+ 	r = kvmppc_mmu_hpte_sysinit();
  	return r;
  }
  
++<<<<<<< HEAD
 +static void kvmppc_book3s_exit(void)
++=======
+ void kvmppc_book3s_exit_pr(void)
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  {
+ 	kvmppc_pr_ops = NULL;
  	kvmppc_mmu_hpte_sysexit();
- 	kvm_exit();
  }
  
++<<<<<<< HEAD
 +module_init(kvmppc_book3s_init);
 +module_exit(kvmppc_book3s_exit);
++=======
+ /*
+  * We only support separate modules for book3s 64
+  */
+ #ifdef CONFIG_PPC_BOOK3S_64
+ 
+ module_init(kvmppc_book3s_init_pr);
+ module_exit(kvmppc_book3s_exit_pr);
+ 
+ MODULE_LICENSE("GPL");
+ #endif
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
diff --cc arch/powerpc/kvm/book3s_xics.c
index a3a5cb8ee7ea,f7a5108a3483..000000000000
--- a/arch/powerpc/kvm/book3s_xics.c
+++ b/arch/powerpc/kvm/book3s_xics.c
@@@ -818,7 -818,7 +818,11 @@@ int kvmppc_xics_hcall(struct kvm_vcpu *
  	}
  
  	/* Check for real mode returning too hard */
++<<<<<<< HEAD
 +	if (xics->real_mode)
++=======
+ 	if (xics->real_mode && vcpu->kvm->arch.kvm_ops->is_hv_enabled)
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		return kvmppc_xics_rm_complete(vcpu, req);
  
  	switch (req) {
diff --cc arch/powerpc/kvm/booke.c
index 85c3c7dce77a,15d0149511eb..000000000000
--- a/arch/powerpc/kvm/booke.c
+++ b/arch/powerpc/kvm/booke.c
@@@ -1414,8 -1472,7 +1414,12 @@@ int kvm_arch_vcpu_ioctl_get_sregs(struc
  
  	get_sregs_base(vcpu, sregs);
  	get_sregs_arch206(vcpu, sregs);
++<<<<<<< HEAD
 +	kvmppc_core_get_sregs(vcpu, sregs);
 +	return 0;
++=======
+ 	return vcpu->kvm->arch.kvm_ops->get_sregs(vcpu, sregs);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
@@@ -1434,7 -1491,7 +1438,11 @@@
  	if (ret < 0)
  		return ret;
  
++<<<<<<< HEAD
 +	return kvmppc_core_set_sregs(vcpu, sregs);
++=======
+ 	return vcpu->kvm->arch.kvm_ops->set_sregs(vcpu, sregs);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
@@@ -1478,10 -1542,13 +1486,14 @@@
  		val = get_reg_val(reg->id, vcpu->arch.tsr);
  		break;
  	case KVM_REG_PPC_DEBUG_INST:
 -		val = get_reg_val(reg->id, KVMPPC_INST_EHPRIV_DEBUG);
 -		break;
 -	case KVM_REG_PPC_VRSAVE:
 -		val = get_reg_val(reg->id, vcpu->arch.vrsave);
 +		val = get_reg_val(reg->id, KVMPPC_INST_EHPRIV);
  		break;
  	default:
++<<<<<<< HEAD
 +		r = kvmppc_get_one_reg(vcpu, reg->id, &val);
++=======
+ 		r = vcpu->kvm->arch.kvm_ops->get_one_reg(vcpu, reg->id, &val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		break;
  	}
  
@@@ -1553,8 -1627,11 +1565,12 @@@ int kvm_vcpu_ioctl_set_one_reg(struct k
  		kvmppc_set_tcr(vcpu, tcr);
  		break;
  	}
 -	case KVM_REG_PPC_VRSAVE:
 -		vcpu->arch.vrsave = set_reg_val(reg->id, val);
 -		break;
  	default:
++<<<<<<< HEAD
 +		r = kvmppc_set_one_reg(vcpu, reg->id, &val);
++=======
+ 		r = vcpu->kvm->arch.kvm_ops->set_one_reg(vcpu, reg->id, &val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		break;
  	}
  
@@@ -1682,6 -1904,44 +1698,47 @@@ void kvmppc_booke_vcpu_put(struct kvm_v
  {
  	current->thread.kvm_vcpu = NULL;
  	vcpu->cpu = -1;
++<<<<<<< HEAD
++=======
+ 
+ 	/* Clear pending debug event in DBSR */
+ 	kvmppc_clear_dbsr();
+ }
+ 
+ void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->mmu_destroy(vcpu);
+ }
+ 
+ int kvmppc_core_init_vm(struct kvm *kvm)
+ {
+ 	return kvm->arch.kvm_ops->init_vm(kvm);
+ }
+ 
+ struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)
+ {
+ 	return kvm->arch.kvm_ops->vcpu_create(kvm, id);
+ }
+ 
+ void kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);
+ }
+ 
+ void kvmppc_core_destroy_vm(struct kvm *kvm)
+ {
+ 	kvm->arch.kvm_ops->destroy_vm(kvm);
+ }
+ 
+ void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_load(vcpu, cpu);
+ }
+ 
+ void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->kvm->arch.kvm_ops->vcpu_put(vcpu);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  int __init kvmppc_booke_init(void)
diff --cc arch/powerpc/kvm/e500.c
index ce6b73c29612,497b142f651c..000000000000
--- a/arch/powerpc/kvm/e500.c
+++ b/arch/powerpc/kvm/e500.c
@@@ -534,7 -555,14 +534,18 @@@ static int __init kvmppc_e500_init(void
  	flush_icache_range(kvmppc_booke_handlers, kvmppc_booke_handlers +
  			   ivor[max_ivor] + handler_len);
  
++<<<<<<< HEAD
 +	return kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
++=======
+ 	r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ 	if (r)
+ 		goto err_out;
+ 	kvm_ops_e500.owner = THIS_MODULE;
+ 	kvmppc_pr_ops = &kvm_ops_e500;
+ 
+ err_out:
+ 	return r;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  static void __exit kvmppc_e500_exit(void)
diff --cc arch/powerpc/kvm/e500mc.c
index 19c8379575f7,4132cd2fc171..000000000000
--- a/arch/powerpc/kvm/e500mc.c
+++ b/arch/powerpc/kvm/e500mc.c
@@@ -353,7 -373,14 +353,18 @@@ static int __init kvmppc_e500mc_init(vo
  	kvmppc_init_lpid(64);
  	kvmppc_claim_lpid(0); /* host */
  
++<<<<<<< HEAD
 +	return kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
++=======
+ 	r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ 	if (r)
+ 		goto err_out;
+ 	kvm_ops_e500mc.owner = THIS_MODULE;
+ 	kvmppc_pr_ops = &kvm_ops_e500mc;
+ 
+ err_out:
+ 	return r;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
  
  static void __exit kvmppc_e500mc_exit(void)
diff --cc arch/powerpc/kvm/emulate.c
index 6d20566d074e,2f9a0873b44f..000000000000
--- a/arch/powerpc/kvm/emulate.c
+++ b/arch/powerpc/kvm/emulate.c
@@@ -126,9 -126,12 +126,14 @@@ static int kvmppc_emulate_mtspr(struct 
  		vcpu->arch.shared->sprg3 = spr_val;
  		break;
  
 -	/* PIR can legally be written, but we ignore it */
 -	case SPRN_PIR: break;
 -
  	default:
++<<<<<<< HEAD
 +		emulated = kvmppc_core_emulate_mtspr(vcpu, sprn,
 +						     spr_val);
++=======
+ 		emulated = vcpu->kvm->arch.kvm_ops->emulate_mtspr(vcpu, sprn,
+ 								  spr_val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		if (emulated == EMULATE_FAIL)
  			printk(KERN_INFO "mtspr: unknown spr "
  				"0x%x\n", sprn);
@@@ -188,8 -191,8 +193,13 @@@ static int kvmppc_emulate_mfspr(struct 
  		spr_val = kvmppc_get_dec(vcpu, get_tb());
  		break;
  	default:
++<<<<<<< HEAD
 +		emulated = kvmppc_core_emulate_mfspr(vcpu, sprn,
 +						     &spr_val);
++=======
+ 		emulated = vcpu->kvm->arch.kvm_ops->emulate_mfspr(vcpu, sprn,
+ 								  &spr_val);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		if (unlikely(emulated == EMULATE_FAIL)) {
  			printk(KERN_INFO "mfspr: unknown spr "
  				"0x%x\n", sprn);
@@@ -461,7 -464,8 +471,12 @@@ int kvmppc_emulate_instruction(struct k
  	}
  
  	if (emulated == EMULATE_FAIL) {
++<<<<<<< HEAD
 +		emulated = kvmppc_core_emulate_op(run, vcpu, inst, &advance);
++=======
+ 		emulated = vcpu->kvm->arch.kvm_ops->emulate_op(run, vcpu, inst,
+ 							       &advance);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		if (emulated == EMULATE_AGAIN) {
  			advance = 0;
  		} else if (emulated == EMULATE_FAIL) {
diff --cc arch/powerpc/kvm/powerpc.c
index f55e14cd1762,0320c1721caa..000000000000
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@@ -39,6 -40,12 +40,15 @@@
  #define CREATE_TRACE_POINTS
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ struct kvmppc_ops *kvmppc_hv_ops;
+ EXPORT_SYMBOL_GPL(kvmppc_hv_ops);
+ struct kvmppc_ops *kvmppc_pr_ops;
+ EXPORT_SYMBOL_GPL(kvmppc_pr_ops);
+ 
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
  {
  	return !!(v->arch.pending_exceptions) ||
@@@ -192,11 -199,9 +202,15 @@@ int kvmppc_sanity_check(struct kvm_vcp
  	if ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)
  		goto out;
  
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
  	/* HV KVM can only do PAPR mode for now */
++<<<<<<< HEAD
 +	if (!vcpu->arch.papr_enabled)
++=======
+ 	if (!vcpu->arch.papr_enabled && vcpu->kvm->arch.kvm_ops->is_hv_enabled)
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		goto out;
 +#endif
  
  #ifdef CONFIG_KVM_BOOKE_HV
  	if (!cpu_has_feature(CPU_FTR_EMB_HV))
@@@ -327,11 -365,10 +373,16 @@@ int kvm_dev_ioctl_check_extension(long 
  #if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
  	case KVM_CAP_SW_TLB:
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_KVM_MPIC
 +	case KVM_CAP_IRQ_MPIC:
 +#endif
 +		r = 1;
++=======
+ 		/* We support this only for PR */
+ 		r = !hv_enabled;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		break;
 -#ifdef CONFIG_KVM_MMIO
  	case KVM_CAP_COALESCED_MMIO:
  		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
  		break;
@@@ -346,32 -389,37 +397,54 @@@
  		r = 1;
  		break;
  #endif /* CONFIG_PPC_BOOK3S_64 */
 -#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
  	case KVM_CAP_PPC_SMT:
++<<<<<<< HEAD
 +		r = threads_per_core;
 +		break;
 +	case KVM_CAP_PPC_RMA:
 +		r = 1;
++=======
+ 		if (hv_enabled)
+ 			r = threads_per_core;
+ 		else
+ 			r = 0;
+ 		break;
+ 	case KVM_CAP_PPC_RMA:
+ 		r = hv_enabled;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		/* PPC970 requires an RMA */
 -		if (r && cpu_has_feature(CPU_FTR_ARCH_201))
 +		if (cpu_has_feature(CPU_FTR_ARCH_201))
  			r = 2;
  		break;
  #endif
  	case KVM_CAP_SYNC_MMU:
++<<<<<<< HEAD
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
 +		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
++=======
+ #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+ 		if (hv_enabled)
+ 			r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+ 		else
+ 			r = 0;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  #elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)
  		r = 1;
  #else
  		r = 0;
 -#endif
  		break;
 -#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 +#endif
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
  	case KVM_CAP_PPC_HTAB_FD:
++<<<<<<< HEAD
 +		r = 1;
++=======
+ 		r = hv_enabled;
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		break;
  #endif
 +		break;
  	case KVM_CAP_NR_VCPUS:
  		/*
  		 * Recommending a number of CPUs is somewhat arbitrary; we
@@@ -379,11 -427,10 +452,18 @@@
  		 * will have secondary threads "offline"), and for other KVM
  		 * implementations just count online CPUs.
  		 */
++<<<<<<< HEAD
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
 +		r = num_present_cpus();
 +#else
 +		r = num_online_cpus();
 +#endif
++=======
+ 		if (hv_enabled)
+ 			r = num_present_cpus();
+ 		else
+ 			r = num_online_cpus();
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		break;
  	case KVM_CAP_MAX_VCPUS:
  		r = KVM_MAX_VCPUS;
@@@ -1024,52 -1074,12 +1104,57 @@@ long kvm_arch_vm_ioctl(struct file *fil
  		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
  		goto out;
  	}
 +#endif /* CONFIG_PPC_BOOK3S_64 */
 +
 +#ifdef CONFIG_KVM_BOOK3S_64_HV
 +	case KVM_ALLOCATE_RMA: {
 +		struct kvm_allocate_rma rma;
 +		struct kvm *kvm = filp->private_data;
 +
 +		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
 +		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
 +			r = -EFAULT;
 +		break;
 +	}
 +
 +	case KVM_PPC_ALLOCATE_HTAB: {
 +		u32 htab_order;
 +
 +		r = -EFAULT;
 +		if (get_user(htab_order, (u32 __user *)argp))
 +			break;
 +		r = kvmppc_alloc_reset_hpt(kvm, &htab_order);
 +		if (r)
 +			break;
 +		r = -EFAULT;
 +		if (put_user(htab_order, (u32 __user *)argp))
 +			break;
 +		r = 0;
 +		break;
 +	}
 +
 +	case KVM_PPC_GET_HTAB_FD: {
 +		struct kvm_get_htab_fd ghf;
 +
 +		r = -EFAULT;
 +		if (copy_from_user(&ghf, argp, sizeof(ghf)))
 +			break;
 +		r = kvm_vm_ioctl_get_htab_fd(kvm, &ghf);
 +		break;
 +	}
 +#endif /* CONFIG_KVM_BOOK3S_64_HV */
 +
 +#ifdef CONFIG_PPC_BOOK3S_64
  	case KVM_PPC_GET_SMMU_INFO: {
  		struct kvm_ppc_smmu_info info;
+ 		struct kvm *kvm = filp->private_data;
  
  		memset(&info, 0, sizeof(info));
++<<<<<<< HEAD
 +		r = kvm_vm_ioctl_get_smmu_info(kvm, &info);
++=======
+ 		r = kvm->arch.kvm_ops->get_smmu_info(kvm, &info);
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  		if (r >= 0 && copy_to_user(argp, &info, sizeof(info)))
  			r = -EFAULT;
  		break;
@@@ -1080,11 -1090,15 +1165,19 @@@
  		r = kvm_vm_ioctl_rtas_define_token(kvm, argp);
  		break;
  	}
++<<<<<<< HEAD
 +#endif /* CONFIG_PPC_BOOK3S_64 */
++=======
+ 	default: {
+ 		struct kvm *kvm = filp->private_data;
+ 		r = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);
+ 	}
+ #else /* CONFIG_PPC_BOOK3S_64 */
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  	default:
  		r = -ENOTTY;
 -#endif
  	}
 +
  out:
  	return r;
  }
@@@ -1130,4 -1148,5 +1223,8 @@@ int kvm_arch_init(void *opaque
  
  void kvm_arch_exit(void)
  {
++<<<<<<< HEAD
++=======
+ 
++>>>>>>> cbbc58d4fdfa (kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine)
  }
* Unmerged path arch/powerpc/kvm/book3s.h
diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c4b760bc75d8..034a91773147 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -275,6 +275,7 @@ struct kvm_arch {
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_xics *xics;
 #endif
+	struct kvmppc_ops *kvm_ops;
 };
 
 /*
* Unmerged path arch/powerpc/include/asm/kvm_ppc.h
* Unmerged path arch/powerpc/kvm/44x.c
* Unmerged path arch/powerpc/kvm/book3s.c
* Unmerged path arch/powerpc/kvm/book3s.h
* Unmerged path arch/powerpc/kvm/book3s_hv.c
* Unmerged path arch/powerpc/kvm/book3s_pr.c
* Unmerged path arch/powerpc/kvm/book3s_xics.c
* Unmerged path arch/powerpc/kvm/booke.c
* Unmerged path arch/powerpc/kvm/e500.c
* Unmerged path arch/powerpc/kvm/e500mc.c
* Unmerged path arch/powerpc/kvm/emulate.c
* Unmerged path arch/powerpc/kvm/powerpc.c
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 1ba9af99e2bc..ff64f76c5e00 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -519,6 +519,10 @@ struct kvm_ppc_smmu_info {
 /* machine type bits, to be used as argument to KVM_CREATE_VM */
 #define KVM_VM_S390_UCONTROL	1
 
+/* on ppc, 0 indicate default, 1 should force HV and 2 PR */
+#define KVM_VM_PPC_HV 1
+#define KVM_VM_PPC_PR 2
+
 #define KVM_S390_SIE_PAGE_OFFSET 1
 
 /*
