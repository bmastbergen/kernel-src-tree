sched/balancing: Reduce the rate of needless idle load balancing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Tim Chen <tim.c.chen@linux.intel.com>
commit ed61bbc69c773465782476c7e5869fa5607fa73a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/ed61bbc6.failed

The current no_hz idle load balancer do load balancing for *all* idle cpus,
even though the time due to load balance for a particular
idle cpu could be still a while in the future.  This introduces a much
higher load balancing rate than what is necessary.  The patch
changes the behavior by only doing idle load balancing on
behalf of an idle cpu only when it is due for load balancing.

On SGI's systems with over 3000 cores, the cpu responsible for idle balancing
got overwhelmed with idle balancing, and introduces a lot of OS noise
to workloads.  This patch fixes the issue.

	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Acked-by: Russ Anderson <rja@sgi.com>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Reviewed-by: Jason Low <jason.low2@hp.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Len Brown <len.brown@intel.com>
	Cc: Dimitri Sivanich <sivanich@sgi.com>
	Cc: Hedi Berriche <hedi@sgi.com>
	Cc: Andi Kleen <andi@firstfloor.org>
	Cc: MichelLespinasse <walken@google.com>
	Cc: Peter Hurley <peter@hurleysoftware.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/1400621967.2970.280.camel@schen9-DESK
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit ed61bbc69c773465782476c7e5869fa5607fa73a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 32588e344590,7a0c000b6005..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -6847,12 -7193,17 +6847,26 @@@ static void nohz_idle_balance(int this_
  
  		rq = cpu_rq(balance_cpu);
  
++<<<<<<< HEAD
 +		raw_spin_lock_irq(&rq->lock);
 +		update_rq_clock(rq);
 +		update_idle_cpu_load(rq);
 +		raw_spin_unlock_irq(&rq->lock);
 +
 +		rebalance_domains(balance_cpu, CPU_IDLE);
++=======
+ 		/*
+ 		 * If time for next balance is due,
+ 		 * do the balance.
+ 		 */
+ 		if (time_after_eq(jiffies, rq->next_balance)) {
+ 			raw_spin_lock_irq(&rq->lock);
+ 			update_rq_clock(rq);
+ 			update_idle_cpu_load(rq);
+ 			raw_spin_unlock_irq(&rq->lock);
+ 			rebalance_domains(rq, CPU_IDLE);
+ 		}
++>>>>>>> ed61bbc69c77 (sched/balancing: Reduce the rate of needless idle load balancing)
  
  		if (time_after(this_rq->next_balance, rq->next_balance))
  			this_rq->next_balance = rq->next_balance;
* Unmerged path kernel/sched/fair.c
