iw_cxgb4: Max fastreg depth depends on DSGL support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [infiniband] cxgb4: Max fastreg depth depends on DSGL support (Sai Vemuri) [1124947]
Rebuild_FUZZ: 96.97%
commit-author Hariprasad S <hariprasad@chelsio.com>
commit ee30f7d507c0f3b3499bbe84d14849a6b5ac9484
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/ee30f7d5.failed

The max depth of a fastreg mr depends on whether the device supports
DSGL or not.  So compute it dynamically based on the device support and
the module use_dsgl option.

	Signed-off-by: Steve Wise <swise@opengridcomputing.com>
	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit ee30f7d507c0f3b3499bbe84d14849a6b5ac9484)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/cxgb4/mem.c
#	drivers/infiniband/hw/cxgb4/provider.c
#	drivers/infiniband/hw/cxgb4/qp.c
diff --cc drivers/infiniband/hw/cxgb4/mem.c
index f9ca072a99ed,9274c909cd19..000000000000
--- a/drivers/infiniband/hw/cxgb4/mem.c
+++ b/drivers/infiniband/hw/cxgb4/mem.c
@@@ -833,6 -615,7 +833,10 @@@ struct ib_mr *c4iw_alloc_fast_reg_mr(st
  	u32 mmid;
  	u32 stag = 0;
  	int ret = 0;
++<<<<<<< HEAD
++=======
+ 	int length = roundup(max_num_sg * sizeof(u64), 32);
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  
  	php = to_c4iw_pd(pd);
  	rhp = php->rhp;
diff --cc drivers/infiniband/hw/cxgb4/provider.c
index 79429256023a,8669f48ebd8e..000000000000
--- a/drivers/infiniband/hw/cxgb4/provider.c
+++ b/drivers/infiniband/hw/cxgb4/provider.c
@@@ -328,7 -339,8 +328,12 @@@ static int c4iw_query_device(struct ib_
  	props->max_mr = c4iw_num_stags(&dev->rdev);
  	props->max_pd = T4_MAX_NUM_PD;
  	props->local_ca_ack_delay = 0;
++<<<<<<< HEAD
 +	props->max_fast_reg_page_list_len = T4_MAX_FR_DEPTH;
++=======
+ 	props->max_fast_reg_page_list_len =
+ 		t4_max_fr_depth(dev->rdev.lldi.ulptx_memwrite_dsgl && use_dsgl);
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  
  	return 0;
  }
diff --cc drivers/infiniband/hw/cxgb4/qp.c
index 9b4a8b88908e,d7293132ee86..000000000000
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@@ -556,40 -605,35 +556,53 @@@ static int build_rdma_recv(struct c4iw_
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int build_fastreg(struct t4_sq *sq, union t4_wr *wqe,
 +			 struct ib_send_wr *wr, u8 *len16, u8 t5dev)
++=======
+ static int build_memreg(struct t4_sq *sq, union t4_wr *wqe,
+ 			struct ib_reg_wr *wr, u8 *len16, bool dsgl_supported)
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  {
 -	struct c4iw_mr *mhp = to_c4iw_mr(wr->mr);
 +
  	struct fw_ri_immd *imdp;
  	__be64 *p;
  	int i;
 -	int pbllen = roundup(mhp->mpl_len * sizeof(u64), 32);
 +	int pbllen = roundup(wr->wr.fast_reg.page_list_len * sizeof(u64), 32);
  	int rem;
  
++<<<<<<< HEAD
 +	if (wr->wr.fast_reg.page_list_len > T4_MAX_FR_DEPTH)
++=======
+ 	if (mhp->mpl_len > t4_max_fr_depth(dsgl_supported && use_dsgl))
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  		return -EINVAL;
  
  	wqe->fr.qpbinde_to_dcacpu = 0;
 -	wqe->fr.pgsz_shift = ilog2(wr->mr->page_size) - 12;
 +	wqe->fr.pgsz_shift = wr->wr.fast_reg.page_shift - 12;
  	wqe->fr.addr_type = FW_RI_VA_BASED_TO;
 -	wqe->fr.mem_perms = c4iw_ib_to_tpt_access(wr->access);
 +	wqe->fr.mem_perms = c4iw_ib_to_tpt_access(wr->wr.fast_reg.access_flags);
  	wqe->fr.len_hi = 0;
 -	wqe->fr.len_lo = cpu_to_be32(mhp->ibmr.length);
 -	wqe->fr.stag = cpu_to_be32(wr->key);
 -	wqe->fr.va_hi = cpu_to_be32(mhp->ibmr.iova >> 32);
 -	wqe->fr.va_lo_fbo = cpu_to_be32(mhp->ibmr.iova &
 +	wqe->fr.len_lo = cpu_to_be32(wr->wr.fast_reg.length);
 +	wqe->fr.stag = cpu_to_be32(wr->wr.fast_reg.rkey);
 +	wqe->fr.va_hi = cpu_to_be32(wr->wr.fast_reg.iova_start >> 32);
 +	wqe->fr.va_lo_fbo = cpu_to_be32(wr->wr.fast_reg.iova_start &
  					0xffffffff);
  
++<<<<<<< HEAD
 +	if (t5dev && use_dsgl && (pbllen > max_fr_immd)) {
 +		struct c4iw_fr_page_list *c4pl =
 +			to_c4iw_fr_page_list(wr->wr.fast_reg.page_list);
++=======
+ 	if (dsgl_supported && use_dsgl && (pbllen > max_fr_immd)) {
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  		struct fw_ri_dsgl *sglp;
  
 -		for (i = 0; i < mhp->mpl_len; i++)
 -			mhp->mpl[i] = (__force u64)cpu_to_be64((u64)mhp->mpl[i]);
 +		for (i = 0; i < wr->wr.fast_reg.page_list_len; i++) {
 +			wr->wr.fast_reg.page_list->page_list[i] = (__force u64)
 +				cpu_to_be64((u64)
 +				wr->wr.fast_reg.page_list->page_list[i]);
 +		}
  
  		sglp = (struct fw_ri_dsgl *)(&wqe->fr + 1);
  		sglp->op = FW_RI_DATA_DSGL;
@@@ -763,13 -804,11 +776,18 @@@ int c4iw_post_send(struct ib_qp *ibqp, 
  			if (!qhp->wq.sq.oldest_read)
  				qhp->wq.sq.oldest_read = swsqe;
  			break;
 -		case IB_WR_REG_MR:
 +		case IB_WR_FAST_REG_MR:
  			fw_opcode = FW_RI_FR_NSMR_WR;
  			swsqe->opcode = FW_RI_FAST_REGISTER;
++<<<<<<< HEAD
 +			err = build_fastreg(&qhp->wq.sq, wqe, wr, &len16,
 +					    is_t5(
 +					    qhp->rhp->rdev.lldi.adapter_type) ?
 +					    1 : 0);
++=======
+ 			err = build_memreg(&qhp->wq.sq, wqe, reg_wr(wr), &len16,
+ 				qhp->rhp->rdev.lldi.ulptx_memwrite_dsgl);
++>>>>>>> ee30f7d507c0 (iw_cxgb4: Max fastreg depth depends on DSGL support)
  			break;
  		case IB_WR_LOCAL_INV:
  			if (wr->send_flags & IB_SEND_FENCE)
* Unmerged path drivers/infiniband/hw/cxgb4/mem.c
* Unmerged path drivers/infiniband/hw/cxgb4/provider.c
* Unmerged path drivers/infiniband/hw/cxgb4/qp.c
