xprtrdma: Split the completion queue

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [net] sunrpc/xprtrdma: Split the completion queue (Steve Dickson) [1113248]
Rebuild_FUZZ: 91.14%
commit-author Chuck Lever <chuck.lever@oracle.com>
commit fc66448549bbb77f2f1a38b270ab2d6b6a22da33
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/fc664485.failed

The current CQ handler uses the ib_wc.opcode field to distinguish
between event types. However, the contents of that field are not
reliable if the completion status is not IB_WC_SUCCESS.

When an error completion occurs on a send event, the CQ handler
schedules a tasklet with something that is not a struct rpcrdma_rep.
This is never correct behavior, and sometimes it results in a panic.

To resolve this issue, split the completion queue into a send CQ and
a receive CQ. The send CQ handler now handles only struct rpcrdma_mw
wr_id's, and the receive CQ handler now handles only struct
rpcrdma_rep wr_id's.

Fix suggested by Shirley Ma <shirley.ma@oracle.com>

	Reported-by: Rafael Reiter <rafael.reiter@ims.co.at>
Fixes: 5c635e09cec0feeeb310968e51dad01040244851
BugLink: https://bugzilla.kernel.org/show_bug.cgi?id=73211
	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Klemens Senn <klemens.senn@ims.co.at>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit fc66448549bbb77f2f1a38b270ab2d6b6a22da33)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
diff --cc net/sunrpc/xprtrdma/verbs.c
index 871eaa4ede37,af2d097c221f..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -142,74 -142,27 +142,68 @@@ rpcrdma_cq_async_error_upcall(struct ib
  	}
  }
  
- static inline
- void rpcrdma_event_process(struct ib_wc *wc)
+ static void
+ rpcrdma_sendcq_process_wc(struct ib_wc *wc)
  {
- 	struct rpcrdma_mw *frmr;
- 	struct rpcrdma_rep *rep =
- 			(struct rpcrdma_rep *)(unsigned long) wc->wr_id;
+ 	struct rpcrdma_mw *frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
  
- 	dprintk("RPC:       %s: event rep %p status %X opcode %X length %u\n",
- 		__func__, rep, wc->status, wc->opcode, wc->byte_len);
+ 	dprintk("RPC:       %s: frmr %p status %X opcode %d\n",
+ 		__func__, frmr, wc->status, wc->opcode);
  
++<<<<<<< HEAD
 +	if (!rep) /* send or bind completion that we don't care about */
++=======
+ 	if (wc->wr_id == 0ULL)
  		return;
- 
- 	if (IB_WC_SUCCESS != wc->status) {
- 		dprintk("RPC:       %s: WC opcode %d status %X, connection lost\n",
- 			__func__, wc->opcode, wc->status);
- 		rep->rr_len = ~0U;
- 		if (wc->opcode != IB_WC_FAST_REG_MR && wc->opcode != IB_WC_LOCAL_INV)
- 			rpcrdma_schedule_tasklet(rep);
+ 	if (wc->status != IB_WC_SUCCESS)
++>>>>>>> fc66448549bb (xprtrdma: Split the completion queue)
  		return;
- 	}
  
- 	switch (wc->opcode) {
- 	case IB_WC_FAST_REG_MR:
- 		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
+ 	if (wc->opcode == IB_WC_FAST_REG_MR)
  		frmr->r.frmr.state = FRMR_IS_VALID;
- 		break;
- 	case IB_WC_LOCAL_INV:
- 		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
+ 	else if (wc->opcode == IB_WC_LOCAL_INV)
  		frmr->r.frmr.state = FRMR_IS_INVALID;
++<<<<<<< HEAD
 +		break;
 +	case IB_WC_RECV:
 +		rep->rr_len = wc->byte_len;
 +		ib_dma_sync_single_for_cpu(
 +			rdmab_to_ia(rep->rr_buffer)->ri_id->device,
 +			rep->rr_iov.addr, rep->rr_len, DMA_FROM_DEVICE);
 +		/* Keep (only) the most recent credits, after check validity */
 +		if (rep->rr_len >= 16) {
 +			struct rpcrdma_msg *p =
 +					(struct rpcrdma_msg *) rep->rr_base;
 +			unsigned int credits = ntohl(p->rm_credit);
 +			if (credits == 0) {
 +				dprintk("RPC:       %s: server"
 +					" dropped credits to 0!\n", __func__);
 +				/* don't deadlock */
 +				credits = 1;
 +			} else if (credits > rep->rr_buffer->rb_max_requests) {
 +				dprintk("RPC:       %s: server"
 +					" over-crediting: %d (%d)\n",
 +					__func__, credits,
 +					rep->rr_buffer->rb_max_requests);
 +				credits = rep->rr_buffer->rb_max_requests;
 +			}
 +			atomic_set(&rep->rr_buffer->rb_credits, credits);
 +		}
 +		/* fall through */
 +	case IB_WC_BIND_MW:
 +		rpcrdma_schedule_tasklet(rep);
 +		break;
 +	default:
 +		dprintk("RPC:       %s: unexpected WC event %X\n",
 +			__func__, wc->opcode);
 +		break;
 +	}
++=======
++>>>>>>> fc66448549bb (xprtrdma: Split the completion queue)
  }
  
- static inline int
- rpcrdma_cq_poll(struct ib_cq *cq)
+ static int
+ rpcrdma_sendcq_poll(struct ib_cq *cq)
  {
  	struct ib_wc wc;
  	int rc;
@@@ -231,9 -249,8 +290,12 @@@ rpcrdma_recvcq_poll(struct ib_cq *cq
  }
  
  /*
-  * rpcrdma_cq_event_upcall
+  * Handle receive completions.
   *
++<<<<<<< HEAD
 + * This upcall handles recv, send, bind and unbind events.
++=======
++>>>>>>> fc66448549bb (xprtrdma: Split the completion queue)
   * It is reentrant but processes single events in order to maintain
   * ordering of receives to keep server credits.
   *
@@@ -727,15 -706,7 +791,19 @@@ rpcrdma_ep_create(struct rpcrdma_ep *ep
  		ep->rep_attr.cap.max_recv_sge);
  
  	/* set trigger for requesting send completion */
++<<<<<<< HEAD
 +	ep->rep_cqinit = ep->rep_attr.cap.max_send_wr/2 /*  - 1*/;
 +	switch (ia->ri_memreg_strategy) {
 +	case RPCRDMA_MEMWINDOWS_ASYNC:
 +	case RPCRDMA_MEMWINDOWS:
 +		ep->rep_cqinit -= RPCRDMA_MAX_SEGS;
 +		break;
 +	default:
 +		break;
 +	}
++=======
+ 	ep->rep_cqinit = ep->rep_attr.cap.max_send_wr/2 - 1;
++>>>>>>> fc66448549bb (xprtrdma: Split the completion queue)
  	if (ep->rep_cqinit <= 2)
  		ep->rep_cqinit = 0;
  	INIT_CQCOUNT(ep);
@@@ -743,18 -714,12 +811,21 @@@
  	init_waitqueue_head(&ep->rep_connect_wait);
  	INIT_DELAYED_WORK(&ep->rep_connect_worker, rpcrdma_connect_worker);
  
++<<<<<<< HEAD
 +	/*
 +	 * Create a single cq for receive dto and mw_bind (only ever
 +	 * care about unbind, really). Send completions are suppressed.
 +	 * Use single threaded tasklet upcalls to maintain ordering.
 +	 */
 +	ep->rep_cq = ib_create_cq(ia->ri_id->device, rpcrdma_cq_event_upcall,
++=======
+ 	sendcq = ib_create_cq(ia->ri_id->device, rpcrdma_sendcq_upcall,
++>>>>>>> fc66448549bb (xprtrdma: Split the completion queue)
  				  rpcrdma_cq_async_error_upcall, NULL,
- 				  ep->rep_attr.cap.max_recv_wr +
  				  ep->rep_attr.cap.max_send_wr + 1, 0);
- 	if (IS_ERR(ep->rep_cq)) {
- 		rc = PTR_ERR(ep->rep_cq);
- 		dprintk("RPC:       %s: ib_create_cq failed: %i\n",
+ 	if (IS_ERR(sendcq)) {
+ 		rc = PTR_ERR(sendcq);
+ 		dprintk("RPC:       %s: failed to create send CQ: %i\n",
  			__func__, rc);
  		goto out1;
  	}
* Unmerged path net/sunrpc/xprtrdma/verbs.c
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cd7bc9218343..e9586e456dfd 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -79,7 +79,6 @@ struct rpcrdma_ep {
 	int			rep_cqinit;
 	int			rep_connected;
 	struct rpcrdma_ia	*rep_ia;
-	struct ib_cq		*rep_cq;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
 	struct ib_sge		rep_pad;	/* holds zeroed pad */
