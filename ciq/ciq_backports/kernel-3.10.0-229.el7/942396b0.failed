hyperv: Fix the total_data_buflen in send path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Haiyang Zhang <haiyangz@microsoft.com>
commit 942396b01989d54977120f3625e5ba31afe7a75c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/942396b0.failed

total_data_buflen is used by netvsc_send() to decide if a packet can be put
into send buffer. It should also include the size of RNDIS message before the
Ethernet frame. Otherwise, a messge with total size bigger than send_section_size
may be copied into the send buffer, and cause data corruption.

[Request to include this patch to the Stable branches]

	Signed-off-by: Haiyang Zhang <haiyangz@microsoft.com>
	Reviewed-by: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 942396b01989d54977120f3625e5ba31afe7a75c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/netvsc_drv.c
diff --cc drivers/net/hyperv/netvsc_drv.c
index 854b31f1a85a,78ec33f5100b..000000000000
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@@ -169,55 -413,152 +169,188 @@@ static int netvsc_start_xmit(struct sk_
  
  	packet->vlan_tci = skb->vlan_tci;
  
 -	packet->q_idx = skb_get_queue_mapping(skb);
 +	packet->extension = (void *)(unsigned long)packet +
 +				sizeof(struct hv_netvsc_packet) +
 +				    (num_pages * sizeof(struct hv_page_buffer));
 +
 +	/* If the rndis msg goes beyond 1 page, we will add 1 later */
 +	packet->page_buf_cnt = num_pages - 1;
  
 -	packet->is_data_pkt = true;
 +	/* Initialize it from the skb */
  	packet->total_data_buflen = skb->len;
  
 -	packet->rndis_msg = (struct rndis_message *)((unsigned long)packet +
 -				sizeof(struct hv_netvsc_packet) +
 -				(num_data_pgs * sizeof(struct hv_page_buffer)));
 +	/* Start filling in the page buffers starting after RNDIS buffer. */
 +	packet->page_buf[1].pfn = virt_to_phys(skb->data) >> PAGE_SHIFT;
 +	packet->page_buf[1].offset
 +		= (unsigned long)skb->data & (PAGE_SIZE - 1);
 +	if (npg_data == 1)
 +		packet->page_buf[1].len = skb_headlen(skb);
 +	else
 +		packet->page_buf[1].len = PAGE_SIZE
 +			- packet->page_buf[1].offset;
 +
 +	for (i = 2; i <= npg_data; i++) {
 +		packet->page_buf[i].pfn = virt_to_phys(skb->data
 +			+ PAGE_SIZE * (i-1)) >> PAGE_SHIFT;
 +		packet->page_buf[i].offset = 0;
 +		packet->page_buf[i].len = PAGE_SIZE;
 +	}
 +	if (npg_data > 1)
 +		packet->page_buf[npg_data].len = (((unsigned long)skb->data
 +			+ skb_headlen(skb) - 1) & (PAGE_SIZE - 1)) + 1;
 +
 +	/* Additional fragments are after SKB data */
 +	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
 +		const skb_frag_t *f = &skb_shinfo(skb)->frags[i];
 +
 +		packet->page_buf[i+npg_data+1].pfn =
 +			page_to_pfn(skb_frag_page(f));
 +		packet->page_buf[i+npg_data+1].offset = f->page_offset;
 +		packet->page_buf[i+npg_data+1].len = skb_frag_size(f);
 +	}
  
  	/* Set the completion routine */
 -	packet->send_completion = netvsc_xmit_completion;
 -	packet->send_completion_ctx = packet;
 -	packet->send_completion_tid = (unsigned long)skb;
 -
 +	packet->completion.send.send_completion = netvsc_xmit_completion;
 +	packet->completion.send.send_completion_ctx = packet;
 +	packet->completion.send.send_completion_tid = (unsigned long)skb;
 +
++<<<<<<< HEAD
 +	ret = rndis_filter_send(net_device_ctx->device_ctx,
 +				  packet);
++=======
+ 	isvlan = packet->vlan_tci & VLAN_TAG_PRESENT;
+ 
+ 	/* Add the rndis header */
+ 	rndis_msg = packet->rndis_msg;
+ 	rndis_msg->ndis_msg_type = RNDIS_MSG_PACKET;
+ 	rndis_msg->msg_len = packet->total_data_buflen;
+ 	rndis_pkt = &rndis_msg->msg.pkt;
+ 	rndis_pkt->data_offset = sizeof(struct rndis_packet);
+ 	rndis_pkt->data_len = packet->total_data_buflen;
+ 	rndis_pkt->per_pkt_info_offset = sizeof(struct rndis_packet);
+ 
+ 	rndis_msg_size = RNDIS_MESSAGE_SIZE(struct rndis_packet);
+ 
+ 	hash = skb_get_hash_raw(skb);
+ 	if (hash != 0 && net->real_num_tx_queues > 1) {
+ 		rndis_msg_size += NDIS_HASH_PPI_SIZE;
+ 		ppi = init_ppi_data(rndis_msg, NDIS_HASH_PPI_SIZE,
+ 				    NBL_HASH_VALUE);
+ 		*(u32 *)((void *)ppi + ppi->ppi_offset) = hash;
+ 	}
+ 
+ 	if (isvlan) {
+ 		struct ndis_pkt_8021q_info *vlan;
+ 
+ 		rndis_msg_size += NDIS_VLAN_PPI_SIZE;
+ 		ppi = init_ppi_data(rndis_msg, NDIS_VLAN_PPI_SIZE,
+ 					IEEE_8021Q_INFO);
+ 		vlan = (struct ndis_pkt_8021q_info *)((void *)ppi +
+ 						ppi->ppi_offset);
+ 		vlan->vlanid = packet->vlan_tci & VLAN_VID_MASK;
+ 		vlan->pri = (packet->vlan_tci & VLAN_PRIO_MASK) >>
+ 				VLAN_PRIO_SHIFT;
+ 	}
+ 
+ 	net_trans_info = get_net_transport_info(skb, &hdr_offset);
+ 	if (net_trans_info == TRANSPORT_INFO_NOT_IP)
+ 		goto do_send;
+ 
+ 	/*
+ 	 * Setup the sendside checksum offload only if this is not a
+ 	 * GSO packet.
+ 	 */
+ 	if (skb_is_gso(skb))
+ 		goto do_lso;
+ 
+ 	if ((skb->ip_summed == CHECKSUM_NONE) ||
+ 	    (skb->ip_summed == CHECKSUM_UNNECESSARY))
+ 		goto do_send;
+ 
+ 	rndis_msg_size += NDIS_CSUM_PPI_SIZE;
+ 	ppi = init_ppi_data(rndis_msg, NDIS_CSUM_PPI_SIZE,
+ 			    TCPIP_CHKSUM_PKTINFO);
+ 
+ 	csum_info = (struct ndis_tcp_ip_checksum_info *)((void *)ppi +
+ 			ppi->ppi_offset);
+ 
+ 	if (net_trans_info & (INFO_IPV4 << 16))
+ 		csum_info->transmit.is_ipv4 = 1;
+ 	else
+ 		csum_info->transmit.is_ipv6 = 1;
+ 
+ 	if (net_trans_info & INFO_TCP) {
+ 		csum_info->transmit.tcp_checksum = 1;
+ 		csum_info->transmit.tcp_header_offset = hdr_offset;
+ 	} else if (net_trans_info & INFO_UDP) {
+ 		/* UDP checksum offload is not supported on ws2008r2.
+ 		 * Furthermore, on ws2012 and ws2012r2, there are some
+ 		 * issues with udp checksum offload from Linux guests.
+ 		 * (these are host issues).
+ 		 * For now compute the checksum here.
+ 		 */
+ 		struct udphdr *uh;
+ 		u16 udp_len;
+ 
+ 		ret = skb_cow_head(skb, 0);
+ 		if (ret)
+ 			goto drop;
+ 
+ 		uh = udp_hdr(skb);
+ 		udp_len = ntohs(uh->len);
+ 		uh->check = 0;
+ 		uh->check = csum_tcpudp_magic(ip_hdr(skb)->saddr,
+ 					      ip_hdr(skb)->daddr,
+ 					      udp_len, IPPROTO_UDP,
+ 					      csum_partial(uh, udp_len, 0));
+ 		if (uh->check == 0)
+ 			uh->check = CSUM_MANGLED_0;
+ 
+ 		csum_info->transmit.udp_checksum = 0;
+ 	}
+ 	goto do_send;
+ 
+ do_lso:
+ 	rndis_msg_size += NDIS_LSO_PPI_SIZE;
+ 	ppi = init_ppi_data(rndis_msg, NDIS_LSO_PPI_SIZE,
+ 			    TCP_LARGESEND_PKTINFO);
+ 
+ 	lso_info = (struct ndis_tcp_lso_info *)((void *)ppi +
+ 			ppi->ppi_offset);
+ 
+ 	lso_info->lso_v2_transmit.type = NDIS_TCP_LARGE_SEND_OFFLOAD_V2_TYPE;
+ 	if (net_trans_info & (INFO_IPV4 << 16)) {
+ 		lso_info->lso_v2_transmit.ip_version =
+ 			NDIS_TCP_LARGE_SEND_OFFLOAD_IPV4;
+ 		ip_hdr(skb)->tot_len = 0;
+ 		ip_hdr(skb)->check = 0;
+ 		tcp_hdr(skb)->check =
+ 		~csum_tcpudp_magic(ip_hdr(skb)->saddr,
+ 				   ip_hdr(skb)->daddr, 0, IPPROTO_TCP, 0);
+ 	} else {
+ 		lso_info->lso_v2_transmit.ip_version =
+ 			NDIS_TCP_LARGE_SEND_OFFLOAD_IPV6;
+ 		ipv6_hdr(skb)->payload_len = 0;
+ 		tcp_hdr(skb)->check =
+ 		~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
+ 				&ipv6_hdr(skb)->daddr, 0, IPPROTO_TCP, 0);
+ 	}
+ 	lso_info->lso_v2_transmit.tcp_header_offset = hdr_offset;
+ 	lso_info->lso_v2_transmit.mss = skb_shinfo(skb)->gso_size;
+ 
+ do_send:
+ 	/* Start filling in the page buffers with the rndis hdr */
+ 	rndis_msg->msg_len += rndis_msg_size;
+ 	packet->total_data_buflen = rndis_msg->msg_len;
+ 	packet->page_buf_cnt = init_page_array(rndis_msg, rndis_msg_size,
+ 					skb, &packet->page_buf[0]);
+ 
+ 	ret = netvsc_send(net_device_ctx->device_ctx, packet);
+ 
+ drop:
++>>>>>>> 942396b01989 (hyperv: Fix the total_data_buflen in send path)
  	if (ret == 0) {
 -		net->stats.tx_bytes += skb_length;
 +		net->stats.tx_bytes += skb->len;
  		net->stats.tx_packets++;
  	} else {
  		kfree(packet);
* Unmerged path drivers/net/hyperv/netvsc_drv.c
