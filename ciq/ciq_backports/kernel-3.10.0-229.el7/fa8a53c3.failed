aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Benjamin LaHaise <bcrl@kvack.org>
commit fa8a53c39f3fdde98c9eace6a9b412143f0f6ed6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/fa8a53c3.failed

As reported by Tang Chen, Gu Zheng and Yasuaki Isimatsu, the following issues
exist in the aio ring page migration support.

As a result, for example, we have the following problem:

            thread 1                      |              thread 2
                                          |
aio_migratepage()                         |
 |-> take ctx->completion_lock            |
 |-> migrate_page_copy(new, old)          |
 |   *NOW*, ctx->ring_pages[idx] == old   |
                                          |
                                          |    *NOW*, ctx->ring_pages[idx] == old
                                          |    aio_read_events_ring()
                                          |     |-> ring = kmap_atomic(ctx->ring_pages[0])
                                          |     |-> ring->head = head;          *HERE, write to the old ring page*
                                          |     |-> kunmap_atomic(ring);
                                          |
 |-> ctx->ring_pages[idx] = new           |
 |   *BUT NOW*, the content of            |
 |    ring_pages[idx] is old.             |
 |-> release ctx->completion_lock         |

As above, the new ring page will not be updated.

Fix this issue, as well as prevent races in aio_ring_setup() by holding
the ring_lock mutex during kioctx setup and page migration.  This avoids
the overhead of taking another spinlock in aio_read_events_ring() as Tang's
and Gu's original fix did, pushing the overhead into the migration code.

Note that to handle the nesting of ring_lock inside of mmap_sem, the
migratepage operation uses mutex_trylock().  Page migration is not a 100%
critical operation in this case, so the ocassional failure can be
tolerated.  This issue was reported by Sasha Levin.

Based on feedback from Linus, avoid the extra taking of ctx->completion_lock.
Instead, make page migration fully serialised by mapping->private_lock, and
have aio_free_ring() simply disconnect the kioctx from the mapping by calling
put_aio_ring_file() before touching ctx->ring_pages[].  This simplifies the
error handling logic in aio_migratepage(), and should improve robustness.

v4: always do mutex_unlock() in cases when kioctx setup fails.

	Reported-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
	Reported-by: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>
	Cc: Tang Chen <tangchen@cn.fujitsu.com>
	Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
	Cc: stable@vger.kernel.org
(cherry picked from commit fa8a53c39f3fdde98c9eace6a9b412143f0f6ed6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/aio.c
diff --cc fs/aio.c
index e90f40ffd1ab,12a3de0ee6da..000000000000
--- a/fs/aio.c
+++ b/fs/aio.c
@@@ -224,10 -260,10 +230,14 @@@ static void aio_free_ring(struct kioct
  		put_page(page);
  	}
  
++<<<<<<< HEAD
 +	put_aio_ring_file(ctx);
 +
 +	if (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages)
++=======
+ 	if (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages) {
++>>>>>>> fa8a53c39f3f (aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration)
  		kfree(ctx->ring_pages);
 -		ctx->ring_pages = NULL;
 -	}
  }
  
  static int aio_ring_mmap(struct file *file, struct vm_area_struct *vma)
@@@ -511,42 -534,78 +518,92 @@@ static void free_ioctx(struct kioctx *c
  
  	spin_unlock_irq(&ctx->ctx_lock);
  
 -	percpu_ref_kill(&ctx->reqs);
 -	percpu_ref_put(&ctx->reqs);
 -}
 +	ring = kmap_atomic(ctx->ring_pages[0]);
 +	head = ring->head;
 +	kunmap_atomic(ring);
  
 -static int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)
 -{
 -	unsigned i, new_nr;
 -	struct kioctx_table *table, *old;
 -	struct aio_ring *ring;
 +	while (atomic_read(&ctx->reqs_active) > 0) {
 +		wait_event(ctx->wait,
 +				head != ctx->tail ||
 +				atomic_read(&ctx->reqs_active) <= 0);
  
 -	spin_lock(&mm->ioctx_lock);
 -	rcu_read_lock();
 -	table = rcu_dereference(mm->ioctx_table);
 +		avail = (head <= ctx->tail ? ctx->tail : ctx->nr_events) - head;
  
++<<<<<<< HEAD
 +		atomic_sub(avail, &ctx->reqs_active);
 +		head += avail;
 +		head %= ctx->nr_events;
++=======
+ 	while (1) {
+ 		if (table)
+ 			for (i = 0; i < table->nr; i++)
+ 				if (!table->table[i]) {
+ 					ctx->id = i;
+ 					table->table[i] = ctx;
+ 					rcu_read_unlock();
+ 					spin_unlock(&mm->ioctx_lock);
+ 
+ 					/* While kioctx setup is in progress,
+ 					 * we are protected from page migration
+ 					 * changes ring_pages by ->ring_lock.
+ 					 */
+ 					ring = kmap_atomic(ctx->ring_pages[0]);
+ 					ring->id = ctx->id;
+ 					kunmap_atomic(ring);
+ 					return 0;
+ 				}
+ 
+ 		new_nr = (table ? table->nr : 1) * 4;
+ 
+ 		rcu_read_unlock();
+ 		spin_unlock(&mm->ioctx_lock);
+ 
+ 		table = kzalloc(sizeof(*table) + sizeof(struct kioctx *) *
+ 				new_nr, GFP_KERNEL);
+ 		if (!table)
+ 			return -ENOMEM;
+ 
+ 		table->nr = new_nr;
+ 
+ 		spin_lock(&mm->ioctx_lock);
+ 		rcu_read_lock();
+ 		old = rcu_dereference(mm->ioctx_table);
+ 
+ 		if (!old) {
+ 			rcu_assign_pointer(mm->ioctx_table, table);
+ 		} else if (table->nr > old->nr) {
+ 			memcpy(table->table, old->table,
+ 			       old->nr * sizeof(struct kioctx *));
+ 
+ 			rcu_assign_pointer(mm->ioctx_table, table);
+ 			kfree_rcu(old, rcu);
+ 		} else {
+ 			kfree(table);
+ 			table = old;
+ 		}
++>>>>>>> fa8a53c39f3f (aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration)
  	}
 +
 +	WARN_ON(atomic_read(&ctx->reqs_active) < 0);
 +
 +	aio_free_ring(ctx);
 +
 +	pr_debug("freeing %p\n", ctx);
 +
 +	/*
 +	 * Here the call_rcu() is between the wait_event() for reqs_active to
 +	 * hit 0, and freeing the ioctx.
 +	 *
 +	 * aio_complete() decrements reqs_active, but it has to touch the ioctx
 +	 * after to issue a wakeup so we use rcu.
 +	 */
 +	call_rcu(&ctx->rcu_head, free_ioctx_rcu);
  }
  
 -static void aio_nr_sub(unsigned nr)
 +static void put_ioctx(struct kioctx *ctx)
  {
 -	spin_lock(&aio_nr_lock);
 -	if (WARN_ON(aio_nr - nr > aio_nr))
 -		aio_nr = 0;
 -	else
 -		aio_nr -= nr;
 -	spin_unlock(&aio_nr_lock);
 +	if (unlikely(atomic_dec_and_test(&ctx->users)))
 +		free_ioctx(ctx);
  }
  
  /* ioctx_alloc
@@@ -574,17 -645,34 +631,47 @@@ static struct kioctx *ioctx_alloc(unsig
  
  	ctx->max_reqs = nr_events;
  
++<<<<<<< HEAD
 +	atomic_set(&ctx->users, 2);
 +	atomic_set(&ctx->dead, 0);
 +	spin_lock_init(&ctx->ctx_lock);
 +	spin_lock_init(&ctx->completion_lock);
 +	mutex_init(&ctx->ring_lock);
++=======
+ 	spin_lock_init(&ctx->ctx_lock);
+ 	spin_lock_init(&ctx->completion_lock);
+ 	mutex_init(&ctx->ring_lock);
+ 	/* Protect against page migration throughout kiotx setup by keeping
+ 	 * the ring_lock mutex held until setup is complete. */
+ 	mutex_lock(&ctx->ring_lock);
++>>>>>>> fa8a53c39f3f (aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration)
  	init_waitqueue_head(&ctx->wait);
  
  	INIT_LIST_HEAD(&ctx->active_reqs);
  
++<<<<<<< HEAD
 +	if (aio_setup_ring(ctx) < 0)
 +		goto out_freectx;
++=======
+ 	if (percpu_ref_init(&ctx->users, free_ioctx_users))
+ 		goto err;
+ 
+ 	if (percpu_ref_init(&ctx->reqs, free_ioctx_reqs))
+ 		goto err;
+ 
+ 	ctx->cpu = alloc_percpu(struct kioctx_cpu);
+ 	if (!ctx->cpu)
+ 		goto err;
+ 
+ 	err = aio_setup_ring(ctx);
+ 	if (err < 0)
+ 		goto err;
+ 
+ 	atomic_set(&ctx->reqs_available, ctx->nr_events - 1);
+ 	ctx->req_batch = (ctx->nr_events - 1) / (num_possible_cpus() * 4);
+ 	if (ctx->req_batch < 1)
+ 		ctx->req_batch = 1;
++>>>>>>> fa8a53c39f3f (aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration)
  
  	/* limit the number of system wide aios */
  	spin_lock(&aio_nr_lock);
@@@ -596,20 -685,29 +683,31 @@@
  	aio_nr += ctx->max_reqs;
  	spin_unlock(&aio_nr_lock);
  
 -	percpu_ref_get(&ctx->users);	/* io_setup() will drop this ref */
 -	percpu_ref_get(&ctx->reqs);	/* free_ioctx_users() will drop this */
 -
 -	err = ioctx_add_table(ctx, mm);
 -	if (err)
 -		goto err_cleanup;
 +	/* now link into global list. */
 +	spin_lock(&mm->ioctx_lock);
 +	hlist_add_head_rcu(&ctx->list, &mm->ioctx_list);
 +	spin_unlock(&mm->ioctx_lock);
  
+ 	/* Release the ring_lock mutex now that all setup is complete. */
+ 	mutex_unlock(&ctx->ring_lock);
+ 
  	pr_debug("allocated ioctx %p[%ld]: mm=%p mask=0x%x\n",
  		 ctx, ctx->user_id, mm, ctx->nr_events);
  	return ctx;
  
 -err_cleanup:
 -	aio_nr_sub(ctx->max_reqs);
 -err_ctx:
 +out_cleanup:
 +	err = -EAGAIN;
  	aio_free_ring(ctx);
++<<<<<<< HEAD
 +out_freectx:
 +	put_aio_ring_file(ctx);
++=======
+ err:
+ 	mutex_unlock(&ctx->ring_lock);
+ 	free_percpu(ctx->cpu);
+ 	free_percpu(ctx->reqs.pcpu_count);
+ 	free_percpu(ctx->users.pcpu_count);
++>>>>>>> fa8a53c39f3f (aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration)
  	kmem_cache_free(kioctx_cachep, ctx);
  	pr_debug("error allocating ioctx %d\n", err);
  	return ERR_PTR(err);
@@@ -922,13 -1037,15 +1020,14 @@@ static long aio_read_events_ring(struc
  
  	mutex_lock(&ctx->ring_lock);
  
+ 	/* Access to ->ring_pages here is protected by ctx->ring_lock. */
  	ring = kmap_atomic(ctx->ring_pages[0]);
  	head = ring->head;
 -	tail = ring->tail;
  	kunmap_atomic(ring);
  
 -	pr_debug("h%u t%u m%u\n", head, tail, ctx->nr_events);
 +	pr_debug("h%u t%u m%u\n", head, ctx->tail, ctx->nr_events);
  
 -	if (head == tail)
 +	if (head == ctx->tail)
  		goto out;
  
  	while (ret < nr) {
* Unmerged path fs/aio.c
