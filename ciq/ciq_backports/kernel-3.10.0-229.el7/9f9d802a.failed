xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 9f9d802a28a107937ecda4ff78de2ab5cedd439d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/9f9d802a.failed

FAST_REG_MR Work Requests update a Memory Region's rkey. Rkey's are
used to block unwanted access to the memory controlled by an MR. The
rkey is passed to the receiver (the NFS server, in our case), and is
also used by xprtrdma to invalidate the MR when the RPC is complete.

When a FAST_REG_MR Work Request is flushed after a transport
disconnect, xprtrdma cannot tell whether the WR actually hit the
adapter or not. So it is indeterminant at that point whether the
existing rkey is still valid.

After the transport connection is re-established, the next
FAST_REG_MR or LOCAL_INV Work Request against that MR can sometimes
fail because the rkey value does not match what xprtrdma expects.

The only reliable way to recover in this case is to deregister and
register the MR before it is used again. These operations can be
done only in a process context, so handle it in the transport
connect worker.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Tested-by: Shirley Ma <shirley.ma@oracle.com>
	Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 9f9d802a28a107937ecda4ff78de2ab5cedd439d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/verbs.c
index 499e0d7e7773,3a6376a77fcc..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -142,91 -144,135 +144,172 @@@ rpcrdma_cq_async_error_upcall(struct ib
  	}
  }
  
++<<<<<<< HEAD
 +static inline
 +void rpcrdma_event_process(struct ib_wc *wc)
++=======
+ static void
+ rpcrdma_sendcq_process_wc(struct ib_wc *wc)
+ {
+ 	struct rpcrdma_mw *frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
+ 
+ 	dprintk("RPC:       %s: frmr %p status %X opcode %d\n",
+ 		__func__, frmr, wc->status, wc->opcode);
+ 
+ 	if (wc->wr_id == 0ULL)
+ 		return;
+ 	if (wc->status != IB_WC_SUCCESS) {
+ 		frmr->r.frmr.fr_state = FRMR_IS_STALE;
+ 		return;
+ 	}
+ 
+ 	if (wc->opcode == IB_WC_FAST_REG_MR)
+ 		frmr->r.frmr.fr_state = FRMR_IS_VALID;
+ 	else if (wc->opcode == IB_WC_LOCAL_INV)
+ 		frmr->r.frmr.fr_state = FRMR_IS_INVALID;
+ }
+ 
+ static int
+ rpcrdma_sendcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)
+ {
+ 	struct ib_wc *wcs;
+ 	int budget, count, rc;
+ 
+ 	budget = RPCRDMA_WC_BUDGET / RPCRDMA_POLLSIZE;
+ 	do {
+ 		wcs = ep->rep_send_wcs;
+ 
+ 		rc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);
+ 		if (rc <= 0)
+ 			return rc;
+ 
+ 		count = rc;
+ 		while (count-- > 0)
+ 			rpcrdma_sendcq_process_wc(wcs++);
+ 	} while (rc == RPCRDMA_POLLSIZE && --budget);
+ 	return 0;
+ }
+ 
+ /*
+  * Handle send, fast_reg_mr, and local_inv completions.
+  *
+  * Send events are typically suppressed and thus do not result
+  * in an upcall. Occasionally one is signaled, however. This
+  * prevents the provider's completion queue from wrapping and
+  * losing a completion.
+  */
+ static void
+ rpcrdma_sendcq_upcall(struct ib_cq *cq, void *cq_context)
+ {
+ 	struct rpcrdma_ep *ep = (struct rpcrdma_ep *)cq_context;
+ 	int rc;
+ 
+ 	rc = rpcrdma_sendcq_poll(cq, ep);
+ 	if (rc) {
+ 		dprintk("RPC:       %s: ib_poll_cq failed: %i\n",
+ 			__func__, rc);
+ 		return;
+ 	}
+ 
+ 	rc = ib_req_notify_cq(cq,
+ 			IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);
+ 	if (rc == 0)
+ 		return;
+ 	if (rc < 0) {
+ 		dprintk("RPC:       %s: ib_req_notify_cq failed: %i\n",
+ 			__func__, rc);
+ 		return;
+ 	}
+ 
+ 	rpcrdma_sendcq_poll(cq, ep);
+ }
+ 
+ static void
+ rpcrdma_recvcq_process_wc(struct ib_wc *wc)
++>>>>>>> 9f9d802a28a1 (xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect)
  {
 +	struct rpcrdma_mw *frmr;
  	struct rpcrdma_rep *rep =
 -			(struct rpcrdma_rep *)(unsigned long)wc->wr_id;
 +			(struct rpcrdma_rep *)(unsigned long) wc->wr_id;
  
 -	dprintk("RPC:       %s: rep %p status %X opcode %X length %u\n",
 +	dprintk("RPC:       %s: event rep %p status %X opcode %X length %u\n",
  		__func__, rep, wc->status, wc->opcode, wc->byte_len);
  
 -	if (wc->status != IB_WC_SUCCESS) {
 +	if (!rep) /* send or bind completion that we don't care about */
 +		return;
 +
 +	if (IB_WC_SUCCESS != wc->status) {
 +		dprintk("RPC:       %s: WC opcode %d status %X, connection lost\n",
 +			__func__, wc->opcode, wc->status);
  		rep->rr_len = ~0U;
 -		goto out_schedule;
 -	}
 -	if (wc->opcode != IB_WC_RECV)
 +		if (wc->opcode != IB_WC_FAST_REG_MR && wc->opcode != IB_WC_LOCAL_INV)
 +			rpcrdma_schedule_tasklet(rep);
  		return;
 +	}
  
 -	rep->rr_len = wc->byte_len;
 -	ib_dma_sync_single_for_cpu(rdmab_to_ia(rep->rr_buffer)->ri_id->device,
 +	switch (wc->opcode) {
 +	case IB_WC_FAST_REG_MR:
 +		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
 +		frmr->r.frmr.state = FRMR_IS_VALID;
 +		break;
 +	case IB_WC_LOCAL_INV:
 +		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
 +		frmr->r.frmr.state = FRMR_IS_INVALID;
 +		break;
 +	case IB_WC_RECV:
 +		rep->rr_len = wc->byte_len;
 +		ib_dma_sync_single_for_cpu(
 +			rdmab_to_ia(rep->rr_buffer)->ri_id->device,
  			rep->rr_iov.addr, rep->rr_len, DMA_FROM_DEVICE);
 -
 -	if (rep->rr_len >= 16) {
 -		struct rpcrdma_msg *p = (struct rpcrdma_msg *)rep->rr_base;
 -		unsigned int credits = ntohl(p->rm_credit);
 -
 -		if (credits == 0)
 -			credits = 1;	/* don't deadlock */
 -		else if (credits > rep->rr_buffer->rb_max_requests)
 -			credits = rep->rr_buffer->rb_max_requests;
 -		atomic_set(&rep->rr_buffer->rb_credits, credits);
 +		/* Keep (only) the most recent credits, after check validity */
 +		if (rep->rr_len >= 16) {
 +			struct rpcrdma_msg *p =
 +					(struct rpcrdma_msg *) rep->rr_base;
 +			unsigned int credits = ntohl(p->rm_credit);
 +			if (credits == 0) {
 +				dprintk("RPC:       %s: server"
 +					" dropped credits to 0!\n", __func__);
 +				/* don't deadlock */
 +				credits = 1;
 +			} else if (credits > rep->rr_buffer->rb_max_requests) {
 +				dprintk("RPC:       %s: server"
 +					" over-crediting: %d (%d)\n",
 +					__func__, credits,
 +					rep->rr_buffer->rb_max_requests);
 +				credits = rep->rr_buffer->rb_max_requests;
 +			}
 +			atomic_set(&rep->rr_buffer->rb_credits, credits);
 +		}
 +		/* fall through */
 +	case IB_WC_BIND_MW:
 +		rpcrdma_schedule_tasklet(rep);
 +		break;
 +	default:
 +		dprintk("RPC:       %s: unexpected WC event %X\n",
 +			__func__, wc->opcode);
 +		break;
  	}
 -
 -out_schedule:
 -	rpcrdma_schedule_tasklet(rep);
  }
  
 -static int
 -rpcrdma_recvcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)
 +static inline int
 +rpcrdma_cq_poll(struct ib_cq *cq)
  {
 -	struct ib_wc *wcs;
 -	int budget, count, rc;
 -
 -	budget = RPCRDMA_WC_BUDGET / RPCRDMA_POLLSIZE;
 -	do {
 -		wcs = ep->rep_recv_wcs;
 +	struct ib_wc wc;
 +	int rc;
  
 -		rc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);
 -		if (rc <= 0)
 +	for (;;) {
 +		rc = ib_poll_cq(cq, 1, &wc);
 +		if (rc < 0) {
 +			dprintk("RPC:       %s: ib_poll_cq failed %i\n",
 +				__func__, rc);
  			return rc;
 +		}
 +		if (rc == 0)
 +			break;
 +
 +		rpcrdma_event_process(&wc);
 +	}
  
 -		count = rc;
 -		while (count-- > 0)
 -			rpcrdma_recvcq_process_wc(wcs++);
 -	} while (rc == RPCRDMA_POLLSIZE && --budget);
  	return 0;
  }
  
@@@ -855,8 -883,11 +938,11 @@@ retry
  		if (rc && rc != -ENOTCONN)
  			dprintk("RPC:       %s: rpcrdma_ep_disconnect"
  				" status %i\n", __func__, rc);
 -		rpcrdma_flush_cqs(ep);
 +		rpcrdma_clean_cq(ep->rep_cq);
  
+ 		if (ia->ri_memreg_strategy == RPCRDMA_FRMR)
+ 			rpcrdma_reset_frmrs(ia);
+ 
  		xprt = container_of(ia, struct rpcrdma_xprt, rx_ia);
  		id = rpcrdma_create_id(xprt, ia,
  				(struct sockaddr *)&xprt->rx_data.addr);
@@@ -1261,6 -1235,151 +1347,126 @@@ rpcrdma_buffer_destroy(struct rpcrdma_b
  	kfree(buf->rb_pool);
  }
  
++<<<<<<< HEAD
++=======
+ /* After a disconnect, a flushed FAST_REG_MR can leave an FRMR in
+  * an unusable state. Find FRMRs in this state and dereg / reg
+  * each.  FRMRs that are VALID and attached to an rpcrdma_req are
+  * also torn down.
+  *
+  * This gives all in-use FRMRs a fresh rkey and leaves them INVALID.
+  *
+  * This is invoked only in the transport connect worker in order
+  * to serialize with rpcrdma_register_frmr_external().
+  */
+ static void
+ rpcrdma_reset_frmrs(struct rpcrdma_ia *ia)
+ {
+ 	struct rpcrdma_xprt *r_xprt =
+ 				container_of(ia, struct rpcrdma_xprt, rx_ia);
+ 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
+ 	struct list_head *pos;
+ 	struct rpcrdma_mw *r;
+ 	int rc;
+ 
+ 	list_for_each(pos, &buf->rb_all) {
+ 		r = list_entry(pos, struct rpcrdma_mw, mw_all);
+ 
+ 		if (r->r.frmr.fr_state == FRMR_IS_INVALID)
+ 			continue;
+ 
+ 		rc = ib_dereg_mr(r->r.frmr.fr_mr);
+ 		if (rc)
+ 			dprintk("RPC:       %s: ib_dereg_mr failed %i\n",
+ 				__func__, rc);
+ 		ib_free_fast_reg_page_list(r->r.frmr.fr_pgl);
+ 
+ 		r->r.frmr.fr_mr = ib_alloc_fast_reg_mr(ia->ri_pd,
+ 					ia->ri_max_frmr_depth);
+ 		if (IS_ERR(r->r.frmr.fr_mr)) {
+ 			rc = PTR_ERR(r->r.frmr.fr_mr);
+ 			dprintk("RPC:       %s: ib_alloc_fast_reg_mr"
+ 				" failed %i\n", __func__, rc);
+ 			continue;
+ 		}
+ 		r->r.frmr.fr_pgl = ib_alloc_fast_reg_page_list(
+ 					ia->ri_id->device,
+ 					ia->ri_max_frmr_depth);
+ 		if (IS_ERR(r->r.frmr.fr_pgl)) {
+ 			rc = PTR_ERR(r->r.frmr.fr_pgl);
+ 			dprintk("RPC:       %s: "
+ 				"ib_alloc_fast_reg_page_list "
+ 				"failed %i\n", __func__, rc);
+ 
+ 			ib_dereg_mr(r->r.frmr.fr_mr);
+ 			continue;
+ 		}
+ 		r->r.frmr.fr_state = FRMR_IS_INVALID;
+ 	}
+ }
+ 
+ /* "*mw" can be NULL when rpcrdma_buffer_get_mrs() fails, leaving
+  * some req segments uninitialized.
+  */
+ static void
+ rpcrdma_buffer_put_mr(struct rpcrdma_mw **mw, struct rpcrdma_buffer *buf)
+ {
+ 	if (*mw) {
+ 		list_add_tail(&(*mw)->mw_list, &buf->rb_mws);
+ 		*mw = NULL;
+ 	}
+ }
+ 
+ /* Cycle mw's back in reverse order, and "spin" them.
+  * This delays and scrambles reuse as much as possible.
+  */
+ static void
+ rpcrdma_buffer_put_mrs(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)
+ {
+ 	struct rpcrdma_mr_seg *seg = req->rl_segments;
+ 	struct rpcrdma_mr_seg *seg1 = seg;
+ 	int i;
+ 
+ 	for (i = 1, seg++; i < RPCRDMA_MAX_SEGS; seg++, i++)
+ 		rpcrdma_buffer_put_mr(&seg->mr_chunk.rl_mw, buf);
+ 	rpcrdma_buffer_put_mr(&seg1->mr_chunk.rl_mw, buf);
+ }
+ 
+ static void
+ rpcrdma_buffer_put_sendbuf(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)
+ {
+ 	buf->rb_send_bufs[--buf->rb_send_index] = req;
+ 	req->rl_niovs = 0;
+ 	if (req->rl_reply) {
+ 		buf->rb_recv_bufs[--buf->rb_recv_index] = req->rl_reply;
+ 		req->rl_reply->rr_func = NULL;
+ 		req->rl_reply = NULL;
+ 	}
+ }
+ 
+ static struct rpcrdma_req *
+ rpcrdma_buffer_get_mrs(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)
+ {
+ 	struct rpcrdma_mw *r;
+ 	int i;
+ 
+ 	i = RPCRDMA_MAX_SEGS - 1;
+ 	while (!list_empty(&buf->rb_mws)) {
+ 		r = list_entry(buf->rb_mws.next,
+ 			       struct rpcrdma_mw, mw_list);
+ 		list_del(&r->mw_list);
+ 		req->rl_segments[i].mr_chunk.rl_mw = r;
+ 		if (unlikely(i-- == 0))
+ 			return req;	/* Success */
+ 	}
+ 
+ 	/* Not enough entries on rb_mws for this req */
+ 	rpcrdma_buffer_put_sendbuf(req, buf);
+ 	rpcrdma_buffer_put_mrs(req, buf);
+ 	return NULL;
+ }
+ 
++>>>>>>> 9f9d802a28a1 (xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect)
  /*
   * Get a set of request/reply buffers.
   *
@@@ -1541,15 -1636,14 +1747,19 @@@ rpcrdma_register_frmr_external(struct r
  			break;
  	}
  	dprintk("RPC:       %s: Using frmr %p to map %d segments\n",
 -		__func__, mw, i);
 +		__func__, seg1->mr_chunk.rl_mw, i);
  
++<<<<<<< HEAD
 +	if (unlikely(seg1->mr_chunk.rl_mw->r.frmr.state == FRMR_IS_VALID)) {
++=======
+ 	if (unlikely(frmr->fr_state != FRMR_IS_INVALID)) {
++>>>>>>> 9f9d802a28a1 (xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect)
  		dprintk("RPC:       %s: frmr %x left valid, posting invalidate.\n",
 -			__func__, mr->rkey);
 +			__func__,
 +			seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey);
  		/* Invalidate before using. */
  		memset(&invalidate_wr, 0, sizeof invalidate_wr);
 -		invalidate_wr.wr_id = (unsigned long)(void *)mw;
 +		invalidate_wr.wr_id = (unsigned long)(void *)seg1->mr_chunk.rl_mw;
  		invalidate_wr.next = &frmr_wr;
  		invalidate_wr.opcode = IB_WR_LOCAL_INV;
  		invalidate_wr.send_flags = IB_SEND_SIGNALED;
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index 4ef6e3f9b67c,1ee6db30abc5..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -135,6 -146,40 +135,43 @@@ struct rpcrdma_rep 
  };
  
  /*
++<<<<<<< HEAD
++=======
+  * struct rpcrdma_mw - external memory region metadata
+  *
+  * An external memory region is any buffer or page that is registered
+  * on the fly (ie, not pre-registered).
+  *
+  * Each rpcrdma_buffer has a list of free MWs anchored in rb_mws. During
+  * call_allocate, rpcrdma_buffer_get() assigns one to each segment in
+  * an rpcrdma_req. Then rpcrdma_register_external() grabs these to keep
+  * track of registration metadata while each RPC is pending.
+  * rpcrdma_deregister_external() uses this metadata to unmap and
+  * release these resources when an RPC is complete.
+  */
+ enum rpcrdma_frmr_state {
+ 	FRMR_IS_INVALID,	/* ready to be used */
+ 	FRMR_IS_VALID,		/* in use */
+ 	FRMR_IS_STALE,		/* failed completion */
+ };
+ 
+ struct rpcrdma_frmr {
+ 	struct ib_fast_reg_page_list	*fr_pgl;
+ 	struct ib_mr			*fr_mr;
+ 	enum rpcrdma_frmr_state		fr_state;
+ };
+ 
+ struct rpcrdma_mw {
+ 	union {
+ 		struct ib_fmr		*fmr;
+ 		struct rpcrdma_frmr	frmr;
+ 	} r;
+ 	struct list_head	mw_list;
+ 	struct list_head	mw_all;
+ };
+ 
+ /*
++>>>>>>> 9f9d802a28a1 (xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect)
   * struct rpcrdma_req -- structure central to the request/reply sequence.
   *
   * N of these are associated with a transport instance, and stored in
* Unmerged path net/sunrpc/xprtrdma/verbs.c
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
