uninline destroy_super(), consolidate alloc_super()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [fs] super: uninline destroy_super(), consolidate alloc_super() (Gustavo Duarte) [1112805]
Rebuild_FUZZ: 93.58%
commit-author Al Viro <viro@zeniv.linux.org.uk>
commit 7eb5e8826911f2792179f99e77e75fbb7ef53a4a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/7eb5e882.failed

	Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
(cherry picked from commit 7eb5e8826911f2792179f99e77e75fbb7ef53a4a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/super.c
diff --cc fs/super.c
index 68307c029228,efa6e488a95c..000000000000
--- a/fs/super.c
+++ b/fs/super.c
@@@ -150,106 -164,83 +144,175 @@@ static struct super_block *alloc_super(
  {
  	struct super_block *s = kzalloc(sizeof(struct super_block),  GFP_USER);
  	static const struct super_operations default_op;
+ 	int i;
  
++<<<<<<< HEAD
 +	if (s) {
 +		if (security_sb_alloc(s)) {
 +			/*
 +			 * We cannot call security_sb_free() without
 +			 * security_sb_alloc() succeeding. So bail out manually
 +			 */
 +			kfree(s);
 +			s = NULL;
 +			goto out;
 +		}
- #ifdef CONFIG_SMP
- 		s->s_files = alloc_percpu(struct list_head);
- 		if (!s->s_files)
- 			goto err_out;
- 		else {
- 			int i;
++=======
+ 	if (!s)
+ 		return NULL;
  
- 			for_each_possible_cpu(i)
- 				INIT_LIST_HEAD(per_cpu_ptr(s->s_files, i));
- 		}
+ 	if (security_sb_alloc(s))
+ 		goto fail;
+ 
++>>>>>>> 7eb5e8826911 (uninline destroy_super(), consolidate alloc_super())
+ #ifdef CONFIG_SMP
+ 	s->s_files = alloc_percpu(struct list_head);
+ 	if (!s->s_files)
+ 		goto fail;
+ 	for_each_possible_cpu(i)
+ 		INIT_LIST_HEAD(per_cpu_ptr(s->s_files, i));
  #else
- 		INIT_LIST_HEAD(&s->s_files);
+ 	INIT_LIST_HEAD(&s->s_files);
  #endif
++<<<<<<< HEAD
 +		if (init_sb_writers(s, type))
 +			goto err_out;
 +		s->s_flags = flags;
 +		s->s_bdi = &default_backing_dev_info;
 +		INIT_HLIST_NODE(&s->s_instances);
 +		INIT_HLIST_BL_HEAD(&s->s_anon);
 +		INIT_LIST_HEAD(&s->s_inodes);
 +		INIT_LIST_HEAD(&s->s_dentry_lru);
 +		INIT_LIST_HEAD(&s->s_inode_lru);
 +		spin_lock_init(&s->s_inode_lru_lock);
 +		INIT_LIST_HEAD(&s->s_mounts);
 +		init_rwsem(&s->s_umount);
 +		lockdep_set_class(&s->s_umount, &type->s_umount_key);
 +		/*
 +		 * sget() can have s_umount recursion.
 +		 *
 +		 * When it cannot find a suitable sb, it allocates a new
 +		 * one (this one), and tries again to find a suitable old
 +		 * one.
 +		 *
 +		 * In case that succeeds, it will acquire the s_umount
 +		 * lock of the old one. Since these are clearly distrinct
 +		 * locks, and this object isn't exposed yet, there's no
 +		 * risk of deadlocks.
 +		 *
 +		 * Annotate this by putting this lock in a different
 +		 * subclass.
 +		 */
 +		down_write_nested(&s->s_umount, SINGLE_DEPTH_NESTING);
 +		s->s_count = 1;
 +		atomic_set(&s->s_active, 1);
 +		mutex_init(&s->s_vfs_rename_mutex);
 +		lockdep_set_class(&s->s_vfs_rename_mutex, &type->s_vfs_rename_key);
 +		mutex_init(&s->s_dquot.dqio_mutex);
 +		mutex_init(&s->s_dquot.dqonoff_mutex);
 +		init_rwsem(&s->s_dquot.dqptr_sem);
 +		s->s_maxbytes = MAX_NON_LFS;
 +		s->s_op = &default_op;
 +		s->s_time_gran = 1000000000;
 +		s->cleancache_poolid = -1;
 +
 +		s->s_shrink.seeks = DEFAULT_SEEKS;
 +		s->s_shrink.shrink = prune_super;
 +		s->s_shrink.batch = 1024;
++=======
+ 	for (i = 0; i < SB_FREEZE_LEVELS; i++) {
+ 		if (percpu_counter_init(&s->s_writers.counter[i], 0) < 0)
+ 			goto fail;
+ 		lockdep_init_map(&s->s_writers.lock_map[i], sb_writers_name[i],
+ 				 &type->s_writers_key[i], 0);
++>>>>>>> 7eb5e8826911 (uninline destroy_super(), consolidate alloc_super())
  	}
- out:
+ 	init_waitqueue_head(&s->s_writers.wait);
+ 	init_waitqueue_head(&s->s_writers.wait_unfrozen);
+ 	s->s_flags = flags;
+ 	s->s_bdi = &default_backing_dev_info;
+ 	INIT_HLIST_NODE(&s->s_instances);
+ 	INIT_HLIST_BL_HEAD(&s->s_anon);
+ 	INIT_LIST_HEAD(&s->s_inodes);
+ 
+ 	if (list_lru_init(&s->s_dentry_lru))
+ 		goto fail;
+ 	if (list_lru_init(&s->s_inode_lru))
+ 		goto fail;
+ 
+ 	INIT_LIST_HEAD(&s->s_mounts);
+ 	init_rwsem(&s->s_umount);
+ 	lockdep_set_class(&s->s_umount, &type->s_umount_key);
+ 	/*
+ 	 * sget() can have s_umount recursion.
+ 	 *
+ 	 * When it cannot find a suitable sb, it allocates a new
+ 	 * one (this one), and tries again to find a suitable old
+ 	 * one.
+ 	 *
+ 	 * In case that succeeds, it will acquire the s_umount
+ 	 * lock of the old one. Since these are clearly distrinct
+ 	 * locks, and this object isn't exposed yet, there's no
+ 	 * risk of deadlocks.
+ 	 *
+ 	 * Annotate this by putting this lock in a different
+ 	 * subclass.
+ 	 */
+ 	down_write_nested(&s->s_umount, SINGLE_DEPTH_NESTING);
+ 	s->s_count = 1;
+ 	atomic_set(&s->s_active, 1);
+ 	mutex_init(&s->s_vfs_rename_mutex);
+ 	lockdep_set_class(&s->s_vfs_rename_mutex, &type->s_vfs_rename_key);
+ 	mutex_init(&s->s_dquot.dqio_mutex);
+ 	mutex_init(&s->s_dquot.dqonoff_mutex);
+ 	init_rwsem(&s->s_dquot.dqptr_sem);
+ 	s->s_maxbytes = MAX_NON_LFS;
+ 	s->s_op = &default_op;
+ 	s->s_time_gran = 1000000000;
+ 	s->cleancache_poolid = -1;
+ 
+ 	s->s_shrink.seeks = DEFAULT_SEEKS;
+ 	s->s_shrink.scan_objects = super_cache_scan;
+ 	s->s_shrink.count_objects = super_cache_count;
+ 	s->s_shrink.batch = 1024;
+ 	s->s_shrink.flags = SHRINKER_NUMA_AWARE;
  	return s;
++<<<<<<< HEAD
 +err_out:
 +	security_sb_free(s);
 +#ifdef CONFIG_SMP
 +	if (s->s_files)
 +		free_percpu(s->s_files);
 +#endif
 +	destroy_sb_writers(s);
 +	kfree(s);
 +	s = NULL;
 +	goto out;
 +}
 +
 +/**
 + *	destroy_super	-	frees a superblock
 + *	@s: superblock to free
 + *
 + *	Frees a superblock.
 + */
 +static inline void destroy_super(struct super_block *s)
 +{
 +#ifdef CONFIG_SMP
 +	free_percpu(s->s_files);
 +#endif
 +	destroy_sb_writers(s);
 +	security_sb_free(s);
 +	WARN_ON(!list_empty(&s->s_mounts));
 +	kfree(s->s_subtype);
 +	kfree(s->s_options);
 +	kfree(s);
++=======
+ 
+ fail:
+ 	destroy_super(s);
+ 	return NULL;
++>>>>>>> 7eb5e8826911 (uninline destroy_super(), consolidate alloc_super())
  }
  
  /* Superblock refcounting  */
* Unmerged path fs/super.c
