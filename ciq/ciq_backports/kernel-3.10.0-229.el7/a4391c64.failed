blk-mq: bump max tag depth to 10K tags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Jens Axboe <axboe@fb.com>
commit a4391c6465d9c978fd4bded12e34bdde3f5458f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/a4391c64.failed

For some scsi-mq cases, the tag map can be huge. So increase the
max number of tags we support.

Additionally, don't fail with EINVAL if a user requests too many
tags. Warn that the tag depth has been adjusted down, and store
the new value inside the tag_set passed in.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit a4391c6465d9c978fd4bded12e34bdde3f5458f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index f1f3d27fe9e1,a6ee74e27957..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1470,6 -1967,92 +1470,95 @@@ static int __cpuinit blk_mq_queue_reini
  	return NOTIFY_OK;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Alloc a tag set to be associated with one or more request queues.
+  * May fail with EINVAL for various error conditions. May adjust the
+  * requested depth down, if if it too large. In that case, the set
+  * value will be stored in set->queue_depth.
+  */
+ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
+ {
+ 	int i;
+ 
+ 	if (!set->nr_hw_queues)
+ 		return -EINVAL;
+ 	if (!set->queue_depth)
+ 		return -EINVAL;
+ 	if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)
+ 		return -EINVAL;
+ 
+ 	if (!set->nr_hw_queues || !set->ops->queue_rq || !set->ops->map_queue)
+ 		return -EINVAL;
+ 
+ 	if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+ 		pr_info("blk-mq: reduced tag depth to %u\n",
+ 			BLK_MQ_MAX_DEPTH);
+ 		set->queue_depth = BLK_MQ_MAX_DEPTH;
+ 	}
+ 
+ 	set->tags = kmalloc_node(set->nr_hw_queues *
+ 				 sizeof(struct blk_mq_tags *),
+ 				 GFP_KERNEL, set->numa_node);
+ 	if (!set->tags)
+ 		goto out;
+ 
+ 	for (i = 0; i < set->nr_hw_queues; i++) {
+ 		set->tags[i] = blk_mq_init_rq_map(set, i);
+ 		if (!set->tags[i])
+ 			goto out_unwind;
+ 	}
+ 
+ 	mutex_init(&set->tag_list_lock);
+ 	INIT_LIST_HEAD(&set->tag_list);
+ 
+ 	return 0;
+ 
+ out_unwind:
+ 	while (--i >= 0)
+ 		blk_mq_free_rq_map(set, set->tags[i], i);
+ out:
+ 	return -ENOMEM;
+ }
+ EXPORT_SYMBOL(blk_mq_alloc_tag_set);
+ 
+ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < set->nr_hw_queues; i++) {
+ 		if (set->tags[i])
+ 			blk_mq_free_rq_map(set, set->tags[i], i);
+ 	}
+ 
+ 	kfree(set->tags);
+ }
+ EXPORT_SYMBOL(blk_mq_free_tag_set);
+ 
+ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
+ {
+ 	struct blk_mq_tag_set *set = q->tag_set;
+ 	struct blk_mq_hw_ctx *hctx;
+ 	int i, ret;
+ 
+ 	if (!set || nr > set->queue_depth)
+ 		return -EINVAL;
+ 
+ 	ret = 0;
+ 	queue_for_each_hw_ctx(q, hctx, i) {
+ 		ret = blk_mq_tag_update_depth(hctx->tags, nr);
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	if (!ret)
+ 		q->nr_requests = nr;
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> a4391c6465d9 (blk-mq: bump max tag depth to 10K tags)
  void blk_mq_disable_hotplug(void)
  {
  	mutex_lock(&all_q_mutex);
diff --cc include/linux/blk-mq.h
index 0f2259d5e784,a002cf191427..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -108,16 -128,24 +108,22 @@@ enum 
  
  	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
  	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
 -	BLK_MQ_F_TAG_SHARED	= 1 << 2,
 -	BLK_MQ_F_SG_MERGE	= 1 << 3,
 -	BLK_MQ_F_SYSFS_UP	= 1 << 4,
  
  	BLK_MQ_S_STOPPED	= 0,
 -	BLK_MQ_S_TAG_ACTIVE	= 1,
  
++<<<<<<< HEAD
 +	BLK_MQ_MAX_DEPTH	= 2048,
++=======
+ 	BLK_MQ_MAX_DEPTH	= 10240,
+ 
+ 	BLK_MQ_CPU_WORK_BATCH	= 8,
++>>>>>>> a4391c6465d9 (blk-mq: bump max tag depth to 10K tags)
  };
  
 -struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 +struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
  int blk_mq_register_disk(struct gendisk *);
  void blk_mq_unregister_disk(struct gendisk *);
 -
 -int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);
 -void blk_mq_free_tag_set(struct blk_mq_tag_set *set);
 +void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
  
  void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
  
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
