KVM: irqchip: Provide and use accessors for irq routing table

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/irqchip: Provide and use accessors for irq routing table (David Gibson) [1123145 1123133 1123367]
Rebuild_FUZZ: 97.52%
commit-author Paul Mackerras <paulus@samba.org>
commit 8ba918d488caded2c4368b0b922eb905fe3bb101
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/8ba918d4.failed

This provides accessor functions for the KVM interrupt mappings, in
order to reduce the amount of code that accesses the fields of the
kvm_irq_routing_table struct, and restrict that code to one file,
virt/kvm/irqchip.c.  The new functions are kvm_irq_map_gsi(), which
maps from a global interrupt number to a set of IRQ routing entries,
and kvm_irq_map_chip_pin, which maps from IRQ chip and pin numbers to
a global interrupt number.

This also moves the update of kvm_irq_routing_table::chip[][]
into irqchip.c, out of the various kvm_set_routing_entry
implementations.  That means that none of the kvm_set_routing_entry
implementations need the kvm_irq_routing_table argument anymore,
so this removes it.

This does not change any locking or data lifetime rules.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Tested-by: Eric Auger <eric.auger@linaro.org>
	Tested-by: Cornelia Huck <cornelia.huck@de.ibm.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8ba918d488caded2c4368b0b922eb905fe3bb101)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kvm/interrupt.c
diff --cc arch/s390/kvm/interrupt.c
index 7f1f7ac5cf7f,f4c819bfc193..000000000000
--- a/arch/s390/kvm/interrupt.c
+++ b/arch/s390/kvm/interrupt.c
@@@ -832,9 -1059,527 +832,531 @@@ int kvm_s390_inject_vcpu(struct kvm_vcp
  	if (inti->type == KVM_S390_SIGP_STOP)
  		li->action_bits |= ACTION_STOP_ON_STOP;
  	atomic_set_mask(CPUSTAT_EXT_INT, li->cpuflags);
 -	spin_unlock(&li->lock);
 +	if (waitqueue_active(&vcpu->wq))
 +		wake_up_interruptible(&vcpu->wq);
 +	spin_unlock_bh(&li->lock);
  	mutex_unlock(&vcpu->kvm->lock);
 -	kvm_s390_vcpu_wakeup(vcpu);
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ void kvm_s390_clear_float_irqs(struct kvm *kvm)
+ {
+ 	struct kvm_s390_float_interrupt *fi;
+ 	struct kvm_s390_interrupt_info	*n, *inti = NULL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	fi = &kvm->arch.float_int;
+ 	spin_lock(&fi->lock);
+ 	list_for_each_entry_safe(inti, n, &fi->list, list) {
+ 		list_del(&inti->list);
+ 		kfree(inti);
+ 	}
+ 	fi->irq_count = 0;
+ 	atomic_set(&fi->active, 0);
+ 	spin_unlock(&fi->lock);
+ 	mutex_unlock(&kvm->lock);
+ }
+ 
+ static inline int copy_irq_to_user(struct kvm_s390_interrupt_info *inti,
+ 				   u8 *addr)
+ {
+ 	struct kvm_s390_irq __user *uptr = (struct kvm_s390_irq __user *) addr;
+ 	struct kvm_s390_irq irq = {0};
+ 
+ 	irq.type = inti->type;
+ 	switch (inti->type) {
+ 	case KVM_S390_INT_PFAULT_INIT:
+ 	case KVM_S390_INT_PFAULT_DONE:
+ 	case KVM_S390_INT_VIRTIO:
+ 	case KVM_S390_INT_SERVICE:
+ 		irq.u.ext = inti->ext;
+ 		break;
+ 	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
+ 		irq.u.io = inti->io;
+ 		break;
+ 	case KVM_S390_MCHK:
+ 		irq.u.mchk = inti->mchk;
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (copy_to_user(uptr, &irq, sizeof(irq)))
+ 		return -EFAULT;
+ 
+ 	return 0;
+ }
+ 
+ static int get_all_floating_irqs(struct kvm *kvm, __u8 *buf, __u64 len)
+ {
+ 	struct kvm_s390_interrupt_info *inti;
+ 	struct kvm_s390_float_interrupt *fi;
+ 	int ret = 0;
+ 	int n = 0;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	fi = &kvm->arch.float_int;
+ 	spin_lock(&fi->lock);
+ 
+ 	list_for_each_entry(inti, &fi->list, list) {
+ 		if (len < sizeof(struct kvm_s390_irq)) {
+ 			/* signal userspace to try again */
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 		ret = copy_irq_to_user(inti, buf);
+ 		if (ret)
+ 			break;
+ 		buf += sizeof(struct kvm_s390_irq);
+ 		len -= sizeof(struct kvm_s390_irq);
+ 		n++;
+ 	}
+ 
+ 	spin_unlock(&fi->lock);
+ 	mutex_unlock(&kvm->lock);
+ 
+ 	return ret < 0 ? ret : n;
+ }
+ 
+ static int flic_get_attr(struct kvm_device *dev, struct kvm_device_attr *attr)
+ {
+ 	int r;
+ 
+ 	switch (attr->group) {
+ 	case KVM_DEV_FLIC_GET_ALL_IRQS:
+ 		r = get_all_floating_irqs(dev->kvm, (u8 *) attr->addr,
+ 					  attr->attr);
+ 		break;
+ 	default:
+ 		r = -EINVAL;
+ 	}
+ 
+ 	return r;
+ }
+ 
+ static inline int copy_irq_from_user(struct kvm_s390_interrupt_info *inti,
+ 				     u64 addr)
+ {
+ 	struct kvm_s390_irq __user *uptr = (struct kvm_s390_irq __user *) addr;
+ 	void *target = NULL;
+ 	void __user *source;
+ 	u64 size;
+ 
+ 	if (get_user(inti->type, (u64 __user *)addr))
+ 		return -EFAULT;
+ 
+ 	switch (inti->type) {
+ 	case KVM_S390_INT_PFAULT_INIT:
+ 	case KVM_S390_INT_PFAULT_DONE:
+ 	case KVM_S390_INT_VIRTIO:
+ 	case KVM_S390_INT_SERVICE:
+ 		target = (void *) &inti->ext;
+ 		source = &uptr->u.ext;
+ 		size = sizeof(inti->ext);
+ 		break;
+ 	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
+ 		target = (void *) &inti->io;
+ 		source = &uptr->u.io;
+ 		size = sizeof(inti->io);
+ 		break;
+ 	case KVM_S390_MCHK:
+ 		target = (void *) &inti->mchk;
+ 		source = &uptr->u.mchk;
+ 		size = sizeof(inti->mchk);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (copy_from_user(target, source, size))
+ 		return -EFAULT;
+ 
+ 	return 0;
+ }
+ 
+ static int enqueue_floating_irq(struct kvm_device *dev,
+ 				struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_interrupt_info *inti = NULL;
+ 	int r = 0;
+ 	int len = attr->attr;
+ 
+ 	if (len % sizeof(struct kvm_s390_irq) != 0)
+ 		return -EINVAL;
+ 	else if (len > KVM_S390_FLIC_MAX_BUFFER)
+ 		return -EINVAL;
+ 
+ 	while (len >= sizeof(struct kvm_s390_irq)) {
+ 		inti = kzalloc(sizeof(*inti), GFP_KERNEL);
+ 		if (!inti)
+ 			return -ENOMEM;
+ 
+ 		r = copy_irq_from_user(inti, attr->addr);
+ 		if (r) {
+ 			kfree(inti);
+ 			return r;
+ 		}
+ 		r = __inject_vm(dev->kvm, inti);
+ 		if (r) {
+ 			kfree(inti);
+ 			return r;
+ 		}
+ 		len -= sizeof(struct kvm_s390_irq);
+ 		attr->addr += sizeof(struct kvm_s390_irq);
+ 	}
+ 
+ 	return r;
+ }
+ 
+ static struct s390_io_adapter *get_io_adapter(struct kvm *kvm, unsigned int id)
+ {
+ 	if (id >= MAX_S390_IO_ADAPTERS)
+ 		return NULL;
+ 	return kvm->arch.adapters[id];
+ }
+ 
+ static int register_io_adapter(struct kvm_device *dev,
+ 			       struct kvm_device_attr *attr)
+ {
+ 	struct s390_io_adapter *adapter;
+ 	struct kvm_s390_io_adapter adapter_info;
+ 
+ 	if (copy_from_user(&adapter_info,
+ 			   (void __user *)attr->addr, sizeof(adapter_info)))
+ 		return -EFAULT;
+ 
+ 	if ((adapter_info.id >= MAX_S390_IO_ADAPTERS) ||
+ 	    (dev->kvm->arch.adapters[adapter_info.id] != NULL))
+ 		return -EINVAL;
+ 
+ 	adapter = kzalloc(sizeof(*adapter), GFP_KERNEL);
+ 	if (!adapter)
+ 		return -ENOMEM;
+ 
+ 	INIT_LIST_HEAD(&adapter->maps);
+ 	init_rwsem(&adapter->maps_lock);
+ 	atomic_set(&adapter->nr_maps, 0);
+ 	adapter->id = adapter_info.id;
+ 	adapter->isc = adapter_info.isc;
+ 	adapter->maskable = adapter_info.maskable;
+ 	adapter->masked = false;
+ 	adapter->swap = adapter_info.swap;
+ 	dev->kvm->arch.adapters[adapter->id] = adapter;
+ 
+ 	return 0;
+ }
+ 
+ int kvm_s390_mask_adapter(struct kvm *kvm, unsigned int id, bool masked)
+ {
+ 	int ret;
+ 	struct s390_io_adapter *adapter = get_io_adapter(kvm, id);
+ 
+ 	if (!adapter || !adapter->maskable)
+ 		return -EINVAL;
+ 	ret = adapter->masked;
+ 	adapter->masked = masked;
+ 	return ret;
+ }
+ 
+ static int kvm_s390_adapter_map(struct kvm *kvm, unsigned int id, __u64 addr)
+ {
+ 	struct s390_io_adapter *adapter = get_io_adapter(kvm, id);
+ 	struct s390_map_info *map;
+ 	int ret;
+ 
+ 	if (!adapter || !addr)
+ 		return -EINVAL;
+ 
+ 	map = kzalloc(sizeof(*map), GFP_KERNEL);
+ 	if (!map) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 	INIT_LIST_HEAD(&map->list);
+ 	map->guest_addr = addr;
+ 	map->addr = gmap_translate(addr, kvm->arch.gmap);
+ 	if (map->addr == -EFAULT) {
+ 		ret = -EFAULT;
+ 		goto out;
+ 	}
+ 	ret = get_user_pages_fast(map->addr, 1, 1, &map->page);
+ 	if (ret < 0)
+ 		goto out;
+ 	BUG_ON(ret != 1);
+ 	down_write(&adapter->maps_lock);
+ 	if (atomic_inc_return(&adapter->nr_maps) < MAX_S390_ADAPTER_MAPS) {
+ 		list_add_tail(&map->list, &adapter->maps);
+ 		ret = 0;
+ 	} else {
+ 		put_page(map->page);
+ 		ret = -EINVAL;
+ 	}
+ 	up_write(&adapter->maps_lock);
+ out:
+ 	if (ret)
+ 		kfree(map);
+ 	return ret;
+ }
+ 
+ static int kvm_s390_adapter_unmap(struct kvm *kvm, unsigned int id, __u64 addr)
+ {
+ 	struct s390_io_adapter *adapter = get_io_adapter(kvm, id);
+ 	struct s390_map_info *map, *tmp;
+ 	int found = 0;
+ 
+ 	if (!adapter || !addr)
+ 		return -EINVAL;
+ 
+ 	down_write(&adapter->maps_lock);
+ 	list_for_each_entry_safe(map, tmp, &adapter->maps, list) {
+ 		if (map->guest_addr == addr) {
+ 			found = 1;
+ 			atomic_dec(&adapter->nr_maps);
+ 			list_del(&map->list);
+ 			put_page(map->page);
+ 			kfree(map);
+ 			break;
+ 		}
+ 	}
+ 	up_write(&adapter->maps_lock);
+ 
+ 	return found ? 0 : -EINVAL;
+ }
+ 
+ void kvm_s390_destroy_adapters(struct kvm *kvm)
+ {
+ 	int i;
+ 	struct s390_map_info *map, *tmp;
+ 
+ 	for (i = 0; i < MAX_S390_IO_ADAPTERS; i++) {
+ 		if (!kvm->arch.adapters[i])
+ 			continue;
+ 		list_for_each_entry_safe(map, tmp,
+ 					 &kvm->arch.adapters[i]->maps, list) {
+ 			list_del(&map->list);
+ 			put_page(map->page);
+ 			kfree(map);
+ 		}
+ 		kfree(kvm->arch.adapters[i]);
+ 	}
+ }
+ 
+ static int modify_io_adapter(struct kvm_device *dev,
+ 			     struct kvm_device_attr *attr)
+ {
+ 	struct kvm_s390_io_adapter_req req;
+ 	struct s390_io_adapter *adapter;
+ 	int ret;
+ 
+ 	if (copy_from_user(&req, (void __user *)attr->addr, sizeof(req)))
+ 		return -EFAULT;
+ 
+ 	adapter = get_io_adapter(dev->kvm, req.id);
+ 	if (!adapter)
+ 		return -EINVAL;
+ 	switch (req.type) {
+ 	case KVM_S390_IO_ADAPTER_MASK:
+ 		ret = kvm_s390_mask_adapter(dev->kvm, req.id, req.mask);
+ 		if (ret > 0)
+ 			ret = 0;
+ 		break;
+ 	case KVM_S390_IO_ADAPTER_MAP:
+ 		ret = kvm_s390_adapter_map(dev->kvm, req.id, req.addr);
+ 		break;
+ 	case KVM_S390_IO_ADAPTER_UNMAP:
+ 		ret = kvm_s390_adapter_unmap(dev->kvm, req.id, req.addr);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int flic_set_attr(struct kvm_device *dev, struct kvm_device_attr *attr)
+ {
+ 	int r = 0;
+ 	unsigned int i;
+ 	struct kvm_vcpu *vcpu;
+ 
+ 	switch (attr->group) {
+ 	case KVM_DEV_FLIC_ENQUEUE:
+ 		r = enqueue_floating_irq(dev, attr);
+ 		break;
+ 	case KVM_DEV_FLIC_CLEAR_IRQS:
+ 		r = 0;
+ 		kvm_s390_clear_float_irqs(dev->kvm);
+ 		break;
+ 	case KVM_DEV_FLIC_APF_ENABLE:
+ 		dev->kvm->arch.gmap->pfault_enabled = 1;
+ 		break;
+ 	case KVM_DEV_FLIC_APF_DISABLE_WAIT:
+ 		dev->kvm->arch.gmap->pfault_enabled = 0;
+ 		/*
+ 		 * Make sure no async faults are in transition when
+ 		 * clearing the queues. So we don't need to worry
+ 		 * about late coming workers.
+ 		 */
+ 		synchronize_srcu(&dev->kvm->srcu);
+ 		kvm_for_each_vcpu(i, vcpu, dev->kvm)
+ 			kvm_clear_async_pf_completion_queue(vcpu);
+ 		break;
+ 	case KVM_DEV_FLIC_ADAPTER_REGISTER:
+ 		r = register_io_adapter(dev, attr);
+ 		break;
+ 	case KVM_DEV_FLIC_ADAPTER_MODIFY:
+ 		r = modify_io_adapter(dev, attr);
+ 		break;
+ 	default:
+ 		r = -EINVAL;
+ 	}
+ 
+ 	return r;
+ }
+ 
+ static int flic_create(struct kvm_device *dev, u32 type)
+ {
+ 	if (!dev)
+ 		return -EINVAL;
+ 	if (dev->kvm->arch.flic)
+ 		return -EINVAL;
+ 	dev->kvm->arch.flic = dev;
+ 	return 0;
+ }
+ 
+ static void flic_destroy(struct kvm_device *dev)
+ {
+ 	dev->kvm->arch.flic = NULL;
+ 	kfree(dev);
+ }
+ 
+ /* s390 floating irq controller (flic) */
+ struct kvm_device_ops kvm_flic_ops = {
+ 	.name = "kvm-flic",
+ 	.get_attr = flic_get_attr,
+ 	.set_attr = flic_set_attr,
+ 	.create = flic_create,
+ 	.destroy = flic_destroy,
+ };
+ 
+ static unsigned long get_ind_bit(__u64 addr, unsigned long bit_nr, bool swap)
+ {
+ 	unsigned long bit;
+ 
+ 	bit = bit_nr + (addr % PAGE_SIZE) * 8;
+ 
+ 	return swap ? (bit ^ (BITS_PER_LONG - 1)) : bit;
+ }
+ 
+ static struct s390_map_info *get_map_info(struct s390_io_adapter *adapter,
+ 					  u64 addr)
+ {
+ 	struct s390_map_info *map;
+ 
+ 	if (!adapter)
+ 		return NULL;
+ 
+ 	list_for_each_entry(map, &adapter->maps, list) {
+ 		if (map->guest_addr == addr)
+ 			return map;
+ 	}
+ 	return NULL;
+ }
+ 
+ static int adapter_indicators_set(struct kvm *kvm,
+ 				  struct s390_io_adapter *adapter,
+ 				  struct kvm_s390_adapter_int *adapter_int)
+ {
+ 	unsigned long bit;
+ 	int summary_set, idx;
+ 	struct s390_map_info *info;
+ 	void *map;
+ 
+ 	info = get_map_info(adapter, adapter_int->ind_addr);
+ 	if (!info)
+ 		return -1;
+ 	map = page_address(info->page);
+ 	bit = get_ind_bit(info->addr, adapter_int->ind_offset, adapter->swap);
+ 	set_bit(bit, map);
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 	mark_page_dirty(kvm, info->guest_addr >> PAGE_SHIFT);
+ 	set_page_dirty_lock(info->page);
+ 	info = get_map_info(adapter, adapter_int->summary_addr);
+ 	if (!info) {
+ 		srcu_read_unlock(&kvm->srcu, idx);
+ 		return -1;
+ 	}
+ 	map = page_address(info->page);
+ 	bit = get_ind_bit(info->addr, adapter_int->summary_offset,
+ 			  adapter->swap);
+ 	summary_set = test_and_set_bit(bit, map);
+ 	mark_page_dirty(kvm, info->guest_addr >> PAGE_SHIFT);
+ 	set_page_dirty_lock(info->page);
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 	return summary_set ? 0 : 1;
+ }
+ 
+ /*
+  * < 0 - not injected due to error
+  * = 0 - coalesced, summary indicator already active
+  * > 0 - injected interrupt
+  */
+ static int set_adapter_int(struct kvm_kernel_irq_routing_entry *e,
+ 			   struct kvm *kvm, int irq_source_id, int level,
+ 			   bool line_status)
+ {
+ 	int ret;
+ 	struct s390_io_adapter *adapter;
+ 
+ 	/* We're only interested in the 0->1 transition. */
+ 	if (!level)
+ 		return 0;
+ 	adapter = get_io_adapter(kvm, e->adapter.adapter_id);
+ 	if (!adapter)
+ 		return -1;
+ 	down_read(&adapter->maps_lock);
+ 	ret = adapter_indicators_set(kvm, adapter, &e->adapter);
+ 	up_read(&adapter->maps_lock);
+ 	if ((ret > 0) && !adapter->masked) {
+ 		struct kvm_s390_interrupt s390int = {
+ 			.type = KVM_S390_INT_IO(1, 0, 0, 0),
+ 			.parm = 0,
+ 			.parm64 = (adapter->isc << 27) | 0x80000000,
+ 		};
+ 		ret = kvm_s390_inject_vm(kvm, &s390int);
+ 		if (ret == 0)
+ 			ret = 1;
+ 	}
+ 	return ret;
+ }
+ 
+ int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
+ 			  const struct kvm_irq_routing_entry *ue)
+ {
+ 	int ret;
+ 
+ 	switch (ue->type) {
+ 	case KVM_IRQ_ROUTING_S390_ADAPTER:
+ 		e->set = set_adapter_int;
+ 		e->adapter.summary_addr = ue->u.adapter.summary_addr;
+ 		e->adapter.ind_addr = ue->u.adapter.ind_addr;
+ 		e->adapter.summary_offset = ue->u.adapter.summary_offset;
+ 		e->adapter.ind_offset = ue->u.adapter.ind_offset;
+ 		e->adapter.adapter_id = ue->u.adapter.adapter_id;
+ 		ret = 0;
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm,
+ 		int irq_source_id, int level, bool line_status)
+ {
+ 	return -EINVAL;
+ }
++>>>>>>> 8ba918d488ca (KVM: irqchip: Provide and use accessors for irq routing table)
diff --git a/arch/powerpc/kvm/mpic.c b/arch/powerpc/kvm/mpic.c
index efbd9962a209..bb164860f832 100644
--- a/arch/powerpc/kvm/mpic.c
+++ b/arch/powerpc/kvm/mpic.c
@@ -1823,8 +1823,7 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	return 0;
 }
 
-int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
-			  struct kvm_kernel_irq_routing_entry *e,
+int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
 {
 	int r = -EINVAL;
@@ -1836,7 +1835,6 @@ int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
 		e->irqchip.pin = ue->u.irqchip.pin;
 		if (e->irqchip.pin >= KVM_IRQCHIP_NUM_PINS)
 			goto out;
-		rt->chip[ue->u.irqchip.irqchip][e->irqchip.pin] = ue->gsi;
 		break;
 	case KVM_IRQ_ROUTING_MSI:
 		e->set = kvm_set_msi;
* Unmerged path arch/s390/kvm/interrupt.c
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bfb5f6dbf6ea..99283ff84cdf 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -740,6 +740,11 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 			     bool mask);
 
+int kvm_irq_map_gsi(struct kvm_kernel_irq_routing_entry *entries,
+		    struct kvm_irq_routing_table *irq_rt, int gsi);
+int kvm_irq_map_chip_pin(struct kvm_irq_routing_table *irq_rt,
+			 unsigned irqchip, unsigned pin);
+
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status);
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
@@ -919,8 +924,7 @@ int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,
 			unsigned flags);
-int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
-			  struct kvm_kernel_irq_routing_entry *e,
+int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue);
 void kvm_free_irq_routing(struct kvm *kvm);
 
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 69e2c5d400cf..47d8a06d538e 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -282,20 +282,22 @@ static void irqfd_update(struct kvm *kvm, struct _irqfd *irqfd,
 			 struct kvm_irq_routing_table *irq_rt)
 {
 	struct kvm_kernel_irq_routing_entry *e;
+	struct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];
+	int i, n_entries;
+
+	n_entries = kvm_irq_map_gsi(entries, irq_rt, irqfd->gsi);
 
 	write_seqcount_begin(&irqfd->irq_entry_sc);
 
 	irqfd->irq_entry.type = 0;
-	if (irqfd->gsi >= irq_rt->nr_rt_entries)
-		goto out;
 
-	hlist_for_each_entry(e, &irq_rt->map[irqfd->gsi], link) {
+	e = entries;
+	for (i = 0; i < n_entries; ++i, ++e) {
 		/* Only fast-path MSI. */
 		if (e->type == KVM_IRQ_ROUTING_MSI)
 			irqfd->irq_entry = *e;
 	}
 
- out:
 	write_seqcount_end(&irqfd->irq_entry_sc);
 }
 
diff --git a/virt/kvm/irq_comm.c b/virt/kvm/irq_comm.c
index ced4a542a031..6e73af292f5f 100644
--- a/virt/kvm/irq_comm.c
+++ b/virt/kvm/irq_comm.c
@@ -160,6 +160,7 @@ static int kvm_set_msi_inatomic(struct kvm_kernel_irq_routing_entry *e,
  */
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level)
 {
+	struct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];
 	struct kvm_kernel_irq_routing_entry *e;
 	int ret = -EINVAL;
 	struct kvm_irq_routing_table *irq_rt;
@@ -177,14 +178,13 @@ int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level)
 	 */
 	idx = srcu_read_lock(&kvm->irq_srcu);
 	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
-	if (irq < irq_rt->nr_rt_entries)
-		hlist_for_each_entry(e, &irq_rt->map[irq], link) {
-			if (likely(e->type == KVM_IRQ_ROUTING_MSI))
-				ret = kvm_set_msi_inatomic(e, kvm);
-			else
-				ret = -EWOULDBLOCK;
-			break;
-		}
+	if (kvm_irq_map_gsi(entries, irq_rt, irq) > 0) {
+		e = &entries[0];
+		if (likely(e->type == KVM_IRQ_ROUTING_MSI))
+			ret = kvm_set_msi_inatomic(e, kvm);
+		else
+			ret = -EWOULDBLOCK;
+	}
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 	return ret;
 }
@@ -272,8 +272,7 @@ void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 }
 
-int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
-			  struct kvm_kernel_irq_routing_entry *e,
+int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
 {
 	int r = -EINVAL;
@@ -304,7 +303,6 @@ int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
 		e->irqchip.pin = ue->u.irqchip.pin + delta;
 		if (e->irqchip.pin >= max_pin)
 			goto out;
-		rt->chip[ue->u.irqchip.irqchip][e->irqchip.pin] = ue->gsi;
 		break;
 	case KVM_IRQ_ROUTING_MSI:
 		e->set = kvm_set_msi;
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index b43c275775cd..f4648dd94888 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -31,13 +31,37 @@
 #include <trace/events/kvm.h>
 #include "irq.h"
 
+int kvm_irq_map_gsi(struct kvm_kernel_irq_routing_entry *entries,
+		    struct kvm_irq_routing_table *irq_rt, int gsi)
+{
+	struct kvm_kernel_irq_routing_entry *e;
+	int n = 0;
+
+	if (gsi < irq_rt->nr_rt_entries) {
+		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+			entries[n] = *e;
+			++n;
+		}
+	}
+
+	return n;
+}
+
+int kvm_irq_map_chip_pin(struct kvm_irq_routing_table *irq_rt,
+			 unsigned irqchip, unsigned pin)
+{
+	return irq_rt->chip[irqchip][pin];
+}
+
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
+	struct kvm_irq_routing_table *irq_rt;
 	struct kvm_irq_ack_notifier *kian;
 	int gsi, idx;
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
-	gsi = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu)->chip[irqchip][pin];
+	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	gsi = kvm_irq_map_chip_pin(irq_rt, irqchip, pin);
 	if (gsi != -1)
 		hlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,
 					 link)
@@ -54,13 +78,15 @@ EXPORT_SYMBOL_GPL(kvm_irq_has_notifier);
 
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
+	struct kvm_irq_routing_table *irq_rt;
 	struct kvm_irq_ack_notifier *kian;
 	int gsi, idx;
 
 	trace_kvm_ack_irq(irqchip, pin);
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
-	gsi = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu)->chip[irqchip][pin];
+	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	gsi = kvm_irq_map_chip_pin(irq_rt, irqchip, pin);
 	if (gsi != -1)
 		hlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,
 					 link)
@@ -115,8 +141,8 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status)
 {
-	struct kvm_kernel_irq_routing_entry *e, irq_set[KVM_NR_IRQCHIPS];
-	int ret = -1, i = 0, idx;
+	struct kvm_kernel_irq_routing_entry irq_set[KVM_NR_IRQCHIPS];
+	int ret = -1, i, idx;
 	struct kvm_irq_routing_table *irq_rt;
 
 	trace_kvm_set_irq(irq, level, irq_source_id);
@@ -127,9 +153,7 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 	 */
 	idx = srcu_read_lock(&kvm->irq_srcu);
 	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
-	if (irq < irq_rt->nr_rt_entries)
-		hlist_for_each_entry(e, &irq_rt->map[irq], link)
-			irq_set[i++] = *e;
+	i = kvm_irq_map_gsi(irq_set, irq_rt, irq);
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 
 	while(i--) {
@@ -171,9 +195,11 @@ static int setup_routing_entry(struct kvm_irq_routing_table *rt,
 
 	e->gsi = ue->gsi;
 	e->type = ue->type;
-	r = kvm_set_routing_entry(rt, e, ue);
+	r = kvm_set_routing_entry(e, ue);
 	if (r)
 		goto out;
+	if (e->type == KVM_IRQ_ROUTING_IRQCHIP)
+		rt->chip[e->irqchip.irqchip][e->irqchip.pin] = e->gsi;
 
 	hlist_add_head(&e->link, &rt->map[e->gsi]);
 	r = 0;
