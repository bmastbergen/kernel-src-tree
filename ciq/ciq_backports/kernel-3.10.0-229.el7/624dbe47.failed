blk-mq: avoid code duplication

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Ming Lei <tom.leiming@gmail.com>
commit 624dbe47541643b72868a59b2c0059bb53dc923f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/624dbe47.failed

blk_mq_exit_hw_queues() and blk_mq_free_hw_queues()
are introduced to avoid code duplication.

	Signed-off-by: Ming Lei <tom.leiming@gmail.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 624dbe47541643b72868a59b2c0059bb53dc923f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index a6fc109357ae,07851753a049..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1155,11 -1452,114 +1155,43 @@@ err_rq_map
  	return 0;
  }
  
 -static int blk_mq_hctx_cpu_offline(struct blk_mq_hw_ctx *hctx, int cpu)
 -{
 -	struct request_queue *q = hctx->queue;
 -	struct blk_mq_ctx *ctx;
 -	LIST_HEAD(tmp);
 -
 -	/*
 -	 * Move ctx entries to new CPU, if this one is going away.
 -	 */
 -	ctx = __blk_mq_get_ctx(q, cpu);
 -
 -	spin_lock(&ctx->lock);
 -	if (!list_empty(&ctx->rq_list)) {
 -		list_splice_init(&ctx->rq_list, &tmp);
 -		blk_mq_hctx_clear_pending(hctx, ctx);
 -	}
 -	spin_unlock(&ctx->lock);
 -
 -	if (list_empty(&tmp))
 -		return NOTIFY_OK;
 -
 -	ctx = blk_mq_get_ctx(q);
 -	spin_lock(&ctx->lock);
 -
 -	while (!list_empty(&tmp)) {
 -		struct request *rq;
 -
 -		rq = list_first_entry(&tmp, struct request, queuelist);
 -		rq->mq_ctx = ctx;
 -		list_move_tail(&rq->queuelist, &ctx->rq_list);
 -	}
 -
 -	hctx = q->mq_ops->map_queue(q, ctx->cpu);
 -	blk_mq_hctx_mark_pending(hctx, ctx);
 -
 -	spin_unlock(&ctx->lock);
 -
 -	blk_mq_run_hw_queue(hctx, true);
 -	blk_mq_put_ctx(ctx);
 -	return NOTIFY_OK;
 -}
 -
 -static int blk_mq_hctx_cpu_online(struct blk_mq_hw_ctx *hctx, int cpu)
 -{
 -	struct request_queue *q = hctx->queue;
 -	struct blk_mq_tag_set *set = q->tag_set;
 -
 -	if (set->tags[hctx->queue_num])
 -		return NOTIFY_OK;
 -
 -	set->tags[hctx->queue_num] = blk_mq_init_rq_map(set, hctx->queue_num);
 -	if (!set->tags[hctx->queue_num])
 -		return NOTIFY_STOP;
 -
 -	hctx->tags = set->tags[hctx->queue_num];
 -	return NOTIFY_OK;
 -}
 -
 -static int blk_mq_hctx_notify(void *data, unsigned long action,
 -			      unsigned int cpu)
 -{
 -	struct blk_mq_hw_ctx *hctx = data;
 -
 -	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN)
 -		return blk_mq_hctx_cpu_offline(hctx, cpu);
 -	else if (action == CPU_ONLINE || action == CPU_ONLINE_FROZEN)
 -		return blk_mq_hctx_cpu_online(hctx, cpu);
 -
 -	return NOTIFY_OK;
 -}
 -
+ static void blk_mq_exit_hw_queues(struct request_queue *q,
+ 		struct blk_mq_tag_set *set, int nr_queue)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	unsigned int i;
+ 
+ 	queue_for_each_hw_ctx(q, hctx, i) {
+ 		if (i == nr_queue)
+ 			break;
+ 
+ 		if (set->ops->exit_hctx)
+ 			set->ops->exit_hctx(hctx, i);
+ 
+ 		blk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);
+ 		kfree(hctx->ctxs);
+ 		blk_mq_free_bitmap(&hctx->ctx_map);
+ 	}
+ 
+ }
+ 
+ static void blk_mq_free_hw_queues(struct request_queue *q,
+ 		struct blk_mq_tag_set *set)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	unsigned int i;
+ 
+ 	queue_for_each_hw_ctx(q, hctx, i) {
+ 		free_cpumask_var(hctx->cpumask);
+ 		set->ops->free_hctx(hctx, i);
+ 	}
+ }
+ 
  static int blk_mq_init_hw_queues(struct request_queue *q,
 -		struct blk_mq_tag_set *set)
 +				 struct blk_mq_reg *reg, void *driver_data)
  {
  	struct blk_mq_hw_ctx *hctx;
- 	unsigned int i, j;
+ 	unsigned int i;
  
  	/*
  	 * Initialize hardware queues
@@@ -1217,18 -1611,7 +1249,22 @@@
  	/*
  	 * Init failed
  	 */
++<<<<<<< HEAD
 +	queue_for_each_hw_ctx(q, hctx, j) {
 +		if (i == j)
 +			break;
 +
 +		if (reg->ops->exit_hctx)
 +			reg->ops->exit_hctx(hctx, j);
 +
 +		blk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);
 +		blk_mq_free_rq_map(hctx);
 +		kfree(hctx->ctxs);
 +		kfree(hctx->ctx_map);
 +	}
++=======
+ 	blk_mq_exit_hw_queues(q, set, i);
++>>>>>>> 624dbe475416 (blk-mq: avoid code duplication)
  
  	return 1;
  }
@@@ -1407,19 -1860,12 +1443,25 @@@ EXPORT_SYMBOL(blk_mq_init_queue)
  
  void blk_mq_free_queue(struct request_queue *q)
  {
- 	struct blk_mq_hw_ctx *hctx;
- 	int i;
+ 	struct blk_mq_tag_set	*set = q->tag_set;
  
++<<<<<<< HEAD
 +	queue_for_each_hw_ctx(q, hctx, i) {
 +		kfree(hctx->ctx_map);
 +		kfree(hctx->ctxs);
 +		blk_mq_free_rq_map(hctx);
 +		blk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);
 +		if (q->mq_ops->exit_hctx)
 +			q->mq_ops->exit_hctx(hctx, i);
 +		free_cpumask_var(hctx->cpumask);
 +		q->mq_ops->free_hctx(hctx, i);
 +	}
++=======
+ 	blk_mq_del_queue_tag_set(q);
+ 
+ 	blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
+ 	blk_mq_free_hw_queues(q, set);
++>>>>>>> 624dbe475416 (blk-mq: avoid code duplication)
  
  	free_percpu(q->queue_ctx);
  	kfree(q->queue_hw_ctx);
* Unmerged path block/blk-mq.c
