sched/fair: Implement fast idling of CPUs when the system is partially loaded

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Tim Chen <tim.c.chen@linux.intel.com>
commit 4486edd12b5ac8a9af7a5e16e4b9eeb3b8339c10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/4486edd1.failed

When a system is lightly loaded (i.e. no more than 1 job per cpu),
attempt to pull job to a cpu before putting it to idle is unnecessary and
can be skipped.  This patch adds an indicator so the scheduler can know
when there's no more than 1 active job is on any CPU in the system to
skip needless job pulls.

On a 4 socket machine with a request/response kind of workload from
clients, we saw about 0.13 msec delay when we go through a full load
balance to try pull job from all the other cpus.  While 0.1 msec was
spent on processing the request and generating a response, the 0.13 msec
load balance overhead was actually more than the actual work being done.
This overhead can be skipped much of the time for lightly loaded systems.

With this patch, we tested with a netperf request/response workload that
has the server busy with half the cpus in a 4 socket system.  We found
the patch eliminated 75% of the load balance attempts before idling a cpu.

The overhead of setting/clearing the indicator is low as we already gather
the necessary info while we call add_nr_running() and update_sd_lb_stats.()
We switch to full load balance load immediately if any cpu got more than
one job on its run queue in add_nr_running.  We'll clear the indicator
to avoid load balance when we detect no cpu's have more than one job
when we scan the work queues in update_sg_lb_stats().  We are aggressive
in turning on the load balance and opportunistic in skipping the load
balance.

	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Acked-by: Rik van Riel <riel@redhat.com>
	Acked-by: Jason Low <jason.low2@hp.com>
	Cc: "Paul E.McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Davidlohr Bueso <davidlohr@hp.com>
	Cc: Alex Shi <alex.shi@linaro.org>
	Cc: Michel Lespinasse <walken@google.com>
	Cc: Peter Hurley <peter@hurleysoftware.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1403551009.2970.613.camel@schen9-DESK
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4486edd12b5ac8a9af7a5e16e4b9eeb3b8339c10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
#	kernel/sched/sched.h
diff --cc kernel/sched/fair.c
index 32588e344590,e3ff3d1c4780..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5585,9 -5866,9 +5585,10 @@@ static inline int sg_imbalanced(struct 
   */
  static inline void update_sg_lb_stats(struct lb_env *env,
  			struct sched_group *group, int load_idx,
- 			int local_group, struct sg_lb_stats *sgs)
+ 			int local_group, struct sg_lb_stats *sgs,
+ 			bool *overload)
  {
 +	unsigned long nr_running;
  	unsigned long load;
  	int i;
  
@@@ -5603,7 -5884,11 +5604,15 @@@
  			load = source_load(i, load_idx);
  
  		sgs->group_load += load;
++<<<<<<< HEAD
 +		sgs->sum_nr_running += nr_running;
++=======
+ 		sgs->sum_nr_running += rq->nr_running;
+ 
+ 		if (rq->nr_running > 1)
+ 			*overload = true;
+ 
++>>>>>>> 4486edd12b5a (sched/fair: Implement fast idling of CPUs when the system is partially loaded)
  #ifdef CONFIG_NUMA_BALANCING
  		sgs->nr_numa_running += rq->nr_numa_running;
  		sgs->nr_preferred_running += rq->nr_preferred_running;
@@@ -5736,12 -6012,19 +5746,17 @@@ static inline void update_sd_lb_stats(s
  
  		local_group = cpumask_test_cpu(env->dst_cpu, sched_group_cpus(sg));
  		if (local_group) {
 -			sds->local = sg;
 -			sgs = &sds->local_stat;
 -
 -			if (env->idle != CPU_NEWLY_IDLE ||
 -			    time_after_eq(jiffies, sg->sgc->next_update))
 -				update_group_capacity(env->sd, env->dst_cpu);
 +			sds->this = sg;
 +			sgs = &sds->this_stat;
  		}
  
++<<<<<<< HEAD
 +		memset(sgs, 0, sizeof(*sgs));
 +		update_sg_lb_stats(env, sg, load_idx, local_group, sgs);
++=======
+ 		update_sg_lb_stats(env, sg, load_idx, local_group, sgs,
+ 						&overload);
 -
 -		if (local_group)
 -			goto next_group;
++>>>>>>> 4486edd12b5a (sched/fair: Implement fast idling of CPUs when the system is partially loaded)
  
  		/*
  		 * In case the child domain prefers tasks go to siblings
@@@ -6440,17 -6764,32 +6462,30 @@@ out
   * idle_balance is called by schedule() if this_cpu is about to become
   * idle. Attempts to pull tasks from other CPUs.
   */
 -static int idle_balance(struct rq *this_rq)
 +void idle_balance(int this_cpu, struct rq *this_rq)
  {
 -	unsigned long next_balance = jiffies + HZ;
 -	int this_cpu = this_rq->cpu;
  	struct sched_domain *sd;
  	int pulled_task = 0;
 +	unsigned long next_balance = jiffies + HZ;
  	u64 curr_cost = 0;
  
 -	idle_enter_fair(this_rq);
 -
 -	/*
 -	 * We must set idle_stamp _before_ calling idle_balance(), such that we
 -	 * measure the duration of idle_balance() as idle time.
 -	 */
  	this_rq->idle_stamp = rq_clock(this_rq);
  
++<<<<<<< HEAD
 +	if (this_rq->avg_idle < sysctl_sched_migration_cost)
 +		return;
++=======
+ 	if (this_rq->avg_idle < sysctl_sched_migration_cost ||
+ 	    !this_rq->rd->overload) {
+ 		rcu_read_lock();
+ 		sd = rcu_dereference_check_sched_domain(this_rq->sd);
+ 		if (sd)
+ 			update_next_balance(sd, 0, &next_balance);
+ 		rcu_read_unlock();
+ 
+ 		goto out;
+ 	}
++>>>>>>> 4486edd12b5a (sched/fair: Implement fast idling of CPUs when the system is partially loaded)
  
  	/*
  	 * Drop the rq->lock, but keep IRQ/preempt disabled.
diff --cc kernel/sched/sched.h
index 7156b1652b1b,0191ed563bdd..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -376,6 -477,18 +376,9 @@@ struct root_domain 
  	cpumask_var_t span;
  	cpumask_var_t online;
  
+ 	/* Indicate more than one runnable task for any CPU */
+ 	bool overload;
+ 
 -	/*
 -	 * The bit corresponding to a CPU gets set here if such CPU has more
 -	 * than one runnable -deadline task (as it is below for RT tasks).
 -	 */
 -	cpumask_var_t dlo_mask;
 -	atomic_t dlo_count;
 -	struct dl_bw dl_bw;
 -	struct cpudl cpudl;
 -
  	/*
  	 * The "RT overload" flag: it gets set if a CPU has more than
  	 * one runnable RT task.
@@@ -1100,36 -1205,47 +1103,46 @@@ extern void resched_cpu(int cpu)
  extern struct rt_bandwidth def_rt_bandwidth;
  extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
  
 -extern struct dl_bandwidth def_dl_bandwidth;
 -extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
 -extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 -
 -unsigned long to_ratio(u64 period, u64 runtime);
 -
  extern void update_idle_cpu_load(struct rq *this_rq);
  
 -extern void init_task_runnable_average(struct task_struct *p);
 -
 -static inline void add_nr_running(struct rq *rq, unsigned count)
 +#ifdef CONFIG_PARAVIRT
 +static inline u64 steal_ticks(u64 steal)
  {
 -	unsigned prev_nr = rq->nr_running;
 +	if (unlikely(steal > NSEC_PER_SEC))
 +		return div_u64(steal, TICK_NSEC);
 +
 +	return __iter_div_u64_rem(steal, TICK_NSEC, &steal);
 +}
 +#endif
  
 -	rq->nr_running = prev_nr + count;
 +static inline void inc_nr_running(struct rq *rq)
 +{
 +	rq->nr_running++;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_NO_HZ_FULL
 +	if (rq->nr_running == 2) {
++=======
+ 	if (prev_nr < 2 && rq->nr_running >= 2) {
+ #ifdef CONFIG_SMP
+ 		if (!rq->rd->overload)
+ 			rq->rd->overload = true;
+ #endif
+ 
+ #ifdef CONFIG_NO_HZ_FULL
++>>>>>>> 4486edd12b5a (sched/fair: Implement fast idling of CPUs when the system is partially loaded)
  		if (tick_nohz_full_cpu(rq->cpu)) {
 -			/*
 -			 * Tick is needed if more than one task runs on a CPU.
 -			 * Send the target an IPI to kick it out of nohz mode.
 -			 *
 -			 * We assume that IPI implies full memory barrier and the
 -			 * new value of rq->nr_running is visible on reception
 -			 * from the target.
 -			 */
 -			tick_nohz_full_kick_cpu(rq->cpu);
 +			/* Order rq->nr_running write against the IPI */
 +			smp_wmb();
 +			smp_send_reschedule(rq->cpu);
  		}
-        }
  #endif
+ 	}
  }
  
 -static inline void sub_nr_running(struct rq *rq, unsigned count)
 +static inline void dec_nr_running(struct rq *rq)
  {
 -	rq->nr_running -= count;
 +	rq->nr_running--;
  }
  
  static inline void rq_last_tick_reset(struct rq *rq)
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/sched.h
