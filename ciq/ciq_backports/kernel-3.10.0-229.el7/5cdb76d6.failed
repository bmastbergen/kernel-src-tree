uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [kernel] uprobes: Rename arch_uprobe->def to ->defparam, minor comment updates (Oleg Nesterov) [1073627]
Rebuild_FUZZ: 97.18%
commit-author Oleg Nesterov <oleg@redhat.com>
commit 5cdb76d6f0b657c1140de74ed5af7cc8c5ed5faf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/5cdb76d6.failed

Purely cosmetic, no changes in .o,

1. As Jim pointed out arch_uprobe->def looks ambiguous, rename it to
   ->defparam.

2. Add the comment into default_post_xol_op() to explain "regs->sp +=".

3. Remove the stale part of the comment in arch_uprobe_analyze_insn().

	Suggested-by: Jim Keniston <jkenisto@us.ibm.com>
	Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
	Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Oleg Nesterov <oleg@redhat.com>
(cherry picked from commit 5cdb76d6f0b657c1140de74ed5af7cc8c5ed5faf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/uprobes.h
#	arch/x86/kernel/uprobes.c
diff --cc arch/x86/include/asm/uprobes.h
index 6e5197910fd8,74f4c2ff6427..000000000000
--- a/arch/x86/include/asm/uprobes.h
+++ b/arch/x86/include/asm/uprobes.h
@@@ -33,12 -33,27 +33,33 @@@ typedef u8 uprobe_opcode_t
  #define UPROBE_SWBP_INSN		0xcc
  #define UPROBE_SWBP_INSN_SIZE		   1
  
 -struct uprobe_xol_ops;
 -
  struct arch_uprobe {
++<<<<<<< HEAD
 +	u16				fixups;
 +	u8				insn[MAX_UINSN_BYTES];
 +#ifdef CONFIG_X86_64
 +	unsigned long			rip_rela_target_address;
 +#endif
++=======
+ 	union {
+ 		u8			insn[MAX_UINSN_BYTES];
+ 		u8			ixol[MAX_UINSN_BYTES];
+ 	};
+ 
+ 	const struct uprobe_xol_ops	*ops;
+ 
+ 	union {
+ 		struct {
+ 			s32	offs;
+ 			u8	ilen;
+ 			u8	opc1;
+ 		}			branch;
+ 		struct {
+ 			u8	fixups;
+ 			u8	ilen;
+ 		} 			defparam;
+ 	};
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  };
  
  struct arch_uprobe_task {
diff --cc arch/x86/kernel/uprobes.c
index 99569dc5b83d,5d1cbfe4ae58..000000000000
--- a/arch/x86/kernel/uprobes.c
+++ b/arch/x86/kernel/uprobes.c
@@@ -248,10 -254,9 +248,16 @@@ static inline bool is_64bit_mm(struct m
   * If arch_uprobe->insn doesn't use rip-relative addressing, return
   * immediately.  Otherwise, rewrite the instruction so that it accesses
   * its memory operand indirectly through a scratch register.  Set
++<<<<<<< HEAD
 + * arch_uprobe->fixups and arch_uprobe->rip_rela_target_address
 + * accordingly.  (The contents of the scratch register will be saved
 + * before we single-step the modified instruction, and restored
 + * afterward.)
++=======
+  * defparam->fixups accordingly. (The contents of the scratch register
+  * will be saved before we single-step the modified instruction,
+  * and restored afterward).
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
   *
   * We do this because a rip-relative instruction can access only a
   * relatively small area (+/- 2 GB from the instruction), and the XOL
@@@ -286,86 -311,122 +292,186 @@@ handle_riprel_insn(struct arch_uprobe *
  	}
  
  	/*
++<<<<<<< HEAD
++=======
+ 	 * Convert from rip-relative addressing to register-relative addressing
+ 	 * via a scratch register.
+ 	 *
+ 	 * This is tricky since there are insns with modrm byte
+ 	 * which also use registers not encoded in modrm byte:
+ 	 * [i]div/[i]mul: implicitly use dx:ax
+ 	 * shift ops: implicitly use cx
+ 	 * cmpxchg: implicitly uses ax
+ 	 * cmpxchg8/16b: implicitly uses dx:ax and bx:cx
+ 	 *   Encoding: 0f c7/1 modrm
+ 	 *   The code below thinks that reg=1 (cx), chooses si as scratch.
+ 	 * mulx: implicitly uses dx: mulx r/m,r1,r2 does r1:r2 = dx * r/m.
+ 	 *   First appeared in Haswell (BMI2 insn). It is vex-encoded.
+ 	 *   Example where none of bx,cx,dx can be used as scratch reg:
+ 	 *   c4 e2 63 f6 0d disp32   mulx disp32(%rip),%ebx,%ecx
+ 	 * [v]pcmpistri: implicitly uses cx, xmm0
+ 	 * [v]pcmpistrm: implicitly uses xmm0
+ 	 * [v]pcmpestri: implicitly uses ax, dx, cx, xmm0
+ 	 * [v]pcmpestrm: implicitly uses ax, dx, xmm0
+ 	 *   Evil SSE4.2 string comparison ops from hell.
+ 	 * maskmovq/[v]maskmovdqu: implicitly uses (ds:rdi) as destination.
+ 	 *   Encoding: 0f f7 modrm, 66 0f f7 modrm, vex-encoded: c5 f9 f7 modrm.
+ 	 *   Store op1, byte-masked by op2 msb's in each byte, to (ds:rdi).
+ 	 *   AMD says it has no 3-operand form (vex.vvvv must be 1111)
+ 	 *   and that it can have only register operands, not mem
+ 	 *   (its modrm byte must have mode=11).
+ 	 *   If these restrictions will ever be lifted,
+ 	 *   we'll need code to prevent selection of di as scratch reg!
+ 	 *
+ 	 * Summary: I don't know any insns with modrm byte which
+ 	 * use SI register implicitly. DI register is used only
+ 	 * by one insn (maskmovq) and BX register is used
+ 	 * only by one too (cmpxchg8b).
+ 	 * BP is stack-segment based (may be a problem?).
+ 	 * AX, DX, CX are off-limits (many implicit users).
+ 	 * SP is unusable (it's stack pointer - think about "pop mem";
+ 	 * also, rsp+disp32 needs sib encoding -> insn length change).
+ 	 */
+ 
+ 	reg = MODRM_REG(insn);	/* Fetch modrm.reg */
+ 	reg2 = 0xff;		/* Fetch vex.vvvv */
+ 	if (insn->vex_prefix.nbytes == 2)
+ 		reg2 = insn->vex_prefix.bytes[1];
+ 	else if (insn->vex_prefix.nbytes == 3)
+ 		reg2 = insn->vex_prefix.bytes[2];
+ 	/*
+ 	 * TODO: add XOP, EXEV vvvv reading.
+ 	 *
+ 	 * vex.vvvv field is in bits 6-3, bits are inverted.
+ 	 * But in 32-bit mode, high-order bit may be ignored.
+ 	 * Therefore, let's consider only 3 low-order bits.
+ 	 */
+ 	reg2 = ((reg2 >> 3) & 0x7) ^ 0x7;
+ 	/*
+ 	 * Register numbering is ax,cx,dx,bx, sp,bp,si,di, r8..r15.
+ 	 *
+ 	 * Choose scratch reg. Order is important: must not select bx
+ 	 * if we can use si (cmpxchg8b case!)
+ 	 */
+ 	if (reg != 6 && reg2 != 6) {
+ 		reg2 = 6;
+ 		auprobe->defparam.fixups |= UPROBE_FIX_RIP_SI;
+ 	} else if (reg != 7 && reg2 != 7) {
+ 		reg2 = 7;
+ 		auprobe->defparam.fixups |= UPROBE_FIX_RIP_DI;
+ 		/* TODO (paranoia): force maskmovq to not use di */
+ 	} else {
+ 		reg2 = 3;
+ 		auprobe->defparam.fixups |= UPROBE_FIX_RIP_BX;
+ 	}
+ 	/*
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  	 * Point cursor at the modrm byte.  The next 4 bytes are the
  	 * displacement.  Beyond the displacement, for some instructions,
  	 * is the immediate operand.
  	 */
  	cursor = auprobe->insn + insn_offset_modrm(insn);
 +	insn_get_length(insn);
 +
++<<<<<<< HEAD
  	/*
 -	 * Change modrm from "00 reg 101" to "10 reg reg2". Example:
 -	 * 89 05 disp32  mov %eax,disp32(%rip) becomes
 -	 * 89 86 disp32  mov %eax,disp32(%rsi)
 +	 * Convert from rip-relative addressing to indirect addressing
 +	 * via a scratch register.  Change the r/m field from 0x5 (%rip)
 +	 * to 0x0 (%rax) or 0x1 (%rcx), and squeeze out the offset field.
  	 */
 -	*cursor = 0x80 | (reg << 3) | reg2;
 -}
 +	reg = MODRM_REG(insn);
 +	if (reg == 0) {
 +		/*
 +		 * The register operand (if any) is either the A register
 +		 * (%rax, %eax, etc.) or (if the 0x4 bit is set in the
 +		 * REX prefix) %r8.  In any case, we know the C register
 +		 * is NOT the register operand, so we use %rcx (register
 +		 * #1) for the scratch register.
 +		 */
 +		auprobe->fixups = UPROBE_FIX_RIP_CX;
 +		/* Change modrm from 00 000 101 to 00 000 001. */
 +		*cursor = 0x1;
 +	} else {
 +		/* Use %rax (register #0) for the scratch register. */
 +		auprobe->fixups = UPROBE_FIX_RIP_AX;
 +		/* Change modrm from 00 xxx 101 to 00 xxx 000 */
 +		*cursor = (reg << 3);
 +	}
  
 +	/* Target address = address of next instruction + (signed) offset */
 +	auprobe->rip_rela_target_address = (long)insn->length + insn->displacement.value;
 +
 +	/* Displacement field is gone; slide immediate field (if any) over. */
 +	if (insn->immediate.nbytes) {
 +		cursor++;
 +		memmove(cursor, cursor + insn->displacement.nbytes, insn->immediate.nbytes);
 +	}
++=======
+ static inline unsigned long *
+ scratch_reg(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_RIP_SI)
+ 		return &regs->si;
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_RIP_DI)
+ 		return &regs->di;
+ 	return &regs->bx;
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  }
  
  /*
   * If we're emulating a rip-relative instruction, save the contents
   * of the scratch register and store the target address in that register.
   */
 -static void riprel_pre_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)
 +static void
 +pre_xol_rip_insn(struct arch_uprobe *auprobe, struct pt_regs *regs,
 +				struct arch_uprobe_task *autask)
  {
++<<<<<<< HEAD
 +	if (auprobe->fixups & UPROBE_FIX_RIP_AX) {
 +		autask->saved_scratch_register = regs->ax;
 +		regs->ax = current->utask->vaddr;
 +		regs->ax += auprobe->rip_rela_target_address;
 +	} else if (auprobe->fixups & UPROBE_FIX_RIP_CX) {
 +		autask->saved_scratch_register = regs->cx;
 +		regs->cx = current->utask->vaddr;
 +		regs->cx += auprobe->rip_rela_target_address;
++=======
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_RIP_MASK) {
+ 		struct uprobe_task *utask = current->utask;
+ 		unsigned long *sr = scratch_reg(auprobe, regs);
+ 
+ 		utask->autask.saved_scratch_register = *sr;
+ 		*sr = utask->vaddr + auprobe->defparam.ilen;
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  	}
  }
  
 -static void riprel_post_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)
 +static void
 +handle_riprel_post_xol(struct arch_uprobe *auprobe, struct pt_regs *regs, long *correction)
  {
++<<<<<<< HEAD
 +	if (auprobe->fixups & (UPROBE_FIX_RIP_AX | UPROBE_FIX_RIP_CX)) {
 +		struct arch_uprobe_task *autask;
++=======
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_RIP_MASK) {
+ 		struct uprobe_task *utask = current->utask;
+ 		unsigned long *sr = scratch_reg(auprobe, regs);
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  
 -		*sr = utask->autask.saved_scratch_register;
 +		autask = &current->utask->autask;
 +		if (auprobe->fixups & UPROBE_FIX_RIP_AX)
 +			regs->ax = autask->saved_scratch_register;
 +		else
 +			regs->cx = autask->saved_scratch_register;
 +
 +		/*
 +		 * The original instruction includes a displacement, and so
 +		 * is 4 bytes longer than what we've just single-stepped.
 +		 * Caller may need to apply other fixups to handle stuff
 +		 * like "jmpq *...(%rip)" and "callq *...(%rip)".
 +		 */
 +		if (correction)
 +			*correction += 4;
  	}
  }
  #else /* 32-bit: */
@@@ -389,6 -448,246 +495,249 @@@ static void handle_riprel_post_xol(stru
  }
  #endif /* CONFIG_X86_64 */
  
++<<<<<<< HEAD
++=======
+ struct uprobe_xol_ops {
+ 	bool	(*emulate)(struct arch_uprobe *, struct pt_regs *);
+ 	int	(*pre_xol)(struct arch_uprobe *, struct pt_regs *);
+ 	int	(*post_xol)(struct arch_uprobe *, struct pt_regs *);
+ 	void	(*abort)(struct arch_uprobe *, struct pt_regs *);
+ };
+ 
+ static inline int sizeof_long(void)
+ {
+ 	return is_ia32_task() ? 4 : 8;
+ }
+ 
+ static int default_pre_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	riprel_pre_xol(auprobe, regs);
+ 	return 0;
+ }
+ 
+ static int push_ret_address(struct pt_regs *regs, unsigned long ip)
+ {
+ 	unsigned long new_sp = regs->sp - sizeof_long();
+ 
+ 	if (copy_to_user((void __user *)new_sp, &ip, sizeof_long()))
+ 		return -EFAULT;
+ 
+ 	regs->sp = new_sp;
+ 	return 0;
+ }
+ 
+ /*
+  * We have to fix things up as follows:
+  *
+  * Typically, the new ip is relative to the copied instruction.  We need
+  * to make it relative to the original instruction (FIX_IP).  Exceptions
+  * are return instructions and absolute or indirect jump or call instructions.
+  *
+  * If the single-stepped instruction was a call, the return address that
+  * is atop the stack is the address following the copied instruction.  We
+  * need to make it the address following the original instruction (FIX_CALL).
+  *
+  * If the original instruction was a rip-relative instruction such as
+  * "movl %edx,0xnnnn(%rip)", we have instead executed an equivalent
+  * instruction using a scratch register -- e.g., "movl %edx,0xnnnn(%rsi)".
+  * We need to restore the contents of the scratch register
+  * (FIX_RIP_reg).
+  */
+ static int default_post_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	struct uprobe_task *utask = current->utask;
+ 
+ 	riprel_post_xol(auprobe, regs);
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_IP) {
+ 		long correction = utask->vaddr - utask->xol_vaddr;
+ 		regs->ip += correction;
+ 	} else if (auprobe->defparam.fixups & UPROBE_FIX_CALL) {
+ 		regs->sp += sizeof_long(); /* Pop incorrect return address */
+ 		if (push_ret_address(regs, utask->vaddr + auprobe->defparam.ilen))
+ 			return -ERESTART;
+ 	}
+ 	/* popf; tell the caller to not touch TF */
+ 	if (auprobe->defparam.fixups & UPROBE_FIX_SETF)
+ 		utask->autask.saved_tf = true;
+ 
+ 	return 0;
+ }
+ 
+ static void default_abort_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	riprel_post_xol(auprobe, regs);
+ }
+ 
+ static struct uprobe_xol_ops default_xol_ops = {
+ 	.pre_xol  = default_pre_xol_op,
+ 	.post_xol = default_post_xol_op,
+ 	.abort	  = default_abort_op,
+ };
+ 
+ static bool branch_is_call(struct arch_uprobe *auprobe)
+ {
+ 	return auprobe->branch.opc1 == 0xe8;
+ }
+ 
+ #define CASE_COND					\
+ 	COND(70, 71, XF(OF))				\
+ 	COND(72, 73, XF(CF))				\
+ 	COND(74, 75, XF(ZF))				\
+ 	COND(78, 79, XF(SF))				\
+ 	COND(7a, 7b, XF(PF))				\
+ 	COND(76, 77, XF(CF) || XF(ZF))			\
+ 	COND(7c, 7d, XF(SF) != XF(OF))			\
+ 	COND(7e, 7f, XF(ZF) || XF(SF) != XF(OF))
+ 
+ #define COND(op_y, op_n, expr)				\
+ 	case 0x ## op_y: DO((expr) != 0)		\
+ 	case 0x ## op_n: DO((expr) == 0)
+ 
+ #define XF(xf)	(!!(flags & X86_EFLAGS_ ## xf))
+ 
+ static bool is_cond_jmp_opcode(u8 opcode)
+ {
+ 	switch (opcode) {
+ 	#define DO(expr)	\
+ 		return true;
+ 	CASE_COND
+ 	#undef	DO
+ 
+ 	default:
+ 		return false;
+ 	}
+ }
+ 
+ static bool check_jmp_cond(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	unsigned long flags = regs->flags;
+ 
+ 	switch (auprobe->branch.opc1) {
+ 	#define DO(expr)	\
+ 		return expr;
+ 	CASE_COND
+ 	#undef	DO
+ 
+ 	default:	/* not a conditional jmp */
+ 		return true;
+ 	}
+ }
+ 
+ #undef	XF
+ #undef	COND
+ #undef	CASE_COND
+ 
+ static bool branch_emulate_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	unsigned long new_ip = regs->ip += auprobe->branch.ilen;
+ 	unsigned long offs = (long)auprobe->branch.offs;
+ 
+ 	if (branch_is_call(auprobe)) {
+ 		/*
+ 		 * If it fails we execute this (mangled, see the comment in
+ 		 * branch_clear_offset) insn out-of-line. In the likely case
+ 		 * this should trigger the trap, and the probed application
+ 		 * should die or restart the same insn after it handles the
+ 		 * signal, arch_uprobe_post_xol() won't be even called.
+ 		 *
+ 		 * But there is corner case, see the comment in ->post_xol().
+ 		 */
+ 		if (push_ret_address(regs, new_ip))
+ 			return false;
+ 	} else if (!check_jmp_cond(auprobe, regs)) {
+ 		offs = 0;
+ 	}
+ 
+ 	regs->ip = new_ip + offs;
+ 	return true;
+ }
+ 
+ static int branch_post_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
+ {
+ 	BUG_ON(!branch_is_call(auprobe));
+ 	/*
+ 	 * We can only get here if branch_emulate_op() failed to push the ret
+ 	 * address _and_ another thread expanded our stack before the (mangled)
+ 	 * "call" insn was executed out-of-line. Just restore ->sp and restart.
+ 	 * We could also restore ->ip and try to call branch_emulate_op() again.
+ 	 */
+ 	regs->sp += sizeof_long();
+ 	return -ERESTART;
+ }
+ 
+ static void branch_clear_offset(struct arch_uprobe *auprobe, struct insn *insn)
+ {
+ 	/*
+ 	 * Turn this insn into "call 1f; 1:", this is what we will execute
+ 	 * out-of-line if ->emulate() fails. We only need this to generate
+ 	 * a trap, so that the probed task receives the correct signal with
+ 	 * the properly filled siginfo.
+ 	 *
+ 	 * But see the comment in ->post_xol(), in the unlikely case it can
+ 	 * succeed. So we need to ensure that the new ->ip can not fall into
+ 	 * the non-canonical area and trigger #GP.
+ 	 *
+ 	 * We could turn it into (say) "pushf", but then we would need to
+ 	 * divorce ->insn[] and ->ixol[]. We need to preserve the 1st byte
+ 	 * of ->insn[] for set_orig_insn().
+ 	 */
+ 	memset(auprobe->insn + insn_offset_immediate(insn),
+ 		0, insn->immediate.nbytes);
+ }
+ 
+ static struct uprobe_xol_ops branch_xol_ops = {
+ 	.emulate  = branch_emulate_op,
+ 	.post_xol = branch_post_xol_op,
+ };
+ 
+ /* Returns -ENOSYS if branch_xol_ops doesn't handle this insn */
+ static int branch_setup_xol_ops(struct arch_uprobe *auprobe, struct insn *insn)
+ {
+ 	u8 opc1 = OPCODE1(insn);
+ 	int i;
+ 
+ 	switch (opc1) {
+ 	case 0xeb:	/* jmp 8 */
+ 	case 0xe9:	/* jmp 32 */
+ 	case 0x90:	/* prefix* + nop; same as jmp with .offs = 0 */
+ 		break;
+ 
+ 	case 0xe8:	/* call relative */
+ 		branch_clear_offset(auprobe, insn);
+ 		break;
+ 
+ 	case 0x0f:
+ 		if (insn->opcode.nbytes != 2)
+ 			return -ENOSYS;
+ 		/*
+ 		 * If it is a "near" conditional jmp, OPCODE2() - 0x10 matches
+ 		 * OPCODE1() of the "short" jmp which checks the same condition.
+ 		 */
+ 		opc1 = OPCODE2(insn) - 0x10;
+ 	default:
+ 		if (!is_cond_jmp_opcode(opc1))
+ 			return -ENOSYS;
+ 	}
+ 
+ 	/*
+ 	 * 16-bit overrides such as CALLW (66 e8 nn nn) are not supported.
+ 	 * Intel and AMD behavior differ in 64-bit mode: Intel ignores 66 prefix.
+ 	 * No one uses these insns, reject any branch insns with such prefix.
+ 	 */
+ 	for (i = 0; i < insn->prefixes.nbytes; i++) {
+ 		if (insn->prefixes.bytes[i] == 0x66)
+ 			return -ENOTSUPP;
+ 	}
+ 
+ 	auprobe->branch.opc1 = opc1;
+ 	auprobe->branch.ilen = insn->length;
+ 	auprobe->branch.offs = insn->immediate.value;
+ 
+ 	auprobe->ops = &branch_xol_ops;
+ 	return 0;
+ }
+ 
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  /**
   * arch_uprobe_analyze_insn - instruction analysis including validity and fixups.
   * @mm: the probed address space.
@@@ -406,14 -705,17 +755,23 @@@ int arch_uprobe_analyze_insn(struct arc
  	if (ret)
  		return ret;
  
 -	ret = branch_setup_xol_ops(auprobe, &insn);
 -	if (ret != -ENOSYS)
 -		return ret;
 -
  	/*
++<<<<<<< HEAD
 +	 * Figure out which fixups arch_uprobe_post_xol() will need to perform,
 +	 * and annotate arch_uprobe->fixups accordingly. To start with, ->fixups
 +	 * is either zero or it reflects rip-related fixups.
 +	 */
 +	switch (OPCODE1(&insn)) {
 +	case 0x9d:		/* popf */
 +		auprobe->fixups |= UPROBE_FIX_SETF;
++=======
+ 	 * Figure out which fixups default_post_xol_op() will need to perform,
+ 	 * and annotate defparam->fixups accordingly.
+ 	 */
+ 	switch (OPCODE1(&insn)) {
+ 	case 0x9d:		/* popf */
+ 		auprobe->defparam.fixups |= UPROBE_FIX_SETF;
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  		break;
  	case 0xc3:		/* ret or lret -- ip is correct */
  	case 0xcb:
@@@ -441,14 -738,13 +799,19 @@@
  		}
  		/* fall through */
  	default:
 -		riprel_analyze(auprobe, &insn);
 +		handle_riprel_insn(auprobe, &insn);
  	}
  
++<<<<<<< HEAD
 +	if (fix_ip)
 +		auprobe->fixups |= UPROBE_FIX_IP;
 +	if (fix_call)
 +		auprobe->fixups |= UPROBE_FIX_CALL;
++=======
+ 	auprobe->defparam.ilen = insn.length;
+ 	auprobe->defparam.fixups |= fix_ip_or_call;
++>>>>>>> 5cdb76d6f0b6 (uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates)
  
 -	auprobe->ops = &default_xol_ops;
  	return 0;
  }
  
* Unmerged path arch/x86/include/asm/uprobes.h
* Unmerged path arch/x86/kernel/uprobes.c
