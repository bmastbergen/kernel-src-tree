ext4: enforce we are operating on a regular file in ext4_zero_range()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author jon ernst <jonernst07@gmail.com>
commit 6c5e73d3a26b73bfcac0b4a932cb918177d067f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/6c5e73d3.failed

	Signed-off-by: Jon Ernst <jonernst07@gmail.com>
	Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
(cherry picked from commit 6c5e73d3a26b73bfcac0b4a932cb918177d067f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/extents.c
diff --cc fs/ext4/extents.c
index f6d4d07b9039,2f49b12a4c40..000000000000
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@@ -4657,6 -4721,221 +4657,224 @@@ retry
  		goto retry;
  	}
  
++<<<<<<< HEAD
++=======
+ 	return ret > 0 ? ret2 : ret;
+ }
+ 
+ static long ext4_zero_range(struct file *file, loff_t offset,
+ 			    loff_t len, int mode)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle = NULL;
+ 	unsigned int max_blocks;
+ 	loff_t new_size = 0;
+ 	int ret = 0;
+ 	int flags;
+ 	int partial;
+ 	loff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	struct address_space *mapping = inode->i_mapping;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	trace_ext4_zero_range(inode, offset, len, mode);
+ 
+ 	if (!S_ISREG(inode->i_mode))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Write out all dirty pages to avoid race conditions
+ 	 * Then release them.
+ 	 */
+ 	if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
+ 		ret = filemap_write_and_wait_range(mapping, offset,
+ 						   offset + len - 1);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/*
+ 	 * Round up offset. This is not fallocate, we neet to zero out
+ 	 * blocks, so convert interior block aligned part of the range to
+ 	 * unwritten and possibly manually zero out unaligned parts of the
+ 	 * range.
+ 	 */
+ 	start = round_up(offset, 1 << blkbits);
+ 	end = round_down((offset + len), 1 << blkbits);
+ 
+ 	if (start < offset || end > offset + len)
+ 		return -EINVAL;
+ 	partial = (offset + len) & ((1 << blkbits) - 1);
+ 
+ 	lblk = start >> blkbits;
+ 	max_blocks = (end >> blkbits);
+ 	if (max_blocks < lblk)
+ 		max_blocks = 0;
+ 	else
+ 		max_blocks -= lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNINIT_EXT |
+ 		EXT4_GET_BLOCKS_CONVERT_UNWRITTEN;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	/*
+ 	 * Indirect files do not support unwritten extnets
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {
+ 		ret = -EOPNOTSUPP;
+ 		goto out_mutex;
+ 	}
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out_mutex;
+ 		/*
+ 		 * If we have a partial block after EOF we have to allocate
+ 		 * the entire block.
+ 		 */
+ 		if (partial)
+ 			max_blocks += 1;
+ 	}
+ 
+ 	if (max_blocks > 0) {
+ 
+ 		/* Now release the pages and zero block aligned part of pages*/
+ 		truncate_pagecache_range(inode, start, end - 1);
+ 
+ 		/* Wait all existing dio workers, newcomers will block on i_mutex */
+ 		ext4_inode_block_unlocked_dio(inode);
+ 		inode_dio_wait(inode);
+ 
+ 		/*
+ 		 * Remove entire range from the extent status tree.
+ 		 */
+ 		ret = ext4_es_remove_extent(inode, lblk, max_blocks);
+ 		if (ret)
+ 			goto out_dio;
+ 
+ 		ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags,
+ 					     mode);
+ 		if (ret)
+ 			goto out_dio;
+ 	}
+ 
+ 	handle = ext4_journal_start(inode, EXT4_HT_MISC, 4);
+ 	if (IS_ERR(handle)) {
+ 		ret = PTR_ERR(handle);
+ 		ext4_std_error(inode->i_sb, ret);
+ 		goto out_dio;
+ 	}
+ 
+ 	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
+ 
+ 	if (new_size) {
+ 		if (new_size > i_size_read(inode))
+ 			i_size_write(inode, new_size);
+ 		if (new_size > EXT4_I(inode)->i_disksize)
+ 			ext4_update_i_disksize(inode, new_size);
+ 	} else {
+ 		/*
+ 		* Mark that we allocate beyond EOF so the subsequent truncate
+ 		* can proceed even if the new size is the same as i_size.
+ 		*/
+ 		if ((offset + len) > i_size_read(inode))
+ 			ext4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);
+ 	}
+ 
+ 	ext4_mark_inode_dirty(handle, inode);
+ 
+ 	/* Zero out partial block at the edges of the range */
+ 	ret = ext4_zero_partial_blocks(handle, inode, offset, len);
+ 
+ 	if (file->f_flags & O_SYNC)
+ 		ext4_handle_sync(handle);
+ 
+ 	ext4_journal_stop(handle);
+ out_dio:
+ 	ext4_inode_resume_unlocked_dio(inode);
+ out_mutex:
+ 	mutex_unlock(&inode->i_mutex);
+ 	return ret;
+ }
+ 
+ /*
+  * preallocate space for a file. This implements ext4's fallocate file
+  * operation, which gets called from sys_fallocate system call.
+  * For block-mapped files, posix_fallocate should fall back to the method
+  * of writing zeroes to the required new blocks (the same behavior which is
+  * expected for file systems which do not support fallocate() system call).
+  */
+ long ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle;
+ 	loff_t new_size = 0;
+ 	unsigned int max_blocks;
+ 	int ret = 0;
+ 	int flags;
+ 	ext4_lblk_t lblk;
+ 	struct timespec tv;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	/* Return error if mode is not supported */
+ 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+ 		     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_PUNCH_HOLE)
+ 		return ext4_punch_hole(inode, offset, len);
+ 
+ 	ret = ext4_convert_inline_data(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * currently supporting (pre)allocate mode for extent-based
+ 	 * files _only_
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_COLLAPSE_RANGE)
+ 		return ext4_collapse_range(inode, offset, len);
+ 
+ 	if (mode & FALLOC_FL_ZERO_RANGE)
+ 		return ext4_zero_range(file, offset, len, mode);
+ 
+ 	trace_ext4_fallocate_enter(inode, offset, len, mode);
+ 	lblk = offset >> blkbits;
+ 	/*
+ 	 * We can't just convert len to max_blocks because
+ 	 * If blocksize = 4096 offset = 3072 and len = 2048
+ 	 */
+ 	max_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)
+ 		- lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNINIT_EXT;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags, mode);
+ 	if (ret)
+ 		goto out;
+ 
++>>>>>>> 6c5e73d3a26b (ext4: enforce we are operating on a regular file in ext4_zero_range())
  	handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
  	if (IS_ERR(handle))
  		goto out;
* Unmerged path fs/ext4/extents.c
