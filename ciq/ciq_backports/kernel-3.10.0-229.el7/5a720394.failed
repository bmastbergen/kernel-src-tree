mm: teach truncate_inode_pages_range() to handle non page aligned ranges

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Lukas Czerner <lczerner@redhat.com>
commit 5a7203947a1d9b6f3a00a39fda08c2466489555f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/5a720394.failed

This commit changes truncate_inode_pages_range() so it can handle non
page aligned regions of the truncate. Currently we can hit BUG_ON when
the end of the range is not page aligned, but we can handle unaligned
start of the range.

Being able to handle non page aligned regions of the page can help file
system punch_hole implementations and save some work, because once we're
holding the page we might as well deal with it right away.

In previous commits we've changed ->invalidatepage() prototype to accept
'length' argument to be able to specify range to invalidate. No we can
use that new ability in truncate_inode_pages_range().

	Signed-off-by: Lukas Czerner <lczerner@redhat.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Hugh Dickins <hughd@google.com>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>

(cherry picked from commit 5a7203947a1d9b6f3a00a39fda08c2466489555f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/truncate.c
diff --cc mm/truncate.c
index bb5271c88b1a,e2e8a8a7eb9d..000000000000
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@@ -85,17 -49,9 +85,20 @@@ void do_invalidatepage(struct page *pag
  		invalidatepage = block_invalidatepage;
  #endif
  	if (invalidatepage)
 -		(*invalidatepage)(page, offset, length);
 +		(*invalidatepage)(page, offset);
  }
  
++<<<<<<< HEAD
 +static inline void truncate_partial_page(struct page *page, unsigned partial)
 +{
 +	zero_user_segment(page, partial, PAGE_CACHE_SIZE);
 +	cleancache_invalidate_page(page->mapping, page);
 +	if (page_has_private(page))
 +		do_invalidatepage(page, partial);
 +}
 +
++=======
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  /*
   * This cancels just the dirty bit on the kernel page itself, it
   * does NOT actually remove dirty bits on any mmap's that may be
@@@ -243,40 -203,52 +250,78 @@@ int invalidate_inode_page(struct page *
  void truncate_inode_pages_range(struct address_space *mapping,
  				loff_t lstart, loff_t lend)
  {
++<<<<<<< HEAD
 +	const pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;
 +	const unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);
 +	pgoff_t indices[PAGEVEC_SIZE];
 +	struct pagevec pvec;
 +	pgoff_t index;
 +	pgoff_t end;
 +	int i;
++=======
+ 	pgoff_t		start;		/* inclusive */
+ 	pgoff_t		end;		/* exclusive */
+ 	unsigned int	partial_start;	/* inclusive */
+ 	unsigned int	partial_end;	/* exclusive */
+ 	struct pagevec	pvec;
+ 	pgoff_t		index;
+ 	int		i;
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  
  	cleancache_invalidate_inode(mapping);
 -	if (mapping->nrpages == 0)
 +	if (mapping->nrpages == 0 && mapping->nrshadows == 0)
  		return;
  
- 	BUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));
- 	end = (lend >> PAGE_CACHE_SHIFT);
+ 	/* Offsets within partial pages */
+ 	partial_start = lstart & (PAGE_CACHE_SIZE - 1);
+ 	partial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);
+ 
+ 	/*
+ 	 * 'start' and 'end' always covers the range of pages to be fully
+ 	 * truncated. Partial pages are covered with 'partial_start' at the
+ 	 * start of the range and 'partial_end' at the end of the range.
+ 	 * Note that 'end' is exclusive while 'lend' is inclusive.
+ 	 */
+ 	start = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+ 	if (lend == -1)
+ 		/*
+ 		 * lend == -1 indicates end-of-file so we have to set 'end'
+ 		 * to the highest possible pgoff_t and since the type is
+ 		 * unsigned we're using -1.
+ 		 */
+ 		end = -1;
+ 	else
+ 		end = (lend + 1) >> PAGE_CACHE_SHIFT;
  
  	pagevec_init(&pvec, 0);
  	index = start;
++<<<<<<< HEAD
 +	while (index <= end && __pagevec_lookup(&pvec, mapping, index,
 +			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
 +			indices)) {
++=======
+ 	while (index < end && pagevec_lookup(&pvec, mapping, index,
+ 			min(end - index, (pgoff_t)PAGEVEC_SIZE))) {
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  		mem_cgroup_uncharge_start();
  		for (i = 0; i < pagevec_count(&pvec); i++) {
  			struct page *page = pvec.pages[i];
  
  			/* We rely upon deletion not changing page->index */
++<<<<<<< HEAD
 +			index = indices[i];
 +			if (index > end)
++=======
+ 			index = page->index;
+ 			if (index >= end)
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  				break;
  
 +			if (radix_tree_exceptional_entry(page)) {
 +				clear_exceptional_entry(mapping, index, page);
 +				continue;
 +			}
 +
  			if (!trylock_page(page))
  				continue;
  			WARN_ON(page->index != index);
@@@ -307,16 -307,14 +381,25 @@@
  	index = start;
  	for ( ; ; ) {
  		cond_resched();
++<<<<<<< HEAD
 +		if (!__pagevec_lookup(&pvec, mapping, index,
 +			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
 +			indices)) {
++=======
+ 		if (!pagevec_lookup(&pvec, mapping, index,
+ 			min(end - index, (pgoff_t)PAGEVEC_SIZE))) {
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  			if (index == start)
  				break;
  			index = start;
  			continue;
  		}
++<<<<<<< HEAD
 +		if (index == start && indices[0] > end) {
 +			pagevec_remove_exceptionals(&pvec);
++=======
+ 		if (index == start && pvec.pages[0]->index >= end) {
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  			pagevec_release(&pvec);
  			break;
  		}
@@@ -325,15 -323,10 +408,20 @@@
  			struct page *page = pvec.pages[i];
  
  			/* We rely upon deletion not changing page->index */
++<<<<<<< HEAD
 +			index = indices[i];
 +			if (index > end)
++=======
+ 			index = page->index;
+ 			if (index >= end)
++>>>>>>> 5a7203947a1d (mm: teach truncate_inode_pages_range() to handle non page aligned ranges)
  				break;
  
 +			if (radix_tree_exceptional_entry(page)) {
 +				clear_exceptional_entry(mapping, index, page);
 +				continue;
 +			}
 +
  			lock_page(page);
  			WARN_ON(page->index != index);
  			wait_on_page_writeback(page);
* Unmerged path mm/truncate.c
