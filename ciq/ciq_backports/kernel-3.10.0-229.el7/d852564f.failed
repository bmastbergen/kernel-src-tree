blk-mq: remove blk_mq_alloc_request_pinned

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Christoph Hellwig <hch@lst.de>
commit d852564f8c88b0604490234fdeeb6fb47e4bcc7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/d852564f.failed

We now only have one caller left and can open code it there in a cleaner
way.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit d852564f8c88b0604490234fdeeb6fb47e4bcc7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 2ab20aca2fd8,ae14749b530c..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -193,74 -212,85 +193,123 @@@ static void blk_mq_rq_ctx_init(struct r
  	ctx->rq_dispatched[rw_is_sync(rw_flags)]++;
  }
  
++<<<<<<< HEAD
 +static struct request *blk_mq_alloc_request_pinned(struct request_queue *q,
 +						   int rw, gfp_t gfp,
 +						   bool reserved)
 +{
 +	struct request *rq;
 +
 +	do {
 +		struct blk_mq_ctx *ctx = blk_mq_get_ctx(q);
 +		struct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +
 +		rq = __blk_mq_alloc_request(hctx, gfp & ~__GFP_WAIT, reserved);
 +		if (rq) {
 +			blk_mq_rq_ctx_init(q, ctx, rq, rw);
 +			break;
 +		}
 +
 +		if (gfp & __GFP_WAIT) {
 +			__blk_mq_run_hw_queue(hctx);
 +			blk_mq_put_ctx(ctx);
 +		} else {
 +			blk_mq_put_ctx(ctx);
 +			break;
 +		}
 +
 +		blk_mq_wait_for_tags(hctx->tags);
 +	} while (1);
 +
 +	return rq;
 +}
 +
 +struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp)
++=======
+ static struct request *
+ __blk_mq_alloc_request(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
+ 		struct blk_mq_ctx *ctx, int rw, gfp_t gfp, bool reserved)
  {
  	struct request *rq;
+ 	unsigned int tag;
+ 
+ 	tag = blk_mq_get_tag(hctx, &ctx->last_tag, gfp, reserved);
+ 	if (tag != BLK_MQ_TAG_FAIL) {
+ 		rq = hctx->tags->rqs[tag];
+ 
+ 		rq->cmd_flags = 0;
+ 		if (blk_mq_tag_busy(hctx)) {
+ 			rq->cmd_flags = REQ_MQ_INFLIGHT;
+ 			atomic_inc(&hctx->nr_active);
+ 		}
+ 
+ 		rq->tag = tag;
+ 		blk_mq_rq_ctx_init(q, ctx, rq, rw);
+ 		return rq;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,
+ 		bool reserved)
++>>>>>>> d852564f8c88 (blk-mq: remove blk_mq_alloc_request_pinned)
+ {
+ 	struct blk_mq_ctx *ctx;
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct request *rq;
  
  	if (blk_mq_queue_enter(q))
  		return NULL;
  
++<<<<<<< HEAD
 +	rq = blk_mq_alloc_request_pinned(q, rw, gfp, false);
 +	if (rq)
 +		blk_mq_put_ctx(rq->mq_ctx);
++=======
+ 	ctx = blk_mq_get_ctx(q);
+ 	hctx = q->mq_ops->map_queue(q, ctx->cpu);
+ 
+ 	rq = __blk_mq_alloc_request(q, hctx, ctx, rw, gfp & ~__GFP_WAIT,
+ 				    reserved);
+ 	if (!rq && (gfp & __GFP_WAIT)) {
+ 		__blk_mq_run_hw_queue(hctx);
+ 		blk_mq_put_ctx(ctx);
+ 
+ 		ctx = blk_mq_get_ctx(q);
+ 		hctx = q->mq_ops->map_queue(q, ctx->cpu);
+ 		rq =  __blk_mq_alloc_request(q, hctx, ctx, rw, gfp, reserved);
+ 	}
+ 	blk_mq_put_ctx(ctx);
++>>>>>>> d852564f8c88 (blk-mq: remove blk_mq_alloc_request_pinned)
 +	return rq;
 +}
 +
 +struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw,
 +					      gfp_t gfp)
 +{
 +	struct request *rq;
 +
 +	if (blk_mq_queue_enter(q))
 +		return NULL;
 +
 +	rq = blk_mq_alloc_request_pinned(q, rw, gfp, true);
 +	if (rq)
 +		blk_mq_put_ctx(rq->mq_ctx);
  	return rq;
  }
 -EXPORT_SYMBOL(blk_mq_alloc_request);
 +EXPORT_SYMBOL(blk_mq_alloc_reserved_request);
 +
 +/*
 + * Re-init and set pdu, if we have it
 + */
 +void blk_mq_rq_init(struct blk_mq_hw_ctx *hctx, struct request *rq)
 +{
 +	blk_rq_init(hctx->queue, rq);
 +
 +	if (hctx->cmd_size)
 +		rq->special = blk_mq_rq_to_pdu(rq);
 +}
  
  static void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,
  				  struct blk_mq_ctx *ctx, struct request *rq)
* Unmerged path block/blk-mq.c
