blk-mq: fix potential oops on out-of-memory in __blk_mq_alloc_rq_maps()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Jens Axboe <axboe@fb.com>
commit 6b55e1f2d0a5e462e52678278ab749468f1db81c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/6b55e1f2.failed

__blk_mq_alloc_rq_maps() can be invoked multiple times, if we scale
back the queue depth if we are low on memory. So don't clear
set->tags when we fail, this is handled directly in
the parent function, blk_mq_alloc_tag_set().

	Reported-by: Robert Elliott  <Elliott@hp.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 6b55e1f2d0a5e462e52678278ab749468f1db81c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 6fcf1deefe8d,e83d306907da..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1487,6 -1943,140 +1487,143 @@@ static int __cpuinit blk_mq_queue_reini
  	return NOTIFY_OK;
  }
  
++<<<<<<< HEAD
++=======
+ static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < set->nr_hw_queues; i++) {
+ 		set->tags[i] = blk_mq_init_rq_map(set, i);
+ 		if (!set->tags[i])
+ 			goto out_unwind;
+ 	}
+ 
+ 	return 0;
+ 
+ out_unwind:
+ 	while (--i >= 0)
+ 		blk_mq_free_rq_map(set, set->tags[i], i);
+ 
+ 	return -ENOMEM;
+ }
+ 
+ /*
+  * Allocate the request maps associated with this tag_set. Note that this
+  * may reduce the depth asked for, if memory is tight. set->queue_depth
+  * will be updated to reflect the allocated depth.
+  */
+ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
+ {
+ 	unsigned int depth;
+ 	int err;
+ 
+ 	depth = set->queue_depth;
+ 	do {
+ 		err = __blk_mq_alloc_rq_maps(set);
+ 		if (!err)
+ 			break;
+ 
+ 		set->queue_depth >>= 1;
+ 		if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
+ 			err = -ENOMEM;
+ 			break;
+ 		}
+ 	} while (set->queue_depth);
+ 
+ 	if (!set->queue_depth || err) {
+ 		pr_err("blk-mq: failed to allocate request map\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (depth != set->queue_depth)
+ 		pr_info("blk-mq: reduced tag depth (%u -> %u)\n",
+ 						depth, set->queue_depth);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Alloc a tag set to be associated with one or more request queues.
+  * May fail with EINVAL for various error conditions. May adjust the
+  * requested depth down, if if it too large. In that case, the set
+  * value will be stored in set->queue_depth.
+  */
+ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
+ {
+ 	if (!set->nr_hw_queues)
+ 		return -EINVAL;
+ 	if (!set->queue_depth)
+ 		return -EINVAL;
+ 	if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)
+ 		return -EINVAL;
+ 
+ 	if (!set->nr_hw_queues || !set->ops->queue_rq || !set->ops->map_queue)
+ 		return -EINVAL;
+ 
+ 	if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+ 		pr_info("blk-mq: reduced tag depth to %u\n",
+ 			BLK_MQ_MAX_DEPTH);
+ 		set->queue_depth = BLK_MQ_MAX_DEPTH;
+ 	}
+ 
+ 	set->tags = kmalloc_node(set->nr_hw_queues *
+ 				 sizeof(struct blk_mq_tags *),
+ 				 GFP_KERNEL, set->numa_node);
+ 	if (!set->tags)
+ 		return -ENOMEM;
+ 
+ 	if (blk_mq_alloc_rq_maps(set))
+ 		goto enomem;
+ 
+ 	mutex_init(&set->tag_list_lock);
+ 	INIT_LIST_HEAD(&set->tag_list);
+ 
+ 	return 0;
+ enomem:
+ 	kfree(set->tags);
+ 	set->tags = NULL;
+ 	return -ENOMEM;
+ }
+ EXPORT_SYMBOL(blk_mq_alloc_tag_set);
+ 
+ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < set->nr_hw_queues; i++) {
+ 		if (set->tags[i])
+ 			blk_mq_free_rq_map(set, set->tags[i], i);
+ 	}
+ 
+ 	kfree(set->tags);
+ 	set->tags = NULL;
+ }
+ EXPORT_SYMBOL(blk_mq_free_tag_set);
+ 
+ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
+ {
+ 	struct blk_mq_tag_set *set = q->tag_set;
+ 	struct blk_mq_hw_ctx *hctx;
+ 	int i, ret;
+ 
+ 	if (!set || nr > set->queue_depth)
+ 		return -EINVAL;
+ 
+ 	ret = 0;
+ 	queue_for_each_hw_ctx(q, hctx, i) {
+ 		ret = blk_mq_tag_update_depth(hctx->tags, nr);
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	if (!ret)
+ 		q->nr_requests = nr;
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 6b55e1f2d0a5 (blk-mq: fix potential oops on out-of-memory in __blk_mq_alloc_rq_maps())
  void blk_mq_disable_hotplug(void)
  {
  	mutex_lock(&all_q_mutex);
* Unmerged path block/blk-mq.c
