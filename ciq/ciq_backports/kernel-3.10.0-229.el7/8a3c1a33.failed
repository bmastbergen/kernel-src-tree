KVM: mmu: change useless int return types to void

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/mmu: change useless int return types to void (Paolo Bonzini) [1116936]
Rebuild_FUZZ: 96.91%
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 8a3c1a33476f6bfebd07954e2277dbc88003bd37
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/8a3c1a33.failed

kvm_mmu initialization is mostly filling in function pointers, there is
no way for it to fail.  Clean up unused return values.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Gleb Natapov <gleb@redhat.com>
(cherry picked from commit 8a3c1a33476f6bfebd07954e2277dbc88003bd37)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/mmu.h
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/mmu.c
index 94dde056e31c,40772ef0f2b1..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3433,18 -3419,11 +3433,23 @@@ out_unlock
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void nonpaging_free(struct kvm_vcpu *vcpu)
 +{
 +	mmu_free_roots(vcpu);
 +}
 +
 +static int nonpaging_init_context(struct kvm_vcpu *vcpu,
 +				  struct kvm_mmu *context)
++=======
+ static void nonpaging_init_context(struct kvm_vcpu *vcpu,
+ 				   struct kvm_mmu *context)
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
  {
 +	context->new_cr3 = nonpaging_new_cr3;
  	context->page_fault = nonpaging_page_fault;
  	context->gva_to_gpa = nonpaging_gva_to_gpa;
 +	context->free = nonpaging_free;
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
  	context->update_pte = nonpaging_update_pte;
@@@ -3735,15 -3738,12 +3737,12 @@@ static void init_kvm_tdp_mmu(struct kvm
  		context->gva_to_gpa = paging32_gva_to_gpa;
  	}
  
 -	update_permission_bitmask(vcpu, context, false);
 +	update_permission_bitmask(vcpu, context);
  	update_last_pte_bitmap(vcpu, context);
- 
- 	return 0;
  }
  
- int kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context)
+ void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context)
  {
- 	int r;
  	bool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);
  	ASSERT(vcpu);
  	ASSERT(!VALID_PAGE(vcpu->arch.mmu.root_hpa));
@@@ -3765,12 -3765,32 +3764,36 @@@
  }
  EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
  
++<<<<<<< HEAD
 +static int init_kvm_softmmu(struct kvm_vcpu *vcpu)
++=======
+ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
+ 		bool execonly)
  {
- 	int r = kvm_init_shadow_mmu(vcpu, vcpu->arch.walk_mmu);
+ 	ASSERT(vcpu);
+ 	ASSERT(!VALID_PAGE(vcpu->arch.mmu.root_hpa));
  
+ 	context->shadow_root_level = kvm_x86_ops->get_tdp_level();
+ 
+ 	context->nx = true;
+ 	context->page_fault = ept_page_fault;
+ 	context->gva_to_gpa = ept_gva_to_gpa;
+ 	context->sync_page = ept_sync_page;
+ 	context->invlpg = ept_invlpg;
+ 	context->update_pte = ept_update_pte;
+ 	context->root_level = context->shadow_root_level;
+ 	context->root_hpa = INVALID_PAGE;
+ 	context->direct_map = false;
+ 
+ 	update_permission_bitmask(vcpu, context, true);
+ 	reset_rsvds_bits_mask_ept(vcpu, context, execonly);
+ }
+ EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);
+ 
+ static void init_kvm_softmmu(struct kvm_vcpu *vcpu)
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
+ {
+ 	kvm_init_shadow_mmu(vcpu, vcpu->arch.walk_mmu);
  	vcpu->arch.walk_mmu->set_cr3           = kvm_x86_ops->set_cr3;
  	vcpu->arch.walk_mmu->get_cr3           = get_cr3;
  	vcpu->arch.walk_mmu->get_pdptr         = kvm_pdptr_read;
@@@ -3814,13 -3832,11 +3835,11 @@@ static void init_kvm_nested_mmu(struct 
  		g_context->gva_to_gpa = paging32_gva_to_gpa_nested;
  	}
  
 -	update_permission_bitmask(vcpu, g_context, false);
 +	update_permission_bitmask(vcpu, g_context);
  	update_last_pte_bitmap(vcpu, g_context);
- 
- 	return 0;
  }
  
- static int init_kvm_mmu(struct kvm_vcpu *vcpu)
+ static void init_kvm_mmu(struct kvm_vcpu *vcpu)
  {
  	if (mmu_is_nested(vcpu))
  		return init_kvm_nested_mmu(vcpu);
@@@ -3830,18 -3846,12 +3849,27 @@@
  		return init_kvm_softmmu(vcpu);
  }
  
++<<<<<<< HEAD
 +static void destroy_kvm_mmu(struct kvm_vcpu *vcpu)
++=======
+ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
  {
  	ASSERT(vcpu);
 +	if (VALID_PAGE(vcpu->arch.mmu.root_hpa))
 +		/* mmu.free() should set root_hpa = INVALID_PAGE */
 +		vcpu->arch.mmu.free(vcpu);
 +}
  
++<<<<<<< HEAD
 +int kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 +{
 +	destroy_kvm_mmu(vcpu);
 +	return init_kvm_mmu(vcpu);
++=======
+ 	kvm_mmu_unload(vcpu);
+ 	init_kvm_mmu(vcpu);
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
  
diff --cc arch/x86/kvm/mmu.h
index 5b59c573aba7,292615274358..000000000000
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@@ -70,7 -70,9 +70,13 @@@ enum 
  };
  
  int handle_mmio_page_fault_common(struct kvm_vcpu *vcpu, u64 addr, bool direct);
++<<<<<<< HEAD
 +int kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
++=======
+ void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
+ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
+ 		bool execonly);
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
  
  static inline unsigned int kvm_mmu_available_pages(struct kvm *kvm)
  {
diff --cc arch/x86/kvm/vmx.c
index 63b66c24cc34,2db91645a6cc..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -7472,6 -7476,60 +7472,63 @@@ static void vmx_set_supported_cpuid(u3
  		entry->ecx |= bit(X86_FEATURE_VMX);
  }
  
++<<<<<<< HEAD
++=======
+ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
+ 		struct x86_exception *fault)
+ {
+ 	struct vmcs12 *vmcs12;
+ 	nested_vmx_vmexit(vcpu);
+ 	vmcs12 = get_vmcs12(vcpu);
+ 
+ 	if (fault->error_code & PFERR_RSVD_MASK)
+ 		vmcs12->vm_exit_reason = EXIT_REASON_EPT_MISCONFIG;
+ 	else
+ 		vmcs12->vm_exit_reason = EXIT_REASON_EPT_VIOLATION;
+ 	vmcs12->exit_qualification = vcpu->arch.exit_qualification;
+ 	vmcs12->guest_physical_address = fault->address;
+ }
+ 
+ /* Callbacks for nested_ept_init_mmu_context: */
+ 
+ static unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu)
+ {
+ 	/* return the page table to be shadowed - in our case, EPT12 */
+ 	return get_vmcs12(vcpu)->ept_pointer;
+ }
+ 
+ static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
+ {
+ 	kvm_init_shadow_ept_mmu(vcpu, &vcpu->arch.mmu,
+ 			nested_vmx_ept_caps & VMX_EPT_EXECUTE_ONLY_BIT);
+ 
+ 	vcpu->arch.mmu.set_cr3           = vmx_set_cr3;
+ 	vcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;
+ 	vcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;
+ 
+ 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
+ }
+ 
+ static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->arch.walk_mmu = &vcpu->arch.mmu;
+ }
+ 
+ static void vmx_inject_page_fault_nested(struct kvm_vcpu *vcpu,
+ 		struct x86_exception *fault)
+ {
+ 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
+ 
+ 	WARN_ON(!is_guest_mode(vcpu));
+ 
+ 	/* TODO: also check PFEC_MATCH/MASK, not just EB.PF. */
+ 	if (vmcs12->exception_bitmap & (1u << PF_VECTOR))
+ 		nested_vmx_vmexit(vcpu);
+ 	else
+ 		kvm_inject_page_fault(vcpu, fault);
+ }
+ 
++>>>>>>> 8a3c1a33476f (KVM: mmu: change useless int return types to void)
  /*
   * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
   * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e785612376c0..77df4a8ea2d0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -783,11 +783,11 @@ void kvm_mmu_module_exit(void);
 
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
-int kvm_mmu_setup(struct kvm_vcpu *vcpu);
+void kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 
-int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
+void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
 void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/mmu.h
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index f8df3cc9d2a8..476296b12ae3 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -1959,11 +1959,9 @@ static void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,
 	nested_svm_vmexit(svm);
 }
 
-static int nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)
+static void nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)
 {
-	int r;
-
-	r = kvm_init_shadow_mmu(vcpu, &vcpu->arch.mmu);
+	kvm_init_shadow_mmu(vcpu, &vcpu->arch.mmu);
 
 	vcpu->arch.mmu.set_cr3           = nested_svm_set_tdp_cr3;
 	vcpu->arch.mmu.get_cr3           = nested_svm_get_tdp_cr3;
@@ -1971,8 +1969,6 @@ static int nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.mmu.inject_page_fault = nested_svm_inject_npf_exit;
 	vcpu->arch.mmu.shadow_root_level = get_npt_level();
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
-
-	return r;
 }
 
 static void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index f0cdd181f16a..ceb0c0aefa25 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -6699,7 +6699,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (r)
 		return r;
 	kvm_vcpu_reset(vcpu);
-	r = kvm_mmu_setup(vcpu);
+	kvm_mmu_setup(vcpu);
 	vcpu_put(vcpu);
 
 	return r;
