KVM: PPC: Book3S HV: Basic little-endian guest support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s hv - Basic little-endian guest support (Don Zickus) [1127366]
Rebuild_FUZZ: 94.44%
commit-author Anton Blanchard <anton@samba.org>
commit d682916a381ac7c8eb965c10ab64bc7cc2f18647
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/d682916a.failed

We create a guest MSR from scratch when delivering exceptions in
a few places.  Instead of extracting LPCR[ILE] and inserting it
into MSR_LE each time, we simply create a new variable intr_msr which
contains the entire MSR to use.  For a little-endian guest, userspace
needs to set the ILE (interrupt little-endian) bit in the LPCR for
each vcpu (or at least one vcpu in each virtual core).

[paulus@samba.org - removed H_SET_MODE implementation from original
version of the patch, and made kvmppc_set_lpcr update vcpu->arch.intr_msr.]

	Signed-off-by: Anton Blanchard <anton@samba.org>
	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit d682916a381ac7c8eb965c10ab64bc7cc2f18647)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index 0ae4c28ae4b6,ecb76357c9d0..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -642,62 -559,261 +642,68 @@@ toc_tlbie_lock
  	addi	r6,r6,VCPU_SLB_SIZE
  	bdnz	1b
  9:
 -	/* Increment yield count if they have a VPA */
 -	ld	r3, VCPU_VPA(r4)
 -	cmpdi	r3, 0
 -	beq	25f
 -	lwz	r5, LPPACA_YIELDCOUNT(r3)
 -	addi	r5, r5, 1
 -	stw	r5, LPPACA_YIELDCOUNT(r3)
 -	li	r6, 1
 -	stb	r6, VCPU_VPA_DIRTY(r4)
 -25:
  
 -BEGIN_FTR_SECTION
 -	/* Save purr/spurr */
 -	mfspr	r5,SPRN_PURR
 -	mfspr	r6,SPRN_SPURR
 -	std	r5,HSTATE_PURR(r13)
 -	std	r6,HSTATE_SPURR(r13)
 -	ld	r7,VCPU_PURR(r4)
 -	ld	r8,VCPU_SPURR(r4)
 -	mtspr	SPRN_PURR,r7
 -	mtspr	SPRN_SPURR,r8
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +	/* Restore state of CTRL run bit; assume 1 on entry */
 +	lwz	r5,VCPU_CTRL(r4)
 +	andi.	r5,r5,1
 +	bne	4f
 +	mfspr	r6,SPRN_CTRLF
 +	clrrdi	r6,r6,1
 +	mtspr	SPRN_CTRLT,r6
 +4:
 +	ld	r6, VCPU_CTR(r4)
 +	lwz	r7, VCPU_XER(r4)
 +
 +	mtctr	r6
 +	mtxer	r7
 +
 +	ld	r10, VCPU_PC(r4)
 +	ld	r11, VCPU_MSR(r4)
 +kvmppc_cede_reentry:		/* r4 = vcpu, r13 = paca */
 +	ld	r6, VCPU_SRR0(r4)
 +	ld	r7, VCPU_SRR1(r4)
 +
 +	/* r11 = vcpu->arch.msr & ~MSR_HV */
 +	rldicl	r11, r11, 63 - MSR_HV_LG, 1
 +	rotldi	r11, r11, 1 + MSR_HV_LG
 +	ori	r11, r11, MSR_ME
  
 +	/* Check if we can deliver an external or decrementer interrupt now */
 +	ld	r0,VCPU_PENDING_EXC(r4)
 +	lis	r8,(1 << BOOK3S_IRQPRIO_EXTERNAL_LEVEL)@h
 +	and	r0,r0,r8
 +	cmpdi	cr1,r0,0
 +	andi.	r0,r11,MSR_EE
 +	beq	cr1,11f
  BEGIN_FTR_SECTION
 -	/* Set partition DABR */
 -	/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */
 -	lwz	r5,VCPU_DABRX(r4)
 -	ld	r6,VCPU_DABR(r4)
 -	mtspr	SPRN_DABRX,r5
 -	mtspr	SPRN_DABR,r6
 - BEGIN_FTR_SECTION_NESTED(89)
 +	mfspr	r8,SPRN_LPCR
 +	ori	r8,r8,LPCR_MER
 +	mtspr	SPRN_LPCR,r8
  	isync
 - END_FTR_SECTION_NESTED(CPU_FTR_ARCH_206, CPU_FTR_ARCH_206, 89)
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 +END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +	beq	5f
 +	li	r0,BOOK3S_INTERRUPT_EXTERNAL
 +12:	mr	r6,r10
 +	mr	r10,r0
++<<<<<<< HEAD
 +	mr	r7,r11
 +	li	r11,(MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
 +	rotldi	r11,r11,63
 +	b	5f
 +11:	beq	5f
 +	mfspr	r0,SPRN_DEC
 +	cmpwi	r0,0
 +	li	r0,BOOK3S_INTERRUPT_DECREMENTER
 +	blt	12b
  
 -	/* Load guest PMU registers */
 -	/* R4 is live here (vcpu pointer) */
 -	li	r3, 1
 -	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
 -	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
 -	isync
 -	lwz	r3, VCPU_PMC(r4)	/* always load up guest PMU registers */
 -	lwz	r5, VCPU_PMC + 4(r4)	/* to prevent information leak */
 -	lwz	r6, VCPU_PMC + 8(r4)
 -	lwz	r7, VCPU_PMC + 12(r4)
 -	lwz	r8, VCPU_PMC + 16(r4)
 -	lwz	r9, VCPU_PMC + 20(r4)
 -BEGIN_FTR_SECTION
 -	lwz	r10, VCPU_PMC + 24(r4)
 -	lwz	r11, VCPU_PMC + 28(r4)
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 -	mtspr	SPRN_PMC1, r3
 -	mtspr	SPRN_PMC2, r5
 -	mtspr	SPRN_PMC3, r6
 -	mtspr	SPRN_PMC4, r7
 -	mtspr	SPRN_PMC5, r8
 -	mtspr	SPRN_PMC6, r9
 -BEGIN_FTR_SECTION
 -	mtspr	SPRN_PMC7, r10
 -	mtspr	SPRN_PMC8, r11
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 -	ld	r3, VCPU_MMCR(r4)
 -	ld	r5, VCPU_MMCR + 8(r4)
 -	ld	r6, VCPU_MMCR + 16(r4)
 -	ld	r7, VCPU_SIAR(r4)
 -	ld	r8, VCPU_SDAR(r4)
 -	mtspr	SPRN_MMCR1, r5
 -	mtspr	SPRN_MMCRA, r6
 -	mtspr	SPRN_SIAR, r7
 -	mtspr	SPRN_SDAR, r8
 -BEGIN_FTR_SECTION
 -	ld	r5, VCPU_MMCR + 24(r4)
 -	ld	r6, VCPU_SIER(r4)
 -	lwz	r7, VCPU_PMC + 24(r4)
 -	lwz	r8, VCPU_PMC + 28(r4)
 -	ld	r9, VCPU_MMCR + 32(r4)
 -	mtspr	SPRN_MMCR2, r5
 -	mtspr	SPRN_SIER, r6
 -	mtspr	SPRN_SPMC1, r7
 -	mtspr	SPRN_SPMC2, r8
 -	mtspr	SPRN_MMCRS, r9
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
 -	mtspr	SPRN_MMCR0, r3
 -	isync
 -
 -	/* Load up FP, VMX and VSX registers */
 -	bl	kvmppc_load_fp
 -
 -	ld	r14, VCPU_GPR(R14)(r4)
 -	ld	r15, VCPU_GPR(R15)(r4)
 -	ld	r16, VCPU_GPR(R16)(r4)
 -	ld	r17, VCPU_GPR(R17)(r4)
 -	ld	r18, VCPU_GPR(R18)(r4)
 -	ld	r19, VCPU_GPR(R19)(r4)
 -	ld	r20, VCPU_GPR(R20)(r4)
 -	ld	r21, VCPU_GPR(R21)(r4)
 -	ld	r22, VCPU_GPR(R22)(r4)
 -	ld	r23, VCPU_GPR(R23)(r4)
 -	ld	r24, VCPU_GPR(R24)(r4)
 -	ld	r25, VCPU_GPR(R25)(r4)
 -	ld	r26, VCPU_GPR(R26)(r4)
 -	ld	r27, VCPU_GPR(R27)(r4)
 -	ld	r28, VCPU_GPR(R28)(r4)
 -	ld	r29, VCPU_GPR(R29)(r4)
 -	ld	r30, VCPU_GPR(R30)(r4)
 -	ld	r31, VCPU_GPR(R31)(r4)
 -
 -BEGIN_FTR_SECTION
 -	/* Switch DSCR to guest value */
 -	ld	r5, VCPU_DSCR(r4)
 -	mtspr	SPRN_DSCR, r5
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -BEGIN_FTR_SECTION
 -	/* Skip next section on POWER7 or PPC970 */
 -	b	8f
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 -	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
 -	mfmsr	r8
 -	li	r0, 1
 -	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
 -	mtmsrd	r8
 -
 -	/* Load up POWER8-specific registers */
 -	ld	r5, VCPU_IAMR(r4)
 -	lwz	r6, VCPU_PSPB(r4)
 -	ld	r7, VCPU_FSCR(r4)
 -	mtspr	SPRN_IAMR, r5
 -	mtspr	SPRN_PSPB, r6
 -	mtspr	SPRN_FSCR, r7
 -	ld	r5, VCPU_DAWR(r4)
 -	ld	r6, VCPU_DAWRX(r4)
 -	ld	r7, VCPU_CIABR(r4)
 -	ld	r8, VCPU_TAR(r4)
 -	mtspr	SPRN_DAWR, r5
 -	mtspr	SPRN_DAWRX, r6
 -	mtspr	SPRN_CIABR, r7
 -	mtspr	SPRN_TAR, r8
 -	ld	r5, VCPU_IC(r4)
 -	ld	r6, VCPU_VTB(r4)
 -	mtspr	SPRN_IC, r5
 -	mtspr	SPRN_VTB, r6
 -	ld	r5, VCPU_TFHAR(r4)
 -	ld	r6, VCPU_TFIAR(r4)
 -	ld	r7, VCPU_TEXASR(r4)
 -	ld	r8, VCPU_EBBHR(r4)
 -	mtspr	SPRN_TFHAR, r5
 -	mtspr	SPRN_TFIAR, r6
 -	mtspr	SPRN_TEXASR, r7
 -	mtspr	SPRN_EBBHR, r8
 -	ld	r5, VCPU_EBBRR(r4)
 -	ld	r6, VCPU_BESCR(r4)
 -	ld	r7, VCPU_CSIGR(r4)
 -	ld	r8, VCPU_TACR(r4)
 -	mtspr	SPRN_EBBRR, r5
 -	mtspr	SPRN_BESCR, r6
 -	mtspr	SPRN_CSIGR, r7
 -	mtspr	SPRN_TACR, r8
 -	ld	r5, VCPU_TCSCR(r4)
 -	ld	r6, VCPU_ACOP(r4)
 -	lwz	r7, VCPU_GUEST_PID(r4)
 -	ld	r8, VCPU_WORT(r4)
 -	mtspr	SPRN_TCSCR, r5
 -	mtspr	SPRN_ACOP, r6
 -	mtspr	SPRN_PID, r7
 -	mtspr	SPRN_WORT, r8
 -8:
 -
 -	/*
 -	 * Set the decrementer to the guest decrementer.
 -	 */
 -	ld	r8,VCPU_DEC_EXPIRES(r4)
 -	mftb	r7
 -	subf	r3,r7,r8
 -	mtspr	SPRN_DEC,r3
 -	stw	r3,VCPU_DEC(r4)
 -
 -	ld	r5, VCPU_SPRG0(r4)
 -	ld	r6, VCPU_SPRG1(r4)
 -	ld	r7, VCPU_SPRG2(r4)
 -	ld	r8, VCPU_SPRG3(r4)
 -	mtspr	SPRN_SPRG0, r5
 -	mtspr	SPRN_SPRG1, r6
 -	mtspr	SPRN_SPRG2, r7
 -	mtspr	SPRN_SPRG3, r8
 -
 -	/* Load up DAR and DSISR */
 -	ld	r5, VCPU_DAR(r4)
 -	lwz	r6, VCPU_DSISR(r4)
 -	mtspr	SPRN_DAR, r5
 -	mtspr	SPRN_DSISR, r6
 -
 -BEGIN_FTR_SECTION
 -	/* Restore AMR and UAMOR, set AMOR to all 1s */
 -	ld	r5,VCPU_AMR(r4)
 -	ld	r6,VCPU_UAMOR(r4)
 -	li	r7,-1
 -	mtspr	SPRN_AMR,r5
 -	mtspr	SPRN_UAMOR,r6
 -	mtspr	SPRN_AMOR,r7
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -	/* Restore state of CTRL run bit; assume 1 on entry */
 -	lwz	r5,VCPU_CTRL(r4)
 -	andi.	r5,r5,1
 -	bne	4f
 -	mfspr	r6,SPRN_CTRLF
 -	clrrdi	r6,r6,1
 -	mtspr	SPRN_CTRLT,r6
 -4:
 -	ld	r6, VCPU_CTR(r4)
 -	lwz	r7, VCPU_XER(r4)
 -
 -	mtctr	r6
 -	mtxer	r7
 -
 -kvmppc_cede_reentry:		/* r4 = vcpu, r13 = paca */
 -	ld	r10, VCPU_PC(r4)
 -	ld	r11, VCPU_MSR(r4)
 -	ld	r6, VCPU_SRR0(r4)
 -	ld	r7, VCPU_SRR1(r4)
 -	mtspr	SPRN_SRR0, r6
 +	/* Move SRR0 and SRR1 into the respective regs */
 +5:	mtspr	SPRN_SRR0, r6
  	mtspr	SPRN_SRR1, r7
 -
 -deliver_guest_interrupt:
 -	/* r11 = vcpu->arch.msr & ~MSR_HV */
 -	rldicl	r11, r11, 63 - MSR_HV_LG, 1
 -	rotldi	r11, r11, 1 + MSR_HV_LG
 -	ori	r11, r11, MSR_ME
 -
 -	/* Check if we can deliver an external or decrementer interrupt now */
 -	ld	r0, VCPU_PENDING_EXC(r4)
 -	rldicl	r0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63
 -	cmpdi	cr1, r0, 0
 -	andi.	r8, r11, MSR_EE
 -BEGIN_FTR_SECTION
 -	mfspr	r8, SPRN_LPCR
 -	/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */
 -	rldimi	r8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH
 -	mtspr	SPRN_LPCR, r8
 -	isync
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -	beq	5f
 -	li	r0, BOOK3S_INTERRUPT_EXTERNAL
 -	bne	cr1, 12f
 -	mfspr	r0, SPRN_DEC
 -	cmpwi	r0, 0
 -	li	r0, BOOK3S_INTERRUPT_DECREMENTER
 -	bge	5f
 -
 -12:	mtspr	SPRN_SRR0, r10
 -	mr	r10,r0
++=======
+ 	mtspr	SPRN_SRR1, r11
+ 	ld	r11, VCPU_INTR_MSR(r4)
+ 5:
++>>>>>>> d682916a381a (KVM: PPC: Book3S HV: Basic little-endian guest support)
  
  /*
   * Required state:
@@@ -1814,10 -1990,47 +1817,9 @@@ machine_check_realmode
  	beq	mc_cont
  	/* If not, deliver a machine check.  SRR0/1 are already set */
  	li	r10, BOOK3S_INTERRUPT_MACHINE_CHECK
- 	li	r11, (MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
- 	rotldi	r11, r11, 63
+ 	ld	r11, VCPU_INTR_MSR(r9)
  	b	fast_interrupt_c_return
  
 -/*
 - * Check the reason we woke from nap, and take appropriate action.
 - * Returns:
 - *	0 if nothing needs to be done
 - *	1 if something happened that needs to be handled by the host
 - *	-1 if there was a guest wakeup (IPI)
 - *
 - * Also sets r12 to the interrupt vector for any interrupt that needs
 - * to be handled now by the host (0x500 for external interrupt), or zero.
 - */
 -kvmppc_check_wake_reason:
 -	mfspr	r6, SPRN_SRR1
 -BEGIN_FTR_SECTION
 -	rlwinm	r6, r6, 45-31, 0xf	/* extract wake reason field (P8) */
 -FTR_SECTION_ELSE
 -	rlwinm	r6, r6, 45-31, 0xe	/* P7 wake reason field is 3 bits */
 -ALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)
 -	cmpwi	r6, 8			/* was it an external interrupt? */
 -	li	r12, BOOK3S_INTERRUPT_EXTERNAL
 -	beq	kvmppc_read_intr	/* if so, see what it was */
 -	li	r3, 0
 -	li	r12, 0
 -	cmpwi	r6, 6			/* was it the decrementer? */
 -	beq	0f
 -BEGIN_FTR_SECTION
 -	cmpwi	r6, 5			/* privileged doorbell? */
 -	beq	0f
 -	cmpwi	r6, 3			/* hypervisor doorbell? */
 -	beq	3f
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
 -	li	r3, 1			/* anything else, return 1 */
 -0:	blr
 -
 -	/* hypervisor doorbell */
 -3:	li	r12, BOOK3S_INTERRUPT_H_DOORBELL
 -	li	r3, 1
 -	blr
 -
  /*
   * Determine what sort of external interrupt is pending (if any).
   * Returns:
diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 80c8a73bb706..5b3d52664ed6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -622,6 +622,7 @@ struct kvm_vcpu_arch {
 	spinlock_t tbacct_lock;
 	u64 busy_stolen;
 	u64 busy_preempt;
+	unsigned long intr_msr;
 #endif
 };
 
diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 79fa5c26d629..9ec1b17a20cf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -484,6 +484,7 @@ int main(void)
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
+	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index c420372ed942..9b35d8b5e196 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -266,7 +266,7 @@ void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)
 
 static void kvmppc_mmu_book3s_64_hv_reset_msr(struct kvm_vcpu *vcpu)
 {
-	kvmppc_set_msr(vcpu, MSR_SF | MSR_ME);
+	kvmppc_set_msr(vcpu, vcpu->arch.intr_msr);
 }
 
 /*
diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
index bd4792543205..b172cea9d428 100644
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@ -783,6 +783,27 @@ static void kvmppc_set_lpcr(struct kvm_vcpu *vcpu, u64 new_lpcr)
 	u64 mask;
 
 	spin_lock(&vc->lock);
+	/*
+	 * If ILE (interrupt little-endian) has changed, update the
+	 * MSR_LE bit in the intr_msr for each vcpu in this vcore.
+	 */
+	if ((new_lpcr & LPCR_ILE) != (vc->lpcr & LPCR_ILE)) {
+		struct kvm *kvm = vcpu->kvm;
+		struct kvm_vcpu *vcpu;
+		int i;
+
+		mutex_lock(&kvm->lock);
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			if (vcpu->arch.vcore != vc)
+				continue;
+			if (new_lpcr & LPCR_ILE)
+				vcpu->arch.intr_msr |= MSR_LE;
+			else
+				vcpu->arch.intr_msr &= ~MSR_LE;
+		}
+		mutex_unlock(&kvm->lock);
+	}
+
 	/*
 	 * Userspace can only modify DPFD (default prefetch depth),
 	 * ILE (interrupt little-endian) and TC (translation control).
@@ -998,6 +1019,7 @@ struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)
 	spin_lock_init(&vcpu->arch.vpa_update_lock);
 	spin_lock_init(&vcpu->arch.tbacct_lock);
 	vcpu->arch.busy_preempt = TB_NIL;
+	vcpu->arch.intr_msr = MSR_SF | MSR_ME;
 
 	kvmppc_mmu_book3s_hv_init(vcpu);
 
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
