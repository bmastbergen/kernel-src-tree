timekeeping: Rework frequency adjustments to work better w/ nohz

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author John Stultz <john.stultz@linaro.org>
commit dc491596f6394382fbc74ad331156207d619fa0a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/dc491596.failed

The existing timekeeping_adjust logic has always been complicated
to understand. Further, since it was developed prior to NOHZ becoming
common, its not surprising it performs poorly when NOHZ is enabled.

Since Miroslav pointed out the problematic nature of the existing code
in the NOHZ case, I've tried to refactor the code to perform better.

The problem with the previous approach was that it tried to adjust
for the total cumulative error using a scaled dampening factor. This
resulted in large errors to be corrected slowly, while small errors
were corrected quickly. With NOHZ the timekeeping code doesn't know
how far out the next tick will be, so this results in bad
over-correction to small errors, and insufficient correction to large
errors.

Inspired by Miroslav's patch, I've refactored the code to try to
address the correction in two steps.

1) Check the future freq error for the next tick, and if the frequency
error is large, try to make sure we correct it so it doesn't cause
much accumulated error.

2) Then make a small single unit adjustment to correct any cumulative
error that has collected over time.

This method performs fairly well in the simulator Miroslav created.

Major credit to Miroslav for pointing out the issue, providing the
original patch to resolve this, a simulator for testing, as well as
helping debug and resolve issues in my implementation so that it
performed closer to his original implementation.

	Cc: Miroslav Lichvar <mlichvar@redhat.com>
	Cc: Richard Cochran <richardcochran@gmail.com>
	Cc: Prarit Bhargava <prarit@redhat.com>
	Reported-by: Miroslav Lichvar <mlichvar@redhat.com>
	Signed-off-by: John Stultz <john.stultz@linaro.org>
(cherry picked from commit dc491596f6394382fbc74ad331156207d619fa0a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/timekeeper_internal.h
#	kernel/time/timekeeping.c
diff --cc include/linux/timekeeper_internal.h
index c1825eb436ed,f7ac48d2edf5..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,77 -10,91 +10,81 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
 -/**
 - * struct tk_read_base - base structure for timekeeping readout
 - * @clock:	Current clocksource used for timekeeping.
 - * @read:	Read function of @clock
 - * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
 - * @cycle_last: @clock cycle value at last update
 - * @mult:	NTP adjusted multiplier for scaled math conversion
 - * @shift:	Shift value for scaled math conversion
 - * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
 - * @base_mono:  ktime_t (nanoseconds) base time for readout
 - *
 - * This struct has size 56 byte on 64 bit. Together with a seqcount it
 - * occupies a single 64byte cache line.
 - *
 - * The struct is separate from struct timekeeper as it is also used
 - * for a fast NMI safe accessor to clock monotonic.
 - */
 -struct tk_read_base {
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
  	struct clocksource	*clock;
 -	cycle_t			(*read)(struct clocksource *cs);
 -	cycle_t			mask;
 -	cycle_t			cycle_last;
 +	/* NTP adjusted clock multiplier */
  	u32			mult;
 +	/* The shift value of the current clocksource. */
  	u32			shift;
 -	u64			xtime_nsec;
 -	ktime_t			base_mono;
 -};
 -
 -/**
 - * struct timekeeper - Structure holding internal timekeeping values.
 - * @tkr:		The readout base structure
 - * @xtime_sec:		Current CLOCK_REALTIME time in seconds
 - * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
 - * @offs_real:		Offset clock monotonic -> clock realtime
 - * @offs_boot:		Offset clock monotonic -> clock boottime
 - * @offs_tai:		Offset clock monotonic -> clock tai
 - * @tai_offset:		The current UTC to TAI offset in seconds
 - * @base_raw:		Monotonic raw base time in ktime_t format
 - * @raw_time:		Monotonic raw base time in timespec64 format
 - * @cycle_interval:	Number of clock cycles in one NTP interval
 - * @xtime_interval:	Number of clock shifted nano seconds in one NTP
 - *			interval.
 - * @xtime_remainder:	Shifted nano seconds left over when rounding
 - *			@cycle_interval
 - * @raw_interval:	Raw nano seconds accumulated per NTP interval.
 - * @ntp_error:		Difference between accumulated time and NTP time in ntp
 - *			shifted nano seconds.
 - * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
 - *			ntp shifted nano seconds.
 - *
 - * Note: For timespec(64) based interfaces wall_to_monotonic is what
 - * we need to add to xtime (or xtime corrected for sub jiffie times)
 - * to get to monotonic time.  Monotonic is pegged at zero at system
 - * boot time, so wall_to_monotonic will be negative, however, we will
 - * ALWAYS keep the tv_nsec part positive so we can use the usual
 - * normalization.
 - *
 - * wall_to_monotonic is moved after resume from suspend for the
 - * monotonic time not to jump. We need to add total_sleep_time to
 - * wall_to_monotonic to get the real boot based time offset.
 - *
 - * wall_to_monotonic is no longer the boot time, getboottime must be
 - * used instead.
 - */
 -struct timekeeper {
 -	struct tk_read_base	tkr;
 -	u64			xtime_sec;
 -	struct timespec64	wall_to_monotonic;
 -	ktime_t			offs_real;
 -	ktime_t			offs_boot;
 -	ktime_t			offs_tai;
 -	s32			tai_offset;
 -	ktime_t			base_raw;
 -	struct timespec64	raw_time;
 -
 -	/* The following members are for timekeeping internal use */
 +	/* Number of clock cycles in one NTP interval. */
  	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
  	u64			xtime_interval;
 +	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
 +	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
 +	/* Difference between accumulated time and NTP time in ntp
 +	 * shifted nano seconds. */
  	s64			ntp_error;
 +	/* Shift conversion between clock shifted nano seconds and
 +	 * ntp shifted nano seconds. */
  	u32			ntp_error_shift;
++<<<<<<< HEAD
 +
 +	/*
 +	 * wall_to_monotonic is what we need to add to xtime (or xtime corrected
 +	 * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
 +	 * at zero at system boot time, so wall_to_monotonic will be negative,
 +	 * however, we will ALWAYS keep the tv_nsec part positive so we can use
 +	 * the usual normalization.
 +	 *
 +	 * wall_to_monotonic is moved after resume from suspend for the
 +	 * monotonic time not to jump. We need to add total_sleep_time to
 +	 * wall_to_monotonic to get the real boot based time offset.
 +	 *
 +	 * - wall_to_monotonic is no longer the boot time, getboottime must be
 +	 * used instead.
 +	 */
 +	struct timespec		wall_to_monotonic;
 +	/* Offset clock monotonic -> clock realtime */
 +	ktime_t			offs_real;
 +	/* time spent in suspend */
 +	struct timespec		total_sleep_time;
 +	/* Offset clock monotonic -> clock boottime */
 +	ktime_t			offs_boot;
 +	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
 +	struct timespec		raw_time;
 +	/* The current UTC to TAI offset in seconds */
 +	s32			tai_offset;
 +	/* Offset clock monotonic -> clock tai */
 +	ktime_t			offs_tai;
 +
++=======
+ 	u32			ntp_err_mult;
++>>>>>>> dc491596f639 (timekeeping: Rework frequency adjustments to work better w/ nohz)
  };
  
 +static inline struct timespec tk_xtime(struct timekeeper *tk)
 +{
 +	struct timespec ts;
 +
 +	ts.tv_sec = tk->xtime_sec;
 +	ts.tv_nsec = (long)(tk->xtime_nsec >> tk->shift);
 +	return ts;
 +}
 +
 +
  #ifdef CONFIG_GENERIC_TIME_VSYSCALL
  
  extern void update_vsyscall(struct timekeeper *tk);
diff --cc kernel/time/timekeeping.c
index 1c5b0fcd83b2,43c706a7a728..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -140,7 -177,8 +140,12 @@@ static void tk_setup_internals(struct t
  	 * active clocksource. These value will be adjusted via NTP
  	 * to counteract clock drifting.
  	 */
++<<<<<<< HEAD
 +	tk->mult = clock->mult;
++=======
+ 	tk->tkr.mult = clock->mult;
+ 	tk->ntp_err_mult = 0;
++>>>>>>> dc491596f639 (timekeeping: Rework frequency adjustments to work better w/ nohz)
  }
  
  /* Timekeeper helper functions. */
@@@ -1031,126 -1261,25 +1036,102 @@@ static int __init timekeeping_init_ops(
  device_initcall(timekeeping_init_ops);
  
  /*
-  * If the error is already larger, we look ahead even further
-  * to compensate for late or lost adjustments.
+  * Apply a multiplier adjustment to the timekeeper
   */
- static __always_inline int timekeeping_bigadjust(struct timekeeper *tk,
- 						 s64 error, s64 *interval,
- 						 s64 *offset)
+ static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,
+ 							 s64 offset,
+ 							 bool negative,
+ 							 int adj_scale)
  {
- 	s64 tick_error, i;
- 	u32 look_ahead, adj;
- 	s32 error2, mult;
- 
- 	/*
- 	 * Use the current error value to determine how much to look ahead.
- 	 * The larger the error the slower we adjust for it to avoid problems
- 	 * with losing too many ticks, otherwise we would overadjust and
- 	 * produce an even larger error.  The smaller the adjustment the
- 	 * faster we try to adjust for it, as lost ticks can do less harm
- 	 * here.  This is tuned so that an error of about 1 msec is adjusted
- 	 * within about 1 sec (or 2^20 nsec in 2^SHIFT_HZ ticks).
- 	 */
- 	error2 = tk->ntp_error >> (NTP_SCALE_SHIFT + 22 - 2 * SHIFT_HZ);
- 	error2 = abs(error2);
- 	for (look_ahead = 0; error2 > 0; look_ahead++)
- 		error2 >>= 2;
+ 	s64 interval = tk->cycle_interval;
+ 	s32 mult_adj = 1;
  
- 	/*
- 	 * Now calculate the error in (1 << look_ahead) ticks, but first
- 	 * remove the single look ahead already included in the error.
- 	 */
- 	tick_error = ntp_tick_length() >> (tk->ntp_error_shift + 1);
- 	tick_error -= tk->xtime_interval >> 1;
- 	error = ((error - tick_error) >> look_ahead) + tick_error;
- 
- 	/* Finally calculate the adjustment shift value.  */
- 	i = *interval;
- 	mult = 1;
- 	if (error < 0) {
- 		error = -error;
- 		*interval = -*interval;
- 		*offset = -*offset;
- 		mult = -1;
+ 	if (negative) {
+ 		mult_adj = -mult_adj;
+ 		interval = -interval;
+ 		offset  = -offset;
  	}
- 	for (adj = 0; error > i; adj++)
- 		error >>= 1;
+ 	mult_adj <<= adj_scale;
+ 	interval <<= adj_scale;
+ 	offset <<= adj_scale;
  
++<<<<<<< HEAD
 +	*interval <<= adj;
 +	*offset <<= adj;
 +	return mult << adj;
 +}
 +
 +/*
 + * Adjust the multiplier to reduce the error value,
 + * this is optimized for the most common adjustments of -1,0,1,
 + * for other values we can do a bit more work.
 + */
 +static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
 +{
 +	s64 error, interval = tk->cycle_interval;
 +	int adj;
 +
 +	/*
 +	 * The point of this is to check if the error is greater than half
 +	 * an interval.
 +	 *
 +	 * First we shift it down from NTP_SHIFT to clocksource->shifted nsecs.
 +	 *
 +	 * Note we subtract one in the shift, so that error is really error*2.
 +	 * This "saves" dividing(shifting) interval twice, but keeps the
 +	 * (error > interval) comparison as still measuring if error is
 +	 * larger than half an interval.
 +	 *
 +	 * Note: It does not "save" on aggravation when reading the code.
 +	 */
 +	error = tk->ntp_error >> (tk->ntp_error_shift - 1);
 +	if (error > interval) {
 +		/*
 +		 * We now divide error by 4(via shift), which checks if
 +		 * the error is greater than twice the interval.
 +		 * If it is greater, we need a bigadjust, if its smaller,
 +		 * we can adjust by 1.
 +		 */
 +		error >>= 2;
 +		/*
 +		 * XXX - In update_wall_time, we round up to the next
 +		 * nanosecond, and store the amount rounded up into
 +		 * the error. This causes the likely below to be unlikely.
 +		 *
 +		 * The proper fix is to avoid rounding up by using
 +		 * the high precision tk->xtime_nsec instead of
 +		 * xtime.tv_nsec everywhere. Fixing this will take some
 +		 * time.
 +		 */
 +		if (likely(error <= interval))
 +			adj = 1;
 +		else
 +			adj = timekeeping_bigadjust(tk, error, &interval, &offset);
 +	} else {
 +		if (error < -interval) {
 +			/* See comment above, this is just switched for the negative */
 +			error >>= 2;
 +			if (likely(error >= -interval)) {
 +				adj = -1;
 +				interval = -interval;
 +				offset = -offset;
 +			} else {
 +				adj = timekeeping_bigadjust(tk, error, &interval, &offset);
 +			}
 +		} else {
 +			goto out_adjust;
 +		}
 +	}
 +
 +	if (unlikely(tk->clock->maxadj &&
 +		(tk->mult + adj > tk->clock->mult + tk->clock->maxadj))) {
 +		printk_once(KERN_WARNING
 +			"Adjusting %s more than 11%% (%ld vs %ld)\n",
 +			tk->clock->name, (long)tk->mult + adj,
 +			(long)tk->clock->mult + tk->clock->maxadj);
 +	}
++=======
++>>>>>>> dc491596f639 (timekeeping: Rework frequency adjustments to work better w/ nohz)
  	/*
  	 * So the following can be confusing.
  	 *
@@@ -1200,12 -1329,76 +1181,80 @@@
  	 *
  	 * XXX - TODO: Doc ntp_error calculation.
  	 */
++<<<<<<< HEAD
 +	tk->mult += adj;
++=======
+ 	tk->tkr.mult += mult_adj;
++>>>>>>> dc491596f639 (timekeeping: Rework frequency adjustments to work better w/ nohz)
  	tk->xtime_interval += interval;
 -	tk->tkr.xtime_nsec -= offset;
 +	tk->xtime_nsec -= offset;
  	tk->ntp_error -= (interval - offset) << tk->ntp_error_shift;
+ }
+ 
+ /*
+  * Calculate the multiplier adjustment needed to match the frequency
+  * specified by NTP
+  */
+ static __always_inline void timekeeping_freqadjust(struct timekeeper *tk,
+ 							s64 offset)
+ {
+ 	s64 interval = tk->cycle_interval;
+ 	s64 xinterval = tk->xtime_interval;
+ 	s64 tick_error;
+ 	bool negative;
+ 	u32 adj;
+ 
+ 	/* Remove any current error adj from freq calculation */
+ 	if (tk->ntp_err_mult)
+ 		xinterval -= tk->cycle_interval;
+ 
+ 	/* Calculate current error per tick */
+ 	tick_error = ntp_tick_length() >> tk->ntp_error_shift;
+ 	tick_error -= (xinterval + tk->xtime_remainder);
+ 
+ 	/* Don't worry about correcting it if its small */
+ 	if (likely((tick_error >= 0) && (tick_error <= interval)))
+ 		return;
+ 
+ 	/* preserve the direction of correction */
+ 	negative = (tick_error < 0);
+ 
+ 	/* Sort out the magnitude of the correction */
+ 	tick_error = abs(tick_error);
+ 	for (adj = 0; tick_error > interval; adj++)
+ 		tick_error >>= 1;
+ 
+ 	/* scale the corrections */
+ 	timekeeping_apply_adjustment(tk, offset, negative, adj);
+ }
+ 
+ /*
+  * Adjust the timekeeper's multiplier to the correct frequency
+  * and also to reduce the accumulated error value.
+  */
+ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
+ {
+ 	/* Correct for the current frequency error */
+ 	timekeeping_freqadjust(tk, offset);
+ 
+ 	/* Next make a small adjustment to fix any cumulative error */
+ 	if (!tk->ntp_err_mult && (tk->ntp_error > 0)) {
+ 		tk->ntp_err_mult = 1;
+ 		timekeeping_apply_adjustment(tk, offset, 0, 0);
+ 	} else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {
+ 		/* Undo any existing error adjustment */
+ 		timekeeping_apply_adjustment(tk, offset, 1, 0);
+ 		tk->ntp_err_mult = 0;
+ 	}
+ 
+ 	if (unlikely(tk->tkr.clock->maxadj &&
+ 		(tk->tkr.mult > tk->tkr.clock->mult + tk->tkr.clock->maxadj))) {
+ 		printk_once(KERN_WARNING
+ 			"Adjusting %s more than 11%% (%ld vs %ld)\n",
+ 			tk->tkr.clock->name, (long)tk->tkr.mult,
+ 			(long)tk->tkr.clock->mult + tk->tkr.clock->maxadj);
+ 	}
  
- out_adjust:
  	/*
  	 * It may be possible that when we entered this function, xtime_nsec
  	 * was very small.  Further, if we're slightly speeding the clocksource
@@@ -1220,12 -1413,11 +1269,11 @@@
  	 * We'll correct this error next time through this function, when
  	 * xtime_nsec is not as small.
  	 */
 -	if (unlikely((s64)tk->tkr.xtime_nsec < 0)) {
 -		s64 neg = -(s64)tk->tkr.xtime_nsec;
 -		tk->tkr.xtime_nsec = 0;
 +	if (unlikely((s64)tk->xtime_nsec < 0)) {
 +		s64 neg = -(s64)tk->xtime_nsec;
 +		tk->xtime_nsec = 0;
  		tk->ntp_error += neg << tk->ntp_error_shift;
  	}
- 
  }
  
  /**
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/time/timekeeping.c
