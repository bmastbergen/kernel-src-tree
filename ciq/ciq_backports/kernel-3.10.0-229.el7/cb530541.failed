ext4: fix up error handling for mpage_map_and_submit_extent()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Theodore Ts'o <tytso@mit.edu>
commit cb530541182bee14112675046331f20a1c831507
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/cb530541.failed

The function mpage_released_unused_page() must only be called once;
otherwise the kernel will BUG() when the second call to
mpage_released_unused_page() tries to unlock the pages which had been
unlocked by the first call.

Also restructure the error handling so that we only give up on writing
the dirty pages in the case of ENOSPC where retrying the allocation
won't help.  Otherwise, a transient failure, such as a kmalloc()
failure in calling ext4_map_blocks() might cause us to give up on
those pages, leading to a scary message in /var/log/messages plus data
loss.

	Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
	Reviewed-by: Jan Kara <jack@suse.cz>
(cherry picked from commit cb530541182bee14112675046331f20a1c831507)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/inode.c
diff --cc fs/ext4/inode.c
index fb9fc258dab4,0188e65e1f58..000000000000
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -2209,8 -1906,325 +2209,328 @@@ static int ext4_writepage(struct page *
  #define MAX_WRITEPAGES_EXTENT_LEN 2048
  
  /*
++<<<<<<< HEAD
++=======
+  * mpage_add_bh_to_extent - try to add bh to extent of blocks to map
+  *
+  * @mpd - extent of blocks
+  * @lblk - logical number of the block in the file
+  * @b_state - b_state of the buffer head added
+  *
+  * the function is used to collect contig. blocks in same state
+  */
+ static int mpage_add_bh_to_extent(struct mpage_da_data *mpd, ext4_lblk_t lblk,
+ 				  unsigned long b_state)
+ {
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 
+ 	/* Don't go larger than mballoc is willing to allocate */
+ 	if (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN)
+ 		return 0;
+ 
+ 	/* First block in the extent? */
+ 	if (map->m_len == 0) {
+ 		map->m_lblk = lblk;
+ 		map->m_len = 1;
+ 		map->m_flags = b_state & BH_FLAGS;
+ 		return 1;
+ 	}
+ 
+ 	/* Can we merge the block to our big extent? */
+ 	if (lblk == map->m_lblk + map->m_len &&
+ 	    (b_state & BH_FLAGS) == map->m_flags) {
+ 		map->m_len++;
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static bool add_page_bufs_to_extent(struct mpage_da_data *mpd,
+ 				    struct buffer_head *head,
+ 				    struct buffer_head *bh,
+ 				    ext4_lblk_t lblk)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	ext4_lblk_t blocks = (i_size_read(inode) + (1 << inode->i_blkbits) - 1)
+ 							>> inode->i_blkbits;
+ 
+ 	do {
+ 		BUG_ON(buffer_locked(bh));
+ 
+ 		if (!buffer_dirty(bh) || !buffer_mapped(bh) ||
+ 		    (!buffer_delay(bh) && !buffer_unwritten(bh)) ||
+ 		    lblk >= blocks) {
+ 			/* Found extent to map? */
+ 			if (mpd->map.m_len)
+ 				return false;
+ 			if (lblk >= blocks)
+ 				return true;
+ 			continue;
+ 		}
+ 		if (!mpage_add_bh_to_extent(mpd, lblk, bh->b_state))
+ 			return false;
+ 	} while (lblk++, (bh = bh->b_this_page) != head);
+ 	return true;
+ }
+ 
+ static int mpage_submit_page(struct mpage_da_data *mpd, struct page *page)
+ {
+ 	int len;
+ 	loff_t size = i_size_read(mpd->inode);
+ 	int err;
+ 
+ 	BUG_ON(page->index != mpd->first_page);
+ 	if (page->index == size >> PAGE_CACHE_SHIFT)
+ 		len = size & ~PAGE_CACHE_MASK;
+ 	else
+ 		len = PAGE_CACHE_SIZE;
+ 	clear_page_dirty_for_io(page);
+ 	err = ext4_bio_write_page(&mpd->io_submit, page, len, mpd->wbc);
+ 	if (!err)
+ 		mpd->wbc->nr_to_write--;
+ 	mpd->first_page++;
+ 
+ 	return err;
+ }
+ 
+ /*
+  * mpage_map_buffers - update buffers corresponding to changed extent and
+  *		       submit fully mapped pages for IO
+  *
+  * @mpd - description of extent to map, on return next extent to map
+  *
+  * Scan buffers corresponding to changed extent (we expect corresponding pages
+  * to be already locked) and update buffer state according to new extent state.
+  * We map delalloc buffers to their physical location, clear unwritten bits,
+  * and mark buffers as uninit when we perform writes to uninitialized extents
+  * and do extent conversion after IO is finished. If the last page is not fully
+  * mapped, we update @map to the next extent in the last page that needs
+  * mapping. Otherwise we submit the page for IO.
+  */
+ static int mpage_map_and_submit_buffers(struct mpage_da_data *mpd)
+ {
+ 	struct pagevec pvec;
+ 	int nr_pages, i;
+ 	struct inode *inode = mpd->inode;
+ 	struct buffer_head *head, *bh;
+ 	int bpp_bits = PAGE_CACHE_SHIFT - inode->i_blkbits;
+ 	ext4_lblk_t blocks = (i_size_read(inode) + (1 << inode->i_blkbits) - 1)
+ 							>> inode->i_blkbits;
+ 	pgoff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	sector_t pblock;
+ 	int err;
+ 
+ 	start = mpd->map.m_lblk >> bpp_bits;
+ 	end = (mpd->map.m_lblk + mpd->map.m_len - 1) >> bpp_bits;
+ 	lblk = start << bpp_bits;
+ 	pblock = mpd->map.m_pblk;
+ 
+ 	pagevec_init(&pvec, 0);
+ 	while (start <= end) {
+ 		nr_pages = pagevec_lookup(&pvec, inode->i_mapping, start,
+ 					  PAGEVEC_SIZE);
+ 		if (nr_pages == 0)
+ 			break;
+ 		for (i = 0; i < nr_pages; i++) {
+ 			struct page *page = pvec.pages[i];
+ 
+ 			if (page->index > end)
+ 				break;
+ 			/* Upto 'end' pages must be contiguous */
+ 			BUG_ON(page->index != start);
+ 			bh = head = page_buffers(page);
+ 			do {
+ 				if (lblk < mpd->map.m_lblk)
+ 					continue;
+ 				if (lblk >= mpd->map.m_lblk + mpd->map.m_len) {
+ 					/*
+ 					 * Buffer after end of mapped extent.
+ 					 * Find next buffer in the page to map.
+ 					 */
+ 					mpd->map.m_len = 0;
+ 					mpd->map.m_flags = 0;
+ 					add_page_bufs_to_extent(mpd, head, bh,
+ 								lblk);
+ 					pagevec_release(&pvec);
+ 					return 0;
+ 				}
+ 				if (buffer_delay(bh)) {
+ 					clear_buffer_delay(bh);
+ 					bh->b_blocknr = pblock++;
+ 				}
+ 				clear_buffer_unwritten(bh);
+ 			} while (++lblk < blocks &&
+ 				 (bh = bh->b_this_page) != head);
+ 
+ 			/*
+ 			 * FIXME: This is going to break if dioread_nolock
+ 			 * supports blocksize < pagesize as we will try to
+ 			 * convert potentially unmapped parts of inode.
+ 			 */
+ 			mpd->io_submit.io_end->size += PAGE_CACHE_SIZE;
+ 			/* Page fully mapped - let IO run! */
+ 			err = mpage_submit_page(mpd, page);
+ 			if (err < 0) {
+ 				pagevec_release(&pvec);
+ 				return err;
+ 			}
+ 			start++;
+ 		}
+ 		pagevec_release(&pvec);
+ 	}
+ 	/* Extent fully mapped and matches with page boundary. We are done. */
+ 	mpd->map.m_len = 0;
+ 	mpd->map.m_flags = 0;
+ 	return 0;
+ }
+ 
+ static int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int get_blocks_flags;
+ 	int err;
+ 
+ 	trace_ext4_da_write_pages_extent(inode, map);
+ 	/*
+ 	 * Call ext4_map_blocks() to allocate any delayed allocation blocks, or
+ 	 * to convert an uninitialized extent to be initialized (in the case
+ 	 * where we have written into one or more preallocated blocks).  It is
+ 	 * possible that we're going to need more metadata blocks than
+ 	 * previously reserved. However we must not fail because we're in
+ 	 * writeback and there is nothing we can do about it so it might result
+ 	 * in data loss.  So use reserved blocks to allocate metadata if
+ 	 * possible.
+ 	 *
+ 	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if the blocks
+ 	 * in question are delalloc blocks.  This affects functions in many
+ 	 * different parts of the allocation call path.  This flag exists
+ 	 * primarily because we don't want to change *many* call functions, so
+ 	 * ext4_map_blocks() will set the EXT4_STATE_DELALLOC_RESERVED flag
+ 	 * once the inode's allocation semaphore is taken.
+ 	 */
+ 	get_blocks_flags = EXT4_GET_BLOCKS_CREATE |
+ 			   EXT4_GET_BLOCKS_METADATA_NOFAIL;
+ 	if (ext4_should_dioread_nolock(inode))
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;
+ 	if (map->m_flags & (1 << BH_Delay))
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;
+ 
+ 	err = ext4_map_blocks(handle, inode, map, get_blocks_flags);
+ 	if (err < 0)
+ 		return err;
+ 	if (map->m_flags & EXT4_MAP_UNINIT) {
+ 		if (!mpd->io_submit.io_end->handle &&
+ 		    ext4_handle_valid(handle)) {
+ 			mpd->io_submit.io_end->handle = handle->h_rsv_handle;
+ 			handle->h_rsv_handle = NULL;
+ 		}
+ 		ext4_set_io_unwritten_flag(inode, mpd->io_submit.io_end);
+ 	}
+ 
+ 	BUG_ON(map->m_len == 0);
+ 	if (map->m_flags & EXT4_MAP_NEW) {
+ 		struct block_device *bdev = inode->i_sb->s_bdev;
+ 		int i;
+ 
+ 		for (i = 0; i < map->m_len; i++)
+ 			unmap_underlying_metadata(bdev, map->m_pblk + i);
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * mpage_map_and_submit_extent - map extent starting at mpd->lblk of length
+  *				 mpd->len and submit pages underlying it for IO
+  *
+  * @handle - handle for journal operations
+  * @mpd - extent to map
+  *
+  * The function maps extent starting at mpd->lblk of length mpd->len. If it is
+  * delayed, blocks are allocated, if it is unwritten, we may need to convert
+  * them to initialized or split the described range from larger unwritten
+  * extent. Note that we need not map all the described range since allocation
+  * can return less blocks or the range is covered by more unwritten extents. We
+  * cannot map more because we are limited by reserved transaction credits. On
+  * the other hand we always make sure that the last touched page is fully
+  * mapped so that it can be written out (and thus forward progress is
+  * guaranteed). After mapping we submit all mapped pages for IO.
+  */
+ static int mpage_map_and_submit_extent(handle_t *handle,
+ 				       struct mpage_da_data *mpd,
+ 				       bool *give_up_on_write)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int err;
+ 	loff_t disksize;
+ 
+ 	mpd->io_submit.io_end->offset =
+ 				((loff_t)map->m_lblk) << inode->i_blkbits;
+ 	while (map->m_len) {
+ 		err = mpage_map_one_extent(handle, mpd);
+ 		if (err < 0) {
+ 			struct super_block *sb = inode->i_sb;
+ 
+ 			if (EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)
+ 				goto invalidate_dirty_pages;
+ 			/*
+ 			 * Let the uper layers retry transient errors.
+ 			 * In the case of ENOSPC, if ext4_count_free_blocks()
+ 			 * is non-zero, a commit should free up blocks.
+ 			 */
+ 			if ((err == -ENOMEM) ||
+ 			    (err == -ENOSPC && ext4_count_free_clusters(sb)))
+ 				return err;
+ 			ext4_msg(sb, KERN_CRIT,
+ 				 "Delayed block allocation failed for "
+ 				 "inode %lu at logical offset %llu with"
+ 				 " max blocks %u with error %d",
+ 				 inode->i_ino,
+ 				 (unsigned long long)map->m_lblk,
+ 				 (unsigned)map->m_len, -err);
+ 			ext4_msg(sb, KERN_CRIT,
+ 				 "This should not happen!! Data will "
+ 				 "be lost\n");
+ 			if (err == -ENOSPC)
+ 				ext4_print_free_blocks(inode);
+ 		invalidate_dirty_pages:
+ 			*give_up_on_write = true;
+ 			return err;
+ 		}
+ 		/*
+ 		 * Update buffer state, submit mapped pages, and get us new
+ 		 * extent to map
+ 		 */
+ 		err = mpage_map_and_submit_buffers(mpd);
+ 		if (err < 0)
+ 			return err;
+ 	}
+ 
+ 	/* Update on-disk size after IO is submitted */
+ 	disksize = ((loff_t)mpd->first_page) << PAGE_CACHE_SHIFT;
+ 	if (disksize > i_size_read(inode))
+ 		disksize = i_size_read(inode);
+ 	if (disksize > EXT4_I(inode)->i_disksize) {
+ 		int err2;
+ 
+ 		ext4_update_i_disksize(inode, disksize);
+ 		err2 = ext4_mark_inode_dirty(handle, inode);
+ 		if (err2)
+ 			ext4_error(inode->i_sb,
+ 				   "Failed to mark inode %lu dirty",
+ 				   inode->i_ino);
+ 		if (!err)
+ 			err = err2;
+ 	}
+ 	return err;
+ }
+ 
+ /*
++>>>>>>> cb530541182b (ext4: fix up error handling for mpage_map_and_submit_extent())
   * Calculate the total number of credits to reserve for one writepages
 - * iteration. This is called from ext4_writepages(). We map an extent of
 + * iteration. This is called from ext4_da_writepages(). We map an extent of
   * upto MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping
   * the last partial page. So in total we can map MAX_WRITEPAGES_EXTENT_LEN +
   * bpp - 1 blocks in bpp different extents.
@@@ -2397,16 -2374,14 +2717,17 @@@ static int ext4_da_writepages(struct ad
  	handle_t *handle = NULL;
  	struct mpage_da_data mpd;
  	struct inode *inode = mapping->host;
 -	int needed_blocks, rsv_blocks = 0, ret = 0;
 +	int pages_written = 0;
 +	int range_cyclic, cycled = 1, io_done = 0;
 +	int needed_blocks, ret = 0;
 +	loff_t range_start = wbc->range_start;
  	struct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);
 -	bool done;
 +	pgoff_t done_index = 0;
 +	pgoff_t end;
  	struct blk_plug plug;
+ 	bool give_up_on_write = false;
  
 -	trace_ext4_writepages(inode, wbc);
 +	trace_ext4_da_writepages(inode, wbc);
  
  	/*
  	 * No pages to write? This is mainly a kludge to avoid starting
@@@ -2470,33 -2488,38 +2791,60 @@@ retry
  			ext4_msg(inode->i_sb, KERN_CRIT, "%s: jbd2_start: "
  			       "%ld pages, ino %lu; err %d", __func__,
  				wbc->nr_to_write, inode->i_ino, ret);
 -			/* Release allocated io_end */
 -			ext4_put_io_end(mpd.io_submit.io_end);
 -			break;
 +			blk_finish_plug(&plug);
 +			goto out_writepages;
  		}
  
++<<<<<<< HEAD
 +		/*
 +		 * Now call write_cache_pages_da() to find the next
 +		 * contiguous region of logical blocks that need
 +		 * blocks to be allocated by ext4 and submit them.
 +		 */
 +		ret = write_cache_pages_da(handle, mapping,
 +					   wbc, &mpd, &done_index);
 +		/*
 +		 * If we have a contiguous extent of pages and we
 +		 * haven't done the I/O yet, map the blocks and submit
 +		 * them for I/O.
 +		 */
 +		if (!mpd.io_done && mpd.next_page != mpd.first_page) {
 +			mpage_da_map_and_submit(&mpd);
 +			ret = MPAGE_DA_EXTENT_TAIL;
++=======
+ 		trace_ext4_da_write_pages(inode, mpd.first_page, mpd.wbc);
+ 		ret = mpage_prepare_extent_to_map(&mpd);
+ 		if (!ret) {
+ 			if (mpd.map.m_len)
+ 				ret = mpage_map_and_submit_extent(handle, &mpd,
+ 					&give_up_on_write);
+ 			else {
+ 				/*
+ 				 * We scanned the whole range (or exhausted
+ 				 * nr_to_write), submitted what was mapped and
+ 				 * didn't find anything needing mapping. We are
+ 				 * done.
+ 				 */
+ 				done = true;
+ 			}
++>>>>>>> cb530541182b (ext4: fix up error handling for mpage_map_and_submit_extent())
  		}
 +		trace_ext4_da_write_pages(inode, &mpd);
 +		wbc->nr_to_write -= mpd.pages_written;
 +
  		ext4_journal_stop(handle);
++<<<<<<< HEAD
++=======
+ 		/* Submit prepared bio */
+ 		ext4_io_submit(&mpd.io_submit);
+ 		/* Unlock pages we didn't use */
+ 		mpage_release_unused_pages(&mpd, give_up_on_write);
+ 		/* Drop our io_end reference we got from init */
+ 		ext4_put_io_end(mpd.io_submit.io_end);
++>>>>>>> cb530541182b (ext4: fix up error handling for mpage_map_and_submit_extent())
  
 -		if (ret == -ENOSPC && sbi->s_journal) {
 -			/*
 -			 * Commit the transaction which would
 +		if ((mpd.retval == -ENOSPC) && sbi->s_journal) {
 +			/* commit the transaction which would
  			 * free blocks released in the transaction
  			 * and try again
  			 */
* Unmerged path fs/ext4/inode.c
