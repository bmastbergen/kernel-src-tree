ext4: use transaction reservation for extent conversion in ext4_end_io

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Jan Kara <jack@suse.cz>
commit 6b523df4fb5ae281ddbc817f40504b33e6226554
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/6b523df4.failed

Later we would like to clear PageWriteback bit only after extent
conversion from unwritten to written extents is performed.  However it
is not possible to start a transaction after PageWriteback is set
because that violates lock ordering (and is easy to deadlock).  So we
have to reserve a transaction before locking pages and sending them
for IO and later we use the transaction for extent conversion from
ext4_end_io().

	Reviewed-by: Zheng Liu <wenqing.lz@taobao.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
(cherry picked from commit 6b523df4fb5ae281ddbc817f40504b33e6226554)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/inode.c
diff --cc fs/ext4/inode.c
index d26ab19320c0,510dba785db4..000000000000
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -1402,135 -1407,24 +1402,153 @@@ static void ext4_da_page_release_reserv
   * Delayed allocation stuff
   */
  
++<<<<<<< HEAD
 +static void ext4_da_block_invalidatepages(struct mpage_da_data *mpd);
++=======
+ struct mpage_da_data {
+ 	struct inode *inode;
+ 	struct writeback_control *wbc;
+ 
+ 	pgoff_t first_page;	/* The first page to write */
+ 	pgoff_t next_page;	/* Current page to examine */
+ 	pgoff_t last_page;	/* Last page to examine */
+ 	/*
+ 	 * Extent to map - this can be after first_page because that can be
+ 	 * fully mapped. We somewhat abuse m_flags to store whether the extent
+ 	 * is delalloc or unwritten.
+ 	 */
+ 	struct ext4_map_blocks map;
+ 	struct ext4_io_submit io_submit;	/* IO submission data */
+ };
++>>>>>>> 6b523df4fb5a (ext4: use transaction reservation for extent conversion in ext4_end_io)
 +
 +/*
 + * mpage_da_submit_io - walks through extent of pages and try to write
 + * them with writepage() call back
 + *
 + * @mpd->inode: inode
 + * @mpd->first_page: first page of the extent
 + * @mpd->next_page: page after the last page of the extent
 + *
 + * By the time mpage_da_submit_io() is called we expect all blocks
 + * to be allocated. this may be wrong if allocation failed.
 + *
 + * As pages are already locked by write_cache_pages(), we can't use it
 + */
 +static int mpage_da_submit_io(struct mpage_da_data *mpd,
 +			      struct ext4_map_blocks *map)
 +{
 +	struct pagevec pvec;
 +	unsigned long index, end;
 +	int ret = 0, err, nr_pages, i;
 +	struct inode *inode = mpd->inode;
 +	struct address_space *mapping = inode->i_mapping;
 +	loff_t size = i_size_read(inode);
 +	unsigned int len, block_start;
 +	struct buffer_head *bh, *page_bufs = NULL;
 +	sector_t pblock = 0, cur_logical = 0;
 +	struct ext4_io_submit io_submit;
 +
 +	BUG_ON(mpd->next_page <= mpd->first_page);
 +	ext4_io_submit_init(&io_submit, mpd->wbc);
 +	io_submit.io_end = ext4_init_io_end(inode, GFP_NOFS);
 +	if (!io_submit.io_end) {
 +		ext4_da_block_invalidatepages(mpd);
 +		return -ENOMEM;
 +	}
 +	/*
 +	 * We need to start from the first_page to the next_page - 1
 +	 * to make sure we also write the mapped dirty buffer_heads.
 +	 * If we look at mpd->b_blocknr we would only be looking
 +	 * at the currently mapped buffer_heads.
 +	 */
 +	index = mpd->first_page;
 +	end = mpd->next_page - 1;
 +
 +	pagevec_init(&pvec, 0);
 +	while (index <= end) {
 +		nr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);
 +		if (nr_pages == 0)
 +			break;
 +		for (i = 0; i < nr_pages; i++) {
 +			int skip_page = 0;
 +			struct page *page = pvec.pages[i];
 +
 +			index = page->index;
 +			if (index > end)
 +				break;
 +
 +			if (index == size >> PAGE_CACHE_SHIFT)
 +				len = size & ~PAGE_CACHE_MASK;
 +			else
 +				len = PAGE_CACHE_SIZE;
 +			if (map) {
 +				cur_logical = index << (PAGE_CACHE_SHIFT -
 +							inode->i_blkbits);
 +				pblock = map->m_pblk + (cur_logical -
 +							map->m_lblk);
 +			}
 +			index++;
 +
 +			BUG_ON(!PageLocked(page));
 +			BUG_ON(PageWriteback(page));
 +
 +			bh = page_bufs = page_buffers(page);
 +			block_start = 0;
 +			do {
 +				if (map && (cur_logical >= map->m_lblk) &&
 +				    (cur_logical <= (map->m_lblk +
 +						     (map->m_len - 1)))) {
 +					if (buffer_delay(bh)) {
 +						clear_buffer_delay(bh);
 +						bh->b_blocknr = pblock;
 +					}
 +					if (buffer_unwritten(bh) ||
 +					    buffer_mapped(bh))
 +						BUG_ON(bh->b_blocknr != pblock);
 +					if (map->m_flags & EXT4_MAP_UNINIT)
 +						set_buffer_uninit(bh);
 +					clear_buffer_unwritten(bh);
 +				}
 +
 +				/*
 +				 * skip page if block allocation undone and
 +				 * block is dirty
 +				 */
 +				if (ext4_bh_delay_or_unwritten(NULL, bh))
 +					skip_page = 1;
 +				bh = bh->b_this_page;
 +				block_start += bh->b_size;
 +				cur_logical++;
 +				pblock++;
 +			} while (bh != page_bufs);
 +
 +			if (skip_page) {
 +				unlock_page(page);
 +				continue;
 +			}
 +
 +			clear_page_dirty_for_io(page);
 +			err = ext4_bio_write_page(&io_submit, page, len,
 +						  mpd->wbc);
 +			if (!err)
 +				mpd->pages_written++;
 +			/*
 +			 * In error case, we have to continue because
 +			 * remaining pages are still locked
 +			 */
 +			if (ret == 0)
 +				ret = err;
 +		}
 +		pagevec_release(&pvec);
 +	}
 +	ext4_io_submit(&io_submit);
 +	/* Drop io_end reference we got from init */
 +	ext4_put_io_end_defer(io_submit.io_end);
 +	return ret;
 +}
  
 -static void mpage_release_unused_pages(struct mpage_da_data *mpd,
 -				       bool invalidate)
 +static void ext4_da_block_invalidatepages(struct mpage_da_data *mpd)
  {
  	int nr_pages, i;
  	pgoff_t index, end;
@@@ -2207,6 -1900,321 +2225,324 @@@ static int ext4_writepage(struct page *
  #define MAX_WRITEPAGES_EXTENT_LEN 2048
  
  /*
++<<<<<<< HEAD
++=======
+  * mpage_add_bh_to_extent - try to add bh to extent of blocks to map
+  *
+  * @mpd - extent of blocks
+  * @lblk - logical number of the block in the file
+  * @b_state - b_state of the buffer head added
+  *
+  * the function is used to collect contig. blocks in same state
+  */
+ static int mpage_add_bh_to_extent(struct mpage_da_data *mpd, ext4_lblk_t lblk,
+ 				  unsigned long b_state)
+ {
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 
+ 	/* Don't go larger than mballoc is willing to allocate */
+ 	if (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN)
+ 		return 0;
+ 
+ 	/* First block in the extent? */
+ 	if (map->m_len == 0) {
+ 		map->m_lblk = lblk;
+ 		map->m_len = 1;
+ 		map->m_flags = b_state & BH_FLAGS;
+ 		return 1;
+ 	}
+ 
+ 	/* Can we merge the block to our big extent? */
+ 	if (lblk == map->m_lblk + map->m_len &&
+ 	    (b_state & BH_FLAGS) == map->m_flags) {
+ 		map->m_len++;
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static bool add_page_bufs_to_extent(struct mpage_da_data *mpd,
+ 				    struct buffer_head *head,
+ 				    struct buffer_head *bh,
+ 				    ext4_lblk_t lblk)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	ext4_lblk_t blocks = (i_size_read(inode) + (1 << inode->i_blkbits) - 1)
+ 							>> inode->i_blkbits;
+ 
+ 	do {
+ 		BUG_ON(buffer_locked(bh));
+ 
+ 		if (!buffer_dirty(bh) || !buffer_mapped(bh) ||
+ 		    (!buffer_delay(bh) && !buffer_unwritten(bh)) ||
+ 		    lblk >= blocks) {
+ 			/* Found extent to map? */
+ 			if (mpd->map.m_len)
+ 				return false;
+ 			if (lblk >= blocks)
+ 				return true;
+ 			continue;
+ 		}
+ 		if (!mpage_add_bh_to_extent(mpd, lblk, bh->b_state))
+ 			return false;
+ 	} while (lblk++, (bh = bh->b_this_page) != head);
+ 	return true;
+ }
+ 
+ static int mpage_submit_page(struct mpage_da_data *mpd, struct page *page)
+ {
+ 	int len;
+ 	loff_t size = i_size_read(mpd->inode);
+ 	int err;
+ 
+ 	BUG_ON(page->index != mpd->first_page);
+ 	if (page->index == size >> PAGE_CACHE_SHIFT)
+ 		len = size & ~PAGE_CACHE_MASK;
+ 	else
+ 		len = PAGE_CACHE_SIZE;
+ 	clear_page_dirty_for_io(page);
+ 	err = ext4_bio_write_page(&mpd->io_submit, page, len, mpd->wbc);
+ 	if (!err)
+ 		mpd->wbc->nr_to_write--;
+ 	mpd->first_page++;
+ 
+ 	return err;
+ }
+ 
+ /*
+  * mpage_map_buffers - update buffers corresponding to changed extent and
+  *		       submit fully mapped pages for IO
+  *
+  * @mpd - description of extent to map, on return next extent to map
+  *
+  * Scan buffers corresponding to changed extent (we expect corresponding pages
+  * to be already locked) and update buffer state according to new extent state.
+  * We map delalloc buffers to their physical location, clear unwritten bits,
+  * and mark buffers as uninit when we perform writes to uninitialized extents
+  * and do extent conversion after IO is finished. If the last page is not fully
+  * mapped, we update @map to the next extent in the last page that needs
+  * mapping. Otherwise we submit the page for IO.
+  */
+ static int mpage_map_and_submit_buffers(struct mpage_da_data *mpd)
+ {
+ 	struct pagevec pvec;
+ 	int nr_pages, i;
+ 	struct inode *inode = mpd->inode;
+ 	struct buffer_head *head, *bh;
+ 	int bpp_bits = PAGE_CACHE_SHIFT - inode->i_blkbits;
+ 	ext4_lblk_t blocks = (i_size_read(inode) + (1 << inode->i_blkbits) - 1)
+ 							>> inode->i_blkbits;
+ 	pgoff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	sector_t pblock;
+ 	int err;
+ 
+ 	start = mpd->map.m_lblk >> bpp_bits;
+ 	end = (mpd->map.m_lblk + mpd->map.m_len - 1) >> bpp_bits;
+ 	lblk = start << bpp_bits;
+ 	pblock = mpd->map.m_pblk;
+ 
+ 	pagevec_init(&pvec, 0);
+ 	while (start <= end) {
+ 		nr_pages = pagevec_lookup(&pvec, inode->i_mapping, start,
+ 					  PAGEVEC_SIZE);
+ 		if (nr_pages == 0)
+ 			break;
+ 		for (i = 0; i < nr_pages; i++) {
+ 			struct page *page = pvec.pages[i];
+ 
+ 			if (page->index > end)
+ 				break;
+ 			/* Upto 'end' pages must be contiguous */
+ 			BUG_ON(page->index != start);
+ 			bh = head = page_buffers(page);
+ 			do {
+ 				if (lblk < mpd->map.m_lblk)
+ 					continue;
+ 				if (lblk >= mpd->map.m_lblk + mpd->map.m_len) {
+ 					/*
+ 					 * Buffer after end of mapped extent.
+ 					 * Find next buffer in the page to map.
+ 					 */
+ 					mpd->map.m_len = 0;
+ 					mpd->map.m_flags = 0;
+ 					add_page_bufs_to_extent(mpd, head, bh,
+ 								lblk);
+ 					pagevec_release(&pvec);
+ 					return 0;
+ 				}
+ 				if (buffer_delay(bh)) {
+ 					clear_buffer_delay(bh);
+ 					bh->b_blocknr = pblock++;
+ 				}
+ 				clear_buffer_unwritten(bh);
+ 			} while (++lblk < blocks &&
+ 				 (bh = bh->b_this_page) != head);
+ 
+ 			/*
+ 			 * FIXME: This is going to break if dioread_nolock
+ 			 * supports blocksize < pagesize as we will try to
+ 			 * convert potentially unmapped parts of inode.
+ 			 */
+ 			mpd->io_submit.io_end->size += PAGE_CACHE_SIZE;
+ 			/* Page fully mapped - let IO run! */
+ 			err = mpage_submit_page(mpd, page);
+ 			if (err < 0) {
+ 				pagevec_release(&pvec);
+ 				return err;
+ 			}
+ 			start++;
+ 		}
+ 		pagevec_release(&pvec);
+ 	}
+ 	/* Extent fully mapped and matches with page boundary. We are done. */
+ 	mpd->map.m_len = 0;
+ 	mpd->map.m_flags = 0;
+ 	return 0;
+ }
+ 
+ static int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int get_blocks_flags;
+ 	int err;
+ 
+ 	trace_ext4_da_write_pages_extent(inode, map);
+ 	/*
+ 	 * Call ext4_map_blocks() to allocate any delayed allocation blocks, or
+ 	 * to convert an uninitialized extent to be initialized (in the case
+ 	 * where we have written into one or more preallocated blocks).  It is
+ 	 * possible that we're going to need more metadata blocks than
+ 	 * previously reserved. However we must not fail because we're in
+ 	 * writeback and there is nothing we can do about it so it might result
+ 	 * in data loss.  So use reserved blocks to allocate metadata if
+ 	 * possible.
+ 	 *
+ 	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if the blocks
+ 	 * in question are delalloc blocks.  This affects functions in many
+ 	 * different parts of the allocation call path.  This flag exists
+ 	 * primarily because we don't want to change *many* call functions, so
+ 	 * ext4_map_blocks() will set the EXT4_STATE_DELALLOC_RESERVED flag
+ 	 * once the inode's allocation semaphore is taken.
+ 	 */
+ 	get_blocks_flags = EXT4_GET_BLOCKS_CREATE |
+ 			   EXT4_GET_BLOCKS_METADATA_NOFAIL;
+ 	if (ext4_should_dioread_nolock(inode))
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;
+ 	if (map->m_flags & (1 << BH_Delay))
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;
+ 
+ 	err = ext4_map_blocks(handle, inode, map, get_blocks_flags);
+ 	if (err < 0)
+ 		return err;
+ 	if (map->m_flags & EXT4_MAP_UNINIT) {
+ 		if (!mpd->io_submit.io_end->handle &&
+ 		    ext4_handle_valid(handle)) {
+ 			mpd->io_submit.io_end->handle = handle->h_rsv_handle;
+ 			handle->h_rsv_handle = NULL;
+ 		}
+ 		ext4_set_io_unwritten_flag(inode, mpd->io_submit.io_end);
+ 	}
+ 
+ 	BUG_ON(map->m_len == 0);
+ 	if (map->m_flags & EXT4_MAP_NEW) {
+ 		struct block_device *bdev = inode->i_sb->s_bdev;
+ 		int i;
+ 
+ 		for (i = 0; i < map->m_len; i++)
+ 			unmap_underlying_metadata(bdev, map->m_pblk + i);
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * mpage_map_and_submit_extent - map extent starting at mpd->lblk of length
+  *				 mpd->len and submit pages underlying it for IO
+  *
+  * @handle - handle for journal operations
+  * @mpd - extent to map
+  *
+  * The function maps extent starting at mpd->lblk of length mpd->len. If it is
+  * delayed, blocks are allocated, if it is unwritten, we may need to convert
+  * them to initialized or split the described range from larger unwritten
+  * extent. Note that we need not map all the described range since allocation
+  * can return less blocks or the range is covered by more unwritten extents. We
+  * cannot map more because we are limited by reserved transaction credits. On
+  * the other hand we always make sure that the last touched page is fully
+  * mapped so that it can be written out (and thus forward progress is
+  * guaranteed). After mapping we submit all mapped pages for IO.
+  */
+ static int mpage_map_and_submit_extent(handle_t *handle,
+ 				       struct mpage_da_data *mpd)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int err;
+ 	loff_t disksize;
+ 
+ 	mpd->io_submit.io_end->offset =
+ 				((loff_t)map->m_lblk) << inode->i_blkbits;
+ 	while (map->m_len) {
+ 		err = mpage_map_one_extent(handle, mpd);
+ 		if (err < 0) {
+ 			struct super_block *sb = inode->i_sb;
+ 
+ 			/*
+ 			 * Need to commit transaction to free blocks. Let upper
+ 			 * layers sort it out.
+ 			 */
+ 			if (err == -ENOSPC && ext4_count_free_clusters(sb))
+ 				return -ENOSPC;
+ 
+ 			if (!(EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)) {
+ 				ext4_msg(sb, KERN_CRIT,
+ 					 "Delayed block allocation failed for "
+ 					 "inode %lu at logical offset %llu with"
+ 					 " max blocks %u with error %d",
+ 					 inode->i_ino,
+ 					 (unsigned long long)map->m_lblk,
+ 					 (unsigned)map->m_len, err);
+ 				ext4_msg(sb, KERN_CRIT,
+ 					 "This should not happen!! Data will "
+ 					 "be lost\n");
+ 				if (err == -ENOSPC)
+ 					ext4_print_free_blocks(inode);
+ 			}
+ 			/* invalidate all the pages */
+ 			mpage_release_unused_pages(mpd, true);
+ 			return err;
+ 		}
+ 		/*
+ 		 * Update buffer state, submit mapped pages, and get us new
+ 		 * extent to map
+ 		 */
+ 		err = mpage_map_and_submit_buffers(mpd);
+ 		if (err < 0)
+ 			return err;
+ 	}
+ 
+ 	/* Update on-disk size after IO is submitted */
+ 	disksize = ((loff_t)mpd->first_page) << PAGE_CACHE_SHIFT;
+ 	if (disksize > i_size_read(inode))
+ 		disksize = i_size_read(inode);
+ 	if (disksize > EXT4_I(inode)->i_disksize) {
+ 		int err2;
+ 
+ 		ext4_update_i_disksize(inode, disksize);
+ 		err2 = ext4_mark_inode_dirty(handle, inode);
+ 		if (err2)
+ 			ext4_error(inode->i_sb,
+ 				   "Failed to mark inode %lu dirty",
+ 				   inode->i_ino);
+ 		if (!err)
+ 			err = err2;
+ 	}
+ 	return err;
+ }
+ 
+ /*
++>>>>>>> 6b523df4fb5a (ext4: use transaction reservation for extent conversion in ext4_end_io)
   * Calculate the total number of credits to reserve for one writepages
   * iteration. This is called from ext4_da_writepages(). We map an extent of
   * upto MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping
@@@ -2395,13 -2357,10 +2731,17 @@@ static int ext4_da_writepages(struct ad
  	handle_t *handle = NULL;
  	struct mpage_da_data mpd;
  	struct inode *inode = mapping->host;
++<<<<<<< HEAD
 +	int pages_written = 0;
 +	int range_cyclic, cycled = 1, io_done = 0;
 +	int needed_blocks, ret = 0;
 +	loff_t range_start = wbc->range_start;
++=======
+ 	int needed_blocks, rsv_blocks = 0, ret = 0;
++>>>>>>> 6b523df4fb5a (ext4: use transaction reservation for extent conversion in ext4_end_io)
  	struct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);
 -	bool done;
 +	pgoff_t done_index = 0;
 +	pgoff_t end;
  	struct blk_plug plug;
  
  	trace_ext4_da_writepages(inode, wbc);
@@@ -2427,6 -2386,32 +2767,35 @@@
  	if (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED))
  		return -EROFS;
  
++<<<<<<< HEAD
++=======
+ 	if (ext4_should_dioread_nolock(inode)) {
+ 		/*
+ 		 * We may need to convert upto one extent per block in
+ 		 * the page and we may dirty the inode.
+ 		 */
+ 		rsv_blocks = 1 + (PAGE_CACHE_SIZE >> inode->i_blkbits);
+ 	}
+ 
+ 	/*
+ 	 * If we have inline data and arrive here, it means that
+ 	 * we will soon create the block for the 1st page, so
+ 	 * we'd better clear the inline data here.
+ 	 */
+ 	if (ext4_has_inline_data(inode)) {
+ 		/* Just inode will be modified... */
+ 		handle = ext4_journal_start(inode, EXT4_HT_INODE, 1);
+ 		if (IS_ERR(handle)) {
+ 			ret = PTR_ERR(handle);
+ 			goto out_writepages;
+ 		}
+ 		BUG_ON(ext4_test_inode_state(inode,
+ 				EXT4_STATE_MAY_INLINE_DATA));
+ 		ext4_destroy_inline_data(handle, inode);
+ 		ext4_journal_stop(handle);
+ 	}
+ 
++>>>>>>> 6b523df4fb5a (ext4: use transaction reservation for extent conversion in ext4_end_io)
  	if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
  		range_whole = 1;
  
@@@ -2460,9 -2452,9 +2829,15 @@@ retry
  		BUG_ON(ext4_should_journal_data(inode));
  		needed_blocks = ext4_da_writepages_trans_blocks(inode);
  
++<<<<<<< HEAD
 +		/* start a new transaction*/
 +		handle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,
 +					    needed_blocks);
++=======
+ 		/* start a new transaction */
+ 		handle = ext4_journal_start_with_reserve(inode,
+ 				EXT4_HT_WRITE_PAGE, needed_blocks, rsv_blocks);
++>>>>>>> 6b523df4fb5a (ext4: use transaction reservation for extent conversion in ext4_end_io)
  		if (IS_ERR(handle)) {
  			ret = PTR_ERR(handle);
  			ext4_msg(inode->i_sb, KERN_CRIT, "%s: jbd2_start: "
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index d137a1e96f9b..d22127b7b8e3 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -200,10 +200,13 @@ struct mpage_da_data {
 #define EXT4_IO_END_DIRECT	0x0004
 
 /*
- * For converting uninitialized extents on a work queue.
+ * For converting uninitialized extents on a work queue. 'handle' is used for
+ * buffered writeback.
  */
 typedef struct ext4_io_end {
 	struct list_head	list;		/* per-file finished IO list */
+	handle_t		*handle;	/* handle reserved for extent
+						 * conversion */
 	struct inode		*inode;		/* file being written to */
 	unsigned int		flag;		/* unwritten or not */
 	loff_t			offset;		/* offset in the file */
@@ -1344,6 +1347,9 @@ static inline void ext4_set_io_unwritten_flag(struct inode *inode,
 					      struct ext4_io_end *io_end)
 {
 	if (!(io_end->flag & EXT4_IO_END_UNWRITTEN)) {
+		/* Writeback has to have coversion transaction reserved */
+		WARN_ON(EXT4_SB(inode->i_sb)->s_journal && !io_end->handle &&
+			!(io_end->flag & EXT4_IO_END_DIRECT));
 		io_end->flag |= EXT4_IO_END_UNWRITTEN;
 		atomic_inc(&EXT4_I(inode)->i_unwritten);
 	}
@@ -2614,8 +2620,8 @@ extern void ext4_ext_init(struct super_block *);
 extern void ext4_ext_release(struct super_block *);
 extern long ext4_fallocate(struct file *file, int mode, loff_t offset,
 			  loff_t len);
-extern int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,
-			  ssize_t len);
+extern int ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,
+					  loff_t offset, ssize_t len);
 extern int ext4_map_blocks(handle_t *handle, struct inode *inode,
 			   struct ext4_map_blocks *map, int flags);
 extern int ext4_ext_calc_metadata_amount(struct inode *inode,
diff --git a/fs/ext4/ext4_jbd2.h b/fs/ext4/ext4_jbd2.h
index fdd865eb1879..2877258d9497 100644
--- a/fs/ext4/ext4_jbd2.h
+++ b/fs/ext4/ext4_jbd2.h
@@ -134,7 +134,8 @@ static inline int ext4_jbd2_credits_xattr(struct inode *inode)
 #define EXT4_HT_MIGRATE          8
 #define EXT4_HT_MOVE_EXTENTS     9
 #define EXT4_HT_XATTR           10
-#define EXT4_HT_MAX             11
+#define EXT4_HT_EXT_CONVERT     11
+#define EXT4_HT_MAX             12
 
 /**
  *   struct ext4_journal_cb_entry - Base structure for callback information.
@@ -319,7 +320,7 @@ static inline handle_t *__ext4_journal_start(struct inode *inode,
 #define ext4_journal_stop(handle) \
 	__ext4_journal_stop(__func__, __LINE__, (handle))
 
-#define ext4_journal_start_reserve(handle, type) \
+#define ext4_journal_start_reserved(handle, type) \
 	__ext4_journal_start_reserved((handle), __LINE__, (type))
 
 handle_t *__ext4_journal_start_reserved(handle_t *handle, unsigned int line,
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index e64022b15df1..daaa2b12e2b0 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -4581,10 +4581,9 @@ retry:
  * function, to convert the fallocated extents after IO is completed.
  * Returns 0 on success.
  */
-int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,
-				    ssize_t len)
+int ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,
+				   loff_t offset, ssize_t len)
 {
-	handle_t *handle;
 	unsigned int max_blocks;
 	int ret = 0;
 	int ret2 = 0;
@@ -4599,16 +4598,32 @@ int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,
 	max_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -
 		      map.m_lblk);
 	/*
-	 * credits to insert 1 extent into extent tree
+	 * This is somewhat ugly but the idea is clear: When transaction is
+	 * reserved, everything goes into it. Otherwise we rather start several
+	 * smaller transactions for conversion of each extent separately.
 	 */
-	credits = ext4_chunk_trans_blocks(inode, max_blocks);
+	if (handle) {
+		handle = ext4_journal_start_reserved(handle,
+						     EXT4_HT_EXT_CONVERT);
+		if (IS_ERR(handle))
+			return PTR_ERR(handle);
+		credits = 0;
+	} else {
+		/*
+		 * credits to insert 1 extent into extent tree
+		 */
+		credits = ext4_chunk_trans_blocks(inode, max_blocks);
+	}
 	while (ret >= 0 && ret < max_blocks) {
 		map.m_lblk += ret;
 		map.m_len = (max_blocks -= ret);
-		handle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS, credits);
-		if (IS_ERR(handle)) {
-			ret = PTR_ERR(handle);
-			break;
+		if (credits) {
+			handle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,
+						    credits);
+			if (IS_ERR(handle)) {
+				ret = PTR_ERR(handle);
+				break;
+			}
 		}
 		ret = ext4_map_blocks(handle, inode, &map,
 				      EXT4_GET_BLOCKS_IO_CONVERT_EXT);
@@ -4619,10 +4634,13 @@ int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,
 				     inode->i_ino, map.m_lblk,
 				     map.m_len, ret);
 		ext4_mark_inode_dirty(handle, inode);
-		ret2 = ext4_journal_stop(handle);
-		if (ret <= 0 || ret2 )
+		if (credits)
+			ret2 = ext4_journal_stop(handle);
+		if (ret <= 0 || ret2)
 			break;
 	}
+	if (!credits)
+		ret2 = ext4_journal_stop(handle);
 	return ret > 0 ? ret2 : ret;
 }
 
* Unmerged path fs/ext4/inode.c
diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index 34eb8521b0c1..2870e2d2ccb3 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -67,6 +67,7 @@ static void ext4_release_io_end(ext4_io_end_t *io_end)
 {
 	BUG_ON(!list_empty(&io_end->list));
 	BUG_ON(io_end->flag & EXT4_IO_END_UNWRITTEN);
+	WARN_ON(io_end->handle);
 
 	if (atomic_dec_and_test(&EXT4_I(io_end->inode)->i_ioend_count))
 		wake_up_all(ext4_ioend_wq(io_end->inode));
@@ -93,13 +94,15 @@ static int ext4_end_io(ext4_io_end_t *io)
 	struct inode *inode = io->inode;
 	loff_t offset = io->offset;
 	ssize_t size = io->size;
+	handle_t *handle = io->handle;
 	int ret = 0;
 
 	ext4_debug("ext4_end_io_nolock: io 0x%p from inode %lu,list->next 0x%p,"
 		   "list->prev 0x%p\n",
 		   io, inode->i_ino, io->list.next, io->list.prev);
 
-	ret = ext4_convert_unwritten_extents(inode, offset, size);
+	io->handle = NULL;	/* Following call will use up the handle */
+	ret = ext4_convert_unwritten_extents(handle, inode, offset, size);
 	if (ret < 0) {
 		ext4_msg(inode->i_sb, KERN_EMERG,
 			 "failed to convert unwritten extents to written "
@@ -229,8 +232,10 @@ int ext4_put_io_end(ext4_io_end_t *io_end)
 
 	if (atomic_dec_and_test(&io_end->count)) {
 		if (io_end->flag & EXT4_IO_END_UNWRITTEN) {
-			err = ext4_convert_unwritten_extents(io_end->inode,
-						io_end->offset, io_end->size);
+			err = ext4_convert_unwritten_extents(io_end->handle,
+						io_end->inode, io_end->offset,
+						io_end->size);
+			io_end->handle = NULL;
 			ext4_clear_io_unwritten_flag(io_end);
 		}
 		ext4_release_io_end(io_end);
