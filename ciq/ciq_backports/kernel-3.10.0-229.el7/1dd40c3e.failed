dm: introduce dm_accept_partial_bio

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit 1dd40c3ecd9b8a4ab91dbf2e6ce10b82a3b5ae63
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/1dd40c3e.failed

The function dm_accept_partial_bio allows the target to specify how many
sectors of the current bio it will process.  If the target only wants to
accept part of the bio, it calls dm_accept_partial_bio and the DM core
sends the rest of the data in next bio.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 1dd40c3ecd9b8a4ab91dbf2e6ce10b82a3b5ae63)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index a8b8e3774456,97940fc8c302..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -1271,7 -1240,7 +1311,11 @@@ static struct dm_target_io *alloc_tio(s
  
  static void __clone_and_map_simple_bio(struct clone_info *ci,
  				       struct dm_target *ti,
++<<<<<<< HEAD
 +				       unsigned target_bio_nr, sector_t len)
++=======
+ 				       unsigned target_bio_nr, unsigned *len)
++>>>>>>> 1dd40c3ecd9b (dm: introduce dm_accept_partial_bio)
  {
  	struct dm_target_io *tio = alloc_tio(ci, ti, ci->bio->bi_max_vecs, target_bio_nr);
  	struct bio *clone = &tio->clone;
@@@ -1281,15 -1252,15 +1327,19 @@@
  	 * ci->bio->bi_max_vecs is BIO_INLINE_VECS anyway, for both flush
  	 * and discard, so no need for concern about wasted bvec allocations.
  	 */
 -	 __bio_clone_fast(clone, ci->bio);
 +	 __bio_clone(clone, ci->bio);
  	if (len)
- 		bio_setup_sector(clone, ci->sector, len);
+ 		bio_setup_sector(clone, ci->sector, *len);
  
  	__map_bio(tio);
  }
  
  static void __send_duplicate_bios(struct clone_info *ci, struct dm_target *ti,
++<<<<<<< HEAD
 +				  unsigned num_bios, sector_t len)
++=======
+ 				  unsigned num_bios, unsigned *len)
++>>>>>>> 1dd40c3ecd9b (dm: introduce dm_accept_partial_bio)
  {
  	unsigned target_bio_nr;
  
@@@ -1310,10 -1281,7 +1360,14 @@@ static int __send_empty_flush(struct cl
  }
  
  static void __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,
++<<<<<<< HEAD
 +				     sector_t sector, int nr_iovecs,
 +				     unsigned short idx, unsigned short bv_count,
 +				     unsigned offset, unsigned len,
 +				     unsigned split_bvec)
++=======
+ 				     sector_t sector, unsigned *len)
++>>>>>>> 1dd40c3ecd9b (dm: introduce dm_accept_partial_bio)
  {
  	struct bio *bio = ci->bio;
  	struct dm_target_io *tio;
@@@ -1327,11 -1295,9 +1381,17 @@@
  		num_target_bios = ti->num_write_bios(ti, bio);
  
  	for (target_bio_nr = 0; target_bio_nr < num_target_bios; target_bio_nr++) {
++<<<<<<< HEAD
 +		tio = alloc_tio(ci, ti, nr_iovecs, target_bio_nr);
 +		if (split_bvec)
 +			clone_split_bio(tio, bio, sector, idx, offset, len);
 +		else
 +			clone_bio(tio, bio, sector, idx, bv_count, len);
++=======
+ 		tio = alloc_tio(ci, ti, 0, target_bio_nr);
+ 		tio->len_ptr = len;
+ 		clone_bio(tio, bio, sector, *len);
++>>>>>>> 1dd40c3ecd9b (dm: introduce dm_accept_partial_bio)
  		__map_bio(tio);
  	}
  }
@@@ -1379,11 -1345,11 +1439,11 @@@ static int __send_changing_extent_only(
  			return -EOPNOTSUPP;
  
  		if (is_split_required && !is_split_required(ti))
 -			len = min((sector_t)ci->sector_count, max_io_len_target_boundary(ci->sector, ti));
 +			len = min(ci->sector_count, max_io_len_target_boundary(ci->sector, ti));
  		else
 -			len = min((sector_t)ci->sector_count, max_io_len(ci->sector, ti));
 +			len = min(ci->sector_count, max_io_len(ci->sector, ti));
  
- 		__send_duplicate_bios(ci, ti, num_bios, len);
+ 		__send_duplicate_bios(ci, ti, num_bios, &len);
  
  		ci->sector += len;
  	} while (ci->sector_count -= len);
@@@ -1475,41 -1386,14 +1535,45 @@@ static int __split_and_process_non_flus
  	if (!dm_target_is_valid(ti))
  		return -EIO;
  
 -	len = min_t(sector_t, max_io_len(ci->sector, ti), ci->sector_count);
 +	max = max_io_len(ci->sector, ti);
  
++<<<<<<< HEAD
 +	/*
 +	 * Optimise for the simple case where we can do all of
 +	 * the remaining io with a single clone.
 +	 */
 +	if (ci->sector_count <= max) {
 +		__clone_and_map_data_bio(ci, ti, ci->sector, bio->bi_max_vecs,
 +					 ci->idx, bio->bi_vcnt - ci->idx, 0,
 +					 ci->sector_count, 0);
 +		ci->sector_count = 0;
 +		return 0;
 +	}
++=======
+ 	__clone_and_map_data_bio(ci, ti, ci->sector, &len);
++>>>>>>> 1dd40c3ecd9b (dm: introduce dm_accept_partial_bio)
 +
 +	/*
 +	 * There are some bvecs that don't span targets.
 +	 * Do as many of these as possible.
 +	 */
 +	if (to_sector(bio->bi_io_vec[ci->idx].bv_len) <= max) {
 +		len = __len_within_target(ci, max, &idx);
  
 -	ci->sector += len;
 -	ci->sector_count -= len;
 +		__clone_and_map_data_bio(ci, ti, ci->sector, bio->bi_max_vecs,
 +					 ci->idx, idx - ci->idx, 0, len, 0);
  
 -	return 0;
 +		ci->sector += len;
 +		ci->sector_count -= len;
 +		ci->idx = idx;
 +
 +		return 0;
 +	}
 +
 +	/*
 +	 * Handle a bvec that must be split between two or more targets.
 +	 */
 +	return __split_bvec_across_targets(ci, ti, max);
  }
  
  /*
* Unmerged path drivers/md/dm.c
diff --git a/include/linux/device-mapper.h b/include/linux/device-mapper.h
index 02688404d10f..e1707de043ae 100644
--- a/include/linux/device-mapper.h
+++ b/include/linux/device-mapper.h
@@ -285,6 +285,7 @@ struct dm_target_io {
 	struct dm_io *io;
 	struct dm_target *ti;
 	unsigned target_bio_nr;
+	unsigned *len_ptr;
 	struct bio clone;
 };
 
@@ -395,6 +396,7 @@ int dm_copy_name_and_uuid(struct mapped_device *md, char *name, char *uuid);
 struct gendisk *dm_disk(struct mapped_device *md);
 int dm_suspended(struct dm_target *ti);
 int dm_noflush_suspending(struct dm_target *ti);
+void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors);
 union map_info *dm_get_rq_mapinfo(struct request *rq);
 
 struct queue_limits *dm_get_queue_limits(struct mapped_device *md);
