powerpc: Fix unsafe accesses to parameter area in ELFv2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [powerpc] Fix unsafe accesses to parameter area in ELFv2 (Don Zickus) [1127366]
Rebuild_FUZZ: 91.09%
commit-author Ulrich Weigand <ulrich.weigand@de.ibm.com>
commit 752a6422fec3c0f5f9d4ac43d92f5dd13e22fde4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/752a6422.failed

Some of the assembler files in lib/ make use of the fact that in the
ELFv1 ABI, the caller guarantees to provide stack space to save the
parameter registers r3 ... r10.  This guarantee is no longer present
in ELFv2 for functions that have no variable argument list and no
more than 8 arguments.

Change the affected routines to temporarily store registers in the
red zone and/or the top of their own stack frame (in the space
provided to save r31 .. r29, which is actually not used in these
routines).

In opal_query_takeover, simply always allocate a stack frame;
the routine is not performance critical.

	Signed-off-by: Ulrich Weigand <ulrich.weigand@de.ibm.com>
	Signed-off-by: Anton Blanchard <anton@samba.org>
(cherry picked from commit 752a6422fec3c0f5f9d4ac43d92f5dd13e22fde4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/lib/copypage_power7.S
#	arch/powerpc/lib/copyuser_power7.S
#	arch/powerpc/lib/memcpy_64.S
#	arch/powerpc/lib/memcpy_power7.S
diff --cc arch/powerpc/lib/copypage_power7.S
index 395c594722a2,d7dafb3777ac..000000000000
--- a/arch/powerpc/lib/copypage_power7.S
+++ b/arch/powerpc/lib/copypage_power7.S
@@@ -56,15 -56,15 +56,25 @@@ _GLOBAL(copypage_power7
  
  #ifdef CONFIG_ALTIVEC
  	mflr	r0
++<<<<<<< HEAD
 +	std	r3,48(r1)
 +	std	r4,56(r1)
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	std	r4,-STACKFRAMESIZE+STK_REG(R30)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	std	r0,16(r1)
  	stdu	r1,-STACKFRAMESIZE(r1)
 -	bl	enter_vmx_copy
 +	bl	.enter_vmx_copy
  	cmpwi	r3,0
  	ld	r0,STACKFRAMESIZE+16(r1)
++<<<<<<< HEAD
 +	ld	r3,STACKFRAMESIZE+48(r1)
 +	ld	r4,STACKFRAMESIZE+56(r1)
++=======
+ 	ld	r3,STK_REG(R31)(r1)
+ 	ld	r4,STK_REG(R30)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	mtlr	r0
  
  	li	r0,(PAGE_SIZE/128)
diff --cc arch/powerpc/lib/copyuser_power7.S
index e8e9c36dc784,c46c876ac96a..000000000000
--- a/arch/powerpc/lib/copyuser_power7.S
+++ b/arch/powerpc/lib/copyuser_power7.S
@@@ -85,9 -85,9 +85,15 @@@
  .Lexit:
  	addi	r1,r1,STACKFRAMESIZE
  .Ldo_err1:
++<<<<<<< HEAD
 +	ld	r3,48(r1)
 +	ld	r4,56(r1)
 +	ld	r5,64(r1)
++=======
+ 	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	ld	r4,-STACKFRAMESIZE+STK_REG(R30)(r1)
+ 	ld	r5,-STACKFRAMESIZE+STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	b	__copy_tofrom_user_base
  
  
@@@ -96,18 -96,18 +102,30 @@@ _GLOBAL(__copy_tofrom_user_power7
  	cmpldi	r5,16
  	cmpldi	cr1,r5,4096
  
++<<<<<<< HEAD
 +	std	r3,48(r1)
 +	std	r4,56(r1)
 +	std	r5,64(r1)
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	std	r4,-STACKFRAMESIZE+STK_REG(R30)(r1)
+ 	std	r5,-STACKFRAMESIZE+STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  
  	blt	.Lshort_copy
  	bgt	cr1,.Lvmx_copy
  #else
  	cmpldi	r5,16
  
++<<<<<<< HEAD
 +	std	r3,48(r1)
 +	std	r4,56(r1)
 +	std	r5,64(r1)
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	std	r4,-STACKFRAMESIZE+STK_REG(R30)(r1)
+ 	std	r5,-STACKFRAMESIZE+STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  
  	blt	.Lshort_copy
  #endif
@@@ -295,12 -295,12 +313,18 @@@ err1;	stb	r0,0(r3
  	mflr	r0
  	std	r0,16(r1)
  	stdu	r1,-STACKFRAMESIZE(r1)
 -	bl	enter_vmx_usercopy
 +	bl	.enter_vmx_usercopy
  	cmpwi	cr1,r3,0
  	ld	r0,STACKFRAMESIZE+16(r1)
++<<<<<<< HEAD
 +	ld	r3,STACKFRAMESIZE+48(r1)
 +	ld	r4,STACKFRAMESIZE+56(r1)
 +	ld	r5,STACKFRAMESIZE+64(r1)
++=======
+ 	ld	r3,STK_REG(R31)(r1)
+ 	ld	r4,STK_REG(R30)(r1)
+ 	ld	r5,STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	mtlr	r0
  
  	/*
diff --cc arch/powerpc/lib/memcpy_64.S
index d2bbbc8d7dc0,9d3960c16fde..000000000000
--- a/arch/powerpc/lib/memcpy_64.S
+++ b/arch/powerpc/lib/memcpy_64.S
@@@ -12,9 -12,11 +12,13 @@@
  	.align	7
  _GLOBAL(memcpy)
  BEGIN_FTR_SECTION
++<<<<<<< HEAD
 +	std	r3,48(r1)	/* save destination pointer for return value */
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)	/* save destination pointer for return value */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  FTR_SECTION_ELSE
 -#ifndef SELFTEST
  	b	memcpy_power7
 -#endif
  ALT_FTR_SECTION_END_IFCLR(CPU_FTR_VMX_COPY)
  	PPC_MTOCRF(0x01,r5)
  	cmpldi	cr1,r5,16
@@@ -71,7 -73,7 +75,11 @@@ END_FTR_SECTION_IFCLR(CPU_FTR_UNALIGNED
  2:	bf	cr7*4+3,3f
  	lbz	r9,8(r4)
  	stb	r9,0(r3)
++<<<<<<< HEAD
 +3:	ld	r3,48(r1)	/* return dest pointer */
++=======
+ 3:	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)	/* return dest pointer */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	blr
  
  .Lsrc_unaligned:
@@@ -154,7 -156,7 +162,11 @@@
  2:	bf	cr7*4+3,3f
  	rotldi	r9,r9,8
  	stb	r9,0(r3)
++<<<<<<< HEAD
 +3:	ld	r3,48(r1)	/* return dest pointer */
++=======
+ 3:	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)	/* return dest pointer */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	blr
  
  .Ldst_unaligned:
@@@ -199,5 -201,5 +211,9 @@@
  3:	bf	cr7*4+3,4f
  	lbz	r0,0(r4)
  	stb	r0,0(r3)
++<<<<<<< HEAD
 +4:	ld	r3,48(r1)	/* return dest pointer */
++=======
+ 4:	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)	/* return dest pointer */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	blr
diff --cc arch/powerpc/lib/memcpy_power7.S
index e4177dbea6bd,2ff5c142f87b..000000000000
--- a/arch/powerpc/lib/memcpy_power7.S
+++ b/arch/powerpc/lib/memcpy_power7.S
@@@ -33,14 -33,14 +33,22 @@@ _GLOBAL(memcpy_power7
  	cmpldi	r5,16
  	cmpldi	cr1,r5,4096
  
++<<<<<<< HEAD
 +	std	r3,48(r1)
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  
  	blt	.Lshort_copy
  	bgt	cr1,.Lvmx_copy
  #else
  	cmpldi	r5,16
  
++<<<<<<< HEAD
 +	std	r3,48(r1)
++=======
+ 	std	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  
  	blt	.Lshort_copy
  #endif
@@@ -216,7 -216,7 +224,11 @@@
  	lbz	r0,0(r4)
  	stb	r0,0(r3)
  
++<<<<<<< HEAD
 +15:	ld	r3,48(r1)
++=======
+ 15:	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	blr
  
  .Lunwind_stack_nonvmx_copy:
@@@ -226,16 -226,16 +238,27 @@@
  #ifdef CONFIG_ALTIVEC
  .Lvmx_copy:
  	mflr	r0
++<<<<<<< HEAD
 +	std	r4,56(r1)
 +	std	r5,64(r1)
++=======
+ 	std	r4,-STACKFRAMESIZE+STK_REG(R30)(r1)
+ 	std	r5,-STACKFRAMESIZE+STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	std	r0,16(r1)
  	stdu	r1,-STACKFRAMESIZE(r1)
 -	bl	enter_vmx_copy
 +	bl	.enter_vmx_copy
  	cmpwi	cr1,r3,0
  	ld	r0,STACKFRAMESIZE+16(r1)
++<<<<<<< HEAD
 +	ld	r3,STACKFRAMESIZE+48(r1)
 +	ld	r4,STACKFRAMESIZE+56(r1)
 +	ld	r5,STACKFRAMESIZE+64(r1)
++=======
+ 	ld	r3,STK_REG(R31)(r1)
+ 	ld	r4,STK_REG(R30)(r1)
+ 	ld	r5,STK_REG(R29)(r1)
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  	mtlr	r0
  
  	/*
@@@ -447,8 -447,8 +470,13 @@@
  	stb	r0,0(r3)
  
  15:	addi	r1,r1,STACKFRAMESIZE
++<<<<<<< HEAD
 +	ld	r3,48(r1)
 +	b	.exit_vmx_copy		/* tail call optimise */
++=======
+ 	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	b	exit_vmx_copy		/* tail call optimise */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  
  .Lvmx_unaligned_copy:
  	/* Get the destination 16B aligned */
@@@ -651,6 -651,6 +679,11 @@@
  	stb	r0,0(r3)
  
  15:	addi	r1,r1,STACKFRAMESIZE
++<<<<<<< HEAD
 +	ld	r3,48(r1)
 +	b	.exit_vmx_copy		/* tail call optimise */
++=======
+ 	ld	r3,-STACKFRAMESIZE+STK_REG(R31)(r1)
+ 	b	exit_vmx_copy		/* tail call optimise */
++>>>>>>> 752a6422fec3 (powerpc: Fix unsafe accesses to parameter area in ELFv2)
  #endif /* CONFiG_ALTIVEC */
* Unmerged path arch/powerpc/lib/copypage_power7.S
* Unmerged path arch/powerpc/lib/copyuser_power7.S
* Unmerged path arch/powerpc/lib/memcpy_64.S
* Unmerged path arch/powerpc/lib/memcpy_power7.S
diff --git a/arch/powerpc/platforms/powernv/opal-takeover.S b/arch/powerpc/platforms/powernv/opal-takeover.S
index 3cd262897c27..11a3169ee583 100644
--- a/arch/powerpc/platforms/powernv/opal-takeover.S
+++ b/arch/powerpc/platforms/powernv/opal-takeover.S
@@ -21,11 +21,13 @@
 _GLOBAL(opal_query_takeover)
 	mfcr	r0
 	stw	r0,8(r1)
+	stdu	r1,-STACKFRAMESIZE(r1)
 	std	r3,STK_PARAM(R3)(r1)
 	std	r4,STK_PARAM(R4)(r1)
 	li	r3,H_HAL_TAKEOVER
 	li	r4,H_HAL_TAKEOVER_QUERY_MAGIC
 	HVSC
+	addi	r1,r1,STACKFRAMESIZE
 	ld	r10,STK_PARAM(R3)(r1)
 	std	r4,0(r10)
 	ld	r10,STK_PARAM(R4)(r1)
