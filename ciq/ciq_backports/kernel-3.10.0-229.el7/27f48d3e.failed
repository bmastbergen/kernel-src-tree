kexec-bzImage64: support for loading bzImage using 64bit entry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Vivek Goyal <vgoyal@redhat.com>
commit 27f48d3e633be23656a097baa3be336e04a82d84
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/27f48d3e.failed

This is loader specific code which can load bzImage and set it up for
64bit entry.  This does not take care of 32bit entry or real mode entry.

32bit mode entry can be implemented if somebody needs it.

	Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Michael Kerrisk <mtk.manpages@gmail.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: Eric Biederman <ebiederm@xmission.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Matthew Garrett <mjg59@srcf.ucam.org>
	Cc: Greg Kroah-Hartman <greg@kroah.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: WANG Chao <chaowang@redhat.com>
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 27f48d3e633be23656a097baa3be336e04a82d84)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kexec.h
#	arch/x86/kernel/machine_kexec_64.c
#	include/linux/kexec.h
#	kernel/kexec.c
diff --cc arch/x86/include/asm/kexec.h
index c1bf8d95576d,0dfccced4edf..000000000000
--- a/arch/x86/include/asm/kexec.h
+++ b/arch/x86/include/asm/kexec.h
@@@ -23,7 -23,7 +23,11 @@@
  
  #include <asm/page.h>
  #include <asm/ptrace.h>
++<<<<<<< HEAD
 +#include <asm-generic/kexec.h>
++=======
+ #include <asm/bootparam.h>
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
  
  /*
   * KEXEC_SOURCE_MEMORY_LIMIT maximum page get_free_page can return.
diff --cc arch/x86/kernel/machine_kexec_64.c
index 4eabc160696f,18d0f9e0b6da..000000000000
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@@ -21,7 -23,12 +21,15 @@@
  #include <asm/tlbflush.h>
  #include <asm/mmu_context.h>
  #include <asm/debugreg.h>
+ #include <asm/kexec-bzimage64.h>
  
++<<<<<<< HEAD
++=======
+ static struct kexec_file_ops *kexec_file_loaders[] = {
+ 		&kexec_bzImage64_ops,
+ };
+ 
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
  static void free_transition_pgtable(struct kimage *image)
  {
  	free_page((unsigned long)image->arch.pud);
@@@ -279,5 -286,188 +287,189 @@@ void arch_crash_save_vmcoreinfo(void
  	VMCOREINFO_SYMBOL(node_data);
  	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
  #endif
 -	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 -			      (unsigned long)&_text - __START_KERNEL);
  }
  
++<<<<<<< HEAD
++=======
+ /* arch-dependent functionality related to kexec file-based syscall */
+ 
+ int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
+ 				  unsigned long buf_len)
+ {
+ 	int i, ret = -ENOEXEC;
+ 	struct kexec_file_ops *fops;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(kexec_file_loaders); i++) {
+ 		fops = kexec_file_loaders[i];
+ 		if (!fops || !fops->probe)
+ 			continue;
+ 
+ 		ret = fops->probe(buf, buf_len);
+ 		if (!ret) {
+ 			image->fops = fops;
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ void *arch_kexec_kernel_image_load(struct kimage *image)
+ {
+ 	if (!image->fops || !image->fops->load)
+ 		return ERR_PTR(-ENOEXEC);
+ 
+ 	return image->fops->load(image, image->kernel_buf,
+ 				 image->kernel_buf_len, image->initrd_buf,
+ 				 image->initrd_buf_len, image->cmdline_buf,
+ 				 image->cmdline_buf_len);
+ }
+ 
+ int arch_kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ 	if (!image->fops || !image->fops->cleanup)
+ 		return 0;
+ 
+ 	return image->fops->cleanup(image->image_loader_data);
+ }
+ 
+ /*
+  * Apply purgatory relocations.
+  *
+  * ehdr: Pointer to elf headers
+  * sechdrs: Pointer to section headers.
+  * relsec: section index of SHT_RELA section.
+  *
+  * TODO: Some of the code belongs to generic code. Move that in kexec.c.
+  */
+ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
+ 				     Elf64_Shdr *sechdrs, unsigned int relsec)
+ {
+ 	unsigned int i;
+ 	Elf64_Rela *rel;
+ 	Elf64_Sym *sym;
+ 	void *location;
+ 	Elf64_Shdr *section, *symtabsec;
+ 	unsigned long address, sec_base, value;
+ 	const char *strtab, *name, *shstrtab;
+ 
+ 	/*
+ 	 * ->sh_offset has been modified to keep the pointer to section
+ 	 * contents in memory
+ 	 */
+ 	rel = (void *)sechdrs[relsec].sh_offset;
+ 
+ 	/* Section to which relocations apply */
+ 	section = &sechdrs[sechdrs[relsec].sh_info];
+ 
+ 	pr_debug("Applying relocate section %u to %u\n", relsec,
+ 		 sechdrs[relsec].sh_info);
+ 
+ 	/* Associated symbol table */
+ 	symtabsec = &sechdrs[sechdrs[relsec].sh_link];
+ 
+ 	/* String table */
+ 	if (symtabsec->sh_link >= ehdr->e_shnum) {
+ 		/* Invalid strtab section number */
+ 		pr_err("Invalid string table section index %d\n",
+ 		       symtabsec->sh_link);
+ 		return -ENOEXEC;
+ 	}
+ 
+ 	strtab = (char *)sechdrs[symtabsec->sh_link].sh_offset;
+ 
+ 	/* section header string table */
+ 	shstrtab = (char *)sechdrs[ehdr->e_shstrndx].sh_offset;
+ 
+ 	for (i = 0; i < sechdrs[relsec].sh_size / sizeof(*rel); i++) {
+ 
+ 		/*
+ 		 * rel[i].r_offset contains byte offset from beginning
+ 		 * of section to the storage unit affected.
+ 		 *
+ 		 * This is location to update (->sh_offset). This is temporary
+ 		 * buffer where section is currently loaded. This will finally
+ 		 * be loaded to a different address later, pointed to by
+ 		 * ->sh_addr. kexec takes care of moving it
+ 		 *  (kexec_load_segment()).
+ 		 */
+ 		location = (void *)(section->sh_offset + rel[i].r_offset);
+ 
+ 		/* Final address of the location */
+ 		address = section->sh_addr + rel[i].r_offset;
+ 
+ 		/*
+ 		 * rel[i].r_info contains information about symbol table index
+ 		 * w.r.t which relocation must be made and type of relocation
+ 		 * to apply. ELF64_R_SYM() and ELF64_R_TYPE() macros get
+ 		 * these respectively.
+ 		 */
+ 		sym = (Elf64_Sym *)symtabsec->sh_offset +
+ 				ELF64_R_SYM(rel[i].r_info);
+ 
+ 		if (sym->st_name)
+ 			name = strtab + sym->st_name;
+ 		else
+ 			name = shstrtab + sechdrs[sym->st_shndx].sh_name;
+ 
+ 		pr_debug("Symbol: %s info: %02x shndx: %02x value=%llx size: %llx\n",
+ 			 name, sym->st_info, sym->st_shndx, sym->st_value,
+ 			 sym->st_size);
+ 
+ 		if (sym->st_shndx == SHN_UNDEF) {
+ 			pr_err("Undefined symbol: %s\n", name);
+ 			return -ENOEXEC;
+ 		}
+ 
+ 		if (sym->st_shndx == SHN_COMMON) {
+ 			pr_err("symbol '%s' in common section\n", name);
+ 			return -ENOEXEC;
+ 		}
+ 
+ 		if (sym->st_shndx == SHN_ABS)
+ 			sec_base = 0;
+ 		else if (sym->st_shndx >= ehdr->e_shnum) {
+ 			pr_err("Invalid section %d for symbol %s\n",
+ 			       sym->st_shndx, name);
+ 			return -ENOEXEC;
+ 		} else
+ 			sec_base = sechdrs[sym->st_shndx].sh_addr;
+ 
+ 		value = sym->st_value;
+ 		value += sec_base;
+ 		value += rel[i].r_addend;
+ 
+ 		switch (ELF64_R_TYPE(rel[i].r_info)) {
+ 		case R_X86_64_NONE:
+ 			break;
+ 		case R_X86_64_64:
+ 			*(u64 *)location = value;
+ 			break;
+ 		case R_X86_64_32:
+ 			*(u32 *)location = value;
+ 			if (value != *(u32 *)location)
+ 				goto overflow;
+ 			break;
+ 		case R_X86_64_32S:
+ 			*(s32 *)location = value;
+ 			if ((s64)value != *(s32 *)location)
+ 				goto overflow;
+ 			break;
+ 		case R_X86_64_PC32:
+ 			value -= (u64)address;
+ 			*(u32 *)location = value;
+ 			break;
+ 		default:
+ 			pr_err("Unknown rela relocation: %llu\n",
+ 			       ELF64_R_TYPE(rel[i].r_info));
+ 			return -ENOEXEC;
+ 		}
+ 	}
+ 	return 0;
+ 
+ overflow:
+ 	pr_err("Overflow in relocation type %d value 0x%lx\n",
+ 	       (int)ELF64_R_TYPE(rel[i].r_info), value);
+ 	return -ENOEXEC;
+ }
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
diff --cc include/linux/kexec.h
index 66d56ac0f64c,9481703b0e7a..000000000000
--- a/include/linux/kexec.h
+++ b/include/linux/kexec.h
@@@ -125,9 -149,54 +125,24 @@@ struct kimage 
  #ifdef ARCH_HAS_KIMAGE_ARCH
  	struct kimage_arch arch;
  #endif
 -
 -	/* Additional fields for file based kexec syscall */
 -	void *kernel_buf;
 -	unsigned long kernel_buf_len;
 -
 -	void *initrd_buf;
 -	unsigned long initrd_buf_len;
 -
 -	char *cmdline_buf;
 -	unsigned long cmdline_buf_len;
 -
 -	/* File operations provided by image loader */
 -	struct kexec_file_ops *fops;
 -
 -	/* Image loader handling the kernel can store a pointer here */
 -	void *image_loader_data;
 -
 -	/* Information for loading purgatory */
 -	struct purgatory_info purgatory_info;
  };
  
 -/*
 - * Keeps track of buffer parameters as provided by caller for requesting
 - * memory placement of buffer.
 - */
 -struct kexec_buf {
 -	struct kimage *image;
 -	char *buffer;
 -	unsigned long bufsz;
 -	unsigned long memsz;
 -	unsigned long buf_align;
 -	unsigned long buf_min;
 -	unsigned long buf_max;
 -	bool top_down;		/* allocate from top of memory hole */
 -};
  
++<<<<<<< HEAD
++=======
+ typedef int (kexec_probe_t)(const char *kernel_buf, unsigned long kernel_size);
+ typedef void *(kexec_load_t)(struct kimage *image, char *kernel_buf,
+ 			     unsigned long kernel_len, char *initrd,
+ 			     unsigned long initrd_len, char *cmdline,
+ 			     unsigned long cmdline_len);
+ typedef int (kexec_cleanup_t)(void *loader_data);
+ 
+ struct kexec_file_ops {
+ 	kexec_probe_t *probe;
+ 	kexec_load_t *load;
+ 	kexec_cleanup_t *cleanup;
+ };
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
  
  /* kexec interface functions */
  extern void machine_kexec(struct kimage *image);
diff --cc kernel/kexec.c
index 4b1d9ca3907c,0926f2a3ed03..000000000000
--- a/kernel/kexec.c
+++ b/kernel/kexec.c
@@@ -293,57 -361,232 +293,251 @@@ static int kimage_crash_alloc(struct ki
  		goto out;
  	}
  
 -	/* Don't hand 0 to vmalloc, it whines. */
 -	if (stat.size == 0) {
 -		ret = -EINVAL;
 +	/* Allocate and initialize a controlling structure */
 +	result = do_kimage_alloc(&image, entry, nr_segments, segments);
 +	if (result)
  		goto out;
 -	}
  
 -	*buf = vmalloc(stat.size);
 -	if (!*buf) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 +	/* Enable the special crash kernel control page
 +	 * allocation policy.
 +	 */
 +	image->control_page = crashk_res.start;
 +	image->type = KEXEC_TYPE_CRASH;
  
 -	pos = 0;
 -	while (pos < stat.size) {
 -		bytes = kernel_read(f.file, pos, (char *)(*buf) + pos,
 -				    stat.size - pos);
 -		if (bytes < 0) {
 -			vfree(*buf);
 -			ret = bytes;
 -			goto out;
 -		}
 +	/*
 +	 * Verify we have good destination addresses.  Normally
 +	 * the caller is responsible for making certain we don't
 +	 * attempt to load the new image into invalid or reserved
 +	 * areas of RAM.  But crash kernels are preloaded into a
 +	 * reserved area of ram.  We must ensure the addresses
 +	 * are in the reserved area otherwise preloading the
 +	 * kernel could corrupt things.
 +	 */
 +	result = -EADDRNOTAVAIL;
 +	for (i = 0; i < nr_segments; i++) {
 +		unsigned long mstart, mend;
  
 -		if (bytes == 0)
 -			break;
 -		pos += bytes;
 +		mstart = image->segment[i].mem;
 +		mend = mstart + image->segment[i].memsz - 1;
 +		/* Ensure we are within the crash kernel limits */
 +		if ((mstart < crashk_res.start) || (mend > crashk_res.end))
 +			goto out_free;
  	}
  
 -	if (pos != stat.size) {
 -		ret = -EBADF;
 -		vfree(*buf);
 -		goto out;
 +	/*
 +	 * Find a location for the control code buffer, and add
 +	 * the vector of segments so that it's pages will also be
 +	 * counted as destination pages.
 +	 */
 +	result = -ENOMEM;
 +	image->control_code_page = kimage_alloc_control_pages(image,
 +					   get_order(KEXEC_CONTROL_PAGE_SIZE));
 +	if (!image->control_code_page) {
 +		printk(KERN_ERR "Could not allocate control_code_buffer\n");
 +		goto out_free;
  	}
  
 -	*buf_len = pos;
 +	*rimage = image;
 +	return 0;
 +
 +out_free:
 +	kfree(image);
  out:
++<<<<<<< HEAD
 +	return result;
++=======
+ 	fdput(f);
+ 	return ret;
+ }
+ 
+ /* Architectures can provide this probe function */
+ int __weak arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
+ 					 unsigned long buf_len)
+ {
+ 	return -ENOEXEC;
+ }
+ 
+ void * __weak arch_kexec_kernel_image_load(struct kimage *image)
+ {
+ 	return ERR_PTR(-ENOEXEC);
+ }
+ 
+ void __weak arch_kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ }
+ 
+ /* Apply relocations of type RELA */
+ int __weak
+ arch_kexec_apply_relocations_add(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
+ 				 unsigned int relsec)
+ {
+ 	pr_err("RELA relocation unsupported.\n");
+ 	return -ENOEXEC;
+ }
+ 
+ /* Apply relocations of type REL */
+ int __weak
+ arch_kexec_apply_relocations(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
+ 			     unsigned int relsec)
+ {
+ 	pr_err("REL relocation unsupported.\n");
+ 	return -ENOEXEC;
+ }
+ 
+ /*
+  * Free up memory used by kernel, initrd, and comand line. This is temporary
+  * memory allocation which is not needed any more after these buffers have
+  * been loaded into separate segments and have been copied elsewhere.
+  */
+ static void kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 
+ 	vfree(image->kernel_buf);
+ 	image->kernel_buf = NULL;
+ 
+ 	vfree(image->initrd_buf);
+ 	image->initrd_buf = NULL;
+ 
+ 	kfree(image->cmdline_buf);
+ 	image->cmdline_buf = NULL;
+ 
+ 	vfree(pi->purgatory_buf);
+ 	pi->purgatory_buf = NULL;
+ 
+ 	vfree(pi->sechdrs);
+ 	pi->sechdrs = NULL;
+ 
+ 	/* See if architecture has anything to cleanup post load */
+ 	arch_kimage_file_post_load_cleanup(image);
+ 
+ 	/*
+ 	 * Above call should have called into bootloader to free up
+ 	 * any data stored in kimage->image_loader_data. It should
+ 	 * be ok now to free it up.
+ 	 */
+ 	kfree(image->image_loader_data);
+ 	image->image_loader_data = NULL;
+ }
+ 
+ /*
+  * In file mode list of segments is prepared by kernel. Copy relevant
+  * data from user space, do error checking, prepare segment list
+  */
+ static int
+ kimage_file_prepare_segments(struct kimage *image, int kernel_fd, int initrd_fd,
+ 			     const char __user *cmdline_ptr,
+ 			     unsigned long cmdline_len, unsigned flags)
+ {
+ 	int ret = 0;
+ 	void *ldata;
+ 
+ 	ret = copy_file_from_fd(kernel_fd, &image->kernel_buf,
+ 				&image->kernel_buf_len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Call arch image probe handlers */
+ 	ret = arch_kexec_kernel_image_probe(image, image->kernel_buf,
+ 					    image->kernel_buf_len);
+ 
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* It is possible that there no initramfs is being loaded */
+ 	if (!(flags & KEXEC_FILE_NO_INITRAMFS)) {
+ 		ret = copy_file_from_fd(initrd_fd, &image->initrd_buf,
+ 					&image->initrd_buf_len);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	if (cmdline_len) {
+ 		image->cmdline_buf = kzalloc(cmdline_len, GFP_KERNEL);
+ 		if (!image->cmdline_buf) {
+ 			ret = -ENOMEM;
+ 			goto out;
+ 		}
+ 
+ 		ret = copy_from_user(image->cmdline_buf, cmdline_ptr,
+ 				     cmdline_len);
+ 		if (ret) {
+ 			ret = -EFAULT;
+ 			goto out;
+ 		}
+ 
+ 		image->cmdline_buf_len = cmdline_len;
+ 
+ 		/* command line should be a string with last byte null */
+ 		if (image->cmdline_buf[cmdline_len - 1] != '\0') {
+ 			ret = -EINVAL;
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	/* Call arch image load handlers */
+ 	ldata = arch_kexec_kernel_image_load(image);
+ 
+ 	if (IS_ERR(ldata)) {
+ 		ret = PTR_ERR(ldata);
+ 		goto out;
+ 	}
+ 
+ 	image->image_loader_data = ldata;
+ out:
+ 	/* In case of error, free up all allocated memory in this function */
+ 	if (ret)
+ 		kimage_file_post_load_cleanup(image);
+ 	return ret;
+ }
+ 
+ static int
+ kimage_file_alloc_init(struct kimage **rimage, int kernel_fd,
+ 		       int initrd_fd, const char __user *cmdline_ptr,
+ 		       unsigned long cmdline_len, unsigned long flags)
+ {
+ 	int ret;
+ 	struct kimage *image;
+ 
+ 	image = do_kimage_alloc_init();
+ 	if (!image)
+ 		return -ENOMEM;
+ 
+ 	image->file_mode = 1;
+ 
+ 	ret = kimage_file_prepare_segments(image, kernel_fd, initrd_fd,
+ 					   cmdline_ptr, cmdline_len, flags);
+ 	if (ret)
+ 		goto out_free_image;
+ 
+ 	ret = sanity_check_segment_list(image);
+ 	if (ret)
+ 		goto out_free_post_load_bufs;
+ 
+ 	ret = -ENOMEM;
+ 	image->control_code_page = kimage_alloc_control_pages(image,
+ 					   get_order(KEXEC_CONTROL_PAGE_SIZE));
+ 	if (!image->control_code_page) {
+ 		pr_err("Could not allocate control_code_buffer\n");
+ 		goto out_free_post_load_bufs;
+ 	}
+ 
+ 	image->swap_page = kimage_alloc_control_pages(image, 0);
+ 	if (!image->swap_page) {
+ 		pr_err(KERN_ERR "Could not allocate swap buffer\n");
+ 		goto out_free_control_pages;
+ 	}
+ 
+ 	*rimage = image;
+ 	return 0;
+ out_free_control_pages:
+ 	kimage_free_page_list(&image->control_pages);
+ out_free_post_load_bufs:
+ 	kimage_file_post_load_cleanup(image);
+ out_free_image:
+ 	kfree(image);
+ 	return ret;
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
  }
  
  static int kimage_is_destination_range(struct kimage *image,
@@@ -664,6 -906,14 +858,17 @@@ static void kimage_free(struct kimage *
  
  	/* Free the kexec control pages... */
  	kimage_free_page_list(&image->control_pages);
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Free up any temporary buffers allocated. This might hit if
+ 	 * error occurred much later after buffer allocation.
+ 	 */
+ 	if (image->file_mode)
+ 		kimage_file_post_load_cleanup(image);
+ 
++>>>>>>> 27f48d3e633b (kexec-bzImage64: support for loading bzImage using 64bit entry)
  	kfree(image);
  }
  
diff --git a/arch/x86/include/asm/kexec-bzimage64.h b/arch/x86/include/asm/kexec-bzimage64.h
new file mode 100644
index 000000000000..d1b5d194e31d
--- /dev/null
+++ b/arch/x86/include/asm/kexec-bzimage64.h
@@ -0,0 +1,6 @@
+#ifndef _ASM_KEXEC_BZIMAGE64_H
+#define _ASM_KEXEC_BZIMAGE64_H
+
+extern struct kexec_file_ops kexec_bzImage64_ops;
+
+#endif  /* _ASM_KEXE_BZIMAGE64_H */
* Unmerged path arch/x86/include/asm/kexec.h
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 977103e1fd3d..f516fa197aae 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -118,4 +118,5 @@ ifeq ($(CONFIG_X86_64),y)
 
 	obj-$(CONFIG_PCI_MMCONFIG)	+= mmconf-fam10h_64.o
 	obj-y				+= vsmp_64.o
+	obj-$(CONFIG_KEXEC)		+= kexec-bzimage64.o
 endif
diff --git a/arch/x86/kernel/kexec-bzimage64.c b/arch/x86/kernel/kexec-bzimage64.c
new file mode 100644
index 000000000000..bcedd100192f
--- /dev/null
+++ b/arch/x86/kernel/kexec-bzimage64.c
@@ -0,0 +1,375 @@
+/*
+ * Kexec bzImage loader
+ *
+ * Copyright (C) 2014 Red Hat Inc.
+ * Authors:
+ *      Vivek Goyal <vgoyal@redhat.com>
+ *
+ * This source code is licensed under the GNU General Public License,
+ * Version 2.  See the file COPYING for more details.
+ */
+
+#define pr_fmt(fmt)	"kexec-bzImage64: " fmt
+
+#include <linux/string.h>
+#include <linux/printk.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/kexec.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+
+#include <asm/bootparam.h>
+#include <asm/setup.h>
+
+/*
+ * Defines lowest physical address for various segments. Not sure where
+ * exactly these limits came from. Current bzimage64 loader in kexec-tools
+ * uses these so I am retaining it. It can be changed over time as we gain
+ * more insight.
+ */
+#define MIN_PURGATORY_ADDR	0x3000
+#define MIN_BOOTPARAM_ADDR	0x3000
+#define MIN_KERNEL_LOAD_ADDR	0x100000
+#define MIN_INITRD_LOAD_ADDR	0x1000000
+
+/*
+ * This is a place holder for all boot loader specific data structure which
+ * gets allocated in one call but gets freed much later during cleanup
+ * time. Right now there is only one field but it can grow as need be.
+ */
+struct bzimage64_data {
+	/*
+	 * Temporary buffer to hold bootparams buffer. This should be
+	 * freed once the bootparam segment has been loaded.
+	 */
+	void *bootparams_buf;
+};
+
+static int setup_initrd(struct boot_params *params,
+		unsigned long initrd_load_addr, unsigned long initrd_len)
+{
+	params->hdr.ramdisk_image = initrd_load_addr & 0xffffffffUL;
+	params->hdr.ramdisk_size = initrd_len & 0xffffffffUL;
+
+	params->ext_ramdisk_image = initrd_load_addr >> 32;
+	params->ext_ramdisk_size = initrd_len >> 32;
+
+	return 0;
+}
+
+static int setup_cmdline(struct boot_params *params,
+			 unsigned long bootparams_load_addr,
+			 unsigned long cmdline_offset, char *cmdline,
+			 unsigned long cmdline_len)
+{
+	char *cmdline_ptr = ((char *)params) + cmdline_offset;
+	unsigned long cmdline_ptr_phys;
+	uint32_t cmdline_low_32, cmdline_ext_32;
+
+	memcpy(cmdline_ptr, cmdline, cmdline_len);
+	cmdline_ptr[cmdline_len - 1] = '\0';
+
+	cmdline_ptr_phys = bootparams_load_addr + cmdline_offset;
+	cmdline_low_32 = cmdline_ptr_phys & 0xffffffffUL;
+	cmdline_ext_32 = cmdline_ptr_phys >> 32;
+
+	params->hdr.cmd_line_ptr = cmdline_low_32;
+	if (cmdline_ext_32)
+		params->ext_cmd_line_ptr = cmdline_ext_32;
+
+	return 0;
+}
+
+static int setup_memory_map_entries(struct boot_params *params)
+{
+	unsigned int nr_e820_entries;
+
+	nr_e820_entries = e820_saved.nr_map;
+
+	/* TODO: Pass entries more than E820MAX in bootparams setup data */
+	if (nr_e820_entries > E820MAX)
+		nr_e820_entries = E820MAX;
+
+	params->e820_entries = nr_e820_entries;
+	memcpy(&params->e820_map, &e820_saved.map,
+	       nr_e820_entries * sizeof(struct e820entry));
+
+	return 0;
+}
+
+static int setup_boot_parameters(struct boot_params *params)
+{
+	unsigned int nr_e820_entries;
+	unsigned long long mem_k, start, end;
+	int i;
+
+	/* Get subarch from existing bootparams */
+	params->hdr.hardware_subarch = boot_params.hdr.hardware_subarch;
+
+	/* Copying screen_info will do? */
+	memcpy(&params->screen_info, &boot_params.screen_info,
+				sizeof(struct screen_info));
+
+	/* Fill in memsize later */
+	params->screen_info.ext_mem_k = 0;
+	params->alt_mem_k = 0;
+
+	/* Default APM info */
+	memset(&params->apm_bios_info, 0, sizeof(params->apm_bios_info));
+
+	/* Default drive info */
+	memset(&params->hd0_info, 0, sizeof(params->hd0_info));
+	memset(&params->hd1_info, 0, sizeof(params->hd1_info));
+
+	/* Default sysdesc table */
+	params->sys_desc_table.length = 0;
+
+	setup_memory_map_entries(params);
+	nr_e820_entries = params->e820_entries;
+
+	for (i = 0; i < nr_e820_entries; i++) {
+		if (params->e820_map[i].type != E820_RAM)
+			continue;
+		start = params->e820_map[i].addr;
+		end = params->e820_map[i].addr + params->e820_map[i].size - 1;
+
+		if ((start <= 0x100000) && end > 0x100000) {
+			mem_k = (end >> 10) - (0x100000 >> 10);
+			params->screen_info.ext_mem_k = mem_k;
+			params->alt_mem_k = mem_k;
+			if (mem_k > 0xfc00)
+				params->screen_info.ext_mem_k = 0xfc00; /* 64M*/
+			if (mem_k > 0xffffffff)
+				params->alt_mem_k = 0xffffffff;
+		}
+	}
+
+	/* Setup EDD info */
+	memcpy(params->eddbuf, boot_params.eddbuf,
+				EDDMAXNR * sizeof(struct edd_info));
+	params->eddbuf_entries = boot_params.eddbuf_entries;
+
+	memcpy(params->edd_mbr_sig_buffer, boot_params.edd_mbr_sig_buffer,
+	       EDD_MBR_SIG_MAX * sizeof(unsigned int));
+
+	return 0;
+}
+
+int bzImage64_probe(const char *buf, unsigned long len)
+{
+	int ret = -ENOEXEC;
+	struct setup_header *header;
+
+	/* kernel should be atleast two sectors long */
+	if (len < 2 * 512) {
+		pr_err("File is too short to be a bzImage\n");
+		return ret;
+	}
+
+	header = (struct setup_header *)(buf + offsetof(struct boot_params, hdr));
+	if (memcmp((char *)&header->header, "HdrS", 4) != 0) {
+		pr_err("Not a bzImage\n");
+		return ret;
+	}
+
+	if (header->boot_flag != 0xAA55) {
+		pr_err("No x86 boot sector present\n");
+		return ret;
+	}
+
+	if (header->version < 0x020C) {
+		pr_err("Must be at least protocol version 2.12\n");
+		return ret;
+	}
+
+	if (!(header->loadflags & LOADED_HIGH)) {
+		pr_err("zImage not a bzImage\n");
+		return ret;
+	}
+
+	if (!(header->xloadflags & XLF_KERNEL_64)) {
+		pr_err("Not a bzImage64. XLF_KERNEL_64 is not set.\n");
+		return ret;
+	}
+
+	if (!(header->xloadflags & XLF_CAN_BE_LOADED_ABOVE_4G)) {
+		pr_err("XLF_CAN_BE_LOADED_ABOVE_4G is not set.\n");
+		return ret;
+	}
+
+	/* I've got a bzImage */
+	pr_debug("It's a relocatable bzImage64\n");
+	ret = 0;
+
+	return ret;
+}
+
+void *bzImage64_load(struct kimage *image, char *kernel,
+		     unsigned long kernel_len, char *initrd,
+		     unsigned long initrd_len, char *cmdline,
+		     unsigned long cmdline_len)
+{
+
+	struct setup_header *header;
+	int setup_sects, kern16_size, ret = 0;
+	unsigned long setup_header_size, params_cmdline_sz;
+	struct boot_params *params;
+	unsigned long bootparam_load_addr, kernel_load_addr, initrd_load_addr;
+	unsigned long purgatory_load_addr;
+	unsigned long kernel_bufsz, kernel_memsz, kernel_align;
+	char *kernel_buf;
+	struct bzimage64_data *ldata;
+	struct kexec_entry64_regs regs64;
+	void *stack;
+	unsigned int setup_hdr_offset = offsetof(struct boot_params, hdr);
+
+	header = (struct setup_header *)(kernel + setup_hdr_offset);
+	setup_sects = header->setup_sects;
+	if (setup_sects == 0)
+		setup_sects = 4;
+
+	kern16_size = (setup_sects + 1) * 512;
+	if (kernel_len < kern16_size) {
+		pr_err("bzImage truncated\n");
+		return ERR_PTR(-ENOEXEC);
+	}
+
+	if (cmdline_len > header->cmdline_size) {
+		pr_err("Kernel command line too long\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	/*
+	 * Load purgatory. For 64bit entry point, purgatory  code can be
+	 * anywhere.
+	 */
+	ret = kexec_load_purgatory(image, MIN_PURGATORY_ADDR, ULONG_MAX, 1,
+				   &purgatory_load_addr);
+	if (ret) {
+		pr_err("Loading purgatory failed\n");
+		return ERR_PTR(ret);
+	}
+
+	pr_debug("Loaded purgatory at 0x%lx\n", purgatory_load_addr);
+
+	/* Load Bootparams and cmdline */
+	params_cmdline_sz = sizeof(struct boot_params) + cmdline_len;
+	params = kzalloc(params_cmdline_sz, GFP_KERNEL);
+	if (!params)
+		return ERR_PTR(-ENOMEM);
+
+	/* Copy setup header onto bootparams. Documentation/x86/boot.txt */
+	setup_header_size = 0x0202 + kernel[0x0201] - setup_hdr_offset;
+
+	/* Is there a limit on setup header size? */
+	memcpy(&params->hdr, (kernel + setup_hdr_offset), setup_header_size);
+
+	ret = kexec_add_buffer(image, (char *)params, params_cmdline_sz,
+			       params_cmdline_sz, 16, MIN_BOOTPARAM_ADDR,
+			       ULONG_MAX, 1, &bootparam_load_addr);
+	if (ret)
+		goto out_free_params;
+	pr_debug("Loaded boot_param and command line at 0x%lx bufsz=0x%lx memsz=0x%lx\n",
+		 bootparam_load_addr, params_cmdline_sz, params_cmdline_sz);
+
+	/* Load kernel */
+	kernel_buf = kernel + kern16_size;
+	kernel_bufsz =  kernel_len - kern16_size;
+	kernel_memsz = PAGE_ALIGN(header->init_size);
+	kernel_align = header->kernel_alignment;
+
+	ret = kexec_add_buffer(image, kernel_buf,
+			       kernel_bufsz, kernel_memsz, kernel_align,
+			       MIN_KERNEL_LOAD_ADDR, ULONG_MAX, 1,
+			       &kernel_load_addr);
+	if (ret)
+		goto out_free_params;
+
+	pr_debug("Loaded 64bit kernel at 0x%lx bufsz=0x%lx memsz=0x%lx\n",
+		 kernel_load_addr, kernel_memsz, kernel_memsz);
+
+	/* Load initrd high */
+	if (initrd) {
+		ret = kexec_add_buffer(image, initrd, initrd_len, initrd_len,
+				       PAGE_SIZE, MIN_INITRD_LOAD_ADDR,
+				       ULONG_MAX, 1, &initrd_load_addr);
+		if (ret)
+			goto out_free_params;
+
+		pr_debug("Loaded initrd at 0x%lx bufsz=0x%lx memsz=0x%lx\n",
+				initrd_load_addr, initrd_len, initrd_len);
+
+		setup_initrd(params, initrd_load_addr, initrd_len);
+	}
+
+	setup_cmdline(params, bootparam_load_addr, sizeof(struct boot_params),
+		      cmdline, cmdline_len);
+
+	/* bootloader info. Do we need a separate ID for kexec kernel loader? */
+	params->hdr.type_of_loader = 0x0D << 4;
+	params->hdr.loadflags = 0;
+
+	/* Setup purgatory regs for entry */
+	ret = kexec_purgatory_get_set_symbol(image, "entry64_regs", &regs64,
+					     sizeof(regs64), 1);
+	if (ret)
+		goto out_free_params;
+
+	regs64.rbx = 0; /* Bootstrap Processor */
+	regs64.rsi = bootparam_load_addr;
+	regs64.rip = kernel_load_addr + 0x200;
+	stack = kexec_purgatory_get_symbol_addr(image, "stack_end");
+	if (IS_ERR(stack)) {
+		pr_err("Could not find address of symbol stack_end\n");
+		ret = -EINVAL;
+		goto out_free_params;
+	}
+
+	regs64.rsp = (unsigned long)stack;
+	ret = kexec_purgatory_get_set_symbol(image, "entry64_regs", &regs64,
+					     sizeof(regs64), 0);
+	if (ret)
+		goto out_free_params;
+
+	setup_boot_parameters(params);
+
+	/* Allocate loader specific data */
+	ldata = kzalloc(sizeof(struct bzimage64_data), GFP_KERNEL);
+	if (!ldata) {
+		ret = -ENOMEM;
+		goto out_free_params;
+	}
+
+	/*
+	 * Store pointer to params so that it could be freed after loading
+	 * params segment has been loaded and contents have been copied
+	 * somewhere else.
+	 */
+	ldata->bootparams_buf = params;
+	return ldata;
+
+out_free_params:
+	kfree(params);
+	return ERR_PTR(ret);
+}
+
+/* This cleanup function is called after various segments have been loaded */
+int bzImage64_cleanup(void *loader_data)
+{
+	struct bzimage64_data *ldata = loader_data;
+
+	if (!ldata)
+		return 0;
+
+	kfree(ldata->bootparams_buf);
+	ldata->bootparams_buf = NULL;
+
+	return 0;
+}
+
+struct kexec_file_ops kexec_bzImage64_ops = {
+	.probe = bzImage64_probe,
+	.load = bzImage64_load,
+	.cleanup = bzImage64_cleanup,
+};
* Unmerged path arch/x86/kernel/machine_kexec_64.c
* Unmerged path include/linux/kexec.h
* Unmerged path kernel/kexec.c
