ext4: move i_size,i_disksize update routines to helper function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [fs] ext4: move i_size, i_disksize update routines to helper function (Lukas Czerner) [1150171]
Rebuild_FUZZ: 99.21%
commit-author Dmitry Monakhov <dmonakhov@openvz.org>
commit 4631dbf677ded0419fee35ca7408285dabfaef1a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/4631dbf6.failed

	Cc: stable@vger.kernel.org # needed for bug fix patches
	Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>
(cherry picked from commit 4631dbf677ded0419fee35ca7408285dabfaef1a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/extents.c
diff --cc fs/ext4/extents.c
index eeefa2fc5389,f0e6934291dd..000000000000
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@@ -4657,20 -4719,233 +4657,247 @@@ retry
  		goto retry;
  	}
  
++<<<<<<< HEAD
++=======
+ 	return ret > 0 ? ret2 : ret;
+ }
+ 
+ static long ext4_zero_range(struct file *file, loff_t offset,
+ 			    loff_t len, int mode)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle = NULL;
+ 	unsigned int max_blocks;
+ 	loff_t new_size = 0;
+ 	int ret = 0;
+ 	int flags;
+ 	int partial;
+ 	loff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	struct address_space *mapping = inode->i_mapping;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	trace_ext4_zero_range(inode, offset, len, mode);
+ 
+ 	if (!S_ISREG(inode->i_mode))
+ 		return -EINVAL;
+ 
+ 	/* Call ext4_force_commit to flush all data in case of data=journal. */
+ 	if (ext4_should_journal_data(inode)) {
+ 		ret = ext4_force_commit(inode->i_sb);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/*
+ 	 * Write out all dirty pages to avoid race conditions
+ 	 * Then release them.
+ 	 */
+ 	if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
+ 		ret = filemap_write_and_wait_range(mapping, offset,
+ 						   offset + len - 1);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/*
+ 	 * Round up offset. This is not fallocate, we neet to zero out
+ 	 * blocks, so convert interior block aligned part of the range to
+ 	 * unwritten and possibly manually zero out unaligned parts of the
+ 	 * range.
+ 	 */
+ 	start = round_up(offset, 1 << blkbits);
+ 	end = round_down((offset + len), 1 << blkbits);
+ 
+ 	if (start < offset || end > offset + len)
+ 		return -EINVAL;
+ 	partial = (offset + len) & ((1 << blkbits) - 1);
+ 
+ 	lblk = start >> blkbits;
+ 	max_blocks = (end >> blkbits);
+ 	if (max_blocks < lblk)
+ 		max_blocks = 0;
+ 	else
+ 		max_blocks -= lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT |
+ 		EXT4_GET_BLOCKS_CONVERT_UNWRITTEN;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	/*
+ 	 * Indirect files do not support unwritten extnets
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {
+ 		ret = -EOPNOTSUPP;
+ 		goto out_mutex;
+ 	}
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out_mutex;
+ 		/*
+ 		 * If we have a partial block after EOF we have to allocate
+ 		 * the entire block.
+ 		 */
+ 		if (partial)
+ 			max_blocks += 1;
+ 	}
+ 
+ 	if (max_blocks > 0) {
+ 
+ 		/* Now release the pages and zero block aligned part of pages*/
+ 		truncate_pagecache_range(inode, start, end - 1);
+ 
+ 		/* Wait all existing dio workers, newcomers will block on i_mutex */
+ 		ext4_inode_block_unlocked_dio(inode);
+ 		inode_dio_wait(inode);
+ 
+ 		/*
+ 		 * Remove entire range from the extent status tree.
+ 		 */
+ 		ret = ext4_es_remove_extent(inode, lblk, max_blocks);
+ 		if (ret)
+ 			goto out_dio;
+ 
+ 		ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags,
+ 					     mode);
+ 		if (ret)
+ 			goto out_dio;
+ 	}
+ 
+ 	handle = ext4_journal_start(inode, EXT4_HT_MISC, 4);
+ 	if (IS_ERR(handle)) {
+ 		ret = PTR_ERR(handle);
+ 		ext4_std_error(inode->i_sb, ret);
+ 		goto out_dio;
+ 	}
+ 
+ 	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
+ 	if (new_size) {
+ 		ext4_update_inode_size(inode, new_size);
+ 	} else {
+ 		/*
+ 		* Mark that we allocate beyond EOF so the subsequent truncate
+ 		* can proceed even if the new size is the same as i_size.
+ 		*/
+ 		if ((offset + len) > i_size_read(inode))
+ 			ext4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);
+ 	}
+ 
+ 	ext4_mark_inode_dirty(handle, inode);
+ 
+ 	/* Zero out partial block at the edges of the range */
+ 	ret = ext4_zero_partial_blocks(handle, inode, offset, len);
+ 
+ 	if (file->f_flags & O_SYNC)
+ 		ext4_handle_sync(handle);
+ 
+ 	ext4_journal_stop(handle);
+ out_dio:
+ 	ext4_inode_resume_unlocked_dio(inode);
+ out_mutex:
+ 	mutex_unlock(&inode->i_mutex);
+ 	return ret;
+ }
+ 
+ /*
+  * preallocate space for a file. This implements ext4's fallocate file
+  * operation, which gets called from sys_fallocate system call.
+  * For block-mapped files, posix_fallocate should fall back to the method
+  * of writing zeroes to the required new blocks (the same behavior which is
+  * expected for file systems which do not support fallocate() system call).
+  */
+ long ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle;
+ 	loff_t new_size = 0;
+ 	unsigned int max_blocks;
+ 	int ret = 0;
+ 	int flags;
+ 	ext4_lblk_t lblk;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	/* Return error if mode is not supported */
+ 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+ 		     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_PUNCH_HOLE)
+ 		return ext4_punch_hole(inode, offset, len);
+ 
+ 	ret = ext4_convert_inline_data(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * currently supporting (pre)allocate mode for extent-based
+ 	 * files _only_
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_COLLAPSE_RANGE)
+ 		return ext4_collapse_range(inode, offset, len);
+ 
+ 	if (mode & FALLOC_FL_ZERO_RANGE)
+ 		return ext4_zero_range(file, offset, len, mode);
+ 
+ 	trace_ext4_fallocate_enter(inode, offset, len, mode);
+ 	lblk = offset >> blkbits;
+ 	/*
+ 	 * We can't just convert len to max_blocks because
+ 	 * If blocksize = 4096 offset = 3072 and len = 2048
+ 	 */
+ 	max_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)
+ 		- lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags, mode);
+ 	if (ret)
+ 		goto out;
+ 
++>>>>>>> 4631dbf677de (ext4: move i_size,i_disksize update routines to helper function)
  	handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
  	if (IS_ERR(handle))
  		goto out;
  
- 	tv = inode->i_ctime = ext4_current_time(inode);
+ 	inode->i_ctime = ext4_current_time(inode);
  
++<<<<<<< HEAD
 +	if (ret > 0 && new_size) {
 +		if (new_size > i_size_read(inode)) {
 +			i_size_write(inode, new_size);
 +			inode->i_mtime = tv;
 +		}
 +		if (new_size > EXT4_I(inode)->i_disksize)
 +			ext4_update_i_disksize(inode, new_size);
 +	} else if (ret > 0 && !new_size) {
++=======
+ 	if (new_size) {
+ 		if (ext4_update_inode_size(inode, new_size) & 0x1)
+ 			inode->i_mtime = inode->i_ctime;
+ 	} else {
++>>>>>>> 4631dbf677de (ext4: move i_size,i_disksize update routines to helper function)
  		/*
  		* Mark that we allocate beyond EOF so the subsequent truncate
  		* can proceed even if the new size is the same as i_size.
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 87e15359709b..f04c9342be96 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -2481,6 +2481,22 @@ static inline void ext4_update_i_disksize(struct inode *inode, loff_t newsize)
 	return ;
 }
 
+/* Update i_size, i_disksize. Requires i_mutex to avoid races with truncate */
+static inline int ext4_update_inode_size(struct inode *inode, loff_t newsize)
+{
+	int changed = 0;
+
+	if (newsize > inode->i_size) {
+		i_size_write(inode, newsize);
+		changed = 1;
+	}
+	if (newsize > EXT4_I(inode)->i_disksize) {
+		ext4_update_i_disksize(inode, newsize);
+		changed |= 2;
+	}
+	return changed;
+}
+
 struct ext4_group_info {
 	unsigned long   bb_state;
 	struct rb_root  bb_free_root;
* Unmerged path fs/ext4/extents.c
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 935ffef6262d..4a1087cc0e3f 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1076,27 +1076,11 @@ static int ext4_write_end(struct file *file,
 	} else
 		copied = block_write_end(file, mapping, pos,
 					 len, copied, page, fsdata);
-
 	/*
-	 * No need to use i_size_read() here, the i_size
-	 * cannot change under us because we hole i_mutex.
-	 *
-	 * But it's important to update i_size while still holding page lock:
+	 * it's important to update i_size while still holding page lock:
 	 * page writeout could otherwise come in and zero beyond i_size.
 	 */
-	if (pos + copied > inode->i_size) {
-		i_size_write(inode, pos + copied);
-		i_size_changed = 1;
-	}
-
-	if (pos + copied > EXT4_I(inode)->i_disksize) {
-		/* We need to mark inode dirty even if
-		 * new_i_size is less that inode->i_size
-		 * but greater than i_disksize. (hint delalloc)
-		 */
-		ext4_update_i_disksize(inode, (pos + copied));
-		i_size_changed = 1;
-	}
+	i_size_changed = ext4_update_inode_size(inode, pos + copied);
 	unlock_page(page);
 	page_cache_release(page);
 
@@ -1144,7 +1128,7 @@ static int ext4_journalled_write_end(struct file *file,
 	int ret = 0, ret2;
 	int partial = 0;
 	unsigned from, to;
-	loff_t new_i_size;
+	int size_changed = 0;
 
 	trace_ext4_journalled_write_end(inode, pos, len, copied);
 	from = pos & (PAGE_CACHE_SIZE - 1);
@@ -1167,20 +1151,18 @@ static int ext4_journalled_write_end(struct file *file,
 		if (!partial)
 			SetPageUptodate(page);
 	}
-	new_i_size = pos + copied;
-	if (new_i_size > inode->i_size)
-		i_size_write(inode, pos+copied);
+	size_changed = ext4_update_inode_size(inode, pos + copied);
 	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
 	EXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;
-	if (new_i_size > EXT4_I(inode)->i_disksize) {
-		ext4_update_i_disksize(inode, new_i_size);
+	unlock_page(page);
+	page_cache_release(page);
+
+	if (size_changed) {
 		ret2 = ext4_mark_inode_dirty(handle, inode);
 		if (!ret)
 			ret = ret2;
 	}
 
-	unlock_page(page);
-	page_cache_release(page);
 	if (pos + len > inode->i_size && ext4_can_truncate(inode))
 		/* if we have allocated more blocks and copied
 		 * less. We will have blocks allocated outside
