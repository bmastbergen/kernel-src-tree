xprtrdma: Update rkeys after transport reconnect

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 6ab59945f292a5c6cbc4a6c2011f1a732a116af2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/6ab59945.failed

Various reports of:

  rpcrdma_qp_async_error_upcall: QP error 3 on device mlx4_0
		ep ffff8800bfd3e848

Ensure that rkeys in already-marshalled RPC/RDMA headers are
refreshed after the QP has been replaced by a reconnect.

BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=249
	Suggested-by: Selvin Xavier <Selvin.Xavier@Emulex.Com>
	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Tested-by: Shirley Ma <shirley.ma@oracle.com>
	Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 6ab59945f292a5c6cbc4a6c2011f1a732a116af2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/rpc_rdma.c
#	net/sunrpc/xprtrdma/transport.c
diff --cc net/sunrpc/xprtrdma/rpc_rdma.c
index d8cf393c8eb6,54422f73b03b..000000000000
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@@ -278,9 -274,31 +270,31 @@@ out
  	for (pos = 0; nchunks--;)
  		pos += rpcrdma_deregister_external(
  				&req->rl_segments[pos], r_xprt);
 -	return n;
 +	return 0;
  }
  
+ /*
+  * Marshal chunks. This routine returns the header length
+  * consumed by marshaling.
+  *
+  * Returns positive RPC/RDMA header size, or negative errno.
+  */
+ 
+ ssize_t
+ rpcrdma_marshal_chunks(struct rpc_rqst *rqst, ssize_t result)
+ {
+ 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
+ 	struct rpcrdma_msg *headerp = (struct rpcrdma_msg *)req->rl_base;
+ 
+ 	if (req->rl_rtype != rpcrdma_noch)
+ 		result = rpcrdma_create_chunks(rqst, &rqst->rq_snd_buf,
+ 					       headerp, req->rl_rtype);
+ 	else if (req->rl_wtype != rpcrdma_noch)
+ 		result = rpcrdma_create_chunks(rqst, &rqst->rq_rcv_buf,
+ 					       headerp, req->rl_wtype);
+ 	return result;
+ }
+ 
  /*
   * Copy write data inline.
   * This function is used for "small" requests. Data which is passed
@@@ -369,8 -389,8 +383,13 @@@ rpcrdma_marshal_req(struct rpc_rqst *rq
  	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
  	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
  	char *base;
++<<<<<<< HEAD
 +	size_t hdrlen, rpclen, padlen;
 +	enum rpcrdma_chunktype rtype, wtype;
++=======
+ 	size_t rpclen, padlen;
+ 	ssize_t hdrlen;
++>>>>>>> 6ab59945f292 (xprtrdma: Update rkeys after transport reconnect)
  	struct rpcrdma_msg *headerp;
  
  	/*
@@@ -431,16 -451,20 +450,26 @@@
  	 * TBD check NFSv4 setacl
  	 */
  	if (rqst->rq_snd_buf.len <= RPCRDMA_INLINE_WRITE_THRESHOLD(rqst))
- 		rtype = rpcrdma_noch;
+ 		req->rl_rtype = rpcrdma_noch;
  	else if (rqst->rq_snd_buf.page_len == 0)
- 		rtype = rpcrdma_areadch;
+ 		req->rl_rtype = rpcrdma_areadch;
  	else
- 		rtype = rpcrdma_readch;
+ 		req->rl_rtype = rpcrdma_readch;
  
  	/* The following simplification is not true forever */
++<<<<<<< HEAD
 +	if (rtype != rpcrdma_noch && wtype == rpcrdma_replych)
 +		wtype = rpcrdma_noch;
 +	BUG_ON(rtype != rpcrdma_noch && wtype != rpcrdma_noch);
++=======
+ 	if (req->rl_rtype != rpcrdma_noch && req->rl_wtype == rpcrdma_replych)
+ 		req->rl_wtype = rpcrdma_noch;
+ 	if (req->rl_rtype != rpcrdma_noch && req->rl_wtype != rpcrdma_noch) {
+ 		dprintk("RPC:       %s: cannot marshal multiple chunk lists\n",
+ 			__func__);
+ 		return -EIO;
+ 	}
++>>>>>>> 6ab59945f292 (xprtrdma: Update rkeys after transport reconnect)
  
  	hdrlen = 28; /*sizeof *headerp;*/
  	padlen = 0;
@@@ -465,8 -489,11 +494,16 @@@
  			headerp->rm_body.rm_padded.rm_pempty[1] = xdr_zero;
  			headerp->rm_body.rm_padded.rm_pempty[2] = xdr_zero;
  			hdrlen += 2 * sizeof(u32); /* extra words in padhdr */
++<<<<<<< HEAD
 +			BUG_ON(wtype != rpcrdma_noch);
 +
++=======
+ 			if (req->rl_wtype != rpcrdma_noch) {
+ 				dprintk("RPC:       %s: invalid chunk list\n",
+ 					__func__);
+ 				return -EIO;
+ 			}
++>>>>>>> 6ab59945f292 (xprtrdma: Update rkeys after transport reconnect)
  		} else {
  			headerp->rm_body.rm_nochunks.rm_empty[0] = xdr_zero;
  			headerp->rm_body.rm_nochunks.rm_empty[1] = xdr_zero;
@@@ -483,28 -510,14 +520,39 @@@
  			 * on receive. Therefore, we request a reply chunk
  			 * for non-writes wherever feasible and efficient.
  			 */
++<<<<<<< HEAD
 +			if (wtype == rpcrdma_noch &&
 +			    r_xprt->rx_ia.ri_memreg_strategy > RPCRDMA_REGISTER)
 +				wtype = rpcrdma_replych;
 +		}
 +	}
 +
 +	/*
 +	 * Marshal chunks. This routine will return the header length
 +	 * consumed by marshaling.
 +	 */
 +	if (rtype != rpcrdma_noch) {
 +		hdrlen = rpcrdma_create_chunks(rqst,
 +					&rqst->rq_snd_buf, headerp, rtype);
 +		wtype = rtype;	/* simplify dprintk */
 +
 +	} else if (wtype != rpcrdma_noch) {
 +		hdrlen = rpcrdma_create_chunks(rqst,
 +					&rqst->rq_rcv_buf, headerp, wtype);
 +	}
 +
 +	if (hdrlen == 0)
 +		return -1;
++=======
+ 			if (req->rl_wtype == rpcrdma_noch)
+ 				req->rl_wtype = rpcrdma_replych;
+ 		}
+ 	}
+ 
+ 	hdrlen = rpcrdma_marshal_chunks(rqst, hdrlen);
+ 	if (hdrlen < 0)
+ 		return hdrlen;
++>>>>>>> 6ab59945f292 (xprtrdma: Update rkeys after transport reconnect)
  
  	dprintk("RPC:       %s: %s: hdrlen %zd rpclen %zd padlen %zd"
  		" headerp 0x%p base 0x%p lkey 0x%x\n",
diff --cc net/sunrpc/xprtrdma/transport.c
index 3f26245da10e,f6d280b31dc9..000000000000
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@@ -621,14 -597,14 +621,25 @@@ xprt_rdma_send_request(struct rpc_task 
  	struct rpc_xprt *xprt = rqst->rq_xprt;
  	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
  	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
++<<<<<<< HEAD
 +
 +	/* marshal the send itself */
 +	if (req->rl_niovs == 0 && rpcrdma_marshal_req(rqst) != 0) {
 +		r_xprt->rx_stats.failed_marshal_count++;
 +		dprintk("RPC:       %s: rpcrdma_marshal_req failed\n",
 +			__func__);
 +		return -EIO;
 +	}
++=======
+ 	int rc = 0;
+ 
+ 	if (req->rl_niovs == 0)
+ 		rc = rpcrdma_marshal_req(rqst);
+ 	else if (r_xprt->rx_ia.ri_memreg_strategy == RPCRDMA_FRMR)
+ 		rc = rpcrdma_marshal_chunks(rqst, 0);
+ 	if (rc < 0)
+ 		goto failed_marshal;
++>>>>>>> 6ab59945f292 (xprtrdma: Update rkeys after transport reconnect)
  
  	if (req->rl_reply == NULL) 		/* e.g. reconnection */
  		rpcrdma_recv_buffer_get(req);
* Unmerged path net/sunrpc/xprtrdma/rpc_rdma.c
* Unmerged path net/sunrpc/xprtrdma/transport.c
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4ef6e3f9b67c..4402173e156d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -95,6 +95,14 @@ struct rpcrdma_ep {
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 
+enum rpcrdma_chunktype {
+	rpcrdma_noch = 0,
+	rpcrdma_readch,
+	rpcrdma_areadch,
+	rpcrdma_writech,
+	rpcrdma_replych
+};
+
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv
  * and complete a reply, asychronously. It needs several pieces of
@@ -190,6 +198,7 @@ struct rpcrdma_req {
 	unsigned int	rl_niovs;	/* 0, 2 or 4 */
 	unsigned int	rl_nchunks;	/* non-zero if chunks */
 	unsigned int	rl_connect_cookie;	/* retry detection */
+	enum rpcrdma_chunktype	rl_rtype, rl_wtype;
 	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */
@@ -346,6 +355,7 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
+ssize_t rpcrdma_marshal_chunks(struct rpc_rqst *, ssize_t);
 int rpcrdma_marshal_req(struct rpc_rqst *);
 size_t rpcrdma_max_payload(struct rpcrdma_xprt *);
 
