aio: fix reqs_available handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Benjamin LaHaise <bcrl@kvack.org>
commit d856f32a86b2b015ab180ab7a55e455ed8d3ccc5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/d856f32a.failed

As reported by Dan Aloni, commit f8567a3845ac ("aio: fix aio request
leak when events are reaped by userspace") introduces a regression when
user code attempts to perform io_submit() with more events than are
available in the ring buffer.  Reverting that commit would reintroduce a
regression when user space event reaping is used.

Fixing this bug is a bit more involved than the previous attempts to fix
this regression.  Since we do not have a single point at which we can
count events as being reaped by user space and io_getevents(), we have
to track event completion by looking at the number of events left in the
event ring.  So long as there are as many events in the ring buffer as
there have been completion events generate, we cannot call
put_reqs_available().  The code to check for this is now placed in
refill_reqs_available().

A test program from Dan and modified by me for verifying this bug is available
at http://www.kvack.org/~bcrl/20140824-aio_bug.c .

	Reported-by: Dan Aloni <dan@kernelim.com>
	Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>
	Acked-by: Dan Aloni <dan@kernelim.com>
	Cc: Kent Overstreet <kmo@daterainc.com>
	Cc: Mateusz Guzik <mguzik@redhat.com>
	Cc: Petr Matousek <pmatouse@redhat.com>
	Cc: stable@vger.kernel.org      # v3.16 and anything that f8567a3845ac was backported to
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d856f32a86b2b015ab180ab7a55e455ed8d3ccc5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/aio.c
diff --cc fs/aio.c
index e90f40ffd1ab,97bc62cbe2da..000000000000
--- a/fs/aio.c
+++ b/fs/aio.c
@@@ -689,49 -785,154 +690,119 @@@ EXPORT_SYMBOL(wait_on_sync_kiocb)
   */
  void exit_aio(struct mm_struct *mm)
  {
 -	struct kioctx_table *table = rcu_dereference_raw(mm->ioctx_table);
 -	int i;
 -
 -	if (!table)
 -		return;
 -
 -	for (i = 0; i < table->nr; ++i) {
 -		struct kioctx *ctx = table->table[i];
 -
 -		if (!ctx)
 -			continue;
 +	struct kioctx *ctx;
 +	struct hlist_node *n;
 +
 +	hlist_for_each_entry_safe(ctx, n, &mm->ioctx_list, list) {
 +		if (1 != atomic_read(&ctx->users))
 +			printk(KERN_DEBUG
 +				"exit_aio:ioctx still alive: %d %d %d\n",
 +				atomic_read(&ctx->users),
 +				atomic_read(&ctx->dead),
 +				atomic_read(&ctx->reqs_active));
  		/*
 -		 * We don't need to bother with munmap() here - exit_mmap(mm)
 -		 * is coming and it'll unmap everything. And we simply can't,
 -		 * this is not necessarily our ->mm.
 -		 * Since kill_ioctx() uses non-zero ->mmap_size as indicator
 -		 * that it needs to unmap the area, just set it to 0.
 +		 * We don't need to bother with munmap() here -
 +		 * exit_mmap(mm) is coming and it'll unmap everything.
 +		 * Since aio_free_ring() uses non-zero ->mmap_size
 +		 * as indicator that it needs to unmap the area,
 +		 * just set it to 0; aio_free_ring() is the only
 +		 * place that uses ->mmap_size, so it's safe.
  		 */
  		ctx->mmap_size = 0;
 -		kill_ioctx(mm, ctx, NULL);
 -	}
 -
 -	RCU_INIT_POINTER(mm->ioctx_table, NULL);
 -	kfree(table);
 -}
 -
 -static void put_reqs_available(struct kioctx *ctx, unsigned nr)
 -{
 -	struct kioctx_cpu *kcpu;
 -	unsigned long flags;
 -
 -	local_irq_save(flags);
 -	kcpu = this_cpu_ptr(ctx->cpu);
 -	kcpu->reqs_available += nr;
  
 -	while (kcpu->reqs_available >= ctx->req_batch * 2) {
 -		kcpu->reqs_available -= ctx->req_batch;
 -		atomic_add(ctx->req_batch, &ctx->reqs_available);
 +		kill_ioctx(mm, ctx);
  	}
 -
 -	local_irq_restore(flags);
 -}
 -
 -static bool get_reqs_available(struct kioctx *ctx)
 -{
 -	struct kioctx_cpu *kcpu;
 -	bool ret = false;
 -	unsigned long flags;
 -
 -	local_irq_save(flags);
 -	kcpu = this_cpu_ptr(ctx->cpu);
 -	if (!kcpu->reqs_available) {
 -		int old, avail = atomic_read(&ctx->reqs_available);
 -
 -		do {
 -			if (avail < ctx->req_batch)
 -				goto out;
 -
 -			old = avail;
 -			avail = atomic_cmpxchg(&ctx->reqs_available,
 -					       avail, avail - ctx->req_batch);
 -		} while (avail != old);
 -
 -		kcpu->reqs_available += ctx->req_batch;
 -	}
 -
 -	ret = true;
 -	kcpu->reqs_available--;
 -out:
 -	local_irq_restore(flags);
 -	return ret;
  }
  
+ /* refill_reqs_available
+  *	Updates the reqs_available reference counts used for tracking the
+  *	number of free slots in the completion ring.  This can be called
+  *	from aio_complete() (to optimistically update reqs_available) or
+  *	from aio_get_req() (the we're out of events case).  It must be
+  *	called holding ctx->completion_lock.
+  */
+ static void refill_reqs_available(struct kioctx *ctx, unsigned head,
+                                   unsigned tail)
+ {
+ 	unsigned events_in_ring, completed;
+ 
+ 	/* Clamp head since userland can write to it. */
+ 	head %= ctx->nr_events;
+ 	if (head <= tail)
+ 		events_in_ring = tail - head;
+ 	else
+ 		events_in_ring = ctx->nr_events - (head - tail);
+ 
+ 	completed = ctx->completed_events;
+ 	if (events_in_ring < completed)
+ 		completed -= events_in_ring;
+ 	else
+ 		completed = 0;
+ 
+ 	if (!completed)
+ 		return;
+ 
+ 	ctx->completed_events -= completed;
+ 	put_reqs_available(ctx, completed);
+ }
+ 
+ /* user_refill_reqs_available
+  *	Called to refill reqs_available when aio_get_req() encounters an
+  *	out of space in the completion ring.
+  */
+ static void user_refill_reqs_available(struct kioctx *ctx)
+ {
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (ctx->completed_events) {
+ 		struct aio_ring *ring;
+ 		unsigned head;
+ 
+ 		/* Access of ring->head may race with aio_read_events_ring()
+ 		 * here, but that's okay since whether we read the old version
+ 		 * or the new version, and either will be valid.  The important
+ 		 * part is that head cannot pass tail since we prevent
+ 		 * aio_complete() from updating tail by holding
+ 		 * ctx->completion_lock.  Even if head is invalid, the check
+ 		 * against ctx->completed_events below will make sure we do the
+ 		 * safe/right thing.
+ 		 */
+ 		ring = kmap_atomic(ctx->ring_pages[0]);
+ 		head = ring->head;
+ 		kunmap_atomic(ring);
+ 
+ 		refill_reqs_available(ctx, head, ctx->tail);
+ 	}
+ 
+ 	spin_unlock_irq(&ctx->completion_lock);
+ }
+ 
  /* aio_get_req
 - *	Allocate a slot for an aio request.
 - * Returns NULL if no requests are free.
 + *	Allocate a slot for an aio request.  Increments the ki_users count
 + * of the kioctx so that the kioctx stays around until all requests are
 + * complete.  Returns NULL if no requests are free.
 + *
 + * Returns with kiocb->ki_users set to 2.  The io submit code path holds
 + * an extra reference while submitting the i/o.
 + * This prevents races between the aio code path referencing the
 + * req (after submitting it) and aio_complete() freeing the req.
   */
  static inline struct kiocb *aio_get_req(struct kioctx *ctx)
  {
  	struct kiocb *req;
  
++<<<<<<< HEAD
 +	if (atomic_read(&ctx->reqs_active) >= ctx->nr_events)
 +		return NULL;
++=======
+ 	if (!get_reqs_available(ctx)) {
+ 		user_refill_reqs_available(ctx);
+ 		if (!get_reqs_available(ctx))
+ 			return NULL;
+ 	}
++>>>>>>> d856f32a86b2 (aio: fix reqs_available handling)
 +
 +	if (atomic_inc_return(&ctx->reqs_active) > ctx->nr_events - 1)
 +		goto out_put;
  
  	req = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL|__GFP_ZERO);
  	if (unlikely(!req))
@@@ -888,9 -1073,8 +963,13 @@@ void aio_complete(struct kiocb *iocb, l
  	if (iocb->ki_eventfd != NULL)
  		eventfd_signal(iocb->ki_eventfd, 1);
  
 +put_rq:
  	/* everything turned out well, dispose of the aiocb. */
++<<<<<<< HEAD
 +	aio_put_req(iocb);
++=======
+ 	kiocb_free(iocb);
++>>>>>>> d856f32a86b2 (aio: fix reqs_available handling)
  
  	/*
  	 * We have to order our ring_info tail store above and test
* Unmerged path fs/aio.c
