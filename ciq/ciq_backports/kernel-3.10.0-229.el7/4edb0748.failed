vmstat: create fold_diff

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Christoph Lameter <cl@linux.com>
commit 4edb0748b23887140578d68f5f4e6e2de337a481
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/4edb0748.failed

Both functions that update global counters use the same mechanism.

Create a function that contains the common code.

	Signed-off-by: Christoph Lameter <cl@linux.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
CC: Tejun Heo <tj@kernel.org>
	Cc: Joonsoo Kim <js1304@gmail.com>
	Cc: Alexey Dobriyan <adobriyan@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4edb0748b23887140578d68f5f4e6e2de337a481)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmstat.c
diff --cc mm/vmstat.c
index 79736a1e5a9a,158ca6494bc6..000000000000
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@@ -414,12 -414,17 +414,21 @@@ void dec_zone_page_state(struct page *p
  EXPORT_SYMBOL(dec_zone_page_state);
  #endif
  
+ static inline void fold_diff(int *diff)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+ 		if (diff[i])
+ 			atomic_long_add(diff[i], &vm_stat[i]);
+ }
+ 
  /*
 - * Update the zone counters for the current cpu.
 + * Update the zone counters for one cpu.
 + *
 + * The cpu specified must be either the current cpu or a processor that
 + * is not online. If it is the current cpu then the execution thread must
 + * be pinned to the current cpu.
   *
   * Note that refresh_cpu_vm_stats strives to only access
   * node local memory. The per cpu pagesets on remote zones are placed
@@@ -487,13 -492,40 +496,43 @@@ void refresh_cpu_vm_stats(int cpu
  			drain_zone_pages(zone, &p->pcp);
  #endif
  	}
+ 	fold_diff(global_diff);
+ }
  
- 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
- 		if (global_diff[i])
- 			atomic_long_add(global_diff[i], &vm_stat[i]);
+ /*
++<<<<<<< HEAD
++=======
+  * Fold the data for an offline cpu into the global array.
+  * There cannot be any access by the offline cpu and therefore
+  * synchronization is simplified.
+  */
+ void cpu_vm_stats_fold(int cpu)
+ {
+ 	struct zone *zone;
+ 	int i;
+ 	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };
+ 
+ 	for_each_populated_zone(zone) {
+ 		struct per_cpu_pageset *p;
+ 
+ 		p = per_cpu_ptr(zone->pageset, cpu);
+ 
+ 		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+ 			if (p->vm_stat_diff[i]) {
+ 				int v;
+ 
+ 				v = p->vm_stat_diff[i];
+ 				p->vm_stat_diff[i] = 0;
+ 				atomic_long_add(v, &zone->vm_stat[i]);
+ 				global_diff[i] += v;
+ 			}
+ 	}
+ 
+ 	fold_diff(global_diff);
  }
  
  /*
++>>>>>>> 4edb0748b238 (vmstat: create fold_diff)
   * this is only called if !populated_zone(zone), which implies no other users of
   * pset->vm_stat_diff[] exsist.
   */
* Unmerged path mm/vmstat.c
