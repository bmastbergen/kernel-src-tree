aio: block exit_aio() until all context requests are completed

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Gu Zheng <guz.fnst@cn.fujitsu.com>
commit 6098b45b32e6baeacc04790773ced9340601d511
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/6098b45b.failed

It seems that exit_aio() also needs to wait for all iocbs to complete (like
io_destroy), but we missed the wait step in current implemention, so fix
it in the same way as we did in io_destroy.

	Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
	Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>
	Cc: stable@vger.kernel.org
(cherry picked from commit 6098b45b32e6baeacc04790773ced9340601d511)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/aio.c
diff --cc fs/aio.c
index e90f40ffd1ab,733750096b71..000000000000
--- a/fs/aio.c
+++ b/fs/aio.c
@@@ -689,28 -785,144 +689,48 @@@ EXPORT_SYMBOL(wait_on_sync_kiocb)
   */
  void exit_aio(struct mm_struct *mm)
  {
 -	struct kioctx_table *table = rcu_dereference_raw(mm->ioctx_table);
 -	int i;
 -
 +	struct kioctx *ctx;
 +	struct hlist_node *n;
 +
++<<<<<<< HEAD
 +	hlist_for_each_entry_safe(ctx, n, &mm->ioctx_list, list) {
 +		if (1 != atomic_read(&ctx->users))
 +			printk(KERN_DEBUG
 +				"exit_aio:ioctx still alive: %d %d %d\n",
 +				atomic_read(&ctx->users),
 +				atomic_read(&ctx->dead),
 +				atomic_read(&ctx->reqs_active));
++=======
+ 	if (!table)
+ 		return;
+ 
+ 	for (i = 0; i < table->nr; ++i) {
+ 		struct kioctx *ctx = table->table[i];
+ 		struct completion requests_done =
+ 			COMPLETION_INITIALIZER_ONSTACK(requests_done);
+ 
+ 		if (!ctx)
+ 			continue;
++>>>>>>> 6098b45b32e6 (aio: block exit_aio() until all context requests are completed)
  		/*
 -		 * We don't need to bother with munmap() here - exit_mmap(mm)
 -		 * is coming and it'll unmap everything. And we simply can't,
 -		 * this is not necessarily our ->mm.
 -		 * Since kill_ioctx() uses non-zero ->mmap_size as indicator
 -		 * that it needs to unmap the area, just set it to 0.
 +		 * We don't need to bother with munmap() here -
 +		 * exit_mmap(mm) is coming and it'll unmap everything.
 +		 * Since aio_free_ring() uses non-zero ->mmap_size
 +		 * as indicator that it needs to unmap the area,
 +		 * just set it to 0; aio_free_ring() is the only
 +		 * place that uses ->mmap_size, so it's safe.
  		 */
  		ctx->mmap_size = 0;
++<<<<<<< HEAD
 +
 +		kill_ioctx(mm, ctx);
++=======
+ 		kill_ioctx(mm, ctx, &requests_done);
+ 
+ 		/* Wait until all IO for the context are done. */
+ 		wait_for_completion(&requests_done);
++>>>>>>> 6098b45b32e6 (aio: block exit_aio() until all context requests are completed)
  	}
 -
 -	RCU_INIT_POINTER(mm->ioctx_table, NULL);
 -	kfree(table);
 -}
 -
 -static void put_reqs_available(struct kioctx *ctx, unsigned nr)
 -{
 -	struct kioctx_cpu *kcpu;
 -	unsigned long flags;
 -
 -	local_irq_save(flags);
 -	kcpu = this_cpu_ptr(ctx->cpu);
 -	kcpu->reqs_available += nr;
 -
 -	while (kcpu->reqs_available >= ctx->req_batch * 2) {
 -		kcpu->reqs_available -= ctx->req_batch;
 -		atomic_add(ctx->req_batch, &ctx->reqs_available);
 -	}
 -
 -	local_irq_restore(flags);
 -}
 -
 -static bool get_reqs_available(struct kioctx *ctx)
 -{
 -	struct kioctx_cpu *kcpu;
 -	bool ret = false;
 -	unsigned long flags;
 -
 -	local_irq_save(flags);
 -	kcpu = this_cpu_ptr(ctx->cpu);
 -	if (!kcpu->reqs_available) {
 -		int old, avail = atomic_read(&ctx->reqs_available);
 -
 -		do {
 -			if (avail < ctx->req_batch)
 -				goto out;
 -
 -			old = avail;
 -			avail = atomic_cmpxchg(&ctx->reqs_available,
 -					       avail, avail - ctx->req_batch);
 -		} while (avail != old);
 -
 -		kcpu->reqs_available += ctx->req_batch;
 -	}
 -
 -	ret = true;
 -	kcpu->reqs_available--;
 -out:
 -	local_irq_restore(flags);
 -	return ret;
 -}
 -
 -/* refill_reqs_available
 - *	Updates the reqs_available reference counts used for tracking the
 - *	number of free slots in the completion ring.  This can be called
 - *	from aio_complete() (to optimistically update reqs_available) or
 - *	from aio_get_req() (the we're out of events case).  It must be
 - *	called holding ctx->completion_lock.
 - */
 -static void refill_reqs_available(struct kioctx *ctx, unsigned head,
 -                                  unsigned tail)
 -{
 -	unsigned events_in_ring, completed;
 -
 -	/* Clamp head since userland can write to it. */
 -	head %= ctx->nr_events;
 -	if (head <= tail)
 -		events_in_ring = tail - head;
 -	else
 -		events_in_ring = ctx->nr_events - (head - tail);
 -
 -	completed = ctx->completed_events;
 -	if (events_in_ring < completed)
 -		completed -= events_in_ring;
 -	else
 -		completed = 0;
 -
 -	if (!completed)
 -		return;
 -
 -	ctx->completed_events -= completed;
 -	put_reqs_available(ctx, completed);
 -}
 -
 -/* user_refill_reqs_available
 - *	Called to refill reqs_available when aio_get_req() encounters an
 - *	out of space in the completion ring.
 - */
 -static void user_refill_reqs_available(struct kioctx *ctx)
 -{
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (ctx->completed_events) {
 -		struct aio_ring *ring;
 -		unsigned head;
 -
 -		/* Access of ring->head may race with aio_read_events_ring()
 -		 * here, but that's okay since whether we read the old version
 -		 * or the new version, and either will be valid.  The important
 -		 * part is that head cannot pass tail since we prevent
 -		 * aio_complete() from updating tail by holding
 -		 * ctx->completion_lock.  Even if head is invalid, the check
 -		 * against ctx->completed_events below will make sure we do the
 -		 * safe/right thing.
 -		 */
 -		ring = kmap_atomic(ctx->ring_pages[0]);
 -		head = ring->head;
 -		kunmap_atomic(ring);
 -
 -		refill_reqs_available(ctx, head, ctx->tail);
 -	}
 -
 -	spin_unlock_irq(&ctx->completion_lock);
  }
  
  /* aio_get_req
* Unmerged path fs/aio.c
