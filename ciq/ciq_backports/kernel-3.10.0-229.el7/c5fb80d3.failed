KVM: PPC: Book3S HV: Fix decrementer timeouts with non-zero TB offset

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s hv - Fix decrementer timeouts with non-zero TB offset (Don Zickus) [1127366]
Rebuild_FUZZ: 95.65%
commit-author Paul Mackerras <paulus@samba.org>
commit c5fb80d3b24f6280bd6f608d8f2a02139a0fabaf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/c5fb80d3.failed

Commit c7699822bc21 ("KVM: PPC: Book3S HV: Make physical thread 0 do
the MMU switching") reordered the guest entry/exit code so that most
of the guest register save/restore code happened in guest MMU context.
A side effect of that is that the timebase still contains the guest
timebase value at the point where we compute and use vcpu->arch.dec_expires,
and therefore that is now a guest timebase value rather than a host
timebase value.  That in turn means that the timeouts computed in
kvmppc_set_timer() are wrong if the timebase offset for the guest is
non-zero.  The consequence of that is things such as "sleep 1" in a
guest after migration may sleep for much longer than they should.

This fixes the problem by converting between guest and host timebase
values as necessary, by adding or subtracting the timebase offset.
This also fixes an incorrect comment.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Acked-by: Scott Wood <scottwood@freescale.com>
(cherry picked from commit c5fb80d3b24f6280bd6f608d8f2a02139a0fabaf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index 0ae4c28ae4b6,42bd2e694b1b..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -642,6 -565,315 +642,318 @@@ toc_tlbie_lock
  	addi	r6,r6,VCPU_SLB_SIZE
  	bdnz	1b
  9:
++<<<<<<< HEAD
++=======
+ 	/* Increment yield count if they have a VPA */
+ 	ld	r3, VCPU_VPA(r4)
+ 	cmpdi	r3, 0
+ 	beq	25f
+ 	lwz	r5, LPPACA_YIELDCOUNT(r3)
+ 	addi	r5, r5, 1
+ 	stw	r5, LPPACA_YIELDCOUNT(r3)
+ 	li	r6, 1
+ 	stb	r6, VCPU_VPA_DIRTY(r4)
+ 25:
+ 
+ BEGIN_FTR_SECTION
+ 	/* Save purr/spurr */
+ 	mfspr	r5,SPRN_PURR
+ 	mfspr	r6,SPRN_SPURR
+ 	std	r5,HSTATE_PURR(r13)
+ 	std	r6,HSTATE_SPURR(r13)
+ 	ld	r7,VCPU_PURR(r4)
+ 	ld	r8,VCPU_SPURR(r4)
+ 	mtspr	SPRN_PURR,r7
+ 	mtspr	SPRN_SPURR,r8
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Set partition DABR */
+ 	/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */
+ 	lwz	r5,VCPU_DABRX(r4)
+ 	ld	r6,VCPU_DABR(r4)
+ 	mtspr	SPRN_DABRX,r5
+ 	mtspr	SPRN_DABR,r6
+  BEGIN_FTR_SECTION_NESTED(89)
+ 	isync
+  END_FTR_SECTION_NESTED(CPU_FTR_ARCH_206, CPU_FTR_ARCH_206, 89)
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 
+ #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+ BEGIN_FTR_SECTION
+ 	b	skip_tm
+ END_FTR_SECTION_IFCLR(CPU_FTR_TM)
+ 
+ 	/* Turn on TM/FP/VSX/VMX so we can restore them. */
+ 	mfmsr	r5
+ 	li	r6, MSR_TM >> 32
+ 	sldi	r6, r6, 32
+ 	or	r5, r5, r6
+ 	ori	r5, r5, MSR_FP
+ 	oris	r5, r5, (MSR_VEC | MSR_VSX)@h
+ 	mtmsrd	r5
+ 
+ 	/*
+ 	 * The user may change these outside of a transaction, so they must
+ 	 * always be context switched.
+ 	 */
+ 	ld	r5, VCPU_TFHAR(r4)
+ 	ld	r6, VCPU_TFIAR(r4)
+ 	ld	r7, VCPU_TEXASR(r4)
+ 	mtspr	SPRN_TFHAR, r5
+ 	mtspr	SPRN_TFIAR, r6
+ 	mtspr	SPRN_TEXASR, r7
+ 
+ 	ld	r5, VCPU_MSR(r4)
+ 	rldicl. r5, r5, 64 - MSR_TS_S_LG, 62
+ 	beq	skip_tm	/* TM not active in guest */
+ 
+ 	/* Make sure the failure summary is set, otherwise we'll program check
+ 	 * when we trechkpt.  It's possible that this might have been not set
+ 	 * on a kvmppc_set_one_reg() call but we shouldn't let this crash the
+ 	 * host.
+ 	 */
+ 	oris	r7, r7, (TEXASR_FS)@h
+ 	mtspr	SPRN_TEXASR, r7
+ 
+ 	/*
+ 	 * We need to load up the checkpointed state for the guest.
+ 	 * We need to do this early as it will blow away any GPRs, VSRs and
+ 	 * some SPRs.
+ 	 */
+ 
+ 	mr	r31, r4
+ 	addi	r3, r31, VCPU_FPRS_TM
+ 	bl	.load_fp_state
+ 	addi	r3, r31, VCPU_VRS_TM
+ 	bl	.load_vr_state
+ 	mr	r4, r31
+ 	lwz	r7, VCPU_VRSAVE_TM(r4)
+ 	mtspr	SPRN_VRSAVE, r7
+ 
+ 	ld	r5, VCPU_LR_TM(r4)
+ 	lwz	r6, VCPU_CR_TM(r4)
+ 	ld	r7, VCPU_CTR_TM(r4)
+ 	ld	r8, VCPU_AMR_TM(r4)
+ 	ld	r9, VCPU_TAR_TM(r4)
+ 	mtlr	r5
+ 	mtcr	r6
+ 	mtctr	r7
+ 	mtspr	SPRN_AMR, r8
+ 	mtspr	SPRN_TAR, r9
+ 
+ 	/*
+ 	 * Load up PPR and DSCR values but don't put them in the actual SPRs
+ 	 * till the last moment to avoid running with userspace PPR and DSCR for
+ 	 * too long.
+ 	 */
+ 	ld	r29, VCPU_DSCR_TM(r4)
+ 	ld	r30, VCPU_PPR_TM(r4)
+ 
+ 	std	r2, PACATMSCRATCH(r13) /* Save TOC */
+ 
+ 	/* Clear the MSR RI since r1, r13 are all going to be foobar. */
+ 	li	r5, 0
+ 	mtmsrd	r5, 1
+ 
+ 	/* Load GPRs r0-r28 */
+ 	reg = 0
+ 	.rept	29
+ 	ld	reg, VCPU_GPRS_TM(reg)(r31)
+ 	reg = reg + 1
+ 	.endr
+ 
+ 	mtspr	SPRN_DSCR, r29
+ 	mtspr	SPRN_PPR, r30
+ 
+ 	/* Load final GPRs */
+ 	ld	29, VCPU_GPRS_TM(29)(r31)
+ 	ld	30, VCPU_GPRS_TM(30)(r31)
+ 	ld	31, VCPU_GPRS_TM(31)(r31)
+ 
+ 	/* TM checkpointed state is now setup.  All GPRs are now volatile. */
+ 	TRECHKPT
+ 
+ 	/* Now let's get back the state we need. */
+ 	HMT_MEDIUM
+ 	GET_PACA(r13)
+ 	ld	r29, HSTATE_DSCR(r13)
+ 	mtspr	SPRN_DSCR, r29
+ 	ld	r4, HSTATE_KVM_VCPU(r13)
+ 	ld	r1, HSTATE_HOST_R1(r13)
+ 	ld	r2, PACATMSCRATCH(r13)
+ 
+ 	/* Set the MSR RI since we have our registers back. */
+ 	li	r5, MSR_RI
+ 	mtmsrd	r5, 1
+ skip_tm:
+ #endif
+ 
+ 	/* Load guest PMU registers */
+ 	/* R4 is live here (vcpu pointer) */
+ 	li	r3, 1
+ 	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
+ 	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
+ 	isync
+ 	lwz	r3, VCPU_PMC(r4)	/* always load up guest PMU registers */
+ 	lwz	r5, VCPU_PMC + 4(r4)	/* to prevent information leak */
+ 	lwz	r6, VCPU_PMC + 8(r4)
+ 	lwz	r7, VCPU_PMC + 12(r4)
+ 	lwz	r8, VCPU_PMC + 16(r4)
+ 	lwz	r9, VCPU_PMC + 20(r4)
+ BEGIN_FTR_SECTION
+ 	lwz	r10, VCPU_PMC + 24(r4)
+ 	lwz	r11, VCPU_PMC + 28(r4)
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	mtspr	SPRN_PMC1, r3
+ 	mtspr	SPRN_PMC2, r5
+ 	mtspr	SPRN_PMC3, r6
+ 	mtspr	SPRN_PMC4, r7
+ 	mtspr	SPRN_PMC5, r8
+ 	mtspr	SPRN_PMC6, r9
+ BEGIN_FTR_SECTION
+ 	mtspr	SPRN_PMC7, r10
+ 	mtspr	SPRN_PMC8, r11
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	ld	r3, VCPU_MMCR(r4)
+ 	ld	r5, VCPU_MMCR + 8(r4)
+ 	ld	r6, VCPU_MMCR + 16(r4)
+ 	ld	r7, VCPU_SIAR(r4)
+ 	ld	r8, VCPU_SDAR(r4)
+ 	mtspr	SPRN_MMCR1, r5
+ 	mtspr	SPRN_MMCRA, r6
+ 	mtspr	SPRN_SIAR, r7
+ 	mtspr	SPRN_SDAR, r8
+ BEGIN_FTR_SECTION
+ 	ld	r5, VCPU_MMCR + 24(r4)
+ 	ld	r6, VCPU_SIER(r4)
+ 	lwz	r7, VCPU_PMC + 24(r4)
+ 	lwz	r8, VCPU_PMC + 28(r4)
+ 	ld	r9, VCPU_MMCR + 32(r4)
+ 	mtspr	SPRN_MMCR2, r5
+ 	mtspr	SPRN_SIER, r6
+ 	mtspr	SPRN_SPMC1, r7
+ 	mtspr	SPRN_SPMC2, r8
+ 	mtspr	SPRN_MMCRS, r9
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
+ 	mtspr	SPRN_MMCR0, r3
+ 	isync
+ 
+ 	/* Load up FP, VMX and VSX registers */
+ 	bl	kvmppc_load_fp
+ 
+ 	ld	r14, VCPU_GPR(R14)(r4)
+ 	ld	r15, VCPU_GPR(R15)(r4)
+ 	ld	r16, VCPU_GPR(R16)(r4)
+ 	ld	r17, VCPU_GPR(R17)(r4)
+ 	ld	r18, VCPU_GPR(R18)(r4)
+ 	ld	r19, VCPU_GPR(R19)(r4)
+ 	ld	r20, VCPU_GPR(R20)(r4)
+ 	ld	r21, VCPU_GPR(R21)(r4)
+ 	ld	r22, VCPU_GPR(R22)(r4)
+ 	ld	r23, VCPU_GPR(R23)(r4)
+ 	ld	r24, VCPU_GPR(R24)(r4)
+ 	ld	r25, VCPU_GPR(R25)(r4)
+ 	ld	r26, VCPU_GPR(R26)(r4)
+ 	ld	r27, VCPU_GPR(R27)(r4)
+ 	ld	r28, VCPU_GPR(R28)(r4)
+ 	ld	r29, VCPU_GPR(R29)(r4)
+ 	ld	r30, VCPU_GPR(R30)(r4)
+ 	ld	r31, VCPU_GPR(R31)(r4)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Switch DSCR to guest value */
+ 	ld	r5, VCPU_DSCR(r4)
+ 	mtspr	SPRN_DSCR, r5
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Skip next section on POWER7 or PPC970 */
+ 	b	8f
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
+ 	mfmsr	r8
+ 	li	r0, 1
+ 	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
+ 	mtmsrd	r8
+ 
+ 	/* Load up POWER8-specific registers */
+ 	ld	r5, VCPU_IAMR(r4)
+ 	lwz	r6, VCPU_PSPB(r4)
+ 	ld	r7, VCPU_FSCR(r4)
+ 	mtspr	SPRN_IAMR, r5
+ 	mtspr	SPRN_PSPB, r6
+ 	mtspr	SPRN_FSCR, r7
+ 	ld	r5, VCPU_DAWR(r4)
+ 	ld	r6, VCPU_DAWRX(r4)
+ 	ld	r7, VCPU_CIABR(r4)
+ 	ld	r8, VCPU_TAR(r4)
+ 	mtspr	SPRN_DAWR, r5
+ 	mtspr	SPRN_DAWRX, r6
+ 	mtspr	SPRN_CIABR, r7
+ 	mtspr	SPRN_TAR, r8
+ 	ld	r5, VCPU_IC(r4)
+ 	ld	r6, VCPU_VTB(r4)
+ 	mtspr	SPRN_IC, r5
+ 	mtspr	SPRN_VTB, r6
+ 	ld	r8, VCPU_EBBHR(r4)
+ 	mtspr	SPRN_EBBHR, r8
+ 	ld	r5, VCPU_EBBRR(r4)
+ 	ld	r6, VCPU_BESCR(r4)
+ 	ld	r7, VCPU_CSIGR(r4)
+ 	ld	r8, VCPU_TACR(r4)
+ 	mtspr	SPRN_EBBRR, r5
+ 	mtspr	SPRN_BESCR, r6
+ 	mtspr	SPRN_CSIGR, r7
+ 	mtspr	SPRN_TACR, r8
+ 	ld	r5, VCPU_TCSCR(r4)
+ 	ld	r6, VCPU_ACOP(r4)
+ 	lwz	r7, VCPU_GUEST_PID(r4)
+ 	ld	r8, VCPU_WORT(r4)
+ 	mtspr	SPRN_TCSCR, r5
+ 	mtspr	SPRN_ACOP, r6
+ 	mtspr	SPRN_PID, r7
+ 	mtspr	SPRN_WORT, r8
+ 8:
+ 
+ 	/*
+ 	 * Set the decrementer to the guest decrementer.
+ 	 */
+ 	ld	r8,VCPU_DEC_EXPIRES(r4)
+ 	/* r8 is a host timebase value here, convert to guest TB */
+ 	ld	r5,HSTATE_KVM_VCORE(r13)
+ 	ld	r6,VCORE_TB_OFFSET(r5)
+ 	add	r8,r8,r6
+ 	mftb	r7
+ 	subf	r3,r7,r8
+ 	mtspr	SPRN_DEC,r3
+ 	stw	r3,VCPU_DEC(r4)
+ 
+ 	ld	r5, VCPU_SPRG0(r4)
+ 	ld	r6, VCPU_SPRG1(r4)
+ 	ld	r7, VCPU_SPRG2(r4)
+ 	ld	r8, VCPU_SPRG3(r4)
+ 	mtspr	SPRN_SPRG0, r5
+ 	mtspr	SPRN_SPRG1, r6
+ 	mtspr	SPRN_SPRG2, r7
+ 	mtspr	SPRN_SPRG3, r8
+ 
+ 	/* Load up DAR and DSISR */
+ 	ld	r5, VCPU_DAR(r4)
+ 	lwz	r6, VCPU_DSISR(r4)
+ 	mtspr	SPRN_DAR, r5
+ 	mtspr	SPRN_DSISR, r6
+ 
+ BEGIN_FTR_SECTION
+ 	/* Restore AMR and UAMOR, set AMOR to all 1s */
+ 	ld	r5,VCPU_AMR(r4)
+ 	ld	r6,VCPU_UAMOR(r4)
+ 	li	r7,-1
+ 	mtspr	SPRN_AMR,r5
+ 	mtspr	SPRN_UAMOR,r6
+ 	mtspr	SPRN_AMOR,r7
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
++>>>>>>> c5fb80d3b24f (KVM: PPC: Book3S HV: Fix decrementer timeouts with non-zero TB offset)
  
  	/* Restore state of CTRL run bit; assume 1 on entry */
  	lwz	r5,VCPU_CTRL(r4)
@@@ -993,235 -1203,54 +1305,239 @@@ BEGIN_FTR_SECTIO
  	mtspr	SPRN_SPURR,r4
  END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_201)
  
 -	/* Save DEC */
 -	mfspr	r5,SPRN_DEC
 -	mftb	r6
 -	extsw	r5,r5
 -	add	r5,r5,r6
 -	/* r5 is a guest timebase value here, convert to host TB */
 -	ld	r3,HSTATE_KVM_VCORE(r13)
 -	ld	r4,VCORE_TB_OFFSET(r3)
 -	subf	r5,r4,r5
 -	std	r5,VCPU_DEC_EXPIRES(r9)
 +	/* Clear out SLB */
 +	li	r5,0
 +	slbmte	r5,r5
 +	slbia
 +	ptesync
  
 +hdec_soon:			/* r9 = vcpu, r12 = trap, r13 = paca */
  BEGIN_FTR_SECTION
 -	b	8f
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 -	/* Save POWER8-specific registers */
 -	mfspr	r5, SPRN_IAMR
 -	mfspr	r6, SPRN_PSPB
 -	mfspr	r7, SPRN_FSCR
 -	std	r5, VCPU_IAMR(r9)
 -	stw	r6, VCPU_PSPB(r9)
 -	std	r7, VCPU_FSCR(r9)
 -	mfspr	r5, SPRN_IC
 -	mfspr	r6, SPRN_VTB
 -	mfspr	r7, SPRN_TAR
 -	std	r5, VCPU_IC(r9)
 -	std	r6, VCPU_VTB(r9)
 -	std	r7, VCPU_TAR(r9)
 -	mfspr	r8, SPRN_EBBHR
 -	std	r8, VCPU_EBBHR(r9)
 -	mfspr	r5, SPRN_EBBRR
 -	mfspr	r6, SPRN_BESCR
 -	mfspr	r7, SPRN_CSIGR
 -	mfspr	r8, SPRN_TACR
 -	std	r5, VCPU_EBBRR(r9)
 -	std	r6, VCPU_BESCR(r9)
 -	std	r7, VCPU_CSIGR(r9)
 -	std	r8, VCPU_TACR(r9)
 -	mfspr	r5, SPRN_TCSCR
 -	mfspr	r6, SPRN_ACOP
 -	mfspr	r7, SPRN_PID
 -	mfspr	r8, SPRN_WORT
 -	std	r5, VCPU_TCSCR(r9)
 -	std	r6, VCPU_ACOP(r9)
 -	stw	r7, VCPU_GUEST_PID(r9)
 -	std	r8, VCPU_WORT(r9)
 -8:
 +	b	32f
 +END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 +	/*
 +	 * POWER7 guest -> host partition switch code.
 +	 * We don't have to lock against tlbies but we do
 +	 * have to coordinate the hardware threads.
 +	 */
 +	/* Increment the threads-exiting-guest count in the 0xff00
 +	   bits of vcore->entry_exit_count */
 +	ld	r5,HSTATE_KVM_VCORE(r13)
 +	addi	r6,r5,VCORE_ENTRY_EXIT
 +41:	lwarx	r3,0,r6
 +	addi	r0,r3,0x100
 +	stwcx.	r0,0,r6
 +	bne	41b
 +	isync		/* order stwcx. vs. reading napping_threads */
  
 -	/* Save and reset AMR and UAMOR before turning on the MMU */
 +	/*
 +	 * At this point we have an interrupt that we have to pass
 +	 * up to the kernel or qemu; we can't handle it in real mode.
 +	 * Thus we have to do a partition switch, so we have to
 +	 * collect the other threads, if we are the first thread
 +	 * to take an interrupt.  To do this, we set the HDEC to 0,
 +	 * which causes an HDEC interrupt in all threads within 2ns
 +	 * because the HDEC register is shared between all 4 threads.
 +	 * However, we don't need to bother if this is an HDEC
 +	 * interrupt, since the other threads will already be on their
 +	 * way here in that case.
 +	 */
 +	cmpwi	r3,0x100	/* Are we the first here? */
 +	bge	43f
 +	cmpwi	r3,1		/* Are any other threads in the guest? */
 +	ble	43f
 +	cmpwi	r12,BOOK3S_INTERRUPT_HV_DECREMENTER
 +	beq	40f
 +	li	r0,0
 +	mtspr	SPRN_HDEC,r0
 +40:
 +	/*
 +	 * Send an IPI to any napping threads, since an HDEC interrupt
 +	 * doesn't wake CPUs up from nap.
 +	 */
 +	lwz	r3,VCORE_NAPPING_THREADS(r5)
 +	lwz	r4,VCPU_PTID(r9)
 +	li	r0,1
 +	sld	r0,r0,r4
 +	andc.	r3,r3,r0		/* no sense IPI'ing ourselves */
 +	beq	43f
 +	/* Order entry/exit update vs. IPIs */
 +	sync
 +	mulli	r4,r4,PACA_SIZE		/* get paca for thread 0 */
 +	subf	r6,r4,r13
 +42:	andi.	r0,r3,1
 +	beq	44f
 +	ld	r8,HSTATE_XICS_PHYS(r6)	/* get thread's XICS reg addr */
 +	li	r0,IPI_PRIORITY
 +	li	r7,XICS_MFRR
 +	stbcix	r0,r7,r8		/* trigger the IPI */
 +44:	srdi.	r3,r3,1
 +	addi	r6,r6,PACA_SIZE
 +	bne	42b
 +
 +	/* Secondary threads wait for primary to do partition switch */
 +43:	ld	r4,VCPU_KVM(r9)		/* pointer to struct kvm */
 +	ld	r5,HSTATE_KVM_VCORE(r13)
 +	lwz	r3,VCPU_PTID(r9)
 +	cmpwi	r3,0
 +	beq	15f
 +	HMT_LOW
 +13:	lbz	r3,VCORE_IN_GUEST(r5)
 +	cmpwi	r3,0
 +	bne	13b
 +	HMT_MEDIUM
 +	b	16f
 +
 +	/* Primary thread waits for all the secondaries to exit guest */
 +15:	lwz	r3,VCORE_ENTRY_EXIT(r5)
 +	srwi	r0,r3,8
 +	clrldi	r3,r3,56
 +	cmpw	r3,r0
 +	bne	15b
 +	isync
 +
 +	/* Primary thread switches back to host partition */
 +	ld	r6,KVM_HOST_SDR1(r4)
 +	lwz	r7,KVM_HOST_LPID(r4)
 +	li	r8,LPID_RSVD		/* switch to reserved LPID */
 +	mtspr	SPRN_LPID,r8
 +	ptesync
 +	mtspr	SPRN_SDR1,r6		/* switch to partition page table */
 +	mtspr	SPRN_LPID,r7
 +	isync
 +
 +	/* Subtract timebase offset from timebase */
 +	ld	r8,VCORE_TB_OFFSET(r5)
 +	cmpdi	r8,0
 +	beq	17f
 +	mftb	r6			/* current host timebase */
 +	subf	r8,r8,r6
 +	mtspr	SPRN_TBU40,r8		/* update upper 40 bits */
 +	mftb	r7			/* check if lower 24 bits overflowed */
 +	clrldi	r6,r6,40
 +	clrldi	r7,r7,40
 +	cmpld	r7,r6
 +	bge	17f
 +	addis	r8,r8,0x100		/* if so, increment upper 40 bits */
 +	mtspr	SPRN_TBU40,r8
 +
 +	/* Reset PCR */
 +17:	ld	r0, VCORE_PCR(r5)
 +	cmpdi	r0, 0
 +	beq	18f
 +	li	r0, 0
 +	mtspr	SPRN_PCR, r0
 +18:
 +	/* Signal secondary CPUs to continue */
 +	stb	r0,VCORE_IN_GUEST(r5)
 +	lis	r8,0x7fff		/* MAX_INT@h */
 +	mtspr	SPRN_HDEC,r8
 +
 +16:	ld	r8,KVM_HOST_LPCR(r4)
 +	mtspr	SPRN_LPCR,r8
 +	isync
 +	b	33f
 +
 +	/*
 +	 * PPC970 guest -> host partition switch code.
 +	 * We have to lock against concurrent tlbies, and
 +	 * we have to flush the whole TLB.
 +	 */
 +32:	ld	r4,VCPU_KVM(r9)		/* pointer to struct kvm */
 +
 +	/* Take the guest's tlbie_lock */
 +#ifdef __BIG_ENDIAN__
 +	lwz	r8,PACA_LOCK_TOKEN(r13)
 +#else
 +	lwz	r8,PACAPACAINDEX(r13)
 +#endif
 +	addi	r3,r4,KVM_TLBIE_LOCK
 +24:	lwarx	r0,0,r3
 +	cmpwi	r0,0
 +	bne	24b
 +	stwcx.	r8,0,r3
 +	bne	24b
 +	isync
 +
 +	ld	r7,KVM_HOST_LPCR(r4)	/* use kvm->arch.host_lpcr for HID4 */
 +	li	r0,0x18f
 +	rotldi	r0,r0,HID4_LPID5_SH	/* all lpid bits in HID4 = 1 */
 +	or	r0,r7,r0
 +	ptesync
 +	sync
 +	mtspr	SPRN_HID4,r0		/* switch to reserved LPID */
 +	isync
 +	li	r0,0
 +	stw	r0,0(r3)		/* drop guest tlbie_lock */
 +
 +	/* invalidate the whole TLB */
 +	li	r0,256
 +	mtctr	r0
 +	li	r6,0
 +25:	tlbiel	r6
 +	addi	r6,r6,0x1000
 +	bdnz	25b
 +	ptesync
 +
 +	/* take native_tlbie_lock */
 +	ld	r3,toc_tlbie_lock@toc(2)
 +24:	lwarx	r0,0,r3
 +	cmpwi	r0,0
 +	bne	24b
 +	stwcx.	r8,0,r3
 +	bne	24b
 +	isync
 +
 +	ld	r6,KVM_HOST_SDR1(r4)
 +	mtspr	SPRN_SDR1,r6		/* switch to host page table */
 +
 +	/* Set up host HID4 value */
 +	sync
 +	mtspr	SPRN_HID4,r7
 +	isync
 +	li	r0,0
 +	stw	r0,0(r3)		/* drop native_tlbie_lock */
 +
 +	lis	r8,0x7fff		/* MAX_INT@h */
 +	mtspr	SPRN_HDEC,r8
 +
 +	/* Disable HDEC interrupts */
 +	mfspr	r0,SPRN_HID0
 +	li	r3,0
 +	rldimi	r0,r3, HID0_HDICE_SH, 64-HID0_HDICE_SH-1
 +	sync
 +	mtspr	SPRN_HID0,r0
 +	mfspr	r0,SPRN_HID0
 +	mfspr	r0,SPRN_HID0
 +	mfspr	r0,SPRN_HID0
 +	mfspr	r0,SPRN_HID0
 +	mfspr	r0,SPRN_HID0
 +	mfspr	r0,SPRN_HID0
 +
 +	/* load host SLB entries */
 +33:	ld	r8,PACA_SLBSHADOWPTR(r13)
 +
 +	.rept	SLB_NUM_BOLTED
 +	ld	r5,SLBSHADOW_SAVEAREA(r8)
 +	ld	r6,SLBSHADOW_SAVEAREA+8(r8)
 +	andis.	r7,r5,SLB_ESID_V@h
 +	beq	1f
 +	slbmte	r6,r5
 +1:	addi	r8,r8,16
 +	.endr
 +
 +	/* Save DEC */
 +	mfspr	r5,SPRN_DEC
 +	mftb	r6
 +	extsw	r5,r5
 +	add	r5,r5,r6
++	/* r5 is a guest timebase value here, convert to host TB */
++	ld	r3,HSTATE_KVM_VCORE(r13)
++	ld	r4,VCORE_TB_OFFSET(r3)
++	subf	r5,r4,r5
 +	std	r5,VCPU_DEC_EXPIRES(r9)
 +
 +	/* Save and reset AMR and UAMOR before turning on the MMU */
  BEGIN_FTR_SECTION
  	mfspr	r5,SPRN_AMR
  	mfspr	r6,SPRN_UAMOR
@@@ -1346,13 -1454,150 +1662,156 @@@ secondary_too_late
  	cmpwi	r3,0
  	bne	13b
  	HMT_MEDIUM
++<<<<<<< HEAD
 +	li	r0, KVM_GUEST_MODE_NONE
 +	stb	r0, HSTATE_IN_GUEST(r13)
 +	ld	r11,PACA_SLBSHADOWPTR(r13)
++=======
+ 	b	16f
+ 
+ 	/* Primary thread waits for all the secondaries to exit guest */
+ 15:	lwz	r3,VCORE_ENTRY_EXIT(r5)
+ 	srwi	r0,r3,8
+ 	clrldi	r3,r3,56
+ 	cmpw	r3,r0
+ 	bne	15b
+ 	isync
+ 
+ 	/* Primary thread switches back to host partition */
+ 	ld	r6,KVM_HOST_SDR1(r4)
+ 	lwz	r7,KVM_HOST_LPID(r4)
+ 	li	r8,LPID_RSVD		/* switch to reserved LPID */
+ 	mtspr	SPRN_LPID,r8
+ 	ptesync
+ 	mtspr	SPRN_SDR1,r6		/* switch to partition page table */
+ 	mtspr	SPRN_LPID,r7
+ 	isync
+ 
+ BEGIN_FTR_SECTION
+ 	/* DPDES is shared between threads */
+ 	mfspr	r7, SPRN_DPDES
+ 	std	r7, VCORE_DPDES(r5)
+ 	/* clear DPDES so we don't get guest doorbells in the host */
+ 	li	r8, 0
+ 	mtspr	SPRN_DPDES, r8
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
+ 
+ 	/* Subtract timebase offset from timebase */
+ 	ld	r8,VCORE_TB_OFFSET(r5)
+ 	cmpdi	r8,0
+ 	beq	17f
+ 	mftb	r6			/* current guest timebase */
+ 	subf	r8,r8,r6
+ 	mtspr	SPRN_TBU40,r8		/* update upper 40 bits */
+ 	mftb	r7			/* check if lower 24 bits overflowed */
+ 	clrldi	r6,r6,40
+ 	clrldi	r7,r7,40
+ 	cmpld	r7,r6
+ 	bge	17f
+ 	addis	r8,r8,0x100		/* if so, increment upper 40 bits */
+ 	mtspr	SPRN_TBU40,r8
+ 
+ 	/* Reset PCR */
+ 17:	ld	r0, VCORE_PCR(r5)
+ 	cmpdi	r0, 0
+ 	beq	18f
+ 	li	r0, 0
+ 	mtspr	SPRN_PCR, r0
+ 18:
+ 	/* Signal secondary CPUs to continue */
+ 	stb	r0,VCORE_IN_GUEST(r5)
+ 	lis	r8,0x7fff		/* MAX_INT@h */
+ 	mtspr	SPRN_HDEC,r8
+ 
+ 16:	ld	r8,KVM_HOST_LPCR(r4)
+ 	mtspr	SPRN_LPCR,r8
+ 	isync
+ 	b	33f
+ 
+ 	/*
+ 	 * PPC970 guest -> host partition switch code.
+ 	 * We have to lock against concurrent tlbies, and
+ 	 * we have to flush the whole TLB.
+ 	 */
+ 32:	ld	r5,HSTATE_KVM_VCORE(r13)
+ 	ld	r4,VCORE_KVM(r5)	/* pointer to struct kvm */
+ 
+ 	/* Take the guest's tlbie_lock */
+ #ifdef __BIG_ENDIAN__
+ 	lwz	r8,PACA_LOCK_TOKEN(r13)
+ #else
+ 	lwz	r8,PACAPACAINDEX(r13)
+ #endif
+ 	addi	r3,r4,KVM_TLBIE_LOCK
+ 24:	lwarx	r0,0,r3
+ 	cmpwi	r0,0
+ 	bne	24b
+ 	stwcx.	r8,0,r3
+ 	bne	24b
+ 	isync
+ 
+ 	ld	r7,KVM_HOST_LPCR(r4)	/* use kvm->arch.host_lpcr for HID4 */
+ 	li	r0,0x18f
+ 	rotldi	r0,r0,HID4_LPID5_SH	/* all lpid bits in HID4 = 1 */
+ 	or	r0,r7,r0
+ 	ptesync
+ 	sync
+ 	mtspr	SPRN_HID4,r0		/* switch to reserved LPID */
+ 	isync
+ 	li	r0,0
+ 	stw	r0,0(r3)		/* drop guest tlbie_lock */
+ 
+ 	/* invalidate the whole TLB */
+ 	li	r0,256
+ 	mtctr	r0
+ 	li	r6,0
+ 25:	tlbiel	r6
+ 	addi	r6,r6,0x1000
+ 	bdnz	25b
+ 	ptesync
+ 
+ 	/* take native_tlbie_lock */
+ 	ld	r3,toc_tlbie_lock@toc(2)
+ 24:	lwarx	r0,0,r3
+ 	cmpwi	r0,0
+ 	bne	24b
+ 	stwcx.	r8,0,r3
+ 	bne	24b
+ 	isync
+ 
+ 	ld	r6,KVM_HOST_SDR1(r4)
+ 	mtspr	SPRN_SDR1,r6		/* switch to host page table */
+ 
+ 	/* Set up host HID4 value */
+ 	sync
+ 	mtspr	SPRN_HID4,r7
+ 	isync
+ 	li	r0,0
+ 	stw	r0,0(r3)		/* drop native_tlbie_lock */
+ 
+ 	lis	r8,0x7fff		/* MAX_INT@h */
+ 	mtspr	SPRN_HDEC,r8
+ 
+ 	/* Disable HDEC interrupts */
+ 	mfspr	r0,SPRN_HID0
+ 	li	r3,0
+ 	rldimi	r0,r3, HID0_HDICE_SH, 64-HID0_HDICE_SH-1
+ 	sync
+ 	mtspr	SPRN_HID0,r0
+ 	mfspr	r0,SPRN_HID0
+ 	mfspr	r0,SPRN_HID0
+ 	mfspr	r0,SPRN_HID0
+ 	mfspr	r0,SPRN_HID0
+ 	mfspr	r0,SPRN_HID0
+ 	mfspr	r0,SPRN_HID0
+ 
+ 	/* load host SLB entries */
+ 33:	ld	r8,PACA_SLBSHADOWPTR(r13)
++>>>>>>> c5fb80d3b24f (KVM: PPC: Book3S HV: Fix decrementer timeouts with non-zero TB offset)
  
  	.rept	SLB_NUM_BOLTED
 -	ld	r5,SLBSHADOW_SAVEAREA(r8)
 -	ld	r6,SLBSHADOW_SAVEAREA+8(r8)
 +	ld	r5,SLBSHADOW_SAVEAREA(r11)
 +	ld	r6,SLBSHADOW_SAVEAREA+8(r11)
  	andis.	r7,r5,SLB_ESID_V@h
  	beq	1f
  	slbmte	r6,r5
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
