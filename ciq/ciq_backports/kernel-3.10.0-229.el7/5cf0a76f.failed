iommu/vt-d: Clean up size handling for intel_iommu_unmap()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [iommu] vt-d: Clean up size handling for intel_iommu_unmap() (Myron Stowe) [1129880 1087643]
Rebuild_FUZZ: 94.55%
commit-author David Woodhouse <David.Woodhouse@intel.com>
commit 5cf0a76fa2179d246fc0375d733bdccffd59382b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/5cf0a76f.failed

We have this horrid API where iommu_unmap() can unmap more than it's asked
to, if the IOVA in question happens to be mapped with a large page.

Instead of propagating this nonsense to the point where we end up returning
the page order from dma_pte_clear_range(), let's just do it once and adjust
the 'size' parameter accordingly.

Augment pfn_to_dma_pte() to return the level at which the PTE was found,
which will also be useful later if we end up changing the API for
iommu_iova_to_phys() to behave the same way as is being discussed upstream.

	Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
(cherry picked from commit 5cf0a76fa2179d246fc0375d733bdccffd59382b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index 9b6d839e77fe,6472bf15bef2..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -915,9 -904,6 +921,12 @@@ static void dma_pte_clear_range(struct 
  				   (void *)pte - (void *)first_pte);
  
  	} while (start_pfn && start_pfn <= last_pfn);
++<<<<<<< HEAD
 +
 +	order = (large_page - 1) * 9;
 +	return order;
++=======
++>>>>>>> 5cf0a76fa217 (iommu/vt-d: Clean up size handling for intel_iommu_unmap())
  }
  
  static void dma_pte_free_level(struct dmar_domain *domain, int level,
* Unmerged path drivers/iommu/intel-iommu.c
