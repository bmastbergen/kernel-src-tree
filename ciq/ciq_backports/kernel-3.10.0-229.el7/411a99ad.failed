nfs: clear_request_commit while holding i_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Weston Andros Adamson <dros@primarydata.com>
commit 411a99adffb4f993eee29759f744de01487044ac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/411a99ad.failed

	Signed-off-by: Weston Andros Adamson <dros@primarydata.com>
	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit 411a99adffb4f993eee29759f744de01487044ac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/nfs/write.c
diff --cc fs/nfs/write.c
index a89cdd72f163,1065de26190b..000000000000
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@@ -343,36 -315,242 +343,226 @@@ static void nfs_end_page_writeback(stru
  		clear_bdi_congested(&nfss->backing_dev_info, BLK_RW_ASYNC);
  }
  
++<<<<<<< HEAD
 +static struct nfs_page *nfs_find_and_lock_request(struct page *page, bool nonblock)
++=======
+ 
+ /* nfs_page_group_clear_bits
+  *   @req - an nfs request
+  * clears all page group related bits from @req
+  */
+ static void
+ nfs_page_group_clear_bits(struct nfs_page *req)
+ {
+ 	clear_bit(PG_TEARDOWN, &req->wb_flags);
+ 	clear_bit(PG_UNLOCKPAGE, &req->wb_flags);
+ 	clear_bit(PG_UPTODATE, &req->wb_flags);
+ 	clear_bit(PG_WB_END, &req->wb_flags);
+ 	clear_bit(PG_REMOVE, &req->wb_flags);
+ }
+ 
+ 
+ /*
+  * nfs_unroll_locks_and_wait -  unlock all newly locked reqs and wait on @req
+  *
+  * this is a helper function for nfs_lock_and_join_requests
+  *
+  * @inode - inode associated with request page group, must be holding inode lock
+  * @head  - head request of page group, must be holding head lock
+  * @req   - request that couldn't lock and needs to wait on the req bit lock
+  * @nonblock - if true, don't actually wait
+  *
+  * NOTE: this must be called holding page_group bit lock and inode spin lock
+  *       and BOTH will be released before returning.
+  *
+  * returns 0 on success, < 0 on error.
+  */
+ static int
+ nfs_unroll_locks_and_wait(struct inode *inode, struct nfs_page *head,
+ 			  struct nfs_page *req, bool nonblock)
+ 	__releases(&inode->i_lock)
+ {
+ 	struct nfs_page *tmp;
+ 	int ret;
+ 
+ 	/* relinquish all the locks successfully grabbed this run */
+ 	for (tmp = head ; tmp != req; tmp = tmp->wb_this_page)
+ 		nfs_unlock_request(tmp);
+ 
+ 	WARN_ON_ONCE(test_bit(PG_TEARDOWN, &req->wb_flags));
+ 
+ 	/* grab a ref on the request that will be waited on */
+ 	kref_get(&req->wb_kref);
+ 
+ 	nfs_page_group_unlock(head);
+ 	spin_unlock(&inode->i_lock);
+ 
+ 	/* release ref from nfs_page_find_head_request_locked */
+ 	nfs_release_request(head);
+ 
+ 	if (!nonblock)
+ 		ret = nfs_wait_on_request(req);
+ 	else
+ 		ret = -EAGAIN;
+ 	nfs_release_request(req);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * nfs_destroy_unlinked_subrequests - destroy recently unlinked subrequests
+  *
+  * @destroy_list - request list (using wb_this_page) terminated by @old_head
+  * @old_head - the old head of the list
+  *
+  * All subrequests must be locked and removed from all lists, so at this point
+  * they are only "active" in this function, and possibly in nfs_wait_on_request
+  * with a reference held by some other context.
+  */
+ static void
+ nfs_destroy_unlinked_subrequests(struct nfs_page *destroy_list,
+ 				 struct nfs_page *old_head)
+ {
+ 	while (destroy_list) {
+ 		struct nfs_page *subreq = destroy_list;
+ 
+ 		destroy_list = (subreq->wb_this_page == old_head) ?
+ 				   NULL : subreq->wb_this_page;
+ 
+ 		WARN_ON_ONCE(old_head != subreq->wb_head);
+ 
+ 		/* make sure old group is not used */
+ 		subreq->wb_head = subreq;
+ 		subreq->wb_this_page = subreq;
+ 
+ 		/* subreq is now totally disconnected from page group or any
+ 		 * write / commit lists. last chance to wake any waiters */
+ 		nfs_unlock_request(subreq);
+ 
+ 		if (!test_bit(PG_TEARDOWN, &subreq->wb_flags)) {
+ 			/* release ref on old head request */
+ 			nfs_release_request(old_head);
+ 
+ 			nfs_page_group_clear_bits(subreq);
+ 
+ 			/* release the PG_INODE_REF reference */
+ 			if (test_and_clear_bit(PG_INODE_REF, &subreq->wb_flags))
+ 				nfs_release_request(subreq);
+ 			else
+ 				WARN_ON_ONCE(1);
+ 		} else {
+ 			WARN_ON_ONCE(test_bit(PG_CLEAN, &subreq->wb_flags));
+ 			/* zombie requests have already released the last
+ 			 * reference and were waiting on the rest of the
+ 			 * group to complete. Since it's no longer part of a
+ 			 * group, simply free the request */
+ 			nfs_page_group_clear_bits(subreq);
+ 			nfs_free_request(subreq);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * nfs_lock_and_join_requests - join all subreqs to the head req and return
+  *                              a locked reference, cancelling any pending
+  *                              operations for this page.
+  *
+  * @page - the page used to lookup the "page group" of nfs_page structures
+  * @nonblock - if true, don't block waiting for request locks
+  *
+  * This function joins all sub requests to the head request by first
+  * locking all requests in the group, cancelling any pending operations
+  * and finally updating the head request to cover the whole range covered by
+  * the (former) group.  All subrequests are removed from any write or commit
+  * lists, unlinked from the group and destroyed.
+  *
+  * Returns a locked, referenced pointer to the head request - which after
+  * this call is guaranteed to be the only request associated with the page.
+  * Returns NULL if no requests are found for @page, or a ERR_PTR if an
+  * error was encountered.
+  */
+ static struct nfs_page *
+ nfs_lock_and_join_requests(struct page *page, bool nonblock)
++>>>>>>> 411a99adffb4 (nfs: clear_request_commit while holding i_lock)
  {
  	struct inode *inode = page_file_mapping(page)->host;
 -	struct nfs_page *head, *subreq;
 -	struct nfs_page *destroy_list = NULL;
 -	unsigned int total_bytes;
 +	struct nfs_page *req;
  	int ret;
  
 -try_again:
 -	total_bytes = 0;
 -
 -	WARN_ON_ONCE(destroy_list);
 -
  	spin_lock(&inode->i_lock);
 -
 -	/*
 -	 * A reference is taken only on the head request which acts as a
 -	 * reference to the whole page group - the group will not be destroyed
 -	 * until the head reference is released.
 -	 */
 -	head = nfs_page_find_head_request_locked(NFS_I(inode), page);
 -
 -	if (!head) {
 -		spin_unlock(&inode->i_lock);
 -		return NULL;
 -	}
 -
 -	/* lock each request in the page group */
 -	ret = nfs_page_group_lock(head, false);
 -	if (ret < 0)
 -		return ERR_PTR(ret);
 -	subreq = head;
 -	do {
 -		/*
 -		 * Subrequests are always contiguous, non overlapping
 -		 * and in order. If not, it's a programming error.
 +	for (;;) {
 +		req = nfs_page_find_head_request_locked(NFS_I(inode), page);
 +		if (req == NULL)
 +			break;
 +		if (nfs_lock_request(req))
 +			break;
 +		/* Note: If we hold the page lock, as is the case in nfs_writepage,
 +		 *	 then the call to nfs_lock_request() will always
 +		 *	 succeed provided that someone hasn't already marked the
 +		 *	 request as dirty (in which case we don't care).
  		 */
 -		WARN_ON_ONCE(subreq->wb_offset !=
 -		     (head->wb_offset + total_bytes));
 -
 -		/* keep track of how many bytes this group covers */
 -		total_bytes += subreq->wb_bytes;
 -
 -		if (!nfs_lock_request(subreq)) {
 -			/* releases page group bit lock and
 -			 * inode spin lock and all references */
 -			ret = nfs_unroll_locks_and_wait(inode, head,
 -				subreq, nonblock);
 -
 -			if (ret == 0)
 -				goto try_again;
 -
 +		spin_unlock(&inode->i_lock);
 +		if (!nonblock)
 +			ret = nfs_wait_on_request(req);
 +		else
 +			ret = -EAGAIN;
 +		nfs_release_request(req);
 +		if (ret != 0)
  			return ERR_PTR(ret);
++<<<<<<< HEAD
 +		spin_lock(&inode->i_lock);
 +	}
 +	spin_unlock(&inode->i_lock);
 +	return req;
++=======
+ 		}
+ 
+ 		subreq = subreq->wb_this_page;
+ 	} while (subreq != head);
+ 
+ 	/* Now that all requests are locked, make sure they aren't on any list.
+ 	 * Commit list removal accounting is done after locks are dropped */
+ 	subreq = head;
+ 	do {
+ 		nfs_clear_request_commit(subreq);
+ 		subreq = subreq->wb_this_page;
+ 	} while (subreq != head);
+ 
+ 	/* unlink subrequests from head, destroy them later */
+ 	if (head->wb_this_page != head) {
+ 		/* destroy list will be terminated by head */
+ 		destroy_list = head->wb_this_page;
+ 		head->wb_this_page = head;
+ 
+ 		/* change head request to cover whole range that
+ 		 * the former page group covered */
+ 		head->wb_bytes = total_bytes;
+ 	}
+ 
+ 	/*
+ 	 * prepare head request to be added to new pgio descriptor
+ 	 */
+ 	nfs_page_group_clear_bits(head);
+ 
+ 	/*
+ 	 * some part of the group was still on the inode list - otherwise
+ 	 * the group wouldn't be involved in async write.
+ 	 * grab a reference for the head request, iff it needs one.
+ 	 */
+ 	if (!test_and_set_bit(PG_INODE_REF, &head->wb_flags))
+ 		kref_get(&head->wb_kref);
+ 
+ 	nfs_page_group_unlock(head);
+ 
+ 	/* drop lock to clean uprequests on destroy list */
+ 	spin_unlock(&inode->i_lock);
+ 
+ 	nfs_destroy_unlinked_subrequests(destroy_list, head);
+ 
+ 	/* still holds ref on head from nfs_page_find_head_request_locked
+ 	 * and still has lock on head from lock loop */
+ 	return head;
++>>>>>>> 411a99adffb4 (nfs: clear_request_commit while holding i_lock)
  }
  
  /*
diff --git a/fs/nfs/filelayout/filelayout.c b/fs/nfs/filelayout/filelayout.c
index bcea4ec949e4..44b49ca1c12a 100644
--- a/fs/nfs/filelayout/filelayout.c
+++ b/fs/nfs/filelayout/filelayout.c
@@ -1029,6 +1029,7 @@ static u32 select_bucket_index(struct nfs4_filelayout_segment *fl, u32 j)
 
 /* The generic layer is about to remove the req from the commit list.
  * If this will make the bucket empty, it will need to put the lseg reference.
+ * Note this is must be called holding the inode (/cinfo) lock
  */
 static void
 filelayout_clear_request_commit(struct nfs_page *req,
@@ -1036,7 +1037,6 @@ filelayout_clear_request_commit(struct nfs_page *req,
 {
 	struct pnfs_layout_segment *freeme = NULL;
 
-	spin_lock(cinfo->lock);
 	if (!test_and_clear_bit(PG_COMMIT_TO_DS, &req->wb_flags))
 		goto out;
 	cinfo->ds->nwritten--;
@@ -1051,8 +1051,7 @@ filelayout_clear_request_commit(struct nfs_page *req,
 	}
 out:
 	nfs_request_remove_commit_list(req, cinfo);
-	spin_unlock(cinfo->lock);
-	pnfs_put_lseg(freeme);
+	pnfs_put_lseg_async(freeme);
 }
 
 static void
* Unmerged path fs/nfs/write.c
