perf tools: Save callchain info for each cumulative entry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [tools] perf: Save callchain info for each cumulative entry (Jiri Olsa) [1134356]
Rebuild_FUZZ: 94.44%
commit-author Namhyung Kim <namhyung@kernel.org>
commit be7f855a3eebe07f797b9e4a43bf59bab8ca3dbe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/be7f855a.failed

When accumulating callchain entry, also save current snapshot of the
chain so that it can show the rest of the chain.

	Signed-off-by: Namhyung Kim <namhyung@kernel.org>
	Tested-by: Arun Sharma <asharma@fb.com>
	Tested-by: Rodrigo Campos <rodrigo@sdfg.com.ar>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/1401335910-16832-10-git-send-email-namhyung@kernel.org
	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
(cherry picked from commit be7f855a3eebe07f797b9e4a43bf59bab8ca3dbe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/util/hist.c
diff --cc tools/perf/util/hist.c
index 9820956c30b9,c6f5f5251aad..000000000000
--- a/tools/perf/util/hist.c
+++ b/tools/perf/util/hist.c
@@@ -438,7 -455,446 +438,450 @@@ struct hist_entry *__hists__add_entry(s
  		.transaction = transaction,
  	};
  
++<<<<<<< HEAD
 +	return add_hist_entry(hists, &entry, al);
++=======
+ 	return add_hist_entry(hists, &entry, al, sample_self);
+ }
+ 
+ static int
+ iter_next_nop_entry(struct hist_entry_iter *iter __maybe_unused,
+ 		    struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_next_nop_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_prepare_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_sample *sample = iter->sample;
+ 	struct mem_info *mi;
+ 
+ 	mi = sample__resolve_mem(sample, al);
+ 	if (mi == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->priv = mi;
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	u64 cost;
+ 	struct mem_info *mi = iter->priv;
+ 	struct hist_entry *he;
+ 
+ 	if (mi == NULL)
+ 		return -EINVAL;
+ 
+ 	cost = iter->sample->weight;
+ 	if (!cost)
+ 		cost = 1;
+ 
+ 	/*
+ 	 * must pass period=weight in order to get the correct
+ 	 * sorting from hists__collapse_resort() which is solely
+ 	 * based on periods. We want sorting be done on nr_events * weight
+ 	 * and this is indirectly achieved by passing period=weight here
+ 	 * and the he_stat__add_period() function.
+ 	 */
+ 	he = __hists__add_entry(&iter->evsel->hists, al, iter->parent, NULL, mi,
+ 				cost, cost, 0, true);
+ 	if (!he)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_mem_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct hist_entry *he = iter->he;
+ 	struct mem_info *mx;
+ 	int err = -EINVAL;
+ 
+ 	if (he == NULL)
+ 		goto out;
+ 
+ 	if (ui__has_annotation()) {
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 		if (err)
+ 			goto out;
+ 
+ 		mx = he->mem_info;
+ 		err = addr_map_symbol__inc_samples(&mx->daddr, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	err = hist_entry__append_callchain(he, iter->sample);
+ 
+ out:
+ 	/*
+ 	 * We don't need to free iter->priv (mem_info) here since
+ 	 * the mem info was either already freed in add_hist_entry() or
+ 	 * passed to a new hist entry by hist_entry__new().
+ 	 */
+ 	iter->priv = NULL;
+ 
+ 	iter->he = NULL;
+ 	return err;
+ }
+ 
+ static int
+ iter_prepare_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi;
+ 	struct perf_sample *sample = iter->sample;
+ 
+ 	bi = sample__resolve_bstack(sample, al);
+ 	if (!bi)
+ 		return -ENOMEM;
+ 
+ 	iter->curr = 0;
+ 	iter->total = sample->branch_stack->nr;
+ 
+ 	iter->priv = bi;
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_branch_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			     struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_next_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi = iter->priv;
+ 	int i = iter->curr;
+ 
+ 	if (bi == NULL)
+ 		return 0;
+ 
+ 	if (iter->curr >= iter->total)
+ 		return 0;
+ 
+ 	al->map = bi[i].to.map;
+ 	al->sym = bi[i].to.sym;
+ 	al->addr = bi[i].to.addr;
+ 	return 1;
+ }
+ 
+ static int
+ iter_add_next_branch_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct branch_info *bi, *bx;
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct hist_entry *he = NULL;
+ 	int i = iter->curr;
+ 	int err = 0;
+ 
+ 	bi = iter->priv;
+ 
+ 	if (iter->hide_unresolved && !(bi[i].from.sym && bi[i].to.sym))
+ 		goto out;
+ 
+ 	/*
+ 	 * The report shows the percentage of total branches captured
+ 	 * and not events sampled. Thus we use a pseudo period of 1.
+ 	 */
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, &bi[i], NULL,
+ 				1, 1, 0, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	if (ui__has_annotation()) {
+ 		bx = he->branch_info;
+ 		err = addr_map_symbol__inc_samples(&bx->from, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 
+ 		err = addr_map_symbol__inc_samples(&bx->to, evsel->idx);
+ 		if (err)
+ 			goto out;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ out:
+ 	iter->he = he;
+ 	iter->curr++;
+ 	return err;
+ }
+ 
+ static int
+ iter_finish_branch_entry(struct hist_entry_iter *iter,
+ 			 struct addr_location *al __maybe_unused)
+ {
+ 	zfree(&iter->priv);
+ 	iter->he = NULL;
+ 
+ 	return iter->curr >= iter->total ? 0 : -1;
+ }
+ 
+ static int
+ iter_prepare_normal_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			  struct addr_location *al __maybe_unused)
+ {
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_normal_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry *he;
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_normal_entry(struct hist_entry_iter *iter, struct addr_location *al)
+ {
+ 	int err;
+ 	struct hist_entry *he = iter->he;
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 
+ 	if (he == NULL)
+ 		return 0;
+ 
+ 	iter->he = NULL;
+ 
+ 	if (ui__has_annotation()) {
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	return hist_entry__append_callchain(he, sample);
+ }
+ 
+ static int
+ iter_prepare_cumulative_entry(struct hist_entry_iter *iter __maybe_unused,
+ 			      struct addr_location *al __maybe_unused)
+ {
+ 	struct hist_entry **he_cache;
+ 
+ 	callchain_cursor_commit(&callchain_cursor);
+ 
+ 	/*
+ 	 * This is for detecting cycles or recursions so that they're
+ 	 * cumulated only one time to prevent entries more than 100%
+ 	 * overhead.
+ 	 */
+ 	he_cache = malloc(sizeof(*he_cache) * (PERF_MAX_STACK_DEPTH + 1));
+ 	if (he_cache == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->priv = he_cache;
+ 	iter->curr = 0;
+ 
+ 	return 0;
+ }
+ 
+ static int
+ iter_add_single_cumulative_entry(struct hist_entry_iter *iter,
+ 				 struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry **he_cache = iter->priv;
+ 	struct hist_entry *he;
+ 	int err = 0;
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, true);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	he_cache[iter->curr++] = he;
+ 
+ 	callchain_append(he->callchain, &callchain_cursor, sample->period);
+ 
+ 	/*
+ 	 * We need to re-initialize the cursor since callchain_append()
+ 	 * advanced the cursor to the end.
+ 	 */
+ 	callchain_cursor_commit(&callchain_cursor);
+ 
+ 	/*
+ 	 * The iter->he will be over-written after ->add_next_entry()
+ 	 * called so inc stats for the original entry now.
+ 	 */
+ 	if (ui__has_annotation())
+ 		err = hist_entry__inc_addr_samples(he, evsel->idx, al->addr);
+ 
+ 	hists__inc_nr_samples(&evsel->hists, he->filtered);
+ 
+ 	return err;
+ }
+ 
+ static int
+ iter_next_cumulative_entry(struct hist_entry_iter *iter,
+ 			   struct addr_location *al)
+ {
+ 	struct callchain_cursor_node *node;
+ 
+ 	node = callchain_cursor_current(&callchain_cursor);
+ 	if (node == NULL)
+ 		return 0;
+ 
+ 	return fill_callchain_info(al, node, iter->hide_unresolved);
+ }
+ 
+ static int
+ iter_add_next_cumulative_entry(struct hist_entry_iter *iter,
+ 			       struct addr_location *al)
+ {
+ 	struct perf_evsel *evsel = iter->evsel;
+ 	struct perf_sample *sample = iter->sample;
+ 	struct hist_entry **he_cache = iter->priv;
+ 	struct hist_entry *he;
+ 	struct hist_entry he_tmp = {
+ 		.cpu = al->cpu,
+ 		.thread = al->thread,
+ 		.comm = thread__comm(al->thread),
+ 		.ip = al->addr,
+ 		.ms = {
+ 			.map = al->map,
+ 			.sym = al->sym,
+ 		},
+ 		.parent = iter->parent,
+ 	};
+ 	int i;
+ 	struct callchain_cursor cursor;
+ 
+ 	callchain_cursor_snapshot(&cursor, &callchain_cursor);
+ 
+ 	callchain_cursor_advance(&callchain_cursor);
+ 
+ 	/*
+ 	 * Check if there's duplicate entries in the callchain.
+ 	 * It's possible that it has cycles or recursive calls.
+ 	 */
+ 	for (i = 0; i < iter->curr; i++) {
+ 		if (hist_entry__cmp(he_cache[i], &he_tmp) == 0)
+ 			return 0;
+ 	}
+ 
+ 	he = __hists__add_entry(&evsel->hists, al, iter->parent, NULL, NULL,
+ 				sample->period, sample->weight,
+ 				sample->transaction, false);
+ 	if (he == NULL)
+ 		return -ENOMEM;
+ 
+ 	iter->he = he;
+ 	he_cache[iter->curr++] = he;
+ 
+ 	callchain_append(he->callchain, &cursor, sample->period);
+ 	return 0;
+ }
+ 
+ static int
+ iter_finish_cumulative_entry(struct hist_entry_iter *iter,
+ 			     struct addr_location *al __maybe_unused)
+ {
+ 	zfree(&iter->priv);
+ 	iter->he = NULL;
+ 
+ 	return 0;
+ }
+ 
+ const struct hist_iter_ops hist_iter_mem = {
+ 	.prepare_entry 		= iter_prepare_mem_entry,
+ 	.add_single_entry 	= iter_add_single_mem_entry,
+ 	.next_entry 		= iter_next_nop_entry,
+ 	.add_next_entry 	= iter_add_next_nop_entry,
+ 	.finish_entry 		= iter_finish_mem_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_branch = {
+ 	.prepare_entry 		= iter_prepare_branch_entry,
+ 	.add_single_entry 	= iter_add_single_branch_entry,
+ 	.next_entry 		= iter_next_branch_entry,
+ 	.add_next_entry 	= iter_add_next_branch_entry,
+ 	.finish_entry 		= iter_finish_branch_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_normal = {
+ 	.prepare_entry 		= iter_prepare_normal_entry,
+ 	.add_single_entry 	= iter_add_single_normal_entry,
+ 	.next_entry 		= iter_next_nop_entry,
+ 	.add_next_entry 	= iter_add_next_nop_entry,
+ 	.finish_entry 		= iter_finish_normal_entry,
+ };
+ 
+ const struct hist_iter_ops hist_iter_cumulative = {
+ 	.prepare_entry 		= iter_prepare_cumulative_entry,
+ 	.add_single_entry 	= iter_add_single_cumulative_entry,
+ 	.next_entry 		= iter_next_cumulative_entry,
+ 	.add_next_entry 	= iter_add_next_cumulative_entry,
+ 	.finish_entry 		= iter_finish_cumulative_entry,
+ };
+ 
+ int hist_entry_iter__add(struct hist_entry_iter *iter, struct addr_location *al,
+ 			 struct perf_evsel *evsel, struct perf_sample *sample,
+ 			 int max_stack_depth)
+ {
+ 	int err, err2;
+ 
+ 	err = sample__resolve_callchain(sample, &iter->parent, evsel, al,
+ 					max_stack_depth);
+ 	if (err)
+ 		return err;
+ 
+ 	iter->evsel = evsel;
+ 	iter->sample = sample;
+ 
+ 	err = iter->ops->prepare_entry(iter, al);
+ 	if (err)
+ 		goto out;
+ 
+ 	err = iter->ops->add_single_entry(iter, al);
+ 	if (err)
+ 		goto out;
+ 
+ 	while (iter->ops->next_entry(iter, al)) {
+ 		err = iter->ops->add_next_entry(iter, al);
+ 		if (err)
+ 			break;
+ 	}
+ 
+ out:
+ 	err2 = iter->ops->finish_entry(iter, al);
+ 	if (!err)
+ 		err = err2;
+ 
+ 	return err;
++>>>>>>> be7f855a3eeb (perf tools: Save callchain info for each cumulative entry)
  }
  
  int64_t
* Unmerged path tools/perf/util/hist.c
