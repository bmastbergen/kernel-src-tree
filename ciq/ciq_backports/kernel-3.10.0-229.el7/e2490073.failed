blk-mq: call blk_mq_start_request from ->queue_rq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Christoph Hellwig <hch@lst.de>
commit e2490073cd7c3d6f6ef6e029a208edd4d38efac4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/e2490073.failed

When we call blk_mq_start_request from the core blk-mq code before calling into
->queue_rq there is a racy window where the timeout handler can hit before we've
fully set up the driver specific part of the command.

Move the call to blk_mq_start_request into the driver so the driver can start
the request only once it is fully set up.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit e2490073cd7c3d6f6ef6e029a208edd4d38efac4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	drivers/block/mtip32xx/mtip32xx.c
#	drivers/scsi/scsi_lib.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index 6fcf1deefe8d,141f2e06803a..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -331,7 -384,7 +331,11 @@@ void blk_mq_complete_request(struct req
  }
  EXPORT_SYMBOL(blk_mq_complete_request);
  
++<<<<<<< HEAD
 +static void blk_mq_start_request(struct request *rq, bool last)
++=======
+ void blk_mq_start_request(struct request *rq)
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  {
  	struct request_queue *q = rq->q;
  
@@@ -355,31 -421,110 +359,38 @@@
  		 */
  		rq->nr_phys_segments++;
  	}
 +
 +	/*
 +	 * Flag the last request in the series so that drivers know when IO
 +	 * should be kicked off, if they don't do it on a per-request basis.
 +	 *
 +	 * Note: the flag isn't the only condition drivers should do kick off.
 +	 * If drive is busy, the last request might not have the bit set.
 +	 */
 +	if (last)
 +		rq->cmd_flags |= REQ_END;
  }
+ EXPORT_SYMBOL(blk_mq_start_request);
  
 -static void __blk_mq_requeue_request(struct request *rq)
 +static void blk_mq_requeue_request(struct request *rq)
  {
  	struct request_queue *q = rq->q;
  
  	trace_block_rq_requeue(q, rq);
- 	clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
  
++<<<<<<< HEAD
 +	rq->cmd_flags &= ~REQ_END;
 +
 +	if (q->dma_drain_size && blk_rq_bytes(rq))
 +		rq->nr_phys_segments--;
++=======
+ 	if (test_and_clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {
+ 		if (q->dma_drain_size && blk_rq_bytes(rq))
+ 			rq->nr_phys_segments--;
+ 	}
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  }
  
 -void blk_mq_requeue_request(struct request *rq)
 -{
 -	__blk_mq_requeue_request(rq);
 -	blk_clear_rq_complete(rq);
 -
 -	BUG_ON(blk_queued_rq(rq));
 -	blk_mq_add_to_requeue_list(rq, true);
 -}
 -EXPORT_SYMBOL(blk_mq_requeue_request);
 -
 -static void blk_mq_requeue_work(struct work_struct *work)
 -{
 -	struct request_queue *q =
 -		container_of(work, struct request_queue, requeue_work);
 -	LIST_HEAD(rq_list);
 -	struct request *rq, *next;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&q->requeue_lock, flags);
 -	list_splice_init(&q->requeue_list, &rq_list);
 -	spin_unlock_irqrestore(&q->requeue_lock, flags);
 -
 -	list_for_each_entry_safe(rq, next, &rq_list, queuelist) {
 -		if (!(rq->cmd_flags & REQ_SOFTBARRIER))
 -			continue;
 -
 -		rq->cmd_flags &= ~REQ_SOFTBARRIER;
 -		list_del_init(&rq->queuelist);
 -		blk_mq_insert_request(rq, true, false, false);
 -	}
 -
 -	while (!list_empty(&rq_list)) {
 -		rq = list_entry(rq_list.next, struct request, queuelist);
 -		list_del_init(&rq->queuelist);
 -		blk_mq_insert_request(rq, false, false, false);
 -	}
 -
 -	/*
 -	 * Use the start variant of queue running here, so that running
 -	 * the requeue work will kick stopped queues.
 -	 */
 -	blk_mq_start_hw_queues(q);
 -}
 -
 -void blk_mq_add_to_requeue_list(struct request *rq, bool at_head)
 -{
 -	struct request_queue *q = rq->q;
 -	unsigned long flags;
 -
 -	/*
 -	 * We abuse this flag that is otherwise used by the I/O scheduler to
 -	 * request head insertation from the workqueue.
 -	 */
 -	BUG_ON(rq->cmd_flags & REQ_SOFTBARRIER);
 -
 -	spin_lock_irqsave(&q->requeue_lock, flags);
 -	if (at_head) {
 -		rq->cmd_flags |= REQ_SOFTBARRIER;
 -		list_add(&rq->queuelist, &q->requeue_list);
 -	} else {
 -		list_add_tail(&rq->queuelist, &q->requeue_list);
 -	}
 -	spin_unlock_irqrestore(&q->requeue_lock, flags);
 -}
 -EXPORT_SYMBOL(blk_mq_add_to_requeue_list);
 -
 -void blk_mq_kick_requeue_list(struct request_queue *q)
 -{
 -	kblockd_schedule_work(&q->requeue_work);
 -}
 -EXPORT_SYMBOL(blk_mq_kick_requeue_list);
 -
 -static inline bool is_flush_request(struct request *rq, unsigned int tag)
 -{
 -	return ((rq->cmd_flags & REQ_FLUSH_SEQ) &&
 -			rq->q->flush_rq->tag == tag);
 -}
 -
 -struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 -{
 -	struct request *rq = tags->rqs[tag];
 -
 -	if (!is_flush_request(rq, tag))
 -		return rq;
 -
 -	return rq->q->flush_rq;
 -}
 -EXPORT_SYMBOL(blk_mq_tag_to_rq);
 -
  struct blk_mq_timeout_data {
  	struct blk_mq_hw_ctx *hctx;
  	unsigned long *next;
@@@ -547,9 -745,7 +558,13 @@@ static void __blk_mq_run_hw_queue(struc
  		rq = list_first_entry(&rq_list, struct request, queuelist);
  		list_del_init(&rq->queuelist);
  
++<<<<<<< HEAD
 +		blk_mq_start_request(rq, list_empty(&rq_list));
 +
 +		ret = q->mq_ops->queue_rq(hctx, rq);
++=======
+ 		ret = q->mq_ops->queue_rq(hctx, rq, list_empty(&rq_list));
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  		switch (ret) {
  		case BLK_MQ_RQ_QUEUE_OK:
  			queued++;
@@@ -824,18 -1066,171 +839,85 @@@ void blk_mq_flush_plug_list(struct blk_
  static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
  {
  	init_request_from_bio(rq, bio);
 -
 -	if (blk_do_io_stat(rq))
 -		blk_account_io_start(rq, 1);
 -}
 -
 -static inline bool hctx_allow_merges(struct blk_mq_hw_ctx *hctx)
 -{
 -	return (hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
 -		!blk_queue_nomerges(hctx->queue);
 +	blk_account_io_start(rq, 1);
  }
  
 -static inline bool blk_mq_merge_queue_io(struct blk_mq_hw_ctx *hctx,
 -					 struct blk_mq_ctx *ctx,
 -					 struct request *rq, struct bio *bio)
 +static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
  {
 -	if (!hctx_allow_merges(hctx)) {
 -		blk_mq_bio_to_request(rq, bio);
 -		spin_lock(&ctx->lock);
 -insert_rq:
 -		__blk_mq_insert_request(hctx, rq, false);
 -		spin_unlock(&ctx->lock);
 -		return false;
 -	} else {
 -		struct request_queue *q = hctx->queue;
 +	struct blk_mq_hw_ctx *hctx;
 +	struct blk_mq_ctx *ctx;
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
 +	int rw = bio_data_dir(bio);
 +	struct request *rq;
++<<<<<<< HEAD
++=======
+ 
 -		spin_lock(&ctx->lock);
 -		if (!blk_mq_attempt_merge(q, ctx, bio)) {
 -			blk_mq_bio_to_request(rq, bio);
 -			goto insert_rq;
 -		}
++	blk_queue_bounce(q, &bio);
+ 
 -		spin_unlock(&ctx->lock);
 -		__blk_mq_free_request(hctx, ctx, rq);
 -		return true;
 -	}
 -}
 -
 -struct blk_map_ctx {
 -	struct blk_mq_hw_ctx *hctx;
 -	struct blk_mq_ctx *ctx;
 -};
 -
 -static struct request *blk_mq_map_request(struct request_queue *q,
 -					  struct bio *bio,
 -					  struct blk_map_ctx *data)
 -{
 -	struct blk_mq_hw_ctx *hctx;
 -	struct blk_mq_ctx *ctx;
 -	struct request *rq;
 -	int rw = bio_data_dir(bio);
 -	struct blk_mq_alloc_data alloc_data;
 -
 -	if (unlikely(blk_mq_queue_enter(q))) {
 -		bio_endio(bio, -EIO);
 -		return NULL;
 -	}
 -
 -	ctx = blk_mq_get_ctx(q);
 -	hctx = q->mq_ops->map_queue(q, ctx->cpu);
 -
 -	if (rw_is_sync(bio->bi_rw))
 -		rw |= REQ_SYNC;
 -
 -	trace_block_getrq(q, bio, rw);
 -	blk_mq_set_alloc_data(&alloc_data, q, GFP_ATOMIC, false, ctx,
 -			hctx);
 -	rq = __blk_mq_alloc_request(&alloc_data, rw);
 -	if (unlikely(!rq)) {
 -		__blk_mq_run_hw_queue(hctx);
 -		blk_mq_put_ctx(ctx);
 -		trace_block_sleeprq(q, bio, rw);
 -
 -		ctx = blk_mq_get_ctx(q);
 -		hctx = q->mq_ops->map_queue(q, ctx->cpu);
 -		blk_mq_set_alloc_data(&alloc_data, q,
 -				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);
 -		rq = __blk_mq_alloc_request(&alloc_data, rw);
 -		ctx = alloc_data.ctx;
 -		hctx = alloc_data.hctx;
 -	}
 -
 -	hctx->queued++;
 -	data->hctx = hctx;
 -	data->ctx = ctx;
 -	return rq;
 -}
 -
 -/*
 - * Multiple hardware queue variant. This will not use per-process plugs,
 - * but will attempt to bypass the hctx queueing if we can go straight to
 - * hardware for SYNC IO.
 - */
 -static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
 -{
 -	const int is_sync = rw_is_sync(bio->bi_rw);
 -	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
 -	struct blk_map_ctx data;
 -	struct request *rq;
 -
 -	blk_queue_bounce(q, &bio);
 -
 -	if (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {
 -		bio_endio(bio, -EIO);
 -		return;
++	if (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {
++		bio_endio(bio, -EIO);
++		return;
+ 	}
+ 
+ 	rq = blk_mq_map_request(q, bio, &data);
+ 	if (unlikely(!rq))
+ 		return;
+ 
+ 	if (unlikely(is_flush_fua)) {
+ 		blk_mq_bio_to_request(rq, bio);
+ 		blk_insert_flush(rq);
+ 		goto run_queue;
+ 	}
+ 
+ 	if (is_sync) {
+ 		int ret;
+ 
+ 		blk_mq_bio_to_request(rq, bio);
+ 
+ 		/*
+ 		 * For OK queue, we are done. For error, kill it. Any other
+ 		 * error (busy), just add it to our list as we previously
+ 		 * would have done
+ 		 */
+ 		ret = q->mq_ops->queue_rq(data.hctx, rq, true);
+ 		if (ret == BLK_MQ_RQ_QUEUE_OK)
+ 			goto done;
+ 		else {
+ 			__blk_mq_requeue_request(rq);
+ 
+ 			if (ret == BLK_MQ_RQ_QUEUE_ERROR) {
+ 				rq->errors = -EIO;
+ 				blk_mq_end_io(rq, rq->errors);
+ 				goto done;
+ 			}
+ 		}
+ 	}
+ 
+ 	if (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {
+ 		/*
+ 		 * For a SYNC request, send it to the hardware immediately. For
+ 		 * an ASYNC request, just ensure that we run it later on. The
+ 		 * latter allows for merging opportunities and more efficient
+ 		 * dispatching.
+ 		 */
+ run_queue:
+ 		blk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);
+ 	}
+ done:
+ 	blk_mq_put_ctx(data.ctx);
+ }
+ 
+ /*
+  * Single hardware queue variant. This will attempt to use any per-process
+  * plug for merging and IO deferral.
+  */
+ static void blk_sq_make_request(struct request_queue *q, struct bio *bio)
+ {
+ 	const int is_sync = rw_is_sync(bio->bi_rw);
+ 	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  	unsigned int use_plug, request_count = 0;
 -	struct blk_map_ctx data;
 -	struct request *rq;
  
  	/*
  	 * If we have multiple hardware queues, just go directly to
diff --cc drivers/block/mtip32xx/mtip32xx.c
index 9912f3d7777e,4042440a0470..000000000000
--- a/drivers/block/mtip32xx/mtip32xx.c
+++ b/drivers/block/mtip32xx/mtip32xx.c
@@@ -4054,75 -3726,122 +4054,143 @@@ static void mtip_make_request(struct re
  		}
  		if (unlikely(test_bit(MTIP_DDF_WRITE_PROTECT_BIT,
  							&dd->dd_flag) &&
 -				rq_data_dir(rq))) {
 -			return -ENODATA;
 +				bio_data_dir(bio))) {
 +			bio_endio(bio, -ENODATA);
 +			return;
 +		}
 +		if (unlikely(test_bit(MTIP_DDF_SEC_LOCK_BIT, &dd->dd_flag))) {
 +			bio_endio(bio, -ENODATA);
 +			return;
 +		}
 +		if (test_bit(MTIP_DDF_REBUILD_FAILED_BIT, &dd->dd_flag)) {
 +			bio_endio(bio, -ENXIO);
 +			return;
  		}
 -		if (unlikely(test_bit(MTIP_DDF_SEC_LOCK_BIT, &dd->dd_flag)))
 -			return -ENODATA;
 -		if (test_bit(MTIP_DDF_REBUILD_FAILED_BIT, &dd->dd_flag))
 -			return -ENXIO;
  	}
  
++<<<<<<< HEAD
 +	if (unlikely(bio->bi_rw & REQ_DISCARD)) {
 +		bio_endio(bio, mtip_send_trim(dd, bio->bi_sector,
 +						bio_sectors(bio)));
++=======
+ 	if (rq->cmd_flags & REQ_DISCARD) {
+ 		int err;
+ 
+ 		err = mtip_send_trim(dd, blk_rq_pos(rq), blk_rq_sectors(rq));
+ 		blk_mq_end_io(rq, err);
+ 		return 0;
+ 	}
+ 
+ 	/* Create the scatter list for this request. */
+ 	nents = blk_rq_map_sg(hctx->queue, rq, cmd->sg);
+ 
+ 	/* Issue the read/write. */
+ 	mtip_hw_submit_io(dd, rq, cmd, nents, hctx);
+ 	return 0;
+ }
+ 
+ static bool mtip_check_unal_depth(struct blk_mq_hw_ctx *hctx,
+ 				  struct request *rq)
+ {
+ 	struct driver_data *dd = hctx->queue->queuedata;
+ 	struct mtip_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 	if (rq_data_dir(rq) == READ || !dd->unal_qdepth)
+ 		return false;
+ 
+ 	/*
+ 	 * If unaligned depth must be limited on this controller, mark it
+ 	 * as unaligned if the IO isn't on a 4k boundary (start of length).
+ 	 */
+ 	if (blk_rq_sectors(rq) <= 64) {
+ 		if ((blk_rq_pos(rq) & 7) || (blk_rq_sectors(rq) & 7))
+ 			cmd->unaligned = 1;
+ 	}
+ 
+ 	if (cmd->unaligned && down_trylock(&dd->port->cmd_slot_unal))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static int mtip_queue_rq(struct blk_mq_hw_ctx *hctx, struct request *rq,
+ 		bool last)
+ {
+ 	int ret;
+ 
+ 	if (unlikely(mtip_check_unal_depth(hctx, rq)))
+ 		return BLK_MQ_RQ_QUEUE_BUSY;
+ 
+ 	blk_mq_start_request(rq);
+ 
+ 	ret = mtip_submit_request(hctx, rq);
+ 	if (likely(!ret))
+ 		return BLK_MQ_RQ_QUEUE_OK;
+ 
+ 	rq->errors = ret;
+ 	return BLK_MQ_RQ_QUEUE_ERROR;
+ }
+ 
+ static void mtip_free_cmd(void *data, struct request *rq,
+ 			  unsigned int hctx_idx, unsigned int request_idx)
+ {
+ 	struct driver_data *dd = data;
+ 	struct mtip_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 	if (!cmd->command)
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  		return;
 +	}
  
 -	dmam_free_coherent(&dd->pdev->dev, CMD_DMA_ALLOC_SZ,
 -				cmd->command, cmd->command_dma);
 -}
 -
 -static int mtip_init_cmd(void *data, struct request *rq, unsigned int hctx_idx,
 -			 unsigned int request_idx, unsigned int numa_node)
 -{
 -	struct driver_data *dd = data;
 -	struct mtip_cmd *cmd = blk_mq_rq_to_pdu(rq);
 -	u32 host_cap_64 = readl(dd->mmio + HOST_CAP) & HOST_CAP_64;
 -
 -	cmd->command = dmam_alloc_coherent(&dd->pdev->dev, CMD_DMA_ALLOC_SZ,
 -			&cmd->command_dma, GFP_KERNEL);
 -	if (!cmd->command)
 -		return -ENOMEM;
 +	if (unlikely(!bio_has_data(bio))) {
 +		blk_queue_flush(queue, 0);
 +		bio_endio(bio, 0);
 +		return;
 +	}
  
 -	memset(cmd->command, 0, CMD_DMA_ALLOC_SZ);
 +	if (bio_data_dir(bio) == WRITE && bio_sectors(bio) <= 64 &&
 +							dd->unal_qdepth) {
 +		if (bio->bi_sector % 8 != 0) /* Unaligned on 4k boundaries */
 +			unaligned = 1;
 +		else if (bio_sectors(bio) % 8 != 0) /* Aligned but not 4k/8k */
 +			unaligned = 1;
 +	}
  
 -	/* Point the command headers at the command tables. */
 -	cmd->command_header = dd->port->command_list +
 -				(sizeof(struct mtip_cmd_hdr) * request_idx);
 -	cmd->command_header_dma = dd->port->command_list_dma +
 -				(sizeof(struct mtip_cmd_hdr) * request_idx);
 +	sg = mtip_hw_get_scatterlist(dd, &tag, unaligned);
 +	if (likely(sg != NULL)) {
 +		blk_queue_bounce(queue, &bio);
  
 -	if (host_cap_64)
 -		cmd->command_header->ctbau = __force_bit2int cpu_to_le32((cmd->command_dma >> 16) >> 16);
 +		if (unlikely((bio)->bi_vcnt > MTIP_MAX_SG)) {
 +			dev_warn(&dd->pdev->dev,
 +				"Maximum number of SGL entries exceeded\n");
 +			bio_io_error(bio);
 +			mtip_hw_release_scatterlist(dd, tag, unaligned);
 +			return;
 +		}
  
 -	cmd->command_header->ctba = __force_bit2int cpu_to_le32(cmd->command_dma & 0xFFFFFFFF);
 +		/* Create the scatter list for this bio. */
 +		bio_for_each_segment(bvec, bio, i) {
 +			sg_set_page(&sg[nents],
 +					bvec->bv_page,
 +					bvec->bv_len,
 +					bvec->bv_offset);
 +			nents++;
 +		}
  
 -	sg_init_table(cmd->sg, MTIP_MAX_SG);
 -	return 0;
 +		/* Issue the read/write. */
 +		mtip_hw_submit_io(dd,
 +				bio->bi_sector,
 +				bio_sectors(bio),
 +				nents,
 +				tag,
 +				bio_endio,
 +				bio,
 +				bio_data_dir(bio),
 +				unaligned);
 +	} else
 +		bio_io_error(bio);
  }
  
 -static struct blk_mq_ops mtip_mq_ops = {
 -	.queue_rq	= mtip_queue_rq,
 -	.map_queue	= blk_mq_map_queue,
 -	.init_request	= mtip_init_cmd,
 -	.exit_request	= mtip_free_cmd,
 -};
 -
  /*
   * Block layer initialization function.
   *
diff --cc drivers/scsi/scsi_lib.c
index 224fcb585fa9,2dcd9078de48..000000000000
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@@ -1624,7 -1778,182 +1624,186 @@@ out_delay
  		blk_delay_queue(q, SCSI_QUEUE_DELAY);
  }
  
++<<<<<<< HEAD
 +u64 scsi_calculate_bounce_limit(struct Scsi_Host *shost)
++=======
+ static inline int prep_to_mq(int ret)
+ {
+ 	switch (ret) {
+ 	case BLKPREP_OK:
+ 		return 0;
+ 	case BLKPREP_DEFER:
+ 		return BLK_MQ_RQ_QUEUE_BUSY;
+ 	default:
+ 		return BLK_MQ_RQ_QUEUE_ERROR;
+ 	}
+ }
+ 
+ static int scsi_mq_prep_fn(struct request *req)
+ {
+ 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ 	struct scsi_device *sdev = req->q->queuedata;
+ 	struct Scsi_Host *shost = sdev->host;
+ 	unsigned char *sense_buf = cmd->sense_buffer;
+ 	struct scatterlist *sg;
+ 
+ 	memset(cmd, 0, sizeof(struct scsi_cmnd));
+ 
+ 	req->special = cmd;
+ 
+ 	cmd->request = req;
+ 	cmd->device = sdev;
+ 	cmd->sense_buffer = sense_buf;
+ 
+ 	cmd->tag = req->tag;
+ 
+ 	cmd->cmnd = req->cmd;
+ 	cmd->prot_op = SCSI_PROT_NORMAL;
+ 
+ 	INIT_LIST_HEAD(&cmd->list);
+ 	INIT_DELAYED_WORK(&cmd->abort_work, scmd_eh_abort_handler);
+ 	cmd->jiffies_at_alloc = jiffies;
+ 
+ 	/*
+ 	 * XXX: cmd_list lookups are only used by two drivers, try to get
+ 	 * rid of this list in common code.
+ 	 */
+ 	spin_lock_irq(&sdev->list_lock);
+ 	list_add_tail(&cmd->list, &sdev->cmd_list);
+ 	spin_unlock_irq(&sdev->list_lock);
+ 
+ 	sg = (void *)cmd + sizeof(struct scsi_cmnd) + shost->hostt->cmd_size;
+ 	cmd->sdb.table.sgl = sg;
+ 
+ 	if (scsi_host_get_prot(shost)) {
+ 		cmd->prot_sdb = (void *)sg +
+ 			shost->sg_tablesize * sizeof(struct scatterlist);
+ 		memset(cmd->prot_sdb, 0, sizeof(struct scsi_data_buffer));
+ 
+ 		cmd->prot_sdb->table.sgl =
+ 			(struct scatterlist *)(cmd->prot_sdb + 1);
+ 	}
+ 
+ 	if (blk_bidi_rq(req)) {
+ 		struct request *next_rq = req->next_rq;
+ 		struct scsi_data_buffer *bidi_sdb = blk_mq_rq_to_pdu(next_rq);
+ 
+ 		memset(bidi_sdb, 0, sizeof(struct scsi_data_buffer));
+ 		bidi_sdb->table.sgl =
+ 			(struct scatterlist *)(bidi_sdb + 1);
+ 
+ 		next_rq->special = bidi_sdb;
+ 	}
+ 
+ 	return scsi_setup_cmnd(sdev, req);
+ }
+ 
+ static void scsi_mq_done(struct scsi_cmnd *cmd)
+ {
+ 	trace_scsi_dispatch_cmd_done(cmd);
+ 	blk_mq_complete_request(cmd->request);
+ }
+ 
+ static int scsi_queue_rq(struct blk_mq_hw_ctx *hctx, struct request *req,
+ 		bool last)
+ {
+ 	struct request_queue *q = req->q;
+ 	struct scsi_device *sdev = q->queuedata;
+ 	struct Scsi_Host *shost = sdev->host;
+ 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ 	int ret;
+ 	int reason;
+ 
+ 	ret = prep_to_mq(scsi_prep_state_check(sdev, req));
+ 	if (ret)
+ 		goto out;
+ 
+ 	ret = BLK_MQ_RQ_QUEUE_BUSY;
+ 	if (!get_device(&sdev->sdev_gendev))
+ 		goto out;
+ 
+ 	if (!scsi_dev_queue_ready(q, sdev))
+ 		goto out_put_device;
+ 	if (!scsi_target_queue_ready(shost, sdev))
+ 		goto out_dec_device_busy;
+ 	if (!scsi_host_queue_ready(q, shost, sdev))
+ 		goto out_dec_target_busy;
+ 
+ 	if (!(req->cmd_flags & REQ_DONTPREP)) {
+ 		ret = prep_to_mq(scsi_mq_prep_fn(req));
+ 		if (ret)
+ 			goto out_dec_host_busy;
+ 		req->cmd_flags |= REQ_DONTPREP;
+ 	}
+ 
+ 	scsi_init_cmd_errh(cmd);
+ 	cmd->scsi_done = scsi_mq_done;
+ 
+ 	blk_mq_start_request(req);
+ 	reason = scsi_dispatch_cmd(cmd);
+ 	if (reason) {
+ 		scsi_set_blocked(cmd, reason);
+ 		ret = BLK_MQ_RQ_QUEUE_BUSY;
+ 		goto out_dec_host_busy;
+ 	}
+ 
+ 	return BLK_MQ_RQ_QUEUE_OK;
+ 
+ out_dec_host_busy:
+ 	atomic_dec(&shost->host_busy);
+ out_dec_target_busy:
+ 	if (scsi_target(sdev)->can_queue > 0)
+ 		atomic_dec(&scsi_target(sdev)->target_busy);
+ out_dec_device_busy:
+ 	atomic_dec(&sdev->device_busy);
+ out_put_device:
+ 	put_device(&sdev->sdev_gendev);
+ out:
+ 	switch (ret) {
+ 	case BLK_MQ_RQ_QUEUE_BUSY:
+ 		blk_mq_stop_hw_queue(hctx);
+ 		if (atomic_read(&sdev->device_busy) == 0 &&
+ 		    !scsi_device_blocked(sdev))
+ 			blk_mq_delay_queue(hctx, SCSI_QUEUE_DELAY);
+ 		break;
+ 	case BLK_MQ_RQ_QUEUE_ERROR:
+ 		/*
+ 		 * Make sure to release all allocated ressources when
+ 		 * we hit an error, as we will never see this command
+ 		 * again.
+ 		 */
+ 		if (req->cmd_flags & REQ_DONTPREP)
+ 			scsi_mq_uninit_cmd(cmd);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int scsi_init_request(void *data, struct request *rq,
+ 		unsigned int hctx_idx, unsigned int request_idx,
+ 		unsigned int numa_node)
+ {
+ 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 	cmd->sense_buffer = kzalloc_node(SCSI_SENSE_BUFFERSIZE, GFP_KERNEL,
+ 			numa_node);
+ 	if (!cmd->sense_buffer)
+ 		return -ENOMEM;
+ 	return 0;
+ }
+ 
+ static void scsi_exit_request(void *data, struct request *rq,
+ 		unsigned int hctx_idx, unsigned int request_idx)
+ {
+ 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 	kfree(cmd->sense_buffer);
+ }
+ 
+ static u64 scsi_calculate_bounce_limit(struct Scsi_Host *shost)
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  {
  	struct device *host_dev;
  	u64 bounce_limit = 0xffffffff;
diff --cc include/linux/blk-mq.h
index 712a6b843fbe,878b6f71da48..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -126,26 -152,20 +126,32 @@@ void blk_mq_insert_request(struct reque
  void blk_mq_run_queues(struct request_queue *q, bool async);
  void blk_mq_free_request(struct request *rq);
  bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 -struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 -		gfp_t gfp, bool reserved);
 -struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 +struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
 +struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
 +struct request *blk_mq_rq_from_tag(struct request_queue *q, unsigned int tag);
  
  struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 -struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 +struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_reg *, unsigned int);
 +void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
  
++<<<<<<< HEAD
 +bool blk_mq_end_io_partial(struct request *rq, int error,
 +		unsigned int nr_bytes);
 +static inline void blk_mq_end_io(struct request *rq, int error)
 +{
 +	bool done = !blk_mq_end_io_partial(rq, error, blk_rq_bytes(rq));
 +	BUG_ON(!done);
 +}
++=======
+ void blk_mq_start_request(struct request *rq);
+ void blk_mq_end_io(struct request *rq, int error);
+ void __blk_mq_end_io(struct request *rq, int error);
++>>>>>>> e2490073cd7c (blk-mq: call blk_mq_start_request from ->queue_rq)
  
 -void blk_mq_requeue_request(struct request *rq);
 -void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
 -void blk_mq_kick_requeue_list(struct request_queue *q);
 +/*
 + * Complete request through potential IPI for right placement. Driver must
 + * have defined a mq_ops->complete() hook for this.
 + */
  void blk_mq_complete_request(struct request *rq);
  
  void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
* Unmerged path block/blk-mq.c
* Unmerged path drivers/block/mtip32xx/mtip32xx.c
diff --git a/drivers/block/null_blk.c b/drivers/block/null_blk.c
index 3ae5f19b54ef..05e79aef37e9 100644
--- a/drivers/block/null_blk.c
+++ b/drivers/block/null_blk.c
@@ -316,6 +316,8 @@ static int null_queue_rq(struct blk_mq_hw_ctx *hctx, struct request *rq)
 	cmd->rq = rq;
 	cmd->nq = hctx->driver_data;
 
+	blk_mq_start_request(rq);
+
 	null_handle_cmd(cmd);
 	return BLK_MQ_RQ_QUEUE_OK;
 }
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 94062c5cfd56..666226889f65 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -191,6 +191,8 @@ static int virtio_queue_rq(struct blk_mq_hw_ctx *hctx, struct request *req)
 		}
 	}
 
+	blk_mq_start_request(req);
+
 	num = blk_rq_map_sg(hctx->queue, vbr->req, vbr->sg);
 	if (num) {
 		if (rq_data_dir(vbr->req) == WRITE)
* Unmerged path drivers/scsi/scsi_lib.c
* Unmerged path include/linux/blk-mq.h
