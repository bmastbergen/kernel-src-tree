blk-mq: add timer in blk_mq_start_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Ming Lei <tom.leiming@gmail.com>
commit 2b8393b43ec672bb263009cd74c056ab01d6ac17
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/2b8393b4.failed

This way will become consistent with non-mq case, also
avoid to update rq->deadline twice for mq.

The comment said: "We do this early, to ensure we are on
the right CPU.", but no percpu stuff is used in blk_add_timer(),
so it isn't necessary. Even when inserting from plug list, there
is no such guarantee at all.

	Signed-off-by: Ming Lei <tom.leiming@gmail.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 2b8393b43ec672bb263009cd74c056ab01d6ac17)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index d5f8ac70935d,e11f5f8e0313..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -356,14 -408,21 +356,29 @@@ static void blk_mq_start_request(struc
  	trace_block_rq_issue(q, rq);
  
  	rq->resid_len = blk_rq_bytes(rq);
 -	if (unlikely(blk_bidi_rq(rq)))
 -		rq->next_rq->resid_len = blk_rq_bytes(rq->next_rq);
  
++<<<<<<< HEAD
 +	/*
 +	 * Just mark start time and set the started bit. Due to memory
 +	 * ordering, we know we'll see the correct deadline as long as
 +	 * REQ_ATOMIC_STARTED is seen.
 +	 */
 +	rq->deadline = jiffies + q->rq_timeout;
 +	set_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
++=======
+ 	blk_add_timer(rq);
+ 
+ 	/*
+ 	 * Mark us as started and clear complete. Complete might have been
+ 	 * set if requeue raced with timeout, which then marked it as
+ 	 * complete. So be sure to clear complete again when we start
+ 	 * the request, otherwise we'll ignore the completion event.
+ 	 */
+ 	if (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags))
+ 		set_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
+ 	if (test_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags))
+ 		clear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);
++>>>>>>> 2b8393b43ec6 (blk-mq: add timer in blk_mq_start_request)
  
  	if (q->dma_drain_size && blk_rq_bytes(rq)) {
  		/*
@@@ -714,12 -961,8 +729,15 @@@ static void __blk_mq_insert_request(str
  		list_add(&rq->queuelist, &ctx->rq_list);
  	else
  		list_add_tail(&rq->queuelist, &ctx->rq_list);
 -
  	blk_mq_hctx_mark_pending(hctx, ctx);
++<<<<<<< HEAD
 +
 +	/*
 +	 * We do this early, to ensure we are on the right CPU.
 +	 */
 +	blk_mq_add_timer(rq);
++=======
++>>>>>>> 2b8393b43ec6 (blk-mq: add timer in blk_mq_start_request)
  }
  
  void blk_mq_insert_request(struct request *rq, bool at_head, bool run_queue,
@@@ -854,11 -1132,124 +872,79 @@@ static void blk_mq_make_request(struct 
  {
  	struct blk_mq_hw_ctx *hctx;
  	struct blk_mq_ctx *ctx;
 -	struct request *rq;
 -	int rw = bio_data_dir(bio);
 -	struct blk_mq_alloc_data alloc_data;
 -
 -	if (unlikely(blk_mq_queue_enter(q))) {
 -		bio_endio(bio, -EIO);
 -		return NULL;
 -	}
 -
 -	ctx = blk_mq_get_ctx(q);
 -	hctx = q->mq_ops->map_queue(q, ctx->cpu);
 -
 -	if (rw_is_sync(bio->bi_rw))
 -		rw |= REQ_SYNC;
 -
 -	trace_block_getrq(q, bio, rw);
 -	blk_mq_set_alloc_data(&alloc_data, q, GFP_ATOMIC, false, ctx,
 -			hctx);
 -	rq = __blk_mq_alloc_request(&alloc_data, rw);
 -	if (unlikely(!rq)) {
 -		__blk_mq_run_hw_queue(hctx);
 -		blk_mq_put_ctx(ctx);
 -		trace_block_sleeprq(q, bio, rw);
 -
 -		ctx = blk_mq_get_ctx(q);
 -		hctx = q->mq_ops->map_queue(q, ctx->cpu);
 -		blk_mq_set_alloc_data(&alloc_data, q,
 -				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);
 -		rq = __blk_mq_alloc_request(&alloc_data, rw);
 -		ctx = alloc_data.ctx;
 -		hctx = alloc_data.hctx;
 -	}
 -
 -	hctx->queued++;
 -	data->hctx = hctx;
 -	data->ctx = ctx;
 -	return rq;
 -}
 -
 -/*
 - * Multiple hardware queue variant. This will not use per-process plugs,
 - * but will attempt to bypass the hctx queueing if we can go straight to
 - * hardware for SYNC IO.
 - */
 -static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
 -{
  	const int is_sync = rw_is_sync(bio->bi_rw);
  	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
 -	struct blk_map_ctx data;
 +	int rw = bio_data_dir(bio);
  	struct request *rq;
++<<<<<<< HEAD
++=======
+ 
+ 	blk_queue_bounce(q, &bio);
+ 
+ 	if (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {
+ 		bio_endio(bio, -EIO);
+ 		return;
+ 	}
+ 
+ 	rq = blk_mq_map_request(q, bio, &data);
+ 	if (unlikely(!rq))
+ 		return;
+ 
+ 	if (unlikely(is_flush_fua)) {
+ 		blk_mq_bio_to_request(rq, bio);
+ 		blk_insert_flush(rq);
+ 		goto run_queue;
+ 	}
+ 
+ 	if (is_sync) {
+ 		int ret;
+ 
+ 		blk_mq_bio_to_request(rq, bio);
+ 		blk_mq_start_request(rq, true);
+ 
+ 		/*
+ 		 * For OK queue, we are done. For error, kill it. Any other
+ 		 * error (busy), just add it to our list as we previously
+ 		 * would have done
+ 		 */
+ 		ret = q->mq_ops->queue_rq(data.hctx, rq);
+ 		if (ret == BLK_MQ_RQ_QUEUE_OK)
+ 			goto done;
+ 		else {
+ 			__blk_mq_requeue_request(rq);
+ 
+ 			if (ret == BLK_MQ_RQ_QUEUE_ERROR) {
+ 				rq->errors = -EIO;
+ 				blk_mq_end_io(rq, rq->errors);
+ 				goto done;
+ 			}
+ 		}
+ 	}
+ 
+ 	if (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {
+ 		/*
+ 		 * For a SYNC request, send it to the hardware immediately. For
+ 		 * an ASYNC request, just ensure that we run it later on. The
+ 		 * latter allows for merging opportunities and more efficient
+ 		 * dispatching.
+ 		 */
+ run_queue:
+ 		blk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);
+ 	}
+ done:
+ 	blk_mq_put_ctx(data.ctx);
+ }
+ 
+ /*
+  * Single hardware queue variant. This will attempt to use any per-process
+  * plug for merging and IO deferral.
+  */
+ static void blk_sq_make_request(struct request_queue *q, struct bio *bio)
+ {
+ 	const int is_sync = rw_is_sync(bio->bi_rw);
+ 	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
++>>>>>>> 2b8393b43ec6 (blk-mq: add timer in blk_mq_start_request)
  	unsigned int use_plug, request_count = 0;
 -	struct blk_map_ctx data;
 -	struct request *rq;
  
  	/*
  	 * If we have multiple hardware queues, just go directly to
* Unmerged path block/blk-mq.c
