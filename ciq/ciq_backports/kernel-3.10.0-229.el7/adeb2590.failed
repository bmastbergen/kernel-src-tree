iommu/vt-d: fix memory leakage caused by commit ea8ea46

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [iommu] vt-d: fix memory leakage caused by commit ea8ea46 (Myron Stowe) [1129880 1087643]
Rebuild_FUZZ: 94.23%
commit-author Jiang Liu <jiang.liu@linux.intel.com>
commit adeb25905c644350baf1f446bcd856517e58060e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/adeb2590.failed

Commit ea8ea46 "iommu/vt-d: Clean up and fix page table clear/free
behaviour" introduces possible leakage of DMA page tables due to:
        for (pte = page_address(pg); !first_pte_in_page(pte); pte++) {
                if (dma_pte_present(pte) && !dma_pte_superpage(pte))
                        freelist = dma_pte_list_pagetables(domain, level - 1,
                                                           pte, freelist);
        }

For the first pte in a page, first_pte_in_page(pte) will always be true,
thus dma_pte_list_pagetables() will never be called and leak DMA page
tables if level is bigger than 1.

	Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
	Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
(cherry picked from commit adeb25905c644350baf1f446bcd856517e58060e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index 3869cde5d60a,13dc2318e17a..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -988,6 -990,125 +988,128 @@@ static void dma_pte_free_pagetable(stru
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /* When a page at a given level is being unlinked from its parent, we don't
+    need to *modify* it at all. All we need to do is make a list of all the
+    pages which can be freed just as soon as we've flushed the IOTLB and we
+    know the hardware page-walk will no longer touch them.
+    The 'pte' argument is the *parent* PTE, pointing to the page that is to
+    be freed. */
+ static struct page *dma_pte_list_pagetables(struct dmar_domain *domain,
+ 					    int level, struct dma_pte *pte,
+ 					    struct page *freelist)
+ {
+ 	struct page *pg;
+ 
+ 	pg = pfn_to_page(dma_pte_addr(pte) >> PAGE_SHIFT);
+ 	pg->freelist = freelist;
+ 	freelist = pg;
+ 
+ 	if (level == 1)
+ 		return freelist;
+ 
+ 	pte = page_address(pg);
+ 	do {
+ 		if (dma_pte_present(pte) && !dma_pte_superpage(pte))
+ 			freelist = dma_pte_list_pagetables(domain, level - 1,
+ 							   pte, freelist);
+ 		pte++;
+ 	} while (!first_pte_in_page(pte));
+ 
+ 	return freelist;
+ }
+ 
+ static struct page *dma_pte_clear_level(struct dmar_domain *domain, int level,
+ 					struct dma_pte *pte, unsigned long pfn,
+ 					unsigned long start_pfn,
+ 					unsigned long last_pfn,
+ 					struct page *freelist)
+ {
+ 	struct dma_pte *first_pte = NULL, *last_pte = NULL;
+ 
+ 	pfn = max(start_pfn, pfn);
+ 	pte = &pte[pfn_level_offset(pfn, level)];
+ 
+ 	do {
+ 		unsigned long level_pfn;
+ 
+ 		if (!dma_pte_present(pte))
+ 			goto next;
+ 
+ 		level_pfn = pfn & level_mask(level);
+ 
+ 		/* If range covers entire pagetable, free it */
+ 		if (start_pfn <= level_pfn &&
+ 		    last_pfn >= level_pfn + level_size(level) - 1) {
+ 			/* These suborbinate page tables are going away entirely. Don't
+ 			   bother to clear them; we're just going to *free* them. */
+ 			if (level > 1 && !dma_pte_superpage(pte))
+ 				freelist = dma_pte_list_pagetables(domain, level - 1, pte, freelist);
+ 
+ 			dma_clear_pte(pte);
+ 			if (!first_pte)
+ 				first_pte = pte;
+ 			last_pte = pte;
+ 		} else if (level > 1) {
+ 			/* Recurse down into a level that isn't *entirely* obsolete */
+ 			freelist = dma_pte_clear_level(domain, level - 1,
+ 						       phys_to_virt(dma_pte_addr(pte)),
+ 						       level_pfn, start_pfn, last_pfn,
+ 						       freelist);
+ 		}
+ next:
+ 		pfn += level_size(level);
+ 	} while (!first_pte_in_page(++pte) && pfn <= last_pfn);
+ 
+ 	if (first_pte)
+ 		domain_flush_cache(domain, first_pte,
+ 				   (void *)++last_pte - (void *)first_pte);
+ 
+ 	return freelist;
+ }
+ 
+ /* We can't just free the pages because the IOMMU may still be walking
+    the page tables, and may have cached the intermediate levels. The
+    pages can only be freed after the IOTLB flush has been done. */
+ struct page *domain_unmap(struct dmar_domain *domain,
+ 			  unsigned long start_pfn,
+ 			  unsigned long last_pfn)
+ {
+ 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
+ 	struct page *freelist = NULL;
+ 
+ 	BUG_ON(addr_width < BITS_PER_LONG && start_pfn >> addr_width);
+ 	BUG_ON(addr_width < BITS_PER_LONG && last_pfn >> addr_width);
+ 	BUG_ON(start_pfn > last_pfn);
+ 
+ 	/* we don't need lock here; nobody else touches the iova range */
+ 	freelist = dma_pte_clear_level(domain, agaw_to_level(domain->agaw),
+ 				       domain->pgd, 0, start_pfn, last_pfn, NULL);
+ 
+ 	/* free pgd */
+ 	if (start_pfn == 0 && last_pfn == DOMAIN_MAX_PFN(domain->gaw)) {
+ 		struct page *pgd_page = virt_to_page(domain->pgd);
+ 		pgd_page->freelist = freelist;
+ 		freelist = pgd_page;
+ 
+ 		domain->pgd = NULL;
+ 	}
+ 
+ 	return freelist;
+ }
+ 
+ void dma_free_pagelist(struct page *freelist)
+ {
+ 	struct page *pg;
+ 
+ 	while ((pg = freelist)) {
+ 		freelist = pg->freelist;
+ 		free_pgtable_page(page_address(pg));
+ 	}
+ }
+ 
++>>>>>>> adeb25905c64 (iommu/vt-d: fix memory leakage caused by commit ea8ea46)
  /* iommu handling */
  static int iommu_alloc_root_entry(struct intel_iommu *iommu)
  {
* Unmerged path drivers/iommu/intel-iommu.c
