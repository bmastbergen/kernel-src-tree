dm thin: sort the deferred cells

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [md] dm-thin: sort the deferred cells (Mike Snitzer) [1156164]
Rebuild_FUZZ: 96.88%
commit-author Joe Thornber <ejt@redhat.com>
commit ac4c3f34a9af63092b3fbfafe34c3e966fbd96c5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/ac4c3f34.failed

Sort the cells in logical block order before processing each cell in
process_thin_deferred_cells().  This significantly improves the ondisk
layout on rotational storage, whereby improving read performance.

	Signed-off-by: Joe Thornber <ejt@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit ac4c3f34a9af63092b3fbfafe34c3e966fbd96c5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-thin.c
diff --cc drivers/md/dm-thin.c
index 13e9a0386d73,b9d25026ab84..000000000000
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@@ -202,8 -203,11 +203,10 @@@ struct pool_features 
  
  struct thin_c;
  typedef void (*process_bio_fn)(struct thin_c *tc, struct bio *bio);
 -typedef void (*process_cell_fn)(struct thin_c *tc, struct dm_bio_prison_cell *cell);
  typedef void (*process_mapping_fn)(struct dm_thin_new_mapping *m);
  
+ #define CELL_SORT_ARRAY_SIZE 8192
+ 
  struct pool {
  	struct list_head list;
  	struct dm_target *ti;	/* Only set if a pool target is bound */
@@@ -246,8 -250,13 +249,10 @@@
  	process_bio_fn process_bio;
  	process_bio_fn process_discard;
  
 -	process_cell_fn process_cell;
 -	process_cell_fn process_discard_cell;
 -
  	process_mapping_fn process_prepared_mapping;
  	process_mapping_fn process_prepared_discard;
+ 
+ 	struct dm_bio_prison_cell *cell_sort_array[CELL_SORT_ARRAY_SIZE];
  };
  
  static enum pool_mode get_pool_mode(struct pool *pool);
@@@ -1618,6 -1805,88 +1623,91 @@@ static void process_thin_deferred_bios(
  	blk_finish_plug(&plug);
  }
  
++<<<<<<< HEAD
++=======
+ static int cmp_cells(const void *lhs, const void *rhs)
+ {
+ 	struct dm_bio_prison_cell *lhs_cell = *((struct dm_bio_prison_cell **) lhs);
+ 	struct dm_bio_prison_cell *rhs_cell = *((struct dm_bio_prison_cell **) rhs);
+ 
+ 	BUG_ON(!lhs_cell->holder);
+ 	BUG_ON(!rhs_cell->holder);
+ 
+ 	if (lhs_cell->holder->bi_iter.bi_sector < rhs_cell->holder->bi_iter.bi_sector)
+ 		return -1;
+ 
+ 	if (lhs_cell->holder->bi_iter.bi_sector > rhs_cell->holder->bi_iter.bi_sector)
+ 		return 1;
+ 
+ 	return 0;
+ }
+ 
+ static unsigned sort_cells(struct pool *pool, struct list_head *cells)
+ {
+ 	unsigned count = 0;
+ 	struct dm_bio_prison_cell *cell, *tmp;
+ 
+ 	list_for_each_entry_safe(cell, tmp, cells, user_list) {
+ 		if (count >= CELL_SORT_ARRAY_SIZE)
+ 			break;
+ 
+ 		pool->cell_sort_array[count++] = cell;
+ 		list_del(&cell->user_list);
+ 	}
+ 
+ 	sort(pool->cell_sort_array, count, sizeof(cell), cmp_cells, NULL);
+ 
+ 	return count;
+ }
+ 
+ static void process_thin_deferred_cells(struct thin_c *tc)
+ {
+ 	struct pool *pool = tc->pool;
+ 	unsigned long flags;
+ 	struct list_head cells;
+ 	struct dm_bio_prison_cell *cell;
+ 	unsigned i, j, count;
+ 
+ 	INIT_LIST_HEAD(&cells);
+ 
+ 	spin_lock_irqsave(&tc->lock, flags);
+ 	list_splice_init(&tc->deferred_cells, &cells);
+ 	spin_unlock_irqrestore(&tc->lock, flags);
+ 
+ 	if (list_empty(&cells))
+ 		return;
+ 
+ 	do {
+ 		count = sort_cells(tc->pool, &cells);
+ 
+ 		for (i = 0; i < count; i++) {
+ 			cell = pool->cell_sort_array[i];
+ 			BUG_ON(!cell->holder);
+ 
+ 			/*
+ 			 * If we've got no free new_mapping structs, and processing
+ 			 * this bio might require one, we pause until there are some
+ 			 * prepared mappings to process.
+ 			 */
+ 			if (ensure_next_mapping(pool)) {
+ 				for (j = i; j < count; j++)
+ 					list_add(&pool->cell_sort_array[j]->user_list, &cells);
+ 
+ 				spin_lock_irqsave(&tc->lock, flags);
+ 				list_splice(&cells, &tc->deferred_cells);
+ 				spin_unlock_irqrestore(&tc->lock, flags);
+ 				return;
+ 			}
+ 
+ 			if (cell->holder->bi_rw & REQ_DISCARD)
+ 				pool->process_discard_cell(tc, cell);
+ 			else
+ 				pool->process_cell(tc, cell);
+ 		}
+ 	} while (!list_empty(&cells));
+ }
+ 
++>>>>>>> ac4c3f34a9af (dm thin: sort the deferred cells)
  static void thin_get(struct thin_c *tc);
  static void thin_put(struct thin_c *tc);
  
* Unmerged path drivers/md/dm-thin.c
