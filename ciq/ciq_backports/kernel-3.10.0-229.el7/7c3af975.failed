nfs: don't sleep with inode lock in lock_and_join_requests

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Weston Andros Adamson <dros@primarydata.com>
commit 7c3af975257383ece54b83c0505d3e0656cb7daf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/7c3af975.failed

This handles the 'nonblock=false' case in nfs_lock_and_join_requests.
If the group is already locked and blocking is allowed, drop the inode lock
and wait for the group lock to be cleared before trying it all again.
This should fix warnings found in peterz's tree (sched/wait branch), where
might_sleep() checks are added to wait.[ch].

	Reported-by: Fengguang Wu <fengguang.wu@intel.com>
	Signed-off-by: Weston Andros Adamson <dros@primarydata.com>
	Reviewed-by: Peng Tao <tao.peng@primarydata.com>
	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit 7c3af975257383ece54b83c0505d3e0656cb7daf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/nfs/pagelist.c
#	fs/nfs/write.c
#	include/linux/nfs_page.h
diff --cc fs/nfs/pagelist.c
index 3e37cab99ceb,4ec67f8d70aa..000000000000
--- a/fs/nfs/pagelist.c
+++ b/fs/nfs/pagelist.c
@@@ -133,6 -136,187 +133,190 @@@ nfs_iocounter_wait(struct nfs_io_counte
  	return __nfs_iocounter_wait(c);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * nfs_page_group_lock - lock the head of the page group
+  * @req - request in group that is to be locked
+  * @nonblock - if true don't block waiting for lock
+  *
+  * this lock must be held if modifying the page group list
+  *
+  * return 0 on success, < 0 on error: -EDELAY if nonblocking or the
+  * result from wait_on_bit_lock
+  *
+  * NOTE: calling with nonblock=false should always have set the
+  *       lock bit (see fs/buffer.c and other uses of wait_on_bit_lock
+  *       with TASK_UNINTERRUPTIBLE), so there is no need to check the result.
+  */
+ int
+ nfs_page_group_lock(struct nfs_page *req, bool nonblock)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	if (!test_and_set_bit(PG_HEADLOCK, &head->wb_flags))
+ 		return 0;
+ 
+ 	if (!nonblock)
+ 		return wait_on_bit_lock(&head->wb_flags, PG_HEADLOCK,
+ 				TASK_UNINTERRUPTIBLE);
+ 
+ 	return -EAGAIN;
+ }
+ 
+ /*
+  * nfs_page_group_lock_wait - wait for the lock to clear, but don't grab it
+  * @req - a request in the group
+  *
+  * This is a blocking call to wait for the group lock to be cleared.
+  */
+ void
+ nfs_page_group_lock_wait(struct nfs_page *req)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	wait_on_bit(&head->wb_flags, PG_HEADLOCK,
+ 		TASK_UNINTERRUPTIBLE);
+ }
+ 
+ /*
+  * nfs_page_group_unlock - unlock the head of the page group
+  * @req - request in group that is to be unlocked
+  */
+ void
+ nfs_page_group_unlock(struct nfs_page *req)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	smp_mb__before_atomic();
+ 	clear_bit(PG_HEADLOCK, &head->wb_flags);
+ 	smp_mb__after_atomic();
+ 	wake_up_bit(&head->wb_flags, PG_HEADLOCK);
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit_locked
+  *
+  * must be called with page group lock held
+  */
+ static bool
+ nfs_page_group_sync_on_bit_locked(struct nfs_page *req, unsigned int bit)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 	struct nfs_page *tmp;
+ 
+ 	WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &head->wb_flags));
+ 	WARN_ON_ONCE(test_and_set_bit(bit, &req->wb_flags));
+ 
+ 	tmp = req->wb_this_page;
+ 	while (tmp != req) {
+ 		if (!test_bit(bit, &tmp->wb_flags))
+ 			return false;
+ 		tmp = tmp->wb_this_page;
+ 	}
+ 
+ 	/* true! reset all bits */
+ 	tmp = req;
+ 	do {
+ 		clear_bit(bit, &tmp->wb_flags);
+ 		tmp = tmp->wb_this_page;
+ 	} while (tmp != req);
+ 
+ 	return true;
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit - set bit on current request, but only
+  *   return true if the bit is set for all requests in page group
+  * @req - request in page group
+  * @bit - PG_* bit that is used to sync page group
+  */
+ bool nfs_page_group_sync_on_bit(struct nfs_page *req, unsigned int bit)
+ {
+ 	bool ret;
+ 
+ 	nfs_page_group_lock(req, false);
+ 	ret = nfs_page_group_sync_on_bit_locked(req, bit);
+ 	nfs_page_group_unlock(req);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * nfs_page_group_init - Initialize the page group linkage for @req
+  * @req - a new nfs request
+  * @prev - the previous request in page group, or NULL if @req is the first
+  *         or only request in the group (the head).
+  */
+ static inline void
+ nfs_page_group_init(struct nfs_page *req, struct nfs_page *prev)
+ {
+ 	WARN_ON_ONCE(prev == req);
+ 
+ 	if (!prev) {
+ 		/* a head request */
+ 		req->wb_head = req;
+ 		req->wb_this_page = req;
+ 	} else {
+ 		/* a subrequest */
+ 		WARN_ON_ONCE(prev->wb_this_page != prev->wb_head);
+ 		WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &prev->wb_head->wb_flags));
+ 		req->wb_head = prev->wb_head;
+ 		req->wb_this_page = prev->wb_this_page;
+ 		prev->wb_this_page = req;
+ 
+ 		/* All subrequests take a ref on the head request until
+ 		 * nfs_page_group_destroy is called */
+ 		kref_get(&req->wb_head->wb_kref);
+ 
+ 		/* grab extra ref if head request has extra ref from
+ 		 * the write/commit path to handle handoff between write
+ 		 * and commit lists */
+ 		if (test_bit(PG_INODE_REF, &prev->wb_head->wb_flags)) {
+ 			set_bit(PG_INODE_REF, &req->wb_flags);
+ 			kref_get(&req->wb_kref);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * nfs_page_group_destroy - sync the destruction of page groups
+  * @req - request that no longer needs the page group
+  *
+  * releases the page group reference from each member once all
+  * members have called this function.
+  */
+ static void
+ nfs_page_group_destroy(struct kref *kref)
+ {
+ 	struct nfs_page *req = container_of(kref, struct nfs_page, wb_kref);
+ 	struct nfs_page *tmp, *next;
+ 
+ 	/* subrequests must release the ref on the head request */
+ 	if (req->wb_head != req)
+ 		nfs_release_request(req->wb_head);
+ 
+ 	if (!nfs_page_group_sync_on_bit(req, PG_TEARDOWN))
+ 		return;
+ 
+ 	tmp = req;
+ 	do {
+ 		next = tmp->wb_this_page;
+ 		/* unlink and free */
+ 		tmp->wb_this_page = tmp;
+ 		tmp->wb_head = tmp;
+ 		nfs_free_request(tmp);
+ 		tmp = next;
+ 	} while (tmp != req);
+ }
+ 
++>>>>>>> 7c3af9752573 (nfs: don't sleep with inode lock in lock_and_join_requests)
  /**
   * nfs_create_request - Create an NFS read/write request.
   * @ctx: open context to use
diff --cc fs/nfs/write.c
index a89cdd72f163,175d5d073ccf..000000000000
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@@ -343,36 -315,255 +343,74 @@@ static void nfs_end_page_writeback(stru
  		clear_bdi_congested(&nfss->backing_dev_info, BLK_RW_ASYNC);
  }
  
 -
 -/* nfs_page_group_clear_bits
 - *   @req - an nfs request
 - * clears all page group related bits from @req
 - */
 -static void
 -nfs_page_group_clear_bits(struct nfs_page *req)
 -{
 -	clear_bit(PG_TEARDOWN, &req->wb_flags);
 -	clear_bit(PG_UNLOCKPAGE, &req->wb_flags);
 -	clear_bit(PG_UPTODATE, &req->wb_flags);
 -	clear_bit(PG_WB_END, &req->wb_flags);
 -	clear_bit(PG_REMOVE, &req->wb_flags);
 -}
 -
 -
 -/*
 - * nfs_unroll_locks_and_wait -  unlock all newly locked reqs and wait on @req
 - *
 - * this is a helper function for nfs_lock_and_join_requests
 - *
 - * @inode - inode associated with request page group, must be holding inode lock
 - * @head  - head request of page group, must be holding head lock
 - * @req   - request that couldn't lock and needs to wait on the req bit lock
 - * @nonblock - if true, don't actually wait
 - *
 - * NOTE: this must be called holding page_group bit lock and inode spin lock
 - *       and BOTH will be released before returning.
 - *
 - * returns 0 on success, < 0 on error.
 - */
 -static int
 -nfs_unroll_locks_and_wait(struct inode *inode, struct nfs_page *head,
 -			  struct nfs_page *req, bool nonblock)
 -	__releases(&inode->i_lock)
 -{
 -	struct nfs_page *tmp;
 -	int ret;
 -
 -	/* relinquish all the locks successfully grabbed this run */
 -	for (tmp = head ; tmp != req; tmp = tmp->wb_this_page)
 -		nfs_unlock_request(tmp);
 -
 -	WARN_ON_ONCE(test_bit(PG_TEARDOWN, &req->wb_flags));
 -
 -	/* grab a ref on the request that will be waited on */
 -	kref_get(&req->wb_kref);
 -
 -	nfs_page_group_unlock(head);
 -	spin_unlock(&inode->i_lock);
 -
 -	/* release ref from nfs_page_find_head_request_locked */
 -	nfs_release_request(head);
 -
 -	if (!nonblock)
 -		ret = nfs_wait_on_request(req);
 -	else
 -		ret = -EAGAIN;
 -	nfs_release_request(req);
 -
 -	return ret;
 -}
 -
 -/*
 - * nfs_destroy_unlinked_subrequests - destroy recently unlinked subrequests
 - *
 - * @destroy_list - request list (using wb_this_page) terminated by @old_head
 - * @old_head - the old head of the list
 - *
 - * All subrequests must be locked and removed from all lists, so at this point
 - * they are only "active" in this function, and possibly in nfs_wait_on_request
 - * with a reference held by some other context.
 - */
 -static void
 -nfs_destroy_unlinked_subrequests(struct nfs_page *destroy_list,
 -				 struct nfs_page *old_head)
 -{
 -	while (destroy_list) {
 -		struct nfs_page *subreq = destroy_list;
 -
 -		destroy_list = (subreq->wb_this_page == old_head) ?
 -				   NULL : subreq->wb_this_page;
 -
 -		WARN_ON_ONCE(old_head != subreq->wb_head);
 -
 -		/* make sure old group is not used */
 -		subreq->wb_head = subreq;
 -		subreq->wb_this_page = subreq;
 -
 -		/* subreq is now totally disconnected from page group or any
 -		 * write / commit lists. last chance to wake any waiters */
 -		nfs_unlock_request(subreq);
 -
 -		if (!test_bit(PG_TEARDOWN, &subreq->wb_flags)) {
 -			/* release ref on old head request */
 -			nfs_release_request(old_head);
 -
 -			nfs_page_group_clear_bits(subreq);
 -
 -			/* release the PG_INODE_REF reference */
 -			if (test_and_clear_bit(PG_INODE_REF, &subreq->wb_flags))
 -				nfs_release_request(subreq);
 -			else
 -				WARN_ON_ONCE(1);
 -		} else {
 -			WARN_ON_ONCE(test_bit(PG_CLEAN, &subreq->wb_flags));
 -			/* zombie requests have already released the last
 -			 * reference and were waiting on the rest of the
 -			 * group to complete. Since it's no longer part of a
 -			 * group, simply free the request */
 -			nfs_page_group_clear_bits(subreq);
 -			nfs_free_request(subreq);
 -		}
 -	}
 -}
 -
 -/*
 - * nfs_lock_and_join_requests - join all subreqs to the head req and return
 - *                              a locked reference, cancelling any pending
 - *                              operations for this page.
 - *
 - * @page - the page used to lookup the "page group" of nfs_page structures
 - * @nonblock - if true, don't block waiting for request locks
 - *
 - * This function joins all sub requests to the head request by first
 - * locking all requests in the group, cancelling any pending operations
 - * and finally updating the head request to cover the whole range covered by
 - * the (former) group.  All subrequests are removed from any write or commit
 - * lists, unlinked from the group and destroyed.
 - *
 - * Returns a locked, referenced pointer to the head request - which after
 - * this call is guaranteed to be the only request associated with the page.
 - * Returns NULL if no requests are found for @page, or a ERR_PTR if an
 - * error was encountered.
 - */
 -static struct nfs_page *
 -nfs_lock_and_join_requests(struct page *page, bool nonblock)
 +static struct nfs_page *nfs_find_and_lock_request(struct page *page, bool nonblock)
  {
  	struct inode *inode = page_file_mapping(page)->host;
 -	struct nfs_page *head, *subreq;
 -	struct nfs_page *destroy_list = NULL;
 -	unsigned int total_bytes;
 +	struct nfs_page *req;
  	int ret;
  
 -try_again:
 -	total_bytes = 0;
 -
 -	WARN_ON_ONCE(destroy_list);
 -
  	spin_lock(&inode->i_lock);
++<<<<<<< HEAD
 +	for (;;) {
 +		req = nfs_page_find_head_request_locked(NFS_I(inode), page);
 +		if (req == NULL)
 +			break;
 +		if (nfs_lock_request(req))
 +			break;
 +		/* Note: If we hold the page lock, as is the case in nfs_writepage,
 +		 *	 then the call to nfs_lock_request() will always
 +		 *	 succeed provided that someone hasn't already marked the
 +		 *	 request as dirty (in which case we don't care).
++=======
+ 
+ 	/*
+ 	 * A reference is taken only on the head request which acts as a
+ 	 * reference to the whole page group - the group will not be destroyed
+ 	 * until the head reference is released.
+ 	 */
+ 	head = nfs_page_find_head_request_locked(NFS_I(inode), page);
+ 
+ 	if (!head) {
+ 		spin_unlock(&inode->i_lock);
+ 		return NULL;
+ 	}
+ 
+ 	/* holding inode lock, so always make a non-blocking call to try the
+ 	 * page group lock */
+ 	ret = nfs_page_group_lock(head, true);
+ 	if (ret < 0) {
+ 		spin_unlock(&inode->i_lock);
+ 
+ 		if (!nonblock && ret == -EAGAIN) {
+ 			nfs_page_group_lock_wait(head);
+ 			nfs_release_request(head);
+ 			goto try_again;
+ 		}
+ 
+ 		nfs_release_request(head);
+ 		return ERR_PTR(ret);
+ 	}
+ 
+ 	/* lock each request in the page group */
+ 	subreq = head;
+ 	do {
+ 		/*
+ 		 * Subrequests are always contiguous, non overlapping
+ 		 * and in order. If not, it's a programming error.
++>>>>>>> 7c3af9752573 (nfs: don't sleep with inode lock in lock_and_join_requests)
  		 */
 -		WARN_ON_ONCE(subreq->wb_offset !=
 -		     (head->wb_offset + total_bytes));
 -
 -		/* keep track of how many bytes this group covers */
 -		total_bytes += subreq->wb_bytes;
 -
 -		if (!nfs_lock_request(subreq)) {
 -			/* releases page group bit lock and
 -			 * inode spin lock and all references */
 -			ret = nfs_unroll_locks_and_wait(inode, head,
 -				subreq, nonblock);
 -
 -			if (ret == 0)
 -				goto try_again;
 -
 +		spin_unlock(&inode->i_lock);
 +		if (!nonblock)
 +			ret = nfs_wait_on_request(req);
 +		else
 +			ret = -EAGAIN;
 +		nfs_release_request(req);
 +		if (ret != 0)
  			return ERR_PTR(ret);
 -		}
 -
 -		subreq = subreq->wb_this_page;
 -	} while (subreq != head);
 -
 -	/* Now that all requests are locked, make sure they aren't on any list.
 -	 * Commit list removal accounting is done after locks are dropped */
 -	subreq = head;
 -	do {
 -		nfs_clear_request_commit(subreq);
 -		subreq = subreq->wb_this_page;
 -	} while (subreq != head);
 -
 -	/* unlink subrequests from head, destroy them later */
 -	if (head->wb_this_page != head) {
 -		/* destroy list will be terminated by head */
 -		destroy_list = head->wb_this_page;
 -		head->wb_this_page = head;
 -
 -		/* change head request to cover whole range that
 -		 * the former page group covered */
 -		head->wb_bytes = total_bytes;
 +		spin_lock(&inode->i_lock);
  	}
 -
 -	/*
 -	 * prepare head request to be added to new pgio descriptor
 -	 */
 -	nfs_page_group_clear_bits(head);
 -
 -	/*
 -	 * some part of the group was still on the inode list - otherwise
 -	 * the group wouldn't be involved in async write.
 -	 * grab a reference for the head request, iff it needs one.
 -	 */
 -	if (!test_and_set_bit(PG_INODE_REF, &head->wb_flags))
 -		kref_get(&head->wb_kref);
 -
 -	nfs_page_group_unlock(head);
 -
 -	/* drop lock to clean uprequests on destroy list */
  	spin_unlock(&inode->i_lock);
 -
 -	nfs_destroy_unlinked_subrequests(destroy_list, head);
 -
 -	/* still holds ref on head from nfs_page_find_head_request_locked
 -	 * and still has lock on head from lock loop */
 -	return head;
 +	return req;
  }
  
  /*
diff --cc include/linux/nfs_page.h
index 214e09851870,6c3e06ee2fb7..000000000000
--- a/include/linux/nfs_page.h
+++ b/include/linux/nfs_page.h
@@@ -95,7 -121,11 +95,15 @@@ extern size_t nfs_generic_pg_test(struc
  				struct nfs_page *req);
  extern  int nfs_wait_on_request(struct nfs_page *);
  extern	void nfs_unlock_request(struct nfs_page *req);
++<<<<<<< HEAD
 +extern	void nfs_unlock_and_release_request(struct nfs_page *req);
++=======
+ extern	void nfs_unlock_and_release_request(struct nfs_page *);
+ extern int nfs_page_group_lock(struct nfs_page *, bool);
+ extern void nfs_page_group_lock_wait(struct nfs_page *);
+ extern void nfs_page_group_unlock(struct nfs_page *);
+ extern bool nfs_page_group_sync_on_bit(struct nfs_page *, unsigned int);
++>>>>>>> 7c3af9752573 (nfs: don't sleep with inode lock in lock_and_join_requests)
  
  /*
   * Lock the page of an asynchronous request
* Unmerged path fs/nfs/pagelist.c
* Unmerged path fs/nfs/write.c
* Unmerged path include/linux/nfs_page.h
