arch: Mass conversion of smp_mb__*()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [ethernet] i40e: Mass conversion of smp_mb__*() (Stefan Assmann) [1091126]
Rebuild_FUZZ: 88.89%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 4e857c58efeb99393cba5a5d0d8ec7117183137c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/4e857c58.failed

Mostly scripted conversion of the smp_mb__* barriers.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Link: http://lkml.kernel.org/n/tip-55dhyhocezdw1dg7u19hmh1u@git.kernel.org
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: linux-arch@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4e857c58efeb99393cba5a5d0d8ec7117183137c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/drm_irq.c
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
#	drivers/net/ethernet/freescale/gianfar.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
#	drivers/target/target_core_alua.c
#	drivers/target/target_core_transport.c
#	drivers/tty/n_tty.c
#	drivers/vhost/scsi.c
#	fs/jbd2/commit.c
#	kernel/rcutree_plugin.h
#	net/ipv4/tcp_output.c
diff --cc drivers/gpu/drm/drm_irq.c
index f92da0a32f0d,ec5c3f4cdd01..000000000000
--- a/drivers/gpu/drm/drm_irq.c
+++ b/drivers/gpu/drm/drm_irq.c
@@@ -157,8 -155,8 +157,13 @@@ static void vblank_disable_and_save(str
  	 * hope for the best.
  	 */
  	if ((vblrc > 0) && (abs64(diff_ns) > 1000000)) {
++<<<<<<< HEAD
 +		atomic_inc(&dev->_vblank_count[crtc]);
 +		smp_mb__after_atomic_inc();
++=======
+ 		atomic_inc(&dev->vblank[crtc].count);
+ 		smp_mb__after_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	}
  
  	/* Invalidate all timestamps while vblank irq's are off. */
@@@ -934,9 -864,9 +939,15 @@@ static void drm_update_vblank_count(str
  		vblanktimestamp(dev, crtc, tslot) = t_vblank;
  	}
  
++<<<<<<< HEAD
 +	smp_mb__before_atomic_inc();
 +	atomic_add(diff, &dev->_vblank_count[crtc]);
 +	smp_mb__after_atomic_inc();
++=======
+ 	smp_mb__before_atomic();
+ 	atomic_add(diff, &dev->vblank[crtc].count);
+ 	smp_mb__after_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  }
  
  /**
@@@ -1400,9 -1330,9 +1411,15 @@@ bool drm_handle_vblank(struct drm_devic
  		/* Increment cooked vblank count. This also atomically commits
  		 * the timestamp computed above.
  		 */
++<<<<<<< HEAD
 +		smp_mb__before_atomic_inc();
 +		atomic_inc(&dev->_vblank_count[crtc]);
 +		smp_mb__after_atomic_inc();
++=======
+ 		smp_mb__before_atomic();
+ 		atomic_inc(&dev->vblank[crtc].count);
+ 		smp_mb__after_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	} else {
  		DRM_DEBUG("crtc %d: Redundant vblirq ignored. diff_ns = %d\n",
  			  crtc, (int) diff_ns);
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
index 1fbd79453a08,dd57c7c5a3da..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
@@@ -4937,3 -4935,15 +4937,18 @@@ void bnx2x_update_coalesce_sb_index(str
  	disable = disable ? 1 : (usec ? 0 : 1);
  	storm_memset_hc_disable(bp, port, fw_sb_id, sb_index, disable);
  }
++<<<<<<< HEAD
++=======
+ 
+ void bnx2x_schedule_sp_rtnl(struct bnx2x *bp, enum sp_rtnl_flag flag,
+ 			    u32 verbose)
+ {
+ 	smp_mb__before_atomic();
+ 	set_bit(flag, &bp->sp_rtnl_state);
+ 	smp_mb__after_atomic();
+ 	DP((BNX2X_MSG_SP | verbose), "Scheduling sp_rtnl task [Flag: %d]\n",
+ 	   flag);
+ 	schedule_delayed_work(&bp->sp_rtnl_task, 0);
+ }
+ EXPORT_SYMBOL(bnx2x_schedule_sp_rtnl);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
index 1b0a9f9c968a,f82ac5ac2336..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
@@@ -2354,11 -1626,17 +2354,11 @@@ stati
  void bnx2x_vf_handle_filters_eqe(struct bnx2x *bp,
  				 struct bnx2x_virtf *vf)
  {
- 	smp_mb__before_clear_bit();
+ 	smp_mb__before_atomic();
  	clear_bit(BNX2X_FILTER_RX_MODE_PENDING, &vf->filter_state);
- 	smp_mb__after_clear_bit();
+ 	smp_mb__after_atomic();
  }
  
 -static void bnx2x_vf_handle_rss_update_eqe(struct bnx2x *bp,
 -					   struct bnx2x_virtf *vf)
 -{
 -	vf->rss_conf_obj.raw.clear_pending(&vf->rss_conf_obj.raw);
 -}
 -
  int bnx2x_iov_eq_sp_event(struct bnx2x *bp, union event_ring_elem *elem)
  {
  	struct bnx2x_virtf *vf;
@@@ -3792,3 -2941,28 +3792,31 @@@ void bnx2x_iov_channel_down(struct bnx2
  		bnx2x_post_vf_bulletin(bp, vf_idx);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ void bnx2x_iov_task(struct work_struct *work)
+ {
+ 	struct bnx2x *bp = container_of(work, struct bnx2x, iov_task.work);
+ 
+ 	if (!netif_running(bp->dev))
+ 		return;
+ 
+ 	if (test_and_clear_bit(BNX2X_IOV_HANDLE_FLR,
+ 			       &bp->iov_task_state))
+ 		bnx2x_vf_handle_flr_event(bp);
+ 
+ 	if (test_and_clear_bit(BNX2X_IOV_HANDLE_VF_MSG,
+ 			       &bp->iov_task_state))
+ 		bnx2x_vf_mbx(bp);
+ }
+ 
+ void bnx2x_schedule_iov_task(struct bnx2x *bp, enum bnx2x_iov_flag flag)
+ {
+ 	smp_mb__before_atomic();
+ 	set_bit(flag, &bp->iov_task_state);
+ 	smp_mb__after_atomic();
+ 	DP(BNX2X_MSG_IOV, "Scheduling iov task [Flag: %d]\n", flag);
+ 	queue_delayed_work(bnx2x_iov_wq, &bp->iov_task, 0);
+ }
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
diff --cc drivers/net/ethernet/freescale/gianfar.c
index 2375a01715a0,d82f092cae90..000000000000
--- a/drivers/net/ethernet/freescale/gianfar.c
+++ b/drivers/net/ethernet/freescale/gianfar.c
@@@ -1667,8 -1794,17 +1667,22 @@@ static void free_grp_irqs(struct gfar_p
  void stop_gfar(struct net_device *dev)
  {
  	struct gfar_private *priv = netdev_priv(dev);
++<<<<<<< HEAD
 +	unsigned long flags;
 +	int i;
++=======
+ 
+ 	netif_tx_stop_all_queues(dev);
+ 
+ 	smp_mb__before_atomic();
+ 	set_bit(GFAR_DOWN, &priv->state);
+ 	smp_mb__after_atomic();
+ 
+ 	disable_napi(priv);
+ 
+ 	/* disable ints and gracefully shut down Rx/Tx DMA */
+ 	gfar_halt(priv);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  
  	phy_stop(priv->phydev);
  
@@@ -1942,8 -2025,29 +1956,34 @@@ int startup_gfar(struct net_device *nde
  		}
  	}
  
++<<<<<<< HEAD
 +	/* Start the controller */
 +	gfar_start(ndev);
++=======
+ 	return 0;
+ }
+ 
+ /* Bring the controller up and running */
+ int startup_gfar(struct net_device *ndev)
+ {
+ 	struct gfar_private *priv = netdev_priv(ndev);
+ 	int err;
+ 
+ 	gfar_mac_reset(priv);
+ 
+ 	err = gfar_alloc_skb_resources(ndev);
+ 	if (err)
+ 		return err;
+ 
+ 	gfar_init_tx_rx_base(priv);
+ 
+ 	smp_mb__before_atomic();
+ 	clear_bit(GFAR_DOWN, &priv->state);
+ 	smp_mb__after_atomic();
+ 
+ 	/* Start Rx/Tx DMA and enable the interrupts */
+ 	gfar_start(priv);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  
  	phy_start(priv->phydev);
  
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 7aba452833e5,2fecc2626de5..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -4330,6 -4671,7 +4330,10 @@@ static void ixgbe_up_complete(struct ix
  	if (hw->mac.ops.enable_tx_laser)
  		hw->mac.ops.enable_tx_laser(hw);
  
++<<<<<<< HEAD
++=======
+ 	smp_mb__before_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	clear_bit(__IXGBE_DOWN, &adapter->state);
  	ixgbe_napi_enable_all(adapter);
  
@@@ -5242,6 -5567,8 +5246,11 @@@ static int ixgbe_resume(struct pci_dev 
  		e_dev_err("Cannot enable PCI device from suspend\n");
  		return err;
  	}
++<<<<<<< HEAD
++=======
+ 	smp_mb__before_atomic();
+ 	clear_bit(__IXGBE_DISABLED, &adapter->state);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	pci_set_master(pdev);
  
  	pci_wake_from_d3(pdev, false);
@@@ -8061,6 -8541,9 +8070,12 @@@ static pci_ers_result_t ixgbe_io_slot_r
  		e_err(probe, "Cannot re-enable PCI device after reset.\n");
  		result = PCI_ERS_RESULT_DISCONNECT;
  	} else {
++<<<<<<< HEAD
++=======
+ 		smp_mb__before_atomic();
+ 		clear_bit(__IXGBE_DISABLED, &adapter->state);
+ 		adapter->hw.hw_addr = adapter->io_addr;
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  		pci_set_master(pdev);
  		pci_restore_state(pdev);
  		pci_save_state(pdev);
diff --cc drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 885e407df9ec,de2793b06305..000000000000
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@@ -1455,6 -1668,7 +1455,10 @@@ static void ixgbevf_up_complete(struct 
  
  	spin_unlock_bh(&adapter->mbx_lock);
  
++<<<<<<< HEAD
++=======
+ 	smp_mb__before_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	clear_bit(__IXGBEVF_DOWN, &adapter->state);
  	ixgbevf_napi_enable_all(adapter);
  
@@@ -3213,6 -3354,8 +3217,11 @@@ static int ixgbevf_resume(struct pci_de
  		dev_err(&pdev->dev, "Cannot enable PCI device from suspend\n");
  		return err;
  	}
++<<<<<<< HEAD
++=======
+ 	smp_mb__before_atomic();
+ 	clear_bit(__IXGBEVF_DISABLED, &adapter->state);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	pci_set_master(pdev);
  
  	ixgbevf_reset(adapter);
@@@ -3558,6 -3712,8 +3567,11 @@@ static pci_ers_result_t ixgbevf_io_slot
  		return PCI_ERS_RESULT_DISCONNECT;
  	}
  
++<<<<<<< HEAD
++=======
+ 	smp_mb__before_atomic();
+ 	clear_bit(__IXGBEVF_DISABLED, &adapter->state);
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	pci_set_master(pdev);
  
  	ixgbevf_reset(adapter);
diff --cc drivers/target/target_core_alua.c
index f608fbc14a27,0b79b852f4b2..000000000000
--- a/drivers/target/target_core_alua.c
+++ b/drivers/target/target_core_alua.c
@@@ -860,13 -1043,96 +860,102 @@@ static int core_alua_do_transition_tg_p
  	/*
  	 * Set the current primary ALUA access state to the requested new state
  	 */
 -	atomic_set(&tg_pt_gp->tg_pt_gp_alua_access_state,
 -		   tg_pt_gp->tg_pt_gp_alua_pending_state);
 +	atomic_set(&tg_pt_gp->tg_pt_gp_alua_access_state, new_state);
  
  	pr_debug("Successful %s ALUA transition TG PT Group: %s ID: %hu"
++<<<<<<< HEAD
 +		" from primary access state %s to %s\n", (explict) ? "explict" :
 +		"implict", config_item_name(&tg_pt_gp->tg_pt_gp_group.cg_item),
 +		tg_pt_gp->tg_pt_gp_id, core_alua_dump_state(old_state),
 +		core_alua_dump_state(new_state));
++=======
+ 		" from primary access state %s to %s\n", (explicit) ? "explicit" :
+ 		"implicit", config_item_name(&tg_pt_gp->tg_pt_gp_group.cg_item),
+ 		tg_pt_gp->tg_pt_gp_id,
+ 		core_alua_dump_state(tg_pt_gp->tg_pt_gp_alua_previous_state),
+ 		core_alua_dump_state(tg_pt_gp->tg_pt_gp_alua_pending_state));
+ 	spin_lock(&dev->t10_alua.tg_pt_gps_lock);
+ 	atomic_dec(&tg_pt_gp->tg_pt_gp_ref_cnt);
+ 	smp_mb__after_atomic();
+ 	spin_unlock(&dev->t10_alua.tg_pt_gps_lock);
+ 
+ 	if (tg_pt_gp->tg_pt_gp_transition_complete)
+ 		complete(tg_pt_gp->tg_pt_gp_transition_complete);
+ }
+ 
+ static int core_alua_do_transition_tg_pt(
+ 	struct t10_alua_tg_pt_gp *tg_pt_gp,
+ 	int new_state,
+ 	int explicit)
+ {
+ 	struct se_device *dev = tg_pt_gp->tg_pt_gp_dev;
+ 	DECLARE_COMPLETION_ONSTACK(wait);
+ 
+ 	/* Nothing to be done here */
+ 	if (atomic_read(&tg_pt_gp->tg_pt_gp_alua_access_state) == new_state)
+ 		return 0;
+ 
+ 	if (new_state == ALUA_ACCESS_STATE_TRANSITION)
+ 		return -EAGAIN;
+ 
+ 	/*
+ 	 * Flush any pending transitions
+ 	 */
+ 	if (!explicit && tg_pt_gp->tg_pt_gp_implicit_trans_secs &&
+ 	    atomic_read(&tg_pt_gp->tg_pt_gp_alua_access_state) ==
+ 	    ALUA_ACCESS_STATE_TRANSITION) {
+ 		/* Just in case */
+ 		tg_pt_gp->tg_pt_gp_alua_pending_state = new_state;
+ 		tg_pt_gp->tg_pt_gp_transition_complete = &wait;
+ 		flush_delayed_work(&tg_pt_gp->tg_pt_gp_transition_work);
+ 		wait_for_completion(&wait);
+ 		tg_pt_gp->tg_pt_gp_transition_complete = NULL;
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * Save the old primary ALUA access state, and set the current state
+ 	 * to ALUA_ACCESS_STATE_TRANSITION.
+ 	 */
+ 	tg_pt_gp->tg_pt_gp_alua_previous_state =
+ 		atomic_read(&tg_pt_gp->tg_pt_gp_alua_access_state);
+ 	tg_pt_gp->tg_pt_gp_alua_pending_state = new_state;
+ 
+ 	atomic_set(&tg_pt_gp->tg_pt_gp_alua_access_state,
+ 			ALUA_ACCESS_STATE_TRANSITION);
+ 	tg_pt_gp->tg_pt_gp_alua_access_status = (explicit) ?
+ 				ALUA_STATUS_ALTERED_BY_EXPLICIT_STPG :
+ 				ALUA_STATUS_ALTERED_BY_IMPLICIT_ALUA;
+ 
+ 	/*
+ 	 * Check for the optional ALUA primary state transition delay
+ 	 */
+ 	if (tg_pt_gp->tg_pt_gp_trans_delay_msecs != 0)
+ 		msleep_interruptible(tg_pt_gp->tg_pt_gp_trans_delay_msecs);
+ 
+ 	/*
+ 	 * Take a reference for workqueue item
+ 	 */
+ 	spin_lock(&dev->t10_alua.tg_pt_gps_lock);
+ 	atomic_inc(&tg_pt_gp->tg_pt_gp_ref_cnt);
+ 	smp_mb__after_atomic();
+ 	spin_unlock(&dev->t10_alua.tg_pt_gps_lock);
+ 
+ 	if (!explicit && tg_pt_gp->tg_pt_gp_implicit_trans_secs) {
+ 		unsigned long transition_tmo;
+ 
+ 		transition_tmo = tg_pt_gp->tg_pt_gp_implicit_trans_secs * HZ;
+ 		queue_delayed_work(tg_pt_gp->tg_pt_gp_dev->tmr_wq,
+ 				   &tg_pt_gp->tg_pt_gp_transition_work,
+ 				   transition_tmo);
+ 	} else {
+ 		tg_pt_gp->tg_pt_gp_transition_complete = &wait;
+ 		queue_delayed_work(tg_pt_gp->tg_pt_gp_dev->tmr_wq,
+ 				   &tg_pt_gp->tg_pt_gp_transition_work, 0);
+ 		wait_for_completion(&wait);
+ 		tg_pt_gp->tg_pt_gp_transition_complete = NULL;
+ 	}
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  
  	return 0;
  }
@@@ -913,12 -1171,13 +1002,17 @@@ int core_alua_do_port_transition
  		 * core_alua_do_transition_tg_pt() will always return
  		 * success.
  		 */
 -		l_tg_pt_gp->tg_pt_gp_alua_port = l_port;
 -		l_tg_pt_gp->tg_pt_gp_alua_nacl = l_nacl;
 -		rc = core_alua_do_transition_tg_pt(l_tg_pt_gp,
 -						   new_state, explicit);
 +		core_alua_do_transition_tg_pt(l_tg_pt_gp, l_port, l_nacl,
 +					md_buf, new_state, explict);
  		atomic_dec(&lu_gp->lu_gp_ref_cnt);
++<<<<<<< HEAD
 +		smp_mb__after_atomic_dec();
 +		kfree(md_buf);
 +		return 0;
++=======
+ 		smp_mb__after_atomic();
+ 		return rc;
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	}
  	/*
  	 * For all other LU groups aside from 'default_lu_gp', walk all of
@@@ -953,14 -1212,14 +1047,14 @@@
  				continue;
  
  			if (l_tg_pt_gp == tg_pt_gp) {
 -				tg_pt_gp->tg_pt_gp_alua_port = l_port;
 -				tg_pt_gp->tg_pt_gp_alua_nacl = l_nacl;
 +				port = l_port;
 +				nacl = l_nacl;
  			} else {
 -				tg_pt_gp->tg_pt_gp_alua_port = NULL;
 -				tg_pt_gp->tg_pt_gp_alua_nacl = NULL;
 +				port = NULL;
 +				nacl = NULL;
  			}
  			atomic_inc(&tg_pt_gp->tg_pt_gp_ref_cnt);
- 			smp_mb__after_atomic_inc();
+ 			smp_mb__after_atomic();
  			spin_unlock(&dev->t10_alua.tg_pt_gps_lock);
  			/*
  			 * core_alua_do_transition_tg_pt() will always return
@@@ -971,7 -1230,9 +1065,13 @@@
  
  			spin_lock(&dev->t10_alua.tg_pt_gps_lock);
  			atomic_dec(&tg_pt_gp->tg_pt_gp_ref_cnt);
++<<<<<<< HEAD
 +			smp_mb__after_atomic_dec();
++=======
+ 			smp_mb__after_atomic();
+ 			if (rc)
+ 				break;
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  		}
  		spin_unlock(&dev->t10_alua.tg_pt_gps_lock);
  
@@@ -981,16 -1242,18 +1081,21 @@@
  	}
  	spin_unlock(&lu_gp->lu_gp_lock);
  
 -	if (!rc) {
 -		pr_debug("Successfully processed LU Group: %s all ALUA TG PT"
 -			 " Group IDs: %hu %s transition to primary state: %s\n",
 -			 config_item_name(&lu_gp->lu_gp_group.cg_item),
 -			 l_tg_pt_gp->tg_pt_gp_id,
 -			 (explicit) ? "explicit" : "implicit",
 -			 core_alua_dump_state(new_state));
 -	}
 +	pr_debug("Successfully processed LU Group: %s all ALUA TG PT"
 +		" Group IDs: %hu %s transition to primary state: %s\n",
 +		config_item_name(&lu_gp->lu_gp_group.cg_item),
 +		l_tg_pt_gp->tg_pt_gp_id, (explict) ? "explict" : "implict",
 +		core_alua_dump_state(new_state));
  
  	atomic_dec(&lu_gp->lu_gp_ref_cnt);
++<<<<<<< HEAD
 +	smp_mb__after_atomic_dec();
 +	kfree(md_buf);
 +	return 0;
++=======
+ 	smp_mb__after_atomic();
+ 	return rc;
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  }
  
  /*
diff --cc drivers/target/target_core_transport.c
index 305cd33a9a56,4badca1cd625..000000000000
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@@ -2821,7 -2874,8 +2821,12 @@@ void transport_send_task_abort(struct s
  	if (cmd->data_direction == DMA_TO_DEVICE) {
  		if (cmd->se_tfo->write_pending_status(cmd) != 0) {
  			cmd->transport_state |= CMD_T_ABORTED;
++<<<<<<< HEAD
 +			smp_mb__after_atomic_inc();
++=======
+ 			cmd->se_cmd_flags |= SCF_SEND_DELAYED_TAS;
+ 			smp_mb__after_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  			return;
  		}
  	}
diff --cc drivers/tty/n_tty.c
index 6cfe4019abc6,746ae80b972f..000000000000
--- a/drivers/tty/n_tty.c
+++ b/drivers/tty/n_tty.c
@@@ -1725,6 -1956,104 +1725,107 @@@ static int copy_from_read_buf(struct tt
  	return retval;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  *	canon_copy_from_read_buf	-	copy read data in canonical mode
+  *	@tty: terminal device
+  *	@b: user data
+  *	@nr: size of data
+  *
+  *	Helper function for n_tty_read.  It is only called when ICANON is on;
+  *	it copies one line of input up to and including the line-delimiting
+  *	character into the user-space buffer.
+  *
+  *	NB: When termios is changed from non-canonical to canonical mode and
+  *	the read buffer contains data, n_tty_set_termios() simulates an EOF
+  *	push (as if C-d were input) _without_ the DISABLED_CHAR in the buffer.
+  *	This causes data already processed as input to be immediately available
+  *	as input although a newline has not been received.
+  *
+  *	Called under the atomic_read_lock mutex
+  *
+  *	n_tty_read()/consumer path:
+  *		caller holds non-exclusive termios_rwsem
+  *		read_tail published
+  */
+ 
+ static int canon_copy_from_read_buf(struct tty_struct *tty,
+ 				    unsigned char __user **b,
+ 				    size_t *nr)
+ {
+ 	struct n_tty_data *ldata = tty->disc_data;
+ 	size_t n, size, more, c;
+ 	size_t eol;
+ 	size_t tail;
+ 	int ret, found = 0;
+ 	bool eof_push = 0;
+ 
+ 	/* N.B. avoid overrun if nr == 0 */
+ 	n = min(*nr, read_cnt(ldata));
+ 	if (!n)
+ 		return 0;
+ 
+ 	tail = ldata->read_tail & (N_TTY_BUF_SIZE - 1);
+ 	size = min_t(size_t, tail + n, N_TTY_BUF_SIZE);
+ 
+ 	n_tty_trace("%s: nr:%zu tail:%zu n:%zu size:%zu\n",
+ 		    __func__, *nr, tail, n, size);
+ 
+ 	eol = find_next_bit(ldata->read_flags, size, tail);
+ 	more = n - (size - tail);
+ 	if (eol == N_TTY_BUF_SIZE && more) {
+ 		/* scan wrapped without finding set bit */
+ 		eol = find_next_bit(ldata->read_flags, more, 0);
+ 		if (eol != more)
+ 			found = 1;
+ 	} else if (eol != size)
+ 		found = 1;
+ 
+ 	size = N_TTY_BUF_SIZE - tail;
+ 	n = eol - tail;
+ 	if (n > 4096)
+ 		n += 4096;
+ 	n += found;
+ 	c = n;
+ 
+ 	if (found && !ldata->push && read_buf(ldata, eol) == __DISABLED_CHAR) {
+ 		n--;
+ 		eof_push = !n && ldata->read_tail != ldata->line_start;
+ 	}
+ 
+ 	n_tty_trace("%s: eol:%zu found:%d n:%zu c:%zu size:%zu more:%zu\n",
+ 		    __func__, eol, found, n, c, size, more);
+ 
+ 	if (n > size) {
+ 		ret = copy_to_user(*b, read_buf_addr(ldata, tail), size);
+ 		if (ret)
+ 			return -EFAULT;
+ 		ret = copy_to_user(*b + size, ldata->read_buf, n - size);
+ 	} else
+ 		ret = copy_to_user(*b, read_buf_addr(ldata, tail), n);
+ 
+ 	if (ret)
+ 		return -EFAULT;
+ 	*b += n;
+ 	*nr -= n;
+ 
+ 	if (found)
+ 		clear_bit(eol, ldata->read_flags);
+ 	smp_mb__after_atomic();
+ 	ldata->read_tail += c;
+ 
+ 	if (found) {
+ 		if (!ldata->push)
+ 			ldata->line_start = ldata->read_tail;
+ 		else
+ 			ldata->push = 0;
+ 		tty_audit_push(tty);
+ 	}
+ 	return eof_push ? -EAGAIN : 0;
+ }
+ 
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  extern ssize_t redirected_tty_write(struct file *, const char __user *,
  							size_t, loff_t *);
  
diff --cc drivers/vhost/scsi.c
index 962c7e3c3baa,aeb513108448..000000000000
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@@ -1197,13 -1252,13 +1197,20 @@@ static int vhost_scsi_set_endpoint
  				ret = -EEXIST;
  				goto out;
  			}
++<<<<<<< HEAD
 +			tv_tpg->tv_tpg_vhost_count++;
 +			tv_tpg->vhost_scsi = vs;
 +			vs_tpg[tv_tpg->tport_tpgt] = tv_tpg;
 +			smp_mb__after_atomic_inc();
++=======
+ 			tpg->tv_tpg_vhost_count++;
+ 			tpg->vhost_scsi = vs;
+ 			vs_tpg[tpg->tport_tpgt] = tpg;
+ 			smp_mb__after_atomic();
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  			match = true;
  		}
 -		mutex_unlock(&tpg->tv_tpg_mutex);
 +		mutex_unlock(&tv_tpg->tv_tpg_mutex);
  	}
  
  	if (match) {
diff --cc fs/jbd2/commit.c
index ab3c9d9839b3,6fac74349856..000000000000
--- a/fs/jbd2/commit.c
+++ b/fs/jbd2/commit.c
@@@ -39,6 -41,11 +39,14 @@@ static void journal_end_buffer_io_sync(
  		set_buffer_uptodate(bh);
  	else
  		clear_buffer_uptodate(bh);
++<<<<<<< HEAD
++=======
+ 	if (orig_bh) {
+ 		clear_bit_unlock(BH_Shadow, &orig_bh->b_state);
+ 		smp_mb__after_atomic();
+ 		wake_up_bit(&orig_bh->b_state, BH_Shadow);
+ 	}
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  	unlock_buffer(bh);
  }
  
diff --cc kernel/rcutree_plugin.h
index 30ac20a9809b,56db2f853e43..000000000000
--- a/kernel/rcutree_plugin.h
+++ b/kernel/rcutree_plugin.h
@@@ -2350,3 -2472,445 +2350,448 @@@ static void rcu_kick_nohz_cpu(int cpu
  		smp_send_reschedule(cpu);
  #endif /* #ifdef CONFIG_NO_HZ_FULL */
  }
++<<<<<<< HEAD:kernel/rcutree_plugin.h
++=======
+ 
+ 
+ #ifdef CONFIG_NO_HZ_FULL_SYSIDLE
+ 
+ /*
+  * Define RCU flavor that holds sysidle state.  This needs to be the
+  * most active flavor of RCU.
+  */
+ #ifdef CONFIG_PREEMPT_RCU
+ static struct rcu_state *rcu_sysidle_state = &rcu_preempt_state;
+ #else /* #ifdef CONFIG_PREEMPT_RCU */
+ static struct rcu_state *rcu_sysidle_state = &rcu_sched_state;
+ #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
+ 
+ static int full_sysidle_state;		/* Current system-idle state. */
+ #define RCU_SYSIDLE_NOT		0	/* Some CPU is not idle. */
+ #define RCU_SYSIDLE_SHORT	1	/* All CPUs idle for brief period. */
+ #define RCU_SYSIDLE_LONG	2	/* All CPUs idle for long enough. */
+ #define RCU_SYSIDLE_FULL	3	/* All CPUs idle, ready for sysidle. */
+ #define RCU_SYSIDLE_FULL_NOTED	4	/* Actually entered sysidle state. */
+ 
+ /*
+  * Invoked to note exit from irq or task transition to idle.  Note that
+  * usermode execution does -not- count as idle here!  After all, we want
+  * to detect full-system idle states, not RCU quiescent states and grace
+  * periods.  The caller must have disabled interrupts.
+  */
+ static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq)
+ {
+ 	unsigned long j;
+ 
+ 	/* Adjust nesting, check for fully idle. */
+ 	if (irq) {
+ 		rdtp->dynticks_idle_nesting--;
+ 		WARN_ON_ONCE(rdtp->dynticks_idle_nesting < 0);
+ 		if (rdtp->dynticks_idle_nesting != 0)
+ 			return;  /* Still not fully idle. */
+ 	} else {
+ 		if ((rdtp->dynticks_idle_nesting & DYNTICK_TASK_NEST_MASK) ==
+ 		    DYNTICK_TASK_NEST_VALUE) {
+ 			rdtp->dynticks_idle_nesting = 0;
+ 		} else {
+ 			rdtp->dynticks_idle_nesting -= DYNTICK_TASK_NEST_VALUE;
+ 			WARN_ON_ONCE(rdtp->dynticks_idle_nesting < 0);
+ 			return;  /* Still not fully idle. */
+ 		}
+ 	}
+ 
+ 	/* Record start of fully idle period. */
+ 	j = jiffies;
+ 	ACCESS_ONCE(rdtp->dynticks_idle_jiffies) = j;
+ 	smp_mb__before_atomic();
+ 	atomic_inc(&rdtp->dynticks_idle);
+ 	smp_mb__after_atomic();
+ 	WARN_ON_ONCE(atomic_read(&rdtp->dynticks_idle) & 0x1);
+ }
+ 
+ /*
+  * Unconditionally force exit from full system-idle state.  This is
+  * invoked when a normal CPU exits idle, but must be called separately
+  * for the timekeeping CPU (tick_do_timer_cpu).  The reason for this
+  * is that the timekeeping CPU is permitted to take scheduling-clock
+  * interrupts while the system is in system-idle state, and of course
+  * rcu_sysidle_exit() has no way of distinguishing a scheduling-clock
+  * interrupt from any other type of interrupt.
+  */
+ void rcu_sysidle_force_exit(void)
+ {
+ 	int oldstate = ACCESS_ONCE(full_sysidle_state);
+ 	int newoldstate;
+ 
+ 	/*
+ 	 * Each pass through the following loop attempts to exit full
+ 	 * system-idle state.  If contention proves to be a problem,
+ 	 * a trylock-based contention tree could be used here.
+ 	 */
+ 	while (oldstate > RCU_SYSIDLE_SHORT) {
+ 		newoldstate = cmpxchg(&full_sysidle_state,
+ 				      oldstate, RCU_SYSIDLE_NOT);
+ 		if (oldstate == newoldstate &&
+ 		    oldstate == RCU_SYSIDLE_FULL_NOTED) {
+ 			rcu_kick_nohz_cpu(tick_do_timer_cpu);
+ 			return; /* We cleared it, done! */
+ 		}
+ 		oldstate = newoldstate;
+ 	}
+ 	smp_mb(); /* Order initial oldstate fetch vs. later non-idle work. */
+ }
+ 
+ /*
+  * Invoked to note entry to irq or task transition from idle.  Note that
+  * usermode execution does -not- count as idle here!  The caller must
+  * have disabled interrupts.
+  */
+ static void rcu_sysidle_exit(struct rcu_dynticks *rdtp, int irq)
+ {
+ 	/* Adjust nesting, check for already non-idle. */
+ 	if (irq) {
+ 		rdtp->dynticks_idle_nesting++;
+ 		WARN_ON_ONCE(rdtp->dynticks_idle_nesting <= 0);
+ 		if (rdtp->dynticks_idle_nesting != 1)
+ 			return; /* Already non-idle. */
+ 	} else {
+ 		/*
+ 		 * Allow for irq misnesting.  Yes, it really is possible
+ 		 * to enter an irq handler then never leave it, and maybe
+ 		 * also vice versa.  Handle both possibilities.
+ 		 */
+ 		if (rdtp->dynticks_idle_nesting & DYNTICK_TASK_NEST_MASK) {
+ 			rdtp->dynticks_idle_nesting += DYNTICK_TASK_NEST_VALUE;
+ 			WARN_ON_ONCE(rdtp->dynticks_idle_nesting <= 0);
+ 			return; /* Already non-idle. */
+ 		} else {
+ 			rdtp->dynticks_idle_nesting = DYNTICK_TASK_EXIT_IDLE;
+ 		}
+ 	}
+ 
+ 	/* Record end of idle period. */
+ 	smp_mb__before_atomic();
+ 	atomic_inc(&rdtp->dynticks_idle);
+ 	smp_mb__after_atomic();
+ 	WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks_idle) & 0x1));
+ 
+ 	/*
+ 	 * If we are the timekeeping CPU, we are permitted to be non-idle
+ 	 * during a system-idle state.  This must be the case, because
+ 	 * the timekeeping CPU has to take scheduling-clock interrupts
+ 	 * during the time that the system is transitioning to full
+ 	 * system-idle state.  This means that the timekeeping CPU must
+ 	 * invoke rcu_sysidle_force_exit() directly if it does anything
+ 	 * more than take a scheduling-clock interrupt.
+ 	 */
+ 	if (smp_processor_id() == tick_do_timer_cpu)
+ 		return;
+ 
+ 	/* Update system-idle state: We are clearly no longer fully idle! */
+ 	rcu_sysidle_force_exit();
+ }
+ 
+ /*
+  * Check to see if the current CPU is idle.  Note that usermode execution
+  * does not count as idle.  The caller must have disabled interrupts.
+  */
+ static void rcu_sysidle_check_cpu(struct rcu_data *rdp, bool *isidle,
+ 				  unsigned long *maxj)
+ {
+ 	int cur;
+ 	unsigned long j;
+ 	struct rcu_dynticks *rdtp = rdp->dynticks;
+ 
+ 	/*
+ 	 * If some other CPU has already reported non-idle, if this is
+ 	 * not the flavor of RCU that tracks sysidle state, or if this
+ 	 * is an offline or the timekeeping CPU, nothing to do.
+ 	 */
+ 	if (!*isidle || rdp->rsp != rcu_sysidle_state ||
+ 	    cpu_is_offline(rdp->cpu) || rdp->cpu == tick_do_timer_cpu)
+ 		return;
+ 	if (rcu_gp_in_progress(rdp->rsp))
+ 		WARN_ON_ONCE(smp_processor_id() != tick_do_timer_cpu);
+ 
+ 	/* Pick up current idle and NMI-nesting counter and check. */
+ 	cur = atomic_read(&rdtp->dynticks_idle);
+ 	if (cur & 0x1) {
+ 		*isidle = false; /* We are not idle! */
+ 		return;
+ 	}
+ 	smp_mb(); /* Read counters before timestamps. */
+ 
+ 	/* Pick up timestamps. */
+ 	j = ACCESS_ONCE(rdtp->dynticks_idle_jiffies);
+ 	/* If this CPU entered idle more recently, update maxj timestamp. */
+ 	if (ULONG_CMP_LT(*maxj, j))
+ 		*maxj = j;
+ }
+ 
+ /*
+  * Is this the flavor of RCU that is handling full-system idle?
+  */
+ static bool is_sysidle_rcu_state(struct rcu_state *rsp)
+ {
+ 	return rsp == rcu_sysidle_state;
+ }
+ 
+ /*
+  * Bind the grace-period kthread for the sysidle flavor of RCU to the
+  * timekeeping CPU.
+  */
+ static void rcu_bind_gp_kthread(void)
+ {
+ 	int cpu = ACCESS_ONCE(tick_do_timer_cpu);
+ 
+ 	if (cpu < 0 || cpu >= nr_cpu_ids)
+ 		return;
+ 	if (raw_smp_processor_id() != cpu)
+ 		set_cpus_allowed_ptr(current, cpumask_of(cpu));
+ }
+ 
+ /*
+  * Return a delay in jiffies based on the number of CPUs, rcu_node
+  * leaf fanout, and jiffies tick rate.  The idea is to allow larger
+  * systems more time to transition to full-idle state in order to
+  * avoid the cache thrashing that otherwise occur on the state variable.
+  * Really small systems (less than a couple of tens of CPUs) should
+  * instead use a single global atomically incremented counter, and later
+  * versions of this will automatically reconfigure themselves accordingly.
+  */
+ static unsigned long rcu_sysidle_delay(void)
+ {
+ 	if (nr_cpu_ids <= CONFIG_NO_HZ_FULL_SYSIDLE_SMALL)
+ 		return 0;
+ 	return DIV_ROUND_UP(nr_cpu_ids * HZ, rcu_fanout_leaf * 1000);
+ }
+ 
+ /*
+  * Advance the full-system-idle state.  This is invoked when all of
+  * the non-timekeeping CPUs are idle.
+  */
+ static void rcu_sysidle(unsigned long j)
+ {
+ 	/* Check the current state. */
+ 	switch (ACCESS_ONCE(full_sysidle_state)) {
+ 	case RCU_SYSIDLE_NOT:
+ 
+ 		/* First time all are idle, so note a short idle period. */
+ 		ACCESS_ONCE(full_sysidle_state) = RCU_SYSIDLE_SHORT;
+ 		break;
+ 
+ 	case RCU_SYSIDLE_SHORT:
+ 
+ 		/*
+ 		 * Idle for a bit, time to advance to next state?
+ 		 * cmpxchg failure means race with non-idle, let them win.
+ 		 */
+ 		if (ULONG_CMP_GE(jiffies, j + rcu_sysidle_delay()))
+ 			(void)cmpxchg(&full_sysidle_state,
+ 				      RCU_SYSIDLE_SHORT, RCU_SYSIDLE_LONG);
+ 		break;
+ 
+ 	case RCU_SYSIDLE_LONG:
+ 
+ 		/*
+ 		 * Do an additional check pass before advancing to full.
+ 		 * cmpxchg failure means race with non-idle, let them win.
+ 		 */
+ 		if (ULONG_CMP_GE(jiffies, j + rcu_sysidle_delay()))
+ 			(void)cmpxchg(&full_sysidle_state,
+ 				      RCU_SYSIDLE_LONG, RCU_SYSIDLE_FULL);
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ }
+ 
+ /*
+  * Found a non-idle non-timekeeping CPU, so kick the system-idle state
+  * back to the beginning.
+  */
+ static void rcu_sysidle_cancel(void)
+ {
+ 	smp_mb();
+ 	ACCESS_ONCE(full_sysidle_state) = RCU_SYSIDLE_NOT;
+ }
+ 
+ /*
+  * Update the sysidle state based on the results of a force-quiescent-state
+  * scan of the CPUs' dyntick-idle state.
+  */
+ static void rcu_sysidle_report(struct rcu_state *rsp, int isidle,
+ 			       unsigned long maxj, bool gpkt)
+ {
+ 	if (rsp != rcu_sysidle_state)
+ 		return;  /* Wrong flavor, ignore. */
+ 	if (gpkt && nr_cpu_ids <= CONFIG_NO_HZ_FULL_SYSIDLE_SMALL)
+ 		return;  /* Running state machine from timekeeping CPU. */
+ 	if (isidle)
+ 		rcu_sysidle(maxj);    /* More idle! */
+ 	else
+ 		rcu_sysidle_cancel(); /* Idle is over. */
+ }
+ 
+ /*
+  * Wrapper for rcu_sysidle_report() when called from the grace-period
+  * kthread's context.
+  */
+ static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
+ 				  unsigned long maxj)
+ {
+ 	rcu_sysidle_report(rsp, isidle, maxj, true);
+ }
+ 
+ /* Callback and function for forcing an RCU grace period. */
+ struct rcu_sysidle_head {
+ 	struct rcu_head rh;
+ 	int inuse;
+ };
+ 
+ static void rcu_sysidle_cb(struct rcu_head *rhp)
+ {
+ 	struct rcu_sysidle_head *rshp;
+ 
+ 	/*
+ 	 * The following memory barrier is needed to replace the
+ 	 * memory barriers that would normally be in the memory
+ 	 * allocator.
+ 	 */
+ 	smp_mb();  /* grace period precedes setting inuse. */
+ 
+ 	rshp = container_of(rhp, struct rcu_sysidle_head, rh);
+ 	ACCESS_ONCE(rshp->inuse) = 0;
+ }
+ 
+ /*
+  * Check to see if the system is fully idle, other than the timekeeping CPU.
+  * The caller must have disabled interrupts.
+  */
+ bool rcu_sys_is_idle(void)
+ {
+ 	static struct rcu_sysidle_head rsh;
+ 	int rss = ACCESS_ONCE(full_sysidle_state);
+ 
+ 	if (WARN_ON_ONCE(smp_processor_id() != tick_do_timer_cpu))
+ 		return false;
+ 
+ 	/* Handle small-system case by doing a full scan of CPUs. */
+ 	if (nr_cpu_ids <= CONFIG_NO_HZ_FULL_SYSIDLE_SMALL) {
+ 		int oldrss = rss - 1;
+ 
+ 		/*
+ 		 * One pass to advance to each state up to _FULL.
+ 		 * Give up if any pass fails to advance the state.
+ 		 */
+ 		while (rss < RCU_SYSIDLE_FULL && oldrss < rss) {
+ 			int cpu;
+ 			bool isidle = true;
+ 			unsigned long maxj = jiffies - ULONG_MAX / 4;
+ 			struct rcu_data *rdp;
+ 
+ 			/* Scan all the CPUs looking for nonidle CPUs. */
+ 			for_each_possible_cpu(cpu) {
+ 				rdp = per_cpu_ptr(rcu_sysidle_state->rda, cpu);
+ 				rcu_sysidle_check_cpu(rdp, &isidle, &maxj);
+ 				if (!isidle)
+ 					break;
+ 			}
+ 			rcu_sysidle_report(rcu_sysidle_state,
+ 					   isidle, maxj, false);
+ 			oldrss = rss;
+ 			rss = ACCESS_ONCE(full_sysidle_state);
+ 		}
+ 	}
+ 
+ 	/* If this is the first observation of an idle period, record it. */
+ 	if (rss == RCU_SYSIDLE_FULL) {
+ 		rss = cmpxchg(&full_sysidle_state,
+ 			      RCU_SYSIDLE_FULL, RCU_SYSIDLE_FULL_NOTED);
+ 		return rss == RCU_SYSIDLE_FULL;
+ 	}
+ 
+ 	smp_mb(); /* ensure rss load happens before later caller actions. */
+ 
+ 	/* If already fully idle, tell the caller (in case of races). */
+ 	if (rss == RCU_SYSIDLE_FULL_NOTED)
+ 		return true;
+ 
+ 	/*
+ 	 * If we aren't there yet, and a grace period is not in flight,
+ 	 * initiate a grace period.  Either way, tell the caller that
+ 	 * we are not there yet.  We use an xchg() rather than an assignment
+ 	 * to make up for the memory barriers that would otherwise be
+ 	 * provided by the memory allocator.
+ 	 */
+ 	if (nr_cpu_ids > CONFIG_NO_HZ_FULL_SYSIDLE_SMALL &&
+ 	    !rcu_gp_in_progress(rcu_sysidle_state) &&
+ 	    !rsh.inuse && xchg(&rsh.inuse, 1) == 0)
+ 		call_rcu(&rsh.rh, rcu_sysidle_cb);
+ 	return false;
+ }
+ 
+ /*
+  * Initialize dynticks sysidle state for CPUs coming online.
+  */
+ static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp)
+ {
+ 	rdtp->dynticks_idle_nesting = DYNTICK_TASK_NEST_VALUE;
+ }
+ 
+ #else /* #ifdef CONFIG_NO_HZ_FULL_SYSIDLE */
+ 
+ static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq)
+ {
+ }
+ 
+ static void rcu_sysidle_exit(struct rcu_dynticks *rdtp, int irq)
+ {
+ }
+ 
+ static void rcu_sysidle_check_cpu(struct rcu_data *rdp, bool *isidle,
+ 				  unsigned long *maxj)
+ {
+ }
+ 
+ static bool is_sysidle_rcu_state(struct rcu_state *rsp)
+ {
+ 	return false;
+ }
+ 
+ static void rcu_bind_gp_kthread(void)
+ {
+ }
+ 
+ static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
+ 				  unsigned long maxj)
+ {
+ }
+ 
+ static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp)
+ {
+ }
+ 
+ #endif /* #else #ifdef CONFIG_NO_HZ_FULL_SYSIDLE */
+ 
+ /*
+  * Is this CPU a NO_HZ_FULL CPU that should ignore RCU so that the
+  * grace-period kthread will do force_quiescent_state() processing?
+  * The idea is to avoid waking up RCU core processing on such a
+  * CPU unless the grace period has extended for too long.
+  *
+  * This code relies on the fact that all NO_HZ_FULL CPUs are also
+  * CONFIG_RCU_NOCB_CPU CPUs.
+  */
+ static bool rcu_nohz_full_cpu(struct rcu_state *rsp)
+ {
+ #ifdef CONFIG_NO_HZ_FULL
+ 	if (tick_nohz_full_cpu(smp_processor_id()) &&
+ 	    (!rcu_gp_in_progress(rsp) ||
+ 	     ULONG_CMP_LT(jiffies, ACCESS_ONCE(rsp->gp_start) + HZ)))
+ 		return 1;
+ #endif /* #ifdef CONFIG_NO_HZ_FULL */
+ 	return 0;
+ }
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*()):kernel/rcu/tree_plugin.h
diff --cc net/ipv4/tcp_output.c
index 145a4df33510,366cf06587b8..000000000000
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@@ -1877,7 -1927,13 +1877,17 @@@ static bool tcp_write_xmit(struct sock 
  
  		if (atomic_read(&sk->sk_wmem_alloc) > limit) {
  			set_bit(TSQ_THROTTLED, &tp->tsq_flags);
++<<<<<<< HEAD
 +			break;
++=======
+ 			/* It is possible TX completion already happened
+ 			 * before we set TSQ_THROTTLED, so we must
+ 			 * test again the condition.
+ 			 */
+ 			smp_mb__after_atomic();
+ 			if (atomic_read(&sk->sk_wmem_alloc) > limit)
+ 				break;
++>>>>>>> 4e857c58efeb (arch: Mass conversion of smp_mb__*())
  		}
  
  		limit = mss_now;
diff --git a/block/blk-iopoll.c b/block/blk-iopoll.c
index 4b8d9b541112..ce34edfc7818 100644
--- a/block/blk-iopoll.c
+++ b/block/blk-iopoll.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(blk_iopoll_sched);
 void __blk_iopoll_complete(struct blk_iopoll *iop)
 {
 	list_del(&iop->list);
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit_unlock(IOPOLL_F_SCHED, &iop->state);
 }
 EXPORT_SYMBOL(__blk_iopoll_complete);
@@ -164,7 +164,7 @@ EXPORT_SYMBOL(blk_iopoll_disable);
 void blk_iopoll_enable(struct blk_iopoll *iop)
 {
 	BUG_ON(!test_bit(IOPOLL_F_SCHED, &iop->state));
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit_unlock(IOPOLL_F_SCHED, &iop->state);
 }
 EXPORT_SYMBOL(blk_iopoll_enable);
diff --git a/crypto/chainiv.c b/crypto/chainiv.c
index 834d8dd3d4fc..9c294c8f9a07 100644
--- a/crypto/chainiv.c
+++ b/crypto/chainiv.c
@@ -126,7 +126,7 @@ static int async_chainiv_schedule_work(struct async_chainiv_ctx *ctx)
 	int err = ctx->err;
 
 	if (!ctx->queue.qlen) {
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(CHAINIV_STATE_INUSE, &ctx->state);
 
 		if (!ctx->queue.qlen ||
diff --git a/drivers/base/power/domain.c b/drivers/base/power/domain.c
index bfb8955c406c..2aa5a9eed020 100644
--- a/drivers/base/power/domain.c
+++ b/drivers/base/power/domain.c
@@ -106,7 +106,7 @@ static bool genpd_sd_counter_dec(struct generic_pm_domain *genpd)
 static void genpd_sd_counter_inc(struct generic_pm_domain *genpd)
 {
 	atomic_inc(&genpd->sd_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 static void genpd_acquire_lock(struct generic_pm_domain *genpd)
diff --git a/drivers/block/mtip32xx/mtip32xx.c b/drivers/block/mtip32xx/mtip32xx.c
index 9912f3d7777e..f893383307ac 100644
--- a/drivers/block/mtip32xx/mtip32xx.c
+++ b/drivers/block/mtip32xx/mtip32xx.c
@@ -224,9 +224,9 @@ static int get_slot(struct mtip_port *port)
  */
 static inline void release_slot(struct mtip_port *port, int tag)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(tag, port->allocated);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 /*
diff --git a/drivers/cpuidle/coupled.c b/drivers/cpuidle/coupled.c
index fe853903fe10..530f03b6ac83 100644
--- a/drivers/cpuidle/coupled.c
+++ b/drivers/cpuidle/coupled.c
@@ -159,7 +159,7 @@ void cpuidle_coupled_parallel_barrier(struct cpuidle_device *dev, atomic_t *a)
 {
 	int n = dev->coupled->online_count;
 
-	smp_mb__before_atomic_inc();
+	smp_mb__before_atomic();
 	atomic_inc(a);
 
 	while (atomic_read(a) < n)
diff --git a/drivers/firewire/ohci.c b/drivers/firewire/ohci.c
index 0f3e3047e29c..28603100dc6d 100644
--- a/drivers/firewire/ohci.c
+++ b/drivers/firewire/ohci.c
@@ -3493,7 +3493,7 @@ static int ohci_flush_iso_completions(struct fw_iso_context *base)
 		}
 
 		clear_bit_unlock(0, &ctx->flushing_completions);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 	}
 
 	tasklet_enable(&ctx->context.tasklet);
* Unmerged path drivers/gpu/drm/drm_irq.c
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index 94d4a6c2b196..4a1a323ea470 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -1554,7 +1554,7 @@ static void i915_error_work_func(struct work_struct *work)
 			 * updates before
 			 * the counter increment.
 			 */
-			smp_mb__before_atomic_inc();
+			smp_mb__before_atomic();
 			atomic_inc(&dev_priv->gpu_error.reset_counter);
 
 			kobject_uevent_env(&dev->primary->kdev.kobj,
diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6bc016eabdc9..38c07ad3570a 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1141,7 +1141,7 @@ static inline bool cached_dev_get(struct cached_dev *dc)
 		return false;
 
 	/* Paired with the mb in cached_dev_attach */
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	return true;
 }
 
diff --git a/drivers/md/bcache/closure.h b/drivers/md/bcache/closure.h
index 00039924ea9d..f0f6e9814c91 100644
--- a/drivers/md/bcache/closure.h
+++ b/drivers/md/bcache/closure.h
@@ -627,7 +627,7 @@ static inline void set_closure_fn(struct closure *cl, closure_fn *fn,
 	cl->fn = fn;
 	cl->wq = wq;
 	/* between atomic_dec() in closure_put() */
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 }
 
 #define continue_at(_cl, _fn, _wq)					\
diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c
index 09ef26a7ad63..f0146e558a52 100644
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@ -607,9 +607,9 @@ static void write_endio(struct bio *bio, int error)
 
 	BUG_ON(!test_bit(B_WRITING, &b->state));
 
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(B_WRITING, &b->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	wake_up_bit(&b->state, B_WRITING);
 }
@@ -997,9 +997,9 @@ static void read_endio(struct bio *bio, int error)
 
 	BUG_ON(!test_bit(B_READING, &b->state));
 
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(B_READING, &b->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	wake_up_bit(&b->state, B_READING);
 }
diff --git a/drivers/md/dm-snap.c b/drivers/md/dm-snap.c
index 717718558bd9..452ebbd88621 100644
--- a/drivers/md/dm-snap.c
+++ b/drivers/md/dm-snap.c
@@ -642,7 +642,7 @@ static void free_pending_exception(struct dm_snap_pending_exception *pe)
 	struct dm_snapshot *s = pe->snap;
 
 	mempool_free(pe, s->pending_pool);
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&s->pending_exceptions_count);
 }
 
@@ -783,7 +783,7 @@ static int init_hash_tables(struct dm_snapshot *s)
 static void merge_shutdown(struct dm_snapshot *s)
 {
 	clear_bit_unlock(RUNNING_MERGE, &s->state_bits);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&s->state_bits, RUNNING_MERGE);
 }
 
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index a8b8e3774456..79d270b6a6b1 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -2606,7 +2606,7 @@ static void dm_wq_work(struct work_struct *work)
 static void dm_queue_flush(struct mapped_device *md)
 {
 	clear_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	queue_work(md->wq, &md->work);
 }
 
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index fe42c3b793e4..af4e2f2eee54 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -4147,7 +4147,7 @@ static void raid5_unplug(struct blk_plug_cb *blk_cb, bool from_schedule)
 			 * STRIPE_ON_UNPLUG_LIST clear but the stripe
 			 * is still in our list
 			 */
-			smp_mb__before_clear_bit();
+			smp_mb__before_atomic();
 			clear_bit(STRIPE_ON_UNPLUG_LIST, &sh->state);
 			__release_stripe(conf, sh);
 			cnt++;
diff --git a/drivers/media/usb/dvb-usb-v2/dvb_usb_core.c b/drivers/media/usb/dvb-usb-v2/dvb_usb_core.c
index 19f6737d9817..2cb88473a5e5 100644
--- a/drivers/media/usb/dvb-usb-v2/dvb_usb_core.c
+++ b/drivers/media/usb/dvb-usb-v2/dvb_usb_core.c
@@ -399,7 +399,7 @@ static int dvb_usb_stop_feed(struct dvb_demux_feed *dvbdmxfeed)
 
 	/* clear 'streaming' status bit */
 	clear_bit(ADAP_STREAMING, &adap->state_bits);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&adap->state_bits, ADAP_STREAMING);
 skip_feed_stop:
 
@@ -550,7 +550,7 @@ static int dvb_usb_fe_init(struct dvb_frontend *fe)
 err:
 	if (!adap->suspend_resume_active) {
 		clear_bit(ADAP_INIT, &adap->state_bits);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		wake_up_bit(&adap->state_bits, ADAP_INIT);
 	}
 
@@ -591,7 +591,7 @@ err:
 	if (!adap->suspend_resume_active) {
 		adap->active_fe = -1;
 		clear_bit(ADAP_SLEEP, &adap->state_bits);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		wake_up_bit(&adap->state_bits, ADAP_SLEEP);
 	}
 
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
index 66e145bc11da..63b4b55a452f 100644
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
@@ -1837,10 +1837,10 @@ void bnx2x_sp_event(struct bnx2x_fastpath *fp, union eth_rx_cqe *rr_cqe)
 	/* SRIOV: reschedule any 'in_progress' operations */
 	bnx2x_iov_sp_event(bp, cid, true);
 
-	smp_mb__before_atomic_inc();
+	smp_mb__before_atomic();
 	atomic_inc(&bp->cq_spq_left);
 	/* push the change in bp->spq_left and towards the memory */
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 
 	DP(BNX2X_MSG_SP, "bp->cq_spq_left %x\n", atomic_read(&bp->cq_spq_left));
 
@@ -1855,11 +1855,11 @@ void bnx2x_sp_event(struct bnx2x_fastpath *fp, union eth_rx_cqe *rr_cqe)
 		 * sp_state is cleared, and this order prevents
 		 * races
 		 */
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		set_bit(BNX2X_AFEX_PENDING_VIFSET_MCP_ACK, &bp->sp_state);
 		wmb();
 		clear_bit(BNX2X_AFEX_FCOE_Q_UPDATE_PENDING, &bp->sp_state);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		/* schedule the sp task as mcp ack is required */
 		bnx2x_schedule_sp_task(bp);
@@ -5137,9 +5137,9 @@ static void bnx2x_after_function_update(struct bnx2x *bp)
 		__clear_bit(RAMROD_COMP_WAIT, &queue_params.ramrod_flags);
 
 		/* mark latest Q bit */
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		set_bit(BNX2X_AFEX_FCOE_Q_UPDATE_PENDING, &bp->sp_state);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		/* send Q update ramrod for FCoE Q */
 		rc = bnx2x_queue_state_change(bp, &queue_params);
@@ -5368,7 +5368,7 @@ next_spqe:
 		spqe_cnt++;
 	} /* for */
 
-	smp_mb__before_atomic_inc();
+	smp_mb__before_atomic();
 	atomic_add(spqe_cnt, &bp->eq_spq_left);
 
 	bp->eq_cons = sw_cons;
@@ -13723,9 +13723,9 @@ static int bnx2x_drv_ctl(struct net_device *dev, struct drv_ctl_info *ctl)
 	case DRV_CTL_RET_L2_SPQ_CREDIT_CMD: {
 		int count = ctl->data.credit.credit_count;
 
-		smp_mb__before_atomic_inc();
+		smp_mb__before_atomic();
 		atomic_add(count, &bp->cq_spq_left);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		break;
 	}
 	case DRV_CTL_ULP_REGISTER_CMD: {
diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sp.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sp.c
index 7ee099caee2f..d3356e893700 100644
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sp.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sp.c
@@ -258,16 +258,16 @@ static bool bnx2x_raw_check_pending(struct bnx2x_raw_obj *o)
 
 static void bnx2x_raw_clear_pending(struct bnx2x_raw_obj *o)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(o->state, o->pstate);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static void bnx2x_raw_set_pending(struct bnx2x_raw_obj *o)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	set_bit(o->state, o->pstate);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 /**
@@ -2131,7 +2131,7 @@ static int bnx2x_set_rx_mode_e1x(struct bnx2x *bp,
 
 	/* The operation is completed */
 	clear_bit(p->state, p->pstate);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return 0;
 }
@@ -3576,16 +3576,16 @@ error_exit1:
 
 static void bnx2x_mcast_clear_sched(struct bnx2x_mcast_obj *o)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(o->sched_state, o->raw.pstate);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static void bnx2x_mcast_set_sched(struct bnx2x_mcast_obj *o)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	set_bit(o->sched_state, o->raw.pstate);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static bool bnx2x_mcast_check_sched(struct bnx2x_mcast_obj *o)
@@ -4210,7 +4210,7 @@ int bnx2x_queue_state_change(struct bnx2x *bp,
 		if (rc) {
 			o->next_state = BNX2X_Q_STATE_MAX;
 			clear_bit(pending_bit, pending);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 			return rc;
 		}
 
@@ -4298,7 +4298,7 @@ static int bnx2x_queue_comp_cmd(struct bnx2x *bp,
 	wmb();
 
 	clear_bit(cmd, &o->pending);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return 0;
 }
@@ -5242,7 +5242,7 @@ static inline int bnx2x_func_state_change_comp(struct bnx2x *bp,
 	wmb();
 
 	clear_bit(cmd, &o->pending);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return 0;
 }
@@ -5877,7 +5877,7 @@ int bnx2x_func_state_change(struct bnx2x *bp,
 		if (rc) {
 			o->next_state = BNX2X_F_STATE_MAX;
 			clear_bit(cmd, pending);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 			return rc;
 		}
 
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
diff --git a/drivers/net/ethernet/broadcom/cnic.c b/drivers/net/ethernet/broadcom/cnic.c
index bf82f215c15b..cb4a6657e780 100644
--- a/drivers/net/ethernet/broadcom/cnic.c
+++ b/drivers/net/ethernet/broadcom/cnic.c
@@ -436,7 +436,7 @@ static int cnic_offld_prep(struct cnic_sock *csk)
 static int cnic_close_prep(struct cnic_sock *csk)
 {
 	clear_bit(SK_F_CONNECT_START, &csk->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	if (test_and_clear_bit(SK_F_OFFLD_COMPLETE, &csk->flags)) {
 		while (test_and_set_bit(SK_F_OFFLD_SCHED, &csk->flags))
@@ -450,7 +450,7 @@ static int cnic_close_prep(struct cnic_sock *csk)
 static int cnic_abort_prep(struct cnic_sock *csk)
 {
 	clear_bit(SK_F_CONNECT_START, &csk->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	while (test_and_set_bit(SK_F_OFFLD_SCHED, &csk->flags))
 		msleep(1);
@@ -3639,7 +3639,7 @@ static int cnic_cm_destroy(struct cnic_sock *csk)
 
 	csk_hold(csk);
 	clear_bit(SK_F_INUSE, &csk->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	while (atomic_read(&csk->ref_count) != 1)
 		msleep(1);
 	cnic_cm_cleanup(csk);
@@ -4019,7 +4019,7 @@ static void cnic_cm_process_kcqe(struct cnic_dev *dev, struct kcqe *kcqe)
 			 L4_KCQE_COMPLETION_STATUS_PARITY_ERROR)
 			set_bit(SK_F_HW_ERR, &csk->flags);
 
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(SK_F_OFFLD_SCHED, &csk->flags);
 		cnic_cm_upcall(cp, csk, opcode);
 		break;
diff --git a/drivers/net/ethernet/brocade/bna/bnad.c b/drivers/net/ethernet/brocade/bna/bnad.c
index c892d6f45891..b0bedad872c6 100644
--- a/drivers/net/ethernet/brocade/bna/bnad.c
+++ b/drivers/net/ethernet/brocade/bna/bnad.c
@@ -249,7 +249,7 @@ bnad_tx_complete(struct bnad *bnad, struct bna_tcb *tcb)
 	if (likely(test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)))
 		bna_ib_ack(tcb->i_dbell, sent);
 
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);
 
 	return sent;
@@ -1126,7 +1126,7 @@ bnad_tx_cleanup(struct delayed_work *work)
 
 		bnad_txq_cleanup(bnad, tcb);
 
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);
 	}
 
@@ -3002,7 +3002,7 @@ bnad_start_xmit(struct sk_buff *skb, struct net_device *netdev)
 			sent = bnad_txcmpl_process(bnad, tcb);
 			if (likely(test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)))
 				bna_ib_ack(tcb->i_dbell, sent);
-			smp_mb__before_clear_bit();
+			smp_mb__before_atomic();
 			clear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);
 		} else {
 			netif_stop_queue(netdev);
diff --git a/drivers/net/ethernet/chelsio/cxgb/cxgb2.c b/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
index 9624cfe7df57..42dd5fa2e243 100644
--- a/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
+++ b/drivers/net/ethernet/chelsio/cxgb/cxgb2.c
@@ -283,7 +283,7 @@ static int cxgb_close(struct net_device *dev)
 	if (adapter->params.stats_update_period &&
 	    !(adapter->open_device_map & PORT_MASK)) {
 		/* Stop statistics accumulation. */
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		spin_lock(&adapter->work_lock);   /* sync with update task */
 		spin_unlock(&adapter->work_lock);
 		cancel_mac_stats_update(adapter);
diff --git a/drivers/net/ethernet/chelsio/cxgb3/sge.c b/drivers/net/ethernet/chelsio/cxgb3/sge.c
index 674deb0435ab..568f78ebd0be 100644
--- a/drivers/net/ethernet/chelsio/cxgb3/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/sge.c
@@ -1379,7 +1379,7 @@ static inline int check_desc_avail(struct adapter *adap, struct sge_txq *q,
 		struct sge_qset *qs = txq_to_qset(q, qid);
 
 		set_bit(qid, &qs->txq_stopped);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		if (should_restart_tx(q) &&
 		    test_and_clear_bit(qid, &qs->txq_stopped))
@@ -1492,7 +1492,7 @@ static void restart_ctrlq(unsigned long data)
 
 	if (!skb_queue_empty(&q->sendq)) {
 		set_bit(TXQ_CTRL, &qs->txq_stopped);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		if (should_restart_tx(q) &&
 		    test_and_clear_bit(TXQ_CTRL, &qs->txq_stopped))
@@ -1696,7 +1696,7 @@ again:	reclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);
 
 		if (unlikely(q->size - q->in_use < ndesc)) {
 			set_bit(TXQ_OFLD, &qs->txq_stopped);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 
 			if (should_restart_tx(q) &&
 			    test_and_clear_bit(TXQ_OFLD, &qs->txq_stopped))
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index cc3511a5cd0c..78494b2f6a83 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -2001,7 +2001,7 @@ static void sge_rx_timer_cb(unsigned long data)
 			struct sge_fl *fl = s->egr_map[id];
 
 			clear_bit(id, s->starving_fl);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 
 			if (fl_starving(fl)) {
 				rxq = container_of(fl, struct sge_eth_rxq, fl);
diff --git a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
index 0a89963c48ce..bcf035b84640 100644
--- a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
@@ -1951,7 +1951,7 @@ static void sge_rx_timer_cb(unsigned long data)
 			struct sge_fl *fl = s->egr_map[id];
 
 			clear_bit(id, s->starving_fl);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 
 			/*
 			 * Since we are accessing fl without a lock there's a
* Unmerged path drivers/net/ethernet/freescale/gianfar.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_main.c b/drivers/net/ethernet/intel/i40e/i40e_main.c
index 1ba8c8967077..e2cd76d96018 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@ -4651,7 +4651,7 @@ static void i40e_service_event_complete(struct i40e_pf *pf)
 	BUG_ON(!test_bit(__I40E_SERVICE_SCHED, &pf->state));
 
 	/* flush memory to make sure state is correct before next watchog */
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(__I40E_SERVICE_SCHED, &pf->state);
 }
 
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
* Unmerged path drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
diff --git a/drivers/net/wireless/ti/wlcore/main.c b/drivers/net/wireless/ti/wlcore/main.c
index 953111a502ee..2f77ab35ffab 100644
--- a/drivers/net/wireless/ti/wlcore/main.c
+++ b/drivers/net/wireless/ti/wlcore/main.c
@@ -548,7 +548,7 @@ static int wlcore_irq_locked(struct wl1271 *wl)
 		 * wl1271_ps_elp_wakeup cannot be called concurrently.
 		 */
 		clear_bit(WL1271_FLAG_IRQ_RUNNING, &wl->flags);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		ret = wlcore_fw_status(wl, wl->fw_status_1, wl->fw_status_2);
 		if (ret < 0)
diff --git a/drivers/pci/xen-pcifront.c b/drivers/pci/xen-pcifront.c
index f7197a790341..0282f1fe9c42 100644
--- a/drivers/pci/xen-pcifront.c
+++ b/drivers/pci/xen-pcifront.c
@@ -655,9 +655,9 @@ static void pcifront_do_aer(struct work_struct *data)
 	notify_remote_via_evtchn(pdev->evtchn);
 
 	/*in case of we lost an aer request in four lines time_window*/
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(_PDEVB_op_active, &pdev->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	schedule_pcifront_aer_op(pdev);
 
diff --git a/drivers/scsi/isci/remote_device.c b/drivers/scsi/isci/remote_device.c
index 96a26f454673..cc51f38b116d 100644
--- a/drivers/scsi/isci/remote_device.c
+++ b/drivers/scsi/isci/remote_device.c
@@ -1541,7 +1541,7 @@ void isci_remote_device_release(struct kref *kref)
 	clear_bit(IDEV_STOP_PENDING, &idev->flags);
 	clear_bit(IDEV_IO_READY, &idev->flags);
 	clear_bit(IDEV_GONE, &idev->flags);
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(IDEV_ALLOCATED, &idev->flags);
 	wake_up(&ihost->eventq);
 }
diff --git a/drivers/target/loopback/tcm_loop.c b/drivers/target/loopback/tcm_loop.c
index 7c908141cc8a..61b9600e54ee 100644
--- a/drivers/target/loopback/tcm_loop.c
+++ b/drivers/target/loopback/tcm_loop.c
@@ -826,7 +826,7 @@ static int tcm_loop_port_link(
 	struct tcm_loop_hba *tl_hba = tl_tpg->tl_hba;
 
 	atomic_inc(&tl_tpg->tl_tpg_port_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	/*
 	 * Add Linux/SCSI struct scsi_device by HCTL
 	 */
@@ -861,7 +861,7 @@ static void tcm_loop_port_unlink(
 	scsi_device_put(sd);
 
 	atomic_dec(&tl_tpg->tl_tpg_port_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 
 	pr_debug("TCM_Loop_ConfigFS: Port Unlink Successful\n");
 }
* Unmerged path drivers/target/target_core_alua.c
diff --git a/drivers/target/target_core_device.c b/drivers/target/target_core_device.c
index 4630481b6043..d9b0f64ad4f2 100644
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@ -223,7 +223,7 @@ struct se_dev_entry *core_get_se_deve_from_rtpi(
 			continue;
 
 		atomic_inc(&deve->pr_ref_count);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		spin_unlock_irq(&nacl->device_list_lock);
 
 		return deve;
@@ -1275,7 +1275,7 @@ int core_dev_add_initiator_node_lun_acl(
 	spin_lock(&lun->lun_acl_lock);
 	list_add_tail(&lacl->lacl_list, &lun->lun_acl_list);
 	atomic_inc(&lun->lun_acl_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	spin_unlock(&lun->lun_acl_lock);
 
 	pr_debug("%s_TPG[%hu]_LUN[%u->%u] - Added %s ACL for "
@@ -1309,7 +1309,7 @@ int core_dev_del_initiator_node_lun_acl(
 	spin_lock(&lun->lun_acl_lock);
 	list_del(&lacl->lacl_list);
 	atomic_dec(&lun->lun_acl_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 	spin_unlock(&lun->lun_acl_lock);
 
 	core_disable_device_list_for_node(lun, NULL, lacl->mapped_lun,
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index caf05399990f..862c2cf7d891 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -289,7 +289,7 @@ static void iblock_bio_done(struct bio *bio, int err)
 		 * Bump the ib_bio_err_cnt and release bio.
 		 */
 		atomic_inc(&ibr->ib_bio_err_cnt);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 	}
 
 	bio_put(bio);
diff --git a/drivers/target/target_core_pr.c b/drivers/target/target_core_pr.c
index 04a74938bb43..07b11d10a005 100644
--- a/drivers/target/target_core_pr.c
+++ b/drivers/target/target_core_pr.c
@@ -673,7 +673,7 @@ static struct t10_pr_registration *__core_scsi3_alloc_registration(
 	spin_lock(&dev->se_port_lock);
 	list_for_each_entry_safe(port, port_tmp, &dev->dev_sep_list, sep_list) {
 		atomic_inc(&port->sep_tg_pt_ref_cnt);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		spin_unlock(&dev->se_port_lock);
 
 		spin_lock_bh(&port->sep_alua_lock);
@@ -708,7 +708,7 @@ static struct t10_pr_registration *__core_scsi3_alloc_registration(
 				continue;
 
 			atomic_inc(&deve_tmp->pr_ref_count);
-			smp_mb__after_atomic_inc();
+			smp_mb__after_atomic();
 			spin_unlock_bh(&port->sep_alua_lock);
 			/*
 			 * Grab a configfs group dependency that is released
@@ -721,9 +721,9 @@ static struct t10_pr_registration *__core_scsi3_alloc_registration(
 				pr_err("core_scsi3_lunacl_depend"
 						"_item() failed\n");
 				atomic_dec(&port->sep_tg_pt_ref_cnt);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				atomic_dec(&deve_tmp->pr_ref_count);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				goto out;
 			}
 			/*
@@ -738,9 +738,9 @@ static struct t10_pr_registration *__core_scsi3_alloc_registration(
 						sa_res_key, all_tg_pt, aptpl);
 			if (!pr_reg_atp) {
 				atomic_dec(&port->sep_tg_pt_ref_cnt);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				atomic_dec(&deve_tmp->pr_ref_count);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				core_scsi3_lunacl_undepend_item(deve_tmp);
 				goto out;
 			}
@@ -753,7 +753,7 @@ static struct t10_pr_registration *__core_scsi3_alloc_registration(
 
 		spin_lock(&dev->se_port_lock);
 		atomic_dec(&port->sep_tg_pt_ref_cnt);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 	}
 	spin_unlock(&dev->se_port_lock);
 
@@ -1113,7 +1113,7 @@ static struct t10_pr_registration *__core_scsi3_locate_pr_reg(
 					continue;
 			}
 			atomic_inc(&pr_reg->pr_res_holders);
-			smp_mb__after_atomic_inc();
+			smp_mb__after_atomic();
 			spin_unlock(&pr_tmpl->registration_lock);
 			return pr_reg;
 		}
@@ -1128,7 +1128,7 @@ static struct t10_pr_registration *__core_scsi3_locate_pr_reg(
 			continue;
 
 		atomic_inc(&pr_reg->pr_res_holders);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		spin_unlock(&pr_tmpl->registration_lock);
 		return pr_reg;
 	}
@@ -1158,7 +1158,7 @@ static struct t10_pr_registration *core_scsi3_locate_pr_reg(
 static void core_scsi3_put_pr_reg(struct t10_pr_registration *pr_reg)
 {
 	atomic_dec(&pr_reg->pr_res_holders);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static int core_scsi3_check_implict_release(
@@ -1356,7 +1356,7 @@ static void core_scsi3_tpg_undepend_item(struct se_portal_group *tpg)
 			&tpg->tpg_group.cg_item);
 
 	atomic_dec(&tpg->tpg_pr_ref_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static int core_scsi3_nodeacl_depend_item(struct se_node_acl *nacl)
@@ -1376,7 +1376,7 @@ static void core_scsi3_nodeacl_undepend_item(struct se_node_acl *nacl)
 
 	if (nacl->dynamic_node_acl) {
 		atomic_dec(&nacl->acl_pr_ref_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 		return;
 	}
 
@@ -1384,7 +1384,7 @@ static void core_scsi3_nodeacl_undepend_item(struct se_node_acl *nacl)
 			&nacl->acl_group.cg_item);
 
 	atomic_dec(&nacl->acl_pr_ref_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static int core_scsi3_lunacl_depend_item(struct se_dev_entry *se_deve)
@@ -1415,7 +1415,7 @@ static void core_scsi3_lunacl_undepend_item(struct se_dev_entry *se_deve)
 	 */
 	if (!lun_acl) {
 		atomic_dec(&se_deve->pr_ref_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 		return;
 	}
 	nacl = lun_acl->se_lun_nacl;
@@ -1425,7 +1425,7 @@ static void core_scsi3_lunacl_undepend_item(struct se_dev_entry *se_deve)
 			&lun_acl->se_lun_group.cg_item);
 
 	atomic_dec(&se_deve->pr_ref_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static sense_reason_t
@@ -1559,14 +1559,14 @@ core_scsi3_decode_spec_i_port(
 				continue;
 
 			atomic_inc(&tmp_tpg->tpg_pr_ref_count);
-			smp_mb__after_atomic_inc();
+			smp_mb__after_atomic();
 			spin_unlock(&dev->se_port_lock);
 
 			if (core_scsi3_tpg_depend_item(tmp_tpg)) {
 				pr_err(" core_scsi3_tpg_depend_item()"
 					" for tmp_tpg\n");
 				atomic_dec(&tmp_tpg->tpg_pr_ref_count);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 				goto out_unmap;
 			}
@@ -1580,7 +1580,7 @@ core_scsi3_decode_spec_i_port(
 						tmp_tpg, i_str);
 			if (dest_node_acl) {
 				atomic_inc(&dest_node_acl->acl_pr_ref_count);
-				smp_mb__after_atomic_inc();
+				smp_mb__after_atomic();
 			}
 			spin_unlock_irq(&tmp_tpg->acl_node_lock);
 
@@ -1594,7 +1594,7 @@ core_scsi3_decode_spec_i_port(
 				pr_err("configfs_depend_item() failed"
 					" for dest_node_acl->acl_group\n");
 				atomic_dec(&dest_node_acl->acl_pr_ref_count);
-				smp_mb__after_atomic_dec();
+				smp_mb__after_atomic();
 				core_scsi3_tpg_undepend_item(tmp_tpg);
 				ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 				goto out_unmap;
@@ -1654,7 +1654,7 @@ core_scsi3_decode_spec_i_port(
 			pr_err("core_scsi3_lunacl_depend_item()"
 					" failed\n");
 			atomic_dec(&dest_se_deve->pr_ref_count);
-			smp_mb__after_atomic_dec();
+			smp_mb__after_atomic();
 			core_scsi3_nodeacl_undepend_item(dest_node_acl);
 			core_scsi3_tpg_undepend_item(dest_tpg);
 			ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
@@ -3308,14 +3308,14 @@ core_scsi3_emulate_pro_register_and_move(struct se_cmd *cmd, u64 res_key,
 			continue;
 
 		atomic_inc(&dest_se_tpg->tpg_pr_ref_count);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		spin_unlock(&dev->se_port_lock);
 
 		if (core_scsi3_tpg_depend_item(dest_se_tpg)) {
 			pr_err("core_scsi3_tpg_depend_item() failed"
 				" for dest_se_tpg\n");
 			atomic_dec(&dest_se_tpg->tpg_pr_ref_count);
-			smp_mb__after_atomic_dec();
+			smp_mb__after_atomic();
 			ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 			goto out_put_pr_reg;
 		}
@@ -3413,7 +3413,7 @@ after_iport_check:
 				initiator_str);
 	if (dest_node_acl) {
 		atomic_inc(&dest_node_acl->acl_pr_ref_count);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 	}
 	spin_unlock_irq(&dest_se_tpg->acl_node_lock);
 
@@ -3429,7 +3429,7 @@ after_iport_check:
 		pr_err("core_scsi3_nodeacl_depend_item() for"
 			" dest_node_acl\n");
 		atomic_dec(&dest_node_acl->acl_pr_ref_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 		dest_node_acl = NULL;
 		ret = TCM_INVALID_PARAMETER_LIST;
 		goto out;
@@ -3454,7 +3454,7 @@ after_iport_check:
 	if (core_scsi3_lunacl_depend_item(dest_se_deve)) {
 		pr_err("core_scsi3_lunacl_depend_item() failed\n");
 		atomic_dec(&dest_se_deve->pr_ref_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 		dest_se_deve = NULL;
 		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 		goto out;
@@ -4038,7 +4038,7 @@ core_scsi3_pri_read_full_status(struct se_cmd *cmd)
 		add_desc_len = 0;
 
 		atomic_inc(&pr_reg->pr_res_holders);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		spin_unlock(&pr_tmpl->registration_lock);
 		/*
 		 * Determine expected length of $FABRIC_MOD specific
@@ -4052,7 +4052,7 @@ core_scsi3_pri_read_full_status(struct se_cmd *cmd)
 				" out of buffer: %d\n", cmd->data_length);
 			spin_lock(&pr_tmpl->registration_lock);
 			atomic_dec(&pr_reg->pr_res_holders);
-			smp_mb__after_atomic_dec();
+			smp_mb__after_atomic();
 			break;
 		}
 		/*
@@ -4114,7 +4114,7 @@ core_scsi3_pri_read_full_status(struct se_cmd *cmd)
 
 		spin_lock(&pr_tmpl->registration_lock);
 		atomic_dec(&pr_reg->pr_res_holders);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 		/*
 		 * Set the ADDITIONAL DESCRIPTOR LENGTH
 		 */
* Unmerged path drivers/target/target_core_transport.c
diff --git a/drivers/target/target_core_ua.c b/drivers/target/target_core_ua.c
index bf0e390ce2d7..7973e6c577c9 100644
--- a/drivers/target/target_core_ua.c
+++ b/drivers/target/target_core_ua.c
@@ -163,7 +163,7 @@ int core_scsi3_ua_allocate(
 		spin_unlock_irq(&nacl->device_list_lock);
 
 		atomic_inc(&deve->ua_count);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		return 0;
 	}
 	list_add_tail(&ua->ua_nacl_list, &deve->ua_list);
@@ -176,7 +176,7 @@ int core_scsi3_ua_allocate(
 		asc, ascq);
 
 	atomic_inc(&deve->ua_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	return 0;
 }
 
@@ -191,7 +191,7 @@ void core_scsi3_ua_release_all(
 		kmem_cache_free(se_ua_cache, ua);
 
 		atomic_dec(&deve->ua_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 	}
 	spin_unlock(&deve->ua_lock);
 }
@@ -252,7 +252,7 @@ void core_scsi3_ua_for_check_condition(
 		kmem_cache_free(se_ua_cache, ua);
 
 		atomic_dec(&deve->ua_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 	}
 	spin_unlock(&deve->ua_lock);
 	spin_unlock_irq(&nacl->device_list_lock);
@@ -311,7 +311,7 @@ int core_scsi3_ua_clear_for_request_sense(
 		kmem_cache_free(se_ua_cache, ua);
 
 		atomic_dec(&deve->ua_count);
-		smp_mb__after_atomic_dec();
+		smp_mb__after_atomic();
 	}
 	spin_unlock(&deve->ua_lock);
 	spin_unlock_irq(&nacl->device_list_lock);
* Unmerged path drivers/tty/n_tty.c
diff --git a/drivers/tty/serial/mxs-auart.c b/drivers/tty/serial/mxs-auart.c
index f85b8e6d0346..497cd1bdfe17 100644
--- a/drivers/tty/serial/mxs-auart.c
+++ b/drivers/tty/serial/mxs-auart.c
@@ -200,7 +200,7 @@ static void dma_tx_callback(void *param)
 
 	/* clear the bit used to serialize the DMA tx. */
 	clear_bit(MXS_AUART_DMA_TX_SYNC, &s->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	/* wake up the possible processes. */
 	if (uart_circ_chars_pending(xmit) < WAKEUP_CHARS)
@@ -275,7 +275,7 @@ static void mxs_auart_tx_chars(struct mxs_auart_port *s)
 			mxs_auart_dma_tx(s, i);
 		} else {
 			clear_bit(MXS_AUART_DMA_TX_SYNC, &s->flags);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 		}
 		return;
 	}
diff --git a/drivers/usb/gadget/tcm_usb_gadget.c b/drivers/usb/gadget/tcm_usb_gadget.c
index 7cacd6ae818e..dd7aa4d58730 100644
--- a/drivers/usb/gadget/tcm_usb_gadget.c
+++ b/drivers/usb/gadget/tcm_usb_gadget.c
@@ -1847,7 +1847,7 @@ static int usbg_port_link(struct se_portal_group *se_tpg, struct se_lun *lun)
 	struct usbg_tpg *tpg = container_of(se_tpg, struct usbg_tpg, se_tpg);
 
 	atomic_inc(&tpg->tpg_port_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	return 0;
 }
 
@@ -1857,7 +1857,7 @@ static void usbg_port_unlink(struct se_portal_group *se_tpg,
 	struct usbg_tpg *tpg = container_of(se_tpg, struct usbg_tpg, se_tpg);
 
 	atomic_dec(&tpg->tpg_port_count);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static int usbg_check_stop_free(struct se_cmd *se_cmd)
diff --git a/drivers/usb/serial/usb_wwan.c b/drivers/usb/serial/usb_wwan.c
index 85365784040b..ee0f2d301f20 100644
--- a/drivers/usb/serial/usb_wwan.c
+++ b/drivers/usb/serial/usb_wwan.c
@@ -325,7 +325,7 @@ static void usb_wwan_outdat_callback(struct urb *urb)
 
 	for (i = 0; i < N_OUT_URB; ++i) {
 		if (portdata->out_urbs[i] == urb) {
-			smp_mb__before_clear_bit();
+			smp_mb__before_atomic();
 			clear_bit(i, &portdata->out_busy);
 			break;
 		}
* Unmerged path drivers/vhost/scsi.c
diff --git a/drivers/w1/w1_family.c b/drivers/w1/w1_family.c
index e9309778ee72..f4ace0398534 100644
--- a/drivers/w1/w1_family.c
+++ b/drivers/w1/w1_family.c
@@ -131,9 +131,9 @@ void w1_family_get(struct w1_family *f)
 
 void __w1_family_get(struct w1_family *f)
 {
-	smp_mb__before_atomic_inc();
+	smp_mb__before_atomic();
 	atomic_inc(&f->refcnt);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 EXPORT_SYMBOL(w1_unregister_family);
diff --git a/drivers/xen/xen-pciback/pciback_ops.c b/drivers/xen/xen-pciback/pciback_ops.c
index 64eb0cd8b8af..27c2c8820540 100644
--- a/drivers/xen/xen-pciback/pciback_ops.c
+++ b/drivers/xen/xen-pciback/pciback_ops.c
@@ -348,9 +348,9 @@ void xen_pcibk_do_op(struct work_struct *data)
 	notify_remote_via_irq(pdev->evtchn_irq);
 
 	/* Mark that we're done. */
-	smp_mb__before_clear_bit(); /* /after/ clearing PCIF_active */
+	smp_mb__before_atomic(); /* /after/ clearing PCIF_active */
 	clear_bit(_PDEVF_op_active, &pdev->flags);
-	smp_mb__after_clear_bit(); /* /before/ final check for work */
+	smp_mb__after_atomic(); /* /before/ final check for work */
 
 	/* Check to see if the driver domain tried to start another request in
 	 * between clearing _XEN_PCIF_active and clearing _PDEVF_op_active.
diff --git a/fs/btrfs/btrfs_inode.h b/fs/btrfs/btrfs_inode.h
index a0cf3e56fe20..4794923c410c 100644
--- a/fs/btrfs/btrfs_inode.h
+++ b/fs/btrfs/btrfs_inode.h
@@ -279,7 +279,7 @@ static inline void btrfs_inode_block_unlocked_dio(struct inode *inode)
 
 static inline void btrfs_inode_resume_unlocked_dio(struct inode *inode)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(BTRFS_INODE_READDIO_NEED_LOCK,
 		  &BTRFS_I(inode)->runtime_flags);
 }
diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
index 2138a56bf5a0..fc553352be75 100644
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -3473,7 +3473,7 @@ lock_extent_buffer_for_io(struct extent_buffer *eb,
 static void end_extent_buffer_writeback(struct extent_buffer *eb)
 {
 	clear_bit(EXTENT_BUFFER_WRITEBACK, &eb->bflags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&eb->bflags, EXTENT_BUFFER_WRITEBACK);
 }
 
diff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c
index 8dd528f713f6..a7f6ba86d493 100644
--- a/fs/btrfs/inode.c
+++ b/fs/btrfs/inode.c
@@ -7137,7 +7137,7 @@ static void btrfs_end_dio_bio(struct bio *bio, int err)
 		 * before atomic variable goto zero, we must make sure
 		 * dip->errors is perceived to be set.
 		 */
-		smp_mb__before_atomic_dec();
+		smp_mb__before_atomic();
 	}
 
 	/* if there are more bios still pending for this dio, just exit */
@@ -7317,7 +7317,7 @@ out_err:
 	 * before atomic variable goto zero, we must
 	 * make sure dip->errors is perceived to be set.
 	 */
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	if (atomic_dec_and_test(&dip->pending_bios))
 		bio_io_error(dip->orig_bio);
 
@@ -7459,7 +7459,7 @@ static ssize_t btrfs_direct_IO(int rw, struct kiocb *iocb,
 		return 0;
 
 	atomic_inc(&inode->i_dio_count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 
 	/*
 	 * The generic stuff only does filemap_write_and_wait_range, which
diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c
index 2bac83258b4b..e968d9b3e4ad 100644
--- a/fs/btrfs/ioctl.c
+++ b/fs/btrfs/ioctl.c
@@ -643,7 +643,7 @@ static int create_snapshot(struct btrfs_root *root, struct inode *dir,
 		return -EINVAL;
 
 	atomic_inc(&root->will_be_snapshoted);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	btrfs_wait_nocow_write(root);
 
 	ret = btrfs_start_delalloc_inodes(root, 0);
diff --git a/fs/buffer.c b/fs/buffer.c
index 14665e6445c0..d52f0ab6ceeb 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -77,7 +77,7 @@ EXPORT_SYMBOL(__lock_buffer);
 void unlock_buffer(struct buffer_head *bh)
 {
 	clear_bit_unlock(BH_Lock, &bh->b_state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&bh->b_state, BH_Lock);
 }
 EXPORT_SYMBOL(unlock_buffer);
diff --git a/fs/ext4/resize.c b/fs/ext4/resize.c
index 9579efd5d406..85ba2efee2a2 100644
--- a/fs/ext4/resize.c
+++ b/fs/ext4/resize.c
@@ -42,7 +42,7 @@ int ext4_resize_begin(struct super_block *sb)
 void ext4_resize_end(struct super_block *sb)
 {
 	clear_bit_unlock(EXT4_RESIZING, &EXT4_SB(sb)->s_resize_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static ext4_group_t ext4_meta_bg_first_group(struct super_block *sb,
diff --git a/fs/gfs2/glock.c b/fs/gfs2/glock.c
index bd3f86130bbe..5ec5e006a611 100644
--- a/fs/gfs2/glock.c
+++ b/fs/gfs2/glock.c
@@ -275,7 +275,7 @@ static inline int may_grant(const struct gfs2_glock *gl, const struct gfs2_holde
 static void gfs2_holder_wake(struct gfs2_holder *gh)
 {
 	clear_bit(HIF_WAIT, &gh->gh_iflags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&gh->gh_iflags, HIF_WAIT);
 }
 
@@ -409,7 +409,7 @@ static void gfs2_demote_wake(struct gfs2_glock *gl)
 {
 	gl->gl_demote_state = LM_ST_EXCLUSIVE;
 	clear_bit(GLF_DEMOTE, &gl->gl_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&gl->gl_flags, GLF_DEMOTE);
 }
 
@@ -618,7 +618,7 @@ out:
 
 out_sched:
 	clear_bit(GLF_LOCK, &gl->gl_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	gl->gl_lockref.count++;
 	if (queue_delayed_work(glock_workqueue, &gl->gl_work, 0) == 0)
 		gl->gl_lockref.count--;
@@ -626,7 +626,7 @@ out_sched:
 
 out_unlock:
 	clear_bit(GLF_LOCK, &gl->gl_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	return;
 }
 
diff --git a/fs/gfs2/glops.c b/fs/gfs2/glops.c
index 48654eaae8b6..1bee2a6da21a 100644
--- a/fs/gfs2/glops.c
+++ b/fs/gfs2/glops.c
@@ -225,7 +225,7 @@ static void inode_go_sync(struct gfs2_glock *gl)
 	 * Writeback of the data mapping may cause the dirty flag to be set
 	 * so we have to clear it again here.
 	 */
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(GLF_DIRTY, &gl->gl_flags);
 }
 
diff --git a/fs/gfs2/lock_dlm.c b/fs/gfs2/lock_dlm.c
index c8423d6de6c3..59a1bf32d9d3 100644
--- a/fs/gfs2/lock_dlm.c
+++ b/fs/gfs2/lock_dlm.c
@@ -1132,7 +1132,7 @@ static void gdlm_recover_done(void *arg, struct dlm_slot *slots, int num_slots,
 		queue_delayed_work(gfs2_control_wq, &sdp->sd_control_work, 0);
 
 	clear_bit(DFL_DLM_RECOVERY, &ls->ls_recover_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&ls->ls_recover_flags, DFL_DLM_RECOVERY);
 	spin_unlock(&ls->ls_recover_spin);
 }
@@ -1269,7 +1269,7 @@ static int gdlm_mount(struct gfs2_sbd *sdp, const char *table)
 
 	ls->ls_first = !!test_bit(DFL_FIRST_MOUNT, &ls->ls_recover_flags);
 	clear_bit(SDF_NOJOURNALID, &sdp->sd_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&sdp->sd_flags, SDF_NOJOURNALID);
 	return 0;
 
diff --git a/fs/gfs2/recovery.c b/fs/gfs2/recovery.c
index 7ad4094d68c0..fe7a56fb6084 100644
--- a/fs/gfs2/recovery.c
+++ b/fs/gfs2/recovery.c
@@ -587,7 +587,7 @@ fail:
 	gfs2_recovery_done(sdp, jd->jd_jid, LM_RD_GAVEUP);
 done:
 	clear_bit(JDF_RECOVERY, &jd->jd_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&jd->jd_flags, JDF_RECOVERY);
 }
 
diff --git a/fs/gfs2/sys.c b/fs/gfs2/sys.c
index 70d3a3d45aff..c3b3025915a4 100644
--- a/fs/gfs2/sys.c
+++ b/fs/gfs2/sys.c
@@ -332,7 +332,7 @@ static ssize_t block_store(struct gfs2_sbd *sdp, const char *buf, size_t len)
 		set_bit(DFL_BLOCK_LOCKS, &ls->ls_recover_flags);
 	else if (val == 0) {
 		clear_bit(DFL_BLOCK_LOCKS, &ls->ls_recover_flags);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		gfs2_glock_thaw(sdp);
 	} else {
 		ret = -EINVAL;
@@ -484,7 +484,7 @@ static ssize_t jid_store(struct gfs2_sbd *sdp, const char *buf, size_t len)
 		rv = jid = -EINVAL;
 	sdp->sd_lockstruct.ls_jid = jid;
 	clear_bit(SDF_NOJOURNALID, &sdp->sd_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&sdp->sd_flags, SDF_NOJOURNALID);
 out:
 	spin_unlock(&sdp->sd_jindex_spin);
* Unmerged path fs/jbd2/commit.c
diff --git a/fs/nfs/dir.c b/fs/nfs/dir.c
index d131604cdf5a..f25756dbe3ec 100644
--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@ -1999,9 +1999,9 @@ static void nfs_access_free_entry(struct nfs_access_entry *entry)
 {
 	put_rpccred(entry->cred);
 	kfree(entry);
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_long_dec(&nfs_access_nr_entries);
-	smp_mb__after_atomic_dec();
+	smp_mb__after_atomic();
 }
 
 static void nfs_access_free_list(struct list_head *head)
@@ -2047,9 +2047,9 @@ int nfs_access_cache_shrinker(struct shrinker *shrink,
 		else {
 remove_lru_entry:
 			list_del_init(&nfsi->access_cache_inode_lru);
-			smp_mb__before_clear_bit();
+			smp_mb__before_atomic();
 			clear_bit(NFS_INO_ACL_LRU_SET, &nfsi->flags);
-			smp_mb__after_clear_bit();
+			smp_mb__after_atomic();
 		}
 		spin_unlock(&inode->i_lock);
 	}
@@ -2191,9 +2191,9 @@ void nfs_access_add_cache(struct inode *inode, struct nfs_access_entry *set)
 	nfs_access_add_rbtree(inode, cache);
 
 	/* Update accounting */
-	smp_mb__before_atomic_inc();
+	smp_mb__before_atomic();
 	atomic_long_inc(&nfs_access_nr_entries);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 
 	/* Add inode to global LRU list */
 	if (!test_bit(NFS_INO_ACL_LRU_SET, &NFS_I(inode)->flags)) {
diff --git a/fs/nfs/filelayout/filelayoutdev.c b/fs/nfs/filelayout/filelayoutdev.c
index b52cb3727be3..704930f12885 100644
--- a/fs/nfs/filelayout/filelayoutdev.c
+++ b/fs/nfs/filelayout/filelayoutdev.c
@@ -789,9 +789,9 @@ static void nfs4_wait_ds_connect(struct nfs4_pnfs_ds *ds)
 
 static void nfs4_clear_ds_conn_bit(struct nfs4_pnfs_ds *ds)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(NFS4DS_CONNECTING, &ds->ds_state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&ds->ds_state, NFS4DS_CONNECTING);
 }
 
diff --git a/fs/nfs/inode.c b/fs/nfs/inode.c
index 4dde8b333a12..f807b15d1bee 100644
--- a/fs/nfs/inode.c
+++ b/fs/nfs/inode.c
@@ -1080,7 +1080,7 @@ int nfs_revalidate_mapping(struct inode *inode, struct address_space *mapping)
 	trace_nfs_invalidate_mapping_exit(inode, ret);
 
 	clear_bit_unlock(NFS_INO_INVALIDATING, bitlock);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(bitlock, NFS_INO_INVALIDATING);
 out:
 	return ret;
diff --git a/fs/nfs/nfs4state.c b/fs/nfs/nfs4state.c
index 2cbe696804fb..183ff931fc21 100644
--- a/fs/nfs/nfs4state.c
+++ b/fs/nfs/nfs4state.c
@@ -1140,9 +1140,9 @@ static int nfs4_run_state_manager(void *);
 
 static void nfs4_clear_state_manager_bit(struct nfs_client *clp)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(NFS4CLNT_MANAGER_RUNNING, &clp->cl_state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&clp->cl_state, NFS4CLNT_MANAGER_RUNNING);
 	rpc_wake_up(&clp->cl_rpcwaitq);
 }
diff --git a/fs/nfs/pagelist.c b/fs/nfs/pagelist.c
index 00f3d2d2a504..d1f4ec4df78b 100644
--- a/fs/nfs/pagelist.c
+++ b/fs/nfs/pagelist.c
@@ -95,7 +95,7 @@ nfs_iocounter_dec(struct nfs_io_counter *c)
 {
 	if (atomic_dec_and_test(&c->io_count)) {
 		clear_bit(NFS_IO_INPROGRESS, &c->flags);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		wake_up_bit(&c->flags, NFS_IO_INPROGRESS);
 	}
 }
@@ -191,9 +191,9 @@ void nfs_unlock_request(struct nfs_page *req)
 		printk(KERN_ERR "NFS: Invalid unlock attempted\n");
 		BUG();
 	}
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(PG_BUSY, &req->wb_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&req->wb_flags, PG_BUSY);
 }
 
diff --git a/fs/nfs/pnfs.c b/fs/nfs/pnfs.c
index eed474e3fcc7..1c0aece17af0 100644
--- a/fs/nfs/pnfs.c
+++ b/fs/nfs/pnfs.c
@@ -1793,7 +1793,7 @@ static void pnfs_clear_layoutcommitting(struct inode *inode)
 	unsigned long *bitlock = &NFS_I(inode)->flags;
 
 	clear_bit_unlock(NFS_INO_LAYOUTCOMMITTING, bitlock);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(bitlock, NFS_INO_LAYOUTCOMMITTING);
 }
 
diff --git a/fs/nfs/pnfs.h b/fs/nfs/pnfs.h
index bb1f061f76db..f4a0692b11e4 100644
--- a/fs/nfs/pnfs.h
+++ b/fs/nfs/pnfs.h
@@ -271,7 +271,7 @@ pnfs_get_lseg(struct pnfs_layout_segment *lseg)
 {
 	if (lseg) {
 		atomic_inc(&lseg->pls_refcount);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 	}
 	return lseg;
 }
diff --git a/fs/nfs/write.c b/fs/nfs/write.c
index 7571542ecbdc..a601b4924e78 100644
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@ -464,7 +464,7 @@ int nfs_writepages(struct address_space *mapping, struct writeback_control *wbc)
 	nfs_pageio_complete(&pgio);
 
 	clear_bit_unlock(NFS_INO_FLUSHING, bitlock);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(bitlock, NFS_INO_FLUSHING);
 
 	if (err < 0)
@@ -1556,7 +1556,7 @@ static int nfs_commit_set_lock(struct nfs_inode *nfsi, int may_wait)
 static void nfs_commit_clear_lock(struct nfs_inode *nfsi)
 {
 	clear_bit(NFS_INO_COMMIT, &nfsi->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_bit(&nfsi->flags, NFS_INO_COMMIT);
 }
 
diff --git a/fs/ubifs/lpt_commit.c b/fs/ubifs/lpt_commit.c
index 4b826abb1528..45d4e96a6bac 100644
--- a/fs/ubifs/lpt_commit.c
+++ b/fs/ubifs/lpt_commit.c
@@ -460,9 +460,9 @@ static int write_cnodes(struct ubifs_info *c)
 		 * important.
 		 */
 		clear_bit(DIRTY_CNODE, &cnode->flags);
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(COW_CNODE, &cnode->flags);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		offs += len;
 		dbg_chk_lpt_sz(c, 1, len);
 		cnode = cnode->cnext;
diff --git a/fs/ubifs/tnc_commit.c b/fs/ubifs/tnc_commit.c
index 52a6559275c4..3600994f8411 100644
--- a/fs/ubifs/tnc_commit.c
+++ b/fs/ubifs/tnc_commit.c
@@ -895,9 +895,9 @@ static int write_index(struct ubifs_info *c)
 		 * the reason for the second barrier.
 		 */
 		clear_bit(DIRTY_ZNODE, &znode->flags);
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(COW_ZNODE, &znode->flags);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 		/*
 		 * We have marked the znode as clean but have not updated the
diff --git a/include/asm-generic/bitops/atomic.h b/include/asm-generic/bitops/atomic.h
index 9ae6c34dc191..49673510b484 100644
--- a/include/asm-generic/bitops/atomic.h
+++ b/include/asm-generic/bitops/atomic.h
@@ -80,7 +80,7 @@ static inline void set_bit(int nr, volatile unsigned long *addr)
  *
  * clear_bit() is atomic and may not be reordered.  However, it does
  * not contain a memory barrier, so if it is used for locking purposes,
- * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()
+ * you should call smp_mb__before_atomic() and/or smp_mb__after_atomic()
  * in order to ensure changes are visible on other processors.
  */
 static inline void clear_bit(int nr, volatile unsigned long *addr)
diff --git a/include/asm-generic/bitops/lock.h b/include/asm-generic/bitops/lock.h
index 308a9e22c802..c30266e94806 100644
--- a/include/asm-generic/bitops/lock.h
+++ b/include/asm-generic/bitops/lock.h
@@ -20,7 +20,7 @@
  */
 #define clear_bit_unlock(nr, addr)	\
 do {					\
-	smp_mb__before_clear_bit();	\
+	smp_mb__before_atomic();	\
 	clear_bit(nr, addr);		\
 } while (0)
 
diff --git a/include/linux/buffer_head.h b/include/linux/buffer_head.h
index f45511e6d2c9..ca31306f7b59 100644
--- a/include/linux/buffer_head.h
+++ b/include/linux/buffer_head.h
@@ -275,7 +275,7 @@ static inline void get_bh(struct buffer_head *bh)
 
 static inline void put_bh(struct buffer_head *bh)
 {
-        smp_mb__before_atomic_dec();
+        smp_mb__before_atomic();
         atomic_dec(&bh->b_count);
 }
 
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index 9f3c275e053e..ec274e0f4ed2 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -649,7 +649,7 @@ static inline void hd_ref_init(struct hd_struct *part)
 static inline void hd_struct_get(struct hd_struct *part)
 {
 	atomic_inc(&part->ref);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 static inline int hd_struct_try_get(struct hd_struct *part)
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index 5fa5afeeb759..b0c7db7d982f 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -527,7 +527,7 @@ static inline int tasklet_trylock(struct tasklet_struct *t)
 
 static inline void tasklet_unlock(struct tasklet_struct *t)
 {
-	smp_mb__before_clear_bit(); 
+	smp_mb__before_atomic();
 	clear_bit(TASKLET_STATE_RUN, &(t)->state);
 }
 
@@ -575,7 +575,7 @@ static inline void tasklet_hi_schedule_first(struct tasklet_struct *t)
 static inline void tasklet_disable_nosync(struct tasklet_struct *t)
 {
 	atomic_inc(&t->count);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 static inline void tasklet_disable(struct tasklet_struct *t)
@@ -587,13 +587,13 @@ static inline void tasklet_disable(struct tasklet_struct *t)
 
 static inline void tasklet_enable(struct tasklet_struct *t)
 {
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&t->count);
 }
 
 static inline void tasklet_hi_enable(struct tasklet_struct *t)
 {
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&t->count);
 }
 
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 42e58d229b1f..b5c20930943e 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -499,7 +499,7 @@ static inline void napi_disable(struct napi_struct *n)
 static inline void napi_enable(struct napi_struct *n)
 {
 	BUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(NAPI_STATE_SCHED, &n->state);
 }
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7212d381393c..d233be5e80ec 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2643,10 +2643,8 @@ static inline bool __must_check current_set_polling_and_test(void)
 	/*
 	 * Polling state must be visible before we test NEED_RESCHED,
 	 * paired by resched_task()
-	 *
-	 * XXX: assumes set/clear bit are identical barrier wise.
 	 */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return unlikely(tif_need_resched());
 }
@@ -2664,7 +2662,7 @@ static inline bool __must_check current_clr_polling_and_test(void)
 	 * Polling state must be visible before we test NEED_RESCHED,
 	 * paired by resched_task()
 	 */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return unlikely(tif_need_resched());
 }
diff --git a/include/linux/sunrpc/sched.h b/include/linux/sunrpc/sched.h
index 3a847de83fab..ad7dbe2cfecd 100644
--- a/include/linux/sunrpc/sched.h
+++ b/include/linux/sunrpc/sched.h
@@ -142,18 +142,18 @@ struct rpc_task_setup {
 				test_and_set_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)
 #define rpc_clear_running(t)	\
 	do { \
-		smp_mb__before_clear_bit(); \
+		smp_mb__before_atomic(); \
 		clear_bit(RPC_TASK_RUNNING, &(t)->tk_runstate); \
-		smp_mb__after_clear_bit(); \
+		smp_mb__after_atomic(); \
 	} while (0)
 
 #define RPC_IS_QUEUED(t)	test_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)
 #define rpc_set_queued(t)	set_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)
 #define rpc_clear_queued(t)	\
 	do { \
-		smp_mb__before_clear_bit(); \
+		smp_mb__before_atomic(); \
 		clear_bit(RPC_TASK_QUEUED, &(t)->tk_runstate); \
-		smp_mb__after_clear_bit(); \
+		smp_mb__after_atomic(); \
 	} while (0)
 
 #define RPC_IS_ACTIVATED(t)	test_bit(RPC_TASK_ACTIVE, &(t)->tk_runstate)
diff --git a/include/linux/sunrpc/xprt.h b/include/linux/sunrpc/xprt.h
index 5903d2c0ab4d..fcbfe8783243 100644
--- a/include/linux/sunrpc/xprt.h
+++ b/include/linux/sunrpc/xprt.h
@@ -385,9 +385,9 @@ static inline int xprt_test_and_clear_connected(struct rpc_xprt *xprt)
 
 static inline void xprt_clear_connecting(struct rpc_xprt *xprt)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(XPRT_CONNECTING, &xprt->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static inline int xprt_connecting(struct rpc_xprt *xprt)
@@ -417,9 +417,9 @@ static inline void xprt_clear_bound(struct rpc_xprt *xprt)
 
 static inline void xprt_clear_binding(struct rpc_xprt *xprt)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(XPRT_BINDING, &xprt->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static inline int xprt_test_and_set_binding(struct rpc_xprt *xprt)
diff --git a/include/linux/tracehook.h b/include/linux/tracehook.h
index 1e98b5530425..6f8ab7da27c4 100644
--- a/include/linux/tracehook.h
+++ b/include/linux/tracehook.h
@@ -191,7 +191,7 @@ static inline void tracehook_notify_resume(struct pt_regs *regs)
 	 * pairs with task_work_add()->set_notify_resume() after
 	 * hlist_add_head(task->task_works);
 	 */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	if (unlikely(current->task_works))
 		task_work_run();
 }
diff --git a/include/net/ip_vs.h b/include/net/ip_vs.h
index f0c13a386bf3..688328691a2e 100644
--- a/include/net/ip_vs.h
+++ b/include/net/ip_vs.h
@@ -1195,7 +1195,7 @@ static inline bool __ip_vs_conn_get(struct ip_vs_conn *cp)
 /* put back the conn without restarting its timer */
 static inline void __ip_vs_conn_put(struct ip_vs_conn *cp)
 {
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&cp->refcnt);
 }
 extern void ip_vs_conn_put(struct ip_vs_conn *cp);
@@ -1403,7 +1403,7 @@ static inline void ip_vs_dest_hold(struct ip_vs_dest *dest)
 
 static inline void ip_vs_dest_put(struct ip_vs_dest *dest)
 {
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&dest->refcnt);
 }
 
diff --git a/kernel/debug/debug_core.c b/kernel/debug/debug_core.c
index 0506d447aed2..4ed34636ece1 100644
--- a/kernel/debug/debug_core.c
+++ b/kernel/debug/debug_core.c
@@ -526,7 +526,7 @@ return_normal:
 			kgdb_info[cpu].exception_state &=
 				~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);
 			kgdb_info[cpu].enter_kgdb--;
-			smp_mb__before_atomic_dec();
+			smp_mb__before_atomic();
 			atomic_dec(&slaves_in_kgdb);
 			dbg_touch_watchdogs();
 			local_irq_restore(flags);
@@ -650,7 +650,7 @@ kgdb_restore:
 	kgdb_info[cpu].exception_state &=
 		~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);
 	kgdb_info[cpu].enter_kgdb--;
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&masters_in_kgdb);
 	/* Free kgdb_active */
 	atomic_set(&kgdb_active, -1);
diff --git a/kernel/futex.c b/kernel/futex.c
index 08dc343a22dc..ae273bf8ba65 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -251,7 +251,7 @@ static inline void futex_get_mm(union futex_key *key)
 	 * get_futex_key() implies a full barrier. This is relied upon
 	 * as full barrier (B), see the ordering comment above.
 	 */
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 /*
@@ -264,7 +264,7 @@ static inline void hb_waiters_inc(struct futex_hash_bucket *hb)
 	/*
 	 * Full barrier (A), see the ordering comment above.
 	 */
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 #endif
 }
 
diff --git a/kernel/kmod.c b/kernel/kmod.c
index 8241906c4b61..d7a4557e3c20 100644
--- a/kernel/kmod.c
+++ b/kernel/kmod.c
@@ -495,7 +495,7 @@ int __usermodehelper_disable(enum umh_disable_depth depth)
 static void helper_lock(void)
 {
 	atomic_inc(&running_helpers);
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 }
 
 static void helper_unlock(void)
diff --git a/kernel/rcutree.c b/kernel/rcutree.c
index 08c426f3f623..6233042d87f8 100644
--- a/kernel/rcutree.c
+++ b/kernel/rcutree.c
@@ -357,9 +357,9 @@ static void rcu_eqs_enter_common(struct rcu_dynticks *rdtp, long long oldval,
 	}
 	rcu_prepare_for_idle(smp_processor_id());
 	/* CPUs seeing atomic_inc() must see prior RCU read-side crit sects */
-	smp_mb__before_atomic_inc();  /* See above. */
+	smp_mb__before_atomic();  /* See above. */
 	atomic_inc(&rdtp->dynticks);
-	smp_mb__after_atomic_inc();  /* Force ordering with next sojourn. */
+	smp_mb__after_atomic();  /* Force ordering with next sojourn. */
 	WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
 
 	/*
@@ -495,10 +495,10 @@ void rcu_irq_exit(void)
 static void rcu_eqs_exit_common(struct rcu_dynticks *rdtp, long long oldval,
 			       int user)
 {
-	smp_mb__before_atomic_inc();  /* Force ordering w/previous sojourn. */
+	smp_mb__before_atomic();  /* Force ordering w/previous sojourn. */
 	atomic_inc(&rdtp->dynticks);
 	/* CPUs seeing atomic_inc() must see later RCU read-side crit sects */
-	smp_mb__after_atomic_inc();  /* See above. */
+	smp_mb__after_atomic();  /* See above. */
 	WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
 	rcu_cleanup_after_idle(smp_processor_id());
 	trace_rcu_dyntick("End", oldval, rdtp->dynticks_nesting);
@@ -641,10 +641,10 @@ void rcu_nmi_enter(void)
 	    (atomic_read(&rdtp->dynticks) & 0x1))
 		return;
 	rdtp->dynticks_nmi_nesting++;
-	smp_mb__before_atomic_inc();  /* Force delay from prior write. */
+	smp_mb__before_atomic();  /* Force delay from prior write. */
 	atomic_inc(&rdtp->dynticks);
 	/* CPUs seeing atomic_inc() must see later RCU read-side crit sects */
-	smp_mb__after_atomic_inc();  /* See above. */
+	smp_mb__after_atomic();  /* See above. */
 	WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
 }
 
@@ -663,9 +663,9 @@ void rcu_nmi_exit(void)
 	    --rdtp->dynticks_nmi_nesting != 0)
 		return;
 	/* CPUs seeing atomic_inc() must see prior RCU read-side crit sects */
-	smp_mb__before_atomic_inc();  /* See above. */
+	smp_mb__before_atomic();  /* See above. */
 	atomic_inc(&rdtp->dynticks);
-	smp_mb__after_atomic_inc();  /* Force delay to next write. */
+	smp_mb__after_atomic();  /* Force delay to next write. */
 	WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
 }
 
@@ -2659,7 +2659,7 @@ void synchronize_sched_expedited(void)
 		s = atomic_long_read(&rsp->expedited_done);
 		if (ULONG_CMP_GE((ulong)s, (ulong)firstsnap)) {
 			/* ensure test happens before caller kfree */
-			smp_mb__before_atomic_inc(); /* ^^^ */
+			smp_mb__before_atomic(); /* ^^^ */
 			atomic_long_inc(&rsp->expedited_workdone1);
 			return;
 		}
@@ -2677,7 +2677,7 @@ void synchronize_sched_expedited(void)
 		s = atomic_long_read(&rsp->expedited_done);
 		if (ULONG_CMP_GE((ulong)s, (ulong)firstsnap)) {
 			/* ensure test happens before caller kfree */
-			smp_mb__before_atomic_inc(); /* ^^^ */
+			smp_mb__before_atomic(); /* ^^^ */
 			atomic_long_inc(&rsp->expedited_workdone2);
 			return;
 		}
@@ -2706,7 +2706,7 @@ void synchronize_sched_expedited(void)
 		s = atomic_long_read(&rsp->expedited_done);
 		if (ULONG_CMP_GE((ulong)s, (ulong)snap)) {
 			/* ensure test happens before caller kfree */
-			smp_mb__before_atomic_inc(); /* ^^^ */
+			smp_mb__before_atomic(); /* ^^^ */
 			atomic_long_inc(&rsp->expedited_done_lost);
 			break;
 		}
* Unmerged path kernel/rcutree_plugin.h
diff --git a/kernel/sched/cpupri.c b/kernel/sched/cpupri.c
index 8b836b376d91..746bc9344969 100644
--- a/kernel/sched/cpupri.c
+++ b/kernel/sched/cpupri.c
@@ -165,7 +165,7 @@ void cpupri_set(struct cpupri *cp, int cpu, int newpri)
 		 * do a write memory barrier, and then update the count, to
 		 * make sure the vector is visible when count is set.
 		 */
-		smp_mb__before_atomic_inc();
+		smp_mb__before_atomic();
 		atomic_inc(&(vec)->count);
 		do_mb = 1;
 	}
@@ -185,14 +185,14 @@ void cpupri_set(struct cpupri *cp, int cpu, int newpri)
 		 * the new priority vec.
 		 */
 		if (do_mb)
-			smp_mb__after_atomic_inc();
+			smp_mb__after_atomic();
 
 		/*
 		 * When removing from the vector, we decrement the counter first
 		 * do a memory barrier and then clear the mask.
 		 */
 		atomic_dec(&(vec)->count);
-		smp_mb__after_atomic_inc();
+		smp_mb__after_atomic();
 		cpumask_clear_cpu(cpu, vec->mask);
 	}
 
diff --git a/kernel/wait.c b/kernel/wait.c
index 6698e0c04ead..011970463b42 100644
--- a/kernel/wait.c
+++ b/kernel/wait.c
@@ -267,7 +267,7 @@ EXPORT_SYMBOL(__wake_up_bit);
  *
  * In order for this to function properly, as it uses waitqueue_active()
  * internally, some kind of memory barrier must be done prior to calling
- * this. Typically, this will be smp_mb__after_clear_bit(), but in some
+ * this. Typically, this will be smp_mb__after_atomic(), but in some
  * cases where bitflags are manipulated non-atomically under a lock, one
  * may need to use a less regular barrier, such fs/inode.c's smp_mb(),
  * because spin_unlock() does not guarantee a memory barrier.
diff --git a/mm/backing-dev.c b/mm/backing-dev.c
index e04454cdb33f..0a1ca43aefa2 100644
--- a/mm/backing-dev.c
+++ b/mm/backing-dev.c
@@ -546,7 +546,7 @@ void clear_bdi_congested(struct backing_dev_info *bdi, int sync)
 	bit = sync ? BDI_sync_congested : BDI_async_congested;
 	if (test_and_clear_bit(bit, &bdi->state))
 		atomic_dec(&nr_bdi_congested[sync]);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	if (waitqueue_active(wqh))
 		wake_up(wqh);
 }
diff --git a/mm/filemap.c b/mm/filemap.c
index 7bf72f0d4958..0d51f2cd5b31 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -736,7 +736,7 @@ void unlock_page(struct page *page)
 {
 	VM_BUG_ON(!PageLocked(page));
 	clear_bit_unlock(PG_locked, &page->flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_page(page, PG_locked);
 }
 EXPORT_SYMBOL(unlock_page);
@@ -753,7 +753,7 @@ void end_page_writeback(struct page *page)
 	if (!test_clear_page_writeback(page))
 		BUG();
 
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	wake_up_page(page, PG_writeback);
 }
 EXPORT_SYMBOL(end_page_writeback);
diff --git a/net/atm/pppoatm.c b/net/atm/pppoatm.c
index 8c93267ce969..c4e09846d1de 100644
--- a/net/atm/pppoatm.c
+++ b/net/atm/pppoatm.c
@@ -252,7 +252,7 @@ static int pppoatm_may_send(struct pppoatm_vcc *pvcc, int size)
 	 * we need to ensure there's a memory barrier after it. The bit
 	 * *must* be set before we do the atomic_inc() on pvcc->inflight.
 	 * There's no smp_mb__after_set_bit(), so it's this or abuse
-	 * smp_mb__after_clear_bit().
+	 * smp_mb__after_atomic().
 	 */
 	test_and_set_bit(BLOCKED, &pvcc->blocked);
 
diff --git a/net/bluetooth/hci_event.c b/net/bluetooth/hci_event.c
index dcaa6dbbab2c..28a473d2dfd2 100644
--- a/net/bluetooth/hci_event.c
+++ b/net/bluetooth/hci_event.c
@@ -48,7 +48,7 @@ static void hci_cc_inquiry_cancel(struct hci_dev *hdev, struct sk_buff *skb)
 	}
 
 	clear_bit(HCI_INQUIRY, &hdev->flags);
-	smp_mb__after_clear_bit(); /* wake_up_bit advises about this barrier */
+	smp_mb__after_atomic(); /* wake_up_bit advises about this barrier */
 	wake_up_bit(&hdev->flags, HCI_INQUIRY);
 
 	hci_dev_lock(hdev);
@@ -1601,7 +1601,7 @@ static void hci_inquiry_complete_evt(struct hci_dev *hdev, struct sk_buff *skb)
 	if (!test_and_clear_bit(HCI_INQUIRY, &hdev->flags))
 		return;
 
-	smp_mb__after_clear_bit(); /* wake_up_bit advises about this barrier */
+	smp_mb__after_atomic(); /* wake_up_bit advises about this barrier */
 	wake_up_bit(&hdev->flags, HCI_INQUIRY);
 
 	if (!test_bit(HCI_MGMT, &hdev->dev_flags))
diff --git a/net/core/dev.c b/net/core/dev.c
index d06cd723bba7..40fc40eba4b8 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -1319,7 +1319,7 @@ static int __dev_close_many(struct list_head *head)
 		 * dev->stop() will invoke napi_disable() on all of it's
 		 * napi_struct instances on this device.
 		 */
-		smp_mb__after_clear_bit(); /* Commit netif_running(). */
+		smp_mb__after_atomic(); /* Commit netif_running(). */
 	}
 
 	dev_deactivate_many(head);
@@ -3244,7 +3244,7 @@ static void net_tx_action(struct softirq_action *h)
 
 			root_lock = qdisc_lock(q);
 			if (spin_trylock(root_lock)) {
-				smp_mb__before_clear_bit();
+				smp_mb__before_atomic();
 				clear_bit(__QDISC_STATE_SCHED,
 					  &q->state);
 				qdisc_run(q);
@@ -3254,7 +3254,7 @@ static void net_tx_action(struct softirq_action *h)
 					      &q->state)) {
 					__netif_reschedule(q);
 				} else {
-					smp_mb__before_clear_bit();
+					smp_mb__before_atomic();
 					clear_bit(__QDISC_STATE_SCHED,
 						  &q->state);
 				}
@@ -4099,7 +4099,7 @@ void __napi_complete(struct napi_struct *n)
 	BUG_ON(n->gro_list);
 
 	list_del(&n->poll_list);
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(NAPI_STATE_SCHED, &n->state);
 }
 EXPORT_SYMBOL(__napi_complete);
diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 8f82a5cc3851..7a9b89fd46cd 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -144,7 +144,7 @@ static void linkwatch_do_dev(struct net_device *dev)
 	 * Make sure the above read is complete since it can be
 	 * rewritten as soon as we clear the bit below.
 	 */
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 
 	/* We are about to handle this device,
 	 * so new events can be accepted
diff --git a/net/ipv4/inetpeer.c b/net/ipv4/inetpeer.c
index 33d5537881ed..4b779f2906c7 100644
--- a/net/ipv4/inetpeer.c
+++ b/net/ipv4/inetpeer.c
@@ -529,7 +529,7 @@ EXPORT_SYMBOL_GPL(inet_getpeer);
 void inet_putpeer(struct inet_peer *p)
 {
 	p->dtime = (__u32)jiffies;
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&p->refcnt);
 }
 EXPORT_SYMBOL_GPL(inet_putpeer);
* Unmerged path net/ipv4/tcp_output.c
diff --git a/net/netfilter/nf_conntrack_core.c b/net/netfilter/nf_conntrack_core.c
index 6b78bb8292d1..b16848d9a25a 100644
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@ -800,7 +800,7 @@ void nf_conntrack_free(struct nf_conn *ct)
 	nf_ct_ext_destroy(ct);
 	nf_ct_ext_free(ct);
 	kmem_cache_free(net->ct.nf_conntrack_cachep, ct);
-	smp_mb__before_atomic_dec();
+	smp_mb__before_atomic();
 	atomic_dec(&net->ct.count);
 }
 EXPORT_SYMBOL_GPL(nf_conntrack_free);
diff --git a/net/rds/ib_recv.c b/net/rds/ib_recv.c
index 8eb9501e3d60..b8e11e5fa767 100644
--- a/net/rds/ib_recv.c
+++ b/net/rds/ib_recv.c
@@ -599,7 +599,7 @@ static void rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq,
 {
 	atomic64_set(&ic->i_ack_next, seq);
 	if (ack_required) {
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		set_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
 	}
 }
@@ -607,7 +607,7 @@ static void rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq,
 static u64 rds_ib_get_ack(struct rds_ib_connection *ic)
 {
 	clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return atomic64_read(&ic->i_ack_next);
 }
diff --git a/net/rds/iw_recv.c b/net/rds/iw_recv.c
index 45033358358e..aa8bf6786008 100644
--- a/net/rds/iw_recv.c
+++ b/net/rds/iw_recv.c
@@ -429,7 +429,7 @@ static void rds_iw_set_ack(struct rds_iw_connection *ic, u64 seq,
 {
 	atomic64_set(&ic->i_ack_next, seq);
 	if (ack_required) {
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		set_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
 	}
 }
@@ -437,7 +437,7 @@ static void rds_iw_set_ack(struct rds_iw_connection *ic, u64 seq,
 static u64 rds_iw_get_ack(struct rds_iw_connection *ic)
 {
 	clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return atomic64_read(&ic->i_ack_next);
 }
diff --git a/net/rds/send.c b/net/rds/send.c
index 88eace57dd6b..406662076dff 100644
--- a/net/rds/send.c
+++ b/net/rds/send.c
@@ -107,7 +107,7 @@ static int acquire_in_xmit(struct rds_connection *conn)
 static void release_in_xmit(struct rds_connection *conn)
 {
 	clear_bit(RDS_IN_XMIT, &conn->c_flags);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	/*
 	 * We don't use wait_on_bit()/wake_up_bit() because our waking is in a
 	 * hot path and finding waiters is very rare.  We don't want to walk
@@ -661,7 +661,7 @@ void rds_send_drop_acked(struct rds_connection *conn, u64 ack,
 
 	/* order flag updates with spin locks */
 	if (!list_empty(&list))
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 
 	spin_unlock_irqrestore(&conn->c_lock, flags);
 
@@ -691,7 +691,7 @@ void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in *dest)
 	}
 
 	/* order flag updates with the rs lock */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	spin_unlock_irqrestore(&rs->rs_lock, flags);
 
diff --git a/net/rds/tcp_send.c b/net/rds/tcp_send.c
index 81cf5a4c5e40..53b17ca0dff5 100644
--- a/net/rds/tcp_send.c
+++ b/net/rds/tcp_send.c
@@ -93,7 +93,7 @@ int rds_tcp_xmit(struct rds_connection *conn, struct rds_message *rm,
 		rm->m_ack_seq = tc->t_last_sent_nxt +
 				sizeof(struct rds_header) +
 				be32_to_cpu(rm->m_inc.i_hdr.h_len) - 1;
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		set_bit(RDS_MSG_HAS_ACK_SEQ, &rm->m_flags);
 		tc->t_last_expected_una = rm->m_ack_seq + 1;
 
diff --git a/net/sunrpc/auth.c b/net/sunrpc/auth.c
index 415159061cd0..f5843e07169d 100644
--- a/net/sunrpc/auth.c
+++ b/net/sunrpc/auth.c
@@ -296,7 +296,7 @@ static void
 rpcauth_unhash_cred_locked(struct rpc_cred *cred)
 {
 	hlist_del_rcu(&cred->cr_hash);
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(RPCAUTH_CRED_HASHED, &cred->cr_flags);
 }
 
diff --git a/net/sunrpc/auth_gss/auth_gss.c b/net/sunrpc/auth_gss/auth_gss.c
index 51906ca485e6..7e2c0fac2a2d 100644
--- a/net/sunrpc/auth_gss/auth_gss.c
+++ b/net/sunrpc/auth_gss/auth_gss.c
@@ -142,7 +142,7 @@ gss_cred_set_ctx(struct rpc_cred *cred, struct gss_cl_ctx *ctx)
 	gss_get_ctx(ctx);
 	rcu_assign_pointer(gss_cred->gc_ctx, ctx);
 	set_bit(RPCAUTH_CRED_UPTODATE, &cred->cr_flags);
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(RPCAUTH_CRED_NEW, &cred->cr_flags);
 }
 
diff --git a/net/sunrpc/backchannel_rqst.c b/net/sunrpc/backchannel_rqst.c
index d0f3e69c948e..d0caf6322187 100644
--- a/net/sunrpc/backchannel_rqst.c
+++ b/net/sunrpc/backchannel_rqst.c
@@ -242,10 +242,10 @@ void xprt_free_bc_request(struct rpc_rqst *req)
 	dprintk("RPC:       free backchannel req=%p\n", req);
 
 	req->rq_connect_cookie = xprt->connect_cookie - 1;
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	WARN_ON_ONCE(!test_bit(RPC_BC_PA_IN_USE, &req->rq_bc_pa_state));
 	clear_bit(RPC_BC_PA_IN_USE, &req->rq_bc_pa_state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	if (!xprt_need_to_requeue(xprt)) {
 		/*
diff --git a/net/sunrpc/xprt.c b/net/sunrpc/xprt.c
index 6563acf5643e..5f61f6577b87 100644
--- a/net/sunrpc/xprt.c
+++ b/net/sunrpc/xprt.c
@@ -212,9 +212,9 @@ static void xprt_clear_locked(struct rpc_xprt *xprt)
 {
 	xprt->snd_task = NULL;
 	if (!test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(XPRT_LOCKED, &xprt->state);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 	} else
 		queue_work(rpciod_workqueue, &xprt->task_cleanup);
 }
diff --git a/net/sunrpc/xprtsock.c b/net/sunrpc/xprtsock.c
index 3d85759a9ca9..1dbf9cf6cc1d 100644
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@ -893,11 +893,11 @@ static void xs_close(struct rpc_xprt *xprt)
 	xs_reset_transport(transport);
 	xprt->reestablish_timeout = 0;
 
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(XPRT_CONNECTION_ABORT, &xprt->state);
 	clear_bit(XPRT_CLOSE_WAIT, &xprt->state);
 	clear_bit(XPRT_CLOSING, &xprt->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	xprt_disconnect_done(xprt);
 }
 
@@ -1497,12 +1497,12 @@ static void xs_tcp_cancel_linger_timeout(struct rpc_xprt *xprt)
 
 static void xs_sock_reset_connection_flags(struct rpc_xprt *xprt)
 {
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 	clear_bit(XPRT_CONNECTION_ABORT, &xprt->state);
 	clear_bit(XPRT_CONNECTION_CLOSE, &xprt->state);
 	clear_bit(XPRT_CLOSE_WAIT, &xprt->state);
 	clear_bit(XPRT_CLOSING, &xprt->state);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 }
 
 static void xs_sock_mark_closed(struct rpc_xprt *xprt)
@@ -1556,10 +1556,10 @@ static void xs_tcp_state_change(struct sock *sk)
 		xprt->connect_cookie++;
 		xprt->reestablish_timeout = 0;
 		set_bit(XPRT_CLOSING, &xprt->state);
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(XPRT_CONNECTED, &xprt->state);
 		clear_bit(XPRT_CLOSE_WAIT, &xprt->state);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		xs_tcp_schedule_linger_timeout(xprt, xs_tcp_fin_timeout);
 		break;
 	case TCP_CLOSE_WAIT:
@@ -1578,9 +1578,9 @@ static void xs_tcp_state_change(struct sock *sk)
 	case TCP_LAST_ACK:
 		set_bit(XPRT_CLOSING, &xprt->state);
 		xs_tcp_schedule_linger_timeout(xprt, xs_tcp_fin_timeout);
-		smp_mb__before_clear_bit();
+		smp_mb__before_atomic();
 		clear_bit(XPRT_CONNECTED, &xprt->state);
-		smp_mb__after_clear_bit();
+		smp_mb__after_atomic();
 		break;
 	case TCP_CLOSE:
 		xs_tcp_cancel_linger_timeout(xprt);
diff --git a/net/unix/af_unix.c b/net/unix/af_unix.c
index 8e33cac231ca..dd27d44a77e5 100644
--- a/net/unix/af_unix.c
+++ b/net/unix/af_unix.c
@@ -1197,7 +1197,7 @@ restart:
 	sk->sk_state	= TCP_ESTABLISHED;
 	sock_hold(newsk);
 
-	smp_mb__after_atomic_inc();	/* sock_hold() does an atomic_inc() */
+	smp_mb__after_atomic();	/* sock_hold() does an atomic_inc() */
 	unix_peer(sk)	= newsk;
 
 	unix_state_unlock(sk);
diff --git a/sound/pci/bt87x.c b/sound/pci/bt87x.c
index 18802039497a..0897a0df43d7 100644
--- a/sound/pci/bt87x.c
+++ b/sound/pci/bt87x.c
@@ -435,7 +435,7 @@ static int snd_bt87x_pcm_open(struct snd_pcm_substream *substream)
 
 _error:
 	clear_bit(0, &chip->opened);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	return err;
 }
 
@@ -450,7 +450,7 @@ static int snd_bt87x_close(struct snd_pcm_substream *substream)
 
 	chip->substream = NULL;
 	clear_bit(0, &chip->opened);
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 	return 0;
 }
 
