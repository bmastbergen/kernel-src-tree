kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s - Add is_hv_enabled to kvmppc_ops (Don Zickus) [1127366]
Rebuild_FUZZ: 90.20%
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
commit 699cc87641c123128bf3a4e12c0a8d739b1ac2f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/699cc876.failed

This help us to identify whether we are running with hypervisor mode KVM
enabled. The change is needed so that we can have both HV and PR kvm
enabled in the same kernel.

If both HV and PR KVM are included, interrupts come in to the HV version
of the kvmppc_interrupt code, which then jumps to the PR handler,
renamed to kvmppc_interrupt_pr, if the guest is a PR guest.

Allowing both PR and HV in the same kernel required some changes to
kvm_dev_ioctl_check_extension(), since the values returned now can't
be selected with #ifdefs as much as previously. We look at is_hv_enabled
to return the right value when checking for capabilities.For capabilities that
are only provided by HV KVM, we return the HV value only if
is_hv_enabled is true. For capabilities provided by PR KVM but not HV,
we return the PR value only if is_hv_enabled is false.

NOTE: in later patch we replace is_hv_enabled with a static inline
function comparing kvm_ppc_ops

	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit 699cc87641c123128bf3a4e12c0a8d739b1ac2f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/kvm_ppc.h
#	arch/powerpc/kvm/book3s_hv.c
#	arch/powerpc/kvm/book3s_pr.c
diff --cc arch/powerpc/include/asm/kvm_ppc.h
index 1823f38906c6,c13f15db476c..000000000000
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@@ -177,6 -168,67 +177,70 @@@ extern int kvmppc_xics_get_xive(struct 
  extern int kvmppc_xics_int_on(struct kvm *kvm, u32 irq);
  extern int kvmppc_xics_int_off(struct kvm *kvm, u32 irq);
  
++<<<<<<< HEAD
++=======
+ union kvmppc_one_reg {
+ 	u32	wval;
+ 	u64	dval;
+ 	vector128 vval;
+ 	u64	vsxval[2];
+ 	struct {
+ 		u64	addr;
+ 		u64	length;
+ 	}	vpaval;
+ };
+ 
+ struct kvmppc_ops {
+ 	bool is_hv_enabled;
+ 	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+ 	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+ 	int (*get_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+ 			   union kvmppc_one_reg *val);
+ 	int (*set_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+ 			   union kvmppc_one_reg *val);
+ 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
+ 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
+ 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
+ 	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+ 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned int id);
+ 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+ 	int (*check_requests)(struct kvm_vcpu *vcpu);
+ 	int (*get_dirty_log)(struct kvm *kvm, struct kvm_dirty_log *log);
+ 	void (*flush_memslot)(struct kvm *kvm, struct kvm_memory_slot *memslot);
+ 	int (*prepare_memory_region)(struct kvm *kvm,
+ 				     struct kvm_memory_slot *memslot,
+ 				     struct kvm_userspace_memory_region *mem);
+ 	void (*commit_memory_region)(struct kvm *kvm,
+ 				     struct kvm_userspace_memory_region *mem,
+ 				     const struct kvm_memory_slot *old);
+ 	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
+ 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
+ 			   unsigned long end);
+ 	int (*age_hva)(struct kvm *kvm, unsigned long hva);
+ 	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
+ 	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
+ 	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
+ 	void (*free_memslot)(struct kvm_memory_slot *free,
+ 			     struct kvm_memory_slot *dont);
+ 	int (*create_memslot)(struct kvm_memory_slot *slot,
+ 			      unsigned long npages);
+ 	int (*init_vm)(struct kvm *kvm);
+ 	void (*destroy_vm)(struct kvm *kvm);
+ 	int (*check_processor_compat)(void);
+ 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);
+ 	int (*emulate_op)(struct kvm_run *run, struct kvm_vcpu *vcpu,
+ 			  unsigned int inst, int *advance);
+ 	int (*emulate_mtspr)(struct kvm_vcpu *vcpu, int sprn, ulong spr_val);
+ 	int (*emulate_mfspr)(struct kvm_vcpu *vcpu, int sprn, ulong *spr_val);
+ 	void (*fast_vcpu_kick)(struct kvm_vcpu *vcpu);
+ 	long (*arch_vm_ioctl)(struct file *filp, unsigned int ioctl,
+ 			      unsigned long arg);
+ 
+ };
+ 
+ extern struct kvmppc_ops *kvmppc_ops;
+ 
++>>>>>>> 699cc87641c1 (kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops)
  /*
   * Cuts out inst bits with ordering according to spec.
   * That means the leftmost bit is zero. All given bits are included.
diff --cc arch/powerpc/kvm/book3s_hv.c
index b327916c88e9,0782c8688c8b..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -2087,7 -2099,102 +2087,106 @@@ int kvmppc_core_emulate_mfspr(struct kv
  	return EMULATE_FAIL;
  }
  
++<<<<<<< HEAD
 +static int kvmppc_book3s_hv_init(void)
++=======
+ static int kvmppc_core_check_processor_compat_hv(void)
+ {
+ 	if (!cpu_has_feature(CPU_FTR_HVMODE))
+ 		return -EIO;
+ 	return 0;
+ }
+ 
+ static long kvm_arch_vm_ioctl_hv(struct file *filp,
+ 				 unsigned int ioctl, unsigned long arg)
+ {
+ 	struct kvm *kvm __maybe_unused = filp->private_data;
+ 	void __user *argp = (void __user *)arg;
+ 	long r;
+ 
+ 	switch (ioctl) {
+ 
+ 	case KVM_ALLOCATE_RMA: {
+ 		struct kvm_allocate_rma rma;
+ 		struct kvm *kvm = filp->private_data;
+ 
+ 		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
+ 		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
+ 			r = -EFAULT;
+ 		break;
+ 	}
+ 
+ 	case KVM_PPC_ALLOCATE_HTAB: {
+ 		u32 htab_order;
+ 
+ 		r = -EFAULT;
+ 		if (get_user(htab_order, (u32 __user *)argp))
+ 			break;
+ 		r = kvmppc_alloc_reset_hpt(kvm, &htab_order);
+ 		if (r)
+ 			break;
+ 		r = -EFAULT;
+ 		if (put_user(htab_order, (u32 __user *)argp))
+ 			break;
+ 		r = 0;
+ 		break;
+ 	}
+ 
+ 	case KVM_PPC_GET_HTAB_FD: {
+ 		struct kvm_get_htab_fd ghf;
+ 
+ 		r = -EFAULT;
+ 		if (copy_from_user(&ghf, argp, sizeof(ghf)))
+ 			break;
+ 		r = kvm_vm_ioctl_get_htab_fd(kvm, &ghf);
+ 		break;
+ 	}
+ 
+ 	default:
+ 		r = -ENOTTY;
+ 	}
+ 
+ 	return r;
+ }
+ 
+ static struct kvmppc_ops kvmppc_hv_ops = {
+ 	.is_hv_enabled = true,
+ 	.get_sregs = kvm_arch_vcpu_ioctl_get_sregs_hv,
+ 	.set_sregs = kvm_arch_vcpu_ioctl_set_sregs_hv,
+ 	.get_one_reg = kvmppc_get_one_reg_hv,
+ 	.set_one_reg = kvmppc_set_one_reg_hv,
+ 	.vcpu_load   = kvmppc_core_vcpu_load_hv,
+ 	.vcpu_put    = kvmppc_core_vcpu_put_hv,
+ 	.set_msr     = kvmppc_set_msr_hv,
+ 	.vcpu_run    = kvmppc_vcpu_run_hv,
+ 	.vcpu_create = kvmppc_core_vcpu_create_hv,
+ 	.vcpu_free   = kvmppc_core_vcpu_free_hv,
+ 	.check_requests = kvmppc_core_check_requests_hv,
+ 	.get_dirty_log  = kvm_vm_ioctl_get_dirty_log_hv,
+ 	.flush_memslot  = kvmppc_core_flush_memslot_hv,
+ 	.prepare_memory_region = kvmppc_core_prepare_memory_region_hv,
+ 	.commit_memory_region  = kvmppc_core_commit_memory_region_hv,
+ 	.unmap_hva = kvm_unmap_hva_hv,
+ 	.unmap_hva_range = kvm_unmap_hva_range_hv,
+ 	.age_hva  = kvm_age_hva_hv,
+ 	.test_age_hva = kvm_test_age_hva_hv,
+ 	.set_spte_hva = kvm_set_spte_hva_hv,
+ 	.mmu_destroy  = kvmppc_mmu_destroy_hv,
+ 	.free_memslot = kvmppc_core_free_memslot_hv,
+ 	.create_memslot = kvmppc_core_create_memslot_hv,
+ 	.init_vm =  kvmppc_core_init_vm_hv,
+ 	.destroy_vm = kvmppc_core_destroy_vm_hv,
+ 	.check_processor_compat = kvmppc_core_check_processor_compat_hv,
+ 	.get_smmu_info = kvm_vm_ioctl_get_smmu_info_hv,
+ 	.emulate_op = kvmppc_core_emulate_op_hv,
+ 	.emulate_mtspr = kvmppc_core_emulate_mtspr_hv,
+ 	.emulate_mfspr = kvmppc_core_emulate_mfspr_hv,
+ 	.fast_vcpu_kick = kvmppc_fast_vcpu_kick_hv,
+ 	.arch_vm_ioctl  = kvm_arch_vm_ioctl_hv,
+ };
+ 
+ static int kvmppc_book3s_init_hv(void)
++>>>>>>> 699cc87641c1 (kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops)
  {
  	int r;
  
diff --cc arch/powerpc/kvm/book3s_pr.c
index d10e1186f935,b6a525d7b5c3..000000000000
--- a/arch/powerpc/kvm/book3s_pr.c
+++ b/arch/powerpc/kvm/book3s_pr.c
@@@ -1493,7 -1510,55 +1493,59 @@@ void kvmppc_core_destroy_vm(struct kvm 
  	}
  }
  
++<<<<<<< HEAD
 +static int kvmppc_book3s_init(void)
++=======
+ static int kvmppc_core_check_processor_compat_pr(void)
+ {
+ 	/* we are always compatible */
+ 	return 0;
+ }
+ 
+ static long kvm_arch_vm_ioctl_pr(struct file *filp,
+ 				 unsigned int ioctl, unsigned long arg)
+ {
+ 	return -ENOTTY;
+ }
+ 
+ static struct kvmppc_ops kvmppc_pr_ops = {
+ 	.is_hv_enabled = false,
+ 	.get_sregs = kvm_arch_vcpu_ioctl_get_sregs_pr,
+ 	.set_sregs = kvm_arch_vcpu_ioctl_set_sregs_pr,
+ 	.get_one_reg = kvmppc_get_one_reg_pr,
+ 	.set_one_reg = kvmppc_set_one_reg_pr,
+ 	.vcpu_load   = kvmppc_core_vcpu_load_pr,
+ 	.vcpu_put    = kvmppc_core_vcpu_put_pr,
+ 	.set_msr     = kvmppc_set_msr_pr,
+ 	.vcpu_run    = kvmppc_vcpu_run_pr,
+ 	.vcpu_create = kvmppc_core_vcpu_create_pr,
+ 	.vcpu_free   = kvmppc_core_vcpu_free_pr,
+ 	.check_requests = kvmppc_core_check_requests_pr,
+ 	.get_dirty_log = kvm_vm_ioctl_get_dirty_log_pr,
+ 	.flush_memslot = kvmppc_core_flush_memslot_pr,
+ 	.prepare_memory_region = kvmppc_core_prepare_memory_region_pr,
+ 	.commit_memory_region = kvmppc_core_commit_memory_region_pr,
+ 	.unmap_hva = kvm_unmap_hva_pr,
+ 	.unmap_hva_range = kvm_unmap_hva_range_pr,
+ 	.age_hva  = kvm_age_hva_pr,
+ 	.test_age_hva = kvm_test_age_hva_pr,
+ 	.set_spte_hva = kvm_set_spte_hva_pr,
+ 	.mmu_destroy  = kvmppc_mmu_destroy_pr,
+ 	.free_memslot = kvmppc_core_free_memslot_pr,
+ 	.create_memslot = kvmppc_core_create_memslot_pr,
+ 	.init_vm = kvmppc_core_init_vm_pr,
+ 	.destroy_vm = kvmppc_core_destroy_vm_pr,
+ 	.check_processor_compat = kvmppc_core_check_processor_compat_pr,
+ 	.get_smmu_info = kvm_vm_ioctl_get_smmu_info_pr,
+ 	.emulate_op = kvmppc_core_emulate_op_pr,
+ 	.emulate_mtspr = kvmppc_core_emulate_mtspr_pr,
+ 	.emulate_mfspr = kvmppc_core_emulate_mfspr_pr,
+ 	.fast_vcpu_kick = kvm_vcpu_kick,
+ 	.arch_vm_ioctl  = kvm_arch_vm_ioctl_pr,
+ };
+ 
+ static int kvmppc_book3s_init_pr(void)
++>>>>>>> 699cc87641c1 (kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops)
  {
  	int r;
  
diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h
index 99ef8711e906..eb7e6eae467f 100644
--- a/arch/powerpc/include/asm/kvm_book3s.h
+++ b/arch/powerpc/include/asm/kvm_book3s.h
@@ -302,59 +302,6 @@ static inline ulong kvmppc_get_fault_dar(struct kvm_vcpu *vcpu)
 	return vcpu->arch.fault_dar;
 }
 
-#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
-
-static inline unsigned long kvmppc_interrupt_offset(struct kvm_vcpu *vcpu)
-{
-	return to_book3s(vcpu)->hior;
-}
-
-static inline void kvmppc_update_int_pending(struct kvm_vcpu *vcpu,
-			unsigned long pending_now, unsigned long old_pending)
-{
-	if (pending_now)
-		vcpu->arch.shared->int_pending = 1;
-	else if (old_pending)
-		vcpu->arch.shared->int_pending = 0;
-}
-
-static inline bool kvmppc_critical_section(struct kvm_vcpu *vcpu)
-{
-	ulong crit_raw = vcpu->arch.shared->critical;
-	ulong crit_r1 = kvmppc_get_gpr(vcpu, 1);
-	bool crit;
-
-	/* Truncate crit indicators in 32 bit mode */
-	if (!(vcpu->arch.shared->msr & MSR_SF)) {
-		crit_raw &= 0xffffffff;
-		crit_r1 &= 0xffffffff;
-	}
-
-	/* Critical section when crit == r1 */
-	crit = (crit_raw == crit_r1);
-	/* ... and we're in supervisor mode */
-	crit = crit && !(vcpu->arch.shared->msr & MSR_PR);
-
-	return crit;
-}
-#else /* CONFIG_KVM_BOOK3S_PR_POSSIBLE */
-
-static inline unsigned long kvmppc_interrupt_offset(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
-static inline void kvmppc_update_int_pending(struct kvm_vcpu *vcpu,
-			unsigned long pending_now, unsigned long old_pending)
-{
-}
-
-static inline bool kvmppc_critical_section(struct kvm_vcpu *vcpu)
-{
-	return false;
-}
-#endif
-
 /* Magic register values loaded into r3 and r4 before the 'sc' assembly
  * instruction for the OSI hypercalls */
 #define OSI_SC_MAGIC_R3			0x113724FA
* Unmerged path arch/powerpc/include/asm/kvm_ppc.h
diff --git a/arch/powerpc/kvm/book3s.c b/arch/powerpc/kvm/book3s.c
index 807103ad2628..26e1670094c7 100644
--- a/arch/powerpc/kvm/book3s.c
+++ b/arch/powerpc/kvm/book3s.c
@@ -69,6 +69,50 @@ void kvmppc_core_load_guest_debugstate(struct kvm_vcpu *vcpu)
 {
 }
 
+static inline unsigned long kvmppc_interrupt_offset(struct kvm_vcpu *vcpu)
+{
+	if (!kvmppc_ops->is_hv_enabled)
+		return to_book3s(vcpu)->hior;
+	return 0;
+}
+
+static inline void kvmppc_update_int_pending(struct kvm_vcpu *vcpu,
+			unsigned long pending_now, unsigned long old_pending)
+{
+	if (kvmppc_ops->is_hv_enabled)
+		return;
+	if (pending_now)
+		vcpu->arch.shared->int_pending = 1;
+	else if (old_pending)
+		vcpu->arch.shared->int_pending = 0;
+}
+
+static inline bool kvmppc_critical_section(struct kvm_vcpu *vcpu)
+{
+	ulong crit_raw;
+	ulong crit_r1;
+	bool crit;
+
+	if (kvmppc_ops->is_hv_enabled)
+		return false;
+
+	crit_raw = vcpu->arch.shared->critical;
+	crit_r1 = kvmppc_get_gpr(vcpu, 1);
+
+	/* Truncate crit indicators in 32 bit mode */
+	if (!(vcpu->arch.shared->msr & MSR_SF)) {
+		crit_raw &= 0xffffffff;
+		crit_r1 &= 0xffffffff;
+	}
+
+	/* Critical section when crit == r1 */
+	crit = (crit_raw == crit_r1);
+	/* ... and we're in supervisor mode */
+	crit = crit && !(vcpu->arch.shared->msr & MSR_PR);
+
+	return crit;
+}
+
 void kvmppc_inject_interrupt(struct kvm_vcpu *vcpu, int vec, u64 flags)
 {
 	vcpu->arch.shared->srr0 = kvmppc_get_pc(vcpu);
* Unmerged path arch/powerpc/kvm/book3s_hv.c
* Unmerged path arch/powerpc/kvm/book3s_pr.c
diff --git a/arch/powerpc/kvm/book3s_xics.c b/arch/powerpc/kvm/book3s_xics.c
index a3a5cb8ee7ea..33bd90047bd5 100644
--- a/arch/powerpc/kvm/book3s_xics.c
+++ b/arch/powerpc/kvm/book3s_xics.c
@@ -818,7 +818,7 @@ int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 req)
 	}
 
 	/* Check for real mode returning too hard */
-	if (xics->real_mode)
+	if (xics->real_mode && kvmppc_ops->is_hv_enabled)
 		return kvmppc_xics_rm_complete(vcpu, req);
 
 	switch (req) {
diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f55e14cd1762..ee2e1dcb80da 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -50,7 +50,6 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
-#ifndef CONFIG_KVM_BOOK3S_64_HV
 /*
  * Common checks before entering the guest world.  Call with interrupts
  * disabled.
@@ -125,7 +124,6 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 
 	return r;
 }
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
@@ -192,11 +190,9 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 	if ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)
 		goto out;
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
 	/* HV KVM can only do PAPR mode for now */
-	if (!vcpu->arch.papr_enabled)
+	if (!vcpu->arch.papr_enabled && kvmppc_ops->is_hv_enabled)
 		goto out;
-#endif
 
 #ifdef CONFIG_KVM_BOOKE_HV
 	if (!cpu_has_feature(CPU_FTR_EMB_HV))
@@ -320,22 +316,26 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_DEVICE_CTRL:
 		r = 1;
 		break;
-#ifndef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
 #if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB:
 #endif
-#ifdef CONFIG_KVM_MPIC
-	case KVM_CAP_IRQ_MPIC:
-#endif
-		r = 1;
+		/* We support this only for PR */
+		r = !kvmppc_ops->is_hv_enabled;
 		break;
+#ifdef CONFIG_KVM_MMIO
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;
 #endif
+#ifdef CONFIG_KVM_MPIC
+	case KVM_CAP_IRQ_MPIC:
+		r = 1;
+		break;
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_PPC_ALLOC_HTAB:
@@ -346,32 +346,37 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = 1;
 		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
-		r = threads_per_core;
+		if (kvmppc_ops->is_hv_enabled)
+			r = threads_per_core;
+		else
+			r = 0;
 		break;
 	case KVM_CAP_PPC_RMA:
-		r = 1;
+		r = kvmppc_ops->is_hv_enabled;
 		/* PPC970 requires an RMA */
-		if (cpu_has_feature(CPU_FTR_ARCH_201))
+		if (r && cpu_has_feature(CPU_FTR_ARCH_201))
 			r = 2;
 		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
-#ifdef CONFIG_KVM_BOOK3S_64_HV
-		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+		if (kvmppc_ops->is_hv_enabled)
+			r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+		else
+			r = 0;
 #elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 		r = 1;
 #else
 		r = 0;
-		break;
 #endif
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+		break;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_HTAB_FD:
-		r = 1;
+		r = kvmppc_ops->is_hv_enabled;
 		break;
 #endif
-		break;
 	case KVM_CAP_NR_VCPUS:
 		/*
 		 * Recommending a number of CPUs is somewhat arbitrary; we
@@ -379,11 +384,10 @@ int kvm_dev_ioctl_check_extension(long ext)
 		 * will have secondary threads "offline"), and for other KVM
 		 * implementations just count online CPUs.
 		 */
-#ifdef CONFIG_KVM_BOOK3S_64_HV
-		r = num_present_cpus();
-#else
-		r = num_online_cpus();
-#endif
+		if (kvmppc_ops->is_hv_enabled)
+			r = num_present_cpus();
+		else
+			r = num_online_cpus();
 		break;
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
