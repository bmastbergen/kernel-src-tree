timekeeping: Use cached ntp_tick_length when accumulating error

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author John Stultz <john.stultz@linaro.org>
commit 375f45b5b53a91dfa8f0c11328e0e044f82acbed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/375f45b5.failed

By caching the ntp_tick_length() when we correct the frequency error,
and then using that cached value to accumulate error, we avoid large
initial errors when the tick length is changed.

This makes convergence happen much faster in the simulator, since the
initial error doesn't have to be slowly whittled away.

This initially seems like an accounting error, but Miroslav pointed out
that ntp_tick_length() can change mid-tick, so when we apply it in the
error accumulation, we are applying any recent change to the entire tick.

This approach chooses to apply changes in the ntp_tick_length() only to
the next tick, which allows us to calculate the freq correction before
using the new tick length, which avoids accummulating error.

Credit to Miroslav for pointing this out and providing the original patch
this functionality has been pulled out from, along with the rational.

	Cc: Miroslav Lichvar <mlichvar@redhat.com>
	Cc: Richard Cochran <richardcochran@gmail.com>
	Cc: Prarit Bhargava <prarit@redhat.com>
	Reported-by: Miroslav Lichvar <mlichvar@redhat.com>
	Signed-off-by: John Stultz <john.stultz@linaro.org>
(cherry picked from commit 375f45b5b53a91dfa8f0c11328e0e044f82acbed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/timekeeper_internal.h
#	kernel/time/timekeeping.c
diff --cc include/linux/timekeeper_internal.h
index c1825eb436ed,e9660e52dc09..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,30 -10,93 +10,40 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
 -/**
 - * struct tk_read_base - base structure for timekeeping readout
 - * @clock:	Current clocksource used for timekeeping.
 - * @read:	Read function of @clock
 - * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
 - * @cycle_last: @clock cycle value at last update
 - * @mult:	NTP adjusted multiplier for scaled math conversion
 - * @shift:	Shift value for scaled math conversion
 - * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
 - * @base_mono:  ktime_t (nanoseconds) base time for readout
 - *
 - * This struct has size 56 byte on 64 bit. Together with a seqcount it
 - * occupies a single 64byte cache line.
 - *
 - * The struct is separate from struct timekeeper as it is also used
 - * for a fast NMI safe accessor to clock monotonic.
 - */
 -struct tk_read_base {
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
  	struct clocksource	*clock;
 -	cycle_t			(*read)(struct clocksource *cs);
 -	cycle_t			mask;
 -	cycle_t			cycle_last;
 +	/* NTP adjusted clock multiplier */
  	u32			mult;
 +	/* The shift value of the current clocksource. */
  	u32			shift;
 -	u64			xtime_nsec;
 -	ktime_t			base_mono;
 -};
 -
 -/**
 - * struct timekeeper - Structure holding internal timekeeping values.
 - * @tkr:		The readout base structure
 - * @xtime_sec:		Current CLOCK_REALTIME time in seconds
 - * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
 - * @offs_real:		Offset clock monotonic -> clock realtime
 - * @offs_boot:		Offset clock monotonic -> clock boottime
 - * @offs_tai:		Offset clock monotonic -> clock tai
 - * @tai_offset:		The current UTC to TAI offset in seconds
 - * @base_raw:		Monotonic raw base time in ktime_t format
 - * @raw_time:		Monotonic raw base time in timespec64 format
 - * @cycle_interval:	Number of clock cycles in one NTP interval
 - * @xtime_interval:	Number of clock shifted nano seconds in one NTP
 - *			interval.
 - * @xtime_remainder:	Shifted nano seconds left over when rounding
 - *			@cycle_interval
 - * @raw_interval:	Raw nano seconds accumulated per NTP interval.
 - * @ntp_error:		Difference between accumulated time and NTP time in ntp
 - *			shifted nano seconds.
 - * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
 - *			ntp shifted nano seconds.
 - *
 - * Note: For timespec(64) based interfaces wall_to_monotonic is what
 - * we need to add to xtime (or xtime corrected for sub jiffie times)
 - * to get to monotonic time.  Monotonic is pegged at zero at system
 - * boot time, so wall_to_monotonic will be negative, however, we will
 - * ALWAYS keep the tv_nsec part positive so we can use the usual
 - * normalization.
 - *
 - * wall_to_monotonic is moved after resume from suspend for the
 - * monotonic time not to jump. We need to add total_sleep_time to
 - * wall_to_monotonic to get the real boot based time offset.
 - *
 - * wall_to_monotonic is no longer the boot time, getboottime must be
 - * used instead.
 - */
 -struct timekeeper {
 -	struct tk_read_base	tkr;
 -	u64			xtime_sec;
 -	struct timespec64	wall_to_monotonic;
 -	ktime_t			offs_real;
 -	ktime_t			offs_boot;
 -	ktime_t			offs_tai;
 -	s32			tai_offset;
 -	ktime_t			base_raw;
 -	struct timespec64	raw_time;
 -
 -	/* The following members are for timekeeping internal use */
 +	/* Number of clock cycles in one NTP interval. */
  	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
  	u64			xtime_interval;
 +	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
 +	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
++<<<<<<< HEAD
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
++=======
+ 	/* The ntp_tick_length() value currently being used.
+ 	 * This cached copy ensures we consistently apply the tick
+ 	 * length for an entire tick, as ntp_tick_length may change
+ 	 * mid-tick, and we don't want to apply that new value to
+ 	 * the tick in progress.
+ 	 */
+ 	u64			ntp_tick;
++>>>>>>> 375f45b5b53a (timekeeping: Use cached ntp_tick_length when accumulating error)
  	/* Difference between accumulated time and NTP time in ntp
  	 * shifted nano seconds. */
  	s64			ntp_error;
diff --cc kernel/time/timekeeping.c
index 1c5b0fcd83b2,f36b02838a47..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -1200,12 -1330,78 +1201,82 @@@ static void timekeeping_adjust(struct t
  	 *
  	 * XXX - TODO: Doc ntp_error calculation.
  	 */
 -	tk->tkr.mult += mult_adj;
 +	tk->mult += adj;
  	tk->xtime_interval += interval;
 -	tk->tkr.xtime_nsec -= offset;
 +	tk->xtime_nsec -= offset;
  	tk->ntp_error -= (interval - offset) << tk->ntp_error_shift;
++<<<<<<< HEAD
++=======
+ }
+ 
+ /*
+  * Calculate the multiplier adjustment needed to match the frequency
+  * specified by NTP
+  */
+ static __always_inline void timekeeping_freqadjust(struct timekeeper *tk,
+ 							s64 offset)
+ {
+ 	s64 interval = tk->cycle_interval;
+ 	s64 xinterval = tk->xtime_interval;
+ 	s64 tick_error;
+ 	bool negative;
+ 	u32 adj;
+ 
+ 	/* Remove any current error adj from freq calculation */
+ 	if (tk->ntp_err_mult)
+ 		xinterval -= tk->cycle_interval;
+ 
+ 	tk->ntp_tick = ntp_tick_length();
+ 
+ 	/* Calculate current error per tick */
+ 	tick_error = ntp_tick_length() >> tk->ntp_error_shift;
+ 	tick_error -= (xinterval + tk->xtime_remainder);
+ 
+ 	/* Don't worry about correcting it if its small */
+ 	if (likely((tick_error >= 0) && (tick_error <= interval)))
+ 		return;
+ 
+ 	/* preserve the direction of correction */
+ 	negative = (tick_error < 0);
+ 
+ 	/* Sort out the magnitude of the correction */
+ 	tick_error = abs(tick_error);
+ 	for (adj = 0; tick_error > interval; adj++)
+ 		tick_error >>= 1;
+ 
+ 	/* scale the corrections */
+ 	timekeeping_apply_adjustment(tk, offset, negative, adj);
+ }
+ 
+ /*
+  * Adjust the timekeeper's multiplier to the correct frequency
+  * and also to reduce the accumulated error value.
+  */
+ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
+ {
+ 	/* Correct for the current frequency error */
+ 	timekeeping_freqadjust(tk, offset);
+ 
+ 	/* Next make a small adjustment to fix any cumulative error */
+ 	if (!tk->ntp_err_mult && (tk->ntp_error > 0)) {
+ 		tk->ntp_err_mult = 1;
+ 		timekeeping_apply_adjustment(tk, offset, 0, 0);
+ 	} else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {
+ 		/* Undo any existing error adjustment */
+ 		timekeeping_apply_adjustment(tk, offset, 1, 0);
+ 		tk->ntp_err_mult = 0;
+ 	}
+ 
+ 	if (unlikely(tk->tkr.clock->maxadj &&
+ 		(tk->tkr.mult > tk->tkr.clock->mult + tk->tkr.clock->maxadj))) {
+ 		printk_once(KERN_WARNING
+ 			"Adjusting %s more than 11%% (%ld vs %ld)\n",
+ 			tk->tkr.clock->name, (long)tk->tkr.mult,
+ 			(long)tk->tkr.clock->mult + tk->tkr.clock->maxadj);
+ 	}
++>>>>>>> 375f45b5b53a (timekeeping: Use cached ntp_tick_length when accumulating error)
  
 +out_adjust:
  	/*
  	 * It may be possible that when we entered this function, xtime_nsec
  	 * was very small.  Further, if we're slightly speeding the clocksource
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/time/timekeeping.c
