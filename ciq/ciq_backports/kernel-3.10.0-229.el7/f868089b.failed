NFS: Fix a potential busy wait in nfs_page_group_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Trond Myklebust <trond.myklebust@primarydata.com>
commit f868089b09b51bd17ee41dedb96f98a1d0952fec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/f868089b.failed

We cannot allow nfs_page_group_lock to use TASK_KILLABLE here, since
the loop would cause a busy wait if somebody kills the task.

	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit f868089b09b51bd17ee41dedb96f98a1d0952fec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/nfs/pagelist.c
diff --cc fs/nfs/pagelist.c
index 00f3d2d2a504,a8759825ac76..000000000000
--- a/fs/nfs/pagelist.c
+++ b/fs/nfs/pagelist.c
@@@ -133,6 -138,151 +133,154 @@@ nfs_iocounter_wait(struct nfs_io_counte
  	return __nfs_iocounter_wait(c);
  }
  
++<<<<<<< HEAD
++=======
+ static int nfs_wait_bit_uninterruptible(void *word)
+ {
+ 	io_schedule();
+ 	return 0;
+ }
+ 
+ /*
+  * nfs_page_group_lock - lock the head of the page group
+  * @req - request in group that is to be locked
+  *
+  * this lock must be held if modifying the page group list
+  */
+ void
+ nfs_page_group_lock(struct nfs_page *req)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	wait_on_bit_lock(&head->wb_flags, PG_HEADLOCK,
+ 			nfs_wait_bit_uninterruptible,
+ 			TASK_UNINTERRUPTIBLE);
+ }
+ 
+ /*
+  * nfs_page_group_unlock - unlock the head of the page group
+  * @req - request in group that is to be unlocked
+  */
+ void
+ nfs_page_group_unlock(struct nfs_page *req)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 
+ 	WARN_ON_ONCE(head != head->wb_head);
+ 
+ 	smp_mb__before_clear_bit();
+ 	clear_bit(PG_HEADLOCK, &head->wb_flags);
+ 	smp_mb__after_clear_bit();
+ 	wake_up_bit(&head->wb_flags, PG_HEADLOCK);
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit_locked
+  *
+  * must be called with page group lock held
+  */
+ static bool
+ nfs_page_group_sync_on_bit_locked(struct nfs_page *req, unsigned int bit)
+ {
+ 	struct nfs_page *head = req->wb_head;
+ 	struct nfs_page *tmp;
+ 
+ 	WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &head->wb_flags));
+ 	WARN_ON_ONCE(test_and_set_bit(bit, &req->wb_flags));
+ 
+ 	tmp = req->wb_this_page;
+ 	while (tmp != req) {
+ 		if (!test_bit(bit, &tmp->wb_flags))
+ 			return false;
+ 		tmp = tmp->wb_this_page;
+ 	}
+ 
+ 	/* true! reset all bits */
+ 	tmp = req;
+ 	do {
+ 		clear_bit(bit, &tmp->wb_flags);
+ 		tmp = tmp->wb_this_page;
+ 	} while (tmp != req);
+ 
+ 	return true;
+ }
+ 
+ /*
+  * nfs_page_group_sync_on_bit - set bit on current request, but only
+  *   return true if the bit is set for all requests in page group
+  * @req - request in page group
+  * @bit - PG_* bit that is used to sync page group
+  */
+ bool nfs_page_group_sync_on_bit(struct nfs_page *req, unsigned int bit)
+ {
+ 	bool ret;
+ 
+ 	nfs_page_group_lock(req);
+ 	ret = nfs_page_group_sync_on_bit_locked(req, bit);
+ 	nfs_page_group_unlock(req);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * nfs_page_group_init - Initialize the page group linkage for @req
+  * @req - a new nfs request
+  * @prev - the previous request in page group, or NULL if @req is the first
+  *         or only request in the group (the head).
+  */
+ static inline void
+ nfs_page_group_init(struct nfs_page *req, struct nfs_page *prev)
+ {
+ 	WARN_ON_ONCE(prev == req);
+ 
+ 	if (!prev) {
+ 		req->wb_head = req;
+ 		req->wb_this_page = req;
+ 	} else {
+ 		WARN_ON_ONCE(prev->wb_this_page != prev->wb_head);
+ 		WARN_ON_ONCE(!test_bit(PG_HEADLOCK, &prev->wb_head->wb_flags));
+ 		req->wb_head = prev->wb_head;
+ 		req->wb_this_page = prev->wb_this_page;
+ 		prev->wb_this_page = req;
+ 
+ 		/* grab extra ref if head request has extra ref from
+ 		 * the write/commit path to handle handoff between write
+ 		 * and commit lists */
+ 		if (test_bit(PG_INODE_REF, &prev->wb_head->wb_flags))
+ 			kref_get(&req->wb_kref);
+ 	}
+ }
+ 
+ /*
+  * nfs_page_group_destroy - sync the destruction of page groups
+  * @req - request that no longer needs the page group
+  *
+  * releases the page group reference from each member once all
+  * members have called this function.
+  */
+ static void
+ nfs_page_group_destroy(struct kref *kref)
+ {
+ 	struct nfs_page *req = container_of(kref, struct nfs_page, wb_kref);
+ 	struct nfs_page *tmp, *next;
+ 
+ 	if (!nfs_page_group_sync_on_bit(req, PG_TEARDOWN))
+ 		return;
+ 
+ 	tmp = req;
+ 	do {
+ 		next = tmp->wb_this_page;
+ 		/* unlink and free */
+ 		tmp->wb_this_page = tmp;
+ 		tmp->wb_head = tmp;
+ 		nfs_free_request(tmp);
+ 		tmp = next;
+ 	} while (tmp != req);
+ }
+ 
++>>>>>>> f868089b09b5 (NFS: Fix a potential busy wait in nfs_page_group_lock)
  /**
   * nfs_create_request - Create an NFS read/write request.
   * @ctx: open context to use
@@@ -253,15 -412,9 +401,9 @@@ static void nfs_free_request(struct kre
  
  void nfs_release_request(struct nfs_page *req)
  {
 -	kref_put(&req->wb_kref, nfs_page_group_destroy);
 +	kref_put(&req->wb_kref, nfs_free_request);
  }
  
- static int nfs_wait_bit_uninterruptible(void *word)
- {
- 	io_schedule();
- 	return 0;
- }
- 
  /**
   * nfs_wait_on_request - Wait for a request to complete.
   * @req: request to wait upon.
* Unmerged path fs/nfs/pagelist.c
