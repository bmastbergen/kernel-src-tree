CPU hotplug, smp: flush any pending IPI callbacks before CPU offline

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [kernel] smp: flush any pending IPI callbacks before CPU offline (Gustavo Duarte) [1100093]
Rebuild_FUZZ: 89.43%
commit-author Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
commit 8d056c48e486249e6487910b83e0f3be7c14acf7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/8d056c48.failed

There is a race between the CPU offline code (within stop-machine) and
the smp-call-function code, which can lead to getting IPIs on the
outgoing CPU, *after* it has gone offline.

Specifically, this can happen when using
smp_call_function_single_async() to send the IPI, since this API allows
sending asynchronous IPIs from IRQ disabled contexts.  The exact race
condition is described below.

During CPU offline, in stop-machine, we don't enforce any rule in the
_DISABLE_IRQ stage, regarding the order in which the outgoing CPU and
the other CPUs disable their local interrupts.  Due to this, we can
encounter a situation in which an IPI is sent by one of the other CPUs
to the outgoing CPU (while it is *still* online), but the outgoing CPU
ends up noticing it only *after* it has gone offline.

              CPU 1                                         CPU 2
          (Online CPU)                               (CPU going offline)

       Enter _PREPARE stage                          Enter _PREPARE stage

                                                     Enter _DISABLE_IRQ stage

                                                   =
       Got a device interrupt, and                 | Didn't notice the IPI
       the interrupt handler sent an               | since interrupts were
       IPI to CPU 2 using                          | disabled on this CPU.
       smp_call_function_single_async()            |
                                                   =

       Enter _DISABLE_IRQ stage

       Enter _RUN stage                              Enter _RUN stage

                                  =
       Busy loop with interrupts  |                  Invoke take_cpu_down()
       disabled.                  |                  and take CPU 2 offline
                                  =

       Enter _EXIT stage                             Enter _EXIT stage

       Re-enable interrupts                          Re-enable interrupts

                                                     The pending IPI is noted
                                                     immediately, but alas,
                                                     the CPU is offline at
                                                     this point.

This of course, makes the smp-call-function IPI handler code running on
CPU 2 unhappy and it complains about "receiving an IPI on an offline
CPU".

One real example of the scenario on CPU 1 is the block layer's
complete-request call-path:

	__blk_complete_request() [interrupt-handler]
	    raise_blk_irq()
	        smp_call_function_single_async()

However, if we look closely, the block layer does check that the target
CPU is online before firing the IPI.  So in this case, it is actually
the unfortunate ordering/timing of events in the stop-machine phase that
leads to receiving IPIs after the target CPU has gone offline.

In reality, getting a late IPI on an offline CPU is not too bad by
itself (this can happen even due to hardware latencies in IPI
send-receive).  It is a bug only if the target CPU really went offline
without executing all the callbacks queued on its list.  (Note that a
CPU is free to execute its pending smp-call-function callbacks in a
batch, without waiting for the corresponding IPIs to arrive for each one
of those callbacks).

So, fixing this issue can be broken up into two parts:

1. Ensure that a CPU goes offline only after executing all the
   callbacks queued on it.

2. Modify the warning condition in the smp-call-function IPI handler
   code such that it warns only if an offline CPU got an IPI *and* that
   CPU had gone offline with callbacks still pending in its queue.

Achieving part 1 is straight-forward - just flush (execute) all the
queued callbacks on the outgoing CPU in the CPU_DYING stage[1],
including those callbacks for which the source CPU's IPIs might not have
been received on the outgoing CPU yet.  Once we do this, an IPI that
arrives late on the CPU going offline (either due to the race mentioned
above, or due to hardware latencies) will be completely harmless, since
the outgoing CPU would have executed all the queued callbacks before
going offline.

Overall, this fix (parts 1 and 2 put together) additionally guarantees
that we will see a warning only when the *IPI-sender code* is buggy -
that is, if it queues the callback _after_ the target CPU has gone
offline.

[1].  The CPU_DYING part needs a little more explanation: by the time we
execute the CPU_DYING notifier callbacks, the CPU would have already
been marked offline.  But we want to flush out the pending callbacks at
this stage, ignoring the fact that the CPU is offline.  So restructure
the IPI handler code so that we can by-pass the "is-cpu-offline?" check
in this particular case.  (Of course, the right solution here is to fix
CPU hotplug to mark the CPU offline _after_ invoking the CPU_DYING
notifiers, but this requires a lot of audit to ensure that this change
doesn't break any existing code; hence lets go with the solution
proposed above until that is done).

[akpm@linux-foundation.org: coding-style fixes]
	Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
	Suggested-by: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Gautham R Shenoy <ego@linux.vnet.ibm.com>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Mike Galbraith <mgalbraith@suse.de>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Rusty Russell <rusty@rustcorp.com.au>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Sachin Kamat <sachin.kamat@samsung.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8d056c48e486249e6487910b83e0f3be7c14acf7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/smp.c
diff --cc kernel/smp.c
index 74100ac030d3,80c33f8de14f..000000000000
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@@ -29,13 -27,10 +29,15 @@@ struct call_function_data 
  
  static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);
  
 -static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);
 +struct call_single_queue {
 +	struct list_head	list;
 +	raw_spinlock_t		lock;
 +};
 +
 +static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_single_queue, call_single_queue);
  
+ static void flush_smp_call_function_queue(bool warn_cpu_offline);
+ 
  static int
  hotplug_cfd(struct notifier_block *nfb, unsigned long action, void *hcpu)
  {
@@@ -68,9 -58,22 +71,23 @@@
  	case CPU_DEAD:
  	case CPU_DEAD_FROZEN:
  		free_cpumask_var(cfd->cpumask);
 +		free_cpumask_var(cfd->cpumask_ipi);
  		free_percpu(cfd->csd);
  		break;
+ 
+ 	case CPU_DYING:
+ 	case CPU_DYING_FROZEN:
+ 		/*
+ 		 * The IPIs for the smp-call-function callbacks queued by other
+ 		 * CPUs might arrive late, either due to hardware latencies or
+ 		 * because this CPU disabled interrupts (inside stop-machine)
+ 		 * before the IPIs were sent. So flush out any pending callbacks
+ 		 * explicitly (without waiting for the IPIs to arrive), to
+ 		 * ensure that the outgoing CPU doesn't go offline with work
+ 		 * still pending.
+ 		 */
+ 		flush_smp_call_function_queue(false);
+ 		break;
  #endif
  	};
  
@@@ -170,34 -190,65 +187,74 @@@ static void generic_exec_single(int cpu
  
  	if (wait)
  		csd_lock_wait(csd);
 -
 -	return 0;
  }
  
- /*
-  * Invoked by arch to handle an IPI for call function single. Must be
-  * called from the arch with interrupts disabled.
+ /**
+  * generic_smp_call_function_single_interrupt - Execute SMP IPI callbacks
+  *
+  * Invoked by arch to handle an IPI for call function single.
+  * Must be called with interrupts disabled.
   */
  void generic_smp_call_function_single_interrupt(void)
  {
++<<<<<<< HEAD
 +	struct call_single_queue *q = &__get_cpu_var(call_single_queue);
 +	LIST_HEAD(list);
 +
 +	/*
 +	 * Shouldn't receive this interrupt on a cpu that is not yet online.
 +	 */
 +	WARN_ON_ONCE(!cpu_online(smp_processor_id()));
++=======
+ 	flush_smp_call_function_queue(true);
+ }
+ 
+ /**
+  * flush_smp_call_function_queue - Flush pending smp-call-function callbacks
+  *
+  * @warn_cpu_offline: If set to 'true', warn if callbacks were queued on an
+  *		      offline CPU. Skip this check if set to 'false'.
+  *
+  * Flush any pending smp-call-function callbacks queued on this CPU. This is
+  * invoked by the generic IPI handler, as well as by a CPU about to go offline,
+  * to ensure that all pending IPI callbacks are run before it goes completely
+  * offline.
+  *
+  * Loop through the call_single_queue and run all the queued callbacks.
+  * Must be called with interrupts disabled.
+  */
+ static void flush_smp_call_function_queue(bool warn_cpu_offline)
+ {
+ 	struct llist_head *head;
+ 	struct llist_node *entry;
+ 	struct call_single_data *csd, *csd_next;
+ 	static bool warned;
+ 
+ 	WARN_ON(!irqs_disabled());
+ 
+ 	head = &__get_cpu_var(call_single_queue);
+ 	entry = llist_del_all(head);
+ 	entry = llist_reverse_order(entry);
+ 
+ 	/* There shouldn't be any pending callbacks on an offline CPU. */
+ 	if (unlikely(warn_cpu_offline && !cpu_online(smp_processor_id()) &&
+ 		     !warned && !llist_empty(head))) {
+ 		warned = true;
+ 		WARN(1, "IPI on offline CPU %d\n", smp_processor_id());
++>>>>>>> 8d056c48e486 (CPU hotplug, smp: flush any pending IPI callbacks before CPU offline)
  
 -		/*
 -		 * We don't have to use the _safe() variant here
 -		 * because we are not invoking the IPI handlers yet.
 -		 */
 -		llist_for_each_entry(csd, entry, llist)
 -			pr_warn("IPI callback %pS sent to offline CPU\n",
 -				csd->func);
 -	}
 +	raw_spin_lock(&q->lock);
 +	list_replace_init(&q->list, &list);
 +	raw_spin_unlock(&q->lock);
 +
 +	while (!list_empty(&list)) {
 +		struct call_single_data *csd;
 +
 +		csd = list_entry(list.next, struct call_single_data, list);
 +		list_del(&csd->list);
  
 -	llist_for_each_entry_safe(csd, csd_next, entry, llist) {
  		csd->func(csd->info);
 +
  		csd_unlock(csd);
  	}
  }
* Unmerged path kernel/smp.c
