KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s hv - Consolidate code that checks reason for wake from nap (Don Zickus) [1127366]
Rebuild_FUZZ: 95.95%
commit-author Paul Mackerras <paulus@samba.org>
commit e3bbbbfa13ea2901050a58b2cb382df7974e7373
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/e3bbbbfa.failed

Currently in book3s_hv_rmhandlers.S we have three places where we
have woken up from nap mode and we check the reason field in SRR1
to see what event woke us up.  This consolidates them into a new
function, kvmppc_check_wake_reason.  It looks at the wake reason
field in SRR1, and if it indicates that an external interrupt caused
the wakeup, calls kvmppc_read_intr to check what sort of interrupt
it was.

This also consolidates the two places where we synthesize an external
interrupt (0x500 vector) for the guest.  Now, if the guest exit code
finds that there was an external interrupt which has been handled
(i.e. it was an IPI indicating that there is now an interrupt pending
for the guest), it jumps to deliver_guest_interrupt, which is in the
last part of the guest entry code, where we synthesize guest external
and decrementer interrupts.  That code has been streamlined a little
and now clears LPCR[MER] when appropriate as well as setting it.

The extra clearing of any pending IPI on a secondary, offline CPU
thread before going back to nap mode has been removed.  It is no longer
necessary now that we have code to read and acknowledge IPIs in the
guest exit path.

This fixes a minor bug in the H_CEDE real-mode handling - previously,
if we found that other threads were already exiting the guest when we
were about to go to nap mode, we would branch to the cede wakeup path
and end up looking in SRR1 for a wakeup reason.  Now we branch to a
point after we have checked the wakeup reason.

This also fixes a minor bug in kvmppc_read_intr - previously it could
return 0xff rather than 1, in the case where we find that a host IPI
is pending after we have cleared the IPI.  Now it returns 1.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit e3bbbbfa13ea2901050a58b2cb382df7974e7373)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index 4e71b0446184,386e141dbf16..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -155,8 -160,71 +155,74 @@@ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206
  
  13:	b	machine_check_fwnmi
  
++<<<<<<< HEAD
++=======
+ 
+ kvmppc_primary_no_guest:
+ 	/* We handle this much like a ceded vcpu */
+ 	/* set our bit in napping_threads */
+ 	ld	r5, HSTATE_KVM_VCORE(r13)
+ 	lbz	r7, HSTATE_PTID(r13)
+ 	li	r0, 1
+ 	sld	r0, r0, r7
+ 	addi	r6, r5, VCORE_NAPPING_THREADS
+ 1:	lwarx	r3, 0, r6
+ 	or	r3, r3, r0
+ 	stwcx.	r3, 0, r6
+ 	bne	1b
+ 	/* order napping_threads update vs testing entry_exit_count */
+ 	isync
+ 	li	r12, 0
+ 	lwz	r7, VCORE_ENTRY_EXIT(r5)
+ 	cmpwi	r7, 0x100
+ 	bge	kvm_novcpu_exit	/* another thread already exiting */
+ 	li	r3, NAPPING_NOVCPU
+ 	stb	r3, HSTATE_NAPPING(r13)
+ 	li	r3, 1
+ 	stb	r3, HSTATE_HWTHREAD_REQ(r13)
+ 
+ 	b	kvm_do_nap
+ 
+ kvm_novcpu_wakeup:
+ 	ld	r1, HSTATE_HOST_R1(r13)
+ 	ld	r5, HSTATE_KVM_VCORE(r13)
+ 	li	r0, 0
+ 	stb	r0, HSTATE_NAPPING(r13)
+ 	stb	r0, HSTATE_HWTHREAD_REQ(r13)
+ 
+ 	/* check the wake reason */
+ 	bl	kvmppc_check_wake_reason
+ 	
+ 	/* see if any other thread is already exiting */
+ 	lwz	r0, VCORE_ENTRY_EXIT(r5)
+ 	cmpwi	r0, 0x100
+ 	bge	kvm_novcpu_exit
+ 
+ 	/* clear our bit in napping_threads */
+ 	lbz	r7, HSTATE_PTID(r13)
+ 	li	r0, 1
+ 	sld	r0, r0, r7
+ 	addi	r6, r5, VCORE_NAPPING_THREADS
+ 4:	lwarx	r7, 0, r6
+ 	andc	r7, r7, r0
+ 	stwcx.	r7, 0, r6
+ 	bne	4b
+ 
+ 	/* See if the wake reason means we need to exit */
+ 	cmpdi	r3, 0
+ 	bge	kvm_novcpu_exit
+ 
+ 	/* Got an IPI but other vcpus aren't yet exiting, must be a latecomer */
+ 	ld	r4, HSTATE_KVM_VCPU(r13)
+ 	cmpdi	r4, 0
+ 	bne	kvmppc_got_guest
+ 
+ kvm_novcpu_exit:
+ 	b	hdec_soon
+ 
++>>>>>>> e3bbbbfa13ea (KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap)
  /*
 - * We come in here when wakened from nap mode.
 + * We come in here when wakened from nap mode on a secondary hw thread.
   * Relocation is off and most register values are lost.
   * r13 points to the PACA.
   */
@@@ -209,40 -266,19 +261,36 @@@ kvm_start_guest
  	cmpdi	r4,0
  	/* if we have no vcpu to run, go back to sleep */
  	beq	kvm_no_guest
- 	b	30f
  
++<<<<<<< HEAD
 +27:	/* XXX should handle hypervisor maintenance interrupts etc. here */
 +	b	kvm_no_guest
 +28:	/* SRR1 said external but ICP said nope?? */
 +	b	kvm_no_guest
 +29:	/* External non-IPI interrupt to offline secondary thread? help?? */
 +	stw	r8,HSTATE_SAVED_XIRR(r13)
 +	b	kvm_no_guest
 +
 +30:	bl	kvmppc_hv_entry
++=======
+ 	/* Set HSTATE_DSCR(r13) to something sensible */
+ 	LOAD_REG_ADDR(r6, dscr_default)
+ 	ld	r6, 0(r6)
+ 	std	r6, HSTATE_DSCR(r13)
+ 
+ 	bl	kvmppc_hv_entry
++>>>>>>> e3bbbbfa13ea (KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap)
  
  	/* Back from the guest, go back to nap */
  	/* Clear our vcpu pointer so we don't come back in early */
  	li	r0, 0
  	std	r0, HSTATE_KVM_VCPU(r13)
 +	/*
 +	 * Make sure we clear HSTATE_KVM_VCPU(r13) before incrementing
 +	 * the nap_count, because once the increment to nap_count is
 +	 * visible we could be given another vcpu.
 +	 */
  	lwsync
- 	/* Clear any pending IPI - we're an offline thread */
- 	ld	r5, HSTATE_XICS_PHYS(r13)
- 	li	r7, XICS_XIRR
- 	lwzcix	r3, r5, r7		/* ack any pending interrupt */
- 	rlwinm.	r0, r3, 0, 0xffffff	/* any pending? */
- 	beq	37f
- 	sync
- 	li	r0, 0xff
- 	li	r6, XICS_MFRR
- 	stbcix	r0, r5, r6		/* clear the IPI */
- 	stwcix	r3, r5, r7		/* EOI it */
- 37:	sync
  
  	/* increment the nap count and then go to nap mode */
  	ld	r4, HSTATE_KVM_VCORE(r13)
@@@ -642,62 -559,262 +690,61 @@@ toc_tlbie_lock
  	addi	r6,r6,VCPU_SLB_SIZE
  	bdnz	1b
  9:
 -	/* Increment yield count if they have a VPA */
 -	ld	r3, VCPU_VPA(r4)
 -	cmpdi	r3, 0
 -	beq	25f
 -	lwz	r5, LPPACA_YIELDCOUNT(r3)
 -	addi	r5, r5, 1
 -	stw	r5, LPPACA_YIELDCOUNT(r3)
 -	li	r6, 1
 -	stb	r6, VCPU_VPA_DIRTY(r4)
 -25:
  
 -BEGIN_FTR_SECTION
 -	/* Save purr/spurr */
 -	mfspr	r5,SPRN_PURR
 -	mfspr	r6,SPRN_SPURR
 -	std	r5,HSTATE_PURR(r13)
 -	std	r6,HSTATE_SPURR(r13)
 -	ld	r7,VCPU_PURR(r4)
 -	ld	r8,VCPU_SPURR(r4)
 -	mtspr	SPRN_PURR,r7
 -	mtspr	SPRN_SPURR,r8
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +	/* Restore state of CTRL run bit; assume 1 on entry */
 +	lwz	r5,VCPU_CTRL(r4)
 +	andi.	r5,r5,1
 +	bne	4f
 +	mfspr	r6,SPRN_CTRLF
 +	clrrdi	r6,r6,1
 +	mtspr	SPRN_CTRLT,r6
 +4:
 +	ld	r6, VCPU_CTR(r4)
 +	lwz	r7, VCPU_XER(r4)
 +
 +	mtctr	r6
 +	mtxer	r7
 +
++kvmppc_cede_reentry:		/* r4 = vcpu, r13 = paca */
 +	ld	r10, VCPU_PC(r4)
 +	ld	r11, VCPU_MSR(r4)
- kvmppc_cede_reentry:		/* r4 = vcpu, r13 = paca */
 +	ld	r6, VCPU_SRR0(r4)
 +	ld	r7, VCPU_SRR1(r4)
++	mtspr	SPRN_SRR0, r6
++	mtspr	SPRN_SRR1, r7
  
++deliver_guest_interrupt:
 +	/* r11 = vcpu->arch.msr & ~MSR_HV */
 +	rldicl	r11, r11, 63 - MSR_HV_LG, 1
 +	rotldi	r11, r11, 1 + MSR_HV_LG
 +	ori	r11, r11, MSR_ME
 +
 +	/* Check if we can deliver an external or decrementer interrupt now */
- 	ld	r0,VCPU_PENDING_EXC(r4)
- 	lis	r8,(1 << BOOK3S_IRQPRIO_EXTERNAL_LEVEL)@h
- 	and	r0,r0,r8
- 	cmpdi	cr1,r0,0
- 	andi.	r0,r11,MSR_EE
- 	beq	cr1,11f
++	ld	r0, VCPU_PENDING_EXC(r4)
++	rldicl	r0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63
++	cmpdi	cr1, r0, 0
++	andi.	r8, r11, MSR_EE
  BEGIN_FTR_SECTION
- 	mfspr	r8,SPRN_LPCR
- 	ori	r8,r8,LPCR_MER
- 	mtspr	SPRN_LPCR,r8
 -	/* Set partition DABR */
 -	/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */
 -	li	r5,3
 -	ld	r6,VCPU_DABR(r4)
 -	mtspr	SPRN_DABRX,r5
 -	mtspr	SPRN_DABR,r6
 - BEGIN_FTR_SECTION_NESTED(89)
++	mfspr	r8, SPRN_LPCR
++	/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */
++	rldimi	r8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH
++	mtspr	SPRN_LPCR, r8
  	isync
 - END_FTR_SECTION_NESTED(CPU_FTR_ARCH_206, CPU_FTR_ARCH_206, 89)
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 -
 -	/* Load guest PMU registers */
 -	/* R4 is live here (vcpu pointer) */
 -	li	r3, 1
 -	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
 -	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
 -	isync
 -	lwz	r3, VCPU_PMC(r4)	/* always load up guest PMU registers */
 -	lwz	r5, VCPU_PMC + 4(r4)	/* to prevent information leak */
 -	lwz	r6, VCPU_PMC + 8(r4)
 -	lwz	r7, VCPU_PMC + 12(r4)
 -	lwz	r8, VCPU_PMC + 16(r4)
 -	lwz	r9, VCPU_PMC + 20(r4)
 -BEGIN_FTR_SECTION
 -	lwz	r10, VCPU_PMC + 24(r4)
 -	lwz	r11, VCPU_PMC + 28(r4)
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 -	mtspr	SPRN_PMC1, r3
 -	mtspr	SPRN_PMC2, r5
 -	mtspr	SPRN_PMC3, r6
 -	mtspr	SPRN_PMC4, r7
 -	mtspr	SPRN_PMC5, r8
 -	mtspr	SPRN_PMC6, r9
 -BEGIN_FTR_SECTION
 -	mtspr	SPRN_PMC7, r10
 -	mtspr	SPRN_PMC8, r11
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 -	ld	r3, VCPU_MMCR(r4)
 -	ld	r5, VCPU_MMCR + 8(r4)
 -	ld	r6, VCPU_MMCR + 16(r4)
 -	ld	r7, VCPU_SIAR(r4)
 -	ld	r8, VCPU_SDAR(r4)
 -	mtspr	SPRN_MMCR1, r5
 -	mtspr	SPRN_MMCRA, r6
 -	mtspr	SPRN_SIAR, r7
 -	mtspr	SPRN_SDAR, r8
 -BEGIN_FTR_SECTION
 -	ld	r5, VCPU_MMCR + 24(r4)
 -	ld	r6, VCPU_SIER(r4)
 -	lwz	r7, VCPU_PMC + 24(r4)
 -	lwz	r8, VCPU_PMC + 28(r4)
 -	ld	r9, VCPU_MMCR + 32(r4)
 -	mtspr	SPRN_MMCR2, r5
 -	mtspr	SPRN_SIER, r6
 -	mtspr	SPRN_SPMC1, r7
 -	mtspr	SPRN_SPMC2, r8
 -	mtspr	SPRN_MMCRS, r9
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
 -	mtspr	SPRN_MMCR0, r3
 -	isync
 -
 -	/* Load up FP, VMX and VSX registers */
 -	bl	kvmppc_load_fp
 -
 -	ld	r14, VCPU_GPR(R14)(r4)
 -	ld	r15, VCPU_GPR(R15)(r4)
 -	ld	r16, VCPU_GPR(R16)(r4)
 -	ld	r17, VCPU_GPR(R17)(r4)
 -	ld	r18, VCPU_GPR(R18)(r4)
 -	ld	r19, VCPU_GPR(R19)(r4)
 -	ld	r20, VCPU_GPR(R20)(r4)
 -	ld	r21, VCPU_GPR(R21)(r4)
 -	ld	r22, VCPU_GPR(R22)(r4)
 -	ld	r23, VCPU_GPR(R23)(r4)
 -	ld	r24, VCPU_GPR(R24)(r4)
 -	ld	r25, VCPU_GPR(R25)(r4)
 -	ld	r26, VCPU_GPR(R26)(r4)
 -	ld	r27, VCPU_GPR(R27)(r4)
 -	ld	r28, VCPU_GPR(R28)(r4)
 -	ld	r29, VCPU_GPR(R29)(r4)
 -	ld	r30, VCPU_GPR(R30)(r4)
 -	ld	r31, VCPU_GPR(R31)(r4)
 -
 -BEGIN_FTR_SECTION
 -	/* Switch DSCR to guest value */
 -	ld	r5, VCPU_DSCR(r4)
 -	mtspr	SPRN_DSCR, r5
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -BEGIN_FTR_SECTION
 -	/* Skip next section on POWER7 or PPC970 */
 -	b	8f
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 -	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
 -	mfmsr	r8
 -	li	r0, 1
 -	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
 -	mtmsrd	r8
 -
 -	/* Load up POWER8-specific registers */
 -	ld	r5, VCPU_IAMR(r4)
 -	lwz	r6, VCPU_PSPB(r4)
 -	ld	r7, VCPU_FSCR(r4)
 -	mtspr	SPRN_IAMR, r5
 -	mtspr	SPRN_PSPB, r6
 -	mtspr	SPRN_FSCR, r7
 -	ld	r5, VCPU_DAWR(r4)
 -	ld	r6, VCPU_DAWRX(r4)
 -	ld	r7, VCPU_CIABR(r4)
 -	ld	r8, VCPU_TAR(r4)
 -	mtspr	SPRN_DAWR, r5
 -	mtspr	SPRN_DAWRX, r6
 -	mtspr	SPRN_CIABR, r7
 -	mtspr	SPRN_TAR, r8
 -	ld	r5, VCPU_IC(r4)
 -	ld	r6, VCPU_VTB(r4)
 -	mtspr	SPRN_IC, r5
 -	mtspr	SPRN_VTB, r6
 -	ld	r5, VCPU_TFHAR(r4)
 -	ld	r6, VCPU_TFIAR(r4)
 -	ld	r7, VCPU_TEXASR(r4)
 -	ld	r8, VCPU_EBBHR(r4)
 -	mtspr	SPRN_TFHAR, r5
 -	mtspr	SPRN_TFIAR, r6
 -	mtspr	SPRN_TEXASR, r7
 -	mtspr	SPRN_EBBHR, r8
 -	ld	r5, VCPU_EBBRR(r4)
 -	ld	r6, VCPU_BESCR(r4)
 -	ld	r7, VCPU_CSIGR(r4)
 -	ld	r8, VCPU_TACR(r4)
 -	mtspr	SPRN_EBBRR, r5
 -	mtspr	SPRN_BESCR, r6
 -	mtspr	SPRN_CSIGR, r7
 -	mtspr	SPRN_TACR, r8
 -	ld	r5, VCPU_TCSCR(r4)
 -	ld	r6, VCPU_ACOP(r4)
 -	lwz	r7, VCPU_GUEST_PID(r4)
 -	ld	r8, VCPU_WORT(r4)
 -	mtspr	SPRN_TCSCR, r5
 -	mtspr	SPRN_ACOP, r6
 -	mtspr	SPRN_PID, r7
 -	mtspr	SPRN_WORT, r8
 -8:
 -
 -	/*
 -	 * Set the decrementer to the guest decrementer.
 -	 */
 -	ld	r8,VCPU_DEC_EXPIRES(r4)
 -	mftb	r7
 -	subf	r3,r7,r8
 -	mtspr	SPRN_DEC,r3
 -	stw	r3,VCPU_DEC(r4)
 -
 -	ld	r5, VCPU_SPRG0(r4)
 -	ld	r6, VCPU_SPRG1(r4)
 -	ld	r7, VCPU_SPRG2(r4)
 -	ld	r8, VCPU_SPRG3(r4)
 -	mtspr	SPRN_SPRG0, r5
 -	mtspr	SPRN_SPRG1, r6
 -	mtspr	SPRN_SPRG2, r7
 -	mtspr	SPRN_SPRG3, r8
 -
 -	/* Load up DAR and DSISR */
 -	ld	r5, VCPU_DAR(r4)
 -	lwz	r6, VCPU_DSISR(r4)
 -	mtspr	SPRN_DAR, r5
 -	mtspr	SPRN_DSISR, r6
 -
 -BEGIN_FTR_SECTION
 -	/* Restore AMR and UAMOR, set AMOR to all 1s */
 -	ld	r5,VCPU_AMR(r4)
 -	ld	r6,VCPU_UAMOR(r4)
 -	li	r7,-1
 -	mtspr	SPRN_AMR,r5
 -	mtspr	SPRN_UAMOR,r6
 -	mtspr	SPRN_AMOR,r7
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -	/* Restore state of CTRL run bit; assume 1 on entry */
 -	lwz	r5,VCPU_CTRL(r4)
 -	andi.	r5,r5,1
 -	bne	4f
 -	mfspr	r6,SPRN_CTRLF
 -	clrrdi	r6,r6,1
 -	mtspr	SPRN_CTRLT,r6
 -4:
 -	ld	r6, VCPU_CTR(r4)
 -	lwz	r7, VCPU_XER(r4)
 -
 -	mtctr	r6
 -	mtxer	r7
 -
 -kvmppc_cede_reentry:		/* r4 = vcpu, r13 = paca */
 -	ld	r10, VCPU_PC(r4)
 -	ld	r11, VCPU_MSR(r4)
 -	ld	r6, VCPU_SRR0(r4)
 -	ld	r7, VCPU_SRR1(r4)
 -	mtspr	SPRN_SRR0, r6
 -	mtspr	SPRN_SRR1, r7
 -
 -deliver_guest_interrupt:
 -	/* r11 = vcpu->arch.msr & ~MSR_HV */
 -	rldicl	r11, r11, 63 - MSR_HV_LG, 1
 -	rotldi	r11, r11, 1 + MSR_HV_LG
 -	ori	r11, r11, MSR_ME
 -
 -	/* Check if we can deliver an external or decrementer interrupt now */
 -	ld	r0, VCPU_PENDING_EXC(r4)
 -	rldicl	r0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63
 -	cmpdi	cr1, r0, 0
 -	andi.	r8, r11, MSR_EE
 -BEGIN_FTR_SECTION
 -	mfspr	r8, SPRN_LPCR
 -	/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */
 -	rldimi	r8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH
 -	mtspr	SPRN_LPCR, r8
 -	isync
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -	beq	5f
 -	li	r0, BOOK3S_INTERRUPT_EXTERNAL
 -	bne	cr1, 12f
 -	mfspr	r0, SPRN_DEC
 -	cmpwi	r0, 0
 -	li	r0, BOOK3S_INTERRUPT_DECREMENTER
 -	bge	5f
 +END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +	beq	5f
- 	li	r0,BOOK3S_INTERRUPT_EXTERNAL
- 12:	mr	r6,r10
++	li	r0, BOOK3S_INTERRUPT_EXTERNAL
++	bne	cr1, 12f
++	mfspr	r0, SPRN_DEC
++	cmpwi	r0, 0
++	li	r0, BOOK3S_INTERRUPT_DECREMENTER
++	bge	5f
+ 
+ 12:	mtspr	SPRN_SRR0, r10
  	mr	r10,r0
- 	mr	r7,r11
+ 	mtspr	SPRN_SRR1, r11
  	li	r11,(MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
  	rotldi	r11,r11,63
- 	b	5f
- 11:	beq	5f
- 	mfspr	r0,SPRN_DEC
- 	cmpwi	r0,0
- 	li	r0,BOOK3S_INTERRUPT_DECREMENTER
- 	blt	12b
- 
- 	/* Move SRR0 and SRR1 into the respective regs */
- 5:	mtspr	SPRN_SRR0, r6
- 	mtspr	SPRN_SRR1, r7
+ 5:
  
  /*
   * Required state:
@@@ -872,127 -989,286 +919,107 @@@ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206
  2:
  	/* See if this is an hcall we can handle in real mode */
  	cmpwi	r12,BOOK3S_INTERRUPT_SYSCALL
 -	beq	hcall_try_real_mode
 -
 -	/* Only handle external interrupts here on arch 206 and later */
 -BEGIN_FTR_SECTION
 -	b	ext_interrupt_to_host
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_206)
 -
 -	/* External interrupt ? */
 -	cmpwi	r12, BOOK3S_INTERRUPT_EXTERNAL
 -	bne+	ext_interrupt_to_host
 -
 -	/* External interrupt, first check for host_ipi. If this is
 -	 * set, we know the host wants us out so let's do it now
 -	 */
 -	bl	kvmppc_read_intr
 -	cmpdi	r3, 0
 -	bgt	ext_interrupt_to_host
 -
 -	/* Check if any CPU is heading out to the host, if so head out too */
 -	ld	r5, HSTATE_KVM_VCORE(r13)
 -	lwz	r0, VCORE_ENTRY_EXIT(r5)
 -	cmpwi	r0, 0x100
 -	bge	ext_interrupt_to_host
 -
 -	/* Return to guest after delivering any pending interrupt */
 -	mr	r4, r9
 -	b	deliver_guest_interrupt
 -
 -ext_interrupt_to_host:
 -
 -guest_exit_cont:		/* r9 = vcpu, r12 = trap, r13 = paca */
 -	/* Save more register state  */
 -	mfdar	r6
 -	mfdsisr	r7
 -	std	r6, VCPU_DAR(r9)
 -	stw	r7, VCPU_DSISR(r9)
 -BEGIN_FTR_SECTION
 -	/* don't overwrite fault_dar/fault_dsisr if HDSI */
 -	cmpwi	r12,BOOK3S_INTERRUPT_H_DATA_STORAGE
 -	beq	6f
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -	std	r6, VCPU_FAULT_DAR(r9)
 -	stw	r7, VCPU_FAULT_DSISR(r9)
 -
 -	/* See if it is a machine check */
 -	cmpwi	r12, BOOK3S_INTERRUPT_MACHINE_CHECK
 -	beq	machine_check_realmode
 -mc_cont:
 -
 -	/* Save guest CTRL register, set runlatch to 1 */
 -6:	mfspr	r6,SPRN_CTRLF
 -	stw	r6,VCPU_CTRL(r9)
 -	andi.	r0,r6,1
 -	bne	4f
 -	ori	r6,r6,1
 -	mtspr	SPRN_CTRLT,r6
 -4:
 -	/* Read the guest SLB and save it away */
 -	lwz	r0,VCPU_SLB_NR(r9)	/* number of entries in SLB */
 -	mtctr	r0
 -	li	r6,0
 -	addi	r7,r9,VCPU_SLB
 -	li	r5,0
 -1:	slbmfee	r8,r6
 -	andis.	r0,r8,SLB_ESID_V@h
 -	beq	2f
 -	add	r8,r8,r6		/* put index in */
 -	slbmfev	r3,r6
 -	std	r8,VCPU_SLB_E(r7)
 -	std	r3,VCPU_SLB_V(r7)
 -	addi	r7,r7,VCPU_SLB_SIZE
 -	addi	r5,r5,1
 -2:	addi	r6,r6,1
 -	bdnz	1b
 -	stw	r5,VCPU_SLB_MAX(r9)
 -
 -	/*
 -	 * Save the guest PURR/SPURR
 -	 */
 -BEGIN_FTR_SECTION
 -	mfspr	r5,SPRN_PURR
 -	mfspr	r6,SPRN_SPURR
 -	ld	r7,VCPU_PURR(r9)
 -	ld	r8,VCPU_SPURR(r9)
 -	std	r5,VCPU_PURR(r9)
 -	std	r6,VCPU_SPURR(r9)
 -	subf	r5,r7,r5
 -	subf	r6,r8,r6
 -
 -	/*
 -	 * Restore host PURR/SPURR and add guest times
 -	 * so that the time in the guest gets accounted.
 -	 */
 -	ld	r3,HSTATE_PURR(r13)
 -	ld	r4,HSTATE_SPURR(r13)
 -	add	r3,r3,r5
 -	add	r4,r4,r6
 -	mtspr	SPRN_PURR,r3
 -	mtspr	SPRN_SPURR,r4
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_201)
 -
 -	/* Save DEC */
 -	mfspr	r5,SPRN_DEC
 -	mftb	r6
 -	extsw	r5,r5
 -	add	r5,r5,r6
 -	std	r5,VCPU_DEC_EXPIRES(r9)
 -
 -BEGIN_FTR_SECTION
 -	b	8f
 -END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
 -	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
 -	mfmsr	r8
 -	li	r0, 1
 -	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
 -	mtmsrd	r8
 -
 -	/* Save POWER8-specific registers */
 -	mfspr	r5, SPRN_IAMR
 -	mfspr	r6, SPRN_PSPB
 -	mfspr	r7, SPRN_FSCR
 -	std	r5, VCPU_IAMR(r9)
 -	stw	r6, VCPU_PSPB(r9)
 -	std	r7, VCPU_FSCR(r9)
 -	mfspr	r5, SPRN_IC
 -	mfspr	r6, SPRN_VTB
 -	mfspr	r7, SPRN_TAR
 -	std	r5, VCPU_IC(r9)
 -	std	r6, VCPU_VTB(r9)
 -	std	r7, VCPU_TAR(r9)
 -	mfspr	r5, SPRN_TFHAR
 -	mfspr	r6, SPRN_TFIAR
 -	mfspr	r7, SPRN_TEXASR
 -	mfspr	r8, SPRN_EBBHR
 -	std	r5, VCPU_TFHAR(r9)
 -	std	r6, VCPU_TFIAR(r9)
 -	std	r7, VCPU_TEXASR(r9)
 -	std	r8, VCPU_EBBHR(r9)
 -	mfspr	r5, SPRN_EBBRR
 -	mfspr	r6, SPRN_BESCR
 -	mfspr	r7, SPRN_CSIGR
 -	mfspr	r8, SPRN_TACR
 -	std	r5, VCPU_EBBRR(r9)
 -	std	r6, VCPU_BESCR(r9)
 -	std	r7, VCPU_CSIGR(r9)
 -	std	r8, VCPU_TACR(r9)
 -	mfspr	r5, SPRN_TCSCR
 -	mfspr	r6, SPRN_ACOP
 -	mfspr	r7, SPRN_PID
 -	mfspr	r8, SPRN_WORT
 -	std	r5, VCPU_TCSCR(r9)
 -	std	r6, VCPU_ACOP(r9)
 -	stw	r7, VCPU_GUEST_PID(r9)
 -	std	r8, VCPU_WORT(r9)
 -8:
 -
 -	/* Save and reset AMR and UAMOR before turning on the MMU */
 -BEGIN_FTR_SECTION
 -	mfspr	r5,SPRN_AMR
 -	mfspr	r6,SPRN_UAMOR
 -	std	r5,VCPU_AMR(r9)
 -	std	r6,VCPU_UAMOR(r9)
 -	li	r6,0
 -	mtspr	SPRN_AMR,r6
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -	/* Switch DSCR back to host value */
 -BEGIN_FTR_SECTION
 -	mfspr	r8, SPRN_DSCR
 -	ld	r7, HSTATE_DSCR(r13)
 -	std	r8, VCPU_DSCR(r9)
 -	mtspr	SPRN_DSCR, r7
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -
 -	/* Save non-volatile GPRs */
 -	std	r14, VCPU_GPR(R14)(r9)
 -	std	r15, VCPU_GPR(R15)(r9)
 -	std	r16, VCPU_GPR(R16)(r9)
 -	std	r17, VCPU_GPR(R17)(r9)
 -	std	r18, VCPU_GPR(R18)(r9)
 -	std	r19, VCPU_GPR(R19)(r9)
 -	std	r20, VCPU_GPR(R20)(r9)
 -	std	r21, VCPU_GPR(R21)(r9)
 -	std	r22, VCPU_GPR(R22)(r9)
 -	std	r23, VCPU_GPR(R23)(r9)
 -	std	r24, VCPU_GPR(R24)(r9)
 -	std	r25, VCPU_GPR(R25)(r9)
 -	std	r26, VCPU_GPR(R26)(r9)
 -	std	r27, VCPU_GPR(R27)(r9)
 -	std	r28, VCPU_GPR(R28)(r9)
 -	std	r29, VCPU_GPR(R29)(r9)
 -	std	r30, VCPU_GPR(R30)(r9)
 -	std	r31, VCPU_GPR(R31)(r9)
 -
 -	/* Save SPRGs */
 -	mfspr	r3, SPRN_SPRG0
 -	mfspr	r4, SPRN_SPRG1
 -	mfspr	r5, SPRN_SPRG2
 -	mfspr	r6, SPRN_SPRG3
 -	std	r3, VCPU_SPRG0(r9)
 -	std	r4, VCPU_SPRG1(r9)
 -	std	r5, VCPU_SPRG2(r9)
 -	std	r6, VCPU_SPRG3(r9)
 -
 -	/* save FP state */
 -	mr	r3, r9
 -	bl	kvmppc_save_fp
 -
 -	/* Increment yield count if they have a VPA */
 -	ld	r8, VCPU_VPA(r9)	/* do they have a VPA? */
 -	cmpdi	r8, 0
 -	beq	25f
 -	lwz	r3, LPPACA_YIELDCOUNT(r8)
 -	addi	r3, r3, 1
 -	stw	r3, LPPACA_YIELDCOUNT(r8)
 -	li	r3, 1
 -	stb	r3, VCPU_VPA_DIRTY(r9)
 -25:
 -	/* Save PMU registers if requested */
 -	/* r8 and cr0.eq are live here */
 -	li	r3, 1
 -	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
 -	mfspr	r4, SPRN_MMCR0		/* save MMCR0 */
 -	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
 -	mfspr	r6, SPRN_MMCRA
 -BEGIN_FTR_SECTION
 -	/* On P7, clear MMCRA in order to disable SDAR updates */
 -	li	r7, 0
 -	mtspr	SPRN_MMCRA, r7
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 -	isync
 -	beq	21f			/* if no VPA, save PMU stuff anyway */
 -	lbz	r7, LPPACA_PMCINUSE(r8)
 -	cmpwi	r7, 0			/* did they ask for PMU stuff to be saved? */
 -	bne	21f
 -	std	r3, VCPU_MMCR(r9)	/* if not, set saved MMCR0 to FC */
 -	b	22f
 -21:	mfspr	r5, SPRN_MMCR1
 -	mfspr	r7, SPRN_SIAR
 -	mfspr	r8, SPRN_SDAR
 -	std	r4, VCPU_MMCR(r9)
 -	std	r5, VCPU_MMCR + 8(r9)
 -	std	r6, VCPU_MMCR + 16(r9)
 -	std	r7, VCPU_SIAR(r9)
 -	std	r8, VCPU_SDAR(r9)
 -	mfspr	r3, SPRN_PMC1
 -	mfspr	r4, SPRN_PMC2
 -	mfspr	r5, SPRN_PMC3
 -	mfspr	r6, SPRN_PMC4
 -	mfspr	r7, SPRN_PMC5
 -	mfspr	r8, SPRN_PMC6
 +	beq	hcall_try_real_mode
 +
 +	/* Only handle external interrupts here on arch 206 and later */
  BEGIN_FTR_SECTION
 -	mfspr	r10, SPRN_PMC7
 -	mfspr	r11, SPRN_PMC8
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 -	stw	r3, VCPU_PMC(r9)
 -	stw	r4, VCPU_PMC + 4(r9)
 -	stw	r5, VCPU_PMC + 8(r9)
 -	stw	r6, VCPU_PMC + 12(r9)
 -	stw	r7, VCPU_PMC + 16(r9)
 -	stw	r8, VCPU_PMC + 20(r9)
 +	b	ext_interrupt_to_host
 +END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_206)
 +
 +	/* External interrupt ? */
 +	cmpwi	r12, BOOK3S_INTERRUPT_EXTERNAL
 +	bne+	ext_interrupt_to_host
 +
 +	/* External interrupt, first check for host_ipi. If this is
 +	 * set, we know the host wants us out so let's do it now
 +	 */
- do_ext_interrupt:
 +	bl	kvmppc_read_intr
 +	cmpdi	r3, 0
 +	bgt	ext_interrupt_to_host
 +
- 	/* Allright, looks like an IPI for the guest, we need to set MER */
 +	/* Check if any CPU is heading out to the host, if so head out too */
 +	ld	r5, HSTATE_KVM_VCORE(r13)
 +	lwz	r0, VCORE_ENTRY_EXIT(r5)
 +	cmpwi	r0, 0x100
 +	bge	ext_interrupt_to_host
 +
- 	/* See if there is a pending interrupt for the guest */
- 	mfspr	r8, SPRN_LPCR
- 	ld	r0, VCPU_PENDING_EXC(r9)
- 	/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */
- 	rldicl.	r0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63
- 	rldimi	r8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH
- 	beq	2f
- 
- 	/* And if the guest EE is set, we can deliver immediately, else
- 	 * we return to the guest with MER set
- 	 */
- 	andi.	r0, r11, MSR_EE
- 	beq	2f
- 	mtspr	SPRN_SRR0, r10
- 	mtspr	SPRN_SRR1, r11
- 	li	r10, BOOK3S_INTERRUPT_EXTERNAL
- 	li	r11, (MSR_ME << 1) | 1	/* synthesize MSR_SF | MSR_ME */
- 	rotldi	r11, r11, 63
- 2:	mr	r4, r9
- 	mtspr	SPRN_LPCR, r8
- 	b	fast_guest_return
++	/* Return to guest after delivering any pending interrupt */
++	mr	r4, r9
++	b	deliver_guest_interrupt
 +
 +ext_interrupt_to_host:
 +
 +guest_exit_cont:		/* r9 = vcpu, r12 = trap, r13 = paca */
 +	/* Save more register state  */
 +	mfdar	r6
 +	mfdsisr	r7
 +	std	r6, VCPU_DAR(r9)
 +	stw	r7, VCPU_DSISR(r9)
  BEGIN_FTR_SECTION
 -	stw	r10, VCPU_PMC + 24(r9)
 -	stw	r11, VCPU_PMC + 28(r9)
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
 +	/* don't overwrite fault_dar/fault_dsisr if HDSI */
 +	cmpwi	r12,BOOK3S_INTERRUPT_H_DATA_STORAGE
 +	beq	6f
 +END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
 +	std	r6, VCPU_FAULT_DAR(r9)
 +	stw	r7, VCPU_FAULT_DSISR(r9)
 +
 +	/* See if it is a machine check */
 +	cmpwi	r12, BOOK3S_INTERRUPT_MACHINE_CHECK
 +	beq	machine_check_realmode
 +mc_cont:
 +
 +	/* Save guest CTRL register, set runlatch to 1 */
 +6:	mfspr	r6,SPRN_CTRLF
 +	stw	r6,VCPU_CTRL(r9)
 +	andi.	r0,r6,1
 +	bne	4f
 +	ori	r6,r6,1
 +	mtspr	SPRN_CTRLT,r6
 +4:
 +	/* Read the guest SLB and save it away */
 +	lwz	r0,VCPU_SLB_NR(r9)	/* number of entries in SLB */
 +	mtctr	r0
 +	li	r6,0
 +	addi	r7,r9,VCPU_SLB
 +	li	r5,0
 +1:	slbmfee	r8,r6
 +	andis.	r0,r8,SLB_ESID_V@h
 +	beq	2f
 +	add	r8,r8,r6		/* put index in */
 +	slbmfev	r3,r6
 +	std	r8,VCPU_SLB_E(r7)
 +	std	r3,VCPU_SLB_V(r7)
 +	addi	r7,r7,VCPU_SLB_SIZE
 +	addi	r5,r5,1
 +2:	addi	r6,r6,1
 +	bdnz	1b
 +	stw	r5,VCPU_SLB_MAX(r9)
 +
 +	/*
 +	 * Save the guest PURR/SPURR
 +	 */
  BEGIN_FTR_SECTION
 -	mfspr	r4, SPRN_MMCR2
 -	mfspr	r5, SPRN_SIER
 -	mfspr	r6, SPRN_SPMC1
 -	mfspr	r7, SPRN_SPMC2
 -	mfspr	r8, SPRN_MMCRS
 -	std	r4, VCPU_MMCR + 24(r9)
 -	std	r5, VCPU_SIER(r9)
 -	stw	r6, VCPU_PMC + 24(r9)
 -	stw	r7, VCPU_PMC + 28(r9)
 -	std	r8, VCPU_MMCR + 32(r9)
 -	lis	r4, 0x8000
 -	mtspr	SPRN_MMCRS, r4
 -END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
 -22:
 +	mfspr	r5,SPRN_PURR
 +	mfspr	r6,SPRN_SPURR
 +	ld	r7,VCPU_PURR(r9)
 +	ld	r8,VCPU_SPURR(r9)
 +	std	r5,VCPU_PURR(r9)
 +	std	r6,VCPU_SPURR(r9)
 +	subf	r5,r7,r5
 +	subf	r6,r8,r6
 +
 +	/*
 +	 * Restore host PURR/SPURR and add guest times
 +	 * so that the time in the guest gets accounted.
 +	 */
 +	ld	r3,HSTATE_PURR(r13)
 +	ld	r4,HSTATE_SPURR(r13)
 +	add	r3,r3,r5
 +	add	r4,r4,r6
 +	mtspr	SPRN_PURR,r3
 +	mtspr	SPRN_SPURR,r4
 +END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_201)
 +
  	/* Clear out SLB */
  	li	r5,0
  	slbmte	r5,r5
@@@ -1672,11 -1819,10 +1699,15 @@@ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_206
  	bge	kvm_cede_exit
  	stwcx.	r4,0,r6
  	bne	31b
 -	li	r0,NAPPING_CEDE
 -	stb	r0,HSTATE_NAPPING(r13)
  	/* order napping_threads update vs testing entry_exit_count */
++<<<<<<< HEAD
 +	isync
 +	li	r0,1
 +	stb	r0,HSTATE_NAPPING(r13)
 +	mr	r4,r3
++=======
+ 	lwsync
++>>>>>>> e3bbbbfa13ea (KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap)
  	lwz	r7,VCORE_ENTRY_EXIT(r5)
  	cmpwi	r7,0x100
  	bge	33f		/* another thread already exiting */
@@@ -1758,12 -1909,15 +1794,20 @@@ kvm_end_cede
  	ld	r29, VCPU_GPR(R29)(r4)
  	ld	r30, VCPU_GPR(R30)(r4)
  	ld	r31, VCPU_GPR(R31)(r4)
+  
+ 	/* Check the wake reason in SRR1 to see why we got here */
+ 	bl	kvmppc_check_wake_reason
  
  	/* clear our bit in vcore->napping_threads */
++<<<<<<< HEAD
 +33:	ld	r5,HSTATE_KVM_VCORE(r13)
 +	lwz	r3,VCPU_PTID(r4)
++=======
+ 34:	ld	r5,HSTATE_KVM_VCORE(r13)
+ 	lbz	r7,HSTATE_PTID(r13)
++>>>>>>> e3bbbbfa13ea (KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap)
  	li	r0,1
- 	sld	r0,r0,r3
+ 	sld	r0,r0,r7
  	addi	r6,r5,VCORE_NAPPING_THREADS
  32:	lwarx	r7,0,r6
  	andc	r7,r7,r0
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
