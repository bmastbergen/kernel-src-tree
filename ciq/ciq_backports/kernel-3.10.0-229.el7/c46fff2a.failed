smp: Rename __smp_call_function_single() to smp_call_function_single_async()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Frederic Weisbecker <fweisbec@gmail.com>
commit c46fff2a3b29794b35d717b5680a27f31a6a6bc0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/c46fff2a.failed

The name __smp_call_function_single() doesn't tell much about the
properties of this function, especially when compared to
smp_call_function_single().

The comments above the implementation are also misleading. The main
point of this function is actually not to be able to embed the csd
in an object. This is actually a requirement that result from the
purpose of this function which is to raise an IPI asynchronously.

As such it can be called with interrupts disabled. And this feature
comes at the cost of the caller who then needs to serialize the
IPIs on this csd.

Lets rename the function and enhance the comments so that they reflect
these properties.

	Suggested-by: Christoph Hellwig <hch@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Jens Axboe <axboe@fb.com>
	Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit c46fff2a3b29794b35d717b5680a27f31a6a6bc0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	block/blk-softirq.c
#	drivers/cpuidle/coupled.c
#	include/linux/smp.h
#	kernel/sched/core.c
#	kernel/smp.c
#	kernel/up.c
#	net/core/dev.c
diff --cc block/blk-mq.c
index 32c41cfd8e8a,6468a715a0e4..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -320,7 -353,7 +320,11 @@@ void __blk_mq_complete_request(struct r
  		rq->csd.func = __blk_mq_complete_request_remote;
  		rq->csd.info = rq;
  		rq->csd.flags = 0;
++<<<<<<< HEAD
 +		__smp_call_function_single(ctx->cpu, &rq->csd, 0);
++=======
+ 		smp_call_function_single_async(ctx->cpu, &rq->csd);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  	} else {
  		rq->q->softirq_done_fn(rq);
  	}
diff --cc block/blk-softirq.c
index ec9e60636f43,ebd6b6f1bdeb..000000000000
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@@ -65,7 -70,7 +65,11 @@@ static int raise_blk_irq(int cpu, struc
  		data->info = rq;
  		data->flags = 0;
  
++<<<<<<< HEAD
 +		__smp_call_function_single(cpu, data, 0);
++=======
+ 		smp_call_function_single_async(cpu, data);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  		return 0;
  	}
  
diff --cc drivers/cpuidle/coupled.c
index fe853903fe10,cb6654bfad77..000000000000
--- a/drivers/cpuidle/coupled.c
+++ b/drivers/cpuidle/coupled.c
@@@ -323,7 -323,7 +323,11 @@@ static void cpuidle_coupled_poke(int cp
  	struct call_single_data *csd = &per_cpu(cpuidle_coupled_poke_cb, cpu);
  
  	if (!cpumask_test_and_set_cpu(cpu, &cpuidle_coupled_poke_pending))
++<<<<<<< HEAD
 +		__smp_call_function_single(cpu, csd, 0);
++=======
+ 		smp_call_function_single_async(cpu, csd);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  }
  
  /**
diff --cc include/linux/smp.h
index 2bbbb7e1e96a,633f5edd7470..000000000000
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@@ -29,8 -29,28 +29,33 @@@ extern unsigned int total_cpus
  int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
  			     int wait);
  
++<<<<<<< HEAD
 +void __smp_call_function_single(int cpuid, struct call_single_data *data,
 +				int wait);
++=======
+ /*
+  * Call a function on all processors
+  */
+ int on_each_cpu(smp_call_func_t func, void *info, int wait);
+ 
+ /*
+  * Call a function on processors specified by mask, which might include
+  * the local one.
+  */
+ void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
+ 		void *info, bool wait);
+ 
+ /*
+  * Call a function on each processor for which the supplied function
+  * cond_func returns a positive value. This may include the local
+  * processor.
+  */
+ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
+ 		smp_call_func_t func, void *info, bool wait,
+ 		gfp_t gfp_flags);
+ 
+ int smp_call_function_single_async(int cpu, struct call_single_data *csd);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  
  #ifdef CONFIG_SMP
  
diff --cc kernel/sched/core.c
index d145b810c4ff,0cca04a53de0..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -430,9 -430,9 +430,13 @@@ void hrtick_start(struct rq *rq, u64 de
  	hrtimer_set_expires(timer, time);
  
  	if (rq == this_rq()) {
 -		__hrtick_restart(rq);
 +		hrtimer_restart(timer);
  	} else if (!rq->hrtick_csd_pending) {
++<<<<<<< HEAD
 +		__smp_call_function_single(cpu_of(rq), &rq->hrtick_csd, 0);
++=======
+ 		smp_call_function_single_async(cpu_of(rq), &rq->hrtick_csd);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  		rq->hrtick_csd_pending = 1;
  	}
  }
diff --cc kernel/smp.c
index 74100ac030d3,06d574e42c72..000000000000
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@@ -264,6 -237,34 +264,37 @@@ int smp_call_function_single(int cpu, s
  }
  EXPORT_SYMBOL(smp_call_function_single);
  
++<<<<<<< HEAD
++=======
+ /**
+  * smp_call_function_single_async(): Run an asynchronous function on a
+  * 			         specific CPU.
+  * @cpu: The CPU to run on.
+  * @csd: Pre-allocated and setup data structure
+  *
+  * Like smp_call_function_single(), but the call is asynchonous and
+  * can thus be done from contexts with disabled interrupts.
+  *
+  * The caller passes his own pre-allocated data structure
+  * (ie: embedded in an object) and is responsible for synchronizing it
+  * such that the IPIs performed on the @csd are strictly serialized.
+  *
+  * NOTE: Be careful, there is unfortunately no current debugging facility to
+  * validate the correctness of this serialization.
+  */
+ int smp_call_function_single_async(int cpu, struct call_single_data *csd)
+ {
+ 	int err = 0;
+ 
+ 	preempt_disable();
+ 	err = generic_exec_single(cpu, csd, csd->func, csd->info, 0);
+ 	preempt_enable();
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(smp_call_function_single_async);
+ 
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  /*
   * smp_call_function_any - Run a function on any of the given cpus
   * @mask: The mask of cpus it can run on.
diff --cc kernel/up.c
index 7841a634056c,1760bf3d1463..000000000000
--- a/kernel/up.c
+++ b/kernel/up.c
@@@ -20,13 -22,63 +20,71 @@@ int smp_call_function_single(int cpu, v
  }
  EXPORT_SYMBOL(smp_call_function_single);
  
++<<<<<<< HEAD
 +void __smp_call_function_single(int cpu, struct call_single_data *csd,
 +				int wait)
++=======
+ int smp_call_function_single_async(int cpu, struct call_single_data *csd)
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  {
  	unsigned long flags;
  
  	local_irq_save(flags);
  	csd->func(csd->info);
  	local_irq_restore(flags);
 -	return 0;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(__smp_call_function_single);
++=======
+ EXPORT_SYMBOL(smp_call_function_single_async);
+ 
+ int on_each_cpu(smp_call_func_t func, void *info, int wait)
+ {
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+ 	func(info);
+ 	local_irq_restore(flags);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(on_each_cpu);
+ 
+ /*
+  * Note we still need to test the mask even for UP
+  * because we actually can get an empty mask from
+  * code that on SMP might call us without the local
+  * CPU in the mask.
+  */
+ void on_each_cpu_mask(const struct cpumask *mask,
+ 		      smp_call_func_t func, void *info, bool wait)
+ {
+ 	unsigned long flags;
+ 
+ 	if (cpumask_test_cpu(0, mask)) {
+ 		local_irq_save(flags);
+ 		func(info);
+ 		local_irq_restore(flags);
+ 	}
+ }
+ EXPORT_SYMBOL(on_each_cpu_mask);
+ 
+ /*
+  * Preemption is disabled here to make sure the cond_func is called under the
+  * same condtions in UP and SMP.
+  */
+ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
+ 		      smp_call_func_t func, void *info, bool wait,
+ 		      gfp_t gfp_flags)
+ {
+ 	unsigned long flags;
+ 
+ 	preempt_disable();
+ 	if (cond_func(0, info)) {
+ 		local_irq_save(flags);
+ 		func(info);
+ 		local_irq_restore(flags);
+ 	}
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL(on_each_cpu_cond);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
diff --cc net/core/dev.c
index d06cd723bba7,ac7a2abb7f1a..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -4011,8 -4128,8 +4011,13 @@@ static void net_rps_action_and_irq_enab
  			struct softnet_data *next = remsd->rps_ipi_next;
  
  			if (cpu_online(remsd->cpu))
++<<<<<<< HEAD
 +				__smp_call_function_single(remsd->cpu,
 +							   &remsd->csd, 0);
++=======
+ 				smp_call_function_single_async(remsd->cpu,
+ 							   &remsd->csd);
++>>>>>>> c46fff2a3b29 (smp: Rename __smp_call_function_single() to smp_call_function_single_async())
  			remsd = next;
  		}
  	} else
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-softirq.c
* Unmerged path drivers/cpuidle/coupled.c
* Unmerged path include/linux/smp.h
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/smp.c
* Unmerged path kernel/up.c
* Unmerged path net/core/dev.c
