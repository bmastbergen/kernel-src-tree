ext4: rename uninitialized extents to unwritten

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Lukas Czerner <lczerner@redhat.com>
commit 556615dcbf38b0a92a9e659f52c06686270dfc16
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/556615dc.failed

Currently in ext4 there is quite a mess when it comes to naming
unwritten extents. Sometimes we call it uninitialized and sometimes we
refer to it as unwritten.

The right name for the extent which has been allocated but does not
contain any written data is _unwritten_. Other file systems are
using this name consistently, even the buffer head state refers to it as
unwritten. We need to fix this confusion in ext4.

This commit changes every reference to an uninitialized extent (meaning
allocated but unwritten) to unwritten extent. This includes comments,
function names and variable names. It even covers abbreviation of the
word uninitialized (such as uninit) and some misspellings.

This commit does not change any of the code paths at all. This has been
confirmed by comparing md5sums of the assembly code of each object file
after all the function names were stripped from it.

	Signed-off-by: Lukas Czerner <lczerner@redhat.com>
	Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
(cherry picked from commit 556615dcbf38b0a92a9e659f52c06686270dfc16)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/ext4.h
#	fs/ext4/extents.c
#	fs/ext4/inode.c
diff --cc fs/ext4/ext4.h
index 55b17b213a87,86c2cda208ea..000000000000
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@@ -197,11 -181,10 +197,16 @@@ struct mpage_da_data 
   * Flags for ext4_io_end->flags
   */
  #define	EXT4_IO_END_UNWRITTEN	0x0001
 +#define EXT4_IO_END_ERROR	0x0002
 +#define EXT4_IO_END_DIRECT	0x0004
  
  /*
++<<<<<<< HEAD
 + * For converting uninitialized extents on a work queue.
++=======
+  * For converting unwritten extents on a work queue. 'handle' is used for
+  * buffered writeback.
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
   */
  typedef struct ext4_io_end {
  	struct list_head	list;		/* per-file finished IO list */
diff --cc fs/ext4/extents.c
index 9fa9f8170933,e305a31641f2..000000000000
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@@ -3313,7 -3313,12 +3313,16 @@@ static int ext4_split_extent(handle_t *
  		return PTR_ERR(path);
  	depth = ext_depth(inode);
  	ex = path[depth].p_ext;
++<<<<<<< HEAD
 +	uninitialized = ext4_ext_is_uninitialized(ex);
++=======
+ 	if (!ex) {
+ 		EXT4_ERROR_INODE(inode, "unexpected hole at %lu",
+ 				 (unsigned long) map->m_lblk);
+ 		return -EIO;
+ 	}
+ 	unwritten = ext4_ext_is_unwritten(ex);
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  	split_flag1 = 0;
  
  	if (map->m_lblk >= ee_block) {
@@@ -3607,17 -3612,19 +3616,17 @@@ out
   *   b> Splits in two extents: Write is happening at either end of the extent
   *   c> Splits in three extents: Somone is writing in middle of the extent
   *
 - * This works the same way in the case of initialized -> unwritten conversion.
 - *
   * One of more index blocks maybe needed if the extent tree grow after
-  * the uninitialized extent split. To prevent ENOSPC occur at the IO
-  * complete, we need to split the uninitialized extent before DIO submit
-  * the IO. The uninitialized extent called at this time will be split
-  * into three uninitialized extent(at most). After IO complete, the part
+  * the unwritten extent split. To prevent ENOSPC occur at the IO
+  * complete, we need to split the unwritten extent before DIO submit
+  * the IO. The unwritten extent called at this time will be split
+  * into three unwritten extent(at most). After IO complete, the part
   * being filled will be convert to initialized by the end_io callback function
   * via ext4_convert_unwritten_extents().
   *
-  * Returns the size of uninitialized extent to be written on success.
+  * Returns the size of unwritten extent to be written on success.
   */
 -static int ext4_split_convert_extents(handle_t *handle,
 +static int ext4_split_unwritten_extents(handle_t *handle,
  					struct inode *inode,
  					struct ext4_map_blocks *map,
  					struct ext4_ext_path *path,
@@@ -3646,14 -3653,79 +3655,89 @@@
  	ee_block = le32_to_cpu(ex->ee_block);
  	ee_len = ext4_ext_get_actual_len(ex);
  
++<<<<<<< HEAD
 +	split_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;
 +	split_flag |= EXT4_EXT_MARK_UNINIT2;
 +	if (flags & EXT4_GET_BLOCKS_CONVERT)
 +		split_flag |= EXT4_EXT_DATA_VALID2;
++=======
+ 	/* Convert to unwritten */
+ 	if (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) {
+ 		split_flag |= EXT4_EXT_DATA_VALID1;
+ 	/* Convert to initialized */
+ 	} else if (flags & EXT4_GET_BLOCKS_CONVERT) {
+ 		split_flag |= ee_block + ee_len <= eof_block ?
+ 			      EXT4_EXT_MAY_ZEROOUT : 0;
+ 		split_flag |= (EXT4_EXT_MARK_UNWRIT2 | EXT4_EXT_DATA_VALID2);
+ 	}
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  	flags |= EXT4_GET_BLOCKS_PRE_IO;
  	return ext4_split_extent(handle, inode, path, map, split_flag, flags);
  }
  
++<<<<<<< HEAD
++=======
+ static int ext4_convert_initialized_extents(handle_t *handle,
+ 					    struct inode *inode,
+ 					    struct ext4_map_blocks *map,
+ 					    struct ext4_ext_path *path)
+ {
+ 	struct ext4_extent *ex;
+ 	ext4_lblk_t ee_block;
+ 	unsigned int ee_len;
+ 	int depth;
+ 	int err = 0;
+ 
+ 	depth = ext_depth(inode);
+ 	ex = path[depth].p_ext;
+ 	ee_block = le32_to_cpu(ex->ee_block);
+ 	ee_len = ext4_ext_get_actual_len(ex);
+ 
+ 	ext_debug("%s: inode %lu, logical"
+ 		"block %llu, max_blocks %u\n", __func__, inode->i_ino,
+ 		  (unsigned long long)ee_block, ee_len);
+ 
+ 	if (ee_block != map->m_lblk || ee_len > map->m_len) {
+ 		err = ext4_split_convert_extents(handle, inode, map, path,
+ 				EXT4_GET_BLOCKS_CONVERT_UNWRITTEN);
+ 		if (err < 0)
+ 			goto out;
+ 		ext4_ext_drop_refs(path);
+ 		path = ext4_ext_find_extent(inode, map->m_lblk, path, 0);
+ 		if (IS_ERR(path)) {
+ 			err = PTR_ERR(path);
+ 			goto out;
+ 		}
+ 		depth = ext_depth(inode);
+ 		ex = path[depth].p_ext;
+ 		if (!ex) {
+ 			EXT4_ERROR_INODE(inode, "unexpected hole at %lu",
+ 					 (unsigned long) map->m_lblk);
+ 			err = -EIO;
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	err = ext4_ext_get_access(handle, inode, path + depth);
+ 	if (err)
+ 		goto out;
+ 	/* first mark the extent as unwritten */
+ 	ext4_ext_mark_unwritten(ex);
+ 
+ 	/* note: ext4_ext_correct_indexes() isn't needed here because
+ 	 * borders are not changed
+ 	 */
+ 	ext4_ext_try_to_merge(handle, inode, path, ex);
+ 
+ 	/* Mark modified extent as dirty */
+ 	err = ext4_ext_dirty(handle, inode, path + path->p_depth);
+ out:
+ 	ext4_ext_show_leaf(inode, path);
+ 	return err;
+ }
+ 
+ 
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  static int ext4_convert_unwritten_extents_endio(handle_t *handle,
  						struct inode *inode,
  						struct ext4_map_blocks *map,
@@@ -3889,7 -3961,39 +3973,43 @@@ get_reserved_cluster_alloc(struct inod
  }
  
  static int
++<<<<<<< HEAD
 +ext4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,
++=======
+ ext4_ext_convert_initialized_extent(handle_t *handle, struct inode *inode,
+ 			struct ext4_map_blocks *map,
+ 			struct ext4_ext_path *path, int flags,
+ 			unsigned int allocated, ext4_fsblk_t newblock)
+ {
+ 	int ret = 0;
+ 	int err = 0;
+ 
+ 	/*
+ 	 * Make sure that the extent is no bigger than we support with
+ 	 * unwritten extent
+ 	 */
+ 	if (map->m_len > EXT_UNWRITTEN_MAX_LEN)
+ 		map->m_len = EXT_UNWRITTEN_MAX_LEN / 2;
+ 
+ 	ret = ext4_convert_initialized_extents(handle, inode, map,
+ 						path);
+ 	if (ret >= 0) {
+ 		ext4_update_inode_fsync_trans(handle, inode, 1);
+ 		err = check_eofblocks_fl(handle, inode, map->m_lblk,
+ 					 path, map->m_len);
+ 	} else
+ 		err = ret;
+ 	map->m_flags |= EXT4_MAP_UNWRITTEN;
+ 	if (allocated > map->m_len)
+ 		allocated = map->m_len;
+ 	map->m_len = allocated;
+ 
+ 	return err ? err : allocated;
+ }
+ 
+ static int
+ ext4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode,
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  			struct ext4_map_blocks *map,
  			struct ext4_ext_path *path, int flags,
  			unsigned int allocated, ext4_fsblk_t newblock)
@@@ -4204,8 -4306,9 +4324,8 @@@ int ext4_ext_map_blocks(handle_t *handl
  		ext4_fsblk_t ee_start = ext4_ext_pblock(ex);
  		unsigned short ee_len;
  
 -
  		/*
- 		 * Uninitialized extents are treated as holes, except that
+ 		 * unwritten extents are treated as holes, except that
  		 * we split out initialized portions during a write.
  		 */
  		ee_len = ext4_ext_get_actual_len(ex);
@@@ -4220,10 -4323,20 +4340,24 @@@
  			ext_debug("%u fit into %u:%d -> %llu\n", map->m_lblk,
  				  ee_block, ee_len, newblock);
  
++<<<<<<< HEAD
 +			if (!ext4_ext_is_uninitialized(ex))
++=======
+ 			/*
+ 			 * If the extent is initialized check whether the
+ 			 * caller wants to convert it to unwritten.
+ 			 */
+ 			if ((!ext4_ext_is_unwritten(ex)) &&
+ 			    (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) {
+ 				allocated = ext4_ext_convert_initialized_extent(
+ 						handle, inode, map, path, flags,
+ 						allocated, newblock);
+ 				goto out2;
+ 			} else if (!ext4_ext_is_unwritten(ex))
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  				goto out;
  
- 			ret = ext4_ext_handle_uninitialized_extents(
+ 			ret = ext4_ext_handle_unwritten_extents(
  				handle, inode, map, path, flags,
  				allocated, newblock);
  			if (ret < 0)
@@@ -4624,13 -4679,18 +4758,17 @@@ long ext4_fallocate(struct file *file, 
  	 * that it doesn't get unnecessarily split into multiple
  	 * extents.
  	 */
++<<<<<<< HEAD
 +	if (len <= EXT_UNINIT_MAX_LEN << blkbits)
++=======
+ 	if (len <= EXT_UNWRITTEN_MAX_LEN)
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  		flags |= EXT4_GET_BLOCKS_NO_NORMALIZE;
  
 -	/*
 -	 * credits to insert 1 extent into extent tree
 -	 */
 -	credits = ext4_chunk_trans_blocks(inode, len);
 -
  retry:
 -	while (ret >= 0 && ret < len) {
 +	while (ret >= 0 && ret < max_blocks) {
  		map.m_lblk = map.m_lblk + ret;
 -		map.m_len = len = len - ret;
 +		map.m_len = max_blocks = max_blocks - ret;
  		handle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,
  					    credits);
  		if (IS_ERR(handle)) {
@@@ -4657,6 -4717,221 +4795,224 @@@
  		goto retry;
  	}
  
++<<<<<<< HEAD
++=======
+ 	return ret > 0 ? ret2 : ret;
+ }
+ 
+ static long ext4_zero_range(struct file *file, loff_t offset,
+ 			    loff_t len, int mode)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle = NULL;
+ 	unsigned int max_blocks;
+ 	loff_t new_size = 0;
+ 	int ret = 0;
+ 	int flags;
+ 	int partial;
+ 	loff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	struct address_space *mapping = inode->i_mapping;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	trace_ext4_zero_range(inode, offset, len, mode);
+ 
+ 	if (!S_ISREG(inode->i_mode))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Write out all dirty pages to avoid race conditions
+ 	 * Then release them.
+ 	 */
+ 	if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
+ 		ret = filemap_write_and_wait_range(mapping, offset,
+ 						   offset + len - 1);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/*
+ 	 * Round up offset. This is not fallocate, we neet to zero out
+ 	 * blocks, so convert interior block aligned part of the range to
+ 	 * unwritten and possibly manually zero out unaligned parts of the
+ 	 * range.
+ 	 */
+ 	start = round_up(offset, 1 << blkbits);
+ 	end = round_down((offset + len), 1 << blkbits);
+ 
+ 	if (start < offset || end > offset + len)
+ 		return -EINVAL;
+ 	partial = (offset + len) & ((1 << blkbits) - 1);
+ 
+ 	lblk = start >> blkbits;
+ 	max_blocks = (end >> blkbits);
+ 	if (max_blocks < lblk)
+ 		max_blocks = 0;
+ 	else
+ 		max_blocks -= lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT |
+ 		EXT4_GET_BLOCKS_CONVERT_UNWRITTEN;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	/*
+ 	 * Indirect files do not support unwritten extnets
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {
+ 		ret = -EOPNOTSUPP;
+ 		goto out_mutex;
+ 	}
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out_mutex;
+ 		/*
+ 		 * If we have a partial block after EOF we have to allocate
+ 		 * the entire block.
+ 		 */
+ 		if (partial)
+ 			max_blocks += 1;
+ 	}
+ 
+ 	if (max_blocks > 0) {
+ 
+ 		/* Now release the pages and zero block aligned part of pages*/
+ 		truncate_pagecache_range(inode, start, end - 1);
+ 
+ 		/* Wait all existing dio workers, newcomers will block on i_mutex */
+ 		ext4_inode_block_unlocked_dio(inode);
+ 		inode_dio_wait(inode);
+ 
+ 		/*
+ 		 * Remove entire range from the extent status tree.
+ 		 */
+ 		ret = ext4_es_remove_extent(inode, lblk, max_blocks);
+ 		if (ret)
+ 			goto out_dio;
+ 
+ 		ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags,
+ 					     mode);
+ 		if (ret)
+ 			goto out_dio;
+ 	}
+ 
+ 	handle = ext4_journal_start(inode, EXT4_HT_MISC, 4);
+ 	if (IS_ERR(handle)) {
+ 		ret = PTR_ERR(handle);
+ 		ext4_std_error(inode->i_sb, ret);
+ 		goto out_dio;
+ 	}
+ 
+ 	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
+ 
+ 	if (new_size) {
+ 		if (new_size > i_size_read(inode))
+ 			i_size_write(inode, new_size);
+ 		if (new_size > EXT4_I(inode)->i_disksize)
+ 			ext4_update_i_disksize(inode, new_size);
+ 	} else {
+ 		/*
+ 		* Mark that we allocate beyond EOF so the subsequent truncate
+ 		* can proceed even if the new size is the same as i_size.
+ 		*/
+ 		if ((offset + len) > i_size_read(inode))
+ 			ext4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);
+ 	}
+ 
+ 	ext4_mark_inode_dirty(handle, inode);
+ 
+ 	/* Zero out partial block at the edges of the range */
+ 	ret = ext4_zero_partial_blocks(handle, inode, offset, len);
+ 
+ 	if (file->f_flags & O_SYNC)
+ 		ext4_handle_sync(handle);
+ 
+ 	ext4_journal_stop(handle);
+ out_dio:
+ 	ext4_inode_resume_unlocked_dio(inode);
+ out_mutex:
+ 	mutex_unlock(&inode->i_mutex);
+ 	return ret;
+ }
+ 
+ /*
+  * preallocate space for a file. This implements ext4's fallocate file
+  * operation, which gets called from sys_fallocate system call.
+  * For block-mapped files, posix_fallocate should fall back to the method
+  * of writing zeroes to the required new blocks (the same behavior which is
+  * expected for file systems which do not support fallocate() system call).
+  */
+ long ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)
+ {
+ 	struct inode *inode = file_inode(file);
+ 	handle_t *handle;
+ 	loff_t new_size = 0;
+ 	unsigned int max_blocks;
+ 	int ret = 0;
+ 	int flags;
+ 	ext4_lblk_t lblk;
+ 	struct timespec tv;
+ 	unsigned int blkbits = inode->i_blkbits;
+ 
+ 	/* Return error if mode is not supported */
+ 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+ 		     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_PUNCH_HOLE)
+ 		return ext4_punch_hole(inode, offset, len);
+ 
+ 	ret = ext4_convert_inline_data(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * currently supporting (pre)allocate mode for extent-based
+ 	 * files _only_
+ 	 */
+ 	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (mode & FALLOC_FL_COLLAPSE_RANGE)
+ 		return ext4_collapse_range(inode, offset, len);
+ 
+ 	if (mode & FALLOC_FL_ZERO_RANGE)
+ 		return ext4_zero_range(file, offset, len, mode);
+ 
+ 	trace_ext4_fallocate_enter(inode, offset, len, mode);
+ 	lblk = offset >> blkbits;
+ 	/*
+ 	 * We can't just convert len to max_blocks because
+ 	 * If blocksize = 4096 offset = 3072 and len = 2048
+ 	 */
+ 	max_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)
+ 		- lblk;
+ 
+ 	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;
+ 	if (mode & FALLOC_FL_KEEP_SIZE)
+ 		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
+ 	     offset + len > i_size_read(inode)) {
+ 		new_size = offset + len;
+ 		ret = inode_newsize_ok(inode, new_size);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	ret = ext4_alloc_file_blocks(file, lblk, max_blocks, flags, mode);
+ 	if (ret)
+ 		goto out;
+ 
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
  	handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
  	if (IS_ERR(handle))
  		goto out;
diff --cc fs/ext4/inode.c
index 51d61142834b,7dfbcbba67d5..000000000000
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -2202,9 -1929,356 +2202,359 @@@ static int ext4_writepage(struct page *
  #define MAX_WRITEPAGES_EXTENT_LEN 2048
  
  /*
++<<<<<<< HEAD
++=======
+  * mpage_add_bh_to_extent - try to add bh to extent of blocks to map
+  *
+  * @mpd - extent of blocks
+  * @lblk - logical number of the block in the file
+  * @bh - buffer head we want to add to the extent
+  *
+  * The function is used to collect contig. blocks in the same state. If the
+  * buffer doesn't require mapping for writeback and we haven't started the
+  * extent of buffers to map yet, the function returns 'true' immediately - the
+  * caller can write the buffer right away. Otherwise the function returns true
+  * if the block has been added to the extent, false if the block couldn't be
+  * added.
+  */
+ static bool mpage_add_bh_to_extent(struct mpage_da_data *mpd, ext4_lblk_t lblk,
+ 				   struct buffer_head *bh)
+ {
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 
+ 	/* Buffer that doesn't need mapping for writeback? */
+ 	if (!buffer_dirty(bh) || !buffer_mapped(bh) ||
+ 	    (!buffer_delay(bh) && !buffer_unwritten(bh))) {
+ 		/* So far no extent to map => we write the buffer right away */
+ 		if (map->m_len == 0)
+ 			return true;
+ 		return false;
+ 	}
+ 
+ 	/* First block in the extent? */
+ 	if (map->m_len == 0) {
+ 		map->m_lblk = lblk;
+ 		map->m_len = 1;
+ 		map->m_flags = bh->b_state & BH_FLAGS;
+ 		return true;
+ 	}
+ 
+ 	/* Don't go larger than mballoc is willing to allocate */
+ 	if (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN)
+ 		return false;
+ 
+ 	/* Can we merge the block to our big extent? */
+ 	if (lblk == map->m_lblk + map->m_len &&
+ 	    (bh->b_state & BH_FLAGS) == map->m_flags) {
+ 		map->m_len++;
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
+ /*
+  * mpage_process_page_bufs - submit page buffers for IO or add them to extent
+  *
+  * @mpd - extent of blocks for mapping
+  * @head - the first buffer in the page
+  * @bh - buffer we should start processing from
+  * @lblk - logical number of the block in the file corresponding to @bh
+  *
+  * Walk through page buffers from @bh upto @head (exclusive) and either submit
+  * the page for IO if all buffers in this page were mapped and there's no
+  * accumulated extent of buffers to map or add buffers in the page to the
+  * extent of buffers to map. The function returns 1 if the caller can continue
+  * by processing the next page, 0 if it should stop adding buffers to the
+  * extent to map because we cannot extend it anymore. It can also return value
+  * < 0 in case of error during IO submission.
+  */
+ static int mpage_process_page_bufs(struct mpage_da_data *mpd,
+ 				   struct buffer_head *head,
+ 				   struct buffer_head *bh,
+ 				   ext4_lblk_t lblk)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	int err;
+ 	ext4_lblk_t blocks = (i_size_read(inode) + (1 << inode->i_blkbits) - 1)
+ 							>> inode->i_blkbits;
+ 
+ 	do {
+ 		BUG_ON(buffer_locked(bh));
+ 
+ 		if (lblk >= blocks || !mpage_add_bh_to_extent(mpd, lblk, bh)) {
+ 			/* Found extent to map? */
+ 			if (mpd->map.m_len)
+ 				return 0;
+ 			/* Everything mapped so far and we hit EOF */
+ 			break;
+ 		}
+ 	} while (lblk++, (bh = bh->b_this_page) != head);
+ 	/* So far everything mapped? Submit the page for IO. */
+ 	if (mpd->map.m_len == 0) {
+ 		err = mpage_submit_page(mpd, head->b_page);
+ 		if (err < 0)
+ 			return err;
+ 	}
+ 	return lblk < blocks;
+ }
+ 
+ /*
+  * mpage_map_buffers - update buffers corresponding to changed extent and
+  *		       submit fully mapped pages for IO
+  *
+  * @mpd - description of extent to map, on return next extent to map
+  *
+  * Scan buffers corresponding to changed extent (we expect corresponding pages
+  * to be already locked) and update buffer state according to new extent state.
+  * We map delalloc buffers to their physical location, clear unwritten bits,
+  * and mark buffers as uninit when we perform writes to unwritten extents
+  * and do extent conversion after IO is finished. If the last page is not fully
+  * mapped, we update @map to the next extent in the last page that needs
+  * mapping. Otherwise we submit the page for IO.
+  */
+ static int mpage_map_and_submit_buffers(struct mpage_da_data *mpd)
+ {
+ 	struct pagevec pvec;
+ 	int nr_pages, i;
+ 	struct inode *inode = mpd->inode;
+ 	struct buffer_head *head, *bh;
+ 	int bpp_bits = PAGE_CACHE_SHIFT - inode->i_blkbits;
+ 	pgoff_t start, end;
+ 	ext4_lblk_t lblk;
+ 	sector_t pblock;
+ 	int err;
+ 
+ 	start = mpd->map.m_lblk >> bpp_bits;
+ 	end = (mpd->map.m_lblk + mpd->map.m_len - 1) >> bpp_bits;
+ 	lblk = start << bpp_bits;
+ 	pblock = mpd->map.m_pblk;
+ 
+ 	pagevec_init(&pvec, 0);
+ 	while (start <= end) {
+ 		nr_pages = pagevec_lookup(&pvec, inode->i_mapping, start,
+ 					  PAGEVEC_SIZE);
+ 		if (nr_pages == 0)
+ 			break;
+ 		for (i = 0; i < nr_pages; i++) {
+ 			struct page *page = pvec.pages[i];
+ 
+ 			if (page->index > end)
+ 				break;
+ 			/* Up to 'end' pages must be contiguous */
+ 			BUG_ON(page->index != start);
+ 			bh = head = page_buffers(page);
+ 			do {
+ 				if (lblk < mpd->map.m_lblk)
+ 					continue;
+ 				if (lblk >= mpd->map.m_lblk + mpd->map.m_len) {
+ 					/*
+ 					 * Buffer after end of mapped extent.
+ 					 * Find next buffer in the page to map.
+ 					 */
+ 					mpd->map.m_len = 0;
+ 					mpd->map.m_flags = 0;
+ 					/*
+ 					 * FIXME: If dioread_nolock supports
+ 					 * blocksize < pagesize, we need to make
+ 					 * sure we add size mapped so far to
+ 					 * io_end->size as the following call
+ 					 * can submit the page for IO.
+ 					 */
+ 					err = mpage_process_page_bufs(mpd, head,
+ 								      bh, lblk);
+ 					pagevec_release(&pvec);
+ 					if (err > 0)
+ 						err = 0;
+ 					return err;
+ 				}
+ 				if (buffer_delay(bh)) {
+ 					clear_buffer_delay(bh);
+ 					bh->b_blocknr = pblock++;
+ 				}
+ 				clear_buffer_unwritten(bh);
+ 			} while (lblk++, (bh = bh->b_this_page) != head);
+ 
+ 			/*
+ 			 * FIXME: This is going to break if dioread_nolock
+ 			 * supports blocksize < pagesize as we will try to
+ 			 * convert potentially unmapped parts of inode.
+ 			 */
+ 			mpd->io_submit.io_end->size += PAGE_CACHE_SIZE;
+ 			/* Page fully mapped - let IO run! */
+ 			err = mpage_submit_page(mpd, page);
+ 			if (err < 0) {
+ 				pagevec_release(&pvec);
+ 				return err;
+ 			}
+ 			start++;
+ 		}
+ 		pagevec_release(&pvec);
+ 	}
+ 	/* Extent fully mapped and matches with page boundary. We are done. */
+ 	mpd->map.m_len = 0;
+ 	mpd->map.m_flags = 0;
+ 	return 0;
+ }
+ 
+ static int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int get_blocks_flags;
+ 	int err, dioread_nolock;
+ 
+ 	trace_ext4_da_write_pages_extent(inode, map);
+ 	/*
+ 	 * Call ext4_map_blocks() to allocate any delayed allocation blocks, or
+ 	 * to convert an unwritten extent to be initialized (in the case
+ 	 * where we have written into one or more preallocated blocks).  It is
+ 	 * possible that we're going to need more metadata blocks than
+ 	 * previously reserved. However we must not fail because we're in
+ 	 * writeback and there is nothing we can do about it so it might result
+ 	 * in data loss.  So use reserved blocks to allocate metadata if
+ 	 * possible.
+ 	 *
+ 	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if the blocks
+ 	 * in question are delalloc blocks.  This affects functions in many
+ 	 * different parts of the allocation call path.  This flag exists
+ 	 * primarily because we don't want to change *many* call functions, so
+ 	 * ext4_map_blocks() will set the EXT4_STATE_DELALLOC_RESERVED flag
+ 	 * once the inode's allocation semaphore is taken.
+ 	 */
+ 	get_blocks_flags = EXT4_GET_BLOCKS_CREATE |
+ 			   EXT4_GET_BLOCKS_METADATA_NOFAIL;
+ 	dioread_nolock = ext4_should_dioread_nolock(inode);
+ 	if (dioread_nolock)
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;
+ 	if (map->m_flags & (1 << BH_Delay))
+ 		get_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;
+ 
+ 	err = ext4_map_blocks(handle, inode, map, get_blocks_flags);
+ 	if (err < 0)
+ 		return err;
+ 	if (dioread_nolock && (map->m_flags & EXT4_MAP_UNWRITTEN)) {
+ 		if (!mpd->io_submit.io_end->handle &&
+ 		    ext4_handle_valid(handle)) {
+ 			mpd->io_submit.io_end->handle = handle->h_rsv_handle;
+ 			handle->h_rsv_handle = NULL;
+ 		}
+ 		ext4_set_io_unwritten_flag(inode, mpd->io_submit.io_end);
+ 	}
+ 
+ 	BUG_ON(map->m_len == 0);
+ 	if (map->m_flags & EXT4_MAP_NEW) {
+ 		struct block_device *bdev = inode->i_sb->s_bdev;
+ 		int i;
+ 
+ 		for (i = 0; i < map->m_len; i++)
+ 			unmap_underlying_metadata(bdev, map->m_pblk + i);
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * mpage_map_and_submit_extent - map extent starting at mpd->lblk of length
+  *				 mpd->len and submit pages underlying it for IO
+  *
+  * @handle - handle for journal operations
+  * @mpd - extent to map
+  * @give_up_on_write - we set this to true iff there is a fatal error and there
+  *                     is no hope of writing the data. The caller should discard
+  *                     dirty pages to avoid infinite loops.
+  *
+  * The function maps extent starting at mpd->lblk of length mpd->len. If it is
+  * delayed, blocks are allocated, if it is unwritten, we may need to convert
+  * them to initialized or split the described range from larger unwritten
+  * extent. Note that we need not map all the described range since allocation
+  * can return less blocks or the range is covered by more unwritten extents. We
+  * cannot map more because we are limited by reserved transaction credits. On
+  * the other hand we always make sure that the last touched page is fully
+  * mapped so that it can be written out (and thus forward progress is
+  * guaranteed). After mapping we submit all mapped pages for IO.
+  */
+ static int mpage_map_and_submit_extent(handle_t *handle,
+ 				       struct mpage_da_data *mpd,
+ 				       bool *give_up_on_write)
+ {
+ 	struct inode *inode = mpd->inode;
+ 	struct ext4_map_blocks *map = &mpd->map;
+ 	int err;
+ 	loff_t disksize;
+ 
+ 	mpd->io_submit.io_end->offset =
+ 				((loff_t)map->m_lblk) << inode->i_blkbits;
+ 	do {
+ 		err = mpage_map_one_extent(handle, mpd);
+ 		if (err < 0) {
+ 			struct super_block *sb = inode->i_sb;
+ 
+ 			if (EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)
+ 				goto invalidate_dirty_pages;
+ 			/*
+ 			 * Let the uper layers retry transient errors.
+ 			 * In the case of ENOSPC, if ext4_count_free_blocks()
+ 			 * is non-zero, a commit should free up blocks.
+ 			 */
+ 			if ((err == -ENOMEM) ||
+ 			    (err == -ENOSPC && ext4_count_free_clusters(sb)))
+ 				return err;
+ 			ext4_msg(sb, KERN_CRIT,
+ 				 "Delayed block allocation failed for "
+ 				 "inode %lu at logical offset %llu with"
+ 				 " max blocks %u with error %d",
+ 				 inode->i_ino,
+ 				 (unsigned long long)map->m_lblk,
+ 				 (unsigned)map->m_len, -err);
+ 			ext4_msg(sb, KERN_CRIT,
+ 				 "This should not happen!! Data will "
+ 				 "be lost\n");
+ 			if (err == -ENOSPC)
+ 				ext4_print_free_blocks(inode);
+ 		invalidate_dirty_pages:
+ 			*give_up_on_write = true;
+ 			return err;
+ 		}
+ 		/*
+ 		 * Update buffer state, submit mapped pages, and get us new
+ 		 * extent to map
+ 		 */
+ 		err = mpage_map_and_submit_buffers(mpd);
+ 		if (err < 0)
+ 			return err;
+ 	} while (map->m_len);
+ 
+ 	/*
+ 	 * Update on-disk size after IO is submitted.  Races with
+ 	 * truncate are avoided by checking i_size under i_data_sem.
+ 	 */
+ 	disksize = ((loff_t)mpd->first_page) << PAGE_CACHE_SHIFT;
+ 	if (disksize > EXT4_I(inode)->i_disksize) {
+ 		int err2;
+ 		loff_t i_size;
+ 
+ 		down_write(&EXT4_I(inode)->i_data_sem);
+ 		i_size = i_size_read(inode);
+ 		if (disksize > i_size)
+ 			disksize = i_size;
+ 		if (disksize > EXT4_I(inode)->i_disksize)
+ 			EXT4_I(inode)->i_disksize = disksize;
+ 		err2 = ext4_mark_inode_dirty(handle, inode);
+ 		up_write(&EXT4_I(inode)->i_data_sem);
+ 		if (err2)
+ 			ext4_error(inode->i_sb,
+ 				   "Failed to mark inode %lu dirty",
+ 				   inode->i_ino);
+ 		if (!err)
+ 			err = err2;
+ 	}
+ 	return err;
+ }
+ 
+ /*
++>>>>>>> 556615dcbf38 (ext4: rename uninitialized extents to unwritten)
   * Calculate the total number of credits to reserve for one writepages
 - * iteration. This is called from ext4_writepages(). We map an extent of
 - * up to MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping
 + * iteration. This is called from ext4_da_writepages(). We map an extent of
 + * upto MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping
   * the last partial page. So in total we can map MAX_WRITEPAGES_EXTENT_LEN +
   * bpp - 1 blocks in bpp different extents.
   */
* Unmerged path fs/ext4/ext4.h
diff --git a/fs/ext4/ext4_extents.h b/fs/ext4/ext4_extents.h
index 5074fe23f19e..a867f5ca9991 100644
--- a/fs/ext4/ext4_extents.h
+++ b/fs/ext4/ext4_extents.h
@@ -137,21 +137,21 @@ struct ext4_ext_path {
  * EXT_INIT_MAX_LEN is the maximum number of blocks we can have in an
  * initialized extent. This is 2^15 and not (2^16 - 1), since we use the
  * MSB of ee_len field in the extent datastructure to signify if this
- * particular extent is an initialized extent or an uninitialized (i.e.
+ * particular extent is an initialized extent or an unwritten (i.e.
  * preallocated).
- * EXT_UNINIT_MAX_LEN is the maximum number of blocks we can have in an
- * uninitialized extent.
+ * EXT_UNWRITTEN_MAX_LEN is the maximum number of blocks we can have in an
+ * unwritten extent.
  * If ee_len is <= 0x8000, it is an initialized extent. Otherwise, it is an
- * uninitialized one. In other words, if MSB of ee_len is set, it is an
- * uninitialized extent with only one special scenario when ee_len = 0x8000.
- * In this case we can not have an uninitialized extent of zero length and
+ * unwritten one. In other words, if MSB of ee_len is set, it is an
+ * unwritten extent with only one special scenario when ee_len = 0x8000.
+ * In this case we can not have an unwritten extent of zero length and
  * thus we make it as a special case of initialized extent with 0x8000 length.
  * This way we get better extent-to-group alignment for initialized extents.
  * Hence, the maximum number of blocks we can have in an *initialized*
- * extent is 2^15 (32768) and in an *uninitialized* extent is 2^15-1 (32767).
+ * extent is 2^15 (32768) and in an *unwritten* extent is 2^15-1 (32767).
  */
 #define EXT_INIT_MAX_LEN	(1UL << 15)
-#define EXT_UNINIT_MAX_LEN	(EXT_INIT_MAX_LEN - 1)
+#define EXT_UNWRITTEN_MAX_LEN	(EXT_INIT_MAX_LEN - 1)
 
 
 #define EXT_FIRST_EXTENT(__hdr__) \
@@ -187,14 +187,14 @@ static inline unsigned short ext_depth(struct inode *inode)
 	return le16_to_cpu(ext_inode_hdr(inode)->eh_depth);
 }
 
-static inline void ext4_ext_mark_uninitialized(struct ext4_extent *ext)
+static inline void ext4_ext_mark_unwritten(struct ext4_extent *ext)
 {
-	/* We can not have an uninitialized extent of zero length! */
+	/* We can not have an unwritten extent of zero length! */
 	BUG_ON((le16_to_cpu(ext->ee_len) & ~EXT_INIT_MAX_LEN) == 0);
 	ext->ee_len |= cpu_to_le16(EXT_INIT_MAX_LEN);
 }
 
-static inline int ext4_ext_is_uninitialized(struct ext4_extent *ext)
+static inline int ext4_ext_is_unwritten(struct ext4_extent *ext)
 {
 	/* Extent with ee_len of 0x8000 is treated as an initialized extent */
 	return (le16_to_cpu(ext->ee_len) > EXT_INIT_MAX_LEN);
* Unmerged path fs/ext4/extents.c
diff --git a/fs/ext4/extents_status.c b/fs/ext4/extents_status.c
index 4567602db66f..f0621997428a 100644
--- a/fs/ext4/extents_status.c
+++ b/fs/ext4/extents_status.c
@@ -433,7 +433,7 @@ static void ext4_es_insert_extent_ext_check(struct inode *inode,
 		ee_start = ext4_ext_pblock(ex);
 		ee_len = ext4_ext_get_actual_len(ex);
 
-		ee_status = ext4_ext_is_uninitialized(ex) ? 1 : 0;
+		ee_status = ext4_ext_is_unwritten(ex) ? 1 : 0;
 		es_status = ext4_es_is_unwritten(es) ? 1 : 0;
 
 		/*
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 4635788e14bf..9709dd1ab3c1 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -136,7 +136,7 @@ ext4_file_dio_write(struct kiocb *iocb, const struct iovec *iov,
 		/*
 		 * 'err==len' means that all of blocks has been preallocated no
 		 * matter they are initialized or not.  For excluding
-		 * uninitialized extents, we need to check m_flags.  There are
+		 * unwritten extents, we need to check m_flags.  There are
 		 * two conditions that indicate for initialized extents.
 		 * 1) If we hit extent cache, EXT4_MAP_MAPPED flag is returned;
 		 * 2) If we do a real lookup, non-flags are returned.
* Unmerged path fs/ext4/inode.c
diff --git a/fs/ext4/move_extent.c b/fs/ext4/move_extent.c
index 58ee7dc87669..1b809fe51da1 100644
--- a/fs/ext4/move_extent.c
+++ b/fs/ext4/move_extent.c
@@ -57,8 +57,8 @@ get_ext_path(struct inode *inode, ext4_lblk_t lblock,
 static void
 copy_extent_status(struct ext4_extent *src, struct ext4_extent *dest)
 {
-	if (ext4_ext_is_uninitialized(src))
-		ext4_ext_mark_uninitialized(dest);
+	if (ext4_ext_is_unwritten(src))
+		ext4_ext_mark_unwritten(dest);
 	else
 		dest->ee_len = cpu_to_le16(ext4_ext_get_actual_len(dest));
 }
@@ -593,14 +593,14 @@ mext_calc_swap_extents(struct ext4_extent *tmp_dext,
  * @inode:		inode in question
  * @from:		block offset of inode
  * @count:		block count to be checked
- * @uninit:		extents expected to be uninitialized
+ * @unwritten:		extents expected to be unwritten
  * @err:		pointer to save error value
  *
  * Return 1 if all extents in range has expected type, and zero otherwise.
  */
 static int
 mext_check_coverage(struct inode *inode, ext4_lblk_t from, ext4_lblk_t count,
-			  int uninit, int *err)
+		    int unwritten, int *err)
 {
 	struct ext4_ext_path *path = NULL;
 	struct ext4_extent *ext;
@@ -611,7 +611,7 @@ mext_check_coverage(struct inode *inode, ext4_lblk_t from, ext4_lblk_t count,
 		if (*err)
 			goto out;
 		ext = path[ext_depth(inode)].p_ext;
-		if (uninit != ext4_ext_is_uninitialized(ext))
+		if (unwritten != ext4_ext_is_unwritten(ext))
 			goto out;
 		from += ext4_ext_get_actual_len(ext);
 		ext4_ext_drop_refs(path);
@@ -894,7 +894,7 @@ out:
  * @orig_page_offset:		page index on original file
  * @data_offset_in_page:	block index where data swapping starts
  * @block_len_in_page:		the number of blocks to be swapped
- * @uninit:			orig extent is uninitialized or not
+ * @unwritten:			orig extent is unwritten or not
  * @err:			pointer to save return value
  *
  * Save the data in original inode blocks and replace original inode extents
@@ -905,7 +905,7 @@ out:
 static int
 move_extent_per_page(struct file *o_filp, struct inode *donor_inode,
 		  pgoff_t orig_page_offset, int data_offset_in_page,
-		  int block_len_in_page, int uninit, int *err)
+		  int block_len_in_page, int unwritten, int *err)
 {
 	struct inode *orig_inode = file_inode(o_filp);
 	struct page *pagep[2] = {NULL, NULL};
@@ -962,27 +962,27 @@ again:
 	if (unlikely(*err < 0))
 		goto stop_journal;
 	/*
-	 * If orig extent was uninitialized it can become initialized
+	 * If orig extent was unwritten it can become initialized
 	 * at any time after i_data_sem was dropped, in order to
 	 * serialize with delalloc we have recheck extent while we
 	 * hold page's lock, if it is still the case data copy is not
 	 * necessary, just swap data blocks between orig and donor.
 	 */
-	if (uninit) {
+	if (unwritten) {
 		ext4_double_down_write_data_sem(orig_inode, donor_inode);
 		/* If any of extents in range became initialized we have to
 		 * fallback to data copying */
-		uninit = mext_check_coverage(orig_inode, orig_blk_offset,
-					     block_len_in_page, 1, err);
+		unwritten = mext_check_coverage(orig_inode, orig_blk_offset,
+						block_len_in_page, 1, err);
 		if (*err)
 			goto drop_data_sem;
 
-		uninit &= mext_check_coverage(donor_inode, orig_blk_offset,
-					      block_len_in_page, 1, err);
+		unwritten &= mext_check_coverage(donor_inode, orig_blk_offset,
+						 block_len_in_page, 1, err);
 		if (*err)
 			goto drop_data_sem;
 
-		if (!uninit) {
+		if (!unwritten) {
 			ext4_double_up_write_data_sem(orig_inode, donor_inode);
 			goto data_copy;
 		}
@@ -1259,7 +1259,7 @@ ext4_move_extents(struct file *o_filp, struct file *d_filp,
 	int blocks_per_page = PAGE_CACHE_SIZE >> orig_inode->i_blkbits;
 	int data_offset_in_page;
 	int block_len_in_page;
-	int uninit;
+	int unwritten;
 
 	if (orig_inode->i_sb != donor_inode->i_sb) {
 		ext4_debug("ext4 move extent: The argument files "
@@ -1391,8 +1391,8 @@ ext4_move_extents(struct file *o_filp, struct file *d_filp,
 		    !last_extent)
 			continue;
 
-		/* Is original extent is uninitialized */
-		uninit = ext4_ext_is_uninitialized(ext_prev);
+		/* Is original extent is unwritten */
+		unwritten = ext4_ext_is_unwritten(ext_prev);
 
 		data_offset_in_page = seq_start % blocks_per_page;
 
@@ -1432,8 +1432,8 @@ ext4_move_extents(struct file *o_filp, struct file *d_filp,
 						o_filp, donor_inode,
 						orig_page_offset,
 						data_offset_in_page,
-						block_len_in_page, uninit,
-						&ret);
+						block_len_in_page,
+						unwritten, &ret);
 
 			/* Count how many blocks we have exchanged */
 			*moved_len += block_len_in_page;
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index aaf7985d43b1..52dd031e99fa 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -3336,7 +3336,7 @@ static ext4_fsblk_t ext4_calculate_resv_clusters(struct super_block *sb)
 	 * By default we reserve 2% or 4096 clusters, whichever is smaller.
 	 * This should cover the situations where we can not afford to run
 	 * out of space like for example punch hole, or converting
-	 * uninitialized extents in delalloc path. In most cases such
+	 * unwritten extents in delalloc path. In most cases such
 	 * allocation would require 1, or 2 blocks, higher numbers are
 	 * very rare.
 	 */
diff --git a/include/trace/events/ext4.h b/include/trace/events/ext4.h
index 459eed33acf1..8d31483f14c9 100644
--- a/include/trace/events/ext4.h
+++ b/include/trace/events/ext4.h
@@ -41,7 +41,7 @@ struct extent_status;
 
 #define show_map_flags(flags) __print_flags(flags, "|",			\
 	{ EXT4_GET_BLOCKS_CREATE,		"CREATE" },		\
-	{ EXT4_GET_BLOCKS_UNINIT_EXT,		"UNINIT" },		\
+	{ EXT4_GET_BLOCKS_UNWRIT_EXT,		"UNWRIT" },		\
 	{ EXT4_GET_BLOCKS_DELALLOC_RESERVE,	"DELALLOC" },		\
 	{ EXT4_GET_BLOCKS_PRE_IO,		"PRE_IO" },		\
 	{ EXT4_GET_BLOCKS_CONVERT,		"CONVERT" },		\
@@ -1486,7 +1486,7 @@ DEFINE_EVENT(ext4__truncate, ext4_truncate_exit,
 	TP_ARGS(inode)
 );
 
-/* 'ux' is the uninitialized extent. */
+/* 'ux' is the unwritten extent. */
 TRACE_EVENT(ext4_ext_convert_to_initialized_enter,
 	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map,
 		 struct ext4_extent *ux),
@@ -1522,7 +1522,7 @@ TRACE_EVENT(ext4_ext_convert_to_initialized_enter,
 );
 
 /*
- * 'ux' is the uninitialized extent.
+ * 'ux' is the unwritten extent.
  * 'ix' is the initialized extent to which blocks are transferred.
  */
 TRACE_EVENT(ext4_ext_convert_to_initialized_fastpath,
@@ -1800,7 +1800,7 @@ DEFINE_EVENT(ext4__trim, ext4_trim_all_free,
 	TP_ARGS(sb, group, start, len)
 );
 
-TRACE_EVENT(ext4_ext_handle_uninitialized_extents,
+TRACE_EVENT(ext4_ext_handle_unwritten_extents,
 	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map, int flags,
 		 unsigned int allocated, ext4_fsblk_t newblock),
 
