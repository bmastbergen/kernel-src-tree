xprtrmda: Reduce calls to ib_poll_cq() in completion handlers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [net] sunrpc/xprtrdma: Reduce calls to ib_poll_cq() in completion handlers (Steve Dickson) [1113248]
Rebuild_FUZZ: 93.02%
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 1c00dd0776543608e13c74a527660cb8cd28a74f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/1c00dd07.failed

Change the completion handlers to grab up to 16 items per
ib_poll_cq() call. No extra ib_poll_cq() is needed if fewer than 16
items are returned.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 1c00dd0776543608e13c74a527660cb8cd28a74f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
diff --cc net/sunrpc/xprtrdma/verbs.c
index 871eaa4ede37,b8caee91661c..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -142,91 -142,131 +142,188 @@@ rpcrdma_cq_async_error_upcall(struct ib
  	}
  }
  
++<<<<<<< HEAD
 +static inline
 +void rpcrdma_event_process(struct ib_wc *wc)
++=======
+ static void
+ rpcrdma_sendcq_process_wc(struct ib_wc *wc)
+ {
+ 	struct rpcrdma_mw *frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
+ 
+ 	dprintk("RPC:       %s: frmr %p status %X opcode %d\n",
+ 		__func__, frmr, wc->status, wc->opcode);
+ 
+ 	if (wc->wr_id == 0ULL)
+ 		return;
+ 	if (wc->status != IB_WC_SUCCESS)
+ 		return;
+ 
+ 	if (wc->opcode == IB_WC_FAST_REG_MR)
+ 		frmr->r.frmr.state = FRMR_IS_VALID;
+ 	else if (wc->opcode == IB_WC_LOCAL_INV)
+ 		frmr->r.frmr.state = FRMR_IS_INVALID;
+ }
+ 
+ static int
+ rpcrdma_sendcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)
+ {
+ 	struct ib_wc *wcs;
+ 	int count, rc;
+ 
+ 	do {
+ 		wcs = ep->rep_send_wcs;
+ 
+ 		rc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);
+ 		if (rc <= 0)
+ 			return rc;
+ 
+ 		count = rc;
+ 		while (count-- > 0)
+ 			rpcrdma_sendcq_process_wc(wcs++);
+ 	} while (rc == RPCRDMA_POLLSIZE);
+ 	return 0;
+ }
+ 
+ /*
+  * Handle send, fast_reg_mr, and local_inv completions.
+  *
+  * Send events are typically suppressed and thus do not result
+  * in an upcall. Occasionally one is signaled, however. This
+  * prevents the provider's completion queue from wrapping and
+  * losing a completion.
+  */
+ static void
+ rpcrdma_sendcq_upcall(struct ib_cq *cq, void *cq_context)
+ {
+ 	struct rpcrdma_ep *ep = (struct rpcrdma_ep *)cq_context;
+ 	int rc;
+ 
+ 	rc = rpcrdma_sendcq_poll(cq, ep);
+ 	if (rc) {
+ 		dprintk("RPC:       %s: ib_poll_cq failed: %i\n",
+ 			__func__, rc);
+ 		return;
+ 	}
+ 
+ 	rc = ib_req_notify_cq(cq,
+ 			IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);
+ 	if (rc == 0)
+ 		return;
+ 	if (rc < 0) {
+ 		dprintk("RPC:       %s: ib_req_notify_cq failed: %i\n",
+ 			__func__, rc);
+ 		return;
+ 	}
+ 
+ 	rpcrdma_sendcq_poll(cq, ep);
+ }
+ 
+ static void
+ rpcrdma_recvcq_process_wc(struct ib_wc *wc)
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  {
 +	struct rpcrdma_mw *frmr;
  	struct rpcrdma_rep *rep =
 -			(struct rpcrdma_rep *)(unsigned long)wc->wr_id;
 +			(struct rpcrdma_rep *)(unsigned long) wc->wr_id;
  
 -	dprintk("RPC:       %s: rep %p status %X opcode %X length %u\n",
 +	dprintk("RPC:       %s: event rep %p status %X opcode %X length %u\n",
  		__func__, rep, wc->status, wc->opcode, wc->byte_len);
  
 -	if (wc->status != IB_WC_SUCCESS) {
 +	if (!rep) /* send or bind completion that we don't care about */
 +		return;
 +
 +	if (IB_WC_SUCCESS != wc->status) {
 +		dprintk("RPC:       %s: WC opcode %d status %X, connection lost\n",
 +			__func__, wc->opcode, wc->status);
  		rep->rr_len = ~0U;
 -		goto out_schedule;
 -	}
 -	if (wc->opcode != IB_WC_RECV)
 +		if (wc->opcode != IB_WC_FAST_REG_MR && wc->opcode != IB_WC_LOCAL_INV)
 +			rpcrdma_schedule_tasklet(rep);
  		return;
 +	}
  
 -	rep->rr_len = wc->byte_len;
 -	ib_dma_sync_single_for_cpu(rdmab_to_ia(rep->rr_buffer)->ri_id->device,
 +	switch (wc->opcode) {
 +	case IB_WC_FAST_REG_MR:
 +		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
 +		frmr->r.frmr.state = FRMR_IS_VALID;
 +		break;
 +	case IB_WC_LOCAL_INV:
 +		frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;
 +		frmr->r.frmr.state = FRMR_IS_INVALID;
 +		break;
 +	case IB_WC_RECV:
 +		rep->rr_len = wc->byte_len;
 +		ib_dma_sync_single_for_cpu(
 +			rdmab_to_ia(rep->rr_buffer)->ri_id->device,
  			rep->rr_iov.addr, rep->rr_len, DMA_FROM_DEVICE);
 -
 -	if (rep->rr_len >= 16) {
 -		struct rpcrdma_msg *p = (struct rpcrdma_msg *)rep->rr_base;
 -		unsigned int credits = ntohl(p->rm_credit);
 -
 -		if (credits == 0)
 -			credits = 1;	/* don't deadlock */
 -		else if (credits > rep->rr_buffer->rb_max_requests)
 -			credits = rep->rr_buffer->rb_max_requests;
 -		atomic_set(&rep->rr_buffer->rb_credits, credits);
 +		/* Keep (only) the most recent credits, after check validity */
 +		if (rep->rr_len >= 16) {
 +			struct rpcrdma_msg *p =
 +					(struct rpcrdma_msg *) rep->rr_base;
 +			unsigned int credits = ntohl(p->rm_credit);
 +			if (credits == 0) {
 +				dprintk("RPC:       %s: server"
 +					" dropped credits to 0!\n", __func__);
 +				/* don't deadlock */
 +				credits = 1;
 +			} else if (credits > rep->rr_buffer->rb_max_requests) {
 +				dprintk("RPC:       %s: server"
 +					" over-crediting: %d (%d)\n",
 +					__func__, credits,
 +					rep->rr_buffer->rb_max_requests);
 +				credits = rep->rr_buffer->rb_max_requests;
 +			}
 +			atomic_set(&rep->rr_buffer->rb_credits, credits);
 +		}
 +		/* fall through */
 +	case IB_WC_BIND_MW:
 +		rpcrdma_schedule_tasklet(rep);
 +		break;
 +	default:
 +		dprintk("RPC:       %s: unexpected WC event %X\n",
 +			__func__, wc->opcode);
 +		break;
  	}
 -
 -out_schedule:
 -	rpcrdma_schedule_tasklet(rep);
  }
  
++<<<<<<< HEAD
 +static inline int
 +rpcrdma_cq_poll(struct ib_cq *cq)
++=======
+ static int
+ rpcrdma_recvcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  {
- 	struct ib_wc wc;
- 	int rc;
+ 	struct ib_wc *wcs;
+ 	int count, rc;
  
++<<<<<<< HEAD
 +	for (;;) {
 +		rc = ib_poll_cq(cq, 1, &wc);
 +		if (rc < 0) {
 +			dprintk("RPC:       %s: ib_poll_cq failed %i\n",
 +				__func__, rc);
 +			return rc;
 +		}
 +		if (rc == 0)
 +			break;
 +
 +		rpcrdma_event_process(&wc);
 +	}
 +
++=======
+ 	do {
+ 		wcs = ep->rep_recv_wcs;
+ 
+ 		rc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);
+ 		if (rc <= 0)
+ 			return rc;
+ 
+ 		count = rc;
+ 		while (count-- > 0)
+ 			rpcrdma_recvcq_process_wc(wcs++);
+ 	} while (rc == RPCRDMA_POLLSIZE);
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  	return 0;
  }
  
@@@ -242,26 -281,31 +339,45 @@@
   * connection shutdown. That is, the structures required for
   * the completion of the reply handler must remain intact until
   * all memory has been reclaimed.
 + *
 + * Note that send events are suppressed and do not result in an upcall.
   */
  static void
 -rpcrdma_recvcq_upcall(struct ib_cq *cq, void *cq_context)
 +rpcrdma_cq_event_upcall(struct ib_cq *cq, void *context)
  {
+ 	struct rpcrdma_ep *ep = (struct rpcrdma_ep *)cq_context;
  	int rc;
  
++<<<<<<< HEAD
 +	rc = rpcrdma_cq_poll(cq);
 +	if (rc)
 +		return;
 +
 +	rc = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);
++=======
+ 	rc = rpcrdma_recvcq_poll(cq, ep);
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  	if (rc) {
 -		dprintk("RPC:       %s: ib_poll_cq failed: %i\n",
 +		dprintk("RPC:       %s: ib_req_notify_cq failed %i\n",
  			__func__, rc);
  		return;
  	}
  
++<<<<<<< HEAD
 +	rpcrdma_cq_poll(cq);
++=======
+ 	rc = ib_req_notify_cq(cq,
+ 			IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);
+ 	if (rc == 0)
+ 		return;
+ 	if (rc < 0) {
+ 		dprintk("RPC:       %s: ib_req_notify_cq failed: %i\n",
+ 			__func__, rc);
+ 		return;
+ 	}
+ 
+ 	rpcrdma_recvcq_poll(cq, ep);
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  }
  
  #ifdef RPC_DEBUG
@@@ -743,18 -740,12 +859,23 @@@ rpcrdma_ep_create(struct rpcrdma_ep *ep
  	init_waitqueue_head(&ep->rep_connect_wait);
  	INIT_DELAYED_WORK(&ep->rep_connect_worker, rpcrdma_connect_worker);
  
++<<<<<<< HEAD
 +	/*
 +	 * Create a single cq for receive dto and mw_bind (only ever
 +	 * care about unbind, really). Send completions are suppressed.
 +	 * Use single threaded tasklet upcalls to maintain ordering.
 +	 */
 +	ep->rep_cq = ib_create_cq(ia->ri_id->device, rpcrdma_cq_event_upcall,
 +				  rpcrdma_cq_async_error_upcall, NULL,
 +				  ep->rep_attr.cap.max_recv_wr +
++=======
+ 	sendcq = ib_create_cq(ia->ri_id->device, rpcrdma_sendcq_upcall,
+ 				  rpcrdma_cq_async_error_upcall, ep,
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  				  ep->rep_attr.cap.max_send_wr + 1, 0);
 -	if (IS_ERR(sendcq)) {
 -		rc = PTR_ERR(sendcq);
 -		dprintk("RPC:       %s: failed to create send CQ: %i\n",
 +	if (IS_ERR(ep->rep_cq)) {
 +		rc = PTR_ERR(ep->rep_cq);
 +		dprintk("RPC:       %s: ib_create_cq failed: %i\n",
  			__func__, rc);
  		goto out1;
  	}
@@@ -766,8 -757,26 +887,31 @@@
  		goto out2;
  	}
  
++<<<<<<< HEAD
 +	ep->rep_attr.send_cq = ep->rep_cq;
 +	ep->rep_attr.recv_cq = ep->rep_cq;
++=======
+ 	recvcq = ib_create_cq(ia->ri_id->device, rpcrdma_recvcq_upcall,
+ 				  rpcrdma_cq_async_error_upcall, ep,
+ 				  ep->rep_attr.cap.max_recv_wr + 1, 0);
+ 	if (IS_ERR(recvcq)) {
+ 		rc = PTR_ERR(recvcq);
+ 		dprintk("RPC:       %s: failed to create recv CQ: %i\n",
+ 			__func__, rc);
+ 		goto out2;
+ 	}
+ 
+ 	rc = ib_req_notify_cq(recvcq, IB_CQ_NEXT_COMP);
+ 	if (rc) {
+ 		dprintk("RPC:       %s: ib_req_notify_cq failed: %i\n",
+ 			__func__, rc);
+ 		ib_destroy_cq(recvcq);
+ 		goto out2;
+ 	}
+ 
+ 	ep->rep_attr.send_cq = sendcq;
+ 	ep->rep_attr.recv_cq = recvcq;
++>>>>>>> 1c00dd077654 (xprtrmda: Reduce calls to ib_poll_cq() in completion handlers)
  
  	/* Initialize cma parameters */
  
* Unmerged path net/sunrpc/xprtrdma/verbs.c
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cd7bc9218343..0d22613c3dbe 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -74,6 +74,8 @@ struct rpcrdma_ia {
  * RDMA Endpoint -- one per transport instance
  */
 
+#define RPCRDMA_POLLSIZE	(16)
+
 struct rpcrdma_ep {
 	atomic_t		rep_cqcount;
 	int			rep_cqinit;
@@ -89,6 +91,8 @@ struct rpcrdma_ep {
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
+	struct ib_wc		rep_send_wcs[RPCRDMA_POLLSIZE];
+	struct ib_wc		rep_recv_wcs[RPCRDMA_POLLSIZE];
 };
 
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
