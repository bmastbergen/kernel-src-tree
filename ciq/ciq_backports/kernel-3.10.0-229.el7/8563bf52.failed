KVM: PPC: Book3S HV: Add support for DABRX register on POWER7

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/ppc: book3s hv - Add support for DABRX register on POWER7 (Don Zickus) [1127366]
Rebuild_FUZZ: 95.08%
commit-author Paul Mackerras <paulus@samba.org>
commit 8563bf52d509213e746295341ab52896b562ca5e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/8563bf52.failed

The DABRX (DABR extension) register on POWER7 processors provides finer
control over which accesses cause a data breakpoint interrupt.  It
contains 3 bits which indicate whether to enable accesses in user,
kernel and hypervisor modes respectively to cause data breakpoint
interrupts, plus one bit that enables both real mode and virtual mode
accesses to cause interrupts.  Currently, KVM sets DABRX to allow
both kernel and user accesses to cause interrupts while in the guest.

This adds support for the guest to specify other values for DABRX.
PAPR defines a H_SET_XDABR hcall to allow the guest to set both DABR
and DABRX with one call.  This adds a real-mode implementation of
H_SET_XDABR, which shares most of its code with the existing H_SET_DABR
implementation.  To support this, we add a per-vcpu field to store the
DABRX value plus code to get and set it via the ONE_REG interface.

For Linux guests to use this new hcall, userspace needs to add
"hcall-xdabr" to the set of strings in the /chosen/hypertas-functions
property in the device tree.  If userspace does this and then migrates
the guest to a host where the kernel doesn't include this patch, then
userspace will need to implement H_SET_XDABR by writing the specified
DABR value to the DABR using the ONE_REG interface.  In that case, the
old kernel will set DABRX to DABRX_USER | DABRX_KERNEL.  That should
still work correctly, at least for Linux guests, since Linux guests
cope with getting data breakpoint interrupts in modes that weren't
requested by just ignoring the interrupt, and Linux guests never set
DABRX_BTI.

The other thing this does is to make H_SET_DABR and H_SET_XDABR work
on POWER8, which has the DAWR and DAWRX instead of DABR/X.  Guests that
know about POWER8 should use H_SET_MODE rather than H_SET_[X]DABR, but
guests running in POWER7 compatibility mode will still use H_SET_[X]DABR.
For them, this adds the logic to convert DABR/X values into DAWR/X values
on POWER8.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit 8563bf52d509213e746295341ab52896b562ca5e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kernel/asm-offsets.c
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/kernel/asm-offsets.c
index 79fa5c26d629,239a857f1141..000000000000
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@@ -492,8 -488,15 +492,15 @@@ int main(void
  	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
  	DEFINE(VCPU_AMR, offsetof(struct kvm_vcpu, arch.amr));
  	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
 -	DEFINE(VCPU_IAMR, offsetof(struct kvm_vcpu, arch.iamr));
  	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
  	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
++<<<<<<< HEAD
++=======
+ 	DEFINE(VCPU_DABRX, offsetof(struct kvm_vcpu, arch.dabrx));
+ 	DEFINE(VCPU_DAWR, offsetof(struct kvm_vcpu, arch.dawr));
+ 	DEFINE(VCPU_DAWRX, offsetof(struct kvm_vcpu, arch.dawrx));
+ 	DEFINE(VCPU_CIABR, offsetof(struct kvm_vcpu, arch.ciabr));
++>>>>>>> 8563bf52d509 (KVM: PPC: Book3S HV: Add support for DABRX register on POWER7)
  	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
  	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
  	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index 0ae4c28ae4b6,56299349e94b..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -642,6 -559,207 +642,210 @@@ toc_tlbie_lock
  	addi	r6,r6,VCPU_SLB_SIZE
  	bdnz	1b
  9:
++<<<<<<< HEAD
++=======
+ 	/* Increment yield count if they have a VPA */
+ 	ld	r3, VCPU_VPA(r4)
+ 	cmpdi	r3, 0
+ 	beq	25f
+ 	lwz	r5, LPPACA_YIELDCOUNT(r3)
+ 	addi	r5, r5, 1
+ 	stw	r5, LPPACA_YIELDCOUNT(r3)
+ 	li	r6, 1
+ 	stb	r6, VCPU_VPA_DIRTY(r4)
+ 25:
+ 
+ BEGIN_FTR_SECTION
+ 	/* Save purr/spurr */
+ 	mfspr	r5,SPRN_PURR
+ 	mfspr	r6,SPRN_SPURR
+ 	std	r5,HSTATE_PURR(r13)
+ 	std	r6,HSTATE_SPURR(r13)
+ 	ld	r7,VCPU_PURR(r4)
+ 	ld	r8,VCPU_SPURR(r4)
+ 	mtspr	SPRN_PURR,r7
+ 	mtspr	SPRN_SPURR,r8
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Set partition DABR */
+ 	/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */
+ 	lwz	r5,VCPU_DABRX(r4)
+ 	ld	r6,VCPU_DABR(r4)
+ 	mtspr	SPRN_DABRX,r5
+ 	mtspr	SPRN_DABR,r6
+  BEGIN_FTR_SECTION_NESTED(89)
+ 	isync
+  END_FTR_SECTION_NESTED(CPU_FTR_ARCH_206, CPU_FTR_ARCH_206, 89)
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 
+ 	/* Load guest PMU registers */
+ 	/* R4 is live here (vcpu pointer) */
+ 	li	r3, 1
+ 	sldi	r3, r3, 31		/* MMCR0_FC (freeze counters) bit */
+ 	mtspr	SPRN_MMCR0, r3		/* freeze all counters, disable ints */
+ 	isync
+ 	lwz	r3, VCPU_PMC(r4)	/* always load up guest PMU registers */
+ 	lwz	r5, VCPU_PMC + 4(r4)	/* to prevent information leak */
+ 	lwz	r6, VCPU_PMC + 8(r4)
+ 	lwz	r7, VCPU_PMC + 12(r4)
+ 	lwz	r8, VCPU_PMC + 16(r4)
+ 	lwz	r9, VCPU_PMC + 20(r4)
+ BEGIN_FTR_SECTION
+ 	lwz	r10, VCPU_PMC + 24(r4)
+ 	lwz	r11, VCPU_PMC + 28(r4)
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	mtspr	SPRN_PMC1, r3
+ 	mtspr	SPRN_PMC2, r5
+ 	mtspr	SPRN_PMC3, r6
+ 	mtspr	SPRN_PMC4, r7
+ 	mtspr	SPRN_PMC5, r8
+ 	mtspr	SPRN_PMC6, r9
+ BEGIN_FTR_SECTION
+ 	mtspr	SPRN_PMC7, r10
+ 	mtspr	SPRN_PMC8, r11
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_201)
+ 	ld	r3, VCPU_MMCR(r4)
+ 	ld	r5, VCPU_MMCR + 8(r4)
+ 	ld	r6, VCPU_MMCR + 16(r4)
+ 	ld	r7, VCPU_SIAR(r4)
+ 	ld	r8, VCPU_SDAR(r4)
+ 	mtspr	SPRN_MMCR1, r5
+ 	mtspr	SPRN_MMCRA, r6
+ 	mtspr	SPRN_SIAR, r7
+ 	mtspr	SPRN_SDAR, r8
+ BEGIN_FTR_SECTION
+ 	ld	r5, VCPU_MMCR + 24(r4)
+ 	ld	r6, VCPU_SIER(r4)
+ 	lwz	r7, VCPU_PMC + 24(r4)
+ 	lwz	r8, VCPU_PMC + 28(r4)
+ 	ld	r9, VCPU_MMCR + 32(r4)
+ 	mtspr	SPRN_MMCR2, r5
+ 	mtspr	SPRN_SIER, r6
+ 	mtspr	SPRN_SPMC1, r7
+ 	mtspr	SPRN_SPMC2, r8
+ 	mtspr	SPRN_MMCRS, r9
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)
+ 	mtspr	SPRN_MMCR0, r3
+ 	isync
+ 
+ 	/* Load up FP, VMX and VSX registers */
+ 	bl	kvmppc_load_fp
+ 
+ 	ld	r14, VCPU_GPR(R14)(r4)
+ 	ld	r15, VCPU_GPR(R15)(r4)
+ 	ld	r16, VCPU_GPR(R16)(r4)
+ 	ld	r17, VCPU_GPR(R17)(r4)
+ 	ld	r18, VCPU_GPR(R18)(r4)
+ 	ld	r19, VCPU_GPR(R19)(r4)
+ 	ld	r20, VCPU_GPR(R20)(r4)
+ 	ld	r21, VCPU_GPR(R21)(r4)
+ 	ld	r22, VCPU_GPR(R22)(r4)
+ 	ld	r23, VCPU_GPR(R23)(r4)
+ 	ld	r24, VCPU_GPR(R24)(r4)
+ 	ld	r25, VCPU_GPR(R25)(r4)
+ 	ld	r26, VCPU_GPR(R26)(r4)
+ 	ld	r27, VCPU_GPR(R27)(r4)
+ 	ld	r28, VCPU_GPR(R28)(r4)
+ 	ld	r29, VCPU_GPR(R29)(r4)
+ 	ld	r30, VCPU_GPR(R30)(r4)
+ 	ld	r31, VCPU_GPR(R31)(r4)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Switch DSCR to guest value */
+ 	ld	r5, VCPU_DSCR(r4)
+ 	mtspr	SPRN_DSCR, r5
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
+ 
+ BEGIN_FTR_SECTION
+ 	/* Skip next section on POWER7 or PPC970 */
+ 	b	8f
+ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)
+ 	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
+ 	mfmsr	r8
+ 	li	r0, 1
+ 	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
+ 	mtmsrd	r8
+ 
+ 	/* Load up POWER8-specific registers */
+ 	ld	r5, VCPU_IAMR(r4)
+ 	lwz	r6, VCPU_PSPB(r4)
+ 	ld	r7, VCPU_FSCR(r4)
+ 	mtspr	SPRN_IAMR, r5
+ 	mtspr	SPRN_PSPB, r6
+ 	mtspr	SPRN_FSCR, r7
+ 	ld	r5, VCPU_DAWR(r4)
+ 	ld	r6, VCPU_DAWRX(r4)
+ 	ld	r7, VCPU_CIABR(r4)
+ 	ld	r8, VCPU_TAR(r4)
+ 	mtspr	SPRN_DAWR, r5
+ 	mtspr	SPRN_DAWRX, r6
+ 	mtspr	SPRN_CIABR, r7
+ 	mtspr	SPRN_TAR, r8
+ 	ld	r5, VCPU_IC(r4)
+ 	ld	r6, VCPU_VTB(r4)
+ 	mtspr	SPRN_IC, r5
+ 	mtspr	SPRN_VTB, r6
+ 	ld	r5, VCPU_TFHAR(r4)
+ 	ld	r6, VCPU_TFIAR(r4)
+ 	ld	r7, VCPU_TEXASR(r4)
+ 	ld	r8, VCPU_EBBHR(r4)
+ 	mtspr	SPRN_TFHAR, r5
+ 	mtspr	SPRN_TFIAR, r6
+ 	mtspr	SPRN_TEXASR, r7
+ 	mtspr	SPRN_EBBHR, r8
+ 	ld	r5, VCPU_EBBRR(r4)
+ 	ld	r6, VCPU_BESCR(r4)
+ 	ld	r7, VCPU_CSIGR(r4)
+ 	ld	r8, VCPU_TACR(r4)
+ 	mtspr	SPRN_EBBRR, r5
+ 	mtspr	SPRN_BESCR, r6
+ 	mtspr	SPRN_CSIGR, r7
+ 	mtspr	SPRN_TACR, r8
+ 	ld	r5, VCPU_TCSCR(r4)
+ 	ld	r6, VCPU_ACOP(r4)
+ 	lwz	r7, VCPU_GUEST_PID(r4)
+ 	ld	r8, VCPU_WORT(r4)
+ 	mtspr	SPRN_TCSCR, r5
+ 	mtspr	SPRN_ACOP, r6
+ 	mtspr	SPRN_PID, r7
+ 	mtspr	SPRN_WORT, r8
+ 8:
+ 
+ 	/*
+ 	 * Set the decrementer to the guest decrementer.
+ 	 */
+ 	ld	r8,VCPU_DEC_EXPIRES(r4)
+ 	mftb	r7
+ 	subf	r3,r7,r8
+ 	mtspr	SPRN_DEC,r3
+ 	stw	r3,VCPU_DEC(r4)
+ 
+ 	ld	r5, VCPU_SPRG0(r4)
+ 	ld	r6, VCPU_SPRG1(r4)
+ 	ld	r7, VCPU_SPRG2(r4)
+ 	ld	r8, VCPU_SPRG3(r4)
+ 	mtspr	SPRN_SPRG0, r5
+ 	mtspr	SPRN_SPRG1, r6
+ 	mtspr	SPRN_SPRG2, r7
+ 	mtspr	SPRN_SPRG3, r8
+ 
+ 	/* Load up DAR and DSISR */
+ 	ld	r5, VCPU_DAR(r4)
+ 	lwz	r6, VCPU_DSISR(r4)
+ 	mtspr	SPRN_DAR, r5
+ 	mtspr	SPRN_DSISR, r6
+ 
+ BEGIN_FTR_SECTION
+ 	/* Restore AMR and UAMOR, set AMOR to all 1s */
+ 	ld	r5,VCPU_AMR(r4)
+ 	ld	r6,VCPU_UAMOR(r4)
+ 	li	r7,-1
+ 	mtspr	SPRN_AMR,r5
+ 	mtspr	SPRN_UAMOR,r6
+ 	mtspr	SPRN_AMOR,r7
+ END_FTR_SECTION_IFSET(CPU_FTR_ARCH_206)
++>>>>>>> 8563bf52d509 (KVM: PPC: Book3S HV: Add support for DABRX register on POWER7)
  
  	/* Restore state of CTRL run bit; assume 1 on entry */
  	lwz	r5,VCPU_CTRL(r4)
diff --git a/Documentation/virtual/kvm/api.txt b/Documentation/virtual/kvm/api.txt
index b2bc745240b2..7a1291a5b87e 100644
--- a/Documentation/virtual/kvm/api.txt
+++ b/Documentation/virtual/kvm/api.txt
@@ -1851,6 +1851,7 @@ registers, find a list below:
   PPC   | KVM_REG_PPC_LPCR	| 64
   PPC   | KVM_REG_PPC_PPR	| 64
   PPC   | KVM_REG_PPC_ARCH_COMPAT 32
+  PPC   | KVM_REG_PPC_DABRX     | 32
   PPC   | KVM_REG_PPC_TM_GPR0	| 64
           ...
   PPC   | KVM_REG_PPC_TM_GPR31	| 64
diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 80c8a73bb706..36042e92537f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -474,6 +474,7 @@ struct kvm_vcpu_arch {
 	ulong amr;
 	ulong uamor;
 	u32 ctrl;
+	u32 dabrx;
 	ulong dabr;
 	ulong cfar;
 	ulong ppr;
diff --git a/arch/powerpc/include/asm/reg.h b/arch/powerpc/include/asm/reg.h
index e983e00762dc..38956c5c60d6 100644
--- a/arch/powerpc/include/asm/reg.h
+++ b/arch/powerpc/include/asm/reg.h
@@ -224,16 +224,20 @@
 #define   CTRL_RUNLATCH	0x1
 #define SPRN_DAWR	0xB4
 #define SPRN_DAWRX	0xBC
-#define   DAWRX_USER	(1UL << 0)
-#define   DAWRX_KERNEL	(1UL << 1)
-#define   DAWRX_HYP	(1UL << 2)
+#define   DAWRX_USER	__MASK(0)
+#define   DAWRX_KERNEL	__MASK(1)
+#define   DAWRX_HYP	__MASK(2)
+#define   DAWRX_WTI	__MASK(3)
+#define   DAWRX_WT	__MASK(4)
+#define   DAWRX_DR	__MASK(5)
+#define   DAWRX_DW	__MASK(6)
 #define SPRN_DABR	0x3F5	/* Data Address Breakpoint Register */
 #define SPRN_DABR2	0x13D	/* e300 */
 #define SPRN_DABRX	0x3F7	/* Data Address Breakpoint Register Extension */
-#define   DABRX_USER	(1UL << 0)
-#define   DABRX_KERNEL	(1UL << 1)
-#define   DABRX_HYP	(1UL << 2)
-#define   DABRX_BTI	(1UL << 3)
+#define   DABRX_USER	__MASK(0)
+#define   DABRX_KERNEL	__MASK(1)
+#define   DABRX_HYP	__MASK(2)
+#define   DABRX_BTI	__MASK(3)
 #define   DABRX_ALL     (DABRX_BTI | DABRX_HYP | DABRX_KERNEL | DABRX_USER)
 #define SPRN_DAR	0x013	/* Data Address Register */
 #define SPRN_DBCR	0x136	/* e300 Data Breakpoint Control Reg */
diff --git a/arch/powerpc/include/uapi/asm/kvm.h b/arch/powerpc/include/uapi/asm/kvm.h
index e420d46d363f..6dce58390c0d 100644
--- a/arch/powerpc/include/uapi/asm/kvm.h
+++ b/arch/powerpc/include/uapi/asm/kvm.h
@@ -539,6 +539,8 @@ struct kvm_get_htab_header {
 /* Architecture compatibility level */
 #define KVM_REG_PPC_ARCH_COMPAT	(KVM_REG_PPC | KVM_REG_SIZE_U32 | 0xb7)
 
+#define KVM_REG_PPC_DABRX	(KVM_REG_PPC | KVM_REG_SIZE_U32 | 0xb8)
+
 /* Transactional Memory checkpointed state:
  * This is all GPRs, all VSX regs and a subset of SPRs
  */
* Unmerged path arch/powerpc/kernel/asm-offsets.c
diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
index bd4792543205..0afdc4d85d14 100644
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@ -804,6 +804,9 @@ int kvmppc_get_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *val)
 	case KVM_REG_PPC_DABR:
 		*val = get_reg_val(id, vcpu->arch.dabr);
 		break;
+	case KVM_REG_PPC_DABRX:
+		*val = get_reg_val(id, vcpu->arch.dabrx);
+		break;
 	case KVM_REG_PPC_DSCR:
 		*val = get_reg_val(id, vcpu->arch.dscr);
 		break;
@@ -885,6 +888,9 @@ int kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *val)
 	case KVM_REG_PPC_DABR:
 		vcpu->arch.dabr = set_reg_val(id, *val);
 		break;
+	case KVM_REG_PPC_DABRX:
+		vcpu->arch.dabrx = set_reg_val(id, *val) & ~DABRX_HYP;
+		break;
 	case KVM_REG_PPC_DSCR:
 		vcpu->arch.dscr = set_reg_val(id, *val);
 		break;
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
