kexec: support for kexec on panic using new system call

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Vivek Goyal <vgoyal@redhat.com>
commit dd5f726076cc7639d9713b334c8c133f77c6757a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/dd5f7260.failed

This patch adds support for loading a kexec on panic (kdump) kernel usning
new system call.

It prepares ELF headers for memory areas to be dumped and for saved cpu
registers.  Also prepares the memory map for second kernel and limits its
boot to reserved areas only.

	Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Michael Kerrisk <mtk.manpages@gmail.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: Eric Biederman <ebiederm@xmission.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Matthew Garrett <mjg59@srcf.ucam.org>
	Cc: Greg Kroah-Hartman <greg@kroah.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: WANG Chao <chaowang@redhat.com>
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit dd5f726076cc7639d9713b334c8c133f77c6757a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/crash.h
#	arch/x86/include/asm/kexec.h
#	arch/x86/kernel/kexec-bzimage64.c
#	arch/x86/kernel/machine_kexec_64.c
#	kernel/kexec.c
diff --cc arch/x86/include/asm/crash.h
index dfcc006f734d,f498411f2500..000000000000
--- a/arch/x86/include/asm/crash.h
+++ b/arch/x86/include/asm/crash.h
@@@ -1,75 -1,9 +1,87 @@@
++<<<<<<< HEAD
 +#ifndef _ASM_I386_CRASH_H
 +#define _ASM_I386_CRASH_H
 +
 +/*
 + * linux/include/asm-i386/crash.h
 + *
 + * Copyright (c) 2004 Red Hat, Inc. All rights reserved.
 + *
 + * This program is free software; you can redistribute it and/or modify
 + * it under the terms of the GNU General Public License as published by
 + * the Free Software Foundation; either version 2, or (at your option)
 + * any later version.
 + *
 + * This program is distributed in the hope that it will be useful,
 + * but WITHOUT ANY WARRANTY; without even the implied warranty of
 + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 + * GNU General Public License for more details.
 + *
 + * You should have received a copy of the GNU General Public License
 + * along with this program; if not, write to the Free Software
 + * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 + *
 + */
 +
 +#ifdef __KERNEL__
 +
 +#include <linux/mm.h>
 +#include <linux/highmem.h>
 +#include <asm/mmzone.h>
 +
 +extern int page_is_ram(unsigned long);
 +
 +static inline void *
 +map_virtual(u64 offset, struct page **pp)
 +{
 +	struct page *page;
 +	unsigned long pfn;
 +	void *vaddr;
 +
 +	pfn = (unsigned long)(offset >> PAGE_SHIFT);
 +
 +	if (!page_is_ram(pfn)) {
 +		printk(KERN_INFO
 +		    "crash memory driver: !page_is_ram(pfn: %lx)\n", pfn);
 +		return NULL;
 +	}
 +
 +	if (!pfn_valid(pfn)) {
 +		printk(KERN_INFO
 +		    "crash memory driver: invalid pfn: %lx )\n", pfn);
 +		return NULL;
 +	}
 +
 +	page = pfn_to_page(pfn);
 +
 +	vaddr = kmap(page);
 +	if (!vaddr) {
 +		printk(KERN_INFO
 +		    "crash memory driver: pfn: %lx kmap(page: %lx) failed\n", 
 +			pfn, (unsigned long)page);
 +		return NULL;
 +	}
 +
 +	*pp = page;
 +	return (vaddr + (offset & (PAGE_SIZE-1)));
 +}
 +
 +static inline void unmap_virtual(struct page *page) 
 +{ 
 +	kunmap(page);
 +}
 +
 +#endif /* __KERNEL__ */
 +
 +#endif /* _ASM_I386_CRASH_H */
++=======
+ #ifndef _ASM_X86_CRASH_H
+ #define _ASM_X86_CRASH_H
+ 
+ int crash_load_segments(struct kimage *image);
+ int crash_copy_backup_region(struct kimage *image);
+ int crash_setup_memmap_entries(struct kimage *image,
+ 		struct boot_params *params);
+ 
+ #endif /* _ASM_X86_CRASH_H */
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
diff --cc arch/x86/include/asm/kexec.h
index c1bf8d95576d,d2434c1cad05..000000000000
--- a/arch/x86/include/asm/kexec.h
+++ b/arch/x86/include/asm/kexec.h
@@@ -23,8 -23,10 +23,10 @@@
  
  #include <asm/page.h>
  #include <asm/ptrace.h>
 -#include <asm/bootparam.h>
 +#include <asm-generic/kexec.h>
  
+ struct kimage;
+ 
  /*
   * KEXEC_SOURCE_MEMORY_LIMIT maximum page get_free_page can return.
   * I.e. Maximum page that is mapped directly into kernel memory,
@@@ -161,6 -167,44 +167,47 @@@ struct kimage_arch 
  	pud_t *pud;
  	pmd_t *pmd;
  	pte_t *pte;
++<<<<<<< HEAD
++=======
+ 	/* Details of backup region */
+ 	unsigned long backup_src_start;
+ 	unsigned long backup_src_sz;
+ 
+ 	/* Physical address of backup segment */
+ 	unsigned long backup_load_addr;
+ 
+ 	/* Core ELF header buffer */
+ 	void *elf_headers;
+ 	unsigned long elf_headers_sz;
+ 	unsigned long elf_load_addr;
+ };
+ #endif /* CONFIG_X86_32 */
+ 
+ #ifdef CONFIG_X86_64
+ /*
+  * Number of elements and order of elements in this structure should match
+  * with the ones in arch/x86/purgatory/entry64.S. If you make a change here
+  * make an appropriate change in purgatory too.
+  */
+ struct kexec_entry64_regs {
+ 	uint64_t rax;
+ 	uint64_t rcx;
+ 	uint64_t rdx;
+ 	uint64_t rbx;
+ 	uint64_t rsp;
+ 	uint64_t rbp;
+ 	uint64_t rsi;
+ 	uint64_t rdi;
+ 	uint64_t r8;
+ 	uint64_t r9;
+ 	uint64_t r10;
+ 	uint64_t r11;
+ 	uint64_t r12;
+ 	uint64_t r13;
+ 	uint64_t r14;
+ 	uint64_t r15;
+ 	uint64_t rip;
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
  };
  #endif
  
diff --cc arch/x86/kernel/machine_kexec_64.c
index 4eabc160696f,9330434da777..000000000000
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@@ -279,5 -323,191 +316,192 @@@ void arch_crash_save_vmcoreinfo(void
  	VMCOREINFO_SYMBOL(node_data);
  	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
  #endif
 -	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 -			      (unsigned long)&_text - __START_KERNEL);
  }
  
++<<<<<<< HEAD
++=======
+ /* arch-dependent functionality related to kexec file-based syscall */
+ 
+ int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
+ 				  unsigned long buf_len)
+ {
+ 	int i, ret = -ENOEXEC;
+ 	struct kexec_file_ops *fops;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(kexec_file_loaders); i++) {
+ 		fops = kexec_file_loaders[i];
+ 		if (!fops || !fops->probe)
+ 			continue;
+ 
+ 		ret = fops->probe(buf, buf_len);
+ 		if (!ret) {
+ 			image->fops = fops;
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ void *arch_kexec_kernel_image_load(struct kimage *image)
+ {
+ 	vfree(image->arch.elf_headers);
+ 	image->arch.elf_headers = NULL;
+ 
+ 	if (!image->fops || !image->fops->load)
+ 		return ERR_PTR(-ENOEXEC);
+ 
+ 	return image->fops->load(image, image->kernel_buf,
+ 				 image->kernel_buf_len, image->initrd_buf,
+ 				 image->initrd_buf_len, image->cmdline_buf,
+ 				 image->cmdline_buf_len);
+ }
+ 
+ int arch_kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ 	if (!image->fops || !image->fops->cleanup)
+ 		return 0;
+ 
+ 	return image->fops->cleanup(image->image_loader_data);
+ }
+ 
+ /*
+  * Apply purgatory relocations.
+  *
+  * ehdr: Pointer to elf headers
+  * sechdrs: Pointer to section headers.
+  * relsec: section index of SHT_RELA section.
+  *
+  * TODO: Some of the code belongs to generic code. Move that in kexec.c.
+  */
+ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
+ 				     Elf64_Shdr *sechdrs, unsigned int relsec)
+ {
+ 	unsigned int i;
+ 	Elf64_Rela *rel;
+ 	Elf64_Sym *sym;
+ 	void *location;
+ 	Elf64_Shdr *section, *symtabsec;
+ 	unsigned long address, sec_base, value;
+ 	const char *strtab, *name, *shstrtab;
+ 
+ 	/*
+ 	 * ->sh_offset has been modified to keep the pointer to section
+ 	 * contents in memory
+ 	 */
+ 	rel = (void *)sechdrs[relsec].sh_offset;
+ 
+ 	/* Section to which relocations apply */
+ 	section = &sechdrs[sechdrs[relsec].sh_info];
+ 
+ 	pr_debug("Applying relocate section %u to %u\n", relsec,
+ 		 sechdrs[relsec].sh_info);
+ 
+ 	/* Associated symbol table */
+ 	symtabsec = &sechdrs[sechdrs[relsec].sh_link];
+ 
+ 	/* String table */
+ 	if (symtabsec->sh_link >= ehdr->e_shnum) {
+ 		/* Invalid strtab section number */
+ 		pr_err("Invalid string table section index %d\n",
+ 		       symtabsec->sh_link);
+ 		return -ENOEXEC;
+ 	}
+ 
+ 	strtab = (char *)sechdrs[symtabsec->sh_link].sh_offset;
+ 
+ 	/* section header string table */
+ 	shstrtab = (char *)sechdrs[ehdr->e_shstrndx].sh_offset;
+ 
+ 	for (i = 0; i < sechdrs[relsec].sh_size / sizeof(*rel); i++) {
+ 
+ 		/*
+ 		 * rel[i].r_offset contains byte offset from beginning
+ 		 * of section to the storage unit affected.
+ 		 *
+ 		 * This is location to update (->sh_offset). This is temporary
+ 		 * buffer where section is currently loaded. This will finally
+ 		 * be loaded to a different address later, pointed to by
+ 		 * ->sh_addr. kexec takes care of moving it
+ 		 *  (kexec_load_segment()).
+ 		 */
+ 		location = (void *)(section->sh_offset + rel[i].r_offset);
+ 
+ 		/* Final address of the location */
+ 		address = section->sh_addr + rel[i].r_offset;
+ 
+ 		/*
+ 		 * rel[i].r_info contains information about symbol table index
+ 		 * w.r.t which relocation must be made and type of relocation
+ 		 * to apply. ELF64_R_SYM() and ELF64_R_TYPE() macros get
+ 		 * these respectively.
+ 		 */
+ 		sym = (Elf64_Sym *)symtabsec->sh_offset +
+ 				ELF64_R_SYM(rel[i].r_info);
+ 
+ 		if (sym->st_name)
+ 			name = strtab + sym->st_name;
+ 		else
+ 			name = shstrtab + sechdrs[sym->st_shndx].sh_name;
+ 
+ 		pr_debug("Symbol: %s info: %02x shndx: %02x value=%llx size: %llx\n",
+ 			 name, sym->st_info, sym->st_shndx, sym->st_value,
+ 			 sym->st_size);
+ 
+ 		if (sym->st_shndx == SHN_UNDEF) {
+ 			pr_err("Undefined symbol: %s\n", name);
+ 			return -ENOEXEC;
+ 		}
+ 
+ 		if (sym->st_shndx == SHN_COMMON) {
+ 			pr_err("symbol '%s' in common section\n", name);
+ 			return -ENOEXEC;
+ 		}
+ 
+ 		if (sym->st_shndx == SHN_ABS)
+ 			sec_base = 0;
+ 		else if (sym->st_shndx >= ehdr->e_shnum) {
+ 			pr_err("Invalid section %d for symbol %s\n",
+ 			       sym->st_shndx, name);
+ 			return -ENOEXEC;
+ 		} else
+ 			sec_base = sechdrs[sym->st_shndx].sh_addr;
+ 
+ 		value = sym->st_value;
+ 		value += sec_base;
+ 		value += rel[i].r_addend;
+ 
+ 		switch (ELF64_R_TYPE(rel[i].r_info)) {
+ 		case R_X86_64_NONE:
+ 			break;
+ 		case R_X86_64_64:
+ 			*(u64 *)location = value;
+ 			break;
+ 		case R_X86_64_32:
+ 			*(u32 *)location = value;
+ 			if (value != *(u32 *)location)
+ 				goto overflow;
+ 			break;
+ 		case R_X86_64_32S:
+ 			*(s32 *)location = value;
+ 			if ((s64)value != *(s32 *)location)
+ 				goto overflow;
+ 			break;
+ 		case R_X86_64_PC32:
+ 			value -= (u64)address;
+ 			*(u32 *)location = value;
+ 			break;
+ 		default:
+ 			pr_err("Unknown rela relocation: %llu\n",
+ 			       ELF64_R_TYPE(rel[i].r_info));
+ 			return -ENOEXEC;
+ 		}
+ 	}
+ 	return 0;
+ 
+ overflow:
+ 	pr_err("Overflow in relocation type %d value 0x%lx\n",
+ 	       (int)ELF64_R_TYPE(rel[i].r_info), value);
+ 	return -ENOEXEC;
+ }
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
diff --cc kernel/kexec.c
index 4b1d9ca3907c,f18c780f9716..000000000000
--- a/kernel/kexec.c
+++ b/kernel/kexec.c
@@@ -293,57 -361,241 +293,260 @@@ static int kimage_crash_alloc(struct ki
  		goto out;
  	}
  
 -	/* Don't hand 0 to vmalloc, it whines. */
 -	if (stat.size == 0) {
 -		ret = -EINVAL;
 +	/* Allocate and initialize a controlling structure */
 +	result = do_kimage_alloc(&image, entry, nr_segments, segments);
 +	if (result)
  		goto out;
 -	}
  
 -	*buf = vmalloc(stat.size);
 -	if (!*buf) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 +	/* Enable the special crash kernel control page
 +	 * allocation policy.
 +	 */
 +	image->control_page = crashk_res.start;
 +	image->type = KEXEC_TYPE_CRASH;
  
 -	pos = 0;
 -	while (pos < stat.size) {
 -		bytes = kernel_read(f.file, pos, (char *)(*buf) + pos,
 -				    stat.size - pos);
 -		if (bytes < 0) {
 -			vfree(*buf);
 -			ret = bytes;
 -			goto out;
 -		}
 +	/*
 +	 * Verify we have good destination addresses.  Normally
 +	 * the caller is responsible for making certain we don't
 +	 * attempt to load the new image into invalid or reserved
 +	 * areas of RAM.  But crash kernels are preloaded into a
 +	 * reserved area of ram.  We must ensure the addresses
 +	 * are in the reserved area otherwise preloading the
 +	 * kernel could corrupt things.
 +	 */
 +	result = -EADDRNOTAVAIL;
 +	for (i = 0; i < nr_segments; i++) {
 +		unsigned long mstart, mend;
  
 -		if (bytes == 0)
 -			break;
 -		pos += bytes;
 +		mstart = image->segment[i].mem;
 +		mend = mstart + image->segment[i].memsz - 1;
 +		/* Ensure we are within the crash kernel limits */
 +		if ((mstart < crashk_res.start) || (mend > crashk_res.end))
 +			goto out_free;
  	}
  
 -	if (pos != stat.size) {
 -		ret = -EBADF;
 -		vfree(*buf);
 -		goto out;
 +	/*
 +	 * Find a location for the control code buffer, and add
 +	 * the vector of segments so that it's pages will also be
 +	 * counted as destination pages.
 +	 */
 +	result = -ENOMEM;
 +	image->control_code_page = kimage_alloc_control_pages(image,
 +					   get_order(KEXEC_CONTROL_PAGE_SIZE));
 +	if (!image->control_code_page) {
 +		printk(KERN_ERR "Could not allocate control_code_buffer\n");
 +		goto out_free;
  	}
  
 -	*buf_len = pos;
 +	*rimage = image;
 +	return 0;
 +
 +out_free:
 +	kfree(image);
  out:
++<<<<<<< HEAD
 +	return result;
++=======
+ 	fdput(f);
+ 	return ret;
+ }
+ 
+ /* Architectures can provide this probe function */
+ int __weak arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
+ 					 unsigned long buf_len)
+ {
+ 	return -ENOEXEC;
+ }
+ 
+ void * __weak arch_kexec_kernel_image_load(struct kimage *image)
+ {
+ 	return ERR_PTR(-ENOEXEC);
+ }
+ 
+ void __weak arch_kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ }
+ 
+ /* Apply relocations of type RELA */
+ int __weak
+ arch_kexec_apply_relocations_add(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
+ 				 unsigned int relsec)
+ {
+ 	pr_err("RELA relocation unsupported.\n");
+ 	return -ENOEXEC;
+ }
+ 
+ /* Apply relocations of type REL */
+ int __weak
+ arch_kexec_apply_relocations(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
+ 			     unsigned int relsec)
+ {
+ 	pr_err("REL relocation unsupported.\n");
+ 	return -ENOEXEC;
+ }
+ 
+ /*
+  * Free up memory used by kernel, initrd, and comand line. This is temporary
+  * memory allocation which is not needed any more after these buffers have
+  * been loaded into separate segments and have been copied elsewhere.
+  */
+ static void kimage_file_post_load_cleanup(struct kimage *image)
+ {
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 
+ 	vfree(image->kernel_buf);
+ 	image->kernel_buf = NULL;
+ 
+ 	vfree(image->initrd_buf);
+ 	image->initrd_buf = NULL;
+ 
+ 	kfree(image->cmdline_buf);
+ 	image->cmdline_buf = NULL;
+ 
+ 	vfree(pi->purgatory_buf);
+ 	pi->purgatory_buf = NULL;
+ 
+ 	vfree(pi->sechdrs);
+ 	pi->sechdrs = NULL;
+ 
+ 	/* See if architecture has anything to cleanup post load */
+ 	arch_kimage_file_post_load_cleanup(image);
+ 
+ 	/*
+ 	 * Above call should have called into bootloader to free up
+ 	 * any data stored in kimage->image_loader_data. It should
+ 	 * be ok now to free it up.
+ 	 */
+ 	kfree(image->image_loader_data);
+ 	image->image_loader_data = NULL;
+ }
+ 
+ /*
+  * In file mode list of segments is prepared by kernel. Copy relevant
+  * data from user space, do error checking, prepare segment list
+  */
+ static int
+ kimage_file_prepare_segments(struct kimage *image, int kernel_fd, int initrd_fd,
+ 			     const char __user *cmdline_ptr,
+ 			     unsigned long cmdline_len, unsigned flags)
+ {
+ 	int ret = 0;
+ 	void *ldata;
+ 
+ 	ret = copy_file_from_fd(kernel_fd, &image->kernel_buf,
+ 				&image->kernel_buf_len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Call arch image probe handlers */
+ 	ret = arch_kexec_kernel_image_probe(image, image->kernel_buf,
+ 					    image->kernel_buf_len);
+ 
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* It is possible that there no initramfs is being loaded */
+ 	if (!(flags & KEXEC_FILE_NO_INITRAMFS)) {
+ 		ret = copy_file_from_fd(initrd_fd, &image->initrd_buf,
+ 					&image->initrd_buf_len);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ 	if (cmdline_len) {
+ 		image->cmdline_buf = kzalloc(cmdline_len, GFP_KERNEL);
+ 		if (!image->cmdline_buf) {
+ 			ret = -ENOMEM;
+ 			goto out;
+ 		}
+ 
+ 		ret = copy_from_user(image->cmdline_buf, cmdline_ptr,
+ 				     cmdline_len);
+ 		if (ret) {
+ 			ret = -EFAULT;
+ 			goto out;
+ 		}
+ 
+ 		image->cmdline_buf_len = cmdline_len;
+ 
+ 		/* command line should be a string with last byte null */
+ 		if (image->cmdline_buf[cmdline_len - 1] != '\0') {
+ 			ret = -EINVAL;
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	/* Call arch image load handlers */
+ 	ldata = arch_kexec_kernel_image_load(image);
+ 
+ 	if (IS_ERR(ldata)) {
+ 		ret = PTR_ERR(ldata);
+ 		goto out;
+ 	}
+ 
+ 	image->image_loader_data = ldata;
+ out:
+ 	/* In case of error, free up all allocated memory in this function */
+ 	if (ret)
+ 		kimage_file_post_load_cleanup(image);
+ 	return ret;
+ }
+ 
+ static int
+ kimage_file_alloc_init(struct kimage **rimage, int kernel_fd,
+ 		       int initrd_fd, const char __user *cmdline_ptr,
+ 		       unsigned long cmdline_len, unsigned long flags)
+ {
+ 	int ret;
+ 	struct kimage *image;
+ 	bool kexec_on_panic = flags & KEXEC_FILE_ON_CRASH;
+ 
+ 	image = do_kimage_alloc_init();
+ 	if (!image)
+ 		return -ENOMEM;
+ 
+ 	image->file_mode = 1;
+ 
+ 	if (kexec_on_panic) {
+ 		/* Enable special crash kernel control page alloc policy. */
+ 		image->control_page = crashk_res.start;
+ 		image->type = KEXEC_TYPE_CRASH;
+ 	}
+ 
+ 	ret = kimage_file_prepare_segments(image, kernel_fd, initrd_fd,
+ 					   cmdline_ptr, cmdline_len, flags);
+ 	if (ret)
+ 		goto out_free_image;
+ 
+ 	ret = sanity_check_segment_list(image);
+ 	if (ret)
+ 		goto out_free_post_load_bufs;
+ 
+ 	ret = -ENOMEM;
+ 	image->control_code_page = kimage_alloc_control_pages(image,
+ 					   get_order(KEXEC_CONTROL_PAGE_SIZE));
+ 	if (!image->control_code_page) {
+ 		pr_err("Could not allocate control_code_buffer\n");
+ 		goto out_free_post_load_bufs;
+ 	}
+ 
+ 	if (!kexec_on_panic) {
+ 		image->swap_page = kimage_alloc_control_pages(image, 0);
+ 		if (!image->swap_page) {
+ 			pr_err(KERN_ERR "Could not allocate swap buffer\n");
+ 			goto out_free_control_pages;
+ 		}
+ 	}
+ 
+ 	*rimage = image;
+ 	return 0;
+ out_free_control_pages:
+ 	kimage_free_page_list(&image->control_pages);
+ out_free_post_load_bufs:
+ 	kimage_file_post_load_cleanup(image);
+ out_free_image:
+ 	kfree(image);
+ 	return ret;
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
  }
  
  static int kimage_is_destination_range(struct kimage *image,
@@@ -888,7 -1166,10 +1100,14 @@@ static int kimage_load_crash_segment(st
  		}
  		ubytes -= uchunk;
  		maddr  += mchunk;
++<<<<<<< HEAD
 +		buf    += mchunk;
++=======
+ 		if (image->file_mode)
+ 			kbuf += mchunk;
+ 		else
+ 			buf += mchunk;
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
  		mbytes -= mchunk;
  	}
  out:
@@@ -1687,7 -1989,684 +1906,688 @@@ static int __init crash_save_vmcoreinfo
  	return 0;
  }
  
++<<<<<<< HEAD
 +module_init(crash_save_vmcoreinfo_init)
++=======
+ subsys_initcall(crash_save_vmcoreinfo_init);
+ 
+ static int __kexec_add_segment(struct kimage *image, char *buf,
+ 			       unsigned long bufsz, unsigned long mem,
+ 			       unsigned long memsz)
+ {
+ 	struct kexec_segment *ksegment;
+ 
+ 	ksegment = &image->segment[image->nr_segments];
+ 	ksegment->kbuf = buf;
+ 	ksegment->bufsz = bufsz;
+ 	ksegment->mem = mem;
+ 	ksegment->memsz = memsz;
+ 	image->nr_segments++;
+ 
+ 	return 0;
+ }
+ 
+ static int locate_mem_hole_top_down(unsigned long start, unsigned long end,
+ 				    struct kexec_buf *kbuf)
+ {
+ 	struct kimage *image = kbuf->image;
+ 	unsigned long temp_start, temp_end;
+ 
+ 	temp_end = min(end, kbuf->buf_max);
+ 	temp_start = temp_end - kbuf->memsz;
+ 
+ 	do {
+ 		/* align down start */
+ 		temp_start = temp_start & (~(kbuf->buf_align - 1));
+ 
+ 		if (temp_start < start || temp_start < kbuf->buf_min)
+ 			return 0;
+ 
+ 		temp_end = temp_start + kbuf->memsz - 1;
+ 
+ 		/*
+ 		 * Make sure this does not conflict with any of existing
+ 		 * segments
+ 		 */
+ 		if (kimage_is_destination_range(image, temp_start, temp_end)) {
+ 			temp_start = temp_start - PAGE_SIZE;
+ 			continue;
+ 		}
+ 
+ 		/* We found a suitable memory range */
+ 		break;
+ 	} while (1);
+ 
+ 	/* If we are here, we found a suitable memory range */
+ 	__kexec_add_segment(image, kbuf->buffer, kbuf->bufsz, temp_start,
+ 			    kbuf->memsz);
+ 
+ 	/* Success, stop navigating through remaining System RAM ranges */
+ 	return 1;
+ }
+ 
+ static int locate_mem_hole_bottom_up(unsigned long start, unsigned long end,
+ 				     struct kexec_buf *kbuf)
+ {
+ 	struct kimage *image = kbuf->image;
+ 	unsigned long temp_start, temp_end;
+ 
+ 	temp_start = max(start, kbuf->buf_min);
+ 
+ 	do {
+ 		temp_start = ALIGN(temp_start, kbuf->buf_align);
+ 		temp_end = temp_start + kbuf->memsz - 1;
+ 
+ 		if (temp_end > end || temp_end > kbuf->buf_max)
+ 			return 0;
+ 		/*
+ 		 * Make sure this does not conflict with any of existing
+ 		 * segments
+ 		 */
+ 		if (kimage_is_destination_range(image, temp_start, temp_end)) {
+ 			temp_start = temp_start + PAGE_SIZE;
+ 			continue;
+ 		}
+ 
+ 		/* We found a suitable memory range */
+ 		break;
+ 	} while (1);
+ 
+ 	/* If we are here, we found a suitable memory range */
+ 	__kexec_add_segment(image, kbuf->buffer, kbuf->bufsz, temp_start,
+ 			    kbuf->memsz);
+ 
+ 	/* Success, stop navigating through remaining System RAM ranges */
+ 	return 1;
+ }
+ 
+ static int locate_mem_hole_callback(u64 start, u64 end, void *arg)
+ {
+ 	struct kexec_buf *kbuf = (struct kexec_buf *)arg;
+ 	unsigned long sz = end - start + 1;
+ 
+ 	/* Returning 0 will take to next memory range */
+ 	if (sz < kbuf->memsz)
+ 		return 0;
+ 
+ 	if (end < kbuf->buf_min || start > kbuf->buf_max)
+ 		return 0;
+ 
+ 	/*
+ 	 * Allocate memory top down with-in ram range. Otherwise bottom up
+ 	 * allocation.
+ 	 */
+ 	if (kbuf->top_down)
+ 		return locate_mem_hole_top_down(start, end, kbuf);
+ 	return locate_mem_hole_bottom_up(start, end, kbuf);
+ }
+ 
+ /*
+  * Helper function for placing a buffer in a kexec segment. This assumes
+  * that kexec_mutex is held.
+  */
+ int kexec_add_buffer(struct kimage *image, char *buffer, unsigned long bufsz,
+ 		     unsigned long memsz, unsigned long buf_align,
+ 		     unsigned long buf_min, unsigned long buf_max,
+ 		     bool top_down, unsigned long *load_addr)
+ {
+ 
+ 	struct kexec_segment *ksegment;
+ 	struct kexec_buf buf, *kbuf;
+ 	int ret;
+ 
+ 	/* Currently adding segment this way is allowed only in file mode */
+ 	if (!image->file_mode)
+ 		return -EINVAL;
+ 
+ 	if (image->nr_segments >= KEXEC_SEGMENT_MAX)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Make sure we are not trying to add buffer after allocating
+ 	 * control pages. All segments need to be placed first before
+ 	 * any control pages are allocated. As control page allocation
+ 	 * logic goes through list of segments to make sure there are
+ 	 * no destination overlaps.
+ 	 */
+ 	if (!list_empty(&image->control_pages)) {
+ 		WARN_ON(1);
+ 		return -EINVAL;
+ 	}
+ 
+ 	memset(&buf, 0, sizeof(struct kexec_buf));
+ 	kbuf = &buf;
+ 	kbuf->image = image;
+ 	kbuf->buffer = buffer;
+ 	kbuf->bufsz = bufsz;
+ 
+ 	kbuf->memsz = ALIGN(memsz, PAGE_SIZE);
+ 	kbuf->buf_align = max(buf_align, PAGE_SIZE);
+ 	kbuf->buf_min = buf_min;
+ 	kbuf->buf_max = buf_max;
+ 	kbuf->top_down = top_down;
+ 
+ 	/* Walk the RAM ranges and allocate a suitable range for the buffer */
+ 	if (image->type == KEXEC_TYPE_CRASH)
+ 		ret = walk_iomem_res("Crash kernel",
+ 				     IORESOURCE_MEM | IORESOURCE_BUSY,
+ 				     crashk_res.start, crashk_res.end, kbuf,
+ 				     locate_mem_hole_callback);
+ 	else
+ 		ret = walk_system_ram_res(0, -1, kbuf,
+ 					  locate_mem_hole_callback);
+ 	if (ret != 1) {
+ 		/* A suitable memory range could not be found for buffer */
+ 		return -EADDRNOTAVAIL;
+ 	}
+ 
+ 	/* Found a suitable memory range */
+ 	ksegment = &image->segment[image->nr_segments - 1];
+ 	*load_addr = ksegment->mem;
+ 	return 0;
+ }
+ 
+ /* Calculate and store the digest of segments */
+ static int kexec_calculate_store_digests(struct kimage *image)
+ {
+ 	struct crypto_shash *tfm;
+ 	struct shash_desc *desc;
+ 	int ret = 0, i, j, zero_buf_sz, sha_region_sz;
+ 	size_t desc_size, nullsz;
+ 	char *digest;
+ 	void *zero_buf;
+ 	struct kexec_sha_region *sha_regions;
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 
+ 	zero_buf = __va(page_to_pfn(ZERO_PAGE(0)) << PAGE_SHIFT);
+ 	zero_buf_sz = PAGE_SIZE;
+ 
+ 	tfm = crypto_alloc_shash("sha256", 0, 0);
+ 	if (IS_ERR(tfm)) {
+ 		ret = PTR_ERR(tfm);
+ 		goto out;
+ 	}
+ 
+ 	desc_size = crypto_shash_descsize(tfm) + sizeof(*desc);
+ 	desc = kzalloc(desc_size, GFP_KERNEL);
+ 	if (!desc) {
+ 		ret = -ENOMEM;
+ 		goto out_free_tfm;
+ 	}
+ 
+ 	sha_region_sz = KEXEC_SEGMENT_MAX * sizeof(struct kexec_sha_region);
+ 	sha_regions = vzalloc(sha_region_sz);
+ 	if (!sha_regions)
+ 		goto out_free_desc;
+ 
+ 	desc->tfm   = tfm;
+ 	desc->flags = 0;
+ 
+ 	ret = crypto_shash_init(desc);
+ 	if (ret < 0)
+ 		goto out_free_sha_regions;
+ 
+ 	digest = kzalloc(SHA256_DIGEST_SIZE, GFP_KERNEL);
+ 	if (!digest) {
+ 		ret = -ENOMEM;
+ 		goto out_free_sha_regions;
+ 	}
+ 
+ 	for (j = i = 0; i < image->nr_segments; i++) {
+ 		struct kexec_segment *ksegment;
+ 
+ 		ksegment = &image->segment[i];
+ 		/*
+ 		 * Skip purgatory as it will be modified once we put digest
+ 		 * info in purgatory.
+ 		 */
+ 		if (ksegment->kbuf == pi->purgatory_buf)
+ 			continue;
+ 
+ 		ret = crypto_shash_update(desc, ksegment->kbuf,
+ 					  ksegment->bufsz);
+ 		if (ret)
+ 			break;
+ 
+ 		/*
+ 		 * Assume rest of the buffer is filled with zero and
+ 		 * update digest accordingly.
+ 		 */
+ 		nullsz = ksegment->memsz - ksegment->bufsz;
+ 		while (nullsz) {
+ 			unsigned long bytes = nullsz;
+ 
+ 			if (bytes > zero_buf_sz)
+ 				bytes = zero_buf_sz;
+ 			ret = crypto_shash_update(desc, zero_buf, bytes);
+ 			if (ret)
+ 				break;
+ 			nullsz -= bytes;
+ 		}
+ 
+ 		if (ret)
+ 			break;
+ 
+ 		sha_regions[j].start = ksegment->mem;
+ 		sha_regions[j].len = ksegment->memsz;
+ 		j++;
+ 	}
+ 
+ 	if (!ret) {
+ 		ret = crypto_shash_final(desc, digest);
+ 		if (ret)
+ 			goto out_free_digest;
+ 		ret = kexec_purgatory_get_set_symbol(image, "sha_regions",
+ 						sha_regions, sha_region_sz, 0);
+ 		if (ret)
+ 			goto out_free_digest;
+ 
+ 		ret = kexec_purgatory_get_set_symbol(image, "sha256_digest",
+ 						digest, SHA256_DIGEST_SIZE, 0);
+ 		if (ret)
+ 			goto out_free_digest;
+ 	}
+ 
+ out_free_digest:
+ 	kfree(digest);
+ out_free_sha_regions:
+ 	vfree(sha_regions);
+ out_free_desc:
+ 	kfree(desc);
+ out_free_tfm:
+ 	kfree(tfm);
+ out:
+ 	return ret;
+ }
+ 
+ /* Actually load purgatory. Lot of code taken from kexec-tools */
+ static int __kexec_load_purgatory(struct kimage *image, unsigned long min,
+ 				  unsigned long max, int top_down)
+ {
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 	unsigned long align, buf_align, bss_align, buf_sz, bss_sz, bss_pad;
+ 	unsigned long memsz, entry, load_addr, curr_load_addr, bss_addr, offset;
+ 	unsigned char *buf_addr, *src;
+ 	int i, ret = 0, entry_sidx = -1;
+ 	const Elf_Shdr *sechdrs_c;
+ 	Elf_Shdr *sechdrs = NULL;
+ 	void *purgatory_buf = NULL;
+ 
+ 	/*
+ 	 * sechdrs_c points to section headers in purgatory and are read
+ 	 * only. No modifications allowed.
+ 	 */
+ 	sechdrs_c = (void *)pi->ehdr + pi->ehdr->e_shoff;
+ 
+ 	/*
+ 	 * We can not modify sechdrs_c[] and its fields. It is read only.
+ 	 * Copy it over to a local copy where one can store some temporary
+ 	 * data and free it at the end. We need to modify ->sh_addr and
+ 	 * ->sh_offset fields to keep track of permanent and temporary
+ 	 * locations of sections.
+ 	 */
+ 	sechdrs = vzalloc(pi->ehdr->e_shnum * sizeof(Elf_Shdr));
+ 	if (!sechdrs)
+ 		return -ENOMEM;
+ 
+ 	memcpy(sechdrs, sechdrs_c, pi->ehdr->e_shnum * sizeof(Elf_Shdr));
+ 
+ 	/*
+ 	 * We seem to have multiple copies of sections. First copy is which
+ 	 * is embedded in kernel in read only section. Some of these sections
+ 	 * will be copied to a temporary buffer and relocated. And these
+ 	 * sections will finally be copied to their final destination at
+ 	 * segment load time.
+ 	 *
+ 	 * Use ->sh_offset to reflect section address in memory. It will
+ 	 * point to original read only copy if section is not allocatable.
+ 	 * Otherwise it will point to temporary copy which will be relocated.
+ 	 *
+ 	 * Use ->sh_addr to contain final address of the section where it
+ 	 * will go during execution time.
+ 	 */
+ 	for (i = 0; i < pi->ehdr->e_shnum; i++) {
+ 		if (sechdrs[i].sh_type == SHT_NOBITS)
+ 			continue;
+ 
+ 		sechdrs[i].sh_offset = (unsigned long)pi->ehdr +
+ 						sechdrs[i].sh_offset;
+ 	}
+ 
+ 	/*
+ 	 * Identify entry point section and make entry relative to section
+ 	 * start.
+ 	 */
+ 	entry = pi->ehdr->e_entry;
+ 	for (i = 0; i < pi->ehdr->e_shnum; i++) {
+ 		if (!(sechdrs[i].sh_flags & SHF_ALLOC))
+ 			continue;
+ 
+ 		if (!(sechdrs[i].sh_flags & SHF_EXECINSTR))
+ 			continue;
+ 
+ 		/* Make entry section relative */
+ 		if (sechdrs[i].sh_addr <= pi->ehdr->e_entry &&
+ 		    ((sechdrs[i].sh_addr + sechdrs[i].sh_size) >
+ 		     pi->ehdr->e_entry)) {
+ 			entry_sidx = i;
+ 			entry -= sechdrs[i].sh_addr;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* Determine how much memory is needed to load relocatable object. */
+ 	buf_align = 1;
+ 	bss_align = 1;
+ 	buf_sz = 0;
+ 	bss_sz = 0;
+ 
+ 	for (i = 0; i < pi->ehdr->e_shnum; i++) {
+ 		if (!(sechdrs[i].sh_flags & SHF_ALLOC))
+ 			continue;
+ 
+ 		align = sechdrs[i].sh_addralign;
+ 		if (sechdrs[i].sh_type != SHT_NOBITS) {
+ 			if (buf_align < align)
+ 				buf_align = align;
+ 			buf_sz = ALIGN(buf_sz, align);
+ 			buf_sz += sechdrs[i].sh_size;
+ 		} else {
+ 			/* bss section */
+ 			if (bss_align < align)
+ 				bss_align = align;
+ 			bss_sz = ALIGN(bss_sz, align);
+ 			bss_sz += sechdrs[i].sh_size;
+ 		}
+ 	}
+ 
+ 	/* Determine the bss padding required to align bss properly */
+ 	bss_pad = 0;
+ 	if (buf_sz & (bss_align - 1))
+ 		bss_pad = bss_align - (buf_sz & (bss_align - 1));
+ 
+ 	memsz = buf_sz + bss_pad + bss_sz;
+ 
+ 	/* Allocate buffer for purgatory */
+ 	purgatory_buf = vzalloc(buf_sz);
+ 	if (!purgatory_buf) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	if (buf_align < bss_align)
+ 		buf_align = bss_align;
+ 
+ 	/* Add buffer to segment list */
+ 	ret = kexec_add_buffer(image, purgatory_buf, buf_sz, memsz,
+ 				buf_align, min, max, top_down,
+ 				&pi->purgatory_load_addr);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* Load SHF_ALLOC sections */
+ 	buf_addr = purgatory_buf;
+ 	load_addr = curr_load_addr = pi->purgatory_load_addr;
+ 	bss_addr = load_addr + buf_sz + bss_pad;
+ 
+ 	for (i = 0; i < pi->ehdr->e_shnum; i++) {
+ 		if (!(sechdrs[i].sh_flags & SHF_ALLOC))
+ 			continue;
+ 
+ 		align = sechdrs[i].sh_addralign;
+ 		if (sechdrs[i].sh_type != SHT_NOBITS) {
+ 			curr_load_addr = ALIGN(curr_load_addr, align);
+ 			offset = curr_load_addr - load_addr;
+ 			/* We already modifed ->sh_offset to keep src addr */
+ 			src = (char *) sechdrs[i].sh_offset;
+ 			memcpy(buf_addr + offset, src, sechdrs[i].sh_size);
+ 
+ 			/* Store load address and source address of section */
+ 			sechdrs[i].sh_addr = curr_load_addr;
+ 
+ 			/*
+ 			 * This section got copied to temporary buffer. Update
+ 			 * ->sh_offset accordingly.
+ 			 */
+ 			sechdrs[i].sh_offset = (unsigned long)(buf_addr + offset);
+ 
+ 			/* Advance to the next address */
+ 			curr_load_addr += sechdrs[i].sh_size;
+ 		} else {
+ 			bss_addr = ALIGN(bss_addr, align);
+ 			sechdrs[i].sh_addr = bss_addr;
+ 			bss_addr += sechdrs[i].sh_size;
+ 		}
+ 	}
+ 
+ 	/* Update entry point based on load address of text section */
+ 	if (entry_sidx >= 0)
+ 		entry += sechdrs[entry_sidx].sh_addr;
+ 
+ 	/* Make kernel jump to purgatory after shutdown */
+ 	image->start = entry;
+ 
+ 	/* Used later to get/set symbol values */
+ 	pi->sechdrs = sechdrs;
+ 
+ 	/*
+ 	 * Used later to identify which section is purgatory and skip it
+ 	 * from checksumming.
+ 	 */
+ 	pi->purgatory_buf = purgatory_buf;
+ 	return ret;
+ out:
+ 	vfree(sechdrs);
+ 	vfree(purgatory_buf);
+ 	return ret;
+ }
+ 
+ static int kexec_apply_relocations(struct kimage *image)
+ {
+ 	int i, ret;
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 	Elf_Shdr *sechdrs = pi->sechdrs;
+ 
+ 	/* Apply relocations */
+ 	for (i = 0; i < pi->ehdr->e_shnum; i++) {
+ 		Elf_Shdr *section, *symtab;
+ 
+ 		if (sechdrs[i].sh_type != SHT_RELA &&
+ 		    sechdrs[i].sh_type != SHT_REL)
+ 			continue;
+ 
+ 		/*
+ 		 * For section of type SHT_RELA/SHT_REL,
+ 		 * ->sh_link contains section header index of associated
+ 		 * symbol table. And ->sh_info contains section header
+ 		 * index of section to which relocations apply.
+ 		 */
+ 		if (sechdrs[i].sh_info >= pi->ehdr->e_shnum ||
+ 		    sechdrs[i].sh_link >= pi->ehdr->e_shnum)
+ 			return -ENOEXEC;
+ 
+ 		section = &sechdrs[sechdrs[i].sh_info];
+ 		symtab = &sechdrs[sechdrs[i].sh_link];
+ 
+ 		if (!(section->sh_flags & SHF_ALLOC))
+ 			continue;
+ 
+ 		/*
+ 		 * symtab->sh_link contain section header index of associated
+ 		 * string table.
+ 		 */
+ 		if (symtab->sh_link >= pi->ehdr->e_shnum)
+ 			/* Invalid section number? */
+ 			continue;
+ 
+ 		/*
+ 		 * Respective archicture needs to provide support for applying
+ 		 * relocations of type SHT_RELA/SHT_REL.
+ 		 */
+ 		if (sechdrs[i].sh_type == SHT_RELA)
+ 			ret = arch_kexec_apply_relocations_add(pi->ehdr,
+ 							       sechdrs, i);
+ 		else if (sechdrs[i].sh_type == SHT_REL)
+ 			ret = arch_kexec_apply_relocations(pi->ehdr,
+ 							   sechdrs, i);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /* Load relocatable purgatory object and relocate it appropriately */
+ int kexec_load_purgatory(struct kimage *image, unsigned long min,
+ 			 unsigned long max, int top_down,
+ 			 unsigned long *load_addr)
+ {
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 	int ret;
+ 
+ 	if (kexec_purgatory_size <= 0)
+ 		return -EINVAL;
+ 
+ 	if (kexec_purgatory_size < sizeof(Elf_Ehdr))
+ 		return -ENOEXEC;
+ 
+ 	pi->ehdr = (Elf_Ehdr *)kexec_purgatory;
+ 
+ 	if (memcmp(pi->ehdr->e_ident, ELFMAG, SELFMAG) != 0
+ 	    || pi->ehdr->e_type != ET_REL
+ 	    || !elf_check_arch(pi->ehdr)
+ 	    || pi->ehdr->e_shentsize != sizeof(Elf_Shdr))
+ 		return -ENOEXEC;
+ 
+ 	if (pi->ehdr->e_shoff >= kexec_purgatory_size
+ 	    || (pi->ehdr->e_shnum * sizeof(Elf_Shdr) >
+ 	    kexec_purgatory_size - pi->ehdr->e_shoff))
+ 		return -ENOEXEC;
+ 
+ 	ret = __kexec_load_purgatory(image, min, max, top_down);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = kexec_apply_relocations(image);
+ 	if (ret)
+ 		goto out;
+ 
+ 	*load_addr = pi->purgatory_load_addr;
+ 	return 0;
+ out:
+ 	vfree(pi->sechdrs);
+ 	vfree(pi->purgatory_buf);
+ 	return ret;
+ }
+ 
+ static Elf_Sym *kexec_purgatory_find_symbol(struct purgatory_info *pi,
+ 					    const char *name)
+ {
+ 	Elf_Sym *syms;
+ 	Elf_Shdr *sechdrs;
+ 	Elf_Ehdr *ehdr;
+ 	int i, k;
+ 	const char *strtab;
+ 
+ 	if (!pi->sechdrs || !pi->ehdr)
+ 		return NULL;
+ 
+ 	sechdrs = pi->sechdrs;
+ 	ehdr = pi->ehdr;
+ 
+ 	for (i = 0; i < ehdr->e_shnum; i++) {
+ 		if (sechdrs[i].sh_type != SHT_SYMTAB)
+ 			continue;
+ 
+ 		if (sechdrs[i].sh_link >= ehdr->e_shnum)
+ 			/* Invalid strtab section number */
+ 			continue;
+ 		strtab = (char *)sechdrs[sechdrs[i].sh_link].sh_offset;
+ 		syms = (Elf_Sym *)sechdrs[i].sh_offset;
+ 
+ 		/* Go through symbols for a match */
+ 		for (k = 0; k < sechdrs[i].sh_size/sizeof(Elf_Sym); k++) {
+ 			if (ELF_ST_BIND(syms[k].st_info) != STB_GLOBAL)
+ 				continue;
+ 
+ 			if (strcmp(strtab + syms[k].st_name, name) != 0)
+ 				continue;
+ 
+ 			if (syms[k].st_shndx == SHN_UNDEF ||
+ 			    syms[k].st_shndx >= ehdr->e_shnum) {
+ 				pr_debug("Symbol: %s has bad section index %d.\n",
+ 						name, syms[k].st_shndx);
+ 				return NULL;
+ 			}
+ 
+ 			/* Found the symbol we are looking for */
+ 			return &syms[k];
+ 		}
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ void *kexec_purgatory_get_symbol_addr(struct kimage *image, const char *name)
+ {
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 	Elf_Sym *sym;
+ 	Elf_Shdr *sechdr;
+ 
+ 	sym = kexec_purgatory_find_symbol(pi, name);
+ 	if (!sym)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	sechdr = &pi->sechdrs[sym->st_shndx];
+ 
+ 	/*
+ 	 * Returns the address where symbol will finally be loaded after
+ 	 * kexec_load_segment()
+ 	 */
+ 	return (void *)(sechdr->sh_addr + sym->st_value);
+ }
+ 
+ /*
+  * Get or set value of a symbol. If "get_value" is true, symbol value is
+  * returned in buf otherwise symbol value is set based on value in buf.
+  */
+ int kexec_purgatory_get_set_symbol(struct kimage *image, const char *name,
+ 				   void *buf, unsigned int size, bool get_value)
+ {
+ 	Elf_Sym *sym;
+ 	Elf_Shdr *sechdrs;
+ 	struct purgatory_info *pi = &image->purgatory_info;
+ 	char *sym_buf;
+ 
+ 	sym = kexec_purgatory_find_symbol(pi, name);
+ 	if (!sym)
+ 		return -EINVAL;
+ 
+ 	if (sym->st_size != size) {
+ 		pr_err("symbol %s size mismatch: expected %lu actual %u\n",
+ 		       name, (unsigned long)sym->st_size, size);
+ 		return -EINVAL;
+ 	}
+ 
+ 	sechdrs = pi->sechdrs;
+ 
+ 	if (sechdrs[sym->st_shndx].sh_type == SHT_NOBITS) {
+ 		pr_err("symbol %s is in a bss section. Cannot %s\n", name,
+ 		       get_value ? "get" : "set");
+ 		return -EINVAL;
+ 	}
+ 
+ 	sym_buf = (unsigned char *)sechdrs[sym->st_shndx].sh_offset +
+ 					sym->st_value;
+ 
+ 	if (get_value)
+ 		memcpy((void *)buf, sym_buf, size);
+ 	else
+ 		memcpy((void *)sym_buf, buf, size);
+ 
+ 	return 0;
+ }
++>>>>>>> dd5f726076cc (kexec: support for kexec on panic using new system call)
  
  /*
   * Move into place and start executing a preloaded standalone
* Unmerged path arch/x86/kernel/kexec-bzimage64.c
* Unmerged path arch/x86/include/asm/crash.h
* Unmerged path arch/x86/include/asm/kexec.h
diff --git a/arch/x86/kernel/crash.c b/arch/x86/kernel/crash.c
index ad97a8beb40b..352be68fa426 100644
--- a/arch/x86/kernel/crash.c
+++ b/arch/x86/kernel/crash.c
@@ -4,9 +4,14 @@
  * Created by: Hariprasad Nellitheertha (hari@in.ibm.com)
  *
  * Copyright (C) IBM Corporation, 2004. All rights reserved.
+ * Copyright (C) Red Hat Inc., 2014. All rights reserved.
+ * Authors:
+ *      Vivek Goyal <vgoyal@redhat.com>
  *
  */
 
+#define pr_fmt(fmt)	"kexec: " fmt
+
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/smp.h>
@@ -16,6 +21,7 @@
 #include <linux/elf.h>
 #include <linux/elfcore.h>
 #include <linux/module.h>
+#include <linux/slab.h>
 
 #include <asm/processor.h>
 #include <asm/hardirq.h>
@@ -28,6 +34,45 @@
 #include <asm/reboot.h>
 #include <asm/virtext.h>
 
+/* Alignment required for elf header segment */
+#define ELF_CORE_HEADER_ALIGN   4096
+
+/* This primarily represents number of split ranges due to exclusion */
+#define CRASH_MAX_RANGES	16
+
+struct crash_mem_range {
+	u64 start, end;
+};
+
+struct crash_mem {
+	unsigned int nr_ranges;
+	struct crash_mem_range ranges[CRASH_MAX_RANGES];
+};
+
+/* Misc data about ram ranges needed to prepare elf headers */
+struct crash_elf_data {
+	struct kimage *image;
+	/*
+	 * Total number of ram ranges we have after various adjustments for
+	 * GART, crash reserved region etc.
+	 */
+	unsigned int max_nr_ranges;
+	unsigned long gart_start, gart_end;
+
+	/* Pointer to elf header */
+	void *ehdr;
+	/* Pointer to next phdr */
+	void *bufp;
+	struct crash_mem mem;
+};
+
+/* Used while preparing memory map entries for second kernel */
+struct crash_memmap_data {
+	struct boot_params *params;
+	/* Type of memory */
+	unsigned int type;
+};
+
 int in_crash_kexec;
 
 /*
@@ -39,6 +84,7 @@ int in_crash_kexec;
  */
 crash_vmclear_fn __rcu *crash_vmclear_loaded_vmcss = NULL;
 EXPORT_SYMBOL_GPL(crash_vmclear_loaded_vmcss);
+unsigned long crash_zero_bytes;
 
 static inline void cpu_crash_vmclear_loaded_vmcss(void)
 {
@@ -135,3 +181,520 @@ void native_machine_crash_shutdown(struct pt_regs *regs)
 #endif
 	crash_save_cpu(regs, safe_smp_processor_id());
 }
+
+#ifdef CONFIG_X86_64
+
+static int get_nr_ram_ranges_callback(unsigned long start_pfn,
+				unsigned long nr_pfn, void *arg)
+{
+	int *nr_ranges = arg;
+
+	(*nr_ranges)++;
+	return 0;
+}
+
+static int get_gart_ranges_callback(u64 start, u64 end, void *arg)
+{
+	struct crash_elf_data *ced = arg;
+
+	ced->gart_start = start;
+	ced->gart_end = end;
+
+	/* Not expecting more than 1 gart aperture */
+	return 1;
+}
+
+
+/* Gather all the required information to prepare elf headers for ram regions */
+static void fill_up_crash_elf_data(struct crash_elf_data *ced,
+				   struct kimage *image)
+{
+	unsigned int nr_ranges = 0;
+
+	ced->image = image;
+
+	walk_system_ram_range(0, -1, &nr_ranges,
+				get_nr_ram_ranges_callback);
+
+	ced->max_nr_ranges = nr_ranges;
+
+	/*
+	 * We don't create ELF headers for GART aperture as an attempt
+	 * to dump this memory in second kernel leads to hang/crash.
+	 * If gart aperture is present, one needs to exclude that region
+	 * and that could lead to need of extra phdr.
+	 */
+	walk_iomem_res("GART", IORESOURCE_MEM, 0, -1,
+				ced, get_gart_ranges_callback);
+
+	/*
+	 * If we have gart region, excluding that could potentially split
+	 * a memory range, resulting in extra header. Account for  that.
+	 */
+	if (ced->gart_end)
+		ced->max_nr_ranges++;
+
+	/* Exclusion of crash region could split memory ranges */
+	ced->max_nr_ranges++;
+
+	/* If crashk_low_res is not 0, another range split possible */
+	if (crashk_low_res.end != 0)
+		ced->max_nr_ranges++;
+}
+
+static int exclude_mem_range(struct crash_mem *mem,
+		unsigned long long mstart, unsigned long long mend)
+{
+	int i, j;
+	unsigned long long start, end;
+	struct crash_mem_range temp_range = {0, 0};
+
+	for (i = 0; i < mem->nr_ranges; i++) {
+		start = mem->ranges[i].start;
+		end = mem->ranges[i].end;
+
+		if (mstart > end || mend < start)
+			continue;
+
+		/* Truncate any area outside of range */
+		if (mstart < start)
+			mstart = start;
+		if (mend > end)
+			mend = end;
+
+		/* Found completely overlapping range */
+		if (mstart == start && mend == end) {
+			mem->ranges[i].start = 0;
+			mem->ranges[i].end = 0;
+			if (i < mem->nr_ranges - 1) {
+				/* Shift rest of the ranges to left */
+				for (j = i; j < mem->nr_ranges - 1; j++) {
+					mem->ranges[j].start =
+						mem->ranges[j+1].start;
+					mem->ranges[j].end =
+							mem->ranges[j+1].end;
+				}
+			}
+			mem->nr_ranges--;
+			return 0;
+		}
+
+		if (mstart > start && mend < end) {
+			/* Split original range */
+			mem->ranges[i].end = mstart - 1;
+			temp_range.start = mend + 1;
+			temp_range.end = end;
+		} else if (mstart != start)
+			mem->ranges[i].end = mstart - 1;
+		else
+			mem->ranges[i].start = mend + 1;
+		break;
+	}
+
+	/* If a split happend, add the split to array */
+	if (!temp_range.end)
+		return 0;
+
+	/* Split happened */
+	if (i == CRASH_MAX_RANGES - 1) {
+		pr_err("Too many crash ranges after split\n");
+		return -ENOMEM;
+	}
+
+	/* Location where new range should go */
+	j = i + 1;
+	if (j < mem->nr_ranges) {
+		/* Move over all ranges one slot towards the end */
+		for (i = mem->nr_ranges - 1; i >= j; i--)
+			mem->ranges[i + 1] = mem->ranges[i];
+	}
+
+	mem->ranges[j].start = temp_range.start;
+	mem->ranges[j].end = temp_range.end;
+	mem->nr_ranges++;
+	return 0;
+}
+
+/*
+ * Look for any unwanted ranges between mstart, mend and remove them. This
+ * might lead to split and split ranges are put in ced->mem.ranges[] array
+ */
+static int elf_header_exclude_ranges(struct crash_elf_data *ced,
+		unsigned long long mstart, unsigned long long mend)
+{
+	struct crash_mem *cmem = &ced->mem;
+	int ret = 0;
+
+	memset(cmem->ranges, 0, sizeof(cmem->ranges));
+
+	cmem->ranges[0].start = mstart;
+	cmem->ranges[0].end = mend;
+	cmem->nr_ranges = 1;
+
+	/* Exclude crashkernel region */
+	ret = exclude_mem_range(cmem, crashk_res.start, crashk_res.end);
+	if (ret)
+		return ret;
+
+	ret = exclude_mem_range(cmem, crashk_low_res.start, crashk_low_res.end);
+	if (ret)
+		return ret;
+
+	/* Exclude GART region */
+	if (ced->gart_end) {
+		ret = exclude_mem_range(cmem, ced->gart_start, ced->gart_end);
+		if (ret)
+			return ret;
+	}
+
+	return ret;
+}
+
+static int prepare_elf64_ram_headers_callback(u64 start, u64 end, void *arg)
+{
+	struct crash_elf_data *ced = arg;
+	Elf64_Ehdr *ehdr;
+	Elf64_Phdr *phdr;
+	unsigned long mstart, mend;
+	struct kimage *image = ced->image;
+	struct crash_mem *cmem;
+	int ret, i;
+
+	ehdr = ced->ehdr;
+
+	/* Exclude unwanted mem ranges */
+	ret = elf_header_exclude_ranges(ced, start, end);
+	if (ret)
+		return ret;
+
+	/* Go through all the ranges in ced->mem.ranges[] and prepare phdr */
+	cmem = &ced->mem;
+
+	for (i = 0; i < cmem->nr_ranges; i++) {
+		mstart = cmem->ranges[i].start;
+		mend = cmem->ranges[i].end;
+
+		phdr = ced->bufp;
+		ced->bufp += sizeof(Elf64_Phdr);
+
+		phdr->p_type = PT_LOAD;
+		phdr->p_flags = PF_R|PF_W|PF_X;
+		phdr->p_offset  = mstart;
+
+		/*
+		 * If a range matches backup region, adjust offset to backup
+		 * segment.
+		 */
+		if (mstart == image->arch.backup_src_start &&
+		    (mend - mstart + 1) == image->arch.backup_src_sz)
+			phdr->p_offset = image->arch.backup_load_addr;
+
+		phdr->p_paddr = mstart;
+		phdr->p_vaddr = (unsigned long long) __va(mstart);
+		phdr->p_filesz = phdr->p_memsz = mend - mstart + 1;
+		phdr->p_align = 0;
+		ehdr->e_phnum++;
+		pr_debug("Crash PT_LOAD elf header. phdr=%p vaddr=0x%llx, paddr=0x%llx, sz=0x%llx e_phnum=%d p_offset=0x%llx\n",
+			phdr, phdr->p_vaddr, phdr->p_paddr, phdr->p_filesz,
+			ehdr->e_phnum, phdr->p_offset);
+	}
+
+	return ret;
+}
+
+static int prepare_elf64_headers(struct crash_elf_data *ced,
+		void **addr, unsigned long *sz)
+{
+	Elf64_Ehdr *ehdr;
+	Elf64_Phdr *phdr;
+	unsigned long nr_cpus = num_possible_cpus(), nr_phdr, elf_sz;
+	unsigned char *buf, *bufp;
+	unsigned int cpu;
+	unsigned long long notes_addr;
+	int ret;
+
+	/* extra phdr for vmcoreinfo elf note */
+	nr_phdr = nr_cpus + 1;
+	nr_phdr += ced->max_nr_ranges;
+
+	/*
+	 * kexec-tools creates an extra PT_LOAD phdr for kernel text mapping
+	 * area on x86_64 (ffffffff80000000 - ffffffffa0000000).
+	 * I think this is required by tools like gdb. So same physical
+	 * memory will be mapped in two elf headers. One will contain kernel
+	 * text virtual addresses and other will have __va(physical) addresses.
+	 */
+
+	nr_phdr++;
+	elf_sz = sizeof(Elf64_Ehdr) + nr_phdr * sizeof(Elf64_Phdr);
+	elf_sz = ALIGN(elf_sz, ELF_CORE_HEADER_ALIGN);
+
+	buf = vzalloc(elf_sz);
+	if (!buf)
+		return -ENOMEM;
+
+	bufp = buf;
+	ehdr = (Elf64_Ehdr *)bufp;
+	bufp += sizeof(Elf64_Ehdr);
+	memcpy(ehdr->e_ident, ELFMAG, SELFMAG);
+	ehdr->e_ident[EI_CLASS] = ELFCLASS64;
+	ehdr->e_ident[EI_DATA] = ELFDATA2LSB;
+	ehdr->e_ident[EI_VERSION] = EV_CURRENT;
+	ehdr->e_ident[EI_OSABI] = ELF_OSABI;
+	memset(ehdr->e_ident + EI_PAD, 0, EI_NIDENT - EI_PAD);
+	ehdr->e_type = ET_CORE;
+	ehdr->e_machine = ELF_ARCH;
+	ehdr->e_version = EV_CURRENT;
+	ehdr->e_phoff = sizeof(Elf64_Ehdr);
+	ehdr->e_ehsize = sizeof(Elf64_Ehdr);
+	ehdr->e_phentsize = sizeof(Elf64_Phdr);
+
+	/* Prepare one phdr of type PT_NOTE for each present cpu */
+	for_each_present_cpu(cpu) {
+		phdr = (Elf64_Phdr *)bufp;
+		bufp += sizeof(Elf64_Phdr);
+		phdr->p_type = PT_NOTE;
+		notes_addr = per_cpu_ptr_to_phys(per_cpu_ptr(crash_notes, cpu));
+		phdr->p_offset = phdr->p_paddr = notes_addr;
+		phdr->p_filesz = phdr->p_memsz = sizeof(note_buf_t);
+		(ehdr->e_phnum)++;
+	}
+
+	/* Prepare one PT_NOTE header for vmcoreinfo */
+	phdr = (Elf64_Phdr *)bufp;
+	bufp += sizeof(Elf64_Phdr);
+	phdr->p_type = PT_NOTE;
+	phdr->p_offset = phdr->p_paddr = paddr_vmcoreinfo_note();
+	phdr->p_filesz = phdr->p_memsz = sizeof(vmcoreinfo_note);
+	(ehdr->e_phnum)++;
+
+#ifdef CONFIG_X86_64
+	/* Prepare PT_LOAD type program header for kernel text region */
+	phdr = (Elf64_Phdr *)bufp;
+	bufp += sizeof(Elf64_Phdr);
+	phdr->p_type = PT_LOAD;
+	phdr->p_flags = PF_R|PF_W|PF_X;
+	phdr->p_vaddr = (Elf64_Addr)_text;
+	phdr->p_filesz = phdr->p_memsz = _end - _text;
+	phdr->p_offset = phdr->p_paddr = __pa_symbol(_text);
+	(ehdr->e_phnum)++;
+#endif
+
+	/* Prepare PT_LOAD headers for system ram chunks. */
+	ced->ehdr = ehdr;
+	ced->bufp = bufp;
+	ret = walk_system_ram_res(0, -1, ced,
+			prepare_elf64_ram_headers_callback);
+	if (ret < 0)
+		return ret;
+
+	*addr = buf;
+	*sz = elf_sz;
+	return 0;
+}
+
+/* Prepare elf headers. Return addr and size */
+static int prepare_elf_headers(struct kimage *image, void **addr,
+					unsigned long *sz)
+{
+	struct crash_elf_data *ced;
+	int ret;
+
+	ced = kzalloc(sizeof(*ced), GFP_KERNEL);
+	if (!ced)
+		return -ENOMEM;
+
+	fill_up_crash_elf_data(ced, image);
+
+	/* By default prepare 64bit headers */
+	ret =  prepare_elf64_headers(ced, addr, sz);
+	kfree(ced);
+	return ret;
+}
+
+static int add_e820_entry(struct boot_params *params, struct e820entry *entry)
+{
+	unsigned int nr_e820_entries;
+
+	nr_e820_entries = params->e820_entries;
+	if (nr_e820_entries >= E820MAX)
+		return 1;
+
+	memcpy(&params->e820_map[nr_e820_entries], entry,
+			sizeof(struct e820entry));
+	params->e820_entries++;
+	return 0;
+}
+
+static int memmap_entry_callback(u64 start, u64 end, void *arg)
+{
+	struct crash_memmap_data *cmd = arg;
+	struct boot_params *params = cmd->params;
+	struct e820entry ei;
+
+	ei.addr = start;
+	ei.size = end - start + 1;
+	ei.type = cmd->type;
+	add_e820_entry(params, &ei);
+
+	return 0;
+}
+
+static int memmap_exclude_ranges(struct kimage *image, struct crash_mem *cmem,
+				 unsigned long long mstart,
+				 unsigned long long mend)
+{
+	unsigned long start, end;
+	int ret = 0;
+
+	cmem->ranges[0].start = mstart;
+	cmem->ranges[0].end = mend;
+	cmem->nr_ranges = 1;
+
+	/* Exclude Backup region */
+	start = image->arch.backup_load_addr;
+	end = start + image->arch.backup_src_sz - 1;
+	ret = exclude_mem_range(cmem, start, end);
+	if (ret)
+		return ret;
+
+	/* Exclude elf header region */
+	start = image->arch.elf_load_addr;
+	end = start + image->arch.elf_headers_sz - 1;
+	return exclude_mem_range(cmem, start, end);
+}
+
+/* Prepare memory map for crash dump kernel */
+int crash_setup_memmap_entries(struct kimage *image, struct boot_params *params)
+{
+	int i, ret = 0;
+	unsigned long flags;
+	struct e820entry ei;
+	struct crash_memmap_data cmd;
+	struct crash_mem *cmem;
+
+	cmem = vzalloc(sizeof(struct crash_mem));
+	if (!cmem)
+		return -ENOMEM;
+
+	memset(&cmd, 0, sizeof(struct crash_memmap_data));
+	cmd.params = params;
+
+	/* Add first 640K segment */
+	ei.addr = image->arch.backup_src_start;
+	ei.size = image->arch.backup_src_sz;
+	ei.type = E820_RAM;
+	add_e820_entry(params, &ei);
+
+	/* Add ACPI tables */
+	cmd.type = E820_ACPI;
+	flags = IORESOURCE_MEM | IORESOURCE_BUSY;
+	walk_iomem_res("ACPI Tables", flags, 0, -1, &cmd,
+		       memmap_entry_callback);
+
+	/* Add ACPI Non-volatile Storage */
+	cmd.type = E820_NVS;
+	walk_iomem_res("ACPI Non-volatile Storage", flags, 0, -1, &cmd,
+			memmap_entry_callback);
+
+	/* Add crashk_low_res region */
+	if (crashk_low_res.end) {
+		ei.addr = crashk_low_res.start;
+		ei.size = crashk_low_res.end - crashk_low_res.start + 1;
+		ei.type = E820_RAM;
+		add_e820_entry(params, &ei);
+	}
+
+	/* Exclude some ranges from crashk_res and add rest to memmap */
+	ret = memmap_exclude_ranges(image, cmem, crashk_res.start,
+						crashk_res.end);
+	if (ret)
+		goto out;
+
+	for (i = 0; i < cmem->nr_ranges; i++) {
+		ei.size = cmem->ranges[i].end - cmem->ranges[i].start + 1;
+
+		/* If entry is less than a page, skip it */
+		if (ei.size < PAGE_SIZE)
+			continue;
+		ei.addr = cmem->ranges[i].start;
+		ei.type = E820_RAM;
+		add_e820_entry(params, &ei);
+	}
+
+out:
+	vfree(cmem);
+	return ret;
+}
+
+static int determine_backup_region(u64 start, u64 end, void *arg)
+{
+	struct kimage *image = arg;
+
+	image->arch.backup_src_start = start;
+	image->arch.backup_src_sz = end - start + 1;
+
+	/* Expecting only one range for backup region */
+	return 1;
+}
+
+int crash_load_segments(struct kimage *image)
+{
+	unsigned long src_start, src_sz, elf_sz;
+	void *elf_addr;
+	int ret;
+
+	/*
+	 * Determine and load a segment for backup area. First 640K RAM
+	 * region is backup source
+	 */
+
+	ret = walk_system_ram_res(KEXEC_BACKUP_SRC_START, KEXEC_BACKUP_SRC_END,
+				image, determine_backup_region);
+
+	/* Zero or postive return values are ok */
+	if (ret < 0)
+		return ret;
+
+	src_start = image->arch.backup_src_start;
+	src_sz = image->arch.backup_src_sz;
+
+	/* Add backup segment. */
+	if (src_sz) {
+		/*
+		 * Ideally there is no source for backup segment. This is
+		 * copied in purgatory after crash. Just add a zero filled
+		 * segment for now to make sure checksum logic works fine.
+		 */
+		ret = kexec_add_buffer(image, (char *)&crash_zero_bytes,
+				       sizeof(crash_zero_bytes), src_sz,
+				       PAGE_SIZE, 0, -1, 0,
+				       &image->arch.backup_load_addr);
+		if (ret)
+			return ret;
+		pr_debug("Loaded backup region at 0x%lx backup_start=0x%lx memsz=0x%lx\n",
+			 image->arch.backup_load_addr, src_start, src_sz);
+	}
+
+	/* Prepare elf headers and add a segment */
+	ret = prepare_elf_headers(image, &elf_addr, &elf_sz);
+	if (ret)
+		return ret;
+
+	image->arch.elf_headers = elf_addr;
+	image->arch.elf_headers_sz = elf_sz;
+
+	ret = kexec_add_buffer(image, (char *)elf_addr, elf_sz, elf_sz,
+			ELF_CORE_HEADER_ALIGN, 0, -1, 0,
+			&image->arch.elf_load_addr);
+	if (ret) {
+		vfree((void *)image->arch.elf_headers);
+		return ret;
+	}
+	pr_debug("Loaded ELF headers at 0x%lx bufsz=0x%lx memsz=0x%lx\n",
+		 image->arch.elf_load_addr, elf_sz, elf_sz);
+
+	return ret;
+}
+
+#endif /* CONFIG_X86_64 */
* Unmerged path arch/x86/kernel/kexec-bzimage64.c
* Unmerged path arch/x86/kernel/machine_kexec_64.c
diff --git a/arch/x86/purgatory/entry64.S b/arch/x86/purgatory/entry64.S
index be3249d7ed2d..d1a4291d3568 100644
--- a/arch/x86/purgatory/entry64.S
+++ b/arch/x86/purgatory/entry64.S
@@ -61,13 +61,13 @@ new_cs_exit:
 	.balign 4
 entry64_regs:
 rax:	.quad 0x0
-rbx:	.quad 0x0
 rcx:	.quad 0x0
 rdx:	.quad 0x0
-rsi:	.quad 0x0
-rdi:	.quad 0x0
+rbx:	.quad 0x0
 rsp:	.quad 0x0
 rbp:	.quad 0x0
+rsi:	.quad 0x0
+rdi:	.quad 0x0
 r8:	.quad 0x0
 r9:	.quad 0x0
 r10:	.quad 0x0
* Unmerged path kernel/kexec.c
