sched/fair: Stop searching for tasks in newidle balance if there are runnable tasks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [kernel] sched/fair: Stop searching for tasks in idle_balance if there are runnable tasks (Larry Woodman) [1103828]
Rebuild_FUZZ: 96.93%
commit-author Jason Low <jason.low2@hp.com>
commit 39a4d9ca77a31503c6317e49742341d0859d5cb2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/39a4d9ca.failed

It was found that when running some workloads (such as AIM7) on large
systems with many cores, CPUs do not remain idle for long. Thus, tasks
can wake/get enqueued while doing idle balancing.

In this patch, while traversing the domains in idle balance, in
addition to checking for pulled_task, we add an extra check for
this_rq->nr_running for determining if we should stop searching for
tasks to pull. If there are runnable tasks on this rq, then we will
stop traversing the domains. This reduces the chance that idle balance
delays a task from running.

This patch resulted in approximately a 6% performance improvement when
running a Java Server workload on an 8 socket machine.

	Signed-off-by: Jason Low <jason.low2@hp.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: daniel.lezcano@linaro.org
	Cc: alex.shi@linaro.org
	Cc: preeti@linux.vnet.ibm.com
	Cc: efault@gmx.de
	Cc: vincent.guittot@linaro.org
	Cc: morten.rasmussen@arm.com
	Cc: aswin@hp.com
	Cc: chegu_vinod@hp.com
Link: http://lkml.kernel.org/r/1398303035-18255-4-git-send-email-jason.low2@hp.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 39a4d9ca77a31503c6317e49742341d0859d5cb2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 32588e344590,28ccf502c63c..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -6473,10 -6713,9 +6473,9 @@@ void idle_balance(int this_cpu, struct 
  		if (sd->flags & SD_BALANCE_NEWIDLE) {
  			t0 = sched_clock_cpu(this_cpu);
  
- 			/* If we've pulled tasks over stop searching: */
  			pulled_task = load_balance(this_cpu, this_rq,
  						   sd, CPU_NEWLY_IDLE,
 -						   &continue_balancing);
 +						   &should_balance);
  
  			domain_cost = sched_clock_cpu(this_cpu) - t0;
  			if (domain_cost > sd->max_newidle_lb_cost)
@@@ -6488,10 -6727,13 +6487,19 @@@
  		interval = msecs_to_jiffies(sd->balance_interval);
  		if (time_after(next_balance, sd->last_balance + interval))
  			next_balance = sd->last_balance + interval;
++<<<<<<< HEAD
 +		if (pulled_task) {
 +			this_rq->idle_stamp = 0;
++=======
+ 
+ 		/*
+ 		 * Stop searching for tasks to pull if there are
+ 		 * now runnable tasks on this rq.
+ 		 */
+ 		if (pulled_task || this_rq->nr_running > 0)
++>>>>>>> 39a4d9ca77a3 (sched/fair: Stop searching for tasks in newidle balance if there are runnable tasks)
  			break;
 +		}
  	}
  	rcu_read_unlock();
  
* Unmerged path kernel/sched/fair.c
