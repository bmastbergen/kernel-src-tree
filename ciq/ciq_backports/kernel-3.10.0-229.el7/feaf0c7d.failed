KVM: nVMX: Do not generate #DF if #PF happens during exception delivery into L2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
Rebuild_CHGLOG: - [virt] kvm/nvmx: Do not generate #DF if #PF happens during exception delivery into L2 (Paolo Bonzini) [1116936]
Rebuild_FUZZ: 98.09%
commit-author Gleb Natapov <gleb@redhat.com>
commit feaf0c7dc473fefa1f263d88788f57e39b4b007e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/feaf0c7d.failed

If #PF happens during delivery of an exception into L2 and L1 also do
not have the page mapped in its shadow page table then L0 needs to
generate vmexit to L2 with original event in IDT_VECTORING_INFO, but
current code combines both exception and generates #DF instead. Fix that
by providing nVMX specific function to handle page faults during page
table walk that handles this case correctly.

	Signed-off-by: Gleb Natapov <gleb@redhat.com>
	Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit feaf0c7dc473fefa1f263d88788f57e39b4b007e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 63b66c24cc34,be7fd0e1ad42..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -7472,6 -7476,62 +7472,65 @@@ static void vmx_set_supported_cpuid(u3
  		entry->ecx |= bit(X86_FEATURE_VMX);
  }
  
++<<<<<<< HEAD
++=======
+ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
+ 		struct x86_exception *fault)
+ {
+ 	struct vmcs12 *vmcs12;
+ 	nested_vmx_vmexit(vcpu);
+ 	vmcs12 = get_vmcs12(vcpu);
+ 
+ 	if (fault->error_code & PFERR_RSVD_MASK)
+ 		vmcs12->vm_exit_reason = EXIT_REASON_EPT_MISCONFIG;
+ 	else
+ 		vmcs12->vm_exit_reason = EXIT_REASON_EPT_VIOLATION;
+ 	vmcs12->exit_qualification = vcpu->arch.exit_qualification;
+ 	vmcs12->guest_physical_address = fault->address;
+ }
+ 
+ /* Callbacks for nested_ept_init_mmu_context: */
+ 
+ static unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu)
+ {
+ 	/* return the page table to be shadowed - in our case, EPT12 */
+ 	return get_vmcs12(vcpu)->ept_pointer;
+ }
+ 
+ static int nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
+ {
+ 	int r = kvm_init_shadow_ept_mmu(vcpu, &vcpu->arch.mmu,
+ 			nested_vmx_ept_caps & VMX_EPT_EXECUTE_ONLY_BIT);
+ 
+ 	vcpu->arch.mmu.set_cr3           = vmx_set_cr3;
+ 	vcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;
+ 	vcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;
+ 
+ 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
+ 
+ 	return r;
+ }
+ 
+ static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->arch.walk_mmu = &vcpu->arch.mmu;
+ }
+ 
+ static void vmx_inject_page_fault_nested(struct kvm_vcpu *vcpu,
+ 		struct x86_exception *fault)
+ {
+ 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
+ 
+ 	WARN_ON(!is_guest_mode(vcpu));
+ 
+ 	/* TODO: also check PFEC_MATCH/MASK, not just EB.PF. */
+ 	if (vmcs12->exception_bitmap & (1u << PF_VECTOR))
+ 		nested_vmx_vmexit(vcpu);
+ 	else
+ 		kvm_inject_page_fault(vcpu, fault);
+ }
+ 
++>>>>>>> feaf0c7dc473 (KVM: nVMX: Do not generate #DF if #PF happens during exception delivery into L2)
  /*
   * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
   * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
* Unmerged path arch/x86/kvm/vmx.c
