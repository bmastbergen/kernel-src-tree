blk-mq: fix WARNING "percpu_ref_kill() called more than once!"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Ming Lei <ming.lei@canonical.com>
commit dd840087086f3b93ac20f7472b4fca59aff7b79f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/dd840087.failed

Before doing queue release, the queue has been freezed already
by blk_cleanup_queue(), so needn't to freeze queue for deleting
tag set.

This patch fixes the WARNING of "percpu_ref_kill() called more than once!"
which is triggered during unloading block driver.

	Cc: Tejun Heo <tj@kernel.org>
	Signed-off-by: Ming Lei <ming.lei@canonical.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit dd840087086f3b93ac20f7472b4fca59aff7b79f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index e7914e35b358,ac8a0413664e..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1307,10 -1659,78 +1307,60 @@@ static void blk_mq_map_swqueue(struct r
  		ctx->index_hw = hctx->nr_ctx;
  		hctx->ctxs[hctx->nr_ctx++] = ctx;
  	}
 -
 -	queue_for_each_hw_ctx(q, hctx, i) {
 -		/*
 -		 * If not software queues are mapped to this hardware queue,
 -		 * disable it and free the request entries
 -		 */
 -		if (!hctx->nr_ctx) {
 -			struct blk_mq_tag_set *set = q->tag_set;
 -
 -			if (set->tags[i]) {
 -				blk_mq_free_rq_map(set, set->tags[i], i);
 -				set->tags[i] = NULL;
 -				hctx->tags = NULL;
 -			}
 -			continue;
 -		}
 -
 -		/*
 -		 * Initialize batch roundrobin counts
 -		 */
 -		hctx->next_cpu = cpumask_first(hctx->cpumask);
 -		hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
 -	}
  }
  
++<<<<<<< HEAD
 +struct request_queue *blk_mq_init_queue(struct blk_mq_reg *reg,
 +					void *driver_data)
++=======
+ static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct request_queue *q;
+ 	bool shared;
+ 	int i;
+ 
+ 	if (set->tag_list.next == set->tag_list.prev)
+ 		shared = false;
+ 	else
+ 		shared = true;
+ 
+ 	list_for_each_entry(q, &set->tag_list, tag_set_list) {
+ 		blk_mq_freeze_queue(q);
+ 
+ 		queue_for_each_hw_ctx(q, hctx, i) {
+ 			if (shared)
+ 				hctx->flags |= BLK_MQ_F_TAG_SHARED;
+ 			else
+ 				hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+ 		}
+ 		blk_mq_unfreeze_queue(q);
+ 	}
+ }
+ 
+ static void blk_mq_del_queue_tag_set(struct request_queue *q)
+ {
+ 	struct blk_mq_tag_set *set = q->tag_set;
+ 
+ 	mutex_lock(&set->tag_list_lock);
+ 	list_del_init(&q->tag_set_list);
+ 	blk_mq_update_tag_set_depth(set);
+ 	mutex_unlock(&set->tag_list_lock);
+ }
+ 
+ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
+ 				     struct request_queue *q)
+ {
+ 	q->tag_set = set;
+ 
+ 	mutex_lock(&set->tag_list_lock);
+ 	list_add_tail(&q->tag_set_list, &set->tag_list);
+ 	blk_mq_update_tag_set_depth(set);
+ 	mutex_unlock(&set->tag_list_lock);
+ }
+ 
+ struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
++>>>>>>> dd840087086f (blk-mq: fix WARNING "percpu_ref_kill() called more than once!")
  {
  	struct blk_mq_hw_ctx **hctxs;
  	struct blk_mq_ctx __percpu *ctx;
* Unmerged path block/blk-mq.c
