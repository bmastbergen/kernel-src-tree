mmu_notifiers: rename invalidate_range notifier

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-427.16.1.el9_4
commit-author Alistair Popple <apopple@nvidia.com>
commit 1af5a8109904b7f00828e7f9f63f5695b42f8215
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-427.16.1.el9_4/1af5a810.failed

There are two main use cases for mmu notifiers.  One is by KVM which uses
mmu_notifier_invalidate_range_start()/end() to manage a software TLB.

The other is to manage hardware TLBs which need to use the
invalidate_range() callback because HW can establish new TLB entries at
any time.  Hence using start/end() can lead to memory corruption as these
callbacks happen too soon/late during page unmap.

mmu notifier users should therefore either use the start()/end() callbacks
or the invalidate_range() callbacks.  To make this usage clearer rename
the invalidate_range() callback to arch_invalidate_secondary_tlbs() and
update documention.

Link: https://lkml.kernel.org/r/6f77248cd25545c8020a54b4e567e8b72be4dca1.1690292440.git-series.apopple@nvidia.com
	Signed-off-by: Alistair Popple <apopple@nvidia.com>
	Suggested-by: Jason Gunthorpe <jgg@nvidia.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Cc: Andrew Donnellan <ajd@linux.ibm.com>
	Cc: Chaitanya Kumar Borah <chaitanya.kumar.borah@intel.com>
	Cc: Frederic Barrat <fbarrat@linux.ibm.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Kevin Tian <kevin.tian@intel.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Nicholas Piggin <npiggin@gmail.com>
	Cc: Nicolin Chen <nicolinc@nvidia.com>
	Cc: Robin Murphy <robin.murphy@arm.com>
	Cc: Sean Christopherson <seanjc@google.com>
	Cc: SeongJae Park <sj@kernel.org>
	Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
	Cc: Will Deacon <will@kernel.org>
	Cc: Zhi Wang <zhi.wang.linux@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 1af5a8109904b7f00828e7f9f63f5695b42f8215)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c
#	include/linux/mmu_notifier.h
diff --cc drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c
index d4b4aa4df554,dbc812a0e57e..000000000000
--- a/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c
+++ b/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c
@@@ -186,18 -186,10 +186,25 @@@ static void arm_smmu_free_shared_cd(str
  	}
  }
  
++<<<<<<< HEAD
 +/*
 + * Cloned from the MAX_TLBI_OPS in arch/arm64/include/asm/tlbflush.h, this
 + * is used as a threshold to replace per-page TLBI commands to issue in the
 + * command queue with an address-space TLBI command, when SMMU w/o a range
 + * invalidation feature handles too many per-page TLBI commands, which will
 + * otherwise result in a soft lockup.
 + */
 +#define CMDQ_MAX_TLBI_OPS		(1 << (PAGE_SHIFT - 3))
 +
 +static void arm_smmu_mm_invalidate_range(struct mmu_notifier *mn,
 +					 struct mm_struct *mm,
 +					 unsigned long start, unsigned long end)
++=======
+ static void arm_smmu_mm_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,
+ 						struct mm_struct *mm,
+ 						unsigned long start,
+ 						unsigned long end)
++>>>>>>> 1af5a8109904 (mmu_notifiers: rename invalidate_range notifier)
  {
  	struct arm_smmu_mmu_notifier *smmu_mn = mn_to_smmu(mn);
  	struct arm_smmu_domain *smmu_domain = smmu_mn->domain;
diff --cc include/linux/mmu_notifier.h
index d6c06e140277,6e3c857606f1..000000000000
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@@ -396,10 -395,9 +396,16 @@@ extern int __mmu_notifier_test_young(st
  extern void __mmu_notifier_change_pte(struct mm_struct *mm,
  				      unsigned long address, pte_t pte);
  extern int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *r);
++<<<<<<< HEAD
 +extern void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *r,
 +				  bool only_end);
 +extern void __mmu_notifier_invalidate_range(struct mm_struct *mm,
 +				  unsigned long start, unsigned long end);
++=======
+ extern void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *r);
+ extern void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
+ 					unsigned long start, unsigned long end);
++>>>>>>> 1af5a8109904 (mmu_notifiers: rename invalidate_range notifier)
  extern bool
  mmu_notifier_range_update_to_read_only(const struct mmu_notifier_range *range);
  
@@@ -482,21 -480,14 +488,21 @@@ mmu_notifier_invalidate_range_end(struc
  		might_sleep();
  
  	if (mm_has_notifiers(range->mm))
 -		__mmu_notifier_invalidate_range_end(range);
 +		__mmu_notifier_invalidate_range_end(range, false);
 +}
 +
 +static inline void
 +mmu_notifier_invalidate_range_only_end(struct mmu_notifier_range *range)
 +{
 +	if (mm_has_notifiers(range->mm))
 +		__mmu_notifier_invalidate_range_end(range, true);
  }
  
- static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,
- 				  unsigned long start, unsigned long end)
+ static inline void mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
+ 					unsigned long start, unsigned long end)
  {
  	if (mm_has_notifiers(mm))
- 		__mmu_notifier_invalidate_range(mm, start, end);
+ 		__mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);
  }
  
  static inline void mmu_notifier_subscriptions_init(struct mm_struct *mm)
@@@ -714,12 -664,7 +720,16 @@@ void mmu_notifier_invalidate_range_end(
  {
  }
  
++<<<<<<< HEAD
 +static inline void
 +mmu_notifier_invalidate_range_only_end(struct mmu_notifier_range *range)
 +{
 +}
 +
 +static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,
++=======
+ static inline void mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
++>>>>>>> 1af5a8109904 (mmu_notifiers: rename invalidate_range notifier)
  				  unsigned long start, unsigned long end)
  {
  }
diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h
index f3b27708ebd6..dc155103fb6e 100644
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@ -253,7 +253,7 @@ static inline void flush_tlb_mm(struct mm_struct *mm)
 	__tlbi(aside1is, asid);
 	__tlbi_user(aside1is, asid);
 	dsb(ish);
-	mmu_notifier_invalidate_range(mm, 0, -1UL);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
 }
 
 static inline void flush_tlb_page_nosync(struct vm_area_struct *vma,
@@ -265,7 +265,7 @@ static inline void flush_tlb_page_nosync(struct vm_area_struct *vma,
 	addr = __TLBI_VADDR(uaddr, ASID(vma->vm_mm));
 	__tlbi(vale1is, addr);
 	__tlbi_user(vale1is, addr);
-	mmu_notifier_invalidate_range(mm, uaddr & PAGE_MASK,
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, uaddr & PAGE_MASK,
 						(uaddr & PAGE_MASK) + PAGE_SIZE);
 }
 
@@ -380,7 +380,7 @@ static inline void __flush_tlb_range(struct vm_area_struct *vma,
 		__flush_tlb_range_op(vae1is, start, pages, stride, asid, tlb_level, true);
 
 	dsb(ish);
-	mmu_notifier_invalidate_range(vma->vm_mm, start, end);
+	mmu_notifier_arch_invalidate_secondary_tlbs(vma->vm_mm, start, end);
 }
 
 static inline void flush_tlb_range(struct vm_area_struct *vma,
diff --git a/arch/powerpc/mm/book3s64/radix_hugetlbpage.c b/arch/powerpc/mm/book3s64/radix_hugetlbpage.c
index f3fb49fd32fe..17075c78d4bc 100644
--- a/arch/powerpc/mm/book3s64/radix_hugetlbpage.c
+++ b/arch/powerpc/mm/book3s64/radix_hugetlbpage.c
@@ -39,7 +39,7 @@ void radix__flush_hugetlb_tlb_range(struct vm_area_struct *vma, unsigned long st
 		radix__flush_tlb_pwc_range_psize(vma->vm_mm, start, end, psize);
 	else
 		radix__flush_tlb_range_psize(vma->vm_mm, start, end, psize);
-	mmu_notifier_invalidate_range(vma->vm_mm, start, end);
+	mmu_notifier_arch_invalidate_secondary_tlbs(vma->vm_mm, start, end);
 }
 
 void radix__huge_ptep_modify_prot_commit(struct vm_area_struct *vma,
diff --git a/arch/powerpc/mm/book3s64/radix_tlb.c b/arch/powerpc/mm/book3s64/radix_tlb.c
index b198b49f72ea..b3d33b8ea0b8 100644
--- a/arch/powerpc/mm/book3s64/radix_tlb.c
+++ b/arch/powerpc/mm/book3s64/radix_tlb.c
@@ -976,7 +976,7 @@ void radix__flush_tlb_mm(struct mm_struct *mm)
 		}
 	}
 	preempt_enable();
-	mmu_notifier_invalidate_range(mm, 0, -1UL);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
 }
 EXPORT_SYMBOL(radix__flush_tlb_mm);
 
@@ -1010,7 +1010,7 @@ static void __flush_all_mm(struct mm_struct *mm, bool fullmm)
 			_tlbiel_pid_multicast(mm, pid, RIC_FLUSH_ALL);
 	}
 	preempt_enable();
-	mmu_notifier_invalidate_range(mm, 0, -1UL);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
 }
 
 void radix__flush_all_mm(struct mm_struct *mm)
@@ -1220,7 +1220,7 @@ static inline void __radix__flush_tlb_range(struct mm_struct *mm,
 	}
 out:
 	preempt_enable();
-	mmu_notifier_invalidate_range(mm, start, end);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);
 }
 
 void radix__flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
@@ -1388,7 +1388,7 @@ static void __radix__flush_tlb_range_psize(struct mm_struct *mm,
 	}
 out:
 	preempt_enable();
-	mmu_notifier_invalidate_range(mm, start, end);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);
 }
 
 void radix__flush_tlb_range_psize(struct mm_struct *mm, unsigned long start,
diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h
index b0191e63390e..01d8828570a9 100644
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -270,7 +270,7 @@ static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 {
 	inc_mm_tlb_gen(mm);
 	cpumask_or(&batch->cpumask, &batch->cpumask, mm_cpumask(mm));
-	mmu_notifier_invalidate_range(mm, 0, -1UL);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
 }
 
 extern void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch);
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index 028fc159107e..e1139432d25e 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -1037,7 +1037,7 @@ void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 
 	put_flush_tlb_info();
 	put_cpu();
-	mmu_notifier_invalidate_range(mm, start, end);
+	mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);
 }
 
 
diff --git a/drivers/iommu/amd/iommu_v2.c b/drivers/iommu/amd/iommu_v2.c
index 1df1d227bd26..fb38bb9def7f 100644
--- a/drivers/iommu/amd/iommu_v2.c
+++ b/drivers/iommu/amd/iommu_v2.c
@@ -358,9 +358,9 @@ static struct pasid_state *mn_to_state(struct mmu_notifier *mn)
 	return container_of(mn, struct pasid_state, mn);
 }
 
-static void mn_invalidate_range(struct mmu_notifier *mn,
-				struct mm_struct *mm,
-				unsigned long start, unsigned long end)
+static void mn_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,
+					struct mm_struct *mm,
+					unsigned long start, unsigned long end)
 {
 	struct pasid_state *pasid_state;
 	struct device_state *dev_state;
@@ -394,8 +394,8 @@ static void mn_release(struct mmu_notifier *mn, struct mm_struct *mm)
 }
 
 static const struct mmu_notifier_ops iommu_mn = {
-	.release		= mn_release,
-	.invalidate_range       = mn_invalidate_range,
+	.release			= mn_release,
+	.arch_invalidate_secondary_tlbs	= mn_arch_invalidate_secondary_tlbs,
 };
 
 static void set_pri_tag_status(struct pasid_state *pasid_state,
* Unmerged path drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c
diff --git a/drivers/iommu/intel/svm.c b/drivers/iommu/intel/svm.c
index 9fbae9af6615..50a481c895b8 100644
--- a/drivers/iommu/intel/svm.c
+++ b/drivers/iommu/intel/svm.c
@@ -217,9 +217,9 @@ static void intel_flush_svm_range(struct intel_svm *svm, unsigned long address,
 }
 
 /* Pages have been freed at this point */
-static void intel_invalidate_range(struct mmu_notifier *mn,
-				   struct mm_struct *mm,
-				   unsigned long start, unsigned long end)
+static void intel_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,
+					struct mm_struct *mm,
+					unsigned long start, unsigned long end)
 {
 	struct intel_svm *svm = container_of(mn, struct intel_svm, notifier);
 
@@ -254,7 +254,7 @@ static void intel_mm_release(struct mmu_notifier *mn, struct mm_struct *mm)
 
 static const struct mmu_notifier_ops intel_mmuops = {
 	.release = intel_mm_release,
-	.invalidate_range = intel_invalidate_range,
+	.arch_invalidate_secondary_tlbs = intel_arch_invalidate_secondary_tlbs,
 };
 
 static int pasid_to_svm_sdev(struct device *dev, unsigned int pasid,
diff --git a/drivers/misc/ocxl/link.c b/drivers/misc/ocxl/link.c
index ab039c115381..be425810385c 100644
--- a/drivers/misc/ocxl/link.c
+++ b/drivers/misc/ocxl/link.c
@@ -490,9 +490,9 @@ void ocxl_link_release(struct pci_dev *dev, void *link_handle)
 }
 EXPORT_SYMBOL_GPL(ocxl_link_release);
 
-static void invalidate_range(struct mmu_notifier *mn,
-			     struct mm_struct *mm,
-			     unsigned long start, unsigned long end)
+static void arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,
+					struct mm_struct *mm,
+					unsigned long start, unsigned long end)
 {
 	struct pe_data *pe_data = container_of(mn, struct pe_data, mmu_notifier);
 	struct ocxl_link *link = pe_data->link;
@@ -508,7 +508,7 @@ static void invalidate_range(struct mmu_notifier *mn,
 }
 
 static const struct mmu_notifier_ops ocxl_mmu_notifier_ops = {
-	.invalidate_range = invalidate_range,
+	.arch_invalidate_secondary_tlbs = arch_invalidate_secondary_tlbs,
 };
 
 static u64 calculate_cfg_state(bool kernel)
* Unmerged path include/linux/mmu_notifier.h
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index ed2724055d2f..0f7fe64f5513 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2136,8 +2136,8 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	if (is_huge_zero_pmd(*pmd)) {
 		/*
 		 * FIXME: Do we want to invalidate secondary mmu by calling
-		 * mmu_notifier_invalidate_range() see comments below inside
-		 * __split_huge_pmd() ?
+		 * mmu_notifier_arch_invalidate_secondary_tlbs() see comments below
+		 * inside __split_huge_pmd() ?
 		 *
 		 * We are going from a zero huge page write protected to zero
 		 * small page also write protected so it does not seems useful
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 48d644c15b95..dba791490a27 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6735,8 +6735,9 @@ long hugetlb_change_protection(struct vm_area_struct *vma,
 	else
 		flush_hugetlb_tlb_range(vma, start, end);
 	/*
-	 * No need to call mmu_notifier_invalidate_range() we are downgrading
-	 * page table protection not changing it to point to a new page.
+	 * No need to call mmu_notifier_arch_invalidate_secondary_tlbs() we are
+	 * downgrading page table protection not changing it to point to a new
+	 * page.
 	 *
 	 * See Documentation/mm/mmu_notifier.rst
 	 */
@@ -7381,7 +7382,7 @@ void hugetlb_unshare_all_pmds(struct vm_area_struct *vma)
 	i_mmap_unlock_write(vma->vm_file->f_mapping);
 	hugetlb_vma_unlock_write(vma);
 	/*
-	 * No need to call mmu_notifier_invalidate_range(), see
+	 * No need to call mmu_notifier_arch_invalidate_secondary_tlbs(), see
 	 * Documentation/mm/mmu_notifier.rst.
 	 */
 	mmu_notifier_invalidate_range_end(&range);
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index cafd24c0ae5a..a65deb42f399 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -604,8 +604,8 @@ void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range,
 	lock_map_release(&__mmu_notifier_invalidate_range_start_map);
 }
 
-void __mmu_notifier_invalidate_range(struct mm_struct *mm,
-				  unsigned long start, unsigned long end)
+void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
+					unsigned long start, unsigned long end)
 {
 	struct mmu_notifier *subscription;
 	int id;
@@ -614,9 +614,10 @@ void __mmu_notifier_invalidate_range(struct mm_struct *mm,
 	hlist_for_each_entry_rcu(subscription,
 				 &mm->notifier_subscriptions->list, hlist,
 				 srcu_read_lock_held(&srcu)) {
-		if (subscription->ops->invalidate_range)
-			subscription->ops->invalidate_range(subscription, mm,
-							    start, end);
+		if (subscription->ops->arch_invalidate_secondary_tlbs)
+			subscription->ops->arch_invalidate_secondary_tlbs(
+				subscription, mm,
+				start, end);
 	}
 	srcu_read_unlock(&srcu, id);
 }
@@ -635,6 +636,16 @@ int __mmu_notifier_register(struct mmu_notifier *subscription,
 	mmap_assert_write_locked(mm);
 	BUG_ON(atomic_read(&mm->mm_users) <= 0);
 
+	/*
+	 * Subsystems should only register for invalidate_secondary_tlbs() or
+	 * invalidate_range_start()/end() callbacks, not both.
+	 */
+	if (WARN_ON_ONCE(subscription &&
+			 (subscription->ops->arch_invalidate_secondary_tlbs &&
+			 (subscription->ops->invalidate_range_start ||
+			  subscription->ops->invalidate_range_end))))
+		return -EINVAL;
+
 	if (!mm->notifier_subscriptions) {
 		/*
 		 * kmalloc cannot be called under mm_take_all_locks(), but we
