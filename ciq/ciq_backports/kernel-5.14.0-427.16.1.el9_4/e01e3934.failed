tls: fix race between tx work scheduling and socket close

jira LE-1907
cve CVE-2024-26585
Rebuild_History Non-Buildable kernel-5.14.0-427.16.1.el9_4
commit-author Jakub Kicinski <kuba@kernel.org>
commit e01e3934a1b2d122919f73bc6ddbe1cdafc4bbdb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-427.16.1.el9_4/e01e3934.failed

Similarly to previous commit, the submitting thread (recvmsg/sendmsg)
may exit as soon as the async crypto handler calls complete().
Reorder scheduling the work before calling complete().
This seems more logical in the first place, as it's
the inverse order of what the submitting thread will do.

	Reported-by: valis <sec@valis.email>
Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
	Reviewed-by: Simon Horman <horms@kernel.org>
	Reviewed-by: Sabrina Dubroca <sd@queasysnail.net>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e01e3934a1b2d122919f73bc6ddbe1cdafc4bbdb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/tls/tls_sw.c
diff --cc net/tls/tls_sw.c
index 4c918094226a,9374a61cef00..000000000000
--- a/net/tls/tls_sw.c
+++ b/net/tls/tls_sw.c
@@@ -420,22 -439,23 +420,26 @@@ tx_err
  	return rc;
  }
  
 -static void tls_encrypt_done(void *data, int err)
 +static void tls_encrypt_done(struct crypto_async_request *req, int err)
  {
 -	struct tls_sw_context_tx *ctx;
 -	struct tls_context *tls_ctx;
 -	struct tls_prot_info *prot;
 -	struct tls_rec *rec = data;
 +	struct aead_request *aead_req = (struct aead_request *)req;
 +	struct sock *sk = req->data;
 +	struct tls_context *tls_ctx = tls_get_ctx(sk);
 +	struct tls_prot_info *prot = &tls_ctx->prot_info;
 +	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
  	struct scatterlist *sge;
  	struct sk_msg *msg_en;
++<<<<<<< HEAD
 +	struct tls_rec *rec;
 +	bool ready = false;
 +	int pending;
++=======
+ 	struct sock *sk;
++>>>>>>> e01e3934a1b2 (tls: fix race between tx work scheduling and socket close)
  
 +	rec = container_of(aead_req, struct tls_rec, aead_req);
  	msg_en = &rec->msg_encrypted;
  
 -	sk = rec->sk;
 -	tls_ctx = tls_get_ctx(sk);
 -	prot = &tls_ctx->prot_info;
 -	ctx = tls_sw_ctx_tx(tls_ctx);
 -
  	sge = sk_msg_elem(msg_en, msg_en->sg.curr);
  	sge->offset -= prot->prepend_size;
  	sge->length += prot->prepend_size;
@@@ -462,23 -482,25 +466,30 @@@
  		/* If received record is at head of tx_list, schedule tx */
  		first_rec = list_first_entry(&ctx->tx_list,
  					     struct tls_rec, list);
- 		if (rec == first_rec)
- 			ready = true;
+ 		if (rec == first_rec) {
+ 			/* Schedule the transmission */
+ 			if (!test_and_set_bit(BIT_TX_SCHEDULED,
+ 					      &ctx->tx_bitmask))
+ 				schedule_delayed_work(&ctx->tx_work.work, 1);
+ 		}
  	}
  
 -	if (atomic_dec_and_test(&ctx->encrypt_pending))
 +	spin_lock_bh(&ctx->encrypt_compl_lock);
 +	pending = atomic_dec_return(&ctx->encrypt_pending);
 +
 +	if (!pending && ctx->async_notify)
  		complete(&ctx->async_wait.completion);
 -}
++<<<<<<< HEAD
 +	spin_unlock_bh(&ctx->encrypt_compl_lock);
  
 -static int tls_encrypt_async_wait(struct tls_sw_context_tx *ctx)
 -{
 -	if (!atomic_dec_and_test(&ctx->encrypt_pending))
 -		crypto_wait_req(-EINPROGRESS, &ctx->async_wait);
 -	atomic_inc(&ctx->encrypt_pending);
 +	if (!ready)
 +		return;
  
 -	return ctx->async_wait.err;
 +	/* Schedule the transmission */
 +	if (!test_and_set_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask))
 +		schedule_delayed_work(&ctx->tx_work.work, 1);
++=======
++>>>>>>> e01e3934a1b2 (tls: fix race between tx work scheduling and socket close)
  }
  
  static int tls_do_encryption(struct sock *sk,
* Unmerged path net/tls/tls_sw.c
