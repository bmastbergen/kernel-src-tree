x86/bugs: Report Intel retbleed vulnerability

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 6ad0ad2bf8a67e27d1f9d006a1dabb0e1c360cc3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/6ad0ad2b.failed

Skylake suffers from RSB underflow speculation issues; report this
vulnerability and it's mitigation (spectre_v2=ibrs).

  [jpoimboe: cleanups, eibrs]

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit 6ad0ad2bf8a67e27d1f9d006a1dabb0e1c360cc3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
diff --cc arch/x86/kernel/cpu/bugs.c
index a7e93ef81745,05f29db9473b..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -649,10 -790,203 +649,209 @@@ static int __init nospectre_v1_cmdline(
  }
  early_param("nospectre_v1", nospectre_v1_cmdline);
  
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
  #undef pr_fmt
++<<<<<<< HEAD
 +#define pr_fmt(fmt)     "Spectre V2 : " fmt
 +
 +enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;
++=======
+ #define pr_fmt(fmt)     "RETBleed: " fmt
+ 
+ enum retbleed_mitigation {
+ 	RETBLEED_MITIGATION_NONE,
+ 	RETBLEED_MITIGATION_UNRET,
+ 	RETBLEED_MITIGATION_IBRS,
+ 	RETBLEED_MITIGATION_EIBRS,
+ };
+ 
+ enum retbleed_mitigation_cmd {
+ 	RETBLEED_CMD_OFF,
+ 	RETBLEED_CMD_AUTO,
+ 	RETBLEED_CMD_UNRET,
+ };
+ 
+ const char * const retbleed_strings[] = {
+ 	[RETBLEED_MITIGATION_NONE]	= "Vulnerable",
+ 	[RETBLEED_MITIGATION_UNRET]	= "Mitigation: untrained return thunk",
+ 	[RETBLEED_MITIGATION_IBRS]	= "Mitigation: IBRS",
+ 	[RETBLEED_MITIGATION_EIBRS]	= "Mitigation: Enhanced IBRS",
+ };
+ 
+ static enum retbleed_mitigation retbleed_mitigation __ro_after_init =
+ 	RETBLEED_MITIGATION_NONE;
+ static enum retbleed_mitigation_cmd retbleed_cmd __ro_after_init =
+ 	RETBLEED_CMD_AUTO;
+ 
+ static int __ro_after_init retbleed_nosmt = false;
+ 
+ static int __init retbleed_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	while (str) {
+ 		char *next = strchr(str, ',');
+ 		if (next) {
+ 			*next = 0;
+ 			next++;
+ 		}
+ 
+ 		if (!strcmp(str, "off")) {
+ 			retbleed_cmd = RETBLEED_CMD_OFF;
+ 		} else if (!strcmp(str, "auto")) {
+ 			retbleed_cmd = RETBLEED_CMD_AUTO;
+ 		} else if (!strcmp(str, "unret")) {
+ 			retbleed_cmd = RETBLEED_CMD_UNRET;
+ 		} else if (!strcmp(str, "nosmt")) {
+ 			retbleed_nosmt = true;
+ 		} else {
+ 			pr_err("Ignoring unknown retbleed option (%s).", str);
+ 		}
+ 
+ 		str = next;
+ 	}
+ 
+ 	return 0;
+ }
+ early_param("retbleed", retbleed_parse_cmdline);
+ 
+ #define RETBLEED_UNTRAIN_MSG "WARNING: BTB untrained return thunk mitigation is only effective on AMD/Hygon!\n"
+ #define RETBLEED_COMPILER_MSG "WARNING: kernel not compiled with RETPOLINE or -mfunction-return capable compiler!\n"
+ #define RETBLEED_INTEL_MSG "WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!\n"
+ 
+ static void __init retbleed_select_mitigation(void)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_RETBLEED) || cpu_mitigations_off())
+ 		return;
+ 
+ 	switch (retbleed_cmd) {
+ 	case RETBLEED_CMD_OFF:
+ 		return;
+ 
+ 	case RETBLEED_CMD_UNRET:
+ 		retbleed_mitigation = RETBLEED_MITIGATION_UNRET;
+ 		break;
+ 
+ 	case RETBLEED_CMD_AUTO:
+ 	default:
+ 		if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||
+ 		    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)
+ 			retbleed_mitigation = RETBLEED_MITIGATION_UNRET;
+ 
+ 		/*
+ 		 * The Intel mitigation (IBRS) was already selected in
+ 		 * spectre_v2_select_mitigation().
+ 		 */
+ 
+ 		break;
+ 	}
+ 
+ 	switch (retbleed_mitigation) {
+ 	case RETBLEED_MITIGATION_UNRET:
+ 
+ 		if (!IS_ENABLED(CONFIG_RETPOLINE) ||
+ 		    !IS_ENABLED(CONFIG_CC_HAS_RETURN_THUNK)) {
+ 			pr_err(RETBLEED_COMPILER_MSG);
+ 			retbleed_mitigation = RETBLEED_MITIGATION_NONE;
+ 			break;
+ 		}
+ 
+ 		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+ 		setup_force_cpu_cap(X86_FEATURE_UNRET);
+ 
+ 		if (!boot_cpu_has(X86_FEATURE_STIBP) &&
+ 		    (retbleed_nosmt || cpu_mitigations_auto_nosmt()))
+ 			cpu_smt_disable(false);
+ 
+ 		if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&
+ 		    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)
+ 			pr_err(RETBLEED_UNTRAIN_MSG);
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * Let IBRS trump all on Intel without affecting the effects of the
+ 	 * retbleed= cmdline option.
+ 	 */
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) {
+ 		switch (spectre_v2_enabled) {
+ 		case SPECTRE_V2_IBRS:
+ 			retbleed_mitigation = RETBLEED_MITIGATION_IBRS;
+ 			break;
+ 		case SPECTRE_V2_EIBRS:
+ 		case SPECTRE_V2_EIBRS_RETPOLINE:
+ 		case SPECTRE_V2_EIBRS_LFENCE:
+ 			retbleed_mitigation = RETBLEED_MITIGATION_EIBRS;
+ 			break;
+ 		default:
+ 			pr_err(RETBLEED_INTEL_MSG);
+ 		}
+ 	}
+ 
+ 	pr_info("%s\n", retbleed_strings[retbleed_mitigation]);
+ }
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Spectre V2 : " fmt
+ 
+ static enum spectre_v2_user_mitigation spectre_v2_user_stibp __ro_after_init =
+ 	SPECTRE_V2_USER_NONE;
+ static enum spectre_v2_user_mitigation spectre_v2_user_ibpb __ro_after_init =
+ 	SPECTRE_V2_USER_NONE;
+ 
+ #ifdef CONFIG_RETPOLINE
+ static bool spectre_v2_bad_module;
+ 
+ bool retpoline_module_ok(bool has_retpoline)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ 		return true;
+ 
+ 	pr_err("System may be vulnerable to spectre v2\n");
+ 	spectre_v2_bad_module = true;
+ 	return false;
+ }
+ 
+ static inline const char *spectre_v2_module_string(void)
+ {
+ 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ }
+ #else
+ static inline const char *spectre_v2_module_string(void) { return ""; }
+ #endif
+ 
+ #define SPECTRE_V2_LFENCE_MSG "WARNING: LFENCE mitigation is not recommended for this CPU, data leaks possible!\n"
+ #define SPECTRE_V2_EIBRS_EBPF_MSG "WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!\n"
+ #define SPECTRE_V2_EIBRS_LFENCE_EBPF_SMT_MSG "WARNING: Unprivileged eBPF is enabled with eIBRS+LFENCE mitigation and SMT, data leaks possible via Spectre v2 BHB attacks!\n"
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ void unpriv_ebpf_notify(int new_state)
+ {
+ 	if (new_state)
+ 		return;
+ 
+ 	/* Unprivileged eBPF is enabled */
+ 
+ 	switch (spectre_v2_enabled) {
+ 	case SPECTRE_V2_EIBRS:
+ 		pr_err(SPECTRE_V2_EIBRS_EBPF_MSG);
+ 		break;
+ 	case SPECTRE_V2_EIBRS_LFENCE:
+ 		if (sched_smt_active())
+ 			pr_err(SPECTRE_V2_EIBRS_LFENCE_EBPF_SMT_MSG);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ }
+ #endif
++>>>>>>> 6ad0ad2bf8a6 (x86/bugs: Report Intel retbleed vulnerability)
  
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
diff --cc arch/x86/kernel/cpu/common.c
index 2e803501e8c0,75a5c720c852..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -1002,28 -1227,42 +1002,58 @@@ static const __initconst struct x86_cpu
  
  static const struct x86_cpu_id cpu_vuln_blacklist[] __initconst = {
  	VULNBL_INTEL_STEPPINGS(IVYBRIDGE,	X86_STEPPING_ANY,		SRBDS),
 -	VULNBL_INTEL_STEPPINGS(HASWELL,		X86_STEPPING_ANY,		SRBDS),
 -	VULNBL_INTEL_STEPPINGS(HASWELL_L,	X86_STEPPING_ANY,		SRBDS),
 -	VULNBL_INTEL_STEPPINGS(HASWELL_G,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(HASWELL_CORE,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(HASWELL_ULT,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(HASWELL_GT3E,	X86_STEPPING_ANY,		SRBDS),
  	VULNBL_INTEL_STEPPINGS(HASWELL_X,	BIT(2) | BIT(4),		MMIO),
 -	VULNBL_INTEL_STEPPINGS(BROADWELL_D,	X86_STEPPINGS(0x3, 0x5),	MMIO),
 -	VULNBL_INTEL_STEPPINGS(BROADWELL_G,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(BROADWELL_XEON_D, X86_STEPPINGS(0x3, 0x5),	MMIO),
 +	VULNBL_INTEL_STEPPINGS(BROADWELL_GT3E,	X86_STEPPING_ANY,		SRBDS),
  	VULNBL_INTEL_STEPPINGS(BROADWELL_X,	X86_STEPPING_ANY,		MMIO),
++<<<<<<< HEAD
 +	VULNBL_INTEL_STEPPINGS(BROADWELL_CORE,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(SKYLAKE_MOBILE,	X86_STEPPINGS(0x3, 0x3),	SRBDS | MMIO),
 +	VULNBL_INTEL_STEPPINGS(SKYLAKE_MOBILE,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(SKYLAKE_X,	BIT(3) | BIT(4) | BIT(6) |
 +						BIT(7) | BIT(0xB),		MMIO),
 +	VULNBL_INTEL_STEPPINGS(SKYLAKE_DESKTOP, X86_STEPPINGS(0x3, 0x3),	SRBDS | MMIO),
 +	VULNBL_INTEL_STEPPINGS(SKYLAKE_DESKTOP,	X86_STEPPING_ANY,		SRBDS),
 +	VULNBL_INTEL_STEPPINGS(KABYLAKE_MOBILE, X86_STEPPINGS(0x9, 0xC),	SRBDS | MMIO),
 +	VULNBL_INTEL_STEPPINGS(KABYLAKE_MOBILE,	X86_STEPPINGS(0x0, 0x8),	SRBDS),
 +	VULNBL_INTEL_STEPPINGS(KABYLAKE_DESKTOP, X86_STEPPINGS(0x9, 0xD),	SRBDS | MMIO),
 +	VULNBL_INTEL_STEPPINGS(KABYLAKE_DESKTOP, X86_STEPPINGS(0x0, 0x8),	SRBDS),
 +	VULNBL_INTEL_STEPPINGS(ICELAKE_MOBILE,	X86_STEPPINGS(0x5, 0x5),	MMIO | MMIO_SBDS),
 +	VULNBL_INTEL_STEPPINGS(ICELAKE_DESKTOP,	X86_STEPPINGS(0x1, 0x1),	MMIO),
 +	VULNBL_INTEL_STEPPINGS(ICELAKE_X,	X86_STEPPINGS(0x4, 0x6),	MMIO),
 +	VULNBL_INTEL_STEPPINGS(ATOM_TREMONT_X,	X86_STEPPING_ANY,		MMIO),
++=======
+ 	VULNBL_INTEL_STEPPINGS(BROADWELL,	X86_STEPPING_ANY,		SRBDS),
+ 	VULNBL_INTEL_STEPPINGS(SKYLAKE_L,	X86_STEPPINGS(0x3, 0x3),	SRBDS | MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(SKYLAKE_L,	X86_STEPPING_ANY,		SRBDS),
+ 	VULNBL_INTEL_STEPPINGS(SKYLAKE_X,	BIT(3) | BIT(4) | BIT(6) |
+ 						BIT(7) | BIT(0xB),              MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(SKYLAKE,		X86_STEPPINGS(0x3, 0x3),	SRBDS | MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(SKYLAKE,		X86_STEPPING_ANY,		SRBDS),
+ 	VULNBL_INTEL_STEPPINGS(KABYLAKE_L,	X86_STEPPINGS(0x9, 0xC),	SRBDS | MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(KABYLAKE_L,	X86_STEPPINGS(0x0, 0x8),	SRBDS),
+ 	VULNBL_INTEL_STEPPINGS(KABYLAKE,	X86_STEPPINGS(0x9, 0xD),	SRBDS | MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(KABYLAKE,	X86_STEPPINGS(0x0, 0x8),	SRBDS),
+ 	VULNBL_INTEL_STEPPINGS(ICELAKE_L,	X86_STEPPINGS(0x5, 0x5),	MMIO | MMIO_SBDS | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(ICELAKE_D,	X86_STEPPINGS(0x1, 0x1),	MMIO),
+ 	VULNBL_INTEL_STEPPINGS(ICELAKE_X,	X86_STEPPINGS(0x4, 0x6),	MMIO),
+ 	VULNBL_INTEL_STEPPINGS(COMETLAKE,	BIT(2) | BIT(3) | BIT(5),	MMIO | MMIO_SBDS | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(COMETLAKE_L,	X86_STEPPINGS(0x1, 0x1),	MMIO | MMIO_SBDS | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(COMETLAKE_L,	X86_STEPPINGS(0x0, 0x0),	MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(LAKEFIELD,	X86_STEPPINGS(0x1, 0x1),	MMIO | MMIO_SBDS | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(ROCKETLAKE,	X86_STEPPINGS(0x1, 0x1),	MMIO | RETBLEED),
+ 	VULNBL_INTEL_STEPPINGS(ATOM_TREMONT,	X86_STEPPINGS(0x1, 0x1),	MMIO | MMIO_SBDS),
+ 	VULNBL_INTEL_STEPPINGS(ATOM_TREMONT_D,	X86_STEPPING_ANY,		MMIO),
+ 	VULNBL_INTEL_STEPPINGS(ATOM_TREMONT_L,	X86_STEPPINGS(0x0, 0x0),	MMIO | MMIO_SBDS),
+ 
+ 	VULNBL_AMD(0x15, RETBLEED),
+ 	VULNBL_AMD(0x16, RETBLEED),
+ 	VULNBL_AMD(0x17, RETBLEED),
+ 	VULNBL_HYGON(0x18, RETBLEED),
++>>>>>>> 6ad0ad2bf8a6 (x86/bugs: Report Intel retbleed vulnerability)
  	{}
  };
  
@@@ -1123,6 -1364,9 +1153,12 @@@ static void __init cpu_set_bug_bits(str
  	    !arch_cap_mmio_immune(ia32_cap))
  		setup_force_cpu_bug(X86_BUG_MMIO_STALE_DATA);
  
++<<<<<<< HEAD
++=======
+ 	if ((cpu_matches(cpu_vuln_blacklist, RETBLEED) || (ia32_cap & ARCH_CAP_RSBA)))
+ 		setup_force_cpu_bug(X86_BUG_RETBLEED);
+ 
++>>>>>>> 6ad0ad2bf8a6 (x86/bugs: Report Intel retbleed vulnerability)
  	if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
  		return;
  
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 0607fd9388b3..d465bce6dc78 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -69,6 +69,7 @@
 #define MSR_IA32_ARCH_CAPABILITIES	0x0000010a
 #define ARCH_CAP_RDCL_NO		BIT(0)	/* Not susceptible to Meltdown */
 #define ARCH_CAP_IBRS_ALL		BIT(1)	/* Enhanced IBRS support */
+#define ARCH_CAP_RSBA			BIT(2)	/* RET may use alternative branch predictors */
 #define ARCH_CAP_SKIP_VMENTRY_L1DFLUSH	BIT(3)	/* Skip L1D flush on vmentry */
 #define ARCH_CAP_SSB_NO			BIT(4)	/*
 						 * Not susceptible to Speculative Store Bypass
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
