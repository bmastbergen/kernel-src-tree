x86: Undo return-thunk damage

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 15e67227c49a57837108acfe1c80570e1bd9f962
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/15e67227.failed

Introduce X86_FEATURE_RETHUNK for those afflicted with needing this.

  [ bp: Do only INT3 padding - simpler. ]

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit 15e67227c49a57837108acfe1c80570e1bd9f962)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/alternative.h
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/disabled-features.h
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/module.c
#	arch/x86/kernel/vmlinux.lds.S
diff --cc arch/x86/include/asm/alternative.h
index 20d74d2c35da,9542c582d546..000000000000
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@@ -55,6 -75,9 +55,12 @@@ struct alt_instr 
  
  extern void alternative_instructions(void);
  extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);
++<<<<<<< HEAD
++=======
+ extern void apply_retpolines(s32 *start, s32 *end);
+ extern void apply_returns(s32 *start, s32 *end);
+ extern void apply_ibt_endbr(s32 *start, s32 *end);
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  
  struct module;
  
diff --cc arch/x86/include/asm/cpufeatures.h
index 7cf672fd2f56,295e69090fb8..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -220,62 -222,96 +220,90 @@@
  #define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 or above (Zen) */
  #define X86_FEATURE_L1TF_PTEINV		( 7*32+29) /* "" L1TF workaround PTE inversion */
  #define X86_FEATURE_IBRS_ENHANCED	( 7*32+30) /* Enhanced IBRS */
 -#define X86_FEATURE_MSR_IA32_FEAT_CTL	( 7*32+31) /* "" MSR IA32_FEAT_CTL configured */
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
  
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -#define X86_FEATURE_EPT_AD		( 8*32+17) /* Intel Extended Page Table access-dirty bit */
 -#define X86_FEATURE_VMCALL		( 8*32+18) /* "" Hypervisor supports the VMCALL instruction */
 -#define X86_FEATURE_VMW_VMMCALL		( 8*32+19) /* "" VMware prefers VMMCALL hypercall instruction */
 -#define X86_FEATURE_PVUNLOCK		( 8*32+20) /* "" PV unlock function */
 -#define X86_FEATURE_VCPUPREEMPT		( 8*32+21) /* "" PV vcpu_is_preempted function */
 -#define X86_FEATURE_TDX_GUEST		( 8*32+22) /* Intel Trust Domain Extensions Guest */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_SGX			( 9*32+ 2) /* Software Guard Extensions */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_FDP_EXCPTN_ONLY	( 9*32+ 6) /* "" FPU data pointer updated only on x87 exceptions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_ZERO_FCS_FDS	( 9*32+13) /* "" Zero out FPU CS and FPU DS */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 -#define X86_FEATURE_XFD			(10*32+ 4) /* "" eXtended Feature Disabling */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
++<<<<<<< HEAD
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
++=======
+ /*
+  * Extended auxiliary flags: Linux defined - for features scattered in various
+  * CPUID levels like 0xf, etc.
+  *
+  * Reuse free bits when adding new feature flags!
+  */
+ #define X86_FEATURE_CQM_LLC		(11*32+ 0) /* LLC QoS if 1 */
+ #define X86_FEATURE_CQM_OCCUP_LLC	(11*32+ 1) /* LLC occupancy monitoring */
+ #define X86_FEATURE_CQM_MBM_TOTAL	(11*32+ 2) /* LLC Total MBM monitoring */
+ #define X86_FEATURE_CQM_MBM_LOCAL	(11*32+ 3) /* LLC Local MBM monitoring */
+ #define X86_FEATURE_FENCE_SWAPGS_USER	(11*32+ 4) /* "" LFENCE in user entry SWAPGS path */
+ #define X86_FEATURE_FENCE_SWAPGS_KERNEL	(11*32+ 5) /* "" LFENCE in kernel entry SWAPGS path */
+ #define X86_FEATURE_SPLIT_LOCK_DETECT	(11*32+ 6) /* #AC for split lock */
+ #define X86_FEATURE_PER_THREAD_MBA	(11*32+ 7) /* "" Per-thread Memory Bandwidth Allocation */
+ #define X86_FEATURE_SGX1		(11*32+ 8) /* "" Basic SGX */
+ #define X86_FEATURE_SGX2		(11*32+ 9) /* "" SGX Enclave Dynamic Memory Management (EDMM) */
+ /* FREE!				(11*32+10) */
+ /* FREE!				(11*32+11) */
+ #define X86_FEATURE_RETPOLINE		(11*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_RETPOLINE_LFENCE	(11*32+13) /* "" Use LFENCE for Spectre variant 2 */
+ #define X86_FEATURE_RETHUNK		(11*32+14) /* "" Use REturn THUNK */
+ 
+ /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
+ #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
+ #define X86_FEATURE_AVX512_BF16		(12*32+ 5) /* AVX512 BFLOAT16 instructions */
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 -#define X86_FEATURE_RDPRU		(13*32+ 4) /* Read processor register at user level */
 -#define X86_FEATURE_WBNOINVD		(13*32+ 9) /* WBNOINVD instruction */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
  #define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
  #define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
  #define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
diff --cc arch/x86/include/asm/disabled-features.h
index 476b053ef1c1,641c479cca17..000000000000
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@@ -42,6 -38,44 +42,47 @@@
  # define DISABLE_OSPKE		(1<<(X86_FEATURE_OSPKE & 31))
  #endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_5LEVEL
+ # define DISABLE_LA57	0
+ #else
+ # define DISABLE_LA57	(1<<(X86_FEATURE_LA57 & 31))
+ #endif
+ 
+ #ifdef CONFIG_PAGE_TABLE_ISOLATION
+ # define DISABLE_PTI		0
+ #else
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
+ #ifdef CONFIG_RETPOLINE
+ # define DISABLE_RETPOLINE	0
+ #else
+ # define DISABLE_RETPOLINE	((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETPOLINE_LFENCE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETHUNK & 31)))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_IOMMU_SVM
+ # define DISABLE_ENQCMD		0
+ #else
+ # define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
+ #endif
+ 
+ #ifdef CONFIG_X86_SGX
+ # define DISABLE_SGX	0
+ #else
+ # define DISABLE_SGX	(1 << (X86_FEATURE_SGX & 31))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_TDX_GUEST
+ # define DISABLE_TDX_GUEST	0
+ #else
+ # define DISABLE_TDX_GUEST	(1 << (X86_FEATURE_TDX_GUEST & 31))
+ #endif
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  /*
   * Make sure to add features to the correct mask
   */
diff --cc arch/x86/kernel/alternative.c
index 137dcbca27ba,76b745921509..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -229,18 -114,156 +229,24 @@@ static void __init_or_module add_nops(v
  	}
  }
  
++<<<<<<< HEAD
++=======
+ extern s32 __retpoline_sites[], __retpoline_sites_end[];
+ extern s32 __return_sites[], __return_sites_end[];
+ extern s32 __ibt_endbr_seal[], __ibt_endbr_seal_end[];
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
  extern s32 __smp_locks[], __smp_locks_end[];
 -void text_poke_early(void *addr, const void *opcode, size_t len);
 -
 -/*
 - * Are we looking at a near JMP with a 1 or 4-byte displacement.
 - */
 -static inline bool is_jmp(const u8 opcode)
 -{
 -	return opcode == 0xeb || opcode == 0xe9;
 -}
 -
 -static void __init_or_module
 -recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insn_buff)
 -{
 -	u8 *next_rip, *tgt_rip;
 -	s32 n_dspl, o_dspl;
 -	int repl_len;
 -
 -	if (a->replacementlen != 5)
 -		return;
 -
 -	o_dspl = *(s32 *)(insn_buff + 1);
 -
 -	/* next_rip of the replacement JMP */
 -	next_rip = repl_insn + a->replacementlen;
 -	/* target rip of the replacement JMP */
 -	tgt_rip  = next_rip + o_dspl;
 -	n_dspl = tgt_rip - orig_insn;
 -
 -	DPRINTK("target RIP: %px, new_displ: 0x%x", tgt_rip, n_dspl);
 -
 -	if (tgt_rip - orig_insn >= 0) {
 -		if (n_dspl - 2 <= 127)
 -			goto two_byte_jmp;
 -		else
 -			goto five_byte_jmp;
 -	/* negative offset */
 -	} else {
 -		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
 -			goto two_byte_jmp;
 -		else
 -			goto five_byte_jmp;
 -	}
 -
 -two_byte_jmp:
 -	n_dspl -= 2;
 -
 -	insn_buff[0] = 0xeb;
 -	insn_buff[1] = (s8)n_dspl;
 -	add_nops(insn_buff + 2, 3);
 -
 -	repl_len = 2;
 -	goto done;
 -
 -five_byte_jmp:
 -	n_dspl -= 5;
 -
 -	insn_buff[0] = 0xe9;
 -	*(s32 *)&insn_buff[1] = n_dspl;
 -
 -	repl_len = 5;
 -
 -done:
 +void *text_poke_early(void *addr, const void *opcode, size_t len);
  
 -	DPRINTK("final displ: 0x%08x, JMP 0x%lx",
 -		n_dspl, (unsigned long)orig_insn + n_dspl + repl_len);
 -}
 -
 -/*
 - * optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
 - *
 - * @instr: instruction byte stream
 - * @instrlen: length of the above
 - * @off: offset within @instr where the first NOP has been detected
 - *
 - * Return: number of NOPs found (and replaced).
 - */
 -static __always_inline int optimize_nops_range(u8 *instr, u8 instrlen, int off)
 -{
 -	unsigned long flags;
 -	int i = off, nnops;
 -
 -	while (i < instrlen) {
 -		if (instr[i] != 0x90)
 -			break;
 -
 -		i++;
 -	}
 -
 -	nnops = i - off;
 -
 -	if (nnops <= 1)
 -		return nnops;
 -
 -	local_irq_save(flags);
 -	add_nops(instr + off, nnops);
 -	local_irq_restore(flags);
 -
 -	DUMP_BYTES(instr, instrlen, "%px: [%d:%d) optimized NOPs: ", instr, off, i);
 -
 -	return nnops;
 -}
 -
 -/*
 - * "noinline" to cause control flow change and thus invalidate I$ and
 - * cause refetch after modification.
 - */
 -static void __init_or_module noinline optimize_nops(u8 *instr, size_t len)
 -{
 -	struct insn insn;
 -	int i = 0;
 -
 -	/*
 -	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
 -	 * ones.
 -	 */
 -	for (;;) {
 -		if (insn_decode_kernel(&insn, &instr[i]))
 -			return;
 -
 -		/*
 -		 * See if this and any potentially following NOPs can be
 -		 * optimized.
 -		 */
 -		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
 -			i += optimize_nops_range(instr, len, i);
 -		else
 -			i += insn.length;
 -
 -		if (i >= len)
 -			return;
 -	}
 -}
 +/* Replace instructions with better alternatives for this CPU type.
 +   This runs before SMP is initialized to avoid SMP problems with
 +   self modifying code. This implies that asymmetric systems where
 +   APs have less capabilities than the boot processor are not handled.
 +   Tough. Make sure you disable such features by hand. */
  
 -/*
 - * Replace instructions with better alternatives for this CPU type. This runs
 - * before SMP is initialized to avoid SMP problems with self modifying code.
 - * This implies that asymmetric systems where APs have less capabilities than
 - * the boot processor are not handled. Tough. Make sure you disable such
 - * features by hand.
 - *
 - * Marked "noinline" to cause control flow change and thus insn cache
 - * to refetch changed I$ lines.
 - */
 -void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 -						  struct alt_instr *end)
 +void __init_or_module apply_alternatives(struct alt_instr *start,
 +					 struct alt_instr *end)
  {
  	struct alt_instr *a;
  	u8 *instr, *replacement;
@@@ -278,8 -508,107 +284,111 @@@
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Rewrite the compiler generated return thunk tail-calls.
+  *
+  * For example, convert:
+  *
+  *   JMP __x86_return_thunk
+  *
+  * into:
+  *
+  *   RET
+  */
+ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
+ {
+ 	int i = 0;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+ 		return -1;
+ 
+ 	bytes[i++] = RET_INSN_OPCODE;
+ 
+ 	for (; i < insn->length;)
+ 		bytes[i++] = INT3_INSN_OPCODE;
+ 
+ 	return i;
+ }
+ 
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		void *addr = (void *)s + *s;
+ 		struct insn insn;
+ 		int len, ret;
+ 		u8 bytes[16];
+ 		u8 op1;
+ 
+ 		ret = insn_decode_kernel(&insn, addr);
+ 		if (WARN_ON_ONCE(ret < 0))
+ 			continue;
+ 
+ 		op1 = insn.opcode.bytes[0];
+ 		if (WARN_ON_ONCE(op1 != JMP32_INSN_OPCODE))
+ 			continue;
+ 
+ 		DPRINTK("return thunk at: %pS (%px) len: %d to: %pS",
+ 			addr, addr, insn.length,
+ 			addr + insn.length + insn.immediate.value);
+ 
+ 		len = patch_return(addr, &insn, bytes);
+ 		if (len == insn.length) {
+ 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
+ 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+ 			text_poke_early(addr, bytes, len);
+ 		}
+ 	}
+ }
+ #else /* !CONFIG_RETPOLINE || !CONFIG_OBJTOOL */
+ 
+ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end) { }
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_RETPOLINE && CONFIG_OBJTOOL */
+ 
+ #ifdef CONFIG_X86_KERNEL_IBT
+ 
+ /*
+  * Generated by: objtool --ibt
+  */
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		u32 endbr, poison = gen_endbr_poison();
+ 		void *addr = (void *)s + *s;
+ 
+ 		if (WARN_ON_ONCE(get_kernel_nofault(endbr, addr)))
+ 			continue;
+ 
+ 		if (WARN_ON_ONCE(!is_endbr(endbr)))
+ 			continue;
+ 
+ 		DPRINTK("ENDBR at: %pS (%px)", addr, addr);
+ 
+ 		/*
+ 		 * When we have IBT, the lack of ENDBR will trigger #CP
+ 		 */
+ 		DUMP_BYTES(((u8*)addr), 4, "%px: orig: ", addr);
+ 		DUMP_BYTES(((u8*)&poison), 4, "%px: repl: ", addr);
+ 		text_poke_early(addr, &poison, 4);
+ 	}
+ }
+ 
+ #else
+ 
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_X86_KERNEL_IBT */
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  #ifdef CONFIG_SMP
 +
  static void alternatives_smp_lock(const s32 *start, const s32 *end,
  				  u8 *text, u8 *text_end)
  {
@@@ -488,8 -891,44 +597,45 @@@ void __init alternative_instructions(vo
  	 * patching.
  	 */
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Paravirt patching and alternative patching can be combined to
+ 	 * replace a function call with a short direct code sequence (e.g.
+ 	 * by setting a constant return value instead of doing that in an
+ 	 * external function).
+ 	 * In order to make this work the following sequence is required:
+ 	 * 1. set (artificial) features depending on used paravirt
+ 	 *    functions which can later influence alternative patching
+ 	 * 2. apply paravirt patching (generally replacing an indirect
+ 	 *    function call with a direct one)
+ 	 * 3. apply alternative patching (e.g. replacing a direct function
+ 	 *    call with a custom code sequence)
+ 	 * Doing paravirt patching after alternative patching would clobber
+ 	 * the optimization of the custom code with a function call again.
+ 	 */
+ 	paravirt_set_cap();
+ 
+ 	/*
+ 	 * First patch paravirt functions, such that we overwrite the indirect
+ 	 * call with the direct call.
+ 	 */
+ 	apply_paravirt(__parainstructions, __parainstructions_end);
+ 
+ 	/*
+ 	 * Rewrite the retpolines, must be done before alternatives since
+ 	 * those can rewrite the retpoline thunks.
+ 	 */
+ 	apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ 	apply_returns(__return_sites, __return_sites_end);
+ 
+ 	/*
+ 	 * Then patch alternatives, such that those paravirt calls that are in
+ 	 * alternatives can be overwritten by their immediate fragments.
+ 	 */
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	apply_alternatives(__alt_instructions, __alt_instructions_end);
  
 -	apply_ibt_endbr(__ibt_endbr_seal, __ibt_endbr_seal_end);
 -
  #ifdef CONFIG_SMP
  	/* Patch to UP if other cpus not imminent. */
  	if (!noreplace_smp && (num_present_cpus() == 1 || setup_max_cpus <= 1)) {
diff --cc arch/x86/kernel/module.c
index e93a3a4b53c8,67828d973389..000000000000
--- a/arch/x86/kernel/module.c
+++ b/arch/x86/kernel/module.c
@@@ -249,7 -252,8 +249,12 @@@ int module_finalize(const Elf_Ehdr *hdr
  		    struct module *me)
  {
  	const Elf_Shdr *s, *text = NULL, *alt = NULL, *locks = NULL,
++<<<<<<< HEAD
 +		*para = NULL;
++=======
+ 		*para = NULL, *orc = NULL, *orc_ip = NULL,
+ 		*retpolines = NULL, *returns = NULL, *ibt_endbr = NULL;
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
  
  	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
@@@ -261,8 -265,34 +266,39 @@@
  			locks = s;
  		if (!strcmp(".parainstructions", secstrings + s->sh_name))
  			para = s;
++<<<<<<< HEAD
 +	}
 +
++=======
+ 		if (!strcmp(".orc_unwind", secstrings + s->sh_name))
+ 			orc = s;
+ 		if (!strcmp(".orc_unwind_ip", secstrings + s->sh_name))
+ 			orc_ip = s;
+ 		if (!strcmp(".retpoline_sites", secstrings + s->sh_name))
+ 			retpolines = s;
+ 		if (!strcmp(".return_sites", secstrings + s->sh_name))
+ 			returns = s;
+ 		if (!strcmp(".ibt_endbr_seal", secstrings + s->sh_name))
+ 			ibt_endbr = s;
+ 	}
+ 
+ 	/*
+ 	 * See alternative_instructions() for the ordering rules between the
+ 	 * various patching types.
+ 	 */
+ 	if (para) {
+ 		void *pseg = (void *)para->sh_addr;
+ 		apply_paravirt(pseg, pseg + para->sh_size);
+ 	}
+ 	if (retpolines) {
+ 		void *rseg = (void *)retpolines->sh_addr;
+ 		apply_retpolines(rseg, rseg + retpolines->sh_size);
+ 	}
+ 	if (returns) {
+ 		void *rseg = (void *)returns->sh_addr;
+ 		apply_returns(rseg, rseg + returns->sh_size);
+ 	}
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	if (alt) {
  		/* patch .altinstructions */
  		void *aseg = (void *)alt->sh_addr;
diff --cc arch/x86/kernel/vmlinux.lds.S
index dad7e081f43d,ada7eb738113..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -225,6 -271,36 +225,39 @@@ SECTION
  		__parainstructions_end = .;
  	}
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_RETPOLINE
+ 	/*
+ 	 * List of instructions that call/jmp/jcc to retpoline thunks
+ 	 * __x86_indirect_thunk_*(). These instructions can be patched along
+ 	 * with alternatives, after which the section can be freed.
+ 	 */
+ 	. = ALIGN(8);
+ 	.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {
+ 		__retpoline_sites = .;
+ 		*(.retpoline_sites)
+ 		__retpoline_sites_end = .;
+ 	}
+ 
+ 	. = ALIGN(8);
+ 	.return_sites : AT(ADDR(.return_sites) - LOAD_OFFSET) {
+ 		__return_sites = .;
+ 		*(.return_sites)
+ 		__return_sites_end = .;
+ 	}
+ #endif
+ 
+ #ifdef CONFIG_X86_KERNEL_IBT
+ 	. = ALIGN(8);
+ 	.ibt_endbr_seal : AT(ADDR(.ibt_endbr_seal) - LOAD_OFFSET) {
+ 		__ibt_endbr_seal = .;
+ 		*(.ibt_endbr_seal)
+ 		__ibt_endbr_seal_end = .;
+ 	}
+ #endif
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	/*
  	 * struct alt_inst entries. From the header (alternative.h):
  	 * "Alternative instructions for different CPU types or capabilities"
* Unmerged path arch/x86/include/asm/alternative.h
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/disabled-features.h
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/module.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
