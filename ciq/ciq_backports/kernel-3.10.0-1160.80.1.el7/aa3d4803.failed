x86: Use return-thunk in asm code

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit aa3d480315ba6c3025a60958e1981072ea37c3df
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/aa3d4803.failed

Use the return thunk in asm code. If the thunk isn't needed, it will
get patched into a RET instruction during boot by apply_returns().

Since alternatives can't handle relocations outside of the first
instruction, putting a 'jmp __x86_return_thunk' in one is not valid,
therefore carve out the memmove ERMS path into a separate label and jump
to it.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit aa3d480315ba6c3025a60958e1981072ea37c3df)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/Makefile
#	arch/x86/include/asm/linkage.h
#	arch/x86/lib/memmove_64.S
diff --cc arch/x86/include/asm/linkage.h
index 79327e9483a3,e3ae331cabb1..000000000000
--- a/arch/x86/include/asm/linkage.h
+++ b/arch/x86/include/asm/linkage.h
@@@ -55,7 -19,59 +55,32 @@@
  #define __ALIGN_STR	__stringify(__ALIGN)
  #endif
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_RETPOLINE) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define RET	jmp __x86_return_thunk
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define RET	ret; int3
+ #else
+ #define RET	ret
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
+ #else /* __ASSEMBLY__ */
+ 
+ #if defined(CONFIG_RETPOLINE) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define ASM_RET	"jmp __x86_return_thunk\n\t"
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define ASM_RET	"ret; int3\n\t"
+ #else
+ #define ASM_RET	"ret\n\t"
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  #endif /* __ASSEMBLY__ */
  
 -/* SYM_FUNC_START -- use for global functions */
 -#define SYM_FUNC_START(name)				\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_NOALIGN -- use for global functions, w/o alignment */
 -#define SYM_FUNC_START_NOALIGN(name)			\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL -- use for local functions */
 -#define SYM_FUNC_START_LOCAL(name)			\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL_NOALIGN -- use for local functions, w/o alignment */
 -#define SYM_FUNC_START_LOCAL_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK -- use for weak functions */
 -#define SYM_FUNC_START_WEAK(name)			\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK_NOALIGN -- use for weak functions, w/o alignment */
 -#define SYM_FUNC_START_WEAK_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
 -	ENDBR
 -
  #endif /* _ASM_X86_LINKAGE_H */
  
diff --cc arch/x86/lib/memmove_64.S
index 284f9d71e15b,724bbf83eb5b..000000000000
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@@ -40,7 -36,11 +40,13 @@@ ENTRY(memmove
  	cmp %rdi, %r8
  	jg 2f
  
 -	/* FSRM implies ERMS => no length checks, do the copy directly */
  .Lmemmove_begin_forward:
++<<<<<<< HEAD
++=======
+ 	ALTERNATIVE "cmp $0x20, %rdx; jb 1f", "", X86_FEATURE_FSRM
+ 	ALTERNATIVE "", "jmp .Lmemmove_erms", X86_FEATURE_ERMS
+ 
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  	/*
  	 * movsq instruction have many startup latency
  	 * so we handle small size by general register.
@@@ -202,22 -204,14 +208,33 @@@
  	movb (%rsi), %r11b
  	movb %r11b, (%rdi)
  13:
++<<<<<<< HEAD
 +	retq
 +	CFI_ENDPROC
++=======
+ 	RET
+ 
+ .Lmemmove_erms:
+ 	movq %rdx, %rcx
+ 	rep movsb
+ 	RET
+ SYM_FUNC_END(__memmove)
+ EXPORT_SYMBOL(__memmove)
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
 +
 +	.section .altinstr_replacement,"ax"
 +.Lmemmove_begin_forward_efs:
 +	/* Forward moving data. */
 +	movq %rdx, %rcx
 +	rep movsb
 +	retq
 +.Lmemmove_end_forward_efs:
 +	.previous
  
 -SYM_FUNC_ALIAS_WEAK(memmove, __memmove)
 -EXPORT_SYMBOL(memmove)
 +	.section .altinstructions,"a"
 +	altinstruction_entry .Lmemmove_begin_forward,		\
 +		.Lmemmove_begin_forward_efs,X86_FEATURE_ERMS,	\
 +		.Lmemmove_end_forward-.Lmemmove_begin_forward,	\
 +		.Lmemmove_end_forward_efs-.Lmemmove_begin_forward_efs
 +	.previous
 +ENDPROC(memmove)
* Unmerged path arch/x86/entry/vdso/Makefile
* Unmerged path arch/x86/entry/vdso/Makefile
* Unmerged path arch/x86/include/asm/linkage.h
* Unmerged path arch/x86/lib/memmove_64.S
