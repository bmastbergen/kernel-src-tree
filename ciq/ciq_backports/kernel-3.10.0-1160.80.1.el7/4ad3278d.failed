x86/speculation: Disable RRSBA behavior

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit 4ad3278df6fe2b0852b00d5757fc2ccd8e92c26e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/4ad3278d.failed

Some Intel processors may use alternate predictors for RETs on
RSB-underflow. This condition may be vulnerable to Branch History
Injection (BHI) and intramode-BTI.

Kernel earlier added spectre_v2 mitigation modes (eIBRS+Retpolines,
eIBRS+LFENCE, Retpolines) which protect indirect CALLs and JMPs against
such attacks. However, on RSB-underflow, RET target prediction may
fallback to alternate predictors. As a result, RET's predicted target
may get influenced by branch history.

A new MSR_IA32_SPEC_CTRL bit (RRSBA_DIS_S) controls this fallback
behavior when in kernel mode. When set, RETs will not take predictions
from alternate predictors, hence mitigating RETs as well. Support for
this is enumerated by CPUID.7.2.EDX[RRSBA_CTRL] (bit2).

For spectre v2 mitigation, when a user selects a mitigation that
protects indirect CALLs and JMPs against BHI and intramode-BTI, set
RRSBA_DIS_S also to protect RETs for RSB-underflow case.

	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit 4ad3278df6fe2b0852b00d5757fc2ccd8e92c26e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/scattered.c
#	tools/arch/x86/include/asm/msr-index.h
diff --cc arch/x86/include/asm/cpufeatures.h
index 7cf672fd2f56,00f5227c8459..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -220,62 -222,97 +220,91 @@@
  #define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 or above (Zen) */
  #define X86_FEATURE_L1TF_PTEINV		( 7*32+29) /* "" L1TF workaround PTE inversion */
  #define X86_FEATURE_IBRS_ENHANCED	( 7*32+30) /* Enhanced IBRS */
 -#define X86_FEATURE_MSR_IA32_FEAT_CTL	( 7*32+31) /* "" MSR IA32_FEAT_CTL configured */
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
  
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -#define X86_FEATURE_EPT_AD		( 8*32+17) /* Intel Extended Page Table access-dirty bit */
 -#define X86_FEATURE_VMCALL		( 8*32+18) /* "" Hypervisor supports the VMCALL instruction */
 -#define X86_FEATURE_VMW_VMMCALL		( 8*32+19) /* "" VMware prefers VMMCALL hypercall instruction */
 -#define X86_FEATURE_PVUNLOCK		( 8*32+20) /* "" PV unlock function */
 -#define X86_FEATURE_VCPUPREEMPT		( 8*32+21) /* "" PV vcpu_is_preempted function */
 -#define X86_FEATURE_TDX_GUEST		( 8*32+22) /* Intel Trust Domain Extensions Guest */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_SGX			( 9*32+ 2) /* Software Guard Extensions */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_FDP_EXCPTN_ONLY	( 9*32+ 6) /* "" FPU data pointer updated only on x87 exceptions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_ZERO_FCS_FDS	( 9*32+13) /* "" Zero out FPU CS and FPU DS */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 -#define X86_FEATURE_XFD			(10*32+ 4) /* "" eXtended Feature Disabling */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
++<<<<<<< HEAD
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
++=======
+ /*
+  * Extended auxiliary flags: Linux defined - for features scattered in various
+  * CPUID levels like 0xf, etc.
+  *
+  * Reuse free bits when adding new feature flags!
+  */
+ #define X86_FEATURE_CQM_LLC		(11*32+ 0) /* LLC QoS if 1 */
+ #define X86_FEATURE_CQM_OCCUP_LLC	(11*32+ 1) /* LLC occupancy monitoring */
+ #define X86_FEATURE_CQM_MBM_TOTAL	(11*32+ 2) /* LLC Total MBM monitoring */
+ #define X86_FEATURE_CQM_MBM_LOCAL	(11*32+ 3) /* LLC Local MBM monitoring */
+ #define X86_FEATURE_FENCE_SWAPGS_USER	(11*32+ 4) /* "" LFENCE in user entry SWAPGS path */
+ #define X86_FEATURE_FENCE_SWAPGS_KERNEL	(11*32+ 5) /* "" LFENCE in kernel entry SWAPGS path */
+ #define X86_FEATURE_SPLIT_LOCK_DETECT	(11*32+ 6) /* #AC for split lock */
+ #define X86_FEATURE_PER_THREAD_MBA	(11*32+ 7) /* "" Per-thread Memory Bandwidth Allocation */
+ #define X86_FEATURE_SGX1		(11*32+ 8) /* "" Basic SGX */
+ #define X86_FEATURE_SGX2		(11*32+ 9) /* "" SGX Enclave Dynamic Memory Management (EDMM) */
+ #define X86_FEATURE_ENTRY_IBPB		(11*32+10) /* "" Issue an IBPB on kernel entry */
+ #define X86_FEATURE_RRSBA_CTRL		(11*32+11) /* "" RET prediction control */
+ #define X86_FEATURE_RETPOLINE		(11*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_RETPOLINE_LFENCE	(11*32+13) /* "" Use LFENCE for Spectre variant 2 */
+ #define X86_FEATURE_RETHUNK		(11*32+14) /* "" Use REturn THUNK */
+ #define X86_FEATURE_UNRET		(11*32+15) /* "" AMD BTB untrain return */
+ 
+ /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
+ #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
+ #define X86_FEATURE_AVX512_BF16		(12*32+ 5) /* AVX512 BFLOAT16 instructions */
++>>>>>>> 4ad3278df6fe (x86/speculation: Disable RRSBA behavior)
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 -#define X86_FEATURE_RDPRU		(13*32+ 4) /* Read processor register at user level */
 -#define X86_FEATURE_WBNOINVD		(13*32+ 9) /* WBNOINVD instruction */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
  #define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
  #define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
  #define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
diff --cc arch/x86/kernel/cpu/bugs.c
index a7e93ef81745,0dd04713434b..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -732,17 -1306,39 +732,44 @@@ static enum spectre_v2_mitigation_cmd _
  	return cmd;
  }
  
 -static enum spectre_v2_mitigation __init spectre_v2_select_retpoline(void)
 +void __spectre_v2_select_mitigation(void)
  {
 -	if (!IS_ENABLED(CONFIG_RETPOLINE)) {
 -		pr_err("Kernel not compiled with retpoline; no mitigation available!");
 -		return SPECTRE_V2_NONE;
 +	const bool full_retpoline = IS_ENABLED(CONFIG_RETPOLINE) && retp_compiler();
 +	enum spectre_v2_mitigation_cmd cmd = spectre_v2_cmd;
 +
 +	/* Initialize Indirect Branch Prediction Barrier if supported */
 +	if (boot_cpu_has(X86_FEATURE_IBPB)) {
 +		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 +		pr_info("Enabling Indirect Branch Prediction Barrier\n");
  	}
  
++<<<<<<< HEAD
++=======
+ 	return SPECTRE_V2_RETPOLINE;
+ }
+ 
+ /* Disable in-kernel use of non-RSB RET predictors */
+ static void __init spec_ctrl_disable_kernel_rrsba(void)
+ {
+ 	u64 ia32_cap;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RRSBA_CTRL))
+ 		return;
+ 
+ 	ia32_cap = x86_read_arch_cap_msr();
+ 
+ 	if (ia32_cap & ARCH_CAP_RRSBA) {
+ 		x86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;
+ 		write_spec_ctrl_current(x86_spec_ctrl_base, true);
+ 	}
+ }
+ 
+ static void __init spectre_v2_select_mitigation(void)
+ {
+ 	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
+ 	enum spectre_v2_mitigation mode = SPECTRE_V2_NONE;
+ 
++>>>>>>> 4ad3278df6fe (x86/speculation: Disable RRSBA behavior)
  	/*
  	 * If the CPU is not affected and the command line mode is NONE or AUTO
  	 * then nothing to do.
@@@ -764,39 -1369,192 +791,202 @@@
  		break;
  
  	case SPECTRE_V2_CMD_RETPOLINE:
++<<<<<<< HEAD
 +		spec_ctrl_enable_retpoline();
++=======
+ 		mode = spectre_v2_select_retpoline();
+ 		break;
+ 
+ 	case SPECTRE_V2_CMD_IBRS:
+ 		mode = SPECTRE_V2_IBRS;
+ 		break;
+ 
+ 	case SPECTRE_V2_CMD_EIBRS:
+ 		mode = SPECTRE_V2_EIBRS;
+ 		break;
+ 
+ 	case SPECTRE_V2_CMD_EIBRS_LFENCE:
+ 		mode = SPECTRE_V2_EIBRS_LFENCE;
+ 		break;
+ 
+ 	case SPECTRE_V2_CMD_EIBRS_RETPOLINE:
+ 		mode = SPECTRE_V2_EIBRS_RETPOLINE;
+ 		break;
+ 	}
+ 
+ 	if (mode == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())
+ 		pr_err(SPECTRE_V2_EIBRS_EBPF_MSG);
+ 
+ 	if (spectre_v2_in_ibrs_mode(mode)) {
+ 		x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ 		write_spec_ctrl_current(x86_spec_ctrl_base, true);
+ 	}
+ 
+ 	switch (mode) {
+ 	case SPECTRE_V2_NONE:
+ 	case SPECTRE_V2_EIBRS:
+ 		break;
+ 
+ 	case SPECTRE_V2_IBRS:
+ 		setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+ 		break;
+ 
+ 	case SPECTRE_V2_LFENCE:
+ 	case SPECTRE_V2_EIBRS_LFENCE:
+ 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+ 		fallthrough;
+ 
+ 	case SPECTRE_V2_RETPOLINE:
+ 	case SPECTRE_V2_EIBRS_RETPOLINE:
+ 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * Disable alternate RSB predictions in kernel when indirect CALLs and
+ 	 * JMPs gets protection against BHI and Intramode-BTI, but RET
+ 	 * prediction from a non-RSB predictor is still a risk.
+ 	 */
+ 	if (mode == SPECTRE_V2_EIBRS_LFENCE ||
+ 	    mode == SPECTRE_V2_EIBRS_RETPOLINE ||
+ 	    mode == SPECTRE_V2_RETPOLINE)
+ 		spec_ctrl_disable_kernel_rrsba();
+ 
+ 	spectre_v2_enabled = mode;
+ 	pr_info("%s\n", spectre_v2_strings[mode]);
+ 
+ 	/*
+ 	 * If Spectre v2 protection has been enabled, fill the RSB during a
+ 	 * context switch.  In general there are two types of RSB attacks
+ 	 * across context switches, for which the CALLs/RETs may be unbalanced.
+ 	 *
+ 	 * 1) RSB underflow
+ 	 *
+ 	 *    Some Intel parts have "bottomless RSB".  When the RSB is empty,
+ 	 *    speculated return targets may come from the branch predictor,
+ 	 *    which could have a user-poisoned BTB or BHB entry.
+ 	 *
+ 	 *    AMD has it even worse: *all* returns are speculated from the BTB,
+ 	 *    regardless of the state of the RSB.
+ 	 *
+ 	 *    When IBRS or eIBRS is enabled, the "user -> kernel" attack
+ 	 *    scenario is mitigated by the IBRS branch prediction isolation
+ 	 *    properties, so the RSB buffer filling wouldn't be necessary to
+ 	 *    protect against this type of attack.
+ 	 *
+ 	 *    The "user -> user" attack scenario is mitigated by RSB filling.
+ 	 *
+ 	 * 2) Poisoned RSB entry
+ 	 *
+ 	 *    If the 'next' in-kernel return stack is shorter than 'prev',
+ 	 *    'next' could be tricked into speculating with a user-poisoned RSB
+ 	 *    entry.
+ 	 *
+ 	 *    The "user -> kernel" attack scenario is mitigated by SMEP and
+ 	 *    eIBRS.
+ 	 *
+ 	 *    The "user -> user" scenario, also known as SpectreBHB, requires
+ 	 *    RSB clearing.
+ 	 *
+ 	 * So to mitigate all cases, unconditionally fill RSB on context
+ 	 * switches.
+ 	 *
+ 	 * FIXME: Is this pointless for retbleed-affected AMD?
+ 	 */
+ 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+ 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
+ 
+ 	/*
+ 	 * Similar to context switches, there are two types of RSB attacks
+ 	 * after vmexit:
+ 	 *
+ 	 * 1) RSB underflow
+ 	 *
+ 	 * 2) Poisoned RSB entry
+ 	 *
+ 	 * When retpoline is enabled, both are mitigated by filling/clearing
+ 	 * the RSB.
+ 	 *
+ 	 * When IBRS is enabled, while #1 would be mitigated by the IBRS branch
+ 	 * prediction isolation protections, RSB still needs to be cleared
+ 	 * because of #2.  Note that SMEP provides no protection here, unlike
+ 	 * user-space-poisoned RSB entries.
+ 	 *
+ 	 * eIBRS, on the other hand, has RSB-poisoning protections, so it
+ 	 * doesn't need RSB clearing after vmexit.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) ||
+ 	    boot_cpu_has(X86_FEATURE_KERNEL_IBRS))
+ 		setup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT);
+ 
+ 	/*
+ 	 * Retpoline protects the kernel, but doesn't protect firmware.  IBRS
+ 	 * and Enhanced IBRS protect firmware too, so enable IBRS around
+ 	 * firmware calls only when IBRS / Enhanced IBRS aren't otherwise
+ 	 * enabled.
+ 	 *
+ 	 * Use "mode" to check Enhanced IBRS instead of boot_cpu_has(), because
+ 	 * the user might select retpoline on the kernel command line and if
+ 	 * the CPU supports Enhanced IBRS, kernel might un-intentionally not
+ 	 * enable IBRS around firmware calls.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_IBRS) && !spectre_v2_in_ibrs_mode(mode)) {
+ 		setup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);
+ 		pr_info("Enabling Restricted Speculation for firmware calls\n");
+ 	}
+ 
+ 	/* Set up IBPB and STIBP depending on the general spectre V2 command */
+ 	spectre_v2_cmd = cmd;
+ }
+ 
+ static void update_stibp_msr(void * __unused)
+ {
+ 	u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ 	write_spec_ctrl_current(val, true);
+ }
+ 
+ /* Update x86_spec_ctrl_base in case SMT state changed. */
+ static void update_stibp_strict(void)
+ {
+ 	u64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;
+ 
+ 	if (sched_smt_active())
+ 		mask |= SPEC_CTRL_STIBP;
+ 
+ 	if (mask == x86_spec_ctrl_base)
++>>>>>>> 4ad3278df6fe (x86/speculation: Disable RRSBA behavior)
  		return;
  
 -	pr_info("Update user space SMT mitigation: STIBP %s\n",
 -		mask & SPEC_CTRL_STIBP ? "always-on" : "off");
 -	x86_spec_ctrl_base = mask;
 -	on_each_cpu(update_stibp_msr, NULL, 1);
 +	case SPECTRE_V2_CMD_IBRS:
 +		if (spec_ctrl_force_enable_ibrs())
 +			return;
 +		break;
 +
 +	case SPECTRE_V2_CMD_IBRS_ALWAYS:
 +		if (spec_ctrl_enable_ibrs_always() ||
 +		    spec_ctrl_force_enable_ibp_disabled())
 +			return;
 +		break;
 +
 +	case SPECTRE_V2_CMD_RETPOLINE_IBRS_USER:
 +		if (spec_ctrl_enable_retpoline_ibrs_user())
 +			return;
 +		break;
 +	}
 +
 +	if (spec_ctrl_cond_enable_ibrs(full_retpoline))
 +		return;
 +
 +	if (spec_ctrl_cond_enable_ibp_disabled())
 +		return;
 +
 +	spec_ctrl_enable_retpoline();
  }
  
 -/* Update the static key controlling the evaluation of TIF_SPEC_IB */
 -static void update_indir_branch_cond(void)
 +void spectre_v2_print_mitigation(void)
  {
 -	if (sched_smt_active())
 -		static_branch_enable(&switch_to_cond_stibp);
 -	else
 -		static_branch_disable(&switch_to_cond_stibp);
 +
 +	pr_info("%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
  }
  
  #undef pr_fmt
diff --cc arch/x86/kernel/cpu/scattered.c
index 907fb85a842f,fd44b54c90d5..000000000000
--- a/arch/x86/kernel/cpu/scattered.c
+++ b/arch/x86/kernel/cpu/scattered.c
@@@ -17,24 -18,36 +17,49 @@@ struct cpuid_bit 
  	u32 sub_leaf;
  };
  
 -/*
 - * Please keep the leaf sorted by cpuid_bit.level for faster search.
 - * X86_FEATURE_MBA is supported by both Intel and AMD. But the CPUID
 - * levels are different and there is a separate entry for each.
 - */
 +/* Please keep the leaf sorted by cpuid_bit.level for faster search. */
  static const struct cpuid_bit cpuid_bits[] = {
++<<<<<<< HEAD
 +	{ X86_FEATURE_APERFMPERF,	CPUID_ECX, 0, 0x00000006, 0 },
 +	{ X86_FEATURE_EPB,		CPUID_ECX, 3, 0x00000006, 0 },
 +	{ X86_FEATURE_INTEL_PT,		CPUID_EBX,25, 0x00000007, 0 },
 +	{ X86_FEATURE_CAT_L3,		CPUID_EBX, 1, 0x00000010, 0 },
 +	{ X86_FEATURE_CAT_L2,		CPUID_EBX, 2, 0x00000010, 0 },
 +	{ X86_FEATURE_CDP_L3,		CPUID_ECX, 2, 0x00000010, 1 },
 +	{ X86_FEATURE_CDP_L2,		CPUID_ECX, 2, 0x00000010, 2 },
 +	{ X86_FEATURE_MBA,		CPUID_EBX, 3, 0x00000010, 0 },
 +	{ X86_FEATURE_HW_PSTATE,	CPUID_EDX, 7, 0x80000007, 0 },
 +	{ X86_FEATURE_CPB,		CPUID_EDX, 9, 0x80000007, 0 },
 +	{ X86_FEATURE_PROC_FEEDBACK,	CPUID_EDX,11, 0x80000007, 0 },
 +	{ X86_FEATURE_SME,		CPUID_EAX, 0, 0x8000001f, 0 },
 +	{ 0, 0, 0, 0 }
++=======
+ 	{ X86_FEATURE_APERFMPERF,       CPUID_ECX,  0, 0x00000006, 0 },
+ 	{ X86_FEATURE_EPB,		CPUID_ECX,  3, 0x00000006, 0 },
+ 	{ X86_FEATURE_INTEL_PPIN,	CPUID_EBX,  0, 0x00000007, 1 },
+ 	{ X86_FEATURE_RRSBA_CTRL,	CPUID_EDX,  2, 0x00000007, 2 },
+ 	{ X86_FEATURE_CQM_LLC,		CPUID_EDX,  1, 0x0000000f, 0 },
+ 	{ X86_FEATURE_CQM_OCCUP_LLC,	CPUID_EDX,  0, 0x0000000f, 1 },
+ 	{ X86_FEATURE_CQM_MBM_TOTAL,	CPUID_EDX,  1, 0x0000000f, 1 },
+ 	{ X86_FEATURE_CQM_MBM_LOCAL,	CPUID_EDX,  2, 0x0000000f, 1 },
+ 	{ X86_FEATURE_CAT_L3,		CPUID_EBX,  1, 0x00000010, 0 },
+ 	{ X86_FEATURE_CAT_L2,		CPUID_EBX,  2, 0x00000010, 0 },
+ 	{ X86_FEATURE_CDP_L3,		CPUID_ECX,  2, 0x00000010, 1 },
+ 	{ X86_FEATURE_CDP_L2,		CPUID_ECX,  2, 0x00000010, 2 },
+ 	{ X86_FEATURE_MBA,		CPUID_EBX,  3, 0x00000010, 0 },
+ 	{ X86_FEATURE_PER_THREAD_MBA,	CPUID_ECX,  0, 0x00000010, 3 },
+ 	{ X86_FEATURE_SGX1,		CPUID_EAX,  0, 0x00000012, 0 },
+ 	{ X86_FEATURE_SGX2,		CPUID_EAX,  1, 0x00000012, 0 },
+ 	{ X86_FEATURE_HW_PSTATE,	CPUID_EDX,  7, 0x80000007, 0 },
+ 	{ X86_FEATURE_CPB,		CPUID_EDX,  9, 0x80000007, 0 },
+ 	{ X86_FEATURE_PROC_FEEDBACK,    CPUID_EDX, 11, 0x80000007, 0 },
+ 	{ X86_FEATURE_MBA,		CPUID_EBX,  6, 0x80000008, 0 },
+ 	{ X86_FEATURE_PERFMON_V2,	CPUID_EAX,  0, 0x80000022, 0 },
+ 	{ 0, 0, 0, 0, 0 }
++>>>>>>> 4ad3278df6fe (x86/speculation: Disable RRSBA behavior)
  };
  
 +
  void init_scattered_cpuid_features(struct cpuinfo_x86 *c)
  {
  	u32 max_level;
* Unmerged path tools/arch/x86/include/asm/msr-index.h
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 0607fd9388b3..3160903dd3d0 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -43,6 +43,8 @@
 #define SPEC_CTRL_STIBP			BIT(SPEC_CTRL_STIBP_SHIFT)	/* STIBP mask */
 #define SPEC_CTRL_SSBD_SHIFT		2	   /* Speculative Store Bypass Disable bit */
 #define SPEC_CTRL_SSBD			BIT(SPEC_CTRL_SSBD_SHIFT)	/* Speculative Store Bypass Disable */
+#define SPEC_CTRL_RRSBA_DIS_S_SHIFT	6	   /* Disable RRSBA behavior */
+#define SPEC_CTRL_RRSBA_DIS_S		BIT(SPEC_CTRL_RRSBA_DIS_S_SHIFT)
 
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
@@ -116,6 +118,13 @@
 						 * bit available to control VERW
 						 * behavior.
 						 */
+#define ARCH_CAP_RRSBA			BIT(19)	/*
+						 * Indicates RET may use predictors
+						 * other than the RSB. With eIBRS
+						 * enabled predictions in kernel mode
+						 * are restricted to targets in
+						 * kernel.
+						 */
 
 #define MSR_IA32_FLUSH_CMD		0x0000010b
 #define L1D_FLUSH			BIT(0)	/*
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/scattered.c
* Unmerged path tools/arch/x86/include/asm/msr-index.h
