x86: Add magic AMD return-thunk

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit a149180fbcf336e97ce4eb2cdc13672727feb94d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/a149180f.failed

Note: needs to be in a section distinct from Retpolines such that the
Retpoline RET substitution cannot possibly use immediate jumps.

ORC unwinding for zen_untrain_ret() and __x86_return_thunk() is a
little tricky but works due to the fact that zen_untrain_ret() doesn't
have any stack ops and as such will emit a single ORC entry at the
start (+0x3f).

Meanwhile, unwinding an IP, including the __x86_return_thunk() one
(+0x40) will search for the largest ORC entry smaller or equal to the
IP, these will find the one ORC entry (+0x3f) and all works.

  [ Alexandre: SVM part. ]
  [ bp: Build fix, massages. ]

	Suggested-by: Andrew Cooper <Andrew.Cooper3@citrix.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit a149180fbcf336e97ce4eb2cdc13672727feb94d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/entry_64.S
#	arch/x86/entry/entry_64_compat.S
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/disabled-features.h
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/kvm/svm/vmenter.S
#	arch/x86/lib/retpoline.S
#	tools/objtool/check.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 7cf672fd2f56,fa3d0db1470e..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -220,62 -222,97 +220,91 @@@
  #define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 or above (Zen) */
  #define X86_FEATURE_L1TF_PTEINV		( 7*32+29) /* "" L1TF workaround PTE inversion */
  #define X86_FEATURE_IBRS_ENHANCED	( 7*32+30) /* Enhanced IBRS */
 -#define X86_FEATURE_MSR_IA32_FEAT_CTL	( 7*32+31) /* "" MSR IA32_FEAT_CTL configured */
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
  
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -#define X86_FEATURE_EPT_AD		( 8*32+17) /* Intel Extended Page Table access-dirty bit */
 -#define X86_FEATURE_VMCALL		( 8*32+18) /* "" Hypervisor supports the VMCALL instruction */
 -#define X86_FEATURE_VMW_VMMCALL		( 8*32+19) /* "" VMware prefers VMMCALL hypercall instruction */
 -#define X86_FEATURE_PVUNLOCK		( 8*32+20) /* "" PV unlock function */
 -#define X86_FEATURE_VCPUPREEMPT		( 8*32+21) /* "" PV vcpu_is_preempted function */
 -#define X86_FEATURE_TDX_GUEST		( 8*32+22) /* Intel Trust Domain Extensions Guest */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_SGX			( 9*32+ 2) /* Software Guard Extensions */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_FDP_EXCPTN_ONLY	( 9*32+ 6) /* "" FPU data pointer updated only on x87 exceptions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_ZERO_FCS_FDS	( 9*32+13) /* "" Zero out FPU CS and FPU DS */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 -#define X86_FEATURE_XFD			(10*32+ 4) /* "" eXtended Feature Disabling */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
++<<<<<<< HEAD
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
++=======
+ /*
+  * Extended auxiliary flags: Linux defined - for features scattered in various
+  * CPUID levels like 0xf, etc.
+  *
+  * Reuse free bits when adding new feature flags!
+  */
+ #define X86_FEATURE_CQM_LLC		(11*32+ 0) /* LLC QoS if 1 */
+ #define X86_FEATURE_CQM_OCCUP_LLC	(11*32+ 1) /* LLC occupancy monitoring */
+ #define X86_FEATURE_CQM_MBM_TOTAL	(11*32+ 2) /* LLC Total MBM monitoring */
+ #define X86_FEATURE_CQM_MBM_LOCAL	(11*32+ 3) /* LLC Local MBM monitoring */
+ #define X86_FEATURE_FENCE_SWAPGS_USER	(11*32+ 4) /* "" LFENCE in user entry SWAPGS path */
+ #define X86_FEATURE_FENCE_SWAPGS_KERNEL	(11*32+ 5) /* "" LFENCE in kernel entry SWAPGS path */
+ #define X86_FEATURE_SPLIT_LOCK_DETECT	(11*32+ 6) /* #AC for split lock */
+ #define X86_FEATURE_PER_THREAD_MBA	(11*32+ 7) /* "" Per-thread Memory Bandwidth Allocation */
+ #define X86_FEATURE_SGX1		(11*32+ 8) /* "" Basic SGX */
+ #define X86_FEATURE_SGX2		(11*32+ 9) /* "" SGX Enclave Dynamic Memory Management (EDMM) */
+ /* FREE!				(11*32+10) */
+ /* FREE!				(11*32+11) */
+ #define X86_FEATURE_RETPOLINE		(11*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_RETPOLINE_LFENCE	(11*32+13) /* "" Use LFENCE for Spectre variant 2 */
+ #define X86_FEATURE_RETHUNK		(11*32+14) /* "" Use REturn THUNK */
+ #define X86_FEATURE_UNRET		(11*32+15) /* "" AMD BTB untrain return */
+ 
+ /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
+ #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
+ #define X86_FEATURE_AVX512_BF16		(12*32+ 5) /* AVX512 BFLOAT16 instructions */
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 -#define X86_FEATURE_RDPRU		(13*32+ 4) /* Read processor register at user level */
 -#define X86_FEATURE_WBNOINVD		(13*32+ 9) /* WBNOINVD instruction */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
  #define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
  #define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
  #define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
diff --cc arch/x86/include/asm/disabled-features.h
index 476b053ef1c1,db75da511a36..000000000000
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@@ -42,6 -38,45 +42,48 @@@
  # define DISABLE_OSPKE		(1<<(X86_FEATURE_OSPKE & 31))
  #endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_5LEVEL
+ # define DISABLE_LA57	0
+ #else
+ # define DISABLE_LA57	(1<<(X86_FEATURE_LA57 & 31))
+ #endif
+ 
+ #ifdef CONFIG_PAGE_TABLE_ISOLATION
+ # define DISABLE_PTI		0
+ #else
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
+ #ifdef CONFIG_RETPOLINE
+ # define DISABLE_RETPOLINE	0
+ #else
+ # define DISABLE_RETPOLINE	((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETPOLINE_LFENCE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETHUNK & 31)) | \
+ 				 (1 << (X86_FEATURE_UNRET & 31)))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_IOMMU_SVM
+ # define DISABLE_ENQCMD		0
+ #else
+ # define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
+ #endif
+ 
+ #ifdef CONFIG_X86_SGX
+ # define DISABLE_SGX	0
+ #else
+ # define DISABLE_SGX	(1 << (X86_FEATURE_SGX & 31))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_TDX_GUEST
+ # define DISABLE_TDX_GUEST	0
+ #else
+ # define DISABLE_TDX_GUEST	(1 << (X86_FEATURE_TDX_GUEST & 31))
+ #endif
+ 
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  /*
   * Make sure to add features to the correct mask
   */
diff --cc arch/x86/include/asm/nospec-branch.h
index 7f927c4692a2,5ca60ae0d14f..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -139,44 -91,113 +139,84 @@@
  .endm
  
  .macro CALL_NOSPEC reg:req
 -#ifdef CONFIG_RETPOLINE
 -	ALTERNATIVE_2 __stringify(ANNOTATE_RETPOLINE_SAFE; call *%\reg), \
 -		      __stringify(call __x86_indirect_thunk_\reg), X86_FEATURE_RETPOLINE, \
 -		      __stringify(lfence; ANNOTATE_RETPOLINE_SAFE; call *%\reg), X86_FEATURE_RETPOLINE_LFENCE
 -#else
 -	call	*%\reg
 -#endif
 +	STATIC_JUMP .Lretp_\@, retp_enabled_key
 +	call *\reg
 +	jmp	.Ldone_\@
 +
 +.Lretp_\@:
 +	__CALL_NOSPEC \reg
 +
 +.Ldone_\@:
  .endm
  
 - /*
 -  * A simpler FILL_RETURN_BUFFER macro. Don't make people use the CPP
 -  * monstrosity above, manually.
 -  */
 -.macro FILL_RETURN_BUFFER reg:req nr:req ftr:req
 -#ifdef CONFIG_RETPOLINE
 -	ALTERNATIVE "jmp .Lskip_rsb_\@", "", \ftr
 -	__FILL_RETURN_BUFFER(\reg,\nr,%_ASM_SP)
 -.Lskip_rsb_\@:
 -#endif
 +/*
 + * MDS_USER_CLEAR_CPU_BUFFERS macro is the assembly equivalent of
 + * mds_user_clear_cpu_buffers(). Like the C version, the __KERNEL_DS
 + * is used for verw.
 + * Note: The ZF flag will be clobbered after calling this macro.
 + */
 +.macro MDS_USER_CLEAR_CPU_BUFFERS
 +	STATIC_JUMP .Lverw_\@, mds_user_clear
 +	jmp	.Ldone_\@
 +	.balign 2
 +.Lds_\@:
 +	.word	__KERNEL_DS
 +.Lverw_\@:
 +	verw	.Lds_\@(%rip)
 +.Ldone_\@:
  .endm
  
+ /*
+  * Mitigate RETBleed for AMD/Hygon Zen uarch. Requires KERNEL CR3 because the
+  * return thunk isn't mapped into the userspace tables (then again, AMD
+  * typically has NO_MELTDOWN).
+  *
+  * Doesn't clobber any registers but does require a stable stack.
+  *
+  * As such, this must be placed after every *SWITCH_TO_KERNEL_CR3 at a point
+  * where we have a stack but before any RET instruction.
+  */
+ .macro UNTRAIN_RET
+ #ifdef CONFIG_RETPOLINE
+ 	ALTERNATIVE "", "call zen_untrain_ret", X86_FEATURE_UNRET
+ #endif
+ .endm
+ 
  #else /* __ASSEMBLY__ */
  
++<<<<<<< HEAD
 +#if defined(CONFIG_X86_64) && defined(RETPOLINE)
++=======
+ #define ANNOTATE_RETPOLINE_SAFE					\
+ 	"999:\n\t"						\
+ 	".pushsection .discard.retpoline_safe\n\t"		\
+ 	_ASM_PTR " 999b\n\t"					\
+ 	".popsection\n\t"
+ 
+ typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
+ extern retpoline_thunk_t __x86_indirect_thunk_array[];
+ 
+ extern void __x86_return_thunk(void);
+ extern void zen_untrain_ret(void);
+ 
+ #ifdef CONFIG_RETPOLINE
+ 
+ #define GEN(reg) \
+ 	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ #ifdef CONFIG_X86_64
+ 
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  /*
 - * Inline asm uses the %V modifier which is only in newer GCC
 - * which is ensured when CONFIG_RETPOLINE is defined.
 + * Since the inline asm uses the %V modifier which is only in newer GCC,
 + * the 64-bit one is dependent on RETPOLINE not CONFIG_RETPOLINE.
   */
 -# define CALL_NOSPEC						\
 -	ALTERNATIVE_2(						\
 -	ANNOTATE_RETPOLINE_SAFE					\
 -	"call *%[thunk_target]\n",				\
 -	"call __x86_indirect_thunk_%V[thunk_target]\n",		\
 -	X86_FEATURE_RETPOLINE,					\
 -	"lfence;\n"						\
 -	ANNOTATE_RETPOLINE_SAFE					\
 -	"call *%[thunk_target]\n",				\
 -	X86_FEATURE_RETPOLINE_LFENCE)
 -
 -# define THUNK_TARGET(addr) [thunk_target] "r" (addr)
 -
 -#else /* CONFIG_X86_32 */
 -/*
 - * For i386 we use the original ret-equivalent retpoline, because
 - * otherwise we'll run out of registers. We don't care about CET
 - * here, anyway.
 - */
 -# define CALL_NOSPEC						\
 -	ALTERNATIVE_2(						\
 -	ANNOTATE_RETPOLINE_SAFE					\
 -	"call *%[thunk_target]\n",				\
 -	"       jmp    904f;\n"					\
 -	"       .align 16\n"					\
 -	"901:	call   903f;\n"					\
 -	"902:	pause;\n"					\
 -	"    	lfence;\n"					\
 -	"       jmp    902b;\n"					\
 -	"       .align 16\n"					\
 -	"903:	lea    4(%%esp), %%esp;\n"			\
 -	"       pushl  %[thunk_target];\n"			\
 -	"       ret;\n"						\
 -	"       .align 16\n"					\
 -	"904:	call   901b;\n",				\
 -	X86_FEATURE_RETPOLINE,					\
 -	"lfence;\n"						\
 -	ANNOTATE_RETPOLINE_SAFE					\
 -	"call *%[thunk_target]\n",				\
 -	X86_FEATURE_RETPOLINE_LFENCE)
 +#define CALL_NOSPEC						\
 +	"call __x86_indirect_thunk_%V[thunk_target]\n"
 +#define THUNK_TARGET(addr) [thunk_target] "r" (addr)
  
 -# define THUNK_TARGET(addr) [thunk_target] "rm" (addr)
 -#endif
  #else /* No retpoline for C / inline asm */
  # define CALL_NOSPEC "call *%[thunk_target]\n"
  # define THUNK_TARGET(addr) [thunk_target] "rm" (addr)
diff --cc arch/x86/kernel/vmlinux.lds.S
index dad7e081f43d,c375420036fb..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -106,30 -131,28 +106,39 @@@ SECTION
  		SCHED_TEXT
  		CPUIDLE_TEXT
  		LOCK_TEXT
 +		ALIGN_KAISER()
  		KPROBES_TEXT
 -		ALIGN_ENTRY_TEXT_BEGIN
  		ENTRY_TEXT
 -		ALIGN_ENTRY_TEXT_END
 -		SOFTIRQENTRY_TEXT
 -		STATIC_CALL_TEXT
 +		IRQENTRY_TEXT
 +		ALIGN_KAISER()
 +		*(.fixup)
  		*(.gnu.warning)
 +		/* End of text section */
 +		_etext = .;
 +	} :text = 0x9090
  
++<<<<<<< HEAD
 +	NOTES :text :note
++=======
+ #ifdef CONFIG_RETPOLINE
+ 		__indirect_thunk_start = .;
+ 		*(.text.__x86.*)
+ 		__indirect_thunk_end = .;
+ #endif
+ 	} :text =0xcccc
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
 -	/* End of text section, which should occupy whole number of pages */
 -	_etext = .;
 -	. = ALIGN(PAGE_SIZE);
 +	EXCEPTION_TABLE(16) :text = 0x9090
 +
 +	MC_EXCEPTION_TABLE(16) :text = 0x9090
  
 -	X86_ALIGN_RODATA_BEGIN
 +#if defined(CONFIG_DEBUG_RODATA)
 +	/* .text should occupy whole number of pages */
 +	. = ALIGN(PAGE_SIZE);
 +#endif
 +	X64_ALIGN_DEBUG_RODATA_BEGIN
  	RO_DATA(PAGE_SIZE)
 -	X86_ALIGN_RODATA_END
 +	X64_ALIGN_DEBUG_RODATA_END
  
  	/* Data */
  	.data : AT(ADDR(.data) - LOAD_OFFSET) {
diff --cc arch/x86/lib/retpoline.S
index b3bec3619100,fdd16163b996..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -21,23 -45,94 +21,111 @@@ ENDPROC(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 +#define GENERATE_THUNK(reg) THUNK reg
 +
++<<<<<<< HEAD
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
 +#endif
++=======
+ #define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
+ #define EXPORT_THUNK(reg)	__EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_START(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) THUNK reg
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_END(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) EXPORT_THUNK(reg)
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ /*
+  * This function name is magical and is used by -mfunction-return=thunk-extern
+  * for the compiler to generate JMPs to it.
+  */
+ 	.section .text.__x86.return_thunk
+ 
+ /*
+  * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
+  * 1) The RET at __x86_return_thunk must be on a 64 byte boundary, for
+  *    alignment within the BTB.
+  * 2) The instruction at zen_untrain_ret must contain, and not
+  *    end with, the 0xc3 byte of the RET.
+  * 3) STIBP must be enabled, or SMT disabled, to prevent the sibling thread
+  *    from re-poisioning the BTB prediction.
+  */
+ 	.align 64
+ 	.skip 63, 0xcc
+ SYM_FUNC_START_NOALIGN(zen_untrain_ret);
+ 
+ 	/*
+ 	 * As executed from zen_untrain_ret, this is:
+ 	 *
+ 	 *   TEST $0xcc, %bl
+ 	 *   LFENCE
+ 	 *   JMP __x86_return_thunk
+ 	 *
+ 	 * Executing the TEST instruction has a side effect of evicting any BTB
+ 	 * prediction (potentially attacker controlled) attached to the RET, as
+ 	 * __x86_return_thunk + 1 isn't an instruction boundary at the moment.
+ 	 */
+ 	.byte	0xf6
+ 
+ 	/*
+ 	 * As executed from __x86_return_thunk, this is a plain RET.
+ 	 *
+ 	 * As part of the TEST above, RET is the ModRM byte, and INT3 the imm8.
+ 	 *
+ 	 * We subsequently jump backwards and architecturally execute the RET.
+ 	 * This creates a correct BTB prediction (type=ret), but in the
+ 	 * meantime we suffer Straight Line Speculation (because the type was
+ 	 * no branch) which is halted by the INT3.
+ 	 *
+ 	 * With SMT enabled and STIBP active, a sibling thread cannot poison
+ 	 * RET's prediction to a type of its choice, but can evict the
+ 	 * prediction due to competitive sharing. If the prediction is
+ 	 * evicted, __x86_return_thunk will suffer Straight Line Speculation
+ 	 * which will be contained safely by the INT3.
+ 	 */
+ SYM_INNER_LABEL(__x86_return_thunk, SYM_L_GLOBAL)
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
+ 
+ 	/*
+ 	 * Ensure the TEST decoding / BTB invalidation is complete.
+ 	 */
+ 	lfence
+ 
+ 	/*
+ 	 * Jump back and execute the RET in the middle of the TEST instruction.
+ 	 * INT3 is for SLS protection.
+ 	 */
+ 	jmp __x86_return_thunk
+ 	int3
+ SYM_FUNC_END(zen_untrain_ret)
+ __EXPORT_THUNK(zen_untrain_ret)
+ 
+ EXPORT_SYMBOL(__x86_return_thunk)
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/entry/entry_64_compat.S
* Unmerged path arch/x86/kvm/svm/vmenter.S
* Unmerged path tools/objtool/check.c
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/entry/entry_64_compat.S
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/disabled-features.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/kvm/svm/vmenter.S
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path tools/objtool/check.c
