mm, page_owner: convert page_owner_inited to static key

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
Rebuild_CHGLOG: - [mm] mm/page_owner: convert page_owner_inited to static key (Rafael Aquini) [1781726]
Rebuild_FUZZ: 97.25%
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 7dd80b8af0bcd705a9ef2fa272c082882616a499
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/7dd80b8a.failed

CONFIG_PAGE_OWNER attempts to impose negligible runtime overhead when
enabled during compilation, but not actually enabled during runtime by
boot param page_owner=on.  This overhead can be further reduced using
the static key mechanism, which this patch does.

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 7dd80b8af0bcd705a9ef2fa272c082882616a499)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/page_owner.h
#	mm/page_owner.c
#	mm/vmstat.c
diff --cc mm/vmstat.c
index b9429ed4eed4,69ce64f7b8d7..000000000000
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@@ -1035,6 -1036,104 +1035,107 @@@ static int pagetypeinfo_showblockcount(
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PAGE_OWNER
+ static void pagetypeinfo_showmixedcount_print(struct seq_file *m,
+ 							pg_data_t *pgdat,
+ 							struct zone *zone)
+ {
+ 	struct page *page;
+ 	struct page_ext *page_ext;
+ 	unsigned long pfn = zone->zone_start_pfn, block_end_pfn;
+ 	unsigned long end_pfn = pfn + zone->spanned_pages;
+ 	unsigned long count[MIGRATE_TYPES] = { 0, };
+ 	int pageblock_mt, page_mt;
+ 	int i;
+ 
+ 	/* Scan block by block. First and last block may be incomplete */
+ 	pfn = zone->zone_start_pfn;
+ 
+ 	/*
+ 	 * Walk the zone in pageblock_nr_pages steps. If a page block spans
+ 	 * a zone boundary, it will be double counted between zones. This does
+ 	 * not matter as the mixed block count will still be correct
+ 	 */
+ 	for (; pfn < end_pfn; ) {
+ 		if (!pfn_valid(pfn)) {
+ 			pfn = ALIGN(pfn + 1, MAX_ORDER_NR_PAGES);
+ 			continue;
+ 		}
+ 
+ 		block_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);
+ 		block_end_pfn = min(block_end_pfn, end_pfn);
+ 
+ 		page = pfn_to_page(pfn);
+ 		pageblock_mt = get_pfnblock_migratetype(page, pfn);
+ 
+ 		for (; pfn < block_end_pfn; pfn++) {
+ 			if (!pfn_valid_within(pfn))
+ 				continue;
+ 
+ 			page = pfn_to_page(pfn);
+ 			if (PageBuddy(page)) {
+ 				pfn += (1UL << page_order(page)) - 1;
+ 				continue;
+ 			}
+ 
+ 			if (PageReserved(page))
+ 				continue;
+ 
+ 			page_ext = lookup_page_ext(page);
+ 
+ 			if (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))
+ 				continue;
+ 
+ 			page_mt = gfpflags_to_migratetype(page_ext->gfp_mask);
+ 			if (pageblock_mt != page_mt) {
+ 				if (is_migrate_cma(pageblock_mt))
+ 					count[MIGRATE_MOVABLE]++;
+ 				else
+ 					count[pageblock_mt]++;
+ 
+ 				pfn = block_end_pfn;
+ 				break;
+ 			}
+ 			pfn += (1UL << page_ext->order) - 1;
+ 		}
+ 	}
+ 
+ 	/* Print counts */
+ 	seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
+ 	for (i = 0; i < MIGRATE_TYPES; i++)
+ 		seq_printf(m, "%12lu ", count[i]);
+ 	seq_putc(m, '\n');
+ }
+ #endif /* CONFIG_PAGE_OWNER */
+ 
+ /*
+  * Print out the number of pageblocks for each migratetype that contain pages
+  * of other types. This gives an indication of how well fallbacks are being
+  * contained by rmqueue_fallback(). It requires information from PAGE_OWNER
+  * to determine what is going on
+  */
+ static void pagetypeinfo_showmixedcount(struct seq_file *m, pg_data_t *pgdat)
+ {
+ #ifdef CONFIG_PAGE_OWNER
+ 	int mtype;
+ 
+ 	if (!static_branch_unlikely(&page_owner_inited))
+ 		return;
+ 
+ 	drain_all_pages(NULL);
+ 
+ 	seq_printf(m, "\n%-23s", "Number of mixed blocks ");
+ 	for (mtype = 0; mtype < MIGRATE_TYPES; mtype++)
+ 		seq_printf(m, "%12s ", migratetype_names[mtype]);
+ 	seq_putc(m, '\n');
+ 
+ 	walk_zones_in_node(m, pgdat, pagetypeinfo_showmixedcount_print);
+ #endif /* CONFIG_PAGE_OWNER */
+ }
+ 
++>>>>>>> 7dd80b8af0bc (mm, page_owner: convert page_owner_inited to static key)
  /*
   * This prints out statistics in relation to grouping pages by mobility.
   * It is expensive to collect so do not constantly read the file.
* Unmerged path include/linux/page_owner.h
* Unmerged path mm/page_owner.c
diff --git a/Documentation/vm/page_owner.txt b/Documentation/vm/page_owner.txt
index 8f3ce9b3aa11..ffff1439076a 100644
--- a/Documentation/vm/page_owner.txt
+++ b/Documentation/vm/page_owner.txt
@@ -28,10 +28,11 @@ with page owner and page owner is disabled in runtime due to no enabling
 boot option, runtime overhead is marginal. If disabled in runtime, it
 doesn't require memory to store owner information, so there is no runtime
 memory overhead. And, page owner inserts just two unlikely branches into
-the page allocator hotpath and if it returns false then allocation is
-done like as the kernel without page owner. These two unlikely branches
-would not affect to allocation performance. Following is the kernel's
-code size change due to this facility.
+the page allocator hotpath and if not enabled, then allocation is done
+like as the kernel without page owner. These two unlikely branches should
+not affect to allocation performance, especially if the static keys jump
+label patching functionality is available. Following is the kernel's code
+size change due to this facility.
 
 - Without page owner
    text    data     bss     dec     hex filename
* Unmerged path include/linux/page_owner.h
* Unmerged path mm/page_owner.c
* Unmerged path mm/vmstat.c
