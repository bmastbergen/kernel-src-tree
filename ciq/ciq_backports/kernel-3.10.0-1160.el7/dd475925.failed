ext4: limit number of scanned extents in status tree shrinker

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Jan Kara <jack@suse.cz>
commit dd4759255188771e60cf3455982959a1ba04f4eb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/dd475925.failed

Currently we scan extent status trees of inodes until we reclaim nr_to_scan
extents. This can however require a lot of scanning when there are lots
of delayed extents (as those cannot be reclaimed).

Change shrinker to work as shrinkers are supposed to and *scan* only
nr_to_scan extents regardless of how many extents did we actually
reclaim. We however need to be careful and avoid scanning each status
tree from the beginning - that could lead to a situation where we would
not be able to reclaim anything at all when first nr_to_scan extents in
the tree are always unreclaimable. We remember with each inode offset
where we stopped scanning and continue from there when we next come
across the inode.

Note that we also need to update places calling __es_shrink() manually
to pass reasonable nr_to_scan to have a chance of reclaiming anything and
not just 1.

	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>
(cherry picked from commit dd4759255188771e60cf3455982959a1ba04f4eb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/ext4.h
#	fs/ext4/extents_status.c
#	fs/ext4/super.c
diff --cc fs/ext4/ext4.h
index 8897de79c790,4186ec84f835..000000000000
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@@ -953,9 -878,12 +953,18 @@@ struct ext4_inode_info 
  	/* extents status tree */
  	struct ext4_es_tree i_es_tree;
  	rwlock_t i_es_lock;
++<<<<<<< HEAD
 +	struct list_head i_es_lru;
 +	unsigned int i_es_lru_nr;	/* protected by i_es_lock */
 +	unsigned long i_touch_when;	/* jiffies of last accessing */
++=======
+ 	struct list_head i_es_list;
+ 	unsigned int i_es_all_nr;	/* protected by i_es_lock */
+ 	unsigned int i_es_shk_nr;	/* protected by i_es_lock */
+ 	ext4_lblk_t i_es_shrink_lblk;	/* Offset where we start searching for
+ 					   extents to shrink. Protected by
+ 					   i_es_lock  */
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  
  	/* ialloc */
  	ext4_group_t	i_last_alloc_group;
@@@ -1393,10 -1324,11 +1402,18 @@@ struct ext4_sb_info 
  
  	/* Reclaim extents from extent status tree */
  	struct shrinker s_es_shrinker;
++<<<<<<< HEAD
 +	struct list_head s_es_lru;
 +	unsigned long s_es_last_sorted;
 +	struct percpu_counter s_extent_cache_cnt;
 +	spinlock_t s_es_lru_lock ____cacheline_aligned_in_smp;
++=======
+ 	struct list_head s_es_list;	/* List of inodes with reclaimable extents */
+ 	long s_es_nr_inode;
+ 	struct ext4_es_stats s_es_stats;
+ 	struct mb_cache *s_mb_cache;
+ 	spinlock_t s_es_lock ____cacheline_aligned_in_smp;
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  
  	/* Ratelimit ext4 messages. */
  	struct ratelimit_state s_err_ratelimit_state;
diff --cc fs/ext4/extents_status.c
index 3ef7f932e809,8f2aac4006d2..000000000000
--- a/fs/ext4/extents_status.c
+++ b/fs/ext4/extents_status.c
@@@ -145,10 -147,9 +145,16 @@@ static struct kmem_cache *ext4_es_cache
  static int __es_insert_extent(struct inode *inode, struct extent_status *newes);
  static int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,
  			      ext4_lblk_t end);
++<<<<<<< HEAD
 +static int __es_try_to_reclaim_extents(struct ext4_inode_info *ei,
 +				       int nr_to_scan);
 +static int __ext4_es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,
 +			    struct ext4_inode_info *locked_ei);
++=======
+ static int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan);
+ static int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,
+ 		       struct ext4_inode_info *locked_ei);
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  
  int __init ext4_init_es(void)
  {
@@@ -683,8 -714,8 +689,13 @@@ int ext4_es_insert_extent(struct inode 
  		goto error;
  retry:
  	err = __es_insert_extent(inode, &newes);
++<<<<<<< HEAD
 +	if (err == -ENOMEM && __ext4_es_shrink(EXT4_SB(inode->i_sb), 1,
 +					       EXT4_I(inode)))
++=======
+ 	if (err == -ENOMEM && __es_shrink(EXT4_SB(inode->i_sb),
+ 					  128, EXT4_I(inode)))
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  		goto retry;
  	if (err == -ENOMEM && !ext4_es_is_delayed(&newes))
  		err = 0;
@@@ -836,8 -872,8 +847,13 @@@ retry
  				es->es_lblk = orig_es.es_lblk;
  				es->es_len = orig_es.es_len;
  				if ((err == -ENOMEM) &&
++<<<<<<< HEAD
 +				    __ext4_es_shrink(EXT4_SB(inode->i_sb), 1,
 +						     EXT4_I(inode)))
++=======
+ 				    __es_shrink(EXT4_SB(inode->i_sb),
+ 							128, EXT4_I(inode)))
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  					goto retry;
  				goto out;
  			}
@@@ -916,80 -957,61 +932,103 @@@ int ext4_es_remove_extent(struct inode 
  	return err;
  }
  
 -static int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,
 -		       struct ext4_inode_info *locked_ei)
 +static int ext4_inode_touch_time_cmp(void *priv, struct list_head *a,
 +				     struct list_head *b)
 +{
 +	struct ext4_inode_info *eia, *eib;
 +	eia = list_entry(a, struct ext4_inode_info, i_es_lru);
 +	eib = list_entry(b, struct ext4_inode_info, i_es_lru);
 +
 +	if (ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) &&
 +	    !ext4_test_inode_state(&eib->vfs_inode, EXT4_STATE_EXT_PRECACHED))
 +		return 1;
 +	if (!ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) &&
 +	    ext4_test_inode_state(&eib->vfs_inode, EXT4_STATE_EXT_PRECACHED))
 +		return -1;
 +	if (eia->i_touch_when == eib->i_touch_when)
 +		return 0;
 +	if (time_after(eia->i_touch_when, eib->i_touch_when))
 +		return 1;
 +	else
 +		return -1;
 +}
 +
 +static int __ext4_es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,
 +			    struct ext4_inode_info *locked_ei)
  {
  	struct ext4_inode_info *ei;
 -	struct ext4_es_stats *es_stats;
 -	ktime_t start_time;
 -	u64 scan_time;
 -	int nr_to_walk;
 -	int nr_shrunk = 0;
 -	int retried = 0, nr_skipped = 0;
 +	struct list_head *cur, *tmp;
 +	LIST_HEAD(skipped);
 +	int ret, nr_shrunk = 0;
 +	int retried = 0, skip_precached = 1, nr_skipped = 0;
  
 -	es_stats = &sbi->s_es_stats;
 -	start_time = ktime_get();
 +	spin_lock(&sbi->s_es_lru_lock);
  
  retry:
++<<<<<<< HEAD
 +	list_for_each_safe(cur, tmp, &sbi->s_es_lru) {
 +		/*
 +		 * If we have already reclaimed all extents from extent
 +		 * status tree, just stop the loop immediately.
 +		 */
 +		if (percpu_counter_read_positive(&sbi->s_extent_cache_cnt) == 0)
 +			break;
 +
 +		ei = list_entry(cur, struct ext4_inode_info, i_es_lru);
++=======
+ 	spin_lock(&sbi->s_es_lock);
+ 	nr_to_walk = sbi->s_es_nr_inode;
+ 	while (nr_to_walk-- > 0) {
+ 		if (list_empty(&sbi->s_es_list)) {
+ 			spin_unlock(&sbi->s_es_lock);
+ 			goto out;
+ 		}
+ 		ei = list_first_entry(&sbi->s_es_list, struct ext4_inode_info,
+ 				      i_es_list);
+ 		/* Move the inode to the tail */
+ 		list_move_tail(&ei->i_es_list, &sbi->s_es_list);
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  
  		/*
 -		 * Normally we try hard to avoid shrinking precached inodes,
 -		 * but we will as a last resort.
 +		 * Skip the inode that is newer than the last_sorted
 +		 * time.  Normally we try hard to avoid shrinking
 +		 * precached inodes, but we will as a last resort.
  		 */
 -		if (!retried && ext4_test_inode_state(&ei->vfs_inode,
 -						EXT4_STATE_EXT_PRECACHED)) {
 +		if ((sbi->s_es_last_sorted < ei->i_touch_when) ||
 +		    (skip_precached && ext4_test_inode_state(&ei->vfs_inode,
 +						EXT4_STATE_EXT_PRECACHED))) {
  			nr_skipped++;
 +			list_move_tail(cur, &skipped);
  			continue;
  		}
  
 -		if (ei == locked_ei || !write_trylock(&ei->i_es_lock)) {
 -			nr_skipped++;
 +		if (ei->i_es_lru_nr == 0 || ei == locked_ei ||
 +		    !write_trylock(&ei->i_es_lock))
  			continue;
 -		}
 -		/*
 -		 * Now we hold i_es_lock which protects us from inode reclaim
 -		 * freeing inode under us
 -		 */
 -		spin_unlock(&sbi->s_es_lock);
  
++<<<<<<< HEAD
 +		ret = __es_try_to_reclaim_extents(ei, nr_to_scan);
 +		if (ei->i_es_lru_nr == 0)
 +			list_del_init(&ei->i_es_lru);
 +		write_unlock(&ei->i_es_lock);
 +
 +		nr_shrunk += ret;
 +		nr_to_scan -= ret;
 +		if (nr_to_scan == 0)
 +			break;
++=======
+ 		nr_shrunk += es_reclaim_extents(ei, &nr_to_scan);
+ 		write_unlock(&ei->i_es_lock);
+ 
+ 		if (nr_to_scan <= 0)
+ 			goto out;
+ 		spin_lock(&sbi->s_es_lock);
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  	}
 -	spin_unlock(&sbi->s_es_lock);
 +
 +	/* Move the newer inodes into the tail of the LRU list. */
 +	list_splice_tail(&skipped, &sbi->s_es_lru);
 +	INIT_LIST_HEAD(&skipped);
  
  	/*
  	 * If we skipped any inodes, and we weren't able to make any
@@@ -1011,11 -1022,26 +1050,11 @@@
  		goto retry;
  	}
  
 +	spin_unlock(&sbi->s_es_lru_lock);
 +
  	if (locked_ei && nr_shrunk == 0)
- 		nr_shrunk = __es_try_to_reclaim_extents(locked_ei, nr_to_scan);
+ 		nr_shrunk = es_reclaim_extents(locked_ei, &nr_to_scan);
  
 -out:
 -	scan_time = ktime_to_ns(ktime_sub(ktime_get(), start_time));
 -	if (likely(es_stats->es_stats_scan_time))
 -		es_stats->es_stats_scan_time = (scan_time +
 -				es_stats->es_stats_scan_time*3) / 4;
 -	else
 -		es_stats->es_stats_scan_time = scan_time;
 -	if (scan_time > es_stats->es_stats_max_scan_time)
 -		es_stats->es_stats_max_scan_time = scan_time;
 -	if (likely(es_stats->es_stats_shrunk))
 -		es_stats->es_stats_shrunk = (nr_shrunk +
 -				es_stats->es_stats_shrunk*3) / 4;
 -	else
 -		es_stats->es_stats_shrunk = nr_shrunk;
 -
 -	trace_ext4_es_shrink(sbi->s_sb, nr_shrunk, scan_time,
 -			     nr_skipped, retried);
  	return nr_shrunk;
  }
  
@@@ -1032,76 -1071,180 +1071,90 @@@ static int ext4_es_shrink(struct shrink
  	if (!nr_to_scan)
  		return ret;
  
 -	nr_shrunk = __es_shrink(sbi, nr_to_scan, NULL);
 -
 -	trace_ext4_es_shrink_scan_exit(sbi->s_sb, nr_shrunk, ret);
 -	return nr_shrunk;
 -}
 -
 -static void *ext4_es_seq_shrinker_info_start(struct seq_file *seq, loff_t *pos)
 -{
 -	return *pos ? NULL : SEQ_START_TOKEN;
 -}
 +	nr_shrunk = __ext4_es_shrink(sbi, nr_to_scan, NULL);
  
 -static void *
 -ext4_es_seq_shrinker_info_next(struct seq_file *seq, void *v, loff_t *pos)
 -{
 -	return NULL;
 +	ret = percpu_counter_read_positive(&sbi->s_extent_cache_cnt);
 +	trace_ext4_es_shrink_exit(sbi->s_sb, nr_shrunk, ret);
 +	return ret;
  }
  
 -static int ext4_es_seq_shrinker_info_show(struct seq_file *seq, void *v)
 +void ext4_es_register_shrinker(struct ext4_sb_info *sbi)
  {
 -	struct ext4_sb_info *sbi = seq->private;
 -	struct ext4_es_stats *es_stats = &sbi->s_es_stats;
 -	struct ext4_inode_info *ei, *max = NULL;
 -	unsigned int inode_cnt = 0;
 -
 -	if (v != SEQ_START_TOKEN)
 -		return 0;
 -
 -	/* here we just find an inode that has the max nr. of objects */
 -	spin_lock(&sbi->s_es_lock);
 -	list_for_each_entry(ei, &sbi->s_es_list, i_es_list) {
 -		inode_cnt++;
 -		if (max && max->i_es_all_nr < ei->i_es_all_nr)
 -			max = ei;
 -		else if (!max)
 -			max = ei;
 -	}
 -	spin_unlock(&sbi->s_es_lock);
 -
 -	seq_printf(seq, "stats:\n  %lld objects\n  %lld reclaimable objects\n",
 -		   percpu_counter_sum_positive(&es_stats->es_stats_all_cnt),
 -		   percpu_counter_sum_positive(&es_stats->es_stats_shk_cnt));
 -	seq_printf(seq, "  %lu/%lu cache hits/misses\n",
 -		   es_stats->es_stats_cache_hits,
 -		   es_stats->es_stats_cache_misses);
 -	if (inode_cnt)
 -		seq_printf(seq, "  %d inodes on list\n", inode_cnt);
 -
 -	seq_printf(seq, "average:\n  %llu us scan time\n",
 -	    div_u64(es_stats->es_stats_scan_time, 1000));
 -	seq_printf(seq, "  %lu shrunk objects\n", es_stats->es_stats_shrunk);
 -	if (inode_cnt)
 -		seq_printf(seq,
 -		    "maximum:\n  %lu inode (%u objects, %u reclaimable)\n"
 -		    "  %llu us max scan time\n",
 -		    max->vfs_inode.i_ino, max->i_es_all_nr, max->i_es_shk_nr,
 -		    div_u64(es_stats->es_stats_max_scan_time, 1000));
 -
 -	return 0;
 +	INIT_LIST_HEAD(&sbi->s_es_lru);
 +	spin_lock_init(&sbi->s_es_lru_lock);
 +	sbi->s_es_last_sorted = 0;
 +	sbi->s_es_shrinker.shrink = ext4_es_shrink;
 +	sbi->s_es_shrinker.seeks = DEFAULT_SEEKS;
 +	register_shrinker(&sbi->s_es_shrinker);
  }
  
 -static void ext4_es_seq_shrinker_info_stop(struct seq_file *seq, void *v)
 +void ext4_es_unregister_shrinker(struct ext4_sb_info *sbi)
  {
 +	unregister_shrinker(&sbi->s_es_shrinker);
  }
  
 -static const struct seq_operations ext4_es_seq_shrinker_info_ops = {
 -	.start = ext4_es_seq_shrinker_info_start,
 -	.next  = ext4_es_seq_shrinker_info_next,
 -	.stop  = ext4_es_seq_shrinker_info_stop,
 -	.show  = ext4_es_seq_shrinker_info_show,
 -};
 -
 -static int
 -ext4_es_seq_shrinker_info_open(struct inode *inode, struct file *file)
++<<<<<<< HEAD
 +void ext4_es_lru_add(struct inode *inode)
  {
 -	int ret;
 +	struct ext4_inode_info *ei = EXT4_I(inode);
 +	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
  
 -	ret = seq_open(file, &ext4_es_seq_shrinker_info_ops);
 -	if (!ret) {
 -		struct seq_file *m = file->private_data;
 -		m->private = PDE_DATA(inode);
 -	}
 +	ei->i_touch_when = jiffies;
  
 -	return ret;
 -}
 +	if (!list_empty(&ei->i_es_lru))
 +		return;
  
 -static int
 -ext4_es_seq_shrinker_info_release(struct inode *inode, struct file *file)
 -{
 -	return seq_release(inode, file);
 +	spin_lock(&sbi->s_es_lru_lock);
 +	if (list_empty(&ei->i_es_lru))
 +		list_add_tail(&ei->i_es_lru, &sbi->s_es_lru);
 +	spin_unlock(&sbi->s_es_lru_lock);
  }
  
 -static const struct file_operations ext4_es_seq_shrinker_info_fops = {
 -	.owner		= THIS_MODULE,
 -	.open		= ext4_es_seq_shrinker_info_open,
 -	.read		= seq_read,
 -	.llseek		= seq_lseek,
 -	.release	= ext4_es_seq_shrinker_info_release,
 -};
 -
 -int ext4_es_register_shrinker(struct ext4_sb_info *sbi)
 +void ext4_es_lru_del(struct inode *inode)
  {
 -	int err;
 -
 -	INIT_LIST_HEAD(&sbi->s_es_list);
 -	sbi->s_es_nr_inode = 0;
 -	spin_lock_init(&sbi->s_es_lock);
 -	sbi->s_es_stats.es_stats_shrunk = 0;
 -	sbi->s_es_stats.es_stats_cache_hits = 0;
 -	sbi->s_es_stats.es_stats_cache_misses = 0;
 -	sbi->s_es_stats.es_stats_scan_time = 0;
 -	sbi->s_es_stats.es_stats_max_scan_time = 0;
 -	err = percpu_counter_init(&sbi->s_es_stats.es_stats_all_cnt, 0, GFP_KERNEL);
 -	if (err)
 -		return err;
 -	err = percpu_counter_init(&sbi->s_es_stats.es_stats_shk_cnt, 0, GFP_KERNEL);
 -	if (err)
 -		goto err1;
 -
 -	sbi->s_es_shrinker.scan_objects = ext4_es_scan;
 -	sbi->s_es_shrinker.count_objects = ext4_es_count;
 -	sbi->s_es_shrinker.seeks = DEFAULT_SEEKS;
 -	err = register_shrinker(&sbi->s_es_shrinker);
 -	if (err)
 -		goto err2;
 -
 -	if (sbi->s_proc)
 -		proc_create_data("es_shrinker_info", S_IRUGO, sbi->s_proc,
 -				 &ext4_es_seq_shrinker_info_fops, sbi);
 -
 -	return 0;
 -
 -err2:
 -	percpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);
 -err1:
 -	percpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);
 -	return err;
 -}
 +	struct ext4_inode_info *ei = EXT4_I(inode);
 +	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
  
 -void ext4_es_unregister_shrinker(struct ext4_sb_info *sbi)
 -{
 -	if (sbi->s_proc)
 -		remove_proc_entry("es_shrinker_info", sbi->s_proc);
 -	percpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);
 -	percpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);
 -	unregister_shrinker(&sbi->s_es_shrinker);
 +	spin_lock(&sbi->s_es_lru_lock);
 +	if (!list_empty(&ei->i_es_lru))
 +		list_del_init(&ei->i_es_lru);
 +	spin_unlock(&sbi->s_es_lru_lock);
  }
  
 +static int __es_try_to_reclaim_extents(struct ext4_inode_info *ei,
 +				       int nr_to_scan)
++=======
+ /*
+  * Shrink extents in given inode from ei->i_es_shrink_lblk till end. Scan at
+  * most *nr_to_scan extents, update *nr_to_scan accordingly.
+  *
+  * Return 0 if we hit end of tree / interval, 1 if we exhausted nr_to_scan.
+  * Increment *nr_shrunk by the number of reclaimed extents. Also update
+  * ei->i_es_shrink_lblk to where we should continue scanning.
+  */
+ static int es_do_reclaim_extents(struct ext4_inode_info *ei, ext4_lblk_t end,
+ 				 int *nr_to_scan, int *nr_shrunk)
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  {
  	struct inode *inode = &ei->vfs_inode;
  	struct ext4_es_tree *tree = &ei->i_es_tree;
- 	struct rb_node *node;
  	struct extent_status *es;
++<<<<<<< HEAD
 +	int nr_shrunk = 0;
- 	static DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,
- 				      DEFAULT_RATELIMIT_BURST);
- 
- 	if (ei->i_es_lru_nr == 0)
- 		return 0;
++=======
+ 	struct rb_node *node;
  
- 	if (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) &&
- 	    __ratelimit(&_rs))
- 		ext4_warning(inode->i_sb, "forced shrink of precached extents");
+ 	es = __es_tree_search(&tree->root, ei->i_es_shrink_lblk);
+ 	if (!es)
+ 		goto out_wrap;
+ 	node = &es->rb_node;
+ 	while (*nr_to_scan > 0) {
+ 		if (es->es_lblk > end) {
+ 			ei->i_es_shrink_lblk = end + 1;
+ 			return 0;
+ 		}
  
- 	node = rb_first(&tree->root);
- 	while (node != NULL) {
- 		es = rb_entry(node, struct extent_status, rb_node);
+ 		(*nr_to_scan)--;
  		node = rb_next(&es->rb_node);
  		/*
  		 * We can't reclaim delayed extent from status tree because
@@@ -1110,11 -1253,38 +1163,39 @@@
  		if (!ext4_es_is_delayed(es)) {
  			rb_erase(&es->rb_node, &tree->root);
  			ext4_es_free_extent(inode, es);
- 			nr_shrunk++;
- 			if (--nr_to_scan == 0)
- 				break;
+ 			(*nr_shrunk)++;
  		}
+ 		if (!node)
+ 			goto out_wrap;
+ 		es = rb_entry(node, struct extent_status, rb_node);
  	}
- 	tree->cache_es = NULL;
+ 	ei->i_es_shrink_lblk = es->es_lblk;
+ 	return 1;
+ out_wrap:
+ 	ei->i_es_shrink_lblk = 0;
+ 	return 0;
+ }
+ 
+ static int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan)
+ {
+ 	struct inode *inode = &ei->vfs_inode;
+ 	int nr_shrunk = 0;
+ 	ext4_lblk_t start = ei->i_es_shrink_lblk;
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
+ 	static DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,
+ 				      DEFAULT_RATELIMIT_BURST);
+ 
 -	if (ei->i_es_shk_nr == 0)
++	if (ei->i_es_lru_nr == 0)
+ 		return 0;
+ 
+ 	if (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) &&
+ 	    __ratelimit(&_rs))
+ 		ext4_warning(inode->i_sb, "forced shrink of precached extents");
+ 
+ 	if (!es_do_reclaim_extents(ei, EXT_MAX_BLOCKS, nr_to_scan, &nr_shrunk) &&
+ 	    start != 0)
+ 		es_do_reclaim_extents(ei, start - 1, nr_to_scan, &nr_shrunk);
+ 
+ 	ei->i_es_tree.cache_es = NULL;
  	return nr_shrunk;
  }
diff --cc fs/ext4/super.c
index 417c0071ce41,48318497e8e9..000000000000
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@@ -918,9 -871,10 +918,16 @@@ static struct inode *ext4_alloc_inode(s
  	spin_lock_init(&ei->i_prealloc_lock);
  	ext4_es_init_tree(&ei->i_es_tree);
  	rwlock_init(&ei->i_es_lock);
++<<<<<<< HEAD
 +	INIT_LIST_HEAD(&ei->i_es_lru);
 +	ei->i_es_lru_nr = 0;
 +	ei->i_touch_when = 0;
++=======
+ 	INIT_LIST_HEAD(&ei->i_es_list);
+ 	ei->i_es_all_nr = 0;
+ 	ei->i_es_shk_nr = 0;
+ 	ei->i_es_shrink_lblk = 0;
++>>>>>>> dd4759255188 (ext4: limit number of scanned extents in status tree shrinker)
  	ei->i_reserved_data_blocks = 0;
  	ei->i_reserved_meta_blocks = 0;
  	ei->i_allocated_meta_blocks = 0;
* Unmerged path fs/ext4/ext4.h
* Unmerged path fs/ext4/extents_status.c
* Unmerged path fs/ext4/super.c
