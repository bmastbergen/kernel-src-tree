ftrace: Introduce PERMANENT ftrace_ops flag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Miroslav Benes <mbenes@suse.cz>
commit 7162431dcf72032835d369c8d7b51311df407938
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/7162431d.failed

Livepatch uses ftrace for redirection to new patched functions. It means
that if ftrace is disabled, all live patched functions are disabled as
well. Toggling global 'ftrace_enabled' sysctl thus affect it directly.
It is not a problem per se, because only administrator can set sysctl
values, but it still may be surprising.

Introduce PERMANENT ftrace_ops flag to amend this. If the
FTRACE_OPS_FL_PERMANENT is set on any ftrace ops, the tracing cannot be
disabled by disabling ftrace_enabled. Equally, a callback with the flag
set cannot be registered if ftrace_enabled is disabled.

Link: http://lkml.kernel.org/r/20191016113316.13415-2-mbenes@suse.cz

	Reviewed-by: Petr Mladek <pmladek@suse.com>
	Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
	Signed-off-by: Miroslav Benes <mbenes@suse.cz>
	Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
(cherry picked from commit 7162431dcf72032835d369c8d7b51311df407938)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/trace/ftrace-uses.rst
#	Documentation/trace/ftrace.rst
#	include/linux/ftrace.h
diff --cc include/linux/ftrace.h
index e2b452fac236,8385cafe4f9f..000000000000
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@@ -100,29 -139,70 +100,56 @@@ typedef void (*ftrace_func_t)(unsigned 
   *            SAVE_REGS. If another ops with this flag set is already registered
   *            for any of the functions that this ops will be registered for, then
   *            this ops will fail to register or set_filter_ip.
++<<<<<<< HEAD
 + */
 +enum {
 +	FTRACE_OPS_FL_ENABLED			= 1 << 0,
 +	FTRACE_OPS_FL_GLOBAL			= 1 << 1,
 +	FTRACE_OPS_FL_DYNAMIC			= 1 << 2,
 +	FTRACE_OPS_FL_CONTROL			= 1 << 3,
 +	FTRACE_OPS_FL_SAVE_REGS			= 1 << 4,
 +	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
 +	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
 +	FTRACE_OPS_FL_STUB			= 1 << 7,
 +	FTRACE_OPS_FL_INITIALIZED		= 1 << 8,
 +	FTRACE_OPS_FL_IPMODIFY			= 1 << 9,
++=======
+  * PID     - Is affected by set_ftrace_pid (allows filtering on those pids)
+  * RCU     - Set when the ops can only be called when RCU is watching.
+  * TRACE_ARRAY - The ops->private points to a trace_array descriptor.
+  * PERMANENT - Set when the ops is permanent and should not be affected by
+  *             ftrace_enabled.
+  */
+ enum {
+ 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
+ 	FTRACE_OPS_FL_DYNAMIC			= 1 << 1,
+ 	FTRACE_OPS_FL_SAVE_REGS			= 1 << 2,
+ 	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 3,
+ 	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 4,
+ 	FTRACE_OPS_FL_STUB			= 1 << 5,
+ 	FTRACE_OPS_FL_INITIALIZED		= 1 << 6,
+ 	FTRACE_OPS_FL_DELETED			= 1 << 7,
+ 	FTRACE_OPS_FL_ADDING			= 1 << 8,
+ 	FTRACE_OPS_FL_REMOVING			= 1 << 9,
+ 	FTRACE_OPS_FL_MODIFYING			= 1 << 10,
+ 	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 11,
+ 	FTRACE_OPS_FL_IPMODIFY			= 1 << 12,
+ 	FTRACE_OPS_FL_PID			= 1 << 13,
+ 	FTRACE_OPS_FL_RCU			= 1 << 14,
+ 	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 15,
+ 	FTRACE_OPS_FL_PERMANENT                 = 1 << 16,
++>>>>>>> 7162431dcf72 (ftrace: Introduce PERMANENT ftrace_ops flag)
  };
  
 -#ifdef CONFIG_DYNAMIC_FTRACE
 -/* The hash used to know what functions callbacks trace */
 -struct ftrace_ops_hash {
 -	struct ftrace_hash __rcu	*notrace_hash;
 -	struct ftrace_hash __rcu	*filter_hash;
 -	struct mutex			regex_lock;
 -};
 -
 -void ftrace_free_init_mem(void);
 -void ftrace_free_mem(struct module *mod, void *start, void *end);
 -#else
 -static inline void ftrace_free_init_mem(void) { }
 -static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
 -#endif
 -
 -/*
 - * Note, ftrace_ops can be referenced outside of RCU protection, unless
 - * the RCU flag is set. If ftrace_ops is allocated and not part of kernel
 - * core data, the unregistering of it will perform a scheduling on all CPUs
 - * to make sure that there are no more users. Depending on the load of the
 - * system that may take a bit of time.
 - *
 - * Any private data added must also take care not to be freed and if private
 - * data is added to a ftrace_ops that is in core code, the user of the
 - * ftrace_ops must perform a schedule_on_each_cpu() before freeing it.
 - */
  struct ftrace_ops {
  	ftrace_func_t			func;
 -	struct ftrace_ops __rcu		*next;
 +	struct ftrace_ops		*next;
  	unsigned long			flags;
 -	void				*private;
 -	ftrace_func_t			saved_func;
 +	int __percpu			*disabled;
  #ifdef CONFIG_DYNAMIC_FTRACE
 -	struct ftrace_ops_hash		local_hash;
 -	struct ftrace_ops_hash		*func_hash;
 -	struct ftrace_ops_hash		old_hash;
 -	unsigned long			trampoline;
 -	unsigned long			trampoline_size;
 +	struct ftrace_hash		*notrace_hash;
 +	struct ftrace_hash		*filter_hash;
 +	struct mutex			regex_lock;
  #endif
  };
  
* Unmerged path Documentation/trace/ftrace-uses.rst
* Unmerged path Documentation/trace/ftrace.rst
* Unmerged path Documentation/trace/ftrace-uses.rst
* Unmerged path Documentation/trace/ftrace.rst
* Unmerged path include/linux/ftrace.h
diff --git a/kernel/livepatch/patch.c b/kernel/livepatch/patch.c
index 99cb3ad05eb4..efb0c62c03d1 100644
--- a/kernel/livepatch/patch.c
+++ b/kernel/livepatch/patch.c
@@ -208,7 +208,8 @@ static int klp_patch_func(struct klp_func *func)
 		ops->fops.func = klp_ftrace_handler;
 		ops->fops.flags = FTRACE_OPS_FL_SAVE_REGS |
 				  FTRACE_OPS_FL_DYNAMIC |
-				  FTRACE_OPS_FL_IPMODIFY;
+				  FTRACE_OPS_FL_IPMODIFY |
+				  FTRACE_OPS_FL_PERMANENT;
 
 		list_add(&ops->node, &klp_ops);
 
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 6a691495c059..5b34c655bd01 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -452,6 +452,8 @@ static int __register_ftrace_function(struct ftrace_ops *ops)
 	if (ops->flags & FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED)
 		ops->flags |= FTRACE_OPS_FL_SAVE_REGS;
 #endif
+	if (!ftrace_enabled && (ops->flags & FTRACE_OPS_FL_PERMANENT))
+		return -EBUSY;
 
 	if (!core_kernel_data((unsigned long)ops))
 		ops->flags |= FTRACE_OPS_FL_DYNAMIC;
@@ -4970,6 +4972,18 @@ int unregister_ftrace_function(struct ftrace_ops *ops)
 }
 EXPORT_SYMBOL_GPL(unregister_ftrace_function);
 
+static bool is_permanent_ops_registered(void)
+{
+	struct ftrace_ops *op;
+
+	do_for_each_ftrace_op(op, ftrace_ops_list) {
+		if (op->flags & FTRACE_OPS_FL_PERMANENT)
+			return true;
+	} while_for_each_ftrace_op(op);
+
+	return false;
+}
+
 int
 ftrace_enable_sysctl(struct ctl_table *table, int write,
 		     void __user *buffer, size_t *lenp,
@@ -4987,8 +5001,6 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 	if (ret || !write || (last_ftrace_enabled == !!ftrace_enabled))
 		goto out;
 
-	last_ftrace_enabled = !!ftrace_enabled;
-
 	if (ftrace_enabled) {
 
 		ftrace_startup_sysctl();
@@ -4998,12 +5010,19 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 			update_ftrace_function();
 
 	} else {
+		if (is_permanent_ops_registered()) {
+			ftrace_enabled = true;
+			ret = -EBUSY;
+			goto out;
+		}
+
 		/* stopping ftrace calls (just send to ftrace_stub) */
 		ftrace_trace_function = ftrace_stub;
 
 		ftrace_shutdown_sysctl();
 	}
 
+	last_ftrace_enabled = !!ftrace_enabled;
  out:
 	mutex_unlock(&ftrace_lock);
 	return ret;
