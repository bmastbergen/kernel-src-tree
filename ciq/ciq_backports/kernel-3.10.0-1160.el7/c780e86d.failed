blktrace: Protect q->blk_trace with RCU

jira LE-1907
cve CVE-2019-19768
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Jan Kara <jack@suse.cz>
commit c780e86dd48ef6467a1146cf7d0fe1e05a635039
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/c780e86d.failed

KASAN is reporting that __blk_add_trace() has a use-after-free issue
when accessing q->blk_trace. Indeed the switching of block tracing (and
thus eventual freeing of q->blk_trace) is completely unsynchronized with
the currently running tracing and thus it can happen that the blk_trace
structure is being freed just while __blk_add_trace() works on it.
Protect accesses to q->blk_trace by RCU during tracing and make sure we
wait for the end of RCU grace period when shutting down tracing. Luckily
that is rare enough event that we can afford that. Note that postponing
the freeing of blk_trace to an RCU callback should better be avoided as
it could have unexpected user visible side-effects as debugfs files
would be still existing for a short while block tracing has been shut
down.

Link: https://bugzilla.kernel.org/show_bug.cgi?id=205711
CC: stable@vger.kernel.org
	Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Tested-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Bart Van Assche <bvanassche@acm.org>
	Reported-by: Tristan Madani <tristmd@gmail.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c780e86dd48ef6467a1146cf7d0fe1e05a635039)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blkdev.h
#	include/linux/blktrace_api.h
#	kernel/trace/blktrace.c
diff --cc include/linux/blkdev.h
index 8bef3eb8d49a,10455b2bbbb4..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -499,7 -524,8 +499,12 @@@ struct request_queue 
  	unsigned int		sg_reserved_size;
  	int			node;
  #ifdef CONFIG_BLK_DEV_IO_TRACE
++<<<<<<< HEAD
 +	struct blk_trace	*blk_trace;
++=======
+ 	struct blk_trace __rcu	*blk_trace;
+ 	struct mutex		blk_trace_mutex;
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  #endif
  	/*
  	 * for flush operations
diff --cc include/linux/blktrace_api.h
index 21bedade284a,3b6ff5902edc..000000000000
--- a/include/linux/blktrace_api.h
+++ b/include/linux/blktrace_api.h
@@@ -44,14 -49,32 +44,37 @@@ void __trace_note_message(struct blk_tr
   *     NOTE: Can not use 'static inline' due to presence of var args...
   *
   **/
 -#define blk_add_cgroup_trace_msg(q, cg, fmt, ...)			\
 +#define blk_add_trace_msg(q, fmt, ...)					\
  	do {								\
- 		struct blk_trace *bt = (q)->blk_trace;			\
+ 		struct blk_trace *bt;					\
+ 									\
+ 		rcu_read_lock();					\
+ 		bt = rcu_dereference((q)->blk_trace);			\
  		if (unlikely(bt))					\
++<<<<<<< HEAD
 +			__trace_note_message(bt, fmt, ##__VA_ARGS__);	\
++=======
+ 			__trace_note_message(bt, cg, fmt, ##__VA_ARGS__);\
+ 		rcu_read_unlock();					\
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  	} while (0)
 -#define blk_add_trace_msg(q, fmt, ...)					\
 -	blk_add_cgroup_trace_msg(q, NULL, fmt, ##__VA_ARGS__)
  #define BLK_TN_MAX_MSG		128
  
++<<<<<<< HEAD
++=======
+ static inline bool blk_trace_note_message_enabled(struct request_queue *q)
+ {
+ 	struct blk_trace *bt;
+ 	bool ret;
+ 
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
+ 	ret = bt && (bt->act_mask & BLK_TC_NOTIFY);
+ 	rcu_read_unlock();
+ 	return ret;
+ }
+ 
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  extern void blk_add_driver_data(struct request_queue *q, struct request *rq,
  				void *data, size_t len);
  extern int blk_trace_setup(struct request_queue *q, char *name, dev_t dev,
diff --cc kernel/trace/blktrace.c
index d9820b3be08a,4560878f0bac..000000000000
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@@ -731,6 -752,36 +734,39 @@@ void blk_trace_shutdown(struct request_
  	mutex_unlock(&q->blk_trace_mutex);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BLK_CGROUP
+ static u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
+ {
+ 	struct blk_trace *bt;
+ 
+ 	/* We don't use the 'bt' value here except as an optimization... */
+ 	bt = rcu_dereference_protected(q->blk_trace, 1);
+ 	if (!bt || !(blk_tracer_flags.val & TRACE_BLK_OPT_CGROUP))
+ 		return 0;
+ 
+ 	if (!bio->bi_blkg)
+ 		return 0;
+ 	return cgroup_id(bio_blkcg(bio)->css.cgroup);
+ }
+ #else
+ u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)
+ {
+ 	return 0;
+ }
+ #endif
+ 
+ static u64
+ blk_trace_request_get_cgid(struct request_queue *q, struct request *rq)
+ {
+ 	if (!rq->bio)
+ 		return 0;
+ 	/* Use the first bio */
+ 	return blk_trace_bio_get_cgid(q, rq->bio);
+ }
+ 
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  /*
   * blktrace probes
   */
@@@ -746,29 -798,26 +782,43 @@@
   *     Records an action against a request. Will log the bio offset + size.
   *
   **/
 -static void blk_add_trace_rq(struct request *rq, int error,
 -			     unsigned int nr_bytes, u32 what, u64 cgid)
 +static void blk_add_trace_rq(struct request_queue *q, struct request *rq,
 +			     unsigned int nr_bytes, u32 what)
  {
++<<<<<<< HEAD
 +	struct blk_trace *bt = q->blk_trace;
++=======
+ 	struct blk_trace *bt;
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  
- 	if (likely(!bt))
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(rq->q->blk_trace);
+ 	if (likely(!bt)) {
+ 		rcu_read_unlock();
  		return;
+ 	}
  
 -	if (blk_rq_is_passthrough(rq))
 +	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
  		what |= BLK_TC_ACT(BLK_TC_PC);
 -	else
 +		__blk_add_trace(bt, 0, nr_bytes, rq->cmd_flags,
 +				what, rq->errors, rq->cmd_len, rq->cmd);
 +	} else  {
  		what |= BLK_TC_ACT(BLK_TC_FS);
 +		__blk_add_trace(bt, blk_rq_pos(rq), nr_bytes,
 +				rq->cmd_flags, what, rq->errors, 0, NULL);
 +	}
 +}
  
++<<<<<<< HEAD
 +static void blk_add_trace_rq_abort(void *ignore,
 +				   struct request_queue *q, struct request *rq)
 +{
 +	blk_add_trace_rq(q, rq, blk_rq_bytes(rq), BLK_TA_ABORT);
++=======
+ 	__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),
+ 			rq->cmd_flags, what, error, 0, NULL, cgid);
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  static void blk_add_trace_rq_insert(void *ignore,
@@@ -812,16 -863,19 +862,27 @@@ static void blk_add_trace_rq_complete(v
  static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,
  			      u32 what, int error)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  
- 	if (likely(!bt))
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
+ 	if (likely(!bt)) {
+ 		rcu_read_unlock();
  		return;
+ 	}
  
++<<<<<<< HEAD
 +	if (!error && !bio_flagged(bio, BIO_UPTODATE))
 +		error = EIO;
 +
 +	__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw, what,
 +			error, 0, NULL);
++=======
+ 	__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ 			bio_op(bio), bio->bi_opf, what, error, 0, NULL,
+ 			blk_trace_bio_get_cgid(q, bio));
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  static void blk_add_trace_bio_bounce(void *ignore,
@@@ -866,10 -920,14 +927,18 @@@ static void blk_add_trace_getrq(void *i
  	if (bio)
  		blk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);
  	else {
- 		struct blk_trace *bt = q->blk_trace;
+ 		struct blk_trace *bt;
  
+ 		rcu_read_lock();
+ 		bt = rcu_dereference(q->blk_trace);
  		if (bt)
++<<<<<<< HEAD
 +			__blk_add_trace(bt, 0, 0, rw, BLK_TA_GETRQ, 0, 0, NULL);
++=======
+ 			__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,
+ 					NULL, 0);
+ 		rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  	}
  }
  
@@@ -881,20 -939,26 +950,35 @@@ static void blk_add_trace_sleeprq(void 
  	if (bio)
  		blk_add_trace_bio(q, bio, BLK_TA_SLEEPRQ, 0);
  	else {
- 		struct blk_trace *bt = q->blk_trace;
+ 		struct blk_trace *bt;
  
+ 		rcu_read_lock();
+ 		bt = rcu_dereference(q->blk_trace);
  		if (bt)
++<<<<<<< HEAD
 +			__blk_add_trace(bt, 0, 0, rw, BLK_TA_SLEEPRQ,
 +					0, 0, NULL);
++=======
+ 			__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,
+ 					0, 0, NULL, 0);
+ 		rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  	}
  }
  
  static void blk_add_trace_plug(void *ignore, struct request_queue *q)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
  	if (bt)
++<<<<<<< HEAD
 +		__blk_add_trace(bt, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL);
++=======
+ 		__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  static void blk_add_trace_unplug(void *ignore, struct request_queue *q,
@@@ -911,8 -977,9 +997,9 @@@
  		else
  			what = BLK_TA_UNPLUG_TIMER;
  
 -		__blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);
 +		__blk_add_trace(bt, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu);
  	}
+ 	rcu_read_unlock();
  }
  
  static void blk_add_trace_split(void *ignore,
@@@ -924,10 -993,12 +1013,11 @@@
  	if (bt) {
  		__be64 rpdu = cpu_to_be64(pdu);
  
 -		__blk_add_trace(bt, bio->bi_iter.bi_sector,
 -				bio->bi_iter.bi_size, bio_op(bio), bio->bi_opf,
 -				BLK_TA_SPLIT, bio->bi_status, sizeof(rpdu),
 -				&rpdu, blk_trace_bio_get_cgid(q, bio));
 +		__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw,
 +				BLK_TA_SPLIT, !bio_flagged(bio, BIO_UPTODATE),
 +				sizeof(rpdu), &rpdu);
  	}
+ 	rcu_read_unlock();
  }
  
  /**
@@@ -947,19 -1018,24 +1037,30 @@@ static void blk_add_trace_bio_remap(voi
  				    struct request_queue *q, struct bio *bio,
  				    dev_t dev, sector_t from)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  	struct blk_io_trace_remap r;
  
- 	if (likely(!bt))
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
+ 	if (likely(!bt)) {
+ 		rcu_read_unlock();
  		return;
+ 	}
  
  	r.device_from = cpu_to_be32(dev);
 -	r.device_to   = cpu_to_be32(bio_dev(bio));
 +	r.device_to   = cpu_to_be32(bio->bi_bdev->bd_dev);
  	r.sector_from = cpu_to_be64(from);
  
++<<<<<<< HEAD
 +	__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw,
 +			BLK_TA_REMAP, !bio_flagged(bio, BIO_UPTODATE),
 +			sizeof(r), &r);
++=======
+ 	__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ 			bio_op(bio), bio->bi_opf, BLK_TA_REMAP, bio->bi_status,
+ 			sizeof(r), &r, blk_trace_bio_get_cgid(q, bio));
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  /**
@@@ -991,8 -1071,9 +1096,14 @@@ static void blk_add_trace_rq_remap(voi
  	r.sector_from = cpu_to_be64(from);
  
  	__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),
++<<<<<<< HEAD
 +			rq_data_dir(rq), BLK_TA_REMAP, !!rq->errors,
 +			sizeof(r), &r);
++=======
+ 			rq_data_dir(rq), 0, BLK_TA_REMAP, 0,
+ 			sizeof(r), &r, blk_trace_request_get_cgid(q, rq));
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  
  /**
@@@ -1010,17 -1091,19 +1121,28 @@@ void blk_add_driver_data(struct request
  			 struct request *rq,
  			 void *data, size_t len)
  {
- 	struct blk_trace *bt = q->blk_trace;
+ 	struct blk_trace *bt;
  
- 	if (likely(!bt))
+ 	rcu_read_lock();
+ 	bt = rcu_dereference(q->blk_trace);
+ 	if (likely(!bt)) {
+ 		rcu_read_unlock();
  		return;
+ 	}
  
++<<<<<<< HEAD
 +	if (rq->cmd_type == REQ_TYPE_BLOCK_PC)
 +		__blk_add_trace(bt, 0, blk_rq_bytes(rq), 0,
 +				BLK_TA_DRV_DATA, rq->errors, len, data);
 +	else
 +		__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq), 0,
 +				BLK_TA_DRV_DATA, rq->errors, len, data);
++=======
+ 	__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,
+ 				BLK_TA_DRV_DATA, 0, len, data,
+ 				blk_trace_request_get_cgid(q, rq));
+ 	rcu_read_unlock();
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  }
  EXPORT_SYMBOL_GPL(blk_add_driver_data);
  
@@@ -1763,7 -1881,13 +1890,16 @@@ static ssize_t sysfs_blk_trace_attr_sto
  
  	mutex_lock(&q->blk_trace_mutex);
  
+ 	bt = rcu_dereference_protected(q->blk_trace,
+ 				       lockdep_is_held(&q->blk_trace_mutex));
  	if (attr == &dev_attr_enable) {
++<<<<<<< HEAD
++=======
+ 		if (!!value == !!bt) {
+ 			ret = 0;
+ 			goto out_unlock_bdev;
+ 		}
++>>>>>>> c780e86dd48e (blktrace: Protect q->blk_trace with RCU)
  		if (value)
  			ret = blk_trace_setup_queue(q, bdev);
  		else
* Unmerged path include/linux/blkdev.h
* Unmerged path include/linux/blktrace_api.h
* Unmerged path kernel/trace/blktrace.c
