esp4: add length check for UDP encapsulation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Sabrina Dubroca <sd@queasysnail.net>
commit 8dfb4eba4100e7cdd161a8baef2d8d61b7a7e62e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/8dfb4eba.failed

esp_output_udp_encap can produce a length that doesn't fit in the 16
bits of a UDP header's length field. In that case, we'll send a
fragmented packet whose length is larger than IP_MAX_MTU (resulting in
"Oversized IP packet" warnings on receive) and with a bogus UDP
length.

To prevent this, add a length check to esp_output_udp_encap and return
 -EMSGSIZE on failure.

This seems to be older than git history.

	Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
	Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
(cherry picked from commit 8dfb4eba4100e7cdd161a8baef2d8d61b7a7e62e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/esp4.c
diff --cc net/ipv4/esp4.c
index f9913e931a16,fb065a8937ea..000000000000
--- a/net/ipv4/esp4.c
+++ b/net/ipv4/esp4.c
@@@ -197,82 -223,260 +197,279 @@@ static int esp_output(struct xfrm_stat
  			tail[i] = i + 1;
  	} while (0);
  	tail[plen - 2] = plen - 2;
 -	tail[plen - 1] = proto;
 -}
 +	tail[plen - 1] = *skb_mac_header(skb);
 +	pskb_put(skb, trailer, clen - skb->len + alen);
  
++<<<<<<< HEAD
 +	skb_push(skb, -skb_network_offset(skb));
 +	esph = ip_esp_hdr(skb);
 +	*skb_mac_header(skb) = IPPROTO_ESP;
 +
 +	/* this is non-NULL only with UDP Encapsulation */
 +	if (x->encap) {
 +		struct xfrm_encap_tmpl *encap = x->encap;
 +		struct udphdr *uh;
 +		__be32 *udpdata32;
 +		__be16 sport, dport;
 +		int encap_type;
++=======
+ static int esp_output_udp_encap(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)
+ {
+ 	int encap_type;
+ 	struct udphdr *uh;
+ 	__be32 *udpdata32;
+ 	__be16 sport, dport;
+ 	struct xfrm_encap_tmpl *encap = x->encap;
+ 	struct ip_esp_hdr *esph = esp->esph;
+ 	unsigned int len;
+ 
+ 	spin_lock_bh(&x->lock);
+ 	sport = encap->encap_sport;
+ 	dport = encap->encap_dport;
+ 	encap_type = encap->encap_type;
+ 	spin_unlock_bh(&x->lock);
+ 
+ 	len = skb->len + esp->tailen - skb_transport_offset(skb);
+ 	if (len + sizeof(struct iphdr) >= IP_MAX_MTU)
+ 		return -EMSGSIZE;
+ 
+ 	uh = (struct udphdr *)esph;
+ 	uh->source = sport;
+ 	uh->dest = dport;
+ 	uh->len = htons(len);
+ 	uh->check = 0;
+ 
+ 	switch (encap_type) {
+ 	default:
+ 	case UDP_ENCAP_ESPINUDP:
+ 		esph = (struct ip_esp_hdr *)(uh + 1);
+ 		break;
+ 	case UDP_ENCAP_ESPINUDP_NON_IKE:
+ 		udpdata32 = (__be32 *)(uh + 1);
+ 		udpdata32[0] = udpdata32[1] = 0;
+ 		esph = (struct ip_esp_hdr *)(udpdata32 + 2);
+ 		break;
+ 	}
+ 
+ 	*skb_mac_header(skb) = IPPROTO_UDP;
+ 	esp->esph = esph;
+ 
+ 	return 0;
+ }
+ 
+ int esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)
+ {
+ 	u8 *tail;
+ 	u8 *vaddr;
+ 	int nfrags;
+ 	int esph_offset;
+ 	struct page *page;
+ 	struct sk_buff *trailer;
+ 	int tailen = esp->tailen;
+ 
+ 	/* this is non-NULL only with UDP Encapsulation */
+ 	if (x->encap) {
+ 		int err = esp_output_udp_encap(x, skb, esp);
+ 
+ 		if (err < 0)
+ 			return err;
+ 	}
+ 
+ 	if (!skb_cloned(skb)) {
+ 		if (tailen <= skb_tailroom(skb)) {
+ 			nfrags = 1;
+ 			trailer = skb;
+ 			tail = skb_tail_pointer(trailer);
+ 
+ 			goto skip_cow;
+ 		} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)
+ 			   && !skb_has_frag_list(skb)) {
+ 			int allocsize;
+ 			struct sock *sk = skb->sk;
+ 			struct page_frag *pfrag = &x->xfrag;
+ 
+ 			esp->inplace = false;
+ 
+ 			allocsize = ALIGN(tailen, L1_CACHE_BYTES);
+ 
+ 			spin_lock_bh(&x->lock);
+ 
+ 			if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ 				spin_unlock_bh(&x->lock);
+ 				goto cow;
+ 			}
+ 
+ 			page = pfrag->page;
+ 			get_page(page);
+ 
+ 			vaddr = kmap_atomic(page);
+ 
+ 			tail = vaddr + pfrag->offset;
+ 
+ 			esp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);
+ 
+ 			kunmap_atomic(vaddr);
+ 
+ 			nfrags = skb_shinfo(skb)->nr_frags;
+ 
+ 			__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,
+ 					     tailen);
+ 			skb_shinfo(skb)->nr_frags = ++nfrags;
+ 
+ 			pfrag->offset = pfrag->offset + allocsize;
+ 
+ 			spin_unlock_bh(&x->lock);
+ 
+ 			nfrags++;
+ 
+ 			skb->len += tailen;
+ 			skb->data_len += tailen;
+ 			skb->truesize += tailen;
+ 			if (sk && sk_fullsock(sk))
+ 				refcount_add(tailen, &sk->sk_wmem_alloc);
+ 
+ 			goto out;
+ 		}
+ 	}
+ 
+ cow:
+ 	esph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);
+ 
+ 	nfrags = skb_cow_data(skb, tailen, &trailer);
+ 	if (nfrags < 0)
+ 		goto out;
+ 	tail = skb_tail_pointer(trailer);
+ 	esp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);
+ 
+ skip_cow:
+ 	esp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);
+ 	pskb_put(skb, trailer, tailen);
+ 
+ out:
+ 	return nfrags;
+ }
+ EXPORT_SYMBOL_GPL(esp_output_head);
+ 
+ int esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)
+ {
+ 	u8 *iv;
+ 	int alen;
+ 	void *tmp;
+ 	int ivlen;
+ 	int assoclen;
+ 	int extralen;
+ 	struct page *page;
+ 	struct ip_esp_hdr *esph;
+ 	struct crypto_aead *aead;
+ 	struct aead_request *req;
+ 	struct scatterlist *sg, *dsg;
+ 	struct esp_output_extra *extra;
+ 	int err = -ENOMEM;
+ 
+ 	assoclen = sizeof(struct ip_esp_hdr);
+ 	extralen = 0;
+ 
+ 	if (x->props.flags & XFRM_STATE_ESN) {
+ 		extralen += sizeof(*extra);
+ 		assoclen += sizeof(__be32);
+ 	}
+ 
+ 	aead = x->data;
+ 	alen = crypto_aead_authsize(aead);
+ 	ivlen = crypto_aead_ivsize(aead);
+ 
+ 	tmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);
+ 	if (!tmp)
+ 		goto error;
+ 
+ 	extra = esp_tmp_extra(tmp);
+ 	iv = esp_tmp_iv(aead, tmp, extralen);
+ 	req = esp_tmp_req(aead, iv);
+ 	sg = esp_req_sg(aead, req);
+ 
+ 	if (esp->inplace)
+ 		dsg = sg;
+ 	else
+ 		dsg = &sg[esp->nfrags];
+ 
+ 	esph = esp_output_set_extra(skb, x, esp->esph, extra);
+ 	esp->esph = esph;
+ 
+ 	sg_init_table(sg, esp->nfrags);
+ 	err = skb_to_sgvec(skb, sg,
+ 		           (unsigned char *)esph - skb->data,
+ 		           assoclen + ivlen + esp->clen + alen);
+ 	if (unlikely(err < 0))
+ 		goto error_free;
+ 
+ 	if (!esp->inplace) {
+ 		int allocsize;
+ 		struct page_frag *pfrag = &x->xfrag;
+ 
+ 		allocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);
++>>>>>>> 8dfb4eba4100 (esp4: add length check for UDP encapsulation)
  
  		spin_lock_bh(&x->lock);
 -		if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
 -			spin_unlock_bh(&x->lock);
 -			goto error_free;
 -		}
 +		sport = encap->encap_sport;
 +		dport = encap->encap_dport;
 +		encap_type = encap->encap_type;
 +		spin_unlock_bh(&x->lock);
  
 -		skb_shinfo(skb)->nr_frags = 1;
 +		uh = (struct udphdr *)esph;
 +		uh->source = sport;
 +		uh->dest = dport;
 +		uh->len = htons(skb->len - skb_transport_offset(skb));
 +		uh->check = 0;
  
 -		page = pfrag->page;
 -		get_page(page);
 -		/* replace page frags in skb with new page */
 -		__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);
 -		pfrag->offset = pfrag->offset + allocsize;
 -		spin_unlock_bh(&x->lock);
 +		switch (encap_type) {
 +		default:
 +		case UDP_ENCAP_ESPINUDP:
 +			esph = (struct ip_esp_hdr *)(uh + 1);
 +			break;
 +		case UDP_ENCAP_ESPINUDP_NON_IKE:
 +			udpdata32 = (__be32 *)(uh + 1);
 +			udpdata32[0] = udpdata32[1] = 0;
 +			esph = (struct ip_esp_hdr *)(udpdata32 + 2);
 +			break;
 +		}
  
 -		sg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);
 -		err = skb_to_sgvec(skb, dsg,
 -			           (unsigned char *)esph - skb->data,
 -			           assoclen + ivlen + esp->clen + alen);
 -		if (unlikely(err < 0))
 -			goto error_free;
 +		*skb_mac_header(skb) = IPPROTO_UDP;
  	}
  
 -	if ((x->props.flags & XFRM_STATE_ESN))
 -		aead_request_set_callback(req, 0, esp_output_done_esn, skb);
 -	else
 -		aead_request_set_callback(req, 0, esp_output_done, skb);
 +	esph->spi = x->id.spi;
 +	esph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);
  
 -	aead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);
 -	aead_request_set_ad(req, assoclen);
 +	sg_init_table(sg, nfrags);
 +	skb_to_sgvec(skb, sg,
 +		     esph->enc_data + crypto_aead_ivsize(aead) - skb->data,
 +		     clen + alen);
  
 -	memset(iv, 0, ivlen);
 -	memcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),
 -	       min(ivlen, 8));
 +	if ((x->props.flags & XFRM_STATE_ESN)) {
 +		sg_init_table(asg, 3);
 +		sg_set_buf(asg, &esph->spi, sizeof(__be32));
 +		*seqhi = htonl(XFRM_SKB_CB(skb)->seq.output.hi);
 +		sg_set_buf(asg + 1, seqhi, seqhilen);
 +		sg_set_buf(asg + 2, &esph->seq_no, sizeof(__be32));
 +	} else
 +		sg_init_one(asg, esph, sizeof(*esph));
 +
 +	aead_givcrypt_set_callback(req, 0, esp_output_done, skb);
 +	aead_givcrypt_set_crypt(req, sg, sg, clen, iv);
 +	aead_givcrypt_set_assoc(req, asg, assoclen);
 +	aead_givcrypt_set_giv(req, esph->enc_data,
 +			      XFRM_SKB_CB(skb)->seq.output.low +
 +			      ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));
  
  	ESP_SKB_CB(skb)->tmp = tmp;
 -	err = crypto_aead_encrypt(req);
 -
 -	switch (err) {
 -	case -EINPROGRESS:
 +	err = crypto_aead_givencrypt(req);
 +	if (err == -EINPROGRESS)
  		goto error;
  
 -	case -ENOSPC:
 +	if (err == -EBUSY)
  		err = NET_XMIT_DROP;
 -		break;
 -
 -	case 0:
 -		if ((x->props.flags & XFRM_STATE_ESN))
 -			esp_output_restore_header(skb);
 -	}
 -
 -	if (sg != dsg)
 -		esp_ssg_unref(x, tmp);
  
 -error_free:
  	kfree(tmp);
 +
  error:
  	return err;
  }
* Unmerged path net/ipv4/esp4.c
