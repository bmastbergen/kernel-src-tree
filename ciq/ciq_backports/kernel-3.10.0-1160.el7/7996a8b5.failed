blk-mq: fix hang caused by freeze/unfreeze sequence

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Bob Liu <bob.liu@oracle.com>
commit 7996a8b5511a72465b0b286763c2d8f412b8874a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/7996a8b5.failed

The following is a description of a hang in blk_mq_freeze_queue_wait().
The hang happens on attempt to freeze a queue while another task does
queue unfreeze.

The root cause is an incorrect sequence of percpu_ref_resurrect() and
percpu_ref_kill() and as a result those two can be swapped:

 CPU#0                         CPU#1
 ----------------              -----------------
 q1 = blk_mq_init_queue(shared_tags)

                                q2 = blk_mq_init_queue(shared_tags):
                                  blk_mq_add_queue_tag_set(shared_tags):
                                    blk_mq_update_tag_set_depth(shared_tags):
				     list_for_each_entry()
                                      blk_mq_freeze_queue(q1)
                                       > percpu_ref_kill()
                                       > blk_mq_freeze_queue_wait()

 blk_cleanup_queue(q1)
  blk_mq_freeze_queue(q1)
   > percpu_ref_kill()
                 ^^^^^^ freeze_depth can't guarantee the order

                                      blk_mq_unfreeze_queue()
                                        > percpu_ref_resurrect()

   > blk_mq_freeze_queue_wait()
                 ^^^^^^ Hang here!!!!

This wrong sequence raises kernel warning:
percpu_ref_kill_and_confirm called more than once on blk_queue_usage_counter_release!
WARNING: CPU: 0 PID: 11854 at lib/percpu-refcount.c:336 percpu_ref_kill_and_confirm+0x99/0xb0

But the most unpleasant effect is a hang of a blk_mq_freeze_queue_wait(),
which waits for a zero of a q_usage_counter, which never happens
because percpu-ref was reinited (instead of being killed) and stays in
PERCPU state forever.

How to reproduce:
 - "insmod null_blk.ko shared_tags=1 nr_devices=0 queue_mode=2"
 - cpu0: python Script.py 0; taskset the corresponding process running on cpu0
 - cpu1: python Script.py 1; taskset the corresponding process running on cpu1

 Script.py:
 ------
 #!/usr/bin/python3

import os
import sys

while True:
    on = "echo 1 > /sys/kernel/config/nullb/%s/power" % sys.argv[1]
    off = "echo 0 > /sys/kernel/config/nullb/%s/power" % sys.argv[1]
    os.system(on)
    os.system(off)
------

This bug was first reported and fixed by Roman, previous discussion:
[1] Message id: 1443287365-4244-7-git-send-email-akinobu.mita@gmail.com
[2] Message id: 1443563240-29306-6-git-send-email-tj@kernel.org
[3] https://patchwork.kernel.org/patch/9268199/

	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Bart Van Assche <bvanassche@acm.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Roman Pen <roman.penyaev@profitbricks.com>
	Signed-off-by: Bob Liu <bob.liu@oracle.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7996a8b5511a72465b0b286763c2d8f412b8874a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq.c
#	include/linux/blkdev.h
diff --cc block/blk-core.c
index e7298f9a55a9,1bf83a0df0f6..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -778,8 -413,9 +778,14 @@@ int blk_queue_enter(struct request_queu
  		smp_rmb();
  
  		wait_event(q->mq_freeze_wq,
++<<<<<<< HEAD
 +			   (atomic_read(&q->mq_freeze_depth) == 0 &&
 +			    (preempt || !blk_queue_preempt_only(q))) ||
++=======
+ 			   (!q->mq_freeze_depth &&
+ 			    (pm || (blk_pm_request_resume(q),
+ 				    !blk_queue_pm_only(q)))) ||
++>>>>>>> 7996a8b5511a (blk-mq: fix hang caused by freeze/unfreeze sequence)
  			   blk_queue_dying(q));
  		if (blk_queue_dying(q))
  			return -ENODEV;
@@@ -872,24 -500,13 +878,25 @@@ struct request_queue *blk_alloc_queue_n
  	mutex_init(&q->blk_trace_mutex);
  #endif
  	mutex_init(&q->sysfs_lock);
 -	spin_lock_init(&q->queue_lock);
 +	spin_lock_init(&q->__queue_lock);
  
 -	init_waitqueue_head(&q->mq_freeze_wq);
 -	mutex_init(&q->mq_freeze_lock);
 +	if (!q->mq_ops)
 +		q->queue_lock = lock ? : &q->__queue_lock;
  
  	/*
 -	 * Init percpu_ref in atomic mode so that it's faster to shutdown.
 +	 * A queue starts its life with bypass turned on to avoid
 +	 * unnecessary bypass on/off overhead and nasty surprises during
 +	 * init.  The initial bypass will be finished when the queue is
 +	 * registered by blk_register_queue().
 +	 */
 +	q->bypass_depth = 1;
 +	__set_bit(QUEUE_FLAG_BYPASS, &q->queue_flags);
 +
 +	init_waitqueue_head(&q->mq_freeze_wq);
++	mutex_init(&q->mq_freeze_lock);
 +
 +	/*
 +	 * Init percpu_ref in atomic mode so that it's faster to shutdown.
  	 * See blk_register_queue() for details.
  	 */
  	if (percpu_ref_init(&q->q_usage_counter,
diff --cc block/blk-mq.c
index 05de6b361214,32b8ad3d341b..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -124,13 -144,14 +124,18 @@@ void blk_mq_in_flight_rw(struct request
  
  void blk_freeze_queue_start(struct request_queue *q)
  {
- 	int freeze_depth;
- 
- 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
- 	if (freeze_depth == 1) {
+ 	mutex_lock(&q->mq_freeze_lock);
+ 	if (++q->mq_freeze_depth == 1) {
  		percpu_ref_kill(&q->q_usage_counter);
++<<<<<<< HEAD
 +		if (q->mq_ops)
++=======
+ 		mutex_unlock(&q->mq_freeze_lock);
+ 		if (queue_is_mq(q))
++>>>>>>> 7996a8b5511a (blk-mq: fix hang caused by freeze/unfreeze sequence)
  			blk_mq_run_hw_queues(q, false);
+ 	} else {
+ 		mutex_unlock(&q->mq_freeze_lock);
  	}
  }
  EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
@@@ -181,14 -200,14 +186,23 @@@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue)
  
  void blk_mq_unfreeze_queue(struct request_queue *q)
  {
++<<<<<<< HEAD
 +	int freeze_depth;
 +
 +	freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
 +	WARN_ON_ONCE(freeze_depth < 0);
 +	if (!freeze_depth) {
 +		percpu_ref_reinit(&q->q_usage_counter);
++=======
+ 	mutex_lock(&q->mq_freeze_lock);
+ 	q->mq_freeze_depth--;
+ 	WARN_ON_ONCE(q->mq_freeze_depth < 0);
+ 	if (!q->mq_freeze_depth) {
+ 		percpu_ref_resurrect(&q->q_usage_counter);
++>>>>>>> 7996a8b5511a (blk-mq: fix hang caused by freeze/unfreeze sequence)
  		wake_up_all(&q->mq_freeze_wq);
  	}
+ 	mutex_unlock(&q->mq_freeze_lock);
  }
  EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  
diff --cc include/linux/blkdev.h
index 8bef3eb8d49a,592669bcc536..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -517,11 -535,16 +517,22 @@@ struct request_queue 
  
  	struct mutex		sysfs_lock;
  
++<<<<<<< HEAD
 +	int			bypass_depth;
++=======
+ 	/*
+ 	 * for reusing dead hctx instance in case of updating
+ 	 * nr_hw_queues
+ 	 */
+ 	struct list_head	unused_hctx_list;
+ 	spinlock_t		unused_hctx_lock;
+ 
+ 	int			mq_freeze_depth;
++>>>>>>> 7996a8b5511a (blk-mq: fix hang caused by freeze/unfreeze sequence)
  
  #if defined(CONFIG_BLK_DEV_BSG)
 +	bsg_job_fn		*bsg_job_fn;
 +	int			bsg_job_size;
  	struct bsg_class_device bsg_dev;
  #endif
  
@@@ -531,165 -554,66 +542,174 @@@
  #endif
  	struct rcu_head		rcu_head;
  	wait_queue_head_t	mq_freeze_wq;
++<<<<<<< HEAD
 +	RH_KABI_DEPRECATE(struct percpu_counter, mq_usage_counter)
 +	struct list_head	all_q_node;
++=======
+ 	/*
+ 	 * Protect concurrent access to q_usage_counter by
+ 	 * percpu_ref_kill() and percpu_ref_reinit().
+ 	 */
+ 	struct mutex		mq_freeze_lock;
+ 	struct percpu_ref	q_usage_counter;
++>>>>>>> 7996a8b5511a (blk-mq: fix hang caused by freeze/unfreeze sequence)
 +
 +	RH_KABI_EXTEND(unprep_rq_fn		*unprep_rq_fn)
 +
 +	RH_KABI_EXTEND(struct blk_mq_tag_set	*tag_set)
 +	RH_KABI_EXTEND(struct list_head		tag_set_list)
 +
 +	RH_KABI_EXTEND(struct list_head		requeue_list)
 +	RH_KABI_EXTEND(spinlock_t			requeue_lock)
 +	/* requeue_work's type is changed from 'work_struct' to 'delayed_work' below */
 +	RH_KABI_EXTEND(struct work_struct	rh_reserved_requeue_work)
 +	RH_KABI_EXTEND(atomic_t				mq_freeze_depth)
 +	RH_KABI_EXTEND(struct blk_flush_queue   *fq)
 +	RH_KABI_EXTEND(struct percpu_ref	q_usage_counter)
 +	RH_KABI_EXTEND(bool			mq_sysfs_init_done)
 +	RH_KABI_EXTEND(struct work_struct	timeout_work)
 +	RH_KABI_EXTEND(struct delayed_work	requeue_work)
 +	RH_KABI_EXTEND(struct blk_queue_stats	*stats)
 +	RH_KABI_EXTEND(struct blk_stat_callback	*poll_cb)
 +	RH_KABI_EXTEND(struct blk_rq_stat	poll_stat[2])
 +	RH_KABI_EXTEND(atomic_t		shared_hctx_restart)
 +	RH_KABI_EXTEND(unsigned int		queue_depth)
  
 -	struct blk_mq_tag_set	*tag_set;
 -	struct list_head	tag_set_list;
 -	struct bio_set		bio_split;
 +	/*
 +	 * The flag need to be set if this queue is blk-mq queue and at
 +	 * the top of other blk-mq queues, such as DM/MPATH. We don't know
 +	 * if there are such 3rd party queues, and if there are, they
 +	 * need to set this flag too. This flag is for avoiding IO hang
 +	 * in blk_mq_queue_reinit_notify().
 +	 */
 +	RH_KABI_EXTEND(unsigned int         front_queue:1)
  
 +	/*
 +	 * The flag need to be set for queues which are depended by other
 +	 * IO queues, so far, the only one is NVMe's admin queue. This flag
 +	 * is for avoiding IO hang in blk_mq_queue_reinit_notify().
 +	 */
 +	RH_KABI_EXTEND(unsigned int         tail_queue:1)
 +
 +	/* This flag is set if the driver can split bio */
 +	RH_KABI_EXTEND(unsigned int         can_split_bio:1)
  #ifdef CONFIG_BLK_DEBUG_FS
 -	struct dentry		*debugfs_dir;
 -	struct dentry		*sched_debugfs_dir;
 -	struct dentry		*rqos_debugfs_dir;
 +	RH_KABI_EXTEND(struct dentry		*debugfs_dir)
 +	RH_KABI_EXTEND(struct dentry		*sched_debugfs_dir)
 +#endif
 +#ifdef CONFIG_BLK_DEV_IO_TRACE
 +	RH_KABI_EXTEND(struct mutex		blk_trace_mutex)
  #endif
  
 -	bool			mq_sysfs_init_done;
 -
 -	size_t			cmd_size;
 -
 -	struct work_struct	release_work;
 -
 -#define BLK_MAX_WRITE_HINTS	5
 -	u64			write_hints[BLK_MAX_WRITE_HINTS];
 +	RH_KABI_EXTEND(init_rq_fn		*init_rq_fn)
 +	RH_KABI_EXTEND(exit_rq_fn		*exit_rq_fn)
 +	RH_KABI_EXTEND(size_t			cmd_size)
 +	RH_KABI_EXTEND(void			*rq_alloc_data)
  };
  
 -#define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 -#define QUEUE_FLAG_DYING	1	/* queue being torn down */
 -#define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
 -#define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
 -#define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
 -#define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
 -#define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
 -#define QUEUE_FLAG_IO_STAT	7	/* do disk/partitions IO accounting */
 -#define QUEUE_FLAG_DISCARD	8	/* supports DISCARD */
 -#define QUEUE_FLAG_NOXMERGES	9	/* No extended merges */
 -#define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
 -#define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
 -#define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 -#define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 -#define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
 -#define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
 -#define QUEUE_FLAG_WC		17	/* Write back caching */
 -#define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 -#define QUEUE_FLAG_DAX		19	/* device supports DAX */
 -#define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 -#define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 -#define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 -#define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
 -#define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 -#define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 +#define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 +#define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
 +#define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
 +#define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 +#define QUEUE_FLAG_DYING	5	/* queue being torn down */
 +#define QUEUE_FLAG_BYPASS	6	/* act as dumb FIFO queue */
 +#define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
 +#define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
 +#define QUEUE_FLAG_SAME_COMP	9	/* complete on same CPU-group */
 +#define QUEUE_FLAG_FAIL_IO     10	/* fake timeout */
 +#define QUEUE_FLAG_STACKABLE   11	/* supports request stacking */
 +#define QUEUE_FLAG_NONROT      12	/* non-rotational device (SSD) */
 +#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 +#define QUEUE_FLAG_IO_STAT     13	/* do IO stats */
 +#define QUEUE_FLAG_DISCARD     14	/* supports DISCARD */
 +#define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
 +#define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
 +#define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
 +#define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 +#define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 +#define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
 +#define QUEUE_FLAG_REGISTERED  25	/* queue has been registered to a disk */
 +#define QUEUE_FLAG_STATS       26	/* track rq completion times */
 +#define QUEUE_FLAG_POLL_STATS  27	/* collecting stats for hybrid polling */
 +#define QUEUE_FLAG_PREEMPT_ONLY	28	/* only process REQ_PREEMPT requests */
 +#define QUEUE_FLAG_QUIESCED    29	/* queue has been quiesced */
 +#define QUEUE_FLAG_SCSI_PASSTHROUGH 30	/* queue supports SCSI commands */
 +
 +#define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 +				 (1 << QUEUE_FLAG_STACKABLE)	|	\
 +				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 +				 (1 << QUEUE_FLAG_ADD_RANDOM))
  
  #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 +				 (1 << QUEUE_FLAG_STACKABLE)	|	\
  				 (1 << QUEUE_FLAG_SAME_COMP))
  
 -void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 -void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
 -bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 +static inline void queue_lockdep_assert_held(struct request_queue *q)
 +{
 +	if (q->queue_lock)
 +		lockdep_assert_held(q->queue_lock);
 +}
 +
 +static inline void queue_flag_set_unlocked(unsigned int flag,
 +					   struct request_queue *q)
 +{
 +	__set_bit(flag, &q->queue_flags);
 +}
 +
 +static inline int queue_flag_test_and_clear(unsigned int flag,
 +					    struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +
 +	if (test_bit(flag, &q->queue_flags)) {
 +		__clear_bit(flag, &q->queue_flags);
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static inline int queue_flag_test_and_set(unsigned int flag,
 +					  struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +
 +	if (!test_bit(flag, &q->queue_flags)) {
 +		__set_bit(flag, &q->queue_flags);
 +		return 0;
 +	}
  
 +	return 1;
 +}
 +
 +static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +	__set_bit(flag, &q->queue_flags);
 +}
 +
 +static inline void queue_flag_clear_unlocked(unsigned int flag,
 +					     struct request_queue *q)
 +{
 +	__clear_bit(flag, &q->queue_flags);
 +}
 +
 +static inline int queue_in_flight(struct request_queue *q)
 +{
 +	return q->in_flight[0] + q->in_flight[1];
 +}
 +
 +static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 +{
 +	queue_lockdep_assert_held(q);
 +	__clear_bit(flag, &q->queue_flags);
 +}
 +
 +#define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
  #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
  #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
  #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blkdev.h
