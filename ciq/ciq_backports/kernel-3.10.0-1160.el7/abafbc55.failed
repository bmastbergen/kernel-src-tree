vfio-pci: Invalidate mmaps and block MMIO access on disabled memory

jira LE-1907
cve CVE-2020-12888
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Alex Williamson <alex.williamson@redhat.com>
commit abafbc551fddede3e0a08dee1dcde08fc0eb8476
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/abafbc55.failed

Accessing the disabled memory space of a PCI device would typically
result in a master abort response on conventional PCI, or an
unsupported request on PCI express.  The user would generally see
these as a -1 response for the read return data and the write would be
silently discarded, possibly with an uncorrected, non-fatal AER error
triggered on the host.  Some systems however take it upon themselves
to bring down the entire system when they see something that might
indicate a loss of data, such as this discarded write to a disabled
memory space.

To avoid this, we want to try to block the user from accessing memory
spaces while they're disabled.  We start with a semaphore around the
memory enable bit, where writers modify the memory enable state and
must be serialized, while readers make use of the memory region and
can access in parallel.  Writers include both direct manipulation via
the command register, as well as any reset path where the internal
mechanics of the reset may both explicitly and implicitly disable
memory access, and manipulation of the MSI-X configuration, where the
MSI-X vector table resides in MMIO space of the device.  Readers
include the read and write file ops to access the vfio device fd
offsets as well as memory mapped access.  In the latter case, we make
use of our new vma list support to zap, or invalidate, those memory
mappings in order to force them to be faulted back in on access.

Our semaphore usage will stall user access to MMIO spaces across
internal operations like reset, but the user might experience new
behavior when trying to access the MMIO space while disabled via the
PCI command register.  Access via read or write while disabled will
return -EIO and access via memory maps will result in a SIGBUS.  This
is expected to be compatible with known use cases and potentially
provides better error handling capabilities than present in the
hardware, while avoiding the more readily accessible and severe
platform error responses that might otherwise occur.

Fixes: CVE-2020-12888
	Reviewed-by: Peter Xu <peterx@redhat.com>
	Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
(cherry picked from commit abafbc551fddede3e0a08dee1dcde08fc0eb8476)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vfio/pci/vfio_pci.c
#	drivers/vfio/pci/vfio_pci_intrs.c
#	drivers/vfio/pci/vfio_pci_private.h
#	drivers/vfio/pci/vfio_pci_rdwr.c
diff --cc drivers/vfio/pci/vfio_pci.c
index 2b338a10c308,aabba6439a5b..000000000000
--- a/drivers/vfio/pci/vfio_pci.c
+++ b/drivers/vfio/pci/vfio_pci.c
@@@ -28,6 -25,8 +28,11 @@@
  #include <linux/uaccess.h>
  #include <linux/vfio.h>
  #include <linux/vgaarb.h>
++<<<<<<< HEAD
++=======
+ #include <linux/nospec.h>
+ #include <linux/sched/mm.h>
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  
  #include "vfio_pci_private.h"
  
@@@ -174,6 -184,8 +179,11 @@@ no_mmap
  }
  
  static void vfio_pci_try_bus_reset(struct vfio_pci_device *vdev);
++<<<<<<< HEAD
++=======
+ static void vfio_pci_disable(struct vfio_pci_device *vdev);
+ static int vfio_pci_try_zap_and_vma_lock_cb(struct pci_dev *pdev, void *data);
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  
  /*
   * INTx masking requires the ability to disable INTx signaling via PCI_COMMAND
@@@ -1019,14 -1168,63 +1042,70 @@@ reset_info_exit
  		ret = vfio_pci_for_each_slot_or_bus(vdev->pdev,
  						    vfio_pci_validate_devs,
  						    &info, slot);
++<<<<<<< HEAD
 +		if (!ret)
 +			/* User has access, do the reset */
 +			ret = slot ? pci_try_reset_slot(vdev->pdev->slot) :
 +				     pci_try_reset_bus(vdev->pdev->bus);
++=======
+ 		if (ret)
+ 			goto hot_reset_release;
+ 
+ 		devs.max_index = count;
+ 		devs.devices = kcalloc(count, sizeof(struct vfio_device *),
+ 				       GFP_KERNEL);
+ 		if (!devs.devices) {
+ 			ret = -ENOMEM;
+ 			goto hot_reset_release;
+ 		}
+ 
+ 		/*
+ 		 * We need to get memory_lock for each device, but devices
+ 		 * can share mmap_sem, therefore we need to zap and hold
+ 		 * the vma_lock for each device, and only then get each
+ 		 * memory_lock.
+ 		 */
+ 		ret = vfio_pci_for_each_slot_or_bus(vdev->pdev,
+ 					    vfio_pci_try_zap_and_vma_lock_cb,
+ 					    &devs, slot);
+ 		if (ret)
+ 			goto hot_reset_release;
+ 
+ 		for (; mem_idx < devs.cur_index; mem_idx++) {
+ 			struct vfio_pci_device *tmp;
+ 
+ 			tmp = vfio_device_data(devs.devices[mem_idx]);
+ 
+ 			ret = down_write_trylock(&tmp->memory_lock);
+ 			if (!ret) {
+ 				ret = -EBUSY;
+ 				goto hot_reset_release;
+ 			}
+ 			mutex_unlock(&tmp->vma_lock);
+ 		}
+ 
+ 		/* User has access, do the reset */
+ 		ret = pci_reset_bus(vdev->pdev);
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  
  hot_reset_release:
- 		for (i--; i >= 0; i--)
- 			vfio_group_put_external_user(groups[i].group);
+ 		for (i = 0; i < devs.cur_index; i++) {
+ 			struct vfio_device *device;
+ 			struct vfio_pci_device *tmp;
+ 
+ 			device = devs.devices[i];
+ 			tmp = vfio_device_data(device);
+ 
+ 			if (i < mem_idx)
+ 				up_write(&tmp->memory_lock);
+ 			else
+ 				mutex_unlock(&tmp->vma_lock);
+ 			vfio_device_put(device);
+ 		}
+ 		kfree(devs.devices);
+ 
+ 		for (group_idx--; group_idx >= 0; group_idx--)
+ 			vfio_group_put_external_user(groups[group_idx].group);
  
  		kfree(groups);
  		return ret;
@@@ -1085,6 -1364,202 +1164,205 @@@ static ssize_t vfio_pci_write(void *dev
  	return vfio_pci_rw(device_data, (char __user *)buf, count, ppos, true);
  }
  
++<<<<<<< HEAD
++=======
+ /* Return 1 on zap and vma_lock acquired, 0 on contention (only with @try) */
+ static int vfio_pci_zap_and_vma_lock(struct vfio_pci_device *vdev, bool try)
+ {
+ 	struct vfio_pci_mmap_vma *mmap_vma, *tmp;
+ 
+ 	/*
+ 	 * Lock ordering:
+ 	 * vma_lock is nested under mmap_sem for vm_ops callback paths.
+ 	 * The memory_lock semaphore is used by both code paths calling
+ 	 * into this function to zap vmas and the vm_ops.fault callback
+ 	 * to protect the memory enable state of the device.
+ 	 *
+ 	 * When zapping vmas we need to maintain the mmap_sem => vma_lock
+ 	 * ordering, which requires using vma_lock to walk vma_list to
+ 	 * acquire an mm, then dropping vma_lock to get the mmap_sem and
+ 	 * reacquiring vma_lock.  This logic is derived from similar
+ 	 * requirements in uverbs_user_mmap_disassociate().
+ 	 *
+ 	 * mmap_sem must always be the top-level lock when it is taken.
+ 	 * Therefore we can only hold the memory_lock write lock when
+ 	 * vma_list is empty, as we'd need to take mmap_sem to clear
+ 	 * entries.  vma_list can only be guaranteed empty when holding
+ 	 * vma_lock, thus memory_lock is nested under vma_lock.
+ 	 *
+ 	 * This enables the vm_ops.fault callback to acquire vma_lock,
+ 	 * followed by memory_lock read lock, while already holding
+ 	 * mmap_sem without risk of deadlock.
+ 	 */
+ 	while (1) {
+ 		struct mm_struct *mm = NULL;
+ 
+ 		if (try) {
+ 			if (!mutex_trylock(&vdev->vma_lock))
+ 				return 0;
+ 		} else {
+ 			mutex_lock(&vdev->vma_lock);
+ 		}
+ 		while (!list_empty(&vdev->vma_list)) {
+ 			mmap_vma = list_first_entry(&vdev->vma_list,
+ 						    struct vfio_pci_mmap_vma,
+ 						    vma_next);
+ 			mm = mmap_vma->vma->vm_mm;
+ 			if (mmget_not_zero(mm))
+ 				break;
+ 
+ 			list_del(&mmap_vma->vma_next);
+ 			kfree(mmap_vma);
+ 			mm = NULL;
+ 		}
+ 		if (!mm)
+ 			return 1;
+ 		mutex_unlock(&vdev->vma_lock);
+ 
+ 		if (try) {
+ 			if (!down_read_trylock(&mm->mmap_sem)) {
+ 				mmput(mm);
+ 				return 0;
+ 			}
+ 		} else {
+ 			down_read(&mm->mmap_sem);
+ 		}
+ 		if (mmget_still_valid(mm)) {
+ 			if (try) {
+ 				if (!mutex_trylock(&vdev->vma_lock)) {
+ 					up_read(&mm->mmap_sem);
+ 					mmput(mm);
+ 					return 0;
+ 				}
+ 			} else {
+ 				mutex_lock(&vdev->vma_lock);
+ 			}
+ 			list_for_each_entry_safe(mmap_vma, tmp,
+ 						 &vdev->vma_list, vma_next) {
+ 				struct vm_area_struct *vma = mmap_vma->vma;
+ 
+ 				if (vma->vm_mm != mm)
+ 					continue;
+ 
+ 				list_del(&mmap_vma->vma_next);
+ 				kfree(mmap_vma);
+ 
+ 				zap_vma_ptes(vma, vma->vm_start,
+ 					     vma->vm_end - vma->vm_start);
+ 			}
+ 			mutex_unlock(&vdev->vma_lock);
+ 		}
+ 		up_read(&mm->mmap_sem);
+ 		mmput(mm);
+ 	}
+ }
+ 
+ void vfio_pci_zap_and_down_write_memory_lock(struct vfio_pci_device *vdev)
+ {
+ 	vfio_pci_zap_and_vma_lock(vdev, false);
+ 	down_write(&vdev->memory_lock);
+ 	mutex_unlock(&vdev->vma_lock);
+ }
+ 
+ u16 vfio_pci_memory_lock_and_enable(struct vfio_pci_device *vdev)
+ {
+ 	u16 cmd;
+ 
+ 	down_write(&vdev->memory_lock);
+ 	pci_read_config_word(vdev->pdev, PCI_COMMAND, &cmd);
+ 	if (!(cmd & PCI_COMMAND_MEMORY))
+ 		pci_write_config_word(vdev->pdev, PCI_COMMAND,
+ 				      cmd | PCI_COMMAND_MEMORY);
+ 
+ 	return cmd;
+ }
+ 
+ void vfio_pci_memory_unlock_and_restore(struct vfio_pci_device *vdev, u16 cmd)
+ {
+ 	pci_write_config_word(vdev->pdev, PCI_COMMAND, cmd);
+ 	up_write(&vdev->memory_lock);
+ }
+ 
+ /* Caller holds vma_lock */
+ static int __vfio_pci_add_vma(struct vfio_pci_device *vdev,
+ 			      struct vm_area_struct *vma)
+ {
+ 	struct vfio_pci_mmap_vma *mmap_vma;
+ 
+ 	mmap_vma = kmalloc(sizeof(*mmap_vma), GFP_KERNEL);
+ 	if (!mmap_vma)
+ 		return -ENOMEM;
+ 
+ 	mmap_vma->vma = vma;
+ 	list_add(&mmap_vma->vma_next, &vdev->vma_list);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Zap mmaps on open so that we can fault them in on access and therefore
+  * our vma_list only tracks mappings accessed since last zap.
+  */
+ static void vfio_pci_mmap_open(struct vm_area_struct *vma)
+ {
+ 	zap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);
+ }
+ 
+ static void vfio_pci_mmap_close(struct vm_area_struct *vma)
+ {
+ 	struct vfio_pci_device *vdev = vma->vm_private_data;
+ 	struct vfio_pci_mmap_vma *mmap_vma;
+ 
+ 	mutex_lock(&vdev->vma_lock);
+ 	list_for_each_entry(mmap_vma, &vdev->vma_list, vma_next) {
+ 		if (mmap_vma->vma == vma) {
+ 			list_del(&mmap_vma->vma_next);
+ 			kfree(mmap_vma);
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&vdev->vma_lock);
+ }
+ 
+ static vm_fault_t vfio_pci_mmap_fault(struct vm_fault *vmf)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct vfio_pci_device *vdev = vma->vm_private_data;
+ 	vm_fault_t ret = VM_FAULT_NOPAGE;
+ 
+ 	mutex_lock(&vdev->vma_lock);
+ 	down_read(&vdev->memory_lock);
+ 
+ 	if (!__vfio_pci_memory_enabled(vdev)) {
+ 		ret = VM_FAULT_SIGBUS;
+ 		mutex_unlock(&vdev->vma_lock);
+ 		goto up_out;
+ 	}
+ 
+ 	if (__vfio_pci_add_vma(vdev, vma)) {
+ 		ret = VM_FAULT_OOM;
+ 		mutex_unlock(&vdev->vma_lock);
+ 		goto up_out;
+ 	}
+ 
+ 	mutex_unlock(&vdev->vma_lock);
+ 
+ 	if (remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,
+ 			    vma->vm_end - vma->vm_start, vma->vm_page_prot))
+ 		ret = VM_FAULT_SIGBUS;
+ 
+ up_out:
+ 	up_read(&vdev->memory_lock);
+ 	return ret;
+ }
+ 
+ static const struct vm_operations_struct vfio_pci_mmap_ops = {
+ 	.open = vfio_pci_mmap_open,
+ 	.close = vfio_pci_mmap_close,
+ 	.fault = vfio_pci_mmap_fault,
+ };
+ 
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  static int vfio_pci_mmap(void *device_data, struct vm_area_struct *vma)
  {
  	struct vfio_pci_device *vdev = device_data;
@@@ -1222,12 -1873,34 +1500,20 @@@ static int vfio_pci_probe(struct pci_de
  	vdev->irq_type = VFIO_PCI_NUM_IRQS;
  	mutex_init(&vdev->igate);
  	spin_lock_init(&vdev->irqlock);
++<<<<<<< HEAD
++=======
+ 	mutex_init(&vdev->ioeventfds_lock);
+ 	INIT_LIST_HEAD(&vdev->ioeventfds_list);
+ 	mutex_init(&vdev->vma_lock);
+ 	INIT_LIST_HEAD(&vdev->vma_list);
+ 	init_rwsem(&vdev->memory_lock);
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  
  	ret = vfio_add_group_dev(&pdev->dev, &vfio_pci_ops, vdev);
 -	if (ret)
 -		goto out_free;
 -
 -	ret = vfio_pci_reflck_attach(vdev);
 -	if (ret)
 -		goto out_del_group_dev;
 -
 -	if (pdev->is_physfn) {
 -		vdev->vf_token = kzalloc(sizeof(*vdev->vf_token), GFP_KERNEL);
 -		if (!vdev->vf_token) {
 -			ret = -ENOMEM;
 -			goto out_reflck;
 -		}
 -
 -		mutex_init(&vdev->vf_token->lock);
 -		uuid_gen(&vdev->vf_token->uuid);
 -
 -		vdev->nb.notifier_call = vfio_pci_bus_notifier;
 -		ret = bus_register_notifier(&pci_bus_type, &vdev->nb);
 -		if (ret)
 -			goto out_vf_token;
 +	if (ret) {
 +		vfio_iommu_group_put(group, &pdev->dev);
 +		kfree(vdev);
 +		return ret;
  	}
  
  	if (vfio_pci_is_vga(pdev)) {
@@@ -1292,20 -2043,125 +1578,125 @@@ static const struct pci_error_handlers 
  };
  
  static struct pci_driver vfio_pci_driver = {
 -	.name			= "vfio-pci",
 -	.id_table		= NULL, /* only dynamic ids */
 -	.probe			= vfio_pci_probe,
 -	.remove			= vfio_pci_remove,
 -	.sriov_configure	= vfio_pci_sriov_configure,
 -	.err_handler		= &vfio_err_handlers,
 +	.name		= "vfio-pci",
 +	.id_table	= NULL, /* only dynamic ids */
 +	.probe		= vfio_pci_probe,
 +	.remove		= vfio_pci_remove,
 +	.err_handler	= &vfio_err_handlers,
  };
  
++<<<<<<< HEAD
 +struct vfio_devices {
 +	struct vfio_device **devices;
 +	int cur_index;
 +	int max_index;
 +};
 +
 +static int vfio_pci_get_devs(struct pci_dev *pdev, void *data)
++=======
+ static DEFINE_MUTEX(reflck_lock);
+ 
+ static struct vfio_pci_reflck *vfio_pci_reflck_alloc(void)
+ {
+ 	struct vfio_pci_reflck *reflck;
+ 
+ 	reflck = kzalloc(sizeof(*reflck), GFP_KERNEL);
+ 	if (!reflck)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	kref_init(&reflck->kref);
+ 	mutex_init(&reflck->lock);
+ 
+ 	return reflck;
+ }
+ 
+ static void vfio_pci_reflck_get(struct vfio_pci_reflck *reflck)
+ {
+ 	kref_get(&reflck->kref);
+ }
+ 
+ static int vfio_pci_reflck_find(struct pci_dev *pdev, void *data)
+ {
+ 	struct vfio_pci_reflck **preflck = data;
+ 	struct vfio_device *device;
+ 	struct vfio_pci_device *vdev;
+ 
+ 	device = vfio_device_get_from_dev(&pdev->dev);
+ 	if (!device)
+ 		return 0;
+ 
+ 	if (pci_dev_driver(pdev) != &vfio_pci_driver) {
+ 		vfio_device_put(device);
+ 		return 0;
+ 	}
+ 
+ 	vdev = vfio_device_data(device);
+ 
+ 	if (vdev->reflck) {
+ 		vfio_pci_reflck_get(vdev->reflck);
+ 		*preflck = vdev->reflck;
+ 		vfio_device_put(device);
+ 		return 1;
+ 	}
+ 
+ 	vfio_device_put(device);
+ 	return 0;
+ }
+ 
+ static int vfio_pci_reflck_attach(struct vfio_pci_device *vdev)
+ {
+ 	bool slot = !pci_probe_reset_slot(vdev->pdev->slot);
+ 
+ 	mutex_lock(&reflck_lock);
+ 
+ 	if (pci_is_root_bus(vdev->pdev->bus) ||
+ 	    vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_reflck_find,
+ 					  &vdev->reflck, slot) <= 0)
+ 		vdev->reflck = vfio_pci_reflck_alloc();
+ 
+ 	mutex_unlock(&reflck_lock);
+ 
+ 	return PTR_ERR_OR_ZERO(vdev->reflck);
+ }
+ 
+ static void vfio_pci_reflck_release(struct kref *kref)
+ {
+ 	struct vfio_pci_reflck *reflck = container_of(kref,
+ 						      struct vfio_pci_reflck,
+ 						      kref);
+ 
+ 	kfree(reflck);
+ 	mutex_unlock(&reflck_lock);
+ }
+ 
+ static void vfio_pci_reflck_put(struct vfio_pci_reflck *reflck)
+ {
+ 	kref_put_mutex(&reflck->kref, vfio_pci_reflck_release, &reflck_lock);
+ }
+ 
+ static int vfio_pci_get_unused_devs(struct pci_dev *pdev, void *data)
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
+ {
+ 	struct vfio_devices *devs = data;
+ 	struct vfio_device *device;
 -	struct vfio_pci_device *vdev;
+ 
+ 	if (devs->cur_index == devs->max_index)
+ 		return -ENOSPC;
+ 
+ 	device = vfio_device_get_from_dev(&pdev->dev);
+ 	if (!device)
+ 		return -EINVAL;
+ 
+ 	if (pci_dev_driver(pdev) != &vfio_pci_driver) {
+ 		vfio_device_put(device);
+ 		return -EBUSY;
+ 	}
+ 
 -	vdev = vfio_device_data(device);
 -
 -	/* Fault if the device is not unused */
 -	if (vdev->refcnt) {
 -		vfio_device_put(device);
 -		return -EBUSY;
 -	}
 -
+ 	devs->devices[devs->cur_index++] = device;
+ 	return 0;
+ }
+ 
+ static int vfio_pci_try_zap_and_vma_lock_cb(struct pci_dev *pdev, void *data)
  {
  	struct vfio_devices *devs = data;
  	struct vfio_device *device;
diff --cc drivers/vfio/pci/vfio_pci_intrs.c
index 76fef40f21bd,1d9fb2592945..000000000000
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@@ -461,7 -247,9 +461,8 @@@ static irqreturn_t vfio_msihandler(int 
  static int vfio_msi_enable(struct vfio_pci_device *vdev, int nvec, bool msix)
  {
  	struct pci_dev *pdev = vdev->pdev;
 -	unsigned int flag = msix ? PCI_IRQ_MSIX : PCI_IRQ_MSI;
  	int ret;
+ 	u16 cmd;
  
  	if (!is_irq_none(vdev))
  		return -EINVAL;
@@@ -470,36 -258,17 +471,49 @@@
  	if (!vdev->ctx)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	if (msix) {
 +		int i;
 +
 +		vdev->msix = kzalloc(nvec * sizeof(struct msix_entry),
 +				     GFP_KERNEL);
 +		if (!vdev->msix) {
 +			kfree(vdev->ctx);
 +			return -ENOMEM;
 +		}
 +
 +		for (i = 0; i < nvec; i++)
 +			vdev->msix[i].entry = i;
 +
 +		ret = pci_enable_msix_range(pdev, vdev->msix, 1, nvec);
 +		if (ret < nvec) {
 +			if (ret > 0)
 +				pci_disable_msix(pdev);
 +			kfree(vdev->msix);
 +			kfree(vdev->ctx);
 +			return ret;
 +		}
 +	} else {
 +		ret = pci_enable_msi_range(pdev, 1, nvec);
 +		if (ret < nvec) {
 +			if (ret > 0)
 +				pci_disable_msi(pdev);
 +			kfree(vdev->ctx);
 +			return ret;
 +		}
++=======
+ 	/* return the number of supported vectors if we can't get all: */
+ 	cmd = vfio_pci_memory_lock_and_enable(vdev);
+ 	ret = pci_alloc_irq_vectors(pdev, 1, nvec, flag);
+ 	if (ret < nvec) {
+ 		if (ret > 0)
+ 			pci_free_irq_vectors(pdev);
+ 		vfio_pci_memory_unlock_and_restore(vdev, cmd);
+ 		kfree(vdev->ctx);
+ 		return ret;
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  	}
+ 	vfio_pci_memory_unlock_and_restore(vdev, cmd);
  
  	vdev->num_ctx = nvec;
  	vdev->irq_type = msix ? VFIO_PCI_MSIX_IRQ_INDEX :
@@@ -611,19 -387,18 +632,26 @@@ static void vfio_msi_disable(struct vfi
  {
  	struct pci_dev *pdev = vdev->pdev;
  	int i;
+ 	u16 cmd;
  
  	for (i = 0; i < vdev->num_ctx; i++) {
 -		vfio_virqfd_disable(&vdev->ctx[i].unmask);
 -		vfio_virqfd_disable(&vdev->ctx[i].mask);
 +		virqfd_disable(vdev, &vdev->ctx[i].unmask);
 +		virqfd_disable(vdev, &vdev->ctx[i].mask);
  	}
  
  	vfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);
  
++<<<<<<< HEAD
 +	if (msix) {
 +		pci_disable_msix(vdev->pdev);
 +		kfree(vdev->msix);
 +	} else
 +		pci_disable_msi(pdev);
++=======
+ 	cmd = vfio_pci_memory_lock_and_enable(vdev);
+ 	pci_free_irq_vectors(pdev);
+ 	vfio_pci_memory_unlock_and_restore(vdev, cmd);
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  
  	/*
  	 * Both disable paths above use pci_intx_for_msi() to clear DisINTx
diff --cc drivers/vfio/pci/vfio_pci_private.h
index 24980778532d,86a02aff8735..000000000000
--- a/drivers/vfio/pci/vfio_pci_private.h
+++ b/drivers/vfio/pci/vfio_pci_private.h
@@@ -96,6 -132,14 +96,16 @@@ struct vfio_pci_device 
  	struct eventfd_ctx	*err_trigger;
  	struct eventfd_ctx	*req_trigger;
  	struct list_head	dummy_resources_list;
++<<<<<<< HEAD
++=======
+ 	struct mutex		ioeventfds_lock;
+ 	struct list_head	ioeventfds_list;
+ 	struct vfio_pci_vf_token	*vf_token;
+ 	struct notifier_block	nb;
+ 	struct mutex		vma_lock;
+ 	struct list_head	vma_list;
+ 	struct rw_semaphore	memory_lock;
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  };
  
  #define is_intx(vdev) (vdev->irq_type == VFIO_PCI_INTX_IRQ_INDEX)
@@@ -134,10 -178,35 +144,24 @@@ extern int vfio_pci_register_dev_region
  					unsigned int type, unsigned int subtype,
  					const struct vfio_pci_regops *ops,
  					size_t size, u32 flags, void *data);
++<<<<<<< HEAD
++=======
+ 
+ extern int vfio_pci_set_power_state(struct vfio_pci_device *vdev,
+ 				    pci_power_t state);
+ 
+ extern bool __vfio_pci_memory_enabled(struct vfio_pci_device *vdev);
+ extern void vfio_pci_zap_and_down_write_memory_lock(struct vfio_pci_device
+ 						    *vdev);
+ extern u16 vfio_pci_memory_lock_and_enable(struct vfio_pci_device *vdev);
+ extern void vfio_pci_memory_unlock_and_restore(struct vfio_pci_device *vdev,
+ 					       u16 cmd);
+ 
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
  #ifdef CONFIG_VFIO_PCI_IGD
 -extern int vfio_pci_igd_init(struct vfio_pci_device *vdev);
 -#else
 -static inline int vfio_pci_igd_init(struct vfio_pci_device *vdev)
 -{
 -	return -ENODEV;
 -}
 -#endif
 -#ifdef CONFIG_VFIO_PCI_NVLINK2
 -extern int vfio_pci_nvdia_v100_nvlink2_init(struct vfio_pci_device *vdev);
 -extern int vfio_pci_ibm_npu2_init(struct vfio_pci_device *vdev);
 +extern int vfio_pci_igd_opregion_init(struct vfio_pci_device *vdev);
  #else
 -static inline int vfio_pci_nvdia_v100_nvlink2_init(struct vfio_pci_device *vdev)
 -{
 -	return -ENODEV;
 -}
 -
 -static inline int vfio_pci_ibm_npu2_init(struct vfio_pci_device *vdev)
 +static inline int vfio_pci_igd_opregion_init(struct vfio_pci_device *vdev)
  {
  	return -ENODEV;
  }
diff --cc drivers/vfio/pci/vfio_pci_rdwr.c
index 210db24d2204,916b184df3a5..000000000000
--- a/drivers/vfio/pci/vfio_pci_rdwr.c
+++ b/drivers/vfio/pci/vfio_pci_rdwr.c
@@@ -122,13 -162,17 +122,14 @@@ ssize_t vfio_pci_bar_rw(struct vfio_pci
  	size_t x_start = 0, x_end = 0;
  	resource_size_t end;
  	void __iomem *io;
+ 	struct resource *res = &vdev->pdev->resource[bar];
  	ssize_t done;
  
 -	if (pci_resource_start(pdev, bar))
 -		end = pci_resource_len(pdev, bar);
 -	else if (bar == PCI_ROM_RESOURCE &&
 -		 pdev->resource[bar].flags & IORESOURCE_ROM_SHADOW)
 -		end = 0x20000;
 -	else
 +	if (!pci_resource_start(pdev, bar))
  		return -EINVAL;
  
 +	end = pci_resource_len(pdev, bar);
 +
  	if (pos >= end)
  		return -EINVAL;
  
@@@ -141,25 -193,20 +150,36 @@@
  		 * filling large ROM BARs much faster.
  		 */
  		io = pci_map_rom(pdev, &x_start);
- 		if (!io)
- 			return -ENOMEM;
+ 		if (!io) {
+ 			done = -ENOMEM;
+ 			goto out;
+ 		}
  		x_end = end;
++<<<<<<< HEAD
 +	} else if (!vdev->barmap[bar]) {
 +		int ret;
 +
 +		ret = pci_request_selected_regions(pdev, 1 << bar, "vfio");
 +		if (ret)
 +			return ret;
++=======
+ 	} else {
+ 		int ret = vfio_pci_setup_barmap(vdev, bar);
+ 		if (ret) {
+ 			done = ret;
+ 			goto out;
+ 		}
++>>>>>>> abafbc551fdd (vfio-pci: Invalidate mmaps and block MMIO access on disabled memory)
 +
 +		io = pci_iomap(pdev, bar, 0);
 +		if (!io) {
 +			pci_release_selected_regions(pdev, 1 << bar);
 +			return -ENOMEM;
 +		}
  
 +		vdev->barmap[bar] = io;
 +	} else
  		io = vdev->barmap[bar];
 -	}
  
  	if (bar == vdev->msix_bar) {
  		x_start = vdev->msix_offset;
* Unmerged path drivers/vfio/pci/vfio_pci.c
diff --git a/drivers/vfio/pci/vfio_pci_config.c b/drivers/vfio/pci/vfio_pci_config.c
index 4082bfbce848..defa44e8bf45 100644
--- a/drivers/vfio/pci/vfio_pci_config.c
+++ b/drivers/vfio/pci/vfio_pci_config.c
@@ -398,6 +398,14 @@ static inline void p_setd(struct perm_bits *p, int off, u32 virt, u32 write)
 	*(__le32 *)(&p->write[off]) = cpu_to_le32(write);
 }
 
+/* Caller should hold memory_lock semaphore */
+bool __vfio_pci_memory_enabled(struct vfio_pci_device *vdev)
+{
+	u16 cmd = le16_to_cpu(*(__le16 *)&vdev->vconfig[PCI_COMMAND]);
+
+	return cmd & PCI_COMMAND_MEMORY;
+}
+
 /*
  * Restore the *real* BARs after we detect a FLR or backdoor reset.
  * (backdoor = some device specific technique that we didn't catch)
@@ -553,13 +561,18 @@ static int vfio_basic_config_write(struct vfio_pci_device *vdev, int pos,
 
 		new_cmd = le32_to_cpu(val);
 
+		phys_io = !!(phys_cmd & PCI_COMMAND_IO);
+		virt_io = !!(le16_to_cpu(*virt_cmd) & PCI_COMMAND_IO);
+		new_io = !!(new_cmd & PCI_COMMAND_IO);
+
 		phys_mem = !!(phys_cmd & PCI_COMMAND_MEMORY);
 		virt_mem = !!(le16_to_cpu(*virt_cmd) & PCI_COMMAND_MEMORY);
 		new_mem = !!(new_cmd & PCI_COMMAND_MEMORY);
 
-		phys_io = !!(phys_cmd & PCI_COMMAND_IO);
-		virt_io = !!(le16_to_cpu(*virt_cmd) & PCI_COMMAND_IO);
-		new_io = !!(new_cmd & PCI_COMMAND_IO);
+		if (!new_mem)
+			vfio_pci_zap_and_down_write_memory_lock(vdev);
+		else
+			down_write(&vdev->memory_lock);
 
 		/*
 		 * If the user is writing mem/io enable (new_mem/io) and we
@@ -576,8 +589,11 @@ static int vfio_basic_config_write(struct vfio_pci_device *vdev, int pos,
 	}
 
 	count = vfio_default_config_write(vdev, pos, count, perm, offset, val);
-	if (count < 0)
+	if (count < 0) {
+		if (offset == PCI_COMMAND)
+			up_write(&vdev->memory_lock);
 		return count;
+	}
 
 	/*
 	 * Save current memory/io enable bits in vconfig to allow for
@@ -588,6 +604,8 @@ static int vfio_basic_config_write(struct vfio_pci_device *vdev, int pos,
 
 		*virt_cmd &= cpu_to_le16(~mask);
 		*virt_cmd |= cpu_to_le16(new_cmd & mask);
+
+		up_write(&vdev->memory_lock);
 	}
 
 	/* Emulate INTx disable */
@@ -825,8 +843,11 @@ static int vfio_exp_config_write(struct vfio_pci_device *vdev, int pos,
 						 pos - offset + PCI_EXP_DEVCAP,
 						 &cap);
 
-		if (!ret && (cap & PCI_EXP_DEVCAP_FLR))
+		if (!ret && (cap & PCI_EXP_DEVCAP_FLR)) {
+			vfio_pci_zap_and_down_write_memory_lock(vdev);
 			pci_try_reset_function(vdev->pdev);
+			up_write(&vdev->memory_lock);
+		}
 	}
 
 	/*
@@ -904,8 +925,11 @@ static int vfio_af_config_write(struct vfio_pci_device *vdev, int pos,
 						pos - offset + PCI_AF_CAP,
 						&cap);
 
-		if (!ret && (cap & PCI_AF_CAP_FLR) && (cap & PCI_AF_CAP_TP))
+		if (!ret && (cap & PCI_AF_CAP_FLR) && (cap & PCI_AF_CAP_TP)) {
+			vfio_pci_zap_and_down_write_memory_lock(vdev);
 			pci_try_reset_function(vdev->pdev);
+			up_write(&vdev->memory_lock);
+		}
 	}
 
 	return count;
* Unmerged path drivers/vfio/pci/vfio_pci_intrs.c
* Unmerged path drivers/vfio/pci/vfio_pci_private.h
* Unmerged path drivers/vfio/pci/vfio_pci_rdwr.c
