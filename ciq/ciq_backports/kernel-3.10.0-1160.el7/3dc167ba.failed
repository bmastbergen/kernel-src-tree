sched/cputime: Improve cputime_adjust()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Oleg Nesterov <oleg@redhat.com>
commit 3dc167ba5729ddd2d8e3fa1841653792c295d3f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/3dc167ba.failed

People report that utime and stime from /proc/<pid>/stat become very
wrong when the numbers are big enough, especially if you watch these
counters incrementally.

Specifically, the current implementation of: stime*rtime/total,
results in a saw-tooth function on top of the desired line, where the
teeth grow in size the larger the values become. IOW, it has a
relative error.

The result is that, when watching incrementally as time progresses
(for large values), we'll see periods of pure stime or utime increase,
irrespective of the actual ratio we're striving for.

Replace scale_stime() with a math64.h helper: mul_u64_u64_div_u64()
that is far more accurate. This also allows architectures to override
the implementation -- for instance they can opt for the old algorithm
if this new one turns out to be too expensive for them.

	Signed-off-by: Oleg Nesterov <oleg@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200519172506.GA317395@hirez.programming.kicks-ass.net
(cherry picked from commit 3dc167ba5729ddd2d8e3fa1841653792c295d3f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/div64.h
#	include/linux/math64.h
#	kernel/sched/cputime.c
diff --cc arch/x86/include/asm/div64.h
index af95c47d5c9e,b8f1dc0761e4..000000000000
--- a/arch/x86/include/asm/div64.h
+++ b/arch/x86/include/asm/div64.h
@@@ -72,6 -73,29 +72,32 @@@ static inline u64 mul_u32_u32(u32 a, u3
  
  #else
  # include <asm-generic/div64.h>
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Will generate an #DE when the result doesn't fit u64, could fix with an
+  * __ex_table[] entry when it becomes an issue.
+  */
+ static inline u64 mul_u64_u64_div_u64(u64 a, u64 mul, u64 div)
+ {
+ 	u64 q;
+ 
+ 	asm ("mulq %2; divq %3" : "=a" (q)
+ 				: "a" (a), "rm" (mul), "rm" (div)
+ 				: "rdx");
+ 
+ 	return q;
+ }
+ #define mul_u64_u64_div_u64 mul_u64_u64_div_u64
+ 
+ static inline u64 mul_u64_u32_div(u64 a, u32 mul, u32 div)
+ {
+ 	return mul_u64_u64_div_u64(a, mul, div);
+ }
+ #define mul_u64_u32_div	mul_u64_u32_div
+ 
++>>>>>>> 3dc167ba5729 (sched/cputime: Improve cputime_adjust())
  #endif /* CONFIG_X86_32 */
  
  #endif /* _ASM_X86_DIV64_H */
diff --cc include/linux/math64.h
index 80690c96c734,d097119419e6..000000000000
--- a/include/linux/math64.h
+++ b/include/linux/math64.h
@@@ -253,4 -263,22 +253,25 @@@ static inline u64 mul_u64_u32_div(u64 a
  }
  #endif /* mul_u64_u32_div */
  
++<<<<<<< HEAD
++=======
+ u64 mul_u64_u64_div_u64(u64 a, u64 mul, u64 div);
+ 
+ #define DIV64_U64_ROUND_UP(ll, d)	\
+ 	({ u64 _tmp = (d); div64_u64((ll) + _tmp - 1, _tmp); })
+ 
+ /**
+  * DIV64_U64_ROUND_CLOSEST - unsigned 64bit divide with 64bit divisor rounded to nearest integer
+  * @dividend: unsigned 64bit dividend
+  * @divisor: unsigned 64bit divisor
+  *
+  * Divide unsigned 64bit dividend by unsigned 64bit divisor
+  * and round to closest integer.
+  *
+  * Return: dividend / divisor rounded to nearest integer
+  */
+ #define DIV64_U64_ROUND_CLOSEST(dividend, divisor)	\
+ 	({ u64 _tmp = (divisor); div64_u64((dividend) + _tmp / 2, _tmp); })
+ 
++>>>>>>> 3dc167ba5729 (sched/cputime: Improve cputime_adjust())
  #endif /* _LINUX_MATH64_H */
diff --cc kernel/sched/cputime.c
index 1c241971a39e,5a55d2300452..000000000000
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@@ -508,54 -509,17 +508,57 @@@ void account_idle_ticks(unsigned long t
  		return;
  	}
  
 -	cputime = ticks * TICK_NSEC;
 -	steal = steal_account_process_time(ULONG_MAX);
 +	account_idle_time(jiffies_to_cputime(ticks));
 +}
  
 -	if (steal >= cputime)
 -		return;
 +/*
++<<<<<<< HEAD
 + * Perform (stime * rtime) / total, but avoid multiplication overflow by
 + * loosing precision when the numbers are big.
 + */
 +static cputime_t scale_stime(u64 stime, u64 rtime, u64 total)
 +{
 +	u64 scaled;
 +
 +	for (;;) {
 +		/* Make sure "rtime" is the bigger of stime/rtime */
 +		if (stime > rtime)
 +			swap(rtime, stime);
 +
 +		/* Make sure 'total' fits in 32 bits */
 +		if (total >> 32)
 +			goto drop_precision;
 +
 +		/* Does rtime (and thus stime) fit in 32 bits? */
 +		if (!(rtime >> 32))
 +			break;
  
 -	cputime -= steal;
 -	account_idle_time(cputime);
 +		/* Can we just balance rtime/stime rather than dropping bits? */
 +		if (stime >> 31)
 +			goto drop_precision;
 +
 +		/* We can grow stime and shrink rtime and try to make them both fit */
 +		stime <<= 1;
 +		rtime >>= 1;
 +		continue;
 +
 +drop_precision:
 +		/* We drop from rtime, it has more bits than stime */
 +		rtime >>= 1;
 +		total >>= 1;
 +	}
 +
 +	/*
 +	 * Make sure gcc understands that this is a 32x32->64 multiply,
 +	 * followed by a 64/32->64 divide.
 +	 */
 +	scaled = div_u64((u64) (u32) stime * (u64) (u32) rtime, (u32)total);
 +	return (__force cputime_t) scaled;
  }
  
  /*
++=======
++>>>>>>> 3dc167ba5729 (sched/cputime: Improve cputime_adjust())
   * Adjust tick based cputime random precision against scheduler runtime
   * accounting.
   *
@@@ -614,8 -578,7 +617,12 @@@ static void cputime_adjust(struct task_
  		goto update;
  	}
  
++<<<<<<< HEAD
 +	stime = scale_stime((__force u64)stime, (__force u64)rtime,
 +			    (__force u64)(stime + utime));
++=======
+ 	stime = mul_u64_u64_div_u64(stime, rtime, stime + utime);
++>>>>>>> 3dc167ba5729 (sched/cputime: Improve cputime_adjust())
  
  update:
  	/*
* Unmerged path arch/x86/include/asm/div64.h
* Unmerged path include/linux/math64.h
* Unmerged path kernel/sched/cputime.c
diff --git a/lib/div64.c b/lib/div64.c
index 4382ad77777e..040b8b8d2171 100644
--- a/lib/div64.c
+++ b/lib/div64.c
@@ -181,3 +181,44 @@ u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
 	return __iter_div_u64_rem(dividend, divisor, remainder);
 }
 EXPORT_SYMBOL(iter_div_u64_rem);
+
+#ifndef mul_u64_u64_div_u64
+u64 mul_u64_u64_div_u64(u64 a, u64 b, u64 c)
+{
+	u64 res = 0, div, rem;
+	int shift;
+
+	/* can a * b overflow ? */
+	if (ilog2(a) + ilog2(b) > 62) {
+		/*
+		 * (b * a) / c is equal to
+		 *
+		 *      (b / c) * a +
+		 *      (b % c) * a / c
+		 *
+		 * if nothing overflows. Can the 1st multiplication
+		 * overflow? Yes, but we do not care: this can only
+		 * happen if the end result can't fit in u64 anyway.
+		 *
+		 * So the code below does
+		 *
+		 *      res = (b / c) * a;
+		 *      b = b % c;
+		 */
+		div = div64_u64_rem(b, c, &rem);
+		res = div * a;
+		b = rem;
+
+		shift = ilog2(a) + ilog2(b) - 62;
+		if (shift > 0) {
+			/* drop precision */
+			b >>= shift;
+			c >>= shift;
+			if (!c)
+				return res;
+		}
+	}
+
+	return res + div64_u64(a * b, c);
+}
+#endif
