Revert "mm: always flush VMA ranges affected by zap_page_range"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.el7
commit-author Rik van Riel <riel@surriel.com>
commit 50c150f26261e723523f077a67378736fa7511a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.el7/50c150f2.failed

There was a bug in Linux that could cause madvise (and mprotect?) system
calls to return to userspace without the TLB having been flushed for all
the pages involved.

This could happen when multiple threads of a process made simultaneous
madvise and/or mprotect calls.

This was noticed in the summer of 2017, at which time two solutions
were created:

  56236a59556c ("mm: refactor TLB gathering API")
  99baac21e458 ("mm: fix MADV_[FREE|DONTNEED] TLB flush miss problem")
and
  4647706ebeee ("mm: always flush VMA ranges affected by zap_page_range")

We need only one of these solutions, and the former appears to be a
little more efficient than the latter, so revert that one.

This reverts 4647706ebeee6e50 ("mm: always flush VMA ranges affected by
zap_page_range")

Link: http://lkml.kernel.org/r/20180706131019.51e3a5f0@imladris.surriel.com
	Signed-off-by: Rik van Riel <riel@surriel.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Nicholas Piggin <npiggin@gmail.com>
	Cc: Nadav Amit <nadav.amit@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 50c150f26261e723523f077a67378736fa7511a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index d5f82c19f23a,19f47d7b9b86..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1515,18 -1613,8 +1515,23 @@@ void zap_page_range(struct vm_area_stru
  	tlb_gather_mmu(&tlb, mm, start, end);
  	update_hiwater_rss(mm);
  	mmu_notifier_invalidate_range_start(mm, start, end);
++<<<<<<< HEAD
 +	for ( ; vma && vma->vm_start < end; vma = vma->vm_next) {
 +		unmap_single_vma(&tlb, vma, start, end, details);
 +		/*
 +		 * zap_page_range does not specify whether mmap_sem should be
 +		 * held for read or write. That allows parallel zap_page_range
 +		 * operations to unmap a PTE and defer a flush meaning that
 +		 * this call observes pte_none and fails to flush the TLB.
 +		 * Rather than adding a complex API, ensure that no stale
 +		 * TLB entries exist when this call returns.
 +		 */
 +		flush_tlb_range(vma, start, end);
 +	}
++=======
+ 	for ( ; vma && vma->vm_start < end; vma = vma->vm_next)
+ 		unmap_single_vma(&tlb, vma, start, end, NULL);
++>>>>>>> 50c150f26261 (Revert "mm: always flush VMA ranges affected by zap_page_range")
  	mmu_notifier_invalidate_range_end(mm, start, end);
  	tlb_finish_mmu(&tlb, start, end);
  }
* Unmerged path mm/memory.c
