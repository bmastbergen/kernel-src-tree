percpu: increase minimum percpu allocation size and align first regions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit d2f3c3849461baefdbb39123abde1054d46bf22e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/d2f3c384.failed

This patch increases the minimum allocation size of percpu memory to
4-bytes. This change will help minimize the metadata overhead
associated with the bitmap allocator. The assumption is that most
allocations will be of objects or structs greater than 2 bytes with
integers or longs being used rather than shorts.

The first chunk regions are now aligned with the minimum allocation
size. The reserved region is expected to be set as a multiple of the
minimum allocation size. The static region is aligned up and the delta
is removed from the dynamic size. This works because the dynamic size is
increased to be page aligned. If the static size is not minimum
allocation size aligned, then there must be a gap that is added to the
dynamic size. The dynamic size will never be smaller than the set value.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit d2f3c3849461baefdbb39123abde1054d46bf22e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,dc755721c333..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -888,14 -956,15 +888,14 @@@ static void __percpu *pcpu_alloc(size_
  	 * We want the lowest bit of offset available for in-use/free
  	 * indicator, so force >= 16bit alignment and make size even.
  	 */
- 	if (unlikely(align < 2))
- 		align = 2;
+ 	if (unlikely(align < PCPU_MIN_ALLOC_SIZE))
+ 		align = PCPU_MIN_ALLOC_SIZE;
  
- 	size = ALIGN(size, 2);
+ 	size = ALIGN(size, PCPU_MIN_ALLOC_SIZE);
  
 -	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||
 -		     !is_power_of_2(align))) {
 -		WARN(true, "illegal size (%zu) or align (%zu) for percpu allocation\n",
 -		     size, align);
 +	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {
 +		WARN(do_warn, "illegal size (%zu) or align (%zu) for "
 +		     "percpu allocation\n", size, align);
  		return NULL;
  	}
  
@@@ -1557,9 -1652,9 +1557,15 @@@ int __init pcpu_setup_first_chunk(cons
  {
  	static int smap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
  	static int dmap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
++<<<<<<< HEAD
 +	size_t dyn_size = ai->dyn_size;
 +	size_t size_sum = ai->static_size + ai->reserved_size + dyn_size;
 +	struct pcpu_chunk *schunk, *dchunk = NULL;
++=======
+ 	size_t size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
+ 	size_t static_size, dyn_size;
+ 	struct pcpu_chunk *chunk;
++>>>>>>> d2f3c3849461 (percpu: increase minimum percpu allocation size and align first regions)
  	unsigned long *group_offsets;
  	size_t *group_sizes;
  	unsigned long *unit_off;
@@@ -1589,6 -1686,8 +1595,11 @@@
  	PCPU_SETUP_BUG_ON(offset_in_page(ai->unit_size));
  	PCPU_SETUP_BUG_ON(ai->unit_size < PCPU_MIN_UNIT_SIZE);
  	PCPU_SETUP_BUG_ON(ai->dyn_size < PERCPU_DYNAMIC_EARLY_SIZE);
++<<<<<<< HEAD
++=======
+ 	PCPU_SETUP_BUG_ON(!ai->dyn_size);
+ 	PCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->reserved_size, PCPU_MIN_ALLOC_SIZE));
++>>>>>>> d2f3c3849461 (percpu: increase minimum percpu allocation size and align first regions)
  	PCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);
  
  	/* process group information and build config tables accordingly */
@@@ -1665,56 -1766,38 +1676,89 @@@
  		INIT_LIST_HEAD(&pcpu_slot[i]);
  
  	/*
++<<<<<<< HEAD
 +	 * Initialize static chunk.  If reserved_size is zero, the
 +	 * static chunk covers static area + dynamic allocation area
 +	 * in the first chunk.  If reserved_size is not zero, it
 +	 * covers static area + reserved area (mostly used for module
 +	 * static percpu allocation).
 +	 */
 +	schunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +	INIT_LIST_HEAD(&schunk->list);
 +	INIT_LIST_HEAD(&schunk->map_extend_list);
 +	schunk->base_addr = base_addr;
 +	schunk->map = smap;
 +	schunk->map_alloc = ARRAY_SIZE(smap);
 +	schunk->immutable = true;
 +	bitmap_fill(schunk->populated, pcpu_unit_pages);
 +	schunk->nr_populated = pcpu_unit_pages;
 +
 +	if (ai->reserved_size) {
 +		schunk->free_size = ai->reserved_size;
 +		pcpu_reserved_chunk = schunk;
 +		pcpu_reserved_chunk_limit = ai->static_size + ai->reserved_size;
 +	} else {
 +		schunk->free_size = dyn_size;
 +		dyn_size = 0;			/* dynamic area covered */
 +	}
 +	schunk->contig_hint = schunk->free_size;
 +
 +	schunk->map[0] = 1;
 +	schunk->map[1] = ai->static_size;
 +	schunk->map_used = 1;
 +	if (schunk->free_size)
 +		schunk->map[++schunk->map_used] = ai->static_size + schunk->free_size;
 +	schunk->map[schunk->map_used] |= 1;
++=======
+ 	 * The end of the static region needs to be aligned with the
+ 	 * minimum allocation size as this offsets the reserved and
+ 	 * dynamic region.  The first chunk ends page aligned by
+ 	 * expanding the dynamic region, therefore the dynamic region
+ 	 * can be shrunk to compensate while still staying above the
+ 	 * configured sizes.
+ 	 */
+ 	static_size = ALIGN(ai->static_size, PCPU_MIN_ALLOC_SIZE);
+ 	dyn_size = ai->dyn_size - (static_size - ai->static_size);
+ 
+ 	/*
+ 	 * Initialize first chunk.
+ 	 * If the reserved_size is non-zero, this initializes the reserved
+ 	 * chunk.  If the reserved_size is zero, the reserved chunk is NULL
+ 	 * and the dynamic region is initialized here.  The first chunk,
+ 	 * pcpu_first_chunk, will always point to the chunk that serves
+ 	 * the dynamic region.
+ 	 */
+ 	tmp_addr = (unsigned long)base_addr + static_size;
+ 	map_size = ai->reserved_size ?: dyn_size;
+ 	chunk = pcpu_alloc_first_chunk(tmp_addr, map_size, smap,
+ 				       ARRAY_SIZE(smap));
++>>>>>>> d2f3c3849461 (percpu: increase minimum percpu allocation size and align first regions)
  
  	/* init dynamic chunk if necessary */
 -	if (ai->reserved_size) {
 -		pcpu_reserved_chunk = chunk;
 -
 +	if (dyn_size) {
 +		dchunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +		INIT_LIST_HEAD(&dchunk->list);
 +		INIT_LIST_HEAD(&dchunk->map_extend_list);
 +		dchunk->base_addr = base_addr;
 +		dchunk->map = dmap;
 +		dchunk->map_alloc = ARRAY_SIZE(dmap);
 +		dchunk->immutable = true;
 +		bitmap_fill(dchunk->populated, pcpu_unit_pages);
 +		dchunk->nr_populated = pcpu_unit_pages;
 +
++<<<<<<< HEAD
 +		dchunk->contig_hint = dchunk->free_size = dyn_size;
 +		dchunk->map[0] = 1;
 +		dchunk->map[1] = pcpu_reserved_chunk_limit;
 +		dchunk->map[2] = (pcpu_reserved_chunk_limit + dchunk->free_size) | 1;
 +		dchunk->map_used = 2;
++=======
+ 		tmp_addr = (unsigned long)base_addr + static_size +
+ 			   ai->reserved_size;
+ 		map_size = dyn_size;
+ 		chunk = pcpu_alloc_first_chunk(tmp_addr, map_size, dmap,
+ 					       ARRAY_SIZE(dmap));
++>>>>>>> d2f3c3849461 (percpu: increase minimum percpu allocation size and align first regions)
  	}
  
  	/* link the first chunk in */
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index 004298923ccf..13286e3b7e99 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -52,6 +52,10 @@
 /* minimum unit size, also is the maximum supported allocation size */
 #define PCPU_MIN_UNIT_SIZE		PFN_ALIGN(32 << 10)
 
+/* minimum allocation size and shift in bytes */
+#define PCPU_MIN_ALLOC_SHIFT		2
+#define PCPU_MIN_ALLOC_SIZE		(1 << PCPU_MIN_ALLOC_SHIFT)
+
 /*
  * Percpu allocator can serve percpu allocations before slab is
  * initialized which allows slab to depend on the percpu allocator.
* Unmerged path mm/percpu.c
