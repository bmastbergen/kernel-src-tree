KVM: vmx: implement MSR_IA32_TSX_CTRL disable RTM functionality

jira LE-1907
cve CVE-2019-19338
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit c11f83e0626bdc2b6c550fc8b9b6eeefbd8cefaa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/c11f83e0.failed

The current guest mitigation of TAA is both too heavy and not really
sufficient.  It is too heavy because it will cause some affected CPUs
(those that have MDS_NO but lack TAA_NO) to fall back to VERW and
get the corresponding slowdown.  It is not really sufficient because
it will cause the MDS_NO bit to disappear upon microcode update, so
that VMs started before the microcode update will not be runnable
anymore afterwards, even with tsx=on.

Instead, if tsx=on on the host, we can emulate MSR_IA32_TSX_CTRL for
the guest and let it run without the VERW mitigation.  Even though
MSR_IA32_TSX_CTRL is quite heavyweight, and we do not want to write
it on every vmentry, we can use the shared MSR functionality because
the host kernel need not protect itself from TSX-based side-channels.

	Tested-by: Jim Mattson <jmattson@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c11f83e0626bdc2b6c550fc8b9b6eeefbd8cefaa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index 5be8b7fd8c76,fc54e3905fe3..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1035,11 -1214,169 +1035,144 @@@ static u32 emulated_msrs[] = 
  	MSR_IA32_MCG_CTL,
  	MSR_IA32_MCG_EXT_CTL,
  	MSR_IA32_SMBASE,
 -	MSR_SMI_COUNT,
 -	MSR_PLATFORM_INFO,
 -	MSR_MISC_FEATURES_ENABLES,
  	MSR_AMD64_VIRT_SPEC_CTRL,
 -	MSR_IA32_POWER_CTL,
 -
 -	/*
 -	 * The following list leaves out MSRs whose values are determined
 -	 * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.
 -	 * We always support the "true" VMX control MSRs, even if the host
 -	 * processor does not, so I am putting these registers here rather
 -	 * than in msrs_to_save_all.
 -	 */
 -	MSR_IA32_VMX_BASIC,
 -	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
 -	MSR_IA32_VMX_TRUE_PROCBASED_CTLS,
 -	MSR_IA32_VMX_TRUE_EXIT_CTLS,
 -	MSR_IA32_VMX_TRUE_ENTRY_CTLS,
 -	MSR_IA32_VMX_MISC,
 -	MSR_IA32_VMX_CR0_FIXED0,
 -	MSR_IA32_VMX_CR4_FIXED0,
 -	MSR_IA32_VMX_VMCS_ENUM,
 -	MSR_IA32_VMX_PROCBASED_CTLS2,
 -	MSR_IA32_VMX_EPT_VPID_CAP,
 -	MSR_IA32_VMX_VMFUNC,
 -
 -	MSR_K7_HWCR,
 -	MSR_KVM_POLL_CONTROL,
  };
  
 -static u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];
  static unsigned num_emulated_msrs;
  
++<<<<<<< HEAD
++=======
+ /*
+  * List of msr numbers which are used to expose MSR-based features that
+  * can be used by a hypervisor to validate requested CPU features.
+  */
+ static const u32 msr_based_features_all[] = {
+ 	MSR_IA32_VMX_BASIC,
+ 	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
+ 	MSR_IA32_VMX_PINBASED_CTLS,
+ 	MSR_IA32_VMX_TRUE_PROCBASED_CTLS,
+ 	MSR_IA32_VMX_PROCBASED_CTLS,
+ 	MSR_IA32_VMX_TRUE_EXIT_CTLS,
+ 	MSR_IA32_VMX_EXIT_CTLS,
+ 	MSR_IA32_VMX_TRUE_ENTRY_CTLS,
+ 	MSR_IA32_VMX_ENTRY_CTLS,
+ 	MSR_IA32_VMX_MISC,
+ 	MSR_IA32_VMX_CR0_FIXED0,
+ 	MSR_IA32_VMX_CR0_FIXED1,
+ 	MSR_IA32_VMX_CR4_FIXED0,
+ 	MSR_IA32_VMX_CR4_FIXED1,
+ 	MSR_IA32_VMX_VMCS_ENUM,
+ 	MSR_IA32_VMX_PROCBASED_CTLS2,
+ 	MSR_IA32_VMX_EPT_VPID_CAP,
+ 	MSR_IA32_VMX_VMFUNC,
+ 
+ 	MSR_F10H_DECFG,
+ 	MSR_IA32_UCODE_REV,
+ 	MSR_IA32_ARCH_CAPABILITIES,
+ };
+ 
+ static u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];
+ static unsigned int num_msr_based_features;
+ 
+ static u64 kvm_get_arch_capabilities(void)
+ {
+ 	u64 data = 0;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))
+ 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, data);
+ 
+ 	/*
+ 	 * If nx_huge_pages is enabled, KVM's shadow paging will ensure that
+ 	 * the nested hypervisor runs with NX huge pages.  If it is not,
+ 	 * L1 is anyway vulnerable to ITLB_MULTIHIT explots from other
+ 	 * L1 guests, so it need not worry about its own (L2) guests.
+ 	 */
+ 	data |= ARCH_CAP_PSCHANGE_MC_NO;
+ 
+ 	/*
+ 	 * If we're doing cache flushes (either "always" or "cond")
+ 	 * we will do one whenever the guest does a vmlaunch/vmresume.
+ 	 * If an outer hypervisor is doing the cache flush for us
+ 	 * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that
+ 	 * capability to the guest too, and if EPT is disabled we're not
+ 	 * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will
+ 	 * require a nested hypervisor to do a flush of its own.
+ 	 */
+ 	if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+ 		data |= ARCH_CAP_SKIP_VMENTRY_L1DFLUSH;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))
+ 		data |= ARCH_CAP_RDCL_NO;
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		data |= ARCH_CAP_SSB_NO;
+ 	if (!boot_cpu_has_bug(X86_BUG_MDS))
+ 		data |= ARCH_CAP_MDS_NO;
+ 
+ 	/*
+ 	 * On TAA affected systems:
+ 	 *      - nothing to do if TSX is disabled on the host.
+ 	 *      - we emulate TSX_CTRL if present on the host.
+ 	 *	  This lets the guest use VERW to clear CPU buffers.
+ 	 */
+ 	if (!boot_cpu_has(X86_FEATURE_RTM))
+ 		data &= ~(ARCH_CAP_TAA_NO | ARCH_CAP_TSX_CTRL_MSR);
+ 	else if (!boot_cpu_has_bug(X86_BUG_TAA))
+ 		data |= ARCH_CAP_TAA_NO;
+ 
+ 	return data;
+ }
+ EXPORT_SYMBOL_GPL(kvm_get_arch_capabilities);
+ 
+ static int kvm_get_msr_feature(struct kvm_msr_entry *msr)
+ {
+ 	switch (msr->index) {
+ 	case MSR_IA32_ARCH_CAPABILITIES:
+ 		msr->data = kvm_get_arch_capabilities();
+ 		break;
+ 	case MSR_IA32_UCODE_REV:
+ 		rdmsrl_safe(msr->index, &msr->data);
+ 		break;
+ 	default:
+ 		if (kvm_x86_ops->get_msr_feature(msr))
+ 			return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
+ {
+ 	struct kvm_msr_entry msr;
+ 	int r;
+ 
+ 	msr.index = index;
+ 	r = kvm_get_msr_feature(&msr);
+ 	if (r)
+ 		return r;
+ 
+ 	*data = msr.data;
+ 
+ 	return 0;
+ }
+ 
+ static bool __kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)
+ {
+ 	if (efer & EFER_FFXSR && !guest_cpuid_has(vcpu, X86_FEATURE_FXSR_OPT))
+ 		return false;
+ 
+ 	if (efer & EFER_SVME && !guest_cpuid_has(vcpu, X86_FEATURE_SVM))
+ 		return false;
+ 
+ 	if (efer & (EFER_LME | EFER_LMA) &&
+ 	    !guest_cpuid_has(vcpu, X86_FEATURE_LM))
+ 		return false;
+ 
+ 	if (efer & EFER_NX && !guest_cpuid_has(vcpu, X86_FEATURE_NX))
+ 		return false;
+ 
+ 	return true;
+ 
+ }
++>>>>>>> c11f83e0626b (KVM: vmx: implement MSR_IA32_TSX_CTRL disable RTM functionality)
  bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)
  {
  	if (efer & efer_reserved_bits)
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
