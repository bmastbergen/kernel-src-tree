mm, memory_failure: Teach memory_failure() about dev_pagemap pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 6100e34b2526e1dc3dbcc47fea2677974d6aaea5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/6100e34b.failed

mce: Uncorrected hardware memory error in user-access at af34214200
    {1}[Hardware Error]: It has been corrected by h/w and requires no further action
    mce: [Hardware Error]: Machine check events logged
    {1}[Hardware Error]: event severity: corrected
    Memory failure: 0xaf34214: reserved kernel page still referenced by 1 users
    [..]
    Memory failure: 0xaf34214: recovery action for reserved kernel page: Failed
    mce: Memory error not recovered

In contrast to typical memory, dev_pagemap pages may be dax mapped. With
dax there is no possibility to map in another page dynamically since dax
establishes 1:1 physical address to file offset associations. Also
dev_pagemap pages associated with NVDIMM / persistent memory devices can
internal remap/repair addresses with poison. While memory_failure()
assumes that it can discard typical poisoned pages and keep them
unmapped indefinitely, dev_pagemap pages may be returned to service
after the error is cleared.

Teach memory_failure() to detect and handle MEMORY_DEVICE_HOST
dev_pagemap pages that have poison consumed by userspace. Mark the
memory as UC instead of unmapping it completely to allow ongoing access
via the device driver (nd_pmem). Later, nd_pmem will grow support for
marking the page back to WB when the error is cleared.

	Cc: Jan Kara <jack@suse.cz>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Jérôme Glisse <jglisse@redhat.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
(cherry picked from commit 6100e34b2526e1dc3dbcc47fea2677974d6aaea5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	mm/memory-failure.c
diff --cc include/linux/mm.h
index 7f7944c84855,374e5e9284f7..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -2434,12 -2689,49 +2434,51 @@@ extern int get_hwpoison_page(struct pag
  extern int sysctl_memory_failure_early_kill;
  extern int sysctl_memory_failure_recovery;
  extern void shake_page(struct page *p, int access);
 -extern atomic_long_t num_poisoned_pages __read_mostly;
 +extern atomic_long_t num_poisoned_pages;
  extern int soft_offline_page(struct page *page, int flags);
  
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Error handlers for various types of pages.
+  */
+ enum mf_result {
+ 	MF_IGNORED,	/* Error: cannot be handled */
+ 	MF_FAILED,	/* Error: handling failed */
+ 	MF_DELAYED,	/* Will be handled later */
+ 	MF_RECOVERED,	/* Successfully recovered */
+ };
+ 
+ enum mf_action_page_type {
+ 	MF_MSG_KERNEL,
+ 	MF_MSG_KERNEL_HIGH_ORDER,
+ 	MF_MSG_SLAB,
+ 	MF_MSG_DIFFERENT_COMPOUND,
+ 	MF_MSG_POISONED_HUGE,
+ 	MF_MSG_HUGE,
+ 	MF_MSG_FREE_HUGE,
+ 	MF_MSG_NON_PMD_HUGE,
+ 	MF_MSG_UNMAP_FAILED,
+ 	MF_MSG_DIRTY_SWAPCACHE,
+ 	MF_MSG_CLEAN_SWAPCACHE,
+ 	MF_MSG_DIRTY_MLOCKED_LRU,
+ 	MF_MSG_CLEAN_MLOCKED_LRU,
+ 	MF_MSG_DIRTY_UNEVICTABLE_LRU,
+ 	MF_MSG_CLEAN_UNEVICTABLE_LRU,
+ 	MF_MSG_DIRTY_LRU,
+ 	MF_MSG_CLEAN_LRU,
+ 	MF_MSG_TRUNCATED_LRU,
+ 	MF_MSG_BUDDY,
+ 	MF_MSG_BUDDY_2ND,
+ 	MF_MSG_DAX,
+ 	MF_MSG_UNKNOWN,
+ };
+ 
++>>>>>>> 6100e34b2526 (mm, memory_failure: Teach memory_failure() about dev_pagemap pages)
  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)
  extern void clear_huge_page(struct page *page,
 -			    unsigned long addr_hint,
 +			    unsigned long addr,
  			    unsigned int pages_per_huge_page);
  extern void copy_user_huge_page(struct page *dst, struct page *src,
  				unsigned long addr, struct vm_area_struct *vma,
diff --cc mm/memory-failure.c
index 2d6d813952ef,32a644d9c2ee..000000000000
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@@ -54,8 -55,11 +54,9 @@@
  #include <linux/hugetlb.h>
  #include <linux/memory_hotplug.h>
  #include <linux/mm_inline.h>
+ #include <linux/memremap.h>
  #include <linux/kfifo.h>
 -#include <linux/ratelimit.h>
  #include "internal.h"
 -#include "ras/ras_event.h"
  
  int sysctl_memory_failure_early_kill __read_mostly = 0;
  
@@@ -294,6 -205,100 +295,103 @@@ struct to_kill 
  };
  
  /*
++<<<<<<< HEAD
++=======
+  * Send all the processes who have the page mapped a signal.
+  * ``action optional'' if they are not immediately affected by the error
+  * ``action required'' if error happened in current execution context
+  */
+ static int kill_proc(struct to_kill *tk, unsigned long pfn, int flags)
+ {
+ 	struct task_struct *t = tk->tsk;
+ 	short addr_lsb = tk->size_shift;
+ 	int ret;
+ 
+ 	pr_err("Memory failure: %#lx: Killing %s:%d due to hardware memory corruption\n",
+ 		pfn, t->comm, t->pid);
+ 
+ 	if ((flags & MF_ACTION_REQUIRED) && t->mm == current->mm) {
+ 		ret = force_sig_mceerr(BUS_MCEERR_AR, (void __user *)tk->addr,
+ 				       addr_lsb, current);
+ 	} else {
+ 		/*
+ 		 * Don't use force here, it's convenient if the signal
+ 		 * can be temporarily blocked.
+ 		 * This could cause a loop when the user sets SIGBUS
+ 		 * to SIG_IGN, but hopefully no one will do that?
+ 		 */
+ 		ret = send_sig_mceerr(BUS_MCEERR_AO, (void __user *)tk->addr,
+ 				      addr_lsb, t);  /* synchronous? */
+ 	}
+ 	if (ret < 0)
+ 		pr_info("Memory failure: Error sending signal to %s:%d: %d\n",
+ 			t->comm, t->pid, ret);
+ 	return ret;
+ }
+ 
+ /*
+  * When a unknown page type is encountered drain as many buffers as possible
+  * in the hope to turn the page into a LRU or free page, which we can handle.
+  */
+ void shake_page(struct page *p, int access)
+ {
+ 	if (PageHuge(p))
+ 		return;
+ 
+ 	if (!PageSlab(p)) {
+ 		lru_add_drain_all();
+ 		if (PageLRU(p))
+ 			return;
+ 		drain_all_pages(page_zone(p));
+ 		if (PageLRU(p) || is_free_buddy_page(p))
+ 			return;
+ 	}
+ 
+ 	/*
+ 	 * Only call shrink_node_slabs here (which would also shrink
+ 	 * other caches) if access is not potentially fatal.
+ 	 */
+ 	if (access)
+ 		drop_slab_node(page_to_nid(p));
+ }
+ EXPORT_SYMBOL_GPL(shake_page);
+ 
+ static unsigned long dev_pagemap_mapping_shift(struct page *page,
+ 		struct vm_area_struct *vma)
+ {
+ 	unsigned long address = vma_address(page, vma);
+ 	pgd_t *pgd;
+ 	p4d_t *p4d;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 
+ 	pgd = pgd_offset(vma->vm_mm, address);
+ 	if (!pgd_present(*pgd))
+ 		return 0;
+ 	p4d = p4d_offset(pgd, address);
+ 	if (!p4d_present(*p4d))
+ 		return 0;
+ 	pud = pud_offset(p4d, address);
+ 	if (!pud_present(*pud))
+ 		return 0;
+ 	if (pud_devmap(*pud))
+ 		return PUD_SHIFT;
+ 	pmd = pmd_offset(pud, address);
+ 	if (!pmd_present(*pmd))
+ 		return 0;
+ 	if (pmd_devmap(*pmd))
+ 		return PMD_SHIFT;
+ 	pte = pte_offset_map(pmd, address);
+ 	if (!pte_present(*pte))
+ 		return 0;
+ 	if (pte_devmap(*pte))
+ 		return PAGE_SHIFT;
+ 	return 0;
+ }
+ 
+ /*
++>>>>>>> 6100e34b2526 (mm, memory_failure: Teach memory_failure() about dev_pagemap pages)
   * Failure handling: if we can't find or can't kill a process there's
   * not much we can do.	We just print a message and ignore otherwise.
   */
@@@ -323,6 -327,10 +421,13 @@@ static void add_to_kill(struct task_str
  	}
  	tk->addr = page_address_in_vma(p, vma);
  	tk->addr_valid = 1;
++<<<<<<< HEAD
++=======
+ 	if (is_zone_device_page(p))
+ 		tk->size_shift = dev_pagemap_mapping_shift(p, vma);
+ 	else
+ 		tk->size_shift = compound_order(compound_head(p)) + PAGE_SHIFT;
++>>>>>>> 6100e34b2526 (mm, memory_failure: Teach memory_failure() about dev_pagemap pages)
  
  	/*
  	 * In theory we don't have to kill when the page was
@@@ -330,8 -338,8 +435,13 @@@
  	 * likely very rare kill anyways just out of paranoia, but use
  	 * a SIGKILL because the error is not contained anymore.
  	 */
++<<<<<<< HEAD
 +	if (tk->addr == -EFAULT) {
 +		pr_info("MCE: Unable to find user space address %lx in %s\n",
++=======
+ 	if (tk->addr == -EFAULT || tk->size_shift == 0) {
+ 		pr_info("Memory failure: Unable to find user space address %lx in %s\n",
++>>>>>>> 6100e34b2526 (mm, memory_failure: Teach memory_failure() about dev_pagemap pages)
  			page_to_pfn(p), tsk->comm);
  		tk->addr_valid = 0;
  	}
@@@ -520,22 -524,36 +630,48 @@@ static void collect_procs(struct page *
  	kfree(tk);
  }
  
 -static const char *action_name[] = {
 -	[MF_IGNORED] = "Ignored",
 -	[MF_FAILED] = "Failed",
 -	[MF_DELAYED] = "Delayed",
 -	[MF_RECOVERED] = "Recovered",
 +/*
 + * Error handlers for various types of pages.
 + */
 +
 +enum outcome {
 +	IGNORED,	/* Error: cannot be handled */
 +	FAILED,		/* Error: handling failed */
 +	DELAYED,	/* Will be handled later */
 +	RECOVERED,	/* Successfully recovered */
  };
  
++<<<<<<< HEAD
 +static const char *action_name[] = {
 +	[IGNORED] = "Ignored",
 +	[FAILED] = "Failed",
 +	[DELAYED] = "Delayed",
 +	[RECOVERED] = "Recovered",
++=======
+ static const char * const action_page_types[] = {
+ 	[MF_MSG_KERNEL]			= "reserved kernel page",
+ 	[MF_MSG_KERNEL_HIGH_ORDER]	= "high-order kernel page",
+ 	[MF_MSG_SLAB]			= "kernel slab page",
+ 	[MF_MSG_DIFFERENT_COMPOUND]	= "different compound page after locking",
+ 	[MF_MSG_POISONED_HUGE]		= "huge page already hardware poisoned",
+ 	[MF_MSG_HUGE]			= "huge page",
+ 	[MF_MSG_FREE_HUGE]		= "free huge page",
+ 	[MF_MSG_NON_PMD_HUGE]		= "non-pmd-sized huge page",
+ 	[MF_MSG_UNMAP_FAILED]		= "unmapping failed page",
+ 	[MF_MSG_DIRTY_SWAPCACHE]	= "dirty swapcache page",
+ 	[MF_MSG_CLEAN_SWAPCACHE]	= "clean swapcache page",
+ 	[MF_MSG_DIRTY_MLOCKED_LRU]	= "dirty mlocked LRU page",
+ 	[MF_MSG_CLEAN_MLOCKED_LRU]	= "clean mlocked LRU page",
+ 	[MF_MSG_DIRTY_UNEVICTABLE_LRU]	= "dirty unevictable LRU page",
+ 	[MF_MSG_CLEAN_UNEVICTABLE_LRU]	= "clean unevictable LRU page",
+ 	[MF_MSG_DIRTY_LRU]		= "dirty LRU page",
+ 	[MF_MSG_CLEAN_LRU]		= "clean LRU page",
+ 	[MF_MSG_TRUNCATED_LRU]		= "already truncated LRU page",
+ 	[MF_MSG_BUDDY]			= "free buddy page",
+ 	[MF_MSG_BUDDY_2ND]		= "free buddy page (2nd try)",
+ 	[MF_MSG_DAX]			= "dax page",
+ 	[MF_MSG_UNKNOWN]		= "unknown page",
++>>>>>>> 6100e34b2526 (mm, memory_failure: Teach memory_failure() about dev_pagemap pages)
  };
  
  /*
@@@ -1016,28 -1050,183 +1152,105 @@@ static int hwpoison_user_mappings(struc
  	 * any accesses to the poisoned memory.
  	 */
  	forcekill = PageDirty(hpage) || (flags & MF_MUST_KILL);
 -	kill_procs(&tokill, forcekill, !unmap_success, pfn, flags);
 +	kill_procs(&tokill, forcekill, trapno,
 +		      ret != SWAP_SUCCESS, p, pfn, flags);
  
 -	return unmap_success;
 +	return ret;
  }
  
 -static int identify_page_state(unsigned long pfn, struct page *p,
 -				unsigned long page_flags)
 +static void set_page_hwpoison_huge_page(struct page *hpage)
  {
 -	struct page_state *ps;
 -
 -	/*
 -	 * The first check uses the current page flags which may not have any
 -	 * relevant information. The second check with the saved page flags is
 -	 * carried out only if the first check can't determine the page status.
 -	 */
 -	for (ps = error_states;; ps++)
 -		if ((p->flags & ps->mask) == ps->res)
 -			break;
 -
 -	page_flags |= (p->flags & (1UL << PG_dirty));
 -
 -	if (!ps->mask)
 -		for (ps = error_states;; ps++)
 -			if ((page_flags & ps->mask) == ps->res)
 -				break;
 -	return page_action(ps, p, pfn);
 +	int i;
 +	int nr_pages = 1 << compound_order(hpage);
 +	for (i = 0; i < nr_pages; i++)
 +		SetPageHWPoison(hpage + i);
  }
  
 -static int memory_failure_hugetlb(unsigned long pfn, int flags)
 +static void clear_page_hwpoison_huge_page(struct page *hpage)
  {
 -	struct page *p = pfn_to_page(pfn);
 -	struct page *head = compound_head(p);
 -	int res;
 -	unsigned long page_flags;
 -
 -	if (TestSetPageHWPoison(head)) {
 -		pr_err("Memory failure: %#lx: already hardware poisoned\n",
 -		       pfn);
 -		return 0;
 -	}
 -
 -	num_poisoned_pages_inc();
 -
 -	if (!(flags & MF_COUNT_INCREASED) && !get_hwpoison_page(p)) {
 -		/*
 -		 * Check "filter hit" and "race with other subpage."
 -		 */
 -		lock_page(head);
 -		if (PageHWPoison(head)) {
 -			if ((hwpoison_filter(p) && TestClearPageHWPoison(p))
 -			    || (p != head && TestSetPageHWPoison(head))) {
 -				num_poisoned_pages_dec();
 -				unlock_page(head);
 -				return 0;
 -			}
 -		}
 -		unlock_page(head);
 -		dissolve_free_huge_page(p);
 -		action_result(pfn, MF_MSG_FREE_HUGE, MF_DELAYED);
 -		return 0;
 -	}
 -
 -	lock_page(head);
 -	page_flags = head->flags;
 -
 -	if (!PageHWPoison(head)) {
 -		pr_err("Memory failure: %#lx: just unpoisoned\n", pfn);
 -		num_poisoned_pages_dec();
 -		unlock_page(head);
 -		put_hwpoison_page(head);
 -		return 0;
 -	}
 -
 -	/*
 -	 * TODO: hwpoison for pud-sized hugetlb doesn't work right now, so
 -	 * simply disable it. In order to make it work properly, we need
 -	 * make sure that:
 -	 *  - conversion of a pud that maps an error hugetlb into hwpoison
 -	 *    entry properly works, and
 -	 *  - other mm code walking over page table is aware of pud-aligned
 -	 *    hwpoison entries.
 -	 */
 -	if (huge_page_size(page_hstate(head)) > PMD_SIZE) {
 -		action_result(pfn, MF_MSG_NON_PMD_HUGE, MF_IGNORED);
 -		res = -EBUSY;
 -		goto out;
 -	}
 -
 -	if (!hwpoison_user_mappings(p, pfn, flags, &head)) {
 -		action_result(pfn, MF_MSG_UNMAP_FAILED, MF_IGNORED);
 -		res = -EBUSY;
 -		goto out;
 -	}
 -
 -	res = identify_page_state(pfn, p, page_flags);
 -out:
 -	unlock_page(head);
 -	return res;
 +	int i;
 +	int nr_pages = 1 << compound_order(hpage);
 +	for (i = 0; i < nr_pages; i++)
 +		ClearPageHWPoison(hpage + i);
  }
  
+ static int memory_failure_dev_pagemap(unsigned long pfn, int flags,
+ 		struct dev_pagemap *pgmap)
+ {
+ 	struct page *page = pfn_to_page(pfn);
+ 	const bool unmap_success = true;
+ 	unsigned long size = 0;
+ 	struct to_kill *tk;
+ 	LIST_HEAD(tokill);
+ 	int rc = -EBUSY;
+ 	loff_t start;
+ 
+ 	/*
+ 	 * Prevent the inode from being freed while we are interrogating
+ 	 * the address_space, typically this would be handled by
+ 	 * lock_page(), but dax pages do not use the page lock. This
+ 	 * also prevents changes to the mapping of this pfn until
+ 	 * poison signaling is complete.
+ 	 */
+ 	if (!dax_lock_mapping_entry(page))
+ 		goto out;
+ 
+ 	if (hwpoison_filter(page)) {
+ 		rc = 0;
+ 		goto unlock;
+ 	}
+ 
+ 	switch (pgmap->type) {
+ 	case MEMORY_DEVICE_PRIVATE:
+ 	case MEMORY_DEVICE_PUBLIC:
+ 		/*
+ 		 * TODO: Handle HMM pages which may need coordination
+ 		 * with device-side memory.
+ 		 */
+ 		goto unlock;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * Use this flag as an indication that the dax page has been
+ 	 * remapped UC to prevent speculative consumption of poison.
+ 	 */
+ 	SetPageHWPoison(page);
+ 
+ 	/*
+ 	 * Unlike System-RAM there is no possibility to swap in a
+ 	 * different physical page at a given virtual address, so all
+ 	 * userspace consumption of ZONE_DEVICE memory necessitates
+ 	 * SIGBUS (i.e. MF_MUST_KILL)
+ 	 */
+ 	flags |= MF_ACTION_REQUIRED | MF_MUST_KILL;
+ 	collect_procs(page, &tokill, flags & MF_ACTION_REQUIRED);
+ 
+ 	list_for_each_entry(tk, &tokill, nd)
+ 		if (tk->size_shift)
+ 			size = max(size, 1UL << tk->size_shift);
+ 	if (size) {
+ 		/*
+ 		 * Unmap the largest mapping to avoid breaking up
+ 		 * device-dax mappings which are constant size. The
+ 		 * actual size of the mapping being torn down is
+ 		 * communicated in siginfo, see kill_proc()
+ 		 */
+ 		start = (page->index << PAGE_SHIFT) & ~(size - 1);
+ 		unmap_mapping_range(page->mapping, start, start + size, 0);
+ 	}
+ 	kill_procs(&tokill, flags & MF_MUST_KILL, !unmap_success, pfn, flags);
+ 	rc = 0;
+ unlock:
+ 	dax_unlock_mapping_entry(page);
+ out:
+ 	/* drop pgmap ref acquired in caller */
+ 	put_dev_pagemap(pgmap);
+ 	action_result(pfn, MF_MSG_DAX, rc ? MF_FAILED : MF_RECOVERED);
+ 	return rc;
+ }
+ 
  /**
   * memory_failure - Handle memory failure of a page.
   * @pfn: Page Number of the corrupted page
@@@ -1062,8 -1249,8 +1275,9 @@@ int memory_failure(unsigned long pfn, i
  	struct page *p;
  	struct page *hpage;
  	struct page *orig_head;
+ 	struct dev_pagemap *pgmap;
  	int res;
 +	unsigned int nr_pages;
  	unsigned long page_flags;
  
  	if (!sysctl_memory_failure_recovery)
@@@ -1076,10 -1262,16 +1290,14 @@@
  		return -ENXIO;
  	}
  
+ 	pgmap = get_dev_pagemap(pfn, NULL);
+ 	if (pgmap)
+ 		return memory_failure_dev_pagemap(pfn, flags, pgmap);
+ 
  	p = pfn_to_page(pfn);
 -	if (PageHuge(p))
 -		return memory_failure_hugetlb(pfn, flags);
 +	orig_head = hpage = compound_head(p);
  	if (TestSetPageHWPoison(p)) {
 -		pr_err("Memory failure: %#lx: already hardware poisoned\n",
 -			pfn);
 +		printk(KERN_ERR "MCE %#lx: already hardware poisoned\n", pfn);
  		return 0;
  	}
  
* Unmerged path include/linux/mm.h
* Unmerged path mm/memory-failure.c
