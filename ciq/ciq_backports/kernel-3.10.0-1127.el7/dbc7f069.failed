sched: Use replace normalize_task() with __sched_setscheduler()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit dbc7f069b93a249340e974d6e8f55656280d8701
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/dbc7f069.failed

Reduce duplicate logic; normalize_task() is a simplified version of
__sched_setscheduler(). Parametrize the difference and collapse.

This reduces the amount of check_class_changed() sites.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: ktkhai@parallels.com
	Cc: rostedt@goodmis.org
	Cc: juri.lelli@gmail.com
	Cc: pang.xunlei@linaro.org
	Cc: oleg@redhat.com
	Cc: wanpeng.li@linux.intel.com
	Cc: umgwanakikbuti@gmail.com
Link: http://lkml.kernel.org/r/20150611124742.532642391@infradead.org
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit dbc7f069b93a249340e974d6e8f55656280d8701)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 9474c46ea21e,b610ef9e522f..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -4631,12 -3422,28 +4631,12 @@@ static bool check_same_owner(struct tas
  	return match;
  }
  
 -static bool dl_param_changed(struct task_struct *p,
 -		const struct sched_attr *attr)
 -{
 -	struct sched_dl_entity *dl_se = &p->dl;
 -
 -	if (dl_se->dl_runtime != attr->sched_runtime ||
 -		dl_se->dl_deadline != attr->sched_deadline ||
 -		dl_se->dl_period != attr->sched_period ||
 -		dl_se->flags != attr->sched_flags)
 -		return true;
 -
 -	return false;
 -}
 -
  static int __sched_setscheduler(struct task_struct *p,
  				const struct sched_attr *attr,
- 				bool user)
+ 				bool user, bool pi)
  {
 -	int newprio = dl_policy(attr->sched_policy) ? MAX_DL_PRIO - 1 :
 -		      MAX_RT_PRIO - 1 - attr->sched_priority;
  	int retval, oldprio, oldpolicy = -1, queued, running;
 -	int new_effective_prio, policy = attr->sched_policy;
 +	int policy = attr->sched_policy;
  	unsigned long flags;
  	const struct sched_class *prev_class;
  	struct rq *rq;
@@@ -4813,18 -3621,34 +4813,44 @@@ change
  		return -EBUSY;
  	}
  
++<<<<<<< HEAD
++=======
+ 	p->sched_reset_on_fork = reset_on_fork;
+ 	oldprio = p->prio;
+ 
+ 	if (pi) {
+ 		/*
+ 		 * Take priority boosted tasks into account. If the new
+ 		 * effective priority is unchanged, we just store the new
+ 		 * normal parameters and do not touch the scheduler class and
+ 		 * the runqueue. This will be done when the task deboost
+ 		 * itself.
+ 		 */
+ 		new_effective_prio = rt_mutex_get_effective_prio(p, newprio);
+ 		if (new_effective_prio == oldprio) {
+ 			__setscheduler_params(p, attr);
+ 			task_rq_unlock(rq, p, &flags);
+ 			return 0;
+ 		}
+ 	}
+ 
++>>>>>>> dbc7f069b93a (sched: Use replace normalize_task() with __sched_setscheduler())
  	queued = task_on_rq_queued(p);
  	running = task_current(rq, p);
  	if (queued)
 -		dequeue_task(rq, p, 0);
 +		dequeue_task(rq, p, DEQUEUE_SAVE);
  	if (running)
 -		put_prev_task(rq, p);
 +		p->sched_class->put_prev_task(rq, p);
  
 +	p->sched_reset_on_fork = reset_on_fork;
 +
 +	oldprio = p->prio;
  	prev_class = p->sched_class;
++<<<<<<< HEAD
 +	__setscheduler(rq, p, attr);
++=======
+ 	__setscheduler(rq, p, attr, pi);
++>>>>>>> dbc7f069b93a (sched: Use replace normalize_task() with __sched_setscheduler())
  
  	if (running)
  		p->sched_class->set_curr_task(rq);
@@@ -8742,35 -7364,15 +8769,39 @@@ EXPORT_SYMBOL(__might_sleep)
  #endif
  
  #ifdef CONFIG_MAGIC_SYSRQ
++<<<<<<< HEAD
 +static void normalize_task(struct rq *rq, struct task_struct *p)
 +{
 +	const struct sched_class *prev_class = p->sched_class;
 +	struct sched_attr attr = {
 +		.sched_policy = SCHED_NORMAL,
 +	};
 +	int old_prio = p->prio;
 +	int queued;
 +
 +	queued = task_on_rq_queued(p);
 +	if (queued)
 +		dequeue_task(rq, p, DEQUEUE_SAVE);
 +	__setscheduler(rq, p, &attr);
 +	if (queued) {
 +		enqueue_task(rq, p, ENQUEUE_RESTORE);
 +		resched_curr(rq);
 +	}
 +
 +	check_class_changed(rq, p, prev_class, old_prio);
 +}
 +
++=======
++>>>>>>> dbc7f069b93a (sched: Use replace normalize_task() with __sched_setscheduler())
  void normalize_rt_tasks(void)
  {
  	struct task_struct *g, *p;
- 	unsigned long flags;
- 	struct rq *rq;
+ 	struct sched_attr attr = {
+ 		.sched_policy = SCHED_NORMAL,
+ 	};
  
 -	read_lock(&tasklist_lock);
 -	for_each_process_thread(g, p) {
 +	qread_lock_irqsave(&tasklist_lock, flags);
 +	do_each_thread(g, p) {
  		/*
  		 * Only normalize user tasks:
  		 */
@@@ -8794,16 -7396,9 +8825,22 @@@
  			continue;
  		}
  
++<<<<<<< HEAD
 +		raw_spin_lock(&p->pi_lock);
 +		rq = __task_rq_lock(p);
 +
 +		normalize_task(rq, p);
 +
 +		__task_rq_unlock(rq);
 +		raw_spin_unlock(&p->pi_lock);
 +	} while_each_thread(g, p);
 +
 +	qread_unlock_irqrestore(&tasklist_lock, flags);
++=======
+ 		__sched_setscheduler(p, &attr, false, false);
+ 	}
+ 	read_unlock(&tasklist_lock);
++>>>>>>> dbc7f069b93a (sched: Use replace normalize_task() with __sched_setscheduler())
  }
  
  #endif /* CONFIG_MAGIC_SYSRQ */
* Unmerged path kernel/sched/core.c
