x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Tony Luck <tony.luck@intel.com>
commit fd0e786d9d09024f67bd71ec094b110237dc3840
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/fd0e786d.failed

In the following commit:

  ce0fa3e56ad2 ("x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages")

... we added code to memory_failure() to unmap the page from the
kernel 1:1 virtual address space to avoid speculative access to the
page logging additional errors.

But memory_failure() may not always succeed in taking the page offline,
especially if the page belongs to the kernel.  This can happen if
there are too many corrected errors on a page and either mcelog(8)
or drivers/ras/cec.c asks to take a page offline.

Since we remove the 1:1 mapping early in memory_failure(), we can
end up with the page unmapped, but still in use. On the next access
the kernel crashes :-(

There are also various debug paths that call memory_failure() to simulate
occurrence of an error. Since there is no actual error in memory, we
don't need to map out the page for those cases.

Revert most of the previous attempt and keep the solution local to
arch/x86/kernel/cpu/mcheck/mce.c. Unmap the page only when:

	1) there is a real error
	2) memory_failure() succeeds.

All of this only applies to 64-bit systems. 32-bit kernel doesn't map
all of memory into kernel space. It isn't worth adding the code to unmap
the piece that is mapped because nobody would run a 32-bit kernel on a
machine that has recoverable machine checks.

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dave <dave.hansen@intel.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Robert (Persistent Memory) <elliott@hpe.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-mm@kvack.org
	Cc: stable@vger.kernel.org #v4.14
Fixes: ce0fa3e56ad2 ("x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages")
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit fd0e786d9d09024f67bd71ec094b110237dc3840)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/mcheck/mce-internal.h
#	arch/x86/kernel/cpu/mcheck/mce.c
#	include/linux/mm_inline.h
#	mm/memory-failure.c
diff --cc arch/x86/kernel/cpu/mcheck/mce-internal.h
index de20902ecf23,e956eb267061..000000000000
--- a/arch/x86/kernel/cpu/mcheck/mce-internal.h
+++ b/arch/x86/kernel/cpu/mcheck/mce-internal.h
@@@ -96,3 -100,34 +96,37 @@@ static inline bool mce_cmp(struct mce *
  		m1->addr != m2->addr ||
  		m1->misc != m2->misc;
  }
++<<<<<<< HEAD
++=======
+ 
+ extern struct device_attribute dev_attr_trigger;
+ 
+ #ifdef CONFIG_X86_MCELOG_LEGACY
+ void mce_work_trigger(void);
+ void mce_register_injector_chain(struct notifier_block *nb);
+ void mce_unregister_injector_chain(struct notifier_block *nb);
+ #else
+ static inline void mce_work_trigger(void)	{ }
+ static inline void mce_register_injector_chain(struct notifier_block *nb)	{ }
+ static inline void mce_unregister_injector_chain(struct notifier_block *nb)	{ }
+ #endif
+ 
+ extern struct mca_config mca_cfg;
+ 
+ #ifndef CONFIG_X86_64
+ /*
+  * On 32-bit systems it would be difficult to safely unmap a poison page
+  * from the kernel 1:1 map because there are no non-canonical addresses that
+  * we can use to refer to the address without risking a speculative access.
+  * However, this isn't much of an issue because:
+  * 1) Few unmappable pages are in the 1:1 map. Most are in HIGHMEM which
+  *    are only mapped into the kernel as needed
+  * 2) Few people would run a 32-bit kernel on a machine that supports
+  *    recoverable errors because they have too much memory to boot 32-bit.
+  */
+ static inline void mce_unmap_kpfn(unsigned long pfn) {}
+ #define mce_unmap_kpfn mce_unmap_kpfn
+ #endif
+ 
+ #endif /* __X86_MCE_INTERNAL_H__ */
++>>>>>>> fd0e786d9d09 (x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages)
diff --cc arch/x86/kernel/cpu/mcheck/mce.c
index f77fd7cdb99d,8ff94d1e2dce..000000000000
--- a/arch/x86/kernel/cpu/mcheck/mce.c
+++ b/arch/x86/kernel/cpu/mcheck/mce.c
@@@ -565,7 -594,8 +569,12 @@@ static int srao_decode_notifier(struct 
  
  	if (mce_usable_address(mce) && (mce->severity == MCE_AO_SEVERITY)) {
  		pfn = mce->addr >> PAGE_SHIFT;
++<<<<<<< HEAD
 +		memory_failure(pfn, MCE_VECTOR, 0);
++=======
+ 		if (!memory_failure(pfn, 0))
+ 			mce_unmap_kpfn(pfn);
++>>>>>>> fd0e786d9d09 (x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages)
  	}
  
  	return NOTIFY_OK;
@@@ -1082,12 -1059,54 +1091,57 @@@ static int do_memory_failure(struct mc
  	pr_err("Uncorrected hardware memory error in user-access at %llx", m->addr);
  	if (!(m->mcgstatus & MCG_STATUS_RIPV))
  		flags |= MF_MUST_KILL;
 -	ret = memory_failure(m->addr >> PAGE_SHIFT, flags);
 +	ret = memory_failure(m->addr >> PAGE_SHIFT, MCE_VECTOR, flags);
  	if (ret)
  		pr_err("Memory error not recovered");
+ 	else
+ 		mce_unmap_kpfn(m->addr >> PAGE_SHIFT);
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ #ifndef mce_unmap_kpfn
+ static void mce_unmap_kpfn(unsigned long pfn)
+ {
+ 	unsigned long decoy_addr;
+ 
+ 	/*
+ 	 * Unmap this page from the kernel 1:1 mappings to make sure
+ 	 * we don't log more errors because of speculative access to
+ 	 * the page.
+ 	 * We would like to just call:
+ 	 *	set_memory_np((unsigned long)pfn_to_kaddr(pfn), 1);
+ 	 * but doing that would radically increase the odds of a
+ 	 * speculative access to the poison page because we'd have
+ 	 * the virtual address of the kernel 1:1 mapping sitting
+ 	 * around in registers.
+ 	 * Instead we get tricky.  We create a non-canonical address
+ 	 * that looks just like the one we want, but has bit 63 flipped.
+ 	 * This relies on set_memory_np() not checking whether we passed
+ 	 * a legal address.
+ 	 */
+ 
+ /*
+  * Build time check to see if we have a spare virtual bit. Don't want
+  * to leave this until run time because most developers don't have a
+  * system that can exercise this code path. This will only become a
+  * problem if/when we move beyond 5-level page tables.
+  *
+  * Hard code "9" here because cpp doesn't grok ilog2(PTRS_PER_PGD)
+  */
+ #if PGDIR_SHIFT + 9 < 63
+ 	decoy_addr = (pfn << PAGE_SHIFT) + (PAGE_OFFSET ^ BIT(63));
+ #else
+ #error "no unused virtual bit available"
+ #endif
+ 
+ 	if (set_memory_np(decoy_addr, 1))
+ 		pr_warn("Could not invalidate pfn=0x%lx from 1:1 map\n", pfn);
+ }
+ #endif
+ 
++>>>>>>> fd0e786d9d09 (x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages)
  /*
   * The actual machine check handler. This only handles real
   * exceptions when something got corrupted coming in through int 18.
diff --cc include/linux/mm_inline.h
index cf55945c83fb,10191c28fc04..000000000000
--- a/include/linux/mm_inline.h
+++ b/include/linux/mm_inline.h
@@@ -100,4 -125,6 +100,9 @@@ static __always_inline enum lru_list pa
  	return lru;
  }
  
++<<<<<<< HEAD
++=======
+ #define lru_to_page(head) (list_entry((head)->prev, struct page, lru))
+ 
++>>>>>>> fd0e786d9d09 (x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages)
  #endif
diff --cc mm/memory-failure.c
index 2d6d813952ef,8291b75f42c8..000000000000
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@@ -1083,18 -1139,8 +1083,23 @@@ int memory_failure(unsigned long pfn, i
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Currently errors on hugetlbfs pages are measured in hugepage units,
 +	 * so nr_pages should be 1 << compound_order.  OTOH when errors are on
 +	 * transparent hugepages, they are supposed to be split and error
 +	 * measurement is done in normal page units.  So nr_pages should be one
 +	 * in this case.
 +	 */
 +	if (PageHuge(p))
 +		nr_pages = 1 << compound_order(hpage);
 +	else /* normal page or thp */
 +		nr_pages = 1;
 +	atomic_long_add(nr_pages, &num_poisoned_pages);
++=======
+ 	orig_head = hpage = compound_head(p);
+ 	num_poisoned_pages_inc();
++>>>>>>> fd0e786d9d09 (x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages)
  
  	/*
  	 * We need/can do nothing about count=0 pages.
* Unmerged path arch/x86/kernel/cpu/mcheck/mce-internal.h
* Unmerged path arch/x86/kernel/cpu/mcheck/mce.c
* Unmerged path include/linux/mm_inline.h
* Unmerged path mm/memory-failure.c
