device-dax: Enable page_mapping()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 2232c6382a453db73d2e723df1b52030066e135e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/2232c638.failed

In support of enabling memory_failure() handling for device-dax
mappings, set the ->mapping association of pages backing device-dax
mappings. The rmap implementation requires page_mapping() to return the
address_space hosting the vmas that map the page.

The ->mapping pointer is never cleared. There is no possibility for the
page to become associated with another address_space while the device is
enabled. When the device is disabled the 'struct page' array for the
device is destroyed / later reinitialized to zero.

	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
(cherry picked from commit 2232c6382a453db73d2e723df1b52030066e135e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/device.c
diff --cc drivers/dax/device.c
index 571ef367efdd,95cfcfd612df..000000000000
--- a/drivers/dax/device.c
+++ b/drivers/dax/device.c
@@@ -249,14 -244,12 +249,18 @@@ __weak phys_addr_t dax_pgoff_to_phys(st
  	return -1;
  }
  
++<<<<<<< HEAD
 +static int __dev_dax_pte_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
++=======
+ static vm_fault_t __dev_dax_pte_fault(struct dev_dax *dev_dax,
+ 				struct vm_fault *vmf, pfn_t *pfn)
++>>>>>>> 2232c6382a45 (device-dax: Enable page_mapping())
  {
 +	unsigned long vaddr = (unsigned long) vmf->virtual_address;
  	struct device *dev = &dev_dax->dev;
  	struct dax_region *dax_region;
 +	int rc = VM_FAULT_SIGBUS;
  	phys_addr_t phys;
- 	pfn_t pfn;
  	unsigned int fault_size = PAGE_SIZE;
  
  	if (check_vma(dev_dax, vmf->vma, __func__))
@@@ -278,29 -271,20 +282,36 @@@
  		return VM_FAULT_SIGBUS;
  	}
  
- 	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
+ 	*pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
++<<<<<<< HEAD
 +	rc = vm_insert_mixed(vmf->vma, vaddr, pfn);
 +
 +	if (rc == -ENOMEM)
 +		return VM_FAULT_OOM;
 +	if (rc < 0 && rc != -EBUSY)
 +		return VM_FAULT_SIGBUS;
 +
 +	return VM_FAULT_NOPAGE;
 +}
 +
 +static int __dev_dax_pmd_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
++=======
+ 	return vmf_insert_mixed(vmf->vma, vmf->address, *pfn);
+ }
+ 
+ static vm_fault_t __dev_dax_pmd_fault(struct dev_dax *dev_dax,
+ 				struct vm_fault *vmf, pfn_t *pfn)
++>>>>>>> 2232c6382a45 (device-dax: Enable page_mapping())
  {
 -	unsigned long pmd_addr = vmf->address & PMD_MASK;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	unsigned long pmd_addr = address & PMD_MASK;
  	struct device *dev = &dev_dax->dev;
  	struct dax_region *dax_region;
  	phys_addr_t phys;
  	pgoff_t pgoff;
- 	pfn_t pfn;
  	unsigned int fault_size = PMD_SIZE;
 +	int ret;
  
  	if (check_vma(dev_dax, vmf->vma, __func__))
  		return VM_FAULT_SIGBUS;
@@@ -335,27 -319,17 +346,36 @@@
  		return VM_FAULT_SIGBUS;
  	}
  
- 	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
+ 	*pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
++<<<<<<< HEAD
 +	ret = vmf_insert_pfn_pmd(vmf->vma, address, vmf->pmd, pfn,
++=======
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd, *pfn,
++>>>>>>> 2232c6382a45 (device-dax: Enable page_mapping())
  			vmf->flags & FAULT_FLAG_WRITE);
 +	/*
 +	 * RHEL-only: vmf_insert_pfn_pmd() returns VM_FAULT_FALLBACK if our
 +	 * hugepage PMD insertion collided with a PMD that is a parent of
 +	 * PTEs.  This is a failure case for filesystem DAX but should never
 +	 * happen for device DAX.  If it does, SIGBUS because device DAX
 +	 * doesn't mix page sizes in a given namespace.
 +	 */
 +	if (ret == VM_FAULT_FALLBACK)
 +		ret = VM_FAULT_SIGBUS;
 +	return ret;
  }
  
  #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
++<<<<<<< HEAD
 +static int __dev_dax_pud_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
++=======
+ static vm_fault_t __dev_dax_pud_fault(struct dev_dax *dev_dax,
+ 				struct vm_fault *vmf, pfn_t *pfn)
++>>>>>>> 2232c6382a45 (device-dax: Enable page_mapping())
  {
 -	unsigned long pud_addr = vmf->address & PUD_MASK;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	unsigned long pud_addr = address & PUD_MASK;
  	struct device *dev = &dev_dax->dev;
  	struct dax_region *dax_region;
  	phys_addr_t phys;
@@@ -397,23 -370,26 +416,34 @@@
  		return VM_FAULT_SIGBUS;
  	}
  
- 	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
+ 	*pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
++<<<<<<< HEAD
 +	return vmf_insert_pfn_pud(vmf->vma, address, vmf->pud, pfn,
 +			vmf->flags & FAULT_FLAG_WRITE);
 +}
 +#else
 +static int __dev_dax_pud_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
++=======
+ 	return vmf_insert_pfn_pud(vmf->vma, vmf->address, vmf->pud, *pfn,
+ 			vmf->flags & FAULT_FLAG_WRITE);
+ }
+ #else
+ static vm_fault_t __dev_dax_pud_fault(struct dev_dax *dev_dax,
+ 				struct vm_fault *vmf, pfn_t *pfn)
++>>>>>>> 2232c6382a45 (device-dax: Enable page_mapping())
  {
  	return VM_FAULT_FALLBACK;
  }
  #endif /* !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
  
 -static vm_fault_t dev_dax_huge_fault(struct vm_fault *vmf,
 +static int dev_dax_huge_fault(struct vm_fault *vmf,
  		enum page_entry_size pe_size)
  {
- 	int rc, id;
  	struct file *filp = vmf->vma->vm_file;
+ 	unsigned long fault_size;
+ 	int rc, id;
+ 	pfn_t pfn;
  	struct dev_dax *dev_dax = filp->private_data;
  
  	dev_dbg(&dev_dax->dev, "%s: %s (%#lx - %#lx) size = %d\n", current->comm,
* Unmerged path drivers/dax/device.c
