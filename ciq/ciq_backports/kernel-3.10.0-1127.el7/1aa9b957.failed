kvm: x86: mmu: Recovery of shattered NX large pages

jira LE-1907
cve CVE-2018-12207
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Junaid Shahid <junaids@google.com>
commit 1aa9b9572b10529c2e64e2b8f44025d86e124308
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/1aa9b957.failed

The page table pages corresponding to broken down large pages are zapped in
FIFO order, so that the large page can potentially be recovered, if it is
not longer being used for execution.  This removes the performance penalty
for walking deeper EPT page tables.

By default, one large page will last about one hour once the guest
reaches a steady state.

	Signed-off-by: Junaid Shahid <junaids@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 1aa9b9572b10529c2e64e2b8f44025d86e124308)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/kernel-parameters.txt
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu.c
#	virt/kvm/kvm_main.c
diff --cc Documentation/kernel-parameters.txt
index 4ee8a23b1fd1,8dee8f68fe15..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -1567,6 -2055,25 +1567,28 @@@ bytes respectively. Such letter suffixe
  			KVM MMU at runtime.
  			Default is 0 (off)
  
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 	kvm.nx_huge_pages=
+ 			[KVM] Controls the software workaround for the
+ 			X86_BUG_ITLB_MULTIHIT bug.
+ 			force	: Always deploy workaround.
+ 			off	: Never deploy workaround.
+ 			auto    : Deploy workaround based on the presence of
+ 				  X86_BUG_ITLB_MULTIHIT.
+ 
+ 			Default is 'auto'.
+ 
+ 			If the software workaround is enabled for the host,
+ 			guests do need not to enable it for nested guests.
+ 
+ 	kvm.nx_huge_pages_recovery_ratio=
+ 			[KVM] Controls how many 4KiB pages are periodically zapped
+ 			back to huge pages.  0 disables the recovery, otherwise if
+ 			the value is N KVM will zap 1/Nth of the 4KiB pages every
+ 			minute.  The default is 60.
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages):Documentation/admin-guide/kernel-parameters.txt
  	kvm-amd.nested=	[KVM,AMD] Allow nested virtualization in KVM/SVM.
  			Default is 1 (enabled)
  
diff --cc arch/x86/include/asm/kvm_host.h
index 35f22ae3e497,4fc61483919a..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -283,6 -312,12 +283,15 @@@ struct kvm_rmap_head 
  struct kvm_mmu_page {
  	struct list_head link;
  	struct hlist_node hash_link;
++<<<<<<< HEAD
++=======
+ 	struct list_head lpage_disallowed_link;
+ 
+ 	bool unsync;
+ 	u8 mmu_valid_gen;
+ 	bool mmio_cached;
+ 	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  
  	/*
  	 * The following two entries are used to key the shadow page in the
@@@ -843,12 -933,11 +853,20 @@@ struct kvm_arch 
  	bool x2apic_format;
  	bool x2apic_broadcast_quirk_disabled;
  
++<<<<<<< HEAD
 +	/* Struct members for AVIC */
 +	u32 avic_vm_id;
 +	u32 ldr_mode;
 +	struct page *avic_logical_id_table_page;
 +	struct page *avic_physical_id_table_page;
 +	struct hlist_node hnode;
++=======
+ 	bool guest_can_read_msr_platform_info;
+ 	bool exception_payload_enabled;
+ 
+ 	struct kvm_pmu_event_filter *pmu_event_filter;
+ 	struct task_struct *nx_lpage_recovery_thread;
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  };
  
  struct kvm_vm_stat {
diff --cc arch/x86/kvm/mmu.c
index 9724e772f5d0,529589a42afb..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -47,6 -48,30 +48,33 @@@
  #include <asm/kvm_page_track.h>
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ extern bool itlb_multihit_kvm_mitigation;
+ 
+ static int __read_mostly nx_huge_pages = -1;
+ static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
+ 
+ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp);
+ static int set_nx_huge_pages_recovery_ratio(const char *val, const struct kernel_param *kp);
+ 
+ static struct kernel_param_ops nx_huge_pages_ops = {
+ 	.set = set_nx_huge_pages,
+ 	.get = param_get_bool,
+ };
+ 
+ static struct kernel_param_ops nx_huge_pages_recovery_ratio_ops = {
+ 	.set = set_nx_huge_pages_recovery_ratio,
+ 	.get = param_get_uint,
+ };
+ 
+ module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ module_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_ratio_ops,
+ 		&nx_huge_pages_recovery_ratio, 0644);
+ __MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, "uint");
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  /*
   * When setting this variable to true it enables Two-Dimensional-Paging
   * where the hardware walks 2 page tables:
@@@ -1044,6 -1220,17 +1072,20 @@@ static void account_shadowed(struct kv
  	kvm_mmu_gfn_disallow_lpage(slot, gfn);
  }
  
++<<<<<<< HEAD
++=======
+ static void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
+ {
+ 	if (sp->lpage_disallowed)
+ 		return;
+ 
+ 	++kvm->stat.nx_lpage_splits;
+ 	list_add_tail(&sp->lpage_disallowed_link,
+ 		      &kvm->arch.lpage_disallowed_mmu_pages);
+ 	sp->lpage_disallowed = true;
+ }
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
  {
  	struct kvm_memslots *slots;
@@@ -1061,6 -1248,13 +1103,16 @@@
  	kvm_mmu_gfn_allow_lpage(slot, gfn);
  }
  
++<<<<<<< HEAD
++=======
+ static void unaccount_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
+ {
+ 	--kvm->stat.nx_lpage_splits;
+ 	sp->lpage_disallowed = false;
+ 	list_del(&sp->lpage_disallowed_link);
+ }
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  static bool __mmu_gfn_lpage_is_disallowed(gfn_t gfn, int level,
  					  struct kvm_memory_slot *slot)
  {
@@@ -5822,8 -6222,88 +5874,86 @@@ static void mmu_destroy_caches(void
  	kmem_cache_destroy(mmu_page_header_cache);
  }
  
++<<<<<<< HEAD
++=======
+ static void kvm_set_mmio_spte_mask(void)
+ {
+ 	u64 mask;
+ 
+ 	/*
+ 	 * Set the reserved bits and the present bit of an paging-structure
+ 	 * entry to generate page fault with PFER.RSV = 1.
+ 	 */
+ 
+ 	/*
+ 	 * Mask the uppermost physical address bit, which would be reserved as
+ 	 * long as the supported physical address width is less than 52.
+ 	 */
+ 	mask = 1ull << 51;
+ 
+ 	/* Set the present bit. */
+ 	mask |= 1ull;
+ 
+ 	/*
+ 	 * If reserved bit is not supported, clear the present bit to disable
+ 	 * mmio page fault.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_X86_64) && shadow_phys_bits == 52)
+ 		mask &= ~1ull;
+ 
+ 	kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ }
+ 
+ static bool get_nx_auto_mode(void)
+ {
+ 	/* Return true when CPU has the bug, and mitigations are ON */
+ 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
+ }
+ 
+ static void __set_nx_huge_pages(bool val)
+ {
+ 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ }
+ 
+ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
+ {
+ 	bool old_val = nx_huge_pages;
+ 	bool new_val;
+ 
+ 	/* In "auto" mode deploy workaround only if CPU has the bug. */
+ 	if (sysfs_streq(val, "off"))
+ 		new_val = 0;
+ 	else if (sysfs_streq(val, "force"))
+ 		new_val = 1;
+ 	else if (sysfs_streq(val, "auto"))
+ 		new_val = get_nx_auto_mode();
+ 	else if (strtobool(val, &new_val) < 0)
+ 		return -EINVAL;
+ 
+ 	__set_nx_huge_pages(new_val);
+ 
+ 	if (new_val != old_val) {
+ 		struct kvm *kvm;
+ 		int idx;
+ 
+ 		mutex_lock(&kvm_lock);
+ 
+ 		list_for_each_entry(kvm, &vm_list, vm_list) {
+ 			idx = srcu_read_lock(&kvm->srcu);
+ 			kvm_mmu_zap_all_fast(kvm);
+ 			srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 			wake_up_process(kvm->arch.nx_lpage_recovery_thread);
+ 		}
+ 		mutex_unlock(&kvm_lock);
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  int kvm_mmu_module_init(void)
  {
 -	int ret = -ENOMEM;
 -
 -	if (nx_huge_pages == -1)
 -		__set_nx_huge_pages(get_nx_auto_mode());
 -
  	/*
  	 * MMU roles use union aliasing which is, generally speaking, an
  	 * undefined behavior. However, we supposedly know how compilers behave
diff --cc virt/kvm/kvm_main.c
index af3df0f2e28f,4aab3547a165..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -577,10 -626,28 +577,27 @@@ static int kvm_create_vm_debugfs(struc
  	return 0;
  }
  
+ /*
+  * Called after the VM is otherwise initialized, but just before adding it to
+  * the vm_list.
+  */
+ int __weak kvm_arch_post_init_vm(struct kvm *kvm)
+ {
+ 	return 0;
+ }
+ 
+ /*
+  * Called just after removing the VM from the vm_list, but before doing any
+  * other destruction.
+  */
+ void __weak kvm_arch_pre_destroy_vm(struct kvm *kvm)
+ {
+ }
+ 
  static struct kvm *kvm_create_vm(unsigned long type)
  {
 +	int r, i;
  	struct kvm *kvm = kvm_arch_alloc_vm();
 -	int r = -ENOMEM;
 -	int i;
  
  	if (!kvm)
  		return ERR_PTR(-ENOMEM);
@@@ -620,14 -697,12 +637,18 @@@
  		goto out_err_no_srcu;
  	if (init_srcu_struct(&kvm->irq_srcu))
  		goto out_err_no_irq_srcu;
 +	for (i = 0; i < KVM_NR_BUSES; i++) {
 +		kvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),
 +					GFP_KERNEL);
 +		if (!kvm->buses[i])
 +			goto out_err;
 +	}
  
  	r = kvm_init_mmu_notifier(kvm);
+ 	if (r)
+ 		goto out_err_no_mmu_notifier;
+ 
+ 	r = kvm_arch_post_init_vm(kvm);
  	if (r)
  		goto out_err;
  
@@@ -674,12 -761,19 +700,18 @@@ static void kvm_destroy_vm(struct kvm *
  	kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
  	kvm_destroy_vm_debugfs(kvm);
  	kvm_arch_sync_events(kvm);
 -	mutex_lock(&kvm_lock);
 +	spin_lock(&kvm_lock);
  	list_del(&kvm->vm_list);
++<<<<<<< HEAD
 +	spin_unlock(&kvm_lock);
++=======
+ 	mutex_unlock(&kvm_lock);
+ 	kvm_arch_pre_destroy_vm(kvm);
+ 
++>>>>>>> 1aa9b9572b10 (kvm: x86: mmu: Recovery of shattered NX large pages)
  	kvm_free_irq_routing(kvm);
 -	for (i = 0; i < KVM_NR_BUSES; i++) {
 -		struct kvm_io_bus *bus = kvm_get_bus(kvm, i);
 -
 -		if (bus)
 -			kvm_io_bus_destroy(bus);
 -		kvm->buses[i] = NULL;
 -	}
 +	for (i = 0; i < KVM_NR_BUSES; i++)
 +		kvm_io_bus_destroy(kvm->buses[i]);
  	kvm_coalesced_mmio_free(kvm);
  #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
  	mmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index c5ac2a472ee1..b72acc279030 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -198,4 +198,8 @@ void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn);
 int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu);
+
+int kvm_mmu_post_init_vm(struct kvm *kvm);
+void kvm_mmu_pre_destroy_vm(struct kvm *kvm);
+
 #endif
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 69feb8491cac..dc1ee2c4fda5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -8076,6 +8076,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	INIT_HLIST_HEAD(&kvm->arch.mask_notifier_list);
 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
 	INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+	INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
 	INIT_LIST_HEAD(&kvm->arch.assigned_dev_head);
 	atomic_set(&kvm->arch.noncoherent_dma_count, 0);
 
@@ -8105,6 +8106,11 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	return 0;
 }
 
+int kvm_arch_post_init_vm(struct kvm *kvm)
+{
+	return kvm_mmu_post_init_vm(kvm);
+}
+
 static void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -8211,6 +8217,11 @@ int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 }
 EXPORT_SYMBOL_GPL(x86_set_memory_region);
 
+void kvm_arch_pre_destroy_vm(struct kvm *kvm)
+{
+	kvm_mmu_pre_destroy_vm(kvm);
+}
+
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
* Unmerged path virt/kvm/kvm_main.c
