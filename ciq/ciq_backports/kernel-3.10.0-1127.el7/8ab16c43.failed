percpu: change the number of pages marked in the first_chunk pop bitmap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 8ab16c43ea79098f4126432c6b199a5d6ba24b6d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/8ab16c43.failed

The populated bitmap represents the state of the pages the chunk serves.
Prior, the bitmap was marked completely used as the first chunk was
allocated and immutable. This is misleading because the first chunk may
not be completely filled. Additionally, with moving the base_addr up in
the previous patch, the population check no longer corresponds to what
was being checked.

This patch modifies the population map to be only the number of pages
the region serves and to make what it was checking correspond correctly
again. The change is to remove any misunderstanding between the size of
the populated bitmap and the actual size of it. The work function page
iterators now use nr_pages for the check rather than pcpu_unit_pages
because nr_populated is now chunk specific. Without this, the work
function would try to populate the remainder of these chunks despite it
not serving any more than nr_pages when nr_pages is set less than
pcpu_unit_pages.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 8ab16c43ea79098f4126432c6b199a5d6ba24b6d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,773dafea181e..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -720,6 -720,65 +720,68 @@@ static void pcpu_free_area(struct pcpu_
  	pcpu_chunk_relocate(chunk, oslot);
  }
  
++<<<<<<< HEAD
++=======
+ static struct pcpu_chunk * __init pcpu_alloc_first_chunk(unsigned long tmp_addr,
+ 							 int map_size,
+ 							 int *map,
+ 							 int init_map_size)
+ {
+ 	struct pcpu_chunk *chunk;
+ 	unsigned long aligned_addr;
+ 	int start_offset, region_size;
+ 
+ 	/* region calculations */
+ 	aligned_addr = tmp_addr & PAGE_MASK;
+ 
+ 	start_offset = tmp_addr - aligned_addr;
+ 
+ 	region_size = PFN_ALIGN(start_offset + map_size);
+ 
+ 	/* allocate chunk */
+ 	chunk = memblock_virt_alloc(sizeof(struct pcpu_chunk) +
+ 				    BITS_TO_LONGS(region_size >> PAGE_SHIFT),
+ 				    0);
+ 
+ 	INIT_LIST_HEAD(&chunk->list);
+ 	INIT_LIST_HEAD(&chunk->map_extend_list);
+ 
+ 	chunk->base_addr = (void *)aligned_addr;
+ 	chunk->start_offset = start_offset;
+ 	chunk->end_offset = region_size - chunk->start_offset - map_size;
+ 
+ 	chunk->nr_pages = region_size >> PAGE_SHIFT;
+ 
+ 	chunk->map = map;
+ 	chunk->map_alloc = init_map_size;
+ 
+ 	/* manage populated page bitmap */
+ 	chunk->immutable = true;
+ 	bitmap_fill(chunk->populated, chunk->nr_pages);
+ 	chunk->nr_populated = chunk->nr_pages;
+ 
+ 	chunk->contig_hint = chunk->free_size = map_size;
+ 
+ 	if (chunk->start_offset) {
+ 		/* hide the beginning of the bitmap */
+ 		chunk->map[0] = 1;
+ 		chunk->map[1] = chunk->start_offset;
+ 		chunk->map_used = 1;
+ 	}
+ 
+ 	/* set chunk's free region */
+ 	chunk->map[++chunk->map_used] =
+ 		(chunk->start_offset + chunk->free_size) | 1;
+ 
+ 	if (chunk->end_offset) {
+ 		/* hide the end of the bitmap */
+ 		chunk->map[++chunk->map_used] = region_size | 1;
+ 	}
+ 
+ 	return chunk;
+ }
+ 
++>>>>>>> 8ab16c43ea79 (percpu: change the number of pages marked in the first_chunk pop bitmap)
  static struct pcpu_chunk *pcpu_alloc_chunk(void)
  {
  	struct pcpu_chunk *chunk;
* Unmerged path mm/percpu.c
