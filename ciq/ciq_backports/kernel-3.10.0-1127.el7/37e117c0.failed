sched: Guarantee task priority in pick_next_task()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 37e117c07b89194aae7062bc63bde1104c03db02
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/37e117c0.failed

Michael spotted that the idle_balance() push down created a task
priority problem.

Previously, when we called idle_balance() before pick_next_task() it
wasn't a problem when -- because of the rq->lock droppage -- an rt/dl
task slipped in.

Similarly for pre_schedule(), rt pre-schedule could have a dl task
slip in.

But by pulling it into the pick_next_task() loop, we'll not try a
higher task priority again.

Cure this by creating a re-start condition in pick_next_task(); and
triggering this from pick_next_task_{rt,fair}().

It also fixes a live-lock where we get stuck in pick_next_task_fair()
due to idle_balance() seeing !0 nr_running but there not actually
being any fair tasks about.

	Reported-by: Michael Wang <wangyun@linux.vnet.ibm.com>
Fixes: 38033c37faab ("sched: Push down pre_schedule() and idle_balance()")
	Tested-by: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Juri Lelli <juri.lelli@gmail.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
Link: http://lkml.kernel.org/r/20140224121218.GR15586@twins.programming.kicks-ass.net
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 37e117c07b89194aae7062bc63bde1104c03db02)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
#	kernel/sched/rt.c
#	kernel/sched/sched.h
diff --cc kernel/sched/core.c
index 9474c46ea21e,cde573d3f12e..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -3463,31 -2584,30 +3463,47 @@@ static void put_prev_task(struct rq *rq
   * Pick up the highest-prio task:
   */
  static inline struct task_struct *
 -pick_next_task(struct rq *rq, struct task_struct *prev)
 +pick_next_task(struct rq *rq)
  {
- 	const struct sched_class *class;
+ 	const struct sched_class *class = &fair_sched_class;
  	struct task_struct *p;
  
  	/*
  	 * Optimization: we know that if all tasks are in
  	 * the fair class we can call that function directly:
  	 */
++<<<<<<< HEAD
 +	if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
 +		p = fair_sched_class.pick_next_task(rq);
 +		if (likely(p))
++=======
+ 	if (likely(prev->sched_class == class &&
+ 		   rq->nr_running == rq->cfs.h_nr_running)) {
+ 		p = fair_sched_class.pick_next_task(rq, prev);
+ 		if (likely(p && p != RETRY_TASK))
++>>>>>>> 37e117c07b89 (sched: Guarantee task priority in pick_next_task())
  			return p;
 +
 +		/* assumes fair_sched_class->next == idle_sched_class */
 +		else
 +			p = idle_sched_class.pick_next_task(rq);
 +
 +		return p;
  	}
  
+ again:
  	for_each_class(class) {
++<<<<<<< HEAD
 +		p = class->pick_next_task(rq);
 +		if (p)
++=======
+ 		p = class->pick_next_task(rq, prev);
+ 		if (p) {
+ 			if (unlikely(p == RETRY_TASK))
+ 				goto again;
++>>>>>>> 37e117c07b89 (sched: Guarantee task priority in pick_next_task())
  			return p;
+ 		}
  	}
  
  	BUG(); /* the idle class will always have a runnable task */
diff --cc kernel/sched/fair.c
index c5d4b4242380,16042b58a32f..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -4905,12 -4686,14 +4905,13 @@@ static struct task_struct *pick_next_ta
  	struct cfs_rq *cfs_rq = &rq->cfs;
  	struct sched_entity *se;
  	struct task_struct *p;
+ 	int new_tasks;
  
 -again:
  #ifdef CONFIG_FAIR_GROUP_SCHED
  	if (!cfs_rq->nr_running)
 -		goto idle;
 +		return NULL;
  
 -	if (prev->sched_class != &fair_sched_class)
 +	if (!prev || prev->sched_class != &fair_sched_class)
  		goto simple;
  
  	/*
@@@ -4998,6 -4783,22 +4999,25 @@@ simple
  		hrtick_start_fair(rq, p);
  
  	return p;
++<<<<<<< HEAD
++=======
+ 
+ idle:
+ 	/*
+ 	 * Because idle_balance() releases (and re-acquires) rq->lock, it is
+ 	 * possible for any higher priority task to appear. In that case we
+ 	 * must re-start the pick_next_entity() loop.
+ 	 */
+ 	new_tasks = idle_balance(rq);
+ 
+ 	if (rq->nr_running != rq->cfs.h_nr_running)
+ 		return RETRY_TASK;
+ 
+ 	if (new_tasks)
+ 		goto again;
+ 
+ 	return NULL;
++>>>>>>> 37e117c07b89 (sched: Guarantee task priority in pick_next_task())
  }
  
  /*
diff --cc kernel/sched/rt.c
index 6b68ceb9a68d,398b3f990823..000000000000
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@@ -1370,9 -1354,32 +1370,35 @@@ static struct task_struct *_pick_next_t
  	return p;
  }
  
 -static struct task_struct *
 -pick_next_task_rt(struct rq *rq, struct task_struct *prev)
 +static struct task_struct *pick_next_task_rt(struct rq *rq)
  {
++<<<<<<< HEAD
 +	struct task_struct *p = _pick_next_task_rt(rq);
++=======
+ 	struct task_struct *p;
+ 	struct rt_rq *rt_rq = &rq->rt;
+ 
+ 	if (need_pull_rt_task(rq, prev)) {
+ 		pull_rt_task(rq);
+ 		/*
+ 		 * pull_rt_task() can drop (and re-acquire) rq->lock; this
+ 		 * means a dl task can slip in, in which case we need to
+ 		 * re-start task selection.
+ 		 */
+ 		if (unlikely(rq->dl.dl_nr_running))
+ 			return RETRY_TASK;
+ 	}
+ 
+ 	if (!rt_rq->rt_nr_running)
+ 		return NULL;
+ 
+ 	if (rt_rq_throttled(rt_rq))
+ 		return NULL;
+ 
+ 	put_prev_task(rq, prev);
+ 
+ 	p = _pick_next_task_rt(rq);
++>>>>>>> 37e117c07b89 (sched: Guarantee task priority in pick_next_task())
  
  	/* The running task is never eligible for pushing */
  	if (p)
diff --cc kernel/sched/sched.h
index 38c9ae998b34,1929deb3f29d..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -1232,19 -1080,19 +1232,21 @@@ static const u32 prio_to_wmult[40] = 
   /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
  };
  
 -#define ENQUEUE_WAKEUP		1
 -#define ENQUEUE_HEAD		2
 +#define ENQUEUE_WAKEUP		0x01
 +#define ENQUEUE_RESTORE 	0x02
  #ifdef CONFIG_SMP
 -#define ENQUEUE_WAKING		4	/* sched_class::task_waking was called */
 +#define ENQUEUE_MIGRATED	0x04
  #else
 -#define ENQUEUE_WAKING		0
 +#define ENQUEUE_MIGRATED	0x00
  #endif
 -#define ENQUEUE_REPLENISH	8
 +#define ENQUEUE_HEAD		0x08
 +#define ENQUEUE_REPLENISH	0x10
  
 -#define DEQUEUE_SLEEP		1
 +#define DEQUEUE_SLEEP		0x01
 +#define DEQUEUE_SAVE		0x02
  
+ #define RETRY_TASK		((void *)-1UL)
+ 
  struct sched_class {
  	const struct sched_class *next;
  
@@@ -1255,7 -1103,16 +1257,20 @@@
  
  	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
  
++<<<<<<< HEAD
 +	struct task_struct * (*pick_next_task) (struct rq *rq);
++=======
+ 	/*
+ 	 * It is the responsibility of the pick_next_task() method that will
+ 	 * return the next task to call put_prev_task() on the @prev task or
+ 	 * something equivalent.
+ 	 *
+ 	 * May return RETRY_TASK when it finds a higher prio class has runnable
+ 	 * tasks.
+ 	 */
+ 	struct task_struct * (*pick_next_task) (struct rq *rq,
+ 						struct task_struct *prev);
++>>>>>>> 37e117c07b89 (sched: Guarantee task priority in pick_next_task())
  	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
  
  #ifdef CONFIG_SMP
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/rt.c
* Unmerged path kernel/sched/sched.h
