KVM: x86: fix presentation of TSX feature in ARCH_CAPABILITIES

jira LE-1907
cve CVE-2019-19338
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit cbbaa2727aa3ae9e0a844803da7cef7fd3b94f2b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/cbbaa272.failed

KVM does not implement MSR_IA32_TSX_CTRL, so it must not be presented
to the guests.  It is also confusing to have !ARCH_CAP_TSX_CTRL_MSR &&
!RTM && ARCH_CAP_TAA_NO: lack of MSR_IA32_TSX_CTRL suggests TSX was not
hidden (it actually was), yet the value says that TSX is not vulnerable
to microarchitectural data sampling.  Fix both.

	Cc: stable@vger.kernel.org
	Tested-by: Jim Mattson <jmattson@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit cbbaa2727aa3ae9e0a844803da7cef7fd3b94f2b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index 69feb8491cac,6ea735d632e9..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1034,11 -1213,182 +1034,157 @@@ static u32 emulated_msrs[] = 
  	MSR_IA32_MCG_CTL,
  	MSR_IA32_MCG_EXT_CTL,
  	MSR_IA32_SMBASE,
 -	MSR_SMI_COUNT,
 -	MSR_PLATFORM_INFO,
 -	MSR_MISC_FEATURES_ENABLES,
  	MSR_AMD64_VIRT_SPEC_CTRL,
 -	MSR_IA32_POWER_CTL,
 -
 -	/*
 -	 * The following list leaves out MSRs whose values are determined
 -	 * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.
 -	 * We always support the "true" VMX control MSRs, even if the host
 -	 * processor does not, so I am putting these registers here rather
 -	 * than in msrs_to_save_all.
 -	 */
 -	MSR_IA32_VMX_BASIC,
 -	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
 -	MSR_IA32_VMX_TRUE_PROCBASED_CTLS,
 -	MSR_IA32_VMX_TRUE_EXIT_CTLS,
 -	MSR_IA32_VMX_TRUE_ENTRY_CTLS,
 -	MSR_IA32_VMX_MISC,
 -	MSR_IA32_VMX_CR0_FIXED0,
 -	MSR_IA32_VMX_CR4_FIXED0,
 -	MSR_IA32_VMX_VMCS_ENUM,
 -	MSR_IA32_VMX_PROCBASED_CTLS2,
 -	MSR_IA32_VMX_EPT_VPID_CAP,
 -	MSR_IA32_VMX_VMFUNC,
 -
 -	MSR_K7_HWCR,
 -	MSR_KVM_POLL_CONTROL,
  };
  
 -static u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];
  static unsigned num_emulated_msrs;
  
++<<<<<<< HEAD
++=======
+ /*
+  * List of msr numbers which are used to expose MSR-based features that
+  * can be used by a hypervisor to validate requested CPU features.
+  */
+ static const u32 msr_based_features_all[] = {
+ 	MSR_IA32_VMX_BASIC,
+ 	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
+ 	MSR_IA32_VMX_PINBASED_CTLS,
+ 	MSR_IA32_VMX_TRUE_PROCBASED_CTLS,
+ 	MSR_IA32_VMX_PROCBASED_CTLS,
+ 	MSR_IA32_VMX_TRUE_EXIT_CTLS,
+ 	MSR_IA32_VMX_EXIT_CTLS,
+ 	MSR_IA32_VMX_TRUE_ENTRY_CTLS,
+ 	MSR_IA32_VMX_ENTRY_CTLS,
+ 	MSR_IA32_VMX_MISC,
+ 	MSR_IA32_VMX_CR0_FIXED0,
+ 	MSR_IA32_VMX_CR0_FIXED1,
+ 	MSR_IA32_VMX_CR4_FIXED0,
+ 	MSR_IA32_VMX_CR4_FIXED1,
+ 	MSR_IA32_VMX_VMCS_ENUM,
+ 	MSR_IA32_VMX_PROCBASED_CTLS2,
+ 	MSR_IA32_VMX_EPT_VPID_CAP,
+ 	MSR_IA32_VMX_VMFUNC,
+ 
+ 	MSR_F10H_DECFG,
+ 	MSR_IA32_UCODE_REV,
+ 	MSR_IA32_ARCH_CAPABILITIES,
+ };
+ 
+ static u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];
+ static unsigned int num_msr_based_features;
+ 
+ static u64 kvm_get_arch_capabilities(void)
+ {
+ 	u64 data = 0;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))
+ 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, data);
+ 
+ 	/*
+ 	 * If nx_huge_pages is enabled, KVM's shadow paging will ensure that
+ 	 * the nested hypervisor runs with NX huge pages.  If it is not,
+ 	 * L1 is anyway vulnerable to ITLB_MULTIHIT explots from other
+ 	 * L1 guests, so it need not worry about its own (L2) guests.
+ 	 */
+ 	data |= ARCH_CAP_PSCHANGE_MC_NO;
+ 
+ 	/*
+ 	 * If we're doing cache flushes (either "always" or "cond")
+ 	 * we will do one whenever the guest does a vmlaunch/vmresume.
+ 	 * If an outer hypervisor is doing the cache flush for us
+ 	 * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that
+ 	 * capability to the guest too, and if EPT is disabled we're not
+ 	 * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will
+ 	 * require a nested hypervisor to do a flush of its own.
+ 	 */
+ 	if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+ 		data |= ARCH_CAP_SKIP_VMENTRY_L1DFLUSH;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))
+ 		data |= ARCH_CAP_RDCL_NO;
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		data |= ARCH_CAP_SSB_NO;
+ 	if (!boot_cpu_has_bug(X86_BUG_MDS))
+ 		data |= ARCH_CAP_MDS_NO;
+ 
+ 	/*
+ 	 * On TAA affected systems, export MDS_NO=0 when:
+ 	 *	- TSX is enabled on the host, i.e. X86_FEATURE_RTM=1.
+ 	 *	- Updated microcode is present. This is detected by
+ 	 *	  the presence of ARCH_CAP_TSX_CTRL_MSR and ensures
+ 	 *	  that VERW clears CPU buffers.
+ 	 *
+ 	 * When MDS_NO=0 is exported, guests deploy clear CPU buffer
+ 	 * mitigation and don't complain:
+ 	 *
+ 	 *	"Vulnerable: Clear CPU buffers attempted, no microcode"
+ 	 *
+ 	 * If TSX is disabled on the system, guests are also mitigated against
+ 	 * TAA and clear CPU buffer mitigation is not required for guests.
+ 	 */
+ 	if (!boot_cpu_has(X86_FEATURE_RTM))
+ 		data &= ~ARCH_CAP_TAA_NO;
+ 	else if (!boot_cpu_has_bug(X86_BUG_TAA))
+ 		data |= ARCH_CAP_TAA_NO;
+ 	else if (data & ARCH_CAP_TSX_CTRL_MSR)
+ 		data &= ~ARCH_CAP_MDS_NO;
+ 
+ 	/* KVM does not emulate MSR_IA32_TSX_CTRL.  */
+ 	data &= ~ARCH_CAP_TSX_CTRL_MSR;
+ 	return data;
+ }
+ EXPORT_SYMBOL_GPL(kvm_get_arch_capabilities);
+ 
+ static int kvm_get_msr_feature(struct kvm_msr_entry *msr)
+ {
+ 	switch (msr->index) {
+ 	case MSR_IA32_ARCH_CAPABILITIES:
+ 		msr->data = kvm_get_arch_capabilities();
+ 		break;
+ 	case MSR_IA32_UCODE_REV:
+ 		rdmsrl_safe(msr->index, &msr->data);
+ 		break;
+ 	default:
+ 		if (kvm_x86_ops->get_msr_feature(msr))
+ 			return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
+ {
+ 	struct kvm_msr_entry msr;
+ 	int r;
+ 
+ 	msr.index = index;
+ 	r = kvm_get_msr_feature(&msr);
+ 	if (r)
+ 		return r;
+ 
+ 	*data = msr.data;
+ 
+ 	return 0;
+ }
+ 
+ static bool __kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)
+ {
+ 	if (efer & EFER_FFXSR && !guest_cpuid_has(vcpu, X86_FEATURE_FXSR_OPT))
+ 		return false;
+ 
+ 	if (efer & EFER_SVME && !guest_cpuid_has(vcpu, X86_FEATURE_SVM))
+ 		return false;
+ 
+ 	if (efer & (EFER_LME | EFER_LMA) &&
+ 	    !guest_cpuid_has(vcpu, X86_FEATURE_LM))
+ 		return false;
+ 
+ 	if (efer & EFER_NX && !guest_cpuid_has(vcpu, X86_FEATURE_NX))
+ 		return false;
+ 
+ 	return true;
+ 
+ }
++>>>>>>> cbbaa2727aa3 (KVM: x86: fix presentation of TSX feature in ARCH_CAPABILITIES)
  bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)
  {
  	if (efer & efer_reserved_bits)
* Unmerged path arch/x86/kvm/x86.c
