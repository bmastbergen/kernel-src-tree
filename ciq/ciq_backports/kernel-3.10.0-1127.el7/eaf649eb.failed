mm: swap: clean up swap readahead

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Minchan Kim <minchan@kernel.org>
commit eaf649ebc3acfbb235ce31cebd06e4876d05758e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/eaf649eb.failed

When I see recent change of swap readahead, I am very unhappy about
current code structure which diverges two swap readahead algorithm in
do_swap_page.  This patch is to clean it up.

Main motivation is that fault handler doesn't need to be aware of
readahead algorithms but just should call swapin_readahead.

As first step, this patch cleans up a little bit but not perfect (I just
separate for review easier) so next patch will make the goal complete.

[minchan@kernel.org: do not check readahead flag with THP anon]
  Link: http://lkml.kernel.org/r/874lm83zho.fsf@yhuang-dev.intel.com
  Link: http://lkml.kernel.org/r/20180227232611.169883-1-minchan@kernel.org
Link: http://lkml.kernel.org/r/1509520520-32367-2-git-send-email-minchan@kernel.org
Link: http://lkml.kernel.org/r/20180220085249.151400-2-minchan@kernel.org
	Signed-off-by: Minchan Kim <minchan@kernel.org>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Huang Ying <ying.huang@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit eaf649ebc3acfbb235ce31cebd06e4876d05758e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swap.h
#	mm/memory.c
#	mm/swap_state.c
diff --cc include/linux/swap.h
index 55477d42411d,fa92177d863e..000000000000
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@@ -480,15 -424,8 +480,20 @@@ extern struct page *__read_swap_cache_a
  			bool *new_page_allocated);
  extern struct page *swapin_readahead(swp_entry_t, gfp_t,
  			struct vm_area_struct *vma, unsigned long addr);
++<<<<<<< HEAD
 +
 +extern struct page *swap_readahead_detect(struct vm_area_struct *vma,
 +					  struct vma_swap_readahead *swap_ra,
 +					  pte_t *page_table, pte_t orig_pte,
 +					  unsigned long address);
 +extern struct page *do_swap_page_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 +					   struct vm_area_struct *vma,
 +					   unsigned long address,
 +					   struct vma_swap_readahead *swap_ra);
++=======
+ extern struct page *do_swap_page_readahead(swp_entry_t fentry, gfp_t gfp_mask,
+ 					   struct vm_fault *vmf);
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  
  /* linux/mm/swapfile.c */
  extern atomic_long_t nr_swap_pages;
@@@ -607,17 -544,8 +612,22 @@@ static inline bool swap_use_vma_readahe
  	return false;
  }
  
++<<<<<<< HEAD
 +static inline struct page *swap_readahead_detect(
 +	struct vm_area_struct *vma, struct vma_swap_readahead *swap_ra,
 +	pte_t *page_table, pte_t orig_pte, unsigned long address)
 +{
 +	return NULL;
 +}
 +
 +static inline struct page *do_swap_page_readahead(
 +	swp_entry_t fentry, gfp_t gfp_mask,
 +	struct vm_area_struct *vma, unsigned long address,
 +	struct vma_swap_readahead *swap_ra)
++=======
+ static inline struct page *do_swap_page_readahead(swp_entry_t fentry,
+ 				gfp_t gfp_mask, struct vm_fault *vmf)
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  {
  	return NULL;
  }
diff --cc mm/memory.c
index 4bb8ecbab1ee,bc1ccff79538..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2749,39 -2875,31 +2749,47 @@@ EXPORT_SYMBOL(unmap_mapping_range)
  /*
   * We enter with non-exclusive mmap_sem (to exclude vma changes,
   * but allow concurrent faults), and pte mapped but not yet locked.
 - * We return with pte unmapped and unlocked.
 - *
 - * We return with the mmap_sem locked or unlocked in the same cases
 - * as does filemap_fault().
 + * We return with mmap_sem still held, but pte unmapped and unlocked.
   */
 -int do_swap_page(struct vm_fault *vmf)
 +static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 +		unsigned long address, pte_t *page_table, pmd_t *pmd,
 +		unsigned int flags, pte_t orig_pte)
  {
++<<<<<<< HEAD
 +	spinlock_t *ptl;
 +	struct page *page = NULL, *swapcache;
++=======
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct page *page = NULL, *swapcache;
+ 	struct mem_cgroup *memcg;
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  	swp_entry_t entry;
  	pte_t pte;
  	int locked;
 +	struct mem_cgroup *ptr;
 +	struct vma_swap_readahead swap_ra;
  	int exclusive = 0;
  	int ret = 0;
- 	bool vma_readahead = swap_use_vma_readahead();
  
++<<<<<<< HEAD
 +	if (vma_readahead)
 +		page = swap_readahead_detect(vma, &swap_ra,
 +					     page_table, orig_pte, address);
 +
 +	if (!pte_unmap_same(mm, pmd, page_table, orig_pte)) {
 +		if (page)
 +			put_page(page);
 +			// page_cache_release(page);
++=======
+ 	if (!pte_unmap_same(vma->vm_mm, vmf->pmd, vmf->pte, vmf->orig_pte))
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  		goto out;
- 	}
  
 -	entry = pte_to_swp_entry(vmf->orig_pte);
 +	entry = pte_to_swp_entry(orig_pte);
  	if (unlikely(non_swap_entry(entry))) {
  		if (is_migration_entry(entry)) {
 -			migration_entry_wait(vma->vm_mm, vmf->pmd,
 -					     vmf->address);
 -		} else if (is_device_private_entry(entry)) {
 +			migration_entry_wait(mm, pmd, address);
 +		} else if (is_hmm_entry(entry)) {
  			/*
  			 * For un-addressable device memory we call the pgmap
  			 * fault handler callback. The callback must migrate
@@@ -2797,17 -2915,36 +2805,47 @@@
  		}
  		goto out;
  	}
 -
 -
  	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
++<<<<<<< HEAD
 +	if (!page)
 +		page = lookup_swap_cache(entry, vma_readahead ? vma : NULL,
 +					 address);
 +	if (!page) {
 +		if (vma_readahead)
 +			page = do_swap_page_readahead(entry,
 +				GFP_HIGHUSER_MOVABLE, vma, address, &swap_ra);
 +		else
 +			page = swapin_readahead(entry,
 +				GFP_HIGHUSER_MOVABLE, vma, address);
++=======
+ 	page = lookup_swap_cache(entry, vma, vmf->address);
+ 	swapcache = page;
+ 
+ 	if (!page) {
+ 		struct swap_info_struct *si = swp_swap_info(entry);
+ 
+ 		if (si->flags & SWP_SYNCHRONOUS_IO &&
+ 				__swap_count(si, entry) == 1) {
+ 			/* skip swapcache */
+ 			page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
+ 			if (page) {
+ 				__SetPageLocked(page);
+ 				__SetPageSwapBacked(page);
+ 				set_page_private(page, entry.val);
+ 				lru_cache_add_anon(page);
+ 				swap_readpage(page, true);
+ 			}
+ 		} else {
+ 			if (swap_use_vma_readahead())
+ 				page = do_swap_page_readahead(entry,
+ 					GFP_HIGHUSER_MOVABLE, vmf);
+ 			else
+ 				page = swapin_readahead(entry,
+ 				       GFP_HIGHUSER_MOVABLE, vma, vmf->address);
+ 			swapcache = page;
+ 		}
+ 
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  		if (!page) {
  			/*
  			 * Back out if somebody else faulted in this pte
diff --cc mm/swap_state.c
index 9df3c3db4075,db5da2baafb1..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -324,23 -328,34 +324,37 @@@ void free_pages_and_swap_cache(struct p
   * lock getting page table operations atomic even if we drop the page
   * lock before returning.
   */
 -struct page *lookup_swap_cache(swp_entry_t entry, struct vm_area_struct *vma,
 -			       unsigned long addr)
 +struct page * lookup_swap_cache(swp_entry_t entry, struct vm_area_struct *vma,
 +				unsigned long addr)
  {
  	struct page *page;
- 	unsigned long ra_info;
- 	int win, hits, readahead;
  
 -	page = find_get_page(swap_address_space(entry), swp_offset(entry));
 +	page = find_get_page(swap_address_space(entry), entry.val);
  
  	INC_CACHE_INFO(find_total);
  	if (page) {
+ 		bool vma_ra = swap_use_vma_readahead();
+ 		bool readahead;
+ 
  		INC_CACHE_INFO(find_success);
++<<<<<<< HEAD
++=======
+ 		/*
+ 		 * At the moment, we don't support PG_readahead for anon THP
+ 		 * so let's bail out rather than confusing the readahead stat.
+ 		 */
+ 		if (unlikely(PageTransCompound(page)))
+ 			return page;
+ 
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  		readahead = TestClearPageReadahead(page);
- 		if (vma) {
- 			ra_info = GET_SWAP_RA_VAL(vma);
- 			win = SWAP_RA_WIN(ra_info);
- 			hits = SWAP_RA_HITS(ra_info);
+ 		if (vma && vma_ra) {
+ 			unsigned long ra_val;
+ 			int win, hits;
+ 
+ 			ra_val = GET_SWAP_RA_VAL(vma);
+ 			win = SWAP_RA_WIN(ra_val);
+ 			hits = SWAP_RA_HITS(ra_val);
  			if (readahead)
  				hits = min_t(int, hits + 1, SWAP_RA_HITS_MAX);
  			atomic_long_set(&vma->swap_readahead_info,
@@@ -590,7 -596,7 +605,11 @@@ struct page *swapin_readahead(swp_entry
  		if (!page)
  			continue;
  		if (page_allocated) {
++<<<<<<< HEAD
 +			swap_readpage(page);
++=======
+ 			swap_readpage(page, false);
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  			if (offset != entry_offset) {
  				SetPageReadahead(page);
  				count_vm_event(SWAP_RA);
@@@ -656,13 -659,11 +675,21 @@@ static inline void swap_ra_clamp_pfn(st
  		    PFN_DOWN((faddr & PMD_MASK) + PMD_SIZE));
  }
  
++<<<<<<< HEAD
 +struct page *swap_readahead_detect(struct vm_area_struct *vma,
 +				   struct vma_swap_readahead *swap_ra,
 +				   pte_t *page_table, pte_t orig_pte,
 +				   unsigned long address)
 +{
 +	unsigned long swap_ra_info;
 +	struct page *page;
++=======
+ static void swap_ra_info(struct vm_fault *vmf,
+ 			struct vma_swap_readahead *ra_info)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	unsigned long ra_val;
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  	swp_entry_t entry;
  	unsigned long faddr, pfn, fpfn;
  	unsigned long start, end;
@@@ -675,24 -676,24 +702,34 @@@
  	max_win = 1 << min_t(unsigned int, READ_ONCE(page_cluster),
  			     SWAP_RA_ORDER_CEILING);
  	if (max_win == 1) {
- 		swap_ra->win = 1;
- 		return NULL;
+ 		ra_info->win = 1;
+ 		return;
  	}
  
++<<<<<<< HEAD
 +	faddr = address;
 +	entry = pte_to_swp_entry(orig_pte);
 +	if ((unlikely(non_swap_entry(entry))))
 +		return NULL;
 +	page = lookup_swap_cache(entry, vma, faddr);
 +	if (page)
 +		return page;
++=======
+ 	faddr = vmf->address;
+ 	orig_pte = pte = pte_offset_map(vmf->pmd, faddr);
+ 	entry = pte_to_swp_entry(*pte);
+ 	if ((unlikely(non_swap_entry(entry)))) {
+ 		pte_unmap(orig_pte);
+ 		return;
+ 	}
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  
  	fpfn = PFN_DOWN(faddr);
- 	swap_ra_info = GET_SWAP_RA_VAL(vma);
- 	pfn = PFN_DOWN(SWAP_RA_ADDR(swap_ra_info));
- 	prev_win = SWAP_RA_WIN(swap_ra_info);
- 	hits = SWAP_RA_HITS(swap_ra_info);
- 	swap_ra->win = win = __swapin_nr_pages(pfn, fpfn, hits,
+ 	ra_val = GET_SWAP_RA_VAL(vma);
+ 	pfn = PFN_DOWN(SWAP_RA_ADDR(ra_val));
+ 	prev_win = SWAP_RA_WIN(ra_val);
+ 	hits = SWAP_RA_HITS(ra_val);
+ 	ra_info->win = win = __swapin_nr_pages(pfn, fpfn, hits,
  					       max_win, prev_win);
  	atomic_long_set(&vma->swap_readahead_info,
  			SWAP_RA_VAL(faddr, win, 0));
@@@ -711,13 -714,13 +750,19 @@@
  		swap_ra_clamp_pfn(vma, faddr, fpfn - left, fpfn + win - left,
  				  &start, &end);
  	}
++<<<<<<< HEAD
 +	swap_ra->nr_pte = end - start;
 +	swap_ra->offset = fpfn - start;
 +	pte = page_table - swap_ra->offset;
++=======
+ 	ra_info->nr_pte = end - start;
+ 	ra_info->offset = fpfn - start;
+ 	pte -= ra_info->offset;
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  #ifdef CONFIG_64BIT
- 	swap_ra->ptes = pte;
+ 	ra_info->ptes = pte;
  #else
- 	tpte = swap_ra->ptes;
+ 	tpte = ra_info->ptes;
  	for (pfn = start; pfn != end; pfn++)
  		*tpte++ = *pte++;
  #endif
@@@ -726,11 -728,10 +770,15 @@@
  }
  
  struct page *do_swap_page_readahead(swp_entry_t fentry, gfp_t gfp_mask,
++<<<<<<< HEAD
 +				    struct vm_area_struct *vma,
 +				    unsigned long address,
 +				    struct vma_swap_readahead *swap_ra)
++=======
+ 				    struct vm_fault *vmf)
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  {
  	struct blk_plug plug;
 -	struct vm_area_struct *vma = vmf->vma;
  	struct page *page;
  	pte_t *pte, pentry;
  	swp_entry_t entry;
@@@ -756,8 -759,8 +806,13 @@@
  		if (!page)
  			continue;
  		if (page_allocated) {
++<<<<<<< HEAD
 +			swap_readpage(page);
 +			if (i != swap_ra->offset) {
++=======
+ 			swap_readpage(page, false);
+ 			if (i != ra_info.offset) {
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  				SetPageReadahead(page);
  				count_vm_event(SWAP_RA);
  			}
@@@ -767,7 -770,8 +822,12 @@@
  	blk_finish_plug(&plug);
  	lru_add_drain();
  skip:
++<<<<<<< HEAD
 +	return read_swap_cache_async(fentry, gfp_mask, vma, address);
++=======
+ 	return read_swap_cache_async(fentry, gfp_mask, vma, vmf->address,
+ 				     ra_info.win == 1);
++>>>>>>> eaf649ebc3ac (mm: swap: clean up swap readahead)
  }
  
  #ifdef CONFIG_SYSFS
* Unmerged path include/linux/swap.h
* Unmerged path mm/memory.c
* Unmerged path mm/swap_state.c
