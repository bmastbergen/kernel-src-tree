sched: Reduce contention in update_cfs_rq_blocked_load()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Jason Low <jason.low2@hp.com>
commit 8236d907ab3411ad452280faa8b26c1347327380
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/8236d907.failed

When running workloads on 2+ socket systems, based on perf profiles, the
update_cfs_rq_blocked_load() function often shows up as taking up a
noticeable % of run time.

Much of the contention is in __update_cfs_rq_tg_load_contrib() when we
update the tg load contribution stats.  However, it turns out that in many
cases, they don't need to be updated and "tg_contrib" is 0.

This patch adds a check in __update_cfs_rq_tg_load_contrib() to skip updating
tg load contribution stats when nothing needs to be updated. This reduces the
cacheline contention that would be unnecessary.

	Reviewed-by: Ben Segall <bsegall@google.com>
	Reviewed-by: Waiman Long <Waiman.Long@hp.com>
	Signed-off-by: Jason Low <jason.low2@hp.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Paul Turner <pjt@google.com>
	Cc: jason.low2@hp.com
	Cc: Yuyang Du <yuyang.du@intel.com>
	Cc: Aswin Chandramouleeswaran <aswin@hp.com>
	Cc: Chegu Vinod <chegu_vinod@hp.com>
	Cc: Scott J Norton <scott.norton@hp.com>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/1409643684.19197.15.camel@j-VirtualBox
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 8236d907ab3411ad452280faa8b26c1347327380)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index f3df0d8beb03,be9e97b0d76f..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -2389,8 -2382,11 +2389,16 @@@ static inline void __update_cfs_rq_tg_l
  	tg_contrib = cfs_rq->runnable_load_avg + cfs_rq->blocked_load_avg;
  	tg_contrib -= cfs_rq->tg_load_contrib;
  
++<<<<<<< HEAD
 +	if (force_update || abs64(tg_contrib) > cfs_rq->tg_load_contrib / 8) {
 +		atomic64_add(tg_contrib, &tg->load_avg);
++=======
+ 	if (!tg_contrib)
+ 		return;
+ 
+ 	if (force_update || abs(tg_contrib) > cfs_rq->tg_load_contrib / 8) {
+ 		atomic_long_add(tg_contrib, &tg->load_avg);
++>>>>>>> 8236d907ab34 (sched: Reduce contention in update_cfs_rq_blocked_load())
  		cfs_rq->tg_load_contrib += tg_contrib;
  	}
  }
* Unmerged path kernel/sched/fair.c
