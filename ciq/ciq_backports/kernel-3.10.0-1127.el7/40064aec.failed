percpu: replace area map allocator with bitmap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 40064aeca35c5c14789e2adcf3a1d7e5d4bd65f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/40064aec.failed

The percpu memory allocator is experiencing scalability issues when
allocating and freeing large numbers of counters as in BPF.
Additionally, there is a corner case where iteration is triggered over
all chunks if the contig_hint is the right size, but wrong alignment.

This patch replaces the area map allocator with a basic bitmap allocator
implementation. Each subsequent patch will introduce new features and
replace full scanning functions with faster non-scanning options when
possible.

Implementation:
This patchset removes the area map allocator in favor of a bitmap
allocator backed by metadata blocks. The primary goal is to provide
consistency in performance and memory footprint with a focus on small
allocations (< 64 bytes). The bitmap removes the heavy memmove from the
freeing critical path and provides a consistent memory footprint. The
metadata blocks provide a bound on the amount of scanning required by
maintaining a set of hints.

In an effort to make freeing fast, the metadata is updated on the free
path if the new free area makes a page free, a block free, or spans
across blocks. This causes the chunk's contig hint to potentially be
smaller than what it could allocate by up to the smaller of a page or a
block. If the chunk's contig hint is contained within a block, a check
occurs and the hint is kept accurate. Metadata is always kept accurate
on allocation, so there will not be a situation where a chunk has a
later contig hint than available.

Evaluation:
I have primarily done testing against a simple workload of allocation of
1 million objects (2^20) of varying size. Deallocation was done by in
order, alternating, and in reverse. These numbers were collected after
rebasing ontop of a80099a152. I present the worst-case numbers here:

  Area Map Allocator:

        Object Size | Alloc Time (ms) | Free Time (ms)
        ----------------------------------------------
              4B    |        310      |     4770
             16B    |        557      |     1325
             64B    |        436      |      273
            256B    |        776      |      131
           1024B    |       3280      |      122

  Bitmap Allocator:

        Object Size | Alloc Time (ms) | Free Time (ms)
        ----------------------------------------------
              4B    |        490      |       70
             16B    |        515      |       75
             64B    |        610      |       80
            256B    |        950      |      100
           1024B    |       3520      |      200

This data demonstrates the inability for the area map allocator to
handle less than ideal situations. In the best case of reverse
deallocation, the area map allocator was able to perform within range
of the bitmap allocator. In the worst case situation, freeing took
nearly 5 seconds for 1 million 4-byte objects. The bitmap allocator
dramatically improves the consistency of the free path. The small
allocations performed nearly identical regardless of the freeing
pattern.

While it does add to the allocation latency, the allocation scenario
here is optimal for the area map allocator. The area map allocator runs
into trouble when it is allocating in chunks where the latter half is
full. It is difficult to replicate this, so I present a variant where
the pages are second half filled. Freeing was done sequentially. Below
are the numbers for this scenario:

  Area Map Allocator:

        Object Size | Alloc Time (ms) | Free Time (ms)
        ----------------------------------------------
              4B    |       4118      |     4892
             16B    |       1651      |     1163
             64B    |        598      |      285
            256B    |        771      |      158
           1024B    |       3034      |      160

  Bitmap Allocator:

        Object Size | Alloc Time (ms) | Free Time (ms)
        ----------------------------------------------
              4B    |        481      |       67
             16B    |        506      |       69
             64B    |        636      |       75
            256B    |        892      |       90
           1024B    |       3262      |      147

The data shows a parabolic curve of performance for the area map
allocator. This is due to the memmove operation being the dominant cost
with the lower object sizes as more objects are packed in a chunk and at
higher object sizes, the traversal of the chunk slots is the dominating
cost. The bitmap allocator suffers this problem as well. The above data
shows the inability to scale for the allocation path with the area map
allocator and that the bitmap allocator demonstrates consistent
performance in general.

The second problem of additional scanning can result in the area map
allocator completing in 52 minutes when trying to allocate 1 million
4-byte objects with 8-byte alignment. The same workload takes
approximately 16 seconds to complete for the bitmap allocator.

V2:
Fixed a bug in pcpu_alloc_first_chunk end_offset was setting the bitmap
using bytes instead of bits.

Added a comment to pcpu_cnt_pop_pages to explain bitmap_weight.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 40064aeca35c5c14789e2adcf3a1d7e5d4bd65f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu-internal.h
#	mm/percpu-stats.c
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,986d900e6680..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -74,10 -81,14 +74,21 @@@
  #include <asm/tlbflush.h>
  #include <asm/io.h>
  
++<<<<<<< HEAD
 +#define PCPU_SLOT_BASE_SHIFT		5	/* 1-31 shares the same slot */
 +#define PCPU_DFL_MAP_ALLOC		16	/* start a map with 16 ents */
 +#define PCPU_ATOMIC_MAP_MARGIN_LOW	32
 +#define PCPU_ATOMIC_MAP_MARGIN_HIGH	64
++=======
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/percpu.h>
+ 
+ #include "percpu-internal.h"
+ 
+ /* the slots are sorted by free bytes left, 1-31 bytes share the same slot */
+ #define PCPU_SLOT_BASE_SHIFT		5
+ 
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  #define PCPU_EMPTY_POP_PAGES_LOW	2
  #define PCPU_EMPTY_POP_PAGES_HIGH	4
  
@@@ -312,46 -310,11 +323,14 @@@ static void *pcpu_mem_zalloc(size_t siz
   *
   * Free @ptr.  @ptr should have been allocated using pcpu_mem_zalloc().
   */
 -static void pcpu_mem_free(void *ptr)
 +static void pcpu_mem_free(void *ptr, size_t size)
  {
 -	kvfree(ptr);
 +	if (size <= PAGE_SIZE)
 +		kfree(ptr);
 +	else
 +		vfree(ptr);
  }
  
- /**
-  * pcpu_count_occupied_pages - count the number of pages an area occupies
-  * @chunk: chunk of interest
-  * @i: index of the area in question
-  *
-  * Count the number of pages chunk's @i'th area occupies.  When the area's
-  * start and/or end address isn't aligned to page boundary, the straddled
-  * page is included in the count iff the rest of the page is free.
-  */
- static int pcpu_count_occupied_pages(struct pcpu_chunk *chunk, int i)
- {
- 	int off = chunk->map[i] & ~1;
- 	int end = chunk->map[i + 1] & ~1;
- 
- 	if (!PAGE_ALIGNED(off) && i > 0) {
- 		int prev = chunk->map[i - 1];
- 
- 		if (!(prev & 1) && prev <= round_down(off, PAGE_SIZE))
- 			off = round_down(off, PAGE_SIZE);
- 	}
- 
- 	if (!PAGE_ALIGNED(end) && i + 1 < chunk->map_used) {
- 		int next = chunk->map[i + 1];
- 		int nend = chunk->map[i + 2] & ~1;
- 
- 		if (!(next & 1) && nend >= round_up(end, PAGE_SIZE))
- 			end = round_up(end, PAGE_SIZE);
- 	}
- 
- 	return max_t(int, PFN_DOWN(end) - PFN_UP(off), 0);
- }
- 
  /**
   * pcpu_chunk_relocate - put chunk in the appropriate chunk slot
   * @chunk: chunk of interest
@@@ -428,298 -431,244 +447,390 @@@ static void pcpu_chunk_refresh_hint(str
  }
  
  /**
-  * pcpu_extend_area_map - extend area map of a chunk
+  * pcpu_is_populated - determines if the region is populated
   * @chunk: chunk of interest
-  * @new_alloc: new target allocation length of the area map
+  * @bit_off: chunk offset
+  * @bits: size of area
+  * @next_off: return value for the next offset to start searching
   *
-  * Extend area map of @chunk to have @new_alloc entries.
+  * For atomic allocations, check if the backing pages are populated.
   *
-  * CONTEXT:
-  * Does GFP_KERNEL allocation.  Grabs and releases pcpu_lock.
+  * RETURNS:
+  * Bool if the backing pages are populated.
+  * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
+  */
+ static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
+ 			      int *next_off)
+ {
+ 	int page_start, page_end, rs, re;
+ 
+ 	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
+ 	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
+ 
+ 	rs = page_start;
+ 	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
+ 	if (rs >= page_end)
+ 		return true;
+ 
+ 	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
+ 	return false;
+ }
+ 
+ /**
+  * pcpu_find_block_fit - finds the block index to start searching
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE bytes)
+  * @pop_only: use populated regions only
   *
   * RETURNS:
-  * 0 on success, -errno on failure.
+  * The offset in the bitmap to begin searching.
+  * -1 if no offset is found.
   */
- static int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)
+ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
+ 			       size_t align, bool pop_only)
  {
- 	int *old = NULL, *new = NULL;
- 	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
- 	unsigned long flags;
+ 	int bit_off, bits;
+ 	int re; /* region end */
  
- 	lockdep_assert_held(&pcpu_alloc_mutex);
+ 	pcpu_for_each_unpop_region(chunk->alloc_map, bit_off, re, 0,
+ 				   pcpu_chunk_map_bits(chunk)) {
+ 		bits = re - bit_off;
  
- 	new = pcpu_mem_zalloc(new_size);
- 	if (!new)
- 		return -ENOMEM;
+ 		/* check alignment */
+ 		bits -= ALIGN(bit_off, align) - bit_off;
+ 		bit_off = ALIGN(bit_off, align);
+ 		if (bits < alloc_bits)
+ 			continue;
  
- 	/* acquire pcpu_lock and switch to new area map */
- 	spin_lock_irqsave(&pcpu_lock, flags);
+ 		bits = alloc_bits;
+ 		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
+ 						   &bit_off))
+ 			break;
  
- 	if (new_alloc <= chunk->map_alloc)
- 		goto out_unlock;
+ 		bits = 0;
+ 	}
  
- 	old_size = chunk->map_alloc * sizeof(chunk->map[0]);
- 	old = chunk->map;
+ 	if (bit_off == pcpu_chunk_map_bits(chunk))
+ 		return -1;
  
- 	memcpy(new, old, old_size);
+ 	return bit_off;
+ }
  
- 	chunk->map_alloc = new_alloc;
- 	chunk->map = new;
- 	new = NULL;
+ /**
+  * pcpu_alloc_area - allocates an area from a pcpu_chunk
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE)
+  * @start: bit_off to start searching
+  *
+  * This function takes in a @start offset to begin searching to fit an
+  * allocation of @alloc_bits with alignment @align.  If it confirms a
+  * valid free area, it then updates the allocation and boundary maps
+  * accordingly.
+  *
+  * RETURNS:
+  * Allocated addr offset in @chunk on success.
+  * -1 if no matching area is found.
+  */
+ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
+ 			   size_t align, int start)
+ {
+ 	size_t align_mask = (align) ? (align - 1) : 0;
+ 	int bit_off, end, oslot;
  
- out_unlock:
- 	spin_unlock_irqrestore(&pcpu_lock, flags);
+ 	lockdep_assert_held(&pcpu_lock);
+ 
+ 	oslot = pcpu_chunk_slot(chunk);
  
  	/*
- 	 * pcpu_mem_free() might end up calling vfree() which uses
- 	 * IRQ-unsafe lock and thus can't be called under pcpu_lock.
+ 	 * Search to find a fit.
  	 */
++<<<<<<< HEAD
 +	pcpu_mem_free(old, old_size);
 +	pcpu_mem_free(new, new_size);
++=======
+ 	end = start + alloc_bits;
+ 	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
+ 					     alloc_bits, align_mask);
+ 	if (bit_off >= end)
+ 		return -1;
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  
- 	return 0;
- }
+ 	/* update alloc map */
+ 	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
  
- /**
-  * pcpu_fit_in_area - try to fit the requested allocation in a candidate area
-  * @chunk: chunk the candidate area belongs to
-  * @off: the offset to the start of the candidate area
-  * @this_size: the size of the candidate area
-  * @size: the size of the target allocation
-  * @align: the alignment of the target allocation
-  * @pop_only: only allocate from already populated region
-  *
-  * We're trying to allocate @size bytes aligned at @align.  @chunk's area
-  * at @off sized @this_size is a candidate.  This function determines
-  * whether the target allocation fits in the candidate area and returns the
-  * number of bytes to pad after @off.  If the target area doesn't fit, -1
-  * is returned.
-  *
-  * If @pop_only is %true, this function only considers the already
-  * populated part of the candidate area.
-  */
- static int pcpu_fit_in_area(struct pcpu_chunk *chunk, int off, int this_size,
- 			    int size, int align, bool pop_only)
- {
- 	int cand_off = off;
+ 	/* update boundary map */
+ 	set_bit(bit_off, chunk->bound_map);
+ 	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
+ 	set_bit(bit_off + alloc_bits, chunk->bound_map);
  
- 	while (true) {
- 		int head = ALIGN(cand_off, align) - off;
- 		int page_start, page_end, rs, re;
+ 	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
  
- 		if (this_size < head + size)
- 			return -1;
+ 	pcpu_chunk_refresh_hint(chunk);
  
++<<<<<<< HEAD
 +		if (!pop_only)
 +			return head;
 +
 +		/*
 +		 * If the first unpopulated page is beyond the end of the
 +		 * allocation, the whole allocation is populated;
 +		 * otherwise, retry from the end of the unpopulated area.
 +		 */
 +		page_start = PFN_DOWN(head + off);
 +		page_end = PFN_UP(head + off + size);
 +
 +		rs = page_start;
 +		pcpu_next_unpop(chunk, &rs, &re, PFN_UP(off + this_size));
 +		if (rs >= page_end)
 +			return head;
 +		cand_off = re * PAGE_SIZE;
 +	}
 +}
 +
 +/**
 + * pcpu_alloc_area - allocate area from a pcpu_chunk
 + * @chunk: chunk of interest
 + * @size: wanted size in bytes
 + * @align: wanted align
 + * @pop_only: allocate only from the populated area
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Try to allocate @size bytes area aligned at @align from @chunk.
 + * Note that this function only allocates the offset.  It doesn't
 + * populate or map the area.
 + *
 + * @chunk->map must have at least two free slots.
 + *
 + * CONTEXT:
 + * pcpu_lock.
 + *
 + * RETURNS:
 + * Allocated offset in @chunk on success, -1 if no matching area is
 + * found.
 + */
 +static int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align,
 +			   bool pop_only, int *occ_pages_p)
 +{
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int max_contig = 0;
 +	int i, off;
 +	bool seen_free = false;
 +	int *p;
 +
 +	for (i = chunk->first_free, p = chunk->map + i; i < chunk->map_used; i++, p++) {
 +		int head, tail;
 +		int this_size;
 +
 +		off = *p;
 +		if (off & 1)
 +			continue;
 +
 +		this_size = (p[1] & ~1) - off;
 +
 +		head = pcpu_fit_in_area(chunk, off, this_size, size, align,
 +					pop_only);
 +		if (head < 0) {
 +			if (!seen_free) {
 +				chunk->first_free = i;
 +				seen_free = true;
 +			}
 +			max_contig = max(this_size, max_contig);
 +			continue;
 +		}
 +
 +		/*
 +		 * If head is small or the previous block is free,
 +		 * merge'em.  Note that 'small' is defined as smaller
 +		 * than sizeof(int), which is very small but isn't too
 +		 * uncommon for percpu allocations.
 +		 */
 +		if (head && (head < sizeof(int) || !(p[-1] & 1))) {
 +			*p = off += head;
 +			if (p[-1] & 1)
 +				chunk->free_size -= head;
 +			else
 +				max_contig = max(*p - p[-1], max_contig);
 +			this_size -= head;
 +			head = 0;
 +		}
 +
 +		/* if tail is small, just keep it around */
 +		tail = this_size - head - size;
 +		if (tail < sizeof(int)) {
 +			tail = 0;
 +			size = this_size - head;
 +		}
 +
 +		/* split if warranted */
 +		if (head || tail) {
 +			int nr_extra = !!head + !!tail;
 +
 +			/* insert new subblocks */
 +			memmove(p + nr_extra + 1, p + 1,
 +				sizeof(chunk->map[0]) * (chunk->map_used - i));
 +			chunk->map_used += nr_extra;
 +
 +			if (head) {
 +				if (!seen_free) {
 +					chunk->first_free = i;
 +					seen_free = true;
 +				}
 +				*++p = off += head;
 +				++i;
 +				max_contig = max(head, max_contig);
 +			}
 +			if (tail) {
 +				p[1] = off + size;
 +				max_contig = max(tail, max_contig);
 +			}
 +		}
 +
 +		if (!seen_free)
 +			chunk->first_free = i + 1;
 +
 +		/* update hint and mark allocated */
 +		if (i + 1 == chunk->map_used)
 +			chunk->contig_hint = max_contig; /* fully scanned */
 +		else
 +			chunk->contig_hint = max(chunk->contig_hint,
 +						 max_contig);
 +
 +		chunk->free_size -= size;
 +		*p |= 1;
 +
 +		*occ_pages_p = pcpu_count_occupied_pages(chunk, i);
 +		pcpu_chunk_relocate(chunk, oslot);
 +		return off;
 +	}
 +
 +	chunk->contig_hint = max_contig;	/* fully scanned */
++=======
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  	pcpu_chunk_relocate(chunk, oslot);
  
- 	/* tell the upper layer that this chunk has no matching area */
- 	return -1;
+ 	return bit_off * PCPU_MIN_ALLOC_SIZE;
  }
  
  /**
-  * pcpu_free_area - free area to a pcpu_chunk
+  * pcpu_free_area - frees the corresponding offset
   * @chunk: chunk of interest
-  * @freeme: offset of area to free
-  * @occ_pages_p: out param for the number of pages the area occupies
-  *
-  * Free area starting from @freeme to @chunk.  Note that this function
-  * only modifies the allocation map.  It doesn't depopulate or unmap
-  * the area.
+  * @off: addr offset into chunk
   *
-  * CONTEXT:
-  * pcpu_lock.
+  * This function determines the size of an allocation to free using
+  * the boundary bitmap and clears the allocation map.
   */
- static void pcpu_free_area(struct pcpu_chunk *chunk, int freeme,
- 			   int *occ_pages_p)
+ static void pcpu_free_area(struct pcpu_chunk *chunk, int off)
  {
- 	int oslot = pcpu_chunk_slot(chunk);
- 	int off = 0;
- 	unsigned i, j;
- 	int to_free = 0;
- 	int *p;
+ 	int bit_off, bits, end, oslot;
  
  	lockdep_assert_held(&pcpu_lock);
 -	pcpu_stats_area_dealloc(chunk);
  
- 	freeme |= 1;	/* we are searching for <given offset, in use> pair */
- 
- 	i = 0;
- 	j = chunk->map_used;
- 	while (i != j) {
- 		unsigned k = (i + j) / 2;
- 		off = chunk->map[k];
- 		if (off < freeme)
- 			i = k + 1;
- 		else if (off > freeme)
- 			j = k;
- 		else
- 			i = j = k;
- 	}
- 	BUG_ON(off != freeme);
+ 	oslot = pcpu_chunk_slot(chunk);
+ 
+ 	bit_off = off / PCPU_MIN_ALLOC_SIZE;
+ 
+ 	/* find end index */
+ 	end = find_next_bit(chunk->bound_map, pcpu_chunk_map_bits(chunk),
+ 			    bit_off + 1);
+ 	bits = end - bit_off;
+ 	bitmap_clear(chunk->alloc_map, bit_off, bits);
+ 
+ 	/* update metadata */
+ 	chunk->free_bytes += bits * PCPU_MIN_ALLOC_SIZE;
+ 
+ 	pcpu_chunk_refresh_hint(chunk);
+ 
+ 	pcpu_chunk_relocate(chunk, oslot);
+ }
+ 
++<<<<<<< HEAD
++=======
+ /**
+  * pcpu_alloc_first_chunk - creates chunks that serve the first chunk
+  * @tmp_addr: the start of the region served
+  * @map_size: size of the region served
+  *
+  * This is responsible for creating the chunks that serve the first chunk.  The
+  * base_addr is page aligned down of @tmp_addr while the region end is page
+  * aligned up.  Offsets are kept track of to determine the region served. All
+  * this is done to appease the bitmap allocator in avoiding partial blocks.
+  *
+  * RETURNS:
+  * Chunk serving the region at @tmp_addr of @map_size.
+  */
+ static struct pcpu_chunk * __init pcpu_alloc_first_chunk(unsigned long tmp_addr,
+ 							 int map_size)
+ {
+ 	struct pcpu_chunk *chunk;
+ 	unsigned long aligned_addr;
+ 	int start_offset, offset_bits, region_size, region_bits;
  
- 	if (i < chunk->first_free)
- 		chunk->first_free = i;
+ 	/* region calculations */
+ 	aligned_addr = tmp_addr & PAGE_MASK;
  
- 	p = chunk->map + i;
- 	*p = off &= ~1;
- 	chunk->free_size += (p[1] & ~1) - off;
+ 	start_offset = tmp_addr - aligned_addr;
  
- 	*occ_pages_p = pcpu_count_occupied_pages(chunk, i);
+ 	region_size = PFN_ALIGN(start_offset + map_size);
  
- 	/* merge with next? */
- 	if (!(p[1] & 1))
- 		to_free++;
- 	/* merge with previous? */
- 	if (i > 0 && !(p[-1] & 1)) {
- 		to_free++;
- 		i--;
- 		p--;
+ 	/* allocate chunk */
+ 	chunk = memblock_virt_alloc(sizeof(struct pcpu_chunk) +
+ 				    BITS_TO_LONGS(region_size >> PAGE_SHIFT),
+ 				    0);
+ 
+ 	INIT_LIST_HEAD(&chunk->list);
+ 
+ 	chunk->base_addr = (void *)aligned_addr;
+ 	chunk->start_offset = start_offset;
+ 	chunk->end_offset = region_size - chunk->start_offset - map_size;
+ 
+ 	chunk->nr_pages = region_size >> PAGE_SHIFT;
+ 	region_bits = pcpu_chunk_map_bits(chunk);
+ 
+ 	chunk->alloc_map = memblock_virt_alloc(
+ 				BITS_TO_LONGS(region_bits) *
+ 				sizeof(chunk->alloc_map[0]), 0);
+ 	chunk->bound_map = memblock_virt_alloc(
+ 				BITS_TO_LONGS(region_bits + 1) *
+ 				sizeof(chunk->bound_map[0]), 0);
+ 
+ 	/* manage populated page bitmap */
+ 	chunk->immutable = true;
+ 	bitmap_fill(chunk->populated, chunk->nr_pages);
+ 	chunk->nr_populated = chunk->nr_pages;
+ 	chunk->nr_empty_pop_pages =
+ 		pcpu_cnt_pop_pages(chunk, start_offset / PCPU_MIN_ALLOC_SIZE,
+ 				   map_size / PCPU_MIN_ALLOC_SIZE);
+ 
+ 	chunk->contig_bits = map_size / PCPU_MIN_ALLOC_SIZE;
+ 	chunk->free_bytes = map_size;
+ 
+ 	if (chunk->start_offset) {
+ 		/* hide the beginning of the bitmap */
+ 		offset_bits = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;
+ 		bitmap_set(chunk->alloc_map, 0, offset_bits);
+ 		set_bit(0, chunk->bound_map);
+ 		set_bit(offset_bits, chunk->bound_map);
  	}
- 	if (to_free) {
- 		chunk->map_used -= to_free;
- 		memmove(p + 1, p + 1 + to_free,
- 			(chunk->map_used - i) * sizeof(chunk->map[0]));
+ 
+ 	if (chunk->end_offset) {
+ 		/* hide the end of the bitmap */
+ 		offset_bits = chunk->end_offset / PCPU_MIN_ALLOC_SIZE;
+ 		bitmap_set(chunk->alloc_map,
+ 			   pcpu_chunk_map_bits(chunk) - offset_bits,
+ 			   offset_bits);
+ 		set_bit((start_offset + map_size) / PCPU_MIN_ALLOC_SIZE,
+ 			chunk->bound_map);
+ 		set_bit(region_bits, chunk->bound_map);
  	}
  
- 	chunk->contig_hint = max(chunk->map[i + 1] - chunk->map[i] - 1, chunk->contig_hint);
- 	pcpu_chunk_relocate(chunk, oslot);
+ 	pcpu_chunk_refresh_hint(chunk);
+ 
+ 	return chunk;
  }
  
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  static struct pcpu_chunk *pcpu_alloc_chunk(void)
  {
  	struct pcpu_chunk *chunk;
@@@ -728,32 -678,41 +840,65 @@@
  	if (!chunk)
  		return NULL;
  
++<<<<<<< HEAD
 +	chunk->map = pcpu_mem_zalloc(PCPU_DFL_MAP_ALLOC *
 +						sizeof(chunk->map[0]));
 +	if (!chunk->map) {
 +		pcpu_mem_free(chunk, pcpu_chunk_struct_size);
 +		return NULL;
 +	}
 +
 +	chunk->map_alloc = PCPU_DFL_MAP_ALLOC;
 +	chunk->map[0] = 0;
 +	chunk->map[1] = pcpu_unit_size | 1;
 +	chunk->map_used = 1;
 +
 +	INIT_LIST_HEAD(&chunk->list);
 +	INIT_LIST_HEAD(&chunk->map_extend_list);
 +	chunk->free_size = pcpu_unit_size;
 +	chunk->contig_hint = pcpu_unit_size;
++=======
+ 	INIT_LIST_HEAD(&chunk->list);
+ 	chunk->nr_pages = pcpu_unit_pages;
+ 	region_bits = pcpu_chunk_map_bits(chunk);
+ 
+ 	chunk->alloc_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits) *
+ 					   sizeof(chunk->alloc_map[0]));
+ 	if (!chunk->alloc_map)
+ 		goto alloc_map_fail;
+ 
+ 	chunk->bound_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits + 1) *
+ 					   sizeof(chunk->bound_map[0]));
+ 	if (!chunk->bound_map)
+ 		goto bound_map_fail;
+ 
+ 	/* init metadata */
+ 	chunk->contig_bits = region_bits;
+ 	chunk->free_bytes = chunk->nr_pages * PAGE_SIZE;
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  
  	return chunk;
+ 
+ bound_map_fail:
+ 	pcpu_mem_free(chunk->alloc_map);
+ alloc_map_fail:
+ 	pcpu_mem_free(chunk);
+ 
+ 	return NULL;
  }
  
  static void pcpu_free_chunk(struct pcpu_chunk *chunk)
  {
  	if (!chunk)
  		return;
++<<<<<<< HEAD
 +	pcpu_mem_free(chunk->map, chunk->map_alloc * sizeof(chunk->map[0]));
 +	pcpu_mem_free(chunk, pcpu_chunk_struct_size);
++=======
+ 	pcpu_mem_free(chunk->bound_map);
+ 	pcpu_mem_free(chunk->alloc_map);
+ 	pcpu_mem_free(chunk);
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  }
  
  /**
@@@ -775,7 -738,11 +924,15 @@@ static void pcpu_chunk_populated(struc
  
  	bitmap_set(chunk->populated, page_start, nr);
  	chunk->nr_populated += nr;
++<<<<<<< HEAD
 +	pcpu_nr_empty_pop_pages += nr;
++=======
+ 
+ 	if (!for_alloc) {
+ 		chunk->nr_empty_pop_pages += nr;
+ 		pcpu_nr_empty_pop_pages += nr;
+ 	}
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  }
  
  /**
@@@ -879,23 -846,29 +1036,37 @@@ static void __percpu *pcpu_alloc(size_
  	static int warn_limit = 10;
  	struct pcpu_chunk *chunk;
  	const char *err;
++<<<<<<< HEAD
 +	int occ_pages = 0;
 +	int slot, off, new_alloc, cpu, ret;
++=======
+ 	bool is_atomic = (gfp & GFP_KERNEL) != GFP_KERNEL;
+ 	int slot, off, cpu, ret;
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  	unsigned long flags;
  	void __percpu *ptr;
+ 	size_t bits, bit_align;
  
  	/*
- 	 * We want the lowest bit of offset available for in-use/free
- 	 * indicator, so force >= 16bit alignment and make size even.
+ 	 * There is now a minimum allocation size of PCPU_MIN_ALLOC_SIZE,
+ 	 * therefore alignment must be a minimum of that many bytes.
+ 	 * An allocation may have internal fragmentation from rounding up
+ 	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.
  	 */
 -	if (unlikely(align < PCPU_MIN_ALLOC_SIZE))
 -		align = PCPU_MIN_ALLOC_SIZE;
 +	if (unlikely(align < 2))
 +		align = 2;
  
++<<<<<<< HEAD
 +	size = ALIGN(size, 2);
++=======
+ 	size = ALIGN(size, PCPU_MIN_ALLOC_SIZE);
+ 	bits = size >> PCPU_MIN_ALLOC_SHIFT;
+ 	bit_align = align >> PCPU_MIN_ALLOC_SHIFT;
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  
 -	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||
 -		     !is_power_of_2(align))) {
 -		WARN(true, "illegal size (%zu) or align (%zu) for percpu allocation\n",
 -		     size, align);
 +	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {
 +		WARN(do_warn, "illegal size (%zu) or align (%zu) for "
 +		     "percpu allocation\n", size, align);
  		return NULL;
  	}
  
@@@ -1555,11 -1502,9 +1671,17 @@@ static void pcpu_dump_alloc_info(const 
  int __init pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,
  				  void *base_addr)
  {
++<<<<<<< HEAD
 +	static int smap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
 +	static int dmap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
 +	size_t dyn_size = ai->dyn_size;
 +	size_t size_sum = ai->static_size + ai->reserved_size + dyn_size;
 +	struct pcpu_chunk *schunk, *dchunk = NULL;
++=======
+ 	size_t size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
+ 	size_t static_size, dyn_size;
+ 	struct pcpu_chunk *chunk;
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  	unsigned long *group_offsets;
  	size_t *group_sizes;
  	unsigned long *unit_off;
@@@ -1665,56 -1616,36 +1787,77 @@@
  		INIT_LIST_HEAD(&pcpu_slot[i]);
  
  	/*
 -	 * The end of the static region needs to be aligned with the
 -	 * minimum allocation size as this offsets the reserved and
 -	 * dynamic region.  The first chunk ends page aligned by
 -	 * expanding the dynamic region, therefore the dynamic region
 -	 * can be shrunk to compensate while still staying above the
 -	 * configured sizes.
 +	 * Initialize static chunk.  If reserved_size is zero, the
 +	 * static chunk covers static area + dynamic allocation area
 +	 * in the first chunk.  If reserved_size is not zero, it
 +	 * covers static area + reserved area (mostly used for module
 +	 * static percpu allocation).
  	 */
 -	static_size = ALIGN(ai->static_size, PCPU_MIN_ALLOC_SIZE);
 -	dyn_size = ai->dyn_size - (static_size - ai->static_size);
 -
 +	schunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +	INIT_LIST_HEAD(&schunk->list);
 +	INIT_LIST_HEAD(&schunk->map_extend_list);
 +	schunk->base_addr = base_addr;
 +	schunk->map = smap;
 +	schunk->map_alloc = ARRAY_SIZE(smap);
 +	schunk->immutable = true;
 +	bitmap_fill(schunk->populated, pcpu_unit_pages);
 +	schunk->nr_populated = pcpu_unit_pages;
 +
++<<<<<<< HEAD
 +	if (ai->reserved_size) {
 +		schunk->free_size = ai->reserved_size;
 +		pcpu_reserved_chunk = schunk;
 +		pcpu_reserved_chunk_limit = ai->static_size + ai->reserved_size;
 +	} else {
 +		schunk->free_size = dyn_size;
 +		dyn_size = 0;			/* dynamic area covered */
 +	}
 +	schunk->contig_hint = schunk->free_size;
 +
 +	schunk->map[0] = 1;
 +	schunk->map[1] = ai->static_size;
 +	schunk->map_used = 1;
 +	if (schunk->free_size)
 +		schunk->map[++schunk->map_used] = ai->static_size + schunk->free_size;
 +	schunk->map[schunk->map_used] |= 1;
++=======
+ 	/*
+ 	 * Initialize first chunk.
+ 	 * If the reserved_size is non-zero, this initializes the reserved
+ 	 * chunk.  If the reserved_size is zero, the reserved chunk is NULL
+ 	 * and the dynamic region is initialized here.  The first chunk,
+ 	 * pcpu_first_chunk, will always point to the chunk that serves
+ 	 * the dynamic region.
+ 	 */
+ 	tmp_addr = (unsigned long)base_addr + static_size;
+ 	map_size = ai->reserved_size ?: dyn_size;
+ 	chunk = pcpu_alloc_first_chunk(tmp_addr, map_size);
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  
  	/* init dynamic chunk if necessary */
 -	if (ai->reserved_size) {
 -		pcpu_reserved_chunk = chunk;
 -
 +	if (dyn_size) {
 +		dchunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +		INIT_LIST_HEAD(&dchunk->list);
 +		INIT_LIST_HEAD(&dchunk->map_extend_list);
 +		dchunk->base_addr = base_addr;
 +		dchunk->map = dmap;
 +		dchunk->map_alloc = ARRAY_SIZE(dmap);
 +		dchunk->immutable = true;
 +		bitmap_fill(dchunk->populated, pcpu_unit_pages);
 +		dchunk->nr_populated = pcpu_unit_pages;
 +
++<<<<<<< HEAD
 +		dchunk->contig_hint = dchunk->free_size = dyn_size;
 +		dchunk->map[0] = 1;
 +		dchunk->map[1] = pcpu_reserved_chunk_limit;
 +		dchunk->map[2] = (pcpu_reserved_chunk_limit + dchunk->free_size) | 1;
 +		dchunk->map_used = 2;
++=======
+ 		tmp_addr = (unsigned long)base_addr + static_size +
+ 			   ai->reserved_size;
+ 		map_size = dyn_size;
+ 		chunk = pcpu_alloc_first_chunk(tmp_addr, map_size);
++>>>>>>> 40064aeca35c (percpu: replace area map allocator with bitmap)
  	}
  
  	/* link the first chunk in */
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu-stats.c
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index 004298923ccf..e14f397a468b 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -157,7 +157,6 @@ extern bool is_kernel_percpu_address(unsigned long addr);
 #if !defined(CONFIG_SMP) || !defined(CONFIG_HAVE_SETUP_PER_CPU_AREA)
 extern void __init setup_per_cpu_areas(void);
 #endif
-extern void __init percpu_init_late(void);
 
 extern void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp);
 extern void __percpu *__alloc_percpu(size_t size, size_t align);
diff --git a/init/main.c b/init/main.c
index 8513d4a7bf82..93177dde857d 100644
--- a/init/main.c
+++ b/init/main.c
@@ -478,7 +478,6 @@ static void __init mm_init(void)
 	page_cgroup_init_flatmem();
 	mem_init();
 	kmem_cache_init();
-	percpu_init_late();
 	pgtable_init();
 	vmalloc_init();
 	ioremap_huge_init();
* Unmerged path mm/percpu-internal.h
diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 10e3d0b8a86d..ed709d115d99 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -69,7 +69,7 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
 
 	spin_lock_irq(&pcpu_lock);
-	pcpu_chunk_populated(chunk, 0, nr_pages);
+	pcpu_chunk_populated(chunk, 0, nr_pages, false);
 	spin_unlock_irq(&pcpu_lock);
 
 	return chunk;
* Unmerged path mm/percpu-stats.c
* Unmerged path mm/percpu.c
