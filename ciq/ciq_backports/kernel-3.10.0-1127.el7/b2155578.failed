dm snapshot: rework COW throttling to fix deadlock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit b21555786f18cd77f2311ad89074533109ae3ffa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/b2155578.failed

Commit 721b1d98fb517a ("dm snapshot: Fix excessive memory usage and
workqueue stalls") introduced a semaphore to limit the maximum number of
in-flight kcopyd (COW) jobs.

The implementation of this throttling mechanism is prone to a deadlock:

1. One or more threads write to the origin device causing COW, which is
   performed by kcopyd.

2. At some point some of these threads might reach the s->cow_count
   semaphore limit and block in down(&s->cow_count), holding a read lock
   on _origins_lock.

3. Someone tries to acquire a write lock on _origins_lock, e.g.,
   snapshot_ctr(), which blocks because the threads at step (2) already
   hold a read lock on it.

4. A COW operation completes and kcopyd runs dm-snapshot's completion
   callback, which ends up calling pending_complete().
   pending_complete() tries to resubmit any deferred origin bios. This
   requires acquiring a read lock on _origins_lock, which blocks.

   This happens because the read-write semaphore implementation gives
   priority to writers, meaning that as soon as a writer tries to enter
   the critical section, no readers will be allowed in, until all
   writers have completed their work.

   So, pending_complete() waits for the writer at step (3) to acquire
   and release the lock. This writer waits for the readers at step (2)
   to release the read lock and those readers wait for
   pending_complete() (the kcopyd thread) to signal the s->cow_count
   semaphore: DEADLOCK.

The above was thoroughly analyzed and documented by Nikos Tsironis as
part of his initial proposal for fixing this deadlock, see:
https://www.redhat.com/archives/dm-devel/2019-October/msg00001.html

Fix this deadlock by reworking COW throttling so that it waits without
holding any locks. Add a variable 'in_progress' that counts how many
kcopyd jobs are running. A function wait_for_in_progress() will sleep if
'in_progress' is over the limit. It drops _origins_lock in order to
avoid the deadlock.

	Reported-by: Guruswamy Basavaiah <guru2018@gmail.com>
	Reported-by: Nikos Tsironis <ntsironis@arrikto.com>
	Reviewed-by: Nikos Tsironis <ntsironis@arrikto.com>
	Tested-by: Nikos Tsironis <ntsironis@arrikto.com>
Fixes: 721b1d98fb51 ("dm snapshot: Fix excessive memory usage and workqueue stalls")
	Cc: stable@vger.kernel.org # v5.0+
Depends-on: 4a3f111a73a8c ("dm snapshot: introduce account_start_copy() and account_end_copy()")
	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit b21555786f18cd77f2311ad89074533109ae3ffa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-snap.c
diff --cc drivers/md/dm-snap.c
index 4f3251269638,4fb1a40e68a0..000000000000
--- a/drivers/md/dm-snap.c
+++ b/drivers/md/dm-snap.c
@@@ -1398,6 -1513,51 +1399,54 @@@ static void snapshot_dtr(struct dm_targ
  	kfree(s);
  }
  
++<<<<<<< HEAD
++=======
+ static void account_start_copy(struct dm_snapshot *s)
+ {
+ 	spin_lock(&s->in_progress_wait.lock);
+ 	s->in_progress++;
+ 	spin_unlock(&s->in_progress_wait.lock);
+ }
+ 
+ static void account_end_copy(struct dm_snapshot *s)
+ {
+ 	spin_lock(&s->in_progress_wait.lock);
+ 	BUG_ON(!s->in_progress);
+ 	s->in_progress--;
+ 	if (likely(s->in_progress <= cow_threshold) &&
+ 	    unlikely(waitqueue_active(&s->in_progress_wait)))
+ 		wake_up_locked(&s->in_progress_wait);
+ 	spin_unlock(&s->in_progress_wait.lock);
+ }
+ 
+ static bool wait_for_in_progress(struct dm_snapshot *s, bool unlock_origins)
+ {
+ 	if (unlikely(s->in_progress > cow_threshold)) {
+ 		spin_lock(&s->in_progress_wait.lock);
+ 		if (likely(s->in_progress > cow_threshold)) {
+ 			/*
+ 			 * NOTE: this throttle doesn't account for whether
+ 			 * the caller is servicing an IO that will trigger a COW
+ 			 * so excess throttling may result for chunks not required
+ 			 * to be COW'd.  But if cow_threshold was reached, extra
+ 			 * throttling is unlikely to negatively impact performance.
+ 			 */
+ 			DECLARE_WAITQUEUE(wait, current);
+ 			__add_wait_queue(&s->in_progress_wait, &wait);
+ 			__set_current_state(TASK_UNINTERRUPTIBLE);
+ 			spin_unlock(&s->in_progress_wait.lock);
+ 			if (unlock_origins)
+ 				up_read(&_origins_lock);
+ 			io_schedule();
+ 			remove_wait_queue(&s->in_progress_wait, &wait);
+ 			return false;
+ 		}
+ 		spin_unlock(&s->in_progress_wait.lock);
+ 	}
+ 	return true;
+ }
+ 
++>>>>>>> b21555786f18 (dm snapshot: rework COW throttling to fix deadlock)
  /*
   * Flush a list of buffers.
   */
@@@ -1748,12 -1960,19 +1797,22 @@@ static int snapshot_map(struct dm_targe
  	/* Full snapshots are not usable */
  	/* To get here the table must be live so s->active is always set. */
  	if (!s->valid)
 -		return DM_MAPIO_KILL;
 +		return -EIO;
  
++<<<<<<< HEAD
 +	mutex_lock(&s->lock);
++=======
+ 	if (bio_data_dir(bio) == WRITE) {
+ 		while (unlikely(!wait_for_in_progress(s, false)))
+ 			; /* wait_for_in_progress() has slept */
+ 	}
+ 
+ 	down_read(&s->lock);
+ 	dm_exception_table_lock(&lock);
++>>>>>>> b21555786f18 (dm snapshot: rework COW throttling to fix deadlock)
  
 -	if (!s->valid || (unlikely(s->snapshot_overflowed) &&
 -	    bio_data_dir(bio) == WRITE)) {
 -		r = DM_MAPIO_KILL;
 +	if (!s->valid || (unlikely(s->snapshot_overflowed) && bio_rw(bio) == WRITE)) {
 +		r = -EIO;
  		goto out_unlock;
  	}
  
@@@ -1892,11 -2159,11 +1951,17 @@@ static int snapshot_merge_map(struct dm
  	}
  
  redirect_to_origin:
 -	bio_set_dev(bio, s->origin->bdev);
 +	bio->bi_bdev = s->origin->bdev;
  
++<<<<<<< HEAD
 +	if (bio_rw(bio) == WRITE) {
 +		mutex_unlock(&s->lock);
 +		return do_origin(s->origin, bio);
++=======
+ 	if (bio_data_dir(bio) == WRITE) {
+ 		up_write(&s->lock);
+ 		return do_origin(s->origin, bio, false);
++>>>>>>> b21555786f18 (dm snapshot: rework COW throttling to fix deadlock)
  	}
  
  out_unlock:
@@@ -2244,10 -2543,19 +2309,24 @@@ static int do_origin(struct dm_dev *ori
  	struct origin *o;
  	int r = DM_MAPIO_REMAPPED;
  
+ again:
  	down_read(&_origins_lock);
  	o = __lookup_origin(origin->bdev);
++<<<<<<< HEAD
 +	if (o)
 +		r = __origin_write(&o->snapshots, bio->bi_sector, bio);
++=======
+ 	if (o) {
+ 		if (limit) {
+ 			struct dm_snapshot *s;
+ 			list_for_each_entry(s, &o->snapshots, list)
+ 				if (unlikely(!wait_for_in_progress(s, true)))
+ 					goto again;
+ 		}
+ 
+ 		r = __origin_write(&o->snapshots, bio->bi_iter.bi_sector, bio);
+ 	}
++>>>>>>> b21555786f18 (dm snapshot: rework COW throttling to fix deadlock)
  	up_read(&_origins_lock);
  
  	return r;
@@@ -2343,13 -2651,24 +2422,17 @@@ static void origin_dtr(struct dm_targe
  static int origin_map(struct dm_target *ti, struct bio *bio)
  {
  	struct dm_origin *o = ti->private;
 -	unsigned available_sectors;
 -
 -	bio_set_dev(bio, o->dev->bdev);
 +	bio->bi_bdev = o->dev->bdev;
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH))
 +	if (bio->bi_rw & REQ_FLUSH)
  		return DM_MAPIO_REMAPPED;
  
 -	if (bio_data_dir(bio) != WRITE)
 -		return DM_MAPIO_REMAPPED;
 -
 -	available_sectors = o->split_boundary -
 -		((unsigned)bio->bi_iter.bi_sector & (o->split_boundary - 1));
 -
 -	if (bio_sectors(bio) > available_sectors)
 -		dm_accept_partial_bio(bio, available_sectors);
 -
  	/* Only tell snapshots if this is a write */
++<<<<<<< HEAD
 +	return (bio_rw(bio) == WRITE) ? do_origin(o->dev, bio) : DM_MAPIO_REMAPPED;
++=======
+ 	return do_origin(o->dev, bio, true);
++>>>>>>> b21555786f18 (dm snapshot: rework COW throttling to fix deadlock)
  }
  
  /*
* Unmerged path drivers/md/dm-snap.c
