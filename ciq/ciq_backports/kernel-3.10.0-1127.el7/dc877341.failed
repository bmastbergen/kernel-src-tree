sched: Remove some #ifdeffery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit dc87734106bb6e97c92d8bd81f261fb71976ec2c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/dc877341.failed

Remove a few gratuitous #ifdefs in pick_next_task*().

	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Juri Lelli <juri.lelli@gmail.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/n/tip-nnzddp5c4fijyzzxxrwlxghf@git.kernel.org
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit dc87734106bb6e97c92d8bd81f261fb71976ec2c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/deadline.c
#	kernel/sched/idle_task.c
#	kernel/sched/rt.c
diff --cc kernel/sched/deadline.c
index 75f9634980d1,3185b775dbf7..000000000000
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@@ -434,55 -214,14 +434,66 @@@ static inline int has_pushable_dl_tasks
  
  static int push_dl_task(struct rq *rq);
  
++<<<<<<< HEAD
 +static struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq);
 +
 +static struct rq *dl_task_offline_migration(struct rq *rq, struct task_struct *p)
 +{
 +	struct rq *later_rq = NULL;
 +	bool fallback = false;
 +
 +	later_rq = find_lock_later_rq(p, rq);
 +
 +	if (!later_rq) {
 +		int cpu;
 +
 +		/*
 +		 * If we cannot preempt any rq, fall back to pick any
 +		 * online cpu.
 +		 */
 +		fallback = true;
 +		cpu = cpumask_any_and(cpu_active_mask, tsk_cpus_allowed(p));
 +		if (cpu >= nr_cpu_ids) {
 +			/*
 +			 * Fail to find any suitable cpu.
 +			 * The task will never come back!
 +			 */
 +			BUG_ON(dl_bandwidth_enabled());
 +
 +			/*
 +			 * If admission control is disabled we
 +			 * try a little harder to let the task
 +			 * run.
 +			 */
 +			cpu = cpumask_any(cpu_active_mask);
 +		}
 +		later_rq = cpu_rq(cpu);
 +		double_lock_balance(rq, later_rq);
 +	}
 +
 +	/*
 +	 * By now the task is replenished and enqueued; migrate it.
 +	 */
 +	deactivate_task(rq, p, 0);
 +	set_task_cpu(p, later_rq->cpu);
 +	activate_task(later_rq, p, 0);
 +
 +	if (!fallback)
 +		resched_curr(later_rq);
 +
 +	double_unlock_balance(later_rq, rq);
 +
 +	return later_rq;
++=======
+ static inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)
+ {
+ 	return dl_task(prev);
+ }
+ 
+ static inline void set_post_schedule(struct rq *rq)
+ {
+ 	rq->post_schedule = has_pushable_dl_tasks(rq);
++>>>>>>> dc87734106bb (sched: Remove some #ifdeffery)
  }
  
  #else
@@@ -1589,6 -1023,9 +1613,12 @@@ struct task_struct *pick_next_task_dl(s
  
  	dl_rq = &rq->dl;
  
++<<<<<<< HEAD
++=======
+ 	if (need_pull_dl_task(rq, prev))
+ 		pull_dl_task(rq);
+ 
++>>>>>>> dc87734106bb (sched: Remove some #ifdeffery)
  	if (unlikely(!dl_rq->dl_nr_running))
  		return NULL;
  
@@@ -1601,12 -1040,12 +1631,10 @@@
  	/* Running task will never be pushed. */
         dequeue_pushable_dl_task(rq, p);
  
 -#ifdef CONFIG_SCHED_HRTICK
  	if (hrtick_enabled(rq))
  		start_hrtick_dl(rq, p);
 -#endif
  
- #ifdef CONFIG_SMP
- 	rq->post_schedule = has_pushable_dl_tasks(rq);
- #endif /* CONFIG_SMP */
+ 	set_post_schedule(rq);
  
  	return p;
  }
diff --cc kernel/sched/idle_task.c
index 1a162e4ffb03,1f3725882838..000000000000
--- a/kernel/sched/idle_task.c
+++ b/kernel/sched/idle_task.c
@@@ -25,15 -20,16 +25,13 @@@ static void pre_schedule_idle(struct r
   */
  static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags)
  {
 -	resched_task(rq->idle);
 +	resched_curr(rq);
  }
  
 -static struct task_struct *
 -pick_next_task_idle(struct rq *rq, struct task_struct *prev)
 +static struct task_struct *pick_next_task_idle(struct rq *rq)
  {
 -	put_prev_task(rq, prev);
 -
  	schedstat_inc(rq, sched_goidle);
- #ifdef CONFIG_SMP
  	idle_enter_fair(rq);
- #endif
  	return rq->idle;
  }
  
@@@ -52,6 -48,8 +50,11 @@@ dequeue_task_idle(struct rq *rq, struc
  
  static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
  {
++<<<<<<< HEAD
++=======
+ 	idle_exit_fair(rq);
+ 	rq_last_tick_reset(rq);
++>>>>>>> dc87734106bb (sched: Remove some #ifdeffery)
  }
  
  static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued)
diff --cc kernel/sched/rt.c
index 6b68ceb9a68d,3e488ca6050d..000000000000
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@@ -229,6 -229,14 +229,17 @@@ int alloc_rt_sched_group(struct task_gr
  
  #ifdef CONFIG_SMP
  
++<<<<<<< HEAD
++=======
+ static int pull_rt_task(struct rq *this_rq);
+ 
+ static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)
+ {
+ 	/* Try to pull RT tasks here if we lower this rq's prio */
+ 	return rq->rt.highest_prio.curr > prev->prio;
+ }
+ 
++>>>>>>> dc87734106bb (sched: Remove some #ifdeffery)
  static inline int rt_overloaded(struct rq *rq)
  {
  	return atomic_read(&rq->rd->rto_count);
@@@ -1370,9 -1354,24 +1403,27 @@@ static struct task_struct *_pick_next_t
  	return p;
  }
  
 -static struct task_struct *
 -pick_next_task_rt(struct rq *rq, struct task_struct *prev)
 +static struct task_struct *pick_next_task_rt(struct rq *rq)
  {
++<<<<<<< HEAD
 +	struct task_struct *p = _pick_next_task_rt(rq);
++=======
+ 	struct task_struct *p;
+ 	struct rt_rq *rt_rq = &rq->rt;
+ 
+ 	if (need_pull_rt_task(rq, prev))
+ 		pull_rt_task(rq);
+ 
+ 	if (!rt_rq->rt_nr_running)
+ 		return NULL;
+ 
+ 	if (rt_rq_throttled(rt_rq))
+ 		return NULL;
+ 
+ 	put_prev_task(rq, prev);
+ 
+ 	p = _pick_next_task_rt(rq);
++>>>>>>> dc87734106bb (sched: Remove some #ifdeffery)
  
  	/* The running task is never eligible for pushing */
  	if (p)
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/idle_task.c
* Unmerged path kernel/sched/rt.c
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 38c9ae998b34..7fefd3251ab7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1337,6 +1337,11 @@ static inline void idle_balance(int cpu, struct rq *rq)
 {
 }
 
+#else
+
+static inline void idle_enter_fair(struct rq *rq) { }
+static inline void idle_exit_fair(struct rq *rq) { }
+
 #endif
 
 extern void sysrq_sched_debug_show(void);
