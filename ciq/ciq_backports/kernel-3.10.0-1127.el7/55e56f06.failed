dax: Don't access a freed inode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Matthew Wilcox <willy@infradead.org>
commit 55e56f06ed71d9441f3abd5b1d3c1a870812b3fe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/55e56f06.failed

After we drop the i_pages lock, the inode can be freed at any time.
The get_unlocked_entry() code has no choice but to reacquire the lock,
so it can't be used here.  Create a new wait_entry_unlocked() which takes
care not to acquire the lock or dereference the address_space in any way.

Fixes: c2a7d2a11552 ("filesystem-dax: Introduce dax_lock_mapping_entry()")
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Matthew Wilcox <willy@infradead.org>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 55e56f06ed71d9441f3abd5b1d3c1a870812b3fe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 97c3ab5e2b69,3f592dc18d67..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -279,27 -232,57 +279,59 @@@ static void *get_unlocked_mapping_entry
  	}
  }
  
++<<<<<<< HEAD
 +static void dax_unlock_mapping_entry(struct address_space *mapping,
 +				     pgoff_t index)
++=======
+ /*
+  * The only thing keeping the address space around is the i_pages lock
+  * (it's cycled in clear_inode() after removing the entries from i_pages)
+  * After we call xas_unlock_irq(), we cannot touch xas->xa.
+  */
+ static void wait_entry_unlocked(struct xa_state *xas, void *entry)
+ {
+ 	struct wait_exceptional_entry_queue ewait;
+ 	wait_queue_head_t *wq;
+ 
+ 	init_wait(&ewait.wait);
+ 	ewait.wait.func = wake_exceptional_entry_func;
+ 
+ 	wq = dax_entry_waitqueue(xas, entry, &ewait.key);
+ 	prepare_to_wait_exclusive(wq, &ewait.wait, TASK_UNINTERRUPTIBLE);
+ 	xas_unlock_irq(xas);
+ 	schedule();
+ 	finish_wait(wq, &ewait.wait);
+ 
+ 	/*
+ 	 * Entry lock waits are exclusive. Wake up the next waiter since
+ 	 * we aren't sure we will acquire the entry lock and thus wake
+ 	 * the next waiter up on unlock.
+ 	 */
+ 	if (waitqueue_active(wq))
+ 		__wake_up(wq, TASK_NORMAL, 1, &ewait.key);
+ }
+ 
+ static void put_unlocked_entry(struct xa_state *xas, void *entry)
++>>>>>>> 55e56f06ed71 (dax: Don't access a freed inode)
  {
 -	/* If we were the only waiter woken, wake the next one */
 -	if (entry)
 -		dax_wake_entry(xas, entry, false);
 +	void *entry, **slot;
 +
 +	spin_lock_irq(&mapping->tree_lock);
 +	entry = __radix_tree_lookup(&mapping->page_tree, index, NULL, &slot);
 +	if (WARN_ON_ONCE(!entry || !radix_tree_exceptional_entry(entry) ||
 +			 !slot_locked(mapping, slot))) {
 +		spin_unlock_irq(&mapping->tree_lock);
 +		return;
 +	}
 +	unlock_slot(mapping, slot);
 +	spin_unlock_irq(&mapping->tree_lock);
 +	dax_wake_mapping_entry_waiter(mapping, index, entry, false);
  }
  
 -/*
 - * We used the xa_state to get the entry, but then we locked the entry and
 - * dropped the xa_lock, so we know the xa_state is stale and must be reset
 - * before use.
 - */
 -static void dax_unlock_entry(struct xa_state *xas, void *entry)
 +static void put_locked_mapping_entry(struct address_space *mapping,
 +		pgoff_t index)
  {
 -	void *old;
 -
 -	BUG_ON(dax_is_locked(entry));
 -	xas_reset(xas);
 -	xas_lock_irq(xas);
 -	old = xas_store(xas, entry);
 -	xas_unlock_irq(xas);
 -	BUG_ON(!dax_is_locked(old));
 -	dax_wake_entry(xas, entry, false);
 +	dax_unlock_mapping_entry(mapping, index);
  }
  
  /*
@@@ -397,26 -375,96 +429,100 @@@ static struct page *dax_busy_page(void 
  }
  
  /*
 - * dax_lock_mapping_entry - Lock the DAX entry corresponding to a page
 - * @page: The page whose entry we want to lock
 + * Find radix tree entry at given index. If it points to an exceptional entry,
 + * return it with the radix tree entry locked. If the radix tree doesn't
 + * contain given index, create an empty exceptional entry for the index and
 + * return with it locked.
   *
++<<<<<<< HEAD
 + * When requesting an entry with size RADIX_DAX_PMD, grab_mapping_entry() will
 + * either return that locked entry or will return an error.  This error will
 + * happen if there are any 4k entries within the 2MiB range that we are
 + * requesting.
++=======
+  * Context: Process context.
+  * Return: %true if the entry was locked or does not need to be locked.
+  */
+ bool dax_lock_mapping_entry(struct page *page)
+ {
+ 	XA_STATE(xas, NULL, 0);
+ 	void *entry;
+ 	bool locked;
+ 
+ 	/* Ensure page->mapping isn't freed while we look at it */
+ 	rcu_read_lock();
+ 	for (;;) {
+ 		struct address_space *mapping = READ_ONCE(page->mapping);
+ 
+ 		locked = false;
+ 		if (!mapping || !dax_mapping(mapping))
+ 			break;
+ 
+ 		/*
+ 		 * In the device-dax case there's no need to lock, a
+ 		 * struct dev_pagemap pin is sufficient to keep the
+ 		 * inode alive, and we assume we have dev_pagemap pin
+ 		 * otherwise we would not have a valid pfn_to_page()
+ 		 * translation.
+ 		 */
+ 		locked = true;
+ 		if (S_ISCHR(mapping->host->i_mode))
+ 			break;
+ 
+ 		xas.xa = &mapping->i_pages;
+ 		xas_lock_irq(&xas);
+ 		if (mapping != page->mapping) {
+ 			xas_unlock_irq(&xas);
+ 			continue;
+ 		}
+ 		xas_set(&xas, page->index);
+ 		entry = xas_load(&xas);
+ 		if (dax_is_locked(entry)) {
+ 			rcu_read_unlock();
+ 			wait_entry_unlocked(&xas, entry);
+ 			rcu_read_lock();
+ 			continue;
+ 		}
+ 		dax_lock_entry(&xas, entry);
+ 		xas_unlock_irq(&xas);
+ 		break;
+ 	}
+ 	rcu_read_unlock();
+ 	return locked;
+ }
+ 
+ void dax_unlock_mapping_entry(struct page *page)
+ {
+ 	struct address_space *mapping = page->mapping;
+ 	XA_STATE(xas, &mapping->i_pages, page->index);
+ 	void *entry;
+ 
+ 	if (S_ISCHR(mapping->host->i_mode))
+ 		return;
+ 
+ 	rcu_read_lock();
+ 	entry = xas_load(&xas);
+ 	rcu_read_unlock();
+ 	entry = dax_make_entry(page_to_pfn_t(page), dax_is_pmd_entry(entry));
+ 	dax_unlock_entry(&xas, entry);
+ }
+ 
+ /*
+  * Find page cache entry at given index. If it is a DAX entry, return it
+  * with the entry locked. If the page cache doesn't contain an entry at
+  * that index, add a locked empty entry.
++>>>>>>> 55e56f06ed71 (dax: Don't access a freed inode)
   *
 - * When requesting an entry with size DAX_PMD, grab_mapping_entry() will
 - * either return that locked entry or will return VM_FAULT_FALLBACK.
 - * This will happen if there are any PTE entries within the PMD range
 - * that we are requesting.
 - *
 - * We always favor PTE entries over PMD entries. There isn't a flow where we
 - * evict PTE entries in order to 'upgrade' them to a PMD entry.  A PMD
 - * insertion will fail if it finds any PTE entries already in the tree, and a
 - * PTE insertion will cause an existing PMD entry to be unmapped and
 - * downgraded to PTE entries.  This happens for both PMD zero pages as
 - * well as PMD empty entries.
 + * We always favor 4k entries over 2MiB entries. There isn't a flow where we
 + * evict 4k entries in order to 'upgrade' them to a 2MiB entry.  A 2MiB
 + * insertion will fail if it finds any 4k entries already in the tree, and a
 + * 4k insertion will cause an existing 2MiB entry to be unmapped and
 + * downgraded to 4k entries.  This happens for both 2MiB huge zero pages as
 + * well as 2MiB empty entries.
   *
 - * The exception to this downgrade path is for PMD entries that have
 - * real storage backing them.  We will leave these real PMD entries in
 - * the tree, and PTE writes will simply dirty the entire PMD entry.
 + * The exception to this downgrade path is for 2MiB DAX PMD entries that have
 + * real storage backing them.  We will leave these real 2MiB DAX entries in
 + * the tree, and PTE writes will simply dirty the entire 2MiB DAX entry.
   *
   * Note: Unlike filemap_fault() we don't honor FAULT_FLAG_RETRY flags. For
   * persistent memory the benefit is doubtful. We can add that later if we can
* Unmerged path fs/dax.c
