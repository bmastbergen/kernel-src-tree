percpu: introduce helper to determine if two regions overlap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou <dennis@kernel.org>
commit d9f3a01eebe80180babd8541406490020f184d17
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/d9f3a01e.failed

While block hints were always accurate, it's possible when spanning
across blocks that we miss updating the chunk's contig_hint. Rather than
rely on correctness of the boundaries of hints, do a full overlap
comparison.

A future patch introduces the scan_hint which makes the contig_hint
slightly fuzzy as they can at times be smaller than the actual hint.

	Signed-off-by: Dennis Zhou <dennis@kernel.org>
(cherry picked from commit d9f3a01eebe80180babd8541406490020f184d17)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 60a1f468f968,cbace9e79f2d..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -379,278 -514,529 +379,444 @@@ static void pcpu_chunk_relocate(struct 
  }
  
  /**
 - * pcpu_cnt_pop_pages- counts populated backing pages in range
 + * pcpu_need_to_extend - determine whether chunk area map needs to be extended
   * @chunk: chunk of interest
 - * @bit_off: start offset
 - * @bits: size of area to check
 + * @is_atomic: the allocation context
 + *
 + * Determine whether area map of @chunk needs to be extended.  If
 + * @is_atomic, only the amount necessary for a new allocation is
 + * considered; however, async extension is scheduled if the left amount is
 + * low.  If !@is_atomic, it aims for more empty space.  Combined, this
 + * ensures that the map is likely to have enough available space to
 + * accomodate atomic allocations which can't extend maps directly.
   *
 - * Calculates the number of populated pages in the region
 - * [page_start, page_end).  This keeps track of how many empty populated
 - * pages are available and decide if async work should be scheduled.
 + * CONTEXT:
 + * pcpu_lock.
   *
   * RETURNS:
 - * The nr of populated pages.
 + * New target map allocation length if extension is necessary, 0
 + * otherwise.
   */
 -static inline int pcpu_cnt_pop_pages(struct pcpu_chunk *chunk, int bit_off,
 -				     int bits)
 +static int pcpu_need_to_extend(struct pcpu_chunk *chunk, bool is_atomic)
  {
 -	int page_start = PFN_UP(bit_off * PCPU_MIN_ALLOC_SIZE);
 -	int page_end = PFN_DOWN((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
 +	int margin, new_alloc;
 +
 +	lockdep_assert_held(&pcpu_lock);
 +
 +	if (is_atomic) {
 +		margin = 3;
  
 -	if (page_start >= page_end)
 +		if (chunk->map_alloc <
 +		    chunk->map_used + PCPU_ATOMIC_MAP_MARGIN_LOW) {
 +			if (list_empty(&chunk->map_extend_list)) {
 +				list_add_tail(&chunk->map_extend_list,
 +					      &pcpu_map_extend_chunks);
 +				pcpu_schedule_balance_work();
 +			}
 +		}
 +	} else {
 +		margin = PCPU_ATOMIC_MAP_MARGIN_HIGH;
 +	}
 +
 +	if (chunk->map_alloc >= chunk->map_used + margin)
  		return 0;
  
 -	/*
 -	 * bitmap_weight counts the number of bits set in a bitmap up to
 -	 * the specified number of bits.  This is counting the populated
 -	 * pages up to page_end and then subtracting the populated pages
 -	 * up to page_start to count the populated pages in
 -	 * [page_start, page_end).
 -	 */
 -	return bitmap_weight(chunk->populated, page_end) -
 -	       bitmap_weight(chunk->populated, page_start);
 +	new_alloc = PCPU_DFL_MAP_ALLOC;
 +	while (new_alloc < chunk->map_used + margin)
 +		new_alloc *= 2;
 +
 +	return new_alloc;
  }
  
+ /*
+  * pcpu_region_overlap - determines if two regions overlap
+  * @a: start of first region, inclusive
+  * @b: end of first region, exclusive
+  * @x: start of second region, inclusive
+  * @y: end of second region, exclusive
+  *
+  * This is used to determine if the hint region [a, b) overlaps with the
+  * allocated region [x, y).
+  */
+ static inline bool pcpu_region_overlap(int a, int b, int x, int y)
+ {
+ 	return (a < y) && (x < b);
+ }
+ 
  /**
 - * pcpu_chunk_update - updates the chunk metadata given a free area
 + * pcpu_extend_area_map - extend area map of a chunk
   * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of free area
 + * @new_alloc: new target allocation length of the area map
   *
 - * This updates the chunk's contig hint and starting offset given a free area.
 - * Choose the best starting offset if the contig hint is equal.
 + * Extend area map of @chunk to have @new_alloc entries.
 + *
 + * CONTEXT:
 + * Does GFP_KERNEL allocation.  Grabs and releases pcpu_lock.
 + *
 + * RETURNS:
 + * 0 on success, -errno on failure.
   */
 -static void pcpu_chunk_update(struct pcpu_chunk *chunk, int bit_off, int bits)
 +static int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)
  {
 -	if (bits > chunk->contig_bits) {
 -		chunk->contig_bits_start = bit_off;
 -		chunk->contig_bits = bits;
 -	} else if (bits == chunk->contig_bits && chunk->contig_bits_start &&
 -		   (!bit_off ||
 -		    __ffs(bit_off) > __ffs(chunk->contig_bits_start))) {
 -		/* use the start with the best alignment */
 -		chunk->contig_bits_start = bit_off;
 -	}
 +	int *old = NULL, *new = NULL;
 +	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
 +	unsigned long flags;
 +
 +	lockdep_assert_held(&pcpu_alloc_mutex);
 +
 +	new = pcpu_mem_zalloc(new_size);
 +	if (!new)
 +		return -ENOMEM;
 +
 +	/* acquire pcpu_lock and switch to new area map */
 +	spin_lock_irqsave(&pcpu_lock, flags);
 +
 +	if (new_alloc <= chunk->map_alloc)
 +		goto out_unlock;
 +
 +	old_size = chunk->map_alloc * sizeof(chunk->map[0]);
 +	old = chunk->map;
 +
 +	memcpy(new, old, old_size);
 +
 +	chunk->map_alloc = new_alloc;
 +	chunk->map = new;
 +	new = NULL;
 +
 +out_unlock:
 +	spin_unlock_irqrestore(&pcpu_lock, flags);
 +
 +	/*
 +	 * pcpu_mem_free() might end up calling vfree() which uses
 +	 * IRQ-unsafe lock and thus can't be called under pcpu_lock.
 +	 */
 +	pcpu_mem_free(old, old_size);
 +	pcpu_mem_free(new, new_size);
 +
 +	return 0;
  }
  
  /**
 - * pcpu_chunk_refresh_hint - updates metadata about a chunk
 - * @chunk: chunk of interest
 - *
 - * Iterates over the metadata blocks to find the largest contig area.
 - * It also counts the populated pages and uses the delta to update the
 - * global count.
 - *
 - * Updates:
 - *      chunk->contig_bits
 - *      chunk->contig_bits_start
 - *      nr_empty_pop_pages (chunk and global)
 + * pcpu_fit_in_area - try to fit the requested allocation in a candidate area
 + * @chunk: chunk the candidate area belongs to
 + * @off: the offset to the start of the candidate area
 + * @this_size: the size of the candidate area
 + * @size: the size of the target allocation
 + * @align: the alignment of the target allocation
 + * @pop_only: only allocate from already populated region
 + *
 + * We're trying to allocate @size bytes aligned at @align.  @chunk's area
 + * at @off sized @this_size is a candidate.  This function determines
 + * whether the target allocation fits in the candidate area and returns the
 + * number of bytes to pad after @off.  If the target area doesn't fit, -1
 + * is returned.
 + *
 + * If @pop_only is %true, this function only considers the already
 + * populated part of the candidate area.
   */
 -static void pcpu_chunk_refresh_hint(struct pcpu_chunk *chunk)
 +static int pcpu_fit_in_area(struct pcpu_chunk *chunk, int off, int this_size,
 +			    int size, int align, bool pop_only)
  {
 -	int bit_off, bits, nr_empty_pop_pages;
 +	int cand_off = off;
  
 -	/* clear metadata */
 -	chunk->contig_bits = 0;
 +	while (true) {
 +		int head = ALIGN(cand_off, align) - off;
 +		int page_start, page_end, rs, re;
  
 -	bit_off = chunk->first_bit;
 -	bits = nr_empty_pop_pages = 0;
 -	pcpu_for_each_md_free_region(chunk, bit_off, bits) {
 -		pcpu_chunk_update(chunk, bit_off, bits);
 +		if (this_size < head + size)
 +			return -1;
  
 -		nr_empty_pop_pages += pcpu_cnt_pop_pages(chunk, bit_off, bits);
 -	}
 +		if (!pop_only)
 +			return head;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Keep track of nr_empty_pop_pages.
+ 	 *
+ 	 * The chunk maintains the previous number of free pages it held,
+ 	 * so the delta is used to update the global counter.  The reserved
+ 	 * chunk is not part of the free page count as they are populated
+ 	 * at init and are special to serving reserved allocations.
+ 	 */
+ 	if (chunk != pcpu_reserved_chunk)
+ 		pcpu_nr_empty_pop_pages +=
+ 			(nr_empty_pop_pages - chunk->nr_empty_pop_pages);
+ 
+ 	chunk->nr_empty_pop_pages = nr_empty_pop_pages;
+ }
+ 
+ /**
+  * pcpu_block_update - updates a block given a free area
+  * @block: block of interest
+  * @start: start offset in block
+  * @end: end offset in block
+  *
+  * Updates a block given a known free area.  The region [start, end) is
+  * expected to be the entirety of the free area within a block.  Chooses
+  * the best starting offset if the contig hints are equal.
+  */
+ static void pcpu_block_update(struct pcpu_block_md *block, int start, int end)
+ {
+ 	int contig = end - start;
+ 
+ 	block->first_free = min(block->first_free, start);
+ 	if (start == 0)
+ 		block->left_free = contig;
+ 
+ 	if (end == PCPU_BITMAP_BLOCK_BITS)
+ 		block->right_free = contig;
+ 
+ 	if (contig > block->contig_hint) {
+ 		block->contig_hint_start = start;
+ 		block->contig_hint = contig;
+ 	} else if (block->contig_hint_start && contig == block->contig_hint &&
+ 		   (!start || __ffs(start) > __ffs(block->contig_hint_start))) {
+ 		/* use the start with the best alignment */
+ 		block->contig_hint_start = start;
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_refresh_hint
+  * @chunk: chunk of interest
+  * @index: index of the metadata block
+  *
+  * Scans over the block beginning at first_free and updates the block
+  * metadata accordingly.
+  */
+ static void pcpu_block_refresh_hint(struct pcpu_chunk *chunk, int index)
+ {
+ 	struct pcpu_block_md *block = chunk->md_blocks + index;
+ 	unsigned long *alloc_map = pcpu_index_alloc_map(chunk, index);
+ 	int rs, re;	/* region start, region end */
+ 
+ 	/* clear hints */
+ 	block->contig_hint = 0;
+ 	block->left_free = block->right_free = 0;
+ 
+ 	/* iterate over free areas and update the contig hints */
+ 	pcpu_for_each_unpop_region(alloc_map, rs, re, block->first_free,
+ 				   PCPU_BITMAP_BLOCK_BITS) {
+ 		pcpu_block_update(block, rs, re);
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_update_hint_alloc - update hint on allocation path
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of request
+  *
+  * Updates metadata for the allocation path.  The metadata only has to be
+  * refreshed by a full scan iff the chunk's contig hint is broken.  Block level
+  * scans are required if the block's contig hint is broken.
+  */
+ static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,
+ 					 int bits)
+ {
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/*
+ 	 * Update s_block.
+ 	 * block->first_free must be updated if the allocation takes its place.
+ 	 * If the allocation breaks the contig_hint, a scan is required to
+ 	 * restore this hint.
+ 	 */
+ 	if (s_off == s_block->first_free)
+ 		s_block->first_free = find_next_zero_bit(
+ 					pcpu_index_alloc_map(chunk, s_index),
+ 					PCPU_BITMAP_BLOCK_BITS,
+ 					s_off + bits);
+ 
+ 	if (pcpu_region_overlap(s_block->contig_hint_start,
+ 				s_block->contig_hint_start +
+ 				s_block->contig_hint,
+ 				s_off,
+ 				s_off + bits)) {
+ 		/* block contig hint is broken - scan to fix it */
+ 		pcpu_block_refresh_hint(chunk, s_index);
+ 	} else {
+ 		/* update left and right contig manually */
+ 		s_block->left_free = min(s_block->left_free, s_off);
+ 		if (s_index == e_index)
+ 			s_block->right_free = min_t(int, s_block->right_free,
+ 					PCPU_BITMAP_BLOCK_BITS - e_off);
+ 		else
+ 			s_block->right_free = 0;
+ 	}
+ 
+ 	/*
+ 	 * Update e_block.
+ 	 */
+ 	if (s_index != e_index) {
++>>>>>>> d9f3a01eebe8 (percpu: introduce helper to determine if two regions overlap)
  		/*
 -		 * When the allocation is across blocks, the end is along
 -		 * the left part of the e_block.
 +		 * If the first unpopulated page is beyond the end of the
 +		 * allocation, the whole allocation is populated;
 +		 * otherwise, retry from the end of the unpopulated area.
  		 */
 -		e_block->first_free = find_next_zero_bit(
 -				pcpu_index_alloc_map(chunk, e_index),
 -				PCPU_BITMAP_BLOCK_BITS, e_off);
 -
 -		if (e_off == PCPU_BITMAP_BLOCK_BITS) {
 -			/* reset the block */
 -			e_block++;
 -		} else {
 -			if (e_off > e_block->contig_hint_start) {
 -				/* contig hint is broken - scan to fix it */
 -				pcpu_block_refresh_hint(chunk, e_index);
 -			} else {
 -				e_block->left_free = 0;
 -				e_block->right_free =
 -					min_t(int, e_block->right_free,
 -					      PCPU_BITMAP_BLOCK_BITS - e_off);
 -			}
 -		}
 -
 -		/* update in-between md_blocks */
 -		for (block = s_block + 1; block < e_block; block++) {
 -			block->contig_hint = 0;
 -			block->left_free = 0;
 -			block->right_free = 0;
 -		}
 +		page_start = PFN_DOWN(head + off);
 +		page_end = PFN_UP(head + off + size);
 +
 +		rs = page_start;
 +		pcpu_next_unpop(chunk, &rs, &re, PFN_UP(off + this_size));
 +		if (rs >= page_end)
 +			return head;
 +		cand_off = re * PAGE_SIZE;
  	}
 -
 -	/*
 -	 * The only time a full chunk scan is required is if the chunk
 -	 * contig hint is broken.  Otherwise, it means a smaller space
 -	 * was used and therefore the chunk contig hint is still correct.
 -	 */
 -	if (pcpu_region_overlap(chunk->contig_bits_start,
 -				chunk->contig_bits_start + chunk->contig_bits,
 -				bit_off,
 -				bit_off + bits))
 -		pcpu_chunk_refresh_hint(chunk);
  }
  
  /**
 - * pcpu_block_update_hint_free - updates the block hints on the free path
 + * pcpu_alloc_area - allocate area from a pcpu_chunk
   * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of request
 + * @size: wanted size in bytes
 + * @align: wanted align
 + * @pop_only: allocate only from the populated area
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Try to allocate @size bytes area aligned at @align from @chunk.
 + * Note that this function only allocates the offset.  It doesn't
 + * populate or map the area.
   *
 - * Updates metadata for the allocation path.  This avoids a blind block
 - * refresh by making use of the block contig hints.  If this fails, it scans
 - * forward and backward to determine the extent of the free area.  This is
 - * capped at the boundary of blocks.
 + * @chunk->map must have at least two free slots.
 + *
 + * CONTEXT:
 + * pcpu_lock.
   *
 - * A chunk update is triggered if a page becomes free, a block becomes free,
 - * or the free spans across blocks.  This tradeoff is to minimize iterating
 - * over the block metadata to update chunk->contig_bits.  chunk->contig_bits
 - * may be off by up to a page, but it will never be more than the available
 - * space.  If the contig hint is contained in one block, it will be accurate.
 + * RETURNS:
 + * Allocated offset in @chunk on success, -1 if no matching area is
 + * found.
   */
 -static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 -					int bits)
 +static int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align,
 +			   bool pop_only, int *occ_pages_p)
  {
 -	struct pcpu_block_md *s_block, *e_block, *block;
 -	int s_index, e_index;	/* block indexes of the freed allocation */
 -	int s_off, e_off;	/* block offsets of the freed allocation */
 -	int start, end;		/* start and end of the whole free area */
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int max_contig = 0;
 +	int i, off;
 +	bool seen_free = false;
 +	int *p;
 +
 +	for (i = chunk->first_free, p = chunk->map + i; i < chunk->map_used; i++, p++) {
 +		int head, tail;
 +		int this_size;
 +
 +		off = *p;
 +		if (off & 1)
 +			continue;
  
 -	/*
 -	 * Calculate per block offsets.
 -	 * The calculation uses an inclusive range, but the resulting offsets
 -	 * are [start, end).  e_index always points to the last block in the
 -	 * range.
 -	 */
 -	s_index = pcpu_off_to_block_index(bit_off);
 -	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
 -	s_off = pcpu_off_to_block_off(bit_off);
 -	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
 +		this_size = (p[1] & ~1) - off;
  
 -	s_block = chunk->md_blocks + s_index;
 -	e_block = chunk->md_blocks + e_index;
 +		head = pcpu_fit_in_area(chunk, off, this_size, size, align,
 +					pop_only);
 +		if (head < 0) {
 +			if (!seen_free) {
 +				chunk->first_free = i;
 +				seen_free = true;
 +			}
 +			max_contig = max(this_size, max_contig);
 +			continue;
 +		}
  
 -	/*
 -	 * Check if the freed area aligns with the block->contig_hint.
 -	 * If it does, then the scan to find the beginning/end of the
 -	 * larger free area can be avoided.
 -	 *
 -	 * start and end refer to beginning and end of the free area
 -	 * within each their respective blocks.  This is not necessarily
 -	 * the entire free area as it may span blocks past the beginning
 -	 * or end of the block.
 -	 */
 -	start = s_off;
 -	if (s_off == s_block->contig_hint + s_block->contig_hint_start) {
 -		start = s_block->contig_hint_start;
 -	} else {
  		/*
 -		 * Scan backwards to find the extent of the free area.
 -		 * find_last_bit returns the starting bit, so if the start bit
 -		 * is returned, that means there was no last bit and the
 -		 * remainder of the chunk is free.
 +		 * If head is small or the previous block is free,
 +		 * merge'em.  Note that 'small' is defined as smaller
 +		 * than sizeof(int), which is very small but isn't too
 +		 * uncommon for percpu allocations.
  		 */
 -		int l_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index),
 -					  start);
 -		start = (start == l_bit) ? 0 : l_bit + 1;
 -	}
 -
 -	end = e_off;
 -	if (e_off == e_block->contig_hint_start)
 -		end = e_block->contig_hint_start + e_block->contig_hint;
 -	else
 -		end = find_next_bit(pcpu_index_alloc_map(chunk, e_index),
 -				    PCPU_BITMAP_BLOCK_BITS, end);
 -
 -	/* update s_block */
 -	e_off = (s_index == e_index) ? end : PCPU_BITMAP_BLOCK_BITS;
 -	pcpu_block_update(s_block, start, e_off);
 -
 -	/* freeing in the same block */
 -	if (s_index != e_index) {
 -		/* update e_block */
 -		pcpu_block_update(e_block, 0, end);
 -
 -		/* reset md_blocks in the middle */
 -		for (block = s_block + 1; block < e_block; block++) {
 -			block->first_free = 0;
 -			block->contig_hint_start = 0;
 -			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
 -			block->left_free = PCPU_BITMAP_BLOCK_BITS;
 -			block->right_free = PCPU_BITMAP_BLOCK_BITS;
 +		if (head && (head < sizeof(int) || !(p[-1] & 1))) {
 +			*p = off += head;
 +			if (p[-1] & 1)
 +				chunk->free_size -= head;
 +			else
 +				max_contig = max(*p - p[-1], max_contig);
 +			this_size -= head;
 +			head = 0;
  		}
 -	}
  
 -	/*
 -	 * Refresh chunk metadata when the free makes a page free, a block
 -	 * free, or spans across blocks.  The contig hint may be off by up to
 -	 * a page, but if the hint is contained in a block, it will be accurate
 -	 * with the else condition below.
 -	 */
 -	if ((ALIGN_DOWN(end, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS)) >
 -	     ALIGN(start, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS))) ||
 -	    s_index != e_index)
 -		pcpu_chunk_refresh_hint(chunk);
 -	else
 -		pcpu_chunk_update(chunk, pcpu_block_off_to_off(s_index, start),
 -				  end - start);
 -}
 -
 -/**
 - * pcpu_is_populated - determines if the region is populated
 - * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of area
 - * @next_off: return value for the next offset to start searching
 - *
 - * For atomic allocations, check if the backing pages are populated.
 - *
 - * RETURNS:
 - * Bool if the backing pages are populated.
 - * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
 - */
 -static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
 -			      int *next_off)
 -{
 -	int page_start, page_end, rs, re;
 +		/* if tail is small, just keep it around */
 +		tail = this_size - head - size;
 +		if (tail < sizeof(int)) {
 +			tail = 0;
 +			size = this_size - head;
 +		}
  
 -	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
 -	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
 +		/* split if warranted */
 +		if (head || tail) {
 +			int nr_extra = !!head + !!tail;
 +
 +			/* insert new subblocks */
 +			memmove(p + nr_extra + 1, p + 1,
 +				sizeof(chunk->map[0]) * (chunk->map_used - i));
 +			chunk->map_used += nr_extra;
 +
 +			if (head) {
 +				if (!seen_free) {
 +					chunk->first_free = i;
 +					seen_free = true;
 +				}
 +				*++p = off += head;
 +				++i;
 +				max_contig = max(head, max_contig);
 +			}
 +			if (tail) {
 +				p[1] = off + size;
 +				max_contig = max(tail, max_contig);
 +			}
 +		}
  
 -	rs = page_start;
 -	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
 -	if (rs >= page_end)
 -		return true;
 +		if (!seen_free)
 +			chunk->first_free = i + 1;
  
 -	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
 -	return false;
 -}
 -
 -/**
 - * pcpu_find_block_fit - finds the block index to start searching
 - * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE bytes)
 - * @pop_only: use populated regions only
 - *
 - * Given a chunk and an allocation spec, find the offset to begin searching
 - * for a free region.  This iterates over the bitmap metadata blocks to
 - * find an offset that will be guaranteed to fit the requirements.  It is
 - * not quite first fit as if the allocation does not fit in the contig hint
 - * of a block or chunk, it is skipped.  This errs on the side of caution
 - * to prevent excess iteration.  Poor alignment can cause the allocator to
 - * skip over blocks and chunks that have valid free areas.
 - *
 - * RETURNS:
 - * The offset in the bitmap to begin searching.
 - * -1 if no offset is found.
 - */
 -static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
 -			       size_t align, bool pop_only)
 -{
 -	int bit_off, bits, next_off;
 +		/* update hint and mark allocated */
 +		if (i + 1 == chunk->map_used)
 +			chunk->contig_hint = max_contig; /* fully scanned */
 +		else
 +			chunk->contig_hint = max(chunk->contig_hint,
 +						 max_contig);
  
 -	/*
 -	 * Check to see if the allocation can fit in the chunk's contig hint.
 -	 * This is an optimization to prevent scanning by assuming if it
 -	 * cannot fit in the global hint, there is memory pressure and creating
 -	 * a new chunk would happen soon.
 -	 */
 -	bit_off = ALIGN(chunk->contig_bits_start, align) -
 -		  chunk->contig_bits_start;
 -	if (bit_off + alloc_bits > chunk->contig_bits)
 -		return -1;
 -
 -	bit_off = chunk->first_bit;
 -	bits = 0;
 -	pcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits) {
 -		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
 -						   &next_off))
 -			break;
 +		chunk->free_size -= size;
 +		*p |= 1;
  
 -		bit_off = next_off;
 -		bits = 0;
 +		*occ_pages_p = pcpu_count_occupied_pages(chunk, i);
 +		pcpu_chunk_relocate(chunk, oslot);
 +		return off;
  	}
  
 -	if (bit_off == pcpu_chunk_map_bits(chunk))
 -		return -1;
 -
 -	return bit_off;
 -}
 -
 -/**
 - * pcpu_alloc_area - allocates an area from a pcpu_chunk
 - * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE)
 - * @start: bit_off to start searching
 - *
 - * This function takes in a @start offset to begin searching to fit an
 - * allocation of @alloc_bits with alignment @align.  It needs to scan
 - * the allocation map because if it fits within the block's contig hint,
 - * @start will be block->first_free. This is an attempt to fill the
 - * allocation prior to breaking the contig hint.  The allocation and
 - * boundary maps are updated accordingly if it confirms a valid
 - * free area.
 - *
 - * RETURNS:
 - * Allocated addr offset in @chunk on success.
 - * -1 if no matching area is found.
 - */
 -static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 -			   size_t align, int start)
 -{
 -	size_t align_mask = (align) ? (align - 1) : 0;
 -	int bit_off, end, oslot;
 -
 -	lockdep_assert_held(&pcpu_lock);
 -
 -	oslot = pcpu_chunk_slot(chunk);
++<<<<<<< HEAD
 +	chunk->contig_hint = max_contig;	/* fully scanned */
 +	pcpu_chunk_relocate(chunk, oslot);
  
 +	/* tell the upper layer that this chunk has no matching area */
 +	return -1;
++=======
+ 	/*
 -	 * Search to find a fit.
++	 * The only time a full chunk scan is required is if the chunk
++	 * contig hint is broken.  Otherwise, it means a smaller space
++	 * was used and therefore the chunk contig hint is still correct.
+ 	 */
 -	end = min_t(int, start + alloc_bits + PCPU_BITMAP_BLOCK_BITS,
 -		    pcpu_chunk_map_bits(chunk));
 -	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
 -					     alloc_bits, align_mask);
 -	if (bit_off >= end)
 -		return -1;
 -
 -	/* update alloc map */
 -	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
 -
 -	/* update boundary map */
 -	set_bit(bit_off, chunk->bound_map);
 -	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
 -	set_bit(bit_off + alloc_bits, chunk->bound_map);
 -
 -	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
 -
 -	/* update first free bit */
 -	if (bit_off == chunk->first_bit)
 -		chunk->first_bit = find_next_zero_bit(
 -					chunk->alloc_map,
 -					pcpu_chunk_map_bits(chunk),
 -					bit_off + alloc_bits);
 -
 -	pcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);
 -
 -	pcpu_chunk_relocate(chunk, oslot);
 -
 -	return bit_off * PCPU_MIN_ALLOC_SIZE;
++	if (pcpu_region_overlap(chunk->contig_bits_start,
++				chunk->contig_bits_start + chunk->contig_bits,
++				bit_off,
++				bit_off + bits))
++		pcpu_chunk_refresh_hint(chunk);
++>>>>>>> d9f3a01eebe8 (percpu: introduce helper to determine if two regions overlap)
  }
  
  /**
* Unmerged path mm/percpu.c
