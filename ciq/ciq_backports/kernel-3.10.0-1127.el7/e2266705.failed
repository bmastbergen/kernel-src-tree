percpu: introduce start_offset to pcpu_chunk

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit e22667056644086ca4a5b2986eb4fbf32e03ebab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/e2266705.failed

The reserved chunk arithmetic uses a global variable
pcpu_reserved_chunk_limit that is set in the first chunk init code to
hide a portion of the area map. The bitmap allocator to come will
eventually move the base_addr up and require both the reserved chunk
and static chunk to maintain this offset. pcpu_reserved_chunk_limit is
removed and start_offset is added.

The first chunk that is circulated and is pcpu_first_chunk serves the
dynamic region, the region following the reserved region. The reserved
chunk address check will temporarily use the first chunk to identify its
address range. A following patch will increase the base_addr and remove
this. If there is no reserved chunk, this will check the static region
and return false because those values should never be passed into the
allocator.

Lastly, when linking in the first chunk, make sure to count the right
free region for the number of empty populated pages.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit e22667056644086ca4a5b2986eb4fbf32e03ebab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu-internal.h
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,e94f0d18c421..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -151,18 -145,15 +151,20 @@@ static struct pcpu_chunk *pcpu_first_ch
  
  /*
   * Optional reserved chunk.  This chunk reserves part of the first
-  * chunk and serves it for reserved allocations.  The amount of
-  * reserved offset is in pcpu_reserved_chunk_limit.  When reserved
-  * area doesn't exist, the following variables contain NULL and 0
-  * respectively.
+  * chunk and serves it for reserved allocations.  When the reserved
+  * region doesn't exist, the following variable is NULL.
   */
++<<<<<<< HEAD
 +static struct pcpu_chunk *pcpu_reserved_chunk;
 +static int pcpu_reserved_chunk_limit;
++=======
+ struct pcpu_chunk *pcpu_reserved_chunk __ro_after_init;
++>>>>>>> e22667056644 (percpu: introduce start_offset to pcpu_chunk)
  
 -DEFINE_SPINLOCK(pcpu_lock);	/* all internal data structures */
 +static DEFINE_SPINLOCK(pcpu_lock);	/* all internal data structures */
  static DEFINE_MUTEX(pcpu_alloc_mutex);	/* chunk create/destroy, [de]pop, map ext */
  
 -struct list_head *pcpu_slot __ro_after_init; /* chunk list slots */
 +static struct list_head *pcpu_slot __read_mostly; /* chunk list slots */
  
  /* chunks which need their map areas extended, protected by pcpu_lock */
  static LIST_HEAD(pcpu_map_extend_chunks);
@@@ -1689,14 -1698,13 +1691,21 @@@ int __init pcpu_setup_first_chunk(cons
  		schunk->free_size = dyn_size;
  		dyn_size = 0;			/* dynamic area covered */
  	}
 -
  	schunk->contig_hint = schunk->free_size;
 +
  	schunk->map[0] = 1;
++<<<<<<< HEAD
 +	schunk->map[1] = ai->static_size;
 +	schunk->map_used = 1;
 +	if (schunk->free_size)
 +		schunk->map[++schunk->map_used] = ai->static_size + schunk->free_size;
 +	schunk->map[schunk->map_used] |= 1;
++=======
+ 	schunk->map[1] = schunk->start_offset;
+ 	schunk->map[2] = (ai->static_size + schunk->free_size) | 1;
+ 	schunk->map_used = 2;
+ 	schunk->has_reserved = true;
++>>>>>>> e22667056644 (percpu: introduce start_offset to pcpu_chunk)
  
  	/* init dynamic chunk if necessary */
  	if (dyn_size) {
@@@ -1712,17 -1721,22 +1722,18 @@@
  
  		dchunk->contig_hint = dchunk->free_size = dyn_size;
  		dchunk->map[0] = 1;
- 		dchunk->map[1] = pcpu_reserved_chunk_limit;
- 		dchunk->map[2] = (pcpu_reserved_chunk_limit + dchunk->free_size) | 1;
+ 		dchunk->map[1] = dchunk->start_offset;
+ 		dchunk->map[2] = (dchunk->start_offset + dchunk->free_size) | 1;
  		dchunk->map_used = 2;
 -		dchunk->has_reserved = true;
  	}
  
  	/* link the first chunk in */
  	pcpu_first_chunk = dchunk ?: schunk;
+ 	i = (pcpu_first_chunk->start_offset) ? 1 : 0;
  	pcpu_nr_empty_pop_pages +=
- 		pcpu_count_occupied_pages(pcpu_first_chunk, 1);
+ 		pcpu_count_occupied_pages(pcpu_first_chunk, i);
  	pcpu_chunk_relocate(pcpu_first_chunk, -1);
  
 -	pcpu_stats_chunk_alloc();
 -	trace_percpu_create_chunk(base_addr);
 -
  	/* we're done */
  	pcpu_base_addr = base_addr;
  	return 0;
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu.c
