tracing: Add hook to function tracing for other subsystems to use

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Chunyan Zhang <zhang.chunyan@linaro.org>
commit 478409dd683db76cbcfe7bf8332a37f01deb0a2d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/478409dd.failed

Currently Function traces can be only exported to the ring buffer. This
adds a trace_export concept which can process traces and export
them to a registered destination as an addition to the current
one that outputs to Ftrace - i.e. ring buffer.

In this way, if we want function traces to be sent to other destinations
rather than only to the ring buffer, we just need to register a new
trace_export and implement its own .write() function for writing traces to
storage.

With this patch, only function tracing (trace type is TRACE_FN)
is supported.

Link: http://lkml.kernel.org/r/1479715043-6534-2-git-send-email-zhang.chunyan@linaro.org

	Signed-off-by: Chunyan Zhang <zhang.chunyan@linaro.org>
	Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
(cherry picked from commit 478409dd683db76cbcfe7bf8332a37f01deb0a2d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/trace/trace.c
diff --cc kernel/trace/trace.c
index 1f2fe17a9d70,edccdff8a36d..000000000000
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@@ -1644,18 -2115,143 +1645,141 @@@ void trace_buffer_unlock_commit_regs(st
  {
  	__buffer_unlock_commit(buffer, event);
  
 -	/*
 -	 * If regs is not set, then skip the following callers:
 -	 *   trace_buffer_unlock_commit_regs
 -	 *   event_trigger_unlock_commit
 -	 *   trace_event_buffer_commit
 -	 *   trace_event_raw_event_sched_switch
 -	 * Note, we can still get here via blktrace, wakeup tracer
 -	 * and mmiotrace, but that's ok if they lose a function or
 -	 * two. They are that meaningful.
 -	 */
 -	ftrace_trace_stack(tr, buffer, flags, regs ? 0 : 4, pc, regs);
 +	ftrace_trace_stack_regs(buffer, flags, 0, pc, regs);
  	ftrace_trace_userstack(buffer, flags, pc);
  }
 +EXPORT_SYMBOL_GPL(trace_buffer_unlock_commit_regs);
 +
 +void trace_current_buffer_discard_commit(struct ring_buffer *buffer,
 +					 struct ring_buffer_event *event)
 +{
 +	ring_buffer_discard_commit(buffer, event);
 +}
 +EXPORT_SYMBOL_GPL(trace_current_buffer_discard_commit);
  
+ static void
+ trace_process_export(struct trace_export *export,
+ 	       struct ring_buffer_event *event)
+ {
+ 	struct trace_entry *entry;
+ 	unsigned int size = 0;
+ 
+ 	entry = ring_buffer_event_data(event);
+ 	size = ring_buffer_event_length(event);
+ 	export->write(entry, size);
+ }
+ 
+ static DEFINE_MUTEX(ftrace_export_lock);
+ 
+ static struct trace_export __rcu *ftrace_exports_list __read_mostly;
+ 
+ static DEFINE_STATIC_KEY_FALSE(ftrace_exports_enabled);
+ 
+ static inline void ftrace_exports_enable(void)
+ {
+ 	static_branch_enable(&ftrace_exports_enabled);
+ }
+ 
+ static inline void ftrace_exports_disable(void)
+ {
+ 	static_branch_disable(&ftrace_exports_enabled);
+ }
+ 
+ void ftrace_exports(struct ring_buffer_event *event)
+ {
+ 	struct trace_export *export;
+ 
+ 	preempt_disable_notrace();
+ 
+ 	export = rcu_dereference_raw_notrace(ftrace_exports_list);
+ 	while (export) {
+ 		trace_process_export(export, event);
+ 		export = rcu_dereference_raw_notrace(export->next);
+ 	}
+ 
+ 	preempt_enable_notrace();
+ }
+ 
+ static inline void
+ add_trace_export(struct trace_export **list, struct trace_export *export)
+ {
+ 	rcu_assign_pointer(export->next, *list);
+ 	/*
+ 	 * We are entering export into the list but another
+ 	 * CPU might be walking that list. We need to make sure
+ 	 * the export->next pointer is valid before another CPU sees
+ 	 * the export pointer included into the list.
+ 	 */
+ 	rcu_assign_pointer(*list, export);
+ }
+ 
+ static inline int
+ rm_trace_export(struct trace_export **list, struct trace_export *export)
+ {
+ 	struct trace_export **p;
+ 
+ 	for (p = list; *p != NULL; p = &(*p)->next)
+ 		if (*p == export)
+ 			break;
+ 
+ 	if (*p != export)
+ 		return -1;
+ 
+ 	rcu_assign_pointer(*p, (*p)->next);
+ 
+ 	return 0;
+ }
+ 
+ static inline void
+ add_ftrace_export(struct trace_export **list, struct trace_export *export)
+ {
+ 	if (*list == NULL)
+ 		ftrace_exports_enable();
+ 
+ 	add_trace_export(list, export);
+ }
+ 
+ static inline int
+ rm_ftrace_export(struct trace_export **list, struct trace_export *export)
+ {
+ 	int ret;
+ 
+ 	ret = rm_trace_export(list, export);
+ 	if (*list == NULL)
+ 		ftrace_exports_disable();
+ 
+ 	return ret;
+ }
+ 
+ int register_ftrace_export(struct trace_export *export)
+ {
+ 	if (WARN_ON_ONCE(!export->write))
+ 		return -1;
+ 
+ 	mutex_lock(&ftrace_export_lock);
+ 
+ 	add_ftrace_export(&ftrace_exports_list, export);
+ 
+ 	mutex_unlock(&ftrace_export_lock);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(register_ftrace_export);
+ 
+ int unregister_ftrace_export(struct trace_export *export)
+ {
+ 	int ret;
+ 
+ 	mutex_lock(&ftrace_export_lock);
+ 
+ 	ret = rm_ftrace_export(&ftrace_exports_list, export);
+ 
+ 	mutex_unlock(&ftrace_export_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(unregister_ftrace_export);
+ 
  void
  trace_function(struct trace_array *tr,
  	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
@@@ -1678,19 -2270,13 +1802,26 @@@
  	entry->ip			= ip;
  	entry->parent_ip		= parent_ip;
  
++<<<<<<< HEAD
 +	if (!filter_check_discard(call, entry, buffer, event))
++=======
+ 	if (!call_filter_check_discard(call, entry, buffer, event)) {
+ 		if (static_branch_unlikely(&ftrace_exports_enabled))
+ 			ftrace_exports(event);
++>>>>>>> 478409dd683d (tracing: Add hook to function tracing for other subsystems to use)
  		__buffer_unlock_commit(buffer, event);
+ 	}
  }
  
 +void
 +ftrace(struct trace_array *tr, struct trace_array_cpu *data,
 +       unsigned long ip, unsigned long parent_ip, unsigned long flags,
 +       int pc)
 +{
 +	if (likely(!atomic_read(&data->disabled)))
 +		trace_function(tr, ip, parent_ip, flags, pc);
 +}
 +
  #ifdef CONFIG_STACKTRACE
  
  #define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))
diff --git a/include/linux/trace.h b/include/linux/trace.h
new file mode 100644
index 000000000000..9330a58e2651
--- /dev/null
+++ b/include/linux/trace.h
@@ -0,0 +1,28 @@
+#ifndef _LINUX_TRACE_H
+#define _LINUX_TRACE_H
+
+#ifdef CONFIG_TRACING
+/*
+ * The trace export - an export of Ftrace output. The trace_export
+ * can process traces and export them to a registered destination as
+ * an addition to the current only output of Ftrace - i.e. ring buffer.
+ *
+ * If you want traces to be sent to some other place rather than ring
+ * buffer only, just need to register a new trace_export and implement
+ * its own .write() function for writing traces to the storage.
+ *
+ * next		- pointer to the next trace_export
+ * write	- copy traces which have been delt with ->commit() to
+ *		  the destination
+ */
+struct trace_export {
+	struct trace_export __rcu	*next;
+	void (*write)(const void *, unsigned int);
+};
+
+int register_ftrace_export(struct trace_export *export);
+int unregister_ftrace_export(struct trace_export *export);
+
+#endif	/* CONFIG_TRACING */
+
+#endif	/* _LINUX_TRACE_H */
* Unmerged path kernel/trace/trace.c
