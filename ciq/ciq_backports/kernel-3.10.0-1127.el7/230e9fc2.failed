slab: add SLAB_ACCOUNT flag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Vladimir Davydov <vdavydov@virtuozzo.com>
commit 230e9fc2860450fbb1f33bdcf9093d92d7d91f5b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/230e9fc2.failed

Currently, if we want to account all objects of a particular kmem cache,
we have to pass __GFP_ACCOUNT to each kmem_cache_alloc call, which is
inconvenient.  This patch introduces SLAB_ACCOUNT flag which if passed
to kmem_cache_create will force accounting for every allocation from
this cache even if __GFP_ACCOUNT is not passed.

This patch does not make any of the existing caches use this flag - it
will be done later in the series.

Note, a cache with SLAB_ACCOUNT cannot be merged with a cache w/o
SLAB_ACCOUNT, because merged caches share the same kmem_cache struct and
hence cannot have different sets of SLAB_* flags.  Thus using this flag
will probably reduce the number of merged slabs even if kmem accounting
is not used (only compiled in).

	Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
	Suggested-by: Tejun Heo <tj@kernel.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 230e9fc2860450fbb1f33bdcf9093d92d7d91f5b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/memcontrol.c
#	mm/slab_common.c
diff --cc include/linux/memcontrol.h
index 1494b9982667,5c97265c1c6e..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -477,127 -751,80 +477,167 @@@ static inline bool memcg_kmem_enabled(v
   * conditions, but because they are pretty simple, they are expected to be
   * fast.
   */
 -int __memcg_kmem_charge_memcg(struct page *page, gfp_t gfp, int order,
 -			      struct mem_cgroup *memcg);
 -int __memcg_kmem_charge(struct page *page, gfp_t gfp, int order);
 -void __memcg_kmem_uncharge(struct page *page, int order);
 +bool __memcg_kmem_newpage_charge(gfp_t gfp, struct mem_cgroup **memcg,
 +					int order);
 +void __memcg_kmem_commit_charge(struct page *page,
 +				       struct mem_cgroup *memcg, int order);
 +void __memcg_kmem_uncharge_pages(struct page *page, int order);
  
 -/*
 - * helper for acessing a memcg's index. It will be used as an index in the
 - * child cache array in kmem_cache, and also to derive its name. This function
 - * will return -1 when this is not a kmem-limited memcg.
 +int memcg_cache_id(struct mem_cgroup *memcg);
 +
 +char *memcg_create_cache_name(struct mem_cgroup *memcg,
 +			      struct kmem_cache *root_cache);
 +int memcg_alloc_cache_params(struct mem_cgroup *memcg, struct kmem_cache *s,
 +			     struct kmem_cache *root_cache);
 +void memcg_free_cache_params(struct kmem_cache *s);
 +
 +int memcg_update_cache_size(struct kmem_cache *s, int num_groups);
 +void memcg_update_array_size(int num_groups);
 +
 +struct kmem_cache *
 +__memcg_kmem_get_cache(struct kmem_cache *cachep, gfp_t gfp);
 +
 +void mem_cgroup_destroy_cache(struct kmem_cache *cachep);
 +int __kmem_cache_destroy_memcg_children(struct kmem_cache *s);
 +
 +/**
 + * memcg_kmem_newpage_charge: verify if a new kmem allocation is allowed.
 + * @gfp: the gfp allocation flags.
 + * @memcg: a pointer to the memcg this was charged against.
 + * @order: allocation order.
 + *
 + * returns true if the memcg where the current task belongs can hold this
 + * allocation.
 + *
 + * We return true automatically if this allocation is not to be accounted to
 + * any memcg.
   */
++<<<<<<< HEAD
 +static inline bool
 +memcg_kmem_newpage_charge(gfp_t gfp, struct mem_cgroup **memcg, int order)
 +{
 +	if (!memcg_kmem_enabled())
 +		return true;
 +
 +	/*
 +	 * __GFP_NOFAIL allocations will move on even if charging is not
 +	 * possible. Therefore we don't even try, and have this allocation
 +	 * unaccounted. We could in theory charge it forcibly, but we hope
 +	 * those allocations are rare, and won't be worth the trouble.
 +	 */
 +	if (!(gfp & __GFP_KMEMCG) || (gfp & __GFP_NOFAIL))
 +		return true;
++=======
+ static inline int memcg_cache_id(struct mem_cgroup *memcg)
+ {
+ 	return memcg ? memcg->kmemcg_id : -1;
+ }
+ 
+ struct kmem_cache *__memcg_kmem_get_cache(struct kmem_cache *cachep, gfp_t gfp);
+ void __memcg_kmem_put_cache(struct kmem_cache *cachep);
+ 
+ static inline bool __memcg_kmem_bypass(void)
+ {
+ 	if (!memcg_kmem_enabled())
+ 		return true;
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  	if (in_interrupt() || (!current->mm) || (current->flags & PF_KTHREAD))
  		return true;
 -	return false;
 +
 +	/* If the test is dying, just let it go. */
 +	if (unlikely(fatal_signal_pending(current)))
 +		return true;
 +
 +	return __memcg_kmem_newpage_charge(gfp, memcg, order);
  }
  
  /**
 - * memcg_kmem_charge: charge a kmem page
 - * @page: page to charge
 - * @gfp: reclaim mode
 - * @order: allocation order
 + * memcg_kmem_uncharge_pages: uncharge pages from memcg
 + * @page: pointer to struct page being freed
 + * @order: allocation order.
   *
 - * Returns 0 on success, an error code on failure.
 + * there is no need to specify memcg here, since it is embedded in page_cgroup
   */
++<<<<<<< HEAD
 +static inline void
 +memcg_kmem_uncharge_pages(struct page *page, int order)
++=======
+ static __always_inline int memcg_kmem_charge(struct page *page,
+ 					     gfp_t gfp, int order)
+ {
+ 	if (__memcg_kmem_bypass())
+ 		return 0;
+ 	if (!(gfp & __GFP_ACCOUNT))
+ 		return 0;
+ 	return __memcg_kmem_charge(page, gfp, order);
+ }
+ 
+ /**
+  * memcg_kmem_uncharge: uncharge a kmem page
+  * @page: page to uncharge
+  * @order: allocation order
+  */
+ static __always_inline void memcg_kmem_uncharge(struct page *page, int order)
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  {
  	if (memcg_kmem_enabled())
 -		__memcg_kmem_uncharge(page, order);
 +		__memcg_kmem_uncharge_pages(page, order);
 +}
 +
 +/**
 + * memcg_kmem_commit_charge: embeds correct memcg in a page
 + * @page: pointer to struct page recently allocated
 + * @memcg: the memcg structure we charged against
 + * @order: allocation order.
 + *
 + * Needs to be called after memcg_kmem_newpage_charge, regardless of success or
 + * failure of the allocation. if @page is NULL, this function will revert the
 + * charges. Otherwise, it will commit the memcg given by @memcg to the
 + * corresponding page_cgroup.
 + */
 +static inline void
 +memcg_kmem_commit_charge(struct page *page, struct mem_cgroup *memcg, int order)
 +{
 +	if (memcg_kmem_enabled() && memcg)
 +		__memcg_kmem_commit_charge(page, memcg, order);
  }
  
  /**
   * memcg_kmem_get_cache: selects the correct per-memcg cache for allocation
   * @cachep: the original global kmem cache
-  * @gfp: allocation flags.
   *
 - * All memory allocated from a per-memcg cache is charged to the owner memcg.
 + * This function assumes that the task allocating, which determines the memcg
 + * in the page allocator, belongs to the same cgroup throughout the whole
 + * process.  Misacounting can happen if the task calls memcg_kmem_get_cache()
 + * while belonging to a cgroup, and later on changes. This is considered
 + * acceptable, and should only happen upon task migration.
 + *
 + * Before the cache is created by the memcg core, there is also a possible
 + * imbalance: the task belongs to a memcg, but the cache being allocated from
 + * is the global cache, since the child cache is not yet guaranteed to be
 + * ready. This case is also fine, since in this case the GFP_KMEMCG will not be
 + * passed and the page allocator will not attempt any cgroup accounting.
   */
  static __always_inline struct kmem_cache *
  memcg_kmem_get_cache(struct kmem_cache *cachep, gfp_t gfp)
  {
++<<<<<<< HEAD
 +	if (!memcg_kmem_enabled())
 +		return cachep;
 +	if (gfp & __GFP_NOFAIL)
 +		return cachep;
 +	if (in_interrupt() || (!current->mm) || (current->flags & PF_KTHREAD))
 +		return cachep;
 +	if (unlikely(fatal_signal_pending(current)))
 +		return cachep;
++=======
+ 	if (__memcg_kmem_bypass())
+ 		return cachep;
+ 	return __memcg_kmem_get_cache(cachep, gfp);
+ }
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  
 -static __always_inline void memcg_kmem_put_cache(struct kmem_cache *cachep)
 -{
 -	if (memcg_kmem_enabled())
 -		__memcg_kmem_put_cache(cachep);
 +	return __memcg_kmem_get_cache(cachep, gfp);
  }
  #else
  #define for_each_memcg_cache_index(_idx)	\
diff --cc mm/memcontrol.c
index 16fe5606ed59,4bd6c4513393..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -3487,34 -2356,31 +3487,48 @@@ static void memcg_create_cache_enqueue(
   * Can't be called in interrupt context or from kernel threads.
   * This function needs to be called with rcu_read_lock() held.
   */
++<<<<<<< HEAD
 +struct kmem_cache *__memcg_kmem_get_cache(struct kmem_cache *cachep,
 +					  gfp_t gfp)
++=======
+ struct kmem_cache *__memcg_kmem_get_cache(struct kmem_cache *cachep, gfp_t gfp)
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  {
  	struct mem_cgroup *memcg;
  	struct kmem_cache *memcg_cachep;
 -	int kmemcg_id;
  
 -	VM_BUG_ON(!is_root_cache(cachep));
 +	VM_BUG_ON(!cachep->memcg_params);
 +	VM_BUG_ON(!cachep->memcg_params->is_root_cache);
  
++<<<<<<< HEAD
 +	if (!current->mm || current->memcg_kmem_skip_account)
++=======
+ 	if (cachep->flags & SLAB_ACCOUNT)
+ 		gfp |= __GFP_ACCOUNT;
+ 
+ 	if (!(gfp & __GFP_ACCOUNT))
+ 		return cachep;
+ 
+ 	if (current->memcg_kmem_skip_account)
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  		return cachep;
  
 -	memcg = get_mem_cgroup_from_mm(current->mm);
 -	kmemcg_id = READ_ONCE(memcg->kmemcg_id);
 -	if (kmemcg_id < 0)
 +	rcu_read_lock();
 +	memcg = mem_cgroup_from_task(rcu_dereference(current->mm->owner));
 +
 +	if (!memcg_can_account_kmem(memcg))
  		goto out;
  
 -	memcg_cachep = cache_from_memcg_idx(cachep, kmemcg_id);
 -	if (likely(memcg_cachep))
 -		return memcg_cachep;
 +	memcg_cachep = cache_from_memcg_idx(cachep, memcg_cache_id(memcg));
 +	if (likely(memcg_cachep)) {
 +		cachep = memcg_cachep;
 +		goto out;
 +	}
 +
 +	/* The corresponding put will be done in the workqueue. */
 +	if (!css_tryget(&memcg->css))
 +		goto out;
 +	rcu_read_unlock();
  
  	/*
  	 * If we are in a safe context (can wait, and not in interrupt
diff --cc mm/slab_common.c
index 288b69b9b33e,e016178063e1..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -27,6 -30,43 +27,46 @@@ LIST_HEAD(slab_caches)
  DEFINE_MUTEX(slab_mutex);
  struct kmem_cache *kmem_cache;
  
++<<<<<<< HEAD
++=======
+ /*
+  * Set of flags that will prevent slab merging
+  */
+ #define SLAB_NEVER_MERGE (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \
+ 		SLAB_TRACE | SLAB_DESTROY_BY_RCU | SLAB_NOLEAKTRACE | \
+ 		SLAB_FAILSLAB)
+ 
+ #define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \
+ 			 SLAB_NOTRACK | SLAB_ACCOUNT)
+ 
+ /*
+  * Merge control. If this is set then no merging of slab caches will occur.
+  * (Could be removed. This was introduced to pacify the merge skeptics.)
+  */
+ static int slab_nomerge;
+ 
+ static int __init setup_slab_nomerge(char *str)
+ {
+ 	slab_nomerge = 1;
+ 	return 1;
+ }
+ 
+ #ifdef CONFIG_SLUB
+ __setup_param("slub_nomerge", slub_nomerge, setup_slab_nomerge, 0);
+ #endif
+ 
+ __setup("slab_nomerge", setup_slab_nomerge);
+ 
+ /*
+  * Determine the size of a slab object
+  */
+ unsigned int kmem_cache_size(struct kmem_cache *s)
+ {
+ 	return s->object_size;
+ }
+ EXPORT_SYMBOL(kmem_cache_size);
+ 
++>>>>>>> 230e9fc28604 (slab: add SLAB_ACCOUNT flag)
  #ifdef CONFIG_DEBUG_VM
  static int kmem_cache_sanity_check(const char *name, size_t size)
  {
* Unmerged path include/linux/memcontrol.h
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 08eb6cb8d885..aa082b426aa0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -78,6 +78,11 @@
 #else
 # define SLAB_FAILSLAB		0x00000000UL
 #endif
+#ifdef CONFIG_MEMCG_KMEM
+# define SLAB_ACCOUNT		0x04000000UL	/* Account to memcg */
+#else
+# define SLAB_ACCOUNT		0x00000000UL
+#endif
 
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
* Unmerged path mm/memcontrol.c
diff --git a/mm/slab.h b/mm/slab.h
index 229f59086a96..893e9252952f 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -80,10 +80,11 @@ __kmem_cache_alias(const char *name, size_t size, size_t align,
 
 #if defined(CONFIG_SLAB)
 #define SLAB_CACHE_FLAGS (SLAB_MEM_SPREAD | SLAB_NOLEAKTRACE | \
-			  SLAB_RECLAIM_ACCOUNT | SLAB_TEMPORARY | SLAB_NOTRACK)
+			  SLAB_RECLAIM_ACCOUNT | SLAB_TEMPORARY | \
+			  SLAB_NOTRACK | SLAB_ACCOUNT)
 #elif defined(CONFIG_SLUB)
 #define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE | SLAB_RECLAIM_ACCOUNT | \
-			  SLAB_TEMPORARY | SLAB_NOTRACK)
+			  SLAB_TEMPORARY | SLAB_NOTRACK | SLAB_ACCOUNT)
 #else
 #define SLAB_CACHE_FLAGS (0)
 #endif
* Unmerged path mm/slab_common.c
diff --git a/mm/slub.c b/mm/slub.c
index cfd46f5b97ec..a86201254871 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -5515,6 +5515,8 @@ static char *create_unique_id(struct kmem_cache *s)
 		*p++ = 'F';
 	if (!(s->flags & SLAB_NOTRACK))
 		*p++ = 't';
+	if (s->flags & SLAB_ACCOUNT)
+		*p++ = 'A';
 	if (p != name + 1)
 		*p++ = '-';
 	p += sprintf(p, "%07d", s->size);
