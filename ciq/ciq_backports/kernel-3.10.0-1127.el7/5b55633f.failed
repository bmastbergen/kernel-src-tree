s390/qeth: guard against runt packets

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Julian Wiedmann <jwi@linux.ibm.com>
commit 5b55633f20ee1bb253dc7d915ec2fd35fd865d5a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/5b55633f.failed

Depending on a packet's type, the RX path needs to access fields in the
packet headers and thus requires a minimum packet length.
Enforce this length when building the skb.

On the other hand a single runt packet is no reason to drop the whole
RX buffer. So just skip it, and continue processing on the next packet.

Fixes: 4a71df50047f ("qeth: new qeth device driver")
	Signed-off-by: Julian Wiedmann <jwi@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 5b55633f20ee1bb253dc7d915ec2fd35fd865d5a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/net/qeth_core.h
#	drivers/s390/net/qeth_core_main.c
#	drivers/s390/net/qeth_ethtool.c
diff --cc drivers/s390/net/qeth_core.h
index 607e0ba878ac,7cdebd2e329f..000000000000
--- a/drivers/s390/net/qeth_core.h
+++ b/drivers/s390/net/qeth_core.h
@@@ -480,18 -465,68 +480,72 @@@ enum qeth_out_q_states 
         QETH_OUT_Q_LOCKED_FLUSH,
  };
  
++<<<<<<< HEAD
++=======
+ #define QETH_CARD_STAT_ADD(_c, _stat, _val)	((_c)->stats._stat += (_val))
+ #define QETH_CARD_STAT_INC(_c, _stat)		QETH_CARD_STAT_ADD(_c, _stat, 1)
+ 
+ #define QETH_TXQ_STAT_ADD(_q, _stat, _val)	((_q)->stats._stat += (_val))
+ #define QETH_TXQ_STAT_INC(_q, _stat)		QETH_TXQ_STAT_ADD(_q, _stat, 1)
+ 
+ struct qeth_card_stats {
+ 	u64 rx_bufs;
+ 	u64 rx_skb_csum;
+ 	u64 rx_sg_skbs;
+ 	u64 rx_sg_frags;
+ 	u64 rx_sg_alloc_page;
+ 
+ 	u64 rx_dropped_nomem;
+ 	u64 rx_dropped_notsupp;
+ 	u64 rx_dropped_runt;
+ 
+ 	/* rtnl_link_stats64 */
+ 	u64 rx_packets;
+ 	u64 rx_bytes;
+ 	u64 rx_multicast;
+ 	u64 rx_length_errors;
+ 	u64 rx_frame_errors;
+ 	u64 rx_fifo_errors;
+ };
+ 
+ struct qeth_out_q_stats {
+ 	u64 bufs;
+ 	u64 bufs_pack;
+ 	u64 buf_elements;
+ 	u64 skbs_pack;
+ 	u64 skbs_sg;
+ 	u64 skbs_csum;
+ 	u64 skbs_tso;
+ 	u64 skbs_linearized;
+ 	u64 skbs_linearized_fail;
+ 	u64 tso_bytes;
+ 	u64 packing_mode_switch;
+ 	u64 stopped;
+ 	u64 completion_yield;
+ 	u64 completion_timer;
+ 
+ 	/* rtnl_link_stats64 */
+ 	u64 tx_packets;
+ 	u64 tx_bytes;
+ 	u64 tx_errors;
+ 	u64 tx_dropped;
+ };
+ 
+ #define QETH_TX_TIMER_USECS		500
+ 
++>>>>>>> 5b55633f20ee (s390/qeth: guard against runt packets)
  struct qeth_qdio_out_q {
 -	struct qdio_buffer *qdio_bufs[QDIO_MAX_BUFFERS_PER_Q];
 +	struct qdio_buffer qdio_bufs[QDIO_MAX_BUFFERS_PER_Q];
  	struct qeth_qdio_out_buffer *bufs[QDIO_MAX_BUFFERS_PER_Q];
  	struct qdio_outbuf_state *bufstates; /* convenience pointer */
 -	struct qeth_out_q_stats stats;
 -	u8 next_buf_to_fill;
 -	u8 max_elements;
 -	u8 queue_no;
 -	u8 do_pack;
 +	int queue_no;
  	struct qeth_card *card;
  	atomic_t state;
 +	int do_pack;
 +	/*
 +	 * index of buffer to be filled by driver; state EMPTY or PACKING
 +	 */
 +	int next_buf_to_fill;
  	/*
  	 * number of buffers that are currently filled (PRIMED)
  	 * -> these buffers are hardware-owned
diff --cc drivers/s390/net/qeth_core_main.c
index b1a62a620d2b,7285484212de..000000000000
--- a/drivers/s390/net/qeth_core_main.c
+++ b/drivers/s390/net/qeth_core_main.c
@@@ -5156,8 -5063,10 +5156,14 @@@ struct sk_buff *qeth_core_get_next_skb(
  {
  	struct qdio_buffer_element *element = *__element;
  	struct qdio_buffer *buffer = qethbuffer->buffer;
+ 	unsigned int headroom, linear_len;
  	int offset = *__offset;
++<<<<<<< HEAD
 +	struct sk_buff *skb = NULL;
++=======
+ 	bool use_rx_sg = false;
+ 	struct sk_buff *skb;
++>>>>>>> 5b55633f20ee (s390/qeth: guard against runt packets)
  	int skb_len = 0;
  	void *data_ptr;
  	int data_len;
@@@ -5180,35 -5087,74 +5188,69 @@@
  	switch ((*hdr)->hdr.l2.id) {
  	case QETH_HEADER_TYPE_LAYER2:
  		skb_len = (*hdr)->hdr.l2.pkt_length;
++<<<<<<< HEAD
++		break;
++	case QETH_HEADER_TYPE_LAYER3:
++		skb_len = (*hdr)->hdr.l3.length;
++=======
+ 		linear_len = ETH_HLEN;
+ 		headroom = 0;
  		break;
  	case QETH_HEADER_TYPE_LAYER3:
  		skb_len = (*hdr)->hdr.l3.length;
+ 		if (!IS_LAYER3(card)) {
+ 			QETH_CARD_STAT_INC(card, rx_dropped_notsupp);
+ 			goto walk_packet;
+ 		}
+ 
+ 		if ((*hdr)->hdr.l3.flags & QETH_HDR_PASSTHRU) {
+ 			linear_len = ETH_HLEN;
+ 			headroom = 0;
+ 			break;
+ 		}
+ 
+ 		if ((*hdr)->hdr.l3.flags & QETH_HDR_IPV6)
+ 			linear_len = sizeof(struct ipv6hdr);
+ 		else
+ 			linear_len = sizeof(struct iphdr);
++>>>>>>> 5b55633f20ee (s390/qeth: guard against runt packets)
  		headroom = ETH_HLEN;
  		break;
  	case QETH_HEADER_TYPE_OSN:
  		skb_len = (*hdr)->hdr.osn.pdu_length;
++<<<<<<< HEAD
++=======
+ 		if (!IS_OSN(card)) {
+ 			QETH_CARD_STAT_INC(card, rx_dropped_notsupp);
+ 			goto walk_packet;
+ 		}
+ 
+ 		linear_len = skb_len;
++>>>>>>> 5b55633f20ee (s390/qeth: guard against runt packets)
  		headroom = sizeof(struct qeth_hdr);
  		break;
  	default:
 -		if ((*hdr)->hdr.l2.id & QETH_HEADER_MASK_INVAL)
 -			QETH_CARD_STAT_INC(card, rx_frame_errors);
 -		else
 -			QETH_CARD_STAT_INC(card, rx_dropped_notsupp);
 -
 -		/* Can't determine packet length, drop the whole buffer. */
 -		return NULL;
 +		break;
  	}
  
- 	if (!skb_len)
- 		return NULL;
+ 	if (skb_len < linear_len) {
+ 		QETH_CARD_STAT_INC(card, rx_dropped_runt);
+ 		goto walk_packet;
+ 	}
  
 -	use_rx_sg = (card->options.cq == QETH_CQ_ENABLED) ||
 -		    ((skb_len >= card->options.rx_sg_cb) &&
 -		     !atomic_read(&card->force_alloc_skb) &&
 -		     !IS_OSN(card));
 -
 -	if (use_rx_sg && qethbuffer->rx_skb) {
 -		/* QETH_CQ_ENABLED only: */
 -		skb = qethbuffer->rx_skb;
 -		qethbuffer->rx_skb = NULL;
 +	if (((skb_len >= card->options.rx_sg_cb) &&
 +	     (!(card->info.type == QETH_CARD_TYPE_OSN)) &&
 +	     (!atomic_read(&card->force_alloc_skb))) ||
 +	    (card->options.cq == QETH_CQ_ENABLED)) {
 +		use_rx_sg = 1;
  	} else {
 -		unsigned int linear = (use_rx_sg) ? QETH_RX_PULL_LEN : skb_len;
 -
 -		skb = napi_alloc_skb(&card->napi, linear + headroom);
 +		skb = dev_alloc_skb(skb_len + headroom);
 +		if (!skb)
 +			goto no_mem;
 +		if (headroom)
 +			skb_reserve(skb, headroom);
  	}
  
 -	if (!skb)
 -		QETH_CARD_STAT_INC(card, rx_dropped_nomem);
 -	else if (headroom)
 -		skb_reserve(skb, headroom);
 -
 -walk_packet:
  	data_ptr = element->addr + offset;
  	while (skb_len) {
  		data_len = min(skb_len, (int)(element->length - offset));
@@@ -6096,50 -6107,269 +6138,183 @@@ static int qeth_send_checksum_command(s
  	return 0;
  }
  
 -static int qeth_set_ipa_tso(struct qeth_card *card, bool on,
 -			    enum qeth_prot_versions prot)
 -{
 -	return on ? qeth_set_tso_on(card, prot) : qeth_set_tso_off(card, prot);
 -}
 -
 -static int qeth_set_ipa_rx_csum(struct qeth_card *card, bool on)
 -{
 -	int rc_ipv4 = (on) ? -EOPNOTSUPP : 0;
 -	int rc_ipv6;
 -
 -	if (qeth_is_supported(card, IPA_INBOUND_CHECKSUM))
 -		rc_ipv4 = qeth_set_ipa_csum(card, on, IPA_INBOUND_CHECKSUM,
 -					    QETH_PROT_IPV4);
 -	if (!qeth_is_supported6(card, IPA_INBOUND_CHECKSUM_V6))
 -		/* no/one Offload Assist available, so the rc is trivial */
 -		return rc_ipv4;
 -
 -	rc_ipv6 = qeth_set_ipa_csum(card, on, IPA_INBOUND_CHECKSUM,
 -				    QETH_PROT_IPV6);
 -
 -	if (on)
 -		/* enable: success if any Assist is active */
 -		return (rc_ipv6) ? rc_ipv4 : 0;
 -
 -	/* disable: failure if any Assist is still active */
 -	return (rc_ipv6) ? rc_ipv6 : rc_ipv4;
 -}
 -
 -/**
 - * qeth_enable_hw_features() - (Re-)Enable HW functions for device features
 - * @dev:	a net_device
 - */
 -void qeth_enable_hw_features(struct net_device *dev)
 -{
 -	struct qeth_card *card = dev->ml_priv;
 -	netdev_features_t features;
 -
 -	features = dev->features;
 -	/* force-off any feature that might need an IPA sequence.
 -	 * netdev_update_features() will restart them.
 -	 */
 -	dev->features &= ~dev->hw_features;
 -	/* toggle VLAN filter, so that VIDs are re-programmed: */
 -	if (IS_LAYER2(card) && IS_VM_NIC(card)) {
 -		dev->features &= ~NETIF_F_HW_VLAN_CTAG_FILTER;
 -		dev->wanted_features |= NETIF_F_HW_VLAN_CTAG_FILTER;
 -	}
 -	netdev_update_features(dev);
 -	if (features != dev->features)
 -		dev_warn(&card->gdev->dev,
 -			 "Device recovery failed to restore all offload features\n");
 -}
 -EXPORT_SYMBOL_GPL(qeth_enable_hw_features);
 -
 -int qeth_set_features(struct net_device *dev, netdev_features_t features)
 +int qeth_set_rx_csum(struct qeth_card *card, int on)
  {
 -	struct qeth_card *card = dev->ml_priv;
 -	netdev_features_t changed = dev->features ^ features;
 -	int rc = 0;
 -
 -	QETH_CARD_TEXT(card, 2, "setfeat");
 -	QETH_CARD_HEX(card, 2, &features, sizeof(features));
 +	int rc;
  
 -	if ((changed & NETIF_F_IP_CSUM)) {
 -		rc = qeth_set_ipa_csum(card, features & NETIF_F_IP_CSUM,
 -				       IPA_OUTBOUND_CHECKSUM, QETH_PROT_IPV4);
 -		if (rc)
 -			changed ^= NETIF_F_IP_CSUM;
 -	}
 -	if (changed & NETIF_F_IPV6_CSUM) {
 -		rc = qeth_set_ipa_csum(card, features & NETIF_F_IPV6_CSUM,
 -				       IPA_OUTBOUND_CHECKSUM, QETH_PROT_IPV6);
 -		if (rc)
 -			changed ^= NETIF_F_IPV6_CSUM;
 -	}
 -	if (changed & NETIF_F_RXCSUM) {
 -		rc = qeth_set_ipa_rx_csum(card, features & NETIF_F_RXCSUM);
 -		if (rc)
 -			changed ^= NETIF_F_RXCSUM;
 -	}
 -	if (changed & NETIF_F_TSO) {
 -		rc = qeth_set_ipa_tso(card, features & NETIF_F_TSO,
 -				      QETH_PROT_IPV4);
 +	if (on) {
 +		rc = qeth_send_checksum_command(card);
  		if (rc)
 -			changed ^= NETIF_F_TSO;
 -	}
 -	if (changed & NETIF_F_TSO6) {
 -		rc = qeth_set_ipa_tso(card, features & NETIF_F_TSO6,
 -				      QETH_PROT_IPV6);
 +			return -EIO;
 +		dev_info(&card->gdev->dev,
 +			"HW Checksumming (inbound) enabled\n");
 +	} else {
 +		rc = qeth_send_simple_setassparms(card,
 +			IPA_INBOUND_CHECKSUM, IPA_CMD_ASS_STOP, 0);
  		if (rc)
 -			changed ^= NETIF_F_TSO6;
 +			return -EIO;
  	}
 -
 -	/* everything changed successfully? */
 -	if ((dev->features ^ features) == changed)
 -		return 0;
 -	/* something went wrong. save changed features and return error */
 -	dev->features ^= changed;
 -	return -EIO;
 +	return 0;
  }
 -EXPORT_SYMBOL_GPL(qeth_set_features);
 +EXPORT_SYMBOL_GPL(qeth_set_rx_csum);
  
 -netdev_features_t qeth_fix_features(struct net_device *dev,
 -				    netdev_features_t features)
 +int qeth_start_ipa_tx_checksum(struct qeth_card *card)
  {
 -	struct qeth_card *card = dev->ml_priv;
 +	int rc = 0;
  
 -	QETH_CARD_TEXT(card, 2, "fixfeat");
  	if (!qeth_is_supported(card, IPA_OUTBOUND_CHECKSUM))
 -		features &= ~NETIF_F_IP_CSUM;
 -	if (!qeth_is_supported6(card, IPA_OUTBOUND_CHECKSUM_V6))
 -		features &= ~NETIF_F_IPV6_CSUM;
 -	if (!qeth_is_supported(card, IPA_INBOUND_CHECKSUM) &&
 -	    !qeth_is_supported6(card, IPA_INBOUND_CHECKSUM_V6))
 -		features &= ~NETIF_F_RXCSUM;
 -	if (!qeth_is_supported(card, IPA_OUTBOUND_TSO))
 -		features &= ~NETIF_F_TSO;
 -	if (!qeth_is_supported6(card, IPA_OUTBOUND_TSO))
 -		features &= ~NETIF_F_TSO6;
 -
 -	QETH_CARD_HEX(card, 2, &features, sizeof(features));
 -	return features;
 +		return rc;
 +	rc = qeth_send_simple_setassparms(card, IPA_OUTBOUND_CHECKSUM,
 +					  IPA_CMD_ASS_START, 0);
 +	if (rc)
 +		goto err_out;
 +	rc = qeth_send_simple_setassparms(card, IPA_OUTBOUND_CHECKSUM,
 +					  IPA_CMD_ASS_ENABLE,
 +					  card->info.tx_csum_mask);
 +	if (rc)
 +		goto err_out;
 +
 +	dev_info(&card->gdev->dev, "HW TX Checksumming enabled\n");
 +	return rc;
 +err_out:
 +	dev_warn(&card->gdev->dev, "Enabling HW TX checksumming for %s "
 +		"failed, using SW TX checksumming\n", QETH_CARD_IFNAME(card));
 +	return rc;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(qeth_start_ipa_tx_checksum);
++=======
+ EXPORT_SYMBOL_GPL(qeth_fix_features);
+ 
+ netdev_features_t qeth_features_check(struct sk_buff *skb,
+ 				      struct net_device *dev,
+ 				      netdev_features_t features)
+ {
+ 	/* GSO segmentation builds skbs with
+ 	 *	a (small) linear part for the headers, and
+ 	 *	page frags for the data.
+ 	 * Compared to a linear skb, the header-only part consumes an
+ 	 * additional buffer element. This reduces buffer utilization, and
+ 	 * hurts throughput. So compress small segments into one element.
+ 	 */
+ 	if (netif_needs_gso(skb, features)) {
+ 		/* match skb_segment(): */
+ 		unsigned int doffset = skb->data - skb_mac_header(skb);
+ 		unsigned int hsize = skb_shinfo(skb)->gso_size;
+ 		unsigned int hroom = skb_headroom(skb);
+ 
+ 		/* linearize only if resulting skb allocations are order-0: */
+ 		if (SKB_DATA_ALIGN(hroom + doffset + hsize) <= SKB_MAX_HEAD(0))
+ 			features &= ~NETIF_F_SG;
+ 	}
+ 
+ 	return vlan_features_check(skb, features);
+ }
+ EXPORT_SYMBOL_GPL(qeth_features_check);
+ 
+ void qeth_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+ {
+ 	struct qeth_card *card = dev->ml_priv;
+ 	struct qeth_qdio_out_q *queue;
+ 	unsigned int i;
+ 
+ 	QETH_CARD_TEXT(card, 5, "getstat");
+ 
+ 	stats->rx_packets = card->stats.rx_packets;
+ 	stats->rx_bytes = card->stats.rx_bytes;
+ 	stats->rx_errors = card->stats.rx_length_errors +
+ 			   card->stats.rx_frame_errors +
+ 			   card->stats.rx_fifo_errors;
+ 	stats->rx_dropped = card->stats.rx_dropped_nomem +
+ 			    card->stats.rx_dropped_notsupp +
+ 			    card->stats.rx_dropped_runt;
+ 	stats->multicast = card->stats.rx_multicast;
+ 	stats->rx_length_errors = card->stats.rx_length_errors;
+ 	stats->rx_frame_errors = card->stats.rx_frame_errors;
+ 	stats->rx_fifo_errors = card->stats.rx_fifo_errors;
+ 
+ 	for (i = 0; i < card->qdio.no_out_queues; i++) {
+ 		queue = card->qdio.out_qs[i];
+ 
+ 		stats->tx_packets += queue->stats.tx_packets;
+ 		stats->tx_bytes += queue->stats.tx_bytes;
+ 		stats->tx_errors += queue->stats.tx_errors;
+ 		stats->tx_dropped += queue->stats.tx_dropped;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(qeth_get_stats64);
+ 
+ u16 qeth_iqd_select_queue(struct net_device *dev, struct sk_buff *skb,
+ 			  u8 cast_type, struct net_device *sb_dev)
+ {
+ 	if (cast_type != RTN_UNICAST)
+ 		return QETH_IQD_MCAST_TXQ;
+ 	return QETH_IQD_MIN_UCAST_TXQ;
+ }
+ EXPORT_SYMBOL_GPL(qeth_iqd_select_queue);
+ 
+ int qeth_open(struct net_device *dev)
+ {
+ 	struct qeth_card *card = dev->ml_priv;
+ 
+ 	QETH_CARD_TEXT(card, 4, "qethopen");
+ 
+ 	if (qdio_stop_irq(CARD_DDEV(card), 0) < 0)
+ 		return -EIO;
+ 
+ 	card->data.state = CH_STATE_UP;
+ 	netif_tx_start_all_queues(dev);
+ 
+ 	napi_enable(&card->napi);
+ 	local_bh_disable();
+ 	napi_schedule(&card->napi);
+ 	if (IS_IQD(card)) {
+ 		struct qeth_qdio_out_q *queue;
+ 		unsigned int i;
+ 
+ 		qeth_for_each_output_queue(card, queue, i) {
+ 			netif_tx_napi_add(dev, &queue->napi, qeth_tx_poll,
+ 					  QETH_NAPI_WEIGHT);
+ 			napi_enable(&queue->napi);
+ 			napi_schedule(&queue->napi);
+ 		}
+ 	}
+ 	/* kick-start the NAPI softirq: */
+ 	local_bh_enable();
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(qeth_open);
+ 
+ int qeth_stop(struct net_device *dev)
+ {
+ 	struct qeth_card *card = dev->ml_priv;
+ 
+ 	QETH_CARD_TEXT(card, 4, "qethstop");
+ 	if (IS_IQD(card)) {
+ 		struct qeth_qdio_out_q *queue;
+ 		unsigned int i;
+ 
+ 		/* Quiesce the NAPI instances: */
+ 		qeth_for_each_output_queue(card, queue, i) {
+ 			napi_disable(&queue->napi);
+ 			del_timer_sync(&queue->timer);
+ 		}
+ 
+ 		/* Stop .ndo_start_xmit, might still access queue->napi. */
+ 		netif_tx_disable(dev);
+ 
+ 		/* Queues may get re-allocated, so remove the NAPIs here. */
+ 		qeth_for_each_output_queue(card, queue, i)
+ 			netif_napi_del(&queue->napi);
+ 	} else {
+ 		netif_tx_disable(dev);
+ 	}
+ 
+ 	napi_disable(&card->napi);
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(qeth_stop);
++>>>>>>> 5b55633f20ee (s390/qeth: guard against runt packets)
  
  static int __init qeth_core_init(void)
  {
* Unmerged path drivers/s390/net/qeth_ethtool.c
* Unmerged path drivers/s390/net/qeth_core.h
* Unmerged path drivers/s390/net/qeth_core_main.c
* Unmerged path drivers/s390/net/qeth_ethtool.c
