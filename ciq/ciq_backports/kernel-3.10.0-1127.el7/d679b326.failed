KVM: x86: remove now unneeded hugepage gfn adjustment

jira LE-1907
cve CVE-2018-12207
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit d679b32611c0102ce33b9e1a4e4b94854ed1812a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/d679b326.failed

After the previous patch, the low bits of the gfn are masked in
both FNAME(fetch) and __direct_map, so we do not need to clear them
in transparent_hugepage_adjust.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d679b32611c0102ce33b9e1a4e4b94854ed1812a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 24ebfb6e8f2e,084c1a0d9f98..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3417,14 -3533,8 +3414,19 @@@ static int nonpaging_map(struct kvm_vcp
  	if (make_mmu_pages_available(vcpu) < 0)
  		goto out_unlock;
  	if (likely(!force_pt_level))
++<<<<<<< HEAD
 +		transparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);
 +	r = __direct_map(vcpu, v, write, map_writable, level, gfn, pfn,
 +			 prefault);
 +	spin_unlock(&vcpu->kvm->mmu_lock);
 +
 +
 +	return r;
 +
++=======
+ 		transparent_hugepage_adjust(vcpu, gfn, &pfn, &level);
+ 	r = __direct_map(vcpu, v, write, map_writable, level, pfn, prefault);
++>>>>>>> d679b32611c0 (KVM: x86: remove now unneeded hugepage gfn adjustment)
  out_unlock:
  	spin_unlock(&vcpu->kvm->mmu_lock);
  	kvm_release_pfn_clean(pfn);
@@@ -4062,13 -4159,8 +4064,18 @@@ static int tdp_page_fault(struct kvm_vc
  	if (make_mmu_pages_available(vcpu) < 0)
  		goto out_unlock;
  	if (likely(!force_pt_level))
++<<<<<<< HEAD
 +		transparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);
 +	r = __direct_map(vcpu, gpa, write, map_writable,
 +			 level, gfn, pfn, prefault);
 +	spin_unlock(&vcpu->kvm->mmu_lock);
 +
 +	return r;
 +
++=======
+ 		transparent_hugepage_adjust(vcpu, gfn, &pfn, &level);
+ 	r = __direct_map(vcpu, gpa, write, map_writable, level, pfn, prefault);
++>>>>>>> d679b32611c0 (KVM: x86: remove now unneeded hugepage gfn adjustment)
  out_unlock:
  	spin_unlock(&vcpu->kvm->mmu_lock);
  	kvm_release_pfn_clean(pfn);
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index 0da31d8cbf16..dc9a96c277d3 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -830,7 +830,7 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,
 	if (make_mmu_pages_available(vcpu) < 0)
 		goto out_unlock;
 	if (!force_pt_level)
-		transparent_hugepage_adjust(vcpu, &walker.gfn, &pfn, &level);
+		transparent_hugepage_adjust(vcpu, walker.gfn, &pfn, &level);
 	r = FNAME(fetch)(vcpu, addr, &walker, write_fault,
 			 level, pfn, map_writable, prefault);
 	++vcpu->stat.pf_fixed;
