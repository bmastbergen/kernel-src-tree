mm, hugetlb: do not allocate non-migrateable gigantic pages from movable zones

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Michal Hocko <mhocko@suse.com>
commit 79b63f12abcbbd2caf7064b294af648a87de07ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/79b63f12.failed

alloc_gigantic_page doesn't consider movability of the gigantic hugetlb
when scanning eligible ranges for the allocation.  As 1GB hugetlb pages
are not movable currently this can break the movable zone assumption
that all allocations are migrateable and as such break memory hotplug.

Reorganize the code and use the standard zonelist allocations scheme
that we use for standard hugetbl pages.  htlb_alloc_mask will ensure
that only migratable hugetlb pages will ever see a movable zone.

Link: http://lkml.kernel.org/r/20170803083549.21407-1-mhocko@kernel.org
Fixes: 944d9fec8d7a ("hugetlb: add support for gigantic page allocation at runtime")
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Luiz Capitulino <lcapitulino@redhat.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 79b63f12abcbbd2caf7064b294af648a87de07ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 0f91e2c9c2c6,34625b257128..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1059,10 -1066,11 +1059,15 @@@ static void free_gigantic_page(struct p
  }
  
  static int __alloc_gigantic_page(unsigned long start_pfn,
- 				unsigned long nr_pages)
+ 				unsigned long nr_pages, gfp_t gfp_mask)
  {
  	unsigned long end_pfn = start_pfn + nr_pages;
++<<<<<<< HEAD
 +	return alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE);
++=======
+ 	return alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE,
+ 				  gfp_mask);
++>>>>>>> 79b63f12abcb (mm, hugetlb: do not allocate non-migrateable gigantic pages from movable zones)
  }
  
  static bool pfn_range_valid_gigantic(struct zone *z,
@@@ -1100,19 -1108,24 +1105,28 @@@ static bool zone_spans_last_pfn(const s
  	return zone_spans_pfn(zone, last_pfn);
  }
  
++<<<<<<< HEAD
 +static struct page *alloc_gigantic_page(int nid, unsigned order)
++=======
+ static struct page *alloc_gigantic_page(int nid, struct hstate *h)
++>>>>>>> 79b63f12abcb (mm, hugetlb: do not allocate non-migrateable gigantic pages from movable zones)
  {
+ 	unsigned int order = huge_page_order(h);
  	unsigned long nr_pages = 1 << order;
  	unsigned long ret, pfn, flags;
- 	struct zone *z;
+ 	struct zonelist *zonelist;
+ 	struct zone *zone;
+ 	struct zoneref *z;
+ 	gfp_t gfp_mask;
  
- 	z = NODE_DATA(nid)->node_zones;
- 	for (; z - NODE_DATA(nid)->node_zones < MAX_NR_ZONES; z++) {
- 		spin_lock_irqsave(&z->lock, flags);
+ 	gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
+ 	zonelist = node_zonelist(nid, gfp_mask);
+ 	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), NULL) {
+ 		spin_lock_irqsave(&zone->lock, flags);
  
- 		pfn = ALIGN(z->zone_start_pfn, nr_pages);
- 		while (zone_spans_last_pfn(z, pfn, nr_pages)) {
- 			if (pfn_range_valid_gigantic(z, pfn, nr_pages)) {
+ 		pfn = ALIGN(zone->zone_start_pfn, nr_pages);
+ 		while (zone_spans_last_pfn(zone, pfn, nr_pages)) {
+ 			if (pfn_range_valid_gigantic(zone, pfn, nr_pages)) {
  				/*
  				 * We release the zone lock here because
  				 * alloc_contig_range() will also lock the zone
* Unmerged path mm/hugetlb.c
