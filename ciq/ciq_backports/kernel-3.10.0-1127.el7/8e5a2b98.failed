percpu: update free path with correct new free region

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou <dennis@kernel.org>
commit 8e5a2b9893f36457582596fdade10f6feb2797ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/8e5a2b98.failed

When updating the chunk's contig_hint on the free path of a hint that
does not touch the page boundaries, it was incorrectly using the
starting offset of the free region and the block's contig_hint. This
could lead to incorrect assumptions about fit given a size and better
alignment of the start. Fix this by using (end - start) as this is only
called when updating a hint within a block.

	Signed-off-by: Dennis Zhou <dennis@kernel.org>
	Reviewed-by: Peng Fan <peng.fan@nxp.com>
(cherry picked from commit 8e5a2b9893f36457582596fdade10f6feb2797ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 60a1f468f968,938f295a60d4..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -654,26 -770,216 +654,220 @@@ static int pcpu_alloc_area(struct pcpu_
  }
  
  /**
 - * pcpu_block_update_hint_free - updates the block hints on the free path
 + * pcpu_free_area - free area to a pcpu_chunk
   * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of request
 - *
 - * Updates metadata for the allocation path.  This avoids a blind block
 - * refresh by making use of the block contig hints.  If this fails, it scans
 - * forward and backward to determine the extent of the free area.  This is
 - * capped at the boundary of blocks.
 - *
 - * A chunk update is triggered if a page becomes free, a block becomes free,
 - * or the free spans across blocks.  This tradeoff is to minimize iterating
 - * over the block metadata to update chunk->contig_bits.  chunk->contig_bits
 - * may be off by up to a page, but it will never be more than the available
 - * space.  If the contig hint is contained in one block, it will be accurate.
 + * @freeme: offset of area to free
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Free area starting from @freeme to @chunk.  Note that this function
 + * only modifies the allocation map.  It doesn't depopulate or unmap
 + * the area.
 + *
 + * CONTEXT:
 + * pcpu_lock.
   */
 -static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 -					int bits)
 +static void pcpu_free_area(struct pcpu_chunk *chunk, int freeme,
 +			   int *occ_pages_p)
  {
++<<<<<<< HEAD
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int off = 0;
 +	unsigned i, j;
 +	int to_free = 0;
 +	int *p;
++=======
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 	int start, end;		/* start and end of the whole free area */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/*
+ 	 * Check if the freed area aligns with the block->contig_hint.
+ 	 * If it does, then the scan to find the beginning/end of the
+ 	 * larger free area can be avoided.
+ 	 *
+ 	 * start and end refer to beginning and end of the free area
+ 	 * within each their respective blocks.  This is not necessarily
+ 	 * the entire free area as it may span blocks past the beginning
+ 	 * or end of the block.
+ 	 */
+ 	start = s_off;
+ 	if (s_off == s_block->contig_hint + s_block->contig_hint_start) {
+ 		start = s_block->contig_hint_start;
+ 	} else {
+ 		/*
+ 		 * Scan backwards to find the extent of the free area.
+ 		 * find_last_bit returns the starting bit, so if the start bit
+ 		 * is returned, that means there was no last bit and the
+ 		 * remainder of the chunk is free.
+ 		 */
+ 		int l_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index),
+ 					  start);
+ 		start = (start == l_bit) ? 0 : l_bit + 1;
+ 	}
+ 
+ 	end = e_off;
+ 	if (e_off == e_block->contig_hint_start)
+ 		end = e_block->contig_hint_start + e_block->contig_hint;
+ 	else
+ 		end = find_next_bit(pcpu_index_alloc_map(chunk, e_index),
+ 				    PCPU_BITMAP_BLOCK_BITS, end);
+ 
+ 	/* update s_block */
+ 	e_off = (s_index == e_index) ? end : PCPU_BITMAP_BLOCK_BITS;
+ 	pcpu_block_update(s_block, start, e_off);
+ 
+ 	/* freeing in the same block */
+ 	if (s_index != e_index) {
+ 		/* update e_block */
+ 		pcpu_block_update(e_block, 0, end);
+ 
+ 		/* reset md_blocks in the middle */
+ 		for (block = s_block + 1; block < e_block; block++) {
+ 			block->first_free = 0;
+ 			block->contig_hint_start = 0;
+ 			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
+ 			block->left_free = PCPU_BITMAP_BLOCK_BITS;
+ 			block->right_free = PCPU_BITMAP_BLOCK_BITS;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Refresh chunk metadata when the free makes a page free, a block
+ 	 * free, or spans across blocks.  The contig hint may be off by up to
+ 	 * a page, but if the hint is contained in a block, it will be accurate
+ 	 * with the else condition below.
+ 	 */
+ 	if ((ALIGN_DOWN(end, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS)) >
+ 	     ALIGN(start, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS))) ||
+ 	    s_index != e_index)
+ 		pcpu_chunk_refresh_hint(chunk);
+ 	else
+ 		pcpu_chunk_update(chunk, pcpu_block_off_to_off(s_index, start),
+ 				  end - start);
+ }
+ 
+ /**
+  * pcpu_is_populated - determines if the region is populated
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of area
+  * @next_off: return value for the next offset to start searching
+  *
+  * For atomic allocations, check if the backing pages are populated.
+  *
+  * RETURNS:
+  * Bool if the backing pages are populated.
+  * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
+  */
+ static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
+ 			      int *next_off)
+ {
+ 	int page_start, page_end, rs, re;
+ 
+ 	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
+ 	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
+ 
+ 	rs = page_start;
+ 	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
+ 	if (rs >= page_end)
+ 		return true;
+ 
+ 	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
+ 	return false;
+ }
+ 
+ /**
+  * pcpu_find_block_fit - finds the block index to start searching
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE bytes)
+  * @pop_only: use populated regions only
+  *
+  * Given a chunk and an allocation spec, find the offset to begin searching
+  * for a free region.  This iterates over the bitmap metadata blocks to
+  * find an offset that will be guaranteed to fit the requirements.  It is
+  * not quite first fit as if the allocation does not fit in the contig hint
+  * of a block or chunk, it is skipped.  This errs on the side of caution
+  * to prevent excess iteration.  Poor alignment can cause the allocator to
+  * skip over blocks and chunks that have valid free areas.
+  *
+  * RETURNS:
+  * The offset in the bitmap to begin searching.
+  * -1 if no offset is found.
+  */
+ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
+ 			       size_t align, bool pop_only)
+ {
+ 	int bit_off, bits, next_off;
+ 
+ 	/*
+ 	 * Check to see if the allocation can fit in the chunk's contig hint.
+ 	 * This is an optimization to prevent scanning by assuming if it
+ 	 * cannot fit in the global hint, there is memory pressure and creating
+ 	 * a new chunk would happen soon.
+ 	 */
+ 	bit_off = ALIGN(chunk->contig_bits_start, align) -
+ 		  chunk->contig_bits_start;
+ 	if (bit_off + alloc_bits > chunk->contig_bits)
+ 		return -1;
+ 
+ 	bit_off = chunk->first_bit;
+ 	bits = 0;
+ 	pcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits) {
+ 		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
+ 						   &next_off))
+ 			break;
+ 
+ 		bit_off = next_off;
+ 		bits = 0;
+ 	}
+ 
+ 	if (bit_off == pcpu_chunk_map_bits(chunk))
+ 		return -1;
+ 
+ 	return bit_off;
+ }
+ 
+ /**
+  * pcpu_alloc_area - allocates an area from a pcpu_chunk
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE)
+  * @start: bit_off to start searching
+  *
+  * This function takes in a @start offset to begin searching to fit an
+  * allocation of @alloc_bits with alignment @align.  It needs to scan
+  * the allocation map because if it fits within the block's contig hint,
+  * @start will be block->first_free. This is an attempt to fill the
+  * allocation prior to breaking the contig hint.  The allocation and
+  * boundary maps are updated accordingly if it confirms a valid
+  * free area.
+  *
+  * RETURNS:
+  * Allocated addr offset in @chunk on success.
+  * -1 if no matching area is found.
+  */
+ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
+ 			   size_t align, int start)
+ {
+ 	size_t align_mask = (align) ? (align - 1) : 0;
+ 	int bit_off, end, oslot;
++>>>>>>> 8e5a2b9893f3 (percpu: update free path with correct new free region)
  
  	lockdep_assert_held(&pcpu_lock);
  
* Unmerged path mm/percpu.c
