mm/percpu: add checks for the return value of memblock_alloc*()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit f655f40537916d4b1d6d1a023a778697c75a4fe2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/f655f405.failed

Add panic() calls if memblock_alloc() returns NULL.

The panic() format duplicates the one used by memblock itself and in
order to avoid explosion with long parameters list replace open coded
allocation size calculations with a local variable.

Link: http://lkml.kernel.org/r/1548057848-15136-17-git-send-email-rppt@linux.ibm.com
	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Dennis Zhou <dennis@kernel.org>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Guo Ren <ren_guo@c-sky.com>				[c-sky]
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Juergen Gross <jgross@suse.com>			[Xen]
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Paul Burton <paul.burton@mips.com>
	Cc: Petr Mladek <pmladek@suse.com>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Rob Herring <robh+dt@kernel.org>
	Cc: Rob Herring <robh@kernel.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Stafford Horne <shorne@gmail.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f655f40537916d4b1d6d1a023a778697c75a4fe2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 60a1f468f968,3f9fb3086a9b..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -721,11 -1054,137 +721,115 @@@ static void pcpu_free_area(struct pcpu_
  	pcpu_chunk_relocate(chunk, oslot);
  }
  
 -static void pcpu_init_md_blocks(struct pcpu_chunk *chunk)
 -{
 -	struct pcpu_block_md *md_block;
 -
 -	for (md_block = chunk->md_blocks;
 -	     md_block != chunk->md_blocks + pcpu_chunk_nr_blocks(chunk);
 -	     md_block++) {
 -		md_block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
 -		md_block->left_free = PCPU_BITMAP_BLOCK_BITS;
 -		md_block->right_free = PCPU_BITMAP_BLOCK_BITS;
 -	}
 -}
 -
 -/**
 - * pcpu_alloc_first_chunk - creates chunks that serve the first chunk
 - * @tmp_addr: the start of the region served
 - * @map_size: size of the region served
 - *
 - * This is responsible for creating the chunks that serve the first chunk.  The
 - * base_addr is page aligned down of @tmp_addr while the region end is page
 - * aligned up.  Offsets are kept track of to determine the region served. All
 - * this is done to appease the bitmap allocator in avoiding partial blocks.
 - *
 - * RETURNS:
 - * Chunk serving the region at @tmp_addr of @map_size.
 - */
 -static struct pcpu_chunk * __init pcpu_alloc_first_chunk(unsigned long tmp_addr,
 -							 int map_size)
 +static struct pcpu_chunk *pcpu_alloc_chunk(void)
  {
  	struct pcpu_chunk *chunk;
++<<<<<<< HEAD
 +
 +	chunk = pcpu_mem_zalloc(pcpu_chunk_struct_size);
++=======
+ 	unsigned long aligned_addr, lcm_align;
+ 	int start_offset, offset_bits, region_size, region_bits;
+ 	size_t alloc_size;
+ 
+ 	/* region calculations */
+ 	aligned_addr = tmp_addr & PAGE_MASK;
+ 
+ 	start_offset = tmp_addr - aligned_addr;
+ 
+ 	/*
+ 	 * Align the end of the region with the LCM of PAGE_SIZE and
+ 	 * PCPU_BITMAP_BLOCK_SIZE.  One of these constants is a multiple of
+ 	 * the other.
+ 	 */
+ 	lcm_align = lcm(PAGE_SIZE, PCPU_BITMAP_BLOCK_SIZE);
+ 	region_size = ALIGN(start_offset + map_size, lcm_align);
+ 
+ 	/* allocate chunk */
+ 	alloc_size = sizeof(struct pcpu_chunk) +
+ 		BITS_TO_LONGS(region_size >> PAGE_SHIFT);
+ 	chunk = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	INIT_LIST_HEAD(&chunk->list);
+ 
+ 	chunk->base_addr = (void *)aligned_addr;
+ 	chunk->start_offset = start_offset;
+ 	chunk->end_offset = region_size - chunk->start_offset - map_size;
+ 
+ 	chunk->nr_pages = region_size >> PAGE_SHIFT;
+ 	region_bits = pcpu_chunk_map_bits(chunk);
+ 
+ 	alloc_size = BITS_TO_LONGS(region_bits) * sizeof(chunk->alloc_map[0]);
+ 	chunk->alloc_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->alloc_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size =
+ 		BITS_TO_LONGS(region_bits + 1) * sizeof(chunk->bound_map[0]);
+ 	chunk->bound_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->bound_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = pcpu_chunk_nr_blocks(chunk) * sizeof(chunk->md_blocks[0]);
+ 	chunk->md_blocks = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!chunk->md_blocks)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	pcpu_init_md_blocks(chunk);
+ 
+ 	/* manage populated page bitmap */
+ 	chunk->immutable = true;
+ 	bitmap_fill(chunk->populated, chunk->nr_pages);
+ 	chunk->nr_populated = chunk->nr_pages;
+ 	chunk->nr_empty_pop_pages =
+ 		pcpu_cnt_pop_pages(chunk, start_offset / PCPU_MIN_ALLOC_SIZE,
+ 				   map_size / PCPU_MIN_ALLOC_SIZE);
+ 
+ 	chunk->contig_bits = map_size / PCPU_MIN_ALLOC_SIZE;
+ 	chunk->free_bytes = map_size;
+ 
+ 	if (chunk->start_offset) {
+ 		/* hide the beginning of the bitmap */
+ 		offset_bits = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;
+ 		bitmap_set(chunk->alloc_map, 0, offset_bits);
+ 		set_bit(0, chunk->bound_map);
+ 		set_bit(offset_bits, chunk->bound_map);
+ 
+ 		chunk->first_bit = offset_bits;
+ 
+ 		pcpu_block_update_hint_alloc(chunk, 0, offset_bits);
+ 	}
+ 
+ 	if (chunk->end_offset) {
+ 		/* hide the end of the bitmap */
+ 		offset_bits = chunk->end_offset / PCPU_MIN_ALLOC_SIZE;
+ 		bitmap_set(chunk->alloc_map,
+ 			   pcpu_chunk_map_bits(chunk) - offset_bits,
+ 			   offset_bits);
+ 		set_bit((start_offset + map_size) / PCPU_MIN_ALLOC_SIZE,
+ 			chunk->bound_map);
+ 		set_bit(region_bits, chunk->bound_map);
+ 
+ 		pcpu_block_update_hint_alloc(chunk, pcpu_chunk_map_bits(chunk)
+ 					     - offset_bits, offset_bits);
+ 	}
+ 
+ 	return chunk;
+ }
+ 
+ static struct pcpu_chunk *pcpu_alloc_chunk(gfp_t gfp)
+ {
+ 	struct pcpu_chunk *chunk;
+ 	int region_bits;
+ 
+ 	chunk = pcpu_mem_zalloc(pcpu_chunk_struct_size, gfp);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  	if (!chunk)
  		return NULL;
  
@@@ -1577,6 -2059,9 +1681,12 @@@ int __init pcpu_setup_first_chunk(cons
  	unsigned int cpu;
  	int *unit_map;
  	int group, unit, i;
++<<<<<<< HEAD
++=======
+ 	int map_size;
+ 	unsigned long tmp_addr;
+ 	size_t alloc_size;
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  #define PCPU_SETUP_BUG_ON(cond)	do {					\
  	if (unlikely(cond)) {						\
@@@ -1603,12 -2089,33 +1713,38 @@@
  	PCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);
  
  	/* process group information and build config tables accordingly */
++<<<<<<< HEAD
 +	group_offsets = memblock_virt_alloc(ai->nr_groups *
 +					     sizeof(group_offsets[0]), 0);
 +	group_sizes = memblock_virt_alloc(ai->nr_groups *
 +					   sizeof(group_sizes[0]), 0);
 +	unit_map = memblock_virt_alloc(nr_cpu_ids * sizeof(unit_map[0]), 0);
 +	unit_off = memblock_virt_alloc(nr_cpu_ids * sizeof(unit_off[0]), 0);
++=======
+ 	alloc_size = ai->nr_groups * sizeof(group_offsets[0]);
+ 	group_offsets = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!group_offsets)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = ai->nr_groups * sizeof(group_sizes[0]);
+ 	group_sizes = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!group_sizes)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = nr_cpu_ids * sizeof(unit_map[0]);
+ 	unit_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!unit_map)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
+ 
+ 	alloc_size = nr_cpu_ids * sizeof(unit_off[0]);
+ 	unit_off = memblock_alloc(alloc_size, SMP_CACHE_BYTES);
+ 	if (!unit_off)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      alloc_size);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  	for (cpu = 0; cpu < nr_cpu_ids; cpu++)
  		unit_map[cpu] = UINT_MAX;
@@@ -1670,8 -2179,11 +1806,16 @@@
  	 * empty chunks.
  	 */
  	pcpu_nr_slots = __pcpu_size_to_slot(pcpu_unit_size) + 2;
++<<<<<<< HEAD
 +	pcpu_slot = memblock_virt_alloc(
 +			pcpu_nr_slots * sizeof(pcpu_slot[0]), 0);
++=======
+ 	pcpu_slot = memblock_alloc(pcpu_nr_slots * sizeof(pcpu_slot[0]),
+ 				   SMP_CACHE_BYTES);
+ 	if (!pcpu_slot)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      pcpu_nr_slots * sizeof(pcpu_slot[0]));
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  	for (i = 0; i < pcpu_nr_slots; i++)
  		INIT_LIST_HEAD(&pcpu_slot[i]);
  
@@@ -2131,7 -2637,10 +2275,14 @@@ int __init pcpu_page_first_chunk(size_
  	/* unaligned allocations can't be freed, round up to page size */
  	pages_size = PFN_ALIGN(unit_pages * num_possible_cpus() *
  			       sizeof(pages[0]));
++<<<<<<< HEAD
 +	pages = memblock_virt_alloc(pages_size, 0);
++=======
+ 	pages = memblock_alloc(pages_size, SMP_CACHE_BYTES);
+ 	if (!pages)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      pages_size);
++>>>>>>> f655f4053791 (mm/percpu: add checks for the return value of memblock_alloc*())
  
  	/* allocate pages */
  	j = 0;
* Unmerged path mm/percpu.c
