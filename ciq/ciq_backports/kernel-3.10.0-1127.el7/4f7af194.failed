drm/i915: Support ro ppgtt mapped cmdparser shadow buffers

jira LE-1907
cve CVE-2019-0155
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Jon Bloomfield <jon.bloomfield@intel.com>
commit 4f7af1948abcb18b4772fe1bcd84d7d27d96258c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/4f7af194.failed

For Gen7, the original cmdparser motive was to permit limited
use of register read/write instructions in unprivileged BB's.
This worked by copying the user supplied bb to a kmd owned
bb, and running it in secure mode, from the ggtt, only if
the scanner finds no unsafe commands or registers.

For Gen8+ we can't use this same technique because running bb's
from the ggtt also disables access to ppgtt space. But we also
do not actually require 'secure' execution since we are only
trying to reduce the available command/register set. Instead we
will copy the user buffer to a kmd owned read-only bb in ppgtt,
and run in the usual non-secure mode.

Note that ro pages are only supported by ppgtt (not ggtt), but
luckily that's exactly what we need.

Add the required paths to map the shadow buffer to ppgtt ro for Gen8+

v2: IS_GEN7/IS_GEN (Mika)
v3: rebase
v4: rebase
v5: rebase

	Signed-off-by: Jon Bloomfield <jon.bloomfield@intel.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Dave Airlie <airlied@redhat.com>
	Cc: Takashi Iwai <tiwai@suse.de>
	Cc: Tyler Hicks <tyhicks@canonical.com>
	Signed-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
	Reviewed-by: Chris Wilson <chris.p.wilson@intel.com>
(cherry picked from commit 4f7af1948abcb18b4772fe1bcd84d7d27d96258c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/i915_drv.h
#	drivers/gpu/drm/i915/i915_gem.c
#	drivers/gpu/drm/i915/i915_gem_execbuffer.c
diff --cc drivers/gpu/drm/i915/i915_drv.h
index f30835fe8170,5b338e1b79fd..000000000000
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@@ -2367,51 -2058,33 +2367,64 @@@ intel_info(const struct drm_i915_privat
  #define IS_ICL_REVID(p, since, until) \
  	(IS_ICELAKE(p) && IS_REVID(p, since, until))
  
 -#define IS_LP(dev_priv)	(INTEL_INFO(dev_priv)->is_lp)
 -#define IS_GEN9_LP(dev_priv)	(IS_GEN(dev_priv, 9) && IS_LP(dev_priv))
 -#define IS_GEN9_BC(dev_priv)	(IS_GEN(dev_priv, 9) && !IS_LP(dev_priv))
 -
 -#define HAS_ENGINE(dev_priv, id) (INTEL_INFO(dev_priv)->engine_mask & BIT(id))
 -
 -#define ENGINE_INSTANCES_MASK(dev_priv, first, count) ({		\
 -	unsigned int first__ = (first);					\
 -	unsigned int count__ = (count);					\
 -	(INTEL_INFO(dev_priv)->engine_mask &				\
 -	 GENMASK(first__ + count__ - 1, first__)) >> first__;		\
 -})
 -#define VDBOX_MASK(dev_priv) \
 -	ENGINE_INSTANCES_MASK(dev_priv, VCS0, I915_MAX_VCS)
 -#define VEBOX_MASK(dev_priv) \
 -	ENGINE_INSTANCES_MASK(dev_priv, VECS0, I915_MAX_VECS)
 +/*
 + * The genX designation typically refers to the render engine, so render
 + * capability related checks should use IS_GEN, while display and other checks
 + * have their own (e.g. HAS_PCH_SPLIT for ILK+ display, IS_foo for particular
 + * chips, etc.).
 + */
 +#define IS_GEN2(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(1)))
 +#define IS_GEN3(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(2)))
 +#define IS_GEN4(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(3)))
 +#define IS_GEN5(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(4)))
 +#define IS_GEN6(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(5)))
 +#define IS_GEN7(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(6)))
 +#define IS_GEN8(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(7)))
 +#define IS_GEN9(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(8)))
 +#define IS_GEN10(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(9)))
 +#define IS_GEN11(dev_priv)	(!!((dev_priv)->info.gen_mask & BIT(10)))
  
 +#define IS_LP(dev_priv)	(INTEL_INFO(dev_priv)->is_lp)
 +#define IS_GEN9_LP(dev_priv)	(IS_GEN9(dev_priv) && IS_LP(dev_priv))
 +#define IS_GEN9_BC(dev_priv)	(IS_GEN9(dev_priv) && !IS_LP(dev_priv))
 +
 +#define ENGINE_MASK(id)	BIT(id)
 +#define RENDER_RING	ENGINE_MASK(RCS)
 +#define BSD_RING	ENGINE_MASK(VCS)
 +#define BLT_RING	ENGINE_MASK(BCS)
 +#define VEBOX_RING	ENGINE_MASK(VECS)
 +#define BSD2_RING	ENGINE_MASK(VCS2)
 +#define BSD3_RING	ENGINE_MASK(VCS3)
 +#define BSD4_RING	ENGINE_MASK(VCS4)
 +#define VEBOX2_RING	ENGINE_MASK(VECS2)
 +#define ALL_ENGINES	(~0)
 +
 +#define HAS_ENGINE(dev_priv, id) \
 +	(!!((dev_priv)->info.ring_mask & ENGINE_MASK(id)))
 +
++<<<<<<< HEAD
 +#define HAS_BSD(dev_priv)	HAS_ENGINE(dev_priv, VCS)
 +#define HAS_BSD2(dev_priv)	HAS_ENGINE(dev_priv, VCS2)
 +#define HAS_BLT(dev_priv)	HAS_ENGINE(dev_priv, BCS)
 +#define HAS_VEBOX(dev_priv)	HAS_ENGINE(dev_priv, VECS)
 +
 +#define HAS_LEGACY_SEMAPHORES(dev_priv) IS_GEN7(dev_priv)
 +
 +#define HAS_LLC(dev_priv)	((dev_priv)->info.has_llc)
 +#define HAS_SNOOP(dev_priv)	((dev_priv)->info.has_snoop)
 +#define HAS_EDRAM(dev_priv)	(!!((dev_priv)->edram_cap & EDRAM_ENABLED))
++=======
+ /*
+  * The Gen7 cmdparser copies the scanned buffer to the ggtt for execution
+  * All later gens can run the final buffer from the ppgtt
+  */
+ #define CMDPARSER_USES_GGTT(dev_priv) IS_GEN(dev_priv, 7)
+ 
+ #define HAS_LLC(dev_priv)	(INTEL_INFO(dev_priv)->has_llc)
+ #define HAS_SNOOP(dev_priv)	(INTEL_INFO(dev_priv)->has_snoop)
+ #define HAS_EDRAM(dev_priv)	((dev_priv)->edram_size_mb)
+ #define HAS_SECURE_BATCHES(dev_priv) (INTEL_GEN(dev_priv) < 6)
++>>>>>>> 4f7af1948abc (drm/i915: Support ro ppgtt mapped cmdparser shadow buffers)
  #define HAS_WT(dev_priv)	((IS_HASWELL(dev_priv) || \
  				 IS_BROADWELL(dev_priv)) && HAS_EDRAM(dev_priv))
  
@@@ -2840,154 -2287,26 +2853,162 @@@ i915_gem_object_ggtt_pin(struct drm_i91
  			 u64 alignment,
  			 u64 flags);
  
 -int i915_gem_object_unbind(struct drm_i915_gem_object *obj,
 -			   unsigned long flags);
 -#define I915_GEM_OBJECT_UNBIND_ACTIVE BIT(0)
 +int i915_gem_object_unbind(struct drm_i915_gem_object *obj);
 +void i915_gem_release_mmap(struct drm_i915_gem_object *obj);
  
+ struct i915_vma * __must_check
+ i915_gem_object_pin(struct drm_i915_gem_object *obj,
+ 		    struct i915_address_space *vm,
+ 		    const struct i915_ggtt_view *view,
+ 		    u64 size,
+ 		    u64 alignment,
+ 		    u64 flags);
+ 
  void i915_gem_runtime_suspend(struct drm_i915_private *dev_priv);
  
 +static inline int __sg_page_count(const struct scatterlist *sg)
 +{
 +	return sg->length >> PAGE_SHIFT;
 +}
 +
 +struct scatterlist *
 +i915_gem_object_get_sg(struct drm_i915_gem_object *obj,
 +		       unsigned int n, unsigned int *offset);
 +
 +struct page *
 +i915_gem_object_get_page(struct drm_i915_gem_object *obj,
 +			 unsigned int n);
 +
 +struct page *
 +i915_gem_object_get_dirty_page(struct drm_i915_gem_object *obj,
 +			       unsigned int n);
 +
 +dma_addr_t
 +i915_gem_object_get_dma_address(struct drm_i915_gem_object *obj,
 +				unsigned long n);
 +
 +void __i915_gem_object_set_pages(struct drm_i915_gem_object *obj,
 +				 struct sg_table *pages,
 +				 unsigned int sg_page_sizes);
 +int __i915_gem_object_get_pages(struct drm_i915_gem_object *obj);
 +
  static inline int __must_check
 -i915_mutex_lock_interruptible(struct drm_device *dev)
 +i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)
 +{
 +	might_lock(&obj->mm.lock);
 +
 +	if (atomic_inc_not_zero(&obj->mm.pages_pin_count))
 +		return 0;
 +
 +	return __i915_gem_object_get_pages(obj);
 +}
 +
 +static inline bool
 +i915_gem_object_has_pages(struct drm_i915_gem_object *obj)
 +{
 +	return !IS_ERR_OR_NULL(READ_ONCE(obj->mm.pages));
 +}
 +
 +static inline void
 +__i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)
 +{
 +	GEM_BUG_ON(!i915_gem_object_has_pages(obj));
 +
 +	atomic_inc(&obj->mm.pages_pin_count);
 +}
 +
 +static inline bool
 +i915_gem_object_has_pinned_pages(struct drm_i915_gem_object *obj)
 +{
 +	return atomic_read(&obj->mm.pages_pin_count);
 +}
 +
 +static inline void
 +__i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)
  {
 -	return mutex_lock_interruptible(&dev->struct_mutex);
 +	GEM_BUG_ON(!i915_gem_object_has_pages(obj));
 +	GEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));
 +
 +	atomic_dec(&obj->mm.pages_pin_count);
  }
  
 +static inline void
 +i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)
 +{
 +	__i915_gem_object_unpin_pages(obj);
 +}
 +
 +enum i915_mm_subclass { /* lockdep subclass for obj->mm.lock */
 +	I915_MM_NORMAL = 0,
 +	I915_MM_SHRINKER
 +};
 +
 +void __i915_gem_object_put_pages(struct drm_i915_gem_object *obj,
 +				 enum i915_mm_subclass subclass);
 +void __i915_gem_object_invalidate(struct drm_i915_gem_object *obj);
 +
 +enum i915_map_type {
 +	I915_MAP_WB = 0,
 +	I915_MAP_WC,
 +#define I915_MAP_OVERRIDE BIT(31)
 +	I915_MAP_FORCE_WB = I915_MAP_WB | I915_MAP_OVERRIDE,
 +	I915_MAP_FORCE_WC = I915_MAP_WC | I915_MAP_OVERRIDE,
 +};
 +
 +static inline enum i915_map_type
 +i915_coherent_map_type(struct drm_i915_private *i915)
 +{
 +	return HAS_LLC(i915) ? I915_MAP_WB : I915_MAP_WC;
 +}
 +
 +/**
 + * i915_gem_object_pin_map - return a contiguous mapping of the entire object
 + * @obj: the object to map into kernel address space
 + * @type: the type of mapping, used to select pgprot_t
 + *
 + * Calls i915_gem_object_pin_pages() to prevent reaping of the object's
 + * pages and then returns a contiguous mapping of the backing storage into
 + * the kernel address space. Based on the @type of mapping, the PTE will be
 + * set to either WriteBack or WriteCombine (via pgprot_t).
 + *
 + * The caller is responsible for calling i915_gem_object_unpin_map() when the
 + * mapping is no longer required.
 + *
 + * Returns the pointer through which to access the mapped object, or an
 + * ERR_PTR() on error.
 + */
 +void *__must_check i915_gem_object_pin_map(struct drm_i915_gem_object *obj,
 +					   enum i915_map_type type);
 +
 +/**
 + * i915_gem_object_unpin_map - releases an earlier mapping
 + * @obj: the object to unmap
 + *
 + * After pinning the object and mapping its pages, once you are finished
 + * with your access, call i915_gem_object_unpin_map() to release the pin
 + * upon the mapping. Once the pin count reaches zero, that mapping may be
 + * removed.
 + */
 +static inline void i915_gem_object_unpin_map(struct drm_i915_gem_object *obj)
 +{
 +	i915_gem_object_unpin_pages(obj);
 +}
 +
 +int i915_gem_obj_prepare_shmem_read(struct drm_i915_gem_object *obj,
 +				    unsigned int *needs_clflush);
 +int i915_gem_obj_prepare_shmem_write(struct drm_i915_gem_object *obj,
 +				     unsigned int *needs_clflush);
 +#define CLFLUSH_BEFORE	BIT(0)
 +#define CLFLUSH_AFTER	BIT(1)
 +#define CLFLUSH_FLAGS	(CLFLUSH_BEFORE | CLFLUSH_AFTER)
 +
 +static inline void
 +i915_gem_obj_finish_shmem_access(struct drm_i915_gem_object *obj)
 +{
 +	i915_gem_object_unpin_pages(obj);
 +}
 +
 +int __must_check i915_mutex_lock_interruptible(struct drm_device *dev);
  int i915_gem_dumb_create(struct drm_file *file_priv,
  			 struct drm_device *dev,
  			 struct drm_mode_create_dumb *args);
diff --cc drivers/gpu/drm/i915/i915_gem.c
index b1d867c3ae24,98305d987ac1..000000000000
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@@ -4513,143 -1044,19 +4527,155 @@@ i915_gem_object_pin(struct drm_i915_gem
  			return ERR_PTR(ret);
  	}
  
 -	if (vma->fence && !i915_gem_object_is_tiled(obj)) {
 -		mutex_lock(&vma->vm->mutex);
 -		ret = i915_vma_revoke_fence(vma);
 -		mutex_unlock(&vma->vm->mutex);
 -		if (ret)
 -			return ERR_PTR(ret);
++<<<<<<< HEAD
 +	ret = i915_vma_pin(vma, size, alignment, flags | PIN_GLOBAL);
++=======
++	if (vma->fence && !i915_gem_object_is_tiled(obj)) {
++		mutex_lock(&vma->vm->mutex);
++		ret = i915_vma_revoke_fence(vma);
++		mutex_unlock(&vma->vm->mutex);
++		if (ret)
++			return ERR_PTR(ret);
++	}
++
++	ret = i915_vma_pin(vma, size, alignment, flags);
++>>>>>>> 4f7af1948abc (drm/i915: Support ro ppgtt mapped cmdparser shadow buffers)
 +	if (ret)
 +		return ERR_PTR(ret);
 +
 +	return vma;
 +}
 +
 +static __always_inline unsigned int __busy_read_flag(unsigned int id)
 +{
 +	/* Note that we could alias engines in the execbuf API, but
 +	 * that would be very unwise as it prevents userspace from
 +	 * fine control over engine selection. Ahem.
 +	 *
 +	 * This should be something like EXEC_MAX_ENGINE instead of
 +	 * I915_NUM_ENGINES.
 +	 */
 +	BUILD_BUG_ON(I915_NUM_ENGINES > 16);
 +	return 0x10000 << id;
 +}
 +
 +static __always_inline unsigned int __busy_write_id(unsigned int id)
 +{
 +	/* The uABI guarantees an active writer is also amongst the read
 +	 * engines. This would be true if we accessed the activity tracking
 +	 * under the lock, but as we perform the lookup of the object and
 +	 * its activity locklessly we can not guarantee that the last_write
 +	 * being active implies that we have set the same engine flag from
 +	 * last_read - hence we always set both read and write busy for
 +	 * last_write.
 +	 */
 +	return id | __busy_read_flag(id);
 +}
 +
 +static __always_inline unsigned int
 +__busy_set_if_active(const struct dma_fence *fence,
 +		     unsigned int (*flag)(unsigned int id))
 +{
 +	struct i915_request *rq;
 +
 +	/* We have to check the current hw status of the fence as the uABI
 +	 * guarantees forward progress. We could rely on the idle worker
 +	 * to eventually flush us, but to minimise latency just ask the
 +	 * hardware.
 +	 *
 +	 * Note we only report on the status of native fences.
 +	 */
 +	if (!dma_fence_is_i915(fence))
 +		return 0;
 +
 +	/* opencode to_request() in order to avoid const warnings */
 +	rq = container_of(fence, struct i915_request, fence);
 +	if (i915_request_completed(rq))
 +		return 0;
 +
 +	return flag(rq->engine->uabi_id);
 +}
 +
 +static __always_inline unsigned int
 +busy_check_reader(const struct dma_fence *fence)
 +{
 +	return __busy_set_if_active(fence, __busy_read_flag);
 +}
 +
 +static __always_inline unsigned int
 +busy_check_writer(const struct dma_fence *fence)
 +{
 +	if (!fence)
 +		return 0;
 +
 +	return __busy_set_if_active(fence, __busy_write_id);
 +}
 +
 +int
 +i915_gem_busy_ioctl(struct drm_device *dev, void *data,
 +		    struct drm_file *file)
 +{
 +	struct drm_i915_gem_busy *args = data;
 +	struct drm_i915_gem_object *obj;
 +	struct reservation_object_list *list;
 +	unsigned int seq;
 +	int err;
 +
 +	err = -ENOENT;
 +	rcu_read_lock();
 +	obj = i915_gem_object_lookup_rcu(file, args->handle);
 +	if (!obj)
 +		goto out;
 +
 +	/* A discrepancy here is that we do not report the status of
 +	 * non-i915 fences, i.e. even though we may report the object as idle,
 +	 * a call to set-domain may still stall waiting for foreign rendering.
 +	 * This also means that wait-ioctl may report an object as busy,
 +	 * where busy-ioctl considers it idle.
 +	 *
 +	 * We trade the ability to warn of foreign fences to report on which
 +	 * i915 engines are active for the object.
 +	 *
 +	 * Alternatively, we can trade that extra information on read/write
 +	 * activity with
 +	 *	args->busy =
 +	 *		!reservation_object_test_signaled_rcu(obj->resv, true);
 +	 * to report the overall busyness. This is what the wait-ioctl does.
 +	 *
 +	 */
 +retry:
 +	seq = raw_read_seqcount(&obj->resv->seq);
 +
 +	/* Translate the exclusive fence to the READ *and* WRITE engine */
 +	args->busy = busy_check_writer(rcu_dereference(obj->resv->fence_excl));
 +
 +	/* Translate shared fences to READ set of engines */
 +	list = rcu_dereference(obj->resv->fence);
 +	if (list) {
 +		unsigned int shared_count = list->shared_count, i;
 +
 +		for (i = 0; i < shared_count; ++i) {
 +			struct dma_fence *fence =
 +				rcu_dereference(list->shared[i]);
 +
 +			args->busy |= busy_check_reader(fence);
 +		}
  	}
  
 -	ret = i915_vma_pin(vma, size, alignment, flags);
 -	if (ret)
 -		return ERR_PTR(ret);
 +	if (args->busy && read_seqcount_retry(&obj->resv->seq, seq))
 +		goto retry;
  
 -	return vma;
 +	err = 0;
 +out:
 +	rcu_read_unlock();
 +	return err;
 +}
 +
 +int
 +i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
 +			struct drm_file *file_priv)
 +{
 +	return i915_gem_ring_throttle(dev, file_priv);
  }
  
  int
diff --cc drivers/gpu/drm/i915/i915_gem_execbuffer.c
index 48276ddd0ab4,1f423bb2d644..000000000000
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@@ -1924,9 -1956,37 +1924,37 @@@ static int i915_reset_gen7_sol_offsets(
  	return 0;
  }
  
+ static struct i915_vma *
+ shadow_batch_pin(struct i915_execbuffer *eb, struct drm_i915_gem_object *obj)
+ {
+ 	struct drm_i915_private *dev_priv = eb->i915;
+ 	struct i915_vma * const vma = *eb->vma;
+ 	struct i915_address_space *vm;
+ 	u64 flags;
+ 
+ 	/*
+ 	 * PPGTT backed shadow buffers must be mapped RO, to prevent
+ 	 * post-scan tampering
+ 	 */
+ 	if (CMDPARSER_USES_GGTT(dev_priv)) {
+ 		flags = PIN_GLOBAL;
+ 		vm = &dev_priv->ggtt.vm;
+ 		eb->batch_flags |= I915_DISPATCH_SECURE;
+ 	} else if (vma->vm->has_read_only) {
+ 		flags = PIN_USER;
+ 		vm = vma->vm;
+ 		i915_gem_object_set_readonly(obj);
+ 	} else {
+ 		DRM_DEBUG("Cannot prevent post-scan tampering without RO capable vm\n");
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	return i915_gem_object_pin(obj, vm, NULL, 0, 0, flags);
+ }
+ 
  static struct i915_vma *eb_parse(struct i915_execbuffer *eb)
  {
 -	struct intel_engine_pool_node *pool;
 +	struct drm_i915_gem_object *shadow_batch_obj;
  	struct i915_vma *vma;
  	int err;
  
@@@ -1945,12 -2011,12 +1980,16 @@@
  			vma = NULL;
  		else
  			vma = ERR_PTR(err);
 -		goto err;
 +		goto out;
  	}
  
++<<<<<<< HEAD:drivers/gpu/drm/i915/i915_gem_execbuffer.c
 +	vma = i915_gem_object_ggtt_pin(shadow_batch_obj, NULL, 0, 0, 0);
++=======
+ 	vma = shadow_batch_pin(eb, pool->obj);
++>>>>>>> 4f7af1948abc (drm/i915: Support ro ppgtt mapped cmdparser shadow buffers):drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
  	if (IS_ERR(vma))
 -		goto err;
 +		goto out;
  
  	eb->vma[eb->buffer_count] = i915_vma_get(vma);
  	eb->flags[eb->buffer_count] =
@@@ -1958,8 -2024,15 +1997,20 @@@
  	vma->exec_flags = &eb->flags[eb->buffer_count];
  	eb->buffer_count++;
  
++<<<<<<< HEAD:drivers/gpu/drm/i915/i915_gem_execbuffer.c
 +out:
 +	i915_gem_object_unpin_pages(shadow_batch_obj);
++=======
+ 	eb->batch_start_offset = 0;
+ 	eb->batch = vma;
+ 	/* eb->batch_len unchanged */
+ 
+ 	vma->private = pool;
+ 	return vma;
+ 
+ err:
+ 	intel_engine_pool_put(pool);
++>>>>>>> 4f7af1948abc (drm/i915: Support ro ppgtt mapped cmdparser shadow buffers):drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
  	return vma;
  }
  
* Unmerged path drivers/gpu/drm/i915/i915_drv.h
* Unmerged path drivers/gpu/drm/i915/i915_gem.c
* Unmerged path drivers/gpu/drm/i915/i915_gem_execbuffer.c
