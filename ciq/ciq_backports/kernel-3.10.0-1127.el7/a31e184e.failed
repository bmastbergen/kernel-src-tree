x86/pkeys: Properly copy pkey state at fork()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit a31e184e4f69965c99c04cc5eb8a4920e0c63737
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/a31e184e.failed

Memory protection key behavior should be the same in a child as it was
in the parent before a fork.  But, there is a bug that resets the
state in the child at fork instead of preserving it.

The creation of new mm's is a bit convoluted.  At fork(), the code
does:

  1. memcpy() the parent mm to initialize child
  2. mm_init() to initalize some select stuff stuff
  3. dup_mmap() to create true copies that memcpy() did not do right

For pkeys two bits of state need to be preserved across a fork:
'execute_only_pkey' and 'pkey_allocation_map'.

Those are preserved by the memcpy(), but mm_init() invokes
init_new_context() which overwrites 'execute_only_pkey' and
'pkey_allocation_map' with "new" values.

The author of the code erroneously believed that init_new_context is *only*
called at execve()-time.  But, alas, init_new_context() is used at execve()
and fork().

The result is that, after a fork(), the child's pkey state ends up looking
like it does after an execve(), which is totally wrong.  pkeys that are
already allocated can be allocated again, for instance.

To fix this, add code called by dup_mmap() to copy the pkey state from
parent to child explicitly.  Also add a comment above init_new_context() to
make it more clear to the next poor sod what this code is used for.

Fixes: e8c24d3a23a ("x86/pkeys: Allocation/free syscalls")
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: bp@alien8.de
	Cc: hpa@zytor.com
	Cc: peterz@infradead.org
	Cc: mpe@ellerman.id.au
	Cc: will.deacon@arm.com
	Cc: luto@kernel.org
	Cc: jroedel@suse.de
	Cc: stable@vger.kernel.org
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Joerg Roedel <jroedel@suse.de>
Link: https://lkml.kernel.org/r/20190102215655.7A69518C@viggo.jf.intel.com

(cherry picked from commit a31e184e4f69965c99c04cc5eb8a4920e0c63737)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mmu_context.h
diff --cc arch/x86/include/asm/mmu_context.h
index 2533f1ea6556,19d18fae6ec6..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -54,46 -118,88 +54,50 @@@ static inline void load_mm_ldt(struct m
  	 * that we can see.
  	 */
  
 -	if (unlikely(ldt)) {
 -		if (static_cpu_has(X86_FEATURE_PTI)) {
 -			if (WARN_ON_ONCE((unsigned long)ldt->slot > 1)) {
 -				/*
 -				 * Whoops -- either the new LDT isn't mapped
 -				 * (if slot == -1) or is mapped into a bogus
 -				 * slot (if slot > 1).
 -				 */
 -				clear_LDT();
 -				return;
 -			}
 -
 -			/*
 -			 * If page table isolation is enabled, ldt->entries
 -			 * will not be mapped in the userspace pagetables.
 -			 * Tell the CPU to access the LDT through the alias
 -			 * at ldt_slot_va(ldt->slot).
 -			 */
 -			set_ldt(ldt_slot_va(ldt->slot), ldt->nr_entries);
 -		} else {
 -			set_ldt(ldt->entries, ldt->nr_entries);
 -		}
 -	} else {
 +	if (unlikely(ldt))
 +		set_ldt(ldt->entries, ldt->size);
 +	else
  		clear_LDT();
 -	}
 -#else
 -	clear_LDT();
 -#endif
 +
 +	DEBUG_LOCKS_WARN_ON(preemptible());
  }
  
 -static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)
 +/*
 + * Used for LDT copy/destruction.
 + */
 +int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 +void destroy_context_ldt(struct mm_struct *mm);
 +
 +
 +static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
  {
 -#ifdef CONFIG_MODIFY_LDT_SYSCALL
 -	/*
 -	 * Load the LDT if either the old or new mm had an LDT.
 -	 *
 -	 * An mm will never go from having an LDT to not having an LDT.  Two
 -	 * mms never share an LDT, so we don't gain anything by checking to
 -	 * see whether the LDT changed.  There's also no guarantee that
 -	 * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,
 -	 * then prev->context.ldt will also be non-NULL.
 -	 *
 -	 * If we really cared, we could optimize the case where prev == next
 -	 * and we're exiting lazy mode.  Most of the time, if this happens,
 -	 * we don't actually need to reload LDTR, but modify_ldt() is mostly
 -	 * used by legacy code and emulators where we don't need this level of
 -	 * performance.
 -	 *
 -	 * This uses | instead of || because it generates better code.
 -	 */
 -	if (unlikely((unsigned long)prev->context.ldt |
 -		     (unsigned long)next->context.ldt))
 -		load_mm_ldt(next);
 +#ifdef CONFIG_SMP
 +	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)
 +		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);
  #endif
 -
 -	DEBUG_LOCKS_WARN_ON(preemptible());
  }
  
 -void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);
 +static inline void load_cr3(pgd_t *pgdir)
 +{
 +	__load_cr3(__sme_pa(pgdir));
 +}
  
+ /*
+  * Init a new mm.  Used on mm copies, like at fork()
+  * and on mm's that are brand-new, like at execve().
+  */
  static inline int init_new_context(struct task_struct *tsk,
 -				   struct mm_struct *mm)
 +				       struct mm_struct *mm)
  {
 -	mutex_init(&mm->context.lock);
 -
 -	mm->context.ctx_id = atomic64_inc_return(&last_mm_ctx_id);
 -	atomic64_set(&mm->context.tlb_gen, 0);
 -
 -#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
 +	#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
  	if (cpu_feature_enabled(X86_FEATURE_OSPKE)) {
 -		/* pkey 0 is the default and allocated implicitly */
 -		mm->context.pkey_allocation_map = 0x1;
 +		/* pkey 0 is the default and always allocated */
 +		mm->pkey_allocation_map = 0x1;
  		/* -1 means unallocated or invalid */
 -		mm->context.execute_only_pkey = -1;
 +		mm->execute_only_pkey = -1;
  	}
 -#endif
 -	init_new_context_ldt(mm);
 -	return 0;
 +	#endif
 +	return init_new_context_ldt(tsk, mm);
  }
  static inline void destroy_context(struct mm_struct *mm)
  {
@@@ -207,6 -232,32 +211,35 @@@ do {						
  } while (0)
  #endif
  
++<<<<<<< HEAD
++=======
+ static inline void arch_dup_pkeys(struct mm_struct *oldmm,
+ 				  struct mm_struct *mm)
+ {
+ #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+ 	if (!cpu_feature_enabled(X86_FEATURE_OSPKE))
+ 		return;
+ 
+ 	/* Duplicate the oldmm pkey state in mm: */
+ 	mm->context.pkey_allocation_map = oldmm->context.pkey_allocation_map;
+ 	mm->context.execute_only_pkey   = oldmm->context.execute_only_pkey;
+ #endif
+ }
+ 
+ static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+ {
+ 	arch_dup_pkeys(oldmm, mm);
+ 	paravirt_arch_dup_mmap(oldmm, mm);
+ 	return ldt_dup_context(oldmm, mm);
+ }
+ 
+ static inline void arch_exit_mmap(struct mm_struct *mm)
+ {
+ 	paravirt_arch_exit_mmap(mm);
+ 	ldt_arch_exit_mmap(mm);
+ }
+ 
++>>>>>>> a31e184e4f69 (x86/pkeys: Properly copy pkey state at fork())
  #ifdef CONFIG_X86_64
  static inline bool is_64bit_mm(struct mm_struct *mm)
  {
* Unmerged path arch/x86/include/asm/mmu_context.h
