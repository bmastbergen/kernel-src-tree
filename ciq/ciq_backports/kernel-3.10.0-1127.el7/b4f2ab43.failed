sched: Remove 'cpu' parameter from idle_balance()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Daniel Lezcano <daniel.lezcano@linaro.org>
commit b4f2ab43615e5b36c48fffa99f26aca381839ac6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/b4f2ab43.failed

The cpu parameter passed to idle_balance() is not needed as it could
be retrieved from 'struct rq.'

	Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
	Cc: alex.shi@linaro.org
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1389949444-14821-1-git-send-email-daniel.lezcano@linaro.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b4f2ab43615e5b36c48fffa99f26aca381839ac6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/sched.h
diff --cc kernel/sched/sched.h
index 38c9ae998b34,82c0e02f2a58..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -1313,23 -1157,11 +1313,28 @@@ extern const struct sched_class idle_sc
  
  extern void update_group_power(struct sched_domain *sd, int cpu);
  
++<<<<<<< HEAD
 +extern void trigger_load_balance(struct rq *rq, int cpu);
 +extern void idle_balance(int this_cpu, struct rq *this_rq);
++=======
+ extern void trigger_load_balance(struct rq *rq);
+ extern void idle_balance(struct rq *this_rq);
++>>>>>>> b4f2ab43615e (sched: Remove 'cpu' parameter from idle_balance())
 +
 +extern void sched_cpu_activate(unsigned int cpu);
 +extern void sched_cpu_deactivate(unsigned int cpu);
  
 +/*
 + * Only depends on SMP, FAIR_GROUP_SCHED may be removed when runnable_avg
 + * becomes useful in lb
 + */
 +#if defined(CONFIG_FAIR_GROUP_SCHED)
  extern void idle_enter_fair(struct rq *this_rq);
  extern void idle_exit_fair(struct rq *this_rq);
 +#else
 +static inline void idle_enter_fair(struct rq *this_rq) {}
 +static inline void idle_exit_fair(struct rq *this_rq) {}
 +#endif
  
  #else	/* CONFIG_SMP */
  
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 9474c46ea21e..7249888005f2 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3587,7 +3587,7 @@ need_resched:
 	pre_schedule(rq, prev);
 
 	if (unlikely(!rq->nr_running))
-		idle_balance(cpu, rq);
+		idle_balance(rq);
 
 	put_prev_task(rq, prev);
 	next = pick_next_task(rq);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index f3df0d8beb03..a9f4a0557512 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6859,12 +6859,13 @@ update_next_balance(struct sched_domain *sd, int cpu_busy, unsigned long *next_b
  * idle_balance is called by schedule() if this_cpu is about to become
  * idle. Attempts to pull tasks from other CPUs.
  */
-void idle_balance(int this_cpu, struct rq *this_rq)
+void idle_balance(struct rq *this_rq)
 {
 	unsigned long next_balance = jiffies + HZ;
 	struct sched_domain *sd;
 	int pulled_task = 0;
 	u64 curr_cost = 0;
+	int this_cpu = this_rq->cpu;
 
 	this_rq->idle_stamp = rq_clock(this_rq);
 
* Unmerged path kernel/sched/sched.h
