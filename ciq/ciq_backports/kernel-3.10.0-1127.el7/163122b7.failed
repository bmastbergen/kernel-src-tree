sched/fair: Remove double_lock_balance() from load_balance()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Kirill Tkhai <ktkhai@parallels.com>
commit 163122b7fcfa28c0e4a838fcc8043c616746802e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/163122b7.failed

Avoid double_rq_lock() and use TASK_ON_RQ_MIGRATING for
load_balance(). The advantage is (obviously) not holding two
rq->lock's at the same time and thereby increasing parallelism.

Further note that if there was no task to migrate we will not
have acquired the second rq->lock at all.

The important point to note is that because we acquire dst->lock
immediately after releasing src->lock the potential wait time of
task_rq_lock() callers on TASK_ON_RQ_MIGRATING is not longer
than it would have been in the double rq lock scenario.

	Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Paul Turner <pjt@google.com>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
	Cc: Kirill Tkhai <tkhai@yandex.ru>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/1408528109.23412.94.camel@tkhai
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 163122b7fcfa28c0e4a838fcc8043c616746802e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index c68fec18dead,d3427a8f254b..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5428,13 -5335,24 +5417,29 @@@ int can_migrate_task(struct task_struc
  }
  
  /*
++<<<<<<< HEAD
 + * move_one_task tries to move exactly one task from busiest to this_rq, as
++=======
+  * detach_task() -- detach the task for the migration specified in env
+  */
+ static void detach_task(struct task_struct *p, struct lb_env *env)
+ {
+ 	lockdep_assert_held(&env->src_rq->lock);
+ 
+ 	deactivate_task(env->src_rq, p, 0);
+ 	p->on_rq = TASK_ON_RQ_MIGRATING;
+ 	set_task_cpu(p, env->dst_cpu);
+ }
+ 
+ /*
+  * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as
++>>>>>>> 163122b7fcfa (sched/fair: Remove double_lock_balance() from load_balance())
   * part of active balancing operations within "domain".
 + * Returns 1 if successful and 0 otherwise.
   *
 - * Returns a task if successful and NULL otherwise.
 + * Called with both runqueues locked.
   */
 -static struct task_struct *detach_one_task(struct lb_env *env)
 +static int move_one_task(struct lb_env *env)
  {
  	struct task_struct *p, *n;
  
@@@ -5442,17 -5362,18 +5447,31 @@@
  		if (!can_migrate_task(p, env))
  			continue;
  
++<<<<<<< HEAD
 +		move_task(p, env);
 +
 +		/*
 +		 * Right now, this is only the second place move_task()
 +		 * is called, so we can safely collect move_task()
 +		 * stats here rather than inside move_task().
++=======
+ 		detach_task(p, env);
+ 
+ 		/*
+ 		 * Right now, this is only the second place where
+ 		 * lb_gained[env->idle] is updated (other is detach_tasks)
+ 		 * so we can safely collect stats here rather than
+ 		 * inside detach_tasks().
++>>>>>>> 163122b7fcfa (sched/fair: Remove double_lock_balance() from load_balance())
  		 */
  		schedstat_inc(env->sd, lb_gained[env->idle]);
 -		return p;
 +		return 1;
  	}
++<<<<<<< HEAD
 +	return 0;
++=======
+ 	return NULL;
++>>>>>>> 163122b7fcfa (sched/fair: Remove double_lock_balance() from load_balance())
  }
  
  static const unsigned int sched_nr_migrate_break = 32;
* Unmerged path kernel/sched/fair.c
