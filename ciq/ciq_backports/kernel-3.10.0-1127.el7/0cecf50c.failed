percpu: introduce nr_empty_pop_pages to help empty page accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 0cecf50cf00fbe6858908098ae6c6a9fd1d60724
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/0cecf50c.failed

pcpu_nr_empty_pop_pages is used to ensure there are a handful of free
pages around to serve atomic allocations. A new field, nr_empty_pop_pages,
is added to the pcpu_chunk struct to keep track of the number of empty
pages. This field is needed as the number of empty populated pages is
globally tracked and deltas are used to update in the bitmap allocator.
Pages that contain a hidden area are not considered to be empty. This
new field is exposed in percpu_stats.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 0cecf50cf00fbe6858908098ae6c6a9fd1d60724)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu-internal.h
#	mm/percpu-stats.c
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,657ab0821cf0..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -720,6 -720,70 +720,73 @@@ static void pcpu_free_area(struct pcpu_
  	pcpu_chunk_relocate(chunk, oslot);
  }
  
++<<<<<<< HEAD
++=======
+ static struct pcpu_chunk * __init pcpu_alloc_first_chunk(unsigned long tmp_addr,
+ 							 int map_size,
+ 							 int *map,
+ 							 int init_map_size)
+ {
+ 	struct pcpu_chunk *chunk;
+ 	unsigned long aligned_addr;
+ 	int start_offset, region_size;
+ 
+ 	/* region calculations */
+ 	aligned_addr = tmp_addr & PAGE_MASK;
+ 
+ 	start_offset = tmp_addr - aligned_addr;
+ 
+ 	region_size = PFN_ALIGN(start_offset + map_size);
+ 
+ 	/* allocate chunk */
+ 	chunk = memblock_virt_alloc(sizeof(struct pcpu_chunk) +
+ 				    BITS_TO_LONGS(region_size >> PAGE_SHIFT),
+ 				    0);
+ 
+ 	INIT_LIST_HEAD(&chunk->list);
+ 	INIT_LIST_HEAD(&chunk->map_extend_list);
+ 
+ 	chunk->base_addr = (void *)aligned_addr;
+ 	chunk->start_offset = start_offset;
+ 	chunk->end_offset = region_size - chunk->start_offset - map_size;
+ 
+ 	chunk->nr_pages = region_size >> PAGE_SHIFT;
+ 
+ 	chunk->map = map;
+ 	chunk->map_alloc = init_map_size;
+ 
+ 	/* manage populated page bitmap */
+ 	chunk->immutable = true;
+ 	bitmap_fill(chunk->populated, chunk->nr_pages);
+ 	chunk->nr_populated = chunk->nr_pages;
+ 	chunk->nr_empty_pop_pages = chunk->nr_pages;
+ 
+ 	chunk->contig_hint = chunk->free_size = map_size;
+ 
+ 	if (chunk->start_offset) {
+ 		/* hide the beginning of the bitmap */
+ 		chunk->nr_empty_pop_pages--;
+ 
+ 		chunk->map[0] = 1;
+ 		chunk->map[1] = chunk->start_offset;
+ 		chunk->map_used = 1;
+ 	}
+ 
+ 	/* set chunk's free region */
+ 	chunk->map[++chunk->map_used] =
+ 		(chunk->start_offset + chunk->free_size) | 1;
+ 
+ 	if (chunk->end_offset) {
+ 		/* hide the end of the bitmap */
+ 		chunk->nr_empty_pop_pages--;
+ 
+ 		chunk->map[++chunk->map_used] = region_size | 1;
+ 	}
+ 
+ 	return chunk;
+ }
+ 
++>>>>>>> 0cecf50cf00f (percpu: introduce nr_empty_pop_pages to help empty page accounting)
  static struct pcpu_chunk *pcpu_alloc_chunk(void)
  {
  	struct pcpu_chunk *chunk;
@@@ -1718,11 -1788,13 +1787,16 @@@ int __init pcpu_setup_first_chunk(cons
  	}
  
  	/* link the first chunk in */
++<<<<<<< HEAD
 +	pcpu_first_chunk = dchunk ?: schunk;
 +	pcpu_nr_empty_pop_pages +=
 +		pcpu_count_occupied_pages(pcpu_first_chunk, 1);
++=======
+ 	pcpu_first_chunk = chunk;
+ 	pcpu_nr_empty_pop_pages = pcpu_first_chunk->nr_empty_pop_pages;
++>>>>>>> 0cecf50cf00f (percpu: introduce nr_empty_pop_pages to help empty page accounting)
  	pcpu_chunk_relocate(pcpu_first_chunk, -1);
  
 -	pcpu_stats_chunk_alloc();
 -	trace_percpu_create_chunk(base_addr);
 -
  	/* we're done */
  	pcpu_base_addr = base_addr;
  	return 0;
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu-stats.c
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu-stats.c
* Unmerged path mm/percpu.c
