KVM: x86: Mark expected switch fall-throughs

jira LE-1907
cve CVE-2019-19338
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Gustavo A. R. Silva <gustavo@embeddedor.com>
commit b2869f28e1476cd705bb28c58fd01b0bd661bb99
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/b2869f28.failed

In preparation to enabling -Wimplicit-fallthrough, mark switch
cases where we are expecting to fall through.

This patch fixes the following warnings:

arch/x86/kvm/lapic.c:1037:27: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/lapic.c:1876:3: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/hyperv.c:1637:6: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/svm.c:4396:6: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/mmu.c:4372:36: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/x86.c:3835:6: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/x86.c:7938:23: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/vmx/vmx.c:2015:6: warning: this statement may fall through [-Wimplicit-fallthrough=]
arch/x86/kvm/vmx/vmx.c:1773:6: warning: this statement may fall through [-Wimplicit-fallthrough=]

Warning level 3 was used: -Wimplicit-fallthrough=3

This patch is part of the ongoing efforts to enabling -Wimplicit-fallthrough.

	Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit b2869f28e1476cd705bb28c58fd01b0bd661bb99)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/hyperv.c
index 2b4a2a3ef044,89d20ed1d2e8..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -1302,21 -1620,27 +1302,33 @@@ int kvm_hv_hypercall(struct kvm_vcpu *v
  
  	trace_kvm_hv_hypercall(code, fast, rep_cnt, rep_idx, ingpa, outgpa);
  
 +	/* Hypercall continuation is not supported yet */
 +	if (rep_cnt || rep_idx) {
 +		res = HV_STATUS_INVALID_HYPERCALL_CODE;
 +		goto set_result;
 +	}
 +
  	switch (code) {
  	case HVCALL_NOTIFY_LONG_SPIN_WAIT:
 -		if (unlikely(rep)) {
 -			ret = HV_STATUS_INVALID_HYPERCALL_INPUT;
 -			break;
 -		}
 -		kvm_vcpu_on_spin(vcpu, true);
 +		kvm_vcpu_on_spin(vcpu);
  		break;
++<<<<<<< HEAD
++=======
+ 	case HVCALL_SIGNAL_EVENT:
+ 		if (unlikely(rep)) {
+ 			ret = HV_STATUS_INVALID_HYPERCALL_INPUT;
+ 			break;
+ 		}
+ 		ret = kvm_hvcall_signal_event(vcpu, fast, ingpa);
+ 		if (ret != HV_STATUS_INVALID_PORT_ID)
+ 			break;
+ 		/* fall through - maybe userspace knows this conn_id. */
++>>>>>>> b2869f28e147 (KVM: x86: Mark expected switch fall-throughs)
  	case HVCALL_POST_MESSAGE:
 +	case HVCALL_SIGNAL_EVENT:
  		/* don't bother userspace if it has no way to handle it */
 -		if (unlikely(rep || !vcpu_to_synic(vcpu)->active)) {
 -			ret = HV_STATUS_INVALID_HYPERCALL_INPUT;
 +		if (!vcpu_to_synic(vcpu)->active) {
 +			res = HV_STATUS_INVALID_HYPERCALL_CODE;
  			break;
  		}
  		vcpu->run->exit_reason = KVM_EXIT_HYPERV;
diff --cc arch/x86/kvm/mmu.c
index 9724e772f5d0,da9c42349b1f..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -4297,7 -4365,14 +4297,18 @@@ __reset_rsvds_bits_mask(struct kvm_vcp
  		rsvd_check->rsvd_bits_mask[1][0] =
  			rsvd_check->rsvd_bits_mask[0][0];
  		break;
++<<<<<<< HEAD
 +	case PT64_ROOT_LEVEL:
++=======
+ 	case PT64_ROOT_5LEVEL:
+ 		rsvd_check->rsvd_bits_mask[0][4] = exb_bit_rsvd |
+ 			nonleaf_bit8_rsvd | rsvd_bits(7, 7) |
+ 			rsvd_bits(maxphyaddr, 51);
+ 		rsvd_check->rsvd_bits_mask[1][4] =
+ 			rsvd_check->rsvd_bits_mask[0][4];
+ 		/* fall through */
+ 	case PT64_ROOT_4LEVEL:
++>>>>>>> b2869f28e147 (KVM: x86: Mark expected switch fall-throughs)
  		rsvd_check->rsvd_bits_mask[0][3] = exb_bit_rsvd |
  			nonleaf_bit8_rsvd | rsvd_bits(7, 7) |
  			rsvd_bits(maxphyaddr, 51);
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/hyperv.c
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 182281f5adaf..06e8417db164 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -968,6 +968,7 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 	switch (delivery_mode) {
 	case APIC_DM_LOWEST:
 		vcpu->arch.apic_arb_prio++;
+		/* fall through */
 	case APIC_DM_FIXED:
 		if (unlikely(trig_mode && !level))
 			break;
@@ -1749,6 +1750,7 @@ int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 
 	case APIC_LVT0:
 		apic_manage_nmi_watchdog(apic, val);
+		/* fall through */
 	case APIC_LVTTHMR:
 	case APIC_LVTPC:
 	case APIC_LVT1:
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index e0c07e2bc1d5..b1a6a06f3f70 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -3945,7 +3945,7 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 	case MSR_IA32_APICBASE:
 		if (kvm_vcpu_apicv_active(vcpu))
 			avic_update_vapic_bar(to_svm(vcpu), data);
-		/* Follow through */
+		/* Fall through */
 	default:
 		return kvm_set_msr_common(vcpu, msr);
 	}
* Unmerged path arch/x86/kvm/vmx/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 5be8b7fd8c76..1a168367cbab 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -3382,6 +3382,8 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	case KVM_CAP_HYPERV_SYNIC2:
 		if (cap->args[0])
 			return -EINVAL;
+		/* fall through */
+
 	case KVM_CAP_HYPERV_SYNIC:
 		if (!irqchip_in_kernel(vcpu->kvm))
 			return -EINVAL;
@@ -7033,6 +7035,7 @@ static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 		vcpu->arch.pv.pv_unhalted = false;
 		vcpu->arch.mp_state =
 			KVM_MP_STATE_RUNNABLE;
+		/* fall through */
 	case KVM_MP_STATE_RUNNABLE:
 		vcpu->arch.apf.halted = false;
 		break;
