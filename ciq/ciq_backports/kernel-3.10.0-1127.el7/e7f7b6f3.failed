scsi: lpfc: change snprintf to scnprintf for possible overflow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Silvio Cesare <silvio.cesare@gmail.com>
commit e7f7b6f38a44697428f5a2e7c606de028df2b0e3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/e7f7b6f3.failed

Change snprintf to scnprintf. There are generally two cases where using
snprintf causes problems.

1) Uses of size += snprintf(buf, SIZE - size, fmt, ...)
In this case, if snprintf would have written more characters than what the
buffer size (SIZE) is, then size will end up larger than SIZE. In later
uses of snprintf, SIZE - size will result in a negative number, leading
to problems. Note that size might already be too large by using
size = snprintf before the code reaches a case of size += snprintf.

2) If size is ultimately used as a length parameter for a copy back to user
space, then it will potentially allow for a buffer overflow and information
disclosure when size is greater than SIZE. When the size is used to index
the buffer directly, we can have memory corruption. This also means when
size = snprintf... is used, it may also cause problems since size may become
large.  Copying to userspace is mitigated by the HARDENED_USERCOPY kernel
configuration.

The solution to these issues is to use scnprintf which returns the number of
characters actually written to the buffer, so the size variable will never
exceed SIZE.

	Signed-off-by: Silvio Cesare <silvio.cesare@gmail.com>
	Signed-off-by: Willy Tarreau <w@1wt.eu>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Cc: Dick Kennedy <dick.kennedy@broadcom.com>
	Cc: Dan Carpenter <dan.carpenter@oracle.com>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: Greg KH <greg@kroah.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit e7f7b6f38a44697428f5a2e7c606de028df2b0e3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_debugfs.c
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index f5919f40a134,786413cbb907..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -5032,47 -5197,67 +5036,94 @@@ lpfc_fcp_cpu_map_show(struct device *de
  				phba->cfg_fcp_cpu_map);
  		return len;
  	case 1:
- 		len += snprintf(buf + len, PAGE_SIZE-len,
+ 		len += scnprintf(buf + len, PAGE_SIZE-len,
  				"fcp_cpu_map: HBA centric mapping (%d): "
 -				"%d of %d CPUs online from %d possible CPUs\n",
 -				phba->cfg_fcp_cpu_map, num_online_cpus(),
 -				num_present_cpus(),
 -				phba->sli4_hba.num_possible_cpu);
 +				"%d online CPUs\n",
 +				phba->cfg_fcp_cpu_map,
 +				phba->sli4_hba.num_online_cpu);
 +		break;
 +	case 2:
 +		len += snprintf(buf + len, PAGE_SIZE-len,
 +				"fcp_cpu_map: Driver centric mapping (%d): "
 +				"%d online CPUs\n",
 +				phba->cfg_fcp_cpu_map,
 +				phba->sli4_hba.num_online_cpu);
  		break;
  	}
  
 -	while (phba->sli4_hba.curr_disp_cpu <
 -	       phba->sli4_hba.num_possible_cpu) {
 +	while (phba->sli4_hba.curr_disp_cpu < phba->sli4_hba.num_present_cpu) {
  		cpup = &phba->sli4_hba.cpu_map[phba->sli4_hba.curr_disp_cpu];
  
++<<<<<<< HEAD
 +		/* margin should fit in this and the truncated message */
 +		if (cpup->irq == LPFC_VECTOR_MAP_EMPTY)
 +			len += snprintf(buf + len, PAGE_SIZE-len,
 +					"CPU %02d io_chan %02d "
 +					"physid %d coreid %d\n",
 +					phba->sli4_hba.curr_disp_cpu,
 +					cpup->channel_id, cpup->phys_id,
 +					cpup->core_id);
 +		else
 +			len += snprintf(buf + len, PAGE_SIZE-len,
 +					"CPU %02d io_chan %02d "
 +					"physid %d coreid %d IRQ %d\n",
 +					phba->sli4_hba.curr_disp_cpu,
 +					cpup->channel_id, cpup->phys_id,
 +					cpup->core_id, cpup->irq);
++=======
+ 		if (!cpu_present(phba->sli4_hba.curr_disp_cpu))
+ 			len += scnprintf(buf + len, PAGE_SIZE - len,
+ 					"CPU %02d not present\n",
+ 					phba->sli4_hba.curr_disp_cpu);
+ 		else if (cpup->irq == LPFC_VECTOR_MAP_EMPTY) {
+ 			if (cpup->hdwq == LPFC_VECTOR_MAP_EMPTY)
+ 				len += scnprintf(
+ 					buf + len, PAGE_SIZE - len,
+ 					"CPU %02d hdwq None "
+ 					"physid %d coreid %d ht %d\n",
+ 					phba->sli4_hba.curr_disp_cpu,
+ 					cpup->phys_id,
+ 					cpup->core_id, cpup->hyper);
+ 			else
+ 				len += scnprintf(
+ 					buf + len, PAGE_SIZE - len,
+ 					"CPU %02d EQ %04d hdwq %04d "
+ 					"physid %d coreid %d ht %d\n",
+ 					phba->sli4_hba.curr_disp_cpu,
+ 					cpup->eq, cpup->hdwq, cpup->phys_id,
+ 					cpup->core_id, cpup->hyper);
+ 		} else {
+ 			if (cpup->hdwq == LPFC_VECTOR_MAP_EMPTY)
+ 				len += scnprintf(
+ 					buf + len, PAGE_SIZE - len,
+ 					"CPU %02d hdwq None "
+ 					"physid %d coreid %d ht %d IRQ %d\n",
+ 					phba->sli4_hba.curr_disp_cpu,
+ 					cpup->phys_id,
+ 					cpup->core_id, cpup->hyper, cpup->irq);
+ 			else
+ 				len += scnprintf(
+ 					buf + len, PAGE_SIZE - len,
+ 					"CPU %02d EQ %04d hdwq %04d "
+ 					"physid %d coreid %d ht %d IRQ %d\n",
+ 					phba->sli4_hba.curr_disp_cpu,
+ 					cpup->eq, cpup->hdwq, cpup->phys_id,
+ 					cpup->core_id, cpup->hyper, cpup->irq);
+ 		}
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  
  		phba->sli4_hba.curr_disp_cpu++;
  
  		/* display max number of CPUs keeping some margin */
  		if (phba->sli4_hba.curr_disp_cpu <
 -				phba->sli4_hba.num_possible_cpu &&
 +				phba->sli4_hba.num_present_cpu &&
  				(len >= (PAGE_SIZE - 64))) {
++<<<<<<< HEAD
 +			len += snprintf(buf + len, PAGE_SIZE-len, "more...\n");
++=======
+ 			len += scnprintf(buf + len,
+ 					PAGE_SIZE - len, "more...\n");
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  			break;
  		}
  	}
diff --cc drivers/scsi/lpfc/lpfc_debugfs.c
index 530acda02682,1ee857d9d165..000000000000
--- a/drivers/scsi/lpfc/lpfc_debugfs.c
+++ b/drivers/scsi/lpfc/lpfc_debugfs.c
@@@ -378,6 -380,271 +380,274 @@@ skipit
  	return len;
  }
  
++<<<<<<< HEAD
++=======
+ static int lpfc_debugfs_last_xripool;
+ 
+ /**
+  * lpfc_debugfs_common_xri_data - Dump Hardware Queue info to a buffer
+  * @phba: The HBA to gather host buffer info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the Hardware Queue info from the @phba to @buf up to
+  * @size number of bytes. A header that describes the current hdwq state will be
+  * dumped to @buf first and then info on each hdwq entry will be dumped to @buf
+  * until @size bytes have been dumped or all the hdwq info has been dumped.
+  *
+  * Notes:
+  * This routine will rotate through each configured Hardware Queue each
+  * time called.
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_commonxripools_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	struct lpfc_sli4_hdw_queue *qp;
+ 	int len = 0;
+ 	int i, out;
+ 	unsigned long iflag;
+ 
+ 	for (i = 0; i < phba->cfg_hdw_queue; i++) {
+ 		if (len > (LPFC_DUMP_MULTIXRIPOOL_SIZE - 80))
+ 			break;
+ 		qp = &phba->sli4_hba.hdwq[lpfc_debugfs_last_xripool];
+ 
+ 		len += scnprintf(buf + len, size - len, "HdwQ %d Info ", i);
+ 		spin_lock_irqsave(&qp->abts_scsi_buf_list_lock, iflag);
+ 		spin_lock(&qp->abts_nvme_buf_list_lock);
+ 		spin_lock(&qp->io_buf_list_get_lock);
+ 		spin_lock(&qp->io_buf_list_put_lock);
+ 		out = qp->total_io_bufs - (qp->get_io_bufs + qp->put_io_bufs +
+ 			qp->abts_scsi_io_bufs + qp->abts_nvme_io_bufs);
+ 		len += scnprintf(buf + len, size - len,
+ 				 "tot:%d get:%d put:%d mt:%d "
+ 				 "ABTS scsi:%d nvme:%d Out:%d\n",
+ 			qp->total_io_bufs, qp->get_io_bufs, qp->put_io_bufs,
+ 			qp->empty_io_bufs, qp->abts_scsi_io_bufs,
+ 			qp->abts_nvme_io_bufs, out);
+ 		spin_unlock(&qp->io_buf_list_put_lock);
+ 		spin_unlock(&qp->io_buf_list_get_lock);
+ 		spin_unlock(&qp->abts_nvme_buf_list_lock);
+ 		spin_unlock_irqrestore(&qp->abts_scsi_buf_list_lock, iflag);
+ 
+ 		lpfc_debugfs_last_xripool++;
+ 		if (lpfc_debugfs_last_xripool >= phba->cfg_hdw_queue)
+ 			lpfc_debugfs_last_xripool = 0;
+ 	}
+ 
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_multixripools_data - Display multi-XRI pools information
+  * @phba: The HBA to gather host buffer info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine displays current multi-XRI pools information including XRI
+  * count in public, private and txcmplq. It also displays current high and
+  * low watermark.
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_multixripools_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	u32 i;
+ 	u32 hwq_count;
+ 	struct lpfc_sli4_hdw_queue *qp;
+ 	struct lpfc_multixri_pool *multixri_pool;
+ 	struct lpfc_pvt_pool *pvt_pool;
+ 	struct lpfc_pbl_pool *pbl_pool;
+ 	u32 txcmplq_cnt;
+ 	char tmp[LPFC_DEBUG_OUT_LINE_SZ] = {0};
+ 
+ 	if (phba->sli_rev != LPFC_SLI_REV4)
+ 		return 0;
+ 
+ 	if (!phba->sli4_hba.hdwq)
+ 		return 0;
+ 
+ 	if (!phba->cfg_xri_rebalancing) {
+ 		i = lpfc_debugfs_commonxripools_data(phba, buf, size);
+ 		return i;
+ 	}
+ 
+ 	/*
+ 	 * Pbl: Current number of free XRIs in public pool
+ 	 * Pvt: Current number of free XRIs in private pool
+ 	 * Busy: Current number of outstanding XRIs
+ 	 * HWM: Current high watermark
+ 	 * pvt_empty: Incremented by 1 when IO submission fails (no xri)
+ 	 * pbl_empty: Incremented by 1 when all pbl_pool are empty during
+ 	 *            IO submission
+ 	 */
+ 	scnprintf(tmp, sizeof(tmp),
+ 		  "HWQ:  Pbl  Pvt Busy  HWM |  pvt_empty  pbl_empty ");
+ 	if (strlcat(buf, tmp, size) >= size)
+ 		return strnlen(buf, size);
+ 
+ #ifdef LPFC_MXP_STAT
+ 	/*
+ 	 * MAXH: Max high watermark seen so far
+ 	 * above_lmt: Incremented by 1 if xri_owned > xri_limit during
+ 	 *            IO submission
+ 	 * below_lmt: Incremented by 1 if xri_owned <= xri_limit  during
+ 	 *            IO submission
+ 	 * locPbl_hit: Incremented by 1 if successfully get a batch of XRI from
+ 	 *             local pbl_pool
+ 	 * othPbl_hit: Incremented by 1 if successfully get a batch of XRI from
+ 	 *             other pbl_pool
+ 	 */
+ 	scnprintf(tmp, sizeof(tmp),
+ 		  "MAXH  above_lmt  below_lmt locPbl_hit othPbl_hit");
+ 	if (strlcat(buf, tmp, size) >= size)
+ 		return strnlen(buf, size);
+ 
+ 	/*
+ 	 * sPbl: snapshot of Pbl 15 sec after stat gets cleared
+ 	 * sPvt: snapshot of Pvt 15 sec after stat gets cleared
+ 	 * sBusy: snapshot of Busy 15 sec after stat gets cleared
+ 	 */
+ 	scnprintf(tmp, sizeof(tmp),
+ 		  " | sPbl sPvt sBusy");
+ 	if (strlcat(buf, tmp, size) >= size)
+ 		return strnlen(buf, size);
+ #endif
+ 
+ 	scnprintf(tmp, sizeof(tmp), "\n");
+ 	if (strlcat(buf, tmp, size) >= size)
+ 		return strnlen(buf, size);
+ 
+ 	hwq_count = phba->cfg_hdw_queue;
+ 	for (i = 0; i < hwq_count; i++) {
+ 		qp = &phba->sli4_hba.hdwq[i];
+ 		multixri_pool = qp->p_multixri_pool;
+ 		if (!multixri_pool)
+ 			continue;
+ 		pbl_pool = &multixri_pool->pbl_pool;
+ 		pvt_pool = &multixri_pool->pvt_pool;
+ 		txcmplq_cnt = qp->fcp_wq->pring->txcmplq_cnt;
+ 		if (qp->nvme_wq)
+ 			txcmplq_cnt += qp->nvme_wq->pring->txcmplq_cnt;
+ 
+ 		scnprintf(tmp, sizeof(tmp),
+ 			  "%03d: %4d %4d %4d %4d | %10d %10d ",
+ 			  i, pbl_pool->count, pvt_pool->count,
+ 			  txcmplq_cnt, pvt_pool->high_watermark,
+ 			  qp->empty_io_bufs, multixri_pool->pbl_empty_count);
+ 		if (strlcat(buf, tmp, size) >= size)
+ 			break;
+ 
+ #ifdef LPFC_MXP_STAT
+ 		scnprintf(tmp, sizeof(tmp),
+ 			  "%4d %10d %10d %10d %10d",
+ 			  multixri_pool->stat_max_hwm,
+ 			  multixri_pool->above_limit_count,
+ 			  multixri_pool->below_limit_count,
+ 			  multixri_pool->local_pbl_hit_count,
+ 			  multixri_pool->other_pbl_hit_count);
+ 		if (strlcat(buf, tmp, size) >= size)
+ 			break;
+ 
+ 		scnprintf(tmp, sizeof(tmp),
+ 			  " | %4d %4d %5d",
+ 			  multixri_pool->stat_pbl_count,
+ 			  multixri_pool->stat_pvt_count,
+ 			  multixri_pool->stat_busy_count);
+ 		if (strlcat(buf, tmp, size) >= size)
+ 			break;
+ #endif
+ 
+ 		scnprintf(tmp, sizeof(tmp), "\n");
+ 		if (strlcat(buf, tmp, size) >= size)
+ 			break;
+ 	}
+ 	return strnlen(buf, size);
+ }
+ 
+ 
+ #ifdef LPFC_HDWQ_LOCK_STAT
+ static int lpfc_debugfs_last_lock;
+ 
+ /**
+  * lpfc_debugfs_lockstat_data - Dump Hardware Queue info to a buffer
+  * @phba: The HBA to gather host buffer info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the Hardware Queue info from the @phba to @buf up to
+  * @size number of bytes. A header that describes the current hdwq state will be
+  * dumped to @buf first and then info on each hdwq entry will be dumped to @buf
+  * until @size bytes have been dumped or all the hdwq info has been dumped.
+  *
+  * Notes:
+  * This routine will rotate through each configured Hardware Queue each
+  * time called.
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_lockstat_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	struct lpfc_sli4_hdw_queue *qp;
+ 	int len = 0;
+ 	int i;
+ 
+ 	if (phba->sli_rev != LPFC_SLI_REV4)
+ 		return 0;
+ 
+ 	if (!phba->sli4_hba.hdwq)
+ 		return 0;
+ 
+ 	for (i = 0; i < phba->cfg_hdw_queue; i++) {
+ 		if (len > (LPFC_HDWQINFO_SIZE - 100))
+ 			break;
+ 		qp = &phba->sli4_hba.hdwq[lpfc_debugfs_last_lock];
+ 
+ 		len += scnprintf(buf + len, size - len, "HdwQ %03d Lock ", i);
+ 		if (phba->cfg_xri_rebalancing) {
+ 			len += scnprintf(buf + len, size - len,
+ 					 "get_pvt:%d mv_pvt:%d "
+ 					 "mv2pub:%d mv2pvt:%d "
+ 					 "put_pvt:%d put_pub:%d wq:%d\n",
+ 					 qp->lock_conflict.alloc_pvt_pool,
+ 					 qp->lock_conflict.mv_from_pvt_pool,
+ 					 qp->lock_conflict.mv_to_pub_pool,
+ 					 qp->lock_conflict.mv_to_pvt_pool,
+ 					 qp->lock_conflict.free_pvt_pool,
+ 					 qp->lock_conflict.free_pub_pool,
+ 					 qp->lock_conflict.wq_access);
+ 		} else {
+ 			len += scnprintf(buf + len, size - len,
+ 					 "get:%d put:%d free:%d wq:%d\n",
+ 					 qp->lock_conflict.alloc_xri_get,
+ 					 qp->lock_conflict.alloc_xri_put,
+ 					 qp->lock_conflict.free_xri,
+ 					 qp->lock_conflict.wq_access);
+ 		}
+ 
+ 		lpfc_debugfs_last_lock++;
+ 		if (lpfc_debugfs_last_lock >= phba->cfg_hdw_queue)
+ 			lpfc_debugfs_last_lock = 0;
+ 	}
+ 
+ 	return len;
+ }
+ #endif
+ 
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  static int lpfc_debugfs_last_hba_slim_off;
  
  /**
@@@ -858,20 -1127,20 +1128,20 @@@ lpfc_debugfs_nvmestat_data(struct lpfc_
  				atomic_read(&tgtp->xmt_abort_rsp),
  				atomic_read(&tgtp->xmt_abort_rsp_error));
  
- 		len +=  snprintf(buf + len, size - len, "\n");
+ 		len +=  scnprintf(buf + len, size - len, "\n");
  
  		cnt = 0;
 -		spin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);
 +		spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
  		list_for_each_entry_safe(ctxp, next_ctxp,
  				&phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
  				list) {
  			cnt++;
  		}
 -		spin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);
 +		spin_unlock(&phba->sli4_hba.abts_nvme_buf_list_lock);
  		if (cnt) {
- 			len += snprintf(buf + len, size - len,
+ 			len += scnprintf(buf + len, size - len,
  					"ABORT: %d ctx entries\n", cnt);
 -			spin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);
 +			spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
  			list_for_each_entry_safe(ctxp, next_ctxp,
  				    &phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
  				    list) {
@@@ -909,10 -1178,10 +1179,15 @@@
  		if (!lport)
  			return len;
  
++<<<<<<< HEAD
 +		len += snprintf(buf + len, size - len,
 +				"\nNVME Lport Statistics\n");
++=======
+ 		len += scnprintf(buf + len, size - len,
+ 				"\nNVME HDWQ Statistics\n");
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  
- 		len += snprintf(buf + len, size - len,
+ 		len += scnprintf(buf + len, size - len,
  				"LS: Xmt %016x Cmpl %016x\n",
  				atomic_read(&lport->fc4NvmeLsRequests),
  				atomic_read(&lport->fc4NvmeLsCmpls));
@@@ -936,11 -1201,11 +1211,16 @@@
  			if (i >= 32)
  				continue;
  
++<<<<<<< HEAD
 +			len += snprintf(buf + len, PAGE_SIZE - len,
 +					"FCP (%d): Rd %016llx Wr %016llx "
++=======
+ 			len += scnprintf(buf + len, PAGE_SIZE - len,
+ 					"HDWQ (%d): Rd %016llx Wr %016llx "
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  					"IO %016llx ",
  					i, data1, data2, data3);
- 			len += snprintf(buf + len, PAGE_SIZE - len,
+ 			len += scnprintf(buf + len, PAGE_SIZE - len,
  					"Cmpl %016llx OutIO %016llx\n",
  					tot, ((data1 + data2 + data3) - tot));
  		}
@@@ -1297,62 -1622,73 +1577,124 @@@ static in
  lpfc_debugfs_cpucheck_data(struct lpfc_vport *vport, char *buf, int size)
  {
  	struct lpfc_hba   *phba = vport->phba;
 -	struct lpfc_sli4_hdw_queue *qp;
 -	int i, j, max_cnt;
 +	int i;
  	int len = 0;
 -	uint32_t tot_xmt;
 -	uint32_t tot_rcv;
 -	uint32_t tot_cmpl;
 +	uint32_t tot_xmt = 0;
 +	uint32_t tot_rcv = 0;
 +	uint32_t tot_cmpl = 0;
 +	uint32_t tot_ccmpl = 0;
 +
++<<<<<<< HEAD
 +	if (phba->nvmet_support == 0) {
 +		/* NVME Initiator */
 +		len += snprintf(buf + len, PAGE_SIZE - len,
 +				"CPUcheck %s\n",
 +				(phba->cpucheck_on & LPFC_CHECK_NVME_IO ?
 +					"Enabled" : "Disabled"));
 +		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
 +			if (i >= LPFC_CHECK_CPU_CNT)
 +				break;
 +			len += snprintf(buf + len, PAGE_SIZE - len,
 +					"%02d: xmit x%08x cmpl x%08x\n",
 +					i, phba->cpucheck_xmt_io[i],
 +					phba->cpucheck_cmpl_io[i]);
 +			tot_xmt += phba->cpucheck_xmt_io[i];
 +			tot_cmpl += phba->cpucheck_cmpl_io[i];
 +		}
 +		len += snprintf(buf + len, PAGE_SIZE - len,
 +				"tot:xmit x%08x cmpl x%08x\n",
 +				tot_xmt, tot_cmpl);
 +		return len;
 +	}
  
 +	/* NVME Target */
 +	len += snprintf(buf + len, PAGE_SIZE - len,
 +			"CPUcheck %s ",
 +			(phba->cpucheck_on & LPFC_CHECK_NVMET_IO ?
 +				"IO Enabled - " : "IO Disabled - "));
 +	len += snprintf(buf + len, PAGE_SIZE - len,
 +			"%s\n",
 +			(phba->cpucheck_on & LPFC_CHECK_NVMET_RCV ?
 +				"Rcv Enabled\n" : "Rcv Disabled\n"));
 +	for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
 +		if (i >= LPFC_CHECK_CPU_CNT)
 +			break;
 +		len += snprintf(buf + len, PAGE_SIZE - len,
 +				"%02d: xmit x%08x ccmpl x%08x "
 +				"cmpl x%08x rcv x%08x\n",
 +				i, phba->cpucheck_xmt_io[i],
 +				phba->cpucheck_ccmpl_io[i],
 +				phba->cpucheck_cmpl_io[i],
 +				phba->cpucheck_rcv_io[i]);
 +		tot_xmt += phba->cpucheck_xmt_io[i];
 +		tot_rcv += phba->cpucheck_rcv_io[i];
 +		tot_cmpl += phba->cpucheck_cmpl_io[i];
 +		tot_ccmpl += phba->cpucheck_ccmpl_io[i];
++=======
+ 	len += scnprintf(buf + len, PAGE_SIZE - len,
+ 			"CPUcheck %s ",
+ 			(phba->cpucheck_on & LPFC_CHECK_NVME_IO ?
+ 				"Enabled" : "Disabled"));
+ 	if (phba->nvmet_support) {
+ 		len += scnprintf(buf + len, PAGE_SIZE - len,
+ 				"%s\n",
+ 				(phba->cpucheck_on & LPFC_CHECK_NVMET_RCV ?
+ 					"Rcv Enabled\n" : "Rcv Disabled\n"));
+ 	} else {
+ 		len += scnprintf(buf + len, PAGE_SIZE - len, "\n");
+ 	}
+ 	max_cnt = size - LPFC_DEBUG_OUT_LINE_SZ;
+ 
+ 	for (i = 0; i < phba->cfg_hdw_queue; i++) {
+ 		qp = &phba->sli4_hba.hdwq[i];
+ 
+ 		tot_rcv = 0;
+ 		tot_xmt = 0;
+ 		tot_cmpl = 0;
+ 		for (j = 0; j < LPFC_CHECK_CPU_CNT; j++) {
+ 			tot_xmt += qp->cpucheck_xmt_io[j];
+ 			tot_cmpl += qp->cpucheck_cmpl_io[j];
+ 			if (phba->nvmet_support)
+ 				tot_rcv += qp->cpucheck_rcv_io[j];
+ 		}
+ 
+ 		/* Only display Hardware Qs with something */
+ 		if (!tot_xmt && !tot_cmpl && !tot_rcv)
+ 			continue;
+ 
+ 		len += scnprintf(buf + len, PAGE_SIZE - len,
+ 				"HDWQ %03d: ", i);
+ 		for (j = 0; j < LPFC_CHECK_CPU_CNT; j++) {
+ 			/* Only display non-zero counters */
+ 			if (!qp->cpucheck_xmt_io[j] &&
+ 			    !qp->cpucheck_cmpl_io[j] &&
+ 			    !qp->cpucheck_rcv_io[j])
+ 				continue;
+ 			if (phba->nvmet_support) {
+ 				len += scnprintf(buf + len, PAGE_SIZE - len,
+ 						"CPU %03d: %x/%x/%x ", j,
+ 						qp->cpucheck_rcv_io[j],
+ 						qp->cpucheck_xmt_io[j],
+ 						qp->cpucheck_cmpl_io[j]);
+ 			} else {
+ 				len += scnprintf(buf + len, PAGE_SIZE - len,
+ 						"CPU %03d: %x/%x ", j,
+ 						qp->cpucheck_xmt_io[j],
+ 						qp->cpucheck_cmpl_io[j]);
+ 			}
+ 		}
+ 		len += scnprintf(buf + len, PAGE_SIZE - len,
+ 				"Total: %x\n", tot_xmt);
+ 		if (len >= max_cnt) {
+ 			len += scnprintf(buf + len, PAGE_SIZE - len,
+ 					"Truncated ...\n");
+ 			return len;
+ 		}
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  	}
 +	len += snprintf(buf + len, PAGE_SIZE - len,
 +			"tot:xmit x%08x ccmpl x%08x cmpl x%08x rcv x%08x\n",
 +			tot_xmt, tot_ccmpl, tot_cmpl, tot_rcv);
  	return len;
  }
  
@@@ -3178,13 -3767,13 +3521,18 @@@ __lpfc_idiag_print_wq(struct lpfc_queu
  			"AssocCQID[%04d]: WQ-STAT[oflow:x%x posted:x%llx]\n",
  			qp->assoc_qid, qp->q_cnt_1,
  			(unsigned long long)qp->q_cnt_4);
- 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 	len += scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
  			"\t\tWQID[%02d], QE-CNT[%04d], QE-SZ[%04d], "
 -			"HST-IDX[%04d], PRT-IDX[%04d], NTFI[%03d]",
 +			"HST-IDX[%04d], PRT-IDX[%04d], PST[%03d]",
  			qp->queue_id, qp->entry_count,
  			qp->entry_size, qp->host_index,
++<<<<<<< HEAD
 +			qp->hba_index, qp->entry_repost);
 +	len +=  snprintf(pbuffer + len,
++=======
+ 			qp->hba_index, qp->notify_interval);
+ 	len +=  scnprintf(pbuffer + len,
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  			LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
  	return len;
  }
@@@ -3229,14 -3820,15 +3577,15 @@@ __lpfc_idiag_print_cq(struct lpfc_queu
  			"xabt:x%x wq:x%llx]\n",
  			qp->assoc_qid, qp->q_cnt_1, qp->q_cnt_2,
  			qp->q_cnt_3, (unsigned long long)qp->q_cnt_4);
- 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 	len += scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
  			"\tCQID[%02d], QE-CNT[%04d], QE-SZ[%04d], "
 -			"HST-IDX[%04d], NTFI[%03d], PLMT[%03d]",
 +			"HST-IDX[%04d], PRT-IDX[%04d], PST[%03d]",
  			qp->queue_id, qp->entry_count,
  			qp->entry_size, qp->host_index,
 -			qp->notify_interval, qp->max_proc_limit);
 +			qp->hba_index, qp->entry_repost);
  
- 	len +=  snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
+ 	len +=  scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\n");
  
  	return len;
  }
@@@ -3255,17 -3847,17 +3604,22 @@@ __lpfc_idiag_print_rqpair(struct lpfc_q
  			"posted:x%x rcv:x%llx]\n",
  			qp->assoc_qid, qp->q_cnt_1, qp->q_cnt_2,
  			qp->q_cnt_3, (unsigned long long)qp->q_cnt_4);
- 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 	len += scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
  			"\t\tHQID[%02d], QE-CNT[%04d], QE-SZ[%04d], "
 -			"HST-IDX[%04d], PRT-IDX[%04d], NTFI[%03d]\n",
 +			"HST-IDX[%04d], PRT-IDX[%04d], PST[%03d]\n",
  			qp->queue_id, qp->entry_count, qp->entry_size,
++<<<<<<< HEAD
 +			qp->host_index, qp->hba_index, qp->entry_repost);
 +	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
++=======
+ 			qp->host_index, qp->hba_index, qp->notify_interval);
+ 	len += scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  			"\t\tDQID[%02d], QE-CNT[%04d], QE-SZ[%04d], "
 -			"HST-IDX[%04d], PRT-IDX[%04d], NTFI[%03d]\n",
 +			"HST-IDX[%04d], PRT-IDX[%04d], PST[%03d]\n",
  			datqp->queue_id, datqp->entry_count,
  			datqp->entry_size, datqp->host_index,
 -			datqp->hba_index, datqp->notify_interval);
 +			datqp->hba_index, datqp->entry_repost);
  	return len;
  }
  
@@@ -3350,12 -3936,14 +3704,19 @@@ __lpfc_idiag_print_eq(struct lpfc_queu
  			"cqe_proc:x%x eqe_proc:x%llx eqd %d]\n",
  			eqtype, qp->q_cnt_1, qp->q_cnt_2, qp->q_cnt_3,
  			(unsigned long long)qp->q_cnt_4, qp->q_mode);
- 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 	len += scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
  			"EQID[%02d], QE-CNT[%04d], QE-SZ[%04d], "
 -			"HST-IDX[%04d], NTFI[%03d], PLMT[%03d], AFFIN[%03d]",
 +			"HST-IDX[%04d], PRT-IDX[%04d], PST[%03d]",
  			qp->queue_id, qp->entry_count, qp->entry_size,
++<<<<<<< HEAD
 +			qp->host_index, qp->hba_index, qp->entry_repost);
 +	len +=  snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
++=======
+ 			qp->host_index, qp->notify_interval,
+ 			qp->max_proc_limit, qp->chann);
+ 	len +=  scnprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\n");
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  
  	return len;
  }
@@@ -3401,24 -3989,20 +3762,31 @@@ lpfc_idiag_queinfo_read(struct file *fi
  	spin_lock_irq(&phba->hbalock);
  
  	/* Fast-path event queue */
 -	if (phba->sli4_hba.hdwq && phba->cfg_hdw_queue) {
 +	if (phba->sli4_hba.hba_eq && phba->io_channel_irqs) {
  
  		x = phba->lpfc_idiag_last_eq;
 -		phba->lpfc_idiag_last_eq++;
 -		if (phba->lpfc_idiag_last_eq >= phba->cfg_hdw_queue)
 +		if (phba->cfg_fof && (x >= phba->io_channel_irqs)) {
  			phba->lpfc_idiag_last_eq = 0;
 -
 +			goto fof;
 +		}
 +		phba->lpfc_idiag_last_eq++;
 +		if (phba->lpfc_idiag_last_eq >= phba->io_channel_irqs)
 +			if (phba->cfg_fof == 0)
 +				phba->lpfc_idiag_last_eq = 0;
 +
++<<<<<<< HEAD
 +		len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
 +					"EQ %d out of %d HBA EQs\n",
 +					x, phba->io_channel_irqs);
++=======
+ 		len += scnprintf(pbuffer + len,
+ 				 LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 				 "HDWQ %d out of %d HBA HDWQs\n",
+ 				 x, phba->cfg_hdw_queue);
++>>>>>>> e7f7b6f38a44 (scsi: lpfc: change snprintf to scnprintf for possible overflow)
  
  		/* Fast-path EQ */
 -		qp = phba->sli4_hba.hdwq[x].hba_eq;
 +		qp = phba->sli4_hba.hba_eq[x];
  		if (!qp)
  			goto out;
  
@@@ -3586,9 -4141,9 +3954,9 @@@ lpfc_idiag_queacc_read_qe(char *pbuffer
  			"QE-INDEX[%04d]:\n", index);
  
  	offset = 0;
 -	pentry = lpfc_sli4_qe(pque, index);
 +	pentry = pque->qe[index].address;
  	while (esize > 0) {
- 		len += snprintf(pbuffer+len, LPFC_QUE_ACC_BUF_SIZE-len,
+ 		len += scnprintf(pbuffer+len, LPFC_QUE_ACC_BUF_SIZE-len,
  				"%08x ", *pentry);
  		pentry++;
  		offset += sizeof(uint32_t);
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
diff --git a/drivers/scsi/lpfc/lpfc_ct.c b/drivers/scsi/lpfc/lpfc_ct.c
index 55175c946dcb..f5ad099b0b84 100644
--- a/drivers/scsi/lpfc/lpfc_ct.c
+++ b/drivers/scsi/lpfc/lpfc_ct.c
@@ -1430,7 +1430,7 @@ lpfc_vport_symbolic_port_name(struct lpfc_vport *vport, char *symbol,
 	 * Name object.  NPIV is not in play so this integer
 	 * value is sufficient and unique per FC-ID.
 	 */
-	n = snprintf(symbol, size, "%d", vport->phba->brd_no);
+	n = scnprintf(symbol, size, "%d", vport->phba->brd_no);
 	return n;
 }
 
@@ -1444,26 +1444,26 @@ lpfc_vport_symbolic_node_name(struct lpfc_vport *vport, char *symbol,
 
 	lpfc_decode_firmware_rev(vport->phba, fwrev, 0);
 
-	n = snprintf(symbol, size, "Emulex %s", vport->phba->ModelName);
+	n = scnprintf(symbol, size, "Emulex %s", vport->phba->ModelName);
 	if (size < n)
 		return n;
 
-	n += snprintf(symbol + n, size - n, " FV%s", fwrev);
+	n += scnprintf(symbol + n, size - n, " FV%s", fwrev);
 	if (size < n)
 		return n;
 
-	n += snprintf(symbol + n, size - n, " DV%s.",
+	n += scnprintf(symbol + n, size - n, " DV%s.",
 		      lpfc_release_version);
 	if (size < n)
 		return n;
 
-	n += snprintf(symbol + n, size - n, " HN:%s.",
+	n += scnprintf(symbol + n, size - n, " HN:%s.",
 		      init_utsname()->nodename);
 	if (size < n)
 		return n;
 
 	/* Note :- OS name is "Linux" */
-	n += snprintf(symbol + n, size - n, " OS:%s",
+	n += scnprintf(symbol + n, size - n, " OS:%s",
 		      init_utsname()->sysname);
 	return n;
 }
* Unmerged path drivers/scsi/lpfc/lpfc_debugfs.c
diff --git a/drivers/scsi/lpfc/lpfc_debugfs.h b/drivers/scsi/lpfc/lpfc_debugfs.h
index 30efc7bf91bd..824de3e410ca 100644
--- a/drivers/scsi/lpfc/lpfc_debugfs.h
+++ b/drivers/scsi/lpfc/lpfc_debugfs.h
@@ -342,7 +342,7 @@ lpfc_debug_dump_qe(struct lpfc_queue *q, uint32_t idx)
 	pword = q->qe[idx].address;
 
 	len = 0;
-	len += snprintf(line_buf+len, LPFC_LBUF_SZ-len, "QE[%04d]: ", idx);
+	len += scnprintf(line_buf+len, LPFC_LBUF_SZ-len, "QE[%04d]: ", idx);
 	if (qe_word_cnt > 8)
 		printk(KERN_ERR "%s\n", line_buf);
 
@@ -353,11 +353,11 @@ lpfc_debug_dump_qe(struct lpfc_queue *q, uint32_t idx)
 			if (qe_word_cnt > 8) {
 				len = 0;
 				memset(line_buf, 0, LPFC_LBUF_SZ);
-				len += snprintf(line_buf+len, LPFC_LBUF_SZ-len,
+				len += scnprintf(line_buf+len, LPFC_LBUF_SZ-len,
 						"%03d: ", i);
 			}
 		}
-		len += snprintf(line_buf+len, LPFC_LBUF_SZ-len, "%08x ",
+		len += scnprintf(line_buf+len, LPFC_LBUF_SZ-len, "%08x ",
 				((uint32_t)*pword) & 0xffffffff);
 		pword++;
 	}
