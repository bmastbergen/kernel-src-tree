perf/x86: Support constraint ranges

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 63b79f6ebc464afb730bc45762c820795e276da1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/63b79f6e.failed

Icelake extended the general counters to 8, even when SMT is enabled.
However only a (large) subset of the events can be used on all 8
counters.

The events that can or cannot be used on all counters are organized
in ranges.

A lot of scheduler constraints are required to handle all this.

To avoid blowing up the tables add event code ranges to the constraint
tables, and a new inline function to match them.

Originally-by: Andi Kleen <ak@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # developer hat on
	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # maintainer hat on
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: acme@kernel.org
	Cc: jolsa@kernel.org
Link: https://lkml.kernel.org/r/20190402194509.2832-8-kan.liang@linux.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 63b79f6ebc464afb730bc45762c820795e276da1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/perf_event.h
index f71140a672a3,0ff0c5ae8c29..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -47,12 -49,19 +47,22 @@@ struct event_constraint 
  		unsigned long	idxmsk[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
  		u64		idxmsk64;
  	};
- 	u64	code;
- 	u64	cmask;
- 	int	weight;
- 	int	overlap;
- 	int	flags;
+ 	u64		code;
+ 	u64		cmask;
+ 	int		weight;
+ 	int		overlap;
+ 	int		flags;
+ 	unsigned int	size;
  };
++<<<<<<< HEAD
++=======
+ 
+ static inline bool constraint_match(struct event_constraint *c, u64 ecode)
+ {
+ 	return ((ecode & c->cmask) - c->code) <= (u64)c->size;
+ }
+ 
++>>>>>>> 63b79f6ebc46 (perf/x86: Support constraint ranges)
  /*
   * struct hw_perf_event.flags flags
   */
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index 9266ff826d65..80bc430df945 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -2624,7 +2624,7 @@ x86_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 
 	if (x86_pmu.event_constraints) {
 		for_each_event_constraint(c, x86_pmu.event_constraints) {
-			if ((event->hw.config & c->cmask) == c->code) {
+			if (constraint_match(c, event->hw.config)) {
 				event->hw.flags |= c->flags;
 				return c;
 			}
diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c
index 722756c9e89d..8a182e353b7c 100644
--- a/arch/x86/events/intel/ds.c
+++ b/arch/x86/events/intel/ds.c
@@ -794,7 +794,7 @@ struct event_constraint *intel_pebs_constraints(struct perf_event *event)
 
 	if (x86_pmu.pebs_constraints) {
 		for_each_event_constraint(c, x86_pmu.pebs_constraints) {
-			if ((event->hw.config & c->cmask) == c->code) {
+			if (constraint_match(c, event->hw.config)) {
 				event->hw.flags |= c->flags;
 				return c;
 			}
* Unmerged path arch/x86/events/perf_event.h
