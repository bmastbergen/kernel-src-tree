mm: prevent get_user_pages() from overflowing page refcount

jira LE-1907
cve CVE-2019-11487
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Linus Torvalds <torvalds@linux-foundation.org>
commit 8fde12ca79aff9b5ba951fce1a2641901b8d8e64
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/8fde12ca.failed

If the page refcount wraps around past zero, it will be freed while
there are still four billion references to it.  One of the possible
avenues for an attacker to try to make this happen is by doing direct IO
on a page multiple times.  This patch makes get_user_pages() refuse to
take a new page reference if there are already more than two billion
references to the page.

	Reported-by: Jann Horn <jannh@google.com>
	Acked-by: Matthew Wilcox <willy@infradead.org>
	Cc: stable@kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8fde12ca79aff9b5ba951fce1a2641901b8d8e64)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index c89c0a4959f6,81e0bdefa2cc..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -219,13 -144,23 +219,20 @@@ split_fallthrough
  		}
  	}
  
 -	if (flags & FOLL_SPLIT && PageTransCompound(page)) {
 -		int ret;
 -		get_page(page);
 -		pte_unmap_unlock(ptep, ptl);
 -		lock_page(page);
 -		ret = split_huge_page(page);
 -		unlock_page(page);
 -		put_page(page);
 -		if (ret)
 -			return ERR_PTR(ret);
 -		goto retry;
 -	}
 -
 +	if (flags & FOLL_GET) {
 +		get_page_foll(page);
 +
++<<<<<<< HEAD
 +		/* drop the pgmap reference now that we hold the page */
 +		if (pgmap) {
 +			put_dev_pagemap(pgmap);
 +			pgmap = NULL;
++=======
+ 	if (flags & FOLL_GET) {
+ 		if (unlikely(!try_get_page(page))) {
+ 			page = ERR_PTR(-ENOMEM);
+ 			goto out;
++>>>>>>> 8fde12ca79af (mm: prevent get_user_pages() from overflowing page refcount)
  		}
  	}
  	if (flags & FOLL_TOUCH) {
@@@ -269,28 -206,325 +276,310 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
++<<<<<<< HEAD
++=======
+ 		return NULL;
+ 	return no_page_table(vma, flags);
+ }
+ 
+ static struct page *follow_pmd_mask(struct vm_area_struct *vma,
+ 				    unsigned long address, pud_t *pudp,
+ 				    unsigned int flags,
+ 				    struct follow_page_context *ctx)
+ {
+ 	pmd_t *pmd, pmdval;
+ 	spinlock_t *ptl;
+ 	struct page *page;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 
+ 	pmd = pmd_offset(pudp, address);
+ 	/*
+ 	 * The READ_ONCE() will stabilize the pmdval in a register or
+ 	 * on the stack so that it will stop changing under the code.
+ 	 */
+ 	pmdval = READ_ONCE(*pmd);
+ 	if (pmd_none(pmdval))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(pmdval) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (is_hugepd(__hugepd(pmd_val(pmdval)))) {
+ 		page = follow_huge_pd(vma, address,
+ 				      __hugepd(pmd_val(pmdval)), flags,
+ 				      PMD_SHIFT);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ retry:
+ 	if (!pmd_present(pmdval)) {
+ 		if (likely(!(flags & FOLL_MIGRATION)))
+ 			return no_page_table(vma, flags);
+ 		VM_BUG_ON(thp_migration_supported() &&
+ 				  !is_pmd_migration_entry(pmdval));
+ 		if (is_pmd_migration_entry(pmdval))
+ 			pmd_migration_entry_wait(mm, pmd);
+ 		pmdval = READ_ONCE(*pmd);
+ 		/*
+ 		 * MADV_DONTNEED may convert the pmd to null because
+ 		 * mmap_sem is held in read mode
+ 		 */
+ 		if (pmd_none(pmdval))
+ 			return no_page_table(vma, flags);
+ 		goto retry;
+ 	}
+ 	if (pmd_devmap(pmdval)) {
+ 		ptl = pmd_lock(mm, pmd);
+ 		page = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);
+ 		spin_unlock(ptl);
+ 		if (page)
+ 			return page;
+ 	}
+ 	if (likely(!pmd_trans_huge(pmdval)))
+ 		return follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);
+ 
+ 	if ((flags & FOLL_NUMA) && pmd_protnone(pmdval))
+ 		return no_page_table(vma, flags);
+ 
+ retry_locked:
+ 	ptl = pmd_lock(mm, pmd);
+ 	if (unlikely(pmd_none(*pmd))) {
+ 		spin_unlock(ptl);
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (unlikely(!pmd_present(*pmd))) {
+ 		spin_unlock(ptl);
+ 		if (likely(!(flags & FOLL_MIGRATION)))
+ 			return no_page_table(vma, flags);
+ 		pmd_migration_entry_wait(mm, pmd);
+ 		goto retry_locked;
+ 	}
+ 	if (unlikely(!pmd_trans_huge(*pmd))) {
+ 		spin_unlock(ptl);
+ 		return follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);
+ 	}
+ 	if (flags & FOLL_SPLIT) {
+ 		int ret;
+ 		page = pmd_page(*pmd);
+ 		if (is_huge_zero_page(page)) {
+ 			spin_unlock(ptl);
+ 			ret = 0;
+ 			split_huge_pmd(vma, pmd, address);
+ 			if (pmd_trans_unstable(pmd))
+ 				ret = -EBUSY;
+ 		} else {
+ 			if (unlikely(!try_get_page(page))) {
+ 				spin_unlock(ptl);
+ 				return ERR_PTR(-ENOMEM);
+ 			}
+ 			spin_unlock(ptl);
+ 			lock_page(page);
+ 			ret = split_huge_page(page);
+ 			unlock_page(page);
+ 			put_page(page);
+ 			if (pmd_none(*pmd))
+ 				return no_page_table(vma, flags);
+ 		}
+ 
+ 		return ret ? ERR_PTR(ret) :
+ 			follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);
+ 	}
+ 	page = follow_trans_huge_pmd(vma, address, pmd, flags);
+ 	spin_unlock(ptl);
+ 	ctx->page_mask = HPAGE_PMD_NR - 1;
+ 	return page;
+ }
+ 
+ static struct page *follow_pud_mask(struct vm_area_struct *vma,
+ 				    unsigned long address, p4d_t *p4dp,
+ 				    unsigned int flags,
+ 				    struct follow_page_context *ctx)
+ {
+ 	pud_t *pud;
+ 	spinlock_t *ptl;
+ 	struct page *page;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 
+ 	pud = pud_offset(p4dp, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pud(mm, address, pud, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (is_hugepd(__hugepd(pud_val(*pud)))) {
+ 		page = follow_huge_pd(vma, address,
+ 				      __hugepd(pud_val(*pud)), flags,
+ 				      PUD_SHIFT);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (pud_devmap(*pud)) {
+ 		ptl = pud_lock(mm, pud);
+ 		page = follow_devmap_pud(vma, address, pud, flags, &ctx->pgmap);
+ 		spin_unlock(ptl);
+ 		if (page)
+ 			return page;
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	return follow_pmd_mask(vma, address, pud, flags, ctx);
+ }
+ 
+ static struct page *follow_p4d_mask(struct vm_area_struct *vma,
+ 				    unsigned long address, pgd_t *pgdp,
+ 				    unsigned int flags,
+ 				    struct follow_page_context *ctx)
+ {
+ 	p4d_t *p4d;
+ 	struct page *page;
+ 
+ 	p4d = p4d_offset(pgdp, address);
+ 	if (p4d_none(*p4d))
+ 		return no_page_table(vma, flags);
+ 	BUILD_BUG_ON(p4d_huge(*p4d));
+ 	if (unlikely(p4d_bad(*p4d)))
+ 		return no_page_table(vma, flags);
+ 
+ 	if (is_hugepd(__hugepd(p4d_val(*p4d)))) {
+ 		page = follow_huge_pd(vma, address,
+ 				      __hugepd(p4d_val(*p4d)), flags,
+ 				      P4D_SHIFT);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	return follow_pud_mask(vma, address, p4d, flags, ctx);
+ }
+ 
+ /**
+  * follow_page_mask - look up a page descriptor from a user-virtual address
+  * @vma: vm_area_struct mapping @address
+  * @address: virtual address to look up
+  * @flags: flags modifying lookup behaviour
+  * @ctx: contains dev_pagemap for %ZONE_DEVICE memory pinning and a
+  *       pointer to output page_mask
+  *
+  * @flags can have FOLL_ flags set, defined in <linux/mm.h>
+  *
+  * When getting pages from ZONE_DEVICE memory, the @ctx->pgmap caches
+  * the device's dev_pagemap metadata to avoid repeating expensive lookups.
+  *
+  * On output, the @ctx->page_mask is set according to the size of the page.
+  *
+  * Return: the mapped (struct page *), %NULL if no mapping exists, or
+  * an error pointer if there is a mapping to something not represented
+  * by a page descriptor (see also vm_normal_page()).
+  */
+ struct page *follow_page_mask(struct vm_area_struct *vma,
+ 			      unsigned long address, unsigned int flags,
+ 			      struct follow_page_context *ctx)
+ {
+ 	pgd_t *pgd;
+ 	struct page *page;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 
+ 	ctx->page_mask = 0;
+ 
+ 	/* make this handle hugepd */
+ 	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
+ 	if (!IS_ERR(page)) {
+ 		BUG_ON(flags & FOLL_GET);
++>>>>>>> 8fde12ca79af (mm: prevent get_user_pages() from overflowing page refcount)
  		return page;
 -	}
  
 -	pgd = pgd_offset(mm, address);
 -
 -	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 -		return no_page_table(vma, flags);
 -
 -	if (pgd_huge(*pgd)) {
 -		page = follow_huge_pgd(mm, address, pgd, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (is_hugepd(__hugepd(pgd_val(*pgd)))) {
 -		page = follow_huge_pd(vma, address,
 -				      __hugepd(pgd_val(*pgd)), flags,
 -				      PGDIR_SHIFT);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -
 -	return follow_p4d_mask(vma, address, pgd, flags, ctx);
 -}
 -
 -struct page *follow_page(struct vm_area_struct *vma, unsigned long address,
 -			 unsigned int foll_flags)
 -{
 -	struct follow_page_context ctx = { NULL };
 -	struct page *page;
 -
 -	page = follow_page_mask(vma, address, foll_flags, &ctx);
 -	if (ctx.pgmap)
 -		put_dev_pagemap(ctx.pgmap);
 +no_page_table:
 +	/*
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
 +	 */
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
  	return page;
  }
  
++<<<<<<< HEAD
++=======
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	p4d_t *p4d;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	p4d = p4d_offset(pgd, address);
+ 	BUG_ON(p4d_none(*p4d));
+ 	pud = pud_offset(p4d, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (!pmd_present(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 
+ 		/*
+ 		 * This should never happen (a device public page in the gate
+ 		 * area).
+ 		 */
+ 		if (is_device_public_page(*page))
+ 			goto unmap;
+ 	}
+ 	if (unlikely(!try_get_page(*page))) {
+ 		ret = -ENOMEM;
+ 		goto unmap;
+ 	}
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
+ }
+ 
+ /*
+  * mmap_sem must be held on entry.  If @nonblocking != NULL and
+  * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
+  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
+  */
++>>>>>>> 8fde12ca79af (mm: prevent get_user_pages() from overflowing page refcount)
  static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 -		unsigned long address, unsigned int *flags, int *nonblocking)
 +			unsigned long address, unsigned int *flags, int *nonblocking)
  {
  	unsigned int fault_flags = 0;
 -	vm_fault_t ret;
 +	int ret;
  
  	/* mlock all present pages, but do not fault in new pages */
  	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
@@@ -1143,3 -1346,580 +1432,583 @@@ struct page *get_dump_page(unsigned lon
  	return page;
  }
  #endif /* CONFIG_ELF_CORE */
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Generic Fast GUP
+  *
+  * get_user_pages_fast attempts to pin user pages by walking the page
+  * tables directly and avoids taking locks. Thus the walker needs to be
+  * protected from page table pages being freed from under it, and should
+  * block any THP splits.
+  *
+  * One way to achieve this is to have the walker disable interrupts, and
+  * rely on IPIs from the TLB flushing code blocking before the page table
+  * pages are freed. This is unsuitable for architectures that do not need
+  * to broadcast an IPI when invalidating TLBs.
+  *
+  * Another way to achieve this is to batch up page table containing pages
+  * belonging to more than one mm_user, then rcu_sched a callback to free those
+  * pages. Disabling interrupts will allow the fast_gup walker to both block
+  * the rcu_sched callback, and an IPI that we broadcast for splitting THPs
+  * (which is a relatively rare event). The code below adopts this strategy.
+  *
+  * Before activating this code, please be aware that the following assumptions
+  * are currently made:
+  *
+  *  *) Either HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to
+  *  free pages containing page tables or TLB flushing requires IPI broadcast.
+  *
+  *  *) ptes can be read atomically by the architecture.
+  *
+  *  *) access_ok is sufficient to validate userspace address ranges.
+  *
+  * The last two assumptions can be relaxed by the addition of helper functions.
+  *
+  * This code is based heavily on the PowerPC implementation by Nick Piggin.
+  */
+ #ifdef CONFIG_HAVE_GENERIC_GUP
+ 
+ #ifndef gup_get_pte
+ /*
+  * We assume that the PTE can be read atomically. If this is not the case for
+  * your architecture, please provide the helper.
+  */
+ static inline pte_t gup_get_pte(pte_t *ptep)
+ {
+ 	return READ_ONCE(*ptep);
+ }
+ #endif
+ 
+ static void undo_dev_pagemap(int *nr, int nr_start, struct page **pages)
+ {
+ 	while ((*nr) - nr_start) {
+ 		struct page *page = pages[--(*nr)];
+ 
+ 		ClearPageReferenced(page);
+ 		put_page(page);
+ 	}
+ }
+ 
+ /*
+  * Return the compund head page with ref appropriately incremented,
+  * or NULL if that failed.
+  */
+ static inline struct page *try_get_compound_head(struct page *page, int refs)
+ {
+ 	struct page *head = compound_head(page);
+ 	if (WARN_ON_ONCE(page_ref_count(head) < 0))
+ 		return NULL;
+ 	if (unlikely(!page_cache_add_speculative(head, refs)))
+ 		return NULL;
+ 	return head;
+ }
+ 
+ #ifdef CONFIG_ARCH_HAS_PTE_SPECIAL
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	struct dev_pagemap *pgmap = NULL;
+ 	int nr_start = *nr, ret = 0;
+ 	pte_t *ptep, *ptem;
+ 
+ 	ptem = ptep = pte_offset_map(&pmd, addr);
+ 	do {
+ 		pte_t pte = gup_get_pte(ptep);
+ 		struct page *head, *page;
+ 
+ 		/*
+ 		 * Similar to the PMD case below, NUMA hinting must take slow
+ 		 * path using the pte_protnone check.
+ 		 */
+ 		if (pte_protnone(pte))
+ 			goto pte_unmap;
+ 
+ 		if (!pte_access_permitted(pte, write))
+ 			goto pte_unmap;
+ 
+ 		if (pte_devmap(pte)) {
+ 			pgmap = get_dev_pagemap(pte_pfn(pte), pgmap);
+ 			if (unlikely(!pgmap)) {
+ 				undo_dev_pagemap(nr, nr_start, pages);
+ 				goto pte_unmap;
+ 			}
+ 		} else if (pte_special(pte))
+ 			goto pte_unmap;
+ 
+ 		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
+ 		page = pte_page(pte);
+ 
+ 		head = try_get_compound_head(page, 1);
+ 		if (!head)
+ 			goto pte_unmap;
+ 
+ 		if (unlikely(pte_val(pte) != pte_val(*ptep))) {
+ 			put_page(head);
+ 			goto pte_unmap;
+ 		}
+ 
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 
+ 		SetPageReferenced(page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 
+ 	} while (ptep++, addr += PAGE_SIZE, addr != end);
+ 
+ 	ret = 1;
+ 
+ pte_unmap:
+ 	if (pgmap)
+ 		put_dev_pagemap(pgmap);
+ 	pte_unmap(ptem);
+ 	return ret;
+ }
+ #else
+ 
+ /*
+  * If we can't determine whether or not a pte is special, then fail immediately
+  * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not
+  * to be special.
+  *
+  * For a futex to be placed on a THP tail page, get_futex_key requires a
+  * __get_user_pages_fast implementation that can pin pages. Thus it's still
+  * useful to have gup_huge_pmd even if we can't operate on ptes.
+  */
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_ARCH_HAS_PTE_SPECIAL */
+ 
+ #if defined(__HAVE_ARCH_PTE_DEVMAP) && defined(CONFIG_TRANSPARENT_HUGEPAGE)
+ static int __gup_device_huge(unsigned long pfn, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	int nr_start = *nr;
+ 	struct dev_pagemap *pgmap = NULL;
+ 
+ 	do {
+ 		struct page *page = pfn_to_page(pfn);
+ 
+ 		pgmap = get_dev_pagemap(pfn, pgmap);
+ 		if (unlikely(!pgmap)) {
+ 			undo_dev_pagemap(nr, nr_start, pages);
+ 			return 0;
+ 		}
+ 		SetPageReferenced(page);
+ 		pages[*nr] = page;
+ 		get_page(page);
+ 		(*nr)++;
+ 		pfn++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	if (pgmap)
+ 		put_dev_pagemap(pgmap);
+ 	return 1;
+ }
+ 
+ static int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	unsigned long fault_pfn;
+ 	int nr_start = *nr;
+ 
+ 	fault_pfn = pmd_pfn(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 	if (!__gup_device_huge(fault_pfn, addr, end, pages, nr))
+ 		return 0;
+ 
+ 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+ 		undo_dev_pagemap(nr, nr_start, pages);
+ 		return 0;
+ 	}
+ 	return 1;
+ }
+ 
+ static int __gup_device_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	unsigned long fault_pfn;
+ 	int nr_start = *nr;
+ 
+ 	fault_pfn = pud_pfn(orig) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 	if (!__gup_device_huge(fault_pfn, addr, end, pages, nr))
+ 		return 0;
+ 
+ 	if (unlikely(pud_val(orig) != pud_val(*pudp))) {
+ 		undo_dev_pagemap(nr, nr_start, pages);
+ 		return 0;
+ 	}
+ 	return 1;
+ }
+ #else
+ static int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ 
+ static int __gup_device_huge_pud(pud_t pud, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ #endif
+ 
+ static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (!pmd_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	if (pmd_devmap(orig))
+ 		return __gup_device_huge_pmd(orig, pmdp, addr, end, pages, nr);
+ 
+ 	refs = 0;
+ 	page = pmd_page(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = try_get_compound_head(pmd_page(orig), refs);
+ 	if (!head) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (!pud_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	if (pud_devmap(orig))
+ 		return __gup_device_huge_pud(orig, pudp, addr, end, pages, nr);
+ 
+ 	refs = 0;
+ 	page = pud_page(orig) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = try_get_compound_head(pud_page(orig), refs);
+ 	if (!head) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pud_val(orig) != pud_val(*pudp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,
+ 			unsigned long end, int write,
+ 			struct page **pages, int *nr)
+ {
+ 	int refs;
+ 	struct page *head, *page;
+ 
+ 	if (!pgd_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	BUILD_BUG_ON(pgd_devmap(orig));
+ 	refs = 0;
+ 	page = pgd_page(orig) + ((addr & ~PGDIR_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = try_get_compound_head(pgd_page(orig), refs);
+ 	if (!head) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
+ 		int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pmd_t *pmdp;
+ 
+ 	pmdp = pmd_offset(&pud, addr);
+ 	do {
+ 		pmd_t pmd = READ_ONCE(*pmdp);
+ 
+ 		next = pmd_addr_end(addr, end);
+ 		if (!pmd_present(pmd))
+ 			return 0;
+ 
+ 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd) ||
+ 			     pmd_devmap(pmd))) {
+ 			/*
+ 			 * NUMA hinting faults need to be handled in the GUP
+ 			 * slowpath for accounting purposes and so that they
+ 			 * can be serialised against THP migration.
+ 			 */
+ 			if (pmd_protnone(pmd))
+ 				return 0;
+ 
+ 			if (!gup_huge_pmd(pmd, pmdp, addr, next, write,
+ 				pages, nr))
+ 				return 0;
+ 
+ 		} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {
+ 			/*
+ 			 * architecture have different format for hugetlbfs
+ 			 * pmd format and THP pmd format
+ 			 */
+ 			if (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,
+ 					 PMD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (pmdp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pud_t *pudp;
+ 
+ 	pudp = pud_offset(&p4d, addr);
+ 	do {
+ 		pud_t pud = READ_ONCE(*pudp);
+ 
+ 		next = pud_addr_end(addr, end);
+ 		if (pud_none(pud))
+ 			return 0;
+ 		if (unlikely(pud_huge(pud))) {
+ 			if (!gup_huge_pud(pud, pudp, addr, next, write,
+ 					  pages, nr))
+ 				return 0;
+ 		} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {
+ 			if (!gup_huge_pd(__hugepd(pud_val(pud)), addr,
+ 					 PUD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (pudp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	p4d_t *p4dp;
+ 
+ 	p4dp = p4d_offset(&pgd, addr);
+ 	do {
+ 		p4d_t p4d = READ_ONCE(*p4dp);
+ 
+ 		next = p4d_addr_end(addr, end);
+ 		if (p4d_none(p4d))
+ 			return 0;
+ 		BUILD_BUG_ON(p4d_huge(p4d));
+ 		if (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {
+ 			if (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,
+ 					 P4D_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pud_range(p4d, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (p4dp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static void gup_pgd_range(unsigned long addr, unsigned long end,
+ 		int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pgd_t *pgdp;
+ 
+ 	pgdp = pgd_offset(current->mm, addr);
+ 	do {
+ 		pgd_t pgd = READ_ONCE(*pgdp);
+ 
+ 		next = pgd_addr_end(addr, end);
+ 		if (pgd_none(pgd))
+ 			return;
+ 		if (unlikely(pgd_huge(pgd))) {
+ 			if (!gup_huge_pgd(pgd, pgdp, addr, next, write,
+ 					  pages, nr))
+ 				return;
+ 		} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {
+ 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
+ 					 PGDIR_SHIFT, next, write, pages, nr))
+ 				return;
+ 		} else if (!gup_p4d_range(pgd, addr, next, write, pages, nr))
+ 			return;
+ 	} while (pgdp++, addr = next, addr != end);
+ }
+ 
+ #ifndef gup_fast_permitted
+ /*
+  * Check if it's allowed to use __get_user_pages_fast() for the range, or
+  * we need to fall back to the slow version:
+  */
+ bool gup_fast_permitted(unsigned long start, int nr_pages, int write)
+ {
+ 	unsigned long len, end;
+ 
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 	return end >= start;
+ }
+ #endif
+ 
+ /*
+  * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to
+  * the regular GUP.
+  * Note a difference with get_user_pages_fast: this always returns the
+  * number of pages pinned, 0 if no pages were pinned.
+  */
+ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			  struct page **pages)
+ {
+ 	unsigned long len, end;
+ 	unsigned long flags;
+ 	int nr = 0;
+ 
+ 	start &= PAGE_MASK;
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 
+ 	if (unlikely(!access_ok((void __user *)start, len)))
+ 		return 0;
+ 
+ 	/*
+ 	 * Disable interrupts.  We use the nested form as we can already have
+ 	 * interrupts disabled by get_futex_key.
+ 	 *
+ 	 * With interrupts disabled, we block page table pages from being
+ 	 * freed from under us. See struct mmu_table_batch comments in
+ 	 * include/asm-generic/tlb.h for more details.
+ 	 *
+ 	 * We do not adopt an rcu_read_lock(.) here as we also want to
+ 	 * block IPIs that come from THPs splitting.
+ 	 */
+ 
+ 	if (gup_fast_permitted(start, nr_pages, write)) {
+ 		local_irq_save(flags);
+ 		gup_pgd_range(start, end, write, pages, &nr);
+ 		local_irq_restore(flags);
+ 	}
+ 
+ 	return nr;
+ }
+ 
+ /**
+  * get_user_pages_fast() - pin user pages in memory
+  * @start:	starting user address
+  * @nr_pages:	number of pages from start to pin
+  * @write:	whether pages will be written to
+  * @pages:	array that receives pointers to the pages pinned.
+  *		Should be at least nr_pages long.
+  *
+  * Attempt to pin user pages in memory without taking mm->mmap_sem.
+  * If not successful, it will fall back to taking the lock and
+  * calling get_user_pages().
+  *
+  * Returns number of pages pinned. This may be fewer than the number
+  * requested. If nr_pages is 0 or negative, returns 0. If no pages
+  * were pinned, returns -errno.
+  */
+ int get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			struct page **pages)
+ {
+ 	unsigned long addr, len, end;
+ 	int nr = 0, ret = 0;
+ 
+ 	start &= PAGE_MASK;
+ 	addr = start;
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 
+ 	if (nr_pages <= 0)
+ 		return 0;
+ 
+ 	if (unlikely(!access_ok((void __user *)start, len)))
+ 		return -EFAULT;
+ 
+ 	if (gup_fast_permitted(start, nr_pages, write)) {
+ 		local_irq_disable();
+ 		gup_pgd_range(addr, end, write, pages, &nr);
+ 		local_irq_enable();
+ 		ret = nr;
+ 	}
+ 
+ 	if (nr < nr_pages) {
+ 		/* Try to get the remaining pages with get_user_pages */
+ 		start += nr << PAGE_SHIFT;
+ 		pages += nr;
+ 
+ 		ret = get_user_pages_unlocked(start, nr_pages - nr, pages,
+ 				write ? FOLL_WRITE : 0);
+ 
+ 		/* Have to be a bit careful with return values */
+ 		if (nr > 0) {
+ 			if (ret < 0)
+ 				ret = nr;
+ 			else
+ 				ret += nr;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ #endif /* CONFIG_HAVE_GENERIC_GUP */
++>>>>>>> 8fde12ca79af (mm: prevent get_user_pages() from overflowing page refcount)
* Unmerged path mm/gup.c
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 601926def2e1..767b2026c051 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4340,6 +4340,19 @@ long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 
 		pfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;
 		page = pte_page(huge_ptep_get(pte));
+
+		/*
+		 * Instead of doing 'try_get_page()' below in the same_page
+		 * loop, just check the count once here.
+		 */
+		if (unlikely(page_count(page) <= 0)) {
+			if (pages) {
+				spin_unlock(ptl);
+				remainder = 0;
+				err = -ENOMEM;
+				break;
+			}
+		}
 same_page:
 		if (pages) {
 			pages[i] = mem_map_offset(page, pfn_offset);
