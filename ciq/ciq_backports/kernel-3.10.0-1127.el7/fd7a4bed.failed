sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit fd7a4bed183523275279c9addbf42fce550c2e90
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/fd7a4bed.failed

Remove the direct {push,pull} balancing operations from
switched_{from,to}_rt() / prio_changed_rt() and use the balance
callback queue.

Again, err on the side of too many reschedules; since too few is a
hard bug while too many is just annoying.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: ktkhai@parallels.com
	Cc: rostedt@goodmis.org
	Cc: juri.lelli@gmail.com
	Cc: pang.xunlei@linaro.org
	Cc: oleg@redhat.com
	Cc: wanpeng.li@linux.intel.com
	Cc: umgwanakikbuti@gmail.com
Link: http://lkml.kernel.org/r/20150611124742.766832367@infradead.org
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit fd7a4bed183523275279c9addbf42fce550c2e90)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/rt.c
diff --cc kernel/sched/rt.c
index 6b68ceb9a68d,460f85888b74..000000000000
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@@ -313,6 -354,25 +313,28 @@@ static inline int has_pushable_tasks(st
  	return !plist_head_empty(&rq->rt.pushable_tasks);
  }
  
++<<<<<<< HEAD
++=======
+ static DEFINE_PER_CPU(struct callback_head, rt_push_head);
+ static DEFINE_PER_CPU(struct callback_head, rt_pull_head);
+ 
+ static void push_rt_tasks(struct rq *);
+ static void pull_rt_task(struct rq *);
+ 
+ static inline void queue_push_tasks(struct rq *rq)
+ {
+ 	if (!has_pushable_tasks(rq))
+ 		return;
+ 
+ 	queue_balance_callback(rq, &per_cpu(rt_push_head, rq->cpu), push_rt_tasks);
+ }
+ 
+ static inline void queue_pull_task(struct rq *rq)
+ {
+ 	queue_balance_callback(rq, &per_cpu(rt_pull_head, rq->cpu), pull_rt_task);
+ }
+ 
++>>>>>>> fd7a4bed1835 (sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks)
  static void enqueue_pushable_task(struct rq *rq, struct task_struct *p)
  {
  	plist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);
@@@ -1883,11 -2146,10 +1905,15 @@@ static void switched_from_rt(struct rq 
  	if (!task_on_rq_queued(p) || rq->rt.rt_nr_running)
  		return;
  
++<<<<<<< HEAD
 +	if (pull_rt_task(rq))
 +		resched_curr(rq);
++=======
+ 	queue_pull_task(rq);
++>>>>>>> fd7a4bed1835 (sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks)
  }
  
 -void __init init_sched_rt_class(void)
 +void init_sched_rt_class(void)
  {
  	unsigned int i;
  
@@@ -1916,13 -2176,12 +1940,21 @@@ static void switched_to_rt(struct rq *r
  	 */
  	if (task_on_rq_queued(p) && rq->curr != p) {
  #ifdef CONFIG_SMP
++<<<<<<< HEAD
 +		if (rq->rt.overloaded && push_rt_task(rq) &&
 +		    /* Don't resched if we changed runqueues */
 +		    rq != task_rq(p))
 +			check_resched = 0;
 +#endif /* CONFIG_SMP */
 +		if (check_resched && p->prio < rq->curr->prio)
++=======
+ 		if (p->nr_cpus_allowed > 1 && rq->rt.overloaded)
+ 			queue_push_tasks(rq);
+ #else
+ 		if (p->prio < rq->curr->prio)
++>>>>>>> fd7a4bed1835 (sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks)
  			resched_curr(rq);
+ #endif /* CONFIG_SMP */
  	}
  }
  
* Unmerged path kernel/sched/rt.c
