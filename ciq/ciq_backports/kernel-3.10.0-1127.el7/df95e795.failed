percpu: add tracepoint support for percpu memory

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou <dennisz@fb.com>
commit df95e795a722892a9e0603ce4b9b62fab9f02967
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/df95e795.failed

Add support for tracepoints to the following events: chunk allocation,
chunk free, area allocation, area free, and area allocation failure.
This should let us replay percpu memory requests and evaluate
corresponding decisions.

	Signed-off-by: Dennis Zhou <dennisz@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit df95e795a722892a9e0603ce4b9b62fab9f02967)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu-km.c
#	mm/percpu-vm.c
#	mm/percpu.c
diff --cc mm/percpu-km.c
index 10e3d0b8a86d,2b79e43c626f..000000000000
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@@ -72,6 -72,9 +72,12 @@@ static struct pcpu_chunk *pcpu_create_c
  	pcpu_chunk_populated(chunk, 0, nr_pages);
  	spin_unlock_irq(&pcpu_lock);
  
++<<<<<<< HEAD
++=======
+ 	pcpu_stats_chunk_alloc();
+ 	trace_percpu_create_chunk(chunk->base_addr);
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  	return chunk;
  }
  
@@@ -79,6 -82,9 +85,12 @@@ static void pcpu_destroy_chunk(struct p
  {
  	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
  
++<<<<<<< HEAD
++=======
+ 	pcpu_stats_chunk_dealloc();
+ 	trace_percpu_destroy_chunk(chunk->base_addr);
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  	if (chunk && chunk->data)
  		__free_pages(chunk->data, order_base_2(nr_pages));
  	pcpu_free_chunk(chunk);
diff --cc mm/percpu-vm.c
index 9ac639499bd1,7ad9d94bf547..000000000000
--- a/mm/percpu-vm.c
+++ b/mm/percpu-vm.c
@@@ -343,11 -343,18 +343,24 @@@ static struct pcpu_chunk *pcpu_create_c
  
  	chunk->data = vms;
  	chunk->base_addr = vms[0]->addr - pcpu_group_offsets[0];
++<<<<<<< HEAD
++=======
+ 
+ 	pcpu_stats_chunk_alloc();
+ 	trace_percpu_create_chunk(chunk->base_addr);
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  	return chunk;
  }
  
  static void pcpu_destroy_chunk(struct pcpu_chunk *chunk)
  {
++<<<<<<< HEAD
++=======
+ 	pcpu_stats_chunk_dealloc();
+ 	trace_percpu_destroy_chunk(chunk->base_addr);
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  	if (chunk && chunk->data)
  		pcpu_free_vm_areas(chunk->data, pcpu_nr_groups);
  	pcpu_free_chunk(chunk);
diff --cc mm/percpu.c
index c3b003f3ebc0,a5bc3634d2a9..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -74,6 -76,11 +74,14 @@@
  #include <asm/tlbflush.h>
  #include <asm/io.h>
  
++<<<<<<< HEAD
++=======
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/percpu.h>
+ 
+ #include "percpu-internal.h"
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  #define PCPU_SLOT_BASE_SHIFT		5	/* 1-31 shares the same slot */
  #define PCPU_DFL_MAP_ALLOC		16	/* start a map with 16 ents */
  #define PCPU_ATOMIC_MAP_MARGIN_LOW	32
@@@ -1037,12 -1027,14 +1049,20 @@@ area_found
  fail_unlock:
  	spin_unlock_irqrestore(&pcpu_lock, flags);
  fail:
++<<<<<<< HEAD
 +	if (!is_atomic && do_warn && warn_limit) {
 +		pr_warning("PERCPU: allocation failed, size=%zu align=%zu atomic=%d, %s\n",
 +			   size, align, is_atomic, err);
++=======
+ 	trace_percpu_alloc_percpu_fail(reserved, is_atomic, size, align);
+ 
+ 	if (!is_atomic && warn_limit) {
+ 		pr_warn("allocation failed, size=%zu align=%zu atomic=%d, %s\n",
+ 			size, align, is_atomic, err);
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  		dump_stack();
  		if (!--warn_limit)
 -			pr_info("limit reached, disable warning\n");
 +			pr_info("PERCPU: limit reached, disable warning\n");
  	}
  	if (is_atomic) {
  		/* see the flag handling in pcpu_blance_workfn() */
@@@ -1721,6 -1729,9 +1743,12 @@@ int __init pcpu_setup_first_chunk(cons
  		pcpu_count_occupied_pages(pcpu_first_chunk, 1);
  	pcpu_chunk_relocate(pcpu_first_chunk, -1);
  
++<<<<<<< HEAD
++=======
+ 	pcpu_stats_chunk_alloc();
+ 	trace_percpu_create_chunk(base_addr);
+ 
++>>>>>>> df95e795a722 (percpu: add tracepoint support for percpu memory)
  	/* we're done */
  	pcpu_base_addr = base_addr;
  	return 0;
diff --git a/include/trace/events/percpu.h b/include/trace/events/percpu.h
new file mode 100644
index 000000000000..ad34b1bae047
--- /dev/null
+++ b/include/trace/events/percpu.h
@@ -0,0 +1,125 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM percpu
+
+#if !defined(_TRACE_PERCPU_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_PERCPU_H
+
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(percpu_alloc_percpu,
+
+	TP_PROTO(bool reserved, bool is_atomic, size_t size,
+		 size_t align, void *base_addr, int off, void __percpu *ptr),
+
+	TP_ARGS(reserved, is_atomic, size, align, base_addr, off, ptr),
+
+	TP_STRUCT__entry(
+		__field(	bool,			reserved	)
+		__field(	bool,			is_atomic	)
+		__field(	size_t,			size		)
+		__field(	size_t,			align		)
+		__field(	void *,			base_addr	)
+		__field(	int,			off		)
+		__field(	void __percpu *,	ptr		)
+	),
+
+	TP_fast_assign(
+		__entry->reserved	= reserved;
+		__entry->is_atomic	= is_atomic;
+		__entry->size		= size;
+		__entry->align		= align;
+		__entry->base_addr	= base_addr;
+		__entry->off		= off;
+		__entry->ptr		= ptr;
+	),
+
+	TP_printk("reserved=%d is_atomic=%d size=%zu align=%zu base_addr=%p off=%d ptr=%p",
+		  __entry->reserved, __entry->is_atomic,
+		  __entry->size, __entry->align,
+		  __entry->base_addr, __entry->off, __entry->ptr)
+);
+
+TRACE_EVENT(percpu_free_percpu,
+
+	TP_PROTO(void *base_addr, int off, void __percpu *ptr),
+
+	TP_ARGS(base_addr, off, ptr),
+
+	TP_STRUCT__entry(
+		__field(	void *,			base_addr	)
+		__field(	int,			off		)
+		__field(	void __percpu *,	ptr		)
+	),
+
+	TP_fast_assign(
+		__entry->base_addr	= base_addr;
+		__entry->off		= off;
+		__entry->ptr		= ptr;
+	),
+
+	TP_printk("base_addr=%p off=%d ptr=%p",
+		__entry->base_addr, __entry->off, __entry->ptr)
+);
+
+TRACE_EVENT(percpu_alloc_percpu_fail,
+
+	TP_PROTO(bool reserved, bool is_atomic, size_t size, size_t align),
+
+	TP_ARGS(reserved, is_atomic, size, align),
+
+	TP_STRUCT__entry(
+		__field(	bool,	reserved	)
+		__field(	bool,	is_atomic	)
+		__field(	size_t,	size		)
+		__field(	size_t, align		)
+	),
+
+	TP_fast_assign(
+		__entry->reserved	= reserved;
+		__entry->is_atomic	= is_atomic;
+		__entry->size		= size;
+		__entry->align		= align;
+	),
+
+	TP_printk("reserved=%d is_atomic=%d size=%zu align=%zu",
+		  __entry->reserved, __entry->is_atomic,
+		  __entry->size, __entry->align)
+);
+
+TRACE_EVENT(percpu_create_chunk,
+
+	TP_PROTO(void *base_addr),
+
+	TP_ARGS(base_addr),
+
+	TP_STRUCT__entry(
+		__field(	void *, base_addr	)
+	),
+
+	TP_fast_assign(
+		__entry->base_addr	= base_addr;
+	),
+
+	TP_printk("base_addr=%p", __entry->base_addr)
+);
+
+TRACE_EVENT(percpu_destroy_chunk,
+
+	TP_PROTO(void *base_addr),
+
+	TP_ARGS(base_addr),
+
+	TP_STRUCT__entry(
+		__field(	void *,	base_addr	)
+	),
+
+	TP_fast_assign(
+		__entry->base_addr	= base_addr;
+	),
+
+	TP_printk("base_addr=%p", __entry->base_addr)
+);
+
+#endif /* _TRACE_PERCPU_H */
+
+#include <trace/define_trace.h>
* Unmerged path mm/percpu-km.c
* Unmerged path mm/percpu-vm.c
* Unmerged path mm/percpu.c
