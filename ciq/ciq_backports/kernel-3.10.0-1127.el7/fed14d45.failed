sched/fair: Track cgroup depth

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit fed14d45f945042a15b09de48d7d3d58d9455fc4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/fed14d45.failed

Track depth in cgroup tree, this is useful for things like
find_matching_se() where you need to get to a common parent of two
sched entities.

Keeping the depth avoids having to calculate it on the spot, which
saves a number of possible cache-misses.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1328936700.2476.17.camel@laptop
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit fed14d45f945042a15b09de48d7d3d58d9455fc4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index f3df0d8beb03,748a7ac3388f..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -7648,9 -7249,11 +7637,11 @@@ void init_cfs_rq(struct cfs_rq *cfs_rq
  }
  
  #ifdef CONFIG_FAIR_GROUP_SCHED
 -static void task_move_group_fair(struct task_struct *p, int on_rq)
 +static void task_move_group_fair(struct task_struct *p, int queued)
  {
+ 	struct sched_entity *se = &p->se;
  	struct cfs_rq *cfs_rq;
+ 
  	/*
  	 * If the task was not on the rq at the time of this cgroup movement
  	 * it must have been asleep, sleeping tasks keep their ->vruntime
@@@ -7676,15 -7279,16 +7667,28 @@@
  	 * To prevent boost or penalty in the new cfs_rq caused by delta
  	 * min_vruntime between the two cfs_rqs, we skip vruntime adjustment.
  	 */
++<<<<<<< HEAD
 +	if (!queued && (!p->se.sum_exec_runtime || p->state == TASK_WAKING))
 +		queued = 1;
 +
 +	if (!queued)
 +		p->se.vruntime -= cfs_rq_of(&p->se)->min_vruntime;
 +	set_task_rq(p, task_cpu(p));
 +	if (!queued) {
 +		cfs_rq = cfs_rq_of(&p->se);
 +		p->se.vruntime += cfs_rq->min_vruntime;
++=======
+ 	if (!on_rq && (!se->sum_exec_runtime || p->state == TASK_WAKING))
+ 		on_rq = 1;
+ 
+ 	if (!on_rq)
+ 		se->vruntime -= cfs_rq_of(se)->min_vruntime;
+ 	set_task_rq(p, task_cpu(p));
+ 	se->depth = se->parent ? se->parent->depth + 1 : 0;
+ 	if (!on_rq) {
+ 		cfs_rq = cfs_rq_of(se);
+ 		se->vruntime += cfs_rq->min_vruntime;
++>>>>>>> fed14d45f945 (sched/fair: Track cgroup depth)
  #ifdef CONFIG_SMP
  		/*
  		 * migrate_task_rq_fair() will have removed our previous
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5863a8250fdb..54b998b31e79 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1230,6 +1230,7 @@ struct sched_entity {
 	u64			nr_migrations;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	int			depth;
 	struct sched_entity	*parent;
 	/* rq on which this entity is (to be) queued: */
 	struct cfs_rq		*cfs_rq;
* Unmerged path kernel/sched/fair.c
