md/raid1, raid10: add blktrace records when IO is delayed

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author NeilBrown <neilb@suse.com>
commit 578b54ade8a5e04df6edc14cb68ad0f6f491a1a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/578b54ad.failed

Both raid1 and raid10 will sometimes delay handling an IO request,
such as when resync is happening or there are too many requests queued.

Add some blktrace messsages so we can see when that is happening when
looking for performance artefacts.

	Signed-off-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 578b54ade8a5e04df6edc14cb68ad0f6f491a1a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid1.c
#	drivers/md/raid10.c
diff --cc drivers/md/raid1.c
index b69cd658dbb5,d24adc50a31f..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -72,27 -67,13 +72,30 @@@
   */
  static int max_queued_requests = 1024;
  
 -static void allow_barrier(struct r1conf *conf, sector_t start_next_window,
 -			  sector_t bi_sector);
 -static void lower_barrier(struct r1conf *conf);
 +static void allow_barrier(struct r1conf *conf, sector_t sector_nr);
 +static void lower_barrier(struct r1conf *conf, sector_t sector_nr);
 +
 +/*
 + * 'strct resync_pages' stores actual pages used for doing the resync
 + *  IO, and it is per-bio, so make .bi_private points to it.
 + */
 +static inline struct resync_pages *get_resync_pages(struct bio *bio)
 +{
 +	return bio->bi_private;
 +}
 +
 +/*
 + * for resync bio, r1bio pointer can be retrieved from the per-bio
 + * 'struct resync_pages'.
 + */
 +static inline struct r1bio *get_resync_r1bio(struct bio *bio)
 +{
 +	return get_resync_pages(bio)->raid_bio;
 +}
  
+ #define raid1_log(md, fmt, args...)				\
+ 	do { if ((md)->queue) blk_add_trace_msg((md)->queue, "raid1 " fmt, ##args); } while (0)
+ 
  static void * r1bio_pool_alloc(gfp_t gfp_flags, void *data)
  {
  	struct pool_info *pi = data;
@@@ -968,108 -829,103 +971,151 @@@ static void lower_barrier(struct r1con
  	wake_up(&conf->wait_barrier);
  }
  
 -static bool need_to_wait_for_sync(struct r1conf *conf, struct bio *bio)
 +static void _wait_barrier(struct r1conf *conf, int idx)
  {
 -	bool wait = false;
 +	/*
 +	 * We need to increase conf->nr_pending[idx] very early here,
 +	 * then raise_barrier() can be blocked when it waits for
 +	 * conf->nr_pending[idx] to be 0. Then we can avoid holding
 +	 * conf->resync_lock when there is no barrier raised in same
 +	 * barrier unit bucket. Also if the array is frozen, I/O
 +	 * should be blocked until array is unfrozen.
 +	 */
 +	atomic_inc(&conf->nr_pending[idx]);
 +	/*
 +	 * In _wait_barrier() we firstly increase conf->nr_pending[idx], then
 +	 * check conf->barrier[idx]. In raise_barrier() we firstly increase
 +	 * conf->barrier[idx], then check conf->nr_pending[idx]. A memory
 +	 * barrier is necessary here to make sure conf->barrier[idx] won't be
 +	 * fetched before conf->nr_pending[idx] is increased. Otherwise there
 +	 * will be a race between _wait_barrier() and raise_barrier().
 +	 */
 +	smp_mb__after_atomic();
  
 -	if (conf->array_frozen || !bio)
 -		wait = true;
 -	else if (conf->barrier && bio_data_dir(bio) == WRITE) {
 -		if ((conf->mddev->curr_resync_completed
 -		     >= bio_end_sector(bio)) ||
 -		    (conf->start_next_window + NEXT_NORMALIO_DISTANCE
 -		     <= bio->bi_iter.bi_sector))
 -			wait = false;
 -		else
 -			wait = true;
 -	}
 +	/*
 +	 * Don't worry about checking two atomic_t variables at same time
 +	 * here. If during we check conf->barrier[idx], the array is
 +	 * frozen (conf->array_frozen is 1), and chonf->barrier[idx] is
 +	 * 0, it is safe to return and make the I/O continue. Because the
 +	 * array is frozen, all I/O returned here will eventually complete
 +	 * or be queued, no race will happen. See code comment in
 +	 * frozen_array().
 +	 */
 +	if (!READ_ONCE(conf->array_frozen) &&
 +	    !atomic_read(&conf->barrier[idx]))
 +		return;
  
 -	return wait;
 +	/*
 +	 * After holding conf->resync_lock, conf->nr_pending[idx]
 +	 * should be decreased before waiting for barrier to drop.
 +	 * Otherwise, we may encounter a race condition because
 +	 * raise_barrer() might be waiting for conf->nr_pending[idx]
 +	 * to be 0 at same time.
 +	 */
 +	spin_lock_irq(&conf->resync_lock);
 +	atomic_inc(&conf->nr_waiting[idx]);
 +	atomic_dec(&conf->nr_pending[idx]);
 +	/*
 +	 * In case freeze_array() is waiting for
 +	 * get_unqueued_pending() == extra
 +	 */
 +	wake_up(&conf->wait_barrier);
 +	/* Wait for the barrier in same barrier unit bucket to drop. */
 +	wait_event_lock_irq(conf->wait_barrier,
 +			    !conf->array_frozen &&
 +			     !atomic_read(&conf->barrier[idx]),
 +			    conf->resync_lock);
 +	atomic_inc(&conf->nr_pending[idx]);
 +	atomic_dec(&conf->nr_waiting[idx]);
 +	spin_unlock_irq(&conf->resync_lock);
  }
  
 -static sector_t wait_barrier(struct r1conf *conf, struct bio *bio)
 +static void wait_read_barrier(struct r1conf *conf, sector_t sector_nr)
  {
 -	sector_t sector = 0;
 +	int idx = sector_to_idx(sector_nr);
 +
 +	/*
 +	 * Very similar to _wait_barrier(). The difference is, for read
 +	 * I/O we don't need wait for sync I/O, but if the whole array
 +	 * is frozen, the read I/O still has to wait until the array is
 +	 * unfrozen. Since there is no ordering requirement with
 +	 * conf->barrier[idx] here, memory barrier is unnecessary as well.
 +	 */
 +	atomic_inc(&conf->nr_pending[idx]);
 +
 +	if (!READ_ONCE(conf->array_frozen))
 +		return;
  
  	spin_lock_irq(&conf->resync_lock);
++<<<<<<< HEAD
 +	atomic_inc(&conf->nr_waiting[idx]);
 +	atomic_dec(&conf->nr_pending[idx]);
 +	/*
 +	 * In case freeze_array() is waiting for
 +	 * get_unqueued_pending() == extra
 +	 */
 +	wake_up(&conf->wait_barrier);
 +	/* Wait for array to be unfrozen */
 +	wait_event_lock_irq(conf->wait_barrier,
 +			    !conf->array_frozen,
 +			    conf->resync_lock);
 +	atomic_inc(&conf->nr_pending[idx]);
 +	atomic_dec(&conf->nr_waiting[idx]);
++=======
+ 	if (need_to_wait_for_sync(conf, bio)) {
+ 		conf->nr_waiting++;
+ 		/* Wait for the barrier to drop.
+ 		 * However if there are already pending
+ 		 * requests (preventing the barrier from
+ 		 * rising completely), and the
+ 		 * per-process bio queue isn't empty,
+ 		 * then don't wait, as we need to empty
+ 		 * that queue to allow conf->start_next_window
+ 		 * to increase.
+ 		 */
+ 		raid1_log(conf->mddev, "wait barrier");
+ 		wait_event_lock_irq(conf->wait_barrier,
+ 				    !conf->array_frozen &&
+ 				    (!conf->barrier ||
+ 				     ((conf->start_next_window <
+ 				       conf->next_resync + RESYNC_SECTORS) &&
+ 				      current->bio_list &&
+ 				      !bio_list_empty(current->bio_list))),
+ 				    conf->resync_lock);
+ 		conf->nr_waiting--;
+ 	}
+ 
+ 	if (bio && bio_data_dir(bio) == WRITE) {
+ 		if (bio->bi_iter.bi_sector >= conf->next_resync) {
+ 			if (conf->start_next_window == MaxSector)
+ 				conf->start_next_window =
+ 					conf->next_resync +
+ 					NEXT_NORMALIO_DISTANCE;
+ 
+ 			if ((conf->start_next_window + NEXT_NORMALIO_DISTANCE)
+ 			    <= bio->bi_iter.bi_sector)
+ 				conf->next_window_requests++;
+ 			else
+ 				conf->current_window_requests++;
+ 			sector = conf->start_next_window;
+ 		}
+ 	}
+ 
+ 	conf->nr_pending++;
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  	spin_unlock_irq(&conf->resync_lock);
 -	return sector;
  }
  
 -static void allow_barrier(struct r1conf *conf, sector_t start_next_window,
 -			  sector_t bi_sector)
 +static void wait_barrier(struct r1conf *conf, sector_t sector_nr)
  {
 -	unsigned long flags;
 +	int idx = sector_to_idx(sector_nr);
  
 -	spin_lock_irqsave(&conf->resync_lock, flags);
 -	conf->nr_pending--;
 -	if (start_next_window) {
 -		if (start_next_window == conf->start_next_window) {
 -			if (conf->start_next_window + NEXT_NORMALIO_DISTANCE
 -			    <= bi_sector)
 -				conf->next_window_requests--;
 -			else
 -				conf->current_window_requests--;
 -		} else
 -			conf->current_window_requests--;
 -
 -		if (!conf->current_window_requests) {
 -			if (conf->next_window_requests) {
 -				conf->current_window_requests =
 -					conf->next_window_requests;
 -				conf->next_window_requests = 0;
 -				conf->start_next_window +=
 -					NEXT_NORMALIO_DISTANCE;
 -			} else
 -				conf->start_next_window = MaxSector;
 -		}
 -	}
 -	spin_unlock_irqrestore(&conf->resync_lock, flags);
 +	_wait_barrier(conf, idx);
 +}
 +
 +static void _allow_barrier(struct r1conf *conf, int idx)
 +{
 +	atomic_dec(&conf->nr_pending[idx]);
  	wake_up(&conf->wait_barrier);
  }
  
@@@ -1120,11 -944,11 +1166,19 @@@ static void freeze_array(struct r1conf 
  	 */
  	spin_lock_irq(&conf->resync_lock);
  	conf->array_frozen = 1;
++<<<<<<< HEAD
 +	wait_event_lock_irq_cmd(
 +		conf->wait_barrier,
 +		get_unqueued_pending(conf) == extra,
 +		conf->resync_lock,
 +		flush_pending_writes(conf));
++=======
+ 	raid1_log(conf->mddev, "wait freeze");
+ 	wait_event_lock_irq_cmd(conf->wait_barrier,
+ 				conf->nr_pending == conf->nr_queued+extra,
+ 				conf->resync_lock,
+ 				flush_pending_writes(conf));
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  	spin_unlock_irq(&conf->resync_lock);
  }
  static void unfreeze_array(struct r1conf *conf)
@@@ -1321,11 -1072,149 +1375,153 @@@ static bool raid1_write_request(struct 
  	 * Continue immediately if no resync is active currently.
  	 */
  
 -	md_write_start(mddev, bio); /* wait on superblock update early */
 +	if(!md_write_start(mddev, bio)) /* wait on superblock update early */
 +		return false;
  
++<<<<<<< HEAD
++=======
+ 	if (bio_data_dir(bio) == WRITE &&
+ 	    ((bio_end_sector(bio) > mddev->suspend_lo &&
+ 	    bio->bi_iter.bi_sector < mddev->suspend_hi) ||
+ 	    (mddev_is_clustered(mddev) &&
+ 	     md_cluster_ops->area_resyncing(mddev, WRITE,
+ 		     bio->bi_iter.bi_sector, bio_end_sector(bio))))) {
+ 		/* As the suspend_* range is controlled by
+ 		 * userspace, we want an interruptible
+ 		 * wait.
+ 		 */
+ 		DEFINE_WAIT(w);
+ 		for (;;) {
+ 			flush_signals(current);
+ 			prepare_to_wait(&conf->wait_barrier,
+ 					&w, TASK_INTERRUPTIBLE);
+ 			if (bio_end_sector(bio) <= mddev->suspend_lo ||
+ 			    bio->bi_iter.bi_sector >= mddev->suspend_hi ||
+ 			    (mddev_is_clustered(mddev) &&
+ 			     !md_cluster_ops->area_resyncing(mddev, WRITE,
+ 				     bio->bi_iter.bi_sector, bio_end_sector(bio))))
+ 				break;
+ 			schedule();
+ 		}
+ 		finish_wait(&conf->wait_barrier, &w);
+ 	}
+ 
+ 	start_next_window = wait_barrier(conf, bio);
+ 
+ 	bitmap = mddev->bitmap;
+ 
+ 	/*
+ 	 * make_request() can abort the operation when read-ahead is being
+ 	 * used and no empty request is available.
+ 	 *
+ 	 */
+ 	r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
+ 
+ 	r1_bio->master_bio = bio;
+ 	r1_bio->sectors = bio_sectors(bio);
+ 	r1_bio->state = 0;
+ 	r1_bio->mddev = mddev;
+ 	r1_bio->sector = bio->bi_iter.bi_sector;
+ 
+ 	/* We might need to issue multiple reads to different
+ 	 * devices if there are bad blocks around, so we keep
+ 	 * track of the number of reads in bio->bi_phys_segments.
+ 	 * If this is 0, there is only one r1_bio and no locking
+ 	 * will be needed when requests complete.  If it is
+ 	 * non-zero, then it is the number of not-completed requests.
+ 	 */
+ 	bio->bi_phys_segments = 0;
+ 	bio_clear_flag(bio, BIO_SEG_VALID);
+ 
+ 	if (rw == READ) {
+ 		/*
+ 		 * read balancing logic:
+ 		 */
+ 		int rdisk;
+ 
+ read_again:
+ 		rdisk = read_balance(conf, r1_bio, &max_sectors);
+ 
+ 		if (rdisk < 0) {
+ 			/* couldn't find anywhere to read from */
+ 			raid_end_bio_io(r1_bio);
+ 			return;
+ 		}
+ 		mirror = conf->mirrors + rdisk;
+ 
+ 		if (test_bit(WriteMostly, &mirror->rdev->flags) &&
+ 		    bitmap) {
+ 			/* Reading from a write-mostly device must
+ 			 * take care not to over-take any writes
+ 			 * that are 'behind'
+ 			 */
+ 			raid1_log(mddev, "wait behind writes");
+ 			wait_event(bitmap->behind_wait,
+ 				   atomic_read(&bitmap->behind_writes) == 0);
+ 		}
+ 		r1_bio->read_disk = rdisk;
+ 		r1_bio->start_next_window = 0;
+ 
+ 		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 		bio_trim(read_bio, r1_bio->sector - bio->bi_iter.bi_sector,
+ 			 max_sectors);
+ 
+ 		r1_bio->bios[rdisk] = read_bio;
+ 
+ 		read_bio->bi_iter.bi_sector = r1_bio->sector +
+ 			mirror->rdev->data_offset;
+ 		read_bio->bi_bdev = mirror->rdev->bdev;
+ 		read_bio->bi_end_io = raid1_end_read_request;
+ 		bio_set_op_attrs(read_bio, op, do_sync);
+ 		read_bio->bi_private = r1_bio;
+ 
+ 		if (mddev->gendisk)
+ 			trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 					      read_bio, disk_devt(mddev->gendisk),
+ 					      r1_bio->sector);
+ 
+ 		if (max_sectors < r1_bio->sectors) {
+ 			/* could not read all from this device, so we will
+ 			 * need another r1_bio.
+ 			 */
+ 
+ 			sectors_handled = (r1_bio->sector + max_sectors
+ 					   - bio->bi_iter.bi_sector);
+ 			r1_bio->sectors = max_sectors;
+ 			spin_lock_irq(&conf->device_lock);
+ 			if (bio->bi_phys_segments == 0)
+ 				bio->bi_phys_segments = 2;
+ 			else
+ 				bio->bi_phys_segments++;
+ 			spin_unlock_irq(&conf->device_lock);
+ 			/* Cannot call generic_make_request directly
+ 			 * as that will be queued in __make_request
+ 			 * and subsequent mempool_alloc might block waiting
+ 			 * for it.  So hand bio over to raid1d.
+ 			 */
+ 			reschedule_retry(r1_bio);
+ 
+ 			r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
+ 
+ 			r1_bio->master_bio = bio;
+ 			r1_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 			r1_bio->state = 0;
+ 			r1_bio->mddev = mddev;
+ 			r1_bio->sector = bio->bi_iter.bi_sector +
+ 				sectors_handled;
+ 			goto read_again;
+ 		} else
+ 			generic_make_request(read_bio);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * WRITE:
+ 	 */
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
+ 		raid1_log(mddev, "wait queued");
  		wait_event(conf->wait_barrier,
  			   conf->pending_count < max_queued_requests);
  	}
@@@ -1420,8 -1305,19 +1616,13 @@@
  			if (r1_bio->bios[j])
  				rdev_dec_pending(conf->mirrors[j].rdev, mddev);
  		r1_bio->state = 0;
++<<<<<<< HEAD
 +		allow_barrier(conf, r1_bio->sector);
++=======
+ 		allow_barrier(conf, start_next_window, bio->bi_iter.bi_sector);
+ 		raid1_log(mddev, "wait rdev %d blocked", blocked_rdev->raid_disk);
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  		md_wait_for_blocked_rdev(blocked_rdev, mddev);
 -		start_next_window = wait_barrier(conf, bio);
 -		/*
 -		 * We must make sure the multi r1bios of bio have
 -		 * the same value of bi_phys_segments
 -		 */
 -		if (bio->bi_phys_segments && old &&
 -		    old != start_next_window)
 -			/* Wait for the former r1bio(s) to complete */
 -			wait_event(conf->wait_barrier,
 -				   bio->bi_phys_segments == 1);
  		goto retry_write;
  	}
  
diff --cc drivers/md/raid10.c
index ca76012444cf,bd8c884d4596..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -103,26 -103,11 +103,31 @@@ static int enough(struct r10conf *conf
  static sector_t reshape_request(struct mddev *mddev, sector_t sector_nr,
  				int *skipped);
  static void reshape_request_write(struct mddev *mddev, struct r10bio *r10_bio);
 -static void end_reshape_write(struct bio *bio);
 +static void end_reshape_write(struct bio *bio, int error);
  static void end_reshape(struct r10conf *conf);
  
++<<<<<<< HEAD
 +/*
 + * 'strct resync_pages' stores actual pages used for doing the resync
 + *  IO, and it is per-bio, so make .bi_private points to it.
 + */
 +static inline struct resync_pages *get_resync_pages(struct bio *bio)
 +{
 +	return bio->bi_private;
 +}
 +
 +/*
 + * for resync bio, r10bio pointer can be retrieved from the per-bio
 + * 'struct resync_pages'.
 + */
 +static inline struct r10bio *get_resync_r10bio(struct bio *bio)
 +{
 +	return get_resync_pages(bio)->raid_bio;
 +}
++=======
+ #define raid10_log(md, fmt, args...)				\
+ 	do { if ((md)->queue) blk_add_trace_msg((md)->queue, "raid10 " fmt, ##args); } while (0)
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  
  static void * r10bio_pool_alloc(gfp_t gfp_flags, void *data)
  {
@@@ -1382,27 -1082,140 +1388,46 @@@ static void raid10_write_request(struc
  
  	md_write_start(mddev, bio);
  
 -	/*
 -	 * Register the new request and wait if the reconstruction
 -	 * thread has put up a bar for new requests.
 -	 * Continue immediately if no resync is active currently.
 -	 */
 -	wait_barrier(conf);
 -
  	sectors = bio_sectors(bio);
++<<<<<<< HEAD
++=======
+ 	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
+ 	    bio->bi_iter.bi_sector < conf->reshape_progress &&
+ 	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
+ 		/* IO spans the reshape position.  Need to wait for
+ 		 * reshape to pass
+ 		 */
+ 		raid10_log(conf->mddev, "wait reshape");
+ 		allow_barrier(conf);
+ 		wait_event(conf->wait_barrier,
+ 			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
+ 			   conf->reshape_progress >= bio->bi_iter.bi_sector +
+ 			   sectors);
+ 		wait_barrier(conf);
+ 	}
++>>>>>>> 578b54ade8a5 (md/raid1, raid10: add blktrace records when IO is delayed)
  	if (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
 -	    bio_data_dir(bio) == WRITE &&
  	    (mddev->reshape_backwards
 -	     ? (bio->bi_iter.bi_sector < conf->reshape_safe &&
 -		bio->bi_iter.bi_sector + sectors > conf->reshape_progress)
 -	     : (bio->bi_iter.bi_sector + sectors > conf->reshape_safe &&
 -		bio->bi_iter.bi_sector < conf->reshape_progress))) {
 +	     ? (bio->bi_sector < conf->reshape_safe &&
 +		bio->bi_sector + sectors > conf->reshape_progress)
 +	     : (bio->bi_sector + sectors > conf->reshape_safe &&
 +		bio->bi_sector < conf->reshape_progress))) {
 +		gmb();
  		/* Need to update reshape_position in metadata */
  		mddev->reshape_position = conf->reshape_progress;
 -		set_mask_bits(&mddev->flags, 0,
 -			      BIT(MD_CHANGE_DEVS) | BIT(MD_CHANGE_PENDING));
 +		set_mask_bits(&mddev->sb_flags, 0,
 +			      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));
  		md_wakeup_thread(mddev->thread);
+ 		raid10_log(conf->mddev, "wait reshape metadata");
  		wait_event(mddev->sb_wait,
 -			   !test_bit(MD_CHANGE_PENDING, &mddev->flags));
 +			   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags));
  
  		conf->reshape_safe = mddev->reshape_position;
  	}
  
 -	r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
 -
 -	r10_bio->master_bio = bio;
 -	r10_bio->sectors = sectors;
 -
 -	r10_bio->mddev = mddev;
 -	r10_bio->sector = bio->bi_iter.bi_sector;
 -	r10_bio->state = 0;
 -
 -	/* We might need to issue multiple reads to different
 -	 * devices if there are bad blocks around, so we keep
 -	 * track of the number of reads in bio->bi_phys_segments.
 -	 * If this is 0, there is only one r10_bio and no locking
 -	 * will be needed when the request completes.  If it is
 -	 * non-zero, then it is the number of not-completed requests.
 -	 */
 -	bio->bi_phys_segments = 0;
 -	bio_clear_flag(bio, BIO_SEG_VALID);
 -
 -	if (rw == READ) {
 -		/*
 -		 * read balancing logic:
 -		 */
 -		struct md_rdev *rdev;
 -		int slot;
 -
 -read_again:
 -		rdev = read_balance(conf, r10_bio, &max_sectors);
 -		if (!rdev) {
 -			raid_end_bio_io(r10_bio);
 -			return;
 -		}
 -		slot = r10_bio->read_slot;
 -
 -		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 -		bio_trim(read_bio, r10_bio->sector - bio->bi_iter.bi_sector,
 -			 max_sectors);
 -
 -		r10_bio->devs[slot].bio = read_bio;
 -		r10_bio->devs[slot].rdev = rdev;
 -
 -		read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
 -			choose_data_offset(r10_bio, rdev);
 -		read_bio->bi_bdev = rdev->bdev;
 -		read_bio->bi_end_io = raid10_end_read_request;
 -		bio_set_op_attrs(read_bio, op, do_sync);
 -		read_bio->bi_private = r10_bio;
 -
 -		if (mddev->gendisk)
 -			trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
 -					      read_bio, disk_devt(mddev->gendisk),
 -					      r10_bio->sector);
 -		if (max_sectors < r10_bio->sectors) {
 -			/* Could not read all from this device, so we will
 -			 * need another r10_bio.
 -			 */
 -			sectors_handled = (r10_bio->sector + max_sectors
 -					   - bio->bi_iter.bi_sector);
 -			r10_bio->sectors = max_sectors;
 -			spin_lock_irq(&conf->device_lock);
 -			if (bio->bi_phys_segments == 0)
 -				bio->bi_phys_segments = 2;
 -			else
 -				bio->bi_phys_segments++;
 -			spin_unlock_irq(&conf->device_lock);
 -			/* Cannot call generic_make_request directly
 -			 * as that will be queued in __generic_make_request
 -			 * and subsequent mempool_alloc might block
 -			 * waiting for it.  so hand bio over to raid10d.
 -			 */
 -			reschedule_retry(r10_bio);
 -
 -			r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
 -
 -			r10_bio->master_bio = bio;
 -			r10_bio->sectors = bio_sectors(bio) - sectors_handled;
 -			r10_bio->state = 0;
 -			r10_bio->mddev = mddev;
 -			r10_bio->sector = bio->bi_iter.bi_sector +
 -				sectors_handled;
 -			goto read_again;
 -		} else
 -			generic_make_request(read_bio);
 -		return;
 -	}
 -
 -	/*
 -	 * WRITE:
 -	 */
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
+ 		raid10_log(mddev, "wait queued");
  		wait_event(conf->wait_barrier,
  			   conf->pending_count < max_queued_requests);
  	}
* Unmerged path drivers/md/raid1.c
* Unmerged path drivers/md/raid10.c
