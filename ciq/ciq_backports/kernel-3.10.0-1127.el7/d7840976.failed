net: Use skb accessors in network drivers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Matthew Wilcox (Oracle) <willy@infradead.org>
commit d7840976e3915669382c62ddd1700960f348328e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/d7840976.failed

In preparation for unifying the skb_frag and bio_vec, use the fine
accessors which already exist and use skb_frag_t instead of
struct skb_frag_struct.

	Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d7840976e3915669382c62ddd1700960f348328e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chtls/chtls_io.c
#	drivers/hsi/clients/ssi_protocol.c
#	drivers/net/ethernet/3com/3c59x.c
#	drivers/net/ethernet/apm/xgene/xgene_enet_main.c
#	drivers/net/ethernet/broadcom/bgmac.c
#	drivers/net/ethernet/cavium/thunder/nicvf_queues.c
#	drivers/net/ethernet/cortina/gemini.c
#	drivers/net/ethernet/freescale/enetc/enetc.c
#	drivers/net/ethernet/freescale/fec_main.c
#	drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
#	drivers/net/ethernet/hisilicon/hns/hns_enet.c
#	drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
#	drivers/net/ethernet/huawei/hinic/hinic_tx.c
#	drivers/net/ethernet/jme.c
#	drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
#	drivers/net/ethernet/mediatek/mtk_eth_soc.c
#	drivers/net/ethernet/microchip/lan743x_main.c
#	drivers/net/ethernet/qualcomm/emac/emac-mac.c
#	drivers/net/ethernet/synopsys/dwc-xlgmac-desc.c
#	drivers/net/ethernet/synopsys/dwc-xlgmac-net.c
#	drivers/net/wireless/ath/wil6210/txrx_edma.c
#	drivers/net/xen-netback/netback.c
#	drivers/s390/net/qeth_core_main.c
#	drivers/staging/et131x/et131x.c
#	drivers/staging/octeon/ethernet-tx.c
#	drivers/staging/unisys/visornic/visornic_main.c
diff --cc drivers/net/ethernet/3com/3c59x.c
index 82bb07ec24a5,7be91e896f2d..000000000000
--- a/drivers/net/ethernet/3com/3c59x.c
+++ b/drivers/net/ethernet/3com/3c59x.c
@@@ -2175,11 -2173,27 +2175,33 @@@ boomerang_start_xmit(struct sk_buff *sk
  		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
  			skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
  
++<<<<<<< HEAD
++=======
+ 			dma_addr = skb_frag_dma_map(vp->gendev, frag,
+ 						    0,
+ 						    skb_frag_size(frag),
+ 						    DMA_TO_DEVICE);
+ 			if (dma_mapping_error(vp->gendev, dma_addr)) {
+ 				for(i = i-1; i >= 0; i--)
+ 					dma_unmap_page(vp->gendev,
+ 						       le32_to_cpu(vp->tx_ring[entry].frag[i+1].addr),
+ 						       le32_to_cpu(vp->tx_ring[entry].frag[i+1].length),
+ 						       DMA_TO_DEVICE);
+ 
+ 				dma_unmap_single(vp->gendev,
+ 						 le32_to_cpu(vp->tx_ring[entry].frag[0].addr),
+ 						 le32_to_cpu(vp->tx_ring[entry].frag[0].length),
+ 						 DMA_TO_DEVICE);
+ 
+ 				goto out_dma_err;
+ 			}
+ 
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  			vp->tx_ring[entry].frag[i+1].addr =
 -						cpu_to_le32(dma_addr);
 +					cpu_to_le32(pci_map_single(
 +						VORTEX_PCI(vp),
 +						(void *)skb_frag_address(frag),
 +						skb_frag_size(frag), PCI_DMA_TODEVICE));
  
  			if (i == skb_shinfo(skb)->nr_frags-1)
  					vp->tx_ring[entry].frag[i+1].length = cpu_to_le32(skb_frag_size(frag)|LAST_FRAG);
diff --cc drivers/net/ethernet/broadcom/bgmac.c
index 566081066bd6,148734b166f0..000000000000
--- a/drivers/net/ethernet/broadcom/bgmac.c
+++ b/drivers/net/ethernet/broadcom/bgmac.c
@@@ -127,27 -159,38 +127,55 @@@ static netdev_tx_t bgmac_dma_tx_add(str
  		return NETDEV_TX_BUSY;
  	}
  
 -	slot->dma_addr = dma_map_single(dma_dev, skb->data, skb_headlen(skb),
 +	slot = &ring->slots[ring->end];
 +	slot->skb = skb;
 +	slot->dma_addr = dma_map_single(dma_dev, skb->data, skb->len,
  					DMA_TO_DEVICE);
++<<<<<<< HEAD
 +	if (dma_mapping_error(dma_dev, slot->dma_addr)) {
 +		bgmac_err(bgmac, "Mapping error of skb on ring 0x%X\n",
 +			  ring->mmio_base);
 +		goto err_stop_drop;
++=======
+ 	if (unlikely(dma_mapping_error(dma_dev, slot->dma_addr)))
+ 		goto err_dma_head;
+ 
+ 	flags = BGMAC_DESC_CTL0_SOF;
+ 	if (!nr_frags)
+ 		flags |= BGMAC_DESC_CTL0_EOF | BGMAC_DESC_CTL0_IOC;
+ 
+ 	bgmac_dma_tx_add_buf(bgmac, ring, index, skb_headlen(skb), flags);
+ 	flags = 0;
+ 
+ 	for (i = 0; i < nr_frags; i++) {
+ 		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+ 		int len = skb_frag_size(frag);
+ 
+ 		index = (index + 1) % BGMAC_TX_RING_SLOTS;
+ 		slot = &ring->slots[index];
+ 		slot->dma_addr = skb_frag_dma_map(dma_dev, frag, 0,
+ 						  len, DMA_TO_DEVICE);
+ 		if (unlikely(dma_mapping_error(dma_dev, slot->dma_addr)))
+ 			goto err_dma;
+ 
+ 		if (i == nr_frags - 1)
+ 			flags |= BGMAC_DESC_CTL0_EOF | BGMAC_DESC_CTL0_IOC;
+ 
+ 		bgmac_dma_tx_add_buf(bgmac, ring, index, len, flags);
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  	}
  
 -	slot->skb = skb;
 -	ring->end += nr_frags + 1;
 -	netdev_sent_queue(net_dev, skb->len);
 +	ctl0 = BGMAC_DESC_CTL0_IOC | BGMAC_DESC_CTL0_SOF | BGMAC_DESC_CTL0_EOF;
 +	if (ring->end == ring->num_slots - 1)
 +		ctl0 |= BGMAC_DESC_CTL0_EOT;
 +	ctl1 = skb->len & BGMAC_DESC_CTL1_LEN;
 +
 +	dma_desc = ring->cpu_base;
 +	dma_desc += ring->end;
 +	dma_desc->addr_low = cpu_to_le32(lower_32_bits(slot->dma_addr));
 +	dma_desc->addr_high = cpu_to_le32(upper_32_bits(slot->dma_addr));
 +	dma_desc->ctl0 = cpu_to_le32(ctl0);
 +	dma_desc->ctl1 = cpu_to_le32(ctl1);
  
  	wmb();
  
diff --cc drivers/net/ethernet/freescale/fec_main.c
index 5039c86e13f6,c01d3ec3e9af..000000000000
--- a/drivers/net/ethernet/freescale/fec_main.c
+++ b/drivers/net/ethernet/freescale/fec_main.c
@@@ -264,6 -339,420 +264,423 @@@ fec_enet_clear_csum(struct sk_buff *skb
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static struct bufdesc *
+ fec_enet_txq_submit_frag_skb(struct fec_enet_priv_tx_q *txq,
+ 			     struct sk_buff *skb,
+ 			     struct net_device *ndev)
+ {
+ 	struct fec_enet_private *fep = netdev_priv(ndev);
+ 	struct bufdesc *bdp = txq->bd.cur;
+ 	struct bufdesc_ex *ebdp;
+ 	int nr_frags = skb_shinfo(skb)->nr_frags;
+ 	int frag, frag_len;
+ 	unsigned short status;
+ 	unsigned int estatus = 0;
+ 	skb_frag_t *this_frag;
+ 	unsigned int index;
+ 	void *bufaddr;
+ 	dma_addr_t addr;
+ 	int i;
+ 
+ 	for (frag = 0; frag < nr_frags; frag++) {
+ 		this_frag = &skb_shinfo(skb)->frags[frag];
+ 		bdp = fec_enet_get_nextdesc(bdp, &txq->bd);
+ 		ebdp = (struct bufdesc_ex *)bdp;
+ 
+ 		status = fec16_to_cpu(bdp->cbd_sc);
+ 		status &= ~BD_ENET_TX_STATS;
+ 		status |= (BD_ENET_TX_TC | BD_ENET_TX_READY);
+ 		frag_len = skb_frag_size(&skb_shinfo(skb)->frags[frag]);
+ 
+ 		/* Handle the last BD specially */
+ 		if (frag == nr_frags - 1) {
+ 			status |= (BD_ENET_TX_INTR | BD_ENET_TX_LAST);
+ 			if (fep->bufdesc_ex) {
+ 				estatus |= BD_ENET_TX_INT;
+ 				if (unlikely(skb_shinfo(skb)->tx_flags &
+ 					SKBTX_HW_TSTAMP && fep->hwts_tx_en))
+ 					estatus |= BD_ENET_TX_TS;
+ 			}
+ 		}
+ 
+ 		if (fep->bufdesc_ex) {
+ 			if (fep->quirks & FEC_QUIRK_HAS_AVB)
+ 				estatus |= FEC_TX_BD_FTYPE(txq->bd.qid);
+ 			if (skb->ip_summed == CHECKSUM_PARTIAL)
+ 				estatus |= BD_ENET_TX_PINS | BD_ENET_TX_IINS;
+ 			ebdp->cbd_bdu = 0;
+ 			ebdp->cbd_esc = cpu_to_fec32(estatus);
+ 		}
+ 
+ 		bufaddr = skb_frag_address(this_frag);
+ 
+ 		index = fec_enet_get_bd_index(bdp, &txq->bd);
+ 		if (((unsigned long) bufaddr) & fep->tx_align ||
+ 			fep->quirks & FEC_QUIRK_SWAP_FRAME) {
+ 			memcpy(txq->tx_bounce[index], bufaddr, frag_len);
+ 			bufaddr = txq->tx_bounce[index];
+ 
+ 			if (fep->quirks & FEC_QUIRK_SWAP_FRAME)
+ 				swap_buffer(bufaddr, frag_len);
+ 		}
+ 
+ 		addr = dma_map_single(&fep->pdev->dev, bufaddr, frag_len,
+ 				      DMA_TO_DEVICE);
+ 		if (dma_mapping_error(&fep->pdev->dev, addr)) {
+ 			if (net_ratelimit())
+ 				netdev_err(ndev, "Tx DMA memory map failed\n");
+ 			goto dma_mapping_error;
+ 		}
+ 
+ 		bdp->cbd_bufaddr = cpu_to_fec32(addr);
+ 		bdp->cbd_datlen = cpu_to_fec16(frag_len);
+ 		/* Make sure the updates to rest of the descriptor are
+ 		 * performed before transferring ownership.
+ 		 */
+ 		wmb();
+ 		bdp->cbd_sc = cpu_to_fec16(status);
+ 	}
+ 
+ 	return bdp;
+ dma_mapping_error:
+ 	bdp = txq->bd.cur;
+ 	for (i = 0; i < frag; i++) {
+ 		bdp = fec_enet_get_nextdesc(bdp, &txq->bd);
+ 		dma_unmap_single(&fep->pdev->dev, fec32_to_cpu(bdp->cbd_bufaddr),
+ 				 fec16_to_cpu(bdp->cbd_datlen), DMA_TO_DEVICE);
+ 	}
+ 	return ERR_PTR(-ENOMEM);
+ }
+ 
+ static int fec_enet_txq_submit_skb(struct fec_enet_priv_tx_q *txq,
+ 				   struct sk_buff *skb, struct net_device *ndev)
+ {
+ 	struct fec_enet_private *fep = netdev_priv(ndev);
+ 	int nr_frags = skb_shinfo(skb)->nr_frags;
+ 	struct bufdesc *bdp, *last_bdp;
+ 	void *bufaddr;
+ 	dma_addr_t addr;
+ 	unsigned short status;
+ 	unsigned short buflen;
+ 	unsigned int estatus = 0;
+ 	unsigned int index;
+ 	int entries_free;
+ 
+ 	entries_free = fec_enet_get_free_txdesc_num(txq);
+ 	if (entries_free < MAX_SKB_FRAGS + 1) {
+ 		dev_kfree_skb_any(skb);
+ 		if (net_ratelimit())
+ 			netdev_err(ndev, "NOT enough BD for SG!\n");
+ 		return NETDEV_TX_OK;
+ 	}
+ 
+ 	/* Protocol checksum off-load for TCP and UDP. */
+ 	if (fec_enet_clear_csum(skb, ndev)) {
+ 		dev_kfree_skb_any(skb);
+ 		return NETDEV_TX_OK;
+ 	}
+ 
+ 	/* Fill in a Tx ring entry */
+ 	bdp = txq->bd.cur;
+ 	last_bdp = bdp;
+ 	status = fec16_to_cpu(bdp->cbd_sc);
+ 	status &= ~BD_ENET_TX_STATS;
+ 
+ 	/* Set buffer length and buffer pointer */
+ 	bufaddr = skb->data;
+ 	buflen = skb_headlen(skb);
+ 
+ 	index = fec_enet_get_bd_index(bdp, &txq->bd);
+ 	if (((unsigned long) bufaddr) & fep->tx_align ||
+ 		fep->quirks & FEC_QUIRK_SWAP_FRAME) {
+ 		memcpy(txq->tx_bounce[index], skb->data, buflen);
+ 		bufaddr = txq->tx_bounce[index];
+ 
+ 		if (fep->quirks & FEC_QUIRK_SWAP_FRAME)
+ 			swap_buffer(bufaddr, buflen);
+ 	}
+ 
+ 	/* Push the data cache so the CPM does not get stale memory data. */
+ 	addr = dma_map_single(&fep->pdev->dev, bufaddr, buflen, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(&fep->pdev->dev, addr)) {
+ 		dev_kfree_skb_any(skb);
+ 		if (net_ratelimit())
+ 			netdev_err(ndev, "Tx DMA memory map failed\n");
+ 		return NETDEV_TX_OK;
+ 	}
+ 
+ 	if (nr_frags) {
+ 		last_bdp = fec_enet_txq_submit_frag_skb(txq, skb, ndev);
+ 		if (IS_ERR(last_bdp)) {
+ 			dma_unmap_single(&fep->pdev->dev, addr,
+ 					 buflen, DMA_TO_DEVICE);
+ 			dev_kfree_skb_any(skb);
+ 			return NETDEV_TX_OK;
+ 		}
+ 	} else {
+ 		status |= (BD_ENET_TX_INTR | BD_ENET_TX_LAST);
+ 		if (fep->bufdesc_ex) {
+ 			estatus = BD_ENET_TX_INT;
+ 			if (unlikely(skb_shinfo(skb)->tx_flags &
+ 				SKBTX_HW_TSTAMP && fep->hwts_tx_en))
+ 				estatus |= BD_ENET_TX_TS;
+ 		}
+ 	}
+ 	bdp->cbd_bufaddr = cpu_to_fec32(addr);
+ 	bdp->cbd_datlen = cpu_to_fec16(buflen);
+ 
+ 	if (fep->bufdesc_ex) {
+ 
+ 		struct bufdesc_ex *ebdp = (struct bufdesc_ex *)bdp;
+ 
+ 		if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP &&
+ 			fep->hwts_tx_en))
+ 			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+ 
+ 		if (fep->quirks & FEC_QUIRK_HAS_AVB)
+ 			estatus |= FEC_TX_BD_FTYPE(txq->bd.qid);
+ 
+ 		if (skb->ip_summed == CHECKSUM_PARTIAL)
+ 			estatus |= BD_ENET_TX_PINS | BD_ENET_TX_IINS;
+ 
+ 		ebdp->cbd_bdu = 0;
+ 		ebdp->cbd_esc = cpu_to_fec32(estatus);
+ 	}
+ 
+ 	index = fec_enet_get_bd_index(last_bdp, &txq->bd);
+ 	/* Save skb pointer */
+ 	txq->tx_skbuff[index] = skb;
+ 
+ 	/* Make sure the updates to rest of the descriptor are performed before
+ 	 * transferring ownership.
+ 	 */
+ 	wmb();
+ 
+ 	/* Send it on its way.  Tell FEC it's ready, interrupt when done,
+ 	 * it's the last BD of the frame, and to put the CRC on the end.
+ 	 */
+ 	status |= (BD_ENET_TX_READY | BD_ENET_TX_TC);
+ 	bdp->cbd_sc = cpu_to_fec16(status);
+ 
+ 	/* If this was the last BD in the ring, start at the beginning again. */
+ 	bdp = fec_enet_get_nextdesc(last_bdp, &txq->bd);
+ 
+ 	skb_tx_timestamp(skb);
+ 
+ 	/* Make sure the update to bdp and tx_skbuff are performed before
+ 	 * txq->bd.cur.
+ 	 */
+ 	wmb();
+ 	txq->bd.cur = bdp;
+ 
+ 	/* Trigger transmission start */
+ 	writel(0, txq->bd.reg_desc_active);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ fec_enet_txq_put_data_tso(struct fec_enet_priv_tx_q *txq, struct sk_buff *skb,
+ 			  struct net_device *ndev,
+ 			  struct bufdesc *bdp, int index, char *data,
+ 			  int size, bool last_tcp, bool is_last)
+ {
+ 	struct fec_enet_private *fep = netdev_priv(ndev);
+ 	struct bufdesc_ex *ebdp = container_of(bdp, struct bufdesc_ex, desc);
+ 	unsigned short status;
+ 	unsigned int estatus = 0;
+ 	dma_addr_t addr;
+ 
+ 	status = fec16_to_cpu(bdp->cbd_sc);
+ 	status &= ~BD_ENET_TX_STATS;
+ 
+ 	status |= (BD_ENET_TX_TC | BD_ENET_TX_READY);
+ 
+ 	if (((unsigned long) data) & fep->tx_align ||
+ 		fep->quirks & FEC_QUIRK_SWAP_FRAME) {
+ 		memcpy(txq->tx_bounce[index], data, size);
+ 		data = txq->tx_bounce[index];
+ 
+ 		if (fep->quirks & FEC_QUIRK_SWAP_FRAME)
+ 			swap_buffer(data, size);
+ 	}
+ 
+ 	addr = dma_map_single(&fep->pdev->dev, data, size, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(&fep->pdev->dev, addr)) {
+ 		dev_kfree_skb_any(skb);
+ 		if (net_ratelimit())
+ 			netdev_err(ndev, "Tx DMA memory map failed\n");
+ 		return NETDEV_TX_BUSY;
+ 	}
+ 
+ 	bdp->cbd_datlen = cpu_to_fec16(size);
+ 	bdp->cbd_bufaddr = cpu_to_fec32(addr);
+ 
+ 	if (fep->bufdesc_ex) {
+ 		if (fep->quirks & FEC_QUIRK_HAS_AVB)
+ 			estatus |= FEC_TX_BD_FTYPE(txq->bd.qid);
+ 		if (skb->ip_summed == CHECKSUM_PARTIAL)
+ 			estatus |= BD_ENET_TX_PINS | BD_ENET_TX_IINS;
+ 		ebdp->cbd_bdu = 0;
+ 		ebdp->cbd_esc = cpu_to_fec32(estatus);
+ 	}
+ 
+ 	/* Handle the last BD specially */
+ 	if (last_tcp)
+ 		status |= (BD_ENET_TX_LAST | BD_ENET_TX_TC);
+ 	if (is_last) {
+ 		status |= BD_ENET_TX_INTR;
+ 		if (fep->bufdesc_ex)
+ 			ebdp->cbd_esc |= cpu_to_fec32(BD_ENET_TX_INT);
+ 	}
+ 
+ 	bdp->cbd_sc = cpu_to_fec16(status);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ fec_enet_txq_put_hdr_tso(struct fec_enet_priv_tx_q *txq,
+ 			 struct sk_buff *skb, struct net_device *ndev,
+ 			 struct bufdesc *bdp, int index)
+ {
+ 	struct fec_enet_private *fep = netdev_priv(ndev);
+ 	int hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+ 	struct bufdesc_ex *ebdp = container_of(bdp, struct bufdesc_ex, desc);
+ 	void *bufaddr;
+ 	unsigned long dmabuf;
+ 	unsigned short status;
+ 	unsigned int estatus = 0;
+ 
+ 	status = fec16_to_cpu(bdp->cbd_sc);
+ 	status &= ~BD_ENET_TX_STATS;
+ 	status |= (BD_ENET_TX_TC | BD_ENET_TX_READY);
+ 
+ 	bufaddr = txq->tso_hdrs + index * TSO_HEADER_SIZE;
+ 	dmabuf = txq->tso_hdrs_dma + index * TSO_HEADER_SIZE;
+ 	if (((unsigned long)bufaddr) & fep->tx_align ||
+ 		fep->quirks & FEC_QUIRK_SWAP_FRAME) {
+ 		memcpy(txq->tx_bounce[index], skb->data, hdr_len);
+ 		bufaddr = txq->tx_bounce[index];
+ 
+ 		if (fep->quirks & FEC_QUIRK_SWAP_FRAME)
+ 			swap_buffer(bufaddr, hdr_len);
+ 
+ 		dmabuf = dma_map_single(&fep->pdev->dev, bufaddr,
+ 					hdr_len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(&fep->pdev->dev, dmabuf)) {
+ 			dev_kfree_skb_any(skb);
+ 			if (net_ratelimit())
+ 				netdev_err(ndev, "Tx DMA memory map failed\n");
+ 			return NETDEV_TX_BUSY;
+ 		}
+ 	}
+ 
+ 	bdp->cbd_bufaddr = cpu_to_fec32(dmabuf);
+ 	bdp->cbd_datlen = cpu_to_fec16(hdr_len);
+ 
+ 	if (fep->bufdesc_ex) {
+ 		if (fep->quirks & FEC_QUIRK_HAS_AVB)
+ 			estatus |= FEC_TX_BD_FTYPE(txq->bd.qid);
+ 		if (skb->ip_summed == CHECKSUM_PARTIAL)
+ 			estatus |= BD_ENET_TX_PINS | BD_ENET_TX_IINS;
+ 		ebdp->cbd_bdu = 0;
+ 		ebdp->cbd_esc = cpu_to_fec32(estatus);
+ 	}
+ 
+ 	bdp->cbd_sc = cpu_to_fec16(status);
+ 
+ 	return 0;
+ }
+ 
+ static int fec_enet_txq_submit_tso(struct fec_enet_priv_tx_q *txq,
+ 				   struct sk_buff *skb,
+ 				   struct net_device *ndev)
+ {
+ 	struct fec_enet_private *fep = netdev_priv(ndev);
+ 	int hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+ 	int total_len, data_left;
+ 	struct bufdesc *bdp = txq->bd.cur;
+ 	struct tso_t tso;
+ 	unsigned int index = 0;
+ 	int ret;
+ 
+ 	if (tso_count_descs(skb) >= fec_enet_get_free_txdesc_num(txq)) {
+ 		dev_kfree_skb_any(skb);
+ 		if (net_ratelimit())
+ 			netdev_err(ndev, "NOT enough BD for TSO!\n");
+ 		return NETDEV_TX_OK;
+ 	}
+ 
+ 	/* Protocol checksum off-load for TCP and UDP. */
+ 	if (fec_enet_clear_csum(skb, ndev)) {
+ 		dev_kfree_skb_any(skb);
+ 		return NETDEV_TX_OK;
+ 	}
+ 
+ 	/* Initialize the TSO handler, and prepare the first payload */
+ 	tso_start(skb, &tso);
+ 
+ 	total_len = skb->len - hdr_len;
+ 	while (total_len > 0) {
+ 		char *hdr;
+ 
+ 		index = fec_enet_get_bd_index(bdp, &txq->bd);
+ 		data_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);
+ 		total_len -= data_left;
+ 
+ 		/* prepare packet headers: MAC + IP + TCP */
+ 		hdr = txq->tso_hdrs + index * TSO_HEADER_SIZE;
+ 		tso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);
+ 		ret = fec_enet_txq_put_hdr_tso(txq, skb, ndev, bdp, index);
+ 		if (ret)
+ 			goto err_release;
+ 
+ 		while (data_left > 0) {
+ 			int size;
+ 
+ 			size = min_t(int, tso.size, data_left);
+ 			bdp = fec_enet_get_nextdesc(bdp, &txq->bd);
+ 			index = fec_enet_get_bd_index(bdp, &txq->bd);
+ 			ret = fec_enet_txq_put_data_tso(txq, skb, ndev,
+ 							bdp, index,
+ 							tso.data, size,
+ 							size == data_left,
+ 							total_len == 0);
+ 			if (ret)
+ 				goto err_release;
+ 
+ 			data_left -= size;
+ 			tso_build_data(skb, &tso, size);
+ 		}
+ 
+ 		bdp = fec_enet_get_nextdesc(bdp, &txq->bd);
+ 	}
+ 
+ 	/* Save skb pointer */
+ 	txq->tx_skbuff[index] = skb;
+ 
+ 	skb_tx_timestamp(skb);
+ 	txq->bd.cur = bdp;
+ 
+ 	/* Trigger transmission start */
+ 	if (!(fep->quirks & FEC_QUIRK_ERR007885) ||
+ 	    !readl(txq->bd.reg_desc_active) ||
+ 	    !readl(txq->bd.reg_desc_active) ||
+ 	    !readl(txq->bd.reg_desc_active) ||
+ 	    !readl(txq->bd.reg_desc_active))
+ 		writel(0, txq->bd.reg_desc_active);
+ 
+ 	return 0;
+ 
+ err_release:
+ 	/* TODO: Release all used data descriptors for TSO */
+ 	return ret;
+ }
+ 
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  static netdev_tx_t
  fec_enet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
  {
diff --cc drivers/net/ethernet/jme.c
index dac58884674e,ff6393fd64ac..000000000000
--- a/drivers/net/ethernet/jme.c
+++ b/drivers/net/ethernet/jme.c
@@@ -2032,17 -2030,22 +2032,24 @@@ jme_map_tx_skb(struct jme_adapter *jme
  	bool hidma = jme->dev->features & NETIF_F_HIGHDMA;
  	int i, nr_frags = skb_shinfo(skb)->nr_frags;
  	int mask = jme->tx_ring_mask;
- 	const struct skb_frag_struct *frag;
  	u32 len;
 -	int ret = 0;
  
  	for (i = 0 ; i < nr_frags ; ++i) {
- 		frag = &skb_shinfo(skb)->frags[i];
+ 		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+ 
  		ctxdesc = txdesc + ((idx + i + 2) & (mask));
  		ctxbi = txbi + ((idx + i + 2) & (mask));
  
 -		ret = jme_fill_tx_map(jme->pdev, ctxdesc, ctxbi,
 +		jme_fill_tx_map(jme->pdev, ctxdesc, ctxbi,
  				skb_frag_page(frag),
  				frag->page_offset, skb_frag_size(frag), hidma);
++<<<<<<< HEAD
++=======
+ 		if (ret) {
+ 			jme_drop_tx_map(jme, idx, i);
+ 			goto out;
+ 		}
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  	}
  
  	len = skb_is_nonlinear(skb) ? skb_headlen(skb) : skb->len;
diff --cc drivers/net/xen-netback/netback.c
index 9039f043a913,a96c5c2a2c5a..000000000000
--- a/drivers/net/xen-netback/netback.c
+++ b/drivers/net/xen-netback/netback.c
@@@ -1637,17 -1020,84 +1637,84 @@@ static unsigned xen_netbk_tx_build_gops
  			break;
  	}
  
 -	(*map_ops) = gop - queue->tx_map_ops;
 -	return;
 +	return gop - netbk->tx_copy_ops;
  }
  
 -/* Consolidate skb with a frag_list into a brand new one with local pages on
 - * frags. Returns 0 or -ENOMEM if can't allocate new pages.
 - */
 -static int xenvif_handle_frag_list(struct xenvif_queue *queue, struct sk_buff *skb)
 +static void xen_netbk_tx_submit(struct xen_netbk *netbk)
  {
++<<<<<<< HEAD
 +	struct gnttab_copy *gop = netbk->tx_copy_ops;
++=======
+ 	unsigned int offset = skb_headlen(skb);
+ 	skb_frag_t frags[MAX_SKB_FRAGS];
+ 	int i, f;
+ 	struct ubuf_info *uarg;
+ 	struct sk_buff *nskb = skb_shinfo(skb)->frag_list;
+ 
+ 	queue->stats.tx_zerocopy_sent += 2;
+ 	queue->stats.tx_frag_overflow++;
+ 
+ 	xenvif_fill_frags(queue, nskb);
+ 	/* Subtract frags size, we will correct it later */
+ 	skb->truesize -= skb->data_len;
+ 	skb->len += nskb->len;
+ 	skb->data_len += nskb->len;
+ 
+ 	/* create a brand new frags array and coalesce there */
+ 	for (i = 0; offset < skb->len; i++) {
+ 		struct page *page;
+ 		unsigned int len;
+ 
+ 		BUG_ON(i >= MAX_SKB_FRAGS);
+ 		page = alloc_page(GFP_ATOMIC);
+ 		if (!page) {
+ 			int j;
+ 			skb->truesize += skb->data_len;
+ 			for (j = 0; j < i; j++)
+ 				put_page(skb_frag_page(&frags[j]));
+ 			return -ENOMEM;
+ 		}
+ 
+ 		if (offset + PAGE_SIZE < skb->len)
+ 			len = PAGE_SIZE;
+ 		else
+ 			len = skb->len - offset;
+ 		if (skb_copy_bits(skb, offset, page_address(page), len))
+ 			BUG();
+ 
+ 		offset += len;
+ 		__skb_frag_set_page(&frags[i], page);
+ 		frags[i].page_offset = 0;
+ 		skb_frag_size_set(&frags[i], len);
+ 	}
+ 
+ 	/* Release all the original (foreign) frags. */
+ 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
+ 		skb_frag_unref(skb, f);
+ 	uarg = skb_shinfo(skb)->destructor_arg;
+ 	/* increase inflight counter to offset decrement in callback */
+ 	atomic_inc(&queue->inflight_packets);
+ 	uarg->callback(uarg, true);
+ 	skb_shinfo(skb)->destructor_arg = NULL;
+ 
+ 	/* Fill the skb with the new (local) frags. */
+ 	memcpy(skb_shinfo(skb)->frags, frags, i * sizeof(skb_frag_t));
+ 	skb_shinfo(skb)->nr_frags = i;
+ 	skb->truesize += i * PAGE_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int xenvif_tx_submit(struct xenvif_queue *queue)
+ {
+ 	struct gnttab_map_grant_ref *gop_map = queue->tx_map_ops;
+ 	struct gnttab_copy *gop_copy = queue->tx_copy_ops;
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  	struct sk_buff *skb;
 -	int work_done = 0;
  
 -	while ((skb = __skb_dequeue(&queue->tx_queue)) != NULL) {
 +	while ((skb = __skb_dequeue(&netbk->tx_queue)) != NULL) {
  		struct xen_netif_tx_request *txp;
 +		struct xenvif *vif;
  		u16 pending_idx;
  		unsigned data_len;
  
diff --cc drivers/s390/net/qeth_core_main.c
index b1a62a620d2b,5aa0f1268bca..000000000000
--- a/drivers/s390/net/qeth_core_main.c
+++ b/drivers/s390/net/qeth_core_main.c
@@@ -3784,20 -3503,23 +3784,28 @@@ int qeth_get_priority_queue(struct qeth
  }
  EXPORT_SYMBOL_GPL(qeth_get_priority_queue);
  
 -/**
 - * qeth_get_elements_for_frags() -	find number of SBALEs for skb frags.
 - * @skb:				SKB address
 - *
 - * Returns the number of pages, and thus QDIO buffer elements, needed to cover
 - * fragmented part of the SKB. Returns zero for linear SKB.
 - */
 -static int qeth_get_elements_for_frags(struct sk_buff *skb)
 +int qeth_get_elements_for_frags(struct sk_buff *skb)
  {
 -	int cnt, elements = 0;
 +	int cnt, length, e, elements = 0;
 +	struct skb_frag_struct *frag;
 +	char *data;
  
  	for (cnt = 0; cnt < skb_shinfo(skb)->nr_frags; cnt++) {
++<<<<<<< HEAD
 +		frag = &skb_shinfo(skb)->frags[cnt];
 +		data = (char *)page_to_phys(skb_frag_page(frag)) +
 +			frag->page_offset;
 +		length = frag->size;
 +		e = PFN_UP((unsigned long)data + length) -
 +			PFN_DOWN((unsigned long)data);
 +		elements += e;
++=======
+ 		skb_frag_t *frag = &skb_shinfo(skb)->frags[cnt];
+ 
+ 		elements += qeth_get_elements_for_range(
+ 			(addr_t)skb_frag_address(frag),
+ 			(addr_t)skb_frag_address(frag) + skb_frag_size(frag));
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  	}
  	return elements;
  }
diff --cc drivers/staging/et131x/et131x.c
index f73e58f5ef8d,e43d922f043e..000000000000
--- a/drivers/staging/et131x/et131x.c
+++ b/drivers/staging/et131x/et131x.c
@@@ -2893,10 -2426,10 +2893,15 @@@ static int nic_send_packet(struct et131
  	u32 thiscopy, remainder;
  	struct sk_buff *skb = tcb->skb;
  	u32 nr_frags = skb_shinfo(skb)->nr_frags + 1;
++<<<<<<< HEAD:drivers/staging/et131x/et131x.c
 +	struct skb_frag_struct *frags = &skb_shinfo(skb)->frags[0];
 +	unsigned long flags;
 +	struct phy_device *phydev = adapter->phydev;
++=======
+ 	skb_frag_t *frags = &skb_shinfo(skb)->frags[0];
+ 	struct phy_device *phydev = adapter->netdev->phydev;
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers):drivers/net/ethernet/agere/et131x.c
  	dma_addr_t dma_addr;
 -	struct tx_ring *tx_ring = &adapter->tx_ring;
  
  	/* Part of the optimizations of this send routine restrict us to
  	 * sending 24 fragments at a pass.  In practice we should never see
diff --cc drivers/staging/octeon/ethernet-tx.c
index 658e430bece4,cc12c78f73f1..000000000000
--- a/drivers/staging/octeon/ethernet-tx.c
+++ b/drivers/staging/octeon/ethernet-tx.c
@@@ -276,8 -280,10 +276,15 @@@ int cvm_oct_xmit(struct sk_buff *skb, s
  		hw_buffer.s.size = skb_headlen(skb);
  		CVM_OCT_SKB_CB(skb)[0] = hw_buffer.u64;
  		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
++<<<<<<< HEAD
 +			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i;
 +			hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)(page_address(fs->page.p) + fs->page_offset));
++=======
+ 			skb_frag_t *fs = skb_shinfo(skb)->frags + i;
+ 
+ 			hw_buffer.s.addr =
+ 				XKPHYS_TO_PHYS((u64)skb_frag_address(fs));
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  			hw_buffer.s.size = fs->size;
  			CVM_OCT_SKB_CB(skb)[i + 1] = hw_buffer.u64;
  		}
diff --cc drivers/staging/unisys/visornic/visornic_main.c
index 10459e8373b7,b889b04a6e25..000000000000
--- a/drivers/staging/unisys/visornic/visornic_main.c
+++ b/drivers/staging/unisys/visornic/visornic_main.c
@@@ -270,15 -281,13 +270,23 @@@ visor_copy_fragsinfo_from_skb(struct sk
  		if ((count + numfrags) > frags_max)
  			return -EINVAL;
  
 -		for (frag = 0; frag < numfrags; frag++) {
 +		for (ii = 0; ii < numfrags; ii++) {
  			count = add_physinfo_entries(page_to_pfn(
++<<<<<<< HEAD
 +				skb_frag_page(&skb_shinfo(skb)->frags[ii])),
 +					      skb_shinfo(skb)->frags[ii].
 +					      page_offset,
 +					      skb_shinfo(skb)->frags[ii].
 +					      size, count, frags_max, frags);
 +			/*
 +			 * add_physinfo_entries only returns
++=======
+ 				  skb_frag_page(&skb_shinfo(skb)->frags[frag])),
+ 				  skb_shinfo(skb)->frags[frag].page_offset,
+ 				  skb_frag_size(&skb_shinfo(skb)->frags[frag]),
+ 				  count, frags_max, frags);
+ 			/* add_physinfo_entries only returns
++>>>>>>> d7840976e391 (net: Use skb accessors in network drivers)
  			 * zero if the frags array is out of room
  			 * That should never happen because we
  			 * fail above, if count+numfrags > frags_max.
* Unmerged path drivers/crypto/chelsio/chtls/chtls_io.c
* Unmerged path drivers/hsi/clients/ssi_protocol.c
* Unmerged path drivers/net/ethernet/apm/xgene/xgene_enet_main.c
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_queues.c
* Unmerged path drivers/net/ethernet/cortina/gemini.c
* Unmerged path drivers/net/ethernet/freescale/enetc/enetc.c
* Unmerged path drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
* Unmerged path drivers/net/ethernet/hisilicon/hns/hns_enet.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_tx.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
* Unmerged path drivers/net/ethernet/mediatek/mtk_eth_soc.c
* Unmerged path drivers/net/ethernet/microchip/lan743x_main.c
* Unmerged path drivers/net/ethernet/qualcomm/emac/emac-mac.c
* Unmerged path drivers/net/ethernet/synopsys/dwc-xlgmac-desc.c
* Unmerged path drivers/net/ethernet/synopsys/dwc-xlgmac-net.c
* Unmerged path drivers/net/wireless/ath/wil6210/txrx_edma.c
* Unmerged path drivers/crypto/chelsio/chtls/chtls_io.c
* Unmerged path drivers/hsi/clients/ssi_protocol.c
diff --git a/drivers/infiniband/hw/hfi1/vnic_sdma.c b/drivers/infiniband/hw/hfi1/vnic_sdma.c
index 77a344792c8a..105f3f985551 100644
--- a/drivers/infiniband/hw/hfi1/vnic_sdma.c
+++ b/drivers/infiniband/hw/hfi1/vnic_sdma.c
@@ -102,7 +102,7 @@ static noinline int build_vnic_ulp_payload(struct sdma_engine *sde,
 		goto bail_txadd;
 
 	for (i = 0; i < skb_shinfo(tx->skb)->nr_frags; i++) {
-		struct skb_frag_struct *frag = &skb_shinfo(tx->skb)->frags[i];
+		skb_frag_t *frag = &skb_shinfo(tx->skb)->frags[i];
 
 		/* combine physically continuous fragments later? */
 		ret = sdma_txadd_page(sde->dd,
* Unmerged path drivers/net/ethernet/3com/3c59x.c
diff --git a/drivers/net/ethernet/amd/xgbe/xgbe-desc.c b/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
index 45d92304068e..67bce292e9da 100644
--- a/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
@@ -527,7 +527,7 @@ static int xgbe_map_tx_skb(struct xgbe_channel *channel, struct sk_buff *skb)
 	struct xgbe_ring *ring = channel->tx_ring;
 	struct xgbe_ring_data *rdata;
 	struct xgbe_packet_data *packet;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	dma_addr_t skb_dma;
 	unsigned int start_index, cur_index;
 	unsigned int offset, tso, vlan, datalen, len;
diff --git a/drivers/net/ethernet/amd/xgbe/xgbe-drv.c b/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
index 187468595e09..56dec570e397 100644
--- a/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
@@ -1834,7 +1834,7 @@ static void xgbe_packet_info(struct xgbe_prv_data *pdata,
 			     struct xgbe_ring *ring, struct sk_buff *skb,
 			     struct xgbe_packet_data *packet)
 {
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	unsigned int context_desc;
 	unsigned int len;
 	unsigned int i;
* Unmerged path drivers/net/ethernet/apm/xgene/xgene_enet_main.c
diff --git a/drivers/net/ethernet/atheros/alx/main.c b/drivers/net/ethernet/atheros/alx/main.c
index a1f6adf6682e..a14b42c9947e 100644
--- a/drivers/net/ethernet/atheros/alx/main.c
+++ b/drivers/net/ethernet/atheros/alx/main.c
@@ -1496,9 +1496,7 @@ static int alx_map_tx_skb(struct alx_tx_queue *txq, struct sk_buff *skb)
 	tpd->len = cpu_to_le16(maplen);
 
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++) {
-		struct skb_frag_struct *frag;
-
-		frag = &skb_shinfo(skb)->frags[f];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 
 		if (++txq->write_idx == txq->count)
 			txq->write_idx = 0;
diff --git a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
index fcadee4e057a..23f47ca2a7de 100644
--- a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+++ b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
@@ -2145,9 +2145,7 @@ static int atl1c_tx_map(struct atl1c_adapter *adapter,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		struct skb_frag_struct *frag;
-
-		frag = &skb_shinfo(skb)->frags[f];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 
 		use_tpd = atl1c_get_tpd(adapter, type);
 		memcpy(use_tpd, tpd, sizeof(struct atl1c_tpd_desc));
diff --git a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
index 585a55327c62..e47e92221512 100644
--- a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
+++ b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
@@ -1745,11 +1745,10 @@ static int atl1e_tx_map(struct atl1e_adapter *adapter,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 		u16 i;
 		u16 seg_num;
 
-		frag = &skb_shinfo(skb)->frags[f];
 		buf_len = skb_frag_size(frag);
 
 		seg_num = (buf_len + MAX_TX_BUF_LEN - 1) / MAX_TX_BUF_LEN;
diff --git a/drivers/net/ethernet/atheros/atlx/atl1.c b/drivers/net/ethernet/atheros/atlx/atl1.c
index 65a67b7f3162..018da1c944b0 100644
--- a/drivers/net/ethernet/atheros/atlx/atl1.c
+++ b/drivers/net/ethernet/atheros/atlx/atl1.c
@@ -2271,10 +2271,9 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 		u16 i, nseg;
 
-		frag = &skb_shinfo(skb)->frags[f];
 		buf_len = skb_frag_size(frag);
 
 		nseg = (buf_len + ATL1_MAX_TX_BUF_LEN - 1) /
* Unmerged path drivers/net/ethernet/broadcom/bgmac.c
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt.c b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
index 989bbf7ac2d8..f34bcf22761d 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
@@ -897,7 +897,7 @@ static struct sk_buff *bnxt_rx_page_skb(struct bnxt *bp,
 {
 	unsigned int payload = offset_and_len >> 16;
 	unsigned int len = offset_and_len & 0xffff;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	struct page *page = data;
 	u16 prod = rxr->rx_prod;
 	struct sk_buff *skb;
diff --git a/drivers/net/ethernet/brocade/bna/bnad.c b/drivers/net/ethernet/brocade/bna/bnad.c
index 44a0cf4a713f..10ee03b258ba 100644
--- a/drivers/net/ethernet/brocade/bna/bnad.c
+++ b/drivers/net/ethernet/brocade/bna/bnad.c
@@ -3042,7 +3042,7 @@ bnad_start_xmit(struct sk_buff *skb, struct net_device *netdev)
 	head_unmap->nvecs++;
 
 	for (i = 0, vect_id = 0; i < vectors - 1; i++) {
-		const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 		u32		size = skb_frag_size(frag);
 
 		if (unlikely(size == 0)) {
diff --git a/drivers/net/ethernet/calxeda/xgmac.c b/drivers/net/ethernet/calxeda/xgmac.c
index 97f7fe38c1c3..bbc22602a041 100644
--- a/drivers/net/ethernet/calxeda/xgmac.c
+++ b/drivers/net/ethernet/calxeda/xgmac.c
@@ -1095,7 +1095,7 @@ static netdev_tx_t xgmac_xmit(struct sk_buff *skb, struct net_device *dev)
 	for (i = 0; i < nfrags; i++) {
 		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 
-		len = frag->size;
+		len = skb_frag_size(frag);
 
 		paddr = skb_frag_dma_map(priv->device, frag, 0, len,
 					 DMA_TO_DEVICE);
diff --git a/drivers/net/ethernet/cavium/liquidio/lio_main.c b/drivers/net/ethernet/cavium/liquidio/lio_main.c
index a7c8432dc877..fe86b0aeca4d 100644
--- a/drivers/net/ethernet/cavium/liquidio/lio_main.c
+++ b/drivers/net/ethernet/cavium/liquidio/lio_main.c
@@ -1516,11 +1516,11 @@ static void free_netsgbuf(void *buf)
 
 	i = 1;
 	while (frags--) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];
 
 		pci_unmap_page((lio->oct_dev)->pci_dev,
 			       g->sg[(i >> 2)].ptr[(i & 3)],
-			       frag->size, DMA_TO_DEVICE);
+			       skb_frag_size(frag), DMA_TO_DEVICE);
 		i++;
 	}
 
@@ -1559,11 +1559,11 @@ static void free_netsgbuf_with_resp(void *buf)
 
 	i = 1;
 	while (frags--) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];
 
 		pci_unmap_page((lio->oct_dev)->pci_dev,
 			       g->sg[(i >> 2)].ptr[(i & 3)],
-			       frag->size, DMA_TO_DEVICE);
+			       skb_frag_size(frag), DMA_TO_DEVICE);
 		i++;
 	}
 
@@ -2427,7 +2427,7 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 
 	} else {
 		int i, frags;
-		struct skb_frag_struct *frag;
+		skb_frag_t *frag;
 		struct octnic_gather *g;
 
 		spin_lock(&lio->glist_lock[q_idx]);
@@ -2465,11 +2465,9 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 			frag = &skb_shinfo(skb)->frags[i - 1];
 
 			g->sg[(i >> 2)].ptr[(i & 3)] =
-				dma_map_page(&oct->pci_dev->dev,
-					     frag->page.p,
-					     frag->page_offset,
-					     frag->size,
-					     DMA_TO_DEVICE);
+				skb_frag_dma_map(&oct->pci_dev->dev,
+					         frag, 0, skb_frag_size(frag),
+						 DMA_TO_DEVICE);
 
 			if (dma_mapping_error(&oct->pci_dev->dev,
 					      g->sg[i >> 2].ptr[i & 3])) {
@@ -2481,7 +2479,7 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 					frag = &skb_shinfo(skb)->frags[j - 1];
 					dma_unmap_page(&oct->pci_dev->dev,
 						       g->sg[j >> 2].ptr[j & 3],
-						       frag->size,
+						       skb_frag_size(frag),
 						       DMA_TO_DEVICE);
 				}
 				dev_err(&oct->pci_dev->dev, "%s DMA mapping error 3\n",
@@ -2489,7 +2487,8 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 				return NETDEV_TX_BUSY;
 			}
 
-			add_sg_size(&g->sg[(i >> 2)], frag->size, (i & 3));
+			add_sg_size(&g->sg[(i >> 2)], skb_frag_size(frag),
+				    (i & 3));
 			i++;
 		}
 
diff --git a/drivers/net/ethernet/cavium/liquidio/lio_vf_main.c b/drivers/net/ethernet/cavium/liquidio/lio_vf_main.c
index 4ab9e99808b8..7567b9422e9d 100644
--- a/drivers/net/ethernet/cavium/liquidio/lio_vf_main.c
+++ b/drivers/net/ethernet/cavium/liquidio/lio_vf_main.c
@@ -859,11 +859,11 @@ static void free_netsgbuf(void *buf)
 
 	i = 1;
 	while (frags--) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];
 
 		pci_unmap_page((lio->oct_dev)->pci_dev,
 			       g->sg[(i >> 2)].ptr[(i & 3)],
-			       frag->size, DMA_TO_DEVICE);
+			       skb_frag_size(frag), DMA_TO_DEVICE);
 		i++;
 	}
 
@@ -903,11 +903,11 @@ static void free_netsgbuf_with_resp(void *buf)
 
 	i = 1;
 	while (frags--) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];
 
 		pci_unmap_page((lio->oct_dev)->pci_dev,
 			       g->sg[(i >> 2)].ptr[(i & 3)],
-			       frag->size, DMA_TO_DEVICE);
+			       skb_frag_size(frag), DMA_TO_DEVICE);
 		i++;
 	}
 
@@ -1501,7 +1501,7 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 		ndata.reqtype = REQTYPE_NORESP_NET;
 
 	} else {
-		struct skb_frag_struct *frag;
+		skb_frag_t *frag;
 		struct octnic_gather *g;
 		int i, frags;
 
@@ -1539,11 +1539,9 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 			frag = &skb_shinfo(skb)->frags[i - 1];
 
 			g->sg[(i >> 2)].ptr[(i & 3)] =
-				dma_map_page(&oct->pci_dev->dev,
-					     frag->page.p,
-					     frag->page_offset,
-					     frag->size,
-					     DMA_TO_DEVICE);
+				skb_frag_dma_map(&oct->pci_dev->dev,
+						 frag, 0, skb_frag_size(frag),
+						 DMA_TO_DEVICE);
 			if (dma_mapping_error(&oct->pci_dev->dev,
 					      g->sg[i >> 2].ptr[i & 3])) {
 				dma_unmap_single(&oct->pci_dev->dev,
@@ -1554,7 +1552,7 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 					frag = &skb_shinfo(skb)->frags[j - 1];
 					dma_unmap_page(&oct->pci_dev->dev,
 						       g->sg[j >> 2].ptr[j & 3],
-						       frag->size,
+						       skb_frag_size(frag),
 						       DMA_TO_DEVICE);
 				}
 				dev_err(&oct->pci_dev->dev, "%s DMA mapping error 3\n",
@@ -1562,7 +1560,8 @@ static int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)
 				return NETDEV_TX_BUSY;
 			}
 
-			add_sg_size(&g->sg[(i >> 2)], frag->size, (i & 3));
+			add_sg_size(&g->sg[(i >> 2)], skb_frag_size(frag),
+				    (i & 3));
 			i++;
 		}
 
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_queues.c
diff --git a/drivers/net/ethernet/chelsio/cxgb3/sge.c b/drivers/net/ethernet/chelsio/cxgb3/sge.c
index 36e0a1dcac88..61be2b023d10 100644
--- a/drivers/net/ethernet/chelsio/cxgb3/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/sge.c
@@ -2132,7 +2132,7 @@ static void lro_add_page(struct adapter *adap, struct sge_qset *qs,
 	struct port_info *pi = netdev_priv(qs->netdev);
 	struct sk_buff *skb = NULL;
 	struct cpl_rx_pkt *cpl;
-	struct skb_frag_struct *rx_frag;
+	skb_frag_t *rx_frag;
 	int nr_frags;
 	int offset = 0;
 
* Unmerged path drivers/net/ethernet/cortina/gemini.c
diff --git a/drivers/net/ethernet/emulex/benet/be_main.c b/drivers/net/ethernet/emulex/benet/be_main.c
index 93b7dcd8bfc8..ed16e4522fe8 100644
--- a/drivers/net/ethernet/emulex/benet/be_main.c
+++ b/drivers/net/ethernet/emulex/benet/be_main.c
@@ -1010,7 +1010,7 @@ static u32 be_xmit_enqueue(struct be_adapter *adapter, struct be_tx_obj *txo,
 	}
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 		len = skb_frag_size(frag);
 
 		busaddr = skb_frag_dma_map(dev, frag, 0, len, DMA_TO_DEVICE);
* Unmerged path drivers/net/ethernet/freescale/enetc/enetc.c
* Unmerged path drivers/net/ethernet/freescale/fec_main.c
* Unmerged path drivers/net/ethernet/hisilicon/hix5hd2_gmac.c
* Unmerged path drivers/net/ethernet/hisilicon/hns/hns_enet.c
* Unmerged path drivers/net/ethernet/hisilicon/hns3/hns3_enet.c
* Unmerged path drivers/net/ethernet/huawei/hinic/hinic_tx.c
diff --git a/drivers/net/ethernet/ibm/emac/core.c b/drivers/net/ethernet/ibm/emac/core.c
index 673fc13efc95..bcf37050f4d8 100644
--- a/drivers/net/ethernet/ibm/emac/core.c
+++ b/drivers/net/ethernet/ibm/emac/core.c
@@ -1486,7 +1486,7 @@ static int emac_start_xmit_sg(struct sk_buff *skb, struct net_device *ndev)
 				       ctrl);
 	/* skb fragments */
 	for (i = 0; i < nr_frags; ++i) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 		len = skb_frag_size(frag);
 
 		if (unlikely(dev->tx_cnt + mal_tx_chunks(len) >= NUM_TX_BUFF))
diff --git a/drivers/net/ethernet/intel/e1000/e1000_main.c b/drivers/net/ethernet/intel/e1000/e1000_main.c
index 96fa0fb87f1f..9b8bdf12ff07 100644
--- a/drivers/net/ethernet/intel/e1000/e1000_main.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_main.c
@@ -2928,9 +2928,8 @@ static int e1000_tx_map(struct e1000_adapter *adapter,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 
-		frag = &skb_shinfo(skb)->frags[f];
 		len = skb_frag_size(frag);
 		offset = 0;
 
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index c4cb91f7cf39..9142f4c0dbcb 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -5556,9 +5556,8 @@ static int e1000_tx_map(struct e1000_ring *tx_ring, struct sk_buff *skb,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 
-		frag = &skb_shinfo(skb)->frags[f];
 		len = skb_frag_size(frag);
 		offset = 0;
 
diff --git a/drivers/net/ethernet/intel/fm10k/fm10k_main.c b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
index 01377c92b0a2..1c86bfb52824 100644
--- a/drivers/net/ethernet/intel/fm10k/fm10k_main.c
+++ b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
@@ -929,7 +929,7 @@ static void fm10k_tx_map(struct fm10k_ring *tx_ring,
 	struct sk_buff *skb = first->skb;
 	struct fm10k_tx_buffer *tx_buffer;
 	struct fm10k_tx_desc *tx_desc;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	unsigned char *data;
 	dma_addr_t dma;
 	unsigned int data_len, size;
@@ -1057,7 +1057,8 @@ netdev_tx_t fm10k_xmit_frame_ring(struct sk_buff *skb,
 	 * otherwise try next time
 	 */
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
-		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
+		count += TXD_USE_COUNT(skb_frag_size(
+						&skb_shinfo(skb)->frags[f]));
 
 	if (fm10k_maybe_stop_tx(tx_ring, count + 3)) {
 		tx_ring->tx_stats.tx_busy++;
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 189cbf32ab16..6952e0a89ebc 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -3273,7 +3273,7 @@ int __i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size)
  **/
 bool __i40e_chk_linearize(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag, *stale;
+	const skb_frag_t *frag, *stale;
 	int nr_frags, sum;
 
 	/* no need to check if number of frags is less than 7 */
@@ -3360,7 +3360,7 @@ static inline int i40e_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 {
 	unsigned int data_len = skb->data_len;
 	unsigned int size = skb_headlen(skb);
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	struct i40e_tx_buffer *tx_bi;
 	struct i40e_tx_desc *tx_desc;
 	u16 i = tx_ring->next_to_use;
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.h b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 57c2e85fcef0..053fb0bf0897 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@ -523,7 +523,7 @@ static inline u32 i40e_get_head(struct i40e_ring *tx_ring)
  **/
 static inline int i40e_xmit_descriptor_count(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+	const skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
 	unsigned int nr_frags = skb_shinfo(skb)->nr_frags;
 	int count = 0, size = skb_headlen(skb);
 
diff --git a/drivers/net/ethernet/intel/iavf/iavf_txrx.c b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
index 7832eab247f7..58162ac73c54 100644
--- a/drivers/net/ethernet/intel/iavf/iavf_txrx.c
+++ b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
@@ -2154,7 +2154,7 @@ static void iavf_create_tx_ctx(struct iavf_ring *tx_ring,
  **/
 bool __iavf_chk_linearize(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag, *stale;
+	const skb_frag_t *frag, *stale;
 	int nr_frags, sum;
 
 	/* no need to check if number of frags is less than 7 */
@@ -2262,7 +2262,7 @@ static inline void iavf_tx_map(struct iavf_ring *tx_ring, struct sk_buff *skb,
 {
 	unsigned int data_len = skb->data_len;
 	unsigned int size = skb_headlen(skb);
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	struct iavf_tx_buffer *tx_bi;
 	struct iavf_tx_desc *tx_desc;
 	u16 i = tx_ring->next_to_use;
diff --git a/drivers/net/ethernet/intel/iavf/iavf_txrx.h b/drivers/net/ethernet/intel/iavf/iavf_txrx.h
index 80374bdfbb80..ac3ef21bee55 100644
--- a/drivers/net/ethernet/intel/iavf/iavf_txrx.h
+++ b/drivers/net/ethernet/intel/iavf/iavf_txrx.h
@@ -464,7 +464,7 @@ bool __iavf_chk_linearize(struct sk_buff *skb);
  **/
 static inline int iavf_xmit_descriptor_count(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+	const skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
 	unsigned int nr_frags = skb_shinfo(skb)->nr_frags;
 	int count = 0, size = skb_headlen(skb);
 
diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.c b/drivers/net/ethernet/intel/ice/ice_txrx.c
index c289d97f477d..e25a911b5908 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@ -1237,7 +1237,7 @@ ice_tx_map(struct ice_ring *tx_ring, struct ice_tx_buf *first,
 {
 	u64 td_offset, td_tag, td_cmd;
 	u16 i = tx_ring->next_to_use;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	unsigned int data_len, size;
 	struct ice_tx_desc *tx_desc;
 	struct ice_tx_buf *tx_buf;
@@ -1644,7 +1644,7 @@ static unsigned int ice_txd_use_count(unsigned int size)
  */
 static unsigned int ice_xmit_desc_count(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+	const skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
 	unsigned int nr_frags = skb_shinfo(skb)->nr_frags;
 	unsigned int count = 0, size = skb_headlen(skb);
 
@@ -1675,7 +1675,7 @@ static unsigned int ice_xmit_desc_count(struct sk_buff *skb)
  */
 static bool __ice_chk_linearize(struct sk_buff *skb)
 {
-	const struct skb_frag_struct *frag, *stale;
+	const skb_frag_t *frag, *stale;
 	int nr_frags, sum;
 
 	/* no need to check if number of frags is less than 7 */
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index 613f8603385e..e9c40ddac5af 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -5963,7 +5963,7 @@ static int igb_tx_map(struct igb_ring *tx_ring,
 	struct sk_buff *skb = first->skb;
 	struct igb_tx_buffer *tx_buffer;
 	union e1000_adv_tx_desc *tx_desc;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	dma_addr_t dma;
 	unsigned int data_len, size;
 	u32 tx_flags = first->tx_flags;
@@ -6124,7 +6124,8 @@ netdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,
 	 * otherwise try next time
 	 */
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
-		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
+		count += TXD_USE_COUNT(skb_frag_size(
+						&skb_shinfo(skb)->frags[f]));
 
 	if (igb_maybe_stop_tx(tx_ring, count + 3)) {
 		/* this is a hard error */
diff --git a/drivers/net/ethernet/intel/igbvf/netdev.c b/drivers/net/ethernet/intel/igbvf/netdev.c
index d15f77156e88..a4c69bfba971 100644
--- a/drivers/net/ethernet/intel/igbvf/netdev.c
+++ b/drivers/net/ethernet/intel/igbvf/netdev.c
@@ -2174,7 +2174,7 @@ static inline int igbvf_tx_map_adv(struct igbvf_adapter *adapter,
 		goto dma_error;
 
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag;
 
 		count++;
 		i++;
diff --git a/drivers/net/ethernet/intel/igc/igc_main.c b/drivers/net/ethernet/intel/igc/igc_main.c
index a1f9b5520ec8..569aedac3996 100644
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@ -843,7 +843,7 @@ static int igc_tx_map(struct igc_ring *tx_ring,
 	struct igc_tx_buffer *tx_buffer;
 	union igc_adv_tx_desc *tx_desc;
 	u32 tx_flags = first->tx_flags;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	u16 i = tx_ring->next_to_use;
 	unsigned int data_len, size;
 	dma_addr_t dma;
@@ -1002,7 +1002,8 @@ static netdev_tx_t igc_xmit_frame_ring(struct sk_buff *skb,
 	 * otherwise try next time
 	 */
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
-		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
+		count += TXD_USE_COUNT(skb_frag_size(
+						&skb_shinfo(skb)->frags[f]));
 
 	if (igc_maybe_stop_tx(tx_ring, count + 3)) {
 		/* this is a hard error */
diff --git a/drivers/net/ethernet/intel/ixgb/ixgb_main.c b/drivers/net/ethernet/intel/ixgb/ixgb_main.c
index dd42930f2b56..1f674a0ab085 100644
--- a/drivers/net/ethernet/intel/ixgb/ixgb_main.c
+++ b/drivers/net/ethernet/intel/ixgb/ixgb_main.c
@@ -1371,9 +1371,7 @@ ixgb_tx_map(struct ixgb_adapter *adapter, struct sk_buff *skb,
 	}
 
 	for (f = 0; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag;
-
-		frag = &skb_shinfo(skb)->frags[f];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 		len = skb_frag_size(frag);
 		offset = 0;
 
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 1503690521e8..790e1ce4a00a 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -1835,7 +1835,7 @@ static bool ixgbe_is_non_eop(struct ixgbe_ring *rx_ring,
 static void ixgbe_pull_tail(struct ixgbe_ring *rx_ring,
 			    struct sk_buff *skb)
 {
-	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
 	unsigned char *va;
 	unsigned int pull_len;
 
@@ -1894,7 +1894,7 @@ static void ixgbe_dma_sync_frag(struct ixgbe_ring *rx_ring,
 					      skb_headlen(skb),
 					      DMA_FROM_DEVICE);
 	} else {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
 
 		dma_sync_single_range_for_cpu(rx_ring->dev,
 					      IXGBE_CB(skb)->dma,
@@ -8237,7 +8237,7 @@ static int ixgbe_tx_map(struct ixgbe_ring *tx_ring,
 	struct sk_buff *skb = first->skb;
 	struct ixgbe_tx_buffer *tx_buffer;
 	union ixgbe_adv_tx_desc *tx_desc;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	dma_addr_t dma;
 	unsigned int data_len, size;
 	u32 tx_flags = first->tx_flags;
@@ -8666,7 +8666,8 @@ netdev_tx_t ixgbe_xmit_frame_ring(struct sk_buff *skb,
 	 * otherwise try next time
 	 */
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
-		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
+		count += TXD_USE_COUNT(skb_frag_size(
+						&skb_shinfo(skb)->frags[f]));
 
 	if (ixgbe_maybe_stop_tx(tx_ring, count + 3)) {
 		tx_ring->tx_stats.tx_busy++;
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 6c8e96b83521..90bc16c5eefc 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -3974,7 +3974,7 @@ static void ixgbevf_tx_map(struct ixgbevf_ring *tx_ring,
 	struct sk_buff *skb = first->skb;
 	struct ixgbevf_tx_buffer *tx_buffer;
 	union ixgbe_adv_tx_desc *tx_desc;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	dma_addr_t dma;
 	unsigned int data_len, size;
 	u32 tx_flags = first->tx_flags;
* Unmerged path drivers/net/ethernet/jme.c
diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index bcf995e5c9b0..10b52a075e5f 100644
--- a/drivers/net/ethernet/marvell/mvneta.c
+++ b/drivers/net/ethernet/marvell/mvneta.c
@@ -1432,10 +1432,10 @@ static int mvneta_tx_frag_process(struct mvneta_port *pp, struct sk_buff *skb,
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
 		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
-		void *addr = page_address(frag->page.p) + frag->page_offset;
+		void *addr = skb_frag_address(frag);
 
 		tx_desc = mvneta_txq_next_desc_get(txq);
-		tx_desc->data_size = frag->size;
+		tx_desc->data_size = skb_frag_size(frag);
 
 		tx_desc->buf_phys_addr =
 			dma_map_single(pp->dev->dev.parent, addr,
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
* Unmerged path drivers/net/ethernet/mediatek/mtk_eth_soc.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_tx.c b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
index 332b22052c6b..c7cd6143e6c1 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@ -775,9 +775,7 @@ static bool mlx4_en_build_dma_wqe(struct mlx4_en_priv *priv,
 
 	/* Map fragments if any */
 	for (i_frag = shinfo->nr_frags - 1; i_frag >= 0; i_frag--) {
-		const struct skb_frag_struct *frag;
-
-		frag = &shinfo->frags[i_frag];
+		const skb_frag_t *frag = &shinfo->frags[i_frag];
 		byte_count = skb_frag_size(frag);
 		dma = skb_frag_dma_map(ddev, frag,
 				       0, byte_count,
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index e310a0de21c4..6356607f3a96 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -263,7 +263,7 @@ mlx5e_txwqe_build_dsegs(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 	}
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 		int fsz = skb_frag_size(frag);
 
 		dma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,
* Unmerged path drivers/net/ethernet/microchip/lan743x_main.c
diff --git a/drivers/net/ethernet/myricom/myri10ge/myri10ge.c b/drivers/net/ethernet/myricom/myri10ge/myri10ge.c
index eb1a6c95ce8d..4eefad3df23d 100644
--- a/drivers/net/ethernet/myricom/myri10ge/myri10ge.c
+++ b/drivers/net/ethernet/myricom/myri10ge/myri10ge.c
@@ -1392,7 +1392,7 @@ myri10ge_vlan_rx(struct net_device *dev, void *addr, struct sk_buff *skb)
 {
 	u8 *va;
 	struct vlan_ethhdr *veh;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	__wsum vsum;
 
 	va = addr;
@@ -1424,7 +1424,7 @@ myri10ge_rx_done(struct myri10ge_slice_state *ss, int len, __wsum csum)
 {
 	struct myri10ge_priv *mgp = ss->mgp;
 	struct sk_buff *skb;
-	struct skb_frag_struct *rx_frags;
+	skb_frag_t *rx_frags;
 	struct myri10ge_rx_buf *rx;
 	int i, idx, remainder, bytes;
 	struct pci_dev *pdev = mgp->pdev;
@@ -1466,7 +1466,7 @@ myri10ge_rx_done(struct myri10ge_slice_state *ss, int len, __wsum csum)
 		return 0;
 	}
 	rx_frags = skb_shinfo(skb)->frags;
-	/* Fill skb_frag_struct(s) with data from our receive */
+	/* Fill skb_frag_t(s) with data from our receive */
 	for (i = 0, remainder = len; remainder > 0; i++) {
 		myri10ge_unmap_rx_page(pdev, &rx->info[idx], bytes);
 		skb_fill_page_desc(skb, i, rx->info[idx].page,
@@ -1480,7 +1480,7 @@ myri10ge_rx_done(struct myri10ge_slice_state *ss, int len, __wsum csum)
 
 	/* remove padding */
 	rx_frags[0].page_offset += MXGEFW_PAD;
-	rx_frags[0].size -= MXGEFW_PAD;
+	skb_frag_size_sub(&rx_frags[0], MXGEFW_PAD);
 	len -= MXGEFW_PAD;
 
 	skb->len = len;
@@ -2826,7 +2826,7 @@ static netdev_tx_t myri10ge_xmit(struct sk_buff *skb,
 	struct myri10ge_slice_state *ss;
 	struct mcp_kreq_ether_send *req;
 	struct myri10ge_tx_buf *tx;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	struct netdev_queue *netdev_queue;
 	dma_addr_t bus;
 	u32 low;
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 51dab2dc7c45..d93a693fb8ed 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@ -804,7 +804,7 @@ static int nfp_net_prep_port_id(struct sk_buff *skb)
 static int nfp_net_tx(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct nfp_net *nn = netdev_priv(netdev);
-	const struct skb_frag_struct *frag;
+	const skb_frag_t *frag;
 	int f, nr_frags, wr_idx, md_bytes;
 	struct nfp_net_tx_ring *tx_ring;
 	struct nfp_net_r_vector *r_vec;
@@ -978,7 +978,7 @@ static void nfp_net_tx_complete(struct nfp_net_tx_ring *tx_ring, int budget)
 	todo = D_IDX(tx_ring, qcp_rd_p - tx_ring->qcp_rd_p);
 
 	while (todo--) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag;
 		struct nfp_net_tx_buf *tx_buf;
 		struct sk_buff *skb;
 		int fidx, nr_frags;
@@ -1093,7 +1093,7 @@ static bool nfp_net_xdp_complete(struct nfp_net_tx_ring *tx_ring)
 static void
 nfp_net_tx_ring_reset(struct nfp_net_dp *dp, struct nfp_net_tx_ring *tx_ring)
 {
-	const struct skb_frag_struct *frag;
+	const skb_frag_t *frag;
 	struct netdev_queue *nd_q;
 
 	while (!tx_ring->is_xdp && tx_ring->rd_p != tx_ring->wr_p) {
diff --git a/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c b/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
index 1a81a12910c5..d040ff79c67a 100644
--- a/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
+++ b/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
@@ -2002,7 +2002,7 @@ netxen_map_tx_skb(struct pci_dev *pdev,
 		struct sk_buff *skb, struct netxen_cmd_buffer *pbuf)
 {
 	struct netxen_skb_frag *nf;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	int i, nr_frags;
 	dma_addr_t map;
 
@@ -2065,7 +2065,7 @@ netxen_nic_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
 	struct pci_dev *pdev;
 	int i, k;
 	int delta = 0;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 
 	u32 producer;
 	int frag_count;
diff --git a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
index 424613219ddd..f4afaa21d28d 100644
--- a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
+++ b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
@@ -579,7 +579,7 @@ static int qlcnic_map_tx_skb(struct pci_dev *pdev, struct sk_buff *skb,
 			     struct qlcnic_cmd_buffer *pbuf)
 {
 	struct qlcnic_skb_frag *nf;
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	int i, nr_frags;
 	dma_addr_t map;
 
* Unmerged path drivers/net/ethernet/qualcomm/emac/emac-mac.c
* Unmerged path drivers/net/ethernet/synopsys/dwc-xlgmac-desc.c
* Unmerged path drivers/net/ethernet/synopsys/dwc-xlgmac-net.c
diff --git a/drivers/net/ethernet/tehuti/tehuti.c b/drivers/net/ethernet/tehuti/tehuti.c
index 1fb55c854f26..d8870091fbaf 100644
--- a/drivers/net/ethernet/tehuti/tehuti.c
+++ b/drivers/net/ethernet/tehuti/tehuti.c
@@ -1515,7 +1515,7 @@ bdx_tx_map_skb(struct bdx_priv *priv, struct sk_buff *skb,
 	bdx_tx_db_inc_wptr(db);
 
 	for (i = 0; i < nr_frags; i++) {
-		const struct skb_frag_struct *frag;
+		const skb_frag_t *frag;
 
 		frag = &skb_shinfo(skb)->frags[i];
 		db->wptr->len = skb_frag_size(frag);
diff --git a/drivers/net/usb/usbnet.c b/drivers/net/usb/usbnet.c
index a87f045f0fb8..2deb9594db6d 100644
--- a/drivers/net/usb/usbnet.c
+++ b/drivers/net/usb/usbnet.c
@@ -1337,10 +1337,10 @@ static int build_dma_sg(const struct sk_buff *skb, struct urb *urb)
 	total_len += skb_headlen(skb);
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		struct skb_frag_struct *f = &skb_shinfo(skb)->frags[i];
+		skb_frag_t *f = &skb_shinfo(skb)->frags[i];
 
 		total_len += skb_frag_size(f);
-		sg_set_page(&urb->sg[i + s], f->page.p, f->size,
+		sg_set_page(&urb->sg[i + s], skb_frag_page(f), skb_frag_size(f),
 				f->page_offset);
 	}
 	urb->transfer_buffer_length = total_len;
diff --git a/drivers/net/vmxnet3/vmxnet3_drv.c b/drivers/net/vmxnet3/vmxnet3_drv.c
index 8a4b65781988..f7bca015fb35 100644
--- a/drivers/net/vmxnet3/vmxnet3_drv.c
+++ b/drivers/net/vmxnet3/vmxnet3_drv.c
@@ -657,8 +657,7 @@ static void
 vmxnet3_append_frag(struct sk_buff *skb, struct Vmxnet3_RxCompDesc *rcd,
 		    struct vmxnet3_rx_buf_info *rbi)
 {
-	struct skb_frag_struct *frag = skb_shinfo(skb)->frags +
-		skb_shinfo(skb)->nr_frags;
+	skb_frag_t *frag = skb_shinfo(skb)->frags + skb_shinfo(skb)->nr_frags;
 
 	BUG_ON(skb_shinfo(skb)->nr_frags >= MAX_SKB_FRAGS);
 
@@ -755,7 +754,7 @@ vmxnet3_map_pkt(struct sk_buff *skb, struct vmxnet3_tx_ctx *ctx,
 	}
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 		u32 buf_size;
 
 		buf_offset = 0;
@@ -954,7 +953,7 @@ static int txd_estimate(const struct sk_buff *skb)
 	int i;
 
 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 
 		count += VMXNET3_TXD_NEEDED(skb_frag_size(frag));
 	}
diff --git a/drivers/net/wireless/ath/wil6210/debugfs.c b/drivers/net/wireless/ath/wil6210/debugfs.c
index 6db00c167d2e..4faba76d06bc 100644
--- a/drivers/net/wireless/ath/wil6210/debugfs.c
+++ b/drivers/net/wireless/ath/wil6210/debugfs.c
@@ -878,8 +878,7 @@ static void wil_seq_print_skb(struct seq_file *s, struct sk_buff *skb)
 	if (nr_frags) {
 		seq_printf(s, "    nr_frags = %d\n", nr_frags);
 		for (i = 0; i < nr_frags; i++) {
-			const struct skb_frag_struct *frag =
-					&skb_shinfo(skb)->frags[i];
+			const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 
 			len = skb_frag_size(frag);
 			p = skb_frag_address_safe(frag);
diff --git a/drivers/net/wireless/ath/wil6210/txrx.c b/drivers/net/wireless/ath/wil6210/txrx.c
index 389c718cd257..14fe9f6da60e 100644
--- a/drivers/net/wireless/ath/wil6210/txrx.c
+++ b/drivers/net/wireless/ath/wil6210/txrx.c
@@ -1538,7 +1538,7 @@ static int __wil_tx_vring_tso(struct wil6210_priv *wil, struct vring *vring,
 				     len);
 		} else {
 			frag = &skb_shinfo(skb)->frags[f];
-			len = frag->size;
+			len = skb_frag_size(frag);
 			wil_dbg_txrx(wil, "TSO: frag[%d]: len %u\n", f, len);
 		}
 
@@ -1559,8 +1559,8 @@ static int __wil_tx_vring_tso(struct wil6210_priv *wil, struct vring *vring,
 
 			if (!headlen) {
 				pa = skb_frag_dma_map(dev, frag,
-						      frag->size - len, lenmss,
-						      DMA_TO_DEVICE);
+						      skb_frag_size(frag) - len,
+						      lenmss, DMA_TO_DEVICE);
 				vring->ctx[i].mapped_as = wil_mapped_as_page;
 			} else {
 				pa = dma_map_single(dev,
@@ -1771,8 +1771,7 @@ static int __wil_tx_vring(struct wil6210_priv *wil, struct vring *vring,
 
 	/* middle segments */
 	for (; f < nr_frags; f++) {
-		const struct skb_frag_struct *frag =
-				&skb_shinfo(skb)->frags[f];
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 		int len = skb_frag_size(frag);
 
 		*_d = *d;
* Unmerged path drivers/net/wireless/ath/wil6210/txrx_edma.c
* Unmerged path drivers/net/xen-netback/netback.c
* Unmerged path drivers/s390/net/qeth_core_main.c
diff --git a/drivers/scsi/fcoe/fcoe_transport.c b/drivers/scsi/fcoe/fcoe_transport.c
index 9171a19ca891..ab0854c5eb8f 100644
--- a/drivers/scsi/fcoe/fcoe_transport.c
+++ b/drivers/scsi/fcoe/fcoe_transport.c
@@ -315,7 +315,7 @@ EXPORT_SYMBOL_GPL(fcoe_get_wwn);
 u32 fcoe_fc_crc(struct fc_frame *fp)
 {
 	struct sk_buff *skb = fp_skb(fp);
-	struct skb_frag_struct *frag;
+	skb_frag_t *frag;
 	unsigned char *data;
 	unsigned long off, len, clen;
 	u32 crc;
* Unmerged path drivers/staging/et131x/et131x.c
* Unmerged path drivers/staging/octeon/ethernet-tx.c
* Unmerged path drivers/staging/unisys/visornic/visornic_main.c
diff --git a/drivers/target/iscsi/cxgbit/cxgbit_target.c b/drivers/target/iscsi/cxgbit/cxgbit_target.c
index 2b88d3e850f2..e8c180b5be3c 100644
--- a/drivers/target/iscsi/cxgbit/cxgbit_target.c
+++ b/drivers/target/iscsi/cxgbit/cxgbit_target.c
@@ -932,9 +932,9 @@ cxgbit_handle_immediate_data(struct iscsi_cmd *cmd, struct iscsi_scsi_req *hdr,
 		skb_frag_t *dfrag = &ssi->frags[pdu_cb->dfrag_idx];
 
 		sg_init_table(&ccmd->sg, 1);
-		sg_set_page(&ccmd->sg, dfrag->page.p, skb_frag_size(dfrag),
-			    dfrag->page_offset);
-		get_page(dfrag->page.p);
+		sg_set_page(&ccmd->sg, skb_frag_page(dfrag),
+				skb_frag_size(dfrag), dfrag->page_offset);
+		get_page(skb_frag_page(dfrag));
 
 		cmd->se_cmd.t_data_sg = &ccmd->sg;
 		cmd->se_cmd.t_data_nents = 1;
@@ -1436,7 +1436,8 @@ static void cxgbit_lro_skb_dump(struct sk_buff *skb)
 			pdu_cb->ddigest, pdu_cb->frags);
 	for (i = 0; i < ssi->nr_frags; i++)
 		pr_info("skb 0x%p, frag %d, off %u, sz %u.\n",
-			skb, i, ssi->frags[i].page_offset, ssi->frags[i].size);
+			skb, i, ssi->frags[i].page_offset,
+			skb_frag_size(&ssi->frags[i]));
 }
 
 static void cxgbit_lro_hskb_reset(struct cxgbit_sock *csk)
@@ -1480,7 +1481,7 @@ cxgbit_lro_skb_merge(struct cxgbit_sock *csk, struct sk_buff *skb, u8 pdu_idx)
 		hpdu_cb->frags++;
 		hpdu_cb->hfrag_idx = hfrag_idx;
 
-		len = hssi->frags[hfrag_idx].size;
+		len = skb_frag_size(&hssi->frags[hfrag_idx]);;
 		hskb->len += len;
 		hskb->data_len += len;
 		hskb->truesize += len;
@@ -1500,7 +1501,7 @@ cxgbit_lro_skb_merge(struct cxgbit_sock *csk, struct sk_buff *skb, u8 pdu_idx)
 
 			get_page(skb_frag_page(&hssi->frags[dfrag_idx]));
 
-			len += hssi->frags[dfrag_idx].size;
+			len += skb_frag_size(&hssi->frags[dfrag_idx]);
 
 			hssi->nr_frags++;
 			hpdu_cb->frags++;
