x86/pkeys: Do not special case protection key 0

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 2fa9d1cfaf0e02f8abef0757002bff12dfcfa4e6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/2fa9d1cf.failed

mm_pkey_is_allocated() treats pkey 0 as unallocated.  That is
inconsistent with the manpages, and also inconsistent with
mm->context.pkey_allocation_map.  Stop special casing it and only
disallow values that are actually bad (< 0).

The end-user visible effect of this is that you can now use
mprotect_pkey() to set pkey=0.

This is a bit nicer than what Ram proposed[1] because it is simpler
and removes special-casing for pkey 0.  On the other hand, it does
allow applications to pkey_free() pkey-0, but that's just a silly
thing to do, so we are not going to protect against it.

The scenario that could happen is similar to what happens if you free
any other pkey that is in use: it might get reallocated later and used
to protect some other data.  The most likely scenario is that pkey-0
comes back from pkey_alloc(), an access-disable or write-disable bit
is set in PKRU for it, and the next stack access will SIGSEGV.  It's
not horribly different from if you mprotect()'d your stack or heap to
be unreadable or unwritable, which is generally very foolish, but also
not explicitly prevented by the kernel.

1. http://lkml.kernel.org/r/1522112702-27853-1-git-send-email-linuxram@us.ibm.com

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>p
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Michael Ellermen <mpe@ellerman.id.au>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ram Pai <linuxram@us.ibm.com>
	Cc: Shuah Khan <shuah@kernel.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-mm@kvack.org
	Cc: stable@vger.kernel.org
Fixes: 58ab9a088dda ("x86/pkeys: Check against max pkey to avoid overflows")
Link: http://lkml.kernel.org/r/20180509171358.47FD785E@viggo.jf.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 2fa9d1cfaf0e02f8abef0757002bff12dfcfa4e6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mmu_context.h
diff --cc arch/x86/include/asm/mmu_context.h
index 2533f1ea6556,cf9911b5a53c..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -54,46 -123,84 +54,51 @@@ static inline void load_mm_ldt(struct m
  	 * that we can see.
  	 */
  
 -	if (unlikely(ldt)) {
 -		if (static_cpu_has(X86_FEATURE_PTI)) {
 -			if (WARN_ON_ONCE((unsigned long)ldt->slot > 1)) {
 -				/*
 -				 * Whoops -- either the new LDT isn't mapped
 -				 * (if slot == -1) or is mapped into a bogus
 -				 * slot (if slot > 1).
 -				 */
 -				clear_LDT();
 -				return;
 -			}
 -
 -			/*
 -			 * If page table isolation is enabled, ldt->entries
 -			 * will not be mapped in the userspace pagetables.
 -			 * Tell the CPU to access the LDT through the alias
 -			 * at ldt_slot_va(ldt->slot).
 -			 */
 -			set_ldt(ldt_slot_va(ldt->slot), ldt->nr_entries);
 -		} else {
 -			set_ldt(ldt->entries, ldt->nr_entries);
 -		}
 -	} else {
 +	if (unlikely(ldt))
 +		set_ldt(ldt->entries, ldt->size);
 +	else
  		clear_LDT();
 -	}
 -#else
 -	clear_LDT();
 -#endif
 +
 +	DEBUG_LOCKS_WARN_ON(preemptible());
  }
  
 -static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)
 +/*
 + * Used for LDT copy/destruction.
 + */
 +int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 +void destroy_context_ldt(struct mm_struct *mm);
 +
 +
 +static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
  {
 -#ifdef CONFIG_MODIFY_LDT_SYSCALL
 -	/*
 -	 * Load the LDT if either the old or new mm had an LDT.
 -	 *
 -	 * An mm will never go from having an LDT to not having an LDT.  Two
 -	 * mms never share an LDT, so we don't gain anything by checking to
 -	 * see whether the LDT changed.  There's also no guarantee that
 -	 * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,
 -	 * then prev->context.ldt will also be non-NULL.
 -	 *
 -	 * If we really cared, we could optimize the case where prev == next
 -	 * and we're exiting lazy mode.  Most of the time, if this happens,
 -	 * we don't actually need to reload LDTR, but modify_ldt() is mostly
 -	 * used by legacy code and emulators where we don't need this level of
 -	 * performance.
 -	 *
 -	 * This uses | instead of || because it generates better code.
 -	 */
 -	if (unlikely((unsigned long)prev->context.ldt |
 -		     (unsigned long)next->context.ldt))
 -		load_mm_ldt(next);
 +#ifdef CONFIG_SMP
 +	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)
 +		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);
  #endif
 -
 -	DEBUG_LOCKS_WARN_ON(preemptible());
  }
  
 -void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);
 +static inline void load_cr3(pgd_t *pgdir)
 +{
 +	__load_cr3(__sme_pa(pgdir));
 +}
  
  static inline int init_new_context(struct task_struct *tsk,
 -				   struct mm_struct *mm)
 +				       struct mm_struct *mm)
  {
 -	mutex_init(&mm->context.lock);
 -
 -	mm->context.ctx_id = atomic64_inc_return(&last_mm_ctx_id);
 -	atomic64_set(&mm->context.tlb_gen, 0);
 -
 -#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
 +	#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
  	if (cpu_feature_enabled(X86_FEATURE_OSPKE)) {
++<<<<<<< HEAD
 +		/* pkey 0 is the default and always allocated */
 +		mm->pkey_allocation_map = 0x1;
++=======
+ 		/* pkey 0 is the default and allocated implicitly */
+ 		mm->context.pkey_allocation_map = 0x1;
++>>>>>>> 2fa9d1cfaf0e (x86/pkeys: Do not special case protection key 0)
  		/* -1 means unallocated or invalid */
 -		mm->context.execute_only_pkey = -1;
 +		mm->execute_only_pkey = -1;
  	}
 -#endif
 -	init_new_context_ldt(mm);
 -	return 0;
 +	#endif
 +	return init_new_context_ldt(tsk, mm);
  }
  static inline void destroy_context(struct mm_struct *mm)
  {
* Unmerged path arch/x86/include/asm/mmu_context.h
diff --git a/arch/x86/include/asm/pkeys.h b/arch/x86/include/asm/pkeys.h
index c4516718b94f..7edd87c0cf23 100644
--- a/arch/x86/include/asm/pkeys.h
+++ b/arch/x86/include/asm/pkeys.h
@@ -50,10 +50,10 @@ bool mm_pkey_is_allocated(struct mm_struct *mm, int pkey)
 {
 	/*
 	 * "Allocated" pkeys are those that have been returned
-	 * from pkey_alloc().  pkey 0 is special, and never
-	 * returned from pkey_alloc().
+	 * from pkey_alloc() or pkey 0 which is allocated
+	 * implicitly when the mm is created.
 	 */
-	if (pkey <= 0)
+	if (pkey < 0)
 		return false;
 	if (pkey >= arch_max_pkey())
 		return false;
