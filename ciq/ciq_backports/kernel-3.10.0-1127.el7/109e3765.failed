md: add block tracing for bio_remapping

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author NeilBrown <neilb@suse.com>
commit 109e37653033a5fcd3bf8cab0ed6a7ff433f758a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/109e3765.failed

The block tracing infrastructure (accessed with blktrace/blkparse)
supports the tracing of mapping bios from one device to another.
This is currently used when a bio in a partition is mapped to the
whole device, when bios are mapped by dm, and for mapping in md/raid5.
Other md personalities do not include this tracing yet, so add it.

When a read-error is detected we redirect the request to a different device.
This could justifiably be seen as a new mapping for the originial bio,
or a secondary mapping for the bio that errors.  This patch uses
the second option.

When md is used under dm-raid, the mappings are not traced as we do
not have access to the block device number of the parent.

	Signed-off-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 109e37653033a5fcd3bf8cab0ed6a7ff433f758a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/md-linear.c
#	drivers/md/raid0.c
#	drivers/md/raid1.c
#	drivers/md/raid10.c
diff --cc drivers/md/md-linear.c
index e15de8e23224,5975c9915684..000000000000
--- a/drivers/md/md-linear.c
+++ b/drivers/md/md-linear.c
@@@ -21,8 -21,9 +21,9 @@@
  #include <linux/seq_file.h>
  #include <linux/module.h>
  #include <linux/slab.h>
+ #include <trace/events/block.h>
  #include "md.h"
 -#include "linear.h"
 +#include "md-linear.h"
  
  /*
   * find which device holds a particular offset
@@@ -286,79 -215,66 +287,119 @@@ static void linear_free(struct mddev *m
  	kfree(conf);
  }
  
 -static void linear_make_request(struct mddev *mddev, struct bio *bio)
 +static bool linear_make_request(struct mddev *mddev, struct bio *bio)
  {
 -	char b[BDEVNAME_SIZE];
  	struct dev_info *tmp_dev;
 -	struct bio *split;
 -	sector_t start_sector, end_sector, data_offset;
 -
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 +	sector_t start_sector;
 +	unsigned int max_sectors = blk_queue_get_max_sectors(mddev->queue,
 +			bio->bi_rw);
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
 +
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
  		md_flush_request(mddev, bio);
 -		return;
 +		return true;
  	}
  
++<<<<<<< HEAD:drivers/md/md-linear.c
 +	if (!do_discard && !do_same && bio_sectors(bio) > max_sectors) {
 +		struct bio_pair2 *bp = bio_split2(bio, max_sectors);
 +		if (!bp) {
 +			bio_io_error(bio);
 +			return true;
++=======
+ 	do {
+ 		sector_t bio_sector = bio->bi_iter.bi_sector;
+ 		tmp_dev = which_dev(mddev, bio_sector);
+ 		start_sector = tmp_dev->end_sector - tmp_dev->rdev->sectors;
+ 		end_sector = tmp_dev->end_sector;
+ 		data_offset = tmp_dev->rdev->data_offset;
+ 		bio->bi_bdev = tmp_dev->rdev->bdev;
+ 
+ 		if (unlikely(bio_sector >= end_sector ||
+ 			     bio_sector < start_sector))
+ 			goto out_of_bounds;
+ 
+ 		if (unlikely(bio_end_sector(bio) > end_sector)) {
+ 			/* This bio crosses a device boundary, so we have to
+ 			 * split it.
+ 			 */
+ 			split = bio_split(bio, end_sector - bio_sector,
+ 					  GFP_NOIO, fs_bio_set);
+ 			bio_chain(split, bio);
+ 		} else {
+ 			split = bio;
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping):drivers/md/linear.c
  		}
  
 -		split->bi_iter.bi_sector = split->bi_iter.bi_sector -
 -			start_sector + data_offset;
 +		generic_make_request(bp->bio1);
 +		generic_make_request(bp->bio2);
 +		bio_pair2_release(bp);
 +		return true;
 +	}
  
++<<<<<<< HEAD:drivers/md/md-linear.c
 +	tmp_dev = which_dev(mddev, bio->bi_sector);
 +	start_sector = tmp_dev->end_sector - tmp_dev->rdev->sectors;
++=======
+ 		if (unlikely((bio_op(split) == REQ_OP_DISCARD) &&
+ 			 !blk_queue_discard(bdev_get_queue(split->bi_bdev)))) {
+ 			/* Just ignore it */
+ 			bio_endio(split);
+ 		} else {
+ 			if (mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(split->bi_bdev),
+ 						      split, disk_devt(mddev->gendisk),
+ 						      bio_sector);
+ 			generic_make_request(split);
+ 		}
+ 	} while (split != bio);
+ 	return;
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping):drivers/md/linear.c
 +
 +	if (unlikely(bio->bi_sector >= (tmp_dev->end_sector)
 +		     || (bio->bi_sector < start_sector))) {
 +		char b[BDEVNAME_SIZE];
 +
 +		pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
 +		       mdname(mddev),
 +		       (unsigned long long)bio->bi_sector,
 +		       bdevname(tmp_dev->rdev->bdev, b),
 +		       (unsigned long long)tmp_dev->rdev->sectors,
 +		       (unsigned long long)start_sector);
 +		bio_io_error(bio);
 +		return true;
 +	}
 +	if (unlikely(bio_end_sector(bio) > tmp_dev->end_sector)) {
 +		/* This bio crosses a device boundary, so we have to
 +		 * split it.
 +		 */
 +		struct bio_pair *bp;
 +		sector_t end_sector = tmp_dev->end_sector;
 +
 +		bp = bio_split(bio, end_sector - bio->bi_sector);
 +
 +		linear_make_request(mddev, &bp->bio1);
 +		linear_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
 +	}
 +		    
 +	bio->bi_bdev = tmp_dev->rdev->bdev;
 +	bio->bi_sector = bio->bi_sector - start_sector
 +		+ tmp_dev->rdev->data_offset;
 +
 +	if (unlikely((bio->bi_rw & REQ_DISCARD) &&
 +		     !blk_queue_discard(bdev_get_queue(bio->bi_bdev)))) {
 +		/* Just ignore it */
 +		bio_endio(bio, 0);
 +		return true;
 +	}
  
 -out_of_bounds:
 -	pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
 -	       mdname(mddev),
 -	       (unsigned long long)bio->bi_iter.bi_sector,
 -	       bdevname(tmp_dev->rdev->bdev, b),
 -	       (unsigned long long)tmp_dev->rdev->sectors,
 -	       (unsigned long long)start_sector);
 -	bio_io_error(bio);
 +	mddev_check_writesame(mddev, bio);
 +	generic_make_request(bio);
 +	return true;
  }
  
  static void linear_status (struct seq_file *seq, struct mddev *mddev)
diff --cc drivers/md/raid0.c
index 0bfe93a75836,e628f187e5ad..000000000000
--- a/drivers/md/raid0.c
+++ b/drivers/md/raid0.c
@@@ -513,237 -452,55 +514,264 @@@ static inline int is_io_in_chunk_bounda
  	}
  }
  
 -static void raid0_make_request(struct mddev *mddev, struct bio *bio)
 +static struct bio *next_bio(struct bio *bio, int rw, unsigned int nr_pages,
 +			   gfp_t gfp)
  {
 -	struct strip_zone *zone;
 -	struct md_rdev *tmp_dev;
 -	struct bio *split;
 +	struct bio *new = bio_alloc(gfp, nr_pages);
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 -		md_flush_request(mddev, bio);
 -		return;
 +	if (bio) {
 +		bio_chain(bio, new);
 +		submit_bio(rw, bio);
  	}
  
++<<<<<<< HEAD
 +	return new;
 +}
++=======
+ 	do {
+ 		sector_t bio_sector = bio->bi_iter.bi_sector;
+ 		sector_t sector = bio_sector;
+ 		unsigned chunk_sects = mddev->chunk_sectors;
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  
 -		unsigned sectors = chunk_sects -
 -			(likely(is_power_of_2(chunk_sects))
 -			 ? (sector & (chunk_sects-1))
 -			 : sector_div(sector, chunk_sects));
 -
 +static int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 +				 sector_t nr_sects, gfp_t gfp_mask, int type,
 +				 struct bio **biop)
 +{
 +	struct request_queue *q = bdev_get_queue(bdev);
 +	struct bio *bio = *biop;
 +	unsigned int max_discard_sectors, granularity;
 +	int alignment;
 +
++<<<<<<< HEAD
 +	if (!q)
 +		return -ENXIO;
 +	if (!blk_queue_discard(q))
 +		return -EOPNOTSUPP;
 +	if ((type & REQ_SECURE) && !blk_queue_secdiscard(q))
 +		return -EOPNOTSUPP;
++=======
+ 		/* Restore due to sector_div */
+ 		sector = bio_sector;
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  
 -		if (sectors < bio_sectors(bio)) {
 -			split = bio_split(bio, sectors, GFP_NOIO, fs_bio_set);
 -			bio_chain(split, bio);
 -		} else {
 -			split = bio;
 +	/* Zero-sector (unknown) and one-sector granularities are the same.  */
 +	granularity = max(q->limits.discard_granularity >> 9, 1U);
 +	alignment = (bdev_discard_alignment(bdev) >> 9) % granularity;
 +
 +	/*
 +	 * Ensure that max_discard_sectors is of the proper
 +	 * granularity, so that requests stay aligned after a split.
 +	 */
 +	max_discard_sectors = min(q->limits.max_discard_sectors, UINT_MAX >> 9);
 +	max_discard_sectors -= max_discard_sectors % granularity;
 +	if (unlikely(!max_discard_sectors)) {
 +		/* Avoid infinite loop below. Being cautious never hurts. */
 +		return -EOPNOTSUPP;
 +	}
 +
 +	while (nr_sects) {
 +		unsigned int req_sects;
 +		sector_t end_sect, tmp;
 +
 +		req_sects = min_t(sector_t, nr_sects, max_discard_sectors);
 +
 +		/**
 +		 * If splitting a request, and the next starting sector would be
 +		 * misaligned, stop the discard at the previous aligned sector.
 +		 */
 +		end_sect = sector + req_sects;
 +		tmp = end_sect;
 +		if (req_sects < nr_sects &&
 +		    sector_div(tmp, granularity) != alignment) {
 +			end_sect = end_sect - alignment;
 +			sector_div(end_sect, granularity);
 +			end_sect = end_sect * granularity + alignment;
 +			req_sects = end_sect - sector;
  		}
  
 -		zone = find_zone(mddev->private, &sector);
 -		tmp_dev = map_sector(mddev, zone, sector, &sector);
 -		split->bi_bdev = tmp_dev->bdev;
 -		split->bi_iter.bi_sector = sector + zone->dev_start +
 -			tmp_dev->data_offset;
 +		bio = next_bio(bio, type, 1, gfp_mask);
 +		bio->bi_sector = sector;
 +		bio->bi_bdev = bdev;
 +
++<<<<<<< HEAD
 +		bio->bi_size = req_sects << 9;
 +		nr_sects -= req_sects;
 +		sector = end_sect;
 +
 +		/*
 +		 * We can loop for a long time in here, if someone does
 +		 * full device discards (like mkfs). Be nice and allow
 +		 * us to schedule out to avoid softlocking if preempt
 +		 * is disabled.
 +		 */
 +		cond_resched();
 +	}
 +
 +	*biop = bio;
 +	return 0;
 +}
 +
 +static void raid0_handle_discard(struct mddev *mddev, struct bio *bio)
 +{
 +	struct r0conf *conf = mddev->private;
 +	struct strip_zone *zone;
 +	sector_t start = bio->bi_sector;
 +	sector_t end;
 +	unsigned int stripe_size;
 +	sector_t first_stripe_index, last_stripe_index;
 +	sector_t start_disk_offset;
 +	unsigned int start_disk_index;
 +	sector_t end_disk_offset;
 +	unsigned int end_disk_index;
 +	unsigned int disk;
 +
 +	zone = find_zone(conf, &start);
 +
 +	if (bio_end_sector(bio) > zone->zone_end) {
 +		struct bio_pair *bp = bio_split(bio, zone->zone_end -
 +						     bio->bi_sector);
 +		raid0_handle_discard(mddev, &bp->bio1);
 +		raid0_handle_discard(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return;
 +	} else
 +		end = bio_end_sector(bio);
 +
 +	if (zone != conf->strip_zone)
 +		end = end - zone[-1].zone_end;
 +
 +	/* Now start and end is the offset in zone */
 +	stripe_size = zone->nb_dev * mddev->chunk_sectors;
 +
 +	first_stripe_index = start;
 +	sector_div(first_stripe_index, stripe_size);
 +	last_stripe_index = end;
 +	sector_div(last_stripe_index, stripe_size);
 +
 +	start_disk_index = (int)(start - first_stripe_index * stripe_size) /
 +		mddev->chunk_sectors;
 +	start_disk_offset = ((int)(start - first_stripe_index * stripe_size) %
 +		mddev->chunk_sectors) +
 +		first_stripe_index * mddev->chunk_sectors;
 +	end_disk_index = (int)(end - last_stripe_index * stripe_size) /
 +		mddev->chunk_sectors;
 +	end_disk_offset = ((int)(end - last_stripe_index * stripe_size) %
 +		mddev->chunk_sectors) +
 +		last_stripe_index * mddev->chunk_sectors;
 +
 +	for (disk = 0; disk < zone->nb_dev; disk++) {
 +		sector_t dev_start, dev_end;
 +		struct bio *discard_bio = NULL;
 +		struct md_rdev *rdev;
 +
 +		if (disk < start_disk_index)
 +			dev_start = (first_stripe_index + 1) *
 +				mddev->chunk_sectors;
 +		else if (disk > start_disk_index)
 +			dev_start = first_stripe_index * mddev->chunk_sectors;
 +		else
 +			dev_start = start_disk_offset;
 +
 +		if (disk < end_disk_index)
 +			dev_end = (last_stripe_index + 1) * mddev->chunk_sectors;
 +		else if (disk > end_disk_index)
 +			dev_end = last_stripe_index * mddev->chunk_sectors;
 +		else
 +			dev_end = end_disk_offset;
 +
 +		if (dev_end <= dev_start)
 +			continue;
 +
 +		rdev = conf->devlist[(zone - conf->strip_zone) *
 +			conf->strip_zone[0].nb_dev + disk];
 +
 +		if (__blkdev_issue_discard(rdev->bdev,
 +			dev_start + zone->dev_start + rdev->data_offset,
 +			dev_end - dev_start, GFP_NOIO, REQ_WRITE | REQ_DISCARD,
 +			&discard_bio) ||
 +		    !discard_bio)
 +			continue;
 +		bio_chain(discard_bio, bio);
 +		submit_bio(REQ_WRITE | REQ_DISCARD, discard_bio);
 +	}
 +	bio_endio(bio, 0);
 +}
 +
 +
 +static bool raid0_make_request(struct mddev *mddev, struct bio *bio)
 +{
 +	unsigned int chunk_sects;
 +	sector_t sector_offset;
 +	struct strip_zone *zone;
 +	struct md_rdev *tmp_dev;
 +
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
 +	}
  
 +	if (unlikely((bio_op(bio) == REQ_OP_DISCARD))) {
 +		raid0_handle_discard(mddev, bio);
 +		return true;
 +	}
 +
 +	chunk_sects = mddev->chunk_sectors;
 +	if (unlikely(!is_io_in_chunk_boundary(mddev, chunk_sects, bio))) {
 +		sector_t sector = bio->bi_sector;
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		if (likely(is_power_of_2(chunk_sects)))
 +			bp = bio_split(bio, chunk_sects - (sector &
 +							   (chunk_sects-1)));
 +		else
 +			bp = bio_split(bio, chunk_sects -
 +				       sector_div(sector, chunk_sects));
 +		raid0_make_request(mddev, &bp->bio1);
 +		raid0_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
 +	}
 +
 +	sector_offset = bio->bi_sector;
 +	zone = find_zone(mddev->private, &sector_offset);
 +	tmp_dev = map_sector(mddev, zone, bio->bi_sector,
 +			     &sector_offset);
 +	bio->bi_bdev = tmp_dev->bdev;
 +	bio->bi_sector = sector_offset + zone->dev_start +
 +		tmp_dev->data_offset;
 +
 +	mddev_check_writesame(mddev, bio);
 +	generic_make_request(bio);
 +	return true;
 +
 +bad_map:
 +	printk("md/raid0:%s: make_request bug: can't convert block across chunks"
 +	       " or bigger than %dk %llu %d\n",
 +	       mdname(mddev), chunk_sects / 2,
 +	       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +	bio_io_error(bio);
 +	return true;
++=======
+ 		if (unlikely((bio_op(split) == REQ_OP_DISCARD) &&
+ 			 !blk_queue_discard(bdev_get_queue(split->bi_bdev)))) {
+ 			/* Just ignore it */
+ 			bio_endio(split);
+ 		} else {
+ 			if (mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(split->bi_bdev),
+ 						      split, disk_devt(mddev->gendisk),
+ 						      bio_sector);
+ 			generic_make_request(split);
+ 		}
+ 	} while (split != bio);
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  }
  
  static void raid0_status(struct seq_file *seq, struct mddev *mddev)
diff --cc drivers/md/raid1.c
index b69cd658dbb5,2dc1934925ec..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -37,15 -37,10 +37,16 @@@
  #include <linux/module.h>
  #include <linux/seq_file.h>
  #include <linux/ratelimit.h>
+ #include <trace/events/block.h>
  #include "md.h"
  #include "raid1.h"
 -#include "bitmap.h"
 +#include "md-bitmap.h"
 +
 +#define UNSUPPORTED_MDDEV_FLAGS		\
 +	((1L << MD_HAS_JOURNAL) |	\
 +	 (1L << MD_JOURNAL_CLEAN) |	\
 +	 (1L << MD_HAS_PPL) |           \
 +	 (1L << MD_HAS_MULTIPLE_PPLS))
  
  /*
   * Number of guaranteed r1bios in case of extreme VM load:
@@@ -1321,9 -1067,145 +1322,149 @@@ static bool raid1_write_request(struct 
  	 * Continue immediately if no resync is active currently.
  	 */
  
 -	md_write_start(mddev, bio); /* wait on superblock update early */
 +	if(!md_write_start(mddev, bio)) /* wait on superblock update early */
 +		return false;
  
++<<<<<<< HEAD
++=======
+ 	if (bio_data_dir(bio) == WRITE &&
+ 	    ((bio_end_sector(bio) > mddev->suspend_lo &&
+ 	    bio->bi_iter.bi_sector < mddev->suspend_hi) ||
+ 	    (mddev_is_clustered(mddev) &&
+ 	     md_cluster_ops->area_resyncing(mddev, WRITE,
+ 		     bio->bi_iter.bi_sector, bio_end_sector(bio))))) {
+ 		/* As the suspend_* range is controlled by
+ 		 * userspace, we want an interruptible
+ 		 * wait.
+ 		 */
+ 		DEFINE_WAIT(w);
+ 		for (;;) {
+ 			flush_signals(current);
+ 			prepare_to_wait(&conf->wait_barrier,
+ 					&w, TASK_INTERRUPTIBLE);
+ 			if (bio_end_sector(bio) <= mddev->suspend_lo ||
+ 			    bio->bi_iter.bi_sector >= mddev->suspend_hi ||
+ 			    (mddev_is_clustered(mddev) &&
+ 			     !md_cluster_ops->area_resyncing(mddev, WRITE,
+ 				     bio->bi_iter.bi_sector, bio_end_sector(bio))))
+ 				break;
+ 			schedule();
+ 		}
+ 		finish_wait(&conf->wait_barrier, &w);
+ 	}
+ 
+ 	start_next_window = wait_barrier(conf, bio);
+ 
+ 	bitmap = mddev->bitmap;
+ 
+ 	/*
+ 	 * make_request() can abort the operation when read-ahead is being
+ 	 * used and no empty request is available.
+ 	 *
+ 	 */
+ 	r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
+ 
+ 	r1_bio->master_bio = bio;
+ 	r1_bio->sectors = bio_sectors(bio);
+ 	r1_bio->state = 0;
+ 	r1_bio->mddev = mddev;
+ 	r1_bio->sector = bio->bi_iter.bi_sector;
+ 
+ 	/* We might need to issue multiple reads to different
+ 	 * devices if there are bad blocks around, so we keep
+ 	 * track of the number of reads in bio->bi_phys_segments.
+ 	 * If this is 0, there is only one r1_bio and no locking
+ 	 * will be needed when requests complete.  If it is
+ 	 * non-zero, then it is the number of not-completed requests.
+ 	 */
+ 	bio->bi_phys_segments = 0;
+ 	bio_clear_flag(bio, BIO_SEG_VALID);
+ 
+ 	if (rw == READ) {
+ 		/*
+ 		 * read balancing logic:
+ 		 */
+ 		int rdisk;
+ 
+ read_again:
+ 		rdisk = read_balance(conf, r1_bio, &max_sectors);
+ 
+ 		if (rdisk < 0) {
+ 			/* couldn't find anywhere to read from */
+ 			raid_end_bio_io(r1_bio);
+ 			return;
+ 		}
+ 		mirror = conf->mirrors + rdisk;
+ 
+ 		if (test_bit(WriteMostly, &mirror->rdev->flags) &&
+ 		    bitmap) {
+ 			/* Reading from a write-mostly device must
+ 			 * take care not to over-take any writes
+ 			 * that are 'behind'
+ 			 */
+ 			wait_event(bitmap->behind_wait,
+ 				   atomic_read(&bitmap->behind_writes) == 0);
+ 		}
+ 		r1_bio->read_disk = rdisk;
+ 		r1_bio->start_next_window = 0;
+ 
+ 		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 		bio_trim(read_bio, r1_bio->sector - bio->bi_iter.bi_sector,
+ 			 max_sectors);
+ 
+ 		r1_bio->bios[rdisk] = read_bio;
+ 
+ 		read_bio->bi_iter.bi_sector = r1_bio->sector +
+ 			mirror->rdev->data_offset;
+ 		read_bio->bi_bdev = mirror->rdev->bdev;
+ 		read_bio->bi_end_io = raid1_end_read_request;
+ 		bio_set_op_attrs(read_bio, op, do_sync);
+ 		read_bio->bi_private = r1_bio;
+ 
+ 		if (mddev->gendisk)
+ 			trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 					      read_bio, disk_devt(mddev->gendisk),
+ 					      r1_bio->sector);
+ 
+ 		if (max_sectors < r1_bio->sectors) {
+ 			/* could not read all from this device, so we will
+ 			 * need another r1_bio.
+ 			 */
+ 
+ 			sectors_handled = (r1_bio->sector + max_sectors
+ 					   - bio->bi_iter.bi_sector);
+ 			r1_bio->sectors = max_sectors;
+ 			spin_lock_irq(&conf->device_lock);
+ 			if (bio->bi_phys_segments == 0)
+ 				bio->bi_phys_segments = 2;
+ 			else
+ 				bio->bi_phys_segments++;
+ 			spin_unlock_irq(&conf->device_lock);
+ 			/* Cannot call generic_make_request directly
+ 			 * as that will be queued in __make_request
+ 			 * and subsequent mempool_alloc might block waiting
+ 			 * for it.  So hand bio over to raid1d.
+ 			 */
+ 			reschedule_retry(r1_bio);
+ 
+ 			r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
+ 
+ 			r1_bio->master_bio = bio;
+ 			r1_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 			r1_bio->state = 0;
+ 			r1_bio->mddev = mddev;
+ 			r1_bio->sector = bio->bi_iter.bi_sector +
+ 				sectors_handled;
+ 			goto read_again;
+ 		} else
+ 			generic_make_request(read_bio);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * WRITE:
+ 	 */
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
  		wait_event(conf->wait_barrier,
@@@ -1479,16 -1371,11 +1620,16 @@@
  
  		r1_bio->bios[i] = mbio;
  
 -		mbio->bi_iter.bi_sector	= (r1_bio->sector +
 +		mbio->bi_sector	= (r1_bio->sector +
  				   conf->mirrors[i].rdev->data_offset);
- 		mbio->bi_bdev = (void*)conf->mirrors[i].rdev;
+ 		mbio->bi_bdev = conf->mirrors[i].rdev->bdev;
  		mbio->bi_end_io	= raid1_end_write_request;
 -		bio_set_op_attrs(mbio, op, do_flush_fua | do_sync);
 +		mbio->bi_rw =
 +			WRITE | do_fua | do_sync | do_discard | do_same;
 +		if (test_bit(FailFast, &conf->mirrors[i].rdev->flags) &&
 +		    !test_bit(WriteMostly, &conf->mirrors[i].rdev->flags) &&
 +		    conf->raid_disks - mddev->degraded > 1)
 +			mbio->bi_rw |= MD_FAILFAST;
  		mbio->bi_private = r1_bio;
  
  		atomic_inc(&r1_bio->remaining);
@@@ -2547,19 -2362,35 +2699,33 @@@ read_more
  			/* Drat - have to split this up more */
  			struct bio *mbio = r1_bio->master_bio;
  			int sectors_handled = (r1_bio->sector + max_sectors
 -					       - mbio->bi_iter.bi_sector);
 +					       - mbio->bi_sector);
  			r1_bio->sectors = max_sectors;
++<<<<<<< HEAD
 +			bio_inc_remaining(mbio);
++=======
+ 			spin_lock_irq(&conf->device_lock);
+ 			if (mbio->bi_phys_segments == 0)
+ 				mbio->bi_phys_segments = 2;
+ 			else
+ 				mbio->bi_phys_segments++;
+ 			spin_unlock_irq(&conf->device_lock);
+ 			trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
+ 					      bio, bio_dev, bio_sector);
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  			generic_make_request(bio);
  			bio = NULL;
  
 -			r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
 -
 -			r1_bio->master_bio = mbio;
 -			r1_bio->sectors = bio_sectors(mbio) - sectors_handled;
 -			r1_bio->state = 0;
 +			r1_bio = alloc_r1bio(mddev, mbio, sectors_handled);
  			set_bit(R1BIO_ReadError, &r1_bio->state);
 -			r1_bio->mddev = mddev;
 -			r1_bio->sector = mbio->bi_iter.bi_sector +
 -				sectors_handled;
 +			inc_pending(conf, r1_bio->sector);
  
  			goto read_more;
- 		} else
+ 		} else {
+ 			trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
+ 					      bio, bio_dev, bio_sector);
  			generic_make_request(bio);
+ 		}
  	}
  }
  
diff --cc drivers/md/raid10.c
index ca76012444cf,67f0034d4956..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -1401,6 -1118,95 +1402,98 @@@ static void raid10_write_request(struc
  		conf->reshape_safe = mddev->reshape_position;
  	}
  
++<<<<<<< HEAD
++=======
+ 	r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 	r10_bio->master_bio = bio;
+ 	r10_bio->sectors = sectors;
+ 
+ 	r10_bio->mddev = mddev;
+ 	r10_bio->sector = bio->bi_iter.bi_sector;
+ 	r10_bio->state = 0;
+ 
+ 	/* We might need to issue multiple reads to different
+ 	 * devices if there are bad blocks around, so we keep
+ 	 * track of the number of reads in bio->bi_phys_segments.
+ 	 * If this is 0, there is only one r10_bio and no locking
+ 	 * will be needed when the request completes.  If it is
+ 	 * non-zero, then it is the number of not-completed requests.
+ 	 */
+ 	bio->bi_phys_segments = 0;
+ 	bio_clear_flag(bio, BIO_SEG_VALID);
+ 
+ 	if (rw == READ) {
+ 		/*
+ 		 * read balancing logic:
+ 		 */
+ 		struct md_rdev *rdev;
+ 		int slot;
+ 
+ read_again:
+ 		rdev = read_balance(conf, r10_bio, &max_sectors);
+ 		if (!rdev) {
+ 			raid_end_bio_io(r10_bio);
+ 			return;
+ 		}
+ 		slot = r10_bio->read_slot;
+ 
+ 		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 		bio_trim(read_bio, r10_bio->sector - bio->bi_iter.bi_sector,
+ 			 max_sectors);
+ 
+ 		r10_bio->devs[slot].bio = read_bio;
+ 		r10_bio->devs[slot].rdev = rdev;
+ 
+ 		read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
+ 			choose_data_offset(r10_bio, rdev);
+ 		read_bio->bi_bdev = rdev->bdev;
+ 		read_bio->bi_end_io = raid10_end_read_request;
+ 		bio_set_op_attrs(read_bio, op, do_sync);
+ 		read_bio->bi_private = r10_bio;
+ 
+ 		if (mddev->gendisk)
+ 			trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 					      read_bio, disk_devt(mddev->gendisk),
+ 					      r10_bio->sector);
+ 		if (max_sectors < r10_bio->sectors) {
+ 			/* Could not read all from this device, so we will
+ 			 * need another r10_bio.
+ 			 */
+ 			sectors_handled = (r10_bio->sector + max_sectors
+ 					   - bio->bi_iter.bi_sector);
+ 			r10_bio->sectors = max_sectors;
+ 			spin_lock_irq(&conf->device_lock);
+ 			if (bio->bi_phys_segments == 0)
+ 				bio->bi_phys_segments = 2;
+ 			else
+ 				bio->bi_phys_segments++;
+ 			spin_unlock_irq(&conf->device_lock);
+ 			/* Cannot call generic_make_request directly
+ 			 * as that will be queued in __generic_make_request
+ 			 * and subsequent mempool_alloc might block
+ 			 * waiting for it.  so hand bio over to raid10d.
+ 			 */
+ 			reschedule_retry(r10_bio);
+ 
+ 			r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 			r10_bio->master_bio = bio;
+ 			r10_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 			r10_bio->state = 0;
+ 			r10_bio->mddev = mddev;
+ 			r10_bio->sector = bio->bi_iter.bi_sector +
+ 				sectors_handled;
+ 			goto read_again;
+ 		} else
+ 			generic_make_request(read_bio);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * WRITE:
+ 	 */
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
  		wait_event(conf->wait_barrier,
@@@ -1546,12 -1360,87 +1639,96 @@@ retry_write
  	bitmap_startwrite(mddev->bitmap, r10_bio->sector, r10_bio->sectors, 0);
  
  	for (i = 0; i < conf->copies; i++) {
++<<<<<<< HEAD
 +		if (r10_bio->devs[i].bio)
 +			raid10_write_one_disk(mddev, r10_bio, bio, false,
 +					      i, max_sectors);
 +		if (r10_bio->devs[i].repl_bio)
 +			raid10_write_one_disk(mddev, r10_bio, bio, true,
 +					      i, max_sectors);
++=======
+ 		struct bio *mbio;
+ 		int d = r10_bio->devs[i].devnum;
+ 		if (r10_bio->devs[i].bio) {
+ 			struct md_rdev *rdev = conf->mirrors[d].rdev;
+ 			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 			bio_trim(mbio, r10_bio->sector - bio->bi_iter.bi_sector,
+ 				 max_sectors);
+ 			r10_bio->devs[i].bio = mbio;
+ 
+ 			mbio->bi_iter.bi_sector	= (r10_bio->devs[i].addr+
+ 					   choose_data_offset(r10_bio,
+ 							      rdev));
+ 			mbio->bi_bdev = rdev->bdev;
+ 			mbio->bi_end_io	= raid10_end_write_request;
+ 			bio_set_op_attrs(mbio, op, do_sync | do_fua);
+ 			mbio->bi_private = r10_bio;
+ 
+ 			if (conf->mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
+ 						      mbio, disk_devt(conf->mddev->gendisk),
+ 						      r10_bio->sector);
+ 			/* flush_pending_writes() needs access to the rdev so...*/
+ 			mbio->bi_bdev = (void*)rdev;
+ 
+ 			atomic_inc(&r10_bio->remaining);
+ 
+ 			cb = blk_check_plugged(raid10_unplug, mddev,
+ 					       sizeof(*plug));
+ 			if (cb)
+ 				plug = container_of(cb, struct raid10_plug_cb,
+ 						    cb);
+ 			else
+ 				plug = NULL;
+ 			spin_lock_irqsave(&conf->device_lock, flags);
+ 			if (plug) {
+ 				bio_list_add(&plug->pending, mbio);
+ 				plug->pending_cnt++;
+ 			} else {
+ 				bio_list_add(&conf->pending_bio_list, mbio);
+ 				conf->pending_count++;
+ 			}
+ 			spin_unlock_irqrestore(&conf->device_lock, flags);
+ 			if (!plug)
+ 				md_wakeup_thread(mddev->thread);
+ 		}
+ 
+ 		if (r10_bio->devs[i].repl_bio) {
+ 			struct md_rdev *rdev = conf->mirrors[d].replacement;
+ 			if (rdev == NULL) {
+ 				/* Replacement just got moved to main 'rdev' */
+ 				smp_mb();
+ 				rdev = conf->mirrors[d].rdev;
+ 			}
+ 			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 			bio_trim(mbio, r10_bio->sector - bio->bi_iter.bi_sector,
+ 				 max_sectors);
+ 			r10_bio->devs[i].repl_bio = mbio;
+ 
+ 			mbio->bi_iter.bi_sector	= (r10_bio->devs[i].addr +
+ 					   choose_data_offset(
+ 						   r10_bio, rdev));
+ 			mbio->bi_bdev = rdev->bdev;
+ 			mbio->bi_end_io	= raid10_end_write_request;
+ 			bio_set_op_attrs(mbio, op, do_sync | do_fua);
+ 			mbio->bi_private = r10_bio;
+ 
+ 			if (conf->mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
+ 						      mbio, disk_devt(conf->mddev->gendisk),
+ 						      r10_bio->sector);
+ 			/* flush_pending_writes() needs access to the rdev so...*/
+ 			mbio->bi_bdev = (void*)rdev;
+ 
+ 			atomic_inc(&r10_bio->remaining);
+ 			spin_lock_irqsave(&conf->device_lock, flags);
+ 			bio_list_add(&conf->pending_bio_list, mbio);
+ 			conf->pending_count++;
+ 			spin_unlock_irqrestore(&conf->device_lock, flags);
+ 			if (!mddev_check_plugged(mddev))
+ 				md_wakeup_thread(mddev->thread);
+ 		}
++>>>>>>> 109e37653033 (md: add block tracing for bio_remapping)
  	}
  
  	/* Don't remove the bias on 'remaining' (one_write_done) until
@@@ -2799,18 -2560,19 +2980,22 @@@ read_more
  			   (unsigned long long)r10_bio->sector);
  	bio = bio_clone_mddev(r10_bio->master_bio,
  			      GFP_NOIO, mddev);
 -	bio_trim(bio, r10_bio->sector - bio->bi_iter.bi_sector, max_sectors);
 +	bio_trim(bio, r10_bio->sector - bio->bi_sector, max_sectors);
  	r10_bio->devs[slot].bio = bio;
  	r10_bio->devs[slot].rdev = rdev;
 -	bio->bi_iter.bi_sector = r10_bio->devs[slot].addr
 +	bio->bi_sector = r10_bio->devs[slot].addr
  		+ choose_data_offset(r10_bio, rdev);
  	bio->bi_bdev = rdev->bdev;
 -	bio_set_op_attrs(bio, REQ_OP_READ, do_sync);
 +	bio->bi_rw = READ | do_sync;
 +	if (test_bit(FailFast, &rdev->flags) &&
 +	    test_bit(R10BIO_FailFast, &r10_bio->state))
 +		bio->bi_rw |= MD_FAILFAST;
  	bio->bi_private = r10_bio;
  	bio->bi_end_io = raid10_end_read_request;
+ 	trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
+ 			      bio, bio_dev,
+ 			      bio_last_sector - r10_bio->sectors);
+ 
  	if (max_sectors < r10_bio->sectors) {
  		/* Drat - have to split this up more */
  		struct bio *mbio = r10_bio->master_bio;
* Unmerged path drivers/md/md-linear.c
* Unmerged path drivers/md/raid0.c
* Unmerged path drivers/md/raid1.c
* Unmerged path drivers/md/raid10.c
