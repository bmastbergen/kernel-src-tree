mm/mmap.c: do not blow on PROT_NONE MAP_FIXED holes in the stack

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Michal Hocko <mhocko@suse.com>
commit 561b5e0709e4a248c67d024d4d94b6e31e3edf2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/561b5e07.failed

Commit 1be7107fbe18 ("mm: larger stack guard gap, between vmas") has
introduced a regression in some rust and Java environments which are
trying to implement their own stack guard page.  They are punching a new
MAP_FIXED mapping inside the existing stack Vma.

This will confuse expand_{downwards,upwards} into thinking that the
stack expansion would in fact get us too close to an existing non-stack
vma which is a correct behavior wrt safety.  It is a real regression on
the other hand.

Let's work around the problem by considering PROT_NONE mapping as a part
of the stack.  This is a gros hack but overflowing to such a mapping
would trap anyway an we only can hope that usespace knows what it is
doing and handle it propely.

Fixes: 1be7107fbe18 ("mm: larger stack guard gap, between vmas")
Link: http://lkml.kernel.org/r/20170705182849.GA18027@dhcp22.suse.cz
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Debugged-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Ben Hutchings <ben@decadent.org.uk>
	Cc: Willy Tarreau <w@1wt.eu>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 561b5e0709e4a248c67d024d4d94b6e31e3edf2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mmap.c
diff --cc mm/mmap.c
index d07a55987643,d1902872414f..000000000000
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@@ -2367,13 -2230,30 +2367,38 @@@ int expand_upwards(struct vm_area_struc
  	if (!(vma->vm_flags & VM_GROWSUP))
  		return -EFAULT;
  
++<<<<<<< HEAD
 +	/*
 +	 * We must make sure the anon_vma is allocated
 +	 * so that the anon_vma locking is not a noop.
 +	 */
++=======
+ 	/* Guard against exceeding limits of the address space. */
+ 	address &= PAGE_MASK;
+ 	if (address >= TASK_SIZE)
+ 		return -ENOMEM;
+ 	address += PAGE_SIZE;
+ 
+ 	/* Enforce stack_guard_gap */
+ 	gap_addr = address + stack_guard_gap;
+ 
+ 	/* Guard against overflow */
+ 	if (gap_addr < address || gap_addr > TASK_SIZE)
+ 		gap_addr = TASK_SIZE;
+ 
+ 	next = vma->vm_next;
+ 	if (next && next->vm_start < gap_addr &&
+ 			(next->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
+ 		if (!(next->vm_flags & VM_GROWSUP))
+ 			return -ENOMEM;
+ 		/* Check that both stack segments have the same anon_vma? */
+ 	}
+ 
+ 	/* We must make sure the anon_vma is allocated. */
++>>>>>>> 561b5e0709e4 (mm/mmap.c: do not blow on PROT_NONE MAP_FIXED holes in the stack)
  	if (unlikely(anon_vma_prepare(vma)))
  		return -ENOMEM;
 +	vma_lock_anon_vma(vma);
  
  	/*
  	 * vma->vm_start/vm_end cannot change under us because the caller
@@@ -2471,12 -2326,12 +2496,18 @@@ int expand_downwards(struct vm_area_str
  
  	/* Enforce stack_guard_gap */
  	gap_addr = address - stack_guard_gap;
 -	if (gap_addr > address)
 +	if (gap_addr > address) 
  		return -ENOMEM;
 +	
  	prev = vma->vm_prev;
++<<<<<<< HEAD
 +	if (prev && prev->vm_end > gap_addr) {
 +		if (!(prev->vm_flags & VM_GROWSDOWN)) 
++=======
+ 	if (prev && prev->vm_end > gap_addr &&
+ 			(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
+ 		if (!(prev->vm_flags & VM_GROWSDOWN))
++>>>>>>> 561b5e0709e4 (mm/mmap.c: do not blow on PROT_NONE MAP_FIXED holes in the stack)
  			return -ENOMEM;
  		/* Check that both stack segments have the same anon_vma? */
  	}
* Unmerged path mm/mmap.c
