mm: convert printk(KERN_<LEVEL> to pr_<level>

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Joe Perches <joe@perches.com>
commit 1170532bb49f9468aedabdc1d5a560e2521a2bcc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/1170532b.failed

Most of the mm subsystem uses pr_<level> so make it consistent.

Miscellanea:

 - Realign arguments
 - Add missing newline to format
 - kmemleak-test.c has a "kmemleak: " prefix added to the
   "Kmemleak testing" logging message via pr_fmt

	Signed-off-by: Joe Perches <joe@perches.com>
	Acked-by: Tejun Heo <tj@kernel.org>	[percpu]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1170532bb49f9468aedabdc1d5a560e2521a2bcc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/dmapool.c
#	mm/internal.h
#	mm/kmemcheck.c
#	mm/memory-failure.c
#	mm/page_alloc.c
#	mm/page_io.c
#	mm/slab.c
#	mm/sparse-vmemmap.c
#	mm/sparse.c
#	mm/swap_cgroup.c
diff --cc mm/dmapool.c
index 0b33dd0bf614,abcbfe86c25a..000000000000
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@@ -436,13 -450,11 +434,18 @@@ void dma_pool_free(struct dma_pool *poo
  			}
  			spin_unlock_irqrestore(&pool->lock, flags);
  			if (pool->dev)
 -				dev_err(pool->dev, "dma_pool_free %s, dma %Lx already free\n",
 -					pool->name, (unsigned long long)dma);
 +				dev_err(pool->dev, "dma_pool_free %s, dma %Lx "
 +					"already free\n", pool->name,
 +					(unsigned long long)dma);
  			else
++<<<<<<< HEAD
 +				printk(KERN_ERR "dma_pool_free %s, dma %Lx "
 +					"already free\n", pool->name,
 +					(unsigned long long)dma);
++=======
+ 				pr_err("dma_pool_free %s, dma %Lx already free\n",
+ 				       pool->name, (unsigned long long)dma);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  			return;
  		}
  	}
diff --cc mm/internal.h
index af10783b4dc0,7449392c6faa..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -303,8 -385,10 +303,15 @@@ extern int mminit_loglevel
  #define mminit_dprintk(level, prefix, fmt, arg...) \
  do { \
  	if (level < mminit_loglevel) { \
++<<<<<<< HEAD
 +		printk(level <= MMINIT_WARNING ? KERN_WARNING : KERN_DEBUG); \
 +		printk(KERN_CONT "mminit::" prefix " " fmt, ##arg); \
++=======
+ 		if (level <= MMINIT_WARNING) \
+ 			pr_warn("mminit::" prefix " " fmt, ##arg);	\
+ 		else \
+ 			printk(KERN_DEBUG "mminit::" prefix " " fmt, ##arg); \
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	} \
  } while (0)
  
diff --cc mm/kmemcheck.c
index fd814fd61319,5bf191756a4a..000000000000
--- a/mm/kmemcheck.c
+++ b/mm/kmemcheck.c
@@@ -19,8 -20,7 +19,12 @@@ void kmemcheck_alloc_shadow(struct pag
  	shadow = alloc_pages_node(node, flags | __GFP_NOTRACK, order);
  	if (!shadow) {
  		if (printk_ratelimit())
++<<<<<<< HEAD
 +			printk(KERN_ERR "kmemcheck: failed to allocate "
 +				"shadow bitmap\n");
++=======
+ 			pr_err("kmemcheck: failed to allocate shadow bitmap\n");
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		return;
  	}
  
diff --cc mm/memory-failure.c
index 2d6d813952ef,5a544c6c0717..000000000000
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@@ -577,8 -559,8 +573,13 @@@ static int me_kernel(struct page *p, un
   */
  static int me_unknown(struct page *p, unsigned long pfn)
  {
++<<<<<<< HEAD
 +	printk(KERN_ERR "MCE %#lx: Unknown page state\n", pfn);
 +	return FAILED;
++=======
+ 	pr_err("MCE %#lx: Unknown page state\n", pfn);
+ 	return MF_FAILED;
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  }
  
  /*
@@@ -636,10 -618,9 +637,9 @@@ static int me_pagecache_clean(struct pa
  		 * This fails on dirty or anything with private pages
  		 */
  		if (invalidate_inode_page(p))
 -			ret = MF_RECOVERED;
 +			ret = RECOVERED;
  		else
- 			printk(KERN_INFO "MCE %#lx: Failed to invalidate\n",
- 				pfn);
+ 			pr_info("MCE %#lx: Failed to invalidate\n", pfn);
  	}
  	return ret;
  }
@@@ -869,15 -846,14 +869,21 @@@ static int page_action(struct page_stat
  	result = ps->action(p, pfn);
  
  	count = page_count(p) - 1;
 -	if (ps->action == me_swapcache_dirty && result == MF_DELAYED)
 +	if (ps->action == me_swapcache_dirty && result == DELAYED)
  		count--;
  	if (count != 0) {
++<<<<<<< HEAD
 +		printk(KERN_ERR
 +		       "MCE %#lx: %s page still referenced by %d users\n",
 +		       pfn, ps->msg, count);
 +		result = FAILED;
++=======
+ 		pr_err("MCE %#lx: %s still referenced by %d users\n",
+ 		       pfn, action_page_types[ps->type], count);
+ 		result = MF_FAILED;
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	}
 -	action_result(pfn, ps->type, result);
 +	action_result(pfn, ps->msg, result);
  
  	/* Could do more checks here if page looks ok */
  	/*
@@@ -995,16 -964,9 +999,16 @@@ static int hwpoison_user_mappings(struc
  
  	ret = try_to_unmap(hpage, ttu);
  	if (ret != SWAP_SUCCESS)
- 		printk(KERN_ERR "MCE %#lx: failed to unmap page (mapcount=%d)\n",
- 				pfn, page_mapcount(hpage));
+ 		pr_err("MCE %#lx: failed to unmap page (mapcount=%d)\n",
+ 		       pfn, page_mapcount(hpage));
  
 +	/*
 +	 * try_to_unmap() might put mlocked page in lru cache, so call
 +	 * shake_page() again to ensure that it's flushed.
 +	 */
 +	if (mlocked)
 +		shake_page(hpage, 0);
 +
  	/*
  	 * Now that the dirty bit has been propagated to the
  	 * struct page and all unmaps done we can decide if
@@@ -1213,11 -1170,11 +1215,19 @@@ int memory_failure(unsigned long pfn, i
  	 * unpoison always clear PG_hwpoison inside page lock
  	 */
  	if (!PageHWPoison(p)) {
++<<<<<<< HEAD
 +		printk(KERN_ERR "MCE %#lx: just unpoisoned\n", pfn);
 +		atomic_long_sub(nr_pages, &num_poisoned_pages);
 +		put_page(hpage);
 +		res = 0;
 +		goto out;
++=======
+ 		pr_err("MCE %#lx: just unpoisoned\n", pfn);
+ 		num_poisoned_pages_sub(nr_pages);
+ 		unlock_page(hpage);
+ 		put_hwpoison_page(hpage);
+ 		return 0;
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	}
  	if (hwpoison_filter(p)) {
  		if (TestClearPageHWPoison(p))
diff --cc mm/page_alloc.c
index 10b3d04ee8fe,2a9eaec770b0..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -4209,9 -4073,7 +4209,13 @@@ static int __parse_numa_zonelist_order(
  	} else if (*s == 'z' || *s == 'Z') {
  		user_zonelist_order = ZONELIST_ORDER_ZONE;
  	} else {
++<<<<<<< HEAD
 +		printk(KERN_WARNING
 +			"Ignoring invalid numa_zonelist_order value:  "
 +			"%s\n", s);
++=======
+ 		pr_warn("Ignoring invalid numa_zonelist_order value:  %s\n", s);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		return -EINVAL;
  	}
  	return 0;
@@@ -5789,16 -5449,17 +5793,30 @@@ static void __paginginit free_area_init
  		 * and per-cpu initialisations
  		 */
  		memmap_pages = calc_memmap_size(size, realsize);
++<<<<<<< HEAD
 +		if (freesize >= memmap_pages) {
 +			freesize -= memmap_pages;
 +			if (memmap_pages)
 +				printk(KERN_DEBUG
 +				       "  %s zone: %lu pages used for memmap\n",
 +				       zone_names[j], memmap_pages);
 +		} else
 +			printk(KERN_WARNING
 +				"  %s zone: %lu pages exceeds freesize %lu\n",
 +				zone_names[j], memmap_pages, freesize);
++=======
+ 		if (!is_highmem_idx(j)) {
+ 			if (freesize >= memmap_pages) {
+ 				freesize -= memmap_pages;
+ 				if (memmap_pages)
+ 					printk(KERN_DEBUG
+ 					       "  %s zone: %lu pages used for memmap\n",
+ 					       zone_names[j], memmap_pages);
+ 			} else
+ 				pr_warn("  %s zone: %lu pages exceeds freesize %lu\n",
+ 					zone_names[j], memmap_pages, freesize);
+ 		}
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  
  		/* Account for reserved pages */
  		if (j == 0 && freesize > dma_reserve) {
diff --cc mm/page_io.c
index 01eaa01e62d0,ff74e512f029..000000000000
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@@ -59,31 -56,75 +59,46 @@@ void end_swap_bio_write(struct bio *bio
  		 * Also clear PG_reclaim to avoid rotate_reclaimable_page()
  		 */
  		set_page_dirty(page);
++<<<<<<< HEAD
 +		printk(KERN_ALERT "Write-error on swap-device (%u:%u:%Lu)\n",
 +				imajor(bio->bi_bdev->bd_inode),
 +				iminor(bio->bi_bdev->bd_inode),
 +				(unsigned long long)bio->bi_sector);
++=======
+ 		pr_alert("Write-error on swap-device (%u:%u:%llu)\n",
+ 			 imajor(bio->bi_bdev->bd_inode),
+ 			 iminor(bio->bi_bdev->bd_inode),
+ 			 (unsigned long long)bio->bi_iter.bi_sector);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		ClearPageReclaim(page);
  	}
  	end_page_writeback(page);
  	bio_put(bio);
  }
  
 -static void end_swap_bio_read(struct bio *bio)
 +void end_swap_bio_read(struct bio *bio, int err)
  {
 +	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
  	struct page *page = bio->bi_io_vec[0].bv_page;
  
 -	if (bio->bi_error) {
 +	if (!uptodate) {
  		SetPageError(page);
  		ClearPageUptodate(page);
++<<<<<<< HEAD
 +		printk(KERN_ALERT "Read-error on swap-device (%u:%u:%Lu)\n",
 +				imajor(bio->bi_bdev->bd_inode),
 +				iminor(bio->bi_bdev->bd_inode),
 +				(unsigned long long)bio->bi_sector);
 +	} else {
 +		SetPageUptodate(page);
++=======
+ 		pr_alert("Read-error on swap-device (%u:%u:%llu)\n",
+ 			 imajor(bio->bi_bdev->bd_inode),
+ 			 iminor(bio->bi_bdev->bd_inode),
+ 			 (unsigned long long)bio->bi_iter.bi_sector);
+ 		goto out;
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	}
 -
 -	SetPageUptodate(page);
 -
 -	/*
 -	 * There is no guarantee that the page is in swap cache - the software
 -	 * suspend code (at least) uses end_swap_bio_read() against a non-
 -	 * swapcache page.  So we must check PG_swapcache before proceeding with
 -	 * this optimization.
 -	 */
 -	if (likely(PageSwapCache(page))) {
 -		struct swap_info_struct *sis;
 -
 -		sis = page_swap_info(page);
 -		if (sis->flags & SWP_BLKDEV) {
 -			/*
 -			 * The swap subsystem performs lazy swap slot freeing,
 -			 * expecting that the page will be swapped out again.
 -			 * So we can avoid an unnecessary write if the page
 -			 * isn't redirtied.
 -			 * This is good for real swap storage because we can
 -			 * reduce unnecessary I/O and enhance wear-leveling
 -			 * if an SSD is used as the as swap device.
 -			 * But if in-memory swap device (eg zram) is used,
 -			 * this causes a duplicated copy between uncompressed
 -			 * data in VM-owned memory and compressed data in
 -			 * zram-owned memory.  So let's free zram-owned memory
 -			 * and make the VM-owned decompressed page *dirty*,
 -			 * so the page should be swapped out somewhere again if
 -			 * we again wish to reclaim it.
 -			 */
 -			struct gendisk *disk = sis->bdev->bd_disk;
 -			if (disk->fops->swap_slot_free_notify) {
 -				swp_entry_t entry;
 -				unsigned long offset;
 -
 -				entry.val = page_private(page);
 -				offset = swp_offset(entry);
 -
 -				SetPageDirty(page);
 -				disk->fops->swap_slot_free_notify(sis->bdev,
 -						offset);
 -			}
 -		}
 -	}
 -
 -out:
  	unlock_page(page);
  	bio_put(bio);
  }
diff --cc mm/slab.c
index 0a4735511ea7,e719a5cb3396..000000000000
--- a/mm/slab.c
+++ b/mm/slab.c
@@@ -1885,13 -1566,11 +1885,19 @@@ static void dump_line(char *data, int o
  	if (bad_count == 1) {
  		error ^= POISON_FREE;
  		if (!(error & (error - 1))) {
++<<<<<<< HEAD
 +			printk(KERN_ERR "Single bit error detected. Probably "
 +					"bad RAM.\n");
 +#ifdef CONFIG_X86
 +			printk(KERN_ERR "Run memtest86+ or a similar memory "
 +					"test tool.\n");
++=======
+ 			pr_err("Single bit error detected. Probably bad RAM.\n");
+ #ifdef CONFIG_X86
+ 			pr_err("Run memtest86+ or a similar memory test tool.\n");
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  #else
- 			printk(KERN_ERR "Run a memory test tool.\n");
+ 			pr_err("Run a memory test tool.\n");
  #endif
  		}
  	}
@@@ -1967,22 -1649,20 +1973,20 @@@ static void check_poison_obj(struct kme
  		/* Print some data about the neighboring objects, if they
  		 * exist:
  		 */
 -		struct page *page = virt_to_head_page(objp);
 +		struct slab *slabp = virt_to_slab(objp);
  		unsigned int objnr;
  
 -		objnr = obj_to_index(cachep, page, objp);
 +		objnr = obj_to_index(cachep, slabp, objp);
  		if (objnr) {
 -			objp = index_to_obj(cachep, page, objnr - 1);
 +			objp = index_to_obj(cachep, slabp, objnr - 1);
  			realobj = (char *)objp + obj_offset(cachep);
- 			printk(KERN_ERR "Prev obj: start=%p, len=%d\n",
- 			       realobj, size);
+ 			pr_err("Prev obj: start=%p, len=%d\n", realobj, size);
  			print_objinfo(cachep, objp, 2);
  		}
  		if (objnr + 1 < cachep->num) {
 -			objp = index_to_obj(cachep, page, objnr + 1);
 +			objp = index_to_obj(cachep, slabp, objnr + 1);
  			realobj = (char *)objp + obj_offset(cachep);
- 			printk(KERN_ERR "Next obj: start=%p, len=%d\n",
- 			       realobj, size);
+ 			pr_err("Next obj: start=%p, len=%d\n", realobj, size);
  			print_objinfo(cachep, objp, 2);
  		}
  	}
@@@ -2702,24 -2451,27 +2706,34 @@@ static void *slab_get_obj(struct kmem_c
  	return objp;
  }
  
 -static void slab_put_obj(struct kmem_cache *cachep,
 -			struct page *page, void *objp)
 +static void slab_put_obj(struct kmem_cache *cachep, struct slab *slabp,
 +				void *objp, int nodeid)
  {
 -	unsigned int objnr = obj_to_index(cachep, page, objp);
 +	unsigned int objnr = obj_to_index(cachep, slabp, objp);
 +
++<<<<<<< HEAD
  #if DEBUG
 -	unsigned int i;
 +	/* Verify that the slab belongs to the intended node */
 +	WARN_ON(slabp->nodeid != nodeid);
  
 +	if (slab_bufctl(slabp)[objnr] + 1 <= SLAB_LIMIT + 1) {
 +		printk(KERN_ERR "slab: double free detected in cache "
 +				"'%s', objp %p\n", cachep->name, objp);
 +		BUG();
++=======
+ 	/* Verify double free bug */
+ 	for (i = page->active; i < cachep->num; i++) {
+ 		if (get_free_obj(page, i) == objnr) {
+ 			pr_err("slab: double free detected in cache '%s', objp %p\n",
+ 			       cachep->name, objp);
+ 			BUG();
+ 		}
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	}
  #endif
 -	page->active--;
 -	if (!page->freelist)
 -		page->freelist = objp + obj_offset(cachep);
 -
 -	set_free_obj(page, page->active, objnr);
 +	slab_bufctl(slabp)[objnr] = slabp->free;
 +	slabp->free = objnr;
 +	slabp->inuse--;
  }
  
  /*
@@@ -3076,12 -2893,10 +3090,19 @@@ static void *cache_alloc_debugcheck_aft
  	if (cachep->flags & SLAB_RED_ZONE) {
  		if (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||
  				*dbg_redzone2(cachep, objp) != RED_INACTIVE) {
++<<<<<<< HEAD
 +			slab_error(cachep, "double free, or memory outside"
 +						" object was overwritten");
 +			printk(KERN_ERR
 +				"%p: redzone 1:0x%llx, redzone 2:0x%llx\n",
 +				objp, *dbg_redzone1(cachep, objp),
 +				*dbg_redzone2(cachep, objp));
++=======
+ 			slab_error(cachep, "double free, or memory outside object was overwritten");
+ 			pr_err("%p: redzone 1:0x%llx, redzone 2:0x%llx\n",
+ 			       objp, *dbg_redzone1(cachep, objp),
+ 			       *dbg_redzone2(cachep, objp));
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		}
  		*dbg_redzone1(cachep, objp) = RED_ACTIVE;
  		*dbg_redzone2(cachep, objp) = RED_ACTIVE;
diff --cc mm/sparse-vmemmap.c
index 8ec6748e418b,68885dcbaf40..000000000000
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@@ -144,8 -166,8 +144,13 @@@ void __meminit vmemmap_verify(pte_t *pt
  	int actual_node = early_pfn_to_nid(pfn);
  
  	if (node_distance(actual_node, node) > LOCAL_DISTANCE)
++<<<<<<< HEAD
 +		printk(KERN_WARNING "[%lx-%lx] potential offnode "
 +			"page_structs\n", start, end - 1);
++=======
+ 		pr_warn("[%lx-%lx] potential offnode page_structs\n",
+ 			start, end - 1);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  }
  
  pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)
@@@ -271,8 -292,8 +276,13 @@@ void __init sparse_mem_maps_populate_no
  		if (map_map[pnum])
  			continue;
  		ms = __nr_to_section(pnum);
++<<<<<<< HEAD
 +		printk(KERN_ERR "%s: sparsemem memory map backing failed "
 +			"some memory will not be available.\n", __func__);
++=======
+ 		pr_err("%s: sparsemem memory map backing failed some memory will not be available\n",
+ 		       __func__);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		ms->section_mem_map = 0;
  	}
  
diff --cc mm/sparse.c
index 2dd4aed4ce3c,5d0cf4540364..000000000000
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@@ -427,8 -425,8 +424,13 @@@ void __init sparse_mem_maps_populate_no
  		if (map_map[pnum])
  			continue;
  		ms = __nr_to_section(pnum);
++<<<<<<< HEAD
 +		printk(KERN_ERR "%s: sparsemem memory map backing failed "
 +			"some memory will not be available.\n", __func__);
++=======
+ 		pr_err("%s: sparsemem memory map backing failed some memory will not be available\n",
+ 		       __func__);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  		ms->section_mem_map = 0;
  	}
  }
@@@ -454,8 -453,8 +456,13 @@@ static struct page __init *sparse_early
  	if (map)
  		return map;
  
++<<<<<<< HEAD
 +	printk(KERN_ERR "%s: sparsemem memory map backing failed "
 +			"some memory will not be available.\n", __func__);
++=======
+ 	pr_err("%s: sparsemem memory map backing failed some memory will not be available\n",
+ 	       __func__);
++>>>>>>> 1170532bb49f (mm: convert printk(KERN_<LEVEL> to pr_<level>)
  	ms->section_mem_map = 0;
  	return NULL;
  }
* Unmerged path mm/swap_cgroup.c
diff --git a/mm/backing-dev.c b/mm/backing-dev.c
index a9b6d3fe1e17..fbf2de433b75 100644
--- a/mm/backing-dev.c
+++ b/mm/backing-dev.c
@@ -664,8 +664,8 @@ int pdflush_proc_obsolete(struct ctl_table *table, int write,
 
 	if (copy_to_user(buffer, kbuf, sizeof(kbuf)))
 		return -EFAULT;
-	printk_once(KERN_WARNING "%s exported in /proc is scheduled for removal\n",
-			table->procname);
+	pr_warn_once("%s exported in /proc is scheduled for removal\n",
+		     table->procname);
 
 	*lenp = 2;
 	*ppos += *lenp;
diff --git a/mm/bootmem.c b/mm/bootmem.c
index 9727f4fb7aae..5a86f80c5c81 100644
--- a/mm/bootmem.c
+++ b/mm/bootmem.c
@@ -50,8 +50,7 @@ early_param("bootmem_debug", bootmem_debug_setup);
 
 #define bdebug(fmt, args...) ({				\
 	if (unlikely(bootmem_debug))			\
-		printk(KERN_INFO			\
-			"bootmem::%s " fmt,		\
+		pr_info("bootmem::%s " fmt,		\
 			__func__, ## args);		\
 })
 
@@ -684,7 +683,7 @@ static void * __init ___alloc_bootmem(unsigned long size, unsigned long align,
 	/*
 	 * Whoops, we cannot satisfy the allocation request.
 	 */
-	printk(KERN_ALERT "bootmem alloc of %lu bytes failed!\n", size);
+	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
 	return NULL;
 }
@@ -759,7 +758,7 @@ void * __init ___alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 	if (ptr)
 		return ptr;
 
-	printk(KERN_ALERT "bootmem alloc of %lu bytes failed!\n", size);
+	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
 	return NULL;
 }
* Unmerged path mm/dmapool.c
* Unmerged path mm/internal.h
* Unmerged path mm/kmemcheck.c
diff --git a/mm/kmemleak-test.c b/mm/kmemleak-test.c
index ff0d9779cec8..4c58b862dcf4 100644
--- a/mm/kmemleak-test.c
+++ b/mm/kmemleak-test.c
@@ -47,7 +47,7 @@ static int __init kmemleak_test_init(void)
 	struct test_node *elem;
 	int i;
 
-	printk(KERN_INFO "Kmemleak testing\n");
+	pr_info("Kmemleak testing\n");
 
 	/* make some orphan objects */
 	pr_info("kmemleak: kmalloc(32) = %p\n", kmalloc(32, GFP_KERNEL));
* Unmerged path mm/memory-failure.c
diff --git a/mm/memory.c b/mm/memory.c
index 4bb8ecbab1ee..ffb2dd4b7ac3 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -739,9 +739,8 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 			return;
 		}
 		if (nr_unshown) {
-			printk(KERN_ALERT
-				"BUG: Bad page map: %lu messages suppressed\n",
-				nr_unshown);
+			pr_alert("BUG: Bad page map: %lu messages suppressed\n",
+				 nr_unshown);
 			nr_unshown = 0;
 		}
 		nr_shown = 0;
@@ -752,15 +751,13 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 	mapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;
 	index = linear_page_index(vma, addr);
 
-	printk(KERN_ALERT
-		"BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\n",
-		current->comm,
-		(long long)pte_val(pte), (long long)pmd_val(*pmd));
+	pr_alert("BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\n",
+		 current->comm,
+		 (long long)pte_val(pte), (long long)pmd_val(*pmd));
 	if (page)
 		dump_page(page, "bad pte");
-	printk(KERN_ALERT
-		"addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%lx\n",
-		(void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
+	pr_alert("addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%lx\n",
+		 (void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
 	/*
 	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
 	 */
diff --git a/mm/mm_init.c b/mm/mm_init.c
index e5d8eaec6ee3..0762fda6c80d 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -53,13 +53,12 @@ void mminit_verify_zonelist(void)
 			/* Iterate the zonelist */
 			for_each_zone_zonelist(zone, z, zonelist, zoneid) {
 #ifdef CONFIG_NUMA
-				printk(KERN_CONT "%d:%s ",
-					zone->node, zone->name);
+				pr_cont("%d:%s ", zone->node, zone->name);
 #else
-				printk(KERN_CONT "0:%s ", zone->name);
+				pr_cont("0:%s ", zone->name);
 #endif /* CONFIG_NUMA */
 			}
-			printk(KERN_CONT "\n");
+			pr_cont("\n");
 		}
 	}
 }
diff --git a/mm/nobootmem.c b/mm/nobootmem.c
index 2a31a9ed47cf..feac32535ed2 100644
--- a/mm/nobootmem.c
+++ b/mm/nobootmem.c
@@ -272,7 +272,7 @@ static void * __init ___alloc_bootmem(unsigned long size, unsigned long align,
 	/*
 	 * Whoops, we cannot satisfy the allocation request.
 	 */
-	printk(KERN_ALERT "bootmem alloc of %lu bytes failed!\n", size);
+	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
 	return NULL;
 }
@@ -344,7 +344,7 @@ void * __init ___alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 	if (ptr)
 		return ptr;
 
-	printk(KERN_ALERT "bootmem alloc of %lu bytes failed!\n", size);
+	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
 	return NULL;
 }
* Unmerged path mm/page_alloc.c
* Unmerged path mm/page_io.c
diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 10e3d0b8a86d..0db94b748986 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -95,7 +95,7 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 
 	/* all units must be in a single group */
 	if (ai->nr_groups != 1) {
-		printk(KERN_CRIT "percpu: can't handle more than one groups\n");
+		pr_crit("percpu: can't handle more than one groups\n");
 		return -EINVAL;
 	}
 
@@ -103,8 +103,8 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 	alloc_pages = roundup_pow_of_two(nr_pages);
 
 	if (alloc_pages > nr_pages)
-		printk(KERN_WARNING "percpu: wasting %zu pages per chunk\n",
-		       alloc_pages - nr_pages);
+		pr_warn("percpu: wasting %zu pages per chunk\n",
+			alloc_pages - nr_pages);
 
 	return 0;
 }
diff --git a/mm/percpu.c b/mm/percpu.c
index 895c2996b902..caf0664c0e14 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -1476,20 +1476,20 @@ static void pcpu_dump_alloc_info(const char *lvl,
 		for (alloc_end += gi->nr_units / upa;
 		     alloc < alloc_end; alloc++) {
 			if (!(alloc % apl)) {
-				printk(KERN_CONT "\n");
+				pr_cont("\n");
 				printk("%spcpu-alloc: ", lvl);
 			}
-			printk(KERN_CONT "[%0*d] ", group_width, group);
+			pr_cont("[%0*d] ", group_width, group);
 
 			for (unit_end += upa; unit < unit_end; unit++)
 				if (gi->cpu_map[unit] != NR_CPUS)
-					printk(KERN_CONT "%0*d ", cpu_width,
-					       gi->cpu_map[unit]);
+					pr_cont("%0*d ",
+						cpu_width, gi->cpu_map[unit]);
 				else
-					printk(KERN_CONT "%s ", empty_str);
+					pr_cont("%s ", empty_str);
 		}
 	}
-	printk(KERN_CONT "\n");
+	pr_cont("\n");
 }
 
 /**
diff --git a/mm/shmem.c b/mm/shmem.c
index c56908222c8c..ffca6bb21f1e 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -3063,9 +3063,8 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 		if ((value = strchr(this_char,'=')) != NULL) {
 			*value++ = 0;
 		} else {
-			printk(KERN_ERR
-			    "tmpfs: No value for mount option '%s'\n",
-			    this_char);
+			pr_err("tmpfs: No value for mount option '%s'\n",
+			       this_char);
 			goto error;
 		}
 
@@ -3120,8 +3119,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 			if (mpol_parse_str(value, &mpol))
 				goto bad_val;
 		} else {
-			printk(KERN_ERR "tmpfs: Bad mount option %s\n",
-			       this_char);
+			pr_err("tmpfs: Bad mount option %s\n", this_char);
 			goto error;
 		}
 	}
@@ -3129,7 +3127,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 	return 0;
 
 bad_val:
-	printk(KERN_ERR "tmpfs: Bad value '%s' for mount option '%s'\n",
+	pr_err("tmpfs: Bad value '%s' for mount option '%s'\n",
 	       value, this_char);
 error:
 	mpol_put(mpol);
@@ -3531,14 +3529,14 @@ int __init shmem_init(void)
 
 	error = register_filesystem(&shmem_fs_type);
 	if (error) {
-		printk(KERN_ERR "Could not register tmpfs\n");
+		pr_err("Could not register tmpfs\n");
 		goto out2;
 	}
 
 	shm_mnt = kern_mount(&shmem_fs_type);
 	if (IS_ERR(shm_mnt)) {
 		error = PTR_ERR(shm_mnt);
-		printk(KERN_ERR "Could not kern_mount tmpfs\n");
+		pr_err("Could not kern_mount tmpfs\n");
 		goto out1;
 	}
 	return 0;
* Unmerged path mm/slab.c
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 288b69b9b33e..91fc29673b3d 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -286,7 +286,7 @@ out_unlock:
 			panic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",
 				name, err);
 		else {
-			printk(KERN_WARNING "kmem_cache_create(%s) failed with error %d",
+			pr_warn("kmem_cache_create(%s) failed with error %d\n",
 				name, err);
 			dump_stack();
 		}
* Unmerged path mm/sparse-vmemmap.c
* Unmerged path mm/sparse.c
* Unmerged path mm/swap_cgroup.c
