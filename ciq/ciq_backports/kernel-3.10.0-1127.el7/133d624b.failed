dma: Introduce dma_max_mapping_size()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Joerg Roedel <jroedel@suse.de>
commit 133d624b1cee16906134e92d5befb843b58bcf31
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/133d624b.failed

The function returns the maximum size that can be mapped
using DMA-API functions. The patch also adds the
implementation for direct DMA and a new dma_map_ops pointer
so that other implementations can expose their limit.

	Cc: stable@vger.kernel.org
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
(cherry picked from commit 133d624b1cee16906134e92d5befb843b58bcf31)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
#	kernel/dma/direct.c
#	kernel/dma/mapping.c
diff --cc include/linux/dma-mapping.h
index c162bfb08917,5b21f14802e1..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -63,23 -126,17 +63,27 @@@ struct dma_map_ops 
  	void (*sync_sg_for_device)(struct device *dev,
  				   struct scatterlist *sg, int nents,
  				   enum dma_data_direction dir);
 -	void (*cache_sync)(struct device *dev, void *vaddr, size_t size,
 -			enum dma_data_direction direction);
 +	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
  	int (*dma_supported)(struct device *dev, u64 mask);
 +	int (*set_dma_mask)(struct device *dev, u64 mask);
 +#ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
  	u64 (*get_required_mask)(struct device *dev);
++<<<<<<< HEAD
 +#endif
 +	int is_phys;
 +	RH_KABI_EXTEND(dma_addr_t (*map_resource)(struct device *dev, phys_addr_t phys_addr,
 +			       size_t size, enum dma_data_direction dir,
 +			       struct dma_attrs *attrs))
 +	RH_KABI_EXTEND(void (*unmap_resource)(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction dir,
 +			   struct dma_attrs *attrs))
++=======
+ 	size_t (*max_mapping_size)(struct device *dev);
++>>>>>>> 133d624b1cee (dma: Introduce dma_max_mapping_size())
  };
  
 -#define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
 -
 -extern const struct dma_map_ops dma_virt_ops;
 -extern const struct dma_map_ops dma_dummy_ops;
 +extern struct dma_map_ops dma_noop_ops;
 +extern struct dma_map_ops dma_virt_ops;
  
  #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
  
@@@ -97,12 -154,119 +101,121 @@@ static inline int is_device_dma_capable
  	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
+ /*
+  * These three functions are only for dma allocator.
+  * Don't use them in device drivers.
+  */
+ int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,
+ 				       dma_addr_t *dma_handle, void **ret);
+ int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr);
+ 
+ int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,
+ 			    void *cpu_addr, size_t size, int *ret);
+ 
+ void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle);
+ int dma_release_from_global_coherent(int order, void *vaddr);
+ int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,
+ 				  size_t size, int *ret);
+ 
+ #else
+ #define dma_alloc_from_dev_coherent(dev, size, handle, ret) (0)
+ #define dma_release_from_dev_coherent(dev, order, vaddr) (0)
+ #define dma_mmap_from_dev_coherent(dev, vma, vaddr, order, ret) (0)
+ 
+ static inline void *dma_alloc_from_global_coherent(ssize_t size,
+ 						   dma_addr_t *dma_handle)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int dma_release_from_global_coherent(int order, void *vaddr)
+ {
+ 	return 0;
+ }
+ 
+ static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,
+ 						void *cpu_addr, size_t size,
+ 						int *ret)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
+ 
+ static inline bool dma_is_direct(const struct dma_map_ops *ops)
+ {
+ 	return likely(!ops);
+ }
+ 
+ /*
+  * All the dma_direct_* declarations are here just for the indirect call bypass,
+  * and must not be used directly drivers!
+  */
+ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
+ 		unsigned long offset, size_t size, enum dma_data_direction dir,
+ 		unsigned long attrs);
+ int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+ 		enum dma_data_direction dir, unsigned long attrs);
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+ void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+ #else
+ static inline void dma_direct_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_direct_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ }
+ #endif
+ 
+ #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+     defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+     defined(CONFIG_SWIOTLB)
+ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs);
+ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs);
+ void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+ void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+ #else
+ static inline void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline void dma_direct_unmap_sg(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ }
+ static inline void dma_direct_sync_single_for_cpu(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+ {
+ }
+ #endif
+ 
+ size_t dma_direct_max_mapping_size(struct device *dev);
+ 
++>>>>>>> 133d624b1cee (dma: Introduce dma_max_mapping_size())
  #ifdef CONFIG_HAS_DMA
  #include <asm/dma-mapping.h>
 -
 -static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
  {
 -	if (dev && dev->dma_ops)
 -		return dev->dma_ops;
 +	if (dev && dev->device_rh && dev->device_rh->dma_ops)
 +		return dev->device_rh->dma_ops;
  	return get_arch_dma_ops(dev ? dev->bus : NULL);
  }
  
@@@ -335,15 -434,183 +448,191 @@@ dma_sync_sg_for_device(struct device *d
  
  }
  
++<<<<<<< HEAD
 +#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, NULL)
 +#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, NULL)
 +#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, NULL)
 +#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, NULL)
 +#define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, NULL)
 +#define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, NULL)
++=======
+ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+ {
+ 	debug_dma_mapping_error(dev, dma_addr);
+ 
+ 	if (dma_addr == DMA_MAPPING_ERROR)
+ 		return -ENOMEM;
+ 	return 0;
+ }
+ 
+ void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t flag, unsigned long attrs);
+ void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs);
+ void *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t gfp, unsigned long attrs);
+ void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
+ 		dma_addr_t dma_handle);
+ void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+ 		enum dma_data_direction dir);
+ int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs);
+ int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs);
+ int dma_supported(struct device *dev, u64 mask);
+ int dma_set_mask(struct device *dev, u64 mask);
+ int dma_set_coherent_mask(struct device *dev, u64 mask);
+ u64 dma_get_required_mask(struct device *dev);
+ size_t dma_max_mapping_size(struct device *dev);
+ #else /* CONFIG_HAS_DMA */
+ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+ 		struct page *page, size_t offset, size_t size,
+ 		enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return DMA_MAPPING_ERROR;
+ }
+ static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
+ 		int nents, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return 0;
+ }
+ static inline void dma_unmap_sg_attrs(struct device *dev,
+ 		struct scatterlist *sg, int nents, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ }
+ static inline dma_addr_t dma_map_resource(struct device *dev,
+ 		phys_addr_t phys_addr, size_t size, enum dma_data_direction dir,
+ 		unsigned long attrs)
+ {
+ 	return DMA_MAPPING_ERROR;
+ }
+ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ }
+ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_single_for_device(struct device *dev,
+ 		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_sg_for_cpu(struct device *dev,
+ 		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+ {
+ }
+ static inline void dma_sync_sg_for_device(struct device *dev,
+ 		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+ {
+ }
+ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+ {
+ 	return -ENOMEM;
+ }
+ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t flag, unsigned long attrs)
+ {
+ 	return NULL;
+ }
+ static void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ }
+ static inline void *dmam_alloc_attrs(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
+ {
+ 	return NULL;
+ }
+ static inline void dmam_free_coherent(struct device *dev, size_t size,
+ 		void *vaddr, dma_addr_t dma_handle)
+ {
+ }
+ static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ }
+ static inline int dma_get_sgtable_attrs(struct device *dev,
+ 		struct sg_table *sgt, void *cpu_addr, dma_addr_t dma_addr,
+ 		size_t size, unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ static inline int dma_supported(struct device *dev, u64 mask)
+ {
+ 	return 0;
+ }
+ static inline int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	return -EIO;
+ }
+ static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
+ {
+ 	return -EIO;
+ }
+ static inline u64 dma_get_required_mask(struct device *dev)
+ {
+ 	return 0;
+ }
+ static inline size_t dma_max_mapping_size(struct device *dev)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_HAS_DMA */
+ 
+ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	debug_dma_map_single(dev, ptr, size);
+ 	return dma_map_page_attrs(dev, virt_to_page(ptr), offset_in_page(ptr),
+ 			size, dir, attrs);
+ }
+ 
+ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	return dma_unmap_page_attrs(dev, addr, size, dir, attrs);
+ }
+ 
+ static inline void dma_sync_single_range_for_cpu(struct device *dev,
+ 		dma_addr_t addr, unsigned long offset, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ 	return dma_sync_single_for_cpu(dev, addr + offset, size, dir);
+ }
+ 
+ static inline void dma_sync_single_range_for_device(struct device *dev,
+ 		dma_addr_t addr, unsigned long offset, size_t size,
+ 		enum dma_data_direction dir)
+ {
+ 	return dma_sync_single_for_device(dev, addr + offset, size, dir);
+ }
+ 
+ #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
+ #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
+ #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
+ #define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
+ #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
+ #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
+ #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
+ #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
++>>>>>>> 133d624b1cee (dma: Introduce dma_max_mapping_size())
  
  extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 -		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 -		unsigned long attrs);
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
  
  void *dma_common_contiguous_remap(struct page *page, size_t size,
  			unsigned long vm_flags,
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/mapping.c
diff --git a/Documentation/DMA-API.txt b/Documentation/DMA-API.txt
index bed1e451d511..a2dfb340bbc2 100644
--- a/Documentation/DMA-API.txt
+++ b/Documentation/DMA-API.txt
@@ -178,6 +178,14 @@ Requesting the required mask does not alter the current mask.  If you
 wish to take advantage of it, you should issue a dma_set_mask()
 call to set the mask to the value returned.
 
+::
+
+	size_t
+	dma_direct_max_mapping_size(struct device *dev);
+
+Returns the maximum size of a mapping for the device. The size parameter
+of the mapping functions like dma_map_single(), dma_map_page() and
+others should not be larger than the returned value.
 
 Part Id - Streaming DMA mappings
 --------------------------------
* Unmerged path include/linux/dma-mapping.h
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/mapping.c
