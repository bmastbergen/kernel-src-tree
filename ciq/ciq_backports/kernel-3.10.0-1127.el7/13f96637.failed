percpu: skip chunks if the alloc does not fit in the contig hint

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 13f966373f9296c0da2fb2764654cce520b3a6b4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/13f96637.failed

This patch adds chunk->contig_bits_start to keep track of the contig
hint's offset and the check to skip the chunk if it does not fit. If
the chunk's contig hint starting offset cannot satisfy an allocation,
the allocator assumes there is enough memory pressure in this chunk to
either use a different chunk or create a new one. This accepts a less
tight packing for a smoother latency curve.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 13f966373f9296c0da2fb2764654cce520b3a6b4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu-internal.h
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,734745a0c9b6..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -428,228 -395,353 +428,459 @@@ static int pcpu_need_to_extend(struct p
  }
  
  /**
 - * pcpu_chunk_update - updates the chunk metadata given a free area
 + * pcpu_extend_area_map - extend area map of a chunk
   * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of free area
 + * @new_alloc: new target allocation length of the area map
   *
++<<<<<<< HEAD
 + * Extend area map of @chunk to have @new_alloc entries.
 + *
 + * CONTEXT:
 + * Does GFP_KERNEL allocation.  Grabs and releases pcpu_lock.
++=======
+  * This updates the chunk's contig hint and starting offset given a free area.
+  */
+ static void pcpu_chunk_update(struct pcpu_chunk *chunk, int bit_off, int bits)
+ {
+ 	if (bits > chunk->contig_bits) {
+ 		chunk->contig_bits_start = bit_off;
+ 		chunk->contig_bits = bits;
+ 	}
+ }
+ 
+ /**
+  * pcpu_chunk_refresh_hint - updates metadata about a chunk
+  * @chunk: chunk of interest
+  *
+  * Iterates over the chunk to find the largest free area.
+  *
+  * Updates:
+  *      chunk->contig_bits
+  *      chunk->contig_bits_start
+  *      nr_empty_pop_pages
+  */
+ static void pcpu_chunk_refresh_hint(struct pcpu_chunk *chunk)
+ {
+ 	int bits, nr_empty_pop_pages;
+ 	int rs, re; /* region start, region end */
+ 
+ 	/* clear metadata */
+ 	chunk->contig_bits = 0;
+ 
+ 	bits = nr_empty_pop_pages = 0;
+ 	pcpu_for_each_unpop_region(chunk->alloc_map, rs, re, chunk->first_bit,
+ 				   pcpu_chunk_map_bits(chunk)) {
+ 		bits = re - rs;
+ 
+ 		pcpu_chunk_update(chunk, rs, bits);
+ 
+ 		nr_empty_pop_pages += pcpu_cnt_pop_pages(chunk, rs, bits);
+ 	}
+ 
+ 	/*
+ 	 * Keep track of nr_empty_pop_pages.
+ 	 *
+ 	 * The chunk maintains the previous number of free pages it held,
+ 	 * so the delta is used to update the global counter.  The reserved
+ 	 * chunk is not part of the free page count as they are populated
+ 	 * at init and are special to serving reserved allocations.
+ 	 */
+ 	if (chunk != pcpu_reserved_chunk)
+ 		pcpu_nr_empty_pop_pages +=
+ 			(nr_empty_pop_pages - chunk->nr_empty_pop_pages);
+ 
+ 	chunk->nr_empty_pop_pages = nr_empty_pop_pages;
+ }
+ 
+ /**
+  * pcpu_block_update - updates a block given a free area
+  * @block: block of interest
+  * @start: start offset in block
+  * @end: end offset in block
+  *
+  * Updates a block given a known free area.  The region [start, end) is
+  * expected to be the entirety of the free area within a block.
+  */
+ static void pcpu_block_update(struct pcpu_block_md *block, int start, int end)
+ {
+ 	int contig = end - start;
+ 
+ 	block->first_free = min(block->first_free, start);
+ 	if (start == 0)
+ 		block->left_free = contig;
+ 
+ 	if (end == PCPU_BITMAP_BLOCK_BITS)
+ 		block->right_free = contig;
+ 
+ 	if (contig > block->contig_hint) {
+ 		block->contig_hint_start = start;
+ 		block->contig_hint = contig;
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_refresh_hint
+  * @chunk: chunk of interest
+  * @index: index of the metadata block
+  *
+  * Scans over the block beginning at first_free and updates the block
+  * metadata accordingly.
+  */
+ static void pcpu_block_refresh_hint(struct pcpu_chunk *chunk, int index)
+ {
+ 	struct pcpu_block_md *block = chunk->md_blocks + index;
+ 	unsigned long *alloc_map = pcpu_index_alloc_map(chunk, index);
+ 	int rs, re;	/* region start, region end */
+ 
+ 	/* clear hints */
+ 	block->contig_hint = 0;
+ 	block->left_free = block->right_free = 0;
+ 
+ 	/* iterate over free areas and update the contig hints */
+ 	pcpu_for_each_unpop_region(alloc_map, rs, re, block->first_free,
+ 				   PCPU_BITMAP_BLOCK_BITS) {
+ 		pcpu_block_update(block, rs, re);
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_update_hint_alloc - update hint on allocation path
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of request
+  */
+ static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,
+ 					 int bits)
+ {
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/*
+ 	 * Update s_block.
+ 	 */
+ 	pcpu_block_refresh_hint(chunk, s_index);
+ 
+ 	/*
+ 	 * Update e_block.
+ 	 */
+ 	if (s_index != e_index) {
+ 		pcpu_block_refresh_hint(chunk, e_index);
+ 
+ 		/* update in-between md_blocks */
+ 		for (block = s_block + 1; block < e_block; block++) {
+ 			block->contig_hint = 0;
+ 			block->left_free = 0;
+ 			block->right_free = 0;
+ 		}
+ 	}
+ 
+ 	pcpu_chunk_refresh_hint(chunk);
+ }
+ 
+ /**
+  * pcpu_block_update_hint_free - updates the block hints on the free path
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of request
+  */
+ static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
+ 					int bits)
+ {
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/* update s_block */
+ 	pcpu_block_refresh_hint(chunk, s_index);
+ 
+ 	/* freeing in the same block */
+ 	if (s_index != e_index) {
+ 		/* update e_block */
+ 		pcpu_block_refresh_hint(chunk, e_index);
+ 
+ 		/* reset md_blocks in the middle */
+ 		for (block = s_block + 1; block < e_block; block++) {
+ 			block->first_free = 0;
+ 			block->contig_hint_start = 0;
+ 			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
+ 			block->left_free = PCPU_BITMAP_BLOCK_BITS;
+ 			block->right_free = PCPU_BITMAP_BLOCK_BITS;
+ 		}
+ 	}
+ 
+ 	pcpu_chunk_refresh_hint(chunk);
+ }
+ 
+ /**
+  * pcpu_is_populated - determines if the region is populated
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of area
+  * @next_off: return value for the next offset to start searching
+  *
+  * For atomic allocations, check if the backing pages are populated.
++>>>>>>> 13f966373f92 (percpu: skip chunks if the alloc does not fit in the contig hint)
   *
   * RETURNS:
 - * Bool if the backing pages are populated.
 - * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
 + * 0 on success, -errno on failure.
   */
 -static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
 -			      int *next_off)
 +static int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)
  {
 -	int page_start, page_end, rs, re;
 +	int *old = NULL, *new = NULL;
 +	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
 +	unsigned long flags;
  
 -	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
 -	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
 +	lockdep_assert_held(&pcpu_alloc_mutex);
  
 -	rs = page_start;
 -	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
 -	if (rs >= page_end)
 -		return true;
 +	new = pcpu_mem_zalloc(new_size);
 +	if (!new)
 +		return -ENOMEM;
  
 -	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
 -	return false;
 +	/* acquire pcpu_lock and switch to new area map */
 +	spin_lock_irqsave(&pcpu_lock, flags);
 +
 +	if (new_alloc <= chunk->map_alloc)
 +		goto out_unlock;
 +
 +	old_size = chunk->map_alloc * sizeof(chunk->map[0]);
 +	old = chunk->map;
 +
 +	memcpy(new, old, old_size);
 +
 +	chunk->map_alloc = new_alloc;
 +	chunk->map = new;
 +	new = NULL;
 +
 +out_unlock:
 +	spin_unlock_irqrestore(&pcpu_lock, flags);
 +
 +	/*
 +	 * pcpu_mem_free() might end up calling vfree() which uses
 +	 * IRQ-unsafe lock and thus can't be called under pcpu_lock.
 +	 */
 +	pcpu_mem_free(old, old_size);
 +	pcpu_mem_free(new, new_size);
 +
 +	return 0;
 +}
 +
 +/**
 + * pcpu_fit_in_area - try to fit the requested allocation in a candidate area
 + * @chunk: chunk the candidate area belongs to
 + * @off: the offset to the start of the candidate area
 + * @this_size: the size of the candidate area
 + * @size: the size of the target allocation
 + * @align: the alignment of the target allocation
 + * @pop_only: only allocate from already populated region
 + *
 + * We're trying to allocate @size bytes aligned at @align.  @chunk's area
 + * at @off sized @this_size is a candidate.  This function determines
 + * whether the target allocation fits in the candidate area and returns the
 + * number of bytes to pad after @off.  If the target area doesn't fit, -1
 + * is returned.
 + *
 + * If @pop_only is %true, this function only considers the already
 + * populated part of the candidate area.
 + */
 +static int pcpu_fit_in_area(struct pcpu_chunk *chunk, int off, int this_size,
 +			    int size, int align, bool pop_only)
 +{
 +	int cand_off = off;
 +
 +	while (true) {
 +		int head = ALIGN(cand_off, align) - off;
 +		int page_start, page_end, rs, re;
 +
 +		if (this_size < head + size)
 +			return -1;
 +
 +		if (!pop_only)
 +			return head;
 +
 +		/*
 +		 * If the first unpopulated page is beyond the end of the
 +		 * allocation, the whole allocation is populated;
 +		 * otherwise, retry from the end of the unpopulated area.
 +		 */
 +		page_start = PFN_DOWN(head + off);
 +		page_end = PFN_UP(head + off + size);
 +
 +		rs = page_start;
 +		pcpu_next_unpop(chunk, &rs, &re, PFN_UP(off + this_size));
 +		if (rs >= page_end)
 +			return head;
 +		cand_off = re * PAGE_SIZE;
 +	}
  }
  
  /**
 - * pcpu_find_block_fit - finds the block index to start searching
 + * pcpu_alloc_area - allocate area from a pcpu_chunk
   * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE bytes)
 - * @pop_only: use populated regions only
 + * @size: wanted size in bytes
 + * @align: wanted align
 + * @pop_only: allocate only from the populated area
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Try to allocate @size bytes area aligned at @align from @chunk.
 + * Note that this function only allocates the offset.  It doesn't
 + * populate or map the area.
 + *
 + * @chunk->map must have at least two free slots.
 + *
 + * CONTEXT:
 + * pcpu_lock.
   *
   * RETURNS:
 - * The offset in the bitmap to begin searching.
 - * -1 if no offset is found.
 + * Allocated offset in @chunk on success, -1 if no matching area is
 + * found.
   */
 -static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
 -			       size_t align, bool pop_only)
 +static int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align,
 +			   bool pop_only, int *occ_pages_p)
  {
 -	int bit_off, bits;
 -	int re; /* region end */
 -
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int max_contig = 0;
 +	int i, off;
 +	bool seen_free = false;
 +	int *p;
 +
++<<<<<<< HEAD
 +	for (i = chunk->first_free, p = chunk->map + i; i < chunk->map_used; i++, p++) {
 +		int head, tail;
 +		int this_size;
++=======
+ 	/*
+ 	 * Check to see if the allocation can fit in the chunk's contig hint.
+ 	 * This is an optimization to prevent scanning by assuming if it
+ 	 * cannot fit in the global hint, there is memory pressure and creating
+ 	 * a new chunk would happen soon.
+ 	 */
+ 	bit_off = ALIGN(chunk->contig_bits_start, align) -
+ 		  chunk->contig_bits_start;
+ 	if (bit_off + alloc_bits > chunk->contig_bits)
+ 		return -1;
+ 
+ 	pcpu_for_each_unpop_region(chunk->alloc_map, bit_off, re,
+ 				   chunk->first_bit,
+ 				   pcpu_chunk_map_bits(chunk)) {
+ 		bits = re - bit_off;
++>>>>>>> 13f966373f92 (percpu: skip chunks if the alloc does not fit in the contig hint)
  
 -		/* check alignment */
 -		bits -= ALIGN(bit_off, align) - bit_off;
 -		bit_off = ALIGN(bit_off, align);
 -		if (bits < alloc_bits)
 +		off = *p;
 +		if (off & 1)
  			continue;
  
 -		bits = alloc_bits;
 -		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
 -						   &bit_off))
 -			break;
 +		this_size = (p[1] & ~1) - off;
  
 -		bits = 0;
 -	}
 -
 -	if (bit_off == pcpu_chunk_map_bits(chunk))
 -		return -1;
 -
 -	return bit_off;
 -}
 -
 -/**
 - * pcpu_alloc_area - allocates an area from a pcpu_chunk
 - * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE)
 - * @start: bit_off to start searching
 - *
 - * This function takes in a @start offset to begin searching to fit an
 - * allocation of @alloc_bits with alignment @align.  If it confirms a
 - * valid free area, it then updates the allocation and boundary maps
 - * accordingly.
 - *
 - * RETURNS:
 - * Allocated addr offset in @chunk on success.
 - * -1 if no matching area is found.
 - */
 -static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 -			   size_t align, int start)
 -{
 -	size_t align_mask = (align) ? (align - 1) : 0;
 -	int bit_off, end, oslot;
 -
 -	lockdep_assert_held(&pcpu_lock);
 +		head = pcpu_fit_in_area(chunk, off, this_size, size, align,
 +					pop_only);
 +		if (head < 0) {
 +			if (!seen_free) {
 +				chunk->first_free = i;
 +				seen_free = true;
 +			}
 +			max_contig = max(this_size, max_contig);
 +			continue;
 +		}
  
 -	oslot = pcpu_chunk_slot(chunk);
 +		/*
 +		 * If head is small or the previous block is free,
 +		 * merge'em.  Note that 'small' is defined as smaller
 +		 * than sizeof(int), which is very small but isn't too
 +		 * uncommon for percpu allocations.
 +		 */
 +		if (head && (head < sizeof(int) || !(p[-1] & 1))) {
 +			*p = off += head;
 +			if (p[-1] & 1)
 +				chunk->free_size -= head;
 +			else
 +				max_contig = max(*p - p[-1], max_contig);
 +			this_size -= head;
 +			head = 0;
 +		}
  
 -	/*
 -	 * Search to find a fit.
 -	 */
 -	end = start + alloc_bits;
 -	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
 -					     alloc_bits, align_mask);
 -	if (bit_off >= end)
 -		return -1;
 +		/* if tail is small, just keep it around */
 +		tail = this_size - head - size;
 +		if (tail < sizeof(int)) {
 +			tail = 0;
 +			size = this_size - head;
 +		}
  
 -	/* update alloc map */
 -	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
 +		/* split if warranted */
 +		if (head || tail) {
 +			int nr_extra = !!head + !!tail;
 +
 +			/* insert new subblocks */
 +			memmove(p + nr_extra + 1, p + 1,
 +				sizeof(chunk->map[0]) * (chunk->map_used - i));
 +			chunk->map_used += nr_extra;
 +
 +			if (head) {
 +				if (!seen_free) {
 +					chunk->first_free = i;
 +					seen_free = true;
 +				}
 +				*++p = off += head;
 +				++i;
 +				max_contig = max(head, max_contig);
 +			}
 +			if (tail) {
 +				p[1] = off + size;
 +				max_contig = max(tail, max_contig);
 +			}
 +		}
  
 -	/* update boundary map */
 -	set_bit(bit_off, chunk->bound_map);
 -	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
 -	set_bit(bit_off + alloc_bits, chunk->bound_map);
 +		if (!seen_free)
 +			chunk->first_free = i + 1;
  
 -	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
 +		/* update hint and mark allocated */
 +		if (i + 1 == chunk->map_used)
 +			chunk->contig_hint = max_contig; /* fully scanned */
 +		else
 +			chunk->contig_hint = max(chunk->contig_hint,
 +						 max_contig);
  
 -	/* update first free bit */
 -	if (bit_off == chunk->first_bit)
 -		chunk->first_bit = find_next_zero_bit(
 -					chunk->alloc_map,
 -					pcpu_chunk_map_bits(chunk),
 -					bit_off + alloc_bits);
 +		chunk->free_size -= size;
 +		*p |= 1;
  
 -	pcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);
 +		*occ_pages_p = pcpu_count_occupied_pages(chunk, i);
 +		pcpu_chunk_relocate(chunk, oslot);
 +		return off;
 +	}
  
 +	chunk->contig_hint = max_contig;	/* fully scanned */
  	pcpu_chunk_relocate(chunk, oslot);
  
 -	return bit_off * PCPU_MIN_ALLOC_SIZE;
 +	/* tell the upper layer that this chunk has no matching area */
 +	return -1;
  }
  
  /**
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu-internal.h
* Unmerged path mm/percpu.c
