percpu: use block scan_hint to only scan forward

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou <dennis@kernel.org>
commit da3afdd5bb5428fd38b4b64f2d5e897c3bb78354
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/da3afdd5.failed

Blocks now remember the latest scan_hint. This can be used on the
allocation path as when a contig_hint is broken, we can promote the
scan_hint to the contig_hint and scan forward from there. This works
because pcpu_block_refresh_hint() is only called on the allocation path
while block free regions are updated manually in
pcpu_block_update_hint_free().

	Signed-off-by: Dennis Zhou <dennis@kernel.org>
(cherry picked from commit da3afdd5bb5428fd38b4b64f2d5e897c3bb78354)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 60a1f468f968,c5250e162d4d..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -370,12 -553,364 +370,313 @@@ static void pcpu_chunk_relocate(struct 
  {
  	int nslot = pcpu_chunk_slot(chunk);
  
++<<<<<<< HEAD
 +	if (chunk != pcpu_reserved_chunk && oslot != nslot) {
 +		if (oslot < nslot)
 +			list_move(&chunk->list, &pcpu_slot[nslot]);
++=======
+ 	if (oslot != nslot)
+ 		__pcpu_chunk_move(chunk, nslot, oslot < nslot);
+ }
+ 
+ /*
+  * pcpu_update_empty_pages - update empty page counters
+  * @chunk: chunk of interest
+  * @nr: nr of empty pages
+  *
+  * This is used to keep track of the empty pages now based on the premise
+  * a md_block covers a page.  The hint update functions recognize if a block
+  * is made full or broken to calculate deltas for keeping track of free pages.
+  */
+ static inline void pcpu_update_empty_pages(struct pcpu_chunk *chunk, int nr)
+ {
+ 	chunk->nr_empty_pop_pages += nr;
+ 	if (chunk != pcpu_reserved_chunk)
+ 		pcpu_nr_empty_pop_pages += nr;
+ }
+ 
+ /*
+  * pcpu_region_overlap - determines if two regions overlap
+  * @a: start of first region, inclusive
+  * @b: end of first region, exclusive
+  * @x: start of second region, inclusive
+  * @y: end of second region, exclusive
+  *
+  * This is used to determine if the hint region [a, b) overlaps with the
+  * allocated region [x, y).
+  */
+ static inline bool pcpu_region_overlap(int a, int b, int x, int y)
+ {
+ 	return (a < y) && (x < b);
+ }
+ 
+ /**
+  * pcpu_chunk_update - updates the chunk metadata given a free area
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of free area
+  *
+  * This updates the chunk's contig hint and starting offset given a free area.
+  * Choose the best starting offset if the contig hint is equal.
+  */
+ static void pcpu_chunk_update(struct pcpu_chunk *chunk, int bit_off, int bits)
+ {
+ 	if (bits > chunk->contig_bits) {
+ 		chunk->contig_bits_start = bit_off;
+ 		chunk->contig_bits = bits;
+ 	} else if (bits == chunk->contig_bits && chunk->contig_bits_start &&
+ 		   (!bit_off ||
+ 		    __ffs(bit_off) > __ffs(chunk->contig_bits_start))) {
+ 		/* use the start with the best alignment */
+ 		chunk->contig_bits_start = bit_off;
+ 	}
+ }
+ 
+ /**
+  * pcpu_chunk_refresh_hint - updates metadata about a chunk
+  * @chunk: chunk of interest
+  *
+  * Iterates over the metadata blocks to find the largest contig area.
+  * It also counts the populated pages and uses the delta to update the
+  * global count.
+  *
+  * Updates:
+  *      chunk->contig_bits
+  *      chunk->contig_bits_start
+  */
+ static void pcpu_chunk_refresh_hint(struct pcpu_chunk *chunk)
+ {
+ 	int bit_off, bits;
+ 
+ 	/* clear metadata */
+ 	chunk->contig_bits = 0;
+ 
+ 	bit_off = chunk->first_bit;
+ 	bits = 0;
+ 	pcpu_for_each_md_free_region(chunk, bit_off, bits) {
+ 		pcpu_chunk_update(chunk, bit_off, bits);
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_update - updates a block given a free area
+  * @block: block of interest
+  * @start: start offset in block
+  * @end: end offset in block
+  *
+  * Updates a block given a known free area.  The region [start, end) is
+  * expected to be the entirety of the free area within a block.  Chooses
+  * the best starting offset if the contig hints are equal.
+  */
+ static void pcpu_block_update(struct pcpu_block_md *block, int start, int end)
+ {
+ 	int contig = end - start;
+ 
+ 	block->first_free = min(block->first_free, start);
+ 	if (start == 0)
+ 		block->left_free = contig;
+ 
+ 	if (end == PCPU_BITMAP_BLOCK_BITS)
+ 		block->right_free = contig;
+ 
+ 	if (contig > block->contig_hint) {
+ 		/* promote the old contig_hint to be the new scan_hint */
+ 		if (start > block->contig_hint_start) {
+ 			if (block->contig_hint > block->scan_hint) {
+ 				block->scan_hint_start =
+ 					block->contig_hint_start;
+ 				block->scan_hint = block->contig_hint;
+ 			} else if (start < block->scan_hint_start) {
+ 				/*
+ 				 * The old contig_hint == scan_hint.  But, the
+ 				 * new contig is larger so hold the invariant
+ 				 * scan_hint_start < contig_hint_start.
+ 				 */
+ 				block->scan_hint = 0;
+ 			}
+ 		} else {
+ 			block->scan_hint = 0;
+ 		}
+ 		block->contig_hint_start = start;
+ 		block->contig_hint = contig;
+ 	} else if (contig == block->contig_hint) {
+ 		if (block->contig_hint_start &&
+ 		    (!start ||
+ 		     __ffs(start) > __ffs(block->contig_hint_start))) {
+ 			/* start has a better alignment so use it */
+ 			block->contig_hint_start = start;
+ 			if (start < block->scan_hint_start &&
+ 			    block->contig_hint > block->scan_hint)
+ 				block->scan_hint = 0;
+ 		} else if (start > block->scan_hint_start ||
+ 			   block->contig_hint > block->scan_hint) {
+ 			/*
+ 			 * Knowing contig == contig_hint, update the scan_hint
+ 			 * if it is farther than or larger than the current
+ 			 * scan_hint.
+ 			 */
+ 			block->scan_hint_start = start;
+ 			block->scan_hint = contig;
+ 		}
+ 	} else {
+ 		/*
+ 		 * The region is smaller than the contig_hint.  So only update
+ 		 * the scan_hint if it is larger than or equal and farther than
+ 		 * the current scan_hint.
+ 		 */
+ 		if ((start < block->contig_hint_start &&
+ 		     (contig > block->scan_hint ||
+ 		      (contig == block->scan_hint &&
+ 		       start > block->scan_hint_start)))) {
+ 			block->scan_hint_start = start;
+ 			block->scan_hint = contig;
+ 		}
+ 	}
+ }
+ 
+ /*
+  * pcpu_block_update_scan - update a block given a free area from a scan
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of free area
+  *
+  * Finding the final allocation spot first goes through pcpu_find_block_fit()
+  * to find a block that can hold the allocation and then pcpu_alloc_area()
+  * where a scan is used.  When allocations require specific alignments,
+  * we can inadvertently create holes which will not be seen in the alloc
+  * or free paths.
+  *
+  * This takes a given free area hole and updates a block as it may change the
+  * scan_hint.  We need to scan backwards to ensure we don't miss free bits
+  * from alignment.
+  */
+ static void pcpu_block_update_scan(struct pcpu_chunk *chunk, int bit_off,
+ 				   int bits)
+ {
+ 	int s_off = pcpu_off_to_block_off(bit_off);
+ 	int e_off = s_off + bits;
+ 	int s_index, l_bit;
+ 	struct pcpu_block_md *block;
+ 
+ 	if (e_off > PCPU_BITMAP_BLOCK_BITS)
+ 		return;
+ 
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	block = chunk->md_blocks + s_index;
+ 
+ 	/* scan backwards in case of alignment skipping free bits */
+ 	l_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index), s_off);
+ 	s_off = (s_off == l_bit) ? 0 : l_bit + 1;
+ 
+ 	pcpu_block_update(block, s_off, e_off);
+ }
+ 
+ /**
+  * pcpu_block_refresh_hint
+  * @chunk: chunk of interest
+  * @index: index of the metadata block
+  *
+  * Scans over the block beginning at first_free and updates the block
+  * metadata accordingly.
+  */
+ static void pcpu_block_refresh_hint(struct pcpu_chunk *chunk, int index)
+ {
+ 	struct pcpu_block_md *block = chunk->md_blocks + index;
+ 	unsigned long *alloc_map = pcpu_index_alloc_map(chunk, index);
+ 	int rs, re, start;	/* region start, region end */
+ 
+ 	/* promote scan_hint to contig_hint */
+ 	if (block->scan_hint) {
+ 		start = block->scan_hint_start + block->scan_hint;
+ 		block->contig_hint_start = block->scan_hint_start;
+ 		block->contig_hint = block->scan_hint;
+ 		block->scan_hint = 0;
+ 	} else {
+ 		start = block->first_free;
+ 		block->contig_hint = 0;
+ 	}
+ 
+ 	block->right_free = 0;
+ 
+ 	/* iterate over free areas and update the contig hints */
+ 	pcpu_for_each_unpop_region(alloc_map, rs, re, start,
+ 				   PCPU_BITMAP_BLOCK_BITS) {
+ 		pcpu_block_update(block, rs, re);
+ 	}
+ }
+ 
+ /**
+  * pcpu_block_update_hint_alloc - update hint on allocation path
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of request
+  *
+  * Updates metadata for the allocation path.  The metadata only has to be
+  * refreshed by a full scan iff the chunk's contig hint is broken.  Block level
+  * scans are required if the block's contig hint is broken.
+  */
+ static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,
+ 					 int bits)
+ {
+ 	int nr_empty_pages = 0;
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/*
+ 	 * Update s_block.
+ 	 * block->first_free must be updated if the allocation takes its place.
+ 	 * If the allocation breaks the contig_hint, a scan is required to
+ 	 * restore this hint.
+ 	 */
+ 	if (s_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)
+ 		nr_empty_pages++;
+ 
+ 	if (s_off == s_block->first_free)
+ 		s_block->first_free = find_next_zero_bit(
+ 					pcpu_index_alloc_map(chunk, s_index),
+ 					PCPU_BITMAP_BLOCK_BITS,
+ 					s_off + bits);
+ 
+ 	if (pcpu_region_overlap(s_block->scan_hint_start,
+ 				s_block->scan_hint_start + s_block->scan_hint,
+ 				s_off,
+ 				s_off + bits))
+ 		s_block->scan_hint = 0;
+ 
+ 	if (pcpu_region_overlap(s_block->contig_hint_start,
+ 				s_block->contig_hint_start +
+ 				s_block->contig_hint,
+ 				s_off,
+ 				s_off + bits)) {
+ 		/* block contig hint is broken - scan to fix it */
+ 		if (!s_off)
+ 			s_block->left_free = 0;
+ 		pcpu_block_refresh_hint(chunk, s_index);
+ 	} else {
+ 		/* update left and right contig manually */
+ 		s_block->left_free = min(s_block->left_free, s_off);
+ 		if (s_index == e_index)
+ 			s_block->right_free = min_t(int, s_block->right_free,
+ 					PCPU_BITMAP_BLOCK_BITS - e_off);
++>>>>>>> da3afdd5bb54 (percpu: use block scan_hint to only scan forward)
  		else
 -			s_block->right_free = 0;
 +			list_move_tail(&chunk->list, &pcpu_slot[nslot]);
  	}
 -
 -	/*
 -	 * Update e_block.
 -	 */
 -	if (s_index != e_index) {
 -		if (e_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)
 -			nr_empty_pages++;
 -
 -		/*
 -		 * When the allocation is across blocks, the end is along
 -		 * the left part of the e_block.
 -		 */
 -		e_block->first_free = find_next_zero_bit(
 -				pcpu_index_alloc_map(chunk, e_index),
 -				PCPU_BITMAP_BLOCK_BITS, e_off);
 -
 -		if (e_off == PCPU_BITMAP_BLOCK_BITS) {
 -			/* reset the block */
 -			e_block++;
 -		} else {
 -			if (e_off > e_block->scan_hint_start)
 -				e_block->scan_hint = 0;
 -
 -			e_block->left_free = 0;
 -			if (e_off > e_block->contig_hint_start) {
 -				/* contig hint is broken - scan to fix it */
 -				pcpu_block_refresh_hint(chunk, e_index);
 -			} else {
 -				e_block->right_free =
 -					min_t(int, e_block->right_free,
 -					      PCPU_BITMAP_BLOCK_BITS - e_off);
 -			}
 -		}
 -
 -		/* update in-between md_blocks */
 -		nr_empty_pages += (e_index - s_index - 1);
 -		for (block = s_block + 1; block < e_block; block++) {
 -			block->scan_hint = 0;
 -			block->contig_hint = 0;
 -			block->left_free = 0;
 -			block->right_free = 0;
 -		}
 -	}
 -
 -	if (nr_empty_pages)
 -		pcpu_update_empty_pages(chunk, -nr_empty_pages);
 -
 -	/*
 -	 * The only time a full chunk scan is required is if the chunk
 -	 * contig hint is broken.  Otherwise, it means a smaller space
 -	 * was used and therefore the chunk contig hint is still correct.
 -	 */
 -	if (pcpu_region_overlap(chunk->contig_bits_start,
 -				chunk->contig_bits_start + chunk->contig_bits,
 -				bit_off,
 -				bit_off + bits))
 -		pcpu_chunk_refresh_hint(chunk);
  }
  
  /**
@@@ -429,228 -1027,258 +730,239 @@@ static int pcpu_need_to_extend(struct p
  }
  
  /**
 - * pcpu_is_populated - determines if the region is populated
 + * pcpu_extend_area_map - extend area map of a chunk
   * @chunk: chunk of interest
 - * @bit_off: chunk offset
 - * @bits: size of area
 - * @next_off: return value for the next offset to start searching
 + * @new_alloc: new target allocation length of the area map
 + *
 + * Extend area map of @chunk to have @new_alloc entries.
   *
 - * For atomic allocations, check if the backing pages are populated.
 + * CONTEXT:
 + * Does GFP_KERNEL allocation.  Grabs and releases pcpu_lock.
   *
   * RETURNS:
 - * Bool if the backing pages are populated.
 - * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
 + * 0 on success, -errno on failure.
   */
 -static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
 -			      int *next_off)
 +static int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)
  {
 -	int page_start, page_end, rs, re;
 +	int *old = NULL, *new = NULL;
 +	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
 +	unsigned long flags;
  
 -	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
 -	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
 +	lockdep_assert_held(&pcpu_alloc_mutex);
  
 -	rs = page_start;
 -	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
 -	if (rs >= page_end)
 -		return true;
 +	new = pcpu_mem_zalloc(new_size);
 +	if (!new)
 +		return -ENOMEM;
  
 -	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
 -	return false;
 -}
 +	/* acquire pcpu_lock and switch to new area map */
 +	spin_lock_irqsave(&pcpu_lock, flags);
  
 -/**
 - * pcpu_find_block_fit - finds the block index to start searching
 - * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE bytes)
 - * @pop_only: use populated regions only
 - *
 - * Given a chunk and an allocation spec, find the offset to begin searching
 - * for a free region.  This iterates over the bitmap metadata blocks to
 - * find an offset that will be guaranteed to fit the requirements.  It is
 - * not quite first fit as if the allocation does not fit in the contig hint
 - * of a block or chunk, it is skipped.  This errs on the side of caution
 - * to prevent excess iteration.  Poor alignment can cause the allocator to
 - * skip over blocks and chunks that have valid free areas.
 - *
 - * RETURNS:
 - * The offset in the bitmap to begin searching.
 - * -1 if no offset is found.
 - */
 -static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
 -			       size_t align, bool pop_only)
 -{
 -	int bit_off, bits, next_off;
 +	if (new_alloc <= chunk->map_alloc)
 +		goto out_unlock;
  
 -	/*
 -	 * Check to see if the allocation can fit in the chunk's contig hint.
 -	 * This is an optimization to prevent scanning by assuming if it
 -	 * cannot fit in the global hint, there is memory pressure and creating
 -	 * a new chunk would happen soon.
 -	 */
 -	bit_off = ALIGN(chunk->contig_bits_start, align) -
 -		  chunk->contig_bits_start;
 -	if (bit_off + alloc_bits > chunk->contig_bits)
 -		return -1;
 +	old_size = chunk->map_alloc * sizeof(chunk->map[0]);
 +	old = chunk->map;
  
 -	bit_off = chunk->first_bit;
 -	bits = 0;
 -	pcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits) {
 -		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
 -						   &next_off))
 -			break;
 +	memcpy(new, old, old_size);
  
 -		bit_off = next_off;
 -		bits = 0;
 -	}
 +	chunk->map_alloc = new_alloc;
 +	chunk->map = new;
 +	new = NULL;
  
 -	if (bit_off == pcpu_chunk_map_bits(chunk))
 -		return -1;
 +out_unlock:
 +	spin_unlock_irqrestore(&pcpu_lock, flags);
  
 -	return bit_off;
 +	/*
 +	 * pcpu_mem_free() might end up calling vfree() which uses
 +	 * IRQ-unsafe lock and thus can't be called under pcpu_lock.
 +	 */
 +	pcpu_mem_free(old, old_size);
 +	pcpu_mem_free(new, new_size);
 +
 +	return 0;
  }
  
 -/*
 - * pcpu_find_zero_area - modified from bitmap_find_next_zero_area_off()
 - * @map: the address to base the search on
 - * @size: the bitmap size in bits
 - * @start: the bitnumber to start searching at
 - * @nr: the number of zeroed bits we're looking for
 - * @align_mask: alignment mask for zero area
 - * @largest_off: offset of the largest area skipped
 - * @largest_bits: size of the largest area skipped
 - *
 - * The @align_mask should be one less than a power of 2.
 - *
 - * This is a modified version of bitmap_find_next_zero_area_off() to remember
 - * the largest area that was skipped.  This is imperfect, but in general is
 - * good enough.  The largest remembered region is the largest failed region
 - * seen.  This does not include anything we possibly skipped due to alignment.
 - * pcpu_block_update_scan() does scan backwards to try and recover what was
 - * lost to alignment.  While this can cause scanning to miss earlier possible
 - * free areas, smaller allocations will eventually fill those holes.
 +/**
 + * pcpu_fit_in_area - try to fit the requested allocation in a candidate area
 + * @chunk: chunk the candidate area belongs to
 + * @off: the offset to the start of the candidate area
 + * @this_size: the size of the candidate area
 + * @size: the size of the target allocation
 + * @align: the alignment of the target allocation
 + * @pop_only: only allocate from already populated region
 + *
 + * We're trying to allocate @size bytes aligned at @align.  @chunk's area
 + * at @off sized @this_size is a candidate.  This function determines
 + * whether the target allocation fits in the candidate area and returns the
 + * number of bytes to pad after @off.  If the target area doesn't fit, -1
 + * is returned.
 + *
 + * If @pop_only is %true, this function only considers the already
 + * populated part of the candidate area.
   */
 -static unsigned long pcpu_find_zero_area(unsigned long *map,
 -					 unsigned long size,
 -					 unsigned long start,
 -					 unsigned long nr,
 -					 unsigned long align_mask,
 -					 unsigned long *largest_off,
 -					 unsigned long *largest_bits)
 +static int pcpu_fit_in_area(struct pcpu_chunk *chunk, int off, int this_size,
 +			    int size, int align, bool pop_only)
  {
 -	unsigned long index, end, i, area_off, area_bits;
 -again:
 -	index = find_next_zero_bit(map, size, start);
 -
 -	/* Align allocation */
 -	index = __ALIGN_MASK(index, align_mask);
 -	area_off = index;
 -
 -	end = index + nr;
 -	if (end > size)
 -		return end;
 -	i = find_next_bit(map, end, index);
 -	if (i < end) {
 -		area_bits = i - area_off;
 -		/* remember largest unused area with best alignment */
 -		if (area_bits > *largest_bits ||
 -		    (area_bits == *largest_bits && *largest_off &&
 -		     (!area_off || __ffs(area_off) > __ffs(*largest_off)))) {
 -			*largest_off = area_off;
 -			*largest_bits = area_bits;
 -		}
 +	int cand_off = off;
 +
 +	while (true) {
 +		int head = ALIGN(cand_off, align) - off;
 +		int page_start, page_end, rs, re;
  
 -		start = i + 1;
 -		goto again;
 +		if (this_size < head + size)
 +			return -1;
 +
 +		if (!pop_only)
 +			return head;
 +
 +		/*
 +		 * If the first unpopulated page is beyond the end of the
 +		 * allocation, the whole allocation is populated;
 +		 * otherwise, retry from the end of the unpopulated area.
 +		 */
 +		page_start = PFN_DOWN(head + off);
 +		page_end = PFN_UP(head + off + size);
 +
 +		rs = page_start;
 +		pcpu_next_unpop(chunk, &rs, &re, PFN_UP(off + this_size));
 +		if (rs >= page_end)
 +			return head;
 +		cand_off = re * PAGE_SIZE;
  	}
 -	return index;
  }
  
++<<<<<<< HEAD
  /**
 - * pcpu_alloc_area - allocates an area from a pcpu_chunk
 + * pcpu_alloc_area - allocate area from a pcpu_chunk
   * @chunk: chunk of interest
 - * @alloc_bits: size of request in allocation units
 - * @align: alignment of area (max PAGE_SIZE)
 - * @start: bit_off to start searching
 + * @size: wanted size in bytes
 + * @align: wanted align
 + * @pop_only: allocate only from the populated area
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Try to allocate @size bytes area aligned at @align from @chunk.
 + * Note that this function only allocates the offset.  It doesn't
 + * populate or map the area.
   *
 - * This function takes in a @start offset to begin searching to fit an
 - * allocation of @alloc_bits with alignment @align.  It needs to scan
 - * the allocation map because if it fits within the block's contig hint,
 - * @start will be block->first_free. This is an attempt to fill the
 - * allocation prior to breaking the contig hint.  The allocation and
 - * boundary maps are updated accordingly if it confirms a valid
 - * free area.
 + * @chunk->map must have at least two free slots.
 + *
 + * CONTEXT:
 + * pcpu_lock.
   *
   * RETURNS:
 - * Allocated addr offset in @chunk on success.
 - * -1 if no matching area is found.
 + * Allocated offset in @chunk on success, -1 if no matching area is
 + * found.
   */
 -static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 -			   size_t align, int start)
 +static int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align,
 +			   bool pop_only, int *occ_pages_p)
  {
 -	size_t align_mask = (align) ? (align - 1) : 0;
 -	unsigned long area_off = 0, area_bits = 0;
 -	int bit_off, end, oslot;
 -
 -	lockdep_assert_held(&pcpu_lock);
 -
 -	oslot = pcpu_chunk_slot(chunk);
 -
 -	/*
 -	 * Search to find a fit.
 -	 */
 -	end = min_t(int, start + alloc_bits + PCPU_BITMAP_BLOCK_BITS,
 -		    pcpu_chunk_map_bits(chunk));
 -	bit_off = pcpu_find_zero_area(chunk->alloc_map, end, start, alloc_bits,
 -				      align_mask, &area_off, &area_bits);
 -	if (bit_off >= end)
 -		return -1;
 -
 -	if (area_bits)
 -		pcpu_block_update_scan(chunk, area_off, area_bits);
 -
 -	/* update alloc map */
 -	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
 -
 -	/* update boundary map */
 -	set_bit(bit_off, chunk->bound_map);
 -	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
 -	set_bit(bit_off + alloc_bits, chunk->bound_map);
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int max_contig = 0;
 +	int i, off;
 +	bool seen_free = false;
 +	int *p;
  
 -	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
 +	for (i = chunk->first_free, p = chunk->map + i; i < chunk->map_used; i++, p++) {
 +		int head, tail;
 +		int this_size;
  
 -	/* update first free bit */
 -	if (bit_off == chunk->first_bit)
 -		chunk->first_bit = find_next_zero_bit(
 -					chunk->alloc_map,
 -					pcpu_chunk_map_bits(chunk),
 -					bit_off + alloc_bits);
 -
 -	pcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);
 -
 -	pcpu_chunk_relocate(chunk, oslot);
 +		off = *p;
 +		if (off & 1)
 +			continue;
  
 -	return bit_off * PCPU_MIN_ALLOC_SIZE;
 -}
 +		this_size = (p[1] & ~1) - off;
  
 -/**
 - * pcpu_free_area - frees the corresponding offset
 - * @chunk: chunk of interest
 - * @off: addr offset into chunk
 - *
 - * This function determines the size of an allocation to free using
 - * the boundary bitmap and clears the allocation map.
 - */
 -static void pcpu_free_area(struct pcpu_chunk *chunk, int off)
 -{
 -	int bit_off, bits, end, oslot;
 +		head = pcpu_fit_in_area(chunk, off, this_size, size, align,
 +					pop_only);
 +		if (head < 0) {
 +			if (!seen_free) {
 +				chunk->first_free = i;
 +				seen_free = true;
 +			}
 +			max_contig = max(this_size, max_contig);
 +			continue;
 +		}
  
 -	lockdep_assert_held(&pcpu_lock);
 -	pcpu_stats_area_dealloc(chunk);
 +		/*
 +		 * If head is small or the previous block is free,
 +		 * merge'em.  Note that 'small' is defined as smaller
 +		 * than sizeof(int), which is very small but isn't too
 +		 * uncommon for percpu allocations.
 +		 */
 +		if (head && (head < sizeof(int) || !(p[-1] & 1))) {
 +			*p = off += head;
 +			if (p[-1] & 1)
 +				chunk->free_size -= head;
 +			else
 +				max_contig = max(*p - p[-1], max_contig);
 +			this_size -= head;
 +			head = 0;
 +		}
  
 -	oslot = pcpu_chunk_slot(chunk);
 +		/* if tail is small, just keep it around */
 +		tail = this_size - head - size;
 +		if (tail < sizeof(int)) {
 +			tail = 0;
 +			size = this_size - head;
 +		}
  
 -	bit_off = off / PCPU_MIN_ALLOC_SIZE;
 +		/* split if warranted */
 +		if (head || tail) {
 +			int nr_extra = !!head + !!tail;
 +
 +			/* insert new subblocks */
 +			memmove(p + nr_extra + 1, p + 1,
 +				sizeof(chunk->map[0]) * (chunk->map_used - i));
 +			chunk->map_used += nr_extra;
 +
 +			if (head) {
 +				if (!seen_free) {
 +					chunk->first_free = i;
 +					seen_free = true;
 +				}
 +				*++p = off += head;
 +				++i;
 +				max_contig = max(head, max_contig);
 +			}
 +			if (tail) {
 +				p[1] = off + size;
 +				max_contig = max(tail, max_contig);
++=======
++			e_block->left_free = 0;
++			if (e_off > e_block->contig_hint_start) {
++				/* contig hint is broken - scan to fix it */
++				pcpu_block_refresh_hint(chunk, e_index);
++			} else {
++				e_block->right_free =
++					min_t(int, e_block->right_free,
++					      PCPU_BITMAP_BLOCK_BITS - e_off);
++>>>>>>> da3afdd5bb54 (percpu: use block scan_hint to only scan forward)
 +			}
 +		}
  
 -	/* find end index */
 -	end = find_next_bit(chunk->bound_map, pcpu_chunk_map_bits(chunk),
 -			    bit_off + 1);
 -	bits = end - bit_off;
 -	bitmap_clear(chunk->alloc_map, bit_off, bits);
 +		if (!seen_free)
 +			chunk->first_free = i + 1;
  
 -	/* update metadata */
 -	chunk->free_bytes += bits * PCPU_MIN_ALLOC_SIZE;
 +		/* update hint and mark allocated */
 +		if (i + 1 == chunk->map_used)
 +			chunk->contig_hint = max_contig; /* fully scanned */
 +		else
 +			chunk->contig_hint = max(chunk->contig_hint,
 +						 max_contig);
  
 -	/* update first free bit */
 -	chunk->first_bit = min(chunk->first_bit, bit_off);
 +		chunk->free_size -= size;
 +		*p |= 1;
  
 -	pcpu_block_update_hint_free(chunk, bit_off, bits);
 +		*occ_pages_p = pcpu_count_occupied_pages(chunk, i);
 +		pcpu_chunk_relocate(chunk, oslot);
 +		return off;
 +	}
  
 +	chunk->contig_hint = max_contig;	/* fully scanned */
  	pcpu_chunk_relocate(chunk, oslot);
 -}
  
 -static void pcpu_init_md_blocks(struct pcpu_chunk *chunk)
 -{
 -	struct pcpu_block_md *md_block;
 -
 -	for (md_block = chunk->md_blocks;
 -	     md_block != chunk->md_blocks + pcpu_chunk_nr_blocks(chunk);
 -	     md_block++) {
 -		md_block->scan_hint = 0;
 -		md_block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
 -		md_block->left_free = PCPU_BITMAP_BLOCK_BITS;
 -		md_block->right_free = PCPU_BITMAP_BLOCK_BITS;
 -	}
 +	/* tell the upper layer that this chunk has no matching area */
 +	return -1;
  }
  
  /**
* Unmerged path mm/percpu.c
