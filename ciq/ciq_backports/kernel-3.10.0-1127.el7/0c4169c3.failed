percpu: setup_first_chunk rename schunk/dchunk to chunk

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit 0c4169c3d11722a26773bdc0144c97fadd47d905
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/0c4169c3.failed

There is no need to have the static chunk and dynamic chunk be named
separately as the allocations are sequential. This preemptively solves
the misnomer problem with the base_addrs being moved up in the following
patch. It also removes a ternary operation deciding the first chunk.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 0c4169c3d11722a26773bdc0144c97fadd47d905)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,e08ed61ea70a..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -1557,9 -1601,8 +1557,14 @@@ int __init pcpu_setup_first_chunk(cons
  {
  	static int smap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
  	static int dmap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
++<<<<<<< HEAD
 +	size_t dyn_size = ai->dyn_size;
 +	size_t size_sum = ai->static_size + ai->reserved_size + dyn_size;
 +	struct pcpu_chunk *schunk, *dchunk = NULL;
++=======
+ 	size_t size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
+ 	struct pcpu_chunk *chunk;
++>>>>>>> 0c4169c3d117 (percpu: setup_first_chunk rename schunk/dchunk to chunk)
  	unsigned long *group_offsets;
  	size_t *group_sizes;
  	unsigned long *unit_off;
@@@ -1671,58 -1718,32 +1676,80 @@@
  	 * covers static area + reserved area (mostly used for module
  	 * static percpu allocation).
  	 */
++<<<<<<< HEAD
 +	schunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +	INIT_LIST_HEAD(&schunk->list);
 +	INIT_LIST_HEAD(&schunk->map_extend_list);
 +	schunk->base_addr = base_addr;
 +	schunk->map = smap;
 +	schunk->map_alloc = ARRAY_SIZE(smap);
 +	schunk->immutable = true;
 +	bitmap_fill(schunk->populated, pcpu_unit_pages);
 +	schunk->nr_populated = pcpu_unit_pages;
 +
 +	if (ai->reserved_size) {
 +		schunk->free_size = ai->reserved_size;
 +		pcpu_reserved_chunk = schunk;
 +		pcpu_reserved_chunk_limit = ai->static_size + ai->reserved_size;
 +	} else {
 +		schunk->free_size = dyn_size;
 +		dyn_size = 0;			/* dynamic area covered */
 +	}
 +	schunk->contig_hint = schunk->free_size;
 +
 +	schunk->map[0] = 1;
 +	schunk->map[1] = ai->static_size;
 +	schunk->map_used = 1;
 +	if (schunk->free_size)
 +		schunk->map[++schunk->map_used] = ai->static_size + schunk->free_size;
 +	schunk->map[schunk->map_used] |= 1;
 +
 +	/* init dynamic chunk if necessary */
 +	if (dyn_size) {
 +		dchunk = memblock_virt_alloc(pcpu_chunk_struct_size, 0);
 +		INIT_LIST_HEAD(&dchunk->list);
 +		INIT_LIST_HEAD(&dchunk->map_extend_list);
 +		dchunk->base_addr = base_addr;
 +		dchunk->map = dmap;
 +		dchunk->map_alloc = ARRAY_SIZE(dmap);
 +		dchunk->immutable = true;
 +		bitmap_fill(dchunk->populated, pcpu_unit_pages);
 +		dchunk->nr_populated = pcpu_unit_pages;
 +
 +		dchunk->contig_hint = dchunk->free_size = dyn_size;
 +		dchunk->map[0] = 1;
 +		dchunk->map[1] = pcpu_reserved_chunk_limit;
 +		dchunk->map[2] = (pcpu_reserved_chunk_limit + dchunk->free_size) | 1;
 +		dchunk->map_used = 2;
 +	}
 +
 +	/* link the first chunk in */
 +	pcpu_first_chunk = dchunk ?: schunk;
++=======
+ 	start_offset = ai->static_size;
+ 	map_size = ai->reserved_size ?: ai->dyn_size;
+ 	chunk = pcpu_alloc_first_chunk(base_addr, start_offset, map_size, smap,
+ 				       ARRAY_SIZE(smap));
+ 
+ 	/* init dynamic chunk if necessary */
+ 	if (ai->reserved_size) {
+ 		pcpu_reserved_chunk = chunk;
+ 
+ 		start_offset = ai->static_size + ai->reserved_size;
+ 		map_size = ai->dyn_size;
+ 		chunk = pcpu_alloc_first_chunk(base_addr, start_offset,
+ 					       map_size, dmap,
+ 					       ARRAY_SIZE(dmap));
+ 	}
+ 
+ 	/* link the first chunk in */
+ 	pcpu_first_chunk = chunk;
+ 	i = (pcpu_first_chunk->start_offset) ? 1 : 0;
++>>>>>>> 0c4169c3d117 (percpu: setup_first_chunk rename schunk/dchunk to chunk)
  	pcpu_nr_empty_pop_pages +=
 -		pcpu_count_occupied_pages(pcpu_first_chunk, i);
 +		pcpu_count_occupied_pages(pcpu_first_chunk, 1);
  	pcpu_chunk_relocate(pcpu_first_chunk, -1);
  
 -	pcpu_stats_chunk_alloc();
 -	trace_percpu_create_chunk(base_addr);
 -
  	/* we're done */
  	pcpu_base_addr = base_addr;
  	return 0;
* Unmerged path mm/percpu.c
