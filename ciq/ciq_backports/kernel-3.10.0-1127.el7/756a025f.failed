mm: coalesce split strings

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Joe Perches <joe@perches.com>
commit 756a025f00091918d9d09ca3229defb160b409c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/756a025f.failed

Kernel style prefers a single string over split strings when the string is
'user-visible'.

Miscellanea:

 - Add a missing newline
 - Realign arguments

	Signed-off-by: Joe Perches <joe@perches.com>
	Acked-by: Tejun Heo <tj@kernel.org>	[percpu]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 756a025f00091918d9d09ca3229defb160b409c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
#	mm/kasan/report.c
#	mm/kmemleak.c
#	mm/mempolicy.c
#	mm/mmap.c
#	mm/oom_kill.c
#	mm/page_alloc.c
#	mm/page_owner.c
#	mm/percpu.c
#	mm/slab.c
#	mm/slab_common.c
#	mm/slub.c
#	mm/swapfile.c
#	mm/vmalloc.c
diff --cc mm/huge_memory.c
index 5c622aeff5fe,e1a177c20791..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -134,14 -166,17 +134,22 @@@ static int set_recommended_min_free_kby
  			      (unsigned long) nr_free_buffer_pages() / 20);
  	recommended_min <<= (PAGE_SHIFT-10);
  
++<<<<<<< HEAD
 +	if (recommended_min > min_free_kbytes)
++=======
+ 	if (recommended_min > min_free_kbytes) {
+ 		if (user_min_free_kbytes >= 0)
+ 			pr_info("raising min_free_kbytes from %d to %lu to help transparent hugepage allocations\n",
+ 				min_free_kbytes, recommended_min);
+ 
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  		min_free_kbytes = recommended_min;
 -	}
  	setup_per_zone_wmarks();
 +	return 0;
  }
 +late_initcall(set_recommended_min_free_kbytes);
  
 -static int start_stop_khugepaged(void)
 +static int start_khugepaged(void)
  {
  	int err = 0;
  	if (khugepaged_enabled()) {
diff --cc mm/kmemleak.c
index 8c78f1372f6b,e6429926e957..000000000000
--- a/mm/kmemleak.c
+++ b/mm/kmemleak.c
@@@ -574,14 -596,15 +574,13 @@@ static struct kmemleak_object *create_o
  		else if (parent->pointer + parent->size <= ptr)
  			link = &parent->rb_node.rb_right;
  		else {
- 			kmemleak_stop("Cannot insert 0x%lx into the object "
- 				      "search tree (overlaps existing)\n",
+ 			kmemleak_stop("Cannot insert 0x%lx into the object search tree (overlaps existing)\n",
  				      ptr);
 -			/*
 -			 * No need for parent->lock here since "parent" cannot
 -			 * be freed while the kmemleak_lock is held.
 -			 */
 -			dump_object_info(parent);
  			kmem_cache_free(object_cache, object);
 -			object = NULL;
 +			object = parent;
 +			spin_lock(&object->lock);
 +			dump_object_info(object);
 +			spin_unlock(&object->lock);
  			goto out;
  		}
  	}
@@@ -650,11 -666,11 +649,11 @@@ static void delete_object_part(unsigne
  	struct kmemleak_object *object;
  	unsigned long start, end;
  
 -	object = find_and_remove_object(ptr, 1);
 +	object = find_and_get_object(ptr, 1);
  	if (!object) {
  #ifdef DEBUG
- 		kmemleak_warn("Partially freeing unknown object at 0x%08lx "
- 			      "(size %zu)\n", ptr, size);
+ 		kmemleak_warn("Partially freeing unknown object at 0x%08lx (size %zu)\n",
+ 			      ptr, size);
  #endif
  		return;
  	}
@@@ -1381,9 -1459,12 +1380,18 @@@ static void kmemleak_scan(void
  	}
  	rcu_read_unlock();
  
++<<<<<<< HEAD
 +	if (new_leaks)
 +		pr_info("%d new suspected memory leaks (see "
 +			"/sys/kernel/debug/kmemleak)\n", new_leaks);
++=======
+ 	if (new_leaks) {
+ 		kmemleak_found_leaks = true;
+ 
+ 		pr_info("%d new suspected memory leaks (see /sys/kernel/debug/kmemleak)\n",
+ 			new_leaks);
+ 	}
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  
  }
  
@@@ -1683,19 -1782,19 +1691,33 @@@ static const struct file_operations kme
   */
  static void kmemleak_do_cleanup(struct work_struct *work)
  {
 +	struct kmemleak_object *object;
 +	bool cleanup = scan_thread == NULL;
 +
 +	mutex_lock(&scan_mutex);
  	stop_scan_thread();
  
++<<<<<<< HEAD
 +	if (cleanup) {
 +		rcu_read_lock();
 +		list_for_each_entry_rcu(object, &object_list, object_list)
 +			delete_object_full(object->pointer);
 +		rcu_read_unlock();
 +	}
 +	mutex_unlock(&scan_mutex);
++=======
+ 	/*
+ 	 * Once the scan thread has stopped, it is safe to no longer track
+ 	 * object freeing. Ordering of the scan thread stopping and the memory
+ 	 * accesses below is guaranteed by the kthread_stop() function.
+ 	 */
+ 	kmemleak_free_enabled = 0;
+ 
+ 	if (!kmemleak_found_leaks)
+ 		__kmemleak_do_cleanup();
+ 	else
+ 		pr_info("Kmemleak disabled without freeing internal data. Reclaim the memory with \"echo clear > /sys/kernel/debug/kmemleak\".\n");
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  }
  
  static DECLARE_WORK(cleanup_work, kmemleak_do_cleanup);
diff --cc mm/mempolicy.c
index cd72d600a9bb,b25de27b83d0..000000000000
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@@ -2611,10 -2558,8 +2611,15 @@@ static void __init check_numabalancing_
  	if (numabalancing_override)
  		set_numabalancing_state(numabalancing_override == 1);
  
++<<<<<<< HEAD
 +	if (nr_node_ids > 1 && !numabalancing_override) {
 +		printk(KERN_INFO "%s automatic NUMA balancing. "
 +			"Configure with numa_balancing= or the "
 +			"kernel.numa_balancing sysctl",
++=======
+ 	if (num_online_nodes() > 1 && !numabalancing_override) {
+ 		pr_info("%s automatic NUMA balancing. Configure with numa_balancing= or the kernel.numa_balancing sysctl\n",
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  			numabalancing_default ? "Enabling" : "Disabling");
  		set_numabalancing_state(numabalancing_default);
  	}
diff --cc mm/mmap.c
index d07a55987643,e06345aafa03..000000000000
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@@ -2883,6 -2503,98 +2883,101 @@@ SYSCALL_DEFINE2(munmap, unsigned long, 
  	return vm_munmap(addr, len);
  }
  
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Emulation of deprecated remap_file_pages() syscall.
+  */
+ SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,
+ 		unsigned long, prot, unsigned long, pgoff, unsigned long, flags)
+ {
+ 
+ 	struct mm_struct *mm = current->mm;
+ 	struct vm_area_struct *vma;
+ 	unsigned long populate = 0;
+ 	unsigned long ret = -EINVAL;
+ 	struct file *file;
+ 
+ 	pr_warn_once("%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.txt.\n",
+ 		     current->comm, current->pid);
+ 
+ 	if (prot)
+ 		return ret;
+ 	start = start & PAGE_MASK;
+ 	size = size & PAGE_MASK;
+ 
+ 	if (start + size <= start)
+ 		return ret;
+ 
+ 	/* Does pgoff wrap? */
+ 	if (pgoff + (size >> PAGE_SHIFT) < pgoff)
+ 		return ret;
+ 
+ 	down_write(&mm->mmap_sem);
+ 	vma = find_vma(mm, start);
+ 
+ 	if (!vma || !(vma->vm_flags & VM_SHARED))
+ 		goto out;
+ 
+ 	if (start < vma->vm_start)
+ 		goto out;
+ 
+ 	if (start + size > vma->vm_end) {
+ 		struct vm_area_struct *next;
+ 
+ 		for (next = vma->vm_next; next; next = next->vm_next) {
+ 			/* hole between vmas ? */
+ 			if (next->vm_start != next->vm_prev->vm_end)
+ 				goto out;
+ 
+ 			if (next->vm_file != vma->vm_file)
+ 				goto out;
+ 
+ 			if (next->vm_flags != vma->vm_flags)
+ 				goto out;
+ 
+ 			if (start + size <= next->vm_end)
+ 				break;
+ 		}
+ 
+ 		if (!next)
+ 			goto out;
+ 	}
+ 
+ 	prot |= vma->vm_flags & VM_READ ? PROT_READ : 0;
+ 	prot |= vma->vm_flags & VM_WRITE ? PROT_WRITE : 0;
+ 	prot |= vma->vm_flags & VM_EXEC ? PROT_EXEC : 0;
+ 
+ 	flags &= MAP_NONBLOCK;
+ 	flags |= MAP_SHARED | MAP_FIXED | MAP_POPULATE;
+ 	if (vma->vm_flags & VM_LOCKED) {
+ 		struct vm_area_struct *tmp;
+ 		flags |= MAP_LOCKED;
+ 
+ 		/* drop PG_Mlocked flag for over-mapped range */
+ 		for (tmp = vma; tmp->vm_start >= start + size;
+ 				tmp = tmp->vm_next) {
+ 			munlock_vma_pages_range(tmp,
+ 					max(tmp->vm_start, start),
+ 					min(tmp->vm_end, start + size));
+ 		}
+ 	}
+ 
+ 	file = get_file(vma->vm_file);
+ 	ret = do_mmap_pgoff(vma->vm_file, start, size,
+ 			prot, flags, pgoff, &populate);
+ 	fput(file);
+ out:
+ 	up_write(&mm->mmap_sem);
+ 	if (populate)
+ 		mm_populate(ret, populate);
+ 	if (!IS_ERR_VALUE(ret))
+ 		ret = 0;
+ 	return ret;
+ }
+ 
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  static inline void verify_mm_writelocked(struct mm_struct *mm)
  {
  #ifdef CONFIG_DEBUG_VM
@@@ -3192,18 -2876,62 +3287,31 @@@ struct vm_area_struct *copy_vma(struct 
   * Return true if the calling process may expand its vm space by the passed
   * number of pages
   */
 -bool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)
 +int may_expand_vm(struct mm_struct *mm, unsigned long npages)
  {
 -	if (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)
 -		return false;
 +	unsigned long cur = mm->total_vm;	/* pages */
 +	unsigned long lim;
  
++<<<<<<< HEAD
 +	lim = rlimit(RLIMIT_AS) >> PAGE_SHIFT;
++=======
+ 	if (is_data_mapping(flags) &&
+ 	    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {
+ 		if (ignore_rlimit_data)
+ 			pr_warn_once("%s (%d): VmData %lu exceed data ulimit %lu. Will be forbidden soon.\n",
+ 				     current->comm, current->pid,
+ 				     (mm->data_vm + npages) << PAGE_SHIFT,
+ 				     rlimit(RLIMIT_DATA));
+ 		else
+ 			return false;
+ 	}
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  
 -	return true;
 -}
 -
 -void vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)
 -{
 -	mm->total_vm += npages;
 -
 -	if (is_exec_mapping(flags))
 -		mm->exec_vm += npages;
 -	else if (is_stack_mapping(flags))
 -		mm->stack_vm += npages;
 -	else if (is_data_mapping(flags))
 -		mm->data_vm += npages;
 -}
 -
 -static int special_mapping_fault(struct vm_area_struct *vma,
 -				 struct vm_fault *vmf);
 -
 -/*
 - * Having a close hook prevents vma merging regardless of flags.
 - */
 -static void special_mapping_close(struct vm_area_struct *vma)
 -{
 -}
 -
 -static const char *special_mapping_name(struct vm_area_struct *vma)
 -{
 -	return ((struct vm_special_mapping *)vma->vm_private_data)->name;
 +	if (cur + npages > lim)
 +		return 0;
 +	return 1;
  }
  
 -static const struct vm_operations_struct special_mapping_vmops = {
 -	.close = special_mapping_close,
 -	.fault = special_mapping_fault,
 -	.name = special_mapping_name,
 -};
 -
 -static const struct vm_operations_struct legacy_special_mapping_vmops = {
 -	.close = special_mapping_close,
 -	.fault = special_mapping_fault,
 -};
  
  static int special_mapping_fault(struct vm_area_struct *vma,
  				struct vm_fault *vmf)
diff --cc mm/oom_kill.c
index 9079d092d30f,fde3d374c0af..000000000000
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@@ -383,16 -380,14 +383,21 @@@ static void dump_tasks(const struct mem
  	rcu_read_unlock();
  }
  
 -static void dump_header(struct oom_control *oc, struct task_struct *p,
 -			struct mem_cgroup *memcg)
 +static void dump_header(struct task_struct *p, gfp_t gfp_mask, int order,
 +			struct mem_cgroup *memcg, const nodemask_t *nodemask)
  {
++<<<<<<< HEAD
 +	task_lock(current);
 +	pr_warning("%s invoked oom-killer: gfp_mask=0x%x, order=%d, "
 +		"oom_score_adj=%hd\n",
 +		current->comm, gfp_mask, order,
++=======
+ 	pr_warn("%s invoked oom-killer: gfp_mask=%#x(%pGg), order=%d, oom_score_adj=%hd\n",
+ 		current->comm, oc->gfp_mask, &oc->gfp_mask, oc->order,
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  		current->signal->oom_score_adj);
 -
 -	cpuset_print_current_mems_allowed();
 +	cpuset_print_task_mems_allowed(current);
 +	task_unlock(current);
  	dump_stack();
  	if (memcg)
  		mem_cgroup_print_oom_info(memcg, p);
diff --cc mm/page_alloc.c
index 10b3d04ee8fe,42cf199652a5..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -4721,14 -4538,13 +4720,22 @@@ void __ref build_all_zonelists(pg_data_
  	else
  		page_group_by_mobility_disabled = 0;
  
++<<<<<<< HEAD
 +	printk("Built %i zonelists in %s order, mobility grouping %s.  "
 +		"Total pages: %ld\n",
 +			nr_online_nodes,
 +			zonelist_order_name[current_zonelist_order],
 +			page_group_by_mobility_disabled ? "off" : "on",
 +			vm_total_pages);
++=======
+ 	pr_info("Built %i zonelists in %s order, mobility grouping %s.  Total pages: %ld\n",
+ 		nr_online_nodes,
+ 		zonelist_order_name[current_zonelist_order],
+ 		page_group_by_mobility_disabled ? "off" : "on",
+ 		vm_total_pages);
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  #ifdef CONFIG_NUMA
 -	pr_info("Policy zone: %s\n", zone_names[policy_zone]);
 +	printk("Policy zone: %s\n", zone_names[policy_zone]);
  #endif
  }
  
@@@ -6478,22 -6140,21 +6485,27 @@@ void __init mem_init_print_info(const c
  
  #undef	adj_init_size
  
++<<<<<<< HEAD
 +	printk("Memory: %luK/%luK available "
 +	       "(%luK kernel code, %luK rwdata, %luK rodata, "
 +	       "%luK init, %luK bss, %luK reserved, %luK cma-reserved"
++=======
+ 	pr_info("Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved"
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  #ifdef	CONFIG_HIGHMEM
- 	       ", %luK highmem"
+ 		", %luK highmem"
  #endif
- 	       "%s%s)\n",
- 	       nr_free_pages() << (PAGE_SHIFT-10), physpages << (PAGE_SHIFT-10),
- 	       codesize >> 10, datasize >> 10, rosize >> 10,
- 	       (init_data_size + init_code_size) >> 10, bss_size >> 10,
- 	       (physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT-10),
- 	       totalcma_pages << (PAGE_SHIFT-10),
+ 		"%s%s)\n",
+ 		nr_free_pages() << (PAGE_SHIFT - 10),
+ 		physpages << (PAGE_SHIFT - 10),
+ 		codesize >> 10, datasize >> 10, rosize >> 10,
+ 		(init_data_size + init_code_size) >> 10, bss_size >> 10,
+ 		(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),
+ 		totalcma_pages << (PAGE_SHIFT - 10),
  #ifdef	CONFIG_HIGHMEM
- 	       totalhigh_pages << (PAGE_SHIFT-10),
+ 		totalhigh_pages << (PAGE_SHIFT - 10),
  #endif
- 	       str ? ", " : "", str ? str : "");
+ 		str ? ", " : "", str ? str : "");
  }
  
  /**
diff --cc mm/percpu.c
index 895c2996b902,1571547e7b01..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -892,8 -888,8 +892,13 @@@ static void __percpu *pcpu_alloc(size_
  	size = ALIGN(size, 2);
  
  	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {
++<<<<<<< HEAD
 +		WARN(do_warn, "illegal size (%zu) or align (%zu) for "
 +		     "percpu allocation\n", size, align);
++=======
+ 		WARN(true, "illegal size (%zu) or align (%zu) for percpu allocation\n",
+ 		     size, align);
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  		return NULL;
  	}
  
diff --cc mm/slab.c
index 0a4735511ea7,e558f8593a22..000000000000
--- a/mm/slab.c
+++ b/mm/slab.c
@@@ -2656,23 -2394,38 +2652,21 @@@ static void cache_init_objs(struct kmem
  
  		if (cachep->flags & SLAB_RED_ZONE) {
  			if (*dbg_redzone2(cachep, objp) != RED_INACTIVE)
- 				slab_error(cachep, "constructor overwrote the"
- 					   " end of an object");
+ 				slab_error(cachep, "constructor overwrote the end of an object");
  			if (*dbg_redzone1(cachep, objp) != RED_INACTIVE)
- 				slab_error(cachep, "constructor overwrote the"
- 					   " start of an object");
+ 				slab_error(cachep, "constructor overwrote the start of an object");
  		}
 -		/* need to poison the objs? */
 -		if (cachep->flags & SLAB_POISON) {
 -			poison_obj(cachep, objp, POISON_FREE);
 -			slab_kernel_map(cachep, objp, 0, 0);
 -		}
 -	}
 +		if ((cachep->size % PAGE_SIZE) == 0 &&
 +			    OFF_SLAB(cachep) && cachep->flags & SLAB_POISON)
 +			kernel_map_pages(virt_to_page(objp),
 +					 cachep->size / PAGE_SIZE, 0);
 +#else
 +		if (cachep->ctor)
 +			cachep->ctor(objp);
  #endif
 -}
 -
 -static void cache_init_objs(struct kmem_cache *cachep,
 -			    struct page *page)
 -{
 -	int i;
 -
 -	cache_init_objs_debug(cachep, page);
 -
 -	if (OBJFREELIST_SLAB(cachep)) {
 -		page->freelist = index_to_obj(cachep, page, cachep->num - 1) +
 -						obj_offset(cachep);
 -	}
 -
 -	for (i = 0; i < cachep->num; i++) {
 -		/* constructor could break poison info */
 -		if (DEBUG == 0 && cachep->ctor)
 -			cachep->ctor(index_to_obj(cachep, page, i));
 -
 -		set_free_obj(page, i, i);
 +		slab_bufctl(slabp)[i] = i + 1;
  	}
 +	slab_bufctl(slabp)[i - 1] = BUFCTL_END;
  }
  
  static void kmem_flagcheck(struct kmem_cache *cachep, gfp_t flags)
@@@ -2702,24 -2453,27 +2696,34 @@@ static void *slab_get_obj(struct kmem_c
  	return objp;
  }
  
 -static void slab_put_obj(struct kmem_cache *cachep,
 -			struct page *page, void *objp)
 +static void slab_put_obj(struct kmem_cache *cachep, struct slab *slabp,
 +				void *objp, int nodeid)
  {
 -	unsigned int objnr = obj_to_index(cachep, page, objp);
 +	unsigned int objnr = obj_to_index(cachep, slabp, objp);
 +
++<<<<<<< HEAD
  #if DEBUG
 -	unsigned int i;
 +	/* Verify that the slab belongs to the intended node */
 +	WARN_ON(slabp->nodeid != nodeid);
  
 +	if (slab_bufctl(slabp)[objnr] + 1 <= SLAB_LIMIT + 1) {
 +		printk(KERN_ERR "slab: double free detected in cache "
 +				"'%s', objp %p\n", cachep->name, objp);
 +		BUG();
++=======
+ 	/* Verify double free bug */
+ 	for (i = page->active; i < cachep->num; i++) {
+ 		if (get_free_obj(page, i) == objnr) {
+ 			printk(KERN_ERR "slab: double free detected in cache '%s', objp %p\n",
+ 			       cachep->name, objp);
+ 			BUG();
+ 		}
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  	}
  #endif
 -	page->active--;
 -	if (!page->freelist)
 -		page->freelist = objp + obj_offset(cachep);
 -
 -	set_free_obj(page, page->active, objnr);
 +	slab_bufctl(slabp)[objnr] = slabp->free;
 +	slabp->free = objnr;
 +	slabp->inuse--;
  }
  
  /*
diff --cc mm/slab_common.c
index 288b69b9b33e,e885e11a316f..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -351,34 -721,46 +351,40 @@@ void kmem_cache_destroy(struct kmem_cac
  	if (s->refcount)
  		goto out_unlock;
  
 -	err = shutdown_memcg_caches(s, &release, &need_rcu_barrier);
 -	if (!err)
 -		err = shutdown_cache(s, &release, &need_rcu_barrier);
 +	if (kmem_cache_destroy_memcg_children(s) != 0)
 +		goto out_unlock;
  
++<<<<<<< HEAD
 +	list_del(&s->list);
 +	if (__kmem_cache_shutdown(s) != 0) {
 +		list_add(&s->list, &slab_caches);
 +		printk(KERN_ERR "kmem_cache_destroy %s: "
 +		       "Slab cache still has objects\n", s->name);
++=======
+ 	if (err) {
+ 		pr_err("kmem_cache_destroy %s: Slab cache still has objects\n",
+ 		       s->name);
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  		dump_stack();
 +		goto out_unlock;
  	}
 -out_unlock:
  	mutex_unlock(&slab_mutex);
 +	if (s->flags & SLAB_DESTROY_BY_RCU)
 +		rcu_barrier();
  
 -	put_online_mems();
 -	put_online_cpus();
 -
 -	release_caches(&release, need_rcu_barrier);
 -}
 -EXPORT_SYMBOL(kmem_cache_destroy);
 -
 -/**
 - * kmem_cache_shrink - Shrink a cache.
 - * @cachep: The cache to shrink.
 - *
 - * Releases as many slabs as possible for a cache.
 - * To help debugging, a zero exit status indicates all slabs were released.
 - */
 -int kmem_cache_shrink(struct kmem_cache *cachep)
 -{
 -	int ret;
 +	memcg_free_cache_params(s);
 +	kfree(s->name);
 +	kmem_cache_free(kmem_cache, s);
 +	goto out_put_cpus;
  
 -	get_online_cpus();
 -	get_online_mems();
 -	ret = __kmem_cache_shrink(cachep, false);
 -	put_online_mems();
 +out_unlock:
 +	mutex_unlock(&slab_mutex);
 +out_put_cpus:
  	put_online_cpus();
 -	return ret;
  }
 -EXPORT_SYMBOL(kmem_cache_shrink);
 +EXPORT_SYMBOL(kmem_cache_destroy);
  
 -bool slab_is_available(void)
 +int slab_is_available(void)
  {
  	return slab_state >= UP;
  }
diff --cc mm/slub.c
index cfd46f5b97ec,7277413ebc8b..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -1166,12 -1117,11 +1166,12 @@@ static inline int free_consistency_chec
  
  	if (unlikely(s != page->slab_cache)) {
  		if (!PageSlab(page)) {
- 			slab_err(s, page, "Attempt to free object(0x%p) "
- 				"outside of slab", object);
+ 			slab_err(s, page, "Attempt to free object(0x%p) outside of slab",
+ 				 object);
  		} else if (!page->slab_cache) {
 -			pr_err("SLUB <none>: no slab for object 0x%p.\n",
 -			       object);
 +			printk(KERN_ERR
 +				"SLUB <none>: no slab for object 0x%p.\n",
 +						object);
  			dump_stack();
  		} else
  			object_err(s, page, object,
@@@ -3441,10 -3439,9 +3441,16 @@@ static int kmem_cache_open(struct kmem_
  	free_kmem_cache_nodes(s);
  error:
  	if (flags & SLAB_PANIC)
++<<<<<<< HEAD
 +		panic("Cannot create slab %s size=%lu realsize=%u "
 +			"order=%u offset=%u flags=%lx\n",
 +			s->name, (unsigned long)s->size, s->size, oo_order(s->oo),
 +			s->offset, flags);
++=======
+ 		panic("Cannot create slab %s size=%lu realsize=%u order=%u offset=%u flags=%lx\n",
+ 		      s->name, (unsigned long)s->size, s->size,
+ 		      oo_order(s->oo), s->offset, flags);
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  	return -EINVAL;
  }
  
diff --cc mm/swapfile.c
index 902dfeaeb7df,b86cf26a586b..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -2911,8 -2526,7 +2911,12 @@@ SYSCALL_DEFINE2(swapon, const char __us
  		  (swap_flags & SWAP_FLAG_PRIO_MASK) >> SWAP_FLAG_PRIO_SHIFT;
  	enable_swap_info(p, prio, swap_map, cluster_info, frontswap_map);
  
++<<<<<<< HEAD
 +	printk(KERN_INFO "Adding %uk swap on %s.  "
 +			"Priority:%d extents:%d across:%lluk %s%s%s%s%s\n",
++=======
+ 	pr_info("Adding %uk swap on %s.  Priority:%d extents:%d across:%lluk %s%s%s%s%s\n",
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  		p->pages<<(PAGE_SHIFT-10), name->name, p->prio,
  		nr_extents, (unsigned long long)span<<(PAGE_SHIFT-10),
  		(p->flags & SWP_SOLIDSTATE) ? "SS" : "",
diff --cc mm/vmalloc.c
index 90de46594b5f,e86c24ee9445..000000000000
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@@ -460,9 -469,8 +460,14 @@@ overflow
  		goto retry;
  	}
  	if (printk_ratelimit())
++<<<<<<< HEAD
 +		printk(KERN_WARNING
 +			"vmap allocation for size %lu failed: "
 +			"use vmalloc=<size> to increase size.\n", size);
++=======
+ 		pr_warn("vmap allocation for size %lu failed: use vmalloc=<size> to increase size\n",
+ 			size);
++>>>>>>> 756a025f0009 (mm: coalesce split strings)
  	kfree(va);
  	return ERR_PTR(-EBUSY);
  }
* Unmerged path mm/kasan/report.c
* Unmerged path mm/page_owner.c
diff --git a/mm/dmapool.c b/mm/dmapool.c
index 0b33dd0bf614..716c55b05564 100644
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@ -436,13 +436,11 @@ void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)
 			}
 			spin_unlock_irqrestore(&pool->lock, flags);
 			if (pool->dev)
-				dev_err(pool->dev, "dma_pool_free %s, dma %Lx "
-					"already free\n", pool->name,
-					(unsigned long long)dma);
+				dev_err(pool->dev, "dma_pool_free %s, dma %Lx already free\n",
+					pool->name, (unsigned long long)dma);
 			else
-				printk(KERN_ERR "dma_pool_free %s, dma %Lx "
-					"already free\n", pool->name,
-					(unsigned long long)dma);
+				printk(KERN_ERR "dma_pool_free %s, dma %Lx already free\n",
+					pool->name, (unsigned long long)dma);
 			return;
 		}
 	}
* Unmerged path mm/huge_memory.c
* Unmerged path mm/kasan/report.c
diff --git a/mm/kmemcheck.c b/mm/kmemcheck.c
index fd814fd61319..85791c490c48 100644
--- a/mm/kmemcheck.c
+++ b/mm/kmemcheck.c
@@ -19,8 +19,7 @@ void kmemcheck_alloc_shadow(struct page *page, int order, gfp_t flags, int node)
 	shadow = alloc_pages_node(node, flags | __GFP_NOTRACK, order);
 	if (!shadow) {
 		if (printk_ratelimit())
-			printk(KERN_ERR "kmemcheck: failed to allocate "
-				"shadow bitmap\n");
+			printk(KERN_ERR "kmemcheck: failed to allocate shadow bitmap\n");
 		return;
 	}
 
* Unmerged path mm/kmemleak.c
diff --git a/mm/memblock.c b/mm/memblock.c
index 52a3803db1d5..eb9722a591f3 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -233,8 +233,7 @@ phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,
 		 * so we use WARN_ONCE() here to see the stack trace if
 		 * fail happens.
 		 */
-		WARN_ONCE(1, "memblock: bottom-up allocation failed, "
-			     "memory hotunplug may be affected\n");
+		WARN_ONCE(1, "memblock: bottom-up allocation failed, memory hotunplug may be affected\n");
 	}
 
 	return __memblock_find_range_top_down(start, end, size, align, nid,
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index 0352de66ba23..e4e69db7278c 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -1927,8 +1927,7 @@ static int is_memblock_offlined_cb(struct memory_block *mem, void *arg)
 
 		beginpa = PFN_PHYS(section_nr_to_pfn(mem->start_section_nr));
 		endpa = PFN_PHYS(section_nr_to_pfn(mem->end_section_nr + 1))-1;
-		pr_warn("removing memory fails, because memory "
-			"[%pa-%pa] is onlined\n",
+		pr_warn("removing memory fails, because memory [%pa-%pa] is onlined\n",
 			&beginpa, &endpa);
 	}
 
* Unmerged path mm/mempolicy.c
* Unmerged path mm/mmap.c
* Unmerged path mm/oom_kill.c
* Unmerged path mm/page_alloc.c
* Unmerged path mm/page_owner.c
* Unmerged path mm/percpu.c
* Unmerged path mm/slab.c
* Unmerged path mm/slab_common.c
* Unmerged path mm/slub.c
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 8ec6748e418b..124ddccc52a2 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -144,8 +144,8 @@ void __meminit vmemmap_verify(pte_t *pte, int node,
 	int actual_node = early_pfn_to_nid(pfn);
 
 	if (node_distance(actual_node, node) > LOCAL_DISTANCE)
-		printk(KERN_WARNING "[%lx-%lx] potential offnode "
-			"page_structs\n", start, end - 1);
+		printk(KERN_WARNING "[%lx-%lx] potential offnode page_structs\n",
+		       start, end - 1);
 }
 
 pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)
@@ -271,8 +271,8 @@ void __init sparse_mem_maps_populate_node(struct page **map_map,
 		if (map_map[pnum])
 			continue;
 		ms = __nr_to_section(pnum);
-		printk(KERN_ERR "%s: sparsemem memory map backing failed "
-			"some memory will not be available.\n", __func__);
+		printk(KERN_ERR "%s: sparsemem memory map backing failed some memory will not be available.\n",
+		       __func__);
 		ms->section_mem_map = 0;
 	}
 
diff --git a/mm/sparse.c b/mm/sparse.c
index 2dd4aed4ce3c..6658c15d5140 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -427,8 +427,8 @@ void __init sparse_mem_maps_populate_node(struct page **map_map,
 		if (map_map[pnum])
 			continue;
 		ms = __nr_to_section(pnum);
-		printk(KERN_ERR "%s: sparsemem memory map backing failed "
-			"some memory will not be available.\n", __func__);
+		printk(KERN_ERR "%s: sparsemem memory map backing failed some memory will not be available.\n",
+		       __func__);
 		ms->section_mem_map = 0;
 	}
 }
@@ -454,8 +454,8 @@ static struct page __init *sparse_early_mem_map_alloc(unsigned long pnum)
 	if (map)
 		return map;
 
-	printk(KERN_ERR "%s: sparsemem memory map backing failed "
-			"some memory will not be available.\n", __func__);
+	printk(KERN_ERR "%s: sparsemem memory map backing failed some memory will not be available.\n",
+	       __func__);
 	ms->section_mem_map = 0;
 	return NULL;
 }
* Unmerged path mm/swapfile.c
* Unmerged path mm/vmalloc.c
