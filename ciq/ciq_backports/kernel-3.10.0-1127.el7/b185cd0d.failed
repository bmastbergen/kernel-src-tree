percpu: update free path to take advantage of contig hints

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Dennis Zhou (Facebook) <dennisszhou@gmail.com>
commit b185cd0dc61c14875155e7bcc3f2c139b6feefd2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/b185cd0d.failed

The bitmap allocator must keep metadata consistent. The easiest way is
to scan after every allocation for each affected block and the entire
chunk. This is rather expensive.

The free path can take advantage of current contig hints to prevent
scanning within the start and end block.  If a scan is needed, it can
be done by scanning backwards from the start and forwards from the end
to identify the entire free area this can be combined with. The blocks
can then be updated by some basic checks rather than complete block
scans.

A chunk scan happens when the freed area makes a page free, a block
free, or spans across blocks. This is necessary as the contig hint at
this point could span across blocks. The check uses the minimum of page
size and the block size to allow for variable sized blocks. There is a
tradeoff here with not updating after every free. It is possible a
contig hint in one block can be merged with the contig hint in the next
block. This means the contig hint can be off by up to a page. However,
if the chunk's contig hint is contained in one block, the contig hint
will be accurate.

	Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit b185cd0dc61c14875155e7bcc3f2c139b6feefd2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 3c1f6f694380,57b3168eae08..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -265,21 -266,45 +265,26 @@@ static void __maybe_unused pcpu_next_po
  }
  
  /*
 - * Bitmap region iterators.  Iterates over the bitmap between
 - * [@start, @end) in @chunk.  @rs and @re should be integer variables
 - * and will be set to start and end index of the current free region.
 + * (Un)populated page region iterators.  Iterate over (un)populated
 + * page regions between @start and @end in @chunk.  @rs and @re should
 + * be integer variables and will be set to start and end page index of
 + * the current region.
   */
 -#define pcpu_for_each_unpop_region(bitmap, rs, re, start, end)		     \
 -	for ((rs) = (start), pcpu_next_unpop((bitmap), &(rs), &(re), (end)); \
 -	     (rs) < (re);						     \
 -	     (rs) = (re) + 1, pcpu_next_unpop((bitmap), &(rs), &(re), (end)))
 +#define pcpu_for_each_unpop_region(chunk, rs, re, start, end)		    \
 +	for ((rs) = (start), pcpu_next_unpop((chunk), &(rs), &(re), (end)); \
 +	     (rs) < (re);						    \
 +	     (rs) = (re) + 1, pcpu_next_unpop((chunk), &(rs), &(re), (end)))
  
 -#define pcpu_for_each_pop_region(bitmap, rs, re, start, end)		     \
 -	for ((rs) = (start), pcpu_next_pop((bitmap), &(rs), &(re), (end));   \
 -	     (rs) < (re);						     \
 -	     (rs) = (re) + 1, pcpu_next_pop((bitmap), &(rs), &(re), (end)))
 -
 -/*
 - * The following are helper functions to help access bitmaps and convert
 - * between bitmap offsets to address offsets.
 - */
 -static unsigned long *pcpu_index_alloc_map(struct pcpu_chunk *chunk, int index)
 -{
 -	return chunk->alloc_map +
 -	       (index * PCPU_BITMAP_BLOCK_BITS / BITS_PER_LONG);
 -}
 -
 -static unsigned long pcpu_off_to_block_index(int off)
 -{
 -	return off / PCPU_BITMAP_BLOCK_BITS;
 -}
 -
 -static unsigned long pcpu_off_to_block_off(int off)
 -{
 -	return off & (PCPU_BITMAP_BLOCK_BITS - 1);
 -}
 +#define pcpu_for_each_pop_region(chunk, rs, re, start, end)		    \
 +	for ((rs) = (start), pcpu_next_pop((chunk), &(rs), &(re), (end));   \
 +	     (rs) < (re);						    \
 +	     (rs) = (re) + 1, pcpu_next_pop((chunk), &(rs), &(re), (end)))
  
+ static unsigned long pcpu_block_off_to_off(int index, int off)
+ {
+ 	return index * PCPU_BITMAP_BLOCK_BITS + off;
+ }
+ 
  /**
   * pcpu_mem_zalloc - allocate memory
   * @size: bytes to allocate
@@@ -653,26 -624,214 +658,234 @@@ static int pcpu_alloc_area(struct pcpu_
  }
  
  /**
 - * pcpu_block_update_hint_free - updates the block hints on the free path
 + * pcpu_free_area - free area to a pcpu_chunk
   * @chunk: chunk of interest
++<<<<<<< HEAD
 + * @freeme: offset of area to free
 + * @occ_pages_p: out param for the number of pages the area occupies
 + *
 + * Free area starting from @freeme to @chunk.  Note that this function
 + * only modifies the allocation map.  It doesn't depopulate or unmap
 + * the area.
 + *
 + * CONTEXT:
 + * pcpu_lock.
++=======
+  * @bit_off: chunk offset
+  * @bits: size of request
+  *
+  * Updates metadata for the allocation path.  This avoids a blind block
+  * refresh by making use of the block contig hints.  If this fails, it scans
+  * forward and backward to determine the extent of the free area.  This is
+  * capped at the boundary of blocks.
+  *
+  * A chunk update is triggered if a page becomes free, a block becomes free,
+  * or the free spans across blocks.  This tradeoff is to minimize iterating
+  * over the block metadata to update chunk->contig_bits.  chunk->contig_bits
+  * may be off by up to a page, but it will never be more than the available
+  * space.  If the contig hint is contained in one block, it will be accurate.
++>>>>>>> b185cd0dc61c (percpu: update free path to take advantage of contig hints)
   */
 -static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 -					int bits)
 +static void pcpu_free_area(struct pcpu_chunk *chunk, int freeme,
 +			   int *occ_pages_p)
  {
++<<<<<<< HEAD
 +	int oslot = pcpu_chunk_slot(chunk);
 +	int off = 0;
 +	unsigned i, j;
 +	int to_free = 0;
 +	int *p;
++=======
+ 	struct pcpu_block_md *s_block, *e_block, *block;
+ 	int s_index, e_index;	/* block indexes of the freed allocation */
+ 	int s_off, e_off;	/* block offsets of the freed allocation */
+ 	int start, end;		/* start and end of the whole free area */
+ 
+ 	/*
+ 	 * Calculate per block offsets.
+ 	 * The calculation uses an inclusive range, but the resulting offsets
+ 	 * are [start, end).  e_index always points to the last block in the
+ 	 * range.
+ 	 */
+ 	s_index = pcpu_off_to_block_index(bit_off);
+ 	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
+ 	s_off = pcpu_off_to_block_off(bit_off);
+ 	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
+ 
+ 	s_block = chunk->md_blocks + s_index;
+ 	e_block = chunk->md_blocks + e_index;
+ 
+ 	/*
+ 	 * Check if the freed area aligns with the block->contig_hint.
+ 	 * If it does, then the scan to find the beginning/end of the
+ 	 * larger free area can be avoided.
+ 	 *
+ 	 * start and end refer to beginning and end of the free area
+ 	 * within each their respective blocks.  This is not necessarily
+ 	 * the entire free area as it may span blocks past the beginning
+ 	 * or end of the block.
+ 	 */
+ 	start = s_off;
+ 	if (s_off == s_block->contig_hint + s_block->contig_hint_start) {
+ 		start = s_block->contig_hint_start;
+ 	} else {
+ 		/*
+ 		 * Scan backwards to find the extent of the free area.
+ 		 * find_last_bit returns the starting bit, so if the start bit
+ 		 * is returned, that means there was no last bit and the
+ 		 * remainder of the chunk is free.
+ 		 */
+ 		int l_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index),
+ 					  start);
+ 		start = (start == l_bit) ? 0 : l_bit + 1;
+ 	}
+ 
+ 	end = e_off;
+ 	if (e_off == e_block->contig_hint_start)
+ 		end = e_block->contig_hint_start + e_block->contig_hint;
+ 	else
+ 		end = find_next_bit(pcpu_index_alloc_map(chunk, e_index),
+ 				    PCPU_BITMAP_BLOCK_BITS, end);
+ 
+ 	/* update s_block */
+ 	e_off = (s_index == e_index) ? end : PCPU_BITMAP_BLOCK_BITS;
+ 	pcpu_block_update(s_block, start, e_off);
+ 
+ 	/* freeing in the same block */
+ 	if (s_index != e_index) {
+ 		/* update e_block */
+ 		pcpu_block_update(e_block, 0, end);
+ 
+ 		/* reset md_blocks in the middle */
+ 		for (block = s_block + 1; block < e_block; block++) {
+ 			block->first_free = 0;
+ 			block->contig_hint_start = 0;
+ 			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
+ 			block->left_free = PCPU_BITMAP_BLOCK_BITS;
+ 			block->right_free = PCPU_BITMAP_BLOCK_BITS;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Refresh chunk metadata when the free makes a page free, a block
+ 	 * free, or spans across blocks.  The contig hint may be off by up to
+ 	 * a page, but if the hint is contained in a block, it will be accurate
+ 	 * with the else condition below.
+ 	 */
+ 	if ((ALIGN_DOWN(end, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS)) >
+ 	     ALIGN(start, min(PCPU_BITS_PER_PAGE, PCPU_BITMAP_BLOCK_BITS))) ||
+ 	    s_index != e_index)
+ 		pcpu_chunk_refresh_hint(chunk);
+ 	else
+ 		pcpu_chunk_update(chunk, pcpu_block_off_to_off(s_index, start),
+ 				  s_block->contig_hint);
+ }
+ 
+ /**
+  * pcpu_is_populated - determines if the region is populated
+  * @chunk: chunk of interest
+  * @bit_off: chunk offset
+  * @bits: size of area
+  * @next_off: return value for the next offset to start searching
+  *
+  * For atomic allocations, check if the backing pages are populated.
+  *
+  * RETURNS:
+  * Bool if the backing pages are populated.
+  * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.
+  */
+ static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
+ 			      int *next_off)
+ {
+ 	int page_start, page_end, rs, re;
+ 
+ 	page_start = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);
+ 	page_end = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
+ 
+ 	rs = page_start;
+ 	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
+ 	if (rs >= page_end)
+ 		return true;
+ 
+ 	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
+ 	return false;
+ }
+ 
+ /**
+  * pcpu_find_block_fit - finds the block index to start searching
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE bytes)
+  * @pop_only: use populated regions only
+  *
+  * RETURNS:
+  * The offset in the bitmap to begin searching.
+  * -1 if no offset is found.
+  */
+ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
+ 			       size_t align, bool pop_only)
+ {
+ 	int bit_off, bits;
+ 	int re; /* region end */
+ 
+ 	/*
+ 	 * Check to see if the allocation can fit in the chunk's contig hint.
+ 	 * This is an optimization to prevent scanning by assuming if it
+ 	 * cannot fit in the global hint, there is memory pressure and creating
+ 	 * a new chunk would happen soon.
+ 	 */
+ 	bit_off = ALIGN(chunk->contig_bits_start, align) -
+ 		  chunk->contig_bits_start;
+ 	if (bit_off + alloc_bits > chunk->contig_bits)
+ 		return -1;
+ 
+ 	pcpu_for_each_unpop_region(chunk->alloc_map, bit_off, re,
+ 				   chunk->first_bit,
+ 				   pcpu_chunk_map_bits(chunk)) {
+ 		bits = re - bit_off;
+ 
+ 		/* check alignment */
+ 		bits -= ALIGN(bit_off, align) - bit_off;
+ 		bit_off = ALIGN(bit_off, align);
+ 		if (bits < alloc_bits)
+ 			continue;
+ 
+ 		bits = alloc_bits;
+ 		if (!pop_only || pcpu_is_populated(chunk, bit_off, bits,
+ 						   &bit_off))
+ 			break;
+ 
+ 		bits = 0;
+ 	}
+ 
+ 	if (bit_off == pcpu_chunk_map_bits(chunk))
+ 		return -1;
+ 
+ 	return bit_off;
+ }
+ 
+ /**
+  * pcpu_alloc_area - allocates an area from a pcpu_chunk
+  * @chunk: chunk of interest
+  * @alloc_bits: size of request in allocation units
+  * @align: alignment of area (max PAGE_SIZE)
+  * @start: bit_off to start searching
+  *
+  * This function takes in a @start offset to begin searching to fit an
+  * allocation of @alloc_bits with alignment @align.  If it confirms a
+  * valid free area, it then updates the allocation and boundary maps
+  * accordingly.
+  *
+  * RETURNS:
+  * Allocated addr offset in @chunk on success.
+  * -1 if no matching area is found.
+  */
+ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
+ 			   size_t align, int start)
+ {
+ 	size_t align_mask = (align) ? (align - 1) : 0;
+ 	int bit_off, end, oslot;
++>>>>>>> b185cd0dc61c (percpu: update free path to take advantage of contig hints)
  
  	lockdep_assert_held(&pcpu_lock);
  
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index 004298923ccf..14daee5160e3 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -23,6 +23,9 @@
 	 PERCPU_MODULE_RESERVE)
 #endif
 
+/* number of bits per page, used to trigger a scan if blocks are > PAGE_SIZE */
+#define PCPU_BITS_PER_PAGE		(PAGE_SIZE >> PCPU_MIN_ALLOC_SHIFT)
+
 /*
  * Must be an lvalue. Since @var must be a simple identifier,
  * we force a syntax error here if it isn't.
* Unmerged path mm/percpu.c
