mm: numa: avoid waiting on freed migrated pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Mark Rutland <mark.rutland@arm.com>
commit 3c226c637b69104f6b9f1c6ec5b08d7b741b3229
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/3c226c63.failed

In do_huge_pmd_numa_page(), we attempt to handle a migrating thp pmd by
waiting until the pmd is unlocked before we return and retry.  However,
we can race with migrate_misplaced_transhuge_page():

    // do_huge_pmd_numa_page                // migrate_misplaced_transhuge_page()
    // Holds 0 refs on page                 // Holds 2 refs on page

    vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
    /* ... */
    if (pmd_trans_migrating(*vmf->pmd)) {
            page = pmd_page(*vmf->pmd);
            spin_unlock(vmf->ptl);
                                            ptl = pmd_lock(mm, pmd);
                                            if (page_count(page) != 2)) {
                                                    /* roll back */
                                            }
                                            /* ... */
                                            mlock_migrate_page(new_page, page);
                                            /* ... */
                                            spin_unlock(ptl);
                                            put_page(page);
                                            put_page(page); // page freed here
            wait_on_page_locked(page);
            goto out;
    }

This can result in the freed page having its waiters flag set
unexpectedly, which trips the PAGE_FLAGS_CHECK_AT_PREP checks in the
page alloc/free functions.  This has been observed on arm64 KVM guests.

We can avoid this by having do_huge_pmd_numa_page() take a reference on
the page before dropping the pmd lock, mirroring what we do in
__migration_entry_wait().

When we hit the race, migrate_misplaced_transhuge_page() will see the
reference and abort the migration, as it may do today in other cases.

Fixes: b8916634b77bffb2 ("mm: Prevent parallel splits during THP migration")
Link: http://lkml.kernel.org/r/1497349722-6731-2-git-send-email-will.deacon@arm.com
	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Steve Capper <steve.capper@arm.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3c226c637b69104f6b9f1c6ec5b08d7b741b3229)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 5c622aeff5fe,88c6167f194d..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1652,8 -1426,11 +1652,15 @@@ int do_huge_pmd_numa_page(struct vm_fau
  	 */
  	if (unlikely(pmd_trans_migrating(*vmf->pmd))) {
  		page = pmd_page(*vmf->pmd);
++<<<<<<< HEAD
 +		spin_unlock(ptl);
++=======
+ 		if (!get_page_unless_zero(page))
+ 			goto out_unlock;
+ 		spin_unlock(vmf->ptl);
++>>>>>>> 3c226c637b69 (mm: numa: avoid waiting on freed migrated pages)
  		wait_on_page_locked(page);
+ 		put_page(page);
  		goto out;
  	}
  
@@@ -1689,9 -1462,12 +1696,16 @@@
  
  	/* Migration could have started since the pmd_trans_migrating check */
  	if (!page_locked) {
++<<<<<<< HEAD
 +		spin_unlock(ptl);
- 		wait_on_page_locked(page);
++=======
  		page_nid = -1;
+ 		if (!get_page_unless_zero(page))
+ 			goto out_unlock;
+ 		spin_unlock(vmf->ptl);
++>>>>>>> 3c226c637b69 (mm: numa: avoid waiting on freed migrated pages)
+ 		wait_on_page_locked(page);
+ 		put_page(page);
  		goto out;
  	}
  
* Unmerged path mm/huge_memory.c
