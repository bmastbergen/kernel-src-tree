s390/jump_label: replace stop_machine with smp_call_function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
Rebuild_CHGLOG: - [s390] jump_label: replace stop_machine with smp_call_function (Hendrik Brueckner) [1720389 1720387]
Rebuild_FUZZ: 95.65%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit a646ef398e72a2ac40bea974808ffcf1bea4e7f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/a646ef39.failed

The use of stop_machine to replace the mask bits of the jump label branch
is a very heavy-weight operation. This is in fact not necessary, the
mask of the branch can simply be updated, followed by a signal processor
to all the other CPUs to force them to pick up the modified instruction.

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
[heiko.carstens@de.ibm.com]: Change jump_label_make_nop() so we get
                             brcl 0,offset instead of brcl 0,0. This
                             makes sure that only the mask part of the
                             instruction gets changed when updated.
	Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
(cherry picked from commit a646ef398e72a2ac40bea974808ffcf1bea4e7f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kernel/jump_label.c
#	arch/s390/mm/maccess.c
diff --cc arch/s390/kernel/jump_label.c
index b987ab2c1541,e193630a7d2a..000000000000
--- a/arch/s390/kernel/jump_label.c
+++ b/arch/s390/kernel/jump_label.c
@@@ -22,43 -20,72 +22,87 @@@ struct insn_args 
  	enum jump_label_type type;
  };
  
++<<<<<<< HEAD
++=======
+ static void jump_label_make_nop(struct jump_entry *entry, struct insn *insn)
+ {
+ 	/* brcl 0,offset */
+ 	insn->opcode = 0xc004;
+ 	insn->offset = (jump_entry_target(entry) - jump_entry_code(entry)) >> 1;
+ }
+ 
+ static void jump_label_make_branch(struct jump_entry *entry, struct insn *insn)
+ {
+ 	/* brcl 15,offset */
+ 	insn->opcode = 0xc0f4;
+ 	insn->offset = (jump_entry_target(entry) - jump_entry_code(entry)) >> 1;
+ }
+ 
+ static void jump_label_bug(struct jump_entry *entry, struct insn *expected,
+ 			   struct insn *new)
+ {
+ 	unsigned char *ipc = (unsigned char *)jump_entry_code(entry);
+ 	unsigned char *ipe = (unsigned char *)expected;
+ 	unsigned char *ipn = (unsigned char *)new;
+ 
+ 	pr_emerg("Jump label code mismatch at %pS [%p]\n", ipc, ipc);
+ 	pr_emerg("Found:    %6ph\n", ipc);
+ 	pr_emerg("Expected: %6ph\n", ipe);
+ 	pr_emerg("New:      %6ph\n", ipn);
+ 	panic("Corrupted kernel text");
+ }
+ 
+ static struct insn orignop = {
+ 	.opcode = 0xc004,
+ 	.offset = JUMP_LABEL_NOP_OFFSET >> 1,
+ };
+ 
++>>>>>>> a646ef398e72 (s390/jump_label: replace stop_machine with smp_call_function)
  static void __jump_label_transform(struct jump_entry *entry,
 -				   enum jump_label_type type,
 -				   int init)
 +				   enum jump_label_type type)
  {
 -	void *code = (void *)jump_entry_code(entry);
 -	struct insn old, new;
 +	struct insn insn;
 +	int rc;
  
 -	if (type == JUMP_LABEL_JMP) {
 -		jump_label_make_nop(entry, &old);
 -		jump_label_make_branch(entry, &new);
 -	} else {
 -		jump_label_make_branch(entry, &old);
 -		jump_label_make_nop(entry, &new);
 -	}
 -	if (init) {
 -		if (memcmp(code, &orignop, sizeof(orignop)))
 -			jump_label_bug(entry, &orignop, &new);
 +	if (type == JUMP_LABEL_ENABLE) {
 +		/* brcl 15,offset */
 +		insn.opcode = 0xc0f4;
 +		insn.offset = (entry->target - entry->code) >> 1;
  	} else {
 -		if (memcmp(code, &old, sizeof(old)))
 -			jump_label_bug(entry, &old, &new);
 +		/* brcl 0,0 */
 +		insn.opcode = 0xc004;
 +		insn.offset = 0;
  	}
 -	s390_kernel_write(code, &new, sizeof(new));
 +
 +	rc = probe_kernel_write((void *)entry->code, &insn, JUMP_LABEL_NOP_SIZE);
 +	WARN_ON_ONCE(rc < 0);
  }
  
- static int __sm_arch_jump_label_transform(void *data)
+ static void __jump_label_sync(void *dummy)
  {
++<<<<<<< HEAD
 +	struct insn_args *args = data;
 +
 +	__jump_label_transform(args->entry, args->type);
 +	return 0;
++=======
++>>>>>>> a646ef398e72 (s390/jump_label: replace stop_machine with smp_call_function)
  }
  
  void arch_jump_label_transform(struct jump_entry *entry,
  			       enum jump_label_type type)
  {
++<<<<<<< HEAD
 +	struct insn_args args;
 +
 +	args.entry = entry;
 +	args.type = type;
 +
 +	stop_machine(__sm_arch_jump_label_transform, &args, NULL);
++=======
+ 	__jump_label_transform(entry, type, 0);
+ 	smp_call_function(__jump_label_sync, NULL, 1);
++>>>>>>> a646ef398e72 (s390/jump_label: replace stop_machine with smp_call_function)
  }
  
  void arch_jump_label_transform_static(struct jump_entry *entry,
diff --cc arch/s390/mm/maccess.c
index a2df9800a8b2,1864a8bb9622..000000000000
--- a/arch/s390/mm/maccess.c
+++ b/arch/s390/mm/maccess.c
@@@ -13,54 -14,60 +13,79 @@@
  #include <linux/errno.h>
  #include <linux/gfp.h>
  #include <linux/cpu.h>
 +#include <linux/export.h>
  #include <asm/ctl_reg.h>
 -#include <asm/io.h>
 -#include <asm/stacktrace.h>
  
 -static notrace long s390_kernel_write_odd(void *dst, const void *src, size_t size)
 +/*
++<<<<<<< HEAD
 + * This function writes to kernel memory bypassing DAT and possible
 + * write protection. It copies one to four bytes from src to dst
 + * using the stura instruction.
 + * Returns the number of bytes copied or -EFAULT.
 + */
 +static long probe_kernel_write_odd(void *dst, const void *src, size_t size)
  {
 -	unsigned long aligned, offset, count;
 -	char tmp[8];
 +	unsigned long count, aligned;
 +	int offset, mask;
 +	int rc = -EFAULT;
  
 -	aligned = (unsigned long) dst & ~7UL;
 -	offset = (unsigned long) dst & 7UL;
 -	size = min(8UL - offset, size);
 -	count = size - 1;
 +	aligned = (unsigned long) dst & ~3UL;
 +	offset = (unsigned long) dst & 3;
 +	count = min_t(unsigned long, 4 - offset, size);
 +	mask = (0xf << (4 - count)) & 0xf;
 +	mask >>= offset;
  	asm volatile(
  		"	bras	1,0f\n"
 -		"	mvc	0(1,%4),0(%5)\n"
 -		"0:	mvc	0(8,%3),0(%0)\n"
 -		"	ex	%1,0(1)\n"
 -		"	lg	%1,0(%3)\n"
 -		"	lra	%0,0(%0)\n"
 -		"	sturg	%1,%0\n"
 -		: "+&a" (aligned), "+&a" (count), "=m" (tmp)
 -		: "a" (&tmp), "a" (&tmp[offset]), "a" (src)
 -		: "cc", "memory", "1");
 -	return size;
 +		"	icm	0,0,0(%3)\n"
 +		"0:	l	0,0(%1)\n"
 +		"	lra	%1,0(%1)\n"
 +		"1:	ex	%2,0(1)\n"
 +		"2:	stura	0,%1\n"
 +		"	la	%0,0\n"
 +		"3:\n"
 +		EX_TABLE(0b,3b) EX_TABLE(1b,3b) EX_TABLE(2b,3b)
 +		: "+d" (rc), "+a" (aligned)
 +		: "a" (mask), "a" (src) : "cc", "memory", "0", "1");
 +	return rc ? rc : count;
  }
  
 -/*
 +long probe_kernel_write(void *dst, const void *src, size_t size)
 +{
 +	long copied = 0;
++=======
+  * s390_kernel_write - write to kernel memory bypassing DAT
+  * @dst: destination address
+  * @src: source address
+  * @size: number of bytes to copy
+  *
+  * This function writes to kernel memory bypassing DAT and possible page table
+  * write protection. It writes to the destination using the sturg instruction.
+  * Therefore we have a read-modify-write sequence: the function reads eight
+  * bytes from destination at an eight byte boundary, modifies the bytes
+  * requested and writes the result back in a loop.
+  */
+ static DEFINE_SPINLOCK(s390_kernel_write_lock);
+ 
+ void notrace s390_kernel_write(void *dst, const void *src, size_t size)
+ {
+ 	unsigned long flags;
+ 	long copied;
++>>>>>>> a646ef398e72 (s390/jump_label: replace stop_machine with smp_call_function)
  
+ 	spin_lock_irqsave(&s390_kernel_write_lock, flags);
  	while (size) {
 -		copied = s390_kernel_write_odd(dst, src, size);
 +		copied = probe_kernel_write_odd(dst, src, size);
 +		if (copied < 0)
 +			break;
  		dst += copied;
  		src += copied;
  		size -= copied;
  	}
++<<<<<<< HEAD
 +	return copied < 0 ? -EFAULT : 0;
++=======
+ 	spin_unlock_irqrestore(&s390_kernel_write_lock, flags);
++>>>>>>> a646ef398e72 (s390/jump_label: replace stop_machine with smp_call_function)
  }
  
  static int __memcpy_real(void *dest, void *src, size_t count)
* Unmerged path arch/s390/kernel/jump_label.c
* Unmerged path arch/s390/mm/maccess.c
