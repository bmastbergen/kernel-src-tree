x86/speculation: Fix incorrect MDS/TAA mitigation status

jira LE-1907
cve CVE-2019-11135
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Waiman Long <longman@redhat.com>
commit 64870ed1b12e235cfca3f6c6da75b542c973ff78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/64870ed1.failed

For MDS vulnerable processors with TSX support, enabling either MDS or
TAA mitigations will enable the use of VERW to flush internal processor
buffers at the right code path. IOW, they are either both mitigated
or both not. However, if the command line options are inconsistent,
the vulnerabilites sysfs files may not report the mitigation status
correctly.

For example, with only the "mds=off" option:

  vulnerabilities/mds:Vulnerable; SMT vulnerable
  vulnerabilities/tsx_async_abort:Mitigation: Clear CPU buffers; SMT vulnerable

The mds vulnerabilities file has wrong status in this case. Similarly,
the taa vulnerability file will be wrong with mds mitigation on, but
taa off.

Change taa_select_mitigation() to sync up the two mitigation status
and have them turned off if both "mds=off" and "tsx_async_abort=off"
are present.

Update documentation to emphasize the fact that both "mds=off" and
"tsx_async_abort=off" have to be specified together for processors that
are affected by both TAA and MDS to be effective.

 [ bp: Massage and add kernel-parameters.txt change too. ]

Fixes: 1b42f017415b ("x86/speculation/taa: Add mitigation for TSX Async Abort")
	Signed-off-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: linux-doc@vger.kernel.org
	Cc: Mark Gross <mgross@linux.intel.com>
	Cc: <stable@vger.kernel.org>
	Cc: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Tyler Hicks <tyhicks@canonical.com>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/20191115161445.30809-2-longman@redhat.com
(cherry picked from commit 64870ed1b12e235cfca3f6c6da75b542c973ff78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/hw-vuln/tsx_async_abort.rst
#	Documentation/kernel-parameters.txt
#	arch/x86/kernel/cpu/bugs.c
diff --cc Documentation/kernel-parameters.txt
index 4ee8a23b1fd1,9983ac73b66d..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -3709,6 -4873,83 +3715,86 @@@ bytes respectively. Such letter suffixe
  			Used to run time disable IRQ_TIME_ACCOUNTING on any
  			platforms where RDTSC is slow and this accounting
  			can add overhead.
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 			[x86] unstable: mark the TSC clocksource as unstable, this
+ 			marks the TSC unconditionally unstable at bootup and
+ 			avoids any further wobbles once the TSC watchdog notices.
+ 			[x86] nowatchdog: disable clocksource watchdog. Used
+ 			in situations with strict latency requirements (where
+ 			interruptions from clocksource watchdog are not
+ 			acceptable).
+ 
+ 	tsx=		[X86] Control Transactional Synchronization
+ 			Extensions (TSX) feature in Intel processors that
+ 			support TSX control.
+ 
+ 			This parameter controls the TSX feature. The options are:
+ 
+ 			on	- Enable TSX on the system. Although there are
+ 				mitigations for all known security vulnerabilities,
+ 				TSX has been known to be an accelerator for
+ 				several previous speculation-related CVEs, and
+ 				so there may be unknown	security risks associated
+ 				with leaving it enabled.
+ 
+ 			off	- Disable TSX on the system. (Note that this
+ 				option takes effect only on newer CPUs which are
+ 				not vulnerable to MDS, i.e., have
+ 				MSR_IA32_ARCH_CAPABILITIES.MDS_NO=1 and which get
+ 				the new IA32_TSX_CTRL MSR through a microcode
+ 				update. This new MSR allows for the reliable
+ 				deactivation of the TSX functionality.)
+ 
+ 			auto	- Disable TSX if X86_BUG_TAA is present,
+ 				  otherwise enable TSX on the system.
+ 
+ 			Not specifying this option is equivalent to tsx=off.
+ 
+ 			See Documentation/admin-guide/hw-vuln/tsx_async_abort.rst
+ 			for more details.
+ 
+ 	tsx_async_abort= [X86,INTEL] Control mitigation for the TSX Async
+ 			Abort (TAA) vulnerability.
+ 
+ 			Similar to Micro-architectural Data Sampling (MDS)
+ 			certain CPUs that support Transactional
+ 			Synchronization Extensions (TSX) are vulnerable to an
+ 			exploit against CPU internal buffers which can forward
+ 			information to a disclosure gadget under certain
+ 			conditions.
+ 
+ 			In vulnerable processors, the speculatively forwarded
+ 			data can be used in a cache side channel attack, to
+ 			access data to which the attacker does not have direct
+ 			access.
+ 
+ 			This parameter controls the TAA mitigation.  The
+ 			options are:
+ 
+ 			full       - Enable TAA mitigation on vulnerable CPUs
+ 				     if TSX is enabled.
+ 
+ 			full,nosmt - Enable TAA mitigation and disable SMT on
+ 				     vulnerable CPUs. If TSX is disabled, SMT
+ 				     is not disabled because CPU is not
+ 				     vulnerable to cross-thread TAA attacks.
+ 			off        - Unconditionally disable TAA mitigation
+ 
+ 			On MDS-affected machines, tsx_async_abort=off can be
+ 			prevented by an active MDS mitigation as both vulnerabilities
+ 			are mitigated with the same mechanism so in order to disable
+ 			this mitigation, you need to specify mds=off too.
+ 
+ 			Not specifying this option is equivalent to
+ 			tsx_async_abort=full.  On CPUs which are MDS affected
+ 			and deploy MDS mitigation, TAA mitigation is not
+ 			required and doesn't provide any additional
+ 			mitigation.
+ 
+ 			For details see:
+ 			Documentation/admin-guide/hw-vuln/tsx_async_abort.rst
++>>>>>>> 64870ed1b12e (x86/speculation: Fix incorrect MDS/TAA mitigation status):Documentation/admin-guide/kernel-parameters.txt
  
  	turbografx.map[2|3]=	[HW,JOY]
  			TurboGraFX parallel port interface
diff --cc arch/x86/kernel/cpu/bugs.c
index 9ec6cfa4f503,cb513eaa0df1..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -216,9 -271,233 +216,211 @@@ static int __init mds_cmdline(char *str
  early_param("mds", mds_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"TAA: " fmt
+ 
+ /* Default mitigation for TAA-affected CPUs */
+ static enum taa_mitigations taa_mitigation __ro_after_init = TAA_MITIGATION_VERW;
+ static bool taa_nosmt __ro_after_init;
+ 
+ static const char * const taa_strings[] = {
+ 	[TAA_MITIGATION_OFF]		= "Vulnerable",
+ 	[TAA_MITIGATION_UCODE_NEEDED]	= "Vulnerable: Clear CPU buffers attempted, no microcode",
+ 	[TAA_MITIGATION_VERW]		= "Mitigation: Clear CPU buffers",
+ 	[TAA_MITIGATION_TSX_DISABLED]	= "Mitigation: TSX disabled",
+ };
+ 
+ static void __init taa_select_mitigation(void)
+ {
+ 	u64 ia32_cap;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_TAA)) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 		return;
+ 	}
+ 
+ 	/* TSX previously disabled by tsx=off */
+ 	if (!boot_cpu_has(X86_FEATURE_RTM)) {
+ 		taa_mitigation = TAA_MITIGATION_TSX_DISABLED;
+ 		goto out;
+ 	}
+ 
+ 	if (cpu_mitigations_off()) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * TAA mitigation via VERW is turned off if both
+ 	 * tsx_async_abort=off and mds=off are specified.
+ 	 */
+ 	if (taa_mitigation == TAA_MITIGATION_OFF &&
+ 	    mds_mitigation == MDS_MITIGATION_OFF)
+ 		goto out;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_MD_CLEAR))
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 	else
+ 		taa_mitigation = TAA_MITIGATION_UCODE_NEEDED;
+ 
+ 	/*
+ 	 * VERW doesn't clear the CPU buffers when MD_CLEAR=1 and MDS_NO=1.
+ 	 * A microcode update fixes this behavior to clear CPU buffers. It also
+ 	 * adds support for MSR_IA32_TSX_CTRL which is enumerated by the
+ 	 * ARCH_CAP_TSX_CTRL_MSR bit.
+ 	 *
+ 	 * On MDS_NO=1 CPUs if ARCH_CAP_TSX_CTRL_MSR is not set, microcode
+ 	 * update is required.
+ 	 */
+ 	ia32_cap = x86_read_arch_cap_msr();
+ 	if ( (ia32_cap & ARCH_CAP_MDS_NO) &&
+ 	    !(ia32_cap & ARCH_CAP_TSX_CTRL_MSR))
+ 		taa_mitigation = TAA_MITIGATION_UCODE_NEEDED;
+ 
+ 	/*
+ 	 * TSX is enabled, select alternate mitigation for TAA which is
+ 	 * the same as MDS. Enable MDS static branch to clear CPU buffers.
+ 	 *
+ 	 * For guests that can't determine whether the correct microcode is
+ 	 * present on host, enable the mitigation for UCODE_NEEDED as well.
+ 	 */
+ 	static_branch_enable(&mds_user_clear);
+ 
+ 	if (taa_nosmt || cpu_mitigations_auto_nosmt())
+ 		cpu_smt_disable(false);
+ 
+ 	/*
+ 	 * Update MDS mitigation, if necessary, as the mds_user_clear is
+ 	 * now enabled for TAA mitigation.
+ 	 */
+ 	if (mds_mitigation == MDS_MITIGATION_OFF &&
+ 	    boot_cpu_has_bug(X86_BUG_MDS)) {
+ 		mds_mitigation = MDS_MITIGATION_FULL;
+ 		mds_select_mitigation();
+ 	}
+ out:
+ 	pr_info("%s\n", taa_strings[taa_mitigation]);
+ }
+ 
+ static int __init tsx_async_abort_parse_cmdline(char *str)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_TAA))
+ 		return 0;
+ 
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off")) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 	} else if (!strcmp(str, "full")) {
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 	} else if (!strcmp(str, "full,nosmt")) {
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 		taa_nosmt = true;
+ 	}
+ 
+ 	return 0;
+ }
+ early_param("tsx_async_abort", tsx_async_abort_parse_cmdline);
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Spectre V1 : " fmt
+ 
+ enum spectre_v1_mitigation {
+ 	SPECTRE_V1_MITIGATION_NONE,
+ 	SPECTRE_V1_MITIGATION_AUTO,
+ };
+ 
+ static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
+ 	SPECTRE_V1_MITIGATION_AUTO;
+ 
+ static const char * const spectre_v1_strings[] = {
+ 	[SPECTRE_V1_MITIGATION_NONE] = "Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers",
+ 	[SPECTRE_V1_MITIGATION_AUTO] = "Mitigation: usercopy/swapgs barriers and __user pointer sanitization",
+ };
+ 
+ /*
+  * Does SMAP provide full mitigation against speculative kernel access to
+  * userspace?
+  */
+ static bool smap_works_speculatively(void)
+ {
+ 	if (!boot_cpu_has(X86_FEATURE_SMAP))
+ 		return false;
+ 
+ 	/*
+ 	 * On CPUs which are vulnerable to Meltdown, SMAP does not
+ 	 * prevent speculative access to user data in the L1 cache.
+ 	 * Consider SMAP to be non-functional as a mitigation on these
+ 	 * CPUs.
+ 	 */
+ 	if (boot_cpu_has(X86_BUG_CPU_MELTDOWN))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static void __init spectre_v1_select_mitigation(void)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+ 		spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ 		return;
+ 	}
+ 
+ 	if (spectre_v1_mitigation == SPECTRE_V1_MITIGATION_AUTO) {
+ 		/*
+ 		 * With Spectre v1, a user can speculatively control either
+ 		 * path of a conditional swapgs with a user-controlled GS
+ 		 * value.  The mitigation is to add lfences to both code paths.
+ 		 *
+ 		 * If FSGSBASE is enabled, the user can put a kernel address in
+ 		 * GS, in which case SMAP provides no protection.
+ 		 *
+ 		 * [ NOTE: Don't check for X86_FEATURE_FSGSBASE until the
+ 		 *	   FSGSBASE enablement patches have been merged. ]
+ 		 *
+ 		 * If FSGSBASE is disabled, the user can only put a user space
+ 		 * address in GS.  That makes an attack harder, but still
+ 		 * possible if there's no SMAP protection.
+ 		 */
+ 		if (!smap_works_speculatively()) {
+ 			/*
+ 			 * Mitigation can be provided from SWAPGS itself or
+ 			 * PTI as the CR3 write in the Meltdown mitigation
+ 			 * is serializing.
+ 			 *
+ 			 * If neither is there, mitigate with an LFENCE to
+ 			 * stop speculation through swapgs.
+ 			 */
+ 			if (boot_cpu_has_bug(X86_BUG_SWAPGS) &&
+ 			    !boot_cpu_has(X86_FEATURE_PTI))
+ 				setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_USER);
+ 
+ 			/*
+ 			 * Enable lfences in the kernel entry (non-swapgs)
+ 			 * paths, to prevent user entry from speculatively
+ 			 * skipping swapgs.
+ 			 */
+ 			setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_KERNEL);
+ 		}
+ 	}
+ 
+ 	pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ }
+ 
+ static int __init nospectre_v1_cmdline(char *str)
+ {
+ 	spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ 	return 0;
+ }
+ early_param("nospectre_v1", nospectre_v1_cmdline);
+ 
+ #undef pr_fmt
++>>>>>>> 64870ed1b12e (x86/speculation: Fix incorrect MDS/TAA mitigation status)
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
 -static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
 -	SPECTRE_V2_NONE;
 -
 -static enum spectre_v2_user_mitigation spectre_v2_user __ro_after_init =
 -	SPECTRE_V2_USER_NONE;
 -
 -#ifdef CONFIG_RETPOLINE
 -static bool spectre_v2_bad_module;
 -
 -bool retpoline_module_ok(bool has_retpoline)
 -{
 -	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
 -		return true;
 -
 -	pr_err("System may be vulnerable to spectre v2\n");
 -	spectre_v2_bad_module = true;
 -	return false;
 -}
 -
 -static inline const char *spectre_v2_module_string(void)
 -{
 -	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
 -}
 -#else
 -static inline const char *spectre_v2_module_string(void) { return ""; }
 -#endif
 +enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;
  
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
* Unmerged path Documentation/admin-guide/hw-vuln/tsx_async_abort.rst
diff --git a/Documentation/admin-guide/hw-vuln/mds.rst b/Documentation/admin-guide/hw-vuln/mds.rst
index e3a796c0d3a2..2d19c9f4c1fe 100644
--- a/Documentation/admin-guide/hw-vuln/mds.rst
+++ b/Documentation/admin-guide/hw-vuln/mds.rst
@@ -265,8 +265,11 @@ time with the option "mds=". The valid arguments for this option are:
 
   ============  =============================================================
 
-Not specifying this option is equivalent to "mds=full".
-
+Not specifying this option is equivalent to "mds=full". For processors
+that are affected by both TAA (TSX Asynchronous Abort) and MDS,
+specifying just "mds=off" without an accompanying "tsx_async_abort=off"
+will have no effect as the same mitigation is used for both
+vulnerabilities.
 
 Mitigation selection guide
 --------------------------
* Unmerged path Documentation/admin-guide/hw-vuln/tsx_async_abort.rst
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/x86/kernel/cpu/bugs.c
