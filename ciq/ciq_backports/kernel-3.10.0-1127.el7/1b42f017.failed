x86/speculation/taa: Add mitigation for TSX Async Abort

jira LE-1907
cve CVE-2019-11135
Rebuild_History Non-Buildable kernel-3.10.0-1127.el7
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit 1b42f017415b46c317e71d41c34ec088417a1883
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1127.el7/1b42f017.failed

TSX Async Abort (TAA) is a side channel vulnerability to the internal
buffers in some Intel processors similar to Microachitectural Data
Sampling (MDS). In this case, certain loads may speculatively pass
invalid data to dependent operations when an asynchronous abort
condition is pending in a TSX transaction.

This includes loads with no fault or assist condition. Such loads may
speculatively expose stale data from the uarch data structures as in
MDS. Scope of exposure is within the same-thread and cross-thread. This
issue affects all current processors that support TSX, but do not have
ARCH_CAP_TAA_NO (bit 8) set in MSR_IA32_ARCH_CAPABILITIES.

On CPUs which have their IA32_ARCH_CAPABILITIES MSR bit MDS_NO=0,
CPUID.MD_CLEAR=1 and the MDS mitigation is clearing the CPU buffers
using VERW or L1D_FLUSH, there is no additional mitigation needed for
TAA. On affected CPUs with MDS_NO=1 this issue can be mitigated by
disabling the Transactional Synchronization Extensions (TSX) feature.

A new MSR IA32_TSX_CTRL in future and current processors after a
microcode update can be used to control the TSX feature. There are two
bits in that MSR:

* TSX_CTRL_RTM_DISABLE disables the TSX sub-feature Restricted
Transactional Memory (RTM).

* TSX_CTRL_CPUID_CLEAR clears the RTM enumeration in CPUID. The other
TSX sub-feature, Hardware Lock Elision (HLE), is unconditionally
disabled with updated microcode but still enumerated as present by
CPUID(EAX=7).EBX{bit4}.

The second mitigation approach is similar to MDS which is clearing the
affected CPU buffers on return to user space and when entering a guest.
Relevant microcode update is required for the mitigation to work.  More
details on this approach can be found here:

  https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html

The TSX feature can be controlled by the "tsx" command line parameter.
If it is force-enabled then "Clear CPU buffers" (MDS mitigation) is
deployed. The effective mitigation state can be read from sysfs.

 [ bp:
   - massage + comments cleanup
   - s/TAA_MITIGATION_TSX_DISABLE/TAA_MITIGATION_TSX_DISABLED/g - Josh.
   - remove partial TAA mitigation in update_mds_branch_idle() - Josh.
   - s/tsx_async_abort_cmdline/tsx_async_abort_parse_cmdline/g
 ]

	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

(cherry picked from commit 1b42f017415b46c317e71d41c34ec088417a1883)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/processor.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
diff --cc arch/x86/include/asm/cpufeatures.h
index dbaf1dd9cc7a,989e03544f18..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -358,5 -398,7 +358,10 @@@
  #define X86_BUG_L1TF			X86_BUG(18) /* CPU is affected by L1 Terminal Fault */
  #define X86_BUG_MDS			X86_BUG(19) /* CPU is affected by Microarchitectural data sampling */
  #define X86_BUG_MSBDS_ONLY		X86_BUG(20) /* CPU is only affected by the  MSDBS variant of BUG_MDS */
++<<<<<<< HEAD
++=======
+ #define X86_BUG_SWAPGS			X86_BUG(21) /* CPU is affected by speculation through SWAPGS */
+ #define X86_BUG_TAA			X86_BUG(22) /* CPU is affected by TSX Async Abort(TAA) */
++>>>>>>> 1b42f017415b (x86/speculation/taa: Add mitigation for TSX Async Abort)
  
  #endif /* _ASM_X86_CPUFEATURES_H */
diff --cc arch/x86/include/asm/processor.h
index 004d7b98c9bf,54f5d54280f6..000000000000
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@@ -1109,6 -988,11 +1109,15 @@@ enum mds_mitigations 
  	MDS_MITIGATION_VMWERV,
  };
  
++<<<<<<< HEAD
 +extern enum mds_mitigations mds_mitigation;
++=======
+ enum taa_mitigations {
+ 	TAA_MITIGATION_OFF,
+ 	TAA_MITIGATION_UCODE_NEEDED,
+ 	TAA_MITIGATION_VERW,
+ 	TAA_MITIGATION_TSX_DISABLED,
+ };
++>>>>>>> 1b42f017415b (x86/speculation/taa: Add mitigation for TSX Async Abort)
  
  #endif /* _ASM_X86_PROCESSOR_H */
diff --cc arch/x86/kernel/cpu/bugs.c
index 9ec6cfa4f503,58fe3746e333..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -22,28 -27,50 +22,32 @@@
  #include <asm/paravirt.h>
  #include <asm/alternative.h>
  #include <asm/pgtable.h>
 -#include <asm/set_memory.h>
 -#include <asm/intel-family.h>
 -#include <asm/e820/api.h>
 +#include <asm/cacheflush.h>
 +#include <asm/spec_ctrl.h>
  #include <asm/hypervisor.h>
 +#include <asm/intel-family.h>
 +#include <linux/prctl.h>
 +#include <linux/sched/smt.h>
  
 -#include "cpu.h"
  
 -static void __init spectre_v1_select_mitigation(void);
  static void __init spectre_v2_select_mitigation(void);
 -static void __init ssb_select_mitigation(void);
 +static void __init ssb_parse_cmdline(void);
 +void ssb_select_mitigation(void);
  static void __init l1tf_select_mitigation(void);
  static void __init mds_select_mitigation(void);
++<<<<<<< HEAD
 +extern void spec_ctrl_save_msr(void);
++=======
+ static void __init taa_select_mitigation(void);
++>>>>>>> 1b42f017415b (x86/speculation/taa: Add mitigation for TSX Async Abort)
  
 -/* The base value of the SPEC_CTRL MSR that always has to be preserved. */
 -u64 x86_spec_ctrl_base;
 -EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
  static DEFINE_MUTEX(spec_ctrl_mutex);
  
 -/*
 - * The vendor and possibly platform specific bits which can be modified in
 - * x86_spec_ctrl_base.
 - */
 -static u64 __ro_after_init x86_spec_ctrl_mask = SPEC_CTRL_IBRS;
 -
 -/*
 - * AMD specific MSR info for Speculative Store Bypass control.
 - * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().
 - */
 -u64 __ro_after_init x86_amd_ls_cfg_base;
 -u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
 -
 -/* Control conditional STIBP in switch_to() */
 -DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
 -/* Control conditional IBPB in switch_mm() */
 -DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
 -/* Control unconditional IBPB in switch_mm() */
 -DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
 -
  /* Control MDS CPU buffer clear before returning to user space */
 -DEFINE_STATIC_KEY_FALSE(mds_user_clear);
 +struct static_key mds_user_clear = STATIC_KEY_INIT_FALSE;
  EXPORT_SYMBOL_GPL(mds_user_clear);
  /* Control MDS CPU buffer clear before idling (halt, mwait) */
 -DEFINE_STATIC_KEY_FALSE(mds_idle_clear);
 +struct static_key mds_idle_clear = STATIC_KEY_INIT_FALSE;
  EXPORT_SYMBOL_GPL(mds_idle_clear);
  
  void __init check_bugs(void)
@@@ -64,22 -89,24 +68,23 @@@
  	}
  
  	/*
 -	 * Read the SPEC_CTRL MSR to account for reserved bits which may
 -	 * have unknown values. AMD64_LS_CFG MSR is cached in the early AMD
 -	 * init code as it is not enumerated and depends on the family.
 +	 * Select proper mitigation for any exposure to the Speculative Store
 +	 * Bypass vulnerability (exposed as a bug in "Memory Disambiguation")
 +	 * This has to be done before spec_ctrl_init() to make sure that its
 +	 * SPEC_CTRL MSR value is properly set up.
  	 */
 -	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 -		rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -
 -	/* Allow STIBP in MSR_SPEC_CTRL if supported */
 -	if (boot_cpu_has(X86_FEATURE_STIBP))
 -		x86_spec_ctrl_mask |= SPEC_CTRL_STIBP;
 +	ssb_parse_cmdline();
 +	ssb_select_mitigation();
  
 -	/* Select the proper CPU mitigations before patching alternatives: */
 -	spectre_v1_select_mitigation();
 +	spec_ctrl_init();
  	spectre_v2_select_mitigation();
 -	ssb_select_mitigation();
 +
 +	spec_ctrl_cpu_init();
 +
  	l1tf_select_mitigation();
 +
  	mds_select_mitigation();
+ 	taa_select_mitigation();
  
  	arch_smt_update();
  
@@@ -216,9 -271,220 +221,198 @@@ static int __init mds_cmdline(char *str
  early_param("mds", mds_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"TAA: " fmt
+ 
+ /* Default mitigation for TAA-affected CPUs */
+ static enum taa_mitigations taa_mitigation __ro_after_init = TAA_MITIGATION_VERW;
+ static bool taa_nosmt __ro_after_init;
+ 
+ static const char * const taa_strings[] = {
+ 	[TAA_MITIGATION_OFF]		= "Vulnerable",
+ 	[TAA_MITIGATION_UCODE_NEEDED]	= "Vulnerable: Clear CPU buffers attempted, no microcode",
+ 	[TAA_MITIGATION_VERW]		= "Mitigation: Clear CPU buffers",
+ 	[TAA_MITIGATION_TSX_DISABLED]	= "Mitigation: TSX disabled",
+ };
+ 
+ static void __init taa_select_mitigation(void)
+ {
+ 	u64 ia32_cap;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_TAA)) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 		return;
+ 	}
+ 
+ 	/* TSX previously disabled by tsx=off */
+ 	if (!boot_cpu_has(X86_FEATURE_RTM)) {
+ 		taa_mitigation = TAA_MITIGATION_TSX_DISABLED;
+ 		goto out;
+ 	}
+ 
+ 	if (cpu_mitigations_off()) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 		return;
+ 	}
+ 
+ 	/* TAA mitigation is turned off on the cmdline (tsx_async_abort=off) */
+ 	if (taa_mitigation == TAA_MITIGATION_OFF)
+ 		goto out;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_MD_CLEAR))
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 	else
+ 		taa_mitigation = TAA_MITIGATION_UCODE_NEEDED;
+ 
+ 	/*
+ 	 * VERW doesn't clear the CPU buffers when MD_CLEAR=1 and MDS_NO=1.
+ 	 * A microcode update fixes this behavior to clear CPU buffers. It also
+ 	 * adds support for MSR_IA32_TSX_CTRL which is enumerated by the
+ 	 * ARCH_CAP_TSX_CTRL_MSR bit.
+ 	 *
+ 	 * On MDS_NO=1 CPUs if ARCH_CAP_TSX_CTRL_MSR is not set, microcode
+ 	 * update is required.
+ 	 */
+ 	ia32_cap = x86_read_arch_cap_msr();
+ 	if ( (ia32_cap & ARCH_CAP_MDS_NO) &&
+ 	    !(ia32_cap & ARCH_CAP_TSX_CTRL_MSR))
+ 		taa_mitigation = TAA_MITIGATION_UCODE_NEEDED;
+ 
+ 	/*
+ 	 * TSX is enabled, select alternate mitigation for TAA which is
+ 	 * the same as MDS. Enable MDS static branch to clear CPU buffers.
+ 	 *
+ 	 * For guests that can't determine whether the correct microcode is
+ 	 * present on host, enable the mitigation for UCODE_NEEDED as well.
+ 	 */
+ 	static_branch_enable(&mds_user_clear);
+ 
+ 	if (taa_nosmt || cpu_mitigations_auto_nosmt())
+ 		cpu_smt_disable(false);
+ 
+ out:
+ 	pr_info("%s\n", taa_strings[taa_mitigation]);
+ }
+ 
+ static int __init tsx_async_abort_parse_cmdline(char *str)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_TAA))
+ 		return 0;
+ 
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off")) {
+ 		taa_mitigation = TAA_MITIGATION_OFF;
+ 	} else if (!strcmp(str, "full")) {
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 	} else if (!strcmp(str, "full,nosmt")) {
+ 		taa_mitigation = TAA_MITIGATION_VERW;
+ 		taa_nosmt = true;
+ 	}
+ 
+ 	return 0;
+ }
+ early_param("tsx_async_abort", tsx_async_abort_parse_cmdline);
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Spectre V1 : " fmt
+ 
+ enum spectre_v1_mitigation {
+ 	SPECTRE_V1_MITIGATION_NONE,
+ 	SPECTRE_V1_MITIGATION_AUTO,
+ };
+ 
+ static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
+ 	SPECTRE_V1_MITIGATION_AUTO;
+ 
+ static const char * const spectre_v1_strings[] = {
+ 	[SPECTRE_V1_MITIGATION_NONE] = "Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers",
+ 	[SPECTRE_V1_MITIGATION_AUTO] = "Mitigation: usercopy/swapgs barriers and __user pointer sanitization",
+ };
+ 
+ /*
+  * Does SMAP provide full mitigation against speculative kernel access to
+  * userspace?
+  */
+ static bool smap_works_speculatively(void)
+ {
+ 	if (!boot_cpu_has(X86_FEATURE_SMAP))
+ 		return false;
+ 
+ 	/*
+ 	 * On CPUs which are vulnerable to Meltdown, SMAP does not
+ 	 * prevent speculative access to user data in the L1 cache.
+ 	 * Consider SMAP to be non-functional as a mitigation on these
+ 	 * CPUs.
+ 	 */
+ 	if (boot_cpu_has(X86_BUG_CPU_MELTDOWN))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static void __init spectre_v1_select_mitigation(void)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+ 		spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ 		return;
+ 	}
+ 
+ 	if (spectre_v1_mitigation == SPECTRE_V1_MITIGATION_AUTO) {
+ 		/*
+ 		 * With Spectre v1, a user can speculatively control either
+ 		 * path of a conditional swapgs with a user-controlled GS
+ 		 * value.  The mitigation is to add lfences to both code paths.
+ 		 *
+ 		 * If FSGSBASE is enabled, the user can put a kernel address in
+ 		 * GS, in which case SMAP provides no protection.
+ 		 *
+ 		 * [ NOTE: Don't check for X86_FEATURE_FSGSBASE until the
+ 		 *	   FSGSBASE enablement patches have been merged. ]
+ 		 *
+ 		 * If FSGSBASE is disabled, the user can only put a user space
+ 		 * address in GS.  That makes an attack harder, but still
+ 		 * possible if there's no SMAP protection.
+ 		 */
+ 		if (!smap_works_speculatively()) {
+ 			/*
+ 			 * Mitigation can be provided from SWAPGS itself or
+ 			 * PTI as the CR3 write in the Meltdown mitigation
+ 			 * is serializing.
+ 			 *
+ 			 * If neither is there, mitigate with an LFENCE to
+ 			 * stop speculation through swapgs.
+ 			 */
+ 			if (boot_cpu_has_bug(X86_BUG_SWAPGS) &&
+ 			    !boot_cpu_has(X86_FEATURE_PTI))
+ 				setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_USER);
+ 
+ 			/*
+ 			 * Enable lfences in the kernel entry (non-swapgs)
+ 			 * paths, to prevent user entry from speculatively
+ 			 * skipping swapgs.
+ 			 */
+ 			setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_KERNEL);
+ 		}
+ 	}
+ 
+ 	pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ }
+ 
+ static int __init nospectre_v1_cmdline(char *str)
+ {
+ 	spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ 	return 0;
+ }
+ early_param("nospectre_v1", nospectre_v1_cmdline);
+ 
+ #undef pr_fmt
++>>>>>>> 1b42f017415b (x86/speculation/taa: Add mitigation for TSX Async Abort)
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
 -static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
 -	SPECTRE_V2_NONE;
 -
 -static enum spectre_v2_user_mitigation spectre_v2_user __ro_after_init =
 -	SPECTRE_V2_USER_NONE;
 -
 -#ifdef CONFIG_RETPOLINE
 -static bool spectre_v2_bad_module;
 -
 -bool retpoline_module_ok(bool has_retpoline)
 -{
 -	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
 -		return true;
 -
 -	pr_err("System may be vulnerable to spectre v2\n");
 -	spectre_v2_bad_module = true;
 -	return false;
 -}
 -
 -static inline const char *spectre_v2_module_string(void)
 -{
 -	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
 -}
 -#else
 -static inline const char *spectre_v2_module_string(void) { return ""; }
 -#endif
 +enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;
  
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
@@@ -409,12 -882,12 +603,13 @@@ static void update_mds_branch_idle(void
  }
  
  #define MDS_MSG_SMT "MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.\n"
+ #define TAA_MSG_SMT "TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.\n"
  
 -void cpu_bugs_smt_update(void)
 +void arch_smt_update(void)
  {
 -	/* Enhanced IBRS implies STIBP. No update required. */
 -	if (spectre_v2_enabled == SPECTRE_V2_IBRS_ENHANCED)
 +	u64 mask;
 +
 +	if (!stibp_needed())
  		return;
  
  	mutex_lock(&spec_ctrl_mutex);
diff --cc arch/x86/kernel/cpu/common.c
index 0591d15edb0a,f8b8afc8f5b5..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -988,6 -1125,24 +988,27 @@@ static void __init cpu_set_bug_bits(str
  			setup_force_cpu_bug(X86_BUG_MSBDS_ONLY);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!cpu_matches(NO_SWAPGS))
+ 		setup_force_cpu_bug(X86_BUG_SWAPGS);
+ 
+ 	/*
+ 	 * When the CPU is not mitigated for TAA (TAA_NO=0) set TAA bug when:
+ 	 *	- TSX is supported or
+ 	 *	- TSX_CTRL is present
+ 	 *
+ 	 * TSX_CTRL check is needed for cases when TSX could be disabled before
+ 	 * the kernel boot e.g. kexec.
+ 	 * TSX_CTRL check alone is not sufficient for cases when the microcode
+ 	 * update is not present or running as guest that don't get TSX_CTRL.
+ 	 */
+ 	if (!(ia32_cap & ARCH_CAP_TAA_NO) &&
+ 	    (cpu_has(c, X86_FEATURE_RTM) ||
+ 	     (ia32_cap & ARCH_CAP_TSX_CTRL_MSR)))
+ 		setup_force_cpu_bug(X86_BUG_TAA);
+ 
++>>>>>>> 1b42f017415b (x86/speculation/taa: Add mitigation for TSX Async Abort)
  	if (cpu_matches(NO_MELTDOWN))
  		return;
  
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 199886d2c459..d6e842bbbfe3 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -81,6 +81,10 @@
 						  * Sampling (MDS) vulnerabilities.
 						  */
 #define ARCH_CAP_TSX_CTRL_MSR		BIT(7)	/* MSR for TSX control is available. */
+#define ARCH_CAP_TAA_NO			BIT(8)	/*
+						 * Not susceptible to
+						 * TSX Async Abort (TAA) vulnerabilities.
+						 */
 
 #define MSR_IA32_FLUSH_CMD		0x0000010b
 #define L1D_FLUSH			BIT(0)	/*
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index 36fcdc464e86..331b1be00077 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -254,7 +254,7 @@ extern struct static_key mds_idle_clear;
 #include <asm/segment.h>
 
 /**
- * mds_clear_cpu_buffers - Mitigation for MDS vulnerability
+ * mds_clear_cpu_buffers - Mitigation for MDS and TAA vulnerability
  *
  * This uses the otherwise unused and obsolete VERW instruction in
  * combination with microcode which triggers a CPU buffer flush when the
@@ -277,7 +277,7 @@ static inline void mds_clear_cpu_buffers(void)
 }
 
 /**
- * mds_user_clear_cpu_buffers - Mitigation for MDS vulnerability
+ * mds_user_clear_cpu_buffers - Mitigation for MDS and TAA vulnerability
  *
  * Clear CPU buffers if the corresponding static key is enabled
  *
* Unmerged path arch/x86/include/asm/processor.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
