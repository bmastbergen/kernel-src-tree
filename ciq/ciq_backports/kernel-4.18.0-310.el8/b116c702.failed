RDMA/umem: Use ib_dma_max_seg_size instead of dma_get_max_seg_size

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Christoph Hellwig <hch@lst.de>
commit b116c702791a9834e6485f67ca6267d9fdf59b87
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/b116c702.failed

RDMA ULPs must not call DMA mapping APIs directly but instead use the
ib_dma_* wrappers.

Fixes: 0c16d9635e3a ("RDMA/umem: Move to allocate SG table from pages")
Link: https://lore.kernel.org/r/20201106181941.1878556-3-hch@lst.de
	Reported-by: Jason Gunthorpe <jgg@nvidia.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit b116c702791a9834e6485f67ca6267d9fdf59b87)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
diff --cc drivers/infiniband/core/umem.c
index a6826e4a7de2,7ca4112e3e8f..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -296,15 -228,19 +296,29 @@@ struct ib_umem *ib_umem_get(struct ib_u
  			goto umem_release;
  
  		cur_base += ret * PAGE_SIZE;
++<<<<<<< HEAD
 +		npages   -= ret;
 +
 +		sg = ib_umem_add_sg_table(sg, page_list, ret,
 +			dma_get_max_seg_size(context->device->dma_device),
 +			&umem->sg_nents);
++=======
+ 		npages -= ret;
+ 		sg = __sg_alloc_table_from_pages(&umem->sg_head, page_list, ret,
+ 				0, ret << PAGE_SHIFT,
+ 				ib_dma_max_seg_size(device), sg, npages,
+ 				GFP_KERNEL);
+ 		umem->sg_nents = umem->sg_head.nents;
+ 		if (IS_ERR(sg)) {
+ 			unpin_user_pages_dirty_lock(page_list, ret, 0);
+ 			ret = PTR_ERR(sg);
+ 			goto umem_release;
+ 		}
++>>>>>>> b116c702791a (RDMA/umem: Use ib_dma_max_seg_size instead of dma_get_max_seg_size)
  	}
  
 +	sg_mark_end(sg);
 +
  	if (access & IB_ACCESS_RELAXED_ORDERING)
  		dma_attr |= DMA_ATTR_WEAK_ORDERING;
  
* Unmerged path drivers/infiniband/core/umem.c
