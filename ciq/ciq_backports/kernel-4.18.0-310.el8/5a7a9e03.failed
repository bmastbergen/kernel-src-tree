RDMA/core: remove use of dma_virt_ops

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 5a7a9e038b032137ae9c45d5429f18a2ffdf7d42
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/5a7a9e03.failed

Use the ib_dma_* helpers to skip the DMA translation instead.  This
removes the last user if dma_virt_ops and keeps the weird layering
violation inside the RDMA core instead of burderning the DMA mapping
subsystems with it.  This also means the software RDMA drivers now don't
have to mess with DMA parameters that are not relevant to them at all, and
that in the future we can use PCI P2P transfers even for software RDMA, as
there is no first fake layer of DMA mapping that the P2P DMA support.

Link: https://lore.kernel.org/r/20201106181941.1878556-8-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Mike Marciniszyn <mike.marciniszyn@cornelisnetworks.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 5a7a9e038b032137ae9c45d5429f18a2ffdf7d42)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
#	drivers/infiniband/sw/rdmavt/Kconfig
#	drivers/infiniband/sw/rdmavt/vt.c
#	drivers/infiniband/sw/rxe/Kconfig
#	drivers/infiniband/sw/rxe/rxe_verbs.c
#	drivers/infiniband/sw/siw/siw_main.c
diff --cc drivers/infiniband/core/device.c
index 30a7bf1c3b41,3ab1edea6acb..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -1182,56 -1209,6 +1182,59 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void setup_dma_device(struct ib_device *device)
 +{
 +	struct device *parent = device->dev.parent;
 +
 +	WARN_ON_ONCE(device->dma_device);
 +	if (device->dev.dma_ops) {
 +		/*
 +		 * The caller provided custom DMA operations. Copy the
 +		 * DMA-related fields that are used by e.g. dma_alloc_coherent()
 +		 * into device->dev.
 +		 */
 +		device->dma_device = &device->dev;
 +		if (!device->dev.dma_mask) {
 +			if (parent)
 +				device->dev.dma_mask = parent->dma_mask;
 +			else
 +				WARN_ON_ONCE(true);
 +		}
 +		if (!device->dev.coherent_dma_mask) {
 +			if (parent)
 +				device->dev.coherent_dma_mask =
 +					parent->coherent_dma_mask;
 +			else
 +				WARN_ON_ONCE(true);
 +		}
 +	} else {
 +		/*
 +		 * The caller did not provide custom DMA operations. Use the
 +		 * DMA mapping operations of the parent device.
 +		 */
 +		WARN_ON_ONCE(!parent);
 +		device->dma_device = parent;
 +	}
 +
 +	if (!device->dev.dma_parms) {
 +		if (parent) {
 +			/*
 +			 * The caller did not provide DMA parameters, so
 +			 * 'parent' probably represents a PCI device. The PCI
 +			 * core sets the maximum segment size to 64
 +			 * KB. Increase this parameter to 2 GB.
 +			 */
 +			device->dev.dma_parms = parent->dma_parms;
 +			dma_set_max_seg_size(device->dma_device, SZ_2G);
 +		} else {
 +			WARN_ON_ONCE(true);
 +		}
 +	}
 +}
 +
++=======
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  /*
   * setup_device() allocates memory and sets up data that requires calling the
   * device ops, this is the only reason these actions are not done during
@@@ -1372,6 -1354,14 +1375,17 @@@ int ib_register_device(struct ib_devic
  	if (ret)
  		return ret;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * If the caller does not provide a DMA capable device then the IB core
+ 	 * will set up ib_sge and scatterlist structures that stash the kernel
+ 	 * virtual address into the address field.
+ 	 */
+ 	WARN_ON(dma_device && !dma_device->dma_parms);
+ 	device->dma_device = dma_device;
+ 
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  	ret = setup_device(device);
  	if (ret)
  		return ret;
diff --cc drivers/infiniband/sw/rdmavt/Kconfig
index a297f13eb666,0df48b3a6b56..000000000000
--- a/drivers/infiniband/sw/rdmavt/Kconfig
+++ b/drivers/infiniband/sw/rdmavt/Kconfig
@@@ -4,6 -4,5 +4,10 @@@ config INFINIBAND_RDMAV
  	depends on INFINIBAND_VIRT_DMA
  	depends on X86_64
  	depends on PCI
++<<<<<<< HEAD
 +	select DMA_VIRT_OPS
 +	---help---
++=======
+ 	help
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  	This is a common software verbs provider for RDMA networks.
diff --cc drivers/infiniband/sw/rdmavt/vt.c
index 2d534c450f3c,49cec85a372a..000000000000
--- a/drivers/infiniband/sw/rdmavt/vt.c
+++ b/drivers/infiniband/sw/rdmavt/vt.c
@@@ -578,9 -579,6 +578,12 @@@ int rvt_register_device(struct rvt_dev_
  	/* Completion queues */
  	spin_lock_init(&rdi->n_cqs_lock);
  
++<<<<<<< HEAD
 +	/* DMA Operations */
 +	rdi->ibdev.dev.dma_ops = rdi->ibdev.dev.dma_ops ? : &dma_virt_ops;
 +
++=======
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  	/* Protection Domain */
  	spin_lock_init(&rdi->n_pds_lock);
  	rdi->n_pds_allocated = 0;
diff --cc drivers/infiniband/sw/rxe/Kconfig
index 71a773f607bb,452149066792..000000000000
--- a/drivers/infiniband/sw/rxe/Kconfig
+++ b/drivers/infiniband/sw/rxe/Kconfig
@@@ -5,8 -5,7 +5,12 @@@ config RDMA_RX
  	depends on INFINIBAND_VIRT_DMA
  	select NET_UDP_TUNNEL
  	select CRYPTO_CRC32
++<<<<<<< HEAD
 +	select DMA_VIRT_OPS
 +	---help---
++=======
+ 	help
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  	This driver implements the InfiniBand RDMA transport over
  	the Linux network stack. It enables a system with a
  	standard Ethernet adapter to interoperate with a RoCE
diff --cc drivers/infiniband/sw/rxe/rxe_verbs.c
index 6c3189603f41,a2bd91aaa5de..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@@ -1115,45 -1138,9 +1115,48 @@@ int rxe_register_device(struct rxe_dev 
  	dev->local_dma_lkey = 0;
  	addrconf_addr_eui48((unsigned char *)&dev->node_guid,
  			    rxe->ndev->dev_addr);
 -
 -	dev->uverbs_cmd_mask |= BIT_ULL(IB_USER_VERBS_CMD_POST_SEND) |
 -				BIT_ULL(IB_USER_VERBS_CMD_REQ_NOTIFY_CQ);
++<<<<<<< HEAD
 +	dev->dev.dma_ops = &dma_virt_ops;
 +	dev->dev.dma_parms = &rxe->dma_parms;
 +	rxe->dma_parms = (struct device_dma_parameters)
 +		{ .max_segment_size = SZ_2G };
 +	dma_coerce_mask_and_coherent(&dev->dev,
 +				     dma_get_required_mask(&dev->dev));
++=======
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
 +
 +	dev->uverbs_cmd_mask = BIT_ULL(IB_USER_VERBS_CMD_GET_CONTEXT)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_QUERY_DEVICE)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_QUERY_PORT)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_ALLOC_PD)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DEALLOC_PD)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_CREATE_SRQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_MODIFY_SRQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_QUERY_SRQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DESTROY_SRQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_POST_SRQ_RECV)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_CREATE_QP)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_MODIFY_QP)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_QUERY_QP)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DESTROY_QP)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_POST_SEND)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_POST_RECV)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_CREATE_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_RESIZE_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DESTROY_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_POLL_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_PEEK_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_REQ_NOTIFY_CQ)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_REG_MR)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DEREG_MR)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_CREATE_AH)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_MODIFY_AH)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_QUERY_AH)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DESTROY_AH)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_ATTACH_MCAST)
 +	    | BIT_ULL(IB_USER_VERBS_CMD_DETACH_MCAST)
 +	    ;
  
  	ib_set_device_ops(dev, &rxe_dev_ops);
  	err = ib_device_set_netdev(&rxe->ib_dev, rxe->ndev, 1);
diff --cc drivers/infiniband/sw/siw/siw_main.c
index 3b707def889f,d9de062852c4..000000000000
--- a/drivers/infiniband/sw/siw/siw_main.c
+++ b/drivers/infiniband/sw/siw/siw_main.c
@@@ -382,10 -360,6 +382,13 @@@ static struct siw_device *siw_device_cr
  	 */
  	base_dev->phys_port_cnt = 1;
  	base_dev->dev.parent = parent;
++<<<<<<< HEAD
 +	base_dev->dev.dma_ops = &dma_virt_ops;
 +	base_dev->dev.dma_parms = &sdev->dma_parms;
 +	sdev->dma_parms = (struct device_dma_parameters)
 +		{ .max_segment_size = SZ_2G };
++=======
++>>>>>>> 5a7a9e038b03 (RDMA/core: remove use of dma_virt_ops)
  	base_dev->num_comp_vectors = num_possible_cpus();
  
  	xa_init_flags(&sdev->qp_xa, XA_FLAGS_ALLOC1);
* Unmerged path drivers/infiniband/core/device.c
diff --git a/drivers/infiniband/core/rw.c b/drivers/infiniband/core/rw.c
index 614cff89fc71..3d9c11f670ef 100644
--- a/drivers/infiniband/core/rw.c
+++ b/drivers/infiniband/core/rw.c
@@ -285,8 +285,11 @@ static void rdma_rw_unmap_sg(struct ib_device *dev, struct scatterlist *sg,
 static int rdma_rw_map_sg(struct ib_device *dev, struct scatterlist *sg,
 			  u32 sg_cnt, enum dma_data_direction dir)
 {
-	if (is_pci_p2pdma_page(sg_page(sg)))
+	if (is_pci_p2pdma_page(sg_page(sg))) {
+		if (WARN_ON_ONCE(ib_uses_virt_dma(dev)))
+			return 0;
 		return pci_p2pdma_map_sg(dev->dma_device, sg, sg_cnt, dir);
+	}
 	return ib_dma_map_sg(dev, sg, sg_cnt, dir);
 }
 
* Unmerged path drivers/infiniband/sw/rdmavt/Kconfig
diff --git a/drivers/infiniband/sw/rdmavt/mr.c b/drivers/infiniband/sw/rdmavt/mr.c
index ca5e8d348a31..82e0fa920fba 100644
--- a/drivers/infiniband/sw/rdmavt/mr.c
+++ b/drivers/infiniband/sw/rdmavt/mr.c
@@ -324,8 +324,6 @@ static void __rvt_free_mr(struct rvt_mr *mr)
  * @acc: access flags
  *
  * Return: the memory region on success, otherwise returns an errno.
- * Note that all DMA addresses should be created via the functions in
- * struct dma_virt_ops.
  */
 struct ib_mr *rvt_get_dma_mr(struct ib_pd *pd, int acc)
 {
@@ -766,7 +764,7 @@ int rvt_lkey_ok(struct rvt_lkey_table *rkt, struct rvt_pd *pd,
 
 	/*
 	 * We use LKEY == zero for kernel virtual addresses
-	 * (see rvt_get_dma_mr() and dma_virt_ops).
+	 * (see rvt_get_dma_mr()).
 	 */
 	if (sge->lkey == 0) {
 		struct rvt_dev_info *dev = ib_to_rvt(pd->ibpd.device);
@@ -877,7 +875,7 @@ int rvt_rkey_ok(struct rvt_qp *qp, struct rvt_sge *sge,
 
 	/*
 	 * We use RKEY == zero for kernel virtual addresses
-	 * (see rvt_get_dma_mr() and dma_virt_ops).
+	 * (see rvt_get_dma_mr()).
 	 */
 	rcu_read_lock();
 	if (rkey == 0) {
* Unmerged path drivers/infiniband/sw/rdmavt/vt.c
* Unmerged path drivers/infiniband/sw/rxe/Kconfig
* Unmerged path drivers/infiniband/sw/rxe/rxe_verbs.c
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 57967fc39c04..79e0a5a878da 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -351,7 +351,6 @@ struct rxe_port {
 struct rxe_dev {
 	struct ib_device	ib_dev;
 	struct ib_device_attr	attr;
-	struct device_dma_parameters dma_parms;
 	int			max_ucontext;
 	int			max_inline_data;
 	struct mutex	usdev_lock;
diff --git a/drivers/infiniband/sw/siw/Kconfig b/drivers/infiniband/sw/siw/Kconfig
index 3450ba5081df..1b5105cbabae 100644
--- a/drivers/infiniband/sw/siw/Kconfig
+++ b/drivers/infiniband/sw/siw/Kconfig
@@ -2,7 +2,6 @@ config RDMA_SIW
 	tristate "Software RDMA over TCP/IP (iWARP) driver"
 	depends on INET && INFINIBAND && LIBCRC32C
 	depends on INFINIBAND_VIRT_DMA
-	select DMA_VIRT_OPS
 	help
 	This driver implements the iWARP RDMA transport over
 	the Linux TCP/IP network stack. It enables a system with a
diff --git a/drivers/infiniband/sw/siw/siw.h b/drivers/infiniband/sw/siw/siw.h
index 6f17392f975a..368959ae9a8c 100644
--- a/drivers/infiniband/sw/siw/siw.h
+++ b/drivers/infiniband/sw/siw/siw.h
@@ -69,7 +69,6 @@ struct siw_pd {
 
 struct siw_device {
 	struct ib_device base_dev;
-	struct device_dma_parameters dma_parms;
 	struct net_device *netdev;
 	struct siw_dev_cap attrs;
 
* Unmerged path drivers/infiniband/sw/siw/siw_main.c
diff --git a/drivers/nvme/target/rdma.c b/drivers/nvme/target/rdma.c
index 0082f05789ba..42fc2a515f27 100644
--- a/drivers/nvme/target/rdma.c
+++ b/drivers/nvme/target/rdma.c
@@ -422,7 +422,8 @@ static int nvmet_rdma_alloc_rsp(struct nvmet_rdma_device *ndev,
 	if (ib_dma_mapping_error(ndev->device, r->send_sge.addr))
 		goto out_free_rsp;
 
-	r->req.p2p_client = &ndev->device->dev;
+	if (!ib_uses_virt_dma(ndev->device))
+		r->req.p2p_client = &ndev->device->dev;
 	r->send_sge.length = sizeof(*r->req.cqe);
 	r->send_sge.lkey = ndev->pd->local_dma_lkey;
 
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4f2a3eeb85db..b4926a816192 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3958,6 +3958,16 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
 		-ENOSYS;
 }
 
+/*
+ * Drivers that don't need a DMA mapping at the RDMA layer, set dma_device to
+ * NULL. This causes the ib_dma* helpers to just stash the kernel virtual
+ * address into the dma address.
+ */
+static inline bool ib_uses_virt_dma(struct ib_device *dev)
+{
+	return IS_ENABLED(CONFIG_INFINIBAND_VIRT_DMA) && !dev->dma_device;
+}
+
 /**
  * ib_dma_mapping_error - check a DMA addr for error
  * @dev: The device for which the dma_addr was created
@@ -3965,6 +3975,8 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
  */
 static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
+	if (ib_uses_virt_dma(dev))
+		return 0;
 	return dma_mapping_error(dev->dma_device, dma_addr);
 }
 
@@ -3979,6 +3991,8 @@ static inline u64 ib_dma_map_single(struct ib_device *dev,
 				    void *cpu_addr, size_t size,
 				    enum dma_data_direction direction)
 {
+	if (ib_uses_virt_dma(dev))
+		return (uintptr_t)cpu_addr;
 	return dma_map_single(dev->dma_device, cpu_addr, size, direction);
 }
 
@@ -3993,7 +4007,8 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 				       u64 addr, size_t size,
 				       enum dma_data_direction direction)
 {
-	dma_unmap_single(dev->dma_device, addr, size, direction);
+	if (!ib_uses_virt_dma(dev))
+		dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
 /**
@@ -4010,6 +4025,8 @@ static inline u64 ib_dma_map_page(struct ib_device *dev,
 				  size_t size,
 					 enum dma_data_direction direction)
 {
+	if (ib_uses_virt_dma(dev))
+		return (uintptr_t)(page_address(page) + offset);
 	return dma_map_page(dev->dma_device, page, offset, size, direction);
 }
 
@@ -4024,7 +4041,30 @@ static inline void ib_dma_unmap_page(struct ib_device *dev,
 				     u64 addr, size_t size,
 				     enum dma_data_direction direction)
 {
-	dma_unmap_page(dev->dma_device, addr, size, direction);
+	if (!ib_uses_virt_dma(dev))
+		dma_unmap_page(dev->dma_device, addr, size, direction);
+}
+
+int ib_dma_virt_map_sg(struct ib_device *dev, struct scatterlist *sg, int nents);
+static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
+				      struct scatterlist *sg, int nents,
+				      enum dma_data_direction direction,
+				      unsigned long dma_attrs)
+{
+	if (ib_uses_virt_dma(dev))
+		return ib_dma_virt_map_sg(dev, sg, nents);
+	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
+				dma_attrs);
+}
+
+static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
+					 struct scatterlist *sg, int nents,
+					 enum dma_data_direction direction,
+					 unsigned long dma_attrs)
+{
+	if (!ib_uses_virt_dma(dev))
+		dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction,
+				   dma_attrs);
 }
 
 /**
@@ -4038,7 +4078,7 @@ static inline int ib_dma_map_sg(struct ib_device *dev,
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction direction)
 {
-	return dma_map_sg(dev->dma_device, sg, nents, direction);
+	return ib_dma_map_sg_attrs(dev, sg, nents, direction, 0);
 }
 
 /**
@@ -4052,24 +4092,7 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction direction)
 {
-	dma_unmap_sg(dev->dma_device, sg, nents, direction);
-}
-
-static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
-				      struct scatterlist *sg, int nents,
-				      enum dma_data_direction direction,
-				      unsigned long dma_attrs)
-{
-	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
-				dma_attrs);
-}
-
-static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
-					 struct scatterlist *sg, int nents,
-					 enum dma_data_direction direction,
-					 unsigned long dma_attrs)
-{
-	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
+	ib_dma_unmap_sg_attrs(dev, sg, nents, direction, 0);
 }
 
 /**
@@ -4080,6 +4103,8 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
  */
 static inline unsigned int ib_dma_max_seg_size(struct ib_device *dev)
 {
+	if (ib_uses_virt_dma(dev))
+		return UINT_MAX;
 	return dma_get_max_seg_size(dev->dma_device);
 }
 
@@ -4095,7 +4120,8 @@ static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
 					      size_t size,
 					      enum dma_data_direction dir)
 {
-	dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
+	if (!ib_uses_virt_dma(dev))
+		dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
 }
 
 /**
@@ -4110,7 +4136,8 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
 						 size_t size,
 						 enum dma_data_direction dir)
 {
-	dma_sync_single_for_device(dev->dma_device, addr, size, dir);
+	if (!ib_uses_virt_dma(dev))
+		dma_sync_single_for_device(dev->dma_device, addr, size, dir);
 }
 
 /* ib_reg_user_mr - register a memory region for virtual addresses from kernel
