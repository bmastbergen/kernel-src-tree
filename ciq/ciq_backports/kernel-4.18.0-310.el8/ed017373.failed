mm: use self-explanatory macros rather than "2"

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Yu Zhao <yuzhao@google.com>
commit ed0173733dd468883198c3136284394320b8fad6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/ed017373.failed

	Signed-off-by: Yu Zhao <yuzhao@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Alex Shi <alex.shi@linux.alibaba.com>
Link: http://lkml.kernel.org/r/20200831175042.3527153-2-yuzhao@google.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ed0173733dd468883198c3136284394320b8fad6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
diff --cc include/linux/mmzone.h
index ae118ed2cb9b,7e0ea3fe95ca..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -268,18 -266,7 +268,22 @@@ static inline bool is_active_lru(enum l
  	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
  }
  
++<<<<<<< HEAD
 +struct zone_reclaim_stat {
 +	/*
 +	 * The pageout code in vmscan.c keeps track of how many of the
 +	 * mem/swap backed and file backed pages are referenced.
 +	 * The higher the rotated/scanned ratio, the more valuable
 +	 * that cache is.
 +	 *
 +	 * The anon LRU stats live in [0], file LRU stats in [1]
 +	 */
 +	unsigned long		recent_rotated[2];
 +	unsigned long		recent_scanned[2];
 +};
++=======
+ #define ANON_AND_FILE 2
++>>>>>>> ed0173733dd4 (mm: use self-explanatory macros rather than "2")
  
  enum lruvec_flags {
  	LRUVEC_CONGESTED,		/* lruvec has many dirty pages
@@@ -294,19 -281,14 +298,25 @@@ struct lruvec 
  	 * over the other. As the observed cost of reclaiming one LRU
  	 * increases, the reclaim scan balance tips toward the other.
  	 */
 -	unsigned long			anon_cost;
 -	unsigned long			file_cost;
 +	RH_KABI_REPLACE_SPLIT(
 +		struct zone_reclaim_stat	reclaim_stat,
 +		unsigned long			anon_cost,
 +		unsigned long			file_cost,
 +		/* Refaults at the time of last reclaim cycle, anon=0, file=1 */
 +		unsigned long			refaults[2])
 +
  	/* Non-resident age, driven by LRU movement */
++<<<<<<< HEAD
 +	atomic_long_t			RH_KABI_RENAME(inactive_age,
 +						       nonresident_age);
++=======
+ 	atomic_long_t			nonresident_age;
+ 	/* Refaults at the time of last reclaim cycle */
+ 	unsigned long			refaults[ANON_AND_FILE];
++>>>>>>> ed0173733dd4 (mm: use self-explanatory macros rather than "2")
  	/* Various lruvec state flags (enum lruvec_flags) */
 -	unsigned long			flags;
 +	RH_KABI_REPLACE(unsigned long		refaults,
 +			unsigned long		flags)
  #ifdef CONFIG_MEMCG
  	struct pglist_data *pgdat;
  #endif
@@@ -541,8 -564,10 +553,15 @@@ struct zone 
  #if defined CONFIG_COMPACTION || defined CONFIG_CMA
  	/* pfn where compaction free scanner should start */
  	unsigned long		compact_cached_free_pfn;
++<<<<<<< HEAD
 +	/* pfn where async and sync compaction migration scanner should start */
 +	unsigned long		compact_cached_migrate_pfn[2];
++=======
+ 	/* pfn where compaction migration scanner should start */
+ 	unsigned long		compact_cached_migrate_pfn[ASYNC_AND_SYNC];
+ 	unsigned long		compact_init_migrate_pfn;
+ 	unsigned long		compact_init_free_pfn;
++>>>>>>> ed0173733dd4 (mm: use self-explanatory macros rather than "2")
  #endif
  
  #ifdef CONFIG_COMPACTION
* Unmerged path include/linux/mmzone.h
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 2f7cb92a9add..0bc0dc1f3352 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -28,7 +28,7 @@ struct reclaim_stat {
 	unsigned nr_writeback;
 	unsigned nr_immediate;
 	unsigned nr_pageout;
-	unsigned nr_activate[2];
+	unsigned nr_activate[ANON_AND_FILE];
 	unsigned nr_ref_keep;
 	unsigned nr_unmap_fail;
 	unsigned nr_lazyfree_fail;
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 86d55e8472f2..610772deed0a 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2244,7 +2244,7 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
 	unsigned long anon_cost, file_cost, total_cost;
 	int swappiness = mem_cgroup_swappiness(memcg);
-	u64 fraction[2];
+	u64 fraction[ANON_AND_FILE];
 	u64 denominator = 0;	/* gcc */
 	enum scan_balance scan_balance;
 	unsigned long ap, fp;
