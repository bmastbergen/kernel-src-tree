KVM: nSVM: Optimize vmcb12 to vmcb02 save area copies

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Cathy Avery <cavery@redhat.com>
commit 8173396e94c10dccde5e890f1bb31d11c05cae68
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/8173396e.failed

Use the vmcb12 control clean field to determine which vmcb12.save
registers were marked dirty in order to minimize register copies
when switching from L1 to L2. Those vmcb12 registers marked as dirty need
to be copied to L0's vmcb02 as they will be used to update the vmcb
state cache for the L2 VMRUN.  In the case where we have a different
vmcb12 from the last L2 VMRUN all vmcb12.save registers must be
copied over to L2.save.

Tested:
kvm-unit-tests
kvm selftests
Fedora L1 L2

	Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Cathy Avery <cavery@redhat.com>
Message-Id: <20210301200844.2000-1-cavery@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8173396e94c10dccde5e890f1bb31d11c05cae68)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
#	arch/x86/kvm/svm/svm.c
diff --cc arch/x86/kvm/svm/nested.c
index e8796fbe795c,8523f60adb92..000000000000
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@@ -396,19 -411,42 +396,45 @@@ static int nested_svm_load_cr3(struct k
  	return 0;
  }
  
 -void nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)
 +static void nested_prepare_vmcb_save(struct vcpu_svm *svm, struct vmcb *vmcb12)
  {
++<<<<<<< HEAD
++=======
+ 	if (!svm->nested.vmcb02.ptr)
+ 		return;
+ 
+ 	/* FIXME: merge g_pat from vmcb01 and vmcb12.  */
+ 	svm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;
+ }
+ 
+ static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)
+ {
+ 	bool new_vmcb12 = false;
+ 
+ 	nested_vmcb02_compute_g_pat(svm);
+ 
++>>>>>>> 8173396e94c1 (KVM: nSVM: Optimize vmcb12 to vmcb02 save area copies)
  	/* Load the nested guest state */
- 	svm->vmcb->save.es = vmcb12->save.es;
- 	svm->vmcb->save.cs = vmcb12->save.cs;
- 	svm->vmcb->save.ss = vmcb12->save.ss;
- 	svm->vmcb->save.ds = vmcb12->save.ds;
- 	svm->vmcb->save.cpl = vmcb12->save.cpl;
- 	vmcb_mark_dirty(svm->vmcb, VMCB_SEG);
  
- 	svm->vmcb->save.gdtr = vmcb12->save.gdtr;
- 	svm->vmcb->save.idtr = vmcb12->save.idtr;
- 	vmcb_mark_dirty(svm->vmcb, VMCB_DT);
+ 	if (svm->nested.vmcb12_gpa != svm->nested.last_vmcb12_gpa) {
+ 		new_vmcb12 = true;
+ 		svm->nested.last_vmcb12_gpa = svm->nested.vmcb12_gpa;
+ 	}
+ 
+ 	if (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_SEG))) {
+ 		svm->vmcb->save.es = vmcb12->save.es;
+ 		svm->vmcb->save.cs = vmcb12->save.cs;
+ 		svm->vmcb->save.ss = vmcb12->save.ss;
+ 		svm->vmcb->save.ds = vmcb12->save.ds;
+ 		svm->vmcb->save.cpl = vmcb12->save.cpl;
+ 		vmcb_mark_dirty(svm->vmcb, VMCB_SEG);
+ 	}
+ 
+ 	if (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DT))) {
+ 		svm->vmcb->save.gdtr = vmcb12->save.gdtr;
+ 		svm->vmcb->save.idtr = vmcb12->save.idtr;
+ 		vmcb_mark_dirty(svm->vmcb, VMCB_DT);
+ 	}
  
  	kvm_set_rflags(&svm->vcpu, vmcb12->save.rflags | X86_EFLAGS_FIXED);
  	svm_set_efer(&svm->vcpu, vmcb12->save.efer);
@@@ -426,12 -464,15 +452,15 @@@
  	svm->vmcb->save.rsp = vmcb12->save.rsp;
  	svm->vmcb->save.rip = vmcb12->save.rip;
  
- 	svm->vmcb->save.dr7 = vmcb12->save.dr7 | DR7_FIXED_1;
- 	svm->vcpu.arch.dr6  = vmcb12->save.dr6 | DR6_ACTIVE_LOW;
- 	vmcb_mark_dirty(svm->vmcb, VMCB_DR);
+ 	/* These bits will be set properly on the first execution when new_vmc12 is true */
+ 	if (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DR))) {
+ 		svm->vmcb->save.dr7 = vmcb12->save.dr7 | DR7_FIXED_1;
+ 		svm->vcpu.arch.dr6  = vmcb12->save.dr6 | DR6_ACTIVE_LOW;
+ 		vmcb_mark_dirty(svm->vmcb, VMCB_DR);
+ 	}
  }
  
 -static void nested_vmcb02_prepare_control(struct vcpu_svm *svm)
 +static void nested_prepare_vmcb_control(struct vcpu_svm *svm)
  {
  	const u32 mask = V_INTR_MASKING_MASK | V_GIF_ENABLE_MASK | V_GIF_MASK;
  
diff --cc arch/x86/kvm/svm/svm.c
index dd9e65c2714d,d5ee3bbcd098..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -1238,9 -1234,10 +1238,14 @@@ static void init_vmcb(struct vcpu_svm *
  	svm->asid = 0;
  
  	svm->nested.vmcb12_gpa = 0;
++<<<<<<< HEAD
 +	svm->vcpu.arch.hflags = 0;
++=======
+ 	svm->nested.last_vmcb12_gpa = 0;
+ 	vcpu->arch.hflags = 0;
++>>>>>>> 8173396e94c1 (KVM: nSVM: Optimize vmcb12 to vmcb02 save area copies)
  
 -	if (!kvm_pause_in_guest(vcpu->kvm)) {
 +	if (!kvm_pause_in_guest(svm->vcpu.kvm)) {
  		control->pause_filter_count = pause_filter_count;
  		if (pause_filter_thresh)
  			control->pause_filter_thresh = pause_filter_thresh;
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/svm.c
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index de600f536464..e7f0346fb89a 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -89,6 +89,7 @@ struct svm_nested_state {
 	u64 hsave_msr;
 	u64 vm_cr_msr;
 	u64 vmcb12_gpa;
+	u64 last_vmcb12_gpa;
 
 	/* These are the merged vectors */
 	u32 *msrpm;
@@ -212,6 +213,11 @@ static inline void vmcb_mark_dirty(struct vmcb *vmcb, int bit)
 	vmcb->control.clean &= ~(1 << bit);
 }
 
+static inline bool vmcb_is_dirty(struct vmcb *vmcb, int bit)
+{
+        return !test_bit(bit, (unsigned long *)&vmcb->control.clean);
+}
+
 static inline struct vcpu_svm *to_svm(struct kvm_vcpu *vcpu)
 {
 	return container_of(vcpu, struct vcpu_svm, vcpu);
