block: don't allocate inline bvecs if this bioset needn't bvecs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Ming Lei <ming.lei@redhat.com>
commit 9f180e315a93cde559ac1c9c4c5ce980aa681c1c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/9f180e31.failed

The inline bvecs won't be used if user needn't bvecs by not passing
BIOSET_NEED_BVECS, so don't allocate bvecs in this situation.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
	Tested-by: Pavel Begunkov <asml.silence@gmail.com>
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 9f180e315a93cde559ac1c9c4c5ce980aa681c1c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/bio.c
diff --cc block/bio.c
index 528aaf459c37,8ccda51dd831..000000000000
--- a/block/bio.c
+++ b/block/bio.c
@@@ -70,63 -59,55 +70,99 @@@ struct bio_slab 
  	char name[8];
  };
  static DEFINE_MUTEX(bio_slab_lock);
 -static DEFINE_XARRAY(bio_slabs);
 +static struct bio_slab *bio_slabs;
 +static unsigned int bio_slab_nr, bio_slab_max;
  
 -static struct bio_slab *create_bio_slab(unsigned int size)
 +static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)
  {
++<<<<<<< HEAD
 +	unsigned int sz = sizeof(struct bio) + extra_size;
 +	struct kmem_cache *slab = NULL;
 +	struct bio_slab *bslab, *new_bio_slabs;
 +	unsigned int new_bio_slab_max;
 +	unsigned int i, entry = -1;
++=======
+ 	struct bio_slab *bslab = kzalloc(sizeof(*bslab), GFP_KERNEL);
+ 
+ 	if (!bslab)
+ 		return NULL;
+ 
+ 	snprintf(bslab->name, sizeof(bslab->name), "bio-%d", size);
+ 	bslab->slab = kmem_cache_create(bslab->name, size,
+ 			ARCH_KMALLOC_MINALIGN, SLAB_HWCACHE_ALIGN, NULL);
+ 	if (!bslab->slab)
+ 		goto fail_alloc_slab;
+ 
+ 	bslab->slab_ref = 1;
+ 	bslab->slab_size = size;
+ 
+ 	if (!xa_err(xa_store(&bio_slabs, size, bslab, GFP_KERNEL)))
+ 		return bslab;
+ 
+ 	kmem_cache_destroy(bslab->slab);
+ 
+ fail_alloc_slab:
+ 	kfree(bslab);
+ 	return NULL;
+ }
+ 
+ static inline unsigned int bs_bio_slab_size(struct bio_set *bs)
+ {
+ 	return bs->front_pad + sizeof(struct bio) + bs->back_pad;
+ }
+ 
+ static struct kmem_cache *bio_find_or_create_slab(struct bio_set *bs)
+ {
+ 	unsigned int size = bs_bio_slab_size(bs);
+ 	struct bio_slab *bslab;
++>>>>>>> 9f180e315a93 (block: don't allocate inline bvecs if this bioset needn't bvecs)
  
  	mutex_lock(&bio_slab_lock);
 -	bslab = xa_load(&bio_slabs, size);
 -	if (bslab)
 -		bslab->slab_ref++;
 -	else
 -		bslab = create_bio_slab(size);
 -	mutex_unlock(&bio_slab_lock);
  
 -	if (bslab)
 -		return bslab->slab;
 -	return NULL;
 +	i = 0;
 +	while (i < bio_slab_nr) {
 +		bslab = &bio_slabs[i];
 +
 +		if (!bslab->slab && entry == -1)
 +			entry = i;
 +		else if (bslab->slab_size == sz) {
 +			slab = bslab->slab;
 +			bslab->slab_ref++;
 +			break;
 +		}
 +		i++;
 +	}
 +
 +	if (slab)
 +		goto out_unlock;
 +
 +	if (bio_slab_nr == bio_slab_max && entry == -1) {
 +		new_bio_slab_max = bio_slab_max << 1;
 +		new_bio_slabs = krealloc(bio_slabs,
 +					 new_bio_slab_max * sizeof(struct bio_slab),
 +					 GFP_KERNEL);
 +		if (!new_bio_slabs)
 +			goto out_unlock;
 +		bio_slab_max = new_bio_slab_max;
 +		bio_slabs = new_bio_slabs;
 +	}
 +	if (entry == -1)
 +		entry = bio_slab_nr++;
 +
 +	bslab = &bio_slabs[entry];
 +
 +	snprintf(bslab->name, sizeof(bslab->name), "bio-%d", entry);
 +	slab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,
 +				 SLAB_HWCACHE_ALIGN, NULL);
 +	if (!slab)
 +		goto out_unlock;
 +
 +	bslab->slab = slab;
 +	bslab->slab_ref = 1;
 +	bslab->slab_size = sz;
 +out_unlock:
 +	mutex_unlock(&bio_slab_lock);
 +	return slab;
  }
  
  static void bio_put_slab(struct bio_set *bs)
@@@ -2191,9 -1561,11 +2227,13 @@@ int bioset_init(struct bio_set *bs
  		unsigned int front_pad,
  		int flags)
  {
 +	unsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);
 +
  	bs->front_pad = front_pad;
+ 	if (flags & BIOSET_NEED_BVECS)
+ 		bs->back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);
+ 	else
+ 		bs->back_pad = 0;
  
  	spin_lock_init(&bs->rescue_lock);
  	bio_list_init(&bs->rescue_list);
* Unmerged path block/bio.c
diff --git a/include/linux/bio.h b/include/linux/bio.h
index e97215c76ee7..ac0ffd9a0959 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -695,6 +695,7 @@ struct bio_set {
 	mempool_t bvec_integrity_pool;
 #endif
 
+	unsigned int back_pad;
 	/*
 	 * Deadlock avoidance for stacking block drivers: see comments in
 	 * bio_alloc_bioset() for details
