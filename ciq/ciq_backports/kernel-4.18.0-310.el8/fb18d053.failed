selftest: kvm: x86: test KVM_GET_CPUID2 and guest visible CPUIDs against KVM_GET_SUPPORTED_CPUID

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit fb18d053b7f823e6a9acf62d1be5b986ca614253
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/fb18d053.failed

Commit 181f494888d5 ("KVM: x86: fix CPUID entries returned by
KVM_GET_CPUID2 ioctl") revealed that we're not testing KVM_GET_CPUID2
ioctl at all. Add a test for it and also check that from inside the guest
visible CPUIDs are equal to it's output.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20210129161821.74635-1-vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit fb18d053b7f823e6a9acf62d1be5b986ca614253)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/kvm/Makefile
#	tools/testing/selftests/kvm/include/x86_64/processor.h
#	tools/testing/selftests/kvm/lib/x86_64/processor.c
diff --cc tools/testing/selftests/kvm/Makefile
index 0f73faa651c0,9616f77cc5af..000000000000
--- a/tools/testing/selftests/kvm/Makefile
+++ b/tools/testing/selftests/kvm/Makefile
@@@ -40,7 -40,7 +40,11 @@@ LIBKVM_s390x = lib/s390x/processor.c li
  
  TEST_GEN_PROGS_x86_64 = x86_64/cr4_cpuid_sync_test
  TEST_GEN_PROGS_x86_64 += x86_64/evmcs_test
++<<<<<<< HEAD
 +TEST_GEN_PROGS_x86_64 += x86_64/hyperv_clock
++=======
+ TEST_GEN_PROGS_x86_64 += x86_64/get_cpuid_test
++>>>>>>> fb18d053b7f8 (selftest: kvm: x86: test KVM_GET_CPUID2 and guest visible CPUIDs against KVM_GET_SUPPORTED_CPUID)
  TEST_GEN_PROGS_x86_64 += x86_64/hyperv_cpuid
  TEST_GEN_PROGS_x86_64 += x86_64/kvm_pv_test
  TEST_GEN_PROGS_x86_64 += x86_64/mmio_warning_test
diff --cc tools/testing/selftests/kvm/include/x86_64/processor.h
index 74ce0fe420ab,582dfb23f30c..000000000000
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@@ -338,8 -351,10 +351,12 @@@ void vcpu_load_state(struct kvm_vm *vm
  		     struct kvm_x86_state *state);
  
  struct kvm_msr_list *kvm_get_msr_index_list(void);
 -uint64_t kvm_get_feature_msr(uint64_t msr_index);
 -struct kvm_cpuid2 *kvm_get_supported_cpuid(void);
  
++<<<<<<< HEAD
 +struct kvm_cpuid2 *kvm_get_supported_cpuid(void);
++=======
+ struct kvm_cpuid2 *vcpu_get_cpuid(struct kvm_vm *vm, uint32_t vcpuid);
++>>>>>>> fb18d053b7f8 (selftest: kvm: x86: test KVM_GET_CPUID2 and guest visible CPUIDs against KVM_GET_SUPPORTED_CPUID)
  void vcpu_set_cpuid(struct kvm_vm *vm, uint32_t vcpuid,
  		    struct kvm_cpuid2 *cpuid);
  
diff --cc tools/testing/selftests/kvm/lib/x86_64/processor.c
index 95e1a757c629,5a15264c2ef2..000000000000
--- a/tools/testing/selftests/kvm/lib/x86_64/processor.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/processor.c
@@@ -670,6 -670,82 +670,85 @@@ struct kvm_cpuid2 *kvm_get_supported_cp
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * KVM Get MSR
+  *
+  * Input Args:
+  *   msr_index - Index of MSR
+  *
+  * Output Args: None
+  *
+  * Return: On success, value of the MSR. On failure a TEST_ASSERT is produced.
+  *
+  * Get value of MSR for VCPU.
+  */
+ uint64_t kvm_get_feature_msr(uint64_t msr_index)
+ {
+ 	struct {
+ 		struct kvm_msrs header;
+ 		struct kvm_msr_entry entry;
+ 	} buffer = {};
+ 	int r, kvm_fd;
+ 
+ 	buffer.header.nmsrs = 1;
+ 	buffer.entry.index = msr_index;
+ 	kvm_fd = open(KVM_DEV_PATH, O_RDONLY);
+ 	if (kvm_fd < 0)
+ 		exit(KSFT_SKIP);
+ 
+ 	r = ioctl(kvm_fd, KVM_GET_MSRS, &buffer.header);
+ 	TEST_ASSERT(r == 1, "KVM_GET_MSRS IOCTL failed,\n"
+ 		"  rc: %i errno: %i", r, errno);
+ 
+ 	close(kvm_fd);
+ 	return buffer.entry.data;
+ }
+ 
+ /*
+  * VM VCPU CPUID Set
+  *
+  * Input Args:
+  *   vm - Virtual Machine
+  *   vcpuid - VCPU id
+  *
+  * Output Args: None
+  *
+  * Return: KVM CPUID (KVM_GET_CPUID2)
+  *
+  * Set the VCPU's CPUID.
+  */
+ struct kvm_cpuid2 *vcpu_get_cpuid(struct kvm_vm *vm, uint32_t vcpuid)
+ {
+ 	struct vcpu *vcpu = vcpu_find(vm, vcpuid);
+ 	struct kvm_cpuid2 *cpuid;
+ 	int rc, max_ent;
+ 
+ 	TEST_ASSERT(vcpu != NULL, "vcpu not found, vcpuid: %u", vcpuid);
+ 
+ 	cpuid = allocate_kvm_cpuid2();
+ 	max_ent = cpuid->nent;
+ 
+ 	for (cpuid->nent = 1; cpuid->nent <= max_ent; cpuid->nent++) {
+ 		rc = ioctl(vcpu->fd, KVM_GET_CPUID2, cpuid);
+ 		if (!rc)
+ 			break;
+ 
+ 		TEST_ASSERT(rc == -1 && errno == E2BIG,
+ 			    "KVM_GET_CPUID2 should either succeed or give E2BIG: %d %d",
+ 			    rc, errno);
+ 	}
+ 
+ 	TEST_ASSERT(rc == 0, "KVM_GET_CPUID2 failed, rc: %i errno: %i",
+ 		    rc, errno);
+ 
+ 	return cpuid;
+ }
+ 
+ 
+ 
+ /*
++>>>>>>> fb18d053b7f8 (selftest: kvm: x86: test KVM_GET_CPUID2 and guest visible CPUIDs against KVM_GET_SUPPORTED_CPUID)
   * Locate a cpuid entry.
   *
   * Input Args:
diff --git a/tools/testing/selftests/kvm/.gitignore b/tools/testing/selftests/kvm/.gitignore
index f8b179b23438..7f75fafcc199 100644
--- a/tools/testing/selftests/kvm/.gitignore
+++ b/tools/testing/selftests/kvm/.gitignore
@@ -1,6 +1,7 @@
 /x86_64/cr4_cpuid_sync_test
 /x86_64/debug_regs
 /x86_64/evmcs_test
+/x86_64/get_cpuid_test
 /x86_64/kvm_pv_test
 /x86_64/hyperv_cpuid
 /x86_64/platform_info_test
* Unmerged path tools/testing/selftests/kvm/Makefile
* Unmerged path tools/testing/selftests/kvm/include/x86_64/processor.h
* Unmerged path tools/testing/selftests/kvm/lib/x86_64/processor.c
diff --git a/tools/testing/selftests/kvm/x86_64/get_cpuid_test.c b/tools/testing/selftests/kvm/x86_64/get_cpuid_test.c
new file mode 100644
index 000000000000..9b78e8889638
--- /dev/null
+++ b/tools/testing/selftests/kvm/x86_64/get_cpuid_test.c
@@ -0,0 +1,175 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2021, Red Hat Inc.
+ *
+ * Generic tests for KVM CPUID set/get ioctls
+ */
+#include <asm/kvm_para.h>
+#include <linux/kvm_para.h>
+#include <stdint.h>
+
+#include "test_util.h"
+#include "kvm_util.h"
+#include "processor.h"
+
+#define VCPU_ID 0
+
+/* CPUIDs known to differ */
+struct {
+	u32 function;
+	u32 index;
+} mangled_cpuids[] = {
+	{.function = 0xd, .index = 0},
+};
+
+static void test_guest_cpuids(struct kvm_cpuid2 *guest_cpuid)
+{
+	int i;
+	u32 eax, ebx, ecx, edx;
+
+	for (i = 0; i < guest_cpuid->nent; i++) {
+		eax = guest_cpuid->entries[i].function;
+		ecx = guest_cpuid->entries[i].index;
+
+		cpuid(&eax, &ebx, &ecx, &edx);
+
+		GUEST_ASSERT(eax == guest_cpuid->entries[i].eax &&
+			     ebx == guest_cpuid->entries[i].ebx &&
+			     ecx == guest_cpuid->entries[i].ecx &&
+			     edx == guest_cpuid->entries[i].edx);
+	}
+
+}
+
+static void test_cpuid_40000000(struct kvm_cpuid2 *guest_cpuid)
+{
+	u32 eax = 0x40000000, ebx, ecx = 0, edx;
+
+	cpuid(&eax, &ebx, &ecx, &edx);
+
+	GUEST_ASSERT(eax == 0x40000001);
+}
+
+static void guest_main(struct kvm_cpuid2 *guest_cpuid)
+{
+	GUEST_SYNC(1);
+
+	test_guest_cpuids(guest_cpuid);
+
+	GUEST_SYNC(2);
+
+	test_cpuid_40000000(guest_cpuid);
+
+	GUEST_DONE();
+}
+
+static bool is_cpuid_mangled(struct kvm_cpuid_entry2 *entrie)
+{
+	int i;
+
+	for (i = 0; i < sizeof(mangled_cpuids); i++) {
+		if (mangled_cpuids[i].function == entrie->function &&
+		    mangled_cpuids[i].index == entrie->index)
+			return true;
+	}
+
+	return false;
+}
+
+static void check_cpuid(struct kvm_cpuid2 *cpuid, struct kvm_cpuid_entry2 *entrie)
+{
+	int i;
+
+	for (i = 0; i < cpuid->nent; i++) {
+		if (cpuid->entries[i].function == entrie->function &&
+		    cpuid->entries[i].index == entrie->index) {
+			if (is_cpuid_mangled(entrie))
+				return;
+
+			TEST_ASSERT(cpuid->entries[i].eax == entrie->eax &&
+				    cpuid->entries[i].ebx == entrie->ebx &&
+				    cpuid->entries[i].ecx == entrie->ecx &&
+				    cpuid->entries[i].edx == entrie->edx,
+				    "CPUID 0x%x.%x differ: 0x%x:0x%x:0x%x:0x%x vs 0x%x:0x%x:0x%x:0x%x",
+				    entrie->function, entrie->index,
+				    cpuid->entries[i].eax, cpuid->entries[i].ebx,
+				    cpuid->entries[i].ecx, cpuid->entries[i].edx,
+				    entrie->eax, entrie->ebx, entrie->ecx, entrie->edx);
+			return;
+		}
+	}
+
+	TEST_ASSERT(false, "CPUID 0x%x.%x not found", entrie->function, entrie->index);
+}
+
+static void compare_cpuids(struct kvm_cpuid2 *cpuid1, struct kvm_cpuid2 *cpuid2)
+{
+	int i;
+
+	for (i = 0; i < cpuid1->nent; i++)
+		check_cpuid(cpuid2, &cpuid1->entries[i]);
+
+	for (i = 0; i < cpuid2->nent; i++)
+		check_cpuid(cpuid1, &cpuid2->entries[i]);
+}
+
+static void run_vcpu(struct kvm_vm *vm, uint32_t vcpuid, int stage)
+{
+	struct ucall uc;
+
+	_vcpu_run(vm, vcpuid);
+
+	switch (get_ucall(vm, vcpuid, &uc)) {
+	case UCALL_SYNC:
+		TEST_ASSERT(!strcmp((const char *)uc.args[0], "hello") &&
+			    uc.args[1] == stage + 1,
+			    "Stage %d: Unexpected register values vmexit, got %lx",
+			    stage + 1, (ulong)uc.args[1]);
+		return;
+	case UCALL_DONE:
+		return;
+	case UCALL_ABORT:
+		TEST_ASSERT(false, "%s at %s:%ld\n\tvalues: %#lx, %#lx", (const char *)uc.args[0],
+			    __FILE__, uc.args[1], uc.args[2], uc.args[3]);
+	default:
+		TEST_ASSERT(false, "Unexpected exit: %s",
+			    exit_reason_str(vcpu_state(vm, vcpuid)->exit_reason));
+	}
+}
+
+struct kvm_cpuid2 *vcpu_alloc_cpuid(struct kvm_vm *vm, vm_vaddr_t *p_gva, struct kvm_cpuid2 *cpuid)
+{
+	int size = sizeof(*cpuid) + cpuid->nent * sizeof(cpuid->entries[0]);
+	vm_vaddr_t gva = vm_vaddr_alloc(vm, size,
+					getpagesize(), 0, 0);
+	struct kvm_cpuid2 *guest_cpuids = addr_gva2hva(vm, gva);
+
+	memcpy(guest_cpuids, cpuid, size);
+
+	*p_gva = gva;
+	return guest_cpuids;
+}
+
+int main(void)
+{
+	struct kvm_cpuid2 *supp_cpuid, *cpuid2;
+	vm_vaddr_t cpuid_gva;
+	struct kvm_vm *vm;
+	int stage;
+
+	vm = vm_create_default(VCPU_ID, 0, guest_main);
+
+	supp_cpuid = kvm_get_supported_cpuid();
+	cpuid2 = vcpu_get_cpuid(vm, VCPU_ID);
+
+	compare_cpuids(supp_cpuid, cpuid2);
+
+	vcpu_alloc_cpuid(vm, &cpuid_gva, cpuid2);
+
+	vcpu_args_set(vm, VCPU_ID, 1, cpuid_gva);
+
+	for (stage = 0; stage < 3; stage++)
+		run_vcpu(vm, VCPU_ID, stage);
+
+	kvm_vm_free(vm);
+}
