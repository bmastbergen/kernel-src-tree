mm/mempolicy: use a standard migration target allocation callback

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit a097631160c3b71a12db224ece7d72208c9b6846
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/a0976311.failed

There is a well-defined migration target allocation callback.  Use it.

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Roman Gushchin <guro@fb.com>
Link: http://lkml.kernel.org/r/1594622517-20681-7-git-send-email-iamjoonsoo.kim@lge.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a097631160c3b71a12db224ece7d72208c9b6846)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/internal.h
#	mm/mempolicy.c
diff --cc mm/internal.h
index 22b2813bf4c6,d11a9a8d2135..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -602,5 -613,11 +602,15 @@@ static inline bool is_migrate_highatomi
  }
  
  void setup_zone_pageset(struct zone *zone);
++<<<<<<< HEAD
 +extern struct page *alloc_new_node_page(struct page *page, unsigned long node);
++=======
+ 
+ struct migration_target_control {
+ 	int nid;		/* preferred node id */
+ 	nodemask_t *nmask;
+ 	gfp_t gfp_mask;
+ };
+ 
++>>>>>>> a097631160c3 (mm/mempolicy: use a standard migration target allocation callback)
  #endif	/* __MM_INTERNAL_H */
diff --cc mm/mempolicy.c
index d7fe3c1d5457,afaa09ff9f6c..000000000000
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@@ -1030,27 -1065,6 +1030,30 @@@ static int migrate_page_add(struct pag
  	return 0;
  }
  
++<<<<<<< HEAD
 +/* page allocation callback for NUMA node migration */
 +struct page *alloc_new_node_page(struct page *page, unsigned long node)
 +{
 +	if (PageHuge(page))
 +		return alloc_huge_page_node(page_hstate(compound_head(page)),
 +					node);
 +	else if (PageTransHuge(page)) {
 +		struct page *thp;
 +
 +		thp = alloc_pages_node(node,
 +			(GFP_TRANSHUGE | __GFP_THISNODE),
 +			HPAGE_PMD_ORDER);
 +		if (!thp)
 +			return NULL;
 +		prep_transhuge_page(thp);
 +		return thp;
 +	} else
 +		return __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE |
 +						    __GFP_THISNODE, 0);
 +}
 +
++=======
++>>>>>>> a097631160c3 (mm/mempolicy: use a standard migration target allocation callback)
  /*
   * Migrate pages from one node to a target node.
   * Returns error or the number of pages not migrated.
* Unmerged path mm/internal.h
* Unmerged path mm/mempolicy.c
diff --git a/mm/migrate.c b/mm/migrate.c
index d90db1e8e9a6..7ba2e2426779 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1534,9 +1534,13 @@ static int do_move_pages_to_node(struct mm_struct *mm,
 		struct list_head *pagelist, int node)
 {
 	int err;
+	struct migration_target_control mtc = {
+		.nid = node,
+		.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,
+	};
 
-	err = migrate_pages(pagelist, alloc_new_node_page, NULL, node,
-			MIGRATE_SYNC, MR_SYSCALL);
+	err = migrate_pages(pagelist, alloc_migration_target, NULL,
+			(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL);
 	if (err)
 		putback_movable_pages(pagelist);
 	return err;
