kcsan: Add kcsan_set_access_mask() support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Marco Elver <elver@google.com>
commit 81af89e15862909881ff010a0adb67148487e88a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/81af89e1.failed

When setting up an access mask with kcsan_set_access_mask(), KCSAN will
only report races if concurrent changes to bits set in access_mask are
observed. Conveying access_mask via a separate call avoids introducing
overhead in the common-case fast-path.

	Acked-by: John Hubbard <jhubbard@nvidia.com>
	Signed-off-by: Marco Elver <elver@google.com>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 81af89e15862909881ff010a0adb67148487e88a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kcsan-checks.h
#	kernel/kcsan/core.c
#	kernel/kcsan/kcsan.h
#	kernel/kcsan/report.c
diff --cc include/linux/kcsan-checks.h
index 4289e1bad23d,4ef5233ff3f0..000000000000
--- a/include/linux/kcsan-checks.h
+++ b/include/linux/kcsan-checks.h
@@@ -26,13 -32,68 +26,75 @@@
   */
  void __kcsan_check_access(const volatile void *ptr, size_t size, int type);
  
++<<<<<<< HEAD
 +#else
 +static inline void __kcsan_check_access(const volatile void *ptr, size_t size,
 +					int type) { }
 +#endif
++=======
+ /**
+  * kcsan_nestable_atomic_begin - begin nestable atomic region
+  *
+  * Accesses within the atomic region may appear to race with other accesses but
+  * should be considered atomic.
+  */
+ void kcsan_nestable_atomic_begin(void);
+ 
+ /**
+  * kcsan_nestable_atomic_end - end nestable atomic region
+  */
+ void kcsan_nestable_atomic_end(void);
+ 
+ /**
+  * kcsan_flat_atomic_begin - begin flat atomic region
+  *
+  * Accesses within the atomic region may appear to race with other accesses but
+  * should be considered atomic.
+  */
+ void kcsan_flat_atomic_begin(void);
+ 
+ /**
+  * kcsan_flat_atomic_end - end flat atomic region
+  */
+ void kcsan_flat_atomic_end(void);
+ 
+ /**
+  * kcsan_atomic_next - consider following accesses as atomic
+  *
+  * Force treating the next n memory accesses for the current context as atomic
+  * operations.
+  *
+  * @n number of following memory accesses to treat as atomic.
+  */
+ void kcsan_atomic_next(int n);
+ 
+ /**
+  * kcsan_set_access_mask - set access mask
+  *
+  * Set the access mask for all accesses for the current context if non-zero.
+  * Only value changes to bits set in the mask will be reported.
+  *
+  * @mask bitmask
+  */
+ void kcsan_set_access_mask(unsigned long mask);
+ 
+ #else /* CONFIG_KCSAN */
+ 
+ static inline void __kcsan_check_access(const volatile void *ptr, size_t size,
+ 					int type) { }
+ 
+ static inline void kcsan_nestable_atomic_begin(void)	{ }
+ static inline void kcsan_nestable_atomic_end(void)	{ }
+ static inline void kcsan_flat_atomic_begin(void)	{ }
+ static inline void kcsan_flat_atomic_end(void)		{ }
+ static inline void kcsan_atomic_next(int n)		{ }
+ static inline void kcsan_set_access_mask(unsigned long mask) { }
+ 
+ #endif /* CONFIG_KCSAN */
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  
  /*
 - * kcsan_*: Only calls into the runtime when the particular compilation unit has
 + * kcsan_*: Only calls into runtime when the particular compilation unit has
   * KCSAN instrumentation enabled. May be used in header files.
   */
  #ifdef __SANITIZE_THREAD__
diff --cc kernel/kcsan/core.c
index c40f2454f5a3,589b1e7f0f25..000000000000
--- a/kernel/kcsan/core.c
+++ b/kernel/kcsan/core.c
@@@ -19,10 -35,11 +19,18 @@@ bool kcsan_enabled
  
  /* Per-CPU kcsan_ctx for interrupts */
  static DEFINE_PER_CPU(struct kcsan_ctx, kcsan_cpu_ctx) = {
++<<<<<<< HEAD
 +	.disable_count = 0,
 +	.atomic_next = 0,
 +	.atomic_nest_count = 0,
 +	.in_flat_atomic = false,
++=======
+ 	.disable_count		= 0,
+ 	.atomic_next		= 0,
+ 	.atomic_nest_count	= 0,
+ 	.in_flat_atomic		= false,
+ 	.access_mask		= 0,
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  };
  
  /*
@@@ -306,7 -351,8 +323,12 @@@ static noinline void kcsan_setup_watchp
  		u32 _4;
  		u64 _8;
  	} expect_value;
++<<<<<<< HEAD
 +	bool value_change = false;
++=======
+ 	unsigned long access_mask;
+ 	enum kcsan_value_change value_change = KCSAN_VALUE_CHANGE_MAYBE;
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  	unsigned long ua_flags = user_access_save();
  	unsigned long irq_flags;
  
@@@ -399,18 -446,27 +421,40 @@@
  	 * Re-read value, and check if it is as expected; if not, we infer a
  	 * racy access.
  	 */
+ 	access_mask = get_ctx()->access_mask;
  	switch (size) {
  	case 1:
++<<<<<<< HEAD
 +		value_change = expect_value._1 != READ_ONCE(*(const u8 *)ptr);
 +		break;
 +	case 2:
 +		value_change = expect_value._2 != READ_ONCE(*(const u16 *)ptr);
 +		break;
 +	case 4:
 +		value_change = expect_value._4 != READ_ONCE(*(const u32 *)ptr);
 +		break;
 +	case 8:
 +		value_change = expect_value._8 != READ_ONCE(*(const u64 *)ptr);
++=======
+ 		expect_value._1 ^= READ_ONCE(*(const u8 *)ptr);
+ 		if (access_mask)
+ 			expect_value._1 &= (u8)access_mask;
+ 		break;
+ 	case 2:
+ 		expect_value._2 ^= READ_ONCE(*(const u16 *)ptr);
+ 		if (access_mask)
+ 			expect_value._2 &= (u16)access_mask;
+ 		break;
+ 	case 4:
+ 		expect_value._4 ^= READ_ONCE(*(const u32 *)ptr);
+ 		if (access_mask)
+ 			expect_value._4 &= (u32)access_mask;
+ 		break;
+ 	case 8:
+ 		expect_value._8 ^= READ_ONCE(*(const u64 *)ptr);
+ 		if (access_mask)
+ 			expect_value._8 &= (u64)access_mask;
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  		break;
  	default:
  		break; /* ignore; we do not diff the values */
@@@ -419,16 -475,49 +463,37 @@@
  	/* Check if this access raced with another. */
  	if (!remove_watchpoint(watchpoint)) {
  		/*
++<<<<<<< HEAD
++=======
+ 		 * Depending on the access type, map a value_change of MAYBE to
+ 		 * TRUE (always report) or FALSE (never report).
+ 		 */
+ 		if (value_change == KCSAN_VALUE_CHANGE_MAYBE) {
+ 			if (access_mask != 0) {
+ 				/*
+ 				 * For access with access_mask, we require a
+ 				 * value-change, as it is likely that races on
+ 				 * ~access_mask bits are expected.
+ 				 */
+ 				value_change = KCSAN_VALUE_CHANGE_FALSE;
+ 			} else if (size > 8 || is_assert) {
+ 				/* Always assume a value-change. */
+ 				value_change = KCSAN_VALUE_CHANGE_TRUE;
+ 			}
+ 		}
+ 
+ 		/*
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  		 * No need to increment 'data_races' counter, as the racing
  		 * thread already did.
 -		 *
 -		 * Count 'assert_failures' for each failed ASSERT access,
 -		 * therefore both this thread and the racing thread may
 -		 * increment this counter.
  		 */
 -		if (is_assert && value_change == KCSAN_VALUE_CHANGE_TRUE)
 -			kcsan_counter_inc(KCSAN_COUNTER_ASSERT_FAILURES);
 -
 -		kcsan_report(ptr, size, type, value_change, smp_processor_id(),
 -			     KCSAN_REPORT_RACE_SIGNAL);
 -	} else if (value_change == KCSAN_VALUE_CHANGE_TRUE) {
 +		kcsan_report(ptr, size, is_write, size > 8 || value_change,
 +			     smp_processor_id(), KCSAN_REPORT_RACE_SIGNAL);
 +	} else if (value_change) {
  		/* Inferring a race, since the value should not have changed. */
 -
  		kcsan_counter_inc(KCSAN_COUNTER_RACES_UNKNOWN_ORIGIN);
 -		if (is_assert)
 -			kcsan_counter_inc(KCSAN_COUNTER_ASSERT_FAILURES);
 -
 -		if (IS_ENABLED(CONFIG_KCSAN_REPORT_RACE_UNKNOWN_ORIGIN) || is_assert)
 -			kcsan_report(ptr, size, type, KCSAN_VALUE_CHANGE_TRUE,
 +		if (IS_ENABLED(CONFIG_KCSAN_REPORT_RACE_UNKNOWN_ORIGIN))
 +			kcsan_report(ptr, size, is_write, true,
  				     smp_processor_id(),
  				     KCSAN_REPORT_RACE_UNKNOWN_ORIGIN);
  	}
diff --cc kernel/kcsan/kcsan.h
index 1bb2f1c0d61e,892de5120c1b..000000000000
--- a/kernel/kcsan/kcsan.h
+++ b/kernel/kcsan/kcsan.h
@@@ -79,7 -86,28 +79,32 @@@ void kcsan_counter_dec(enum kcsan_count
   * Returns true if data races in the function symbol that maps to func_addr
   * (offsets are ignored) should *not* be reported.
   */
++<<<<<<< HEAD
 +bool kcsan_skip_report_debugfs(unsigned long func_addr);
++=======
+ extern bool kcsan_skip_report_debugfs(unsigned long func_addr);
+ 
+ /*
+  * Value-change states.
+  */
+ enum kcsan_value_change {
+ 	/*
+ 	 * Did not observe a value-change, however, it is valid to report the
+ 	 * race, depending on preferences.
+ 	 */
+ 	KCSAN_VALUE_CHANGE_MAYBE,
+ 
+ 	/*
+ 	 * Did not observe a value-change, and it is invalid to report the race.
+ 	 */
+ 	KCSAN_VALUE_CHANGE_FALSE,
+ 
+ 	/*
+ 	 * The value was observed to change, and the race should be reported.
+ 	 */
+ 	KCSAN_VALUE_CHANGE_TRUE,
+ };
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  
  enum kcsan_report_type {
  	/*
diff --cc kernel/kcsan/report.c
index ce5abefc35a9,11c791b886f3..000000000000
--- a/kernel/kcsan/report.c
+++ b/kernel/kcsan/report.c
@@@ -41,11 -76,84 +41,36 @@@ static DEFINE_SPINLOCK(report_lock)
  /*
   * Special rules to skip reporting.
   */
 -static bool
 -skip_report(enum kcsan_value_change value_change, unsigned long top_frame)
 +static bool skip_report(bool is_write, bool value_change,
 +			unsigned long top_frame)
  {
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY) && is_write &&
 +	    !value_change) {
++=======
+ 	/* Should never get here if value_change==FALSE. */
+ 	WARN_ON_ONCE(value_change == KCSAN_VALUE_CHANGE_FALSE);
+ 
+ 	/*
+ 	 * The first call to skip_report always has value_change==TRUE, since we
+ 	 * cannot know the value written of an instrumented access. For the 2nd
+ 	 * call there are 6 cases with CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY:
+ 	 *
+ 	 * 1. read watchpoint, conflicting write (value_change==TRUE): report;
+ 	 * 2. read watchpoint, conflicting write (value_change==MAYBE): skip;
+ 	 * 3. write watchpoint, conflicting write (value_change==TRUE): report;
+ 	 * 4. write watchpoint, conflicting write (value_change==MAYBE): skip;
+ 	 * 5. write watchpoint, conflicting read (value_change==MAYBE): skip;
+ 	 * 6. write watchpoint, conflicting read (value_change==TRUE): report;
+ 	 *
+ 	 * Cases 1-4 are intuitive and expected; case 5 ensures we do not report
+ 	 * data races where the write may have rewritten the same value; case 6
+ 	 * is possible either if the size is larger than what we check value
+ 	 * changes for or the access type is KCSAN_ACCESS_ASSERT.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY) &&
+ 	    value_change == KCSAN_VALUE_CHANGE_MAYBE) {
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  		/*
  		 * The access is a write, but the data value did not change.
  		 *
@@@ -355,10 -495,16 +380,23 @@@ void kcsan_report(const volatile void *
  	lockdep_off();
  
  	kcsan_disable_current();
++<<<<<<< HEAD
 +	if (prepare_report(&flags, ptr, size, is_write, cpu_id, type)) {
 +		if (print_report(ptr, size, is_write, value_change, cpu_id,
 +				 type) &&
 +		    panic_on_warn)
++=======
+ 	if (prepare_report(&flags, ptr, size, access_type, cpu_id, type)) {
+ 		/*
+ 		 * Never report if value_change is FALSE, only if we it is
+ 		 * either TRUE or MAYBE. In case of MAYBE, further filtering may
+ 		 * be done once we know the full stack trace in print_report().
+ 		 */
+ 		bool reported = value_change != KCSAN_VALUE_CHANGE_FALSE &&
+ 				print_report(ptr, size, access_type, value_change, cpu_id, type);
+ 
+ 		if (reported && panic_on_warn)
++>>>>>>> 81af89e15862 (kcsan: Add kcsan_set_access_mask() support)
  			panic("panic_on_warn set ...\n");
  
  		release_report(&flags, type);
* Unmerged path include/linux/kcsan-checks.h
diff --git a/include/linux/kcsan.h b/include/linux/kcsan.h
index 9047048fee84..e7fbe4cad07e 100644
--- a/include/linux/kcsan.h
+++ b/include/linux/kcsan.h
@@ -35,6 +35,11 @@ struct kcsan_ctx {
 	 */
 	int atomic_nest_count;
 	bool in_flat_atomic;
+
+	/*
+	 * Access mask for all accesses if non-zero.
+	 */
+	unsigned long access_mask;
 };
 
 /**
diff --git a/init/init_task.c b/init/init_task.c
index 156eca11f768..b112f1abc24d 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -174,6 +174,7 @@ struct task_struct init_task
 		.atomic_next		= 0,
 		.atomic_nest_count	= 0,
 		.in_flat_atomic		= false,
+		.access_mask		= 0,
 	},
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
* Unmerged path kernel/kcsan/core.c
* Unmerged path kernel/kcsan/kcsan.h
* Unmerged path kernel/kcsan/report.c
