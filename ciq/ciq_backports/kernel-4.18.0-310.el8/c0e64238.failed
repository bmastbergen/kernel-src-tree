KVM: x86/mmu: Protect the tdp_mmu_roots list with RCU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Ben Gardon <bgardon@google.com>
commit c0e64238ac53e8226e3fe72279e5e76253e85f88
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/c0e64238.failed

Protect the contents of the TDP MMU roots list with RCU in preparation
for a future patch which will allow the iterator macro to be used under
the MMU lock in read mode.

	Signed-off-by: Ben Gardon <bgardon@google.com>
Message-Id: <20210401233736.638171-9-bgardon@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c0e64238ac53e8226e3fe72279e5e76253e85f88)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index f8ed7b2ec807,ef65fd941317..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -65,29 -83,42 +83,53 @@@ void kvm_tdp_mmu_put_root(struct kvm *k
  
  	zap_gfn_range(kvm, root, 0, max_gfn, false, false);
  
- 	tdp_mmu_free_sp(root);
+ 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
  }
  
 -/*
 - * Finds the next valid root after root (or the first valid root if root
 - * is NULL), takes a reference on it, and returns that next root. If root
 - * is not NULL, this thread should have already taken a reference on it, and
 - * that reference will be dropped. If no valid root is found, this
 - * function will return NULL.
 - */
 -static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 -					      struct kvm_mmu_page *prev_root)
 +static inline bool tdp_mmu_next_root_valid(struct kvm *kvm,
 +					   struct kvm_mmu_page *root)
 +{
 +	lockdep_assert_held_write(&kvm->mmu_lock);
 +
 +	if (list_entry_is_head(root, &kvm->arch.tdp_mmu_roots, link))
 +		return false;
 +
 +	kvm_tdp_mmu_get_root(kvm, root);
 +	return true;
 +
 +}
 +
 +static inline struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 +						     struct kvm_mmu_page *root)
  {
  	struct kvm_mmu_page *next_root;
  
++<<<<<<< HEAD
 +	next_root = list_next_entry(root, link);
 +	kvm_tdp_mmu_put_root(kvm, root);
++=======
+ 	lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	rcu_read_lock();
+ 
+ 	if (prev_root)
+ 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+ 						  &prev_root->link,
+ 						  typeof(*prev_root), link);
+ 	else
+ 		next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+ 						   typeof(*next_root), link);
+ 
+ 	while (next_root && !kvm_tdp_mmu_get_root(kvm, next_root))
+ 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+ 				&next_root->link, typeof(*next_root), link);
+ 
+ 	rcu_read_unlock();
+ 
+ 	if (prev_root)
+ 		kvm_tdp_mmu_put_root(kvm, prev_root);
+ 
++>>>>>>> c0e64238ac53 (KVM: x86/mmu: Protect the tdp_mmu_roots list with RCU)
  	return next_root;
  }
  
@@@ -97,14 -128,19 +139,30 @@@
   * if exiting the loop early, the caller must drop the reference to the most
   * recent root. (Unless keeping a live reference is desirable.)
   */
++<<<<<<< HEAD
 +#define for_each_tdp_mmu_root_yield_safe(_kvm, _root)				\
 +	for (_root = list_first_entry(&_kvm->arch.tdp_mmu_roots,	\
 +				      typeof(*_root), link);		\
 +	     tdp_mmu_next_root_valid(_kvm, _root);			\
 +	     _root = tdp_mmu_next_root(_kvm, _root))
 +
 +#define for_each_tdp_mmu_root(_kvm, _root)				\
 +	list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link)
++=======
+ #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id)	\
+ 	for (_root = tdp_mmu_next_root(_kvm, NULL);		\
+ 	     _root;						\
+ 	     _root = tdp_mmu_next_root(_kvm, _root))		\
+ 		if (kvm_mmu_page_as_id(_root) != _as_id) {	\
+ 		} else
+ 
+ #define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
+ 	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
+ 				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
+ 				lockdep_is_held(&kvm->arch.tdp_mmu_pages_lock))	\
+ 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
+ 		} else
++>>>>>>> c0e64238ac53 (KVM: x86/mmu: Protect the tdp_mmu_roots list with RCU)
  
  static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
  						   int level)
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6d67ff1f4c5a..27b064f23e56 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1036,25 +1036,36 @@ struct kvm_arch {
 	bool tdp_mmu_enabled;
 
 	/*
-	 * List of struct kvmp_mmu_pages being used as roots.
+	 * List of struct kvm_mmu_pages being used as roots.
 	 * All struct kvm_mmu_pages in the list should have
 	 * tdp_mmu_page set.
-	 * All struct kvm_mmu_pages in the list should have a positive
-	 * root_count except when a thread holds the MMU lock and is removing
-	 * an entry from the list.
+	 *
+	 * For reads, this list is protected by:
+	 *	the MMU lock in read mode + RCU or
+	 *	the MMU lock in write mode
+	 *
+	 * For writes, this list is protected by:
+	 *	the MMU lock in read mode + the tdp_mmu_pages_lock or
+	 *	the MMU lock in write mode
+	 *
+	 * Roots will remain in the list until their tdp_mmu_root_count
+	 * drops to zero, at which point the thread that decremented the
+	 * count to zero should removed the root from the list and clean
+	 * it up, freeing the root after an RCU grace period.
 	 */
 	struct list_head tdp_mmu_roots;
 
 	/*
 	 * List of struct kvmp_mmu_pages not being used as roots.
 	 * All struct kvm_mmu_pages in the list should have
-	 * tdp_mmu_page set and a root_count of 0.
+	 * tdp_mmu_page set and a tdp_mmu_root_count of 0.
 	 */
 	struct list_head tdp_mmu_pages;
 
 	/*
 	 * Protects accesses to the following fields when the MMU lock
 	 * is held in read mode:
+	 *  - tdp_mmu_roots (above)
 	 *  - tdp_mmu_pages (above)
 	 *  - the link field of struct kvm_mmu_pages used by the TDP MMU
 	 *  - lpage_disallowed_mmu_pages
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
