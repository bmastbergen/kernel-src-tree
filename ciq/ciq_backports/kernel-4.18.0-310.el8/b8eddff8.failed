mm: memcontrol: add file_thp, shmem_thp to memory.stat

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit b8eddff8886b173b0a0f21a3bb1a594cc6d974d1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/b8eddff8.failed

As huge page usage in the page cache and for shmem files proliferates in
our production environment, the performance monitoring team has asked for
per-cgroup stats on those pages.

We already track and export anon_thp per cgroup.  We already track file
THP and shmem THP per node, so making them per-cgroup is only a matter of
switching from node to lruvec counters.  All callsites are in places where
the pages are charged and locked, so page->memcg is stable.

[hannes@cmpxchg.org: add documentation]
  Link: https://lkml.kernel.org/r/20201026174029.GC548555@cmpxchg.org

Link: https://lkml.kernel.org/r/20201022151844.489337-1-hannes@cmpxchg.org
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Rik van Riel <riel@surriel.com>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: David Rientjes <rientjes@google.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b8eddff8886b173b0a0f21a3bb1a594cc6d974d1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index e4ee0600b869,c3654510fb70..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -1473,6 -1489,74 +1473,77 @@@ static bool mem_cgroup_wait_acct_move(s
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ struct memory_stat {
+ 	const char *name;
+ 	unsigned int ratio;
+ 	unsigned int idx;
+ };
+ 
+ static struct memory_stat memory_stats[] = {
+ 	{ "anon", PAGE_SIZE, NR_ANON_MAPPED },
+ 	{ "file", PAGE_SIZE, NR_FILE_PAGES },
+ 	{ "kernel_stack", 1024, NR_KERNEL_STACK_KB },
+ 	{ "percpu", 1, MEMCG_PERCPU_B },
+ 	{ "sock", PAGE_SIZE, MEMCG_SOCK },
+ 	{ "shmem", PAGE_SIZE, NR_SHMEM },
+ 	{ "file_mapped", PAGE_SIZE, NR_FILE_MAPPED },
+ 	{ "file_dirty", PAGE_SIZE, NR_FILE_DIRTY },
+ 	{ "file_writeback", PAGE_SIZE, NR_WRITEBACK },
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/*
+ 	 * The ratio will be initialized in memory_stats_init(). Because
+ 	 * on some architectures, the macro of HPAGE_PMD_SIZE is not
+ 	 * constant(e.g. powerpc).
+ 	 */
+ 	{ "anon_thp", 0, NR_ANON_THPS },
+ 	{ "file_thp", 0, NR_FILE_THPS },
+ 	{ "shmem_thp", 0, NR_SHMEM_THPS },
+ #endif
+ 	{ "inactive_anon", PAGE_SIZE, NR_INACTIVE_ANON },
+ 	{ "active_anon", PAGE_SIZE, NR_ACTIVE_ANON },
+ 	{ "inactive_file", PAGE_SIZE, NR_INACTIVE_FILE },
+ 	{ "active_file", PAGE_SIZE, NR_ACTIVE_FILE },
+ 	{ "unevictable", PAGE_SIZE, NR_UNEVICTABLE },
+ 
+ 	/*
+ 	 * Note: The slab_reclaimable and slab_unreclaimable must be
+ 	 * together and slab_reclaimable must be in front.
+ 	 */
+ 	{ "slab_reclaimable", 1, NR_SLAB_RECLAIMABLE_B },
+ 	{ "slab_unreclaimable", 1, NR_SLAB_UNRECLAIMABLE_B },
+ 
+ 	/* The memory events */
+ 	{ "workingset_refault_anon", 1, WORKINGSET_REFAULT_ANON },
+ 	{ "workingset_refault_file", 1, WORKINGSET_REFAULT_FILE },
+ 	{ "workingset_activate_anon", 1, WORKINGSET_ACTIVATE_ANON },
+ 	{ "workingset_activate_file", 1, WORKINGSET_ACTIVATE_FILE },
+ 	{ "workingset_restore_anon", 1, WORKINGSET_RESTORE_ANON },
+ 	{ "workingset_restore_file", 1, WORKINGSET_RESTORE_FILE },
+ 	{ "workingset_nodereclaim", 1, WORKINGSET_NODERECLAIM },
+ };
+ 
+ static int __init memory_stats_init(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(memory_stats); i++) {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 		if (memory_stats[i].idx == NR_ANON_THPS ||
+ 		    memory_stats[i].idx == NR_FILE_THPS ||
+ 		    memory_stats[i].idx == NR_SHMEM_THPS)
+ 			memory_stats[i].ratio = HPAGE_PMD_SIZE;
+ #endif
+ 		VM_BUG_ON(!memory_stats[i].ratio);
+ 		VM_BUG_ON(memory_stats[i].idx >= MEMCG_NR_STAT);
+ 	}
+ 
+ 	return 0;
+ }
+ pure_initcall(memory_stats_init);
+ 
++>>>>>>> b8eddff8886b (mm: memcontrol: add file_thp, shmem_thp to memory.stat)
  static char *memory_stat_format(struct mem_cgroup *memcg)
  {
  	struct seq_buf s;
diff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst
index bc0fc04d2632..f743a8da58c3 100644
--- a/Documentation/admin-guide/cgroup-v2.rst
+++ b/Documentation/admin-guide/cgroup-v2.rst
@@ -1268,6 +1268,14 @@ PAGE_SIZE multiple when read back.
 		Amount of memory used in anonymous mappings backed by
 		transparent hugepages
 
+	  file_thp
+		Amount of cached filesystem data backed by transparent
+		hugepages
+
+	  shmem_thp
+		Amount of shm, tmpfs, shared anonymous mmap()s backed by
+		transparent hugepages
+
 	  inactive_anon, active_anon, inactive_file, active_file, unevictable
 		Amount of memory, swap-backed and filesystem-backed,
 		on the internal memory management lists used by the
diff --git a/mm/filemap.c b/mm/filemap.c
index 43e3b7ebb660..2b929f59fbc9 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -202,9 +202,9 @@ static void unaccount_page_cache_page(struct address_space *mapping,
 	if (PageSwapBacked(page)) {
 		__mod_lruvec_page_state(page, NR_SHMEM, -nr);
 		if (PageTransHuge(page))
-			__dec_node_page_state(page, NR_SHMEM_THPS);
+			__dec_lruvec_page_state(page, NR_SHMEM_THPS);
 	} else if (PageTransHuge(page)) {
-		__dec_node_page_state(page, NR_FILE_THPS);
+		__dec_lruvec_page_state(page, NR_FILE_THPS);
 		filemap_nr_thps_dec(mapping);
 	}
 
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index f8f2e337b025..e3da465f5e2c 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2636,9 +2636,9 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 		spin_unlock(&ds_queue->split_queue_lock);
 		if (mapping) {
 			if (PageSwapBacked(head))
-				__dec_node_page_state(head, NR_SHMEM_THPS);
+				__dec_lruvec_page_state(head, NR_SHMEM_THPS);
 			else
-				__dec_node_page_state(head, NR_FILE_THPS);
+				__dec_lruvec_page_state(head, NR_FILE_THPS);
 		}
 
 		__split_huge_page(page, list, end, flags);
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 2f68c8fcd2bc..1797ab089748 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -1584,9 +1584,9 @@ static void collapse_file(struct mm_struct *mm,
 	}
 
 	if (is_shmem)
-		__inc_node_page_state(new_page, NR_SHMEM_THPS);
+		__inc_lruvec_page_state(new_page, NR_SHMEM_THPS);
 	else {
-		__inc_node_page_state(new_page, NR_FILE_THPS);
+		__inc_lruvec_page_state(new_page, NR_FILE_THPS);
 		filemap_nr_thps_inc(mapping);
 	}
 
* Unmerged path mm/memcontrol.c
diff --git a/mm/shmem.c b/mm/shmem.c
index 743eb238d81e..5963377f983e 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -649,7 +649,7 @@ static int shmem_add_to_page_cache(struct page *page,
 		}
 		if (PageTransHuge(page)) {
 			count_vm_event(THP_FILE_ALLOC);
-			__inc_node_page_state(page, NR_SHMEM_THPS);
+			__inc_lruvec_page_state(page, NR_SHMEM_THPS);
 		}
 		mapping->nrpages += nr;
 		__mod_lruvec_page_state(page, NR_FILE_PAGES, nr);
