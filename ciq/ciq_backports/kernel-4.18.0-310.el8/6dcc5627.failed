x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Jiri Slaby <jslaby@suse.cz>
commit 6dcc5627f6aec4cb1d1494d06a48d8061db06a04
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/6dcc5627.failed

These are all functions which are invoked from elsewhere, so annotate
them as global using the new SYM_FUNC_START and their ENDPROC's by
SYM_FUNC_END.

Make sure ENTRY/ENDPROC is not defined on X86_64, given these were the
last users.

	Signed-off-by: Jiri Slaby <jslaby@suse.cz>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com> [hibernate]
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com> [xen bits]
	Acked-by: Herbert Xu <herbert@gondor.apana.org.au> [crypto]
	Cc: Allison Randal <allison@lohutok.net>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Andy Shevchenko <andy@infradead.org>
	Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
	Cc: Armijn Hemel <armijn@tjaldur.nl>
	Cc: Cao jin <caoj.fnst@cn.fujitsu.com>
	Cc: Darren Hart <dvhart@infradead.org>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Enrico Weigelt <info@metux.net>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Herbert Xu <herbert@gondor.apana.org.au>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jim Mattson <jmattson@google.com>
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Kate Stewart <kstewart@linuxfoundation.org>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: kvm ML <kvm@vger.kernel.org>
	Cc: Len Brown <len.brown@intel.com>
	Cc: linux-arch@vger.kernel.org
	Cc: linux-crypto@vger.kernel.org
	Cc: linux-efi <linux-efi@vger.kernel.org>
	Cc: linux-efi@vger.kernel.org
	Cc: linux-pm@vger.kernel.org
	Cc: Mark Rutland <mark.rutland@arm.com>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Pavel Machek <pavel@ucw.cz>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: platform-driver-x86@vger.kernel.org
	Cc: "Radim Krčmář" <rkrcmar@redhat.com>
	Cc: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: Stefano Stabellini <sstabellini@kernel.org>
	Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
	Cc: Wanpeng Li <wanpengli@tencent.com>
	Cc: Wei Huang <wei@redhat.com>
	Cc: x86-ml <x86@kernel.org>
	Cc: xen-devel@lists.xenproject.org
	Cc: Xiaoyao Li <xiaoyao.li@linux.intel.com>
Link: https://lkml.kernel.org/r/20191011115108.12392-25-jslaby@suse.cz
(cherry picked from commit 6dcc5627f6aec4cb1d1494d06a48d8061db06a04)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/mem_encrypt.S
#	arch/x86/crypto/chacha-avx2-x86_64.S
#	arch/x86/crypto/chacha-avx512vl-x86_64.S
#	arch/x86/crypto/chacha20-ssse3-x86_64.S
#	arch/x86/crypto/crct10dif-pcl-asm_64.S
#	arch/x86/crypto/nh-avx2-x86_64.S
#	arch/x86/crypto/nh-sse2-x86_64.S
#	arch/x86/kernel/acpi/wakeup_64.S
#	arch/x86/kvm/vmx/vmenter.S
#	arch/x86/lib/memmove_64.S
#	arch/x86/lib/memset_64.S
#	arch/x86/platform/efi/efi_thunk_64.S
#	arch/x86/xen/xen-asm.S
#	arch/x86/xen/xen-asm_64.S
diff --cc arch/x86/boot/compressed/mem_encrypt.S
index 96f448c527a6,dd07e7b41b11..000000000000
--- a/arch/x86/boot/compressed/mem_encrypt.S
+++ b/arch/x86/boot/compressed/mem_encrypt.S
@@@ -68,13 -65,10 +68,17 @@@ SYM_FUNC_START(get_sev_encryption_bit
  #endif	/* CONFIG_AMD_MEM_ENCRYPT */
  
  	ret
- ENDPROC(get_sev_encryption_bit)
+ SYM_FUNC_END(get_sev_encryption_bit)
  
  	.code64
++<<<<<<< HEAD
 +
 +#include "../../kernel/sev_verify_cbit.S"
 +
 +ENTRY(set_sev_encryption_mask)
++=======
+ SYM_FUNC_START(set_sev_encryption_mask)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  #ifdef CONFIG_AMD_MEM_ENCRYPT
  	push	%rbp
  	push	%rdx
diff --cc arch/x86/crypto/chacha20-ssse3-x86_64.S
index 512a2b500fd1,a38ab2512a6f..000000000000
--- a/arch/x86/crypto/chacha20-ssse3-x86_64.S
+++ b/arch/x86/crypto/chacha20-ssse3-x86_64.S
@@@ -118,39 -105,129 +118,127 @@@ ENTRY(chacha20_block_xor_ssse3
  	# x3 = shuffle32(x3, MASK(0, 3, 2, 1))
  	pshufd		$0x39,%xmm3,%xmm3
  
 -	sub		$2,%r8d
 +	dec		%ecx
  	jnz		.Ldoubleround
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
++=======
+ 	ret
+ SYM_FUNC_END(chacha_permute)
+ 
+ SYM_FUNC_START(chacha_block_xor_ssse3)
+ 	# %rdi: Input state matrix, s
+ 	# %rsi: up to 1 data block output, o
+ 	# %rdx: up to 1 data block input, i
+ 	# %rcx: input/output length in bytes
+ 	# %r8d: nrounds
+ 	FRAME_BEGIN
+ 
+ 	# x0..3 = s0..3
+ 	movdqa		0x00(%rdi),%xmm0
+ 	movdqa		0x10(%rdi),%xmm1
+ 	movdqa		0x20(%rdi),%xmm2
+ 	movdqa		0x30(%rdi),%xmm3
+ 	movdqa		%xmm0,%xmm8
+ 	movdqa		%xmm1,%xmm9
+ 	movdqa		%xmm2,%xmm10
+ 	movdqa		%xmm3,%xmm11
+ 
+ 	mov		%rcx,%rax
+ 	call		chacha_permute
+ 
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*):arch/x86/crypto/chacha-ssse3-x86_64.S
  	# o0 = i0 ^ (x0 + s0)
 -	paddd		%xmm8,%xmm0
 -	cmp		$0x10,%rax
 -	jl		.Lxorpart
  	movdqu		0x00(%rdx),%xmm4
 +	paddd		%xmm8,%xmm0
  	pxor		%xmm4,%xmm0
  	movdqu		%xmm0,0x00(%rsi)
  	# o1 = i1 ^ (x1 + s1)
 +	movdqu		0x10(%rdx),%xmm5
  	paddd		%xmm9,%xmm1
 -	movdqa		%xmm1,%xmm0
 -	cmp		$0x20,%rax
 -	jl		.Lxorpart
 -	movdqu		0x10(%rdx),%xmm0
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0x10(%rsi)
 +	pxor		%xmm5,%xmm1
 +	movdqu		%xmm1,0x10(%rsi)
  	# o2 = i2 ^ (x2 + s2)
 +	movdqu		0x20(%rdx),%xmm6
  	paddd		%xmm10,%xmm2
 -	movdqa		%xmm2,%xmm0
 -	cmp		$0x30,%rax
 -	jl		.Lxorpart
 -	movdqu		0x20(%rdx),%xmm0
 -	pxor		%xmm2,%xmm0
 -	movdqu		%xmm0,0x20(%rsi)
 +	pxor		%xmm6,%xmm2
 +	movdqu		%xmm2,0x20(%rsi)
  	# o3 = i3 ^ (x3 + s3)
 +	movdqu		0x30(%rdx),%xmm7
  	paddd		%xmm11,%xmm3
 -	movdqa		%xmm3,%xmm0
 -	cmp		$0x40,%rax
 -	jl		.Lxorpart
 -	movdqu		0x30(%rdx),%xmm0
 -	pxor		%xmm3,%xmm0
 -	movdqu		%xmm0,0x30(%rsi)
 -
 -.Ldone:
 -	FRAME_END
 +	pxor		%xmm7,%xmm3
 +	movdqu		%xmm3,0x30(%rsi)
 +
  	ret
 +ENDPROC(chacha20_block_xor_ssse3)
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +ENTRY(chacha20_4block_xor_ssse3)
++=======
+ .Lxorpart:
+ 	# xor remaining bytes from partial register into output
+ 	mov		%rax,%r9
+ 	and		$0x0f,%r9
+ 	jz		.Ldone
+ 	and		$~0x0f,%rax
+ 
+ 	mov		%rsi,%r11
+ 
+ 	lea		8(%rsp),%r10
+ 	sub		$0x10,%rsp
+ 	and		$~31,%rsp
+ 
+ 	lea		(%rdx,%rax),%rsi
+ 	mov		%rsp,%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	pxor		0x00(%rsp),%xmm0
+ 	movdqa		%xmm0,0x00(%rsp)
+ 
+ 	mov		%rsp,%rsi
+ 	lea		(%r11,%rax),%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	lea		-8(%r10),%rsp
+ 	jmp		.Ldone
+ 
+ SYM_FUNC_END(chacha_block_xor_ssse3)
+ 
+ SYM_FUNC_START(hchacha_block_ssse3)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*):arch/x86/crypto/chacha-ssse3-x86_64.S
  	# %rdi: Input state matrix, s
 -	# %rsi: output (8 32-bit words)
 -	# %edx: nrounds
 -	FRAME_BEGIN
 +	# %rsi: 4 data blocks output, o
 +	# %rdx: 4 data blocks input, i
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +	# This function encrypts four consecutive ChaCha20 blocks by loading the
++=======
+ 	movdqa		0x00(%rdi),%xmm0
+ 	movdqa		0x10(%rdi),%xmm1
+ 	movdqa		0x20(%rdi),%xmm2
+ 	movdqa		0x30(%rdi),%xmm3
+ 
+ 	mov		%edx,%r8d
+ 	call		chacha_permute
+ 
+ 	movdqu		%xmm0,0x00(%rsi)
+ 	movdqu		%xmm3,0x10(%rsi)
+ 
+ 	FRAME_END
+ 	ret
+ SYM_FUNC_END(hchacha_block_ssse3)
+ 
+ SYM_FUNC_START(chacha_4block_xor_ssse3)
+ 	# %rdi: Input state matrix, s
+ 	# %rsi: up to 4 data blocks output, o
+ 	# %rdx: up to 4 data blocks input, i
+ 	# %rcx: input/output length in bytes
+ 	# %r8d: nrounds
+ 
+ 	# This function encrypts four consecutive ChaCha blocks by loading the
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*):arch/x86/crypto/chacha-ssse3-x86_64.S
  	# the state matrix in SSE registers four times. As we need some scratch
  	# registers, we save the first four registers on the stack. The
  	# algorithm performs each operation on the corresponding word of each
@@@ -588,43 -736,56 +676,72 @@@
  	movdqu		0xc0(%rdx),%xmm1
  	pxor		%xmm1,%xmm0
  	movdqu		%xmm0,0xc0(%rsi)
 -
 -	movdqu		%xmm7,%xmm0
 -	cmp		$0xe0,%rax
 -	jl		.Lxorpart4
 +	movdqu		0x10(%rdx),%xmm1
 +	pxor		%xmm1,%xmm4
 +	movdqu		%xmm4,0x10(%rsi)
 +	movdqu		0x90(%rdx),%xmm1
 +	pxor		%xmm1,%xmm5
 +	movdqu		%xmm5,0x90(%rsi)
 +	movdqu		0x50(%rdx),%xmm1
 +	pxor		%xmm1,%xmm6
 +	movdqu		%xmm6,0x50(%rsi)
  	movdqu		0xd0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xd0(%rsi)
 -
 -	movdqu		%xmm11,%xmm0
 -	cmp		$0xf0,%rax
 -	jl		.Lxorpart4
 +	pxor		%xmm1,%xmm7
 +	movdqu		%xmm7,0xd0(%rsi)
 +	movdqu		0x20(%rdx),%xmm1
 +	pxor		%xmm1,%xmm8
 +	movdqu		%xmm8,0x20(%rsi)
 +	movdqu		0xa0(%rdx),%xmm1
 +	pxor		%xmm1,%xmm9
 +	movdqu		%xmm9,0xa0(%rsi)
 +	movdqu		0x60(%rdx),%xmm1
 +	pxor		%xmm1,%xmm10
 +	movdqu		%xmm10,0x60(%rsi)
  	movdqu		0xe0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xe0(%rsi)
 -
 -	movdqu		%xmm15,%xmm0
 -	cmp		$0x100,%rax
 -	jl		.Lxorpart4
 +	pxor		%xmm1,%xmm11
 +	movdqu		%xmm11,0xe0(%rsi)
 +	movdqu		0x30(%rdx),%xmm1
 +	pxor		%xmm1,%xmm12
 +	movdqu		%xmm12,0x30(%rsi)
 +	movdqu		0xb0(%rdx),%xmm1
 +	pxor		%xmm1,%xmm13
 +	movdqu		%xmm13,0xb0(%rsi)
 +	movdqu		0x70(%rdx),%xmm1
 +	pxor		%xmm1,%xmm14
 +	movdqu		%xmm14,0x70(%rsi)
  	movdqu		0xf0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xf0(%rsi)
 +	pxor		%xmm1,%xmm15
 +	movdqu		%xmm15,0xf0(%rsi)
  
 -.Ldone4:
  	lea		-8(%r10),%rsp
  	ret
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +ENDPROC(chacha20_4block_xor_ssse3)
++=======
+ 
+ .Lxorpart4:
+ 	# xor remaining bytes from partial register into output
+ 	mov		%rax,%r9
+ 	and		$0x0f,%r9
+ 	jz		.Ldone4
+ 	and		$~0x0f,%rax
+ 
+ 	mov		%rsi,%r11
+ 
+ 	lea		(%rdx,%rax),%rsi
+ 	mov		%rsp,%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	pxor		0x00(%rsp),%xmm0
+ 	movdqa		%xmm0,0x00(%rsp)
+ 
+ 	mov		%rsp,%rsi
+ 	lea		(%r11,%rax),%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	jmp		.Ldone4
+ 
+ SYM_FUNC_END(chacha_4block_xor_ssse3)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*):arch/x86/crypto/chacha-ssse3-x86_64.S
diff --cc arch/x86/crypto/crct10dif-pcl-asm_64.S
index de04d3e98d8d,b2533d63030e..000000000000
--- a/arch/x86/crypto/crct10dif-pcl-asm_64.S
+++ b/arch/x86/crypto/crct10dif-pcl-asm_64.S
@@@ -62,497 -54,233 +62,546 @@@
  
  .text
  
 -#define		init_crc	%edi
 -#define		buf		%rsi
 -#define		len		%rdx
 +#define        arg1 %rdi
 +#define        arg2 %rsi
 +#define        arg3 %rdx
  
 -#define		FOLD_CONSTS	%xmm10
 -#define		BSWAP_MASK	%xmm11
 +#define        arg1_low32 %edi
  
++<<<<<<< HEAD
 +ENTRY(crc_t10dif_pcl)
 +.align 16
++=======
+ # Fold reg1, reg2 into the next 32 data bytes, storing the result back into
+ # reg1, reg2.
+ .macro	fold_32_bytes	offset, reg1, reg2
+ 	movdqu	\offset(buf), %xmm9
+ 	movdqu	\offset+16(buf), %xmm12
+ 	pshufb	BSWAP_MASK, %xmm9
+ 	pshufb	BSWAP_MASK, %xmm12
+ 	movdqa	\reg1, %xmm8
+ 	movdqa	\reg2, %xmm13
+ 	pclmulqdq	$0x00, FOLD_CONSTS, \reg1
+ 	pclmulqdq	$0x11, FOLD_CONSTS, %xmm8
+ 	pclmulqdq	$0x00, FOLD_CONSTS, \reg2
+ 	pclmulqdq	$0x11, FOLD_CONSTS, %xmm13
+ 	pxor	%xmm9 , \reg1
+ 	xorps	%xmm8 , \reg1
+ 	pxor	%xmm12, \reg2
+ 	xorps	%xmm13, \reg2
+ .endm
+ 
+ # Fold src_reg into dst_reg.
+ .macro	fold_16_bytes	src_reg, dst_reg
+ 	movdqa	\src_reg, %xmm8
+ 	pclmulqdq	$0x11, FOLD_CONSTS, \src_reg
+ 	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
+ 	pxor	%xmm8, \dst_reg
+ 	xorps	\src_reg, \dst_reg
+ .endm
+ 
+ #
+ # u16 crc_t10dif_pcl(u16 init_crc, const *u8 buf, size_t len);
+ #
+ # Assumes len >= 16.
+ #
+ .align 16
+ SYM_FUNC_START(crc_t10dif_pcl)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
 +
 +	# adjust the 16-bit initial_crc value, scale it to 32 bits
 +	shl	$16, arg1_low32
 +
 +	# Allocate Stack Space
 +	mov     %rsp, %rcx
 +	sub	$16*2, %rsp
 +	# align stack to 16 byte boundary
 +	and     $~(0x10 - 1), %rsp
 +
 +	# check if smaller than 256
 +	cmp	$256, arg3
 +
 +	# for sizes less than 128, we can't fold 64B at a time...
 +	jl	_less_than_128
 +
 +
 +	# load the initial crc value
 +	movd	arg1_low32, %xmm10	# initial crc
 +
 +	# crc value does not need to be byte-reflected, but it needs
 +	# to be moved to the high part of the register.
 +	# because data will be byte-reflected and will align with
 +	# initial crc at correct place.
 +	pslldq	$12, %xmm10
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +	# receive the initial 64B data, xor the initial crc value
 +	movdqu	16*0(arg2), %xmm0
 +	movdqu	16*1(arg2), %xmm1
 +	movdqu	16*2(arg2), %xmm2
 +	movdqu	16*3(arg2), %xmm3
 +	movdqu	16*4(arg2), %xmm4
 +	movdqu	16*5(arg2), %xmm5
 +	movdqu	16*6(arg2), %xmm6
 +	movdqu	16*7(arg2), %xmm7
 +
 +	pshufb	%xmm11, %xmm0
 +	# XOR the initial_crc value
 +	pxor	%xmm10, %xmm0
 +	pshufb	%xmm11, %xmm1
 +	pshufb	%xmm11, %xmm2
 +	pshufb	%xmm11, %xmm3
 +	pshufb	%xmm11, %xmm4
 +	pshufb	%xmm11, %xmm5
 +	pshufb	%xmm11, %xmm6
 +	pshufb	%xmm11, %xmm7
 +
 +	movdqa	rk3(%rip), %xmm10	#xmm10 has rk3 and rk4
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +
 +	#################################################################
 +	# we subtract 256 instead of 128 to save one instruction from the loop
 +	sub	$256, arg3
 +
 +	# at this section of the code, there is 64*x+y (0<=y<64) bytes of
 +	# buffer. The _fold_64_B_loop will fold 64B at a time
 +	# until we have 64+y Bytes of buffer
 +
 +
 +	# fold 64B at a time. This section of the code folds 4 xmm
 +	# registers in parallel
 +_fold_64_B_loop:
 +
 +	# update the buffer pointer
 +	add	$128, arg2		#    buf += 64#
 +
 +	movdqu	16*0(arg2), %xmm9
 +	movdqu	16*1(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm0, %xmm8
 +	movdqa	%xmm1, %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm0
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm1
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm0
 +	xorps	%xmm8 , %xmm0
 +	pxor	%xmm12, %xmm1
 +	xorps	%xmm13, %xmm1
 +
 +	movdqu	16*2(arg2), %xmm9
 +	movdqu	16*3(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm2, %xmm8
 +	movdqa	%xmm3, %xmm13
 +	pclmulqdq	$0x0, %xmm10, %xmm2
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0, %xmm10, %xmm3
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm2
 +	xorps	%xmm8 , %xmm2
 +	pxor	%xmm12, %xmm3
 +	xorps	%xmm13, %xmm3
 +
 +	movdqu	16*4(arg2), %xmm9
 +	movdqu	16*5(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm4, %xmm8
 +	movdqa	%xmm5, %xmm13
 +	pclmulqdq	$0x0,  %xmm10, %xmm4
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0,  %xmm10, %xmm5
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 ,  %xmm4
 +	xorps	%xmm8 ,  %xmm4
 +	pxor	%xmm12,  %xmm5
 +	xorps	%xmm13,  %xmm5
 +
 +	movdqu	16*6(arg2), %xmm9
 +	movdqu	16*7(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm6 , %xmm8
 +	movdqa	%xmm7 , %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm6
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm6
 +	xorps	%xmm8 , %xmm6
 +	pxor	%xmm12, %xmm7
 +	xorps	%xmm13, %xmm7
 +
 +	sub	$128, arg3
 +
 +	# check if there is another 64B in the buffer to be able to fold
 +	jge	_fold_64_B_loop
 +	##################################################################
 +
 +
 +	add	$128, arg2
 +	# at this point, the buffer pointer is pointing at the last y Bytes
 +	# of the buffer the 64B of folded data is in 4 of the xmm
 +	# registers: xmm0, xmm1, xmm2, xmm3
 +
 +
 +	# fold the 8 xmm registers to 1 xmm register with different constants
 +
 +	movdqa	rk9(%rip), %xmm10
 +	movdqa	%xmm0, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm0
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm0, %xmm7
  
 -	movdqa	.Lbswap_mask(%rip), BSWAP_MASK
 -
 -	# For sizes less than 256 bytes, we can't fold 128 bytes at a time.
 -	cmp	$256, len
 -	jl	.Lless_than_256_bytes
 -
 -	# Load the first 128 data bytes.  Byte swapping is necessary to make the
 -	# bit order match the polynomial coefficient order.
 -	movdqu	16*0(buf), %xmm0
 -	movdqu	16*1(buf), %xmm1
 -	movdqu	16*2(buf), %xmm2
 -	movdqu	16*3(buf), %xmm3
 -	movdqu	16*4(buf), %xmm4
 -	movdqu	16*5(buf), %xmm5
 -	movdqu	16*6(buf), %xmm6
 -	movdqu	16*7(buf), %xmm7
 -	add	$128, buf
 -	pshufb	BSWAP_MASK, %xmm0
 -	pshufb	BSWAP_MASK, %xmm1
 -	pshufb	BSWAP_MASK, %xmm2
 -	pshufb	BSWAP_MASK, %xmm3
 -	pshufb	BSWAP_MASK, %xmm4
 -	pshufb	BSWAP_MASK, %xmm5
 -	pshufb	BSWAP_MASK, %xmm6
 -	pshufb	BSWAP_MASK, %xmm7
 -
 -	# XOR the first 16 data *bits* with the initial CRC value.
 -	pxor	%xmm8, %xmm8
 -	pinsrw	$7, init_crc, %xmm8
 -	pxor	%xmm8, %xmm0
 -
 -	movdqa	.Lfold_across_128_bytes_consts(%rip), FOLD_CONSTS
 -
 -	# Subtract 128 for the 128 data bytes just consumed.  Subtract another
 -	# 128 to simplify the termination condition of the following loop.
 -	sub	$256, len
 -
 -	# While >= 128 data bytes remain (not counting xmm0-7), fold the 128
 -	# bytes xmm0-7 into them, storing the result back into xmm0-7.
 -.Lfold_128_bytes_loop:
 -	fold_32_bytes	0, %xmm0, %xmm1
 -	fold_32_bytes	32, %xmm2, %xmm3
 -	fold_32_bytes	64, %xmm4, %xmm5
 -	fold_32_bytes	96, %xmm6, %xmm7
 -	add	$128, buf
 -	sub	$128, len
 -	jge	.Lfold_128_bytes_loop
 -
 -	# Now fold the 112 bytes in xmm0-xmm6 into the 16 bytes in xmm7.
 -
 -	# Fold across 64 bytes.
 -	movdqa	.Lfold_across_64_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm0, %xmm4
 -	fold_16_bytes	%xmm1, %xmm5
 -	fold_16_bytes	%xmm2, %xmm6
 -	fold_16_bytes	%xmm3, %xmm7
 -	# Fold across 32 bytes.
 -	movdqa	.Lfold_across_32_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm4, %xmm6
 -	fold_16_bytes	%xmm5, %xmm7
 -	# Fold across 16 bytes.
 -	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm6, %xmm7
 -
 -	# Add 128 to get the correct number of data bytes remaining in 0...127
 -	# (not counting xmm7), following the previous extra subtraction by 128.
 -	# Then subtract 16 to simplify the termination condition of the
 -	# following loop.
 -	add	$128-16, len
 -
 -	# While >= 16 data bytes remain (not counting xmm7), fold the 16 bytes
 -	# xmm7 into them, storing the result back into xmm7.
 -	jl	.Lfold_16_bytes_loop_done
 -.Lfold_16_bytes_loop:
 -	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	movdqa	rk11(%rip), %xmm10
 +	movdqa	%xmm1, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm1
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	movdqu	(buf), %xmm0
 -	pshufb	BSWAP_MASK, %xmm0
 -	pxor	%xmm0 , %xmm7
 -	add	$16, buf
 -	sub	$16, len
 -	jge	.Lfold_16_bytes_loop
 +	xorps	%xmm1, %xmm7
  
 -.Lfold_16_bytes_loop_done:
 -	# Add 16 to get the correct number of data bytes remaining in 0...15
 -	# (not counting xmm7), following the previous extra subtraction by 16.
 -	add	$16, len
 -	je	.Lreduce_final_16_bytes
 +	movdqa	rk13(%rip), %xmm10
 +	movdqa	%xmm2, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm2
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm2, %xmm7
 +
 +	movdqa	rk15(%rip), %xmm10
 +	movdqa	%xmm3, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm3
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm3, %xmm7
 +
 +	movdqa	rk17(%rip), %xmm10
 +	movdqa	%xmm4, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm4
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm4, %xmm7
 +
 +	movdqa	rk19(%rip), %xmm10
 +	movdqa	%xmm5, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm5
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm5, %xmm7
 +
 +	movdqa	rk1(%rip), %xmm10	#xmm10 has rk1 and rk2
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +	movdqa	%xmm6, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm6
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm6, %xmm7
 +
 +
 +	# instead of 64, we add 48 to the loop counter to save 1 instruction
 +	# from the loop instead of a cmp instruction, we use the negative
 +	# flag with the jl instruction
 +	add	$128-16, arg3
 +	jl	_final_reduction_for_128
  
 -.Lhandle_partial_segment:
 -	# Reduce the last '16 + len' bytes where 1 <= len <= 15 and the first 16
 -	# bytes are in xmm7 and the rest are the remaining data in 'buf'.  To do
 -	# this without needing a fold constant for each possible 'len', redivide
 -	# the bytes into a first chunk of 'len' bytes and a second chunk of 16
 -	# bytes, then fold the first chunk into the second.
 +	# now we have 16+y bytes left to reduce. 16 Bytes is in register xmm7
 +	# and the rest is in memory. We can fold 16 bytes at a time if y>=16
 +	# continue folding 16B at a time
  
 +_16B_reduction_loop:
 +	movdqa	%xmm7, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	movdqu	(arg2), %xmm0
 +	pshufb	%xmm11, %xmm0
 +	pxor	%xmm0 , %xmm7
 +	add	$16, arg2
 +	sub	$16, arg3
 +	# instead of a cmp instruction, we utilize the flags with the
 +	# jge instruction equivalent of: cmp arg3, 16-16
 +	# check if there is any more 16B in the buffer to be able to fold
 +	jge	_16B_reduction_loop
 +
 +	#now we have 16+z bytes left to reduce, where 0<= z < 16.
 +	#first, we reduce the data in the xmm7 register
 +
 +
 +_final_reduction_for_128:
 +	# check if any more data to fold. If not, compute the CRC of
 +	# the final 128 bits
 +	add	$16, arg3
 +	je	_128_done
 +
 +	# here we are getting data that is less than 16 bytes.
 +	# since we know that there was data before the pointer, we can
 +	# offset the input pointer before the actual point, to receive
 +	# exactly 16 bytes. after that the registers need to be adjusted.
 +_get_last_two_xmms:
  	movdqa	%xmm7, %xmm2
  
 -	# xmm1 = last 16 original data bytes
 -	movdqu	-16(buf, len), %xmm1
 -	pshufb	BSWAP_MASK, %xmm1
 +	movdqu	-16(arg2, arg3), %xmm1
 +	pshufb	%xmm11, %xmm1
  
 -	# xmm2 = high order part of second chunk: xmm7 left-shifted by 'len' bytes.
 -	lea	.Lbyteshift_table+16(%rip), %rax
 -	sub	len, %rax
 +	# get rid of the extra data that was loaded before
 +	# load the shift constant
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	arg3, %rax
  	movdqu	(%rax), %xmm0
 +
 +	# shift xmm2 to the left by arg3 bytes
  	pshufb	%xmm0, %xmm2
  
 -	# xmm7 = first chunk: xmm7 right-shifted by '16-len' bytes.
 -	pxor	.Lmask1(%rip), %xmm0
 +	# shift xmm7 to the right by 16-arg3 bytes
 +	pxor	mask1(%rip), %xmm0
  	pshufb	%xmm0, %xmm7
 -
 -	# xmm1 = second chunk: 'len' bytes from xmm1 (low-order bytes),
 -	# then '16-len' bytes from xmm2 (high-order bytes).
  	pblendvb	%xmm2, %xmm1	#xmm0 is implicit
  
 -	# Fold the first chunk into the second chunk, storing the result in xmm7.
 +	# fold 16 Bytes
 +	movdqa	%xmm1, %xmm2
  	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	pxor	%xmm1, %xmm7
 +	pxor	%xmm2, %xmm7
  
 -.Lreduce_final_16_bytes:
 -	# Reduce the 128-bit value M(x), stored in xmm7, to the final 16-bit CRC
 +_128_done:
 +	# compute crc of a 128-bit value
 +	movdqa	rk5(%rip), %xmm10	# rk5 and rk6 in xmm10
 +	movdqa	%xmm7, %xmm0
  
 -	# Load 'x^48 * (x^48 mod G(x))' and 'x^48 * (x^80 mod G(x))'.
 -	movdqa	.Lfinal_fold_consts(%rip), FOLD_CONSTS
 +	#64b fold
 +	pclmulqdq	$0x1, %xmm10, %xmm7
 +	pslldq	$8   ,  %xmm0
 +	pxor	%xmm0,  %xmm7
  
 -	# Fold the high 64 bits into the low 64 bits, while also multiplying by
 -	# x^64.  This produces a 128-bit value congruent to x^64 * M(x) and
 -	# whose low 48 bits are 0.
 +	#32b fold
  	movdqa	%xmm7, %xmm0
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high bits * x^48 * (x^80 mod G(x))
 -	pslldq	$8, %xmm0
 -	pxor	%xmm0, %xmm7			  # + low bits * x^64
  
 -	# Fold the high 32 bits into the low 96 bits.  This produces a 96-bit
 -	# value congruent to x^64 * M(x) and whose low 48 bits are 0.
 -	movdqa	%xmm7, %xmm0
 -	pand	.Lmask2(%rip), %xmm0		  # zero high 32 bits
 -	psrldq	$12, %xmm7			  # extract high 32 bits
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # high 32 bits * x^48 * (x^48 mod G(x))
 -	pxor	%xmm0, %xmm7			  # + low bits
 +	pand	mask2(%rip), %xmm0
  
 -	# Load G(x) and floor(x^48 / G(x)).
 -	movdqa	.Lbarrett_reduction_consts(%rip), FOLD_CONSTS
 +	psrldq	$12, %xmm7
 +	pclmulqdq	$0x10, %xmm10, %xmm7
 +	pxor	%xmm0, %xmm7
  
 -	# Use Barrett reduction to compute the final CRC value.
++<<<<<<< HEAD
 +	#barrett reduction
 +_barrett:
 +	movdqa	rk7(%rip), %xmm10	# rk7 and rk8 in xmm10
  	movdqa	%xmm7, %xmm0
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high 32 bits * floor(x^48 / G(x))
 -	psrlq	$32, %xmm7			  # /= x^32
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # *= G(x)
 -	psrlq	$48, %xmm0
 -	pxor	%xmm7, %xmm0		     # + low 16 nonzero bits
 -	# Final CRC value (x^16 * M(x)) mod G(x) is in low 16 bits of xmm0.
 -
 -	pextrw	$0, %xmm0, %eax
 +	pclmulqdq	$0x01, %xmm10, %xmm7
 +	pslldq	$4, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +
 +	pslldq	$4, %xmm7
 +	pxor	%xmm0, %xmm7
 +	pextrd	$1, %xmm7, %eax
 +
 +_cleanup:
 +	# scale the result back to 16 bits
 +	shr	$16, %eax
 +	mov     %rcx, %rsp
  	ret
  
 +########################################################################
 +
  .align 16
 -.Lless_than_256_bytes:
 -	# Checksumming a buffer of length 16...255 bytes
 +_less_than_128:
  
 -	# Load the first 16 data bytes.
 -	movdqu	(buf), %xmm7
 -	pshufb	BSWAP_MASK, %xmm7
 -	add	$16, buf
 +	# check if there is enough buffer to be able to fold 16B at a time
 +	cmp	$32, arg3
 +	jl	_less_than_32
 +	movdqa  SHUF_MASK(%rip), %xmm11
  
 -	# XOR the first 16 data *bits* with the initial CRC value.
 -	pxor	%xmm0, %xmm0
 -	pinsrw	$7, init_crc, %xmm0
 +	# now if there is, load the constants
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
  	pxor	%xmm0, %xmm7
  
 +
 +	# update the buffer pointer
 +	add	$16, arg2
 +
 +	# update the counter. subtract 32 instead of 16 to save one
 +	# instruction from the loop
 +	sub	$32, arg3
 +
 +	jmp	_16B_reduction_loop
 +
 +
 +.align 16
 +_less_than_32:
 +	# mov initial crc to the return value. this is necessary for
 +	# zero-length buffers.
 +	mov	arg1_low32, %eax
 +	test	arg3, arg3
 +	je	_cleanup
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +
 +	cmp	$16, arg3
 +	je	_exact_16_left
 +	jl	_less_than_16_left
 +
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +	add	$16, arg2
 +	sub	$16, arg3
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +	jmp	_get_last_two_xmms
 +
 +
 +.align 16
 +_less_than_16_left:
 +	# use stack space to load data less than 16 bytes, zero-out
 +	# the 16B in memory first.
 +
 +	pxor	%xmm1, %xmm1
 +	mov	%rsp, %r11
 +	movdqa	%xmm1, (%r11)
 +
 +	cmp	$4, arg3
 +	jl	_only_less_than_4
 +
 +	# backup the counter value
 +	mov	arg3, %r9
 +	cmp	$8, arg3
 +	jl	_less_than_8_left
 +
 +	# load 8 Bytes
 +	mov	(arg2), %rax
 +	mov	%rax, (%r11)
 +	add	$8, %r11
 +	sub	$8, arg3
 +	add	$8, arg2
 +_less_than_8_left:
 +
 +	cmp	$4, arg3
 +	jl	_less_than_4_left
 +
 +	# load 4 Bytes
 +	mov	(arg2), %eax
 +	mov	%eax, (%r11)
 +	add	$4, %r11
 +	sub	$4, arg3
 +	add	$4, arg2
 +_less_than_4_left:
 +
 +	cmp	$2, arg3
 +	jl	_less_than_2_left
 +
 +	# load 2 Bytes
 +	mov	(arg2), %ax
 +	mov	%ax, (%r11)
 +	add	$2, %r11
 +	sub	$2, arg3
 +	add	$2, arg2
 +_less_than_2_left:
 +	cmp     $1, arg3
 +        jl      _zero_left
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +_zero_left:
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +
 +	# shl r9, 4
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	%r9, %rax
 +	movdqu	(%rax), %xmm0
 +	pxor	mask1(%rip), %xmm0
 +
 +	pshufb	%xmm0, %xmm7
 +	jmp	_128_done
 +
 +.align 16
 +_exact_16_left:
 +	movdqu	(arg2), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	jmp	_128_done
 +
 +_only_less_than_4:
 +	cmp	$3, arg3
 +	jl	_only_less_than_3
 +
 +	# load 3 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	mov	2(arg2), %al
 +	mov	%al, 2(%r11)
 +
 +	movdqa	 (%rsp), %xmm7
 +	pshufb	 %xmm11, %xmm7
 +	pxor	 %xmm0 , %xmm7  # xor the initial crc value
 +
 +	psrldq	$5, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_3:
 +	cmp	$2, arg3
 +	jl	_only_less_than_2
 +
 +	# load 2 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$6, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_2:
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$7, %xmm7
 +
 +	jmp	_barrett
 +
 +ENDPROC(crc_t10dif_pcl)
++=======
+ 	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
+ 	cmp	$16, len
+ 	je	.Lreduce_final_16_bytes		# len == 16
+ 	sub	$32, len
+ 	jge	.Lfold_16_bytes_loop		# 32 <= len <= 255
+ 	add	$16, len
+ 	jmp	.Lhandle_partial_segment	# 17 <= len <= 31
+ SYM_FUNC_END(crc_t10dif_pcl)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  
  .section	.rodata, "a", @progbits
  .align 16
diff --cc arch/x86/kernel/acpi/wakeup_64.S
index 510fa12aab73,c8daa92f38dc..000000000000
--- a/arch/x86/kernel/acpi/wakeup_64.S
+++ b/arch/x86/kernel/acpi/wakeup_64.S
@@@ -34,12 -40,9 +34,16 @@@ SYM_FUNC_START(wakeup_long64
  
  	movq	saved_rip, %rax
  	jmp	*%rax
- ENDPROC(wakeup_long64)
+ SYM_FUNC_END(wakeup_long64)
  
++<<<<<<< HEAD
 +bogus_64_magic:
 +	jmp	bogus_64_magic
 +
 +ENTRY(do_suspend_lowlevel)
++=======
+ SYM_FUNC_START(do_suspend_lowlevel)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  	FRAME_BEGIN
  	subq	$8, %rsp
  	xorl	%eax, %eax
diff --cc arch/x86/kvm/vmx/vmenter.S
index e2c725edcf3c,81ada2ce99e7..000000000000
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@@ -59,10 -58,14 +59,10 @@@ SYM_FUNC_START(vmx_vmenter
  	ret
  4:	ud2
  
 -	.pushsection .fixup, "ax"
 -5:	jmp 3b
 -	.popsection
 -
 -	_ASM_EXTABLE(1b, 5b)
 -	_ASM_EXTABLE(2b, 5b)
 +	_ASM_EXTABLE(1b, 3b)
 +	_ASM_EXTABLE(2b, 3b)
  
- ENDPROC(vmx_vmenter)
+ SYM_FUNC_END(vmx_vmenter)
  
  /**
   * vmx_vmexit - Handle a VMX VM-Exit
@@@ -233,95 -233,4 +233,99 @@@ SYM_FUNC_START(__vmx_vcpu_run
  	/* VM-Fail.  Out-of-line to avoid a taken Jcc after VM-Exit. */
  2:	mov $1, %eax
  	jmp 1b
++<<<<<<< HEAD
 +ENDPROC(__vmx_vcpu_run)
 +
 +/**
 + * vmread_error_trampoline - Trampoline from inline asm to vmread_error()
 + * @field:	VMCS field encoding that failed
 + * @fault:	%true if the VMREAD faulted, %false if it failed
 +
 + * Save and restore volatile registers across a call to vmread_error().  Note,
 + * all parameters are passed on the stack.
 + */
 +ENTRY(vmread_error_trampoline)
 +	push %_ASM_BP
 +	mov  %_ASM_SP, %_ASM_BP
 +
 +	push %_ASM_AX
 +	push %_ASM_CX
 +	push %_ASM_DX
 +#ifdef CONFIG_X86_64
 +	push %rdi
 +	push %rsi
 +	push %r8
 +	push %r9
 +	push %r10
 +	push %r11
 +#endif
 +#ifdef CONFIG_X86_64
 +	/* Load @field and @fault to arg1 and arg2 respectively. */
 +	mov 3*WORD_SIZE(%rbp), %_ASM_ARG2
 +	mov 2*WORD_SIZE(%rbp), %_ASM_ARG1
 +#else
 +	/* Parameters are passed on the stack for 32-bit (see asmlinkage). */
 +	push 3*WORD_SIZE(%ebp)
 +	push 2*WORD_SIZE(%ebp)
 +#endif
 +
 +	call vmread_error
 +
 +#ifndef CONFIG_X86_64
 +	add $8, %esp
 +#endif
 +
 +	/* Zero out @fault, which will be popped into the result register. */
 +	_ASM_MOV $0, 3*WORD_SIZE(%_ASM_BP)
 +
 +#ifdef CONFIG_X86_64
 +	pop %r11
 +	pop %r10
 +	pop %r9
 +	pop %r8
 +	pop %rsi
 +	pop %rdi
 +#endif
 +	pop %_ASM_DX
 +	pop %_ASM_CX
 +	pop %_ASM_AX
 +	pop %_ASM_BP
 +
 +	ret
 +ENDPROC(vmread_error_trampoline)
 +
 +ENTRY(vmx_do_interrupt_nmi_irqoff)
 +	/*
 +	 * Unconditionally create a stack frame, getting the correct RSP on the
 +	 * stack (for x86-64) would take two instructions anyways, and RBP can
 +	 * be used to restore RSP to make objtool happy (see below).
 +	 */
 +	push %_ASM_BP
 +	mov %_ASM_SP, %_ASM_BP
 +
 +#ifdef CONFIG_X86_64
 +	/*
 +	 * Align RSP to a 16-byte boundary (to emulate CPU behavior) before
 +	 * creating the synthetic interrupt stack frame for the IRQ/NMI.
 +	 */
 +	and  $-16, %rsp
 +	push $__KERNEL_DS
 +	push %rbp
 +#endif
 +	pushf
 +	push $__KERNEL_CS
 +	CALL_NOSPEC %_ASM_ARG1
 +
 +	/*
 +	 * "Restore" RSP from RBP, even though IRET has already unwound RSP to
 +	 * the correct value.  objtool doesn't know the callee will IRET and,
 +	 * without the explicit restore, thinks the stack is getting walloped.
 +	 * Using an unwind hint is problematic due to x86-64's dynamic alignment.
 +	 */
 +	mov %_ASM_BP, %_ASM_SP
 +	pop %_ASM_BP
 +	ret
 +ENDPROC(vmx_do_interrupt_nmi_irqoff)
++=======
+ SYM_FUNC_END(__vmx_vcpu_run)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
diff --cc arch/x86/lib/memmove_64.S
index cf84a0f38bd1,337830d7a59c..000000000000
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@@ -26,10 -26,13 +26,15 @@@
   */
  .weak memmove
  
++<<<<<<< HEAD
 +ENTRY(memmove)
 +ENTRY(__memmove)
++=======
+ SYM_FUNC_START_ALIAS(memmove)
+ SYM_FUNC_START(__memmove)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  
 -	/* Handle more 32 bytes in loop */
  	mov %rdi, %rax
 -	cmp $0x20, %rdx
 -	jb	1f
  
  	/* Decide forward/backward copy mode */
  	cmp %rdi, %rsi
@@@ -208,7 -207,7 +213,12 @@@
  	movb %r11b, (%rdi)
  13:
  	retq
++<<<<<<< HEAD
 +ENDPROC(__memmove)
 +ENDPROC(memmove)
++=======
+ SYM_FUNC_END(__memmove)
+ SYM_FUNC_END_ALIAS(memmove)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  EXPORT_SYMBOL(__memmove)
  EXPORT_SYMBOL(memmove)
diff --cc arch/x86/lib/memset_64.S
index 9bc861c71e75,9ff15ee404a4..000000000000
--- a/arch/x86/lib/memset_64.S
+++ b/arch/x86/lib/memset_64.S
@@@ -19,8 -19,8 +19,13 @@@
   *
   * rax   original destination
   */
++<<<<<<< HEAD
 +ENTRY(memset)
 +ENTRY(__memset)
++=======
+ SYM_FUNC_START_ALIAS(memset)
+ SYM_FUNC_START(__memset)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  	/*
  	 * Some CPUs support enhanced REP MOVSB/STOSB feature. It is recommended
  	 * to use it when possible. If not available, use fast string instructions.
@@@ -43,8 -43,8 +48,13 @@@
  	rep stosb
  	movq %r9,%rax
  	ret
++<<<<<<< HEAD
 +ENDPROC(memset)
 +ENDPROC(__memset)
++=======
+ SYM_FUNC_END(__memset)
+ SYM_FUNC_END_ALIAS(memset)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  EXPORT_SYMBOL(memset)
  EXPORT_SYMBOL(__memset)
  
diff --cc arch/x86/platform/efi/efi_thunk_64.S
index f38302f3452f,3189f1394701..000000000000
--- a/arch/x86/platform/efi/efi_thunk_64.S
+++ b/arch/x86/platform/efi/efi_thunk_64.S
@@@ -42,18 -41,51 +42,55 @@@ SYM_FUNC_START(efi64_thunk
  	movq	$__START_KERNEL_map, %rax
  	subq	phys_base(%rip), %rax
  
 -	/*
 -	 * Push some physical addresses onto the stack. This is easier
 -	 * to do now in a code64 section while the assembler can address
 -	 * 64-bit values. Note that all the addresses on the stack are
 -	 * 32-bit.
 -	 */
 -	subq	$16, %rsp
 -	leaq	efi_exit32(%rip), %rbx
 +	leaq	1f(%rip), %rbp
 +	leaq	2f(%rip), %rbx
 +	subq	%rax, %rbp
  	subq	%rax, %rbx
 -	movl	%ebx, 8(%rsp)
  
++<<<<<<< HEAD
 +	subq	$28, %rsp
 +	movl	%ebx, 0x0(%rsp)		/* return address */
 +	movl	%esi, 0x4(%rsp)
 +	movl	%edx, 0x8(%rsp)
 +	movl	%ecx, 0xc(%rsp)
 +	movl	%r8d, 0x10(%rsp)
 +	movl	%r9d, 0x14(%rsp)
++=======
+ 	leaq	__efi64_thunk(%rip), %rbx
+ 	subq	%rax, %rbx
+ 	call	*%rbx
+ 
+ 	movq	efi_saved_sp(%rip), %rsp
+ 	pop	%rbx
+ 	pop	%rbp
+ 	retq
+ SYM_FUNC_END(efi64_thunk)
+ 
+ /*
+  * We run this function from the 1:1 mapping.
+  *
+  * This function must be invoked with a 1:1 mapped stack.
+  */
+ SYM_FUNC_START_LOCAL(__efi64_thunk)
+ 	movl	%ds, %eax
+ 	push	%rax
+ 	movl	%es, %eax
+ 	push	%rax
+ 	movl	%ss, %eax
+ 	push	%rax
+ 
+ 	subq	$32, %rsp
+ 	movl	%esi, 0x0(%rsp)
+ 	movl	%edx, 0x4(%rsp)
+ 	movl	%ecx, 0x8(%rsp)
+ 	movq	%r8, %rsi
+ 	movl	%esi, 0xc(%rsp)
+ 	movq	%r9, %rsi
+ 	movl	%esi,  0x10(%rsp)
+ 
+ 	leaq	1f(%rip), %rbx
+ 	movq	%rbx, func_rt_ptr(%rip)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  
  	/* Switch to 32-bit descriptor */
  	pushq	$__KERNEL32_CS
diff --cc arch/x86/xen/xen-asm.S
index 8019edd0125c,508fe204520b..000000000000
--- a/arch/x86/xen/xen-asm.S
+++ b/arch/x86/xen/xen-asm.S
@@@ -134,4 -135,19 +134,23 @@@ SYM_FUNC_START(check_events
  #endif
  	FRAME_END
  	ret
++<<<<<<< HEAD
 +ENDPROC(check_events)
++=======
+ SYM_FUNC_END(check_events)
+ 
+ SYM_FUNC_START(xen_read_cr2)
+ 	FRAME_BEGIN
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu), %_ASM_AX
+ 	_ASM_MOV XEN_vcpu_info_arch_cr2(%_ASM_AX), %_ASM_AX
+ 	FRAME_END
+ 	ret
+ SYM_FUNC_END(xen_read_cr2);
+ 
+ SYM_FUNC_START(xen_read_cr2_direct)
+ 	FRAME_BEGIN
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_arch_cr2, %_ASM_AX
+ 	FRAME_END
+ 	ret
+ SYM_FUNC_END(xen_read_cr2_direct);
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
diff --cc arch/x86/xen/xen-asm_64.S
index 1e9ef0ba30a5,0a0fd168683a..000000000000
--- a/arch/x86/xen/xen-asm_64.S
+++ b/arch/x86/xen/xen-asm_64.S
@@@ -168,13 -169,13 +168,23 @@@ SYM_FUNC_END(xen_sysenter_target
  
  #else /* !CONFIG_IA32_EMULATION */
  
++<<<<<<< HEAD
 +ENTRY(xen_syscall32_target)
 +ENTRY(xen_sysenter_target)
++=======
+ SYM_FUNC_START_ALIAS(xen_syscall32_target)
+ SYM_FUNC_START(xen_sysenter_target)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  	lea 16(%rsp), %rsp	/* strip %rcx, %r11 */
  	mov $-ENOSYS, %rax
  	pushq $0
  	jmp hypercall_iret
++<<<<<<< HEAD
 +ENDPROC(xen_syscall32_target)
 +ENDPROC(xen_sysenter_target)
++=======
+ SYM_FUNC_END(xen_sysenter_target)
+ SYM_FUNC_END_ALIAS(xen_syscall32_target)
++>>>>>>> 6dcc5627f6ae (x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*)
  
  #endif	/* CONFIG_IA32_EMULATION */
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
diff --git a/arch/x86/boot/compressed/efi_thunk_64.S b/arch/x86/boot/compressed/efi_thunk_64.S
index bff9ab7c6317..5af41b130925 100644
--- a/arch/x86/boot/compressed/efi_thunk_64.S
+++ b/arch/x86/boot/compressed/efi_thunk_64.S
@@ -23,7 +23,7 @@
 
 	.code64
 	.text
-ENTRY(efi64_thunk)
+SYM_FUNC_START(efi64_thunk)
 	push	%rbp
 	push	%rbx
 
@@ -97,7 +97,7 @@ ENTRY(efi64_thunk)
 	pop	%rbx
 	pop	%rbp
 	ret
-ENDPROC(efi64_thunk)
+SYM_FUNC_END(efi64_thunk)
 
 ENTRY(efi_exit32)
 	movq	func_rt_ptr(%rip), %rax
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index c0dbdcfcb3bc..b099220a9ebf 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -45,7 +45,7 @@
 
 	__HEAD
 	.code32
-ENTRY(startup_32)
+SYM_FUNC_START(startup_32)
 	/*
 	 * 32bit entry is 0 and it is ABI so immutable!
 	 * If we come here directly from a bootloader,
@@ -222,11 +222,11 @@ ENTRY(startup_32)
 
 	/* Jump from 32bit compatibility mode into 64bit mode. */
 	lret
-ENDPROC(startup_32)
+SYM_FUNC_END(startup_32)
 
 #ifdef CONFIG_EFI_MIXED
 	.org 0x190
-ENTRY(efi32_stub_entry)
+SYM_FUNC_START(efi32_stub_entry)
 	add	$0x4, %esp		/* Discard return address */
 	popl	%ecx
 	popl	%edx
@@ -245,7 +245,7 @@ ENTRY(efi32_stub_entry)
 	movl	%eax, efi_config(%ebp)
 
 	jmp	startup_32
-ENDPROC(efi32_stub_entry)
+SYM_FUNC_END(efi32_stub_entry)
 #endif
 
 	.code64
@@ -431,7 +431,7 @@ trampoline_return:
 #ifdef CONFIG_EFI_STUB
 
 /* The entry point for the PE/COFF executable is efi_pe_entry. */
-ENTRY(efi_pe_entry)
+SYM_FUNC_START(efi_pe_entry)
 	movq	%rcx, efi64_config(%rip)	/* Handle */
 	movq	%rdx, efi64_config+8(%rip) /* EFI System table pointer */
 
@@ -480,10 +480,10 @@ fail:
 	movl	BP_code32_start(%esi), %eax
 	leaq	startup_64(%rax), %rax
 	jmp	*%rax
-ENDPROC(efi_pe_entry)
+SYM_FUNC_END(efi_pe_entry)
 
 	.org 0x390
-ENTRY(efi64_stub_entry)
+SYM_FUNC_START(efi64_stub_entry)
 	movq	%rdi, efi64_config(%rip)	/* Handle */
 	movq	%rsi, efi64_config+8(%rip) /* EFI System table pointer */
 
@@ -492,7 +492,7 @@ ENTRY(efi64_stub_entry)
 
 	movq	%rdx, %rsi
 	jmp	handover_entry
-ENDPROC(efi64_stub_entry)
+SYM_FUNC_END(efi64_stub_entry)
 #endif
 
 	.text
* Unmerged path arch/x86/boot/compressed/mem_encrypt.S
diff --git a/arch/x86/crypto/aegis128-aesni-asm.S b/arch/x86/crypto/aegis128-aesni-asm.S
index 5f7e43d4f64a..47ef1254d27e 100644
--- a/arch/x86/crypto/aegis128-aesni-asm.S
+++ b/arch/x86/crypto/aegis128-aesni-asm.S
@@ -189,7 +189,7 @@ ENDPROC(__store_partial)
 /*
  * void crypto_aegis128_aesni_init(void *state, const void *key, const void *iv);
  */
-ENTRY(crypto_aegis128_aesni_init)
+SYM_FUNC_START(crypto_aegis128_aesni_init)
 	FRAME_BEGIN
 
 	/* load IV: */
@@ -229,13 +229,13 @@ ENTRY(crypto_aegis128_aesni_init)
 
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_init)
+SYM_FUNC_END(crypto_aegis128_aesni_init)
 
 /*
  * void crypto_aegis128_aesni_ad(void *state, unsigned int length,
  *                               const void *data);
  */
-ENTRY(crypto_aegis128_aesni_ad)
+SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	FRAME_BEGIN
 
 	cmp $0x10, LEN
@@ -381,7 +381,7 @@ ENTRY(crypto_aegis128_aesni_ad)
 .Lad_out:
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_ad)
+SYM_FUNC_END(crypto_aegis128_aesni_ad)
 
 .macro encrypt_block a s0 s1 s2 s3 s4 i
 	movdq\a (\i * 0x10)(SRC), MSG
@@ -405,7 +405,7 @@ ENDPROC(crypto_aegis128_aesni_ad)
  * void crypto_aegis128_aesni_enc(void *state, unsigned int length,
  *                                const void *src, void *dst);
  */
-ENTRY(crypto_aegis128_aesni_enc)
+SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	FRAME_BEGIN
 
 	cmp $0x10, LEN
@@ -496,13 +496,13 @@ ENTRY(crypto_aegis128_aesni_enc)
 .Lenc_out:
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_enc)
+SYM_FUNC_END(crypto_aegis128_aesni_enc)
 
 /*
  * void crypto_aegis128_aesni_enc_tail(void *state, unsigned int length,
  *                                     const void *src, void *dst);
  */
-ENTRY(crypto_aegis128_aesni_enc_tail)
+SYM_FUNC_START(crypto_aegis128_aesni_enc_tail)
 	FRAME_BEGIN
 
 	/* load the state: */
@@ -536,7 +536,7 @@ ENTRY(crypto_aegis128_aesni_enc_tail)
 
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_enc_tail)
+SYM_FUNC_END(crypto_aegis128_aesni_enc_tail)
 
 .macro decrypt_block a s0 s1 s2 s3 s4 i
 	movdq\a (\i * 0x10)(SRC), MSG
@@ -559,7 +559,7 @@ ENDPROC(crypto_aegis128_aesni_enc_tail)
  * void crypto_aegis128_aesni_dec(void *state, unsigned int length,
  *                                const void *src, void *dst);
  */
-ENTRY(crypto_aegis128_aesni_dec)
+SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	FRAME_BEGIN
 
 	cmp $0x10, LEN
@@ -650,13 +650,13 @@ ENTRY(crypto_aegis128_aesni_dec)
 .Ldec_out:
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_dec)
+SYM_FUNC_END(crypto_aegis128_aesni_dec)
 
 /*
  * void crypto_aegis128_aesni_dec_tail(void *state, unsigned int length,
  *                                     const void *src, void *dst);
  */
-ENTRY(crypto_aegis128_aesni_dec_tail)
+SYM_FUNC_START(crypto_aegis128_aesni_dec_tail)
 	FRAME_BEGIN
 
 	/* load the state: */
@@ -700,13 +700,13 @@ ENTRY(crypto_aegis128_aesni_dec_tail)
 
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_dec_tail)
+SYM_FUNC_END(crypto_aegis128_aesni_dec_tail)
 
 /*
  * void crypto_aegis128_aesni_final(void *state, void *tag_xor,
  *                                  u64 assoclen, u64 cryptlen);
  */
-ENTRY(crypto_aegis128_aesni_final)
+SYM_FUNC_START(crypto_aegis128_aesni_final)
 	FRAME_BEGIN
 
 	/* load the state: */
@@ -747,4 +747,4 @@ ENTRY(crypto_aegis128_aesni_final)
 
 	FRAME_END
 	ret
-ENDPROC(crypto_aegis128_aesni_final)
+SYM_FUNC_END(crypto_aegis128_aesni_final)
diff --git a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
index 5f6a5af9c489..ec437db1fa54 100644
--- a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
+++ b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
@@ -544,11 +544,11 @@ ddq_add_8:
  * aes_ctr_enc_128_avx_by8(void *in, void *iv, void *keys, void *out,
  *			unsigned int num_bytes)
  */
-ENTRY(aes_ctr_enc_128_avx_by8)
+SYM_FUNC_START(aes_ctr_enc_128_avx_by8)
 	/* call the aes main loop */
 	do_aes_ctrmain KEY_128
 
-ENDPROC(aes_ctr_enc_128_avx_by8)
+SYM_FUNC_END(aes_ctr_enc_128_avx_by8)
 
 /*
  * routine to do AES192 CTR enc/decrypt "by8"
@@ -557,11 +557,11 @@ ENDPROC(aes_ctr_enc_128_avx_by8)
  * aes_ctr_enc_192_avx_by8(void *in, void *iv, void *keys, void *out,
  *			unsigned int num_bytes)
  */
-ENTRY(aes_ctr_enc_192_avx_by8)
+SYM_FUNC_START(aes_ctr_enc_192_avx_by8)
 	/* call the aes main loop */
 	do_aes_ctrmain KEY_192
 
-ENDPROC(aes_ctr_enc_192_avx_by8)
+SYM_FUNC_END(aes_ctr_enc_192_avx_by8)
 
 /*
  * routine to do AES256 CTR enc/decrypt "by8"
@@ -570,8 +570,8 @@ ENDPROC(aes_ctr_enc_192_avx_by8)
  * aes_ctr_enc_256_avx_by8(void *in, void *iv, void *keys, void *out,
  *			unsigned int num_bytes)
  */
-ENTRY(aes_ctr_enc_256_avx_by8)
+SYM_FUNC_START(aes_ctr_enc_256_avx_by8)
 	/* call the aes main loop */
 	do_aes_ctrmain KEY_256
 
-ENDPROC(aes_ctr_enc_256_avx_by8)
+SYM_FUNC_END(aes_ctr_enc_256_avx_by8)
diff --git a/arch/x86/crypto/aesni-intel_asm.S b/arch/x86/crypto/aesni-intel_asm.S
index 9bd139569b41..5b708b4226fd 100644
--- a/arch/x86/crypto/aesni-intel_asm.S
+++ b/arch/x86/crypto/aesni-intel_asm.S
@@ -1596,7 +1596,7 @@ _esb_loop_\@:
 * poly = x^128 + x^127 + x^126 + x^121 + 1
 *
 *****************************************************************************/
-ENTRY(aesni_gcm_dec)
+SYM_FUNC_START(aesni_gcm_dec)
 	FUNC_SAVE
 
 	GCM_INIT %arg6, arg7, arg8, arg9
@@ -1604,7 +1604,7 @@ ENTRY(aesni_gcm_dec)
 	GCM_COMPLETE arg10, arg11
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_dec)
+SYM_FUNC_END(aesni_gcm_dec)
 
 
 /*****************************************************************************
@@ -1684,7 +1684,7 @@ ENDPROC(aesni_gcm_dec)
 *
 * poly = x^128 + x^127 + x^126 + x^121 + 1
 ***************************************************************************/
-ENTRY(aesni_gcm_enc)
+SYM_FUNC_START(aesni_gcm_enc)
 	FUNC_SAVE
 
 	GCM_INIT %arg6, arg7, arg8, arg9
@@ -1693,7 +1693,7 @@ ENTRY(aesni_gcm_enc)
 	GCM_COMPLETE arg10, arg11
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_enc)
+SYM_FUNC_END(aesni_gcm_enc)
 
 /*****************************************************************************
 * void aesni_gcm_init(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
@@ -1706,12 +1706,12 @@ ENDPROC(aesni_gcm_enc)
 *                     const u8 *aad,      // Additional Authentication Data (AAD)
 *                     u64 aad_len)        // Length of AAD in bytes.
 */
-ENTRY(aesni_gcm_init)
+SYM_FUNC_START(aesni_gcm_init)
 	FUNC_SAVE
 	GCM_INIT %arg3, %arg4,%arg5, %arg6
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_init)
+SYM_FUNC_END(aesni_gcm_init)
 
 /*****************************************************************************
 * void aesni_gcm_enc_update(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
@@ -1721,12 +1721,12 @@ ENDPROC(aesni_gcm_init)
 *                    const u8 *in,       // Plaintext input
 *                    u64 plaintext_len,  // Length of data in bytes for encryption.
 */
-ENTRY(aesni_gcm_enc_update)
+SYM_FUNC_START(aesni_gcm_enc_update)
 	FUNC_SAVE
 	GCM_ENC_DEC enc
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_enc_update)
+SYM_FUNC_END(aesni_gcm_enc_update)
 
 /*****************************************************************************
 * void aesni_gcm_dec_update(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
@@ -1736,12 +1736,12 @@ ENDPROC(aesni_gcm_enc_update)
 *                    const u8 *in,       // Plaintext input
 *                    u64 plaintext_len,  // Length of data in bytes for encryption.
 */
-ENTRY(aesni_gcm_dec_update)
+SYM_FUNC_START(aesni_gcm_dec_update)
 	FUNC_SAVE
 	GCM_ENC_DEC dec
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_dec_update)
+SYM_FUNC_END(aesni_gcm_dec_update)
 
 /*****************************************************************************
 * void aesni_gcm_finalize(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
@@ -1751,12 +1751,12 @@ ENDPROC(aesni_gcm_dec_update)
 *                    u64 auth_tag_len);  // Authenticated Tag Length in bytes. Valid values are 16 (most likely),
 *                                        // 12 or 8.
 */
-ENTRY(aesni_gcm_finalize)
+SYM_FUNC_START(aesni_gcm_finalize)
 	FUNC_SAVE
 	GCM_COMPLETE %arg3 %arg4
 	FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_finalize)
+SYM_FUNC_END(aesni_gcm_finalize)
 
 #endif
 
@@ -1838,7 +1838,7 @@ ENDPROC(_key_expansion_256b)
  * int aesni_set_key(struct crypto_aes_ctx *ctx, const u8 *in_key,
  *                   unsigned int key_len)
  */
-ENTRY(aesni_set_key)
+SYM_FUNC_START(aesni_set_key)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl KEYP
@@ -1947,12 +1947,12 @@ ENTRY(aesni_set_key)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_set_key)
+SYM_FUNC_END(aesni_set_key)
 
 /*
  * void aesni_enc(struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
  */
-ENTRY(aesni_enc)
+SYM_FUNC_START(aesni_enc)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl KEYP
@@ -1971,7 +1971,7 @@ ENTRY(aesni_enc)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_enc)
+SYM_FUNC_END(aesni_enc)
 
 /*
  * _aesni_enc1:		internal ABI
@@ -2143,7 +2143,7 @@ ENDPROC(_aesni_enc4)
 /*
  * void aesni_dec (struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
  */
-ENTRY(aesni_dec)
+SYM_FUNC_START(aesni_dec)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl KEYP
@@ -2163,7 +2163,7 @@ ENTRY(aesni_dec)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_dec)
+SYM_FUNC_END(aesni_dec)
 
 /*
  * _aesni_dec1:		internal ABI
@@ -2336,7 +2336,7 @@ ENDPROC(_aesni_dec4)
  * void aesni_ecb_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len)
  */
-ENTRY(aesni_ecb_enc)
+SYM_FUNC_START(aesni_ecb_enc)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl LEN
@@ -2390,13 +2390,13 @@ ENTRY(aesni_ecb_enc)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_ecb_enc)
+SYM_FUNC_END(aesni_ecb_enc)
 
 /*
  * void aesni_ecb_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len);
  */
-ENTRY(aesni_ecb_dec)
+SYM_FUNC_START(aesni_ecb_dec)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl LEN
@@ -2451,13 +2451,13 @@ ENTRY(aesni_ecb_dec)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_ecb_dec)
+SYM_FUNC_END(aesni_ecb_dec)
 
 /*
  * void aesni_cbc_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len, u8 *iv)
  */
-ENTRY(aesni_cbc_enc)
+SYM_FUNC_START(aesni_cbc_enc)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl IVP
@@ -2495,13 +2495,13 @@ ENTRY(aesni_cbc_enc)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_cbc_enc)
+SYM_FUNC_END(aesni_cbc_enc)
 
 /*
  * void aesni_cbc_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len, u8 *iv)
  */
-ENTRY(aesni_cbc_dec)
+SYM_FUNC_START(aesni_cbc_dec)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl IVP
@@ -2588,7 +2588,7 @@ ENTRY(aesni_cbc_dec)
 #endif
 	FRAME_END
 	ret
-ENDPROC(aesni_cbc_dec)
+SYM_FUNC_END(aesni_cbc_dec)
 
 #ifdef __x86_64__
 .pushsection .rodata
@@ -2652,7 +2652,7 @@ ENDPROC(_aesni_inc)
  * void aesni_ctr_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len, u8 *iv)
  */
-ENTRY(aesni_ctr_enc)
+SYM_FUNC_START(aesni_ctr_enc)
 	FRAME_BEGIN
 	cmp $16, LEN
 	jb .Lctr_enc_just_ret
@@ -2709,7 +2709,7 @@ ENTRY(aesni_ctr_enc)
 .Lctr_enc_just_ret:
 	FRAME_END
 	ret
-ENDPROC(aesni_ctr_enc)
+SYM_FUNC_END(aesni_ctr_enc)
 
 /*
  * _aesni_gf128mul_x_ble:		internal ABI
@@ -2733,7 +2733,7 @@ ENDPROC(aesni_ctr_enc)
  * void aesni_xts_crypt8(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *			 bool enc, u8 *iv)
  */
-ENTRY(aesni_xts_crypt8)
+SYM_FUNC_START(aesni_xts_crypt8)
 	FRAME_BEGIN
 	cmpb $0, %cl
 	movl $0, %ecx
@@ -2837,6 +2837,6 @@ ENTRY(aesni_xts_crypt8)
 
 	FRAME_END
 	ret
-ENDPROC(aesni_xts_crypt8)
+SYM_FUNC_END(aesni_xts_crypt8)
 
 #endif
diff --git a/arch/x86/crypto/aesni-intel_avx-x86_64.S b/arch/x86/crypto/aesni-intel_avx-x86_64.S
index 91c039ab5699..bfa1c0b3e5b4 100644
--- a/arch/x86/crypto/aesni-intel_avx-x86_64.S
+++ b/arch/x86/crypto/aesni-intel_avx-x86_64.S
@@ -1775,12 +1775,12 @@ _initial_blocks_done\@:
 #        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 #        u64     aad_len) /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
 #############################################################
-ENTRY(aesni_gcm_init_avx_gen2)
+SYM_FUNC_START(aesni_gcm_init_avx_gen2)
         FUNC_SAVE
         INIT GHASH_MUL_AVX, PRECOMPUTE_AVX
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_init_avx_gen2)
+SYM_FUNC_END(aesni_gcm_init_avx_gen2)
 
 ###############################################################################
 #void   aesni_gcm_enc_update_avx_gen2(
@@ -1790,7 +1790,7 @@ ENDPROC(aesni_gcm_init_avx_gen2)
 #        const   u8 *in, /* Plaintext input */
 #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
 ###############################################################################
-ENTRY(aesni_gcm_enc_update_avx_gen2)
+SYM_FUNC_START(aesni_gcm_enc_update_avx_gen2)
         FUNC_SAVE
         mov     keysize, %eax
         cmp     $32, %eax
@@ -1809,7 +1809,7 @@ key_256_enc_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 13
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_enc_update_avx_gen2)
+SYM_FUNC_END(aesni_gcm_enc_update_avx_gen2)
 
 ###############################################################################
 #void   aesni_gcm_dec_update_avx_gen2(
@@ -1819,7 +1819,7 @@ ENDPROC(aesni_gcm_enc_update_avx_gen2)
 #        const   u8 *in, /* Ciphertext input */
 #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
 ###############################################################################
-ENTRY(aesni_gcm_dec_update_avx_gen2)
+SYM_FUNC_START(aesni_gcm_dec_update_avx_gen2)
         FUNC_SAVE
         mov     keysize,%eax
         cmp     $32, %eax
@@ -1838,7 +1838,7 @@ key_256_dec_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 13
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_dec_update_avx_gen2)
+SYM_FUNC_END(aesni_gcm_dec_update_avx_gen2)
 
 ###############################################################################
 #void   aesni_gcm_finalize_avx_gen2(
@@ -1848,7 +1848,7 @@ ENDPROC(aesni_gcm_dec_update_avx_gen2)
 #        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
 #				Valid values are 16 (most likely), 12 or 8. */
 ###############################################################################
-ENTRY(aesni_gcm_finalize_avx_gen2)
+SYM_FUNC_START(aesni_gcm_finalize_avx_gen2)
         FUNC_SAVE
         mov	keysize,%eax
         cmp     $32, %eax
@@ -1867,7 +1867,7 @@ key_256_finalize:
         GCM_COMPLETE GHASH_MUL_AVX, 13, arg3, arg4
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_finalize_avx_gen2)
+SYM_FUNC_END(aesni_gcm_finalize_avx_gen2)
 
 #endif /* CONFIG_AS_AVX */
 
@@ -2746,12 +2746,12 @@ _initial_blocks_done\@:
 #        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 #        u64     aad_len) /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
 #############################################################
-ENTRY(aesni_gcm_init_avx_gen4)
+SYM_FUNC_START(aesni_gcm_init_avx_gen4)
         FUNC_SAVE
         INIT GHASH_MUL_AVX2, PRECOMPUTE_AVX2
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_init_avx_gen4)
+SYM_FUNC_END(aesni_gcm_init_avx_gen4)
 
 ###############################################################################
 #void   aesni_gcm_enc_avx_gen4(
@@ -2761,7 +2761,7 @@ ENDPROC(aesni_gcm_init_avx_gen4)
 #        const   u8 *in, /* Plaintext input */
 #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
 ###############################################################################
-ENTRY(aesni_gcm_enc_update_avx_gen4)
+SYM_FUNC_START(aesni_gcm_enc_update_avx_gen4)
         FUNC_SAVE
         mov     keysize,%eax
         cmp     $32, %eax
@@ -2780,7 +2780,7 @@ key_256_enc_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 13
         FUNC_RESTORE
 	ret
-ENDPROC(aesni_gcm_enc_update_avx_gen4)
+SYM_FUNC_END(aesni_gcm_enc_update_avx_gen4)
 
 ###############################################################################
 #void   aesni_gcm_dec_update_avx_gen4(
@@ -2790,7 +2790,7 @@ ENDPROC(aesni_gcm_enc_update_avx_gen4)
 #        const   u8 *in, /* Ciphertext input */
 #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
 ###############################################################################
-ENTRY(aesni_gcm_dec_update_avx_gen4)
+SYM_FUNC_START(aesni_gcm_dec_update_avx_gen4)
         FUNC_SAVE
         mov     keysize,%eax
         cmp     $32, %eax
@@ -2809,7 +2809,7 @@ key_256_dec_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 13
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_dec_update_avx_gen4)
+SYM_FUNC_END(aesni_gcm_dec_update_avx_gen4)
 
 ###############################################################################
 #void   aesni_gcm_finalize_avx_gen4(
@@ -2819,7 +2819,7 @@ ENDPROC(aesni_gcm_dec_update_avx_gen4)
 #        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
 #                              Valid values are 16 (most likely), 12 or 8. */
 ###############################################################################
-ENTRY(aesni_gcm_finalize_avx_gen4)
+SYM_FUNC_START(aesni_gcm_finalize_avx_gen4)
         FUNC_SAVE
         mov	keysize,%eax
         cmp     $32, %eax
@@ -2838,6 +2838,6 @@ key_256_finalize4:
         GCM_COMPLETE GHASH_MUL_AVX2, 13, arg3, arg4
         FUNC_RESTORE
         ret
-ENDPROC(aesni_gcm_finalize_avx_gen4)
+SYM_FUNC_END(aesni_gcm_finalize_avx_gen4)
 
 #endif /* CONFIG_AS_AVX2 */
diff --git a/arch/x86/crypto/blowfish-x86_64-asm_64.S b/arch/x86/crypto/blowfish-x86_64-asm_64.S
index 8c1fcb6bad21..70c34850ee0b 100644
--- a/arch/x86/crypto/blowfish-x86_64-asm_64.S
+++ b/arch/x86/crypto/blowfish-x86_64-asm_64.S
@@ -118,7 +118,7 @@
 	bswapq 			RX0; \
 	xorq RX0, 		(RIO);
 
-ENTRY(__blowfish_enc_blk)
+SYM_FUNC_START(__blowfish_enc_blk)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -154,9 +154,9 @@ ENTRY(__blowfish_enc_blk)
 .L__enc_xor:
 	xor_block();
 	ret;
-ENDPROC(__blowfish_enc_blk)
+SYM_FUNC_END(__blowfish_enc_blk)
 
-ENTRY(blowfish_dec_blk)
+SYM_FUNC_START(blowfish_dec_blk)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -186,7 +186,7 @@ ENTRY(blowfish_dec_blk)
 	movq %r11, %r12;
 
 	ret;
-ENDPROC(blowfish_dec_blk)
+SYM_FUNC_END(blowfish_dec_blk)
 
 /**********************************************************************
   4-way blowfish, four blocks parallel
@@ -298,7 +298,7 @@ ENDPROC(blowfish_dec_blk)
 	bswapq 			RX3; \
 	xorq RX3,		24(RIO);
 
-ENTRY(__blowfish_enc_blk_4way)
+SYM_FUNC_START(__blowfish_enc_blk_4way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -345,9 +345,9 @@ ENTRY(__blowfish_enc_blk_4way)
 	popq %rbx;
 	popq %r12;
 	ret;
-ENDPROC(__blowfish_enc_blk_4way)
+SYM_FUNC_END(__blowfish_enc_blk_4way)
 
-ENTRY(blowfish_dec_blk_4way)
+SYM_FUNC_START(blowfish_dec_blk_4way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -380,4 +380,4 @@ ENTRY(blowfish_dec_blk_4way)
 	popq %r12;
 
 	ret;
-ENDPROC(blowfish_dec_blk_4way)
+SYM_FUNC_END(blowfish_dec_blk_4way)
diff --git a/arch/x86/crypto/camellia-aesni-avx-asm_64.S b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
index a14af6eb09cb..5d5e86e56495 100644
--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
@@ -893,7 +893,7 @@ __camellia_dec_blk16:
 	jmp .Ldec_max24;
 ENDPROC(__camellia_dec_blk16)
 
-ENTRY(camellia_ecb_enc_16way)
+SYM_FUNC_START(camellia_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -916,9 +916,9 @@ ENTRY(camellia_ecb_enc_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ecb_enc_16way)
+SYM_FUNC_END(camellia_ecb_enc_16way)
 
-ENTRY(camellia_ecb_dec_16way)
+SYM_FUNC_START(camellia_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -946,9 +946,9 @@ ENTRY(camellia_ecb_dec_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ecb_dec_16way)
+SYM_FUNC_END(camellia_ecb_dec_16way)
 
-ENTRY(camellia_cbc_dec_16way)
+SYM_FUNC_START(camellia_cbc_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -997,7 +997,7 @@ ENTRY(camellia_cbc_dec_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_cbc_dec_16way)
+SYM_FUNC_END(camellia_cbc_dec_16way)
 
 #define inc_le128(x, minus_one, tmp) \
 	vpcmpeqq minus_one, x, tmp; \
@@ -1005,7 +1005,7 @@ ENDPROC(camellia_cbc_dec_16way)
 	vpslldq $8, tmp, tmp; \
 	vpsubq tmp, x, x;
 
-ENTRY(camellia_ctr_16way)
+SYM_FUNC_START(camellia_ctr_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -1110,7 +1110,7 @@ ENTRY(camellia_ctr_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ctr_16way)
+SYM_FUNC_END(camellia_ctr_16way)
 
 #define gf128mul_x_ble(iv, mask, tmp) \
 	vpsrad $31, iv, tmp; \
@@ -1256,7 +1256,7 @@ camellia_xts_crypt_16way:
 	ret;
 ENDPROC(camellia_xts_crypt_16way)
 
-ENTRY(camellia_xts_enc_16way)
+SYM_FUNC_START(camellia_xts_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -1268,9 +1268,9 @@ ENTRY(camellia_xts_enc_16way)
 	leaq __camellia_enc_blk16, %r9;
 
 	jmp camellia_xts_crypt_16way;
-ENDPROC(camellia_xts_enc_16way)
+SYM_FUNC_END(camellia_xts_enc_16way)
 
-ENTRY(camellia_xts_dec_16way)
+SYM_FUNC_START(camellia_xts_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -1286,4 +1286,4 @@ ENTRY(camellia_xts_dec_16way)
 	leaq __camellia_dec_blk16, %r9;
 
 	jmp camellia_xts_crypt_16way;
-ENDPROC(camellia_xts_dec_16way)
+SYM_FUNC_END(camellia_xts_dec_16way)
diff --git a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
index b66bbfa62f50..e2b94a12db82 100644
--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
@@ -936,7 +936,7 @@ __camellia_dec_blk32:
 	jmp .Ldec_max24;
 ENDPROC(__camellia_dec_blk32)
 
-ENTRY(camellia_ecb_enc_32way)
+SYM_FUNC_START(camellia_ecb_enc_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -963,9 +963,9 @@ ENTRY(camellia_ecb_enc_32way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ecb_enc_32way)
+SYM_FUNC_END(camellia_ecb_enc_32way)
 
-ENTRY(camellia_ecb_dec_32way)
+SYM_FUNC_START(camellia_ecb_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -997,9 +997,9 @@ ENTRY(camellia_ecb_dec_32way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ecb_dec_32way)
+SYM_FUNC_END(camellia_ecb_dec_32way)
 
-ENTRY(camellia_cbc_dec_32way)
+SYM_FUNC_START(camellia_cbc_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1065,7 +1065,7 @@ ENTRY(camellia_cbc_dec_32way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_cbc_dec_32way)
+SYM_FUNC_END(camellia_cbc_dec_32way)
 
 #define inc_le128(x, minus_one, tmp) \
 	vpcmpeqq minus_one, x, tmp; \
@@ -1081,7 +1081,7 @@ ENDPROC(camellia_cbc_dec_32way)
 	vpslldq $8, tmp1, tmp1; \
 	vpsubq tmp1, x, x;
 
-ENTRY(camellia_ctr_32way)
+SYM_FUNC_START(camellia_ctr_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1205,7 +1205,7 @@ ENTRY(camellia_ctr_32way)
 
 	FRAME_END
 	ret;
-ENDPROC(camellia_ctr_32way)
+SYM_FUNC_END(camellia_ctr_32way)
 
 #define gf128mul_x_ble(iv, mask, tmp) \
 	vpsrad $31, iv, tmp; \
@@ -1374,7 +1374,7 @@ camellia_xts_crypt_32way:
 	ret;
 ENDPROC(camellia_xts_crypt_32way)
 
-ENTRY(camellia_xts_enc_32way)
+SYM_FUNC_START(camellia_xts_enc_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1387,9 +1387,9 @@ ENTRY(camellia_xts_enc_32way)
 	leaq __camellia_enc_blk32, %r9;
 
 	jmp camellia_xts_crypt_32way;
-ENDPROC(camellia_xts_enc_32way)
+SYM_FUNC_END(camellia_xts_enc_32way)
 
-ENTRY(camellia_xts_dec_32way)
+SYM_FUNC_START(camellia_xts_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1405,4 +1405,4 @@ ENTRY(camellia_xts_dec_32way)
 	leaq __camellia_dec_blk32, %r9;
 
 	jmp camellia_xts_crypt_32way;
-ENDPROC(camellia_xts_dec_32way)
+SYM_FUNC_END(camellia_xts_dec_32way)
diff --git a/arch/x86/crypto/camellia-x86_64-asm_64.S b/arch/x86/crypto/camellia-x86_64-asm_64.S
index 95ba6956a7f6..4d77c9dcddbd 100644
--- a/arch/x86/crypto/camellia-x86_64-asm_64.S
+++ b/arch/x86/crypto/camellia-x86_64-asm_64.S
@@ -190,7 +190,7 @@
 	bswapq				RAB0; \
 	movq RAB0,			4*2(RIO);
 
-ENTRY(__camellia_enc_blk)
+SYM_FUNC_START(__camellia_enc_blk)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -235,9 +235,9 @@ ENTRY(__camellia_enc_blk)
 
 	movq RR12, %r12;
 	ret;
-ENDPROC(__camellia_enc_blk)
+SYM_FUNC_END(__camellia_enc_blk)
 
-ENTRY(camellia_dec_blk)
+SYM_FUNC_START(camellia_dec_blk)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -273,7 +273,7 @@ ENTRY(camellia_dec_blk)
 
 	movq RR12, %r12;
 	ret;
-ENDPROC(camellia_dec_blk)
+SYM_FUNC_END(camellia_dec_blk)
 
 /**********************************************************************
   2-way camellia
@@ -424,7 +424,7 @@ ENDPROC(camellia_dec_blk)
 		bswapq				RAB1; \
 		movq RAB1,			12*2(RIO);
 
-ENTRY(__camellia_enc_blk_2way)
+SYM_FUNC_START(__camellia_enc_blk_2way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -471,9 +471,9 @@ ENTRY(__camellia_enc_blk_2way)
 	movq RR12, %r12;
 	popq %rbx;
 	ret;
-ENDPROC(__camellia_enc_blk_2way)
+SYM_FUNC_END(__camellia_enc_blk_2way)
 
-ENTRY(camellia_dec_blk_2way)
+SYM_FUNC_START(camellia_dec_blk_2way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -511,4 +511,4 @@ ENTRY(camellia_dec_blk_2way)
 	movq RR12, %r12;
 	movq RXOR, %rbx;
 	ret;
-ENDPROC(camellia_dec_blk_2way)
+SYM_FUNC_END(camellia_dec_blk_2way)
diff --git a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
index 86107c961bb4..910f9a46b5da 100644
--- a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
@@ -374,7 +374,7 @@ __cast5_dec_blk16:
 	jmp .L__dec_tail;
 ENDPROC(__cast5_dec_blk16)
 
-ENTRY(cast5_ecb_enc_16way)
+SYM_FUNC_START(cast5_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -409,9 +409,9 @@ ENTRY(cast5_ecb_enc_16way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast5_ecb_enc_16way)
+SYM_FUNC_END(cast5_ecb_enc_16way)
 
-ENTRY(cast5_ecb_dec_16way)
+SYM_FUNC_START(cast5_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -447,9 +447,9 @@ ENTRY(cast5_ecb_dec_16way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast5_ecb_dec_16way)
+SYM_FUNC_END(cast5_ecb_dec_16way)
 
-ENTRY(cast5_cbc_dec_16way)
+SYM_FUNC_START(cast5_cbc_dec_16way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -499,9 +499,9 @@ ENTRY(cast5_cbc_dec_16way)
 	popq %r12;
 	FRAME_END
 	ret;
-ENDPROC(cast5_cbc_dec_16way)
+SYM_FUNC_END(cast5_cbc_dec_16way)
 
-ENTRY(cast5_ctr_16way)
+SYM_FUNC_START(cast5_ctr_16way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -575,4 +575,4 @@ ENTRY(cast5_ctr_16way)
 	popq %r12;
 	FRAME_END
 	ret;
-ENDPROC(cast5_ctr_16way)
+SYM_FUNC_END(cast5_ctr_16way)
diff --git a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
index 7f30b6f0d72c..0e452186a2e4 100644
--- a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
@@ -356,7 +356,7 @@ __cast6_dec_blk8:
 	ret;
 ENDPROC(__cast6_dec_blk8)
 
-ENTRY(cast6_ecb_enc_8way)
+SYM_FUNC_START(cast6_ecb_enc_8way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -377,9 +377,9 @@ ENTRY(cast6_ecb_enc_8way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast6_ecb_enc_8way)
+SYM_FUNC_END(cast6_ecb_enc_8way)
 
-ENTRY(cast6_ecb_dec_8way)
+SYM_FUNC_START(cast6_ecb_dec_8way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -400,9 +400,9 @@ ENTRY(cast6_ecb_dec_8way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast6_ecb_dec_8way)
+SYM_FUNC_END(cast6_ecb_dec_8way)
 
-ENTRY(cast6_cbc_dec_8way)
+SYM_FUNC_START(cast6_cbc_dec_8way)
 	/* input:
 	 *	%rdi: ctx
 	 *	%rsi: dst
@@ -426,9 +426,9 @@ ENTRY(cast6_cbc_dec_8way)
 	popq %r12;
 	FRAME_END
 	ret;
-ENDPROC(cast6_cbc_dec_8way)
+SYM_FUNC_END(cast6_cbc_dec_8way)
 
-ENTRY(cast6_ctr_8way)
+SYM_FUNC_START(cast6_ctr_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -454,9 +454,9 @@ ENTRY(cast6_ctr_8way)
 	popq %r12;
 	FRAME_END
 	ret;
-ENDPROC(cast6_ctr_8way)
+SYM_FUNC_END(cast6_ctr_8way)
 
-ENTRY(cast6_xts_enc_8way)
+SYM_FUNC_START(cast6_xts_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -481,9 +481,9 @@ ENTRY(cast6_xts_enc_8way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast6_xts_enc_8way)
+SYM_FUNC_END(cast6_xts_enc_8way)
 
-ENTRY(cast6_xts_dec_8way)
+SYM_FUNC_START(cast6_xts_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -508,4 +508,4 @@ ENTRY(cast6_xts_dec_8way)
 	popq %r15;
 	FRAME_END
 	ret;
-ENDPROC(cast6_xts_dec_8way)
+SYM_FUNC_END(cast6_xts_dec_8way)
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/chacha20-ssse3-x86_64.S
diff --git a/arch/x86/crypto/crc32-pclmul_asm.S b/arch/x86/crypto/crc32-pclmul_asm.S
index 1c099dc08cc3..9fd28ff65bc2 100644
--- a/arch/x86/crypto/crc32-pclmul_asm.S
+++ b/arch/x86/crypto/crc32-pclmul_asm.S
@@ -103,7 +103,7 @@
  *	                     size_t len, uint crc32)
  */
 
-ENTRY(crc32_pclmul_le_16) /* buffer and buffer size are 16 bytes aligned */
+SYM_FUNC_START(crc32_pclmul_le_16) /* buffer and buffer size are 16 bytes aligned */
 	movdqa  (BUF), %xmm1
 	movdqa  0x10(BUF), %xmm2
 	movdqa  0x20(BUF), %xmm3
@@ -238,4 +238,4 @@ fold_64:
 	PEXTRD  0x01, %xmm1, %eax
 
 	ret
-ENDPROC(crc32_pclmul_le_16)
+SYM_FUNC_END(crc32_pclmul_le_16)
diff --git a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
index d9b734d0c8cc..0e6690e3618c 100644
--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
@@ -74,7 +74,7 @@
 # unsigned int crc_pcl(u8 *buffer, int len, unsigned int crc_init);
 
 .text
-ENTRY(crc_pcl)
+SYM_FUNC_START(crc_pcl)
 #define    bufp		%rdi
 #define    bufp_dw	%edi
 #define    bufp_w	%di
@@ -311,7 +311,7 @@ do_return:
 	popq    %rdi
 	popq    %rbx
         ret
-ENDPROC(crc_pcl)
+SYM_FUNC_END(crc_pcl)
 
 .section	.rodata, "a", @progbits
         ################################################################
* Unmerged path arch/x86/crypto/crct10dif-pcl-asm_64.S
diff --git a/arch/x86/crypto/des3_ede-asm_64.S b/arch/x86/crypto/des3_ede-asm_64.S
index 8e49ce117494..82779c08029b 100644
--- a/arch/x86/crypto/des3_ede-asm_64.S
+++ b/arch/x86/crypto/des3_ede-asm_64.S
@@ -171,7 +171,7 @@
 	movl   left##d,   (io); \
 	movl   right##d, 4(io);
 
-ENTRY(des3_ede_x86_64_crypt_blk)
+SYM_FUNC_START(des3_ede_x86_64_crypt_blk)
 	/* input:
 	 *	%rdi: round keys, CTX
 	 *	%rsi: dst
@@ -253,7 +253,7 @@ ENTRY(des3_ede_x86_64_crypt_blk)
 	popq %rbx;
 
 	ret;
-ENDPROC(des3_ede_x86_64_crypt_blk)
+SYM_FUNC_END(des3_ede_x86_64_crypt_blk)
 
 /***********************************************************************
  * 3-way 3DES
@@ -427,7 +427,7 @@ ENDPROC(des3_ede_x86_64_crypt_blk)
 #define __movq(src, dst) \
 	movq src, dst;
 
-ENTRY(des3_ede_x86_64_crypt_blk_3way)
+SYM_FUNC_START(des3_ede_x86_64_crypt_blk_3way)
 	/* input:
 	 *	%rdi: ctx, round keys
 	 *	%rsi: dst (3 blocks)
@@ -538,7 +538,7 @@ ENTRY(des3_ede_x86_64_crypt_blk_3way)
 	popq %rbx;
 
 	ret;
-ENDPROC(des3_ede_x86_64_crypt_blk_3way)
+SYM_FUNC_END(des3_ede_x86_64_crypt_blk_3way)
 
 .section	.rodata, "a", @progbits
 .align 16
diff --git a/arch/x86/crypto/ghash-clmulni-intel_asm.S b/arch/x86/crypto/ghash-clmulni-intel_asm.S
index f94375a8dcd1..681cecc4343d 100644
--- a/arch/x86/crypto/ghash-clmulni-intel_asm.S
+++ b/arch/x86/crypto/ghash-clmulni-intel_asm.S
@@ -93,7 +93,7 @@ __clmul_gf128mul_ble:
 ENDPROC(__clmul_gf128mul_ble)
 
 /* void clmul_ghash_mul(char *dst, const u128 *shash) */
-ENTRY(clmul_ghash_mul)
+SYM_FUNC_START(clmul_ghash_mul)
 	FRAME_BEGIN
 	movups (%rdi), DATA
 	movups (%rsi), SHASH
@@ -104,13 +104,13 @@ ENTRY(clmul_ghash_mul)
 	movups DATA, (%rdi)
 	FRAME_END
 	ret
-ENDPROC(clmul_ghash_mul)
+SYM_FUNC_END(clmul_ghash_mul)
 
 /*
  * void clmul_ghash_update(char *dst, const char *src, unsigned int srclen,
  *			   const u128 *shash);
  */
-ENTRY(clmul_ghash_update)
+SYM_FUNC_START(clmul_ghash_update)
 	FRAME_BEGIN
 	cmp $16, %rdx
 	jb .Lupdate_just_ret	# check length
@@ -133,4 +133,4 @@ ENTRY(clmul_ghash_update)
 .Lupdate_just_ret:
 	FRAME_END
 	ret
-ENDPROC(clmul_ghash_update)
+SYM_FUNC_END(clmul_ghash_update)
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
diff --git a/arch/x86/crypto/poly1305-avx2-x86_64.S b/arch/x86/crypto/poly1305-avx2-x86_64.S
index 3b6e70d085da..68b0f4386dc4 100644
--- a/arch/x86/crypto/poly1305-avx2-x86_64.S
+++ b/arch/x86/crypto/poly1305-avx2-x86_64.S
@@ -83,7 +83,7 @@ ORMASK:	.octa 0x00000000010000000000000001000000
 #define d3 %r12
 #define d4 %r13
 
-ENTRY(poly1305_4block_avx2)
+SYM_FUNC_START(poly1305_4block_avx2)
 	# %rdi: Accumulator h[5]
 	# %rsi: 64 byte input block m
 	# %rdx: Poly1305 key r[5]
@@ -385,4 +385,4 @@ ENTRY(poly1305_4block_avx2)
 	pop		%r12
 	pop		%rbx
 	ret
-ENDPROC(poly1305_4block_avx2)
+SYM_FUNC_END(poly1305_4block_avx2)
diff --git a/arch/x86/crypto/poly1305-sse2-x86_64.S b/arch/x86/crypto/poly1305-sse2-x86_64.S
index c88c670cb5fc..66715fbedc18 100644
--- a/arch/x86/crypto/poly1305-sse2-x86_64.S
+++ b/arch/x86/crypto/poly1305-sse2-x86_64.S
@@ -50,7 +50,7 @@ ORMASK:	.octa 0x00000000010000000000000001000000
 #define d3 %r11
 #define d4 %r12
 
-ENTRY(poly1305_block_sse2)
+SYM_FUNC_START(poly1305_block_sse2)
 	# %rdi: Accumulator h[5]
 	# %rsi: 16 byte input block m
 	# %rdx: Poly1305 key r[5]
@@ -276,7 +276,7 @@ ENTRY(poly1305_block_sse2)
 	pop		%r12
 	pop		%rbx
 	ret
-ENDPROC(poly1305_block_sse2)
+SYM_FUNC_END(poly1305_block_sse2)
 
 
 #define u0 0x00(%r8)
@@ -301,7 +301,7 @@ ENDPROC(poly1305_block_sse2)
 #undef d0
 #define d0 %r13
 
-ENTRY(poly1305_2block_sse2)
+SYM_FUNC_START(poly1305_2block_sse2)
 	# %rdi: Accumulator h[5]
 	# %rsi: 16 byte input block m
 	# %rdx: Poly1305 key r[5]
@@ -581,4 +581,4 @@ ENTRY(poly1305_2block_sse2)
 	pop		%r12
 	pop		%rbx
 	ret
-ENDPROC(poly1305_2block_sse2)
+SYM_FUNC_END(poly1305_2block_sse2)
diff --git a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
index 2925077f8c6a..5739ad63fbbe 100644
--- a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
@@ -677,7 +677,7 @@ __serpent_dec_blk8_avx:
 	ret;
 ENDPROC(__serpent_dec_blk8_avx)
 
-ENTRY(serpent_ecb_enc_8way_avx)
+SYM_FUNC_START(serpent_ecb_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -693,9 +693,9 @@ ENTRY(serpent_ecb_enc_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ecb_enc_8way_avx)
+SYM_FUNC_END(serpent_ecb_enc_8way_avx)
 
-ENTRY(serpent_ecb_dec_8way_avx)
+SYM_FUNC_START(serpent_ecb_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -711,9 +711,9 @@ ENTRY(serpent_ecb_dec_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ecb_dec_8way_avx)
+SYM_FUNC_END(serpent_ecb_dec_8way_avx)
 
-ENTRY(serpent_cbc_dec_8way_avx)
+SYM_FUNC_START(serpent_cbc_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -729,9 +729,9 @@ ENTRY(serpent_cbc_dec_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_cbc_dec_8way_avx)
+SYM_FUNC_END(serpent_cbc_dec_8way_avx)
 
-ENTRY(serpent_ctr_8way_avx)
+SYM_FUNC_START(serpent_ctr_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -749,9 +749,9 @@ ENTRY(serpent_ctr_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ctr_8way_avx)
+SYM_FUNC_END(serpent_ctr_8way_avx)
 
-ENTRY(serpent_xts_enc_8way_avx)
+SYM_FUNC_START(serpent_xts_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -771,9 +771,9 @@ ENTRY(serpent_xts_enc_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_xts_enc_8way_avx)
+SYM_FUNC_END(serpent_xts_enc_8way_avx)
 
-ENTRY(serpent_xts_dec_8way_avx)
+SYM_FUNC_START(serpent_xts_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -793,4 +793,4 @@ ENTRY(serpent_xts_dec_8way_avx)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_xts_dec_8way_avx)
+SYM_FUNC_END(serpent_xts_dec_8way_avx)
diff --git a/arch/x86/crypto/serpent-avx2-asm_64.S b/arch/x86/crypto/serpent-avx2-asm_64.S
index d67888f2a52a..59fa0a88bd75 100644
--- a/arch/x86/crypto/serpent-avx2-asm_64.S
+++ b/arch/x86/crypto/serpent-avx2-asm_64.S
@@ -673,7 +673,7 @@ __serpent_dec_blk16:
 	ret;
 ENDPROC(__serpent_dec_blk16)
 
-ENTRY(serpent_ecb_enc_16way)
+SYM_FUNC_START(serpent_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -693,9 +693,9 @@ ENTRY(serpent_ecb_enc_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ecb_enc_16way)
+SYM_FUNC_END(serpent_ecb_enc_16way)
 
-ENTRY(serpent_ecb_dec_16way)
+SYM_FUNC_START(serpent_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -715,9 +715,9 @@ ENTRY(serpent_ecb_dec_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ecb_dec_16way)
+SYM_FUNC_END(serpent_ecb_dec_16way)
 
-ENTRY(serpent_cbc_dec_16way)
+SYM_FUNC_START(serpent_cbc_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -738,9 +738,9 @@ ENTRY(serpent_cbc_dec_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_cbc_dec_16way)
+SYM_FUNC_END(serpent_cbc_dec_16way)
 
-ENTRY(serpent_ctr_16way)
+SYM_FUNC_START(serpent_ctr_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -763,9 +763,9 @@ ENTRY(serpent_ctr_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_ctr_16way)
+SYM_FUNC_END(serpent_ctr_16way)
 
-ENTRY(serpent_xts_enc_16way)
+SYM_FUNC_START(serpent_xts_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -789,9 +789,9 @@ ENTRY(serpent_xts_enc_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_xts_enc_16way)
+SYM_FUNC_END(serpent_xts_enc_16way)
 
-ENTRY(serpent_xts_dec_16way)
+SYM_FUNC_START(serpent_xts_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -815,4 +815,4 @@ ENTRY(serpent_xts_dec_16way)
 
 	FRAME_END
 	ret;
-ENDPROC(serpent_xts_dec_16way)
+SYM_FUNC_END(serpent_xts_dec_16way)
diff --git a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
index acc066c7c6b2..bdeee900df63 100644
--- a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
@@ -634,7 +634,7 @@
 	pxor t0,		x3; \
 	movdqu x3,		(3*4*4)(out);
 
-ENTRY(__serpent_enc_blk_8way)
+SYM_FUNC_START(__serpent_enc_blk_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -697,9 +697,9 @@ ENTRY(__serpent_enc_blk_8way)
 	xor_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
 	ret;
-ENDPROC(__serpent_enc_blk_8way)
+SYM_FUNC_END(__serpent_enc_blk_8way)
 
-ENTRY(serpent_dec_blk_8way)
+SYM_FUNC_START(serpent_dec_blk_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -751,4 +751,4 @@ ENTRY(serpent_dec_blk_8way)
 	write_blocks(%rax, RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
 	ret;
-ENDPROC(serpent_dec_blk_8way)
+SYM_FUNC_END(serpent_dec_blk_8way)
diff --git a/arch/x86/crypto/sha1_avx2_x86_64_asm.S b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
index 9f712a7dfd79..6decc85ef7b7 100644
--- a/arch/x86/crypto/sha1_avx2_x86_64_asm.S
+++ b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
@@ -634,7 +634,7 @@ _loop3:
  * param: function's name
  */
 .macro SHA1_VECTOR_ASM  name
-	ENTRY(\name)
+	SYM_FUNC_START(\name)
 
 	push	%rbx
 	push	%r12
@@ -676,7 +676,7 @@ _loop3:
 
 	ret
 
-	ENDPROC(\name)
+	SYM_FUNC_END(\name)
 .endm
 
 .section .rodata
diff --git a/arch/x86/crypto/sha1_ni_asm.S b/arch/x86/crypto/sha1_ni_asm.S
index ebbdba72ae07..11efe3a45a1f 100644
--- a/arch/x86/crypto/sha1_ni_asm.S
+++ b/arch/x86/crypto/sha1_ni_asm.S
@@ -95,7 +95,7 @@
  */
 .text
 .align 32
-ENTRY(sha1_ni_transform)
+SYM_FUNC_START(sha1_ni_transform)
 	mov		%rsp, RSPSAVE
 	sub		$FRAME_SIZE, %rsp
 	and		$~0xF, %rsp
@@ -291,7 +291,7 @@ ENTRY(sha1_ni_transform)
 	mov		RSPSAVE, %rsp
 
 	ret
-ENDPROC(sha1_ni_transform)
+SYM_FUNC_END(sha1_ni_transform)
 
 .section	.rodata.cst16.PSHUFFLE_BYTE_FLIP_MASK, "aM", @progbits, 16
 .align 16
diff --git a/arch/x86/crypto/sha1_ssse3_asm.S b/arch/x86/crypto/sha1_ssse3_asm.S
index 613d0bfc3d84..265caba3d113 100644
--- a/arch/x86/crypto/sha1_ssse3_asm.S
+++ b/arch/x86/crypto/sha1_ssse3_asm.S
@@ -71,7 +71,7 @@
  * param: function's name
  */
 .macro SHA1_VECTOR_ASM  name
-	ENTRY(\name)
+	SYM_FUNC_START(\name)
 
 	push	%rbx
 	push	%r12
@@ -105,7 +105,7 @@
 	pop	%rbx
 	ret
 
-	ENDPROC(\name)
+	SYM_FUNC_END(\name)
 .endm
 
 /*
diff --git a/arch/x86/crypto/sha256-avx-asm.S b/arch/x86/crypto/sha256-avx-asm.S
index 001bbcf93c79..22e14c8dd2e4 100644
--- a/arch/x86/crypto/sha256-avx-asm.S
+++ b/arch/x86/crypto/sha256-avx-asm.S
@@ -347,7 +347,7 @@ a = TMP_
 ## arg 3 : Num blocks
 ########################################################################
 .text
-ENTRY(sha256_transform_avx)
+SYM_FUNC_START(sha256_transform_avx)
 .align 32
 	pushq   %rbx
 	pushq   %r12
@@ -460,7 +460,7 @@ done_hash:
 	popq	%r12
 	popq    %rbx
 	ret
-ENDPROC(sha256_transform_avx)
+SYM_FUNC_END(sha256_transform_avx)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
 .align 64
diff --git a/arch/x86/crypto/sha256-avx2-asm.S b/arch/x86/crypto/sha256-avx2-asm.S
index 1420db15dcdd..519b551ad576 100644
--- a/arch/x86/crypto/sha256-avx2-asm.S
+++ b/arch/x86/crypto/sha256-avx2-asm.S
@@ -526,7 +526,7 @@ STACK_SIZE	= _RSP      + _RSP_SIZE
 ## arg 3 : Num blocks
 ########################################################################
 .text
-ENTRY(sha256_transform_rorx)
+SYM_FUNC_START(sha256_transform_rorx)
 .align 32
 	pushq	%rbx
 	pushq	%r12
@@ -713,7 +713,7 @@ done_hash:
 	popq	%r12
 	popq	%rbx
 	ret
-ENDPROC(sha256_transform_rorx)
+SYM_FUNC_END(sha256_transform_rorx)
 
 .section	.rodata.cst512.K256, "aM", @progbits, 512
 .align 64
diff --git a/arch/x86/crypto/sha256-ssse3-asm.S b/arch/x86/crypto/sha256-ssse3-asm.S
index c6c05ed2c16a..69cc2f91dc4c 100644
--- a/arch/x86/crypto/sha256-ssse3-asm.S
+++ b/arch/x86/crypto/sha256-ssse3-asm.S
@@ -353,7 +353,7 @@ a = TMP_
 ## arg 3 : Num blocks
 ########################################################################
 .text
-ENTRY(sha256_transform_ssse3)
+SYM_FUNC_START(sha256_transform_ssse3)
 .align 32
 	pushq   %rbx
 	pushq   %r12
@@ -471,7 +471,7 @@ done_hash:
 	popq    %rbx
 
 	ret
-ENDPROC(sha256_transform_ssse3)
+SYM_FUNC_END(sha256_transform_ssse3)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
 .align 64
diff --git a/arch/x86/crypto/sha256_ni_asm.S b/arch/x86/crypto/sha256_ni_asm.S
index fb58f58ecfbc..7abade04a3a3 100644
--- a/arch/x86/crypto/sha256_ni_asm.S
+++ b/arch/x86/crypto/sha256_ni_asm.S
@@ -97,7 +97,7 @@
 
 .text
 .align 32
-ENTRY(sha256_ni_transform)
+SYM_FUNC_START(sha256_ni_transform)
 
 	shl		$6, NUM_BLKS		/*  convert to bytes */
 	jz		.Ldone_hash
@@ -327,7 +327,7 @@ ENTRY(sha256_ni_transform)
 .Ldone_hash:
 
 	ret
-ENDPROC(sha256_ni_transform)
+SYM_FUNC_END(sha256_ni_transform)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
 .align 64
diff --git a/arch/x86/crypto/sha512-avx-asm.S b/arch/x86/crypto/sha512-avx-asm.S
index 39235fefe6f7..3704ddd7e5d5 100644
--- a/arch/x86/crypto/sha512-avx-asm.S
+++ b/arch/x86/crypto/sha512-avx-asm.S
@@ -277,7 +277,7 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 # message blocks.
 # L is the message length in SHA512 blocks
 ########################################################################
-ENTRY(sha512_transform_avx)
+SYM_FUNC_START(sha512_transform_avx)
 	cmp $0, msglen
 	je nowork
 
@@ -365,7 +365,7 @@ updateblock:
 
 nowork:
 	ret
-ENDPROC(sha512_transform_avx)
+SYM_FUNC_END(sha512_transform_avx)
 
 ########################################################################
 ### Binary Data
diff --git a/arch/x86/crypto/sha512-avx2-asm.S b/arch/x86/crypto/sha512-avx2-asm.S
index b16d56005162..80d830e7ee09 100644
--- a/arch/x86/crypto/sha512-avx2-asm.S
+++ b/arch/x86/crypto/sha512-avx2-asm.S
@@ -569,7 +569,7 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 #   message blocks.
 # L is the message length in SHA512 blocks
 ########################################################################
-ENTRY(sha512_transform_rorx)
+SYM_FUNC_START(sha512_transform_rorx)
 	# Allocate Stack Space
 	mov	%rsp, %rax
 	sub	$frame_size, %rsp
@@ -682,7 +682,7 @@ done_hash:
 	# Restore Stack Pointer
 	mov	frame_RSPSAVE(%rsp), %rsp
 	ret
-ENDPROC(sha512_transform_rorx)
+SYM_FUNC_END(sha512_transform_rorx)
 
 ########################################################################
 ### Binary Data
diff --git a/arch/x86/crypto/sha512-ssse3-asm.S b/arch/x86/crypto/sha512-ssse3-asm.S
index 66bbd9058a90..838f984e95d9 100644
--- a/arch/x86/crypto/sha512-ssse3-asm.S
+++ b/arch/x86/crypto/sha512-ssse3-asm.S
@@ -275,7 +275,7 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 #   message blocks.
 # L is the message length in SHA512 blocks.
 ########################################################################
-ENTRY(sha512_transform_ssse3)
+SYM_FUNC_START(sha512_transform_ssse3)
 
 	cmp $0, msglen
 	je nowork
@@ -364,7 +364,7 @@ updateblock:
 
 nowork:
 	ret
-ENDPROC(sha512_transform_ssse3)
+SYM_FUNC_END(sha512_transform_ssse3)
 
 ########################################################################
 ### Binary Data
diff --git a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
index 73b471da3622..e631e8c84a98 100644
--- a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
@@ -330,7 +330,7 @@ __twofish_dec_blk8:
 	ret;
 ENDPROC(__twofish_dec_blk8)
 
-ENTRY(twofish_ecb_enc_8way)
+SYM_FUNC_START(twofish_ecb_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -348,9 +348,9 @@ ENTRY(twofish_ecb_enc_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_ecb_enc_8way)
+SYM_FUNC_END(twofish_ecb_enc_8way)
 
-ENTRY(twofish_ecb_dec_8way)
+SYM_FUNC_START(twofish_ecb_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -368,9 +368,9 @@ ENTRY(twofish_ecb_dec_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_ecb_dec_8way)
+SYM_FUNC_END(twofish_ecb_dec_8way)
 
-ENTRY(twofish_cbc_dec_8way)
+SYM_FUNC_START(twofish_cbc_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -393,9 +393,9 @@ ENTRY(twofish_cbc_dec_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_cbc_dec_8way)
+SYM_FUNC_END(twofish_cbc_dec_8way)
 
-ENTRY(twofish_ctr_8way)
+SYM_FUNC_START(twofish_ctr_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -420,9 +420,9 @@ ENTRY(twofish_ctr_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_ctr_8way)
+SYM_FUNC_END(twofish_ctr_8way)
 
-ENTRY(twofish_xts_enc_8way)
+SYM_FUNC_START(twofish_xts_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -444,9 +444,9 @@ ENTRY(twofish_xts_enc_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_xts_enc_8way)
+SYM_FUNC_END(twofish_xts_enc_8way)
 
-ENTRY(twofish_xts_dec_8way)
+SYM_FUNC_START(twofish_xts_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -468,4 +468,4 @@ ENTRY(twofish_xts_dec_8way)
 
 	FRAME_END
 	ret;
-ENDPROC(twofish_xts_dec_8way)
+SYM_FUNC_END(twofish_xts_dec_8way)
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
index e7273a606a07..c830aef77070 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
@@ -235,7 +235,7 @@
 	rorq $32,			RAB2; \
 	outunpack3(mov, RIO, 2, RAB, 2);
 
-ENTRY(__twofish_enc_blk_3way)
+SYM_FUNC_START(__twofish_enc_blk_3way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -282,9 +282,9 @@ ENTRY(__twofish_enc_blk_3way)
 	popq %r12;
 	popq %r13;
 	ret;
-ENDPROC(__twofish_enc_blk_3way)
+SYM_FUNC_END(__twofish_enc_blk_3way)
 
-ENTRY(twofish_dec_blk_3way)
+SYM_FUNC_START(twofish_dec_blk_3way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -317,4 +317,4 @@ ENTRY(twofish_dec_blk_3way)
 	popq %r12;
 	popq %r13;
 	ret;
-ENDPROC(twofish_dec_blk_3way)
+SYM_FUNC_END(twofish_dec_blk_3way)
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64.S b/arch/x86/crypto/twofish-x86_64-asm_64.S
index a350c990dc86..74ef6c55d75f 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64.S
@@ -215,7 +215,7 @@
 	xor	%r8d,		d ## D;\
 	ror	$1,		d ## D;
 
-ENTRY(twofish_enc_blk)
+SYM_FUNC_START(twofish_enc_blk)
 	pushq    R1
 
 	/* %rdi contains the ctx address */
@@ -266,9 +266,9 @@ ENTRY(twofish_enc_blk)
 	popq	R1
 	movl	$1,%eax
 	ret
-ENDPROC(twofish_enc_blk)
+SYM_FUNC_END(twofish_enc_blk)
 
-ENTRY(twofish_dec_blk)
+SYM_FUNC_START(twofish_dec_blk)
 	pushq    R1
 
 	/* %rdi contains the ctx address */
@@ -318,4 +318,4 @@ ENTRY(twofish_dec_blk)
 	popq	R1
 	movl	$1,%eax
 	ret
-ENDPROC(twofish_dec_blk)
+SYM_FUNC_END(twofish_dec_blk)
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 6ebaa7c43b8e..a8660c758840 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -15,7 +15,7 @@
  *			at the top of the kernel process stack.
  *
  * Some macro usage:
- * - ENTRY/END:		Define functions in the symbol table.
+ * - SYM_FUNC_START/END:Define functions in the symbol table.
  * - TRACE_IRQ_*:	Trace hardirq state for lock debugging.
  * - idtentry:		Define exception entry points.
  */
@@ -1057,7 +1057,7 @@ idtentry vmm_communication		do_vmm_communication		has_error_code=1
 	 * Reload gs selector with exception handling
 	 * edi:  new selector
 	 */
-ENTRY(native_load_gs_index)
+SYM_FUNC_START(native_load_gs_index)
 	FRAME_BEGIN
 	pushfq
 	DISABLE_INTERRUPTS(CLBR_ANY & ~CLBR_RDI)
@@ -1071,7 +1071,7 @@ ENTRY(native_load_gs_index)
 	popfq
 	FRAME_END
 	ret
-ENDPROC(native_load_gs_index)
+SYM_FUNC_END(native_load_gs_index)
 EXPORT_SYMBOL(native_load_gs_index)
 
 	_ASM_EXTABLE(.Lgs_change, .Lbad_gs)
@@ -1091,7 +1091,7 @@ EXPORT_SYMBOL(native_load_gs_index)
 	.previous
 
 /* Call softirq on interrupt stack. Interrupts are off. */
-ENTRY(do_softirq_own_stack)
+SYM_FUNC_START(do_softirq_own_stack)
 	pushq	%rbp
 	mov	%rsp, %rbp
 	ENTER_IRQ_STACK regs=0 old_rsp=%r11
@@ -1099,7 +1099,7 @@ ENTRY(do_softirq_own_stack)
 	LEAVE_IRQ_STACK regs=0
 	leaveq
 	ret
-ENDPROC(do_softirq_own_stack)
+SYM_FUNC_END(do_softirq_own_stack)
 
 #ifdef CONFIG_XEN_PV
 idtentry hypervisor_callback xen_do_hypervisor_callback has_error_code=0
diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S
index 0f660ab3af8f..0876d2f7c7cd 100644
--- a/arch/x86/entry/entry_64_compat.S
+++ b/arch/x86/entry/entry_64_compat.S
@@ -47,7 +47,7 @@
  * ebp  user stack
  * 0(%ebp) arg6
  */
-ENTRY(entry_SYSENTER_compat)
+SYM_FUNC_START(entry_SYSENTER_compat)
 	/* Interrupts are off on entry. */
 	SWAPGS
 
@@ -150,7 +150,7 @@ ENTRY(entry_SYSENTER_compat)
 	popfq
 	jmp	.Lsysenter_flags_fixed
 SYM_INNER_LABEL(__end_entry_SYSENTER_compat, SYM_L_GLOBAL)
-ENDPROC(entry_SYSENTER_compat)
+SYM_FUNC_END(entry_SYSENTER_compat)
 
 /*
  * 32-bit SYSCALL entry.
* Unmerged path arch/x86/kernel/acpi/wakeup_64.S
diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S
index b09dfa3df148..b2466c5e1ace 100644
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@ -134,11 +134,11 @@ EXPORT_SYMBOL(__fentry__)
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 
-ENTRY(function_hook)
+SYM_FUNC_START(function_hook)
 	retq
-ENDPROC(function_hook)
+SYM_FUNC_END(function_hook)
 
-ENTRY(ftrace_caller)
+SYM_FUNC_START(ftrace_caller)
 	/* save_mcount_regs fills in first two parameters */
 	save_mcount_regs
 
@@ -172,9 +172,9 @@ SYM_INNER_LABEL(ftrace_graph_call, SYM_L_GLOBAL)
 /* This is weak to keep gas from relaxing the jumps */
 WEAK(ftrace_stub)
 	retq
-ENDPROC(ftrace_caller)
+SYM_FUNC_END(ftrace_caller)
 
-ENTRY(ftrace_regs_caller)
+SYM_FUNC_START(ftrace_regs_caller)
 	/* Save the current flags before any operations that can change them */
 	pushfq
 
@@ -273,12 +273,12 @@ SYM_INNER_LABEL(ftrace_regs_caller_end, SYM_L_GLOBAL)
 
 	jmp ftrace_epilogue
 
-ENDPROC(ftrace_regs_caller)
+SYM_FUNC_END(ftrace_regs_caller)
 
 
 #else /* ! CONFIG_DYNAMIC_FTRACE */
 
-ENTRY(function_hook)
+SYM_FUNC_START(function_hook)
 	cmpq $ftrace_stub, ftrace_trace_function
 	jnz trace
 
@@ -309,11 +309,11 @@ trace:
 	restore_mcount_regs
 
 	jmp fgraph_trace
-ENDPROC(function_hook)
+SYM_FUNC_END(function_hook)
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-ENTRY(ftrace_graph_caller)
+SYM_FUNC_START(ftrace_graph_caller)
 	/* Saves rbp into %rdx and fills first parameter  */
 	save_mcount_regs
 
@@ -324,7 +324,7 @@ ENTRY(ftrace_graph_caller)
 	restore_mcount_regs
 
 	retq
-ENDPROC(ftrace_graph_caller)
+SYM_FUNC_END(ftrace_graph_caller)
 
 ENTRY(return_to_handler)
 	UNWIND_HINT_EMPTY
diff --git a/arch/x86/kernel/irqflags.S b/arch/x86/kernel/irqflags.S
index ddeeaac8adda..0db0375235b4 100644
--- a/arch/x86/kernel/irqflags.S
+++ b/arch/x86/kernel/irqflags.S
@@ -7,20 +7,20 @@
 /*
  * unsigned long native_save_fl(void)
  */
-ENTRY(native_save_fl)
+SYM_FUNC_START(native_save_fl)
 	pushf
 	pop %_ASM_AX
 	ret
-ENDPROC(native_save_fl)
+SYM_FUNC_END(native_save_fl)
 EXPORT_SYMBOL(native_save_fl)
 
 /*
  * void native_restore_fl(unsigned long flags)
  * %eax/%rdi: flags
  */
-ENTRY(native_restore_fl)
+SYM_FUNC_START(native_restore_fl)
 	push %_ASM_ARG1
 	popf
 	ret
-ENDPROC(native_restore_fl)
+SYM_FUNC_END(native_restore_fl)
 EXPORT_SYMBOL(native_restore_fl)
* Unmerged path arch/x86/kvm/vmx/vmenter.S
diff --git a/arch/x86/lib/checksum_32.S b/arch/x86/lib/checksum_32.S
index ad8e0906d1ea..db1d1dd5ae35 100644
--- a/arch/x86/lib/checksum_32.S
+++ b/arch/x86/lib/checksum_32.S
@@ -284,7 +284,7 @@ unsigned int csum_partial_copy_generic (const char *src, char *dst,
 #define ARGBASE 16		
 #define FP		12
 		
-ENTRY(csum_partial_copy_generic)
+SYM_FUNC_START(csum_partial_copy_generic)
 	subl  $4,%esp	
 	pushl %edi
 	pushl %esi
@@ -402,7 +402,7 @@ DST(	movb %cl, (%edi)	)
 	popl %edi
 	popl %ecx			# equivalent to addl $4,%esp
 	ret	
-ENDPROC(csum_partial_copy_generic)
+SYM_FUNC_END(csum_partial_copy_generic)
 
 #else
 
@@ -420,7 +420,7 @@ ENDPROC(csum_partial_copy_generic)
 
 #define ARGBASE 12
 		
-ENTRY(csum_partial_copy_generic)
+SYM_FUNC_START(csum_partial_copy_generic)
 	pushl %ebx
 	pushl %edi
 	pushl %esi
@@ -487,7 +487,7 @@ DST(	movb %dl, (%edi)         )
 	popl %edi
 	popl %ebx
 	ret
-ENDPROC(csum_partial_copy_generic)
+SYM_FUNC_END(csum_partial_copy_generic)
 				
 #undef ROUND
 #undef ROUND1		
diff --git a/arch/x86/lib/clear_page_64.S b/arch/x86/lib/clear_page_64.S
index 88acd349911b..47aa2830010b 100644
--- a/arch/x86/lib/clear_page_64.S
+++ b/arch/x86/lib/clear_page_64.S
@@ -12,15 +12,15 @@
  * Zero a page.
  * %rdi	- page
  */
-ENTRY(clear_page_rep)
+SYM_FUNC_START(clear_page_rep)
 	movl $4096/8,%ecx
 	xorl %eax,%eax
 	rep stosq
 	ret
-ENDPROC(clear_page_rep)
+SYM_FUNC_END(clear_page_rep)
 EXPORT_SYMBOL_GPL(clear_page_rep)
 
-ENTRY(clear_page_orig)
+SYM_FUNC_START(clear_page_orig)
 	xorl   %eax,%eax
 	movl   $4096/64,%ecx
 	.p2align 4
@@ -39,13 +39,13 @@ ENTRY(clear_page_orig)
 	jnz	.Lloop
 	nop
 	ret
-ENDPROC(clear_page_orig)
+SYM_FUNC_END(clear_page_orig)
 EXPORT_SYMBOL_GPL(clear_page_orig)
 
-ENTRY(clear_page_erms)
+SYM_FUNC_START(clear_page_erms)
 	movl $4096,%ecx
 	xorl %eax,%eax
 	rep stosb
 	ret
-ENDPROC(clear_page_erms)
+SYM_FUNC_END(clear_page_erms)
 EXPORT_SYMBOL_GPL(clear_page_erms)
diff --git a/arch/x86/lib/cmpxchg16b_emu.S b/arch/x86/lib/cmpxchg16b_emu.S
index 9b330242e740..b6ba6360b3ca 100644
--- a/arch/x86/lib/cmpxchg16b_emu.S
+++ b/arch/x86/lib/cmpxchg16b_emu.S
@@ -19,7 +19,7 @@
  * %rcx : high 64 bits of new value
  * %al  : Operation successful
  */
-ENTRY(this_cpu_cmpxchg16b_emu)
+SYM_FUNC_START(this_cpu_cmpxchg16b_emu)
 
 #
 # Emulate 'cmpxchg16b %gs:(%rsi)' except we return the result in %al not
@@ -50,4 +50,4 @@ ENTRY(this_cpu_cmpxchg16b_emu)
 	xor %al,%al
 	ret
 
-ENDPROC(this_cpu_cmpxchg16b_emu)
+SYM_FUNC_END(this_cpu_cmpxchg16b_emu)
diff --git a/arch/x86/lib/cmpxchg8b_emu.S b/arch/x86/lib/cmpxchg8b_emu.S
index 03a186fc06ea..77aa18db3968 100644
--- a/arch/x86/lib/cmpxchg8b_emu.S
+++ b/arch/x86/lib/cmpxchg8b_emu.S
@@ -19,7 +19,7 @@
  * %ebx : low 32 bits of new value
  * %ecx : high 32 bits of new value
  */
-ENTRY(cmpxchg8b_emu)
+SYM_FUNC_START(cmpxchg8b_emu)
 
 #
 # Emulate 'cmpxchg8b (%esi)' on UP except we don't
@@ -48,5 +48,5 @@ ENTRY(cmpxchg8b_emu)
 	popfl
 	ret
 
-ENDPROC(cmpxchg8b_emu)
+SYM_FUNC_END(cmpxchg8b_emu)
 EXPORT_SYMBOL(cmpxchg8b_emu)
diff --git a/arch/x86/lib/copy_page_64.S b/arch/x86/lib/copy_page_64.S
index fd2d09afa097..7d2148c14713 100644
--- a/arch/x86/lib/copy_page_64.S
+++ b/arch/x86/lib/copy_page_64.S
@@ -13,12 +13,12 @@
  * prefetch distance based on SMP/UP.
  */
 	ALIGN
-ENTRY(copy_page)
+SYM_FUNC_START(copy_page)
 	ALTERNATIVE "jmp copy_page_regs", "", X86_FEATURE_REP_GOOD
 	movl	$4096/8, %ecx
 	rep	movsq
 	ret
-ENDPROC(copy_page)
+SYM_FUNC_END(copy_page)
 EXPORT_SYMBOL(copy_page)
 
 ENTRY(copy_page_regs)
diff --git a/arch/x86/lib/copy_user_64.S b/arch/x86/lib/copy_user_64.S
index 8bfc57509016..a41fa4de85f2 100644
--- a/arch/x86/lib/copy_user_64.S
+++ b/arch/x86/lib/copy_user_64.S
@@ -53,7 +53,7 @@
  * Output:
  * eax uncopied bytes or 0 if successful.
  */
-ENTRY(copy_user_generic_unrolled)
+SYM_FUNC_START(copy_user_generic_unrolled)
 	ASM_STAC
 	cmpl $8,%edx
 	jb 20f		/* less then 8 bytes, go to byte copy loop */
@@ -136,7 +136,7 @@ ENTRY(copy_user_generic_unrolled)
 	_ASM_EXTABLE_UA(19b, 40b)
 	_ASM_EXTABLE_UA(21b, 50b)
 	_ASM_EXTABLE_UA(22b, 50b)
-ENDPROC(copy_user_generic_unrolled)
+SYM_FUNC_END(copy_user_generic_unrolled)
 EXPORT_SYMBOL(copy_user_generic_unrolled)
 
 /* Some CPUs run faster using the string copy instructions.
@@ -157,7 +157,7 @@ EXPORT_SYMBOL(copy_user_generic_unrolled)
  * Output:
  * eax uncopied bytes or 0 if successful.
  */
-ENTRY(copy_user_generic_string)
+SYM_FUNC_START(copy_user_generic_string)
 	ASM_STAC
 	cmpl $8,%edx
 	jb 2f		/* less than 8 bytes, go to byte copy loop */
@@ -182,7 +182,7 @@ ENTRY(copy_user_generic_string)
 
 	_ASM_EXTABLE_UA(1b, 11b)
 	_ASM_EXTABLE_UA(3b, 12b)
-ENDPROC(copy_user_generic_string)
+SYM_FUNC_END(copy_user_generic_string)
 EXPORT_SYMBOL(copy_user_generic_string)
 
 /*
@@ -197,7 +197,7 @@ EXPORT_SYMBOL(copy_user_generic_string)
  * Output:
  * eax uncopied bytes or 0 if successful.
  */
-ENTRY(copy_user_enhanced_fast_string)
+SYM_FUNC_START(copy_user_enhanced_fast_string)
 	ASM_STAC
 	cmpl $64,%edx
 	jb .L_copy_short_string	/* less then 64 bytes, avoid the costly 'rep' */
@@ -214,7 +214,7 @@ ENTRY(copy_user_enhanced_fast_string)
 	.previous
 
 	_ASM_EXTABLE_UA(1b, 12b)
-ENDPROC(copy_user_enhanced_fast_string)
+SYM_FUNC_END(copy_user_enhanced_fast_string)
 EXPORT_SYMBOL(copy_user_enhanced_fast_string)
 
 /*
@@ -250,7 +250,7 @@ END(.Lcopy_user_handle_tail)
  *  - Require 8-byte alignment when size is 8 bytes or larger.
  *  - Require 4-byte alignment when size is 4 bytes.
  */
-ENTRY(__copy_user_nocache)
+SYM_FUNC_START(__copy_user_nocache)
 	ASM_STAC
 
 	/* If size is less than 8 bytes, go to 4-byte copy */
@@ -389,5 +389,5 @@ ENTRY(__copy_user_nocache)
 	_ASM_EXTABLE_UA(31b, .L_fixup_4b_copy)
 	_ASM_EXTABLE_UA(40b, .L_fixup_1b_copy)
 	_ASM_EXTABLE_UA(41b, .L_fixup_1b_copy)
-ENDPROC(__copy_user_nocache)
+SYM_FUNC_END(__copy_user_nocache)
 EXPORT_SYMBOL(__copy_user_nocache)
diff --git a/arch/x86/lib/csum-copy_64.S b/arch/x86/lib/csum-copy_64.S
index a4a379e79259..3394a8ff7fd0 100644
--- a/arch/x86/lib/csum-copy_64.S
+++ b/arch/x86/lib/csum-copy_64.S
@@ -49,7 +49,7 @@
 	.endm
 
 
-ENTRY(csum_partial_copy_generic)
+SYM_FUNC_START(csum_partial_copy_generic)
 	cmpl	$3*64, %edx
 	jle	.Lignore
 
@@ -225,4 +225,4 @@ ENTRY(csum_partial_copy_generic)
 	jz   .Lende
 	movl $-EFAULT, (%rax)
 	jmp .Lende
-ENDPROC(csum_partial_copy_generic)
+SYM_FUNC_END(csum_partial_copy_generic)
diff --git a/arch/x86/lib/getuser.S b/arch/x86/lib/getuser.S
index 9578eb88fc87..a21ee2882611 100644
--- a/arch/x86/lib/getuser.S
+++ b/arch/x86/lib/getuser.S
@@ -36,7 +36,7 @@
 #include <asm/export.h>
 
 	.text
-ENTRY(__get_user_1)
+SYM_FUNC_START(__get_user_1)
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user
@@ -47,10 +47,10 @@ ENTRY(__get_user_1)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__get_user_1)
+SYM_FUNC_END(__get_user_1)
 EXPORT_SYMBOL(__get_user_1)
 
-ENTRY(__get_user_2)
+SYM_FUNC_START(__get_user_2)
 	add $1,%_ASM_AX
 	jc bad_get_user
 	mov PER_CPU_VAR(current_task), %_ASM_DX
@@ -63,10 +63,10 @@ ENTRY(__get_user_2)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__get_user_2)
+SYM_FUNC_END(__get_user_2)
 EXPORT_SYMBOL(__get_user_2)
 
-ENTRY(__get_user_4)
+SYM_FUNC_START(__get_user_4)
 	add $3,%_ASM_AX
 	jc bad_get_user
 	mov PER_CPU_VAR(current_task), %_ASM_DX
@@ -79,10 +79,10 @@ ENTRY(__get_user_4)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__get_user_4)
+SYM_FUNC_END(__get_user_4)
 EXPORT_SYMBOL(__get_user_4)
 
-ENTRY(__get_user_8)
+SYM_FUNC_START(__get_user_8)
 #ifdef CONFIG_X86_64
 	add $7,%_ASM_AX
 	jc bad_get_user
@@ -111,7 +111,7 @@ ENTRY(__get_user_8)
 	ASM_CLAC
 	ret
 #endif
-ENDPROC(__get_user_8)
+SYM_FUNC_END(__get_user_8)
 EXPORT_SYMBOL(__get_user_8)
 
 
diff --git a/arch/x86/lib/hweight.S b/arch/x86/lib/hweight.S
index a14f9939c365..dbf8cc97b7f5 100644
--- a/arch/x86/lib/hweight.S
+++ b/arch/x86/lib/hweight.S
@@ -8,7 +8,7 @@
  * unsigned int __sw_hweight32(unsigned int w)
  * %rdi: w
  */
-ENTRY(__sw_hweight32)
+SYM_FUNC_START(__sw_hweight32)
 
 #ifdef CONFIG_X86_64
 	movl %edi, %eax				# w
@@ -33,10 +33,10 @@ ENTRY(__sw_hweight32)
 	shrl $24, %eax				# w = w_tmp >> 24
 	__ASM_SIZE(pop,) %__ASM_REG(dx)
 	ret
-ENDPROC(__sw_hweight32)
+SYM_FUNC_END(__sw_hweight32)
 EXPORT_SYMBOL(__sw_hweight32)
 
-ENTRY(__sw_hweight64)
+SYM_FUNC_START(__sw_hweight64)
 #ifdef CONFIG_X86_64
 	pushq   %rdi
 	pushq   %rdx
@@ -79,5 +79,5 @@ ENTRY(__sw_hweight64)
 	popl    %ecx
 	ret
 #endif
-ENDPROC(__sw_hweight64)
+SYM_FUNC_END(__sw_hweight64)
 EXPORT_SYMBOL(__sw_hweight64)
diff --git a/arch/x86/lib/iomap_copy_64.S b/arch/x86/lib/iomap_copy_64.S
index 33147fef3452..2246fbf32fa8 100644
--- a/arch/x86/lib/iomap_copy_64.S
+++ b/arch/x86/lib/iomap_copy_64.S
@@ -20,8 +20,8 @@
 /*
  * override generic version in lib/iomap_copy.c
  */
-ENTRY(__iowrite32_copy)
+SYM_FUNC_START(__iowrite32_copy)
 	movl %edx,%ecx
 	rep movsd
 	ret
-ENDPROC(__iowrite32_copy)
+SYM_FUNC_END(__iowrite32_copy)
diff --git a/arch/x86/lib/memcpy_64.S b/arch/x86/lib/memcpy_64.S
index 3b24dc05251c..27211a02859d 100644
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@ -192,7 +192,7 @@ MCSAFE_TEST_CTL
  * Note that we only catch machine checks when reading the source addresses.
  * Writes to target are posted and don't generate machine checks.
  */
-ENTRY(__memcpy_mcsafe)
+SYM_FUNC_START(__memcpy_mcsafe)
 	cmpl $8, %edx
 	/* Less than 8 bytes? Go to byte copy loop */
 	jb .L_no_whole_words
@@ -258,7 +258,7 @@ ENTRY(__memcpy_mcsafe)
 .L_done_memcpy_trap:
 	xorl %eax, %eax
 	ret
-ENDPROC(__memcpy_mcsafe)
+SYM_FUNC_END(__memcpy_mcsafe)
 EXPORT_SYMBOL_GPL(__memcpy_mcsafe)
 
 	.section .fixup, "ax"
* Unmerged path arch/x86/lib/memmove_64.S
* Unmerged path arch/x86/lib/memset_64.S
diff --git a/arch/x86/lib/msr-reg.S b/arch/x86/lib/msr-reg.S
index ed33cbab3958..a2b9caa5274c 100644
--- a/arch/x86/lib/msr-reg.S
+++ b/arch/x86/lib/msr-reg.S
@@ -12,7 +12,7 @@
  *
  */
 .macro op_safe_regs op
-ENTRY(\op\()_safe_regs)
+SYM_FUNC_START(\op\()_safe_regs)
 	pushq %rbx
 	pushq %r12
 	movq	%rdi, %r10	/* Save pointer */
@@ -41,13 +41,13 @@ ENTRY(\op\()_safe_regs)
 	jmp     2b
 
 	_ASM_EXTABLE(1b, 3b)
-ENDPROC(\op\()_safe_regs)
+SYM_FUNC_END(\op\()_safe_regs)
 .endm
 
 #else /* X86_32 */
 
 .macro op_safe_regs op
-ENTRY(\op\()_safe_regs)
+SYM_FUNC_START(\op\()_safe_regs)
 	pushl %ebx
 	pushl %ebp
 	pushl %esi
@@ -83,7 +83,7 @@ ENTRY(\op\()_safe_regs)
 	jmp     2b
 
 	_ASM_EXTABLE(1b, 3b)
-ENDPROC(\op\()_safe_regs)
+SYM_FUNC_END(\op\()_safe_regs)
 .endm
 
 #endif
diff --git a/arch/x86/lib/putuser.S b/arch/x86/lib/putuser.S
index 126dd6a9ec9b..742ebfaf8b56 100644
--- a/arch/x86/lib/putuser.S
+++ b/arch/x86/lib/putuser.S
@@ -34,7 +34,7 @@
 #define ENTER	mov PER_CPU_VAR(current_task), %_ASM_BX
 
 .text
-ENTRY(__put_user_1)
+SYM_FUNC_START(__put_user_1)
 	ENTER
 	cmp TASK_addr_limit(%_ASM_BX),%_ASM_CX
 	jae .Lbad_put_user
@@ -43,10 +43,10 @@ ENTRY(__put_user_1)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__put_user_1)
+SYM_FUNC_END(__put_user_1)
 EXPORT_SYMBOL(__put_user_1)
 
-ENTRY(__put_user_2)
+SYM_FUNC_START(__put_user_2)
 	ENTER
 	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX
 	sub $1,%_ASM_BX
@@ -57,10 +57,10 @@ ENTRY(__put_user_2)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__put_user_2)
+SYM_FUNC_END(__put_user_2)
 EXPORT_SYMBOL(__put_user_2)
 
-ENTRY(__put_user_4)
+SYM_FUNC_START(__put_user_4)
 	ENTER
 	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX
 	sub $3,%_ASM_BX
@@ -71,10 +71,10 @@ ENTRY(__put_user_4)
 	xor %eax,%eax
 	ASM_CLAC
 	ret
-ENDPROC(__put_user_4)
+SYM_FUNC_END(__put_user_4)
 EXPORT_SYMBOL(__put_user_4)
 
-ENTRY(__put_user_8)
+SYM_FUNC_START(__put_user_8)
 	ENTER
 	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX
 	sub $7,%_ASM_BX
@@ -88,7 +88,7 @@ ENTRY(__put_user_8)
 	xor %eax,%eax
 	ASM_CLAC
 	RET
-ENDPROC(__put_user_8)
+SYM_FUNC_END(__put_user_8)
 EXPORT_SYMBOL(__put_user_8)
 
 .Lbad_put_user_clac:
diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S
index c909961e678a..363ec132df7e 100644
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@ -11,11 +11,11 @@
 .macro THUNK reg
 	.section .text.__x86.indirect_thunk
 
-ENTRY(__x86_indirect_thunk_\reg)
+SYM_FUNC_START(__x86_indirect_thunk_\reg)
 	CFI_STARTPROC
 	JMP_NOSPEC %\reg
 	CFI_ENDPROC
-ENDPROC(__x86_indirect_thunk_\reg)
+SYM_FUNC_END(__x86_indirect_thunk_\reg)
 .endm
 
 /*
diff --git a/arch/x86/mm/mem_encrypt_boot.S b/arch/x86/mm/mem_encrypt_boot.S
index 40a6085063d6..2c0a6fbd4fe8 100644
--- a/arch/x86/mm/mem_encrypt_boot.S
+++ b/arch/x86/mm/mem_encrypt_boot.S
@@ -19,7 +19,7 @@
 
 	.text
 	.code64
-ENTRY(sme_encrypt_execute)
+SYM_FUNC_START(sme_encrypt_execute)
 
 	/*
 	 * Entry parameters:
@@ -69,9 +69,9 @@ ENTRY(sme_encrypt_execute)
 	pop	%rbp
 
 	ret
-ENDPROC(sme_encrypt_execute)
+SYM_FUNC_END(sme_encrypt_execute)
 
-ENTRY(__enc_copy)
+SYM_FUNC_START(__enc_copy)
 /*
  * Routine used to encrypt memory in place.
  *   This routine must be run outside of the kernel proper since
@@ -156,4 +156,4 @@ ENTRY(__enc_copy)
 
 	ret
 .L__enc_copy_end:
-ENDPROC(__enc_copy)
+SYM_FUNC_END(__enc_copy)
diff --git a/arch/x86/platform/efi/efi_stub_64.S b/arch/x86/platform/efi/efi_stub_64.S
index 5a7debb4a5bc..e7e1020f4ccb 100644
--- a/arch/x86/platform/efi/efi_stub_64.S
+++ b/arch/x86/platform/efi/efi_stub_64.S
@@ -10,7 +10,7 @@
 #include <linux/linkage.h>
 #include <asm/nospec-branch.h>
 
-ENTRY(efi_call)
+SYM_FUNC_START(efi_call)
 	pushq %rbp
 	movq %rsp, %rbp
 	and $~0xf, %rsp
@@ -24,4 +24,4 @@ ENTRY(efi_call)
 	CALL_NOSPEC %rdi
 	leave
 	ret
-ENDPROC(efi_call)
+SYM_FUNC_END(efi_call)
* Unmerged path arch/x86/platform/efi/efi_thunk_64.S
diff --git a/arch/x86/power/hibernate_asm_64.S b/arch/x86/power/hibernate_asm_64.S
index fd369a6e9ff8..c9b10d061423 100644
--- a/arch/x86/power/hibernate_asm_64.S
+++ b/arch/x86/power/hibernate_asm_64.S
@@ -23,7 +23,7 @@
 #include <asm/processor-flags.h>
 #include <asm/frame.h>
 
-ENTRY(swsusp_arch_suspend)
+SYM_FUNC_START(swsusp_arch_suspend)
 	movq	$saved_context, %rax
 	movq	%rsp, pt_regs_sp(%rax)
 	movq	%rbp, pt_regs_bp(%rax)
@@ -51,7 +51,7 @@ ENTRY(swsusp_arch_suspend)
 	call swsusp_save
 	FRAME_END
 	ret
-ENDPROC(swsusp_arch_suspend)
+SYM_FUNC_END(swsusp_arch_suspend)
 
 ENTRY(restore_image)
 	/* prepare to jump to the image kernel */
@@ -101,7 +101,7 @@ ENTRY(core_restore_code)
 
 	 /* code below belongs to the image kernel */
 	.align PAGE_SIZE
-ENTRY(restore_registers)
+SYM_FUNC_START(restore_registers)
 	/* go back to the original page tables */
 	movq    %r9, %cr3
 
@@ -143,4 +143,4 @@ ENTRY(restore_registers)
 	movq	%rax, in_suspend(%rip)
 
 	ret
-ENDPROC(restore_registers)
+SYM_FUNC_END(restore_registers)
* Unmerged path arch/x86/xen/xen-asm.S
* Unmerged path arch/x86/xen/xen-asm_64.S
diff --git a/include/linux/linkage.h b/include/linux/linkage.h
index f3ae8f3dea2c..a554986efa22 100644
--- a/include/linux/linkage.h
+++ b/include/linux/linkage.h
@@ -112,11 +112,13 @@
 	name:
 #endif
 
+#ifndef CONFIG_X86_64
 #ifndef ENTRY
 /* deprecated, use SYM_FUNC_START */
 #define ENTRY(name) \
 	SYM_FUNC_START(name)
 #endif
+#endif /* CONFIG_X86_64 */
 #endif /* LINKER_SCRIPT */
 
 #ifndef WEAK
@@ -131,6 +133,7 @@
 	.size name, .-name
 #endif
 
+#ifndef CONFIG_X86_64
 /* If symbol 'name' is treated as a subroutine (gets called, and returns)
  * then please use ENDPROC to mark 'name' as STT_FUNC for the benefit of
  * static analysis tools such as stack depth analyzer.
@@ -140,6 +143,7 @@
 #define ENDPROC(name) \
 	SYM_FUNC_END(name)
 #endif
+#endif /* CONFIG_X86_64 */
 
 /* === generic annotations === */
 
