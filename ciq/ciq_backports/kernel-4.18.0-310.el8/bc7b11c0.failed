x86/asm/64: Change all ENTRY+END to SYM_CODE_*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Jiri Slaby <jslaby@suse.cz>
commit bc7b11c04ee9c9b0451ebf85bf64e0de69fdbb17
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/bc7b11c0.failed

Change all assembly code which is marked using END (and not ENDPROC).
Switch all these to the appropriate new annotation SYM_CODE_START and
SYM_CODE_END.

	Signed-off-by: Jiri Slaby <jslaby@suse.cz>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com> [xen bits]
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Cao jin <caoj.fnst@cn.fujitsu.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jiri Kosina <jkosina@suse.cz>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: linux-arch@vger.kernel.org
	Cc: Maran Wilson <maran.wilson@oracle.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stefano Stabellini <sstabellini@kernel.org>
	Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: x86-ml <x86@kernel.org>
	Cc: xen-devel@lists.xenproject.org
Link: https://lkml.kernel.org/r/20191011115108.12392-24-jslaby@suse.cz
(cherry picked from commit bc7b11c04ee9c9b0451ebf85bf64e0de69fdbb17)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/entry_64.S
diff --cc arch/x86/entry/entry_64.S
index 6ebaa7c43b8e,13e4fe378e5a..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -245,7 -279,8 +245,12 @@@ SYM_CODE_END(entry_SYSCALL_64
   * %rdi: prev task
   * %rsi: next task
   */
++<<<<<<< HEAD
 +ENTRY(__switch_to_asm)
++=======
+ SYM_CODE_START(__switch_to_asm)
+ 	UNWIND_HINT_FUNC
++>>>>>>> bc7b11c04ee9 (x86/asm/64: Change all ENTRY+END to SYM_CODE_*)
  	/*
  	 * Save callee-saved registers
  	 * This must match the order in inactive_task_frame
@@@ -288,7 -321,7 +293,11 @@@
  	popq	%rbp
  
  	jmp	__switch_to
++<<<<<<< HEAD
 +ENDPROC(__switch_to_asm)
++=======
+ SYM_CODE_END(__switch_to_asm)
++>>>>>>> bc7b11c04ee9 (x86/asm/64: Change all ENTRY+END to SYM_CODE_*)
  
  /*
   * A newly forked process directly context switches into this address.
@@@ -478,8 -511,8 +487,13 @@@ SYM_CODE_END(spurious_entries_start
   * | return address					|
   * +----------------------------------------------------+
   */
++<<<<<<< HEAD
 +ENTRY(interrupt_entry)
 +	UNWIND_HINT_IRET_REGS offset=16
++=======
+ SYM_CODE_START(interrupt_entry)
+ 	UNWIND_HINT_FUNC
++>>>>>>> bc7b11c04ee9 (x86/asm/64: Change all ENTRY+END to SYM_CODE_*)
  	ASM_CLAC
  	cld
  
@@@ -547,83 -579,9 +561,83 @@@
  	TRACE_IRQS_OFF
  
  	ret
- END(interrupt_entry)
+ SYM_CODE_END(interrupt_entry)
  _ASM_NOKPROBE(interrupt_entry)
  
 +#ifdef CONFIG_AMD_MEM_ENCRYPT
 +/**
 + * idtentry_vc - Macro to generate entry stub for #VC
 + * @vector:		Vector number
 + * @asmsym:		ASM symbol for the entry point
 + * @cfunc:		C function to be called
 + *
 + * The macro emits code to set up the kernel context for #VC. The #VC handler
 + * runs on an IST stack and needs to be able to cause nested #VC exceptions.
 + *
 + * To make this work the #VC entry code tries its best to pretend it doesn't use
 + * an IST stack by switching to the task stack if coming from user-space (which
 + * includes early SYSCALL entry path) or back to the stack in the IRET frame if
 + * entered from kernel-mode.
 + *
 + * If entered from kernel-mode the return stack is validated first, and if it is
 + * not safe to use (e.g. because it points to the entry stack) the #VC handler
 + * will switch to a fall-back stack (VC2) and call a special handler function.
 + *
 + * The macro is only used for one vector, but it is planned to be extended in
 + * the future for the #HV exception.
 + */
 +.macro idtentry_vc vector asmsym cfunc
 +SYM_CODE_START(\asmsym)
 +	UNWIND_HINT_IRET_REGS
 +	ASM_CLAC
 +
 +	/*
 +	 * If the entry is from userspace, switch stacks and treat it as
 +	 * a normal entry.
 +	 */
 +	testb	$3, CS-ORIG_RAX(%rsp)
 +	jnz	.Lfrom_usermode_switch_stack_\@
 +
 +	/* paranoid_entry returns GS information for paranoid_exit in EBX. */
 +	call	paranoid_entry
 +
 +	UNWIND_HINT_REGS
 +
 +	/*
 +	 * Switch off the IST stack to make it free for nested exceptions. The
 +	 * vc_switch_off_ist() function will switch back to the interrupted
 +	 * stack if it is safe to do so. If not it switches to the VC fall-back
 +	 * stack.
 +	 */
 +	movq	%rsp, %rdi		/* pt_regs pointer */
 +	call	vc_switch_off_ist
 +	movq	%rax, %rsp		/* Switch to new stack */
 +
 +	UNWIND_HINT_REGS
 +
 +	/* Update pt_regs */
 +	movq	ORIG_RAX(%rsp), %rsi	/* get error code into 2nd argument*/
 +	movq	$-1, ORIG_RAX(%rsp)	/* no syscall to restart */
 +
 +	movq	%rsp, %rdi		/* pt_regs pointer */
 +
 +	call	\cfunc
 +
 +	/*
 +	 * No need to switch back to the IST stack. The current stack is either
 +	 * identical to the stack in the IRET frame or the VC fall-back stack,
 +	 * so it is definitly mapped even with PTI enabled.
 +	 */
 +	jmp	paranoid_exit
 +
 +	/* Switch to the regular task stack */
 +.Lfrom_usermode_switch_stack_\@:
 +	idtentry_body safe_stack_\cfunc, has_error_code=1
 +
 +_ASM_NOKPROBE(\asmsym)
 +SYM_CODE_END(\asmsym)
 +.endm
 +#endif
  
  /* Interrupt entry/exit. */
  
@@@ -984,8 -967,8 +998,13 @@@ apicinterrupt IRQ_WORK_VECTOR			irq_wor
   * @paranoid == 2 is special: the stub will never switch stacks.  This is for
   * #DF: if the thread stack is somehow unusable, we'll still get a useful OOPS.
   */
++<<<<<<< HEAD
 +.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1 ist_offset=0 create_gap=0
 +ENTRY(\sym)
++=======
+ .macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1 ist_offset=0 create_gap=0 read_cr2=0
+ SYM_CODE_START(\sym)
++>>>>>>> bc7b11c04ee9 (x86/asm/64: Change all ENTRY+END to SYM_CODE_*)
  	UNWIND_HINT_IRET_REGS offset=\has_error_code*8
  
  	/* Sanity check */
@@@ -1819,22 -1719,28 +1838,36 @@@ nmi_restore
  	 * about espfix64 on the way back to kernel mode.
  	 */
  	iretq
- END(nmi)
+ SYM_CODE_END(nmi)
  
++<<<<<<< HEAD
 +ENTRY(ignore_sysret)
 +	UNWIND_HINT_EMPTY
 +	mov	$-ENOSYS, %eax
 +	sysret
 +END(ignore_sysret)
++=======
+ #ifndef CONFIG_IA32_EMULATION
+ /*
+  * This handles SYSCALL from 32-bit code.  There is no way to program
+  * MSRs to fully disable 32-bit SYSCALL.
+  */
+ SYM_CODE_START(ignore_sysret)
+ 	UNWIND_HINT_EMPTY
+ 	mov	$-ENOSYS, %eax
+ 	sysret
+ SYM_CODE_END(ignore_sysret)
+ #endif
++>>>>>>> bc7b11c04ee9 (x86/asm/64: Change all ENTRY+END to SYM_CODE_*)
  
- ENTRY(rewind_stack_do_exit)
+ SYM_CODE_START(rewind_stack_do_exit)
  	UNWIND_HINT_FUNC
  	/* Prevent any naive code from trying to unwind to our caller. */
  	xorl	%ebp, %ebp
  
  	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rax
  	leaq	-PTREGS_SIZE(%rax), %rsp
 -	UNWIND_HINT_FUNC sp_offset=PTREGS_SIZE
 +	UNWIND_HINT_REGS
  
  	call	do_exit
- END(rewind_stack_do_exit)
+ SYM_CODE_END(rewind_stack_do_exit)
* Unmerged path arch/x86/entry/entry_64.S
diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S
index 0f660ab3af8f..328bebb5ccbf 100644
--- a/arch/x86/entry/entry_64_compat.S
+++ b/arch/x86/entry/entry_64_compat.S
@@ -199,7 +199,7 @@ ENDPROC(entry_SYSENTER_compat)
  * esp  user stack
  * 0(%esp) arg6
  */
-ENTRY(entry_SYSCALL_compat)
+SYM_CODE_START(entry_SYSCALL_compat)
 	/* Interrupts are off on entry. */
 	swapgs
 
@@ -312,7 +312,7 @@ sysret32_from_system_call:
 	xorl	%r10d, %r10d
 	swapgs
 	sysretl
-END(entry_SYSCALL_compat)
+SYM_CODE_END(entry_SYSCALL_compat)
 
 /*
  * 32-bit legacy system call entry.
@@ -340,7 +340,7 @@ END(entry_SYSCALL_compat)
  * edi  arg5
  * ebp  arg6
  */
-ENTRY(entry_INT80_compat)
+SYM_CODE_START(entry_INT80_compat)
 	/*
 	 * Interrupts are off on entry.
 	 */
@@ -417,4 +417,4 @@ ENTRY(entry_INT80_compat)
 	/* Go back to user mode. */
 	TRACE_IRQS_ON
 	jmp	swapgs_restore_regs_and_return_to_usermode
-END(entry_INT80_compat)
+SYM_CODE_END(entry_INT80_compat)
diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S
index b09dfa3df148..52ce53dfe76f 100644
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@ -326,7 +326,7 @@ ENTRY(ftrace_graph_caller)
 	retq
 ENDPROC(ftrace_graph_caller)
 
-ENTRY(return_to_handler)
+SYM_CODE_START(return_to_handler)
 	UNWIND_HINT_EMPTY
 	subq  $24, %rsp
 
@@ -342,5 +342,5 @@ ENTRY(return_to_handler)
 	movq (%rsp), %rax
 	addq $24, %rsp
 	JMP_NOSPEC %rdi
-END(return_to_handler)
+SYM_CODE_END(return_to_handler)
 #endif
diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S
index 9f98aa625962..1acfea5805c6 100644
--- a/arch/x86/kernel/head_64.S
+++ b/arch/x86/kernel/head_64.S
@@ -107,7 +107,7 @@ SYM_CODE_START_NOALIGN(startup_64)
 	jmp 1f
 SYM_CODE_END(startup_64)
 
-ENTRY(secondary_startup_64)
+SYM_CODE_START(secondary_startup_64)
 	UNWIND_HINT_EMPTY
 	/*
 	 * At this point the CPU runs in 64bit mode CS.L = 1 CS.D = 0,
@@ -292,7 +292,7 @@ SYM_INNER_LABEL(secondary_startup_64_no_verify, SYM_L_GLOBAL)
 	pushq	%rax		# target address in negative space
 	lretq
 .Lafter_lret:
-END(secondary_startup_64)
+SYM_CODE_END(secondary_startup_64)
 
 #include "verify_cpu.S"
 #include "sev_verify_cbit.S"
@@ -303,11 +303,11 @@ END(secondary_startup_64)
  * up already except stack. We just set up stack here. Then call
  * start_secondary() via .Ljump_to_C_code.
  */
-ENTRY(start_cpu0)
+SYM_CODE_START(start_cpu0)
 	UNWIND_HINT_EMPTY
 	movq	initial_stack(%rip), %rsp
 	jmp	.Ljump_to_C_code
-END(start_cpu0)
+SYM_CODE_END(start_cpu0)
 #endif
 
 #ifdef CONFIG_AMD_MEM_ENCRYPT
@@ -362,7 +362,7 @@ SYM_DATA(initial_vc_handler,   .quad handle_vc_boot_ghcb)
 	__FINITDATA
 
 	__INIT
-ENTRY(early_idt_handler_array)
+SYM_CODE_START(early_idt_handler_array)
 	i = 0
 	.rept NUM_EXCEPTION_VECTORS
 	.if ((EXCEPTION_ERRCODE_MASK >> i) & 1) == 0
@@ -378,7 +378,7 @@ ENTRY(early_idt_handler_array)
 	.fill early_idt_handler_array + i*EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc
 	.endr
 	UNWIND_HINT_IRET_REGS offset=16
-END(early_idt_handler_array)
+SYM_CODE_END(early_idt_handler_array)
 
 early_idt_handler_common:
 	/*
diff --git a/arch/x86/xen/xen-asm_64.S b/arch/x86/xen/xen-asm_64.S
index 1e9ef0ba30a5..c0e61bcf44e3 100644
--- a/arch/x86/xen/xen-asm_64.S
+++ b/arch/x86/xen/xen-asm_64.S
@@ -20,11 +20,11 @@
 #include <linux/linkage.h>
 
 .macro xen_pv_trap name
-ENTRY(xen_\name)
+SYM_CODE_START(xen_\name)
 	pop %rcx
 	pop %r11
 	jmp  \name
-END(xen_\name)
+SYM_CODE_END(xen_\name)
 _ASM_NOKPROBE(xen_\name)
 .endm
 
@@ -58,7 +58,7 @@ xen_pv_trap entry_INT80_compat
 xen_pv_trap hypervisor_callback
 
 	__INIT
-ENTRY(xen_early_idt_handler_array)
+SYM_CODE_START(xen_early_idt_handler_array)
 	i = 0
 	.rept NUM_EXCEPTION_VECTORS
 	pop %rcx
@@ -67,7 +67,7 @@ ENTRY(xen_early_idt_handler_array)
 	i = i + 1
 	.fill xen_early_idt_handler_array + i*XEN_EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc
 	.endr
-END(xen_early_idt_handler_array)
+SYM_CODE_END(xen_early_idt_handler_array)
 	__FINIT
 
 hypercall_iret = hypercall_page + __HYPERVISOR_iret * 32
diff --git a/arch/x86/xen/xen-head.S b/arch/x86/xen/xen-head.S
index c1d8b90aa4e2..1d0cee3163e4 100644
--- a/arch/x86/xen/xen-head.S
+++ b/arch/x86/xen/xen-head.S
@@ -22,7 +22,7 @@
 
 #ifdef CONFIG_XEN_PV
 	__INIT
-ENTRY(startup_xen)
+SYM_CODE_START(startup_xen)
 	UNWIND_HINT_EMPTY
 	cld
 
@@ -52,13 +52,13 @@ ENTRY(startup_xen)
 #endif
 
 	jmp xen_start_kernel
-END(startup_xen)
+SYM_CODE_END(startup_xen)
 	__FINIT
 #endif
 
 .pushsection .text
 	.balign PAGE_SIZE
-ENTRY(hypercall_page)
+SYM_CODE_START(hypercall_page)
 	.rept (PAGE_SIZE / 32)
 		UNWIND_HINT_EMPTY
 		.skip 32
@@ -69,7 +69,7 @@ ENTRY(hypercall_page)
 	.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32
 #include <asm/xen-hypercalls.h>
 #undef HYPERCALL
-END(hypercall_page)
+SYM_CODE_END(hypercall_page)
 .popsection
 
 	ELFNOTE(Xen, XEN_ELFNOTE_GUEST_OS,       .asciz "linux")
