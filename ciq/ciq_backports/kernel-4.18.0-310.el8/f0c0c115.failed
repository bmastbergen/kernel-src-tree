mm: memcontrol: account pagetables per node

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit f0c0c115fb81940f4dba0644ac2a8a43b39c83f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/f0c0c115.failed

For many workloads, pagetable consumption is significant and it makes
sense to expose it in the memory.stat for the memory cgroups.  However at
the moment, the pagetables are accounted per-zone.  Converting them to
per-node and using the right interface will correctly account for the
memory cgroups as well.

[akpm@linux-foundation.org: export __mod_lruvec_page_state to modules for arch/mips/kvm/]

Link: https://lkml.kernel.org/r/20201130212541.2781790-3-shakeelb@google.com
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Roman Gushchin <guro@fb.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f0c0c115fb81940f4dba0644ac2a8a43b39c83f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/cgroup-v2.rst
#	drivers/base/node.c
#	fs/proc/meminfo.c
#	include/linux/mm.h
#	include/linux/mmzone.h
#	mm/memcontrol.c
#	mm/page_alloc.c
#	mm/vmstat.c
diff --cc Documentation/admin-guide/cgroup-v2.rst
index bc0fc04d2632,63521cd36ce5..000000000000
--- a/Documentation/admin-guide/cgroup-v2.rst
+++ b/Documentation/admin-guide/cgroup-v2.rst
@@@ -1238,11 -1274,10 +1238,18 @@@ PAGE_SIZE multiple when read back
  	  kernel_stack
  		Amount of memory allocated to kernel stacks.
  
++<<<<<<< HEAD
 +	  slab
 +		Amount of memory used for storing in-kernel data
 +		structures.
 +
 +	  percpu
++=======
+ 	  pagetables
+                 Amount of memory allocated for page tables.
+ 
+ 	  percpu(npn)
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  		Amount of memory used for storing per-cpu kernel
  		data structures.
  
diff --cc drivers/base/node.c
index 8633d5d90a7d,04f71c7bc3f8..000000000000
--- a/drivers/base/node.c
+++ b/drivers/base/node.c
@@@ -369,99 -376,105 +369,122 @@@ static ssize_t node_read_meminfo(struc
  	si_meminfo_node(&i, nid);
  	sreclaimable = node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B);
  	sunreclaimable = node_page_state_pages(pgdat, NR_SLAB_UNRECLAIMABLE_B);
 -	len = sysfs_emit_at(buf, len,
 -			    "Node %d MemTotal:       %8lu kB\n"
 -			    "Node %d MemFree:        %8lu kB\n"
 -			    "Node %d MemUsed:        %8lu kB\n"
 -			    "Node %d Active:         %8lu kB\n"
 -			    "Node %d Inactive:       %8lu kB\n"
 -			    "Node %d Active(anon):   %8lu kB\n"
 -			    "Node %d Inactive(anon): %8lu kB\n"
 -			    "Node %d Active(file):   %8lu kB\n"
 -			    "Node %d Inactive(file): %8lu kB\n"
 -			    "Node %d Unevictable:    %8lu kB\n"
 -			    "Node %d Mlocked:        %8lu kB\n",
 -			    nid, K(i.totalram),
 -			    nid, K(i.freeram),
 -			    nid, K(i.totalram - i.freeram),
 -			    nid, K(node_page_state(pgdat, NR_ACTIVE_ANON) +
 -				   node_page_state(pgdat, NR_ACTIVE_FILE)),
 -			    nid, K(node_page_state(pgdat, NR_INACTIVE_ANON) +
 -				   node_page_state(pgdat, NR_INACTIVE_FILE)),
 -			    nid, K(node_page_state(pgdat, NR_ACTIVE_ANON)),
 -			    nid, K(node_page_state(pgdat, NR_INACTIVE_ANON)),
 -			    nid, K(node_page_state(pgdat, NR_ACTIVE_FILE)),
 -			    nid, K(node_page_state(pgdat, NR_INACTIVE_FILE)),
 -			    nid, K(node_page_state(pgdat, NR_UNEVICTABLE)),
 -			    nid, K(sum_zone_node_page_state(nid, NR_MLOCK)));
 +	n = sprintf(buf,
 +		       "Node %d MemTotal:       %8lu kB\n"
 +		       "Node %d MemFree:        %8lu kB\n"
 +		       "Node %d MemUsed:        %8lu kB\n"
 +		       "Node %d Active:         %8lu kB\n"
 +		       "Node %d Inactive:       %8lu kB\n"
 +		       "Node %d Active(anon):   %8lu kB\n"
 +		       "Node %d Inactive(anon): %8lu kB\n"
 +		       "Node %d Active(file):   %8lu kB\n"
 +		       "Node %d Inactive(file): %8lu kB\n"
 +		       "Node %d Unevictable:    %8lu kB\n"
 +		       "Node %d Mlocked:        %8lu kB\n",
 +		       nid, K(i.totalram),
 +		       nid, K(i.freeram),
 +		       nid, K(i.totalram - i.freeram),
 +		       nid, K(node_page_state(pgdat, NR_ACTIVE_ANON) +
 +				node_page_state(pgdat, NR_ACTIVE_FILE)),
 +		       nid, K(node_page_state(pgdat, NR_INACTIVE_ANON) +
 +				node_page_state(pgdat, NR_INACTIVE_FILE)),
 +		       nid, K(node_page_state(pgdat, NR_ACTIVE_ANON)),
 +		       nid, K(node_page_state(pgdat, NR_INACTIVE_ANON)),
 +		       nid, K(node_page_state(pgdat, NR_ACTIVE_FILE)),
 +		       nid, K(node_page_state(pgdat, NR_INACTIVE_FILE)),
 +		       nid, K(node_page_state(pgdat, NR_UNEVICTABLE)),
 +		       nid, K(sum_zone_node_page_state(nid, NR_MLOCK)));
  
  #ifdef CONFIG_HIGHMEM
 -	len += sysfs_emit_at(buf, len,
 -			     "Node %d HighTotal:      %8lu kB\n"
 -			     "Node %d HighFree:       %8lu kB\n"
 -			     "Node %d LowTotal:       %8lu kB\n"
 -			     "Node %d LowFree:        %8lu kB\n",
 -			     nid, K(i.totalhigh),
 -			     nid, K(i.freehigh),
 -			     nid, K(i.totalram - i.totalhigh),
 -			     nid, K(i.freeram - i.freehigh));
 +	n += sprintf(buf + n,
 +		       "Node %d HighTotal:      %8lu kB\n"
 +		       "Node %d HighFree:       %8lu kB\n"
 +		       "Node %d LowTotal:       %8lu kB\n"
 +		       "Node %d LowFree:        %8lu kB\n",
 +		       nid, K(i.totalhigh),
 +		       nid, K(i.freehigh),
 +		       nid, K(i.totalram - i.totalhigh),
 +		       nid, K(i.freeram - i.freehigh));
  #endif
 -	len += sysfs_emit_at(buf, len,
 -			     "Node %d Dirty:          %8lu kB\n"
 -			     "Node %d Writeback:      %8lu kB\n"
 -			     "Node %d FilePages:      %8lu kB\n"
 -			     "Node %d Mapped:         %8lu kB\n"
 -			     "Node %d AnonPages:      %8lu kB\n"
 -			     "Node %d Shmem:          %8lu kB\n"
 -			     "Node %d KernelStack:    %8lu kB\n"
 -#ifdef CONFIG_SHADOW_CALL_STACK
 -			     "Node %d ShadowCallStack:%8lu kB\n"
 -#endif
 -			     "Node %d PageTables:     %8lu kB\n"
 -			     "Node %d NFS_Unstable:   %8lu kB\n"
 -			     "Node %d Bounce:         %8lu kB\n"
 -			     "Node %d WritebackTmp:   %8lu kB\n"
 -			     "Node %d KReclaimable:   %8lu kB\n"
 -			     "Node %d Slab:           %8lu kB\n"
 -			     "Node %d SReclaimable:   %8lu kB\n"
 -			     "Node %d SUnreclaim:     %8lu kB\n"
 +	n += sprintf(buf + n,
 +		       "Node %d Dirty:          %8lu kB\n"
 +		       "Node %d Writeback:      %8lu kB\n"
 +		       "Node %d FilePages:      %8lu kB\n"
 +		       "Node %d Mapped:         %8lu kB\n"
 +		       "Node %d AnonPages:      %8lu kB\n"
 +		       "Node %d Shmem:          %8lu kB\n"
 +		       "Node %d KernelStack:    %8lu kB\n"
 +		       "Node %d PageTables:     %8lu kB\n"
 +		       "Node %d NFS_Unstable:   %8lu kB\n"
 +		       "Node %d Bounce:         %8lu kB\n"
 +		       "Node %d WritebackTmp:   %8lu kB\n"
 +		       "Node %d KReclaimable:   %8lu kB\n"
 +		       "Node %d Slab:           %8lu kB\n"
 +		       "Node %d SReclaimable:   %8lu kB\n"
 +		       "Node %d SUnreclaim:     %8lu kB\n"
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -			     "Node %d AnonHugePages:  %8lu kB\n"
 -			     "Node %d ShmemHugePages: %8lu kB\n"
 -			     "Node %d ShmemPmdMapped: %8lu kB\n"
 -			     "Node %d FileHugePages: %8lu kB\n"
 -			     "Node %d FilePmdMapped: %8lu kB\n"
 +		       "Node %d AnonHugePages:  %8lu kB\n"
 +		       "Node %d ShmemHugePages: %8lu kB\n"
 +		       "Node %d ShmemPmdMapped: %8lu kB\n"
 +		       "Node %d FileHugePages: %8lu kB\n"
 +		       "Node %d FilePmdMapped: %8lu kB\n"
  #endif
++<<<<<<< HEAD
 +			,
 +		       nid, K(node_page_state(pgdat, NR_FILE_DIRTY)),
 +		       nid, K(node_page_state(pgdat, NR_WRITEBACK)),
 +		       nid, K(node_page_state(pgdat, NR_FILE_PAGES)),
 +		       nid, K(node_page_state(pgdat, NR_FILE_MAPPED)),
 +		       nid, K(node_page_state(pgdat, NR_ANON_MAPPED)),
 +		       nid, K(i.sharedram),
 +		       nid, sum_zone_node_page_state(nid, NR_KERNEL_STACK_KB),
 +		       nid, K(sum_zone_node_page_state(nid, NR_PAGETABLE)),
 +		       nid, 0UL,
 +		       nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),
 +		       nid, K(node_page_state(pgdat, NR_WRITEBACK_TEMP)),
 +		       nid, K(sreclaimable +
 +			      node_page_state(pgdat, NR_KERNEL_MISC_RECLAIMABLE)),
 +		       nid, K(sreclaimable + sunreclaimable),
 +		       nid, K(sreclaimable),
 +		       nid, K(sunreclaimable)
++=======
+ 			     ,
+ 			     nid, K(node_page_state(pgdat, NR_FILE_DIRTY)),
+ 			     nid, K(node_page_state(pgdat, NR_WRITEBACK)),
+ 			     nid, K(node_page_state(pgdat, NR_FILE_PAGES)),
+ 			     nid, K(node_page_state(pgdat, NR_FILE_MAPPED)),
+ 			     nid, K(node_page_state(pgdat, NR_ANON_MAPPED)),
+ 			     nid, K(i.sharedram),
+ 			     nid, node_page_state(pgdat, NR_KERNEL_STACK_KB),
+ #ifdef CONFIG_SHADOW_CALL_STACK
+ 			     nid, node_page_state(pgdat, NR_KERNEL_SCS_KB),
+ #endif
+ 			     nid, K(node_page_state(pgdat, NR_PAGETABLE)),
+ 			     nid, 0UL,
+ 			     nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),
+ 			     nid, K(node_page_state(pgdat, NR_WRITEBACK_TEMP)),
+ 			     nid, K(sreclaimable +
+ 				    node_page_state(pgdat, NR_KERNEL_MISC_RECLAIMABLE)),
+ 			     nid, K(sreclaimable + sunreclaimable),
+ 			     nid, K(sreclaimable),
+ 			     nid, K(sunreclaimable)
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -			     ,
 -			     nid, K(node_page_state(pgdat, NR_ANON_THPS) *
 -				    HPAGE_PMD_NR),
 -			     nid, K(node_page_state(pgdat, NR_SHMEM_THPS) *
 -				    HPAGE_PMD_NR),
 -			     nid, K(node_page_state(pgdat, NR_SHMEM_PMDMAPPED) *
 -				    HPAGE_PMD_NR),
 -			     nid, K(node_page_state(pgdat, NR_FILE_THPS) *
 -				    HPAGE_PMD_NR),
 -			     nid, K(node_page_state(pgdat, NR_FILE_PMDMAPPED) *
 -				    HPAGE_PMD_NR)
 +		       ,
 +		       nid, K(node_page_state(pgdat, NR_ANON_THPS) *
 +				       HPAGE_PMD_NR),
 +		       nid, K(node_page_state(pgdat, NR_SHMEM_THPS) *
 +				       HPAGE_PMD_NR),
 +		       nid, K(node_page_state(pgdat, NR_SHMEM_PMDMAPPED) *
 +				       HPAGE_PMD_NR),
 +		       nid, K(node_page_state(pgdat, NR_FILE_THPS) *
 +				       HPAGE_PMD_NR),
 +		       nid, K(node_page_state(pgdat, NR_FILE_PMDMAPPED) *
 +				       HPAGE_PMD_NR)
  #endif
 -			    );
 -	len += hugetlb_report_node_meminfo(buf, len, nid);
 -	return len;
 +		       );
 +	n += hugetlb_report_node_meminfo(nid, buf + n);
 +	return n;
  }
  
  #undef K
diff --cc fs/proc/meminfo.c
index 89a6b114397f,d6fc74619625..000000000000
--- a/fs/proc/meminfo.c
+++ b/fs/proc/meminfo.c
@@@ -103,12 -101,13 +103,16 @@@ static int meminfo_proc_show(struct seq
  	show_val_kb(m, "SReclaimable:   ", sreclaimable);
  	show_val_kb(m, "SUnreclaim:     ", sunreclaim);
  	seq_printf(m, "KernelStack:    %8lu kB\n",
 -		   global_node_page_state(NR_KERNEL_STACK_KB));
 -#ifdef CONFIG_SHADOW_CALL_STACK
 -	seq_printf(m, "ShadowCallStack:%8lu kB\n",
 -		   global_node_page_state(NR_KERNEL_SCS_KB));
 -#endif
 +		   global_zone_page_state(NR_KERNEL_STACK_KB));
  	show_val_kb(m, "PageTables:     ",
++<<<<<<< HEAD
 +		    global_zone_page_state(NR_PAGETABLE));
 +#ifdef CONFIG_QUICKLIST
 +	show_val_kb(m, "Quicklists:     ", quicklist_total_size());
 +#endif
++=======
+ 		    global_node_page_state(NR_PAGETABLE));
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  
  	show_val_kb(m, "NFS_Unstable:   ", 0);
  	show_val_kb(m, "Bounce:         ",
diff --cc include/linux/mm.h
index 32403a098175,5bbbf4aeee94..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -2131,6 -2293,22 +2131,25 @@@ static inline spinlock_t *pmd_lock(stru
  	return ptl;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool pgtable_pmd_page_ctor(struct page *page)
+ {
+ 	if (!pmd_ptlock_init(page))
+ 		return false;
+ 	__SetPageTable(page);
+ 	inc_lruvec_page_state(page, NR_PAGETABLE);
+ 	return true;
+ }
+ 
+ static inline void pgtable_pmd_page_dtor(struct page *page)
+ {
+ 	pmd_ptlock_free(page);
+ 	__ClearPageTable(page);
+ 	dec_lruvec_page_state(page, NR_PAGETABLE);
+ }
+ 
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  /*
   * No scalability reason to split PUD locks yet, but follow the same pattern
   * as the PMD locks to make it easier if we decide to.  The VM should not be
diff --cc include/linux/mmzone.h
index ae118ed2cb9b,cca2a4443c9c..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -156,8 -152,6 +156,11 @@@ enum zone_stat_item 
  	NR_ZONE_UNEVICTABLE,
  	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
  	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
++<<<<<<< HEAD
 +	NR_PAGETABLE,		/* used for pagetables */
 +	NR_KERNEL_STACK_KB,	/* measured in KiB */
++=======
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  	/* Second 128 byte cacheline */
  	NR_BOUNCE,
  #if IS_ENABLED(CONFIG_ZSMALLOC)
@@@ -206,9 -199,14 +209,20 @@@ enum node_stat_item 
  	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
  	NR_DIRTIED,		/* page dirtyings since bootup */
  	NR_WRITTEN,		/* page writings since bootup */
++<<<<<<< HEAD
 +	RH_KABI_RENAME(NR_INDIRECTLY_RECLAIMABLE_BYTES,
 +		       NR_KERNEL_MISC_RECLAIMABLE),
 +				/* reclaimable non-slab kernel pages */
++=======
+ 	NR_KERNEL_MISC_RECLAIMABLE,	/* reclaimable non-slab kernel pages */
+ 	NR_FOLL_PIN_ACQUIRED,	/* via: pin_user_page(), gup flag: FOLL_PIN */
+ 	NR_FOLL_PIN_RELEASED,	/* pages returned via unpin_user_page() */
+ 	NR_KERNEL_STACK_KB,	/* measured in KiB */
+ #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)
+ 	NR_KERNEL_SCS_KB,	/* measured in KiB */
+ #endif
+ 	NR_PAGETABLE,		/* used for pagetables */
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  	NR_VM_NODE_STAT_ITEMS
  };
  
diff --cc mm/memcontrol.c
index 867d18f31616,b9419a3605eb..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -867,7 -853,25 +867,29 @@@ void __mod_lruvec_state(struct lruvec *
  		__mod_memcg_lruvec_state(lruvec, idx, val);
  }
  
++<<<<<<< HEAD
 +void __mod_lruvec_slab_state(void *p, enum node_stat_item idx, int val)
++=======
+ void __mod_lruvec_page_state(struct page *page, enum node_stat_item idx,
+ 			     int val)
+ {
+ 	struct page *head = compound_head(page); /* rmap on tail pages */
+ 	pg_data_t *pgdat = page_pgdat(page);
+ 	struct lruvec *lruvec;
+ 
+ 	/* Untracked pages have no memcg, no lruvec. Update only the node */
+ 	if (!head->mem_cgroup) {
+ 		__mod_node_page_state(pgdat, idx, val);
+ 		return;
+ 	}
+ 
+ 	lruvec = mem_cgroup_lruvec(head->mem_cgroup, pgdat);
+ 	__mod_lruvec_state(lruvec, idx, val);
+ }
+ EXPORT_SYMBOL(__mod_lruvec_page_state);
+ 
+ void __mod_lruvec_kmem_state(void *p, enum node_stat_item idx, int val)
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  {
  	pg_data_t *pgdat = page_pgdat(virt_to_page(p));
  	struct mem_cgroup *memcg;
@@@ -1468,6 -1484,75 +1490,78 @@@ static bool mem_cgroup_wait_acct_move(s
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ struct memory_stat {
+ 	const char *name;
+ 	unsigned int ratio;
+ 	unsigned int idx;
+ };
+ 
+ static struct memory_stat memory_stats[] = {
+ 	{ "anon", PAGE_SIZE, NR_ANON_MAPPED },
+ 	{ "file", PAGE_SIZE, NR_FILE_PAGES },
+ 	{ "kernel_stack", 1024, NR_KERNEL_STACK_KB },
+ 	{ "pagetables", PAGE_SIZE, NR_PAGETABLE },
+ 	{ "percpu", 1, MEMCG_PERCPU_B },
+ 	{ "sock", PAGE_SIZE, MEMCG_SOCK },
+ 	{ "shmem", PAGE_SIZE, NR_SHMEM },
+ 	{ "file_mapped", PAGE_SIZE, NR_FILE_MAPPED },
+ 	{ "file_dirty", PAGE_SIZE, NR_FILE_DIRTY },
+ 	{ "file_writeback", PAGE_SIZE, NR_WRITEBACK },
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/*
+ 	 * The ratio will be initialized in memory_stats_init(). Because
+ 	 * on some architectures, the macro of HPAGE_PMD_SIZE is not
+ 	 * constant(e.g. powerpc).
+ 	 */
+ 	{ "anon_thp", 0, NR_ANON_THPS },
+ 	{ "file_thp", 0, NR_FILE_THPS },
+ 	{ "shmem_thp", 0, NR_SHMEM_THPS },
+ #endif
+ 	{ "inactive_anon", PAGE_SIZE, NR_INACTIVE_ANON },
+ 	{ "active_anon", PAGE_SIZE, NR_ACTIVE_ANON },
+ 	{ "inactive_file", PAGE_SIZE, NR_INACTIVE_FILE },
+ 	{ "active_file", PAGE_SIZE, NR_ACTIVE_FILE },
+ 	{ "unevictable", PAGE_SIZE, NR_UNEVICTABLE },
+ 
+ 	/*
+ 	 * Note: The slab_reclaimable and slab_unreclaimable must be
+ 	 * together and slab_reclaimable must be in front.
+ 	 */
+ 	{ "slab_reclaimable", 1, NR_SLAB_RECLAIMABLE_B },
+ 	{ "slab_unreclaimable", 1, NR_SLAB_UNRECLAIMABLE_B },
+ 
+ 	/* The memory events */
+ 	{ "workingset_refault_anon", 1, WORKINGSET_REFAULT_ANON },
+ 	{ "workingset_refault_file", 1, WORKINGSET_REFAULT_FILE },
+ 	{ "workingset_activate_anon", 1, WORKINGSET_ACTIVATE_ANON },
+ 	{ "workingset_activate_file", 1, WORKINGSET_ACTIVATE_FILE },
+ 	{ "workingset_restore_anon", 1, WORKINGSET_RESTORE_ANON },
+ 	{ "workingset_restore_file", 1, WORKINGSET_RESTORE_FILE },
+ 	{ "workingset_nodereclaim", 1, WORKINGSET_NODERECLAIM },
+ };
+ 
+ static int __init memory_stats_init(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(memory_stats); i++) {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 		if (memory_stats[i].idx == NR_ANON_THPS ||
+ 		    memory_stats[i].idx == NR_FILE_THPS ||
+ 		    memory_stats[i].idx == NR_SHMEM_THPS)
+ 			memory_stats[i].ratio = HPAGE_PMD_SIZE;
+ #endif
+ 		VM_BUG_ON(!memory_stats[i].ratio);
+ 		VM_BUG_ON(memory_stats[i].idx >= MEMCG_NR_STAT);
+ 	}
+ 
+ 	return 0;
+ }
+ pure_initcall(memory_stats_init);
+ 
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  static char *memory_stat_format(struct mem_cgroup *memcg)
  {
  	struct seq_buf s;
diff --cc mm/page_alloc.c
index 8c7425895151,743fb2bccecc..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -5390,6 -5493,11 +5390,14 @@@ void show_free_areas(unsigned int filte
  			" anon_thp: %lukB"
  #endif
  			" writeback_tmp:%lukB"
++<<<<<<< HEAD
++=======
+ 			" kernel_stack:%lukB"
+ #ifdef CONFIG_SHADOW_CALL_STACK
+ 			" shadow_call_stack:%lukB"
+ #endif
+ 			" pagetables:%lukB"
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  			" all_unreclaimable? %s"
  			"\n",
  			pgdat->node_id,
@@@ -5411,6 -5519,11 +5419,14 @@@
  			K(node_page_state(pgdat, NR_ANON_THPS) * HPAGE_PMD_NR),
  #endif
  			K(node_page_state(pgdat, NR_WRITEBACK_TEMP)),
++<<<<<<< HEAD
++=======
+ 			node_page_state(pgdat, NR_KERNEL_STACK_KB),
+ #ifdef CONFIG_SHADOW_CALL_STACK
+ 			node_page_state(pgdat, NR_KERNEL_SCS_KB),
+ #endif
+ 			K(node_page_state(pgdat, NR_PAGETABLE)),
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  			pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?
  				"yes" : "no");
  	}
@@@ -5441,8 -5555,6 +5457,11 @@@
  			" present:%lukB"
  			" managed:%lukB"
  			" mlocked:%lukB"
++<<<<<<< HEAD
 +			" kernel_stack:%lukB"
 +			" pagetables:%lukB"
++=======
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  			" bounce:%lukB"
  			" free_pcp:%lukB"
  			" local_pcp:%ukB"
@@@ -5462,8 -5575,6 +5481,11 @@@
  			K(zone->present_pages),
  			K(zone_managed_pages(zone)),
  			K(zone_page_state(zone, NR_MLOCK)),
++<<<<<<< HEAD
 +			zone_page_state(zone, NR_KERNEL_STACK_KB),
 +			K(zone_page_state(zone, NR_PAGETABLE)),
++=======
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  			K(zone_page_state(zone, NR_BOUNCE)),
  			K(free_pcp),
  			K(this_cpu_read(zone->pageset->pcp.count)),
diff --cc mm/vmstat.c
index d9662e48d82b,da36e3b0aab2..000000000000
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@@ -1156,8 -1157,6 +1156,11 @@@ const char * const vmstat_text[] = 
  	"nr_zone_unevictable",
  	"nr_zone_write_pending",
  	"nr_mlock",
++<<<<<<< HEAD
 +	"nr_page_table_pages",
 +	"nr_kernel_stack",
++=======
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  	"nr_bounce",
  #if IS_ENABLED(CONFIG_ZSMALLOC)
  	"nr_zspages",
@@@ -1209,6 -1208,13 +1212,16 @@@
  	"nr_dirtied",
  	"nr_written",
  	"nr_kernel_misc_reclaimable",
++<<<<<<< HEAD
++=======
+ 	"nr_foll_pin_acquired",
+ 	"nr_foll_pin_released",
+ 	"nr_kernel_stack",
+ #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)
+ 	"nr_shadow_call_stack",
+ #endif
+ 	"nr_page_table_pages",
++>>>>>>> f0c0c115fb81 (mm: memcontrol: account pagetables per node)
  
  	/* enum writeback_stat_item counters */
  	"nr_dirty_threshold",
* Unmerged path Documentation/admin-guide/cgroup-v2.rst
diff --git a/arch/nds32/mm/mm-nds32.c b/arch/nds32/mm/mm-nds32.c
index 3b43798d754f..436cadee9f15 100644
--- a/arch/nds32/mm/mm-nds32.c
+++ b/arch/nds32/mm/mm-nds32.c
@@ -32,8 +32,8 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 	cpu_dcache_wb_range((unsigned long)new_pgd,
 			    (unsigned long)new_pgd +
 			    PTRS_PER_PGD * sizeof(pgd_t));
-	inc_zone_page_state(virt_to_page((unsigned long *)new_pgd),
-			    NR_PAGETABLE);
+	inc_lruvec_page_state(virt_to_page((unsigned long *)new_pgd),
+			      NR_PAGETABLE);
 
 	return new_pgd;
 }
@@ -57,7 +57,7 @@ void pgd_free(struct mm_struct *mm, pgd_t * pgd)
 
 	pte = pmd_page(*pmd);
 	pmd_clear(pmd);
-	dec_zone_page_state(virt_to_page((unsigned long *)pgd), NR_PAGETABLE);
+	dec_lruvec_page_state(virt_to_page((unsigned long *)pgd), NR_PAGETABLE);
 	pte_free(mm, pte);
 	mm_dec_nr_ptes(mm);
 	pmd_free(mm, pmd);
* Unmerged path drivers/base/node.c
* Unmerged path fs/proc/meminfo.c
* Unmerged path include/linux/mm.h
* Unmerged path include/linux/mmzone.h
* Unmerged path mm/memcontrol.c
* Unmerged path mm/page_alloc.c
* Unmerged path mm/vmstat.c
