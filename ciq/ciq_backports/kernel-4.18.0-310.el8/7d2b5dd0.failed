sched/numa: Allow a floating imbalance between NUMA nodes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit 7d2b5dd0bcc48095651f1b85f751eef610b3e034
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/7d2b5dd0.failed

Currently, an imbalance is only allowed when a destination node
is almost completely idle. This solved one basic class of problems
and was the cautious approach.

This patch revisits the possibility that NUMA nodes can be imbalanced
until 25% of the CPUs are occupied. The reasoning behind 25% is somewhat
superficial -- it's half the cores when HT is enabled.  At higher
utilisations, balancing should continue as normal and keep things even
until scheduler domains are fully busy or over utilised.

Note that this is not expected to be a universal win. Any benchmark
that prefers spreading as wide as possible with limited communication
will favour the old behaviour as there is more memory bandwidth.
Workloads that communicate heavily in pairs such as netperf or tbench
benefit. For the tests I ran, the vast majority of workloads saw
a benefit so it seems to be a worthwhile trade-off.

	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lkml.kernel.org/r/20201120090630.3286-4-mgorman@techsingularity.net
(cherry picked from commit 7d2b5dd0bcc48095651f1b85f751eef610b3e034)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index f6a2a903388f,377c77b35751..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -8904,21 -8997,16 +8906,24 @@@ next_group
  
  #define NUMA_IMBALANCE_MIN 2
  
- static inline long adjust_numa_imbalance(int imbalance, int dst_running)
+ static inline long adjust_numa_imbalance(int imbalance,
+ 				int dst_running, int dst_weight)
  {
++<<<<<<< HEAD
 +/* RHEL only: This is crippled because it causes performance losses on some 
 + * workloads. */
 +#if 0
 +	unsigned int imbalance_min;
 +
++=======
++>>>>>>> 7d2b5dd0bcc4 (sched/numa: Allow a floating imbalance between NUMA nodes)
  	/*
  	 * Allow a small imbalance based on a simple pair of communicating
- 	 * tasks that remain local when the source domain is almost idle.
+ 	 * tasks that remain local when the destination is lightly loaded.
  	 */
- 	imbalance_min = NUMA_IMBALANCE_MIN;
- 	if (dst_running <= imbalance_min)
+ 	if (dst_running < (dst_weight >> 2) && imbalance <= NUMA_IMBALANCE_MIN)
  		return 0;
 -
 +#endif
  	return imbalance;
  }
  
* Unmerged path kernel/sched/fair.c
