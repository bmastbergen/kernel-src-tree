mm/swap.c: serialize memcg changes in pagevec_lru_move_fn

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Alex Shi <alex.shi@linux.alibaba.com>
commit fc574c23558c63799dd99a9bb1d62e33708abaf5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/fc574c23.failed

Hugh Dickins' found a memcg change bug on original version: If we want to
change the pgdat->lru_lock to memcg's lruvec lock, we have to serialize
mem_cgroup_move_account during pagevec_lru_move_fn.  The possible bad
scenario would like:

	cpu 0					cpu 1
lruvec = mem_cgroup_page_lruvec()
					if (!isolate_lru_page())
						mem_cgroup_move_account

spin_lock_irqsave(&lruvec->lru_lock <== wrong lock.

So we need TestClearPageLRU to block isolate_lru_page(), that serializes
the memcg change.  and then removing the PageLRU check in move_fn callee
as the consequence.

__pagevec_lru_add_fn() is different from the others, because the pages it
deals with are, by definition, not yet on the lru.  TestClearPageLRU is
not needed and would not work, so __pagevec_lru_add() goes its own way.

Link: https://lkml.kernel.org/r/1604566549-62481-17-git-send-email-alex.shi@linux.alibaba.com
	Reported-by: Hugh Dickins <hughd@google.com>
	Signed-off-by: Alex Shi <alex.shi@linux.alibaba.com>
	Acked-by: Hugh Dickins <hughd@google.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Alexander Duyck <alexander.duyck@gmail.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: "Chen, Rong A" <rong.a.chen@intel.com>
	Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
	Cc: "Huang, Ying" <ying.huang@intel.com>
	Cc: Jann Horn <jannh@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Kirill A. Shutemov <kirill@shutemov.name>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mika Penttil√§ <mika.penttila@nextfour.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Cc: Wei Yang <richard.weiyang@gmail.com>
	Cc: Yang Shi <yang.shi@linux.alibaba.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fc574c23558c63799dd99a9bb1d62e33708abaf5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index cb800c9dc782,d952af770053..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -222,8 -222,14 +222,18 @@@ static void pagevec_lru_move_fn(struct 
  			spin_lock_irqsave(&pgdat->lru_lock, flags);
  		}
  
+ 		/* block memcg migration during page moving between lru */
+ 		if (!TestClearPageLRU(page))
+ 			continue;
+ 
  		lruvec = mem_cgroup_page_lruvec(page, pgdat);
++<<<<<<< HEAD
 +		(*move_fn)(page, lruvec, arg);
++=======
+ 		(*move_fn)(page, lruvec);
+ 
+ 		SetPageLRU(page);
++>>>>>>> fc574c23558c (mm/swap.c: serialize memcg changes in pagevec_lru_move_fn)
  	}
  	if (pgdat)
  		spin_unlock_irqrestore(&pgdat->lru_lock, flags);
@@@ -231,12 -237,9 +241,16 @@@
  	pagevec_reinit(pvec);
  }
  
 -static void pagevec_move_tail_fn(struct page *page, struct lruvec *lruvec)
 +static void pagevec_move_tail_fn(struct page *page, struct lruvec *lruvec,
 +				 void *arg)
  {
++<<<<<<< HEAD
 +	int *pgmoved = arg;
 +
 +	if (PageLRU(page) && !PageUnevictable(page)) {
++=======
+ 	if (!PageUnevictable(page)) {
++>>>>>>> fc574c23558c (mm/swap.c: serialize memcg changes in pagevec_lru_move_fn)
  		del_page_from_lru_list(page, lruvec, page_lru(page));
  		ClearPageActive(page);
  		add_page_to_lru_list_tail(page, lruvec, page_lru(page));
@@@ -314,10 -310,9 +328,10 @@@ void lru_note_cost_page(struct page *pa
  		      page_is_file_lru(page), thp_nr_pages(page));
  }
  
 -static void __activate_page(struct page *page, struct lruvec *lruvec)
 +static void __activate_page(struct page *page, struct lruvec *lruvec,
 +			    void *arg)
  {
- 	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
+ 	if (!PageActive(page) && !PageUnevictable(page)) {
  		int lru = page_lru_base_type(page);
  		int nr_pages = thp_nr_pages(page);
  
@@@ -378,7 -368,8 +392,12 @@@ void activate_page(struct page *page
  
  	page = compound_head(page);
  	spin_lock_irq(&pgdat->lru_lock);
++<<<<<<< HEAD
 +	__activate_page(page, mem_cgroup_page_lruvec(page, pgdat), NULL);
++=======
+ 	if (PageLRU(page))
+ 		__activate_page(page, mem_cgroup_page_lruvec(page, pgdat));
++>>>>>>> fc574c23558c (mm/swap.c: serialize memcg changes in pagevec_lru_move_fn)
  	spin_unlock_irq(&pgdat->lru_lock);
  }
  #endif
@@@ -580,10 -564,9 +596,10 @@@ static void lru_deactivate_file_fn(stru
  	}
  }
  
 -static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec)
 +static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,
 +			    void *arg)
  {
- 	if (PageLRU(page) && PageActive(page) && !PageUnevictable(page)) {
+ 	if (PageActive(page) && !PageUnevictable(page)) {
  		int lru = page_lru_base_type(page);
  		int nr_pages = thp_nr_pages(page);
  
@@@ -598,10 -581,9 +614,10 @@@
  	}
  }
  
 -static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec)
 +static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,
 +			    void *arg)
  {
- 	if (PageLRU(page) && PageAnon(page) && PageSwapBacked(page) &&
+ 	if (PageAnon(page) && PageSwapBacked(page) &&
  	    !PageSwapCache(page) && !PageUnevictable(page)) {
  		bool active = PageActive(page);
  		int nr_pages = thp_nr_pages(page);
@@@ -1010,7 -1025,29 +1026,33 @@@ static void __pagevec_lru_add_fn(struc
   */
  void __pagevec_lru_add(struct pagevec *pvec)
  {
++<<<<<<< HEAD
 +	pagevec_lru_move_fn(pvec, __pagevec_lru_add_fn, NULL);
++=======
+ 	int i;
+ 	struct pglist_data *pgdat = NULL;
+ 	struct lruvec *lruvec;
+ 	unsigned long flags = 0;
+ 
+ 	for (i = 0; i < pagevec_count(pvec); i++) {
+ 		struct page *page = pvec->pages[i];
+ 		struct pglist_data *pagepgdat = page_pgdat(page);
+ 
+ 		if (pagepgdat != pgdat) {
+ 			if (pgdat)
+ 				spin_unlock_irqrestore(&pgdat->lru_lock, flags);
+ 			pgdat = pagepgdat;
+ 			spin_lock_irqsave(&pgdat->lru_lock, flags);
+ 		}
+ 
+ 		lruvec = mem_cgroup_page_lruvec(page, pgdat);
+ 		__pagevec_lru_add_fn(page, lruvec);
+ 	}
+ 	if (pgdat)
+ 		spin_unlock_irqrestore(&pgdat->lru_lock, flags);
+ 	release_pages(pvec->pages, pvec->nr);
+ 	pagevec_reinit(pvec);
++>>>>>>> fc574c23558c (mm/swap.c: serialize memcg changes in pagevec_lru_move_fn)
  }
  
  /**
* Unmerged path mm/swap.c
