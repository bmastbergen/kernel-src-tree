mm/gup: use a standard migration target allocation callback

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit ed03d924587e7dfc54f1ace81e5f5a497a7d9666
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/ed03d924.failed

There is a well-defined migration target allocation callback. Use it.

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.ibm.com>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Roman Gushchin <guro@fb.com>
Link: http://lkml.kernel.org/r/1596180906-8442-3-git-send-email-iamjoonsoo.kim@lge.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ed03d924587e7dfc54f1ace81e5f5a497a7d9666)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index eb225cc5e706,e9d1d0cc18f0..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -1511,57 -1609,6 +1511,60 @@@ static bool check_dax_vmas(struct vm_ar
  }
  
  #ifdef CONFIG_CMA
++<<<<<<< HEAD
 +static struct page *new_non_cma_page(struct page *page, unsigned long private)
 +{
 +	/*
 +	 * We want to make sure we allocate the new page from the same node
 +	 * as the source page.
 +	 */
 +	int nid = page_to_nid(page);
 +	/*
 +	 * Trying to allocate a page for migration. Ignore allocation
 +	 * failure warnings. We don't force __GFP_THISNODE here because
 +	 * this node here is the node where we have CMA reservation and
 +	 * in some case these nodes will have really less non movable
 +	 * allocation memory.
 +	 */
 +	gfp_t gfp_mask = GFP_USER | __GFP_NOWARN;
 +
 +	if (PageHighMem(page))
 +		gfp_mask |= __GFP_HIGHMEM;
 +
 +#ifdef CONFIG_HUGETLB_PAGE
 +	if (PageHuge(page)) {
 +		struct hstate *h = page_hstate(page);
 +		/*
 +		 * We don't want to dequeue from the pool because pool pages will
 +		 * mostly be from the CMA region.
 +		 */
 +		return alloc_migrate_huge_page(h, gfp_mask, nid, NULL);
 +	}
 +#endif
 +	if (PageTransHuge(page)) {
 +		struct page *thp;
 +		/*
 +		 * ignore allocation failure warnings
 +		 */
 +		gfp_t thp_gfpmask = GFP_TRANSHUGE | __GFP_NOWARN;
 +
 +		/*
 +		 * Remove the movable mask so that we don't allocate from
 +		 * CMA area again.
 +		 */
 +		thp_gfpmask &= ~__GFP_MOVABLE;
 +		thp = __alloc_pages_node(nid, thp_gfpmask, HPAGE_PMD_ORDER);
 +		if (!thp)
 +			return NULL;
 +		prep_transhuge_page(thp);
 +		return thp;
 +	}
 +
 +	return __alloc_pages_node(nid, gfp_mask, 0);
 +}
 +
++=======
++>>>>>>> ed03d924587e (mm/gup: use a standard migration target allocation callback)
  static long check_and_migrate_cma_pages(struct task_struct *tsk,
  					struct mm_struct *mm,
  					unsigned long start,
* Unmerged path mm/gup.c
