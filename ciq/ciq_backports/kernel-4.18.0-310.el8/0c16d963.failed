RDMA/umem: Move to allocate SG table from pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Maor Gottlieb <maorg@nvidia.com>
commit 0c16d9635e3a51377e5815b9f8e14f497a4dbb42
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/0c16d963.failed

Remove the implementation of ib_umem_add_sg_table and instead
call to __sg_alloc_table_from_pages which already has the logic to
merge contiguous pages.

Besides that it removes duplicated functionality, it reduces the
memory consumption of the SG table significantly. Prior to this
patch, the SG table was allocated in advance regardless consideration
of contiguous pages.

In huge pages system of 2MB page size, without this change, the SG table
would contain x512 SG entries.
E.g. for 100GB memory registration:

	 Number of entries	Size
Before 	      26214400          600.0MB
After            51200		  1.2MB

Link: https://lore.kernel.org/r/20201004154340.1080481-5-leon@kernel.org
	Signed-off-by: Maor Gottlieb <maorg@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 0c16d9635e3a51377e5815b9f8e14f497a4dbb42)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
diff --cc drivers/infiniband/core/umem.c
index a6826e4a7de2,52c55e908bee..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -218,17 -134,9 +151,17 @@@ struct ib_umem *ib_umem_get(struct ib_u
  	struct mm_struct *mm;
  	unsigned long npages;
  	int ret;
- 	struct scatterlist *sg;
+ 	struct scatterlist *sg = NULL;
  	unsigned int gup_flags = FOLL_WRITE;
  
 +	if (!udata)
 +		return ERR_PTR(-EIO);
 +
 +	context = container_of(udata, struct uverbs_attr_bundle, driver_udata)
 +			  ->context;
 +	if (!context)
 +		return ERR_PTR(-EIO);
 +
  	/*
  	 * If the combination of the addr and size requested for this memory
  	 * region causes an integer overflow, return error.
@@@ -296,15 -198,19 +223,27 @@@
  			goto umem_release;
  
  		cur_base += ret * PAGE_SIZE;
++<<<<<<< HEAD
 +		npages   -= ret;
 +
 +		sg = ib_umem_add_sg_table(sg, page_list, ret,
 +			dma_get_max_seg_size(context->device->dma_device),
 +			&umem->sg_nents);
++=======
+ 		npages -= ret;
+ 		sg = __sg_alloc_table_from_pages(
+ 			&umem->sg_head, page_list, ret, 0, ret << PAGE_SHIFT,
+ 			dma_get_max_seg_size(device->dma_device), sg, npages,
+ 			GFP_KERNEL);
+ 		umem->sg_nents = umem->sg_head.nents;
+ 		if (IS_ERR(sg)) {
+ 			unpin_user_pages_dirty_lock(page_list, ret, 0);
+ 			ret = PTR_ERR(sg);
+ 			goto umem_release;
+ 		}
++>>>>>>> 0c16d9635e3a (RDMA/umem: Move to allocate SG table from pages)
  	}
  
- 	sg_mark_end(sg);
- 
  	if (access & IB_ACCESS_RELAXED_ORDERING)
  		dma_attr |= DMA_ATTR_WEAK_ORDERING;
  
@@@ -322,8 -227,7 +261,12 @@@
  	goto out;
  
  umem_release:
++<<<<<<< HEAD
 +	__ib_umem_release(context->device, umem, 0);
 +vma:
++=======
+ 	__ib_umem_release(device, umem, 0);
++>>>>>>> 0c16d9635e3a (RDMA/umem: Move to allocate SG table from pages)
  	atomic64_sub(ib_umem_num_pages(umem), &mm->pinned_vm);
  out:
  	free_page((unsigned long) page_list);
* Unmerged path drivers/infiniband/core/umem.c
