KVM: x86/mmu: Add helper to generate mask of reserved HPA bits

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 6f8e65a60168567cc59f9b99980ea9112d4152f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/6f8e65a6.failed

Add a helper to generate the mask of reserved PA bits in the host.

No functional change intended.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210204000117.3303214-10-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 6f8e65a60168567cc59f9b99980ea9112d4152f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index c1839b40631e,86af58294272..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -4113,9 -4120,14 +4113,14 @@@ static void reset_rsvds_bits_mask_ept(s
  		struct kvm_mmu *context, bool execonly)
  {
  	__reset_rsvds_bits_mask_ept(&context->guest_rsvd_check,
 -				    vcpu->arch.reserved_gpa_bits, execonly);
 +				    cpuid_maxphyaddr(vcpu), execonly);
  }
  
+ static inline u64 reserved_hpa_bits(void)
+ {
+ 	return rsvd_bits(shadow_phys_bits, 63);
+ }
+ 
  /*
   * the page table on host is the shadow page table for the page
   * table in guest or amd nested guest, its mmu features completely
@@@ -4135,7 -4147,7 +4140,11 @@@ reset_shadow_zero_bits_mask(struct kvm_
  	 */
  	shadow_zero_check = &context->shadow_zero_check;
  	__reset_rsvds_bits_mask(vcpu, shadow_zero_check,
++<<<<<<< HEAD
 +				shadow_phys_bits,
++=======
+ 				reserved_hpa_bits(),
++>>>>>>> 6f8e65a60168 (KVM: x86/mmu: Add helper to generate mask of reserved HPA bits)
  				context->shadow_root_level, uses_nx,
  				guest_cpuid_has(vcpu, X86_FEATURE_GBPAGES),
  				is_pse(vcpu), true);
@@@ -4172,14 -4184,13 +4181,22 @@@ reset_tdp_shadow_zero_bits_mask(struct 
  
  	if (boot_cpu_is_amd())
  		__reset_rsvds_bits_mask(vcpu, shadow_zero_check,
++<<<<<<< HEAD
 +					shadow_phys_bits,
++=======
+ 					reserved_hpa_bits(),
++>>>>>>> 6f8e65a60168 (KVM: x86/mmu: Add helper to generate mask of reserved HPA bits)
  					context->shadow_root_level, false,
  					boot_cpu_has(X86_FEATURE_GBPAGES),
  					true, true);
  	else
  		__reset_rsvds_bits_mask_ept(shadow_zero_check,
++<<<<<<< HEAD
 +					    shadow_phys_bits,
 +					    false);
++=======
+ 					    reserved_hpa_bits(), false);
++>>>>>>> 6f8e65a60168 (KVM: x86/mmu: Add helper to generate mask of reserved HPA bits)
  
  	if (!shadow_me_mask)
  		return;
@@@ -4199,7 -4210,7 +4216,11 @@@ reset_ept_shadow_zero_bits_mask(struct 
  				struct kvm_mmu *context, bool execonly)
  {
  	__reset_rsvds_bits_mask_ept(&context->shadow_zero_check,
++<<<<<<< HEAD
 +				    shadow_phys_bits, execonly);
++=======
+ 				    reserved_hpa_bits(), execonly);
++>>>>>>> 6f8e65a60168 (KVM: x86/mmu: Add helper to generate mask of reserved HPA bits)
  }
  
  #define BYTE_MASK(access) \
* Unmerged path arch/x86/kvm/mmu/mmu.c
