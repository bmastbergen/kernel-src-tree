powerpc/pseries: Move some PAPR paravirt functions to their own file

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Nicholas Piggin <npiggin@gmail.com>
commit 20d444d06f97504d165b08558678b4737dcefb02
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/20d444d0.failed

These functions will be used by the queued spinlock implementation,
and may be useful elsewhere too, so move them out of spinlock.h.

	Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
	Acked-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20200724131423.1362108-2-npiggin@gmail.com
(cherry picked from commit 20d444d06f97504d165b08558678b4737dcefb02)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/spinlock.h
diff --cc arch/powerpc/include/asm/spinlock.h
index b322a6b3c984,79be9bb10bbb..000000000000
--- a/arch/powerpc/include/asm/spinlock.h
+++ b/arch/powerpc/include/asm/spinlock.h
@@@ -12,20 -13,13 +12,19 @@@
   *
   * Type of int is used as a full 64b word is not necessary.
   *
 + * This program is free software; you can redistribute it and/or
 + * modify it under the terms of the GNU General Public License
 + * as published by the Free Software Foundation; either version
 + * 2 of the License, or (at your option) any later version.
 + *
   * (the type definitions are in asm/spinlock_types.h)
   */
- #include <linux/jump_label.h>
  #include <linux/irqflags.h>
+ #include <asm/paravirt.h>
  #ifdef CONFIG_PPC64
  #include <asm/paca.h>
- #include <asm/hvcall.h>
  #endif
 +#include <asm/asm-compat.h>
  #include <asm/synch.h>
  #include <asm/ppc-opcode.h>
  
@@@ -108,15 -90,29 +95,34 @@@ static inline int arch_spin_trylock(arc
  
  #if defined(CONFIG_PPC_SPLPAR)
  /* We only yield to the hypervisor if we are in shared processor mode */
 -void splpar_spin_yield(arch_spinlock_t *lock);
 -void splpar_rw_yield(arch_rwlock_t *lock);
 +#define SHARED_PROCESSOR (static_branch_unlikely(&shared_processor))
 +extern void __spin_yield(arch_spinlock_t *lock);
 +extern void __rw_yield(arch_rwlock_t *lock);
  #else /* SPLPAR */
 -static inline void splpar_spin_yield(arch_spinlock_t *lock) {};
 -static inline void splpar_rw_yield(arch_rwlock_t *lock) {};
 +#define __spin_yield(x)	barrier()
 +#define __rw_yield(x)	barrier()
 +#define SHARED_PROCESSOR	0
  #endif
  
++<<<<<<< HEAD
++=======
+ static inline void spin_yield(arch_spinlock_t *lock)
+ {
+ 	if (is_shared_processor())
+ 		splpar_spin_yield(lock);
+ 	else
+ 		barrier();
+ }
+ 
+ static inline void rw_yield(arch_rwlock_t *lock)
+ {
+ 	if (is_shared_processor())
+ 		splpar_rw_yield(lock);
+ 	else
+ 		barrier();
+ }
+ 
++>>>>>>> 20d444d06f97 (powerpc/pseries: Move some PAPR paravirt functions to their own file)
  static inline void arch_spin_lock(arch_spinlock_t *lock)
  {
  	while (1) {
diff --git a/arch/powerpc/include/asm/paravirt.h b/arch/powerpc/include/asm/paravirt.h
new file mode 100644
index 000000000000..339e8533464b
--- /dev/null
+++ b/arch/powerpc/include/asm/paravirt.h
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+#ifndef _ASM_POWERPC_PARAVIRT_H
+#define _ASM_POWERPC_PARAVIRT_H
+
+#include <linux/jump_label.h>
+#include <asm/smp.h>
+#ifdef CONFIG_PPC64
+#include <asm/paca.h>
+#include <asm/hvcall.h>
+#endif
+
+#ifdef CONFIG_PPC_SPLPAR
+DECLARE_STATIC_KEY_FALSE(shared_processor);
+
+static inline bool is_shared_processor(void)
+{
+	return static_branch_unlikely(&shared_processor);
+}
+
+/* If bit 0 is set, the cpu has been preempted */
+static inline u32 yield_count_of(int cpu)
+{
+	__be32 yield_count = READ_ONCE(lppaca_of(cpu).yield_count);
+	return be32_to_cpu(yield_count);
+}
+
+static inline void yield_to_preempted(int cpu, u32 yield_count)
+{
+	plpar_hcall_norets(H_CONFER, get_hard_smp_processor_id(cpu), yield_count);
+}
+#else
+static inline bool is_shared_processor(void)
+{
+	return false;
+}
+
+static inline u32 yield_count_of(int cpu)
+{
+	return 0;
+}
+
+extern void ___bad_yield_to_preempted(void);
+static inline void yield_to_preempted(int cpu, u32 yield_count)
+{
+	___bad_yield_to_preempted(); /* This would be a bug */
+}
+#endif
+
+#define vcpu_is_preempted vcpu_is_preempted
+static inline bool vcpu_is_preempted(int cpu)
+{
+	if (!is_shared_processor())
+		return false;
+	if (yield_count_of(cpu) & 1)
+		return true;
+	return false;
+}
+
+#endif /* _ASM_POWERPC_PARAVIRT_H */
* Unmerged path arch/powerpc/include/asm/spinlock.h
diff --git a/arch/powerpc/lib/locks.c b/arch/powerpc/lib/locks.c
index b7b1237d4aa6..387de03a20f4 100644
--- a/arch/powerpc/lib/locks.c
+++ b/arch/powerpc/lib/locks.c
@@ -32,14 +32,14 @@ void __spin_yield(arch_spinlock_t *lock)
 		return;
 	holder_cpu = lock_value & 0xffff;
 	BUG_ON(holder_cpu >= NR_CPUS);
-	yield_count = be32_to_cpu(lppaca_of(holder_cpu).yield_count);
+
+	yield_count = yield_count_of(holder_cpu);
 	if ((yield_count & 1) == 0)
 		return;		/* virtual cpu is currently running */
 	rmb();
 	if (lock->slock != lock_value)
 		return;		/* something has changed */
-	plpar_hcall_norets(H_CONFER,
-		get_hard_smp_processor_id(holder_cpu), yield_count);
+	yield_to_preempted(holder_cpu, yield_count);
 }
 EXPORT_SYMBOL_GPL(__spin_yield);
 
@@ -58,13 +58,13 @@ void __rw_yield(arch_rwlock_t *rw)
 		return;		/* no write lock at present */
 	holder_cpu = lock_value & 0xffff;
 	BUG_ON(holder_cpu >= NR_CPUS);
-	yield_count = be32_to_cpu(lppaca_of(holder_cpu).yield_count);
+
+	yield_count = yield_count_of(holder_cpu);
 	if ((yield_count & 1) == 0)
 		return;		/* virtual cpu is currently running */
 	rmb();
 	if (rw->lock != lock_value)
 		return;		/* something has changed */
-	plpar_hcall_norets(H_CONFER,
-		get_hard_smp_processor_id(holder_cpu), yield_count);
+	yield_to_preempted(holder_cpu, yield_count);
 }
 #endif
