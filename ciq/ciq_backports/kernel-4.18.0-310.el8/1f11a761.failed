RDMA: Check create_flags during create_qp

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit 1f11a7610e50982150b90b31d1f749f6217fbde6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/1f11a761.failed

Each driver should check that the QP attrs create_flags is supported.
Unfortuantely when create_flags was added to the QP attrs the drivers were
not updated. uverbs_ex_cmd_mask was used to block it - even though kernel
drivers use these flags too.

Check that flags is zero in all drivers that don't use it, remove
IB_USER_VERBS_EX_CMD_CREATE_QP from uverbs_ex_cmd_mask. Fix the error code
to be EOPNOTSUPP.

Link: https://lore.kernel.org/r/8-v1-caa70ba3d1ab+1436e-ucmd_mask_jgg@nvidia.com
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 1f11a7610e50982150b90b31d1f749f6217fbde6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/device.c
#	drivers/infiniband/hw/hns/hns_roce_qp.c
#	drivers/infiniband/hw/mlx4/main.c
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/core/device.c
index 30a7bf1c3b41,e5f5d656951d..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -601,9 -601,49 +601,55 @@@ struct ib_device *_ib_alloc_device(size
  	init_completion(&device->unreg_completion);
  	INIT_WORK(&device->unregistration_work, ib_unregister_work);
  
++<<<<<<< HEAD
 +	spin_lock_init(&device->cq_pools_lock);
 +	for (i = 0; i < ARRAY_SIZE(device->cq_pools); i++)
 +		INIT_LIST_HEAD(&device->cq_pools[i]);
++=======
+ 	device->uverbs_cmd_mask =
+ 		BIT_ULL(IB_USER_VERBS_CMD_ALLOC_MW) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_ALLOC_PD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_ATTACH_MCAST) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CLOSE_XRCD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_XSRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEALLOC_MW) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEALLOC_PD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEREG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DETACH_MCAST) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_GET_CONTEXT) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_MODIFY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_MODIFY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_OPEN_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_OPEN_XRCD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_DEVICE) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_PORT) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_REG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_REREG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_RESIZE_CQ);
+ 
+ 	device->uverbs_ex_cmd_mask =
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_CREATE_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_CREATE_FLOW) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_CREATE_QP) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_CREATE_RWQ_IND_TBL) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_CREATE_WQ) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_DESTROY_FLOW) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_DESTROY_RWQ_IND_TBL) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_DESTROY_WQ) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_MODIFY_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_MODIFY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_MODIFY_WQ) |
+ 		BIT_ULL(IB_USER_VERBS_EX_CMD_QUERY_DEVICE);
++>>>>>>> 1f11a7610e50 (RDMA: Check create_flags during create_qp)
  
  	return device;
  }
diff --cc drivers/infiniband/hw/hns/hns_roce_qp.c
index ed75cc0e9ed0,04a70681bea1..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_qp.c
+++ b/drivers/infiniband/hw/hns/hns_roce_qp.c
@@@ -529,6 -595,288 +529,291 @@@ static int hns_roce_qp_has_rq(struct ib
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ static int alloc_rq_inline_buf(struct hns_roce_qp *hr_qp,
+ 			       struct ib_qp_init_attr *init_attr)
+ {
+ 	u32 max_recv_sge = init_attr->cap.max_recv_sge;
+ 	u32 wqe_cnt = hr_qp->rq_inl_buf.wqe_cnt;
+ 	struct hns_roce_rinl_wqe *wqe_list;
+ 	int i;
+ 
+ 	/* allocate recv inline buf */
+ 	wqe_list = kcalloc(wqe_cnt, sizeof(struct hns_roce_rinl_wqe),
+ 			   GFP_KERNEL);
+ 
+ 	if (!wqe_list)
+ 		goto err;
+ 
+ 	/* Allocate a continuous buffer for all inline sge we need */
+ 	wqe_list[0].sg_list = kcalloc(wqe_cnt, (max_recv_sge *
+ 				      sizeof(struct hns_roce_rinl_sge)),
+ 				      GFP_KERNEL);
+ 	if (!wqe_list[0].sg_list)
+ 		goto err_wqe_list;
+ 
+ 	/* Assign buffers of sg_list to each inline wqe */
+ 	for (i = 1; i < wqe_cnt; i++)
+ 		wqe_list[i].sg_list = &wqe_list[0].sg_list[i * max_recv_sge];
+ 
+ 	hr_qp->rq_inl_buf.wqe_list = wqe_list;
+ 
+ 	return 0;
+ 
+ err_wqe_list:
+ 	kfree(wqe_list);
+ 
+ err:
+ 	return -ENOMEM;
+ }
+ 
+ static void free_rq_inline_buf(struct hns_roce_qp *hr_qp)
+ {
+ 	if (hr_qp->rq_inl_buf.wqe_list)
+ 		kfree(hr_qp->rq_inl_buf.wqe_list[0].sg_list);
+ 	kfree(hr_qp->rq_inl_buf.wqe_list);
+ }
+ 
+ static int alloc_qp_buf(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,
+ 			struct ib_qp_init_attr *init_attr,
+ 			struct ib_udata *udata, unsigned long addr)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_buf_attr buf_attr = {};
+ 	int ret;
+ 
+ 	if (!udata && hr_qp->rq_inl_buf.wqe_cnt) {
+ 		ret = alloc_rq_inline_buf(hr_qp, init_attr);
+ 		if (ret) {
+ 			ibdev_err(ibdev,
+ 				  "failed to alloc inline buf, ret = %d.\n",
+ 				  ret);
+ 			return ret;
+ 		}
+ 	} else {
+ 		hr_qp->rq_inl_buf.wqe_list = NULL;
+ 	}
+ 
+ 	ret = set_wqe_buf_attr(hr_dev, hr_qp, &buf_attr);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to split WQE buf, ret = %d.\n", ret);
+ 		goto err_inline;
+ 	}
+ 	ret = hns_roce_mtr_create(hr_dev, &hr_qp->mtr, &buf_attr,
+ 				  HNS_HW_PAGE_SHIFT + hr_dev->caps.mtt_ba_pg_sz,
+ 				  udata, addr);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to create WQE mtr, ret = %d.\n", ret);
+ 		goto err_inline;
+ 	}
+ 
+ 	return 0;
+ err_inline:
+ 	free_rq_inline_buf(hr_qp);
+ 
+ 	return ret;
+ }
+ 
+ static void free_qp_buf(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)
+ {
+ 	hns_roce_mtr_destroy(hr_dev, &hr_qp->mtr);
+ 	free_rq_inline_buf(hr_qp);
+ }
+ 
+ static inline bool user_qp_has_sdb(struct hns_roce_dev *hr_dev,
+ 				   struct ib_qp_init_attr *init_attr,
+ 				   struct ib_udata *udata,
+ 				   struct hns_roce_ib_create_qp_resp *resp,
+ 				   struct hns_roce_ib_create_qp *ucmd)
+ {
+ 	return ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_SQ_RECORD_DB) &&
+ 		udata->outlen >= offsetofend(typeof(*resp), cap_flags) &&
+ 		hns_roce_qp_has_sq(init_attr) &&
+ 		udata->inlen >= offsetofend(typeof(*ucmd), sdb_addr));
+ }
+ 
+ static inline bool user_qp_has_rdb(struct hns_roce_dev *hr_dev,
+ 				   struct ib_qp_init_attr *init_attr,
+ 				   struct ib_udata *udata,
+ 				   struct hns_roce_ib_create_qp_resp *resp)
+ {
+ 	return ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB) &&
+ 		udata->outlen >= offsetofend(typeof(*resp), cap_flags) &&
+ 		hns_roce_qp_has_rq(init_attr));
+ }
+ 
+ static inline bool kernel_qp_has_rdb(struct hns_roce_dev *hr_dev,
+ 				     struct ib_qp_init_attr *init_attr)
+ {
+ 	return ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB) &&
+ 		hns_roce_qp_has_rq(init_attr));
+ }
+ 
+ static int alloc_qp_db(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,
+ 		       struct ib_qp_init_attr *init_attr,
+ 		       struct ib_udata *udata,
+ 		       struct hns_roce_ib_create_qp *ucmd,
+ 		       struct hns_roce_ib_create_qp_resp *resp)
+ {
+ 	struct hns_roce_ucontext *uctx = rdma_udata_to_drv_context(
+ 		udata, struct hns_roce_ucontext, ibucontext);
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	int ret;
+ 
+ 	if (udata) {
+ 		if (user_qp_has_sdb(hr_dev, init_attr, udata, resp, ucmd)) {
+ 			ret = hns_roce_db_map_user(uctx, udata, ucmd->sdb_addr,
+ 						   &hr_qp->sdb);
+ 			if (ret) {
+ 				ibdev_err(ibdev,
+ 					  "Failed to map user SQ doorbell\n");
+ 				goto err_out;
+ 			}
+ 			hr_qp->en_flags |= HNS_ROCE_QP_CAP_SQ_RECORD_DB;
+ 			resp->cap_flags |= HNS_ROCE_QP_CAP_SQ_RECORD_DB;
+ 		}
+ 
+ 		if (user_qp_has_rdb(hr_dev, init_attr, udata, resp)) {
+ 			ret = hns_roce_db_map_user(uctx, udata, ucmd->db_addr,
+ 						   &hr_qp->rdb);
+ 			if (ret) {
+ 				ibdev_err(ibdev,
+ 					  "Failed to map user RQ doorbell\n");
+ 				goto err_sdb;
+ 			}
+ 			hr_qp->en_flags |= HNS_ROCE_QP_CAP_RQ_RECORD_DB;
+ 			resp->cap_flags |= HNS_ROCE_QP_CAP_RQ_RECORD_DB;
+ 		}
+ 	} else {
+ 		/* QP doorbell register address */
+ 		hr_qp->sq.db_reg_l = hr_dev->reg_base + hr_dev->sdb_offset +
+ 				     DB_REG_OFFSET * hr_dev->priv_uar.index;
+ 		hr_qp->rq.db_reg_l = hr_dev->reg_base + hr_dev->odb_offset +
+ 				     DB_REG_OFFSET * hr_dev->priv_uar.index;
+ 
+ 		if (kernel_qp_has_rdb(hr_dev, init_attr)) {
+ 			ret = hns_roce_alloc_db(hr_dev, &hr_qp->rdb, 0);
+ 			if (ret) {
+ 				ibdev_err(ibdev,
+ 					  "Failed to alloc kernel RQ doorbell\n");
+ 				goto err_out;
+ 			}
+ 			*hr_qp->rdb.db_record = 0;
+ 			hr_qp->en_flags |= HNS_ROCE_QP_CAP_RQ_RECORD_DB;
+ 		}
+ 	}
+ 
+ 	return 0;
+ err_sdb:
+ 	if (udata && hr_qp->en_flags & HNS_ROCE_QP_CAP_SQ_RECORD_DB)
+ 		hns_roce_db_unmap_user(uctx, &hr_qp->sdb);
+ err_out:
+ 	return ret;
+ }
+ 
+ static void free_qp_db(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,
+ 		       struct ib_udata *udata)
+ {
+ 	struct hns_roce_ucontext *uctx = rdma_udata_to_drv_context(
+ 		udata, struct hns_roce_ucontext, ibucontext);
+ 
+ 	if (udata) {
+ 		if (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)
+ 			hns_roce_db_unmap_user(uctx, &hr_qp->rdb);
+ 		if (hr_qp->en_flags & HNS_ROCE_QP_CAP_SQ_RECORD_DB)
+ 			hns_roce_db_unmap_user(uctx, &hr_qp->sdb);
+ 	} else {
+ 		if (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)
+ 			hns_roce_free_db(hr_dev, &hr_qp->rdb);
+ 	}
+ }
+ 
+ static int alloc_kernel_wrid(struct hns_roce_dev *hr_dev,
+ 			     struct hns_roce_qp *hr_qp)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	u64 *sq_wrid = NULL;
+ 	u64 *rq_wrid = NULL;
+ 	int ret;
+ 
+ 	sq_wrid = kcalloc(hr_qp->sq.wqe_cnt, sizeof(u64), GFP_KERNEL);
+ 	if (ZERO_OR_NULL_PTR(sq_wrid)) {
+ 		ibdev_err(ibdev, "Failed to alloc SQ wrid\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (hr_qp->rq.wqe_cnt) {
+ 		rq_wrid = kcalloc(hr_qp->rq.wqe_cnt, sizeof(u64), GFP_KERNEL);
+ 		if (ZERO_OR_NULL_PTR(rq_wrid)) {
+ 			ibdev_err(ibdev, "Failed to alloc RQ wrid\n");
+ 			ret = -ENOMEM;
+ 			goto err_sq;
+ 		}
+ 	}
+ 
+ 	hr_qp->sq.wrid = sq_wrid;
+ 	hr_qp->rq.wrid = rq_wrid;
+ 	return 0;
+ err_sq:
+ 	kfree(sq_wrid);
+ 
+ 	return ret;
+ }
+ 
+ static void free_kernel_wrid(struct hns_roce_qp *hr_qp)
+ {
+ 	kfree(hr_qp->rq.wrid);
+ 	kfree(hr_qp->sq.wrid);
+ }
+ 
+ static int set_qp_param(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,
+ 			struct ib_qp_init_attr *init_attr,
+ 			struct ib_udata *udata,
+ 			struct hns_roce_ib_create_qp *ucmd)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	int ret;
+ 
+ 	hr_qp->ibqp.qp_type = init_attr->qp_type;
+ 
+ 	if (init_attr->cap.max_inline_data > hr_dev->caps.max_sq_inline)
+ 		init_attr->cap.max_inline_data = hr_dev->caps.max_sq_inline;
+ 
+ 	hr_qp->max_inline_data = init_attr->cap.max_inline_data;
+ 
+ 	if (init_attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		hr_qp->sq_signal_bits = IB_SIGNAL_ALL_WR;
+ 	else
+ 		hr_qp->sq_signal_bits = IB_SIGNAL_REQ_WR;
+ 
+ 	ret = set_rq_size(hr_dev, &init_attr->cap, hr_qp,
+ 			  hns_roce_qp_has_rq(init_attr));
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to set user RQ size, ret = %d.\n",
+ 			  ret);
+ 		return ret;
+ 	}
+ 
+ 	if (udata) {
+ 		if (ib_copy_from_udata(ucmd, udata, sizeof(*ucmd))) {
+ 			ibdev_err(ibdev, "Failed to copy QP ucmd\n");
+ 			return -EFAULT;
+ 		}
+ 
+ 		ret = set_user_sq_size(hr_dev, &init_attr->cap, hr_qp, ucmd);
+ 		if (ret)
+ 			ibdev_err(ibdev, "Failed to set user SQ size\n");
+ 	} else {
+ 		ret = set_kernel_sq_size(hr_dev, &init_attr->cap, hr_qp);
+ 		if (ret)
+ 			ibdev_err(ibdev, "Failed to set kernel SQ size\n");
+ 	}
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 1f11a7610e50 (RDMA: Check create_flags during create_qp)
  static int hns_roce_create_qp_common(struct hns_roce_dev *hr_dev,
  				     struct ib_pd *ib_pd,
  				     struct ib_qp_init_attr *init_attr,
@@@ -551,229 -893,61 +836,236 @@@
  	spin_lock_init(&hr_qp->rq.lock);
  
  	hr_qp->state = IB_QPS_RESET;
 -	hr_qp->flush_flag = 0;
  
++<<<<<<< HEAD
 +	hr_qp->ibqp.qp_type = init_attr->qp_type;
 +
 +	if (init_attr->sq_sig_type == IB_SIGNAL_ALL_WR)
 +		hr_qp->sq_signal_bits = cpu_to_le32(IB_SIGNAL_ALL_WR);
 +	else
 +		hr_qp->sq_signal_bits = cpu_to_le32(IB_SIGNAL_REQ_WR);
 +
 +	ret = hns_roce_set_rq_size(hr_dev, &init_attr->cap, udata,
 +				   !!init_attr->srq, hr_qp);
++=======
+ 	if (init_attr->create_flags)
+ 		return -EOPNOTSUPP;
+ 
+ 	ret = set_qp_param(hr_dev, hr_qp, init_attr, udata, &ucmd);
++>>>>>>> 1f11a7610e50 (RDMA: Check create_flags during create_qp)
  	if (ret) {
 -		ibdev_err(ibdev, "Failed to set QP param\n");
 -		return ret;
 +		dev_err(dev, "hns_roce_set_rq_size failed\n");
 +		goto err_out;
  	}
  
 -	if (!udata) {
 -		ret = alloc_kernel_wrid(hr_dev, hr_qp);
 -		if (ret) {
 -			ibdev_err(ibdev, "Failed to alloc wrid\n");
 -			return ret;
 +	if (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RQ_INLINE) {
 +		/* allocate recv inline buf */
 +		hr_qp->rq_inl_buf.wqe_list = kcalloc(hr_qp->rq.wqe_cnt,
 +					       sizeof(struct hns_roce_rinl_wqe),
 +					       GFP_KERNEL);
 +		if (!hr_qp->rq_inl_buf.wqe_list) {
 +			ret = -ENOMEM;
 +			goto err_out;
  		}
 -	}
  
 -	ret = alloc_qp_db(hr_dev, hr_qp, init_attr, udata, &ucmd, &resp);
 -	if (ret) {
 -		ibdev_err(ibdev, "Failed to alloc QP doorbell\n");
 -		goto err_wrid;
 -	}
 +		hr_qp->rq_inl_buf.wqe_cnt = hr_qp->rq.wqe_cnt;
  
 -	ret = alloc_qp_buf(hr_dev, hr_qp, init_attr, udata, ucmd.buf_addr);
 -	if (ret) {
 -		ibdev_err(ibdev, "Failed to alloc QP buffer\n");
 -		goto err_db;
 -	}
 +		/* Firstly, allocate a list of sge space buffer */
 +		hr_qp->rq_inl_buf.wqe_list[0].sg_list =
 +					kcalloc(hr_qp->rq_inl_buf.wqe_cnt,
 +					       init_attr->cap.max_recv_sge *
 +					       sizeof(struct hns_roce_rinl_sge),
 +					       GFP_KERNEL);
 +		if (!hr_qp->rq_inl_buf.wqe_list[0].sg_list) {
 +			ret = -ENOMEM;
 +			goto err_wqe_list;
 +		}
  
 -	ret = alloc_qpn(hr_dev, hr_qp);
 -	if (ret) {
 -		ibdev_err(ibdev, "Failed to alloc QPN\n");
 -		goto err_buf;
 +		for (i = 1; i < hr_qp->rq_inl_buf.wqe_cnt; i++)
 +			/* Secondly, reallocate the buffer */
 +			hr_qp->rq_inl_buf.wqe_list[i].sg_list =
 +				&hr_qp->rq_inl_buf.wqe_list[0].sg_list[i *
 +				init_attr->cap.max_recv_sge];
  	}
  
 -	ret = alloc_qpc(hr_dev, hr_qp);
 -	if (ret) {
 -		ibdev_err(ibdev, "Failed to alloc QP context\n");
 -		goto err_qpn;
 +	if (udata) {
 +		if (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {
 +			dev_err(dev, "ib_copy_from_udata error for create qp\n");
 +			ret = -EFAULT;
 +			goto err_rq_sge_list;
 +		}
 +
 +		ret = hns_roce_set_user_sq_size(hr_dev, &init_attr->cap, hr_qp,
 +						&ucmd);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_set_user_sq_size error for create qp\n");
 +			goto err_rq_sge_list;
 +		}
 +
 +		hr_qp->umem = ib_umem_get(udata, ucmd.buf_addr,
 +					  hr_qp->buff_size, 0);
 +		if (IS_ERR(hr_qp->umem)) {
 +			dev_err(dev, "ib_umem_get error for create qp\n");
 +			ret = PTR_ERR(hr_qp->umem);
 +			goto err_rq_sge_list;
 +		}
 +
 +		hr_qp->mtt.mtt_type = MTT_TYPE_WQE;
 +		page_shift = PAGE_SHIFT;
 +		if (hr_dev->caps.mtt_buf_pg_sz) {
 +			npages = (ib_umem_page_count(hr_qp->umem) +
 +				  (1 << hr_dev->caps.mtt_buf_pg_sz) - 1) /
 +				 (1 << hr_dev->caps.mtt_buf_pg_sz);
 +			page_shift += hr_dev->caps.mtt_buf_pg_sz;
 +			ret = hns_roce_mtt_init(hr_dev, npages,
 +				    page_shift,
 +				    &hr_qp->mtt);
 +		} else {
 +			ret = hns_roce_mtt_init(hr_dev,
 +						ib_umem_page_count(hr_qp->umem),
 +						page_shift, &hr_qp->mtt);
 +		}
 +		if (ret) {
 +			dev_err(dev, "hns_roce_mtt_init error for create qp\n");
 +			goto err_buf;
 +		}
 +
 +		ret = hns_roce_ib_umem_write_mtt(hr_dev, &hr_qp->mtt,
 +						 hr_qp->umem);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_ib_umem_write_mtt error for create qp\n");
 +			goto err_mtt;
 +		}
 +
 +		if ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_SQ_RECORD_DB) &&
 +		    (udata->inlen >= sizeof(ucmd)) &&
 +		    (udata->outlen >= sizeof(resp)) &&
 +		    hns_roce_qp_has_sq(init_attr)) {
 +			ret = hns_roce_db_map_user(uctx, udata, ucmd.sdb_addr,
 +						   &hr_qp->sdb);
 +			if (ret) {
 +				dev_err(dev, "sq record doorbell map failed!\n");
 +				goto err_mtt;
 +			}
 +
 +			/* indicate kernel supports sq record db */
 +			resp.cap_flags |= HNS_ROCE_SUPPORT_SQ_RECORD_DB;
 +			hr_qp->sdb_en = 1;
 +		}
 +
 +		if ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB) &&
 +		    (udata->outlen >= sizeof(resp)) &&
 +		    hns_roce_qp_has_rq(init_attr)) {
 +			ret = hns_roce_db_map_user(uctx, udata, ucmd.db_addr,
 +						   &hr_qp->rdb);
 +			if (ret) {
 +				dev_err(dev, "rq record doorbell map failed!\n");
 +				goto err_sq_dbmap;
 +			}
 +
 +			/* indicate kernel supports rq record db */
 +			resp.cap_flags |= HNS_ROCE_SUPPORT_RQ_RECORD_DB;
 +			hr_qp->rdb_en = 1;
 +		}
 +	} else {
 +		if (init_attr->create_flags &
 +		    IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK) {
 +			dev_err(dev, "init_attr->create_flags error!\n");
 +			ret = -EINVAL;
 +			goto err_rq_sge_list;
 +		}
 +
 +		if (init_attr->create_flags & IB_QP_CREATE_IPOIB_UD_LSO) {
 +			dev_err(dev, "init_attr->create_flags error!\n");
 +			ret = -EINVAL;
 +			goto err_rq_sge_list;
 +		}
 +
 +		/* Set SQ size */
 +		ret = hns_roce_set_kernel_sq_size(hr_dev, &init_attr->cap,
 +						  hr_qp);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_set_kernel_sq_size error!\n");
 +			goto err_rq_sge_list;
 +		}
 +
 +		/* QP doorbell register address */
 +		hr_qp->sq.db_reg_l = hr_dev->reg_base + hr_dev->sdb_offset +
 +				     DB_REG_OFFSET * hr_dev->priv_uar.index;
 +		hr_qp->rq.db_reg_l = hr_dev->reg_base + hr_dev->odb_offset +
 +				     DB_REG_OFFSET * hr_dev->priv_uar.index;
 +
 +		if ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_RECORD_DB) &&
 +		    hns_roce_qp_has_rq(init_attr)) {
 +			ret = hns_roce_alloc_db(hr_dev, &hr_qp->rdb, 0);
 +			if (ret) {
 +				dev_err(dev, "rq record doorbell alloc failed!\n");
 +				goto err_rq_sge_list;
 +			}
 +			*hr_qp->rdb.db_record = 0;
 +			hr_qp->rdb_en = 1;
 +		}
 +
 +		/* Allocate QP buf */
 +		page_shift = PAGE_SHIFT + hr_dev->caps.mtt_buf_pg_sz;
 +		if (hns_roce_buf_alloc(hr_dev, hr_qp->buff_size,
 +				       (1 << page_shift) * 2,
 +				       &hr_qp->hr_buf, page_shift)) {
 +			dev_err(dev, "hns_roce_buf_alloc error!\n");
 +			ret = -ENOMEM;
 +			goto err_db;
 +		}
 +
 +		hr_qp->mtt.mtt_type = MTT_TYPE_WQE;
 +		/* Write MTT */
 +		ret = hns_roce_mtt_init(hr_dev, hr_qp->hr_buf.npages,
 +					hr_qp->hr_buf.page_shift, &hr_qp->mtt);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_mtt_init error for kernel create qp\n");
 +			goto err_buf;
 +		}
 +
 +		ret = hns_roce_buf_write_mtt(hr_dev, &hr_qp->mtt,
 +					     &hr_qp->hr_buf);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_buf_write_mtt error for kernel create qp\n");
 +			goto err_mtt;
 +		}
 +
 +		hr_qp->sq.wrid = kmalloc_array(hr_qp->sq.wqe_cnt, sizeof(u64),
 +					       GFP_KERNEL);
 +		hr_qp->rq.wrid = kmalloc_array(hr_qp->rq.wqe_cnt, sizeof(u64),
 +					       GFP_KERNEL);
 +		if (!hr_qp->sq.wrid || !hr_qp->rq.wrid) {
 +			ret = -ENOMEM;
 +			goto err_wrid;
 +		}
  	}
  
 -	ret = hns_roce_qp_store(hr_dev, hr_qp, init_attr);
 -	if (ret) {
 -		ibdev_err(ibdev, "Failed to store QP\n");
 -		goto err_qpc;
 +	if (sqpn) {
 +		qpn = sqpn;
 +	} else {
 +		/* Get QPN */
 +		ret = hns_roce_reserve_range_qp(hr_dev, 1, 1, &qpn);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_reserve_range_qp alloc qpn error\n");
 +			goto err_wrid;
 +		}
  	}
  
 -	if (udata) {
 -		ret = ib_copy_to_udata(udata, &resp,
 -				       min(udata->outlen, sizeof(resp)));
 +	if (init_attr->qp_type == IB_QPT_GSI &&
 +	    hr_dev->hw_rev == HNS_ROCE_HW_VER1) {
 +		/* In v1 engine, GSI QP context in RoCE engine's register */
 +		ret = hns_roce_gsi_qp_alloc(hr_dev, qpn, hr_qp);
  		if (ret) {
 -			ibdev_err(ibdev, "copy qp resp failed!\n");
 -			goto err_store;
 +			dev_err(dev, "hns_roce_qp_alloc failed!\n");
 +			goto err_qpn;
 +		}
 +	} else {
 +		ret = hns_roce_qp_alloc(hr_dev, qpn, hr_qp);
 +		if (ret) {
 +			dev_err(dev, "hns_roce_qp_alloc failed!\n");
 +			goto err_qpn;
  		}
  	}
  
diff --cc drivers/infiniband/hw/mlx4/main.c
index 83d23bae44ad,84d8fcbb85ef..000000000000
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@@ -2655,38 -2657,7 +2655,41 @@@ static void *mlx4_ib_add(struct mlx4_de
  	ibdev->ib_dev.num_comp_vectors	= dev->caps.num_comp_vectors;
  	ibdev->ib_dev.dev.parent	= &dev->persist->pdev->dev;
  
 +	ibdev->ib_dev.uverbs_cmd_mask	=
 +		(1ull << IB_USER_VERBS_CMD_GET_CONTEXT)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE)	|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_PORT)		|
 +		(1ull << IB_USER_VERBS_CMD_ALLOC_PD)		|
 +		(1ull << IB_USER_VERBS_CMD_DEALLOC_PD)		|
 +		(1ull << IB_USER_VERBS_CMD_REG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_REREG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_DEREG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL)	|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_RESIZE_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_MODIFY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_ATTACH_MCAST)	|
 +		(1ull << IB_USER_VERBS_CMD_DETACH_MCAST)	|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_MODIFY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_XSRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_OPEN_QP);
 +
  	ib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_ops);
++<<<<<<< HEAD
 +	ibdev->ib_dev.uverbs_ex_cmd_mask |=
 +		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_CQ) |
 +		(1ull << IB_USER_VERBS_EX_CMD_QUERY_DEVICE) |
 +		(1ull << IB_USER_VERBS_EX_CMD_CREATE_CQ) |
 +		(1ull << IB_USER_VERBS_EX_CMD_CREATE_QP);
++=======
++>>>>>>> 1f11a7610e50 (RDMA: Check create_flags during create_qp)
  
  	if ((dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS) &&
  	    ((mlx4_ib_port_link_layer(&ibdev->ib_dev, 1) ==
diff --cc drivers/infiniband/hw/mlx5/main.c
index 97d5f414e693,daa1de6497e7..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -4150,41 -4139,9 +4150,45 @@@ static int mlx5_ib_stage_caps_init(stru
  	struct mlx5_core_dev *mdev = dev->mdev;
  	int err;
  
 -	dev->ib_dev.uverbs_cmd_mask |=
 +	dev->ib_dev.uverbs_cmd_mask	=
 +		(1ull << IB_USER_VERBS_CMD_GET_CONTEXT)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE)	|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_PORT)		|
 +		(1ull << IB_USER_VERBS_CMD_ALLOC_PD)		|
 +		(1ull << IB_USER_VERBS_CMD_DEALLOC_PD)		|
  		(1ull << IB_USER_VERBS_CMD_CREATE_AH)		|
++<<<<<<< HEAD
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_AH)		|
 +		(1ull << IB_USER_VERBS_CMD_REG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_REREG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_DEREG_MR)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL)	|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_RESIZE_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_CQ)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_MODIFY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_QP)		|
 +		(1ull << IB_USER_VERBS_CMD_ATTACH_MCAST)	|
 +		(1ull << IB_USER_VERBS_CMD_DETACH_MCAST)	|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_MODIFY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_QUERY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_DESTROY_SRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_CREATE_XSRQ)		|
 +		(1ull << IB_USER_VERBS_CMD_OPEN_QP);
 +	dev->ib_dev.uverbs_ex_cmd_mask =
 +		(1ull << IB_USER_VERBS_EX_CMD_QUERY_DEVICE)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_CREATE_CQ)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_CREATE_QP)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_QP)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_CQ)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_CREATE_FLOW)	|
 +		(1ull << IB_USER_VERBS_EX_CMD_DESTROY_FLOW);
++=======
+ 		(1ull << IB_USER_VERBS_CMD_DESTROY_AH);
++>>>>>>> 1f11a7610e50 (RDMA: Check create_flags during create_qp)
  
  	if (MLX5_CAP_GEN(mdev, ipoib_enhanced_offloads) &&
  	    IS_ENABLED(CONFIG_MLX5_CORE_IPOIB))
* Unmerged path drivers/infiniband/core/device.c
diff --git a/drivers/infiniband/hw/bnxt_re/ib_verbs.c b/drivers/infiniband/hw/bnxt_re/ib_verbs.c
index f9705083f078..332c31c055af 100644
--- a/drivers/infiniband/hw/bnxt_re/ib_verbs.c
+++ b/drivers/infiniband/hw/bnxt_re/ib_verbs.c
@@ -1275,10 +1275,12 @@ static int bnxt_re_init_qp_attr(struct bnxt_re_qp *qp, struct bnxt_re_pd *pd,
 	}
 	qplqp->mtu = ib_mtu_enum_to_int(iboe_get_mtu(rdev->netdev->mtu));
 	qplqp->dpi = &rdev->dpi_privileged; /* Doorbell page */
-	if (init_attr->create_flags)
+	if (init_attr->create_flags) {
 		ibdev_dbg(&rdev->ibdev,
 			  "QP create flags 0x%x not supported",
 			  init_attr->create_flags);
+		return -EOPNOTSUPP;
+	}
 
 	/* Setup CQs */
 	if (init_attr->send_cq) {
diff --git a/drivers/infiniband/hw/cxgb4/qp.c b/drivers/infiniband/hw/cxgb4/qp.c
index 4d7acd7576eb..a5c625e36fd7 100644
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@ -2128,7 +2128,7 @@ struct ib_qp *c4iw_create_qp(struct ib_pd *pd, struct ib_qp_init_attr *attrs,
 
 	pr_debug("ib_pd %p\n", pd);
 
-	if (attrs->qp_type != IB_QPT_RC)
+	if (attrs->qp_type != IB_QPT_RC || attrs->create_flags)
 		return ERR_PTR(-EOPNOTSUPP);
 
 	php = to_c4iw_pd(pd);
* Unmerged path drivers/infiniband/hw/hns/hns_roce_qp.c
diff --git a/drivers/infiniband/hw/i40iw/i40iw_verbs.c b/drivers/infiniband/hw/i40iw/i40iw_verbs.c
index f03d9769941c..584d349bc064 100644
--- a/drivers/infiniband/hw/i40iw/i40iw_verbs.c
+++ b/drivers/infiniband/hw/i40iw/i40iw_verbs.c
@@ -557,7 +557,7 @@ static struct ib_qp *i40iw_create_qp(struct ib_pd *ibpd,
 		return ERR_PTR(-ENODEV);
 
 	if (init_attr->create_flags)
-		return ERR_PTR(-EINVAL);
+		return ERR_PTR(-EOPNOTSUPP);
 	if (init_attr->cap.max_inline_data > I40IW_MAX_INLINE_DATA_SIZE)
 		init_attr->cap.max_inline_data = I40IW_MAX_INLINE_DATA_SIZE;
 
* Unmerged path drivers/infiniband/hw/mlx4/main.c
diff --git a/drivers/infiniband/hw/mlx4/qp.c b/drivers/infiniband/hw/mlx4/qp.c
index 7bf33a449bf7..c3b96223575f 100644
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@ -1492,7 +1492,7 @@ static int _mlx4_ib_create_qp(struct ib_pd *pd, struct mlx4_ib_qp *qp,
 					MLX4_IB_SRIOV_SQP |
 					MLX4_IB_QP_NETIF |
 					MLX4_IB_QP_CREATE_ROCE_V2_GSI))
-		return -EINVAL;
+		return -EOPNOTSUPP;
 
 	if (init_attr->create_flags & IB_QP_CREATE_NETIF_QP) {
 		if (init_attr->qp_type != IB_QPT_UD)
* Unmerged path drivers/infiniband/hw/mlx5/main.c
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index a6560ec2d863..52569e7fce74 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -2713,11 +2713,12 @@ static int process_create_flags(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 	process_create_flag(dev, &create_flags, MLX5_IB_QP_CREATE_SQPN_QP1,
 			    true, qp);
 
-	if (create_flags)
+	if (create_flags) {
 		mlx5_ib_dbg(dev, "Create QP has unsupported flags 0x%X\n",
 			    create_flags);
-
-	return (create_flags) ? -EINVAL : 0;
+		return -EOPNOTSUPP;
+	}
+	return 0;
 }
 
 static int process_udata_size(struct mlx5_ib_dev *dev,
diff --git a/drivers/infiniband/hw/mthca/mthca_provider.c b/drivers/infiniband/hw/mthca/mthca_provider.c
index 368e0268177f..ea3958edb9b9 100644
--- a/drivers/infiniband/hw/mthca/mthca_provider.c
+++ b/drivers/infiniband/hw/mthca/mthca_provider.c
@@ -469,7 +469,7 @@ static struct ib_qp *mthca_create_qp(struct ib_pd *pd,
 	int err;
 
 	if (init_attr->create_flags)
-		return ERR_PTR(-EINVAL);
+		return ERR_PTR(-EOPNOTSUPP);
 
 	switch (init_attr->qp_type) {
 	case IB_QPT_RC:
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
index bbeb7234154f..86ddc2f26d01 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
@@ -1312,6 +1312,9 @@ struct ib_qp *ocrdma_create_qp(struct ib_pd *ibpd,
 	struct ocrdma_create_qp_ureq ureq;
 	u16 dpp_credit_lmt, dpp_offset;
 
+	if (attrs->create_flags)
+		return ERR_PTR(-EOPNOTSUPP);
+
 	status = ocrdma_check_qp_params(ibpd, dev, attrs, udata);
 	if (status)
 		goto gen_err;
diff --git a/drivers/infiniband/hw/qedr/verbs.c b/drivers/infiniband/hw/qedr/verbs.c
index b1278a5a7205..491695e42408 100644
--- a/drivers/infiniband/hw/qedr/verbs.c
+++ b/drivers/infiniband/hw/qedr/verbs.c
@@ -2217,6 +2217,9 @@ struct ib_qp *qedr_create_qp(struct ib_pd *ibpd,
 	struct ib_qp *ibqp;
 	int rc = 0;
 
+	if (attrs->create_flags)
+		return ERR_PTR(-EOPNOTSUPP);
+
 	if (attrs->qp_type == IB_QPT_XRC_TGT) {
 		xrcd = get_qedr_xrcd(attrs->xrcd);
 		dev = get_qedr_dev(xrcd->ibxrcd.device);
diff --git a/drivers/infiniband/hw/usnic/usnic_ib_verbs.c b/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
index b8dca2ea9769..54243ab456f0 100644
--- a/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
+++ b/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
@@ -495,7 +495,7 @@ struct ib_qp *usnic_ib_create_qp(struct ib_pd *pd,
 	us_ibdev = to_usdev(pd->device);
 
 	if (init_attr->create_flags)
-		return ERR_PTR(-EINVAL);
+		return ERR_PTR(-EOPNOTSUPP);
 
 	err = ib_copy_from_udata(&cmd, udata, sizeof(cmd));
 	if (err) {
diff --git a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_qp.c b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_qp.c
index af09d9d56c4b..bdcfbcd8a2d5 100644
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_qp.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_qp.c
@@ -209,7 +209,7 @@ struct ib_qp *pvrdma_create_qp(struct ib_pd *pd,
 		dev_warn(&dev->pdev->dev,
 			 "invalid create queuepair flags %#x\n",
 			 init_attr->create_flags);
-		return ERR_PTR(-EINVAL);
+		return ERR_PTR(-EOPNOTSUPP);
 	}
 
 	if (init_attr->qp_type != IB_QPT_RC &&
diff --git a/drivers/infiniband/sw/rdmavt/qp.c b/drivers/infiniband/sw/rdmavt/qp.c
index 332a8ba94b81..6792f91e93b9 100644
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@ -1083,10 +1083,11 @@ struct ib_qp *rvt_create_qp(struct ib_pd *ibpd,
 	if (!rdi)
 		return ERR_PTR(-EINVAL);
 
+	if (init_attr->create_flags & ~IB_QP_CREATE_NETDEV_USE)
+		return ERR_PTR(-EOPNOTSUPP);
+
 	if (init_attr->cap.max_send_sge > rdi->dparms.props.max_send_sge ||
-	    init_attr->cap.max_send_wr > rdi->dparms.props.max_qp_wr ||
-	    (init_attr->create_flags &&
-	     init_attr->create_flags != IB_QP_CREATE_NETDEV_USE))
+	    init_attr->cap.max_send_wr > rdi->dparms.props.max_qp_wr)
 		return ERR_PTR(-EINVAL);
 
 	/* Check receive queue parameters if no SRQ is specified. */
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 6f655b55dded..aa04534540ce 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -391,6 +391,9 @@ static struct ib_qp *rxe_create_qp(struct ib_pd *ibpd,
 		uresp = udata->outbuf;
 	}
 
+	if (init->create_flags)
+		return ERR_PTR(-EOPNOTSUPP);
+
 	err = rxe_qp_chk_init(rxe, init);
 	if (err)
 		goto err1;
diff --git a/drivers/infiniband/sw/siw/siw_verbs.c b/drivers/infiniband/sw/siw/siw_verbs.c
index db44999ac086..733d1cc18114 100644
--- a/drivers/infiniband/sw/siw/siw_verbs.c
+++ b/drivers/infiniband/sw/siw/siw_verbs.c
@@ -307,6 +307,9 @@ struct ib_qp *siw_create_qp(struct ib_pd *pd,
 
 	siw_dbg(base_dev, "create new QP\n");
 
+	if (attrs->create_flags)
+		return ERR_PTR(-EOPNOTSUPP);
+
 	if (atomic_inc_return(&sdev->num_qp) > SIW_MAX_QP) {
 		siw_dbg(base_dev, "too many QP's\n");
 		rv = -ENOMEM;
