KVM: x86: Fold "write-protect large" use case into generic write-protect

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Sean Christopherson <seanjc@google.com>
commit a1419f8b5bab477d96a71d1c37da0784fb18dc51
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/a1419f8b.failed

Drop kvm_mmu_slot_largepage_remove_write_access() and refactor its sole
caller to use kvm_mmu_slot_remove_write_access().  Remove the now-unused
slot_handle_large_level() and slot_handle_all_level() helpers.

No functional change intended.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210213005015.1651772-14-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a1419f8b5bab477d96a71d1c37da0784fb18dc51)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/mmu/mmu.c
index d71bddea6b0a,e2178e0526d9..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5608,41 -5567,7 +5592,44 @@@ void kvm_mmu_slot_leaf_clear_dirty(stru
  	if (flush)
  		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
  }
 +EXPORT_SYMBOL_GPL(kvm_mmu_slot_leaf_clear_dirty);
 +
++<<<<<<< HEAD
 +void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 +					struct kvm_memory_slot *memslot)
 +{
 +	bool flush;
 +
 +	write_lock(&kvm->mmu_lock);
 +	flush = slot_handle_large_level(kvm, memslot, slot_rmap_write_protect,
 +					false);
 +	if (is_tdp_mmu_enabled(kvm))
 +		flush |= kvm_tdp_mmu_wrprot_slot(kvm, memslot, PG_LEVEL_2M);
 +	write_unlock(&kvm->mmu_lock);
 +
 +	if (flush)
 +		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
 +}
 +EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
 +
 +void kvm_mmu_slot_set_dirty(struct kvm *kvm,
 +			    struct kvm_memory_slot *memslot)
 +{
 +	bool flush;
 +
 +	write_lock(&kvm->mmu_lock);
 +	flush = slot_handle_all_level(kvm, memslot, __rmap_set_dirty, false);
 +	if (is_tdp_mmu_enabled(kvm))
 +		flush |= kvm_tdp_mmu_slot_set_dirty(kvm, memslot);
 +	write_unlock(&kvm->mmu_lock);
 +
 +	if (flush)
 +		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
 +}
 +EXPORT_SYMBOL_GPL(kvm_mmu_slot_set_dirty);
  
++=======
++>>>>>>> a1419f8b5bab (KVM: x86: Fold "write-protect large" use case into generic write-protect)
  void kvm_mmu_zap_all(struct kvm *kvm)
  {
  	struct kvm_mmu_page *sp, *node;
diff --cc arch/x86/kvm/x86.c
index 7d556c7e50f6,1d2bc89431a2..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -10648,62 -10813,41 +10648,84 @@@ static void kvm_mmu_slot_apply_flags(st
  	if (WARN_ON_ONCE(!((old->flags ^ new->flags) & KVM_MEM_LOG_DIRTY_PAGES)))
  		return;
  
 -	if (!log_dirty_pages) {
 -		/*
 -		 * Dirty logging tracks sptes in 4k granularity, meaning that
 -		 * large sptes have to be split.  If live migration succeeds,
 -		 * the guest in the source machine will be destroyed and large
 -		 * sptes will be created in the destination.  However, if the
 -		 * guest continues to run in the source machine (for example if
 -		 * live migration fails), small sptes will remain around and
 -		 * cause bad performance.
 -		 *
 -		 * Scan sptes if dirty logging has been stopped, dropping those
 -		 * which can be collapsed into a single large-page spte.  Later
 -		 * page faults will create the large-page sptes.
 -		 */
 +	/*
 +	 * Dirty logging tracks sptes in 4k granularity, meaning that large
 +	 * sptes have to be split.  If live migration is successful, the guest
 +	 * in the source machine will be destroyed and large sptes will be
 +	 * created in the destination. However, if the guest continues to run
 +	 * in the source machine (for example if live migration fails), small
 +	 * sptes will remain around and cause bad performance.
 +	 *
 +	 * Scan sptes if dirty logging has been stopped, dropping those
 +	 * which can be collapsed into a single large-page spte.  Later
 +	 * page faults will create the large-page sptes.
 +	 *
 +	 * There is no need to do this in any of the following cases:
 +	 * CREATE:      No dirty mappings will already exist.
 +	 * MOVE/DELETE: The old mappings will already have been cleaned up by
 +	 *		kvm_arch_flush_shadow_memslot()
 +	 */
 +	if (!(new->flags & KVM_MEM_LOG_DIRTY_PAGES))
  		kvm_mmu_zap_collapsible_sptes(kvm, new);
++<<<<<<< HEAD
 +
 +	/*
 +	 * Enable or disable dirty logging for the slot.
 +	 *
 +	 * For KVM_MR_DELETE and KVM_MR_MOVE, the shadow pages of the old
 +	 * slot have been zapped so no dirty logging updates are needed for
 +	 * the old slot.
 +	 * For KVM_MR_CREATE and KVM_MR_MOVE, once the new slot is visible
 +	 * any mappings that might be created in it will consume the
 +	 * properties of the new slot and do not need to be updated here.
 +	 *
 +	 * When PML is enabled, the kvm_x86_ops dirty logging hooks are
 +	 * called to enable/disable dirty logging.
 +	 *
 +	 * When disabling dirty logging with PML enabled, the D-bit is set
 +	 * for sptes in the slot in order to prevent unnecessary GPA
 +	 * logging in the PML buffer (and potential PML buffer full VMEXIT).
 +	 * This guarantees leaving PML enabled for the guest's lifetime
 +	 * won't have any additional overhead from PML when the guest is
 +	 * running with dirty logging disabled.
 +	 *
 +	 * When enabling dirty logging, large sptes are write-protected
 +	 * so they can be split on first write.  New large sptes cannot
 +	 * be created for this slot until the end of the logging.
 +	 * See the comments in fast_page_fault().
 +	 * For small sptes, nothing is done if the dirty log is in the
 +	 * initial-all-set state.  Otherwise, depending on whether pml
 +	 * is enabled the D-bit or the W-bit will be cleared.
 +	 */
 +	if (new->flags & KVM_MEM_LOG_DIRTY_PAGES) {
 +		if (kvm_x86_ops.slot_enable_log_dirty) {
 +			kvm_x86_ops.slot_enable_log_dirty(kvm, new);
 +		} else {
 +			int level =
 +				kvm_dirty_log_manual_protect_and_init_set(kvm) ?
 +				PG_LEVEL_2M : PG_LEVEL_4K;
++=======
+ 	} else {
+ 		/* By default, write-protect everything to log writes. */
+ 		int level = PG_LEVEL_4K;
+ 
+ 		if (kvm_x86_ops.cpu_dirty_log_size) {
+ 			/*
+ 			 * Clear all dirty bits, unless pages are treated as
+ 			 * dirty from the get-go.
+ 			 */
+ 			if (!kvm_dirty_log_manual_protect_and_init_set(kvm))
+ 				kvm_mmu_slot_leaf_clear_dirty(kvm, new);
++>>>>>>> a1419f8b5bab (KVM: x86: Fold "write-protect large" use case into generic write-protect)
  
+ 			/*
+ 			 * Write-protect large pages on write so that dirty
+ 			 * logging happens at 4k granularity.  No need to
+ 			 * write-protect small SPTEs since write accesses are
+ 			 * logged by the CPU via dirty bits.
+ 			 */
+ 			level = PG_LEVEL_2M;
+ 		} else if (kvm_dirty_log_manual_protect_and_init_set(kvm)) {
  			/*
  			 * If we're with initial-all-set, we don't need
  			 * to write protect any small page because
@@@ -10712,11 -10856,9 +10734,15 @@@
  			 * so that the page split can happen lazily on
  			 * the first write to the huge page.
  			 */
- 			kvm_mmu_slot_remove_write_access(kvm, new, level);
+ 			level = PG_LEVEL_2M;
  		}
++<<<<<<< HEAD
 +	} else {
 +		if (kvm_x86_ops.slot_disable_log_dirty)
 +			kvm_x86_ops.slot_disable_log_dirty(kvm, new);
++=======
+ 		kvm_mmu_slot_remove_write_access(kvm, new, level);
++>>>>>>> a1419f8b5bab (KVM: x86: Fold "write-protect large" use case into generic write-protect)
  	}
  }
  
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/x86.c
