KVM: x86/mmu: Re-add const qualifier in kvm_tdp_mmu_zap_collapsible_sptes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Ben Gardon <bgardon@google.com>
commit 8ca6f063b73d3754213d009efb3df486c8fe52d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/8ca6f063.failed

kvm_tdp_mmu_zap_collapsible_sptes unnecessarily removes the const
qualifier from its memlsot argument, leading to a compiler warning. Add
the const annotation and pass it to subsequent functions.

	Signed-off-by: Ben Gardon <bgardon@google.com>
Message-Id: <20210401233736.638171-2-bgardon@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8ca6f063b73d3754213d009efb3df486c8fe52d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.h
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 53618b739090,5f517ef06e61..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -1367,23 -1230,16 +1367,29 @@@ static bool zap_collapsible_spte_range(
   * Clear non-leaf entries (and free associated page tables) which could
   * be replaced by large mappings, for GFNs within the slot.
   */
++<<<<<<< HEAD
 +void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 +				       struct kvm_memory_slot *slot)
++=======
+ bool kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
+ 				       const struct kvm_memory_slot *slot,
+ 				       bool flush)
++>>>>>>> 8ca6f063b73d (KVM: x86/mmu: Re-add const qualifier in kvm_tdp_mmu_zap_collapsible_sptes)
  {
  	struct kvm_mmu_page *root;
 +	bool flush = false;
 +	int root_as_id;
 +
 +	for_each_tdp_mmu_root_yield_safe(kvm, root) {
 +		root_as_id = kvm_mmu_page_as_id(root);
 +		if (root_as_id != slot->as_id)
 +			continue;
  
 -	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id)
  		flush = zap_collapsible_spte_range(kvm, root, slot, flush);
 +	}
  
 -	return flush;
 +	if (flush)
 +		kvm_flush_remote_tlbs(kvm);
  }
  
  /*
diff --cc arch/x86/kvm/mmu/tdp_mmu.h
index aae82a46a7bf,11050994e7e4..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@@ -56,9 -52,9 +56,15 @@@ void kvm_tdp_mmu_clear_dirty_pt_masked(
  				       struct kvm_memory_slot *slot,
  				       gfn_t gfn, unsigned long mask,
  				       bool wrprot);
++<<<<<<< HEAD
 +bool kvm_tdp_mmu_slot_set_dirty(struct kvm *kvm, struct kvm_memory_slot *slot);
 +void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 +				       struct kvm_memory_slot *slot);
++=======
+ bool kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
+ 				       const struct kvm_memory_slot *slot,
+ 				       bool flush);
++>>>>>>> 8ca6f063b73d (KVM: x86/mmu: Re-add const qualifier in kvm_tdp_mmu_zap_collapsible_sptes)
  
  bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  				   struct kvm_memory_slot *slot, gfn_t gfn);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 47be0781c99b..6a232c48d91d 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -714,8 +714,7 @@ static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
  * handling slots that are not large page aligned.
  */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
-					      struct kvm_memory_slot *slot,
-					      int level)
+		const struct kvm_memory_slot *slot, int level)
 {
 	unsigned long idx;
 
@@ -2754,7 +2753,7 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 }
 
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
-				  struct kvm_memory_slot *slot)
+				  const struct kvm_memory_slot *slot)
 {
 	unsigned long hva;
 	pte_t *pte;
@@ -2780,8 +2779,9 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	return level;
 }
 
-int kvm_mmu_max_mapping_level(struct kvm *kvm, struct kvm_memory_slot *slot,
-			      gfn_t gfn, kvm_pfn_t pfn, int max_level)
+int kvm_mmu_max_mapping_level(struct kvm *kvm,
+			      const struct kvm_memory_slot *slot, gfn_t gfn,
+			      kvm_pfn_t pfn, int max_level)
 {
 	struct kvm_lpage_info *linfo;
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index e03267e93459..fc88f62d7bd9 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -156,8 +156,9 @@ enum {
 #define SET_SPTE_NEED_REMOTE_TLB_FLUSH	BIT(1)
 #define SET_SPTE_SPURIOUS		BIT(2)
 
-int kvm_mmu_max_mapping_level(struct kvm *kvm, struct kvm_memory_slot *slot,
-			      gfn_t gfn, kvm_pfn_t pfn, int max_level);
+int kvm_mmu_max_mapping_level(struct kvm *kvm,
+			      const struct kvm_memory_slot *slot, gfn_t gfn,
+			      kvm_pfn_t pfn, int max_level);
 int kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
 			    int max_level, kvm_pfn_t *pfnp,
 			    bool huge_page_disallowed, int *req_level);
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.h
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 70d22f91c066..104f0664b1cb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1114,7 +1114,7 @@ __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 }
 
 static inline unsigned long
-__gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
+__gfn_to_hva_memslot(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
 }
