RDMA: Clean MW allocation and free flows

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit d18bb3e15201918b8d07e85a6e010ca5ed28dad5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/d18bb3e1.failed

Move allocation and destruction of memory windows under ib_core
responsibility and clean drivers to ensure that no updates to MW
ib_core structures are done in driver layer.

Link: https://lore.kernel.org/r/20200902081623.746359-2-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit d18bb3e15201918b8d07e85a6e010ca5ed28dad5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_device.h
#	drivers/infiniband/hw/hns/hns_roce_main.c
#	drivers/infiniband/hw/hns/hns_roce_mr.c
diff --cc drivers/infiniband/hw/hns/hns_roce_device.h
index 5d3a5b83b971,3cab23f5ac69..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@@ -968,14 -1194,20 +968,21 @@@ struct ib_mr *hns_roce_reg_user_mr(stru
  int hns_roce_rereg_user_mr(struct ib_mr *mr, int flags, u64 start, u64 length,
  			   u64 virt_addr, int mr_access_flags, struct ib_pd *pd,
  			   struct ib_udata *udata);
 -struct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 -				u32 max_num_sg);
 -int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 -		       unsigned int *sg_offset);
  int hns_roce_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
 -int hns_roce_hw_destroy_mpt(struct hns_roce_dev *hr_dev,
 -			    struct hns_roce_cmd_mailbox *mailbox,
 -			    unsigned long mpt_index);
 +int hns_roce_hw2sw_mpt(struct hns_roce_dev *hr_dev,
 +		       struct hns_roce_cmd_mailbox *mailbox,
 +		       unsigned long mpt_index);
  unsigned long key_to_hw_index(u32 key);
  
++<<<<<<< HEAD
 +void hns_roce_buf_free(struct hns_roce_dev *hr_dev, u32 size,
 +		       struct hns_roce_buf *buf);
++=======
+ int hns_roce_alloc_mw(struct ib_mw *mw, struct ib_udata *udata);
+ int hns_roce_dealloc_mw(struct ib_mw *ibmw);
+ 
+ void hns_roce_buf_free(struct hns_roce_dev *hr_dev, struct hns_roce_buf *buf);
++>>>>>>> d18bb3e15201 (RDMA: Clean MW allocation and free flows)
  int hns_roce_buf_alloc(struct hns_roce_dev *hr_dev, u32 size, u32 max_direct,
  		       struct hns_roce_buf *buf, u32 page_shift);
  
diff --cc drivers/infiniband/hw/hns/hns_roce_main.c
index 8b1f50f7c10f,2b4d75733e72..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_main.c
+++ b/drivers/infiniband/hw/hns/hns_roce_main.c
@@@ -457,6 -451,25 +457,28 @@@ static const struct ib_device_ops hns_r
  	.rereg_user_mr = hns_roce_rereg_user_mr,
  };
  
++<<<<<<< HEAD
++=======
+ static const struct ib_device_ops hns_roce_dev_mw_ops = {
+ 	.alloc_mw = hns_roce_alloc_mw,
+ 	.dealloc_mw = hns_roce_dealloc_mw,
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_mw, hns_roce_mw, ibmw),
+ };
+ 
+ static const struct ib_device_ops hns_roce_dev_frmr_ops = {
+ 	.alloc_mr = hns_roce_alloc_mr,
+ 	.map_mr_sg = hns_roce_map_mr_sg,
+ };
+ 
+ static const struct ib_device_ops hns_roce_dev_srq_ops = {
+ 	.create_srq = hns_roce_create_srq,
+ 	.destroy_srq = hns_roce_destroy_srq,
+ 
+ 	INIT_RDMA_OBJ_SIZE(ib_srq, hns_roce_srq, ibsrq),
+ };
+ 
++>>>>>>> d18bb3e15201 (RDMA: Clean MW allocation and free flows)
  static int hns_roce_register_device(struct hns_roce_dev *hr_dev)
  {
  	int ret;
diff --cc drivers/infiniband/hw/hns/hns_roce_mr.c
index ba1754ca6d84,7f81a695e9af..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_mr.c
+++ b/drivers/infiniband/hw/hns/hns_roce_mr.c
@@@ -1211,3 -413,704 +1211,707 @@@ int hns_roce_dereg_mr(struct ib_mr *ibm
  
  	return ret;
  }
++<<<<<<< HEAD
++=======
+ 
+ struct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
+ 				u32 max_num_sg)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(pd->device);
+ 	struct device *dev = hr_dev->dev;
+ 	struct hns_roce_mr *mr;
+ 	u64 length;
+ 	int ret;
+ 
+ 	if (mr_type != IB_MR_TYPE_MEM_REG)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	if (max_num_sg > HNS_ROCE_FRMR_MAX_PA) {
+ 		dev_err(dev, "max_num_sg larger than %d\n",
+ 			HNS_ROCE_FRMR_MAX_PA);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
+ 	if (!mr)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mr->type = MR_TYPE_FRMR;
+ 
+ 	/* Allocate memory region key */
+ 	length = max_num_sg * (1 << PAGE_SHIFT);
+ 	ret = alloc_mr_key(hr_dev, mr, to_hr_pd(pd)->pdn, 0, length, 0);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	ret = alloc_mr_pbl(hr_dev, mr, length, NULL, 0, 0);
+ 	if (ret)
+ 		goto err_key;
+ 
+ 	ret = hns_roce_mr_enable(hr_dev, mr);
+ 	if (ret)
+ 		goto err_pbl;
+ 
+ 	mr->ibmr.rkey = mr->ibmr.lkey = mr->key;
+ 	mr->ibmr.length = length;
+ 
+ 	return &mr->ibmr;
+ 
+ err_key:
+ 	free_mr_key(hr_dev, mr);
+ err_pbl:
+ 	free_mr_pbl(hr_dev, mr);
+ err_free:
+ 	kfree(mr);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int hns_roce_set_page(struct ib_mr *ibmr, u64 addr)
+ {
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 
+ 	if (likely(mr->npages < mr->pbl_mtr.hem_cfg.buf_pg_count)) {
+ 		mr->page_list[mr->npages++] = addr;
+ 		return 0;
+ 	}
+ 
+ 	return -ENOBUFS;
+ }
+ 
+ int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
+ 		       unsigned int *sg_offset)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 	struct hns_roce_mtr *mtr = &mr->pbl_mtr;
+ 	int ret = 0;
+ 
+ 	mr->npages = 0;
+ 	mr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count,
+ 				 sizeof(dma_addr_t), GFP_KERNEL);
+ 	if (!mr->page_list)
+ 		return ret;
+ 
+ 	ret = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page);
+ 	if (ret < 1) {
+ 		ibdev_err(ibdev, "failed to store sg pages %d %d, cnt = %d.\n",
+ 			  mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, ret);
+ 		goto err_page_list;
+ 	}
+ 
+ 	mtr->hem_cfg.region[0].offset = 0;
+ 	mtr->hem_cfg.region[0].count = mr->npages;
+ 	mtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num;
+ 	mtr->hem_cfg.region_count = 1;
+ 	ret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to map sg mtr, ret = %d.\n", ret);
+ 		ret = 0;
+ 	} else {
+ 		mr->pbl_mtr.hem_cfg.buf_pg_shift = ilog2(ibmr->page_size);
+ 		ret = mr->npages;
+ 	}
+ 
+ err_page_list:
+ 	kvfree(mr->page_list);
+ 	mr->page_list = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static void hns_roce_mw_free(struct hns_roce_dev *hr_dev,
+ 			     struct hns_roce_mw *mw)
+ {
+ 	struct device *dev = hr_dev->dev;
+ 	int ret;
+ 
+ 	if (mw->enabled) {
+ 		ret = hns_roce_hw_destroy_mpt(hr_dev, NULL,
+ 					      key_to_hw_index(mw->rkey) &
+ 					      (hr_dev->caps.num_mtpts - 1));
+ 		if (ret)
+ 			dev_warn(dev, "MW DESTROY_MPT failed (%d)\n", ret);
+ 
+ 		hns_roce_table_put(hr_dev, &hr_dev->mr_table.mtpt_table,
+ 				   key_to_hw_index(mw->rkey));
+ 	}
+ 
+ 	hns_roce_bitmap_free(&hr_dev->mr_table.mtpt_bitmap,
+ 			     key_to_hw_index(mw->rkey), BITMAP_NO_RR);
+ }
+ 
+ static int hns_roce_mw_enable(struct hns_roce_dev *hr_dev,
+ 			      struct hns_roce_mw *mw)
+ {
+ 	struct hns_roce_mr_table *mr_table = &hr_dev->mr_table;
+ 	struct hns_roce_cmd_mailbox *mailbox;
+ 	struct device *dev = hr_dev->dev;
+ 	unsigned long mtpt_idx = key_to_hw_index(mw->rkey);
+ 	int ret;
+ 
+ 	/* prepare HEM entry memory */
+ 	ret = hns_roce_table_get(hr_dev, &mr_table->mtpt_table, mtpt_idx);
+ 	if (ret)
+ 		return ret;
+ 
+ 	mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 	if (IS_ERR(mailbox)) {
+ 		ret = PTR_ERR(mailbox);
+ 		goto err_table;
+ 	}
+ 
+ 	ret = hr_dev->hw->mw_write_mtpt(mailbox->buf, mw);
+ 	if (ret) {
+ 		dev_err(dev, "MW write mtpt fail!\n");
+ 		goto err_page;
+ 	}
+ 
+ 	ret = hns_roce_hw_create_mpt(hr_dev, mailbox,
+ 				     mtpt_idx & (hr_dev->caps.num_mtpts - 1));
+ 	if (ret) {
+ 		dev_err(dev, "MW CREATE_MPT failed (%d)\n", ret);
+ 		goto err_page;
+ 	}
+ 
+ 	mw->enabled = 1;
+ 
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ 	return 0;
+ 
+ err_page:
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ err_table:
+ 	hns_roce_table_put(hr_dev, &mr_table->mtpt_table, mtpt_idx);
+ 
+ 	return ret;
+ }
+ 
+ int hns_roce_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmw->device);
+ 	struct hns_roce_mw *mw = to_hr_mw(ibmw);
+ 	unsigned long index = 0;
+ 	int ret;
+ 
+ 	/* Allocate a key for mw from bitmap */
+ 	ret = hns_roce_bitmap_alloc(&hr_dev->mr_table.mtpt_bitmap, &index);
+ 	if (ret)
+ 		return ret;
+ 
+ 	mw->rkey = hw_index_to_key(index);
+ 
+ 	ibmw->rkey = mw->rkey;
+ 	mw->pdn = to_hr_pd(ibmw->pd)->pdn;
+ 	mw->pbl_hop_num = hr_dev->caps.pbl_hop_num;
+ 	mw->pbl_ba_pg_sz = hr_dev->caps.pbl_ba_pg_sz;
+ 	mw->pbl_buf_pg_sz = hr_dev->caps.pbl_buf_pg_sz;
+ 
+ 	ret = hns_roce_mw_enable(hr_dev, mw);
+ 	if (ret)
+ 		goto err_mw;
+ 
+ 	return 0;
+ 
+ err_mw:
+ 	hns_roce_mw_free(hr_dev, mw);
+ 	return ret;
+ }
+ 
+ int hns_roce_dealloc_mw(struct ib_mw *ibmw)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmw->device);
+ 	struct hns_roce_mw *mw = to_hr_mw(ibmw);
+ 
+ 	hns_roce_mw_free(hr_dev, mw);
+ 	return 0;
+ }
+ 
+ static int mtr_map_region(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			  dma_addr_t *pages, struct hns_roce_buf_region *region)
+ {
+ 	__le64 *mtts;
+ 	int offset;
+ 	int count;
+ 	int npage;
+ 	u64 addr;
+ 	int end;
+ 	int i;
+ 
+ 	/* if hopnum is 0, buffer cannot store BAs, so skip write mtt */
+ 	if (!region->hopnum)
+ 		return 0;
+ 
+ 	offset = region->offset;
+ 	end = offset + region->count;
+ 	npage = 0;
+ 	while (offset < end) {
+ 		mtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,
+ 						  offset, &count, NULL);
+ 		if (!mtts)
+ 			return -ENOBUFS;
+ 
+ 		for (i = 0; i < count; i++) {
+ 			if (hr_dev->hw_rev == HNS_ROCE_HW_VER1)
+ 				addr = to_hr_hw_page_addr(pages[npage]);
+ 			else
+ 				addr = pages[npage];
+ 
+ 			mtts[i] = cpu_to_le64(addr);
+ 			npage++;
+ 		}
+ 		offset += count;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static inline bool mtr_has_mtt(struct hns_roce_buf_attr *attr)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < attr->region_count; i++)
+ 		if (attr->region[i].hopnum != HNS_ROCE_HOP_NUM_0 &&
+ 		    attr->region[i].hopnum > 0)
+ 			return true;
+ 
+ 	/* because the mtr only one root base address, when hopnum is 0 means
+ 	 * root base address equals the first buffer address, thus all alloced
+ 	 * memory must in a continuous space accessed by direct mode.
+ 	 */
+ 	return false;
+ }
+ 
+ static inline size_t mtr_bufs_size(struct hns_roce_buf_attr *attr)
+ {
+ 	size_t size = 0;
+ 	int i;
+ 
+ 	for (i = 0; i < attr->region_count; i++)
+ 		size += attr->region[i].size;
+ 
+ 	return size;
+ }
+ 
+ static inline size_t mtr_kmem_direct_size(bool is_direct, size_t alloc_size,
+ 					  unsigned int page_shift)
+ {
+ 	if (is_direct)
+ 		return ALIGN(alloc_size, 1 << page_shift);
+ 	else
+ 		return HNS_HW_DIRECT_PAGE_COUNT << page_shift;
+ }
+ 
+ /*
+  * check the given pages in continuous address space
+  * Returns 0 on success, or the error page num.
+  */
+ static inline int mtr_check_direct_pages(dma_addr_t *pages, int page_count,
+ 					 unsigned int page_shift)
+ {
+ 	size_t page_size = 1 << page_shift;
+ 	int i;
+ 
+ 	for (i = 1; i < page_count; i++)
+ 		if (pages[i] - pages[i - 1] != page_size)
+ 			return i;
+ 
+ 	return 0;
+ }
+ 
+ static void mtr_free_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)
+ {
+ 	/* release user buffers */
+ 	if (mtr->umem) {
+ 		ib_umem_release(mtr->umem);
+ 		mtr->umem = NULL;
+ 	}
+ 
+ 	/* release kernel buffers */
+ 	if (mtr->kmem) {
+ 		hns_roce_buf_free(hr_dev, mtr->kmem);
+ 		kfree(mtr->kmem);
+ 		mtr->kmem = NULL;
+ 	}
+ }
+ 
+ static int mtr_alloc_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			  struct hns_roce_buf_attr *buf_attr, bool is_direct,
+ 			  struct ib_udata *udata, unsigned long user_addr)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	unsigned int best_pg_shift;
+ 	int all_pg_count = 0;
+ 	size_t direct_size;
+ 	size_t total_size;
+ 	int ret;
+ 
+ 	total_size = mtr_bufs_size(buf_attr);
+ 	if (total_size < 1) {
+ 		ibdev_err(ibdev, "Failed to check mtr size\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (udata) {
+ 		unsigned long pgsz_bitmap;
+ 		unsigned long page_size;
+ 
+ 		mtr->kmem = NULL;
+ 		mtr->umem = ib_umem_get(ibdev, user_addr, total_size,
+ 					buf_attr->user_access);
+ 		if (IS_ERR_OR_NULL(mtr->umem)) {
+ 			ibdev_err(ibdev, "Failed to get umem, ret %ld\n",
+ 				  PTR_ERR(mtr->umem));
+ 			return -ENOMEM;
+ 		}
+ 		if (buf_attr->fixed_page)
+ 			pgsz_bitmap = 1 << buf_attr->page_shift;
+ 		else
+ 			pgsz_bitmap = GENMASK(buf_attr->page_shift, PAGE_SHIFT);
+ 
+ 		page_size = ib_umem_find_best_pgsz(mtr->umem, pgsz_bitmap,
+ 						   user_addr);
+ 		if (!page_size)
+ 			return -EINVAL;
+ 		best_pg_shift = order_base_2(page_size);
+ 		all_pg_count = ib_umem_num_dma_blocks(mtr->umem, page_size);
+ 		ret = 0;
+ 	} else {
+ 		mtr->umem = NULL;
+ 		mtr->kmem = kzalloc(sizeof(*mtr->kmem), GFP_KERNEL);
+ 		if (!mtr->kmem) {
+ 			ibdev_err(ibdev, "Failed to alloc kmem\n");
+ 			return -ENOMEM;
+ 		}
+ 		direct_size = mtr_kmem_direct_size(is_direct, total_size,
+ 						   buf_attr->page_shift);
+ 		ret = hns_roce_buf_alloc(hr_dev, total_size, direct_size,
+ 					 mtr->kmem, buf_attr->page_shift);
+ 		if (ret) {
+ 			ibdev_err(ibdev, "Failed to alloc kmem, ret %d\n", ret);
+ 			goto err_alloc_mem;
+ 		}
+ 		best_pg_shift = buf_attr->page_shift;
+ 		all_pg_count = mtr->kmem->npages;
+ 	}
+ 
+ 	/* must bigger than minimum hardware page shift */
+ 	if (best_pg_shift < HNS_HW_PAGE_SHIFT || all_pg_count < 1) {
+ 		ret = -EINVAL;
+ 		ibdev_err(ibdev, "Failed to check mtr page shift %d count %d\n",
+ 			  best_pg_shift, all_pg_count);
+ 		goto err_alloc_mem;
+ 	}
+ 
+ 	mtr->hem_cfg.buf_pg_shift = best_pg_shift;
+ 	mtr->hem_cfg.buf_pg_count = all_pg_count;
+ 
+ 	return 0;
+ err_alloc_mem:
+ 	mtr_free_bufs(hr_dev, mtr);
+ 	return ret;
+ }
+ 
+ static int mtr_get_pages(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			 dma_addr_t *pages, int count, unsigned int page_shift)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	int npage;
+ 	int err;
+ 
+ 	if (mtr->umem)
+ 		npage = hns_roce_get_umem_bufs(hr_dev, pages, count, 0,
+ 					       mtr->umem, page_shift);
+ 	else
+ 		npage = hns_roce_get_kmem_bufs(hr_dev, pages, count, 0,
+ 					       mtr->kmem);
+ 
+ 	if (mtr->hem_cfg.is_direct && npage > 1) {
+ 		err = mtr_check_direct_pages(pages, npage, page_shift);
+ 		if (err) {
+ 			ibdev_err(ibdev, "Failed to check %s direct page-%d\n",
+ 				  mtr->umem ? "user" : "kernel", err);
+ 			npage = err;
+ 		}
+ 	}
+ 
+ 	return npage;
+ }
+ 
+ int hns_roce_mtr_map(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 		     dma_addr_t *pages, int page_cnt)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_buf_region *r;
+ 	int err;
+ 	int i;
+ 
+ 	/*
+ 	 * Only use the first page address as root ba when hopnum is 0, this
+ 	 * is because the addresses of all pages are consecutive in this case.
+ 	 */
+ 	if (mtr->hem_cfg.is_direct) {
+ 		mtr->hem_cfg.root_ba = pages[0];
+ 		return 0;
+ 	}
+ 
+ 	for (i = 0; i < mtr->hem_cfg.region_count; i++) {
+ 		r = &mtr->hem_cfg.region[i];
+ 		if (r->offset + r->count > page_cnt) {
+ 			err = -EINVAL;
+ 			ibdev_err(ibdev,
+ 				  "Failed to check mtr%d end %d + %d, max %d\n",
+ 				  i, r->offset, r->count, page_cnt);
+ 			return err;
+ 		}
+ 
+ 		err = mtr_map_region(hr_dev, mtr, &pages[r->offset], r);
+ 		if (err) {
+ 			ibdev_err(ibdev,
+ 				  "Failed to map mtr%d offset %d, err %d\n",
+ 				  i, r->offset, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int hns_roce_mtr_find(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 		      int offset, u64 *mtt_buf, int mtt_max, u64 *base_addr)
+ {
+ 	struct hns_roce_hem_cfg *cfg = &mtr->hem_cfg;
+ 	int start_index;
+ 	int mtt_count;
+ 	int total = 0;
+ 	__le64 *mtts;
+ 	int npage;
+ 	u64 addr;
+ 	int left;
+ 
+ 	if (!mtt_buf || mtt_max < 1)
+ 		goto done;
+ 
+ 	/* no mtt memory in direct mode, so just return the buffer address */
+ 	if (cfg->is_direct) {
+ 		start_index = offset >> HNS_HW_PAGE_SHIFT;
+ 		for (mtt_count = 0; mtt_count < cfg->region_count &&
+ 		     total < mtt_max; mtt_count++) {
+ 			npage = cfg->region[mtt_count].offset;
+ 			if (npage < start_index)
+ 				continue;
+ 
+ 			addr = cfg->root_ba + (npage << HNS_HW_PAGE_SHIFT);
+ 			if (hr_dev->hw_rev == HNS_ROCE_HW_VER1)
+ 				mtt_buf[total] = to_hr_hw_page_addr(addr);
+ 			else
+ 				mtt_buf[total] = addr;
+ 
+ 			total++;
+ 		}
+ 
+ 		goto done;
+ 	}
+ 
+ 	start_index = offset >> cfg->buf_pg_shift;
+ 	left = mtt_max;
+ 	while (left > 0) {
+ 		mtt_count = 0;
+ 		mtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,
+ 						  start_index + total,
+ 						  &mtt_count, NULL);
+ 		if (!mtts || !mtt_count)
+ 			goto done;
+ 
+ 		npage = min(mtt_count, left);
+ 		left -= npage;
+ 		for (mtt_count = 0; mtt_count < npage; mtt_count++)
+ 			mtt_buf[total++] = le64_to_cpu(mtts[mtt_count]);
+ 	}
+ 
+ done:
+ 	if (base_addr)
+ 		*base_addr = cfg->root_ba;
+ 
+ 	return total;
+ }
+ 
+ static int mtr_init_buf_cfg(struct hns_roce_dev *hr_dev,
+ 			    struct hns_roce_buf_attr *attr,
+ 			    struct hns_roce_hem_cfg *cfg,
+ 			    unsigned int *buf_page_shift)
+ {
+ 	struct hns_roce_buf_region *r;
+ 	unsigned int page_shift;
+ 	int page_cnt = 0;
+ 	size_t buf_size;
+ 	int region_cnt;
+ 
+ 	if (cfg->is_direct) {
+ 		buf_size = cfg->buf_pg_count << cfg->buf_pg_shift;
+ 		page_cnt = DIV_ROUND_UP(buf_size, HNS_HW_PAGE_SIZE);
+ 		/*
+ 		 * When HEM buffer use level-0 addressing, the page size equals
+ 		 * the buffer size, and the the page size = 4K * 2^N.
+ 		 */
+ 		cfg->buf_pg_shift = HNS_HW_PAGE_SHIFT + order_base_2(page_cnt);
+ 		if (attr->region_count > 1) {
+ 			cfg->buf_pg_count = page_cnt;
+ 			page_shift = HNS_HW_PAGE_SHIFT;
+ 		} else {
+ 			cfg->buf_pg_count = 1;
+ 			page_shift = cfg->buf_pg_shift;
+ 			if (buf_size != 1 << page_shift) {
+ 				ibdev_err(&hr_dev->ib_dev,
+ 					  "failed to check direct size %zu shift %d.\n",
+ 					  buf_size, page_shift);
+ 				return -EINVAL;
+ 			}
+ 		}
+ 	} else {
+ 		page_shift = cfg->buf_pg_shift;
+ 	}
+ 
+ 	/* convert buffer size to page index and page count */
+ 	for (page_cnt = 0, region_cnt = 0; page_cnt < cfg->buf_pg_count &&
+ 	     region_cnt < attr->region_count &&
+ 	     region_cnt < ARRAY_SIZE(cfg->region); region_cnt++) {
+ 		r = &cfg->region[region_cnt];
+ 		r->offset = page_cnt;
+ 		buf_size = hr_hw_page_align(attr->region[region_cnt].size);
+ 		r->count = DIV_ROUND_UP(buf_size, 1 << page_shift);
+ 		page_cnt += r->count;
+ 		r->hopnum = to_hr_hem_hopnum(attr->region[region_cnt].hopnum,
+ 					     r->count);
+ 	}
+ 
+ 	if (region_cnt < 1) {
+ 		ibdev_err(&hr_dev->ib_dev,
+ 			  "failed to check mtr region count, pages = %d.\n",
+ 			  cfg->buf_pg_count);
+ 		return -ENOBUFS;
+ 	}
+ 
+ 	cfg->region_count = region_cnt;
+ 	*buf_page_shift = page_shift;
+ 
+ 	return page_cnt;
+ }
+ 
+ /**
+  * hns_roce_mtr_create - Create hns memory translate region.
+  *
+  * @mtr: memory translate region
+  * @buf_attr: buffer attribute for creating mtr
+  * @ba_page_shift: page shift for multi-hop base address table
+  * @udata: user space context, if it's NULL, means kernel space
+  * @user_addr: userspace virtual address to start at
+  */
+ int hns_roce_mtr_create(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			struct hns_roce_buf_attr *buf_attr,
+ 			unsigned int ba_page_shift, struct ib_udata *udata,
+ 			unsigned long user_addr)
+ {
+ 	struct hns_roce_hem_cfg *cfg = &mtr->hem_cfg;
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	unsigned int buf_page_shift = 0;
+ 	dma_addr_t *pages = NULL;
+ 	int all_pg_cnt;
+ 	int get_pg_cnt;
+ 	int ret = 0;
+ 
+ 	/* if disable mtt, all pages must in a continuous address range */
+ 	cfg->is_direct = !mtr_has_mtt(buf_attr);
+ 
+ 	/* if buffer only need mtt, just init the hem cfg */
+ 	if (buf_attr->mtt_only) {
+ 		cfg->buf_pg_shift = buf_attr->page_shift;
+ 		cfg->buf_pg_count = mtr_bufs_size(buf_attr) >>
+ 				    buf_attr->page_shift;
+ 		mtr->umem = NULL;
+ 		mtr->kmem = NULL;
+ 	} else {
+ 		ret = mtr_alloc_bufs(hr_dev, mtr, buf_attr, cfg->is_direct,
+ 				     udata, user_addr);
+ 		if (ret) {
+ 			ibdev_err(ibdev,
+ 				  "failed to alloc mtr bufs, ret = %d.\n", ret);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	all_pg_cnt = mtr_init_buf_cfg(hr_dev, buf_attr, cfg, &buf_page_shift);
+ 	if (all_pg_cnt < 1) {
+ 		ret = -ENOBUFS;
+ 		ibdev_err(ibdev, "failed to init mtr buf cfg.\n");
+ 		goto err_alloc_bufs;
+ 	}
+ 
+ 	hns_roce_hem_list_init(&mtr->hem_list);
+ 	if (!cfg->is_direct) {
+ 		ret = hns_roce_hem_list_request(hr_dev, &mtr->hem_list,
+ 						cfg->region, cfg->region_count,
+ 						ba_page_shift);
+ 		if (ret) {
+ 			ibdev_err(ibdev, "failed to request mtr hem, ret = %d.\n",
+ 				  ret);
+ 			goto err_alloc_bufs;
+ 		}
+ 		cfg->root_ba = mtr->hem_list.root_ba;
+ 		cfg->ba_pg_shift = ba_page_shift;
+ 	} else {
+ 		cfg->ba_pg_shift = cfg->buf_pg_shift;
+ 	}
+ 
+ 	/* no buffer to map */
+ 	if (buf_attr->mtt_only)
+ 		return 0;
+ 
+ 	/* alloc a tmp array to store buffer's dma address */
+ 	pages = kvcalloc(all_pg_cnt, sizeof(dma_addr_t), GFP_KERNEL);
+ 	if (!pages) {
+ 		ret = -ENOMEM;
+ 		ibdev_err(ibdev, "failed to alloc mtr page list %d.\n",
+ 			  all_pg_cnt);
+ 		goto err_alloc_hem_list;
+ 	}
+ 
+ 	get_pg_cnt = mtr_get_pages(hr_dev, mtr, pages, all_pg_cnt,
+ 				   buf_page_shift);
+ 	if (get_pg_cnt != all_pg_cnt) {
+ 		ibdev_err(ibdev, "failed to get mtr page %d != %d.\n",
+ 			  get_pg_cnt, all_pg_cnt);
+ 		ret = -ENOBUFS;
+ 		goto err_alloc_page_list;
+ 	}
+ 
+ 	/* write buffer's dma address to BA table */
+ 	ret = hns_roce_mtr_map(hr_dev, mtr, pages, all_pg_cnt);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to map mtr pages, ret = %d.\n", ret);
+ 		goto err_alloc_page_list;
+ 	}
+ 
+ 	/* drop tmp array */
+ 	kvfree(pages);
+ 	return 0;
+ err_alloc_page_list:
+ 	kvfree(pages);
+ err_alloc_hem_list:
+ 	hns_roce_hem_list_release(hr_dev, &mtr->hem_list);
+ err_alloc_bufs:
+ 	mtr_free_bufs(hr_dev, mtr);
+ 	return ret;
+ }
+ 
+ void hns_roce_mtr_destroy(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)
+ {
+ 	/* release multi-hop addressing resource */
+ 	hns_roce_hem_list_release(hr_dev, &mtr->hem_list);
+ 
+ 	/* free buffers */
+ 	mtr_free_bufs(hr_dev, mtr);
+ }
++>>>>>>> d18bb3e15201 (RDMA: Clean MW allocation and free flows)
diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index ac330fd276a1..76d2059096fd 100644
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -2698,6 +2698,7 @@ void ib_set_device_ops(struct ib_device *dev, const struct ib_device_ops *ops)
 	SET_OBJ_SIZE(dev_ops, ib_ah);
 	SET_OBJ_SIZE(dev_ops, ib_counters);
 	SET_OBJ_SIZE(dev_ops, ib_cq);
+	SET_OBJ_SIZE(dev_ops, ib_mw);
 	SET_OBJ_SIZE(dev_ops, ib_pd);
 	SET_OBJ_SIZE(dev_ops, ib_srq);
 	SET_OBJ_SIZE(dev_ops, ib_ucontext);
diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c
index 93ca58eb96f8..56f089e3a952 100644
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -860,7 +860,7 @@ static int ib_uverbs_dereg_mr(struct uverbs_attr_bundle *attrs)
 static int ib_uverbs_alloc_mw(struct uverbs_attr_bundle *attrs)
 {
 	struct ib_uverbs_alloc_mw      cmd;
-	struct ib_uverbs_alloc_mw_resp resp;
+	struct ib_uverbs_alloc_mw_resp resp = {};
 	struct ib_uobject             *uobj;
 	struct ib_pd                  *pd;
 	struct ib_mw                  *mw;
@@ -886,15 +886,21 @@ static int ib_uverbs_alloc_mw(struct uverbs_attr_bundle *attrs)
 		goto err_put;
 	}
 
-	mw = pd->device->ops.alloc_mw(pd, cmd.mw_type, &attrs->driver_udata);
-	if (IS_ERR(mw)) {
-		ret = PTR_ERR(mw);
+	mw = rdma_zalloc_drv_obj(ib_dev, ib_mw);
+	if (!mw) {
+		ret = -ENOMEM;
 		goto err_put;
 	}
 
-	mw->device  = pd->device;
-	mw->pd      = pd;
+	mw->device = ib_dev;
+	mw->pd = pd;
 	mw->uobject = uobj;
+	mw->type = cmd.mw_type;
+
+	ret = pd->device->ops.alloc_mw(mw, &attrs->driver_udata);
+	if (ret)
+		goto err_alloc;
+
 	atomic_inc(&pd->usecnt);
 
 	uobj->object = mw;
@@ -905,6 +911,8 @@ static int ib_uverbs_alloc_mw(struct uverbs_attr_bundle *attrs)
 	resp.mw_handle = uobj->id;
 	return uverbs_response(attrs, &resp, sizeof(resp));
 
+err_alloc:
+	kfree(mw);
 err_put:
 	uobj_put_obj_read(pd);
 err_free:
diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c
index 468e83e0598f..673cc947084b 100644
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -108,8 +108,11 @@ int uverbs_dealloc_mw(struct ib_mw *mw)
 	int ret;
 
 	ret = mw->device->ops.dealloc_mw(mw);
-	if (!ret)
-		atomic_dec(&pd->usecnt);
+	if (ret)
+		return ret;
+
+	atomic_dec(&pd->usecnt);
+	kfree(mw);
 	return ret;
 }
 
diff --git a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
index fe27ebd080de..da8800a88773 100644
--- a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
+++ b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
@@ -1040,8 +1040,7 @@ int c4iw_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		   unsigned int *sg_offset);
 int c4iw_dealloc_mw(struct ib_mw *mw);
 void c4iw_dealloc(struct uld_ctx *ctx);
-struct ib_mw *c4iw_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			    struct ib_udata *udata);
+int c4iw_alloc_mw(struct ib_mw *mw, struct ib_udata *udata);
 struct ib_mr *c4iw_reg_user_mr(struct ib_pd *pd, u64 start,
 					   u64 length, u64 virt, int acc,
 					   struct ib_udata *udata);
diff --git a/drivers/infiniband/hw/cxgb4/mem.c b/drivers/infiniband/hw/cxgb4/mem.c
index f81fb8534c45..2439a33cea8e 100644
--- a/drivers/infiniband/hw/cxgb4/mem.c
+++ b/drivers/infiniband/hw/cxgb4/mem.c
@@ -604,30 +604,23 @@ struct ib_mr *c4iw_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 	return ERR_PTR(err);
 }
 
-struct ib_mw *c4iw_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			    struct ib_udata *udata)
+int c4iw_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
 {
+	struct c4iw_mw *mhp = to_c4iw_mw(ibmw);
 	struct c4iw_dev *rhp;
 	struct c4iw_pd *php;
-	struct c4iw_mw *mhp;
 	u32 mmid;
 	u32 stag = 0;
 	int ret;
 
-	if (type != IB_MW_TYPE_1)
-		return ERR_PTR(-EINVAL);
+	if (ibmw->type != IB_MW_TYPE_1)
+		return -EINVAL;
 
-	php = to_c4iw_pd(pd);
+	php = to_c4iw_pd(ibmw->pd);
 	rhp = php->rhp;
-	mhp = kzalloc(sizeof(*mhp), GFP_KERNEL);
-	if (!mhp)
-		return ERR_PTR(-ENOMEM);
-
 	mhp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);
-	if (!mhp->wr_waitp) {
-		ret = -ENOMEM;
-		goto free_mhp;
-	}
+	if (!mhp->wr_waitp)
+		return -ENOMEM;
 
 	mhp->dereg_skb = alloc_skb(SGE_MAX_WR_LEN, GFP_KERNEL);
 	if (!mhp->dereg_skb) {
@@ -638,18 +631,19 @@ struct ib_mw *c4iw_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 	ret = allocate_window(&rhp->rdev, &stag, php->pdid, mhp->wr_waitp);
 	if (ret)
 		goto free_skb;
+
 	mhp->rhp = rhp;
 	mhp->attr.pdid = php->pdid;
 	mhp->attr.type = FW_RI_STAG_MW;
 	mhp->attr.stag = stag;
 	mmid = (stag) >> 8;
-	mhp->ibmw.rkey = stag;
+	ibmw->rkey = stag;
 	if (xa_insert_irq(&rhp->mrs, mmid, mhp, GFP_KERNEL)) {
 		ret = -ENOMEM;
 		goto dealloc_win;
 	}
 	pr_debug("mmid 0x%x mhp %p stag 0x%x\n", mmid, mhp, stag);
-	return &(mhp->ibmw);
+	return 0;
 
 dealloc_win:
 	deallocate_window(&rhp->rdev, mhp->attr.stag, mhp->dereg_skb,
@@ -658,9 +652,7 @@ struct ib_mw *c4iw_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 	kfree_skb(mhp->dereg_skb);
 free_wr_wait:
 	c4iw_put_wr_wait(mhp->wr_waitp);
-free_mhp:
-	kfree(mhp);
-	return ERR_PTR(ret);
+	return ret;
 }
 
 int c4iw_dealloc_mw(struct ib_mw *mw)
@@ -677,8 +669,6 @@ int c4iw_dealloc_mw(struct ib_mw *mw)
 			  mhp->wr_waitp);
 	kfree_skb(mhp->dereg_skb);
 	c4iw_put_wr_wait(mhp->wr_waitp);
-	pr_debug("ib_mw %p mmid 0x%x ptr %p\n", mw, mmid, mhp);
-	kfree(mhp);
 	return 0;
 }
 
diff --git a/drivers/infiniband/hw/cxgb4/provider.c b/drivers/infiniband/hw/cxgb4/provider.c
index 14acc29b648f..cef182e38bf5 100644
--- a/drivers/infiniband/hw/cxgb4/provider.c
+++ b/drivers/infiniband/hw/cxgb4/provider.c
@@ -498,8 +498,10 @@ static const struct ib_device_ops c4iw_dev_ops = {
 	.query_qp = c4iw_ib_query_qp,
 	.reg_user_mr = c4iw_reg_user_mr,
 	.req_notify_cq = c4iw_arm_cq,
-	INIT_RDMA_OBJ_SIZE(ib_pd, c4iw_pd, ibpd),
+
 	INIT_RDMA_OBJ_SIZE(ib_cq, c4iw_cq, ibcq),
+	INIT_RDMA_OBJ_SIZE(ib_mw, c4iw_mw, ibmw),
+	INIT_RDMA_OBJ_SIZE(ib_pd, c4iw_pd, ibpd),
 	INIT_RDMA_OBJ_SIZE(ib_srq, c4iw_srq, ibsrq),
 	INIT_RDMA_OBJ_SIZE(ib_ucontext, c4iw_ucontext, ibucontext),
 };
* Unmerged path drivers/infiniband/hw/hns/hns_roce_device.h
* Unmerged path drivers/infiniband/hw/hns/hns_roce_main.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_mr.c
diff --git a/drivers/infiniband/hw/mlx4/main.c b/drivers/infiniband/hw/mlx4/main.c
index 3a5e3c55dbf8..41c419928a00 100644
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -2583,6 +2583,8 @@ static const struct ib_device_ops mlx4_ib_dev_wq_ops = {
 static const struct ib_device_ops mlx4_ib_dev_mw_ops = {
 	.alloc_mw = mlx4_ib_alloc_mw,
 	.dealloc_mw = mlx4_ib_dealloc_mw,
+
+	INIT_RDMA_OBJ_SIZE(ib_mw, mlx4_ib_mw, ibmw),
 };
 
 static const struct ib_device_ops mlx4_ib_dev_xrc_ops = {
diff --git a/drivers/infiniband/hw/mlx4/mlx4_ib.h b/drivers/infiniband/hw/mlx4/mlx4_ib.h
index 016cd5044954..16684a06f781 100644
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@ -729,8 +729,7 @@ struct ib_mr *mlx4_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 virt_addr, int access_flags,
 				  struct ib_udata *udata);
 int mlx4_ib_dereg_mr(struct ib_mr *mr, struct ib_udata *udata);
-struct ib_mw *mlx4_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			       struct ib_udata *udata);
+int mlx4_ib_alloc_mw(struct ib_mw *mw, struct ib_udata *udata);
 int mlx4_ib_dealloc_mw(struct ib_mw *mw);
 struct ib_mr *mlx4_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 			       u32 max_num_sg);
diff --git a/drivers/infiniband/hw/mlx4/mr.c b/drivers/infiniband/hw/mlx4/mr.c
index 8fb195bd3582..cc6fc34528b0 100644
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -611,37 +611,27 @@ int mlx4_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
 	return 0;
 }
 
-struct ib_mw *mlx4_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			       struct ib_udata *udata)
+int mlx4_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
 {
-	struct mlx4_ib_dev *dev = to_mdev(pd->device);
-	struct mlx4_ib_mw *mw;
+	struct mlx4_ib_dev *dev = to_mdev(ibmw->device);
+	struct mlx4_ib_mw *mw = to_mmw(ibmw);
 	int err;
 
-	mw = kmalloc(sizeof(*mw), GFP_KERNEL);
-	if (!mw)
-		return ERR_PTR(-ENOMEM);
-
-	err = mlx4_mw_alloc(dev->dev, to_mpd(pd)->pdn,
-			    to_mlx4_type(type), &mw->mmw);
+	err = mlx4_mw_alloc(dev->dev, to_mpd(ibmw->pd)->pdn,
+			    to_mlx4_type(ibmw->type), &mw->mmw);
 	if (err)
-		goto err_free;
+		return err;
 
 	err = mlx4_mw_enable(dev->dev, &mw->mmw);
 	if (err)
 		goto err_mw;
 
-	mw->ibmw.rkey = mw->mmw.key;
-
-	return &mw->ibmw;
+	ibmw->rkey = mw->mmw.key;
+	return 0;
 
 err_mw:
 	mlx4_mw_free(dev->dev, &mw->mmw);
-
-err_free:
-	kfree(mw);
-
-	return ERR_PTR(err);
+	return err;
 }
 
 int mlx4_ib_dealloc_mw(struct ib_mw *ibmw)
@@ -649,8 +639,6 @@ int mlx4_ib_dealloc_mw(struct ib_mw *ibmw)
 	struct mlx4_ib_mw *mw = to_mmw(ibmw);
 
 	mlx4_mw_free(to_mdev(ibmw->device)->dev, &mw->mmw);
-	kfree(mw);
-
 	return 0;
 }
 
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 6bac2d15122c..cd21c4f43a99 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -4103,6 +4103,8 @@ static const struct ib_device_ops mlx5_ib_dev_sriov_ops = {
 static const struct ib_device_ops mlx5_ib_dev_mw_ops = {
 	.alloc_mw = mlx5_ib_alloc_mw,
 	.dealloc_mw = mlx5_ib_dealloc_mw,
+
+	INIT_RDMA_OBJ_SIZE(ib_mw, mlx5_ib_mw, ibmw),
 };
 
 static const struct ib_device_ops mlx5_ib_dev_xrc_ops = {
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5f7962b115a7..d84a0beff66e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1169,8 +1169,7 @@ int mlx5_ib_advise_mr(struct ib_pd *pd,
 		      struct ib_sge *sg_list,
 		      u32 num_sge,
 		      struct uverbs_attr_bundle *attrs);
-struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			       struct ib_udata *udata);
+int mlx5_ib_alloc_mw(struct ib_mw *mw, struct ib_udata *udata);
 int mlx5_ib_dealloc_mw(struct ib_mw *mw);
 int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 		       int page_shift, int flags);
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index c53c4ad7bc7d..57167b4ec0cd 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -1982,12 +1982,11 @@ struct ib_mr *mlx5_ib_alloc_mr_integrity(struct ib_pd *pd,
 				  max_num_meta_sg);
 }
 
-struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
-			       struct ib_udata *udata)
+int mlx5_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
 {
-	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+	struct mlx5_ib_dev *dev = to_mdev(ibmw->device);
 	int inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
-	struct mlx5_ib_mw *mw = NULL;
+	struct mlx5_ib_mw *mw = to_mmw(ibmw);
 	u32 *in = NULL;
 	void *mkc;
 	int ndescs;
@@ -2000,21 +1999,20 @@ struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 
 	err = ib_copy_from_udata(&req, udata, min(udata->inlen, sizeof(req)));
 	if (err)
-		return ERR_PTR(err);
+		return err;
 
 	if (req.comp_mask || req.reserved1 || req.reserved2)
-		return ERR_PTR(-EOPNOTSUPP);
+		return -EOPNOTSUPP;
 
 	if (udata->inlen > sizeof(req) &&
 	    !ib_is_udata_cleared(udata, sizeof(req),
 				 udata->inlen - sizeof(req)))
-		return ERR_PTR(-EOPNOTSUPP);
+		return -EOPNOTSUPP;
 
 	ndescs = req.num_klms ? roundup(req.num_klms, 4) : roundup(1, 4);
 
-	mw = kzalloc(sizeof(*mw), GFP_KERNEL);
 	in = kzalloc(inlen, GFP_KERNEL);
-	if (!mw || !in) {
+	if (!in) {
 		err = -ENOMEM;
 		goto free;
 	}
@@ -2023,11 +2021,11 @@ struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 
 	MLX5_SET(mkc, mkc, free, 1);
 	MLX5_SET(mkc, mkc, translations_octword_size, ndescs);
-	MLX5_SET(mkc, mkc, pd, to_mpd(pd)->pdn);
+	MLX5_SET(mkc, mkc, pd, to_mpd(ibmw->pd)->pdn);
 	MLX5_SET(mkc, mkc, umr_en, 1);
 	MLX5_SET(mkc, mkc, lr, 1);
 	MLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_KLMS);
-	MLX5_SET(mkc, mkc, en_rinval, !!((type == IB_MW_TYPE_2)));
+	MLX5_SET(mkc, mkc, en_rinval, !!((ibmw->type == IB_MW_TYPE_2)));
 	MLX5_SET(mkc, mkc, qpn, 0xffffff);
 
 	err = mlx5_ib_create_mkey(dev, &mw->mmkey, in, inlen);
@@ -2035,17 +2033,15 @@ struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 		goto free;
 
 	mw->mmkey.type = MLX5_MKEY_MW;
-	mw->ibmw.rkey = mw->mmkey.key;
+	ibmw->rkey = mw->mmkey.key;
 	mw->ndescs = ndescs;
 
 	resp.response_length = min(offsetof(typeof(resp), response_length) +
 				   sizeof(resp.response_length), udata->outlen);
 	if (resp.response_length) {
 		err = ib_copy_to_udata(udata, &resp, resp.response_length);
-		if (err) {
-			mlx5_core_destroy_mkey(dev->mdev, &mw->mmkey);
-			goto free;
-		}
+		if (err)
+			goto free_mkey;
 	}
 
 	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
@@ -2057,21 +2053,19 @@ struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 	}
 
 	kfree(in);
-	return &mw->ibmw;
+	return 0;
 
 free_mkey:
 	mlx5_core_destroy_mkey(dev->mdev, &mw->mmkey);
 free:
-	kfree(mw);
 	kfree(in);
-	return ERR_PTR(err);
+	return err;
 }
 
 int mlx5_ib_dealloc_mw(struct ib_mw *mw)
 {
 	struct mlx5_ib_dev *dev = to_mdev(mw->device);
 	struct mlx5_ib_mw *mmw = to_mmw(mw);
-	int err;
 
 	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
 		xa_erase(&dev->odp_mkeys, mlx5_base_mkey(mmw->mmkey.key));
@@ -2082,11 +2076,7 @@ int mlx5_ib_dealloc_mw(struct ib_mw *mw)
 		synchronize_srcu(&dev->odp_srcu);
 	}
 
-	err = mlx5_core_destroy_mkey(dev->mdev, &mmw->mmkey);
-	if (err)
-		return err;
-	kfree(mmw);
-	return 0;
+	return mlx5_core_destroy_mkey(dev->mdev, &mmw->mmkey);
 }
 
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 112cae0273d1..6a7963fc210e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2449,8 +2449,7 @@ struct ib_device_ops {
 			 unsigned int *sg_offset);
 	int (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
 			       struct ib_mr_status *mr_status);
-	struct ib_mw *(*alloc_mw)(struct ib_pd *pd, enum ib_mw_type type,
-				  struct ib_udata *udata);
+	int (*alloc_mw)(struct ib_mw *mw, struct ib_udata *udata);
 	int (*dealloc_mw)(struct ib_mw *mw);
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
@@ -2613,6 +2612,7 @@ struct ib_device_ops {
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_counters);
 	DECLARE_RDMA_OBJ_SIZE(ib_cq);
+	DECLARE_RDMA_OBJ_SIZE(ib_mw);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 	DECLARE_RDMA_OBJ_SIZE(ib_srq);
 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
