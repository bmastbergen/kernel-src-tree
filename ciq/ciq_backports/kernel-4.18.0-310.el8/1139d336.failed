mm/hugetlb.c: fix pages per hugetlb calculation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit 1139d336fff425f9a20374945cdd28eb44d09fa8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/1139d336.failed

The routine hpage_nr_pages() was incorrectly used to calculate the number
of base pages in a hugetlb page.  hpage_nr_pages is designed to be called
for THP pages and will return HPAGE_PMD_NR for hugetlb pages of any size.

Due to the context in which hpage_nr_pages was called, it is unlikely to
produce a user visible error.  The routine with the incorrect call is only
exercised in the case of hugetlb memory error or migration.  In addition,
this would need to be on an architecture which supports huge page sizes
less than PMD_SIZE.  And, the vma containing the huge page would also need
to smaller than PMD_SIZE.

Fixes: c0d0381ade79 ("hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization")
	Reported-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: <stable@vger.kernel.org>
Link: http://lkml.kernel.org/r/20200629185003.97202-1-mike.kravetz@oracle.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1139d336fff425f9a20374945cdd28eb44d09fa8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 5899c1f83920,fab4485b9e52..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1404,6 -1563,106 +1404,109 @@@ int PageHeadHuge(struct page *page_head
  	return page_head[1].compound_dtor == HUGETLB_PAGE_DTOR;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Find address_space associated with hugetlbfs page.
+  * Upon entry page is locked and page 'was' mapped although mapped state
+  * could change.  If necessary, use anon_vma to find vma and associated
+  * address space.  The returned mapping may be stale, but it can not be
+  * invalid as page lock (which is held) is required to destroy mapping.
+  */
+ static struct address_space *_get_hugetlb_page_mapping(struct page *hpage)
+ {
+ 	struct anon_vma *anon_vma;
+ 	pgoff_t pgoff_start, pgoff_end;
+ 	struct anon_vma_chain *avc;
+ 	struct address_space *mapping = page_mapping(hpage);
+ 
+ 	/* Simple file based mapping */
+ 	if (mapping)
+ 		return mapping;
+ 
+ 	/*
+ 	 * Even anonymous hugetlbfs mappings are associated with an
+ 	 * underlying hugetlbfs file (see hugetlb_file_setup in mmap
+ 	 * code).  Find a vma associated with the anonymous vma, and
+ 	 * use the file pointer to get address_space.
+ 	 */
+ 	anon_vma = page_lock_anon_vma_read(hpage);
+ 	if (!anon_vma)
+ 		return mapping;  /* NULL */
+ 
+ 	/* Use first found vma */
+ 	pgoff_start = page_to_pgoff(hpage);
+ 	pgoff_end = pgoff_start + pages_per_huge_page(page_hstate(hpage)) - 1;
+ 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root,
+ 					pgoff_start, pgoff_end) {
+ 		struct vm_area_struct *vma = avc->vma;
+ 
+ 		mapping = vma->vm_file->f_mapping;
+ 		break;
+ 	}
+ 
+ 	anon_vma_unlock_read(anon_vma);
+ 	return mapping;
+ }
+ 
+ /*
+  * Find and lock address space (mapping) in write mode.
+  *
+  * Upon entry, the page is locked which allows us to find the mapping
+  * even in the case of an anon page.  However, locking order dictates
+  * the i_mmap_rwsem be acquired BEFORE the page lock.  This is hugetlbfs
+  * specific.  So, we first try to lock the sema while still holding the
+  * page lock.  If this works, great!  If not, then we need to drop the
+  * page lock and then acquire i_mmap_rwsem and reacquire page lock.  Of
+  * course, need to revalidate state along the way.
+  */
+ struct address_space *hugetlb_page_mapping_lock_write(struct page *hpage)
+ {
+ 	struct address_space *mapping, *mapping2;
+ 
+ 	mapping = _get_hugetlb_page_mapping(hpage);
+ retry:
+ 	if (!mapping)
+ 		return mapping;
+ 
+ 	/*
+ 	 * If no contention, take lock and return
+ 	 */
+ 	if (i_mmap_trylock_write(mapping))
+ 		return mapping;
+ 
+ 	/*
+ 	 * Must drop page lock and wait on mapping sema.
+ 	 * Note:  Once page lock is dropped, mapping could become invalid.
+ 	 * As a hack, increase map count until we lock page again.
+ 	 */
+ 	atomic_inc(&hpage->_mapcount);
+ 	unlock_page(hpage);
+ 	i_mmap_lock_write(mapping);
+ 	lock_page(hpage);
+ 	atomic_add_negative(-1, &hpage->_mapcount);
+ 
+ 	/* verify page is still mapped */
+ 	if (!page_mapped(hpage)) {
+ 		i_mmap_unlock_write(mapping);
+ 		return NULL;
+ 	}
+ 
+ 	/*
+ 	 * Get address space again and verify it is the same one
+ 	 * we locked.  If not, drop lock and retry.
+ 	 */
+ 	mapping2 = _get_hugetlb_page_mapping(hpage);
+ 	if (mapping2 != mapping) {
+ 		i_mmap_unlock_write(mapping);
+ 		mapping = mapping2;
+ 		goto retry;
+ 	}
+ 
+ 	return mapping;
+ }
+ 
++>>>>>>> 1139d336fff4 (mm/hugetlb.c: fix pages per hugetlb calculation)
  pgoff_t __basepage_index(struct page *page)
  {
  	struct page *page_head = compound_head(page);
* Unmerged path mm/hugetlb.c
