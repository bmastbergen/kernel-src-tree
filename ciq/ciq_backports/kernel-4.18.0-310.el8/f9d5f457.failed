nvme: freeze the queue over ->lba_shift updates

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Christoph Hellwig <hch@lst.de>
commit f9d5f4579feafa721dba2f350fc064a1852c6f8c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/f9d5f457.failed

Ensure that there can't be any I/O in flight went we change the disk
geometry in nvme_update_ns_info, most notable the LBA size by lifting
the queue free from nvme_update_disk_info into the caller

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Keith Busch <kbusch@kernel.org>
	Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
(cherry picked from commit f9d5f4579feafa721dba2f350fc064a1852c6f8c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/core.c
diff --cc drivers/nvme/host/core.c
index 7c2cc9e85e5d,82cd03c0ba21..000000000000
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -1969,15 -2017,11 +1969,15 @@@ static void nvme_update_disk_info(struc
  	unsigned short bs = 1 << ns->lba_shift;
  	u32 atomic_bs, phys_bs, io_opt = 0;
  
 +	/*
 +	 * The block layer can't support LBA sizes larger than the page size
 +	 * yet, so catch this early and don't allow block I/O.
 +	 */
  	if (ns->lba_shift > PAGE_SHIFT) {
 -		/* unsupported block size, set capacity to 0 later */
 +		capacity = 0;
  		bs = (1 << 9);
  	}
- 	blk_mq_freeze_queue(disk->queue);
+ 
  	blk_integrity_unregister(disk);
  
  	atomic_bs = phys_bs = bs;
@@@ -2076,11 -2125,13 +2074,12 @@@ static void nvme_set_chunk_sectors(stru
  	blk_queue_chunk_sectors(ns->queue, iob);
  }
  
 -static int nvme_update_ns_info(struct nvme_ns *ns, struct nvme_id_ns *id)
 +static int __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
  {
 -	unsigned lbaf = id->flbas & NVME_NS_FLBAS_LBA_MASK;
 +	struct nvme_ns *ns = disk->private_data;
  	struct nvme_ctrl *ctrl = ns->ctrl;
 -	int ret;
  
+ 	blk_mq_freeze_queue(ns->disk->queue);
  	/*
  	 * If identify namespace failed, use default 512 byte block size so
  	 * block layer can use before failing read/write for 0 capacity.
@@@ -2092,58 -2143,49 +2091,94 @@@
  	switch (ns->head->ids.csi) {
  	case NVME_CSI_NVM:
  		break;
++<<<<<<< HEAD
++=======
+ 	case NVME_CSI_ZNS:
+ 		ret = nvme_update_zone_info(ns, lbaf);
+ 		if (ret) {
+ 			dev_warn(ctrl->device,
+ 				"failed to add zoned namespace:%u ret:%d\n",
+ 				ns->head->ns_id, ret);
+ 			goto out_unfreeze;
+ 		}
+ 		break;
++>>>>>>> f9d5f4579fea (nvme: freeze the queue over ->lba_shift updates)
  	default:
 -		dev_warn(ctrl->device, "unknown csi:%u ns:%u\n",
 +		dev_warn(ctrl->device, "unknown csi:%d ns:%d\n",
  			ns->head->ids.csi, ns->head->ns_id);
- 		return -ENODEV;
+ 		ret = -ENODEV;
+ 		goto out_unfreeze;
  	}
  
++<<<<<<< HEAD
 +	ns->features = 0;
 +	ns->ms = le16_to_cpu(id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ms);
 +	/* the PI implementation requires metadata equal t10 pi tuple size */
 +	if (ns->ms == sizeof(struct t10_pi_tuple))
 +		ns->pi_type = id->dps & NVME_NS_DPS_PI_MASK;
 +	else
 +		ns->pi_type = 0;
 +
 +	if (ns->ms) {
 +		/*
 +		 * For PCIe only the separate metadata pointer is supported,
 +		 * as the block layer supplies metadata in a separate bio_vec
 +		 * chain. For Fabrics, only metadata as part of extended data
 +		 * LBA is supported on the wire per the Fabrics specification,
 +		 * but the HBA/HCA will do the remapping from the separate
 +		 * metadata buffers for us.
 +		 */
 +		if (id->flbas & NVME_NS_FLBAS_META_EXT) {
 +			ns->features |= NVME_NS_EXT_LBAS;
 +			if ((ctrl->ops->flags & NVME_F_FABRICS) &&
 +			    (ctrl->ops->flags & NVME_F_METADATA_SUPPORTED) &&
 +			    ctrl->max_integrity_segments)
 +				ns->features |= NVME_NS_METADATA_SUPPORTED;
 +		} else {
 +			if (WARN_ON_ONCE(ctrl->ops->flags & NVME_F_FABRICS))
 +				return -EINVAL;
 +			if (ctrl->ops->flags & NVME_F_METADATA_SUPPORTED)
 +				ns->features |= NVME_NS_METADATA_SUPPORTED;
 +		}
 +	}
 +
 +	nvme_set_chunk_sectors(ns, id);
 +	nvme_update_disk_info(disk, ns, id);
++=======
+ 	ret = nvme_configure_metadata(ns, id);
+ 	if (ret)
+ 		goto out_unfreeze;
+ 	nvme_set_chunk_sectors(ns, id);
+ 	nvme_update_disk_info(ns->disk, ns, id);
+ 	blk_mq_unfreeze_queue(ns->disk->queue);
+ 
++>>>>>>> f9d5f4579fea (nvme: freeze the queue over ->lba_shift updates)
  #ifdef CONFIG_NVME_MULTIPATH
  	if (ns->head->disk) {
+ 		blk_mq_freeze_queue(ns->head->disk->queue);
  		nvme_update_disk_info(ns->head->disk, ns, id);
++<<<<<<< HEAD
 +		blk_queue_stack_limits(ns->head->disk->queue, ns->queue);
 +		nvme_mpath_update_disk_size(ns->head->disk);
++=======
+ 		blk_stack_limits(&ns->head->disk->queue->limits,
+ 				 &ns->queue->limits, 0);
+ 		blk_queue_update_readahead(ns->head->disk->queue);
+ 		nvme_update_bdev_size(ns->head->disk);
+ 		blk_mq_unfreeze_queue(ns->head->disk->queue);
++>>>>>>> f9d5f4579fea (nvme: freeze the queue over ->lba_shift updates)
  	}
  #endif
  	return 0;
+ 
+ out_unfreeze:
+ 	blk_mq_unfreeze_queue(ns->disk->queue);
+ 	return ret;
  }
  
 -static int nvme_validate_ns(struct nvme_ns *ns)
 +static int nvme_revalidate_disk(struct gendisk *disk)
  {
 +	struct nvme_ns *ns = disk->private_data;
  	struct nvme_ctrl *ctrl = ns->ctrl;
  	struct nvme_id_ns *id;
  	struct nvme_ns_ids ids;
* Unmerged path drivers/nvme/host/core.c
