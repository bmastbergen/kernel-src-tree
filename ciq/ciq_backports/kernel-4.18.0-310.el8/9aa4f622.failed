KVM: vmx/pmu: Release guest LBR event via lazy release mechanism

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Like Xu <like.xu@linux.intel.com>
commit 9aa4f622460f9287e57804dbeb219bfef29f04a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/9aa4f622.failed

The vPMU uses GUEST_LBR_IN_USE_IDX (bit 58) in 'pmu->pmc_in_use' to
indicate whether a guest LBR event is still needed by the vcpu. If the
vcpu no longer accesses LBR related registers within a scheduling time
slice, and the enable bit of LBR has been unset, vPMU will treat the
guest LBR event as a bland event of a vPMC counter and release it
as usual. Also, the pass-through state of LBR records msrs is cancelled.

	Signed-off-by: Like Xu <like.xu@linux.intel.com>
Message-Id: <20210201051039.255478-10-like.xu@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 9aa4f622460f9287e57804dbeb219bfef29f04a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/pmu.h
#	arch/x86/kvm/vmx/pmu_intel.c
diff --cc arch/x86/kvm/pmu.h
index 067fef51760c,7b30bc967af3..000000000000
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@@ -39,6 -39,8 +39,11 @@@ struct kvm_pmu_ops 
  	void (*refresh)(struct kvm_vcpu *vcpu);
  	void (*init)(struct kvm_vcpu *vcpu);
  	void (*reset)(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
++=======
+ 	void (*deliver_pmi)(struct kvm_vcpu *vcpu);
+ 	void (*cleanup)(struct kvm_vcpu *vcpu);
++>>>>>>> 9aa4f622460f (KVM: vmx/pmu: Release guest LBR event via lazy release mechanism)
  };
  
  static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
diff --cc arch/x86/kvm/vmx/pmu_intel.c
index 48690db4db6a,d1df618cb7de..000000000000
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@@ -277,6 -306,48 +280,51 @@@ int intel_pmu_create_guest_lbr_event(st
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * It's safe to access LBR msrs from guest when they have not
+  * been passthrough since the host would help restore or reset
+  * the LBR msrs records when the guest LBR event is scheduled in.
+  */
+ static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
+ 				     struct msr_data *msr_info, bool read)
+ {
+ 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
+ 	u32 index = msr_info->index;
+ 
+ 	if (!intel_pmu_is_valid_lbr_msr(vcpu, index))
+ 		return false;
+ 
+ 	if (!lbr_desc->event && !intel_pmu_create_guest_lbr_event(vcpu))
+ 		goto dummy;
+ 
+ 	/*
+ 	 * Disable irq to ensure the LBR feature doesn't get reclaimed by the
+ 	 * host at the time the value is read from the msr, and this avoids the
+ 	 * host LBR value to be leaked to the guest. If LBR has been reclaimed,
+ 	 * return 0 on guest reads.
+ 	 */
+ 	local_irq_disable();
+ 	if (lbr_desc->event->state == PERF_EVENT_STATE_ACTIVE) {
+ 		if (read)
+ 			rdmsrl(index, msr_info->data);
+ 		else
+ 			wrmsrl(index, msr_info->data);
+ 		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+ 		local_irq_enable();
+ 		return true;
+ 	}
+ 	clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+ 	local_irq_enable();
+ 
+ dummy:
+ 	if (read)
+ 		msr_info->data = 0;
+ 	return true;
+ }
+ 
++>>>>>>> 9aa4f622460f (KVM: vmx/pmu: Release guest LBR event via lazy release mechanism)
  static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  {
  	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@@ -517,6 -594,117 +568,120 @@@ static void intel_pmu_reset(struct kvm_
  	intel_pmu_release_guest_lbr_event(vcpu);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Emulate LBR_On_PMI behavior for 1 < pmu.version < 4.
+  *
+  * If Freeze_LBR_On_PMI = 1, the LBR is frozen on PMI and
+  * the KVM emulates to clear the LBR bit (bit 0) in IA32_DEBUGCTL.
+  *
+  * Guest needs to re-enable LBR to resume branches recording.
+  */
+ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
+ {
+ 	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);
+ 
+ 	if (data & DEBUGCTLMSR_FREEZE_LBRS_ON_PMI) {
+ 		data &= ~DEBUGCTLMSR_LBR;
+ 		vmcs_write64(GUEST_IA32_DEBUGCTL, data);
+ 	}
+ }
+ 
+ static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
+ {
+ 	u8 version = vcpu_to_pmu(vcpu)->version;
+ 
+ 	if (!intel_pmu_lbr_is_enabled(vcpu))
+ 		return;
+ 
+ 	if (version > 1 && version < 4)
+ 		intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);
+ }
+ 
+ static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
+ {
+ 	struct x86_pmu_lbr *lbr = vcpu_to_lbr_records(vcpu);
+ 	int i;
+ 
+ 	for (i = 0; i < lbr->nr; i++) {
+ 		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ 		vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ 		if (lbr->info)
+ 			vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ 	}
+ 
+ 	vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ 	vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ }
+ 
+ static inline void vmx_disable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)
+ {
+ 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
+ 
+ 	if (!lbr_desc->msr_passthrough)
+ 		return;
+ 
+ 	vmx_update_intercept_for_lbr_msrs(vcpu, true);
+ 	lbr_desc->msr_passthrough = false;
+ }
+ 
+ static inline void vmx_enable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)
+ {
+ 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
+ 
+ 	if (lbr_desc->msr_passthrough)
+ 		return;
+ 
+ 	vmx_update_intercept_for_lbr_msrs(vcpu, false);
+ 	lbr_desc->msr_passthrough = true;
+ }
+ 
+ /*
+  * Higher priority host perf events (e.g. cpu pinned) could reclaim the
+  * pmu resources (e.g. LBR) that were assigned to the guest. This is
+  * usually done via ipi calls (more details in perf_install_in_context).
+  *
+  * Before entering the non-root mode (with irq disabled here), double
+  * confirm that the pmu features enabled to the guest are not reclaimed
+  * by higher priority host events. Otherwise, disallow vcpu's access to
+  * the reclaimed features.
+  */
+ void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
+ 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
+ 
+ 	if (!lbr_desc->event) {
+ 		vmx_disable_lbr_msrs_passthrough(vcpu);
+ 		if (vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR)
+ 			goto warn;
+ 		if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+ 			goto warn;
+ 		return;
+ 	}
+ 
+ 	if (lbr_desc->event->state < PERF_EVENT_STATE_ACTIVE) {
+ 		vmx_disable_lbr_msrs_passthrough(vcpu);
+ 		__clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+ 		goto warn;
+ 	} else
+ 		vmx_enable_lbr_msrs_passthrough(vcpu);
+ 
+ 	return;
+ 
+ warn:
+ 	pr_warn_ratelimited("kvm: vcpu-%d: fail to passthrough LBR.\n",
+ 		vcpu->vcpu_id);
+ }
+ 
+ static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
+ {
+ 	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))
+ 		intel_pmu_release_guest_lbr_event(vcpu);
+ }
+ 
++>>>>>>> 9aa4f622460f (KVM: vmx/pmu: Release guest LBR event via lazy release mechanism)
  struct kvm_pmu_ops intel_pmu_ops = {
  	.find_arch_event = intel_find_arch_event,
  	.find_fixed_event = intel_find_fixed_event,
@@@ -531,4 -719,6 +696,9 @@@
  	.refresh = intel_pmu_refresh,
  	.init = intel_pmu_init,
  	.reset = intel_pmu_reset,
++<<<<<<< HEAD
++=======
+ 	.deliver_pmi = intel_pmu_deliver_pmi,
+ 	.cleanup = intel_pmu_cleanup,
++>>>>>>> 9aa4f622460f (KVM: vmx/pmu: Release guest LBR event via lazy release mechanism)
  };
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 88b8f072cbd9..27d0aa9bad8f 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -473,6 +473,9 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 			pmc_stop_counter(pmc);
 	}
 
+	if (kvm_x86_ops.pmu_ops->cleanup)
+		kvm_x86_ops.pmu_ops->cleanup(vcpu);
+
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
* Unmerged path arch/x86/kvm/pmu.h
* Unmerged path arch/x86/kvm/vmx/pmu_intel.c
