tick/nohz: Kick only _queued_ task whose tick dependency is updated

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Marcelo Tosatti <mtosatti@redhat.com>
commit a1dfb6311c7739e21e160bc4c5575a1b21b48c87
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/a1dfb631.failed

When the tick dependency of a task is updated, we want it to aknowledge
the new state and restart the tick if needed. If the task is not
running, we don't need to kick it because it will observe the new
dependency upon scheduling in. But if the task is running, we may need
to send an IPI to it so that it gets notified.

Unfortunately we don't have the means to check if a task is running
in a race free way. Checking p->on_cpu in a synchronized way against
p->tick_dep_mask would imply adding a full barrier between
prepare_task_switch() and tick_nohz_task_switch(), which we want to
avoid in this fast-path.

Therefore we blindly fire an IPI to the task's CPU.

Meanwhile we can check if the task is queued on the CPU rq because
p->on_rq is always set to TASK_ON_RQ_QUEUED _before_ schedule() and its
full barrier that precedes tick_nohz_task_switch(). And if the task is
queued on a nohz_full CPU, it also has fair chances to be running as the
isolation constraints prescribe running single tasks on full dynticks
CPUs.

So use this as a trick to check if we can spare an IPI toward a
non-running task.

NOTE: For the ordering to be correct, it is assumed that we never
deactivate a task while it is running, the only exception being the task
deactivating itself while scheduling out.

	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
	Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
Link: https://lore.kernel.org/r/20210512232924.150322-9-frederic@kernel.org
(cherry picked from commit a1dfb6311c7739e21e160bc4c5575a1b21b48c87)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/time/tick-sched.c
diff --cc kernel/sched/core.c
index ca684ba3673f,78e480f7881a..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -872,6 -871,730 +872,733 @@@ static void set_load_weight(struct task
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_UCLAMP_TASK
+ /*
+  * Serializes updates of utilization clamp values
+  *
+  * The (slow-path) user-space triggers utilization clamp value updates which
+  * can require updates on (fast-path) scheduler's data structures used to
+  * support enqueue/dequeue operations.
+  * While the per-CPU rq lock protects fast-path update operations, user-space
+  * requests are serialized using a mutex to reduce the risk of conflicting
+  * updates or API abuses.
+  */
+ static DEFINE_MUTEX(uclamp_mutex);
+ 
+ /* Max allowed minimum utilization */
+ unsigned int sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
+ 
+ /* Max allowed maximum utilization */
+ unsigned int sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
+ 
+ /*
+  * By default RT tasks run at the maximum performance point/capacity of the
+  * system. Uclamp enforces this by always setting UCLAMP_MIN of RT tasks to
+  * SCHED_CAPACITY_SCALE.
+  *
+  * This knob allows admins to change the default behavior when uclamp is being
+  * used. In battery powered devices, particularly, running at the maximum
+  * capacity and frequency will increase energy consumption and shorten the
+  * battery life.
+  *
+  * This knob only affects RT tasks that their uclamp_se->user_defined == false.
+  *
+  * This knob will not override the system default sched_util_clamp_min defined
+  * above.
+  */
+ unsigned int sysctl_sched_uclamp_util_min_rt_default = SCHED_CAPACITY_SCALE;
+ 
+ /* All clamps are required to be less or equal than these values */
+ static struct uclamp_se uclamp_default[UCLAMP_CNT];
+ 
+ /*
+  * This static key is used to reduce the uclamp overhead in the fast path. It
+  * primarily disables the call to uclamp_rq_{inc, dec}() in
+  * enqueue/dequeue_task().
+  *
+  * This allows users to continue to enable uclamp in their kernel config with
+  * minimum uclamp overhead in the fast path.
+  *
+  * As soon as userspace modifies any of the uclamp knobs, the static key is
+  * enabled, since we have an actual users that make use of uclamp
+  * functionality.
+  *
+  * The knobs that would enable this static key are:
+  *
+  *   * A task modifying its uclamp value with sched_setattr().
+  *   * An admin modifying the sysctl_sched_uclamp_{min, max} via procfs.
+  *   * An admin modifying the cgroup cpu.uclamp.{min, max}
+  */
+ DEFINE_STATIC_KEY_FALSE(sched_uclamp_used);
+ 
+ /* Integer rounded range for each bucket */
+ #define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
+ 
+ #define for_each_clamp_id(clamp_id) \
+ 	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
+ 
+ static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
+ {
+ 	return min_t(unsigned int, clamp_value / UCLAMP_BUCKET_DELTA, UCLAMP_BUCKETS - 1);
+ }
+ 
+ static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
+ {
+ 	if (clamp_id == UCLAMP_MIN)
+ 		return 0;
+ 	return SCHED_CAPACITY_SCALE;
+ }
+ 
+ static inline void uclamp_se_set(struct uclamp_se *uc_se,
+ 				 unsigned int value, bool user_defined)
+ {
+ 	uc_se->value = value;
+ 	uc_se->bucket_id = uclamp_bucket_id(value);
+ 	uc_se->user_defined = user_defined;
+ }
+ 
+ static inline unsigned int
+ uclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,
+ 		  unsigned int clamp_value)
+ {
+ 	/*
+ 	 * Avoid blocked utilization pushing up the frequency when we go
+ 	 * idle (which drops the max-clamp) by retaining the last known
+ 	 * max-clamp.
+ 	 */
+ 	if (clamp_id == UCLAMP_MAX) {
+ 		rq->uclamp_flags |= UCLAMP_FLAG_IDLE;
+ 		return clamp_value;
+ 	}
+ 
+ 	return uclamp_none(UCLAMP_MIN);
+ }
+ 
+ static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
+ 				     unsigned int clamp_value)
+ {
+ 	/* Reset max-clamp retention only on idle exit */
+ 	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
+ 		return;
+ 
+ 	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
+ }
+ 
+ static inline
+ unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
+ 				   unsigned int clamp_value)
+ {
+ 	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
+ 	int bucket_id = UCLAMP_BUCKETS - 1;
+ 
+ 	/*
+ 	 * Since both min and max clamps are max aggregated, find the
+ 	 * top most bucket with tasks in.
+ 	 */
+ 	for ( ; bucket_id >= 0; bucket_id--) {
+ 		if (!bucket[bucket_id].tasks)
+ 			continue;
+ 		return bucket[bucket_id].value;
+ 	}
+ 
+ 	/* No tasks -- default clamp values */
+ 	return uclamp_idle_value(rq, clamp_id, clamp_value);
+ }
+ 
+ static void __uclamp_update_util_min_rt_default(struct task_struct *p)
+ {
+ 	unsigned int default_util_min;
+ 	struct uclamp_se *uc_se;
+ 
+ 	lockdep_assert_held(&p->pi_lock);
+ 
+ 	uc_se = &p->uclamp_req[UCLAMP_MIN];
+ 
+ 	/* Only sync if user didn't override the default */
+ 	if (uc_se->user_defined)
+ 		return;
+ 
+ 	default_util_min = sysctl_sched_uclamp_util_min_rt_default;
+ 	uclamp_se_set(uc_se, default_util_min, false);
+ }
+ 
+ static void uclamp_update_util_min_rt_default(struct task_struct *p)
+ {
+ 	struct rq_flags rf;
+ 	struct rq *rq;
+ 
+ 	if (!rt_task(p))
+ 		return;
+ 
+ 	/* Protect updates to p->uclamp_* */
+ 	rq = task_rq_lock(p, &rf);
+ 	__uclamp_update_util_min_rt_default(p);
+ 	task_rq_unlock(rq, p, &rf);
+ }
+ 
+ static void uclamp_sync_util_min_rt_default(void)
+ {
+ 	struct task_struct *g, *p;
+ 
+ 	/*
+ 	 * copy_process()			sysctl_uclamp
+ 	 *					  uclamp_min_rt = X;
+ 	 *   write_lock(&tasklist_lock)		  read_lock(&tasklist_lock)
+ 	 *   // link thread			  smp_mb__after_spinlock()
+ 	 *   write_unlock(&tasklist_lock)	  read_unlock(&tasklist_lock);
+ 	 *   sched_post_fork()			  for_each_process_thread()
+ 	 *     __uclamp_sync_rt()		    __uclamp_sync_rt()
+ 	 *
+ 	 * Ensures that either sched_post_fork() will observe the new
+ 	 * uclamp_min_rt or for_each_process_thread() will observe the new
+ 	 * task.
+ 	 */
+ 	read_lock(&tasklist_lock);
+ 	smp_mb__after_spinlock();
+ 	read_unlock(&tasklist_lock);
+ 
+ 	rcu_read_lock();
+ 	for_each_process_thread(g, p)
+ 		uclamp_update_util_min_rt_default(p);
+ 	rcu_read_unlock();
+ }
+ 
+ static inline struct uclamp_se
+ uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ 	struct uclamp_se uc_max;
+ 
+ 	/*
+ 	 * Tasks in autogroups or root task group will be
+ 	 * restricted by system defaults.
+ 	 */
+ 	if (task_group_is_autogroup(task_group(p)))
+ 		return uc_req;
+ 	if (task_group(p) == &root_task_group)
+ 		return uc_req;
+ 
+ 	uc_max = task_group(p)->uclamp[clamp_id];
+ 	if (uc_req.value > uc_max.value || !uc_req.user_defined)
+ 		return uc_max;
+ #endif
+ 
+ 	return uc_req;
+ }
+ 
+ /*
+  * The effective clamp bucket index of a task depends on, by increasing
+  * priority:
+  * - the task specific clamp value, when explicitly requested from userspace
+  * - the task group effective clamp value, for tasks not either in the root
+  *   group or in an autogroup
+  * - the system default clamp value, defined by the sysadmin
+  */
+ static inline struct uclamp_se
+ uclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);
+ 	struct uclamp_se uc_max = uclamp_default[clamp_id];
+ 
+ 	/* System default restrictions always apply */
+ 	if (unlikely(uc_req.value > uc_max.value))
+ 		return uc_max;
+ 
+ 	return uc_req;
+ }
+ 
+ unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_eff;
+ 
+ 	/* Task currently refcounted: use back-annotated (effective) value */
+ 	if (p->uclamp[clamp_id].active)
+ 		return (unsigned long)p->uclamp[clamp_id].value;
+ 
+ 	uc_eff = uclamp_eff_get(p, clamp_id);
+ 
+ 	return (unsigned long)uc_eff.value;
+ }
+ 
+ /*
+  * When a task is enqueued on a rq, the clamp bucket currently defined by the
+  * task's uclamp::bucket_id is refcounted on that rq. This also immediately
+  * updates the rq's clamp value if required.
+  *
+  * Tasks can have a task-specific value requested from user-space, track
+  * within each bucket the maximum value for tasks refcounted in it.
+  * This "local max aggregation" allows to track the exact "requested" value
+  * for each bucket when all its RUNNABLE tasks require the same clamp.
+  */
+ static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
+ 				    enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	/* Update task effective clamp */
+ 	p->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 	bucket->tasks++;
+ 	uc_se->active = true;
+ 
+ 	uclamp_idle_reset(rq, clamp_id, uc_se->value);
+ 
+ 	/*
+ 	 * Local max aggregation: rq buckets always track the max
+ 	 * "requested" clamp value of its RUNNABLE tasks.
+ 	 */
+ 	if (bucket->tasks == 1 || uc_se->value > bucket->value)
+ 		bucket->value = uc_se->value;
+ 
+ 	if (uc_se->value > READ_ONCE(uc_rq->value))
+ 		WRITE_ONCE(uc_rq->value, uc_se->value);
+ }
+ 
+ /*
+  * When a task is dequeued from a rq, the clamp bucket refcounted by the task
+  * is released. If this is the last task reference counting the rq's max
+  * active clamp value, then the rq's clamp value is updated.
+  *
+  * Both refcounted tasks and rq's cached clamp values are expected to be
+  * always valid. If it's detected they are not, as defensive programming,
+  * enforce the expected state and warn.
+  */
+ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
+ 				    enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 	unsigned int bkt_clamp;
+ 	unsigned int rq_clamp;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	/*
+ 	 * If sched_uclamp_used was enabled after task @p was enqueued,
+ 	 * we could end up with unbalanced call to uclamp_rq_dec_id().
+ 	 *
+ 	 * In this case the uc_se->active flag should be false since no uclamp
+ 	 * accounting was performed at enqueue time and we can just return
+ 	 * here.
+ 	 *
+ 	 * Need to be careful of the following enqueue/dequeue ordering
+ 	 * problem too
+ 	 *
+ 	 *	enqueue(taskA)
+ 	 *	// sched_uclamp_used gets enabled
+ 	 *	enqueue(taskB)
+ 	 *	dequeue(taskA)
+ 	 *	// Must not decrement bucket->tasks here
+ 	 *	dequeue(taskB)
+ 	 *
+ 	 * where we could end up with stale data in uc_se and
+ 	 * bucket[uc_se->bucket_id].
+ 	 *
+ 	 * The following check here eliminates the possibility of such race.
+ 	 */
+ 	if (unlikely(!uc_se->active))
+ 		return;
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 
+ 	SCHED_WARN_ON(!bucket->tasks);
+ 	if (likely(bucket->tasks))
+ 		bucket->tasks--;
+ 
+ 	uc_se->active = false;
+ 
+ 	/*
+ 	 * Keep "local max aggregation" simple and accept to (possibly)
+ 	 * overboost some RUNNABLE tasks in the same bucket.
+ 	 * The rq clamp bucket value is reset to its base value whenever
+ 	 * there are no more RUNNABLE tasks refcounting it.
+ 	 */
+ 	if (likely(bucket->tasks))
+ 		return;
+ 
+ 	rq_clamp = READ_ONCE(uc_rq->value);
+ 	/*
+ 	 * Defensive programming: this should never happen. If it happens,
+ 	 * e.g. due to future modification, warn and fixup the expected value.
+ 	 */
+ 	SCHED_WARN_ON(bucket->value > rq_clamp);
+ 	if (bucket->value >= rq_clamp) {
+ 		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
+ 		WRITE_ONCE(uc_rq->value, bkt_clamp);
+ 	}
+ }
+ 
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * Avoid any overhead until uclamp is actually used by the userspace.
+ 	 *
+ 	 * The condition is constructed such that a NOP is generated when
+ 	 * sched_uclamp_used is disabled.
+ 	 */
+ 	if (!static_branch_unlikely(&sched_uclamp_used))
+ 		return;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_inc_id(rq, p, clamp_id);
+ 
+ 	/* Reset clamp idle holding when there is one RUNNABLE task */
+ 	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
+ 		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
+ }
+ 
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * Avoid any overhead until uclamp is actually used by the userspace.
+ 	 *
+ 	 * The condition is constructed such that a NOP is generated when
+ 	 * sched_uclamp_used is disabled.
+ 	 */
+ 	if (!static_branch_unlikely(&sched_uclamp_used))
+ 		return;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_dec_id(rq, p, clamp_id);
+ }
+ 
+ static inline void
+ uclamp_update_active(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct rq_flags rf;
+ 	struct rq *rq;
+ 
+ 	/*
+ 	 * Lock the task and the rq where the task is (or was) queued.
+ 	 *
+ 	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the
+ 	 * price to pay to safely serialize util_{min,max} updates with
+ 	 * enqueues, dequeues and migration operations.
+ 	 * This is the same locking schema used by __set_cpus_allowed_ptr().
+ 	 */
+ 	rq = task_rq_lock(p, &rf);
+ 
+ 	/*
+ 	 * Setting the clamp bucket is serialized by task_rq_lock().
+ 	 * If the task is not yet RUNNABLE and its task_struct is not
+ 	 * affecting a valid clamp bucket, the next time it's enqueued,
+ 	 * it will already see the updated clamp bucket value.
+ 	 */
+ 	if (p->uclamp[clamp_id].active) {
+ 		uclamp_rq_dec_id(rq, p, clamp_id);
+ 		uclamp_rq_inc_id(rq, p, clamp_id);
+ 	}
+ 
+ 	task_rq_unlock(rq, p, &rf);
+ }
+ 
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ static inline void
+ uclamp_update_active_tasks(struct cgroup_subsys_state *css,
+ 			   unsigned int clamps)
+ {
+ 	enum uclamp_id clamp_id;
+ 	struct css_task_iter it;
+ 	struct task_struct *p;
+ 
+ 	css_task_iter_start(css, 0, &it);
+ 	while ((p = css_task_iter_next(&it))) {
+ 		for_each_clamp_id(clamp_id) {
+ 			if ((0x1 << clamp_id) & clamps)
+ 				uclamp_update_active(p, clamp_id);
+ 		}
+ 	}
+ 	css_task_iter_end(&it);
+ }
+ 
+ static void cpu_util_update_eff(struct cgroup_subsys_state *css);
+ static void uclamp_update_root_tg(void)
+ {
+ 	struct task_group *tg = &root_task_group;
+ 
+ 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],
+ 		      sysctl_sched_uclamp_util_min, false);
+ 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],
+ 		      sysctl_sched_uclamp_util_max, false);
+ 
+ 	rcu_read_lock();
+ 	cpu_util_update_eff(&root_task_group.css);
+ 	rcu_read_unlock();
+ }
+ #else
+ static void uclamp_update_root_tg(void) { }
+ #endif
+ 
+ int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
+ 				void *buffer, size_t *lenp, loff_t *ppos)
+ {
+ 	bool update_root_tg = false;
+ 	int old_min, old_max, old_min_rt;
+ 	int result;
+ 
+ 	mutex_lock(&uclamp_mutex);
+ 	old_min = sysctl_sched_uclamp_util_min;
+ 	old_max = sysctl_sched_uclamp_util_max;
+ 	old_min_rt = sysctl_sched_uclamp_util_min_rt_default;
+ 
+ 	result = proc_dointvec(table, write, buffer, lenp, ppos);
+ 	if (result)
+ 		goto undo;
+ 	if (!write)
+ 		goto done;
+ 
+ 	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
+ 	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE	||
+ 	    sysctl_sched_uclamp_util_min_rt_default > SCHED_CAPACITY_SCALE) {
+ 
+ 		result = -EINVAL;
+ 		goto undo;
+ 	}
+ 
+ 	if (old_min != sysctl_sched_uclamp_util_min) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MIN],
+ 			      sysctl_sched_uclamp_util_min, false);
+ 		update_root_tg = true;
+ 	}
+ 	if (old_max != sysctl_sched_uclamp_util_max) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MAX],
+ 			      sysctl_sched_uclamp_util_max, false);
+ 		update_root_tg = true;
+ 	}
+ 
+ 	if (update_root_tg) {
+ 		static_branch_enable(&sched_uclamp_used);
+ 		uclamp_update_root_tg();
+ 	}
+ 
+ 	if (old_min_rt != sysctl_sched_uclamp_util_min_rt_default) {
+ 		static_branch_enable(&sched_uclamp_used);
+ 		uclamp_sync_util_min_rt_default();
+ 	}
+ 
+ 	/*
+ 	 * We update all RUNNABLE tasks only when task groups are in use.
+ 	 * Otherwise, keep it simple and do just a lazy update at each next
+ 	 * task enqueue time.
+ 	 */
+ 
+ 	goto done;
+ 
+ undo:
+ 	sysctl_sched_uclamp_util_min = old_min;
+ 	sysctl_sched_uclamp_util_max = old_max;
+ 	sysctl_sched_uclamp_util_min_rt_default = old_min_rt;
+ done:
+ 	mutex_unlock(&uclamp_mutex);
+ 
+ 	return result;
+ }
+ 
+ static int uclamp_validate(struct task_struct *p,
+ 			   const struct sched_attr *attr)
+ {
+ 	int util_min = p->uclamp_req[UCLAMP_MIN].value;
+ 	int util_max = p->uclamp_req[UCLAMP_MAX].value;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {
+ 		util_min = attr->sched_util_min;
+ 
+ 		if (util_min + 1 > SCHED_CAPACITY_SCALE + 1)
+ 			return -EINVAL;
+ 	}
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {
+ 		util_max = attr->sched_util_max;
+ 
+ 		if (util_max + 1 > SCHED_CAPACITY_SCALE + 1)
+ 			return -EINVAL;
+ 	}
+ 
+ 	if (util_min != -1 && util_max != -1 && util_min > util_max)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * We have valid uclamp attributes; make sure uclamp is enabled.
+ 	 *
+ 	 * We need to do that here, because enabling static branches is a
+ 	 * blocking operation which obviously cannot be done while holding
+ 	 * scheduler locks.
+ 	 */
+ 	static_branch_enable(&sched_uclamp_used);
+ 
+ 	return 0;
+ }
+ 
+ static bool uclamp_reset(const struct sched_attr *attr,
+ 			 enum uclamp_id clamp_id,
+ 			 struct uclamp_se *uc_se)
+ {
+ 	/* Reset on sched class change for a non user-defined clamp value. */
+ 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)) &&
+ 	    !uc_se->user_defined)
+ 		return true;
+ 
+ 	/* Reset on sched_util_{min,max} == -1. */
+ 	if (clamp_id == UCLAMP_MIN &&
+ 	    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&
+ 	    attr->sched_util_min == -1) {
+ 		return true;
+ 	}
+ 
+ 	if (clamp_id == UCLAMP_MAX &&
+ 	    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&
+ 	    attr->sched_util_max == -1) {
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		struct uclamp_se *uc_se = &p->uclamp_req[clamp_id];
+ 		unsigned int value;
+ 
+ 		if (!uclamp_reset(attr, clamp_id, uc_se))
+ 			continue;
+ 
+ 		/*
+ 		 * RT by default have a 100% boost value that could be modified
+ 		 * at runtime.
+ 		 */
+ 		if (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))
+ 			value = sysctl_sched_uclamp_util_min_rt_default;
+ 		else
+ 			value = uclamp_none(clamp_id);
+ 
+ 		uclamp_se_set(uc_se, value, false);
+ 
+ 	}
+ 
+ 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
+ 		return;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&
+ 	    attr->sched_util_min != -1) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
+ 			      attr->sched_util_min, true);
+ 	}
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&
+ 	    attr->sched_util_max != -1) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
+ 			      attr->sched_util_max, true);
+ 	}
+ }
+ 
+ static void uclamp_fork(struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here
+ 	 * as the task is still at its early fork stages.
+ 	 */
+ 	for_each_clamp_id(clamp_id)
+ 		p->uclamp[clamp_id].active = false;
+ 
+ 	if (likely(!p->sched_reset_on_fork))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_se_set(&p->uclamp_req[clamp_id],
+ 			      uclamp_none(clamp_id), false);
+ 	}
+ }
+ 
+ static void uclamp_post_fork(struct task_struct *p)
+ {
+ 	uclamp_update_util_min_rt_default(p);
+ }
+ 
+ static void __init init_uclamp_rq(struct rq *rq)
+ {
+ 	enum uclamp_id clamp_id;
+ 	struct uclamp_rq *uc_rq = rq->uclamp;
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uc_rq[clamp_id] = (struct uclamp_rq) {
+ 			.value = uclamp_none(clamp_id)
+ 		};
+ 	}
+ 
+ 	rq->uclamp_flags = 0;
+ }
+ 
+ static void __init init_uclamp(void)
+ {
+ 	struct uclamp_se uc_max = {};
+ 	enum uclamp_id clamp_id;
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu)
+ 		init_uclamp_rq(cpu_rq(cpu));
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_se_set(&init_task.uclamp_req[clamp_id],
+ 			      uclamp_none(clamp_id), false);
+ 	}
+ 
+ 	/* System defaults allow max clamp values for both indexes */
+ 	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_default[clamp_id] = uc_max;
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ 		root_task_group.uclamp_req[clamp_id] = uc_max;
+ 		root_task_group.uclamp[clamp_id] = uc_max;
+ #endif
+ 	}
+ }
+ 
+ #else /* CONFIG_UCLAMP_TASK */
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
+ static inline int uclamp_validate(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr) { }
+ static inline void uclamp_fork(struct task_struct *p) { }
+ static inline void uclamp_post_fork(struct task_struct *p) { }
+ static inline void init_uclamp(void) { }
+ #endif /* CONFIG_UCLAMP_TASK */
+ 
+ bool sched_task_on_rq(struct task_struct *p)
+ {
+ 	return task_on_rq_queued(p);
+ }
+ 
++>>>>>>> a1dfb6311c77 (tick/nohz: Kick only _queued_ task whose tick dependency is updated)
  static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
  {
  	if (!(flags & ENQUEUE_NOCLOCK))
diff --cc kernel/time/tick-sched.c
index a94202a270b5,197a3bd882ad..000000000000
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@@ -268,6 -322,46 +268,49 @@@ void tick_nohz_full_kick_cpu(int cpu
  	irq_work_queue_on(&per_cpu(nohz_full_kick_work, cpu), cpu);
  }
  
++<<<<<<< HEAD
++=======
+ static void tick_nohz_kick_task(struct task_struct *tsk)
+ {
+ 	int cpu;
+ 
+ 	/*
+ 	 * If the task is not running, run_posix_cpu_timers()
+ 	 * has nothing to elapse, IPI can then be spared.
+ 	 *
+ 	 * activate_task()                      STORE p->tick_dep_mask
+ 	 *   STORE p->on_rq
+ 	 * __schedule() (switch to task 'p')    smp_mb() (atomic_fetch_or())
+ 	 *   LOCK rq->lock                      LOAD p->on_rq
+ 	 *   smp_mb__after_spin_lock()
+ 	 *   tick_nohz_task_switch()
+ 	 *     LOAD p->tick_dep_mask
+ 	 */
+ 	if (!sched_task_on_rq(tsk))
+ 		return;
+ 
+ 	/*
+ 	 * If the task concurrently migrates to another CPU,
+ 	 * we guarantee it sees the new tick dependency upon
+ 	 * schedule.
+ 	 *
+ 	 * set_task_cpu(p, cpu);
+ 	 *   STORE p->cpu = @cpu
+ 	 * __schedule() (switch to task 'p')
+ 	 *   LOCK rq->lock
+ 	 *   smp_mb__after_spin_lock()          STORE p->tick_dep_mask
+ 	 *   tick_nohz_task_switch()            smp_mb() (atomic_fetch_or())
+ 	 *      LOAD p->tick_dep_mask           LOAD p->cpu
+ 	 */
+ 	cpu = task_cpu(tsk);
+ 
+ 	preempt_disable();
+ 	if (cpu_online(cpu))
+ 		tick_nohz_full_kick_cpu(cpu);
+ 	preempt_enable();
+ }
+ 
++>>>>>>> a1dfb6311c77 (tick/nohz: Kick only _queued_ task whose tick dependency is updated)
  /*
   * Kick all full dynticks CPUs in order to force these to re-evaluate
   * their dependency on the tick and restart it if necessary.
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8bd4b72a9c76..4a6958eb78ae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1894,6 +1894,8 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 
 #endif /* CONFIG_SMP */
 
+extern bool sched_task_on_rq(struct task_struct *p);
+
 /*
  * In order to reduce various lock holder preemption latencies provide an
  * interface to see if a vCPU is currently running or not.
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/time/tick-sched.c
