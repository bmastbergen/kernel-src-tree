RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit b045db62f6f61c2f0f993696abe620379db34163
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/b045db62.failed

SRQ uses a quantized and scaled page_offset, which is another variation of
ib_umem_find_best_pgsz(). Add mlx5_umem_find_best_quantized_pgoff() to
perform this calculation for each mailbox. A macro shows how the
calculation is directly connected to the mailbox format.

This new routine replaces the limited mlx5_ib_cont_pages() and
mlx5_ib_get_buf_offset() pairing which would reject valid configurations
rather than adjust the page_size to make it work.

In turn this is much more aggressive about choosing large page sizes for
these objects and when THP is enabled it will now often find a single page
solution.

Link: https://lore.kernel.org/r/20201115114311.136250-2-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit b045db62f6f61c2f0f993696abe620379db34163)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mem.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/srq.c
#	include/rdma/ib_umem.h
diff --cc drivers/infiniband/hw/mlx5/mem.c
index c19ec9fd8a63,fd9778113d26..000000000000
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@@ -165,13 -107,51 +165,61 @@@ void __mlx5_ib_populate_pas(struct mlx5
  	}
  }
  
++<<<<<<< HEAD
 +void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int page_shift, __be64 *pas, int access_flags)
 +{
 +	return __mlx5_ib_populate_pas(dev, umem, page_shift, 0,
 +				      ib_umem_num_pages(umem), pas,
 +				      access_flags);
 +}
++=======
+ /*
+  * Compute the page shift and page_offset for mailboxes that use a quantized
+  * page_offset. The granulatity of the page offset scales according to page
+  * size.
+  */
+ unsigned long __mlx5_umem_find_best_quantized_pgoff(
+ 	struct ib_umem *umem, unsigned long pgsz_bitmap,
+ 	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
+ 	unsigned int *page_offset_quantized)
+ {
+ 	const u64 page_offset_mask = (1 << page_offset_bits) - 1;
+ 	unsigned long page_size;
+ 	u64 page_offset;
+ 
+ 	page_size = ib_umem_find_best_pgoff(umem, pgsz_bitmap, pgoff_bitmask);
+ 	if (!page_size)
+ 		return 0;
+ 
+ 	/*
+ 	 * page size is the largest possible page size.
+ 	 *
+ 	 * Reduce the page_size, and thus the page_offset and quanta, until the
+ 	 * page_offset fits into the mailbox field. Once page_size < scale this
+ 	 * loop is guaranteed to terminate.
+ 	 */
+ 	page_offset = ib_umem_dma_offset(umem, page_size);
+ 	while (page_offset & ~(u64)(page_offset_mask * (page_size / scale))) {
+ 		page_size /= 2;
+ 		page_offset = ib_umem_dma_offset(umem, page_size);
+ 	}
+ 
+ 	/*
+ 	 * The address is not aligned, or otherwise cannot be represented by the
+ 	 * page_offset.
+ 	 */
+ 	if (!(pgsz_bitmap & page_size))
+ 		return 0;
+ 
+ 	*page_offset_quantized =
+ 		(unsigned long)page_offset / (page_size / scale);
+ 	if (WARN_ON(*page_offset_quantized > page_offset_mask))
+ 		return 0;
+ 	return page_size;
+ }
+ 
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset)
  {
  	u64 page_size;
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2c6dfd189e0c,2f08a5b4a438..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -42,6 -42,64 +42,67 @@@
  
  #define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
  
++<<<<<<< HEAD
++=======
+ static __always_inline unsigned long
+ __mlx5_log_page_size_to_bitmap(unsigned int log_pgsz_bits,
+ 			       unsigned int pgsz_shift)
+ {
+ 	unsigned int largest_pg_shift =
+ 		min_t(unsigned long, (1ULL << log_pgsz_bits) - 1 + pgsz_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	/*
+ 	 * Despite a command allowing it, the device does not support lower than
+ 	 * 4k page size.
+ 	 */
+ 	pgsz_shift = max_t(unsigned int, MLX5_ADAPTER_PAGE_SHIFT, pgsz_shift);
+ 	return GENMASK(largest_pg_shift, pgsz_shift);
+ }
+ 
+ /*
+  * For mkc users, instead of a page_offset the command has a start_iova which
+  * specifies both the page_offset and the on-the-wire IOVA
+  */
+ #define mlx5_umem_find_best_pgsz(umem, typ, log_pgsz_fld, pgsz_shift, iova)    \
+ 	ib_umem_find_best_pgsz(umem,                                           \
+ 			       __mlx5_log_page_size_to_bitmap(                 \
+ 				       __mlx5_bit_sz(typ, log_pgsz_fld),       \
+ 				       pgsz_shift),                            \
+ 			       iova)
+ 
+ static __always_inline unsigned long
+ __mlx5_page_offset_to_bitmask(unsigned int page_offset_bits,
+ 			      unsigned int offset_shift)
+ {
+ 	unsigned int largest_offset_shift =
+ 		min_t(unsigned long, page_offset_bits - 1 + offset_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	return GENMASK(largest_offset_shift, offset_shift);
+ }
+ 
+ /*
+  * QP/CQ/WQ/etc type commands take a page offset that satisifies:
+  *   page_offset_quantized * (page_size/scale) = page_offset
+  * Which restricts allowed page sizes to ones that satisify the above.
+  */
+ unsigned long __mlx5_umem_find_best_quantized_pgoff(
+ 	struct ib_umem *umem, unsigned long pgsz_bitmap,
+ 	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
+ 	unsigned int *page_offset_quantized);
+ #define mlx5_umem_find_best_quantized_pgoff(umem, typ, log_pgsz_fld,           \
+ 					    pgsz_shift, page_offset_fld,       \
+ 					    scale, page_offset_quantized)      \
+ 	__mlx5_umem_find_best_quantized_pgoff(                                 \
+ 		umem,                                                          \
+ 		__mlx5_log_page_size_to_bitmap(                                \
+ 			__mlx5_bit_sz(typ, log_pgsz_fld), pgsz_shift),         \
+ 		__mlx5_bit_sz(typ, page_offset_fld),                           \
+ 		GENMASK(31, order_base_2(scale)), scale,                       \
+ 		page_offset_quantized)
+ 
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  enum {
  	MLX5_IB_MMAP_OFFSET_START = 9,
  	MLX5_IB_MMAP_OFFSET_END = 255,
diff --cc drivers/infiniband/hw/mlx5/srq.c
index caae9501fb79,7dfdc9e54866..000000000000
--- a/drivers/infiniband/hw/mlx5/srq.c
+++ b/drivers/infiniband/hw/mlx5/srq.c
@@@ -51,10 -51,8 +51,15 @@@ static int create_srq_user(struct ib_p
  		udata, struct mlx5_ib_ucontext, ibucontext);
  	size_t ucmdlen;
  	int err;
++<<<<<<< HEAD
 +	int npages;
 +	int page_shift;
 +	int ncont;
 +	u32 offset;
++=======
+ 	unsigned int page_offset_quantized;
+ 	unsigned int page_size;
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  	u32 uidx = MLX5_IB_DEFAULT_UIDX;
  
  	ucmdlen = min(udata->inlen, sizeof(ucmd));
@@@ -87,22 -85,22 +92,38 @@@
  		return err;
  	}
  
++<<<<<<< HEAD
 +	mlx5_ib_cont_pages(srq->umem, ucmd.buf_addr, 0, &npages,
 +			   &page_shift, &ncont, NULL);
 +	err = mlx5_ib_get_buf_offset(ucmd.buf_addr, page_shift,
 +				     &offset);
 +	if (err) {
++=======
+ 	page_size = mlx5_umem_find_best_quantized_pgoff(
+ 		srq->umem, srqc, log_page_size, MLX5_ADAPTER_PAGE_SHIFT,
+ 		page_offset, 64, &page_offset_quantized);
+ 	if (!page_size) {
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  		mlx5_ib_warn(dev, "bad offset\n");
  		goto err_umem;
  	}
  
++<<<<<<< HEAD
 +	in->pas = kvcalloc(ncont, sizeof(*in->pas), GFP_KERNEL);
++=======
+ 	in->pas = kvcalloc(ib_umem_num_dma_blocks(srq->umem, page_size),
+ 			   sizeof(*in->pas), GFP_KERNEL);
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  	if (!in->pas) {
  		err = -ENOMEM;
  		goto err_umem;
  	}
  
++<<<<<<< HEAD
 +	mlx5_ib_populate_pas(dev, srq->umem, page_shift, in->pas, 0);
++=======
+ 	mlx5_ib_populate_pas(srq->umem, page_size, in->pas, 0);
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  
  	err = mlx5_ib_db_map_user(ucontext, udata, ucmd.db_addr, &srq->db);
  	if (err) {
diff --cc include/rdma/ib_umem.h
index a50fd187bbe4,7752211c9638..000000000000
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@@ -33,16 -34,51 +33,34 @@@ static inline int ib_umem_offset(struc
  	return umem->address & ~PAGE_MASK;
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long ib_umem_dma_offset(struct ib_umem *umem,
+ 					       unsigned long pgsz)
+ {
+ 	return (sg_dma_address(umem->sg_head.sgl) + ib_umem_offset(umem)) &
+ 	       (pgsz - 1);
+ }
+ 
+ static inline size_t ib_umem_num_dma_blocks(struct ib_umem *umem,
+ 					    unsigned long pgsz)
+ {
+ 	return (size_t)((ALIGN(umem->iova + umem->length, pgsz) -
+ 			 ALIGN_DOWN(umem->iova, pgsz))) /
+ 	       pgsz;
+ }
+ 
++>>>>>>> b045db62f6f6 (RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ)
  static inline size_t ib_umem_num_pages(struct ib_umem *umem)
  {
 -	return ib_umem_num_dma_blocks(umem, PAGE_SIZE);
 +	return (ALIGN(umem->address + umem->length, PAGE_SIZE) -
 +		ALIGN_DOWN(umem->address, PAGE_SIZE)) >>
 +	       PAGE_SHIFT;
  }
  
 -static inline void __rdma_umem_block_iter_start(struct ib_block_iter *biter,
 -						struct ib_umem *umem,
 -						unsigned long pgsz)
 -{
 -	__rdma_block_iter_start(biter, umem->sg_head.sgl, umem->nmap, pgsz);
 -}
 -
 -/**
 - * rdma_umem_for_each_dma_block - iterate over contiguous DMA blocks of the umem
 - * @umem: umem to iterate over
 - * @pgsz: Page size to split the list into
 - *
 - * pgsz must be <= PAGE_SIZE or computed by ib_umem_find_best_pgsz(). The
 - * returned DMA blocks will be aligned to pgsz and span the range:
 - * ALIGN_DOWN(umem->address, pgsz) to ALIGN(umem->address + umem->length, pgsz)
 - *
 - * Performs exactly ib_umem_num_dma_blocks() iterations.
 - */
 -#define rdma_umem_for_each_dma_block(umem, biter, pgsz)                        \
 -	for (__rdma_umem_block_iter_start(biter, umem, pgsz);                  \
 -	     __rdma_block_iter_next(biter);)
 -
  #ifdef CONFIG_INFINIBAND_USER_MEM
  
 -struct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,
 +struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
  			    size_t size, int access);
  void ib_umem_release(struct ib_umem *umem);
  int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
* Unmerged path drivers/infiniband/hw/mlx5/mem.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/srq.c
* Unmerged path include/rdma/ib_umem.h
