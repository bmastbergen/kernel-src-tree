mm: memcontrol: make the slab calculation consistent

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Muchun Song <songmuchun@bytedance.com>
commit fff66b79a19c9b3f2aa02b0a32fe598977c89eea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/fff66b79.failed

Although the ratio of the slab is one, we also should read the ratio from
the related memory_stats instead of hard-coding.  And the local variable
of size is already the value of slab_unreclaimable.  So we do not need to
read again.

To do this we need some code like below:

if (unlikely(memory_stats[i].idx == NR_SLAB_UNRECLAIMABLE_B)) {
-	size = memcg_page_state(memcg, NR_SLAB_RECLAIMABLE_B) +
-	       memcg_page_state(memcg, NR_SLAB_UNRECLAIMABLE_B);
+       VM_BUG_ON(i < 1);
+       VM_BUG_ON(memory_stats[i - 1].idx != NR_SLAB_RECLAIMABLE_B);
+	size += memcg_page_state(memcg, memory_stats[i - 1].idx) *
+		memory_stats[i - 1].ratio;

It requires a series of VM_BUG_ONs or comments to ensure these two items
are actually adjacent and in the right order.  So it would probably be
easier to implement this using a wrapper that has a big switch() for unit
conversion.

More details about this discussion can refer to:

    https://lore.kernel.org/patchwork/patch/1348611/

This would fix the ratio inconsistency and get rid of the order
guarantee.

Link: https://lkml.kernel.org/r/20201228164110.2838-8-songmuchun@bytedance.com
	Signed-off-by: Muchun Song <songmuchun@bytedance.com>
	Cc: Alexey Dobriyan <adobriyan@gmail.com>
	Cc: Feng Tang <feng.tang@intel.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: NeilBrown <neilb@suse.de>
	Cc: Pankaj Gupta <pankaj.gupta@cloud.ionos.com>
	Cc: Rafael. J. Wysocki <rafael@kernel.org>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Roman Gushchin <guro@fb.com>
	Cc: Sami Tolvanen <samitolvanen@google.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fff66b79a19c9b3f2aa02b0a32fe598977c89eea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 4d65b541b4a3,f27e862a6845..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -1468,6 -1510,73 +1468,76 @@@ static bool mem_cgroup_wait_acct_move(s
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ struct memory_stat {
+ 	const char *name;
+ 	unsigned int idx;
+ };
+ 
+ static const struct memory_stat memory_stats[] = {
+ 	{ "anon",			NR_ANON_MAPPED			},
+ 	{ "file",			NR_FILE_PAGES			},
+ 	{ "kernel_stack",		NR_KERNEL_STACK_KB		},
+ 	{ "pagetables",			NR_PAGETABLE			},
+ 	{ "percpu",			MEMCG_PERCPU_B			},
+ 	{ "sock",			MEMCG_SOCK			},
+ 	{ "shmem",			NR_SHMEM			},
+ 	{ "file_mapped",		NR_FILE_MAPPED			},
+ 	{ "file_dirty",			NR_FILE_DIRTY			},
+ 	{ "file_writeback",		NR_WRITEBACK			},
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	{ "anon_thp",			NR_ANON_THPS			},
+ 	{ "file_thp",			NR_FILE_THPS			},
+ 	{ "shmem_thp",			NR_SHMEM_THPS			},
+ #endif
+ 	{ "inactive_anon",		NR_INACTIVE_ANON		},
+ 	{ "active_anon",		NR_ACTIVE_ANON			},
+ 	{ "inactive_file",		NR_INACTIVE_FILE		},
+ 	{ "active_file",		NR_ACTIVE_FILE			},
+ 	{ "unevictable",		NR_UNEVICTABLE			},
+ 	{ "slab_reclaimable",		NR_SLAB_RECLAIMABLE_B		},
+ 	{ "slab_unreclaimable",		NR_SLAB_UNRECLAIMABLE_B		},
+ 
+ 	/* The memory events */
+ 	{ "workingset_refault_anon",	WORKINGSET_REFAULT_ANON		},
+ 	{ "workingset_refault_file",	WORKINGSET_REFAULT_FILE		},
+ 	{ "workingset_activate_anon",	WORKINGSET_ACTIVATE_ANON	},
+ 	{ "workingset_activate_file",	WORKINGSET_ACTIVATE_FILE	},
+ 	{ "workingset_restore_anon",	WORKINGSET_RESTORE_ANON		},
+ 	{ "workingset_restore_file",	WORKINGSET_RESTORE_FILE		},
+ 	{ "workingset_nodereclaim",	WORKINGSET_NODERECLAIM		},
+ };
+ 
+ /* Translate stat items to the correct unit for memory.stat output */
+ static int memcg_page_state_unit(int item)
+ {
+ 	switch (item) {
+ 	case MEMCG_PERCPU_B:
+ 	case NR_SLAB_RECLAIMABLE_B:
+ 	case NR_SLAB_UNRECLAIMABLE_B:
+ 	case WORKINGSET_REFAULT_ANON:
+ 	case WORKINGSET_REFAULT_FILE:
+ 	case WORKINGSET_ACTIVATE_ANON:
+ 	case WORKINGSET_ACTIVATE_FILE:
+ 	case WORKINGSET_RESTORE_ANON:
+ 	case WORKINGSET_RESTORE_FILE:
+ 	case WORKINGSET_NODERECLAIM:
+ 		return 1;
+ 	case NR_KERNEL_STACK_KB:
+ 		return SZ_1K;
+ 	default:
+ 		return PAGE_SIZE;
+ 	}
+ }
+ 
+ static inline unsigned long memcg_page_state_output(struct mem_cgroup *memcg,
+ 						    int item)
+ {
+ 	return memcg_page_state(memcg, item) * memcg_page_state_unit(item);
+ }
+ 
++>>>>>>> fff66b79a19c (mm: memcontrol: make the slab calculation consistent)
  static char *memory_stat_format(struct mem_cgroup *memcg)
  {
  	struct seq_buf s;
@@@ -1488,52 -1597,18 +1558,64 @@@
  	 * Current memory state:
  	 */
  
 -	for (i = 0; i < ARRAY_SIZE(memory_stats); i++) {
 -		u64 size;
 +	seq_buf_printf(&s, "anon %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_ANON_MAPPED) *
 +		       PAGE_SIZE);
 +	seq_buf_printf(&s, "file %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_FILE_PAGES) *
 +		       PAGE_SIZE);
 +	seq_buf_printf(&s, "kernel_stack %llu\n",
 +		       (u64)memcg_page_state(memcg, MEMCG_KERNEL_STACK_KB) *
 +		       1024);
 +	seq_buf_printf(&s, "slab %llu\n",
 +		       (u64)(memcg_page_state(memcg, NR_SLAB_RECLAIMABLE_B) +
 +			     memcg_page_state(memcg, NR_SLAB_UNRECLAIMABLE_B)));
 +	seq_buf_printf(&s, "percpu %llu\n",
 +		       (u64)memcg_page_state(memcg, MEMCG_PERCPU_B));
 +	seq_buf_printf(&s, "sock %llu\n",
 +		       (u64)memcg_page_state(memcg, MEMCG_SOCK) *
 +		       PAGE_SIZE);
 +
++<<<<<<< HEAD
 +	seq_buf_printf(&s, "shmem %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_SHMEM) *
 +		       PAGE_SIZE);
 +	seq_buf_printf(&s, "file_mapped %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_FILE_MAPPED) *
 +		       PAGE_SIZE);
 +	seq_buf_printf(&s, "file_dirty %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_FILE_DIRTY) *
 +		       PAGE_SIZE);
 +	seq_buf_printf(&s, "file_writeback %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_WRITEBACK) *
 +		       PAGE_SIZE);
  
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +	seq_buf_printf(&s, "anon_thp %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_ANON_THPS) *
 +		       HPAGE_PMD_SIZE);
 +#endif
 +
 +	for (i = 0; i < NR_LRU_LISTS; i++)
 +		seq_buf_printf(&s, "%s %llu\n", lru_list_name(i),
 +			       (u64)memcg_page_state(memcg, NR_LRU_BASE + i) *
 +			       PAGE_SIZE);
 +
 +	seq_buf_printf(&s, "slab_reclaimable %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_SLAB_RECLAIMABLE_B));
 +	seq_buf_printf(&s, "slab_unreclaimable %llu\n",
 +		       (u64)memcg_page_state(memcg, NR_SLAB_UNRECLAIMABLE_B));
++=======
+ 		size = memcg_page_state_output(memcg, memory_stats[i].idx);
+ 		seq_buf_printf(&s, "%s %llu\n", memory_stats[i].name, size);
+ 
+ 		if (unlikely(memory_stats[i].idx == NR_SLAB_UNRECLAIMABLE_B)) {
+ 			size += memcg_page_state_output(memcg,
+ 							NR_SLAB_RECLAIMABLE_B);
+ 			seq_buf_printf(&s, "slab %llu\n", size);
+ 		}
+ 	}
++>>>>>>> fff66b79a19c (mm: memcontrol: make the slab calculation consistent)
  
  	/* Accumulated memory events */
  
@@@ -6411,6 -6395,41 +6493,44 @@@ static int memory_stat_show(struct seq_
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA
+ static inline unsigned long lruvec_page_state_output(struct lruvec *lruvec,
+ 						     int item)
+ {
+ 	return lruvec_page_state(lruvec, item) * memcg_page_state_unit(item);
+ }
+ 
+ static int memory_numa_stat_show(struct seq_file *m, void *v)
+ {
+ 	int i;
+ 	struct mem_cgroup *memcg = mem_cgroup_from_seq(m);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(memory_stats); i++) {
+ 		int nid;
+ 
+ 		if (memory_stats[i].idx >= NR_VM_NODE_STAT_ITEMS)
+ 			continue;
+ 
+ 		seq_printf(m, "%s", memory_stats[i].name);
+ 		for_each_node_state(nid, N_MEMORY) {
+ 			u64 size;
+ 			struct lruvec *lruvec;
+ 
+ 			lruvec = mem_cgroup_lruvec(memcg, NODE_DATA(nid));
+ 			size = lruvec_page_state_output(lruvec,
+ 							memory_stats[i].idx);
+ 			seq_printf(m, " N%d=%llu", nid, size);
+ 		}
+ 		seq_putc(m, '\n');
+ 	}
+ 
+ 	return 0;
+ }
+ #endif
+ 
++>>>>>>> fff66b79a19c (mm: memcontrol: make the slab calculation consistent)
  static int memory_oom_group_show(struct seq_file *m, void *v)
  {
  	struct mem_cgroup *memcg = mem_cgroup_from_seq(m);
* Unmerged path mm/memcontrol.c
