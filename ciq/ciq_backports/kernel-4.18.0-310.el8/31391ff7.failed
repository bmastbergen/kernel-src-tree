powerpc/spinlocks: Rename SPLPAR-only spinlocks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Christopher M. Riedl <cmr@informatik.wtf>
commit 31391ff7ea1ef557a804475436501f33ff0ead95
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/31391ff7.failed

The __rw_yield and __spin_yield locks only pertain to SPLPAR mode.
Rename them to make this relationship obvious.

	Signed-off-by: Christopher M. Riedl <cmr@informatik.wtf>
	Reviewed-by: Andrew Donnellan <ajd@linux.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20190813031314.1828-3-cmr@informatik.wtf
(cherry picked from commit 31391ff7ea1ef557a804475436501f33ff0ead95)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/spinlock.h
diff --cc arch/powerpc/include/asm/spinlock.h
index b322a6b3c984,0d04d468f660..000000000000
--- a/arch/powerpc/include/asm/spinlock.h
+++ b/arch/powerpc/include/asm/spinlock.h
@@@ -108,9 -101,10 +108,16 @@@ static inline int arch_spin_trylock(arc
  
  #if defined(CONFIG_PPC_SPLPAR)
  /* We only yield to the hypervisor if we are in shared processor mode */
++<<<<<<< HEAD
 +#define SHARED_PROCESSOR (static_branch_unlikely(&shared_processor))
 +extern void __spin_yield(arch_spinlock_t *lock);
 +extern void __rw_yield(arch_rwlock_t *lock);
++=======
+ void splpar_spin_yield(arch_spinlock_t *lock);
+ void splpar_rw_yield(arch_rwlock_t *lock);
+ #define __spin_yield(x) splpar_spin_yield(x)
+ #define __rw_yield(x) splpar_rw_yield(x)
++>>>>>>> 31391ff7ea1e (powerpc/spinlocks: Rename SPLPAR-only spinlocks)
  #else /* SPLPAR */
  #define __spin_yield(x)	barrier()
  #define __rw_yield(x)	barrier()
* Unmerged path arch/powerpc/include/asm/spinlock.h
diff --git a/arch/powerpc/lib/locks.c b/arch/powerpc/lib/locks.c
index b7b1237d4aa6..1920328d8aa6 100644
--- a/arch/powerpc/lib/locks.c
+++ b/arch/powerpc/lib/locks.c
@@ -23,7 +23,7 @@
 #include <asm/hvcall.h>
 #include <asm/smp.h>
 
-void __spin_yield(arch_spinlock_t *lock)
+void splpar_spin_yield(arch_spinlock_t *lock)
 {
 	unsigned int lock_value, holder_cpu, yield_count;
 
@@ -41,14 +41,14 @@ void __spin_yield(arch_spinlock_t *lock)
 	plpar_hcall_norets(H_CONFER,
 		get_hard_smp_processor_id(holder_cpu), yield_count);
 }
-EXPORT_SYMBOL_GPL(__spin_yield);
+EXPORT_SYMBOL_GPL(splpar_spin_yield);
 
 /*
  * Waiting for a read lock or a write lock on a rwlock...
  * This turns out to be the same for read and write locks, since
  * we only know the holder if it is write-locked.
  */
-void __rw_yield(arch_rwlock_t *rw)
+void splpar_rw_yield(arch_rwlock_t *rw)
 {
 	int lock_value;
 	unsigned int holder_cpu, yield_count;
