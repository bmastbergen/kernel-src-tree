sched/core: Fix typos in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Tal Zussman <tz2294@columbia.edu>
commit b19a888c1e9bdf12e0d8dd9aeb887ca7de91c8a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/b19a888c.failed

	Signed-off-by: Tal Zussman <tz2294@columbia.edu>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20201113005156.GA8408@charmander
(cherry picked from commit b19a888c1e9bdf12e0d8dd9aeb887ca7de91c8a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index e10102d83a0c,a9e6d630eb83..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -872,6 -871,693 +872,696 @@@ static void set_load_weight(struct task
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_UCLAMP_TASK
+ /*
+  * Serializes updates of utilization clamp values
+  *
+  * The (slow-path) user-space triggers utilization clamp value updates which
+  * can require updates on (fast-path) scheduler's data structures used to
+  * support enqueue/dequeue operations.
+  * While the per-CPU rq lock protects fast-path update operations, user-space
+  * requests are serialized using a mutex to reduce the risk of conflicting
+  * updates or API abuses.
+  */
+ static DEFINE_MUTEX(uclamp_mutex);
+ 
+ /* Max allowed minimum utilization */
+ unsigned int sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
+ 
+ /* Max allowed maximum utilization */
+ unsigned int sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
+ 
+ /*
+  * By default RT tasks run at the maximum performance point/capacity of the
+  * system. Uclamp enforces this by always setting UCLAMP_MIN of RT tasks to
+  * SCHED_CAPACITY_SCALE.
+  *
+  * This knob allows admins to change the default behavior when uclamp is being
+  * used. In battery powered devices, particularly, running at the maximum
+  * capacity and frequency will increase energy consumption and shorten the
+  * battery life.
+  *
+  * This knob only affects RT tasks that their uclamp_se->user_defined == false.
+  *
+  * This knob will not override the system default sched_util_clamp_min defined
+  * above.
+  */
+ unsigned int sysctl_sched_uclamp_util_min_rt_default = SCHED_CAPACITY_SCALE;
+ 
+ /* All clamps are required to be less or equal than these values */
+ static struct uclamp_se uclamp_default[UCLAMP_CNT];
+ 
+ /*
+  * This static key is used to reduce the uclamp overhead in the fast path. It
+  * primarily disables the call to uclamp_rq_{inc, dec}() in
+  * enqueue/dequeue_task().
+  *
+  * This allows users to continue to enable uclamp in their kernel config with
+  * minimum uclamp overhead in the fast path.
+  *
+  * As soon as userspace modifies any of the uclamp knobs, the static key is
+  * enabled, since we have an actual users that make use of uclamp
+  * functionality.
+  *
+  * The knobs that would enable this static key are:
+  *
+  *   * A task modifying its uclamp value with sched_setattr().
+  *   * An admin modifying the sysctl_sched_uclamp_{min, max} via procfs.
+  *   * An admin modifying the cgroup cpu.uclamp.{min, max}
+  */
+ DEFINE_STATIC_KEY_FALSE(sched_uclamp_used);
+ 
+ /* Integer rounded range for each bucket */
+ #define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
+ 
+ #define for_each_clamp_id(clamp_id) \
+ 	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
+ 
+ static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
+ {
+ 	return clamp_value / UCLAMP_BUCKET_DELTA;
+ }
+ 
+ static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
+ {
+ 	if (clamp_id == UCLAMP_MIN)
+ 		return 0;
+ 	return SCHED_CAPACITY_SCALE;
+ }
+ 
+ static inline void uclamp_se_set(struct uclamp_se *uc_se,
+ 				 unsigned int value, bool user_defined)
+ {
+ 	uc_se->value = value;
+ 	uc_se->bucket_id = uclamp_bucket_id(value);
+ 	uc_se->user_defined = user_defined;
+ }
+ 
+ static inline unsigned int
+ uclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,
+ 		  unsigned int clamp_value)
+ {
+ 	/*
+ 	 * Avoid blocked utilization pushing up the frequency when we go
+ 	 * idle (which drops the max-clamp) by retaining the last known
+ 	 * max-clamp.
+ 	 */
+ 	if (clamp_id == UCLAMP_MAX) {
+ 		rq->uclamp_flags |= UCLAMP_FLAG_IDLE;
+ 		return clamp_value;
+ 	}
+ 
+ 	return uclamp_none(UCLAMP_MIN);
+ }
+ 
+ static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
+ 				     unsigned int clamp_value)
+ {
+ 	/* Reset max-clamp retention only on idle exit */
+ 	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
+ 		return;
+ 
+ 	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
+ }
+ 
+ static inline
+ unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
+ 				   unsigned int clamp_value)
+ {
+ 	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
+ 	int bucket_id = UCLAMP_BUCKETS - 1;
+ 
+ 	/*
+ 	 * Since both min and max clamps are max aggregated, find the
+ 	 * top most bucket with tasks in.
+ 	 */
+ 	for ( ; bucket_id >= 0; bucket_id--) {
+ 		if (!bucket[bucket_id].tasks)
+ 			continue;
+ 		return bucket[bucket_id].value;
+ 	}
+ 
+ 	/* No tasks -- default clamp values */
+ 	return uclamp_idle_value(rq, clamp_id, clamp_value);
+ }
+ 
+ static void __uclamp_update_util_min_rt_default(struct task_struct *p)
+ {
+ 	unsigned int default_util_min;
+ 	struct uclamp_se *uc_se;
+ 
+ 	lockdep_assert_held(&p->pi_lock);
+ 
+ 	uc_se = &p->uclamp_req[UCLAMP_MIN];
+ 
+ 	/* Only sync if user didn't override the default */
+ 	if (uc_se->user_defined)
+ 		return;
+ 
+ 	default_util_min = sysctl_sched_uclamp_util_min_rt_default;
+ 	uclamp_se_set(uc_se, default_util_min, false);
+ }
+ 
+ static void uclamp_update_util_min_rt_default(struct task_struct *p)
+ {
+ 	struct rq_flags rf;
+ 	struct rq *rq;
+ 
+ 	if (!rt_task(p))
+ 		return;
+ 
+ 	/* Protect updates to p->uclamp_* */
+ 	rq = task_rq_lock(p, &rf);
+ 	__uclamp_update_util_min_rt_default(p);
+ 	task_rq_unlock(rq, p, &rf);
+ }
+ 
+ static void uclamp_sync_util_min_rt_default(void)
+ {
+ 	struct task_struct *g, *p;
+ 
+ 	/*
+ 	 * copy_process()			sysctl_uclamp
+ 	 *					  uclamp_min_rt = X;
+ 	 *   write_lock(&tasklist_lock)		  read_lock(&tasklist_lock)
+ 	 *   // link thread			  smp_mb__after_spinlock()
+ 	 *   write_unlock(&tasklist_lock)	  read_unlock(&tasklist_lock);
+ 	 *   sched_post_fork()			  for_each_process_thread()
+ 	 *     __uclamp_sync_rt()		    __uclamp_sync_rt()
+ 	 *
+ 	 * Ensures that either sched_post_fork() will observe the new
+ 	 * uclamp_min_rt or for_each_process_thread() will observe the new
+ 	 * task.
+ 	 */
+ 	read_lock(&tasklist_lock);
+ 	smp_mb__after_spinlock();
+ 	read_unlock(&tasklist_lock);
+ 
+ 	rcu_read_lock();
+ 	for_each_process_thread(g, p)
+ 		uclamp_update_util_min_rt_default(p);
+ 	rcu_read_unlock();
+ }
+ 
+ static inline struct uclamp_se
+ uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ 	struct uclamp_se uc_max;
+ 
+ 	/*
+ 	 * Tasks in autogroups or root task group will be
+ 	 * restricted by system defaults.
+ 	 */
+ 	if (task_group_is_autogroup(task_group(p)))
+ 		return uc_req;
+ 	if (task_group(p) == &root_task_group)
+ 		return uc_req;
+ 
+ 	uc_max = task_group(p)->uclamp[clamp_id];
+ 	if (uc_req.value > uc_max.value || !uc_req.user_defined)
+ 		return uc_max;
+ #endif
+ 
+ 	return uc_req;
+ }
+ 
+ /*
+  * The effective clamp bucket index of a task depends on, by increasing
+  * priority:
+  * - the task specific clamp value, when explicitly requested from userspace
+  * - the task group effective clamp value, for tasks not either in the root
+  *   group or in an autogroup
+  * - the system default clamp value, defined by the sysadmin
+  */
+ static inline struct uclamp_se
+ uclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);
+ 	struct uclamp_se uc_max = uclamp_default[clamp_id];
+ 
+ 	/* System default restrictions always apply */
+ 	if (unlikely(uc_req.value > uc_max.value))
+ 		return uc_max;
+ 
+ 	return uc_req;
+ }
+ 
+ unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_se uc_eff;
+ 
+ 	/* Task currently refcounted: use back-annotated (effective) value */
+ 	if (p->uclamp[clamp_id].active)
+ 		return (unsigned long)p->uclamp[clamp_id].value;
+ 
+ 	uc_eff = uclamp_eff_get(p, clamp_id);
+ 
+ 	return (unsigned long)uc_eff.value;
+ }
+ 
+ /*
+  * When a task is enqueued on a rq, the clamp bucket currently defined by the
+  * task's uclamp::bucket_id is refcounted on that rq. This also immediately
+  * updates the rq's clamp value if required.
+  *
+  * Tasks can have a task-specific value requested from user-space, track
+  * within each bucket the maximum value for tasks refcounted in it.
+  * This "local max aggregation" allows to track the exact "requested" value
+  * for each bucket when all its RUNNABLE tasks require the same clamp.
+  */
+ static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
+ 				    enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	/* Update task effective clamp */
+ 	p->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 	bucket->tasks++;
+ 	uc_se->active = true;
+ 
+ 	uclamp_idle_reset(rq, clamp_id, uc_se->value);
+ 
+ 	/*
+ 	 * Local max aggregation: rq buckets always track the max
+ 	 * "requested" clamp value of its RUNNABLE tasks.
+ 	 */
+ 	if (bucket->tasks == 1 || uc_se->value > bucket->value)
+ 		bucket->value = uc_se->value;
+ 
+ 	if (uc_se->value > READ_ONCE(uc_rq->value))
+ 		WRITE_ONCE(uc_rq->value, uc_se->value);
+ }
+ 
+ /*
+  * When a task is dequeued from a rq, the clamp bucket refcounted by the task
+  * is released. If this is the last task reference counting the rq's max
+  * active clamp value, then the rq's clamp value is updated.
+  *
+  * Both refcounted tasks and rq's cached clamp values are expected to be
+  * always valid. If it's detected they are not, as defensive programming,
+  * enforce the expected state and warn.
+  */
+ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
+ 				    enum uclamp_id clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 	unsigned int bkt_clamp;
+ 	unsigned int rq_clamp;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	/*
+ 	 * If sched_uclamp_used was enabled after task @p was enqueued,
+ 	 * we could end up with unbalanced call to uclamp_rq_dec_id().
+ 	 *
+ 	 * In this case the uc_se->active flag should be false since no uclamp
+ 	 * accounting was performed at enqueue time and we can just return
+ 	 * here.
+ 	 *
+ 	 * Need to be careful of the following enqueue/dequeue ordering
+ 	 * problem too
+ 	 *
+ 	 *	enqueue(taskA)
+ 	 *	// sched_uclamp_used gets enabled
+ 	 *	enqueue(taskB)
+ 	 *	dequeue(taskA)
+ 	 *	// Must not decrement bucket->tasks here
+ 	 *	dequeue(taskB)
+ 	 *
+ 	 * where we could end up with stale data in uc_se and
+ 	 * bucket[uc_se->bucket_id].
+ 	 *
+ 	 * The following check here eliminates the possibility of such race.
+ 	 */
+ 	if (unlikely(!uc_se->active))
+ 		return;
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 
+ 	SCHED_WARN_ON(!bucket->tasks);
+ 	if (likely(bucket->tasks))
+ 		bucket->tasks--;
+ 
+ 	uc_se->active = false;
+ 
+ 	/*
+ 	 * Keep "local max aggregation" simple and accept to (possibly)
+ 	 * overboost some RUNNABLE tasks in the same bucket.
+ 	 * The rq clamp bucket value is reset to its base value whenever
+ 	 * there are no more RUNNABLE tasks refcounting it.
+ 	 */
+ 	if (likely(bucket->tasks))
+ 		return;
+ 
+ 	rq_clamp = READ_ONCE(uc_rq->value);
+ 	/*
+ 	 * Defensive programming: this should never happen. If it happens,
+ 	 * e.g. due to future modification, warn and fixup the expected value.
+ 	 */
+ 	SCHED_WARN_ON(bucket->value > rq_clamp);
+ 	if (bucket->value >= rq_clamp) {
+ 		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
+ 		WRITE_ONCE(uc_rq->value, bkt_clamp);
+ 	}
+ }
+ 
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * Avoid any overhead until uclamp is actually used by the userspace.
+ 	 *
+ 	 * The condition is constructed such that a NOP is generated when
+ 	 * sched_uclamp_used is disabled.
+ 	 */
+ 	if (!static_branch_unlikely(&sched_uclamp_used))
+ 		return;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_inc_id(rq, p, clamp_id);
+ 
+ 	/* Reset clamp idle holding when there is one RUNNABLE task */
+ 	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
+ 		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
+ }
+ 
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * Avoid any overhead until uclamp is actually used by the userspace.
+ 	 *
+ 	 * The condition is constructed such that a NOP is generated when
+ 	 * sched_uclamp_used is disabled.
+ 	 */
+ 	if (!static_branch_unlikely(&sched_uclamp_used))
+ 		return;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_dec_id(rq, p, clamp_id);
+ }
+ 
+ static inline void
+ uclamp_update_active(struct task_struct *p, enum uclamp_id clamp_id)
+ {
+ 	struct rq_flags rf;
+ 	struct rq *rq;
+ 
+ 	/*
+ 	 * Lock the task and the rq where the task is (or was) queued.
+ 	 *
+ 	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the
+ 	 * price to pay to safely serialize util_{min,max} updates with
+ 	 * enqueues, dequeues and migration operations.
+ 	 * This is the same locking schema used by __set_cpus_allowed_ptr().
+ 	 */
+ 	rq = task_rq_lock(p, &rf);
+ 
+ 	/*
+ 	 * Setting the clamp bucket is serialized by task_rq_lock().
+ 	 * If the task is not yet RUNNABLE and its task_struct is not
+ 	 * affecting a valid clamp bucket, the next time it's enqueued,
+ 	 * it will already see the updated clamp bucket value.
+ 	 */
+ 	if (p->uclamp[clamp_id].active) {
+ 		uclamp_rq_dec_id(rq, p, clamp_id);
+ 		uclamp_rq_inc_id(rq, p, clamp_id);
+ 	}
+ 
+ 	task_rq_unlock(rq, p, &rf);
+ }
+ 
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ static inline void
+ uclamp_update_active_tasks(struct cgroup_subsys_state *css,
+ 			   unsigned int clamps)
+ {
+ 	enum uclamp_id clamp_id;
+ 	struct css_task_iter it;
+ 	struct task_struct *p;
+ 
+ 	css_task_iter_start(css, 0, &it);
+ 	while ((p = css_task_iter_next(&it))) {
+ 		for_each_clamp_id(clamp_id) {
+ 			if ((0x1 << clamp_id) & clamps)
+ 				uclamp_update_active(p, clamp_id);
+ 		}
+ 	}
+ 	css_task_iter_end(&it);
+ }
+ 
+ static void cpu_util_update_eff(struct cgroup_subsys_state *css);
+ static void uclamp_update_root_tg(void)
+ {
+ 	struct task_group *tg = &root_task_group;
+ 
+ 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],
+ 		      sysctl_sched_uclamp_util_min, false);
+ 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],
+ 		      sysctl_sched_uclamp_util_max, false);
+ 
+ 	rcu_read_lock();
+ 	cpu_util_update_eff(&root_task_group.css);
+ 	rcu_read_unlock();
+ }
+ #else
+ static void uclamp_update_root_tg(void) { }
+ #endif
+ 
+ int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
+ 				void *buffer, size_t *lenp, loff_t *ppos)
+ {
+ 	bool update_root_tg = false;
+ 	int old_min, old_max, old_min_rt;
+ 	int result;
+ 
+ 	mutex_lock(&uclamp_mutex);
+ 	old_min = sysctl_sched_uclamp_util_min;
+ 	old_max = sysctl_sched_uclamp_util_max;
+ 	old_min_rt = sysctl_sched_uclamp_util_min_rt_default;
+ 
+ 	result = proc_dointvec(table, write, buffer, lenp, ppos);
+ 	if (result)
+ 		goto undo;
+ 	if (!write)
+ 		goto done;
+ 
+ 	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
+ 	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE	||
+ 	    sysctl_sched_uclamp_util_min_rt_default > SCHED_CAPACITY_SCALE) {
+ 
+ 		result = -EINVAL;
+ 		goto undo;
+ 	}
+ 
+ 	if (old_min != sysctl_sched_uclamp_util_min) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MIN],
+ 			      sysctl_sched_uclamp_util_min, false);
+ 		update_root_tg = true;
+ 	}
+ 	if (old_max != sysctl_sched_uclamp_util_max) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MAX],
+ 			      sysctl_sched_uclamp_util_max, false);
+ 		update_root_tg = true;
+ 	}
+ 
+ 	if (update_root_tg) {
+ 		static_branch_enable(&sched_uclamp_used);
+ 		uclamp_update_root_tg();
+ 	}
+ 
+ 	if (old_min_rt != sysctl_sched_uclamp_util_min_rt_default) {
+ 		static_branch_enable(&sched_uclamp_used);
+ 		uclamp_sync_util_min_rt_default();
+ 	}
+ 
+ 	/*
+ 	 * We update all RUNNABLE tasks only when task groups are in use.
+ 	 * Otherwise, keep it simple and do just a lazy update at each next
+ 	 * task enqueue time.
+ 	 */
+ 
+ 	goto done;
+ 
+ undo:
+ 	sysctl_sched_uclamp_util_min = old_min;
+ 	sysctl_sched_uclamp_util_max = old_max;
+ 	sysctl_sched_uclamp_util_min_rt_default = old_min_rt;
+ done:
+ 	mutex_unlock(&uclamp_mutex);
+ 
+ 	return result;
+ }
+ 
+ static int uclamp_validate(struct task_struct *p,
+ 			   const struct sched_attr *attr)
+ {
+ 	unsigned int lower_bound = p->uclamp_req[UCLAMP_MIN].value;
+ 	unsigned int upper_bound = p->uclamp_req[UCLAMP_MAX].value;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN)
+ 		lower_bound = attr->sched_util_min;
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX)
+ 		upper_bound = attr->sched_util_max;
+ 
+ 	if (lower_bound > upper_bound)
+ 		return -EINVAL;
+ 	if (upper_bound > SCHED_CAPACITY_SCALE)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * We have valid uclamp attributes; make sure uclamp is enabled.
+ 	 *
+ 	 * We need to do that here, because enabling static branches is a
+ 	 * blocking operation which obviously cannot be done while holding
+ 	 * scheduler locks.
+ 	 */
+ 	static_branch_enable(&sched_uclamp_used);
+ 
+ 	return 0;
+ }
+ 
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * On scheduling class change, reset to default clamps for tasks
+ 	 * without a task-specific value.
+ 	 */
+ 	for_each_clamp_id(clamp_id) {
+ 		struct uclamp_se *uc_se = &p->uclamp_req[clamp_id];
+ 
+ 		/* Keep using defined clamps across class changes */
+ 		if (uc_se->user_defined)
+ 			continue;
+ 
+ 		/*
+ 		 * RT by default have a 100% boost value that could be modified
+ 		 * at runtime.
+ 		 */
+ 		if (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))
+ 			__uclamp_update_util_min_rt_default(p);
+ 		else
+ 			uclamp_se_set(uc_se, uclamp_none(clamp_id), false);
+ 
+ 	}
+ 
+ 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
+ 		return;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
+ 			      attr->sched_util_min, true);
+ 	}
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
+ 			      attr->sched_util_max, true);
+ 	}
+ }
+ 
+ static void uclamp_fork(struct task_struct *p)
+ {
+ 	enum uclamp_id clamp_id;
+ 
+ 	/*
+ 	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here
+ 	 * as the task is still at its early fork stages.
+ 	 */
+ 	for_each_clamp_id(clamp_id)
+ 		p->uclamp[clamp_id].active = false;
+ 
+ 	if (likely(!p->sched_reset_on_fork))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_se_set(&p->uclamp_req[clamp_id],
+ 			      uclamp_none(clamp_id), false);
+ 	}
+ }
+ 
+ static void uclamp_post_fork(struct task_struct *p)
+ {
+ 	uclamp_update_util_min_rt_default(p);
+ }
+ 
+ static void __init init_uclamp_rq(struct rq *rq)
+ {
+ 	enum uclamp_id clamp_id;
+ 	struct uclamp_rq *uc_rq = rq->uclamp;
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uc_rq[clamp_id] = (struct uclamp_rq) {
+ 			.value = uclamp_none(clamp_id)
+ 		};
+ 	}
+ 
+ 	rq->uclamp_flags = 0;
+ }
+ 
+ static void __init init_uclamp(void)
+ {
+ 	struct uclamp_se uc_max = {};
+ 	enum uclamp_id clamp_id;
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu)
+ 		init_uclamp_rq(cpu_rq(cpu));
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_se_set(&init_task.uclamp_req[clamp_id],
+ 			      uclamp_none(clamp_id), false);
+ 	}
+ 
+ 	/* System defaults allow max clamp values for both indexes */
+ 	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_default[clamp_id] = uc_max;
+ #ifdef CONFIG_UCLAMP_TASK_GROUP
+ 		root_task_group.uclamp_req[clamp_id] = uc_max;
+ 		root_task_group.uclamp[clamp_id] = uc_max;
+ #endif
+ 	}
+ }
+ 
+ #else /* CONFIG_UCLAMP_TASK */
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
+ static inline int uclamp_validate(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr) { }
+ static inline void uclamp_fork(struct task_struct *p) { }
+ static inline void uclamp_post_fork(struct task_struct *p) { }
+ static inline void init_uclamp(void) { }
+ #endif /* CONFIG_UCLAMP_TASK */
+ 
++>>>>>>> b19a888c1e9b (sched/core: Fix typos in comments)
  static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
  {
  	if (!(flags & ENQUEUE_NOCLOCK))
@@@ -3617,11 -4838,10 +4307,11 @@@ pick_next_task(struct rq *rq, struct ta
  	/*
  	 * Optimization: we know that if all tasks are in the fair class we can
  	 * call that function directly, but only if the @prev task wasn't of a
- 	 * higher scheduling class, because otherwise those loose the
+ 	 * higher scheduling class, because otherwise those lose the
  	 * opportunity to pull in more work from other CPUs.
  	 */
 -	if (likely(prev->sched_class <= &fair_sched_class &&
 +	if (likely((prev->sched_class == &idle_sched_class ||
 +		    prev->sched_class == &fair_sched_class) &&
  		   rq->nr_running == rq->cfs.h_nr_running)) {
  
  		p = pick_next_task_fair(rq, prev, rf);
* Unmerged path kernel/sched/core.c
