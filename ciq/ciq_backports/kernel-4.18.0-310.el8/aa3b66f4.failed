tick/sched: Make jiffies update quick check more robust

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit aa3b66f401b372598b29421bab4d17b631b92407
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/aa3b66f4.failed

The quick check in tick_do_update_jiffies64() whether jiffies need to be
updated is not really correct under all circumstances and on all
architectures, especially not on 32bit systems.

The quick check does:

    if (now < READ_ONCE(tick_next_period))
    	return;

and the counterpart in the update is:

    WRITE_ONCE(tick_next_period, next_update_time);

This has two problems:

  1) On weakly ordered architectures there is no guarantee that the stores
     before the WRITE_ONCE() are visible which means that other CPUs can
     operate on a stale jiffies value.

  2) On 32bit the store of tick_next_period which is an u64 is split into
     two 32bit stores. If the first 32bit store advances tick_next_period
     far out and the second 32bit store is delayed (virt, NMI ...) then
     jiffies will become stale until the second 32bit store happens.

Address this by seperating the handling for 32bit and 64bit.

On 64bit problem #1 is addressed by replacing READ_ONCE() / WRITE_ONCE()
with smp_load_acquire() / smp_store_release().

On 32bit problem #2 is addressed by protecting the quick check with the
jiffies sequence counter. The load and stores can be plain because the
sequence count mechanics provides the required barriers already.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
Link: https://lore.kernel.org/r/87czzpc02w.fsf@nanos.tec.linutronix.de

(cherry picked from commit aa3b66f401b372598b29421bab4d17b631b92407)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/time/tick-sched.c
diff --cc kernel/time/tick-sched.c
index 9b9a2081857d,a9e68936822d..000000000000
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@@ -56,44 -56,98 +56,139 @@@ static ktime_t last_jiffies_update
   */
  static void tick_do_update_jiffies64(ktime_t now)
  {
++<<<<<<< HEAD
 +	unsigned long ticks = 0;
 +	ktime_t delta;
 +
 +	/*
 +	 * Do a quick check without holding jiffies_lock:
 +	 */
 +	delta = ktime_sub(now, last_jiffies_update);
 +	if (delta < tick_period)
 +		return;
 +
 +	/* Reevaluate with jiffies_lock held */
 +	write_seqlock(&jiffies_lock);
 +
 +	delta = ktime_sub(now, last_jiffies_update);
 +	if (delta >= tick_period) {
 +
 +		delta = ktime_sub(delta, tick_period);
 +		last_jiffies_update = ktime_add(last_jiffies_update,
 +						tick_period);
 +
 +		/* Slow path for long timeouts */
 +		if (unlikely(delta >= tick_period)) {
 +			s64 incr = ktime_to_ns(tick_period);
 +
 +			ticks = ktime_divns(delta, incr);
 +
 +			last_jiffies_update = ktime_add_ns(last_jiffies_update,
 +							   incr * ticks);
 +		}
 +		do_timer(++ticks);
 +
 +		/* Keep the tick_next_period variable up to date */
 +		tick_next_period = ktime_add(last_jiffies_update, tick_period);
 +	} else {
 +		write_sequnlock(&jiffies_lock);
 +		return;
 +	}
 +	write_sequnlock(&jiffies_lock);
++=======
+ 	unsigned long ticks = 1;
+ 	ktime_t delta, nextp;
+ 
+ 	/*
+ 	 * 64bit can do a quick check without holding jiffies lock and
+ 	 * without looking at the sequence count. The smp_load_acquire()
+ 	 * pairs with the update done later in this function.
+ 	 *
+ 	 * 32bit cannot do that because the store of tick_next_period
+ 	 * consists of two 32bit stores and the first store could move it
+ 	 * to a random point in the future.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_64BIT)) {
+ 		if (ktime_before(now, smp_load_acquire(&tick_next_period)))
+ 			return;
+ 	} else {
+ 		unsigned int seq;
+ 
+ 		/*
+ 		 * Avoid contention on jiffies_lock and protect the quick
+ 		 * check with the sequence count.
+ 		 */
+ 		do {
+ 			seq = read_seqcount_begin(&jiffies_seq);
+ 			nextp = tick_next_period;
+ 		} while (read_seqcount_retry(&jiffies_seq, seq));
+ 
+ 		if (ktime_before(now, nextp))
+ 			return;
+ 	}
+ 
+ 	/* Quick check failed, i.e. update is required. */
+ 	raw_spin_lock(&jiffies_lock);
+ 	/*
+ 	 * Reevaluate with the lock held. Another CPU might have done the
+ 	 * update already.
+ 	 */
+ 	if (ktime_before(now, tick_next_period)) {
+ 		raw_spin_unlock(&jiffies_lock);
+ 		return;
+ 	}
+ 
+ 	write_seqcount_begin(&jiffies_seq);
+ 
+ 	delta = ktime_sub(now, tick_next_period);
+ 	if (unlikely(delta >= TICK_NSEC)) {
+ 		/* Slow path for long idle sleep times */
+ 		s64 incr = TICK_NSEC;
+ 
+ 		ticks += ktime_divns(delta, incr);
+ 
+ 		last_jiffies_update = ktime_add_ns(last_jiffies_update,
+ 						   incr * ticks);
+ 	} else {
+ 		last_jiffies_update = ktime_add_ns(last_jiffies_update,
+ 						   TICK_NSEC);
+ 	}
+ 
+ 	/* Advance jiffies to complete the jiffies_seq protected job */
+ 	jiffies_64 += ticks;
+ 
+ 	/*
+ 	 * Keep the tick_next_period variable up to date.
+ 	 */
+ 	nextp = ktime_add_ns(last_jiffies_update, TICK_NSEC);
+ 
+ 	if (IS_ENABLED(CONFIG_64BIT)) {
+ 		/*
+ 		 * Pairs with smp_load_acquire() in the lockless quick
+ 		 * check above and ensures that the update to jiffies_64 is
+ 		 * not reordered vs. the store to tick_next_period, neither
+ 		 * by the compiler nor by the CPU.
+ 		 */
+ 		smp_store_release(&tick_next_period, nextp);
+ 	} else {
+ 		/*
+ 		 * A plain store is good enough on 32bit as the quick check
+ 		 * above is protected by the sequence count.
+ 		 */
+ 		tick_next_period = nextp;
+ 	}
+ 
+ 	/*
+ 	 * Release the sequence count. calc_global_load() below is not
+ 	 * protected by it, but jiffies_lock needs to be held to prevent
+ 	 * concurrent invocations.
+ 	 */
+ 	write_seqcount_end(&jiffies_seq);
+ 
+ 	calc_global_load();
+ 
+ 	raw_spin_unlock(&jiffies_lock);
++>>>>>>> aa3b66f401b3 (tick/sched: Make jiffies update quick check more robust)
  	update_wall_time();
  }
  
* Unmerged path kernel/time/tick-sched.c
