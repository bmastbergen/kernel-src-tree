KVM: SVM: remove uneeded fields from host_save_users_msrs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Michael Roth <michael.roth@amd.com>
commit 553cc15f6e8d1467dc09a1fe6e51fcdea5f96471
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/553cc15f.failed

Now that the set of host user MSRs that need to be individually
saved/restored are the same with/without SEV-ES, we can drop the
.sev_es_restored flag and just iterate through the list unconditionally
for both cases. A subsequent patch can then move these loops to a
common path.

	Signed-off-by: Michael Roth <michael.roth@amd.com>
Message-Id: <20210202190126.2185715-3-michael.roth@amd.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 553cc15f6e8d1467dc09a1fe6e51fcdea5f96471)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/sev.c
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
diff --cc arch/x86/kvm/svm/sev.c
index ee83ab312c36,87167ef8ca23..000000000000
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@@ -1366,3 -1722,407 +1366,410 @@@ void pre_sev_run(struct vcpu_svm *svm, 
  	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
  	vmcb_mark_dirty(svm->vmcb, VMCB_ASID);
  }
++<<<<<<< HEAD
++=======
+ 
+ #define GHCB_SCRATCH_AREA_LIMIT		(16ULL * PAGE_SIZE)
+ static bool setup_vmgexit_scratch(struct vcpu_svm *svm, bool sync, u64 len)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	struct ghcb *ghcb = svm->ghcb;
+ 	u64 ghcb_scratch_beg, ghcb_scratch_end;
+ 	u64 scratch_gpa_beg, scratch_gpa_end;
+ 	void *scratch_va;
+ 
+ 	scratch_gpa_beg = ghcb_get_sw_scratch(ghcb);
+ 	if (!scratch_gpa_beg) {
+ 		pr_err("vmgexit: scratch gpa not provided\n");
+ 		return false;
+ 	}
+ 
+ 	scratch_gpa_end = scratch_gpa_beg + len;
+ 	if (scratch_gpa_end < scratch_gpa_beg) {
+ 		pr_err("vmgexit: scratch length (%#llx) not valid for scratch address (%#llx)\n",
+ 		       len, scratch_gpa_beg);
+ 		return false;
+ 	}
+ 
+ 	if ((scratch_gpa_beg & PAGE_MASK) == control->ghcb_gpa) {
+ 		/* Scratch area begins within GHCB */
+ 		ghcb_scratch_beg = control->ghcb_gpa +
+ 				   offsetof(struct ghcb, shared_buffer);
+ 		ghcb_scratch_end = control->ghcb_gpa +
+ 				   offsetof(struct ghcb, reserved_1);
+ 
+ 		/*
+ 		 * If the scratch area begins within the GHCB, it must be
+ 		 * completely contained in the GHCB shared buffer area.
+ 		 */
+ 		if (scratch_gpa_beg < ghcb_scratch_beg ||
+ 		    scratch_gpa_end > ghcb_scratch_end) {
+ 			pr_err("vmgexit: scratch area is outside of GHCB shared buffer area (%#llx - %#llx)\n",
+ 			       scratch_gpa_beg, scratch_gpa_end);
+ 			return false;
+ 		}
+ 
+ 		scratch_va = (void *)svm->ghcb;
+ 		scratch_va += (scratch_gpa_beg - control->ghcb_gpa);
+ 	} else {
+ 		/*
+ 		 * The guest memory must be read into a kernel buffer, so
+ 		 * limit the size
+ 		 */
+ 		if (len > GHCB_SCRATCH_AREA_LIMIT) {
+ 			pr_err("vmgexit: scratch area exceeds KVM limits (%#llx requested, %#llx limit)\n",
+ 			       len, GHCB_SCRATCH_AREA_LIMIT);
+ 			return false;
+ 		}
+ 		scratch_va = kzalloc(len, GFP_KERNEL);
+ 		if (!scratch_va)
+ 			return false;
+ 
+ 		if (kvm_read_guest(svm->vcpu.kvm, scratch_gpa_beg, scratch_va, len)) {
+ 			/* Unable to copy scratch area from guest */
+ 			pr_err("vmgexit: kvm_read_guest for scratch area failed\n");
+ 
+ 			kfree(scratch_va);
+ 			return false;
+ 		}
+ 
+ 		/*
+ 		 * The scratch area is outside the GHCB. The operation will
+ 		 * dictate whether the buffer needs to be synced before running
+ 		 * the vCPU next time (i.e. a read was requested so the data
+ 		 * must be written back to the guest memory).
+ 		 */
+ 		svm->ghcb_sa_sync = sync;
+ 		svm->ghcb_sa_free = true;
+ 	}
+ 
+ 	svm->ghcb_sa = scratch_va;
+ 	svm->ghcb_sa_len = len;
+ 
+ 	return true;
+ }
+ 
+ static void set_ghcb_msr_bits(struct vcpu_svm *svm, u64 value, u64 mask,
+ 			      unsigned int pos)
+ {
+ 	svm->vmcb->control.ghcb_gpa &= ~(mask << pos);
+ 	svm->vmcb->control.ghcb_gpa |= (value & mask) << pos;
+ }
+ 
+ static u64 get_ghcb_msr_bits(struct vcpu_svm *svm, u64 mask, unsigned int pos)
+ {
+ 	return (svm->vmcb->control.ghcb_gpa >> pos) & mask;
+ }
+ 
+ static void set_ghcb_msr(struct vcpu_svm *svm, u64 value)
+ {
+ 	svm->vmcb->control.ghcb_gpa = value;
+ }
+ 
+ static int sev_handle_vmgexit_msr_protocol(struct vcpu_svm *svm)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 	u64 ghcb_info;
+ 	int ret = 1;
+ 
+ 	ghcb_info = control->ghcb_gpa & GHCB_MSR_INFO_MASK;
+ 
+ 	trace_kvm_vmgexit_msr_protocol_enter(svm->vcpu.vcpu_id,
+ 					     control->ghcb_gpa);
+ 
+ 	switch (ghcb_info) {
+ 	case GHCB_MSR_SEV_INFO_REQ:
+ 		set_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,
+ 						    GHCB_VERSION_MIN,
+ 						    sev_enc_bit));
+ 		break;
+ 	case GHCB_MSR_CPUID_REQ: {
+ 		u64 cpuid_fn, cpuid_reg, cpuid_value;
+ 
+ 		cpuid_fn = get_ghcb_msr_bits(svm,
+ 					     GHCB_MSR_CPUID_FUNC_MASK,
+ 					     GHCB_MSR_CPUID_FUNC_POS);
+ 
+ 		/* Initialize the registers needed by the CPUID intercept */
+ 		vcpu->arch.regs[VCPU_REGS_RAX] = cpuid_fn;
+ 		vcpu->arch.regs[VCPU_REGS_RCX] = 0;
+ 
+ 		ret = svm_invoke_exit_handler(svm, SVM_EXIT_CPUID);
+ 		if (!ret) {
+ 			ret = -EINVAL;
+ 			break;
+ 		}
+ 
+ 		cpuid_reg = get_ghcb_msr_bits(svm,
+ 					      GHCB_MSR_CPUID_REG_MASK,
+ 					      GHCB_MSR_CPUID_REG_POS);
+ 		if (cpuid_reg == 0)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RAX];
+ 		else if (cpuid_reg == 1)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RBX];
+ 		else if (cpuid_reg == 2)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RCX];
+ 		else
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RDX];
+ 
+ 		set_ghcb_msr_bits(svm, cpuid_value,
+ 				  GHCB_MSR_CPUID_VALUE_MASK,
+ 				  GHCB_MSR_CPUID_VALUE_POS);
+ 
+ 		set_ghcb_msr_bits(svm, GHCB_MSR_CPUID_RESP,
+ 				  GHCB_MSR_INFO_MASK,
+ 				  GHCB_MSR_INFO_POS);
+ 		break;
+ 	}
+ 	case GHCB_MSR_TERM_REQ: {
+ 		u64 reason_set, reason_code;
+ 
+ 		reason_set = get_ghcb_msr_bits(svm,
+ 					       GHCB_MSR_TERM_REASON_SET_MASK,
+ 					       GHCB_MSR_TERM_REASON_SET_POS);
+ 		reason_code = get_ghcb_msr_bits(svm,
+ 						GHCB_MSR_TERM_REASON_MASK,
+ 						GHCB_MSR_TERM_REASON_POS);
+ 		pr_info("SEV-ES guest requested termination: %#llx:%#llx\n",
+ 			reason_set, reason_code);
+ 		fallthrough;
+ 	}
+ 	default:
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	trace_kvm_vmgexit_msr_protocol_exit(svm->vcpu.vcpu_id,
+ 					    control->ghcb_gpa, ret);
+ 
+ 	return ret;
+ }
+ 
+ int sev_handle_vmgexit(struct vcpu_svm *svm)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	u64 ghcb_gpa, exit_code;
+ 	struct ghcb *ghcb;
+ 	int ret;
+ 
+ 	/* Validate the GHCB */
+ 	ghcb_gpa = control->ghcb_gpa;
+ 	if (ghcb_gpa & GHCB_MSR_INFO_MASK)
+ 		return sev_handle_vmgexit_msr_protocol(svm);
+ 
+ 	if (!ghcb_gpa) {
+ 		vcpu_unimpl(&svm->vcpu, "vmgexit: GHCB gpa is not set\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (kvm_vcpu_map(&svm->vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->ghcb_map)) {
+ 		/* Unable to map GHCB from guest */
+ 		vcpu_unimpl(&svm->vcpu, "vmgexit: error mapping GHCB [%#llx] from guest\n",
+ 			    ghcb_gpa);
+ 		return -EINVAL;
+ 	}
+ 
+ 	svm->ghcb = svm->ghcb_map.hva;
+ 	ghcb = svm->ghcb_map.hva;
+ 
+ 	trace_kvm_vmgexit_enter(svm->vcpu.vcpu_id, ghcb);
+ 
+ 	exit_code = ghcb_get_sw_exit_code(ghcb);
+ 
+ 	ret = sev_es_validate_vmgexit(svm);
+ 	if (ret)
+ 		return ret;
+ 
+ 	sev_es_sync_from_ghcb(svm);
+ 	ghcb_set_sw_exit_info_1(ghcb, 0);
+ 	ghcb_set_sw_exit_info_2(ghcb, 0);
+ 
+ 	ret = -EINVAL;
+ 	switch (exit_code) {
+ 	case SVM_VMGEXIT_MMIO_READ:
+ 		if (!setup_vmgexit_scratch(svm, true, control->exit_info_2))
+ 			break;
+ 
+ 		ret = kvm_sev_es_mmio_read(&svm->vcpu,
+ 					   control->exit_info_1,
+ 					   control->exit_info_2,
+ 					   svm->ghcb_sa);
+ 		break;
+ 	case SVM_VMGEXIT_MMIO_WRITE:
+ 		if (!setup_vmgexit_scratch(svm, false, control->exit_info_2))
+ 			break;
+ 
+ 		ret = kvm_sev_es_mmio_write(&svm->vcpu,
+ 					    control->exit_info_1,
+ 					    control->exit_info_2,
+ 					    svm->ghcb_sa);
+ 		break;
+ 	case SVM_VMGEXIT_NMI_COMPLETE:
+ 		ret = svm_invoke_exit_handler(svm, SVM_EXIT_IRET);
+ 		break;
+ 	case SVM_VMGEXIT_AP_HLT_LOOP:
+ 		ret = kvm_emulate_ap_reset_hold(&svm->vcpu);
+ 		break;
+ 	case SVM_VMGEXIT_AP_JUMP_TABLE: {
+ 		struct kvm_sev_info *sev = &to_kvm_svm(svm->vcpu.kvm)->sev_info;
+ 
+ 		switch (control->exit_info_1) {
+ 		case 0:
+ 			/* Set AP jump table address */
+ 			sev->ap_jump_table = control->exit_info_2;
+ 			break;
+ 		case 1:
+ 			/* Get AP jump table address */
+ 			ghcb_set_sw_exit_info_2(ghcb, sev->ap_jump_table);
+ 			break;
+ 		default:
+ 			pr_err("svm: vmgexit: unsupported AP jump table request - exit_info_1=%#llx\n",
+ 			       control->exit_info_1);
+ 			ghcb_set_sw_exit_info_1(ghcb, 1);
+ 			ghcb_set_sw_exit_info_2(ghcb,
+ 						X86_TRAP_UD |
+ 						SVM_EVTINJ_TYPE_EXEPT |
+ 						SVM_EVTINJ_VALID);
+ 		}
+ 
+ 		ret = 1;
+ 		break;
+ 	}
+ 	case SVM_VMGEXIT_UNSUPPORTED_EVENT:
+ 		vcpu_unimpl(&svm->vcpu,
+ 			    "vmgexit: unsupported event - exit_info_1=%#llx, exit_info_2=%#llx\n",
+ 			    control->exit_info_1, control->exit_info_2);
+ 		break;
+ 	default:
+ 		ret = svm_invoke_exit_handler(svm, exit_code);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ int sev_es_string_io(struct vcpu_svm *svm, int size, unsigned int port, int in)
+ {
+ 	if (!setup_vmgexit_scratch(svm, in, svm->vmcb->control.exit_info_2))
+ 		return -EINVAL;
+ 
+ 	return kvm_sev_es_string_io(&svm->vcpu, size, port,
+ 				    svm->ghcb_sa, svm->ghcb_sa_len, in);
+ }
+ 
+ void sev_es_init_vmcb(struct vcpu_svm *svm)
+ {
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 
+ 	svm->vmcb->control.nested_ctl |= SVM_NESTED_CTL_SEV_ES_ENABLE;
+ 	svm->vmcb->control.virt_ext |= LBR_CTL_ENABLE_MASK;
+ 
+ 	/*
+ 	 * An SEV-ES guest requires a VMSA area that is a separate from the
+ 	 * VMCB page. Do not include the encryption mask on the VMSA physical
+ 	 * address since hardware will access it using the guest key.
+ 	 */
+ 	svm->vmcb->control.vmsa_pa = __pa(svm->vmsa);
+ 
+ 	/* Can't intercept CR register access, HV can't modify CR registers */
+ 	svm_clr_intercept(svm, INTERCEPT_CR0_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR4_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR8_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR0_WRITE);
+ 	svm_clr_intercept(svm, INTERCEPT_CR4_WRITE);
+ 	svm_clr_intercept(svm, INTERCEPT_CR8_WRITE);
+ 
+ 	svm_clr_intercept(svm, INTERCEPT_SELECTIVE_CR0);
+ 
+ 	/* Track EFER/CR register changes */
+ 	svm_set_intercept(svm, TRAP_EFER_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR0_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR4_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR8_WRITE);
+ 
+ 	/* No support for enable_vmware_backdoor */
+ 	clr_exception_intercept(svm, GP_VECTOR);
+ 
+ 	/* Can't intercept XSETBV, HV can't modify XCR0 directly */
+ 	svm_clr_intercept(svm, INTERCEPT_XSETBV);
+ 
+ 	/* Clear intercepts on selected MSRs */
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_EFER, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_CR_PAT, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);
+ }
+ 
+ void sev_es_create_vcpu(struct vcpu_svm *svm)
+ {
+ 	/*
+ 	 * Set the GHCB MSR value as per the GHCB specification when creating
+ 	 * a vCPU for an SEV-ES guest.
+ 	 */
+ 	set_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,
+ 					    GHCB_VERSION_MIN,
+ 					    sev_enc_bit));
+ }
+ 
+ void sev_es_vcpu_load(struct vcpu_svm *svm, int cpu)
+ {
+ 	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
+ 	struct vmcb_save_area *hostsa;
+ 	unsigned int i;
+ 
+ 	/*
+ 	 * As an SEV-ES guest, hardware will restore the host state on VMEXIT,
+ 	 * of which one step is to perform a VMLOAD. Since hardware does not
+ 	 * perform a VMSAVE on VMRUN, the host savearea must be updated.
+ 	 */
+ 	vmsave(__sme_page_pa(sd->save_area));
+ 
+ 	/*
+ 	 * Certain MSRs are restored on VMEXIT, only save ones that aren't
+ 	 * restored.
+ 	 */
+ 	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
+ 		rdmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
+ 
+ 	/* XCR0 is restored on VMEXIT, save the current host value */
+ 	hostsa = (struct vmcb_save_area *)(page_address(sd->save_area) + 0x400);
+ 	hostsa->xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
+ 
+ 	/* PKRU is restored on VMEXIT, save the curent host value */
+ 	hostsa->pkru = read_pkru();
+ 
+ 	/* MSR_IA32_XSS is restored on VMEXIT, save the currnet host value */
+ 	hostsa->xss = host_xss;
+ }
+ 
+ void sev_es_vcpu_put(struct vcpu_svm *svm)
+ {
+ 	unsigned int i;
+ 
+ 	/*
+ 	 * Certain MSRs are restored on VMEXIT and were saved with vmsave in
+ 	 * sev_es_vcpu_load() above. Only restore ones that weren't.
+ 	 */
+ 	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
+ 		wrmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
+ }
+ 
+ void sev_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	/* First SIPI: Use the values as initially set by the VMM */
+ 	if (!svm->received_first_sipi) {
+ 		svm->received_first_sipi = true;
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Subsequent SIPI: Return from an AP Reset Hold VMGEXIT, where
+ 	 * the guest will set the CS and RIP. Set SW_EXIT_INFO_2 to a
+ 	 * non-zero value.
+ 	 */
+ 	ghcb_set_sw_exit_info_2(svm->ghcb, 1);
+ }
++>>>>>>> 553cc15f6e8d (KVM: SVM: remove uneeded fields from host_save_users_msrs)
diff --cc arch/x86/kvm/svm/svm.c
index eca4258a8e0e,8b2cbcb50239..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -1393,15 -1424,14 +1393,23 @@@ static void svm_vcpu_load(struct kvm_vc
  		vmcb_mark_all_dirty(svm->vmcb);
  	}
  
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_64
 +	rdmsrl(MSR_GS_BASE, to_svm(vcpu)->host.gs_base);
 +#endif
 +	savesegment(fs, svm->host.fs);
 +	savesegment(gs, svm->host.gs);
 +	svm->host.ldt = kvm_read_ldt();
++=======
+ 	if (sev_es_guest(svm->vcpu.kvm)) {
+ 		sev_es_vcpu_load(svm, cpu);
+ 	} else {
+ 		for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
+ 			rdmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
++>>>>>>> 553cc15f6e8d (KVM: SVM: remove uneeded fields from host_save_users_msrs)
  
 -		vmsave(__sme_page_pa(sd->save_area));
 -	}
 +	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
 +		rdmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
  
  	if (static_cpu_has(X86_FEATURE_TSCRATEMSR)) {
  		u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
@@@ -1429,18 -1459,12 +1437,27 @@@ static void svm_vcpu_put(struct kvm_vcp
  	avic_vcpu_put(vcpu);
  
  	++vcpu->stat.host_state_reload;
++<<<<<<< HEAD
 +	kvm_load_ldt(svm->host.ldt);
 +#ifdef CONFIG_X86_64
 +	loadsegment(fs, svm->host.fs);
 +	wrmsrl(MSR_KERNEL_GS_BASE, current->thread.gsbase);
 +	load_gs_index(svm->host.gs);
 +#else
 +#ifdef CONFIG_X86_32_LAZY_GS
 +	loadsegment(gs, svm->host.gs);
 +#endif
 +#endif
 +	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
 +		wrmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
++=======
+ 	if (sev_es_guest(svm->vcpu.kvm)) {
+ 		sev_es_vcpu_put(svm);
+ 	} else {
+ 		for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)
+ 			wrmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);
+ 	}
++>>>>>>> 553cc15f6e8d (KVM: SVM: remove uneeded fields from host_save_users_msrs)
  }
  
  static unsigned long svm_get_rflags(struct kvm_vcpu *vcpu)
diff --cc arch/x86/kvm/svm/svm.h
index de600f536464,42c1faaddd17..000000000000
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@@ -23,17 -24,11 +23,20 @@@
  #define __sme_page_pa(x) __sme_set(page_to_pfn(x) << PAGE_SHIFT)
  
  static const u32 host_save_user_msrs[] = {
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_64
 +	MSR_STAR, MSR_LSTAR, MSR_CSTAR, MSR_SYSCALL_MASK, MSR_KERNEL_GS_BASE,
 +	MSR_FS_BASE,
 +#endif
 +	MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,
++=======
++>>>>>>> 553cc15f6e8d (KVM: SVM: remove uneeded fields from host_save_users_msrs)
  	MSR_TSC_AUX,
  };
 +
  #define NR_HOST_SAVE_USER_MSRS ARRAY_SIZE(host_save_user_msrs)
  
 -#define MAX_DIRECT_ACCESS_MSRS	18
 +#define MAX_DIRECT_ACCESS_MSRS	15
  #define MSRPM_OFFSETS	16
  extern u32 msrpm_offsets[MSRPM_OFFSETS] __read_mostly;
  extern bool npt_enabled;
* Unmerged path arch/x86/kvm/svm/sev.c
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
