KVM: SVM: Add AP_JUMP_TABLE support in prep for AP booting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 8640ca588b032166d6be6b4d3632d565d6d88e89
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/8640ca58.failed

The GHCB specification requires the hypervisor to save the address of an
AP Jump Table so that, for example, vCPUs that have been parked by UEFI
can be started by the OS. Provide support for the AP Jump Table set/get
exit code.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8640ca588b032166d6be6b4d3632d565d6d88e89)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/sev.c
diff --cc arch/x86/kvm/svm/sev.c
index 035bbd071872,8b5ef0fe4490..000000000000
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@@ -14,10 -14,20 +14,12 @@@
  #include <linux/psp-sev.h>
  #include <linux/pagemap.h>
  #include <linux/swap.h>
 -#include <linux/processor.h>
 -#include <linux/trace_events.h>
 -#include <asm/fpu/internal.h>
  
+ #include <asm/trapnr.h>
+ 
  #include "x86.h"
  #include "svm.h"
 -#include "cpuid.h"
 -#include "trace.h"
  
 -#define __ex(x) __kvm_handle_fault_on_reboot(x)
 -
 -static u8 sev_enc_bit;
  static int sev_flush_asids(void);
  static DECLARE_RWSEM(sev_deactivate_lock);
  static DEFINE_MUTEX(sev_bitmap_lock);
@@@ -1268,6 -1309,319 +1270,322 @@@ void sev_hardware_teardown(void
  	sev_flush_asids();
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Pages used by hardware to hold guest encrypted state must be flushed before
+  * returning them to the system.
+  */
+ static void sev_flush_guest_memory(struct vcpu_svm *svm, void *va,
+ 				   unsigned long len)
+ {
+ 	/*
+ 	 * If hardware enforced cache coherency for encrypted mappings of the
+ 	 * same physical page is supported, nothing to do.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_SME_COHERENT))
+ 		return;
+ 
+ 	/*
+ 	 * If the VM Page Flush MSR is supported, use it to flush the page
+ 	 * (using the page virtual address and the guest ASID).
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_VM_PAGE_FLUSH)) {
+ 		struct kvm_sev_info *sev;
+ 		unsigned long va_start;
+ 		u64 start, stop;
+ 
+ 		/* Align start and stop to page boundaries. */
+ 		va_start = (unsigned long)va;
+ 		start = (u64)va_start & PAGE_MASK;
+ 		stop = PAGE_ALIGN((u64)va_start + len);
+ 
+ 		if (start < stop) {
+ 			sev = &to_kvm_svm(svm->vcpu.kvm)->sev_info;
+ 
+ 			while (start < stop) {
+ 				wrmsrl(MSR_AMD64_VM_PAGE_FLUSH,
+ 				       start | sev->asid);
+ 
+ 				start += PAGE_SIZE;
+ 			}
+ 
+ 			return;
+ 		}
+ 
+ 		WARN(1, "Address overflow, using WBINVD\n");
+ 	}
+ 
+ 	/*
+ 	 * Hardware should always have one of the above features,
+ 	 * but if not, use WBINVD and issue a warning.
+ 	 */
+ 	WARN_ONCE(1, "Using WBINVD to flush guest memory\n");
+ 	wbinvd_on_all_cpus();
+ }
+ 
+ void sev_free_vcpu(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm;
+ 
+ 	if (!sev_es_guest(vcpu->kvm))
+ 		return;
+ 
+ 	svm = to_svm(vcpu);
+ 
+ 	if (vcpu->arch.guest_state_protected)
+ 		sev_flush_guest_memory(svm, svm->vmsa, PAGE_SIZE);
+ 	__free_page(virt_to_page(svm->vmsa));
+ 
+ 	if (svm->ghcb_sa_free)
+ 		kfree(svm->ghcb_sa);
+ }
+ 
+ static void dump_ghcb(struct vcpu_svm *svm)
+ {
+ 	struct ghcb *ghcb = svm->ghcb;
+ 	unsigned int nbits;
+ 
+ 	/* Re-use the dump_invalid_vmcb module parameter */
+ 	if (!dump_invalid_vmcb) {
+ 		pr_warn_ratelimited("set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\n");
+ 		return;
+ 	}
+ 
+ 	nbits = sizeof(ghcb->save.valid_bitmap) * 8;
+ 
+ 	pr_err("GHCB (GPA=%016llx):\n", svm->vmcb->control.ghcb_gpa);
+ 	pr_err("%-20s%016llx is_valid: %u\n", "sw_exit_code",
+ 	       ghcb->save.sw_exit_code, ghcb_sw_exit_code_is_valid(ghcb));
+ 	pr_err("%-20s%016llx is_valid: %u\n", "sw_exit_info_1",
+ 	       ghcb->save.sw_exit_info_1, ghcb_sw_exit_info_1_is_valid(ghcb));
+ 	pr_err("%-20s%016llx is_valid: %u\n", "sw_exit_info_2",
+ 	       ghcb->save.sw_exit_info_2, ghcb_sw_exit_info_2_is_valid(ghcb));
+ 	pr_err("%-20s%016llx is_valid: %u\n", "sw_scratch",
+ 	       ghcb->save.sw_scratch, ghcb_sw_scratch_is_valid(ghcb));
+ 	pr_err("%-20s%*pb\n", "valid_bitmap", nbits, ghcb->save.valid_bitmap);
+ }
+ 
+ static void sev_es_sync_to_ghcb(struct vcpu_svm *svm)
+ {
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 	struct ghcb *ghcb = svm->ghcb;
+ 
+ 	/*
+ 	 * The GHCB protocol so far allows for the following data
+ 	 * to be returned:
+ 	 *   GPRs RAX, RBX, RCX, RDX
+ 	 *
+ 	 * Copy their values to the GHCB if they are dirty.
+ 	 */
+ 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RAX))
+ 		ghcb_set_rax(ghcb, vcpu->arch.regs[VCPU_REGS_RAX]);
+ 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RBX))
+ 		ghcb_set_rbx(ghcb, vcpu->arch.regs[VCPU_REGS_RBX]);
+ 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RCX))
+ 		ghcb_set_rcx(ghcb, vcpu->arch.regs[VCPU_REGS_RCX]);
+ 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RDX))
+ 		ghcb_set_rdx(ghcb, vcpu->arch.regs[VCPU_REGS_RDX]);
+ }
+ 
+ static void sev_es_sync_from_ghcb(struct vcpu_svm *svm)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 	struct ghcb *ghcb = svm->ghcb;
+ 	u64 exit_code;
+ 
+ 	/*
+ 	 * The GHCB protocol so far allows for the following data
+ 	 * to be supplied:
+ 	 *   GPRs RAX, RBX, RCX, RDX
+ 	 *   XCR0
+ 	 *   CPL
+ 	 *
+ 	 * VMMCALL allows the guest to provide extra registers. KVM also
+ 	 * expects RSI for hypercalls, so include that, too.
+ 	 *
+ 	 * Copy their values to the appropriate location if supplied.
+ 	 */
+ 	memset(vcpu->arch.regs, 0, sizeof(vcpu->arch.regs));
+ 
+ 	vcpu->arch.regs[VCPU_REGS_RAX] = ghcb_get_rax_if_valid(ghcb);
+ 	vcpu->arch.regs[VCPU_REGS_RBX] = ghcb_get_rbx_if_valid(ghcb);
+ 	vcpu->arch.regs[VCPU_REGS_RCX] = ghcb_get_rcx_if_valid(ghcb);
+ 	vcpu->arch.regs[VCPU_REGS_RDX] = ghcb_get_rdx_if_valid(ghcb);
+ 	vcpu->arch.regs[VCPU_REGS_RSI] = ghcb_get_rsi_if_valid(ghcb);
+ 
+ 	svm->vmcb->save.cpl = ghcb_get_cpl_if_valid(ghcb);
+ 
+ 	if (ghcb_xcr0_is_valid(ghcb)) {
+ 		vcpu->arch.xcr0 = ghcb_get_xcr0(ghcb);
+ 		kvm_update_cpuid_runtime(vcpu);
+ 	}
+ 
+ 	/* Copy the GHCB exit information into the VMCB fields */
+ 	exit_code = ghcb_get_sw_exit_code(ghcb);
+ 	control->exit_code = lower_32_bits(exit_code);
+ 	control->exit_code_hi = upper_32_bits(exit_code);
+ 	control->exit_info_1 = ghcb_get_sw_exit_info_1(ghcb);
+ 	control->exit_info_2 = ghcb_get_sw_exit_info_2(ghcb);
+ 
+ 	/* Clear the valid entries fields */
+ 	memset(ghcb->save.valid_bitmap, 0, sizeof(ghcb->save.valid_bitmap));
+ }
+ 
+ static int sev_es_validate_vmgexit(struct vcpu_svm *svm)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	struct ghcb *ghcb;
+ 	u64 exit_code = 0;
+ 
+ 	ghcb = svm->ghcb;
+ 
+ 	/* Only GHCB Usage code 0 is supported */
+ 	if (ghcb->ghcb_usage)
+ 		goto vmgexit_err;
+ 
+ 	/*
+ 	 * Retrieve the exit code now even though is may not be marked valid
+ 	 * as it could help with debugging.
+ 	 */
+ 	exit_code = ghcb_get_sw_exit_code(ghcb);
+ 
+ 	if (!ghcb_sw_exit_code_is_valid(ghcb) ||
+ 	    !ghcb_sw_exit_info_1_is_valid(ghcb) ||
+ 	    !ghcb_sw_exit_info_2_is_valid(ghcb))
+ 		goto vmgexit_err;
+ 
+ 	switch (ghcb_get_sw_exit_code(ghcb)) {
+ 	case SVM_EXIT_READ_DR7:
+ 		break;
+ 	case SVM_EXIT_WRITE_DR7:
+ 		if (!ghcb_rax_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_EXIT_RDTSC:
+ 		break;
+ 	case SVM_EXIT_RDPMC:
+ 		if (!ghcb_rcx_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_EXIT_CPUID:
+ 		if (!ghcb_rax_is_valid(ghcb) ||
+ 		    !ghcb_rcx_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		if (ghcb_get_rax(ghcb) == 0xd)
+ 			if (!ghcb_xcr0_is_valid(ghcb))
+ 				goto vmgexit_err;
+ 		break;
+ 	case SVM_EXIT_INVD:
+ 		break;
+ 	case SVM_EXIT_IOIO:
+ 		if (ghcb_get_sw_exit_info_1(ghcb) & SVM_IOIO_STR_MASK) {
+ 			if (!ghcb_sw_scratch_is_valid(ghcb))
+ 				goto vmgexit_err;
+ 		} else {
+ 			if (!(ghcb_get_sw_exit_info_1(ghcb) & SVM_IOIO_TYPE_MASK))
+ 				if (!ghcb_rax_is_valid(ghcb))
+ 					goto vmgexit_err;
+ 		}
+ 		break;
+ 	case SVM_EXIT_MSR:
+ 		if (!ghcb_rcx_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		if (ghcb_get_sw_exit_info_1(ghcb)) {
+ 			if (!ghcb_rax_is_valid(ghcb) ||
+ 			    !ghcb_rdx_is_valid(ghcb))
+ 				goto vmgexit_err;
+ 		}
+ 		break;
+ 	case SVM_EXIT_VMMCALL:
+ 		if (!ghcb_rax_is_valid(ghcb) ||
+ 		    !ghcb_cpl_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_EXIT_RDTSCP:
+ 		break;
+ 	case SVM_EXIT_WBINVD:
+ 		break;
+ 	case SVM_EXIT_MONITOR:
+ 		if (!ghcb_rax_is_valid(ghcb) ||
+ 		    !ghcb_rcx_is_valid(ghcb) ||
+ 		    !ghcb_rdx_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_EXIT_MWAIT:
+ 		if (!ghcb_rax_is_valid(ghcb) ||
+ 		    !ghcb_rcx_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_VMGEXIT_MMIO_READ:
+ 	case SVM_VMGEXIT_MMIO_WRITE:
+ 		if (!ghcb_sw_scratch_is_valid(ghcb))
+ 			goto vmgexit_err;
+ 		break;
+ 	case SVM_VMGEXIT_NMI_COMPLETE:
+ 	case SVM_VMGEXIT_AP_JUMP_TABLE:
+ 	case SVM_VMGEXIT_UNSUPPORTED_EVENT:
+ 		break;
+ 	default:
+ 		goto vmgexit_err;
+ 	}
+ 
+ 	return 0;
+ 
+ vmgexit_err:
+ 	vcpu = &svm->vcpu;
+ 
+ 	if (ghcb->ghcb_usage) {
+ 		vcpu_unimpl(vcpu, "vmgexit: ghcb usage %#x is not valid\n",
+ 			    ghcb->ghcb_usage);
+ 	} else {
+ 		vcpu_unimpl(vcpu, "vmgexit: exit reason %#llx is not valid\n",
+ 			    exit_code);
+ 		dump_ghcb(svm);
+ 	}
+ 
+ 	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ 	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ 	vcpu->run->internal.ndata = 2;
+ 	vcpu->run->internal.data[0] = exit_code;
+ 	vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
+ 
+ 	return -EINVAL;
+ }
+ 
+ static void pre_sev_es_run(struct vcpu_svm *svm)
+ {
+ 	if (!svm->ghcb)
+ 		return;
+ 
+ 	if (svm->ghcb_sa_free) {
+ 		/*
+ 		 * The scratch area lives outside the GHCB, so there is a
+ 		 * buffer that, depending on the operation performed, may
+ 		 * need to be synced, then freed.
+ 		 */
+ 		if (svm->ghcb_sa_sync) {
+ 			kvm_write_guest(svm->vcpu.kvm,
+ 					ghcb_get_sw_scratch(svm->ghcb),
+ 					svm->ghcb_sa, svm->ghcb_sa_len);
+ 			svm->ghcb_sa_sync = false;
+ 		}
+ 
+ 		kfree(svm->ghcb_sa);
+ 		svm->ghcb_sa = NULL;
+ 		svm->ghcb_sa_free = false;
+ 	}
+ 
+ 	trace_kvm_vmgexit_exit(svm->vcpu.vcpu_id, svm->ghcb);
+ 
+ 	sev_es_sync_to_ghcb(svm);
+ 
+ 	kvm_vcpu_unmap(&svm->vcpu, &svm->ghcb_map, true);
+ 	svm->ghcb = NULL;
+ }
+ 
++>>>>>>> 8640ca588b03 (KVM: SVM: Add AP_JUMP_TABLE support in prep for AP booting)
  void pre_sev_run(struct vcpu_svm *svm, int cpu)
  {
  	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
@@@ -1290,3 -1647,394 +1608,397 @@@
  	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
  	vmcb_mark_dirty(svm->vmcb, VMCB_ASID);
  }
++<<<<<<< HEAD
++=======
+ 
+ #define GHCB_SCRATCH_AREA_LIMIT		(16ULL * PAGE_SIZE)
+ static bool setup_vmgexit_scratch(struct vcpu_svm *svm, bool sync, u64 len)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	struct ghcb *ghcb = svm->ghcb;
+ 	u64 ghcb_scratch_beg, ghcb_scratch_end;
+ 	u64 scratch_gpa_beg, scratch_gpa_end;
+ 	void *scratch_va;
+ 
+ 	scratch_gpa_beg = ghcb_get_sw_scratch(ghcb);
+ 	if (!scratch_gpa_beg) {
+ 		pr_err("vmgexit: scratch gpa not provided\n");
+ 		return false;
+ 	}
+ 
+ 	scratch_gpa_end = scratch_gpa_beg + len;
+ 	if (scratch_gpa_end < scratch_gpa_beg) {
+ 		pr_err("vmgexit: scratch length (%#llx) not valid for scratch address (%#llx)\n",
+ 		       len, scratch_gpa_beg);
+ 		return false;
+ 	}
+ 
+ 	if ((scratch_gpa_beg & PAGE_MASK) == control->ghcb_gpa) {
+ 		/* Scratch area begins within GHCB */
+ 		ghcb_scratch_beg = control->ghcb_gpa +
+ 				   offsetof(struct ghcb, shared_buffer);
+ 		ghcb_scratch_end = control->ghcb_gpa +
+ 				   offsetof(struct ghcb, reserved_1);
+ 
+ 		/*
+ 		 * If the scratch area begins within the GHCB, it must be
+ 		 * completely contained in the GHCB shared buffer area.
+ 		 */
+ 		if (scratch_gpa_beg < ghcb_scratch_beg ||
+ 		    scratch_gpa_end > ghcb_scratch_end) {
+ 			pr_err("vmgexit: scratch area is outside of GHCB shared buffer area (%#llx - %#llx)\n",
+ 			       scratch_gpa_beg, scratch_gpa_end);
+ 			return false;
+ 		}
+ 
+ 		scratch_va = (void *)svm->ghcb;
+ 		scratch_va += (scratch_gpa_beg - control->ghcb_gpa);
+ 	} else {
+ 		/*
+ 		 * The guest memory must be read into a kernel buffer, so
+ 		 * limit the size
+ 		 */
+ 		if (len > GHCB_SCRATCH_AREA_LIMIT) {
+ 			pr_err("vmgexit: scratch area exceeds KVM limits (%#llx requested, %#llx limit)\n",
+ 			       len, GHCB_SCRATCH_AREA_LIMIT);
+ 			return false;
+ 		}
+ 		scratch_va = kzalloc(len, GFP_KERNEL);
+ 		if (!scratch_va)
+ 			return false;
+ 
+ 		if (kvm_read_guest(svm->vcpu.kvm, scratch_gpa_beg, scratch_va, len)) {
+ 			/* Unable to copy scratch area from guest */
+ 			pr_err("vmgexit: kvm_read_guest for scratch area failed\n");
+ 
+ 			kfree(scratch_va);
+ 			return false;
+ 		}
+ 
+ 		/*
+ 		 * The scratch area is outside the GHCB. The operation will
+ 		 * dictate whether the buffer needs to be synced before running
+ 		 * the vCPU next time (i.e. a read was requested so the data
+ 		 * must be written back to the guest memory).
+ 		 */
+ 		svm->ghcb_sa_sync = sync;
+ 		svm->ghcb_sa_free = true;
+ 	}
+ 
+ 	svm->ghcb_sa = scratch_va;
+ 	svm->ghcb_sa_len = len;
+ 
+ 	return true;
+ }
+ 
+ static void set_ghcb_msr_bits(struct vcpu_svm *svm, u64 value, u64 mask,
+ 			      unsigned int pos)
+ {
+ 	svm->vmcb->control.ghcb_gpa &= ~(mask << pos);
+ 	svm->vmcb->control.ghcb_gpa |= (value & mask) << pos;
+ }
+ 
+ static u64 get_ghcb_msr_bits(struct vcpu_svm *svm, u64 mask, unsigned int pos)
+ {
+ 	return (svm->vmcb->control.ghcb_gpa >> pos) & mask;
+ }
+ 
+ static void set_ghcb_msr(struct vcpu_svm *svm, u64 value)
+ {
+ 	svm->vmcb->control.ghcb_gpa = value;
+ }
+ 
+ static int sev_handle_vmgexit_msr_protocol(struct vcpu_svm *svm)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 	u64 ghcb_info;
+ 	int ret = 1;
+ 
+ 	ghcb_info = control->ghcb_gpa & GHCB_MSR_INFO_MASK;
+ 
+ 	trace_kvm_vmgexit_msr_protocol_enter(svm->vcpu.vcpu_id,
+ 					     control->ghcb_gpa);
+ 
+ 	switch (ghcb_info) {
+ 	case GHCB_MSR_SEV_INFO_REQ:
+ 		set_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,
+ 						    GHCB_VERSION_MIN,
+ 						    sev_enc_bit));
+ 		break;
+ 	case GHCB_MSR_CPUID_REQ: {
+ 		u64 cpuid_fn, cpuid_reg, cpuid_value;
+ 
+ 		cpuid_fn = get_ghcb_msr_bits(svm,
+ 					     GHCB_MSR_CPUID_FUNC_MASK,
+ 					     GHCB_MSR_CPUID_FUNC_POS);
+ 
+ 		/* Initialize the registers needed by the CPUID intercept */
+ 		vcpu->arch.regs[VCPU_REGS_RAX] = cpuid_fn;
+ 		vcpu->arch.regs[VCPU_REGS_RCX] = 0;
+ 
+ 		ret = svm_invoke_exit_handler(svm, SVM_EXIT_CPUID);
+ 		if (!ret) {
+ 			ret = -EINVAL;
+ 			break;
+ 		}
+ 
+ 		cpuid_reg = get_ghcb_msr_bits(svm,
+ 					      GHCB_MSR_CPUID_REG_MASK,
+ 					      GHCB_MSR_CPUID_REG_POS);
+ 		if (cpuid_reg == 0)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RAX];
+ 		else if (cpuid_reg == 1)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RBX];
+ 		else if (cpuid_reg == 2)
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RCX];
+ 		else
+ 			cpuid_value = vcpu->arch.regs[VCPU_REGS_RDX];
+ 
+ 		set_ghcb_msr_bits(svm, cpuid_value,
+ 				  GHCB_MSR_CPUID_VALUE_MASK,
+ 				  GHCB_MSR_CPUID_VALUE_POS);
+ 
+ 		set_ghcb_msr_bits(svm, GHCB_MSR_CPUID_RESP,
+ 				  GHCB_MSR_INFO_MASK,
+ 				  GHCB_MSR_INFO_POS);
+ 		break;
+ 	}
+ 	case GHCB_MSR_TERM_REQ: {
+ 		u64 reason_set, reason_code;
+ 
+ 		reason_set = get_ghcb_msr_bits(svm,
+ 					       GHCB_MSR_TERM_REASON_SET_MASK,
+ 					       GHCB_MSR_TERM_REASON_SET_POS);
+ 		reason_code = get_ghcb_msr_bits(svm,
+ 						GHCB_MSR_TERM_REASON_MASK,
+ 						GHCB_MSR_TERM_REASON_POS);
+ 		pr_info("SEV-ES guest requested termination: %#llx:%#llx\n",
+ 			reason_set, reason_code);
+ 		fallthrough;
+ 	}
+ 	default:
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	trace_kvm_vmgexit_msr_protocol_exit(svm->vcpu.vcpu_id,
+ 					    control->ghcb_gpa, ret);
+ 
+ 	return ret;
+ }
+ 
+ int sev_handle_vmgexit(struct vcpu_svm *svm)
+ {
+ 	struct vmcb_control_area *control = &svm->vmcb->control;
+ 	u64 ghcb_gpa, exit_code;
+ 	struct ghcb *ghcb;
+ 	int ret;
+ 
+ 	/* Validate the GHCB */
+ 	ghcb_gpa = control->ghcb_gpa;
+ 	if (ghcb_gpa & GHCB_MSR_INFO_MASK)
+ 		return sev_handle_vmgexit_msr_protocol(svm);
+ 
+ 	if (!ghcb_gpa) {
+ 		vcpu_unimpl(&svm->vcpu, "vmgexit: GHCB gpa is not set\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (kvm_vcpu_map(&svm->vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->ghcb_map)) {
+ 		/* Unable to map GHCB from guest */
+ 		vcpu_unimpl(&svm->vcpu, "vmgexit: error mapping GHCB [%#llx] from guest\n",
+ 			    ghcb_gpa);
+ 		return -EINVAL;
+ 	}
+ 
+ 	svm->ghcb = svm->ghcb_map.hva;
+ 	ghcb = svm->ghcb_map.hva;
+ 
+ 	trace_kvm_vmgexit_enter(svm->vcpu.vcpu_id, ghcb);
+ 
+ 	exit_code = ghcb_get_sw_exit_code(ghcb);
+ 
+ 	ret = sev_es_validate_vmgexit(svm);
+ 	if (ret)
+ 		return ret;
+ 
+ 	sev_es_sync_from_ghcb(svm);
+ 	ghcb_set_sw_exit_info_1(ghcb, 0);
+ 	ghcb_set_sw_exit_info_2(ghcb, 0);
+ 
+ 	ret = -EINVAL;
+ 	switch (exit_code) {
+ 	case SVM_VMGEXIT_MMIO_READ:
+ 		if (!setup_vmgexit_scratch(svm, true, control->exit_info_2))
+ 			break;
+ 
+ 		ret = kvm_sev_es_mmio_read(&svm->vcpu,
+ 					   control->exit_info_1,
+ 					   control->exit_info_2,
+ 					   svm->ghcb_sa);
+ 		break;
+ 	case SVM_VMGEXIT_MMIO_WRITE:
+ 		if (!setup_vmgexit_scratch(svm, false, control->exit_info_2))
+ 			break;
+ 
+ 		ret = kvm_sev_es_mmio_write(&svm->vcpu,
+ 					    control->exit_info_1,
+ 					    control->exit_info_2,
+ 					    svm->ghcb_sa);
+ 		break;
+ 	case SVM_VMGEXIT_NMI_COMPLETE:
+ 		ret = svm_invoke_exit_handler(svm, SVM_EXIT_IRET);
+ 		break;
+ 	case SVM_VMGEXIT_AP_JUMP_TABLE: {
+ 		struct kvm_sev_info *sev = &to_kvm_svm(svm->vcpu.kvm)->sev_info;
+ 
+ 		switch (control->exit_info_1) {
+ 		case 0:
+ 			/* Set AP jump table address */
+ 			sev->ap_jump_table = control->exit_info_2;
+ 			break;
+ 		case 1:
+ 			/* Get AP jump table address */
+ 			ghcb_set_sw_exit_info_2(ghcb, sev->ap_jump_table);
+ 			break;
+ 		default:
+ 			pr_err("svm: vmgexit: unsupported AP jump table request - exit_info_1=%#llx\n",
+ 			       control->exit_info_1);
+ 			ghcb_set_sw_exit_info_1(ghcb, 1);
+ 			ghcb_set_sw_exit_info_2(ghcb,
+ 						X86_TRAP_UD |
+ 						SVM_EVTINJ_TYPE_EXEPT |
+ 						SVM_EVTINJ_VALID);
+ 		}
+ 
+ 		ret = 1;
+ 		break;
+ 	}
+ 	case SVM_VMGEXIT_UNSUPPORTED_EVENT:
+ 		vcpu_unimpl(&svm->vcpu,
+ 			    "vmgexit: unsupported event - exit_info_1=%#llx, exit_info_2=%#llx\n",
+ 			    control->exit_info_1, control->exit_info_2);
+ 		break;
+ 	default:
+ 		ret = svm_invoke_exit_handler(svm, exit_code);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ int sev_es_string_io(struct vcpu_svm *svm, int size, unsigned int port, int in)
+ {
+ 	if (!setup_vmgexit_scratch(svm, in, svm->vmcb->control.exit_info_2))
+ 		return -EINVAL;
+ 
+ 	return kvm_sev_es_string_io(&svm->vcpu, size, port,
+ 				    svm->ghcb_sa, svm->ghcb_sa_len, in);
+ }
+ 
+ void sev_es_init_vmcb(struct vcpu_svm *svm)
+ {
+ 	struct kvm_vcpu *vcpu = &svm->vcpu;
+ 
+ 	svm->vmcb->control.nested_ctl |= SVM_NESTED_CTL_SEV_ES_ENABLE;
+ 	svm->vmcb->control.virt_ext |= LBR_CTL_ENABLE_MASK;
+ 
+ 	/*
+ 	 * An SEV-ES guest requires a VMSA area that is a separate from the
+ 	 * VMCB page. Do not include the encryption mask on the VMSA physical
+ 	 * address since hardware will access it using the guest key.
+ 	 */
+ 	svm->vmcb->control.vmsa_pa = __pa(svm->vmsa);
+ 
+ 	/* Can't intercept CR register access, HV can't modify CR registers */
+ 	svm_clr_intercept(svm, INTERCEPT_CR0_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR4_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR8_READ);
+ 	svm_clr_intercept(svm, INTERCEPT_CR0_WRITE);
+ 	svm_clr_intercept(svm, INTERCEPT_CR4_WRITE);
+ 	svm_clr_intercept(svm, INTERCEPT_CR8_WRITE);
+ 
+ 	svm_clr_intercept(svm, INTERCEPT_SELECTIVE_CR0);
+ 
+ 	/* Track EFER/CR register changes */
+ 	svm_set_intercept(svm, TRAP_EFER_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR0_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR4_WRITE);
+ 	svm_set_intercept(svm, TRAP_CR8_WRITE);
+ 
+ 	/* No support for enable_vmware_backdoor */
+ 	clr_exception_intercept(svm, GP_VECTOR);
+ 
+ 	/* Can't intercept XSETBV, HV can't modify XCR0 directly */
+ 	svm_clr_intercept(svm, INTERCEPT_XSETBV);
+ 
+ 	/* Clear intercepts on selected MSRs */
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_EFER, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_CR_PAT, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);
+ 	set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);
+ }
+ 
+ void sev_es_create_vcpu(struct vcpu_svm *svm)
+ {
+ 	/*
+ 	 * Set the GHCB MSR value as per the GHCB specification when creating
+ 	 * a vCPU for an SEV-ES guest.
+ 	 */
+ 	set_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,
+ 					    GHCB_VERSION_MIN,
+ 					    sev_enc_bit));
+ }
+ 
+ void sev_es_vcpu_load(struct vcpu_svm *svm, int cpu)
+ {
+ 	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
+ 	struct vmcb_save_area *hostsa;
+ 	unsigned int i;
+ 
+ 	/*
+ 	 * As an SEV-ES guest, hardware will restore the host state on VMEXIT,
+ 	 * of which one step is to perform a VMLOAD. Since hardware does not
+ 	 * perform a VMSAVE on VMRUN, the host savearea must be updated.
+ 	 */
+ 	asm volatile(__ex("vmsave") : : "a" (__sme_page_pa(sd->save_area)) : "memory");
+ 
+ 	/*
+ 	 * Certain MSRs are restored on VMEXIT, only save ones that aren't
+ 	 * restored.
+ 	 */
+ 	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++) {
+ 		if (host_save_user_msrs[i].sev_es_restored)
+ 			continue;
+ 
+ 		rdmsrl(host_save_user_msrs[i].index, svm->host_user_msrs[i]);
+ 	}
+ 
+ 	/* XCR0 is restored on VMEXIT, save the current host value */
+ 	hostsa = (struct vmcb_save_area *)(page_address(sd->save_area) + 0x400);
+ 	hostsa->xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
+ 
+ 	/* PKRU is restored on VMEXIT, save the curent host value */
+ 	hostsa->pkru = read_pkru();
+ 
+ 	/* MSR_IA32_XSS is restored on VMEXIT, save the currnet host value */
+ 	hostsa->xss = host_xss;
+ }
+ 
+ void sev_es_vcpu_put(struct vcpu_svm *svm)
+ {
+ 	unsigned int i;
+ 
+ 	/*
+ 	 * Certain MSRs are restored on VMEXIT and were saved with vmsave in
+ 	 * sev_es_vcpu_load() above. Only restore ones that weren't.
+ 	 */
+ 	for (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++) {
+ 		if (host_save_user_msrs[i].sev_es_restored)
+ 			continue;
+ 
+ 		wrmsrl(host_save_user_msrs[i].index, svm->host_user_msrs[i]);
+ 	}
+ }
++>>>>>>> 8640ca588b03 (KVM: SVM: Add AP_JUMP_TABLE support in prep for AP booting)
* Unmerged path arch/x86/kvm/svm/sev.c
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 603a54d1695e..aa0b22db3c8a 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -68,6 +68,7 @@ struct kvm_sev_info {
 	int fd;			/* SEV device fd */
 	unsigned long pages_locked; /* Number of pages locked */
 	struct list_head regions_list;  /* List of registered regions */
+	u64 ap_jump_table;	/* SEV-ES AP Jump Table address */
 };
 
 struct kvm_svm {
