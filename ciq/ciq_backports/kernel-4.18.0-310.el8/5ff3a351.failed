KVM: x86: Move trivial instruction-based exit handlers to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-310.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 5ff3a351f687fdd23051e7474f62788c57a7a613
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-310.el8/5ff3a351.failed

Move the trivial exit handlers, e.g. for instructions that KVM
"emulates" as nops, to common x86 code.  Assign the common handlers
directly to the exit handler arrays and drop the vendor trampolines.

Opportunistically use pr_warn_once() where appropriate.

No functional change intended.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210205005750.3841462-7-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5ff3a351f687fdd23051e7474f62788c57a7a613)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/svm/svm.c
index 05deab5ed2e8,628844e6181c..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -2044,24 -2103,10 +2044,28 @@@ static int intr_interception(struct vcp
  	return 1;
  }
  
++<<<<<<< HEAD
 +static int nop_on_interception(struct vcpu_svm *svm)
 +{
 +	return 1;
 +}
 +
 +static int halt_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_halt(&svm->vcpu);
 +}
 +
 +static int vmmcall_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_hypercall(&svm->vcpu);
 +}
 +
 +static int vmload_interception(struct vcpu_svm *svm)
++=======
+ static int vmload_vmsave_interception(struct kvm_vcpu *vcpu, bool vmload)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb12;
 +	struct vmcb *nested_vmcb;
  	struct kvm_host_map map;
  	int ret;
  
@@@ -2276,52 -2305,28 +2280,56 @@@ static int clgi_interception(struct vcp
  	return ret;
  }
  
 -static int invlpga_interception(struct kvm_vcpu *vcpu)
 +static int invlpga_interception(struct vcpu_svm *svm)
  {
 -	trace_kvm_invlpga(to_svm(vcpu)->vmcb->save.rip, kvm_rcx_read(vcpu),
 -			  kvm_rax_read(vcpu));
 +	struct kvm_vcpu *vcpu = &svm->vcpu;
 +
 +	trace_kvm_invlpga(svm->vmcb->save.rip, kvm_rcx_read(&svm->vcpu),
 +			  kvm_rax_read(&svm->vcpu));
  
  	/* Let's treat INVLPGA the same as INVLPG (can be optimized!) */
 -	kvm_mmu_invlpg(vcpu, kvm_rax_read(vcpu));
 +	kvm_mmu_invlpg(vcpu, kvm_rax_read(&svm->vcpu));
 +
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
 +}
 +
 +static int skinit_interception(struct vcpu_svm *svm)
 +{
 +	trace_kvm_skinit(svm->vmcb->save.rip, kvm_rax_read(&svm->vcpu));
 +
 +	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +	return 1;
 +}
  
 -	return kvm_skip_emulated_instruction(vcpu);
++<<<<<<< HEAD
 +static int wbinvd_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_wbinvd(&svm->vcpu);
  }
  
 -static int skinit_interception(struct kvm_vcpu *vcpu)
 +static int xsetbv_interception(struct vcpu_svm *svm)
  {
 -	trace_kvm_skinit(to_svm(vcpu)->vmcb->save.rip, kvm_rax_read(vcpu));
 +	u64 new_bv = kvm_read_edx_eax(&svm->vcpu);
 +	u32 index = kvm_rcx_read(&svm->vcpu);
 +
 +	if (kvm_set_xcr(&svm->vcpu, index, new_bv) == 0) {
 +		return kvm_skip_emulated_instruction(&svm->vcpu);
 +	}
  
 -	kvm_queue_exception(vcpu, UD_VECTOR);
  	return 1;
  }
  
 +static int rdpru_interception(struct vcpu_svm *svm)
 +{
 +	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +	return 1;
 +}
 +
 +static int task_switch_interception(struct vcpu_svm *svm)
++=======
+ static int task_switch_interception(struct kvm_vcpu *vcpu)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
  	u16 tss_selector;
  	int reason;
  	int int_type = svm->vmcb->control.exit_int_info &
@@@ -2384,47 -2389,40 +2392,55 @@@
  			       has_error_code, error_code);
  }
  
 -static int iret_interception(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static int cpuid_interception(struct vcpu_svm *svm)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	return kvm_emulate_cpuid(&svm->vcpu);
 +}
  
 -	++vcpu->stat.nmi_window_exits;
 -	vcpu->arch.hflags |= HF_IRET_MASK;
 -	if (!sev_es_guest(vcpu->kvm)) {
 -		svm_clr_intercept(svm, INTERCEPT_IRET);
 -		svm->nmi_iret_rip = kvm_rip_read(vcpu);
 -	}
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 +static int iret_interception(struct vcpu_svm *svm)
++=======
++static int iret_interception(struct kvm_vcpu *vcpu)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
 +{
 +	++svm->vcpu.stat.nmi_window_exits;
 +	svm_clr_intercept(svm, INTERCEPT_IRET);
 +	svm->vcpu.arch.hflags |= HF_IRET_MASK;
 +	svm->nmi_iret_rip = kvm_rip_read(&svm->vcpu);
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
  	return 1;
  }
  
++<<<<<<< HEAD
 +static int invd_interception(struct vcpu_svm *svm)
 +{
 +	/* Treat an INVD instruction as a NOP and just skip it. */
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
 +}
 +
 +static int invlpg_interception(struct vcpu_svm *svm)
++=======
+ static int invlpg_interception(struct kvm_vcpu *vcpu)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  {
  	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))
 -		return kvm_emulate_instruction(vcpu, 0);
 +		return kvm_emulate_instruction(&svm->vcpu, 0);
  
 -	kvm_mmu_invlpg(vcpu, to_svm(vcpu)->vmcb->control.exit_info_1);
 -	return kvm_skip_emulated_instruction(vcpu);
 +	kvm_mmu_invlpg(&svm->vcpu, svm->vmcb->control.exit_info_1);
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
  }
  
 -static int emulate_on_interception(struct kvm_vcpu *vcpu)
 +static int emulate_on_interception(struct vcpu_svm *svm)
  {
 -	return kvm_emulate_instruction(vcpu, 0);
 +	return kvm_emulate_instruction(&svm->vcpu, 0);
  }
  
 -static int rsm_interception(struct kvm_vcpu *vcpu)
 +static int rsm_interception(struct vcpu_svm *svm)
  {
 -	return kvm_emulate_instruction_from_buffer(vcpu, rsm_ins_bytes, 2);
 +	return kvm_emulate_instruction_from_buffer(&svm->vcpu, rsm_ins_bytes, 2);
  }
  
 -static int rdpmc_interception(struct kvm_vcpu *vcpu)
 +static int rdpmc_interception(struct vcpu_svm *svm)
  {
  	int err;
  
@@@ -2755,9 -2755,18 +2771,22 @@@ static int svm_get_msr(struct kvm_vcpu 
  	return 0;
  }
  
 -static int svm_complete_emulated_msr(struct kvm_vcpu *vcpu, int err)
 +static int rdmsr_interception(struct vcpu_svm *svm)
  {
++<<<<<<< HEAD
 +	return kvm_emulate_rdmsr(&svm->vcpu);
++=======
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 	if (!sev_es_guest(vcpu->kvm) || !err)
+ 		return kvm_complete_insn_gp(vcpu, err);
+ 
+ 	ghcb_set_sw_exit_info_1(svm->ghcb, 1);
+ 	ghcb_set_sw_exit_info_2(svm->ghcb,
+ 				X86_TRAP_GP |
+ 				SVM_EVTINJ_TYPE_EXEPT |
+ 				SVM_EVTINJ_VALID);
+ 	return 1;
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  }
  
  static int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)
@@@ -2941,23 -2952,18 +2970,32 @@@ static int svm_set_msr(struct kvm_vcpu 
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int wrmsr_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_wrmsr(&svm->vcpu);
 +}
 +
 +static int msr_interception(struct vcpu_svm *svm)
 +{
 +	if (svm->vmcb->control.exit_info_1)
 +		return wrmsr_interception(svm);
 +	else
 +		return rdmsr_interception(svm);
++=======
+ static int msr_interception(struct kvm_vcpu *vcpu)
+ {
+ 	if (to_svm(vcpu)->vmcb->control.exit_info_1)
+ 		return kvm_emulate_wrmsr(vcpu);
+ 	else
+ 		return kvm_emulate_rdmsr(vcpu);
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  }
  
 -static int interrupt_window_interception(struct kvm_vcpu *vcpu)
 +static int interrupt_window_interception(struct vcpu_svm *svm)
  {
 -	kvm_make_request(KVM_REQ_EVENT, vcpu);
 -	svm_clear_vintr(to_svm(vcpu));
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	svm_clear_vintr(svm);
  
  	/*
  	 * For AVIC, the only reason to end up here is ExtINTs.
@@@ -2982,26 -2994,9 +3020,30 @@@ static int pause_interception(struct vc
  	return 1;
  }
  
++<<<<<<< HEAD
 +static int nop_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_skip_emulated_instruction(&(svm->vcpu));
 +}
 +
 +static int monitor_interception(struct vcpu_svm *svm)
 +{
 +	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 +	return nop_interception(svm);
 +}
 +
 +static int mwait_interception(struct vcpu_svm *svm)
 +{
 +	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 +	return nop_interception(svm);
 +}
 +
 +static int invpcid_interception(struct vcpu_svm *svm)
++=======
+ static int invpcid_interception(struct kvm_vcpu *vcpu)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct kvm_vcpu *vcpu = &svm->vcpu;
  	unsigned long type;
  	gva_t gva;
  
@@@ -3083,11 -3078,11 +3125,19 @@@ static int (*const svm_exit_handlers[])
  	[SVM_EXIT_STGI]				= stgi_interception,
  	[SVM_EXIT_CLGI]				= clgi_interception,
  	[SVM_EXIT_SKINIT]			= skinit_interception,
++<<<<<<< HEAD
 +	[SVM_EXIT_WBINVD]                       = wbinvd_interception,
 +	[SVM_EXIT_MONITOR]			= monitor_interception,
 +	[SVM_EXIT_MWAIT]			= mwait_interception,
 +	[SVM_EXIT_XSETBV]			= xsetbv_interception,
 +	[SVM_EXIT_RDPRU]			= rdpru_interception,
++=======
+ 	[SVM_EXIT_WBINVD]                       = kvm_emulate_wbinvd,
+ 	[SVM_EXIT_MONITOR]			= kvm_emulate_monitor,
+ 	[SVM_EXIT_MWAIT]			= kvm_emulate_mwait,
+ 	[SVM_EXIT_XSETBV]			= kvm_emulate_xsetbv,
+ 	[SVM_EXIT_RDPRU]			= kvm_handle_invalid_op,
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  	[SVM_EXIT_EFER_WRITE_TRAP]		= efer_trap,
  	[SVM_EXIT_CR0_WRITE_TRAP]		= cr_trap,
  	[SVM_EXIT_CR4_WRITE_TRAP]		= cr_trap,
@@@ -3241,17 -3239,17 +3291,21 @@@ static int svm_invoke_exit_handler(stru
  
  #ifdef CONFIG_RETPOLINE
  	if (exit_code == SVM_EXIT_MSR)
 -		return msr_interception(vcpu);
 +		return msr_interception(svm);
  	else if (exit_code == SVM_EXIT_VINTR)
 -		return interrupt_window_interception(vcpu);
 +		return interrupt_window_interception(svm);
  	else if (exit_code == SVM_EXIT_INTR)
 -		return intr_interception(vcpu);
 +		return intr_interception(svm);
  	else if (exit_code == SVM_EXIT_HLT)
++<<<<<<< HEAD
 +		return halt_interception(svm);
++=======
+ 		return kvm_emulate_halt(vcpu);
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  	else if (exit_code == SVM_EXIT_NPF)
 -		return npf_interception(vcpu);
 +		return npf_interception(svm);
  #endif
 -	return svm_exit_handlers[exit_code](vcpu);
 +	return svm_exit_handlers[exit_code](svm);
  }
  
  static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2,
diff --cc arch/x86/kvm/vmx/vmx.c
index 9c626d532031,4c56f97a5dcf..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -5203,21 -5200,6 +5192,24 @@@ static int handle_rdpmc(struct kvm_vcp
  	return kvm_complete_insn_gp(vcpu, err);
  }
  
++<<<<<<< HEAD
 +static int handle_wbinvd(struct kvm_vcpu *vcpu)
 +{
 +	return kvm_emulate_wbinvd(vcpu);
 +}
 +
 +static int handle_xsetbv(struct kvm_vcpu *vcpu)
 +{
 +	u64 new_bv = kvm_read_edx_eax(vcpu);
 +	u32 index = kvm_rcx_read(vcpu);
 +
 +	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
 +		return kvm_skip_emulated_instruction(vcpu);
 +	return 1;
 +}
 +
++=======
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  static int handle_apic_access(struct kvm_vcpu *vcpu)
  {
  	if (likely(fasteoi)) {
@@@ -5673,8 -5637,8 +5642,13 @@@ static int (*kvm_vmx_exit_handlers[])(s
  	[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,
  	[EXIT_REASON_APIC_WRITE]              = handle_apic_write,
  	[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,
++<<<<<<< HEAD
 +	[EXIT_REASON_WBINVD]                  = handle_wbinvd,
 +	[EXIT_REASON_XSETBV]                  = handle_xsetbv,
++=======
+ 	[EXIT_REASON_WBINVD]                  = kvm_emulate_wbinvd,
+ 	[EXIT_REASON_XSETBV]                  = kvm_emulate_xsetbv,
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  	[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,
  	[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,
  	[EXIT_REASON_GDTR_IDTR]		      = handle_desc,
diff --cc arch/x86/kvm/x86.c
index 703d0876830f,43b9416fff2c..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1779,12 -1787,46 +1779,50 @@@ int kvm_emulate_wrmsr(struct kvm_vcpu *
  }
  EXPORT_SYMBOL_GPL(kvm_emulate_wrmsr);
  
++<<<<<<< HEAD
 +bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
++=======
+ int kvm_emulate_as_nop(struct kvm_vcpu *vcpu)
+ {
+ 	return kvm_skip_emulated_instruction(vcpu);
+ }
+ EXPORT_SYMBOL_GPL(kvm_emulate_as_nop);
+ 
+ int kvm_emulate_invd(struct kvm_vcpu *vcpu)
+ {
+ 	/* Treat an INVD instruction as a NOP and just skip it. */
+ 	return kvm_emulate_as_nop(vcpu);
+ }
+ EXPORT_SYMBOL_GPL(kvm_emulate_invd);
+ 
+ int kvm_emulate_mwait(struct kvm_vcpu *vcpu)
+ {
+ 	pr_warn_once("kvm: MWAIT instruction emulated as NOP!\n");
+ 	return kvm_emulate_as_nop(vcpu);
+ }
+ EXPORT_SYMBOL_GPL(kvm_emulate_mwait);
+ 
+ int kvm_handle_invalid_op(struct kvm_vcpu *vcpu)
+ {
+ 	kvm_queue_exception(vcpu, UD_VECTOR);
+ 	return 1;
+ }
+ EXPORT_SYMBOL_GPL(kvm_handle_invalid_op);
+ 
+ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
+ {
+ 	pr_warn_once("kvm: MONITOR instruction emulated as NOP!\n");
+ 	return kvm_emulate_as_nop(vcpu);
+ }
+ EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
+ 
+ static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
++>>>>>>> 5ff3a351f687 (KVM: x86: Move trivial instruction-based exit handlers to common code)
  {
 -	xfer_to_guest_mode_prepare();
  	return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
 -		xfer_to_guest_mode_work_pending();
 +		need_resched() || signal_pending(current);
  }
 +EXPORT_SYMBOL_GPL(kvm_vcpu_exit_request);
  
  /*
   * The fast path for frequent and performance sensitive wrmsr emulation,
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b3a59b5f9bb5..0d65d041fb80 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1526,6 +1526,11 @@ int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);
 int kvm_emulate_rdmsr(struct kvm_vcpu *vcpu);
 int kvm_emulate_wrmsr(struct kvm_vcpu *vcpu);
+int kvm_emulate_as_nop(struct kvm_vcpu *vcpu);
+int kvm_emulate_invd(struct kvm_vcpu *vcpu);
+int kvm_emulate_mwait(struct kvm_vcpu *vcpu);
+int kvm_handle_invalid_op(struct kvm_vcpu *vcpu);
+int kvm_emulate_monitor(struct kvm_vcpu *vcpu);
 
 int kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in);
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
