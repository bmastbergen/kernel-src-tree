sched/membarrier: reduce the ability to hammer on sys_membarrier

jira LE-1907
cve CVE-2024-26602
Rebuild_History Non-Buildable kernel-3.10.0-1160.118.1.el7
commit-author Linus Torvalds <torvalds@linuxfoundation.org>
commit 944d5fe50f3f03daacfea16300e656a1691c4a23
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.118.1.el7/944d5fe5.failed

On some systems, sys_membarrier can be very expensive, causing overall
slowdowns for everything.  So put a lock on the path in order to
serialize the accesses to prevent the ability for this to be called at
too high of a frequency and saturate the machine.

	Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Reviewed-and-tested-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Acked-by: Borislav Petkov <bp@alien8.de>
Fixes: 22e4ebb97582 ("membarrier: Provide expedited private command")
Fixes: c5f58bd58f43 ("membarrier: Provide GLOBAL_EXPEDITED command")
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 944d5fe50f3f03daacfea16300e656a1691c4a23)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/membarrier.c
diff --cc kernel/sched/membarrier.c
index 3904de319a59,4e715b9b278e..000000000000
--- a/kernel/sched/membarrier.c
+++ b/kernel/sched/membarrier.c
@@@ -28,20 -138,33 +28,23 @@@
   * except MEMBARRIER_CMD_QUERY.
   */
  #ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE
 -#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK			\
 -	(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE			\
 +#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK \
 +	(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE \
  	| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)
  #else
 -#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK	0
 +#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK 0
  #endif
  
 -#ifdef CONFIG_RSEQ
 -#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK		\
 -	(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ			\
 -	| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)
 -#else
 -#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK	0
 -#endif
 -
 -#define MEMBARRIER_CMD_BITMASK						\
 -	(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED	\
 -	| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED			\
 -	| MEMBARRIER_CMD_PRIVATE_EXPEDITED				\
 -	| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED			\
 -	| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK		\
 -	| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK			\
 -	| MEMBARRIER_CMD_GET_REGISTRATIONS)
 +#define MEMBARRIER_CMD_BITMASK	\
 +	(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED \
 +	| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED \
 +	| MEMBARRIER_CMD_PRIVATE_EXPEDITED	\
 +	| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED	\
 +	| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK)
  
+ static DEFINE_MUTEX(membarrier_ipi_mutex);
+ #define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)
+ 
  static void ipi_mb(void *info)
  {
  	smp_mb();	/* IPIs should be serializing but paranoid. */
@@@ -62,17 -259,12 +65,23 @@@ static int membarrier_global_expedited(
  	 */
  	smp_mb();	/* system call entry is not a mb. */
  
 -	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
 -		return -ENOMEM;
 +	/*
 +	 * Expedited membarrier commands guarantee that they won't
 +	 * block, hence the GFP_NOWAIT allocation flag and fallback
 +	 * implementation.
 +	 */
 +	if (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {
 +		/* Fallback for OOM. */
 +		fallback = true;
 +	}
  
++<<<<<<< HEAD
 +	get_online_cpus();
++=======
+ 	SERIALIZE_IPI();
+ 	cpus_read_lock();
+ 	rcu_read_lock();
++>>>>>>> 944d5fe50f3f (sched/membarrier: reduce the ability to hammer on sys_membarrier)
  	for_each_online_cpu(cpu) {
  		struct task_struct *p;
  
@@@ -141,47 -348,75 +150,54 @@@ static int membarrier_private_expedited
  	 */
  	smp_mb();	/* system call entry is not a mb. */
  
 -	if (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
 -		return -ENOMEM;
 +	/*
 +	 * Expedited membarrier commands guarantee that they won't
 +	 * block, hence the GFP_NOWAIT allocation flag and fallback
 +	 * implementation.
 +	 */
 +	if (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {
 +		/* Fallback for OOM. */
 +		fallback = true;
 +	}
  
++<<<<<<< HEAD
 +	get_online_cpus();
 +	for_each_online_cpu(cpu) {
++=======
+ 	SERIALIZE_IPI();
+ 	cpus_read_lock();
+ 
+ 	if (cpu_id >= 0) {
++>>>>>>> 944d5fe50f3f (sched/membarrier: reduce the ability to hammer on sys_membarrier)
  		struct task_struct *p;
  
 -		if (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))
 -			goto out;
 -		rcu_read_lock();
 -		p = rcu_dereference(cpu_rq(cpu_id)->curr);
 -		if (!p || p->mm != mm) {
 -			rcu_read_unlock();
 -			goto out;
 -		}
 -		rcu_read_unlock();
 -	} else {
 -		int cpu;
 -
 -		rcu_read_lock();
 -		for_each_online_cpu(cpu) {
 -			struct task_struct *p;
 -
 -			p = rcu_dereference(cpu_rq(cpu)->curr);
 -			if (p && p->mm == mm)
 -				__cpumask_set_cpu(cpu, tmpmask);
 -		}
 -		rcu_read_unlock();
 -	}
 -
 -	if (cpu_id >= 0) {
 -		/*
 -		 * smp_call_function_single() will call ipi_func() if cpu_id
 -		 * is the calling CPU.
 -		 */
 -		smp_call_function_single(cpu_id, ipi_func, NULL, 1);
 -	} else {
  		/*
 -		 * For regular membarrier, we can save a few cycles by
 -		 * skipping the current cpu -- we're about to do smp_mb()
 -		 * below, and if we migrate to a different cpu, this cpu
 -		 * and the new cpu will execute a full barrier in the
 -		 * scheduler.
 -		 *
 -		 * For SYNC_CORE, we do need a barrier on the current cpu --
 -		 * otherwise, if we are migrated and replaced by a different
 -		 * task in the same mm just before, during, or after
 -		 * membarrier, we will end up with some thread in the mm
 -		 * running without a core sync.
 -		 *
 -		 * For RSEQ, don't rseq_preempt() the caller.  User code
 -		 * is not supposed to issue syscalls at all from inside an
 -		 * rseq critical section.
 +		 * Skipping the current CPU is OK even through we can be
 +		 * migrated at any point. The current CPU, at the point
 +		 * where we read raw_smp_processor_id(), is ensured to
 +		 * be in program order with respect to the caller
 +		 * thread. Therefore, we can skip this CPU from the
 +		 * iteration.
  		 */
 -		if (flags != MEMBARRIER_FLAG_SYNC_CORE) {
 -			preempt_disable();
 -			smp_call_function_many(tmpmask, ipi_func, NULL, true);
 -			preempt_enable();
 -		} else {
 -			on_each_cpu_mask(tmpmask, ipi_func, NULL, true);
 +		if (cpu == raw_smp_processor_id())
 +			continue;
 +		rcu_read_lock();
 +		p = task_rcu_dereference(&cpu_rq(cpu)->curr);
 +		if (p && p->mm == current->mm) {
 +			if (!fallback)
 +				cpumask_set_cpu(cpu, tmpmask);
 +			else
 +				smp_call_function_single(cpu, ipi_mb, NULL, 1);
  		}
 +		rcu_read_unlock();
  	}
 -
 -out:
 -	if (cpu_id < 0)
 +	if (!fallback) {
 +		preempt_disable();
 +		smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
 +		preempt_enable();
  		free_cpumask_var(tmpmask);
 -	cpus_read_unlock();
 +	}
 +	put_online_cpus();
  
  	/*
  	 * Memory barrier on the caller thread _after_ we finished
@@@ -189,6 -424,65 +205,68 @@@
  	 * rq->curr modification in scheduler.
  	 */
  	smp_mb();	/* exit from system call is not a mb */
++<<<<<<< HEAD
++=======
+ 
+ 	return 0;
+ }
+ 
+ static int sync_runqueues_membarrier_state(struct mm_struct *mm)
+ {
+ 	int membarrier_state = atomic_read(&mm->membarrier_state);
+ 	cpumask_var_t tmpmask;
+ 	int cpu;
+ 
+ 	if (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {
+ 		this_cpu_write(runqueues.membarrier_state, membarrier_state);
+ 
+ 		/*
+ 		 * For single mm user, we can simply issue a memory barrier
+ 		 * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the
+ 		 * mm and in the current runqueue to guarantee that no memory
+ 		 * access following registration is reordered before
+ 		 * registration.
+ 		 */
+ 		smp_mb();
+ 		return 0;
+ 	}
+ 
+ 	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
+ 		return -ENOMEM;
+ 
+ 	/*
+ 	 * For mm with multiple users, we need to ensure all future
+ 	 * scheduler executions will observe @mm's new membarrier
+ 	 * state.
+ 	 */
+ 	synchronize_rcu();
+ 
+ 	/*
+ 	 * For each cpu runqueue, if the task's mm match @mm, ensure that all
+ 	 * @mm's membarrier state set bits are also set in the runqueue's
+ 	 * membarrier state. This ensures that a runqueue scheduling
+ 	 * between threads which are users of @mm has its membarrier state
+ 	 * updated.
+ 	 */
+ 	SERIALIZE_IPI();
+ 	cpus_read_lock();
+ 	rcu_read_lock();
+ 	for_each_online_cpu(cpu) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 		struct task_struct *p;
+ 
+ 		p = rcu_dereference(rq->curr);
+ 		if (p && p->mm == mm)
+ 			__cpumask_set_cpu(cpu, tmpmask);
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	on_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);
+ 
+ 	free_cpumask_var(tmpmask);
+ 	cpus_read_unlock();
+ 
++>>>>>>> 944d5fe50f3f (sched/membarrier: reduce the ability to hammer on sys_membarrier)
  	return 0;
  }
  
* Unmerged path kernel/sched/membarrier.c
