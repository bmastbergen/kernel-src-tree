perf/x86: Factor out x86_pmu_show_pmu_cap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-348.el8
commit-author Kan Liang <kan.liang@linux.intel.com>
commit e11c1a7eb302ac8f6f47c18fa662546405a5fd83
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-348.el8/e11c1a7e.failed

The PMU capabilities are different among hybrid PMUs. Perf should dump
the PMU capabilities information for each hybrid PMU.

Factor out x86_pmu_show_pmu_cap() which shows the PMU capabilities
information. The function will be reused later when registering a
dedicated hybrid PMU.

	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Andi Kleen <ak@linux.intel.com>
Link: https://lkml.kernel.org/r/1618237865-33448-16-git-send-email-kan.liang@linux.intel.com
(cherry picked from commit e11c1a7eb302ac8f6f47c18fa662546405a5fd83)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/core.c
diff --cc arch/x86/events/core.c
index 499d8b925c05,2e7ae529524d..000000000000
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@@ -1900,6 -1942,54 +1900,57 @@@ ssize_t x86_event_sysfs_show(char *page
  static struct attribute_group x86_pmu_attr_group;
  static struct attribute_group x86_pmu_caps_group;
  
++<<<<<<< HEAD
++=======
+ static void x86_pmu_static_call_update(void)
+ {
+ 	static_call_update(x86_pmu_handle_irq, x86_pmu.handle_irq);
+ 	static_call_update(x86_pmu_disable_all, x86_pmu.disable_all);
+ 	static_call_update(x86_pmu_enable_all, x86_pmu.enable_all);
+ 	static_call_update(x86_pmu_enable, x86_pmu.enable);
+ 	static_call_update(x86_pmu_disable, x86_pmu.disable);
+ 
+ 	static_call_update(x86_pmu_add, x86_pmu.add);
+ 	static_call_update(x86_pmu_del, x86_pmu.del);
+ 	static_call_update(x86_pmu_read, x86_pmu.read);
+ 
+ 	static_call_update(x86_pmu_schedule_events, x86_pmu.schedule_events);
+ 	static_call_update(x86_pmu_get_event_constraints, x86_pmu.get_event_constraints);
+ 	static_call_update(x86_pmu_put_event_constraints, x86_pmu.put_event_constraints);
+ 
+ 	static_call_update(x86_pmu_start_scheduling, x86_pmu.start_scheduling);
+ 	static_call_update(x86_pmu_commit_scheduling, x86_pmu.commit_scheduling);
+ 	static_call_update(x86_pmu_stop_scheduling, x86_pmu.stop_scheduling);
+ 
+ 	static_call_update(x86_pmu_sched_task, x86_pmu.sched_task);
+ 	static_call_update(x86_pmu_swap_task_ctx, x86_pmu.swap_task_ctx);
+ 
+ 	static_call_update(x86_pmu_drain_pebs, x86_pmu.drain_pebs);
+ 	static_call_update(x86_pmu_pebs_aliases, x86_pmu.pebs_aliases);
+ 
+ 	static_call_update(x86_pmu_guest_get_msrs, x86_pmu.guest_get_msrs);
+ }
+ 
+ static void _x86_pmu_read(struct perf_event *event)
+ {
+ 	x86_perf_event_update(event);
+ }
+ 
+ void x86_pmu_show_pmu_cap(int num_counters, int num_counters_fixed,
+ 			  u64 intel_ctrl)
+ {
+ 	pr_info("... version:                %d\n",     x86_pmu.version);
+ 	pr_info("... bit width:              %d\n",     x86_pmu.cntval_bits);
+ 	pr_info("... generic registers:      %d\n",     num_counters);
+ 	pr_info("... value mask:             %016Lx\n", x86_pmu.cntval_mask);
+ 	pr_info("... max period:             %016Lx\n", x86_pmu.max_period);
+ 	pr_info("... fixed-purpose events:   %lu\n",
+ 			hweight64((((1ULL << num_counters_fixed) - 1)
+ 					<< INTEL_PMC_IDX_FIXED) & intel_ctrl));
+ 	pr_info("... event mask:             %016Lx\n", intel_ctrl);
+ }
+ 
++>>>>>>> e11c1a7eb302 (perf/x86: Factor out x86_pmu_show_pmu_cap)
  static int __init init_hw_perf_events(void)
  {
  	struct x86_pmu_quirk *quirk;
@@@ -1952,16 -2050,17 +2003,9 @@@
  
  	pmu.attr_update = x86_pmu.attr_update;
  
- 	pr_info("... version:                %d\n",     x86_pmu.version);
- 	pr_info("... bit width:              %d\n",     x86_pmu.cntval_bits);
- 	pr_info("... generic registers:      %d\n",     x86_pmu.num_counters);
- 	pr_info("... value mask:             %016Lx\n", x86_pmu.cntval_mask);
- 	pr_info("... max period:             %016Lx\n", x86_pmu.max_period);
- 	pr_info("... fixed-purpose events:   %lu\n",
- 			hweight64((((1ULL << x86_pmu.num_counters_fixed) - 1)
- 					<< INTEL_PMC_IDX_FIXED) & x86_pmu.intel_ctrl));
- 	pr_info("... event mask:             %016Lx\n", x86_pmu.intel_ctrl);
+ 	x86_pmu_show_pmu_cap(x86_pmu.num_counters, x86_pmu.num_counters_fixed,
+ 			     x86_pmu.intel_ctrl);
  
 -	if (!x86_pmu.read)
 -		x86_pmu.read = _x86_pmu_read;
 -
 -	if (!x86_pmu.guest_get_msrs)
 -		x86_pmu.guest_get_msrs = (void *)&__static_call_return0;
 -
 -	x86_pmu_static_call_update();
 -
  	/*
  	 * Install callbacks. Core will call them for each online
  	 * cpu.
* Unmerged path arch/x86/events/core.c
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 8a9ce0e512b6..99cbac0e76ce 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1074,6 +1074,9 @@ void x86_pmu_enable_event(struct perf_event *event);
 
 int x86_pmu_handle_irq(struct pt_regs *regs);
 
+void x86_pmu_show_pmu_cap(int num_counters, int num_counters_fixed,
+			  u64 intel_ctrl);
+
 extern struct event_constraint emptyconstraint;
 
 extern struct event_constraint unconstrained;
