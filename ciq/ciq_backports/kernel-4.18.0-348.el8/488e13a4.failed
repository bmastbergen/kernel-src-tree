perf/x86/lbr: Remove cpuc->lbr_xsave allocation from atomic context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-348.el8
commit-author Like Xu <like.xu@linux.intel.com>
commit 488e13a489e9707a7e81e1991fdd1f20c0f04689
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-348.el8/488e13a4.failed

If the kernel is compiled with the CONFIG_LOCKDEP option, the conditional
might_sleep_if() deep in kmem_cache_alloc() will generate the following
trace, and potentially cause a deadlock when another LBR event is added:

  [] BUG: sleeping function called from invalid context at include/linux/sched/mm.h:196
  [] Call Trace:
  []  kmem_cache_alloc+0x36/0x250
  []  intel_pmu_lbr_add+0x152/0x170
  []  x86_pmu_add+0x83/0xd0

Make it symmetric with the release_lbr_buffers() call and mirror the
existing DS buffers.

Fixes: c085fb8774 ("perf/x86/intel/lbr: Support XSAVES for arch LBR read")
	Signed-off-by: Like Xu <like.xu@linux.intel.com>
[peterz: simplified]
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Tested-by: Kan Liang <kan.liang@linux.intel.com>
Link: https://lkml.kernel.org/r/20210430052247.3079672-2-like.xu@linux.intel.com
(cherry picked from commit 488e13a489e9707a7e81e1991fdd1f20c0f04689)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/intel/lbr.c
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/intel/lbr.c
index d05b80c1c2dd,4409d2cccfda..000000000000
--- a/arch/x86/events/intel/lbr.c
+++ b/arch/x86/events/intel/lbr.c
@@@ -673,6 -695,45 +673,48 @@@ void intel_pmu_lbr_add(struct perf_even
  	perf_sched_cb_inc(event->ctx->pmu);
  	if (!cpuc->lbr_users++ && !event->total_time_running)
  		intel_pmu_lbr_reset();
++<<<<<<< HEAD
++=======
+ }
+ 
+ void release_lbr_buffers(void)
+ {
+ 	struct kmem_cache *kmem_cache;
+ 	struct cpu_hw_events *cpuc;
+ 	int cpu;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_ARCH_LBR))
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		cpuc = per_cpu_ptr(&cpu_hw_events, cpu);
+ 		kmem_cache = x86_get_pmu(cpu)->task_ctx_cache;
+ 		if (kmem_cache && cpuc->lbr_xsave) {
+ 			kmem_cache_free(kmem_cache, cpuc->lbr_xsave);
+ 			cpuc->lbr_xsave = NULL;
+ 		}
+ 	}
++>>>>>>> 488e13a489e9 (perf/x86/lbr: Remove cpuc->lbr_xsave allocation from atomic context)
+ }
+ 
+ void reserve_lbr_buffers(void)
+ {
+ 	struct kmem_cache *kmem_cache;
+ 	struct cpu_hw_events *cpuc;
+ 	int cpu;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_ARCH_LBR))
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		cpuc = per_cpu_ptr(&cpu_hw_events, cpu);
+ 		kmem_cache = x86_get_pmu(cpu)->task_ctx_cache;
+ 		if (!kmem_cache || cpuc->lbr_xsave)
+ 			continue;
+ 
+ 		cpuc->lbr_xsave = kmem_cache_alloc_node(kmem_cache, GFP_KERNEL,
+ 							cpu_to_node(cpu));
+ 	}
  }
  
  void intel_pmu_lbr_del(struct perf_event *event)
diff --cc arch/x86/events/perf_event.h
index 5a929c0e9581,ad87cb36f7c8..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -1183,6 -1242,10 +1183,13 @@@ void release_ds_buffers(void)
  
  void reserve_ds_buffers(void);
  
++<<<<<<< HEAD
++=======
+ void release_lbr_buffers(void);
+ 
+ void reserve_lbr_buffers(void);
+ 
++>>>>>>> 488e13a489e9 (perf/x86/lbr: Remove cpuc->lbr_xsave allocation from atomic context)
  extern struct event_constraint bts_constraint;
  extern struct event_constraint vlbr_constraint;
  
@@@ -1326,6 -1391,14 +1333,17 @@@ static inline void release_ds_buffers(v
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline void release_lbr_buffers(void)
+ {
+ }
+ 
+ static inline void reserve_lbr_buffers(void)
+ {
+ }
+ 
++>>>>>>> 488e13a489e9 (perf/x86/lbr: Remove cpuc->lbr_xsave allocation from atomic context)
  static inline int intel_pmu_init(void)
  {
  	return 0;
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 9bc9849a2314..ec7f2ddfccdc 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -359,10 +359,12 @@ int x86_reserve_hardware(void)
 	if (!atomic_inc_not_zero(&pmc_refcount)) {
 		mutex_lock(&pmc_reserve_mutex);
 		if (atomic_read(&pmc_refcount) == 0) {
-			if (!reserve_pmc_hardware())
+			if (!reserve_pmc_hardware()) {
 				err = -EBUSY;
-			else
+			} else {
 				reserve_ds_buffers();
+				reserve_lbr_buffers();
+			}
 		}
 		if (!err)
 			atomic_inc(&pmc_refcount);
* Unmerged path arch/x86/events/intel/lbr.c
* Unmerged path arch/x86/events/perf_event.h
