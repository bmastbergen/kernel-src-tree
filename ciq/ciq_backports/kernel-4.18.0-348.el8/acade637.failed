perf/x86/intel: Apply mid ACK for small core

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-348.el8
commit-author Kan Liang <kan.liang@linux.intel.com>
commit acade6379930dfa7987f4bd9b26d1a701cc1b542
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-348.el8/acade637.failed

A warning as below may be occasionally triggered in an ADL machine when
these conditions occur:

 - Two perf record commands run one by one. Both record a PEBS event.
 - Both runs on small cores.
 - They have different adaptive PEBS configuration (PEBS_DATA_CFG).

  [ ] WARNING: CPU: 4 PID: 9874 at arch/x86/events/intel/ds.c:1743 setup_pebs_adaptive_sample_data+0x55e/0x5b0
  [ ] RIP: 0010:setup_pebs_adaptive_sample_data+0x55e/0x5b0
  [ ] Call Trace:
  [ ]  <NMI>
  [ ]  intel_pmu_drain_pebs_icl+0x48b/0x810
  [ ]  perf_event_nmi_handler+0x41/0x80
  [ ]  </NMI>
  [ ]  __perf_event_task_sched_in+0x2c2/0x3a0

Different from the big core, the small core requires the ACK right
before re-enabling counters in the NMI handler, otherwise a stale PEBS
record may be dumped into the later NMI handler, which trigger the
warning.

Add a new mid_ack flag to track the case. Add all PMI handler bits in
the struct x86_hybrid_pmu to track the bits for different types of
PMUs.  Apply mid ACK for the small cores on an Alder Lake machine.

The existing hybrid() macro has a compile error when taking address of
a bit-field variable. Add a new macro hybrid_bit() to get the
bit-field value of a given PMU.

Fixes: f83d2f91d259 ("perf/x86/intel: Add Alder Lake Hybrid support")
	Reported-by: Ammy Yi <ammy.yi@intel.com>
	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Andi Kleen <ak@linux.intel.com>
	Tested-by: Ammy Yi <ammy.yi@intel.com>
Link: https://lkml.kernel.org/r/1627997128-57891-1-git-send-email-kan.liang@linux.intel.com
(cherry picked from commit acade6379930dfa7987f4bd9b26d1a701cc1b542)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/intel/core.c
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/intel/core.c
index 802b59942b31,ac6fd2dabf6a..000000000000
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@@ -5866,6 -6120,105 +5872,108 @@@ __init int intel_pmu_init(void
  		name = "sapphire_rapids";
  		break;
  
++<<<<<<< HEAD
++=======
+ 	case INTEL_FAM6_ALDERLAKE:
+ 	case INTEL_FAM6_ALDERLAKE_L:
+ 		/*
+ 		 * Alder Lake has 2 types of CPU, core and atom.
+ 		 *
+ 		 * Initialize the common PerfMon capabilities here.
+ 		 */
+ 		x86_pmu.hybrid_pmu = kcalloc(X86_HYBRID_NUM_PMUS,
+ 					     sizeof(struct x86_hybrid_pmu),
+ 					     GFP_KERNEL);
+ 		if (!x86_pmu.hybrid_pmu)
+ 			return -ENOMEM;
+ 		static_branch_enable(&perf_is_hybrid);
+ 		x86_pmu.num_hybrid_pmus = X86_HYBRID_NUM_PMUS;
+ 
+ 		x86_pmu.pebs_aliases = NULL;
+ 		x86_pmu.pebs_prec_dist = true;
+ 		x86_pmu.pebs_block = true;
+ 		x86_pmu.flags |= PMU_FL_HAS_RSP_1;
+ 		x86_pmu.flags |= PMU_FL_NO_HT_SHARING;
+ 		x86_pmu.flags |= PMU_FL_PEBS_ALL;
+ 		x86_pmu.flags |= PMU_FL_INSTR_LATENCY;
+ 		x86_pmu.flags |= PMU_FL_MEM_LOADS_AUX;
+ 		x86_pmu.lbr_pt_coexist = true;
+ 		intel_pmu_pebs_data_source_skl(false);
+ 		x86_pmu.num_topdown_events = 8;
+ 		x86_pmu.update_topdown_event = adl_update_topdown_event;
+ 		x86_pmu.set_topdown_event_period = adl_set_topdown_event_period;
+ 
+ 		x86_pmu.filter_match = intel_pmu_filter_match;
+ 		x86_pmu.get_event_constraints = adl_get_event_constraints;
+ 		x86_pmu.hw_config = adl_hw_config;
+ 		x86_pmu.limit_period = spr_limit_period;
+ 		x86_pmu.get_hybrid_cpu_type = adl_get_hybrid_cpu_type;
+ 		/*
+ 		 * The rtm_abort_event is used to check whether to enable GPRs
+ 		 * for the RTM abort event. Atom doesn't have the RTM abort
+ 		 * event. There is no harmful to set it in the common
+ 		 * x86_pmu.rtm_abort_event.
+ 		 */
+ 		x86_pmu.rtm_abort_event = X86_CONFIG(.event=0xc9, .umask=0x04);
+ 
+ 		td_attr = adl_hybrid_events_attrs;
+ 		mem_attr = adl_hybrid_mem_attrs;
+ 		tsx_attr = adl_hybrid_tsx_attrs;
+ 		extra_attr = boot_cpu_has(X86_FEATURE_RTM) ?
+ 			adl_hybrid_extra_attr_rtm : adl_hybrid_extra_attr;
+ 
+ 		/* Initialize big core specific PerfMon capabilities.*/
+ 		pmu = &x86_pmu.hybrid_pmu[X86_HYBRID_PMU_CORE_IDX];
+ 		pmu->name = "cpu_core";
+ 		pmu->cpu_type = hybrid_big;
+ 		pmu->late_ack = true;
+ 		if (cpu_feature_enabled(X86_FEATURE_HYBRID_CPU)) {
+ 			pmu->num_counters = x86_pmu.num_counters + 2;
+ 			pmu->num_counters_fixed = x86_pmu.num_counters_fixed + 1;
+ 		} else {
+ 			pmu->num_counters = x86_pmu.num_counters;
+ 			pmu->num_counters_fixed = x86_pmu.num_counters_fixed;
+ 		}
+ 		pmu->max_pebs_events = min_t(unsigned, MAX_PEBS_EVENTS, pmu->num_counters);
+ 		pmu->unconstrained = (struct event_constraint)
+ 					__EVENT_CONSTRAINT(0, (1ULL << pmu->num_counters) - 1,
+ 							   0, pmu->num_counters, 0, 0);
+ 		pmu->intel_cap.capabilities = x86_pmu.intel_cap.capabilities;
+ 		pmu->intel_cap.perf_metrics = 1;
+ 		pmu->intel_cap.pebs_output_pt_available = 0;
+ 
+ 		memcpy(pmu->hw_cache_event_ids, spr_hw_cache_event_ids, sizeof(pmu->hw_cache_event_ids));
+ 		memcpy(pmu->hw_cache_extra_regs, spr_hw_cache_extra_regs, sizeof(pmu->hw_cache_extra_regs));
+ 		pmu->event_constraints = intel_spr_event_constraints;
+ 		pmu->pebs_constraints = intel_spr_pebs_event_constraints;
+ 		pmu->extra_regs = intel_spr_extra_regs;
+ 
+ 		/* Initialize Atom core specific PerfMon capabilities.*/
+ 		pmu = &x86_pmu.hybrid_pmu[X86_HYBRID_PMU_ATOM_IDX];
+ 		pmu->name = "cpu_atom";
+ 		pmu->cpu_type = hybrid_small;
+ 		pmu->mid_ack = true;
+ 		pmu->num_counters = x86_pmu.num_counters;
+ 		pmu->num_counters_fixed = x86_pmu.num_counters_fixed;
+ 		pmu->max_pebs_events = x86_pmu.max_pebs_events;
+ 		pmu->unconstrained = (struct event_constraint)
+ 					__EVENT_CONSTRAINT(0, (1ULL << pmu->num_counters) - 1,
+ 							   0, pmu->num_counters, 0, 0);
+ 		pmu->intel_cap.capabilities = x86_pmu.intel_cap.capabilities;
+ 		pmu->intel_cap.perf_metrics = 0;
+ 		pmu->intel_cap.pebs_output_pt_available = 1;
+ 
+ 		memcpy(pmu->hw_cache_event_ids, glp_hw_cache_event_ids, sizeof(pmu->hw_cache_event_ids));
+ 		memcpy(pmu->hw_cache_extra_regs, tnt_hw_cache_extra_regs, sizeof(pmu->hw_cache_extra_regs));
+ 		pmu->hw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;
+ 		pmu->event_constraints = intel_slm_event_constraints;
+ 		pmu->pebs_constraints = intel_grt_pebs_event_constraints;
+ 		pmu->extra_regs = intel_grt_extra_regs;
+ 		pr_cont("Alderlake Hybrid events, ");
+ 		name = "alderlake_hybrid";
+ 		break;
+ 
++>>>>>>> acade6379930 (perf/x86/intel: Apply mid ACK for small core)
  	default:
  		switch (x86_pmu.version) {
  		case 1:
diff --cc arch/x86/events/perf_event.h
index 5a929c0e9581,e3ac05c97b5e..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -679,6 -690,28 +683,31 @@@ extern struct static_key_false perf_is_
  	__Fp;						\
  }))
  
++<<<<<<< HEAD
++=======
+ #define hybrid_bit(_pmu, _field)			\
+ ({							\
+ 	bool __Fp = x86_pmu._field;			\
+ 							\
+ 	if (is_hybrid() && (_pmu))			\
+ 		__Fp = hybrid_pmu(_pmu)->_field;	\
+ 							\
+ 	__Fp;						\
+ })
+ 
+ enum hybrid_pmu_type {
+ 	hybrid_big		= 0x40,
+ 	hybrid_small		= 0x20,
+ 
+ 	hybrid_big_small	= hybrid_big | hybrid_small,
+ };
+ 
+ #define X86_HYBRID_PMU_ATOM_IDX		0
+ #define X86_HYBRID_PMU_CORE_IDX		1
+ 
+ #define X86_HYBRID_NUM_PMUS		2
+ 
++>>>>>>> acade6379930 (perf/x86/intel: Apply mid ACK for small core)
  /*
   * struct x86_pmu - generic x86 pmu
   */
@@@ -735,8 -768,9 +764,14 @@@ struct x86_pmu 
  	u64		(*limit_period)(struct perf_event *event, u64 l);
  
  	/* PMI handler bits */
++<<<<<<< HEAD
 +	unsigned int	late_ack		:1;
 +
++=======
+ 	unsigned int	late_ack		:1,
+ 			mid_ack			:1,
+ 			enabled_ack		:1;
++>>>>>>> acade6379930 (perf/x86/intel: Apply mid ACK for small core)
  	/*
  	 * sysfs attrs
  	 */
* Unmerged path arch/x86/events/intel/core.c
* Unmerged path arch/x86/events/perf_event.h
