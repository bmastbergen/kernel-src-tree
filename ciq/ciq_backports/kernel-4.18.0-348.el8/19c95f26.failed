arm64: entry.S: Do not preempt from IRQ before all cpufeatures are enabled

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-348.el8
commit-author Julien Thierry <julien.thierry@arm.com>
commit 19c95f261c6558d4c2cbbfacd2d8bb6501384601
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-348.el8/19c95f26.failed

Preempting from IRQ-return means that the task has its PSTATE saved
on the stack, which will get restored when the task is resumed and does
the actual IRQ return.

However, enabling some CPU features requires modifying the PSTATE. This
means that, if a task was scheduled out during an IRQ-return before all
CPU features are enabled, the task might restore a PSTATE that does not
include the feature enablement changes once scheduled back in.

* Task 1:

PAN == 0 ---|                          |---------------
            |                          |<- return from IRQ, PSTATE.PAN = 0
            | <- IRQ                   |
            +--------+ <- preempt()  +--
                                     ^
                                     |
                                     reschedule Task 1, PSTATE.PAN == 1
* Init:
        --------------------+------------------------
                            ^
                            |
                            enable_cpu_features
                            set PSTATE.PAN on all CPUs

Worse than this, since PSTATE is untouched when task switching is done,
a task missing the new bits in PSTATE might affect another task, if both
do direct calls to schedule() (outside of IRQ/exception contexts).

Fix this by preventing preemption on IRQ-return until features are
enabled on all CPUs.

This way the only PSTATE values that are saved on the stack are from
synchronous exceptions. These are expected to be fatal this early, the
exception is BRK for WARN_ON(), but as this uses do_debug_exception()
which keeps IRQs masked, it shouldn't call schedule().

	Signed-off-by: Julien Thierry <julien.thierry@arm.com>
[james: Replaced a really cool hack, with an even simpler static key in C.
 expanded commit message with Julien's cover-letter ascii art]
	Signed-off-by: James Morse <james.morse@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 19c95f261c6558d4c2cbbfacd2d8bb6501384601)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/process.c
diff --cc arch/arm64/kernel/process.c
index 1d01ae2f356c,71f788cd2b18..000000000000
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@@ -28,8 -17,10 +28,9 @@@
  #include <linux/sched/task.h>
  #include <linux/sched/task_stack.h>
  #include <linux/kernel.h>
+ #include <linux/lockdep.h>
  #include <linux/mm.h>
  #include <linux/stddef.h>
 -#include <linux/sysctl.h>
  #include <linux/unistd.h>
  #include <linux/user.h>
  #include <linux/delay.h>
@@@ -575,3 -566,86 +577,89 @@@ void arch_setup_new_exec(void
  
  	ptrauth_thread_init_user(current);
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_ARM64_TAGGED_ADDR_ABI
+ /*
+  * Control the relaxed ABI allowing tagged user addresses into the kernel.
+  */
+ static unsigned int tagged_addr_disabled;
+ 
+ long set_tagged_addr_ctrl(unsigned long arg)
+ {
+ 	if (is_compat_task())
+ 		return -EINVAL;
+ 	if (arg & ~PR_TAGGED_ADDR_ENABLE)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Do not allow the enabling of the tagged address ABI if globally
+ 	 * disabled via sysctl abi.tagged_addr_disabled.
+ 	 */
+ 	if (arg & PR_TAGGED_ADDR_ENABLE && tagged_addr_disabled)
+ 		return -EINVAL;
+ 
+ 	update_thread_flag(TIF_TAGGED_ADDR, arg & PR_TAGGED_ADDR_ENABLE);
+ 
+ 	return 0;
+ }
+ 
+ long get_tagged_addr_ctrl(void)
+ {
+ 	if (is_compat_task())
+ 		return -EINVAL;
+ 
+ 	if (test_thread_flag(TIF_TAGGED_ADDR))
+ 		return PR_TAGGED_ADDR_ENABLE;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Global sysctl to disable the tagged user addresses support. This control
+  * only prevents the tagged address ABI enabling via prctl() and does not
+  * disable it for tasks that already opted in to the relaxed ABI.
+  */
+ static int zero;
+ static int one = 1;
+ 
+ static struct ctl_table tagged_addr_sysctl_table[] = {
+ 	{
+ 		.procname	= "tagged_addr_disabled",
+ 		.mode		= 0644,
+ 		.data		= &tagged_addr_disabled,
+ 		.maxlen		= sizeof(int),
+ 		.proc_handler	= proc_dointvec_minmax,
+ 		.extra1		= &zero,
+ 		.extra2		= &one,
+ 	},
+ 	{ }
+ };
+ 
+ static int __init tagged_addr_init(void)
+ {
+ 	if (!register_sysctl("abi", tagged_addr_sysctl_table))
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ core_initcall(tagged_addr_init);
+ #endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */
+ 
+ asmlinkage void __sched arm64_preempt_schedule_irq(void)
+ {
+ 	lockdep_assert_irqs_disabled();
+ 
+ 	/*
+ 	 * Preempting a task from an IRQ means we leave copies of PSTATE
+ 	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+ 	 * resuming one of these preempted tasks would undo those changes.
+ 	 *
+ 	 * Only allow a task to be preempted once cpufeatures have been
+ 	 * enabled.
+ 	 */
+ 	if (static_branch_likely(&arm64_const_caps_ready))
+ 		preempt_schedule_irq();
+ }
++>>>>>>> 19c95f261c65 (arm64: entry.S: Do not preempt from IRQ before all cpufeatures are enabled)
diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
index 4c404d1bd34d..8defa0bdd30f 100644
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@ -693,7 +693,7 @@ alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 	orr	x24, x24, x0
 alternative_else_nop_endif
 	cbnz	x24, 1f				// preempt count != 0 || NMI return path
-	bl	preempt_schedule_irq		// irq en/disable is done inside
+	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
 1:
 #endif
 
* Unmerged path arch/arm64/kernel/process.c
diff --git a/include/linux/sched.h b/include/linux/sched.h
index e8ac297a2a6d..187fbf3248da 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -223,6 +223,7 @@ extern long schedule_timeout_uninterruptible(long timeout);
 extern long schedule_timeout_idle(long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
+asmlinkage void preempt_schedule_irq(void);
 
 extern int __must_check io_schedule_prepare(void);
 extern void io_schedule_finish(int token);
