perf: Extend PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-348.el8
commit-author Kan Liang <kan.liang@linux.intel.com>
commit 55bcf6ef314ae8ba81bcd74aa760247b635ed47b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-348.el8/55bcf6ef.failed

Current Hardware events and Hardware cache events have special perf
types, PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE. The two types don't
pass the PMU type in the user interface. For a hybrid system, the perf
subsystem doesn't know which PMU the events belong to. The first capable
PMU will always be assigned to the events. The events never get a chance
to run on the other capable PMUs.

Extend the two types to become PMU aware types. The PMU type ID is
stored at attr.config[63:32].

Add a new PMU capability, PERF_PMU_CAP_EXTENDED_HW_TYPE, to indicate a
PMU which supports the extended PERF_TYPE_HARDWARE and
PERF_TYPE_HW_CACHE.

The PMU type is only required when searching a specific PMU. The PMU
specific codes will only be interested in the 'real' config value, which
is stored in the low 32 bit of the event->attr.config. Update the
event->attr.config in the generic code, so the PMU specific codes don't
need to calculate it separately.

If a user specifies a PMU type, but the PMU doesn't support the extended
type, error out.

If an event cannot be initialized in a PMU specified by a user, error
out immediately. Perf should not try to open it on other PMUs.

The new PMU capability is only set for the X86 hybrid PMUs for now.
Other architectures, e.g., ARM, may need it as well. The support on ARM
may be implemented later separately.

	Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/1618237865-33448-22-git-send-email-kan.liang@linux.intel.com
(cherry picked from commit 55bcf6ef314ae8ba81bcd74aa760247b635ed47b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/core.c
diff --cc arch/x86/events/core.c
index 9bc9849a2314,3fe66b7aa721..000000000000
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@@ -1982,9 -2154,46 +1982,52 @@@ static int __init init_hw_perf_events(v
  	if (err)
  		goto out1;
  
++<<<<<<< HEAD
 +	err = perf_pmu_register(&pmu, "cpu", PERF_TYPE_RAW);
 +	if (err)
 +		goto out2;
++=======
+ 	if (!is_hybrid()) {
+ 		err = perf_pmu_register(&pmu, "cpu", PERF_TYPE_RAW);
+ 		if (err)
+ 			goto out2;
+ 	} else {
+ 		u8 cpu_type = get_this_hybrid_cpu_type();
+ 		struct x86_hybrid_pmu *hybrid_pmu;
+ 		int i, j;
+ 
+ 		if (!cpu_type && x86_pmu.get_hybrid_cpu_type)
+ 			cpu_type = x86_pmu.get_hybrid_cpu_type();
+ 
+ 		for (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {
+ 			hybrid_pmu = &x86_pmu.hybrid_pmu[i];
+ 
+ 			hybrid_pmu->pmu = pmu;
+ 			hybrid_pmu->pmu.type = -1;
+ 			hybrid_pmu->pmu.attr_update = x86_pmu.attr_update;
+ 			hybrid_pmu->pmu.capabilities |= PERF_PMU_CAP_HETEROGENEOUS_CPUS;
+ 			hybrid_pmu->pmu.capabilities |= PERF_PMU_CAP_EXTENDED_HW_TYPE;
+ 
+ 			err = perf_pmu_register(&hybrid_pmu->pmu, hybrid_pmu->name,
+ 						(hybrid_pmu->cpu_type == hybrid_big) ? PERF_TYPE_RAW : -1);
+ 			if (err)
+ 				break;
+ 
+ 			if (cpu_type == hybrid_pmu->cpu_type)
+ 				x86_pmu_update_cpu_context(&hybrid_pmu->pmu, raw_smp_processor_id());
+ 		}
+ 
+ 		if (i < x86_pmu.num_hybrid_pmus) {
+ 			for (j = 0; j < i; j++)
+ 				perf_pmu_unregister(&x86_pmu.hybrid_pmu[j].pmu);
+ 			pr_warn("Failed to register hybrid PMUs\n");
+ 			kfree(x86_pmu.hybrid_pmu);
+ 			x86_pmu.hybrid_pmu = NULL;
+ 			x86_pmu.num_hybrid_pmus = 0;
+ 			goto out2;
+ 		}
+ 	}
++>>>>>>> 55bcf6ef314a (perf: Extend PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE)
  
  	return 0;
  
* Unmerged path arch/x86/events/core.c
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8da441c99030..b1ccf7a0c44e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -282,15 +282,16 @@ struct perf_event;
 /**
  * pmu::capabilities flags
  */
-#define PERF_PMU_CAP_NO_INTERRUPT		0x01
-#define PERF_PMU_CAP_NO_NMI			0x02
-#define PERF_PMU_CAP_AUX_NO_SG			0x04
-#define PERF_PMU_CAP_EXTENDED_REGS		0x08
-#define PERF_PMU_CAP_EXCLUSIVE			0x10
-#define PERF_PMU_CAP_ITRACE			0x20
-#define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40
-#define PERF_PMU_CAP_NO_EXCLUDE			0x80
-#define PERF_PMU_CAP_AUX_OUTPUT			0x100
+#define PERF_PMU_CAP_NO_INTERRUPT		0x0001
+#define PERF_PMU_CAP_NO_NMI			0x0002
+#define PERF_PMU_CAP_AUX_NO_SG			0x0004
+#define PERF_PMU_CAP_EXTENDED_REGS		0x0008
+#define PERF_PMU_CAP_EXCLUSIVE			0x0010
+#define PERF_PMU_CAP_ITRACE			0x0020
+#define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x0040
+#define PERF_PMU_CAP_NO_EXCLUDE			0x0080
+#define PERF_PMU_CAP_AUX_OUTPUT			0x0100
+#define PERF_PMU_CAP_EXTENDED_HW_TYPE		0x0200
 
 struct perf_output_handle;
 
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 3cca2d17dfd7..1295157e261a 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -37,6 +37,21 @@ enum perf_type_id {
 	PERF_TYPE_MAX,				/* non-ABI */
 };
 
+/*
+ * attr.config layout for type PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE
+ * PERF_TYPE_HARDWARE:			0xEEEEEEEE000000AA
+ *					AA: hardware event ID
+ *					EEEEEEEE: PMU type ID
+ * PERF_TYPE_HW_CACHE:			0xEEEEEEEE00DDCCBB
+ *					BB: hardware cache ID
+ *					CC: hardware cache op ID
+ *					DD: hardware cache op result ID
+ *					EEEEEEEE: PMU type ID
+ * If the PMU type ID is 0, the PERF_TYPE_RAW will be applied.
+ */
+#define PERF_PMU_TYPE_SHIFT		32
+#define PERF_HW_EVENT_MASK		0xffffffff
+
 /*
  * Generalized performance event event_id types, used by the
  * attr.event_id parameter of the sys_perf_event_open()
diff --git a/kernel/events/core.c b/kernel/events/core.c
index d79f62d0609a..b6f02955c838 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -11092,6 +11092,7 @@ static int perf_try_init_event(struct pmu *pmu, struct perf_event *event)
 
 static struct pmu *perf_init_event(struct perf_event *event)
 {
+	bool extended_type = false;
 	int idx, type, ret;
 	struct pmu *pmu;
 
@@ -11110,16 +11111,27 @@ static struct pmu *perf_init_event(struct perf_event *event)
 	 * are often aliases for PERF_TYPE_RAW.
 	 */
 	type = event->attr.type;
-	if (type == PERF_TYPE_HARDWARE || type == PERF_TYPE_HW_CACHE)
-		type = PERF_TYPE_RAW;
+	if (type == PERF_TYPE_HARDWARE || type == PERF_TYPE_HW_CACHE) {
+		type = event->attr.config >> PERF_PMU_TYPE_SHIFT;
+		if (!type) {
+			type = PERF_TYPE_RAW;
+		} else {
+			extended_type = true;
+			event->attr.config &= PERF_HW_EVENT_MASK;
+		}
+	}
 
 again:
 	rcu_read_lock();
 	pmu = idr_find(&pmu_idr, type);
 	rcu_read_unlock();
 	if (pmu) {
+		if (event->attr.type != type && type != PERF_TYPE_RAW &&
+		    !(pmu->capabilities & PERF_PMU_CAP_EXTENDED_HW_TYPE))
+			goto fail;
+
 		ret = perf_try_init_event(pmu, event);
-		if (ret == -ENOENT && event->attr.type != type) {
+		if (ret == -ENOENT && event->attr.type != type && !extended_type) {
 			type = event->attr.type;
 			goto again;
 		}
@@ -11140,6 +11152,7 @@ static struct pmu *perf_init_event(struct perf_event *event)
 			goto unlock;
 		}
 	}
+fail:
 	pmu = ERR_PTR(-ENOENT);
 unlock:
 	srcu_read_unlock(&pmus_srcu, idx);
