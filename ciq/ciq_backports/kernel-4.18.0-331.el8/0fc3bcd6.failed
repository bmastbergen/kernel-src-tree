gfs2: Clean up the error handling in gfs2_page_mkwrite

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit 0fc3bcd6b6e34281254658bef27c45ec8c19e50c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/0fc3bcd6.failed

We're setting an error number so that block_page_mkwrite_return
translates it into the corresponding VM_FAULT_* code in several places,
but this is getting confusing, so set the VM_FAULT_* codes directly
instead.  (No change in functionality.)

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit 0fc3bcd6b6e34281254658bef27c45ec8c19e50c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/file.c
diff --cc fs/gfs2/file.c
index e69af279b7fd,cf16d61f46ec..000000000000
--- a/fs/gfs2/file.c
+++ b/fs/gfs2/file.c
@@@ -412,49 -425,77 +412,68 @@@ static vm_fault_t gfs2_page_mkwrite(str
  	struct gfs2_inode *ip = GFS2_I(inode);
  	struct gfs2_sbd *sdp = GFS2_SB(inode);
  	struct gfs2_alloc_parms ap = { .aflags = 0, };
 -	u64 offset = page_offset(page);
 +	unsigned long last_index;
 +	u64 pos = page_offset(page);
  	unsigned int data_blocks, ind_blocks, rblocks;
+ 	vm_fault_t ret = VM_FAULT_LOCKED;
  	struct gfs2_holder gh;
 -	unsigned int length;
  	loff_t size;
- 	int ret;
+ 	int err;
  
  	sb_start_pagefault(inode->i_sb);
  
 +	gfs2_size_hint(vmf->vma->vm_file, pos, PAGE_SIZE);
 +
  	gfs2_holder_init(ip->i_gl, LM_ST_EXCLUSIVE, 0, &gh);
- 	ret = gfs2_glock_nq(&gh);
- 	if (ret)
+ 	err = gfs2_glock_nq(&gh);
+ 	if (err) {
+ 		ret = block_page_mkwrite_return(err);
  		goto out_uninit;
+ 	}
  
++<<<<<<< HEAD
++=======
+ 	/* Check page index against inode size */
+ 	size = i_size_read(inode);
+ 	if (offset >= size) {
+ 		ret = VM_FAULT_SIGBUS;
+ 		goto out_unlock;
+ 	}
+ 
++>>>>>>> 0fc3bcd6b6e3 (gfs2: Clean up the error handling in gfs2_page_mkwrite)
  	/* Update file times before taking page lock */
  	file_update_time(vmf->vma->vm_file);
  
  	set_bit(GLF_DIRTY, &ip->i_gl->gl_flags);
  	set_bit(GIF_SW_PAGED, &ip->i_flags);
  
 -	/*
 -	 * iomap_writepage / iomap_writepages currently don't support inline
 -	 * files, so always unstuff here.
 -	 */
 -
 -	if (!gfs2_is_stuffed(ip) &&
 -	    !gfs2_write_alloc_required(ip, offset, length)) {
 +	if (!gfs2_write_alloc_required(ip, pos, PAGE_SIZE)) {
  		lock_page(page);
  		if (!PageUptodate(page) || page->mapping != inode->i_mapping) {
- 			ret = -EAGAIN;
+ 			ret = VM_FAULT_NOPAGE;
  			unlock_page(page);
  		}
  		goto out_unlock;
  	}
  
- 	ret = gfs2_rindex_update(sdp);
- 	if (ret)
+ 	err = gfs2_rindex_update(sdp);
+ 	if (err) {
+ 		ret = block_page_mkwrite_return(err);
  		goto out_unlock;
+ 	}
  
 -	gfs2_write_calc_reserv(ip, length, &data_blocks, &ind_blocks);
 +	gfs2_write_calc_reserv(ip, PAGE_SIZE, &data_blocks, &ind_blocks);
  	ap.target = data_blocks + ind_blocks;
- 	ret = gfs2_quota_lock_check(ip, &ap);
- 	if (ret)
+ 	err = gfs2_quota_lock_check(ip, &ap);
+ 	if (err) {
+ 		ret = block_page_mkwrite_return(err);
  		goto out_unlock;
- 	ret = gfs2_inplace_reserve(ip, &ap);
- 	if (ret)
+ 	}
+ 	err = gfs2_inplace_reserve(ip, &ap);
+ 	if (err) {
+ 		ret = block_page_mkwrite_return(err);
  		goto out_quota_unlock;
+ 	}
  
  	rblocks = RES_DINODE + ind_blocks;
  	if (gfs2_is_jdata(ip))
@@@ -463,34 -504,35 +482,54 @@@
  		rblocks += RES_STATFS + RES_QUOTA;
  		rblocks += gfs2_rg_blocks(ip, data_blocks + ind_blocks);
  	}
- 	ret = gfs2_trans_begin(sdp, rblocks, 0);
- 	if (ret)
+ 	err = gfs2_trans_begin(sdp, rblocks, 0);
+ 	if (err) {
+ 		ret = block_page_mkwrite_return(err);
  		goto out_trans_fail;
+ 	}
  
  	lock_page(page);
++<<<<<<< HEAD
 +	ret = -EINVAL;
 +	size = i_size_read(inode);
 +	last_index = (size - 1) >> PAGE_SHIFT;
 +	/* Check page index against inode size */
 +	if (size == 0 || (page->index > last_index))
 +		goto out_trans_end;
 +
 +	ret = -EAGAIN;
++=======
++>>>>>>> 0fc3bcd6b6e3 (gfs2: Clean up the error handling in gfs2_page_mkwrite)
  	/* If truncated, we must retry the operation, we may have raced
  	 * with the glock demotion code.
  	 */
- 	if (!PageUptodate(page) || page->mapping != inode->i_mapping)
+ 	if (!PageUptodate(page) || page->mapping != inode->i_mapping) {
+ 		ret = VM_FAULT_NOPAGE;
  		goto out_trans_end;
+ 	}
  
  	/* Unstuff, if required, and allocate backing blocks for page */
++<<<<<<< HEAD
 +	ret = 0;
 +	if (gfs2_is_stuffed(ip))
 +		ret = gfs2_unstuff_dinode(ip, page);
 +	if (ret == 0)
 +		ret = gfs2_allocate_page_backing(page, PAGE_SIZE);
++=======
+ 	if (gfs2_is_stuffed(ip)) {
+ 		err = gfs2_unstuff_dinode(ip, page);
+ 		if (err) {
+ 			ret = block_page_mkwrite_return(err);
+ 			goto out_trans_end;
+ 		}
+ 	}
+ 	err = gfs2_allocate_page_backing(page, length);
+ 	if (err)
+ 		ret = block_page_mkwrite_return(err);
++>>>>>>> 0fc3bcd6b6e3 (gfs2: Clean up the error handling in gfs2_page_mkwrite)
  
  out_trans_end:
- 	if (ret)
+ 	if (ret != VM_FAULT_LOCKED)
  		unlock_page(page);
  	gfs2_trans_end(sdp);
  out_trans_fail:
@@@ -506,11 -548,32 +545,11 @@@ out_uninit
  		wait_for_stable_page(page);
  	}
  	sb_end_pagefault(inode->i_sb);
- 	return block_page_mkwrite_return(ret);
+ 	return ret;
  }
  
 -static vm_fault_t gfs2_fault(struct vm_fault *vmf)
 -{
 -	struct inode *inode = file_inode(vmf->vma->vm_file);
 -	struct gfs2_inode *ip = GFS2_I(inode);
 -	struct gfs2_holder gh;
 -	vm_fault_t ret;
 -	int err;
 -
 -	gfs2_holder_init(ip->i_gl, LM_ST_SHARED, 0, &gh);
 -	err = gfs2_glock_nq(&gh);
 -	if (err) {
 -		ret = block_page_mkwrite_return(err);
 -		goto out_uninit;
 -	}
 -	ret = filemap_fault(vmf);
 -	gfs2_glock_dq(&gh);
 -out_uninit:
 -	gfs2_holder_uninit(&gh);
 -	return ret;
 -}
 -
  static const struct vm_operations_struct gfs2_vm_ops = {
 -	.fault = gfs2_fault,
 +	.fault = filemap_fault,
  	.map_pages = filemap_map_pages,
  	.page_mkwrite = gfs2_page_mkwrite,
  };
* Unmerged path fs/gfs2/file.c
