x86/fpu: Add kernel_fpu_begin_mask() to selectively initialize state

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Andy Lutomirski <luto@kernel.org>
commit e45122893a9870813f9bd7b4add4f613e6f29008
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/e4512289.failed

Currently, requesting kernel FPU access doesn't distinguish which parts of
the extended ("FPU") state are needed.  This is nice for simplicity, but
there are a few cases in which it's suboptimal:

 - The vast majority of in-kernel FPU users want XMM/YMM/ZMM state but do
   not use legacy 387 state.  These users want MXCSR initialized but don't
   care about the FPU control word.  Skipping FNINIT would save time.
   (Empirically, FNINIT is several times slower than LDMXCSR.)

 - Code that wants MMX doesn't want or need MXCSR initialized.
   _mmx_memcpy(), for example, can run before CR4.OSFXSR gets set, and
   initializing MXCSR will fail because LDMXCSR generates an #UD when the
   aforementioned CR4 bit is not set.

 - Any future in-kernel users of XFD (eXtended Feature Disable)-capable
   dynamic states will need special handling.

Add a more specific API that allows callers to specify exactly what they
want.

	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Tested-by: Krzysztof Piotr OlÄ™dzki <ole@ans.pl>
Link: https://lkml.kernel.org/r/aff1cac8b8fc7ee900cf73e8f2369966621b053f.1611205691.git.luto@kernel.org
(cherry picked from commit e45122893a9870813f9bd7b4add4f613e6f29008)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/core.c
diff --cc arch/x86/kernel/fpu/core.c
index 00fba2554663,571220ac8bea..000000000000
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@@ -87,39 -82,73 +87,77 @@@ bool irq_fpu_usable(void
  }
  EXPORT_SYMBOL(irq_fpu_usable);
  
 -/*
 - * These must be called with preempt disabled. Returns
 - * 'true' if the FPU state is still intact and we can
 - * keep registers active.
 - *
 - * The legacy FNSAVE instruction cleared all FPU state
 - * unconditionally, so registers are essentially destroyed.
 - * Modern FPU state can be kept in registers, if there are
 - * no pending FP exceptions.
 - */
 -int copy_fpregs_to_fpstate(struct fpu *fpu)
 +static void __kernel_fpu_begin(void)
  {
++<<<<<<< HEAD
 +	struct fpu *fpu = &current->thread.fpu;
++=======
+ 	if (likely(use_xsave())) {
+ 		copy_xregs_to_kernel(&fpu->state.xsave);
+ 
+ 		/*
+ 		 * AVX512 state is tracked here because its use is
+ 		 * known to slow the max clock speed of the core.
+ 		 */
+ 		if (fpu->state.xsave.header.xfeatures & XFEATURE_MASK_AVX512)
+ 			fpu->avx512_timestamp = jiffies;
+ 		return 1;
+ 	}
+ 
+ 	if (likely(use_fxsr())) {
+ 		copy_fxregs_to_kernel(fpu);
+ 		return 1;
+ 	}
+ 
+ 	/*
+ 	 * Legacy FPU register saving, FNSAVE always clears FPU registers,
+ 	 * so we have to mark them inactive:
+ 	 */
+ 	asm volatile("fnsave %[fp]; fwait" : [fp] "=m" (fpu->state.fsave));
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL(copy_fpregs_to_fpstate);
+ 
+ void kernel_fpu_begin_mask(unsigned int kfpu_mask)
+ {
+ 	preempt_disable();
++>>>>>>> e45122893a98 (x86/fpu: Add kernel_fpu_begin_mask() to selectively initialize state)
  
  	WARN_ON_FPU(!irq_fpu_usable());
 -	WARN_ON_FPU(this_cpu_read(in_kernel_fpu));
  
 -	this_cpu_write(in_kernel_fpu, true);
 +	kernel_fpu_disable();
  
 -	if (!(current->flags & PF_KTHREAD) &&
 -	    !test_thread_flag(TIF_NEED_FPU_LOAD)) {
 -		set_thread_flag(TIF_NEED_FPU_LOAD);
 -		/*
 -		 * Ignore return value -- we don't care if reg state
 -		 * is clobbered.
 -		 */
 -		copy_fpregs_to_fpstate(&current->thread.fpu);
 +	if (current->mm) {
 +		if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
 +			set_thread_flag(TIF_NEED_FPU_LOAD);
 +			/*
 +			 * Ignore return value -- we don't care if reg state
 +			 * is clobbered.
 +			 */
 +			copy_fpregs_to_fpstate(fpu);
 +		}
  	}
  	__cpu_invalidate_fpregs_state();
  
- 	if (boot_cpu_has(X86_FEATURE_XMM))
+ 	/* Put sane initial values into the control registers. */
+ 	if (likely(kfpu_mask & KFPU_MXCSR) && boot_cpu_has(X86_FEATURE_XMM))
  		ldmxcsr(MXCSR_DEFAULT);
  
- 	if (boot_cpu_has(X86_FEATURE_FPU))
+ 	if (unlikely(kfpu_mask & KFPU_387) && boot_cpu_has(X86_FEATURE_FPU))
  		asm volatile ("fninit");
  }
++<<<<<<< HEAD
 +
 +void kernel_fpu_begin(void)
 +{
 +	preempt_disable();
 +	__kernel_fpu_begin();
 +}
 +EXPORT_SYMBOL_GPL(kernel_fpu_begin);
++=======
+ EXPORT_SYMBOL_GPL(kernel_fpu_begin_mask);
++>>>>>>> e45122893a98 (x86/fpu: Add kernel_fpu_begin_mask() to selectively initialize state)
  
  void kernel_fpu_end(void)
  {
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index dcd9503b1098..38f4936045ab 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -16,14 +16,25 @@
  * Use kernel_fpu_begin/end() if you intend to use FPU in kernel context. It
  * disables preemption so be careful if you intend to use it for long periods
  * of time.
- * If you intend to use the FPU in softirq you need to check first with
+ * If you intend to use the FPU in irq/softirq you need to check first with
  * irq_fpu_usable() if it is possible.
  */
-extern void kernel_fpu_begin(void);
+
+/* Kernel FPU states to initialize in kernel_fpu_begin_mask() */
+#define KFPU_387	_BITUL(0)	/* 387 state will be initialized */
+#define KFPU_MXCSR	_BITUL(1)	/* MXCSR will be initialized */
+
+extern void kernel_fpu_begin_mask(unsigned int kfpu_mask);
 extern void kernel_fpu_end(void);
 extern bool irq_fpu_usable(void);
 extern void fpregs_mark_activate(void);
 
+/* Code that is unaware of kernel_fpu_begin_mask() can use this */
+static inline void kernel_fpu_begin(void)
+{
+	kernel_fpu_begin_mask(KFPU_387 | KFPU_MXCSR);
+}
+
 /*
  * Use fpregs_lock() while editing CPU's FPU registers or fpu->state.
  * A context switch will (and softirq might) save CPU's FPU registers to
* Unmerged path arch/x86/kernel/fpu/core.c
