powerpc/pseries/mobility: handle premature return from H_JOIN

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Nathan Lynch <nathanl@linux.ibm.com>
commit 274cb1ca2e7ce02cab56f5f4c61a74aeb566f931
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/274cb1ca.failed

The pseries join/suspend sequence in its current form was written with
the assumption that it was the only user of H_PROD and that it needn't
handle spurious successful returns from H_JOIN. That's wrong;
powerpc's paravirt spinlock code uses H_PROD, and CPUs entering
do_join() can be woken prematurely from H_JOIN with a status of
H_SUCCESS as a result. This causes all CPUs to exit the sequence
early, preventing suspend from occurring at all.

Add a 'done' boolean flag to the pseries_suspend_info struct, and have
the waking thread set it before waking the other threads. Threads
which receive H_SUCCESS from H_JOIN retry if the 'done' flag is still
unset.

Fixes: 9327dc0aeef3 ("powerpc/pseries/mobility: use stop_machine for join/suspend")
	Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20210315080045.460331-3-nathanl@linux.ibm.com
(cherry picked from commit 274cb1ca2e7ce02cab56f5f4c61a74aeb566f931)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/pseries/mobility.c
diff --cc arch/powerpc/platforms/pseries/mobility.c
index 5e9fd6f27ae0,e83e0891272d..000000000000
--- a/arch/powerpc/platforms/pseries/mobility.c
+++ b/arch/powerpc/platforms/pseries/mobility.c
@@@ -331,6 -332,319 +331,322 @@@ void post_mobility_fixup(void
  	return;
  }
  
++<<<<<<< HEAD
++=======
+ static int poll_vasi_state(u64 handle, unsigned long *res)
+ {
+ 	unsigned long retbuf[PLPAR_HCALL_BUFSIZE];
+ 	long hvrc;
+ 	int ret;
+ 
+ 	hvrc = plpar_hcall(H_VASI_STATE, retbuf, handle);
+ 	switch (hvrc) {
+ 	case H_SUCCESS:
+ 		ret = 0;
+ 		*res = retbuf[0];
+ 		break;
+ 	case H_PARAMETER:
+ 		ret = -EINVAL;
+ 		break;
+ 	case H_FUNCTION:
+ 		ret = -EOPNOTSUPP;
+ 		break;
+ 	case H_HARDWARE:
+ 	default:
+ 		pr_err("unexpected H_VASI_STATE result %ld\n", hvrc);
+ 		ret = -EIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int wait_for_vasi_session_suspending(u64 handle)
+ {
+ 	unsigned long state;
+ 	int ret;
+ 
+ 	/*
+ 	 * Wait for transition from H_VASI_ENABLED to
+ 	 * H_VASI_SUSPENDING. Treat anything else as an error.
+ 	 */
+ 	while (true) {
+ 		ret = poll_vasi_state(handle, &state);
+ 
+ 		if (ret != 0 || state == H_VASI_SUSPENDING) {
+ 			break;
+ 		} else if (state == H_VASI_ENABLED) {
+ 			ssleep(1);
+ 		} else {
+ 			pr_err("unexpected H_VASI_STATE result %lu\n", state);
+ 			ret = -EIO;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Proceed even if H_VASI_STATE is unavailable. If H_JOIN or
+ 	 * ibm,suspend-me are also unimplemented, we'll recover then.
+ 	 */
+ 	if (ret == -EOPNOTSUPP)
+ 		ret = 0;
+ 
+ 	return ret;
+ }
+ 
+ static void prod_single(unsigned int target_cpu)
+ {
+ 	long hvrc;
+ 	int hwid;
+ 
+ 	hwid = get_hard_smp_processor_id(target_cpu);
+ 	hvrc = plpar_hcall_norets(H_PROD, hwid);
+ 	if (hvrc == H_SUCCESS)
+ 		return;
+ 	pr_err_ratelimited("H_PROD of CPU %u (hwid %d) error: %ld\n",
+ 			   target_cpu, hwid, hvrc);
+ }
+ 
+ static void prod_others(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	for_each_online_cpu(cpu) {
+ 		if (cpu != smp_processor_id())
+ 			prod_single(cpu);
+ 	}
+ }
+ 
+ static u16 clamp_slb_size(void)
+ {
+ 	u16 prev = mmu_slb_size;
+ 
+ 	slb_set_size(SLB_MIN_SIZE);
+ 
+ 	return prev;
+ }
+ 
+ static int do_suspend(void)
+ {
+ 	u16 saved_slb_size;
+ 	int status;
+ 	int ret;
+ 
+ 	pr_info("calling ibm,suspend-me on CPU %i\n", smp_processor_id());
+ 
+ 	/*
+ 	 * The destination processor model may have fewer SLB entries
+ 	 * than the source. We reduce mmu_slb_size to a safe minimum
+ 	 * before suspending in order to minimize the possibility of
+ 	 * programming non-existent entries on the destination. If
+ 	 * suspend fails, we restore it before returning. On success
+ 	 * the OF reconfig path will update it from the new device
+ 	 * tree after resuming on the destination.
+ 	 */
+ 	saved_slb_size = clamp_slb_size();
+ 
+ 	ret = rtas_ibm_suspend_me(&status);
+ 	if (ret != 0) {
+ 		pr_err("ibm,suspend-me error: %d\n", status);
+ 		slb_set_size(saved_slb_size);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * struct pseries_suspend_info - State shared between CPUs for join/suspend.
+  * @counter: Threads are to increment this upon resuming from suspend
+  *           or if an error is received from H_JOIN. The thread which performs
+  *           the first increment (i.e. sets it to 1) is responsible for
+  *           waking the other threads.
+  * @done: False if join/suspend is in progress. True if the operation is
+  *        complete (successful or not).
+  */
+ struct pseries_suspend_info {
+ 	atomic_t counter;
+ 	bool done;
+ };
+ 
+ static int do_join(void *arg)
+ {
+ 	struct pseries_suspend_info *info = arg;
+ 	atomic_t *counter = &info->counter;
+ 	long hvrc;
+ 	int ret;
+ 
+ retry:
+ 	/* Must ensure MSR.EE off for H_JOIN. */
+ 	hard_irq_disable();
+ 	hvrc = plpar_hcall_norets(H_JOIN);
+ 
+ 	switch (hvrc) {
+ 	case H_CONTINUE:
+ 		/*
+ 		 * All other CPUs are offline or in H_JOIN. This CPU
+ 		 * attempts the suspend.
+ 		 */
+ 		ret = do_suspend();
+ 		break;
+ 	case H_SUCCESS:
+ 		/*
+ 		 * The suspend is complete and this cpu has received a
+ 		 * prod, or we've received a stray prod from unrelated
+ 		 * code (e.g. paravirt spinlocks) and we need to join
+ 		 * again.
+ 		 *
+ 		 * This barrier orders the return from H_JOIN above vs
+ 		 * the load of info->done. It pairs with the barrier
+ 		 * in the wakeup/prod path below.
+ 		 */
+ 		smp_mb();
+ 		if (READ_ONCE(info->done) == false) {
+ 			pr_info_ratelimited("premature return from H_JOIN on CPU %i, retrying",
+ 					    smp_processor_id());
+ 			goto retry;
+ 		}
+ 		ret = 0;
+ 		break;
+ 	case H_BAD_MODE:
+ 	case H_HARDWARE:
+ 	default:
+ 		ret = -EIO;
+ 		pr_err_ratelimited("H_JOIN error %ld on CPU %i\n",
+ 				   hvrc, smp_processor_id());
+ 		break;
+ 	}
+ 
+ 	if (atomic_inc_return(counter) == 1) {
+ 		pr_info("CPU %u waking all threads\n", smp_processor_id());
+ 		WRITE_ONCE(info->done, true);
+ 		/*
+ 		 * This barrier orders the store to info->done vs subsequent
+ 		 * H_PRODs to wake the other CPUs. It pairs with the barrier
+ 		 * in the H_SUCCESS case above.
+ 		 */
+ 		smp_mb();
+ 		prod_others();
+ 	}
+ 	/*
+ 	 * Execution may have been suspended for several seconds, so
+ 	 * reset the watchdog.
+ 	 */
+ 	touch_nmi_watchdog();
+ 	return ret;
+ }
+ 
+ /*
+  * Abort reason code byte 0. We use only the 'Migrating partition' value.
+  */
+ enum vasi_aborting_entity {
+ 	ORCHESTRATOR        = 1,
+ 	VSP_SOURCE          = 2,
+ 	PARTITION_FIRMWARE  = 3,
+ 	PLATFORM_FIRMWARE   = 4,
+ 	VSP_TARGET          = 5,
+ 	MIGRATING_PARTITION = 6,
+ };
+ 
+ static void pseries_cancel_migration(u64 handle, int err)
+ {
+ 	u32 reason_code;
+ 	u32 detail;
+ 	u8 entity;
+ 	long hvrc;
+ 
+ 	entity = MIGRATING_PARTITION;
+ 	detail = abs(err) & 0xffffff;
+ 	reason_code = (entity << 24) | detail;
+ 
+ 	hvrc = plpar_hcall_norets(H_VASI_SIGNAL, handle,
+ 				  H_VASI_SIGNAL_CANCEL, reason_code);
+ 	if (hvrc)
+ 		pr_err("H_VASI_SIGNAL error: %ld\n", hvrc);
+ }
+ 
+ static int pseries_suspend(u64 handle)
+ {
+ 	const unsigned int max_attempts = 5;
+ 	unsigned int retry_interval_ms = 1;
+ 	unsigned int attempt = 1;
+ 	int ret;
+ 
+ 	while (true) {
+ 		struct pseries_suspend_info info;
+ 		unsigned long vasi_state;
+ 		int vasi_err;
+ 
+ 		info = (struct pseries_suspend_info) {
+ 			.counter = ATOMIC_INIT(0),
+ 			.done = false,
+ 		};
+ 
+ 		ret = stop_machine(do_join, &info, cpu_online_mask);
+ 		if (ret == 0)
+ 			break;
+ 		/*
+ 		 * Encountered an error. If the VASI stream is still
+ 		 * in Suspending state, it's likely a transient
+ 		 * condition related to some device in the partition
+ 		 * and we can retry in the hope that the cause has
+ 		 * cleared after some delay.
+ 		 *
+ 		 * A better design would allow drivers etc to prepare
+ 		 * for the suspend and avoid conditions which prevent
+ 		 * the suspend from succeeding. For now, we have this
+ 		 * mitigation.
+ 		 */
+ 		pr_notice("Partition suspend attempt %u of %u error: %d\n",
+ 			  attempt, max_attempts, ret);
+ 
+ 		if (attempt == max_attempts)
+ 			break;
+ 
+ 		vasi_err = poll_vasi_state(handle, &vasi_state);
+ 		if (vasi_err == 0) {
+ 			if (vasi_state != H_VASI_SUSPENDING) {
+ 				pr_notice("VASI state %lu after failed suspend\n",
+ 					  vasi_state);
+ 				break;
+ 			}
+ 		} else if (vasi_err != -EOPNOTSUPP) {
+ 			pr_err("VASI state poll error: %d", vasi_err);
+ 			break;
+ 		}
+ 
+ 		pr_notice("Will retry partition suspend after %u ms\n",
+ 			  retry_interval_ms);
+ 
+ 		msleep(retry_interval_ms);
+ 		retry_interval_ms *= 10;
+ 		attempt++;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int pseries_migrate_partition(u64 handle)
+ {
+ 	int ret;
+ 
+ 	ret = wait_for_vasi_session_suspending(handle);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = pseries_suspend(handle);
+ 	if (ret == 0)
+ 		post_mobility_fixup();
+ 	else
+ 		pseries_cancel_migration(handle, ret);
+ 
+ 	return ret;
+ }
+ 
+ int rtas_syscall_dispatch_ibm_suspend_me(u64 handle)
+ {
+ 	return pseries_migrate_partition(handle);
+ }
+ 
++>>>>>>> 274cb1ca2e7c (powerpc/pseries/mobility: handle premature return from H_JOIN)
  static ssize_t migration_store(struct class *class,
  			       struct class_attribute *attr, const char *buf,
  			       size_t count)
* Unmerged path arch/powerpc/platforms/pseries/mobility.c
