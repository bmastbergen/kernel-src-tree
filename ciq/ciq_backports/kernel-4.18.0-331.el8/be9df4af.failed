net, xdp: Introduce xdp_prepare_buff utility routine

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Lorenzo Bianconi <lorenzo@kernel.org>
commit be9df4aff65f18caa79b35f88f42c3d5a43af14f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/be9df4af.failed

Introduce xdp_prepare_buff utility routine to initialize per-descriptor
xdp_buff fields (e.g. xdp_buff pointers). Rely on xdp_prepare_buff() in
all XDP capable drivers.

	Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Reviewed-by: Alexander Duyck <alexanderduyck@fb.com>
	Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
	Acked-by: Shay Agroskin <shayagr@amazon.com>
	Acked-by: Martin Habets <habetsm.xilinx@gmail.com>
	Acked-by: Camelia Groza <camelia.groza@nxp.com>
	Acked-by: Marcin Wojtas <mw@semihalf.com>
Link: https://lore.kernel.org/bpf/45f46f12295972a97da8ca01990b3e71501e9d89.1608670965.git.lorenzo@kernel.org
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit be9df4aff65f18caa79b35f88f42c3d5a43af14f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
#	drivers/net/ethernet/cavium/thunder/nicvf_main.c
#	drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
#	drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
#	drivers/net/ethernet/intel/ice/ice_txrx.c
#	drivers/net/ethernet/marvell/mvneta.c
#	drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/qlogic/qede/qede_fp.c
#	drivers/net/ethernet/sfc/rx.c
#	drivers/net/ethernet/socionext/netsec.c
#	drivers/net/ethernet/ti/cpsw.c
#	drivers/net/ethernet/ti/cpsw_new.c
#	drivers/net/hyperv/netvsc_bpf.c
#	drivers/net/tun.c
#	drivers/net/veth.c
#	drivers/net/virtio_net.c
#	drivers/net/xen-netfront.c
#	include/net/xdp.h
#	net/bpf/test_run.c
#	net/core/dev.c
diff --cc drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
index 2704a4709bc7,641303894341..000000000000
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
@@@ -133,12 -133,9 +133,18 @@@ bool bnxt_rx_xdp(struct bnxt *bp, struc
  	dma_sync_single_for_cpu(&pdev->dev, mapping + offset, *len, bp->rx_dir);
  
  	txr = rxr->bnapi->tx_ring;
++<<<<<<< HEAD
 +	xdp.data_hard_start = *data_ptr - offset;
 +	xdp.data = *data_ptr;
 +	xdp_set_data_meta_invalid(&xdp);
 +	xdp.data_end = *data_ptr + *len;
 +	xdp.rxq = &rxr->xdp_rxq;
 +	xdp.frame_sz = PAGE_SIZE; /* BNXT_RX_PAGE_MODE(bp) when XDP enabled */
++=======
+ 	/* BNXT_RX_PAGE_MODE(bp) when XDP enabled */
+ 	xdp_init_buff(&xdp, PAGE_SIZE, &rxr->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, *data_ptr - offset, offset, *len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  	orig_data = xdp.data;
  
  	rcu_read_lock();
diff --cc drivers/net/ethernet/cavium/thunder/nicvf_main.c
index 3a2bd53c6a5f,c33b4e837515..000000000000
--- a/drivers/net/ethernet/cavium/thunder/nicvf_main.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
@@@ -544,12 -548,11 +545,20 @@@ static inline bool nicvf_xdp_rx(struct 
  	cpu_addr = (u64)phys_to_virt(cpu_addr);
  	page = virt_to_page((void *)cpu_addr);
  
++<<<<<<< HEAD
 +	xdp.data_hard_start = page_address(page);
 +	xdp.data = (void *)cpu_addr;
 +	xdp_set_data_meta_invalid(&xdp);
 +	xdp.data_end = xdp.data + len;
 +	xdp.rxq = &rq->xdp_rxq;
 +	xdp.frame_sz = RCV_FRAG_LEN + XDP_PACKET_HEADROOM;
++=======
+ 	xdp_init_buff(&xdp, RCV_FRAG_LEN + XDP_PACKET_HEADROOM,
+ 		      &rq->xdp_rxq);
+ 	hard_start = page_address(page);
+ 	data = (unsigned char *)cpu_addr;
+ 	xdp_prepare_buff(&xdp, hard_start, data - hard_start, len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  	orig_data = xdp.data;
  
  	rcu_read_lock();
diff --cc drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
index 15fe3d780457,d8e568f6caf3..000000000000
--- a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
@@@ -2224,10 -2441,182 +2224,181 @@@ static enum qman_cb_dqrr_result rx_erro
  	return qman_cb_dqrr_consume;
  }
  
++<<<<<<< HEAD
++=======
+ static int dpaa_xdp_xmit_frame(struct net_device *net_dev,
+ 			       struct xdp_frame *xdpf)
+ {
+ 	struct dpaa_priv *priv = netdev_priv(net_dev);
+ 	struct rtnl_link_stats64 *percpu_stats;
+ 	struct dpaa_percpu_priv *percpu_priv;
+ 	struct dpaa_eth_swbp *swbp;
+ 	struct netdev_queue *txq;
+ 	void *buff_start;
+ 	struct qm_fd fd;
+ 	dma_addr_t addr;
+ 	int err;
+ 
+ 	percpu_priv = this_cpu_ptr(priv->percpu_priv);
+ 	percpu_stats = &percpu_priv->stats;
+ 
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 	if (unlikely(fman_has_errata_a050385())) {
+ 		if (dpaa_a050385_wa_xdpf(priv, &xdpf)) {
+ 			err = -ENOMEM;
+ 			goto out_error;
+ 		}
+ 	}
+ #endif
+ 
+ 	if (xdpf->headroom < DPAA_TX_PRIV_DATA_SIZE) {
+ 		err = -EINVAL;
+ 		goto out_error;
+ 	}
+ 
+ 	buff_start = xdpf->data - xdpf->headroom;
+ 
+ 	/* Leave empty the skb backpointer at the start of the buffer.
+ 	 * Save the XDP frame for easy cleanup on confirmation.
+ 	 */
+ 	swbp = (struct dpaa_eth_swbp *)buff_start;
+ 	swbp->skb = NULL;
+ 	swbp->xdpf = xdpf;
+ 
+ 	qm_fd_clear_fd(&fd);
+ 	fd.bpid = FSL_DPAA_BPID_INV;
+ 	fd.cmd |= cpu_to_be32(FM_FD_CMD_FCO);
+ 	qm_fd_set_contig(&fd, xdpf->headroom, xdpf->len);
+ 
+ 	addr = dma_map_single(priv->tx_dma_dev, buff_start,
+ 			      xdpf->headroom + xdpf->len,
+ 			      DMA_TO_DEVICE);
+ 	if (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {
+ 		err = -EINVAL;
+ 		goto out_error;
+ 	}
+ 
+ 	qm_fd_addr_set64(&fd, addr);
+ 
+ 	/* Bump the trans_start */
+ 	txq = netdev_get_tx_queue(net_dev, smp_processor_id());
+ 	txq->trans_start = jiffies;
+ 
+ 	err = dpaa_xmit(priv, percpu_stats, smp_processor_id(), &fd);
+ 	if (err) {
+ 		dma_unmap_single(priv->tx_dma_dev, addr,
+ 				 qm_fd_get_offset(&fd) + qm_fd_get_length(&fd),
+ 				 DMA_TO_DEVICE);
+ 		goto out_error;
+ 	}
+ 
+ 	return 0;
+ 
+ out_error:
+ 	percpu_stats->tx_errors++;
+ 	return err;
+ }
+ 
+ static u32 dpaa_run_xdp(struct dpaa_priv *priv, struct qm_fd *fd, void *vaddr,
+ 			struct dpaa_fq *dpaa_fq, unsigned int *xdp_meta_len)
+ {
+ 	ssize_t fd_off = qm_fd_get_offset(fd);
+ 	struct bpf_prog *xdp_prog;
+ 	struct xdp_frame *xdpf;
+ 	struct xdp_buff xdp;
+ 	u32 xdp_act;
+ 	int err;
+ 
+ 	rcu_read_lock();
+ 
+ 	xdp_prog = READ_ONCE(priv->xdp_prog);
+ 	if (!xdp_prog) {
+ 		rcu_read_unlock();
+ 		return XDP_PASS;
+ 	}
+ 
+ 	xdp_init_buff(&xdp, DPAA_BP_RAW_SIZE - DPAA_TX_PRIV_DATA_SIZE,
+ 		      &dpaa_fq->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, vaddr + fd_off - XDP_PACKET_HEADROOM,
+ 			 XDP_PACKET_HEADROOM, qm_fd_get_length(fd), true);
+ 
+ 	/* We reserve a fixed headroom of 256 bytes under the erratum and we
+ 	 * offer it all to XDP programs to use. If no room is left for the
+ 	 * xdpf backpointer on TX, we will need to copy the data.
+ 	 * Disable metadata support since data realignments might be required
+ 	 * and the information can be lost.
+ 	 */
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 	if (unlikely(fman_has_errata_a050385())) {
+ 		xdp_set_data_meta_invalid(&xdp);
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 	}
+ #endif
+ 
+ 	xdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);
+ 
+ 	/* Update the length and the offset of the FD */
+ 	qm_fd_set_contig(fd, xdp.data - vaddr, xdp.data_end - xdp.data);
+ 
+ 	switch (xdp_act) {
+ 	case XDP_PASS:
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 		*xdp_meta_len = xdp_data_meta_unsupported(&xdp) ? 0 :
+ 				xdp.data - xdp.data_meta;
+ #else
+ 		*xdp_meta_len = xdp.data - xdp.data_meta;
+ #endif
+ 		break;
+ 	case XDP_TX:
+ 		/* We can access the full headroom when sending the frame
+ 		 * back out
+ 		 */
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 		xdpf = xdp_convert_buff_to_frame(&xdp);
+ 		if (unlikely(!xdpf)) {
+ 			free_pages((unsigned long)vaddr, 0);
+ 			break;
+ 		}
+ 
+ 		if (dpaa_xdp_xmit_frame(priv->net_dev, xdpf))
+ 			xdp_return_frame_rx_napi(xdpf);
+ 
+ 		break;
+ 	case XDP_REDIRECT:
+ 		/* Allow redirect to use the full headroom */
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 
+ 		err = xdp_do_redirect(priv->net_dev, &xdp, xdp_prog);
+ 		if (err) {
+ 			trace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);
+ 			free_pages((unsigned long)vaddr, 0);
+ 		}
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(xdp_act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		/* Free the buffer */
+ 		free_pages((unsigned long)vaddr, 0);
+ 		break;
+ 	}
+ 
+ 	rcu_read_unlock();
+ 
+ 	return xdp_act;
+ }
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  static enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,
  						struct qman_fq *fq,
 -						const struct qm_dqrr_entry *dq,
 -						bool sched_napi)
 +						const struct qm_dqrr_entry *dq)
  {
 -	bool ts_valid = false, hash_valid = false;
 -	struct skb_shared_hwtstamps *shhwtstamps;
 -	unsigned int skb_len, xdp_meta_len = 0;
  	struct rtnl_link_stats64 *percpu_stats;
  	struct dpaa_percpu_priv *percpu_priv;
  	const struct qm_fd *fd = &dq->fd;
diff --cc drivers/net/ethernet/intel/ice/ice_txrx.c
index 7667343b3cdc,422f53997c02..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@@ -1091,8 -1103,12 +1091,14 @@@ int ice_clean_rx_irq(struct ice_ring *r
  
  	/* start the loop to process Rx packets bounded by 'budget' */
  	while (likely(total_rx_pkts < (unsigned int)budget)) {
+ 		unsigned int offset = ice_rx_offset(rx_ring);
  		union ice_32b_rx_flex_desc *rx_desc;
  		struct ice_rx_buf *rx_buf;
++<<<<<<< HEAD
++=======
+ 		unsigned char *hard_start;
+ 		struct sk_buff *skb;
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  		unsigned int size;
  		u16 stat_err_bits;
  		int rx_buf_pgcnt;
diff --cc drivers/net/ethernet/marvell/mvneta.c
index aa243112d05c,6290bfb6494e..000000000000
--- a/drivers/net/ethernet/marvell/mvneta.c
+++ b/drivers/net/ethernet/marvell/mvneta.c
@@@ -1894,11 -1973,378 +1894,381 @@@ static void mvneta_rxq_drop_pkts(struc
  	for (i = 0; i < rxq->size; i++) {
  		struct mvneta_rx_desc *rx_desc = rxq->descs + i;
  		void *data = rxq->buf_virt_addr[i];
 -		if (!data || !(rx_desc->buf_phys_addr))
 -			continue;
  
 -		page_pool_put_full_page(rxq->page_pool, data, false);
 +		dma_unmap_single(pp->dev->dev.parent, rx_desc->buf_phys_addr,
 +				 MVNETA_RX_BUF_SIZE(pp->pkt_size), DMA_FROM_DEVICE);
 +		mvneta_frag_free(pp->frag_size, data);
  	}
++<<<<<<< HEAD
++=======
+ 	if (xdp_rxq_info_is_reg(&rxq->xdp_rxq))
+ 		xdp_rxq_info_unreg(&rxq->xdp_rxq);
+ 	page_pool_destroy(rxq->page_pool);
+ 	rxq->page_pool = NULL;
+ }
+ 
+ static void
+ mvneta_update_stats(struct mvneta_port *pp,
+ 		    struct mvneta_stats *ps)
+ {
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->es.ps.rx_packets += ps->rx_packets;
+ 	stats->es.ps.rx_bytes += ps->rx_bytes;
+ 	/* xdp */
+ 	stats->es.ps.xdp_redirect += ps->xdp_redirect;
+ 	stats->es.ps.xdp_pass += ps->xdp_pass;
+ 	stats->es.ps.xdp_drop += ps->xdp_drop;
+ 	u64_stats_update_end(&stats->syncp);
+ }
+ 
+ static inline
+ int mvneta_rx_refill_queue(struct mvneta_port *pp, struct mvneta_rx_queue *rxq)
+ {
+ 	struct mvneta_rx_desc *rx_desc;
+ 	int curr_desc = rxq->first_to_refill;
+ 	int i;
+ 
+ 	for (i = 0; (i < rxq->refill_num) && (i < 64); i++) {
+ 		rx_desc = rxq->descs + curr_desc;
+ 		if (!(rx_desc->buf_phys_addr)) {
+ 			if (mvneta_rx_refill(pp, rx_desc, rxq, GFP_ATOMIC)) {
+ 				struct mvneta_pcpu_stats *stats;
+ 
+ 				pr_err("Can't refill queue %d. Done %d from %d\n",
+ 				       rxq->id, i, rxq->refill_num);
+ 
+ 				stats = this_cpu_ptr(pp->stats);
+ 				u64_stats_update_begin(&stats->syncp);
+ 				stats->es.refill_error++;
+ 				u64_stats_update_end(&stats->syncp);
+ 				break;
+ 			}
+ 		}
+ 		curr_desc = MVNETA_QUEUE_NEXT_DESC(rxq, curr_desc);
+ 	}
+ 	rxq->refill_num -= i;
+ 	rxq->first_to_refill = curr_desc;
+ 
+ 	return i;
+ }
+ 
+ static void
+ mvneta_xdp_put_buff(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,
+ 		    struct xdp_buff *xdp, struct skb_shared_info *sinfo,
+ 		    int sync_len)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < sinfo->nr_frags; i++)
+ 		page_pool_put_full_page(rxq->page_pool,
+ 					skb_frag_page(&sinfo->frags[i]), true);
+ 	page_pool_put_page(rxq->page_pool, virt_to_head_page(xdp->data),
+ 			   sync_len, true);
+ }
+ 
+ static int
+ mvneta_xdp_submit_frame(struct mvneta_port *pp, struct mvneta_tx_queue *txq,
+ 			struct xdp_frame *xdpf, bool dma_map)
+ {
+ 	struct mvneta_tx_desc *tx_desc;
+ 	struct mvneta_tx_buf *buf;
+ 	dma_addr_t dma_addr;
+ 
+ 	if (txq->count >= txq->tx_stop_threshold)
+ 		return MVNETA_XDP_DROPPED;
+ 
+ 	tx_desc = mvneta_txq_next_desc_get(txq);
+ 
+ 	buf = &txq->buf[txq->txq_put_index];
+ 	if (dma_map) {
+ 		/* ndo_xdp_xmit */
+ 		dma_addr = dma_map_single(pp->dev->dev.parent, xdpf->data,
+ 					  xdpf->len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(pp->dev->dev.parent, dma_addr)) {
+ 			mvneta_txq_desc_put(txq);
+ 			return MVNETA_XDP_DROPPED;
+ 		}
+ 		buf->type = MVNETA_TYPE_XDP_NDO;
+ 	} else {
+ 		struct page *page = virt_to_page(xdpf->data);
+ 
+ 		dma_addr = page_pool_get_dma_addr(page) +
+ 			   sizeof(*xdpf) + xdpf->headroom;
+ 		dma_sync_single_for_device(pp->dev->dev.parent, dma_addr,
+ 					   xdpf->len, DMA_BIDIRECTIONAL);
+ 		buf->type = MVNETA_TYPE_XDP_TX;
+ 	}
+ 	buf->xdpf = xdpf;
+ 
+ 	tx_desc->command = MVNETA_TXD_FLZ_DESC;
+ 	tx_desc->buf_phys_addr = dma_addr;
+ 	tx_desc->data_size = xdpf->len;
+ 
+ 	mvneta_txq_inc_put(txq);
+ 	txq->pending++;
+ 	txq->count++;
+ 
+ 	return MVNETA_XDP_TX;
+ }
+ 
+ static int
+ mvneta_xdp_xmit_back(struct mvneta_port *pp, struct xdp_buff *xdp)
+ {
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 	struct mvneta_tx_queue *txq;
+ 	struct netdev_queue *nq;
+ 	struct xdp_frame *xdpf;
+ 	int cpu;
+ 	u32 ret;
+ 
+ 	xdpf = xdp_convert_buff_to_frame(xdp);
+ 	if (unlikely(!xdpf))
+ 		return MVNETA_XDP_DROPPED;
+ 
+ 	cpu = smp_processor_id();
+ 	txq = &pp->txqs[cpu % txq_number];
+ 	nq = netdev_get_tx_queue(pp->dev, txq->id);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	ret = mvneta_xdp_submit_frame(pp, txq, xdpf, false);
+ 	if (ret == MVNETA_XDP_TX) {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->es.ps.tx_bytes += xdpf->len;
+ 		stats->es.ps.tx_packets++;
+ 		stats->es.ps.xdp_tx++;
+ 		u64_stats_update_end(&stats->syncp);
+ 
+ 		mvneta_txq_pend_desc_add(pp, txq, 0);
+ 	} else {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->es.ps.xdp_tx_err++;
+ 		u64_stats_update_end(&stats->syncp);
+ 	}
+ 	__netif_tx_unlock(nq);
+ 
+ 	return ret;
+ }
+ 
+ static int
+ mvneta_xdp_xmit(struct net_device *dev, int num_frame,
+ 		struct xdp_frame **frames, u32 flags)
+ {
+ 	struct mvneta_port *pp = netdev_priv(dev);
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 	int i, nxmit_byte = 0, nxmit = num_frame;
+ 	int cpu = smp_processor_id();
+ 	struct mvneta_tx_queue *txq;
+ 	struct netdev_queue *nq;
+ 	u32 ret;
+ 
+ 	if (unlikely(test_bit(__MVNETA_DOWN, &pp->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	txq = &pp->txqs[cpu % txq_number];
+ 	nq = netdev_get_tx_queue(pp->dev, txq->id);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	for (i = 0; i < num_frame; i++) {
+ 		ret = mvneta_xdp_submit_frame(pp, txq, frames[i], true);
+ 		if (ret == MVNETA_XDP_TX) {
+ 			nxmit_byte += frames[i]->len;
+ 		} else {
+ 			xdp_return_frame_rx_napi(frames[i]);
+ 			nxmit--;
+ 		}
+ 	}
+ 
+ 	if (unlikely(flags & XDP_XMIT_FLUSH))
+ 		mvneta_txq_pend_desc_add(pp, txq, 0);
+ 	__netif_tx_unlock(nq);
+ 
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->es.ps.tx_bytes += nxmit_byte;
+ 	stats->es.ps.tx_packets += nxmit;
+ 	stats->es.ps.xdp_xmit += nxmit;
+ 	stats->es.ps.xdp_xmit_err += num_frame - nxmit;
+ 	u64_stats_update_end(&stats->syncp);
+ 
+ 	return nxmit;
+ }
+ 
+ static int
+ mvneta_run_xdp(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,
+ 	       struct bpf_prog *prog, struct xdp_buff *xdp,
+ 	       u32 frame_sz, struct mvneta_stats *stats)
+ {
+ 	struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	unsigned int len, data_len, sync;
+ 	u32 ret, act;
+ 
+ 	len = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;
+ 	data_len = xdp->data_end - xdp->data;
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	/* Due xdp_adjust_tail: DMA sync for_device cover max len CPU touch */
+ 	sync = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;
+ 	sync = max(sync, len);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		stats->xdp_pass++;
+ 		return MVNETA_XDP_PASS;
+ 	case XDP_REDIRECT: {
+ 		int err;
+ 
+ 		err = xdp_do_redirect(pp->dev, xdp, prog);
+ 		if (unlikely(err)) {
+ 			mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 			ret = MVNETA_XDP_DROPPED;
+ 		} else {
+ 			ret = MVNETA_XDP_REDIR;
+ 			stats->xdp_redirect++;
+ 		}
+ 		break;
+ 	}
+ 	case XDP_TX:
+ 		ret = mvneta_xdp_xmit_back(pp, xdp);
+ 		if (ret != MVNETA_XDP_TX)
+ 			mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(pp->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 		ret = MVNETA_XDP_DROPPED;
+ 		stats->xdp_drop++;
+ 		break;
+ 	}
+ 
+ 	stats->rx_bytes += frame_sz + xdp->data_end - xdp->data - data_len;
+ 	stats->rx_packets++;
+ 
+ 	return ret;
+ }
+ 
+ static void
+ mvneta_swbm_rx_frame(struct mvneta_port *pp,
+ 		     struct mvneta_rx_desc *rx_desc,
+ 		     struct mvneta_rx_queue *rxq,
+ 		     struct xdp_buff *xdp, int *size,
+ 		     struct page *page)
+ {
+ 	unsigned char *data = page_address(page);
+ 	int data_len = -MVNETA_MH_SIZE, len;
+ 	struct net_device *dev = pp->dev;
+ 	enum dma_data_direction dma_dir;
+ 	struct skb_shared_info *sinfo;
+ 
+ 	if (*size > MVNETA_MAX_RX_BUF_SIZE) {
+ 		len = MVNETA_MAX_RX_BUF_SIZE;
+ 		data_len += len;
+ 	} else {
+ 		len = *size;
+ 		data_len += len - ETH_FCS_LEN;
+ 	}
+ 	*size = *size - len;
+ 
+ 	dma_dir = page_pool_get_dma_dir(rxq->page_pool);
+ 	dma_sync_single_for_cpu(dev->dev.parent,
+ 				rx_desc->buf_phys_addr,
+ 				len, dma_dir);
+ 
+ 	rx_desc->buf_phys_addr = 0;
+ 
+ 	/* Prefetch header */
+ 	prefetch(data);
+ 	xdp_prepare_buff(xdp, data, pp->rx_offset_correction + MVNETA_MH_SIZE,
+ 			 data_len, false);
+ 
+ 	sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	sinfo->nr_frags = 0;
+ }
+ 
+ static void
+ mvneta_swbm_add_rx_fragment(struct mvneta_port *pp,
+ 			    struct mvneta_rx_desc *rx_desc,
+ 			    struct mvneta_rx_queue *rxq,
+ 			    struct xdp_buff *xdp, int *size,
+ 			    struct skb_shared_info *xdp_sinfo,
+ 			    struct page *page)
+ {
+ 	struct net_device *dev = pp->dev;
+ 	enum dma_data_direction dma_dir;
+ 	int data_len, len;
+ 
+ 	if (*size > MVNETA_MAX_RX_BUF_SIZE) {
+ 		len = MVNETA_MAX_RX_BUF_SIZE;
+ 		data_len = len;
+ 	} else {
+ 		len = *size;
+ 		data_len = len - ETH_FCS_LEN;
+ 	}
+ 	dma_dir = page_pool_get_dma_dir(rxq->page_pool);
+ 	dma_sync_single_for_cpu(dev->dev.parent,
+ 				rx_desc->buf_phys_addr,
+ 				len, dma_dir);
+ 	rx_desc->buf_phys_addr = 0;
+ 
+ 	if (data_len > 0 && xdp_sinfo->nr_frags < MAX_SKB_FRAGS) {
+ 		skb_frag_t *frag = &xdp_sinfo->frags[xdp_sinfo->nr_frags++];
+ 
+ 		skb_frag_off_set(frag, pp->rx_offset_correction);
+ 		skb_frag_size_set(frag, data_len);
+ 		__skb_frag_set_page(frag, page);
+ 
+ 		/* last fragment */
+ 		if (len == *size) {
+ 			struct skb_shared_info *sinfo;
+ 
+ 			sinfo = xdp_get_shared_info_from_buff(xdp);
+ 			sinfo->nr_frags = xdp_sinfo->nr_frags;
+ 			memcpy(sinfo->frags, xdp_sinfo->frags,
+ 			       sinfo->nr_frags * sizeof(skb_frag_t));
+ 		}
+ 	} else {
+ 		page_pool_put_full_page(rxq->page_pool, page, true);
+ 	}
+ 	*size -= len;
+ }
+ 
+ static struct sk_buff *
+ mvneta_swbm_build_skb(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,
+ 		      struct xdp_buff *xdp, u32 desc_status)
+ {
+ 	struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	int i, num_frags = sinfo->nr_frags;
+ 	struct sk_buff *skb;
+ 
+ 	skb = build_skb(xdp->data_hard_start, PAGE_SIZE);
+ 	if (!skb)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	page_pool_release_page(rxq->page_pool, virt_to_page(xdp->data));
+ 
+ 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+ 	skb_put(skb, xdp->data_end - xdp->data);
+ 	mvneta_rx_csum(pp, desc_status, skb);
+ 
+ 	for (i = 0; i < num_frags; i++) {
+ 		skb_frag_t *frag = &sinfo->frags[i];
+ 
+ 		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+ 				skb_frag_page(frag), skb_frag_off(frag),
+ 				skb_frag_size(frag), PAGE_SIZE);
+ 		page_pool_release_page(rxq->page_pool, skb_frag_page(frag));
+ 	}
+ 
+ 	return skb;
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  }
  
  /* Main rx processing when using software buffer management */
diff --cc drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
index e29e898961a0,5272f26a3e3e..000000000000
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
@@@ -2685,6 -3562,35 +2685,38 @@@ err_drop_frame
  		else
  			frag_size = bm_pool->frag_size;
  
++<<<<<<< HEAD
++=======
+ 		if (xdp_prog) {
+ 			struct xdp_rxq_info *xdp_rxq;
+ 
+ 			if (bm_pool->pkt_size == MVPP2_BM_SHORT_PKT_SIZE)
+ 				xdp_rxq = &rxq->xdp_rxq_short;
+ 			else
+ 				xdp_rxq = &rxq->xdp_rxq_long;
+ 
+ 			xdp_init_buff(&xdp, PAGE_SIZE, xdp_rxq);
+ 			xdp_prepare_buff(&xdp, data,
+ 					 MVPP2_MH_SIZE + MVPP2_SKB_HEADROOM,
+ 					 rx_bytes, false);
+ 
+ 			ret = mvpp2_run_xdp(port, rxq, xdp_prog, &xdp, pp, &ps);
+ 
+ 			if (ret) {
+ 				xdp_ret |= ret;
+ 				err = mvpp2_rx_refill(port, bm_pool, pp, pool);
+ 				if (err) {
+ 					netdev_err(port->dev, "failed to refill BM pools\n");
+ 					goto err_drop_frame;
+ 				}
+ 
+ 				ps.rx_packets++;
+ 				ps.rx_bytes += rx_bytes;
+ 				continue;
+ 			}
+ 		}
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  		skb = build_skb(data, frag_size);
  		if (!skb) {
  			netdev_warn(port->dev, "skb build failed\n");
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 0a489ef1845b,a63ce7c8b98f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -1121,12 -1126,8 +1121,17 @@@ struct sk_buff *mlx5e_build_linear_skb(
  static void mlx5e_fill_xdp_buff(struct mlx5e_rq *rq, void *va, u16 headroom,
  				u32 len, struct xdp_buff *xdp)
  {
++<<<<<<< HEAD
 +	xdp->data_hard_start = va;
 +	xdp->data = va + headroom;
 +	xdp_set_data_meta_invalid(xdp);
 +	xdp->data_end = xdp->data + len;
 +	xdp->rxq = &rq->xdp_rxq;
 +	xdp->frame_sz = rq->buff.frame0_sz;
++=======
+ 	xdp_init_buff(xdp, rq->buff.frame0_sz, &rq->xdp_rxq);
+ 	xdp_prepare_buff(xdp, va, headroom, len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  }
  
  static struct sk_buff *
diff --cc drivers/net/ethernet/qlogic/qede/qede_fp.c
index e3b29bf427b0,70c8d3cd85c0..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede_fp.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_fp.c
@@@ -1090,12 -1090,9 +1090,18 @@@ static bool qede_rx_xdp(struct qede_de
  	struct xdp_buff xdp;
  	enum xdp_action act;
  
++<<<<<<< HEAD
 +	xdp.data_hard_start = page_address(bd->data);
 +	xdp.data = xdp.data_hard_start + *data_offset;
 +	xdp_set_data_meta_invalid(&xdp);
 +	xdp.data_end = xdp.data + *len;
 +	xdp.rxq = &rxq->xdp_rxq;
 +	xdp.frame_sz = rxq->rx_buf_seg_size; /* PAGE_SIZE when XDP enabled */
++=======
+ 	xdp_init_buff(&xdp, rxq->rx_buf_seg_size, &rxq->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, page_address(bd->data), *data_offset,
+ 			 *len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  	/* Queues always have a full reset currently, so for the time
  	 * being until there's atomic program replace just mark read
diff --cc drivers/net/ethernet/sfc/rx.c
index 4c08cd18f1af,89c5c75f479f..000000000000
--- a/drivers/net/ethernet/sfc/rx.c
+++ b/drivers/net/ethernet/sfc/rx.c
@@@ -290,14 -293,10 +290,21 @@@ static bool efx_do_xdp(struct efx_nic *
  	memcpy(rx_prefix, *ehp - efx->rx_prefix_size,
  	       efx->rx_prefix_size);
  
++<<<<<<< HEAD
 +	xdp.data = *ehp;
 +	xdp.data_hard_start = xdp.data - EFX_XDP_HEADROOM;
 +
 +	/* No support yet for XDP metadata */
 +	xdp_set_data_meta_invalid(&xdp);
 +	xdp.data_end = xdp.data + rx_buf->len;
 +	xdp.rxq = &rx_queue->xdp_rxq_info;
 +	xdp.frame_sz = efx->rx_page_buf_step;
++=======
+ 	xdp_init_buff(&xdp, efx->rx_page_buf_step, &rx_queue->xdp_rxq_info);
+ 	/* No support yet for XDP metadata */
+ 	xdp_prepare_buff(&xdp, *ehp - EFX_XDP_HEADROOM, EFX_XDP_HEADROOM,
+ 			 rx_buf->len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  	xdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);
  	rcu_read_unlock();
diff --cc drivers/net/ethernet/socionext/netsec.c
index e080d3e7c582,3c53051bdacf..000000000000
--- a/drivers/net/ethernet/socionext/netsec.c
+++ b/drivers/net/ethernet/socionext/netsec.c
@@@ -851,6 -820,318 +851,321 @@@ static void netsec_set_tx_de(struct net
  	dring->head = (dring->head + 1) % DESC_NUM;
  }
  
++<<<<<<< HEAD
++=======
+ /* The current driver only supports 1 Txq, this should run under spin_lock() */
+ static u32 netsec_xdp_queue_one(struct netsec_priv *priv,
+ 				struct xdp_frame *xdpf, bool is_ndo)
+ 
+ {
+ 	struct netsec_desc_ring *tx_ring = &priv->desc_ring[NETSEC_RING_TX];
+ 	struct page *page = virt_to_page(xdpf->data);
+ 	struct netsec_tx_pkt_ctrl tx_ctrl = {};
+ 	struct netsec_desc tx_desc;
+ 	dma_addr_t dma_handle;
+ 	u16 filled;
+ 
+ 	if (tx_ring->head >= tx_ring->tail)
+ 		filled = tx_ring->head - tx_ring->tail;
+ 	else
+ 		filled = tx_ring->head + DESC_NUM - tx_ring->tail;
+ 
+ 	if (DESC_NUM - filled <= 1)
+ 		return NETSEC_XDP_CONSUMED;
+ 
+ 	if (is_ndo) {
+ 		/* this is for ndo_xdp_xmit, the buffer needs mapping before
+ 		 * sending
+ 		 */
+ 		dma_handle = dma_map_single(priv->dev, xdpf->data, xdpf->len,
+ 					    DMA_TO_DEVICE);
+ 		if (dma_mapping_error(priv->dev, dma_handle))
+ 			return NETSEC_XDP_CONSUMED;
+ 		tx_desc.buf_type = TYPE_NETSEC_XDP_NDO;
+ 	} else {
+ 		/* This is the device Rx buffer from page_pool. No need to remap
+ 		 * just sync and send it
+ 		 */
+ 		struct netsec_desc_ring *rx_ring =
+ 			&priv->desc_ring[NETSEC_RING_RX];
+ 		enum dma_data_direction dma_dir =
+ 			page_pool_get_dma_dir(rx_ring->page_pool);
+ 
+ 		dma_handle = page_pool_get_dma_addr(page) + xdpf->headroom +
+ 			sizeof(*xdpf);
+ 		dma_sync_single_for_device(priv->dev, dma_handle, xdpf->len,
+ 					   dma_dir);
+ 		tx_desc.buf_type = TYPE_NETSEC_XDP_TX;
+ 	}
+ 
+ 	tx_desc.dma_addr = dma_handle;
+ 	tx_desc.addr = xdpf->data;
+ 	tx_desc.len = xdpf->len;
+ 
+ 	netdev_sent_queue(priv->ndev, xdpf->len);
+ 	netsec_set_tx_de(priv, tx_ring, &tx_ctrl, &tx_desc, xdpf);
+ 
+ 	return NETSEC_XDP_TX;
+ }
+ 
+ static u32 netsec_xdp_xmit_back(struct netsec_priv *priv, struct xdp_buff *xdp)
+ {
+ 	struct netsec_desc_ring *tx_ring = &priv->desc_ring[NETSEC_RING_TX];
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	u32 ret;
+ 
+ 	if (unlikely(!xdpf))
+ 		return NETSEC_XDP_CONSUMED;
+ 
+ 	spin_lock(&tx_ring->lock);
+ 	ret = netsec_xdp_queue_one(priv, xdpf, false);
+ 	spin_unlock(&tx_ring->lock);
+ 
+ 	return ret;
+ }
+ 
+ static u32 netsec_run_xdp(struct netsec_priv *priv, struct bpf_prog *prog,
+ 			  struct xdp_buff *xdp)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_RX];
+ 	unsigned int sync, len = xdp->data_end - xdp->data;
+ 	u32 ret = NETSEC_XDP_PASS;
+ 	struct page *page;
+ 	int err;
+ 	u32 act;
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	/* Due xdp_adjust_tail: DMA sync for_device cover max len CPU touch */
+ 	sync = xdp->data_end - xdp->data_hard_start - NETSEC_RXBUF_HEADROOM;
+ 	sync = max(sync, len);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		ret = NETSEC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		ret = netsec_xdp_xmit_back(priv, xdp);
+ 		if (ret != NETSEC_XDP_TX) {
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(dring->page_pool, page, sync, true);
+ 		}
+ 		break;
+ 	case XDP_REDIRECT:
+ 		err = xdp_do_redirect(priv->ndev, xdp, prog);
+ 		if (!err) {
+ 			ret = NETSEC_XDP_REDIR;
+ 		} else {
+ 			ret = NETSEC_XDP_CONSUMED;
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(dring->page_pool, page, sync, true);
+ 		}
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->ndev, prog, act);
+ 		fallthrough;	/* handle aborts by dropping packet */
+ 	case XDP_DROP:
+ 		ret = NETSEC_XDP_CONSUMED;
+ 		page = virt_to_head_page(xdp->data);
+ 		page_pool_put_page(dring->page_pool, page, sync, true);
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int netsec_process_rx(struct netsec_priv *priv, int budget)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_RX];
+ 	struct net_device *ndev = priv->ndev;
+ 	struct netsec_rx_pkt_info rx_info;
+ 	enum dma_data_direction dma_dir;
+ 	struct bpf_prog *xdp_prog;
+ 	struct xdp_buff xdp;
+ 	u16 xdp_xmit = 0;
+ 	u32 xdp_act = 0;
+ 	int done = 0;
+ 
+ 	xdp_init_buff(&xdp, PAGE_SIZE, &dring->xdp_rxq);
+ 
+ 	rcu_read_lock();
+ 	xdp_prog = READ_ONCE(priv->xdp_prog);
+ 	dma_dir = page_pool_get_dma_dir(dring->page_pool);
+ 
+ 	while (done < budget) {
+ 		u16 idx = dring->tail;
+ 		struct netsec_de *de = dring->vaddr + (DESC_SZ * idx);
+ 		struct netsec_desc *desc = &dring->desc[idx];
+ 		struct page *page = virt_to_page(desc->addr);
+ 		u32 xdp_result = NETSEC_XDP_PASS;
+ 		struct sk_buff *skb = NULL;
+ 		u16 pkt_len, desc_len;
+ 		dma_addr_t dma_handle;
+ 		void *buf_addr;
+ 
+ 		if (de->attr & (1U << NETSEC_RX_PKT_OWN_FIELD)) {
+ 			/* reading the register clears the irq */
+ 			netsec_read(priv, NETSEC_REG_NRM_RX_PKTCNT);
+ 			break;
+ 		}
+ 
+ 		/* This  barrier is needed to keep us from reading
+ 		 * any other fields out of the netsec_de until we have
+ 		 * verified the descriptor has been written back
+ 		 */
+ 		dma_rmb();
+ 		done++;
+ 
+ 		pkt_len = de->buf_len_info >> 16;
+ 		rx_info.err_code = (de->attr >> NETSEC_RX_PKT_ERR_FIELD) &
+ 			NETSEC_RX_PKT_ERR_MASK;
+ 		rx_info.err_flag = (de->attr >> NETSEC_RX_PKT_ER_FIELD) & 1;
+ 		if (rx_info.err_flag) {
+ 			netif_err(priv, drv, priv->ndev,
+ 				  "%s: rx fail err(%d)\n", __func__,
+ 				  rx_info.err_code);
+ 			ndev->stats.rx_dropped++;
+ 			dring->tail = (dring->tail + 1) % DESC_NUM;
+ 			/* reuse buffer page frag */
+ 			netsec_rx_fill(priv, idx, 1);
+ 			continue;
+ 		}
+ 		rx_info.rx_cksum_result =
+ 			(de->attr >> NETSEC_RX_PKT_CO_FIELD) & 3;
+ 
+ 		/* allocate a fresh buffer and map it to the hardware.
+ 		 * This will eventually replace the old buffer in the hardware
+ 		 */
+ 		buf_addr = netsec_alloc_rx_data(priv, &dma_handle, &desc_len);
+ 
+ 		if (unlikely(!buf_addr))
+ 			break;
+ 
+ 		dma_sync_single_for_cpu(priv->dev, desc->dma_addr, pkt_len,
+ 					dma_dir);
+ 		prefetch(desc->addr);
+ 
+ 		xdp_prepare_buff(&xdp, desc->addr, NETSEC_RXBUF_HEADROOM,
+ 				 pkt_len, false);
+ 
+ 		if (xdp_prog) {
+ 			xdp_result = netsec_run_xdp(priv, xdp_prog, &xdp);
+ 			if (xdp_result != NETSEC_XDP_PASS) {
+ 				xdp_act |= xdp_result;
+ 				if (xdp_result == NETSEC_XDP_TX)
+ 					xdp_xmit++;
+ 				goto next;
+ 			}
+ 		}
+ 		skb = build_skb(desc->addr, desc->len + NETSEC_RX_BUF_NON_DATA);
+ 
+ 		if (unlikely(!skb)) {
+ 			/* If skb fails recycle_direct will either unmap and
+ 			 * free the page or refill the cache depending on the
+ 			 * cache state. Since we paid the allocation cost if
+ 			 * building an skb fails try to put the page into cache
+ 			 */
+ 			page_pool_put_page(dring->page_pool, page, pkt_len,
+ 					   true);
+ 			netif_err(priv, drv, priv->ndev,
+ 				  "rx failed to build skb\n");
+ 			break;
+ 		}
+ 		page_pool_release_page(dring->page_pool, page);
+ 
+ 		skb_reserve(skb, xdp.data - xdp.data_hard_start);
+ 		skb_put(skb, xdp.data_end - xdp.data);
+ 		skb->protocol = eth_type_trans(skb, priv->ndev);
+ 
+ 		if (priv->rx_cksum_offload_flag &&
+ 		    rx_info.rx_cksum_result == NETSEC_RX_CKSUM_OK)
+ 			skb->ip_summed = CHECKSUM_UNNECESSARY;
+ 
+ next:
+ 		if (skb)
+ 			napi_gro_receive(&priv->napi, skb);
+ 		if (skb || xdp_result) {
+ 			ndev->stats.rx_packets++;
+ 			ndev->stats.rx_bytes += xdp.data_end - xdp.data;
+ 		}
+ 
+ 		/* Update the descriptor with fresh buffers */
+ 		desc->len = desc_len;
+ 		desc->dma_addr = dma_handle;
+ 		desc->addr = buf_addr;
+ 
+ 		netsec_rx_fill(priv, idx, 1);
+ 		dring->tail = (dring->tail + 1) % DESC_NUM;
+ 	}
+ 	netsec_finalize_xdp_rx(priv, xdp_act, xdp_xmit);
+ 
+ 	rcu_read_unlock();
+ 
+ 	return done;
+ }
+ 
+ static int netsec_napi_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct netsec_priv *priv;
+ 	int done;
+ 
+ 	priv = container_of(napi, struct netsec_priv, napi);
+ 
+ 	netsec_process_tx(priv);
+ 	done = netsec_process_rx(priv, budget);
+ 
+ 	if (done < budget && napi_complete_done(napi, done)) {
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&priv->reglock, flags);
+ 		netsec_write(priv, NETSEC_REG_INTEN_SET,
+ 			     NETSEC_IRQ_RX | NETSEC_IRQ_TX);
+ 		spin_unlock_irqrestore(&priv->reglock, flags);
+ 	}
+ 
+ 	return done;
+ }
+ 
+ 
+ static int netsec_desc_used(struct netsec_desc_ring *dring)
+ {
+ 	int used;
+ 
+ 	if (dring->head >= dring->tail)
+ 		used = dring->head - dring->tail;
+ 	else
+ 		used = dring->head + DESC_NUM - dring->tail;
+ 
+ 	return used;
+ }
+ 
+ static int netsec_check_stop_tx(struct netsec_priv *priv, int used)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_TX];
+ 
+ 	/* keep tail from touching the queue */
+ 	if (DESC_NUM - used < 2) {
+ 		netif_stop_queue(priv->ndev);
+ 
+ 		/* Make sure we read the updated value in case
+ 		 * descriptors got freed
+ 		 */
+ 		smp_rmb();
+ 
+ 		used = netsec_desc_used(dring);
+ 		if (DESC_NUM - used < 2)
+ 			return NETDEV_TX_BUSY;
+ 
+ 		netif_wake_queue(priv->ndev);
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  static netdev_tx_t netsec_netdev_start_xmit(struct sk_buff *skb,
  					    struct net_device *ndev)
  {
diff --cc drivers/net/ethernet/ti/cpsw.c
index 3e34cb8ac1d3,5239318e9686..000000000000
--- a/drivers/net/ethernet/ti/cpsw.c
+++ b/drivers/net/ethernet/ti/cpsw.c
@@@ -809,272 -384,73 +809,320 @@@ static void cpsw_rx_handler(void *token
  		return;
  	}
  
++<<<<<<< HEAD
 +	new_skb = netdev_alloc_skb_ip_align(ndev, cpsw->rx_packet_max);
 +	if (new_skb) {
 +		skb_copy_queue_mapping(new_skb, skb);
 +		skb_put(skb, len);
 +		if (status & CPDMA_RX_VLAN_ENCAP)
 +			cpsw_rx_vlan_encap(skb);
++=======
+ 	new_page = page_pool_dev_alloc_pages(pool);
+ 	if (unlikely(!new_page)) {
+ 		new_page = page;
+ 		ndev->stats.rx_dropped++;
+ 		goto requeue;
+ 	}
+ 
+ 	if (priv->xdp_prog) {
+ 		int headroom = CPSW_HEADROOM, size = len;
+ 
+ 		xdp_init_buff(&xdp, PAGE_SIZE, &priv->xdp_rxq[ch]);
+ 		if (status & CPDMA_RX_VLAN_ENCAP) {
+ 			headroom += CPSW_RX_VLAN_ENCAP_HDR_SIZE;
+ 			size -= CPSW_RX_VLAN_ENCAP_HDR_SIZE;
+ 		}
+ 
+ 		xdp_prepare_buff(&xdp, pa, headroom, size, false);
+ 
+ 		port = priv->emac_port + cpsw->data.dual_emac;
+ 		ret = cpsw_run_xdp(priv, ch, &xdp, page, port);
+ 		if (ret != CPSW_XDP_PASS)
+ 			goto requeue;
+ 
+ 		/* XDP prog might have changed packet data and boundaries */
+ 		len = xdp.data_end - xdp.data;
+ 		headroom = xdp.data - xdp.data_hard_start;
+ 
+ 		/* XDP prog can modify vlan tag, so can't use encap header */
+ 		status &= ~CPDMA_RX_VLAN_ENCAP;
+ 	}
+ 
+ 	/* pass skb to netstack if no XDP prog or returned XDP_PASS */
+ 	skb = build_skb(pa, cpsw_rxbuf_total_len(pkt_size));
+ 	if (!skb) {
+ 		ndev->stats.rx_dropped++;
+ 		page_pool_recycle_direct(pool, page);
+ 		goto requeue;
+ 	}
+ 
+ 	skb_reserve(skb, headroom);
+ 	skb_put(skb, len);
+ 	skb->dev = ndev;
+ 	if (status & CPDMA_RX_VLAN_ENCAP)
+ 		cpsw_rx_vlan_encap(skb);
+ 	if (priv->rx_ts_enabled)
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  		cpts_rx_timestamp(cpsw->cpts, skb);
 -	skb->protocol = eth_type_trans(skb, ndev);
 +		skb->protocol = eth_type_trans(skb, ndev);
 +		netif_receive_skb(skb);
 +		ndev->stats.rx_bytes += len;
 +		ndev->stats.rx_packets++;
 +		kmemleak_not_leak(new_skb);
 +	} else {
 +		ndev->stats.rx_dropped++;
 +		new_skb = skb;
 +	}
  
 -	/* unmap page as no netstack skb page recycling */
 -	page_pool_release_page(pool, page);
 -	netif_receive_skb(skb);
 +requeue:
 +	if (netif_dormant(ndev)) {
 +		dev_kfree_skb_any(new_skb);
 +		return;
 +	}
  
 -	ndev->stats.rx_bytes += len;
 -	ndev->stats.rx_packets++;
 +	ch = cpsw->rxv[skb_get_queue_mapping(new_skb)].ch;
 +	ret = cpdma_chan_submit(ch, new_skb, new_skb->data,
 +				skb_tailroom(new_skb), 0);
 +	if (WARN_ON(ret < 0))
 +		dev_kfree_skb_any(new_skb);
 +}
  
 -requeue:
 -	xmeta = page_address(new_page) + CPSW_XMETA_OFFSET;
 -	xmeta->ndev = ndev;
 -	xmeta->ch = ch;
 +static void cpsw_split_res(struct net_device *ndev)
 +{
 +	struct cpsw_priv *priv = netdev_priv(ndev);
 +	u32 consumed_rate = 0, bigest_rate = 0;
 +	struct cpsw_common *cpsw = priv->cpsw;
 +	struct cpsw_vector *txv = cpsw->txv;
 +	int i, ch_weight, rlim_ch_num = 0;
 +	int budget, bigest_rate_ch = 0;
 +	u32 ch_rate, max_rate;
 +	int ch_budget = 0;
 +
 +	for (i = 0; i < cpsw->tx_ch_num; i++) {
 +		ch_rate = cpdma_chan_get_rate(txv[i].ch);
 +		if (!ch_rate)
 +			continue;
  
 -	dma = page_pool_get_dma_addr(new_page) + CPSW_HEADROOM;
 -	ret = cpdma_chan_submit_mapped(cpsw->rxv[ch].ch, new_page, dma,
 -				       pkt_size, 0);
 -	if (ret < 0) {
 -		WARN_ON(ret == -ENOMEM);
 -		page_pool_recycle_direct(pool, new_page);
 +		rlim_ch_num++;
 +		consumed_rate += ch_rate;
 +	}
 +
 +	if (cpsw->tx_ch_num == rlim_ch_num) {
 +		max_rate = consumed_rate;
 +	} else if (!rlim_ch_num) {
 +		ch_budget = CPSW_POLL_WEIGHT / cpsw->tx_ch_num;
 +		bigest_rate = 0;
 +		max_rate = consumed_rate;
 +	} else {
 +		max_rate = cpsw->speed * 1000;
 +
 +		/* if max_rate is less then expected due to reduced link speed,
 +		 * split proportionally according next potential max speed
 +		 */
 +		if (max_rate < consumed_rate)
 +			max_rate *= 10;
 +
 +		if (max_rate < consumed_rate)
 +			max_rate *= 10;
 +
 +		ch_budget = (consumed_rate * CPSW_POLL_WEIGHT) / max_rate;
 +		ch_budget = (CPSW_POLL_WEIGHT - ch_budget) /
 +			    (cpsw->tx_ch_num - rlim_ch_num);
 +		bigest_rate = (max_rate - consumed_rate) /
 +			      (cpsw->tx_ch_num - rlim_ch_num);
 +	}
 +
 +	/* split tx weight/budget */
 +	budget = CPSW_POLL_WEIGHT;
 +	for (i = 0; i < cpsw->tx_ch_num; i++) {
 +		ch_rate = cpdma_chan_get_rate(txv[i].ch);
 +		if (ch_rate) {
 +			txv[i].budget = (ch_rate * CPSW_POLL_WEIGHT) / max_rate;
 +			if (!txv[i].budget)
 +				txv[i].budget++;
 +			if (ch_rate > bigest_rate) {
 +				bigest_rate_ch = i;
 +				bigest_rate = ch_rate;
 +			}
 +
 +			ch_weight = (ch_rate * 100) / max_rate;
 +			if (!ch_weight)
 +				ch_weight++;
 +			cpdma_chan_set_weight(cpsw->txv[i].ch, ch_weight);
 +		} else {
 +			txv[i].budget = ch_budget;
 +			if (!bigest_rate_ch)
 +				bigest_rate_ch = i;
 +			cpdma_chan_set_weight(cpsw->txv[i].ch, 0);
 +		}
 +
 +		budget -= txv[i].budget;
 +	}
 +
 +	if (budget)
 +		txv[bigest_rate_ch].budget += budget;
 +
 +	/* split rx budget */
 +	budget = CPSW_POLL_WEIGHT;
 +	ch_budget = budget / cpsw->rx_ch_num;
 +	for (i = 0; i < cpsw->rx_ch_num; i++) {
 +		cpsw->rxv[i].budget = ch_budget;
 +		budget -= ch_budget;
 +	}
 +
 +	if (budget)
 +		cpsw->rxv[0].budget += budget;
 +}
 +
 +static irqreturn_t cpsw_tx_interrupt(int irq, void *dev_id)
 +{
 +	struct cpsw_common *cpsw = dev_id;
 +
 +	writel(0, &cpsw->wr_regs->tx_en);
 +	cpdma_ctlr_eoi(cpsw->dma, CPDMA_EOI_TX);
 +
 +	if (cpsw->quirk_irq) {
 +		disable_irq_nosync(cpsw->irqs_table[1]);
 +		cpsw->tx_irq_disabled = true;
 +	}
 +
 +	napi_schedule(&cpsw->napi_tx);
 +	return IRQ_HANDLED;
 +}
 +
 +static irqreturn_t cpsw_rx_interrupt(int irq, void *dev_id)
 +{
 +	struct cpsw_common *cpsw = dev_id;
 +
 +	cpdma_ctlr_eoi(cpsw->dma, CPDMA_EOI_RX);
 +	writel(0, &cpsw->wr_regs->rx_en);
 +
 +	if (cpsw->quirk_irq) {
 +		disable_irq_nosync(cpsw->irqs_table[0]);
 +		cpsw->rx_irq_disabled = true;
 +	}
 +
 +	napi_schedule(&cpsw->napi_rx);
 +	return IRQ_HANDLED;
 +}
 +
 +static int cpsw_tx_mq_poll(struct napi_struct *napi_tx, int budget)
 +{
 +	u32			ch_map;
 +	int			num_tx, cur_budget, ch;
 +	struct cpsw_common	*cpsw = napi_to_cpsw(napi_tx);
 +	struct cpsw_vector	*txv;
 +
 +	/* process every unprocessed channel */
 +	ch_map = cpdma_ctrl_txchs_state(cpsw->dma);
 +	for (ch = 0, num_tx = 0; ch_map; ch_map >>= 1, ch++) {
 +		if (!(ch_map & 0x01))
 +			continue;
 +
 +		txv = &cpsw->txv[ch];
 +		if (unlikely(txv->budget > budget - num_tx))
 +			cur_budget = budget - num_tx;
 +		else
 +			cur_budget = txv->budget;
 +
 +		num_tx += cpdma_chan_process(txv->ch, cur_budget);
 +		if (num_tx >= budget)
 +			break;
 +	}
 +
 +	if (num_tx < budget) {
 +		napi_complete(napi_tx);
 +		writel(0xff, &cpsw->wr_regs->tx_en);
 +	}
 +
 +	return num_tx;
 +}
 +
 +static int cpsw_tx_poll(struct napi_struct *napi_tx, int budget)
 +{
 +	struct cpsw_common *cpsw = napi_to_cpsw(napi_tx);
 +	int num_tx;
 +
 +	num_tx = cpdma_chan_process(cpsw->txv[0].ch, budget);
 +	if (num_tx < budget) {
 +		napi_complete(napi_tx);
 +		writel(0xff, &cpsw->wr_regs->tx_en);
 +		if (cpsw->tx_irq_disabled) {
 +			cpsw->tx_irq_disabled = false;
 +			enable_irq(cpsw->irqs_table[1]);
 +		}
  	}
 +
 +	return num_tx;
 +}
 +
 +static int cpsw_rx_mq_poll(struct napi_struct *napi_rx, int budget)
 +{
 +	u32			ch_map;
 +	int			num_rx, cur_budget, ch;
 +	struct cpsw_common	*cpsw = napi_to_cpsw(napi_rx);
 +	struct cpsw_vector	*rxv;
 +
 +	/* process every unprocessed channel */
 +	ch_map = cpdma_ctrl_rxchs_state(cpsw->dma);
 +	for (ch = 0, num_rx = 0; ch_map; ch_map >>= 1, ch++) {
 +		if (!(ch_map & 0x01))
 +			continue;
 +
 +		rxv = &cpsw->rxv[ch];
 +		if (unlikely(rxv->budget > budget - num_rx))
 +			cur_budget = budget - num_rx;
 +		else
 +			cur_budget = rxv->budget;
 +
 +		num_rx += cpdma_chan_process(rxv->ch, cur_budget);
 +		if (num_rx >= budget)
 +			break;
 +	}
 +
 +	if (num_rx < budget) {
 +		napi_complete_done(napi_rx, num_rx);
 +		writel(0xff, &cpsw->wr_regs->rx_en);
 +	}
 +
 +	return num_rx;
 +}
 +
 +static int cpsw_rx_poll(struct napi_struct *napi_rx, int budget)
 +{
 +	struct cpsw_common *cpsw = napi_to_cpsw(napi_rx);
 +	int num_rx;
 +
 +	num_rx = cpdma_chan_process(cpsw->rxv[0].ch, budget);
 +	if (num_rx < budget) {
 +		napi_complete_done(napi_rx, num_rx);
 +		writel(0xff, &cpsw->wr_regs->rx_en);
 +		if (cpsw->rx_irq_disabled) {
 +			cpsw->rx_irq_disabled = false;
 +			enable_irq(cpsw->irqs_table[0]);
 +		}
 +	}
 +
 +	return num_rx;
 +}
 +
 +static inline void soft_reset(const char *module, void __iomem *reg)
 +{
 +	unsigned long timeout = jiffies + HZ;
 +
 +	writel_relaxed(1, reg);
 +	do {
 +		cpu_relax();
 +	} while ((readl_relaxed(reg) & 1) && time_after(timeout, jiffies));
 +
 +	WARN(readl_relaxed(reg) & 1, "failed to soft-reset %s\n", module);
 +}
 +
 +static void cpsw_set_slave_mac(struct cpsw_slave *slave,
 +			       struct cpsw_priv *priv)
 +{
 +	slave_write(slave, mac_hi(priv->mac_addr), SA_HI);
 +	slave_write(slave, mac_lo(priv->mac_addr), SA_LO);
  }
  
  static void _cpsw_adjust_link(struct cpsw_slave *slave,
diff --cc drivers/net/hyperv/netvsc_bpf.c
index 11f0588a8884,d60dcf6c9829..000000000000
--- a/drivers/net/hyperv/netvsc_bpf.c
+++ b/drivers/net/hyperv/netvsc_bpf.c
@@@ -50,12 -44,8 +50,17 @@@ u32 netvsc_run_xdp(struct net_device *n
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	xdp->data_hard_start = page_address(page);
 +	xdp->data = xdp->data_hard_start + NETVSC_XDP_HDRM;
 +	xdp_set_data_meta_invalid(xdp);
 +	xdp->data_end = xdp->data + len;
 +	xdp->rxq = &nvchan->xdp_rxq;
 +	xdp->frame_sz = PAGE_SIZE;
++=======
+ 	xdp_init_buff(xdp, PAGE_SIZE, &nvchan->xdp_rxq);
+ 	xdp_prepare_buff(xdp, page_address(page), NETVSC_XDP_HDRM, len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  	memcpy(xdp->data, data, len);
  
diff --cc drivers/net/tun.c
index 0d643c7375dc,702215596889..000000000000
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@@ -1719,12 -1599,8 +1719,17 @@@ static struct sk_buff *tun_build_skb(st
  		struct xdp_buff xdp;
  		u32 act;
  
++<<<<<<< HEAD
 +		xdp.data_hard_start = buf;
 +		xdp.data = buf + pad;
 +		xdp_set_data_meta_invalid(&xdp);
 +		xdp.data_end = xdp.data + len;
 +		xdp.rxq = &tfile->xdp_rxq;
 +		xdp.frame_sz = buflen;
++=======
+ 		xdp_init_buff(&xdp, buflen, &tfile->xdp_rxq);
+ 		xdp_prepare_buff(&xdp, buf, pad, len, false);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  		act = bpf_prog_run_xdp(xdp_prog, &xdp);
  		if (act == XDP_REDIRECT || act == XDP_TX) {
diff --cc drivers/net/veth.c
index 2231e9f7d56d,99caae7d1641..000000000000
--- a/drivers/net/veth.c
+++ b/drivers/net/veth.c
@@@ -730,15 -710,11 +730,23 @@@ static struct sk_buff *veth_xdp_rcv_skb
  		skb = nskb;
  	}
  
++<<<<<<< HEAD
 +	xdp.data_hard_start = skb->head;
 +	xdp.data = skb_mac_header(skb);
 +	xdp.data_end = xdp.data + pktlen;
 +	xdp.data_meta = xdp.data;
 +	xdp.rxq = &rq->xdp_rxq;
 +
 +	/* SKB "head" area always have tailroom for skb_shared_info */
 +	xdp.frame_sz = (void *)skb_end_pointer(skb) - xdp.data_hard_start;
 +	xdp.frame_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
++=======
+ 	/* SKB "head" area always have tailroom for skb_shared_info */
+ 	frame_sz = skb_end_pointer(skb) - skb->head;
+ 	frame_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 	xdp_init_buff(&xdp, frame_sz, &rq->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, skb->head, skb->mac_header, pktlen, true);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  	orig_data = xdp.data;
  	orig_data_end = xdp.data_end;
diff --cc drivers/net/virtio_net.c
index 654aba3ff8f3,ba8e63792549..000000000000
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@@ -684,12 -689,9 +684,18 @@@ static struct sk_buff *receive_small(st
  			page = xdp_page;
  		}
  
++<<<<<<< HEAD
 +		xdp.data_hard_start = buf + VIRTNET_RX_PAD + vi->hdr_len;
 +		xdp.data = xdp.data_hard_start + xdp_headroom;
 +		xdp.data_end = xdp.data + len;
 +		xdp.data_meta = xdp.data;
 +		xdp.rxq = &rq->xdp_rxq;
 +		xdp.frame_sz = buflen;
++=======
+ 		xdp_init_buff(&xdp, buflen, &rq->xdp_rxq);
+ 		xdp_prepare_buff(&xdp, buf + VIRTNET_RX_PAD + vi->hdr_len,
+ 				 xdp_headroom, len, true);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  		orig_data = xdp.data;
  		act = bpf_prog_run_xdp(xdp_prog, &xdp);
  		stats->xdp_packets++;
@@@ -853,12 -856,9 +859,18 @@@ static struct sk_buff *receive_mergeabl
  		 * the descriptor on if we get an XDP_TX return code.
  		 */
  		data = page_address(xdp_page) + offset;
++<<<<<<< HEAD
 +		xdp.data_hard_start = data - VIRTIO_XDP_HEADROOM + vi->hdr_len;
 +		xdp.data = data + vi->hdr_len;
 +		xdp.data_end = xdp.data + (len - vi->hdr_len);
 +		xdp.data_meta = xdp.data;
 +		xdp.rxq = &rq->xdp_rxq;
 +		xdp.frame_sz = frame_sz - vi->hdr_len;
++=======
+ 		xdp_init_buff(&xdp, frame_sz - vi->hdr_len, &rq->xdp_rxq);
+ 		xdp_prepare_buff(&xdp, data - VIRTIO_XDP_HEADROOM + vi->hdr_len,
+ 				 VIRTIO_XDP_HEADROOM, len - vi->hdr_len, true);
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  		act = bpf_prog_run_xdp(xdp_prog, &xdp);
  		stats->xdp_packets++;
diff --cc drivers/net/xen-netfront.c
index c9ad8431e87a,c20b78120bb4..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -781,20 -855,69 +781,68 @@@ static int xennet_get_extras(struct net
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static u32 xennet_run_xdp(struct netfront_queue *queue, struct page *pdata,
+ 		   struct xen_netif_rx_response *rx, struct bpf_prog *prog,
+ 		   struct xdp_buff *xdp, bool *need_xdp_flush)
+ {
+ 	struct xdp_frame *xdpf;
+ 	u32 len = rx->status;
+ 	u32 act;
+ 	int err;
+ 
+ 	xdp_init_buff(xdp, XEN_PAGE_SIZE - XDP_PACKET_HEADROOM,
+ 		      &queue->xdp_rxq);
+ 	xdp_prepare_buff(xdp, page_address(pdata), XDP_PACKET_HEADROOM,
+ 			 len, false);
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_TX:
+ 		get_page(pdata);
+ 		xdpf = xdp_convert_buff_to_frame(xdp);
+ 		err = xennet_xdp_xmit(queue->info->netdev, 1, &xdpf, 0);
+ 		if (unlikely(err < 0))
+ 			trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 	case XDP_REDIRECT:
+ 		get_page(pdata);
+ 		err = xdp_do_redirect(queue->info->netdev, xdp, prog);
+ 		*need_xdp_flush = true;
+ 		if (unlikely(err))
+ 			trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 	case XDP_PASS:
+ 	case XDP_DROP:
+ 		break;
+ 
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 	}
+ 
+ 	return act;
+ }
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  static int xennet_get_responses(struct netfront_queue *queue,
  				struct netfront_rx_info *rinfo, RING_IDX rp,
 -				struct sk_buff_head *list,
 -				bool *need_xdp_flush)
 +				struct sk_buff_head *list)
  {
  	struct xen_netif_rx_response *rx = &rinfo->rx;
 -	int max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);
 +	struct xen_netif_extra_info *extras = rinfo->extras;
 +	struct device *dev = &queue->info->netdev->dev;
  	RING_IDX cons = queue->rx.rsp_cons;
  	struct sk_buff *skb = xennet_get_rx_skb(queue, cons);
 -	struct xen_netif_extra_info *extras = rinfo->extras;
  	grant_ref_t ref = xennet_get_rx_ref(queue, cons);
 -	struct device *dev = &queue->info->netdev->dev;
 -	struct bpf_prog *xdp_prog;
 -	struct xdp_buff xdp;
 -	unsigned long ret;
 +	int max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);
  	int slots = 1;
  	int err = 0;
 -	u32 verdict;
 +	unsigned long ret;
  
  	if (rx->flags & XEN_NETRXF_extra_info) {
  		err = xennet_get_extras(queue, extras, rp);
diff --cc include/net/xdp.h
index 1f0783d3cf67,0cf3976ce77c..000000000000
--- a/include/net/xdp.h
+++ b/include/net/xdp.h
@@@ -91,6 -76,25 +91,28 @@@ struct xdp_buff 
  	u32 frame_sz; /* frame size to deduce data_hard_end/reserved tailroom*/
  };
  
++<<<<<<< HEAD
++=======
+ static __always_inline void
+ xdp_init_buff(struct xdp_buff *xdp, u32 frame_sz, struct xdp_rxq_info *rxq)
+ {
+ 	xdp->frame_sz = frame_sz;
+ 	xdp->rxq = rxq;
+ }
+ 
+ static __always_inline void
+ xdp_prepare_buff(struct xdp_buff *xdp, unsigned char *hard_start,
+ 		 int headroom, int data_len, const bool meta_valid)
+ {
+ 	unsigned char *data = hard_start + headroom;
+ 
+ 	xdp->data_hard_start = hard_start;
+ 	xdp->data = data;
+ 	xdp->data_end = data + data_len;
+ 	xdp->data_meta = meta_valid ? data : data + 1;
+ }
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  /* Reserve memory area at end-of data area.
   *
   * This macro reserves tailroom in the XDP buffer by limiting the
diff --cc net/bpf/test_run.c
index 94c1cdee1609,23dfb2010ba6..000000000000
--- a/net/bpf/test_run.c
+++ b/net/bpf/test_run.c
@@@ -637,14 -636,11 +637,22 @@@ int bpf_prog_test_run_xdp(struct bpf_pr
  	if (IS_ERR(data))
  		return PTR_ERR(data);
  
++<<<<<<< HEAD
 +	xdp.data_hard_start = data;
 +	xdp.data = data + headroom;
 +	xdp.data_meta = xdp.data;
 +	xdp.data_end = xdp.data + size;
 +	xdp.frame_sz = headroom + max_data_sz + tailroom;
 +
 +	rxqueue = __netif_get_rx_queue(current->nsproxy->net_ns->loopback_dev, 0);
 +	xdp.rxq = &rxqueue->xdp_rxq;
++=======
+ 	rxqueue = __netif_get_rx_queue(current->nsproxy->net_ns->loopback_dev, 0);
+ 	xdp_init_buff(&xdp, headroom + max_data_sz + tailroom,
+ 		      &rxqueue->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, data, headroom, size, true);
+ 
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  	bpf_prog_change_xdp(NULL, prog);
  	ret = bpf_test_run(prog, &xdp, repeat, &retval, &duration, true);
  	if (ret)
diff --cc net/core/dev.c
index a223efdd8606,55499b017a42..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -4468,14 -4603,14 +4468,18 @@@ static u32 netif_receive_generic_xdp(st
  				     struct xdp_buff *xdp,
  				     struct bpf_prog *xdp_prog)
  {
+ 	void *orig_data, *orig_data_end, *hard_start;
  	struct netdev_rx_queue *rxqueue;
- 	void *orig_data, *orig_data_end;
  	u32 metalen, act = XDP_DROP;
 -	u32 mac_len, frame_sz;
  	__be16 orig_eth_type;
  	struct ethhdr *eth;
  	bool orig_bcast;
++<<<<<<< HEAD
 +	int hlen, off;
 +	u32 mac_len;
++=======
+ 	int off;
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  
  	/* Reinjected packets coming from act_mirred or similar should
  	 * not get XDP generic processing.
@@@ -4507,15 -4642,16 +4511,21 @@@
  	 * header.
  	 */
  	mac_len = skb->data - skb_mac_header(skb);
- 	hlen = skb_headlen(skb) + mac_len;
- 	xdp->data = skb->data - mac_len;
- 	xdp->data_meta = xdp->data;
- 	xdp->data_end = xdp->data + hlen;
- 	xdp->data_hard_start = skb->data - skb_headroom(skb);
+ 	hard_start = skb->data - skb_headroom(skb);
  
  	/* SKB "head" area always have tailroom for skb_shared_info */
++<<<<<<< HEAD
 +	xdp->frame_sz  = (void *)skb_end_pointer(skb) - xdp->data_hard_start;
 +	xdp->frame_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
++=======
+ 	frame_sz = (void *)skb_end_pointer(skb) - hard_start;
+ 	frame_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
+ 
+ 	rxqueue = netif_get_rxqueue(skb);
+ 	xdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);
+ 	xdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,
+ 			 skb_headlen(skb) + mac_len, true);
  
  	orig_data_end = xdp->data_end;
  	orig_data = xdp->data;
@@@ -4523,9 -4659,6 +4533,12 @@@
  	orig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);
  	orig_eth_type = eth->h_proto;
  
++<<<<<<< HEAD
 +	rxqueue = netif_get_rxqueue(skb);
 +	xdp->rxq = &rxqueue->xdp_rxq;
 +
++=======
++>>>>>>> be9df4aff65f (net, xdp: Introduce xdp_prepare_buff utility routine)
  	act = bpf_prog_run_xdp(xdp_prog, xdp);
  
  	/* check if bpf_xdp_adjust_head was used */
* Unmerged path drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
* Unmerged path drivers/net/ethernet/ti/cpsw_new.c
diff --git a/drivers/net/ethernet/amazon/ena/ena_netdev.c b/drivers/net/ethernet/amazon/ena/ena_netdev.c
index 5c8ded6592fe..a4fa62b3a662 100644
--- a/drivers/net/ethernet/amazon/ena/ena_netdev.c
+++ b/drivers/net/ethernet/amazon/ena/ena_netdev.c
@@ -1591,10 +1591,9 @@ static int ena_xdp_handle_buff(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 	int ret;
 
 	rx_info = &rx_ring->rx_buffer_info[rx_ring->ena_bufs[0].req_id];
-	xdp->data = page_address(rx_info->page) + rx_info->page_offset;
-	xdp_set_data_meta_invalid(xdp);
-	xdp->data_hard_start = page_address(rx_info->page);
-	xdp->data_end = xdp->data + rx_ring->ena_bufs[0].len;
+	xdp_prepare_buff(xdp, page_address(rx_info->page),
+			 rx_info->page_offset,
+			 rx_ring->ena_bufs[0].len, false);
 	/* If for some reason we received a bigger packet than
 	 * we expect, then we simply drop it
 	 */
* Unmerged path drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_main.c
* Unmerged path drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
* Unmerged path drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index ee7691cae1b4..3abb22604580 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -2382,12 +2382,12 @@ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
 
 		/* retrieve a buffer from the ring */
 		if (!skb) {
-			xdp.data = page_address(rx_buffer->page) +
-				   rx_buffer->page_offset;
-			xdp.data_meta = xdp.data;
-			xdp.data_hard_start = xdp.data -
-					      i40e_rx_offset(rx_ring);
-			xdp.data_end = xdp.data + size;
+			unsigned int offset = i40e_rx_offset(rx_ring);
+			unsigned char *hard_start;
+
+			hard_start = page_address(rx_buffer->page) +
+				     rx_buffer->page_offset - offset;
+			xdp_prepare_buff(&xdp, hard_start, offset, size, true);
 #if (PAGE_SIZE > 4096)
 			/* At larger PAGE_SIZE, frame_sz depend on len size */
 			xdp.frame_sz = i40e_rx_frame_truesize(rx_ring, size);
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx.c
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index 77a62ecca4d4..dd74025f1434 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -8708,12 +8708,12 @@ static int igb_clean_rx_irq(struct igb_q_vector *q_vector, const int budget)
 
 		/* retrieve a buffer from the ring */
 		if (!skb) {
-			xdp.data = page_address(rx_buffer->page) +
-				   rx_buffer->page_offset;
-			xdp.data_meta = xdp.data;
-			xdp.data_hard_start = xdp.data -
-					      igb_rx_offset(rx_ring);
-			xdp.data_end = xdp.data + size;
+			unsigned int offset = igb_rx_offset(rx_ring);
+			unsigned char *hard_start;
+
+			hard_start = page_address(rx_buffer->page) +
+				     rx_buffer->page_offset - offset;
+			xdp_prepare_buff(&xdp, hard_start, offset, size, true);
 #if (PAGE_SIZE > 4096)
 			/* At larger PAGE_SIZE, frame_sz depend on len size */
 			xdp.frame_sz = igb_rx_frame_truesize(rx_ring, size);
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index c8afd7f05061..0b6e2c807092 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -2323,12 +2323,12 @@ static int ixgbe_clean_rx_irq(struct ixgbe_q_vector *q_vector,
 
 		/* retrieve a buffer from the ring */
 		if (!skb) {
-			xdp.data = page_address(rx_buffer->page) +
-				   rx_buffer->page_offset;
-			xdp.data_meta = xdp.data;
-			xdp.data_hard_start = xdp.data -
-					      ixgbe_rx_offset(rx_ring);
-			xdp.data_end = xdp.data + size;
+			unsigned int offset = ixgbe_rx_offset(rx_ring);
+			unsigned char *hard_start;
+
+			hard_start = page_address(rx_buffer->page) +
+				     rx_buffer->page_offset - offset;
+			xdp_prepare_buff(&xdp, hard_start, offset, size, true);
 #if (PAGE_SIZE > 4096)
 			/* At larger PAGE_SIZE, frame_sz depend on len size */
 			xdp.frame_sz = ixgbe_rx_frame_truesize(rx_ring, size);
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 2a1831357de5..23171048f5ac 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -1158,12 +1158,12 @@ static int ixgbevf_clean_rx_irq(struct ixgbevf_q_vector *q_vector,
 
 		/* retrieve a buffer from the ring */
 		if (!skb) {
-			xdp.data = page_address(rx_buffer->page) +
-				   rx_buffer->page_offset;
-			xdp.data_meta = xdp.data;
-			xdp.data_hard_start = xdp.data -
-					      ixgbevf_rx_offset(rx_ring);
-			xdp.data_end = xdp.data + size;
+			unsigned int offset = ixgbevf_rx_offset(rx_ring);
+			unsigned char *hard_start;
+
+			hard_start = page_address(rx_buffer->page) +
+				     rx_buffer->page_offset - offset;
+			xdp_prepare_buff(&xdp, hard_start, offset, size, true);
 #if (PAGE_SIZE > 4096)
 			/* At larger PAGE_SIZE, frame_sz depend on len size */
 			xdp.frame_sz = ixgbevf_rx_frame_truesize(rx_ring, size);
* Unmerged path drivers/net/ethernet/marvell/mvneta.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_rx.c b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
index 487ace6a5313..829340925935 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@ -778,10 +778,8 @@ int mlx4_en_process_rx_cq(struct net_device *dev, struct mlx4_en_cq *cq, int bud
 						priv->frag_info[0].frag_size,
 						DMA_FROM_DEVICE);
 
-			xdp.data_hard_start = va - frags[0].page_offset;
-			xdp.data = va;
-			xdp_set_data_meta_invalid(&xdp);
-			xdp.data_end = xdp.data + length;
+			xdp_prepare_buff(&xdp, va - frags[0].page_offset,
+					 frags[0].page_offset, length, false);
 			orig_data = xdp.data;
 
 			act = bpf_prog_run_xdp(xdp_prog, &xdp);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 97a3ab6880b7..1891798c5a46 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@ -1914,10 +1914,10 @@ static int nfp_net_rx(struct nfp_net_rx_ring *rx_ring, int budget)
 			unsigned int dma_off;
 			int act;
 
-			xdp.data_hard_start = rxbuf->frag + NFP_NET_RX_BUF_HEADROOM;
-			xdp.data = orig_data;
-			xdp.data_meta = orig_data;
-			xdp.data_end = orig_data + pkt_len;
+			xdp_prepare_buff(&xdp,
+					 rxbuf->frag + NFP_NET_RX_BUF_HEADROOM,
+					 pkt_off - NFP_NET_RX_BUF_HEADROOM,
+					 pkt_len, true);
 
 			act = bpf_prog_run_xdp(xdp_prog, &xdp);
 
* Unmerged path drivers/net/ethernet/qlogic/qede/qede_fp.c
* Unmerged path drivers/net/ethernet/sfc/rx.c
* Unmerged path drivers/net/ethernet/socionext/netsec.c
* Unmerged path drivers/net/ethernet/ti/cpsw.c
* Unmerged path drivers/net/ethernet/ti/cpsw_new.c
* Unmerged path drivers/net/hyperv/netvsc_bpf.c
* Unmerged path drivers/net/tun.c
* Unmerged path drivers/net/veth.c
* Unmerged path drivers/net/virtio_net.c
* Unmerged path drivers/net/xen-netfront.c
* Unmerged path include/net/xdp.h
* Unmerged path net/bpf/test_run.c
* Unmerged path net/core/dev.c
