powerpc/rtas: dispatch partition migration requests to pseries

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Nathan Lynch <nathanl@linux.ibm.com>
commit 4d756894ba75f1afe7945ccafe9afebff50484b6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/4d756894.failed

sys_rtas() cannot call ibm,suspend-me directly in the same way it
handles other inputs. Instead it must dispatch the request to code
that can first perform the H_JOIN sequence before any call to
ibm,suspend-me can succeed. Over time kernel/rtas.c has accreted a fair
amount of platform-specific code to implement this.

Since a different, more robust implementation of the suspend sequence
is now in the pseries platform code, we want to dispatch the request
there.

Note that invoking ibm,suspend-me via the RTAS syscall is all but
deprecated; this change preserves ABI compatibility for old programs
while providing to them the benefit of the new partition suspend
implementation. This is a behavior change in that the kernel performs
the device tree update and firmware activation before returning, but
experimentation indicates this is tolerated fine by legacy user space.

	Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20201207215200.1785968-16-nathanl@linux.ibm.com
(cherry picked from commit 4d756894ba75f1afe7945ccafe9afebff50484b6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/pseries/mobility.c
diff --cc arch/powerpc/platforms/pseries/mobility.c
index a1c6b96d4899,e670180f311d..000000000000
--- a/arch/powerpc/platforms/pseries/mobility.c
+++ b/arch/powerpc/platforms/pseries/mobility.c
@@@ -346,6 -347,279 +346,282 @@@ void post_mobility_fixup(void
  	return;
  }
  
++<<<<<<< HEAD
++=======
+ static int poll_vasi_state(u64 handle, unsigned long *res)
+ {
+ 	unsigned long retbuf[PLPAR_HCALL_BUFSIZE];
+ 	long hvrc;
+ 	int ret;
+ 
+ 	hvrc = plpar_hcall(H_VASI_STATE, retbuf, handle);
+ 	switch (hvrc) {
+ 	case H_SUCCESS:
+ 		ret = 0;
+ 		*res = retbuf[0];
+ 		break;
+ 	case H_PARAMETER:
+ 		ret = -EINVAL;
+ 		break;
+ 	case H_FUNCTION:
+ 		ret = -EOPNOTSUPP;
+ 		break;
+ 	case H_HARDWARE:
+ 	default:
+ 		pr_err("unexpected H_VASI_STATE result %ld\n", hvrc);
+ 		ret = -EIO;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int wait_for_vasi_session_suspending(u64 handle)
+ {
+ 	unsigned long state;
+ 	int ret;
+ 
+ 	/*
+ 	 * Wait for transition from H_VASI_ENABLED to
+ 	 * H_VASI_SUSPENDING. Treat anything else as an error.
+ 	 */
+ 	while (true) {
+ 		ret = poll_vasi_state(handle, &state);
+ 
+ 		if (ret != 0 || state == H_VASI_SUSPENDING) {
+ 			break;
+ 		} else if (state == H_VASI_ENABLED) {
+ 			ssleep(1);
+ 		} else {
+ 			pr_err("unexpected H_VASI_STATE result %lu\n", state);
+ 			ret = -EIO;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Proceed even if H_VASI_STATE is unavailable. If H_JOIN or
+ 	 * ibm,suspend-me are also unimplemented, we'll recover then.
+ 	 */
+ 	if (ret == -EOPNOTSUPP)
+ 		ret = 0;
+ 
+ 	return ret;
+ }
+ 
+ static void prod_single(unsigned int target_cpu)
+ {
+ 	long hvrc;
+ 	int hwid;
+ 
+ 	hwid = get_hard_smp_processor_id(target_cpu);
+ 	hvrc = plpar_hcall_norets(H_PROD, hwid);
+ 	if (hvrc == H_SUCCESS)
+ 		return;
+ 	pr_err_ratelimited("H_PROD of CPU %u (hwid %d) error: %ld\n",
+ 			   target_cpu, hwid, hvrc);
+ }
+ 
+ static void prod_others(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	for_each_online_cpu(cpu) {
+ 		if (cpu != smp_processor_id())
+ 			prod_single(cpu);
+ 	}
+ }
+ 
+ static u16 clamp_slb_size(void)
+ {
+ 	u16 prev = mmu_slb_size;
+ 
+ 	slb_set_size(SLB_MIN_SIZE);
+ 
+ 	return prev;
+ }
+ 
+ static int do_suspend(void)
+ {
+ 	u16 saved_slb_size;
+ 	int status;
+ 	int ret;
+ 
+ 	pr_info("calling ibm,suspend-me on CPU %i\n", smp_processor_id());
+ 
+ 	/*
+ 	 * The destination processor model may have fewer SLB entries
+ 	 * than the source. We reduce mmu_slb_size to a safe minimum
+ 	 * before suspending in order to minimize the possibility of
+ 	 * programming non-existent entries on the destination. If
+ 	 * suspend fails, we restore it before returning. On success
+ 	 * the OF reconfig path will update it from the new device
+ 	 * tree after resuming on the destination.
+ 	 */
+ 	saved_slb_size = clamp_slb_size();
+ 
+ 	ret = rtas_ibm_suspend_me(&status);
+ 	if (ret != 0) {
+ 		pr_err("ibm,suspend-me error: %d\n", status);
+ 		slb_set_size(saved_slb_size);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int do_join(void *arg)
+ {
+ 	atomic_t *counter = arg;
+ 	long hvrc;
+ 	int ret;
+ 
+ 	/* Must ensure MSR.EE off for H_JOIN. */
+ 	hard_irq_disable();
+ 	hvrc = plpar_hcall_norets(H_JOIN);
+ 
+ 	switch (hvrc) {
+ 	case H_CONTINUE:
+ 		/*
+ 		 * All other CPUs are offline or in H_JOIN. This CPU
+ 		 * attempts the suspend.
+ 		 */
+ 		ret = do_suspend();
+ 		break;
+ 	case H_SUCCESS:
+ 		/*
+ 		 * The suspend is complete and this cpu has received a
+ 		 * prod.
+ 		 */
+ 		ret = 0;
+ 		break;
+ 	case H_BAD_MODE:
+ 	case H_HARDWARE:
+ 	default:
+ 		ret = -EIO;
+ 		pr_err_ratelimited("H_JOIN error %ld on CPU %i\n",
+ 				   hvrc, smp_processor_id());
+ 		break;
+ 	}
+ 
+ 	if (atomic_inc_return(counter) == 1) {
+ 		pr_info("CPU %u waking all threads\n", smp_processor_id());
+ 		prod_others();
+ 	}
+ 	/*
+ 	 * Execution may have been suspended for several seconds, so
+ 	 * reset the watchdog.
+ 	 */
+ 	touch_nmi_watchdog();
+ 	return ret;
+ }
+ 
+ /*
+  * Abort reason code byte 0. We use only the 'Migrating partition' value.
+  */
+ enum vasi_aborting_entity {
+ 	ORCHESTRATOR        = 1,
+ 	VSP_SOURCE          = 2,
+ 	PARTITION_FIRMWARE  = 3,
+ 	PLATFORM_FIRMWARE   = 4,
+ 	VSP_TARGET          = 5,
+ 	MIGRATING_PARTITION = 6,
+ };
+ 
+ static void pseries_cancel_migration(u64 handle, int err)
+ {
+ 	u32 reason_code;
+ 	u32 detail;
+ 	u8 entity;
+ 	long hvrc;
+ 
+ 	entity = MIGRATING_PARTITION;
+ 	detail = abs(err) & 0xffffff;
+ 	reason_code = (entity << 24) | detail;
+ 
+ 	hvrc = plpar_hcall_norets(H_VASI_SIGNAL, handle,
+ 				  H_VASI_SIGNAL_CANCEL, reason_code);
+ 	if (hvrc)
+ 		pr_err("H_VASI_SIGNAL error: %ld\n", hvrc);
+ }
+ 
+ static int pseries_suspend(u64 handle)
+ {
+ 	const unsigned int max_attempts = 5;
+ 	unsigned int retry_interval_ms = 1;
+ 	unsigned int attempt = 1;
+ 	int ret;
+ 
+ 	while (true) {
+ 		atomic_t counter = ATOMIC_INIT(0);
+ 		unsigned long vasi_state;
+ 		int vasi_err;
+ 
+ 		ret = stop_machine(do_join, &counter, cpu_online_mask);
+ 		if (ret == 0)
+ 			break;
+ 		/*
+ 		 * Encountered an error. If the VASI stream is still
+ 		 * in Suspending state, it's likely a transient
+ 		 * condition related to some device in the partition
+ 		 * and we can retry in the hope that the cause has
+ 		 * cleared after some delay.
+ 		 *
+ 		 * A better design would allow drivers etc to prepare
+ 		 * for the suspend and avoid conditions which prevent
+ 		 * the suspend from succeeding. For now, we have this
+ 		 * mitigation.
+ 		 */
+ 		pr_notice("Partition suspend attempt %u of %u error: %d\n",
+ 			  attempt, max_attempts, ret);
+ 
+ 		if (attempt == max_attempts)
+ 			break;
+ 
+ 		vasi_err = poll_vasi_state(handle, &vasi_state);
+ 		if (vasi_err == 0) {
+ 			if (vasi_state != H_VASI_SUSPENDING) {
+ 				pr_notice("VASI state %lu after failed suspend\n",
+ 					  vasi_state);
+ 				break;
+ 			}
+ 		} else if (vasi_err != -EOPNOTSUPP) {
+ 			pr_err("VASI state poll error: %d", vasi_err);
+ 			break;
+ 		}
+ 
+ 		pr_notice("Will retry partition suspend after %u ms\n",
+ 			  retry_interval_ms);
+ 
+ 		msleep(retry_interval_ms);
+ 		retry_interval_ms *= 10;
+ 		attempt++;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int pseries_migrate_partition(u64 handle)
+ {
+ 	int ret;
+ 
+ 	ret = wait_for_vasi_session_suspending(handle);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = pseries_suspend(handle);
+ 	if (ret == 0)
+ 		post_mobility_fixup();
+ 	else
+ 		pseries_cancel_migration(handle, ret);
+ 
+ 	return ret;
+ }
+ 
+ int rtas_syscall_dispatch_ibm_suspend_me(u64 handle)
+ {
+ 	return pseries_migrate_partition(handle);
+ }
+ 
++>>>>>>> 4d756894ba75 (powerpc/rtas: dispatch partition migration requests to pseries)
  static ssize_t migration_store(struct class *class,
  			       struct class_attribute *attr, const char *buf,
  			       size_t count)
diff --git a/arch/powerpc/include/asm/rtas.h b/arch/powerpc/include/asm/rtas.h
index 2a80269da3c4..eef70cd8b61f 100644
--- a/arch/powerpc/include/asm/rtas.h
+++ b/arch/powerpc/include/asm/rtas.h
@@ -397,8 +397,13 @@ extern time64_t last_rtas_event;
 extern int clobbering_unread_rtas_event(void);
 extern int pseries_devicetree_update(s32 scope);
 extern void post_mobility_fixup(void);
+int rtas_syscall_dispatch_ibm_suspend_me(u64 handle);
 #else
 static inline int clobbering_unread_rtas_event(void) { return 0; }
+static inline int rtas_syscall_dispatch_ibm_suspend_me(u64 handle)
+{
+	return -EINVAL;
+}
 #endif
 
 #ifdef CONFIG_PPC_RTAS_DAEMON
diff --git a/arch/powerpc/kernel/rtas.c b/arch/powerpc/kernel/rtas.c
index 7920f0612180..d10c5ff7c55c 100644
--- a/arch/powerpc/kernel/rtas.c
+++ b/arch/powerpc/kernel/rtas.c
@@ -1223,7 +1223,7 @@ SYSCALL_DEFINE1(rtas, struct rtas_args __user *, uargs)
 		int rc = 0;
 		u64 handle = ((u64)be32_to_cpu(args.args[0]) << 32)
 		              | be32_to_cpu(args.args[1]);
-		rc = rtas_ibm_suspend_me_unsafe(handle);
+		rc = rtas_syscall_dispatch_ibm_suspend_me(handle);
 		if (rc == -EAGAIN)
 			args.rets[0] = cpu_to_be32(RTAS_NOT_SUSPENDABLE);
 		else if (rc == -EIO)
* Unmerged path arch/powerpc/platforms/pseries/mobility.c
