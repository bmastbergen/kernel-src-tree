x86/setup: Always reserve the first 1M of RAM

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit f1d4d47c5851b348b7713007e152bc68b94d728b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/f1d4d47c.failed

There are BIOSes that are known to corrupt the memory under 1M, or more
precisely under 640K because the memory above 640K is anyway reserved
for the EGA/VGA frame buffer and BIOS.

To prevent usage of the memory that will be potentially clobbered by the
kernel, the beginning of the memory is always reserved. The exact size
of the reserved area is determined by CONFIG_X86_RESERVE_LOW build time
and the "reservelow=" command line option. The reserved range may be
from 4K to 640K with the default of 64K. There are also configurations
that reserve the entire 1M range, like machines with SandyBridge graphic
devices or systems that enable crash kernel.

In addition to the potentially clobbered memory, EBDA of unknown size may
be as low as 128K and the memory above that EBDA start is also reserved
early.

It would have been possible to reserve the entire range under 1M unless for
the real mode trampoline that must reside in that area.

To accommodate placement of the real mode trampoline and keep the memory
safe from being clobbered by BIOS, reserve the first 64K of RAM before
memory allocations are possible and then, after the real mode trampoline
is allocated, reserve the entire range from 0 to 1M.

Update trim_snb_memory() and reserve_real_mode() to avoid redundant
reservations of the same memory range.

Also make sure the memory under 1M is not getting freed by
efi_free_boot_services().

 [ bp: Massage commit message and comments. ]

Fixes: a799c2bd29d1 ("x86/setup: Consolidate early memory reservations")
	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Tested-by: Hugh Dickins <hughd@google.com>
Link: https://bugzilla.kernel.org/show_bug.cgi?id=213177
Link: https://lkml.kernel.org/r/20210601075354.5149-2-rppt@kernel.org
(cherry picked from commit f1d4d47c5851b348b7713007e152bc68b94d728b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/setup.c
diff --cc arch/x86/kernel/setup.c
index 3db4bd3fa6a2,1e720626069a..000000000000
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@@ -731,11 -634,16 +731,24 @@@ static void __init trim_snb_memory(void
  	printk(KERN_DEBUG "reserving inaccessible SNB gfx pages\n");
  
  	/*
++<<<<<<< HEAD
 +	 * Reserve all memory below the 1 MB mark that has not
 +	 * already been reserved.
 +	 */
 +	memblock_reserve(0, 1<<20);
 +	
++=======
+ 	 * SandyBridge integrated graphics devices have a bug that prevents
+ 	 * them from accessing certain memory ranges, namely anything below
+ 	 * 1M and in the pages listed in bad_pages[] above.
+ 	 *
+ 	 * To avoid these pages being ever accessed by SNB gfx devices reserve
+ 	 * bad_pages that have not already been reserved at boot time.
+ 	 * All memory below the 1 MB mark is anyway reserved later during
+ 	 * setup_arch(), so there is no need to reserve it here.
+ 	 */
+ 
++>>>>>>> f1d4d47c5851 (x86/setup: Always reserve the first 1M of RAM)
  	for (i = 0; i < ARRAY_SIZE(bad_pages); i++) {
  		if (memblock_reserve(bad_pages[i], PAGE_SIZE))
  			printk(KERN_WARNING "failed to reserve 0x%08lx\n",
@@@ -823,48 -719,40 +836,78 @@@ static int __init parse_reservelow(cha
  
  early_param("reservelow", parse_reservelow);
  
 -static void __init early_reserve_memory(void)
 +static void __init trim_low_memory_range(void)
  {
++<<<<<<< HEAD
 +	memblock_reserve(0, ALIGN(reserve_low, PAGE_SIZE));
 +}
++=======
+ 	/*
+ 	 * Reserve the memory occupied by the kernel between _text and
+ 	 * __end_of_kernel_reserve symbols. Any kernel sections after the
+ 	 * __end_of_kernel_reserve symbol must be explicitly reserved with a
+ 	 * separate memblock_reserve() or they will be discarded.
+ 	 */
+ 	memblock_reserve(__pa_symbol(_text),
+ 			 (unsigned long)__end_of_kernel_reserve - (unsigned long)_text);
+ 
+ 	/*
+ 	 * The first 4Kb of memory is a BIOS owned area, but generally it is
+ 	 * not listed as such in the E820 table.
+ 	 *
+ 	 * Reserve the first 64K of memory since some BIOSes are known to
+ 	 * corrupt low memory. After the real mode trampoline is allocated the
+ 	 * rest of the memory below 640k is reserved.
+ 	 *
+ 	 * In addition, make sure page 0 is always reserved because on
+ 	 * systems with L1TF its contents can be leaked to user processes.
+ 	 */
+ 	memblock_reserve(0, SZ_64K);
++>>>>>>> f1d4d47c5851 (x86/setup: Always reserve the first 1M of RAM)
  
 -	early_reserve_initrd();
 +static void rh_check_supported(void)
 +{
 +	bool guest;
  
 -	if (efi_enabled(EFI_BOOT))
 -		efi_memblock_x86_reserve_range();
 +	guest = (x86_hyper_type != X86_HYPER_NATIVE || boot_cpu_has(X86_FEATURE_HYPERVISOR));
  
 -	memblock_x86_reserve_range_setup_data();
 +	/* RHEL supports single cpu on guests only */
 +	if (((boot_cpu_data.x86_max_cores * smp_num_siblings) == 1) &&
 +	    !guest && is_kdump_kernel()) {
 +		pr_crit("Detected single cpu native boot.\n");
 +		pr_crit("Important:  In Red Hat Enterprise Linux 8, single threaded, single CPU 64-bit physical systems are unsupported by Red Hat. Please contact your Red Hat support representative for a list of certified and supported systems.");
 +	}
 +
++<<<<<<< HEAD
 +	/*
 +	 * If the RHEL kernel does not support this hardware, the kernel will
 +	 * attempt to boot, but no support is provided for this hardware
 +	 */
 +	switch (boot_cpu_data.x86_vendor) {
 +	case X86_VENDOR_AMD:
 +	case X86_VENDOR_INTEL:
 +		break;
 +	default:
 +		pr_crit("Detected processor %s %s\n",
 +			boot_cpu_data.x86_vendor_id,
 +			boot_cpu_data.x86_model_id);
 +		mark_hardware_unsupported("Processor");
 +		break;
 +	}
  
 +	/*
 +	 * Due to the complexity of x86 lapic & ioapic enumeration, and PCI IRQ
 +	 * routing, ACPI is required for x86.  acpi=off is a valid debug kernel
 +	 * parameter, so just print out a loud warning in case something
 +	 * goes wrong (which is most of the time).
 +	 */
 +	if (acpi_disabled && !guest)
 +		pr_crit("ACPI has been disabled or is not available on this hardware.  This may result in a single cpu boot, incorrect PCI IRQ routing, or boot failure.\n");
++=======
+ 	reserve_ibft_region();
+ 	reserve_bios_regions();
+ 	trim_snb_memory();
++>>>>>>> f1d4d47c5851 (x86/setup: Always reserve the first 1M of RAM)
  }
  
  /*
@@@ -1222,10 -1083,20 +1265,27 @@@ void __init setup_arch(char **cmdline_p
  			(max_pfn_mapped<<PAGE_SHIFT) - 1);
  #endif
  
++<<<<<<< HEAD
 +	reserve_real_mode();
 +
 +	trim_platform_memory_ranges();
 +	trim_low_memory_range();
++=======
+ 	/*
+ 	 * Find free memory for the real mode trampoline and place it
+ 	 * there.
+ 	 * If there is not enough free memory under 1M, on EFI-enabled
+ 	 * systems there will be additional attempt to reclaim the memory
+ 	 * for the real mode trampoline at efi_free_boot_services().
+ 	 *
+ 	 * Unconditionally reserve the entire first 1M of RAM because
+ 	 * BIOSes are know to corrupt low memory and several
+ 	 * hundred kilobytes are not worth complex detection what memory gets
+ 	 * clobbered. Moreover, on machines with SandyBridge graphics or in
+ 	 * setups that use crashkernel the entire 1M is reserved anyway.
+ 	 */
+ 	reserve_real_mode();
++>>>>>>> f1d4d47c5851 (x86/setup: Always reserve the first 1M of RAM)
  
  	init_mem_mapping();
  
* Unmerged path arch/x86/kernel/setup.c
diff --git a/arch/x86/platform/efi/quirks.c b/arch/x86/platform/efi/quirks.c
index 84dce5ab0d67..c41778b88a86 100644
--- a/arch/x86/platform/efi/quirks.c
+++ b/arch/x86/platform/efi/quirks.c
@@ -448,6 +448,18 @@ void __init efi_free_boot_services(void)
 			size -= rm_size;
 		}
 
+		/*
+		 * Don't free memory under 1M for two reasons:
+		 * - BIOS might clobber it
+		 * - Crash kernel needs it to be reserved
+		 */
+		if (start + size < SZ_1M)
+			continue;
+		if (start < SZ_1M) {
+			size -= (SZ_1M - start);
+			start = SZ_1M;
+		}
+
 		memblock_free_late(start, size);
 	}
 
diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c
index 1da4767e3ed4..8cc42761ff25 100644
--- a/arch/x86/realmode/init.c
+++ b/arch/x86/realmode/init.c
@@ -29,14 +29,16 @@ void __init reserve_real_mode(void)
 
 	/* Has to be under 1M so we can execute real-mode AP code. */
 	mem = memblock_find_in_range(0, 1<<20, size, PAGE_SIZE);
-	if (!mem) {
+	if (!mem)
 		pr_info("No sub-1M memory is available for the trampoline\n");
-		return;
-	}
+	else
+		set_real_mode_mem(mem);
 
-	memblock_reserve(mem, size);
-	set_real_mode_mem(mem);
-	crash_reserve_low_1M();
+	/*
+	 * Unconditionally reserve the entire fisrt 1M, see comment in
+	 * setup_arch().
+	 */
+	memblock_reserve(0, SZ_1M);
 }
 
 static void sme_sev_setup_real_mode(struct trampoline_header *th)
