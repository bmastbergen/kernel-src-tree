sched/rt: Fix double enqueue caused by rt_effective_prio

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit f558c2b834ec27e75d37b1c860c139e7b7c3a8e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/f558c2b8.failed

Double enqueues in rt runqueues (list) have been reported while running
a simple test that spawns a number of threads doing a short sleep/run
pattern while being concurrently setscheduled between rt and fair class.

  WARNING: CPU: 3 PID: 2825 at kernel/sched/rt.c:1294 enqueue_task_rt+0x355/0x360
  CPU: 3 PID: 2825 Comm: setsched__13
  RIP: 0010:enqueue_task_rt+0x355/0x360
  Call Trace:
   __sched_setscheduler+0x581/0x9d0
   _sched_setscheduler+0x63/0xa0
   do_sched_setscheduler+0xa0/0x150
   __x64_sys_sched_setscheduler+0x1a/0x30
   do_syscall_64+0x33/0x40
   entry_SYSCALL_64_after_hwframe+0x44/0xae

  list_add double add: new=ffff9867cb629b40, prev=ffff9867cb629b40,
		       next=ffff98679fc67ca0.
  kernel BUG at lib/list_debug.c:31!
  invalid opcode: 0000 [#1] PREEMPT_RT SMP PTI
  CPU: 3 PID: 2825 Comm: setsched__13
  RIP: 0010:__list_add_valid+0x41/0x50
  Call Trace:
   enqueue_task_rt+0x291/0x360
   __sched_setscheduler+0x581/0x9d0
   _sched_setscheduler+0x63/0xa0
   do_sched_setscheduler+0xa0/0x150
   __x64_sys_sched_setscheduler+0x1a/0x30
   do_syscall_64+0x33/0x40
   entry_SYSCALL_64_after_hwframe+0x44/0xae

__sched_setscheduler() uses rt_effective_prio() to handle proper queuing
of priority boosted tasks that are setscheduled while being boosted.
rt_effective_prio() is however called twice per each
__sched_setscheduler() call: first directly by __sched_setscheduler()
before dequeuing the task and then by __setscheduler() to actually do
the priority change. If the priority of the pi_top_task is concurrently
being changed however, it might happen that the two calls return
different results. If, for example, the first call returned the same rt
priority the task was running at and the second one a fair priority, the
task won't be removed by the rt list (on_list still set) and then
enqueued in the fair runqueue. When eventually setscheduled back to rt
it will be seen as enqueued already and the WARNING/BUG be issued.

Fix this by calling rt_effective_prio() only once and then reusing the
return value. While at it refactor code as well for clarity. Concurrent
priority inheritance handling is still safe and will eventually converge
to a new state by following the inheritance chain(s).

Fixes: 0782e63bc6fe ("sched: Handle priority boosted tasks proper in setscheduler()")
[squashed Peterz changes; added changelog]
	Reported-by: Mark Simmons <msimmons@redhat.com>
	Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20210803104501.38333-1-juri.lelli@redhat.com
(cherry picked from commit f558c2b834ec27e75d37b1c860c139e7b7c3a8e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 2df6ed6568f8,20ffcc044134..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -873,57 -1209,790 +873,63 @@@ static void set_load_weight(struct task
  	}
  }
  
 -#ifdef CONFIG_UCLAMP_TASK
 -/*
 - * Serializes updates of utilization clamp values
 - *
 - * The (slow-path) user-space triggers utilization clamp value updates which
 - * can require updates on (fast-path) scheduler's data structures used to
 - * support enqueue/dequeue operations.
 - * While the per-CPU rq lock protects fast-path update operations, user-space
 - * requests are serialized using a mutex to reduce the risk of conflicting
 - * updates or API abuses.
 - */
 -static DEFINE_MUTEX(uclamp_mutex);
 -
 -/* Max allowed minimum utilization */
 -unsigned int sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
 -
 -/* Max allowed maximum utilization */
 -unsigned int sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
 -
 -/*
 - * By default RT tasks run at the maximum performance point/capacity of the
 - * system. Uclamp enforces this by always setting UCLAMP_MIN of RT tasks to
 - * SCHED_CAPACITY_SCALE.
 - *
 - * This knob allows admins to change the default behavior when uclamp is being
 - * used. In battery powered devices, particularly, running at the maximum
 - * capacity and frequency will increase energy consumption and shorten the
 - * battery life.
 - *
 - * This knob only affects RT tasks that their uclamp_se->user_defined == false.
 - *
 - * This knob will not override the system default sched_util_clamp_min defined
 - * above.
 - */
 -unsigned int sysctl_sched_uclamp_util_min_rt_default = SCHED_CAPACITY_SCALE;
 -
 -/* All clamps are required to be less or equal than these values */
 -static struct uclamp_se uclamp_default[UCLAMP_CNT];
 -
 -/*
 - * This static key is used to reduce the uclamp overhead in the fast path. It
 - * primarily disables the call to uclamp_rq_{inc, dec}() in
 - * enqueue/dequeue_task().
 - *
 - * This allows users to continue to enable uclamp in their kernel config with
 - * minimum uclamp overhead in the fast path.
 - *
 - * As soon as userspace modifies any of the uclamp knobs, the static key is
 - * enabled, since we have an actual users that make use of uclamp
 - * functionality.
 - *
 - * The knobs that would enable this static key are:
 - *
 - *   * A task modifying its uclamp value with sched_setattr().
 - *   * An admin modifying the sysctl_sched_uclamp_{min, max} via procfs.
 - *   * An admin modifying the cgroup cpu.uclamp.{min, max}
 - */
 -DEFINE_STATIC_KEY_FALSE(sched_uclamp_used);
 -
 -/* Integer rounded range for each bucket */
 -#define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
 -
 -#define for_each_clamp_id(clamp_id) \
 -	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
 -
 -static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
 +bool sched_task_on_rq(struct task_struct *p)
  {
 -	return min_t(unsigned int, clamp_value / UCLAMP_BUCKET_DELTA, UCLAMP_BUCKETS - 1);
 +	return task_on_rq_queued(p);
  }
  
 -static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
 +static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
  {
 -	if (clamp_id == UCLAMP_MIN)
 -		return 0;
 -	return SCHED_CAPACITY_SCALE;
 -}
 +	if (!(flags & ENQUEUE_NOCLOCK))
 +		update_rq_clock(rq);
  
 -static inline void uclamp_se_set(struct uclamp_se *uc_se,
 -				 unsigned int value, bool user_defined)
 -{
 -	uc_se->value = value;
 -	uc_se->bucket_id = uclamp_bucket_id(value);
 -	uc_se->user_defined = user_defined;
 +	if (!(flags & ENQUEUE_RESTORE)) {
 +		sched_info_queued(rq, p);
 +		psi_enqueue(p, flags & ENQUEUE_WAKEUP);
 +	}
 +
 +	p->sched_class->enqueue_task(rq, p, flags);
  }
  
 -static inline unsigned int
 -uclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,
 -		  unsigned int clamp_value)
 +static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
  {
 -	/*
 -	 * Avoid blocked utilization pushing up the frequency when we go
 -	 * idle (which drops the max-clamp) by retaining the last known
 -	 * max-clamp.
 -	 */
 -	if (clamp_id == UCLAMP_MAX) {
 -		rq->uclamp_flags |= UCLAMP_FLAG_IDLE;
 -		return clamp_value;
 +	if (!(flags & DEQUEUE_NOCLOCK))
 +		update_rq_clock(rq);
 +
 +	if (!(flags & DEQUEUE_SAVE)) {
 +		sched_info_dequeued(rq, p);
 +		psi_dequeue(p, flags & DEQUEUE_SLEEP);
  	}
  
 -	return uclamp_none(UCLAMP_MIN);
 +	p->sched_class->dequeue_task(rq, p, flags);
  }
  
 -static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
 -				     unsigned int clamp_value)
 +void activate_task(struct rq *rq, struct task_struct *p, int flags)
  {
 -	/* Reset max-clamp retention only on idle exit */
 -	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
 -		return;
 +	enqueue_task(rq, p, flags);
  
 -	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
 +	p->on_rq = TASK_ON_RQ_QUEUED;
  }
  
 -static inline
 -unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
 -				   unsigned int clamp_value)
 +void deactivate_task(struct rq *rq, struct task_struct *p, int flags)
  {
 -	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
 -	int bucket_id = UCLAMP_BUCKETS - 1;
 -
 -	/*
 -	 * Since both min and max clamps are max aggregated, find the
 -	 * top most bucket with tasks in.
 -	 */
 -	for ( ; bucket_id >= 0; bucket_id--) {
 -		if (!bucket[bucket_id].tasks)
 -			continue;
 -		return bucket[bucket_id].value;
 -	}
 +	p->on_rq = (flags & DEQUEUE_SLEEP) ? 0 : TASK_ON_RQ_MIGRATING;
  
 -	/* No tasks -- default clamp values */
 -	return uclamp_idle_value(rq, clamp_id, clamp_value);
 +	dequeue_task(rq, p, flags);
  }
  
- /*
-  * __normal_prio - return the priority that is based on the static prio
-  */
- static inline int __normal_prio(struct task_struct *p)
 -static void __uclamp_update_util_min_rt_default(struct task_struct *p)
++static inline int __normal_prio(int policy, int rt_prio, int nice)
  {
- 	return p->static_prio;
 -	unsigned int default_util_min;
 -	struct uclamp_se *uc_se;
 -
 -	lockdep_assert_held(&p->pi_lock);
 -
 -	uc_se = &p->uclamp_req[UCLAMP_MIN];
 -
 -	/* Only sync if user didn't override the default */
 -	if (uc_se->user_defined)
 -		return;
 -
 -	default_util_min = sysctl_sched_uclamp_util_min_rt_default;
 -	uclamp_se_set(uc_se, default_util_min, false);
 -}
 -
 -static void uclamp_update_util_min_rt_default(struct task_struct *p)
 -{
 -	struct rq_flags rf;
 -	struct rq *rq;
 -
 -	if (!rt_task(p))
 -		return;
 -
 -	/* Protect updates to p->uclamp_* */
 -	rq = task_rq_lock(p, &rf);
 -	__uclamp_update_util_min_rt_default(p);
 -	task_rq_unlock(rq, p, &rf);
 -}
 -
 -static void uclamp_sync_util_min_rt_default(void)
 -{
 -	struct task_struct *g, *p;
 -
 -	/*
 -	 * copy_process()			sysctl_uclamp
 -	 *					  uclamp_min_rt = X;
 -	 *   write_lock(&tasklist_lock)		  read_lock(&tasklist_lock)
 -	 *   // link thread			  smp_mb__after_spinlock()
 -	 *   write_unlock(&tasklist_lock)	  read_unlock(&tasklist_lock);
 -	 *   sched_post_fork()			  for_each_process_thread()
 -	 *     __uclamp_sync_rt()		    __uclamp_sync_rt()
 -	 *
 -	 * Ensures that either sched_post_fork() will observe the new
 -	 * uclamp_min_rt or for_each_process_thread() will observe the new
 -	 * task.
 -	 */
 -	read_lock(&tasklist_lock);
 -	smp_mb__after_spinlock();
 -	read_unlock(&tasklist_lock);
 -
 -	rcu_read_lock();
 -	for_each_process_thread(g, p)
 -		uclamp_update_util_min_rt_default(p);
 -	rcu_read_unlock();
 -}
 -
 -static inline struct uclamp_se
 -uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
 -{
 -	/* Copy by value as we could modify it */
 -	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
 -#ifdef CONFIG_UCLAMP_TASK_GROUP
 -	unsigned int tg_min, tg_max, value;
 -
 -	/*
 -	 * Tasks in autogroups or root task group will be
 -	 * restricted by system defaults.
 -	 */
 -	if (task_group_is_autogroup(task_group(p)))
 -		return uc_req;
 -	if (task_group(p) == &root_task_group)
 -		return uc_req;
 -
 -	tg_min = task_group(p)->uclamp[UCLAMP_MIN].value;
 -	tg_max = task_group(p)->uclamp[UCLAMP_MAX].value;
 -	value = uc_req.value;
 -	value = clamp(value, tg_min, tg_max);
 -	uclamp_se_set(&uc_req, value, false);
 -#endif
 -
 -	return uc_req;
 -}
 -
 -/*
 - * The effective clamp bucket index of a task depends on, by increasing
 - * priority:
 - * - the task specific clamp value, when explicitly requested from userspace
 - * - the task group effective clamp value, for tasks not either in the root
 - *   group or in an autogroup
 - * - the system default clamp value, defined by the sysadmin
 - */
 -static inline struct uclamp_se
 -uclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)
 -{
 -	struct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);
 -	struct uclamp_se uc_max = uclamp_default[clamp_id];
 -
 -	/* System default restrictions always apply */
 -	if (unlikely(uc_req.value > uc_max.value))
 -		return uc_max;
 -
 -	return uc_req;
 -}
 -
 -unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id)
 -{
 -	struct uclamp_se uc_eff;
 -
 -	/* Task currently refcounted: use back-annotated (effective) value */
 -	if (p->uclamp[clamp_id].active)
 -		return (unsigned long)p->uclamp[clamp_id].value;
 -
 -	uc_eff = uclamp_eff_get(p, clamp_id);
 -
 -	return (unsigned long)uc_eff.value;
 -}
 -
 -/*
 - * When a task is enqueued on a rq, the clamp bucket currently defined by the
 - * task's uclamp::bucket_id is refcounted on that rq. This also immediately
 - * updates the rq's clamp value if required.
 - *
 - * Tasks can have a task-specific value requested from user-space, track
 - * within each bucket the maximum value for tasks refcounted in it.
 - * This "local max aggregation" allows to track the exact "requested" value
 - * for each bucket when all its RUNNABLE tasks require the same clamp.
 - */
 -static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
 -				    enum uclamp_id clamp_id)
 -{
 -	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
 -	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
 -	struct uclamp_bucket *bucket;
 -
 -	lockdep_assert_rq_held(rq);
 -
 -	/* Update task effective clamp */
 -	p->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);
 -
 -	bucket = &uc_rq->bucket[uc_se->bucket_id];
 -	bucket->tasks++;
 -	uc_se->active = true;
 -
 -	uclamp_idle_reset(rq, clamp_id, uc_se->value);
 -
 -	/*
 -	 * Local max aggregation: rq buckets always track the max
 -	 * "requested" clamp value of its RUNNABLE tasks.
 -	 */
 -	if (bucket->tasks == 1 || uc_se->value > bucket->value)
 -		bucket->value = uc_se->value;
 -
 -	if (uc_se->value > READ_ONCE(uc_rq->value))
 -		WRITE_ONCE(uc_rq->value, uc_se->value);
 -}
 -
 -/*
 - * When a task is dequeued from a rq, the clamp bucket refcounted by the task
 - * is released. If this is the last task reference counting the rq's max
 - * active clamp value, then the rq's clamp value is updated.
 - *
 - * Both refcounted tasks and rq's cached clamp values are expected to be
 - * always valid. If it's detected they are not, as defensive programming,
 - * enforce the expected state and warn.
 - */
 -static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
 -				    enum uclamp_id clamp_id)
 -{
 -	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
 -	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
 -	struct uclamp_bucket *bucket;
 -	unsigned int bkt_clamp;
 -	unsigned int rq_clamp;
 -
 -	lockdep_assert_rq_held(rq);
 -
 -	/*
 -	 * If sched_uclamp_used was enabled after task @p was enqueued,
 -	 * we could end up with unbalanced call to uclamp_rq_dec_id().
 -	 *
 -	 * In this case the uc_se->active flag should be false since no uclamp
 -	 * accounting was performed at enqueue time and we can just return
 -	 * here.
 -	 *
 -	 * Need to be careful of the following enqueue/dequeue ordering
 -	 * problem too
 -	 *
 -	 *	enqueue(taskA)
 -	 *	// sched_uclamp_used gets enabled
 -	 *	enqueue(taskB)
 -	 *	dequeue(taskA)
 -	 *	// Must not decrement bucket->tasks here
 -	 *	dequeue(taskB)
 -	 *
 -	 * where we could end up with stale data in uc_se and
 -	 * bucket[uc_se->bucket_id].
 -	 *
 -	 * The following check here eliminates the possibility of such race.
 -	 */
 -	if (unlikely(!uc_se->active))
 -		return;
 -
 -	bucket = &uc_rq->bucket[uc_se->bucket_id];
 -
 -	SCHED_WARN_ON(!bucket->tasks);
 -	if (likely(bucket->tasks))
 -		bucket->tasks--;
 -
 -	uc_se->active = false;
 -
 -	/*
 -	 * Keep "local max aggregation" simple and accept to (possibly)
 -	 * overboost some RUNNABLE tasks in the same bucket.
 -	 * The rq clamp bucket value is reset to its base value whenever
 -	 * there are no more RUNNABLE tasks refcounting it.
 -	 */
 -	if (likely(bucket->tasks))
 -		return;
 -
 -	rq_clamp = READ_ONCE(uc_rq->value);
 -	/*
 -	 * Defensive programming: this should never happen. If it happens,
 -	 * e.g. due to future modification, warn and fixup the expected value.
 -	 */
 -	SCHED_WARN_ON(bucket->value > rq_clamp);
 -	if (bucket->value >= rq_clamp) {
 -		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
 -		WRITE_ONCE(uc_rq->value, bkt_clamp);
 -	}
 -}
 -
 -static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
 -{
 -	enum uclamp_id clamp_id;
 -
 -	/*
 -	 * Avoid any overhead until uclamp is actually used by the userspace.
 -	 *
 -	 * The condition is constructed such that a NOP is generated when
 -	 * sched_uclamp_used is disabled.
 -	 */
 -	if (!static_branch_unlikely(&sched_uclamp_used))
 -		return;
 -
 -	if (unlikely(!p->sched_class->uclamp_enabled))
 -		return;
 -
 -	for_each_clamp_id(clamp_id)
 -		uclamp_rq_inc_id(rq, p, clamp_id);
 -
 -	/* Reset clamp idle holding when there is one RUNNABLE task */
 -	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
 -		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
 -}
 -
 -static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
 -{
 -	enum uclamp_id clamp_id;
 -
 -	/*
 -	 * Avoid any overhead until uclamp is actually used by the userspace.
 -	 *
 -	 * The condition is constructed such that a NOP is generated when
 -	 * sched_uclamp_used is disabled.
 -	 */
 -	if (!static_branch_unlikely(&sched_uclamp_used))
 -		return;
 -
 -	if (unlikely(!p->sched_class->uclamp_enabled))
 -		return;
 -
 -	for_each_clamp_id(clamp_id)
 -		uclamp_rq_dec_id(rq, p, clamp_id);
 -}
 -
 -static inline void
 -uclamp_update_active(struct task_struct *p)
 -{
 -	enum uclamp_id clamp_id;
 -	struct rq_flags rf;
 -	struct rq *rq;
 -
 -	/*
 -	 * Lock the task and the rq where the task is (or was) queued.
 -	 *
 -	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the
 -	 * price to pay to safely serialize util_{min,max} updates with
 -	 * enqueues, dequeues and migration operations.
 -	 * This is the same locking schema used by __set_cpus_allowed_ptr().
 -	 */
 -	rq = task_rq_lock(p, &rf);
 -
 -	/*
 -	 * Setting the clamp bucket is serialized by task_rq_lock().
 -	 * If the task is not yet RUNNABLE and its task_struct is not
 -	 * affecting a valid clamp bucket, the next time it's enqueued,
 -	 * it will already see the updated clamp bucket value.
 -	 */
 -	for_each_clamp_id(clamp_id) {
 -		if (p->uclamp[clamp_id].active) {
 -			uclamp_rq_dec_id(rq, p, clamp_id);
 -			uclamp_rq_inc_id(rq, p, clamp_id);
 -		}
 -	}
 -
 -	task_rq_unlock(rq, p, &rf);
 -}
 -
 -#ifdef CONFIG_UCLAMP_TASK_GROUP
 -static inline void
 -uclamp_update_active_tasks(struct cgroup_subsys_state *css)
 -{
 -	struct css_task_iter it;
 -	struct task_struct *p;
 -
 -	css_task_iter_start(css, 0, &it);
 -	while ((p = css_task_iter_next(&it)))
 -		uclamp_update_active(p);
 -	css_task_iter_end(&it);
 -}
 -
 -static void cpu_util_update_eff(struct cgroup_subsys_state *css);
 -static void uclamp_update_root_tg(void)
 -{
 -	struct task_group *tg = &root_task_group;
 -
 -	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],
 -		      sysctl_sched_uclamp_util_min, false);
 -	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],
 -		      sysctl_sched_uclamp_util_max, false);
 -
 -	rcu_read_lock();
 -	cpu_util_update_eff(&root_task_group.css);
 -	rcu_read_unlock();
 -}
 -#else
 -static void uclamp_update_root_tg(void) { }
 -#endif
 -
 -int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
 -				void *buffer, size_t *lenp, loff_t *ppos)
 -{
 -	bool update_root_tg = false;
 -	int old_min, old_max, old_min_rt;
 -	int result;
 -
 -	mutex_lock(&uclamp_mutex);
 -	old_min = sysctl_sched_uclamp_util_min;
 -	old_max = sysctl_sched_uclamp_util_max;
 -	old_min_rt = sysctl_sched_uclamp_util_min_rt_default;
 -
 -	result = proc_dointvec(table, write, buffer, lenp, ppos);
 -	if (result)
 -		goto undo;
 -	if (!write)
 -		goto done;
 -
 -	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
 -	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE	||
 -	    sysctl_sched_uclamp_util_min_rt_default > SCHED_CAPACITY_SCALE) {
 -
 -		result = -EINVAL;
 -		goto undo;
 -	}
 -
 -	if (old_min != sysctl_sched_uclamp_util_min) {
 -		uclamp_se_set(&uclamp_default[UCLAMP_MIN],
 -			      sysctl_sched_uclamp_util_min, false);
 -		update_root_tg = true;
 -	}
 -	if (old_max != sysctl_sched_uclamp_util_max) {
 -		uclamp_se_set(&uclamp_default[UCLAMP_MAX],
 -			      sysctl_sched_uclamp_util_max, false);
 -		update_root_tg = true;
 -	}
 -
 -	if (update_root_tg) {
 -		static_branch_enable(&sched_uclamp_used);
 -		uclamp_update_root_tg();
 -	}
 -
 -	if (old_min_rt != sysctl_sched_uclamp_util_min_rt_default) {
 -		static_branch_enable(&sched_uclamp_used);
 -		uclamp_sync_util_min_rt_default();
 -	}
 -
 -	/*
 -	 * We update all RUNNABLE tasks only when task groups are in use.
 -	 * Otherwise, keep it simple and do just a lazy update at each next
 -	 * task enqueue time.
 -	 */
 -
 -	goto done;
 -
 -undo:
 -	sysctl_sched_uclamp_util_min = old_min;
 -	sysctl_sched_uclamp_util_max = old_max;
 -	sysctl_sched_uclamp_util_min_rt_default = old_min_rt;
 -done:
 -	mutex_unlock(&uclamp_mutex);
 -
 -	return result;
 -}
 -
 -static int uclamp_validate(struct task_struct *p,
 -			   const struct sched_attr *attr)
 -{
 -	int util_min = p->uclamp_req[UCLAMP_MIN].value;
 -	int util_max = p->uclamp_req[UCLAMP_MAX].value;
 -
 -	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {
 -		util_min = attr->sched_util_min;
 -
 -		if (util_min + 1 > SCHED_CAPACITY_SCALE + 1)
 -			return -EINVAL;
 -	}
 -
 -	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {
 -		util_max = attr->sched_util_max;
 -
 -		if (util_max + 1 > SCHED_CAPACITY_SCALE + 1)
 -			return -EINVAL;
 -	}
 -
 -	if (util_min != -1 && util_max != -1 && util_min > util_max)
 -		return -EINVAL;
 -
 -	/*
 -	 * We have valid uclamp attributes; make sure uclamp is enabled.
 -	 *
 -	 * We need to do that here, because enabling static branches is a
 -	 * blocking operation which obviously cannot be done while holding
 -	 * scheduler locks.
 -	 */
 -	static_branch_enable(&sched_uclamp_used);
 -
 -	return 0;
 -}
 -
 -static bool uclamp_reset(const struct sched_attr *attr,
 -			 enum uclamp_id clamp_id,
 -			 struct uclamp_se *uc_se)
 -{
 -	/* Reset on sched class change for a non user-defined clamp value. */
 -	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)) &&
 -	    !uc_se->user_defined)
 -		return true;
 -
 -	/* Reset on sched_util_{min,max} == -1. */
 -	if (clamp_id == UCLAMP_MIN &&
 -	    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&
 -	    attr->sched_util_min == -1) {
 -		return true;
 -	}
 -
 -	if (clamp_id == UCLAMP_MAX &&
 -	    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&
 -	    attr->sched_util_max == -1) {
 -		return true;
 -	}
 -
 -	return false;
 -}
 -
 -static void __setscheduler_uclamp(struct task_struct *p,
 -				  const struct sched_attr *attr)
 -{
 -	enum uclamp_id clamp_id;
 -
 -	for_each_clamp_id(clamp_id) {
 -		struct uclamp_se *uc_se = &p->uclamp_req[clamp_id];
 -		unsigned int value;
 -
 -		if (!uclamp_reset(attr, clamp_id, uc_se))
 -			continue;
 -
 -		/*
 -		 * RT by default have a 100% boost value that could be modified
 -		 * at runtime.
 -		 */
 -		if (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))
 -			value = sysctl_sched_uclamp_util_min_rt_default;
 -		else
 -			value = uclamp_none(clamp_id);
 -
 -		uclamp_se_set(uc_se, value, false);
 -
 -	}
 -
 -	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
 -		return;
 -
 -	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&
 -	    attr->sched_util_min != -1) {
 -		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
 -			      attr->sched_util_min, true);
 -	}
 -
 -	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&
 -	    attr->sched_util_max != -1) {
 -		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
 -			      attr->sched_util_max, true);
 -	}
 -}
 -
 -static void uclamp_fork(struct task_struct *p)
 -{
 -	enum uclamp_id clamp_id;
 -
 -	/*
 -	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here
 -	 * as the task is still at its early fork stages.
 -	 */
 -	for_each_clamp_id(clamp_id)
 -		p->uclamp[clamp_id].active = false;
 -
 -	if (likely(!p->sched_reset_on_fork))
 -		return;
 -
 -	for_each_clamp_id(clamp_id) {
 -		uclamp_se_set(&p->uclamp_req[clamp_id],
 -			      uclamp_none(clamp_id), false);
 -	}
 -}
 -
 -static void uclamp_post_fork(struct task_struct *p)
 -{
 -	uclamp_update_util_min_rt_default(p);
 -}
 -
 -static void __init init_uclamp_rq(struct rq *rq)
 -{
 -	enum uclamp_id clamp_id;
 -	struct uclamp_rq *uc_rq = rq->uclamp;
 -
 -	for_each_clamp_id(clamp_id) {
 -		uc_rq[clamp_id] = (struct uclamp_rq) {
 -			.value = uclamp_none(clamp_id)
 -		};
 -	}
 -
 -	rq->uclamp_flags = 0;
 -}
 -
 -static void __init init_uclamp(void)
 -{
 -	struct uclamp_se uc_max = {};
 -	enum uclamp_id clamp_id;
 -	int cpu;
 -
 -	for_each_possible_cpu(cpu)
 -		init_uclamp_rq(cpu_rq(cpu));
 -
 -	for_each_clamp_id(clamp_id) {
 -		uclamp_se_set(&init_task.uclamp_req[clamp_id],
 -			      uclamp_none(clamp_id), false);
 -	}
 -
 -	/* System defaults allow max clamp values for both indexes */
 -	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
 -	for_each_clamp_id(clamp_id) {
 -		uclamp_default[clamp_id] = uc_max;
 -#ifdef CONFIG_UCLAMP_TASK_GROUP
 -		root_task_group.uclamp_req[clamp_id] = uc_max;
 -		root_task_group.uclamp[clamp_id] = uc_max;
 -#endif
 -	}
 -}
 -
 -#else /* CONFIG_UCLAMP_TASK */
 -static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
 -static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
 -static inline int uclamp_validate(struct task_struct *p,
 -				  const struct sched_attr *attr)
 -{
 -	return -EOPNOTSUPP;
 -}
 -static void __setscheduler_uclamp(struct task_struct *p,
 -				  const struct sched_attr *attr) { }
 -static inline void uclamp_fork(struct task_struct *p) { }
 -static inline void uclamp_post_fork(struct task_struct *p) { }
 -static inline void init_uclamp(void) { }
 -#endif /* CONFIG_UCLAMP_TASK */
 -
 -bool sched_task_on_rq(struct task_struct *p)
 -{
 -	return task_on_rq_queued(p);
 -}
 -
 -static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 -{
 -	if (!(flags & ENQUEUE_NOCLOCK))
 -		update_rq_clock(rq);
 -
 -	if (!(flags & ENQUEUE_RESTORE)) {
 -		sched_info_enqueue(rq, p);
 -		psi_enqueue(p, flags & ENQUEUE_WAKEUP);
 -	}
 -
 -	uclamp_rq_inc(rq, p);
 -	p->sched_class->enqueue_task(rq, p, flags);
 -
 -	if (sched_core_enabled(rq))
 -		sched_core_enqueue(rq, p);
 -}
 -
 -static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 -{
 -	if (sched_core_enabled(rq))
 -		sched_core_dequeue(rq, p);
 -
 -	if (!(flags & DEQUEUE_NOCLOCK))
 -		update_rq_clock(rq);
 -
 -	if (!(flags & DEQUEUE_SAVE)) {
 -		sched_info_dequeue(rq, p);
 -		psi_dequeue(p, flags & DEQUEUE_SLEEP);
 -	}
 -
 -	uclamp_rq_dec(rq, p);
 -	p->sched_class->dequeue_task(rq, p, flags);
 -}
 -
 -void activate_task(struct rq *rq, struct task_struct *p, int flags)
 -{
 -	enqueue_task(rq, p, flags);
 -
 -	p->on_rq = TASK_ON_RQ_QUEUED;
 -}
 -
 -void deactivate_task(struct rq *rq, struct task_struct *p, int flags)
 -{
 -	p->on_rq = (flags & DEQUEUE_SLEEP) ? 0 : TASK_ON_RQ_MIGRATING;
 -
 -	dequeue_task(rq, p, flags);
 -}
 -
 -static inline int __normal_prio(int policy, int rt_prio, int nice)
 -{
 -	int prio;
++	int prio;
+ 
+ 	if (dl_policy(policy))
+ 		prio = MAX_DL_PRIO - 1;
+ 	else if (rt_policy(policy))
+ 		prio = MAX_RT_PRIO - 1 - rt_prio;
+ 	else
+ 		prio = NICE_TO_PRIO(nice);
+ 
+ 	return prio;
  }
  
  /*
@@@ -4246,27 -6461,24 +4256,24 @@@ void rt_mutex_setprio(struct task_struc
  		if (!dl_prio(p->normal_prio) ||
  		    (pi_task && dl_prio(pi_task->prio) &&
  		     dl_entity_preempt(&pi_task->dl, &p->dl))) {
 -			p->dl.pi_se = pi_task->dl.pi_se;
 +			p->pi_se = pi_task->pi_se;
  			queue_flag |= ENQUEUE_REPLENISH;
  		} else {
 -			p->dl.pi_se = &p->dl;
 +			p->pi_se = &p->dl;
  		}
- 		p->sched_class = &dl_sched_class;
  	} else if (rt_prio(prio)) {
  		if (dl_prio(oldprio))
 -			p->dl.pi_se = &p->dl;
 +			p->pi_se = &p->dl;
  		if (oldprio < prio)
  			queue_flag |= ENQUEUE_HEAD;
- 		p->sched_class = &rt_sched_class;
  	} else {
  		if (dl_prio(oldprio))
 -			p->dl.pi_se = &p->dl;
 +			p->pi_se = &p->dl;
  		if (rt_prio(oldprio))
  			p->rt.timeout = 0;
- 		p->sched_class = &fair_sched_class;
  	}
  
- 	p->prio = prio;
+ 	__setscheduler_prio(p, prio);
  
  	if (queued)
  		enqueue_task(rq, p, queue_flag);
@@@ -4499,28 -6831,6 +4506,31 @@@ static void __setscheduler_params(struc
  	set_load_weight(p, true);
  }
  
++<<<<<<< HEAD
 +/* Actually do priority change: must hold pi & rq lock. */
 +static void __setscheduler(struct rq *rq, struct task_struct *p,
 +			   const struct sched_attr *attr, bool keep_boost)
 +{
 +	__setscheduler_params(p, attr);
 +
 +	/*
 +	 * Keep a potential priority boosting if called from
 +	 * sched_setscheduler().
 +	 */
 +	p->prio = normal_prio(p);
 +	if (keep_boost)
 +		p->prio = rt_effective_prio(p, p->prio);
 +
 +	if (dl_prio(p->prio))
 +		p->sched_class = &dl_sched_class;
 +	else if (rt_prio(p->prio))
 +		p->sched_class = &rt_sched_class;
 +	else
 +		p->sched_class = &fair_sched_class;
 +}
 +
++=======
++>>>>>>> f558c2b834ec (sched/rt: Fix double enqueue caused by rt_effective_prio)
  /*
   * Check the target process has a UID that matches the current process's:
   */
@@@ -4541,11 -6851,10 +4551,9 @@@ static int __sched_setscheduler(struct 
  				const struct sched_attr *attr,
  				bool user, bool pi)
  {
- 	int newprio = dl_policy(attr->sched_policy) ? MAX_DL_PRIO - 1 :
- 		      MAX_RT_PRIO - 1 - attr->sched_priority;
- 	int retval, oldprio, oldpolicy = -1, queued, running;
- 	int new_effective_prio, policy = attr->sched_policy;
+ 	int oldpolicy = -1, policy = attr->sched_policy;
+ 	int retval, oldprio, newprio, queued, running;
  	const struct sched_class *prev_class;
 -	struct callback_head *head;
  	struct rq_flags rf;
  	int reset_on_fork;
  	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
@@@ -4754,7 -7072,12 +4763,16 @@@ change
  		put_prev_task(rq, p);
  
  	prev_class = p->sched_class;
++<<<<<<< HEAD
 +	__setscheduler(rq, p, attr, pi);
++=======
+ 
+ 	if (!(attr->sched_flags & SCHED_FLAG_KEEP_PARAMS)) {
+ 		__setscheduler_params(p, attr);
+ 		__setscheduler_prio(p, newprio);
+ 	}
+ 	__setscheduler_uclamp(p, attr);
++>>>>>>> f558c2b834ec (sched/rt: Fix double enqueue caused by rt_effective_prio)
  
  	if (queued) {
  		/*
* Unmerged path kernel/sched/core.c
