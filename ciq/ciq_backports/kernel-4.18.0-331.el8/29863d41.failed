net: implement threaded-able napi poll loop support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-331.el8
commit-author Wei Wang <weiwan@google.com>
commit 29863d41bb6e1d969c62fdb15b0961806942960e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-331.el8/29863d41.failed

This patch allows running each napi poll loop inside its own
kernel thread.
The kthread is created during netif_napi_add() if dev->threaded
is set. And threaded mode is enabled in napi_enable(). We will
provide a way to set dev->threaded and enable threaded mode
without a device up/down in the following patch.

Once that threaded mode is enabled and the kthread is
started, napi_schedule() will wake-up such thread instead
of scheduling the softirq.

The threaded poll loop behaves quite likely the net_rx_action,
but it does not have to manipulate local irqs and uses
an explicit scheduling point based on netdev_budget.

Co-developed-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
Co-developed-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
Co-developed-by: Jakub Kicinski <kuba@kernel.org>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
	Signed-off-by: Wei Wang <weiwan@google.com>
	Reviewed-by: Alexander Duyck <alexanderduyck@fb.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 29863d41bb6e1d969c62fdb15b0961806942960e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/netdevice.h
#	net/core/dev.c
diff --cc include/linux/netdevice.h
index 4a78471bc3fe,99fb4ec9573e..000000000000
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@@ -343,14 -347,7 +343,18 @@@ struct napi_struct 
  	struct list_head	dev_list;
  	struct hlist_node	napi_hash_node;
  	unsigned int		napi_id;
++<<<<<<< HEAD
 +
 +	RH_KABI_USE(1, 2, struct list_head	gro_list)
 +	RH_KABI_USE(3, int			defer_hard_irqs_count)
 +	RH_KABI_RESERVE(4)
 +	RH_KABI_RESERVE(5)
 +	RH_KABI_RESERVE(6)
 +	RH_KABI_RESERVE(7)
 +	RH_KABI_AUX_EMBED(napi_struct_extended)
++=======
+ 	struct task_struct	*thread;
++>>>>>>> 29863d41bb6e (net: implement threaded-able napi poll loop support)
  };
  
  enum {
@@@ -2230,44 -2136,20 +2225,49 @@@ struct net_device 
  	struct lock_class_key	*qdisc_tx_busylock;
  	struct lock_class_key	*qdisc_running_key;
  	bool			proto_down;
++<<<<<<< HEAD
 +	RH_KABI_FILL_HOLE(unsigned	wol_enabled:1)
++=======
+ 	unsigned		wol_enabled:1;
+ 	unsigned		threaded:1;
++>>>>>>> 29863d41bb6e (net: implement threaded-able napi poll loop support)
  
 -	struct list_head	net_notifier_list;
 +	/* 23 bits hole remain and... */
 +	/* 4 bytes hole remain prior RH_KABI reservations below */
  
 -#if IS_ENABLED(CONFIG_MACSEC)
 -	/* MACsec management functions */
 -	const struct macsec_ops *macsec_ops;
 -#endif
 -	const struct udp_tunnel_nic_info	*udp_tunnel_nic_info;
 -	struct udp_tunnel_nic	*udp_tunnel_nic;
 +	RH_KABI_USE(1, struct mpls_dev __rcu   *mpls_ptr)
 +	RH_KABI_USE(2, 3, struct list_head	net_notifier_list)
 +	RH_KABI_USE(4, struct xdp_dev_bulk_queue __percpu *xdp_bulkq)
 +	RH_KABI_USE(5, const struct udp_tunnel_nic_info	*udp_tunnel_nic_info)
 +	RH_KABI_USE(6, struct udp_tunnel_nic	*udp_tunnel_nic)
  
  	/* protected by rtnl_lock */
 -	struct bpf_xdp_entity	xdp_state[__MAX_XDP_MODE];
 +	RH_KABI_USE(7, struct bpf_xdp_entity	*xdp_state)
 +
 +	RH_KABI_USE(8, int			napi_defer_hard_irqs)
 +	RH_KABI_RESERVE(9)
 +	RH_KABI_RESERVE(10)
 +	RH_KABI_RESERVE(11)
 +	RH_KABI_RESERVE(12)
 +	RH_KABI_RESERVE(13)
 +	RH_KABI_RESERVE(14)
 +	RH_KABI_RESERVE(15)
 +	RH_KABI_RESERVE(16)
 +	RH_KABI_RESERVE(17)
 +	RH_KABI_RESERVE(18)
 +	RH_KABI_RESERVE(19)
 +	RH_KABI_RESERVE(20)
 +	RH_KABI_RESERVE(21)
 +	RH_KABI_RESERVE(22)
 +	RH_KABI_RESERVE(23)
 +	RH_KABI_RESERVE(24)
 +	RH_KABI_RESERVE(25)
 +	RH_KABI_RESERVE(26)
 +	RH_KABI_RESERVE(27)
 +	RH_KABI_RESERVE(28)
 +	RH_KABI_RESERVE(29)
 +	RH_KABI_RESERVE(30)
 +	RH_KABI_AUX_PTR(net_device_extended)
  };
  #define to_net_dev(d) container_of(d, struct net_device, dev)
  
diff --cc net/core/dev.c
index a223efdd8606,1e35f4f44f3b..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -6373,12 -6792,35 +6417,34 @@@ void napi_disable(struct napi_struct *n
  }
  EXPORT_SYMBOL(napi_disable);
  
++<<<<<<< HEAD
 +static void gro_list_free(struct list_head *head)
++=======
+ /**
+  *	napi_enable - enable NAPI scheduling
+  *	@n: NAPI context
+  *
+  * Resume NAPI from being scheduled on this context.
+  * Must be paired with napi_disable.
+  */
+ void napi_enable(struct napi_struct *n)
+ {
+ 	BUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));
+ 	smp_mb__before_atomic();
+ 	clear_bit(NAPI_STATE_SCHED, &n->state);
+ 	clear_bit(NAPI_STATE_NPSVC, &n->state);
+ 	if (n->dev->threaded && n->thread)
+ 		set_bit(NAPI_STATE_THREADED, &n->state);
+ }
+ EXPORT_SYMBOL(napi_enable);
+ 
+ static void flush_gro_hash(struct napi_struct *napi)
++>>>>>>> 29863d41bb6e (net: implement threaded-able napi poll loop support)
  {
 -	int i;
 -
 -	for (i = 0; i < GRO_HASH_BUCKETS; i++) {
 -		struct sk_buff *skb, *n;
 +	struct sk_buff *skb, *p;
  
 -		list_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)
 -			kfree_skb(skb);
 -		napi->gro_hash[i].count = 0;
 -	}
 +	list_for_each_entry_safe(skb, p, head, list)
 +		kfree_skb(skb);
  }
  
  /* Must be called in process context */
@@@ -6391,9 -6833,13 +6457,19 @@@ void __netif_napi_del(struct napi_struc
  	list_del_rcu(&napi->dev_list);
  	napi_free_frags(napi);
  
++<<<<<<< HEAD
 +	gro_list_free(&napi->gro_list);
 +	INIT_LIST_HEAD(&napi->gro_list);
 +	napi->gro_count = 0;
++=======
+ 	flush_gro_hash(napi);
+ 	napi->gro_bitmask = 0;
+ 
+ 	if (napi->thread) {
+ 		kthread_stop(napi->thread);
+ 		napi->thread = NULL;
+ 	}
++>>>>>>> 29863d41bb6e (net: implement threaded-able napi poll loop support)
  }
  EXPORT_SYMBOL(__netif_napi_del);
  
* Unmerged path include/linux/netdevice.h
* Unmerged path net/core/dev.c
