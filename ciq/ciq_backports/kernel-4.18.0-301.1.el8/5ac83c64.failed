Revert "blk-mq, elevator: Count requests per hctx to improve performance"

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-301.1.el8
commit-author Jan Kara <jack@suse.cz>
commit 5ac83c644f5fb924f0b2c09102ab82fc788f8411
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-301.1.el8/5ac83c64.failed

This reverts commit b445547ec1bbd3e7bf4b1c142550942f70527d95.

Since both mq-deadline and BFQ completely ignore hctx they are passed to
their dispatch function and dispatch whatever request they deem fit
checking whether any request for a particular hctx is queued is just
pointless since we'll very likely get a request from a different hctx
anyway. In the following commit we'll deal with lock contention in these
IO schedulers in presence of multiple HW queues in a different way.

	Signed-off-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5ac83c644f5fb924f0b2c09102ab82fc788f8411)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blk-mq.h
diff --cc include/linux/blk-mq.h
index 4b5138d28034,aabbf6830ffc..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -15,56 -16,143 +15,59 @@@ struct blk_flush_queue
   */
  struct blk_mq_hw_ctx {
  	struct {
 -		/** @lock: Protects the dispatch list. */
  		spinlock_t		lock;
 -		/**
 -		 * @dispatch: Used for requests that are ready to be
 -		 * dispatched to the hardware but for some reason (e.g. lack of
 -		 * resources) could not be sent to the hardware. As soon as the
 -		 * driver can send new requests, requests at this list will
 -		 * be sent first for a fairer dispatch.
 -		 */
  		struct list_head	dispatch;
 -		 /**
 -		  * @state: BLK_MQ_S_* flags. Defines the state of the hw
 -		  * queue (active, scheduled to restart, stopped).
 -		  */
 -		unsigned long		state;
 +		unsigned long		state;		/* BLK_MQ_S_* flags */
  	} ____cacheline_aligned_in_smp;
  
 -	/**
 -	 * @run_work: Used for scheduling a hardware queue run at a later time.
 -	 */
  	struct delayed_work	run_work;
 -	/** @cpumask: Map of available CPUs where this hctx can run. */
  	cpumask_var_t		cpumask;
 -	/**
 -	 * @next_cpu: Used by blk_mq_hctx_next_cpu() for round-robin CPU
 -	 * selection from @cpumask.
 -	 */
  	int			next_cpu;
 -	/**
 -	 * @next_cpu_batch: Counter of how many works left in the batch before
 -	 * changing to the next CPU.
 -	 */
  	int			next_cpu_batch;
  
 -	/** @flags: BLK_MQ_F_* flags. Defines the behaviour of the queue. */
 -	unsigned long		flags;
 +	unsigned long		flags;		/* BLK_MQ_F_* flags */
  
 -	/**
 -	 * @sched_data: Pointer owned by the IO scheduler attached to a request
 -	 * queue. It's up to the IO scheduler how to use this pointer.
 -	 */
  	void			*sched_data;
 -	/**
 -	 * @queue: Pointer to the request queue that owns this hardware context.
 -	 */
  	struct request_queue	*queue;
 -	/** @fq: Queue of requests that need to perform a flush operation. */
  	struct blk_flush_queue	*fq;
  
 -	/**
 -	 * @driver_data: Pointer to data owned by the block driver that created
 -	 * this hctx
 -	 */
  	void			*driver_data;
  
 -	/**
 -	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 -	 * pending request in that software queue.
 -	 */
  	struct sbitmap		ctx_map;
  
 -	/**
 -	 * @dispatch_from: Software queue to be used when no scheduler was
 -	 * selected.
 -	 */
  	struct blk_mq_ctx	*dispatch_from;
 -	/**
 -	 * @dispatch_busy: Number used by blk_mq_update_dispatch_busy() to
 -	 * decide if the hw_queue is busy using Exponential Weighted Moving
 -	 * Average algorithm.
 -	 */
  	unsigned int		dispatch_busy;
  
 -	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
  	unsigned short		type;
 -	/** @nr_ctx: Number of software queues. */
  	unsigned short		nr_ctx;
 -	/** @ctxs: Array of software queues. */
  	struct blk_mq_ctx	**ctxs;
  
 -	/** @dispatch_wait_lock: Lock for dispatch_wait queue. */
  	spinlock_t		dispatch_wait_lock;
 -	/**
 -	 * @dispatch_wait: Waitqueue to put requests when there is no tag
 -	 * available at the moment, to wait for another try in the future.
 -	 */
  	wait_queue_entry_t	dispatch_wait;
 -
 -	/**
 -	 * @wait_index: Index of next available dispatch_wait queue to insert
 -	 * requests.
 -	 */
  	atomic_t		wait_index;
  
 -	/**
 -	 * @tags: Tags owned by the block driver. A tag at this set is only
 -	 * assigned when a request is dispatched from a hardware queue.
 -	 */
  	struct blk_mq_tags	*tags;
 -	/**
 -	 * @sched_tags: Tags owned by I/O scheduler. If there is an I/O
 -	 * scheduler associated with a request queue, a tag is assigned when
 -	 * that request is allocated. Else, this member is not used.
 -	 */
  	struct blk_mq_tags	*sched_tags;
  
 -	/** @queued: Number of queued requests. */
  	unsigned long		queued;
 -	/** @run: Number of dispatched requests. */
  	unsigned long		run;
  #define BLK_MQ_MAX_DISPATCH_ORDER	7
 -	/** @dispatched: Number of dispatch requests by queue. */
  	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
  
 -	/** @numa_node: NUMA node the storage adapter has been connected to. */
  	unsigned int		numa_node;
 -	/** @queue_num: Index of this hardware queue. */
  	unsigned int		queue_num;
  
 -	/**
 -	 * @nr_active: Number of active requests. Only used when a tag set is
 -	 * shared across request queues.
 -	 */
  	atomic_t		nr_active;
++<<<<<<< HEAD
 +	RH_KABI_DEPRECATE(unsigned int,	nr_expired)
++=======
++>>>>>>> 5ac83c644f5f (Revert "blk-mq, elevator: Count requests per hctx to improve performance")
  
 -	/** @cpuhp_online: List to store request if CPU is going to die */
 -	struct hlist_node	cpuhp_online;
 -	/** @cpuhp_dead: List to store request if some CPU die. */
  	struct hlist_node	cpuhp_dead;
 -	/** @kobj: Kernel object for sysfs. */
  	struct kobject		kobj;
  
 -	/** @poll_considered: Count times blk_poll() was called. */
  	unsigned long		poll_considered;
 -	/** @poll_invoked: Count how many requests blk_poll() polled. */
  	unsigned long		poll_invoked;
 -	/** @poll_success: Count how many polled requests were completed. */
  	unsigned long		poll_success;
  
  #ifdef CONFIG_BLK_DEBUG_FS
diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 7b906952de25..9ebbb37b9775 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -4648,9 +4648,6 @@ static bool bfq_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 
-	if (!atomic_read(&hctx->elevator_queued))
-		return false;
-
 	/*
 	 * Avoiding lock: a race on bfqd->busy_queues should cause at
 	 * most a call to dispatch for nothing
@@ -5564,7 +5561,6 @@ static void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,
 		rq = list_first_entry(list, struct request, queuelist);
 		list_del_init(&rq->queuelist);
 		bfq_insert_request(hctx, rq, at_head);
-		atomic_inc(&hctx->elevator_queued);
 	}
 }
 
@@ -5944,7 +5940,6 @@ static void bfq_finish_requeue_request(struct request *rq)
 
 		bfq_completed_request(bfqq, bfqd);
 		bfq_finish_requeue_request_body(bfqq);
-		atomic_dec(&rq->mq_hctx->elevator_queued);
 
 		spin_unlock_irqrestore(&bfqd->lock, flags);
 	} else {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index dc7fc813ad5c..75f6ee28ab67 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -2661,7 +2661,6 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		goto free_hctx;
 
 	atomic_set(&hctx->nr_active, 0);
-	atomic_set(&hctx->elevator_queued, 0);
 	if (node == NUMA_NO_NODE)
 		node = set->numa_node;
 	hctx->numa_node = node;
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 01f4bb4a72ee..b943c475617f 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -395,8 +395,6 @@ static struct request *dd_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	    !list_empty(&dd->fifo_list[WRITE]))
 		blk_mq_sched_mark_restart_hctx(hctx);
 	spin_unlock(&dd->lock);
-	if (rq)
-		atomic_dec(&rq->mq_hctx->elevator_queued);
 
 	return rq;
 }
@@ -543,7 +541,6 @@ static void dd_insert_requests(struct blk_mq_hw_ctx *hctx,
 		rq = list_first_entry(list, struct request, queuelist);
 		list_del_init(&rq->queuelist);
 		dd_insert_request(hctx, rq, at_head);
-		atomic_inc(&hctx->elevator_queued);
 	}
 	spin_unlock(&dd->lock);
 }
@@ -581,9 +578,6 @@ static bool dd_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct deadline_data *dd = hctx->queue->elevator->elevator_data;
 
-	if (!atomic_read(&hctx->elevator_queued))
-		return false;
-
 	return !list_empty_careful(&dd->dispatch) ||
 		!list_empty_careful(&dd->fifo_list[0]) ||
 		!list_empty_careful(&dd->fifo_list[1]);
* Unmerged path include/linux/blk-mq.h
