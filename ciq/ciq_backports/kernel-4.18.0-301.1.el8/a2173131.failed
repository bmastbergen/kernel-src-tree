net/mlx5e: CT: manage the lifetime of the ct entry object

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-301.1.el8
commit-author Oz Shlomo <ozsh@nvidia.com>
commit a2173131526dc845eb1968a15bc192b3fc2ff000
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-301.1.el8/a2173131.failed

The ct entry object is accessed by the ct add, del, stats and restore
methods. In addition, it is referenced from several hash tables.

The lifetime of the ct entry object was not managed which triggered race
conditions as in the following kasan dump:
[ 3374.973945] ==================================================================
[ 3374.988552] BUG: KASAN: use-after-free in memcmp+0x4c/0x98
[ 3374.999590] Read of size 1 at addr ffff00036129ea55 by task ksoftirqd/1/15
[ 3375.016415] CPU: 1 PID: 15 Comm: ksoftirqd/1 Tainted: G           O      5.4.31+ #1
[ 3375.055301] Call trace:
[ 3375.060214]  dump_backtrace+0x0/0x238
[ 3375.067580]  show_stack+0x24/0x30
[ 3375.074244]  dump_stack+0xe0/0x118
[ 3375.081085]  print_address_description.isra.9+0x74/0x3d0
[ 3375.091771]  __kasan_report+0x198/0x1e8
[ 3375.099486]  kasan_report+0xc/0x18
[ 3375.106324]  __asan_load1+0x60/0x68
[ 3375.113338]  memcmp+0x4c/0x98
[ 3375.119409]  mlx5e_tc_ct_restore_flow+0x3a4/0x6f8 [mlx5_core]
[ 3375.131073]  mlx5e_rep_tc_update_skb+0x1d4/0x2f0 [mlx5_core]
[ 3375.142553]  mlx5e_handle_rx_cqe_rep+0x198/0x308 [mlx5_core]
[ 3375.154034]  mlx5e_poll_rx_cq+0x2a0/0x1060 [mlx5_core]
[ 3375.164459]  mlx5e_napi_poll+0x1d4/0xa78 [mlx5_core]
[ 3375.174453]  net_rx_action+0x28c/0x7a8
[ 3375.182004]  __do_softirq+0x1b4/0x5d0

Manage the lifetime of the ct entry object by using synchornization
mechanisms for concurrent access.

Fixes: ac991b48d43c ("net/mlx5e: CT: Offload established flows")
	Signed-off-by: Roi Dayan <roid@nvidia.com>
	Signed-off-by: Oz Shlomo <ozsh@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit a2173131526dc845eb1968a15bc192b3fc2ff000)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c
index 5ca0fb79498c,24e2c0d955b9..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c
@@@ -12,9 -12,10 +12,10 @@@
  #include <net/flow_offload.h>
  #include <net/netfilter/nf_flow_table.h>
  #include <linux/workqueue.h>
+ #include <linux/refcount.h>
  #include <linux/xarray.h>
  
 -#include "lib/fs_chains.h"
 +#include "esw/chains.h"
  #include "en/tc_ct.h"
  #include "en/mod_hdr.h"
  #include "en/mapping.h"
@@@ -52,6 -54,9 +53,12 @@@ struct mlx5_tc_ct_priv 
  	struct mutex control_lock; /* guards parallel adds/dels */
  	struct mapping_ctx *zone_mapping;
  	struct mapping_ctx *labels_mapping;
++<<<<<<< HEAD
++=======
+ 	enum mlx5_flow_namespace_type ns_type;
+ 	struct mlx5_fs_chains *chains;
+ 	spinlock_t ht_lock; /* protects ft entries */
++>>>>>>> a2173131526d (net/mlx5e: CT: manage the lifetime of the ct entry object)
  };
  
  struct mlx5_ct_flow {
@@@ -114,6 -119,16 +121,19 @@@ struct mlx5_ct_tuple 
  	u16 zone;
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_ct_counter {
+ 	struct mlx5_fc *counter;
+ 	refcount_t refcount;
+ 	bool is_shared;
+ };
+ 
+ enum {
+ 	MLX5_CT_ENTRY_FLAG_VALID,
+ };
+ 
++>>>>>>> a2173131526d (net/mlx5e: CT: manage the lifetime of the ct entry object)
  struct mlx5_ct_entry {
  	struct rhash_head node;
  	struct rhash_head tuple_node;
@@@ -713,6 -751,171 +739,174 @@@ err_mod_hdr
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static bool
+ mlx5_tc_ct_entry_valid(struct mlx5_ct_entry *entry)
+ {
+ 	return test_bit(MLX5_CT_ENTRY_FLAG_VALID, &entry->flags);
+ }
+ 
+ static struct mlx5_ct_entry *
+ mlx5_tc_ct_entry_get(struct mlx5_tc_ct_priv *ct_priv, struct mlx5_ct_tuple *tuple)
+ {
+ 	struct mlx5_ct_entry *entry;
+ 
+ 	entry = rhashtable_lookup_fast(&ct_priv->ct_tuples_ht, tuple,
+ 				       tuples_ht_params);
+ 	if (entry && mlx5_tc_ct_entry_valid(entry) &&
+ 	    refcount_inc_not_zero(&entry->refcnt)) {
+ 		return entry;
+ 	} else if (!entry) {
+ 		entry = rhashtable_lookup_fast(&ct_priv->ct_tuples_nat_ht,
+ 					       tuple, tuples_nat_ht_params);
+ 		if (entry && mlx5_tc_ct_entry_valid(entry) &&
+ 		    refcount_inc_not_zero(&entry->refcnt))
+ 			return entry;
+ 	}
+ 
+ 	return entry ? ERR_PTR(-EINVAL) : NULL;
+ }
+ 
+ static void mlx5_tc_ct_entry_remove_from_tuples(struct mlx5_ct_entry *entry)
+ {
+ 	struct mlx5_tc_ct_priv *ct_priv = entry->ct_priv;
+ 
+ 	rhashtable_remove_fast(&ct_priv->ct_tuples_nat_ht,
+ 			       &entry->tuple_nat_node,
+ 			       tuples_nat_ht_params);
+ 	rhashtable_remove_fast(&ct_priv->ct_tuples_ht, &entry->tuple_node,
+ 			       tuples_ht_params);
+ }
+ 
+ static void mlx5_tc_ct_entry_del(struct mlx5_ct_entry *entry)
+ {
+ 	struct mlx5_tc_ct_priv *ct_priv = entry->ct_priv;
+ 
+ 	mlx5_tc_ct_entry_del_rules(ct_priv, entry);
+ 
+ 	spin_lock_bh(&ct_priv->ht_lock);
+ 	mlx5_tc_ct_entry_remove_from_tuples(entry);
+ 	spin_unlock_bh(&ct_priv->ht_lock);
+ 
+ 	mlx5_tc_ct_counter_put(ct_priv, entry);
+ 	kfree(entry);
+ }
+ 
+ static void
+ mlx5_tc_ct_entry_put(struct mlx5_ct_entry *entry)
+ {
+ 	if (!refcount_dec_and_test(&entry->refcnt))
+ 		return;
+ 
+ 	mlx5_tc_ct_entry_del(entry);
+ }
+ 
+ static void mlx5_tc_ct_entry_del_work(struct work_struct *work)
+ {
+ 	struct mlx5_ct_entry *entry = container_of(work, struct mlx5_ct_entry, work);
+ 
+ 	mlx5_tc_ct_entry_del(entry);
+ }
+ 
+ static void
+ __mlx5_tc_ct_entry_put(struct mlx5_ct_entry *entry)
+ {
+ 	struct mlx5e_priv *priv;
+ 
+ 	if (!refcount_dec_and_test(&entry->refcnt))
+ 		return;
+ 
+ 	priv = netdev_priv(entry->ct_priv->netdev);
+ 	INIT_WORK(&entry->work, mlx5_tc_ct_entry_del_work);
+ 	queue_work(priv->wq, &entry->work);
+ }
+ 
+ static struct mlx5_ct_counter *
+ mlx5_tc_ct_counter_create(struct mlx5_tc_ct_priv *ct_priv)
+ {
+ 	struct mlx5_ct_counter *counter;
+ 	int ret;
+ 
+ 	counter = kzalloc(sizeof(*counter), GFP_KERNEL);
+ 	if (!counter)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	counter->is_shared = false;
+ 	counter->counter = mlx5_fc_create(ct_priv->dev, true);
+ 	if (IS_ERR(counter->counter)) {
+ 		ct_dbg("Failed to create counter for ct entry");
+ 		ret = PTR_ERR(counter->counter);
+ 		kfree(counter);
+ 		return ERR_PTR(ret);
+ 	}
+ 
+ 	return counter;
+ }
+ 
+ static struct mlx5_ct_counter *
+ mlx5_tc_ct_shared_counter_get(struct mlx5_tc_ct_priv *ct_priv,
+ 			      struct mlx5_ct_entry *entry)
+ {
+ 	struct mlx5_ct_tuple rev_tuple = entry->tuple;
+ 	struct mlx5_ct_counter *shared_counter;
+ 	struct mlx5_ct_entry *rev_entry;
+ 	__be16 tmp_port;
+ 	int ret;
+ 
+ 	/* get the reversed tuple */
+ 	tmp_port = rev_tuple.port.src;
+ 	rev_tuple.port.src = rev_tuple.port.dst;
+ 	rev_tuple.port.dst = tmp_port;
+ 
+ 	if (rev_tuple.addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
+ 		__be32 tmp_addr = rev_tuple.ip.src_v4;
+ 
+ 		rev_tuple.ip.src_v4 = rev_tuple.ip.dst_v4;
+ 		rev_tuple.ip.dst_v4 = tmp_addr;
+ 	} else if (rev_tuple.addr_type == FLOW_DISSECTOR_KEY_IPV6_ADDRS) {
+ 		struct in6_addr tmp_addr = rev_tuple.ip.src_v6;
+ 
+ 		rev_tuple.ip.src_v6 = rev_tuple.ip.dst_v6;
+ 		rev_tuple.ip.dst_v6 = tmp_addr;
+ 	} else {
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 	}
+ 
+ 	/* Use the same counter as the reverse direction */
+ 	spin_lock_bh(&ct_priv->ht_lock);
+ 	rev_entry = mlx5_tc_ct_entry_get(ct_priv, &rev_tuple);
+ 
+ 	if (IS_ERR(rev_entry)) {
+ 		spin_unlock_bh(&ct_priv->ht_lock);
+ 		goto create_counter;
+ 	}
+ 
+ 	if (rev_entry && refcount_inc_not_zero(&rev_entry->counter->refcount)) {
+ 		ct_dbg("Using shared counter entry=0x%p rev=0x%p\n", entry, rev_entry);
+ 		shared_counter = rev_entry->counter;
+ 		spin_unlock_bh(&ct_priv->ht_lock);
+ 
+ 		mlx5_tc_ct_entry_put(rev_entry);
+ 		return shared_counter;
+ 	}
+ 
+ 	spin_unlock_bh(&ct_priv->ht_lock);
+ 
+ create_counter:
+ 
+ 	shared_counter = mlx5_tc_ct_counter_create(ct_priv);
+ 	if (IS_ERR(shared_counter)) {
+ 		ret = PTR_ERR(shared_counter);
+ 		return ERR_PTR(ret);
+ 	}
+ 
+ 	shared_counter->is_shared = true;
+ 	refcount_set(&shared_counter->refcount, 1);
+ 	return shared_counter;
+ }
+ 
++>>>>>>> a2173131526d (net/mlx5e: CT: manage the lifetime of the ct entry object)
  static int
  mlx5_tc_ct_entry_add_rules(struct mlx5_tc_ct_priv *ct_priv,
  			   struct flow_rule *flow_rule,
@@@ -829,19 -1050,6 +1039,22 @@@ err_set
  	return err;
  }
  
++<<<<<<< HEAD
 +static void
 +mlx5_tc_ct_del_ft_entry(struct mlx5_tc_ct_priv *ct_priv,
 +			struct mlx5_ct_entry *entry)
 +{
 +	mlx5_tc_ct_entry_del_rules(ct_priv, entry);
 +	if (mlx5_tc_ct_entry_has_nat(entry))
 +		rhashtable_remove_fast(&ct_priv->ct_tuples_nat_ht,
 +				       &entry->tuple_nat_node,
 +				       tuples_nat_ht_params);
 +	rhashtable_remove_fast(&ct_priv->ct_tuples_ht, &entry->tuple_node,
 +			       tuples_ht_params);
 +}
 +
++=======
++>>>>>>> a2173131526d (net/mlx5e: CT: manage the lifetime of the ct entry object)
  static int
  mlx5_tc_ct_block_flow_offload_del(struct mlx5_ct_ft *ft,
  				  struct flow_cls_offload *flow)
@@@ -871,12 -1088,21 +1093,21 @@@ mlx5_tc_ct_block_flow_offload_stats(str
  	struct mlx5_ct_entry *entry;
  	u64 lastuse, packets, bytes;
  
- 	entry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie,
- 				       cts_ht_params);
- 	if (!entry)
+ 	spin_lock_bh(&ct_priv->ht_lock);
+ 	entry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie, cts_ht_params);
+ 	if (!entry) {
+ 		spin_unlock_bh(&ct_priv->ht_lock);
  		return -ENOENT;
+ 	}
+ 
+ 	if (!mlx5_tc_ct_entry_valid(entry) || !refcount_inc_not_zero(&entry->refcnt)) {
+ 		spin_unlock_bh(&ct_priv->ht_lock);
+ 		return -EINVAL;
+ 	}
+ 
+ 	spin_unlock_bh(&ct_priv->ht_lock);
  
 -	mlx5_fc_query_cached(entry->counter->counter, &bytes, &packets, &lastuse);
 +	mlx5_fc_query_cached(entry->counter, &bytes, &packets, &lastuse);
  	flow_stats_update(&f->stats, bytes, packets, 0, lastuse,
  			  FLOW_ACTION_HW_STATS_DELAYED);
  
@@@ -1821,12 -2078,18 +2051,22 @@@ mlx5_tc_ct_init(struct mlx5_rep_uplink_
  		goto err_mapping_labels;
  	}
  
++<<<<<<< HEAD
 +	ct_priv->esw = esw;
 +	ct_priv->netdev = rpriv->netdev;
 +	ct_priv->ct = mlx5_esw_chains_create_global_table(esw);
++=======
+ 	spin_lock_init(&ct_priv->ht_lock);
+ 	ct_priv->ns_type = ns_type;
+ 	ct_priv->chains = chains;
+ 	ct_priv->netdev = priv->netdev;
+ 	ct_priv->dev = priv->mdev;
+ 	ct_priv->mod_hdr_tbl = mod_hdr;
+ 	ct_priv->ct = mlx5_chains_create_global_table(chains);
++>>>>>>> a2173131526d (net/mlx5e: CT: manage the lifetime of the ct entry object)
  	if (IS_ERR(ct_priv->ct)) {
  		err = PTR_ERR(ct_priv->ct);
 -		mlx5_core_warn(dev,
 -			       "%s, failed to create ct table err: %d\n",
 -			       INIT_ERR_PREFIX, err);
 +		mlx5_tc_ct_init_err(rpriv, "failed to create ct table", err);
  		goto err_ct_tbl;
  	}
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c
