arm64: tlbflush: Ensure start/end of address range are aligned to stride

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-338.el8
commit-author Will Deacon <will.deacon@arm.com>
commit 01d57485fcdb9f9101a10a18e32d5f8b023cab86
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-338.el8/01d57485.failed

Since commit 3d65b6bbc01e ("arm64: tlbi: Set MAX_TLBI_OPS to
PTRS_PER_PTE"), we resort to per-ASID invalidation when attempting to
perform more than PTRS_PER_PTE invalidation instructions in a single
call to __flush_tlb_range(). Whilst this is beneficial, the mmu_gather
code does not ensure that the end address of the range is rounded-up
to the stride when freeing intermediate page tables in pXX_free_tlb(),
which defeats our range checking.

Align the bounds passed into __flush_tlb_range().

	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Reported-by: Hanjun Guo <guohanjun@huawei.com>
	Tested-by: Hanjun Guo <guohanjun@huawei.com>
	Reviewed-by: Hanjun Guo <guohanjun@huawei.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit 01d57485fcdb9f9101a10a18e32d5f8b023cab86)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/tlbflush.h
diff --cc arch/arm64/include/asm/tlbflush.h
index a5b457f860a3,dff8f9ea5754..000000000000
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@@ -256,13 -192,14 +256,21 @@@ static inline void __flush_tlb_range(st
  				     unsigned long start, unsigned long end,
  				     unsigned long stride, bool last_level)
  {
 -	unsigned long asid = ASID(vma->vm_mm);
 +	struct mm_struct *mm = vma->vm_mm;
 +	unsigned long asid = ASID(mm);
  	unsigned long addr;
 +	enum tlb_flush_types flush;
  
++<<<<<<< HEAD
 +	if ((end - start) > (MAX_TLBI_OPS * stride)) {
 +		flush_tlb_mm(mm);
++=======
+ 	start = round_down(start, stride);
+ 	end = round_up(end, stride);
+ 
+ 	if ((end - start) >= (MAX_TLBI_OPS * stride)) {
+ 		flush_tlb_mm(vma->vm_mm);
++>>>>>>> 01d57485fcdb (arm64: tlbflush: Ensure start/end of address range are aligned to stride)
  		return;
  	}
  
* Unmerged path arch/arm64/include/asm/tlbflush.h
