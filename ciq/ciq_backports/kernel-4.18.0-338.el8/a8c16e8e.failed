crypto/chcr: move nic TLS functionality to drivers/net

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-338.el8
commit-author Rohit Maheshwari <rohitm@chelsio.com>
commit a8c16e8ed624f24b2b082fb9a193e0132a5fd108
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-338.el8/a8c16e8e.failed

This patch moves complete nic tls offload (kTLS) code from crypto
directory to drivers/net/ethernet/chelsio/inline_crypto/ch_ktls
directory. nic TLS is made a separate ULD of cxgb4.

	Signed-off-by: Rohit Maheshwari <rohitm@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a8c16e8ed624f24b2b082fb9a193e0132a5fd108)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/Kconfig
#	drivers/crypto/chelsio/Makefile
#	drivers/crypto/chelsio/chcr_core.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
#	drivers/net/ethernet/chelsio/inline_crypto/Kconfig
#	drivers/net/ethernet/chelsio/inline_crypto/Makefile
diff --cc drivers/crypto/chelsio/Kconfig
index 25cc72cde7ef,f886401af13e..000000000000
--- a/drivers/crypto/chelsio/Kconfig
+++ b/drivers/crypto/chelsio/Kconfig
@@@ -19,35 -21,3 +19,38 @@@ config CRYPTO_DEV_CHELSI
  
  	  To compile this driver as a module, choose M here: the module
  	  will be called chcr.
++<<<<<<< HEAD
 +
 +config CHELSIO_IPSEC_INLINE
 +        bool "Chelsio IPSec XFRM Tx crypto offload"
 +        depends on CHELSIO_T4
 +	depends on CRYPTO_DEV_CHELSIO
 +        depends on XFRM_OFFLOAD
 +        depends on INET_ESP_OFFLOAD || INET6_ESP_OFFLOAD
 +        default n
 +        ---help---
 +          Enable support for IPSec Tx Inline.
 +
 +config CRYPTO_DEV_CHELSIO_TLS
 +        tristate "Chelsio Crypto Inline TLS Driver"
 +        depends on CHELSIO_T4
 +        depends on TLS_TOE
 +        select CRYPTO_DEV_CHELSIO
 +        ---help---
 +          Support Chelsio Inline TLS with Chelsio crypto accelerator.
 +
 +	  To compile this driver as a module, choose M here: the module
 +	  will be called chtls.
 +
 +config CHELSIO_TLS_DEVICE
 +	bool "Chelsio Inline KTLS Offload"
 +	depends on CHELSIO_T4
 +	depends on TLS_DEVICE
 +	select CRYPTO_DEV_CHELSIO
 +	default y
 +	help
 +	  This flag enables support for kernel tls offload over Chelsio T6
 +	  crypto accelerator. CONFIG_CHELSIO_TLS_DEVICE flag can be enabled
 +	  only if CONFIG_TLS and CONFIG_TLS_DEVICE flags are enabled.
++=======
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
diff --cc drivers/crypto/chelsio/Makefile
index d2cd3e262673,2e5df484ab01..000000000000
--- a/drivers/crypto/chelsio/Makefile
+++ b/drivers/crypto/chelsio/Makefile
@@@ -2,8 -3,3 +2,11 @@@ ccflags-y := -Idrivers/net/ethernet/che
  
  obj-$(CONFIG_CRYPTO_DEV_CHELSIO) += chcr.o
  chcr-objs :=  chcr_core.o chcr_algo.o
++<<<<<<< HEAD
 +#ifdef CONFIG_CHELSIO_TLS_DEVICE
 +chcr-objs += chcr_ktls.o
 +#endif
 +chcr-$(CONFIG_CHELSIO_IPSEC_INLINE) += chcr_ipsec.o
 +obj-$(CONFIG_CRYPTO_DEV_CHELSIO_TLS) += chtls/
++=======
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
diff --cc drivers/crypto/chelsio/chcr_core.c
index bd8dac806e7a,40d51d2bd935..000000000000
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@@ -33,23 -33,8 +33,22 @@@ static int cpl_fw6_pld_handler(struct a
  static void *chcr_uld_add(const struct cxgb4_lld_info *lld);
  static int chcr_uld_state_change(void *handle, enum cxgb4_state state);
  
++<<<<<<< HEAD
 +#if defined(CONFIG_CHELSIO_TLS_DEVICE)
 +static const struct tlsdev_ops chcr_ktls_ops = {
 +	.tls_dev_add = chcr_ktls_dev_add,
 +	.tls_dev_del = chcr_ktls_dev_del,
 +};
 +#endif
 +
 +#ifdef CONFIG_CHELSIO_IPSEC_INLINE
 +static void update_netdev_features(void);
 +#endif /* CONFIG_CHELSIO_IPSEC_INLINE */
 +
++=======
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  static chcr_handler_func work_handlers[NUM_CPL_CMDS] = {
  	[CPL_FW6_PLD] = cpl_fw6_pld_handler,
- #ifdef CONFIG_CHELSIO_TLS_DEVICE
- 	[CPL_ACT_OPEN_RPL] = chcr_ktls_cpl_act_open_rpl,
- 	[CPL_SET_TCB_RPL] = chcr_ktls_cpl_set_tcb_rpl,
- #endif
  };
  
  static struct cxgb4_uld_info chcr_uld_info = {
@@@ -60,12 -45,6 +59,15 @@@
  	.add = chcr_uld_add,
  	.state_change = chcr_uld_state_change,
  	.rx_handler = chcr_uld_rx_handler,
++<<<<<<< HEAD
 +#if defined(CONFIG_CHELSIO_IPSEC_INLINE) || defined(CONFIG_CHELSIO_TLS_DEVICE)
 +	.tx_handler = chcr_uld_tx_handler,
 +#endif /* CONFIG_CHELSIO_IPSEC_INLINE || CONFIG_CHELSIO_TLS_DEVICE */
 +#if defined(CONFIG_CHELSIO_TLS_DEVICE)
 +	.tlsdev_ops = &chcr_ktls_ops,
 +#endif
++=======
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  };
  
  static void detach_work_fn(struct work_struct *work)
@@@ -241,23 -220,6 +243,26 @@@ int chcr_uld_rx_handler(void *handle, c
  	return 0;
  }
  
++<<<<<<< HEAD
 +#if defined(CONFIG_CHELSIO_IPSEC_INLINE) || defined(CONFIG_CHELSIO_TLS_DEVICE)
 +int chcr_uld_tx_handler(struct sk_buff *skb, struct net_device *dev)
 +{
 +	/* In case if skb's decrypted bit is set, it's nic tls packet, else it's
 +	 * ipsec packet.
 +	 */
 +#ifdef CONFIG_CHELSIO_TLS_DEVICE
 +	if (skb->decrypted)
 +		return chcr_ktls_xmit(skb, dev);
 +#endif
 +#ifdef CONFIG_CHELSIO_IPSEC_INLINE
 +	return chcr_ipsec_xmit(skb, dev);
 +#endif
 +	return 0;
 +}
 +#endif /* CONFIG_CHELSIO_IPSEC_INLINE || CONFIG_CHELSIO_TLS_DEVICE */
 +
++=======
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  static void chcr_detach_device(struct uld_ctx *u_ctx)
  {
  	struct chcr_dev *dev = &u_ctx->dev;
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 69fdfede6bbe,f55550d3a429..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@@ -1174,6 -1201,12 +1174,15 @@@ struct adapter 
  	struct cxgb4_tc_u32_table *tc_u32;
  	struct chcr_ktls chcr_ktls;
  	struct chcr_stats_debug chcr_stats;
++<<<<<<< HEAD
++=======
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
+ 	struct ch_ktls_stats_debug ch_ktls_stats;
+ #endif
+ #if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
+ 	struct ch_ipsec_stats_debug ch_ipsec_stats;
+ #endif
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  
  	/* TC flower offload */
  	bool tc_flower_initialized;
@@@ -2141,7 -2178,9 +2150,13 @@@ int cxgb_open(struct net_device *dev)
  int cxgb_close(struct net_device *dev);
  void cxgb4_enable_rx(struct adapter *adap, struct sge_rspq *q);
  void cxgb4_quiesce_rx(struct sge_rspq *q);
++<<<<<<< HEAD
 +#ifdef CONFIG_CHELSIO_TLS_DEVICE
++=======
+ int cxgb4_port_mirror_alloc(struct net_device *dev);
+ void cxgb4_port_mirror_free(struct net_device *dev);
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  int cxgb4_set_ktls_feature(struct adapter *adap, bool enable);
  #endif
  #endif /* __CXGB4_H__ */
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
index f6cc46ef72b5,5491a41fd1be..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
@@@ -3488,44 -3548,48 +3488,86 @@@ static int chcr_stats_show(struct seq_f
  		   atomic_read(&adap->chcr_stats.tls_pdu_rx));
  	seq_printf(seq, "TLS Keys (DDR) Count: %10u\n",
  		   atomic_read(&adap->chcr_stats.tls_key));
++<<<<<<< HEAD
 +#ifdef CONFIG_CHELSIO_TLS_DEVICE
++=======
+ #if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
+ 	seq_puts(seq, "\nChelsio Inline IPsec Crypto Accelerator Stats\n");
+ 	seq_printf(seq, "IPSec PDU: %10u\n",
+ 		   atomic_read(&adap->ch_ipsec_stats.ipsec_cnt));
+ #endif
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  	seq_puts(seq, "\nChelsio KTLS Crypto Accelerator Stats\n");
  	seq_printf(seq, "Tx TLS offload refcount:          %20u\n",
  		   refcount_read(&adap->chcr_ktls.ktls_refcount));
  	seq_printf(seq, "Tx HW offload contexts added:     %20llu\n",
++<<<<<<< HEAD
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_ctx));
 +	seq_printf(seq, "Tx connection created:            %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_connection_open));
 +	seq_printf(seq, "Tx connection failed:             %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_connection_fail));
 +	seq_printf(seq, "Tx connection closed:             %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_connection_close));
 +	seq_printf(seq, "Packets passed for encryption :   %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_encrypted_packets));
 +	seq_printf(seq, "Bytes passed for encryption :     %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_encrypted_bytes));
 +	seq_printf(seq, "Tx records send:                  %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_send_records));
 +	seq_printf(seq, "Tx partial start of records:      %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_start_pkts));
 +	seq_printf(seq, "Tx partial middle of records:     %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_middle_pkts));
 +	seq_printf(seq, "Tx partial end of record:         %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_end_pkts));
 +	seq_printf(seq, "Tx complete records:              %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_complete_pkts));
 +	seq_printf(seq, "TX trim pkts :                    %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_trimmed_pkts));
 +	seq_printf(seq, "Tx out of order packets:          %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_ooo));
 +	seq_printf(seq, "Tx drop pkts before HW offload:   %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_skip_no_sync_data));
 +	seq_printf(seq, "Tx drop not synced packets:       %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_drop_no_sync_data));
 +	seq_printf(seq, "Tx drop bypass req:               %20llu\n",
 +		   (u64)atomic64_read(&adap->chcr_stats.ktls_tx_drop_bypass_req));
++=======
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_ctx));
+ 	seq_printf(seq, "Tx connection created:            %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_connection_open));
+ 	seq_printf(seq, "Tx connection failed:             %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_connection_fail));
+ 	seq_printf(seq, "Tx connection closed:             %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_connection_close));
+ 	seq_printf(seq, "Packets passed for encryption :   %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_encrypted_packets));
+ 	seq_printf(seq, "Bytes passed for encryption :     %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_encrypted_bytes));
+ 	seq_printf(seq, "Tx records send:                  %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_send_records));
+ 	seq_printf(seq, "Tx partial start of records:      %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_start_pkts));
+ 	seq_printf(seq, "Tx partial middle of records:     %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_middle_pkts));
+ 	seq_printf(seq, "Tx partial end of record:         %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_end_pkts));
+ 	seq_printf(seq, "Tx complete records:              %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_complete_pkts));
+ 	seq_printf(seq, "TX trim pkts :                    %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_trimmed_pkts));
+ 	seq_printf(seq, "Tx out of order packets:          %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_ooo));
+ 	seq_printf(seq, "Tx drop pkts before HW offload:   %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_skip_no_sync_data));
+ 	seq_printf(seq, "Tx drop not synced packets:       %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_drop_no_sync_data));
+ 	seq_printf(seq, "Tx drop bypass req:               %20llu\n",
+ 		   atomic64_read(&adap->ch_ktls_stats.ktls_tx_drop_bypass_req));
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  #endif
- 
  	return 0;
  }
  DEFINE_SHOW_ATTRIBUTE(chcr_stats);
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index acd7043aee47,a952fe198eb9..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@@ -6027,7 -6396,50 +6027,54 @@@ static int cxgb4_iov_configure(struct p
  }
  #endif /* CONFIG_PCI_IOV */
  
++<<<<<<< HEAD
 +#if defined(CONFIG_CHELSIO_TLS_DEVICE)
++=======
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE) || IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
+ 
+ static int chcr_offload_state(struct adapter *adap,
+ 			      enum cxgb4_netdev_tls_ops op_val)
+ {
+ 	switch (op_val) {
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
+ 	case CXGB4_TLSDEV_OPS:
+ 		if (!adap->uld[CXGB4_ULD_KTLS].handle) {
+ 			dev_dbg(adap->pdev_dev, "ch_ktls driver is not loaded\n");
+ 			return -EOPNOTSUPP;
+ 		}
+ 		if (!adap->uld[CXGB4_ULD_KTLS].tlsdev_ops) {
+ 			dev_dbg(adap->pdev_dev,
+ 				"ch_ktls driver has no registered tlsdev_ops\n");
+ 			return -EOPNOTSUPP;
+ 		}
+ 		break;
+ #endif /* CONFIG_CHELSIO_TLS_DEVICE */
+ #if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
+ 	case CXGB4_XFRMDEV_OPS:
+ 		if (!adap->uld[CXGB4_ULD_IPSEC].handle) {
+ 			dev_dbg(adap->pdev_dev, "chipsec driver is not loaded\n");
+ 			return -EOPNOTSUPP;
+ 		}
+ 		if (!adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops) {
+ 			dev_dbg(adap->pdev_dev,
+ 				"chipsec driver has no registered xfrmdev_ops\n");
+ 			return -EOPNOTSUPP;
+ 		}
+ 		break;
+ #endif /* CONFIG_CHELSIO_IPSEC_INLINE */
+ 	default:
+ 		dev_dbg(adap->pdev_dev,
+ 			"driver has no support for offload %d\n", op_val);
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #endif /* CONFIG_CHELSIO_TLS_DEVICE || CONFIG_CHELSIO_IPSEC_INLINE */
+ 
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  
  static int cxgb4_ktls_dev_add(struct net_device *netdev, struct sock *sk,
  			      enum tls_offload_ctx_dir direction,
@@@ -6075,19 -6478,11 +6122,19 @@@ static void cxgb4_ktls_dev_del(struct n
  	struct adapter *adap = netdev2adap(netdev);
  
  	mutex_lock(&uld_mutex);
 -	if (chcr_offload_state(adap, CXGB4_TLSDEV_OPS))
 +	if (!adap->uld[CXGB4_ULD_CRYPTO].handle) {
 +		dev_err(adap->pdev_dev, "chcr driver is not loaded\n");
  		goto out_unlock;
 +	}
 +
 +	if (!adap->uld[CXGB4_ULD_CRYPTO].tlsdev_ops) {
 +		dev_err(adap->pdev_dev,
 +			"chcr driver has no registered tlsdev_ops\n");
 +		goto out_unlock;
 +	}
  
- 	adap->uld[CXGB4_ULD_CRYPTO].tlsdev_ops->tls_dev_del(netdev, tls_ctx,
- 							    direction);
+ 	adap->uld[CXGB4_ULD_KTLS].tlsdev_ops->tls_dev_del(netdev, tls_ctx,
+ 							  direction);
  	cxgb4_set_ktls_feature(adap, FW_PARAMS_PARAM_DEV_KTLS_HW_DISABLE);
  
  out_unlock:
@@@ -6100,6 -6495,114 +6147,117 @@@ static const struct tlsdev_ops cxgb4_kt
  };
  #endif /* CONFIG_CHELSIO_TLS_DEVICE */
  
++<<<<<<< HEAD
++=======
+ #if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
+ 
+ static int cxgb4_xfrm_add_state(struct xfrm_state *x)
+ {
+ 	struct adapter *adap = netdev2adap(x->xso.dev);
+ 	int ret;
+ 
+ 	if (!mutex_trylock(&uld_mutex)) {
+ 		dev_dbg(adap->pdev_dev,
+ 			"crypto uld critical resource is under use\n");
+ 		return -EBUSY;
+ 	}
+ 	ret = chcr_offload_state(adap, CXGB4_XFRMDEV_OPS);
+ 	if (ret)
+ 		goto out_unlock;
+ 
+ 	ret = adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops->xdo_dev_state_add(x);
+ 
+ out_unlock:
+ 	mutex_unlock(&uld_mutex);
+ 
+ 	return ret;
+ }
+ 
+ static void cxgb4_xfrm_del_state(struct xfrm_state *x)
+ {
+ 	struct adapter *adap = netdev2adap(x->xso.dev);
+ 
+ 	if (!mutex_trylock(&uld_mutex)) {
+ 		dev_dbg(adap->pdev_dev,
+ 			"crypto uld critical resource is under use\n");
+ 		return;
+ 	}
+ 	if (chcr_offload_state(adap, CXGB4_XFRMDEV_OPS))
+ 		goto out_unlock;
+ 
+ 	adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops->xdo_dev_state_delete(x);
+ 
+ out_unlock:
+ 	mutex_unlock(&uld_mutex);
+ }
+ 
+ static void cxgb4_xfrm_free_state(struct xfrm_state *x)
+ {
+ 	struct adapter *adap = netdev2adap(x->xso.dev);
+ 
+ 	if (!mutex_trylock(&uld_mutex)) {
+ 		dev_dbg(adap->pdev_dev,
+ 			"crypto uld critical resource is under use\n");
+ 		return;
+ 	}
+ 	if (chcr_offload_state(adap, CXGB4_XFRMDEV_OPS))
+ 		goto out_unlock;
+ 
+ 	adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops->xdo_dev_state_free(x);
+ 
+ out_unlock:
+ 	mutex_unlock(&uld_mutex);
+ }
+ 
+ static bool cxgb4_ipsec_offload_ok(struct sk_buff *skb, struct xfrm_state *x)
+ {
+ 	struct adapter *adap = netdev2adap(x->xso.dev);
+ 	bool ret = false;
+ 
+ 	if (!mutex_trylock(&uld_mutex)) {
+ 		dev_dbg(adap->pdev_dev,
+ 			"crypto uld critical resource is under use\n");
+ 		return ret;
+ 	}
+ 	if (chcr_offload_state(adap, CXGB4_XFRMDEV_OPS))
+ 		goto out_unlock;
+ 
+ 	ret = adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops->xdo_dev_offload_ok(skb, x);
+ 
+ out_unlock:
+ 	mutex_unlock(&uld_mutex);
+ 	return ret;
+ }
+ 
+ static void cxgb4_advance_esn_state(struct xfrm_state *x)
+ {
+ 	struct adapter *adap = netdev2adap(x->xso.dev);
+ 
+ 	if (!mutex_trylock(&uld_mutex)) {
+ 		dev_dbg(adap->pdev_dev,
+ 			"crypto uld critical resource is under use\n");
+ 		return;
+ 	}
+ 	if (chcr_offload_state(adap, CXGB4_XFRMDEV_OPS))
+ 		goto out_unlock;
+ 
+ 	adap->uld[CXGB4_ULD_IPSEC].xfrmdev_ops->xdo_dev_state_advance_esn(x);
+ 
+ out_unlock:
+ 	mutex_unlock(&uld_mutex);
+ }
+ 
+ static const struct xfrmdev_ops cxgb4_xfrmdev_ops = {
+ 	.xdo_dev_state_add      = cxgb4_xfrm_add_state,
+ 	.xdo_dev_state_delete   = cxgb4_xfrm_del_state,
+ 	.xdo_dev_state_free     = cxgb4_xfrm_free_state,
+ 	.xdo_dev_offload_ok     = cxgb4_ipsec_offload_ok,
+ 	.xdo_dev_state_advance_esn = cxgb4_advance_esn_state,
+ };
+ 
+ #endif /* CONFIG_CHELSIO_IPSEC_INLINE */
+ 
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  static int init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
  {
  	struct net_device *netdev;
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
index a963fd0b4540,ea2fabbdd934..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
@@@ -302,7 -303,9 +303,8 @@@ enum cxgb4_uld 
  	CXGB4_ULD_ISCSI,
  	CXGB4_ULD_ISCSIT,
  	CXGB4_ULD_CRYPTO,
 -	CXGB4_ULD_IPSEC,
  	CXGB4_ULD_TLS,
+ 	CXGB4_ULD_KTLS,
  	CXGB4_ULD_MAX
  };
  
@@@ -361,18 -364,8 +363,23 @@@ struct cxgb4_virt_res 
  	struct cxgb4_range ppod_edram;
  };
  
++<<<<<<< HEAD
 +struct chcr_stats_debug {
 +	atomic_t cipher_rqst;
 +	atomic_t digest_rqst;
 +	atomic_t aead_rqst;
 +	atomic_t complete;
 +	atomic_t error;
 +	atomic_t fallback;
 +	atomic_t ipsec_cnt;
 +	atomic_t tls_pdu_tx;
 +	atomic_t tls_pdu_rx;
 +	atomic_t tls_key;
 +#ifdef CONFIG_CHELSIO_TLS_DEVICE
++=======
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
+ struct ch_ktls_stats_debug {
++>>>>>>> a8c16e8ed624 (crypto/chcr: move nic TLS functionality to drivers/net)
  	atomic64_t ktls_tx_connection_open;
  	atomic64_t ktls_tx_connection_fail;
  	atomic64_t ktls_tx_connection_close;
@@@ -390,10 -383,27 +397,21 @@@
  	atomic64_t ktls_tx_skip_no_sync_data;
  	atomic64_t ktls_tx_drop_no_sync_data;
  	atomic64_t ktls_tx_drop_bypass_req;
- 
+ };
  #endif
+ 
+ struct chcr_stats_debug {
+ 	atomic_t cipher_rqst;
+ 	atomic_t digest_rqst;
+ 	atomic_t aead_rqst;
+ 	atomic_t complete;
+ 	atomic_t error;
+ 	atomic_t fallback;
+ 	atomic_t tls_pdu_tx;
+ 	atomic_t tls_pdu_rx;
+ 	atomic_t tls_key;
  };
  
 -#if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)
 -struct ch_ipsec_stats_debug {
 -	atomic_t ipsec_cnt;
 -};
 -#endif
 -
  #define OCQ_WIN_OFFSET(pdev, vres) \
  	(pci_resource_len((pdev), 2) - roundup_pow_of_two((vres)->ocq.size))
  
@@@ -470,9 -480,12 +488,9 @@@ struct cxgb4_uld_info 
  			      struct napi_struct *napi);
  	void (*lro_flush)(struct t4_lro_mgr *);
  	int (*tx_handler)(struct sk_buff *skb, struct net_device *dev);
- #if IS_ENABLED(CONFIG_TLS_DEVICE)
+ #if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
  	const struct tlsdev_ops *tlsdev_ops;
  #endif
 -#if IS_ENABLED(CONFIG_XFRM_OFFLOAD)
 -	const struct xfrmdev_ops *xfrmdev_ops;
 -#endif
  };
  
  void cxgb4_uld_enable(struct adapter *adap);
* Unmerged path drivers/net/ethernet/chelsio/inline_crypto/Kconfig
* Unmerged path drivers/net/ethernet/chelsio/inline_crypto/Makefile
* Unmerged path drivers/crypto/chelsio/Kconfig
* Unmerged path drivers/crypto/chelsio/Makefile
* Unmerged path drivers/crypto/chelsio/chcr_core.c
diff --git a/drivers/crypto/chelsio/chcr_core.h b/drivers/crypto/chelsio/chcr_core.h
index a751041d615e..8d5fed67307a 100644
--- a/drivers/crypto/chelsio/chcr_core.h
+++ b/drivers/crypto/chelsio/chcr_core.h
@@ -223,16 +223,4 @@ int chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,
 		     int err);
 int chcr_ipsec_xmit(struct sk_buff *skb, struct net_device *dev);
 void chcr_add_xfrmops(const struct cxgb4_lld_info *lld);
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
-int chcr_ktls_cpl_act_open_rpl(struct adapter *adap, unsigned char *input);
-int chcr_ktls_cpl_set_tcb_rpl(struct adapter *adap, unsigned char *input);
-int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev);
-extern int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,
-			     enum tls_offload_ctx_dir direction,
-			     struct tls_crypto_info *crypto_info,
-			     u32 start_offload_tcp_sn);
-extern void chcr_ktls_dev_del(struct net_device *netdev,
-			      struct tls_context *tls_ctx,
-			      enum tls_offload_ctx_dir direction);
-#endif
 #endif /* __CHCR_CORE_H__ */
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_ethtool.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_ethtool.c
index 60c7efff8408..b07aae33044d 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_ethtool.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_ethtool.c
@@ -128,7 +128,7 @@ static char adapter_stats_strings[][ETH_GSTRING_LEN] = {
 	"db_empty               ",
 	"write_coal_success     ",
 	"write_coal_fail        ",
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
+#if  IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
 	"tx_tls_encrypted_packets",
 	"tx_tls_encrypted_bytes  ",
 	"tx_tls_ctx              ",
@@ -265,7 +265,7 @@ struct adapter_stats {
 	u64 db_empty;
 	u64 wc_success;
 	u64 wc_fail;
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
+#if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
 	u64 tx_tls_encrypted_packets;
 	u64 tx_tls_encrypted_bytes;
 	u64 tx_tls_ctx;
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
index fe613bb8bdbb..743af9e654aa 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c
@@ -663,7 +663,7 @@ static int uld_attach(struct adapter *adap, unsigned int uld)
 	return 0;
 }
 
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
+#if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
 static bool cxgb4_uld_in_use(struct adapter *adap)
 {
 	const struct tid_info *t = &adap->tids;
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index d1c0b53b96ee..76d91a04f42e 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -1418,9 +1418,9 @@ static netdev_tx_t cxgb4_eth_xmit(struct sk_buff *skb, struct net_device *dev)
 		return adap->uld[CXGB4_ULD_CRYPTO].tx_handler(skb, dev);
 #endif /* CHELSIO_IPSEC_INLINE */
 
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
+#if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)
 	if (skb->decrypted)
-		return adap->uld[CXGB4_ULD_CRYPTO].tx_handler(skb, dev);
+		return adap->uld[CXGB4_ULD_KTLS].tx_handler(skb, dev);
 #endif /* CHELSIO_TLS_DEVICE */
 
 	qidx = skb_get_queue_mapping(skb);
* Unmerged path drivers/net/ethernet/chelsio/inline_crypto/Kconfig
* Unmerged path drivers/net/ethernet/chelsio/inline_crypto/Makefile
diff --git a/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/Makefile b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/Makefile
new file mode 100644
index 000000000000..5e7d161c3199
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0-only
+ccflags-y := -I $(srctree)/drivers/net/ethernet/chelsio/cxgb4
+
+obj-$(CONFIG_CHELSIO_TLS_DEVICE) += ch_ktls.o
+ch_ktls-objs := chcr_ktls.o
diff --git a/drivers/crypto/chelsio/chcr_common.h b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_common.h
similarity index 87%
rename from drivers/crypto/chelsio/chcr_common.h
rename to drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_common.h
index 33f589cbfba1..38319f4c3121 100644
--- a/drivers/crypto/chelsio/chcr_common.h
+++ b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_common.h
@@ -18,28 +18,6 @@
 #define CHCR_SCMD_AUTH_MODE_GHASH          4
 #define AES_BLOCK_LEN                      16
 
-enum chcr_state {
-	CHCR_INIT = 0,
-	CHCR_ATTACH,
-	CHCR_DETACH,
-};
-
-struct chcr_dev {
-	spinlock_t lock_chcr_dev; /* chcr dev structure lock */
-	enum chcr_state state;
-	atomic_t inflight;
-	int wqretry;
-	struct delayed_work detach_work;
-	struct completion detach_comp;
-	unsigned char tx_channel_id;
-};
-
-struct uld_ctx {
-	struct list_head entry;
-	struct cxgb4_lld_info lldi;
-	struct chcr_dev dev;
-};
-
 struct ktls_key_ctx {
 	__be32 ctx_hdr;
 	u8 salt[CHCR_MAX_SALT];
@@ -77,8 +55,6 @@ struct ktls_key_ctx {
 		      KEY_CONTEXT_SALT_PRESENT_F | \
 		      KEY_CONTEXT_CTX_LEN_V((ctx_len)))
 
-struct uld_ctx *assign_chcr_device(void);
-
 static inline void *chcr_copy_to_txd(const void *src, const struct sge_txq *q,
 				     void *pos, int length)
 {
diff --git a/drivers/crypto/chelsio/chcr_ktls.c b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.c
similarity index 92%
rename from drivers/crypto/chelsio/chcr_ktls.c
rename to drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.c
index 640fa34b6566..d0164a8d3aff 100644
--- a/drivers/crypto/chelsio/chcr_ktls.c
+++ b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.c
@@ -1,10 +1,18 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /* Copyright (C) 2020 Chelsio Communications.  All rights reserved. */
 
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/skbuff.h>
+#include <linux/module.h>
 #include <linux/highmem.h>
+#include <linux/ip.h>
+#include <net/ipv6.h>
+#include <linux/netdevice.h>
 #include "chcr_ktls.h"
-#include "clip_tbl.h"
+
+static LIST_HEAD(uld_ctx_list);
+static DEFINE_MUTEX(dev_mutex);
 
 static int chcr_init_tcb_fields(struct chcr_ktls_info *tx_info);
 /*
@@ -155,7 +163,7 @@ static int chcr_ktls_update_connection_state(struct chcr_ktls_info *tx_info,
 		/* Check if l2t state is valid, then move to ready state. */
 		if (cxgb4_check_l2t_valid(tx_info->l2te)) {
 			tx_info->connection_state = KTLS_CONN_TX_READY;
-			atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_ctx);
+			atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_ctx);
 		}
 		break;
 
@@ -381,9 +389,9 @@ static int chcr_ktls_mark_tcb_close(struct chcr_ktls_info *tx_info)
  * @tls_cts - tls context.
  * @direction - TX/RX crypto direction
  */
-void chcr_ktls_dev_del(struct net_device *netdev,
-		       struct tls_context *tls_ctx,
-		       enum tls_offload_ctx_dir direction)
+static void chcr_ktls_dev_del(struct net_device *netdev,
+			      struct tls_context *tls_ctx,
+			      enum tls_offload_ctx_dir direction)
 {
 	struct chcr_ktls_ofld_ctx_tx *tx_ctx =
 				chcr_get_ktls_tx_context(tls_ctx);
@@ -418,7 +426,7 @@ void chcr_ktls_dev_del(struct net_device *netdev,
 				 tx_info->tid, tx_info->ip_family);
 	}
 
-	atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_connection_close);
+	atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_connection_close);
 	kvfree(tx_info);
 	tx_ctx->chcr_info = NULL;
 	/* release module refcount */
@@ -434,10 +442,10 @@ void chcr_ktls_dev_del(struct net_device *netdev,
  * @direction - TX/RX crypto direction
  * return: SUCCESS/FAILURE.
  */
-int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,
-		      enum tls_offload_ctx_dir direction,
-		      struct tls_crypto_info *crypto_info,
-		      u32 start_offload_tcp_sn)
+static int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,
+			     enum tls_offload_ctx_dir direction,
+			     struct tls_crypto_info *crypto_info,
+			     u32 start_offload_tcp_sn)
 {
 	struct tls_context *tls_ctx = tls_get_ctx(sk);
 	struct chcr_ktls_ofld_ctx_tx *tx_ctx;
@@ -550,12 +558,12 @@ int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,
 		goto out2;
 	}
 
-	atomic64_inc(&adap->chcr_stats.ktls_tx_connection_open);
+	atomic64_inc(&adap->ch_ktls_stats.ktls_tx_connection_open);
 	return 0;
 out2:
 	kvfree(tx_info);
 out:
-	atomic64_inc(&adap->chcr_stats.ktls_tx_connection_fail);
+	atomic64_inc(&adap->ch_ktls_stats.ktls_tx_connection_fail);
 	return ret;
 }
 
@@ -603,7 +611,8 @@ static int chcr_init_tcb_fields(struct chcr_ktls_info *tx_info)
 /*
  * chcr_ktls_cpl_act_open_rpl: connection reply received from TP.
  */
-int chcr_ktls_cpl_act_open_rpl(struct adapter *adap, unsigned char *input)
+static int chcr_ktls_cpl_act_open_rpl(struct adapter *adap,
+				      unsigned char *input)
 {
 	const struct cpl_act_open_rpl *p = (void *)input;
 	struct chcr_ktls_info *tx_info = NULL;
@@ -638,7 +647,7 @@ int chcr_ktls_cpl_act_open_rpl(struct adapter *adap, unsigned char *input)
 /*
  * chcr_ktls_cpl_set_tcb_rpl: TCB reply received from TP.
  */
-int chcr_ktls_cpl_set_tcb_rpl(struct adapter *adap, unsigned char *input)
+static int chcr_ktls_cpl_set_tcb_rpl(struct adapter *adap, unsigned char *input)
 {
 	const struct cpl_set_tcb_rpl *p = (void *)input;
 	struct chcr_ktls_info *tx_info = NULL;
@@ -795,7 +804,7 @@ static int chcr_ktls_xmit_tcb_cpls(struct chcr_ktls_info *tx_info,
 						 TCB_SND_UNA_RAW_V
 						 (TCB_SND_UNA_RAW_M),
 						 TCB_SND_UNA_RAW_V(0), 0);
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_ooo);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_ooo);
 		cpl++;
 	}
 	/* update ack */
@@ -1223,7 +1232,7 @@ static int chcr_ktls_xmit_wr_complete(struct sk_buff *skb,
 
 	chcr_txq_advance(&q->q, ndesc);
 	cxgb4_ring_tx_db(adap, &q->q, ndesc);
-	atomic64_inc(&adap->chcr_stats.ktls_tx_send_records);
+	atomic64_inc(&adap->ch_ktls_stats.ktls_tx_send_records);
 
 	return 0;
 }
@@ -1634,7 +1643,7 @@ static int chcr_end_part_handler(struct chcr_ktls_info *tx_info,
 	/* check if it is a complete record */
 	if (tls_end_offset == record->len) {
 		nskb = skb;
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_complete_pkts);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_complete_pkts);
 	} else {
 		dev_kfree_skb_any(skb);
 
@@ -1652,7 +1661,7 @@ static int chcr_end_part_handler(struct chcr_ktls_info *tx_info,
 		 */
 		if (chcr_ktls_update_snd_una(tx_info, q))
 			goto out;
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_end_pkts);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_end_pkts);
 	}
 
 	if (chcr_ktls_xmit_wr_complete(nskb, tx_info, q, tcp_seq,
@@ -1723,7 +1732,7 @@ static int chcr_short_record_handler(struct chcr_ktls_info *tx_info,
 		/* free the last trimmed portion */
 		dev_kfree_skb_any(skb);
 		skb = tmp_skb;
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_trimmed_pkts);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_trimmed_pkts);
 	}
 	data_len = skb->data_len;
 	/* check if the middle record's start point is 16 byte aligned. CTR
@@ -1795,7 +1804,7 @@ static int chcr_short_record_handler(struct chcr_ktls_info *tx_info,
 		 */
 		if (chcr_ktls_update_snd_una(tx_info, q))
 			goto out;
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_middle_pkts);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_middle_pkts);
 	} else {
 		/* Else means, its a partial first part of the record. Check if
 		 * its only the header, don't need to send for encryption then.
@@ -1810,7 +1819,7 @@ static int chcr_short_record_handler(struct chcr_ktls_info *tx_info,
 			}
 			return 0;
 		}
-		atomic64_inc(&tx_info->adap->chcr_stats.ktls_tx_start_pkts);
+		atomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_start_pkts);
 	}
 
 	if (chcr_ktls_xmit_wr_short(skb, tx_info, q, tcp_seq, tcp_push_no_fin,
@@ -1826,13 +1835,13 @@ static int chcr_short_record_handler(struct chcr_ktls_info *tx_info,
 }
 
 /* nic tls TX handler */
-int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev)
+static int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct chcr_ktls_ofld_ctx_tx *tx_ctx;
+	struct ch_ktls_stats_debug *stats;
 	struct tcphdr *th = tcp_hdr(skb);
 	int data_len, qidx, ret = 0, mss;
 	struct tls_record_info *record;
-	struct chcr_stats_debug *stats;
 	struct chcr_ktls_info *tx_info;
 	u32 tls_end_offset, tcp_seq;
 	struct tls_context *tls_ctx;
@@ -1878,7 +1887,7 @@ int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev)
 		return NETDEV_TX_BUSY;
 
 	adap = tx_info->adap;
-	stats = &adap->chcr_stats;
+	stats = &adap->ch_ktls_stats;
 
 	qidx = skb->queue_mapping;
 	q = &adap->sge.ethtxq[qidx + tx_info->first_qset];
@@ -2015,4 +2024,117 @@ int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev)
 	dev_kfree_skb_any(skb);
 	return NETDEV_TX_OK;
 }
-#endif /* CONFIG_CHELSIO_TLS_DEVICE */
+
+static void *chcr_ktls_uld_add(const struct cxgb4_lld_info *lldi)
+{
+	struct chcr_ktls_uld_ctx *u_ctx;
+
+	pr_info_once("%s - version %s\n", CHCR_KTLS_DRV_DESC,
+		     CHCR_KTLS_DRV_VERSION);
+	u_ctx = kzalloc(sizeof(*u_ctx), GFP_KERNEL);
+	if (!u_ctx) {
+		u_ctx = ERR_PTR(-ENOMEM);
+		goto out;
+	}
+	u_ctx->lldi = *lldi;
+out:
+	return u_ctx;
+}
+
+static const struct tlsdev_ops chcr_ktls_ops = {
+	.tls_dev_add = chcr_ktls_dev_add,
+	.tls_dev_del = chcr_ktls_dev_del,
+};
+
+static chcr_handler_func work_handlers[NUM_CPL_CMDS] = {
+	[CPL_ACT_OPEN_RPL] = chcr_ktls_cpl_act_open_rpl,
+	[CPL_SET_TCB_RPL] = chcr_ktls_cpl_set_tcb_rpl,
+};
+
+static int chcr_ktls_uld_rx_handler(void *handle, const __be64 *rsp,
+				    const struct pkt_gl *pgl)
+{
+	const struct cpl_act_open_rpl *rpl = (struct cpl_act_open_rpl *)rsp;
+	struct chcr_ktls_uld_ctx *u_ctx = handle;
+	u8 opcode = rpl->ot.opcode;
+	struct adapter *adap;
+
+	adap = pci_get_drvdata(u_ctx->lldi.pdev);
+
+	if (!work_handlers[opcode]) {
+		pr_err("Unsupported opcode %d received\n", opcode);
+		return 0;
+	}
+
+	work_handlers[opcode](adap, (unsigned char *)&rsp[1]);
+	return 0;
+}
+
+static int chcr_ktls_uld_state_change(void *handle, enum cxgb4_state new_state)
+{
+	struct chcr_ktls_uld_ctx *u_ctx = handle;
+
+	switch (new_state) {
+	case CXGB4_STATE_UP:
+		pr_info("%s: Up\n", pci_name(u_ctx->lldi.pdev));
+		mutex_lock(&dev_mutex);
+		list_add_tail(&u_ctx->entry, &uld_ctx_list);
+		mutex_unlock(&dev_mutex);
+		break;
+	case CXGB4_STATE_START_RECOVERY:
+	case CXGB4_STATE_DOWN:
+	case CXGB4_STATE_DETACH:
+		pr_info("%s: Down\n", pci_name(u_ctx->lldi.pdev));
+		mutex_lock(&dev_mutex);
+		list_del(&u_ctx->entry);
+		mutex_unlock(&dev_mutex);
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static struct cxgb4_uld_info chcr_ktls_uld_info = {
+	.name = CHCR_KTLS_DRV_MODULE_NAME,
+	.nrxq = 1,
+	.rxq_size = 1024,
+	.add = chcr_ktls_uld_add,
+	.tx_handler = chcr_ktls_xmit,
+	.rx_handler = chcr_ktls_uld_rx_handler,
+	.state_change = chcr_ktls_uld_state_change,
+	.tlsdev_ops = &chcr_ktls_ops,
+};
+
+static int __init chcr_ktls_init(void)
+{
+	cxgb4_register_uld(CXGB4_ULD_KTLS, &chcr_ktls_uld_info);
+	return 0;
+}
+
+static void __exit chcr_ktls_exit(void)
+{
+	struct chcr_ktls_uld_ctx *u_ctx, *tmp;
+	struct adapter *adap;
+
+	pr_info("driver unloaded\n");
+
+	mutex_lock(&dev_mutex);
+	list_for_each_entry_safe(u_ctx, tmp, &uld_ctx_list, entry) {
+		adap = pci_get_drvdata(u_ctx->lldi.pdev);
+		memset(&adap->ch_ktls_stats, 0, sizeof(adap->ch_ktls_stats));
+		list_del(&u_ctx->entry);
+		kfree(u_ctx);
+	}
+	mutex_unlock(&dev_mutex);
+	cxgb4_unregister_uld(CXGB4_ULD_KTLS);
+}
+
+module_init(chcr_ktls_init);
+module_exit(chcr_ktls_exit);
+
+MODULE_DESCRIPTION("Chelsio NIC TLS ULD driver");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Chelsio Communications");
+MODULE_VERSION(CHCR_KTLS_DRV_VERSION);
diff --git a/drivers/crypto/chelsio/chcr_ktls.h b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.h
similarity index 74%
rename from drivers/crypto/chelsio/chcr_ktls.h
rename to drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.h
index 5cbd84b1da05..4ae6ae38c406 100644
--- a/drivers/crypto/chelsio/chcr_ktls.h
+++ b/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.h
@@ -4,14 +4,17 @@
 #ifndef __CHCR_KTLS_H__
 #define __CHCR_KTLS_H__
 
-#ifdef CONFIG_CHELSIO_TLS_DEVICE
-#include <net/tls.h>
 #include "cxgb4.h"
 #include "t4_msg.h"
 #include "t4_tcb.h"
 #include "l2t.h"
 #include "chcr_common.h"
 #include "cxgb4_uld.h"
+#include "clip_tbl.h"
+
+#define CHCR_KTLS_DRV_MODULE_NAME "ch_ktls"
+#define CHCR_KTLS_DRV_VERSION "1.0.0.0-ko"
+#define CHCR_KTLS_DRV_DESC "Chelsio NIC TLS ULD Driver"
 
 #define CHCR_TCB_STATE_CLOSED	0
 #define CHCR_KTLS_KEY_CTX_LEN	16
@@ -69,6 +72,11 @@ struct chcr_ktls_ofld_ctx_tx {
 	struct chcr_ktls_info *chcr_info;
 };
 
+struct chcr_ktls_uld_ctx {
+	struct list_head entry;
+	struct cxgb4_lld_info lldi;
+};
+
 static inline struct chcr_ktls_ofld_ctx_tx *
 chcr_get_ktls_tx_context(struct tls_context *tls_ctx)
 {
@@ -82,22 +90,12 @@ chcr_get_ktls_tx_context(struct tls_context *tls_ctx)
 static inline int chcr_get_first_rx_qid(struct adapter *adap)
 {
 	/* u_ctx is saved in adap, fetch it */
-	struct uld_ctx *u_ctx = adap->uld[CXGB4_ULD_CRYPTO].handle;
+	struct chcr_ktls_uld_ctx *u_ctx = adap->uld[CXGB4_ULD_KTLS].handle;
 
 	if (!u_ctx)
 		return -1;
 	return u_ctx->lldi.rxq_ids[0];
 }
 
-int chcr_ktls_cpl_act_open_rpl(struct adapter *adap, unsigned char *input);
-int chcr_ktls_cpl_set_tcb_rpl(struct adapter *adap, unsigned char *input);
-int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev);
-int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,
-		      enum tls_offload_ctx_dir direction,
-		      struct tls_crypto_info *crypto_info,
-		      u32 start_offload_tcp_sn);
-void chcr_ktls_dev_del(struct net_device *netdev,
-		       struct tls_context *tls_ctx,
-		       enum tls_offload_ctx_dir direction);
-#endif /* CONFIG_CHELSIO_TLS_DEVICE */
+typedef int (*chcr_handler_func)(struct adapter *adap, unsigned char *input);
 #endif /* __CHCR_KTLS_H__ */
